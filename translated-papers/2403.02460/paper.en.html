<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'a mesh to be modified, provide a textual prompt, and hallucinate an updated region (See Figure 1). As we demonstrate, in addition to the contribution to control, our hybrid representation also benefits computational efforts and overall geometric quality, as various priors can be placed more intuitively on the two representations.\n' +
      '\n' +
      'The key technical challenge of the hybrid approach is keeping the two representations synced efficiently. To achieve this, we differentiably render both representations from various angles, and require consistency in RGB renders, opacity and normal maps. Furthermore, we rely on the in-sync-mesh representation to render the SDF at higher resolutions and faster; Instead of the hundreds of samples per rays, we localize the SDF sampling around the mesh surface, and use as little as three samples. Critically, evolving a mesh consistently and stably is an additional challenge. In terms of resolution, a coarse mesh would not be expressive enough for novel details, and a fine mesh is expensive and unstable. Hence, an adaptive tessellation is required, that evolves along with the shape where required. We rely on recent developments in differentiable mesh reconstruction (Barda et al., 2023) to achieve dynamic mesh topology updates, including face splitting, edge collapse, and edge flips. To texture the mesh despite changes in its topology, we contribute a new strategy based on triangle supersampling. Importantly, using this layer, we can maintain mesh properties throughout the optimization.\n' +
      '\n' +
      'As we demonstrate, our hybrid approach allows localized and sequential mesh editing operations, preserving mesh topology and information on one hand, while allowing radical and semantic evolution on the other. In addition, we show overall higher generated geometric quality, thanks to the priors the two representations impose on each other. The hybrid approach and sculpting tool demonstrate how the merits of both leading representations can be combined. In essence, our framework brings the recent and future breakthroughs in neural shape generation closer to artistic workflows, where work is in incremental steps, giving the artist precision and control over the end result, but with unprecedented expressiveness.\n' +
      '\n' +
      '## 2. Related Work\n' +
      '\n' +
      '**3D generative models.** In their seminal work, DreamFusion, pool et al. (Poole et al., 2022) show that Text-to-Image diffusion models can be used to provide gradients to optimize a Neural Radiance Field (NeRF) via Score Distillation Sampling (SDS). Magic3D (Lin et al., 2023) achieves better quality by using a two stage approach: the first stage is similar to DreamFusion, and they note that the quality of the generated object is limited by the high cost of performing volumetric rendering for high resolutions images. The second stage uses a differentiable mesh representation to further refine the generated object, as differentiably rendering meshes in high resolution is significantly cheaper in both time and memory. Magic123 (Qian et al., 2023) further improves upon Magic3d by using both 3d and 2d diffusion priors. ProlificDreamer (Wang et al., 2023) proposes an improvement over SDS, the VSD loss, to drive 3D generation from 2d diffusion priors. Fantasia3d (Chen et al., 2023) and TextMesh (Tsalicoglou et al., 2023) decouple the appearance from the geometry by replacing the NeRF with an SDF, and optimizing a color network separately. TextDeformer (Gao et al., 2023) uses CLIP as prior together with a novel gradient smoothing technique based on mesh Jacobians to deform meshes according to a text prompt.\n' +
      '\n' +
      'The choice of using two stages in Magic3D (Lin et al., 2023) highlights the tradeoffs involved in choosing the right 3D representation for 3D generative models. While implicit functions are well suited for coarse generation because they allow topology updates, meshes can be rasterized very efficiently at a high resolution to get fine details in a second step. However, two-stage pipelines are prone to local minima and crucially, cannot be extended to edit an existing mesh with pre-computed UVs. In contrast, we jointly optimise a hybrid SDF and mesh representation, that can be initialized from an existing mesh and maintain all of its properties during optimization, benefiting from the best of both worlds in a _1-step pipeline_: topology updates from the SDF part and fine details from the mesh part.\n' +
      '\n' +
      '_Focus on local editing._Vox-E (Sella et al., 2023) edit a voxel grid via SDS and use the attention layers to encourage localized edits. However, they can not guarantee where the edit will happen because the localization mechanism is based on soft attention.\n' +
      '\n' +
      '_Concurrent work:_ DreamCraft3D (Sun et al., 2023) builds on the recent SDS works by fine-tuning the diffusion model during the generative process using DreamBooth (Ruiz et al., 2022). Of note, Instant3D (Li et al., 2023) generates a 3D shape in a single forward pass, without require any costly optimization. While it does not allow for local artistic controls similar to the sculpting application we present, we are excited to leverage ideas from this research direction to accelerate our method in the future.\n' +
      '\n' +
      '**Hybrid Representations.** There is no ubiquitous representation in 3D, as there exist in 2D for images, thus several representations exist and have been combined for diverse 3D tasks. The plethora of representations and combination show that there is no one-fit-for-all solution. In this work, we introduce a hybrid representation specialized for generative modeling and focus the related work on hybrids most relevant to this paper. Poursaeed et al. (Poursaeed et al., 2020) uses a coupling of implicit and explicit surface representation for generative 3D modeling, kept in sync by 3D losses. NerfMeshing (Rakotosona et al., 2023) proposes a improved meshing pipeline for NeRFs. In contrast to (Poursaeed et al., 2020), the coupling is not achieved via coupled regularization losses, but explicitly enforced by projection layers from the SDF to the mesh. Finally, DmTeT (Shen et al., 2021) proposes deep marching Tetrahedra as a hybrid representation for high-resolution 3D Shape synthesis, notably used in the concurrent work Magic3D (Lin et al., 2023). Our method uses both a set of regularization losses, as well as a dynamic projection layer based on ROAR (Barda et al., 2023) to keep the SDF and mesh part in sync.\n' +
      '\n' +
      '**Traditional approaches for sculpting meshes.** Many commercial tools employ digital sculpting metaphor for 3D modeling, such as Zbrush (Zbrush, 2024), Mudbox (Autodesk, 2024), or Substance-Modeler (SubstanceModeler, 2024). Motivated by these workflows, geometry processing research focused on improving interactive techniques such as mesh deformation (Jacobson et al., 2014), mesh blending for cut-and-paste (Biermann et al., 2002), local parameterization for adding surface details (Schmidt et al., 2006), symmetry-guided autocompletion (Peng et al., 2018), and version control for collaborative editing (Salvati et al., 2015). Despite these advances, 3Dmodeling remains to be only accessible to experts. As an alternative, example-based approaches have been proposed to democratize 3D modeling tools by using existing geometry from a database of stock 3D models to assemble new shapes from parts (Funkhouser et al., 2004). Subsequent methods have built statistical models over part assemblies (Kalogerakis et al., 2012), and allow high-level semantic control for deformations (Yumer et al., 2015). Despite their accessibility, these tools are often restricted in their domain, and often rely on heavy annotation of stock assets, and thus have received limited use by professional modelers. In this paper, we utilize pretrained 2D generative data prior to enable semantic controls for local and iterative modeling workflow without the need of preannotated 3D stock data.\n' +
      '\n' +
      '## 3. Method\n' +
      '\n' +
      'Given a mesh, a user-highlighted surface region, and a text prompt that describes the desired target, MagicClay optimizes the shape of the selected region so that the resulting mesh matches the target. To drive the shape optimization, we follow current literature and use the Score Distillation Sampling (SDS) technique (Poole et al., 2022) with differentiable rendering to leverage on text-conditioned 2D diffusion and guide the shape optimization. This approach, however, does not perform well when operated on meshes. Meshes are driven by sparse and irregular samples (vertices), and their connectivity mandates a stable and smooth deformation, avoiding self-intersections and flip-overs. For this reason, we employ a neural Signed Distance Field (SDF) to drive the mesh shape optimization and topology updates. We thus propose a hybrid representation that captures both a Signed Distance Field (SDF) and the surface, gaining from the advantages of both worlds. While the SDF allows guiding the shape towards larger-scale complex changes, the mesh allows to capture fine details, and localize control to the user-highlighted surface region.\n' +
      '\n' +
      'In this section we provide details on the hybrid SDF/Mesh representations (Sec. 3.1), how it can be efficiently optimized with SDS guidance (Sec. 3.2), how to effectively use surface and volumetric priors (Sec. 3.3), and how to update the mesh topology during optimization (Sec. 3.4). Figure 2 overviews the full pipeline.\n' +
      '\n' +
      '### Hybrid Representation\n' +
      '\n' +
      'Our hybrid representation consists of a surface (a mesh), a volume (an SDF), and a shared appearance network encoding RGB colors for an input 3D coordinate. Both the surface and the volumetric representations can be differentially rendered, leveraging the shared appearance network to output images with color, normals, and opacity channels. We now detail the three elements of our hybrid shape representation.\n' +
      '\n' +
      'Surface RepresentationWe represent the surface of the shape as a 2-manifold triangular mesh. Mesh topology, or sampling resolution, is locally adapted according to the SDF (see Sec. 3.4 for details). We\n' +
      '\n' +
      'Figure 2. Overview of the hybrid optimization. We jointly optimize a mesh, an SDF and a shared appearance MLP according to an input prompt. We can either optimize the full geometry, or only a user-selected portion of the mesh for an iterative 3D modeling workflow. We start by differentiably rendering both representations, and enforce their consistency. As they are kept in sync, we use the mesh to efficiently sample volumetric rays to render hi-res maps from the SDF in a memory-efficient manner. In addition to the standard SDS loss, the high-res renderings allows using high quality VSD losses (Wang et al., 2023) to evolve the SDF. We keep the mesh surface and the SDF in sync via multi-view consistency constraints on the RGB pixels, the image opacity and the surface normals. The mesh local topology is updated according to the SDF using ROAR (Barda et al., 2023), splitting triangles where geometry is created, and collapsing edges where needed. Additionally, we leverage representation-specific losses to regularize the optimization: an Eikonal loss on the SDF and a smoothness loss on the mesh.\n' +
      '\n' +
      'encode colors for the mesh using an auxiliary appearance network, derived from the SDF itself (see below). We found this approach simpler and more natural than traditional mesh coloring techniques; Using per-vertex colors is sensitive to triangulation, and would require a large number of vertices to match the resolution of the SDF. Using a texture image requires a complex UV parameterization, usually done a priori on a fixed shape. In addition, our surface is continuously optimized and undergoes through topological changes, re-tessellation, and large-scale deformations, which makes it computationally infeasible to apply traditional UV parameterization techniques during this optimization.\n' +
      '\n' +
      'Instead, our hybrid approach offers a simpler approach to shape coloring. To apply the colors from the appearance network to the mesh, we propose to adaptively subdivide each face of the base mesh according to triangle area. Since we only use these subdivided triangles to represent colors they do not have to form a connected mesh unlike traditional subdivision techniques. Thus, we employ MeshColors scheme that was originally proposed for UV-less texturing [23], and has an efficient GPU implementation. In the inset we illustrate the example subdivision, note how sub-triangles on two adjacent faces do not share the vertices along the edge. During rendering we assign a color to each sub-triangle, by using the coordinate of the three associated supersampled vertices to sample the appearance network.\n' +
      '\n' +
      'Signed Distance FunctionsOur volumetric shape representation is chosen off-the-shelf, and conceptually serves as a regularization guiding the mesh evolution using existing state-of-the-art text2shape tools. We use a neural SDF, or an implicit continuous scalar field that can be sampled anywhere in \\(\\mathbb{R}^{3}\\), returning a signed shortest distance to the surface (negative on the inside, positive on the outside). We encode the SDF using a multiresolution hash encoding of features defined over a grid which are then mapped to distance value by a small MLP, following instant-NGP [24]. As in the mesh case, the shared appearance network is sampled to obtain colors during rendering.\n' +
      '\n' +
      'Appearance NetworkThe shared appearance network encodes colors implicitly as a map over \\(\\mathbb{R}^{3}\\). It shares the same hash grid as the SDF, but has a smaller MLP head, with a single hidden layer that take hash grid features as input and outputs RGB values.\n' +
      '\n' +
      '### Hybrid Shape Guidance\n' +
      '\n' +
      'In essence, our shape optimization is based on Score-Distillation Sampling (SDS) to distill gradients from a text prompt. The primary motivation to maintain an SDF representation in addition to the mesh is because SDFs are more robust noisy guidance, which is an inherent property of the multi-view SDS approach (see Figure 5). We thus choose to inject the text guidance only to the auxiliary SDF representation, and propagate the changes to the mesh via the consistency losses (Sec. 3.3) and the topology updates (Sec. 3.4).\n' +
      '\n' +
      'To apply the text guidance and the consistency losses, we need to render both representation differentiably. We use Nvdiffrast [12] to render meshes and VolSDF [13] for volumetric rendering of our SDF. Clearly, as mesh rasterization is much cheaper than volumetric rendering, the process is bottlenecked by the resolution at which we can render the SDF, both in terms of speed and memory. Our hybrid representation uniquely enables a strategy to render SDF faster and cheaper, at a higher resolution of 512x512. This is achieved thanks to the consistency between the mesh and SDF representations throughout the optimization. We can significantly reduce the typical 512 samples per ray necessary for rendering the SDF by using the intersection of the ray with the mesh representation (efficiently calculated by the differentiable mesh renderer). Using the intersection as the center of a small spread of samples (typically 3), this allows for high resolution renders of the SDF (i.e. 512x512 and larger), which are otherwise memory prohibitive. The idea of leveraging the surface to reduce the number of network queries per ray emerged in concurrent works, namely Adaptive Shell [25] and HybridNerf [24], which shows its generality and success in other settings than ours.\n' +
      '\n' +
      'Using this strategy, we render the SDF in 512x512 and apply the VSD loss of those high-res renderings. We also apply regular SDS on lower-res 128x128 renderings by regular VolSDF as we find that this improves the results slightly.\n' +
      '\n' +
      '### Representation Priors\n' +
      '\n' +
      'In addition to the text guidance, we apply representation-specific regularizations and consistency losses that keep both representations in sync.\n' +
      '\n' +
      'Consistency LossThe SDF and the mesh are consistent if their images are in 1 to 1 correspondences from any camera angle. We thus supervise the L2 difference between their RGB renderings, normal maps and opacity maps. If the renderings are made at different resolution, we downsize to the lower resolution before computing the L2 loss.\n' +
      '\n' +
      'Enforcing Localization and Freeze LossTo localize changes to the user-selected area we first fix the mesh vertices in all non-selected regions during optimization by zeroing out gradients outside of user selection. While localization is harder to achieve for SDF, we add a sampling-based freeze loss, which favors regions around fixed vertices to remain unchanged:\n' +
      '\n' +
      '\\[s(v_{\\text{sampled}})=0 \\tag{1}\\]\n' +
      '\n' +
      'where \\(v_{\\text{sampled}}\\) are vertices sampled uniformly over the faces which are not part of the optimization region selected by the user.\n' +
      '\n' +
      'Laplacian (Smoothness) LossWhile it is harder to regularize the surface of an implicit function to be smooth, the explicit representation of the mesh allows to easily define a smoothness term using the Laplacian of the mesh, defined for each vertex:\\[\\delta(x_{i})=x_{i}-\\frac{\\Sigma_{j}x_{j}}{N}, \\tag{2}\\]\n' +
      '\n' +
      'where \\(x_{j}\\) are neighbors of \\(x_{i}\\). The Laplacian vector encodes local geometry changes, with a smooth mesh is defined by low Laplacian vectors, and we use a global smoothness loss:\n' +
      '\n' +
      '\\[L_{\\text{smooth}}=\\Sigma_{i}||\\delta_{i}||. \\tag{3}\\]\n' +
      '\n' +
      'SDF Eikonal LossTo encourage the implicit function to learn a valid SDF representation we use the Eikonal term as a loss. The SDF \\(s\\) is valid if and only if the loss in Eqn 4 is 0:\n' +
      '\n' +
      '\\[L_{\\text{Eik}}=\\Sigma_{x}(||\\nabla s(x)||-1)^{2} \\tag{4}\\]\n' +
      '\n' +
      'SDF opacity and normal LossInspired by TextMesh [14], we also binarize the SDF opacity and apply a Binary Cross Entropy loss to encourage discrete 0 or 1 values. To penalize badly oriented normals of the implicit surface, we apply an L2 penalty to the dot product between the normal and the camera direction if it is negative.\n' +
      '\n' +
      '### Updating the mesh topology\n' +
      '\n' +
      'To maintain consistency between the mesh and SDF, it is necessary to perform local topology edits on the mesh in that increase or decrease mesh resolution where required. Continuous Remeshing [13] pioneered such a local topology update approach, by using the Adam optimizer state as a signal. While this approach works well in a multi-view reconstruction scenario, where the images are sharp, and the camera parameters known, the noise involved in SDS makes the gradients, and by extension the Adam state, very noisy and an unstable signal to trigger those operations. We turn to another work, ROAR [1], particularly well-tailored to our hybrid representation. Within this framework, we use the SDF as the signal to trigger mesh triangle splits.\n' +
      '\n' +
      'In a nutshell, for each triangle on the mesh, ROAR starts by supersampling the triangle into K sub-faces, and projects each sub-vertices on the 0-level set of the SDF \\(s\\) using a projection operator:\n' +
      '\n' +
      '\\[P(x)=-s(x)\\cdot\\nabla s(x) \\tag{5}\\]\n' +
      '\n' +
      'This projection results in a piece-wise linear surface that approximate the implicit surface closest to the initial triangle. The decision to split this triangle is based on the curvature score of this piece of projected surface. If the surface is very curved, then the triangle is split using \\(\\sqrt{3}\\)-subdivision [12]. Similarly, each edge is assigned a score based on the quadratic distance of its vertices to all the planes in the 1-ring of the edge, which intuitively represents how important is the edge to the geometry. If the score is low, then the edge can be collapsed with Qslim [1].\n' +
      '\n' +
      'We refer the interested reader to the ROAR paper [1] for more details, but the important point to note that ROAR offers a principled way to perform edge collapses and face splits in the sense that each iteration of ROAR strictly decrease an energy - the difference between the highest face score and the lowest edge score. It thus exhibits a convergence behavior after enough iterations. We also note that manifoldness is guaranteed to be preserved throughout the iterations.\n' +
      '\n' +
      '## 4. Experiments\n' +
      '\n' +
      'We implement our pipeline in Threestudio [1], and use the implementations of other methods provided in the framework for comparisons with Stable Diffusion v1.5 as the backbone diffusion model. All experiments presented in this work were executed on a single A100-40GB GPU.\n' +
      '\n' +
      'In the rest of this section we compare our representation to prior work on text-conditioned 3D generation (Sec. 4.1), demonstrate its utility in a mesh sculpting application (Sec. 4.2), and compare to a text-driven mesh deformation baseline (Sec. 4.3). We then provide a simple illustrative experiment to motivate the hybrid representation when using SDS guidance with noisy gradients (Sec 4.4), and finally ablate our method (Sec 4.5).\n' +
      '\n' +
      '### Comparison with Generative Methods\n' +
      '\n' +
      'Since MagicClay is a modeling tool, we are primarily interested in evaluating the quality of the geometry and thus focus on mesh renderings without texture. Note that existing 3D generative techniques are not designed to edit a part of an existing mesh, and thus we compare performance of our hybrid approach on the task of text-to-3D generation. We compare against three recent approaches: Fantasia3d [1], ProlificDreamer [23] and TextMesh [14]. The representation used in TextMesh [14] is an SDF, so we run marching cube on their output to evaluate the quality of the mesh. ProlificDreamer uses a two stage approach: first a NeRF-base generation, then a refinement using an explicit representation. Fantasia3d uses DMtet [1] to extract a mesh from an SDF.\n' +
      '\n' +
      'We present our results in Figure 3. Even though each approach generates a representation that can produce realistic RGB renders,\n' +
      '\n' +
      'Figure 3. **Comparison on text-to-3D from scratch.** We compare the quality of the triangular meshes extracted from various state-of-the-art generative methods: Fantasia3d [1], ProlificDreamer [23] and TextMesh [14]. While all methods produce realistic RGB renderings, only our hybrid representation generates smooth geometry.\n' +
      '\n' +
      'extracted meshes often exhibit significant surface artifacts, which make them hardly recognizable without texture (see "Chow Chow puppy" by ProlificDreamer or "Croissant" by TextMesh). By comparison, our geometries are recognizable and smooth thanks to the fact that our hybrid representation enables an explicit regularization of the surface. This validates that MagicClay successfully bridges the generative capabilities of implicit radiance fields with the surface-level controls of meshes.\n' +
      '\n' +
      '### Mesh Sculpting\n' +
      '\n' +
      'We further demonstrate that our method can be used to enable novel iterative 3D sculpting workflows. Starting with an initial mesh, an artist can select region of interest (and optionally sculpt a coarse adjustment to that region) along with a textual prompt describing the desired updated shape. MagicClay generates a modified mesh, which could be iteratively refined with new elements. Note that hybrid representation is essential to this application. First, selecting and adjusting the region of interest is easily accomplished using standard mesh editing tools (Blender, 2024). Second, the generated result tends to be more expressive with additional SDF representation guidance. Third, using the mesh allows us to keep non-selected surface regions intact by zeroing out their deformation gradients, which guarantees that the change will only affect the user-selected region. Refer to Figures 1, 9 and supplemental video for example results. MagicClay generates a high quality edits, that match the rough local edit and adhere to the user\'s text prompt.\n' +
      '\n' +
      '### Comparison with TextDeformer\n' +
      '\n' +
      'Our method is the first to tackle the problem of interactive, localized mesh sculpting via text-based prompts. A naive alternative would be to simply use the existing text-driven mesh deformation technique (Gao et al., 2023) on the user-modified mesh, still aiming to achieve desired changes in the geometry. In Figure 4 we compare our method to this baseline. Note how our method is able to add geometrically complex large-scale details due to guidance from SDF and topological updates. Our method\'s changes are also restricted only to user-modified region, and do not lead to large-scale deformations in the other parts of the input.\n' +
      '\n' +
      '### Analysis of Mesh and SDF robustness to noise\n' +
      '\n' +
      'We now illustrate the motivation for our hybrid representation by a simple controlled experiment, where we aim to reconstruct a fixed 3D target with different levels of noise in the guidance. We formulate it as a reconstruction problem to have a clear ground truth and eliminate ambiguity arising due to text-based objectives. Even though we use synthetic noise, we expect these findings to apply in an SDS setting, where gradients are also noisy due to random noising step performed at each SDS iteration (Poole et al., 2022).\n' +
      '\n' +
      'Given multi-view renderings of a fixed (true) 3D model, we add uncorrelated per-pixel Gaussian noise to each image, and compute L2 pixel-wise loss to guide our shape representation towards the target. As we increase the noise level (by increasing standard deviation) we find that different representations are more prone to errors in reconstructing the target. We use L2 re-projection error with respect to the ground truth shape as our evaluation metric, and compare vanilla Mesh, SDF representations to our hybrid approach (using our mesh rendering), and show results in Figure 5. Note that as the amount of noise increases, the quality of the mesh reconstruction degrades sharply. At the highest noise regimes (standard deviation of 5), the mesh reconstruction degenerates to a blob, while the SDF reconstruction is still recognizable despite surface irregularities. Importantly, the hybrid representation performs better than both individual representation at all levels of noise, and the benefits are the strongest at higher noise level.\n' +
      '\n' +
      '### Ablations\n' +
      '\n' +
      'We perform several ablations to justify the design of our system.\n' +
      '\n' +
      '_No face supersampling for mesh colors._ Figure 6 illustrates the need for the mesh-driven super-sampling of the appearance network based on Mesh Colors schema (Sec 3.1). We optimize our\n' +
      '\n' +
      'Figure 4. **Comparisons to TextDeformer (Gao et al., 2023). Given the user-modified input mesh, we either run TextDeformer or our method to modify it towards the target prompt.**\n' +
      '\n' +
      'Figure 5. **Mesh and SDF robustness to noisy gradients. We optimize a mesh, an SDF our our hybrid representation with multi-view reconstruction losses after applying various noise levels to the ground truth renderings. We report the L2 reprojection error against the ground truth renders. The SDF exhibit more robustness than the mesh to high noise regime, and our hybrid outperforms both.**\n' +
      '\n' +
      'representation with respect to a target image (first column), either sampling colors per-vertex (second column), or using our adaptive sampling using MeshColors (third column). Note that without MeshColors sampling a much higher resolution is needed, which leads to poorer reconstruction and an over-tesellated mesh.\n' +
      '\n' +
      'Not enforcing localization, no freeze lossWe remove the mechanism for enforcing localization via fixing non-selected surface regions and nearby SDF values as discussed in Sec. 3.3. In Figure 7 we show that without this feature, the shapes undergo unintended global changes, potentially erasing the original shape.\n' +
      '\n' +
      'No topology updatesThe topological updates (Sec. 3.4) during the optimization significantly improve the results as they allow to gradually add resolution. Optimizing a fixed-resolution mesh would either result in a shape that only marginally differs from input if the initial resolution is too high (Fig 8, left), or lacks fine details if the initial resolution is too coarse (Fig 8, right).\n' +
      '\n' +
      '## 5. Conclusion\n' +
      '\n' +
      'We presented MagicClay, a generative sculpting tool, backed by our new hybrid SDF and mesh representation. We demonstrated the importance of the hybrid representation through careful analysis and baselines. Key to the success of the generative process is our new rendering strategy that leverages the mesh part of the hybrid representation to localize ray sampled in the volumetric rendering of the SDF. We believe MagicClay is an important step towards turning the recent advancements in text-to-image-from-scratch into an actual modeling tool usable by artists in an iterative workflow.\n' +
      '\n' +
      'LimitationsOur method is inherently constrained by the quality of the SDS gradients. This stems from the inability of the current generative model to generate _consistent_ multi-view images during score-distillation sampling. Each view tracts the optimization in a different direction which results in a noisy process, rendering the emergence of fine details more difficult. That said, MagicClay employs current text-to-shape methods without adaptation, and can easily integrate and benefit from the unavoidable further development of the field. Second, MagicClay is not interactive, as running MagicClay takes 1 hr per prompt on an A100 GPU. We know from the NeRF literature that a couple of dozens of multi-view images are sufficient to reconstruct 3D shapes very efficiently. Hence, we believe that as diffusion models become better at generating consistent images, the number of iterations required by MagicClay will drop significantly, making it much faster.\n' +
      '\n' +
      'Future workWe see several venues for future research. First, we see opportunities to leverage inpainting and depth-conditioned diffusion models. Indeed, this generative process transforms the full object in each rendering, whereas it is clear that some part of the generated image should stay the same as the 3D edit is localized. We think that leveraging this insight would reduce the amount of noise in the 3D optimization helping the optimization reach a better geometry and faster. Second, we would like to explore using image targets with our system. We believe this could help making edits more specific, and also could allow more control, where users can highlight which part of the image should affect the generated shape. Finally, embellishing the surface representation to capture high resolution geometric details (such as normal and bump maps) and connecting it to SDF representation with appropriate consistency losses, can further improve the quality of the resulting shapes.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)\n' +
      '* Autodesk (2024) Autodesk. 2024. Mudbox. [https://www.autodesk.com/products/mudbox](https://www.autodesk.com/products/mudbox).\n' +
      '* Barda et al. (2023) Amir Barda, Yotam Erel, Yotam Kasten, and Amir H. Bermono. 2023. ROAR: Robust Adaptive Reconstruction of Shapes Using Planar Projections. arXiv:2307.00690 [cs.GR].\n' +
      '* Biermann et al. (2002) Henning Biermann, Ioana Martin, Fausto Bernardini, and Denis Zorin. 2002. Cut-and-Paste Editing of Multiresolution Surfaces.\n' +
      '* Blender (2024) Blender. 2024. [http://www.blender.org](http://www.blender.org).\n' +
      '* Chen et al. (2023) Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023. Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. arXiv:2303.13873 [cs.CV]\n' +
      '\n' +
      'Figure 8. **Ablation: no topology updates.** Optimizing the mesh without topology update results in the final generated object being limited by the initial resolution. **Left** When starting with a fine mesh the optimization will often get stuck since each vertex has tiny effect on the objective. **Right** When starting from a coarse mesh, no fine details can be created.\n' +
      '\n' +
      'Figure 6. **Ablation: no color supersampling.** Using mesh-guided supersampling in conjunction with the appearance network allows to decouple geometry and appearance. Using this approach (top right), large faces are used for the mesh, even though the rendering still presents high frequency colors (bottom). When using a color-per-vertex scheme (top middle), significantly larger mesh resolution is required to achieve similar appearance.\n' +
      '\n' +
      'Figure 7. **Ablation: not enforcing localization.** Without localization and freeze losses we observe that shape changes can propagate beyond the user-highlighted area, potentially destroying the original content. Here armadillo was erased by â€œangel wings.\n' +
      '\n' +
      '* [Chen and Zhang2019] Zhiqin Chen and Hao Zhang. 2019. Learning Implicit Fields for Generative Shape Modeling.\n' +
      '* [Funkhouser et al.2004] Thomas Funkhouser, Michael Kazhdan, Philip Shilane, Patrick Min, William Kiefer, Ayellet Tal, Szymon Rusinkiewicz, and David Dobkin. 2004. Modeling by Example. _ACM Transactions on Graphics_ (2004).\n' +
      '* [Gao et al.2023] William Gao, Noon Augerman, Thiabult Groueix, Vladimir G. Kim, and Rana Hancoka. 2023. TextDeformer: Geometry Manipulation using Text Guidance. arXiv:2304.13348 [cs.CV]\n' +
      '* Garland and Heckbert (2023) Michael Garland and Paul S. Heckbert. 2023. Surface Simplification Using Quadric Error Metrics, 8 pages. [https://doi.org/10.1145/359671.3596727](https://doi.org/10.1145/359671.3596727)\n' +
      '* Guo et al. (2020) Yuan-Chou Guo, Ying-Tian Lu, Ruihai Shi, Christian Laorte, Vikram Voelci, Guan Luo, Chi-Hao Chen, Zi-Xin Zou, Chen Wang, Yan-Fei Cao, and Song-Hai Zhang. 2020. threefold: A unified framework for 3D content generation. [https://github.com/thew09cti/bertresearch](https://github.com/thew09cti/bertresearch).\n' +
      '* Jacobson et al. (2014) Alec Jacobson, Zhigang Deng, Ladislav Kavan, and J.P. Lewis. 2014. Skimming: Real-time Shape Deformation.\n' +
      '* Logorakis et al. (2012) Evangelos Kalogorakis, Siddhartha Chaudhuri, Daphne Koller, and Vladlen Koltun. 2012. A Probabilistic Model of Component-Based Shape Synthesis. _ACM Transactions on Graphics_ 31, 4 (2012).\n' +
      '* Kobbelt (2000) Earl Kobbelt. 2000. Sqr(3)-Sublitation. _ACM SIGGRAPH_ 2000 (05 2000).\n' +
      '* Laine et al. (2020) Samulul Laine, James Hellsten, Teero Karras, Yeongho Seel, Jaakko Lehtinen, and Timo Aila. 2020. Modular Primitives for High-Performance Differentiable Rendering. _ACM Transactions on Graphics_ 39, 6 (2020).\n' +
      '* Liao et al. (2023) Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhmrovrovich, and Sali B. 2023. Instal(tm): Fast text-to-3d with sparse-view generation and large reconstruction model.\n' +
      '* Lin et al. (2023) Chen-Hsuan Lin, Jin Gao, Luning Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kresin, Saria Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2023. Magic3D: High-Resolution Text-to-3D Content Creation. arXiv:2110.1404 [cs.CV]\n' +
      '* Millenall et al. (2021) Ben Millenall, Pratul P Srinivasan, Matthew Tanci, Jonathan T Barron, Ravi Ramamoamoorthi, and Ren Ng. 2021. Next: Representing scenes as neural radiance fields for view synthesis. 90-106 pages.\n' +
      '* Muller et al. (2022) Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. _ACM Trans. Graph_ 41, 4: Article 102 (July 2022), 15 pages. [https://doi.org/10.1145/3528223.35302127](https://doi.org/10.1145/3528223.35302127)\n' +
      '* Palinger (2022) Werner Palinger. 2022. Continuous remember for inverse rendering. _Computer Animation and Virtual Workshops_ 39 (7 2022). [https://doi.org/10.1002/cvzn2011](https://doi.org/10.1002/cvzn2011)\n' +
      '* Park et al. (2019) Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation.\n' +
      '* Jung and Wei (2018) Mengqi Peng, Jun Xing, and Li-Yi Wei. 2018. Autocomplete 3D Sculpting.\n' +
      '* Poole et al. (2022) Ben Poole, Ajay Liu, Jonathan T. Barron, and Ben Mildenhall. 2022. DreamFusion: Text-to-3D Using 2D Diffusion. arXiv:2209.14988 [cs.CV]\n' +
      '* ECCV 2020_. Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer International Publishing, Cham, 667-683.\n' +
      '* Qian et al. (2023) Guocheng Qian, Jingxie Abdullah Hamd, Jian Ren, Aaliskandar Sarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard Ghanen. 2023. Magic12: On Image to High-Quality 3D Object Generation Using 2020 and 3D Diffusion Priors. arXiv:2306.17858 [cs.CV]\n' +
      '* Rakotsotsson et al. (2023) Marie-Julie Rakotsson, Fabian Mahmatiel, Diego Martin Arroyo, Michael Niemeyer, Ahbiuut Kunde, and Federico Tonnoliar. 2023. NeRFMashing: Distilling Neural Radio-ance Fields into Geometrically-Accurate M Usehres. arXiv:2303.09431 [cs.CV]\n' +
      '* Ruiz et al. (2022) Natanat Ruiz, Yuanzhe Li, Varun Jagmani, Yael Pirch, Michael Rubinstein, and Kfir Abermann. 2022. DreamDooth: Fine Tuning Text-to-image Diffusion Models for Subject-Driven Generation.\n' +
      '* Salvati et al. (2015) Gabriele Salvati, Christian Santoni, Valentina Tibaldo, and Fabio Pellacini. 2015. MeshHirst: Collaborative Modeling by Sharing and Retargeting Editing Histories. _ACM Trans. Graph_ (2015).\n' +
      '* Schmidt et al. (2006) R Schmidt, C. Grimm, and B Wyvill. 2006. Interactive declong with discrete exponential maps.\n' +
      '* Sella et al. (2023) Rii Sella, Gal Fleelman, Peter Hedman, and Hadar Averbuch-Elor. 2023. Vox-E: Text-guided Voxel Editing of 3D Objects. arXiv:2303.12048 [cs.CV]\n' +
      '* Shen et al. (2021) Tanchang Shen, Jun Gao, Kangueo Yin, Ming-Yu Liu, and Sanja Fidler. 2021. Deep Marching Tetrahedra: A Hybrid Representation for High-Resolution 3D Shape Synthesis.\n' +
      '* Mudgele (2024) SubstoneModele. 2024. Substance Modeler. [https://www.adobe.com/ie/products/substance3d-modeler.html](https://www.adobe.com/ie/products/substance3d-modeler.html)\n' +
      '* Sun et al. (2023) Jingxing Sun, Bo Zhang, Ruhish Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. 2023. DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior. arXiv:2310.1618 [cs.CV]\n' +
      '* Tsalcooglu et al. (2023) Christina Tsalcooglu, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tonnoliar. 2023. TextMesh: Generation of Realistic 3D Meshes From Text Prompts. arXiv:2304.12493 [cs.CV]\n' +
      '* Turki et al. (2023) Haifhem Turki, Vasu Agrawal, Samuel Rota Bulo, Lorenzo Porzi, Peter Kontschieder, Deva Ramanan, Michael Zollhofer, and Christian Richard. 2023. HybridNet: Efficient Neural Rendering via Adaptive Volumetric Surfaces. arXiv:2312.03160 [cs.CV]\n' +
      '* Wang et al. (2023) Zhengyi Wang, Cheng Lu, Yixai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. 2023. Drift-Difference: High-fidelity and Diverse Text-to-3D Generation with Variational Score Distillation. arXiv:2305.16123 [cs.LG]\n' +
      '* Wang et al. (2023) Zian Wang, Tianchang Shen, Merlin Nimier-David, Nicholas Sharp, Jun Gao, Alexander Keller, Sanja Fidler, Thomas Muller, and Zan Ogolic. 2023. Adaptive Shells for Efficient Neural Radiance Field Rendering. : 15 pages. [https://doi.org/10.1145/3618390](https://doi.org/10.1145/3618390)\n' +
      '* Yariv et al. (2021) Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume rendering of neural implicit surfaces.\n' +
      '* Yuksel et al. (2010) Cen Yuksel, John Keyser, and Donald House. 2010. Mesh Colors. _ACM Trans. Graph._ 29 (03 2010). [https://doi.org/10.1145/3713047.371053](https://doi.org/10.1145/3713047.371053)\n' +
      '* Yuner et al. (2015) Ersin Yumer, Siddhina Chaudhuri, Jessica Hodgins, and Levent Burak Kara. 2015. Semantic Shape Editing Using Deformation Handles.\n' +
      '* Zbrush (2024) Zbrush. 2024. [https://www.maxon.net/en/zbrush](https://www.maxon.net/en/zbrush).\n' +
      '\n' +
      'Figure 9: **Sculpting gallery.**_Left_: from a source mesh, the user performs a rough edit in under two minutes, highlighted in yellow. _Right_: MagicClay refines it to match the provided prompt.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>