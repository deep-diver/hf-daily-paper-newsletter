<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 눈을 즐기다: 멀티모달에 대한 혼합 해상도 적응\n' +
      '\n' +
      '대용량 언어 모델\n' +
      '\n' +
      'Gen Luo\n' +
      '\n' +
      'Yiyi Zhou\n' +
      '\n' +
      'Yuxin Zhang\n' +
      '\n' +
      'Xiawu Zheng\n' +
      '\n' +
      'Xiaoshuai Sun\n' +
      '\n' +
      'Rongrong Ji\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '현저한 진보에도 불구하고, 기존의 멀티모달 대형 언어 모델들(MLLM)은 여전히 세분화된 시각적 인식에서 열등하다. 기존의 연구와는 달리 영상 해상도 관점에서 이 문제를 연구하고, 저해상도 및 고해상도 영상 특징의 조합이 이러한 단점을 효과적으로 완화할 수 있음을 밝힌다. 이러한 관찰을 바탕으로, 우리는 MLLM에 대한 새롭고 효율적인 방법, 즉 _Mixture-of-Resolution Adaptation_ (MRA)를 제안한다. 특히, MRA는 해상도가 다른 이미지에 대해 두 가지 시각적 경로를 채택하는데, 여기서 고해상도 시각적 정보는 새로운 _mixture-of-resolution adapters_(MR-Adapters)를 통해 저해상도 경로에 내장된다. 이 설계는 또한 MLLM의 입력 시퀀스 길이를 크게 감소시킨다. MRA를 검증하기 위해 LLaVA라는 최근 MLLM에 적용하고 새로운 모델 _LLaVA-HR_라고 한다. 11개의 비젼 언어(VL) 태스크에 대해 광범위한 실험을 수행하여 LLaVA-HR이 기존의 MLLM보다 8개의 VL 태스크에서, 예를 들어, TextVQA에서 +9.4% 우수함을 보인다. 더 중요한 것은, LLaVA-HR의 훈련 및 추론 모두 LLaVA-1.5보다 MRA, _예:_*20 훈련 시간** 및 **3\\(\\times\\) 추론 속도**_에서 효율적이다. 소스 코드는 [https://github.com/luogen1996/LLaVA-HR](https://github.com/luogen1996/LLaVA-HR]에서 방출된다.\n' +
      '\n' +
      '머신러닝, ICML, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델(LLM)(Touvron et al., 2023; Chen et al., 2020)의 현저한 성공에 힘입어, 멀티모달 대형 언어 모델(MLLM)에 대한 연구도 기계 학습 커뮤니티에 대한 관심의 유입을 받는다(Liu et al., 2023; Luo et al., 2023; Alayrac et al., 2022; Chen et al., 2022; 2023b). 최근 다양한 비전 언어 작업에 대한 돌파구를 달성하면서 LLM을 더 많은 양식으로 확장하는 데 많은 노력이 기울여지고 있다(Goyal et al., 2017; Singh et al., 2019; Hudson and Manning, 2019). 진보에도 불구하고, 기존의 MLLM은 여전히 세분화된 시각적 인식에 미치지 못한다. 예를 들어, 강력한 GPT4-V는 또한 작고 가려진 물체를 식별할 때 환각을 겪는다(Tong et al., 2024). 이러한 단점은 필연적으로 MLLM의 실질적인 사용을 제한한다.\n' +
      '\n' +
      '이러한 단점을 보완하기 위해, 실무자들은 종종 모델 크기를 스케일링하고 트레이닝당 데이터 크기를 증가시키는 것에 의존한다(Alayrac et al., 2022; Li et al., 2023b; Bai et al., 2023). 예를 들어, InstructBLIP(Dai et al., 2023)는 비젼-언어(VL) 정렬을 위해 129M 이상의 이미지-텍스트 쌍을 채택하고, 더 큰 시각적 인코더가 MLLM에 유익함을 보여준다. 이에 의해 동기화된 Qwen-VL(Bai et al., 2023)은 시각적 인코더의 파라미터를 19억 개로 더 증가시키고 15억 개의 사전-트레이닝 데이터를 사용한다. 진보에도 불구하고, 이 패러다임은 엄청나게 비싸며, 종종 약 수천 시간의 GPU 시간을 소비한다.\n' +
      '\n' +
      '이러한 작업과 직교하여 입력 영상 해상도의 관점에서 MLLM의 시각적 단점을 연구한다. 이전 VL 연구에서 밝혀진 바와 같이 (Jiang et al., 2020; Tong\n' +
      '\n' +
      '그림 1: TextVQA에 대한 LLaVA-HR 및 기존 MLLM의 **제로샷 성능 및 추론 속도.** 기존 MLLM은 종종 TextVQA와 같은 세밀한 VL 작업에 미치지 못한다. 이미지 해상도를 높이는 것은 효과적이지만 비용이 많이 드는 솔루션입니다. 제안된 MRA를 통해, LLaVA-HR은 성능을 향상시키기 위해 고해상도 이미지를 효율적으로 채택할 수 있다.\n' +
      '\n' +
      'Luo et al., 2024; Luo et al., 2023b), 입력 이미지의 해상도를 증가시키는 것은 시각적 인식을 개선하기 위한 간단한 해결책이며, 이는 _visual chain-of-thought_(Rose et al., 2023)를 수반하는 MLLM들에 대해 더 중요해진다. 도 1에 도시된 바와 같다. 도 1을 참조하면, LLaVA-1.5(Liu et al., 2023a)의 해상도를 384\\(\\times\\)384에서 672\\(\\times\\)672로 증가시키면 TextVQA(Singh et al., 2019)에서 명백한 성능 향상(+4.6%)을 가져올 수 있다. 그러나 고해상도 이미지의 사용은 MLLM의 이미 높은 계산 비용을 크게 악화시킬 것이다. 예를 들어, \\(448\\times 448\\) 해상도는 기본 \\(336\\times 336\\)에 비해 LLaVA의 계산 복잡도를 약 1.4배 증가시킬 것이다. 또한, MLLM들의 복잡한 구조로 인해, 도 1에 도시된 바와 같이, \\(1,022\\times 1,022\\) 해상도에서의 급격한 감소인, 해상도가 크게 증가함에 따라 트레이닝이 불안정해질 것이다. 시각적 시퀀스들의 길이가 미리 트레이닝된 컨텍스트 길이를 크게 초과하고, 트레이닝 불안정으로 이어지는 것을 가정한다.\n' +
      '\n' +
      '본 논문에서는 MLLM의 고해상도 영상 적응을 위한 새롭고 효율적인 방법인 _mixture-of-resolution adaptation_ (MRA) 방법을 제안한다. 도 1에 도시된 바와 같다. 1, MRA는 높은 해상도와 낮은 해상도의 입력 영상을 동시에 처리하기 위해 혁신적인 이중 시각 경로 설계를 채택한다. 구체적으로, 한 경로는 저해상도 이미지의 글로벌 정보를 인코딩하는 것을 목표로 하는 반면, 다른 경로는 고해상도 이미지에서 세밀한 의미론을 캡처하는 역할을 한다. 한편, 이 두 경로는 고해상도의 시각적 정보를 저해상도 모델링에 포함시키는 새로운 _mixture-of-resolution adapters_(MR-Adapters)를 통해 밀접하게 상호 작용한다. 이러한 방식으로, 매크로 뷰에서 마이크로 뷰에 이르기까지 입력 이미지를 표현하기 위해 훨씬 적은 수의 시각적 토큰을 사용할 수 있다. 이중 경로 구조의 세심한 설계로 MRA는 높은 효율을 유지하면서 1,536\\(\\times\\)1,536 픽셀까지 쉽게 이미지 해상도를 높일 수 있다.\n' +
      '\n' +
      'MRA를 검증하기 위해 LLaVA(Liu et al., 2023b;a)라는 최근 MLLM에 적용하고 새로운 모델을 LLaVA-HR로 명명한다. VQA2.0(Goyal et al., 2017)과 같은 일반적인 VL 태스크와 POPE(Li et al., 2023c)와 같은 새로운 벤치마크를 포함한 11개의 비전 언어(VL) 태스크에 대해 광범위한 실험을 수행한다. 실험 결과, LLaVA-HR은 TextVQA에서 LLaVA-1.5에 비해 11개의 VL 작업 중 8개에서 기존 MLLM보다 우수한 성능을 보였다. 더 중요한 것은, LLaVA-HR의 훈련 및 추론이 비용 효율적이라는 것이다. LLaVA-HR(7B, 1,024 \\(\\times\\) 1,024)의 사전 훈련 및 명령어 튜닝은 8개의 A800 GPU에서 총 20.7시간이 소요되며, 이는 InstructBLIP(Dai et al., 2023) 및 Qwen-VL(Bai et al., 2023)보다 _hundred배 저렴하다. 동일한 해상도로, 그 추론 속도는 LLaVA-1.5보다 _3배 빠르다(Liu et al., 2023a).\n' +
      '\n' +
      '요약하자면, 우리의 기여는 세 배입니다:\n' +
      '\n' +
      '* 우리는 MLLM에 대한 이미지 해상도의 중요성을 밝히고 훈련 및 추론을 효율적으로 유지하면서 고해상도 시각적 정보의 이점을 얻기 위해 새로운 이중 시각적 경로 설계를 채택하는 새롭고 효율적인 적응 방식인 _mixture-of-resolution adaption_(MRA)를 제안한다.\n' +
      '* 우리는 시각적 설명력을 향상시키기 위해 고해상도 정보를 저해상도 시각적 경로에 내장할 수 있는 MRA용 새로운 _mixture-of-resolution adapter_(MR-Adapter)를 제안한다.\n' +
      '* MRA를 기반으로 11개의 VL 작업 중 8개에서 기존 MLLM을 능가하고 대부분의 MLLM보다 훨씬 저렴한 교육 비용을 지출하는 강력한 MLLM, 신조 LLaVA-HR을 제안한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### 멀티모달 대형 언어 모델\n' +
      '\n' +
      '대형 언어 모델들(LLM)(Gilardi et al., 2023; Touvron et al., 2023; Chen et al., 2020)의 큰 성공에 의해, 단부-대-단 멀티모달 대형 언어 모델들(MLLMs)(Liu et al., 2023b; Zhu et al., 2023; Luo et al., 2023a; Fuyu-8B, 2023; Peng et al., 2023; Liu et al., 2023c). 특히, 대부분의 기존 MLLM들은 모듈형 구조(Luo et al., 2023a; Liu et al., 2023b)를 채택하고 있는데, 이는 이를 활용한다.\n' +
      '\n' +
      '그림 2: **기존 MLLM과 LLaVA-HR의 비교. 높은 계산 복잡도로 인해, 기존의 MLLM들(Liu et al., 2023a; Li et al., 2023b)은 종종 저해상도의 입력 이미지들을 사용하는데, 이는 세분화된 시각적 추론에 불충분하다. 제안된 LLaVA-HR은 혼합 해상도 적응을 통해 제한된 추가 비용으로 최대 1,536\\(\\times\\)1,536의 영상 해상도를 증가시킬 수 있다.\n' +
      '\n' +
      '시각적 특징을 LLM의 단어 임베딩 공간에 투영하는 중간 네트워크. 그런 다음 LLM은 자기회귀 방식으로 다양한 VL 작업을 수행하는 데 사용된다. 모듈형 구조를 기반으로 기존 MLLM은 중간 네트워크의 설계로 구분할 수 있다. LLaVA(Liu et al., 2023b)로 표현되는 인기 MLLM들은 종종 시각적 인코더와 LLM을 연결하기 위해 선형 투영 층 또는 MLP 층을 채택한다(Liu et al., 2023b; Chen et al., 2023a; Chen et al., 2023a;b; Peng et al., 2023). 다른 작업들은 시각적 인코더와 LLM 사이의 갭을 브릿지하기 위해 샘플러 기반 모듈들을 채용한다(Bai et al., 2023; Alayrac et al., 2022; Li et al., 2023b; Dai et al., 2023). 이러한 샘플러 기반 모듈은 시각적 토큰의 수를 효과적으로 줄일 수 있지만, 유망한 성능을 달성하기 위해 종종 대규모 사전 훈련을 필요로 한다(Bai et al., 2023; Li et al., 2023b). 효과성에도 불구하고, 대부분의 기존 MLLM은 여전히 낮은 시각적 해상도, 예를 들어,_336\\(\\times\\)336을 채택하여 세밀한 작업에서의 성능을 크게 제한한다.\n' +
      '\n' +
      '### MLLM을 위한 시각적 표현\n' +
      '\n' +
      '더 나은 시각적 표현의 추구는 VL 커뮤니티에서 인기 있는 연구 경향이었다(Lu et al., 2019; Jiang et al., 2020; Radford et al., 2021; Ren et al., 2024). 초기 노력은 주로 VL 모델에 대한 객체 수준 특징을 탐구한다(Lu et al., 2019; Zhang et al., 2021). 대규모 이미지-텍스트 사전-트레이닝에 의해 구동된, CLIP(Radford et al., 2021)로부터의 그리드 특징들은 MLLM(Liu et al., 2023b; Chen et al., 2022; Alayrac et al., 2022)에서 큰 효율 및 일반화를 입증했다. 그리드 특징을 기반으로 기존 연구자들은 주로 시각적 인코더를 확장함으로써 시각적 표현을 개선한다. 예를 들어, PaLI(Chen et al., 2022)는 시각적 인코더의 파라미터를 300억으로 증가시키고 MLLM의 상당한 성능 부스트를 나타낸다. 이와는 대조적으로, 이미지 해상도의 관점에서 MLLM에 대한 시각적 표현을 개선하고, 새롭고 효율적인 해결책, 즉 혼합 해상도 적응을 제안한다.\n' +
      '\n' +
      '## 3 Preliminary\n' +
      '\n' +
      '먼저 이미지 인코더\\(\\mathcal{F}_{\\mathcal{I}}(\\cdot)\\), 중간 네트워크\\(\\mathcal{F}_{\\mathcal{P}}(\\cdot)\\) 및 LLM\\(\\mathcal{F}_{\\mathcal{L}(\\cdot)\\)으로 구성된 멀티모달 대형 언어 모델(MLLM)의 구조를 요약한다.\n' +
      '\n' +
      '특히 입력 영상\\(I\\in\\mathbb{R}^{H\\times W\\times 3}\\)과 텍스트 명령\\(T\\in\\mathbb{R}^{L}\\)이 주어지면, 영상 인코더를 통해 시각적 토큰\\(\\mathbf{F}_{v}\\in\\mathbb{R}^{(h\\times w)\\times d}\\)을 얻고, 텍스트 토큰\\(f_{t}\\in\\mathbb{R}^{l\\times d}\\)은 해당 단어 임베딩으로 표현된다. 시각적 토큰 및 텍스트 토큰에 기초하여, LLM은 타겟 워드를 단계적으로 디코딩할 것이며, 이는 다음과 같이 공식화된다.\n' +
      '\n' +
      '\\[p_{t}=\\prod_{s=1}^{S+1}\\mathcal{L}(R_{s}|\\mathcal{F}_{\\mathcal{P}}(\\mathbf{F}_{v}),f_{t},R_{0:s-1})\\tag{1}\\\n' +
      '\n' +
      '여기서 \\(p_{t}\\in\\mathbb{R}^{m}\\)은 예측 단어의 확률을 나타내고 \\(m\\)은 단어 어휘의 크기를 나타낸다.\n' +
      '\n' +
      '일부 MLLMs(Liu et al., 2023b;a)에서 \\(\\mathcal{F}_{\\mathcal{P}}(\\cdot)\\)는 종종 단순한 선형 층들의 스택으로서, 이는 시각적 토큰들을 LLMs들의 의미 공간 상에 직접 투영하는데 사용된다. 비록 간단하고 효과적이지만, 이 전략은 LLaVA-1.5에서 1,022\\(\\times\\)1,022 해상도에 대한 _예,_5,329 토큰의 해상도가 증가함에 따라 필연적으로 더 긴 시각적 시퀀스로 이어진다. 실제로, 이러한 많은 수의 토큰을 처리하는 것은 MLLM에서 계산적으로 비싸다. 시각적 토큰의 수를 더 줄이기 위해, 최근의 진보들은 LLM이 직접 다룰 수 있는 여러 토큰으로 시각적 특징을 통합하는 \\(\\mathcal{F}_{\\mathcal{P}}(\\cdot)\\), _예를 들어,__QFormer_(Li et al., 2023b)에 대한 샘플러 기반 모듈을 채택한다. 그럼에도 불구하고, 이러한 방법들은 종종 VL 정렬을 달성하기 위해 대규모 사전-훈련을 필요로 한다(Bai et al., 2023; Li et al., 2023b).\n' +
      '\n' +
      '이상의 분석을 바탕으로 고해상도 이미지 적응의 주요 어려움은 빠르게 성장하는 시각적 시퀀스에 있다고 결론지었다. 이 문제는 우리가 더 적은 수의 시각적 토큰으로 더 풍부한 시각적 정보를 효율적으로 인코딩하는 방법을 더 탐구하도록 동기를 부여한다.\n' +
      '\n' +
      '##4 혼합 해상도 적응\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      '위의 문제를 해결하기 위해, 우리는 MLLM에 대한 새롭고 효율적인 방법을 제안하는데, 그 구조는 그림 3에 나와 있다. MRA의 핵심 아이디어는 이중 경로 설계를 통해 고해상도 정보를 저해상도 정보에 삽입하는 것이다. 이 경우, MRA는 더 풍부한 시각적 정보를 인코딩하면서 더 적은 수의 시각적 토큰을 유지할 수 있다.\n' +
      '\n' +
      '특히, 두 해상도의 입력 영상(I_{l}\\in\\mathbb{R}^{H_{l}\\times W_{l}\\times 3}\\)과 \\(I_{h}\\in\\mathbb{R}^{H_{h}\\times W_{h}\\times 3}\\)을 고려할 때, MRA의 과정은 다음과 같이 공식화될 수 있다.\n' +
      '\n' +
      '\\mathbf{F}_{v}=\\mathcal{F}_{\\mathcal{I}_{l}(I_{l}, \\mathcal{F}_{\\mathcal{A}(\\mathbf{F}_{vh}))+\\mathbf{F}_{vh},\\\\&\\mathbff{F}_{vh}=\\mathcal{F}_{\\mathcal{I}_{h}(I_{h}}.\\end{split}\\tag{2}\\mathbf{F}_{vh}}\n' +
      '\n' +
      '여기서, \\(\\mathbf{F}_{vh}\\in\\mathbb{R}^{h}\\times w_{h}\\times d_{h}\\) 및 \\(\\mathbf{F}_{v}\\in\\mathbb{R}^{h}\\times w_{h}\\)은 각각 고해상도 특징과 최종 시각적 특징을 나타낸다. 그리고 \\(\\mathcal{F}_{\\mathcal{I}_{l}}\\) 및 \\(\\mathcal{F}_{\\mathcal{I}_{h}}\\)은 각각 고해상도 및 저해상도 이미지에 대한 시각적 인코더이다. \\(\\mathcal{F}_{\\mathcal{I}_{h}}\\) (\\mathcal{F}_{\\mathcal{A}}\\)는 _mixture-of-resolution adapter_(MR-Adapter)를 나타낸다. Eq. 2, MRA는 고해상도 영상과 저해상도 영상을 동시에 처리하기 위해 이중 시각 경로를 채택한다. 그런 다음 새로운 MR-어댑터를 사용하여 느린 경로에서 빠른 경로로 고해상도 정보를 융합한다. 마지막으로, 두 해상도의 시각적 특징은 식 1을 기반으로 LLM에 의해 결합되고 처리된다.\n' +
      '\n' +
      '듀얼 비주얼 패스\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 3, 이중 시각 경로는 MRA의 핵심 설계이며 두 가지 측면에서 이점이 극대화된다.\n' +
      '\n' +
      '**시각적 기능.** 우선, 이중 시각적 경로는 매크로- 및 마이크로-뷰로부터의 이미지를 처리하며, 이는 인간의 시각적 시스템에 의해 영감을 받는다(Mergan and Maunsell, 1993; Robertson and Lamb, 1991). 특히, Robertson and Lamb (1991)은 시각적 시스템이 서로 다른 경로를 통해 로컬 및 글로벌 의미론을 처리한다는 것을 발견했다. 이 발견을 기반으로 우리는 MRA와 유사한 메커니즘을 채택한다. 구체적으로, 하나의 시각적 경로는 고해상도 이미지 _i.e._, 로컬 뷰로부터의 이미지 처리로부터 세밀한 의미론적(fine-grained semantics)을 캡처하는 것을 목표로 한다. 반면에 다른 경로는 저해상도 이미지에서 글로벌 정보를 인코딩하도록 설계되어 더 큰 수용 필드를 달성한다.\n' +
      '\n' +
      '**시각적 정렬.** 상이한 해상도로 인해, 이들 두 경로는 종종 상이한 형상의 시각적 특징을 생성하여 이들의 빠른 정렬을 방해한다(Yu et al., 2019). 이러한 한계를 극복하기 위해 우리는 저해상도 경로와 고해상도 경로에 대해 각각 다른 다운샘플링 속도를 채택한다. 따라서, 이들의 출력 피처들은 동일한 공간 형상을 유지할 수 있다.\n' +
      '\n' +
      '위의 관찰 결과를 바탕으로 컨볼루션 네트워크(CNN)(Liu et al., 2022)와 비전 트랜스포머(ViT)(Dosovitskiy et al., 2020)를 사용하여 이중 시각적 경로를 설계한다. 구체적으로, CNN은 고해상도 이미지를 처리하기 위해 32의 다운샘플링 스트라이드(downampling stride)를 구비한다. ViT는 14의 다운샘플링 보폭을 갖는 저해상도 이미지들을 인코딩한다. 특히, 이러한 설계들은 또한 MLLM들의 효율성을 보장하며, 여기서 고해상도 이미지들은 효율적인 CNN에 의해 프로세싱되고, 시각적 토큰들의 수는 또한 큰 다운샘플링 보폭을 통해 작게 유지된다.\n' +
      '\n' +
      '### Mixture-of-Resolution Adapter\n' +
      '\n' +
      '두 경로의 특징 학습을 더 잘 협력하기 위해, 우리는 서로 다른 해상도 이미지에서 시각적 정보를 융합하기 위한 _mixture-of-resolution adapter_ (MR-Adapter)를 제안한다. 특히, 고해상도 영상으로부터 추출된 시각적 특징 \\(\\textbf{F}_{vh}\\in\\mathbb{R}^{h\\times w\\times d_{h}}\\)을 이용하여 저해상도 영상 경로에 임베딩한다.\n' +
      '\n' +
      '\\[\\textbf{F}^{\\prime}_{vl}=\\textbf{F}_{vl}+f_{l}(\\textbf{F}_{vl})+g\\cdotf_{h}(\\textbf{F}_{vh}). \\tag{3}\\textbf{F}^{\\prime}_{vl}=\\textbf{F}_{vl}+f_{l}(\\textbf{F}_{vl})+g\\cdotf_{h}(\\textbf{F}_{vh})\n' +
      '\n' +
      '여기서 \\(\\textbf{F}_{vl}\\in\\mathbb{R}^{h\\times w\\times d_{l}}\\)는 저해상도 경로의 특징이다. \\\\textbf{F}_{vl}\\in\\mathbb{R}^{h\\times d_{l}}\\ (f_{l}(\\cdot)\\)와 \\(f_{h}(\\cdot)\\)는 각각 컨볼루션 블록과 MLP 레이어로 설계된 두 개의 매핑 모듈을 나타낸다. \\\\ (g\\)는 고해상도 정보의 가중치를 제어하기 위한 동적 스코어로서,\n' +
      '\n' +
      '{split} g&=\\delta(W_{2}\\sigma(W_{1}f_{v})), \\\\f_{v}&=\\frac{1}{h\\times w}\\sum_{i}^{h}\\sum_{j}^{w}[f_{l}(\\textbf{F}_{vl})^{i,j},f_{h}(\\textbf{F}_{vh}}}}^{i,j}].\\end{split}\\tag{4}\\frac{1}{h\\times w}\\sum_{i}\\sum_{j}^{w}[f_{l}(\\textbf{F}_{vl})^{i,j},f_{h}(\\textbf{F}_{vh}}}}}^{i,j}}}\\frac{1}{h\\times w}\\sum_{i}\\sum_{j}^{w\n' +
      '\n' +
      '여기서, \\([\\cdot]\\)은 연결 연산을 나타내며, \\(W_{1}\\in\\mathbb{R}^{2d\\times\\frac{g}{2}\\) 및 \\(W_{2}\\in\\mathbb{R}^{frac{g}{2}\\times d}\\)은 두 개의 투영 행렬이다. \\\\ (f_{v}\\in\\mathbb{R}^{d}\\)는 풀링된 시각적 특징들이다. \\ (\\sigma\\)와 \\(\\delta\\)는 각각 _GELU_와 _Tanh_의 활성화 함수를 나타낸다.\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 3을 참조하면, 고해상도 정보는 ViT의 각 블록 내의 특징들과 융합될 수 있다. 이 경우 ViT의 저해상도 기능도 풍부한 의미론을 포함하고 있어 MLLM의 시각적 설명력을 향상시킨다.\n' +
      '\n' +
      '도 4: **해상도 혼합 어댑터(MR-Adapter)의 도면.** MR-Adapter는 저해상도 경로에 고해상도 기능을 동적으로 내장할 수 있다.\n' +
      '\n' +
      '도 3: **Mixture-of-Resolution Adaptation (MRA)의 도면 및 LLAVA-HR.** MRA에서의 그것의 전개는 고해상도 및 저해상도 이미지를 각각 처리하기 위해 이중 시각적 경로를 채용한다. 고해상도 정보는 새로운 해상도 혼합 어댑터(MR-Adapter)를 통해 빠른 경로에 내장된다.\n' +
      '\n' +
      'MLLM에 관한 연구\n' +
      '\n' +
      '우리는 LLaVA-1.5(Liu et al., 2023a)라고 불리는 인기 있는 MLLM에 MRA를 적용하고, 새로운 모델, 즉 LLaVA-HR을 구성한다. 학습은 _i.e._, 저해상도 사전 학습 및 고해상도 명령어 튜닝의 두 단계로 구성된다.\n' +
      '\n' +
      '**단계 1: Low-Resolution Pre-training.** LLaVA(Liu et al., 2023b) 및 LLaVA-1.5(Liu et al., 2023a)와 유사하게, 이 스테이지는 시각적 특징들을 LLM의 워드 임베딩들과 정렬시키기 위해 프로젝터를 최적화하는 것을 목표로 한다. 따라서, 영상 인코더 및 LLM은 사전 트레이닝 동안 동결된다. 또한 두 경로에 대해 낮은 해상도를 채택합니다. 이 단계에서는 MR-어댑터가 삽입되지 않고 이중 경로의 출력 기능이 직접 결합된다.\n' +
      '\n' +
      '2단계: High-Resolution Instruction Tuning.** 명령어 튜닝 시, 고해상도 경로의 해상도를 384\\(\\times\\)384에서 1,024\\(\\times\\) 1,024로 크게 증가시키고, 저해상도 경로의 해상도를 336\\(\\times\\) 336에서 448\\(\\times\\) 448까지 시각적으로 정렬되도록 조정한 후, MR-Adapter를 적용하여 두 개의 시각적 경로를 연결한다. 첫 번째 훈련 단계와 달리 전체 MLLM은 고해상도 이미지를 더 잘 수용할 수 있도록 완전히 최적화될 것이다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### 평가 및 측정\n' +
      '\n' +
      'MLLM.**에 대한 멀티모달 벤치마크들.** MME(Fu et al., 2023), POPE(Li et al., 2023c), SEED(Li et al., 2023a) 및 MM-VET(Yu et al., 2023)를 포함하는 MLLM에 대한 4개의 신흥 멀티모달 벤치마크들에 대해 LLaVA-HR을 평가한다. 특히, MME와 MM-VET는 MLLM의 멀티모달 인식과 인지 능력을 평가한다. SEED는 평가의 양식을 이미지와 비디오로 확장한다. POPE는 MLLM의 시각 환각을 평가하는 것을 목표로 한다. 본 논문에서 사용된 메트릭은 기본 설정을 따릅니다. MME의 경우 인지 점수를 보고하기 위해 LLaVA-1.5를 따른다.\n' +
      '\n' +
      '**Common vision-language benchmarks.** 또한 VQAv2(Goyal et al., 2017), GQA(Hudson and Manning, 2019), OKVQA(Marino et al., 2019), OCRVQA(Mishra et al., 2019), ScienceQA(Lu et al., 2022), VizWiz(Gurari et al., 2018) 및 TextVQA를 포함한 7개의 VL 데이터셋에 대한 LLaVA-HR을 평가한다. 특히, ScienceQA(Lu et al., 2022), VizWiz(Gurari et al., 2018) 및 TextVQA는 3개의 **zero-shot task**이며, 그들의 샘플은 우리의 트레이닝 데이터에 나타나지 않는다. 우리는 OCRVQA의 _test_ 집합, VizWiz의 _test_ 집합, OKVQA의 _val_ 집합에 대한 정확도를 보고한다. 우리는 이러한 태스크들의 샘플들을 LLaVA-1.5의 명령어 포맷으로 구성한다(Liu et al., 2023a).\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'LLaVA-HR에서는 CLIP-ViT-L (Radford et al., 2021; Ilharco et al., 2021)과 CLIP-ConvNeXt-L (Liu et al., 2022)을 각각 저해상도 영상과 고해상도 영상을 부호화하기 위한 이중 시각 경로로 사용한다. LLaVA-HR-X에서 CLIP-ConvNeXt-L은 더 강한 CLIP-ConvNeXt-XXL로 대체된다. MR-어댑터는 ViT의 마지막 세 단계에 적용된다. LLaVA-1.5에 이어서, 우리는 먼저 558\\(\\&\\) 이미지-텍스트 쌍을 포함하는 LCS-558K(Liu et al., 2023b) 상에서 LLaVA-HR을 사전 트레이닝한다. 사전 훈련 단계 동안, 시각적 인코더와 LLM은 모두 동결되고, MLP 프로젝터만이 미세 조정된다. 최적화기로는 AdamW(Kingma and Ba, 2014)를 사용하였으며, 학습률과 배치 크기는 각각 1e-3과 256으로 설정하였다. 시각적 해상도는 ViT와 CNN의 경우 각각 336\\(\\times\\)336과 384\\(\\times\\)384로 설정하였다. 명령어 튜닝 시 LLaVA-1.5를 따라 665\\(\\&\\)VL 명령어 데이터를 사용한다. 이 단계에서 전체는\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c c c c|c c c} \\hline \\hline \\multirow{2}{*}{Models} & \\multirow{2}{*}{Resolution} & \\multicolumn{4}{c|}{Vision-Language Tasks} & Training & GPU & Inference \\\\  & & VQAv2 \\(\\uparrow\\) & TextVQA \\(\\uparrow\\) & MME \\(\\uparrow\\) & POPE \\(\\uparrow\\) & Time \\(\\downarrow\\) & Memory \\(\\downarrow\\) & Speed \\(\\uparrow\\) \\\\ \\hline LLaVA-1.5 & 336 pix & 80.44 & 59.41 & 1461.17 & 86.2 & 15.6h & 28G & 23.8 tokens/s \\\\ \\hline LLaVA-HR (ours) & 384 pix & 80.47 & 59.63 & 1522.28 & 86.3 & 17.6h & 34G & 23.8 tokens/s \\\\ \\hline LLaVA-1.5 & 448 pix & 81.17 & 62.17 & 1493.12 & 87.2 & 19.4h & 49G & 19.9 tokens/s \\\\ \\hline LLaVA-HR (ours) & 768 pix & 81.80 & 64.36 & 1524.75 & 88.0 & 18.2h & 38G & 23.5 tokens/s \\\\ \\hline LLaVA-1.5 & 672 pix & 81.54 & 64.23 & 1498.71 & 87.9 & 31.8h & 79G & 12.7 tokens/s \\\\ \\hline LLaVA-HR (ours) & 1024 pix & 81.90 & 67.11 & 1554.90 & 87.6 & 20.7h & 40G & 19.7 tokens/s \\\\ \\hline LLaVA-1.5 & 1022 pix & 74.20 & 37.80 & 1266.90 & 84.4 & 69.4h & N/A1  & 5.6 tokens/s \\\\ \\hline LLaVA-HR (ours) & 1536 pix & 81.82 & 67.96 & 1480.62 & 87.7 & 29.8h & 52G & 12.6 tokens/s \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 상이한 해상도에서 LLaVA-HR 및 LLaVA-1.5(Liu et al., 2023a)의 **성능 및 효율성 비교.** 해상도를 제외하고, LLaVA-HR 및 LLaVA-1.5의 다른 구성은 동일하게 유지된다. 훈련 및 추론 비용은 NVIDIA A800s에서 측정된다. "_N/A_"는 GPU 메모리 오버플로우s1을 나타낸다. "_tokens/s_"는 초당 생성된 토큰의 수를 나타낸다.\n' +
      '\n' +
      '모델은 2e-5의 학습율로 갱신되며, ViT와 CNN의 해상도를 각각 448\\(\\times\\)448과 1,024\\(\\times\\)1,024로 증가시킨다. 트레이닝 에포크는 사전 트레이닝 및 명령어 튜닝을 위해 1로 설정된다.\n' +
      '\n' +
      '### Experimental Results\n' +
      '\n' +
      '3.1 정량적 분석\n' +
      '\n' +
      '**기준선과의 비교** 탭에서 * 도 1을 참조하면, LLaVA-HR과 LLaVA-1.5 (Liu et al., 2023)의 서로 다른 영상 해상도와의 성능 및 효율을 비교한다. 이 표에서 이미지 해상도를 높이면 텍스트VQA에서 LLaVA-1.5의 +4.8%인 4가지 작업에서 두 모델의 성능이 분명히 향상됨을 알 수 있다. 그러나 LLaVA-1.5의 성능은 1,024\\(\\times\\)1,024의 해상도에서 크게 떨어지며, 이를 설명하기 위해 시각적 토큰의 수가 미리 훈련된 LLM의 컨텍스트 길이를 크게 초과함으로써 훈련 중 불안정성을 쉽게 유발한다. 반면, LLaVA-HR의 성능은 384\\(\\times\\)384 해상도에서 1,024\\(\\times\\)1,024 해상도로 지속적으로 개선되었다. 또한, LLaVA-HR의 총 이득은 LLaVA-1.5(Liu et al., 2023), _e.g.,_+8.33%의 LLaVA-HR _vs._+ LLaVA-1.5의 4.82%로 MRA의 효과를 크게 확인할 수 있다.\n' +
      '\n' +
      '탭에서 또한, 유사한 해상도를 갖는 4개의 공통 기준선인 _i.e._\\(\\sim\\)760\\(\\times\\)760을 비교한다. "ViT+MLP"는 LLaVA-1.5의 기본 설정이다. "Conv+MLP"는 시각적 백본을 ConvNeXt(Liu et al., 2022)로 대체하며, 이는 시각적 토큰의 수를 감소시키기 위해 더 큰 다운샘플링 레이트를 사용한다. "ViT+Resampler"와 "ViT+Pooling+MLP"는 시각적 토큰의 수를 줄이기 위한 두 가지 풀링 전략을 의미한다. 알 수 있는 바와 같이, 비교된 모든 방법은 LLaVA-HR에 비해 열등하다. 특히, 시각적 백본으로서 컨볼루션 네트워크를 사용하는 것은 효율성을 크게 향상시키지만, 그 성능은 여전히 MME 상에서 큰 마진, _예를 들어,_-108.9만큼 LLaVA-HR에 뒤처진다(Fu et al., 2023). 마찬가지로 "ViT+Resampler"와 "ViT+Pooling+MLP"도 효율성을 위해 성능을 희생한다. 전반적으로 이러한 비교는 MRA의 설계를 추가로 확인한다.\n' +
      '\n' +
      '효과에도 불구하고 LLaVA-HR의 지출은 비용 효율적이다. 특히 해상도가 384\\(\\times\\)384에서 1,024\\(\\times\\)1,024로 증가하면 LLaVA-1.5의 훈련과 추론이 각각 344.8%와 325% 느려진다. 그러나 이러한 비용은 LLaVA-HR에서 17.6% 및 20.8%로 감소한다. 성능 향상에도 불구하고, LLaVA-HR의 학습 및 추론 속도는 LLaVA-1.5보다 3배 빠르며, GPU 메모리 비용도 LLaVA-HR에 비해 저렴하다. 예를 들어, LLaVA-HR에 대해 1,536\\(\\times\\)1,536의 해상도를 적응시키는 것은 52G GPU 메모리만을 소비하지만, LLaVA-1.5에 대한 동일한 설정은 GPU 메모리 오버플로를 야기할 것이다. 이러한 결과는 MRA 및 LLaVA-HR의 효율성을 크게 확인한다.\n' +
      '\n' +
      '\'절제 연구\' \'탭에서\' 3, 우리는 4개의 VL 벤치마크에서 MRA에 대한 포괄적인 절제 연구를 수행한다. 먼저, 이중 시각 경로의 다양한 디자인을 검증한다. 이러한 결과로 볼 때, 하나의 경로를 제거하면 VQAv2에서 _e.g.,_-1.5%의 성능 저하를 가져올 수 있으며, 또한 고해상도 인코더를 확장하면 저해상도 인코더보다 더 많은 이득을 얻을 수 있다. TextVQA 0.9% 우리는 더 강한 고해상도 이미지 인코더가 세밀한 시각적 정보를 더 잘 캡처할 수 있다고 가정한다. 그런 다음 MRA에서 다양한 융합 방향과 전략을 절제한다. 구체적으로, 융합 방향을 변경하는 것은 분명히 성능을 퇴보시킨다, 예를 들어,_\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\hline \\hline Settings & VQAv2 & TextVQA & MME & POPE & Speed \\\\ \\hline ViT+ MLP & 81.0 & 63.2 & 1436.1 & 87.6 & 10.7 t/s \\\\ Conv+MLP & 80.3 & 64.6 & 1415.9 & 86.6 & 23.7 t/s \\\\ ViT+Resampler & 79.8 & 58.9 & 1403.8 & 85.8 & 27.6 t/s \\\\ ViT+Pooling+MLP & 80.6 & 59.6 & 1480.6 & 86.5 & 23.9 t/s \\\\ MRA (ours) & 81.8 & 64.4 & 1524.8 & 88.0 & 23.5 t/s \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: ** LLaVA-HR에 대한 MRA 및 4개의 기준선의 비교. 시각적 해상도는 \\(\\sim\\)760\\(\\times\\)760.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c} \\hline \\hline Settings & Choices & VQAv2 & TextVQA & MME & POPE \\\\ \\hline \\multirow{3}{*}{L-Res} & ViT-L & 81.8 & 64.4 & 1524.8 & 88.0 \\\\ \\cline{2-6}  & None & 80.3 & 64.6 & 1415.9 & 86.6 \\\\ \\cline{2-6}  & ViT-G & 81.7 & 65.3 & 1469.7 & 87.9 \\\\ \\hline \\multirow{3}{*}{H-Res} & ConvNeXt-L & 81.8 & 64.4 & 1524.8 & 88.0 \\\\ \\cline{2-6}  & None & 80.4 & 59.4 & 1461.2 & 86.2 \\\\ \\cline{2-6}  & ConvNeXt-XXL & 82.3 & 66.5 & 1479.2 & 87.9 \\\\ \\hline \\multirow{3}{*}{Fusion} & High to Low & 81.8 & 64.4 & 1524.8 & 88.0 \\\\ \\cline{2-6}  & Low to High & 81.0 & 62.8 & 1463.5 & 87.3 \\\\ \\hline \\multirow{3}{*}{Fusion} & Sum & 81.8 & 64.4 & 1524.8 & 88.0 \\\\ \\cline{2-6}  & Concat & 81.7 & 64.7 & 1508.8 & 87.3 \\\\ \\hline \\multirow{3}{*}{Struct.} & mlp-conv & 81.8 & 64.4 & 1524.8 & 88.0 \\\\ \\cline{2-6}  & conv-conv & 81.6 & 64.6 & 1499.0 & 87.7 \\\\ \\cline{1-1}  & conv-mlp & 81.5 & 64.2 & 1517.9 & 87.6 \\\\ \\hline \\multirow{3}{*}{Gate} & Tanh & 81.8 & 64.4 & 1524.8 & 88.0 \\\\ \\cline{1-1}  & Sigmoid & 81.7 & 64.3 & 1567.9 & 86.9 \\\\ \\cline{1-1}  & H-sigmoid & 81.6 & 64.4 & 1525.9 & 87.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: LLaVA-HR에 대한 분해능 혼합물 적응의 **절제 연구. 해상도는 768\\(\\times\\)768이며 최종 설정은 회색으로 표시됩니다. L-Res Path.”, “H-Res Path.”, “Fusion Direct.”, “Struct.” 및 “Gate Fuct.”는 MME 상의 저해상도 경로, 고해상도 경로, 융합 방향, 구조 및 게이트 함수를 각각 나타낸다.**-61.3. 마지막으로 해상도 혼합 어댑터의 디자인을 삭제합니다. 특히, 저해상도 및 고해상도 경로에 대한 매핑 모듈의 최상의 선택은 각각 컨볼루션 블록 및 MLP 블록이다. 또한 게이팅 함수의 선택도 성능에 영향을 미치며 _tanh_ 함수가 가장 좋은 성능을 보인다. 이러한 절제는 MR-어댑터의 디자인을 추가로 확인한다.\n' +
      '\n' +
      '**탭에서 기존 MLLMs.**와 비교. 4 - 5, 우리는 11개의 VL 태스크에서 LLaVA-HR을 기존의 MLLM과 비교한다. 4가지 MLLM 벤치마크에서 기존 MLLM에 대한 LLaVA-HR의 포괄적인 이점을 관찰한다. 특히, LLaVA-HR은 MME 벤치마크에서 1554.9점을 달성하여 LLaVA-1.5를 +23.6으로 능가한다. 비디오 평가를 포함하는 벤치마크인 POPE에서는 여전히 LLaVA-HR-X가 기존 MLLM보다 큰 마진(_i.e.,_ +3.7% 이득)으로 능가한다. 또한, LLaVA-HR은 시각 환각의 벤치마크인 _즉,_ POPE에서 최고의 성능을 달성하여 시각 환각이 크게 완화되었음을 시사한다. 특히, Fuyu-8b(Fuyu-8B, 2023)는 고해상도 영상이 가능하지만, LLaVA-HR에 비해 성능이 크게 떨어지며, 예를 들어,_728.6 _vs._728.6 _vs._ 1554.9 on MME.\n' +
      '\n' +
      '탭 도 5는 공통 VL 태스크에 대한 성능 비교를 제공한다. 도메인 내 태스크에서 LLaVA-HR은 VQAv2에서 82.6, OKVQA에서 61.5로 세 가지 태스크에서 가장 좋은 결과를 얻었다. OCRVQA에서 Qwen-VL-Chat은 학습을 위해 더 많은 도메인 내 데이터를 수집하므로 LLaVA-HR보다 더 나은 성능을 보인다. 0-shot 환경에서, 우리는 LLaVA-HR이 세밀한 작업, 예를 들어, VizWiz 및 TextVQA에서 더 중요한 이점을 관찰할 수 있다. 가장 주목할 만한 것은 Qwen-VL-Chat도 24.8M OCR 샘플로 사전 훈련되었지만 여전히 TextVQA에서 LLaVA-HR-X보다 성능이 좋지 않다는 것이다. 이러한 결과는 이러한 작업에 대한 고해상도의 중요성을 시사한다. In\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l|l l l l l|c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c|}{Settings} & \\multicolumn{3}{c|}{Multimodal Benchmarks} & \\multicolumn{1}{c}{Inference} \\\\  & Param. & Res. & Data & MME & POPE & SEED & MM-Vet & Speed \\\\ \\hline BLIP-2 & 14.2B & 224 & 129M & 1293.8 & 85.3 & 46.4 & 22.4 & - \\\\ InstructBLIP & 8.2B & 224 & 130M & - & - & 53.4 & 26.2 & 22.6 t/s \\\\ InstructBLIP & 14.2B & 224 & 130M & 1212.8 & 78.9 & - & 25.6 & - \\\\ QWen-VL-Chat & 9.6B & 448 & 1.4B & 1487.5 & - & 58.2 & - & 17.0 t/s \\\\ Fuyu-8B & 8B & \\(\\sim\\)600 & - & 728.6 & 74.1 & - & 21.4 & 15.6 t/s \\\\ mPLUG-Owl2 & 8.2B & 448 & 400M & 1450.2 & - & 57.8 & **36.2** & 19.6 t/s \\\\ LLaVA-1.5 & 7.2B & 336 & 1.2M & 1510.7 & 85.9 & 58.6 & 30.5 & 23.8 t/s \\\\ LLaVA-1.5 & 13.2B & 336 & 1.2M & 1531.3 & 85.9 & 61.6 & 35.4 & - \\\\ \\hline \\hline LLaVA-HR & 7.4B & 1024 & 1.2M & **1554.9** & 87.6 & 64.2 & 31.2 & 19.7 t/s \\\\ LLaVA-HR & 13.4B & 1024 & 1.2M & 1540.9 & 87.8 & 64.5 & 34.8 & 15.0 t/s \\\\ LLaVA-HR-X & 14B & 1024 & 1.2M & 1487.3 & **88.0** & **65.3** & 35.5 & 12.9 t/s \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **4개의 MLLM 벤치마크에 대한 기존 방법과의 비교.** "Param.", "Res." 및 "Data"는 각각 총 파라미터, 시각적 해상도 및 훈련 데이터 수를 나타낸다. "t/s"는 초당 토큰을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l|l l l l|l l|l} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c|}{Settings} & \\multicolumn{3}{c|}{In-domain Tasks} & \\multicolumn{3}{c}{Zero-shot Tasks} & \\multicolumn{1}{c}{Infer.} \\\\  & Param. & Res. & Data & VQAv2 & GQA & OKVQA & OCRVQA & SQA\\({}^{I}\\) & VizWiz & TextVQA & Speed \\\\ \\hline BLIP-2 & 14.2B & 224 & 129M & 41.0 & 41.0 & 45.9 & 40.6 & 61.0 & 19.6 & 42.5 & - \\\\ InstructBLIP & 8.2B & 224 & 130M & - & 49.2 & - & - & 60.5 & 34.5 & 50.1 & 22.6 t/s \\\\ InstructBLIP & 14.2B & 224 & 130M & - & 49.5 & - & 44.8 & 63.1 & 33.4 & 50.7 & - \\\\ Shikra & 13.2B & 224 & 6.1M & 77.4 & - & - & - & - & - & - \\\\ IDEFICS-9B & 9B & 224 & 354M & 50.9 & - & 38.4 & - & - & 35.5 & 25.9 & 30.5 t/s \\\\ IDEFICS-80B & 80B & 224 & 354M & 60.0 & - & 45.2 & - & - & 36.0 & 30.9 & - \\\\ Qwen-VL-Chat & 9.6B & 448 & 1.4B & 78.2 & 57.5 & 56.6 & **70.5** & 68.2 & 38.9 & 61.5 & 17.0 t/s \\\\ Fuyu-8B & 8B & \\(\\sim\\)600 & - & 74.2 & - & 60.6 & - & - & - & - & 15.6 t/s \\\\ mPLUG-Owl2 & 8.2B & 448 & 400M & 79.4 & 56.1 & 57.7 & - & 68.7 & 54.5 & 58.2 & 19.6 t/s \\\\ LLaVA-1.5 & 7.2B & 336 & 1.2M & 78.5 & 62.0 & - & - & 66.8 & 50.0 & 58.2 & 23.8 t/s \\\\ LLaVA-1.5 & 13.2B & 336 & 1.2M & 80.0 & 63.3 & - & - & **71.6** & 53.6 & 61.3 & - \\\\ \\hline LLaVA-HR & 7.4B & 1024 & 1.2M & 81.9 & 64.2 & 58.9 & 68.4 & 65.1 & 48.7 & 67.1 & 19.7 t/s \\\\ LLaVA-HR & 13.4B & 1024 & 1.2M & 82.3 & 64.8 & 60.7 & 67.7 & 68.1 & **57.9** & 68.1 & 15.0 t/s \\\\ LLaVA-HR-X & 14B & 1024 & 1.2M & **82.6** & **65.2** & **61.5** & 69.0 & 68.0 & 56.6 & **70.9** & 12.9 t/s \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: **7개의 비전 언어 태스크에 대한 기존 방법과의 비교.** SQA\\({}^{I}\\)는 ScienceQA의 _IMG_ 서브세트를 나타낸다.\n' +
      '\n' +
      '대조, 사이언스QA의 대부분의 이미지는 합성적이고 해상도가 낮기 때문에 LLaVA-HR의 이점은 분명하지 않다. 전반적으로, 이러한 결과는 LLaVA-HR 및 우리의 MRA의 효과와 일반화를 크게 확인한다.\n' +
      '\n' +
      '###### 5.3.2 질적 실험\n' +
      '\n' +
      '그림 5(a)에서 우리는 다른 해상도로 LLaVA-HR의 예측을 비교한다. 시각화는 높은 이미지 해상도가 미세한 작업에서 MLLM의 능력을 분명히 향상시킨다는 것을 보여준다. 예를 들어, 해상도가 1,024\\(\\times\\)1,024인 LLaVA-HR은 첫 번째 예에서 작은 보트와 같은 입상 시각적 콘텐츠를 잘 캡처할 수 있다. 게다가, 높은 이미지 해상도는 또한 LLaVA-HR이 더 강한 텍스트 인식 능력을 가능하게 한다. 예를 들어, 제2 예에서 "_wo ich wohne_"의 작고 흐릿한 어구는 고해상도 LLaVA-HR에 의해 정확하게 식별된다. 이러한 결과는 시각적 단점을 해결하는 데 있어 높은 이미지 해상도의 중요성을 크게 확인시켜준다. 도 5(b)에서, 우리는 시각적 정보 추출에서 LLaVA-HR-X, LLaVA-1.5(Liu et al., 2023a) 및 GPT4-V(OpenAI, 2023)의 예측을 추가로 비교한다. 특히, LLaVA-HR-X는 이 도전적 과제에서 GPT4-V와 유사한 능력을 보여준다. 도 5(b)에 도시된 바와 같이, LLaVA-HR-X와 GPT4-V는 운전면허의 거의 모든 시각적 내용을 정확하게 추출하여 JSON 형식으로 정리할 수 있다. GPT4-V와 비교하여 LLaVA-HR-X는 또한 사람의 모발 색상을 정확하게 식별하므로 세밀한 시각적 추론이 필요하다. 대조적으로, LLaVA-1.5는 "_class_" 및 "_SEX_"와 같은 단순한 시각적 콘텐츠만을 인식할 수 있고, 대부분의 시각적 정보를 추출하지 못한다. 이러한 결과는 MLLM의 시각적 단점을 해결하는 데 MRA의 효과를 추가로 검증한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 이미지 해상도의 관점에서 MLLM의 시각적 단점을 연구하고, MLLM의 고해상도 적응을 위한 새롭고 효율적인 방법인 _mixture-of-resolution adaptation_ (MRA) 방법을 제안한다. MRA는 고해상도 및 저해상도의 이미지를 처리하기 위해 이중 시각적 경로를 채택하며, 여기서 고해상도 정보는 새로운 _mixture-of-resolution adapters_(MR-Adapters)를 통해 저해상도 모델링에 내장된다. 우리\n' +
      '\n' +
      '그림 5: **LLaVA-HR 및 기존 MLLMs.** Subfig-(a)의 시각화는 높은 이미지 해상도가 미세한 VL 작업에서 MLLMs의 능력을 크게 향상시킨다는 것을 보여준다. Subfig-(b)에서 LLaVA-HR-X는 시각 정보 추출 2에서 GPT4-V와 유사한 능력을 보여주며, 정답과 오답은 각각 녹색과 적색으로 채색된다.\n' +
      '\n' +
      'LLaVA-1.5라는 인기 있는 MLLM에 MRA를 적용하고 LLaVA-HR이라고 하는 새로운 고해상도 MLLM을 구성한다. 실험 결과는 시각적 단점을 해결하기 위한 LLaVA-HR의 유효성을 검증할 뿐만 아니라 기존 MLLM에 대한 놀라운 효율성을 확인한다.\n' +
      '\n' +
      '인정.이 작업은 중국 국립자연과학재단(No.62025603)인 국가유공청년과학기금(No.2022ZD0118201)의 국가핵심연구개발사업(No.2022ZD0118201)의 지원을 받았다. U21B2037호, U22B2051호, No.62176222호, No.62176223호, No.62176223호, No.62176226호, No.6272386호, No.62072387호, No.62072389호, No.62002305호 및 No.62272401호), 중국 푸젠성 자연과학재단(No.2021J01002호, No.2022J06001호), 및 중국 중앙대학 기초연구기금(Grant No.20720220068호)을 포함할 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: few-shot learning을 위한 시각적 언어 모델. _ arXiv preprint arXiv:2204.14198_, 2022.\n' +
      '* Bai et al. (2023) Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A frontier large vision-language model with versatile abilities. _ arXiv preprint arXiv:2308.12966_, 2023.\n' +
      '* Chen et al. (2023a) Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R. Shikra: Unleashing multimodal llm\'s referential dialogue magic. _ arXiv preprint arXiv:2306.15195_, 2023a.\n' +
      '* Chen et al. (2020) Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. E. Big self-supervised models is strong semi-supervised learners. _ neural information processing systems (NeurIPS)_, 33:22243-22255, 2020에서의 진전이 있다.\n' +
      '* Chen et al. (2022) Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., et al. Pali: A jointly-scaled multilingual language-image model. _ ArXiv:2209.06794_, 2022.\n' +
      '* Chen et al. (2023b) Chen, X., Wang, X., Beyer, L., Kolesnikov, A., Wu, J., Voigtlaender, P., Mustafa, B., Goodman, S., Alabdulmohsin, I., Padlewski, P., et al. Pali-3 비전 언어 모델: Smaller, faster, stronger; _ arXiv preprint arXiv:2310.09199_, 2023b.\n' +
      '* Dai et al. (2023) Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. 인스트럭션 블립: 명령어 튜닝이 있는 범용 비전 언어 모델에 대해. _ arXiv preprint arXiv:2305.06500_, 2023.\n' +
      '* Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. _ arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* Fu et al. (2023) Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu, Z., Lin, W., Yang, J., Zheng, X., et al. Mme: 멀티모달 대형 언어 모델들에 대한 종합 평가 벤치마크. _ arXiv preprint arXiv:2306.13394_, 2023.\n' +
      '* Fuyu-8B(2023) Fuyu-8B. [https://www.adept.ai/blog/fuyu-8b] (https://www.adept.ai/blog/fuyu-8b), 2023.\n' +
      '* Gilardi et al. (2023) Gilardi, F., Alizadeh, M., and Kubli, M. 채팅은 텍스트 주석 작업에 대해 크라우드 워커를 능가합니다. _ arXiv preprint arXiv:2303.15056_, 2023.\n' +
      '* Goyal et al. (2017) Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 6904-6913, 2017.\n' +
      '* Gurari et al. (2018) Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp.3608-3617, 2018.\n' +
      '* Hudson & Manning (2019) Hudson, D. A. and Manning, C. D. Gqa: real-world visual reasoning and compositional question answering을 위한 새로운 데이터셋. 2019년 _CVPR_에서.\n' +
      '* Ilharco et al. (2021) Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave, A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., and Schmidt, L. 오펜클립 2021년 7월. doi: 10.5281/zenodo.5143773. URL[https://doi.org/10.5281/zenodo.5143773](https://doi.org/10.5281/zenodo.5143773). 이 소프트웨어를 사용하신다면 아래와 같이 인용해 주시기 바랍니다.\n' +
      '* Jiang et al. (2020) Jiang, H., Misra, I., Rohrbach, M., Learned-Miller, E., and Chen, X. 시각적 질의 응답을 위한 그리드 피쳐를 방어합니다. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10267-10276, 2020.\n' +
      '* Kingma & Ba(2014) Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. _ arXiv preprint arXiv:1412.6980_, 2014.\n' +
      '* Li 등(2023a) Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y. 종자 벤치: 생성적 이해도를 가진 다중 모드 llms를 벤치마킹 arXiv preprint arXiv:2307.16125_, 2023a.\n' +
      '\n' +
      '* Li 등(2023b) Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: 냉동 이미지 인코더 및 대형 언어 모델을 사용한 부트스트래핑 언어-이미지 사전 트레이닝_ arXiv preprint arXiv:2301.12597_, 2023b.\n' +
      '*Li 등(2023c) Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen, J.-R. 대용량 시각 언어 모델에서 객체 환각을 평가하는 방법. _ arXiv preprint arXiv:2305.10355_, 2023c.\n' +
      '* Liu et al. (2023a) Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning. _ arXiv preprint arXiv:2310.03744_, 2023a.\n' +
      '*Liu et al. (2023b) Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. _NeurIPS_, 2023b.\n' +
      '*Liu et al. (2023c) Liu, S., Cheng, H., Liu, H., Zhang, H., Li, F., Ren, T., Zou, X., Yang, J., Su, H., Zhu, J., Zhang, L., Gao, J., and Li, C. Llava-plus: Learning to use tools for creating multimodal agents, 2023c.\n' +
      '* Liu et al. (2022) Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. 2020년대를 위한 경마장 In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 11976-11986, 2022.\n' +
      '* Lu et al. (2019) Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: 비전 및 언어 작업에 대한 사전 훈련 작업-진단 시각언어 표현. _ ArXiv preprint arXiv:1908.02265_, 2019.\n' +
      '* Lu et al. (2022) Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K. - W., Zhu, S. - C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chain for science question answering. _ 신경 정보 처리 시스템_, 2022의 발전.\n' +
      '* Luo et al. (2023a) Luo, G., Zhou, Y., Ren, T., Chen, S., Sun, X., and Ji, R. 싸고 빠른: 대용량 언어 모델을 위한 효율적인 비전 언어 명령어 튜닝. _ 신경 정보 처리 시스템(NeurIPS)_, 2023a에서의 발전.\n' +
      '* Luo et al. (2023b) Luo, G., Zhou, Y., Sun, J., Sun, X., and Ji, R. 대규모 사전 훈련 시대의 생존자: 표현 이해도를 참조하는 1단계 경험적 연구 IEEE Transactions on Multimedia_, 2023b.\n' +
      '* Marino et al. (2019) Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Okvqa: 외부 지식이 필요한 시각적 질문 답변 벤치마크. _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* Merigan & Maunsell (1993) Merigan, W. H. and Maunsell, J. H. How parallel of the primate visual pathways? _ Nuroscience_, 16(1):369-402, 1993년에 대한 연간 검토.\n' +
      '* Mishra et al. (2019) Mishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A. Ocr-vqa: Visual question answering by reading text in images. In _2019 International conference on document analysis and recognition (ICDAR)_, pp. 947-952. IEEE, 2019.\n' +
      '* OpenAI(2023) OpenAI. Gpt-4v(tsion) 시스템 카드. [https://cdn.openai.com/papers/GPTV_System_Card.pdf] (https://cdn.openai.com/papers/GPTV_System_Card.pdf), 2023.\n' +
      '* Peng et al. (2023) Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., and Wei, F. Kosmos-2: Grounding multimodal large language models to the world. _ arXiv preprint arXiv:2306.14824_, 2023.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. _ arXiv preprint arXiv:2103.00020_, 2021.\n' +
      '* Ren et al. (2024) Ren, T., Liu, S., Zeng, A., Lin, J., Li, K., Cao, H., Chen, J., Huang, X., Chen, Y., Yan, F., Zeng, Z., Zhang, H., Li, F., Yang, J., Li, H., Jiang, Q., and Zhang, L. 접지된 샘: 다양한 시각적 작업을 위한 오픈 월드 모델 조립, 2024.\n' +
      '* Robertson & Lamb (1991) Robertson, L. C. and Lamb, M. R. Neuropsychological contributions to theories of part/whole organization. _ Cognitive psychology_, 23(2):299-330, 1991.\n' +
      '* Rose et al. (2023) Rose, D., Himakunthala, V., Ouyang, A., He, R., Mei, A., Lu, Y., Saxon, M., Sonar, C., Mirza, D., and Wang, W. Y. Visual chain of thought: Bridging logical gap with multimodal infillings. _ arXiv preprint arXiv:2305.02317_, 2023.\n' +
      '* Singh et al. (2019) Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. 읽을 수 있는 vqa 모델을 향합니다. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 8317-8326, 2019.\n' +
      '* Tong et al. (2024) Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., and Xie, S. 눈을 크게 감고? 멀티모달 llms의 시각적 단점을 탐구합니다. _ arXiv preprint arXiv:2401.06209_, 2024.\n' +
      '* Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. - A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* Yu et al. (2019) Yu, J., Li, J., Yu, Z., and Huang, Q. 이미지 캡셔닝을 위한 멀티-뷰 시각적 표현을 갖는 멀티모달 트랜스포머. _ IEEE transactions on circuit and systems for video technology_, 30(12):4467-4480, 2019.\n' +
      '\n' +
      '* Yu et al. (2023) Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. 음-벳: 통합 기능을 위한 대형 멀티모달 모델 평가 arXiv preprint arXiv:2308.02490_, 2023.\n' +
      '* Zhang et al. (2021) Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., and Gao, J. Vinvl: Revisiting visual representations in vision-language models. 2021년 _CVPR_에서.\n' +
      '* Zhu et al. (2023) Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: 고급 대형 언어 모델로 비전 언어 이해력 향상. _ arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>