<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# VisionLLaMA: 비전 태스크를 위한 통합된 LLaMA 인터페이스\n' +
      '\n' +
      '상서추\\({}^{1}\\), 지안린수\\({}^{3}\\), 보장\\({}^{1}\\), 춘화선\\({}^{2}\\)\n' +
      '\n' +
      '({}^{1}\\) Meituan Inc. \\({}^{1}\\) Meituan Inc. \\({}^{1}\\) ({}^{2}\\) 저장대학교, 중국 \\({}^{3}\\) 문샷 AI, 중국\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대용량 언어 모델은 텍스트 입력을 처리하기 위해 변압기 기반 아키텍처 위에 구축된다. 예를 들어, 모델의 LLaMA 패밀리는 많은 오픈 소스 구현 중에서 두드러진다. 동일한 변압기를 사용하여 2D 이미지를 처리할 수 있습니까? 본 논문에서는 이러한 목적을 위해 맞춤화된 **VisionLLaMA**라고 하는 평평하고 피라미드 형태의 LLaMA 유사 비전 트랜스포머를 공개함으로써 이 질문에 답한다. VisionLLaMA는 대부분의 비전 과제를 해결하기 위한 통합적이고 일반적인 모델링 프레임워크이다. 영상 인식 및 특히 영상 생성의 다운스트림 작업의 상당 부분에서 전형적인 사전 훈련 패러다임을 사용하여 그 효과를 광범위하게 평가한다. 많은 경우에 VisionLLaMA는 이전의 최첨단 비전 트랜스포머에 비해 상당한 이득을 보였다. 우리는 VisionLLaMA가 시력 생성과 이해를 위한 강력한 새로운 기준 모델 역할을 할 수 있다고 믿는다. 우리의 코드는 [https://github.com/Mei_tuan-AutoML/VisionLLaMA](https://github.com/Mei_tuan-AutoML/VisionLLaMA)에서 공개될 것이다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델은 연구 커뮤니티에 큰 관심을 불러일으켰다. 가장 영향력 있고 대표적인 작품 중 하나는 LLaMA[66, 67]이다. 최근 많은 연구가 이 아키텍처로 수렴되었으며 다양한 응용 프로그램을 위한 솔루션은 오픈 소스 모델에 기반을 두고 있다. 또한, 다중 모드 모델의 블루밍을 목격했는데, 많은 방법이 텍스트 처리를 위해 LLaMA에 크게 의존하고 시 인식을 위해 CLIP-fashioned[51] 비전 트랜스포머[22]에 크게 의존한다. 한편, 많은 노력들[23, 38, 73]은 LLaMA의 추론 속도 및/또는 메모리 비용을 가속화하기 위해 노력해 왔다. 한마디로, LLaMA는 이제 _de facto_ 아키텍처이다.\n' +
      '\n' +
      '그것의 성공을 관찰하면서, 간단하고 흥미로운 질문은 LLAMA 아키텍처가 비전 양식에서 또 다른 승리가 될 수 있는지 여부이다. 정답이 긍정적이면, 비전과 언어 모델 모두 동일한 통합된 아키텍처를 사용하고 즉석에서 LLaMA를 위해 설계된 다양한 배치 기술을 즐길 수 있다. 불행히도 이 두 양식 사이에는 몇 가지 뚜렷한 차이가 있기 때문에 이 질문에 답하는 것은 사소한 일이 아니다. 첫째, 텍스트 시퀀스는 하나의 차원으로 구성되는 반면, 시각은 둘 이상을 필요로 한다는 것은 상식이다. 두 번째로, 많은 비전 태스크들은 더 나은 수행을 위해 피라미드 백본들에 의존하는 반면, LLaMA는 평범한 인코더이다. 셋째, 다양한 해상도로 입력된 영상과 동영상을 처리할 필요가 있다. 본 논문은 이러한 어려움을 해결하고 서로 다른 양식 간의 구조적 격차를 해소하는 것을 목표로 한다. 우리의 주요 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '1. 언어와 시각의 구조적 차이를 줄이기 위해 LLaMA와 유사한 비전 트랜스포머 아키텍처인 VisionLLaMA를 제안한다.\n' +
      '2. 영상 이해 및 생성을 포함한 일반적인 비전 과제를 해결하기 위해 VisionLLaMA를 적용하기 위한 수단을 조사한다(그림 1). 우리는 잘 알려진 두 가지 비전 아키텍처 체계(플레인과 피라미드)를 검토하고 감독 및 자체 감독 학습 시나리오에서 성능을 평가한다. 또한, 회전된 위치 부호화를 1D에서 2D로 확장하고 임의의 해상도를 수용하기 위해 보간 스케일링을 이용하는 AS2DRoPE(auto-scaled 2D RoPE)를 소개한다.\n' +
      '3. VisionLLaMA는 벨과 휘파람 없이 영상 생성, 분류, 의미론적 분할, 객체 검출과 같은 많은 대표적인 작업에 걸쳐 명확한 여백에 의해 광범위하고 주의 깊게 미세 조정된 비전 트랜스포머를 크게 능가한다. 광범위한 실험을 통해 VisionLLaMA가 기존 비전 트랜스포머보다 빠른 수렴 속도와 우수한 성능을 나타냄을 알 수 있다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**비전 트랜스포머.** ViT[22]는 자연어 처리에서 트랜스포머[68]를 비전 세계에 성공적으로 적용하였으며 DeiT[65], Swin[43], PVT[70], Twins[12]와 같이 보다 효율적이고 강력한 후속 작업이 많이 유도된다. 사전 훈련 패러다임은 ImageNet[19]과 같은 대규모 범주형 레이블 데이터 세트에 대한 지도 학습에서 비지도 학습[25]으로, CLIP[51]에서와 같이 방대한 양의 이미지-텍스트 쌍에 대한 대조 학습으로 전환되었다. DiT[50]은 확산 모델[28, 60]의 잠재 패치에서 작동하는 트랜스포머를 채택하여 일반적으로 사용되는 U-Net 백본[54]을 능가한다.\n' +
      '\n' +
      '**대형 언어/멀티모달 모델** GPT4[48]와 같은 독점 모델이 LLM 경쟁에서 주도권을 잡고 있지만 기술적 세부 사항은 대중에게 숨겨져 있다. 대조적으로, 커뮤니티는 무수한 오픈 소스 대응물을 방출하기 위해 꽃을 피웠다. 예를 들어, BLOOM[57] 및 LLaMA[66]은 폐쇄 모델 GPT-3[6]의 성능을 따라잡는다. LLaMA-2 [67]은 RMSNorm [80], 활성화 함수 SwiGLU [59], 회전 위치 임베딩 RoPE [62], 전용 훈련 파이프라인을 포함하는 아키텍처 트윗 팩에 대해 자세히 설명하며, 이는 RLF(Reinforcement Learning with Human Feedback)에 의해 강화된 자체 감독 사전 훈련 및 감독 미세 조정을 포함한다. 많은 비전 언어 모델[36, 40, 72, 83, 41]이 LLaMA에 구축되어 있으며 시각적 대화, 추론, 인식 등에서 인상적인 결과를 보여준다. LLaMA 아키텍처는 최근 휴대폰 [10, 11]과 같은 자원 제한 멀티모달 시나리오에서도 적용되었으며 잠재적인 응용 프로그램을 보여준다.\n' +
      '\n' +
      '**확산 모델.**확산 모델은 디노이징 확산 확률 모델(DDPMs) [28, 60], 스코어 기반 생성 모델(SGMs) [61, 32] 및 분류기 없는 확산 안내[29]로 대표되는, 이전의 방법론 GAN [24]를 능가하는 이미지 생성을 위한 새로운 사실상의 패러다임이다. 확산 모델의 메커니즘은 데이터에 점차적으로 노이즈를 추가한 다음 노이즈 제거를 학습하는 아이디어에 기초한다. 계산적으로 비싼 트레이닝 및 샘플링 프로세스, 트레이닝을 위한 많은 양의 데이터의 필요성 및 생성 프로세스를 제어하는 어려움에 대한 과제가 남아 있다. 최근 OpenAI는 트랜스포머 기반 텍스트 조건 확산 모델(소라(Sora)[5]로 불리는 가장 큰 모델)을 사용하여 다양한 지속 시간, 해상도 및 종횡비의 비디오와 이미지에 대해 공동으로 훈련하여 실제 장면을 시뮬레이션하는 고충실도 비디오를 제공한다. 최근 및 동시 작업[45]은 유연한 목표 해상도로 이미지 생성을 처리하는 방법을 탐구한다. [45]와 비교하여, 우리의 목표는 다양한 비전 작업을 위한 범용 비전 트랜스포머를 구축하는 것이다.\n' +
      '\n' +
      '**트랜스포머를 위한 위치 인코딩.**트랜스포머 [68]은 원래 정현파 형태의 2D 절대 위치 임베딩과 함께 제공된다. 대조적으로, [58]에서와 같은 상대적인 것들은 입력 토큰들의 관계에 주목하고 가변 길이의 시퀀스들을 처리할 수 있다. 회전 위치 임베딩[62]은 절대 및 상대 위치 정보를 모두 인코딩하기 위해 도입되며, 이는 대형 언어 모델[66]에서 효과적인 것으로 증명된다. 조건부 위치 임베딩[13]은 임의의 입력 해상도에 향상된 성능 및 일반화 가능성의 이점과 함께 입력 영상에 따라 비전 트랜스포머에 대한 위치 정보를 추가하기 위해 제안된다. LLM의 경우, 모델들은 일반적으로 주어진 고정된 컨텍스트 길이[66, 67, 77]로 미리 훈련된 다음 긴 컨텍스트 추론을 지원하기 위해 더 큰 컨텍스트 길이로 미세 조정된다. [8] 간단한 위치 보간으로 LLaMA의 컨텍스트 길이를 확장한다. RoPE의 기본 주파수 조정도 [76]에 의해 연구되어 긴 컨텍스트의 지속적인 훈련이 가능하다. NTK-Aware 스케일링된 RoPE는 LLaMA가 미세 조정 및 최소의 당혹성 저하 없이 확장된 컨텍스트 크기를 가질 수 있게 한다[55].\n' +
      '\n' +
      '**마스크 이미지 모델링.**마스크 이미지 모델링은 강한 표현을 학습하는 강력한 사전 훈련 방식입니다. BEiT[3]은 이산 시각 토큰을 예측하기 위해 마스킹된 임베딩을 갖는 트랜스포머 모델을 사전 트레이닝함으로써 BERT[20]을 컴퓨터 비전으로 확장한다. MAE(Masked Autoencoder) [25]는 입력 이미지의 무작위 패치를 마스킹하고 오토인코더를 학습하여 원본 이미지를 재구성하는 자기 지도 학습 접근법이다. SiMMIM[75]은 경량의 단층 헤드를 사용하여 원시 픽셀 값을 예측하는 MAE 접근법의 단순화된 버전이다. MaskFeat[71]은 마스킹된 패치들의 원시 픽셀 값들뿐만 아니라 핸드크래프트 HOG 기술자[17] 및 딥 피처들과 같은 추가적인 피처들을 예측하는 것을 포함하는 MAE 접근법의 확장으로서, 다운스트림 태스크들에 대한 모델의 성능을 향상시킬 수 있다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Plain Transformer\n' +
      '\n' +
      '우리의 평범한 VisionLLaMA는 ViT[22]의 파이프라인을 따르고 우리는 LLaMA의 아키텍처 설계를 가능한 한 밀접하게 유지한다. H\\(H\\times W\\)의 이미지를 위해, 먼저 변환되고 (N=\\frac{H\\times W}{P^{2}}\\) 중첩되지 않은 패치(X\\in\\mathcal{R}^{N\\times C}\\)로 평탄화된다. 그런 다음 클래스 토큰은 시퀀스의 시작 부분에 프리펜딩되고 전체 시퀀스는 \\(L\\) VisionLLaMA 블록에 의해 처리된다. [22]와 달리, 기본 블록은 위치 인코딩을 쉽게 포함하기 때문에 입력 시퀀스에 위치 인코딩을 추가하지 않는다. 구체적으로, 기본 블록은 위치 인코딩(RoPE) [62] 및 SwiGLU 활성화 [59]의 두 가지 컴포넌트들에 의해 표준 ViT 블록과 상이하다. 우리는 분류 실험을 통해 전자가 더 잘 행동한다는 것을 발견하기 때문에 RMSNorm[80] 대신 LayerNorm[2]를 여전히 활용한다(표 (b)b 참조). 기본 블록은 도 2의 (a)에 예시되어 있다. 비전 태스크에서 1D RoPE를 직접 적용하는 것은 훈련 해상도와는 다른 해상도로 잘 일반화할 수 없다는 점에 유의해야 한다. 따라서 2D 형태로 확장합니다. 이것은 형식적으로 다음과 같이 쓰여질 수 있다.\n' +
      '\n' +
      '{split}\\mathbf{z}_{ij}^{l}&=\\text{MHSA}\\left(\\text{AS2DRoPE}\\left(\\mathbf{z}_{ij}^{l-1}\\right))+\\mathbf{z}_{ij}^{l}&=\\text{SwiGLU}\\left(\\text{ LayerNorm}\\left(\\mathbf{z}^{l}{ij}\\right))+\\mathbf{z}_{ij}^{l},\\i&2,....,m\\,j\\in\\{1,2,....,n\\}.\\end{split}\\tag{1}\\right)\n' +
      '\n' +
      '여기서 \\(z_{ij}^{l}\\)는 (\\(i,j\\)) 위치의 \\(l\\) 블록의 출력을 의미한다.\n' +
      '\n' +
      '### Pyramid Transformer\n' +
      '\n' +
      'Swin[43]과 같은 부가적 상대 위치 인코딩을 활용하는 윈도우 기반 변압기에 VisionLLaMA를 적용하는 것은 간단하다. 이 논문에서, 우리는 엄격하게 통제된 설정 하에서 강력한 피라미드 변압기를 구축하는 방법을 탐구하기 위해 더 강한 베이스라인 트윈스[12]를 선택한다. 트윈스의 원래 아키텍처는 조건부 위치 인코딩 및 인터리브된 로컬-글로벌 정보 교환을 로컬 및 글로벌 주의의 형태로 이용한다. 이러한 구성 요소는 다양한 변압기에서 찾을 수 있으며, 이는 우리의 방법을 따라 다른 피라미드 변압기 변형에서 VisionLLaMA를 적용하는 것이 어렵지 않음을 의미한다. 우리의 목표는 새로운 피라미드 비전 트랜스포머를 발명하는 것이 아니라 기존 비전 트랜스포머를 기반으로 비전LLaMA의 기본 설계를 어떻게 적응하는지 보여주는 것이다. 따라서 아키텍처 및 하이퍼 매개변수에 대한 최소한의 수정 사항을 단순히 준수합니다. [12]의 명칭 규약에 따라, 연속된 두 개의 블록이 다음과 같이 기입될 수 있고,\n' +
      '\n' +
      '\\text{LayerNorm}\\left(\\text{LayerNorm}\\left(\\hat{\\mathbf{z}}^{l}\\right)\\text{LayerNorm}\\left(\\hat{\\mathbf{z}}^{l}\\right)\\text{GSA}\\left(\\text{LayerNorm}\\left(\\hat{\\mathbf{z}}^{l}\\right)\\text{GSA}\\left(\\hat{\\mathbf{z}}^{l}\\right)\\hat{\\mathbf{z}^{l}\\right)+\\hat{\\mathbf{z}^{l}\\right\n' +
      '\n' +
      '여기서 LSA는 그룹 내의 국부적 자기 주의 동작이고 GSA는 각 서브윈도우의 대표키(\\hat{\\mathbf{z}}_{ij}\\in\\mathcal{R}^{k_{1}\\times k_{2}\\times C}\\) 및 \\(m\\times n\\)가 서브윈도우 모양이다.\n' +
      '\n' +
      '우리는 AS2DRoPE에 이미 위치 정보가 포함되어 있기 때문에 피라미드 VisionLLaMA에서 조건부 위치 인코딩을 제거한다. 또한 클래스 토큰을 제거하고 분류 헤드 이전의 GAP(global average pooling)를 [12, 13]으로 사용한다. 이러한 설정에서의 기본 블록은 도 2의 (b)에 예시되어 있다.\n' +
      '\n' +
      '도 2: 일반 트랜스포머의 우리의 VisionLLaMA 블록(a) 및 피라미드 트랜스포머의 그것의 변형 블록(b).\n' +
      '\n' +
      '시퀀스 길이 이상의### 훈련 또는 추론\n' +
      '\n' +
      '**1D RoPE에서 2D.**까지 다양한 입력 해상도를 처리하는 것은 비전 작업에서 일반적인 요구 사항입니다. 합성곱 신경망은 가변 길이를 다루기 위해 슬라이딩 윈도우 메커니즘을 사용한다. 대조적으로, 대부분의 비전 트랜스포머는 로컬 윈도우 동작 또는 보간을 적용한다. 예를 들어, DeiT[65]는 서로 다른 해상도로 훈련될 때 쌍큐빅 보간을 채택한다. CPVT[13]은 컨볼루션 기반 위치 인코딩을 사용한다. 여기서 우리는 1D RoPE[62]의 성능을 평가한다. 구체적으로, 1D RoPE를 갖는 Twins-SVT-S 기반의 피라미드 VisionLLaMA는 224\\(\\times\\)224 입력에서 81.5%의 Top-1 정확도를 달성하지만, 448\\(\\times\\)448에서 평가 시 성능이 0으로 크게 저하되어 1D RoPE를 2D로 확장한다. 멀티 헤드 셀프 어텐션의 경우, 2D RoPE는 상이한 헤드들에 걸쳐 공유된다. 구체적으로, 토큰\\(x_{i,j}\\in\\mathcal{R}^{d}\\)이 주어지면, 위치 부호화된 토큰\\(x_{i,j}^{\\mathrm{PE}}=\\textbf{R}_{i,j}x_{i,j}\\)을 얻을 수 있고, 대각행렬\\(\\textbf{R}_{i,j}\\in\\mathcal{R}^{d\\times d}\\)은 다음과 같이 기입될 수 있다.\n' +
      '\n' +
      '<bmatrix}\\cos(\\theta_{i})&\\cos(\\theta_{i})&0&0&0&0&0(\\theta_{i})&-\\cos(\\theta_{i})&0&0&0&0&0&0(\\theta_{i})&-\\cos(\\theta_{i})&-\\sin(\\theta_{i})&0&0&0&0&0&0&0&0&0&\\cos(\\theta_{i})&-\\sin(\\theta_{i})&0&0&0&0&0&0&0&0&\\ldots(\\theta_{i})&-\\cos(\\theta_{i})&\n' +
      '\n' +
      '여기서 \\(\\theta_{m}=10000^{-m/d}\\) 및 \\(m\\in\\{0,4,8,...,d-4\\}\\). **R**는 직교 행렬임을 유의한다. 우리는 주파수 선택에 약간의 수정을 가하고 [62] 두 축이 동일한 주파수를 공유하도록 한다. 그것은 쉽게 확인할 수 있다.\n' +
      '\n' +
      '\\[R_{i_{1},j_{1}}^{T}R_{i_{2},j_{2}}=R_{i_{1}-i_{2},j_{1}-j_{2}}. \\tag{3}\\]\n' +
      '\n' +
      '**위치 보간은 2D RoPE가 더 잘 일반화하는 데 도움이 된다.** [8]에서 영감을 얻었으며, 이는 보간을 사용하여 LLaMA의 컨텍스트 윈도우를 확장하고, 더 높은 해상도를 수반하는 것은 VisionLLaMA의 2D 컨텍스트 윈도우를 확장하는 것과 유사하다. 확장된 고정된 컨텍스트 길이를 갖는 언어 태스크 [8]과 달리, 객체 검출과 같은 비전 태스크들은 일반적으로 상이한 반복들에서 상이한 샘플링된 해상도들을 처리한다. 본 논문에서는 224\\(\\times\\)224의 입력 해상도를 사용하여 작은 모델을 학습하고, 재학습 없이 더 큰 해상도에 대한 성능을 평가함으로써 보간 또는 외삽의 좋은 전략을 적용할 수 있도록 한다. 결과적으로, 우리는 \'앵커 해상도\'를 기반으로 하는 _auto-scaled interpolation_(소위 AS2DRoPE)를 적용한다. 일반성의 손실 없이 훈련 중 \\(H\\times H\\)의 정사각형 이미지와 앵커 분해능 \\(B\\times B\\)을 처리한다고 가정하고 계산한다.\n' +
      '\n' +
      '\\[\\textbf{R}^{\\prime}_{i,j}x_{i,j}=\\textbf{R}_{i\\cdot B/H,j\\cdot B/H}, \\tag{4}\\]\n' +
      '\n' +
      '이는 효율적으로 구현될 수 있고 추가 비용을 도입하지 않는다. 트레이닝 해상도가 변경되지 않고 유지되는 경우, AS2DRoPE는 2D RoPE로서 퇴화된다.\n' +
      '\n' +
      '피라미드 설정하의 GSA는 요약된 키에 위치 정보를 추가해야 하기 때문에 특별한 처리가 필요하다. 이러한 서브 샘플링된 키들은 특징 맵들 상의 추상화에 의해 생성된다. 일반성의 손실 없이, 우리는 \\(k\\times k\\)의 커널 크기와 \\(k\\)의 스트라이드를 갖는 컨벌루션을 사용한다. 생성된 키의 좌표는 샘플링된 특징들의 평균으로서 공식화될 수 있다. 우리는 그림 3에서 간단한 예를 보여준다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '영상 생성, 분류, 분할 및 검출에 대한 VisionLLaMA의 효과를 평가한다. 달리 명시되지 않는 한, 모든 모델은 8개의 NVIDIA Tesla A100 GPU에 대해 트레이닝된다.\n' +
      '\n' +
      '### Image Generation\n' +
      '\n' +
      '**DiT 프레임워크 기반의 영상 생성** 비전 트랜스포머와 DDPM[28]을 이용한 영상 생성의 대표적인 작업인 DiT 프레임워크[50] 하에서 VisionLLaMA를 적용한다. 구체적으로, 우리는 다른 구성 요소를 변경하지 않고 유지하면서 DiT의 원래 비전 변압기를 VisionLLaMA로 교체한다. 이 제어된 실험은 이미지 생성 작업에 VisionLLaMA의 일반성을 나타낸다. 또한, 최상의 성능을 달성하는 데 차선책일 수 있지만 원래 하이퍼 매개 변수를 변경하지 않는다. 또한 SD[53]의 사전 훈련된 VAE[34] (ft-EMA VAE 모델)를 이용하였으며, 분류기 없는 안내는 \\(1.5\\)의 계수를 이용하였다. 영상의 학습 해상도는 256 \\(\\times\\)256이며 [50]에서 제안한 바와 같이 가장 강한 adaLN-Zero 버전을 구현으로 선택한다. 우리는 또한 신속한 주의[18]와 혼합 정밀도를 사용하여 훈련 속도를 높인다. FID는 작은 구현 세부사항에 민감한 것으로 알려져 있다[49]. 정확한 계산과 공정한 비교를 위해 [21]의 텐서플로우 도구를 [50]으로 사용한다.\n' +
      '\n' +
      '우리는 DDPM의 250개의 샘플 단계를 [50]으로 선택하고 보여준다.\n' +
      '\n' +
      '그림 3: \\(4\\times 4\\) 해상도의 간단한 경우와 \\(2\\times 2\\)의 커널 크기를 사용하여 GSA 키에 대한 위치 보정. 네 점(추상키)의 위치는 (0.5, 0.5), (1, 2.5), (2.5, 0.5), (2.5, 2.5)이다.\n' +
      '\n' +
      '표 1의 결과. 일반적인 관행으로서, FID는 주요 메트릭으로 간주된다. 또한 sFID[47], Precision/Recall[35] 및 Inception Score[56]과 같은 다른 보조 메트릭을 보고한다. 대부분의 실험은 400k 훈련 단계에서 제어된다. VisionLLaMA는 다양한 모델 크기에서 DiT를 크게 능가합니다. 또한 XL 모델의 학습 단계를 2352k 단계로 확장하여 더 긴 학습 시대 설정에서 모델이 더 빠른 수렴 이점을 가지고 있는지 또는 여전히 더 잘 행동하는지 평가한다. DiT-LLaMA-XL/2는 DiT-XL/2보다 0.83 낮은 FID[27]를 가지며, 이는 VisionLLaMA가 DiT보다 더 나은 컴퓨팅 효율을 가질 뿐만 아니라 더 높은 성능을 갖는다는 것을 나타낸다. 우리는 XL 모델을 사용하여 그림 1에서 생성된 일부 샘플을 보여준다.\n' +
      '\n' +
      '**SiT 프레임워크 기반 이미지 생성** SiT[46]은 드리프트 및 확산 계수의 유연한 선택을 가지며, 이는 최근에 제안된 인터폴런트 프레임워크[1]에 의해 뒷받침된다. 명확한 여백으로 비전 트랜스포머를 이용한 영상 생성 성능을 향상시킵니다. 비젼 트랜스포머를 VisionLLaMA로 교체하여 보다 나은 모델 아키텍처의 이점을 평가하며, 이를 SiT-LLaMA라고 한다. 우리의 구현은 신중하게 통제된 실험과 함께 공개된 [46] 코드를 기반으로 한다. 특히, 하이퍼파라미터의 기본 설정은 차선책일 수 있지만 하이퍼파라미터를 변경하지 않습니다. 모든 모델은 동일한 수의 단계를 사용하여 학습됩니다. 우리는 모든 실험에 _linear interpolant_와 속도 모델을 사용한다. 공정한 비교를 위해 250단계의 SDE 샘플러(Euler)를 이용하여 공개된 코드와 샘플 50k 256\\(\\times\\)256 이미지를 재실행하고, 그 결과를 표 2에 보고한다. SiT-LLaMA는 명확한 마진으로 다양한 수준의 용량으로 모델 간 SiT를 균일하게 능가한다. SiT-L/2와 비교하여, SiT-LLaMA-L/2는 5.0 FID만큼 감소하며, 그 크기는 새로운 프레임워크(4.0 FID)의 발명으로부터 부스트보다 크다. 또한 표 13에서 보다 효율적인 ODE 샘플러(도프리5)를 보고하므로 성능 격차가 남아 있다. [46]의 관찰과 유사하게 SDE는 ODE보다 성능이 더 우수하다.\n' +
      '\n' +
      'ImageNet의### 분류\n' +
      '\n' +
      '1 감독 훈련 4.2.1\n' +
      '\n' +
      '이 섹션에서는 공정한 비교를 위해 ImageNet-1K 데이터셋 [19]에 대한 감독 훈련에 중점을 둔다. 다른 데이터 세트나 증류 트릭은 제외합니다. 모든 모델은 ImageNet-1K 트레이닝 세트를 사용하여 트레이닝되며, 표 3에 검증 세트의 정확도를 보고한다.\n' +
      '\n' +
      '**플레인 비전 트랜스포머 비교.**DeiT3[65]는 최첨단 플레인 비전 트랜스포머로, 특별한 데이터 증강을 제안하고 DeiT[64]의 성능을 높이기 위해 광범위한 하이퍼파라미터 탐색을 수행한다. DeiT3의 재생산 동안, 우리는 그것이 하이퍼파라미터에 민감하고 과적합하기 쉽다는 것을 관찰한다. 클래스 토큰을 GAP(글로벌 평균 풀링) [13]으로 교체하면 800 에폭의 훈련 후 DeiT3-Large 모델에 대해 0.7% 상위 1 정확도 하락이 발생한다. 따라서, 일반 트랜스포머에서 GAP 대신 클래스 토큰을 사용하고 그 결과를 표 3에 보고하며, VisionLLaMA는 DeiT3에 필적하는 Top-1 정확도를 달성한다. 자세한 하이퍼파라미터는 부록에 나열되어 있다. 단일 해상도에 대한 정확도는 포괄적인 비교를 제공하지 않으며, 또한 다른 이미지 해상도에 걸친 성능을 [13]으로 평가하고 결과를 표 4에 보고한다. DeiT3의 경우 학습 가능한 위치 인코딩을 위해 바이큐빅 보간을 사용한다. 이 두 모델은 224\\(\\times\\)224의 분해능에서 비슷한 성능을 보이지만, 분해능이 증가하면 그 간격이 커지므로, 이 방법은 객체 검출과 같은 많은 다운스트림 작업에 필수적인 기능인 다른 분해능에 걸쳐 더 잘 일반화된다는 것을 의미한다.\n' +
      '\n' +
      '**Pyramid Vision Transformer.** Twins-SVT[12]와 동일한 아키텍처를 사용하며 상세한 구성은 표 17에 나열되어 있다. VisionLLaMA가 이미 한 종류의 회전식 위치 인코딩을 포함하고 있기 때문에 조건부 위치 인코딩을 제거한다. 따라서 VisionLLaMA는 컨볼루션이 없는 아키텍처이다. 우리는 하이퍼파라미터를 조정하지 않고 [12]에서 제공된 설정을 직접 따른다. 최적이지는 않지만 여전히 경쟁력 있는 성과를 낼 수 있습니다. [12, 13]과 같이 클래스 토큰을 사용하지 않고 GAP를 적용한다. 특히, 모든 모델은 배치 크기가 1024인 300 에폭에 대해 학습되며, 학습률은 0.001로 초기화되고 코사인 전략에 따라 300 에폭 내에서 0으로 감쇠된다. 결과는 표 3에 나와 있으며 본 방법은 다양한 수준의 모델에 걸쳐 트윈스와 유사한 성능을 달성하고 일관되게 스윈[43]보다 우수하다. 이후 섹션에 표시된 인기 있는 다운스트림 작업을 사용하여 피라미드 변압기를 추가로 비교한다.\n' +
      '\n' +
      '4.2.2 자가 지도 훈련\n' +
      '\n' +
      '이미지넷 데이터 세트를 사용하여 자체 감독 비전 트랜스포머[25]의 성능을 평가하기 위한 두 가지 일반적인 접근법이 있다. 이 절에서는 이 두 가지 방법을 기반으로 비교합니다. 공정한 비교를 위해 학습 데이터를 ImageNet-1K로 제한한다. 또한 CLIP[51], DALLE[52] 또는 증류를 사용하는 모든 구성 요소를 제외하며, 이는 직교로 결합되어 성능을 더욱 높일 수 있다. 우리의 구현은 MM-Pretrain 프레임워크[15]를 기반으로 한다. MAE 프레임워크를 사용하고 VisionLLaMA를 사용하여 다른 구성 요소를 변경하지 않고 인코더를 교체한다. 이 사소한 수정된 설정은 우리의 접근법의 역할을 평가하기 위한 통제된 실험을 형성한다. 또한, 우리는 우리의 방법에 최적이 아닌 [25]와 동일한 하이퍼파라미터를 사용한다. 다행히도, 이 간단한 설정은 여전히 강한 기준선에 비해 상당한 성능 향상을 달성한다.\n' +
      '\n' +
      '**풀 파인-튜닝.** 그러한 설정에서, 모델은 먼저 미리 트레이닝된 가중치들을 사용하여 초기화되고, 이어서 완전히 트레이닝가능한 파라미터들을 갖는 여분의 에폭들에 대해 트레이닝된다. ImageNet에서 800개의 에포크로 훈련된 VisionLLaMA-Base는 84.0%의 Top-1 정확도를 달성하며 ViT-Base를 0.8% 초과한다. 이 방법은 SimMIM[75]보다 훈련 속도가 약 3배 빠른 [25]와 같이 0.75의 마스크 비율을 사용한다. 또한 VisionLLaMA가 충분한 훈련 자원이 주어졌을 때 이점을 유지하는지 확인하기 위해 훈련 시기를 1600년으로 늘렸다. VisionLLaMA-Base는 MAE 변종 중 새로운 최첨단 결과를 달성하며, 84.3% top-1 정확도로 ViT-Base보다 0.9% 더 우수하다. 이 결과는 새로운 훈련 목표가 제안된 MaskFeat[71]보다 훨씬 높다. 성능 포화 위험이 있는 완전 미세 조정[69, 42]과 관련하여, 우리의 부스트는 중요하다. 다음으로 추가 평가를 제공하기 위해 선형 프로빙 메트릭에 의존하며, 이는 최근 작업에 의한 대표 학습에 대한 보다 신뢰할 수 있는 평가로 간주된다[9].\n' +
      '\n' +
      '**선형 프로빙.** 이 설정에서, 모델은 SSL 스테이지로부터 미리 트레이닝된 가중치들에 의해 초기화된다. 그런 다음, 훈련 동안 분류기 헤드를 제외하고 전체 백본이 동결된다. 결과는 표 5에 나와 있다. 800 epochs의 훈련 비용으로 VisionLLaMA-Base가 ViT-Base-MAE보다 4.6% 더 우수하다. 또한 1600년대를 위해 훈련된 ViT-Base-MAE를 초과합니다. VisionLLaMA가 1600 에폭에 대해 훈련될 때, VisionLLaMA-Base는 71.7%의 Top-1 정확도를 달성한다. 또한 ViT-Large를 3.6% 초과하는 VisionLLaMA-Large를 갖도록 확장한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline Model & CFG & Flops (G) & Params (M) & Training Steps (K) & Learning Rate & FID\\(\\downarrow\\) & sFID\\(\\downarrow\\) & Precision\\(\\uparrow\\) & Recall\\(\\uparrow\\) & IS\\(\\uparrow\\) \\\\ \\hline DiT-B/4 & N & 5.56 & 130 & 400 & 0.0001 & 68.38 & 12.66 & 36.07 & 54.71 & 20.27 \\\\ DiT-LLaMA-B/4 & N & 5.56 & 130 & 400 & 0.0001 & 63.17 & 12.63 & 38.27 & 56.75 & 22.47 \\\\ DiT-B/4 & Y & 5.56 & 130 & 400 & 0.0001 & 45.38 & 9.97 & 46.89 & 53.66 & 34.27 \\\\ DiT-LLaMA-B/4 & Y & 5.56 & 130 & 400 & 0.0001 & 39.51 & 9.82 & 50.46 & 54.75 & 40.17 \\\\ \\hline DiT-L/4 & N & 19.70 & 458 & 400 & 0.0001 & 44.37 & 8.97 & 48.16 & 61.53 & 32.25 \\\\ DiT-LLaMA-L/4 & N & 19.70 & 458 & 400 & 0.0001 & 40.32 & 9.04 & 49.87 & 61.61 & 36.56 \\\\ DiT-L4 & Y & 19.70 & 458 & 400 & 0.0001 & 22.51 & 7.08 & 62.67 & 55.27 & 66.58 \\\\ DiT-LLaMA-L/4 & Y & 19.70 & 458 & 400 & 0.0001 & 18.64 & 7.01 & 65.40 & 54.35 & 78.52 \\\\ \\hline DiT-XL/4 & N & 29.05 & 675 & 400 & 0.0001 & 43.01 & - & - & - \\\\ DiT-LLaMA-XL/4 & N & 29.05 & 675 & 400 & 0.0001 & 35.99 & 8.48 & 52.31 & 61.65 & 41.18 \\\\ DiT-XL/4 & Y & 29.05 & 675 & 400 & 0.0001 & 22.52 & 7.09 & 62.68 & 55.27 & 66.58 \\\\ DiT-LLaMA-XL/4 & Y & 29.05 & 675 & 400 & 0.0001 & 18.69 & 7.02 & 65.67 & 55.57 & 78.32 \\\\ \\hline DiT-XL/2 & N & 118.64 & 675 & 2352 & 0.0001 & 10.67 & - & - & - \\\\ DiT-LLaMA-XL/2 & N & 118.64 & 675 & 2352 & 0.0001 & 9.84 & 6.47 & 67.45 & 66.71 & 117.72 \\\\ DiT-LLaMA-XL/2 & Y & 118.64 & 675 & 2352 & 0.0001 & **2.42** & 4.51 & 83.03 & 56.82 & 265.39 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: DiT 프레임워크를 이용한 이미지 생성 비교[50]. 모든 모델은 배치 크기가 256인 256\\(\\times\\)256의 이미지 해상도를 사용하여 학습되며, 샘플링된 50k 이미지를 사용하여 메트릭을 계산한다. IS: 개시 점수[56].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline Model & Flops (G) & Params (M) & Training Steps (K) & Learning Rate & FID\\(\\downarrow\\) & sFID\\(\\downarrow\\) & Precision\\(\\uparrow\\) & Recall\\(\\uparrow\\) & IS\\(\\uparrow\\) \\\\ \\hline SiT-S/2 \\({}^{\\dagger}\\) & 6.06 & 33 & 400 & 0.0001 & 58.15 & 9.12 & 41.01 & 60.23 & 24.72 \\\\ SiT-LLaMA-S/2 & 6.06 & 33 & 400 & 0.0001 & 53.90 & 8.78 & 42.98 & 60.36 & 26.74 \\\\ \\hline SiT-B/2 \\({}^{\\dagger}\\) & 23.01 & 130 & 400 & 0.0001 & 35.54 & 6.57 & 52.68 & 64.38 & 42.33 \\\\ SiT-LLaMA-B/2 & 23.01 & 130 & 400 & 0.0001 & 29.53 & 6.32 & 56.07 & 64.07 & 50.13 \\\\ \\hline DiT-L/2 & 80.71 & 458 & 400 & 0.0001 & 23.3 & - & - & - \\\\ SiT-L/2 \\({}^{\\dagger}\\) & 80.71 & 458 & 400 & 0.0001 & 19.34 & 5.28 & 63.00 & 63.60 & 70.47 \\\\ \\hline SiT-LLaMA-L/2 & 80.71 & 458 & 400 & 0.0001 & 14.32 & 5.17 & 66.39 & 63.64 & 86.85 \\\\ \\hline SiT-XL/2 \\({}^{\\dagger}\\) & 118.64 & 675 & 400 & 0.0001 & 16.98 & 5.07 & 65.12 & 64.10 & 77.06 \\\\ SiT-LLaMA-XL/2 & 118.64 & 675 & 400 & 0.0001 & **12.20** & 5.03 & 67.86 & 63.08 & 95.28 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: SiT 프레임워크를 이용한 이미지 생성 비교[46]. 모든 모델은 전체 배치 크기가 256인 256\\(\\times\\)256의 이미지 해상도를 사용하여 학습되며, 분류기가 없는 안내 없이 샘플링된 50k 이미지를 사용하여 메트릭을 계산한다. IS: 개시 점수. FID는 250단계의 SDE Euler sampler로 계산된다. \\ (\\dagger\\): 해제된 코드를 이용하여 결과를 재현하였다.\n' +
      '\n' +
      '### ADE20K의 의미론적 분할\n' +
      '\n' +
      '1 감독 훈련 4.3.1 감독 훈련\n' +
      '\n' +
      '[12, 43]에 이어 ADE20K [82] 데이터 세트에서 의미론적 분할을 사용하여 방법을 평가한다. 공정한 비교를 위해 사전 훈련 단계에서 ImageNet-1K만을 사용하여 기준선을 제한한다. 특히 UperNet[74] 프레임워크를 사용하여 백본을 피라미드 VisionLLaMA로 대체한다. 우리의 구현은 MMSegmentation framework를 기반으로 한다[14]. 모델은 전체 배치 크기가 16인 160k 단계에 대해 훈련되며, 하이퍼파라미터에 대한 자세한 설정은 섹션 B.7에 나와 있으며, 표 6의 결과를 보고한다. 유사한 FLOP에서 본 방법은 Swin과 Twins 모두 1.2% mIoU 이상 우수한 성능을 보인다.\n' +
      '\n' +
      '4.3.2 자가 지도 훈련\n' +
      '\n' +
      '우리는 UperNet[74] 프레임워크를 사용하여 백본의 인기 벤치마크인 ADE20K 데이터셋에 대한 의미론적 분할을 수행한다. 우리는 실험을 신중하게 제어하고 ViT 백본을 VisionLLaMA로 교체하면서 다른 구성 요소와 하이퍼파라미터를 변경하지 않고 유지한다. 본 논문의 구현은 MMSegmentation[14]을 기반으로 하고 세부 하이퍼파라미터는 섹션 B.6에서 제공되며, 그 결과는 표 7에 나와 있다. 800개의 에포크 사전 훈련 그룹에 대해 VisionLLaMA-B는 ViT-Base를 2.8% mIoU만큼 크게 부스팅한다. 그것은 또한 명확한 여백에 의해 추가 훈련 목표 또는 기능[71, 42]을 도입하는 것과 같은 몇몇 다른 수정들을 능가한다. 더욱이, 이러한 접근법들은 훈련 과정에 추가적인 오버헤드를 도입하고 훈련 속도를 늦춘다. 우리는 대형 모델의 시대에 방법의 훈련 속도가 점점 더 중요해지고 있음을 강조한다. 대조적으로, VisionLLaMA는 단지 베이스 모델의 교체를 수반하고 [25]와 동일한 빠른 트레이닝 속도를 갖는다. 원칙적으로, 우리의 방법은 이러한 수정들과 매끄럽게 결합될 수 있다. 우리는 더 긴 사전의 성능을 추가로 평가한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Models & Param & mIoU \\\\  & (M) & (\\%) \\\\ \\hline Swin-S [43] & 81.3 & 47.6 \\\\ Twins-SVT-B [12] & 88.5 & 47.7 \\\\ Pyramid VisionLLaMA-B & 88.5 & **49.1** \\\\ \\hline Swin-B [43] & 121 & 48.1 \\\\ Twins-SVT-L [12] & 133 & 48.8 \\\\ Pyramid VisionLLaMA-L & 133 & **50.0** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: ADE20K 검증 데이터 세트에 대한 상이한 백본과의 성능 비교. 모든 백본은 레이블이 있는 이미지넷-1K에서 사전 훈련됩니다. mIoU는 단일 척도 설정에 의해 평가된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline Model & Param & Setting & Top-1 \\\\  & (M) & & & (\\%) \\\\ \\hline DeiT-Small [64] & 22 & 2241 300E & 79.9 \\\\ CPVT-Small-GAP [13] & 23 & 2241 300E & 81.5 \\\\ DeiT3-Small [65] & 22 & 2241 800E & 81.4 \\\\ VisionLLaMA-S [65] & 22 & 2241 800E & 81.6 \\\\ Swin-T [43] & 29 & 2241 300E & 81.3 \\\\ Twins-SVT-S [12] & 24 & 2241 300E & 81.7 \\\\ Pyramid VisionLLaMA-S & 24 & 2241 300E & 81.6 \\\\ \\hline Swin-S [43] & 50 & 2241 300E & 83.0 \\\\ Twins-SVT-B [12] & 56 & 2241 300E & 83.2 \\\\ Pyramid VisionLLaMA-B & 56 & 2241 300E & 83.2 \\\\ \\hline DeiT3-Base [65] & 86 & 1921 800E + 2241 20E & 83.8 \\\\ VisionLLaMA-B & 86 & 1921 800E + 2241 20E & 83.6 \\\\ \\hline Swin-B [43] & 88 & 2241 300E & 83.3 \\\\ Twins-SVT-L [13] & 99 & 2241 300E & 83.7 \\\\ Pyramid VisionLLaMA-L & 99 & 2241 300E & 83.6 \\\\ \\hline DeiT3-Large\\({}^{\\dagger}\\) & 310 & 1601 800E+2241 20E & 84.5 \\\\ VisionLLaMA-L & 310 & 1601 800E+2241 20E & **84.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 상이한 해상도에 대한 Top-1 정확도 비교. 모델은 224에 대해 훈련되고 다른 해상도에 대해 직접 평가된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Models & Param & mIoU \\\\  & (M) & (\\%) \\\\ \\hline Swin-S [43] & 81.3 & 47.6 \\\\ Twins-SVT-B [12] & 88.5 & 47.7 \\\\ Pyramid VisionLLaMA-B & 88.5 & **49.1** \\\\ \\hline Swin-B [43] & 121 & 48.1 \\\\ Twins-SVT-L [12] & 133 & 48.8 \\\\ Pyramid VisionLLaMA-L & 133 & **50.0** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: ADE20K 검증 데이터 세트에 대한 상이한 백본과의 성능 비교. 모든 백본은 레이블이 있는 이미지넷-1K에서 사전 훈련됩니다. mIoU는 단일 척도 설정에 의해 평가된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Model & Param & Setting & Top-1 \\\\  & (M) & & (\\%) \\\\ \\hline DeiT-Small [64] & 22 & 2241 300E & 79.9 \\\\ CPVT-Small-GAP [13] & 23 & 2241 300E & 81.5 \\\\ DeiT3-Small [65] & 22 & 2241 800E & 81.4 \\\\ VisionLLaMA-S [65] & 22 & 2241 800E & 81.6 \\\\ Swin-T [43] & 29 & 2241 300E & 81.3 \\\\ Twins-SVT-S [12] & 24 & 2241 300E & 81.7 \\\\ Pyramid VisionLLaMA-S & 24 & 2241 300E & 81.6 \\\\ \\hline Swin-S [43] & 50 & 2241 300E & 83.0 \\\\ Twins-SVT-B [12] & 56 & 2241 300E & 83.2 \\\\ Pyramid VisionLLaMA-B & 56 & 2241 300E & 83.2 \\\\ \\hline DeiT3-Base [65] & 86 & 1921 800E + 2241 20E & 83.8 \\\\ VisionLLaMA-B & 86 & 1921 800E + 2241 20E & 83.6 \\\\ \\hline Swin-B [43] & 88 & 2241 300E & 83.3 \\\\ Twins-SVT-L [13] & 99 & 2241 300E & 83.7 \\\\ Pyramid VisionLLaMA-L & 99 & 2241 300E & 83.6 \\\\ \\hline DeiT3-Large\\({}^{\\dagger}\\) & 310 & 1601 800E+2241 20E & 84.5 \\\\ VisionLLaMA-L & 310 & 1601 800E+2241 20E & **84.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: ImageNet-1K 감독 분류에 대한 비교. 모든 모델은 ImageNet-1K 데이터셋을 사용하여 학습된다. \\ (\\dagger\\): 공식 코드를 사용하여 재교육. 1601 800E+2241 20E는 2단계 학습을 의미하며, 모델은 먼저 160\\(\\times\\)160을 사용하여 800 에폭에 대해 학습한 다음, 더 높은 이미지 해상도 224\\(\\times\\)224를 가진 20 에폭에 대해 학습한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Models & Param & mIoU \\\\  & (M) & (\\%) \\\\ \\hline Swin-S [43] & 81.3 & 47.6 \\\\ Twins-SVT-B [12] & 88.5 & 47.7 \\\\ Pyramid VisionLLaMA-B & 88.5 & **49.1** \\\\ \\hline Swin-B [43] & 121 & 48.1 \\\\ Twins-SVT-L [12] & 133 & 48.8 \\\\ Pyramid VisionLLaMA-L & 133 & **50.0** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: ImageNet validation set 상에서 마스킹된 이미지 모델링 SSL 방법과의 비교. \\ (\\dagger\\): MMPretrain에서 재현된다.\n' +
      '\n' +
      '1600의 교육 시기, VisionLLaMA-B는 ADE20K 검증 세트에서 50.2% mIoU를 달성하며, 이는 ViT-B를 2.1% mIoU 증가시킨다.\n' +
      '\n' +
      'COCO의### 객체 검출\n' +
      '\n' +
      '1 감독 훈련\n' +
      '\n' +
      'COCO 이의 탐지 태스크에 대한 피라미드 VisionLLaMA의 성능을 평가한다. 구체적으로 Mask RCNN 프레임워크 [26]을 사용하고, ImageNet-1K 데이터셋에서 300개의 에폭에 대해 사전 학습된 피라미드 VisionLLaMA로 백본을 [12, 43]으로 대체한다. 따라서 우리의 모델은 트윈스와 동일한 수의 매개변수 및 FLOP를 가지고 있다. 우리의 목표는 새로운 최첨단 탐지기를 달성하는 것이 아니기 때문에, 이 신중하게 통제된 실험은 일반성의 손실 없이 우리의 방법의 유효성을 검증하는 데 사용된다. 구현은 MMDetection 프레임워크[7]를 기반으로 하며, 하이퍼파라미터 설정은 섹션 B.8에 제공된다. 표 8의 표준 36 에포크(3\\(\\times\\))에 대한 결과를 보고하며, 신중하게 제어된 설정 하에서 본 모델은 Swin과 Twins 모두보다 우수하다. 구체적으로, VisionLLaMA-B는 Swin-S를 1.5% 박스 mAP 및 1.0 마스크 mAP만큼 초과한다. 더 강한 베이스라인 트윈스-B와 비교하여, 우리의 방법은 또한 1.1% 더 높은 박스 mAP 및 0.8% 더 높은 마스크 mAP의 이점을 갖는다.\n' +
      '\n' +
      '4.4.2 자가 지도 훈련\n' +
      '\n' +
      'ViTDet 프레임워크[39]를 기반으로 VisionLLaMA를 적용하여 피라미드 대응물로서 유사한 성능을 달성한다. 구체적으로 Mask RCNN 검출기를 사용하고 MAE를 사용하여 1600 에폭에 대해 훈련된 vit-Base 백본을 MAE를 사용하여 800 에폭에 대해 사전 훈련된 VisionLLaMA-Base 모델로 대체한다. 원래 ViTDet는 천천히 수렴하며 더 긴 훈련 에포크(_e.g_. 100)와 같은 전용 훈련 전략을 필요로 한다. 최적의 성능을 달성합니다. 훈련 과정에서, 우리는 VisionLLaMA가 30년 후에 유사한 성능을 달성한다는 것을 발견한다. 따라서 표준 3배 훈련 전략을 직접 활용합니다. 우리는 \\(\\beta_{1}=0.9\\)와 \\(\\beta_{2}=0.999\\)의 AdamW 최적화기를 사용한다. 우리는 또한 [39]와 같이 0.7의 계층별 학습률을 사용한다. 초기 학습률은 0.0001이고, 에포크 27과 33에서 0.1만큼 감소하며, 전체 배치 크기는 64이고, 입력 영상의 해상도는 1024\\(\\times\\)1024이므로 학습 비용은 기준선의 **36%**에 불과하다. [39]와는 달리 최적의 하이퍼파라미터를 탐색하지 않는다. 결과는 표 9에 나와 있으며 VisionLLaMA는 ViT-B를 0.6% Box mAP 및 0.8% 마스크 mAP만큼 능가한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline \\multirow{2}{*}{Backbone} & FLOPs & \\multicolumn{4}{c}{Mask R-CNN 3\\(\\times\\) + MS} \\\\ \\cline{3-6}  & (G) & \\(\\text{AP}^{\\text{b}}\\) & \\(\\text{AP}^{\\text{b}}_{50}\\) & \\(\\text{AP}^{\\text{b}}_{75}\\) & \\(\\text{AP}^{\\text{m}}_{50}\\) & \\(\\text{AP}^{\\text{m}}_{50}\\) & \\(\\text{AP}^{\\text{m}}_{75}\\) \\\\ \\hline Swin-S [43] & 222 & 47.6 & 69.4 & 52.5 & 42.8 & 66.5 & 46.4 \\\\ Twins-SVT-B [12] & 224 & 48.0 & 69.5 & 52.7 & 43.0 & 66.8 & 46.6 \\\\ Pyramid VisionLLaMA-B & 224 & **49.1** & **70.5** & **54.0** & **43.8** & **67.4** & **47.0** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: Mask R-CNN 프레임워크를 이용한 COCO val2017 데이터셋에 대한 객체 검출 및 인스턴스 분할 성능. FLOP는 800\\(\\times\\)600 영상을 이용하여 평가하였다. 모든 백본은 ImageNet-1K 데이터 세트에서 300 에폭에 대해 훈련된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Models & Pretrain Epochs & mIoU \\\\  & & (\\%) \\\\ \\hline ViT-B\\({}^{\\dagger}\\) & 800 & 46.2 \\\\ SemMAE [37] & 800 & 46.3 \\\\ MFF-MAE [42] & 800 & 47.9 \\\\ VisionLLaMA-B & 800 & **49.0** \\\\ \\hline \\hline ViT-B & 1600 & 48.1 \\\\ MaskFeat [71] & 1600 & 48.3 \\\\ VisionLLaMA-B & 1600 & **50.2** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: ADE20K 검증 데이터세트에서 상이한 SSL 훈련된 백본과의 성능 비교. 모든 백본은 라벨 없이 이미지넷-1K **에서 미리 훈련된다.** mIoU는 단일 스케일 설정에 의해 평가된다. \\ (\\dagger\\): [14]를 이용하여 결과를 재현한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Model & Pretrained & mAP\\({}^{Box}\\) & mAP\\({}^{Mask}\\) & Epochs \\\\ \\hline Swin-S [43] & ImageNet sup 300e & 47.6 & 42.8 & 36 \\\\ Twins-SVT-B [12] & ImageNet sup 300e & 48.0 & 43.0 & 36 \\\\ ViT-B [39] & MAE 1600e & 51.6 & 45.7 & 100 \\\\ VisionLLaMA-B & MAE 800e & **52.2** & **46.3** & 36 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: ViTDet[39] 기반의 COCO 2017 데이터셋에 대한 객체 검출 결과. sup: ImageNet-1K에서 지도학습\n' +
      '\n' +
      '##5 절제 연구 및 논의\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '달리 명시되지 않는 한, 우리는 Ablation을 수행하기 위해 ViT-Large 모델(160I 800E+224I 20E)을 선택한다.\n' +
      '\n' +
      '**FFN 및 SwiGLU**의 절제. FFN을 SwiGLU로 교체하고 그 결과를 표 10(a)에 보고한다. 따라서 우리는 성능 격차를 관찰하지 않으므로 SwiGLU를 활용하고 LLaMA 아키텍처에 추가 수정을 도입하는 것을 피한다. 이것은 또한 우리가 자기 주의 블록의 삭제에 집중하도록 동기를 부여한다. 다중 헤드 셀프 어텐션을 적용함에 따라, 나머지 두 차이는 정규화 및 위치 인코딩이 된다.\n' +
      '\n' +
      '**정규화 전략의 절제.** 트랜스포머에서 널리 사용되는 두 정규화 방법, 즉 RMSNorm[80]과 LayerNorm[2]를 비교하여 그 결과를 표 10(g)에 보고한다. 후자는 더 나은 최종 성능을 가지며, 이는 _re-centering invariance_가 비전 작업에서 또한 중요하다는 것을 나타낸다. 또한 LayerNorm이 RMSNorm보다 2\\(2\\%\\) 더 느린 반복당 평균 시간만큼 훈련 속도를 보고한다. 따라서 보다 나은 트레이드오프를 위해 RMSNorm 대신 LayerNorm을 선택한다. 트레이닝 속도는 상이한 하드웨어 디바이스들에 걸쳐 상이할 수 있고 또한 전체 아키텍처에 의해 영향을 받을 수 있다는 점에 유의한다.\n' +
      '\n' +
      '다음으로 고정 해상도를 사용하는 정적 경우와 가변 해상도를 사용하는 동적 경우의 두 가지 측면에서 위치 부호화의 역할을 평가한다. 전자는 분류 작업에서 일반적이지만 후자는 분할 및 객체 검출과 같은 다운스트림 작업에서 필수적이다.\n' +
      '\n' +
      '**부분 PE.** RoPE를 사용하여 전체 채널의 비율을 조정하여 표 10(b)의 결과를 보고하는데, 이는 비율이 작은 임계값 이상으로 설정되면 양호한 성능을 보일 수 있다. 우리는 이러한 설정에서 유의미한 차이를 관찰하지 않는다. 따라서 [66]의 기본 설정을 유지하고 [30, 4]를 따르지 않는다.\n' +
      '\n' +
      '**주파수 베이스.** 베이스 주파수를 변경하고 그 결과를 표 10(c)에 보고하며, 이는 큰 범위의 주파수에 대해 성능이 견고하다는 것을 의미한다. 결과적으로 배치에 대한 추가 특수 처리를 피하기 위해 [66]의 기본값을 유지합니다.\n' +
      '\n' +
      '**각 헤드에 대해 공유 PE.** 서로 다른 헤드에 걸쳐 동일한 PE를 공유하는 것(각 헤드에서 주파수가 1에서 10000까지 변동함)이 독립적인 것보다 더 낫다는 것을 발견한다(모든 채널에 걸쳐 주파수가 1에서 10000까지 변동함). 그 결과를 표 10(d)에 나타낸다.\n' +
      '\n' +
      '**특징 추상화 전략** 우리는 일반 \'대형\' 모델을 사용하여 클래스 토큰[22]과 GAP[13]의 두 가지 공통 특징 추출 전략을 비교하고 그 결과를 표 10(e)에 보고한다. 클래스 토큰을 사용하는 것이 [13]과 다른 GAP보다 좋다. 그러나 두 사례의 훈련 설정은 상당히 다릅니다. 우리는 또한 0.3%의 유사한 성능 격차를 관찰하기 위해 DeiT3-L을 사용하여 추가 실험을 한다. 또한 \'몰\'과 \'베이스\' 모델의 성능을 평가한다. 작은 모델에 대한 반대 결론을 보는 것은 흥미롭다. 우리는 [65]에서 사용된 더 높은 드롭-경로 레이트가 GAP와 같은 파라미터가 없는 추상화가 목적에 부합하기 어렵게 한다고 의심한다.\n' +
      '\n' +
      '**위치 인코딩 전략.** 피라미드 VisionLLaMA-S에 학습 가능한 PE[64] 및 PEG[13]와 같은 다른 절대 위치 인코딩 전략도 추가한다. 우리는 강한 기준선이 존재하기 때문에 \'몰\' 모델을 사용하고 그 결과를 표 10(f)에 보고한다. 학습 가능한 PE는 성능을 향상시키지 않지만 PEG는 기준선을 81.6%에서 81.8%로 약간 개선한다. 그러나 우리는 세 가지 측면에 관한 기본 구성 요소로 PEG를 포함하지 않는다. 먼저, 우리는 LLaMA[66]에서 가장 작은 수정들을 유지하려고 노력한다. 둘째, 우리의 목표는 ViT[22]와 같은 다양한 작업에 대한 보편적인 접근법을 제안하고 있다. MAE[25]와 같은 마스킹된 이미지 프레임워크의 경우 백본에 PEG가 포함된 경우 마스킹된 토큰의 감소된 트레이닝 비용을 유지하는 것은 간단하지 않다. 입력에서 [75]와 같이 패치를 마스크하면 훈련 속도가 크게 느려집니다. 더욱이, 인코더에 마스킹된 패치들을 포함하는 것은 인코더로의 데이터 분배 이동을 야기하고, 이는 다운스트림 태스크들의 성능을 심각하게 손상시킨다. 원칙적으로 MAE 프레임워크에 따라 희소 PEG를 적용할 수 있지만 배치 비친화적인 운영자를 도입할 것이다. 희소 컨볼루션이 그의 조밀한 버전으로서 충분한 위치 정보를 포함하는지는 미해결 문제로 남아 있다[13, 33]. 셋째, 모달리티 바인딩 디자인을 피하는 것은 텍스트와 비전을 넘어 다른 모달리티를 다루는 추가 연구의 길을 열어준다.\n' +
      '\n' +
      '**입력 크기에 대한 민감도.** 결과를 표 10(a)에 보고하기 위한 훈련 없이 확대 및 일반적으로 사용되는 해상도에 대한 성능을 추가로 비교한다. 여기서는 피라미드 변압기가 일반 변압기보다 하류 작업에서 더 인기가 있기 때문에 피라미드 변압기를 사용한다. 1D-RoPE가 변경된 결의안으로 심각하게 고통받는 것은 놀라운 일이 아니다. NTK-Aware 보간법(\\(\\alpha=2\\)은 NTK-Aware(\\(\\alpha=1\\))인 2D-RoPE1과 유사한 성능을 보인다. AS2DRoPE는 더 큰 해상도에 대해 최고의 성능을 보여준다.\n' +
      '\n' +
      '각주 1: 동적 NTK-Aware를 적용하여 성능을 224로 유지할 수 있지만, 더 큰 해상도에서 향상된 성능을 가져오지 않습니다.\n' +
      '\n' +
      '### Discussion\n' +
      '\n' +
      '우리는 다양한 작업에서 ViT보다 우리 방법의 우수한 성능 이면의 기본 메커니즘을 추가로 조사한다. 절제 연구에서 알 수 있듯이, 우리의 위치 인코딩 전략은 큰 차이를 만든다. 이 절에서는 향상된 수렴 속도에 대해 논의하고 기본 메커니즘을 이론적으로 합리화하려고 시도한다.\n' +
      '\n' +
      '**융합 속도** 이미지 생성을 위해 훈련 단계에서 성능을 연구합니다. 구체적으로, 100k, 200k, 300k 및 400k 반복에서 체크포인트를 저장하여 충실도 메트릭을 계산한다. SDE는 ODE보다 상당히 느리기 때문에 대신 ODE 샘플러를 사용하도록 선택한다. 엄격하게 통제된 실험의 결과는 표 10에 나열되어 있다. VisionLLaMA는 모든 모델에서 ViT보다 훨씬 빠르게 수렴하는 것으로 판단된다. 300k 훈련 반복을 가진 SiT-LLaMA는 400k 단계로 기준선을 능가한다.\n' +
      '\n' +
      '또한 ImageNet의 지도 학습 환경에서 DeiT3-Large를 사용하여 수렴 속도를 비교하여 그림 4의 800 에포크 동안 상위 1의 검증 정확도를 보여주었으며 VisionLLaMA가 DeiT3-L보다 빠르게 수렴함을 나타낸다. 우리는 MAE 프레임워크 [25]에서 ViT-Base 모델의 800개 에폭에 걸친 훈련 손실을 추가로 비교하고 그림 5에 설명한다. VisionLLaMA는 초기에 훈련 손실이 더 낮고 추세는 끝까지 유지된다.\n' +
      '\n' +
      '**이론 추론** 우리는 이론적인 관점에서 우리의 위치 부호화의 메커니즘에 파고든다. 일반성의 손실 없이 di의 입력 임베딩을 고려할 때\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 11: 이미지넷-1K 상의 일반 변압기 ViT-L/16(DeiT3-L)을 사용한 **절제 실험**. 우리는 상위 1개의 정확도(정확도)를 보고한다. 지정되지 않은 경우 기본값은 160(\\times\\)160의 이미지 해상도에서 800 에폭, 224(\\times\\)224를 사용하여 20 에폭으로 설정되며 기본값은 회색(\\cdot\\dagger\\)으로 표시됨: 릴리즈 코드를 실행한다. 모든 정확도는 1위입니다.\n' +
      '\n' +
      '그림 4: DeiT3의 설정을 사용하여 VisionLLaMA의 더 빠른 수렴.\n' +
      '\n' +
      '그림 5: ViT-B와 비교하여 VisionLLaMA에서 MAE 사전 훈련의 손실 곡선.\n' +
      '\n' +
      'mension \\(d=4\\), 위치에서의 질의 \\((i,j)\\)는 \\(q_{i,j}\\)으로 작성될 수 있다. 2차원 sin-cos 부호화를 이용한 위치 부호화는 \\(k_{i,j}\\)과 \\(p_{i,j}\\)에서 키 벡터를 표현하기 위해 \\(k_{i,j}\\)을 사용한다. 이러한 덧셈 부호화를 이용한 \\(q_{i_{1},j_{1}}\\)과 \\(k_{i_{2},j_{2}}\\) 사이의 내적은 다음과 같이 표기될 수 있으며,\n' +
      '\n' +
      '{T}k_{i_{2},j_{1}}^{T}(q_{i_{1},j_{1}}+p_{i_{2},j_{1}}}^{T}k_{i_{2},j_{1}}}+p_{i_{2},j_{1}}}^{T}k_{i_{2},j_{1}}}+p_{i_{2},j_{1}}}^{T}k_{i_{2},j_{1}}}+p_{i_{2},j_{1}}}^{T}k_{i_{2},j_{1}}}+p_{i_{2},j_{1}}}^{t}{i_{2},j_{1}}}^{i_{2},j_{1}}}^{t}{i_{2},j_{1}}}^{\n' +
      '\n' +
      '첫 번째 항목은 내용물의 내적 내적이다. 두 번째 항목은 장거리 감쇠 효과를 나타내는 \\(f(i_{1}-i_{2},j_{1}-j_{2})\\)의 형태로 위치 효과를 반영한다. 그러나 세 번째 항목 \\(M=q_{i_{1},j_{1}}^{T}p_{i_{2},j_{2}}+p_{i_{1},j_{1}}^{T}k_{i_{2},j_{2}})는 콘텐츠 특징과 직접 상호작용하는 위치를 의미하므로 학습 과정이 느려진다.\n' +
      '\n' +
      '이에 반해 RoPE를 이용한 내부 내적은 다음과 같이 기입할 수 있다.\n' +
      '\n' +
      '[(R_{i_{1},j_{1}}}q_{i_{1},j_{1}}}}}^{T}(R_{i_{2},j_{2}}}k_{i_{2},j_{1}}}^{T}R_{i_{1},j_{1}}}R_{i_{2},j_{2}}}k_{i_{2},j_{1}}}{T}R_{i_{1},j_{1}}}}k_{i_{2},j_{1}}}}k_{i_{2},j_{2}}}}{t}R_{i_{1},j_{1}}}}k_{i_{2},j_{2}}}}{t}{i_{1},j_{1}}}}k_{i_{2},j_{2}}}}{t}{i\n' +
      '\n' +
      '\\(R_{i_{1}-i_{2},j_{1}-j_{2}}\\)은 \\(q\\)과 \\(k\\)의 위치가 가까울 경우 더 큰 절대값을 기여하고 반대일 경우 더 작은 값을 기여한다. 이것은 컨볼루션의 함수와 유사한 사전 편향으로서 특정 지역들을 도입한다. 또한, \\(R_{i_{1}-i_{2},j_{1}-j_{2}}\\)은 0과 1 사이의 인자의 곱셈에 의해 내적을 조정하는데, 이는 \\(f(i_{1}-i_{2},j_{1}-j_{2})\\)의 덧셈보다 유연하고 빠르다. 우리는 이러한 유연성이 변압기가 모델 용량을 효과적으로 활용할 수 있도록 하여 편향 도입 또는 내용에서 위치를 분리하는 데 일부 용량을 할당하지 않고 좋은 표현을 학습할 수 있다고 믿는다. 이와 같이 VisionLLaMA는 더 빠르게 수렴할 뿐만 아니라 최종 성능이 더 우수하다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '간단히 말해서, 우리는 비전 양식에서 LLaMA 아키텍처의 이점을 즐기기 위해 VisionLLaMA를 제시한다. 영상 분류, 탐지 및 분할과 같은 수많은 다운스트림 비전 작업에서 전력을 검증하기 위해 감독 또는 자체 감독 방식으로 훈련된다. 특히 확산 프레임워크 DiT 및 SiT에서 이미지 생성 능력을 탐색하여 그 효능을 확인한다. 우리는 VisionLLaMA가 다운스트림 응용 프로그램의 큰 영역을 촉진하기 위한 새로운 비전 백본 역할을 할 수 있는 강력한 잠재력을 가지고 있다고 결론지었다.\n' +
      '\n' +
      '인정: 이 작업은 부분적으로 중국 국가 핵심 R&D 프로그램(No. 2022ZD0118-700)에 의해 지원되었다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. _arXiv preprint arXiv:2303.08797_, 2023.\n' +
      '* [2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.\n' +
      '* [3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. _arXiv preprint arXiv:2106.08254_, 2021.\n' +
      '* [4] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In _International Conference on Machine Learning_, pages 2397-2430. PMLR, 2023.\n' +
      '* [5] Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Wing Yin Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024.\n' +
      '* [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [7] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection toolbox and benchmark. _arXiv preprint arXiv:1906.07155_, 2019.\n' +
      '* [8] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. _arXiv preprint arXiv:2306.15595_, 2023.\n' +
      '* [9] Xinlei Chen, Zhuang Liu, Saining Xie, and Kaiming He. Deconstructing denoising diffusion models for self-supervised learning. _arXiv preprint arXiv:2401.14404_, 2024.\n' +
      '* [10] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices. _arXiv preprint arXiv:2312.16886_, 2023.\n' +
      '* [11] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model. _arXiv preprint arXiv:2402.03766_, 2024.\n' +
      '* [12] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. In _Adv. Neural Inform. Process. Syst._, 2021.\n' +
      '* [13] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and Chunhua Shen. Conditional positional encodings for vision transformers. In _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* [14] MMSegmentation Contributors. Mmsegmentation: Openmlab semantic segmentation toolbox and benchmark, 2020.\n' +
      '*[15]MMPreTrain Contributors. Openmmlab의 사전 트레이닝 툴박스 및 벤치마크. [https://github.com/openmlab/mmpretrain] (https://github.com/openmlab/mmpretrain), 2023.\n' +
      '* [16] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops_, pages 702-703, 2020.\n' +
      '* [17] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In _2005 IEEE computer society conference on computer vision and pattern recognition (CVPR\'05)_, volume 1, pages 886-893. Ieee, 2005.\n' +
      '* [18] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. _arXiv preprint arXiv:2307.08691_, 2023.\n' +
      '* [19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.\n' +
      '* [20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* [21] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.\n' +
      '* [22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* [23] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. _arXiv preprint arXiv:2210.17323_, 2022.\n' +
      '* [24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In _Advances in Neural Information Processing Systems (NIPS)_, 2014.\n' +
      '* [25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16000-16009, 2022.\n' +
      '* [26] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _Proceedings of the IEEE international conference on computer vision_, pages 2961-2969, 2017.\n' +
      '* [27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [29] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In _NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications_, 2021.\n' +
      '*[30][https://stability.ai/](https://stability.ai/) 안정 코드 3b: 에지 상의 코딩.\n' +
      '* [31] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 646-661. Springer, 2016.\n' +
      '* [32] Aapo Hyv\\({}^{\\prime}\\)arinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 2005.\n' +
      '* [33] Md Amirul Islam*, Sen Jia*, and Neil D. B. Bruce. How much position information do convolutional neural networks encode? In _International Conference on Learning Representations_, 2020.\n' +
      '* [34] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.\n' +
      '* [35] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. _Advances in Neural Information Processing Systems_, 32, 2019.\n' +
      '* [36] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. _arXiv preprint arXiv:2308.00692_, 2023.\n' +
      '* [37] Gang Li, Heliang Zheng, Daqing Liu, Chaoyue Wang, Bing Su, and Changwen Zheng. Semmae: Semantic-guided masking for learning masked autoencoders. _Advances in Neural Information Processing Systems_, 35:14290-14302, 2022.\n' +
      '* [38] Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm tweaking: High-performance low-bit quantization of large language models. In _Thirty-Eighth AAAI Conference on Artificial Intelligence_, 2024.\n' +
      '* [39] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In _European Conference on Computer Vision_, pages 280-296. Springer, 2022.\n' +
      '* [40] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.\n' +
      '* [41] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _NeurIPS_, 2023.\n' +
      '* [42] Yuan Liu, Songyang Zhang, Jiacheng Chen, Zhaohui Yu, Kai Chen, and Dahua Lin. Improving pixel-based mim by reducing wasted modeling capability. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5361-5372, 2023.\n' +
      '* [43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.\n' +
      '\n' +
      '* [44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* [45] Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, and Lei Bai. Fit: Flexible vision transformer for diffusion model. _arXiv preprint arXiv:2402.12376_, 2024.\n' +
      '* [46] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. _arXiv preprint arXiv:2401.08740_, 2024.\n' +
      '* [47] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W Battaglia. Generating images with sparse representations. _arXiv preprint arXiv:2103.03841_, 2021.\n' +
      '* [48] OpenAI. Gpt-4 technical report. 2023. Technical Report.\n' +
      '* [49] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11410-11420, 2022.\n' +
      '* [50] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4195-4205, 2023.\n' +
      '* [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [52] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv, 2022.\n' +
      '* [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '* [54] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.\n' +
      '* [55] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code Ilama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_, 2023.\n' +
      '* [56] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. _Advances in neural information processing systems_, 29, 2016.\n' +
      '* [57] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.\n' +
      '* [58] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. _arXiv preprint arXiv:1803.02155_, 2018.\n' +
      '* [59] Noam Shazeer. Glu variants improve transformer. _arXiv preprint arXiv:2002.05202_, 2020.\n' +
      '* [60] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pages 2256-2265. PMLR, 2015.\n' +
      '* [61] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '* [62] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, page 127063, 2023.\n' +
      '* [63] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2818-2826, 2016.\n' +
      '* [64] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers and distillation through attention. In _International Conference on Machine Learning_, volume 139, pages 10347-10357, July 2021.\n' +
      '* [65] Hugo Touvron, Matthieu Cord, and Herve Jegou. Deit iii: Revenge of the vit. In _European Conference on Computer Vision_, pages 516-533. Springer, 2022.\n' +
      '* [66] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, and Faisal Azhar. Llama: Open and efficient foundation language models. 2023.\n' +
      '* [67] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [69] Kirill Vishniakov, Zhiqiang Shen, and Zhuang Liu. Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy. _arXiv preprint arXiv:2311.09215_, 2023.\n' +
      '* [70] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 568-578, 2021.\n' +
      '* [71] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14668-14678, 2022.\n' +
      '* [72] Fei Wei, Xinyu Zhang, Ailing Zhang, Bo Zhang, and Xiangxiang Chu. Lenna: Language enhanced reasoning detection assistant. _arXiv preprint arXiv:2312.02433_, 2023.\n' +
      '* [73] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In _International Conference on Machine Learning_, pages 38087-38099. PMLR, 2023.\n' +
      '* [74] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In _Proceedings of the European conference on computer vision (ECCV)_, pages 418-434, 2018.\n' +
      '* [75] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9653-9663, 2022.\n' +
      '* [76] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. _arXiv preprint arXiv:2309.16039_, 2023.\n' +
      '* [77] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. _arXiv preprint arXiv:2309.10305_, 2023.\n' +
      '* [78] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. _arXiv preprint arXiv:1708.03888_, 2017.\n' +
      '* [79] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6023-6032, 2019.\n' +
      '* [80] Biao Zhang and Rico Sennrich. Root mean square layer normalization. _Advances in Neural Information Processing Systems_, 32, 2019.\n' +
      '* [81] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.\n' +
      '* [82] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 633-641, 2017.\n' +
      '* [83] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. In _The Twelfth International Conference on Learning Representations_, 2024.\n' +
      '\n' +
      '더 많은 실험\n' +
      '\n' +
      '표 13의 SiT 프레임워크를 기반으로 250단계 ODE 샘플러(도프리5)를 사용하여 이미지 생성을 평가한다.\n' +
      '\n' +
      '## 부록 B 하이퍼파라미터\n' +
      '\n' +
      'ImageNet-1K에서 VisionLLaMA의 지도훈련\n' +
      '\n' +
      '일반 변압기는 [65]와 같은 하이퍼파라미터를 사용한다. 상세한 설정은 표 14에 제공된다. VisionLLaMA-S는 224\\(\\times\\)224의 해상도로 800 에폭에 대해 ImageNet-1K에서 훈련된다. VisionLLaMA-B는 192\\(\\times\\)192의 입력 크기로 800 에폭에 대해 먼저 훈련된 후 20 에폭에 대해 미세 조정된다. VisionLLaMA-L은 먼저 800 에폭에 대해 \\(160\\times 160\\)의 해상도로 훈련된 후, \\(224\\times 224\\)의 20 에폭에 대해 미세 조정된다.\n' +
      '\n' +
      'Pyramid VisionLLaMA의 감독훈련\n' +
      '\n' +
      '우리는 [12]와 같은 설정을 사용한다. 구체적으로, 모든 모델은 AdamW 최적화기를 사용하여 1024의 글로벌 배치 크기를 갖는 300 에폭에 대해 ImageNet-1K 상에서 트레이닝된다. 학습률은 5개의 웜업 에포크 내에서 0.001로 증가하고 코사인 스케줄에 따라 0으로 감쇠된다. 우리는 모든 모델에 대해 [12]와 동일한 데이터 증강과 224\\(\\times\\)224의 이미지 해상도를 사용한다. 과적합을 피하기 위해 우리는 0.05의 중량 감쇠와 드롭 경로[31]를 사용한다(작은 기본 모델과 큰 모델에 대해 각각 0.2, 0.3, 0.5).\n' +
      '\n' +
      'ImageNet에서의### 마스크 이미지 모델링\n' +
      '\n' +
      '운동량 \\(\\beta_{1}=0.9\\)과 \\(\\beta_{2}=0.95\\)의 AdamW 최적화기를 사용하였다. 전체 배치 크기는 4096이며, 초기 학습률은 1.5\\(\\times\\)10\\({}^{-}4\\)이고 800 또는 1600 에폭 내에서 0으로 감쇠된다. 또한 학습 속도를 예열하기 위해 40개의 시대를 사용합니다. [25]와 같이 단순 데이터 증강 RRC(random-resize-crops)만을 사용한다. 게다가, 우리는 0.05의 중량 감소를 사용한다.\n' +
      '\n' +
      '### ImageNet의 선형프로빙\n' +
      '\n' +
      '우리는 [25]의 설정을 따르고 세부 사항을 표 15에 나타낸다.\n' +
      '\n' +
      'SSL 사전 학습된 모델을 위한 ImageNet 상의### SFT\n' +
      '\n' +
      '우리는 [25]의 동일한 설정을 따르고 표 16의 세부 사항을 보여준다. [25]의 0.75가 우리의 방법에 대해 과적합하다는 것을 발견하기 때문에 계층별 학습 속도 감쇠만 수정하고 0.45로 설정했다.\n' +
      '\n' +
      'SSL 사전 학습 모델을 위한### 분할\n' +
      '\n' +
      '[12]의 기본 설정을 따릅니다. 우리는 \\(\\beta_{1}=0.9\\) 및 \\(\\beta_{2}=0.999\\)의 AdamW [44] 최적화기를 사용한다. 전체 배치 크기는 16이며 초기 학습률은 6\\(\\times\\) 10\\({}^{-5}\\)이고 선형적으로 0으로 감쇠한다. 우리는 또한 예열하기 위해 1500번의 반복을 사용한다. 우리는 또한 0.05의 \\(l_{2}\\) 중량 감쇠와 0.1의 낙하 경로 속도 [31]을 사용한다.\n' +
      '\n' +
      '피라미드 변압기의### 분할\n' +
      '\n' +
      '우리는 [12]의 설정을 따르며, 이는 B.6과 거의 동일하다. 우리는 피라미드 VisionLLaMA-B 모델에 대해 0.2의 드롭 경로 레이트를 사용한다.\n' +
      '\n' +
      '피라미드 변압기의 물체 검출\n' +
      '\n' +
      '우리는 [12]와 같은 설정을 사용한다. 우리는 \\(\\beta_{1}=0.9\\) 및 \\(\\beta_{2}=0.999\\)의 AdamW 최적화기를 사용한다. 모든 모델은 전체 배치 크기가 16인 36개의 에폭에 대해 학습되며, 초기 학습률은 1\\(\\times\\)10\\({}^{-4}\\)이고, 에폭 27과 33에서 1000번의 웜업과 10.0의 감쇠를 반복한다. 과적합을 피하기 위해 모든 모델에 \\(l_{2}\\) 가중치 감쇠를 적용한다.\n' +
      '\n' +
      '일반 변압기의 물체 검출\n' +
      '\n' +
      '우리는 \\(\\beta_{1}=0.9\\) 및 \\(\\beta_{2}=0.999\\)의 AdamW 최적화기를 사용한다. 훈련 해상도는 [39]와 같이 1024\\(\\times\\)1024로 고정된다. 이 모델은 전체 배치 크기가 64인 36개의 에폭에 대해 학습되며, 초기 학습률은 1\\(\\times\\)10\\({}^{-4}\\)이고, 에폭 27과 33에서 1000번의 웜업과 10.0의 감쇠를 반복하며, 0.1의 중량 감쇠를 사용하고, 0.7의 계층별 학습률 감쇠를 [39]와 같이 적용한다.\n' +
      '\n' +
      'DiT-LLaMA의### 영상 생성\n' +
      '\n' +
      '우리는 [53]과 같은 VAE를 사용한다. 우리는 운동량 \\(\\beta_{1}=0.9\\)과 \\(\\beta_{2}=0.999\\)을 갖는 AdamW 최적화기를 사용한다. 우리는 모든 모델에 걸쳐 256의 글로벌 배치 크기를 사용합니다. 학습률은 1\\(\\times\\)10\\({}^{-4}\\)으로 고정된다. 학습 해상도는 256 \\(\\times\\)256이며, 추론은 250 단계의 DDPM을 사용한다. 우리는 튜닝 없이 ADM [21]의 기본 설정을 유지합니다. 구체적으로 0.0001에서 0.02까지의 \\(\\beta\\)와 학습 가능한 분산 \\(\\sigma_{\\theta}\\)을 갖는 \\(t_{max}=1000\\) 선형 스케줄을 사용한다.\n' +
      '\n' +
      'SiT-LLaMA의### 영상 생성\n' +
      '\n' +
      '우리는 SD[53]와 같은 VAE를 사용한다. ODE 샘플러는 도파리5와 세트 아톨과 rtol을 각각 1e-6과 1e-3으로 사용한다.\n' +
      '\n' +
      '## 부록 C 구조 설정\n' +
      '\n' +
      '### Pyramid VisionLLaMA\n' +
      '\n' +
      '피라미드 아키텍처의 세부 설정은 표 17과 같다.\n' +
      '\n' +
      '시각 이해를 위한 일반 변압기###\n' +
      '\n' +
      '아키텍처의 상세한 설정은 표 18과 같다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c|c} \\hline \\hline \\multicolumn{2}{c}{Output Size} & \\multicolumn{2}{c}{Layer Name} & \\multicolumn{1}{c}{S} & \\multicolumn{1}{c}{B} & \\multicolumn{1}{c}{L} \\\\ \\hline \\multirow{3}{*}{Stage 1} & \\(\\frac{H}{4}\\) & \\(\\times\\) & \\(\\frac{W}{4}\\) & & Patch Embedding \\({}_{1}\\) & \\(P_{1}=4\\); \\(C_{1}=64\\) & \\(P_{1}=4\\); \\(C_{1}=128\\) \\\\ \\cline{3-6}  & \\(\\frac{H}{4}\\) & \\(\\times\\) & \\(\\frac{W}{4}\\) & & \\(\\left[\\begin{array}{c}LSA\\\\ GSA\\end{array}\\right]\\times 1\\) & & \\(\\left[\\begin{array}{c}LSA\\\\ GSA\\end{array}\\right]\\times 1\\) & & \\(\\left[\\begin{array}{c}LSA\\\\ GSA\\end{array}\\right]\\times 1\\) \\\\ \\hline \\multirow{3}{*}{Stage 2} & \\(\\frac{H}{8}\\) & \\(\\times\\) & \\(\\frac{W}{8}\\) & & \\(\\left[\\begin{array}{c}LSA\\\\ GSA\\end{array}\\right]\\times 1\\) & & \\(\\left[\\begin{array}{c}LSA\\\\ GSA\\end{array}\\right]\\times 1\\) & & \\(\\left[\\begin{array}{c}LSA\\\\ GSA\\end{array}\\right]\\times 1\\) \\\\ \\hline \\multirow{3}{*}{Stage 3} & \\(\\frac{H}{16}\\) & \\(\\times\\) & \\(\\frac{W}{16}\\) & & \\(\\left[\\begin{array}{c}LSA\\\\ GSA\\end{array}\\right]\\times 5\\) & & \\(\\left[\\begin{array}{c}LSA\\\\ GSA\\end{array}\\right]\\times 9\\) & & \\(\\left[\\begin{array}{c}LSA\\\\ GSA\\end{array}\\right]\\times 9\\) \\\\ \\hline \\multirow{3}{*}{Stage 4} & \\(\\frac{H}{32}\\) & \\(\\times\\) & \\(\\frac{W}{32}\\) & & Patch Embedding \\(P_{4}=2\\); \\(C_{4}\\!=\\!512\\) & \\(P_{4}=2\\); \\(C_{4}\\!=\\!768\\) & \\(P_{4}=2\\); \\(C_{4}\\!=\\!1024\\) \\\\ \\cline{3-6}  & \\(\\frac{H}{32}\\) & \\(\\times\\) & \\(\\frac{W}{32}\\) & & \\(\\left[\\begin{array}{c}GSA\\end{array}\\right]\\times 4\\) & \\(\\left[\\begin{array}{c}GSA\\end{array}\\right]\\times 2\\) & \\(\\left[\\begin{array}{c}GSA\\end{array}\\right]\\times 2\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 17: 피라미드 VisionLLaMA의 구성 상세.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Model & Layers & Dims & Heads \\\\ \\hline \\multirow{2}{*}{DiT-LLaMA-S / SiT-LLaMA-S} & 12 & 384 & 6 \\\\ SiT-LLaMA-B / SiT-LLaMA-B & 12 & 768 & 12 \\\\ SiT-LLaMA-L / SiT-LLaMA-L & 24 & 1024 & 16 \\\\ SiT-LLaMA-XL / SiT-LLaMA-XL & 28 & 1152 & 16 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 19: 이미지 생성에 대한 VisionLLaMA에 대한 아키텍처 설정.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>