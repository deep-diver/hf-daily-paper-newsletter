<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# StrokeNUWA : 벡터 그래픽 합성을 위한 토큰화 스트로크\n' +
      '\n' +
      'Zecheng Tang\n' +
      '\n' +
      'Chenfei Wu\n' +
      '\n' +
      'Zekai Zhang\n' +
      '\n' +
      'Mingheng Ni\n' +
      '\n' +
      'Shengming Yin\n' +
      '\n' +
      'Yu Liu\n' +
      '\n' +
      'Zhengyuan Yang\n' +
      '\n' +
      'Lijuan Wang\n' +
      '\n' +
      'Zicheng Liu\n' +
      '\n' +
      'Juntao Li\n' +
      '\n' +
      'Nan Duan\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'LLM을 시각적 합성에 활용하기 위해 전통적인 방법은 래스터 이미지 정보를 특수화된 시각적 모듈을 통해 이산 그리드 토큰으로 변환하는 동시에 시각적 장면의 진정한 의미 표현을 캡처하는 모델의 능력을 방해한다. 본 논문은 영상 정보의 보다 자연스럽고 의미적으로 일관성 있는 분할을 가능하게 함으로써 영상의 대체 표현인 벡터 그래픽이 이러한 한계를 효과적으로 극복할 수 있다고 가정한다. 따라서 우리는 벡터 그래픽의 "스트로크 토큰"이라는 더 나은 시각적 표현을 탐구하는 선구적인 작업인 스트로크뉴와를 소개하며, 이는 본질적으로 시각적 의미학이 풍부하고 LLM과 자연스럽게 호환되며 고도로 압축된다. 스트로크 토큰이 장착된 StrokeNUWA는 벡터 그래픽 생성 태스크에서 다양한 메트릭에 걸쳐 전통적인 LLM 기반 및 최적화 기반 방법을 크게 능가할 수 있다. 또한, StrokeNUWA는 6.9%의 예외적인 SVG 코드 압축률을 갖는 기존 방법의 속도에 대한 추론에서 최대 94%의 속도 향상을 달성하였다.\n' +
      '\n' +
      '머신러닝, ICML, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근, 일반적으로 LLM으로 지칭되는 대형 변압기 기반 언어 모델은 특히 자연 언어 처리(NLP)의 도메인에서 상당한 진전을 이루었다(Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023; Anil et al., 2023). 동시에, LLM은 오디오(Ghosal et al., 2023), 의료(Singhal et al., 2023) 및 로봇 공학(Brohan et al., 2023)과 같은 다른 양식으로 그들의 능력을 점차적으로 확장하고 있다.\n' +
      '\n' +
      '현재의 방법론들(Reddy et al., 2021; Wu et al., 2022; Chang et al., 2022; Kondratyuk et al., 2023)은 VQ-VAE(Van Den Oord et al., 2017) 및 VQ-GAN(Esser et al., 2021)과 같은 특수화된 시각적 모듈들을 통해 연속 시각적 픽셀들을 이산 그리드 토큰들로 변환함으로써 LLM들이 시각적 정보를 생성할 수 있게 한다. 이어서, 이러한 변환된 그리드 토큰들은 LLM에 의해 텍스트 워드 핸들링과 유사한 방식으로 처리되며, 이는 LLM의 생성 모델링 프로세스를 용이하게 한다. 그러나, 확산 모델(Rombach et al., 2022)과 비교할 때, LLMs는 여전히 뒤쳐진다(Lee et al., 2022; Sun et al., 2023). 시각적 작업에서 LLM의 단점은 주로 두 가지 이유로부터 발생한다. 첫째, 변환 프로세스는 본질적으로 한계를 갖는 특정 시각적 모듈에 의존한다. 예를 들어, VQ-GAN(Esser et al., 2021)과 같은 진보된 시각적 모듈들은 아티팩트를 갖는 이미지들의 생성으로 이어질 수 있다(Yu et al., 2023). 둘째, 그리드 토큰들의 사용은 그리드들이 인위적으로 설계되고 본질적으로 의미 인식되지 않기 때문에 시각적 의미론들을 방해할 수 있다. 이러한 인위적 이산화는 시각적 장면의 진정한 의미적 표현을 포착하는 모델의 능력에 제약을 가한다.\n' +
      '\n' +
      '_Is는 시각적 정보의 의미적 무결성을 유지하면서 시각적 정보의 의미적 무결성을 유지하는 시각적 표현이 있으며 프로에 도움이 된다.\n' +
      '\n' +
      '그림 1: "그리드" 토큰의 시각적 표현과 제안된 "스트로크" 토큰 간의 비교. 래스터 이미지에서 픽셀을 토큰화하는 대신 다른 이미지 형식인 확장 가능한 벡터 그래픽(SVG)에서 코드를 토큰화하여 새로운 시각적 표현을 탐구한다. "스트로크" 토큰들은 다음과 같은 장점들을 갖는다: (1) 본질적으로 시각적 의미론을 포함하고, (2) LLM들과 자연스럽게 호환되고, (3) 고도로 압축된다.\n' +
      '\n' +
      'LLMs?__Ms에 의한cesscessing by LLMs?__ 격자 토큰들의 배열이 전형적으로 규칙적이고 균일한 반면, 이미지들 내의 의미 구조는 종종 불규칙하고 복잡하기 때문에, 격자 토큰들의 프레임워크 내에서 그러한 표현을 찾는 것은 자명하지 않다. 도 1에 도시된 바와 같다. 도 1을 참조하면, 돌고래 몸은 임의로 서로 다른 격자형 토큰으로 분할된다. VQ-VAE 방법을 개선하려는 노력(Esser et al., 2021; Yu et al., 2023)이 있었지만, 이들은 래스터 이미지 포맷에 내재된 한계에 의해 근본적으로 제약되어 의미 보존에 병목 현상을 초래한다. 이러한 문제에 비추어, 우리는 벡터 그래픽이라는 대체 이미지 형식을 활용하여 이미지의 의미 개념을 근본적으로 유지하는 새로운 접근법을 제안한다. 픽셀 기반 형식과 달리 벡터 그래픽은 본질적으로 객체의 구성을 드러내어 이미지의 의미 개념을 자연스럽게 캡슐화한다. 예를 들어, 제안된 "스트로크" 토큰은 돌고래를 순차적으로 연결된 스트로크로 분할하는데, 여기서 각 스트로크 단위는 돌고래 지느러미(스트로크 1) 및 후면(스트로크 2)과 같은 완전한 의미 정보를 포함한다.\n' +
      '\n' +
      '우리의 의도는 벡터 그래픽이 래스터 이미지보다 우월하다고 주장하는 것이 아니라 시각적 표현에 대한 새로운 관점을 도입하는 것이라고 언급할 가치가 있다. 우리의 "스트로크" 토큰 개념의 장점은 다음과 같다: (1) 본질적으로 시각적 의미론을 포함한다: 각각의 스트로크 토큰은 본질적으로 시각적 의미론을 포함하며, 이미지 콘텐츠의 보다 직관적인 의미론적 분할을 제공한다; (2) LLM들과 자연스럽게 호환된다: 벡터 그래픽의 생성 프로세스는 자연스럽게 순차적이고 상호연결되며, 이는 LLM들이 정보를 처리하는 방식을 반영한다. 즉, 각각의 스트로크는 그 전후의 것과 관련하여 생성되며, LLM이 보다 자연스럽게 처리할 수 있는 연속적이고 일관된 시퀀스를 확립한다; (3) 고도로 압축된다: 벡터 그래픽에서의 스트로크는 고도로 압축될 수 있고, 각각의 스트로크 토큰이 시각적 정보의 풍부하고 압축된 표현을 캡슐화할 수 있게 하여, 품질 및 의미적 무결성을 유지하면서 데이터 크기를 상당히 감소시킨다.\n' +
      '\n' +
      '이상의 분석을 바탕으로 시각적 모듈에 의존하지 않고 벡터 그래픽을 공예하는 모델인 StrokeNUWA를 소개한다. StrokeNUWA는 VQ-Stroke 모듈과 Encoder-Decoder 모델로 구성된다. 잔차 양자화기 모델 아키텍처(Martinez et al., 2014)에 기초한 VQ-Stroke는 직렬화된 벡터 그래픽 정보를 여러 SVG 토큰으로 압축할 수 있다. 상기 인코더-디코더는\n' +
      '\n' +
      '그림 2: StrokeNUWA에서 생성된 SVG. 각 이미지에 대해 명확성을 위한 부분 키워드를 제공합니다.\n' +
      '\n' +
      '모델은 텍스트 프롬프트에 의해 안내되는 SVG 토큰을 생성하기 위해 미리 훈련된 LLM의 기능을 주로 활용한다.\n' +
      '\n' +
      'SVG(Text-Guided Scalable Vector Graphic) 생성 작업에서 StrokeNUWA와 최적화 기반 방법을 비교한다. 우리의 접근법은 더 높은 CLIPScore(Hessel et al., 2021) 메트릭을 달성하며, 이는 스트로크 토큰을 활용하는 것이 더 풍부한 시각적 의미론을 갖는 콘텐츠를 산출할 수 있음을 시사한다. LLM 기반 기준선에 대해 벤치마킹할 때, 우리의 방법은 모든 메트릭에서 이를 능가하여 스트로크 토큰이 LLM과 효과적으로 통합될 수 있음을 나타낸다. 마지막으로, 벡터 그래픽에 내재된 압축 능력으로 인해, 본 모델은 생성에서 상당한 효율성을 보여 최대 94배의 속도 향상을 달성한다.\n' +
      '\n' +
      '간단히 말해서, 우리의 기여는 다음과 같이 요약될 수 있다:\n' +
      '\n' +
      '* 우리는 특수화된 시각적 모듈에 의존하지 않고 LLM을 통해서만 벡터 그래픽을 합성하기 위해 더 나은 시각적 표현-스트로크 토큰을 탐색하는 선구적인 연구인 StrokeNUWA를 소개한다.\n' +
      '* 벡터 그래픽을 스트로크 토큰으로 압축하도록 설계된 VQ-VAE(Specialized Vector Quantized Variational Autoencoder)인 VQ-Stroke를 제안하여 6.9%의 예외적인 압축률을 제공한다.\n' +
      '* 텍스트 유도 벡터 그래픽 합성 작업에서 스트로크 토큰의 상당한 잠재력을 입증하는 상세한 실험을 수행한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### Visual Representation\n' +
      '\n' +
      '컴퓨터 그래픽의 영역에서, 두 가지 우세한 이미지 포맷이 우세하다: 픽셀 매트릭스를 특징으로 하는 래스터 이미지; 및 일련의 코드 언어 명령을 특징으로 하는 벡터 이미지, a.k.a, Scalable Vector Graphic(SVG). 시각적 합성의 최근 발전은 주로 래스터 이미지의 생성에 초점을 맞추고 있다. 기본 아이디어는 VQ-VAE(Van Den Oord et al., 2017) 및 VQ-GAN(Esser et al., 2021)과 같은 특수화된 시각적 모듈을 통해 연속 이미지 픽셀을 이산 그리드 토큰으로 변환한 다음 LLM을 활용하여 이러한 토큰을 생성하는 것이다(Reddy et al., 2021; Wu et al., 2022; Kondratyuk et al., 2023). 최근에는 Lookup-Free Quantization(Yu et al., 2023), Efficient VQ-GAN(Cao et al., 2023) 등의 고급 아키텍처를 설계하여 "그리드" 토큰을 개선하고자 하는 작업도 있다. 그러나, 이러한 "그리드" 토큰 표현은 격자들이 인위적으로 설계됨에 따라 시각적 의미론을 방해할 수 있으며, 이는 내재적 의미 인식이 부족하고, 교란 및 변조와 같은 시각적 모듈의 내재적 제한의 대상이 되기 쉽다(Hu et al., 2023). 반대로, 우리의 연구는 "스트로크" 토큰의 개념을 제안함으로써 더 나은 시각적 표현을 탐구하는 선구적인 노력이다. \'그리드\' 토큰들과는 달리, "스트로크" 토큰은 본질적으로 강한 의미적 무결성을 제공하는 문맥적으로 연관된 코딩된 언어 명령들에 의해 정의되며, 이는 잠재적으로 전술한 문제들을 완화시킨다.\n' +
      '\n' +
      '### SVG Generation\n' +
      '\n' +
      'SVG 생성은 그래픽을 생성하기 위해 구조화된 코드 생성 방법을 사용하며, 이는 이미지 표현에서 더 나은 해석성, 유연성 및 확장성을 제공한다. SVG 생성의 현재 주류 접근법은 최적화 기반 방법(Su et al., 2023; Jain et al., 2023; Xing et al., 2023)이며, 이는 전통적인 래스터 이미지 생성과 유사성을 공유하며, 미분가능한 래스터라이저를 갖는 타겟 래스터 이미지에 적합하도록 랜덤하게 초기화된 SVG 경로를 반복적으로 정제하는 것을 포함한다(Li et al., 2020). 그러나, 최적화 프로세스는 시간이 많이 소요되고 계산 집약적이므로, 예를 들어 24개의 SVG 경로들로 구성된 SVG 그래픽을 생성하는 것은 20분1을 초과할 수 있다. 대안적으로, 몇몇 최근의 접근법들은 SVG 합성을 위한 코드를 직접 생성하기 위해 자동-회귀 모델들을 채택하기 시작했다(Wang et al., 2022; Wu et al., 2023). 그러나 SVG의 고유한 광범위한 길이 특성과 효과적인 SVG 표현 부족으로 인해 이러한 방법은 LLM을 제한하여 복잡한 SVG를 생성한다. 이러한 문제를 해결하기 위해 VQ-Stroke를 소개하고 "stroke" 토큰의 개념을 제시한다. SVG를 스트로크 토큰으로 변환함으로써, 우리의 접근법은 LLM이 상당히 개선된 추론 속도로 복잡한 SVG를 생성할 수 있게 한다.\n' +
      '\n' +
      '각주 1: 우리는 하나의 NVIDIA V100 GPU 상에서 LIVE(Ma et al., 2022) 및 VectorFusion(Jain et al., 2023)으로 테스트한다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '### Problem Formulation\n' +
      '\n' +
      'SVG 코드는 명령 및 구문 규칙의 세트를 제공하며, 예를 들어 "<rect>" 명령은 그 위치, 폭 및 높이를 갖는 직사각형 형상을 정의하며, 이는 이를 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Name** & **Symbol** & **Argument** & **Example** \\\\ \\hline Move & M & \\((x_{0},y_{0})\\) & \\((x_{1},y_{1})\\) & \\((x_{0},y_{0})\\) \\\\ \\hline Line & L & \\((x_{0},y_{0})\\) & \\((x_{1},y_{1})\\) \\\\ To & L & \\((x_{0},y_{0})\\) & \\((x_{0},y_{0})\\) \\\\ \\hline Cubic & C & \\((x_{0},y_{0})\\) & \\((x_{1},y_{1})\\) \\\\ B\\(\\acute{e}\\)zier & C & \\((x_{0}^{c},c_{0}^{b})\\) & \\((x_{0},y_{0})\\) \\\\  & & \\((c_{1}^{x},c_{1}^{y})\\) & \\((x_{0},y_{0})\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: M, L 및 C를 포함하는 기본 SVG 명령어의 개요로서, 여기서 각 명령어는 하나의 시작점 \\((x_{0},y_{0})\\) 및 하나의 끝점 \\((x_{1},y_{1})\\)을 포함한다. 큐빅 B(\\acute{e}\\)zier 명령어의 경우 두 개의 추가 제어점((c_{0}^{x},c_{0}^{y})과 \\((c_{1}^{x},c_{1}^{y})을 포함한다.\n' +
      '\n' +
      '<rect x="10" y="20" width="50" height="80"/>로 작성된다. 그러나 다수의 SVG 명령 유형을 고려할 때, 이러한 시스템을 만드는 것은 복잡한 데이터 구조를 필요로 할 뿐만 아니라 대규모 데이터 세트가 없으면 LLM은 다양한 범위의 명령을 효과적으로 모델링하는 데 어려움을 겪을 것이다. 따라서 탭에 표시된 대로입니다. 도 1을 참조하면, Iconshop(Wu et al., 2023a)과 DeepSVG(Carlier et al., 2020)에 따라 "Move To", "Line To", "Cubic Bezier"의 세 가지 기본 명령만을 사용하여 각 SVG를 단순화할 수 있다. 예를 들어, "<rect>"와 같은 복잡한 명령들은 그 세 가지 기본 명령들에 의해 구성될 수 있다. 단순화 후, SVG(\\mathcal{G}=\\{\\mathcal{P}_{i}\\}_{i=1}^{N}\\)는 SVG 경로들로 설명될 수 있고, 각각의 SVG 경로\\(\\mathcal{P}_{i}\\)는 기본 명령들\\(M_{i}\\)로 구성된다. \\(\\mathcal{P}_{i}=\\{\\mathcal{C}_{i}}^{j}_{j}=1}^{M_{i}\\), 여기서 \\(\\mathcal{C}_{i}^{j}\\)는 \\(i\\)번째 경로에서 \\(j\\)번째 명령이다. 결국, 각 기본 명령\\(\\mathcal{C}=(T,\\mathcal{V})\\)은 명령 유형\\(T\\in\\texttt{M},\\texttt{L},\\texttt{C}\\})과 대응 위치 인수\\(\\mathcal{V}\\)으로 구성된다.\n' +
      '\n' +
      '### StrokeNUWA\n' +
      '\n' +
      'StrokeNUWA는 SVG 압축을 위한 벡터 양자화-스트로크(VQ-Stroke), SVG 생성을 위한 인코더-디코더 기반 LLM(EDM), 후처리를 위한 SVG 픽서(SF)의 세 가지 핵심 구성 요소를 포함한다. 먼저, VQ-Stroke는 SVG를 스트로크 토큰으로 압축하여 SVG 코드와 이산 스트로크 토큰 사이의 변환을 가능하게 한다. 그런 다음 EDM은 VQ-Stroke에서 생성된 스트로크 토큰을 활용하여 SVG 코드를 생성한다. 마지막으로 SF는 EDM 또는 VQ-Stroke에서 생성된 출력이 SVG 코드의 엄격한 구문 규칙을 항상 준수하지 않을 수 있다는 점을 감안할 때 생성된 SVG의 품질을 개선하도록 설계된 후처리 모듈이다. 아래에서는 각 구성 요소의 세부 사항을 소개하겠습니다.\n' +
      '\n' +
      '###### 3.2.1 벡터 양자화-스트로크\n' +
      '\n' +
      'VQ-Stroke는 SVG 코드를 모델 입력에 적합한 매트릭스 포맷으로 변환하는 "Code to Matrix" 단계와 매트릭스 데이터를 스트로크 토큰으로 변환하는 "Matrix to Token" 단계의 두 가지 주요 단계를 포함한다.\n' +
      '\n' +
      '그림 1에 표시된 행렬에 대한 코드입니다. 도 3을 참조하면, 먼저 단순화된 SVG 코드(Sec. 3.1)를 각 기본 명령어 \\(\\mathcal{C}_{i}^{j}\\)를 규칙 \\(f\\)으로 개별 벡터 \\(\\mathcal{K}_{i}^{j}\\in\\mathbb{R}^{9}\\)로 변환하여 SVG 행렬 형식으로 변환한다:\n' +
      '\n' +
      '[\\mathcal{K}_{i}^{j}=f(\\mathcal{C}_{i}^{j})=\\left(T,x_{0},y_{0},c_{0}^{x},c_{0}^{y},c_{1}^{x},c_{1}^{y},x_{1},y_{1}\\right)_{i}^{j}, \\tag{1}\\right)\n' +
      '\n' +
      '여기서 \\(T\\)은 기본 명령 유형을 나타내고, \\((x_{0},y_{0})\\) 및 \\((x_{1},y_{1})\\)은 시작점과 끝점을 나타내며, \\((c_{0}^{x},c_{0}^{y})\\) 및 \\((c_{1}^{x},c_{1}^{y}))는 각 기본 명령의 제어점으로 사용된다. 그런 다음, 인접한 명령들 간의 상호 연결을 설정하기 위해 각 개별 경로에서 \\(j\\)번째 명령 \\((x_{1},y_{1})_{i}^{j}\\)의 끝점을 후속 \\((x_{0},y_{0})_{i}^{j+1}\\)의 시작점과 동일하게 설정한다.\n' +
      '\n' +
      '그런 다음 SVG\\(\\mathcal{G}\\) 내의 모든 경로를 별개의 기본 명령으로 분해하고 해당 벡터를 행렬 형태로 결합한다.\n' +
      '\n' +
      '(\\mathcal{C}_{i}^{j})\\right)_{i=1}^{N}=\\left(\\left(\\mathcal{C}_{i}}}}\\right)_{j=1}^{M}}\\right)_{i=1}^{N}\\begin{pmatrix}(\\mathcal{K}_{1}^{1};&\\mathcal{K}_{N}^{M}}};&\\mathcal{K}_{N}^{N}}}\\end{pmatrix},\\mathcal{K}_{N}^{N}}}\\mathcal{K}_{N}^{N}}}\\mathcal{K}_{N}^{N}};&\\mathcal{K}_{N}^{N}}}\\vdots&\\vdots\\(\\mathcal{K}_{N}^{2}};&\\mathcal{K}_{N}^{N}}}}}\\\n' +
      '\n' +
      '여기서, ";"는 스택 연산을 나타내고, 각 행렬 행은 개별 명령을 나타낸다. 따라서, 우리는 \\(\\sum_{i=1}^{N}M_{i})\\times 9}\\)의 SVG를 표현하기 위한 구조화된 SVG 행렬 \\(f(\\mathcal{G})\\in\\mathbb{R}^{(\\sum_{i=1}^{N}M_{i})을 얻을 수 있다.\n' +
      '\n' +
      'SVG 행렬 \\(f(\\mathcal{G})\\)을 얻은 후, 우리는 잠재 표현을 통해 행렬을 이산 스트로크 토큰으로 압축하여 \\(f(\\mathcal{G})\\)을 재구성하는 것을 목표로 한다. 도 1에 도시된 바와 같다. 도 3을 참조하면, VQ-Stroke 모델은 Down-Sample 블록, Stroke Codebook \\(\\mathcal{B}\\) 및 Up-Sample 블록으로 구성된다. SVG 행렬 \\(f(\\mathcal{G})\\)은 먼저 다운 샘플 블록들에 의해 인코딩되어 압축된 표현들을 얻는데, 이는 표현 채널들의 수( \\(f(\\mathcal{G})\\)를 증가시키면서 동시에 공간 차원들( \\(f(\\mathcal{G})\\)의 행)을 압축함으로써 보다 컴팩트한 표현, 즉 명령들의 수를 \\(T\\) s.t. \\(T<\\sum_{i=1}^{N}M_{i}\\)으로 압축한다. 그런 다음, 코드북 \\(\\mathcal{B}\\)은 잔류 벡터 양자화와 함께 \\(d\\) 수준의 압축을 동시에 수행함으로써(Martinez et al., 2014), VQ-Stroke가 압축된 rep를 더 잘 모델링할 수 있게 한다.\n' +
      '\n' +
      '도 3: VQ-Stroke의 개요.\n' +
      '\n' +
      '프레젠테이션. 그림 4의 Down-Sample 블록 및 Up-Sample 블록의 세부 아키텍처를 묘사하며, 두 블록 모두 먼저 Conv1d 또는 ConvTranspose1d 모델을 사용하여 특징을 압축하거나 확장하고 ResNet1d 모듈과 추가 Conv1d 모듈에 의해 특징 추출을 성공한다. 낮은 압축률은 VQ-스트로크가 SVG의 미세한 세부사항(첫 번째 및 두 번째 열)을 학습할 수 있도록 하는 반면, 더 공격적인 압축(세 번째 열)은 VQ-스트로크가 SVG의 전체 윤곽을 캡처할 수 있도록 하고, 그림 5에 예시된 바와 같이 낮은 압축률은 VQ-스트로크가 SVG의 미세한 세부사항(첫 번째 및 두 번째 열)을 학습할 수 있도록 하는 반면, 더 공격적인 압축(세 번째 열)은 VQ-스트로크가 SVG의 전체 윤곽을 캡처할 수 있도록 한다. 우리는 Sec. 4.2에서 더 많은 논의를 하고, 마지막으로 Down-Sample 블록들은 코드북에서 출력되는 SVG 잠재표현을 재구성한다.\n' +
      '\n' +
      '이러한 네트워크를 훈련하기 위해 Dhariwal 등을 따라 Equ에서 VQ-Stroke를 공동으로 갱신하기 위해 약속 손실, 코드북 손실 및 재구성 손실을 계산한다. 3:\n' +
      '\n' +
      '\\alpha\\left(\\ell_{codebook}+\\ell_{ commit}\\right)+\\ell_{recon}\\\\&=\\alpha\\left(||\\;\\mathcal{Z}-\\mathrm{sg}[\\mathcal{\\tilde{Z}}]\\;||_{2}^{2}+||\\;\\mathrm{sg}[\\mathcal{Z}]-\\mathcal{\\tilde{Z}\\;||_{2}^{2}}\\mathcal{MSE}(\\widetilde{f(\\mathcal{G})), \\end{split}\\tag{3}\\right}\\mathrm{MSE}(\\widetilde{f(\\mathcal{G})), \\end{split}\\tag{3}\\mathrm{sg}[\\mathcal{z}}\\mmathcal{z}\\mmathcal{f(\\mathcal{G})), \\end{split}\\tag{3}\\mmathrm\n' +
      '\n' +
      '여기서 \\(\\alpha\\)은 하이퍼파라미터, \\(\\mathcal{Z}\\)은 다운샘플 블록에서 압축된 잠재출력, \\(\\mathcal{\\tilde{Z}\\)은 코드북에서 룩업된 잠재출력, \\(\\mathcal{B}\\), \\(\\mathrm{sg}[\\cdot]\\)은 그래디언트 클리핑 연산이다. 또한 훈련 과정을 안정화하기 위해 입력 데이터를 \\([-1,1]\\) 범위로 사전 정규화한다.\n' +
      '\n' +
      '###### 3.2.2 인코더-디코더 기반 LLM\n' +
      '\n' +
      '본 논문에서는 코드북(\\(\\mathcal{B}\\)으로부터 획득한 스트로크 토큰을 예측하기 위해 Encoder-Decoder LLM(EDM)을 사용한다. LLM의 고유한 텍스트 명령어 기능을 고려하여 EDM 인코더를 동결하여 상속된 텍스트 지식을 활용합니다. 이어서, 스트로크 토큰 예측 태스크를 학습하기 위해 EDM 디코더를 미세 조정한다. 스트로크 토큰의 어휘와 원래 LLM의 어휘 사이의 불일치로 인해 추가 스트로크 임베딩 레이어와 스트로크 예측기로 EDM을 확장한다. 그 결과, 학습 가능한 모델 파라미터\\(\\theta\\)와 텍스트 프롬프트\\(\\mathbf{K}\\)이 주어졌을 때, 교차 엔트로피 손실이 있는 로그 확률\\(\\operatorname*{argmax}_{\\theta}\\prod_{i=1}^{T}P(t_{i}\\mid t_{<i},\\mathbf{K})\\)을 최대화한다.\n' +
      '\n' +
      '###### 3.2.3 SVG 고정장치\n' +
      '\n' +
      '에쿼를 보장하지 못하기 때문에 SDM과 EDM의 생성 결과에 중요한 문제가 발생한다. 각 개별 SVG 경로에서 인접한 명령들 간의 상호 연결점의 불일치로 인해 1, 즉 \\(i\\)번째 경로에서 \\((x_{1},y_{1})_{i}^{j}\\neq(x_{0},y_{0})_{i}^{j+1}\\)가 발생한다. 이 문제를 해결하기 위해 SVG Fixer(SF)를 후처리 모듈로 도입하여 생성된 결과를 처리한다. 그것은 경로 클리핑(PC)과 경로 보간(PI)의 두 가지 전략을 포함한다. 구체적으로, PC는 각 SVG 명령의 시작점을 인접한 SVG 명령의 끝점으로 직접 치환한다: \\((x_{0},y_{0})_{i}^{j+1}:=(x_{1},y_{1})_{i}^{j}\\). 한편, PI는 인접한 각 쌍의 SVG 명령어들 사이에 \\(\\mathbb{M}\\)의 추가를 수반한다. 즉, \\((x_{1},y_{1})_{i}^{j}\\neq(x_{0},y_{0})_{i}^{j+1}\\implies\\)에 추가 명령 \\(\\left(\\mathbb{M},(x_{1},y_{1})_{i}^{j},0,0,0,(x_{0},y_{0})_{i}^{j+1}\\implies\\)을 추가하여 이전 명령을 강제하고 다음 인접 명령의 시작 지점으로 이동한다. 도 1에 도시된 바와 같다. 도 5를 참조하면, PC는 SVG의 전체 경로를 간소화하여 보다 간결하게 만들 수 있지만 세부 사항에서 일부 부정확성을 초래할 수 있다. 반면에 PI는 더 많이 생성된 스톡스의 세부 사항을 드러내는 경향이 있지만 더 많은 곡선을 도입할 수 있다. 각 전략에는 고유한 적용 가능한 시나리오가 있습니다.\n' +
      '\n' +
      '도 4: 다운샘플 및 업샘플 블록의 아키텍처.\n' +
      '\n' +
      '그림 5: SVG 재구성의 분석이며, 여기서 \\(C\\)는 PI 설정에서 삽입된 <H> 명령의 수를 나타내는 상수이다. SVG 구성의 명확한 관찰을 용이하게 하기 위해 각 기본 명령을 뚜렷한 색상으로 나타낸다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      'SF, PI를 사용한 스트로크는 가장 낮은 CLIP 점수에 의해 입증된 바와 같이 가장 낮은 FID 점수를 달성하고 주어진 텍스트 프롬프트와 더 높은 일치를 입증함으로써 원래 SVG 그래픽의 더 충실한 근사를 용이하게 한다. 대조적으로, PC 방법은 가장 낮은 EDIT 점수를 달성하기 때문에 원래의 SVG 코드와 더 나은 정렬 결과를 산출한다. 압축 레벨 2(C-4), VQ 스트로크는 6.9%의 주목할 만한 압축 비율(CR)을 달성하여 비교 가능한 CLIPS코어 및 FID에 의해 입증된 바와 같이 C-2와 동등한 성능을 유지한다. 이는 VQ-Stroke가 실질적인 경로 압축에도 불구하고 원래의 SVG 그래픽의 의미적 무결성을 보존함을 시사한다.\n' +
      '\n' +
      '표 2에 예시된 StrokeNUWAAs, StrokeNUWA는 우수한 결과를 달성함으로써 다른 방법들보다 우수하다. 구체적으로, 시각적 성능 측면에서, StrokeNUWA는 가장 낮은 FID 점수(6.513) 및 가장 높은 HPS(16.801)에 의해 나타난 골든 SVG와 더 밀접하게 유사한 그래픽을 생성할 수 있다. 이것은 우리의 스트로크 토큰이 바닐라 접근법(아이콘샵)보다 LLM과 더 큰 호환성을 제공한다는 것을 나타낸다. 또한 StrokeNUWA는 가장 높은 CLIPScore(17.994)를 달성하여 최적화 기반 방법을 능가했다. 이는 StrokeTokens가 시각적 의미론을 효과적으로 캡슐화함을 시사한다. SVG 코드의 품질과 생성 효율 측면에서 스트로크 토큰은 골든 표준과 밀접하게 정렬될 뿐만 아니라 생성 속도, 즉 StrokeNUWA V.S의 약 19초, 즉 최적화 기반 방법 LIVE의 약 30분 정도를 현저하게 향상시킨다. 이것은 원래 SVG 코드에서 스트로크 토큰의 인상적인 압축 기능을 강조하여 압축의 효율성과 품질을 모두 보여줍니다.\n' +
      '\n' +
      '### Qualitative Evaluation\n' +
      '\n' +
      '사례 연구 우리는 그림에서 다양한 복잡도 수준을 가진 VQ 스트로크의 재구성 결과를 보여준다. 도 7 및 그림 1의 StrokeNUWA 및 기타 기준선 간의 정성적 비교를 제시한다. 8(a) VQ 스트로크가\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c c|c} \\hline \\hline \\multirow{2}{*}{Methods} & \\multicolumn{3}{c|}{Visual Performance} & \\multicolumn{3}{c|}{SVG Code Quality} & Generation \\\\ \\cline{2-7}  & FID (\\(\\downarrow\\)) & CLIPScore (\\(\\uparrow\\)) & HPS (\\(\\uparrow\\)) & \\begin{tabular}{c} Recall (\\(\\uparrow\\)) \\\\ (Stroke Token) \\\\ \\end{tabular} & EDIT (\\(\\downarrow\\)) & \\begin{tabular}{c} Optim / Pred \\\\ Length (\\(Avg\\)) \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} Speed (\\(\\downarrow\\)) \\\\ (_per_ SVG) \\\\ \\end{tabular} \\\\ \\hline SD \\& LIVE & 14.236 & 12.908 & 11.210 & 0.028 & - & 160 (32 Path) & \\(\\approx 28.0\\) min \\\\ VectorFusion & 7.754 & 17.539 & 15.901 & 0.079 & - & 2,048 (128 Path) & \\(\\approx 30.0\\) min \\\\ Iconshop & 17.828 & 8.402 & 8.234 & 0.114 & 24,792.476 & 993.244 & \\(\\approx\\) 63.743 sec \\\\ \\hline SVGNUWA (PC) & 6.607 & 17.852 & 16.134 & **0.239** & **9,092.476** & 271.420 & \\(\\approx\\) **19.128** sec \\\\ SVGNUWA (PI) & **6.513** & **17.994** & **16.801** & 0.207 & 12,249.091 & 271.420 & \\(\\approx\\) **19.128** sec \\\\ \\hline Golden SVG & - & - & - & 100\\% & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: StrokeNUWA의 성능, 여기서 "Optim/Pred Length"는 실제 예측된 또는 최적화된 경로 수를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c} \\hline \\hline Methods & FID (\\(\\downarrow\\)) & CLIPScore (\\(\\uparrow\\)) & EDIT (\\(\\downarrow\\)) & CR (\\(\\downarrow\\)) \\\\ \\hline SQM (C-2) & - & - & 1,114.791 & 8.549\\% \\\\ SQM (C-2) + SF (PC) & 3.751 & 19.861 & **1,996.313** & 8.786\\% \\\\ SQM (C-2) + SF (PD) & **3.518** & **20.290** & 1,315.137 & 13.780\\% \\\\ SQM (C-4) + SF (FD) & 4.943 & 17.192 & 2,100.671 & **6.890\\%** \\\\ \\hline Golden SVG & - & - & - & 100\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: SVG 재구성 작업에 대한 VQ-스트로크의 성능, 여기서 C-2 및 C-4는 압축률 2 및 4를 나타낸다.\n' +
      '\n' +
      '그림 8: SVG 생성 작업에서 다른 모델에서 샘플링된 케이스이며, 여기서 CLIPS코어는 각 방법에 대해 생성된 4개의 케이스에 걸쳐 계산된 평균 점수이다.\n' +
      '\n' +
      '그림 7: 차이 VQ-스트록의 재구성 성능.\n' +
      '\n' +
      '코드북 크기가 4,096에 불과한 복잡한 SVG를 복원합니다. 그 다음, 압축률 2(CR-2)에서, VQ-Stroke는 그래픽들 내의 객체들의 에지를 성공적으로 개요화하여, 스트로크 토큰들이 조밀한 표현으로 고도로 압축될 수 있고 본질적으로 시맨틱 세분화를 통합할 수 있음을 입증하며, 이는 시각적 시맨틱을 유지하는 데 필수적이다. StrokeNUWA의 비교와 관련하여 LLM 기반 생성 방법을 사용하면 불완전한 SVG(Iconshop)가 발생할 수 있다는 점에 주목한다. 이는 과도한 SVG 코드 길이와 LLM이 SVG 그래픽에 내장된 주요 정보를 캡처하는 데 어려움을 겪었기 때문이다. 그러나 스트로크 토큰의 사용은 경로를 압축하고 LLM과 호환함으로써 이러한 문제를 완화할 수 있다. 또한, 최적화 기반 방법의 성능은 안정적인 확산 모델에 의해 생성된 출력에 크게 의존한다는 것을 발견했으며, 이는 Sec에서 언급된 그리드 토큰의 제한에 따른다. 도 1을 참조하면, 예를 들어, 시각적 의미들을 캡처하는 것이 어렵고 텍스트 프롬프트와 정렬되지 않는 여분의 시각적 정보를 생성하는 경향이 있다. 또한 최적화 프로세스가 매우 느립니다. 대조적으로, 스트로크 토큰을 이용하는 StrokeNUWA는 본질적으로 시각적 시맨틱 세분화를 포함한다. 결과적으로, 생성된 콘텐츠는 텍스트 시맨틱스에 더 정렬되어, 보다 일관성 있고 의미적으로 정확한 그래픽을 제공한다.\n' +
      '\n' +
      '또한, StrokeNUWA에서 생성된 SVG 출력을 LLM 기반 방법인 아이콘샵으로 생성된 출력과 비교하기 위해 인간 평가를 수행한다. 우리는 50개의 서로 다른 텍스트 프롬프트를 선택하고 평가를 위해 해당 SVG를 생성하도록 모델을 안내한다. 그림 9에 묘사된 바와 같이, 우리의 비교는 프롬프트 정렬(생성된 결과와 텍스트 프롬프트 간의 일관성), 전체 품질(SVG의 일반 등급) 및 그래픽 세부 정보(곡선과 같은 복잡성)의 세 가지 기준에 기초한다. 본 논문에서는 StrokeNUWA(StrokeNUWA)가 시각적 표현으로 간주하는 Iconshop에 비해 더 완전한 콘텐츠(전체 품질)를 얻을 뿐만 아니라 텍스트 프롬프트(더 나은 프롬프트 정렬)4와 더 밀접하게 정렬된 결과를 생성한다는 것을 관찰한다. 스트로크 토큰이 SVG의 세부 정보를 압축한다는 점을 고려할 때, StrokeNUWA(StrokeNUWA)가 그래픽 세부 정보를 생성하는 데 탁월하다는 것은 당연하다.\n' +
      '\n' +
      '각주 4: 아이콘샵에서 프롬프트 정렬이 낮은 주요 이유는 생성된 SVG의 불완전성 때문이기도 하다.\n' +
      '\n' +
      '## 5 Abablation Study\n' +
      '\n' +
      'VQ-스트로크 모델 구조의### 분석\n' +
      '\n' +
      'VQ-Stroke 구조 구성이 스트로크 토큰 성능에 미치는 영향을 조사하기 위해 코드북 크기\\(|\\)\\(\\mathcal{B}\\)\\(|\\)와 코드북 크기\\(\\mathrm{Dim}\\)을 달리하여 실험하였다. 탭에 표시된 대로입니다. 도 4를 참조하면, 각 스트로크 토큰의 차원을 줄이는 동시에 코드북 크기를 증가시킴으로써 VQ-Stroke가 여러 메트릭에 걸쳐 우수한 성능을 달성함을 알 수 있다. 그림 1의 변화 추세를 보여주기 위해 재구성 사례 세트를 샘플링한다. 도 7에 도시된 바와 같이, 이는 더 큰 코드북 크기 및 더 작은 차원으로, VQ-스트로크가 더 큰 정확도, 예를 들어, 더 정확한 라인들로 세부사항들을 묘사할 수 있음을 나타낸다.\n' +
      '\n' +
      'GPT-4의### 비교\n' +
      '\n' +
      '본 논문에서는 GPT-4의 생성 결과를 GPT-4(Achiam et al., 2023)와 비교하여, GPT-4를 유도하는 템플릿을 사용하여 해당 SVG 코드를 생성한다:{KEYWORDS}를 기반으로 아이콘 스타일로 SVG 코드를 생성한다. 그림 1에서 렌더링된 SVG를 보여준다. 8(b)에서 GPT-4는 LLM 기반 방법과 일치하는 간단한 SVG만 생성할 수 있음을 관찰할 수 있다. 더욱이, GPT-4는 종종 관련 텍스트와 일치하지 않는 SVG를 생성한다.\n' +
      '\n' +
      '##6 결론 및 향후 과제\n' +
      '\n' +
      '본 논문은 벡터 그래픽을 통해 이미지를 표현하는 대체 방법으로 우수한 시각적 표현인 "스트로크" 토큰을 탐구하는 선구적인 연구인 StrokeNUWA를 제시한다. 스트로크 토큰은 이미지의 의미 무결성을 보존할 뿐만 아니라 LLM에 의한 처리에 도움이 된다. 더욱이, 벡터 그래픽에서의 스트로크들은 고도로 압축될 수 있다. 실험은 스트로크 토큰이 장착된 LLM이 SVG 합성 작업에서 다양한 메트릭에 걸쳐 우수한 결과를 얻을 수 있음을 나타낸다. 본 논문은 벡터 그래픽 합성 분야에서 스트로크 토큰 표현의 엄청난 잠재력을 보여준다. 앞으로 LLM에 맞춘 고급 비주얼 토큰화 방법을 통해 스트로크 토큰의 품질을 지속적으로 향상시키는 것을 목표로 합니다. 또한, 스트로크 토큰 활용을 보다 광범위한 태스크(SVG Understanding), 도메인(domain, 3D) 및 실제 세계에서 조달된 이미지에 대한 SVG의 생성으로 일반화하고자 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c} \\hline \\hline \\multicolumn{2}{c|}{Settings} & \\multicolumn{1}{c}{FID (\\(\\downarrow\\))} & \\multicolumn{1}{c}{CLIPScore (\\(\\uparrow\\))} & EDIT (\\(\\downarrow\\)) \\\\ \\hline \\(|\\)\\(\\mathcal{B}\\)\\(|\\) & Dim & & & \\\\ \\hline\n' +
      '2048 & 512 & 5.702 & 19.365 & 2,323.810\\\\\n' +
      '4096 & 512 & 3.518 & 20.290 & 1,315.137 \\\\\n' +
      '4096 & 1024 & 3.901 & 20.159 & 1,793.008 \\\\\n' +
      '8192 & 512 & **2.639** & **21.014** & **907.106** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 상이한 VQ-스트로크 설정들 간의 비교.\n' +
      '\n' +
      '그림 9: StrokeNUWA 및 LLM 기반 방법-Iconshop 간의 인간 평가.\n' +
      '\n' +
      '## Impact Statement\n' +
      '\n' +
      '이 작업의 의미는 다양하며 잠재적으로 이미지, 벡터 그래픽의 다른 형식에서 시각적 합성에 혁명을 일으킬 수 있다. 스트로크 토큰이 시각적 표현과 LLM의 상호 작용을 개선함에 따라 LLM을 위해 설계된 시각적 토큰화 기술의 향후 발전이 예상된다. 앞으로 커뮤니티는 스트로크 토큰 적용을 실제 세계의 이미지에 대한 SVG 이해 및 개방형 도메인 SVG 합성을 포함하여 더 넓은 작업 및 영역으로 확장할 수 있다. 우리가 이 초기 분야를 개척함에 따라, 우리는 기계 학습과 그래픽 표현의 그러한 발전이 갖는 심오한 사회적 영향을 의식하고 있다. 자동화된 그래픽 디자인, 확장 가능한 벡터 그래픽 제작 및 향상된 디지털 아리시에 대한 기능은 시각적 콘텐츠에 의존하는 산업의 상당한 변화를 예고한다. 우리의 작업은 예술적 표현과 시각적 소통을 위한 새로운 경로를 만들어 과학계에 기여할 뿐만 아니라 창조적, 기술적, 교육적 분야의 변화를 촉매하는 것이다. 우리는 우리의 작업의 중요성과 현장에 대한 우리의 기여가 윤리적으로 수행되도록 하는 우리의 책임을 인식하고 사회 전체에 이익을 주고 시각적 지형을 민주화하고 책임 있고 신중한 혁신을 통해 그것을 풍요롭게 하는 것을 목표로 한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 기술 보고서. ArXiv:2303.08774. 인용: SS1.\n' +
      '*R. Anil et al. (2023)Plam 2 기술 보고서. ArXiv:2305.10403. 인용: SS1.\n' +
      '* A. Brohan et al. (2023)Carbajal, J., Chebotar, X., Choromanski, T., Ding, Dubes, C., et al. Rt-2: vision-language-action models transfer web knowledge to robotic control. ArXiv:2307.15818. 인용: SS1.\n' +
      '*T. Brown, B. Mann, N., Subbiah, J. D., P. Dhariwal, A. Neelakantan, P. Shyam, G., Askell, et al. (2020)Language models is few-shot learners. neural information processing systems33, pp. 1877-1901. Cited by: SS1.\n' +
      '* S. 조영 음락 황영 류진 자오, K Huang (2023)Efficient-vqgan: 효율적인 비전 트랜스포머를 사용하여 고해상도 이미지 생성을 지향한다. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7368-7377. Cited by: SS1.\n' +
      '* A. Carlier, M. 다넬잔, A. 알라히, R. Timofte(2020)Deepsvg: 벡터 그래픽 애니메이션을 위한 계층적 생성 네트워크. Advances in Neural Information Processing Systems33, pp. 16351-16361. Cited by: SS1.\n' +
      '* H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman(2022)MaskGit: masked generatingative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11315-11325. Cited by: SS1.\n' +
      '* A. Chowdhery, S. 나랑, J 데블린, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. (2022)Palm: scaling language modeling with pathways. ArXiv:2204.02311. 인용: SS1.\n' +
      '*H.W.정룡 허승 롱프레, B. 조프, Y 테이원 페더스, E. 리, X. 왕민 데하니, S. Brahma, et al.(2022)Scaling instruction-finetuned language models. ArXiv:2210.11416. 인용: SS1.\n' +
      '* L. Clouatre와 M Demers(2019) Figr: 파충류를 이용한 수 샷 이미지 생성. ArXiv:1901.02199. 인용: SS1.\n' +
      '* P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever(2020)Jukebox: a generative model for music. ArXiv:2005.00341. 인용: SS1.\n' +
      '* P. Esser, R. Rombach, and B. Ommer(2021)Taming transformer for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873-12883. Cited by: SS1.\n' +
      '* D. Ghosal, N. Majumder, A. Mehrish, S. Poria (2023) Text-to-audio 생성은 명령어 조정 llm과 잠재 확산 모델을 이용한다. ArXiv:2304.13731. 인용: SS1.\n' +
      '* J. Hessel, A. Holtzman, M. 포브스, R. L. 브라스, Y. Choi(2021)Clipscore: 이미지 캡셔닝을 위한 참조 없는 평가 메트릭. ArXiv:2104.08718. 인용: SS1.\n' +
      '* M. 허셀, H. 램샤워, T. Unterthiner, B. Nessler, S. 2개의 시간-스케일 업데이트 규칙에 의해 트레이닝된 Hochreiter(2017)Gans는 로컬 내쉬 평형에 수렴한다. 신경 정보 처리 시스템들(30. 인용: SS1).\n' +
      '*Q. 허기장 진영 Cai, G. Yu, G. Y. Li (2023)Robust semantic communications with masked vq-vae enabled codebook. IEEE Transactions on Wireless Communications 인용: SS1.\n' +
      '* A. Jain, A. Xie, and P. Abbeel(2023)Vectorfusion: text-to-svg by abstracting pixel-based diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1911-1920. Cited by: SS1.\n' +
      '* D. Kondratyuk, L. 유진 구재자마 호눙, H. Adam, H. Akbari, Y. Alon, and V., et al. Videoopoet: A large language model for zero-shot video generation. _ arXiv preprint arXiv:2312.14125_, 2023.\n' +
      '* Lee et al. (2022) Lee, D., Kim, C., Kim, S., Cho, M., and HAN, W. S. Draft-and-revise: effective image generation with context rq-transformer. _ 신경 정보 처리 시스템_, 35:30127-30138, 2022에서의 발전.\n' +
      '* Li 등(2020) Li, T. -M., Lukac, M., Gharbi, M., and Ragan-Kelley, J. Differentiable Vector Graphics rasterization for editing and learning. _ ACM Transactions on Graphics (TOG)_, 39(6):1-15, 2020.\n' +
      '* Ma et al. (2022) Ma, X., Zhou, Y., Xu, X., Sun, B., Filev, V., Orlov, N., Fu, Y., and Shi, H. Towards layer-wise image vectorization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 16314-16323, 2022.\n' +
      '* Martinez et al. (2014) Martinez, J., Hoos, H. H., and Little, J. J. Stacked quantizers for compositional vector compression. _ arXiv preprint arXiv:1411.2173_, 2014.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* Rajbhandari et al. (2020) Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. 제로: 조 단위 매개변수 모델을 훈련하기 위한 메모리 최적화 In _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pp. 1-16. IEEE, 2020.\n' +
      '* Reddy et al. (2021) Reddy, M. D. M., Basha, M. S. M., Hari, M. M. C., and Penchalaiah, M. N. Dall-e: Creating images from text. _ UGC Care Group I Journal_, 8(14):71-75, 2021.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.\n' +
      '* Singhal et al. (2023) Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Hou, L., Clark, K., Pfohl, S., Cole-Lewis, H., Neal, D., et al. arXiv preprint arXiv:2305.09617_, 2023.\n' +
      '* Su et al. (2023) Su, H., Liu, X., Niu, J., Cui, J., Wan, J., Wu, X., and Wang, N. 마블: 원시-wise 심층 강화 학습을 통한 래스터 그레이-레벨 만화 벡터화 _ IEEE Transactions on Circuits and Systems for Video Technology_, 2023.\n' +
      '*Sun et al. (2023) Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., and Wang, X. 다중 모달리티의 생성적 사전 훈련 arXiv preprint arXiv:2307.05222_, 2023.\n' +
      '* Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. - A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* Van Den Oord et al. (2017) Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Wang et al. (2022) Wang, Y., Pu, G., Luo, W., Wang, Y., Xiong, P., Kang, H., and Lian, Z. 내용 인식 레이아웃 추론을 통한 미적 텍스트 로고 합성 In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 2436-2445, 2022.\n' +
      '* Wu et al. (2022) Wu, C., Liang, J., Ji, L., Yang, F., Fang, Y., Jiang, D., and Duan, N. 뉴바: 신경 시각 세계 생성을 위한 시각 합성 사전 훈련. _European conference on computer vision_, pp. 720-736. Springer, 2022.\n' +
      '* Wu et al. (2023a) Wu, R., Su, W., Ma, K., and Liao, J. Iconshop: Text-guided vector icon synthesis with autoregressive transformer. _ ACM Transactions on Graphics (TOG)_, 42(6):1-14, 2023a.\n' +
      '* Wu et al. (2023b) Wu, X., Sun, K., Zhu, F., Zhao, R., and Li, H. Human preference score: text-to-image models with human preference. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 2096-2105, 2023b.\n' +
      '* Xing et al. (2023) Xing, X., Zhou, H., Wang, C., Zhang, J., Xu, D., and Yu, Q. Svgdreamer: Text guided svg generation with diffusion model. _ arXiv preprint arXiv:2312.16476_, 2023.\n' +
      '* Yu et al. (2023) Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A. G., et al. Language model beats diffusion-tokenizer is key to visual generation. _ arXiv preprint arXiv:2310.05737_, 2023.\n' +
      '* Zhang et al. (2023) Zhang, T., Liu, H., Zhang, P., Cheng, Y., and Wang, H. Beyond pixels: Exploring human readable svg generation for simple images with vision language models. _ arXiv preprint arXiv:2311.15543_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>