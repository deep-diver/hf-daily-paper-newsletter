<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis\n' +
      '\n' +
      'Zecheng Tang\n' +
      '\n' +
      'Chenfei Wu\n' +
      '\n' +
      'Zekai Zhang\n' +
      '\n' +
      'Mingheng Ni\n' +
      '\n' +
      'Shengming Yin\n' +
      '\n' +
      'Yu Liu\n' +
      '\n' +
      'Zhengyuan Yang\n' +
      '\n' +
      'Lijuan Wang\n' +
      '\n' +
      'Zicheng Liu\n' +
      '\n' +
      'Juntao Li\n' +
      '\n' +
      'Nan Duan\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'To leverage LLMs for visual synthesis, traditional methods convert raster image information into discrete grid tokens through specialized visual modules, while disrupting the model\'s ability to capture the true semantic representation of visual scenes. This paper posits that an alternative representation of images, vector graphics, can effectively surmount this limitation by enabling a more natural and semantically coherent segmentation of the image information. Thus, we introduce StrokeNUWA, a pioneering work exploring a better visual representation -- "stroke tokens" on vector graphics, which is inherently visual semantics rich, naturally compatible with LLMs, and highly compressed. Equipped with stroke tokens, StrokeNUWA can significantly surpass traditional LLM-based and optimization-based methods across various metrics in the vector graphic generation task. Besides, StrokeNUWA achieves up to a \\(94\\times\\) speedup in inference over the speed of prior methods with an exceptional SVG code compression ratio of 6.9%.\n' +
      '\n' +
      'Machine Learning, ICML, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'In recent years, Large transformer-based Language Models, commonly referred as LLMs, have made significant strides, particularly in the domain of Natural Language Processing (NLP) (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023; Anil et al., 2023). Concurrently, LLMs are gradually expanding their capabilities to other modalities, such as audio (Ghosal et al., 2023), medical (Singhal et al., 2023) and robotics (Brohan et al., 2023).\n' +
      '\n' +
      'Current methodologies (Reddy et al., 2021; Wu et al., 2022; Chang et al., 2022; Kondratyuk et al., 2023) enable LLMs to generate visual information by transforming the continuous visual pixels into to discrete grid tokens via specialized visual modules such as VQ-VAE (Van Den Oord et al., 2017) and VQ-GAN (Esser et al., 2021). Subsequently, these transformed grid tokens are processed by the LLM in a manner akin to textual word handling, which facilitates LLMs\' generative modeling process. However, when compared with diffusion models (Rombach et al., 2022), LLMs still fall behind (Lee et al., 2022; Sun et al., 2023). The shortcomings of LLMs in visual tasks primarily arise from two reasons: First, the transformation process relies on specific visual modules, which inherently possess limitations. For instance, advanced visual modules like VQ-GAN (Esser et al., 2021) can lead to the generation of images with artifact (Yu et al., 2023); Second, the use of grid tokens can disrupt the visual semantics, as the grids are artificially designed and not inherently semantic-aware. This artificial discretization imposes constraints on the model\'s ability to capture the true semantic representation of visual scenes.\n' +
      '\n' +
      '_Is there a visual representation that preserves the semantic integrity of visual information while being conducive to pro\n' +
      '\n' +
      'Figure 1: Comparison between the visual representation of “grid” token and our proposed “stroke” token. Instead of tokenizing pixels from raster images, we explore a novel visual representation by tokenizing codes, from another image format—Scalable Vector Graphic (SVG). “Stroke” tokens have the following advantages: (1) inherently contain visual semantics, (2) naturally compatible with LLMs, and (3) highly compressed.\n' +
      '\n' +
      'cessing by LLMs?_ Finding such a representation within the framework of grid tokens is non-trivial, as the arrangement of grid tokens is typically regular and uniform, whereas the semantic structure within images is often irregular and complex. As illustrated in Fig. 1, the dolphin\'s body is arbitrarily segmented into different grid tokens. Although there have been efforts to improve the VQ-VAE method (Esser et al., 2021; Yu et al., 2023), enhancing the visual representation quality, they are fundamentally constrained by the limitations inherent to raster image formats, leading to bottlenecks in semantic preservation. In light of these challenges, we propose a novel approach that fundamentally retains the semantic concepts of images by utilizing an alternative image format: vector graphics. Different from pixel-based formats, vector graphics intrinsically reveal the construction of objects, naturally encapsulating the semantic concepts of the image. For example, our proposed "stroke" tokens segment the dolphin into sequentially connected strokes, where each stroke unit contains complete semantic information, such as the dolphin\'s fin (stroke 1) and back (stroke 2).\n' +
      '\n' +
      'It is worth mentioning that our intention is not to claim that vector graphics are superior to raster images, but rather to introduce a fresh perspective on visual representation. The advantages of our "stroke" token concept include: (1) Inherently contains visual semantics: each stroke token intrinsically contains visual semantics, offering a more intuitive semantic segmentation of the image content; (2) Naturally compatible with LLMs: the creation process of vector graphics is naturally sequential and interconnected, which mirrors the way LLMs process information. In other words, Each stroke is created in relation to the ones before and after it, establishing a contiguous and coherent sequence that LLMs can process more naturally; (3) Highly compressed: strokes in vector graphics can be highly compressed, allowing each stroke token to encapsulate a rich, compressed representation of the visual information, significantly reducing the data size while maintaining quality and semantic integrity.\n' +
      '\n' +
      'Based on the above analysis, we introduce StrokeNUWA, a model that crafts vector graphics without the reliance on the visual module. StrokeNUWA consists of a VQ-Stroke module and an Encoder-Decoder model. The VQ-Stroke, based on the residual quantizer model architecture (Martinez et al., 2014), can compress serialized vector graphic information into several SVG tokens. The Encoder-Decoder\n' +
      '\n' +
      'Figure 2: SVG generated by StrokeNUWA. For each image, we provide partial keywords for clarity.\n' +
      '\n' +
      'model primarily utilizes the capabilities of a pre-trained LLM to generate SVG tokens guided by text prompts.\n' +
      '\n' +
      'We compare StrokeNUWA with optimization-based methods in the text-guided Scalable Vector Graphic (SVG) generation task. Our approach achieves higher CLIPScore (Hessel et al., 2021) metrics, suggesting that utilizing stroke tokens can yield content with richer visual semantics. When benchmarked against LLM-based baselines, our method surpasses them across all metrics, indicating that stroke tokens can integrate effectively with LLMs. Finally, due to the compression capabilities inherent in vector graphics, our model demonstrates significant efficiency in generation, achieving speed improvements of up to 94 times.\n' +
      '\n' +
      'In a nutshell, our contributions can be outlined as follows:\n' +
      '\n' +
      '* We introduce StrokeNUWA, the pioneering study exploring a better visual representation--stroke token, to synthesize vector graphics solely through LLMs without relying on specialized visual modules.\n' +
      '* We propose VQ-Stroke, a specialized Vector Quantized Variational Autoencoder (VQ-VAE) designed to compress vector graphics into stroke tokens, providing an exceptional compression ratio of 6.9%.\n' +
      '* We conduct detailed experiments that demonstrate the significant potential of stroke tokens in the text-guided vector graphic synthesis task.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Visual Representation\n' +
      '\n' +
      'In the realm of computer graphics, two predominant image formats prevail: raster images, characterized by pixel matrices; and vector images, a.k.a, Scalable Vector Graphic (SVG), characterized by a series of code language commands (Zhang et al., 2023). Recent developments in visual synthesis have predominantly centered on the generation of raster images. The basic idea is to transform the continuous image pixels into discrete grid tokens via specialized visual modules such as VQ-VAE (Van Den Oord et al., 2017) and VQ-GAN (Esser et al., 2021), and then leverage LLMs to generate these tokens (Reddy et al., 2021; Wu et al., 2022; Kondratyuk et al., 2023). Most recently, some works have tried to improve "grid" tokens by designing advanced architectures such as Lookup-Free Quantization (Yu et al., 2023) and Efficient VQ-GAN (Cao et al., 2023). However, these "grid" token representations can disrupt visual semantics as the grids are artificially designed, which lacks inherent semantic awareness, and are easily subject to the visual module\'s intrinsic limitations like disturbances and tampering (Hu et al., 2023). Conversely, our study is a pioneering effort exploring a better visual representation by proposing the concept of the "stroke" token. Different from the \'grid" tokens, the "stroke" token is inherently defined by contextually associated coded language commands that offer strong semantic integrity, potentially mitigating the aforementioned issues.\n' +
      '\n' +
      '### SVG Generation\n' +
      '\n' +
      'SVG generation employs a method of structured code generation for producing graphics, which offers better interpretability, flexibility, and scalability in image representation. The current mainstream approach of SVG generation is optimization-based methods (Su et al., 2023; Jain et al., 2023; Xing et al., 2023), which share a similarity with traditional raster image generation, involving iteratively refining randomly initialized SVG paths to fit a target raster image with a differentiable rasterizer (Li et al., 2020). However, the optimization process is both time-consuming and computationally intensive, e.g., creating an SVG graphic comprised of 24 SVG paths can exceed 20 minutes1. Alternatively, some recent approaches have begun to adopt auto-regressive models to directly generate code for SVG synthesis (Wang et al., 2022; Wu et al., 2023). However, due to the inherent extensive length nature of SVGs and a lack of effective SVG representation, these methods constrain LLMs to generate complex SVGs. To address these challenges, we introduce VQ-Stroke and present the concept of "stroke" tokens. By transforming SVGs into stroke tokens, our approach enables LLMs to produce intricate SVGs with significantly improved inference speed.\n' +
      '\n' +
      'Footnote 1: We test with LIVE (Ma et al., 2022) and VectorFusion (Jain et al., 2023) on one NVIDIA V100 GPU.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '### Problem Formulation\n' +
      '\n' +
      'SVG code provides a suite of command and syntax rules, e.g., the "<rect>" command defines a rectangle shape with its position, width, and height, which can\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Name** & **Symbol** & **Argument** & **Example** \\\\ \\hline Move & M & \\((x_{0},y_{0})\\) & \\((x_{1},y_{1})\\) & \\((x_{0},y_{0})\\) \\\\ \\hline Line & L & \\((x_{0},y_{0})\\) & \\((x_{1},y_{1})\\) \\\\ To & L & \\((x_{0},y_{0})\\) & \\((x_{0},y_{0})\\) \\\\ \\hline Cubic & C & \\((x_{0},y_{0})\\) & \\((x_{1},y_{1})\\) \\\\ B\\(\\acute{e}\\)zier & C & \\((x_{0}^{c},c_{0}^{b})\\) & \\((x_{0},y_{0})\\) \\\\  & & \\((c_{1}^{x},c_{1}^{y})\\) & \\((x_{0},y_{0})\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Overview of basic SVG commands, including M, L, and C, where each command contains one beginning point \\((x_{0},y_{0})\\) and one end point \\((x_{1},y_{1})\\). For Cubic B\\(\\acute{e}\\)zier command, it contains two extra control points \\((c_{0}^{x},c_{0}^{y})\\) and \\((c_{1}^{x},c_{1}^{y})\\).\n' +
      '\n' +
      'be written as <rect x="10" y="20" width="50" height="80"/>. However, considering the multitude of SVG command types, creating such a system not only requires a complex data structure, but without a massive dataset, LLMs would struggle to model the diverse range of commands effectively. Therefore, as shown in Tab. 1, we can simplify each SVG using just three basic commands: "Move To", "Line To", and "Cubic Bezier" by following Iconshop (Wu et al., 2023a) and DeepSVG (Carlier et al., 2020). For instance, intricate commands like "<rect>" can be constructed by those three basic commands. After simplification, an SVG \\(\\mathcal{G}=\\{\\mathcal{P}_{i}\\}_{i=1}^{N}\\) can be described with \\(N\\) SVG paths, with each SVG path \\(\\mathcal{P}_{i}\\) consists of \\(M_{i}\\) basic commands: \\(\\mathcal{P}_{i}=\\{\\mathcal{C}_{i}^{j}\\}_{j=1}^{M_{i}}\\), where \\(\\mathcal{C}_{i}^{j}\\) is the \\(j\\)-th command in the \\(i\\)-th path. Eventually, each basic command \\(\\mathcal{C}=(T,\\mathcal{V})\\) is consist of command type \\(T\\in\\{\\texttt{M},\\texttt{L},\\texttt{C}\\}\\), and the corresponding position argument \\(\\mathcal{V}\\).\n' +
      '\n' +
      '### StrokeNUWA\n' +
      '\n' +
      'StrokeNUWA contains three core components: a Vector Quantized-Stroke (VQ-Stroke) for SVG compression, an Encoder-Decoder-based LLM (EDM) for SVG generation, and an SVG Fixer (SF) for post-processing. Firstly, VQ-Stroke compresses the SVG into stroke tokens, which enables a transformation between the SVG code and the discrete stroke tokens. Then, EDM utilizes the stroke tokens produced from VQ-Stroke to generate SVG code. Finally, SF is a post-processing module designed to refine the quality of the generated SVGs, given that the output generated from the EDM or VQ-Stroke may not always conform to the stringent syntactical rules of SVG code. Below, we will introduce the details of each component.\n' +
      '\n' +
      '#### 3.2.1 Vector Quantized-Stroke\n' +
      '\n' +
      'VQ-Stroke encompasses two main stages: "Code to Matrix" stage that transforms SVG code into the matrix format suitable for model input, and "Matrix to Token" stage that transforms the matrix data into stroke tokens.\n' +
      '\n' +
      'Code to MatrixAs depicted in Fig. 3, we first transform the simplified SVG code (Sec. 3.1) into SVG matrix format by converting each basic command \\(\\mathcal{C}_{i}^{j}\\) to the individual vector \\(\\mathcal{K}_{i}^{j}\\in\\mathbb{R}^{9}\\) with rules \\(f\\):\n' +
      '\n' +
      '\\[\\mathcal{K}_{i}^{j}=f(\\mathcal{C}_{i}^{j})=\\left(T,x_{0},y_{0},c_{0}^{x},c_{0} ^{y},c_{1}^{x},c_{1}^{y},x_{1},y_{1}\\right)_{i}^{j}, \\tag{1}\\]\n' +
      '\n' +
      'where \\(T\\) denotes the basic command type, \\((x_{0},y_{0})\\) and \\((x_{1},y_{1})\\) represent the beginning and the end points, with \\((c_{0}^{x},c_{0}^{y})\\) and \\((c_{1}^{x},c_{1}^{y})\\) as the control points of each basic command. Then, to establish interconnections among the adjacent commands, we set the end point of \\(j\\)-th command \\((x_{1},y_{1})_{i}^{j}\\) equal to the beginning point \\((x_{0},y_{0})_{i}^{j+1}\\) of the subsequent \\((j+1)\\)-th command in each individual path.\n' +
      '\n' +
      'We then decompose all the paths within the SVG \\(\\mathcal{G}\\) into distinct basic commands and combine their corresponding vectors into a matrix form:\n' +
      '\n' +
      '\\[\\begin{split} f(\\mathcal{G})&=\\left(f(P_{i})\\right) _{i=1}^{N}=\\left(\\left(f(\\mathcal{C}_{i}^{j})\\right)_{j=1}^{M_{i}}\\right)_{i=1 }^{N}\\\\ &=\\begin{pmatrix}(\\mathcal{K}_{1}^{1};&\\mathcal{K}_{1}^{2};& \\cdots;&\\mathcal{K}_{1}^{M_{1}})\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ (\\mathcal{K}_{N}^{1};&\\mathcal{K}_{N}^{2};&\\cdots;&\\mathcal{K}_{N}^{M_{N}}) \\end{pmatrix},\\end{split} \\tag{2}\\]\n' +
      '\n' +
      'where ";" denotes the stack operation, and each matrix row represents an individual command. Thus, we can obtain a structured SVG matrix \\(f(\\mathcal{G})\\in\\mathbb{R}^{(\\sum_{i=1}^{N}M_{i})\\times 9}\\) to represent an SVG that contains \\(\\sum_{i=1}^{N}M_{i}\\) individual basic commands.\n' +
      '\n' +
      'Matrix to StrokeAfter obtaining the SVG matrix \\(f(\\mathcal{G})\\), we aim to compress the matrix into discrete stroke tokens via latent representation, with which one can reconstruct the \\(f(\\mathcal{G})\\). As shown in Fig. 3, the VQ-Stroke model is composed of Down-Sample blocks, a Stroke Codebook \\(\\mathcal{B}\\), and Up-Sample blocks. The SVG matrix \\(f(\\mathcal{G})\\) is first encoded by the Down-Sample blocks to obtain the compressed representations, which entails increasing the number of representation channels (column of \\(f(\\mathcal{G})\\)) while concurrently compressing the spatial dimensions (row of \\(f(\\mathcal{G})\\)) to yield a more compact representation, i.e. compressing the number of commands into \\(T\\) s.t. \\(T<\\sum_{i=1}^{N}M_{i}\\). Then, the Codebook \\(\\mathcal{B}\\) simultaneously conducts \\(d\\) levels of compression with residual vector quantization (Martinez et al., 2014), enabling VQ-Stroke to better model the compressed rep\n' +
      '\n' +
      'Figure 3: Overview of VQ-Stroke.\n' +
      '\n' +
      'resentations. We depict the detailed architecture of Down-Sample blocks and Up-Sample blocks in Fig 4, wherein both blocks first utilize a Conv1d or ConvTranspose1d model to compress or expand the features, succeeded by a ResNet1d module and an additional Conv1d module for feature extraction. It is worth mentioning that a low compression rate allows the VQ-Stroke to learn the fine details of SVGs (the first and second columns), while more aggressive compression (the third column) enables the VQ-Stroke to capture the overall contours of the SVGs, As illustrated in Fig.5, a low compression rate allows the VQ-Stroke to learn the fine details of SVGs (the first and second columns), while more aggressive compression (the third column) enables the VQ-Stroke to capture the overall contours of the SVGs. We have more discussion in Sec. 4.2. Finally, the Down-Sample blocks reconstruct the SVG latent representation output from the Codebook \\(\\mathcal{B}\\).\n' +
      '\n' +
      'To train such a network, we follow Dhariwal et al. to calculate the commitment loss, codebook loss, and reconstruction loss to jointly update the VQ-Stroke in Equ. 3:\n' +
      '\n' +
      '\\[\\begin{split}&\\ell_{VQ-Stroke}=\\alpha\\left(\\ell_{codebook}+\\ell_{ commit}\\right)+\\ell_{recon}\\\\ &=\\alpha\\left(||\\;\\mathcal{Z}-\\mathrm{sg}[\\mathcal{\\tilde{Z}}] \\;||_{2}^{2}+||\\;\\mathrm{sg}[\\mathcal{Z}]-\\mathcal{\\tilde{Z}}\\;||_{2}^{2} \\right)\\\\ &+\\mathrm{MSE}(\\widetilde{f(\\mathcal{G})},f(\\mathcal{G})), \\end{split} \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\alpha\\) is the hyper-parameter, \\(\\mathcal{Z}\\) is the compressed latent output from down-sample blocks, \\(\\mathcal{\\tilde{Z}}\\) is the latent looked up from codebook \\(\\mathcal{B}\\), and \\(\\mathrm{sg}[\\cdot]\\) is the gradient clipping operation. Besides, we pre-normalize the input data into the \\([-1,1]\\) range to stabilize the training process.\n' +
      '\n' +
      '#### 3.2.2 Encoder-Decoder-based LLM\n' +
      '\n' +
      'We employ an Encoder-Decoder LLM (EDM) to predict the stroke tokens obtained from the codebook \\(\\mathcal{B}\\). Considering LLM\'s inherent textual instruction capability, we freeze the EDM encoder to leverage its inherited textual knowledge. Subsequently, we fine-tune the EDM decoder to learn the stroke token prediction task. Due to the discrepancy between the vocabulary of stroke tokens and the original LLM\'s vocabulary, we extend EDM with an additional stroke embedding layer and a stroke predictor. Consequently, given the trainable model parameters \\(\\theta\\) and the textual prompt \\(\\mathbf{K}\\), we maximize the log probability \\(\\operatorname*{argmax}_{\\theta}\\prod_{i=1}^{T}P(t_{i}\\mid t_{<i},\\mathbf{K})\\) with the cross-entropy loss.\n' +
      '\n' +
      '#### 3.2.3 SVG Fixer\n' +
      '\n' +
      'A critical issue arises in the generation results from both SDM and EDM, as they fail to guarantee Equ. 1 due to the discrepancies of the interconnection points among adjacent commands in each individual SVG path, i.e., \\((x_{1},y_{1})_{i}^{j}\\neq(x_{0},y_{0})_{i}^{j+1}\\) in \\(i\\)-th path. To address this issue, we introduce the SVG Fixer (SF) as a post-processing module for the generated results. It encompasses two strategies: Path Clipping (PC) and Path Interpolation (PI). Specifically, PC involves the direct substitution of each SVG command\'s beginning point with the endpoint of adjacent SVG commands: \\((x_{0},y_{0})_{i}^{j+1}:=(x_{1},y_{1})_{i}^{j}\\). On the other hand, PI entails the addition of \\(\\mathbb{M}\\) commands between each pair of adjacent, yet non-interconnected SVG commands to bridge the discrepancy, i.e., if \\((x_{1},y_{1})_{i}^{j}\\neq(x_{0},y_{0})_{i}^{j+1}\\implies\\) adding an extra command \\(\\left(\\mathbb{M},(x_{1},y_{1})_{i}^{j},0,0,0,0,(x_{0},y_{0})_{i}^{j+1}\\right)\\) to force the previous command\'s and point to move to the beginning point of the next adjacent command. As shown in Fig. 5, PC can streamline the overall paths of SVGs, making them more succinct, but may lead to some inaccuracies in the details. On the other hand, PI tends to reveal more generated stokes\' details, but it may introduce more curves. Each strategy has its own applicable scenarios.\n' +
      '\n' +
      'Figure 4: Architecture of Down-Sample and Up-Sample Blocks.\n' +
      '\n' +
      'Figure 5: Analysis of SVG reconstruction, where \\(C\\) is a constant representing the number of inserted <H> command in PI setting. To facilitate clear observation of the SVG composition, we represent each basic command with a distinct color.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      'Stroke with SF, PI facilitates a more faithful approximation of the original SVG graphics by achieving the lowest FID score and demonstrating a higher concordance with the given text prompts, as evidenced by the lowest CLIP score. In contrast, the PC method yields better alignment results with the original SVG code as it achieves the lowest EDIT score. Utilizing compression level 2 (C-4), VQ-Stroke attains a notable Compression Ratio (CR) of 6.9%, maintaining performance on par with that of C-2 as evidenced by comparable CLIPScore and FID. This suggests that VQ-Stroke preserves the semantic integrity of the original SVG graphics despite the substantial path compression.\n' +
      '\n' +
      'StrokeNUWAAs illustrated in Table 2, StrokeNUWA outperforms other methods by achieving superior results. Specifically, in terms of visual performance, StrokeNUWA is capable of generating graphics that more closely resemble the Golden SVG--evidenced by the lowest FID score (6.513) and the highest HPS (16.801). This indicates that our Stroke Tokens offer greater compatibility with the LLMs than the vanilla approach (Iconshop). Moreover, StrokeNUWA has attained the highest CLIPScore (17.994), surpassing even Optimization-based methods. This suggests that StrokeTokens encapsulates visual semantics effectively. In terms of the quality of the SVG Code and the efficiency of generation, the Stroke Token not only aligns closely with the Golden standard but also markedly enhances the generation speed, i.e., around 19 seconds of StrokeNUWA V.S. around 30 minutes of Optimization-based method LIVE. This underscores the impressive compressive capabilities of the Stroke token on the original SVG Code, demonstrating both its efficiency and the quality of compression.\n' +
      '\n' +
      '### Qualitative Evaluation\n' +
      '\n' +
      'Case StudyWe show the reconstruction results of VQ-Stroke with varying complexity levels in Fig. 7 and present a qualitative comparison between StrokeNUWA and other baselines in Fig. 8(a). It is impressive that VQ-Stroke can\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c c|c} \\hline \\hline \\multirow{2}{*}{Methods} & \\multicolumn{3}{c|}{Visual Performance} & \\multicolumn{3}{c|}{SVG Code Quality} & Generation \\\\ \\cline{2-7}  & FID (\\(\\downarrow\\)) & CLIPScore (\\(\\uparrow\\)) & HPS (\\(\\uparrow\\)) & \\begin{tabular}{c} Recall (\\(\\uparrow\\)) \\\\ (Stroke Token) \\\\ \\end{tabular} & EDIT (\\(\\downarrow\\)) & \\begin{tabular}{c} Optim / Pred \\\\ Length (\\(Avg\\)) \\\\ \\end{tabular} & \n' +
      '\\begin{tabular}{c} Speed (\\(\\downarrow\\)) \\\\ (_per_ SVG) \\\\ \\end{tabular} \\\\ \\hline SD \\& LIVE & 14.236 & 12.908 & 11.210 & 0.028 & - & 160 (32 Path) & \\(\\approx 28.0\\) min \\\\ VectorFusion & 7.754 & 17.539 & 15.901 & 0.079 & - & 2,048 (128 Path) & \\(\\approx 30.0\\) min \\\\ Iconshop & 17.828 & 8.402 & 8.234 & 0.114 & 24,792.476 & 993.244 & \\(\\approx\\) 63.743 sec \\\\ \\hline SVGNUWA (PC) & 6.607 & 17.852 & 16.134 & **0.239** & **9,092.476** & 271.420 & \\(\\approx\\) **19.128** sec \\\\ SVGNUWA (PI) & **6.513** & **17.994** & **16.801** & 0.207 & 12,249.091 & 271.420 & \\(\\approx\\) **19.128** sec \\\\ \\hline Golden SVG & - & - & - & 100\\% & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Performance of StrokeNUWA, where “Optim/Pred Length” denotes the actual predicted or optimized number of paths.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c} \\hline \\hline Methods & FID (\\(\\downarrow\\)) & CLIPScore (\\(\\uparrow\\)) & EDIT (\\(\\downarrow\\)) & CR (\\(\\downarrow\\)) \\\\ \\hline SQM (C-2) & - & - & 1,114.791 & 8.549\\% \\\\ SQM (C-2) + SF (PC) & 3.751 & 19.861 & **1,996.313** & 8.786\\% \\\\ SQM (C-2) + SF (PD) & **3.518** & **20.290** & 1,315.137 & 13.780\\% \\\\ SQM (C-4) + SF (FD) & 4.943 & 17.192 & 2,100.671 & **6.890\\%** \\\\ \\hline Golden SVG & - & - & - & 100\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Performance of VQ-Stroke on SVG reconstruction task, where C-2 and C-4 denote the Compression Rate 2 and 4.\n' +
      '\n' +
      'Figure 8: Sampled cases from different models in SVG generation task, where the CLIPScore is the average score calculated across four generated cases for each method.\n' +
      '\n' +
      'Figure 7: Reconstruction performance of difference VQ-Strokes.\n' +
      '\n' +
      'reconstruct complex SVGs with a limit of only 4,096 codebook size. Then, at the Compression Rate of 2 (CR-2), VQ-Stroke successfully outlines the edge of objects within the graphics, demonstrating that stroke tokens can be highly compressed with a dense representation and inherently incorporate semantic segmentation, which is essential for retaining visual semantics. Regarding the comparison of StrokeNUWA, we note that employing LLM-based generation methods can result in incomplete SVGs (Iconshop). This is attributed to the excessive SVG code lengths and LLMs struggling to capture the key information embedded within SVG graphics. However, the use of stroke tokens can mitigate these issues by compressing the paths and being compatible with LLMs. Furthermore, we find that the performance of the optimization-based method heavily relies on the outputs generated by the stable diffusion model, which is subject to the limitations of grid tokens mentioned in Sec. 1, e.g., it is hard to capture the visual semantics and tends to generate extra visual information that is not aligned with the text prompt. Besides, the optimization process is extremely slow. In contrast, StrokeNUWA, which utilizes stroke tokens, inherently contains visual semantic segmentation. As a result, the content generated is more aligned with the textual semantics, providing a more coherent and semantically accurate graphic.\n' +
      '\n' +
      'Human EvaluationFurthermore, we conduct a human evaluation to compare the generated SVG outputs from StrokeNUWA with those produced by the LLM-based method, Iconshop. We select 50 different textual prompts and guide the model to generate corresponding SVGs for evaluation. As depicted in Figure 9, our comparison is founded on three criteria: Prompt Alignment (consistency between the generated result and the text prompt), Overall Quality (the general caliber of SVGs), and Graphic Details (intricacies such as curves). We observe that StrokeNUWA, compared to Iconshop, which regards code as visual representation, not only yields more complete content (better Overall Quality) but produces results more closely aligned with the textual prompts (better Prompt Alignment)4. Given that stroke tokens compress the details of SVG, it is natural that StrokeNUWA excels in generating Graphic Details.\n' +
      '\n' +
      'Footnote 4: The main reason for low Prompt Alignment in Iconshop is also due to the incompleteness of the generated SVGs.\n' +
      '\n' +
      '## 5 Ablation Study\n' +
      '\n' +
      '### Analysis of VQ-Stroke Model Architecture\n' +
      '\n' +
      'To investigate the impact of VQ-Stroke architecture configurations on the stroke token performance, we experiment with different codebook sizes \\(|\\)\\(\\mathcal{B}\\)\\(|\\) and codebook dimension \\(\\mathrm{Dim}\\). As shown in Tab. 4, we can observe that by increasing the codebook size while simultaneously reducing the dimension of each stroke token, the VQ-Stroke achieves superior performance across multiple metrics. We sample a set of reconstruction cases to showcase the trend of changes in Fig. 7, which indicates that, with a larger codebook size and smaller dimension, the VQ-Stroke can delineate details with greater accuracy, e.g., straighter lines.\n' +
      '\n' +
      '### Comparison with GPT-4\n' +
      '\n' +
      'We compare the generation results with GPT-4 (Achiam et al., 2023) by employing the following template to guide GPT-4 in producing the corresponding SVG code: Generate SVG codes in icon style based on keywords:{KEYWORDS}. We show the rendered SVGs in Fig. 8(b), where we can observe that GPT-4 can only generate simple SVGs, which is consistent with LLM-based methods. Moreover, GPT-4 often yields SVGs that are incongruent with the associated text.\n' +
      '\n' +
      '## 6 Conclusion and Future Work\n' +
      '\n' +
      'This paper presents StrokeNUWA, a pioneering study that explores a superior visual representation--"stroke" tokens, as an alternative method for expressing images through vector graphics. Stroke tokens not only preserve the semantic integrity of the images but are also conducive to processing by LLMs. Moreover, strokes in vector graphics can be highly compressed. Experiments indicate that, equipped with stroke tokens, LLMs can achieve superior results across various metrics in the SVG synthesis task. This paper showcases the tremendous potential of stroke token representation in the field of vector graphic synthesis. Moving forward, we aim to continue improving the quality of stroke tokens through advanced visual tokenization methods tailored for LLMs. In addition, we intend to generalize stroke token utilization to a broader range of tasks (SVG Understanding), domains (3D), and the generation of SVGs for images sourced from the real world.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c} \\hline \\hline \\multicolumn{2}{c|}{Settings} & \\multicolumn{1}{c}{FID (\\(\\downarrow\\))} & \\multicolumn{1}{c}{CLIPScore (\\(\\uparrow\\))} & EDIT (\\(\\downarrow\\)) \\\\ \\hline \\(|\\)\\(\\mathcal{B}\\)\\(|\\) & Dim & & & \\\\ \\hline\n' +
      '2048 & 512 & 5.702 & 19.365 & 2,323.810 \\\\\n' +
      '4096 & 512 & 3.518 & 20.290 & 1,315.137 \\\\\n' +
      '4096 & 1024 & 3.901 & 20.159 & 1,793.008 \\\\\n' +
      '8192 & 512 & **2.639** & **21.014** & **907.106** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Comparison among different VQ-Stroke Settings.\n' +
      '\n' +
      'Figure 9: Human evaluation between StrokeNUWA and LLM-based method—Iconshop.\n' +
      '\n' +
      '## Impact Statement\n' +
      '\n' +
      'The implications of this work are manifold, potentially revolutionizing the visual synthesis from another format of image, vector graphics. As stroke tokens refine the interplay between visual representation and LLMs, future advancements in visual tokenization techniques designed for LLMs are anticipated. Moving forward, the community can extend stroke token application into wider tasks and domains, including SVG comprehension and open-domain SVG synthesis for images from the real world. As we pioneer this nascent field, we are conscious of the profound societal impact that such advancements in machine learning and graphical representations hold. The capabilities for automated graphic design, scalable vector graphics production, and enhanced digital aristry foreshadow considerable shifts in industries reliant on visual content. By forging new pathways for artistic expression and visual communication, our work stands to not only contribute to the scientific community but also to catalyze transformations in creative, technological, and educational sectors. We recognize the importance of our work and our responsibility to ensure that our contributions to the field are conducted ethically, aiming to benefit society as a whole, democratize the visual landscape, and enrich it through responsible and judicious innovation.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Cited by: SS1.\n' +
      '* R. Anil et al. (2023)Plam 2 technical report. arXiv preprint arXiv:2305.10403. Cited by: SS1.\n' +
      '* A. Brohan et al. (2023)Carbajal, J., Chebotar, X., Choromanski, T., Ding, D., Dubes, C., et al. Rt-2: vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818. Cited by: SS1.\n' +
      '* T. Brown, B. Mann, N., Subbiah, J. D., P. Dhariwal, A. Neelakantan, P. Shyam, G., Askell, et al. (2020)Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.\n' +
      '* S. Cao, Y. Yin, L. Huang, Y. Liu, X. Zhao, and K. Huang (2023)Efficient-vqgan: towards high-resolution image generation with efficient vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7368-7377. Cited by: SS1.\n' +
      '* A. Carlier, M. Danelljan, A. Alahi, and R. Timofte (2020)Deepsvg: a hierarchical generative network for vector graphics animation. Advances in Neural Information Processing Systems33, pp. 16351-16361. Cited by: SS1.\n' +
      '* H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman (2022)MaskGit: masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11315-11325. Cited by: SS1.\n' +
      '* A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. (2022)Palm: scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Cited by: SS1.\n' +
      '* H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, et al. (2022)Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. Cited by: SS1.\n' +
      '* L. Clouatre and M. Demers (2019)Figr: few-shot image generation with reptile. arXiv preprint arXiv:1901.02199. Cited by: SS1.\n' +
      '* P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever (2020)Jukebox: a generative model for music. arXiv preprint arXiv:2005.00341. Cited by: SS1.\n' +
      '* P. Esser, R. Rombach, and B. Ommer (2021)Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873-12883. Cited by: SS1.\n' +
      '* D. Ghosal, N. Majumder, A. Mehrish, and S. Poria (2023)Text-to-audio generation using instruction-tuned llm and latent diffusion model. arXiv preprint arXiv:2304.13731. Cited by: SS1.\n' +
      '* J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi (2021)Clipscore: a reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718. Cited by: SS1.\n' +
      '* M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter (2017)Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems30. Cited by: SS1.\n' +
      '* Q. Hu, G. Zhang, Z. Qin, Y. Cai, G. Yu, and G. Y. Li (2023)Robust semantic communications with masked vq-vae enabled codebook. IEEE Transactions on Wireless Communications. Cited by: SS1.\n' +
      '* A. Jain, A. Xie, and P. Abbeel (2023)Vectorfusion: text-to-svg by abstracting pixel-based diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1911-1920. Cited by: SS1.\n' +
      '* D. Kondratyuk, L. Yu, X. Gu, J. Lezama, J. Huang, R. Hornung, H. Adam, H. Akbari, Y. Alon, andV., et al. Videoopoet: A large language model for zero-shot video generation. _arXiv preprint arXiv:2312.14125_, 2023.\n' +
      '* Lee et al. (2022) Lee, D., Kim, C., Kim, S., Cho, M., and HAN, W. S. Draft-and-revise: Effective image generation with contextual rq-transformer. _Advances in Neural Information Processing Systems_, 35:30127-30138, 2022.\n' +
      '* Li et al. (2020) Li, T.-M., Lukac, M., Gharbi, M., and Ragan-Kelley, J. Differentiable vector graphics rasterization for editing and learning. _ACM Transactions on Graphics (TOG)_, 39(6):1-15, 2020.\n' +
      '* Ma et al. (2022) Ma, X., Zhou, Y., Xu, X., Sun, B., Filev, V., Orlov, N., Fu, Y., and Shi, H. Towards layer-wise image vectorization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 16314-16323, 2022.\n' +
      '* Martinez et al. (2014) Martinez, J., Hoos, H. H., and Little, J. J. Stacked quantizers for compositional vector compression. _arXiv preprint arXiv:1411.2173_, 2014.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* Rajbhandari et al. (2020) Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimizations toward training trillion parameter models. In _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pp. 1-16. IEEE, 2020.\n' +
      '* Reddy et al. (2021) Reddy, M. D. M., Basha, M. S. M., Hari, M. M. C., and Penchalaiah, M. N. Dall-e: Creating images from text. _UGC Care Group I Journal_, 8(14):71-75, 2021.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.\n' +
      '* Singhal et al. (2023) Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Hou, L., Clark, K., Pfohl, S., Cole-Lewis, H., Neal, D., et al. Towards expert-level medical question answering with large language models. _arXiv preprint arXiv:2305.09617_, 2023.\n' +
      '* Su et al. (2023) Su, H., Liu, X., Niu, J., Cui, J., Wan, J., Wu, X., and Wang, N. Marvel: Raster gray-level manga vectorization via primitive-wise deep reinforcement learning. _IEEE Transactions on Circuits and Systems for Video Technology_, 2023.\n' +
      '* Sun et al. (2023) Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., and Wang, X. Generative pretraining in multimodality. _arXiv preprint arXiv:2307.05222_, 2023.\n' +
      '* Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* Van Den Oord et al. (2017) Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* Wang et al. (2022) Wang, Y., Pu, G., Luo, W., Wang, Y., Xiong, P., Kang, H., and Lian, Z. Aesthetic text logo synthesis via content-aware layout inferring. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 2436-2445, 2022.\n' +
      '* Wu et al. (2022) Wu, C., Liang, J., Ji, L., Yang, F., Fang, Y., Jiang, D., and Duan, N. Niuva: Visual synthesis pre-training for neural visual world creation. In _European conference on computer vision_, pp. 720-736. Springer, 2022.\n' +
      '* Wu et al. (2023a) Wu, R., Su, W., Ma, K., and Liao, J. Iconshop: Text-guided vector icon synthesis with autoregressive transformers. _ACM Transactions on Graphics (TOG)_, 42(6):1-14, 2023a.\n' +
      '* Wu et al. (2023b) Wu, X., Sun, K., Zhu, F., Zhao, R., and Li, H. Human preference score: Better aligning text-to-image models with human preference. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 2096-2105, 2023b.\n' +
      '* Xing et al. (2023) Xing, X., Zhou, H., Wang, C., Zhang, J., Xu, D., and Yu, Q. Svgdreamer: Text guided svg generation with diffusion model. _arXiv preprint arXiv:2312.16476_, 2023.\n' +
      '* Yu et al. (2023) Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A. G., et al. Language model beats diffusion-tokenizer is key to visual generation. _arXiv preprint arXiv:2310.05737_, 2023.\n' +
      '* Zhang et al. (2023) Zhang, T., Liu, H., Zhang, P., Cheng, Y., and Wang, H. Beyond pixels: Exploring human-readable svg generation for simple images with vision language models. _arXiv preprint arXiv:2311.15543_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>