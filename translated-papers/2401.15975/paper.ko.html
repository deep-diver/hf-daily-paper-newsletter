<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 안정신원: 첫눈에 아무데나 아무나 삽입\n' +
      '\n' +
      '칭허왕({}^{1}\\), Xu Jia\\({}^{1}\\), Xiaomin Li\\({}^{1}\\), 타이칭 Li\\({}^{1}\\), Liqian Ma\\({}^{2}\\), Yunzhi Zhuge\\({}^{1}\\), Huchuan Lu\\({}^{1}\\)\n' +
      '\n' +
      '다롄 공대, \\({}^{1}\\)**Dalian 공대, \\({}^{2}\\)ZMO AI Inc.**\n' +
      '\n' +
      'Corresponding authors.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근 대규모 사전 훈련된 텍스트 대 이미지 모델의 발전은 고품질 인간 중심 생성을 위한 전례 없는 능력을 보여주었지만 얼굴 정체성을 맞춤화하는 것은 여전히 다루기 어려운 문제이다. 기존의 방법들은 훈련 중 각 피험자마다 여러 개의 이미지를 가지고도 안정적인 아이덴티티 보존과 유연한 편집성을 보장할 수 없다. 본 논문에서는 하나의 얼굴 영상으로 동일성 일관성 있는 재맥락화를 가능하게 하는 StableIdentity를 제안한다. 보다 구체적으로, 입력된 얼굴을 인코딩하기 전에 아이덴티티를 갖는 얼굴 인코더를 채용하고, 이어서 얼굴 표현을 편집가능한 사전이 있는 공간에 랜딩하며, 이는 유명인사 이름으로부터 구성된다. 아이덴티티 사전 및 편집성을 사전 통합함으로써, 학습된 아이덴티티는 다양한 컨텍스트로 어디에나 주입될 수 있다. 또한 입력면에 대한 픽셀 수준의 인식을 높이고 생성의 다양성을 유지하기 위해 마스킹된 2상 확산 손실을 설계한다. 광범위한 실험을 통해 본 논문에서 제안한 방법이 기존의 사용자 정의 방법보다 우수함을 보인다. 또한, 학습된 아이덴티티는 ControlNet과 같은 기성 모듈과 유연하게 결합될 수 있다. 특히, 우리가 가장 잘 아는 한, 우리는 단일 이미지로부터 학습된 아이덴티티를 미세 조정 없이 비디오/3D 생성에 직접 주입하는 최초의 사람이다. 제안된 스테이블 아이덴티티는 이미지, 비디오, 3D 맞춤형 생성 모델을 통합하기 위한 중요한 단계라고 믿는다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '확산 모델[29, 28, 47]의 붐으로 맞춤형 세대는 광범위한 관심을 받았다[31, 45, 18]. 이 작업은 텍스트-이미지 모델들에 새로운 주제(예를 들어, 아이덴티티)를 주입하고, 입력 텍스트 프롬프트를 정렬하면서 다양한 컨텍스트들에서 일관된 주제들을 갖는 이미지들을 생성하는 것을 목표로 한다. 예를 들어, 사용자는 "슈퍼맨 의상 착용"과 같은 흥미로운 사진을 얻기 위해 자신의 사진을 업로드할 수 있다. 맞춤형 세대의 성공은 개인화된 인물 사진[20], 가상 트라이온[5] 및 아트&디자인[26]과 같은 많은 애플리케이션들을 용이하게 할 수 있다.\n' +
      '\n' +
      '그러나 기존의 커스터마이징 방법은 모델의 부품/전체 파라미터를 미세 조정하거나 일반 인코더를 학습하여 이 작업을 해결한다. 파라미터 미세 조정 방법[12, 8, 30]은 최적의 파라미터를 탐색하는 데 오랜 시간이 걸리지만, 종종 신원을 나타내기 위한 부정확한 사소한 해를 반환한다. 특히 단일 영상만을 사용하는 경우, 이러한 방법들은 입력을 과도하게 맞추는 경향이 있어 편집성 저하를 초래한다. 대안적으로, 인코더 기반 방법들[43, 42]은 독특한 아이덴티티 및 세부사항들을 캡처하기 위해 훈련 및 투쟁을 위한 대규모 데이터세트들을 필요로 한다. 더욱이, 현재의 방법에 의해 학습된 정체성은 다양한 맥락에서 목표 정체성과 불일치하기 쉽다. 따라서, 이 과제가 직면한 막대한 과제들(예를 들어, 불안정한 신원 보존, 열악한 편집성, 비효율성)을 해결하기 위한 새로운 프레임워크를 제안하는 것이 시급하다.\n' +
      '\n' +
      '여기에서 우리는 특히 원샷 트레이닝 설정 하에서 인간에 대한 맞춤형 생성, 텍스트 프롬프트와 자연스럽게 통합될 수 있는 단어 임베딩에 신원 정보를 저장하는 방법에 관심이 있다. 우리는 사전 지식이 이 작업에 도움이 될 수 있다고 믿습니다. 한편으로는 얼굴 인식 과제[36]가 충분히 탐색되었으며 사전 훈련된 모델의 신원 인식 능력을 활용할 수 있다. 한편, 방대한 인터넷 데이터를 기반으로 학습된 텍스트-이미지 모델은 다양한 맥락에서 유명인 이름을 가진 이미지를 생성할 수 있으므로 이러한 이름은 사전 편집성이 풍부하다. 이러한 전적을 사용하면 이러한 도전을 완화할 수 있으며 일부 방법[6, 45]은 예비 시도를 했다.\n' +
      '\n' +
      '본 연구에서는 인간 중심의 맞춤형 생성에 앞서 아이덴티티 사전과 편집성을 통합한 스테이블 아이덴티티를 제안한다. 구체적으로, 얼굴 인식 태스크 상에서 미리 훈련된 인코더가 신원 표현을 캡처하기 위해 도입된다. 셀럽 이름은 사용자 정의 생성을 위한 사전 아이덴티티 분포로서 임베딩 공간을 구성하기 위해 수집된다. 사전 훈련된 확산 모델에서 타깃 아이덴티티가 유명인 이름과 같이 수행되도록 장려하기 위해, 우리는 아이덴티티 표현을 이전 공간에 추가로 구축한다. 또한, 보다 안정적인 동일성 및 세립질 재구성을 학습하기 위해, 우리는 각각 잡음 제거 과정의 초기 및 후기 단계에서 전문화된 목표를 할당하는 마스킹된 2단계 확산 손실을 설계한다. 광범위한 실험은 StableIdentity가 최첨단 방법에 대해 호의적으로 수행함을 보여주며, 우리는 동일한 종류의 여러 기준선에 대한 우리의 우월성을 추가로 분석한다. 제안된 방법은 또한 그림 1과 같이 기성 이미지/비디오/3D 모델과 직접 협업할 수 있는 안정적인 일반화 능력을 보여준다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약할 수 있다:\n' +
      '\n' +
      '* 우리는 하나의 얼굴 이미지로 아이덴티티-일관성 재맥락화를 가능하게 하기 위해 아이덴티티 사전 및 편집성을 통합한 스테이블 아이덴티티를 제안한다.\n' +
      '* 픽셀 수준의 세부 사항을 인식하고 다양한 생성을 위해 보다 안정적인 동일성을 학습하기 위해 마스킹된 2상 확산 손실을 설계한다.\n' +
      '* 광범위한 실험은 우리의 방법이 효과적이고 두드러진다는 것을 보여준다. 놀랍게도, 이 방법은 이미지 레벨 모듈과 결합할 수 있을 뿐만 아니라 단일 이미지에서 학습된 아이덴티티가 미세 조정 없이 아이덴티티 일치 맞춤형 비디오/3D 생성을 달성할 수 있는 일반화 능력을 해제할 수 있다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### 텍스트-이미지 확산 모델\n' +
      '\n' +
      '확산 모델[15, 34]은 텍스트 조건 이미지 생성에서 압도적인 성공을 거두어 수많은 고전 작품[29, 24, 13]을 도출했다. 이 중 안정적인 확산[29]은 우수한 오픈 소스 환경으로 널리 사용되고 있다. 실제로, 안정확산은 DDIM 샘플링으로 가우시안 잡음과 텍스트 프롬프트로부터 다양하고 정교한 이미지를 생성할 수 있다[34]. 트레이닝 데이터세트에는 셀럽 사진과 그에 대응하는 이름들이 많이 포함되어 있기 때문에, Stable Diffusion은 셀럽 이름들을 서로 다른 텍스트 프롬프트와 결합하여 다양한 이미지들을 생성할 수 있다. 그러나 일반인들은 이 \'특권\'을 직접 누릴 수 없다. 따라서 보다 광범위한 사용자에게 안정적인 확산을 민주화하기 위해 많은 연구[6, 45, 4]가 맞춤형 생성 과제에 초점을 맞추고 있다.\n' +
      '\n' +
      '### Customized Generation\n' +
      '\n' +
      '현재 맞춤형 생성 방식은 크게 최적화 기반 방식과 인코더 기반 방식으로 나눌 수 있다. 전자는 최적화하는 데 오랜 시간이 필요한 경우가 많은 반면 후자는 대규모 데이터가 필요하고 독특한 정체성을 배우기 위해 고군분투한다. 동일한 주제의 3-5개의 이미지가 주어지면, Textual Inversion[12]는 대상 주제를 나타내기 위해 새로운 단어 임베딩을 최적화한다. 드림부스[30]는 대상 주제에 맞게 전체 모델을 세밀하게 조정합니다. 한편, ELITE[38], InstantBooth[33] 및 IP-Adapter[43]은 인코더를 학습하여 아이덴티티 정보를 어텐션 레이어에 도입한다. FastComposer[40]는 ID를 캡처하기 위해 Stable Diffusion의 전체 U-Net과 함께 인코더를 훈련시킨다. 또한 최적화 기반 방법을 지원하기 위해 인코더를 통합하는 몇 가지 방법이 있으며[39], 성능 천장을 높입니다. Celeb-Basis[45]는 PCA[25]에 의해 셀럽 기반을 구축하기 위해 Stable Diffusion에서 편집 가능한 691개의 셀럽 이름을 수집한다. 상기 기저의 가중치는 기초하여 최적화되는\n' +
      '\n' +
      '그림 1: 단일 입력 이미지가 주어지면 제안된 _StableIdentity_는 다양한 컨텍스트에서 다양한 맞춤형 이미지를 생성할 수 있다. 특히, 학습된 아이덴티티는 ControlNet[48]과 결합되어 비디오(ModelScopeT2V[37]) 및 3D(LucidDreamer[19]) 생성에도 주입될 수 있음을 제시한다.\n' +
      '\n' +
      ' ArcFace 인코더[7]의 출력 상에서, 기저에 가중치를 부여함으로써 새로운 아이덴티티의 표현이 획득될 수 있다. 그러나, 언급된 방법들은 여전히 신원 보존 및 편집성에 대한 불균형을 수행한다.\n' +
      '\n' +
      '이에 비해 본 논문에서 제안하는 방법은 최적화 과정을 상당히 쉽게 하기 전에 아이덴티티와 편집성을 이용하고, 제안된 손실로 보다 안정적인 아이덴티티를 학습한다. 안정적인 확산은 고정되어 있기 때문에 ControlNet[48]과 같은 플러그 앤 플레이 모듈을 원활하게 사용할 수 있다. 더 나아가, 가장 잘 아는 한, 우리는 비디오[37]/3D 세대[19]에 주입된 단일 이미지로부터 학습 아이덴티티를 가능하게 하는 첫 번째 작업이다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '단일 얼굴 이미지가 주어졌을 때, 우리는 다양한 텍스트 프롬프트 하에서 동일성-일관성 재맥락화를 구현하기 위해 그림 2와 같이 워드 임베딩을 통해 동일성을 표현하는 것을 목표로 한다. 이를 위해, 동일성 사전 및 편집성 사전(Sec. 3.2 참조)을 통합하고 마스킹된 2상 확산 손실(Sec. 3.3 참조)을 제안한다.\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      '이 작업에서 우리는 사전 훈련된 안정 확산[29]을 텍스트 대 이미지 모델(SD로 표시됨)로 채택한다. SD는 VAE(\\(\\mathcal{E}\\), \\(\\mathcal{D}\\))[11], 잡음제거 U-Net(\\epsilon_{\\theta}\\) 및 CLIP 텍스트 인코더 \\(e_{text}\\)[27]의 세 가지 구성요소로 구성된다. VAE의 고품질 재구성을 통해 입력 영상의 확산 과정을 잠재 공간 \\(z\\)(\\(z=\\mathcal{E}(x))에서 수행한다. 구체적으로, random timestep \\(t\\)(\\(t\\in[1,1000)\\)), \\(z_{t}\\)을 가중치 조합 \\(z_{0}\\)과 random noise \\(\\epsilon\\)으로 샘플링할 수 있다 (\\(\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})\\)):\n' +
      '\n' +
      '\\[z_{t}=\\sqrt{\\bar{\\alpha}_{t}}z_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon, \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(\\bar{\\alpha}_{t}\\)는 미리 정의된 하이퍼파라미터 집합이다. 한편, 텍스트 프롬프트가 주어지면, \\(p\\)의 토큰화기는 \\(e_{text}\\)을 \\(l\\) 정수 토큰으로 순차적으로 분할하여 인코딩한다. 그 다음, \\(e_{text}\\)의 임베딩 레이어는 사전을 찾아 \\(\\hat{l}\\) 단어 임베딩으로 구성된 해당 임베딩 그룹 \\(g=[v_{1},...,v_{l}],v_{i}\\in\\mathbb{R}^{d}\\)을 얻는다. 그 후, \\(e_{text}\\)의 텍스트 트랜스포머 \\(\\tau_{text}\\)는 주어진 텍스트 프롬프트 \\(p\\)에 부합하는 이미지를 생성하기 위해 모델을 안내하는 \\(g\\)을 추가로 나타낸다. 잠재 \\(z_{t}\\)으로, 트레이닝 프로세스는 다음과 같이 최적화된다:\n' +
      '\n' +
      '\\mathcal{L}_{noise}=\\mathbb{E}_{z,g,\\epsilon,t}\\left[\\|\\epsilon-\\epsilon_{ \\theta}(z_{t},t,\\tau_{text}(g))\\|_{2}^{2}\\right]\\tag{2}\\heta}\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      '**Identity Prior.** 기존 방법들은 높은 수준의 의미론적 학습을 위해 사전 훈련된 CLIP 이미지 인코더와 공통적으로 주제 정보를 추출하는데, 상세한 신원 인식이 부족하다. 따라서 입력 영상으로부터 ID 인식 표현 \\(I\\)을 얻기 위해 얼굴 인식 태스크(FR-ViT로 표기)를 위해 ViT[9] 인코더를 사용한다.\n' +
      '\n' +
      '바닐라 SD의 일반화 가능성과 편집성을 유지하기 위해 FR-ViT 인코더와 SD를 고정한다. [6, 45]에 이어서, 우리는 MLP를 사용하여 \\(I\\)을 두 개의 워드 임베딩 \\([v^{\\prime}_{1},v^{\\prime}_{2}]\\)으로 투영한다:\n' +
      '\n' +
      '\\[[v^{\\prime}_{1},v^{\\prime}_{2}]=MLPs(I) \\tag{3}\\]\n' +
      '\n' +
      '신원 사전 지식을 통해 추가적인 특징 주입 없이 입력 영상에서 얼굴 특징을 확산 모델에 더 효율적으로 주입할 수 있다.\n' +
      '\n' +
      '**편집성 사전.** SD는 대규모 인터넷 데이터에 대해 트레이닝되기 때문에, 유명인 이름을 사용하는 것은 신속한 일치 아이덴티티를 갖는 이미지를 생성할 수 있다. 따라서, 우리는 유명인 이름이 사전 편집성을 가진 공간을 구성한다고 가정한다. 우리는 이 공간에서 691명의 유명인 이름[45]을 샘플링 포인트로 간주하고 이 공간 분포를 단어 임베딩의 평균과 표준 편차로 표현하고자 한다. 그러나, 실제로, 상기 토큰화기는 익숙하지 않은 단어를 다중으로 분해한다\n' +
      '\n' +
      '도 2: 제안된 _StableIdentity_의 개요. 단일 얼굴 이미지가 주어지면, 우리는 먼저 FR-ViT 인코더와 MLP를 사용하여 아이덴티티 표현을 캡처한 다음, 아이덴티티-일관성 편집성을 더 잘 학습하기 위해 구축된 셀럽 임베딩 공간에 랜딩한다. 또한 학습을 위해 \\(\\mathcal{L}_{noise}\\)와 \\(\\mathcal{L}_{rec}\\)을 포함하는 마스킹된 2상 확산 손실을 설계한다.\n' +
      '\n' +
      'tokens(예: _Deschanel_\\(\\rightarrow[561,31328,832]\\))는 결과적으로 다른 셀럽 이름에 의해 생성되는 토큰의 수가 동일하지 않을 수 있다. 균일한 차원으로 편집 가능한 공간을 찾기 위해 이름과 성만으로 구성된 유명인 이름을 선택하고, 각 단어는 하나의 토큰(예: _Tom Cruise_\\(\\rightarrow[2435,6764])에 해당한다. 최종적으로 326명의 유명인 이름을 얻고, 이를 해당 단어 임베딩 \\(C\\in\\mathbb{R}^{326\\times d}\\)으로 인코딩한다.\n' +
      '\n' +
      '셀럽 임베딩과 같은 동일성 일관적 재맥락화 능력을 마스터하기 위해 AdaIN[10]을 사용하여 편집성 사전과 토지\\([v^{\\prime}_{1},v^{\\prime}_{2}]\\)을 셀럽 임베딩 공간에 통합한다.\n' +
      '\n' +
      '\\[v^{*}_{i}=\\sigma(C)(\\frac{v^{\\prime}_{i}-\\mu(v^{\\prime}_{i}}{\\sigma(v^{\\prime}_{i})+\\mu(C),for\\i=1,2\\tag{4}\\)\n' +
      '\n' +
      '여기서 \\(\\mu(v^{\\prime}_{i}),\\sigma(v^{\\prime}_{i})\\)는 스칼라이다. \\\\ (\\mu(C)\\in\\mathbb{R}^{d}\\), \\(\\sigma(C)\\in\\mathbb{R}^{d}\\)의 각 차원은 서로 다른 분포를 가지므로 벡터이다. 이러한 편집성을 가지고, 학습된 임베딩 \\([v^{*}_{1},v^{*}_{2}]\\)은 그림 7과 같이 베이스라인보다 셀럽 임베딩 공간에 더 가깝기 때문에 편집성을 우아하고 효과적으로 향상시킨다. 또한, 셀럽 임베딩 공간 내의 최적화 프로세스를 제한하고 다른 카테고리들로 표류하는 것을 방지한다.\n' +
      '\n' +
      '### Model Training\n' +
      '\n' +
      '**2상 확산 손실** 아키텍쳐 설계 외에도 확산 모델의 학습 목표를 재고합니다. 바닐라 트레이닝 손실\\(\\mathcal{L}_{noise}\\)은 잡음제거 U-Net\\(\\epsilon_{\\theta}\\)을 자극하여 입력\\(z_{t}\\)에 포함된 잡음\\(\\epsilon\\)을 임의의 시간(t\\)에 예측하며, 도입된 \\(\\epsilon\\)은 매번 랜덤하게 샘플링된다. 따라서, 이러한 목적함수는 입력영상에서 아이덴티티를 암묵적이고 비효율적으로 학습할 뿐이다.\n' +
      '\n' +
      'DDIM [34]는 Eq의 변형에 의해 예측된 잡음 제거된 관찰을 제안한다. 1: \\(\\hat{z}_{0}=\\frac{z_{t}-\\sqrt{1-\\hat{\\alpha}\\epsilon_{\\theta}{\\sqrt{\\hat{ \\alpha}_{t}}\\. 순진한 아이디어는 예측 \\(\\hat{z}_{0}\\)과 실제 \\(z_{0}\\)[39]: \\(\\mathcal{L}_{rec}=\\mathbb{E}_{z_{g},e,t}\\left[\\|\\hat{z}_{0}-z_{0}\\|_{2}^{2}\\right]\\\\(z_{0})에 대한 재구성을 명시적으로 최적화할 수 있는 평균 제곱 오차로 \\(\\mathcal{L}_{noise}\\)을 대체하는 것이다. 그러나 타임스텝이 증가함에 따라 예측된 \\(\\hat{z}_{0}\\)은 그림 3과 같이 \\(z_{0}\\)의 실제 분포를 근사하는 것이 더 어려워지는 것을 관찰할 수 있다. 따라서 타임스텝이 클수록 \\(\\mathcal{L}_{rec}\\)은 의미가 떨어지고 심지어는 모델을 잘못 유도하여 픽셀 수준 재구성에 과도하게 초점을 맞춘다. 이를 위해 2상 확산 손실을 timestep \\(\\alpha T\\):\n' +
      '\n' +
      '\\begin{cases}\\mathcal{L}_{diffusion}=\\begin{cases}\\mathbb{E}_{z,g,\\epsilon,t}\\left[\\|\\epsilon-\\epsilon_{\\theta}(z_{t},t,\\tau_{text}(g))\\|_{2}^{2}\\right]&t\\geq \\alpha T,\\\\mathbb{E}_{z,g,\\epsilon,t}\\left[\\|\\hat{z}_{0}-z_{0}^{2}\\right]&t<\\alpha T.\\end{cases}\\tag{5}\\right}\\eq\\alpha T,\\\\mathbb{E}_{z,g,\\epsilon,t}\\left[\\|\\hat{z}_{0}-z_{0}^{2}\\right]&t<\\alpha T.\\end{cases}\\tag{5\n' +
      '\n' +
      '경험적으로 나눗셈 매개변수 \\(\\alpha\\in[0.4,0.6]\\)은 동일성 보존과 다양성 균형(\\(\\alpha=0.6\\)을 기본값으로 하는 좋은 결과를 얻었다. 생성 영상의 레이아웃을 결정하는 잡음 제거 과정의 초기 단계에서 \\(\\mathcal{L}_{noise}\\)을 사용하면 학습된 아이덴티티가 다양한 레이아웃에 적응할 수 있고, 후기 단계에서 \\(\\mathcal{L}_{rec}\\)을 사용하면 입력 영상에 대한 픽셀 수준의 인식을 높여 보다 안정적인 아이덴티티를 학습할 수 있다.\n' +
      '\n' +
      '**마스크 확산 손실.** 관련 없는 배경 학습을 방지하기 위해 마스크 확산 손실[1, 39]도 사용한다. 구체적으로, 입력 영상의 얼굴 마스크 \\(M_{f}\\)와 헤어 마스크 \\(M_{h}\\)을 구하기 위해 사전 훈련된 얼굴 파싱 모델[44]을 사용한다. 상기 트레이닝 손실은 상기 얼굴 영역 및 상기 모발 영역에서 각각 계산되는,\n' +
      '\n' +
      '\\[\\mathcal{L}=M_{f}\\odot\\mathcal{L}_{확산}+\\beta M_{h}\\odot\\mathcal{L}_{확산}. \\tag{6}\\\\tag{\n' +
      '\n' +
      '실험에서는 \\(\\beta=0.1\\)을 기본값으로 설정하였다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setting\n' +
      '\n' +
      '**구현 상세.** 우리의 실험은 안정적인 확산 2.1-베이스를 기반으로 한다. FR-ViT는 얼굴 인식 작업을 위해 미세 조정된 ViT-B/16 인코더이다. 입력된 단일 이미지의 경우 색상 지터, 랜덤 시프트, 랜덤 크롭 및 랜덤 크기를 데이터 증강으로 사용한다. 학습률과 배치크기는 \\(5e-5\\)와 \\(1\\)으로 설정하였다. MLP는 450단계(4분) 동안 훈련된다. 본 논문에서는 공간복을 입고 있는 "\\(v^{*}_{1}\\)\\(v^{*}_{2}\\"과 같은 프롬프트의 자리홀더\\(v^{*}_{1}\\)\\(v^{*}_{2}\\)은 간결함을 위해 생략하였다. 분류기 없는 안내[14]의 척도는 기본적으로 8.5로 설정된다. 우리의 실험은 단일 A800 GPU에서 수행된다.\n' +
      '\n' +
      '**Dataset.** FFHQ[16]에서 무작위로 70개의 non-celeb 이미지를 선택하고 테스트 데이터 세트로 \\(512\\times 512\\)으로 크기를 조정한다. 종합적인 평가를 수행하기 위해 동작, 장식, 속성, 표현 및 배경을 포함하는 40개의 테스트 프롬프트를 사용한다[18].\n' +
      '\n' +
      '**Baselines.** 우리는 제안된 방법을 최적화 기반 방법: Textual Inversion[12], DreamBooth[30], Celeb-Basis[45] 및 인코더 기반 방법: ELITE[38], FastComposer[40], IP-Adapter[43]을 포함하는 베이스라인과 비교한다. 우리는 각 방법으로 공개된 공식 모델을 사용하여 우선 순위를 매긴다. Textual Inversion과 DreamBooth의 경우, 우리는 공정한 비교를 위해 그들의 안정적인 확산 2.1 버전을 사용한다.\n' +
      '\n' +
      '**Evaluation Metrics.** Following DreamBooth[30], 우리는 CLIP[27] 시각적 유사성(**CLIP-I**)을 계산하여 높은 수준의 의미 정렬과 텍스트-이미지 유사성(**CLIP-T**)을 평가하여 편집성을 측정한다. 또한, 생성된 이미지와 동일한 ID의 실제 이미지 사이의 검출된 얼굴 영역에 대해 ArcFace[7]에 의한 **Face Similarity**와 LPIPS[46]에 의한 **Face Diversity**[18, 39]를 계산한다. 그러나 일부 기준선은 다양한 텍스트 프롬프트 하에서 완전히 일관되지 않은 얼굴을 생성할 수 있으며, 이는 얼굴 다양성을 잘못 올린다. 따라서 본 논문에서는 **Trusted Face Diversity**를 제안한다.\n' +
      '\n' +
      '도 3: 다양한 타임스텝 \\(t\\)에서 \\(z_{t}\\)으로부터 예측된 \\(\\hat{z}_{0}\\)을 제시한다. \\ (\\hat{z}_{0}\\) at \\(t=\\{100,200\\}\\)에서 \\(t=300\\)과 유사한 \\(t=300\\)은 간결성을 위해 생략된다.\n' +
      '\n' +
      '각 쌍 이미지에 대한 얼굴 유사도 및 얼굴 다양성으로부터 코사인 거리의 곱을 계산하여 생성된 얼굴이 다양하고 유사한지 여부를 평가한다. 생성 품질을 측정하기 위해 70명의 유명인 이름을 무작위로 샘플링하여 테스트 프롬프트를 의사 지상 진리로 사용하여 이미지를 생성하고, 생성된 이미지와 의사 지상 진리 사이의 프레쳇 인셉션 거리(**FID**)[22]를 경쟁 방법으로 계산한다.\n' +
      '\n' +
      '### Comparison\n' +
      '\n' +
      '**Qualitative Comparison.** 그림 4와 같이 입력으로 단일 이미지를 주어 다양한 프롬프트로 생성 결과를 보여준다. 텍스트 역산은 \\(\\mathcal{L}_{noise}\\)만으로 최적화되며, 이는 서로 다른 맥락에서 동일성에 대한 사소한 해결책으로 이어진다. 드림부스는 입력 얼굴에 맞도록 전체 SD 모델을 미세화하지만, 여전히 유사한 아이덴티티(row\\(1_{th},5_{th}\\))를 학습하지 못하고 전경 얼굴(row\\(2_{th},3_{th}\\))을 복제하는 경향이 있다. 인코더 기반 방법 ELITE와 IP-Adapter는 입력 얼굴의 거친 모양과 속성만 학습하고, 신원 보존과 편집성 모두에서 보통의 성능을 수행한다. FastComposer는 CLIP 영상 인코더와 전체 SD를 이용하여 아이덴티티를 학습하지만, 낮은 화질과 아티팩트(row \\(4_{th},5_{th},6_{th}\\)에 시달린다. Celeb-Basis는 또한 재맥락화를 위한 정확한 아이덴티티를 학습하지 못한다(row \\(1_{th},3_{th}\\). 특히, "lattle art of"를 텍스트 프롬프트로 사용할 때, 모든 베이스라인은 일관되지 않은 동일성을 생성하거나 행 \\(6_{th}\\)에서 원하는 스타일을 얻지 못한다. 비교 결과, 제안된 방법의 효용성은 동일성 보존과 편집성 측면에서 우월함을 보인다.\n' +
      '\n' +
      '또한, 표 1의 정량적 비교를 보고한다. ELITE 및 IP-Adapter와 같은 일부 기준선은 얼굴 구조 및 속성만을 학습하고 정면 뷰 생성에 취약하여 CLIP-I가 더 우수하다. 이 메트릭은 높은 수준의 의미 정렬에 초점을 맞추고 동일성 일관성을 무시한다. 따라서 이러한 방법은 얼굴 유사도(24.54, -15.39)와 신뢰할 수 있는 얼굴 다양성(9.66, -3.95)을 얻을 수 있다.\n' +
      '\n' +
      '그림 4: 우리는 다양한 정체성(다양한 인종 포함)과 다양한 텍스트 프롬프트(장식, 행동, 속성, 배경, 스타일 포함)에 대한 6개의 기준선과의 질적 비교를 제시한다. 제안하는 방법은 일관된 동일성과 뛰어난 편집성(Best view_를 위한_Zoom-in)으로 고품질의 생성을 달성한다. 보충 자료에서 더 많은 결과를 제공합니다.\n' +
      '\n' +
      '또한 최적화 기반 방법인 Textual Inversion과 DreamBooth는 재맥락화를 위한 안정적인 아이덴티티를 학습하지 못하고 입력 얼굴에 과도하게 적합하여 신뢰할 수 없는 얼굴 다양성(-4.26, -9.12)으로 이어지는 경향이 있음을 관찰한다. 제안하는 방법은 CLIP-T(vision-language alignment), Face Sim(identity preservation), Div(Trusted Div) 및 FID(image quality)에서 가장 우수한 성능을 보인다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '우리는 표 2와 그림 5, 6과 같이 다양한 설정에 걸쳐 포괄적인 절제 연구를 수행하며, 인코더 기반 방법에서 일반적으로 채택되는 기준선으로 CLIP 이미지 인코더를 사용한다. [33, 40]에 이어서, 우리는 신원 정보를 추출하기 위해 CLIP 인코더의 마지막 계층의 CLS 특징을 사용한다. 도 5의 col 2에서, CLIP 이미지 인코더는 아이덴티티 보존에 대해 보통(Face Sim에서 우리보다 -1.39)인 것을 관찰할 수 있다. 한편, "w/o AdaIN"의 설정은 편집성을 명시적으로 학습할 수 없고, 학습된 워드 임베딩들의 값 범위를 제한하지 못한다. 정면 면을 생성하는 경향이 있으며 원하는 텍스트 프롬프트를 정렬하지 못하므로 얼굴 유사성은 높지만 CLIP-T, 신뢰할 수 있는 부서 및 FID(-4.22, -1.28, -23.81)가 낮다.\n' +
      '\n' +
      '또한, 훈련 손실에 대한 절제 결과를 보여준다. 마스크드 확산 손실은 효과적인 것으로 입증되었으며 [39, 1] 전경에 초점을 맞추고 배경 누출을 방지하는 데 도움이 된다. "Only \\(\\mathcal{L}_{noise}\\)" 설정의 재구성은 우리보다 열등하고 원하지 않는 변화 및 아티팩트(도 6의 Col 3)에 취약하여, 얼굴 Sim., FID에서 우리보다 낮은 아이덴티티 보존 및 이미지 품질(즉, -0.60, -0.84)을 초래한다. Denoise 과정의 초기 단계에서 무의미한 \\(\\mathcal{L}_{rec}\\)으로 인해 "Only \\(\\mathcal{L}_{rec}\\)" 설정은 인공물(그림 6의 4)과 중간 정도의 정체성만 학습하고 불만족스러운 얼굴 유사성, 신뢰된 다양성, FID(-6.43, -1.12, -15.62)로 이어진다. 이에 비해 제안된 마스킹된 2상 확산 손실은 가장 좋은 결과를 보여주며, 분할 파라미터 \\(\\alpha\\)에 대한 논의는 보충 재료에서 찾을 수 있다.\n' +
      '\n' +
      '## 5 Discussion\n' +
      '\n' +
      '### Downstream Applications\n' +
      '\n' +
      '**포즈 제어 맞춤형 이미지 생성** 미리 훈련된 안정적인 확산이 고정되어 있기 때문에 SD 기반 플러그 앤 플레이 모듈이 우리의 방법과 협력할 수 있다. ControlNet은 키포인트, 에지 맵 등과 같은 추가 입력 조건을 지원하기 위해 미리 훈련된 SD를 제어한다. 본 논문에서는 OpenPose[3]를 예로 하여 인간의 뼈대를 조건으로 한 포즈 영상을 얻는다. 도 1의 2행에 도시된 바와 같이,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & CLIP-T\\(\\uparrow\\)(\\%) & CLIP-T\\(\\uparrow\\)(\\%) & Face Sim.\\(\\uparrow\\)(\\%) & Face Div.\\(\\uparrow\\)(\\%) & Trusted Div.\\(\\uparrow\\)(\\%) & FID\\(\\downarrow\\) \\\\ \\hline Textual Inversion & 61.30 & 28.23 & 31.30 & 37.03 & 10.75 & 28.64 \\\\ DreamBooth & 67.01 & 28.91 & 35.80 & 36.79 & 5.89 & 48.55 \\\\ ELITE & 73.94 & 26.43 & 12.58 & 25.55 & 5.35 & 84.32 \\\\ FastComposer & 72.32 & 28.87 & 36.91 & 28.84 & 13.90 & 47.98 \\\\ IP-Adapter & **85.14** & 23.67 & 21.73 & 25.61 & 11.06 & 78.95 \\\\ Celeb-Basis & 63.69 & 27.84 & 25.55 & **37.85** & 13.41 & 33.72 \\\\ StableIdentity (Ours) & 65.91 & **29.03** & **37.12** & 35.46 & **15.01** & **24.92** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 기준선과의 정량적 비교. \\ (\\uparrow\\)은 높은 것이 좋은 반면, \\(\\downarrow\\)은 낮은 것이 좋은 것을 나타낸다. 가장 좋은 결과는 **bold**로 표시됩니다. 제안된 방법은 텍스트 일관성(CLIP-T), 신원 보존(얼굴 유사성), 생성된 얼굴의 다양성(신뢰된 얼굴 다양성) 및 생성 품질(FID)에 대해 최상의 결과를 얻는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & CLIP-T\\(\\uparrow\\) & Face Sim.\\(\\uparrow\\) & Trusted Div.\\(\\uparrow\\) & FID\\(\\downarrow\\) \\\\ \\hline CLIP Enc. & 28.03 & 35.73 & 14.81 & 25.66 \\\\ w/o AdaIN & 24.81 & **47.81** & 13.73 & 48.73 \\\\ w/o Mask & 28.15 & 34.98 & 14.47 & 25.12 \\\\ Only \\(\\mathcal{L}_{noise}\\) & 28.81 & 36.55 & 14.97 & 25.76 \\\\ Only \\(\\mathcal{L}_{rec}\\) & 27.35 & 30.69 & 13.89 & 40.54 \\\\ Ours & **29.03** & 37.12 & **15.01** & **24.92** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 절제 연구. 또한, 다양한 분할 파라미터 \\(\\alpha\\)를 부가 자료에 적용하여 그 결과를 제시한다.\n' +
      '\n' +
      '그림 5: 모델 아키텍처를 위한 절제 연구. CLIP 영상 부호화기를 사용하고 AdaIN을 제거한 결과를 보인다.\n' +
      '\n' +
      '그림 6: 훈련 손실에 대한 절제 연구. 다양한 손실 설정의 시각화 결과를 제시한다.\n' +
      '\n' +
      '본 논문에서는 StableIdentity와 ControlNet(SD2.1 version)을 통합하여 구조제어와 아이덴티티를 동시에 생성하는 방법을 제안한다.\n' +
      '\n' +
      '**Zero-shot Identity-driven Video/3D Generation.** 우리의 방법은 CLIP 텍스트 인코더 사전의 새로운 아이덴티티를 도입하는 것으로 간주될 수 있다. 따라서 이상적으로는 SD 기반 비디오 및 3D 생성 모델이 CLIP 텍스트 인코더를 미세 조정하지 않는 한 학습된 아이덴티티가 이러한 모델에 직접 주입될 수 있다고 믿는다.\n' +
      '\n' +
      'ModelScopeT2V[37]은 SD2.1의 U-Net에 일부 시간적 구조를 도입하고 대규모 데이터 세트 [32, 2, 41]에서 U-Net을 미세화하는 텍스트 대 비디오 생성 모델이다. 그림 1의 3행과 같이 미세 조정 없이 학습된 아이덴티티를 변경되지 않은 CLIP 텍스트 인코더에 삽입하려고 시도하며, 생성된 비디오는 유망한 아이덴티티 보존 및 텍스트 정렬을 보여준다.\n' +
      '\n' +
      'LucidDreamer[19]는 3D Gaussian Splatting[17] 기반의 text-to-3D 생성 파이프라인으로, 우리와 같이 미리 훈련된 SD2.1로 직접 샘플링을 할 수 있다. 따라서 자연스럽게 저희의 방법과 협업할 수 있습니다. 이와 유사한 방법으로 그림 1의 4행과 같이 학습된 아이덴티티를 이 파이프라인에 삽입한다. 생성된 결과는 안정적인 아이덴티티, 높은 충실도 및 기하학적 일관성을 달성한다. "황금관 착용"의 결과는 정밀한 기하학적 구조와 사실적인 색상을 나타내며 "유화로"는 실제 존재하지 않는 3D 초상화 유화인 원하는 스타일을 얻는다.\n' +
      '\n' +
      '전반적으로, 본 논문에서 제안하는 방법은 기존의 텍스트-비디오/텍스트-3D 모델을 사용하여 신속한 일관성 있는 아이덴티티 기반 비디오/3D 생성을 쉽게 가능하게 한다. 우리는 보충 자료에서 비디오/3D에 대한 더 많은 결과를 보여준다.\n' +
      '\n' +
      '### 단어 임베딩 기반 방법 분석\n' +
      '\n' +
      '텍스트 역산, Celeb-Basis 및 우리의 방법이 모두 단어 임베딩 공간에 최적화되어 있다는 점을 고려하여, 우리는 이러한 방법으로 학습된 70개의 임베딩을 다양한 관점에서 추가로 분석한다. 단어 임베딩의 차원을 맞추기 위해 Textual Inversion은 2-word 버전으로, Celeb-Basis는 SD2.1로 구현하여 분석하였다.\n' +
      '\n' +
      '학습된 임베딩과 셀럽 임베딩의 분포 차이를 직관적으로 보여주기 위해 그림 7에서 단어 임베딩을 시각화하기 위해 _t_-SNE[35]를 사용한다. "셀럽 이름"은 수집된 326개의 셀럽 이름에 해당하는 단어 임베딩을 의미한다. 우리의 분포는 더 적은 특이치로 더 작고 유명인사 이름의 실제 분포에 더 가깝기 때문에 최고의 정체성 일관성 편집성을 달성한다는 것을 관찰할 수 있다. 또한, 표 3에서 학습된 임베딩과 학습 시간의 max & min 값을 비교하였는데, 제안하는 방법은 기존의 같은 종류의 방법보다 빠르고, 값 범위가 실제 셀럽 임베딩에 가장 가깝다.\n' +
      '\n' +
      '또한, 이러한 방법의 일반화 능력을 살펴보기 위해, 그림 8의 언급된 3D 세대 파이프라인 LucidDreamer를 이용하여 학습된 아이덴티티 임베딩과 함께 3D 세대 결과를 직접 제시하고, 명사명 "Tom Cruise"를 프롬프트로 사용하여 표준 결과를 제시한다. 명백하게, 우리의 방법은 모든 3D 뷰에서 셀럽과 같은 결과를 달성하며, 이는 더 나아가 학습된 정체성의 안정적이고 강력한 일반화 능력을 보여준다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 어느 곳에나 누구나 투입할 수 있는 맞춤형 생성 프레임워크인 _StableIdentity_를 제안한다. 아이덴티티와 편집성을 사전에 통합하는 모델 아키텍처는 학습된 아이덴티티가 아이덴티티-일관된 재맥락화 능력을 마스터할 수 있게 한다. 또한, 설계된 마스킹된 2상 확산 손실은 학습된 아이덴티티를 보다 안정적으로 가능하게 한다. 광범위한 정량적 및 정성적 실험을 통해 제안된 방법의 우수성을 입증한다. 놀랍게도, 본 방법은 ControlNet과 같은 플러그 앤 플레이 SD 기반 모듈과 직접 작동할 수 있고, 심지어 학습된 아이덴티티를 미세 조정 없이 기성 비디오/3D 생성 모델에 삽입할 수 있어 뛰어난 효과를 낼 수 있다. 우리의 작업이 이미지, 비디오 및 3D 생성 작업에 비해 커스터마이징의 통일에 기여할 수 있기를 바랍니다.\n' +
      '\n' +
      '그림 8: LucidDreamer 기반의 3D 세대 비교. 우리는 유명인사 "Tom Cruise" (prompt)의 결과를 표준으로 제시하고, 경쟁방법(Best view_를 위한_Zoom-in)에서 학습한 임베딩 \\([v_{1}^{*},v_{2}^{*}]\\)을 사용한 결과를 보인다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & Training time & Max & Min \\\\ \\hline Celeb names & \\(-\\) & 0.0551 & -0.0558 \\\\ Textual Inversion & 43mins & 0.7606 & -0.9043 \\\\ Celeb-Basis & 8mins & 0.1592 & -0.1499 \\\\ StableIdentity (Ours) & 4mins & 0.0557 & -0.0520 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 학습된 임베딩의 학습 시간, 최대값 및 최소값에 대한 단어 임베딩 공간에서 최적화된 베이스라인과의 비교.\n' +
      '\n' +
      '그림 7: Celeb 이름, Textual Inversion, Celeb-Basis 및 우리의 방법과 함께 _t_-SNE를 사용한 단어 임베딩의 2-D 시각화.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Omri Avrahami, Kfir Aherman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from a single image. _arXiv preprint arXiv:2305.16311_, 2023.\n' +
      '* [2] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1728-1738, 2021.\n' +
      '* [3] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7291-7299, 2017.\n' +
      '* [4] Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing Liu, Kang Du, et al. Photoverse: Tuning-free image customization with text-to-image diffusion models. _arXiv preprint arXiv:2309.05793_, 2023.\n' +
      '* [5] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. _arXiv preprint arXiv:2307.09481_, 2023.\n' +
      '* [6] Zhuowei Chen, Shancheng Fang, Wei Liu, Qian He, Mengqi Huang, Yongdong Zhang, and Zhendong Mao. Dreamidentity: Improved editability for efficient face-identity preserved image generation. _arXiv preprint arXiv:2307.00300_, 2023.\n' +
      '* [7] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4690-4699, 2019.\n' +
      '* [8] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-to-image generation via contrastive prompt-tuning. _arXiv preprint arXiv:2211.11337_, 2022.\n' +
      '* [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* [10] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style. _arXiv preprint arXiv:1610.07629_, 2016.\n' +
      '* [11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12873-12883, 2021.\n' +
      '* [12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. _arXiv preprint arXiv:2208.01618_, 2022.\n' +
      '* [13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.\n' +
      '* [14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.\n' +
      '* [16] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4401-4410, 2019.\n' +
      '* [17] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM Transactions on Graphics_, 42(4), 2023.\n' +
      '* [18] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. _arXiv preprint arXiv:2312.04461_, 2023.\n' +
      '* [19] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. _arXiv preprint arXiv:2311.11284_, 2023.\n' +
      '* [20] Yang Liu, Cheng Yu, Lei Shang, Ziheng Wu, Xingjun Wang, Yuze Zhao, Lin Zhu, Chen Cheng, Weitao Chen, Chao Xu, et al. Facechain: A playground for identity-preserving portrait generation. _arXiv preprint arXiv:2308.14256_, 2023.\n' +
      '* [21] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis with multiple subjects. _arXiv preprint arXiv:2305.19327_, 2023.\n' +
      '* [22] Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal? a large-scale study. _arXiv preprint arXiv:1711.10337_, 2017.\n' +
      '* [23] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_, 2023.\n' +
      '* [24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.\n' +
      '* [25] Karl Pearson. Liii. on lines and planes of closest fit to systems of points in space. _The London, Edinburgh_,and Dublin philosophical magazine and journal of science_, 2(11):559-572, 1901.\n' +
      '* [26] Joern Ploennigs and Markus Berger. Ai art in architecture. _AI in Civil Engineering_, 2(1):8, 2023.\n' +
      '* [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.\n' +
      '* [30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.\n' +
      '* [31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. _arXiv preprint arXiv:2307.06949_, 2023.\n' +
      '* [32] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [33] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instanbooth: Personalized text-to-image generation without test-time finetuning. _arXiv preprint arXiv:2304.03411_, 2023.\n' +
      '* [34] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* [35] Laurens Van Der Maaten. Accelerating t-sne using tree-based algorithms. _The journal of machine learning research_, 15(1):3221-3245, 2014.\n' +
      '* [36] Mei Wang and Weihong Deng. Deep face recognition: A survey. _Neurocomputing_, 429:215-244, 2021.\n' +
      '* [37] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Model-elscope text-to-video technical report. _arXiv preprint arXiv:2308.06571_, 2023.\n' +
      '* [38] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. _arXiv preprint arXiv:2302.13848_, 2023.\n' +
      '* [39] Zijie Wu, Chaohui Yu, Zhen Zhu, Fan Wang, and Xiang Bai. Singleinsert: Inserting new concepts from a single image into text-to-image models for flexible editing. _arXiv preprint arXiv:2310.08094_, 2023.\n' +
      '* [40] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multi-subject image generation with localized attention. _arXiv preprint arXiv:2305.10431_, 2023.\n' +
      '* [41] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5288-5296, 2016.\n' +
      '* [42] Yuxuan Yan, Chi Zhang, Rui Wang, Yichao Zhou, Gege Zhang, Pei Cheng, Gang Yu, and Bin Fu. Facestudio: Put your face everywhere in seconds. _arXiv preprint arXiv:2312.02663_, 2023.\n' +
      '* [43] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. _arXiv preprint arXiv:2308.06721_, 2023.\n' +
      '* [44] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In _Proceedings of the European conference on computer vision (ECCV)_, pages 325-341, 2018.\n' +
      '* [45] Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang Qi, Xintao Wang, Ying Shan, and Huicheng Zheng. Inserting anybody in diffusion models via celeb basis. _arXiv preprint arXiv:2306.00926_, 2023.\n' +
      '* [46] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_, 2018.\n' +
      '* [47] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. Text-to-image diffusion model in generative ai: A survey. _arXiv preprint arXiv:2303.07909_, 2023.\n' +
      '* [48] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3836-3847, 2023.\n' +
      '* [49] Yiqun Zhang, Zhenyue Qin, Yang Liu, and Dylan Campbell. Detecting and restoring non-standard hands in stable diffusion generated images. _arXiv preprint arXiv:2312.04236_, 2023.\n' +
      '\n' +
      '## 부록 더 많은 시각적 결과\n' +
      '\n' +
      '**보다 커스터마이징된 결과.** 도 9에 도시된 바와 같이, _StableIdentity_는 다양한 컨텍스트들에서 상이한 인종들을 처리할 수 있다는 것이 관찰될 수 있다. 한편, 그림 10에서 다양한 예술적 스타일을 가진 맞춤형 결과를 보여주며, 생성된 결과들은 전반적으로 만족스러운 아이덴티티 보존성과 편집성을 가지고 있어 제안된 방법의 유효성을 입증한다.\n' +
      '\n' +
      '**StableIdentity & Image/Video/3D Models.** 도 12, 13에 도시된 바와 같이 ControlNet[48], ModelScopeT2V[37], LucidDreamer[19]로 보다 많은 이미지/비디오/3D 맞춤형 생성 결과를 보여주고 있다. Sec. 4.1에서 소개한 바와 같이, StableIdentity는 CLIP 텍스트 인코더 사전의 새로운 Identity를 도입하는 것으로 간주될 수 있다. 따라서, 학습된 아이덴티티는 다양한 컨텍스트에 자연스럽게 삽입되거나 아이덴티티 기반 맞춤형 생성을 위한 비디오/3D 생성 모델에도 삽입될 수 있다. 3D 생성의 제한된 성능으로 인해 [19]에 이어 머리 영역에서만 생성하고 편집하므로 학습된 아이덴티티가 성공적으로 삽입되었는지 여부를 명확하게 입증할 수 있다. 실험 결과, 제안한 방법이 이미지/비디오/3D 생성 모델에 안정적으로 주입되어 동일성 일치 재검증을 달성할 수 있음을 보였다. 또한, 14와 같이 셀럽 사진을 입력으로 하여 보다 맞춤형 생성 결과를 보여준다.\n' +
      '\n' +
      '**더 비교.** 그림 15와 같이 장식, 동작, 배경, 스타일에 대한 기준선과 추가로 비교한다. 명백히, 우리는 신원 보존, 편집성 및 이미지 품질에 대한 최상의 결과를 달성한다. 잘 수행하는 것 같은(행 1,3,4) 드림부스는 입력 이미지에 대한 오버핏 또는 타겟 아이덴티티와 유사한 결과를 생성하지 못한다.\n' +
      '\n' +
      '## 부록 B 구현 상세\n' +
      '\n' +
      '**셀럽 이름을 필터링합니다** Sec에서 언급한 대로입니다. 2, Celeb-Basis[45]는 Stable Diffusion[29]에서 편집 가능한 691명의 유명인 이름을 수집한다. 우리는 두 단어로만 구성된 이름만 걸러내고 해당 토큰의 개수를 세어낸다. 2단어의 백분율\\(\\rightarrow\\){2,3,4,5,6}\n' +
      '\n' +
      '도 9: 다양한 컨텍스트들(커버링 데코레이션, 액션, 속성) 하에서 상이한 아이덴티티들(다양한 인종들을 포함함)에 대해 제안된 _StableIdentity_로 더 생성된 결과들.\n' +
      '\n' +
      '토큰은 각각 56%, 27%, 13%, 3%, 0.3%이다. 보다 정확한 사전 공간을 얻기 위해 샘플링 포인트가 더 많은 2단어\\(\\rightarrow\\)2 토큰의 설정을 선택한다.\n' +
      '\n' +
      '**Division Parameter \\(\\alpha\\.**) Figure 11과 같이 \\([0,0.1,\\cdot\\cdot\\cdot,1]\\)에서 다른 \\(\\alpha\\)의 효과를 제시한다. 경험적으로 \\(\\alpha\\in[0.4,0.6]\\)은 더 나은 아이덴티티 보존성, 편집성 및 이미지 품질을 보여준다. \\(\\alpha\\)가 커지면 무의미한 재구성이 성능 저하로 이어질 것이다.\n' +
      '\n' +
      '## 부록 C 제한사항\n' +
      '\n' +
      '제안된 방법은 새로운 아이덴티티의 커스터마이징 생성을 위한 뛰어난 성능을 달성하고 기성품 이미지/비디오/3D 모델과 협업할 수 있지만,\n' +
      '\n' +
      '그림 11: 분할 매개변수 \\(\\alpha\\)에 대한 매개변수 분석.\n' +
      '\n' +
      '그림 10: 다양한 예술적 스타일에 대해 _StableIdentity_로 추가 맞춤형 결과.\n' +
      '\n' +
      '여전히 몇 가지 한계에 직면해 있습니다. (1) 단어 임베딩 공간에서만 동작하고, SD(Stable Diffusion)를 고정시키기 때문에, SD의 우수한 성능뿐만 아니라, 손의 이상 현상(49)과 같은 몇 가지 단점을 계승한다. (2) 기존의 텍스트-비디오 생성 모델은 다양한 컨텍스트를 가지고 생성할 수 있지만, 인간 중심 생성에 대해서는 여전히 미성숙하다[37]. 비디오 커스터마이징 생성을 위한 제한된 성능으로 이어집니다.\n' +
      '\n' +
      '도 12: 포즈-제어 맞춤형 이미지 생성(StableIdentity & ControlNet) 및 제로-샷 아이덴티티-구동 맞춤형 비디오 생성(StableIdentity & ModelScopeT2V).\n' +
      '\n' +
      '도 13: Zero-shot identity-driven customized 3D generation(StableIdentity & LucidDreamer). 4.1절에서 언급한 바와 같이, 우리는 간결함을 위해 "안경을 착용하는\\(v_{1}^{*}\\)와 같은 프롬프트의 자리 표시자\\(v_{1}^{*}\\)\\(v_{2}^{*}\\)을 생략하였다. 여기서, 학습한 아이덴티티에 대한 3차원 복원을 보여주기 위해 입력 프롬프트로서 "\\(v_{1}^{*}\\)\\(v_{2}^{*}\\)"을 사용한다.\n' +
      '\n' +
      '그림 14: 셀럽 사진을 입력으로 더 많은 이미지/비디오/3D 맞춤형 생성 결과.\n' +
      '\n' +
      '도 15: 다양한 텍스트 프롬프트(장식, 동작, 배경, 스타일)를 갖는 상이한 아이덴티티(다양한 인종 포함)에 대한 보다 정성적 비교. 본 논문에서 제안한 방법은 아이덴티티 보존과 편집성을 위해 가장 좋은 성능을 보인다 (_Zoom-in for the best view_).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>