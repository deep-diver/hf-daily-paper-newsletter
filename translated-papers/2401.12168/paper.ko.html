<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '비전 언어 모델(VLM)은 이미지 캡션, 시각적 질문 응답(VQA), 체화된 계획, 액션 인식, 그리고 더 많은 [2, 18, 25, 33]을 포함한 다양한 과제에 걸쳐 최근 몇 년 동안 상당한 진전을 이루었다. VLM은 광범위한 과제에 대한 강력한 범용 모델이지만 대부분의 최첨단 VLM은 여전히 _spatial_ 추론, 즉 3D 공간에서 물체의 위치 또는 그들 사이의 공간적 관계를 이해하는 데 필요한 작업과 투쟁한다. 공간적 추론 능력은 자신의 권리에 유용하지만 로봇 공학이나 AR과 같은 하류 응용 분야에 대해서도 유용하다. 예를 들어, 공간 추론-임베드 VLM은 보다 나은 범용 보상 주석[54] 및 성공 검출기[19]로 사용될 수 있다.\n' +
      '\n' +
      'VLM과 같은 기반 모델의 탐색은 종종 인간의 능력에 의해 영감을 받았다. 인간은 체화된 경험과 진화적 발달을 통해 타고난 공간적 추론 능력을 가지고 있다. 우리는 복잡한 연쇄 또는 정신적 계산 없이 서로에 대한 물체의 위치 결정 또는 거리 및 크기를 추정하는 것과 같은 공간적 관계를 쉽게 결정한다. 직접 공간 추론 작업의 이러한 자연적 숙련도는 VLM의 현재 한계와 대조되므로 공간 추론의 여러 단계를 필요로 하는 실제 과제를 달성하는 것을 방지한다. 이러한 격차는 우리가 강력한 연구 질문으로 이어지며, 우리는 인간의 것과 유사한 공간적 추론 능력을 가진 VLM을 모방할 수 있는가?\n' +
      '\n' +
      '따라서 현재 VLM의 공간적 추론 능력이 제한된 것은 건축의 근본적인 제한 때문이 아니라 그러한 모델이 훈련되는 규모에서 사용할 수 있는 공통 데이터 세트의 한계라고 가정한다. 예를 들어, 많은 VLM[12, 18, 44]는 제한된 공간 정보를 포함하는 이미지-선택 쌍[13]을 특징으로 하는 인터넷 규모의 데이터세트 상에서 훈련된다. 이는 부분적으로 공간 정보가 풍부한 체화된 데이터를 얻거나 3D 인식 질의에 대한 고품질 인간 주석을 얻는 데 어려움이 있기 때문이다.\n' +
      '\n' +
      '자동 데이터 생성 및 증강 기술은 데이터 제한 문제(38, 53, 56, 66])를 처리하기 위한 하나의 접근법이다. 그러나 대부분의 이전 데이터 생성 노력은 지상 진리 의미 주석으로 광학적 이미지를 렌더링하는 데 초점을 맞추고 있지만 물체의 풍부함과 3D 관계를 간과한다. 이와는 대조적으로 진정한 3D 세계의 다양성과 복잡성을 포착하기 위해 실제 세계 데이터에서 직접 공간 정보를 추출하는 데 중점을 둔다.\n' +
      '\n' +
      '우리의 핵심 통찰력은 최근 시크릿 비전 모델의 발전이 2D 이미지에서 풍부한 3D 공간 주석을 자동으로 생성할 수 있다는 것입니다. 이를 위해, 우리는 VLM의 데이터 생성 및 훈련이 공간적 추론 능력을 향상시킬 수 있도록 하는 공간VLM이라는 시스템을 제안한다. 구체적으로는, 1) 개방 핵 검출, 2) 메트릭 깊이 추정, 3) 의미 세분화 및 4) 객체 중심 자막 모델을 결합하여 실제 데이터를 대규모로 조밀하게 주석할 수 있다. 공간VLM은 비전 모델에 의해 생성된 데이터를 포맷으로 변환하여 캡션, VQA 및 공간 추론 데이터의 혼합물에서 VLM을 훈련시키는 데 사용할 수 있다.\n' +
      '\n' +
      '실험을 통해 훈련된 VLM은 많은 바람직한 능력을 나타낸다. 먼저 질적 공간 질문에 답하는 능력이 크게 향상된다. 둘째, 시끄러운 훈련 데이터에도 불구하고 안정적으로 정량적 추정을 수행할 수 있다. 이러한 능력은 대상 크기에 대한 상식적 지식을 제공할 뿐만 아니라 재배열 작업에 대한 개방형 보상 주석으로 유용하게 사용된다. 셋째, 우리는 자연 언어 인터페이스에서 혜택을 받는 이 공간 비전 언어 모델을 발견하여 강력한 대어 모델과 결합할 때 복잡한 공간 추론 작업을 해결하기 위해 공간 사슬을 수행할 수 있다.\n' +
      '\n' +
      '우리의 주요 기여는요.\n' +
      '\n' +
      '* We 엔도ow VLM 정량적 공간 추론 능력은 인간의 기본 능력이다.\n' +
      '\n' +
      '* 우리는 인터넷 스케일에서 현실 세계 이미지를 기반으로 3D 공간 추론 VQA 데이터를 자동으로 레이블링하는 프레임워크를 설계한다.\n' +
      '* 우리는 데이터 품질, 훈련 파이프라인, 동결/무사용 시각 인코더 등 다양한 학습 레시피를 연구하고 학습 품질에 어떤 영향을 미치는지를 조사한다.\n' +
      '* 우리는 도입된 과제 및 방법에 의해 잠금 해제된 복잡한 추론 및 로봇 공학에서 공간VLM의 새로운 능력을 보여준다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '공간적 이유의 공간적 거리 추정을 배우는 것은 전통적으로 SLAM[8, 21] 또는 깊이 추정[24]과 같은 광범위한 작업의 일부로 다루어져 왔다. 이러한 공간 개념을 추론에 적용할 때, 선행 작업은 종종 명시적인 공간 장면 기억[27, 28] 또는 공간 장면 그래프[31, 32, 62, 63]에 초점을 맞추고 있다. 스펜 그래프는 그들이 인코딩하는 공간 구조에 기초하여 해석 가능하고 구조화되고 통계적 관계 학습을 허용한다. VQA 형식의 공간적 문제에 답하기 위해서는 그 장면 그래프에 대한 경로 조사 문제로 명시적으로 다루어야 한다. 반면에 VLM은 비전-언어 데이터 세트의 느슨하게 구조화된 많은 정보를 전처리한다. 공간적 이해는 장면 그래프와 달리 _implicitly_를 암호화한다. 우리는 깊이 및 3D 구조를 보조 태스크[36, 47]로 가중치에 추론할 수 있다. 우리의 작업에서 우리는 명시적인 기본 장면 그래프 없이 VLM에서 직접 공간적 관계 문제를 다룬다. 질적 측면에서 상대적 관계를 이해하는 것 외에도 장면에서 객체 간의 명시적 메트릭 거리 관계를 추정하는 것도 탐구한다.\n' +
      '\n' +
      '그라운드 비전-언어 모델(LLM)은 인터넷 규모의 데이터에 대해 학습되어 효과적인 커먼센스 이주가 된다. 그러나 LLM(및 확장 VLM)은 사회적 추론[42], 물리적 추론[26], 물리 추론[46] 체화된 과제[1, 34, 58], 공간 추론 과제[44, 55]에서 잘 수행할 필요 정서가 부족할 수 있다. 인터랙티브 세계 경험을 가진 언어 모델은 접지 개선(67, 70)을 보여주지만 펙밍고[2], PaLI[12], PaLM-E[18] 등 대형 비전 모델의 도입은 성능의 도약을 가능하게 했다. 이러한 시각적으로 조정된 모델은 로봇 성공 검출 [18, 20, 57, 68], 액션 예측[7, 59], 보상 예측[16, 23, 48, 50]과 같은 여러 다운스트림 작업에 사용되었다. 이 작업에서 생성된 VQA 데이터 세트에 VLM을 가속화하는 것을 통해 공간적 추론의 문제에 접근한다. 이 작업에 대해 VLM을 직접 조달함으로써, 우리는 기본 VLM의 일반성과 추론 능력을 계승하고 이러한 접근법이 보상 생성과 같은 작업이 가능한 방법을 보여준다.\n' +
      '\n' +
      '비전-언어 데이터베이스에서의 공간 정보는 VQA(예: VQAv2[29], OK-VQA[49], COCO[43], 또는 비주얼 유전체[39]와 같은 과제를 고려하여 VLM[61, 69] 벤치마킹에 초점을 맞추었다. 다른 사람들은 의미 세분화 [5, 37], 객체 검출[11], 또는 객체 식별[15, 60]과 같은 미세 구성 장면 이해에 초점을 맞추었다. 다른 사람들은 실제 [44, 55] 또는 시뮬레이션된 [35] 장면에서 객체 공간 관계(예: 위의, 왼쪽, 오른쪽)에 대한 질문에 답하는 작업으로서 공간적 추론에 특별히 초점을 맞추었다. 이 도메인의 실제 데이터는 인간 라벨러에 의해 생성된 양에 의해 제한될 수 있는 반면 합성 데이터는 본질적으로 결합되는 표현성에 의해 제한된다. 이 작업에서 우리는 실제 데이터를 자동으로 생성하는 방법을 고려하고 공간 관계뿐만 아니라 여러 다운스트림 작업에 직접 적용될 수 있는 메트릭 공간 거리의 문제에 초점을 맞춘다.\n' +
      '\n' +
      '## 3 SpatialVLM\n' +
      '\n' +
      'VLM을 질적으로나 양적으로 공간 추론 능력을 모두 갖추기 위해 VLM을 훈련시키는 데 사용되는 대규모 공간 VQA 데이터세트를 생성할 것을 제안한다. 플라스미하게, 우리는 먼저 개방형 구조적 검출, 메트릭 깊이 추정, 의미론적 세분화 및 객체 중심 자막 모델을 포함하는 오프-쉘 컴퓨터 비전 모델을 변환한 다음 템플릿 기반 접근법을 채택하여 객체 중심 컨텍스트를 추출한 다음 합리적인 품질의 대규모 공간 VQA 데이터를 생성하는 포괄적인 데이터 생성 프레임워크를 설계한다. 생성된 데이터 세트를 사용하여 공간VLM을 훈련하여 직접적인 공간 추론 능력을 학습한 다음 LLM에 내장된 고수준의 커먼센스 추론과 결합하여 체인의 공간적 추론을 해제할 수 있다.\n' +
      '\n' +
      '2D 임팩트입니다.\n' +
      '\n' +
      '오늘날의 VLM의 공간적 추론 능력이 부족한 이유는 건축이 아니라 공간적 추론 훈련 자료가 부족하다는 가설을 세웠다. 이러한 통찰에 따라 공간 추론 질문을 포함하는 VQA 데이터를 생성하는 파이프라인을 설계한다. 파이프라인은 그림 2에 정리되어 다음과 같이 상세하게 기술되어 있다.\n' +
      '\n' +
      '인터넷 규모의 이미지 선택 데이터 세트는 VLM 트레이닝[12]에 널리 사용되었지만 이러한 데이터 세트의 많은 이미지는 단일 오브젝트로 구성되거나 장면 배경(예: 쇼핑 웹사이트 또는 컴퓨터 화면의 스크린샷)이 없기 때문에 공간 추론 QA를 합성하는 데 적합하지 않다. 따라서 데이터 합성 파이프라인의 첫 번째 단계로 CLIP 기반 개방형 동물 분류 모델을 채택하여 모든 이미지를 분류하고 적합하지 않은 이미지를 배제한다.\n' +
      '\n' +
      '그림 2: ** 데이터 합성 파이프라인의 개요**(a) 우리는 CLIP를 사용하여 시끄러운 인터넷 이미지를 걸러내고 장면 수준 사진만 유지한다. (b) 사물 중심 세분화, 깊이 및 자막을 얻기 위해 인터넷 규모의 이미지에 미리 훈련된 전문가 모델을 적용한다. (c) 2D 이미지를 3D 포인트 구름으로 들어 올려 형상 분석 규칙에 의해 파싱되어 3D 바운딩 박스처럼 유용한 특성을 추출할 수 있다. (d) CLIP 유사도 점수(e)를 사용하여 객체 캡션을 클러스터링하여 모호한 질문을 피하기 위해, 우리는 객체 캡션 및 추출된 속성들로부터 수백만 개의 공간적 질문과 답변을 합성한다.\n' +
      '\n' +
      '2D 이미지로부터 객체 중심 공간 컨텍스트를 추출하기 위해, 우리는 객체 중심 정보를 추출하기 위해 지역 제안, 지역 캡션[4], 의미 세분화[41] 모듈을 포함한 일련의 오프 스테프 전문가 모델을 레버리지한다. 이 단계를 통해 픽셀 군집으로 구성된 객체 중심 엔티티와 개방형 변형 자막 설명을 얻는다.\n' +
      '\n' +
      '객체 검출 및 바운딩 박스 위치 결정[40]을 사용하여 생성된 2D 콘텍스트를 3D 콘텍스트로 바꾸는 전통적인 공간 VQA 데이터 세트는 2D 이미지 평면(깊이 또는 고도 맥락의 부족) 및 픽셀 수준 추론( 메트릭 규모의 크기 및 거리 맥락의 부족)으로 제한된다. 우리는 단안 2D 픽셀을 메트릭 규모의 3D 포인트 클라우드로 리프트하기 위해 깊이 추정[6]을 수행한다. 우리는 점 클라우드의 카메라 좌표계를 수평 표면(예: "부어", "테이블 탑" 분할 [9] 및 프레임 전달에 의해 수행되는 지형 좌표계로 추가로 표준화한다. 알고 있는 범위 내에서는, 우리는 인터넷 규모의 이미지를 객체 중심 3D 포인트 클라우드에 최초로 업로드하고 이를 사용하여 3D 공간 추론 감독이 내장된 VQA 데이터를 합성한다.\n' +
      '\n' +
      '시력 해결은 때때로 하나의 이미지에는 유사한 범주의 여러 개 물체가 있어 자막 라벨의 모호성으로 이어진다. 예를 들어, 동일한 캡션 레이블 "케이이크"는 동일한 영상에서 여러 개의 다른 케이크를 의미할 수 있다. 따라서 이러한 객체에 대해 질문을 할 수 있기 전에 참조 표현이 모호하지 않도록 해야 한다. 우리는 이 문제를 해결하는 데 효과적인 경험적으로 검증된 두 가지 주요 디자인 선택을 했다.\n' +
      '\n' +
      '* 우리는 의도적으로 "케이이크"와 같은 고정 및 거친 범주를 생성하는 경향이 있는 공통 객체 검출기를 피하고, FlexCap[4], 사용자 구성 가능한 객체 중심 자막 접근법을 채택하도록 선택한다. 실제로, 각 객체에 대해 \\(1-6\\) 단어 간의 가변 길이의 무작위 자막을 샘플링할 수 있다. 그 결과, 저희 객체 주석은 "집 모양케이크"와 "플라스틱 용기에 있는 쿠푸 케이크"와 같이 미세하게 구성되어 있습니다.\n' +
      '* 우리는 객체 캡션을 증가시키거나 거부함으로써 모호성을 더욱 제거하는 의미 지향적 후처리 알고리즘을 설계한다. 이 알고리즘의 자세한 내용은 부록 A.2에 나와 있다.\n' +
      '\n' +
      'VQA Datasetet. VQA Datasetial Reasoning VQA.\n' +
      '\n' +
      '제3절에서 동기 부여로서 합성 데이터를 사전 조작하여 "스트레이트포워드" 공간 추론 능력을 VLM에 주입하는 데 중점을 둔다. 따라서 이미지에 두 개 이상의 객체(A)와 "B"를 포함하지 않는 공간-합리적 QA 쌍을 합성하고 다음과 같은 두 가지 범주의 질문을 고려한다.\n' +
      '\n' +
      '질적 질문: 일부 공간 관계에 대한 판단을 요청하는 것. "좌측으로 더 많은 2개의 물체 A와 B를 감안할 때, "객체 A가 물체 B보다 더 상승합니까?"와 "A, B 중 폭이 더 큰 것"을 들 수 있다.\n' +
      '\n' +
      '정량적 질문 : 숫자와 단위를 포함하는 보다 미세한 답을 요구하는 질문이다. 그 예로는 "좌측 대상 A가 물체 B에 비해 얼마인가", "B로부터 대상 A가 얼마나 먼가?", "이 같은 질문은 주요 질문 템플릿을 사용하여 B 뒤에 얼마나 멀리 위치하는지 알아내고, 그러한 질문은 불변 후 객체 캡션을 사용하여 객체 이름 엔트리들을 채울 수 있다. 이 특성은 지시 튜닝 작품[64]에 의해 일반적으로 채택된 접근 방식인 템플릿 기반 생성을 수행할 수 있게 한다.\n' +
      '\n' +
      '질문에 대한 답변은 우리가 개발하는 적절한 기능을 통해 얻어지며, 이는 해당 물체의 분절된 포인트 구름과 3D 바운딩 박스를 입력함에 따라 이루어진다.\n' +
      '\n' +
      '우리는 각각 약 20개의 질문 템플릿과 10개의 답변 템플릿을 특징으로 하는 38개의 다양한 유형의 정성적 및 정량적 공간 추론 질문을 지정한다(우리는 부록 A.3에서 예를 보여준다). 또한 간결한 답을 장려하기 위해 표본 추출 편향을 추가합니다. 마지막으로 인간 같은 방식으로 숫자 라운딩을 만들기 위해 부록 A.2에서 인간 정렬 라운딩 메커니즘을 소개합니다. 이러한 접근법을 사용하여 웹리 및 vqa 데이터셋에서 단안 카메라 이미지에 대한 데이터 쌍에 대한 충분한 질문을 생성할 수 있다. 그림 3은 우리가 얻은 몇 가지 예시 합성 질문 답 쌍을 보여준다. 총 1천만 개의 이미지와 20억 개의 직접적인 공간 추론 QA 쌍으로 대규모 데이터 세트를 생성하여 50%의 질적 질문과 50%의 정량적 질문을 특징으로 한다. 객체 캡션 및 거리 단위의 다양성 덕분에 합성 데이터 세트는 객체 설명, 질문 유형 및 필링 측면에서 상당한 다양성을 특징으로 한다.\n' +
      '\n' +
      '공간적 이유요.\n' +
      '\n' +
      '다음과 같이 정의되는 직접 공간연설기는 비전-언어 모형이 공간 작업의 이미지 \\(\\mathcal{I}\\)와 질의 \\(\\mathcal{Q}\\)로 입력되고, 외부 도구를 사용하거나 다른 대형 모델과 상호작용하지 않고 텍스트 문자열의 형식으로 답변 \\(\\mathcal{A}\\)을 출력한다. 우리는 PaLM [14] 백본을 PaLM 2-S[3], 더 작은 변이체로 대체하는 것을 제외하고는 PaLM-E[18]의 동일한 아키텍처 및 훈련 절차를 채택한다. 그런 다음 원래 PaLM-E 데이터 세트와 데이터 세트의 혼합물을 사용하여 모델을 훈련하고 토큰의 5%가 공간 추론 작업에 전념했다. PaLM-E와 유사하게, 우리의 방법은 결합 시 VQA와 기본 체화된 계획을 수행할 수 있는 능력을 가지고 있다. 주요 차이점은 이진 서술어와 정량적 추정 모두에 대한 공간적 추론 질문에 답할 수 있다는 것이다.\n' +
      '\n' +
      '많은 현실 세계 과제에 대한 사상적 사유는 공간적 추론의 여러 단계를 필요로 한다. 예를 들어, 객체 A가 객체 B에 들어갈 수 있는지 여부를 결정하기 위해, 이유를 이유해야 할 것이다.\n' +
      '\n' +
      '그림 3: ** 합성 데이터세트**의 샘플 데이터 엔트리입니다. 비전 전문가 모델의 출력을 감안할 때 데이터 세트의 다양성을 강조하기 위해 정량적 질문 응답 쌍과 질적 질문 응답 쌍을 모두 생성하기 위해 질문 생성 템플릿 세트를 따른다. 공간 개념은 파란색으로 강조됩니다. 이러한 시각적 질문-답변 쌍은 데이터셋에 대한 다른 자막 또는 질의응답과 쉽게 혼합되어 동일한 학습목적을 사용할 수 있다.\n' +
      '\n' +
      '제한 및 제약을 제공합니다. 때때로 지상의 공간 개념(예: 이미지의 카운터가 높이 1미터)과 상식 지식(유아에게 도달할 수 없다는 것)에 대해 이유할 필요가 있다. 공간적 VLM은 강력한 LLM과 결합할 때 복잡한 공간 추론을 수행할 수 있는 접지된 개념들과 질의하기 위해 _천연 언어_ 인터페이스를 제공한다.\n' +
      '\n' +
      '우리는 이 방법 **"사상의 이유"**라고 부른다. 우리의 합성된 데이터는 직접적인 공간적 추론 질문만 포함되지만 VLM이 함께 구성하기 쉽기 때문에 다합 사슬 추론이 필요한 복잡한 질문을 해결하기 쉽다. 소크라테스 모델[71] 및 LLM의 방법과 유사하게 코디네이터[10]으로 LLM(텍스트-다빈치-003)을 사용하여 LLM과 조정 및 소통하여 그림 4와 같이 스패티VLM과 복잡한 문제를 해결하며, LLM은 복잡한 질문을 간단한 질문으로 분해하여 VLM을 질의하고 그 결과를 도출할 수 있다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '다음의 질문에 답하기 위한 실험을 진행합니다.\n' +
      '\n' +
      '**Q1**의 공간 VQA 데이터 생성 및 훈련 파이프라인은 VLM의 일반적인 공간 추론 능력을 향상시키나요? 그리고 그것이 얼마나 잘 수행됩니까?\n' +
      '\n' +
      '***Q2****는 시끄러운 합성 공간 VQA 데이터와 서로 다른 학습 전략이 학습 성과에 어떠한 영향을 미치는가?\n' +
      '\n' +
      '***Q3***는 "직접적인" 공간 추론 능력이 장착된 VLM이 연쇄 추론 및 체화된 계획과 같은 새로운 능력을 해제하는가?\n' +
      '\n' +
      '우리는 PaLM-E 훈련 세트와 공간 VQA 데이터 세트의 혼합물을 사용하여 모델을 훈련한다. VLM의 공간적 추론 제한 사항이 데이터 문제인지 확인하기 위해, 우리는 다음의 최첨단 VLM을 기저부로 선택하고 의미-선택 작업이 무거운 가중치를 차지하고 공간적 VQA 데이터셋이 없는 혼합물에서 훈련했다.\n' +
      '\n' +
      '***GPT-4V1** GPT-4V는 다중 모드 입력을 지원하는 GPT-4 [51] 버전으로, 많은 비전 언어 작업에서 최첨단 성능을 달성한다.\n' +
      '\n' +
      '그림 4: **Chain-topt 공간 추론. 우리는 공간VLM과 함께 사상 공간 추론을 수행할 수 있음을 보여준다. 이 예에서, 시스템은 LLM 내부VLM의 도움으로 "블루 코크가 가능한 경우, 적색 콜라가 가능하고, 테이블 위의 녹색 스펀지가 대략 정삼각형을 형성한다."와 같은 질문에 답할 수 있다.\n' +
      '\n' +
      '**PaLI**[12]. 다중 수정 체인에 대해 훈련된 인코더-디코더 VLM은 캡션 및 시각적 질의 응답 작업에 대한 최첨단 성능을 보여준다. 우리는 실험에서 PaLI-X 55B 변이체를 사용했다.\n' +
      '\n' +
      '**PaLM-E**[18]. VLM은 로봇 데이터뿐만 아니라 인터넷 규모의 비전, 언어 및 비전 언어 데이터에 대해 훈련했다. OKVQA 벤치마크에서 최첨단 성능과 로봇 기획 작업을 수행할 수 있음을 보여준다. 실험 전반에 걸쳐 PaLM-E 12B를 사용했다.\n' +
      '\n' +
      '***PaLM 2-E** 바닐라 PaLM 2-E는 동일한 훈련 절차를 갖지만 보다 최근의 LLM 백본으로 업데이트된 PaLM-E[18] 버전이다. 패티알VLM과의 공유 네트워크 아키텍처 및 훈련 절차로 인해 바닐라 PaLM 2-E는 자연스럽게 기준선으로 작용하여 생성된 데이터의 효과를 연구한다. 나머지 논문에서 구체적으로 언급되지 않는 한 PaLM 2-E는 PaLM 2 기술 보고서 [3]에서 명명 협약 후 매개변수 수 측면에서 PaLM 2-S에 해당한다.\n' +
      '\n' +
      '마지막으로 **LLaVA-1.5**[45] 및 ** 구조BLIP**[17]와 같은 오픈 소스 모델을 고려한다.\n' +
      '\n' +
      '공간 VQA 성능.\n' +
      '\n' +
      'VLM의 공간 추론 능력을 스트레스 검정을 위해서는 성능 접지성이 보장된 공간 추론 VQA 벤치마크가 필요하다. 그러나 문헌에 사용할 수 있는 적절한 벤치마크도 없습니다. 따라서 인간 주석자가 훈련 단계에서 모든 VLM에 반응하지 않는 웹LI 이미지[12]의 하위 집합에 다양한 "직접적인" 정성적 및 정량적 VQAs 세트를 표시하여 벤치마크를 만들었다. 기준문항 및 답변은 제3.2절(부록의 세부 사항 A.1)에서 설명한 합성 데이터 생성 패턴에 따라 다양하고 자유롭다. 우리는 331개의 질적 공간 추론 VQA 쌍과 215개의 정량적 공간 추론 VQA 쌍을 주석을 달았다.\n' +
      '\n' +
      '이러한 질문에 대한 정성적 공간 VQA는 인간 주석이 달린 답변과 VLM 출력 모두 자유형 자연 언어이다. 따라서 VLM의 성능을 평가하기 위해 인간 쥐들을 사용하여 답이 맞는지 확인하고 표 1의 VLM의 성공률을 보여주고 있으며, SpatialVLM은 GPT-4V를 포함한 다른 비전 언어 모델을 능가하여 합성 공간 VQA 데이터를 사용하여 훈련되지 않은 모든 기저부에 비해 훨씬 더 높은 정확도를 달성할 수 있음을 보여준다. 바젤 중 두 번째로 좋은 모델은 LLaVA-1.5로, 시각 지시 튜닝에서 바운딩 박스 및 상응하는 캡션을 사용하여 발생할 수 있다. Anecdotally, LLaVA-1.5는 2D 공간 관계 추론에서 잘 수행되지만 3D 공간 추론에서 모델보다 열등하다는 것을 발견했다. 이 실험은 크고 질 높은 공간 추론 데이터가 최첨단 VLM의 데이터 세트를 전처리하는 데 존재하지 않는 공간 추론 역량의 핵심임을 시사한다.\n' +
      '\n' +
      '이러한 질문에 대해 정량적 공간 VQA는 인간 주석자 답변과 VLM 출력 모두 선호하는 단위를 사용하여 거리, 높이, 상승 등에 대한 자연어 설명이다. 우리는 VLM의 성능을 평가하기 위해 두 가지 메트릭을 설계한다. 먼저 VLM의 성공률을 사용하여 VLM이 정량적 공간적 추론 질문을 이해할 수 있는지 반영하기 위해 숫자를 생성한다. 둘째, 답은 센티미터에서 킬로미터까지 다양할 수 있기 때문에 백분율을 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Method & GPT-4V & LLaVA-1.5 & InstructBLIP & PaLI & PaLM-E & PaLM 2-E & Ours \\\\ \\hline Accuracy & 68.0\\% & 71.3\\% & 60.4\\% & 60.7\\% & 50.2\\% & 50.4\\% & **75.2**\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '이진 술어 예측 작업***에 대한 다양한 VLM의 정확도는 표 1: **이다. 우리의 제안된 방법은 합성 데이터의 첨가로 인해 이진 술어 예측 작업에 대한 기저부를 큰 차이로 능가한다.\n' +
      '\n' +
      'VLM의 추정치가 얼마나 정확한지를 나타내기 위해 지상 진리값의 절반에서 2배로 떨어지는 VLM 답변이다. 결과는 표와 같다. 2, 우리의 모델은 큰 마진을 가진 기저장보다 두 메트릭 모두에서 더 나은 성능을 나타내는 것으로 나타났다. 기준 VLM이 숫자로 구성된 답변을 제공하는 것을 꺼리는 것을 관찰했다. 예를 들어, "_No___"를 회신합니다. "_나 사이의 거리를 알려줄 수 있다"와 같은 질문에 대한 질문은 훈련 데이터의 분포 때문일 수 있다. 또한, 우리는 최첨단 VLM GPT-4V가 면책 텍스트 "_Tm 미안하지만 이미지를 측정하기 위한 정확한 참조가 제공되지 않기 때문에 정확한 거리를 제공할 수 없기 때문에 SI 단위의 거리에 대한 답을 생성하는 것을 종종 자제한다는 것을 발견했다. 우리의 접근 방식은 모든 기저부에 비해 훨씬 더 높은 성공률을 달성하여 거의 절반의 질문에 대한 범위 내 결과를 달성한다. 이 공연은 인간의 주석이 시끄럽고 주석들 간의 합의가 보장되지 않는 경우가 많다는 점에서 주목할 만하다(부록 A.1). 우리의 모델의 성능과 한계를 더 잘 이해하기 위해 우리는 그림의 지상 진리값에 대한 상대적 오차를 시각화했다. 부록의 11. 우리는 SpatialVLM이 카메라에서 물체 \\(1-10\\) 미터기와 같은 중간 범위 장면을 잘 가지고 있음을 발견했다. 이것은 단안 깊이 추정기[6]가 메트릭 정확한 깊이 추정을 안정적으로 출력하는 범위와 일치하며, 이는 우리의 방법이 데이터 합성 파이프라인에서 전문가 비전 모델에서 편향과 한계를 계승함을 나타낸다.\n' +
      '\n' +
      'VQA 데이터 일반에 대한 공간 VQA 데이터가 VQA 일반에 미치는 영향.\n' +
      '\n' +
      '우리가 답하고 싶은 두 번째 질문은 상당한 양의 공간 VQA 데이터와 공동 번역하기 때문에 다른 작업에서 VLM의 성능이 결과적으로 저하될 것인지 여부이다. 우리는 일반적인 VQA 벤치마크에서 공간 VQA 데이터셋 없이 훈련된 바닐라 PaLM 2-E와 모델을 비교하고 표에 요약된 바와 같다. 3, 우리의 모델은 제한된 공간 추론 질문이 포함된 OKVQA 벤치마크에서 PaLM 2-E로 유사한 성능을 달성하고 공간 추론 질문을 포함하는 VQA-v2 테스트-dev 벤치마크에서 약간 더 잘 수행된다. 이것은 VLM이 일반적으로 공간적 추론과 가까운 작업의 분포에 적합하고 있으며 일반적인 VQA 능력에 영향을 미치지 않으면서 공간 VQA 감독으로부터 이익을 얻을 수 있음을 시사하는 것으로 판단된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline  & GPT-4V & LLAV-1.5 & InstructBLIP & PaLI & PaLM-E & PaLM 2-E & Ours \\\\ \\hline Output numbers \\% & 1.0\\% & 20.9\\% & 26.0\\% & 52.0\\% & 83.2\\% & 88.8\\% & **99.0\\%** \\\\ In range [50, 200]\\% & 0.0\\% & 13.0\\% & 7.9\\% & 5.3\\% & 23.7\\% & 33.9\\% & **37.2\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: ** 공간적 관계에 대한 정량적 질문에 대한 서로 다른 VLM의 정확도는 이 표에서 볼 수 있으며, 첫째, 우리의 방법은 기준 방법보다 더 자주 유효한 포맷(시간의 99.0%)을 출력한다. 둘째, 우리의 방법은 기준 방법보다 인간이 주석을 받은 근거 진리에 더 가까운 정량적 거리 추정을 출력한다.\n' +
      '\n' +
      '그림 5: **|에 로봇 그리퍼가 콜라에 접근하고 있는 이미지의 시퀀스를 제시하면, “노란색 그리퍼와 코크 사이의 거리가 무엇일까”라는 공간VLM에 대해 질문한다. 우리는 정확하고 단조적으로 거리 추정을 줄일 수 있으며,****는 정확하고 단조적으로 감소시킬 수 있다.\n' +
      '\n' +
      '동공간적 이유기(ViT) 엔코더의 효과##, 공간적 이유기(ViT)는 비현 전이기(ViT) 엔코더의 효과.\n' +
      '\n' +
      '냉동 ViT(대조적으로ive 객관적인 학습)는 공간 추론을 수행할 수 있는 충분한 정보를 인코딩합니까? 이를 연구하기 위해 110k 훈련 단계에서 시작하여 ViT가 동결된 2개의 훈련 실행으로 분기하며, 하나는 ViT가 동결되지 않은 다른 하나는 ViT가 동결되었다. 우리는 70k 단계에 대해 두 모델을 모두 훈련하고 표 4의 지상 진리 값의 다양한 범위에 속하는 두 모델의 답변 비율을 평가한다.\n' +
      '\n' +
      '지상 진리의 절반 범위 내에서 거친 추정을 하는 것과 같이 더 크고 미세한 거리 추정에 대해 ViT가 동결되지 않은 훈련은 약간 더 나쁘지만 냉동되지 않은 ViT와 비교할 수 있는 것으로 나타났다. 그러나 정확한 정량적 값을 추정하는 것과 같이 보다 미세화된 거리 추정을 위해 냉동되지 않은 ViT를 사용한 모델이 상당히 더 잘 수행되었다. 우리는 전처리된 ViT(비교 또는 분류 손실)가 미세한 공간 정보에서 손실된다고 가정한다. 우리의 모델은 인간 주석의 값 0.9\\(종량)에서 1.1\\(종량) 범위를 예측하기 위한 8.4% 정확도를 달성한다. 인간의 주석이 시끄럽기 때문에 이것은 놀라운 일입니다. 사실, 인간은 0.8미터 내지 1미터의 추정을 선호하기 때문에 시끄러운 추정치를 제공하는 경향이 있다. 넓은 영역에서 비전-언어 모델의 정량적 공간적 추론 능력을 평가하는 것은 여전히 어렵다.\n' +
      '\n' +
      '정작 여러 가지 공간확인요.\n' +
      '\n' +
      '공간 VQA 데이터셋의 정량적 답변이 시끄럽기 때문에 VLM이 많은 양의 시끄러운 훈련 데이터에서 일반화 가능한 정량적 추정을 학습할 수 있는지 연구했다. 그러기 위해 우리는 먼저 고품질 정량 답변을 생성할 수 있는 영역을 마련했습니다. 4.1절에서 논의된 바와 같이 단안 깊이 추정은 가장 소음을 유도하는 데이터 생성 파이프라인의 단계 중 하나이다. 따라서 깊이 카메라를 사용하여 캡처된 근경진 깊이 정보를 제공하는 로봇 조작 데이터 세트를 레버리니다. 그 결과 생성된 정량적인 답은 더욱 정확하다. 본 데이터셋을 이용하여 VLM을 학습시키고, 조작 도메인(그림 5)에서 미세곡선 거리 추정을 수행할 수 있는 모델을 찾아본다. 데이터 정확도를 더욱 보여준다.\n' +
      '\n' +
      '시끄러운 데이터가 VLM 학습에 어떤 영향을 미치는지 연구하기 위해 정확한 조작 공간 VQA 데이터셋의 정량적 답변에 가우시안 노이즈를 추가하고, 다른 노이즈 레벨의 일련의 시끄러운 데이터셋을 얻는다. 시끄러운 데이터 세트를 사용하여 VLM을 훈련하고 조작을 위해 인간 주석이 달린 정량적 공간 VQA 벤치마크를 사용하여 평가한다. 표. 5는 다양한 가우시안 노이즈 표준 편차가 정량적 공간 VQA에 대한 전체 VLM 성능에 어떻게 영향을 미치는지 비교한다. 조작 VQA 데이터셋 내의 객체가 1미터 범위 내에 있기 때문에 평균 제곱 오차(MSE)를 평균 제곱 오차(MSE)를 추가하였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline  & [50, 200]\\% & [66.7, 150]\\% & [90, 110]\\% \\\\ \\hline Frozen ViT & 34.9\\% & 9.3\\% & 5.6\\% \\\\ Unfrozen ViT & **37.2**(+2.3)**\\% & **10.7**(+1.4)**\\% & **8.4**(+2.8)**\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 냉동 또는 냉동되지 않은 ViT와 핀셋에 대한 비교이다. 우리는 거리 추정 작업에 대해 전처리된 ViT를 할당하지 않는 것이 유익하다는 것을 발견했다.\n' +
      '\n' +
      'VLM 성능을 평가하기 위한 금속 및 4.1절에서 정의된 하프 대 베이스 백분율은 서로 다른 노이즈 수준의 데이터셋에 대해 훈련된 VLM이 유사한 공간 추론 정확도를 달성하는 것으로 나타났다. 우리는 이것이 훈련 데이터의 시끄러운 특성과 수동으로 주석이 달린 평가 벤치마크 때문이며, VLM이 시끄러운 데이터에도 불구하고 공간적 추론 공통 센스를 학습할 수 있다고 추측한다. 로봇 실험에서도 이러한 흥미로운 현상을 관찰했다. 그림에서. 6, 거리 추정은 모델이 크게 규칙화되었기 때문에 평균에 대한 편향을 나타낸다.\n' +
      '\n' +
      '여러 가지 명시.\n' +
      '\n' +
      'VLM의 중요한 적용 중 하나는 로봇 공학이다. 최근 작품들은 VLM과 LLM이 로봇 공학 과제에 대한 보편적인 개방형 동물 보상 주석 및 성공 검출기[20] 역할을 할 수 있으며, 이는 유용한 통제 정책을 도출하는데 사용될 수 있음을 보여주었다. 그러나 VLM의 보상 주석 능력은 종종 공간적 인식 부족으로 인해 제한된다. 공간VLM은 이미지로부터 거리나 크기를 정량적으로 추정할 수 있기 때문에 조밀한 보상 주석으로 독특하게 적합하다. 우리는 자연 언어로 과제를 명시하고, 궤적에서 각 프레임에 대한 보상을 주석하기 위해 공간VLM을 요구하는 실제 로봇 실험을 수행한다. 그림 6에서 각 도트는 객체 위치를 나타내고 그 색상은 주석이 달린 보상을 나타낸다. 로봇이 지정된 목표를 향해 진척함에 따라 보상은 단조적으로 증가하는 것을 볼 수 있으며, 이는 공간VLM이 조밀한 보상 주석을 하는 역할을 하는 능력을 나타낸다.\n' +
      '\n' +
      '이 섹션에서는 원소 공간 질문에 답하는 능력이 향상되었음을 감안할 때 공간VLM이 다단계 추론이 필요한 작업을 수행하는 데 사용될 수 있는지 여부를 조사한다. 우리는 그림 1과 그림 4에서 몇 가지 예를 보여주고 있으며, 이 경우 GPT-4에서 공간 추론 하위 모듈로서 공간VLM을 장착할 때 환경의 3개의 객체가 "이종 삼각형"을 형성할 수 있는지 답하는 것과 같은 복잡한 공간 추론 작업을 수행할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Gaussian std & 0 & 0.1 & 0.2 & 0.3 \\\\ \\hline MSE(m) & 0.046 & 0.053 & 0.039 & 0.048 \\\\\n' +
      '[50, 200]\\% & 59.0\\% & 55.8\\% & 61.1\\% & 61.1\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 가우시안 노이즈의 표준 편차(STD)를 사용하여 제어되는 다양한 데이터 노이즈 수준에 대한 \\(|\\) 비교이다. 우리는 적당한 양의 무작위 소음에도 불구하고 우리의 모델이 학습할 수 있다는 것을 발견했다.\n' +
      '\n' +
      '그림 6: 로봇 공학 과제에 대한 보상 발전기로서 \\(|\\)**SpatialVLM은 "자연 언어 퀴어 생존 가능한" 거리 추정 도구를 제공하며 로봇 공학 작업에 사용할 수 있다. 예를 들어, 태스크 \'픽 오렌지 차병\'의 경우, 보상/비용 기능은 \'노란색 그리퍼 손가락과 오렌지 차병 사이의 거리가 무엇일까\'의 응답 기능이 될 수 있다. 그리고 "사과를 그릇에 입력"하는 과제에 대해서는 보상/비용 기능은 "사과와 그릇 사이의 거리가 무엇인가"의 반응의 기능이 될 수 있다. 우리는 다양한 그리퍼 위치를 샘플링하고 위의 산점 도표에서 비용 함수를 보여준다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '결론적으로, 우리의 연구는 공간 추론을 VLM에 추론하는 문제를 해결하고 인터넷 규모의 실제 이미지를 기반으로 3D 공간 추론 VQA 데이터의 자동 생성을 위한 프레임워크를 구성하여 접근한다. 우리는 많은 양의 시끄러운 데이터와 비트리징 ViT로 훈련하는 것과 같은 VLM을 트레이닝하기 위한 레시피에서 다양한 설계 선택을 남용한다. 우리의 직접적인 공간적 질의는 유한한 템플릿 세트에 구축되지만 공간 추론 구성요소가 필요한 보다 복잡한 사슬 추론 추론으로 확장될 수 있음을 보여준다. 공간적 VLM은 또한 로봇 공학 과제에 유용한 것으로 입증되며, 여기서 우리는 3D 공간 인식 VLM이 로봇 공학 과제에 대한 보상 주석을 사용할 수 있음을 보여준다. 보다 미묘한 기하학적 프리미티브에 대한 추가 연구는 또한 3D 기하학에서 완전히 그라운드 공간 추론에 도움이 될 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. _arXiv preprint arXiv:2204.01691_, 2022.\n' +
      '* [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022.\n' +
      '* [3] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* [4] Anonymous. Flexcap: Generating rich, localized, and flexible captions in images. In _Submitted to The Twelfth International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=7Phic60WAg](https://openreview.net/forum?id=7Phic60WAg). under review.\n' +
      '* [5] Ivana Balazevic, David Steiner, Nikhil Parthasarathy, Relja Arandjelovic, and Olivier J Henaff. Towards in-context scene understanding. _arXiv preprint arXiv:2306.01667_, 2023.\n' +
      '* [6] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. _arXiv preprint arXiv:2302.12288_, 2023.\n' +
      '* [7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. _arXiv preprint arXiv:2307.15818_, 2023.\n' +
      '* [8] Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, Jose Neira, Ian Reid, and John J Leonard. Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age. _IEEE Transactions on robotics_, 32(6):1309-1332, 2016.\n' +
      '* [9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. _IEEE transactions on pattern analysis and machine intelligence_, 40(4):834-848, 2017.\n' +
      '\n' +
      '* [10] Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, and Ziwei Liu. Language models are visual reasoning coordinators. In _ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models_, 2023.\n' +
      '* [11] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling framework for object detection. _arXiv preprint arXiv:2109.10852_, 2021.\n' +
      '* [12] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. _arXiv preprint arXiv:2209.06794_, 2022.\n' +
      '* [13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. _arXiv preprint arXiv:1504.00325_, 2015.\n' +
      '* [14] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n' +
      '* [15] Vanya Cohen, Benjamin Burchfiel, Thao Nguyen, Nakul Gopalan, Stefanie Tellex, and George Konidaris. Grounding language attributes to objects using bayesian eigenobjects. In _2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 1187-1194. IEEE, 2019.\n' +
      '* [16] Yuchen Cui, Scott Niekum, Abhinav Gupta, Vikash Kumar, and Aravind Rajeswaran. Can foundation models perform zero-shot task specification for robot manipulation? In _Learning for Dynamics and Control Conference_, pages 893-905. PMLR, 2022.\n' +
      '* [17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.\n' +
      '* [18] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In _Proceedings of the International Conference on Machine Learning_, 2023.\n' +
      '* [19] Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando de Freitas, and Serkan Cabi. Vision-language models as success detectors. _arXiv preprint arXiv:2303.07280_, 2023.\n' +
      '* [20] Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando de Freitas, and Serkan Cabi. Vision-language models as success detectors, 2023.\n' +
      '* [21] Hugh Durrant-Whyte and Tim Bailey. Simultaneous localization and mapping: part i. _IEEE robotics & automation magazine_, 13(2):99-110, 2006.\n' +
      '* [22] Martin Ester, Hans-Peter Kriegel, Jorg Sander, Xiaowei Xu, et al. A density-based algorithm for discovering clusters in large spatial databases with noise. In _kdd_, volume 96, pages 226-231, 1996.\n' +
      '* [23] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. _Advances in Neural Information Processing Systems_, 35:18343-18362, 2022.\n' +
      '\n' +
      '* [24] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2002-2011, 2018.\n' +
      '* [25] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision-language pre-training: Basics, recent advances, and future trends. _Foundations and Trends(r) in Computer Graphics and Vision_, 14(3-4):163-352, 2022.\n' +
      '* [26] Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded vision-language models for robotic manipulation, 2023.\n' +
      '* [27] Theophile Gervet, Soumith Chintala, Dhruv Batra, Jitendra Malik, and Devendra Singh Chaplot. Navigating to objects in the real world. _Science Robotics_, 8(79):eadf6991, 2023.\n' +
      '* [28] Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. Iqa: Visual question answering in interactive environments. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4089-4098, 2018.\n' +
      '* [29] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2017.\n' +
      '* [30] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _Proceedings of the IEEE international conference on computer vision_, pages 2961-2969, 2017.\n' +
      '* [31] Sachithra Hemachandra, Matthew R Walter, Stefanie Tellex, and Seth Teller. Learning spatial-semantic representations from natural language descriptions and scene classifications. In _2014 IEEE International Conference on Robotics and Automation (ICRA)_, pages 2623-2630. IEEE, 2014.\n' +
      '* [32] Marcel Hildebrandt, Hang Li, Rajat Koner, Volker Tresp, and Stephan Gunnemann. Scene graph reasoning for visual question answering, 2020.\n' +
      '* [33] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17980-17989, 2022.\n' +
      '* [34] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In _International Conference on Machine Learning_, pages 9118-9147. PMLR, 2022.\n' +
      '* [35] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning, 2016.\n' +
      '* [36] Mohi Khansari, Daniel Ho, Yuqing Du, Armando Fuentes, Matthew Bennice, Nicolas Sievers, Sean Kirmani, Yunfei Bai, and Eric Jang. Practical imitation learning in the real world via task consistency loss, 2022.\n' +
      '* [37] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* [38] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. _arXiv preprint arXiv:1712.05474_, 2017.\n' +
      '\n' +
      '* Krishna et al. [2016] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Fei-Fei Li. Visual genome: Connecting language and vision using crowdsourced dense image annotations, 2016.\n' +
      '* Krishna et al. [2017] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _International journal of computer vision_, 123:32-73, 2017.\n' +
      '* Kuo et al. [2019] Weicheng Kuo, Anelia Angelova, Jitendra Malik, and Tsung-Yi Lin. Shapemask: Learning to segment novel objects by refining shape priors. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9207-9216, 2019.\n' +
      '* Kwon et al. [2023] Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan, and Dorsa Sadigh. Toward grounded social reasoning, 2023.\n' +
      '* Lin et al. [2015] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context, 2015.\n' +
      '* Liu et al. [2023] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. _Transactions of the Association for Computational Linguistics_, 11:635-651, 2023. ISSN 2307-387X. doi: 10.1162/tacl_a_00566. URL [http://dx.doi.org/10.1162/tacl_a_00566](http://dx.doi.org/10.1162/tacl_a_00566).\n' +
      '* Liu et al. [2023] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '* Liu et al. [2022] Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, and Andrew M Dai. Mind\'s eye: Grounded language model reasoning through simulation. _arXiv preprint arXiv:2210.05359_, 2022.\n' +
      '* Liu et al. [2023] Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, and Anima Anandkumar. Prismer: A vision-language model with an ensemble of experts. _arXiv preprint arXiv:2303.02506_, 2023.\n' +
      '* Mahmoudieh et al. [2022] Parsa Mahmoudieh, Deepak Pathak, and Trevor Darrell. Zero-shot reward specification via grounded natural language. In _International Conference on Machine Learning_, pages 14743-14752. PMLR, 2022.\n' +
      '* Marino et al. [2019] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge, 2019.\n' +
      '* Nair et al. [2022] Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, et al. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. In _Conference on Robot Learning_, pages 1303-1315. PMLR, 2022.\n' +
      '* [51] OpenAI. Gpt-4 technical report, 2023.\n' +
      '* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* Richter et al. [2016] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14_, pages 102-118. Springer, 2016.\n' +
      '\n' +
      '* [54] Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, and David Lindner. Vision-language models are zero-shot reward models for reinforcement learning. _arXiv preprint arXiv:2310.12921_, 2023.\n' +
      '* [55] Julia Rozanova, Deborah Ferreira, Krishna Dubba, Weiwei Cheng, Dell Zhang, and Andre Freitas. Grounding natural language instructions: Can large language models capture spatial information?, 2021.\n' +
      '* [56] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9339-9347, 2019.\n' +
      '* [57] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Madhieni, Nikhil Joshi, et al. Robova: Multimodal long-horizon reasoning for robotics. _arXiv preprint arXiv:2311.00899_, 2023.\n' +
      '* [58] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10740-10749, 2020.\n' +
      '* [59] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In _Conference on Robot Learning_, pages 894-906. PMLR, 2022.\n' +
      '* [60] Jesse Thomason, Mohit Shridhar, Yonatan Bisk, Chris Paxton, and Luke Zettlemoyer. Language grounding with 3d objects. In _Conference on Robot Learning_, pages 1691-1701. PMLR, 2022.\n' +
      '* [61] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for vision-linguistic compositionality. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5238-5248, 2022.\n' +
      '* [62] Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico Tombari. Learning 3d semantic scene graphs from 3d indoor reconstructions. _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, Jun 2020. doi: 10.1109/cvpr42600.2020.00402. URL [http://dx.doi.org/10.1109/cvpr42600.2020.00402](http://dx.doi.org/10.1109/cvpr42600.2020.00402).\n' +
      '* [63] Matthew R Walter, Sachithra Madhaw Hemachandra, Bianca S Homberg, Stefanie Tellex, and Seth Teller. Learning semantic maps from natural language descriptions. Robotics: Science and Systems, 2013.\n' +
      '* [64] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021.\n' +
      '* [65] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.\n' +
      '* [66] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 9068-9079, 2018.\n' +
      '\n' +
      '* [67] Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Language models meet world models: Embodied experiences enhance language models. _arXiv preprint arXiv:2305.10626_, 2023.\n' +
      '* [68] Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey Levine, and Jonathan Tompson. Robotic skill acquisition via instruction augmentation with vision-language models. _arXiv preprint arXiv:2211.11736_, 2022.\n' +
      '* [69] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fangqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. _arXiv preprint arXiv:2306.09265_, 2023.\n' +
      '* [70] Rowan Zellers, Ari Holtzman, Matthew Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi, and Yejin Choi. Piglet: Language grounding through neuro-symbolic interaction in a 3d world. _arXiv preprint arXiv:2106.00188_, 2021.\n' +
      '* [71] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing zero-shot multimodal reasoning with language. _arXiv preprint arXiv:2204.00598_, 2022.\n' +
      '\n' +
      '부록 A.\n' +
      '\n' +
      '조건부 전문가 및 규격.\n' +
      '\n' +
      '공간 VQA 인간 주석 벤치마크는 웹Li 및 로봇 조작 VQA에 대해 546개의 정성적 및 정량적 질문 쌍을 수동으로 표지했다. 인간 주석 파이프라인에서 우리는 3절에서 설명한 공간 VQA 데이터 생성 파이프라인을 사용하여 이미지별 샘플 질문을 제공하며, 인간 주석자는 이미지와 샘플 질문을 살펴보고, 질문이나 유형을 더 적절한 질문을 사용하거나 이미지를 위해 주석을 건너뛰고 싶은지 결정할 것이다. 그런 다음 샘플 질문이나 인간의 입력 질문을 기반으로 인간 주석자는 자신이 생각하는 답을 자연 언어의 형태로 적절하게 유형화할 것이다. 그림. 7, 그림. 8 및 그림. 9는 인간의 주석이 달린 공간 VQA 쌍의 예를 보여준다.\n' +
      '\n' +
      '여기에서 우리는 체인의 공간적 추론을 구현하기 위해 더 자세한 내용을 제공한다. 본고에서 언급한 바와 같이 LLM을 통해 체인의 항문 추론을 수행할 수 있게 되었다.\n' +
      '\n' +
      '그림 8: * <공간 VQA 정량적 벤치마크***의 샘플 질문 응답 쌍.\n' +
      '\n' +
      '그림 7: *.\n' +
      '\n' +
      '그림 9: ** 예 로봇 조작 VQA 정량적 벤치마크**의 질문 응답 쌍은 시각 정보에 대한 우리의 공간VLM에 대한 질의 능력을 가지고 있다. LLM은 시각적 정보 자체를 인지하지 못하므로 자신이 볼 수 없다는 이미지를 볼 수 있는 친구를 물어보는 등 게임을 하는 것처럼 결정을 내리도록 했다. 아래에 완전한 신속한 정보를 제공합니다.\n' +
      '\n' +
      '1.\n' +
      '\n' +
      '당신은 친구와의 시각적 질문에 답하는 게임에 참여하고 있습니다. 이 게임에서는 이미지로부터 답하기 위해 시각적 정보가 필요한 질문을 제공합니다. 당신은 질문을 볼 수 있지만 이미지는 볼 수 없고, 친구는 이미지를 볼 수 있지만 원래 질문은 볼 수 없다. 운 좋게도 질문을 분해하고 친구에게 이미지에 대해 물어볼 수 있습니다. 당신의 친구는 원래 질문에 답하는 데 사용할 수 있는 답변을 제공합니다.\n' +
      '\n' +
      '여기 샘플 대화입니다.\n' +
      '\n' +
      '[질문]은 테이블 청소를 어떻게 해야 하나요? 내가 어떻게 손을 움직여야 하는지에 대해 자세히 설명해주세요.\n' +
      '\n' +
      '[당신은 그 이미지에 어떤 사물이 있나요?\n' +
      '\n' +
      '[친구]에는 빈 콜라 캔, 쓰레기통, 커피 머신이 있습니다.\n' +
      '\n' +
      '[너는 왼쪽 또는 콜라 오른쪽의 쓰레기통이야?\n' +
      '\n' +
      '[친구] 왼쪽이에요.\n' +
      '\n' +
      '[당신은 쓰레기통 또는 콜라가 당신에게서 더 들어갈 수 있습니까?\n' +
      '\n' +
      '[친구는] 깊이가 비슷합니다.\n' +
      '\n' +
      '그림 11: | **Error vs Scene Depth ablation** 자료에서 SpatialVLM이 결국 노이즈에 속한다는 오류는 더 큰 거리에 대한 거리 추정 상대 오차(1.0에서 캡핑) w에 대해 일반적으로 더 큰 오류가 있음을 발견했다. 우리는 이것을 조이페트 [6]의 데이터세트 편향 때문일 수 있다고 가정한다.\n' +
      '\n' +
      '그림 10: | ** 탑 워드 주파수***입니다. 트레이닝 데이터셋에 나타난 상위 단어들, 빨간색 색상은 공간 개념을 논의하는 데 그 단어가 관여함을 나타낸다. 우리의 훈련 데이터는 공간적 질문이 풍부하고 다양하다는 것을 보여준다.\n' +
      '\n' +
      '[당신] 왼쪽은 콜라 캔에 비해 쓰레기통 몇 개예요? [친구] 20센티미터입니다. [정답] 원은 콜라 캔을 잡아야 하고, 왼쪽으로 20센티미터 이동해서 풀어 놓으면 쓰레기통에 떨어집니다. 여기 또 다른 예인데, [질문]는 파란색 병과 노란색 책 사이의 거리가 식물과 콜라 사이의 거리보다 길면 저를 불러요? [당신] 파란색 병과 노란색 책 사이의 거리는 어떻게 되나요? [토올] 0.3m[당신]은 식물과 콜라 사이의 거리가 어떻게 되나요? [친구] 0.7m[로봇]은 파란색 병과 노란색 책 사이의 거리가 0.3m이고 식물 사이의 거리가 0.7m이고 콜라 캔은 0.7m이기 때문에 파란색 병과 노란색 책 사이의 거리는 식물과 콜라 사이의 거리보다 더 길지 않다. [정답]. 여기는 또 다른 예로 [질문] 아이들이 더 쉽게 도달할 수 있는 대상, 흰색과 노란색 토끼 장난감은 맥주의 다크 그린 캔이나? [당신] 흰색과 노란색 토끼 장난감의 상승이 어떻게 되나요? [친구] 0.9m입니다. [당신은 맥주의 다크 그린 캔의 고도가 어떻게 되나요? [친구] 0.2m입니다. [정답] 아이들은 일반적으로 짧기 때문에 고도가 낮은 것에 도달하는 것이 용이하므로 맥주 캔에 도달하는 것이 더 쉬울 것이다. 이제 새로운 질문을 감안할 때 친구에게 관련 시각 정보를 요청하여 질문에 답하려고 노력합니다. [질문] 그렇게 하면 LLM과 공간VLM이 효과적으로 협력하여 올바른 결과를 도출할 수 있다는 것을 알게 된다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '데이터 필터링 단계에서 우리는 두 가지 중요한 목적을 가지고 있으며, 첫째, 인간은 흰 배경 이전에 단일 객체의 사진과 같은 공간적 질문을 거의 할 수 없는 이미지를 걸러낼 것이다. 둘째, 우리의 과정은 2D 이미지를 3D 포인트 클라우드로 리프팅해야 하기 때문에 단안 깊이 추정 모델이 최적화된 값에 가까운 시야가 있기를 바랍니다. 첫 번째 목적을 달성하기 위해 전처리된 CLIP 모델을 사용하여 후보 이미지를 라벨링하고 제품 또는 작품을 나타내는 모델을 필터링한다. 긍정적인 CLIP 라벨에는 "실내 장면의 이폰 사진", "야외 장면의 이폰 사진" 등이 있으며, 음성 라벨은 "단일 물체의 밀착 샷", "흰 배경 앞에 표시된 제품", "그림", "그림", "그래픽 사용자 인터페이스의 스크린샷", "텍스트 조각", " 스케치"가 있다. 우리는 두 번째 목적을 충족시키기 위해 긍정적인 사례에 대한 프리픽스로 "이폰 사진"을 선택한다. 우리는 이 프리픽스가 더 넓은 시야를 가진 데이터와 흔하지 않은 시각 비율을 가진 특정 이미지를 효과적으로 필터링한다는 것을 관찰한다. 데이터 필터링에서 이러한 설계 선택은 왼쪽 이미지가 전문가 모델과 qa 생성의 효과적인 분포 내에 있음을 보장한다.\n' +
      '\n' +
      '방법 섹션에서 언급한 바와 같이, 우리는 질문 답변 데이터를 합성하기 위해 재발 정보를 추출하기 위해 다양한 오프 시트 모델을 사용한다. 여기서는 컨텍스트 추출에 대한 추가 세부 정보를 제공합니다. 데이터 필터링 후, 우리는 지역 제안 네트워크(RPN)를 실행한 다음 비max 억제(NMS) [30]를 실행한다. 각 객체 바운딩 박스에 대해, 우리는 객체를 분할하기 위해 클래스 개요 세분화 모델[41]을 운영한다. 각 바운딩 박스에 대해 FlexCap[4]를 사용하여 \\(1-6\\) 단어 사이에 무작위 길이가 있는 물체 중심 자막을 샘플링한다. 특히 "케이이크"와 같은 매우 거친 범주에 고정되기 때문에 전통적인 물체 검출기를 의도적으로 피하기로 선택한 반면, 우리의 접근법은 그림 2의 이미지에 대해 "가방 모양" 및 "플라스틱 용기 안에 있는 케이크"와 같은 미세한 설명으로 물체를 주석할 수 있다.\n' +
      '\n' +
      '그런 다음 2D 콘텍스트토 3D 콘텍스트 라이프팅은 이미지 상에서 최첨단 메트릭 깊이 검출기, 조예프[6]를 실행한다. 조플롯은 메트릭 깊이(실제 세계 "메이커"에서)를 출력한다. fov 추정과 결합하여 그림 2와 같이 실제 척도로 예시된 바와 같이 2D 이미지를 3D 포인트 구름으로 들어올릴 수 있다.\n' +
      '\n' +
      '이 점 클라우드 처리 단계에서는 주요 그룹에서 크게 벗어나는 이상치 또는 포인트를 제거하여 데이터 정확도를 향상시킨다. 클러스터링 알고리즘 DBSCAN[22]은 밀집된 지역을 중심으로 근접성을 기반으로 포인트를 그룹화하고 희소하고 덜 중요한 지점을 제거한다. 이는 형상의 치수가 측정되는 후속 형상 및 기하학 분석에 이상적인 더 깨끗하고 구조화된 포인트 클라우드를 초래한다. 이미 포인트 클라우드에 대한 의미 세분화를 얻었기 때문에 이 정보를 사용하여 적응 규모에서 이상치를 처리할 수 있다. 더 작은 물체의 경우 각 축을 따라 비례하는 더 작은 임계값을 사용한다. 우리는 이러한 선택이 포인트 클라우드 이상치를 효과적으로 제거하는 동시에 더 작은 객체에 대한 중요한 점도 유지하는 것을 관찰한다. 알고리즘 2에서 아래의 알고리즘 세부 정보를 제공합니다.\n' +
      '\n' +
      '```\n' +
      '입력: 포인트_obj: Points_dscan(0.01, 스케일/20)) 레이블(pcd.nn－pcn) appn(pcd) 관심 페달)=FullPointcloud.)(FullPointcloud10) 평균(포인트_stobj.ststAN(축=50,std=1.2) 페달=pcd.\n' +
      '```\n' +
      '\n' +
      '그림 12 |는 모호성 제거를 나타내는 그림이다.\n' +
      '\n' +
      '코디네이트 캐노네티컬화는 이제 메트릭 스케일로 3D 포인트 클라우드가 있습니다. 그러나 포인트 클라우드는 여전히 카메라 프레임에 있어 우리가 추출할 수 있는 정보를 제한한다. 예를 들어, 영상의 상측에 가까운 객체는 반드시 지면에서 더 이상 되지 않으며, 그 이유는 카메라가 전방 대신 지면을 가리키고 있을 수 있기 때문이다. 이러한 문제를 해결하기 위해 수평면을 검출하여 포인트 클라우드 좌표계를 표준화한다. 우리는 이들 3D 포인트 중 가장 큰 평면에 맞도록 RANSAC를 사용하기 전에 픽셀들을 분할하기 위해 광 중량 분할 모델[9]을 사용한다.\n' +
      '\n' +
      '충분한 지점에 의해 정의된 표면을 감지할 때, 우리는 검출된 평면에 카메라 출처를 투사하여 새로운 출처를 생성함으로써 포인트 클라우드의 좌표를 표준화한다. 우리는 검출된 평면의 정상 축을 z축으로 하고 평면에 카메라의 원래 z축을 z축으로 투사한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:22]\n' +
      '\n' +
      '평균을 잡으세요. 우리는 이것을 로봇 실험에 사용하여 더 미세한 값을 얻었다. 또 다른 방법은 VLM 자체를 일정량의 자릿수를 유지하는 것이다. 이것은 데이터 합성에 추가될 수 있지만, 우리는 이것을 향후 작업에 맡깁니다.\n' +
      '\n' +
      'VLM 훈련은 2e-4의 학습률로 배치 크기 512 및 ADAM 최적기로 다중 모델 대형 언어 모델을 훈련했다. PaLM-E에서 원래VQA 데이터셋과 생성된 공간 VQA 데이터셋의 혼합물을 사용하여 PaLM 2-E-S 모델을 훈련했으며, 처음에 174:2.5의 샘플링 비율로 모델을 훈련했으며, 처음에 우리가 소진한 모든 데이터를 사용하지 않는 110k 단계에 대한 냉동 비전 인코더로 모델을 훈련했다. 따라서 저희가 생성한 데이터는 충분합니다. 그런 다음 실험 섹션에 설명된 바와 같이 융합할 때까지 70k 단계에 대해 비전 인코더가 동결되거나 냉동되지 않은 핀셋을 사용했다.\n' +
      '\n' +
      '질문 및 정확한 테트플레이트.\n' +
      '\n' +
      '방법 섹션에서 언급했듯이, 우리는 템플릿을 통해 질문을 합성하고 쌍을 응답한다. "노란색 바나나"와 "집 모양의 케이크"와 같은 한 쌍의 물체에 대한 설명을 감안할 때, 우리의 데이터 합성 파이프라인은 질문 유형을 기반으로 효과적으로 답을 추출할 수 있다.\n' +
      '\n' +
      '여기에서 그림 14와 그림 15에서 질문과 답변 유형의 분포를 제공하고 각 범주에 대한 설명이 뒤따른다.\n' +
      '\n' +
      '1. **left 술어** A 문제는 물체 A가 뷰어의 관점에서 물체 B의 왼쪽에 있는지 묻는 질문이다. 솔루션은 자연어로 표현된 바이너리 술어가 사실 또는 거짓, 또는 불확실성을 표현하는 어구이다.\n' +
      '2. ** 오른쪽 술어** A 문제는 대상 A가 시청자의 관점에서 물체 B의 권리에 해당하는지 묻는 질문이다. 솔루션은 자연어로 표현된 바이너리 술어가 사실 또는 거짓, 또는 불확실성을 표현하는 어구이다.\n' +
      '3. **above 술어*** A 물체는 물체 A가 물체 B. Requires 좌표 표준화 이상인지 묻는 질문이다. 솔루션은 자연어로 표현된 바이너리 술어가 사실 또는 거짓, 또는 불확실성을 표현하는 어구이다.\n' +
      '4. **below 술어*** A 물체는 물체 A가 물체 B. Requires 좌표 표준화 아래에 있는지 묻는 질문이다. 솔루션은 자연어로 표현된 바이너리 술어가 사실 또는 거짓, 또는 불확실성을 표현하는 어구이다.\n' +
      '5. ** 역전 술어** A 문항은 뷰어의 관점에서 물체 B의 뒤쪽에 있는 물체 A가 있는지 묻는 질문이다. 솔루션은 자연어로 표현된 바이너리 술어가 사실 또는 거짓, 또는 불확실성을 표현하는 어구이다.\n' +
      '6. ** 선의의 술어** A 문제는 물체 A가 물체 B 앞에 있는지 묻는 질문이며, 솔루션은 자연어로 표현된 이진 술어 진품인지 거짓인지, 불확실성을 표현하는 표현이다.\n' +
      '\n' +
      '그림 13: 인간 정렬에서 우리는 인간의 결정 규칙을 모방할 확률로 라운드하는 일련의 규칙을 정의한다.\n' +
      '\n' +
      '7. **tall 술어** A 문제는 물체 A가 물체 B. Requires 좌표 표준화보다 더 큰지 묻는 질문이다. 솔루션은 자연어로 표현된 바이너리 술어가 사실 또는 거짓, 또는 불확실성을 표현하는 어구이다.\n' +
      '8. **ort 술어** A 질문은 물체 A가 물체 B. Requires 좌표 표준화보다 짧은지 묻는 질문이다. 솔루션은 자연어로 표현된 바이너리 술어가 사실 또는 거짓, 또는 불확실성을 표현하는 어구이다.\n' +
      '9. ** 전체의 술어** A 문제는 물체 A가 물체 B보다 더 넓은지 묻는 질문이며, 솔루션은 자연어로 표현된 이진 술어가 사실인지 거짓인지, 불확실성을 표현하는 표현이다.\n' +
      '10. **thin 술어** A 문제는 물체 A가 물체 B보다 얇는지 묻는 질문이며, 솔루션은 자연어로 표현된 이진 술어가 사실인지 거짓인지, 불확실성을 표현하는 표현이다.\n' +
      '11. **빅 술어** A 질문은 물체 A가 물체 B. Requires 좌표 표준화보다 더 큰지 묻는 질문이다. 솔루션은 자연어로 표현된 바이너리 술어가 사실 또는 거짓, 또는 불확실성을 표현하는 어구이다.\n' +
      '12. ** 작은 술어*** A 질문은 물체 A가 물체 B. 리퀴어 좌표 표준화보다 작는지 묻는 질문이다. 솔루션은 자연어로 표현된 바이너리 술어가 사실 또는 거짓, 또는 불확실성을 표현하는 어구이다.\n' +
      '13. **left 선택** A 질문은 대상 A와 객체 B 중 어느 것이 시청자의 관점에서 왼쪽에 더 있는지 묻는 질문이다. 솔루션은 자연어로 표현된 객체명, 또는 불확실성을 표현하는 문구이다.\n' +
      '14. **오른쪽 선택*** A 문항은 대상 A와 객체 B 중 어느 것이 시청자의 관점에서 옳은지 묻는 질문이다. 솔루션은 자연어로 표현된 객체명, 또는 불확실성을 표현하는 문구이다.\n' +
      '15. **above 선택**A 질문은 물체 A와 객체 B 중 어느 것이 더 위상에 있는지 묻는 질문이다. 좌표 표준화를 요구합니다. 솔루션은 자연어로 표현된 객체명, 또는 불확실성을 표현하는 문구이다.\n' +
      '16. **below 선택** A 질문은 물체 A와 객체 B 중 어느 것이 더 아래에 있는지 묻는 질문이다. 좌표 표준화를 요구합니다. 솔루션은 자연어로 표현된 객체명, 또는 불확실성을 표현하는 문구이다.\n' +
      '\n' +
      '17. ** 역전 선택** A 질문은 물체 A와 객체 B 중 어느 것이 더 뒤처졌는지 묻는 질문이다. 솔루션은 자연어로 표현된 객체명, 또는 불확실성을 표현하는 문구이다.\n' +
      '18. ** 선행 선택** A 문항은 뷰어의 관점에서 물체 A와 객체 B의 어느 것이 전방에 더 있는지 묻는 질문이다. 솔루션은 자연어로 표현된 객체명, 또는 불확실성을 표현하는 문구이다.\n' +
      '19. **tall 선택** A 질문은 물체 A와 객체 B 중 어느 것이 키가 큰지 묻는 질문이다. 표준화를 요구합니다. 솔루션은 자연어로 표현된 객체명, 또는 불확실성을 표현하는 문구이다.\n' +
      '\n' +
      '그림 14: 표준화가 실패할 때 생성된 질문 응답 범주의 유통이다.\n' +
      '\n' +
      '20. **ort 선택** A 질문은 물체 A와 객체 B 중 어느 것이 더 짧은지 묻는 질문이다. 표준화를 요구합니다. 솔루션은 자연어로 표현된 객체명, 또는 불확실성을 표현하는 문구이다.\n' +
      '21. ** 전체의 선택** A 질문은 객체 A와 객체 B 중 어느 것이 더 넓는지 묻는 질문이다. 솔루션은 자연어로 표현된 객체명, 또는 불확실성을 표현하는 문구이다.\n' +
      '22. **thin 선택**A 질문은 물체 A와 객체 B 중 어느 것이 더 얇아졌는지 묻는 질문이다. 솔루션은 자연어로 표현된 객체명, 또는 불확실성을 표현하는 문구이다.\n' +
      '23. **빅 선택** A 질문은 물체 A와 객체 B 중 어느 것이 더 큰지 묻는 질문이다. 솔루션은 자연어로 표현된 객체명, 또는 불확실성을 표현하는 문구이다.\n' +
      '24. ** 작은 선택** A 질문은 물체 A와 객체 B 중 어느 것이 더 작는지 묻는 질문이다. 솔루션은 자연어로 표현된 객체명, 또는 불확실성을 표현하는 문구이다.\n' +
      '25. **left-오른쪽은** A 문항으로 두 객체 간의 좌우 비교 관계를 묻는 문항이 있다. 솔루션은 자연어로 표현된 좌우로 표현되거나 불확실성을 표현하는 구절이다.\n' +
      '26. **above-below는** A 문항으로 두 객체 간의 위 각 비교 관계를 묻는 문항이다. 표준화를 요구합니다. 솔루션은 자연어로 표현되는 위무 또는 불확실성을 표현하는 어구이다.\n' +
      '27. ** 역전 선은** A 문항으로 두 객체 간의 배후 비교 관계를 묻는 문항이 있다. 솔루션은 자연어로 뒤쪽으로 표현되거나 불확실성을 표현하는 구절이다.\n' +
      '28. ** tall-short는** A 문항으로 두 객체 간의 높이-단축 비교 관계를 묻는 문항이 있다. 표준화를 요구합니다. 솔루션은 자연어, 또는 불확실성을 표현하는 문구로 표현된다.\n' +
      '29. ** 폭-스트린 구분** A 문항은 두 객체 간의 광범위한 비교 관계를 묻는 문항이다. 솔루션은 자연어로 표현되는 광폭 또는 불확실성을 표현하는 표현이다.\n' +
      '30. ** 빅-작은 분류** A 질문은 두 객체 간의 큰 작은 비교 관계를 묻는 질문이다. 표준화를 요구합니다. 솔루션은 자연어로 크게 표현되거나 불확실성을 표현하는 구절이다.\n' +
      '31. **거리 추정*** 두 물체의 중심 사이의 거리를 묻는 질문이다. 솔루션은 자연어로 표현된 거리이며, 라운딩을 위한 사람 같은 분포이다.\n' +
      '32. **gap 추정*** 두 사물의 격차를 묻는 질문. 솔루션은 자연어로 표현된 거리이며, 라운딩을 위한 사람 같은 분포이다.\n' +
      '33. **height 추정*** 오브젝트 높이에 대해 묻는 질문. 솔루션은 자연어로 표현된 거리이며, 라운딩을 위한 사람 같은 분포이다. 표준화를 요구합니다.\n' +
      '34. **폭 추정***는 물체의 폭을 묻는 질문이다. 용액은 거리입니다.\n' +
      '\n' +
      '그림 15: 표준화가 성공할 때 생성된 질문 응답 범주의 유통이다.\n' +
      '\n' +
      '자연어로 표현되며, 반올림을 위한 인간과 같은 분포이다.\n' +
      '35. **파이낸스 추정***는 물체의 상승을 묻는 질문이다. 솔루션은 자연어로 표현된 거리이며, 라운딩을 위한 사람 같은 분포이다. 표준화를 요구합니다.\n' +
      '36. **수직 거리 추정** A 질문은 두 물체의 중심 사이의 수직 거리를 묻는 질문이다. 솔루션은 자연어로 표현된 거리이며, 라운딩을 위한 사람 같은 분포이다. 표준화를 요구합니다.\n' +
      '37. **수평거리 추정*** 두 물체의 중심 사이의 수평 거리를 묻는 질문이다. 솔루션은 자연어로 표현된 거리이며, 라운딩을 위한 사람 같은 분포이다. 표준화를 요구합니다.\n' +
      '38. **above 차이 추정*** A 문제는 더 높은 물체의 바닥과 덜 상승된 물체의 바닥 사이의 거리를 묻는 질문이다. 솔루션은 자연어로 표현된 거리이며, 라운딩을 위한 사람 같은 분포이다. 표준화를 요구합니다.\n' +
      '39. **벨로 차이 추정*** A 문제는 상승된 적은 물체의 바닥과 더 높은 물체의 바닥 사이의 거리를 묻는 질문이다. 솔루션은 자연어로 표현된 거리이며, 라운딩을 위한 사람 같은 분포이다. 표준화를 요구합니다.\n' +
      '40. ** 역전차 추정*** A 문제는 카메라 광선 단독의 다른 거리 뒤에서 물체가 얼마나 뒤처져 있는지 묻는 질문이다. 솔루션은 자연어로 표현된 거리이며, 라운딩을 위한 사람 같은 분포이다.\n' +
      '41. ** 선차 추정** A 문제는 카메라 광선 단독의 다른 거리 앞에 물체가 어느 정도 있는지를 묻는 문항이다. 솔루션은 자연어로 표현된 거리이며, 라운딩을 위한 사람 같은 분포이다.\n' +
      '42. **left 차이 추정*** A 문제는 시청자의 관점에서 다른 사람의 왼쪽에 물체가 얼마나 있는지 묻는 질문이다. 솔루션은 자연어로 표현된 거리이며, 라운딩을 위한 사람 같은 분포이다.\n' +
      '43. ** 오른쪽 차이 추정*** A 문제는 시청자의 관점에서 목적물이 다른 사람의 권리에 얼마나 미치는지를 묻는 질문이다. 솔루션은 자연어로 표현된 거리이며, 라운딩을 위한 사람 같은 분포이다.\n' +
      '\n' +
      'QA 데이터를 생성하기 위한 작은 질문 세트와 답변 쌍을 제공합니다. 전체 목록은 당사 웹사이트를 참조하십시오.\n' +
      '\n' +
      '"[X]" 거리_분"=[[[B]"[A]" "OJ_B""""""""[A]""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n' +
      '"A]와 A[A] 사이의 거리", "A[A]와 [B]의 거리", "A[A]와 [B]의 거리", "A[A]에서 [B]의 거리", "A[A]에서 [B] 사이의 거리, a[A]는 어느 정도 떨어져 있는가", "A[A]와 [B]의 거리", "A[A]와 [B]의 거리", "A[A]의 거리, [B]의 거리", "A[A]에서 [B]의 거리", "A[A]의 거리", "A[A]와 [B]의 거리", "A[A]의 거리", "A[A]의 거리", "A[A]에서 [B]의 거리", "A[A]에서 [B]는 어느 정도 떨어져?[A]에서 [B]의 거리, [B]의 거리![A]의 거리, [B]의 거리.[A]와 [B]의 거리, [B]의 거리, [B]는 어떻게 멀다고?[A]와 [B]의 거리, [B]의 거리, a~[B]의 거리\n' +
      '\n' +
      '"A]는 [A]와 [B] 사이, [A]는 [X], [A]는 [X], [A]는 [X], [A]는 [B], ]는 [X], ]는 [B], ]는 "[A]는 [X], ]는 [B]와 떨어져 있다.\n' +
      '\n' +
      '"A]에서 [A] 사이의 수직 거리, [A]에서 [B] 사이, [A]에서 세로 거리, [B]까지, －A]는 어느 정도, ＋[A]에서 세로 거리, ＋[B]로, －A-B 사이, ＋[A]와 세로 거리, ＋[A]는 어느 정도, ＋[A]~[B]의 거리.\n' +
      '\n' +
      '"A]는 [A]와 [B] 사이, [A]는 [X], [A]는 [X], [A]는 [X], [B]는 [X], [A]는 [X], [B]는 세로로 떨어져 있다.\n' +
      '\n' +
      '"A-A 사이의 거리", ＋A-A 사이, ＋[B]~[B]의 가로거리", －A~[B]~[A]~[B]의 거리.  －A~[A]~[B]의 거리.  －A~[A]~[B]의 수평거리]  －A~[A]~[A]~[A]~[A]~[A]~[A]~[A]~[A]~[A]~[A]~[A]~[A]~[A]~[A]~[A]~[A]~[A]~[A]의 가로거리]~[A]~[A]~[A]는 수평거리]~[A]~[A]~[A]의 가로거리]~[A]~[A]~[A]~[A]의 가로거리]~[A]~[A]의 거리]~[A]~[A]의 가로거리]~[A]의 수평거리]~[A]~[A]의 거리]\n' +
      '\n' +
      '"A]는 가로에서 [A], [A]는 [X] 사이, [A]는 [X], [A]는 가로로 떨어져, [A]는 [X], [B]는 [X], [A]는 [X], [A]는 가로로 떨어져 있다.\n' +
      '\n' +
      '"A]의 가로폭은 어느 정도인가", <A]의 가로폭은?", <A]의 "[A]는 얼마나 큰가?", <A]의 가로폭은 어떻게 차지하고 있는가>, <A]의 가로폭은 무엇이고, >는 <A]는 얼마나 크다.\n' +
      '\n' +
      ']\n' +
      '\n' +
      '폭_산서=[X][[A]]  ＋[A]의 폭은 [X], ＋[A]는 [X], ＋[A]는 [X], ]는 [X], ]는 "[X], ]는 넓다.\n' +
      '\n' +
      '비레디케이트_쿼션 = [B] 뒤에 [A]가[B]에 비해 더 먼가요", "[A]가 [B]에 위치합니까", "[A]가 [B]에 비해 더 멀어지는가?", "[A]가 [B]에 위치한다.\n' +
      '\n' +
      '응, 그래, 그래, 그래.\n' +
      '\n' +
      '아니, 안 돼, 안 돼.\n' +
      '\n' +
      '\'레디케이트_쿼션=[B] 앞에 [A]가 [B] 앞에 위치하고 있는가?", <A]가 [B] 앞에 위치하고 있는가>, <B]에 비해 [A]가 "[B] 앞에 위치하고 있는가?", <A]가 [B] 앞에 위치하고 있는가?", <A]가 [B] 앞에 위치하고 있는가?", <B]에 비해 [A]가 "B] 앞에 위치하고 있는가?", －A]가 [B]에 가깝습니까?", <B]에 비해 [A]가 [B]에 더 가깝습니까?", ＜[B]에 비해 [A]가 [B]에 가깝습니까?", ＋", ，[B]에 비해 [A]가 [B]에 가깝습니까? <B]와 비교하면 [A]가.\n' +
      '\n' +
      '네, 맞습니다. "네, [B] 앞입니다." "네, [A]는 시청자 앞에 있습니다.\n' +
      '\n' +
      '"아닙니다, 아니야" "아니야, 그렇지 않다" "아니, [B]" "아니, [A]" "아니, [A] 뒤[A]".\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>