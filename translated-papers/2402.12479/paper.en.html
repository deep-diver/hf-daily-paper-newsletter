<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# In deep reinforcement learning, a pruned network is a good network\n' +
      '\n' +
      'Johan Obando-Ceron1,2,3, Aaron Courville2,3 and Pablo Samuel Castro1,2,3\n' +
      '\n' +
      '1Google DeepMind, 2Mila - Quebec AI Institute, 3Universite de Montreal\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters. We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning enables agents to maximize parameter effectiveness. This results in networks that yield dramatic performance improvements over traditional networks and exhibit a type of "scaling law", using only a small fraction of the full network parameters.\n' +
      '\n' +
      '2024-21\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Despite successful examples of deep reinforcement learning (RL) being applied to real-world problems (Bellemare et al., 2020; Berner et al., 2019; Fawzi et al., 2022; Mnih et al., 2015; Vinyals et al., 2019), there is growing evidence of challenges and pathologies arising when training these networks (Ceron et al., 2023; Graesser et al., 2022; Kumar et al., 2021; Lyle et al., 2022; Nikishin et al., 2022; Ostrovski et al., 2021; Sokar et al., 2023). In particular, it has been shown that deep RL agents _under-utilize_ their network\'s parameters: Kumar et al. (2021) demonstrated that there is an implicit underparameterization, Sokar et al. (2023) revealed that a large number of neurons go dormant during training, and Graesser et al. (2022) showed that sparse training methods can maintain performance with a very small fraction of the original network parameters.\n' +
      '\n' +
      'One of the most surprising findings of this last work is that applying the gradual magnitude pruning technique proposed by Zhu and Gupta (2017) on DQN (Mnih et al., 2015) with a ResNet backbone (as introduced in Impala (Espeholt et al., 2018)), results in a 50% performance improvement over the dense counterpart, with only 10% of the original parameters (see the bottom right panel of Figure 1 of Graesser et al. (2022)). Curiously, when the same pruning technique is applied to the original CNN architecture there are no performance improvements, but no degradation either.\n' +
      '\n' +
      'That the same pruning technique can have such qualitatively different, yet non-negative, results by simply changing the underlying architecture is interesting. It suggests that training deep RL agents with non-standard network topologies (as induced by techniques such as gradual magnitude pruning) may be generally useful, and warrants a more profound investigation.\n' +
      '\n' +
      'In this paper we explore gradual magnitude pruning as a general technique for improving the performance of RL agents. We demonstrate that in addition to improving the performance of standard network architectures, the gains increase proportionally with the size of the base network architecture. This last point is significant, as deep RL networks are known to struggle with scaling architectures (Farebrother et al., 2023; Ota et al., 2021; Schwarzer et al., 2023; Taiga et al., 2023).\n' +
      '\n' +
      'Figure 1: **Scaling network widths for ResNet architecture**, for DQN and Rainbow with an Impala-based ResNet. We report the interquantile mean after 40 million environment steps, aggregated over 15 games with 5 seeds each; error bars indicate 95% stratified bootstrap confidence intervals.\n' +
      '\n' +
      'Our main contributions are as follows. We:\n' +
      '\n' +
      '* present gradual magnitude pruning as a general technique for maximizing parameter efficiency in RL;\n' +
      '* demonstrate that networks trained with this technique produce stronger agents than their dense counterparts, and exhibit a type of "scaling law" as we scale the size of the initial network;\n' +
      '* show this technique is generally useful across a varied set of agents and training regimes;\n' +
      '* present in-depth analyses to better understand the reasons behind their benefits.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Scaling in Deep RLDeep neural networks have been the driving factor behind many of the successful applications of reinforcement learning to real-world tasks. However, it has been historically difficult to scale these networks, in a manner similar to what has led to the "scaling laws" in supervised learning, without performance degradation; this is due in large part to exacerbated training instabilities that are endemic to reinforcement learning (Ota et al., 2021; Sinha et al., 2020; Van Hasselt et al., 2018). Recent works that have been able to do so successfully have had to rely on a number of targeted techniques and careful hyper-parameter selection (Ceron et al., 2023; Farebrother et al., 2023; Ota et al., 2021; Schwarzer et al., 2023; Taiga et al., 2023).\n' +
      '\n' +
      'Cobbe et al. (2020); Farebrother et al. (2023) and Schwarzer et al. (2023) switched from the original CNN architecture of Mnih et al. (2015) to a ResNet based architecture, as proposed by Espeholt et al. (2018), which proved to be more amenable to scaling. Cobbe et al. (2020) and Farebrother et al. (2023) observe advantages when increasing the number of features in each layer of the ResNet architecture. Schwarzer et al. (2023) show that the performance of their agent (BBF) continues to grow proportionally with the width of their network. Bjorck et al. (2021) propose spectral normalization to mitigate training instabilities and enable scaling of their architectures. Ceron et al. (2023) propose reducing batch sizes for improved performance, even when scaling networks.\n' +
      '\n' +
      'Sparse Models in Deep RLPrevious studies (Schmitt et al., 2018; Zhang et al., 2019) have employed knowledge distillation with static data to mitigate instability, resulting in small, but dense, agents. Livne and Cohen (2020) introduced policy pruning and shrinking, utilizing iterative policy pruning similar to iterative magnitude pruning (Han et al., 2015), to obtain a sparse DRL agent. The exploration of the lottery ticket hypothesis in DRL was initially undertaken by Yu et al. (2019), and later Vischer et al. (2021) demonstrated that a sparse winning ticket can also be identified through behavior cloning. Sokar et al. (2021) proposed the use of structural evolution of network topology in DRL, achieving 50%\n' +
      '\n' +
      'Figure 3: **Evaluating how varying sparsity affects performance** for DQN with the ResNet architecture and a width multiplier of 3. See Section 4.1 for training details.\n' +
      '\n' +
      'Figure 2: **Gradual magnitude pruning schedules** used in our experiments, to a target sparsity of 95%, as specified in Equation 1.\n' +
      '\n' +
      ' sparsity with no performance degradation. Arnob et al. (2021) introduced single-shot pruning for offline Reinforcement Learning. Graesser et al. (2022) discovered that pruning often yields improved results, and _dynamic_ sparse training methods, where the sparse topology changes throughout training (Evci et al., 2020; Mocanu et al., 2018), can significantly outperform _static_ sparse training, where the sparse topology remains fixed throughout training. Tan et al. (2023) enhance the efficacy of dynamic sparse training through the introduction of a novel delayed multi-step temporal difference target mechanism and a dynamic-capacity replay buffer. Grooten et al. (2023) proposed an automatic noise filtering method, which uses the principles of dynamic sparse training for adjusting the network topology to focus on task-relevant features.\n' +
      '\n' +
      'Overparameterization in Deep RLSong et al. (2019) and Zhang et al. (2018) highlighted and the tendency of RL networks to overfit, while Nikishin et al. (2022) and Sokar et al. (2023) demonstrated the prevalence of plasticity loss in RL networks, leading to a decline in final performance.\n' +
      '\n' +
      'Several strategies have been proposed to mitigate this, such as data augmentation (Cetin et al., 2022; Yarats et al., 2021), dropout (Gal and Ghahramani, 2016), and layer and batch normalization (Ba et al., 2016; Ioffe and Szegedy, 2015). Hiraoka et al. (2021) demonstrated the success of employing dropout and layer normalization in Soft Actor-Critic (Haarnoja et al., 2018), while Liu et al. (2020) identified that applying \\(\\ell_{2}\\) weight regularization on actors can enhance both on- and off-policy algorithms.\n' +
      '\n' +
      'Nikishin et al. (2022) identify a tendency of networks to overfit to early data (the primacy bias), which can hinder subsequent learning, and propose periodic network re-initialization as a means to mitigate it. Similarly, Sokar et al. (2023) proposed re-initializing _dormant neurons_ to improve network plasticity, while Nikishin et al. (2023) propose plasticity injection by temporarily freezing the current network and utilizing newly initialized weights to facilitate continuous learning.\n' +
      '\n' +
      '## 3 Background\n' +
      '\n' +
      'Deep reinforcement learningThe goal in Reinforcement Learning is to optimize the cumulative discounted return over a long horizon, and is typically formulated as a Markov decision process (MDP) \\((\\mathcal{X},\\mathcal{A},P,r,\\boldsymbol{\\nu})\\). An MDP is comprised of a state space \\(\\mathcal{X}\\), an action space \\(\\mathcal{A}\\), a transition dynamics model \\(P:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\Delta(\\mathcal{X})\\) (where \\(\\Delta(X)\\) is a distribution over a set \\(X\\)), a reward function \\(\\mathcal{R}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R}\\), and a discount factor \\(\\boldsymbol{\\nu}\\in[0,1)\\). A policy \\(\\pi:\\mathcal{X}\\rightarrow\\Delta(\\mathcal{A})\\) formalizes an agent\'s behaviour. For a policy \\(\\pi\\), \\(Q^{\\pi}(\\mathbf{x},\\mathbf{a})\\) represents the expected discounted reward achieved by taking action \\(\\mathbf{a}\\) in state \\(\\mathbf{x}\\) and subsequently following the policy \\(\\pi\\): \\(Q^{\\pi}(\\mathbf{x},\\mathbf{a}):=\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}y^{ t}\\mathcal{R}\\left(\\mathbf{x}_{t},\\mathbf{a}_{t}\\right)|\\mathbf{x}_{0}=x, \\mathbf{a}_{0}=a\\right]\\). The optimal Q-function, denoted as \\(Q^{\\star}(\\mathbf{x},\\mathbf{a})\\), satisfies the Bellman recurrence:\n' +
      '\n' +
      '\\[Q^{\\star}(\\mathbf{x},\\mathbf{a})=\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim P( \\mathbf{x}^{\\prime}|\\mathbf{x},\\mathbf{a})}\\left[\\mathcal{R}(\\mathbf{x}, \\mathbf{a})+\\gamma\\max_{\\mathbf{a}^{\\prime}}Q^{\\star}\\left(\\mathbf{x}^{\\prime},\\mathbf{a}^{\\prime}\\right)\\right]\\]\n' +
      '\n' +
      'Most modern value-based methods will approximate \\(Q\\) via a neural network with parameters \\(\\theta\\), denoted as \\(Q_{\\theta}\\). This idea was introduced by Mnih et al. (2015) with their DQN agent, which has served as the basis for most modern deep RL algorithms. The network \\(Q_{\\theta}\\) is typically trained with a _temporal difference loss_, such as:\n' +
      '\n' +
      '\\[L(\\theta)=\\] \\[\\mathbb{E}_{(\\mathbf{x},\\mathbf{a},\\mathbf{r},\\mathbf{x}^{\\prime}) \\sim\\mathcal{D}}\\left[\\left(\\mathbf{r}+\\gamma\\max_{\\mathbf{a}^{\\prime}\\in \\mathcal{A}}\\bar{Q}\\left(\\mathbf{x}^{\\prime},\\mathbf{a}^{\\prime}\\right)-Q_{ \\theta}(\\mathbf{x},\\mathbf{a})\\right)^{2}\\right],\\]\n' +
      '\n' +
      'Figure 4: **Scaling network widths for the original CNN architecture of Mnih et al. (2015), for DQN (left) and Rainbow (right). See Section 4.1 for training details.**\n' +
      '\n' +
      'Here \\(\\mathcal{D}\\) represents a stored collection of transitions \\((\\mathbf{x}_{t},\\mathbf{a}_{t},\\mathbf{r}_{t},\\mathbf{x}_{t+1})\\) which the agent samples from for learning (known as the replay buffer). \\(\\bar{Q}\\) is a static network that infrequently copies its parameters from \\(Q_{\\theta}\\); its purpose is to produce stabler learning targets.\n' +
      '\n' +
      'Rainbow (Hessel et al., 2018) extended, and improved, the original DQN algorithm with double Q-learning (van Hasselt et al., 2016), prioritized experience replay (Schaul et al., 2016), dueling networks (Wang et al., 2016), multi-step returns (Sutton, 1988), distributional reinforcement learning (Bellemare et al., 2017), and noisy networks (Fortunato et al., 2018).\n' +
      '\n' +
      'Gradual pruningIn supervised learning settings there is a broad interest in sparse training techniques, whereby only a subset of the full network parameters are trained/used (Gale et al., 2019). This is motivated by computational and space efficiency, as well as speed of inference. Zhu and Gupta (2017) proposed a polynomial schedule for gradually sparsifying a dense network over the course of training by _pruning_ model parameters with low weight magnitudes.\n' +
      '\n' +
      'Specifically, let \\(s_{F}\\in[0,1]\\) denote the final desired sparsity level (e.g. 0.95 in most of our experiments) and let \\(t_{\\text{start}}\\) and \\(t_{\\text{end}}\\) denote the start and end iterations of pruning, respectively; then the sparsity level at iteration \\(t\\) is given by:\n' +
      '\n' +
      '\\[s_{t}= \\tag{1}\\] \\[s_{F}\\left(1-\\left(1-\\frac{t-t_{\\text{start}}}{t_{\\text{end}}-t_ {\\text{start}}}\\right)^{3}\\right)\\] if \\[t_{\\text{start}}\\leq t\\leq t_{\\text{end}}\\] \\[0.0\\] if \\[t<t_{\\text{start}}\\] \\[s_{F}\\] if \\[t>t_{\\text{end}}\\]\n' +
      '\n' +
      'Graesser et al. (2022) applied this idea to deep RL networks, setting \\(t_{\\text{start}}\\) at 20% of training and \\(t_{\\text{end}}\\) at 80% of training.\n' +
      '\n' +
      '## 4 Pruning boosts deep RL performance\n' +
      '\n' +
      'We investigate the general usefulness of gradual magnitude pruning in deep RL agents in both online and offline settings.\n' +
      '\n' +
      '### Implementation details\n' +
      '\n' +
      'For the base DQN and Rainbow agents we use the Jax implementations of the Dopamine library (Castro et al., 2018) with their default values. It is worth noting that Dopamine provides a "compact" version of the original Rainbow agent, using only multi-step updates, prioritized replay, and distributional RL. For all experiments we use the Impala architecture introduced by Espeholt et al. (2018), which is a 15-layer ResNet, unless otherwise specified. Our reasoning for this is not only because of the results from Graesser et al. (2022), but also due to a number of recent works demonstrating that this architecture results in generally improved performance (Kumar et al., 2022; Schmidt and Schmied, 2021; Schwarzer et al., 2023).\n' +
      '\n' +
      'We use the JaxPruner (Lee et al., 2024) library for gradual magnitude pruning, as it already provides integration with Dopamine. We follow the schedule of Graesser et al. (2022): begin pruning the network 20% into training and stop at 80%, keeping the final sparse network fixed for the rest of training. Figure 2 illustrates the pruning schedules used in our experiments (for 95% sparsity).\n' +
      '\n' +
      'Figure 5: **Scaling replay ratio for Rainbow with the ResNet architecture with a width of 3**. Default replay ratio is 0.25. See Section 4.1 for training details.\n' +
      '\n' +
      'We evaluate our agents on the Arcade Learning Environment (ALE) (Bellemare et al., 2013) on the same 15 games used by Graesser et al. (2022), chosen for their diversity1. For computational considerations, most experiments were conducted over 40 million frames (as opposed to the standard 200 million); in our investigations we found the qualitative differences between algorithms at 40 million frames to be mostly consistent with those at 100 million (e.g. see Figure 10).\n' +
      '\n' +
      'Footnote 1: Discussed in A.4 in Graesser et al. (2022).\n' +
      '\n' +
      'We follow the guidelines outlined by Agarwal et al. (2021) for evaluation: each experiment was run with 5 independent seeds, and we report the human-normalized interquantile mean (IQM), aggregated across the 15 games, configurations, and seeds, along with 95% stratified bootstrap confidence intervals. All experiments were run on NVIDIA Tesla P100 GPUs, and each took approximately 2 days to complete.\n' +
      '\n' +
      '### Online RL\n' +
      '\n' +
      'While Graesser et al. (2022) demonstrates that sparse networks are capable of maintaining agent performance, if these levels of sparsity were too high, performance eventually degrades. This is intuitive, as with higher levels of sparsity, there are fewer active parameters left in the network. One natural question is whether _scaling_ our initial network enables high levels of sparsity. We thus begin our inquiry by applying gradual magnitude pruning on DQN with the Impala architecture, where we have scaled the convolutional layers by a factor of 3. Figure 3 confirms that this is indeed the case: 90% and 95% sparsity produce a 33% performance improvement, and 99% sparsity maintains performance.\n' +
      '\n' +
      'A sparsity of 95% consistently yielded the best performance in our initial explorations, so we primarily focus on this sparsity level for our investigations. Figure 1 is a striking result: we observe close to a 60% (DQN) and 50% (Rainbow) performance improvement over the original (unpruned and unscaled) architectures. Additionally, while the performance of the unpruned architectures decreases monotonically with increasing widths, the performance of the pruned counterparts _increases_ monotonically. In Figure 7 we evaluated pruning on DQN and Rainbow over all 60 Atari 2600 games, confirming our findings are not specific to the 15 games initially selected.\n' +
      '\n' +
      'When switching both agents to using the original CNN architecture of Mnih et al. (2015) we see a similar trend with Rainbow, but see little improvement in DQN (Figure 4). This result is consistent with the findings of Graesser et al. (2022), where no real improvements were observed with pruning on CNN architectures. An interesting observation is that, with this CNN architecture, the performance of DQN does seem to benefit from increased width, while the performance of Rainbow suffer from slight degradation.\n' +
      '\n' +
      'Our findings thus far suggest that the use of gradual magnitude pruning increases the parameter efficiency of these agents. If so, then these sparse networks should also be able to benefit from more gradient updates. The _replay ratio_, which is the number of gradient updates per environment step, measures exactly this; it is well-known that it is difficult to increase this value without performance degradation (D\'Oro et al.,\n' +
      '\n' +
      'Figure 6: **Scaling network widths for offline agents** CQL **(left)** and CQL+C51 **(right)**, both using the ResNet architecture. We report interquantile mean performance with error bars indicating 95% confidence intervals across 17 Atari games. x-axis represents gradient steps; no new data is collected.\n' +
      '\n' +
      '2023; Fedus et al., 2020; Nikishin et al., 2022; Schwarzer et al., 2023). In Figure 5 we can indeed confirm that the pruned architectures maintain a performance lead over the unpruned baseline even at high replay ratio values.\n' +
      '\n' +
      '### Low data regime\n' +
      '\n' +
      'Kaiser et al. (2020) introduced the Atari 100k benchmark to evaluate RL agents in a sample-constrained setting, allowing agents only 100k environment interactions. For this regime, Kostrikov et al. (2020) introduced DrQ, a variant of DQN which makes use of data augmentation; the hyperparameters for this agent were further optimized by Agarwal et al. (2021) in DrQ(\\(\\epsilon\\)). Similarly, Van Hasselt et al. (2019) introduced Data-Efficient Rainbow (DER), which optimized the hyperparameters of Rainbow (Hessel et al., 2018) for this low data regime.\n' +
      '\n' +
      'When evaluated on this low data regime, our pruned agents demonstrated no gains. However, when we ran for 40M environment interactions (as suggested by Ceron et al. (2023)), we do observe significant gains when using gradual magnitude pruning, as shown in Figure 8. Interestingly, In DrQ(\\(\\epsilon\\)) the pruned agents avoid the performance degradation affecting the baseline when trained for longer.\n' +
      '\n' +
      '### Offline RL\n' +
      '\n' +
      'Offline reinforcement learning focuses on training an agent solely from a fixed dataset of samples without any environment interactions. We used two recent state of the art methods from the literature: CQL (Kumar et al., 2020) and CQL+C51 (Kumar et al., 2022), both with the ResNet architecture from Espeholt et al. (2018). Following Kumar et al. (2021), we trained these agents on 17 Atari games for 200 million frames iterations, where where 1 iteration corresponds to 62,500 gradient updates. We assessed the agents by considering a dataset composed of a random 5% sample of all the environment interactions collected by a DQN agent trained for 200M environment steps (Agarwal et al., 2020).\n' +
      '\n' +
      'Note that since we are training for a different number of steps than our previous experiments, we adjust the pruning schedule accordingly. As shown in Figure 6, both CQL and CQL+C51 observe significant gains when using pruned networks, in particular with wider networks. Interestingly, in the offline regime, pruning also helps to avoid performance collapse when using a shallow network (width scale equal to 1), or even improve final performance as in the case of CQL+C51.\n' +
      '\n' +
      '### Actor-Critic methods\n' +
      '\n' +
      'Our investigation thus far has focused on value-based methods. Here we investigate if gradual magnitude pruning can yield performance gains for Soft Actor Critic (SAC; Haarnoja et al., 2018), a popular policy-gradient algorithm. We evaluated SAC on five continuous control environments from the MuJoCo suite (Todorov et al., 2012), using 10 independent seeds for each. In Figure 9 we present the results for Walker2d-v2 and Ant-v2, where we see the advantages of gradual magnitude pruning persist; in the remaining three environments (see Appendix E) there are no real observable gains from pruning.\n' +
      '\n' +
      'Figure 8: **Performance of DrQ(\\(\\epsilon\\)) (left) and DER (right) when trained for 40M frames**. Both agents use a ResNet architecture with a width of 3. See Section 4.1 for training details.\n' +
      '\n' +
      'Figure 7: **Evaluating performance on the full Atari 2600 suite.** DQN (left) and Rainbow (right), both using the ResNet architecture with a width of 3. See Section 4.1 for training details.\n' +
      '\n' +
      '### Stability of the pruned network\n' +
      '\n' +
      'We followed the pruning schedule proposed by Graesser et al. (2022), which adapts naturally to differing training steps (as discussed above for the offline RL experiments). This schedule trains the final sparse network for only the final 20% of training steps. A natural question is whether the resulting sparse network, when trained for longer, is still able to maintain its performance. To evaluate this, we trained DQN for 100 million frames and applied two pruning schedules: the regular schedule we would use for 100M as well as the schedule we would normally use for 40M training steps (see Figure 2).\n' +
      '\n' +
      'As Figure 10 shows, even with the compressed 40M schedule, the pruned network is able to maintains its strong performance. Interestingly, with the compressed schedule the agent achieves a higher performance _faster_ than with the regular one. This suggests there is ample room for exploring alternate pruning schedules.\n' +
      '\n' +
      '## 5 Why is pruning so effective?\n' +
      '\n' +
      'We focus our analyses on four games: BeamRider, Breakout, Enduro, and VideoPinball. For each, we measure the variance of the \\(Q\\) estimates (QVariance); the average norm of the network parameters (ParametersNorm); the average norm of the \\(Q\\)-values (QNorm); the effective rank of the matrix (Srank) as suggested by Kumar et al. (2021), and the fraction of dormant neurons as defined by Sokar et al. (2023). We present our results in Figure 11. What becomes evident from these figures is that pruning **(i)** reduces variance, **(ii)** reduces the norms of the parameters, **(iii)** decreases the number of dormant neurons, and **(iv)** increases the effective rank of the parameters. Some of these observations can be attributed to a form of normalization, whereas others may arise due to increased network plasticity.\n' +
      '\n' +
      '### Comparison to other methods\n' +
      '\n' +
      'In order to disentangle the impact of pruning from normalization and explicit plasticity injection, we compare against existing methods in the literature.\n' +
      '\n' +
      'Normalization baselinesTo investigate the role normalization plays on the performance gains produced by pruning, we consider two types of \\(\\ell_{2}\\) normalization that have proven effective in the literature. The first is weight decay (**WD**), a standard technique that adds an extra term to the loss that penalizes \\(\\ell_{2}\\) norm of the weights, thereby discouraging network parameters from growing too large. The second is **L2**, the regularization approach proposed by Kumar et al. (2022), which is designed to enforce an \\(\\ell_{2}\\) norm of 1 for the parameters.\n' +
      '\n' +
      'Plasticity injection baselinesWe compare against two recent works that proposed methods for directly dealing with loss of plasticity. Nikishin et al. (2022) observed a decline in performancewith an increased replay ratio, attributing it to overfitting on early samples, an effect they termed the "primacy bias". The authors suggested periodically resetting the network and demonstrated that it proved very effective at mitigating the privacy bias, and overfitting in general (this is labeled as **Reset** in our results). Sokar et al. (2023) demonstrated that most deep RL agents suffer from the _dormant neuron phenomenon_, whereby neurons increasingly "turn off" during training of deep RL agents, thus reducing network expressivity. To mitigate this, they proposed a simple and effective method that Recycles Dormant neurons (**ReDo**) throughout training.\n' +
      '\n' +
      'As Figure 12 illustrates, gradual magnitude pruning surpasses all the other regularization methods at all levels of scale, and throughout the entirety of training. Interestingly, most of the regularization methods suffer a degradation when increasing network width. This suggests that the effect of pruning cannot be solely attributed to either a form of normalization or plasticity injection. However, as we will see below, increased plasticity does seem to arise out of its use.\n' +
      '\n' +
      '### Impact on plasticity\n' +
      '\n' +
      'Plasticity is a neural network\'s capacity to rapidly adjust in response to shifting data distributions (Lewandowski et al., 2023; Lyle et al., 2022, 2023); given the non-stationarity of reinforcement learning, it is crucial to maintain to ensure good performance. However, RL networks are known to lose plasticity over the course of training (Lee et al., 2023; Nikishin et al., 2022; Sokar et al., 2023).\n' +
      '\n' +
      'Lyle et al. (2023) conducted an assessment of the covariance structure of gradients to examine the loss landscape of the network, and argued that improved performance, and increased plasticity, is often associated with weaker gradient correlation and reduced gradient interference. Our observations align with these findings, as illustrated in the gradient covariance heat maps in Figure 13. In dense networks, gradients exhibit a notable colinearity, whereas this colinearity is dramatically reduced in the pruned networks.\n' +
      '\n' +
      'Figure 11: **Empirical analyses for four representative games when applying pruning. From left to right: training returns, average \\(Q\\)-target variance, average parameters norm, average \\(Q\\)-estimation norm, _srank_(Kumar et al., 2021), and dormant neurons (Sokar et al., 2023). All results averaged over 5 seeds, shaded areas represent 95% confidence intervals.**\n' +
      '\n' +
      '## 6 Discussion and Conclusion\n' +
      '\n' +
      'Prior work has demonstrated that reinforcement learning agents have a tendency to under-utilize its available parameters, and that this under-utilization increases throughout training and is amplified as network sizes increase (Graesser et al., 2022; Nikishin et al., 2022; Schwarzer et al., 2023; Sokar et al., 2023). RL agents achieve strong performance in the majority of the established benchmarks with small networks (relative to those used in language models, for instance), so this evident parameter-inefficiency may be brushed off as being less critical than other, more algorithmic, considerations.\n' +
      '\n' +
      'As RL continues to grow outside of academic benchmarks and into more complex tasks, it is almost surely going to necessitate larger, and more expressive, networks. In this case parameter efficiency becomes crucial to avoid the performance collapse prior works have shown, as well as for reducing computational costs (Ceron and Castro, 2021).\n' +
      '\n' +
      'Our work provides convincing evidence that sparse training techniques such as gradual magnitude pruning can be effective at maximizing network utilization. Figures 1 and 4 suggest a type of "scaling law", the likes of which has mostly eluded deep RL networks. The results in Figures 6, 8, and 10 all demonstrate that the sparse networks\n' +
      '\n' +
      'Figure 12: **Comparison against network resets (Nikishin et al., 2022), weight decay, ReDo (Sokar et al., 2023) and the normalization of (Kumar et al., 2022). Left: Sample efficiency curves with a width factor of 3; Right: final performance after 40M frames with varying widths (right panel). All experiments run on DQN with the ResNet architecture and a replay ratio of 0.25.**\n' +
      '\n' +
      'Figure 13: **Gradient covariance matrices for Breakout (left) and VideoPinball (right) atari games. Dark red denotes high negative correlation, while dark blue indicates high positive correlation. The use of pruning induces weaker gradient correlation and less gradient interference, as evidenced by the paler hues in the heatmaps for the sparse networks.**produced by pruning are better at maintaining stable performance when trained for longer.\n' +
      '\n' +
      'Collectively, our results demonstrate that, by meaningfully removing network parameters throughout training, we can outperform traditional dense counterparts and continue to improve performance as we grow the initial network architectures. Our results with varied agents and training regimes imply gradual magnitude pruning is a generally useful technique which can be used as a "drop-in" for maximizing agent performance.\n' +
      '\n' +
      'It would be natural, then to explore incorporating gradual magnitude pruning into recent agents that were designed for multi-task generalization (Kumar et al., 2022; Taiga et al., 2023), sample efficiency (D\'Oro et al., 2023; Schwarzer et al., 2023), and generalizability (Hafner et al., 2023). Further, the observed stability of the pruned networks may have implications for methods which rely on fine-tuning or reincarnation (Agarwal et al., 2022).\n' +
      '\n' +
      'Recent advances in hardware accelerators for training sparse networks may result in faster training times, and serve as an incentive for further research in methods for sparse network training. Further, the fact that a consequence of this approach is a network with fewer parameters than when initialized renders it appealing for downstream applications on edge devices.\n' +
      '\n' +
      'At a minimum, we hope this work serves as an invitation to explore non-standard network architectures and topologies as an effective mechanism for maximizing the performance of reinforcement learning agents.\n' +
      '\n' +
      'AcknowledgementsThe authors would like to thank Laura Graesser, Gopeshh Subbaraj and the rest of the Google DeepMind Montreal team for valuable discussions during the preparation of this work. We would also like to thank the Python community Oliphant (2007); Van Rossum and Drake Jr (1995) for developing tools that enabled this work, including NumPy Harris et al. (2020), Matplotlib Hunter (2007) and JAX Bradbury et al. (2018).\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Agarwal et al. (2020) R. Agarwal, D. Schuurmans, and M. Norouzi. An optimistic perspective on offline reinforcement learning. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 104-114. PMLR, 13-18 Jul 2020. URL [https://proceedings.mlr.press/v119/agarwal20c.html](https://proceedings.mlr.press/v119/agarwal20c.html).\n' +
      '* Agarwal et al. (2021) R. Agarwal, M. Schwarzer, P. S. Castro, A. C. Courville, and M. Bellemare. Deep reinforcement learning at the edge of the statistical precipice. _Advances in neural information processing systems_, 34:29304-29320, 2021.\n' +
      '* Agarwal et al. (2022) R. Agarwal, M. Schwarzer, P. S. Castro, A. C. Courville, and M. Bellemare. Reincarnating reinforcement learning: Reusing prior computation to accelerate progress. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 28955-28971. Curran Associates, Inc., 2022.\n' +
      '* Arnob et al. (2021) S. Y. Arnob, R. Ohib, S. Plis, and D. Precup. Single-shot pruning for offline reinforcement learning. _arXiv preprint arXiv:2112.15579_, 2021.\n' +
      '* Ba et al. (2016) J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.\n' +
      '* Bellemare et al. (2013) M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. _Journal of Artificial Intelligence Research_, 47:253-279, jun 2013. doi: 10.1613/jair.3912.\n' +
      '* Bellemare et al. (2017) M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning. In _ICML_, 2017.\n' +
      '* 82, 2020.\n' +
      '* Berner et al. (2021) C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer,S. Hashme, C. Hesse, et al. (2019)Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680. Cited by: SS1.\n' +
      '* N. Bjorck, C. P. Gomes, and K. Q. Weinberger (2021)Towards deeper deep reinforcement learning with spectral normalization. Advances in Neural Information Processing Systems34, pp. 8242-8255. Cited by: SS1.\n' +
      '* J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, et al. (2018)Jax: composable transformations of python+ numpy programs. Cited by: SS1.\n' +
      '* P. S. Castro, S. Moitra, C. Gelada, S. Kumar, and M. G. Bellemare (2018)Dopamine: a research framework for deep reinforcement learning. arXiv preprint arXiv:1812.06110. Cited by: SS1.\n' +
      '* J. S. O. Ceron and P. S. Castro (2021)Revisiting rainbow: promoting more insightful and inclusive deep reinforcement learning research. In International Conference on Machine Learning, pp. 1373-1383. Cited by: SS1.\n' +
      '* J. S. O. Ceron, M. G. Bellemare, and P. S. Castro (2023)Small batch deep reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, pp. 2023. External Links: Link, Document Cited by: SS1.\n' +
      '* E. Cetin, P. J. Ball, S. Roberts, and O. Celiktutan (2022)Stabilizing off-policy deep reinforcement learning from pixels. In International Conference on Machine Learning, pp. 2784-2810. Cited by: SS1.\n' +
      '* K. Cobbe, C. Hesse, J. Hilton, and J. Schulman (2020)Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning, pp. 2048-2056. Cited by: SS1.\n' +
      '* P. D\'Oro, M. Schwarzer, E. Nikishin, P. Bacon, M. G. Bellemare, and A. Courville (2023)Sample-efficient reinforcement learning by breaking the replay ratio barrier. In The Eleventh International Conference on Learning Representations, External Links: Link Cited by: SS1.\n' +
      '* L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. (2018)Impala: scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pp. 1407-1416. Cited by: SS1.\n' +
      '* U. Evci, T. Gale, J. Menick, P. S. Castro, and E. Elsen (2020)Rigging the lottery: making all tickets winners. In International Conference on Machine Learning, pp. 2943-2952. Cited by: SS1.\n' +
      '* J. Farebrother, J. Greaves, R. Agarwal, C. L. Lan, R. Goroshin, P. S. Castro, and M. G. Bellemare (2023)Proto-value networks: scaling representation learning with auxiliary tasks. In Submitted to The Eleventh International Conference on Learning Representations, External Links: Link Cited by: SS1.\n' +
      '* A. Fawzi, M. Balog, A. Huang, T. Hubert, B. Romera-Paredes, M. Barekatain, A. Novikov, F. J. R Ruiz, J. Schrittwieser, G. Swirszcz, et al. (2022)Discovering faster matrix multiplication algorithms with reinforcement learning. Nature610 (7930), pp. 47-53. Cited by: SS1.\n' +
      '* W. Fedus, P. Ramachandran, R. Agarwal, Y. Bengio, H. Larochelle, M. Rowland, and W. Dabney (2020)Revisiting fundamentals of experience replay. In International Conference on Machine Learning, pp. 3061-3071. Cited by: SS1.\n' +
      '* M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih, R. Munos, D. Hassabis, O. Pietquin, C. Blundell, and S. Legg (2018)Noisy networks for exploration. Cited by: SS1.\n' +
      '* Y. Gal and Z. Ghahramani (2016)Dropout as a bayesian approximation: representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059. Cited by: SS1.\n' +
      '* Graesser et al. (2022) L. Graesser, U. Evci, E. Elsen, and P. S. Castro. The state of sparse training in deep reinforcement learning. In _International Conference on Machine Learning_, pages 7766-7792. PMLR, 2022.\n' +
      '* Grooten et al. (2023) B. Grooten, G. Sokar, S. Dohare, E. Mocanu, M. E. Taylor, M. Pechenizkiy, and D. C. Mocanu. Automatic noise filtering with dynamic sparse training in deep reinforcement learning. In _Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems_, AAMAS \'23, page 1932-1941, Richland, SC, 2023. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450394321.\n' +
      '* Haarnoja et al. (2018) T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.\n' +
      '* Hafner et al. (2023) D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world models. _arXiv preprint arXiv:2301.04104_, 2023.\n' +
      '* Han et al. (2015) S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. _arXiv preprint arXiv:1510.00149_, 2015.\n' +
      '* Harris et al. (2020) C. R. Harris, K. J. Millman, S. J. Van Der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, et al. Array programming with numpy. _Nature_, 585(7825):357-362, 2020.\n' +
      '* Hessel et al. (2018) M. Hessel, J. Modayil, H. V. Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. G. Azar, and D. Silver. Rainbow: Combining improvements in deep reinforcement learning. In _AAAI_, 2018.\n' +
      '* Hiraoka et al. (2021) T. Hiraoka, T. Imagawa, T. Hashimoto, T. Onishi, and Y. Tsuruoka. Dropout q-functions for doubly efficient reinforcement learning. In _International Conference on Learning Representations_, 2021.\n' +
      '* Hunter (2007) J. D. Hunter. Matplotlib: A 2d graphics environment. _Computing in science & engineering_, 9(03):90-95, 2007.\n' +
      '* Ioffe and Szegedy (2015) S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International conference on machine learning_, pages 448-456. pmlr, 2015.\n' +
      '* Kaiser et al. (2020) L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn, P. Kozakowski, S. Levine, et al. Model-based reinforcement learning for atari. _International Conference on Learning Representations_, 2020.\n' +
      '* Kostrikov et al. (2020) I. Kostrikov, D. Yarats, and R. Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. _arXiv preprint arXiv:2004.13649_, 2020.\n' +
      '* Kumar et al. (2020) A. Kumar, A. Zhou, G. Tucker, and S. Levine. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1179-1191, 2020.\n' +
      '* Kumar et al. (2021a) A. Kumar, R. Agarwal, D. Ghosh, and S. Levine. Implicit under-parameterization inhibits data-efficient deep reinforcement learning. In _International Conference on Learning Representations_, 2021a. URL [https://openreview.net/forum?id=09bnihsFfXU](https://openreview.net/forum?id=09bnihsFfXU).\n' +
      '* Kumar et al. (2021b) A. Kumar, R. Agarwal, T. Ma, A. Courville, G. Tucker, and S. Levine. Dr3: Value-based deep reinforcement learning requires explicit regularization. _arXiv preprint arXiv:2112.04716_, 2021b.\n' +
      '* Kumar et al. (2022) A. Kumar, R. Agarwal, X. Geng, G. Tucker, and S. Levine. Offline q-learning on diverse multi-task data both scales and generalizes. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* Lee et al. (2023) H. Lee, H. Cho, H. Kim, D. Gwak, J. Kim, J. Choo, S.-Y. Yun, and C. Yun. Plastic: Improving input and label plasticity for sample efficient reinforcement learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.\n' +
      '\n' +
      '* Lee et al. (2024) J. H. Lee, W. Park, N. E. Mitchell, J. Pilault, J. S. O. Ceron, H.-B. Kim, N. Lee, E. Frantar, Y. Long, A. Yazdanbakhsh, et al. Javxpruner: A concise library for sparsity research. In _Conference on Parsimony and Learning_, pages 515-528. PMLR, 2024.\n' +
      '* Lewandowski et al. (2023) A. Lewandowski, H. Tanaka, D. Schuurmans, and M. C. Machado. Curvature explains loss of plasticity. _arXiv preprint arXiv:2312.00246_, 2023.\n' +
      '* Liu et al. (2020) Z. Liu, X. Li, B. Kang, and T. Darrell. Regularization matters in policy optimization-an empirical study on continuous control. In _International Conference on Learning Representations_, 2020.\n' +
      '* Livne and Cohen (2020) D. Livne and K. Cohen. Pops: Policy pruning and shrinking for deep reinforcement learning. _IEEE Journal of Selected Topics in Signal Processing_, 14(4):789-801, May 2020. ISSN 1941-0484. doi: 10.1109/jstsp.2020.2967566. URL [http://dx.doi.org/10.1109/JSTSP.2020.2967566](http://dx.doi.org/10.1109/JSTSP.2020.2967566).\n' +
      '* Lyle et al. (2022) C. Lyle, M. Rowland, and W. Dabney. Understanding and preventing capacity loss in reinforcement learning. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=ZkC8wKoLbQ7](https://openreview.net/forum?id=ZkC8wKoLbQ7).\n' +
      '* Lyle et al. (2023) C. Lyle, Z. Zheng, E. Nikishin, B. A. Pires, R. Pascanu, and W. Dabney. Understanding plasticity in neural networks. In _Proceedings of the 40th International Conference on Machine Learning_, ICML\'23. JMLR.org, 2023.\n' +
      '* Mnih et al. (2015) V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. _Nature_, 518(7540):529-533, Feb. 2015.\n' +
      '* Mocanu et al. (2018) D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu, and A. Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. _Nature communications_, 9(1):2383, 2018.\n' +
      '* Nikishin et al. (2022) E. Nikishin, M. Schwarzer, P. D\'Oro, P.-L. Bacon, and A. Courville. The primacy bias in deep reinforcement learning. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 16828-16847. PMLR, 17-23 Jul 2022.\n' +
      '* Nikishin et al. (2023) E. Nikishin, J. Oh, G. Ostrovski, C. Lyle, R. Pascanu, W. Dabney, and A. Barreto. Deep reinforcement learning with plasticity injection. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL [https://openreview.net/forum?id=jucDLW6G91](https://openreview.net/forum?id=jucDLW6G91).\n' +
      '* Oliphant (2007) T. E. Oliphant. Python for scientific computing. _Computing in Science & Engineering_, 9(3):10-20, 2007. doi: 10.1109/MCSE.2007.58.\n' +
      '* Ostrovski et al. (2021) G. Ostrovski, P. S. Castro, and W. Dabney. The difficulty of passive learning in deep reinforcement learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL [https://openreview.net/forum?id=nPHA8fGicZk](https://openreview.net/forum?id=nPHA8fGicZk).\n' +
      '* Ota et al. (2021) K. Ota, D. K. Jha, and A. Kanezaki. Training larger networks for deep reinforcement learning. _arXiv preprint arXiv:2102.07920_, 2021.\n' +
      '* Schaul et al. (2016) T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. _CoRR_, abs/1511.05952, 2016.\n' +
      '* Schmidt and Schmied (2021) D. Schmidt and T. Schmied. Fast and data-efficient training of rainbow: an experimental study on atari. _arXiv preprint arXiv:2111.10247_, 2021.\n' +
      '* Schmitt et al. (2018) S. Schmitt, J. J. Hudson, A. Zidek, S. Osindero, C. Doersch, W. M. Czarnecki, J. Z. Leibo, H. Kuttler, A. Zisserman, K. Simonyan, et al. Kickstarting deep reinforcement learning. _arXiv preprint arXiv:1803.03835_, 2018.\n' +
      '* Schwarzer et al. (2018) M. Schwarzer, J. S. O. Ceron, A. Courville, M. G. Bellemare, R. Agarwal, and P. S. Castro. Bigger, better, faster: Human-level atari with human-level efficiency. In _International Conference on Machine Learning_, pages 30365-30380. PMLR, 2023.\n' +
      '* Sinha et al. (2020) S. Sinha, H. Bharadhwaj, A. Srinivas, and A. Garg. D2rl: Deep dense architectures in reinforcement learning. _arXiv preprint arXiv:2010.09163_, 2020.\n' +
      '* Sokar et al. (2021) G. Sokar, E. Mocanu, D. C. Mocanu, M. Pechenizkiy, and P. Stone. Dynamic sparse training for deep reinforcement learning. _arXiv preprint arXiv:2106.04217_, 2021.\n' +
      '* Sokar et al. (2021) G. Sokar, R. Agarwal, P. S. Castro, and U. Evci. The dormant neuron phenomenon in deep reinforcement learning. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 32145-32168. PMLR, 23-29 Jul 2023. URL [https://proceedings.mlr.press/v202/sokar23a.html](https://proceedings.mlr.press/v202/sokar23a.html).\n' +
      '* Song et al. (2019) X. Song, Y. Jiang, S. Tu, Y. Du, and B. Neyshabur. Observational overfitting in reinforcement learning. _arXiv preprint arXiv:1912.02975_, 2019.\n' +
      '* Sutton (1988) R. S. Sutton. Learning to predict by the methods of temporal differences. _Machine Learning_, 3(1):9-44, Aug. 1988.\n' +
      '* Taiga et al. (2023) A. A. Taiga, R. Agarwal, J. Farebrother, A. Courville, and M. G. Bellemare. Investigating multi-task pretraining and generalization in reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Tan et al. (2023) Y. Tan, P. Hu, L. Pan, J. Huang, and L. Huang. RLx2: Training a sparse deep reinforcement learning model from scratch. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=DJEEq0Aq7to](https://openreview.net/forum?id=DJEEq0Aq7to).\n' +
      '* Todorov et al. (2012) E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ international conference on intelligent robots and systems_, pages 5026-5033. IEEE, 2012.\n' +
      '* van Hasselt et al. (2016) H. van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. In _Proceedings of the Thirtieth AAAI Conference On Artificial Intelligence (AAAI), 2016_, 2016. cite arxiv:1509.06461Comment: AAAI 2016.\n' +
      '* Van Hasselt et al. (2018) H. Van Hasselt, Y. Doron, F. Strub, M. Hessel, N. Sonnerat, and J. Modayil. Deep reinforcement learning and the deadly triad. _arXiv preprint arXiv:1812.02648_, 2018.\n' +
      '* Van Hasselt et al. (2019) H. P. Van Hasselt, M. Hessel, and J. Aslanides. When to use parametric models in reinforcement learning? _Advances in Neural Information Processing Systems_, 32, 2019.\n' +
      '* Van Rossum and Drake Jr. (1995) G. Van Rossum and F. L. Drake Jr. _Python reference manual_. Centrum voor Wiskunde en Informatica Amsterdam, 1995.\n' +
      '* Vinyals et al. (2019) O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, 575(7782):350-354, 2019.\n' +
      '* Vischer et al. (2021) M. Vischer, R. T. Lange, and H. Sprekeler. On lottery tickets and minimal task representations in deep reinforcement learning. In _International Conference on Learning Representations_, 2021.\n' +
      '* Wang et al. (2016) Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas. Dueling network architectures for deep reinforcement learning. In _Proceedings of the 33rd International Conference on Machine Learning_, volume 48, pages 1995-2003, 2016.\n' +
      '* Yarats et al. (2021) D. Yarats, R. Fergus, and I. Kostrikov. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In _9th International Conference on Learning Representations, ICLR 2021_, 2021.\n' +
      '* Yu et al. (2019) H. Yu, S. Edunov, Y. Tian, and A. S. Morcos. Playing the lottery with rewards and multiple languages: lottery tickets in rl and nlp. In _International Conference on Learning Representations_, 2019.\n' +
      '* Zhang et al. (2019) C. Zhang, O. Vinyals, R. Munos, and S. Bengio. A study on overfitting in deep reinforcement learning. arXiv preprint arXiv:1804.06893_, 2018.\n' +
      '* Zhang et al. (2019) H. Zhang, Z. He, and J. Li. Accelerating the deep reinforcement learning with neural network compression. In _2019 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2019.\n' +
      '* Zhu and Gupta (2017) M. Zhu and S. Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression, 2017.\n' +
      '\n' +
      '## Appendix A Code availability\n' +
      '\n' +
      'Our experiments were built on open source code, mostly from the Dopamine repository. The root directory for these is [https://github.com/google/dopamine/tree/master/dopamine/](https://github.com/google/dopamine/tree/master/dopamine/), and we specify the subdirectories below (with clickable links):\n' +
      '\n' +
      '* DQN and Rainbow agents from /jax/agents/\n' +
      '* Atari-100k agents from /labs/atari-100k/\n' +
      '* Sparsity scripts from JaxPruner /jaxpruner/baselines/dopamine/\n' +
      '* Resnet architecture from /labs/offline-rl/jax/networks.py (**line 108**)\n' +
      '* Dormant neurons metric, Reset, ReDo and Weight Decay from /labs/redo/\n' +
      '\n' +
      'For the srank metric experiments we used code from:\n' +
      '\n' +
      '[https://github.com/google-research/google-research/blob/master/](https://github.com/google-research/google-research/blob/master/)\n' +
      '\n' +
      'generalization_representations_rl_aistats22/coherence/coherence_compute.py\n' +
      '\n' +
      '## Appendix B Atari Game Selection\n' +
      '\n' +
      'Most of our experiments were run with 15 games from the ALE suite (Bellemare et al., 2013), as suggested by Graesser et al. (2022). However, for the Atari 100k agents (subsection 4.3), we used the standard set of 26 games (Kaiser et al., 2020) to be consistent with the benchmark. We also ran some experiments with the full set of 60 games. The specific games are detailed below.\n' +
      '\n' +
      '**15 game subset:** MsPacman, Pong, Qbert, (Assault, Asterix, BeamRider, Boxing, Breakout, Crazy-Climber, DemonAttack, Enduro, FishingDerby, SpaceInvaders, Tutankham, VideoPinball. According to Graesser et al. (2022), these games were selected to be roughly evenly distributed amongst the games ranked by DQN\'s human normalized score in (Mnih et al., 2015) with a lower cut off of approximately 100% of human performance.\n' +
      '\n' +
      '**26 game subset:** Alien, Amidar, Assault, Asterix, BankHeist, BattleZone, Boxing, Breakout, ChopperCommand, CrazyClimber, DemonAttack, Freeway, Frostbite, Gopher, Hero, Jamesbond, Kangaroo, Krull, KungFuMaster, MsPacman, Pong, PrivateEye, Qbert, RoadRunner, Seaquest, UpNDown.\n' +
      '\n' +
      '**60 game set:** The 26 games above in addition to: AirRaid, Asteroids, Atlantis, BeamRider, Berzerk, Bowling, Carnival, Centipede, DoubleDunk, ElevatorAction, Enduro, FishingDerby, Gravitar, IceHockey, JourneyEscape, MontezumaReverge, NameThisGame, Phoenix, Pitfall, Pooyan, Riverraid, Robotank, Skiing, Solaris, SpaceInvaders, StarGunner, Tennis, TimePilot, Tutankham, Venture, VideoPinball, WizardOfWor, YarsRevenge, Zaxxon.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:18]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>