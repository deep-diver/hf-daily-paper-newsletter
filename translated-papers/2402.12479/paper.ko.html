<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 깊은 강화 학습에서 가지치기된 네트워크는 좋은 네트워크이다\n' +
      '\n' +
      'Johan Obando-Ceron1,2,3,Aaron Courville2,3,Pablo Samuel Castro1,2,3\n' +
      '\n' +
      '1구글 딥마인드, 2밀라 - 퀘벡 AI 연구소, 몬트리올 3대학\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근 연구에 따르면 심층 강화 학습 에이전트는 네트워크 매개 변수를 효과적으로 사용하는 데 어려움을 겪고 있다. 우리는 희소 훈련 기술의 장점에 대한 사전 통찰력을 활용하고 점진적인 크기 가지치기가 에이전트가 매개변수 효과를 최대화할 수 있음을 보여준다. 이는 전통적인 네트워크들에 비해 극적인 성능 개선들을 산출하는 네트워크들을 초래하고, 전체 네트워크 파라미터들의 작은 부분만을 사용하여, 일종의 "스케일링 법칙"을 나타낸다.\n' +
      '\n' +
      '2024-21\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '실제 문제에 적용되는 심층 강화 학습(RL)의 성공적인 예들에도 불구하고(벨레메어 외, 2020; Berner 외, 2019; Fawzi 외, 2022; Mnih 외, 2015; Vinyals 외, 2019), 이러한 네트워크를 훈련할 때 발생하는 도전 및 병리의 증거가 증가하고 있다(Ceron 외, 2023; Graesser 외, 2022; Kumar 외, 2021; Lyle 외, 2022; Nikishin 외, 2022; Ostrovski 외, 2021; Sokar 외, 2023). 특히, 심층 RL 에이전트 _under-utilize_ their 네트워크의 파라미터: Kumar et al. (2021)은 암시적 언더파라미터화가 있음을 증명하였고, Sokar et al. (2023)은 훈련 중에 많은 수의 뉴런이 휴면 상태임을 증명하였고, Graesser et al. (2022)는 희소 훈련 방법이 원래의 네트워크 파라미터의 매우 작은 부분으로 성능을 유지할 수 있음을 보여주었다.\n' +
      '\n' +
      '이 마지막 작업의 가장 놀라운 발견 중 하나는 ResNet 백본(임팔라(Espeholt et al., 2018)에서 소개된 바와 같이)으로 DQN(Mnih et al., 2015)에 Zhu 및 Gupta(2017)에 의해 제안된 점진적 크기 프루닝 기술을 적용하면 조밀한 대응물에 대해 50%의 성능 향상을 가져오며, 원래 매개변수의 10%만을 갖는다(Graesser et al. (2022)의 그림 1의 오른쪽 하단 패널 참조). 흥미롭게도, 동일한 가지치기 기술이 원래의 CNN 아키텍처에 적용될 때 성능 개선은 없지만 성능 저하도 없다.\n' +
      '\n' +
      '동일한 가지치기 기술이 질적으로 다를 수 있지만 부정적이지 않은 결과를 가질 수 있다는 것은 기본 아키텍처를 단순히 변경함으로써 흥미롭다. 이는 비표준 네트워크 토폴로지로 심층 RL 에이전트를 훈련하는 것(점진적 크기 가지치기와 같은 기술에 의해 유도됨)이 일반적으로 유용할 수 있으며 보다 심오한 조사가 필요함을 시사한다.\n' +
      '\n' +
      '본 논문에서는 RL 에이전트의 성능을 향상시키기 위한 일반적인 기술로서 점진적인 크기 가지치기를 탐구한다. 우리는 표준 네트워크 아키텍처의 성능을 향상시키는 것 외에도 기본 네트워크 아키텍처의 크기에 비례하여 이득이 증가한다는 것을 보여준다. 이 마지막 포인트는 딥 RL 네트워크들이 스케일링 아키텍처들과 고군분투하는 것으로 알려져 있기 때문에 중요하다(Farebrother et al., 2023; Ota et al., 2021; Schwarzer et al., 2023; Taiga et al., 2023).\n' +
      '\n' +
      '도 1: 임팔라 기반 ResNet을 갖는 DQN 및 레인보우에 대한, ResNet 아키텍처**에 대한 **스케일링 네트워크 폭. 우리는 각각 5개의 시드로 15개 게임에 걸쳐 집계된 4천만 개의 환경 단계 후 정량 평균을 보고하며, 오류 막대는 95% 계층화된 부트스트랩 신뢰 구간을 나타낸다.\n' +
      '\n' +
      '우리의 주요 기여는 다음과 같습니다. 우리\n' +
      '\n' +
      '* RL에서 파라미터 효율을 최대화하기 위한 일반적인 기술로서 점진적인 크기 프루닝을 제시하고;\n' +
      '* 이 기술로 훈련된 네트워크들이 그들의 밀집된 대응물들보다 더 강한 에이전트들을 생성하고, 초기 네트워크의 크기를 스케일링함에 따라 일종의 "스케일링 법칙"을 나타낸다는 것을 입증한다;\n' +
      '*는 이 기술이 다양한 에이전트들 및 훈련 체제들의 세트에 걸쳐 일반적으로 유용함을 보여주는 도면;\n' +
      '* 혜택의 이면에 있는 이유를 더 잘 이해하기 위해 심층 분석을 제시한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '딥 RL 심층 신경망에서의 스케일링은 실제 작업에 대한 강화 학습의 성공적인 많은 응용의 원동력이었다. 그러나, 성능 저하 없이 지도 학습에서 "스케일링 법칙"으로 이어진 것과 유사한 방식으로 이러한 네트워크를 스케일링하는 것은 역사적으로 어려웠으며, 이는 상당 부분 강화 학습 고유의 훈련 불안정성으로 인한 것이다(Ota et al., 2021; Sinha et al., 2020; Van Hasselt et al., 2018). 그렇게 성공적으로 할 수 있었던 최근의 작업들은 다수의 타겟팅된 기술들 및 신중한 하이퍼-파라미터 선택에 의존해야 했다(Ceron et al., 2023; Farebrother et al., 2023; Ota et al., 2021; Schwarzer et al., 2023; Taiga et al., 2023).\n' +
      '\n' +
      'Cobbe et al. (2020); Farebrother et al. (2023) 및 Schwarzer et al. (2023)은 Espeholt et al. (2018)에 의해 제안된 바와 같이, Mnih et al. (2015)의 원래의 CNN 아키텍처로부터 ResNet 기반 아키텍처로 전환되었으며, 이는 스케일링에 더 적합하다는 것이 입증되었다. Cobbe et al. (2020) 및 Farebrother et al. (2023)은 ResNet 아키텍처의 각 계층에서 특징들의 수를 증가시킬 때 이점들을 관찰한다. Schwarzer et al.(2023)은 그들의 에이전트(BBF)의 성능이 그들의 네트워크의 폭에 비례하여 계속 증가한다는 것을 보여준다. Bjorck et al. (2021)은 트레이닝 불안정성을 완화하고 그들의 아키텍처의 스케일링을 가능하게 하기 위해 스펙트럼 정규화를 제안한다. Ceron et al. (2023)은 네트워크를 스케일링하는 경우에도 개선된 성능을 위해 배치 크기를 감소시키는 것을 제안한다.\n' +
      '\n' +
      'Deep RL의 희소 모델 이전 연구(Schmitt et al., 2018; Zhang et al., 2019)는 불안정성을 완화하기 위해 정적 데이터와 함께 지식 증류를 사용하여 작지만 조밀한 에이전트를 생성했다. Livne and Cohen(2020)은 sparse DRL 에이전트를 얻기 위해 반복적 크기 프루닝(Han et al., 2015)과 유사한 반복적 정책 프루닝을 활용하여 정책 프루닝 및 수축을 도입하였다. DRL에서 복권 가설의 탐색은 처음에 Yu 등(2019)에 의해 수행되었고, 나중에 Vischer 등(2021)은 행동 복제를 통해 희박한 당첨 티켓도 식별될 수 있음을 입증했다. Sokar et al.(2021) proposed use of structural evolution of network topology in DRL, achieving 50%\n' +
      '\n' +
      '도 3: ResNet 아키텍처 및 3의 폭 승수로 DQN에 대한 성능**에 대해 다양한 희소성이 얼마나 영향을 미치는지 평가하는 **훈련 세부사항은 섹션 4.1 참조.\n' +
      '\n' +
      '그림 2: 실험에 사용된 ** 점진적 크기 가지치기 일정**은 방정식 1에 명시된 대로 95%의 목표 희소성으로 표시된다.\n' +
      '\n' +
      ' 희소성, 성능 저하 없음. Arnob et al.(2021)은 오프라인 강화학습을 위한 단일 샷 프루닝을 도입하였다. Graesser et al.(2022)은 프루닝이 종종 개선된 결과들을 산출한다는 것을 발견하였고, _dynamic_sparse 트레이닝 방법들은 트레이닝 전체에 걸쳐 희소 토폴로지가 변화하는 경우(Evci et al., 2020; Mocanu et al., 2018)는 트레이닝 전체에 걸쳐 희소 토폴로지가 고정된 채로 유지되는 _static_sparse 트레이닝을 상당히 능가할 수 있다. Tan et al.(2023)은 새로운 지연 다중 단계 시간차 타겟 메커니즘 및 동적-용량 재생 버퍼의 도입을 통해 동적 희소 트레이닝의 효능을 향상시킨다. Grooten et al. (2023)은 태스크 관련 특징들에 초점을 맞추기 위해 네트워크 토폴로지를 조정하기 위한 동적 희소 트레이닝의 원리들을 사용하는 자동 잡음 필터링 방법을 제안하였다.\n' +
      '\n' +
      'Deep RLSong et al. (2019) 및 Zhang et al. (2018)에서 오버파라미터화는 RL 네트워크의 오버핏 경향을 강조하고, Nikishin et al. (2022) 및 Sokar et al. (2023)은 RL 네트워크에서 가소성 손실의 유병률을 입증하여 최종 성능의 저하를 초래했다.\n' +
      '\n' +
      '이를 완화하기 위해 데이터 증강(Cetin et al., 2022; Yarats et al., 2021), 드롭아웃(Gal and Ghahramani, 2016), 레이어 및 배치 정규화(Ba et al., 2016; Ioffe and Szegedy, 2015) 등 여러 전략이 제안되었다. Hiraoka et al. (2021)은 Soft Actor-Critic (Haarnoja et al., 2018)에서 드롭아웃 및 레이어 정규화를 채택하는 성공을 입증한 반면, Liu et al. (2020)은 배우에 \\(\\ell_{2}\\) 가중치 정규화를 적용하는 것이 온 정책 및 오프 정책 알고리즘을 모두 향상시킬 수 있음을 확인했다.\n' +
      '\n' +
      'Nikishin et al.(2022)은 네트워크들이 초기 데이터에 오버핏하는 경향(Primacy bias)을 파악하여 후속 학습을 방해할 수 있으며, 이를 완화하기 위한 수단으로 주기적인 네트워크 재초기화를 제안한다. 마찬가지로, Sokar et al. (2023)은 네트워크 가소성을 향상시키기 위해 재초기화 _dormant neurons_를 제안한 반면, Nikishin et al. (2023)은 현재 네트워크를 일시적으로 동결하고 연속 학습을 용이하게 하기 위해 새롭게 초기화된 가중치를 활용하여 가소성 주입을 제안한다.\n' +
      '\n' +
      '## 3 Background\n' +
      '\n' +
      '심층 강화 학습의 목표는 긴 지평에 걸쳐 누적 할인 수익률을 최적화하는 것이며, 일반적으로 마르코프 결정 과정(MDP)\\((\\mathcal{X},\\mathcal{A},P,r,\\boldsymbol{\\nu})으로 공식화된다. MDP는 상태공간\\(\\mathcal{X}\\), 행동공간\\(\\mathcal{A}\\), 전이역학 모델\\(P:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\Delta(\\mathcal{X})\\)(여기서 \\(\\Delta(X)\\)는 집합 \\(X\\)), 보상함수 \\(\\mathcal{R}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R}\\), 그리고 할인계수 \\(\\boldsymbol{\\nu}\\in[0,1)\\)으로 구성된다. 정책\\(\\pi:\\mathcal{X}\\rightarrow\\Delta(\\mathcal{A})\\)는 에이전트의 행동을 공식화한다. 정책(\\pi\\)에 대해, \\(Q^{\\pi}(\\mathbf{x},\\mathbf{a})\\)은 상태 \\(\\mathbf{x}\\)에서 액션 \\(\\mathbf{a}\\)을 취한 후 정책 \\(\\pi\\): \\(Q^{\\pi}(\\mathbf{x},\\mathbf{a}):=\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}y^{t}\\mathcal{R}\\left(\\mathbf{x}_{t},\\mathbf{a}_{t}\\right)|\\mathbf{x}_{a}_{0}=x,\\mathbf{a}_{0}=a\\right]\\\\(Q^{\\pi}(\\mathbf{x},\\mathbf{a}}}=a\\right]\\(Q^{\\pi}(\\mathbf{x},\\mathbf{ (Q^{\\star}(\\mathbf{x},\\mathbf{a})\\)로 표기된 최적의 Q-함수는 벨만 반복을 만족한다:\n' +
      '\n' +
      '\\mathbbb{E}_{\\mathbf{x}^{\\prime}\\sim P(\\mathbf{x}^{\\prime}|\\mathbf{a}}\\left[\\mathcal{R}(\\mathbf{x}, \\mathbf{a})+\\gamma\\max\\mathbf{a}^{\\prime}Q^{\\star}\\left(\\mathbf{x}^{\\prime},\\mathbf{a}^{\\prime}\\right]]\n' +
      '\n' +
      '대부분의 현대적 가치 기반 방법은 매개변수\\(\\theta\\)를 갖는 신경망을 통해 \\(Q\\)을 근사할 것이며, 이를 \\(Q_{\\theta}\\)이라고 한다. 이 아이디어는 Mnih et al.(2015)에 의해 그들의 DQN 에이전트와 함께 소개되었으며, 이는 대부분의 현대 딥 RL 알고리즘의 기초가 되었다. 네트워크\\(Q_{\\theta}\\)는 전형적으로 다음과 같은 _시간차 손실로 트레이닝된다:\n' +
      '\n' +
      '\\\\mathbb{E}_{(\\mathbf{x},\\mathbf{a},\\mathbf{r},\\mathbf{x}^{\\prime},\\mathbf{x}^{\\prime}) \\sim\\mathcal{D}}\\left[\\left(\\mathbf{r}+\\gamma\\max\\mathbf{a}^{\\prime}\\mathcal{A}}\\bar{Q}\\left(\\mathbf{x}^{\\prime},\\mathbf{a}^{\\prime}\\right)-Q_{\\theta}(\\mathbf{x},\\mathbf{a}^{\\prime}\\right)^{2}\\right],\\\\mathbf{a}\\sim\\mathcal{D}}\\left[\\mathbf{r}+\\gamma\\max\\mathbf{a}^{\\prime}\\mathcal{A}}\\bar{Q}\\left(\\mathbf{x}^{\\prime\n' +
      '\n' +
      '도 4: Mnih 등(2015), DQN(좌측) 및 레인보우(우측)의 원래의 CNN 아키텍처에 대한 **스케일링 네트워크 폭. 교육 세부 정보는 섹션 4.1 참조**\n' +
      '\n' +
      '여기서 \\(\\mathcal{D}\\)는 학습용 에이전트 샘플(리플레이 버퍼로 알려져 있음)의 저장된 전이 모음 \\((\\mathbf{x}_{t},\\mathbf{a}_{t},\\mathbf{r}_{t},\\mathbf{x}_{t+1})을 나타낸다. 정적 네트워크(\\bar{Q}\\)는 (Q_{\\theta}\\)에서 매개 변수를 거의 복사하지 않는 정적 네트워크이며, 그 목적은 안정적인 학습 목표를 생성하는 것이다.\n' +
      '\n' +
      '레인보우(Hessel et al., 2018)는 더블 Q-러닝(van Hasselt et al., 2016), 우선순위화된 경험 재생(Schaul et al., 2016), 듀얼링 네트워크(Wang et al., 2016), 다단계 리턴(Sutton, 1988), 분배 강화 학습(Bellemare et al., 2017), 및 잡음 네트워크(Fortunato et al., 2018)를 갖는 원래의 DQN 알고리즘을 확장 및 개선하였다.\n' +
      '\n' +
      '지도 학습 설정에서 점진적인 가지치기는 희소 트레이닝 기술에 대한 광범위한 관심이 있으며, 이에 의해 전체 네트워크 파라미터의 서브세트만이 트레이닝/사용된다(Gale et al., 2019). 이는 추론 속도뿐만 아니라 계산 및 공간 효율성에 의해 동기 부여된다. Zhu and Gupta(2017)는 낮은 가중치 크기를 갖는 _pruning_ 모델 파라미터에 의해 훈련 과정에서 조밀한 네트워크를 점진적으로 희소화시키기 위한 다항식 스케줄을 제안하였다.\n' +
      '\n' +
      '구체적으로, \\(s_{F}\\in[0,1]\\)은 최종 원하는 희소성 수준(예: 대부분의 실험에서 0.95)을 나타내고, \\(t_{text{start}\\) 및 \\(t_{text{end}\\)은 프루닝의 시작 및 종료 반복을 각각 나타내며, 반복 시 희소성 수준은 다음과 같다.\n' +
      '\n' +
      '√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√\n' +
      '\n' +
      'Graesser et al.(2022)은 이 아이디어를 Deep RL 네트워크에 적용하여 훈련의 20%에서 \\(t_{\\text{start}}\\), 훈련의 80%에서 \\(t_{\\text{end}}\\)을 설정하였다.\n' +
      '\n' +
      '##4 프루닝 부스트 심층 RL 성능\n' +
      '\n' +
      '우리는 온라인 및 오프라인 설정 모두에서 심층 RL 에이전트에서 점진적인 크기 가지치기의 일반적인 유용성을 조사한다.\n' +
      '\n' +
      '### Implementation details\n' +
      '\n' +
      '베이스 DQN 및 레인보우 에이전트들에 대해 우리는 Dopamine 라이브러리(Castro et al., 2018)의 Jax 구현들을 그들의 디폴트 값들과 함께 사용한다. 도파민은 다단계 업데이트, 우선 순위 리플레이 및 배포 RL만 사용하여 원래 레인보우 에이전트의 "컴팩트한" 버전을 제공한다는 점에 주목할 가치가 있다. 모든 실험에 대해서는 달리 명시되지 않는 한 15-layer ResNet인 Espeholt et al.(2018)에 의해 소개된 Impala 아키텍처를 사용한다. 이에 대한 우리의 추론은 Graesser et al.(2022)의 결과뿐만 아니라, 이 아키텍처가 일반적으로 향상된 성능을 초래한다는 것을 입증하는 다수의 최근 작업들 때문이다(Kumar et al., 2022; Schmidt and Schmied, 2021; Schwarzer et al., 2023).\n' +
      '\n' +
      '우리는 JaxPruner(Lee et al., 2024) 라이브러리를 사용하여 점진적인 크기 가지치기를 하는데, 이는 이미 Dopamine과의 통합을 제공하기 때문이다. 우리는 Graesser et al.(2022)의 스케줄에 따른다 : 네트워크 20%를 트레이닝으로 가지치기 시작하고 80%에서 정지하며, 나머지 트레이닝 동안 최종 희소 네트워크를 고정시킨다. 그림 2는 실험에 사용된 가지치기 일정(95% 희소성)을 보여준다.\n' +
      '\n' +
      '도 5: 폭 3**의 ResNet 아키텍처를 가진 Rainbow에 대한 **스케일링 리플레이 비율. 기본 재생 비율은 0.25입니다. 자세한 내용은 섹션 4.1을 참조하십시오.\n' +
      '\n' +
      '아케이드 학습 환경(ALE)(Bellemare et al., 2013)의 에이전트들을 Graesser et al. (2022)에 의해 사용된 15개의 게임에 대해 평가하였고, 그 다양성 1을 위해 선택되었으며, 계산적 고려를 위해 대부분의 실험은 4천만 프레임 이상(표준 2억 프레임과는 대조적으로) 수행되었으며, 우리의 조사에서 4천만 프레임에서의 알고리즘들 간의 질적 차이가 1억 프레임에서의 알고리즘들과 대부분 일치함을 발견하였다(예를 들어, 그림 10 참조).\n' +
      '\n' +
      '각주 1: Graesser et al. (2022)의 A.4에서 논의되었다.\n' +
      '\n' +
      '우리는 평가를 위해 Agarwal et al.(2021)에 의해 요약된 지침을 따른다: 각 실험은 5개의 독립적인 종자로 실행되었고, 95% 계층화된 부트스트랩 신뢰 구간과 함께 15개의 게임, 구성 및 종자에 걸쳐 집계된 인간 정규화 양자 간 평균(IQM)을 보고한다. 모든 실험은 엔비디아 테슬라 P100 GPU에서 실행되었으며 각각 완료하는 데 약 2일이 걸렸다.\n' +
      '\n' +
      '### Online RL\n' +
      '\n' +
      'Graesser et al.(2022)은 희소 네트워크가 에이전트 성능을 유지할 수 있음을 입증하지만, 이러한 희소성의 수준이 너무 높으면 결국 성능이 저하된다. 이것은 더 높은 수준의 희소성과 마찬가지로 네트워크에 더 적은 활성 매개변수가 남아 있기 때문에 직관적이다. 한 가지 자연스러운 질문은 초기 네트워크가 높은 수준의 희소성을 가능하게 하는지 여부이다. 따라서 우리는 임팔라 아키텍처를 사용하여 DQN에 점진적인 크기 가지치기를 적용하여 조사를 시작한다. 여기서 컨볼루션 레이어를 3의 배수로 스케일링했다. 그림 3은 이것이 실제로 사실임을 확인한다: 90%와 95% 희소성은 33%의 성능 향상을 생성하고 99% 희소성은 성능을 유지한다.\n' +
      '\n' +
      '95%의 희소성은 초기 탐색에서 일관되게 최고의 성능을 산출했기 때문에 주로 조사를 위해 이 희소성 수준에 중점을 둔다. 그림 1은 놀라운 결과이며, 원본 아키텍처보다 60%(DQN) 및 50%(레인보우) 성능 향상을 거의 관찰한다. 또한, 프루닝되지 않은 아키텍처의 성능은 폭이 증가함에 따라 단조롭게 감소하는 반면, 프루닝된 대응물의 성능은 단조롭게 증가한다. 그림 7에서 우리는 60개의 아타리 2600개 게임 모두에 걸쳐 DQN과 레인보우에서 가지치기를 평가했으며, 우리의 발견이 처음에 선택된 15개 게임에 국한되지 않음을 확인했다.\n' +
      '\n' +
      'Mnih et al.(2015)의 원래의 CNN 아키텍처를 사용하는 것으로 두 에이전트를 스위칭할 때, 우리는 레인보우와 유사한 경향을 보지만 DQN에서는 거의 개선되지 않는다(도 4). 이 결과는 CNN 아키텍처에서 프루닝으로 실제 개선이 관찰되지 않았던 Graesser et al.(2022)의 결과와 일치한다. 흥미로운 관찰은 이 CNN 아키텍처를 사용하면 DQN의 성능이 너비 증가의 이점을 얻는 반면 레인보우의 성능은 약간의 저하를 겪는 것으로 보인다는 것이다.\n' +
      '\n' +
      '지금까지 우리의 연구 결과는 점진적인 크기 가지치기를 사용하면 이러한 에이전트의 매개변수 효율성이 증가한다는 것을 시사한다. 그렇다면, 이러한 희소 네트워크는 또한 더 많은 그래디언트 업데이트로부터 이익을 얻을 수 있어야 한다. 환경단계별 그래디언트 업데이트 횟수인 _replay ratio_는 이를 정확히 측정하는데, 성능 저하 없이 이 값을 증가시키는 것이 어렵다는 것은 잘 알려져 있다(D\'Oro et al.,\n' +
      '\n' +
      '도 6: ResNet 아키텍처를 사용하여 오프라인 에이전트** CQL **(왼쪽)** 및 CQL+C51 **(오른쪽)**에 대한 **스케일링 네트워크 폭. 우리는 17개의 아타리 게임에 걸쳐 95% 신뢰 구간을 나타내는 오차 막대가 있는 양자 간 평균 성능을 보고한다. x축은 기울기 단계를 나타내며, 새 데이터가 수집되지 않습니다.\n' +
      '\n' +
      '2023; Fedus et al., 2020; Nikishin et al., 2022; Schwarzer et al., 2023). 그림 5에서 프루닝된 아키텍처가 높은 재생 비율 값에서도 프루닝되지 않은 기준선에 대한 성능 리드를 유지한다는 것을 실제로 확인할 수 있다.\n' +
      '\n' +
      '#### 낮은 데이터 체제\n' +
      '\n' +
      'Kaiser et al.(2020)은 샘플 제한 환경에서 RL 에이전트를 평가하기 위해 Atari 100k 벤치마크를 도입하여 에이전트만 100k 환경 상호 작용을 허용한다. 이 체제를 위해 Kostrikov et al. (2020)은 데이터 증강을 사용하는 DQN의 변형인 DrQ를 도입했으며, 이 에이전트에 대한 하이퍼파라미터는 DrQ(\\(\\epsilon\\))에서 Agarwal et al. (2021)에 의해 추가로 최적화되었다. 마찬가지로, Van Hasselt et al.(2019)은 Data-Efficient Rainbow(DER)를 도입하였는데, 이는 이러한 낮은 데이터 레짐에 대해 Rainbow(Hessel et al., 2018)의 하이퍼파라미터를 최적화하였다.\n' +
      '\n' +
      '이 낮은 데이터 체제에 대해 평가했을 때, 가지치기된 에이전트는 아무런 이득도 보여주지 않았다. 그러나, 40M 환경 상호작용(Ceron et al. (2023)에 의해 제안된 바와 같이)을 실행했을 때, 우리는 그림 8과 같이 점진적인 크기 가지치기를 사용할 때 상당한 이득을 관찰한다. 흥미롭게도 DrQ(\\(\\epsilon\\))에서 가지치된 에이전트는 더 오래 훈련될 때 기준선에 영향을 미치는 성능 저하를 방지한다.\n' +
      '\n' +
      '### Offline RL\n' +
      '\n' +
      '오프라인 강화 학습은 환경 상호 작용 없이 고정된 샘플 데이터 세트에서만 에이전트를 훈련시키는 데 중점을 둔다. 문헌의 최신 기술 방법 중 CQL(Kumar et al., 2020) 및 CQL+C51(Kumar et al., 2022) 두 가지를 사용했으며, 둘 다 Espeholt et al.(2018)의 ResNet 아키텍처를 사용하였다. 쿠마르 외(2021)에 이어, 우리는 2억 프레임 반복 동안 17개의 아타리 게임에서 이러한 에이전트를 훈련시켰으며, 여기서 1회 반복은 62,500 그래디언트 업데이트에 해당한다. 200M 환경 단계에 대해 훈련된 DQN 에이전트가 수집한 모든 환경 상호작용의 무작위 5% 샘플로 구성된 데이터 세트를 고려하여 에이전트를 평가했다(Agarwal et al., 2020).\n' +
      '\n' +
      '우리는 이전 실험과 다른 수의 단계에 대해 훈련하고 있기 때문에 그에 따라 가지치기 일정을 조정한다. 그림 6에서 볼 수 있듯이 CQL과 CQL+C51은 모두 프루닝된 네트워크, 특히 더 넓은 네트워크를 사용할 때 상당한 이득을 관찰한다. 흥미롭게도 오프라인 체제에서 가지치기는 또한 얕은 네트워크(1과 동일한 폭 척도)를 사용할 때 성능 붕괴를 방지하거나 CQL+C51의 경우와 같이 최종 성능을 개선하는 데 도움이 된다.\n' +
      '\n' +
      '### Actor-Critic methods\n' +
      '\n' +
      '지금까지 우리의 조사는 가치 기반 방법에 초점을 맞추었다. 여기에서 우리는 점진적인 크기 가지치기가 인기 있는 정책-경사 알고리즘인 소프트 액터 비평(SAC; Haarnoja et al., 2018)에 대한 성능 이득을 산출할 수 있는지 조사한다. 우리는 MuJoCo suite(Todorov et al., 2012)의 5가지 연속 제어 환경에서 각각에 대해 10개의 독립적인 종자를 사용하여 SAC를 평가했다. 그림 9에서 우리는 점진적인 크기 가지치기의 이점이 지속되는 워커2d-v2 및 Ant-v2에 대한 결과를 제시하며, 나머지 세 가지 환경(부록 E 참조)에서는 가지치기로 인한 실제 관찰 가능한 이득이 없다.\n' +
      '\n' +
      '도 8: **40M 프레임**에 대해 트레이닝될 때 DrQ(\\(\\epsilon\\))(좌측) 및 DER(우측)의 성능. 두 에이전트 모두 폭이 3인 ResNet 아키텍처를 사용합니다. 자세한 내용은 섹션 4.1을 참조하십시오.\n' +
      '\n' +
      '도 7: **전체 Atari 2600 suite.** DQN(좌측) 및 레인보우(우측)에 대한 성능 평가, 둘 다 폭이 3인 ResNet 아키텍처를 사용한다. 훈련 세부사항은 섹션 4.1을 참조한다.\n' +
      '\n' +
      '### 프루닝된 네트워크의 안정성\n' +
      '\n' +
      '우리는 Graesser et al.(2022)에 의해 제안된 가지치기 스케줄을 따랐는데, 이는 (오프라인 RL 실험에 대해 위에서 논의된 바와 같이) 상이한 트레이닝 단계들에 자연스럽게 적응한다. 이 일정은 훈련 단계의 최종 20%에 대해서만 최종 희소 네트워크를 훈련한다. 자연스러운 질문은 결과적인 희소 네트워크가 더 오래 훈련될 때 여전히 성능을 유지할 수 있는지 여부이다. 이를 평가하기 위해 DQN을 1억 프레임 동안 훈련하고 100M에 사용할 정규 일정과 40M 훈련 단계에 일반적으로 사용할 일정의 두 가지 가지치기 일정을 적용했다(그림 2 참조).\n' +
      '\n' +
      '그림 10에서 알 수 있듯이 압축된 40M 스케줄로도 프루닝된 네트워크는 강력한 성능을 유지할 수 있다. 흥미롭게도, 압축된 스케줄로 에이전트는 일반 스케줄보다 더 높은 성능 _faster_를 달성한다. 이는 대체 가지치기 일정을 탐색할 수 있는 충분한 여지가 있음을 시사한다.\n' +
      '\n' +
      '##5 가지치기가 왜 그렇게 효과적이죠?\n' +
      '\n' +
      '우리는 빔라이더, 브레이크아웃, 엔두로, 비디오핀볼의 네 가지 게임에 대한 분석을 집중한다. 각각에 대해 \\(Q\\) 추정치의 분산(QVariance); 네트워크 파라미터의 평균 norm(ParametersNorm); \\(Q\\)-값들의 평균 norm(QNorm); Kumar et al. (2021)이 제안한 행렬의 유효 순위(Srank), Sokar et al. (2023)이 정의한 휴면 뉴런의 분율을 측정한다. 우리는 그림 11에 우리의 결과를 제시한다. 이러한 그림에서 분명한 것은 가지치기 **(i)**는 분산을 감소시키고, **(ii)**는 매개변수의 규범을 감소시키며, **(iii)**는 휴면 뉴런의 수를 감소시키고, **(iv)**는 매개변수의 유효 순위를 증가시킨다는 것이다. 이러한 관찰 중 일부는 정규화의 형태에 기인할 수 있는 반면 다른 관찰은 네트워크 가소성의 증가로 인해 발생할 수 있다.\n' +
      '\n' +
      '### 다른 방법과의 비교\n' +
      '\n' +
      '정규화 및 명시적 가소성 주입에서 가지치기의 영향을 분리하기 위해 문헌의 기존 방법과 비교한다.\n' +
      '\n' +
      '정규화 기준에서는 프루닝에 의해 생성된 성능 향상에 대한 역할 정규화 역할을 조사하기 위해 문헌에서 효과가 입증된 두 가지 유형의 \\(\\ell_{2}\\) 정규화를 고려한다. 첫 번째는 가중치 감소(**WD**)로, 가중치의 \\(\\ell_{2}\\) 규범에 불이익을 주는 손실에 추가항을 추가하여 네트워크 매개변수가 너무 크게 성장하는 것을 방지하는 표준 기법이다. 두 번째는 Kumar et al. (2022)에 의해 제안된 규칙화 접근법인 **L2**이며, 이는 매개변수에 대해 1의 \\(\\ell_{2}\\) 규범을 시행하도록 설계되었다.\n' +
      '\n' +
      '가소성 주입 기준선은 소성 손실을 직접 처리하기 위해 제안된 두 가지 최근 작업과 비교된다. 니키신 외(2022)는 리플레이 비율이 증가함에 따라 성능이 감소하는 것을 관찰하여 초기 샘플에 과적합으로 귀인했으며 이는 "친밀도 편향"이라고 하는 효과이다. 저자들은 주기적으로 네트워크를 재설정할 것을 제안했고, 그것이 프라이버시 편향을 완화하고 일반적으로 과적합에 매우 효과적임을 입증했다(이것은 우리의 결과에서 **리셋**로 표시된다). Sokar et al.(2023)은 대부분의 딥 RL 에이전트들이 _dormant neuron phenomenon_으로 고통받고, 이에 의해 딥 RL 에이전트들의 트레이닝 동안 뉴런들이 점점 "off"하고, 따라서 네트워크 표현성을 감소시킨다는 것을 입증하였다. 이를 완화하기 위해 훈련 내내 휴면 뉴런(**ReDo**)을 재활용하는 간단하고 효과적인 방법을 제안했다.\n' +
      '\n' +
      '그림 12에서 알 수 있듯이 점진적인 크기 가지치기는 모든 수준의 규모와 전체 훈련에서 다른 모든 정규화 방법을 능가한다. 흥미롭게도, 대부분의 정규화 방법들은 네트워크 폭을 증가시킬 때 열화를 겪는다. 이는 가지치기의 효과가 정규화 또는 가소성 주입의 형태에만 기인할 수 없음을 시사한다. 그러나 아래에서 볼 수 있듯이 증가된 가소성은 그 사용에서 발생하는 것으로 판단된다.\n' +
      '\n' +
      '가소성에 미치는 영향\n' +
      '\n' +
      '가소성은 데이터 분포의 이동(Lewandowski et al., 2023; Lyle et al., 2022, 2023)에 대응하여 빠르게 조정할 수 있는 신경망의 능력이다(Lewandowski et al., 2023; Lyle et al., 2022, 2023). 강화 학습의 비안정성을 고려할 때, 좋은 성능을 보장하기 위해 유지하는 것이 중요하다. 그러나, RL 네트워크는 훈련 과정에서 가소성을 잃는 것으로 알려져 있다(Lee et al., 2023; Nikishin et al., 2022; Sokar et al., 2023).\n' +
      '\n' +
      'Lyle et al.(2023)은 네트워크의 손실 경관을 조사하기 위해 기울기의 공분산 구조에 대한 평가를 수행했으며 향상된 성능 및 증가된 가소성은 종종 더 약한 기울기 상관 및 감소된 기울기 간섭과 관련이 있다고 주장했다. 우리의 관찰은 그림 13의 구배 공분산 열 지도에 설명된 바와 같이 이러한 발견과 일치한다. 조밀한 네트워크에서 구배는 주목할만한 공동선성을 나타내는 반면, 가지치기 네트워크에서 이러한 공동선성은 극적으로 감소한다.\n' +
      '\n' +
      '그림 11: 가지치기 적용 시 대표 게임 4종에 대한 **실증분석. 좌우: 훈련 수익률, 평균 \\(Q\\)-목표 분산, 평균 파라미터 규범, 평균 \\(Q\\)-추정 규범, _srank_(Kumar et al., 2021), 휴면 뉴런(Sokar et al., 2023). 5개의 종자에 걸쳐 평균화된 모든 결과는 음영 지역이 95% 신뢰 구간을 나타낸다.**\n' +
      '\n' +
      '##6 토론 및 결론\n' +
      '\n' +
      '선행 연구는 강화 학습 에이전트가 사용 가능한 매개 변수를 과소 활용하려는 경향이 있으며, 이러한 과소 활용은 훈련 전반에 걸쳐 증가하고 네트워크 크기가 증가함에 따라 증폭된다는 것을 입증했다(Graesser et al., 2022; Nikishin et al., 2022; Schwarzer et al., 2023; Sokar et al., 2023). RL 에이전트는 작은 네트워크(예: 언어 모델에 사용되는 것과 비교하여)로 구축된 대부분의 벤치마크에서 강력한 성능을 달성하므로 이러한 명백한 매개변수 비효율성은 다른 알고리즘 고려 사항보다 덜 중요한 것으로 무시될 수 있다.\n' +
      '\n' +
      'RL이 학문적 벤치마크를 벗어나 더 복잡한 작업으로 계속 성장함에 따라 더 크고 표현적인 네트워크가 거의 확실히 필요할 것이다. 이 경우 파라미터 효율성은 선행 연구에서 나타난 성능 붕괴를 방지하고 계산 비용을 줄이기 위해 중요하다(Ceron and Castro, 2021).\n' +
      '\n' +
      '우리의 연구는 점진적인 크기 가지치기와 같은 희소 훈련 기술이 네트워크 활용을 최대화하는 데 효과적일 수 있다는 설득력 있는 증거를 제공한다. 그림 1과 4는 대부분 깊은 RL 네트워크를 피한 "스케일링 법칙"의 유형을 제안한다. 도 6, 도 8 및 도 10의 결과는 모두 희소 네트워크를 입증한다\n' +
      '\n' +
      '도 12: **네트워크 재설정(Nikishin et al., 2022), 가중치 감소, ReDo(Sokar et al., 2023) 및 정규화(Kumar et al., 2022)에 대한 비교. 왼쪽: 폭 팩터가 3인 샘플 효율 곡선; 오른쪽: 폭이 변하는 40M 프레임(오른쪽 패널) 이후의 최종 성능. 모든 실험은 ResNet 구조와 0.25.**의 재생 비율로 DQN에서 실행된다.\n' +
      '\n' +
      '도 13: 브레이크아웃(왼쪽) 및 비디오핀볼(오른쪽) 아타리 게임에 대한 **그라디언트 공분산 행렬. 다크 레드는 높은 음의 상관관계를 나타내고, 다크 블루는 높은 양의 상관관계를 나타낸다. 프루닝의 사용은 희소 네트워크에 대한 히트맵의 밝은 색조에 의해 입증된 바와 같이 더 약한 구배 상관 관계와 더 적은 구배 간섭을 유도한다.프루닝에 의해 생성된**는 더 오래 훈련될 때 안정적인 성능을 유지하는 데 더 우수하다.\n' +
      '\n' +
      '종합적으로, 우리의 결과는 훈련 전반에 걸쳐 네트워크 파라미터들을 의미 있게 제거함으로써, 우리가 초기 네트워크 아키텍처들을 성장시킴에 따라 전통적인 밀집된 대응들을 능가하고 계속해서 성능을 향상시킬 수 있다는 것을 입증한다. 다양한 에이전트 및 훈련 체제에 대한 우리의 결과는 점진적인 크기 가지치기가 에이전트 성능을 최대화하기 위한 "드롭인"으로 사용될 수 있는 일반적으로 유용한 기술임을 의미한다.\n' +
      '\n' +
      '다중 작업 일반화를 위해 설계된 최근 에이전트(Kumar et al., 2022; Taiga et al., 2023), 샘플 효율성(D\'Oro et al., 2023; Schwarzer et al., 2023) 및 일반화 가능성(Hafner et al., 2023)에 점진적 크기 가지치기를 통합하는 것이 당연할 것이다. 또한, 프루닝된 네트워크의 관찰된 안정성은 미세 조정 또는 환생에 의존하는 방법에 대한 의미를 가질 수 있다(Agarwal et al., 2022).\n' +
      '\n' +
      '희소 네트워크 트레이닝을 위한 하드웨어 가속기의 최근 발전은 더 빠른 트레이닝 시간을 초래할 수 있으며, 희소 네트워크 트레이닝을 위한 방법에 대한 추가 연구에 대한 인센티브 역할을 한다. 또한, 이 접근법의 결과가 초기화될 때보다 더 적은 파라미터를 갖는 네트워크라는 사실은 에지 디바이스들 상의 다운스트림 애플리케이션들에 대해 매력적으로 만든다.\n' +
      '\n' +
      '최소한, 우리는 이 작업이 강화 학습 에이전트의 성능을 최대화하기 위한 효과적인 메커니즘으로서 비표준 네트워크 아키텍처 및 토폴로지를 탐구하는 초대의 역할을 하기를 바란다.\n' +
      '\n' +
      '저자들은 로라 그레이서, 고페쉬 서브바라지와 구글 딥마인드 몬트리올 팀의 나머지 팀원들에게 이 작품을 준비하는 동안 귀중한 토론에 대해 감사를 표하고 싶다. 또한 파이썬 커뮤니티 Oliphant(2007); Van Rossum and Drake Jr(1995)이 NumPy Harris et al.(2020), Matplotlib Hunter(2007) 및 JAX Bradbury et al.(2018)을 포함하여 이 작업을 가능하게 한 도구를 개발한 것에 대해 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Agarwal et al.(2020) R. Agarwal, D. Schuurmans, M. 노루지 오프라인 강화학습에 대한 낙관적 관점. H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 104-114. PMLR, 13-18 July 2020. URL[https://proceedings.mlr.press/v119/agarwal20c.html](https://proceedings.mlr.press/v119/agarwal20c.html).\n' +
      '* Agarwal et al.(2021) R. 가왈, M. 슈워저, P. S. 카스트로, A. C. 커빌, M. 벨레메어 통계적 벼랑 끝에 있는 심층 강화 학습 신경 정보 처리 시스템_, 34:29304-29320, 2021의 발전.\n' +
      '* Agarwal et al.(2022) R. 가왈, M. 슈워저, P. S. 카스트로, A. C. 커빌, M. 벨레메어 강화 학습의 부활: 선행 연산을 재사용하여 진행을 가속화합니다. 인석 고예조 모하메드, A. 아가왈, D. 벨그레이브, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 28955-28971. Curran Associates, Inc., 2022.\n' +
      '* Arnob et al.(2021) S. Y. Arnob, R. 오립성 Plis, D. Precup. 오프라인 강화 학습을 위한 단일 샷 프루닝. _ arXiv preprint arXiv:2112.15579_, 2021.\n' +
      '* Ba et al. (2016) J. L. Ba, J. R. Kiros, and G. E. Hinton. 레이어 정규화 ArXiv:1607.06450_, 2016.\n' +
      '* Bellemare et al.(2013) M. G. Bellemare, Y. 나다프, J. 베니스, M. 볼링 아케이드 학습환경: 일반 에이전트를 위한 평가 플랫폼. _ Journal of Artificial Intelligence Research_, 47:253-279, jun 2013. doi: 10.1613/jair.3912.\n' +
      '* Bellemare et al.(2017) M. G. Bellemare, W. 대브니, R 무노스 강화 학습에 대한 분배적 관점 2017년 _ICML_에서.\n' +
      '* 82, 2020.\n' +
      '* Berner et al. (2021) C. Berner, G. Brockman, B. Chan, V. 청, P. 데비악, C. 데니슨, D. 파리, Q. 피셔, S. Hashme, C. Hesse, et al.(2019)Dota 2 with large scale deep reinforcement learning. ArXiv:1912.06680. 인용: SS1.\n' +
      '*N. Bjorck, C. P. Gomes, 및 K. Q. Weinberger(2021)는 스펙트럼 정규화와 함께 더 깊은 강화 학습을 향한다. The Advances in Neural Information Processing Systems34, pp. 8242-8255. Cited by: SS1.\n' +
      '* J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, et al. (2018)Jax: composable transformations of python+ numpy programs. 인용: SS1.\n' +
      '* P. S. Castro, S. 모이트라, C. 젤리다, S. Kumar, and M. G. Bellemare (2018)Dopamine: a research framework for deep reinforcement learning. ArXiv:1812.06110. 인용: SS1.\n' +
      '* J. S. O. Ceron and P. S. Castro (2021)Revisiting Rainbow: 보다 통찰력 있고 포괄적인 심층 강화 학습 연구를 촉진한다. In International Conference on Machine Learning, pp. 1373-1383. Cited by: SS1.\n' +
      '* J. S. O. Ceron, M. G. Bellemare, and P. S. Castro(2023)Small batch deep reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, pp. 2023. External Links: Link, Document Cited by: SS1.\n' +
      '* E. Cetin, P. J. Ball, S. 로버츠, O 셀릭투탄(2022) 픽셀로부터의 오프 정책 심층 강화 학습을 안정화한다. In International Conference on Machine Learning, pp. 2784-2810. Cited by: SS1.\n' +
      '*K. Cobbe, C. Hesse, J. Hilton, and J. Schulman (2020) Leveraging procedures generation to benchmark reinforcement learning. In International conference on machine learning, pp. 2048-2056. Cited by: SS1.\n' +
      '* P. D\'Oro, M. Schwarzer, E. Nikishin, P. Bacon, M. G. Bellemare, and A. Courville (2023) Sample-efficient reinforcement learning by break the replay ratio barrier. 학습 표상에 관한 제11차 국제 회의에서 외부 링크: 인용된 링크: SS1.\n' +
      '* L. 에스페홀트, H. 소이어, R. 무노스 시모니안 V. 임태준 워드영 도론 V 피로이우 Harley, I. Dunning, et al.(2018)Impala: scalable distributed deep-rl with importance weighted actor-learner architectureures. In International conference on machine learning, pp. 1407-1416. Cited by: SS1.\n' +
      '*U. 에비티 게일, J. 메닉, P. S. 카스트로, 그리고 E. 엘슨(2020)은 복권에 당첨된다. In International Conference on Machine Learning, pp. 2943-2952. Cited by: SS1.\n' +
      '* J. FareBrother, J. Greaves, R. Agarwal, C. L. Lan, R. Goroshin, P. S. Castro, and M. G. Bellemare (2023)Proto-value network: scaling representation learning with auxiliary tasks. 학습 표상에 관한 제11차 국제 회의에 제출된 경우, 외부 링크: 인용된 링크: SS1.\n' +
      '* A. Fawzi, M. Balog, A. Huang, T. 휴버트, B. 로메라 파레데스, M. Barekatain, A. Novikov, F. J. R Ruiz, J. Schrittwieser, G. Swirszcz, et al.(2022) discovery faster matrix multiplication algorithms with reinforcement learning. Nature610(7930), pp. 47-53. Cited by: SS1.\n' +
      '*W. 페더스, 라마찬드란, R. 아가왈 벵지오, H. 라로셸, M. 롤랜드와 W 다브니(2020) 경험의 근본을 되짚어보는 것. In International Conference on Machine Learning, pp. 3061-3071. Cited by: SS1.\n' +
      '* M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih, R 무노스 D. 하사비스 O. 피에틴, C. 블런델, S. Legg (2018)Noisy network for exploration. 인용: SS1.\n' +
      '*Y. 갈과 Z Ghahramani (2016)Dropout as a bayesian approximation: representing model uncertainty in deep learning. IMT2000 3GPP - 기계 학습에 관한 국제 회의에서, pp. 1050-1059. 인용: SS1.\n' +
      '* Graesser et al.(2022) L. (주)그레이서 에비, E. 엘슨, 그리고 P. S. 카스트로 심층 강화 학습에서 희박한 훈련의 상태. _International Conference on Machine Learning_, pages 7766-7792. PMLR, 2022.\n' +
      '* Grooten et al. (2023) B. Grooten, G. Sokar, S. 도헤어, E 모카누, M E 테일러, M 페케니즈키, D. C. 모카누 딥 강화 학습에서 동적 희소 학습을 통한 자동 노이즈 필터링. _Proceedings of the 2023 International Conference on Autonomous Agent and Multiagent Systems_, AAMAS \'23, page 1932-1941, Richland, SC, 2023. International Foundation for Autonomous Agent and Multiagent Systems. ISBN 9781450394321\n' +
      '* Haarnoja et al. (2018) T. Haarnoja, A. Zhou, P. Abbeel, S. 레빈 소프트 액터-비평가: 확률적 액터를 사용한 오프 정책 최대 엔트로피 심화 강화 학습. _International conference on machine learning_, pages 1861-1870. PMLR, 2018.\n' +
      '* Hafner et al. (2023) D. Hafner, J. Pasukonis, J. Ba, and T. 말도 안 돼 세계 모델을 통해 다양한 도메인을 마스터합니다. _ arXiv preprint arXiv:2301.04104_, 2023.\n' +
      '* Han et al.(2015) S. 한현모, 원종달리 심층 압축: 프루닝, 트레이닝된 양자화 및 허프만 코딩으로 심층 신경망을 압축하는 단계; _ arXiv preprint arXiv:1510.00149_, 2015.\n' +
      '* Harris et al. (2020) C. R. Harris, K. J. Millman, S. J. Van Der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, et al. Array programming with numpy. _ Nature_, 585(7825):357-362, 2020.\n' +
      '* Hessel et al.(2018) M. 헤셀, J. 모다일, H. V. 해셀트, T. Schaul G. Ostrovski Dabney, D. Horgan, B. Piot, M. G. Azar, D. Silver. 무지개: 심층 강화 학습의 개선을 결합합니다. 2018년 _AAAI_에서.\n' +
      '* Hiraoka et al.(2021) T. 히라오카 이마가와 하시모토 오니시, Y. 쓰루오카 이중 효율적인 강화 학습을 위한 드롭아웃 q-함수. _International Conference on Learning Representations_, 2021.\n' +
      '*Hunter(2007) J. D. Hunter. 매트플로틀립: 2D 그래픽 환경. _ Computing in science & engineering_, 9(03):90-95, 2007.\n' +
      '*Ioffe and Szegedy(2015) S. Ioffe and C. Szegedy. 배치 정규화: 내부 공변량 이동을 줄여 심층 네트워크 교육을 가속화합니다. 기계 학습에 관한 국제 회의에서_, 페이지 448-456. pmlr, 2015.\n' +
      '* Kaiser et al.(2020) L. 카이저 바베이자데, P. 밀로스, B. 오신스키, R. H. 캠벨, K. 체코프스키, D. 에르한, C. 핀, P. 코자코프스키, S. Levine, et al. Model-based reinforcement learning for atari. _ International Conference on Learning Representations_, 2020.\n' +
      '* Kostrikov et al.(2020) I. Kostrikov, D. Yarats, and R. 퍼거스 이미지 증강이 필요한 전부입니다: 픽셀에서 심층 강화 학습을 정규화합니다. _ arXiv preprint arXiv:2004.13649_, 2020.\n' +
      '* Kumar et al. (2020) A. Kumar, A. Zhou, G. Tucker, and S. 레빈 오프라인 강화학습을 위한 보수적 q-learning. _ 신경 정보 처리 시스템_, 33:1179-1191, 2020에서의 발전.\n' +
      '* Kumar et al. (2021a) A. Kumar, R. Agarwal, D. Ghosh, S. 레빈 암시적 과소 모수화는 데이터 효율적인 심층 강화 학습을 억제합니다. _International Conference on Learning Representations_, 2021a. URL[https://openreview.net/forum?id=09bnihsFfXU](https://openreview.net/forum?id=09bnihsFfXU).\n' +
      '* Kumar et al. (2021b) A. Kumar, R. 가왈, T. Ma, A. Courville, G. Tucker, S. 레빈 Dr3: 가치 기반 심층 강화 학습은 명시적인 정규화가 필요하다. _ arXiv preprint arXiv:2112.04716_, 2021b.\n' +
      '* Kumar et al. (2022) A. Kumar, R. 가왈, 엑스 Geng, G. Tucker, S. 레빈 다양한 멀티태스크 데이터에 대한 오프라인 q-러닝은 스케일과 일반화를 모두 수행한다. _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* Lee 등(2023) H. Lee, H. Cho, H. Kim, D. Gwak, J. Kim, J. Choo, S. -Y. 윤씨, 윤씨 플라스틱: 샘플 효율적인 강화 학습을 위해 입력 및 라벨 가소성을 개선한다. 30-7차 신경 정보 처리 시스템 회의에서_, 2023.\n' +
      '\n' +
      '* Lee et al.(2024) J. H. Lee, W. 박남언미첼, J. 필로, J. S. O. Ceron, H.-B. 김남 이은환 Long, A. Yazdanbakhsh, et al. Javxpruner: 희소성 연구를 위한 간결한 라이브러리. _Conference on Parsimony and Learning_, pages 515-528. PMLR, 2024.\n' +
      '* Lewandowski et al. (2023) A. Lewandowski, H. Tanaka, D. Schuurmans, and M. C. Machado. 곡률은 가소성의 손실을 설명합니다. _ arXiv preprint arXiv:2312.00246_, 2023.\n' +
      '* Liu et al.(2020) Z. 류진 리병강, T 대럴 정책 최적화의 규칙화 문제-지속적 통제에 대한 경험적 연구. _International Conference on Learning Representations_, 2020.\n' +
      '* Livne and Cohen(2020) D. Livne and K. 세스 팝스: 심층 강화 학습을 위한 정책 가지치기 및 축소 IEEE Journal of Selected Topics in Signal Processing_, 14(4):789-801, May 2020. ISSN 1941-0484. doi: 10.1109/jstsp.2020.2967566. URL[http://dx.doi.org/10.1109/JSTSP.2020.2967566](http://dx.doi.org/10.1109/JSTSP.2020.2967566).\n' +
      '* Lyle et al.(2022) C. Lyle, M. 롤랜드와 W 대브니 강화 학습에서 용량 손실을 이해하고 예방합니다. _International Conference on Learning Representations_, 2022. URL[https://openreview.net/forum?id=ZkC8wKoLbQ7](https://openreview.net/forum?id=ZkC8wKoLbQ7)이다.\n' +
      '* Lyle et al. (2023) C. Lyle, Z. 정은희 파스카누, W. 대브니 신경망에서 가소성을 이해한다. In _Proceedings of the 40th International Conference on Machine Learning_, ICML\'23. JMLR.org, 2023.\n' +
      '*Mnih et al.(2015) V. Mnih K Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. 피터슨, C. 베티, A. 사딕, I. 안토노글루, H. 킹, D. 쿠마란, D. 위어스트라, S. 레그랑 D. 하사비스 심층 강화 학습을 통한 인간 수준의 제어 Nature_, 518(7540):529-533, Feb. 2015년\n' +
      '* Mocanu et al. (2018) D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. 기베스쿠와 A. 리오타 네트워크 과학에서 영감을 받은 적응형 희소 연결성을 가진 인공 신경망의 확장 가능한 훈련. _ Nature communications_, 9(1):2383, 2018.\n' +
      '* Nikishin et al.(2022) E. Nikishin, M. 슈워저, P. 도로, P. L. 베이컨과 A. 쿠르빌 심층 강화 학습에서 가장 중요한 편견 인규 차우두리 제겔카 L. 송세페스바리, G. 니우, S. Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 16828-16847. PMLR, 17-23 July 2022.\n' +
      '* Nikishin et al. (2023) E. Nikishin, J. Oh, G. Ostrovski, C. Lyle, R. 파스카누 대브니와 A. 바레토 가소성 주입을 통한 심층 강화 학습. _30-7th Conference on Neural Information Processing Systems_, 2023. URL[https://openreview.net/forum?id=jucDLW6G91](https://openreview.net/forum?id=jucDLW6G91).\n' +
      '*Oliphant (2007) T. E. Oliphant. 과학 컴퓨팅을 위한 파이썬 Computing in Science & Engineering_, 9(3):10-20, 2007. doi:10.1109/MCSE.2007.58.\n' +
      '* Ostrovski et al. (2021) G. Ostrovski, P. S. Castro, and W. 대브니 심층 강화 학습에서 수동적 학습의 어려움. A. 베이겔지머, Y. Dauphin, P. Liang and J. W. Vaughan, Editors, _Advances in Neural Information Processing Systems_, 2021. URL[https://openreview.net/forum?id=nPHA8fGicZk](https://openreview.net/forum?id=nPHA8fGicZk).\n' +
      '* Ota et al.(2021) K. Ota, D. K. Jha 및 A. Kanezaki. 심층 강화 학습을 위해 더 큰 네트워크를 훈련합니다. _ arXiv preprint arXiv:2102.07920_, 2021.\n' +
      '* Schaul et al.(2016) T. Schaul, J. Quan, I. Antonoglou, D. Silver. 우선 순위화된 경험 재생. _ CoRR_, abs/1511.05952, 2016.\n' +
      '* Schmidt and Schmied(2021) D. Schmidt and T. 쉬미드 무지개의 빠르고 데이터 효율적인 훈련: 아타리에 관한 실험적 연구 _ arXiv preprint arXiv:2111.10247_, 2021.\n' +
      '* Schmitt et al. (2018) S. 슈미트, J. J. 허드슨, A. 지덱, S. Osindero, C. Doersch, W. M. Czarnecki, J. Z. Leibo, H. Kuttler, A. Zisserman, K. Simonyan, et al. Kickstarting deep reinforcement learning. _ arXiv preprint arXiv:1803.03835_, 2018.\n' +
      '* Schwarzer et al. (2018) M. 슈워저, J. S. O. Ceron, A. Courville, M. G. Bellemare, R. Agarwal and P. S. Castro. 더 크고, 더 좋고, 더 빠르다: 인간 수준의 효율성을 가진 인간 수준의 아타리. _International Conference on Machine Learning_, pages 30365-30380. PMLR, 2023.\n' +
      '* Sinha et al.(2020) S. 신하, H. 바라드화즈, A. 스리니바스, A. 가그. D2rl: 강화 학습에서의 깊은 밀집 아키텍처들 _ arXiv preprint arXiv:2010.09163_, 2020.\n' +
      '* Sokar et al. (2021) G. Sokar, E. Mocanu, D. C. Mocanu, M. 페케니즈키, P 스톤 심층 강화 학습을 위한 동적 희소 훈련 arXiv preprint arXiv:2106.04217_, 2021.\n' +
      '* Sokar et al. (2021) G. Sokar, R. Agarwal, P. S. Castro, U. 에비 심층 강화 학습에서의 휴면 뉴런 현상. A. 크라우스, E. 브런스킬, K. 조병하르트 Sabato, and J. Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 32145-32168. PMLR, 23-29 Jul 2023. URL[https://proceedings.mlr.press/v202/sokar23a.html](https://proceedings.mlr.press/v202/sokar23a.html).\n' +
      '* Song et al.(2019) X. 송영 장승 투영 Du, B. Neyshabur 강화 학습에서의 관찰 과적합. _ ArXiv preprint arXiv:1912.02975_, 2019.\n' +
      '* Sutton (1988) R. S. Sutton. 시간차이의 방법으로 예측하는 법을 배운다. _ Machine Learning_, 3(1):9-44, Aug. 1988년\n' +
      '* Taiga 등(2023) A. A. Taiga, R. Agarwal, J. FareBrother, A. Courville, M. G. Bellemare. 강화학습에서 멀티태스크 프리트레이닝과 일반화에 대한 고찰 _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Tan et al.(2023) Y. 탄피후 판재황, L. 황 RLx2: 처음부터 희박한 심층 강화 학습 모델을 훈련시키는 것. _The Eleventh International Conference on Learning Representations_, 2023. URL[https://openreview.net/forum?id=DJEEq0Aq7to](https://openreview.net/forum?id=DJEEq0Aq7to).\n' +
      '* Todorov et al.(2012) E. Todorov, T. 에레즈, Y 타사 Mujoco: 모델 기반 제어를 위한 물리 엔진. _2012 IEEE/RSJ 국제회의에서 지능형 로봇 및 시스템_, 페이지 5026-5033. IEEE, 2012.\n' +
      '* van Hasselt et al. (2016) H. van Hasselt, A. Guez, and D. Silver. 이중 q-러닝을 활용한 심층 강화 학습. The _Proceedings of the Thirtieth AAAI Conference On Artificial Intelligence (AAAI), 2016_, 2016. cite arxiv:1509.06461Comment: AAAI 2016.\n' +
      '* Van Hasselt et al. (2018) H. Van Hasselt, Y. 도론, F 스트럽, M. N. 헤셀 소너트, J. 모다일 심층 강화 학습과 치명적인 삼합회 arXiv preprint arXiv:1812.02648_, 2018.\n' +
      '* Van Hasselt et al. (2019) H. P. Van Hasselt, M. 헤셀과 J. 아슬라니데스 강화학습에서 파라메트릭 모델을 언제 사용할 것인가? _ Neural Information Processing Systems_, 32, 2019에서의 발전\n' +
      '* Van Rossum and Drake Jr. (1995) G. Van Rossum and F. L. Drake Jr. _ 파이썬 참조 매뉴얼_. 센트럼 부어 위스쿤데 인포마티카 암스테르담, 1995년\n' +
      '* Vinyals et al.(2019) O. Vinyals, I. Babuschkin W. M. Czarnecki, M. 마티유, A. Dudzik, J. Chung, D. H. Choi, R. 파월 Ewalds, P. Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. _ Nature_, 575(7782):350-354, 2019.\n' +
      '* Vischer et al.(2021) M. Vischer, R. T. Lange, H. Sprekeler. 복권 및 심층 강화 학습에서 최소한의 작업 표현. _International Conference on Learning Representations_, 2021.\n' +
      '* Wang et al.(2016) Z. 왕태 샤울만 H. H. Hasselt, M. 란콧, N. 프리타스 심층 강화 학습을 위한 결투 네트워크 아키텍처 [Proceedings of the 33rd International Conference on Machine Learning_, volume 48, pages 1995-2003, 2016].\n' +
      '* Yarats et al.(2021) D. Yarats, R. 퍼거스와 나 코스트리코프 이미지 증강은 픽셀에서 심층 강화 학습을 정규화하는 데 필요한 전부입니다. _9th International Conference on Learning Representations, ICLR 2021_, 2021.\n' +
      '* Yu et al. (2019) H. Yu, S. 에두노프 티안, 그리고 A. S. 모르코스 보상과 다국어로 복권을 하는 것: rl과 nlp로 된 복권. _International Conference on Learning Representations_, 2019.\n' +
      '* Zhang et al.(2019) C. Zhang, O. 빈일스, R 무노스, S. 벵지오 심층강화학습에서 과적합에 관한 연구 arXiv preprint arXiv:1804.06893_, 2018.\n' +
      '* Zhang et al.(2019) H. Zhang, Z. 그와 J. 리. 신경망 압축으로 심층 강화 학습을 가속화합니다. In _2019 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2019.\n' +
      '* Zhu and Gupta (2017) M. 주영석 굽타 가지치기 또는 가지치기하지 않기: 2017년 모델 압축에 대한 가지치기의 효능을 탐구한다.\n' +
      '\n' +
      '## 부록 부호 사용 가능 여부\n' +
      '\n' +
      '우리의 실험은 주로 도파민 저장소에서 오픈 소스 코드로 구축되었다. 이에 대한 루트 디렉터리는 [https://github.com/google/dopamine/tree/master/dopamine/](https://github.com/google/dopamine/tree/master/dopamine/)이며 아래 하위 디렉터리를 지정한다(클릭 가능한 링크 포함).\n' +
      '\n' +
      '* DQN 및/jax/agents/의 무지개 에이전트들/\n' +
      '* /labs/atari-100k/의 Atari-100k 제제\n' +
      '* JaxPruner/jaxpruner/baseline/dopamine/의 희소성 스크립트\n' +
      '* Resnet architecture from /labs/offline-rl/jax/networks.py(**line 108**)\n' +
      '* 휴면 뉴런 메트릭, Reset, ReDo and Weight Decay from /labs/redo/\n' +
      '\n' +
      '순위의 메트릭 실험을 위해 우리는 코드를 사용했다:\n' +
      '\n' +
      '[https://github.com/google-research/google-research/blob/master/](https://github.com/google-research/google-research/blob/master/)\n' +
      '\n' +
      'generalization_representations_rl_aistats22/coherence/coherence_compute.py\n' +
      '\n' +
      '## 부록 B 바타리 게임 선택\n' +
      '\n' +
      '실험의 대부분은 ALE suite(Bellemare et al., 2013)에서 Graesser et al.(2022)이 제안한 대로 15개의 게임으로 실행되었다. 그러나 Atari 100k 에이전트(하위 섹션 4.3)의 경우 벤치마크와 일치하도록 26개 게임(Kaiser et al., 2020)의 표준 세트를 사용했다. 우리는 또한 60게임의 전체 세트로 몇 가지 실험을 실행했다. 구체적인 게임은 아래에 자세히 설명되어 있습니다.\n' +
      '\n' +
      '**15 게임 서브세트:** MsPacman, Pong, Qbert, (Assault, Asterix, BeamRider, Boxing, Breakout, Crazy-Climber, DemonAttack, Enduro, FishingDerby, SpaceInvaders, Tutankham, VideoPinball. Graesser et al.(2022)에 따르면, 이들 게임은 인간 성능의 약 100%의 낮은 컷오프와 함께 DQN의 인간 정규화 점수에 의해 순위가 매겨진 게임들(Mnih et al., 2015) 사이에 대략 균등하게 분배되도록 선택되었다.\n' +
      '\n' +
      '**26 게임 서브세트:** 에일리언, 아미다르, 폭행, 아스테릭스, 뱅크히스트, 배틀존, 복싱, 브레이크아웃, 초퍼 커맨드, 크레이지 클라이머, 데몬어택, 고속도로, 동상, 고퍼, 히어로, 제임스본드, 캥거루, 크롤, 쿵푸마스터, MsPacman, 퐁, 프라이빗아이, 큐버트, 로드러너, 시퀘스트, 업다운.\n' +
      '\n' +
      '**60 게임 세트:**에 더하여 위의 26 게임들: 에어레이드, 소행성, 아틀란티스, 빔라이더, 베르제르크, 볼링, 카니발, 센티피드, 더블덩크, 엘리베이터액션, 엔두로, 피싱더비, 그라비타, 아이스하키, 저니 이스케이프, 몬테즈마 레버지, 네임디스게임, 피닉스, 피트폴, 푸얀, 리버레이드, 로봇랭크, 스키, 솔라리스, 스페이스인베이더스, 스타거너, 테니스, 타임파일럿, 투탕컴, 벤처, 비디오핀볼, 위저드오브워, 야르스레벤지, 잭슨.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:18]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>