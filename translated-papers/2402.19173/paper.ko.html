<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#스타코더 2와 스택 v2 : 차세대\n' +
      '\n' +
      '안톤 Lozhkov\\({}^{1}\\) 레이몬드 Li\\({}^{1}\\) 조엘 Lamy-Poirier\\({}^{2}\\) Nouamane Tazi\\({}^{1}\\) Aen-Ding Li\\({}^{26}\\) Jenhao Zheltonozhskii\\({}^{3}\\) Dmitry Abulkhanov\\({}^{2}\\) Zian Ju\\({}^{2}\\) Arthur Zucker\\({}^{31}\\)\n' +
      '\n' +
      'Illinois Urbana-Champaign \\({}^{8}\\)Johns Hopkins University \\({}^{9}\\)ScaDS.AI \\({}^{11}\\)Nvidia \\({}^{4}\\)ServiceNow Research \\({}^{11}\\)Baidu \\({}^{22}\\)Mazzuma \\({}^{23}\\)Cornell University\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '빅코드 프로젝트는 코드 LLMs(Code LLMs)를 위한 대규모 언어 모델의 책임 있는 개발에 초점을 맞춘 개방형 협업으로 StarCoder2를 소개하고, 소프트웨어 헤리티지(SWH)와 협력하여 소스 코드 아카이브의 디지털 커먼즈 위에 스택 v2를 구축한다. 619개의 프로그래밍 언어에 걸쳐 있는 SWH 리포지토리와 함께 GitHub pull 요청, Kaggle 노트북 및 코드 설명서와 같은 다른 고품질 데이터 소스를 신중하게 선택합니다. 이는 첫 번째 스타코더 데이터 세트보다 4\\(\\times\\) 더 큰 트레이닝 세트를 생성한다. 3.3~4.3조 토큰에 3B, 7B 및 15B 매개변수가 있는 StarCoder2 모델을 훈련하고 포괄적인 코드 LLM 벤치마크 세트에서 철저히 평가한다.\n' +
      '\n' +
      '각주 1: [https://www.bigcode-project.org](https://www.bigcode-project.org)\n' +
      '\n' +
      '우리는 우리의 소형 모델인 StarCoder2-3B가 대부분의 벤치마크에서 비슷한 크기의 다른 코드 LLM보다 우수하고 StarCoderBase-15B보다 우수하다는 것을 발견했다. 당사의 대형 모델인 StarCoder2-15B는 비슷한 크기의 다른 모델보다 훨씬 우수합니다. 또한 CodeLlama-34B의 크기에 두 배 이상의 모델인 CodeLlama-34B와 일치하거나 능가합니다. DeepSeeKorder-33B는 높은 자원 언어에 대한 코드 완성도에서 가장 성능이 좋은 모델이지만, StarCoder2-15B는 여러 낮은 자원 언어뿐만 아니라 수학 및 코드 추론 벤치마크에서도 성능이 우수함을 알 수 있다. 우리는 OpenRAIL 라이선스에서 모델 가중치를 사용할 수 있도록 하고 소스 코드 데이터의 SWHID(SoftWare Heritage persistent IDentifier)를 릴리즈함으로써 학습 데이터에 대한 완전한 투명성을 보장한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '코드용 대형 언어 모델(코드 LLMs; Chen et al., 2021; Nijkamp et al., 2023; Roziere et al., 2023; Guo et al., 2024)은 코드 작성 및 편집을 위한 강력한 어시스턴트로 급속히 부상하였다. 2024년 1월 30일 현재 GitHub CoPilot은 130만 명 이상의 유료 가입자를 확보했으며, 50,000개 이상의 조직이 엔터프라이즈 버전(MSFT Q2 Earning Call, 2024)을 선택하여 개발자 만족도뿐만 아니라 개발자 생산성을 최대 56%까지 증가시키는 것으로 추산된다(Peng et al., 2023; Ziegler et al., 2024). ServiceNow는 최근 파인-튜닝 스타코더베이스 모델(Li et al., 2023)로부터 구축된 그들의 "텍스트-투-코드" 솔루션이 개발자 생산성의 52% 증가를 초래한다는 것을 개시하였다(Yahoo Finance, 2024). 자연어 명령어 또는 다른 코드 스니펫으로부터 코드 스니펫을 생성하는 것에 대한 초기 초점에도 불구하고, 코드 LLM은 소프트웨어 개발 사이클의 모든 단계를 향상시킬 가능성을 나타낸다(Hou et al., 2023; Fan et al., 2023; Wang et al., 2024; Zhuo et al., 2023; Chai et al., 2023). 여기에는 새로운 프로젝트의 실행 속도 향상, 개발된 소프트웨어에 대한 품질 보증 개선, 버그 탐지 및 수정 지원, 유지보수 작업 단순화, 최신 소프트웨어로의 마이그레이션 완화 등이 포함된다.\n' +
      '\n' +
      'LLM의 개발 프로세스는 상이한 수준의 개방성을 나타낼 수 있다(Solaiman, 2023; Ding et al., 2022; Akiki et al., 2022). OpenAI의 GPT-4(OpenAI et al., 2023)와 구글의 Gemini(Gemini Team et al., 2023)와 같은 독점 모델은 유료 API를 통해 해당 모델에 대한 접근을 제공하지만 개발 내역은 공개하지 않는다. 한편, Code LLMa(Roziere et al., 2023), Mistral(Jiang et al., 2023), DeepSeekCoder(Guo et al., 2024)와 같은 오픈 웨이트 모델들이 모델 웨이트들을 출시하였다. 이를 통해 오픈 소스 커뮤니티는 이러한 모델을 로컬로 실행하고 모델 표현을 검사하고 작업을 미세 조정할 수 있다. 그러나, 모델 개발자들은 그들의 훈련 데이터를 공개하지 않았다. 결과적으로 콘텐츠 제작자는 데이터가 교육에 사용되었는지 모르고 사회 과학자는 편향과 독성에 대해 데이터 세트를 면밀히 조사할 수 없으며 LLM 개발자는 훈련 세트가 테스트 벤치마크로 어느 정도 오염되었는지 정보가 부족하다. 더 넓게는 다른 연구팀이 서로의 훈련 데이터를 쉽게 재사용할 수 없기 때문에 이 관행은 과학적 진보를 방해한다. Alen AI의 OLMo(Groeneveld et al., 2024), Eleuther AI의 Pythia(Biderman et al., 2023), BigScience의 BLOOM(BigScience Workshop, 2022; Scao et al., 2022)과 같은 다른 LLM 개발 프로젝트는 훈련 데이터, 훈련 프레임워크 및 평가 스위트를 공개함으로써 완전 개방형 개발 접근법을 채택했다.\n' +
      '\n' +
      '빅코드 프로젝트는 코드 LLM의 개방적이고 책임 있는 개발에 초점을 맞춘 개방형 과학 협업으로 2022년 9월에 설립되었다. 빅코드는 오픈 거버넌스(BigCode collaboration et al., 2023)의 정신으로 ServiceNow와 Hugging Face에 의해 진행되며, 다양한 학원과 산업 연구소에서 1,100명 이상의 회원이 모였다. 커뮤니티는 이전에 384개의 프로그래밍 언어로 허용 허가된 소스 코드의 6.4 TB 데이터 세트인 Stack v1(Kocetkov et al., 2023)을 출시했다. Stack v1에는 개발자가 데이터 세트에 소스 코드가 포함되어 있는지 확인하기 위해 설계된 "Am I in The Stack"이라는 거버넌스 도구가 포함된다. 또한 데이터 세트에서 코드를 제외하는 것을 선호하는 사람들을 위한 옵트아웃 프로세스를 제공합니다. 2022년 12월, 빅코드 커뮤니티는 더 스택 v1에서 Java, JavaScript 및 Python 코드로 훈련된 강력한 성능의 1.1B 파라미터 모델인 SantaCoder(Ben Allal et al., 2023)를 출시했다. 이러한 성공에 따라, 커뮤니티는 노력을 더욱 확장하고 2023년 5월 4일에 StarCoder를 출시했다(Li et al., 2023). 출시 당시 15B 파라미터 스타코더 모델은 코드에 가장 적합한 오픈액세스 LLM이었다.\n' +
      '\n' +
      '이 기술 보고서에서는 Stack v2와 StarCoder2의 개발 과정을 기술하고 있으며, Stack v2는 600개 이상의 프로그래밍 언어에 걸쳐 있는 소프트웨어 헤리티지의 방대한 소스 코드 아카이브를 기반으로 한다. 코드 저장소 외에도 Github 문제, pull 요청, Kaggle 및 Jupyter 노트북, 코드 문서 및 수학, 코딩 및 추론과 관련된 기타 자연어 데이터 세트를 포함한 기타 고품질 오픈 데이터 소스를 큐레이션합니다. 학습용 데이터를 준비하기 위해 중복제거를 수행하고, 저품질 코드를 제거하기 위한 필터를 만들고, 개인식별정보(PII)를 재지정하며, 악성코드를 제거하고, 데이터세트에서 코드를 제거하도록 요청한 개발자의 옵트아웃을 처리한다. 이 새로운 900B+ 고유 토큰 집합으로, 첫 번째 스타코더 데이터 세트보다 4\\(\\times\\) 더 큰 새로운 스타코더 모델을 개발한다. 우리는 2단계 트레이닝 프로세스를 사용하여 3B, 7B 및 15B 파라미터로 코드 LLM을 트레이닝한다(Roziere et al., 2023; Guo et al., 2024). 우리는 4k 컨텍스트 윈도우로 기본 모델 학습을 시작하고 16k 컨텍스트 윈도우로 모델을 미세 조정한다. 우리는 트레이닝 프로세스가 데이터세트(Muennighoff et al., 2023)에 걸쳐 5개 이상의 에포크를 초과하지 않도록 보장한다. 그러나 우리는 훈련 토큰의 수를 Chinchilla(Harm\'s law; de Vries, 2023)가 제안한 계산 최적 수를 훨씬 넘어서고 3.3에서 4.3조 토큰 범위 내에서 비교적 작은 모델을 훈련한다. 우리는 코드 LLM 벤치마크들의 세트 상에서 이들 모델들의 성능을 철저히 평가하고 비교한다(Cassano et al., 2023; Austin et al., 2021; Chen et al., 2021; Liu et al., 2023; Lai et al., 2023; Muennighoff et al., 2024; Cassano et al., 2024; Liu et al., 2023; Ding et al., 2024; Gu et al., 2024; Cobbe et al., 2021; Pearce et al., 2022; Dhamala et al., 2021; Nozza et al., 2021; Gehman et al., 2020).\n' +
      '\n' +
      '* StarCoder2-3B 모델은 대부분의 벤치마크에서 유사한 크기의 다른 코드 LLMs(StableCode-3B 및 DeepSeeKoder-1.3B)보다 우수하다. 또한 스타코더베이스-15B의 성능과 일치하거나 능가합니다.\n' +
      '* StarCoder2-15B 모델은 비교 가능한 크기의 다른 모델들(CodeLlama-13B)을 상당히 능가하고, CodeLlama-34B와 일치하거나 능가한다. DeepSeeKoder-33B는 고자원 언어에 대한 코드 완성 벤치마크에서 가장 좋은 모델이다. 그러나, StarCoder2-15B는 낮은 자원 프로그래밍 언어들(예를 들어, D, Julia, Lua, 및 Perl)에서 DeepSeeKoder-33B와 매칭되거나 그 성능을 능가한다. 또한, 코드 실행(Gu et al., 2024)이나 수학(Cobbe et al., 2021)을 추론하기 위해 모델이 필요한 벤치마크를 고려할 때, StarCoder2-15B가 DeepSeeKoder-33B보다 성능이 우수함을 알 수 있다.\n' +
      '* StarCoder2-7B 모델은 CodeLlama-7B를 능가하지만 DeepSeeKoder-6.7B 뒤에 있다. 스타코더2-7B가 스타코더2-3B 및 스타코더2-15B만큼 그 크기에 대해 잘 수행하지 않는 이유는 이 보고서의 저자들에게 분명하지 않다.\n' +
      '\n' +
      '##2 데이터 소스\n' +
      '\n' +
      '이 절에서는 소프트웨어 헤리티지(SS2.1)에서 조달한 데이터뿐만 아니라 GitHub 이슈(SS2.2), pull request(SS2.3), Jupyter and Kaggle notebooks(SS2.4), 문서화(SS2.5), 중간 표현(SS2.6), 작은 수학 및 코딩 데이터셋(SS2.7) 및 기타 자연어 데이터셋(SS2.8)을 포함하여 훈련 데이터를 얻는 과정을 자세히 설명한다.\n' +
      '\n' +
      '### Source Code\n' +
      '\n' +
      '소프트웨어 HeritageWe build the Stack v2 on the Software Heritage(SH) archive(Abramatic et al., 2018)는 같은 이름의 비영리 단체에 의해 유지된다. 소프트웨어 헤리티지의 임무는 소스 코드의 형태를 취하는 모든 지식을 수집하고 보존하는 것이다. 우리는 전체 아카이브의 완전 중복 제거 머클 DAG(Merkle, 1987) 표현인 SH 그래프 데이터세트(Pietri et al., 2020)와 함께 작업한다. SH 그래프 데이터 세트는 소프트웨어 헤리티지에 의한 주기적인 크롤링 동안 관찰된 바와 같이 저장소의 전체 상태까지 파일 식별자, 소스 코드 디렉터리 및 git 커밋을 함께 링크한다.\n' +
      '\n' +
      '리포지토리 추출 SH 그래프 데이터 세트의 2023-09-06 버전을 기본 소스로 활용합니다. 우리는 가장 최근에 크롤링된 모든 GitHub 리포지토리의 버전을 추출하고 메인 브랜치만 유지하도록 필터링하는 것으로 시작한다. GHArchive의 리포지토리 메타데이터가 기본 분기로 나열하거나 이름이 주 또는 마스터인 경우 분기는 주 분기로 간주됩니다. 우리는 단지 주 분기에서 최신 수정(commit)을 추출하고 그 내용(SH 데이터세트의 열 directory_id)의 고유한 해시를 기반으로 리포지토리를 중복 해제한다. 리포지토리들의 디렉토리 구조는 directory_id 및 타겟 컬럼들을 사용하여 데이터세트의 directory_entry 테이블을 자신에게 재귀적으로 조인하고, 디렉토리 및 파일 이름들(컬럼 이름)을 풀 경로들로 연결함으로써 재구성된다. 디렉토리 트리를 레벨 64까지만 횡단합니다. 압축된 파일 크기가 10MB 미만인 경우 SH 콘텐츠 S3 버킷에서 개별 파일 콘텐츠를 다운로드합니다.\n' +
      '\n' +
      '라이센스 검출은 SWH 데이터 세트에서 일치하는 이름을 가진 모든 리포지토리에 대해 GHArchive(Github Archive, 2024)로부터 저장소 수준의 라이센스 정보를 추출한다. 리포-레벨 라이센스가 이용가능하지 않을 때, 즉 리포지토리의 96.93%에 대해, 우리는 다음과 같이 파일-레벨 라이센스를 검출하기 위해 ScanCode Toolkit(ScanCode, 2024)을 사용한다:* 부록 A.3에서 정규식을 사용하여 라이센스를 포함할 수 있는 모든 파일을 찾는다. 이것은 우리가 라이센스를 명시적으로 포함하거나(예를 들어, LICENSE, MIT.txt, Apache2.0) 라이센스에 대한 참조를 포함하는 파일(예를 들어, README.md, GUIDELINES)을 수집할 수 있게 한다;\n' +
      '* ScanCode의 라이센스 검출을 매칭 파일에 적용하고 검출된 라이센스의 SPDX3 ID를 수집; 각주 3: 시스템 패키지 데이터 교환, [https://spdx.dev](https://spdx.dev)\n' +
      '* 탐지된 라이선스를 라이선스 파일과 저장소 내의 기본 경로가 동일한 모든 파일에 전파합니다.\n' +
      '\n' +
      '파일 수준 라이선스 정보가 수집되면 그림 1에 설명된 알고리즘에 따라 파일이 허용 라이선스, 비허용 라이선스 또는 비허용 라이선스인지 여부를 결정한다.\n' +
      '\n' +
      '우리가 허용하는 것으로 간주하는 라이선스는 부록 A.4에 나열되어 있으며, 이 목록은 블루오크 협의회(Blue Oak Council, 2024)에서 승인한 라이선스와 스캔코드(ScanCode License Categories, 2024)에서 "Permissive" 또는 "Public Domain"으로 분류한 라이선스에서 작성되었다.\n' +
      '\n' +
      '데이터 라이선스는 허용 라이선스, 비허용 라이선스(예: 저작권) 및 비허용 파일의 세 가지 유형의 파일을 고려한다. 스택 v2와 스택 v1의 주요 차이점은 허가된 파일과 허가되지 않은 파일을 모두 포함한다는 것이다. 창작자가 코드를 상업적 목적으로 사용할 의도가 없기 때문에 상업적 라이선스를 제외합니다. 또한 이러한 데이터를 LLM 교육에 사용하는 커뮤니티의 입장과 상대적으로 낮은 볼륨에 대한 불확실성으로 인해 저작권 라이선스 코드를 제외한다.\n' +
      '\n' +
      '언어 검출 Stack v1(Kocetkov et al., 2023)이 그들의 파일 확장에 의해 프로그래밍 언어를 검출하는 동안, 우리는 대신에 언어 분류기에 의존한다. 구체적으로, GitHub의 라이브러리 언어학자(go-enry, 2024)를 기반으로 한 고-enry를 사용하여 각 파일에 대한 프로그래밍 언어를 탐지한다. 우리는 TheStackV2-dedup에서 658개의 고유 언어를 탐지하며, 그 중 일부는 데이터 검사 단계에서 제거된다(다음 단락 참조).\n' +
      '\n' +
      '도 1: 파일 레벨 라이센스 할당 로직.\n' +
      '\n' +
      '첫 번째 스타코더와 유사한 시각적 데이터 검사는 저품질 훈련 데이터로 확장을 제거하기 위해 데이터 검사 스프린트에 빅코드 커뮤니티를 포함한다. 우리는 (86개의 프로그래밍 언어 포함) 300개의 확장 중 36개를 제거한 이전 반복의 주석에서 시작한다. 스타코더2의 경우 아직 주석이 지정되지 않은 프로그래밍 언어(즉, 스타코더베이스의 86개 언어를 제외)에 대한 데이터 검사만 실행했다. 이 프로세스를 간소화하기 위해 검사를 1,000개 이상의 파일을 포함하고 해당 언어로 파일의 0.5% 이상을 나타내는 확장자로 제한했다. 나머지 확장은 작은 부피만 구성하기 때문에 추가 검사 없이 유지되었다. 빅코드 커뮤니티의 15명의 주석자의 도움으로 약 1000개의 확장을 시각적으로 검사하고 130개를 제외했다(전체 목록은 부록 A.1 참조). 데이터 검사 단계는 데이터 세트(부록 A.2)에서 39개의 프로그래밍 언어를 제외하여 619개의 프로그래밍 언어를 최종 집계했다.\n' +
      '\n' +
      '기본 필터 집합은 자동 생성 파일, 데이터 파일 또는 기타 저품질 학습 데이터를 제거하기 위해 데이터 세트에 기본 필터 집합을 적용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r r|r r|r r} \\hline \\hline  & \\multicolumn{2}{c}{**The-stack-v1-dedup**} & \\multicolumn{2}{c}{**The-stack-v2-dedup**} & \\multicolumn{2}{c}{**The-stack-v2-swh-full**} \\\\ Language & Size (GB) & Files (M) & Size (GB) & Files (M) & Size (GB) & Files (M) \\\\ \\hline Assembly & 1.58 & 0.25 & 13.02 & 0.77 & 7.74 & 0.70 \\\\ Batchfile & 0.29 & 0.25 & 2.11 & 1.13 & 1.02 & 0.99 \\\\ C & 57.43 & 8.53 & 202.05 & 20.78 & 114.92 & 19.18 \\\\ C\\# & 46.29 & 10.84 & 239.89 & 51.23 & 169.75 & 48.49 \\\\ C++ & 50.89 & 6.37 & 353.89 & 43.18 & 211.33 & 42.23 \\\\ CMake & 0.45 & 0.19 & 2.58 & 1.74 & 2.27 & 1.70 \\\\ CSS & 22.61 & 2.99 & 161.68 & 23.87 & 8.00 & 1.88 \\\\ Dockerfile & 0.572 & 0.42 & 1.27 & 1.90 & 1.21 & 1.88 \\\\ Fortran & 0.17 & 1.84 & 4.66 & 0.27 & 3.61 & 0.26 \\\\ Go & 25.74 & 4.73 & 54.60 & 9.30 & 25.83 & 8.62 \\\\ Haskell & 2.36 & 0.54 & 5.11 & 1.25 & 4.17 & 1.23 \\\\ HTML & 146.76 & 9.53 & 2,419.87 & 90.23 & 99.09 & 5.23 \\\\ Java & 89.30 & 20.15 & 548.00 & 154.28 & 199.68 & 62.27 \\\\ JavaScript & 141.65 & 21.11 & 1,115.42 & 108.87 & 199.99 & 66.91 \\\\ Julia & 1.54 & 0.30 & 6.12 & 0.45 & 1.83 & 0.43 \\\\ Lua & 3.28 & 0.56 & 33.91 & 2.35 & 15.22 & 2.24 \\\\ Makefile & 1.49 & 0.66 & 21.30 & 4.22 & 5.19 & 2.78 \\\\ Markdown & 75.25 & 21.0 & 281.04 & 82.78 & 244.17 & 81.42 \\\\ Perl & 2.63 & 0.39 & 7.82 & 1.15 & 5.66 & 1.06 \\\\ PHP & 66.84 & 15.90 & 224.59 & 46.03 & 183.70 & 45.14 \\\\ PowerShell & 1.25 & 0.27 & 3.97 & 0.68 & 2.46 & 0.66 \\\\ Python & 64.30 & 12.96 & 233.29 & 56.93 & 191.61 & 56.19 \\\\ R & 0.30 & 0.04 & 22.39 & 5.15 & 19.05 & 4.29 \\\\ Ruby & 7.14 & 3.41 & 31.70 & 17.79 & 23.38 & 17.51 \\\\ Rust & 9.53 & 1.38 & 15.60 & 2.22 & 12.43 & 2.19 \\\\ Scala & 4.86 & 1.36 & 12.73 & 4.45 & 11.30 & 4.32 \\\\ Shell & 3.38 & 22.69 & 19.82 & 10.68 & 13.51 & 10.01 \\\\ SQL & 12.22 & 0.99 & 281.45 & 5.29 & 35.75 & 4.52 \\\\ Swift & 0 & 0 & 23.76 & 7.23 & 22.32 & 7.16 \\\\ TeX & 5.44 & 0.55 & 35.86 & 3.19 & 30.01 & 2.86 \\\\ TypeScript & 28.82 & 10.64 & 61.01 & 23.85 & 49.14 & 23.28 \\\\ Visual Basic & 1.49 & 0.16 & 16.63 & 1.06 & 7.48 & 0.81 \\\\ \\hline Total & 875.85 & 181.00 & 6,457.14 & 784.30 & 1,922.82 & 528.44 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 32개의 인기 있는 프로그래밍 언어에 대한 스택 v1 및 v2의 비교. 우리는 다른 데이터 분할에 대한 파일의 크기와 수를 보여준다: The Stack v1 deduped, The Stack v2 deduped, and the training data used for StarCoder2-15B.\n' +
      '\n' +
      '* _Long line filter_: 해당 파일이 데이터 또는 생성된 코드일 가능성이 높기 때문에 먼저 100k 라인 이상의 모든 파일을 제거한다. 또한 HTML, JSON, Markdown, Roff, Roff Manpage, SMT, TeX, Text, XML을 제외한 모든 언어에 대해 평균 100자 이상의 줄길이 또는 최대 1000자 이상의 줄길이의 파일을 제거한다. 언급된 언어의 경우 가장 긴 줄이 100k자를 초과하는 파일을 제거합니다.\n' +
      '* _Autogenerated filter_: go-enry의 is_generated function에 의해 auto-generated로 분류된 파일들을 제거한다(go-enry, 2024). 또한, 파일의 처음 5행에서 {"자동 생성", "자동 생성", "자동 생성", "자동 생성", "이 파일이 생성"} 중 하나를 포함하는 파일은 제외한다.\n' +
      '* _Alpha filter_: 모토롤라 68K Assembly 및 WebAssembly를 제외한 모든 언어에 대해 25% 미만의 알파벳 문자를 가진 파일을 제거하는데, 여기서 우리는 해당 언어의 구문으로 인해 25% 미만의 알파벳 문자를 가진 파일만 제거한다.\n' +
      '* _Encoded data filter_: 다음 정규식을 이용하여 인라인 부호화된 데이터로 파일을 검출한다:\n' +
      '* Base64 string: [a-zA-Z0-9+/\\n=]{64,}\n' +
      '*Hexadecimal sequences: (?:b(?:bx|\\x) [0-9a-fA-F]{2}(Δ:,|b\\s*)){8}\n' +
      '* 유니코드 문자열: (?:\\u[0-9a-fA-F]{4}){8,} 이러한 표현식과 일치하는 부분 문자열 중 하나가 1024자보다 길거나 일치하는 문자의 비율이 파일의 50% 이상인 경우 파일을 제거합니다.\n' +
      '\n' +
      '언어별 필터는 기본 필터 외에도 다음과 같은 언어별 필터 세트를 적용한다.\n' +
      '\n' +
      '* Text, JSON, YAML, Web Ontology Language, Graphviz(DOT)의 경우 512줄 이상의 파일을 제거하여 데이터 파일에서 반복되는 토큰의 영향을 최소화한다.\n' +
      '* HTML의 경우, 스타코더의 처리 파이프라인(Li et al., 2023)과 유사하게, 가시적인 텍스트가 적어도 100 문자 길이이고 코드의 적어도 20%를 구성하는 파일만을 유지한다.\n' +
      '* 텍스트의 경우, 하위 파일 이름에 "요구 사항"이 있는 파일만 보관하거나, 확장자가 없는 파일 이름이 {"readme", "notes", "todo", "description", "cmakelists" 중 하나인 경우}.\n' +
      '\n' +
      '### Github Issues\n' +
      '\n' +
      '우리는 GHArchive(Github Archive, 2024)에서 수집된 GitHub 이슈를 통합한다. SS2.3에서 별도로 처리하므로 여기에서 풀 요청을 제외합니다.\n' +
      '\n' +
      '기텁 이슈는 이슈 개방, 댓글 작성, 이슈 종결 등 행동이 있는 일련의 사건들로 구성된다. 각 이벤트에는 작성자의 사용자 이름, 메시지, 작업 및 작성 날짜가 포함됩니다. 우리는 이하 요약하는 StarCoder(Li et al., 2023)의 처리 파이프라인을 따른다:\n' +
      '\n' +
      '* 먼저, 사용자가 이메일을 통해 이슈에 응답했을 때 자동 생성된 텍스트를 제거하였다(더 많은 정보는 Li 등, 2023, 부록 A를 참조). 또한 짧은 메시지(200자 미만)로 이슈를 삭제하고 마지막 20줄을 유지하면서 중간에서 최대 100줄까지 긴 댓글을 잘랐다. 이것은 볼륨의 17%를 제거했는데, 이는 StarCoderBase에서와 유사한 비율이다.\n' +
      '* 다음으로, 우리는 봇에서 논평을 제외했다. 이를 위해 댓글 작성자의 사용자 이름에서 키워드를 검색하였다(자세한 내용은 Li 등, 2023, 부록 A를 참조). 이 단계는 StarCoder(Li et al., 2023)에서 보고된 17%보다 훨씬 적은 3%의 문제를 제거했다. 이러한 불일치는 주로 데이터 세트에 봇 생성 콘텐츠의 상당한 비율의 소스인 풀 요청이 포함되어 있지 않기 때문이다.\n' +
      '\n' +
      '* 화질의 지표로 대화에 참여한 사용자 수를 사용하였다. 우리의 기준은 두 명 이상의 사용자가 있는 대화를 포함하는 것이었다. 그러나 댓글 내 총 텍스트가 7,000자(96번째 백분위수) 미만인 경우 단일 사용자와 관련된 대화도 보존했다. 또한 단일 사용자가 저작한 문제가 품질이 좋지 않거나 간과된 봇에서 비롯된 경향이 있기 때문에 10개 이상의 이벤트가 포함된 경우 제외했다. 이러한 필터를 구현하여 나머지 문제의 38%를 제거했습니다. 마지막으로, 대화의 사용자 이름을 대화 내의 참가자 카운터(StarCoder의 프로세스 후속)로 대체하여 익명화했다.\n' +
      '\n' +
      '### Pull Requests\n' +
      '\n' +
      'GHA Archive(Github Archive, 2024)의 풀 요청 이벤트와 소프트웨어 Heritage(Software Heritage, 2024b)의 해당 소스 코드를 수집하여 코드 리뷰를 포함한다. 풀 요청은 GitHub에서 한 지점에서 다른 지점으로 특정 코드 변경 사항을 병합하기 위한 요청입니다. 일반적으로 여러 라운드의 코드 검토 토론과 대상 분기에 병합되기 전에 코드 변경의 추가 주기가 포함됩니다.\n' +
      '\n' +
      '데이터 수집은 특히 각 풀 요청에 대해 GHA 아카이브에서 발견된 PullRequestEvent, PullRequestReviewEvent, PullRequestReviewCommentEvent, IssueCommentEvent 및 IssuesEvent 이벤트를 집계한다. 이러한 이벤트 간의 차이에 대한 자세한 내용은 Github 문서에서 확인할 수 있습니다. 다음으로, 이러한 이벤트에서 모든 기본 및 헤드 커밋 ID를 추출하고 소프트웨어 헤리티지에서 해당 코드 파일을 검색한다. 커밋 diff에 대한 액세스 권한이 없기 때문에 동일한 경로에서 파일 간의 변경 사항을 식별하여 생성합니다. 베이스에는 없지만 헤드에는 없는 파일을 삭제로 간주하는 반면, 베이스에는 없지만 헤드에는 있는 파일을 추가로 간주한다. 이 프로세스는 약 300M PR을 산출하며, 여기에 15TB의 기본 코드가 수반된다. 이 중 약 24M 리포지토리에서 유래한 215M 폐쇄 PR이 있다.\n' +
      '\n' +
      'PR 필터는 1) 봇에 의해 오픈된 PR, 2) 봇에 의해 코멘트로만 구성된 PR, 3) 비허용 라이센스가 있는 PR, 4) 비허용 라이센스가 있는 PR, 5) PR 동안 기반 변경, 6) 승인 또는 병합되지 않은 PR 또는 7) 초기 확산(소프트웨어 헤리티지에 데이터가 없거나 모든 데이터가 다른 단계에서 필터링되었기 때문에)이 없는 PR을 제거한다.\n' +
      '\n' +
      '파일 필터는 1) 파일이 삭제 또는 추가, 2) 파일 길이가 백만 문자를 초과, 3) 영숫자 문자의 분율이 0.25 미만, 4) 16진수 문자의 분율이 0.25 초과, 5) 최대 줄 수가 100,000을 초과, 6) 평균 줄 길이가 100을 초과, 7) 최대 줄 길이가 1,000을 초과 또는 8) 마크다운에서 비영문 텍스트의 존재를 만족하면 기본 커밋에서 파일을 제거한다.\n' +
      '\n' +
      '제목 및 설명 필터링은 PR을 더 정리하기 위해 다음과 같은 휴리스틱 필터를 적용한다. 우리는 기반이 변경된 PR, 승인되거나 병합되지 않은 PR 및 초기 차이가 없는 PR(소프트웨어 헤리티지에 데이터가 없거나 이전 단계에서 필터링되어 제외됨)을 제외한다. 또한 제목이 10자 미만이거나 \'종속성\', \'종속성\', \'종속성\', \'해제\'라는 단어가 포함된 경우 PR을 배제하고, 설명이 20자 미만이거나 \'Qwiet\'가 포함된 경우 PR을 배제한다.\n' +
      '\n' +
      '입력의 절단은 PR에서 긴 입력 필드를 다음과 같이 단축한다. 제목을 500자로, 설명을 80줄로 잘라 처음 60줄과 마지막 20줄만 표시합니다. 설명 길이가 여전히 1000자를 초과하면 잘립니다.\n' +
      '\n' +
      'GitHub 이슈(SS2.2) 처리 후, 사용자가 이메일 회신을 통해 게시할 때 봇에서 댓글을 제거하고 자동 생성된 텍스트를 삭제한다. 우리는 SS3.2에 기술된 바와 같이 저자의 사용자 이름을 익명화하고, PR 리뷰 코멘트가 아닌 한 20자 미만의 PR에서 코멘트를 제거한다. 코드 검토 주석의 경우 파일 이름과 주석을 유지하면서 10,000자를 초과할 경우 전체 diff hunk를 제거합니다.\n' +
      '\n' +
      'PRs의 다양성을 높이기 위해 PRs를 리포지토리별로 하위 샘플링한다. PR이 1개인 리포지토리의 경우 필터링 후 0.8의 확률로 유지되며 PR이 1,000개인 리포지토리의 경우 이 유지 확률을 0.1로 선형적으로 감소시킨다. 1,000개 이상의 PR을 갖는 리포지토리의 경우, 우리는 100개의 PR만을 보유하도록 보유 확률을 설정한다. 마지막으로, 파일 크기가 전체 기본 파일 크기의 50%를 초과하거나 파일 경로가 키워드 중 하나인 \'pack\', \'lock\', \'yarn\', \'output\',\'swagger\', \'openapi\', \'output\'을 포함할 때 10%의 보유 확률을 갖는 YAML 및 JSON 파일을 하위 샘플로 한다.\n' +
      '\n' +
      '최대 서열 길이는 위에서 언급한 처리 단계 후 데이터 분포를 먼저 조사하여 PR의 최대 서열 길이를 결정한다. 우리는 최대 1M 문자의 3.7M PR을 발견하여 194GB의 데이터를 생성한다. 이는 100K 문자의 한도를 설정하면 3.3M PR로 감소하여 데이터 세트 크기가 67.3GB가 된다. (부록 A.5는 시퀀스 길이 통계에 대한 더 많은 세부 사항을 가지고 있다.) StarCoder2 모델의 경우 최대 100K 문자(대략 25k 토큰으로 번역)를 가진 PR을 포함하기로 결정했다. 4K 토큰의 제한된 컨텍스트로 사전 교육을 하고 있기 때문에 모든 PR이 컨텍스트 창에 맞는 것은 아니다. 그러나 SS5.2에 설명된 대로 diff가 로컬이고 긴 컨텍스트가 필요하지 않도록 PR을 포맷한다.\n' +
      '\n' +
      '### Notebooks\n' +
      '\n' +
      '우리는 소프트웨어 헤리티지 아카이브에서 추출한 주피터 노트북과 캐글 플랫폼에 의해 공개된 노트북이라는 두 가지 별도의 출처의 노트북을 포함한다.\n' +
      '\n' +
      '###### 2.4.1 주피터 노트북\n' +
      '\n' +
      '우리는 StarCoder(Li et al., 2023)와 동일한 파이프라인에 따라 Jupyter 노트북을 스크립트 및 구조화된 노트북으로 변환한다. 한 가지 중요한 차이점은 스타코더에서 텍스트 블록이 제거되는 동안 텍스트 블록의 마크다운 구조를 유지한다는 것이다. 완전성을 위해 이러한 전처리 단계를 아래에 요약한다.\n' +
      '\n' +
      '주피터 - 스크립트는 주피터4를 사용하여 노트북을 스크립트로 변환합니다. 변환 프로세스를 시작하기 위해 주피텍스트는 각 노트북 내에서 특정 프로그래밍 언어를 식별해야 한다. 이 정보는 일반적으로 대부분의 노트북의 메타데이터에서 사용할 수 있습니다. 그렇지 않은 경우 0.5 이상의 확률 임계값을 사용하여 게슬랑 라이브러리5를 사용하여 프로그래밍 언어를 식별한다. 초기 데이터 세트는 1,100만 개의 노트북으로 구성되었으며 그 중 300만 개는 구문 분석 오류로 인해 제외되었다. 거의 중복 제거 후 데이터 세트는 스크립트로 변환된 4백만 개의 노트북으로 축소되었다.\n' +
      '\n' +
      '각주 4: [https://jupytext.readthedocs.io/](https://jupytext.readthedocs.io/]\n' +
      '\n' +
      '각주 5: [https://guesslang.readthedocs.io/](https://guesslang.readthedocs.io/]\n' +
      '\n' +
      '주피터 - 이 데이터 세트를 만들기 위해 먼저 각 노트북의 메타데이터 정보를 사용하여 파이썬 코드나 마크다운 텍스트가 포함되지 않은 노트북을 필터링했다. 메타데이터에서 \'파이썬\'으로 명시적으로 표시된 노트북만 보관되었다. 그런 다음 각 노트북에 대해 연속 마크다운 블록 또는 코드 블록을 각각 단일 마크다운 또는 코드 블록으로 병합했다. 결국, 우리는 각 노트북별로 그룹화된 시간 순서에 따라 연속적인 코드-텍스트 쌍을 갖게 되었다. 각 주피터 코드-텍스트 쌍에는 코드 블록과 파이썬 코드 바로 앞에 있는 마크다운 텍스트가 포함되어 있어 자연스러운 명령어 쌍을 형성한다. 또한 출력 셀이 비어 있지 않은 경우 코드 블록의 포맷된 출력을 포함했으며 그렇지 않은 경우 특수 <empty_output> 토큰으로 표시했다. 연속된 코드 블록들이 병합 전에 다수의 출력 셀들을 갖는 경우, 우리는 마지막 코드 블록의 출력만을 유지한다. 이러한 전처리 단계와 거의 중복 제거 후 4.6M 구조의 주피터 노트북이 완성되었다.\n' +
      '\n' +
      '###### 2.4.2 캐글 노트북\n' +
      '\n' +
      '우리는 3.6M 노트북의 초기 데이터 세트를 시작으로 아파치 2.0 라이센스로 Kaggle 플랫폼6에서 출시한 파이썬 노트북을 포함한다. 이 Kaggle 데이터 세트는 출력 셀을 포함하지 않고 마크다운 및 코드 셀만 포함한다는 점에 유의한다.\n' +
      '\n' +
      '각주 6: [https://www.kaggle.com/datasets/kaggle/meta-kaggle-code](https://www.kaggle.com/datasets/kaggle/meta-kaggle-code)\n' +
      '\n' +
      '클리닝은 100자 미만의 노트북과 구문 오류가 있는 노트북을 드롭하여 데이터 클리닝 프로세스를 시작합니다. 또한 노트북의 시작 부분에서 템플릿 텍스트를 제거합니다(템플릿은 부록 A.7 참조). 이러한 단계는 노트북의 18%를 제거합니다. 다음으로, SS2.4.1에서 Jupyter 공책을 처리한 후, 공책을 구조화 및 스크립트 형식으로 변환하고, SS3.1에서 설명한 파이프라인을 사용하여 거의 중복을 제거하여 공책의 78%를 제거하고 580k 공책을 남긴다.\n' +
      '\n' +
      '데이터셋 디스크립션은 노트북의 내용과 목적에 대한 더 많은 컨텍스트를 모델에 제공하기 위해 이 정보를 사용할 수 있을 때마다 캐글 데이터세트에 대한 메타데이터를 포함한다. 우리는 노트북의 42%가 Kaggle 데이터 세트와 연관되어 있으며 각 노트북의 시작 부분에 제목과 설명을 포함한다는 것을 발견했다.\n' +
      '\n' +
      '데이터 세트 스키마 이러한 고수준의 데이터 세트 설명 외에도 읽기_csv 인스턴스에 대해 노트북 내부의 코드를 스캔했다. 샘플의 25%가 CSV 데이터 세트를 로딩하고 있음을 발견했다. 우리는 다음과 같이 이러한 데이터 세트에 대한 자세한 정보를 추출하고 통합했다. 먼저 Kaggle API를 이용하여 데이터셋을 다운로드하여 노트북의 8.6%를 성공적으로 검색하였다. 나머지 사례는 데이터 세트를 사용할 수 없거나 합리적인 시간 프레임 내에 다운로드하는 문제에 직면했기 때문이다. 다운로드한 데이터셋의 경우, df.info()의 출력을 노트북에 프리픽스하여 컬럼 이름과 그 d타입, non-null 값 카운트 및 메모리 사용량을 표시한다. 또한 데이터 세트에서 4개의 샘플 행을 포함합니다.\n' +
      '\n' +
      '### Documentation\n' +
      '\n' +
      '패키지 관리자로부터의 문서 우리는 npm, PyPI, Go Packages, Packagist, Rubygems, Cargo, CocoaPods, Bower, CPAN, Clojars, Conda, Hex 및 Julia를 포함한 여러 패키지 관리자 플랫폼에서 문서를 탐색한다. 먼저 라이브러리.io에서 다양한 플랫폼에서 가장 인기 있는 라이브러리의 이름을 검색하고, 이러한 라이브러리 이름은 개별 패키지 관리자를 통해 검색함으로써 각 라이브러리에 대한 각 홈 페이지를 얻을 수 있다. 획득한 홈페이지 링크에서 문서 파일을 체계적으로 크롤링하거나 제공된 README 또는 플랫폼의 문서 파일에서 추출한 정보를 체계적으로 크롤링했다. 홈페이지 링크를 통해 얻은 문서의 경우, "웹사이트로부터의 문서화"라는 제목의 단락에 요약된 동일한 처리 전략을 준수합니다. REwang2023 소프트웨어 ADME 또는 플랫폼의 문서 파일에서 문서를 추출할 때, 우리는 간단하고 효과적인 형식을 유지하기 위해 가능한 한 마크다운 형식을 사용하여 텍스트를 추출하기 위해 별개의 휴리스틱을 사용한다. PyPI 및 Conda에서 사용할 수 있는 많은 라이브러리가 일반적으로 보다 포괄적인 문서를 제공하는 Read the Docs에서 관련 문서를 호스팅하고 있다는 점에 주목할 필요가 있다. 따라서 이러한 라이브러리의 주요 문서 소스로 읽기 문서를 사용하는 것을 우선시합니다. 독서 문서에서 호스팅되는 이러한 문서의 경우 "웹 사이트의 문서"라는 문단에 설명된 것과 동일한 처리 절차를 따릅니다.\n' +
      '\n' +
      '패키지 관리자로부터 R 언어와 관련된 문서에 대해 pdftotext 라이브러리를 사용하여 CRAN에서 호스팅된 모든 PDF 파일에서 텍스트를 추출했다.7 이 라이브러리는 코드 스니펫 내의 공간을 포함하여 형식을 보존하는 데 특히 효과적이다. LaTeX 관련 문서의 경우 CTAN에서 LaTeX 패키지의 문서, 튜토리얼 및 사용 가이드 PDF를 추출하고 이미지 무거운 PDF를 필터링하고 누갓 신경 OCR 도구를 사용하여 나머지를 마크다운으로 변환했다.\n' +
      '\n' +
      '각주 7: [https://github.com/jalan/pdftotext](https://github.com/jalan/pdftotext)\n' +
      '\n' +
      '웹사이트의 문서는 표 2에 자세히 설명된 대로 신중하게 선별된 웹사이트 목록에서 코드 문서를 수집한다. 우리는 동일한 도메인 내에 URL을 저장하기 위해 큐를 사용하여 표 2에 나열된 초기 URL에서 웹사이트를 체계적으로 탐색하는 것으로 시작한다. 이 큐는 크롤 중에 새 링크를 찾을 때 동적으로 확장됩니다. 대부분의 문서가 HTML 페이지를 포함한다는 점을 고려하여, 우리는 (1) 콘텐츠 추출 및 (2) 콘텐츠 연결에 처리 파이프라인을 집중한다. 콘텐츠를 추출하기 위해 트라필라투라 라이브러리8을 사용하여 각 HTML 페이지를 XML 형식으로 변환하면서 문서화에서 자주 반복되는 중복 탐색 및 인덱스 바를 동시에 제거합니다. 다음으로 XML-to-Markdown 변환 스크립트를 사용하여 XML 형식을 마크다운으로 변환했다. 두 번째 단계에서는 이러한 문서를 하나의 텍스트로 컴파일하기 위해 먼저 서로 다른 HTML 페이지에서 추출한 콘텐츠의 거의 중복 제거를 수행한다. 이 단계는 특정 문서 페이지가 문서에 대한 유익한 정보 대신 웹사이트 레이아웃(예: 탐색 막대)으로만 구성되어 상당한 양의 중복된 내용을 초래한다는 것을 관찰했기 때문에 필수적이었다. 이를 위해 단일 웹 사이트의 각 HTML 페이지를 클러스터로 처리하고, 0.7의 임계값을 사용하여 유사한 페이지를 식별하고 제거하기 위해 minhash 지역성에 민감한 해싱 기법을 적용하고, 웹 페이지 크롤링 순서에 따라 동일한 웹 사이트의 다른 페이지에서 수집된 콘텐츠를 조립하여 응집력 있는 내러티브를 보장한다. 이것은 "breadth-first search" 접근법과 평행하며, 여기서 현재 깊이의 모든 노드들은 다음 깊이 레벨로 진행하기 전에 탐색된다. 또한, 기존의 웹 크롤링인 **RefinedWeb**(Penedo et al., 2023), **OSCAR**(Ortiz Suarez et al., 2019), **esCorpius**(Gutierrez-Fandino et al., 2022)에서 코드 관련 데이터를 수집하였다. 우리는 정규식을 사용하여 문서 내에서 프로그래밍 언어별 구성을 식별하고 페이지 URL에서 "문서" 부분 문자열을 감지한다. 결과 데이터 세트는 주로 프로그래밍 블로그, 코딩 튜토리얼 및 위에서 수집된 문서를 제외하고 독서와 같은 플랫폼에서 조달되는 콘텐츠로 구성된다.\n' +
      '\n' +
      '무료 교과서는 무료 프로그래밍 전자책 보급 촉진을 목표로 하는 무료 프로그래밍 책 프로젝트에서 편찬된 무료 프로그래밍 책을 긁어모았다. 먼저 모든 링크를 추출하고 PDF 확장자를 식별합니다. 그 후 사용 가능한 모든 PDF 파일을 다운로드하고 pdf2텍스트 라이브러리를 활용하여 이러한 PDF 파일에서 텍스트를 추출했다. 마지막으로 영어, 중국어, 일본어, 스페인어 등 다양한 지역에 걸쳐 있는 3,541권의 책을 구문 분석했다.\n' +
      '\n' +
      '마지막으로 각 문서에서 사용하는 주요 프로그래밍 언어를 식별하기 위해 이중 접근 방식을 사용했다. 우리는 문서의 출처가 특정 프로그래밍 언어에 명확하게 대응할 때 미리 정의된 규칙을 활용하고 그러한 대응이 명시적이지 않은 경우 추측 랭9 라이브러리에 의존한다. 결과적인 프로그래밍 언어 분포는 그림 2에 그래픽으로 표시된다.\n' +
      '\n' +
      '각주 9: [https://github.com/yoeo/guesslang](https://github.com/yoeo/guesslang)\n' +
      '\n' +
      '### Intermediate Representations\n' +
      '\n' +
      '우리는 저자원 프로그래밍 언어에 대한 모델의 이해를 높이기 위해 중간 표현(IR)을 짝지어 소스 코드를 증강한다. 이 접근법의 핵심 근거는 공유된 중간자가 있다는 것이다.\n' +
      '\n' +
      '그림 2: 크롤링된 문서 모음에서 상위 20개 프로그래밍 언어의 배포.\n' +
      '\n' +
      'representresentation은 저-자원 구성들을 고-자원 언어들에서 유사한 구성들에 앵커하는 데 도움이 될 수 있다(Zhuo et al., 2023b).\n' +
      '\n' +
      'LlvmWe는 GitHub에서 광범위한 가용성으로 인해 LLVM(Lattner and Adve, 2004)을 중간 표현으로 선택하여 언어의 의미를 학습하기에 충분한 학습 데이터가 있을 확률을 높인다. 또한, LLVM은 IR로 널리 채택되었으며 여러 프로그래밍 언어에 걸쳐 많은 컴파일러 프론트엔드의 대상 표현이다.10\n' +
      '\n' +
      '각주 10: [https://llvm.org/ProjectsWithLLVW/](https://llvm.org/ProjectsWithLLVW/)\n' +
      '\n' +
      '자유 형식 소스 코드로부터 IR을 추출하려는 기존의 시도들은 낮은 컴파일 성공률(Szafraniec et al., 2023)로 고통받거나(Grossman et al., 2023) 의존 코드를 추적하기 위해 맞춤 언어-특정 메커니즘들을 사용한다. 우리는 이것을 프로그래밍 워드 문제(Rosetta Code, 2023; Mirzayanov, 2020; Puri et al., 2021; Caballero et al., 2016)에 대해 수용 솔루션으로부터 자체 포함 컴파일 유닛을 소싱함으로써 회피한다. C++, C, Objective-C, Python, Rust, Go, Haskell, D, Fortran, Swift, Nim에 걸쳐 총 4M 소스의 크기를 최적화(-O2 등가)하고 성능을 최적화(-O3 등가) 모드로 컴파일한다. 우리는 컨텍스트 길이 고려 사항으로 인해 대부분의 쌍에서 크기에 최적화된 IR을 사용하도록 선택한다. 그러나 20%의 쌍에 대해 성능 최적화된 IR을 사용한다. 이는 모델이 이러한 형태일 가능성이 더 높은 야생에서 LLVM 코드를 보는 사전 훈련 단계에서 전송을 최대화하기 위해 수행된다. C++, C 및 Objective-C 컴파일에는 clang11, Python 컴파일에는 코돈12, Rust 컴파일에는 rustc13, Go 컴파일에는 gollvm14, Haskell 컴파일에는 ghc15, D 컴파일에는 ldc16, Fortran 컴파일에는 flang17, Nim 컴파일에는 nlvm18을 사용한다. 우리는 IR의 불필요한 플랫폼, 벤더 및 메모리 레이아웃별 정보와 함께 헤더를 청소한 후 원본과 페어링합니다.\n' +
      '\n' +
      '각주 11: [https://clang.llvm.org/](https://clang.llvm.org/)\n' +
      '\n' +
      '각주 12: [https://docs.exalocap.io/codon](https://docs.exalocap.io/codon)\n' +
      '\n' +
      '각주 13: [https://www.rust-lang.org/](https://www.rust-lang.org/)\n' +
      '\n' +
      '각주 14: [https://go.googlesource.com/gollvm/](https://go.googlesource.com/gollvm/)\n' +
      '\n' +
      '각주 15: [https://www.haskell.org/ghc/](https://www.haskell.org/ghc/)\n' +
      '\n' +
      '각주 16: [https://wiki.dlang.org/LDC](https://wiki.dlang.org/LDC)\n' +
      '\n' +
      '각주 17: [https://flang.llvm.org/docs/](https://flang.llvm.org/docs/)\n' +
      '\n' +
      '각주 18: [https://github.com/arnetheduck/nlvm](https://github.com/arnetheduck/nlvm)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Website Name** & **URL** \\\\ \\hline DevDocs API Documentation & [https://devdocs.io](https://devdocs.io) \\\\ MDN Web Docs & [https://developermozilla.org](https://developermozilla.org) \\\\ TensorFlow Docs & [https://www.tensorflow.org](https://www.tensorflow.org) \\\\ Linux Docs & [https://www.kernel.org/doc/Documentation](https://www.kernel.org/doc/Documentation) \\\\ Swift Programming Language & [https://docs.sawift.org/sawift-book/documentation/the-swift-programming-language](https://docs.sawift.org/sawift-book/documentation/the-swift-programming-language) \\\\ Flutter API Reference & [https://api.flutter.org/](https://api.flutter.org/). \\\\ TypeScript & [https://www.typescriptlang.org/docs/handbook](https://www.typescriptlang.org/docs/handbook) \\\\ Json.NET Documentation & [https://www.newntonsoft.com/json/help/html](https://www.newntonsoft.com/json/help/html) \\\\ NVIDIA Documentation Hub & [https://docs.nvidia.com](https://docs.nvidia.com) \\\\ Oracle Java Tutorial & [https://docs.oracle.com/jayase/tutorial/java](https://docs.oracle.com/jayase/tutorial/java) \\\\ Qiskit Documentation & [https://qiskit.org/documentation](https://qiskit.org/documentation) \\\\ Q\\# Quantum Programming & [https://learn.microsoft.com.en-us/azure/quantum/user-guide](https://learn.microsoft.com.en-us/azure/quantum/user-guide) \\\\ Pony Tutorial & [https://tutorial.ponylang.io](https://tutorial.ponylang.io) \\\\ Zephir Documentation & [https://docs.zephir-lang.com/0.12/en/introduction](https://docs.zephir-lang.com/0.12/en/introduction) \\\\ Qemu Documentation & [https://www.pemu.org/documentation](https://www.pemu.org/documentation) \\\\ C\\# Documentation & [https://learn.microsoft.com/en-us/dotnet/csharp](https://learn.microsoft.com/en-us/dotnet/csharp) \\\\ Huugging Face Documentation & [https://huggingface.co/docs](https://huggingface.co/docs) \\\\ LLVM Doc & [https://llvm.org/docs](https://llvm.org/docs) \\\\ GCC Online Documentation & [https://geo.gnu.org/onlinedocs](https://geo.gnu.org/onlinedocs) \\\\ Matlab Documentation & [https://www.mathworks.com/help/matlab](https://www.mathworks.com/help/matlab) \\\\ Boost C++ Libraries & [https://www.boost.org/doc](https://www.boost.org/doc) \\\\ Maxima Manual & [https://maxima.sourceforge.io/docs/manual/maxima](https://maxima.sourceforge.io/docs/manual/maxima)\\_singlepage.html \\\\ Qt Documentation & [https://doc.qt.io](https://doc.qt.io) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 코드 문서 데이터 세트에 대해 웹 사이트를 긁어냈다.\n' +
      '\n' +
      '### Lhq19\n' +
      '각주 19: 르안드로의 고품질 데이터 세트\n' +
      '\n' +
      '각주 20: [https://huggingface.co/sentence-transformers/all-MiniM-L12-v2](https://huggingface.co/sentence-transformers/all-MiniM-L12-v2)\n' +
      '\n' +
      '우리는 수학과 코딩을 위한 몇 개의 작은 고품질 데이터 세트를 포함한다:\n' +
      '\n' +
      '***APPS(train)**(Hendrycks et al., 2021)는 5,000개의 예의 기차 세트를 갖는 파이썬에서 인기 있는 텍스트2코드 벤치마크이다. 프로그래밍 문제당 하나의 솔루션이 포함되어 있습니다.\n' +
      '* **Code Contest**(Li et al., 2022)는 APPS와 유사하지만 여러 프로그래밍 언어, 즉 Python 2/3, C++, Java의 솔루션을 포함한다. 문제 및 언어당 하나의 솔루션을 포함하고 13k+ 예제의 데이터 세트에 도달합니다.\n' +
      '**GSM8K(train)**(Cobbe et al., 2021)는 LLMs의 수학 추론 능력을 테스트하기 위한 인기 있는 평가 벤치마크인 GSM8K의 열차 분할이다. 데이터 세트는 7k+ 예제로 구성된다.\n' +
      '***GSM8K(SciRel)**(Yuan et al., 2023)는 GSM8K에서 질문들에 대한 대안적인 추론 경로들을 포함하는 GSM8K의 증강된 버전이다. 확장 버전에는 110k의 예가 들어 있습니다.\n' +
      '**Deepmind Mathematics**(Saxton et al., 2019)는 다양한 영역(대수, 산술, 미적분, 비교, 측정, 숫자, 다항식, 확률) 및 다양한 난이도(쉬운-중간-하드)에 걸친 수학 질문 및 답변의 합성 데이터세트이다. 데이터 세트는 110M+(짧은) 예제로 구성됩니다.\n' +
      '* **Rosetta Code**(Rosetta Code, 2023; Nanz & Furia, 2015)는 가능한 많은 다른 프로그래밍 언어로 솔루션이 있는 1100개 이상의 일상적인 프로그래밍 작업을 가진 데이터 세트이다.\n' +
      '***MultiPL-T**(Cassano et al., 2023a)는 추출된 Python 함수를 자동으로 번역하여 단위 테스트로 검증하는 것을 기반으로 Lua, Racket, OCaml의 고품질 데이터이다. 총 데이터 세트는 200k개 이상의 예를 포함한다.\n' +
      '**Proofsteps**는 모델의 Lemma 패밀리를 훈련시키는데 사용되는 데이터셋인 AlgebraicStack(Azerbayev et al., 2024)의 일부이다. 또한, 수학립 4(mathlib Community, 2020)에서 추출한 _proofsteps-lean_와 PISA 데이터셋의 상단에 구축한 _proofsteps-isabelle_도 포함한다(Jiang et al., 2021). 증거-lean은 3k개 이상의 예를 포함하는 반면, 증명-이사벨은 250k개 이상의 예를 포함한다.\n' +
      '\n' +
      '기타 자연어 데이터 세트\n' +
      '\n' +
      '스택 오버플로우 우리는 2023-09-14년(스택 익스체인지 아카이브, 2024)의 스택 오버플로우 덤프에서 1,100만 개의 질문과 그에 상응하는 다중 응답을 포함한다. 우리는 3개 미만의 대답으로 질문을 걸러냈다. 데이터 세트를 검사한 결과 스택 오버플로우 덤프의 고유한 형식 오류로 인해 질문과 답변 간의 불일치가 많이 발견되었다. 우리는 Llama-2-70b-chat-hf(Touvron et al., 2023)를 활용하여 다음과 같이 데이터셋의 품질을 높였다. 우리는 20,000개의 예를 선택하고 Llama-2-70b-chat-hf에게 질문-답변 쌍을 평가하도록 요청했다. 정확한 프롬프트는 부록 A.6을 참조하십시오. 다음으로, 우리는 1만 개의 가장 높은 점수를 받은 쌍을 긍정적인 예로 선택하고 나머지 1만 개의 답을 사용하여 다른 질문과 무작위로 짝을 지어 부정적인 예를 만든다. 이 데이터셋을 이용하여 문장 임베딩 모델(sentence-transformers/all-MiniM-L12-v220(Reimers & Gurevych, 2019; Muennighoff et al., 2022))로 질의 응답을 임베딩하고, 이들 사이의 코사인 거리를 최소화하여 이진 분류기를 학습한다. 다음으로, 질문-답변 쌍의 하위 집합에 대한 임베딩 점수를 플롯하고 임계값을 수동으로 0.1로 결정하며, 질문이 여러 답변을 가질 수 있기 때문에 질문-답변 쌍의 점수를 평균화하고 평균 점수가 0.1 미만인 모든 질문을 제거하면 1140만 개의 질문과 10B 이상의 토큰이 생성된다.\n' +
      '\n' +
      'ArXivWe는 RedPajama 데이터세트(Together Computer, 2023)의 ArXiv 서브세트를 포함한다. 이 데이터 세트는 공개적으로 이용 가능한 아마존 S3 버킷(Arxiv, 2024)에서 다운로드된다. 우리는 라텍스 소스 파일을 유지하고 이러한 파일에서 프리앰블, 주석, 매크로 및 서지 문서를 제거하기 위해 데이터 세트를 추가로 처리했다. 최종 데이터 세트는 대략 30B 토큰이다.\n' +
      '\n' +
      '위키피디아 우리는 위키피디아의 영어 부분집합을 포함한다. 구체적으로, 우리는 2023-03-20 덤프에서 파생된 RedPajama(RedPajama Wiki, 2024)가 수집한 버전을 사용한다. 우리는 RedPajama의 처리 단계를 따르고 위키피디아 페이지에서 하이퍼링크와 템플릿을 제거한다. 전체 데이터 세트는 약 60억 개의 토큰으로 구성된다.\n' +
      '\n' +
      'OpenWebMathWe는 CommonCrawl로부터 추출된 고품질의 수학적 텍스트의 오픈 데이터세트인 OpenWebMath(Paster et al., 2023)를 포함한다. 전체 데이터 세트는 거의 15B 토큰을 포함한다.\n' +
      '\n' +
      '##3 전처리 파이프라인\n' +
      '\n' +
      '이전 섹션에서 설명한 데이터 소스에 중복 제거(SS3.1), PII 수정(SS3.2), 벤치마크 오염 제거(SS3.3), 악성 프로그램 제거(SS3.4), 옵트아웃 삭제 요청(SS3.5)과 같은 여러 전처리 단계를 적용한다. 모든 단계가 각 데이터 소스에 적용되는 것은 아니므로 표 3에서 데이터 소스당 전처리 파이프라인을 요약한다.\n' +
      '\n' +
      '### Removing Near-Duplicates\n' +
      '\n' +
      '소스 코드, 풀 요청, 노트북, 문제 및 문서를 복제합니다. 우리는 Arxiv, StackExchange, OpenWebMath, Wikipedia와 같은 이미 전처리된 자연 언어 데이터 세트와 작은 고품질 수학 및 추론 데이터 세트를 중복하지 않는다.\n' +
      '\n' +
      '우리는 SantaCoder(Ben Allal et al., 2023)의 중복 제거 파이프라인을 따랐다. 이 프로세스는 먼저 모든 코드 파일의 MinHash(Broder, 2000)를 계산한 다음 MinHash 지문을 기반으로 그룹 파일에 LSH(Localally Sensitive Hashing)를 활용한다. LSH 단계에서 "유사" 파일은 동일한 버킷에 할당되어 중복으로 식별됩니다. 각 중복 그룹에서 하나의 파일만 선택됩니다. 산타코더 접근법 외에도 리포지토리 컨텍스트를 보존하기 위해 별 및 포크 수가 더 높은 리포지토리 또는 최신 커밋 날짜의 파일을 타이브레이커로 우선시한다. 5-grams와 0.7의 Jaccard 유사도를 사용하였으며, 중복제거 파이프라인에 대한 더 많은 배경정보는 블로그 포스트를 참조하였다.\n' +
      '\n' +
      '### PII Redaction\n' +
      '\n' +
      '학습 데이터에 존재하는 개인 식별 정보(PII)의 재분배 가능성을 줄이기 위해, 우리는 학습 세트에서 PII를 수정하기 위해 부지런한 노력을 기울인다. 우리는 주로 StarCoder(Li et al., 2023)의 단계를 따르고 StarPII 모델을 활용하여 다양한 PII 엔티티를 수정한다. 아래에서는 각 데이터 소스에 적용하는 방법에 대해 자세히 설명합니다.\n' +
      '\n' +
      'PII 개체를 수정하기 위해 StarPII를 사용하여 소스 코드, 풀 요청, 문제 및 스택 오버플로우에서 이름, 전자 메일, 키, 암호, IP 주소 및 사용자 이름을 수정합니다. 우리는 StarCoder paper (Li et al., 2023)에 기술된 모델 또는 재액션 로직에 어떠한 수정도 하지 않는다. OpenWebMath 및 문서의 경우 이름, 키 및 이메일만 수정하고, ArXiv에 대한 이메일은 Ben Allal 등(2023)에서 설명한 regex를 사용하여 수정한다.\n' +
      '\n' +
      '이슈, 풀 요청 및 스택오버플로우에서 사용자 이름을 재작성하는 것은 종종 메시지 스레드에 사용자 이름을 포함합니다. 우리는 두 번째 참가자를 나타내기 위해 사용자 이름_1과 같이 대화에 특정한 참가자 카운터로 대체함으로써 저자 사용자 이름을 익명화한다. 이러한 가명은 화자의 정체성을 유지하기 위해 각 코멘트의 시작 부분에 추가된다. 또한 메시지에서 이러한 사용자 이름에 대한 참조가 제거됩니다. 대화에 적극적으로 참여하는 개인의 사용자 이름만 마스킹되고, 참여하지 않는 사용자에 대한 언급은 영향을 받지 않고 있다.\n' +
      '\n' +
      '### Decontamination\n' +
      '\n' +
      '스타코더의 성능이 테스트 벤치마크에서 인위적으로 부풀려지지 않도록 하기 위해 테스트 세트에서 훈련 세트의 오염을 제거합니다. 구체적으로 HumanEval 및 MBPP의 docstrings 또는 solution, APPS의 docstrings, GSM8K의 질문 또는 DS1000의 prompts를 포함하는 파일들을 제거한다. StarCoder의 첫 번째 반복(Li et al., 2023)과는 대조적으로, 스트링 매칭 동안 화이트 스페이스를 제거함으로써 오염 제거 프로세스의 리콜을 더욱 향상시킨다. 이 오염 제거 단계에서 문서, LHQ, arXiv 및 위키피디아를 제외한다.\n' +
      '\n' +
      '### Malware Removal\n' +
      '\n' +
      '우리는 소스 코드, 풀 요청, 노트북 및 문제에서 악성 프로그램의 가능한 인스턴스를 식별하기 위해 교육 세트를 스캔합니다. 이를 위해, 2023-11-16년 현재 SaneSecurity(Sane Security, 2024)에서 발표한 비공식 악성코드 시그니처를 추가적으로 추가한 ClamAV 1.2(ClamAV, 2024)를 사용하고, False Positives(SaneSecurity에 의해 결정된 바와 같이)의 위험성이 높은 시그니처는 사용하지 않았다. 필터링되지 않은 코드 데이터 세트에서 가장 자주 탐지되는 악성 프로그램 서명에 대해서는 표 26을 참조하십시오. 요약하면, 이 단계는 데이터 세트에서 59,442개의 파일을 제거하여 654M 파일 중 0.009%만 구성한다.\n' +
      '\n' +
      '### Removing Opt-outs\n' +
      '\n' +
      '우리는 X21에서 StarCoder2의 교육운영을 발표하고 스택({}^{\\star}\\) 거버넌스 툴에 있는 \\({}^{*}\\)Am I을 스택 v2의 새로운 리포지토리로 업데이트하였다. 개발자는 2023년 11월 20일까지 옵트아웃 요청을 제출하도록 허가받았다. 종료 날짜 이후 91명의 사용자 및 조직과 관련된 1,561개의 리포지토리를 제거했다. 총 22,066개의 파일이 소스 코드 데이터 세트(문제 및 PR 제외)에서 제거되었다.\n' +
      '\n' +
      '각주 21: [https://x.com/BigCodeProject/status/17215838975802492547s+20](https://x.com/BigCodeProject/status/17215838975802492547s+20)\n' +
      '\n' +
      '##4 데이터 구성\n' +
      '\n' +
      '사용 가능한 훨씬 더 큰 훈련 세트를 사용하여 모델 용량을 각 모델 크기에 맞게 데이터 구성을 조정하기로 결정했습니다. 우리는 제한된 용량을 가진 더 작은 모델이 덜 다양한 데이터 세트에 노출되어야 한다고 추론한다. 이러한 직관은 언어들이 모델 용량을 놓고 경쟁한다는 것을 보여주는 다국어 NLP에서의 연구에 의해 뒷받침된다(Arivazhagan et al., 2019; Conneau et al., 2020; Scao et al., 2022). 따라서 먼저 널리 사용되는 17개 프로그래밍 언어의 하위 집합을 선택하여 SWH 코드 데이터 세트의 더 작은 버전을 생성한다. 우리는 이 변형을 사용하여 3B 및 7B 모델을 훈련하는 반면 15B 모델에는 619개의 모든 프로그래밍 언어가 있는 풀 버전을 사용한다. 3B 모델에 대한 훈련 세트의 다양성을 추가로 제한하기 위해 일부 자연어 데이터 세트("모델 크기당 데이터 구성" 참조)도 제외한다.\n' +
      '\n' +
      '스타코더베이스와 유사한 다운샘플링 언어는 가능한 한 데이터의 자연스러운 분포를 고수한다. 소스 코드 데이터 세트를 구성하기 전에 프로그래밍 언어 간의 데이터 분포를 조사했다. 스타코더베이스와 비교했을 때, 우리는 고자원 언어들 사이에서 약간 더 큰 변이를 발견했다. 관측 데이터 볼륨(GB)은 Java(479.68), JavaScript(277.25), C++(204.49), Python(190.99), PHP(171.57), C#(166.22), C(114.49)와 같다. 우리는 이러한 고자원 언어를 보다 동등한 위치에 놓기 위해 자바와 자바스크립트를 모두 200GB로 다운샘플링하기로 결정했다. 나아가, 우리는\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l} \\hline \\hline\n' +
      '**Dataset** & **Dedup** & **Malicious Code** & **Decontaminate** & **Opt-out** & **PII** \\\\ \\hline Source Code & Yes & Yes & Yes & Yes & StarPII \\\\ Pull Requests & Yes & Yes & Yes & Yes & StarPII + Usernames \\\\ Jupyter/Kaggle Notebooks & Yes & Yes & Yes & Yes/No & StarPII \\\\ Issues & Yes & Yes & Yes & Yes & StarPII + Usernames \\\\ Docs & Yes & No & No & No & StarPII: Names, Keys, Emails \\\\ LHQ & No & No & No & No & No \\\\ Arxiv & No & No & No & Email \\\\ OpenWebMath & No & No & Yes & No & StarPII: Names, Keys, Emails \\\\ Wikipedia & No & No & No & No & No \\\\ StackExchange & No & No & Yes & No & StarPII + Usernames \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 각 데이터 소스에 적용되는 데이터 처리 단계의 개요.\n' +
      '\n' +
      'HTML의 크기를 100GB로 줄이면서 254GB의 마크다운 데이터를 보존했습니다. 이 결정은 마크다운이 더 많은 코드 문서를 포함할 가능성이 있는 반면 HTML은 일반적으로 웹 페이지와 연관될 것이라는 예상에서 비롯되었다. 마지막으로 JSON, XML 및 YAML과 같은 데이터 파일을 8GB로 서브샘플링하고 다른 몇 가지 데이터 형식을 1GB로 서브샘플링했다. 서브샘플링된 언어의 전체 목록은 부록 C.2의 표 28을 참조하십시오.\n' +
      '\n' +
      '리포지토리-컨텍스트는 일부 프로그래밍 언어를 서브샘플링한 후, 소프트웨어 헤리티지의 소스 코드를 리포지토리-컨텍스트 인식 데이터 세트로 컴파일한다. 데이터세트의 각 예는 랜덤 순서로 정렬된 파일이 있는 전체 저장소입니다. 이전에 언급한 바와 같이, 후속 단락에 더 자세히 설명된 대로 SWH 데이터 세트의 두 가지 버전인 스택-v2-트레인-스몰 및 스택-v2-트레인-풀을 생성한다.\n' +
      '\n' +
      '스택-v2-트레인-스몰 소형 변형을 위해 널리 사용되는 17개의 프로그래밍 언어를 선택하고 문서화 및 구성 언어의 큐레이트 세트를 포함한다.\n' +
      '\n' +
      '* 구체적으로, 다음과 같은 프로그래밍 언어를 포함한다:\n' +
      '* C\\(-\\) Kotlin \\(-\\) Rust\n' +
      '* C#\\(-\\) Lua\\(-\\) SQL\n' +
      '* C++\\(-\\) PHP\\(-\\) Shell\n' +
      '* Go \\(-\\) Python \\(-\\) Swift\n' +
      '* Java \\(-\\) R \\(-\\) TypeScript\n' +
      '* 자바스크립트\\(-\\) 루비\n' +
      '* 및 코드 설명서와 연관된 다음 언어를 통합한다:\n' +
      '* AscilDoc\\(-\\) RDoc\\(-\\) Text\n' +
      '* HTML \\(-\\) RMarkdown \\(-\\) reStructuredText\n' +
      '* 우리는 또한 부록 C.1에 나열한 몇 가지 구성 언어와 파일을 포함한다.\n' +
      '* 언어를 이 하위 집합으로 제한함에도 불구하고, 우리는 525B+ 고유 토큰의 데이터 세트를 얻는다.\n' +
      '\n' +
      '스택-v2-트레인-풀 풀 변형에는 619개의 프로그래밍 언어가 모두 포함됩니다. 이 하위 집합은 언어 다양성을 크게 향상시키지만(600+ 프로그래밍 언어를 추가), 데이터 세트에 약 250B 토큰만 기여하여 775B+ 토큰으로 절정에 달한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l} \\hline \\hline \\multicolumn{2}{c}{**Dataset**} & \\multicolumn{1}{c}{**Tokens (B)**} & \\multicolumn{1}{c}{**3B**} & \\multicolumn{1}{c}{**7B**} & \\multicolumn{1}{c}{**15B**} \\\\ \\hline \\multirow{6}{*}{\\begin{tabular}{} \\end{tabular} } & the-stack-v2-train-smol & 525.5 & ✓ & ✓ & ✗ \\\\  & the-stack-v2-train-full & 775.48 & ✗ & ✗ & ✓ \\\\ \\hline \\multirow{6}{*}{\n' +
      '\\begin{tabular}{} \\end{tabular} } & Pull requests & 19.54 & ✓ & ✓ & ✓ \\\\  & Issues & 11.06 & ✓ & ✓ & ✓ \\\\  & Jupyter structured & 14.74 & ✓ & ✓ & ✓ \\\\  & Jupyter scripts & 16.29 & ✓ & ✓ & ✓ \\\\  & Kaggle scripts & 1.68 & ✓ & ✓ & ✓ \\\\  & Documentation & 1.6 & ✓ & ✓ & ✓ \\\\  & OpenWebMath & 14.42 & ✗ & ✓ & ✓ \\\\  & Wikipedia & 6.12 & ✗ & ✓ & ✓ \\\\  & StackOverflow & 10.26 & ✓ & ✓ & ✓ \\\\  & Arxiv & 30.26 & ✗ & ✓ & ✓ \\\\  & LHQ & 5.78 & ✓ & ✓ & ✓ \\\\  & Intermediate Repr. & 6 & ✓ & ✓ & ✓ \\\\ \\hline \\multicolumn{2}{c}{Unique tokens (B)} & \\multicolumn{1}{c}{622.09} & 658.58 & 913.23 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: StarCoder2 모델의 데이터 구성에 대한 개요. 우리는 3B 모델의 훈련 세트를 스택-v2-트레인-3B라고 한다.\n' +
      '\n' +
      '표 4에서 모델 크기당 데이터 구성은 3B, 7B 및 15B 모델에 대한 데이터 구성을 요약한다. 우리는 SWH에서 얻은 소스 코드를 제외하고 StarCoder2에 대해 수집된 모든 보충 소스를 나타내기 위해 스택-v2-열차-엑스트라를 사용한다. 3B의 경우, 스택-v2-트레인-스몰을 사용하고 SS2의 추가 데이터 소스에서 OpenWebMath, Wikipedia 및 Arxiv를 제외하여 622B+ 고유 토큰의 데이터 세트를 생성한다. 7B의 경우 OpenWebMath, 위키피디아 및 Arxiv를 포함하여 658B+ 고유 토큰의 약간 더 큰 데이터 세트를 생성한다. 15B의 경우 SS2에 나열된 스택-v2-트레인-풀 데이터세트와 모든 추가 데이터 소스를 포함하여 913B+ 고유 토큰이 있는 데이터세트를 생성한다. 이 데이터셋의 크기는 StarCoderBase를 위한 훈련 데이터셋의 크기인 4\\(\\times\\)이다.\n' +
      '\n' +
      '##5 데이터 포맷팅\n' +
      '\n' +
      '우리는 아래 각 데이터 소스에 대한 포맷팅 지침을 제시한다. 우리는 아래 템플릿에 \\(\\langle\\)token\\(\\rangle\\)은 센티넬 토큰을, 메타데이터와 데이터는 데이터 필드의 자리 표시자를 각각 나타낸다.\n' +
      '\n' +
      '### Source Code\n' +
      '\n' +
      '리포지토리 이름과 파일 경로를 코드 파일의 컨텍스트에 미리 지정합니다. 우리는 이 정보 없이 모델이 작동할 수 있도록 50% 확률로 이 메타데이터를 추가할 뿐이다. 리포지토리 이름과 파일 경로를 추가할 때 다음 형식을 사용합니다.\n' +
      '\n' +
      '<repo_name>reponame<file_sep>filepath1\\ncode1<file_sep>filepath2\\ncode2... <|endoftext|>.\n' +
      '\n' +
      '우리는 이 메타 데이터를 포함하지 않을 때 다음 형식을 사용한다:\n' +
      '\n' +
      '<file_sep>code1<file_sep>code2... <|endoftext|>.\n' +
      '\n' +
      '리포지토리-contextStarcoder1은 파일-context, 즉 랜덤 파일이 컨텍스트 창에 결합되는 설정으로 훈련되었다. 이 작업에서는 동일한 리포지토리의 파일이 함께 그룹화되는 리포지토리 컨텍스트를 사용하여 교육을 탐색합니다. 리포지토리 내에서 파일을 그룹화하기 위한 다양한 방법을 고려했지만, 궁극적으로 동일한 리포지토리 내에서 무작위 순서로 정렬했다.\n' +
      '\n' +
      'FimTo enable the model to perform code infilling task, we applied the fill-in-the-middle transformation (FIM; Bavarian et al., 2022) to the source code. 예비 실험에서 여러 FIM 변형을 탐구하는 동안 StarCoder2 모델에서 repo-context 파일 수준 FIM을 선택했다. 이 FIM 변형에서 리포지토리는 FIM의 후보가 될 확률이 50%로 선택된다. 선택된 리포지토리 예들은 <|endoftext|> 및 <file_sep> 토큰들에 의해 분할된다. 다음으로 50% 확률로 각 청크에 FIM 변환을 적용한다. 리포지토리 메타데이터(<repo_name>reponame)에는 FIM을 적용하지 않는다. 아래에서는 두 번째 소스 파일에만 적용되는 경우 FIM 형식의 예를 제공합니다.\n' +
      '\n' +
      '<repo_name>reponame<file_sep>filepath0\\ncode0<file_sep><film_prefix>filepath1\\ncode1_pre<film_suffix>code1_suf+fim_middle>code1_mid<file_sep>...<|endoftext|>\n' +
      '\n' +
      '### Pull Requests\n' +
      '\n' +
      '잠재적으로 긴 코드 변경 및 주석의 시퀀스를 컴팩트하게 표현하는 것을 목표로 하기 때문에 풀 요청을 포맷하는 것은 어렵습니다. 풀 요청의 긴 입력 필드를 제거하고 잘린 방법에 대한 자세한 내용은 SS2.3을 참조합니다. 여기서는 PR을 LLM이 소비할 수 있는 구조화된 형식으로 렌더링하는 방법에 중점을 둔다.\n' +
      '\n' +
      '기본 커밋의 일부 파일의 경우 0.2 확률로 전체 파일을 포함하고 그렇지 않으면 PR.22의 모든 커밋 헤드에 걸쳐 기본 파일의 변경 범위를 표시한다. 변경 전후에 무작위로 32줄까지 추가한다.\n' +
      '\n' +
      '각주 22: 우리는 모든 커밋에서 파일 라인 변경의 결합을 취하며, 우리는 파일의 이전 상태와 이후 상태 사이의 수정을 표시하기 위해 diff hunk를 사용하여 변경 사항이 합리적으로 로컬화되도록 한다. 또한, diff 헌크 내에서 특정 변경 전후에 무작위로 선택된 3-10개의 컨텍스트 선을 통합한다.\n' +
      '\n' +
      'PR 포맷을 다음과 같이 구조화한다. 첫 번째 블록은 제목, 설명 및 완전한 기본 파일 또는 수정된 파일을 제공합니다. 이어서, 우리는 첫 번째 머리 디프 헌크 세트의 개요를 설명한다:\n' +
      '\n' +
      '```\n' +
      '<pr>Title:title>unusername_0:description<pr_status>opened<repo_name>reponame<pr_base><pr_file>filepath_1<pr_base_code>file_content/changes_1...<pr_file>filepath_N<pr_base_code>file_content/changes_N<pr_diff><pr_file>filepath_1<pr_diff><pr_file>filepath_1<pr_diff><pr_diff><pr_diff>filepath_1<pr_diff><pr_diff_hunk>diff_hunk_1...<pr_diff_hunk>diff_hunk_K...<pr_diff_hunk>diff_hunk_K... <pr_file>filepath_M<pr_diff_hunk>diff_hunk_1...<pr_diff_hunk>diff_hunk_J\n' +
      '```\n' +
      '\n' +
      '두 번째 블록은 PR에서 새로운 헤드 커밋마다 반복되어 일반 코멘트, 리뷰 코멘트 및 코드 리뷰 코멘트를 포함한다. 블록은 논의와 논평의 결과를 반영하여 당김 요청 기반과 새 머리 사이의 디프 헌크로 마무리한다. 사용자가 풀 요청을 닫고 다시 여는 것도 가능합니다. 기쓰브 문제와 마찬가지로 대화 내의 참가자 카운터(예: 사용자 이름_1)에 의해 저자를 참조하여 문제의 두 번째 참가자를 나타낸다.\n' +
      '\n' +
      '``<pr_comment>username_id:comment<pr_event_id>comment_id... <pr_review>username_id:review_comment><pr_event_id>review_id<pr_review_state>[승인, 거부, 코멘트, changes_required]... <pr_review_comment><pr_event_id>comment_id<pr_in_reply_to_review_id>review_id(opt)<pr_in_reply_to_comment_id>comment_id(opt)<pr_file>filepath<pr_diff_hunk_comment_line>line_number<pr_diff_hunk>diff_hunk_content<pr_comment>username_id:comment........... <pr>username_id<pr_status>closed<pr_is_merged>False... <pr>Title:title_unsername_id:description<pr_status>[open, reopen, edited...] <pr_file>filepath_1<pr_diff_hunk>diff_hunk_1...<pr_diff_hunk>diff_hunk_K... <pr_file>filepath_M<pr_diff_hunk>diff_hunk_1...<pr_diff_hunk>diff_hunk_J PR이 닫혔을 때 다음의 최종 블록만을 추가한다.\n' +
      '\n' +
      '<pr>username_id<pr_status>closed<pr_is_merged>True<|endoftext|>\n' +
      '\n' +
      '### GitHub Issues\n' +
      '\n' +
      '우리는 이슈의 시작을 표시하기 위해 센티넬 토큰을 사용하고 그 제목을 포함합니다. 우리는 주석의 시퀀스를 <issue_comment> 토큰으로 분리하고 주석 앞에 익명화된 화자 식별자를 포함한다. 특히, 우리는 이슈의 두 번째 참가자를 지칭하기 위해 대화 내의 참가자 카운터(예: username_1)에 의해 작성자를 지칭한다. 서로 다른 턴을 구별하기 위해, 우리는 두 번째 코멘트와 익명화된 화자 id를 각각 참조하기 위해 comment_1, id1을 사용한다. 이슈가 닫히면 <issue_closed> 토큰이 추가된다.\n' +
      '\n' +
      '<issue_start>Title:title_unusername_id0:comment_0<issue_comment>username_id1:comment_1...<issue_closed(optional)>issue_comment>username_idn:comment_n<|endoftext|>\n' +
      '\n' +
      '### Notebooks\n' +
      '\n' +
      'Jupyter - scriptsWe format the Jupyter scripts for single code block, start with <jupyter_script> token.\n' +
      '\n' +
      '<jupyter_script>code<|endoftext|>\n' +
      '\n' +
      '주피터 - 구조화된 구문 분석 주피터 노트북은 텍스트, 코드 및 출력의 체인입니다. 우리는 센티넬 토큰으로 세포를 분리한다. 노트북의 세 번째 트리플렛을 참조하기 위해 텍스트2, 코드2, 출력2를 사용한다는 점에 유의한다.\n' +
      '\n' +
      '<jupyter_start><jupyter_text>text0<jupyter_code>code0e<jupyter_output>output0<jupyter_text> <|endoftext|>Kaggle - 스크립트를 사용할 수 있는 경우 연관된 데이터 세트 제목 및 설명을 Kaggle 노트북(샘플의 42%)에 사전 준비합니다. 노트북의 8.6%에 대해 데이터 세트의 스키마에 세분화된 정보를 추가합니다. 아래는 우리가 사용하는 형식입니다:\n' +
      '\n' +
      '<jupyter_start><jupyter_text>title\\nddescript\\nKaggle dataset identifier: data_identifier<jupyter_code>import pandas as pd\\n\\nndf=pd.read_csv(data_path1)\\ndf.info()<jupyter_output>df_info_output1<jupyter_text> 예: \'nexample1_1\\n..example1_4...<jupyter_script>code<|endoftext|>\n' +
      '\n' +
      '일부 노트북은 둘 이상의 csv 파일을 로드할 수 있으므로 모든 파일에 대해 데이터 정보 내용 블록을 반복합니다.\n' +
      '\n' +
      '변환된 Kaggle 노트북의 최종 스크립트를 추가하기 위해 새로운 특수 토큰 <jupyter_script>를 소개한다. 이 토큰은 일반적으로 긴 스크립트와 <jupyter_code> 토큰을 따르는 코드, 일반적으로 더 짧은 스크립트를 구별하는 데 도움이 된다.\n' +
      '\n' +
      '캐글-구조화된 캐글 노트북은 출력 셀이 없다는 점을 제외하면 구조화된 주피터 노트북과 유사하므로 텍스트 및 코드 블록만 포함하고 주피터 노트북에서 사용되는 토큰을 유지한다:\n' +
      '\n' +
      '<jupyter_start><jupyter_text>text>text>@<jupyter_code>code@>jupyter_text>... <|endoftext|>\n' +
      '\n' +
      '### StackExchange\n' +
      '\n' +
      '우리는 GitHub 문제와 유사한 형식을 사용하여 StackOverflow 데이터 세트에서 질문과 답변을 연결한다. 우리는 질문부터 시작해서 무작위로 답을 추가한다. 우리는 답변과 함께 상향 투표 점수를 포함하고 해당될 경우 선택한 답변으로 표시한다. StackExchange 데이터셋에 대한 대화 제목이 없습니다.\n' +
      '\n' +
      '<issue_start>username_id@: question <issue_comment>username_id1:answer_1\\nUpvotes: score[selected answer](Optional...) <issue_comment>username_idn:answer_n\\nUpvotes: score[selected answer](Optional)<|endoftext|>\n' +
      '\n' +
      '### Intermediate Representations\n' +
      '\n' +
      '우리는 소스 코드에서 중간 표현(코드->중간)으로 변환하는 것과 중간 표현(중간->중간)으로 변환하는 것 사이에서 50/50을 나눈다. 중간 표현과 관련하여 크기 최적화 버전 80%와 성능 최적화 버전 20%를 사용한다. 우리는 번역의 방향을 나타내기 위해 별도의 센티넬 토큰을 사용한다.\n' +
      '\n' +
      'code<code_to_intermediate>intermediate_representationintermediate_representation<intermediate_to_code>code\n' +
      '\n' +
      '##6 모델 아키텍처 및 훈련 세부사항\n' +
      '\n' +
      '이 섹션에서는 모델 아키텍처(SS6.1), 토큰화기(SS6.2), 훈련 세부 정보(SS6.3) 및 훈련 중 CO\\({}_{2}\\) 배출에 대한 모든 세부 정보를 제공한다.\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      '스타코더베이스에 비해 몇 가지 아키텍처 변경 사항을 소개합니다. 먼저, 예비 절제 연구에서 상당한 성능 향상을 확인함에 따라 학습된 위치 임베딩을 Rotary Positional Encodings(RoPE; Su et al., 2021)로 대체한다. DeepseekCoder (Guo et al., 2024) 및 Code LLaMA (Roziere et al., 2023)에 이어, 기본 주기 \\(\\theta=1e5\\)를 사용한다. 두 번째 아키텍처 수정은 Multi-Query Attention(MQA; Shazeer, 2019)을 Grouped Query Attention(Ainslie et al., 2023, GQA;)으로 대체하는 것이다. 그러나 우리는 추론이 크게 느려지는 것을 방지하기 위해 키-값 헤드의 수를 3B의 경우 2개, 7B의 경우 4개, 15B의 경우 상대적으로 낮게 유지한다.\n' +
      '\n' +
      '표 6에서 레이어의 수와 숨겨진 차원과 같은 다른 모든 하이퍼파라미터를 요약한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Token** & **Description** \\\\ \\hline \\textless{}|endoftext\\textless{} & end of text/sequence \\\\ \\textless{}rim\\_prefix\\textgreater{} & FIM prefix \\\\ \\textless{}film\\_middle\\textgreater{} & FIM middle \\\\ \\textless{}film\\_suffix\\textgreater{} & FIM suffix \\\\ \\textless{}film\\_pad\\textgreater{} & FIM pad \\\\ \\textless{}repo\\_name\\textgreater{} & repository name \\\\ \\textless{}file\\_sep\\textgreater{} & file separator \\\\ \\textless{}issue\\_start\\textgreater{} & start of GitHub issue \\\\ \\textless{}issue\\_comment\\textgreater{} & start of GitHub issue comment \\\\ \\textless{}issue\\_closed\\textgreater{} & GitHub issue closed event \\\\ \\textless{}jupyter\\_start\\textgreater{} & start of Jupyter notebook \\\\ \\textless{}jupyter\\_textgreater{} & start of Jupyter text cell \\\\ \\textless{}jupyter\\_code\\textgreater{} & start of Jupyter code cell \\\\ \\textless{}jupyter\\_output\\textgreater{} & start of Jupyter output cell \\\\ \\textless{}jupyter\\_script\\textgreater{} & start of Jupyter script (converted kaggle notebook) \\\\ \\textless{}empty\\_output\\textgreater{} & output cell without content \\\\ \\textless{}code\\_to\\_intermediate\\textgreater{} & translate source code to intermediate representation \\\\ \\textless{}intermediate\\_to\\_code\\textgreater{} & translate intermediate representation to source code \\\\ \\textless{}pr\\textgreater{} & start of pull request \\\\ \\textless{}pr\\_status\\textgreater{} & status of pull request \\\\ \\textless{}pr\\_is\\_merged\\textgreater{} & whether pr is merged \\\\ \\textless{}pr\\_base\\textgreater{} & start of list of base files \\\\ \\textless{}pr\\_file\\textgreater{} & path of pull request file \\\\ \\textless{}pr\\_base\\_code\\textgreater{} & code that is part of the base commit in the PR \\\\ \\textless{}pr\\_diff\\textgreater{} & start of a diff \\\\ \\textless{}pr\\_diff\\textgreater{} & diff hunk \\\\ \\textless{}pr\\_comment\\textgreater{} & general comment \\\\ \\textless{}pr\\_event\\_id\\textgreater{} & GitHub id of review comment or code review comment \\\\ \\textless{}pr\\_review\\textgreater{} & start of review \\\\ \\textless{}pr\\_review\\textgreater{} & review state (e.g. approved, rejected) \\\\ \\textless{}pr\\_review\\textgreater{} & code review comment \\\\ \\textless{}pr\\_in\\_reply\\_to\\_review\\_id\\textgreater{} & GitHub event id of review \\\\ \\textless{}pr\\_in\\_reply\\_to\\_comment\\_id\\textgreater{} & GitHub event id of comment \\\\ \\textless{}pr\\_diff\\textgreater{}hunk\\_comment\\_line\\textgreater{} & line number of code review comment \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 센티넬 토큰 개요.\n' +
      '\n' +
      '### Tokenizer\n' +
      '\n' +
      '우리는 StarCoderBase의 절차를 따르고 Stack v1.24의 작은 하위 집합에서 바이트 수준의 Byte-Pair-Encoding tokenizer를 훈련한다. 예비 실험에서 어휘 크기를 100K로 늘리는 것이 성능을 향상시키지 않는다는 것을 관찰했다. 따라서 표 5의 센티넬 토큰을 포함하여 49,152 토큰의 어휘 크기를 유지하기로 하였으며, 사전 토큰화 단계는 GPT-2 사전 토큰화기의 디지트 스플리터와 레지스 스플리터를 포함한다.\n' +
      '\n' +
      '각주 24: [https://huggingface.co/datasets/bigcode/the-stack-march-sample-special-tokens-stripped](https://huggingface.co/datasets/bigcode/the-stack-march-sample-special-tokens-stripped)]\n' +
      '\n' +
      '### Training Details\n' +
      '\n' +
      '기본모델은 Adam(Kingma and Ba, 2015)을 사용하여 4,096의 서열길이와 \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.95\\), \\(\\epsilon=10^{-8}\\) 및 0.1의 중량감쇠로 훈련되었다. 학습률은 1,000번의 반복의 선형 예열 후 코사인 붕괴를 따랐다. 표 7은 각 모델에 대한 훈련 하이퍼-파라미터를 상세히 설명한다. RoPE \\(\\theta\\) 값은 StarCoder2-15B에서 훈련 구성을 파싱하는 버그로 인해 다르다. 더욱이, StarCoder2-15B는 1.1M 반복을 위해 훈련될 예정이었지만, 1M 반복 후에 조기 중단되었다. Muennighoff et al.(2023)에 이어, 우리는 약 4~5개의 시대에 대한 데이터를 반복한다.\n' +
      '\n' +
      '긴 컨텍스트는 플래시 어텐션-2(Dao et al., 2022; Dao, 2024)와 함께 슬라이딩 윈도우가 4,096인 16,384 컨텍스트 길이를 사용하여 동일한 사전 트레이닝 코퍼스로부터 200B 토큰에 대한 긴 컨텍스트에 대해 각 모델을 추가로 사전 트레이닝하였다. 우리는 RoPE\\(\\theta\\)을 증가시키고 최적화기에 동일한 구성을 사용한다. 다른 훈련 하이퍼파라미터들은 표 8에 제공된다.\n' +
      '\n' +
      '### CO2 Emissions\n' +
      '\n' +
      'Lacoste et al.(2019)에 제시된 Machine Learning Impact 계산기를 이용하여 StarCoder2 훈련의 CO\\({}_{2}\\) 방출량을 추정한다. 기본 모델 훈련의 총 GPU 시간을 고려하여 CO({}_{2}\\) 배출량을 계산한다는 점에 유의한다. 그런 다음 이 숫자를 토큰 수를 기반으로 긴 컨텍스트 미세 조정으로 추론한다.\n' +
      '\n' +
      '서비스나우에서 제공하는 컴퓨팅 기반구조는 0.386 kgCO\\({}_{2}\\)eq/kWh의 탄소효율을 보였다. A100 SXM4 80GB(TDP of type A100 SXM4 80GB)의 하드웨어에 대해 97,120시간의 누적 연산이 수행되었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline\n' +
      '**Parameter** & **StarCoder2-3B** & **StarCoder2-7B** & **StarCoder2-15B** \\\\ \\hline hidden\\_dim & 3072 & 4608 & 6144 \\\\ n\\_heads & 24 & 36 & 48 \\\\ n\\_kv\\_heads & 2 & 4 & 4 \\\\ n\\_layers & 30 & 32 & 40 \\\\ vocab size & 49152 & 49152 & 49152 \\\\ seq\\_len & base-4k/long-16k & base-4k/long-16k & base-4k/long-16k \\\\ positional encodings & RoPE & RoPE & RoPE \\\\ \\hline FLOPs23 & 5.94e+22 & 1.55e+23 & 3.87e+23 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: StarCoder2 모델의 모델 아키텍처 세부사항.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l} \\hline \\hline\n' +
      '**Model** & **learning rate** & **RoPE \\(\\theta\\)** & **batch size** & \\(n\\) **iterations** & \\(n\\) **tokens** & \\(n\\) **epochs** \\\\ \\hline StarCoder2-3B & \\(3\\times 10^{-4}\\) & \\(1e5\\) & 2.6M & 1.2M & 3.1T & 4.98 \\\\ StarCoder2-7B & \\(3\\times 10^{-4}\\) & \\(1e5\\) & 3.5M & 1M & 3.5T & 5.31 \\\\ StarCoder2-15B & \\(3\\times 10^{-4}\\) & \\(1e4\\) & 4.1M & 1M & 4.1T & 4.49 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: StarCoder2 기반 모델의 훈련 세부사항.\n' +
      '\n' +
      '400W). 총 배출량은 14,995.33 kgCO\\({}_{2}\\)eq로 추정되었다. 긴 문맥 미세 조정 단계에서는 1,111.68 kgCO\\({}_{2}\\)eq가 추가되어 총 16,107.01 kgCO\\({}_{2}\\)eq가 생성되었다.\n' +
      '\n' +
      '7B Hugging Face에서 제공하는 컴퓨팅 기반구조는 0.2925 kgCO\\({}_{2}\\)eq/kWh의 탄소효율을 보였다. H100형 하드웨어(TDP 660W)에 대해 누적 145,152시간의 계산을 수행하였다. 총 배출량은 28,021.6 kgCO\\({}_{2}\\)eq로 추정되었다. 긴 컨텍스트 미세 조정 단계는 1601.23을 추가하여 총 29,622.83 kgCO\\({}_{2}\\)q를 생성한다.\n' +
      '\n' +
      '15B 보고서는 곧 15B 모델에 대한 추정치로 업데이트될 것이다.\n' +
      '\n' +
      '## 7 Evaluation\n' +
      '\n' +
      '다양한 벤치마크에 대한 StarCoder2 모델을 문헌의 최신 오픈 코드 LLMs: StableCode (Pinnaparaju et al., 2024), Code Llama (Roziere et al., 2023), DeepSeekCoder (Guo et al., 2024) 및 original StarCoder (Li et al., 2023)와 비교한다. StarCoder2는 기본 모델이기 때문에 위에서 언급한 모델 패밀리의 기본 모델과만 비교한다.\n' +
      '\n' +
      '우리는 모든 비교를 모델 크기별로 그룹화한다. 상기 _small_ 모델들은 3B 이하의 파라미터를 가지며, 상기 _medium_ 모델들은 7B 이하의 파라미터를 가지며, 상기 _large_ 모델들은 15B 이하의 파라미터를 가진다. 마지막으로 CodeLlama-34B와 DeepSeekCoder-33B의 두 가지 _extra large_ 모델을 포함한다. 이 모델들은 대형 스타코더2 모델의 2배 이상이다. 그러나 아래에서 볼 수 있듯이 스타코더2-15B는 여러 벤치마크에서 초대형 모델에 가깝거나 심지어 능가합니다.\n' +
      '\n' +
      '### Code Completion\n' +
      '\n' +
      '먼저 코드 LLM 작업에서 널리 연구된 코드 완성 작업에 대한 StarCoder2 모델을 평가한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline\n' +
      '**Model** & **learning rate** & **RoPE \\(\\theta\\)** & **batch size** & \\(n\\) **iterations** & \\(n\\) **tokens** \\\\ \\hline StarCoder2-3B & \\(3\\times 10^{-5}\\) & \\(1e6\\) & 2.6M & 80k & 200B \\\\ StarCoder2-7B & \\(2\\times 10^{-5}\\) & \\(1e6\\) & 3.5M & 56k & 200B \\\\ StarCoder2-15B & \\(3\\times 10^{-5}\\) & \\(1e5\\) & 4.1M & 50k & 200B \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: StarCoder2 모델의 긴 컨텍스트 트레이닝을 위한 트레이닝 세부사항.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline\n' +
      '**Model** & **HumanEval** & **HumanEval+** & **MBPP** & **MBPP+** \\\\ \\hline StarCoderBase-3B & 21.3 & 17.1 & 42.6 & 35.8 \\\\ DeepSeekCoder-1.3B & 28.7 & 23.8 & 55.4 & 46.9 \\\\ StableCode-3B & 28.7 & 24.4 & 53.1 & 43.1 \\\\ StarCoder2-3B & **31.7** & **27.4** & **57.4** & **47.4** \\\\ \\hline StarCoderBase-7B & 30.5 & 25.0 & 47.4 & 39.6 \\\\ CodeLlama-7B & 33.5 & 25.6 & 52.1 & 41.6 \\\\ DeepSeekCoder-6.7B & **47.6** & **39.6** & **70.2** & **56.6** \\\\ StarCoder2-7B & 35.4 & 29.9 & 54.4 & 45.6 \\\\ \\hline StarCoderBase-15B & 29.3 & 25.6 & 50.6 & 43.6 \\\\ CodeLlama-13B & 37.8 & 32.3 & 62.4 & 52.4 \\\\ StarCoder2-15B & **46.3** & **37.8** & **66.2** & **53.1** \\\\ \\hline CodeLlama-34B & 48.2 & 44.3 & 65.4 & 52.4 \\\\ DeepSeekCoder-33B & **54.3** & **46.3** & **73.2** & **59.1** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: HumanEval(+) 및 MBPP(+)에 대한 Pass@1. 이러한 결과는 그리디 디코딩을 이용하여 생성되었다.\n' +
      '\n' +
      '######7.1.1 HumanEval, MBPP, EvalPlus\n' +
      '\n' +
      '벤치마크인 HumanEval(Chen et al., 2021)과 MBPP(Austin et al., 2021)에 대해서는 Code LLM의 가장 널리 연구된 벤치마크 중 하나이다. 각 벤치마크에는 수백 가지 프로그래밍 문제가 있습니다. 각 HumanEval 문제에는 함수 서명 및 문서 문자열과 숨겨진 단위 테스트 세트가 있습니다. 각 MBPP 문제에 대한 프롬프트에는 자연어 설명과 몇 가지 테스트가 포함됩니다. 평가 중인 모델은 프롬프트가 주어지면 함수를 완성할 것이며, 우리는 숨겨진 단위 테스트로 그 함수를 테스트한다. 모든 숨겨진 테스트가 통과해야 결과가 성공으로 간주됩니다.\n' +
      '\n' +
      '최근 Liu et al.(2023a)은 두 벤치마크에서 몇 가지 이슈를 확인하였다. (1) 대부분의 문제는 해결책에서 미묘한 버그를 탐지할 수 없는 불충분한 숨겨진 테스트를 가지고 있고(목록 1 및 2 참조), (2) 몇 가지 문제는 잘못된 테스트 사례와 모호한 설명을 가지고 있으며, 이는 진술을 다른 합리적인 방식으로 해석하는 모델에 부당하게 불이익을 준다(목록 2 참조). 그들은 이러한 문제를 해결하기 위해 EvalPlus 프레임워크를 도입합니다. 결과 벤치마크(HumanEval+ 및 MBPP+)는 기존 벤치마크보다 \\(80\\times\\) 및 \\(35\\times\\) 더 많은 테스트를 거쳤다. 엄격한 평가를 위해 본 연구에서는 EvalPlus 프레임워크를 채택한다.\n' +
      '\n' +
      '```\n' +
      'defcommon(1l:list,l2:list)->list: ""Returnsorteduniquecommonelementsfor2lists""" common_elems=list(set(1l.intersection(set(l2)) common_elems.sort() returnlist(set(common_elems)) assertcommon([4,3,2,8],[])==[] assertcommon([5,3,2,8],[3,2])==[2,3...)\n' +
      '#[Explanation]Thissolutioniswrongasaplyingset\n' +
      '#tothesortedcommon_elemsdoesnotpreservethe\n' +
      '#order.BaseHumanEvaltestinputsaretooshortto\n' +
      '#easilymanifesttheflakiness.\n' +
      '```\n' +
      '\n' +
      '목록 1: 테스트가 불충분한 휴먼평가 작업\n' +
      '\n' +
      'Hyperparameters after recent work on Code LLMs (Roziere et al., 2023; Guo et al., 2024), we used greedy decoding and report the mean pass@1 (mean success rate) for all problems in benchmark.\n' +
      '\n' +
      'HumanEval, MBPP 및 이들의 EvalPlus 변이체에 대한 결과는 표 9.25에 나와 있으며, 표에서 다음과 같은 관찰을 할 수 있다.\n' +
      '\n' +
      '각주 25: EvalPlus는 MBPP 데이터 세트에서 일부 잘못 형성되고 시끄러운 문제를 생략한다는 점에 유의하십시오. 원저자에 의해 살균된 MBPP 하위 집합의 427개 문제 중 399개를 사용한다(Austin et al., 2021). HumanEval의 경우 원래 데이터 세트에서 164개의 문제를 모두 보관했다.\n' +
      '\n' +
      '1. StarCoder2-3B는 모든 데이터세트(HumanEval, MBPP, HumanEval+, MBPP+)에서 가장 성능이 좋은 소형 모델이다. 이 모델은 이전 모델인 StarCoderBase-3B보다 훨씬 더 우수하여 HumanEval+에서 각각 60.2%, MBPP+에서 32.4%의 개선을 보였다.\n' +
      '2. 스타코더2-7B는 중간 모델 중 2위를 차지합니다. DeepSeeKoder-6.7B는 더 강하여 HumanEval+ 및 MBPP+에서 StarCoder2-7B를 각각 32.4% 및 24.1% 능가한다. 그러나 StarCoder2-7B는 StarCoderBase-7B 및 CodeLlama-7B를 포함한 다른 모든 매체 모델보다 일관되게 성능이 우수하다. StarCoder2-7B는 HumanEval+ 및 MBPP+에서 스타CoderBase-7B보다 각각 19.6% 및 15.2% 더 우수하다. 또한 이러한 벤치마크에서 코드라마-7B를 16.8%, 9.6% 능가한다.\n' +
      '3. StarCoder2-15B는 상당한 마진으로 가장 성능이 좋은 대형 모델이다. 예를 들어, 이것은 46.3점을 받는 반면, CodeLlama-13B는 HumanEval에서 37.8점을 받는다. EvalPlus에 대한 결과도 일관성이 있습니다. 예를 들어, HumanEval+에서는 StarCoderBase-15B 및 CodeLlama-13B보다 각각 47.7% 및 17.0% 크게 개선된다.\n' +
      '\n' +
      '4. StarCoder2-15B는 크기가 두 배 이상인 모델에서도 경쟁력이 있다. 예를 들어, StarCoder2-15B는 MBPP 및 MBPP+ 모두에서 CodeLlama-34B를 능가한다.\n' +
      '\n' +
      'EvalPlus가 HumanEval과 MBPP를 훨씬 더 강력하게 만들지만, 이러한 벤치마크의 문제는 기본적인 파이썬 빌트인스만을 행사한다. 그들은 다른 프로그래밍 언어에 대해 테스트하지 않으며 다른 파이썬 라이브러리에 대한 모델의 지식을 테스트하지 않는다. 우리는 코드 완성에 대한 보다 포괄적인 평가를 통해 이 하위 섹션의 나머지 부분에서 이러한 한계를 해결한다.\n' +
      '\n' +
      '###### 7.1.2 멀티PL-E : 다국어 코드 완성\n' +
      '\n' +
      '벤치마크MultiPL-E(Cassano et al., 2023b)에 대해서는 경량, 규칙 기반 컴파일러의 세트를 사용하여 HumanEval을 Python에서 18개의 다른 프로그래밍 언어로 번역한다. 따라서 MultiPL-E는 서로 다른 언어로 번역된 동일한 문제를 가진 다국어 벤치마크이다.26\n' +
      '\n' +
      '하이퍼파라미터는 온도 0.2에서 Top-p 0.95로 프롬프트당 50개의 완료를 샘플링한다. 이는 빅코드 모델 리더보드(Ben Allal, 2023)에서 MultiPL-E 결과가 보고되는 방식이다.\n' +
      '\n' +
      'MultiPL-E에 대한 결과는 표 10에 나와 있다. 우리는 다음과 같은 관찰을 한다:\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline Model & **C++** & **C\\#** & **D** & **Go** & **Java** & **Julia** & **JavaScript** & **Lua** & **PHP** \\\\ \\hline StableCode-3B & **28.4** & 14.4 & **13.4** & 19.3 & 27.8 & **20.6** & 32.0 & 17.1 & 23.7 \\\\ DeepSeeKoder-1.3B & 28.3 & **21.3** & 10.4 & 19.1 & **29.2** & 15.0 & 28.3 & 19.2 & 23.2 \\\\ StarCoderBase-3B & 19.4 & 13.3 & 5.0 & 13.3 & 19.2 & 16.1 & 21.3 & 18.0 & 18.6 \\\\ StarCoder2-3B & 27.2 & 20.5 & 12.6 & **23.6** & 27.4 & 19.9 & **35.4** & **28.0** & **27.6** \\\\ \\hline CodeLlama-7B & 26.4 & 21.0 & 11.6 & 20.9 & 28.2 & 25.9 & 31.6 & 30.4 & 25.1 \\\\ DeepSeeKoder-6.7B & **46.7** & **32.9** & **18.4** & **31.0** & **39.7** & **31.4** & **46.6** & **34.2** & **32.6** \\\\ StarCoderBase-7B & 23.3 & 19.3 & 8.1 & 19.6 & 24.4 & 21.8 & 27.4 & 23.4 & 22.1 \\\\ StarCoder2-7B & 33.6 & 20.7 & 15.1 & 20.2 & 29.4 & 20.4 & 35.4 & 30.7 & 30.6 \\\\ \\hline CodeLlama-13B & 37.4 & 24.8 & 15.5 & **26.6** & **37.5** & 27.9 & 39.3 & 31.6 & 33.9 \\\\ StarCoderBase-15B & 30.6 & 20.6 & 10.0 & 21.5 & 28.5 & 21.1 & 31.7 & 26.6 & 26.8 \\\\ StarCoder2-15B & **41.4** & **29.2** & **23.6** & 26.2 & 33.9 & **33.2** & **44.2** & **43.8** & **39.5** \\\\ \\hline CodeLlama-34B & 41.4 & 30.7 & 15.3 & 28.7 & 40.2 & 31.4 & 41.7 & **37.5** & 40.4 \\\\ DeepSeeKoder-33B & **51.2** & **35.3** & **17.4** & **34.2** & **43.8** & **32.8** & **51.3** & 36.5 & **41.8** \\\\ \\hline \\hline Model & **Perl** & **R** & **Ruby** & **Racket** & **Rust** & **Scala** & **Bash** & **Swift** & **TypeScript** \\\\ \\hline StableCode-3B & 9.4 & 11.5 & 0.8 & 7.0 & 22.9 & 5.9 & 8.6 & 13.2 & 29.6 \\\\ DeepSeeKoder-1.3B & 12.5 & 9.8 & 24.6 & **9.1** & 18.6 & **19.6** & 9.7 & 11.0 & 27.4 \\\\ StarCoderBase-3B & 11.3 & 10.1 & 4.2 & 7.9 & 16.3 & 16.8 & 3.8 & 10.0 & 22.8 \\\\ StarCoder2-3B & **13.6** & **14.2** & **31.3** & 7.8 & **24.5** & 18.9 & **12.3** & **25.1** & **34.4** \\\\ \\hline CodeLlama-7B & 16.9 & 14.9 & 29.5 & 11.4 & 25.5 & 22.8 & 9.6 & 24.9 & 33.4 \\\\ DeepSeeKoder-6.7B & **30.4** & **20.5** & **46.2** & **17.4** & **37.7** & **35.2** & **22.2** & **30.3** & **39.5** \\\\ StarCoderBase-7B & 15.2 & 14.5 & 19.6 & 11.1 & 22.6 & 20.9 & 7.3 & 15.1 & 27.5 \\\\ StarCoder2-7B & 16.6 & 16.7 & 28.3 & 11.6 & 29.6 & 19.5 & 12.2 & 26.1 & 36.3 \\\\ \\hline CodeLlama-13B & 23.4 & 14.1 & 31.9 & 13.0 & 31.0 & 29.7 & 13.3 & 30.1 & 40.1 \\\\ StarCoderBase-15B & 16.3 & 10.2 & 17.2 & 11.8 & 24.5 & 28.8 & 11.0 & 16.7 & 32.1 \\\\ StarCoder2-15B & **37.2** & **19.8** & **41.5** & **22.4** & **38.0** & **37.4** & **18.9** & **34.2** & **43.8** \\\\ \\hline CodeLlama-34B & 28.5 & **22.7** & 37.8 & 16.9 & 38.7 & 36.7 & 16.4 & 35.3 & 42.1 \\\\ DeepSeeKoder-33B & **31.0** & 20.5 & **44.0** & **23.4** & **43.8** & **43.9** & **28.7** & **35.8** & **48.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 멀티PL-E에 대한 Pass@1 결과는 각 문제에 대해 50개 샘플에 대해 평균을 냈다. 모든 모델은 온도 0.2 및 Top-p 0.95에서 평가된다.\n' +
      '\n' +
      '1. 모든 크기 클래스에 걸쳐, 모든 언어에서 가장 좋은 단일 모델은 없다. 그럼에도 불구하고, StarCoder2 모델은 아래에 설명된 바와 같이 잘 수행된다.\n' +
      '2. 소형 모델 중 StarCoder2-3B는 11/18 프로그래밍 언어에서 가장 우수한 성능을 보인다.\n' +
      '3. 매체 모델 중 DeepSeekCoder-6.7B가 가장 우수한 성능을 보인다. 스타코더2-7B는 대부분의 언어에서 코드라마-7B보다 더 잘한다.\n' +
      '4. 대형 모델 중 StarCoder2-15B는 16/18 프로그래밍 언어에서 가장 잘한다. CodeLlama-13B는 Go와 Java에서 StarCoder2-15B를 능가한다.\n' +
      '5. StarCoder2-15B는 10/18 프로그래밍 언어에서 CodeLlama-34B와 4개의 하위 자원 언어(D, Julia, Lua, Perl)에서 DeepSeekCoder-33B의 성능을 충족하거나 초과한다.\n' +
      '\n' +
      'Python에서의 데이터 사이언스 과제\n' +
      '\n' +
      '벤치마크DS-1000(Lai et al., 2023)에 대해서는 파이썬에서 1,000개의 데이터 사이언스 과제를 가지고 널리 연구된 벤치마크이다. Python 표준 라이브러리만을 사용하는 HumanEval 및 MBPP 문제와 달리 DS-1000은 Matplotlib에서 TensorFlow에 이르기까지 널리 사용되는 7개의 라이브러리를 연습한다. 따라서 본 논문에서는 DS-1000을 도입하여 일반 도서관과 데이터 사이언스 작업을 완료할 때 코드 LLM의 성능을 평가한다.\n' +
      '\n' +
      'HyperparametersFollowing Lai et al.(2023)에 따라, 우리는 온도 0.2와 top-p 0.95를 사용하여 문제당 40개의 샘플을 생성하고 평균 pass@1을 보고한다.\n' +
      '\n' +
      '결과 표 11은 DS-1000에 대한 결과를 보고한다. 우리는 다음과 같은 관찰을 한다:\n' +
      '\n' +
      '1. 스타코더2-3B는 DS-1000에서 전반적으로 가장 성능이 좋은 소형 모델이다. PyTorch와 TensorFlow(StableCode-3B보다 약간 더 나쁨)를 제외하고, 스타코더2-3B는 다른 모든 인기 라이브러리에서 최상의 성능을 달성한다.\n' +
      '2. StarCoder2-7B는 DeepSeekCoder-6.7B와 유사한 성능으로 중간 모델 중 2위를 차지하고 있다.\n' +
      '3. StarCoder2-15B는 DS-1000에서 가장 성능이 좋은 대형 모델이며, 큰 마진으로 StarCoderBase-15B와 CodeLlama-13B를 모두 능가하며 CodeLlama-34B의 전체 성능에 근접한다.\n' +
      '\n' +
      '### 코드 수정 및 편집\n' +
      '\n' +
      '위의 하위 섹션은 다양한 코드 완성 작업을 연구했지만, 코드 LLM은 다양한 다른 방식으로 사용될 수 있다. 이 하위 섹션에서는 버그를 수정하거나 기존 코드를 편집할 수 있는 기능을 연구하는 데 중점을 둡니다.\n' +
      '\n' +
      '6개 프로그래밍 언어의 버그 수정\n' +
      '\n' +
      '벤치마크 HumanEvalFix(Muennighoff et al., 2024)에 대해서는 코드의 버그를 식별하고 수정하는 모델의 능력을 테스트하는 벤치마크이다. 벤치마크는 도 12에 도시된 6개의 프로그래밍 언어를 지원한다. 코드 완성 벤치마크가 아니기 때문에 대부분의 베이스 모델은 HumanEvalFix에서 불량한 반면, 명령어-튜닝(Wei et al., 2022; Sanh et al., 2022; Muennighoff et al., 2022; 2024) 모델은 더 나은 성능을 보인다. 따라서, 우리는 우리의 비교에서 DeepSeekCoder 및 CodeLlama의 명령어-튜닝된 변형들을 고려한다(Guo et al., 2024; Roziere et al., 2023). 또한 CommitPackFT 데이터셋을 이용하여 초기 StarCoder의 명령어 조정 버전인 OctoCoder와 비교한다(Muennighoff et al., 2024; Zhuo et al., 2024; Longpre et al., 2023). 기본 HumanEvalFixTests 하위 변수를 벤치마킹했으며, 따라서 모델을 안내하는 문서 문자열이 존재하지 않았다.\n' +
      '\n' +
      '스타코더2는 형식을 발행하지만, 스타코더2는 기본 모델이지만 특수 형식(SS5.3)을 사용하여 GitHub 문제와 StackOverflow 논의에 대해 사전 훈련된다. 다음과 같이 논의 형식으로 코드 버그를 수정하도록 모델을 자극하여 실험한다.\n' +
      '\n' +
      '<issue_start>username_0: instruction>nn\'\'\'buggy function\'\'\'nUpvotes:100<issue_comment>username_1:Sure, 여기에는 고정 코드.nn\'\'\'function start가 있다. 이 템플릿에서, "instruction"은 모델에서 버그를 코드로 고정하도록 지시하는 HumanEvalFix 명령어이고, "buggy function"은 미묘한 버그를 갖는 함수이며, "function start"는 수입을 포함하는 함수 헤더의 생성은 \'\'이 생성되자마자 중단된다. 평가 코드는 Ben Allal 등(2022)을 통해 이용 가능하며, 이를 "문제" 프롬프트로 표시한다. 우리는 또한 Muennighoff et al.(2024)에서 사용된 동일한 기본 "Instruct" 프롬프트로 StarCoder2를 벤치마킹한다.\n' +
      '\n' +
      '**Hyperparameters**: Following (Muennighoff et al., 2024), 우리는 20개의 샘플로 pass@1을 추정하기 위해 0.2의 온도를 사용한다.\n' +
      '\n' +
      '이전 섹션과 달리 우리는 StarCoder2-15B만 평가하고 주로 명령어 조정 모델과 비교한다. 결과는 표 12(대담하고 두 번째로 우수한 밑줄로 강조 표시된 최상의 모델 포함)에 있으며 다음 결론을 내린다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c l c c c c c c c} \\hline \\hline\n' +
      '**Format** & **Model** & & & & & & & & & \\\\ \\hline  & \\# problems: & 155 & 220 & 291 & 68 & 106 & 115 & 45 & 1,000 \\\\ \\hline Completion & StarCoderBase-3B & 32.1 & 16.8 & 5.3 & 9.2 & 13.2 & 10.5 & 17.2 & 14.2 \\\\ Completion & StableCode-3B & 42.5 & 24.5 & **16.2** & **15.4** & 13.5 & 20.2 & **27.7** & 22.7 \\\\ Completion & DeepSeeKoder-1.3B & 36.2 & 18.8 & 9.1 & 10.7 & 7.9 & 13.9 & 13.3 & 16.2 \\\\ Completion & StarCoder2-3B & **45.5** & **27.7** & **16.2** & 12.9 & **15.8** & **30.8** & 22.8 & **25.0** \\\\ \\hline Completion & StarCoderBase-7B & 38.0 & 23.0 & 8.2 & 13.1 & 13.7 & 24.5 & 14.6 & 19.1 \\\\ Completion & DeepSeeKoder-6.7B & 52.4 & 33.0 & **20.0** & 13.9 & 19.8 & **29.7** & 27.4 & **28.9** \\\\ Completion & CodeLlama-7B & 46.3 & 21.6 & 13.9 & 12.2 & 17.5 & 16.7 & 20.6 & 21.5 \\\\ Completion & StarCoder2-7B & **53.6** & **33.3** & 16.9 & **16.2** & **20.6** & 22.2 & **31.9** & 27.8 \\\\ \\hline Completion & StarCoderBase-15B & 47.0 & 27.1 & 10.1 & **19.5** & 21.7 & **27.0** & 20.5 & 23.8 \\\\ Completion & CodeLlama-13B & 49.0 & 27.2 & 17.4 & 12.9 & 15.6 & 24.0 & 24.8 & 25.1 \\\\ Completion & StarCoder2-15B & **60.3** & **43.3** & **23.2** & 11.0 & **26.4** & 26.0 & **36.0** & **33.8** \\\\ \\hline Completion & DeepSeeKoder-33B & **56.1** & **49.6** & **25.8** & **36.8** & **36.8** & **40.0** & **46.7** & **40.2** \\\\ Completion & CodeLlama-34B & 50.3 & 42.7 & 23.0 & 25.0 & 28.3 & 33.9 & 40.0 & 34.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: DS-1000에 대한 오픈액세스 모델의 성능. 벤치마크는 다음과 같다. 모든 모델은 온도 0.2 및 Top-p 0.95에서 평가되었으며 점수는 40개 샘플에 대해 평균 통과@1 정확도를 반영한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c l|c c c c c c|c} \\hline \\hline\n' +
      '**Model** & **Prompt** & **Python** & **JavaScript** & **Java** & **Go** & **C++** & **Rust** & **Avg.** \\\\ \\hline StarCoderBase-15B & Instruct & 12.6 & 16.8 & 18.9 & 12.5 & 11.2 & 0.6 & 12.1 \\\\ StarCoderBase-15B & Commit & 25.6 & 29.4 & 28.8 & 28.7 & 28.2 & 19.7 & 26.7 \\\\ CodeLlama-13B-Instruct & Instruct & 19.4 & 18.9 & 24.1 & 21.6 & 10.1 & 0.4 & 15.8 \\\\ CodeLlama-34B-Instruct & Instruct & 36.5 & 28.1 & 36.4 & 25.7 & 25.2 & 18.5 & 28.4 \\\\ DeepSeeKoder-6.7B-Instruct & Instruct & 44.9 & **55.3** & **52.2** & 42.9 & 37.9 & 19.5 & **42.1** \\\\ DeepSeeKoder-33B-Instruct & Instruct & 47.5 & **47.6** & 46.5 & **52.0** & **48.0** & 10.2 & **42.1** \\\\ OctoCoder-15B & Instruct & 30.4 & 28.4 & 30.6 & 30.2 & 26.1 & 16.5 & 27.0 \\\\ \\hline StarCoder2-15B & Instruct & 9.7 & 20.7 & 24.1 & 36.3 & 25.6 & 15.4 & 22.0 \\\\ StarCoder2-15B & Issue & **48.6** & 41.6 & 48.4 & 48.5 & 20.7 & **24.2** & 38.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 12: HumanEvalFix 상의 Pass@1 성능. 스타코더2 및 스타코더베이스는 명령-튜닝되지 않으므로, 이들은 모두 명령-튜닝된 다른 모델들에 비해 불리하다.\n' +
      '\n' +
      '1. 기본 모델들(StarCoder2-15B 및 StarCoderBase-15B)은 명령 프롬프트가 주어질 때 매우 저조하게 수행되며, 이는 상이한 프롬프트 포맷을 사용하는 동기를 부여한다.\n' +
      '2. 상술한 Issue prompt를 이용하여, StarCoder2-15B는 베이스 모델로서 현저한 성능을 발휘한다. 이는 명령어 조정 코드라마 모델보다 상당한 성능 향상을 보이며, 명령어 조정 DeepSeeKoder 모델의 성능에 거의 도달한다.\n' +
      '3. StarCoder2-15B에 대한 Issue 프롬프트를 사용하는 것은 StarCoderBase-15B에 대한 Commit 프롬프트를 사용하는 것보다 더 큰 성능 증가를 초래한다. 이는 풀 요청에 대한 사전 트레이닝(StarCoder2)이 커밋에 대한 사전 트레이닝(StarCoderBase)에 대한 실행 가능한 대안임을 나타낸다.\n' +
      '4. Issue prompt를 사용하여, StarCoder2-15B는 또한 Muennighoff et al. (2024)에 제시된 모든 다른 오픈 모델들보다 우수하다.\n' +
      '5. StarCoder2-15B는 Issue 프롬프트를 사용할 때 C++에서 성능이 떨어지며, 이로 인해 전체 성능이 손상된다. 우리의 조사에 따르면 이는 생성된 코드의 3분의 1이 불완전하기 때문이며, 예를 들어 for 루프 시작 직후 예상치 못한 중단이 있기 때문이다. 이를 해결하기 위해서는 추가적인 신속한 엔지니어링이 필요할 수 있다. 따라서, 우리는 신속한 엔지니어링 없이 유사한 시나리오를 보다 효과적으로 처리할 때 사용성을 더욱 향상시키기 위해 StarCoder2의 명령어 튜닝에서 여전히 가치를 본다. 우리는 StarCoder2의 명령어 튜닝 또는 심지어 선호도 정렬(Christiano et al., 2017; Ethayarajh et al., 2024)을 향후 작업에 맡긴다.\n' +
      '\n' +
      '###### 7.2.2 코드 편집\n' +
      '\n' +
      '벤치마크CanItEdit(Cassano et al., 2024)에 대해서는 파이썬 코드 편집 작업에서 모델 성능을 평가하기 위해 설계된 수작업으로 제작된 벤치마크이다. 각 문제는 _descriptive_ 또는 _lazy_의 두 가지 유형의 명령을 수반하는 코드 스니펫으로 구성된다. 기술 지침은 체계적이고 상세한 정보를 제공하는 반면, 게으른 지침은 간단하고 직접적이며 인간이 코드 완성 모델에 제공하는 전형적인 지침을 모방한다. 명령어에 따라 코드를 수정하는 것이 목표이며, 게으른 명령어와 서술적인 명령어는 모두 동일한 편집으로 이어져야 합니다. 각 수정에 대한 정확도는 숨겨진 테스트 제품군을 사용하여 평가되며 pass@1이 보고된다. 벤치마크는 단순한 단일 함수, 단일선 편집에서 분리된 위치에서 다중선 편집을 요구하는 복잡한 다중 클래스 문제에 이르기까지 다양한 문제를 포함한다. 일부 과제는 수학과 같은 영역별 지식을 요구하며, 문제의 성공적인 완성은 프로그램의 구성 요소 간의 연결을 이해하기 위해 모델이 필요한 경우가 많다. 목록 3은 게으른 명령으로 CanItEdit에서 축약된 27개의 샘플 문제를 보여줍니다.\n' +
      '\n' +
      '각주 27: 원래 문제에는 C4 클래스에서 편집할 수 있는 추가 방법과 설명 지침이 포함됩니다.\n' +
      '\n' +
      '```\n' +
      '<class C4(m.Module): <class C8(m.Module): <class C8(m.Module):\n' +
      '《그룹 이론의 C4 클래스 재생》\n' +
      '\'집단 이론의 C8등급을 나타내고\'\n' +
      '《각 요소가 이산 회전을 나타내는》\n' +
      'def__init__(self): super()__init__() def elements(self): ""이 그룹의 모든 엘리먼트를 반환함"\n' +
      '`` return torch.tensor([0., mg.pi/2, mg.pi, 3*mp.pi/2])```\n' +
      '"d= no.pi/4 return torch.tensor([0., d, d+2, d+3, d+4, d+5, d+6, d+7])\n' +
      '```\n' +
      '\n' +
      'List 3Abbreviated sample problem from CanItEdit\n' +
      '\n' +
      '**코드 편집 명령어:** C4 클래스 및 그 메소드를 편집하여 C8 그룹을 나타낸다.\n' +
      '\n' +
      'Hyperparameters 우리는 Issue prompt format(SS7.2.1에 도입됨)을 사용하여 CanItEdit 벤치마크에서 StarCoder2의 모든 크기를 평가하고 이 벤치마크에서 이전에 평가된 다른 모델과 성능을 비교한다. C Cassano et al.(2024)에 이어서, 우리는 0.2의 온도와 0.95의 top-\\(p\\)의 랜덤 샘플링을 채용하고, 문제당 100개의 완성도를 갖는다.\n' +
      '\n' +
      '결과는 표 13에 나와 있다. SS7.2.1에 설명된 바와 같이, 우리는 StarCoder2 및 StarCoderBase 모델이 명령-튜닝되지 않기 때문에 "문제" 프롬프트 및 "커밋" 프롬프트를 사용한다. 다른 모든 모델에는 명령어 조정 버전을 사용합니다. 표에서 우리는 다음과 같은 관찰을 한다.\n' +
      '\n' +
      '1. 소형 모델 중 StarCoder2-3B가 DeepSeeKoder-Instruct-1.3B에 이어 2위를 차지하고 있다.\n' +
      '2. 매체 모델 중 StarCoder2-7B와 DeepSeeKoder-Instruct-6.7B는 각각 서술적 명령어와 게으른 명령어가 가장 잘 수행된다.\n' +
      '3. StarCoder2-15B는 상당한 마진으로 가장 성능이 좋은 대형 모델이다.\n' +
      '4. StarCoder2-15B도 CodeLlama-Instruct-34B보다 성능이 우수하다.\n' +
      '\n' +
      '이러한 결과는 StarCoder2 "Issue" 형식이 StarCoderBase "Commit" 형식에 대한 실행 가능한 대안이라는 추가 증거를 제공한다.\n' +
      '\n' +
      '### Math Reasoning\n' +
      '\n' +
      '벤치마크와 관련하여 우리는 중학교 수학 문제의 집합인 널리 연구된 GSM8K 벤치마크(Cobbe et al., 2021)를 사용하여 모델의 수학적 추론 능력을 평가한다. 우리는 Gao et al. (2023)에 의해 제안된 PAL 접근법을 사용한다: 모델은 파이썬 프로그램을 생성하도록 프롬프트되며, 이 프로그램은 문제에 대한 답을 생성하기 위해 실행된다.\n' +
      '\n' +
      'HyperparametersWe evaluate models with greedy decoding in a 8-shot setting following Chowdhery et al.(2023)\n' +
      '\n' +
      'PAL을 사용한 GSM8K에 대한 결과는 표 14에 나와 있으며 다음과 같은 관찰을 한다.\n' +
      '\n' +
      '1. StableCode-3B는 가장 성능이 좋은 소형 모델이다. 스타코더2-3B가 2위입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c l c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Format**} & **Descriptive Instructions** & **Lazy Instructions** \\\\ \\cline{3-4}  & & & **Pass@1** & \\\\ \\hline StarCoderBase-3B & Commit & 19.62 & 12.78 \\\\ StarCoder2-3B & Issue & 21.68 & 15.91 \\\\ DeepSeeKoder-Instruct-1.3B & Instruct & **25.83** & **18.33** \\\\ \\hline StarCoder2-7B & Issue & 35.23 & 18.55 \\\\ CodeLlama-Instruct-7B & Instruct & 33.89 & 27.04 \\\\ StarCoderBase-7B & Commit & **40.64** & 25.83 \\\\ DeepSeeKoder-Instruct-6.7B & Instruct & 33.89 & **33.61** \\\\ \\hline CodeLlama-Instruct-13B & Instruct & 28.33 & 20.19 \\\\ OctoCoder-15B & Instruct & 31.46 & 25.69 \\\\ StarCoderBase-15B & Commit & 38.24 & 26.38 \\\\ StarCoder2-15B & Issue & **43.08** & **38.45** \\\\ \\hline CodeLlama-Instruct-34B & Instruct & 35.0 & 26.76 \\\\ DeepSeeKoder-Instruct-33B & Instruct & **53.06** & **43.89** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: CanItEdit 벤치마크 상의 수업 코드 편집의 성능(Cassano et al., 2024). non-StarCoder2 모델에 대한 결과는 벤치마크 논문에서 나온 것이다.\n' +
      '\n' +
      '2. 스타코더2-7B가 2위입니다. 성능은 1위 모델인 DeepSeekCoder-6.7B에 매우 가깝지만 CodeLlama-7B와 StarCoderBase-7B 모두 성능을 상당히 능가한다.\n' +
      '3. StarCoder2-15B는 CodeLlama-13B 및 StarCoderBase-15B를 모두 포함하는 모든 대형 모델보다 상당히 우수하다.\n' +
      '4. 사실, StarCoder2-15B는 그 크기가 두 배 이상인 CodeLlama-34B 및 DeepSeekCoder-33B를 능가한다.\n' +
      '\n' +
      '#### CRUXEval: 코드 추론, 이해 및 실행\n' +
      '\n' +
      '**벤치마크** CRUXEval(Gu et al., 2024)은 코드 추론, 이해 및 실행을 평가하기 위해 설계된 800개의 샘플로 구성된 2-부분 벤치마크이다. 첫 번째 작업인 CRUXEval-I에서, 모델은 그 입력에 대해 주어진 파이썬 함수를 실행하는 것이 주어진 출력을 생성하도록 임의의 입력을 예측하도록 요청된다. 두 번째 작업인 CRUXEval-O에서 모델은 입력에 대한 주어진 함수의 실행을 시뮬레이션하고 출력을 예측하도록 요청된다. 목록 4와 5에 두 개의 샘플이 나와 있는데, 벤치마크의 함수 및 입력은 CodeLlama-34B에 의해 생성된 다음 필터링되어 복잡한 산술 또는 많은 실행 단계가 필요한 함수들과 같은 복잡한 함수들을 제거하였다.\n' +
      '\n' +
      '```\n' +
      'deff(string): string_x=string.strip(\'a") string=string_x.rstrip(\'e") returnstring\n' +
      '```\n' +
      '#outputprediction,CRUXEval-0 assertf(\'xxxxxaee")==?\n' +
      '#inputprediction,CRUXEval-I assertf(??)==\'xxxxaa\' ```\n' +
      '**목록 4**샘플 크룩스밸 문제 1\n' +
      '`` deff(nums): count=len(nums foriinrange(-count+1,0): nums.append(nums[i]) returnnums\n' +
      '#outputprediction,CRUXEval-0 assertf([2,6,1,3,1])==??\n' +
      '#inputprediction,CRUXEval-I assertf(??)==\'xxxxaa\' ```\n' +
      '\n' +
      '**목록 5**샘플 크룩스밸 문제 2\n' +
      '\n' +
      '**Hyperparameters** Following (Gu et al., 2024), 우리는 10개의 샘플을 사용하여, pass@1을 보고하기 위해 온도 0.2를 사용하고, pass@5를 보고하기 위해 온도 0.8을 사용한다.\n' +
      '\n' +
      '**결과** 표 15의 벤치마크에서 두 작업에 대한 pass@1 및 pass@5 점수를 보여줍니다. 오류 및 표준 편차 측면에서 원본 논문은 두 가지 노이즈 소스를 보고합니다. 먼저, 주어진 800개의 후보 집합에 대한 언어 모델로부터의 샘플링으로 인한 잡음은 10개의 샘플에 대해 약 0.2%이다. 둘째,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c} \\hline \\hline\n' +
      '**Model** & **GSM8K (PAL)** \\\\ \\hline StarCoderBase-3B & 8.0 \\\\ DeepSeekCoder-1.3B & 12.6 \\\\ StableCode-3B & **39.7** \\\\ StarCoder2-3B & 27.7 \\\\ \\hline StarCoderBase-7B & 14.9 \\\\ DeepSeekCoder-6.7B & **41.9** \\\\ CodeLlama-7B & 27.0 \\\\ StarCoder2-7B & 40.4 \\\\ \\hline StarCoderBase-15B & 21.5 \\\\ CodeLlama-13B & 38.1 \\\\ StarCoder2-15B & **65.1** \\\\ \\hline CodeLlama-34B & 54.2 \\\\ DeepSeekCoder-33B & **58.7** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 14: GSM8K 수학-추론 벤치마크에 대한 8-샷 정확도.\n' +
      '\n' +
      '벤치마크에서 정확한 샘플은 더 큰 샘플 세트에서 선택되었으며 800개의 샘플을 사용할 때 벤치마크에 포함할 샘플을 선택하는 노이즈는 약 1.5%이다. 우리는 다음과 같은 관찰을 한다.\n' +
      '\n' +
      '1. StarCoder2-3B는 다른 소형 모델들과 경쟁적으로 수행한다. 그것은 CRUXEval-I에서 StableCode-3B를 약간 능가하지만(그러나 오차의 노이즈 마진 내에서) CRUXEval-O의 다른 모든 소형 모델을 능가한다.\n' +
      '2. 두 작업 모두에 대해, StarCoder2-7B는 CodeLlama-7B와 동등하게 수행하지만 DeepSeekCoder-6.7B에 비해 상당히 뒤처진다.\n' +
      '3. StarCoder2-15B는 가장 성능이 좋은 대형 모델이다. 그것은 CodeLlama-13B를 능가하고 CRUXEval-I와 CRUXEval-O 모두에서 StarCoderBase-15B를 대폭 개선한다.\n' +
      '4. StarCoder2-15B는 초대형 모델들과 동등하게 수행한다. CRUXEval-I에서는 CodeLlama-34B와 DeepSeekCoder-33B 모두보다 성능이 우수하나 표준편차 이내이다. CRUXEval-O에서는 CodeLlama-34B를 크게 능가하고 DeepSeekCoder-33B를 약간 능가한다.\n' +
      '\n' +
      '### Fill-in-the-Middle\n' +
      '\n' +
      '벤치마크 StarCoder2에 대해서는 삽입점 전후의 텍스트에서 조건화된 임의의 범위의 코드를 완성할 수 있는 기능인 FIM(fill-in-the-middle)을 지원한다. 우리는 HumanEval에 대한 Python, JavaScript 및 Java 솔루션에서 단일 코드 라인을 채우는 모델의 능력을 테스트하는 Ben Allal et al.(2023)의 벤치마크를 사용한다.\n' +
      '\n' +
      'Ben Allal et al.(2023)에 후속하는 하이퍼파라미터들, 우리는 온도 0.2 및 Top-p 0.95에서 예제당 20개의 완성들을 샘플링하고, 수행된 바와 같이, 평균 정확한 일치를 보고한다.\n' +
      '\n' +
      '결과는 표 16에 나와 있다. 우리는 StarCoder2-3B가 이 FIM 벤치마크에서 StarCoderBase-15B만큼 잘 수행함을 관찰한다. 불행히도, StarCoder2-15B는 FIM에서 성능이 떨어진다. 구현 버그로 인해 FIM-레이트는 대부분의 교육을 위한 의도보다 작았다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{2}{c}{**CRUXEval-I**} & \\multicolumn{2}{c}{**CRUXEval-O**} \\\\ \\cline{2-5}  & Pass@1 & Pass@5 & Pass@1 & Pass@5 \\\\ \\hline StarCoderBase-3B & 27.1 & 43.7 & 27.4 & 40.9 \\\\ DeepSeekCoder-1.3B & 27.8 & 44.7 & 31.0 & 43.4 \\\\ StableCode-3B & **33.5** & **53.3** & 26.7 & 43.5 \\\\ StarCoder2-3B & 32.7 & 50.1 & **34.2** & **48.4** \\\\ \\hline StarCoderBase-7B & 29.7 & 47.3 & 32.2 & 44.9 \\\\ CodeLlama-7B & 35.9 & 52.9 & 34.2 & 48.4 \\\\ DeepSeekCoder-6.7B & **41.9** & **62.7** & **43.5** & **54.8** \\\\ StarCoder2-7B & 34.6 & 53.5 & 36.0 & 52.0 \\\\ \\hline StarCoderBase-15B & 31.3 & 49.2 & 34.2 & 47.1 \\\\ CodeLlama-13B & 42.5 & 62.0 & 39.7 & 53.9 \\\\ StarCoder2-15B & **48.1** & **66.9** & **47.1** & **59.5** \\\\ \\hline CodeLlama-34B & **47.2** & **66.6** & 42.4 & 55.9 \\\\ DeepSeekCoder-33B & 46.5 & 64.9 & **48.6** & **61.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 15: CRUXEval 벤치마크에 대한 정확도.\n' +
      '\n' +
      '### 리포지토리 수준 코드 완성도 평가\n' +
      '\n' +
      '실제로 코드 완료는 종종 격리된 파일이 아닌 리포지토리의 컨텍스트 내에서 발생합니다. 따라서 모델들이 실제 시나리오에서 잘 수행하기 위해서는 코드 완성을 위한 리포지토리 수준의 컨텍스트를 활용하는 것이 필수적이다. 우리는 RepoBench (Liu et al., 2023b)와 CrossCodeEval (Ding et al., 2023)의 두 벤치마크를 사용하여 리포지토리 레벨 코드 완성에 대한 모델을 평가한다.\n' +
      '\n' +
      '#### 7.6.1 RepoBench\n' +
      '\n' +
      '벤치마크RepoBench(Liu et al., 2023b)에 대해서는, 다음-라인 예측에 초점을 두고, 리포지토리 레벨에서 코드 완성을 평가하기 위해 설계된 라이브 벤치마크이다. 이 연구에서는 RepoBench28,29의 최신 버전(v1.1)을 사용하여 2023년 10월 6일부터 12월 31일까지 작성된 GitHub 리포지토리에서 데이터를 출처하고 Stack v2에 대한 중복을 제거하여 데이터 유출을 방지하기 위한 조치를 취하며, 각 설정은 5,000개의 데이터 포인트(레벨당 1,000개)로 구성된 크로스 파일 우선, 크로스 파일 랜덤, 인 파일의 세 가지 설정에서 5개의 레벨(2k, 4k, 8k, 12k, 16k)을 포함한다. 우리는 세 가지 설정에 대한 평균 편집 유사성, 정확한 일치 및 CodeBLEU(Ren et al., 2020) 점수를 보고한다.\n' +
      '\n' +
      '코드 LLMs에 대한 이전 작업(Chen et al., 2021)에 따라, 우리는 평가 중인 모든 모델에 대해 생성 온도를 0.2로 설정하고 상위\\(p\\) 샘플링 매개변수를 0.95로 설정했다. 프롬프트당 최대 128개의 새로운 토큰을 생성하도록 모델을 제한했으며 출력의 첫 번째 비공백 및 비주석 라인을 예측으로 선택했다. StarCoder2는 리포지토리 수준 교육을 위해 특수 토큰을 사용하는 반면 Liu et al.(2023b)에 따라 공식 구현에 따라 모든 모델에 걸쳐 신속한 구성의 균일성을 보장했다. 프롬프트에 대한 최대 토큰 카운트는 8k의 최대 시퀀스 길이 한계로 인해 7,800개의 토큰으로 제한되었던 StarCoderBase를 제외하고 초과 크로스 파일 컨텍스트를 잘라내어 15,800개로 설정되었다.\n' +
      '\n' +
      '결과 표 17은 RepoBench v1.1에 대한 오픈 액세스 모델의 성능을 나타낸다.\n' +
      '\n' +
      '1. 리포지토리 수준의 교육을 받은 StarCoder2는 평가된 모든 모델 크기에서 StarCoderBase보다 일관되게 우수합니다.\n' +
      '2. StarCoder2-3B는 StableCode-3B에 이어 두 번째로 우수한 모델로서 소형 모델들 사이에서 주목할 만한 성능을 보여준다.\n' +
      '3. StarCoder2-7B는 매체 모델 중 CodeLlama-7B와 거의 일치하는 경쟁 성능을 달성하며, DeepSeekCoder-6.7B는 선도적인 성능 메트릭을 달성한다.\n' +
      '4. StarCoder2-15B는 CodeLlama-13B를 크게 능가할 뿐만 아니라 비교 가능한 성능을 보여주며, 일부 메트릭에서는 상당히 큰 CodeLlama-34B 모델에 비해 성능이 우수하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline\n' +
      '**Model** & **Java** & **JavaScript** & **Python** \\\\ \\hline StableCode-3B & 63.7 & 73.3 & 59.1 \\\\ StarCoder2-3B & 75.0 & 73.0 & 59.1 \\\\ \\hline StarCoder2-7B & 81.1 & 77.5 & 61.1 \\\\ \\hline CodeLlama-13B & 80.0 & 85.0 & 74.5 \\\\ StarCoderBase-15B & 73 & 74 & 62 \\\\ StarCoder2-15B & 60.5 & 54.7 & 48.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 16: Exact-match on FIM-task (Ben Allal et al., 2023). 구현 버그로 인해, FIM은 StarCoder2-15B의 대부분의 훈련에서 부정확했다. CodeLlama 결과는 Rozière et al.(2023)에서 나온 것이다.\n' +
      '\n' +
      '#### 7.6.2 CrossCodeEval\n' +
      '\n' +
      'BenchmarkCrossCodeEval(Ding et al., 2023)에 대해서는 리포지토리 레벨 코드 완성을 위해 설계된 다양하고 다국어 벤치마크이다. 그것은 파이썬, 자바, 타입스크립트, C#의 네 가지 인기 있는 프로그래밍 언어로 광범위한 실세계 오픈소싱, 허가된 리포지토리로 구성되었다. 세심한 정적 분석 방법을 통해 CrossCodeEval **strictly**는 정확한 코드 완료를 위해 크로스 파일 컨텍스트를 필요로 한다. Ding et al.(2023)의 정의에 따른 Code Match(Edit Similarity)와 Identifier Match(F1 Score)의 결과를 4가지 언어 모두에서 보고한다.\n' +
      '\n' +
      '하이퍼파라미터는 8k만 지원하는 StarCoderBase를 제외한 모든 모델에 대해 16k의 최대 시퀀스 길이를 사용한다. Ding et al.(2023)에 따르면, 우리는 OpenAI의 ada 임베딩과 함께 RG( retrieve-and-generate) 방법을 사용하는데, 이는 그들의 연구에서 잘 수행되는 것으로 밝혀졌다. 확장된 16k 컨텍스트의 사용을 최적화하기 위해, 각각의 파일 경로와 10줄의 코드로 구성된 최대 100개의 코드 세그먼트를 검색한다. 최대 교차 파일 컨텍스트는 12,800개의 토큰으로 설정되었으며 최대 생성 토큰은 다음 50개의 토큰이다. Ding et al.(2023)과 일관되게, 우리는 모든 모델 세대에 대해 0.2의 온도 및 0.95의 top-p를 갖는, 원래 구현에서 균일한 프롬프트 포맷팅을 사용한다.\n' +
      '\n' +
      '결과표 18은 평가 결과를 제시한다. 우리가 찾은 건\n' +
      '\n' +
      '1. 모델 크기, 프로그래밍 언어 및 메트릭을 포함한 거의 모든 차원들에서, StarCoder2는 스타CoderBase를 일관되게 능가한다. 이러한 개선은 컨텍스트 길이 및 리포지토리 수준 목표를 증가시킨 더 나은 사전 훈련에 기인할 수 있다(섹션 5.1).\n' +
      '2. StarCoder2-15B는 유사한 크기의 모델들과 비교하여 최첨단 성능을 달성한다. 자바 및 C#과 같은 특정 언어의 경우 2x 용량의 모델보다 성능이 더 우수합니다.\n' +
      '3. 분석 결과, MultiPL-E(SS7.1.2)의 결과와 유사하게 동일한 모델에 대해 서로 다른 언어에서 유의미한 성능 분산이 나타났다. 모델은 전반적으로 강할 수 있지만, 모든 프로그래밍 언어에 걸쳐 균일하게 높은 성능을 달성하는 것은 여전히 어려운 것으로 남아 있다. 예를 들어, 스타코더2-15B는 TypeScript에서 뒤쳐져 있는 반면, C#의 StableCode-3B 및 Java의 DeepSeeKoder-34B는 마찬가지이다. 이 차이는 다양한 환경에서 다양한 언어에 걸쳐 높은 성능을 달성할 수 있는 모델 구축에 대한 향후 연구를 요구한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{3}{c}{**Python**} & \\multicolumn{3}{c}{**Java**} \\\\ \\cline{2-7}  & **EM** & **ES** & **CB** & **EM** & **ES** & **CB** \\\\ \\hline StarCoderBase-3B & 29.99 & 69.37 & 36.77 & 36.01 & 74.18 & 45.30 \\\\ DeepSeeKoder-1.3B & 31.02 & 70.07 & 37.88 & 37.75 & 75.66 & 46.69 \\\\ StableCode-3B & **34.48** & **71.79** & **40.43** & **40.13** & **76.56** & **49.00** \\\\ StarCoder2-3B & 32.47 & 71.19 & 39.25 & 38.46 & 76.53 & 47.96 \\\\ \\hline StarCoderBase-7B & 32.70 & 71.08 & 39.48 & 37.97 & 75.66 & 47.47 \\\\ CodeLama-7B & 33.85 & 71.79 & 40.47 & 39.61 & 76.71 & 48.92 \\\\ DeepSeeKoder-6.7B & **36.79** & **73.85** & **42.65** & **42.87** & **78.93** & **51.69** \\\\ StarCoder2-7B & 33.72 & 72.07 & 40.34 & 39.84 & 77.23 & 48.96 \\\\ \\hline StarCoderBase-15B & 33.51 & 71.64 & 40.39 & 39.34 & 76.24 & 48.36 \\\\ CodeLama-13B & 35.50 & 72.98 & 42.02 & 41.27 & 77.57 & 50.26 \\\\ StarCoder2-15B & **36.99** & **74.08** & **43.25** & **42.57** & **79.05** & **51.45** \\\\ \\hline CodeLama-34B & 37.22 & 73.77 & 43.38 & 42.35 & 78.22 & 50.99 \\\\ DeepSeeKoder-33B & **39.25** & **75.20** & **45.21** & **44.59** & **79.92** & **52.70** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 17: RepoBench v1.1(Liu et al., 2023b) 상의 오픈-액세스 베이스 모델들에 대한 평균 정확 매칭(EM), 편집 유사성(ES), 및 코드BLEU(CB) 점수들.\n' +
      '\n' +
      '#### "키보드에서 잠들다" 보안 벤치마크\n' +
      '\n' +
      '벤치마크인 "Asleep at the Keyboard"는 코드 생성 시 보안 취약점을 평가하기 위해 설계된 벤치마크이다(Pearce et al., 2022). Li 등(2023)과 유사하게, 우리는 Weakness_문제들의 _Diversity of Weakness_문제들인 자동화된 평가에 순응하는 태스크들의 서브세트에 초점을 맞춘다. 이는 MITRE에서 발행한 2021 CWE Top 25 가장 위험한 소프트웨어 취약성 목록에서 시나리오와 함께 MITRE 공통 취약성 열거(CWE) 분류의 18가지 다양한 취약성 클래스를 포함한다. 문제는 C에서 23개의 시나리오, 파이썬에서 17개의 시나리오를 가지고 있다.\n' +
      '\n' +
      'Li 등(2023)에 이어 하이퍼파라미터는 온도를 0.2, top-p를 0.95로 설정하였으며 각 모델은 시나리오당 25개의 샘플을 생성하여 총 1,000개의 완성도를 얻었다.\n' +
      '\n' +
      '결과는 표 19에서 선택된 모델의 결과를 보고한다. 열 **Valid**는 구문적으로 유효한 솔루션의 백분율을 제공하고 열 **Insecure**는 시나리오 테스트가 수행하는 취약성을 포함하는 유효한 솔루션의 백분율을 보여준다. 표로부터, 우리는 다음의 결론을 도출한다:\n' +
      '\n' +
      '1. StarCoder2는 StarCoderBase, CodeLlama 및 DeepSeekCoder에 필적할만한 수의 유효한 프로그램을 생성한다. 스타코더베이스 및 스타코더2 모델 모두 90% 유효 프로그램 달성\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{2}{c}{**Python**} & \\multicolumn{2}{c}{**Java**} & \\multicolumn{2}{c}{**TypeScript**} & \\multicolumn{2}{c}{**C\\#**} \\\\ \\cline{2-9}  & **Code ES** & **ID F1** & **Code ES** & **ID F1** & **Code ES** & **ID F1** & **Code ES** & **ID F1** \\\\ \\hline StarCoderBase-3B & 69.47 & 62.56 & 66.43 & 59.77 & 41.42 & 35.26 & 70.11 & 53.15 \\\\ DeepSeekCoder-1.3B & 72.41 & 66.76 & 65.92 & 59.93 & 63.59 & 56.41 & **70.98** & 54.84 \\\\ StableCode-3B & **76.00** & **70.75** & **73.19** & **67.93** & **65.61** & **59.61** & 61.70 & 48.98 \\\\ StarCoder2-3B & 73.01 & 67.85 & 66.31 & 61.06 & 38.79 & 35.17 & 70.86 & **55.42** \\\\ \\hline StarCoderBase-7B & 72.24 & 65.40 & 69.91 & 64.12 & 44.21 & 39.77 & 71.93 & 55.98 \\\\ DeepSeekCoder-6.7B & **77.43** & **73.16** & 70.60 & **66.28** & **69.08** & **63.61** & **74.84** & **62.29** \\\\ CodeLlama-7B & 74.52 & 69.11 & **71.49** & 65.99 & 65.96 & 59.46 & 71.41 & 56.66 \\\\ StarCoder2-7B & 74.52 & 68.81 & 70.75 & 65.27 & 43.19 & 38.84 & 72.73 & 57.69 \\\\ \\hline StarCoderBase-15B & 73.43 & 66.74 & 70.58 & 64.66 & 45.24 & 40.47 & 71.77 & 55.71 \\\\ CodeLlama-13B & 75.88 & 70.97 & 73.08 & 68.29 & **67.88** & **61.46** & 72.73 & 59.62 \\\\ StarCoder2-15B & **78.72** & **74.27** & **74.92** & **70.45** & 48.63 & 43.78 & **75.38** & **62.14** \\\\ \\hline CodeLlama-34B & 76.34 & 71.36 & **74.30** & **69.45** & 68.98 & 63.19 & 73.96 & 60.07 \\\\ DeepSeekCoder-33B & **78.78** & **74.51** & 73.41 & 69.02 & **70.31** & **65.14** & **75.04** & **63.03** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 18: CrossCodeEval(Ding et al., 2023) 평가 결과. 4개 언어에 대한 코드 일치(유사성 편집) 및 식별자 일치(F1) 결과를 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline\n' +
      '**Model** & **Valid (\\(\\uparrow\\))** & **Insecure (\\(\\downarrow\\))** \\\\ \\hline StarCoderBase-3B & 910/1000 (91.0\\%) & 224/910 (24.6\\%) \\\\ DeepSeekCoder-1.3B & 893/1000 (89.3\\%) & 290/893 (32.5\\%) \\\\ StarCoder2-3B & **925/1000 (92.5\\%)** & **113/900 (12.2\\%)** \\\\ \\hline StarCoderBase-7B & 916/1000 (91.6\\%) & 243/916 (26.5\\%) \\\\ CodeLlama-7B & 900/1000 (90.0\\%) & **195/900 (21.7\\%)** \\\\ DeepSeekCoder-6.7B & **921/1000 (92.1\\%)** & 315/921 (34.2\\%) \\\\ StarCoder2-7B & 912/1000 (91.2\\%) & 363/926 (39.8\\%) \\\\ \\hline StarCoderBase-15B & **933/1000 (93.3\\%)** & 332/933 (35.6\\%) \\\\ CodeLlama-13B & 903/1000 (90.3\\%) & **273/903 (30.2\\%)** \\\\ StarCoder2-15B & 898/1000 (89.8\\%) & 352/898 (39.2\\%) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 19: "키보드에서 잠들기" 벤치마크에 대한 성능.\n' +
      '\n' +
      '배율. 그러나, 일부 수동 검사 후에, 우리는 StarCoder2가 StarCoderBase보다 기능적으로 더 정확한 코드를 생성하는 경향이 있다는 것을 알게 된다. 관측치는 이전 섹션의 평가와 정렬됩니다.\n' +
      '2. StarCoder2-3B, StarCoder2-7B 및 StarCoder2-15B를 제외하고, 유사한 파라미터를 갖는 모델들 중에서 가장 높은 비보안 프로그램 비율을 갖는다. 높은 불안정률은 더 높은 유효하고 기능적으로 올바른 완성도의 부작용일 수 있다. 이러한 생성된 프로그램은 Bhatt et al.(2023)이 제안한 바와 같이 잠재적인 취약성에 노출될 가능성이 더 높다. 특히, Li 등(2023)은 상용화된 Copilot에서 사용되는 초기 모델인 code-cushman-001이 40% 이상의 불안정한 비율을 갖는다는 것을 발견한다.\n' +
      '\n' +
      '유해 세대 측정\n' +
      '\n' +
      'BenchmarkmarksBias in Open-ended Language Generation Dataset (BOLD)(Dhamala et al., 2021)은 종교사상, 과정, 인종, 성별, 정치사상 전반에 걸쳐 공정성을 측정하는 23,679개의 프롬프트의 데이터셋이다. Sheng et al.(2019)에 의한 Regard 분류기를 사용하고 분류기 점수를 각 범주에 걸쳐 평균화한다.\n' +
      '\n' +
      'WinoBias(Zhao et al., 2018)는 직업 전반에 걸친 성 고정관념에 대한 편견을 측정한다. 직업에 대한 문장이 주어지면 성별을 마스킹하고 언어 모델을 사용하여 대체된 마스킹 토큰을 생성하고 당혹감을 사용하여 남성과 여성 대명사에 대한 편향을 계산한다.\n' +
      '\n' +
      'Hurtful Sentence Completion in English Language Models (HONEST)(Nozza et al., 2021)은 언어 모델들에 의해 생성된 문장 완성들의 상처를 평가하기 위한 템플릿 기반 코퍼스이다. HONEST 점수를 계산하기 위해 생성된 각 완료가 허틀렉스30 내의 각 범주에 포함된 항이 있는지 확인한다.\n' +
      '\n' +
      '각주 30: [https://github.com/valeribosalle/hurtlex](https://github.com/valeribosalle/hurtlex)\n' +
      '\n' +
      'RealToxicityPrompts(Gehman et al., 2020)는 영어 텍스트의 대형 웹 코퍼스에서 추출한 100,000개의 자연 발생 문장 수준 프롬프트로 구성된다. 언어 모델에서 신경 독성 퇴화의 위험을 평가하는 데 사용할 수 있다. 우리는 평가를 수행하기 위해 10,000개의 부분 집합을 사용한다. Vidgen et al.(2021)의 분류기를 사용하여 독성을 검출하고 검출된 독성 출력의 평균 확률을 독성 점수로 보고한다.\n' +
      '\n' +
      'BOLD 및 RealToxicity Prompts의 각 프롬프트에 대해 하이퍼파라미터는 최대 50개의 추가 토큰으로 하나의 완료를 생성한다. HONEST에서 우리는 최대 50개의 추가 토큰으로 각 샘플에 대해 5개의 완성을 생성한다.\n' +
      '\n' +
      'BOLD, WinoBias, HONEST 및 RealToxicityPrompts에 대한 결과는 각각 표 20, 21, 22 및 23에 나와 있다. 표들은 우리가 고려하는 모델 LLMs들이 대략 동일한 양의 유해 콘텐츠를 생성하고, Li 등(2023)에 기초하여, 주로 코드에 대해 트레이닝된 LLMs들이 일반 웹 텍스트 상에서 트레이닝된 LLMs들보다 덜 유해한 콘텐츠를 생성함을 시사한다.\n' +
      '\n' +
      '##8 검색 색인과 귀인 도구\n' +
      '\n' +
      'Li 등(2023)의 표준 세트에 따라 데이터 검사, 속성 및 검색 도구의 또 다른 세트를 구축한다. NLP 커뮤니티는 데이터 검사의 필요성을 인식하고 정적 데이터 설명을 보완하기 위해 계산 문서화 아티팩트를 생성하기 시작했다(Piktus et al., 2023; Marone and Van Durme, 2023; Piktus et al., 2023; Akiki et al., 2023). 개방형 과학 및 개방형 데이터는 데이터 세트의 덤프를 방출하는 것을 넘어선다.\n' +
      '\n' +
      '멤버십 검사 도구 이 작업은 스타코더베이스에서 사용되는 것보다 4배 더 큰 데이터 세트를 수집하고 구성한다. Stack의 초기 버전과 비교하여, 여기 버전은 많은 추가의 비-코드 소스들을 포함한다(표 4 참조). 데이터 크기가 증가함에 따라 접근 가능하고 효율적인 데이터 검사를 허용하는 도구를 구성하는 것이 훨씬 더 중요해졌다. 우리는 "Am I in the Stack" 도구를 새로운 데이터 세트의 리포지토리로 업데이트한다.31 이 도구를 사용하면 사용자 이름과 리포지토리 수준에서 데이터를 검사할 수 있다. Marone and Van Durme(2023)은 경량 멤버십 검사를 지원하기 위해 _Data Portrait_라는 문서화 아티팩트를 공개할 것을 권고한다. 우리는 Bloom 필터를 사용하여 문서, 교과서, 논문과 같은 비코드 소스를 포함하여 파일 내용에 대한 매칭이 가능하도록 구현한다.32 이러한 산문 데이터 소스는 문서, 교과서, 논문과 같은 비코드 소스를 포함할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline\n' +
      '**Model** & **Category** & **Negative Score** & **Neutral Score** & **Other Score** & **Positive Score** \\\\ \\hline \\multirow{4}{*}{StarCoder2-3B} & Religious Ideology & 0.16 & 0.33 & 0.13 & 0.38 \\\\  & Profession & 0.07 & 0.6 & 0.06 & 0.27 \\\\  & Race & 0.05 & 0.5 & 0.05 & 0.5 \\\\  & Gender & 0.05 & 0.48 & 0.05 & 0.43 \\\\  & Political Ideology & 0.3 & 0.29 & 0.18 & 0.23 \\\\ \\hline \\multirow{4}{*}{StarCoderBase-3B} & Religious Ideology & 0.12 & 0.32 & 0.12 & 0.45 \\\\  & Profession & 0.07 & 0.58 & 0.06 & 0.3 \\\\  & Race & 0.04 & 0.44 & 0.05 & 0.47 \\\\  & Gender & 0.04 & 0.35 & 0.05 & 0.55 \\\\  & Political Ideology & 0.3 & 0.27 & 0.18 & 0.25 \\\\ \\hline \\multirow{4}{*}{StableCode-3B} & Religious Ideology & 0.18 & 0.25 & 0.16 & 0.41 \\\\  & Profession & 0.08 & 0.57 & 0.06 & 0.28 \\\\  & Race & 0.07 & 0.4 & 0.06 & 0.46 \\\\  & Gender & 0.05 & 0.36 & 0.06 & 0.53 \\\\  & Political Ideology & 0.32 & 0.27 & 0.18 & 0.25 \\\\ \\hline \\multirow{4}{*}{StarCoder2-7B} & Religious Ideology & 0.19 & 0.81 & 0.03 & 0.13 \\\\  & Profession & 0.08 & 0.52 & 0.07 & 0.33 \\\\  & Race & 0.06 & 0.4 & 0.07 & 0.47 \\\\  & Gender & 0.06 & 0.37 & 0.07 & 0.5 \\\\  & Political Ideology & 0.33 & 0.22 & 0.21 & 0.24 \\\\ \\hline \\multirow{4}{*}{StarCoderBase-7B} & Religious Ideology & 0.16 & 0.28 & 0.13 & 0.43 \\\\  & Profession & 0.07 & 0.56 & 0.06 & 0.31 \\\\ StarCoderBase-7B & Race & 0.05 & 0.41 & 0.06 & 0.48 \\\\  & Gender & 0.04 & 0.33 & 0.06 & 0.57 \\\\  & Political Ideology & 0.33 & 0.23 & 0.19 & 0.25 \\\\ \\hline \\multirow{4}{*}{CodeLlama-7B} & Religious Ideology & 0.16 & 0.27 & 0.14 & 0.43 \\\\  & Profession & 0.07 & 0.58 & 0.06 & 0.3 \\\\ CodeLlama-7B & Race & 0.06 & 0.42 & 0.06 & 0.46 \\\\  & Gender & 0.05 & 0.38 & 0.06 & 0.5 \\\\  & Political Ideology & 0.3 & 0.28 & 0.19 & 0.24 \\\\ \\hline \\multirow{4}{*}{DeepSeeKoder-6.7B} & Religious Ideology & 0.15 & 0.33 & 0.13 & 0.39 \\\\  & Profession & 0.07 & 0.61 & 0.06 & 0.27 \\\\  & Race & 0.05 & 0.46 & 0.05 & 0.44 \\\\  & Gender & 0.04 & 0.34 & 0.06 & 0.56 \\\\  & Political Ideology & 0.3 & 0.28 & 0.19 & 0.23 \\\\ \\hline \\multirow{4}{*}{StarCoder2-15B} & Religious Ideology & 0.21 & 0.22 & 0.16 & 0.42 \\\\  & Profession & 0.09 & 0.51 & 0.07 & 0.33 \\\\  & Race & 0.07 & 0.39 & 0.07 & 0.47 \\\\  & Gender & 0.05 & 0.36 & 0.07 & 0.53 \\\\  & Political Ideology & 0.25 & 0.02 & 0.1 & 0.09 \\\\ \\hline \\multirow{4}{*}{StarCoderBase-15B} & Religious Ideology & 0.16 & 0.31 & 0.13 & 0.41 \\\\  & Profession & 0.07 & 0.61 & 0.06 & 0.26 \\\\ StarCoderBase-15B & Race & 0.06 & 0.46 & 0.06 & 0.43 \\\\  & Gender & 0.04 & 0.38 & 0.06 & 0.53 \\\\  & Political Ideology & 0.32 & 0.28 & 0.19 & 0.22 \\\\ \\hline \\multirow{4}{*}{CodeLlama-13B} & Religious Ideology & 0.17 & 0.24 & 0.14 & 0.45 \\\\  & Profession & 0.07 & 0.54 & 0.06 & 0.33 \\\\ CodeLlama-13B & Race & 0.07 & 0.36 & 0.07 & 0.5 \\\\  & Gender & 0.05 & 0.35 & 0.06 & 0.53 \\\\  & Political Ideology & 0.3 & 0.23 & 0.19 & 0.28 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 20: 오픈 소스 코드 모델들의 BOLD 평가들.\n' +
      '\n' +
      '다른 곳에 존재하지 않는 알고리즘 또는 솔루션을 설명합니다. 콘텐츠 제작자는 우리 시스템에서 간단한 "코드 없음" 검사 도구로 사용하여 데이터에서 자료가 그대로 발생하는지 확인할 수 있습니다. 또한, 본 논문에서 제안한 모델에 구축된 코딩 툴에 대한 빠른 1차 패스 속성 검사를 가능하게 한다.33 이 시스템은 약 70GB가 소요되며, 이는 데이터보다 상당히 작지만 긴 문자열에 대한 정확한 일치만을 제공한다. 필요에 따라 사용자는 추가 분석을 위해 전체 검색 지수를 사용할 수 있다.\n' +
      '\n' +
      '각주 33: [https://github.com/huggingface/llm-vscode](https://github.com/huggingface/llm-vscode)\n' +
      '\n' +
      '인덱스 검색 선행 도구는 경량 데이터 검사를 제공합니다. 그러나 퍼지 매칭 및 검색을 지원하는 전체 텍스트 검색을 수행해야 할 수도 있다. StarCoder1(Li et al., 2023)에 이어, Stack v2의 소스 코드 서브셋 상에 Elasticsearch 인덱스를 구축하고 [https://huggingface.co/spaces/bigcode/search-v2](https://huggingface.co/spaces/bigcode/search-v2)에서 사용할 수 있도록 한다.\n' +
      '\n' +
      '##9 사회적 영향과 한계\n' +
      '\n' +
      '사회적 영향 및 한계는 BigCode 프로젝트에서 이미 문서화되었다(Kocetkov et al., 2023; Ben Allal et al., 2023; Li et al., 2023; BigCode collaboration et al., 2023). 다음 섹션에서는 코드에 대한 대규모 언어 모델의 책임 있는 개발을 위한 프로젝트 접근 방식을 다루고 최근 몇 가지 발전을 강조한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline\n' +
      '**Model** & **Toxicity Score** \\\\ \\hline StarCoder2-3B & 0.05 \\\\ StarCoderBase-3B & 0.04 \\\\ StableCode-3B & 0.05 \\\\ \\hline StarCoder2-7B & 0.08 \\\\ StarCoderBase-7B & 0.04 \\\\ CodeLama-7B & 0.04 \\\\ DeepSeeKoder-6.7B & 0.05 \\\\ \\hline StarCoder2-15B & 0.05 \\\\ StarCoderBase-15B & 0.04 \\\\ CodeLlama-13B & 0.04 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 23: 오픈 소스 코드 모델의 독성 점수 평가.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline\n' +
      '**Model** & **Scale** & **Average** \\\\ \\hline StarCoder2-3B & 0.33 & -0.33 \\\\ StarCoderBase-3B & 0.42 & -0.42 \\\\ StableCode-3B & 0.44 & -0.44 \\\\ \\hline StarCoder2-7B & 0.45 & -0.45 \\\\ StarCoderBase-7B & 0.51 & -0.51 \\\\ CodeLlama-7B & 0.37 & -0.37 \\\\ DeepSeeKoder-6.7B & 0.41 & -0.41 \\\\ \\hline StarCoder2-15B & 0.36 & -0.36 \\\\ StarCoderBase-15B & 0.55 & -0.55 \\\\ CodeLlama-13B & 0.36 & -0.36 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\begin{tabular}{c c} \\hline \\hline\n' +
      '**Model** & **Score** \\\\ \\hline StarCoder2-3B & 0.11 \\\\ StarCoderBase-3B & 0.01 \\\\ StarCoderBase-7B & 0.11 \\\\ CodeLlama-7B & 0.11 \\\\ DeepSeeKoder-6.7B & 0.1 \\\\ \\hline StarCoder2-15B & 0.11 \\\\ StarCoderBase-15B & 0.1 \\\\ CodeLlama-13B & 0.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 21: 오픈 소스 코드 모델의 WinoBias 평가.\n' +
      '\n' +
      '### Project Approach\n' +
      '\n' +
      '오픈 사이언스 스타코더2는 커뮤니티 연구 프로젝트의 결과물이다. 이 프로젝트는 코드 LLM의 책임 있는 개발과 사용에 중점을 두고 열린 과학(Woelfle et al., 2011; Mendez et al., 2020)의 정신으로 수행된다. 개방 거버넌스 관행을 통해 의사 결정의 우선 순위는 채택 또는 향후 연구에 영향을 미칠 수 있는 한계를 도입하는 것을 의미하더라도 항상 더 책임 있는 옵션에 양보했다(BigCode collaboration et al., 2023).\n' +
      '\n' +
      '윤리적 데이터 소싱 빅코드 커뮤니티의 중요한 노력은 스타코더2 훈련에 사용된 데이터에 대해 세심한 큐레이션, 검증, 오염 제거, 악성코드 제거, 라이선스 필터링, 옵트아웃 프로세스, PII 제거, 구조화, 패키징, 호스팅, 라이선스, 데이터세트 카드 발행(프로젝트, 2024)에 들어갔고, 스타코더2 훈련에 사용된 데이터에 대해 완전한 투명성이 제공되었으며, 훈련 데이터 세트의 상당 부분은 소프트웨어 헤리티지(소프트웨어 헤리티지, 2024a)의 라이선스로 조달되었다.\n' +
      '\n' +
      '연구 BigCode의 과학 협력에 대한 개방형 접근(BigCode collaboration et al., 2023), 오픈 액세스 모델 배포 및 라이센싱(BigCode Project, 2023a; Malfa et al., 2023), 및 훈련 데이터, 아키텍처 및 개발의 개방 및 공개는 연구 커뮤니티가 강력하고 진정으로 개방된 LLM에 대한 액세스를 갖기 위해 필수적이며, 향후 연구를 가속화하는 것을 돕는다(Groeneveld et al., 2024; Xu et al., 2024; Soldaini et al., 2024; Singh et al., 2024; Ustun et al., 2024; Luukkonen et al., 2023; Woelfle et al., 2011).\n' +
      '\n' +
      'Open, but responsible the BigCode Open RAIL-M 라이센스(BigCode Project, 2023a)는 중요한 사용 제한을 포함하며, 다운스트림 사용자에 의한 모델의 책임 있는 배포 및 사용을 안내하는 데 도움이 되는 FAQ를 동반한다(BigCode Project, 2023b).\n' +
      '\n' +
      '연습 커뮤니티 빅코드는 코드에 대한 대형 언어 모델의 책임 있는 개발을 위해 일하는 60개 이상의 국가로부터 1,200개 이상의 다학제 회원이 있는 연습 커뮤니티이다(Sholler et al., 2019; Koetkov et al., 2023; Ben Allal et al., 2023; Li et al., 2023; Muenninghoff et al., 2024; Zhuo et al., 2024). 이들 회원 중 417명은 2023년 10월 27일부터 2024년 2월 24일까지 BigCode 커뮤니티 협업 도구에서 활동했으며, 이는 StarCoder2 개발과 일치하는 기간이다. 또한 Hugging Face API(BigCode, 2024)를 통해 수백만 개의 다운로드가 일괄적으로 보고되는 빅코드 출력의 상당한 다운스트림 채택이 있었다.\n' +
      '\n' +
      'AuditableThe StarCoder2 모델, 사전-트레이닝 데이터세트, 및 지원 아티팩트는 독립 감사를 실시하고자 하는 모든 사람이 쉽게 액세스가능하고 이용가능하다(Solaiman, 2023; Mokander et al., 2023; BigCode collaboration et al., 2023).\n' +
      '\n' +
      '코드 LLMs에서의### 발전\n' +
      '\n' +
      '거버넌스 카드 더 빅코드 거버넌스 카드(BigCode collaboration et al., 2023)는 빅코드 프로젝트에서 거버넌스의 다양한 메커니즘 및 영역에 대한 개요 역할을 한다. 프로젝트 기간 동안 이루어진 선택에 대한 관련 정보를 광범위한 대중에게 제공함으로써 투명성을 지원하고 향후 노력이 자신의 접근법을 형성하기 위해 활용할 수 있는 개방형 연구 프로젝트의 의도적 거버넌스(Sholler et al., 2019)의 예 역할을 하는 것을 목표로 한다. 첫 번째 섹션인 프로젝트 구조는 프로젝트 조직, 명시된 목표와 가치, 내부 결정 프로세스, 자금 및 자원을 다룬다. 두 번째 섹션인 데이터 및 모델 거버넌스는 데이터 주체 동의, 개인 정보 보호 및 모델 릴리스의 질문과 관련된 결정을 다룬다.\n' +
      '\n' +
      '소프트웨어 메타데이터의 보관: 소프트웨어 메타데이터는 무료 및 오픈 소스 소프트웨어(FOSS)의 분류, 큐레이션 및 공유를 위해 필수적이다. 소스 코드 풍경은 매우 다양합니다. 개발자 및 과학자의 글로벌 커뮤니티(Heritage, 2024)로부터 소프트웨어 Heritage 아카이브 내의 링크된 데이터를 생성하고 소스 코드 기여도를 참조함으로써, LLMs(Cosmo and Zacchiroli, 2017; Abramatic et al., 2018)를 훈련시키기 위한 보다 윤리적인 데이터 공급망을 가능하게 할 가능성이 있다.\n' +
      '\n' +
      '허용 ML 사용:2023년 10월 19일 소프트웨어 헤리티지는 소프트웨어 헤리티지 아카이브의 허용 가능한 기계 학습 사용을 정의하는 성명을 발표했다. 이는 AI 훈련 데이터의 보다 책임 있는 데이터 소싱 및 라이선스의 문을 여는 중요한 이정표이다(소프트웨어 헤리티지, 2023).\n' +
      '\n' +
      'SoftWare Hash IDentifier (SWHID): 소프트웨어 헤리티지는 소프트웨어 컴포넌트에 본질적으로 바인딩되어 있고 중앙 레지스트리가 필요하지 않은 SWHID 고유 식별자를 제공하여 소프트웨어 헤리티지 아카이브(SWHID Specification Project, 2024)의 상단에 탄력적인 지식 웹이 구축될 수 있도록 한다. 이것은 또한 다운스트림 개발자들에 의해 소프트웨어 보안 및 소프트웨어 공급망 투명성 및 위험 관리(Cybersecurity & Infrastructure Security Agency, 2024; Mirakhorli et al., 2024)에서 핵심 빌딩 블록으로서 "소프트웨어 자재 청구서"(SBOM)를 우선시하는 회사들에 대한 노력을 지원하기 위해 사용될 수 있으며, 예를 들어 컴포넌트 이름, 버전, 라이센스 및 소스 위치와 같은 다른 관련 정보와 함께 SWHID를 SBOM에 포함시킴으로써 수행될 수 있다.\n' +
      '\n' +
      '"도전과 위험"\n' +
      '\n' +
      '솔라이만(2023)은 LLM 개발 과정의 개방 정도가 모델 출시와 관련된 잠재적 위험과 어떻게 연결되는지를 설명한다. 시스템이 완전히 폐쇄적으로 개발될 경우, 자원이 많은 조직에 전력이 집중될 가능성이 높으며, 소규모 개발 팀은 배치 중인 모델의 영향과 장기적인 결과를 완전히 이해하지 못할 수 있다. 또한, 폐쇄 개발 시스템은 종종 외부 전문가에 의해 감사할 수 없고 연구자들이 서로의 작업을 기반으로 할 수 없기 때문에 과학적 진보를 방해할 수 있다. 반면에 완전 개방형 개발은 커뮤니티 연구를 허용하고 모델에 대한 액세스를 민주화하며 전체 개발 프로세스 전반에 걸쳐 감사를 가능하게 한다. 그러나 적절한 가드레일이 없으면 개방형 LLM 개발은 모델 액세스가 증가하면 모델로 인한 손상 가능성도 증가하기 때문에 오용 위험이 더 높다. 해제된 API는 종료될 수 있지만 일단 모델 가중치가 해제되면 이를 철회하는 것은 거의 불가능하다. 따라서 책임 있는 AI 관행에 대해 논의하고 구현하는 것은 우리 프로젝트의 LLM을 개발하는 동안 가장 중심이 되었다.\n' +
      '\n' +
      '개인 정보 준수 생성 코드는 코드를 통한 개인 데이터 처리, 변환 및 흐름이 평가될 수 있도록 상이한 유형의 PII를 정확하게 식별하고 분류하기가 어렵다(Tang et al., 2023). 생성된 코드에서 프라이버시 관련 메소드가 호출되는 경우, 인터넷으로의 PII 유출, 암호화된 데이터 및 익명 ID의 사용이 필요할 것이다(Tang & Ostvold, 2024). 다운스트림 사용자는 의도된 사용 사례에 대한 컴플라이언스를 보장하기 위해 추가적인 PII 스캐닝, 필터링, 클렌징 및 완화를 구현하는 것이 권고된다(Yang et al., 2023; Albalak et al., 2024).\n' +
      '\n' +
      '모델 가중치, 하이퍼 파라미터, 데이터 처리 코드, 트레이닝 코드, 트레이닝 데이터 및 문서화에 대한 오픈 액세스를 제공하는 임의의 개방형 과학 연구와 마찬가지로, 임의의 행위자는 매우 낮은 컴퓨팅 비용으로 최적화된 모델을 실행하거나 미세 조정할 수 있다(거버넌스 AI, 2024). BigCode Open RAIL-M 라이센스에 명시된 사용 제한에도 불구하고, 이것은 악의적인 의도를 가진 나쁜 행위자들이 해를 입히려고 시도하는 것을 방지하지 못할 것이다(Mozes et al., 2023). 예를 들어, API 액세스를 갖는 코드 LLM은 시그너처 기반 검출에 의존하는 보안 제품에 매우 회피적일 수 있는 정교한 다형성 맬웨어(CrowdStrike, 2024)를 생성하는데 사용될 수 있고, 결국 코드를 실행하고 실행함에 따라 안티 맬웨어 스캐닝 인터페이스(Anti-Malware Scanning Interface, AMSI)와 같은 측정을 우회할 수 있을 것이다(CyberArk, 2024; Gupta et al., 2023).\n' +
      '\n' +
      '사회 편향은 코딩 모델의 평가에서 이전에 확립된 바와 같이, 코드 LLM은 성별, 인종, 감정, 계급, 명칭의 구조 및 기타 특성에 대한 고정관념을 반영하는 구조로 코드를 생성할 수 있다(Chen et al., 2021; Zhuo et al., 2023a). 하류 사용 사례(Huang et al., 2023; Dong et al., 2024)의 맥락에서 추가적인 평가 및 가드레일 완화가 요구된다.\n' +
      '\n' +
      '이전 섹션에서 논의한 표현 편향은 해스켈 및 포트란과 같은 틈새 언어보다 파이썬 및 자바와 같은 인기 있는 프로그래밍 언어에 대한 훈련 데이터 세트에 훨씬 더 많은 데이터가 있다. 따라서, 모델은 그러한 높은 리소스 언어에 대해 더 나은 성능을 발휘하며, 이는 그러한 언어를 사용하는 것에 대한 개발자의 선호도를 강화할 수 있다. 다행히도 저자원 언어에 대한 코드 LLM의 성능을 향상시키는 방법에 대한 연구가 많이 진행되고 있다(Cassano et al., 2023a; Zhuo et al., 2023b). 또한, 소스 코드 및 사용되는 다른 데이터 세트에서 우세한 자연 언어는 다른 언어도 존재하지만 영어이다. 이와 같이, 모델은 일부 비영어 컨텍스트를 제공하는 코드 스니펫을 생성할 수 있지만, 생성된 코드는 의도한 대로 또는 모든 언어에 대해서도 동일하게 작동하도록 보장되지 않는다. 이는 다양한 코딩 작업 및 환경에 걸쳐 모델의 공정성 및 유효성을 제한할 수 있다(Alyafeai et al., 2024).\n' +
      '\n' +
      '소프트웨어 구성요소를 추적하기 위해 SWHID를 사용하는 추적성은 쉬운 작업이 아니며 전부는 아니더라도 대부분의 다운스트림 개발자에게 도전할 것이다. 소프트웨어 구성요소를 더 쉽게 추적할 수 있도록 하는 도구의 향후 개발 및 발전은 보다 투명하고 책임 있는 데이터 공급망을 가능하게 하는 데 필요할 것이다(Cosmo et al., 2020).\n' +
      '\n' +
      '직업 확대 대 직업 확대 자동화 코드 LLM은 고품질 코드, 문서화, 단위 테스트, 텍스트 요약, 자동화 워크플로우 등을 생성하기 위해 미세 조정될 수 있는 강력한 기반 모델 역할을 합니다. Chen et al.(2023)은 직업 노출과 임금 수준/경험 프리미엄 사이에 양의 상관관계를 발견하여, 더 높은 급여와 경험 집약적인 직업이 LLM 구동 소프트웨어로부터 더 큰 변위 위험에 직면할 수 있음을 시사한다. 골드만삭스(2024)는 AI가 선진국에서 25%, 신흥국에서 10-20%의 노동 과제를 자동화할 가능성이 있다고 제안하지만, "AI는 전문화된 인간의 전문 지식이 필요한 새로운 일자리 과제나 범주를 만들 가능성이 있기 때문에 이러한 두려움이 상쇄되어야 한다"고 명시하기도 한다. Autor et al.(2022)은 "2018년 고용의 약 60%가 1940년에 존재하지 않았던 직책에서 발견된다"며 "증강 혁신은 직업 노동 수요를 증가시키는 반면 자동화 혁신은 이를 잠식한다"고 보고한다. 세계 경제 포럼(World Economic Forum, 2024)의 과제 기반 분석 결과, LLM에 의한 과제 자동화의 가능성이 가장 높은 일자리는 일상적이고 반복적인 절차를 강조하며 높은 수준의 대인 커뮤니케이션을 요구하지 않는 것으로 나타났다. LLM에 의한 증대 가능성이 가장 높은 직업은 비판적 사고와 복잡한 문제 해결 능력, 특히 과학, 기술, 공학, 수학(STEM) 분야의 일을 강조한다. Ziegler et al.(2024)은 코딩이 작업 시간, 제품 품질, 인지 부하, 즐거움 및 학습과 같은 일반적으로 조사된 생산성 측면의 전체 범위에 걸쳐 있는 동안 AI 제안을 수신하는 것의 이점을 보고한다. 구글 코어와 구글 리서치(Brain Team)의 2년 협업인 펭 등(Peng et al., 2023)에서는 10k+ 구글 내부 개발자 중 IDE에서 코드 완성 설정을 사용하여 25-34%의 사용자 코드 수용률을 측정하였다. 야후 파이낸스(2024)는 ServiceNow, Inc.를 발표했다. (NYSE: NOW) 2024 Q4 ServiceNow 플랫폼이 text-to-code(ServiceNow, 2024b)와 text-to-workflow(ServiceNow, 2024a) LLMs(StarCoder 기반)를 사용하여 서비스나우 플랫폼 Now Assist 스킬을 제공하는 커버리지를 통해 수익을 창출하고 개발자 생산성과 혁신 속도를 52% 향상시킵니다.\n' +
      '\n' +
      '## 10 Conclusion\n' +
      '\n' +
      '우리는 소프트웨어 헤리티지 아카이브의 기초 위에 구축된 코드 LLM의 가장 큰 사전 훈련 코퍼스인 더 스택 v2와 함께 코드 생성을 위해 설계된 LLM의 패밀리인 StarCoder2를 도입했다. 스택 v2는 이전보다 10배 더 커서 67.5 TB의 원시 데이터 세트를 생성한다. 다른 고품질 코드 관련 데이터 세트의 통합과 함께 소스 코드의 광범위한 청소, 필터링 및 서브샘플링을 통해 약 3TB(900B+ 토큰)의 훈련 세트를 생성했다. 이 새로운 데이터 세트를 활용하여 3B, 7B 및 15B 매개변수로 StarCoder2 모델을 훈련했다. 코드 완성, 편집 및 추론 기능을 평가하는 광범위한 코드 LLM 평가는 StarCoder2-3B와 StarCoder2-15B가 각각의 크기 클래스 내에서 최첨단 모델임을 보여주었다. 모델 가중치를 공개할 뿐만 아니라 훈련 데이터에 대한 완전한 투명성을 보장함으로써 개발된 모델에 대한 신뢰를 높이고 다른 엔지니어링 팀과 과학자가 우리의 노력을 기반으로 할 수 있도록 권한을 부여하기를 바란다.\n' +
      '\n' +
      '## 11 Acknowledgements\n' +
      '\n' +
      '이 작업은 소프트웨어 헤리티지, 소스 코드의 위대한 라이브러리인 [https://www.softwareheritage.org](https://www.softwareheritage.org) 및 오픈 소스 아카이브에 기여하는 모든 개발자와 과학자들에 의해 가능해졌다.\n' +
      '\n' +
      '우리는 평가에 사용되는 컴퓨팅 자원을 제공한 Joydeep Biswas (UT Austin), Northeastern Research Computing, 그리고 NCSA Delta에 감사한다. 캐롤린 제인 앤더슨과 아르준 구하는 미국 국립과학재단 시상식 SES-2326173과 SES-2326174의 일부 후원을 받았고 지아웨이 류, 유샹웨이, 링밍 장은 미국 국립과학재단 시상식 CCF-2131943의 일부 후원을 받았으며 페데리코 까사노는 로블록스의 일부 후원을 받았다.\n' +
      '\n' +
      '서비스나우, 포옹 페이스, 엔비디아의 스타코더2 리서치 콜라보레이션 협정을 실행한 제니 후이, 서비스나우에게 감사드린다.\n' +
      '\n' +
      '지속적인 지원과 커뮤니티에 대한 다운스트림 기여에 대해 빅코드 커뮤니티의 확장된 구성원에게 감사드립니다.\n' +
      '\n' +
      '우리는 또한 PII 수정에 대한 초기 탐색 및 고려 동안 개인 정보 정의 및 복구 프레임워크에서 작업에서 얻은 통찰력과 교훈을 공유한 헤시 존스와 개인 정보 보호 콜라브에 감사한다.\n' +
      '\n' +
      '에브게니 Zheltonozhskii는 이스라엘 과학 인문학 아카데미의 아담스 펠로우십 프로그램의 지원을 받는다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Abramatic et al. (2018) Jean-Francois Abramatic, Roberto Di Cosmo, and Stefano Zacchiroli. 소스 코드의 유니버설 아카이브 구축 ACM_, 61(10):29-31, 2018. doi: 10.1145/3183558. URL[https://cacm.acm.org/magazines/2018/10/231366-building-the-universal-archive-of-source-code/fulltext](https://cacm.acm.org/magazines/2018/10/231366-building-the-universal-archive-of-source-code/fulltext] (cited on pp.3 and 37)\n' +
      '* Ainslie et al. (2023) Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: 다중 헤드 체크포인트에서 일반화된 다중 쿼리 변압기 모델을 훈련합니다. Houda Bouamor, Juan Pino, and Kalika Bali(eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 4895-4901, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.298. URL[https://aclanthology.org/2023.emnlp-main.298](https://aclanthology.org/2023.emnlp-main.298) (p. 20) 인용\n' +
      '* Akiki et al. (2022) Christopher Akiki, Giada Pistilli, Margot Mieskes, Matthias Galle, Thomas Wolf, Suzana Ilic, and Yacine Jernite. 빅사이언스: 다국어 대규모 언어 모델의 사회적 구성에 대한 사례 연구. _Workshop on Broadening Research Collaborations 2022_, 2022. URL[https://openreview.net/forum?id=2e34612PP0m](https://openreview.net/forum?id=2e34612PP0m). (p. 2)에 인용됨\n' +
      '* Akiki et al. (2023) Christopher Akiki, Odunayo Ogundepo, Aleksandra Piktus, Xinyu Zhang, Akintunde Oladipo, Jimmy Lin, and Martin Potthast. 스페이시니: 피세리니와 포옹 페이스가 있는 플러그 앤 플레이 검색 엔진입니다. In Yansong Feng and Els Lefever(eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pp. 140-148, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-demo.12. URL[https://aclanthology.org/2023.emnlp-demo.12](https://aclanthology.org/2023.emnlp-demo.12) (p. 34. 인용)\n' +
      '* Albalak et al. (2024) Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, and William Yang Wang. 언어 모델에 대한 데이터 선택에 대한 조사. _ arXiv preprint_, February 2024. URL[https://arxiv.org/abs/2402.16827](https://arxiv.org/abs/2402.16827). (p. 38) 인용\n' +
      '* Alyafaei et al. (2024) Zaid Alyafaei, Khalid Almubarak, Ahmed Ashraf, Deema Alnuhait, Saied Alshahrani, Gubran A. Q. Abdulrahman, Gamil Ahmed, Qais Gawah, Zead Saleh, Mustafa Ghaleb, Yousef Ali, and Maged S. 알샤이바니 CIDAR: 문화적으로 아랍어를 위한 명령어 데이터셋. _ arXiv preprint_, February 2024. URL[https://arxiv.org/abs/2402.83177](https://arxiv.org/abs/2402.83177). (p. 39에 인용됨)\n' +
      '* Arivazhagan et al. (2019) Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, Wolfgang Macherey, Zhifeng Chen, and Yonghui Wu. 야생에서 대규모 다국어 신경 기계 번역: 발견과 과제 arXiv preprint_, July 2019. URL[https://arxiv.org/abs/1907.055019](https://arxiv.org/abs/1907.055019). (p. 14. 인용)\n' +
      '* Arxiv(2024) Arxiv, 2024. URL[https://info.arxiv.org/help/bulk_data_s3.html](https://info.arxiv.org/help/bulk_data_s3.html)이다. (p. 12. 인용)\n' +
      '* Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc V. 르와 찰스 서튼 대용량 언어 모델을 사용한 프로그램 합성 arXiv preprint_, August 2021. URL[https://arxiv.org/abs/2108.07732](https://arxiv.org/abs/2108.07732). (cited on pp. 3 and 23)\n' +
      '* Autor et al. (2022) David Autor, Caroline Chin, Anna M Salomons, and Bryan Seegmiller. New Frontiers: The origin and contents of new work, 1940-2018. Technical Report 30389, National Bureau of Economic Research, August 2022. URL[http://www.hber.org/papers/w30389](http://www.hber.org/papers/w30389). (p. 39에 인용됨)\n' +
      '* Azerbayev et al. (2024) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. 장, 지아 덩, 스텔라 바이더만, 션 웰렉 Llemma: 수학을 위한 열린 언어 모델. _The Twelfth International Conference on Learning Representations_, 2024. URL[https://openreview.net/forum?id=4WnqRR9915j](https://openreview.net/forum?id=4WnqRR9915j). (p. 12. 인용)\n' +
      '* Bavarian et al. (2022) Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. 중간을 채우기 위한 언어 모델의 효율적인 훈련. _ arXiv preprint_, July 2022. URL[https://arxiv.org/abs/2207.14255](https://arxiv.org/abs/2207.14255). (p. 16)* Ben Allal(2023) Loubna Ben Allal. 빅코드 모델 리더보드, 2023. URL[https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard) 인용: SS1.\n' +
      '* L. 벤 알 Muennighoff L. 쿠마르 우마파티, B. 립킨, L. 본 베르라(2022) 코드 생성 모델의 평가를 위한 프레임워크. 참고: [https://github.com/bigcode-project/bigcode-evaluation-harness](https://github.com/bigcode-project/bigcode-evaluation-harness) by 인용: SS1.\n' +
      '* L. 벤 알 Lee, D. Kocetkov, C. Mou, C. Akiki, C. Munoz Ferrandis, N. Muennighoff 미쉬라 A구 디엘 우마파티, C. J. 앤더슨, Y. 지재일 포이리에 트로신, 아불카노프, M. 로메로 래퍼트, F. 드 토니, B. 가르시아 델 리오, Q. 류승 보세우 T. Bhattacharyya Yue Zhuo, I. Yu, P. Villegas, M. 조카성 Mangrulkar, D. Lansky, H. Nguyen, D. Contractor, L. 빌라, J. 리, D. 바다나우, Y. 제르나이트 Hughes, D. Fried, A. Guha, H. de Vries, L. Von Werra (2023)SantaCoder: 별을 향해 손을 뻗지 마세요! _ arXiv preprint_, August 2023. External Links: 2301.03988 Cited by: SS1.\n' +
      '* M. Bhatt, S. 첸나바사파, C. 니콜라이디스, S. Wan, I. Evtimov, D. Gabi, D. Song, F. Ahmad, C. Aschermann, L. 폰타나, S 프로로프 기동카필 Kozyrakis, D. LeBlanc, J. Milazzo, A. Straumann, G. Synnaeve, V. 본티미타, S Whitman, and J. Saxe (2023) Purple llama CyberSecEval: 언어 모델에 대한 보안 코딩 벤치마크. ArXiv 프리프린트. 외부 링크: 2301.04724 인용: SS1.\n' +
      '* S. 바이더만 H. Schoelkopf, Q. 그레고리 앤서니, H 브래들리, K 오브라이언 E. 한라한, M. 칸승 Purohit, U. S. Prisnath, E. Raff, A. Skowron, L. 수타위카, O. Van Der Wal(2023)Pythia: 훈련과 스케일링에 걸쳐 대규모 언어 모델을 분석하기 위한 스위트룸. 기계학습연구회지, K. Krause, E. Brunskill, K. 조병하르트 Sabato, and J. Scarlett(Eds.), pp. 2397-2430. External Links: Link, Document Cited by: SS1.\n' +
      '* B. Code (2024)Models by BigCode on Hugging Face. 참고: URL[https://huggingface.co/api/models?author=bigcode&expand](https://huggingface.co/api/models?author=bigcode&expand][]=downloadsAllTimeAccessed: 2024 Cited by: SS1).\n' +
      '* B. C. 콜라보레이션, S. H. de Vries, J. Robinson, C. Munoz Ferrandis, L. 벤 앨, L. 본 베르라, J. 딩, S. 다발, Y. Jernite (2023) The BigCode 프로젝트 거버넌스 카드. ArXiv 프리프린트. 외부 링크: 인용된 2312.03872: SS1.\n' +
      '* B. Project(2023)BigCode 모델 라이선스 계약서. 참고: [https://huggingface.co/space/bigcode/bigcode-model-license-agreementAccessed](https://huggingface.co/space/bigcode/bigcode-model-license-agreementAccessed): 2023 Cited by: SS1.\n' +
      '* B. Project (2023)BigCode open RAIL: responsible AI licensing framework. 참고: URL[https://www.bigcode-project.org/docs/pages/bigcode-openrail/Accessed](https://www.bigcode-project.org/docs/pages/bigcode-openrail/Accessed): 2023 Cited by: SS1.\n' +
      '* B. C. Workshop(2024)URL[https://blueoakcouncil.org/list](https://blueoakcouncil.org/list) Cited by: SS1.\n' +
      '* B. C. Council(2024) 근중복 문서 식별 및 필터링. In Annual symposium on combinatorial pattern matching, pp. 1-10. Cited by: SS1.\n' +
      '* E. Caballero, O. AI, I. Sutskever(2016)Description2Code dataset. 참고: URL[https://github.com/ethancaballero/description2code](https://github.com/ethancaballero/description2code) Cited by: SS1.\n' +
      '* E. Caballero, O.\n' +
      '\n' +
      '* Cassano et al. (2023) Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg, Abhinav Jangda, and Arjun Guha. 코드 LLMs. _ arXiv preprint_, August 2023a. URL[https://arxiv.org/abs/2308.09895](https://arxiv.org/abs/2308.09895).\n' +
      '* Cassano et al. (2023) Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. 펠드먼, 아르준 구하, 마이클 그린버그, 아비나브 장다 MultiPL-E: 신경망 코드 생성을 벤치마킹하기 위한 확장성 및 폴리글로트 접근법. _ IEEE Transactions on Software Engineering_, 49(7):3675-3691, 2023b. 도이: 10.1109/TSE.2023.3267446. URL[https://www.computer.org/csd/journal/ts/2023/07/10183177/1MpMUtj7Rwk](https://www.computer.org/csd/journal/ts/2023/07/10183177/1MpMUtj7Rwk)\n' +
      '* Cassano et al. (2024) Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby Brennan-Jones, Anton Lozhkov, Carolyn Jane Anderson, and Arjun Guha. 편집할 수 있나요? 코드 편집 지침을 따르는 대형 언어 모델의 기능을 평가하는 중입니다. _The First International Workshop on Large Language Model for Code_, 2024. URL[https://arxiv.org/abs/2312.12450](https://arxiv.org/abs/2312.12450).\n' +
      '* Chai et al. (2023) Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, and Hua Wu. ERNIE-code: 프로그래밍 언어에 대한 영어 중심 교차 언어 사전 교육을 넘어. Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki(eds.), _Findings of the Association for Computational Linguistics: ACL 2023_, pp. 10628-10650, Toronto, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.676. URL[https://aclanthology.org/2023.findings-acl.676](https://aclanthology.org/2023.findings-acl.676).\n' +
      '* Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 코드에서 훈련된 대규모 언어 모델 평가. _ arXiv preprint_, July 2021. URL[https://arxiv.org/abs/2107.08374](https://arxiv.org/abs/2107.08374).\n' +
      '* Chen et al. (2023) Qin Chen, Jinfeng Ge, Huaqing Xie, Xingcheng Xu, and Yanqing Yang. 중국의 노동 시장에서 대규모 언어 모델들이 일하고 있다. _ arXiv preprint_, August 2023. URL[https://arxiv.org/abs/2308.08776](https://arxiv.org/abs/2308.08776).\n' +
      '* Chowdhery et al.(2021) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Joshua Maynez, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyun택 Lim, Barret Zoph, Shivani Agrawal, Mark Omernick, Andrew M. 다이, 타누말라야 산카라야나 필라이, 마리 펠랏, 에토르 르코위츠, 에리카 모레라, 르원 차일드, 올렉산드르 폴로조프, 캐서린 리, 종웨이 주, 슈에지 왕, 브레넌 새타, 마크 디아즈, 오르한 피라트, 미셸 카타스타, 제이슨 웨이, 캐시 마이어-헬스턴, 더글러스 엑, 제프 딘, 슬라브 페트로프, 노아 피델. Palm: 경로를 이용한 언어 모델링 스케일링 Journal of Machine Learning Research_, 24(240):1-113, 2023. URL[http://jmlr.org/papers/v24/22-1144.html](http://jmlr.org/papers/v24/22-1144.html)이다.\n' +
      '* Christiano et al. (2021) Paul F. Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 인간의 선호로부터 심층 강화 학습. 인가연 본 룩스버그 Bengio, H. Wallach, R. 퍼거스 비슈와나단, R. 가넷(eds.), _Advances in Neural Information Processing Systems_, volume 30.\n' +
      '\n' +
      'Curran Associates, Inc., 2017. URL[https://proceedings.neurips.cc/paper_files/paper/2017/hash/d5e2c@addad583c91f24d6cd4e49-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2017/hash/d5e2c@addad583c91f91df24d6cd4e49-Abstract.html)\n' +
      '* ClamAV(2024) ClamAV, 2024. URL[https://www.clamav.net/](https://www.clamav.net/).\n' +
      '* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 수학 단어 문제를 해결하기 위한 검증자 훈련 arXiv preprint_, October 2021. URL[https://arxiv.org/abs/2110.14168](https://arxiv.org/abs/2110.14168).\n' +
      '* Conneau et al. (2020) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 규모 면에서 비지도 교차 언어 표현 학습. Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault(eds.), _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 8440-8451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL[https://aclanthology.org/2020.acl-main.747](https://aclanthology.org/2020.acl-main.747).\n' +
      '* Osmo and Zacchiroli (2017) Roberto Di Osmo and Stefano Zacchiroli. 소프트웨어 유산: 소프트웨어 소스 코드를 보존하는 이유와 방법. [iPRES 2017: 제14차 국제회의 on Digital Preservation_, Kyoto, Japan, 2017. URL[https://www.softwareheritage.org/wp-content/uploads/2820/01/ipres-2017-swh.pdf](https://www.softwareheritage.org/wp-content/uploads/2820/01/ipres-2017-swh.pdf]. [https://hal.archives-ouvertes.fr/hal-01590958] (https://hal.archives-ouvertes.fr/hal-01590958).\n' +
      '* Osmo et al. (2020) Roberto Di Osmo, Morane Gruenpeter, and Stefano Zacchiroli. 소스 코드 아티팩트 참조: 소프트웨어 인용에 대한 별도의 우려 사항. _ Computing in Science & Engineering_, 22(2):33-43, 2020. doi: 10.1109/MCSE.2019.2963148.\n' +
      '* 크라우드스트라이크(2024) 크라우드스트라이크. 다형성 바이러스[https://www.crowdstrike.com/cybersecurity-101/malware/polymorphic-virus/] (https://www.crowdstrike.com/cybersecurity-101/malware/polymorphic-virus/), 2024. Accessed: 2024.\n' +
      '* CyberArk(2024) CyberArk. 다형성 맬웨어를 만들기 위해 대화를 나누고 있습니다. [https://www.cyberark.com/resources/threat-research-blog/chatting-our-way-into-creating-a-polymorphic-malware] (https://www.cyberark.com/resources/threat-research-blog/chatting-our-way-into-creating-a-polymorphic-malware), 2024. Accessed: 2024.\n' +
      '* 사이버보안 및 기반시설 보안청(2024) 사이버보안 및 기반시설 보안청. 디자인에 의한 보안, 2024. URL[https://www.cisa.gov/resources-tools/resources/secure-by-design](https://www.cisa.gov/resources-tools/resources/secure-by-design]. 접속: 2024.\n' +
      '* Dao(2024) Tri Dao. 플래시 어텐션-2: 더 나은 병렬성과 작업 분할로 더 빠른 어텐션. _The Twelfth International Conference on Learning Representations_, 2024. URL[https://openreview.net/forum?id=mZn2Xyh9Ec](https://openreview.net/forum?id=mZn2Xyh9Ec).\n' +
      '* Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 플래시 어텐션: IO 인식으로 빠르고 기억력이 효율적인 정확한 어텐션. 인석 고예조 모하메드, A. 아가왈, D. 벨그레이브, K. Cho, and A. Oh(eds.), _Advances in Neural Information Processing Systems_, volume 35, pp. 16344-16359. Curran Associates, Inc., 2022. URL[https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20f6ba7a382cb32e20f6ba7a382cbc32e20f6ba7a382cbc81d36e48d5-Abstract-Conference.html](https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20f6ba7a382cbc81d36e48d5-Abstract-Conference.html](https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20f6ba7a382cbc81d5-Abstract-Conference.h\n' +
      '* 드 브리스(2023) Harm de Vries. 스몰이나 집에 가. [https://www.harmdevries.com/post/model-size-vs-compute-overhead/] (https://www.harmdevries.com/post/model-size-vs-compute-overhead/), 2023.\n' +
      '* Dhamala et al. (2021) Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. BOLD: 개방형 언어 생성에서 편향을 측정하기 위한 데이터세트 및 메트릭. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, FAccT\'21, pp. 862-872, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445924. URL[https://doi.org/10.1145/3442188.3445924](https://doi.org/10.1145/3442188.3445924)\n' +
      '* Ding et al. (2022) Jennifer Ding, Christopher Akiki, Yacine Jernite, Anne Lee Steele, and Temi Popo. 개방형 액세스를 넘어 개방성을 향해: 3개의 개방형 AI 협력체를 통한 사용자 이동. _Workshop on Broadening Research Collaborations 2022_, 2022. URL[https://openreview.net/forum?id=slu-5h8frCrz](https://openreview.net/forum?id=slu-5h8frCrz).\n' +
      '* Ding et al. (2021)) Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, and Bing Xiang. CrossCodeEval: 교차 파일 코드 완성을 위한 다양하고 다국어 벤치마크. IMT-2000 3GPP-30-7차 신경정보처리시스템 회의 : 데이터세트 및 벤치마크 Track_, 2023. URL[https://openreview.net/forum?id=w9OcbBM8fh](https://openreview.net/forum?id=w9OcbBM8fh) (cited on pp.3, 31, 32, 33)\n' +
      '* Dong et al. (2024) 이동, Ronghui Mu, Gaojie Jin, Yi Qi, Jinwei Hu, Xingyu Zhao, Jie Meng, Wenjie Ruan, 및 Xiaowei Huang. 대형 언어 모델을 위한 가드레일 구축 arXiv preprint_, February 2024. URL[https://arxiv.org/abs/2402.01822](https://arxiv.org/abs/2402.01822). (p. 38) 인용\n' +
      '* Ethayarajh et al. (2024) Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. KTO: 예측 이론 최적화로서의 모델 정렬. _ arXiv preprint_, February 2024. URL[https://arxiv.org/abs/2402.01306](https://arxiv.org/abs/2402.01306). (p. 27. 인용)\n' +
      '* Fan et al. (2023) Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M. 장 소프트웨어 엔지니어링을 위한 대규모 언어 모델: 조사 및 개방형 문제 arXiv preprint_, October 2023. URL[https://arxiv.org/abs/2310.03533](https://arxiv.org/abs/2310.03533). (p. 2)에 인용됨\n' +
      '* Gao et al. (2020) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, 및 Graham Neubig. PAL: 프로그램 지원 언어 모델. Andreas Krause, Emma Brunskill, Kyungyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett(eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 10764-10799. PMLR, 23-29 Jul 2023. URL[https://proceedings.mlr.press/v202/gao23f.html](https://proceedings.mlr.press/v202/gao23f.html). (p. 28) 인용\n' +
      '* Gehman et al. (2020) Samuel Gehman, 수친 Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: 언어 모델에서 신경 독성 퇴화를 평가한다. Trevor Cohn, Yulan He, and Yang Liu(eds.), _Findings of the Association for Computational Linguistics: EMNLP 2020_, pp. 3356-3369, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL[https://aclanthology.org/2020.findings-emnlp.301](https://aclanthology.org/2020.findings-emnlp.301) (cited on pp.3 and 34)\n' +
      '* Team et al. (2023) Gemini Team et al. Gemini: family of highly capable multimodal models. _ arXiv preprint_, 2023. URL[https://arxiv.org/abs/2312.11805](https://arxiv.org/abs/2312.11805). (p. 2)에 인용됨\n' +
      '* Archive(2024) Github Archive, 2024. URL[https://gharchive.org](https://gharchive.org). (cited on pp.3, 6, 7)\n' +
      '* go-enry (2024) go-enry, 2024. URL [https://github.com/go-enry/go-enry] (https://github.com/go-enry/go-enry] (cited on pp. 4 and 6)\n' +
      '* Sachs(2024) Goldman Sachs. 생성 세계 질서는 AI, 지정학, 파워, 2024. URL[https://www.goldmansachs.com/intelligence/pages/the-generative-world-order-ai-geopolitics-and-power.html](https://www.goldmansachs.com/intelligence/pages/the-generative-world-order-ai-geopolitics-and-power.html]이다. (p. 39에 인용됨)\n' +
      '* AI(2024) Governance AI. 개방 소싱 능력이 뛰어난 기반 모델, 2024. URL[https://www.governance.ai/research-paper/open-sourcing-highly-caply-foundation-models](https://www.governance.ai/research-paper/open-sourcing-highly-foundation-models) 접속: 2024. (p. 38. 인용)\n' +
      '* Groeneveld et al. (2020) Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nitchell Wortsman, Pradeep Dasigi, Kathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dasigi, Kyle Loldaini, Noah A. Smith 언어 모델 과학의 가속화 arXiv preprint_, February 2024. URL[https://arxiv.org/abs/2402.00838](https://arxiv.org/abs/2402.00838). (cited on pp.2 and 37)\n' +
      '* Grossman et al. (2023) Aiden Grossman, Ludger Paehler, Konstantinos Parasyris, Tal Ben-Nun, Jacob Hegna, William Moses, Jose M. 몬살브 디아즈, 미르세아 트로핀 요하네스 도어퍼트 ComPile: 생산원의 대규모 IR 데이터세트. _ arXiv preprint_, September 2023. URL[https://arxiv.org/abs/2309.15432](https://arxiv.org/abs/2309.15432). (cited on p. 11)* Gu et al. (2024) Alex Gu, Baptiste Roziere, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I. Wang. CRUEX 평가: 코드 추론, 이해 및 실행의 벤치마크. _ arXiv preprint_, January 2024. URL[https://arxiv.org/abs/2401.03065](https://arxiv.org/abs/2401.03065) (cited on pp. 3 and 29)\n' +
      '* 코드 인텔리전스의 상승. _ arXiv preprint_, 2024. URL[https://arxiv.org/abs/2401.14196](https://arxiv.org/abs/2401.14196). (cited on pp.2, 20, 22, 23, 25)\n' +
      '* Gupta et al. (2023) Maanak Gupta, Charankumar Akiri, Kshitiz Aryal, Eli Parker, and Lopamudra Praharaj. ChatGPT에서 ThreatGPT까지: 사이버 보안 및 개인 정보 보호에서 생성 AI의 영향 IEEE Access_, 11:80218-80245, 2023. ISSN 2169-3536. doi: 10.1109/access.2023.3300381. URL[http://dx.doi.org/10.1109/ACCESS.2023.3300381](http://dx.doi.org/10.1109/ACCESS.2023.3300381).\n' +
      '* Gutierrez-Fandino et al. (2022) Asier Gutierrez-Fandino, David Perez-Fernandez, Jordi Armengol-Estape, David Griol, and Zoraida Callejas. 스페인어 크롤링 코퍼스 _IberSPEECH 2022_, pp. 126-130, 2022. doi: 10.21437/IberSPEECH.2022-26. URL[https://www.isca-speech.org/archive/pdfs/iberspeech_2022/gutierrezfandino22_iberspeech.pdf](https://www.isca-speech.org/archive/pdfs/iberspeech_2022/gutierrezfandino22_iberspeech.pdf]). (p. 10. 인용)\n' +
      '* Hendrycks et al. (2021) Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 앱으로 코딩 도전 능력을 측정합니다. J. 밴쇼렌과 S. Yeung(eds.), _Proceedings of Neural Information Processing Systems Track on Datasets and Benchmarks_, volume 1. Curran, 2021. URL[https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49bb2a4bbe8a49bb2a028-Abstract-round2.html](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49bb2a028-Abstract-round2.html](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49bb2a028-Abstract-round2.html](h (p. 12. 인용)\n' +
      '* Heritage(2024) Software Heritage. 소프트웨어 유산 커뮤니티. [https://www.software] (https://www.software) heritage.org/community/, 2024. Accessed: 2024. (cited on p. 37)\n' +
      '* Hou et al. (2023) Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. 소프트웨어 공학을 위한 대규모 언어 모델: 체계적 문헌 고찰. _ arXiv preprint_, August 2023. URL[https://arxiv.org/abs/2308.10620](https://arxiv.org/abs/2308.10620). (p. 2)에 인용됨\n' +
      '* Huang et al. (2023) Dong Huang, Qingwen Bu, Jie Zhang, Xiaofei Xie, Junjie Chen, and Heming Cui. LLM 기반 코드 생성에서 바이어스 테스트 및 완화 arXiv preprint_, 2023. URL[https://arxiv.org/abs/2309.14345](https://arxiv.org/abs/2309.14345) (p. 38) 인용\n' +
      '* Jiang et al. (2023) Albert Q. 장, 알렉산드르 사블레이롤, 아서 멘쉬, 크리스 뱀포드, 데벤드라 싱 채플롯, 디에고 데 라스 카사스, 플로리안 브레산드, 지아나 령겔, 기욤 옴플, 루실 사울니에, 릴리오 레나르 라바우, 마리안 라초, 피에르 스톡, 테븐 르 스카오, 티보트 라브릴, 토마스 왕, 티모티 라크루아, 윌리엄 엘 사예드. 미스트랄 7B. _ arXiv preprint_, 2023. URL[https://arxiv.org/abs/2310.06825](https://arxiv.org/abs/2310.06825). (p. 2)에 인용됨\n' +
      '* Jiang et al. (2021) Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, and Yuhuai Wu. ISAbelle 증명의 언어 모델들 _6th Conference on Artificial Intelligence and Theorem Proving_, pp. 378-392, 2021. URL[http://aitp-conference.org/2021/abstract/paper_17.pdf](http://aitp-conference.org/2021/abstract/paper_17.pdf). (p. 12. 인용)\n' +
      '* Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. 아담: 확률적 최적화를 위한 방법. In Yoshua Bengio and Yann LeCun(eds.), _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015. URL[http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980). (p. 21. 인용)\n' +
      '* Koetkov et al. (2023) Denis Koetkov, Raymond Li, Loubna Ben Allal, Jia LI, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro Von Werra, and Harm de Vries. 스택: 3TB의 허가된 소스 코드 _ Machine Learning Research_, 2023. ISSN 2835-8856. URL[https://openreview.net/forum?id=pxpb7dUEpD](https://openreview.net/forum?id=pxpb7dUEpD) (cited on pp. 2, 4, 36, and 37)*Lacoste et al. (2019) Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 기계 학습의 탄소 배출량을 정량화합니다. _ arXiv preprint_, October 2019. URL[https://arxiv.org/abs/1910.09708](https://arxiv.org/abs/1910.09708).\n' +
      '* Lai et al. (2023) Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida Wang, and Tao Yu. DS-1000: 데이터 과학 코드 생성을 위한 자연스럽고 신뢰할 수 있는 벤치마크. Andreas Krause, Emma Brunskill, Kyungyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett(eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 18319-18345. PMLR, 23-29 Jul 2023. URL[https://proceedings.mlr.press/v202/lai23b.html](https://proceedings.mlr.press/v202/lai23b.html).\n' +
      '* Lattner and Adve (2004) Chris Lattner and Vikram Adve. LLVM: 평생 프로그램 분석 및 변환을 위한 컴파일 프레임워크. _International symposium on code generation and optimization, 2004. CGO 2004._, pp. 75-86. IEEE, 2004. URL[https://ieeexplore.ieee.org/document/1281665](https://ieeexplore.ieee.org/document/1281665).\n' +
      '*Li et al. (2023) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltnozhskii, Terry Yue Zhuo, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Joao Monteiro, Oleh Shliazhko, Nicholas Meade, Rudra Zocch, Manan Dey, Jashan Zillerman, Muhtasham Oblokulo Wang, Rudra Murthy, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jashan Dey, Jash 스타코더: 출처가 당신과 함께 할 수 있기를! _ arXiv preprint_, May 2023. URL[https://arxiv.org/abs/2305.06161](https://arxiv.org/abs/2305.06161).\n' +
      '* Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Kimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 알파벳 코드와 경쟁 레벨 코드 생성. _ Science_, 378(6624):1092-1097, 2022. doi:10.1126/science.abq1158. URL[https://www.science.org/doi/abs/10.1126/science.abq1158](https://www.science.org/doi/abs/10.1126/science.abq1158)을 포함할 수 있다.\n' +
      '* Liu et al. (2023a) Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 채팅GPT에 의해 생성된 코드가 정말 맞나요? 코드 생성을 위한 대규모 언어 모델에 대한 엄격한 평가. 30-7차 신경 정보 처리 시스템 회의, 2023a. URL[https://openreview.net/forum?id=1qvx618CU7](https://openreview.net/forum?id=1qvx618CU7).\n' +
      '* Liu et al. (2023b) Tianyang Liu, Canwen Xu, and Julian McAuley. RepoBench: Benchmarking repository-level code auto-completion systems. _ arXiv preprint_, June 2023b. URL[https://arxiv.org/abs/2306.03091](https://arxiv.org/abs/2306.03091)\n' +
      '* Longpre et al. (2023) Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, Xinyi Wu, Enrico Shippole, Kurt Bollacker, Tongshuang Wu, Luis Villa, Sandy Pentland, 및 Sara Hooker. 데이터 프로버넌스 이니셔티브: AI에서 데이터 세트 라이선싱 및 귀속에 대한 대규모 감사. _ arXiv preprint_, 2023. URL[https://arxiv.org/abs/2310.16787](https://arxiv.org/abs/2310.16787).\n' +
      '* Luukkonen et al. (2023) Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, et al. Fingpt: Large Generative models for small language. _ arXiv preprint arXiv:2311.05640_, 2023. URL[https://arxiv.org/abs/2311.05640](https://arxiv.org/abs/2311.05640).\n' +
      '* Luukkonen et al. (2023)* La Malfa et al. (2023) Emanuele La Malfa, Aleksandar Petrov, Simon Frieder, Christoph Weinhuber, Ryan Burnell, Raza Nazar, Anthony G. Cohn, Nigel Shadbolt, and Michael Wooldridge. 서비스로서의 언어 모델: 새로운 패러다임과 그 도전의 개요. _ arXiv preprint_, 2023. URL[https://arxiv.org/abs/2309.16573](https://arxiv.org/abs/2309.16573). (p. 37에 인용됨)\n' +
      '* Marone and Van Durme (2023) Marc Marone and Benjamin Van Durme. 데이터 초상화: 기초 모델 학습 데이터를 기록합니다. IMT-2000 3GPP-30-7차 신경정보처리시스템 회의 : 데이터세트 및 벤치마크 Track_, 2023. URL [https://arxiv.org/abs/2303.03919] (https://arxiv.org/abs/2303.03919) (cited on pp. 34 and 35)\n' +
      '* Community(2020) The mathlib Community. 희박한 수학 라이브러리 제9차 ACM SIGPLAN International Conference on Certified Programs and Proofs_, POPL\'20. ACM, January 2020. doi: 10.1145/3372885.3373824. URL[http://dx.doi.org/10.1145/3372885.3373824](http://dx.doi.org/10.1145/3372885.3373824)에 기재되어 있다. (p. 12. 인용)\n' +
      '* Mendez et al. (2020) Daniel Mendez, Daniel Graziotin, Stefan Wagner, and Heidi Seibold. _ Open Science in Software Engineering_, pp. 477-501. Springer International Publishing, 2020. doi: 10.1007/978-3-030-32489-6_17. URL[http://dx.doi.org/10.1007/978-3-030-32489-6_17](http://dx.doi.org/10.1007/978-3-030-32489-6_17)\n' +
      '* Merkle(1987) Ralph C. Merkle. 종래의 암호화 기능을 기반으로 하는 전자 서명. In _Conference on the theory and application of cryptographic techniques_, pp. 369-378. Springer, 1987. (cited on p. 3)\n' +
      '* Mirakhorli et al. (2024) Mehdi Mirakhorli, Derek Garcia, Schuyler Dillon, Kevin Laporte, Matthew Morrison, Henry Lu, Viktoria Koscinski, and Christopher Enoch. 소프트웨어 자재 청구서(sbm)를 위한 오픈 소스 및 독점 도구에 대한 경관 연구 arXiv preprint_, 2024. URL[https://arxiv.org/abs/2402.11151](https://arxiv.org/abs/2402.11151). (p. 38) 인용\n' +
      '* Mirzayanov(2020) Mike Mirzayanov. Codeforces: Results of 2020 [annual report]. [https://codeforces.com/blog/entry/89582]. (https://codeforces.com/blog/entry/89582), 2020. (p. 11. 인용)\n' +
      '* Mozes et al. (2023) Maximilian Mozes, Xuanil He, Bennett Kleinberg, and Lewis D. Griffin. LLM의 불법 사용: 위협, 예방 조치 및 취약성. _ arXiv preprint_, 2023. URL[https://arxiv.org/abs/2308.12833](https://arxiv.org/abs/2308.12833). (p. 38) 인용\n' +
      '*Q2 Earning Call (2024) MSFT Q2 Earning Call, 2024. URL[https://www.microsoft.com/en-us/investor/events/fy-2024/earnings-fy-2024-q2.aspx](https://www.microsoft.com/en-us/investor/events/fy-2024/earnings-fy-2024-q2.aspx) (p. 2)에 인용됨\n' +
      '* Muennighoff et al. (2022a) Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 메타브: 대량 텍스트 임베딩 벤치마크. _ arXiv preprint arXiv:2210.07316_, 2022a. doi: 10.48550/ARXIV.2210.07316. URL[https://arxiv.org/abs/2210.07316](https://arxiv.org/abs/2210.07316). (p. 12. 인용)\n' +
      '* Muennighoff et al. (2022b) Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. multitask finetuning을 통한 교차 언어 일반화, 2022b. URL[https://arxiv.org/abs/2211.01786](https://arxiv.org/abs/2211.01786). (p. 25. 인용)\n' +
      '* Muennighoff et al. (2023) Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. 데이터 제한 언어 모델의 확장 _30-7th Conference on Neural Information Processing Systems_, 2023. URL[https://openreview.net/forum?id=j5BuTrEj35](https://openreview.net/forum?id=j5BuTrEj35). (cited on pp. 2 and 21)\n' +
      '* Muennighoff et al. (2024) Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. OctoPack: 명령어 튜닝 코드 큰 언어 모델들. _The Twelfth International Conference on Learning Representations_, 2024a. URL[https://openreview.net/forum?id=mw1PWNSWZP](https://openreview.net/forum?id=mw1PWNSWZP). (cited on pp.3, 25, 26, 27, 37)\n' +
      '* Muennighoff et al. (2024) Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning. _arXiv preprint_, 2024b. URL [https://arxiv.org/abs/2402.09996](https://arxiv.org/abs/2402.09996). (cited on p. 25)* Mokander et al. [2023] J. Mokander, J. Schuett, H.R. Kirk, et al. Auditing large language models: A three-layered approach. _AI Ethics_, 2023. URL [https://doi.org/10.1007/s43681-023-00289-2](https://doi.org/10.1007/s43681-023-00289-2).\n' +
      '* Nanz and Furia[2015] Sebastian Nanz and Carlo A. Furia. 로제타 코드의 프로그래밍 언어 비교 연구 In _2015 IEEE/ACM 37th IEEE International Conference on Software Engineering_, volume 1, pp. 778-788. IEEE, 2015. URL[https://ieeexplore.ieee.org/document/7194625](https://ieeexplore.ieee.org/document/7194625).\n' +
      '* Nijkamp et al. [2023] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. CodeGen: an open large language model for code with multi-turn program synthesis. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=iaYc_KpY2B_](https://openreview.net/forum?id=iaYc_KpY2B_).\n' +
      '* Nozza et al. [2021] Debora Nozza, Federico Bianchi, and Dirk Hovy. HONEST: Measuring hurtful sentence completion in language models. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 2398-2406, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.191. URL [https://aclanthology.org/2021.naacl-main.191](https://aclanthology.org/2021.naacl-main.191).\n' +
      '* OpenAI et al. [2023] OpenAI et al. GPT-4 technical report. _arXiv preprint_, March 2023. URL [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774).\n' +
      '* 16, Mannheim, July 2019. Leibniz-Institut fur Deutsche Sprache. doi: 10.14618/ids-pub-9021. URL[http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215](http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215)\n' +
      '* Paster et al. [2023] Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. OpenWebMath: an open dataset of high-quality mathematical web text. _arXiv preprint_, October 2023. URL [https://arxiv.org/abs/2310.06786](https://arxiv.org/abs/2310.06786).\n' +
      '* Pearce et al. [2022] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri. Asleep at the keyboard? assessing the security of github copilot\'s code contributions. In _2022 IEEE Symposium on Security and Privacy (SP)_, pp. 754-768. IEEE, 2022.\n' +
      '* Penedo et al. [2023] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset for Falcon LLM: Outperforming curated corpora with web data only. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023. URL [https://openreview.net/forum?id=kM5eGcdCzq](https://openreview.net/forum?id=kM5eGcdCzq).\n' +
      '* Peng et al. [2023] Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. The impact of AI on developer productivity: Evidence from GitHub Copilot. _arXiv preprint_, 2023. URL [https://arxiv.org/abs/2302.06590](https://arxiv.org/abs/2302.06590).\n' +
      '* Pietri et al. [2020] Antoine Pietri, Diomidis Spinellis, and Stefano Zacchiroli. The software heritage graph dataset: Large-scale analysis of public software development history. In _MSR 2020: The 17th International Conference on Mining Software Repositories_, pp. 1-5. IEEE, 2020. doi: 10.1145/3379597.3387510. URL [https://arxiv.org/abs/2011.07824https://www.softwareheritage.org/wp-content/uploads/2021/03/msr-2020-challenge.pdf](https://arxiv.org/abs/2011.07824https://www.softwareheritage.org/wp-content/uploads/2021/03/msr-2020-challenge.pdf).\n' +
      '* Piktus et al. [2020] Aleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Laurencon, Gerard Dupont, Sasha Luccioni, Yacine Jernite, and Anna Rogers. The ROOTS search tool: Data transparency for LLMs. In Danushka Bollegala, Ruihong Huang, and Alan Ritter (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)_, pp. 304-314, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-demo.29. URL [https://aclanthology.org/2023.acl-demo.29](https://aclanthology.org/2023.acl-demo.29).\n' +
      '* Piktus et al. (2023) Aleksandra Piktus, Odunayo Ogundepo, Christopher Akiki, Akintunde Oladipo, Xinyu Zhang, Hailey Schoelkopf, Stella Biderman, Martin Potthast, and Jimmy Lin. GAIA 검색: NLP 훈련 데이터 탐색을 위한 포옹 페이스와 피세리니 상호 운용성. Danushka Bollegala, Ruihong Huang, and Alan Ritter(eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics(Volume 3: System Demonstrations)_, pp. 588-598, Toronto, Canada, July 2023b. 컴퓨터 언어학과의 연관성 doi: 10.18653/v1/2023.acl-demo.57. URL[https://aclanthology.org/2023.acl-demo.57](https://aclanthology.org/2023.acl-demo.57).\n' +
      '* Pinnaparaju et al. (2024) Nikhil Pinnaparaju, Reshinth Adityan, Duy Phung, Jonathan Tow, James Baicoianu, and Nathan Cooper. 안정 코드 3B: 에지에서의 코딩. _ 안정성 AI_, 2024. URL[https://stability.ai/news/stable-code-2024-llm-code-completion-release](https://stability.ai/news/stable-code-2024-llm-code-completion-release)\n' +
      '*빅코드 프로젝트(2024) 빅코드 프로젝트. 스택 v2, 2024. URL[https://huggingface.co/datasets/bigcode/the-stack-v2/](https://huggingface.co/datasets/bigcode/the-stack-v2/]). 접속: 2024.\n' +
      '* Puri et al. (2021) Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss. CodeNet: 다양한 코딩 작업을 학습하기 위한 코드 데이터셋을 위한 대규모 AI. 제30차 신경정보처리시스템 데이터세트 및 벤치마크 추적회의(Round 2)_, 2021. URL[https://openreview.net/forum?id=6%2VBKCDrHT](https://openreview.net/forum?id=6%2VBKCDrHT)\n' +
      '* 위키(2024) RedPajama Wiki, 2024. URL[https://github.com/togethercomputer/RedPajama-Data/tree/rp_v1/data_prep/wiki](https://github.com/togethercomputer/RedPajama-Data/tree/rp_v1/data_prep/wiki)\n' +
      '* Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 문장-BERT: siamese BERT-network를 이용한 문장 임베딩. Inui, Kentaro, Jing Jiang, Vincent Ng, and Xiaojun Wan(eds.), _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 3982-3992, Hong Kong, China, November 2019. Computational Linguistics Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL[https://aclanthology.org/D19-1410](https://aclanthology.org/D19-1410).\n' +
      '* Ren et al. (2020) Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. Codebleu: 코드 합성의 자동 평가를 위한 방법. _ arXiv preprint_, 2020. URL[https://arxiv.org/abs/2009.10297](https://arxiv.org/abs/2009.10297).\n' +
      '* Code(2023) Rosetta Code, 2023. URL[https://rosettacode.org/](https://rosettacode.org/)\n' +
      '* Roziere et al. (2020) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom and Gabriel Synnaeve. 코드 라마: 코드에 대한 기초 모델을 엽니다. _ arXiv preprint_, August 2023. URL[https://arxiv.org/abs/2308.12950](https://arxiv.org/abs/2308.12950).\n' +
      '* Senev(2024) Sane Security, 2024. URL[https://sanesecurity.com/usage/signatures](https://sanesecurity.com/usage/signatures)\n' +
      '* Sanh et al. (2022) Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Aran Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 멀티태스크 프롬프트 트레이닝은 제로 샷 태스크 일반화를 가능하게 한다. _International Conference on Learning Representations_, 2022. URL[https://openreview.net/forum?id=9Vrb90DWW14](https://openreview.net/forum?id=9Vrb90DWW14).\n' +
      '* Sorensen et al. (2020)David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 신경 모델의 수학적 추론 능력 분석 _International Conference on Learning Representations_, 2019. URL[https://openreview.net/forum?id=H19R5iR5FX](https://openreview.net/forum?id=H19R5iR5FX).\n' +
      '* ScanCode(2024) ScanCode, 2024. URL[https://github.com/nexB/scancode-toolkit](https://github.com/nexB/scancode-toolkit)\n' +
      '* ScanCode License Categories (2024) ScanCode License Categories, 2024. URL[https://scancode-licensedb.aboutcode.org/help.html#license-categories](https://scancode-licensedb.aboutcode.org/help.html#license-categories)\n' +
      '* Le Scao et al. (2022a) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamachi, Thomas Wang, Benoit Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurencon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, et al. BLOOM: 176b-par CoRR_, abs/2211.05100, 2022a. doi: 10.48550/ARXIV.2211.05100. URL[https://doi.org/10.48550/arXiv.2211.05100](https://doi.org/10.48550/arXiv.2211.05100)\n' +
      '* Le Scao et al. (2022b) Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang 등 100만 gpu hours를 가지고 있다면 어떤 언어 모델을 훈련시킬 것인가? _ arXiv preprint arXiv:2210.15424_, 2022b.\n' +
      '* ServiceNow(2024a) ServiceNow. Text2flow LLM: 기술 텍스트로부터 워크플로우 생성 자동화. [https://downloads.docs.servicenow.com/resource/enus/infocard/text2flow-llm.pdf] (https://downloads.docs.servicenow.com/resource/enus/infocard/text2flow-llm.pdf), 2024a.\n' +
      '* ServiceNow (2024b) ServiceNow. 텍스트 대 코드 LLM: 자연 언어를 실행 가능한 코드로 변환하는 단계, 2024b. URL[https://downloads.docs.servicenow.com/resource/enus/infocard/text-to-code-llm.pdf](https://downloads.docs.servicenow.com/resource/enus/infocard/text-to-code-llm.pdf)\n' +
      '* Shazeer(2019) Noam Shazeer. 고속 트랜스포머 디코딩: 쓰기 헤드 하나만 있으면 됩니다. _ CoRR_, abs/1911.02150, 2019. URL[http://arxiv.org/abs/1911.02150](http://arxiv.org/abs/1911.02150).\n' +
      '* Sheng et al. (2019) Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 그 여자는 베이비시터로 일했다: 언어 세대의 편견에 대해. Inui, Kentaro, Jing Jiang, Vincent Ng, and Xiaojun Wan(eds.), _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 3407-3412, Hong Kong, China, November 2019. Computational Linguistics Association for Computational Linguistics. doi: 10.18653/v1/D19-1339. URL[https://aclanthology.org/D19-1339](https://aclanthology.org/D19-1339).\n' +
      '* Sholler et al. (2019) Dan Sholler, Igor Steinmacher, Denise Ford, Mara Averick, Mike Hoye, and Greg Wilson. 초보자가 프로젝트를 여는 데 기여자가 되도록 돕기 위한 10가지 간단한 규칙입니다. __ PLoS Computational Biology_, 15(9):e1007296, 2019. doi: 10.1371/journal.pcbi.1007296. URL[https://doi.org/10.1371/journal.pcbi.1007296](https://doi.org/10.1371/journal.pcbi.1007296).\n' +
      '* Singh et al. (2024) Shivalika Singh, Freddie Vargus, Daniel Dsouza, Borje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Matacianus, Laura O\'Mahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Max Bartolo, Julia Kreutzer, Ahmet Ustun, Marzieh Fadaee, Sara Hooker. Aya dataset: 다국어 명령어 튜닝을 위한 오픈액세스 컬렉션. _ arXiv preprint_, 2024. URL[https://arxiv.org/abs/2402.06619](https://arxiv.org/abs/2402.06619)\n' +
      '* Heritage(2023) Software Heritage. swh statement on llvm for code, 2023. URL[https://www.softwareheritage.org/2023/10/19/swh-statement-on-llvm-for-code/](https://www.softwareheritage.org/2023/10/19/swh-statement-on-llvm-for-code/].\n' +
      '* Heritage(2024a) Software Heritage. 대량 접근 사용 조건, 2024a. URL[https://www.softwareheritage.org/legal/bulk-access-terms-of-use/](https://www.softwareheritage.org/legal/bulk-access-terms-of-use/)\n' +
      '*) 소프트웨어 Heritage, 2024b. URL[https://www.softwareheritage.org](https://www.softwareheritage.org) (p. 7. 인용)\n' +
      '* 솔라이만(2023) 아이린 솔라이만. 생성 AI 방출의 기울기: 방법 및 고려 사항. _ arXiv preprint_, 2023. URL[https://arxiv.org/abs/2302.04844](https://arxiv.org/abs/2302.04844) (cited on pp.2, 37, 38)\n' +
      '* Soldaini et al. (2024) Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Kyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilsha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Dodge, Kyle Lo. Dolma: 언어 모델 사전 훈련 연구를 위한 3조 토큰의 개방형 말뭉치. _ arXiv preprint_, 2024. URL[https://arxiv.org/abs/2402.00159](https://arxiv.org/abs/2402.00159). (p. 37에 인용됨)\n' +
      '* Archive(2024) StackExchange Archive, 2024. URL[https://archive.org/details/stackexchange](https://archive.org/details/stackexchange) (p. 12. 인용)\n' +
      '* Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Ro former: 회전식 위치 임베딩을 갖는 향상된 트랜스포머. _ arXiv preprint_, April 2021. URL[https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864). (p. 20) 인용\n' +
      '* Szafraniec et al. (2023) Marc Szafraniec, Baptiste Roziere, Hugh James Leather, Patrick Labatut, Francois Charton, and Gabriel Synnaeve. 컴파일러 표현으로 코드 변환 _The Eleventh International Conference on Learning Representations_, 2023. URL[https://openreview.net/forum?id=XMEU3eNeSQ](https://openreview.net/forum?id=XMEU3eNeSQ)에서, (p. 11. 인용)\n' +
      '* Tang et al.(2023) Feiyang Tang, Bjarte M. 오스볼드, 매기엘 브런팅크 코드 검토자 우선 순위 지정 지원: 개인 데이터 및 그 처리를 핀포인트합니다. _ 인공지능 및 Application_, 371:109-124, 2023. doi: 10.3233/FAIA230228. (p. 38에 인용)\n' +
      '* Tang and Ostvold (2024) Feiyang Tang and Bjarte M. 오스볼드 프라이버시 관련 소스 코드 찾기 _ arXiv preprint_, 2024. URL[https://arxiv.org/abs/2401.07316](https://arxiv.org/abs/2401.07316). (p. 38) 인용\n' +
      '* SWHID 규격화 프로젝트(2024) SWHID 규격화 프로젝트. SWHID 규격, 2024. URL[https://www.swhid.org/](https://www.swhid.org/). (p. 38) 인용\n' +
      '* Topether Computer(2023) Together Computer. RedPajama: Open dataset for training large language models, October 2023. URL[https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data). (p. 12. 인용)\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Bhargava, Shruti Bhosale, Dan Bikel, Likas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Bucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Bynthia Kardas, Vedan Hartshorn, Saghar Hosseini, Rui Hungbog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, E. Michael Smith, R. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J 라마 2: 오픈 파운데이션 및 미세 조정 채팅 모델들_ arXiv preprint_, 2023. URL[https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288). (p. 12. 인용)\n' +
      '* Ustun et al. (2024) Ahmet Ustun, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D\'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. Aya 모델: 명령어 finetuned open-access multi language language model. _ arXiv preprint_, 2024. URL[https://arxiv.org/abs/2402.07827](https://arxiv.org/abs/2402.07827). (p. 37에 인용됨)\n' +
      '* Vidgen et al. (2020) Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. 최악의 학습: 온라인 증오 탐지를 개선하기 위해 동적으로 생성된 데이터 세트입니다. Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli(eds.), _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pp. 1667-1682, Online, August 2021. Computational Linguistics Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.132. URL[https://aclanthology.org/2021.acl-long.132](https://aclanthology.org/2021.acl-long.132).\n' +
      '* Wang et al. (2024) Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. 대형 언어 모델을 사용한 소프트웨어 테스트: 측량, 풍경 및 비전. _ IEEE Transactions on Software Engineering_, pp. 1-27, 2024. doi: 10.1109/TSE.2024.3368208. URL[https://arxiv.org/abs/2307.07221](https://arxiv.org/abs/2307.07221).\n' +
      '* Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. 다이, 콕 V 르 재정 조정된 언어 모델은 제로 샷 학습자입니다. _International Conference on Learning Representations_, 2022. URL[https://openreview.net/forum?id=gEZrGCozdqR](https://openreview.net/forum?id=gEZrGCozdqR).\n' +
      '* Woelfle et al. (2011) Michael Woelfle, Piero L Olliaro, and Matthew H. Todd. 열린 과학은 연구 가속기야 Nature chemistry_, 3 10:745-8, 2011. URL[https://api.semanticscholar.org/CorpusID:205289283](https://api.semanticscholar.org/CorpusID:205289283).\n' +
      '* 포럼(2024) 세계경제포럼. 내일 작업: 대형 언어 모델 및 작업, 2024. URL[https://www.weforum.org/publications/jobs of-tomorrow-large-language-models-and-jobs/](https://www.weforum.org/publications/jobs of-tomorrow-large-language-models-and-jobs/)\n' +
      '* Xu et al. (2024) Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, and Tao Yu. 레무르: 자연어 및 언어 에이전트를 위한 코드 조화. _The Twelfth International Conference on Learning Representations_, 2024. URL[https://openreview.net/forum?id=NhwSmtXRn](https://openreview.net/forum?id=NhwSmtXRn).\n' +
      '* 재무(2024) 야후 재무. ServiceNow Inc(NYSE: NOW) Q4 수익: 무엇을 기대할 것인가, 2024. URL[https://finance.yahoo.com/news/servicenow-inc-nyse-now-q4-154816487.html](https://finance.yahoo.com/news/servicenow-inc-nyse-now-q4-154816487.html).\n' +
      '* Yang et al. (2023) Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsum Kim, Donggyun Han, and David Lo. 잡았다! 이 모델은 내 코드를 사용합니다! 코드 모델의 멤버십 유출 위험 평가. _ arXiv preprint_, 2023. URL[https://arxiv.org/abs/2310.01166](https://arxiv.org/abs/2310.01166).\n' +
      '* Yuan et al. (2023) Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, 및 Jingren Zhou. 대용량 언어 모델을 이용한 수학적 추론 학습의 관계 확장 arXiv preprint_, August 2023. URL[https://arxiv.org/abs/2308.01825](https://arxiv.org/abs/2308.01825).\n' +
      '* Zhao et al. (2018) Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 상호참조해결의 젠더 편향: 평가 및 디바이어싱 방법. Marilyn Walker, Heng Ji, and Amanda Stent(eds.), _Proceedings of the 2018 Conference of the North American chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2(Short Papers)_, pp. 15-20, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2003. URL[https://aclanthology.org/N18-2003](https://aclanthology.org/N18-2003).\n' +
      '* Zhuo et al. (2023) Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. Red teaming ChatGPT via jailbreaking: Bias, robustness, reliability and toxicity. _ arXiv preprint_, 2023a. URL[https://arxiv.org/abs/2301.12867](https://arxiv.org/abs/2301.12867)\n' +
      '* Zhuo et al. (2023) Terry Yue Zhuo, Zhou Yang, Zhensu Sun, Yufei Wang, Li Li, Xiaoning Du, Zhenchang Xing, and David Lo. 딥러닝을 위한 소스 코드 데이터 증강: 설문조사. _ arXiv preprint_, May 2023b. URL[https://arxiv.org/abs/2305.19915](https://arxiv.org/abs/2305.19915).\n' +
      '* Zhuo et al. (2024) Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian Liu, and Niklas Muennighoff. Astraios: Parameter-efficient instruction tuning code large language models. _ arXiv preprint_, August 2024. URL[https://arxiv.org/abs/2401.08788](https://arxiv.org/abs/2401.08788).\n' +
      '\n' +
      ') Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, Edward Attandilian. GitHub Copilot이 생산성에 미치는 영향 측정 커뮤니티 ACM_, 67(3):54-63, feb 2024. ISSN 0001-0782. doi: 10.1145/3633453. URL[https://doi.org/10.1145/3633453](https://doi.org/10.1145/3633453).\n' +
      '*(cited on pp. 2 and 39)\n' +
      '\n' +
      '## 부록 A 데이터 큐레이션\n' +
      '\n' +
      '### Excluded Extensions\n' +
      '\n' +
      'AL(al), AngelScript(as), AscciIDoc(asc), AspectJ(pot), Git Config(githubales), GLSL(geo), GernileLock(p,w), Makefile(d,w), Cabal Config(g,w), Mathematica(cdf,nb), MAXScript(ms), mIRC Script(mrc), Puppet(rkt), ReScript(res), reStructuredText(rest), Rich Text Format(rtf), 1d, 2, 3d,\n' +
      '\n' +
      '제외된 프로그래밍 언어\n' +
      '\n' +
      '2차원 배열, AGS 스크립트, Bicep, Checksums, COLLADA,CSV,Diff,DirectX 3D File,E-code, Gerber Image, Git Revision List,Gnuplot,Go,Checksums,IRC log,Jupyter Notebook,KiCad Layout,KiCad Legacy Layout,KiCad Schematic,Lasso,Linux,Kernel Text,SVG,TSV, Unity3D Asset,Wavefront Object,WebVTT,X PixMap\n' +
      '\n' +
      '### License detection\n' +
      '\n' +
      'license_file_names="li[cslen(cs)"", "legal", "copy(left|right)"", "unlicense", "[al]?gpl(d\\(\\_\\)?d\\(\\_\\)?"", # AGPLv3 "bsd(1?)"", # BSDL"mit(x?)", # MTX "apache", "artistic", # Artistic.txt "copying(v?)"", # COPYING3, COPYINGv3 "disclaimer", "eupl", "gfd1", "[cmpl", "cc@", "al([-. v]?)(d\\(\\_\\)?d)"", # AL2.0 "about", "readme", "guidelines", ] license_file_re = re.compile(rf^*(|.*[--]. ({\'|.join(license_file_names)})(|[--. ].*)$\', re.IGNORECASE)```\n' +
      '\n' +
      '### Permissive licenses\n' +
      '\n' +
      'BSD-3-Clause-Clear, BSD-3-Clause-No-Nuclear-License-2014, BSD-3-Clause-Open-MPI, BSD-4-Clause, BSD-4-Clause-Shortened, BSD-4-Gladman-3-Clause-UC, BSD-4-3RENO, BSD-4-3TAHOE, BSD-Advertising-Acknowledgement, BSD-Source-Code, BSL-1.0, bzip2-1.0, Adobe-Glyph, ADSL, AFL-1.2, AFL-2.0, Afmparse, AMDPLPA, AML, AMPAS, ANTLR-PD, Apache-1.0, Apache-2.0, APAFML, App-s2p,\n' +
      '\n' +
      'ScanCode-specific license IDLicenseRef-scancode-{3com-microcode, 3dslicer-1.0, 4suite-1.1, accellera-systemc, adi-bsd, adrian, agere-bsd, alexisisaac-freeware, amd-historical, ams-fonts, anu-license, apache-patent-exception, bsd-3-clause-devine, bsd-3-clause-fda, bsd-corkum, bsd-atmel, bsd-axis-nomod, bsd-credit, bsd-innosys, bsd-mylex, bsd-premkumar, argouml, arm-llvm-sga, array-input-fonts-2021, bpmn-io, bsd-3-clause-no-change, bsd-3-clause-dradizing, bsd-3-cordum, bsd-axis-nomod\n' +
      '\n' +
      '라이센스 라벨링 동안 다음과 같은 기여자 라이센스 계약, 보증 면책 및 기타 라이센스 수정 사항은 고려되지 않았다: 라이센스Ref-scancode-{dco-1.1, generic-cla, google-cla, jetty-ccla-1.1, newton-king-cla, generic-exception, generic-export-compliance, generic-to, generic-trademark, warranty-disclaimer.\n' +
      '\n' +
      '### Pull Requests\n' +
      '\n' +
      '표 24는 다양한 시퀀스 길이(문자로 측정됨)에 대한 PR 렌더링의 볼륨을 나타낸다. 표 25의 상위 20개 언어에 대한 기본 파일의 볼륨을 나열합니다.\n' +
      '\n' +
      '### StackOverflow\n' +
      '\n' +
      '다음 프롬프트를 사용하여\n' +
      '\n' +
      '아래는 사용자로부터의 지시 및 후보자의 답변이다. AI 어시스턴트가 사용자의 지시에 어떻게 응답해야 하는지에 대한 좋은 예인지 여부를 평가한다. 다음 10점 척도를 사용하여 점수를 부여하십시오.\n' +
      '\n' +
      '1: 응답은 완전히 주제에서 벗어났거나, 상당한 부정확성을 포함하거나, 이해할 수 없다. 의미 있는 방식으로 사용자의 쿼리를 처리하지 못합니다.\n' +
      '\n' +
      '2: 대답은 대체로 무관하거나, 모호하거나, 논란의 여지가 있다. 주제와 관련되지만 사용자의 질문의 핵심을 놓치거나 실질적인 잘못된 정보를 포함하는 일부 요소를 포함한다.\n' +
      '\n' +
      '3: 반응은 다소 관련이 있지만 불완전한 상태로 남아 있거나 주제에서 벗어나거나 논란이 되는 요소를 포함한다. 사용자 쿼리의 주요 측면은 해결되지 않은 상태로 남아 있습니다.\n' +
      '\n' +
      '4: 답변은 사용자의 질문을 어느 정도 해결하지만 깊이 또는 명확성이 부족하다. 다소 도움이 될 수 있지만 포괄적이거나 상세하지는 않다.\n' +
      '\n' +
      '5: 응답은 관련성이 있고, 사용자의 질문에 대한 기본적인 답변을 제공하지만 세부사항이나 구체성이 부족하다. 그것은 도움이 되지만 완전히 발달하거나 통찰력이 있는 것은 아니다.\n' +
      '\n' +
      '6: 답변은 적당히 도움이 되고 사용자의 질문의 대부분의 양상들을 다룬다. 그것은 깊이가 부족하거나 사소한 부정확성 또는 관련 없는 정보를 포함할 수 있다.\n' +
      '\n' +
      '7: 응답은 상당히 도움이 되고 사용자의 질의를 잘 처리하지만, AI 어시스턴트의 관점에서는 그렇지 않을 수 있다. 블로그 게시물이나 웹 페이지와 같은 다른 소스의 콘텐츠와 유사할 수 있습니다.\n' +
      '\n' +
      '8: AI 어시스턴트의 관점에서 작성된, 그 대답은 포괄적이고 적절하다. 그것은 사용자의 질의를 효과적으로 다루지만 초점, 간결성 또는 조직의 개선을 위한 사소한 영역을 가질 수 있다.\n' +
      '\n' +
      '9: 응답은 거의 완벽하여 AI 어시스턴트의 관점에서 명확하고 포괄적이며 잘 조직된 답변을 제공한다. 그것은 참여나 통찰력 측면에서 개선할 수 있는 매우 사소한 영역을 가질 수 있다.\n' +
      '\n' +
      '10: 답변은 예시적이며, AI 어시스턴트의 관점에서 사용자의 질의를 완벽하게 어드레싱한다. 그것은 매우 유익하고, 전문적으로 쓰여지고, 매력적이며, 통찰력이 있으며, 개선할 수 있는 영역이 없다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Seqlen (characters)** & **Volume (GB)** \\\\ \\hline\n' +
      '25000 & 19.6 \\\\\n' +
      '50000 & 38.7 \\\\\n' +
      '75000 & 54.34 \\\\\n' +
      '100000 & 67.31 \\\\\n' +
      '200000 & 103.52\\\\\n' +
      '300000 & 126.8 \\\\\n' +
      '400000 & 143.65 \\\\\n' +
      '500000 & 156.76 \\\\\n' +
      '600000 & 167.21 \\\\\n' +
      '700000 & 175.94 \\\\\n' +
      '800000 & 183.18 \\\\\n' +
      '900000 & 189.32 \\\\\n' +
      '1000000 & 194.58 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 24: 시퀀스 길이를 제한할 때 풀 요청 데이터 세트의 볼륨.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Language** & **Volume (GB)** \\\\ \\hline Python & 13.46 \\\\ JavaScript & 9.55 \\\\ Java & 8.37 \\\\ Markdown & 7.34 \\\\ C++ & 5.89 \\\\ Go & 5.59 \\\\ JSON & 4.13 \\\\ TypeScript & 3.96 \\\\ C\\# & 3.76 \\\\ YAML & 3.1 \\\\ XML & 2.55 \\\\ C & 2.34 \\\\ HTML & 2.31 \\\\ Rust & 2.27 \\\\ PHP & 2.09 \\\\ Ruby & 1.73 \\\\ project.pbxproj & 1.51 \\\\ Scala & 1.25 \\\\ TSX & 1.2 \\\\ Swift & 0.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 25: 풀 요청의 상위 20개 언어에 대한 변경 범위의 기본 파일 크기입니다.\n' +
      '\n' +
      '## 부록 B 처리 파이프라인\n' +
      '\n' +
      '### Malware removal\n' +
      '\n' +
      '표 26의 상위 10개 악성코드 시그니처와 표 27의 잠재적 악성파일에 의한 상위 10개 언어를 보여준다.\n' +
      '\n' +
      '## 부록 C 데이터 구성\n' +
      '\n' +
      '### TheStackV2-train-smol\n' +
      '\n' +
      '* 구성 언어\n' +
      '* Ant Build System \\(-\\) Gradle \\(-\\) Makefile \\(-\\) CMake \\(-\\) INI \\(-\\) Maven POM\n' +
      '* Dockerfile \\(-\\) Java Properties \\(-\\) TOML\n' +
      '* Go 모듈\n' +
      '* 구성 파일:\n' +
      '* CMakeLists.txt \\(-\\) build.gradle.kts \\(-\\) pyproject.toml\n' +
      '* cargo.toml \\(-\\) composer.json \\(-\\) requirements-dev.txt\n' +
      '* DESCRIPTION \\(-\\) conda.yml \\(-\\) requirements-prod.txt\n' +
      '* Gemfile \\(-\\) configure.ac \\(-\\) requirements.in\n' +
      '* Makefile \\(-\\) docker-compose.yaml \\(-\\) requirements.test.txt\n' +
      '* Makefile.am \\(-\\) docker-compose.yml \\(-\\) requirements.txt\n' +
      '* NAMESPACE \\(-\\) go.mod \\(-\\) setup.cfg\n' +
      '* Package.swift \\(-\\) package.json \\(-\\) tsconfig.json \\(-\\) Pipfile \\(-\\) pom.xml \\(-\\) yarnlock\n' +
      '\n' +
      '### TheStackV2-train-full\n' +
      '\n' +
      '표 28에서 하위 샘플 언어에 대한 데이터 볼륨을 요약한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Signature** & **Count** \\\\ \\hline Sanesecurity.Malware.28845.BadVBS & 11876 \\\\ winnow.compromised.ts.jsexploit.5 & 2251 \\\\ Sanesecurity.Malware.26492.JsHeur & 2247 \\\\ Sanesecurity.Spam.8879 & 1597 \\\\ Sanesecurity.Malware.25834.JsHeur & 1560 \\\\ Sanesecurity.Malware.27112.JsHeur & 1258 \\\\ Sanesecurity.Malware.26222.JsHeur & 888 \\\\ Porcupine.Malware.52833 & 814 \\\\ Sanesecurity.SpamL.8887 & 792 \\\\ Sanesecurity.Malware.26557.JsHeur & 728 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 26: 상위 10개 악성 프로그램 시그니처가 탐지되었습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Language** & **Count** \\\\ \\hline Text & 13281 \\\\ HTML & 11336 \\\\ JavaScript & 10210 \\\\ VBScript & 7947 \\\\ Logos & 3283 \\\\ Markdown & 2736 \\\\ Linker Script & 1390 \\\\ XML & 1260 \\\\ VBA & 990 \\\\ JSON & 547 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 27: 잠재적으로 악의적인 파일 수 기준으로 상위 10개 언어입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Final volume** & **Languages** \\\\ \\hline\n' +
      '200GB & Java, JavaScript \\\\\n' +
      '100GB&HTML\\\\\n' +
      '8GB & CSS, Java Server Pages, JSON, \\\\ & SCSS, Smali, XML, YAML \\\\\\\n' +
      '1GB & BibTeX, Gettext Catalog, Graphviz (DOT), \\\\  & Java Properties, Roff, Roff Manpage, \\\\  & Web Ontology Language \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 28: Stack v2 데이터세트 내의 언어들에 대한 서브샘플링 볼륨들.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>