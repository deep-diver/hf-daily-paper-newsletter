<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '지난 몇 년 동안 새롭고 창의적인 콘텐츠를 생성할 수 있는 모델과 함께 _생성 AI_가 출현했다. 변압기(Vaswani et al., 2017)와 같은 아키텍처의 돌파구, 하드웨어의 발전, 그리고 스케일링 모델 및 데이터세트에 대한 최근의 초점에 의해, 우리는 이제 일관성 있고 대화적인 언어를 생성할 수 있다(Brown et al., 2020; Radford et al., 2018, 2019), 뿐만 아니라 텍스트 프롬프트로부터 선명하고 미학적으로 만족스러운 이미지를 생성할 수 있다(Ramesh et al., 2021, 2022; Rombach et al., 2022; Saharia et al., 2022). 초기 징후는 비디오 생성이 또 다른 프론티어가 될 것임을 나타내며, 최근의 결과는 이러한 모델이 스케일로부터 또한 이익을 얻을 수 있음을 시사한다(Blattmann et al., 2023; Esser et al., 2023; Ho et al., 2022; Hong et al., 2023). 여전히, 더 몰입적인 경험은 말할 것도 없고, 비디오 생성 모델 및 ChatGPT와 같은 언어 도구의 상호작용 수준과 참여 사이에는 격차가 남아 있다.\n' +
      '\n' +
      '인터넷의 방대한 동영상 코퍼스를 감안할 때 새로운 이미지나 동영상을 생성할 수 있는 모델뿐만 아니라 전체 대화형 경험을 훈련할 수 있다면 어떨까요? 우리는 단일 텍스트 또는 이미지 프롬프트로부터 대화형 환경을 생성할 수 있는 생성 AI의 새로운 패러다임인 _생성 대화형 환경_을 제안한다. 우리의 접근 방식인 지니(Genie)는 200,000시간 이상의 공개적으로 이용 가능한 인터넷 게임 비디오의 대규모 데이터 세트로부터 훈련되며, 동작 또는 텍스트 주석 없이 훈련됨에도 불구하고, 학습된 잠재 동작 공간을 통해 프레임 단위로 제어 가능하다(다른 접근 방식과의 비교를 위해 표 1 참조). 11B 파라미터들에서, 지니(Genie)는 기초 모델들에서 전형적으로 보여지는 속성들을 나타내는데, 이는 완전히 상상된 가상 세계들을 생성하고 재생하는 것을 가능하게 하는 프롬프트로서 보이지 않는 이미지를 취할 수 있다(예를 들어, 도 2).\n' +
      '\n' +
      'Genie는 최첨단 비디오 생성 모델(Gupta et al., 2023; Villegas et al., 2023)의 아이디어를 기반으로 하며, 핵심 설계 선택은 우리의 모든 모델 컴포넌트에서 사용되는 시공간(ST) 트랜스포머(Xu et al., 2020)이다. Genie는 새로운 비디오 토큰화기를 사용하며, 인과적 행동 모델을 통해 잠재된 행동을 추출한다. 비디오 토큰과 잠재 액션은 모두 동적 모델에 전달되며, 동적 모델은 MaskGIT를 사용하여 다음 프레임을 자동으로 예측한다(Chang et al., 2022). 우리는 40M에서 2.7B 매개변수까지 다양한 배치 및 모델 크기에 대한 아키텍처의 엄격한 스케일링 분석을 제공한다. 그 결과, 본 논문의 아키텍처는 추가적인 계산 자원을 사용하여 우아하게 확장되어 최종 11B 파라미터 모델로 이어진다. 우리는 수백 개의 2D 플랫폼 게임으로부터 30,000시간의 인터넷 게임 플레이 비디오의 필터링된 세트에서 지니를 훈련시켜 이 설정을 위한 기반 세계 모델을 생산한다.\n' +
      '\n' +
      '또한, RT1 데이터세트(Brohan et al., 2023)로부터 동작 없는 로봇 비디오에 대한 별도의 모델을 학습하고, 일관된 잠재 동작을 갖는 생성 환경을 학습한다. 마지막으로, 우리는 인터넷 비디오들로부터 학습된 잠재 액션들이 모의 강화 학습(RL) 환경의 보이지 않는 액션-프리 비디오들로부터 정책들을 추론하는데 사용될 수 있음을 보여주며, 이는 지니가 차세대 일반 에이전트들을 훈련시키기 위한 무제한 데이터의 잠금 해제에 대한 키를 보유할 수 있음을 나타낸다(바우어 등,\n' +
      '\n' +
      '그림 2: **다양한 궤적**: 지니는 대화형 환경으로 사용될 수 있는 생성 모델이다. 생성 이미지(위) 또는 손으로 그린 스케치(아래)를 사용하여 다양한 방식으로 모델을 프롬프트할 수 있습니다. 각 시간 단계에서 모델은 다음 프레임을 생성하기 위해 사용자 제공 잠재 액션을 취하여 흥미롭고 다양한 캐릭터 액션이 있는 궤적을 생성한다.\n' +
      '\n' +
      '2023; Clune, 2019; Open Ended Learning Team 등, 2021; Reed 등, 2022).\n' +
      '\n' +
      '## 2 Methodology\n' +
      '\n' +
      '지니는 비디오 전용 데이터로 훈련된 생성적 상호작용 환경이다. 이 섹션에서는 모델의 주요 구성 요소를 설명하기 전에 예비로 시작합니다.\n' +
      '\n' +
      '지니 아키텍처 내의 몇몇 컴포넌트들은 ViT(Vision Transformer)를 기반으로 한다(Dosovitskiy et al., 2021; Vaswani et al., 2017). 특히 변압기의 2차 메모리 비용은 최대 \\(O(10^{4})\\) 토큰을 포함할 수 있는 비디오에 문제를 제기한다. 따라서 우리는 모든 모델 구성 요소에 걸쳐 메모리 효율적인 ST-트랜스포머 아키텍처(Xu et al.(2020), 그림 4 참조)를 채택하여 계산 제약으로 모델 용량을 밸런싱한다.\n' +
      '\n' +
      'ST-변환기는 모든 토큰이 다른 모든 토큰에 적용되는 전통적인 변환기와는 달리, 인터리브된 시공간적 어텐션 레이어를 갖는 \\(L\\) 시공간적 블록을 포함하고, 그 다음에는 피드포워드 레이어(FFW)를 표준 어텐션 블록으로 포함한다. 공간 계층에서 자기 주의는 각 시간 단계 내에서 \\(1\\times H\\times W\\) 토큰에 걸쳐 나타나며, 시간 계층에서는 \\(T\\times 1\\times 1\\) 토큰에 걸쳐 나타난다. 시퀀스 트랜스포머와 유사하게, 시간적 층은 인과 마스크를 갖는 인과 구조를 가정한다. 결정적으로, 우리 아키텍처에서 계산 복잡도의 지배적인 요소(즉, 공간 주의 계층)는 2차보다 프레임 수에 따라 선형적으로 확장되어 확장된 상호 작용에 대해 일관된 동역학을 갖는 비디오 생성에 훨씬 더 효율적이다. 또한 ST 블록에서는 모델의 다른 구성 요소를 크게 개선하기 위해 사후 공간 FFW를 생략하여 공간 및 시간 구성 요소 뒤에 하나의 FFW만 포함한다는 점에 유의해야 한다.\n' +
      '\n' +
      '### Model Components\n' +
      '\n' +
      '도 3에 도시된 바와 같이, 본 모델은 1) 각 프레임 쌍 사이의 잠재 액션\\(\\mathbf{a}\\)을 추론하는 _latent 액션 모델_과 2) 원시 비디오 프레임을 이산 토큰\\(\\mathbf{z}\\)으로 변환하는 _video tokenizer_와 3) 잠재 액션과 과거 프레임 토큰이 주어지면 비디오의 다음 프레임을 예측하는 _dynamics 모델_의 세 가지 핵심 구성 요소를 포함한다. 모델은 표준 자동 회귀 비디오 생성 파이프라인에 따라 두 단계로 훈련된다: 동역학 모델에 사용되는 비디오 토큰화기를 먼저 훈련한다. 그런 다음 우리는 (픽셀에서 직접) 잠재 행동 모델과 (비디오 토큰에서) 동역학 모델을 공동 훈련한다.\n' +
      '\n' +
      '**LAM(Latent Action Model)** 제어 가능한 비디오 생성을 달성하기 위해, 우리는 이전 프레임에서 취해진 액션에 대해 각각의 미래 프레임 예측을 컨디셔닝한다. 그러나, 이러한 액션 라벨들은 인터넷으로부터의 비디오들에서 거의 이용가능하지 않으며, 액션 주석은 획득하는데 비용이 많이 들 수 있다. 대신, 우리는 완전히 감독되지 않은 방식으로 _latent action_을 학습한다(도 5 참조).\n' +
      '\n' +
      '먼저, 인코더는 다음 프레임뿐만 아니라 이전 프레임들(\\tilde{\\mathbf{x}_{1:t}=(x_{1},\\cdots x_{t})\\)을 모두 입력으로 하고, 연속적인 잠재 행동 집합(\\tilde{\\mathbf{a}}_{1:t}=(\\tilde{a}_{1},\\cdots\\tilde{a}_{t})을 출력한다. 디코더는 모든 이전 프레임과 잠재된 동작을 입력으로 하여 다음 프레임을 예측한다\\(\\hat{x}_{t+1}\\).\n' +
      '\n' +
      '모델을 훈련하기 위해 VQ-VAE 기반 목표(van den Oord et al., 2017)를 활용하며, 이는 우리가 예측된 액션의 수를 작은 이산 코드 세트로 제한할 수 있게 한다. 우리는 VQ 코드북의 어휘 크기 \\(|A|\\) 즉, 가능한 최대 잠재 행동 수를 작은 값으로 제한하여 인간의 놀이성을 허용하고 통제성을 더욱 강화한다 (실험에서 \\(|A|=8\\)을 사용한다). 디코더는 과거와 미래 사이의 가장 의미 있는 변화를 부호화해야만 복호화기가 성공적으로 미래 프레임을 복원할 수 있다. 이 점에 유의하십시오.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Model Class & Training Data & Controllability \\\\ \\hline World Models & Video + Actions & Frame-level \\\\ Video Models & Video + Text & Video-level \\\\ Genie & Video & Frame-level \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **새로운 클래스의 생성 모델**: 지니는 프레임 단위로 제어 가능한 새로운 비디오 및 월드 모델이며, 이는 열차 시간에 **만 비디오 데이터**를 필요로 한다.\n' +
      '\n' +
      ' 코더는 LAM 트레이닝 신호를 주기 위해서만 존재한다. 실제로, VQ 코드북과는 별도로, 추론 시간에 전체 LAM이 폐기되고 사용자로부터의 액션으로 대체된다.\n' +
      '\n' +
      '잠재적 행동 모델을 위해 ST-트랜스포머 아키텍처를 활용한다. 시간 계층에서 인과적 마스크는 전체 비디오\\(\\mathbf{\\tilde{a}}_{1:T}\\)을 입력으로 하고 각 프레임 사이의 모든 잠재 행동을 생성할 수 있게 한다.\n' +
      '\n' +
      '**Video Tokenizer** 후속 선행 작업 Gupta et al.(2023); Villegas et al.(2023); Yan et al.(2023), 우리는 차원을 감소시키고 더 높은 품질의 비디오 생성을 가능하게 하기 위해 비디오들을 이산 토큰들로 압축한다(도 6 참조). 본 논문에서는 VQ-VAE(VQ-VAE:VQ-VAE:VQ-VAE:VQ-VAE:VQ-VAE:VQ-VAE:VQ-VAE=(x_{1},x_{2},\\cdots,x_{T})\\in\\mathbbb{R}^{T\\times H\\times W\\times C}\\in\\mathbbb{R}^{T\\times W\\times C}\\in\\mathbf{z}_{1:T}=(z_{1},z_{2},\\cdots,z_{T})\\in\\mathbb{I}^{T\\times D}\\in\\mathbb{R}^{T\\times W\\times C}\\in\\mathbb{R}^{T\\times C}\\in\\mathbb{R}^{T\\times W\\times C}\\in\\mathbb{R}\\times H\\times C}\\t 토나이저는 전체 비디오 시퀀스에 걸쳐 표준 VQ-VQAE 대물렌즈를 사용하여 트레이닝된다.\n' +
      '\n' +
      'Tokenization phase Gupta et al. (2023); Hong et al. (2022); Wu et al. (2022)에서 공간 전용 압축에 초점을 맞춘 이전 작업들과 달리, 인코더와 디코더 모두에서 ST-변환기를 사용하여 인코더에 시간 역학을 통합함으로써 비디오 생성 품질을 향상시킨다. ST-변환기의 인과적 성질에 의해, 각각의 이산 인코딩 \\(z_{t}\\)은 비디오 \\(\\mathbf{x}_{1:t}\\)의 이전에 본 모든 프레임으로부터의 정보를 포함한다. Phenaki Villegas et al. (2023)은 또한 시간 인식 토큰화기인 C-ViViT를 사용하지만, 이 아키텍처는 프레임 수에 따라 비용이 2차적으로 증가함에 따라 계산 집약적이며, 비교에서, 우리의 ST-트랜스포머 기반 토큰화기(ST-ViViT)는 비용이 선형적으로 증가하는 지배적인 요인과 함께 훨씬 더 효율적이다.\n' +
      '\n' +
      '도 4: **ST-트랜스포머 아키텍처**. 이 구조는 공간 계층, 시간 계층 및 피드 포워드 계층을 포함하는 \\(L\\) 시공간 블록으로 구성된다. 각 색상은 단일 자기 주의 맵을 나타내며, 공간 계층은 단일 시간 단계 내에서 \\(H\\times W\\) 토큰에 걸쳐 존재하고, 동일한 토큰은 \\(T\\) 시간 단계에 걸쳐 존재한다.\n' +
      '\n' +
      '도 5: **잠재적 액션 모델**: 라벨링되지 않은 비디오 프레임들로부터 감독되지 않은 액션들 \\(a_{t}\\)을 학습한다.\n' +
      '\n' +
      '도 6: **Video tokenizer**: ST-transformer를 구비한 VQ-VAE.\n' +
      '\n' +
      '프레임 수를 갖는 것을 특징으로 하는 방법.\n' +
      '\n' +
      '**Dynamics Model** 동역학 모델은 디코더 전용 MaskGIT(Chang et al., 2022) 트랜스포머이다(도 7). 각 시간 단계 \\(t\\in[1,T]\\)에서 토큰화된 비디오 \\(\\mathbf{z}_{1:t-1}\\)과 정지 잠재 행동 \\(\\tilde{\\mathbf{a}_{1:t-1}\\)을 취하고 다음 프레임 토큰 \\(\\hat{\\mathbf{z}_{t}\\)을 예측한다. 또한 ST-transformer를 이용하여 모든 프레임(\\(T-1)\\(\\mathbf{z}_{1:T-1}\\)과 잠재 행동(\\(\\tilde{\\mathbf{a}}_{1:T-1}\\)의 토큰을 입력으로 사용하고, 다음 프레임(\\hat{\\mathbf{z}_{2:T}\\)에 대한 예측을 생성한다. 모델은 예측 토큰\\(\\hat{\\mathbf{z}}_{2:T}\\)과 지상진리 토큰\\(\\mathbf{z}_{2:T}\\) 사이의 교차 엔트로피 손실로 학습된다. 열차 시간에 0.5와 1 사이에서 균일하게 샘플링된 베르누이 분포 마스킹 비율에 따라 입력 토큰 \\(\\mathbf{z}_{2:T-1}\\)을 랜덤하게 마스킹한다. 트랜스포머 기반 모델을 포함한 세계 모델을 훈련하기 위한 일반적인 관행은 시간 \\(t\\)의 액션을 해당 프레임에 연결하는 것이다(Micheli et al., 2023; Robine et al., 2023). 그러나 잠재행동과 동역학 모델 모두에 대해 잠재행동을 _additive embeddings_로 처리하는 것이 세대의 통제성을 향상시키는 데 도움이 된다는 것을 발견했다.\n' +
      '\n' +
      '### 추론: 액션 제어 가능한 비디오 생성\n' +
      '\n' +
      '이제 추론 시간에 행동 제어 가능한 비디오 생성을 위해 지니를 사용하는 방법을 설명한다(도 8 참조). 플레이어는 먼저 초기 프레임1의 역할을 하는 이미지\\(x_{1}\\)로 모델을 프롬프트하고, 비디오 인코더를 사용하여 이미지를 토큰화하여 \\(z_{1}\\)을 생성한다. 그 후, 플레이어는 \\([0,|A|)\\) 내에서 임의의 정수 값을 선택하여 취할 이산 잠재 행동 \\(a_{1}\\)을 지정한다. 2 동역학 모델은 이산 입력 \\(a_{1}\\)으로 VQ 코드북에 인덱싱하여 얻은 프레임 토큰 \\(z_{1}\\)과 그에 상응하는 잠재 행동 \\(\\tilde{a}_{1}\\)을 사용하여 다음 프레임 토큰 \\(z_{2}\\)을 예측한다. 이 과정은 토큰화기의 디코더를 사용하여 토큰들을 비디오 프레임들(\\hat{\\mathbf{z}}_{2:T}\\)로 디코딩하는 동안, 액션들이 모델에 계속 전달됨에 따라 자기회귀 방식으로 시퀀스의 나머지\\(\\hat{\\mathbf{z}_{2:T}\\)을 생성하기 위해 반복된다. 우리는 모델을 시작 프레임 및 비디오로부터 추론된 액션들을 전달함으로써 데이터세트로부터 지상 진실 비디오들을 재생성하거나, 액션들을 변경함으로써 완전히 새로운 비디오들(또는 궤적들)을 생성할 수 있다는 점에 유의한다.\n' +
      '\n' +
      '각주 1: 모델은 다양한 수의 프롬프트 프레임에 대해 조정될 수 있다. 여기서 우리는 하나의 이미지로부터 예를 들어 시작한다.\n' +
      '\n' +
      '## 3 실험 결과\n' +
      '\n' +
      '**Datasets** 2D Platformer 게임(이하 "Platformers"라 칭함)의 공개적으로 이용 가능한 인터넷 비디오로부터 수집된 대규모 데이터세트에서 지니를 훈련시킨다. 우리는 플랫폼 사용자와 관련된 키워드에 대해 공개적으로 사용할 수 있는 비디오를 필터링하여 플랫폼 사용자 데이터 세트를 구성하여 160x90 해상도의 10FPS에서 55M 16s 비디오 클립을 생성한다. 최종 데이터 세트는 6.8M 16s 비디오 클립(30k시간)을 포함하고, 다른 인기 있는 인터넷 비디오 데이터 세트의 크기 내에 있다(Bain et al., 2021; Wang et al., 2023). 보다 상세한 내용은 부록 B.1에서 찾을 수 있다. 달리 명시되지 않는 한, 결과는 11B-파라미터 모델을 가지고 있다\n' +
      '\n' +
      '도 8: **Genie Inference**: 프롬프트 프레임은 토큰화되고, 사용자에 의해 취해진 잠재 액션과 결합되고, 반복 생성을 위해 역학 모델에 전달된다. 이어서, 예측된 프레임 토큰들은 토큰화기의 디코더를 통해 이미지 공간으로 다시 디코딩된다.\n' +
      '\n' +
      '그림 7: **다이내믹스 모델**: 비디오 토큰 및 액션 임베딩을 취하고, 미래의 마스킹된 비디오 토큰을 예측한다.\n' +
      '\n' +
      '이 데이터 집합에 대해 훈련되었습니다.\n' +
      '\n' +
      '본 논문에서 제안한 방법의 일반성을 검증하기 위해 RT1 Brohan et al.(2023)의 훈련에 사용되는 로봇 데이터 셋을 고려하였으며, RT1 Brohan et al.(2023)의 로봇 시연 데이터 셋을 별도의 시뮬레이션 데이터 셋과 209k 에피소드의 실제 로봇 데이터를 결합하였다 (Kalashnikov et al., 2018). 이러한 데이터 세트의 액션을 사용하지 않고 단순히 비디오로 취급합니다. 단순화를 위해 여기에서 우리는 이 데이터 세트를 "로봇"이라고 한다.\n' +
      '\n' +
      '**Metrics** 우리는 Genie의 비디오 생성 성능을 두 가지 요인, 즉 _video fidelity_, 즉 비디오 생성의 품질과 _controllability_, 즉 잠재 행동이 비디오 생성에 얼마나 영향을 미치는지 조사한다. 비디오 충실도의 경우, 비디오 품질(Unterthiner et al., 2019)에 대한 인간 평가에 높은 수준의 정렬을 갖는 것으로 밝혀진 비디오 레벨 메트릭인 Frechet 비디오 거리(FVD)를 사용한다. 제어성을 위해 지상진실(\\(\\hat{x}_{t}\\))과 잠재행위(\\(\\hat{x}_{t}\\))에서 추론된 잠재행위에 따라 영상세대가 얼마나 다른지를 측정하는 PSNR(Peak Signal-to-Noise Ratio)을 기반으로 메트릭을 고안하였다. - 랜덤 분포로부터 샘플링된 (\\(\\hat{x}_{t}^{\\prime}\\):\n' +
      '\n' +
      '\\[\\Delta_{t}\\text{PSNR}=\\text{PSNR}(x_{t},\\hat{x}_{t})-\\text{PSNR}(x_{t},\\hat{x} _{t}^{\\prime}),\\]\n' +
      '\n' +
      '여기서 \\(x_{t}\\)은 시간에서의 지상-진실 프레임을 나타내고, \\(\\hat{x}_{t}\\)은 지상-진실 프레임으로부터 추론된 잠재 행동들로부터의 프레임을 나타내며, \\(\\tilde{\\mathbf{a}}_{1:t}\\) 및 범주형 분포로부터 랜덤하게 샘플링된 잠재 행동들의 시퀀스로부터 생성된 동일한 프레임을 나타낸다. 이와 같이, PPSNR이 클수록 랜덤한 잠재행동으로부터 생성된 영상이 지상-진실과 더 많이 다르며, 이는 잠재행동으로부터 더 높은 수준의 통제성을 나타낸다. 모든 실험에서 우리는 \\(t=4\\)의 \\(\\Delta_{t}\\)PSNR을 보고한다.\n' +
      '\n' +
      '**Training Details** 우리의 비디오 토큰라이저는 200M 매개 변수, 패치 크기 4 및 내장 크기 32 및 1024 고유 코드를 사용하는 코드북을 사용하며, 이는 토큰라이저의 재구성 품질과 비디오 예측의 다운스트림 성능 사이의 트레이드 오프를 고려할 때 가장 효과적인 것으로 나타났다. 잠재적 액션 모델은 300M 파라미터, 패치 크기 16, 임베딩 크기 32 및 8개의 고유 코드(잠재적 액션)를 갖는 코드북을 갖는다. 모든 모델링 구성 요소에 대해 FPS가 10인 16프레임의 시퀀스 길이를 사용하고, 동적 모델을 훈련하기 위해 bfloat16과 QK norm을 사용하며, 이는 대규모 훈련을 안정화시키는 것으로 나타났다 (Dehghani et al., 2023; Henry et al., 2020). 추론 시간에 랜덤 샘플링을 사용하여 2의 온도로 각 프레임의 샘플링을 위해 25개의 MaskGIT 단계를 수행한다. 자세한 내용은 부록 C를 참조하십시오.\n' +
      '\n' +
      '### Scaling Results\n' +
      '\n' +
      '이 섹션에서는 모델의 스케일링 동작을 조사합니다. 이를 위해 모델 크기와 배치 크기 모두의 영향을 탐구하는 연구를 수행한다. 구조 및 계산 사용에 대한 자세한 내용은 부록 D를 참조하십시오.\n' +
      '\n' +
      '**스케일링 모델 크기** 고정된 비디오 토큰화기와 액션 모델 아키텍처가 주어지면 40M에서 2.7B 매개변수 범위의 일련의 역학 모델을 훈련한다. 도 9는 아키텍처를 나타낸 도면\n' +
      '\n' +
      '도 9: **스케일링 결과. 왼쪽: 상이한 모델 크기에 대한 트레이닝 곡선, 중간: 각각의 모델 크기에 대한 최종 트레이닝 손실, 지난 300개의 업데이트에 걸쳐 평균화, 오른쪽: 상이한 배치 크기를 갖는 2.3B 모델에 대한 최종 트레이닝 손실.**\n' +
      '\n' +
      '모델 파라미터로 우아하게 스케일링하고, 각각의 크기 증가는 최종 트레이닝 손실의 일관된 감소에 대응한다. 이것은 우리의 접근 방식이 메인 지니 모델로 활용하는 스케일링의 이점을 제공한다는 강력한 표시이다.\n' +
      '\n' +
      '**스케일링 배치 크기** 배치 크기가 128, 256 및 448인 배치 크기가 1.9M, 3.8M 및 6.6M 토큰에 해당하는 2.3B 모델을 고려하여 배치 크기를 스케일링하는 효과도 조사한다. 그림 9에서 볼 수 있듯이 배치 크기를 증가시키면 모델 성능 측면에서 유사하게 유리한 이득을 얻을 수 있다.\n' +
      '\n' +
      '**지니 모델** 모델 크기와 배치 크기를 모두 증가시키는 것이 모델 성능 향상에 도움이 된다는 것은 명백하다. 결과적으로, 최종 모델의 경우 256 TPUv5p를 사용하여 총 125k 단계에 대해 배치 크기가 512인 10.1B 역학 모델을 훈련한다. 토네이저 및 액션 모델과 결합하면 총 10.7B 매개변수가 생성되며, 이는 942B 토큰에 대해 훈련되며, 이를 지니 모델이라고 한다. 웹사이트의 경우 토큰을 360p 비디오에 매핑하는 더 큰 디코더를 훈련하여 추가 매개변수를 추가합니다.\n' +
      '\n' +
      '### Qualitative Results\n' +
      '\n' +
      '이제 지니 모델의 질적 결과를 제시한다. 우리는 플랫폼 데이터 세트에서 훈련된 11B 매개변수 모델과 로보틱스 데이터 세트에서 훈련된 더 작은 모델을 보여준다. 우리의 모델은 다양한 도메인에 걸쳐 고품질 제어 가능한 비디오를 생성합니다. 특히, 텍스트 대 이미지 모델, 손으로 그린 스케치 및 사실적인 사진에서 생성된 것을 포함하여 OOD(Out-of-distribution) 이미지 프롬프트를 사용하여 플랫폼 학습 모델을 정성적으로 평가한다. 이러한 상당히 OOD 입력에 일반화하는 능력은 우리의 접근법의 견고성과 실제 동작을 입력으로 사용하여 실현 가능하지 않았을 대규모 데이터에 대한 훈련의 가치를 강조한다.\n' +
      '\n' +
      '**플랫폼 학습 모델** 도 10은 Imagen2(Ho et al., 2022; van den Oord et al.), (second row) 핸드 드로잉 스케치 및 (bottom row) 실세계 사진을 포함하여 OOD 이미지로부터 프롬프트된 우리 모델의 세대의 예를 보여준다. 지니는 게임과 같은 행동을 볼 때 상상 속의 세계를 살아나게 할 수 있다.\n' +
      '\n' +
      '그림 10: **이미지 프롬프트에서 재생: 텍스트 대 이미지 모델, 손으로 그린 스케치 또는 실제 사진으로 생성된 이미지로 지니에게 프롬프트할 수 있습니다. 각각의 경우에, 우리는 4회 연속으로 잠재적인 행동들 중 하나를 취한 후 프롬프트 프레임과 두 번째 프레임을 보여준다. 각 경우에 일부 이미지가 데이터 세트와 시각적으로 구별됨에도 불구하고 명확한 문자 이동을 볼 수 있다.**\n' +
      '\n' +
      '각 예제와 반응합니다. 우리는 부록 A에서 우리 모델에 의해 더 많은 세대를 선보이며 잠재 행동의 일관성을 강조한다.\n' +
      '\n' +
      '우리 모델의 또 다른 새로운 능력은 플랫폼 게임에서 흔히 볼 수 있는 3D 장면을 이해하고 시차를 모방하는 능력이다. 도 12에서 우리는 Imagen2에 의해 생성된 이미지를 보여주는데, 여기서 잠재된 행동을 취하는 것은 (다른 색깔의 화살표 길이로 표시된 것처럼) 다른 비율로 전경을 배경으로 움직인다.\n' +
      '\n' +
      '**로봇학 학습 모델** 플랫폼상에서 가장 우수한 것으로 밝혀진 동일한 하이퍼파라미터를 사용하여 로봇학 데이터셋에서 2.5B-파라미터 모델을 학습하여 테스트 분할에서 82.7의 FVD를 달성했다. 도 13에 도시된 바와 같이, 이 모델은 텍스트 또는 액션 라벨들(예를 들어, Yang et al. (2023)에서와 같이)을 필요로 하지 않는, 비디오 데이터로부터 구별되고 일관된 액션들을 성공적으로 학습한다. 특히, 우리의 모델은 로봇 팔의 제어뿐만 아니라 다양한 물체의 상호작용 및 변형을 학습한다(도 11). 본 논문에서 제안한 방법은 다양한 응용 분야에 사용될 수 있는 낮은 수준의 제어 가능한 시뮬레이션과 함께 로봇 공학 기반 세계 모델을 생성하기 위해 인터넷에서 더 큰 비디오 데이터 세트를 사용하는 경로를 제공한다는 것을 보여준다.\n' +
      '\n' +
      '### Training Agents\n' +
      '\n' +
      '우리는 언젠가 지니가 일반 요원들을 훈련시키기 위한 기초 세계 모델로 사용될 수 있을 것이라고 믿는다. 그림 14에서 우리는 모델이 시작 프레임이 주어진 보이지 않는 RL 환경에서 다양한 궤적을 생성하는 데 이미 사용될 수 있음을 보여준다. 잠재적 행동이 학습되었는지 추가로 조사합니다.\n' +
      '\n' +
      '그림 11: **변형 가능한 물체를 시뮬레이션하는 학습**: 우리는 모델의 10단계 궤적에서 동일한 동작을 취하는 프레임을 보여준다. 지니는 칩 가방과 같은 물체의 물리적 성질을 학습할 수 있다.\n' +
      '\n' +
      '그림 12: **로보틱스에서의 통제 가능하고 일관된 잠재 행동**: 로보틱스 데이터 세트의 세 가지 다른 시작 프레임에서 시작하는 궤적. 각 열은 동일한 잠재 행동을 다섯 번 취함으로써 얻어지는 프레임을 보여준다. 액션 레이블이 없는 훈련에도 불구하고 동일한 액션은 다양한 프롬프트 프레임에 걸쳐 일관되며 _down_, _up_ 및 _left_와 같은 의미론적 의미를 갖는다.\n' +
      '\n' +
      '그림 13: **로보틱스에서의 통제 가능하고 일관된 잠재 행동**: 로보틱스 데이터 세트의 세 가지 다른 시작 프레임에서 시작하는 궤적. 각 열은 동일한 잠재 행동을 다섯 번 취함으로써 얻어지는 프레임을 보여준다. 액션 레이블이 없는 훈련에도 불구하고 동일한 액션은 다양한 프롬프트 프레임에 걸쳐 일관되며 _down_, _up_ 및 _left_와 같은 의미론적 의미를 갖는다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '제안된 ST-ViViT 구조는 C-ViViT와 공간 전용 ViT에 비해 메모리의 합리적인 트레이드 오프를 위해 향상된 비디오 생성(FVD)과 \\(\\Delta_{\\text{t}}\\)PSNR을 제공한다. 이것은 각각 높은 충실도와 제어 가능성의 비디오를 생성하는 능력을 보여준다. C-ViViT는 전체 시공간 주의 메커니즘을 사용하여 동일한 매개변수 카운트에서 다른 두 아키텍처에 비해 메모리 소비가 상당히 높지만 성능이 향상되지는 않는다. 사실, C-ViViT는 과적합 경향을 나타내어 훈련 중 강력한 정규화가 필요하며, 이는 상당히 낮은 성능을 설명할 수 있다.\n' +
      '\n' +
      '##4 관련 업무\n' +
      '\n' +
      '**World models** Generative interactive environment is considered a class of _World Models_(Ha and Schmidhuber, 2018; Oh et al., 2015), which enable enable next-frame prediction on action inputs(Bamford and Lucas, 2020; Chiappa et al., 2017; Eslami et al., 2018; Hafner et al., 2020, 2021; Kim et al., 2020, 2021; Micheli et al., 2023; Nunes et al., 2020; Pan et al., 2022; Robine et al., 2023). 이러한 모델은 에이전트 훈련 시간에 직접적인 환경 경험 없이 학습 정책에 사용될 수 있기 때문에 에이전트 훈련에 유용할 수 있다. 그러나 모델 자체를 학습하려면 일반적으로 환경에서 직접 얻은 액션 조건 데이터가 필요하다. 대조적으로, 우리의 접근법은 비디오만으로 감독되지 않은 방식으로 세계 모델을 배우려고 한다. 최근, 스케일링 세계 모델에 대한 강조가 다시 증가하고 있다. GAIA-1(Hu et al., 2023) 및 UniSim(Yang et al., 2023)은 각각 자율 주행 및 로봇 조작을 위한 세계 모델을 학습한다. 이러한 접근 방식은 텍스트 및 액션 레이블을 모두 필요로 하는 반면, 우리는 공개적으로 사용 가능한 인터넷 비디오의 비디오 전용 데이터에서 교육하는 데 중점을 둔다.\n' +
      '\n' +
      '**Video models** 우리의 작업은 _video models_에 관한 것으로, 일반적으로 초기 프레임들(또는 텍스트)을 조건화하고 비디오 내의 나머지 프레임들을 예측한다(Blattmann et al., 2023; Brooks et al., 2024; Clark et al., 2019; Finn et al., 2016; Ho et al., 2022a, b; Hoppe et al., 2022; Kalchbrenner et al., 2017; Le Moing et al., 2017; Lotter et al., 2020; Luc et al., 2020; Singer et al., 2023; Walker et al., 2021; Yan et al., 2021; Yu et al., 2023). 우리의 접근법은 토큰화된 이미지 위에 MaskGIT(Chang et al., 2022) 및 ST-트랜스포머(Xu et al., 2020)를 사용하기 때문에 페나키(Villegas et al., 2023), TECO(Yan et al., 2023) 및 MaskViT(Gupta et al., 2023)와 같은 최근의 트랜스포머 기반 모델과 가장 유사하다. 비디오 모델들이 점점 더 제어가능해지고 있는 동안(예를 들어, (Huang et al., 2022)), 우리는 더 많은 에이전트인 목표를 찾고 데이터로부터 _latent 액션 공간_을 명시적으로 학습하여, 사용자들 또는 에이전트들이 잠재 액션-조건 예측들을 사용하여 모델을 "재생"할 수 있게 한다.\n' +
      '\n' +
      '**Playable Video Generation**Genie는 Playable Video Generation(PVG)을 넘어 일반화(Menapace et al., 2021), 여기서 잠재된 액션들은 비디오들로부터 직접 학습된 세계 모델들을 제어하기 위해 사용된다(Menapace et al., 2021, 2022). 지니와 달리 PVG는 프롬프트를 통해 완전히 새로운 환경을 생성하기보다는 도메인별 정적 예를 고려한다. 따라서 이 설정을 넘어서는 스케일링은 일반적인 방법과 교환하는 귀납적 편향을 감소시키면서 사소한 아키텍처 변경이 필요했다.\n' +
      '\n' +
      '**환경 생성** 우리의 작업은 또한 머신 러닝이 게임 코드를 직접 작성하는 언어 모델을 통해 게임 레벨 생성에 매우 효과적인 것으로 입증된 _Procedural Content Generation_(PCG, 예를 들어, Risi and Togelius, 2020, 2020)와 관련이 있다(Summerville et al., 2018), 최근에는 (Sudhakaran et al., 2023; Todd et al., 2023). 언어 모델들 자체는 또한 시각적 컴포넌트가 부족하지만 상호작용 환경(Wong et al., 2023)인 것으로 간주될 수 있다. 이와는 대조적으로, 레벨들은 픽셀들로부터 직접 학습되고 생성될 수 있으며, 이는 우리가 인터넷 비디오 데이터의 다양성을 활용할 수 있게 한다.\n' +
      '\n' +
      '**잠재 행동을 가진 훈련 에이전트** 선행 작업은 관찰(Edwards et al., 2019), 계획(Rybkin* et al., 2019) 및 사전 훈련 RL 에이전트(Schmidt and Jiang, 2024; Ye et al., 2022)로부터의 모방을 위한 잠재 행동을 사용하였다. 이러한 접근법은 잠재 행동 모델과 유사한 목적을 가지고 있지만 규모적으로 적용되지는 않았다. VPT(Baker et al., 2022)는 인간-제공 액션 라벨링된 데이터로부터 학습된 역 역학 모델을 사용하여, 정책 트레이닝에 사용될 수 있는 액션들로 인터넷-스케일 비디오들을 라벨링하는 최근의 접근법이다. 대조적으로, 우리는 비용이 많이 들고 일반화되지 않을 수 있는 지상 진실 액션의 필요성을 피하면서 인터넷 비디오에서 배운 _latent_ 액션을 사용하여 임의 환경에 대한 정책을 추론할 수 있음을 보여주었다.\n' +
      '\n' +
      '##5 결론 및 향후 과제\n' +
      '\n' +
      '우리는 인간이 설계한 시뮬레이션 환경을 통해 누구나, 심지어 어린이도 가능한 한 생성된 세상을 꿈꾸고 창조하고 발을 들여놓을 수 있는 새로운 형태의 생성 AI인 지니를 제안했다. 지니는 비디오 전용 데이터로부터 트레이닝됨에도 불구하고 다양한 대화식 및 제어가능한 환경 세트를 생성하도록 프롬프트될 수 있다.\n' +
      '\n' +
      '모델을 개선할 수 있는 분명한 개선 사항이 있습니다. 지니는 다른 자기회귀변압기 모델의 약점 중 일부를 상속받아 비현실적인 미래를 환각할 수 있다. 그리고 시공간적 표현으로 진전을 이루었지만, 여전히 16프레임의 메모리로 제한되어 있어 긴 지평에 걸쳐 일관된 환경을 얻기 어렵다. 마지막으로, 지니(Genie)는 현재 1FPS를 중심으로 동작하며, 상호작용을 위한 효율적인 프레임 레이트를 달성하기 위해 미래의 발전을 요구한다.\n' +
      '\n' +
      '그럼에도 불구하고, 우리는 지니가 미래의 연구를 위한 엄청난 잠재력을 열어준다고 믿는다. 일반성을 감안할 때, 이 모델은 다양하고 사실적이며 상상된 환경을 시뮬레이션하기 위해 훨씬 더 많은 비율의 인터넷 비디오에서 훈련될 수 있다. 또한 에이전트를 훈련하기 위해 지니를 사용하는 능력에 대해 간단히 언급했을 뿐이지만 풍부하고 다양한 환경의 부족이 RL의 핵심 제한 사항 중 하나임을 감안할 때 보다 일반적으로 유능한 에이전트를 생성하기 위한 새로운 경로를 잠금 해제할 수 있다.\n' +
      '\n' +
      '## Broader Impact\n' +
      '\n' +
      '**사회적 영향** 지니를 통해 많은 사람들이 자신의 게임과 같은 경험을 생성할 수 있습니다. 이는 창의성을 새로운 방식으로 표현하고자 하는 사람들에게 긍정적일 수 있는데, 예를 들어 자신의 상상된 세계를 디자인하고 발을 들여놓을 수 있는 어린이들이다. 우리는 또한 상당한 발전과 함께 이 기술을 사용하여 기존 인간 게임 세대와 창의성을 증폭하고 관련 산업이 차세대 플레이 가능한 세계 개발을 가능하게 하기 위해 지니를 활용할 수 있는 권한을 부여하는 것이 중요하다는 것을 인식합니다.\n' +
      '\n' +
      '**트레이닝 데이터 및 가중치**: 본 논문 또는 웹 사이트에 수반되는 트레이닝된 모델 체크포인트, 모델의 트레이닝 데이터세트 또는 그 데이터로부터 예를 공개하지 않기로 결정했다. 우리는 연구(및 비디오 게임) 커뮤니티에 더 관여하고 그러한 출시가 존중되고 안전하며 책임감을 가질 수 있는 기회를 갖고 싶습니다.\n' +
      '\n' +
      '**재현성**: 계산량이 적은 연구자가 주요 결과를 재현하는 것이 어려울 수 있음을 이해합니다. 이 문제를 완화하기 위해 우리는 단일 중간 범위 TPU(또는 GPU)에서 실행할 수 있는 부록 F에서 더 작은 규모의 완전히 재현 가능한 예를 설명한다. 많은 디자인 선택이 두 설정 사이에서 변환된다는 점을 감안할 때, 우리는 이것이 광범위한 커뮤니티가 우리의 작업으로 인한 추가 연구 방향뿐만 아니라 미래의 건축 개선을 조사하는 것을 가능하게 할 것이라고 믿는다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '마테우스 말리노우스키, 필립 볼, 루이 키르쉬, 캐시디 하딘, 데이비드 브리슨, 에릭 라우, 라스 로우 조수순드, 루카스 스마이라, 베르나르도 아빌라 피어스, 플랫폼 데이터 세트에 대한 도움, 비디오 모델 훈련 및 평가에 대한 귀중한 토론에 대한 루벤 빌레가스, 그리고 전략적 조언과 안내에 대한 아드리안 볼튼, 루실 메리에리, 한나 오프헨쇼, 주빈 가흐라마니, 라이아 해델, 코레이 카부쿠오글루, 다안 위어스트라, 도이나 프리큐프, 에드 허스트에게 감사드린다. 우리는 DeepMind Jax 생태계(Babuschkin et al., 2010)를 이용하며, 특히 앤디 브록에게 모델 훈련에 사용한 내부 프레임워크를 구축해준 것과 모델을 "재생"할 수 있는 초기 인터페이스를 제공한 Arthur Brussee에게 감사한다. 마지막으로, 세네카와 카스피언 클룬의 창의적인 스케치에 감사하며, 잠재적으로 가장 어린 게임 디자이너가 될 수 있습니다.\n' +
      '\n' +
      '## Author Contributions\n' +
      '\n' +
      '우리는 성별로 알파벳순으로 저자를 나열한다. 모든 서신을 Ashley Edwards (edwardsashley@google.com)와 Jack Parker-Holder (jparkerholder@google.com)로 보내주시기 바랍니다.\n' +
      '\n' +
      '### Core Contributors\n' +
      '\n' +
      '**Jake Bruce**: 프로젝트 리더십, 비디오토나이저 연구, 액션 모델 연구, 다이내믹 모델 연구, 스케일링, 모델 데모, 인프라\n' +
      '**Michael Dennis**: 다이나믹스 모델 연구, 스케일링, 메트릭스, 모델 데모, 인프라\n' +
      '**Ashley Edwards**: 지니 컨셉, 프로젝트 리더십, 액션 모델 연구, 에이전트 트레이닝, 모델 데모\n' +
      '**Edward Hughes** : 다이나믹스 모델 연구, 인프라\n' +
      '**Matthew Lai**: 데이터셋 큐레이션, 인프라\n' +
      '***Aditi Mavalankar**: 액션 모델 연구, 메트릭, 에이전트 트레이닝\n' +
      '***Jack Parker-Holder**: 지니 개념, 프로젝트 리더십, 역학 모델 연구, 스케일링, 데이터셋 큐레이션\n' +
      '***Yuge(지미) Shi**: 비디오 토큰라이저 연구, 동역학 모델 연구, 데이터셋 큐레이션, 메트릭스\n' +
      '\n' +
      '### 부분 기여자 및 조언자\n' +
      '\n' +
      '**Chris Apps** : 프로젝트 관리\n' +
      '**Yusuf Aytar** 기술적 조언\n' +
      '사라 베틀 기술적인 조언\n' +
      '**Feryal Behbahani*** 전략적 조언\n' +
      '**Stephanie Chan*** 기술적 조언\n' +
      '**Jeff Clune*** 기술적 조언, 전략적 조언\n' +
      '**루시 곤잘레스** 프로젝트 관리\n' +
      'Nicolas Heess** 전략적인 조언\n' +
      '**사이먼 오신데로*** 기술적 조언\n' +
      '** Sherjil Ozair*** 기술적 조언\n' +
      '**Scott Reed** 기술적 조언\n' +
      '**징웨이 장*** 기술적 조언\n' +
      '**Konrad Zolna*** 스케일링, 기술적 조언\n' +
      '\n' +
      '### Sponsors\n' +
      '\n' +
      '난도 드 프라이타스 전략 조언\n' +
      '지니 콘셉트 프로젝트 리더쉽\n' +
      'Satinder Singh** 전략적인 조언\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Babuschkin et al. (2020) I. Babuschkin, K. 봄리, A. 벨, S. 부파티라주, J. 브루스, P. 부클로프스키, D. 버든, T. Cai, A. Clark, I. Danihelka, et al. The deepmind jax ecosystem, 2020. _URL http://github. com/deepmind_, 2010.\n' +
      '* Bain et al.(2021) M. Bain, A. Nagrani, G. Varol, A. Zisserman. 시간 동결: 엔드 투 엔드 검색을 위한 조인트 비디오 및 이미지 인코더. _2021 IEEE/CVF International Conference on Computer Vision(ICCV)_, pages 1708-1718, Los Alamitos, CA, USA, Oct 2021. IEEE Computer Society. doi: 10.1109/ICCV48922.2021.00175.\n' +
      '* Baker et al. (2022) B. Baker, I. Akkaya, P. Zhokov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. 삼페드로, J. 클룬 비디오 사전 훈련(vpt): 레이블이 지정되지 않은 온라인 비디오를 보고 행동하는 법을 배우는 것; _ 신경 정보 처리 시스템_, 35:24639-24654, 2022에서의 발전.\n' +
      '* Bamford and Lucas (2020) C. Bamford and S. M. Lucas. 신경 게임 엔진: 픽셀에서 일반화할 수 있는 전방 모델을 정확하게 학습합니다. 2020년 Games_에 대한 회의에서.\n' +
      '* Bauer et al. (2023) J. Bauer, K. Baumli, F. Behbahani, A. Bhoopchand, N. 브래들리 슈미그 장남 클레이, A. 콜리스터, V. 다사기 곤잘레스 그레고르 E. 휴즈 S. 카셈 Loks-Thompson, H. Openshaw, J. Parker-Holder, S. 파탁노 페레즈 니에베스 라키셰비치 록타셀 S. 슈뢰커 싱재노스키 투일스 요크, A. 자컬, L. M. 장. 개방형 작업 공간에서 인간 시간 척도 적응 A. 크라우스, E. 브런스킬, K. 조병하르트 Sabato, and J. Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 1887-1935. PMLR, 23-29 Jul 2023.\n' +
      '* Blattmann et al. (2023a) A. Blattmann, T. 도크혼 컬랄, 디 멘델리비치, 엠 킬리안, D. 로렌츠, Y. 리바이, 지 영어, V 볼레티, A 레츠, V 잼파니, R 롬바흐 안정적인 비디오 확산: 잠재 비디오 확산 모델을 대형 데이터 세트로 스케일링하는 것, 2023a.\n' +
      '* Blattmann et al.(2023b) A. Blattmann, R. 롬바흐 H. 링, T. 도크혼, 김승원 피들러와 K 크레이즈 잠복 시간 정렬: 잠재 확산 모델과 고해상도 비디오 합성 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 22563-22575, 2023b.\n' +
      '* Brohan et al. (2021) A. Brohan, N. 브라운, J. 카르바할, Y. 체보타르, J. 다비스, C. 핀, K. 고팔라크리쉬난 Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. 잭슨 제스월 N.J. 조시, R. 줄리안, D. 칼라쉬니코프, Y. 광일열 -H. 이승환 레빈영 루우 Malla, D. Manjunath, I. Mordatch, O. 나첨, C. 파라다, J. 페랄타, E. 페레즈, K. 퍼치, J. 퀴암바오, K. 라오만 류지살라자르 P. 산케티 세이드, J. 싱, S. 손탁케, A. Stone, C. Tan, H. Tran, V. S. 바누크 베가, Q 부엉, F.샤, T. 샤오평수 서태호 유와 B. 지트코비치 Rt-1: 스케일에서의 실세계 제어를 위한 로봇 트랜스포머. _Robotics: Science and Systems_, 2023.\n' +
      '* Brooks et al.(2024) T. 브룩스, B. 피블스, C. 홈즈, W 디퓨영 곽락 징, 디 슈너, 제이 테일러, 티 루먼, E. 루먼, C. W. Y. 잉, R. 왕과 A. 라메시 비디오 생성 모델은 월드 시뮬레이터입니다. 2024. URL[https://openai.com/research/video-generation-models-as-world-simulator](https://openai.com/research/video-generation-models-as-world-simulator)\n' +
      '* Brown et al.(2020) T. 브라운, B. 만, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. 언어 모델은 소수 학습자이다. _ 신경 정보 처리 시스템_, 33:1877-1901, 2020의 발전.\n' +
      '* Chang 등(2022) H. Chang, H. Zhang, L. 장, 류, 그리고 W. T. 프리먼. 마스크킷: 마스크 생성 이미지 트랜스포머입니다. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11315-11325, June 2022.\n' +
      '\n' +
      'S. 치아파 라카니에르, D. 위어스트라, S. 모하메드 재귀 환경 시뮬레이터. _International Conference on Learning Representations_, 2017.\n' +
      '* Clark et al.(2019) A. Clark, J. Donahue, and K. 사이먼 복잡한 데이터셋에서 효율적인 동영상 생성. _ CoRR_, abs/1907.06571, 2019. URL[http://arxiv.org/abs/1907.06571](http://arxiv.org/abs/1907.06571).\n' +
      '* Clune(2019) J. Clune. Ai-gas: Ai-generating algorithms, the alternate paradigm for producing general artificial intelligence. _ arXiv preprint arXiv:1905.10985_, 2019.\n' +
      '* Cobbe et al.(2020) K. 코비, C. 헤세, J. 힐튼, J. 슐먼 강화 학습을 벤치마크하기 위해 절차 생성을 활용합니다. _Proceedings of the 37th International Conference on Machine Learning_, pages 2048-2056, 2020.\n' +
      '* Dehghani et al.(2023) M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. P. Steiner, M. 캐론 가이르호스, I. 알랍둘모신, R. 제나튼, L. 배이어 Tschannen A. Arnab, X 왕철미 루이즈 Minderer J. Puigcerver, U. 에비엠 Kumar, S. V. Steenkiste, G. F. Elsayed, A. Mahendran, F. Yu, A. Oliver, F. Huot, J. Bastings, M. 콜리어, A. A. 그리센코, V. Birodkar, C. N. Vasconcelos, Y. 테이, T Mensink, A. Kolesnikov, F. Pavetic, D. Tran, T. 킵프 루시크, 엑스 Zhai, D. Keysers, J. J. Harmsen, and N. 홀스비 시력 변환기를 220억 개의 매개변수로 확장합니다. A. 크라우스, E. 브런스킬, K. 조병하르트 Sabato, and J. Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 7480-7512. PMLR, 23-29 Jul 2023.\n' +
      '* Dosovitskiy et al.(2021) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. 자이태 Unterthiner, M 데하니 민더러, G. 헤이골드, S. Gelly, J. Uszkoreit, N. 홀스비 이미지는 16x16 단어의 가치가 있습니다: 스케일에서 이미지 인식을 위한 트랜스포머입니다. _International Conference on Learning Representations_, 2021. URL[https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy).\n' +
      '* Edwards et al. (2019) A. Edwards, H. Sahni, Y. 슈뢰커와 C. 이스벨 관찰에서 잠재된 정책을 모방합니다. _International conference on machine learning_, pages 1755-1763. PMLR, 2019.\n' +
      '* Eslami et al.(2018) S. M. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. 가넬로, A. 루더만, A. A. 루수, I. 다니헬카, K. 그레고르, D. P. 라이허트, L. 부싱, T 베버, 오 Vinyals, D. Rosenbaum, N. Rabinowitz, H. King, C. Hillier, M. 보트비닉, D. 위어스트라, K 카부쿠오글루와 D. 하사비스 뉴럴 장면 표현과 렌더링 Science_, 360(6394):1204-1210, 2018. doi: 10.1126/science.aar6170.\n' +
      '* Esser et al. (2023) P. Esser, J. Chiu, P. Atighehchian, J. Granskog, and A. Germanidis. 확산 모델을 사용한 구조 및 내용 유도 비디오 합성. In _2023 IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '*Finn et al. (2016) C. Finn, I. Goodfellow, and S. 레빈 비디오 예측을 통한 물리적 상호작용을 위한 비지도 학습. In _Proceedings of the 30th International Conference on Neural Information Processing Systems_, NIPS\'16, page 64-72, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819\n' +
      '* Gupta et al. (2023) A. Gupta, S. 천영 장정우 마틴-마틴, L. 페이페이 마스킷: 비디오 예측을 위한 마스킹된 시각적 사전-트레이닝. _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Ha and Schmidhuber (2018) D. Ha and J. Schmidhuber. 재발 세계 모델은 정책 진화를 촉진한다. *Proceedings of the 32Nd International Conference on Neural Information Processing Systems_, NeurIPS\'18, pages 2455-2467, 2018.\n' +
      '\n' +
      '* Hafner et al.(2020) D. Hafner, T. Lillicrap, J. Ba and M. 노루지 통제하는 꿈: 잠재된 상상력에 의한 행동을 배우는 것. _International Conference on Learning Representations_, 2020.\n' +
      '* Hafner et al. (2021) D. Hafner, T. P. Lillicrap, M. 노루지, J. 바. 이산 세계 모델로 아타리를 마스터합니다. _International Conference on Learning Representations_, 2021.\n' +
      '* He et al.(2016) K. 그, X 장승 렌과 J. 선 이미지 인식을 위한 딥 잔차 학습. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2016. doi: 10.1109/CVPR.2016.90.\n' +
      '* Henry et al. (2020) A. Henry, P. R. Dachapally, S. S. Pawar, and Y. 첸 변압기에 대한 쿼리-키 정규화. _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 4246-4253, Online, Nov. 2020. Computational Linguistics Association. doi: 10.18653/v1/2020.findings-emnlp.379.\n' +
      '* Ho et al. (2022a) J. Ho, W. 찬찬사하리아 J.황 가오, A. 그리센코, D. P. 킹마, B. 풀, M. 노루지, D. J. 함대, T. 살리만 Imagen video: High definition video generation with diffusion models, 2022a.\n' +
      '* Ho et al.(2022b) J. Ho, T. 살리만, A. 그리센코, W. 찬민 노루지, 디제이 함대 비디오 확산 모델 인석 고예조 모하메드, A. 아가왈, D. 벨그레이브, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 8633-8646. Curran Associates, Inc., 2022b.\n' +
      '* Hong et al.(2022) W. 홍민 딩원 정철 류종완 Cogvideo: 트랜스포머를 통한 텍스트-비디오 생성을 위한 대규모 사전 훈련. _ ArXiv:2205.15868_, 2022.\n' +
      '* Hong et al.(2023) W. 홍민 딩원 정철 류종완 Cogvideo: 트랜스포머를 통한 텍스트-비디오 생성을 위한 대규모 사전 훈련. _The Eleventh International Conference on Learning Representations_, 2023. URL[https://openreview.net/forum?id=rB6TpjAuSRy](https://openreview.net/forum?id=rB6TpjAuSRy)\n' +
      '* Hoppe et al.(2022) T. 호피 A. 메르주 S. 바우어, D. 닐슨, A. 디타디 비디오 예측 및 주입을 위한 확산 모델. _ Transactions on Machine Learning Research_, 2022. ISSN 2835-8856.\n' +
      '* Hu et al. (2023) A. Hu, L. 러셀여지 Murez, G. Fedoseev, A. Kendall, J. Shotton and G. Corrado. 가이아-1: 자율주행을 위한 생성 세계 모델, 2023.\n' +
      '* ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVI_, page 546-564, Berlin, Heidelberg, 2022. Springer-Verlag. ISBN 978-3-031-19786-4\n' +
      '* Jouppi et al. (2020) N. P. Jouppi, D. H. Yoon, G. Kurian, S. 이남 패틸, J. 로든, C. 영, D. 패터슨 심층 신경망을 훈련시키기 위한 도메인-특정 슈퍼컴퓨터. _ ACM_, 63(7):67-78, 2020의 통신.\n' +
      '* Kalashnikov et al. (2018) D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. 칼라크리쉬난, V. Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. _ arXiv preprint arXiv:1806.10293_, 2018.\n' +
      '* Kalchbrenner et al.(2017) N. 칼치브레너, A. 밴 덴 오드, K. 시모니안, I. 다니헬카, O. 빈얼스, A 그레이브스, K 카부쿠오글루 비디오 픽셀 네트워크. D. Precup and Y. W. Teh, Editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 1771-1779. PMLR, 06-11 Aug 2017. URL[https://proceedings.mlr.press/v70/kalchbrenner17a.html](https://proceedings.mlr.press/v70/kalchbrenner17a.html).\n' +
      '\n' +
      '* Kapturowski et al. (2018) S. 캡투로우스키, G. 오스트로프스키, J. 콴, R. 무노스, W. 대브니 순환 경험은 분산 강화 학습에서 다시 재생됩니다. _International conference on learning representations_, 2018.\n' +
      '*Kim et al.(2020) S. W. Kim, Y. Zhou, J. Philion, A. Torralba, S. 피들러 게임간으로 동적 환경을 시뮬레이션하는 학습 In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2020.\n' +
      '* Kim et al. (2021) S. W. Kim, J. Philion, A. Torralba, and S. 피들러 드라이브건: 제어 가능한 고품질 신경 시뮬레이션을 향합니다. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5820-5829, June 2021.\n' +
      '* Le Moing et al. (2021) G. Le Moing, J. Ponce, and C. Schmid. Ccvs: 상황 인지 제어 가능한 비디오 합성. 인민 란자토, A. 베이겔지머, Y. 도핀, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 14042-14055. Curran Associates, Inc., 2021.\n' +
      '* Lotter et al.(2017) W. 로터, G. 크레이먼, D. 콕스 비디오 예측 및 비지도 학습을 위한 딥 예측 코딩 네트워크. _International Conference on Learning Representations_, 2017. URL[https://openreview.net/forum?id=B1ewdt9xe](https://openreview.net/forum?id=B1ewdt9xe).\n' +
      '* Luc 등(2020) P. Luc, A. Clark, S. Dieleman, D. de Las Casas, Y. 도론, A. 캐시러, K. 사이먼 대용량 데이터에 대한 변환 기반 적대적 비디오 예측 _ CoRR_, abs/2003.04035, 2020.\n' +
      '* Menapace et al. (2021) W. 메나페이스, S S. 라투아릴리에리 Tulyakov, A. Siarohin, E. Ricci. 재생 가능한 비디오 생성입니다. _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021_, pages 10061-10070. Computer Vision Foundation/IEEE, 2021.\n' +
      '* Menapace et al. (2022) W. 메나페이스, S Lathuiliere, A. Siarohin, C. Theobalt, S. 툴야코프 골야닉과 E. 리치 재생 가능한 환경: 시공간에서 비디오 조작 In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.\n' +
      '* Micheli et al.(2023) V. 미셸리, E. 알론소, F. 플뢰렛 트랜스포머는 샘플 효율적인 세계 모델입니다. _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Nunes et al. (2020) M. S. Nunes, A. Dehban, P. Moreno, and J. Santos-Victor. 로봇 비디오 예측 모델의 액션 조건 벤치마킹: 비교 연구. _2020 IEEE International Conference on Robotics and Automation (ICRA)_에서, 페이지 8316-8322, 2020. doi: 10.1109/ICRA40945.2020.9196839.\n' +
      '* Volume 2_, NIPS\'15, page 2863-2871, Cambridge, MA, USA, 2015. MIT Press.\n' +
      '* Open Ended Learning Team et al. (2021) Open Ended Learning Team, A. Stooke, A. Mahajan, C. Barros, C. Deck, J. Bauer, J. Sygnowski, M. 트레바츠 엠제이더버그 마티외 맥알레즈 브래들리 슈미그 왕남 포셀 라일라누 휴즈 피트, V. 달리바르와 W. M. 차르네키 개방형 학습은 일반적으로 유능한 에이전트로 이어집니다. _ CoRR_, abs/2107.12808, 2021.\n' +
      '* Oquab et al.(2023) M. 오콰브 다르셋, T 무타카니, H. Vo, M. V. 샤프라니크 Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _ arXiv preprint arXiv:2304.07193_, 2023.\n' +
      '* Pan et al.(2022) M. 판진 주영 왕, X 양 Iso-dream: 세계 모델에서 통제할 수 없는 시각적 역동성을 고립시키고 활용한다. 인석 고예조 모하메드, A. 아가왈, D. 벨그레이브, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 23178-23191. Curran Associates, Inc., 2022.\n' +
      '\n' +
      'A. Radford, K. 나라심한 살리만과 나, 서츠키버 생성적 사전 훈련을 통해 언어 이해도를 향상시킵니다. 2018년\n' +
      '* Radford et al.(2019) A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models is unsupervised multitask learners. _ OpenAI blog_, 1(8):9, 2019.\n' +
      '* Rajbhandari 등(2020) S. 라지반다리, J. 래슬리, 오. 루와세, 그리고 Y 그 사람이요 제로: 조 단위 매개변수 모델을 훈련하기 위한 메모리 최적화 In _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-16. IEEE, 2020.\n' +
      '* Ramesh et al.(2021) A. Ramesh, M. 파블로프 고상 그레이, C. 보스 A. 래드포드, M. 첸과 나, 서츠키버 제로 샷 텍스트 대 이미지 생성 인민 밀라와 T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 8821-8831. PMLR, 18-24 Jul 2021.\n' +
      '* Ramesh et al. (2022) A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. 첸 2022년, 클립 래턴트를 이용한 계층적 텍스트 조건 이미지 생성\n' +
      '*Reed et al.(2022) S. 리드경 Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-maron, M. 기메네스 설스키, 제이 케이, 제이 티 스프링엔버그, 티 Eccles, J. Bruce, A. Razavi, A. Edwards, N. 헤스영 천록 해델오 빈얼스, M Bordbar, N. 데 프리타스 일반 요원입니다 Machine Learning Research_, 2022. ISSN 2835-8856. Featured Certification, Outstanding Certification.\n' +
      '* Risi and Togelius (2020) S. 리시와 J. 토글리어스 절차적 내용 생성을 통한 기계 학습의 일반성 증가 Nature Machine Intelligence_, 2, 08 2020a. doi: 10.1038/s42256-020-0208-z.\n' +
      '* Risi and Togelius (2020) S. 리시와 J. 토글리어스 절차적 콘텐츠 생성: 게임 레벨을 자동으로 생성하는 것부터 머신 러닝의 일반성을 높이는 것까지. _ Nature_, 2020b.\n' +
      '* Robine et al. (2023) J. Robine, M. 호프트만 Uelwer, S. 하멜링 트랜스포머 기반 세계 모델은 100k 상호 작용에 만족합니다. _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Rombach et al.(2022) R. 롬바흐, A. 블랫만, D. 로렌츠, P. 에서, B. 옴머. 잠재 확산 모델을 사용한 고해상도 이미지 합성 _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, June 2022.\n' +
      '* Rybkin* et al.(2019) O. 리빈기 Pertsch*, K. G. Derpanis, K. 다니일리디스, 그리고 A. 재글. 무언가를 하기 전에 할 수 있는 것을 배우세요. _International Conference on Learning Representations_, 2019.\n' +
      '* Saharia et al. (2022) C. Saharia, W. 찬성 색세나 L. 이종황 곤티조롭스, B. K. 아얀, T. 살리만, J. 호, D. J. 함대, M. 노루지 사실적 텍스트-이미지 확산 모델은 깊은 언어 이해도를 가지고 있다. A. H. 오, A. 아가왈, D. 벨그레이브, K. Cho, editorators, _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* 슈미트 및 장(2024) D. 슈미트 및 M. 장 행동 없이 행동하는 법을 배우는 것. _The Twelfth International Conference on Learning Representations_, 2024.\n' +
      '* Shoeybi et al.(2019) M. 회비 패트워리 Puri, P. LeGresley, J. Casper, B. Catanzaro 메가트론-1m: 모델 병렬성을 사용하여 수십억 매개 변수 언어 모델을 훈련합니다. _ CoRR_, abs/1909.08053, 2019. URL[http://arxiv.org/abs/1909.08053](http://arxiv.org/abs/1909.08053).\n' +
      '* Singer et al.(2023) U. 가수 A. 폴리악, T. 헤이즈, X 음정안 장규 허현양 회설, O. 가프니, D. 파릭, S. 굽타와 Y 타이그먼 메이크-어-비디오: 텍스트-비디오 데이터가 없는 텍스트-투-비디오 생성. _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '\n' +
      'S. 수다카란 곤잘레스-두케, C. 글라노이스, M. 프리베르거, E. 나자로, S. 리시 즉시 유도 수준 생성 _Proceedings of the Companion Conference on Genetic and Evolutionary Computation_, pages 179-182, 2023.\n' +
      '* Summerville et al. (2018) A. Summerville, S. 스노드그래스 Guzdial, C. Holmgard, A. K. Hoover, A. Isaksen, A. Nealen, and J. Togelius. 기계 학습(PCGML)을 통한 절차적 콘텐츠 생성. _ IEEE Trans. Games_, 10(3):257-270, 2018.\n' +
      '* Todd et al. (2023) G. Todd, S. Earle, M. U. Nasir, M. C. Green, J. Togelius. 대용량 언어 모델을 통한 레벨 생성 _Proceedings of the 18th International Conference on the Foundation of Digital Games_, pages 1-8, 2023.\n' +
      '* Torabi et al. (2018) F. Torabi, G. Warnell, and P. Stone. 관찰에서 행동 복제. _ arXiv preprint arXiv:1805.01954_, 2018.\n' +
      '* Unterthiner et al.(2019) T. 언터타이너, S 반스틴키스트 쿠라흐 마리니에르 미칼스키와 S 젤리 FVD: 2019년 비디오 생성을 위한 새로운 메트릭.\n' +
      '* van den Oord et al. (2017) A. van den Oord, A. Razavi, B. Uria, Caglar Unlu, C. Nash, C. Wolff, C. Durkan, D. Ding, D. Gorny, E. Gladchenko, F. Riedel, H. Qi, J. Kelly, J. Bauer, J. Donahue, J. Zhang, M. 말리노스키 빈코스키, P. Luc, R. (주)리아이치 스트루델, T. P. I. 샌더 디엘만, Y. 가닌과 Z Eaton-Rosen Imagen 2. URL[https://deepmind.google/technology/imagen-2/](https://deepmind.google/technology/imagen-2/).\n' +
      '* van den Oord et al. (2017) A. van den Oord, O. 빈널스, K 카부쿠오글루 신경 이산 표현 학습. In _Proceedings of the 31th International Conference on Neural Information Processing Systems_, NIPS\'17, page 6309-6318, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964\n' +
      '* Vaswani et al. (2017) A. Vaswani, N. N. 쉐이저 파마르, J. 우즈코리트, L. 존스, A. N. 고메즈, L. 카이저, 나 폴로수킨 주목만 해주시면 됩니다. _Advances in Neural Information Processing Systems_, pages 5998-6008, 2017.\n' +
      '* Villegas et al.(2023) R. M. 빌레가스 바베이자데, P.J. 킨더먼스, H. 모랄도, H. 장, M. T. 사파, S. 카스트로, J. 쿤제, D. 에르한 Phenaki: 열린 도메인 텍스트 설명으로부터 가변 길이 비디오 생성. _International Conference on Learning Representations_, 2023.\n' +
      '* Walker et al. (2021) J. C. Walker, A. Razavi, and A. van den Oord. 2021년 VQVAE로 영상 예측\n' +
      '* Wang et al.(2023) Y. 왕영 허영 이경 이종유 마진 천영 왕필루오 유영 왕락 왕, Y. 카오 인테르비드: 멀티모달 이해 및 생성을 위한 대규모 비디오 텍스트 데이터 세트, 2023.\n' +
      '* Wong et al.(2023) L. Wong, G. Grand, A. K. Lew, N. D. Goodman, V. K. Mansinghka, J. Andreas, and J. B. Tenenbaum. 단어 모델에서 세계 모델로: 자연 언어에서 사고의 확률 언어로 번역, 2023.\n' +
      '* Wu et al. (2022) C. Wu, J. Liang, L. 지필양 방, 장, N. 듀안 Niwa: 신경 시각 세계 생성을 위한 시각 합성 사전 훈련. 유럽 컴퓨터 비전 회의에서 720-736쪽 2022년 스프링어\n' +
      '* Xu et al.(2020) M. 서원 다이창유 가오원 린지중 기, H. 시온그. 교통량 예측을 위한 시공간 변압기 네트워크. _ arXiv preprint arXiv:2001.02908_, 2020.\n' +
      '* Yan et al. (2021) W. 연영 장, P. 아벨, 그리고 A. 스리니바스. Videgopt: vq-vae and transformer, 2021을 이용한 비디오 생성.\n' +
      '\n' +
      '* Yan et al.(2023) W. 양동해프너 제임스, P. 애비엘 비디오 생성을 위한 시간적으로 일관된 변압기. A. 크라우스, E. 브런스킬, K. 조병하르트 Sabato, and J. Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 39062-39098. PMLR, 23-29 Jul 2023.\n' +
      '* Yang et al.(2023) M. 양영 두경 Ghasemipour, J. Tompson, D. Schuurmans, P. Abbeel. 인터랙티브 현실 세계 시뮬레이터를 학습합니다. _ arXiv preprint arXiv:2310.06114_, 2023.\n' +
      '* Ye et al.(2022) W. 예영 장피애빌, Y. 가오 순수 동영상 시청을 통해 제한된 데이터로 능숙한 플레이어가 되세요. _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* Yu et al.(2023) L. 유영 청경 손재자마 H. Zhang H. Chang A. G. Hauptmann M. 양영 하오, I. 에사, L. 장 마스킷: 마스크 생성 비디오 트랜스포머입니다. _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)_, pages 10459-10469, Los Alamitos, CA, USA, jun 2023. IEEE Computer Society. doi: 10.1109/CVPR52729.2023.01008.\n' +
      '\n' +
      '## 추가 예제 궤적\n' +
      '\n' +
      '그림 16: **더 많은 예제 궤적**: 모델은 손으로 그린 스케치, 텍스트에서 이미지 생성 모델로 생성된 이미지 또는 사실적인 사진으로 프롬프트됩니다. 궤적의 동역학을 구동하는 동작은 인간의 입력에 의해 제공된다.\n' +
      '\n' +
      '### Dataset\n' +
      '\n' +
      '#### Platformers Dataset\n' +
      '\n' +
      'Initial DatasetWe는 다음과 같은 기준을 사용하여 공개적으로 사용 가능한 인터넷 비디오를 필터링하여 데이터 세트를 생성했다.\n' +
      '\n' +
      '* 제목은 2D 플랫폼 게임과 관련된 키워드를 포함한다.\n' +
      '* 제목 또는 설명은 "스피드런" 또는 "플레이스루"와 같은 액션 워드를 포함해야 한다.\n' +
      '* 제목은 "영화" 또는 "언박싱"과 같은 부정 단어를 포함하지 않아야 한다.\n' +
      '\n' +
      '그런 다음 각 비디오를 10 FPS에서 16s 클립으로 분할하는데, 이는 클립당 160 프레임에 해당한다. 결과 데이터 세트에는 총 244k 시간인 55M 비디오가 포함되어 있습니다. 키워드를 선택할 때, 유사한 키워드를 공유하는 다른 종류의 비디오에 비해 수적으로 열등하지 않은 2D 플랫폼 게임 플레이 비디오를 일반적으로 생산하는지 확인하기 위해 수동으로 결과를 확인했다.\n' +
      '\n' +
      '필터 파이프라인 데이터 세트의 많은 비디오가 품질이 좋지 않아 모델 성능에 영향을 미친다는 것을 발견했다. 우리는 Baker et al.(2022)과 같이 학습된 분류기를 사용하여 데이터를 체계적으로 필터링하기 위한 확장 가능한 접근법을 제안한다. 첫째, 고품질 비디오를 표시하는 비디오로 정의합니다.\n' +
      '\n' +
      '그림 17: Platformers의 **통제 가능하고 일관된 잠재 동작: Platformers 데이터 세트의 4가지 다른 시작 프레임에서 시작하는 궤적. 각 열은 동일한 잠재 행동을 다섯 번 취함으로써 얻어지는 프레임을 보여준다. 액션 레이블이 없는 훈련에도 불구하고, 다양한 프롬프트 프레임에 걸쳐 동일한 액션이 일관될 뿐만 아니라, 의미론적 의미를 갖는다: _left_, _right_, _jump_ 및 _no-op_.**clear 게임 플레이는 메뉴 스크린 또는 스트리머 페이스와 같은 산만 아이템들을 포함하지 않는다. 그런 다음 이 데이터를 다음과 같이 필터링합니다.\n' +
      '\n' +
      '1. 우리 팀은 약 10시간의 총 인간 노력으로 10k개의 비디오를 손으로 표시했다. 라벨은 5(최고)에서 1(최악의) 품질 범위였다.\n' +
      '2. 2-4로 평가된 엔트리를 모두 삭제하고 5는 양호, 1은 불량으로 분류한 이진 분류로 11M 파라미터 ResNet18(He et al., 2016)을 훈련시켰다.\n' +
      '3. 영상 유지 여부를 결정하기 위해 모델 예측과 신뢰도에 기반한 결정 규칙을 적용한다.\n' +
      '\n' +
      '이전 작업 Baker et al. (2022); Oquab et al. (2023)의 결과와 일치하게, 고품질 데이터를 갖는 Oquab et al. (2023)은 데이터의 양을 능가한다 - 큐레이션된 데이터 세트가 원래 데이터 세트의 크기의 10%를 약간 넘지만, 큐레이션된 데이터 세트에 대해 트레이닝된 모델은 FVD 측면에서 더 우수하지만, 표 4를 참조한다. 우리의 최종 데이터 세트는 총 30k 시간 동안 6.8M 비디오이다.\n' +
      '\n' +
      '표 4 | 데이터세트 큐레이션의 효과.\n' +
      '\n' +
      '### Training details\n' +
      '\n' +
      '#### 잠복 행동 모델 훈련\n' +
      '\n' +
      '우리는 인간 및 AI 에이전트의 재생성 감소를 비용으로 코드 수(즉, 액션 수)를 증가시킴으로써 이점을 발견했다.\n' +
      '\n' +
      '표 5| Platformers action model hyperameters\n' +
      '\n' +
      '모델 입력들은 0과 1 사이에서 정규화되고 디코더의 최종 출력들은 시그모이드(sigmoid)를 통해 배치된다는 점에 유의한다.\n' +
      '\n' +
      '#### 비디오 토큰자 훈련\n' +
      '\n' +
      '여기에서 비디오 토큰화자 교육에 대해 설명합니다. 우리는 인코더보다 디코더를 확장하는 것이 더 효과적이며 배치 크기 증가로 인한 한계 이득을 발견했다(표 6 참조).\n' +
      '\n' +
      '표 8의 하이퍼파라미터를 사용하여 코사인 감쇠와 함께 AdamW 최적화기를 사용하여 300k 단계에 대한 비디오 토큰화기를 훈련한다.\n' +
      '\n' +
      '### 동역학 모델 훈련\n' +
      '\n' +
      '#### 스케일링 실험 상세\n' +
      '\n' +
      '이 섹션에서는 스케일링 실험을 위한 예산 계산뿐만 아니라 아키텍처에 대한 자세한 내용을 제공한다.\n' +
      '\n' +
      '모든 모델에 대한 모델 크기 조정은 256의 배치 크기를 사용하며, 200k 단계에 대해 모든 모델을 훈련하므로 각 실행에 대해 총 750B 훈련 토큰을 사용한다. 모든 실행은 배치 병렬과 스테이지-3 ZeRO 샤딩(Rajbhandari et al., 2020)을 사용하고, 더 큰 모델도 텐서 병렬을 사용한다(Shoeybi et al., 2019). 이 실험을 위해 우리는 TPUv2와 TPUv3을 사용한다(Jouppi et al., 2020). 자세한 내용은 표 10을 참조하십시오.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline batch\\_size & training hardware & FLOPs & PSNR \\\\ \\hline\n' +
      '64 & 64 TPUv2 & \\(4.22\\times 10^{20}\\) & 35.7\\\\\\{Times 10^{20}\\)\n' +
      '384 & 64 TPUv3 & \\(2.57\\times 10^{21}\\) & 36.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 토큰화기 배치 크기 스케일링 하이퍼파라미터.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Component & Parameter & Value \\\\ \\hline Encoder & num\\_layers & 12 \\\\  & d\\_model & 512 \\\\  & num\\_heads & 8 \\\\  & k/q\\_size & 64 \\\\ \\hline Decoder & num\\_layers & 20 \\\\  & d\\_model & 1024 \\\\  & num\\_heads & 16 \\\\  & k/q\\_size & 64 \\\\ \\hline Codebook & num\\_codes & 1024 \\\\  & patch\\_size & 4 \\\\  & latent\\_dim & 32 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: Platformers video tokenizer hyperparameters.\n' +
      '\n' +
      '배치 크기를 스케일링하는 모든 모델은 표 11과 같이 2.3B 매개변수로 동일한 아키텍처를 사용하고 200k 단계를 훈련한다. 세 실행 간의 유일한 차이점은 하드웨어입니다. 128, 256 및 448 배치 크기 모델은 각각 64 TPUv3, 128 TPUv3 및 64 TPUv5p에서 훈련됩니다.\n' +
      '\n' +
      '표 11 | 배치 크기 스케일링 하이퍼파라미터. 모든 모델은 배치 크기만 다른 200k 단계에 대해 다음 아키텍처를 사용합니다.\n' +
      '\n' +
      '지니 모델 최종 지니 모델에 대한 파라미터 카운트, 모델 아키텍처 및 동역학 모델의 계산 사용은 표 12에 나열되어 있으며, 256 TPUv5를 사용하여 총 125k 단계에 대해 배치 크기가 512인 10.1B 동역학 모델을 훈련한다.\n' +
      '\n' +
      '행동 복제 세부 정보\n' +
      '\n' +
      '이 섹션에서는 행동 복제 실험에 대한 자세한 내용을 제공합니다. 우리는 Procgen CoinRun 환경(Cobbe et al., 2020) 내에서 훈련하고 보류된 테스트 세트에서 평가한다. 우리는 R2D2로 훈련된 에이전트로부터 이 환경에서 전문가 시퀀스의 데이터 세트를 가지고 있다고 가정한다(Kapturowski et al., 2018). 그런 다음 에이전트가 이 데이터를 모방하도록 훈련합니다. 특히, 오라클 에이전트는 대응하는 지상-진실 전문가 액션들에 액세스할 수 있다. 우리는 이제 미리 훈련된 LAM을 사용하여 취한 조치를 추론할 수 있는 방법에 대해 논의한다.\n' +
      '\n' +
      '### Genie LAM\n' +
      '\n' +
      '에이전트가 보이지 않는 동영상에서 모방하도록 훈련하기 위해 인터넷 동영상으로 훈련된 지니 모델의 냉동 LAM을 사용할 수 있다. 전문가 시퀀스\\(\\langle x_{t},x_{t+1}\\rangle\\)이 주어지면 해당 시퀀스를 추출한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Parameters & num\\_layers & num\\_heads & d\\_model & k/q size \\\\ \\hline\n' +
      '2.3B & 34 & 20 & 2560 & 128 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 모델 크기 스케일링 아키텍처 및 계산 사용량. 모든 모델은 750B 토큰과 동일한 배치 크기가 256인 200k 단계에 대해 훈련되었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Parameter & Value \\\\ \\hline max\\_lr & 3e-5 \\\\ min\\_lr & 3e-6 \\\\ \\(\\beta_{1}\\) & 0.9 \\\\ \\(\\beta_{2}\\) & 0.9 \\\\ weight\\_decay & 1e-4 \\\\ warmup\\_steps & 5k \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: Dynamics model optimizer hyperparameterslatent action label \\(a_{t}\\gets LAM(x_{t},x_{t+1})\\). 그런 다음 관찰이 주어졌을 때 전문가가 잠재 행동을 취할 가능성을 예측하는 정책\\(\\pi(a_{t}|x_{t})\\)을 훈련한다. 이 절차는 비디오로부터 학습하는 선행 작업들과 유사하다는 점에 유의한다(Baker et al., 2022; Torabi et al., 2018). 그러나 이러한 접근 방식은 비디오 라벨링에 지상 진실 행위를 사용하는 반면, 우리는 완전히 오프라인에서 학습된 잠재 행위를 활용한다.\n' +
      '\n' +
      '추론하는 동안 우리는 정책이 방출하는 잠재 행동을 실제 행동으로 매핑해야 한다. 이를 위해 우리는 액션 라벨이 붙은 전문가 서열의 작은 세트를 활용한다. 전문가 시퀀스\\(\\langle x_{t},u_{t},x_{t+1}\\rangle\\)(우리는 예측된 잠재 행동과의 혼동을 피하기 위해 지상-진실 행동에 대해 \\(u_{t}\\)을 나타냄)이 주어지면, LAM을 사용하여 잠재 행동을 구하고, 매핑된 잠재들로 구성된 사전 \\(a_{t}\\)을 해당 실제 행동 목록에 채운다. 요약하면, 환경으로부터 관측치 \\(x_{t}\\)이 주어졌을 때, 우리는 \\(a_{t}\\sim\\pi(s_{t})\\)로 가장 가능성이 높은 잠재 행동을 얻을 수 있고, 그리고 나서 \\(u_{t}\\sim D[a_{t}]\\으로 대응하는 실제 행동을 취할 수 있다.\n' +
      '\n' +
      '다른 작업은 에이전트의 정책에서 추출된 데이터를 사용하여 잠재 행동에서 실제 행동으로 매핑을 얻었지만(Edwards et al., 2019; Ye et al., 2022), 전문가 데이터를 사용하여 학습된 정책의 품질을 더 잘 평가할 수 있음을 발견했다. 본문에서 볼 수 있듯이 에이전트는 200개 정도의 전문가 레이블로 적응할 수 있었다.\n' +
      '\n' +
      '### Architecture\n' +
      '\n' +
      '우리는 오라클 및 잠복 BC 에이전트 모두에 대한 정책으로서 변압기를 훈련한다. 본 논문에서 제안한 STViViT 구조를 이용하여 프레임 \\(x_{1:t}=(x_{1},\\cdots x_{t})\\)을 부호화한다. 모든 이전 액션들은 원-핫을 통해 배치된 다음, 부가적 임베딩으로서 대응하는 프레임 인코딩과 결합된다. 우리는 훈련과 추론 동안 4의 시퀀스 길이와 16의 배치 크기를 사용한다.\n' +
      '\n' +
      '오라클과 지니 LAM은 모두 타겟이 각각 실제 또는 잠재된 액션인 교차 엔트로피 손실로 훈련된다. 추론하는 동안 우리는 샘플링에 의해 최종 예측을 얻는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Parameter & Value \\\\ \\hline max\\_lr & 3e-5 \\\\ min\\_lr & 3e-6 \\\\ \\(\\beta_{1}\\) & 0.9 \\\\ \\(\\beta_{2}\\) & 0.96 \\\\ weight\\_decay & 1e-4 \\\\ warmup\\_steps & Sk \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: BC 모델 최적화기 하이퍼파라미터\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Component & Parameter & Value \\\\ \\hline Encoder & num\\_layers & 12 \\\\  & d\\_model & 512 \\\\  & patch\\_size & 4 \\\\ \\hline Policy & linear\\_layer & 512 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 14: BC 정책 하이퍼파라미터는 예측된 로짓이다. 우리가 무작위로 10%의 시간을 샘플링했을 때 오라클 에이전트가 더 잘 수행되었음을 발견했다.\n' +
      '\n' +
      '### 재현사례연구\n' +
      '\n' +
      '이 섹션에서는 일주일 이내에 단일 중간 범위 TPU/GPU로 훈련할 수 있는 자체적이고 완전히 재현 가능한 사례 연구를 설명한다.\n' +
      '\n' +
      '### Data Collection\n' +
      '\n' +
      '먼저 모델을 훈련하기 위해 데이터를 수집해야 합니다. Procgen 벤치마크(Cobbe et al., 2020)로부터 CoinRun 환경을 이용하는데, 이는 상당히 간단한 플랫폼과 같은 역학으로 수천 개의 시각적으로 다양한 수준을 가지고 있기 때문이다. "하드" 모드를 사용하여 액션 반복이 없는 무작위 정책을 사용하여 데이터를 수집한다. 우리는 0에서 10,000 사이의 수준 종자를 샘플링하고 총 10M 전환을 위해 각 수준에 대해 1,000개의 타임스테프를 수집한다.\n' +
      '\n' +
      '### 비디오 토큰화자 훈련\n' +
      '\n' +
      '코인런을 위한 비디오 토큰화기는 섹션 C.2에서와 같이 최적화기 구성으로 훈련된 섹션 2.1에 설명된 것과 동일한 설정을 따른다. 이 예에서 주요 차이점은 더 작은 모델 크기를 사용하고(표 15 참조), 배치당 총 768개의 이미지에 대해 길이 16의 48개 시퀀스의 배치 크기를 사용하는 것이다. 16G 메모리가 있는 단일 TPU에 장착하기에 충분합니다. 모델은 300k 단계를 완료하기에 충분한 단일 TPU를 사용하여 3일 동안 훈련된다.\n' +
      '\n' +
      '### 동역학 + 잠재행동모델 훈련\n' +
      '\n' +
      '일단 비디오 토큰화기를 훈련시킨 후, 우리는 잠재 행동과 역학 모델을 공동으로 훈련시킬 수 있다. 다시 한 번 16G 메모리 내부에 모델 트레이닝을 맞추려고 하므로 총 576개의 이미지에 대해 각각 16프레임으로 구성된 36개 시퀀스의 배치 크기를 사용한다. 우리는 위에서 설명한 설정을 사용하여 잠재 행동 모델과 역학 모델을 병렬로 훈련한다(잠재 행동 모델은 섹션 C.1, 역학 모델은 섹션 C.3 참조).\n' +
      '\n' +
      '표 9의 최적화 하이퍼파라미터를 이용하여 200k 단계의 잠재행동과 동역학 모델을 병렬적으로 학습하였으며, 이 모델은 원래의 환경과 유사한 일관된 재생 가능한 잠재행동을 생성함을 확인하였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c} \\hline \\hline Component & Parameter & Value \\\\ \\hline Encoder & num\\_layers & 8 \\\\  & d\\_model & 512 \\\\  & num\\_heads & 8 \\\\ \\hline Decoder & num\\_layers & 8 \\\\  & d\\_model & 512 \\\\  & num\\_heads & 8 \\\\ \\hline Codebook & num\\_codes & 1024 \\\\  & patch\\_size & 4 \\\\  & latent\\_dim & 32 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 15: 코인런 비디오 토큰화기 하이퍼파라미터\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Component & Parameter & Value \\\\ \\hline Encoder & num\\_layers & 8 \\\\  & d\\_model & 512 \\\\  & num\\_heads & 8 \\\\ \\hline Decoder & num\\_layers & 8 \\\\  & d\\_model & 512 \\\\  & num\\_heads & 8 \\\\ \\hline Codebook & num\\_codes & 6 \\\\  & latent\\_dim & 32 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 16: 코인런 액션 모델 하이퍼파라미터\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Component & Parameter & Value \\\\ \\hline Architecture & num\\_layers & 12 \\\\  & d\\_model & 512 \\\\  & num\\_layers & 8 \\\\ Sampling & temperature & 1.0 \\\\  & maskgit\\_steps & 25 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 17: 코인런 다이나믹스 모델 하이퍼파라미터\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>