<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 챗뮤지션: 음악의 이해와 생성\n' +
      '\n' +
      'LLM과 내부적으로\n' +
      '\n' +
      '멀티모달 아트 프로젝션 연구 커뮤니티\n' +
      '\n' +
      '스카이워크 AI PTE. LTD\n' +
      '\n' +
      '홍콩과학기술대학교\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대언어 모델(LLM)은 텍스트 생성에서 인상적인 능력을 보여주지만, 우리는 그들의 능력이 아직 인류의 창조 언어인 음악으로 일반화되지 않았다는 것을 발견했다. 본질적인 음악적 능력을 통합한 오픈소스 LLM인 **ChatMusician1**를 소개합니다. 텍스트 호환 음악 표현, ABC 표기법에 대한 지속적인 사전 훈련 및 미세 조정 LLaMA2를 기반으로 하며 음악은 제2 언어로 취급된다. 챗뮤지션은 외부의 멀티모달 뉴럴 구조나 토키나이저 없이 순수 텍스트 토키나이저로 음악을 이해하고 생성할 수 있다. 흥미롭게도 음악적 능력을 부여하는 것은 언어 능력에 해를 끼치지 않으며, 심지어 조금 더 높은 MMLU 점수를 획득하기도 한다. 우리의 모델은 GPT-4 기준선을 넘어 텍스트, 화음, 멜로디, 모티브, 음악 형태 등을 조건으로 잘 구성된 전체 길이의 음악을 구성할 수 있다. 세심하게 선별된 대학 수준의 음악 이해 벤치마크인 **Music TheoryBench**에서 챗뮤지션은 제로샷 설정에서 LLaMA2와 GPT-3.5를 눈에 띄게 능가합니다. 우리의 연구는 LLM이 음악의 훌륭한 압축기가 될 수 있지만 정복해야 할 상당한 영역이 남아 있음을 보여준다. 4B 토큰 음악 언어 말뭉치 **MusicPile**, 수집된 Music TheoryBench, 코드, 모델 및 데모를 GitHub에서 공개합니다.\n' +
      '\n' +
      '각주 1: 전체 작성자 목록은 기여도 및 확인 섹션을 참조하십시오.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '인공 지능과 예술, 특히 음악의 융합은 인간 창의성의 본질에 대한 심오한 함의를 위해 연구의 중추적인 영역으로 부상했다(Civit et al., 2022). 음악은 그 고유한 구조와 복잡성으로 인해 독특한 위치를 차지하고 있으며, Masataka (2009, 2007); Pino et al. (2023)은 언어와 음악이 동일한 소스로부터 진화했을 수 있음을 시사한다.\n' +
      '\n' +
      '대규모 언어 모델(LLM)은 최근 긴 시퀀스를 생성하는 놀라운 능력으로 다양한 도메인에 혁명을 일으켰다. 연구자들은 음악 생성을 위한 언어 모델링 기술을 탐색해 왔다(Vaswani et al., 2017; Huang et al., 2018; Payne, 2019; Lu et al., 2023; Dhariwal et al., 2020; Agostinelli et al., 2023; Copet et al., 2023; Margulis and Simchy-Gross, 2016; Dai et al., 2022; Jhamtani and Berg-Kirkpatrick, 2019). 상징음악은 자연어와 유사한 방식으로 취급될 수 있을 것으로 보이지만, 이러한 관행들은 음악의 영역에서 많은 별개의 도전들이 직면하고 있음을 보여주었다. 예를 들어, GPT-4와 같은 최신 모델조차도 음악 추론 2에서 무작위보다 약간 더 나은 성능을 보인다. 우리는 음악 구성의 복잡성이 장기적이고 대위법적 문맥 의존성과 음악 음과 텍스트 설명 사이의 복잡한 연결을 포함하여 현재 LLM에서 부적절하게 표현되지 않기 때문이라고 주장한다.\n' +
      '\n' +
      '각주 2: 음악 조각에서 명시적으로 주석이 달리지 않고 음악 주제, 진행 및 스타일에 중요한 다양한 화성, 키, 리듬 및 기타 음악 요소를 추정하는 능력을 **음악 추론**이라고 한다.\n' +
      '\n' +
      '이러한 문제에 대한 해결책을 찾기 위해, 우리는 그림 1과 같은 파이프라인과 함께 내재적 음악 능력을 통합하는 오픈 소스 LLM인 챗뮤지션(ChatMusician)을 제안하며, 우리의 노력은 상징적인 음악 생성과 이해를 위해 LLM을 활용하는 데 중점을 두었다.\n' +
      '\n' +
      '**우리의 기여:**a) 우리는 텍스트 기반 LLM인 _ChatMusician_를 소개하며, 여러 상징 음악 이해 및 생성 작업을 통합하여 레퍼토리를 풍부하게 하면서 기초적인 일반 능력을 유지하거나 잠재적으로 향상시킬 수 있다. b) 경험적 평가는 GPT-4를 능가하고 다양한 음악 생성 작업에서 기준선을 설정하여 다양한 스타일에 걸쳐 일관성 있고 구조화된 음악 조각을 생성하는 능력을 보여주는 우리 모델의 우수한 음악 구성 능력을 보여준다. c) 우리는 음악 이해와 추론의 요소를 포함하는 Inauural 대학 수준의 상징 음악 이해 벤치마크인 _MusicTheoryBench_를 소개한다. 이 벤치마크에서 LMs의 성능은 그들의 한계를 드러내며, 코드와 수학적 추론과 유사한 주의를 요구하는 영역으로 음악의 미지의 영역을 제안한다. d) 벤치마크, 코드 및 4B-토큰 음악 언어 말뭉치 _MusicPile_를 포함하는 완전한 프레임워크를 오픈 소스하여 이 분야의 협업을 육성한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '음악의 발생과 이해에 관한### 이슈\n' +
      '\n' +
      '*음악 생성**의 연구는 음향[1, 16, 17, 18]과 상징적 모달리티[19, 14, 15, 16, 17, 18, 19, 20, 21]로 구분된다. 그러나, 이러한 모델들에 의해 생성된 작품들은 여전히 짧은 맥락(오디오 형태의 30대처럼)으로 제한되고, 완전히 음악적이고 잘 구조화된 것과는 거리가 멀다. Margulis and Simchy-Gross(2016)는 "반복"이 무작위 시퀀스라 하더라도 발췌한 "음악성"을 청취자들이 어떻게 평가하는지에 상당히 긍정적인 영향을 미친다고 주장한다. 초기 규칙 기반 방법은 유연성이 결여된 일부 미리 정의된 패턴으로 반복을 실현하는 반면, Dai et al.(2022)은 딥 러닝 기반 작업은 생성된 음악에서 반복과 음악 구조가 부족할 수 있음을 보여준다.\n' +
      '\n' +
      '*음악 이해**의 풍경은 전통적으로 음악 정보 검색 교환(MIREX)3 데이터 도전 및 MARBLE 벤치마크[23]와 같은 중요한 노력으로 예시된 오디오 중심의 작업에 중점을 두었다. 예를 들어, 그들은 장르 분류, 화음 추정, 멜로디 추출 등과 같은 다양한 오디오 기반 작업을 해결했다. 대조적으로, 우리의 기여는 음악 언어 이해, 고급 음악 이론 이해 및 상징 음악 추론의 도전을 포괄함으로써 기존의 오디오 중심 초점에서 벗어나 음악 테오리벤치의 도입으로 두드러진다.\n' +
      '\n' +
      '각주 3: [https://www.music-ir.org/mirex/wiki/MIREX_HOME](https://www.music-ir.org/mirex/wiki/MIREX_HOME)\n' +
      '\n' +
      '### Music Representations\n' +
      '\n' +
      '그림 2는 다양한 압축률을 가진 주류 음악 표현을 보여준다. 기호음악은 MIDI, humdrum, ABC 표기법(부록 A 상세)과 같은 형식을 포함한다. MIDI는 음악 산업에서 인기가 있기 때문에 쉽게 접근할 수 있는 데이터로 가장 좋아하는 연구[14, 15, 16]였다. 그러나, 집중적인 훈련 요구를 갖는 변압기 모델에 대해 제기된 MIDI의 긴 시퀀스 문제를 해결하기 위해, 시퀀스는 일반적으로 구성의 완전한 연속성을 포착하는 것을 제한하는 더 짧은 단편으로 분할된다. 또한, MIDI의 성능 뉘앙스의 인코딩은 토큰화될 때 양자화 오류와 불안정한 리듬으로 이어질 수 있다.\n' +
      '\n' +
      '따라서 우리는 주목할 만한 이점을 위해 점수 지향적이고 평범한 텍스트 표현인 ABC 표기법을 사용한다. 높은 압축률은 MIDI에 비해 더 짧은 시퀀스 길이를 가져오며, 본질적으로 (예를 들어, 반복된 심볼의 사용에 의해) 음악적 반복 및 구조를 인코딩하여 언어 모델을 사용하여 처리 효율을 향상시킨다. 또한 연주 기술을 나타내는 상세한 음악 기호를 포함하고 양자화 문제를 방지하여 음악 생성에서 리드미컬한 정밀도를 보장합니다.\n' +
      '\n' +
      '그림 1: 챗뮤지션은 웹소싱 음악 지식 및 수공예 음악 악보 생성 지시로부터 학습하고, 음악 생성과 음악 이해를 통일하며, 대학 수준의 음악 이론 질문을 채팅, 작곡, 답변할 수 있다.\n' +
      '\n' +
      ' 에이션. ABC 표기법의 언어 모델과의 호환성은 LLM 애플리케이션으로의 통합을 용이하게 하여 고급 음악 분석 및 생성을 가능하게 한다.\n' +
      '\n' +
      '비언어 영역에서의 복잡한 문제 해결 과제를 위한#### LLMs\n' +
      '\n' +
      '음악을 잘 이해하고 생성하기 위해서는 모티브, 화성, 리듬, 질감 등에 관한 복잡한 순차적 모델링을 다루어야 하며, 잘 조직된 구조와 발산적인 창의성을 절충할 필요가 있다. 근본적인 언어 시퀀스 모델링을 기반으로 최근 LLMs의 발전은 수학, 코드, 게임과 같은 비언어 영역에 걸쳐 복잡한 의사 결정 및 문제 해결 작업에서 일반화 능력을 보여주었지만 아직 음악을 고려하지 않았다. MAmmoTH Yue et al. (2023)은 구조화된 논리적 작업을 처리하고 해결하기 위해 생각 연쇄 및 생각 프로그램 근거의 하이브리드 접근법을 활용하며 수학적 추론과 언어 이해를 연결한다. 프로그래밍 작업을 위한 LLMs의 집합인 CodeLLaMA Roziere et al. (2023)은 일관성 있고 기능적인 코드 시퀀스를 생성하기 위해 텍스트 명령어를 적용하는 LLMs의 능력을 예시한다. Othello-GPT Li 등(2022)은 비선형 프로브 표현, 레이어별 개입 및 잠재 현저성 맵을 사용하여 GPT의 변형을 적용하여 Othello 게임에서의 법적 움직임을 예측한다. ChessGPT Feng et al.(2023)은 과거 체스 게임 데이터와 자연어에 대한 분석적 통찰력을 통합하여 정책 학습과 언어 모델링의 융합을 보여준다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '말코퍼스 큐레이션\n' +
      '\n' +
      '우리가 아는 한, 현재 공개적으로 이용 가능한 음악 관련 자연어 말뭉치는 없다. 다행히도, 우리가 자체 큐레이션할 수 있는 많은 대규모 코퍼라가 있습니다. 모델이 상호 작용하고 대화를 통해 지침을 받을 수 있도록 다양한 도메인의 데이터를 사용합니다. 본 절에서는 LLM에 음악적 능력을 주입하기 위한 최초의 사전 훈련 데이터세트인 MusicPile 데이터셋을 소개한다.\n' +
      '\n' +
      '일반 corpora.Pile Gao et al. (2020), Falcon RefinedWeb Penedo et al. (2023) 및 Wikipedia Wikipedia 기여자 (2023)를 포함하는 대표적인 공개 데이터 세트가 사용된다. 음악 관련 말뭉치를 선별하기 위해 음악 용어 4를 기반으로 파일 필터링 기준으로 음악 관련 단어 집합을 나열하며, 10회 이상 출현하고 도메인 동의의 0.5% 이상을 차지하는 음악 용어 단어만을 포함한다.\n' +
      '\n' +
      '각주 4: [https://en.m.wikipedia.org/wiki/Glossary_of_music_termininology](https://en.m.wikipedia.org/wiki/Glossary_of_music_termininology)\n' +
      '\n' +
      'Instruction and chat data.The instruction datasets Conover et al. (2023); Peng et al. (2023); Wang et al. (2023b)는 LLM을 잠재적인 다운스트림 사용에 적응시키기에 충분히 다양하고 대표적이다. 여러 라운드의 대화를 가능하게 하기 위해, 채팅 코퍼스 Wang 등(2023a)이 포함된다.\n' +
      '\n' +
      '음악 지식 및 음악 요약.노래 제목, 설명, 앨범, 아티스트, 가사, 재생 목록 등의 메타데이터를 포함하여 유튜브의 200만 개의 음악 트랙에 해당하는 메타데이터를 크롤링한다. 500k가 추출됩니다. 우리는 GPT-4를 사용하여 이러한 메타데이터의 요약을 생성한다. 우리는 Self-instructWang 등(2022)에 따라 음악 지식 QA 쌍을 생성한다. 부록 B에서 우리의 주제 개요에 따라 , 255k개의 명령어가 생성되고, GPT-4로 대응하는 답변이 생성된다.\n' +
      '\n' +
      '수학 및 코드 데이터.계산 음악 커뮤니티는 기호 음악 데이터 세트가 부족하며, 수학 Cobbe et al. (2021); Kenney (2023); Yue et al. (2023); Li et al. (2023) 및 코드 Li et al. (2023); Wang et al. (2023a)를 포함한다고 가정한다.\n' +
      '\n' +
      '그림 2: Wav, Codec, MIDI(피아노 롤로 시각화) 및 ABC 표기법을 포함하여 일반적으로 사용되는 음악 표현. 좌에서 우로 갈수록 압축률이 높아집니다.\n' +
      '\n' +
      '상징 음악의 추론력을 향상시킬 수 있다. 경험적으로, 우리는 이것이 음악 LLM의 성능을 향상시키는 데 도움이 된다는 것을 발견했다.\n' +
      '\n' +
      '일반 말뭉치를 제외한 나머지 모든 데이터셋은 하나 이상의 라운드에 대한 대화 양식으로 구성하였다. 음악 언어, 코드, 악보, 수학, 일반의 비율은 각각 10.42%, 2.43%, 18.43%, 4.05%, 64.68%이다. 표 1은 모든 데이터의 개요를 보여준다.\n' +
      '\n' +
      '뮤직 스코어 코퍼레이션\n' +
      '\n' +
      '상징적인 음악 데이터 세트는 계산 음악 커뮤니티에서 드물지만 우리는 세계의 다양한 지역의 음악을 포함하기 위해 노력했다. 지역 정보를 포함하는 악보의 일부분의 분포는 세계 지도에 라벨링되었다. 그림 3과 같이 우리 악보는 다양성이 특징이다. 수집된 코퍼스에 대해 악보 생성을 위한 6개, 음악 이해를 위한 2개 등 총 8개의 대표적인 음악 과제를 설계하였다. 생성 작업은 화음, 멜로디, 모티프5, 음악 형태6, 스타일을 조건으로 한 악보를 생성하는 것을 포함한다. 이해 작업은 사용자 입력 점수로부터 모티프 및 형태를 추출하는 것을 포함한다. 각 작업에 대해 각각 하나의 예를 들어 표 2에 나열된 여러 지침을 만들었다. 음악 명령어와 알고리즘을 큐레이팅하는 과정은 부록 D에 자세히 설명되어 있다.\n' +
      '\n' +
      '각주 5: 음악에서 모티브는 짧은 음악적 사상, 두드러진 반복적 형상, 작곡에서 어느 정도 특별한 중요성을 갖거나 그 특징이 있는 음표의 악편 또는 연속이다\n' +
      '\n' +
      '각주 6: 음악에서 형식은 음악적 구성이나 연주의 구조를 말한다.\n' +
      '\n' +
      '### MusicTheoryBench\n' +
      '\n' +
      '음악 정보 검색의 상당한 발전에도 불구하고, 진보된 음악 이해 역량에 대한 정의는 현재 연구에서 불분명하다. 본 연구에서는 음악에서 기존 LLM의 고급 이해 능력을 측정하기 위해 먼저 음악 지식과 음악 추론이라는 두 가지 음악 이해의 중요한 요소를 정의한다. 그런 다음 현재 LLM의 고급 음악 이해 능력을 평가하기 위해 설계된 벤치마크인 Music TheoryBench를 소개한다.\n' +
      '\n' +
      '**Definition of Music Knowledge and Reasoning.**_Reasoning_은 추론하기 위한 과정을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l} \\hline \\hline\n' +
      '**Datasets** & **Sourced from** & **Tokens** & **\\# Samples** & **Category** & **Format** \\\\ \\hline File (Gao et al., 2020) & public dataset & 0.83B & 18K & general & article \\\\ Falcon-RefinedWeb (Penedo et al., 2023) & public dataset & 0.80B & 101K & general & article \\\\ Wikipedia (Wikipedia contributors, 2023) & public dataset & 0.39B & 588K & general & article \\\\ OpenChat (Wang et al., 2023a) & public dataset & 62.44M & 43K & general & chat \\\\ LinkSoul (Jansi-Suul, 2023) & public dataset & 0.6B & 1.5M & general & chat \\\\ GPTA-Apacne (Gupta et al., 2023) & public dataset & 9.77M & 49K & general & chat \\\\ Dolly (Conover et al., 2023) & public dataset & 3.12M & 14K & general & chat \\\\ Irishman (Wu and Sun, 2023) & public dataset + Human-written Instructions & 0.23B & 868K & music score & chat \\\\ KernSocents (CCALH at Stanford University, 2023) & public dataset + Human-written Instructions & 2.76M & 10K & music score & chat \\\\ Bach (Wu et al., 2023) & public dataset + Human-written Instructions & 0.44M & 349 & music score & chat \\\\ synthetic music chat\\(\\star\\) & public dataset + Human-written Instructions & 0.54B & 50K & music score & chat \\\\ music knowledge\\(\\star\\) & Generated w/ GPT-4 & 0.22B & 255K & music verbal & chat \\\\ music summary\\(\\star\\) & Generated w/ GPT-4 & 0.21B & 500K & music verbal & chat \\\\ GSM8k (Cobe et al., 2021) & public dataset & 1.68M & 7K & math & chat \\\\ math (Kenney, 2023) & public dataset & 7.03M & 37K & math & chat \\\\ MathInstruct (Yue et al., 2023) & public dataset & 55.50M & 188K & math & chat \\\\ Camel-Math (Li et al., 2023) & public dataset & 27.76M & 50K & math & chat \\\\ arxiv:math-instruct-50k (Kenney, 2023) & public dataset & 9.06M & 50K & math & chat \\\\ Camel-Code (Li et al., 2023) & public dataset & 0.13B & 366K & code & chat \\\\ OpenCoder (Wang et al., 2023a) & public dataset & 36.99M & 28K & code & chat \\\\ \\hline Total & & & 4.16B & 5.17M & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: MusicPile의 개요. \\ (\\star\\)는 악보 데이터와 일반 데이터로부터의 합성을 의미한다. \\\\ (\\star\\)는 GPT-4를 촉발함으로써 우리가 선별한 NEW 논리를 의미한다.\n' +
      '\n' +
      '그림 3: MusicPile에 전 세계의 다양한 음악 악보를 포함시켰다. 지역 정보가 포함된 악보의 일부분의 분포는 세계 지도에 파란색 점으로 표시되어 있다.\n' +
      '\n' +
      '기존의 지식 및 관찰에 기초한, 일반적으로 수학.Yu 등(2023); Luo 등(2023). 음악은 작곡가들이 형식, 화성, 음계, 리듬, 조성, 구조적 조직의 원리를 세심하게 계산하는 수학에 비유되는 경우가 많다. 이 세심한 계산은 시간 및 주파수 영역에 걸친 음표의 분포가 확립된 규범을 충족하여 협화음과 즐거운 청각 경험을 산출한다는 것을 보장한다. 구성 과정은 대칭, 전치, 반복, 반전 및 역행 등 복잡한 규칙을 자주 사용한다.\n' +
      '\n' +
      '우리는 악곡에서 명시적으로 주석을 달지는 않았지만 주제, 진행 및 스타일을 이해하는 데 중요한 다양한 화음, 키, 리듬 및 기타 음악 요소를 추론하는 능력으로 _Music Reasoning_을 정의합니다. _Music Reasoning_ 반면에 음악 지식_은 음악 이론, 역사, 악기 특성, 문화적 맥락의 개념 등 음악적 상식에 대한 축적된 이해로 정의되며, 이는 음악 구성, 연주, 감상에 관련된 분석적이고 창의적인 과정을 알려준다. 예는 그림 4에서 찾을 수 있다.\n' +
      '\n' +
      '큐레이션 프로세스.우리는 전문 대학 음악 교사를 고용하여 대학 수준의 교과서와 시험지에 따라 음악 테오리벤치마크를 제작하여 인간 시험 기준과의 일관성을 확보하였다. 그 내용은 음악가 팀의 여러 차례 토론과 리뷰를 거쳤다. 팀은 신중하게 질문을 선택하고 JSON과 ABC 문자열 형식으로 수동으로 컴파일했다. 그런 다음 질문은 음악 지식과 음악 추론 하위 집합으로 레이블이 지정된다. 교사가 중국인이기 때문에 질문의 절반은 중국어로 전달되고, 이후 GPT-4 Azure API로 영어로 번역해 팀에서 교정한다.\n' +
      '\n' +
      '결과 벤치마크는 객관식 질문으로 포맷된 372개의 질문으로 구성되며, 각각 4개의 옵션이 있으며 그 중 하나만 정확하다. 음악지식 269개 문항, 음악추론 98개 문항이 있으며, 소수평가가 가능하도록 5개 문항이 출제되었다.\n' +
      '\n' +
      '지식 부분 집합.음악 지식 부분 집합에서 질문은 동서양 음악의 요소를 포함한다. 음표, 리듬, 박자, 화음, 대위법, 오케스트레이션 및 악기화, 음악 관련 문화, 역사 등 30개 주제를 포함한다(부록 B 참조). 각 전공 영역은 전문가의 지도하에 대상 심사를 받으며, 다양한 하위 범주로 구분된다. 예를 들어, 트라이어드 섹션에서 테스트 세트는 정의, 유형 및 관련 기술을 구체적으로 검사한다.\n' +
      '\n' +
      '그림 4: Music TheoryBench로부터의 (a) 음악 지식 및 (b) 음악 추론의 간단한 예. 질문 a.는 주로 암기를 통해 대답할 수 있는 개념을 포함한다. 질문 b는 _descending_, _natural minor scale_ 및 _leading tone_에 대한 지식과 악보에 기초한 추론을 요구한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline\n' +
      '**Task Name** & **Type** & **Example Instruction** \\\\ \\hline Cated Conditioned Music Generation & G & Describe a musical piece using the given chord compression. [GRODS] \\\\ Musical From Conditioned Music Generation & G & G\\(\\Delta\\) musical work not incorporates the given musical pattern as a central element. [MISCAL FORMS] \\\\ Alphphistic Musical Front and Motif Conditioned Music Generation & G & Describe a musical piece employing the provided motif and an alphabet-based structure. [MISCAL FORMS AI] [MOTIF] \\\\ Terminology Musical Form and Motif Conditioned Music Generation & G & Create two layers incorporating the provided motif in the specified composition structure. [MISCAL FORMS AI] [MOTIF] \\\\ Memory Harmonization & G & Facilitate model combinations to increase the human complexity of the specified musical concept. [MILODY] \\\\ Backly Style Music Generation & G & Provide a musical piece that does insufficient from Back’s propositions. \\\\ Moffent Extraction & U & Analyze the musical and physical composition consistent relations element in every section. [M�MSC] \\\\ Musical Front Extraction & U & Investigate the attributes of this musical emotion and identity to management using suitable music-related forms. [M�MSC] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: MusicPile 내 핸드크래프트된 음악 작업들, 6세대 작업들(Type:G) 및 2이해 작업들(Type:U)을 포함하고, 각 작업에 대한 예시적인 프롬프트를 제공한다. 예에서, 우리는 자연 언어 지시 이외의 정보를 나타내기 위해 대괄호 안의 토큰을 사용한다([MUSICALF ORMA]r epresentsm usicalf ormi na lphabetsa nd[MUSICALFO RMT] reresentsmu sicalfo rmin te rminology.[M OTIF],[M USIC] and [ME LODY]are representinA BC not ation.[CH ORD]isr epresentinc ho rdsym bols.). 디테일 소프트 리 광고.Thi stes of the turesdif ferentlev elsofd if difficult,cor responsetot he high hsch ooland col legelev elsofm us icmaj orstu dents.\n' +
      '\n' +
      '추론 부분 집합.추론 부분 집합의 대부분의 질문은 음악 지식과 추론 능력을 모두 필요로 한다. 이러한 질문에 정확하게 답하려면 주어진 정보에 대한 상세한 분석과 다단계의 논리적 추론, 화음, 멜로디, 음계, 리듬 등을 계산해야 한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Training Settings\n' +
      '\n' +
      'LLaMA2-7B-Base 가중치(Touvron et al., 2023, 2023)로부터 fp16-precision ChatMusician-Base를 초기화하고, 지속적인 사전 훈련과 미세 조정 파이프라인을 적용했다. 데이터 설정은 나중에 소개될 것이다. LoRA 어댑터(Hu et al., 2021)는 어텐션 및 MLP 레이어에 통합되었으며 임베딩 및 모든 선형 레이어에 대한 추가 교육이 이루어졌다. 최대 시퀀스 길이는 2048이었고, 1개의 에폭 사전 훈련과 2개의 에폭 미세 조정을 위해 16개의 80GB-A800 GPU를 사용했다. 메모리 효율성은 DeepSpeed(Rasley et al., 2020)를 사용하였으며, AdamW 최적화기는 1e-4 학습률과 5% 워밍업 코사인 스케줄러를 사용하였다. 경사 클리핑은 1.0으로 설정되었으며 LoRA 매개변수 치수, 알파 및 드롭아웃은 배치 크기가 8인 64, 16 및 0.1로 설정되었다.\n' +
      '\n' +
      '### Data Settings\n' +
      '\n' +
      '사전 훈련 동안 섹션 3의 모든 훈련 데이터를 결합하고 하나의 획기적인 훈련을 수행했다. 사전 훈련된 모델에 대한 서로 다른 데이터의 영향을 탐구하기 위해 감독 미세 조정에서 서로 다른 데이터 비율을 조사하고 악보와 음악 지식 및 음악 요약 데이터 간의 2:1 비율을 경험적으로 결정했다. 이 비율은 좋은 MMLU 성능을 보장하면서 음악 이해뿐만 아니라 음악 생성에서도 우수한 성능을 보였다. 2:1 비율에 따라 먼저 훈련 세트에서 78K 샘플을 샘플링하고 10개의 에폭에 대해 훈련했다. 그런 다음 비율을 유지하고 1.1M 샘플을 포함하는 모든 사용 가능한 악보 데이터를 활용했으며 2개의 에폭에 대해 훈련했다. 데이터 혼합물 설정은 부록 E에 요약되어 있습니다.\n' +
      '\n' +
      '### 평가 및 베이스라인 시스템\n' +
      '\n' +
      '기준 시스템.현재 기호 음악에서 기능을 가진 LLM은 거의 없습니다. 그러나 (Bubeck et al., 2023)의 관찰은 ChatGPT 시리즈가 음악적 능력을 가지고 있음을 시사한다. 따라서 GPT-3.5, GPT-4 및 LLaMA-2를 포함한 몇 가지 인기 있는 LLM 시스템을 기준선으로 선택했다.\n' +
      '\n' +
      '일반 언어 능력의 평가.일반 언어 능력을 평가하기 위해 언어 모델의 사전 훈련 동안 획득된 지식을 평가하기 위해 설계된 선구적인 벤치마크인 MMLU(Massive Multitask Language Understanding) 데이터 세트(Hendrycks et al., 2020)를 채택한다. 공정한 비교를 위해 선택한 기준선과 동일한 5샷 설정에서 모델을 평가한다.\n' +
      '\n' +
      '음악 이해 능력 평가.3절에서 소개한 바와 같이 음악 테오리벤치는 LLM의 음악 지식에 대한 이해와 추론 능력을 검사하는 것을 목표로 본 논문에서 제안한 음악 벤치마크이다. Music TheoryBench의 경우, 0-shot 설정 하에서 최종 결과로서 옵션을 5회 셔플링한 후의 평균 정확도를 보고한다.\n' +
      '\n' +
      '음악 생성 능력에 대한 평가.음악성에 대한 우리의 평가는 주로 인간의 판단에 달려 있다. 또한, 생성된 음악의 구조 및 형식 정확도를 평가하기 위해 구문 수준 반복 메트릭과 구문 분석 성공률 메트릭의 두 가지 특정 메트릭을 개발했다. 또한, 모델의 제어 가능성을 측정하기 위해 평균 백분위수 점수 메트릭을 도입한다.\n' +
      '\n' +
      '##5 결과 및 논의\n' +
      '\n' +
      '### Music Understanding\n' +
      '\n' +
      '제안된 Music TheoryBench를 이용하여 본 논문에서 제안한 모델과 베이스라인 시스템의 음악 이해 능력을 평가한다. 그림 5와 같이 GPT3.5, GPT4, LLaMA2-7B-Base, ChatMusician-Base, ChatMusician on Music TheoryBench의 제로샷 성능을 보고한다. 파란색 막대는 음악 지식 메트릭에 대한 성능을 나타내고 빨간색 막대는 음악 추론 메트릭을 나타낸다. 무작위 기준선은 점선으로 표시된 25%의 점수에 해당한다.\n' +
      '\n' +
      '음악 지식.그림 5에 따르면, 모든 시스템은 음악 지식 메트릭에서 무작위 기준선을 상당히 능가했다. GPT-4는 이 메트릭에서 58.2로 가장 높은 점수를 달성했다. 다음으로 채팅뮤지션-베이스와 채팅뮤지션의 점수가 40.2와 39.5로 GPT-3.5의 점수 31.2와 LLaMA2-7B-Base의 점수 33.3을 상회하여 지속적인 훈련을 통해 모델의 음악 지식 능력이 LLaMA2-7B-Base에 비해 약 7% 포인트 향상되었음을 보여준다. 동시에 정렬세(Zhao et al., 2023)를 관찰했는데, 여기서 미세 조정된 챗뮤지션은 베이스 모델보다 이 메트릭에서 약 0.7점 더 낮은 점수를 받았다.\n' +
      '\n' +
      '음악 추론.지식 메트릭의 성능과 달리 그림 5와 같이 모든 시스템은 음악 추론 메트릭에서 하위 결과를 나타낸다. 대부분의 시스템은 제로 샷 설정에서 기준선을 크게 초과하지 않는다. 놀랍게도, 가장 진보된 시스템인 GPT-4조차도 이 메트릭에서 25.6점에 불과했다. 흥미롭게도 ChatMusician-Base는 음악 추론 메트릭에서 27.1점을 획득하여 GPT-4를 능가했으며, 정렬 세금에도 불구하고 ChatMusician는 여전히 26.3점을 획득하여 제로 샷 음악 추론 메트릭에서 GPT-4를 능가했다.\n' +
      '\n' +
      'GPT-4 Go?Music TheoryBench는 음악 지식과 추론 능력을 정량적으로 평가하기 위한 첫 번째 이니셔티브를 대표한다. 이러한 목적을 달성하기 위해 우리는 벤치마크 내에서 GPT-4의 한계를 탐색하여 기능을 확인하기 위해 노력했다. GPT-4는 강력한 상황 내 학습(ICL)과 CoT(Cain-of-thought) 기술로 유명하다. 따라서 GPT-4 기준선에서 신속한 엔지니어링 기술을 사용하여 5개 샷, CoT 및 음악가 역할극 프롬프트를 포함한 다양한 조건에 걸쳐 Music TheoryBench에서 성능을 평가하기로 결정했다.\n' +
      '\n' +
      '표 3은 서로 다른 프롬프트 엔지니어링 전략 하에서 GPT-4의 성능 점수를 표시한다. 역할 플레이와 5-샷 ICL 기술의 조합을 사용하여 음악 추론에서 39.5의 최고 점수를 달성했다. 한편, CoT와 5-shot ICL 기법의 통합은 음악 지식에서 상위 69.9점으로 나타났다. 이러한 결과는 바닐라 제로샷 접근법의 성능을 크게 능가하지만 여전히 제안된 벤치마크를 완전히 포화시키는 데 미치지 못한다.\n' +
      '\n' +
      '### Music Generation\n' +
      '\n' +
      '이 절에서는 우리가 선택한 ABC 표기 체계가 음악적 구조와 반복을 문자열 형식으로 인코딩하고 압축하는 효율적인 수단임을 증명한다. 그런 다음 우리의 방법론이 음악성을 크게 향상시킨다는 것을 보여주기 위해 질적 및 양적 증거를 모두 제공한다. 또한, 최대 6개의 조건부 음악 생성 작업을 유해한 영향 없이 LLM에 원활하게 통합한다.\n' +
      '\n' +
      'ABC 노테이션의 압축률 5.2.1\n' +
      '\n' +
      '우리는 다양한 음악 표현의 압축률을 평가하기 위해 훈련 코퍼스에서 1,000개의 노래 세트를 샘플링했다. ABC 표기법이 MIDI로 변환되거나 WAV로 렌더링될 수 있으므로 ABC 문자열, MIDI 유사, REMI 및 오디오 코덱과 같은 널리 채택된 음악 표현을 사용하여 이러한 노래를 표현한다. 우리는 ABC 문자열로 표현된 시퀀스 길이가 다른 표현보다 훨씬 적은 가장 짧다는 것을 보여준다.\n' +
      '\n' +
      '<표 4>에서 보는 바와 같이 ABC 표기법은 곡당 평균 토큰 288.21개, 초당 평균 토큰 5.16개에 이른다. 이것은 MIDI 기반 표현의 약 38%이다. 이것은\n' +
      '\n' +
      '그림 5: Music TheoryBench에서의 제로샷 정확도. GPT-3.5, GPT-4, LLaMA2-7B-Base, ChatMusician-Base, ChatMusician를 포함하였다. 파란색 막대는 음악 지식 메트릭 상의 성능을 나타내고 빨간색 막대는 음악 추론 메트릭을 나타낸다. 점선은 25%의 점수로 무작위 기준선에 해당한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & Mus. Knowledge & Mus. Reasoning \\\\ \\hline GPT4-0-shot & 58.2 & 25.6 \\\\ +5-shot ICL & 64.1 & 38.0 \\\\ GPT4-RolePlay & 68.3 & 36.6 \\\\ +5-shot ICL & 68.8 & **39.5** \\\\ GPT4-CoT & 68.4 & 36.7 \\\\ +5-shot ICL & **69.9** & 34.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 우리는 Music TheoryBench의 상한을 확인하기 위해 GPT-4에 대한 신속한 엔지니어링을 추가로 수행했다. 우리는 사고 연쇄, 역할극, 5발 맥락 내 학습의 기술을 포함했다. 음악 지식 척도에서 가장 높은 점수는 69.9점, 음악 추론 척도에서 39.5점이다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      '분석할 수 있습니다. GPT 시리즈의 구문 분석 성공률을 향상시키기 위해 "_Please respond in ABC notation."이라는 지시어로 프롬프트의 접두사를 붙였다. 표 6은 세 가지 시스템에 걸친 비교 성공률을 보여준다. 특히, 챗뮤지션과 GPT-4 모두 90%를 초과하는 성공률을 보인 반면, GPT-3.5는 65.4%의 현저하게 낮은 비율을 달성했다.\n' +
      '\n' +
      'Task-wise Metrics.5 generation 태스크 각각에서 100개의 프롬프트를 샘플링하고, 5개의 음악 생성 태스크에 대한 메트릭으로 평균 백분위수 점수를 계산하면 높을수록 좋다. 그림 8은 우리가 테스트한 각 모델의 각 작업에 대한 세부 점수를 나타낸다. 자세한 내용은 부록 F를 참조하십시오. 우리는 챗뮤지션이 다섯 가지 과제 모두에서 GPT-3.5와 GPT-4를 모두 능가하는 것을 알 수 있다. 과제 "알파벳 음악 형태 및 모티프 음악 생성"에서 GPT-4의 낮은 점수는 이 과제의 GPT-4에 의해 생성된 대부분의 샘플이 기형 ABC 표기법을 포함하기 때문이다.\n' +
      '\n' +
      '### Language Ability\n' +
      '\n' +
      '표 7의 LLaMA2-7B-Base와 비교하여 ChatMusician의 MMLU 점수를 보고한다. 우리의 연구 결과는 ChatMusician와 ChatMusician-Base가 LLaMA2-7B-Base 모델보다 MMLU에서 더 높은 점수를 달성함을 나타낸다. 이거.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline System & Success Rate(\\%) \\\\ \\hline ChatMusician & **99.6** \\\\ GPT-4 & 94.6 \\\\ GPT-3.5 & 65.4 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 챗뮤지션, GPT-4, GPT-3.5에 의해 생성된 ABC 표기 문자열의 파싱 성공률을 평가하였다.\n' +
      '\n' +
      '도 8: 여기서 챗뮤지션, GPT-3.5 및 GPT-4의 총 8개의 음악 과제 중 5개에 대한 평균 백분위수 점수를 제공한다. 과제 명칭은 표 2의 과제들의 약어이다(A 형태 + 모티프는 "알파벳 음악 형태 및 모티프 조건 음악 생성"의 약어이고 T 형태 + 모티프는 "용어 음악 형태 및 모티프 조건 음악 생성"의 약어이다).\n' +
      '\n' +
      '그림 7: ABC 표기법 및 생성된 음악의 대응하는 스태프 표기법. 반복 기호는 두 표기에 모두 파란색으로 표시되며 명확한 구 수준의 반복을 보여준다. 빨간색과 노란색 직사각형은 두 섹션 모두에서 명확한 모티프 수준 반복을 표시한다. 녹색 직사각형은 첫 번째 섹션의 모티브에 따라 변동 노트를 표시합니다.\n' +
      '\n' +
      '내재적인 음악 이해와 생성 능력을 불어넣는 우리의 방법을 통합하는 것이 모델의 일반적인 언어 능력을 손상시키지 않는다는 것을 암시한다. 반대로 어느 정도 높이는 것으로 보인다.\n' +
      '\n' +
      '챗뮤지션의 ##기억효과\n' +
      '\n' +
      '[14]에 이어 챗뮤지션의 암기 능력을 분석한다. 우리는 훈련 세트에서 무작위로 500개의 샘플을 선택하고 지시 프롬프트로 모델을 공급한다. 생성된 ABC 표기법과 지상 진리를 비교한다. 생성된 예제와 그라운드 트루스 토큰이 전체 시퀀스에 대해 동일한 예제의 비율은 0.02%이다. 또한, 부분 매칭은 트레이닝 예제의 0.24%에서 발생하며, 여기서 생성된 및 그라운드 트루스 시퀀스는 그들의 토큰의 적어도 80%를 공유한다.\n' +
      '\n' +
      '## 6 Conclusions\n' +
      '\n' +
      '결론적으로, 본 연구는 고급 음악 추론과 구성이 가능한 혁신적인 오픈 소스 LLM인 챗뮤지션(ChatMusician)을 소개한다. 텍스트와 호환되는 음악 표현을 활용하고 음악과 언어 벤치마크 모두에서 주목할 만한 성과를 달성함으로써 챗뮤지션은 언어 모델 내에서 음악적 창의성을 통합하는 데 중요한 진전을 보여준다. 우리의 연구 결과는 음악과 인공 지능의 융합에서 미개발된 가능성을 강조하면서 음악 이해와 창의성을 위한 강력한 도구로서 LLM의 잠재력을 강조한다. MusicPiles, Music TheoryBench 및 ChatMusician의 공개는 이 흥미로운 영역에서 추가 연구를 위한 귀중한 자원을 제공한다.\n' +
      '\n' +
      '## Limitation\n' +
      '\n' +
      '챗뮤지션의 현재 반복은 주로 아일랜드 음악 스타일로 음악을 생성하는데, 이는 데이터 세트의 상당 부분이 이 장르에서 조달되었기 때문이다. 모델은 환각을 나타내며 수공예 음악 지침의 다양성 부족으로 인해 개방형 음악 생성 작업을 지원하는 데 한계에 직면해 있다.\n' +
      '\n' +
      '## Ethics Statement\n' +
      '\n' +
      '이 모델은 음악 교육에 활용된다면 잠재적으로 학습자를 오도할 수 있는 환상을 보여준다. 또한 이 모델은 암기 효과를 보여주어, 의도치 않게 개인 훈련 데이터를 역류시킬 경우 음악 저작권의 잠재적 침해에 대한 우려를 불러일으킨다. 우리는 암기 효과의 사례를 식별하기 위해 음악 표절 탐지 알고리즘을 개발할 계획이다. 또한, 우리는 환상의 발생을 완화하기 위해 추가 정렬 전략을 구현하는 것을 목표로 한다.\n' +
      '\n' +
      '##7 기부금 및 승인\n' +
      '\n' +
      '**Core**\n' +
      '\n' +
      'Ruibin Yuan, _ryuanab@connect.ust.hk_\n' +
      '\n' +
      '한풍린 _linhanfeng@bjtu.edu.cn_\n' +
      '\n' +
      '이왕 _ezzmonyi@gmail.com_\n' +
      '\n' +
      'Zeyue Tian, _ztianad@connect.ust.hk_\n' +
      '\n' +
      'Shangda Wu, _shangda@mail.ccom.edu.cn_\n' +
      '\n' +
      '**Contributors**\n' +
      '\n' +
      'Tianhao Shen\n' +
      '\n' +
      'Ge Zhang\n' +
      '\n' +
      'Yuhang Wu\n' +
      '\n' +
      'Cong Liu\n' +
      '\n' +
      'Ziya Zhou\n' +
      '\n' +
      'Ziyang Ma\n' +
      '\n' +
      'Liumeng Xue\n' +
      '\n' +
      'Ziyu Wang\n' +
      '\n' +
      'Qin Liu\n' +
      '\n' +
      'Tianyu Zheng\n' +
      '\n' +
      'Yizhi Li\n' +
      '\n' +
      'Yinghao Ma\n' +
      '\n' +
      'Yiming Liang\n' +
      '\n' +
      'Xiaowei Chi\n' +
      '\n' +
      'Ruibo Liu\n' +
      '\n' +
      'Zili Wang\n' +
      '\n' +
      'Pengfei Li\n' +
      '\n' +
      'Jingcheng Wu\n' +
      '\n' +
      '**Advisors**\n' +
      '\n' +
      'Chenghua Lin\n' +
      '\n' +
      'Qifeng Liu\n' +
      '\n' +
      'Tao Jiang\n' +
      '\n' +
      'Wenhao Huang\n' +
      '\n' +
      'Wenhu Chen\n' +
      '\n' +
      'Emmanouil Benetos\n' +
      '\n' +
      'Jie Fu\n' +
      '\n' +
      'Gus Xia\n' +
      '\n' +
      'Roger Dannenberg\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline System & MMLU Score(\\%) \\\\ \\hline ChatMusician-Base & **48.50** \\\\ ChatMusician & 46.80 \\\\ LLaMA2-7B-Base & 46.79 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: ChatMusicians와 LLaMA2-7B-Base의 MMLU 점수.\n' +
      '\n' +
      '## Correspondence\n' +
      '\n' +
      'Wei Xue, _weixue@ust.hk_\n' +
      '\n' +
      '강시인 _시인.강@쿤룬-inc.com_\n' +
      '\n' +
      'Yike Guo, _yikeguo@ust.hk_\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Andrea Agostinelli, Timo I Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. 2023. Musiclm: Generating music from text. _arXiv preprint arXiv:2301.11325_.\n' +
      '* [2] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_.\n' +
      '* [3] CCARH at Stanford University. 2023. A library of virtual musical scores in the humdrum **kern data format.\n' +
      '* [4] Miguel Civit, Javier Civit-Masot, Francisco Cuadrado, and Maria J Escalona. 2022. A systematic review of artificial intelligence-based music generation: Scope, applications, and future trends. _Expert Systems with Applications_, page 118190.\n' +
      '* [5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_.\n' +
      '* [6] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free dolly: Introducing the world\'s first truly open instruction-tuned llm.\n' +
      '* [7] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. 2023. Simple and controllable music generation. _arXiv preprint arXiv:2306.05284_.\n' +
      '* [8] Shuqi Dai, Huiran Yu, and Roger B Dannenberg. 2022. What is missing in deep music generation? a study of repetition and structure in popular music. _arXiv preprint arXiv:2209.00182_.\n' +
      '* [9] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. 2020. Jukebox: A generative model for music. _arXiv preprint arXiv:2005.00341_.\n' +
      '* [10] Chris Donahue, Antoine Caillon, Adam Roberts, Ethan Manilow, Philippe Esling, Andrea Agostinelli, Mauro Verzetti, Ian Simon, Olivier Pietquin, Neil Zeghidour, et al. 2023. Singsong: Generating musical accompaniments from singing. _arXiv preprint arXiv:2301.12662_.\n' +
      '* [11] Alexandre Defossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. 2022. High fidelity neural audio compression.\n' +
      '* [12] Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, and Jun Wang. 2023. Chessgpt: Bridging policy learning and language modeling. _arXiv preprint arXiv:2306.09200_.\n' +
      '* [13] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thute, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. _arXiv preprint arXiv:2101.00027_.\n' +
      '* [14] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. In _International Conference on Learning Representations_.\n' +
      '* [15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_.\n' +
      '* [16] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu, and Douglas Eck. 2018. Music transformer. _arXiv preprint arXiv:1809.04281_.\n' +
      '* [17] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. 2019. Music transformer: Generating music with long-term structure. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net.\n' +
      '* [18] Yu-Siang Huang and Yi-Hsuan Yang. 2020a. Pop music transformer: Beat-based modeling and generation of expressive pop piano compositions. In _MM \'20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020_, pages 1180-1188. ACM.\n' +
      '* [19] Yu-Siang Huang and Yi-Hsuan Yang. 2020b. Pop music transformer: Generating music with rhythm and harmony. _CoRR_, abs/2002.00212.\n' +
      '* [20] Harsh Jhamtani and Taylor Berg-Kirkpatrick. 2019. Modeling self-repetition in music generation using generative adversarial networks. In _Machine Learning for Music Discovery Workshop, ICML_.\n' +
      '* [21] Matthew Kenney. 2023. arxiv-math-instruct-50.\n' +
      '* [22] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbulin, and Bernard Ghanem. 2023. 낙타: 대규모 언어 모델 사회의 "마인드" 탐구를 위한 커뮤니케이션 에이전트.\n' +
      '* Li et al. (2022) Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. 2022. 신흥 세계 표현: 합성 태스크에 대해 훈련된 시퀀스 모델 탐색. _ arXiv preprint arXiv:2210.13382_.\n' +
      '* LinkSoul-AI(2023) LinkSoul-AI. 2023. LinkSoul/instruction_merge_set. [https://huggingface.co/datasets/LinkSoul/instruction_merge_set] (https://huggingface.co/datasets/LinkSoul/instruction_merge_set).\n' +
      '* Lu et al. (2023) Peiling Lu, Xin Xu, Chenfei Kang, Botao Yu, Chengyi Xing, Xu Tan, 및 Jiang Bian. 2023. Musecoco : 텍스트로부터 기호음악을 생성하는 단계; _ arXiv preprint arXiv:2306.00110_.\n' +
      '* Luo et al. (2023) Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empower mathematical reasoning for large language models via reinforced evol-instruct. _ arXiv preprint arXiv:2308.09583_.\n' +
      '* Margulis and Simchy-Gross (2016) Elizabeth Hellmuth Margulis and Rhimmon Simchy-Gross. 2016. Repetition은 랜덤하게 생성된 톤 시퀀스의 음악성을 향상시킨다. _ 음악 인식: An Interdisciplinary Journal_, 33(4):509-514.\n' +
      '* 마사타카(2007) 노부오 마사타카. 2007. Music, Evolution and language. _ Developmental science_, 10(1):35-39.\n' +
      '* 마사타카(2009) 노부오 마사타카. 2009. The origin of language and evolution of music: A comparative perspective. _ Physics of Life Reviews_, 6(1):11-22.\n' +
      '* Oore et al. (2018) Sageev Oore, Ian Simon, Sander Dieleman, Douglas Eck, and Karen Simonyan. 2018. 이 시간 with feeling: Learning expressive musical performance. _ CoRR_, abs/1808.03715.\n' +
      '* Payne(2019) Christine Payne. 2019년, 뮤지넷 OpenAI 블로그.\n' +
      '*페인(2022) 크리스틴 페인. 2022. Musenet. [https://openai.com/research/musenet] (https://openai.com/research/musenet).\n' +
      '* Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. 매를 위한 정제된 웹 데이터세트 llm: 웹 데이터와 함께 큐레이션된 말뭉치를 능가하고 웹 데이터만을 제공한다. _ arXiv preprint arXiv:2306.01116_.\n' +
      '* Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. 명령어 튜닝 with gpt-4. _arXiv preprint arXiv:2304.03277_.\n' +
      '* Pino et al. (2023) Maria Chiara Pino, Marco Giancola, and Simonetta D\'Amico. 2023. 어린이에서의 음악과 언어의 연관성: 최신 리뷰. _ Children_, 10(5):801.\n' +
      '* Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: 시스템 최적화는 1,000억 개 이상의 파라미터를 갖는 딥 러닝 모델을 트레이닝할 수 있게 한다. _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 3505-3506.\n' +
      '* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. 2023. Code llama: code에 대한 오픈 파운데이션 모델 _ arXiv preprint arXiv:2308.12950_.\n' +
      '* Sturm et al. (2015) Bob Sturm, Joao Felipe Santos, and Iryna Korshunova. 2015. Folk music style modeling by recurrent neural networks with long short term memory units. 음악 정보 검색 회의를 위한 제16회 국제 사회에서.\n' +
      '* Thickstun et al. (2023) John Thickstun, David Hall, Chris Donahue, and Percy Liang. 2023. 예상 음악 트랜스포머 _ arXiv preprint arXiv:2306.08620_.\n' +
      '* Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. 개방적이고 효율적인 기초 언어 모델 arXiv preprint arXiv:2302.13971_.\n' +
      '* Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. 2023b. 라마 2: 오픈 파운데이션 및 미세 조정 채팅 모델들_ arXiv preprint arXiv:2307.09288_.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. 주의력만 있으면 됩니다 _ 신경 정보 처리 시스템_, 30의 발전.\n' +
      '* Wang et al. (2023a) Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023a. 오픈챗: 양질의 데이터가 혼합된 오픈소스 언어 모델의 발전 arXiv preprint arXiv:2309.11235_.\n' +
      '* Wang et al. (2023b) Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavid Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. 2023b. 캠스크는 어디까지 갈 수 있죠? 개방형 리소스에 대한 명령어 튜닝 상태를 탐색합니다. _ arXiv preprint arXiv:2306.04751_.\n' +
      '* Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannen Hajishirzi. 2022. 자가 명령어: 언어 모델을 자가 생성 명령어와 정렬한다. _ arXiv preprint arXiv:2212.10560_.\n' +
      '* 기여자(2023) 위키피디아 기여자. 2023. 위키피디아 데이터베이스.\n' +
      '* Wu et al. (2023) Shangda Wu, Xiaobing Li, and Maosong Sun. 2023. 화음-조건 멜로디 조화와 제어 가능한 조화. _ICASSP 2023-2023 IEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5. IEEE.\n' +
      '* 우와 선(2022) 샹다 우와 마송 선. 2022. 텍스트-음악 생성 태스크에서 사전 훈련된 체크포인트의 효능 탐색. _ arXiv preprint arXiv:2211.11216_.\n' +
      '* 우와 선(2023) 샹다 우와 마송 선. 2023. 튜닝포머 : 제어코드로 튜닝을 형성하는 단계; _ arXiv preprint arXiv:2301.02884_.\n' +
      '* Yu et al. (2023) Fei Yu, Hongbo Zhang, and Benyou Wang. 2023. 자연언어 추론, 설문조사. _ arXiv preprint arXiv:2303.14725_.\n' +
      '* Yuan et al. (2023) Ruibin Yuan, Yinghao Ma, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Le Zhuo, Yiqi Liu, Jiawen Huang, Zeyue Tian, et al. 2023. Marble: Music audio representation benchmark for universal evaluation. _ arXiv preprint arXiv:2306.10548_.\n' +
      '* Yue et al. (2023) Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. 매머드: 하이브리드 명령어 튜닝을 통한 수학 일반 모델 구축 arXiv preprint arXiv:2309.05653_.\n' +
      '* Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. 대형 언어 모델에 대한 설문조사. _ arXiv preprint arXiv:2303.18223_.\n' +
      '* Zhuo et al. (2023) Le Zhuo, Zhaokai Wang, Baisen Wang, Yue Liao, Chenxi Bao, Stanley Peng, Songhao Han, Aixi Zhang, Fei Fang, 및 Si Liu. 2023. 비디오 배경 음악 생성: Dataset, 방법 및 평가. IEEE/CVF International Conference on Computer Vision_의 _Proceedings, pages 15637-15647.\n' +
      '\n' +
      'ABC Notation의 소개\n' +
      '\n' +
      'ABC 표기는 음악 표기를 위한 텍스트 기반 시스템이며, 특히 민속 및 전통 곡조를 표기하는 데 인기가 있다. 특별한 음악 글꼴이나 소프트웨어 없이 사람이 쉽게 읽을 수 있고 타이핑이 간편하도록 설계했다. 딥러닝 모델과 인터페이싱할 때 독특한 이점을 제공합니다.\n' +
      '\n' +
      '* **Data Efficiency**: ABC 표기법은 음악적 정보를 텍스트 형식으로 간결하게 표현하여 저장 및 전송 측면에서 매우 효율적이다. 이러한 압축성은 데이터 오버헤드를 최소화하므로 딥러닝 모델을 훈련시킬 때 유리하다.\n' +
      '**쉬운 전처리**: 구조화된 텍스트 형식으로서, ABC 표기는 쉽게 토큰화되고 숫자 시퀀스 또는 임베딩으로 변환될 수 있으며, 이는 신경망에 대한 데이터를 준비하는 데 중요한 단계이다.\n' +
      '**확장성**: ABC 표기의 단순성으로 인해 대규모 데이터 세트의 신속한 수집 및 주석이 가능하다. 딥러닝 모델, 특히 신경망은 방대한 데이터 세트로부터 엄청난 이익을 얻어서 더 나은 훈련과 일반화를 가능하게 한다.\n' +
      '**생성 모델**: ABC 표기법의 텍스트 기반 특성은 텍스트 기반 도메인에서 일관된 시퀀스를 생성하는 데 능숙함을 보여준 LLM과 같은 생성 모델에 탁월한 후보가 된다.\n' +
      '**해석가능성**: ABC 표기법을 사용하는 딥 러닝 모델들에 의해 생성된 출력들은 인간-판독가능하여, 즉각적인 피드백 및 반복적인 정제를 허용한다. 이는 생성된 출력을 이해하고 조정하는 것이 중요한 음악 생성과 같은 작업에서 특히 유용하다.\n' +
      '***기타 모달리티와의 통합**: ABC 표기법은 멀티모달 딥러닝 시스템에서 다른 데이터 모달리티와 쉽게 통합되어 음악 관련 작업에 대한 포괄적인 표현을 제공할 수 있다.\n' +
      '**커뮤니티 지원**: ABC 표기법에서 사용 가능한 곡과 작곡의 방대한 수는 쉽게 사용할 수 있는 풍부한 데이터 세트가 있음을 의미합니다. 딥러닝 모델은 이를 활용하여 다양한 음악 구조와 스타일을 학습할 수 있다.\n' +
      '\n' +
      'ABC 파일은 일련의 헤더와 음악 표기법으로 구성된다. 헤더는 제목, 작곡가, 리듬 등과 같은 곡조에 대한 메타데이터를 제공합니다. 음악 표기 섹션은 멜로디를 정의한다.\n' +
      '\n' +
      '헤더는 보통 한 글자 뒤에 콜론이 오는 것으로 시작한다. 일부 공통 헤더는,\n' +
      '\n' +
      '```\n' +
      'X:ReferenceNumber L:DefaultNoteLength Q:Tempo M:TimeSignature K:KeySignature\n' +
      '```\n' +
      '\n' +
      '상기 음악은 문자, 숫자 및 기호를 이용하여 표현된다:\n' +
      '\n' +
      '* 노트는 문자 a-g(중간 C 위의 옥타브에 대한 노트) 및 A-G(아래 옥타브에 대한 노트)로 표시된다.\n' +
      '* 노트 듀레이션은 숫자를 추가함으로써 주어진다. 예를 들어, A2는 기본 길이의 두 배의 노트를 나타냅니다.\n' +
      '* 샤프, 내추럴, 플랫은 ^, =, _로 표시된다. 예를 들어, ^F는 F sharp이다.\n' +
      '* 화음은 C 장화음의 [ceg]와 같은 대괄호를 사용하여 그룹화됩니다.\n' +
      '* 막대는 기호로 표시된다.\n' +
      '* 삼중항들과 같은 삼중항들은, 예를 들어, (a, b, 및 c의 삼중항에 대한 3abc) 특수 구문을 사용하여 표기된다.\n' +
      '* 다양한 장식과 장식에는 독특한 기호가 있다.\n' +
      '\n' +
      '여기 ABC 표기법의 기본 곡이 있습니다:\n' +
      '\n' +
      '```\n' +
      'X:1 L:1/8 M:3/4 K:D de|^D^f3 g a/gf|^A" e4 AB|^C" = c3 d^G/B" B/AG|^A" A4 fg|"D" a3 gfd|^A" e4 AB|^C" = c3 d e/fg|^D" f4: d/edCA|^Bm^B2 A2 F2|^F#m^A3 B d/edCA|^A" (E4 E) |"G" B3 e^A" dc|^D" d4\n' +
      '```\n' +
      '\n' +
      '이것은 D 장조로 설정된 왈츠를 나타낸다. 기본 노트 길이는 8번째 노트이며, 시간 서명은 월츠의 일반적인 3/4입니다. 이중 콜론(::)은 이 곡조가 두 부분을 가지고 있으며, 각 부분을 반복해야 함을 나타내며, 이는 무용수들에게 춤 시퀀스를 완성할 충분한 시간을 제공하기 위한 전통 무용 음악의 일반적인 관행이다.\n' +
      '\n' +
      '음악테오리벤치의 심사범위\n' +
      '\n' +
      '1. 피치, 노트값, 노테이션 시스템*소리, 음악톤 특성*음악톤성계, 톤행, 스케일 정도*노트의 그룹화*노트값의 스태프, 클레프 및 스테이브*노트값의 분할*반음소 및 전체톤*기질*고조파 시리즈*고조파 시리즈*음악톤성계, 톤행, 스케일 정도*노트값의 그룹화*스태프, 클레프 및 스테이브*노트값의 분할*반음소 및 전체톤*기질*고조파 시리즈*고조파 시리즈*고조파 시리즈*고조파 시리즈*고조파 시리즈*고조파 시리즈*고조파 시리즈*고조파 시리즈*고조파 시리즈*고조파 시리즈*고조파 시리즈*고조파 시리즈*고조파 시리즈*고조파 시리즈*고조파 시리즈\n' +
      '2. 리듬, 비트 및 노트값 조합 * 리듬 및 비트 * 짝수 리듬 분할 및 불규칙 또는 특수 리듬 분할 * 타임 시그너처 및 타입 * 동기화 * 노트값 조합\n' +
      '3. 간격 * 간격의 정의 및 분류 * 정도 및 간격 * Diatonic Intervals 및 Chromatic Intervals * Single Intervals 및 Compound Intervals * 간격의 반전 * 자음 Intervals * Enharmonic Intervals\n' +
      '4. 삼합체 * 삼합체의 정의 * 삼합체의 종류 * 삼합체의 반전\n' +
      '5. 제7화음 * 제7화음의 정의 및 유형 * 제7화음의 위치 및 반전 * 제7화음의 배열 * 고조파 화음 * 화음의 자음 * 제9화음, 제11화음 및 제13화음\n' +
      '6. 모달 스케일 * 키 이름, 키 서명 및 스케일 정도 * 메이저 스케일 * 마이너 스케일 * 중세 모드 * 에스닉 스케일\n' +
      '7. 키들의 관계 * 메이저와 마이너 키들의 관계. * Modal Interchange * Relative Major and Minor * Tone Equil Temperament * Relative Keys\n' +
      '8. 웨스턴 모드 및 톤성 * 모드 및 키 시그니처 * 자연 주 및 작은 크기 * 고조파 주 및 작은 크기 * 멜로딕 주 및 작은 크기 * 모달 분석에서 톤 색도\n' +
      '9. Ethnic Modal Scale and Tonality*Pentatonic Scale*Hexatonic Scale*Heptatonic Scale\n' +
      '10. 전위 * 서양 전위 * 민족 전위\n' +
      '11. 화음진행의 조성분석\n' +
      '12. 모드의 간격 및 현재. * 모드의 간격 * 간격의 해상도 * 모드의 삼화음 * 모드의 일곱 번째 화음 * 우세한 일곱 번째 화음의 해상도\n' +
      '13. 노테이션에서의 전위*간격 전위\n' +
      '14. 색도 척도*주요 색도 척도*미소 색도 척도*동적 표시 및 용어*템포 표시 및 용어\n' +
      '* 반전 및 음성\n' +
      '* 증강된 6화음\n' +
      '* 나폴리 및 차용 화음\n' +
      '\n' +
      '15. 거푸집 및 구조물\n' +
      '* 구, 마침표 및 문장\n' +
      '* 이성분, 삼성분 및 론도 형태\n' +
      '*소나타 알레그로 형태\n' +
      '* 주제 및 변형\n' +
      '* 푸기 및 기타 대위판 형태\n' +
      '\n' +
      '16. Counterpoint\n' +
      '\n' +
      '* 종 반점\n' +
      '* 음성 선도 규칙\n' +
      '* 모방 카운터포인트(Canon, Fugue)\n' +
      '* 모방적 대응점\n' +
      '\n' +
      '17. Melody\n' +
      '\n' +
      '* 멜로딕 시공 및 개발\n' +
      '* 모티티브 개발\n' +
      '* Sequences\n' +
      '\n' +
      '18. 20세기 기술\n' +
      '\n' +
      '* atomomality and Serialism\n' +
      '* 12-톤 기법\n' +
      '* 집합 이론\n' +
      '* Minimalism\n' +
      '* Microtonality\n' +
      '\n' +
      '19. 뮤지컬 스타일과 장르\n' +
      '\n' +
      '* 중세시대부터 현대까지 역사적 개관\n' +
      '* 음악시대에 따른 특성(예: 바로크, 클래식, 로맨틱)\n' +
      '\n' +
      '20. 분석 기술\n' +
      '\n' +
      '* 로마숫자 분석\n' +
      '* Schenkerker 분석\n' +
      '* 그래픽 분석\n' +
      '* Neo Riemannian Theory\n' +
      '\n' +
      '21. 오케스트레이션 및 Instrumentation.\n' +
      '\n' +
      '* 관현악기의 특성\n' +
      '*기구에 따른 글쓰기의 기초\n' +
      '* 전체 오케스트라 스코어링\n' +
      '\n' +
      '22. 음향 및 음향의 과학\n' +
      '\n' +
      '* 오버톤 및 고조파\n' +
      '* Harmonic Series\n' +
      '* Timbre 및 특성\n' +
      '\n' +
      'MusicTheoryBench에서의 5-shot 평가에 사용된## 부록 C 예\n' +
      '\n' +
      '그림 9와 같이 5-shot 평가에 사용된 프롬프트가 있는 보류된 사례를 제시한다.\n' +
      '\n' +
      '## 부록 D 음악 명령어 데이터 집합 큐레이션\n' +
      '\n' +
      '우리는 음악 SFT 데이터의 기반으로 아일랜드인 데이터 세트를 사용했다. 원래 데이터 세트에는 제어 코드와 ABC 표기법의 두 가지 필드가 포함되어 있습니다. 제어 코드는 생성된 상징 음악의 전체적인 구조에 대한 생성 모델 명령이다. 여기서 우리는 더 나은 설명을 위한 제어 코드 샘플을 제공한다:\n' +
      '\n' +
      'S:2 B:5 E:5 B:6 S:2\n' +
      '\n' +
      'S:2는 이 음악 샘플에 2개의 섹션이 있음을 나타내며, 각 섹션은 ABC 표기법의 세그먼테이션 마크에 의해 명확하게 표시된다. B:5는 첫 번째 구간에 5개의 막대가 있음을 나타내고, B:6은 두 번째 구간에 6개의 막대가 있음을 나타낸다. 두 B 섹션 사이의 E:5는 이 샘플에서 두 음악 섹션 사이의 편집 거리 유사성을 나타낸다.\n' +
      '\n' +
      '(n^{\\text{th}}\\) B 구간의 경우, 그 이전까지의 E 구간 수는 \\(n-1\\) 존재하는데, \\(m^{\\text{th}}\\) E 구간은 \\(m^{\\text{th}}\\) B 구간과 \\(n^{\\text{th}}\\) B 구간의 편집거리 유사도를 나타낸다.\n' +
      '\n' +
      '1 뮤지컬 형태 분석 알고리즘\n' +
      '\n' +
      'B 섹션 이전의 각 E 섹션에 대해, 우리는 현재 B 섹션에 대한 유사성 레벨 목록을 구축할 수 있다. 이 목록 각각에서 다음 표준을 사용합니다.\n' +
      '\n' +
      '8 이상의 유사성은 \\(s\\)로 표기된 동일한 섹션으로 볼 수 있는 두 개의 섹션을 나타낸다. 6과 8 사이의 유사성은 \\(v\\)로 표기된 이전 섹션의 변형으로 볼 수 있는 섹션을 나타낸다. 6 아래의 유사성은 \\(d\\)로 표기된 두 개의 다른 섹션을 나타낸다. 제어 코드의 다음 예를 알고리즘 2에 주어라:\n' +
      '\n' +
      'S:4 B:1 E:1 B:8 E:3 E:7 B:1 E:1 E:4 E:1 B:8\n' +
      '\n' +
      '우리는 이 유사성 수준 목록 \\(a=[[d]\\), \\([d,v]\\), \\([d,d,d]]\\)을 각 질문에 주어진 네 가지 옵션(A, B, C, D)에서 다음 질문을 읽는다. 가장 좋은 옵션을 선택하십시오.\n' +
      '\n' +
      '다음 코드 진행 중 위의 예를 가장 잘 설명하는 것은?\n' +
      '\n' +
      'L:1/4\n' +
      '\n' +
      'M:4/4\n' +
      '\n' +
      'K:E\n' +
      '\n' +
      '[G,B,E] [A,CE] [F,B,D] [F,A,C] |] %1\n' +
      '\n' +
      'A. ii\n' +
      '\n' +
      '6\n' +
      '\n' +
      '/\n' +
      '\n' +
      '4 - V - vi\n' +
      '\n' +
      '6 - iii\n' +
      '\n' +
      'B. I\n' +
      '\n' +
      '6 - IV - V6\n' +
      '\n' +
      '4 - ii\n' +
      '\n' +
      'C. IV - V6\n' +
      '\n' +
      '/\n' +
      '\n' +
      '4 - I - ii\n' +
      '\n' +
      'D. iii\n' +
      '\n' +
      '6 - V - I\n' +
      '\n' +
      '6\n' +
      '\n' +
      '/\n' +
      '\n' +
      '4 - IV\n' +
      '\n' +
      'Answer: B\n' +
      '\n' +
      '다음 중 위의 예시에서 7화음을 가장 잘 설명하는 것은 무엇인가?\n' +
      '\n' +
      'L:1/4\n' +
      '\n' +
      'M:4/4\n' +
      '\n' +
      'K:D\n' +
      '\n' +
      '[F8Bd]4|] %1\n' +
      '\n' +
      'A. 제3반전에서의 메이저 7번\n' +
      '\n' +
      'B. 2차 역전에서 우세 7번째\n' +
      '\n' +
      'C. 제3반전에서의 메이저/마이너 7\n' +
      '\n' +
      'D. 2차 반전에서 마이너 7번째\n' +
      '\n' +
      'Answer: A\n' +
      '\n' +
      '다음 중 위의 예에서 노트의 이름은 무엇인가?\n' +
      '\n' +
      'L:1/4\n' +
      '\n' +
      'M:4/4\n' +
      '\n' +
      'K:Cb\n' +
      '\n' +
      'D,4|] %1\n' +
      '\n' +
      'A. B-flat\n' +
      '\n' +
      'B. D\n' +
      '\n' +
      'C. B\n' +
      '\n' +
      'D. D-flat\n' +
      '\n' +
      'Answer: D\n' +
      '\n' +
      '위의 예에서 화음은 다음 중 어느 것으로 가장 잘 설명될 수 있는가?\n' +
      '\n' +
      'L:1/4\n' +
      '\n' +
      'M:4/4\n' +
      '\n' +
      'K:F#\n' +
      '\n' +
      '[EG8]4|] %1\n' +
      '\n' +
      'A. Vio\n' +
      '\n' +
      'B. V\n' +
      '\n' +
      'C. ii\n' +
      '\n' +
      'D. iv\n' +
      '\n' +
      'Answer: A\n' +
      '\n' +
      '[여기서 실제 질문]\n' +
      '\n' +
      '그림 9: Music TheoryBench 벤치마크에서 사용된 5-shot 예제 및 프롬프트.\n' +
      '\n' +
      '그리고 알파벳의 음악적 형태를 표현하기 위해 문자열을 생성하고, 문자의 시작 부분에 문자 \\(A\\)를 넣고, 유사도 레벨 리스트에서 각 하위 리스트를 거친 후, 처음 출현한 \\(s\\)과 \\(v\\)의 색인을 표시한다. 만약 \\(s>v\\)이면, 우리는 \\(s\\)의 색인에 같은 알파벳을 추가할 것이다. 만약 \\(v>s\\)이면, 우리는 소수 부호가 추가된 \\(v\\)의 색인에 알파벳을 추가할 것이다.\n' +
      '\n' +
      '위의 예에서, 우리는 알파벳 음악 형태를 \\(ABB^{\\prime}C\\)으로 얻을 것이다.\n' +
      '\n' +
      '이 알파벳 음악적 형태를 이용하여 우리는 용어로 대변되는 음악적 형태를 생산할 수 있다. 일반적으로 사용되는 몇 가지 음악 형식 용어를 수집하여 음악 이론에서 전통적인 음악 형식인 _Only One Section_, _Binary_, _Ternary_, _Variational_, 확장 음악 형식인 _American Popular_, _Verse/Chorus_, _Verse/Chorus/Bridge_, _Verse/Chorus/Verse/Bridge_, _Through Composed_, 복합 음악 형식인 _Compound Binary_, _Compound Ternary_를 포함하여 세 가지 범주로 나누었다.\n' +
      '\n' +
      '###### d.0.2 Motif 추출 알고리즘\n' +
      '\n' +
      '모티브 추출 알고리즘은 컨트롤 코드에 제공된 구간 길이 정보로 샘플을 각 구간으로 분리한 후 알고리즘 1로 각 구간의 토큰 시퀀스 \\(s\\)를 처리한다.\n' +
      '\n' +
      '```\n' +
      '입력: \\(s^{(0)}\\cdots s^{(n)}\\) for\\(x=0,1,\\cdots,n\\)do if\\(s^{(x)}\\) if\\(s^{(x)}\\)은 막대, 화음, 주석, 장식 기호, 장식 문자, 장식 기호이고 \\(s\\) endif endif \\(m\\)는 새로운 토큰 시퀀스 길이이다. 빈 토큰 빈도 튜플 목록 \\(a\\)을 만듭니다. \\(y=0,1,\\cdots,m)do for\\(z=1,2,\\cdots,8)do if\\(s^{(y)},\\cdots,s^{(y+z)}\\notin a\\)then  Add \\((s^{(y)},\\cdots,s^{(y+z)},1)\\) to \\(a\\) else tuple \\(s^{(y)},\\cdots,s^{(y+z)},k)\\)for \\(a\\)에서 tuple \\(y)},\\cdots,s^{(y+z)},k)\\)endif endfor \\(a\\)에서 tuple \\(y)},\\cdots,s^{(y+z)},k)\\)endif reurn\\(b[0]\\) as the motif\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** ABC 표기 모티프 추출\n' +
      '\n' +
      '## 데이터 혼합물의 부록 E 설정\n' +
      '\n' +
      '제한된 컴퓨팅 능력을 고려하고 데이터 혼합물의 영향을 조사하기 위해 서로 다른 혼합물 비율로 데이터를 52k 또는 78k 크기로 다운샘플링했다. 이를 통해 약 하루 만에 실험이 완료될 수 있다. 모든 설정은 표 8에 나와 있습니다. 표 1에는 데이터 도메인의 범주화가 포함되어 있습니다. 음악 언어는 음악 지식과 음악 요약의 결합을 의미한다. 경험적으로 설정 18은 음악 이해, 음악 생성, 언어 이해 능력 사이에서 균형 잡힌 성능을 제공한다는 것을 발견했다. 그 후, 18에서 1.1M 샘플로 설정을 확장하고 설정 21로 표시했으며, 설정 21은 본 논문에서 보고된 채팅 뮤지션 시스템이다.\n' +
      '\n' +
      '## 평균 백분위 점수 메트릭의 부록 F 세부사항\n' +
      '\n' +
      '각 작업에 대해 먼저 초기 점수를 계산합니다. 화음 조건 음악 생성 태스크의 경우, 초기 점수는 생성된 음악의 화음과 프롬프트의 화음 사이의 편집 거리를 취함으로써 계산된다. 음악 형식 조건화된 음악 생성 과제에 대해, 초기 점수는 생성된 음악으로부터 계산된 음악 형식 집합과 프롬프트에서 음악 형식 집합 간의 차이를 취함으로써 계산된다. 알파벳/종단 음악 형태 및 모티프 조건 음악 생성 과제에 대해, 초기 점수는 생성된 음악으로부터 계산된 음악 형태 집합과 프롬프트 내의 음악 형태 집합 사이의 차이 및 프롬프트 내의 생성된 음악 및 모티프로부터 계산된 모티프의 가장 긴 공통 서브 시퀀스를 모두 취함으로써 계산된다. 멜로디 조화 작업은 생성된 음악에서 멜로디와 프롬프트에서 멜로디 사이의 편집 거리를 취하여 초기 점수를 계산한다.\n' +
      '\n' +
      '우리는 각 과제에 대해 다른 초기 점수 계산 방법을 가지고 있기 때문에 각 과제에 따라 초기 점수의 백분위수를 취하여 동일한 척도로 점수를 정규화한다. 백분위수 값은 표본의 초기 값이 이 작업의 모든 초기 값의 백분율보다 크다는 것을 나타냅니다. 예를 들어, 코드 조건 음악 생성 태스크에서 0.6의 백분위수 값은 샘플의 초기 점수가 코드 조건 음악 생성 태스크에서 모든 초기 점수의 60%보다 크다는 것을 의미한다. 마지막으로, 우리는 각 모델의 각 태스크에 대한 백분위수의 평균값을 취하고 그림 8에서 테스트된 모델에 대한 각 태스크의 평균 백분위수 점수를 생성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline\n' +
      '**ID** & **Setting** & **\\# Samples** & **Epochs** \\\\ \\hline\n' +
      '1 & general + math + code & 52k & 10\\\\\n' +
      '2 & 음악 구두 & 52k & 10\\\\\n' +
      '3 & 음악(언어+악보) & 52k & 10\\\\\n' +
      '4&일반+음악(언어+점수)+코드+수학&78k&10\\\\\n' +
      '5 & 음악(언어+점수) : general = 1:2 & 78k & 10\\\\\n' +
      '6 & 음악(언어+악보) : general = 2:1 & 78k & 10\\\\\n' +
      '7 & 일반 + 수학 + 음악(언어 + 악보) & 78k & 10\\\\\n' +
      '8 & 일반 + 코드 + 음악(언어 + 악보) & 78k & 10\\\\\n' +
      '9 & general(exclude linkoul)+ music(verbal+ score)+code+math&78k&10\\\\\\\n' +
      '10 & 음악 구두 : 일반 + 수학 + 코드 = 1:2 & 78k & 10\\\\\n' +
      '11 & 음악 구두 : 일반 + 수학 + 코드 = 2:1 & 78k & 10\\\\\n' +
      '12 & 음악 구두 : 일반 + 수학 + 코드(en) = 1:2 & 78k & 10\\\\\n' +
      '13 & 음악 구두 : 일반 + 수학 + 코드(en) = 2:1 & 78k & 10\\\\\n' +
      '14 & 음악 구두 : 아이리쉬만 = 5:1 & 52k & 10\\\\\n' +
      '15 & 음악 구두 : 아이리쉬만 = 1:1 & 52k & 10\\\\\n' +
      '16 & 음악 구두 : 합성 음악 채팅 = 5:1 & 52k & 10\\\\\n' +
      '17 & 음악 구두 : general(en) = 1:1 & 52k & 10\\\\\n' +
      '18 & 음악 구두 : 음악 스코어 = 2:1 & 78k & 10\\\\\n' +
      '19 & 음악 구두 + 수학 : 음악 점수 = 2:1 & 78k & 10\\\\\n' +
      '20 & 음악 구두 + 코드 : 음악 스코어 = 2:1 & 78k & 10\\\\\n' +
      '21 & 음악 구두 : 음악 스코어 = 2:1 & 1.1M & 2\\\\\n' +
      '22 & 음악 구두 : 바흐 = 2:1 & 78k & 10\\\\\n' +
      '23 & 악보 : 악보(반백) = 2:1 & 78k & 10\\\\\n' +
      '24 & music verbal : music score(bach repeat 10) = 2:1 & 78k & 10 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 감독된 파인튜닝 단계에서 데이터 혼합물의 설정.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>