<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '대형 언어 모델의 다운스트림 작업 수행을 위한 #스케일링 법칙\n' +
      '\n' +
      'Berivan Isik\n' +
      '\n' +
      '구글 리서치에서 인턴을 하면서 일을 했다. Contact: berivan.isik@stanford.edu\n' +
      '\n' +
      'Natalia Ponomareva\n' +
      '\n' +
      '구글 리서치에서 인턴을 하면서 일을 했다. Contact: berivan.isik@stanford.edu\n' +
      '\n' +
      'Hussein Hazimeh\n' +
      '\n' +
      '우리는 세부 조정 작업 또는 그 위에서 계산된 메트릭을 참조하기 위해 다운스트림이라는 용어를 사용하고 사전 훈련 데이터 세트에서 계산된 메트릭을 참조하기 위해 업스트림이라는 용어를 사용한다.\n' +
      '\n' +
      'Dimitris Paparas\n' +
      '\n' +
      'Sergei Vassilvitskii\n' +
      '\n' +
      'Sanni Koyejo\n' +
      '\n' +
      '구글 리서치에서 인턴을 하면서 일을 했다. Contact: berivan.isik@stanford.edu\n' +
      '\n' +
      'Stanford University\n' +
      '\n' +
      '구글 리서치에서 인턴을 하면서 일을 했다. Contact: berivan.isik@stanford.edu\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '스케일링 법률은 대규모 언어 모델(LLM)의 설계를 안내할 수 있는 중요한 통찰력을 제공한다. 기존 작업은 주로 사전 훈련(업스트림) 손실에 대한 스케일링 법칙을 연구하는 데 중점을 두었다. 그러나, LLM이 감독되지 않은 데이터 세트에서 미리 훈련된 다음 다운스트림 태스크에서 미세 조정되는 전이 학습 설정에서, 우리는 종종 다운스트림 성능에 대해서도 관심을 갖는다. 본 연구에서는 기계 번역 작업을 위해 LLM이 미세 조정되는 전이 학습 설정에서 스케일링 동작을 연구한다. 구체적으로, 우리는 _pretraining_ 데이터의 선택과 그 크기가 다운스트림 크로스 엔트로피와 BLEU 점수의 두 가지 메트릭에 의해 판단된 다운스트림 성능(번역 품질)에 어떻게 영향을 미치는지 조사한다. 우리의 실험은 미세 조정 데이터 세트의 크기와 사전 훈련과 다운스트림 데이터 사이의 분포 정렬이 스케일링 거동에 상당한 영향을 미친다는 것을 나타낸다. 충분한 정렬을 통해 다운스트림 교차 엔트로피 및 BLEU 점수는 더 많은 사전 훈련 데이터로 단조롭게 개선된다. 이러한 경우 로그 법칙을 사용하여 좋은 정확도로 다운스트림 BLEU 점수를 예측할 수 있음을 보여준다. 그러나, 적당한 오정렬이 BLEU 점수가 더 많은 사전 훈련으로 변동하거나 악화되는 반면, 다운스트림 교차 엔트로피는 단조롭게 개선되는 경우도 있다. 이러한 관찰을 분석하여 적절한 사전 훈련 데이터를 선택하기 위한 새로운 실용적인 통찰력을 제공한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '스케일링 법칙은 모델의 성능과 훈련 데이터의 크기 또는 모델의 아키텍처와 같은 주요 설계 요소 사이의 관계를 정량화한다. LLM의 맥락에서 이러한 법률은 모델 개발, 자원 할당 및 적절한 훈련 데이터 선택에 대한 귀중한 지침을 제공한다. 광범위한 연구는 업스트림 퍼플렉시티 또는 교차 엔트로피 손실에 대한 스케일링 법칙(즉, 사전 훈련 데이터에 대해 평가됨)에 초점을 맞추었으며, 이러한 양은 전력 법칙을 사용하여 잘 예측될 수 있음을 입증했다(Kaplan et al., 2020; Hoffmann et al., 2022; Gordon et al., 2021; Hernandez et al., 2022; Fernandes et al., 2023; Bansal et al., 2022; Henighan et al., 2020; Johnson et al., 2018). 그러나 실제 응용에서 LLM은 종종 전이 학습을 겪는데, 먼저 지도되지 않은 데이터에 대해 사전 훈련된 다음 코딩 또는 번역과 같은 특정 다운스트림1 작업에 대해 미세 조정된다. 스케일링 법칙이 다운스트림 태스크 성능을 예측하는 데 사용될 수 있는지에 대한 질문은 중요하지만, 대체로 답변되지 않은 채로 남아 있다(Hernandez et al., 2021; Tay et al., 2021). 여기서, 용어 _task performance_는 크로스 엔트로피와 같은 다음 토큰 예측 메트릭과 다른 정확도, BLEU 점수와 같은 태스크 관련 양을 측정하는 메트릭을 의미한다.\n' +
      '\n' +
      '각주 1: 우리는 세부 조정 작업 또는 그 위에서 계산된 메트릭을 참조하기 위해 다운스트림이라는 용어를 사용하고 사전 훈련 데이터 세트에서 계산된 메트릭을 참조하기 위해 업스트림이라는 용어를 사용한다.\n' +
      '\n' +
      '본 연구에서는 전이 학습을 위한 스케일링 법칙을 연구하고 기계 번역 작업에 초점을 맞춘다. 구체적으로, 태스크에 대한 미세 조정 후 사전 훈련 데이터세트 크기와 _다운스트림 태스크 성능_ 사이의 관계를 살펴본다. 우리는 미세 조정 데이터 크기와 성능 메트릭의 선택 외에도 이 관계는 근본적으로 사전 훈련 데이터와 다운스트림 작업 간의 정렬에 달려 있음을 발견했다. 이전 학습 문헌(Tamkin et al., 2020; Agostinelli et al., 2022)에서도 유사한 관찰이 다른 맥락에서 이루어졌지만, 우리의 연구는 LLM의 다운스트림 성능에 대한 새로운 통찰력과 구체적인 스케일링 법칙을 제공한다.\n' +
      '\n' +
      '우리는 다국어 비지도 데이터 세트에서 LLM을 사전 훈련한 다음 여러 기계 번역 작업에서 LLM을 미세 조정하는 체계적인 실험을 수행한다. 실험 전반에 걸쳐, 우리는 (다운스트림 작업과의 분포 정렬 정도를 제어하기 위해) 사전 훈련 데이터의 유형과 미세 조정 데이터 크기를 변경한다. 우리는 _downstream_BLEU (Papineni et al., 2002) score2와 _downstream_ cross-entropy의 두 가지 메트릭을 연구한다. 우리는 분포가 잘 정렬된 환경에서 BLEU와 _downstream_ 교차 엔트로피가 더 많은 사전 훈련으로 단조롭게 개선된다는 것을 발견했다. 이 설정에서 BLEU 점수는 다음과 같은 로그 법칙을 사용하여 잘 예측될 수 있음을 보여준다. \\(f(D_{p})=(\\log(A\\cdot D_{p}^{\\alpha}))^{\\beta}\\), 여기서 \\(D_{p}\\)는 사전 훈련 데이터의 크기를 나타내고 \\(A\\), \\(\\alpha\\), \\(\\beta\\)는 적합 계수를 나타낸다. 또한, 사전학습 데이터 스케일로서 _downstream_ cross-엔트로피에 대한 멱법칙 \\(L(D_{p})=E+\\frac{A}{D_{p}^{\\alpha}\\)을 제안한다. 사전학습 데이터 스케일로서 _upstream_ cross-엔트로피에 대해 개발된 유사한 법칙들을 반향한다(Kaplan et al., 2020; Hoffmann et al., 2022) 및 finetuning 데이터 세트 크기의 함수로서 _downstream_ cross-엔트로피에 대해 개발된 멱법칙 \\(L(D_{p})=E+\\frac{A}^{\\alpha}\\).\n' +
      '\n' +
      '각주 2: 논문의 나머지 부분에서, 우리는 다운스트림 BLEU 점수를 참조할 때 "다운스트림"을 드롭할 것이다.\n' +
      '\n' +
      '그러나 분포가 충분히 정렬되지 않고 미세 조정 데이터 크기가 상대적으로 작을 때 BLEU 점수가 불분명하고 단조롭지 않은 동작을 나타내는 경우가 있는 반면, _downstream_ 크로스 엔트로피는 여전히 멱법칙에 따라 단조롭게 개선된다. 이 관찰은 BLEU 점수와 같은 태스크 관련 메트릭에 대한 프록시로서 교차 엔트로피를 사용하는 것이 다운스트림 태스크에 대한 프리트레이닝 데이터의 "관련성" 또는 타겟 다운스트림 성능에 대한 프리트레이닝 데이터의 요구되는 크기에 대한 결정을 내리는 데 사용되는 경우 실제로 중요한 오판정을 초래할 수 있음을 시사한다.\n' +
      '\n' +
      '마지막으로, 우리의 경험적 연구는 피네튜닝(번역) 데이터 세트가 이미 충분히 클 때 사전 훈련이 BLEU 점수에 거의 또는 전혀 개선을 가져오지 않는다는 것을 시사하며, 이는 Hernandez et al.(2021)의 결과를 보완한다.\n' +
      '\n' +
      '우리의 기여와 주요 결과는 다음과 같이 요약할 수 있다.\n' +
      '\n' +
      '* 우리는 770-million 및 3-billion 인코더-디코더 T5(Raffel et al., 2020) 모델에 대한 체계적인 실험을 수행하여 _downstream_ cross-entropy 및 BLEU 점수로 측정된 다운스트림 성능이 사전 훈련 데이터 세트 크기로 어떻게 확장되는지 연구한다. 사전 훈련을 위해 영어(en), 독일어(de), 프랑스어(fr) 및 루마니아어(ro)를 포함하여 다국어 C4(MC4) 데이터 세트(Raffel et al., 2020)의 서로 다른 하위 집합을 실험한다. Finetuning을 위해 WMT-17 en-de(Bojar et al., 2017), WMT-15 en-fr(Bojar et al., 2014), WMT-16 en-ro(Bojar et al., 2016)의 번역 과제를 연구한다.\n' +
      '* 프리트레이닝과 다운스트림 태스크의 분포가 잘 정렬될 때 BLEU 점수와 _downstream_cross-entropy가 더 많은 프리트레이닝으로 단조롭게 개선되는 것을 관찰한다. BLEU 점수에 대해, 우리는 새로운 로그 스케일링 법칙을 제안하고 그것이 좋은 예측 정확도를 갖는다는 것을 보여준다.\n' +
      '* 분포가 충분히 정렬되지 않고 미세 조정 데이터 크기가 상대적으로 작을 때, BLEU 점수는 더 많은 사전 훈련-단조적 스케일링 거동을 잃으면서 변동하거나 심지어 악화된다. 이러한 동일한 설정에서, 우리는 _downstream_cross-entropy가 여전히 멱법칙에 따라 단조롭게 확장된다는 것을 발견한다.\n' +
      '* 우리는 BLEU score_와 같은 _downstream 태스크 관련 메트릭을 사용하여 사전 훈련 데이터의 가치를 평가해야 한다고 주장하며, 제안된 BLEU score에 대한 스케일링 법칙을 활용하여 이러한 평가를 위한 실용적인 가이드를 제안한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '변압기에 대한 스케일링 법률. LLM에 대한 스케일링 법률은 모델 크기 및 사전 훈련 데이터의 유형 및 크기와 같은 주요 설계 선택에 대한 결정을 알릴 수 있어 상당한 관심을 끌었다(Kaplan et al., 2020; Hoffmann et al., 2022; Hernandez et al., 2021). 대부분의 선구적인 작업은 더 많은 사전 훈련 데이터, 더 큰 모델 또는 더 긴 훈련(Kaplan et al., 2020; Hoffmann et al., 2022)을 갖는 _upstream_ 크로스-엔트로피 손실 또는 퍼플렉시티 스케일이 어떻게 이루어지는지에 초점을 맞추고 있다. 후속 연구에서는 번역 모델의 스케일링 거동을 분석하였고(Ghorbani et al., 2021; Zhuocheng et al., 2023; Gordon et al., 2021; Fernandes et al., 2023; Bansal et al., 2022; Zhang et al., 2022), 스케일링 법칙 뒤에 대한 이론적 기초를 연구하였으며(Sharma and Kaplan, 2020; Hutter, 2021; Bahri et al., 2021), 또는 비전 모델로 법을 확장하였다(Zhai et al., 2022; Jain et al., 2023). 우리의 연구에 가장 가까운 Hernandez et al. (2021)은 전이 학습을 분석했지만 교차 엔트로피 손실이 _finetuning_ 데이터 척도로서 어떻게 행동하는지 초점을 맞추었다. 우리의 작업과 달리, 그들의 스케일링 법칙은 (피네튜닝) 데이터세트의 크기와 _same_ 데이터세트의 교차 엔트로피 손실 사이의 관계를 설명하며, 이는 피네튜닝 손실과 피네튜닝 데이터세트가 동일한 분포의 샘플에 대해 계산되기 때문에 문헌의 표준 스케일링 법칙에 더 가깝다. 한편, 우리는 _preraining_ 데이터 스케일로서 _finetuning_ 데이터세트 상의 _downstream_ 메트릭에 대한 스케일링 법칙을 제안한다 - 포커스를 "out-of-distribution" 분석으로 전환한다. 우리가 알고 있는 유일한 작업은 사전 훈련 데이터세트 크기의 함수로 _downstream 태스크 성능에 대한 스케일링 법칙을 제안한 것으로 Sun et al.(2017)이 비젼 도메인에서 분류 작업에 집중하고 LLM에 비해 작은 모델을 사용한 것이다.\n' +
      '\n' +
      '프리트레이닝의 전송성 메트릭 및 값.프리트레이닝 데이터가 _upstream_ 및 _downstream_ 성능 모두를 향상시킨다는 것이 일반적으로 제안될 수 있지만, 이 규칙은 비전 도메인에서 도전을 받아왔다. Zoph et al. (2020); He et al. (2019); Shen et al. (2019); Ghiasi et al. (2018); Mikami et al. (2022)는 프리트레이닝이 때때로 _downstream_ task 성능에 영향을 미치지 않을 수 있고 때로는 심지어 성능을 해칠 수 있다는 것을 입증했다. 우리는 LLM에 대한 광범위한 실험을 통해 언어 도메인에서 유사한 관찰을 하고 (a) 사전 훈련 데이터가 태스크와 충분히 정렬되지 않을 때 더 많은 사전 훈련 데이터가 _다운스트림 태스크 성능을 손상시키고 (b) 사전 훈련이 피네튜닝 데이터세트가 충분히 클 때 _다운스트림 태스크 성능을 눈에 띄게 개선하지 않는 경우를 식별한다. 또 다른 관련 작업 라인은 전달 가능성 메트릭에 관한 것이다(Tamkin et al., 2020; Chiang and Lee, 2022; Ibrahim et al., 2022; Tran et al., 2019; Agostinelli et al., 2022; Tran et al., 2019; Nguyen et al., 2020; You et al., 2021; Dai et al., 2019; Huang et al., 2022; Ibrahim et al., 2022; Tran et al., 2019; Bao et al., 2019; Van Asch and Daelemans, 2010; Plank and Van Noord, 2011). 우리는 전이성 메트릭이 스케일링 법칙과는 다른 순위 문제를 해결하기 위해 설계되었다는 점에 주목한다. 예를 들어, 이러한 메트릭들은 주어진 소스 모델들(또는 사전 트레이닝 데이터세트들)의 풀과 같은 질문들에 응답하며, 어떤 소스 모델(또는 사전 트레이닝 데이터세트)이 주어진 타겟 태스크에 대해 미세조정하기에 가장 좋다. 이러한 메트릭은 주요 수량(예: 사전 훈련 데이터 크기)이 스케일링될 때 모델의 성능을 예측하도록 설계되지 않는다.\n' +
      '\n' +
      '##3 전이학습을 위한 척도법칙\n' +
      '\n' +
      '이 섹션에서는 BLEU 점수와 _downstream_ cross-entropy에 대해 제안된 스케일링 법칙을 제시한다. 또한 이러한 법칙이 적용될 때 논의하고 주어진 목표 다운스트림 작업에 대한 사전 훈련 데이터 세트의 가치를 평가하기 위한 실용적인 지침을 제공한다. 실험 결과에 대한 자세한 내용은 이후 5절에서 논의하기로 한다.\n' +
      '\n' +
      '### BLEU 점수를 위한 스케일링 법칙\n' +
      '\n' +
      '파워-로프 스케일링 거동 Kaplan 등(2020); Hoffmann 등(2022), 우리는 도 1, 도 2 및 도 3으로부터 명백한 바와 같이 BLEU 스코어가 로그-로프에 더 가깝게 스케일링된다는 것을 발견한다. 따라서, 우리는 사전 트레이닝 데이터세트 크기 \\(D_{p}\\)의 함수로서 BLEU 스코어에 대한 다음의 스케일링 법칙을 제안한다:\n' +
      '\n' +
      '\\[f(D_{p})=(\\log(A\\cdot D_{p}^{\\alpha}))^{\\beta}, \\tag{1}\\)\n' +
      '\n' +
      '여기서 \\(A\\), \\(\\alpha\\), \\(\\beta\\)는 적합할 계수이다. 우리는 이러한 계수가 사전 훈련 데이터세트를 목표 다운스트림 태스크(언어 1에서 언어 2로의 번역)와 얼마나 정렬하고 피네튜닝(번역) 데이터세트가 얼마나 큰지에 의존한다는 것을 알아차린다. 여러 번역 작업과 다국어 사전 훈련 모델에 걸친 광범위한 실험을 통해 우리는 (1)의 법칙이 부록 B.2에서 정량화하는 작은 예측 오류와 함께 BLEU 점수 스케일링을 실제로 잘 설명한다는 것을 보여준다.\n' +
      '\n' +
      '교차 엔트로피 손실은 항상 좋은 미터법인가요?\n' +
      '\n' +
      '또한 선행 연구에서 _upstream_ 또는 _downstream_ cross-entropy loss가 모델의 _downstream task performance_에 좋은 지표라는 가정을 하였기 때문에 _downstream_ cross-entropy loss와 BLEU score를 경험적으로 비교한다. 사전 훈련 데이터세트 크기 Kaplan 등(2020); Hoffmann 등(2022)의 함수로서 _upstream_ 크로스-엔트로피 손실의 잘 이해된 스케일링 거동에 이어서, 동일한 스케일링 법칙이 또한 _downstream_ 크로스-엔트로피 손실을 다음과 같이 설명할 수 있음을 입증한다.\n' +
      '\n' +
      '\\[L(D_{p})=E+\\frac{A}{D_{p}^{\\alpha}}, \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(E\\), \\(A\\) 및 \\(\\alpha\\)은 최적화될 계수이다. 논문 전반에 걸쳐 우리는 직접 비교를 위해 BLEU 점수와 교차 엔트로피를 함께 보고하고 두 메트릭이 잘 상관되지 않는 여러 사례를 발견한다. 이것은 BLEU 점수와 교차 엔트로피 사이의 불일치를 암시하는 Ghorbani et al.(2021)의 일부 발견들을 지지하지만, 또한 Gordon et al.(2021)이 옹호하는 지수 관계(두 메트릭들 사이의)가 항상 유지되는 것은 아니라는 것을 보여준다. 보다 구체적으로, 우리의 경험적 결과는 사전 훈련 데이터세트 크기가 증가함에 따라 교차 엔트로피 손실이 항상 단조롭게 감소하지만(적절한 학습률과 함께), BLEU 점수는 사전 훈련 데이터가 과제와 충분히 정렬되지 않을 때 비단조적인 경향을 보일 수 있음을 보여준다. 예를 들어, 그림 3-(_top, right_)에서 en-MC4, de-MC4 또는 ro-MC4 사전 훈련 데이터 세트의 크기를 증가시키면 WMT-15 영어-프랑스어(en-fr) 번역 작업에 대한 BLEU 점수가 감소하는 경우가 있다. 초기에는 더 작은 사전 훈련 데이터 세트 크기에 대해 (1)의 법칙을 따를 수 있지만, 스케일링 법칙은 이러한 데이터 세트 및 작업에 대해 더 큰 데이터에 대해 깨진다. 전반적으로, BLEU 점수는 일부 양의 프랑스어를 포함하는 다른 사전 훈련 데이터세트와 비교하여 결코 좋은 값에 도달하지 않는데, 이는 프랑스어를 포함하지 않는 사전 훈련 데이터세트가 이러한 특정 번역 작업과 충분히 정렬되지 않음을 나타낸다. 그러나 그림 3-(_bottom, right_)의 교차 엔트로피 손실만을 살펴보면 모든 사전 훈련 데이터 세트가 모델에 눈에 띄는 개선을 가져오고 모두 사전 훈련 데이터에 추가할 가치가 있으며 이는 잘못된 결정이라고 결론지을 수 있다.\n' +
      '\n' +
      '모델이 성장함에 따라 _다운스트림 태스크 성능_이 어떻게 변하는지를 살펴본 McKenzie et al.(2023)의 태스크 관련 메트릭과 교차 엔트로피 사이의 불일치에 대한 원격 관련 관찰은 LLM이 증가된 모델 크기와 함께 더 나쁜 태스크 성능을 보일 수 있음을 시사하지만, 우리의 발견과 유사하게, 이것은 단조적으로 감소하는 교차 엔트로피 손실에 의해 포착되지 않는다.\n' +
      '\n' +
      '전이학습에서 스케일링 법칙이 얼마 되지 않는가?\n' +
      '\n' +
      '교차 엔트로피 손실은 (2)에서 스케일링 법칙에 의해 캡처될 수 있는 단조롭게 감소하는 추세를 항상 따르지만, 사전 트레이닝 데이터세트 크기를 증가시킬 때 BLEU 점수의 단조로운 증가를 항상 보는 것은 아니다(도 2-(_top, center_) 및 도 3-(_top, right_ 참조). 우리는 사전 훈련 데이터 세트가 번역 작업과 충분히 정렬되지 않을 때만 이러한 일이 발생한다는 것을 관찰하며, 이는 다른 데이터 세트에서 사전 훈련된 모델에 비해 전반적으로 낮은 BLEU 점수를 초래한다. 미세 조정 후 높은 BLEU 점수로 이어지는 사전 훈련된 모델의 경우 BLEU 점수가 단조롭게 증가하고 (1)의 스케일링 법칙으로 잘 설명될 수 있음을 일관되게 볼 수 있다. 따라서 스케일링 법칙이 경험적 BLEU 점수에 적합할 수 있는지 여부는 다운스트림(번역) 작업에 대한 사전 훈련 데이터의 가치를 평가하는 데 좋은 첫 번째 확인이 될 수 있다. 우리는 다음 섹션에서 이것에 대해 더 자세히 설명한다.\n' +
      '\n' +
      '### 데이터 평가 사전 훈련 안내서\n' +
      '\n' +
      '마지막으로, BLEU 점수의 스케일링 행동에 대한 연구 결과를 결합하여, 목표 다운스트림 태스크에 대한 사전 훈련 데이터 세트의 값을 평가하기 위한 다음 가이드를 제안한다:\n' +
      '\n' +
      '1. 프리트레이닝 데이터세트가 주어지면, 주어진 계산 및 시간 제약 조건들 하에서 가능한 한 긴 프리트레이닝. 주기적으로 프리트레이닝 체크포인트들을 선택하고, 그것들에 미세조정하고, 다운스트림 성능 메트릭을 기록한다(섹션 3.3에서의 논의로 인해 교차 엔트로피보다 BLEU 스코어를 추천한다). 각주 3: 반복이 스케일링 거동을 복잡하게 할 수 있으므로 반복 시퀀스를 회피한다(Hernandez et al., 2022; Muennighoff et al., 2023; Tirumala et al., 2023). 이는 사전 훈련이 진행됨에 따라 "더 큰 데이터 세트"에서 각 체크포인트를 효과적으로 사전 훈련한다는 것을 의미한다.\n' +
      '\n' +
      '2. (1)의 법칙은 3개의 계수가 적합하기 때문에, 일단 3쌍의 (보이는 사전 훈련 토큰의 수, BLEU 점수)가 있으면, 우리는 최적의 계수를 찾기 위해 _try_를 갖는다. BLEU 점수가 비단조적인 행동을 하는 경우, 우리는 스케일링 법칙에 맞출 수 없다. 비단조적 행동은 오정렬의 표시일 수 있으므로(섹션 3.3의 논의에 이어), 사용 가능한 최상의 미세 조정 체크포인트의 BLEU 점수를 확인하고 다운스트림 작업에 대해 직접 훈련된 사전 훈련되지 않은 모델의 성능과 비교할 것을 권장한다. 스케일링 법칙이 잘 맞는다면, 우리는 사전 훈련 데이터 세트 크기(또는 더 많은 단계를 위한 사전 훈련)를 증가시킬 때 BLEU 점수에 대한 초기 예측을 한다. 예측된 BLEU 점수에 만족하지 않으면 이 데이터 세트에 대해 사전 훈련할 가치가 없다고 결론지었다. 예측된 BLEU 점수가 충분히 높으면 목표 BLEU 점수에 도달할 때까지 계속 사전 훈련을 한다. 스케일링 법칙이 어느 지점에서든 고장나는 경우, 사전 훈련 데이터 세트가 다운스트림 작업과 충분히 정렬되지 않고 사전 훈련이 더 유익하지 않을 수 있다고 결론지었다.\n' +
      '\n' +
      '##4 실험 설정\n' +
      '\n' +
      '실험에서 우리는 먼저 한 가지 이상의 예제를 통과하지 않고 모델을 사전 훈련한다. 그런 다음 미리 훈련된 모델의 선택된 체크포인트를 세밀하게 조정한다. 당연히 체크포인트 번호와 본 프리트레이닝 토큰의 수 사이에는 일대일 매핑이 존재한다. 이러한 방법으로 (1)과 (2)의 제안된 스케일링 법칙으로 분석하기 위해 (프리트레이닝 토큰의 수, BLEU 점수)와 (프리트레이닝 토큰의 수, _downstream_cross-entropy loss) 쌍을 수집한다. 모든 플롯은 로그 로그 척도에 있습니다.\n' +
      '\n' +
      'Model.We used the three-billion encoder-decoder T5 model with 24 encoder layers, 24 decoder layers, embedding dimension 1024 and 32 head with dimension 128. We note this is the model of T5-3B model in Abnar et al.(2022) 부록 B에서는 더 작은 770만 인코더-디코더 T5 모델을 사용하여 결과를 제공한다. 이 모델은 Raffel 등(2020)에서 T5-Large에 해당한다. 부록 A의 아키텍처에 대해 더 자세히 공유한다. 텍스트를 WordPiece 토큰으로 인코딩하기 위해(Sennrich et al., 2016; Kudo, 2018), MC4 데이터셋의 모든 언어를 포괄하는 크기 \\(250,112\\)의 어휘로 훈련된 SentencePiece(Kudo and Richardson, 2018)를 사용한다(Raffel et al., 2020).\n' +
      '\n' +
      '데이터 세트.MC4 데이터 세트의 영어(en), 독일어(de), 프랑스어(fr) 및 루마니아어(ro) 부분을 사용한다. 이 언어에 대해 개별적으로 사전 훈련과 언어 쌍을 혼합하여 실험한다. 그림 1에서 우리는 (_left_) 50% en-MC4 + 50% de-MC4의 혼합물, (_center_) 50% en-MC4 + 50% fr-MC4의 혼합물 및 (_right_) 50% en-MC4 + 50% ro-MC4의 혼합물에서 사전 훈련된 모델에 대한 결과를 제시한다. 그림 2에서는 en-MC4에서만 사전 훈련된 모델에 대한 결과를 보여준다. 그림 3에서는 이들 외에도 de-MC4에서만 사전 훈련된 모델뿐만 아니라 30% en-MC4 + 70%-fr과 70% en-MC4 + 30%-fr의 혼합물에서도 사전 훈련된 모델에 대한 결과를 제시한다. 또한 de-MC4에서만 사전 훈련된 모델뿐만 아니라 fr-MC4에서만 사전 훈련된 모델, ro-MC4에서만 사전 훈련된 모델에 대한 결과를 제시한다. WMT-17 en-de(Bojar et al., 2017), WMT-15 en-fr(Bojar et al., 2014), WMT-16 en-ro(Bojar et al., 2016). 미세 조정 데이터세트 크기가 스케일링 법칙에 미치는 영향을 이해하기 위해 우리는 때때로 이러한 번역 데이터세트에서 더 작은 무작위로 샘플링된 부분을 사용하고 사용된 토큰의 수를 나타낸다.\n' +
      '\n' +
      'Hyperparameters.preraining 시 ro-MC4 preraining을 제외한 1,000,000\\(1,000,000\\) 단계의 배치크기와 시퀀스길이 512를 사용하였다. ro-MC4의 경우, 그렇지 않으면 서열에 대해 반복을 수행해야 하기 때문에 \\(510,000\\) 단계를 미리 훈련한다. Raffel 등(2020)에 이어, 우리는 "역제곱근" 학습률 스케쥴, \\(\\frac{1}{\\sqrt{\\max(n,k)}}\\)을 사용하며, 여기서 \\(n\\)은 현재 사전 훈련 단계이고 \\(k\\)은 \\(10^{4}\\)으로 설정된다. 우리는 \\(\\{0.05,0.1,0.5,1.0,2.0,5.0\\}\\)에서 기본 학습률을 그리드 탐색하고 _upstream_ 크로스 엔트로피를 기반으로 각 사전 훈련된 모델에 가장 적합한 모델을 선택한다. 핀튜닝 동안, 다시 Raffel et al.(2020)에 이어서, 우리는 300 단계에 대해 128의 배치 크기 및 512의 시퀀스 길이를 사용한다. 우리는 \\({0.001,0.005,0.01,0.05,0.1}\\) 중에서 가장 좋은 것을 선택하여 일정한 학습률을 사용한다. 두 단계 모두 AdaFactor Optimizer(Shazeer and Stern, 2018)를 사용한다.\n' +
      '\n' +
      '스케일링 법칙 계수를 최적화한다. (1)과 (2)의 스케일링 법칙에 계수를 맞추기 위해 Hoffmann et al.(2022)과 유사하게 Huber loss(Huber, 1992)와 L-BFGS 알고리즘(Nocedal, 1980)을 사용하여 이상치가 존재할 때 강건하게 스케일링 법칙을 추정한다. Huber loss는 BLEU score는 \\(\\delta=0.1\\), _downstream_ cross-entropy loss는 \\(\\delta=1e-3\\)을 사용한다. 부록 B.2의 Huber loss를 통해 계산된 예측 오차는 초기화 격자 중에서 가장 적합한 것을 선택하고 계수를 최적화하기 위해 가장 적은 양의 사전 훈련 데이터를 필요로 하는 첫 번째 4개의 데이터 포인트를 사용하고 나머지 데이터 포인트는 보류된 데이터로 남겨 법칙의 정확성을 평가한다. 우리는 두 법칙 모두 최적화해야 할 세 개의 계수를 가지고 있기 때문에 이상적으로는 세 점이면 충분하다는 점에 주목한다. 그러나 더 많은 점을 추가하면 최적화가 이상치에 더 견고해짐으로써 적합도가 향상됩니다. 우리는 부록 A.2에서 스케일링 법칙 계수를 최적화하는 방법에 대해 더 자세히 설명하고, 다음 섹션에서 제시한 최적화된 계수 목록과 각 법칙에 대한 예측 오류는 독자에게 부록 B.2를 참조한다.\n' +
      '\n' +
      '##5 결과 및 분석\n' +
      '\n' +
      '그림 1에서, 우리는 50% en-MC4 + 50% de-MC4의 혼합물 _(left)_, 50% en-MC4 + 50% fr-MC4의 혼합물 _(center)_ 및 50% en-MC4 + 50% ro-MC4의 혼합물 _(right)_의 서로 다른 부분에 대해 사전 훈련된 모델을 분석한다. 이 모델은 _(left)_ en-de, _(center)_ en-fr 및 _(right)_ en-ro 번역 데이터 세트의 서로 다른 부분에 대해 미세 조정된다. 상단 행에서는 BLEU 점수를 보고하고, 하단 행에서는 _downstream_cross-entropy loss를 보고한다. 점선, 점선 및 실선은 내 스케일링 법칙에 해당합니다.\n' +
      '\n' +
      '도 1: **(상단) BLEU 점수 대 사전 훈련 데이터세트 크기: \\(\\mathbf{f(D_{p})=(\\log(A\\cdot D_{p}^{\\alpha}))^{\\beta}}\\)_ (left)_WMT-17 en-to-de 번역 작업. 사전 학습 데이터 세트는 50% en-MC4 + 50% de-MC4를 가지며, 점선, 점선 및 실선 파란색 곡선은 각각 다른 미세 조정 데이터 세트 크기, \\(D_{f}=6M\\), \\(D_{f}=31M\\), \\(D_{f}=3B\\) 토큰에 대한 적합 스케일링 법칙에 해당한다. (center)_WMT-15 en-to-fr 번역 작업. 사전학습 데이터세트에는 50% en-MC4와 50% fr-MC4가 있으며, 점선, 점선 및 솔리드 오렌지 곡선은 각각 다른 미세조정 데이터세트 크기, \\(D_{f}=42M\\), \\(D_{f}=210M\\), \\(D_{f}=21B\\) 토큰에 대한 적합 스케일링 법칙에 해당한다. (right)_ WMT-16 en-to-ro 번역 작업. 사전학습 데이터세트에는 50% en-MC4 + 50% ro-MC4가 있으며, 점선, 점선 및 실선 그린 곡선은 각각 \\(D_{f}=625K\\), \\(D_{f}=3M\\), \\(D_{f}=312M\\) 토큰에 대한 적합 스케일링 법칙에 해당한다. **(bottom) Cross-entropy(CE) validation loss vs pretraining dataset size: \\(\\mathbf{L(D_{p})=E+\\frac{A}{D_{p}^{\\alpha}}\\.** 상위 행과 동일한 모델. 모든 플롯에 대해 마커는 실제 실험 결과이며 검은색 수평 곡선은 작업 데이터 세트에 직접 훈련된 사전 훈련되지 않은 모델에 해당한다. ** 미세 조정 데이터세트 크기는 검은색 수평선을 포함한 모든 곡선에 대해 점선 점선 솔리드 순으로 증가한다.**\n' +
      '\n' +
      '(1) 및 (2) 상이한 미세조정 데이터세트 크기 \\(D_{f}\\)에 대해. 검은색 선들은 피네튜닝 데이터세트의 상이한 부분들에 대해 직접 트레이닝되는 "비-사전 트레이닝된" 모델들(랜덤하게 초기화됨)에 대응한다. 모든 경우에, 스케일링 법칙은 BLEU 점수(\\(\\delta=0.1\\))에 대해 최대 예측 오차가 \\(0.061\\)이고 _downstream_ cross-entropy(\\(\\delta=1e-3\\))에 대해 \\(5.95e-12\\)인 경험적 결과(마커)에 잘 맞다(자세한 내용은 부록 B.2 참조). 예상대로, 미세조정 데이터세트 크기가 증가함에 따라(예를 들어 점선-점선 순서로 진행), BLEU 점수는 증가하고 교차 엔트로피 손실은 매끄럽고 단조롭게 감소한다. 마찬가지로 사전 학습 데이터 세트 크기 \\(D_{p}\\)가 x축을 따라 증가함에 따라 두 메트릭 모두 개선되는 것을 볼 수 있다. 사전 훈련 데이터 세트 크기의 증가에 의한 개선은 더 작은 미세 조정 데이터 세트에 더 효과적이라는 것을 주목하라. 미세 조정 데이터세트가 충분히 큰 경우(예를 들어, 실선), BLEU 점수는 사전 훈련 데이터세트 크기에 관계없이 다소 일정하다. 사실, 우리는 미세조정 데이터세트가 클 때 프리트레이닝되지 않은 모델(검은 선)에 비해 프리트레이닝의 개선이 거의 또는 전혀 보이지 않는다. **이는 이러한 작업의 경우 미세 조정 데이터 세트가 충분히 클 때 모델을 사전 훈련할 필요가 없음을 의미한다. 운 좋게도, 우리는 이것이 그럴 것인지 (즉, 이용가능한 미세조정 데이터가 스케일링 법칙의 사용으로 사전 훈련을 완전히 제거하기에 충분한지) 정확하게 예측할 수 있다. 스케일링 법칙의 계수를 최적화하기 위해 합리적인 계산 비용으로 사전 훈련 데이터 세트의 작은 부분에 모델을 사전 훈련한 다음 섹션 3.4.**에서 제공하는 지침을 따르기만 하면 된다.\n' +
      '\n' +
      '그림 2에서 모든 도표에서 사전 훈련 데이터 세트를 100% en-MC4로 변경한다. 직관적으로, 우리는 이 데이터 세트가 번역 작업에 언어 중 하나를 포함하지 않기 때문에 그림 1의 다국어 쌍보다 번역 작업에 덜 정렬될 것으로 예상한다. 실제로, 우리는 동일한 미세조정 데이터세트 크기에 대해 일반적으로 더 작은 BLEU 점수와 더 높은 교차 엔트로피 손실을 본다. 그림 1의 대부분의 결론은 다음과 같다.\n' +
      '\n' +
      '도 2: **(상단) BLEU 점수 대 사전 훈련 데이터세트 크기: \\(\\mathbf{f(D_{p})=(\\log(A\\cdot D_{p}^{\\alpha}))^{\\beta}}\\)_ (left)_WMT-17 en-to-de 번역 작업. 점선, 점선 및 실선 빨간색 곡선은 각각 다른 미세 조정 데이터 세트 크기, \\(D_{f}=6M\\), \\(D_{f}=31M\\), \\(D_{f}=3B\\) 토큰에 대한 적합된 스케일링 법칙에 해당한다. (center)_WMT-15 en-to-fr 번역 작업. 점선, 점선 및 실선 빨간색 곡선은 각각 다른 미세 조정 데이터 세트 크기, \\(D_{f}=42M\\), \\(D_{f}=210M\\), \\(D_{f}=21B\\) 토큰에 대한 적합된 스케일링 법칙에 해당한다. (right)_ WMT-16 en-to-ro 번역 작업. 점선, 점선 및 솔리드 레드 곡선은 각각 \\(D_{f}=625K\\), \\(D_{f}=3M\\), \\(D_{f}=312M\\) 토큰에 대한 적합 스케일링 법칙에 해당한다. **(bottom) Cross-entropy (CE) validation loss vs pretraining dataset size: \\(\\mathbf{L(D_{p})=E+\\frac{A}{D_{p}^{\\alpha}}}\\)**. 맨 위 행과 같은 모델입니다. 모든 플롯에 대해 마커는 실제 실험 결과이며 검은색 수평 곡선은 작업 데이터 세트에 직접 훈련된 사전 훈련되지 않은 모델에 해당한다. ** 미세 조정 데이터세트 크기는 검은색 수평선을 포함한 모든 곡선에 대해 점선 점선 솔리드 순으로 증가한다.**\n' +
      '\n' +
      '예를 들어, 사전 훈련 데이터는 미세 조정 데이터 세트가 충분히 클 때 덜 중요하다. 한 가지 눈에 띄는 차이점은 en-fr 번역 작업(_center_)에 대한 BLEU 점수이다. 우리는 \\(D_{f}=42M\\) 및 \\(D_{f}=210M\\)에 대해, BLEU 점수에 대한 스케일링 법칙은 미리 훈련된 데이터 세트의 크기가 임계값을 통과하고 교차 엔트로피 손실 스케일이 예상대로 되면 실제로 깨진다는 것을 안다. 이는 BLEU 점수가 더 큰 사전 훈련 데이터 세트에 대해 때때로 감소하기 때문에 직관에 반한다. 스케일링 법칙이 이들 태스크에 대해 최대 0.025의 예측 오차를 갖는 사전 훈련 데이터에 잘 적합하기 때문에, 스케일링 법칙의 이러한 단절은 en-de 또는 en-ro 변환 태스크에서 발생하지 않는다는 것을 주목하라(\\(\\delta=0.1\\)). 이를 더 잘 조사하기 위해 그림 3에서는 언어의 선택으로 인해 덜 정렬된 사전 훈련 데이터 세트를 자세히 살펴본다.\n' +
      '\n' +
      '그림 3-(_left_)에서 사전 훈련 데이터 세트가 100% en-MC4(그림 2-(_left_)와 동일), 50% en-MC4 및 50% de-MC4(그림 1-(_left_)와 동일), 100% de-MC4, 100% fr-MC4(less aligned) 및 100% ro-MC4(less aligned)인 en-de 번역 작업에 대한 스케일링 법칙을 제공한다. 마지막 두 사전 훈련 데이터 세트는 번역 쌍이 이러한 언어를 포함하지 않기 때문에 번역 작업과 가장 덜 정렬될 것으로 예상된다. 그럼에도 불구하고 스케일링 법칙은 BLEU 점수와 교차 엔트로피 손실 모두에 일관되게 잘 맞는다는 것을 알 수 있다. 그러나, 이것은 en-fr 번역 작업의 경우 항상 그런 것은 아니다. 그림 3-(_right_)에서 사전 훈련 데이터 세트가 en-MC4 및 fr-MC4 데이터 세트의 서로 다른 혼합물인 en-fr 번역 작업에 대한 스케일링 법칙을 제공한다. 또한 100% de-MC4와 100% ro-MC4와 같은 "less aligned" 프리트레이닝 데이터 셋을 포함한다. 놀랍게도 BLEU 점수에 대한 스케일링 법칙은 오직 영어(100% en-MC4), 오직 독일어(100% de-MC4), 그리고 오직 루마니아어(100% ro-MC4) 프리트레이닝 데이터 셋에 대해 어느 시점 이후에 깨지는 반면 교차 엔트로피 손실은 항상 (2)의 스케일링 법칙을 따른다. 흥미롭게도, 우리는 오직 프랑스어(100% fr-MC4) 사전 훈련 데이터 세트에 대한 BLEU 점수 스케일링에서 그러한 중단을 관찰하지 않는다 - 사전 훈련에 프랑스어 데이터를 포함하지 않는 것은 en-fr 번역 작업에서 열악한 스케일링으로 이어지지만 영어를 포함하지 않는 것은 그러한 영향을 미치지 않는다는 것을 암시한다. 또한 BLEU 점수가 스케일링이 끊어지는 이 세 가지 사전 훈련 데이터 세트에 대해 가장 낮다는 것을 알 수 있다. **이것은 (1)의 스케일링 법칙이 사전 트레이닝 데이터세트가 좋은 성능을 발생시킨다는 약속을 갖는 한 BLEU 점수에 대해 잘 작동함을 시사한다. 그러나 스케일링 법칙이 잘 맞지 않는 경우 BLEU 점수가 전반적으로 낮은 것으로 의심할 수 있다. 따라서 BLEU 점수에 대한 스케일링 법칙을 적합시킬 수 있는지 여부는 사전 훈련 데이터와 특정 번역 작업 간의 정렬 정도에 대해 좋은 표시를 제공하는 것으로 판단된다.**\n' +
      '\n' +
      '**5.1**: _우리는 그림 3에서 또 다른 흥미로운 현상을 관찰한다. en-de 및 en-fr 작업 모두에 대해, 100% en-MC4는 각각 더 정렬된 50% en-MC4 + 50% de/fr-MC4 균형 데이터 세트보다 BLEU 점수 및 다운스트림 교차 엔트로피가 현저히 악화된다. 그러나 de-MC4와 fr-MC4는 en-de와 en-fr 태스크에서 거의 균형 잡힌 데이터 세트를 수행한다. 우리는 주어진 en-de와 en-fr 과제에 대해 독일어/프랑스어만 사전 훈련이 영어만 사전 훈련보다 더 도움이 되는 이유에 대한 조사를 향후 작업에 맡긴다._\n' +
      '\n' +
      '또한, 도 2 및 도 3의 en-fr 플롯에서 관찰된 태스크 관련 메트릭인 BLEU 점수와의 불일치로 인해 _downstream_ cross-entropy 손실만을 보고 태스크와의 사전 트레이닝 데이터세트의 정렬 정도에 대해 강한 결론을 내릴 수 없음을 강조한다. 이는 두 메트릭이 지수 관계를 갖는다는 고든 등(2021)의 주장에 대한 반대 예제이다. 이를 더 잘 입증하기 위해 그림 4에서는 BLEU 점수 대 _를 제공한다. downstream_cross-entropy log-log plot for en-de and en-fr translation tasks, respectively. 두 메트릭이 en-de 태스크에 대해 그림 4-(_left_)에서 실제로 상관관계가 있는 것처럼 보이지만, 우리는 지수 관계로 명확하게 설명할 수 없는 경우에 그림 4-(_right_)에서 en-fr 태스크에 대해 다소 임의적인 관계를 관찰한다. **이는 _downstream_cross-entropy가 BLEU 점수에 대한 항상 좋은 지표는 아님을 시사한다. 이는 _upstream_cross-entropy 손실에 대해 개발된 스케일링 법칙이 실제로 모델의 다운스트림 거동에 유용한 예측 변수인지 의문을 제기한다.**\n' +
      '\n' +
      '**5.2**: _우리는 또한 BLEU 점수의 정의를 재검토하여 비-평활한 행동의 근본 원인을 더 잘 이해하고 BLEU 점수 계산의 적어도 일부 요소에서 부드러운 단조 척도를 볼 수 있는지 확인한다. BLEU 점수의 공통 형태가_\n' +
      '\n' +
      '\\[\\text{BLEU}=\\text{brevity-penalty}\\cdot\\left(\\prod_{i=1}^{4}\\text{precision}_{i}\\right)^{1/4}, \\tag{3}\\)\n' +
      '\n' +
      'n-gram의 정밀도(\\text{precision}_{n}\\)는 n-gram의 정밀도를 의미하며, 두 번째 항은 n이 1에서 4까지 다양할 때 정밀도의 기하 평균이다. 모든 실험에서 간결성-벌점(=1\\), 즉 비평활성을 관찰한다.\n' +
      '\n' +
      '도 4: **BLEU 점수 대 _ downstream_cross-entropy loss. _ (left)_en-de 번역 작업의 경우 모든 사전 훈련 데이터 세트에 대해 두 메트릭 간의 일관된 상관 관계를 볼 수 있다. 이는 Gordon et al.(2021)의 연구 결과를 뒷받침한다. _ (right)_ en-fr 번역 작업의 경우, 두 메트릭은 보통 임의의 관계를 나타낸다. 때때로 BLEU 점수는 증가하는 반면 교차 엔트로피도 증가한다. (왼쪽)의 en-de 결과와 달리 (Gordon et al., 2021)의 지수 관계는 여기에서 관찰되지 않는다.\n' +
      '\n' +
      '행동은 정밀 용어에 기인할 수 있다. 따라서 (1)의 스케일링 법칙을 포함한 우리의 연구 결과는 정밀-another_다운스트림 _task 메트릭._\n' +
      '\n' +
      '##6 토론 및 결론\n' +
      '\n' +
      '우리는 사전 훈련 데이터가 증가함에 따라 LLM의 다운스트림 성능에 대한 스케일링 동작을 연구하고 _downstream_ 크로스 엔트로피와 BLEU 점수에 대한 스케일링 법칙을 제안한다. 우리는 광범위한 실험을 통해 스케일링 동작이 (1) 사전 훈련과 다운스트림 데이터 간의 정렬 정도 및 (2) 미세 조정 데이터 세트의 크기에 의해 크게 영향을 받는다는 것을 입증한다. 분포가 충분히 정렬된 유리한 경우 로그 스케일링 법칙을 사용하여 BLEU 점수를 정확하게 예측할 수 있음을 보여준다. 그러나 정렬이 적으면 BLEU 점수가 예측 불가능하게 변동하는 경우가 있는 반면 _downstream_ 크로스 엔트로피는 단조롭게 개선된다. 또한 미세 조정 데이터 세트 크기가 충분히 클 때 사전 훈련은 거의 또는 전혀 가치가 없음을 관찰한다.\n' +
      '\n' +
      '우리의 연구 결과는 교차 엔트로피(업스트림이든 다운스트림이든)에 의해서만 결정을 내리지 않고 다운스트림 성능 메트릭을 연구하는 것의 중요성을 강조한다. 이것은 모델들이 스케일링될 때 매끄러운 메트릭들과 비매끄러운 메트릭들 사이의 거동 불일치에 관한 Schaeffer et al.(2023)의 발견들을 반영한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Abnar et al. (2022) Abnar, S., Dehghani, M., Neyshabur, B., and Sedghi, H. (2022). 대규모 사전 훈련의 한계를 탐색합니다. _International Conference on Learning Representations_.\n' +
      '* Agostinelli et al. (2022) Agostinelli, A., Uijlings, J., Mensink, T., and Ferrari, V. (2022). 소스 모델 앙상블을 선택하기 위한 전송성 메트릭. IEEE/CVF Conference on Computer Vision and Pattern Recognition_의 _Proceedings, pages 7936-7946.\n' +
      '* Bahri et al. (2021) Bahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, U. (2021). 신경 스케일링 법칙을 설명하는 것 arXiv preprint arXiv:2102.06701_.\n' +
      '*Bansal et al. (2022) Bansal, Y., Ghorbani, B., Garg, A., Zhang, B., Cherry, C., Neyshabur, B., and Firat, O. (2022). nmt의 데이터 스케일링 법칙: 노이즈 및 아키텍처의 영향. _International Conference on Machine Learning_, pages 1466-1482. PMLR.\n' +
      '* Bao et al. (2019) Bao, Y., Li, Y., Huang, S. - L., Zhang, L., Zheng, L., Zamir, A., and Guibas, L. (2019). 과제 전이 학습에서 전이 가능성에 대한 정보 이론적 접근. _2019 IEEE 국제 회의 on image processing (ICIP)_에서, 페이지 2309-2313. IEEE.\n' +
      '* Bojar et al.(2014) Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., et al.(2014) 통계 기계 번역에 대한 2014년 워크숍의 결과. 통계 기계 번역에 관한 제9회 워크숍의 _Proceedings_, 12-58페이지.\n' +
      '* Bojar et al. (2017) Bojar, O. r., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huang, S., Huck, M., Koehn, P., Liu, Q., Logacheva, V., Monz, C., Negri, M., Post, M., Rubino, R., Specia, L., and Turchi, M. (2017). 기계 번역에 관한 2017년 회의의 결과들(wmt17). [Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers_, pages 169-214, Copenhagen, Denmark. 컴퓨터 언어학과의 연관성\n' +
      '* Bojar et al. (2016) Bojar, O. r., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huck, M., Jimeno Yepes, A., Koehn, P., Logacheva, V., Monz, C., Negri, M., Neveol, A., Neves, M., Popel, M., Post, M., Rubino, R., Scarton, C., Specia, L., Turchi, M., Verspoor, K., and Zampieri, M. (2016). 기계 번역에 관한 2016년 컨퍼런스의 결과. [Proceedings of the First Conference on Machine Translation_, pages 131-198, Berlin, Germany]. 컴퓨터 언어학과의 연관성\n' +
      '* Bojar et al. (2017)Chiang, C.-H. 그리고 이현예 (2022). 사전 훈련된 언어 모델의 전달 가능성에 대한 연구: 인공 데이터 세트를 통한 연구. _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 10518-10525.\n' +
      '* Dai et al. (2019) Dai, X., Karimi, S., Hachey, B., and Paris, C. (2019). 유사도 측정을 사용하여 mer에 대한 사전 훈련 데이터를 선택합니다. _Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics Association: Human Language Technologies, Volume 1(Long and Short Papers)_, pages 1460-1470.\n' +
      '* Fernandes et al. (2023) Fernandes, P., Ghorbani, B., Garcia, X., Freitag, M., and Firat, O. (2023). 다국어 신경 기계 번역을 위한 법칙의 스케일링 arXiv preprint arXiv:2302.09650_.\n' +
      '* Ghiasi et al. (2018) Ghiasi, G., Lin, T. - Y., and Le, Q. V. (2018). Dropblock : 컨벌루션 네트워크를 위한 정규화 방법. _ 신경 정보 처리 시스템들_, 31에서의 진보들.\n' +
      '* Ghorbani 등 (2021) Ghorbani, B., Firat, O., Freitag, M., Bapna, A., Krikun, M., Garcia, X., Chelba, C., and Cherry, C. (2021). 신경 기계 번역을 위한 스케일링 법칙. _International Conference on Learning Representations_.\n' +
      '* Gordon et al. (2021) Gordon, M. A., Duh, K., and Kaplan, J. (2021). 신경 기계 번역을 위한 데이터 및 매개변수 스케일링 법칙. 모엔스 -F., Huang, X., Specia, L., and Yih, S. W.-t., editoritors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 5915-5922, Online and Punta Cana, Dominican Republic. 컴퓨터 언어학과의 연관성\n' +
      '* He et al. (2019) He, K., Girshick, R., and Dollar, P. (2019). 이미제넷 사전 교육을 재고하는 중입니다. IEEE/CVF International Conference on Computer Vision_의 _Proceedings, pages 4918-4927.\n' +
      '* Henighan et al. (2020) Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T. B., Dhariwal, P., Gray, S., et al. (2020). 자기회귀 생성 모델링을 위한 법칙의 확장 - _ arXiv preprint arXiv:2010.14701_.\n' +
      '* Hernandez et al. (2022) Hernandez, D., Brown, T., Conerly, T., DasSarma, N., Drain, D., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Henighan, T., Hume, T., et al. (2022). 반복되는 데이터에서 학습의 법칙과 해석 가능성을 확장합니다. _ arXiv preprint arXiv:2205.10487_.\n' +
      '* Hernandez et al. (2021) Hernandez, D., Kaplan, J., Henighan, T., and McCandlish, S. (2021). 전송을 위한 법률 확대. _ arXiv preprint arXiv:2102.01293_.\n' +
      '* Hoffmann et al.(2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al.(2022) 컴퓨터 최적 대형 언어 모델 훈련 arXiv preprint arXiv:2203.15556_.\n' +
      '* Huang et al. (2022) Huang, L. - K., Huang, J., Rong, Y., Yang, Q., and Wei, Y. (2022). 좌절할 정도로 쉬운 전이 가능성 추정 _International Conference on Machine Learning_, pages 9201-9225. PMLR.\n' +
      '* Huber(1992) Huber, P. J. (1992). 위치 매개변수의 강력한 추정입니다. _Breakthroughs in statistics: Methodology and distribution_, pages 492-518. Springer.\n' +
      '* 허터(2021) 허터, M. (2021). 학습 곡선 이론. _ arXiv preprint arXiv:2102.04074_.\n' +
      '*Ibrahim et al. (2022) Ibrahim, S., Ponomareva, N., and Mazumder, R. (2022). 새로운 것이 항상 더 나은 것은 아니다: 전송성 메트릭, 그 특성, 안정성 및 성능을 재고하는 것. _Joint European Conference on Machine Learning and Knowledge Discovery in Database_, pages 693-709. Springer.\n' +
      '* Jain et al. (2023) Jain, A., Swaminathan, G., Favaro, P., Yang, H., Ravichandran, A., Harutyunyan, H., Achille, A., Dabeer, O., Schiele, B., Swaminathan, A., et al. (2023). 성능 및 데이터 요구 사항을 예측하는 메타 학습 접근 방식입니다. IEEE/CVF Conference on Computer Vision and Pattern Recognition_의 _Proceedings, pages 3623-3632.\n' +
      '* Johnson et al. (2018) Johnson, M., Anderson, P., Dras, M., and Steedman, M. (2018). 더 작은 파일럿 데이터에서 큰 데이터 세트의 정확도를 예측합니다. _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 450-455.\n' +
      '* Johnson et al. (2018) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). 신경 언어 모델의 법칙을 확장합니다. _ arXiv preprint arXiv:2001.08361_.\n' +
      '* 쿠도(2018) 쿠도, T. (2018). 서브워드 정규화: 다수의 서브워드 후보들로 신경망 번역 모델들을 개선한다. _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 66-75.\n' +
      '* Kudo and Richardson (2018) Kudo, T. and Richardson, J. (2018). 문장: 신경 텍스트 처리를 위한 단순하고 언어 독립적인 서브워드 토큰화기와 디토키나이저. _ EMNLP 2018_, 66페이지.\n' +
      '* McKenzie et al.(2023) McKenzie, I. R., Lyzhov, A., Pieler, M., Parrish, A., Mueller, A., Prabhu, A., McLean, E., Kirtland, A., Ross, A., Liu, A., et al.(2023) 역축소: 크기가 클수록 좋지 않습니다. _ arXiv preprint arXiv:2306.09479_.\n' +
      '* Mikami et al. (2022) Mikami, H., Fukumizu, K., Murai, S., Suzuki, S., Kikuchi, Y., Suzuki, T., Maeda, S. -i., and Hayashi, K. (2022). 신2리얼 전송을 위한 스케일링 법칙: 당신의 사전 훈련은 얼마나 효과적입니까? _Joint European Conference on Machine Learning and Knowledge Discovery in Database_, pages 477-492. Springer.\n' +
      '* Muennighoff et al.(2023) Muennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Tazi, N., Piktus, A., Pyysalo, S., Wolf, T., and Raffel, C. (2023). 데이터 제한 언어 모델의 확장 IMT-2000 3GPP-신경정보처리시스템에 관한 제37차 회의\n' +
      '* Nguyen et al. (2020) Nguyen, C., Hassner, T., Seeger, M., and Archambeau, C. (2020). Leep: 학습된 표상의 전이성을 평가하기 위한 새로운 척도. _International Conference on Machine Learning_, pages 7294-7305. PMLR.\n' +
      '* Nocedal(1980) Nocedal, J. (1980). 제한된 저장소로 준뉴턴 행렬을 업데이트하는 단계 _ Mathematics of computation_, 35(151):773-782.\n' +
      '* Papineni et al. (2002) Papineni, K., Roukos, S., Ward, T., and Zhu, W. - J (2002). Bleu: 기계 번역의 자동 평가 방법. _Proceedings of the 40th annual meeting for Computational Linguistics_, pages 311-318.\n' +
      '* Plank and Van Nood(2011) Plank, B. and Van Nood, G. (2011). 파싱을 위한 도메인 유사성의 효과적인 측정입니다. _Proceedings of the 49th Annual Meeting for Computational Linguistics: Human Language Technologies_, pages 1566-1576.\n' +
      '* Raffel 등 (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). 단일 텍스트-텍스트 변환기를 이용한 전이학습의 한계점 탐색 The Journal of Machine Learning Research_, 21(1):5485-5551.\n' +
      '* Schaeffer et al. (2023) Schaeffer, R., Miranda, B., and Koyejo, S. (2023). 대형 언어 모델의 출현 능력은 신기루인가? IMT-2000 3GPP-신경정보처리시스템에 관한 제37차 회의\n' +
      '* Sennrich et al. (2016) Sennrich, R., Haddow, B., and Birch, A. (2016). 하위 단어 단위가 있는 희귀 단어의 신경 기계 번역. _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1715-1725.\n' +
      '* Sharma and Kaplan(2020) Sharma, U. 및 Kaplan, J. (2020). 데이터 매니폴드의 차원으로부터의 신경 스케일링 법칙. _ arXiv preprint arXiv:2004.10802_.\n' +
      '* Shazeer and Stern(2018) Shazeer, N. 및 스턴, M. (2018). 보조인자: 하위 선형 메모리 비용을 갖는 적응형 학습 속도. _International Conference on Machine Learning_, pages 4596-4604. PMLR.\n' +
      '* Shen et al. (2019) Shen, Z., Liu, Z., Li, J., Jiang, Y. - G., Chen, Y., and Xue, X. (2019). 깊은 감독으로 처음부터 객체 탐지 IEEE transaction on pattern analysis and machine intelligence_, 42(2):398-412.\n' +
      '* Sun et al. (2017) Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). 딥러닝 시대에 데이터의 불합리한 효과를 재조명합니다. In _Proceedings of the IEEE international conference on computer vision_, pages 843-852.\n' +
      '\n' +
      'Tamkin, A., Singh, T., Giovanardi, D., and Goodman, N. (2020). 사전 훈련된 언어 모델의 전이 가능성 조사 _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 1393-1401.\n' +
      '* Tay 등(2021) Tay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H. W., Narang, S., Yogatama, D., Vaswani, A., and Metzler, D. (2021). 효율적으로 확장: 사전 훈련 및 미세 조정 변압기의 통찰력. _International Conference on Learning Representations_.\n' +
      '* Tirumala et al.(2023) Tirumala, K., Simig, D., Aghajanyan, A., and Morcos, A. S. (2023). D4: 문서 중복 제거 및 다양화를 통한 llm 사전 훈련 개선. 제37차 신경정보처리시스템 컨퍼런스에서 데이터세트 및 벤치마크 Track_\n' +
      '* Tran et al. (2019) Tran, A. T., Nguyen, C. V., and Hassner, T. (2019). 감독된 분류 작업의 전이성 및 경도. IEEE/CVF International Conference on Computer Vision_의 _Proceedings, pages 1395-1405.\n' +
      '* Van Asch and Daelemans (2010) Van Asch, V. 및 대렘만, W. (2010). 성능 추정을 위해 도메인 유사성을 사용한다. _Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing_, pages 31-36.\n' +
      '* You et al. (2021) You, K., Liu, Y., Wang, J., and Long, M. (2021). 로그me: 전이 학습을 위한 사전 훈련된 모델의 실제 평가. _International Conference on Machine Learning_, pages 12133-12143. PMLR.\n' +
      '* Zhai et al. (2022) Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. (2022). 비전 트랜스포머를 확장합니다. IEEE/CVF Conference on Computer Vision and Pattern Recognition_의 _Proceedings, pages 12104-12113.\n' +
      '* Zhang et al. (2022) Zhang, B., Ghorbani, B., Bapna, A., Cheng, Y., Garcia, X., Shen, J., and Firat, O. (2022). 기계 번역을 위한 언어 모델 아키텍처의 스케일링 및 전송을 검토합니다. _International Conference on Machine Learning_, pages 26176-26192. PMLR.\n' +
      '* Zhuocheng et al. (2023) Zhuocheng, Z., Gu, S., Zhang, M., and Feng, Y. (2023). 문서 신경 기계 번역을 위한 스케일링 법칙. _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 8290-8303.\n' +
      '* Zoph et al. (2020) Zoph, B., Ghiasi, G., Lin, T. - Y., Cui, Y., Liu, H., Cubuk, E. D., and Le, Q. (2020). 사전 훈련과 자기 훈련에 대한 재고 신경 정보 처리 시스템들_, 33:3833-3845의 진보들.\n' +
      '\n' +
      '추가적인 실험 세부사항\n' +
      '\n' +
      '### Model Architectures\n' +
      '\n' +
      '표 1 및 표 2에서 T5-3B 및 T5-770M 모델의 아키텍처 세부사항을 제공한다. 이들 모델은 Raffel 등(2020)에 의해 처음 도입되었다.\n' +
      '\n' +
      '스케일링 법칙 계수의 최적화\n' +
      '\n' +
      '이 절에서는 스케일링 법칙의 계수를 최적화하는 방법에 대해 자세히 설명합니다. Hoffmann et al.(2022)에 이어, 우리는 이상치에 대한 과적합을 최소화하기 위해 Huber loss(Huber, 1992)를 사용한다. 휴버 손실은 최적화 문제에서 이상치 데이터 포인트의 영향을 억제하는데 특히 유용하다. 보다 구체적으로, 값 \\(r\\)이 있는 데이터 포인트가 \\(\\hat{r}\\)으로 법칙에 의해 예측된다면, 그 데이터 포인트에 대한 손실은 \\(\\hat{r}\\)이 될 것이다.\n' +
      '\n' +
      '\\begin{cases}\\frac{1}{2}(r-\\hat{r})=\\begin{cases}\\frac{1}{2}(r-\\hat{r}^{2}&\\text{for }|r-\\hat{r}|\\leq\\delta,\\\\delta\\cdot(|r-\\hat{r}|-\\frac{1}{2}\\delta)&\\text{otherwise.}\\end{cases}\\tag{4}\\text{for }|r-\\hat{r}|-\\frac{1}{2}\\delta}\n' +
      '\n' +
      '일반적으로 BLEU 점수(0에서 100 사이)와 _downstream_ cross-엔트로피 사이의 수치적 범위 차이로 인해, (1)의 BLEU 점수 법칙은 \\(\\delta=0.1\\), (2)의 _downstream_ cross-엔트로피 법칙은 \\(\\delta=1e-3\\)을 사용한다.\n' +
      '\n' +
      '최적화를 위해 L-BFGS 알고리즘(Nocedal, 1980)을 사용한다. 구체적으로, (1)의 BLEU 점수 법칙에 대해, 우리는 해결한다\n' +
      '\n' +
      '\\[\\min_{E,A,\\alpha,\\beta}\\sum_{\\text{Data point}i}\\ell_{\\delta}(\\log f_{i}, \\log\\hat{f}(D_{p_{i}})), \\tag{5}\\\n' +
      '\n' +
      '여기서 \\(D_{p_{i}}\\)는 사전 훈련 데이터 세트 크기이고 \\(f_{i}\\)는 데이터 포인트 \\(i\\)에 대한 BLEU 점수이며, \\(\\hat{f}(\\cdot)\\)는 최적 법칙 \\(f(\\cdot)\\)에 대한 근사치이다. 마찬가지로, (2)의 _downstream_ cross-entropy loss law에 대해서도, 우리는 해결한다\n' +
      '\n' +
      '\\[\\min_{E,A,\\alpha}\\sum_{\\text{Data point}i}\\ell_{\\delta}(\\log L_{i}, \\log\\hat{L}(D_{p_{i}})), \\tag{6}\\\n' +
      '\n' +
      '여기서 \\(D_{p_{i}}\\)은 사전 훈련 데이터 세트 크기이고 \\(L_{i}\\)은 데이터 포인트 \\(i\\)에 대한 _다운스트림_ 교차 엔트로피 손실이고 \\(\\hat{L}(\\cdot)\\)은 최적 법칙 \\(L(\\cdot)\\)에 대한 근사치이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c} \\hline Embedding Dimension & 1024 \\\\ Number of Heads & 32 \\\\ Number of Encoder Layers & 24 \\\\ Number of Decoder Layers & 24 \\\\ Head Dimension & 128 \\\\ MLP Dimension & 16384 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: T5-3B Raffel 등(2020) 아키텍처 세부사항.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c} \\hline Embedding Dimension & 1024 \\\\ Number of Heads & 16 \\\\ Number of Encoder Layers & 24 \\\\ Number of Decoder Layers & 24 \\\\ Head Dimension & 64 \\\\ MLP Dimension & 2816 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: T5-770M Raffel et al.(2020) 아키텍처 세부사항.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      '###축소법칙의 최적계수와 예측오차\n' +
      '\n' +
      '표 3, 4, 5, 6에서 그림 1과 2에 표시된 스케일링 법칙에 대한 최적화된 계수를 예측 오차와 함께 제공한다.\n' +
      '\n' +
      '도 6: **(상단) BLEU 점수 대 사전 훈련 데이터세트 크기: \\(\\mathbf{f(D_{p})=(\\log(A\\cdot D_{p}^{\\alpha}))^{\\beta}}\\)_ (left)_** WMT-17 en-to-de 번역 작업. 점선 및 점선 빨간색 곡선은 각각 다른 미세 조정 데이터 세트 크기, \\(D_{f}=6M\\) 및 \\(D_{f}=31M\\) 토큰에 대한 적합된 스케일링 법칙에 해당한다. _ (right)_WMT-15 en-to-fr 번역 작업. 점선 및 점선 빨간색 곡선은 각각 다른 미세 조정 데이터 세트 크기, \\(D_{f}=42M\\) 및 \\(D_{f}=210M\\) 토큰에 대한 적합 스케일링 법칙에 해당한다. **(bottom) Cross-entropy(CE) validation loss vs pretraining dataset size: \\(\\mathbf{L(D_{p})=E+\\frac{A}{D_{p}^{\\alpha}}.}\\)** 상위 행과 동일한 모델이다. 모든 플롯에 대해 마커는 실제 실험 결과이며 검은색 수평 곡선은 작업 데이터 세트에 직접 훈련된 사전 훈련되지 않은 모델에 해당한다. ** 미세 조정 데이터세트 크기는 검은색 수평선을 포함한 모든 곡선에 대해 점선 순으로 증가한다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c c|c} \\hline \\hline Pretraining Dataset & Finetuning Dataset & Finetuning Dataset Size & \\(E\\) & \\(A\\) & \\(\\alpha\\) & Prediction Error \\\\ \\hline\n' +
      '100\\% en +MC4 & WMT-17 en-de & 6M & \\(-18.88\\) & \\(0.15\\) & \\(3.30\\) & \\(0.014\\) \\\\\\(3.30\\) & \\(0.014\\)\n' +
      '100\\% en +MC4 & WMT-17 en-de & 31M & \\(-1.81\\times 10^{4}\\) & \\(896.12\\) & \\(0.28\\) & \\(0.006\\) \\\\\\(0.006\\)\n' +
      '100\\% en +MC4 & WMT-17 en-de & 3B & \\(1.02\\times 10^{-7}\\) & \\(104.92\\) & \\(0.42\\) & \\(0.015\\) \\\\ \\hline\n' +
      '100\\% en +MC4 & WMT-15 en-fr & 42M & \\(1.00\\) & \\(2.57\\times 10^{-5}\\) & \\(1.11\\times 10^{4}\\) & \\(0.042\\) \\\\\\\n' +
      '100\\% en +MC4 & WMT-15 en-fr & 210M & \\(-6.38\\times 10^{7}\\) & \\(3.43\\times 10^{6}\\) & \\(0.20\\) & \\(0.034\\) \\\\\\(0.034\\)\n' +
      '100\\% en +MC4 & WMT-15 en-fr & 21B & \\(204.81\\) & \\(3.80\\times 10^{14}\\) & \\(9.97\\times 10^{-3}\\) & \\(0.004\\) \\\\ \\hline\n' +
      '100\\% en +MC4 & WMT-16 en-ro & 625K & \\(-10.54\\) & \\(0.55\\) & \\(1.12\\) & \\(0.008\\) \\\\(1.12\\) & \\(0.008\\)\n' +
      '100\\% en +MC4 & WMT-16 en-ro & 3M & \\(-40.41\\) & \\(2.11\\) & \\(0.79\\) & \\(0.025\\) \\\\(0.79\\) & \\(0.025\\)\n' +
      '100\\% en +MC4 & WMT-16 en-ro & 312M & \\(3.61\\) & \\(8.17\\times 10^{5}\\) & \\(0.19\\) & \\(0.018\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 그림 2-(**bottom**)의 결과에 대한 _downstream_cross-entropy law \\(L(D_{p})=E+\\frac{A}{D_{p}^{\\alpha}}\\)에 대한 계수. Huber Loss는 _downstream_ cross-entropy 법칙의 경우 \\(\\delta=10^{-5}\\)을 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c c|c} \\hline \\hline Pretraining Dataset & Finetuning Dataset & Finetuning Dataset Size & \\(E\\) & \\(A\\) & \\(\\alpha\\) & Prediction Error \\\\ \\hline\n' +
      '50\\% en + 50\\% de-MC4 & WMT-17 en-de & 6M & \\(-180.75\\) & \\(9.00\\) & \\(0.75\\) & \\(0.034\\) \\\\(0.034\\)\n' +
      '50\\% en + 50\\% de-MC4 & WMT-17 en-de & 31M & \\(-1.68\\times 10^{3}\\) & \\(84.04\\) & \\(0.49\\) & \\(0.050\\) \\\\(0.050\\)\n' +
      '50\\% en + 50\\% de-MC4 & WMT-17 en-de & 3B & \\(-1.64\\times 10^{8}\\) & \\(9.91\\times 10^{6}\\) & \\(0.19\\) & \\(0.048\\) \\\\ \\hline\n' +
      '50\\% en + 50\\% fr-MC4 & WMT-15 en-fr & 42M & \\(-1.82\\times 10^{4}\\) & \\(8.98\\times 10^{2}\\) & \\(0.42\\) & \\(0.061\\) \\\\\\(0.061\\)\n' +
      '50\\% en + 50\\% fr-MC4 & WMT-15 en-fr & 210M & \\(-2.33\\times 10^{4}\\) & \\(1.21\\times 10^{3}\\) & \\(0.40\\) & \\(0.013\\) \\\\\\(0.013\\)\n' +
      '50\\% en + 50\\% fr-MC4 & WMT-15 en-fr & 21B & \\(5.08\\times 10^{3}\\) & \\(4.61\\times 10^{8}\\) & \\(0.16\\) & \\(0.005\\) \\\\ \\hline\n' +
      '50\\% en + 50\\% ro-MC4 & WMT-16 en-ro & 625K & \\(-36.02\\) & \\(1.77\\) & \\(1.28\\) & \\(0.042\\) \\\\(0.042\\)\n' +
      '50\\% en + 50\\% ro-MC4 & WMT-16 en-ro & 3M & \\(-0.115.03\\) & \\(5.69\\) & \\(0.89\\) & \\(0.015\\) \\\\(0.015\\)\n' +
      '50\\% en + 50\\% ro-MC4 & WMT-16 en-ro & 312M & \\(-1.82\\times 10^{4}\\) & \\(9.04\\times 10^{2}\\) & \\(0.40\\) & \\(0.015\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 그림 1-(**top**)의 결과에 대한 BLEU 점수 법칙 \\(f(D_{p})=(\\log(A\\cdot D_{p}^{\\alpha}))^{\\beta}\\)에 대한 계수. BLEU 점수 법칙은 휴버 손실에 \\(\\delta=0.1\\)을 사용한다. 우리는 \\(A\\) 대신에 \\(\\log A\\)을 보고한다. \\(A\\)은 일반적으로 매우 작고 매우 큰 값을 취하기 때문이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c c|c} \\hline \\hline Pretraining Dataset & Finetuning Dataset & Finetuning Dataset Size & \\(E\\) & \\(A\\) & \\(\\alpha\\) & Prediction Error \\\\ \\hline\n' +
      '50\\% en + 50\\% de-MC4 & WMT-17 en-de & 6M & \\(-18.075\\) & \\(9.00\\) & \\(0.75\\) & \\(0.034\\) \\\\(0.034\\)\n' +
      '50\\% en + 50\\% de-MC4 & WMT-17 en-de & 31M & \\(-1.68\\times 10^{3}\\) & \\(84.04\\) & \\(0.49\\) & \\(0.050\\) \\\\(0.050\\)\n' +
      '50\\% en + 50\\% de-MC4 & WMT-17 en-de & 3B & \\(-1.64\\times 10^{8}\\) & \\(9.91\\times 10^{6}\\) & \\(0.19\\) & \\(0.048\\) \\\\ \\hline\n' +
      '50\\% en + 50\\% fr-MC4 & WMT-15 en-fr & 42M & \\(-1.82\\times 10^{4}\\) & \\(8.98\\times 10^{2}\\) & \\(0.42\\) & \\(0.061\\) \\\\\\(0.061\\)\n' +
      '50\\% en + 50\\% fr-MC4 & WMT-15 en-fr & 210M & \\(-2.33\\times 10^{4}\\) & \\(1.21\\times 10^{3}\\) & \\(0.40\\) & \\(0.013\\) \\\\\\(0.013\\)\n' +
      '50\\% en + 50\\% fr-MC4 & WMT-15 en-fr & 21B & \\(5.08\\times 10^{3}\\) & \\(4.61\\times 10^{8}\\) & \\(0.16\\) & \\(0.005\\) \\\\ \\hline\n' +
      '50\\% en + 50\\% ro-MC4 & WMT-16 en-ro & 625K & \\(-36.02\\) & \\(1.77\\) & \\(1.28\\) & \\(0.042\\) \\\\(0.042\\)\n' +
      '50\\% en + 50\\% ro-MC4 & WMT-16 en-ro & 3M & \\(-0.115.03\\) & \\(5.69\\) & \\(0.89\\) & \\(0.015\\) \\\\(0.015\\)\n' +
      '50\\% en + 50\\% ro-MC4 & WMT-16 en-ro & 312M & \\(-1.82\\times 10^{4}\\) & \\(9.04\\times 10^{2}\\) & \\(0.40\\) & \\(0.015\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 그림 1-(**bottom**)의 결과에 대한 _downstream_cross-entropy law \\(L(D_{p})=E+\\frac{A}{D_{p}^{\\alpha}}\\)에 대한 계수. 그림 1-(**bottom**)의 결과에 대해. Huber Loss는 _downstream_ cross-entropy 법칙의 경우 \\(\\delta=10^{-5}\\)을 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c c|c} \\hline \\hline Pretraining Dataset & Finetuning Dataset & Finetuning Dataset Size & \\(\\log A\\) & \\(\\alpha\\) & \\(\\beta\\) & Prediction Error \\\\ \\hline\n' +
      '100\\% en +MC4 & WMT-17 en-de & 6M & \\(-1.88\\) & \\(0.15\\) & \\(3.30\\) & \\(0.014\\) \\\\\\(3.30\\) & \\(0.014\\)\n' +
      '100\\% en-MC4 & WMT-17 en-de & 31M & \\(-1.81\\times 10^{4}\\) & \\(896.12\\) & \\(0.28\\) & \\(0.006\\) \\\\\\(0.006\\)\n' +
      '100\\% en-MC4 & WMT-17 en-de & 3B & \\(1.02\\times 10^{-7}\\) & \\(104.92\\) & \\(0.42\\) & \\(0.015\\) \\\\ \\hline\n' +
      '100\\% en-MC4 & WMT-15 en-fr & 42M & \\(1.00\\) & \\(2.57\\times 10^{-5}\\) & \\(1.11\\times 10^{4}\\) & \\(0.042\\) \\\\\\\n' +
      '100\\% en-MC4 & WMT-15 en-fr & 210M & \\(-6.38\\times 10^{7}\\) & \\(3.43\\times 10^{6}\\) & \\(0.20\\times 10^{6}\\)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>