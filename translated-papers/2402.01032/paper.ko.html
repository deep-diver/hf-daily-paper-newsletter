<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#Repeat After Me:\n' +
      '\n' +
      '변환기는 복사 시 상태 공간 모델보다 우수합니다.\n' +
      '\n' +
      '변환기는 복사 시 상태 공간 모델보다 우수합니다.\n' +
      '\n' +
      'Samy Jelassi\n' +
      '\n' +
      'David Brandfonbrener\n' +
      '\n' +
      '샴 M. 카카데\n' +
      '\n' +
      'Eran Malach\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '트랜스포머는 시퀀스 모델링을 위한 지배적인 아키텍처이지만, 시퀀스 길이에 의존하지 않는 고정된 크기의 잠재 상태를 사용하는 모델에 대한 관심이 증가하고 있으며, 이를 "일반화된 상태 공간 모델"(GSSMs)이라고 한다. 본 논문에서는 GSSM이 추론-시간 효율성 측면에서 유망하지만, 입력 컨텍스트에서 복사가 필요한 태스크에 대한 트랜스포머 모델에 비해 제한적이라는 것을 보여준다. 본 논문에서는 문자열 복사의 간단한 작업에 대한 이론적 분석을 통해 GSSM이 고정된 크기의 잠재 상태에 의해 근본적으로 제한되는 반면, 2계층 변압기는 지수 길이의 문자열을 복사할 수 있음을 증명한다. 경험적으로, 우리는 트랜스포머가 컨텍스트 복사를 필요로 하는 합성 작업에서 효율성과 일반화 측면에서 GSSM을 능가한다는 것을 발견했다. 마지막으로, 사전 훈련된 대용량 언어 모델을 평가하고, 트랜스포머 모델이 컨텍스트에서 정보를 복사하고 검색하는 데 있어서 상태 공간 모델을 크게 능가한다는 것을 발견한다. 이러한 결과는 변압기와 GSSM 사이의 실질적인 관심 과제에 대한 근본적인 격차를 시사한다.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '트랜스포머(Vaswani et al., 2017)는 현대적 시퀀스 모델링의 작업자로 다양한 작업에서 놀라운 성능을 달성하지만 피할 수 없는 비효율성을 가지고 있다. 구체적으로, 그들은 \\(\\Omega(L)\\) 메모리1을 필요로 하고 길이 \\(L)의 시퀀스의 다음 토큰을 예측하기 위해 계산된다. 이는 트랜스포머와 유사한 성능을 얻을 수 있는 아키텍처를 만들려는 시도에서 붐을 일으켰지만 각 토큰을 예측하기 위해 \\(O(1)\\) 메모리를 사용했다. 이러한 모델의 클래스는 S4(Gu et al., 2021) 또는 Mampa(Gu and Dao, 2023)와 같은 상태 공간 모델뿐만 아니라 전통적인 RNN 모델(Hochreiter and Schmidhuber, 1997) 및 선형 주의와 같이 병렬로 훈련될 수 있는 모델(Katharopoulos et al., 2020; Choromanski et al., 2020) 및 병렬 RNNs(Bradbury et al., 2016; Peng et al., 2023; Sun et al., 2023)을 포함한다. 본 논문에서는 고정 크기 메모리를 사용하는 이 전체 모델 클래스를 "일반화된 상태 공간 모델" 또는 GSSM(섹션 2의 공식 정의 참조)으로 참조할 것이다.\n' +
      '\n' +
      '각주 1: 변압기의 일부 순진한 구현에서는 주의를 계산하기 위해 \\(L\\times L\\) 행렬을 할당하는 것이 일반적이다. 그러나 플래시 어텐션(Dao et al., 2022)과 같은 메모리 효율적인 구현은 \\(O(L)\\) 메모리로 어텐션을 계산한다.\n' +
      '\n' +
      '최근 작업은 GSSM의 인상적인 성능을 보여주었지만 이러한 모델이 향상된 효율성을 위해 무엇을 희생하는지 아직 명확하지 않다. 본 논문에서 희생되는 하나의 특정 능력은 입력 컨텍스트의 일부를 검색하고 반복하는 능력임을 발견한다. 결과적으로, 변압기는 컨텍스트의 임의의 부분에 액세스해야 하는 다양한 작업에서 GSSM보다 우수하다.\n' +
      '\n' +
      '이러한 성능 차이를 이해하기 위해 복사 태스크 2에 대한 이론적 분석을 제시하며, 먼저 간단한 변압기 모델을 통해 변압기의 헤드 수에 지수적인 길이의 문자열을 복사할 수 있음을 보인다. 이 구성은 트랜스포머가 "저장" 및 n 토큰(n-그램)의 시퀀스의 검색을 구현하는 능력에 의존하며, 여기서 n-그램은 어디에서 복사할지를 추적하는 데 사용된다. 대조적으로, 우리는 사소하게 GSSM이 잠재 상태의 크기보다 더 많은 비트로 문자열을 정확하게 복사할 수 없음을 보여준다.\n' +
      '\n' +
      '각주 2: 입력 복사와 학습 데이터의 _not_ 복사를 연구한다는 점에 유의한다(McCoy et al., 2023; Carlini et al., 2022).\n' +
      '\n' +
      '우리의 이론은 표상 표현성을 연구하지만 이러한 표상이 학습될지는 연구하지 않는다. 더욱이, 실제로 큰 GSSM은 적어도 이론적으로 잠재 상태에서 전체 입력을 나타낼 수 있는 충분한 능력을 가질 수 있다. 이러한 문제를 해결하기 위해 우리는 \\(\\sim\\)160M 매개변수의 모델을 사용하여 다양한 합성 실험을 수행한다. 우리는 트랜스포머가 복사를 배우는 데 훨씬 더 효율적이며(그림 0(a)) 또한 더 긴 입력(그림 0(b))에 더 잘 일반화된다는 것을 발견했다. 또한, 우리는 이론적으로 구성된 것과 유사하게 변압기에 의해 학습된 복사본 "알고리즘"이 실제로 n-그램에 의존하여 복사할 위치(그림 3)의 룩업을 수행한다는 것을 실험적으로 검증한다.\n' +
      '\n' +
      '마지막으로, 입력 컨텍스트를 기억하고 액세스하는 능력을 테스트하기 위해 사전 훈련된 모델에 대한 다양한 실험을 제시한다. 특히, 우리는 피티아 트랜스포머(Biderman et al., 2023)가 문맥으로부터 정보를 복사하고 검색하는 것을 포함하는 다양한 메모리 집약적 작업에서 유사한 크기의 Mamba GSSM(Gu and Dao, 2023)을 능가한다는 것을 보여준다(도 0(c)). 이는 Mamba 모델들이 Pile 상의 언어 모델링에서 Pythia 모델들보다 더 낮은 당혹감을 달성하기 때문에 특히 주목할 만하다(Gao et al., 2020). 이러한 실험은 우리가 제기하는 메모리 문제의 실제 관련성을 보여주고 구조적 선택이 훈련 당혹감을 넘어 LLM의 다운스트림 성능에 영향을 미칠 수 있다는 한 가지 방법을 암시한다.\n' +
      '\n' +
      '##2 이론 : 표현능력\n' +
      '\n' +
      '이 섹션에서는 상태 공간 모델과 변압기의 이론적 비교를 위해 복사 작업을 사용한다. 우리는 두 가지 주요 결과를 증명한다. 먼저, 변압기의 크기가 지수인 시퀀스 길이에 대한 복사 작업을 해결하는 소형 변압기를 구성한다. 둘째, 복사 태스크를 해결하기 위해 _any_ 상태 공간 모델 _fails_가 실패하는 것을 보인다. 만약 그 잠재 상태가 시퀀스 길이에 따라 선형적으로 성장하지 않는다면.\n' +
      '\n' +
      '### Setting\n' +
      '\n' +
      '[\\(\\mathbb{D}\\)을 \\(D\\) "알파벳" 토큰을 포함하는 사전으로 하자. 시퀀스-투-시퀀스 모델은 토큰의 입력 시퀀스를 출력 시퀀스에 매핑하는 함수 \\(H:\\mathbb{D}^{*}\\rightarrow\\mathbb{D}^{*}\\)이다. 우리는 입력\\(x_{1},\\dots,x_{i}\\)을 모델에 대한 "프롬프트"로, 출력 시퀀스\\(H(x_{1},\\dots,x_{i})\\)를 생성된 "답변"으로 생각한다.\n' +
      '\n' +
      '시퀀스-토큰 매핑은 함수\\(h:\\mathbb{D}^{*}\\rightarrow\\mathbb{D}\\)이다. 모든 시퀀스 대 토큰 모델 \\(h\\)은 자동 회귀 추론을 통해 시퀀스 대 시퀀스 모델 \\(H\\)을 자연스럽게 정의한다. 즉, 모든 입력 시퀀스 \\(x_{1},\\dots,x_{i}\\in\\mathbb{D}\\)에 대해 재귀적으로 \\(x_{i+j}=h(x_{1},\\dots,x_{i+j-1})\\)을 정의하여 \\(H(x_{1},\\dots,x_{i})=(x_{i+1},x_{i+2},\\dots)\\)로 한다.\n' +
      '\n' +
      '일반화된 상태공간 모델.상태공간\\(\\mathcal{S}\\)은 유한집합이다. 우리는 \\(\\operatorname{mem}(\\mathcal{S})\\)의 상태를 부호화하는데 필요한 비트수, 즉 \\(\\operatorname{mem}(\\mathcal{S})=\\log(|\\mathcal{S}|)\\을 나타낸다. GSSM은 갱신 규칙 \\(u:\\mathcal{S}\\times\\mathbb{D}\\rightarrow\\mathcal{S}\\)과 일부 출력 함수 \\(r:\\mathcal{S}\\rightarrow\\mathbb{D}\\)에 의해 정의되는 시퀀스 모델이다. (s_{0}\\in\\mathcal{S}\\)을 초기상태로 하자. 일부 시퀀스 \\(x_{1},\\dots,x_{L}\\)이 주어지면 반복 \\(i\\)에서 모델의 상태는 \\(S_{i}(x_{1},\\dots,x_{i})\\)으로 표시되고 출력 토큰은 \\(R_{i}(x_{1},\\dots,x_{i})\\으로 표시된다. 상태와 출력은 1) \\(S_{0}(\\emptyset)=s_{0}\\), 2) \\(S_{i}(x_{1},\\dots,x_{i})=u(S_{i-1}(x_{1},\\dots,x_{i-1}),x_{i})\\), 3) \\(R_{i}(x_{1},\\dots,x_{i})=r(S_{i}(x_{1},\\dots,x_{i}))으로 재귀적으로 정의된다.\n' +
      '\n' +
      '임의의 시퀀스 모델에 대해, 1) 입력-독립 메모리(_parameters_) 및 2) 입력-의존 메모리(_activations_)의 두 가지 유형의 메모리 고려 사항이 있다는 것에 유의하는 것이 중요하다. 상기 GSSM 정의는 상기 입력 종속 메모리(_activations_)를 제약하는,\n' +
      '\n' +
      '그림 1: **(a) 복사: 훈련 효율성.** 여기서 우리는 모델을 훈련하여 길이 \\(\\leq 300\\)의 문자열을 복사하고 길이 300의 문자열에 대한 문자열 수준 정확도를 평가한다. 변압기는 GSSM보다 훨씬 빠르게 훈련한다. LSTM은 이 샘플 수 내에서 작업을 학습조차 할 수 없다. **(b) 복사: 길이 일반화.** 여기서 우리는 모든 모델이 완벽한 배포 내 모델이 될 때까지 길이 \\(\\leq 50\\)의 문자열에 복사하도록 모델을 훈련하고 문자열 수준 정확도를 평가한다. 보라색 점선은 최대 트레이닝 스트링 길이를 나타내고 녹색 점선은 트레이닝 동안 컨텍스트 윈도우를 나타낸다. 더 긴 입력에 대해 평가하면 변압기 모델이 GSSM을 극적으로 능가한다. 하드 알리바이 위치 인코딩을 사용하면 훈련 컨텍스트 크기를 훨씬 넘어 일반화할 수도 있습니다. **(c) 미리 훈련된 모델들과 함께 조회.** 여기서 작업은 완전히 맥락에 있는 다양한 길이의 "전화번호부"로부터 숫자를 조회하고 검색하는 것을 요구한다. 우리는 미세 조정 없이 사전 훈련된 모델 1-샷을 평가한다. 피티아(변압기 모델)는 모델 크기에 따라 Mamba(GSSM)보다 훨씬 우수합니다.\n' +
      '\n' +
      '은 \\(\\operatorname{mem}(\\mathcal{S})\\)에 해당하며, 입력-독립 메모리(_parameters_)의 양 또는 상태 업데이트의 런-타임을 어떠한 방식으로도 제한하지 않는다. 우리의 주요 목표는 상태 공간 메모리에 대한 하한을 보여주는 것이기 때문에 다른 모든 고려 사항을 제한 없이 두는 것은 우리의 결과를 강화시킬 뿐이다.\n' +
      '\n' +
      '변환기. 길이\\(L\\)와 차원\\(d\\)의 입력이 주어졌을 때, \\(\\mathbf{x}_{1},\\dots,\\mathbf{x}_{L}\\in\\mathbb{R}^{d}\\)로 표시되며, 주의 헤드는 \\(W_{k},W_{q},W_{v}\\in\\mathbb{R}^{d\\times d}\\으로 매개변수화된다. 우리는 \\(\\mathbf{k}_{i}=W_{k}\\mathbf{x}_{i},\\mathbf{q}_{i}=W_{q}\\mathbf{x}_{i},\\mathbf{v}_{i}=W_{v}\\mathbf{x}_{i}\\)를 나타내고, \\(K_{i}=[\\mathbf{k}_{1},\\dots,\\mathbf{k}_{i}]\\in\\mathbbb{R}^{d\\times i}\\ 및 \\(V_{i}=[\\mathbf{v}_{i},\\mathbf{v}_{i}}=W_{q}\\mathbf{x}_{i},\\mathbf{v}_{i}=W_{v}\\mathbf{x}_{i}\\ 및 \\(K_{i}=[\\mathbf{k}_{1} 우리는 토큰 \\(i\\)에서 헤드의 출력을 \\(\\mathbf{o}_{i}\\in\\mathbb{R}^{d}\\)으로 나타내며, 여기서 \\(\\mathbf{o}_{i}=V_{i}\\cdot\\operatorname{softmax}(K_{i}\\cdot\\mathbf{q}_{i})\\이다.\n' +
      '\n' +
      '변압기의 전체 치수가 \\(dl\\)이 되도록 각 차원 \\(d\\)의 주의헤드를 갖는 변압기를 고려한다. 임베딩은 mapping(\\Psi:\\mathbb{D}\\to\\mathbb{R}^{d}\\)이다. MLP는 함수 \\(f:\\mathbb{R}^{dl}\\to\\mathbb{R}^{dl}\\) s.t. \\(f(\\mathbf{x})=U_{1}\\sigma(U_{2}\\mathbf{x})\\)에 대한 함수 \\(\\sigma\\)이다. 임베딩과 MLP 레이어는 모두 토큰 레벨 상에 적용되는 것으로 가정한다. 어텐션 블록은 병렬로 적용된 \\(l\\) 헤드의 집합이고, 트랜스포머 블록은 어텐션 블록에 이어 \\(l\\) 헤드의 연결된 출력에서 동작하는 MLP이다. 모델의 출력은 최종 레이어의 출력에 기초하여 샘플링된다. 간단하게 하기 위해, 우리는 가장 가능성 있는 토큰을 예측하는 \\(\\operatorname{arg\\,max}\\) "샘플링"에 대해 연구한다.\n' +
      '\n' +
      '복사 작업을 정의하기 위해, (1) _beginning-of-sequence_ token을 \\(\\langle\\textsc{nos}\\rangle\\)로 표기하고, (2) _copy_ token을 \\(\\langle\\textsc{copy}\\rangle\\)으로 표기한다. 그래서 이제 \\(|\\mathbb{D}|=D+2\\)입니다. \\(\\mathbb{D}^{L+2}\\) 이상의 길이-\\(L\\) 복사 분포 \\(\\mathcal{D}_{L}\\)는 \\(\\left\\langle\\textsc{nos}\\right\\rangle,x_{1},\\dots,x_{L},\\langle\\textsc{copy}\\rangle\\) 형태의 문자열을 생성하는데, 여기서 \\(\\mathbf{x}\\in(\\mathbbb{D}\\setminus\\\\langle\\textsc{nos}\\rangle\\,\\langle\\textsc{ copy}\\rangle\\)^{L}\\)은 \\(\\left\\langle\\textsc{nos}\\right\\rangle,x_{1},\\dots,x_{L},\\langle\\textsc{copy}\\rangle\\)의 문자열을 생성한다.\n' +
      '\n' +
      '일부 시퀀스-투-시퀀스 모델 \\(H:\\mathbb{D}^{*}\\to\\mathbb{D}^{*}\\)에 대해, 우리는 복사 분포 \\(\\mathcal{D}_{L}\\)에 대한 \\(H\\)의 오차를 나타낸다.\n' +
      '\n' +
      '\\[\\operatorname{err}_{\\mathcal{D}_{L}(H)=\\Pr_{\\mathcal{D}_{L}[H_{1:L}(\\left \\langle\\textsc{nos}\\right\\rangle,\\mathbf{x},\\left\\langle\\textsc{copy}\\right\\rangle}\\neq\\mathbf{x}]\\]\n' +
      '\n' +
      '여기서 \\(H_{1:L}(\\cdot)\\)는 \\(H\\)에 의해 생성된 첫 번째 \\(L\\) 토큰을 나타낸다. 즉, 우리는 모델이 \\(\\mathbf{x}\\)의 정확한 복사본을 출력할 것으로 기대한다.\n' +
      '\n' +
      '### 변압기는 지수 길이의 입력을 복사할 수 있음\n' +
      '\n' +
      '이 섹션에서는 변압기가 헤드 수에 길이 지수 값을 갖는 입력 시퀀스에 대한 복사 작업을 구현할 수 있음을 보여준다. 즉, 복사 작업에 작은 오류가 발생하는 두 개의 블록을 갖는 트랜스포머를 구성한다.\n' +
      '\n' +
      '구축: 해시 기반 복사.구축의 핵심 아이디어는 먼저 \\(n\\) 토큰(\\(n\\)-그램)의 시퀀스를 "해시"한 다음 자동 회귀의 각 반복에서 가장 최근의 \\(n\\)-그램의 이전 발생에 참석하고 후속 토큰을 출력하는 것이다. 즉, 우리는 변압기가 그림 3에 예시된 복사 알고리즘을 구현할 수 있음을 보여준다(그리고 부록의 알고리즘 1도 참조).\n' +
      '\n' +
      '위치 임베딩: Hard-ALBi. 알고리즘에 설명된 해싱을 수행하기 위해서는 로컬 위치 정보를 활용하여 해시를 정의하고, 이 해시 함수를 전체 입력에 전역적으로 적용할 수 있어야 한다. 이를 위해, 우리는 ALBi(Press et al., 2021)의 하드 버전을 사용하는데, 이는 우리가 _Hard-ALBi_라고 부른다. ALBi에서와 같이, 우리는 \\(i\\)번째 주의 머리에 바이어스 \\(b_{i}\\)을 추가한다. \\(\\mathbf{o}_{i}=V_{i}\\cdot\\operatorname{softmax}(K_{i}\\cdot\\mathbf{q}_{i}+b_{i})\\. 구체적으로 \\(j\\leq i-m\\)의 경우 \\(b_{i}\\) s.t. \\(b_{i,j}=-\\infty\\)을, \\(j>i-m\\)의 경우 \\(b_{i,j}=0\\)을 설정하였다. 우리는 \\(m\\)의 다른 선택을 가진 서로 다른 헤드를 허용하고 또한 위치 임베딩 없이 소프트맥스 주의에 해당하는 \\(m=\\infty\\)을 허용한다. 이를 도 5(c)(부록)에 도시하였다. Hard-ALBi는 우리의 이론적 구성을 위해 도입되지만 섹션 3에서 논의된 바와 같이 경험적으로 상당한 이점을 제공한다는 것을 관찰한다.\n' +
      '\n' +
      '알고리즘 1에 제시된 복사 알고리즘(마찬가지로, 변압기 구성)은 입력에 반복되는 \\(n\\)-그램 패턴이 없는 한 입력 시퀀스를 완벽하게 복사할 수 있다. 따라서, 알고리즘의 에러는 반복된 \\(n\\)-그램의 확률에 의존한다:\n' +
      '\n' +
      '**Definition 2.2**. : \\(\\mathcal{D}_{L}\\)을 복사본 분포로 하자. 어떤 \\(n\\in\\mathbb{N}\\)에 대해 \\(p_{\\mathrm{n-gram}}(\\mathcal{D}_{L})\\)은 \\(x_{1},\\dots,x_{L}\\) 토큰의 두 개의 반복되는 시퀀스를 포함할 확률이다. 즉:\n' +
      '\n' +
      '\\[p_{\\mathrm{n-gram}}(\\mathcal{D}_{L})=\\Pr_{\\mathcal{D}_{L}}[\\exists_{i\\neq j}\\operatorname{s.t.}x_{i},\\dots x_{i+n}=x_{j},\\dots,x_{j+n}]\\exists_{i\\neq j}\\operatorname{s.t.t.x_{i},\\dots,x_{j+n}]\\exexss_{n-gram}(\\mathcal{D}_{L})=\\Pr_{\\mathcal{D}_{L}}[\\exists_{i\\neq j}\\operatorname{s.t.x_{i},\\dots,x_{j+n}}]\\exexss_{n-gram}(\\mathcal{L})=\\Pr_{\\mathcal{D}_{L}}[\\exists_{i\\neq j}\n' +
      '\n' +
      '그림 3: \\(n\\)-그램 기반 복사 알고리즘의 그림이다. 다음 토큰을 예측하기 위해 입력에서 현재 \\(n\\)-그램과 대응하는 \\(n\\)-그램을 매칭한 후 다음 토큰을 출력한다.\n' +
      '\n' +
      '그림 2: 복사 작업의 그림입니다.\n' +
      '\n' +
      '아래에서는 변압기를 사용한 복사에 대한 주요 이론적 결과를 설명하며, 변압기는 반복된 \\(n\\)-그램의 확률에 의해 제한된 오차로 입력을 복사할 수 있음을 보여준다.\n' +
      '\n' +
      '**정리 2.3**: 모든 \\(n\\)에 대해, 모든 \\(2n\\leq L\\leq D^{n\\)에 대해, 그리고 모든 복사 분포에 대해 \\(\\mathcal{D}_{L}\\), \\(\\operatorname{err}_{\\mathcal{D}_{L}}(\\mathcal{T})<p_{\\mathrm{n-gram}}(\\mathcal{D}_{L})\\).\n' +
      '\n' +
      '직관적으로 \\(n\\)-그램이 반복될 확률은 \\(n\\)의 값을 증가시킬 때 빠르게 감소한다. 실제로, 우리는 수열에 대한 균일한 분포에 대해 이 확률이 \\(n\\)으로 _지수적으로 감소한다는 것을 보여준다:\n' +
      '\n' +
      '**Lemma 2.4**.: _Let \\(\\mathcal{D}_{L}\\)는 "알파벳"(비특수) 토큰에 대한 균일 분포로부터 \\(\\mathbf{x}\\)을 샘플링하여 생성된 복사 분포이다. 그리고 \\(p_{\\mathrm{n-gram}}(\\mathcal{D}_{L})<L^{2}D^{-n}\\)._\n' +
      '\n' +
      '위의 결과를 종합하면, 변환기는 입력 시퀀스 길이에 _logarithmically_에만 의존하는 다수의 파라미터를 사용하여 균일한 분포로부터 도출된 토큰의 시퀀스를 복사할 수 있음을 알 수 있다.\n' +
      '\n' +
      '**관상 2.5**.: \\(\\epsilon\\in(0,1/2)\\)과 \\(L\\geq\\Omega(\\log(1/\\epsilon))\\을 수정한다. 균일한 복사분포에 대한 깊이-2 변환기의 차원\\(\\mathcal{T}\\(O(\\log(L/\\epsilon)\\log(D))\\) s.t.\n' +
      '\n' +
      '2.6_Remark 2.6_.: 단순화를 위해 매개변수나 활성화의 정밀도를 제한하지 않지만, \\(O(\\log(L)))) 비트를 사용하여 유한 정밀 변압기에 대해 우리의 결과가 유지된다는 점에 유의한다.\n' +
      '\n' +
      '####### 상태 공간 모델은 메모리 크기 이상의 입력을 복사할 수 없음\n' +
      '\n' +
      '우리는 트랜스포머가 시퀀스 길이에서 매개변수 카운트 _로그ic_와 함께 토큰의 균일한 시퀀스를 복사할 수 있음을 보았다. 우리는 이제 GSSMs _cannot_가 균일한 입력 시퀀스를 복사한다는 것을 보여주는데, 그들의 상태 공간의 용량이 시퀀스 길이의 크기에 따라 _linearly_ 증가하지 않는 한이다. 이것은 직관적이다: 전체 입력 시퀀스를 복사할 수 있기 위해, 모델은 그것을 상태 공간에 저장할 필요가 있으며, 이는 메모리가 시퀀스 길이에 따라 선형적으로 성장해야 한다.\n' +
      '\n' +
      '**정리 2.7**.: 일부 GSSM\\(H\\)을 상태공간\\(\\mathcal{S}\\) 위에 고정한다. 그 다음, 모든 \\(L\\)에 대해, 균일 복사 분포 \\(\\mathcal{D}_{L}\\)에 대해, 모델 \\(H\\)은 오차 \\(\\operatorname{err}_{\\mathcal{D}_{L}}(H)>1-\\frac{|\\mathcal{S}|}{D^{L}}\\)을 갖는다.\n' +
      '\n' +
      '정리 2.7을 고려할 때, 다음의 Corollary는 즉시이다:\n' +
      '\n' +
      '**Corollary 2.8**. : _Fix some \\(L\\in\\mathbb{N}\\) 그런 다음 상태 공간을 갖는 모든 GSSM\\(H\\)\\(\\mathcal{S}\\) s.t.\\(\\operatorname{mem}(\\mathcal{S})<L\\log(D)-1\\)은 균일 복사 분포\\(\\mathcal{D}_{L}\\(\\operatorname{err}_{\\mathcal{D}_{L}}(H)>1/2\\)에 대한 오차\\(\\operatorname{err}_{\\mathcal{D}_{L}(H)>1/2\\)을 갖는다.\n' +
      '\n' +
      '_Remark 2.9_.: 앞서 언급한 바와 같이, 변압기의 입력-의존성 메모리는 시퀀스 길이에 따라 선형적으로 성장하며, 이는 GSSM에 비해 _less_ 메모리-효율적이다. 그러나, 상기 결과로부터, 적어도 복사 작업에 대해, 변압기는 그들의 입력-의존 메모리 측면에서 _거의 최적_임을 주목하는 것이 흥미롭다. 보다 구체적으로 정리 2.3의 함의는 \\(\\tilde{O}(L)\\) 입력 종속 메모리3를 사용하여 길이 \\(L\\)의 입력을 복사할 수 있는 변압기가 존재한다는 것이며, Corollary 2.8로 인해 이것은 실제로 최적(대수 인자까지)이다.\n' +
      '\n' +
      '각주 3: 로그 인자를 숨기기 위해 \\(\\tilde{O}\\)을 사용한다.\n' +
      '\n' +
      '##3 복사를 배우는 단계\n' +
      '\n' +
      '이전 섹션에서는 변압기가 기하급수적으로 긴 시퀀스에 대한 복사 작업을 나타낼 수 있는 반면 GSSM은 제한된 메모리로 인해 긴 시퀀스를 복사하지 못한다는 것을 입증했다. 이러한 결과는 이론적으로 변압기가 GSSM을 능가할 수 있음을 보여주지만, 우리의 이론적 결과는 두 가지 이유로 실제로 그러한 격차가 관찰될 것이라는 것을 입증하지 못한다. 첫째, 트랜스포머가 실제로 예제로부터 복사할 수 있다는 것은 분명하지 않다. 둘째, GSSM은 실제로 큰 잠재 상태 메모리를 사용할 수 있으므로, 우리의 바운드는 토큰의 매우 긴 시퀀스에 대해서만 유지된다. 예를 들어, 1000개의 32비트 부동 소수점 번호의 잠재 상태는 50K 토큰 어휘로부터 적어도 2000개의 토큰을 저장하기에 충분한 비트를 갖는다. 그러나 GSSM은 컨텍스트를 메모리에 맞출 수 있지만 그렇게 하는 방법을 배우지 못할 수 있다.\n' +
      '\n' +
      '이 섹션의 목표는 다음 섹션에서 사전 훈련된 모델을 연구하기 전에 합성 데이터에 대해 처음부터 모델을 훈련할 때 이론적 분석이 실험적으로 입증되는지 확인하는 것이다. 구체적으로, 우리는 그림 2에 표시된 복사 작업의 변형에 대해 트랜스포머 및 GSSM(LSTM(Hochreiter & Schmidhuber, 1997) 및 맘보(Gu & Dao, 2023))을 훈련한다.\n' +
      '\n' +
      '### Experimental setup\n' +
      '\n' +
      '이제 실험 설정에 대한 간략한 개요를 제공합니다. 자세한 내용은 부록 A에서 확인할 수 있다.\n' +
      '\n' +
      '구조.모든 실험에서 Mambo와 변압기가 유사한 수의 매개변수(\\(\\approx 160\\) 백만 매개변수)를 갖도록 모델 하이퍼파라미터를 설정했다. 우리는 큰 LSTM이 (Pascanu et al. (2013)에서 확인된 바와 같이) 훈련하기 어렵다는 것을 발견했기 때문에, 우리는 \\(\\approx 40\\)백만 매개 변수를 가진 훈련에 성공한 가장 큰 LSTM을 사용한다.\n' +
      '\n' +
      '데이터세트.훈련 중에, 우리는 각 에폭에서 64개의 예시들의 배치를 온라인 방식으로 생성한다. 실험 시간에, 우리는 \\(128\\) 예제의 10\\ 배치에서 모델을 평가한다. 우리는 이 10개 배치에 대한 평균 및 표준 편차를 보고한다. 달리 명시되지 않은 경우, 우리의 토큰 공간 \\(\\mathcal{V}\\)은 크기 30이고 알파벳 문자 즉 \\(\\mathcal{V}=\\{a,\\ldots,z,\\left\\langle\\mathrm{aos}\\right\\rangle,\\left\\langle\\mathrm{cos}\\right\\rangle,\\left\\langle\\mathrm{cov}\\right\\rangle\\\\)로 만들어지며, 여기서 \\(\\left\\langle\\mathrm{nos}\\right\\rangle\\)은 문장 토큰의 시작이고 \\(\\left\\langle\\mathrm{nos}\\right\\rangle\\)은 문장 토큰의 끝이고 \\(\\left\\langle\\mathrm{cov}\\right\\rangle\\)은 구분자 토큰이다. 모든 스트링들은 균일하게 샘플링된다. 즉, 먼저 시퀀스의 길이를 샘플링하고, 이어서 스트링의 각 위치를 \\(\\mathcal{V}\\)으로부터 독립적으로 샘플링한다. 마지막으로, 우리는 (Zhou et al., 2023)과 유사하게 트레이닝 동안 i.i.d. 시퀀스들로 컨텍스트를 "pack the context"한다: 우리는 태스크의 다수의 독립적인 샘플들로 컨텍스트를 채운다.\n' +
      '\n' +
      '위치 정보.위치 정보는 트랜스포머의 길이 일반화 용량에도 중요한 역할을 한다(Jelassi et al., 2023; Kazemnejad et al., 2023; Shen et al., 2023). 입력-레이어 위치 임베딩의 이전에 인기 있는 방법들(예를 들어, 정현파(Vaswani et al., 2017) 또는 학습된(Radford et al., 2019))은 각각의 어텐션 층에서의 상대적 위치 인코딩으로 대체되었다(예를 들어, RoPE(Su et al., 2023), 알리비(Press et al., 2021), 또는 NoPE(Kazemnejad et al., 2023). 아래에서는 섹션 2에 도입된 Hard-Alibi 인코딩과 함께 이러한 위치 인코딩을 실험한다.\n' +
      '\n' +
      '### 복사 작업의 데이터 효율성\n' +
      '\n' +
      '우리는 그림 2에 기술된 일련의 입력 토큰을 복사하는 간단한 작업에 대해 모델을 훈련시키는 것으로 시작한다. 모델은 \\(\\leq L\\) 토큰에 이어 _Separation_(\\(\\langle\\mathrm{copy}\\rangle\\)) 토큰이 입력되며, 처음부터 다시 동일한 시퀀스를 출력해야 한다. 이 절에서는 랜덤 길이 \\(\\leq L=300\\)의 문자열을 학습하고 학습 분포에서 샘플링된 평가 문자열에 문자열 수준의 정확도를 기록한다.\n' +
      '\n' +
      '이 실험에 대한 결과는 그림 1a에 나와 있다. 분명히, 변압기와 GSSM 사이에는 큰 격차가 있다. 우리는 변압기가 복사 작업을 배우기 위해 최고의 GSSM보다 100배 적은 샘플이 필요하다는 것을 관찰한다.\n' +
      '\n' +
      '도 1a에 표시된 정확도의 급격한 변화는 로그-스케일링된 x-축 및 메트릭으로서 스트링-레벨 정확도의 선택으로 인한 것임을 유의한다. 그림 9a에서 우리는 GSSM의 학습 과정을 보여주는 더 부드러운 곡선을 생성하는 문자 수준 정확도를 보고한다. LSTM과 관련하여, 우리는 그들이 문자 수준에서도 길이-300 문자열에 대해 학습하지 못한다는 것을 발견했다. 그림 9b에서 우리는 LSTM이 더 짧은 문자열에서 복사하는 법을 배울 수 있고 문자열 길이가 병목 현상임을 보여준다.\n' +
      '\n' +
      '### 복사 작업에 대한 길이 일반화\n' +
      '\n' +
      '사전 실험은 배부 내 학습의 우수한 효율성을 보여준다. 이제 배분을 일반화하기 위해 학습된 기능의 능력을 테스트합니다. 특히, 우리는 짧은 서열에서 더 긴 서열로의 일반화를 고려한다. 이러한 종류의 일반화를 테스트하는 것은 모델이 어떤 기능을 학습했는지, 즉 모델이 "올바른" 복사 작업을 진정으로 학습했는지 또는 훈련된 특정 크기의 시퀀스를 복사하는 것을 방금 학습했는지 여부를 더 잘 이해하는 데 도움이 될 수 있다.\n' +
      '\n' +
      '여기서는 모든 모델을 \\(\\leq 50\\) 토큰의 시퀀스에 대해 학습하고, 최대 \\(1000\\) 토큰의 시퀀스에 대해 테스트하여 문자열 수준의 정확도를 보고한다. 그림 1b에서 볼 수 있듯이 모든 모델은 \\(\\leq 50\\)의 길이에 대한 작업 분포를 (결국) 해결할 수 있지만 변압기 기반 모델은 GSSM에 비해 더 긴 입력에 대해 훨씬 더 나은 일반화를 보여준다. 즉, 입력 길이를 증가시키면 GSSMs(LSTM과 MAMBA)의 성능이 거의 즉시 0으로 떨어지는 반면, 변압기의 성능은 길이에 따라 훨씬 더 서서히 저하되는 것을 관찰할 수 있다.\n' +
      '\n' +
      '위치 정보.그림 1b에서 서로 다른 변압기 모델의 상대적 성능을 살펴보면, 길이 일반화에 위치 부호화가 중요하다는 것이 명확해진다. 특히, ALiBi 및 NoPE 변압기는 더 긴 입력에서 RoPE 모델을 극적으로 능가한다. 이것은 RoPE의 사인파 임베딩이 우리가 더 긴 입력으로 갈 때 ALiBi 또는 NoPE의 붕괴보다 더 극적인 변화를 만들기 때문일 수 있다.\n' +
      '\n' +
      'Hard-ALiBi를 사용한 일반화 개선.트랜스포머가 복사를 배우는 방법에 대한 이해를 테스트하기 위해 이제 해시 기반 복사의 이론적 구성에 사용된 Hard-ALiBi 위치 인코딩에서 스와핑을 고려한다(부록의 하위 섹션 2.2 및 그림 8에 도입). 도 1b는 길이\\(\\leq 50\\)의 시퀀스들에 Hard-ALiBi 임베딩을 통해 트레이닝된 트랜스포머가 길이 1000의 시퀀스들까지 거의 완벽한 길이 일반화를 달성함을 보여준다. 이것은 트레이닝에서 마주친 컨텍스트 길이를 훨씬 넘어선다는 것에 유의한다.\n' +
      '\n' +
      '### 트랜스포머는 n-gram 해싱을 사용하는 법을 배웁니다.\n' +
      '\n' +
      '다음으로, 복사 작업에 대해 훈련된 변압기가 n-그램의 저장 및 검색 메커니즘을 실제로 적용하는지 여부를 결정하려고 한다. 이를 위해, 의도적으로 중복된 n-그램을 포함하는 예제의 분포에 대해 테스트할 때 복사 작업에 대해 훈련된 Hard-ALiBi 위치 인코딩을 사용하여 변압기의 성능을 평가한다. 즉, 우리는 토큰들의 균일한 시퀀스들을 그린 다음, 일부 n-그램을 시퀀스에 이미 나타나는 다른 n-그램으로 랜덤하게 교체하여, 각각의 n-그램이\n' +
      '\n' +
      '그림 4: n-그램이 복제된 데이터에 대한 문자열 수준 복사 정확도. 모델이 더 이상 n-그램 룩업을 수행할 수 없는 동안 복제된 n-그램이 너무 길면 복사가 실패합니다.\n' +
      '\n' +
      '예제에는 항상 동일한 n-그램의 두 개의 사본이 포함되어 있습니다. 일반적으로 다른 토큰이 뒤따릅니다. 그림 0(a)와 같이 복사 작업에 가장 좋은 성능을 보이기 때문에 여기서는 Hard-Alibi 모델을 사용한다. 그림 4는 \\(n\\)의 다양한 선택에 대한 변압기의 성능을 보여준다. 변압기는 \\(n\\leq 4\\)에 대해 거의 동일한 정확도를 유지하지만, 입력들이 5개 이상의 토큰들의 중복 시퀀스를 포함할 때 그 정확도가 떨어지기 시작하는 것을 관찰한다. 이는 변압기가 복사 작업을 수행하기 위해 5그램 검색과 같은 것에 의존함을 시사한다.\n' +
      '\n' +
      '### GSSM은 문맥에서 임의로 검색할 수 없음\n' +
      '\n' +
      '이제 모델이 컨텍스트에서 복사하는 데 사용하는 메커니즘을 조사하기 위한 또 다른 작업인 _n-그램 룩업_ 작업을 소개한다. 이 작업에서 모델은 쿼리를 따르는 k-토큰 키를 조회하기 위해 주어진 n-그램을 키로 사용할 필요가 있다. 우리는 태스크의 두 가지 변형인 _suffix key_와 _prefix key_를 고려한다. 두 변형 모두에서 모델이 학습한 함수를 이해하기 위해 길이 일반화를 평가한다.\n' +
      '\n' +
      '먼저, n-gram lookup의 접미사 키 버전을 고려한다. 이 작업에서 모델은 입력 토큰의 시퀀스\\(L\\), 구분자 및 입력 시퀀스로부터 n-그램을 부여받는다. 그런 다음 모델은 선택한 n-그램에 따라 \\(k\\) 토큰의 시퀀스를 출력해야 한다(그림 5 참조). 이 과제는 인덕션 헤드(Olsson et al., 2022)와 밀접한 관련이 있다. 이 작업은 모델이 쿼리에 액세스할 올바른 키를 효과적으로 찾기 위해 전체 컨텍스트를 "저장"할 수 있어야 합니다. 최대 30개의 토큰의 시퀀스에 대해 모든 모델을 학습하고 그림 5에서 결과를 보여준다. 이 작업에서 변압기는 시퀀스 길이를 최대 100까지 증가시킬 때 상대적으로 적은 성능 저하로 잘 수행되며, 이는 변압기가 n-그램 저장 및 검색을 수행하는 방법을 배울 수 있음을 시사한다. 그러나 GSSM은 훈련 분배를 넘어 성능이 좋지 않다. 직관적으로 이 작업은 여전히 모델이 GSSM이 수행하기 어려운 전체 입력 시퀀스를 저장해야 한다.\n' +
      '\n' +
      '다음으로, n-그램 룩업의 프리픽스 키 버전을 시도한다. 여기서 우리는 처음에 n-gram 키를 제공하고 그 다음에 전체 입력 시퀀스를 제공한다(도 6에 도시됨). 이 버전의 작업에서 모델은 시퀀스가 처리됨에 따라 즉석에서 키를 찾을 수 있기 때문에 전체 입력을 저장할 필요가 없다. 이것은 GSSM들이 키를 스테이트에 기입한 다음 일치하지 않는 입력들을 무시할 수 있기 때문에, GSSM들에 좋다. 실제로, GSSM은 이 변형에서 완벽한 길이 일반화를 달성한다. 흥미롭게도, GSSM은 NoPE 및 ALIBi 변압기를 능가한다(Hard-Alibi 모델은 아니지만). 이러한 위치 임베딩이 상대 위치에서 장거리 해싱 룩업을 효과적으로 수행하는 것을 더 어렵게 만드는 문제일 수 있다고 가정한다. 종합하면, 이러한 결과는 GSSM이 메모리가 제한된 것처럼 보이지만 태스크가 전체 컨텍스트를 저장하는 것보다 입력의 요약만 요구할 때 효과적일 수 있음을 보여준다.\n' +
      '\n' +
      '도 5: **Top**: n-gram 룩업 태스크의 접미사 키 변형의 예시. **Bottom:** 길이\\(\\leq 30\\)의 문자열로 훈련될 때 변압기는 더 긴 입력에서 GSSM을 능가하여 메모리 집약적인 작업에서 우수한 성능을 보여준다.\n' +
      '\n' +
      '도 6: **Top**: n-gram 룩업 태스크의 프리픽스 키 변형의 예시. **Bottom:** 길이 \\(\\leq 30\\)의 문자열로 훈련될 때, GSSM은 하드-알리바이 트랜스포머 뿐만 아니라 다른 트랜스포머보다 성능이 우수하다. 작업의 이러한 약간의 변형은 훨씬 적은 메모리를 필요로 하고 따라서 시간이 지남에 따라 작은 상태를 저장할 때 GSSM의 강점에 더 적합하다.\n' +
      '\n' +
      '##4 사전 훈련된 모델\n' +
      '\n' +
      '이 섹션에서는 긴 문자열 복사, 검색 및 몇 개의 샷 질문 응답과 같은 메모리 집약적 작업에 대해 사전 훈련된 변압기와 사전 훈련된 GSSM의 성능을 비교한다. 이러한 메모리 집약적인 작업에서 변환기는 언어 모델로서 복잡도가 낮은 경우에도 유사한 규모의 GSSM을 능가한다는 것을 보여준다. 이러한 결과는 이전 섹션에서 제기된 GSSM의 한계가 실제 사전 훈련 데이터에 대해 훈련된 대규모 모델에 적용됨을 확인시켜준다.\n' +
      '\n' +
      '### Setup\n' +
      '\n' +
      '아래 실험에서 우리는 410M에서 2.8B 범위의 크기의 피티아 트랜스포머 모델(Biderman et al., 2023)과 유사한 크기의 Mamba 모델(Gu & Dao, 2023)을 비교한다. 이 모든 모델들은 Pile(Gao et al., 2020)에서 사전 트레이닝되어 동일한 토큰타이저를 사용하고 있다. 맘바 모델들은 일반적으로 주어진 크기에 대한 트레이닝 세트에서 약간 더 낮은 당혹감을 갖는다. 피티아와 맘바 모델의 주요 차이점은 건축 디자인입니다.\n' +
      '\n' +
      '이 모델들은 입력 인스턴스 길이를 변화시키면서 성능을 측정하여 비교하였으며, 복사 기반 태스크와 정보 검색 태스크의 두 가지 유형을 고려하였다. 복사 기반 작업은 랜덤 텍스트를 모델에 제시하고 텍스트를 복사하도록 요청하는 것으로 구성된다. 정보 검색 작업에서 우리는 모델에 텍스트를 제공하고 관련 질문을 한다. 이러한 검색 작업은 모델에 질문에 응답하기 위해 입력 텍스트의 작은 청크를 복사할 필요가 있기 때문에 "선택적 복사"로 볼 수 있다. 성능을 측정하기 위해 질문 응답을 고려하여 F1 점수를 보고하는 그림 6(c)를 제외한 모든 실험에서 문자열 수준 정확도를 사용한다. 주어진 문맥 길이를 가진 질문의 수가 제한되어 있기 때문에 50개 이상의 질문을 평가하는 질문 응답을 제외한 모든 작업에 대해 크기 64의 10개 배치 이상의 모델을 평가한다. 자세한 내용은 부록 A에 나와 있다.\n' +
      '\n' +
      '###입력 컨텍스트 복사\n' +
      '\n' +
      '우리는 먼저 미리 훈련된 변압기가 긴 자연어 문자열을 복사할 때 미리 훈련된 GSSM을 능가한다는 것을 관찰한다. 그림 6(a)에서 우리는 다양한 수의 토큰으로 C4 데이터세트(Raffel et al., 2020)에서 무작위로 문자열을 샘플링한다. 우리의 프롬프트는 샘플링된 문자열의 두 사본과 문자열의 첫 번째 단어로 구성되며 모델이 세 번째 사본을 완료할 것으로 예상한다. 가장 작은 변압기 모델조차도 가장 큰 GSSM을 극적으로 능가한다. 이는 큰 GSSM이 컨텍스트를 잠재적으로 저장할 수 있는 충분한 비트를 상태 변수에 가지고 있음에도 불구하고 발생한다. 이는 트랜스포머가 문맥에서 복사하기 쉽게 만드는 트랜스포머의 구조적 편향이라는 생각을 확인시켜준다.\n' +
      '\n' +
      '임의로 균일하게 샘플링된 토큰들의 스트링들과 달리, 자연 텍스트는 종종 압축될 수 있으며, 이는 언어 모델들이 제한된 메모리로도 더 긴 스트링들을 복사할 수 있게 한다. 이것이 중요한지 테스트하기 위해 그림 6(b)에서 위와 같은 실험을 수행하지만 문자열에서 단어의 순서를 무작위로 섞는다. 우리는 단어를 섞을 때 GSSM과 변압기가 모두 작업을 더 나쁘게 수행하지만 그 효과는 GSSM에 대해 더 극명하다는 것을 발견했다. 최대 GSSM은 현재 길이 300의 문자열에 대해 0의 정확도를 얻는데, 이는 입력이 압축이 더 어려울 때 고정된 크기 상태로 인해 GSSM이 어려움을 겪는다는 것을 암시한다.\n' +
      '\n' +
      '### 입력 문맥으로부터의 검색\n' +
      '\n' +
      '복사는 모델 클래스를 분리하기 위한 명확한 작업을 제공하지만, 특별히 현실적인 작업은 아니다. 즉, 그것은 많은 관심 과제와 관련성이 높은 행동 유형의 극단적인 사례를 제시한다. 특히, 많은 작업들은 원하는 출력과 관련된 컨텍스트로부터 특정 정보를 검색하는 것을 요구한다. 이 하위 섹션에서는 결과가 보다 실용적인 과제로 어떻게 전달되는지에 대한 예를 제시한다.\n' +
      '\n' +
      '전화번호부 조회.우리는 먼저 모델에 합성전화번호부를 제공하는 "전화번호부" 실험을 고려하고 이름이 주어졌을 때 전화번호를 반환하도록 요청한다. 우리는 임의로 \\(L\\)의 이름과 연관된 전화번호를 샘플링하여 폰북을 생성한다. 이 전화번호부의 한 줄은 "John Powell: 609-323-7777"처럼 보인다. 모델에 대한 프롬프트는 전화책, 두 개의 몇 개의 예제 및 전화책에서 무작위로 샘플링된 이름의 전화 번호를 묻는 질문으로 구성된다. 그림 9(c)는 폰북의 크기를 변화시키면서 사전 훈련된 변압기와 GSSM에서 얻은 정확도를 보고한다. 폰북의 크기가 충분히 길었을 때 가장 작은 트랜스포머(410M 파라미터)도 가장 큰 GSSMs(2.8B 파라미터)보다 우수한 성능을 보였다 (\\(L\\geq 70\\)). 이는 전체 컨텍스트에 대한 액세스가 필요한 검색 작업에서 GSSM이 관련 정보를 고정 크기 상태로 저장하는 데 어려움을 겪는다는 것을 보여준다.\n' +
      '\n' +
      'Question-Answering.본 실험에서는 SQuAD question-answering dataset (Rajpurkar et al., 2018)에서 2.8B parameter Mamba와 transformer model4를 비교하였다. 이 데이터 세트는 텍스트에 대한 몇 가지 질문과 함께 텍스트 단락을 제공한다. 우리는 대상 질문을 제공하기 전에 질문/답변 쌍(동일한 텍스트에 해당)의 단일 데모를 제공하여 질문에 답하기 위해 모델을 조사한다. 우리는 문단을 길이에 따라 비닝하고 그림 6(c)의 두 모델에 대해 문단 길이의 함수로 F1 점수를 보고한다. 짧은 단락의 경우 피티아 변압기와 맘바가 모두 유사한 성능을 달성하지만, 단락 길이에 따라 맘바의 성능이 더 빠르게 저하되는 반면, 변압기 기반 모델은 더 긴 텍스트에서도 유사한 정확도를 유지한다는 것을 관찰한다. 이 결과는 GSSM의 고정 메모리가 표준 자연 작업에 대한 성능을 제한한다는 것을 보여준다.\n' +
      '\n' +
      '## 5 관련 업무\n' +
      '\n' +
      'RNNs(Merrill, 2019; Merrill et al., 2020) 뿐만 아니라 변압기(Weiss et al., 2021; Merrill et al., 2022; Wei et al., 2022; Sanford et al., 2023; Edelman et al., 2022)와 같은 GSSM의 표현 능력에 대한 광범위한 선행 연구가 존재한다. 역치 회로(Merrill et al., 2022), RASP 언어(Weiss et al., 2021) 또는 1차 논리(Chiang et al., 2023)와 같은 다른 복잡성 클래스와의 비교를 통해 변압기를 연구하는 이전 작업(철저한 검토를 위해 Strobl et al.(2023) 참조). 이러한 작업은 변압기가 특정 문제를 해결하기 위한 알고리즘을 구현하는 방법에 대한 통찰력을 제공하지 않는다. 이와는 대조적으로, 우리의 이론적 결과는 복사 작업을 위한 변압기를 구성하는데, 이는 메커니즘을 설명하고 모델 크기에 엄격한 경계를 제공한다. GSSM이 긴 서열을 복사할 수 없다는 결과와 함께, 우리의 이론은 복사 작업에 대한 다른 서열 모델의 힘을 특성화한다. 트랜스포머와 RNN 사이의 다른 이론적 분리 결과(Sanford et al., 2023; Merrill, 2019)는 덜 실용적인 관련성이 있는 더 복잡한 작업을 사용한다.\n' +
      '\n' +
      '다른 논문은 이전에 검색, 질문 응답 및 상황 내 학습과 같은 작업에 대해 전체 입력 컨텍스트를 활용하는 트랜스포머의 능력을 입증했다(Devlin et al., 2018; Raffel et al., 2020; Petroni et al., 2020; Brown et al., 2020; Liu et al., 2023). 또 다른 작업 라인은 우리가 복사를 위해 관찰한 것과 유사한 검색 작업을 수행하는 트랜스포머에서 "유도 헤드" 메커니즘을 연구했다(Olsson et al., 2022). 그러나 우리가 아는 한 이러한 작업에서 변압기와 유사한 품질의 GSSM 간의 관련 작업에는 비교가 없다.\n' +
      '\n' +
      '우리의 여러 실험은 모델이 과제를 해결하는 "올바른 방법"을 찾았는지 여부를 평가하는 방법으로 길이 일반화를 연구한다. 변압기에서의 길이 일반화에 관한 선행 작업은 데이터 분포(Anil et al., 2022), 위치 임베딩(Kazemnejad et al., 2023), 및 산술 작업(Deletang et al., 2022; Ruoss et al., 2023; Jelassi et al., 2023; Zhou et al., 2023)에 집중되어 왔다. 우리는 이 아이디어들 중 많은 것을 복사 작업으로 확장한다.\n' +
      '\n' +
      '마지막으로, 우리는 변압기가 GSSM을 능가하는 작업에 초점을 맞추지만, GSSM이 변압기를 능가하는 작업도 있다는 점에 주목한다. 예를 들어, Liu et al.(2023)은 트랜스포머가 "플립-플롭 언어 모델링"을 위해 분배를 벗어나 일반화하는 데 실패하는 반면 LSTM은 그렇게 쉽게 한다는 것을 보여준다. 이러한 작업은 시간이 지남에 따라 작은 \\(O(1)\\) 상태 변수를 추적해야 한다. GSSM의 또 다른 이점은 트랜스포머에 비실용적일 수 있는 DNA 서열과 같은 긴 컨텍스트를 입력하는 능력이다(Nguyen et al., 2023).\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      '우리는 이론과 실험을 통해 변압기가 입력 컨텍스트에서 복사하는 데 GSSM보다 더 우수하다는 것을 입증했다. 그러나 상태 공간 모델은 변압기에 비해 많은 장점을 가지고 있음을 강조한다. GSSM의 메모리 및 계산 복잡도는 입력 길이에 따라 증가하지 않으며, 이는 긴 입력에 대한 훈련 및 추론에 이상적이다. 추가적으로, RNN들과 같은 상태 공간 모델들은 긴 시퀀스들(Liu et al., 2023)에 걸쳐 상태 변수들을 추적하는 데 더 우수하며, 이는 긴 일관성 텍스트를 생성하는데 유용할 수 있다. 중요한 것은, 후에서의 언어 처리\n' +
      '\n' +
      '그림 7: **(a) 복사: 자연어 문자열.** 다양한 길이의 C4에서 샘플링된 자연어 문자열을 복사하는 능력에 대해 사전 훈련된 모델을 비교하고 문자열 수준 정확도를 보고한다. 변압기 모델은 GSSM을 상당히 능가합니다. **(b) 복사: 셔플된 문자열.** 문자열이 자연 언어인 것이 중요한지 여부를 테스트하기 위해 이전 실험에서 문자열의 단어 순서를 무작위로 셔플한다. 우리는 이것이 특히 맘바 모델의 성능을 저하시킨다는 것을 발견했다. **(c) 질문 응답(SQUAD).** 우리는 문맥 단락의 길이에 기초하여 데이터세트를 비닝하는 표준 질문 응답 데이터세트 상에서 Pythia와 Mamba를 비교한다. 우리는 맘바 공연이 맥락의 길이에 따라 더 빨리 쇠퇴한다는 것을 발견한다.\n' +
      '\n' +
      '인간 두뇌는 국가 우주 모델이 언어를 처리하는 방식과 훨씬 더 유사한 것으로 보인다(Tikochinski et al., 2024). 따라서 향후 작업은 상태 공간 모델에 주의와 같은 메커니즘을 부여하여 입력에서 관련 텍스트 조각을 검색할 수 있도록 하는 하이브리드 아키텍처를 구축하는 데 중점을 두어야 한다고 믿는다. 실제로 인간은 수열을 암기하는 능력이 놀라울 정도로 제한적이지만(밀러, 1956), 본문을 돌아볼 수 있게 해주면 소설 전체를 번역할 수 있다(셸튼, 1612).\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '도움이 되는 토론에 대해 보아스 바라크에게 감사드립니다. 켐프너 인스티튜트 컴퓨팅 리소스가 이 작업을 가능하게 했습니다. Samy Jelassi는 수학 과학 및 응용 센터의 지원을 인정한다. 이 작업은 자연 및 인공지능 연구를 위한 켐프너 연구소를 설립하기 위한 찬 저커버그 이니셔티브 재단의 선물에 의해 부분적으로 가능하게 되었다. 샴 카카데는 수상자 N00014-22-1-2377로 해군 연구소의 자금 지원을 인정한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Anil et al. (2022) Anil, C., Wu, Y., Andreassen, A., Lewkowycz, A., Misra, V., Ramaseh, V., Slone, A., Gur-Ari, G., Dyer, E., and Neyshabur, B. Exploring length generalizedization in large language models. _ 신경 정보 처리 시스템_, 35:38546-38556, 2022에서의 발전.\n' +
      '* Biderman et al. (2023) Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O\'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: 훈련 및 스케일링에 걸쳐 큰 언어 모델을 분석하기 위한 스위트. In _International Conference on Machine Learning_, pp. 2397-2430. PMLR, 2023.\n' +
      '* Bradbury et al. (2016) Bradbury, J., Merity, S., Xiong, C., and Socher, R. 준순환 신경망. _ ArXiv:1611.01576_, 2016.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models is few-shot learners. _ 신경 정보 처리 시스템_, 33:1877-1901, 2020의 발전.\n' +
      '* Carlini et al. (2022) Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. Quantifying memorization across neural language models. _ arXiv preprint arXiv:2202.07646_, 2022.\n' +
      '* Chiang et al. (2023) Chiang, D., Cholak, P., and Pillay, A. Tighter bounds on the expressivity of transformer encoders. _ arXiv preprint arXiv:2301.10743_, 2023.\n' +
      '* Choromanski et al. (2020) Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. _ arXiv preprint arXiv:2009.14794_, 2020.\n' +
      '* Dao et al. (2022) Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. _ 신경 정보 처리 시스템_, 35:16344-16359, 2022에서의 발전.\n' +
      '* Deletang et al. (2022) Deletang, G., Ruoss, A., Grau-Moya, J., Genewein, T., Wenliang, L. K., Catt, E., Hutter, M., Legg, S., and Ortega, P. A. Neural Networks and chomsky hierarchy. _ arXiv preprint arXiv:2207.02098_, 2022.\n' +
      '* Devlin et al. (2018) Devlin, J., Chang, M. - W., Lee, K., and Toutanova, K. Bert: 언어 이해를 위한 깊은 양방향 변압기의 사전 훈련. _ arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* Edelman et al. (2022) Edelman, B. L., Goel, S., Kakade, S., and Zhang, C. Inductive bias and variable creation in self-attention mechanism. In _International Conference on Machine Learning_, pp. 5793-5831. PMLR, 2022.\n' +
      '* Gao et al. (2020) Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The pile: An 800gb dataset of various text for language modeling. _ ArXiv:2101.00027_, 2020.\n' +
      '* Gu & Dao(2023) Gu, A. and Dao, T. Mamba: 선택적 상태 공간을 갖는 선형-시간 시퀀스 모델링 _ arXiv preprint arXiv:2312.00752_, 2023.\n' +
      '* Gu et al. (2021) Gu, A., Goel, K., and Re, C. Efficiently modeling long sequence with structured state spaces. _ arXiv preprint arXiv:2111.00396_, 2021.\n' +
      '* Hochreiter & Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. Long shortterm memory. _ Neural computation_, 9(8):1735-1780, 1997.\n' +
      '* Jelassi et al. (2023) Jelassi, S., d\'Ascoli, S., Domingo-Enrich, C., Wu, Y., Li, Y., and Charton, F. Length generalized in arithmetic transformer. _ arXiv preprint arXiv:2306.15400_, 2023.\n' +
      '* Katharopoulos et al. (2020) Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformer with linear attention. In _International conference on machine learning_, pp. 5156-5165. PMLR, 2020.\n' +
      '* Kazemnejad et al. (2023) Kazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P., and Reddy, S. 변압기의 길이일반화에 대한 위치부호화의 영향 arXiv preprint arXiv:2305.19466_, 2023.\n' +
      '* Liu et al. (2023a) Liu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. Exposing attention glititch with flip-flop language modeling. _ arXiv preprint arXiv:2306.00946_, 2023a.\n' +
      '\n' +
      '* Liu et al. (2023) Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in middle: How language models use long context. _ arXiv preprint arXiv:2307.03172_, 2023b.\n' +
      '* Loshchilov & Hutter (2017) Loshchilov, I. and Hutter, F. Decoupled Weight decay regularization. _ arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* McCoy et al. (2023) McCoy, R. T., Smolensky, P., Linzen, T., Gao, J., and Celikyilmaz, A. 언어 모델들은 그들의 트레이닝 데이터로부터 얼마나 복사하는가? 까마귀를 이용한 텍스트 생성에서 언어적 신규성을 평가하는 단계. _ The Association for Computational Linguistics_, 11:652-670, 2023의 트랜잭션.\n' +
      '* 메릴(2019) 메릴, W. 오토마타로서의 순차적 신경망. _ ArXiv preprint arXiv:1906.01615_, 2019.\n' +
      '* Merrill et al. (2020) Merrill, W., Weiss, G., Goldberg, Y., Schwartz, R., Smith, N. A., and Yahav, E. formal hierarchy of rnn architectureures. _ arXiv preprint arXiv:2004.08500_, 2020.\n' +
      '* Merrill et al. (2022) Merrill, W., Sabharwal, A., and Smith, N. A. Saturated Transformers are Constant-Depth Threshold Circuits. _ The Association for Computational Linguistics_, 10:843-856, 08 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00493. URL[https://doi.org/10.1162/tacl_a_00493](https://doi.org/10.1162/tacl_a_00493)\n' +
      '* Miller(1956) Miller, G. A. The magic number 7 plus or minus 2: some limit of our capacity of information. _ 심리학 리뷰_, 63:91-97, 1956.\n' +
      '* Nguyen et al. (2023) Nguyen, E., Poli, M., Faizi, M., Thomas, A., Birch-Sykes, C., Wornow, M., Patel, A., Rabideau, C., Massaroli, S., Bengio, Y., et al. Hyenadna: Long range genomic sequence modeling at single nucleotide resolution. _ arXiv preprint arXiv:2306.15794_, 2023.\n' +
      '* Olsson et al. (2022) Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., et al. In-context learning and induction head. _ ArXiv:2209.11895_, 2022.\n' +
      '* Pascanu et al. (2013) Pascanu, R., Mikolov, T., and Bengio, Y. 순환 신경망을 훈련하는 것의 어려움에 대해. In _International conference on machine learning_, pp. 1310-1318. Pmlr, 2013.\n' +
      '* Paszke et al. (2019) Paszke et al. (2019) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Kimelshein, N., Antiga, L., et al. Pytorch: An imperative style, highperformance deep learning library. _ 신경 정보 처리 시스템_, 32, 2019의 발전.\n' +
      '* Peng et al. (2023) Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., et al. Rwkv: Reinventing rnns for the transformer era. _ arXiv preprint arXiv:2305.13048_, 2023.\n' +
      '* Petroni et al. (2020) Petroni, F., Lewis, P., Piktus, A., Rocktaschel, T., Wu, Y., Miller, A. H., and Riedel, S. 문맥이 언어 모델의 사실적 예측에 어떤 영향을 미치는지 arXiv preprint arXiv:2005.04611_, 2020.\n' +
      '* Press et al. (2021) Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear bias enables input length extrapolation. _ arXiv preprint arXiv:2108.12409_, 2021.\n' +
      '* Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners, 2019. URL[https://api.semanticscholar.org/CorpusID:160025533](https://api.semanticscholar.org/CorpusID:160025533).\n' +
      '* Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limit of transfer learning with unified text-to-text transformer. _ The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.\n' +
      '* Rajpurkar et al. (2018) Rajpurkar, P., Jia, R., and Liang, P. Know what you don\'t know: Unanswable questions for squad. _ arXiv preprint arXiv:1806.03822_, 2018.\n' +
      '* Ruoss et al. (2023) Ruoss, A., Deletang, G., Genewein, T., Grau-Moya, J., Csordas, R., Bennani, M., Legg, S., and Veness, J. Randomized position encodings boost length generalized of Transformers. _ arXiv preprint arXiv:2305.16843_, 2023.\n' +
      '* Sanford et al. (2023) Sanford, C., Hsu, D., and Telgarsky, M. 변압기의 표현력 및 한계. _ arXiv preprint arXiv:2306.02896_, 2023.\n' +
      '* Shelton(1612) Shelton, T. _ 라만차의 신비한 신사 돈키호테 1612. 미겔 드 세르반테스가 쓴 글, 토마스 셸튼이 번역한 글.\n' +
      '*Shen et al.(2023) Shen, R., Bubeck, S., Eldan, R., Lee, Y. T., Li, Y., and Zhang, Y. 변압기 연산에 대한 위치 설명 사항. _ arXiv preprint arXiv:2311.14737_, 2023.\n' +
      '* Strobl et al. (2023) Strobl, L., Merrill, W., Weiss, G., Chiang, D., and Angluin, D. Transformers as recognizeizers of formal languages: survey on expressivity. _ arXiv preprint arXiv:2311.00208_, 2023.\n' +
      '* Su et al. (2023) Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. 로포머: 회전 위치 매립을 갖는 향상된 트랜스포머. _ Neurocomputing_, pp. 127063, 2023.\n' +
      '*Sun et al. (2023) Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: A successor to transformer for large language models. _ arXiv preprint arXiv:2307.08621_, 2023.\n' +
      '* Tikochinski et al.(2024) Tikochinski, R., Goldstein, A., Meiri, Y., Hasson, U., and Reichart, R. 뇌에서 긴 텍스트 처리를 위한 점진적인 대규모 언어 모델입니다. 2024년\n' +
      '\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention all you need. _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Wei et al.(2022) Wei, C., Chen, Y., and Ma, T. 통계적으로 의미 있는 근사치: 변압기를 이용한 튜링머신의 근사화에 관한 사례 연구 신경 정보 처리 시스템_, 35:12071-12083, 2022에서의 발전.\n' +
      '* Weiss et al. (2021) Weiss, G., Goldberg, Y., and Yahav, E. Thinking like transformer. In _International Conference on Machine Learning_, pp. 11080-11090. PMLR, 2021.\n' +
      '* Wolf et al. (2019) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Huggingface\'s transformerers: State-of-the-art natural language processing. _ ArXiv preprint arXiv:1910.03771_, 2019.\n' +
      '* Zhou et al. (2023) Zhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O., Susskind, J., Bengio, S., and Nakkiran, P. 어떤 알고리즘이 변압기를 학습할 수 있는가? 길이의 일반화 연구. _ arXiv preprint arXiv:2310.16028_, 2023.\n' +
      '\n' +
      '## 부록 실험 설정\n' +
      '\n' +
      '이 섹션에서는 실험 설정에 대한 추가 세부 정보를 제공합니다. 먼저 변압기 실험에 사용된 위치 인코딩에 대한 설명(A.1 부분)을 제공한 다음 훈련 및 평가 절차(A.2 부분)에 대한 세부 정보를 제공한다.\n' +
      '\n' +
      '변압기의 위치 부호화\n' +
      '\n' +
      '섹션 3의 실험에서 다중 위치 인코딩 방식을 고려한다.\n' +
      '\n' +
      '* NoPE 방식(Kazemnejad et al., 2023)에서 임의의 주의 점수에 위치 정보가 추가되지 않는다(도 8a). 이 아키텍처 선택은 복사 작업을 포함한 여러 작업에서 더 나은 길이의 일반화를 얻는 데 도움이 됩니다.\n' +
      '* ALiBi 방식(Press et al., 2021)은 그들의 거리에 비례하는 페널티로 주의 점수를 편향시킨다(도 8b). \\ (m\\)은 훈련 전에 고정된 머리별 기울기이다.\n' +
      '* 2절에서 소개된 Hard-ALiBi 스킴은 \\(M\\) 마스킹 어텐션 헤드를 가지고 있는데, 여기서 우리는 모델이 직접 이전 토큰에 참석하도록 명시적으로 강제하고 \\(H-M\\) 헤드는 NoPE 어텐션 헤드로 설정된다. 도 8c에서, 우리는 \\(M=4\\) 마스킹된 헤드들을 갖는 경우를 디스플레이한다: 첫 번째 헤드에서, 토큰들은 단지 그들 자신을 돌본다; 두 번째 헤드에서, 토큰들은 그들 자신과 이전 토큰들을 돌본다; 세 번째 헤드에서, 토큰들은 그들 자신, 이전 토큰들 및 두 번째 이전 토큰들을 돌본다. 나머지 \\(H-M\\) 헤드는 NoPE로 설정된다.\n' +
      '\n' +
      '그림 8: **변압기에 대한 위치 인코딩 방식: 실험에서 훈련된 변압기의 서로 다른 위치 인코딩 그림. (a) 임의의 어텐션 헤드에 위치 인코딩이 적용되지 않는 NoPE 인코딩(Kazemnejad et al., 2023)에 대응하고 (b)는 \\(m\\)이 헤드-특정 스칼라인 ALiBi 인코딩(Press et al., 2021)을 묘사하고 (c) 섹션 2에서 도입된 Hard-ALiBi 인코딩을 예시한다. 예를 위해, 우리는 헤드 1, 2 및 3이 그들의 현재 토큰, 그들의 현재 및 선행 토큰 및 그들의 현재, 선행 및 선행 토큰에 참석하도록 강제하는 것을 의미하는 세 개의 헤드를 마스크하는 경우를 고려한다. 나머지 헤드는 NoPE 헤드로 설정됩니다.**\n' +
      '\n' +
      '### 사전 훈련 및 평가 세부사항\n' +
      '\n' +
      '소프트웨어 종속성.우리는 파이토치(Paszke et al., 2019)에서 우리의 모든 훈련을 구현한다. 우리는 HuggingFace library (Wolf et al., 2019)와 Mamba GitHub repository (Gu and Dao, 2023)를 사용한다.\n' +
      '\n' +
      '구조.섹션 3의 실험에서 변압기의 백본은 GPT-NeoX 아키텍처이다. 계층 수는 12, 은닉 크기는 1024, 머리 수는 16으로 설정하였다. 부분 A.1에 기술된 서로 다른 위치 인코딩을 고려한다. 알리바이의 경우, 본 논문에서와 같이 머리 특정 스칼라(m_{h}=2^{-h/2}\\)를 \\(h\\in\\{1,\\dots,H\\}\\)에 대해 설정한다. Hard-Alibi 모델의 경우, 마스크 헤드의 수를 \\(M\\in\\{2,\\dots,10\\}\\)으로 스윕하여 가장 좋은 모델이 \\(M=6\\)에 해당하는 것을 알 수 있었다. Mamba 모델의 경우 레이어 수를 24로 설정하고 은닉 크기를 1024로 설정하였으며, 상태 공간 차원(S\\in\\{16,32,64,128,256\\}\\)을 스윕하여 가장 좋은 모델인 \\(S=32\\)을 찾았다. 이러한 하이퍼파라미터의 선택은 트랜스포머와 맘바 모델 모두 비교 가능한 수의 파라미터를 갖는다는 것을 보장한다. 마지막으로 LSTM은 4겹과 폭 1024로 제작되었습니다.\n' +
      '\n' +
      '교육 하이퍼파라미터.3절에서는 각 에폭에서 64 크기의 배치 크기를 온라인으로 샘플링하고, 컨텍스트를 예로 채워서 그림 0(a)를 제외한 모든 실험에 대해 컨텍스트 길이(\\(C=420\\)를 선택하고 이 컨텍스트에 맞게 가능한 한 많은 예를 포장한다. 그래서 우리의 경우, 한 표본에는 많은 사례가 포함되어 있습니다. 우리는 트랜스포머와 맘바 모두에 대해 15개의 에폭에 대해 실험을 실행하는 반면 LSTM에는 300개의 에폭이 필요하다. 모든 방법은 AdamW 최적화기(Loshchilov and Hutter, 2017)를 이용하여 학습률 5e-5, 선형 속도 감쇠 스케줄, 300단계의 워밍업 및 기본 가중치 감쇠 1e-1로 학습한다. 마지막으로 모든 모델을 학습하기 위해 다음 토큰 예측 손실을 사용하지만 입력 인스턴스에 마스크를 적용하여 레이블에 오류가 발생할 때(입력과 레이블이 공동으로 발생하지 않을 때)에만 페널티를 준다.\n' +
      '\n' +
      '컴퓨팅 리소스.사전 훈련은 모두 RTX8000 GPU를 사용하여 내부 클러스터에서 수행되었다. 우리는 논문에서 결과를 생성하는 데 필요한 최종 훈련 실행이 약 600 GPU 시간이 소요되었다고 추정한다.\n' +
      '\n' +
      '평가 알고리즘.주어진 맥락 길이를 가진 질문의 수가 제한되어 있기 때문에 50개 이상의 질문을 평가하는 질문 응답을 제외한 모든 작업에 대해 크기 64의 10개 배치 이상의 모델을 평가한다.\n' +
      '\n' +
      '디코딩 알고리즘.추론에서 모든 모델은 생성을 위해 그리디 디코딩을 사용하며 온도를 0으로 설정한다.\n' +
      '\n' +
      '## 부록 B 추가 실험\n' +
      '\n' +
      '하위 섹션 B.1에서는 복사 작업의 배포 내 학습에 초점을 맞추고 GSSM이 필요한 샘플 수가 변압기에 필요한 샘플 수보다 훨씬 많다는 것을 보여준다. 하위 섹션 B.2에서는 문자열이 균일하게 샘플링되는 경우 복사 작업에 대해 미리 훈련된 모델의 성능을 연구한다. 이 실험은 복사할 텍스트가 완전히 무작위일 때 미리 훈련된 변압기와 GSSMs 사이의 간격이 훨씬 더 크다는 것을 보여준다.\n' +
      '\n' +
      '### 복사 작업의 데이터 효율성\n' +
      '\n' +
      '이 섹션에서는 그림 0(a)의 데이터 효율성 실험을 보완하기 위해 추가 도표를 제공한다. 다음 사항을 강조하고자 합니다.\n' +
      '\n' +
      '* 그림 0(a)에서 우리는 맘바 학습 곡선에 대한 급격한 전이를 볼 수 있다. 다만, 도 8의 (a)는 문자 수준에서 학습 과정이 보다 원활하다는 것을 보여준다. 게다가, LSTM은 문자 수준에서도 길이-300 문자열로 복사본을 배울 수 없다.\n' +
      '* 우리는 훨씬 더 짧은 문자열, 즉 길이가 \\(\\leq 30\\)인 문자열을 복사하는 학습을 하는 실험을 고려한다. 도 8(b)는 트랜스포머와 맘바 사이의 트레이닝 예제의 관점에서의 갭이 훨씬 더 작다는 것을 보여준다. 즉, 맘바는 단지 10배 더 많은 데이터를 필요로 한다. 또한, LSTM은 복사 작업을 학습할 수 있지만 변압기보다 100배 더 많은 데이터가 필요하다는 것을 알 수 있다.\n' +
      '\n' +
      '유니폼 복사 작업에서 미리 훈련된 모델\n' +
      '\n' +
      '본 절에서는 복사 작업에서 사전 훈련된 맘바 모델보다 사전 훈련된 피시아의 우수성을 보여주는 추가 실험을 제공한다.\n' +
      '\n' +
      '우리는 섹션 3과 동일한 설정을 고려한다: 우리는 고정된 길이를 가진 알파벳 문자의 균일한 문자열을 샘플링하고 모델에 섹션 4.2에 설명된 것과 동일한 프롬프트 형식을 사용하여 복사하도록 요청한다.\n' +
      '\n' +
      '이 설정은 문자열이 더 랜덤하기 때문에 그림 6(b)의 더 극단적인 버전이다: 그림 6(b)에서 명사의 순서는 랜덤했지만 명사는 영어 명사인 반면 그림 6(b)에서는 문자열이 완전히 랜덤하다. 그림 10에서 가장 작은 피티아가 가장 큰 맘바를 능가하는 변압기와 맘바 모델 사이의 명확한 분리를 볼 수 있다. 그러나 그림 6(b)에 비해 1.4B 모델이 거의 100% 정확도를 얻을 수 있기 때문에 피티아 성능이 훨씬 높다.\n' +
      '\n' +
      '그림 10: **Copy: 균일한 문자열.** 문자열이 자연 언어인 것이 중요한지 여부를 테스트하기 위해 균일하게 샘플링된 문자열을 생성한다(생성 과정은 섹션 3에 설명되어 있다). 우리는 이것이 맘바 모델을 열화시키는 반면 피티아 모델은 높은 성능을 유지할 수 있음을 발견했다.\n' +
      '\n' +
      '그림 9: **(a) 긴 문자열을 복사하는 것: 문자 수준 정확도.** 여기서 우리는 길이 \\(\\leq 300\\)의 문자열을 복사하도록 모델을 훈련하고 길이 300의 문자열에 문자 수준 정확도를 평가한다. 변압기는 GSSM보다 훨씬 빠르게 훈련한다. 맘바는 그림 0(a)보다 더 점진적인 학습 곡선을 가지고 있다. LSTM은 문자 수준에서 이 수의 샘플 내에서 작업을 학습할 수도 없다. **(b) 짧은 문자열 복사: 문자열 수준 정확도.** 여기서 우리는 모델을 훈련하여 길이 \\(\\leq 30\\)의 문자열을 복사하고, 길이 30의 문자열에 대한 문자 수준 정확도를 평가한다. 변압기는 GSSM보다 훨씬 빠르게 훈련한다. 그림 0(a)에 비해, 우리는 Mamba가 길이-30 문자열을 복사하는 법을 배우기 위해 훨씬 적은 표본이 필요하다는 것을 안다. LSTM은 복사를 배울 수 있지만 100배 더 많은 훈련 예가 필요하다. **(c) 짧은 문자열 복사: 문자 수준 정확도.** 여기서 우리는 모델을 훈련하여 길이 \\(\\leq 30\\)의 문자열을 복사하고, 길이 30의 문자열에 대한 문자 수준 정확도를 평가하고 문자 수준 정확도를 보고한다.\n' +
      '\n' +
      '## 부록 C 증명서 - 상한선\n' +
      '\n' +
      '이 절에서는 정리 2.3과 렘마 2.4에 대한 자세한 증명을 제공한다.\n' +
      '\n' +
      '### Technical Lemmas\n' +
      '\n' +
      '우리는 정리 2.3의 증명에서 사용하는 몇 가지 기술적인 여권을 소개하는 것으로 시작한다.\n' +
      '\n' +
      '**Lemma C.1**.: _Let \\(h_{t}(\\mathbf{x}_{1},\\dots,\\mathbf{x}_{i})=\\frac{1}{\\min(t,t)}\\sum_{j=\\max(1,i-t+1)}^{i}\\mathbf{x}_{j}\\. 그런 다음 하드-ALiBi 주의 헤드를 사용하여 \\(h_{t}\\)을 계산할 수 있다._\n' +
      '\n' +
      'Proof.: Let \\(W_{k},W_{q}=0\\)(zero matrix) and Let \\(W_{v}=I_{d}\\)(identity matrix) 우리는 \\(b_{i}\\in\\{0, -\\infty\\}^{i}\\) s.t를 선택한다.\n' +
      '\n' +
      '\\[b_{i,j}=\\begin{cases}-\\infty&j\\leq i-t\\0&j>i-t\\end{cases}\\]\n' +
      '\n' +
      '**Lemma C.2**.: _\\(d=\\lceil\\log(D)\\rceil+2\\)라고 가정하자. 그런 다음 임베딩 \\(\\Psi\\) s.t._\n' +
      '\n' +
      '*_For every_\\(x\\in\\mathbb{D}\\)_it는_\\(\\left\\lVert\\Psi(x)\\right\\rVert_{2}=1\\)_and_\\(\\left\\lVert\\Psi(x)\\right\\rVert_{\\infty}\\leq 1\\)_.__\n' +
      '* _For_ \\(x^{\\prime}\\neq x\\)_it는_ \\(\\left\\langle x,x^{\\prime}\\right\\rangle<1-\\frac{1}{d}\\)_.__\n' +
      '* _For every_ \\(x\\neq\\left\\langle\\mathrm{nos}\\right\\rangle\\)_,_\\(\\left\\langle\\Psi(x),\\Psi(\\left\\langle\\mathrm{nos}\\right\\rangle)\\right\\rangle=0\\)_, and every_ \\(x\\neq\\left\\langle\\mathrm{cory}\\right\\rangle\\),_\\(\\left\\langle\\Psi(x),\\Psi(\\left\\langle\\mathrm{cory}\\right\\rangle)\\right\\rangle)\\right\\rangle=0\\)_,_\\(\\left\\langle\\Psi(x),\\Psi(\\left\\langle\\mathrm{cory}\\right\\rangle)\\right\\rangle)\\right\\rangle=0\\)_,_\\(\\left\\langle\\Psi(x),\\Psi(\\left\\langle\\mathrm{cory}\\right\\rangle)\\right\\rangle\n' +
      '\n' +
      '증명: Denote\\(d^{\\prime}=\\lceil\\log(D)\\rceil\\)에서 모든 "비특수" 토큰들을 벡터로 (\\left\\{\\pm\\frac{1}{\\sqrt{d}\\right\\}^{d^{\\prime}\\)로 인코딩할 수 있음을 관찰하고, 이 인코딩을 \\(\\Psi^{\\prime}\\)으로 나타낸다. 자, 정의를 내리자\n' +
      '\n' +
      '[\\Psi(x)=\\begin{cases}[1,0,\\dots,0]&x=\\left\\langle\\mathrm{nos}\\right\\rangle\\\\[0,1,0,\\dots,0]&x=\\left\\langle\\mathrm{cory}\\right\\rangle\\\\[0,0,\\Psi^{\\prime}(x)]&o.w.\\end{cases}\\\n' +
      '\n' +
      '**Lemma C.3**: _Let \\(\\mathbf{z}\\in\\mathbb{R}^{K}\\)은 어떤 상수 \\(a>b>0\\)에 대해 \\(i\\in[K]\\) s.t. \\(z_{i}=a\\)이 있고 모든 \\(j\\neq i\\)에 대해 \\(|z_{j}|\\leq b\\)이 있다. Denote \\(\\mathbf{s}=\\mathrm{softmax}(\\mathbf{z})\\). 그런 다음 모든 \\(j\\neqi\\)에 대해 \\(s_{i}\\geq\\frac{1}{1+K\\exp(b-a)}\\) 및 \\(s_{j}\\leq\\exp(b-a)\\)\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '따라서 우리는 \\(T^{\\rm query}=[\\hat{g}_{0},\\ldots,\\hat{g}_{q-1},n\\cdot g_{1}^{*},\\ldots,n\\cdot g_{q}^{*}]\\을 취할 수 있다.\n' +
      '\n' +
      '이제 우리는 Lemma C.4에서 구조 상단에 위치 임베딩이 없는 단일 주의 헤드를 사용하여 복사 알고리즘을 실현한다는 것을 보여줌으로써 정리 2.3을 증명한다. 첫 번째 블록은 \\(\\operatorname{key}_{i},\\operatorname{query}_{i},\\operatorname{value}_{i}\\)의 올바른 선택을 계산하므로, 주의행렬의 정확한 스케일링을 통해 \\(i\\) 위치에서 두 번째 레이어의 출력이 \\(j\\) s.t. \\(\\operatorname{key}_{j}=\\operatorname{query}_{i}\\)에 해당하는지를 확인한다.\n' +
      '\n' +
      '정리 2.3의 증명: Lemma C.4에 의해 보장되는 트랜스포머 블록의 출력으로 \\(T^{\\rm 값},T^{\\rm 키},T^{\\rm 쿼리}\\)을 둔다. 일부 템퍼쳐\\(\\tau\\in\\mathbb{R}\\)에 대해, 다음 함수는 이 블록의 위에 소프트맥스-어텐션 레이어에 의해 계산될 수 있음을 관찰한다:\n' +
      '\n' +
      '\\[H(\\Psi(x_{1}),\\ldots,\\Psi(x_{i}))=T^{\\rm 값}\\cdot\\operatorname{softmax}(\\tau\\cdot T^{\\rm key}\\cdot T_{i}^{\\rm query})\\]\n' +
      '\n' +
      '여기서, \\(T^{\\rm 값}\\)은 \\(T^{\\rm 값}(\\Psi(x_{1}),\\ldots,\\Psi(x_{i}))\\을 나타낸다.\n' +
      '\n' +
      '현재로서는 \\(\\mathbf{x}\\)의 모든 \\(n\\)-그램이 유일하고, 입력 길이가 \\(K=D^{n}\\)에 대해 \\(2L+2\\leq K\\)을 만족한다고 가정한다.\n' +
      '\n' +
      '**Claim:** 일부 \\(i>L\\), denote \\(\\mathbf{z}=T^{\\rm key}\\cdot T_{i}^{\\rm query}\\)을 고정한다. 그 다음, 모든 \\(j\\neq i-L+1\\)에 대해 \\(z_{i-L+1}=n\\) 및 \\(|z_{j}|<n-\\frac{1}{d}\\)을 갖는다.\n' +
      '\n' +
      '**Proof:** 우리는 다음과 같은 경우로 구분한다:\n' +
      '\n' +
      '만약 \\(i>L+n-1\\)이면, 모든 \\(j\\)에 대해 \\(T_{j}^{\\rm key}\\cdot T_{i}\\cdot[\\Psi(x_{j-1}),\\ldots,\\Psi(x_{j-n})\\Psi(x_{i-t+1})\\\\(j\\neqi(x_{i-t+1})\\\\(t\\neq\\Psi(x_{i-t+1}))\\\\(t\\neq\\Psi(x_{i-t+1})\\\\(t\\neq\\psi(x_{i-t+1})\\\\(t\\neq\\psi(x_{i-t+1}))\\\\(t\\neq\\psi(x_{i-t+1})\\\\(t\\neq\\psi(x_{i-t+1}))\\\\(t\\neq\\psi(x_{i-t 이 경우, 우리는 \\(\\left|T_{j}^{\\rm key}\\cdot T_{i}^{\\rm query}\\right|\\leq n-\\frac{1}{d}\\)을 얻는다.\n' +
      '* 만약 \\(L<i\\leq L+n-1\\)과 \\(j\\leq n\\)이면 \\[T_{j}^{\\rm key}\\cdot T_{i}^{\\rm query}=ne_{j-1}\\cdot e_{i-L}=n\\cdot\\mathbf{1}\\{j=i -L+1\\}\\]을 만족한다.\n' +
      '* \\(L<i\\leq L+n-1\\)과 \\(j>n\\) 다음에 \\[T_{j}^{\\rm key}\\cdot T_{i}^{\\rm query}=\\sum_{t=1}^{n}\\Psi(x_{j-t)\\Psi(x_{i-t+1})\\\\(n\\)-grams가 반복되지 않으므로 \\(\\left|T_{j}^{\\rm key}\\cdot T_{i}^{\\rm query}\\right|\\leq n-\\frac{1}{d}\\)을 얻는다.\n' +
      '\n' +
      '**claim:** 일부 \\(\\epsilon\\in(0,1)\\)과 일부 \\(i>L\\)을 고정하며, \\(\\mathbf{s}=\\operatorname{softmax}(\\tau T^{\\rm key}\\cdot T_{i}^{\\rm query})=\\operatorname{softmax}(\\tau\\cdot\\mathbf{z})\\). 만약 \\(\\tau=d\\ln(\\frac{2K}{\\epsilon})\\)이라면, \\(s_{i-L+1}\\geq 1-\\epsilon\\)과 \\(s_{j}\\leq\\frac{\\epsilon}2K}\\)은 모두 \\(j\\neq i-L+1\\)이다.\n' +
      '\n' +
      '**Proof:** 이전 클레임을 사용하여, Lemma C.3과 함께인지 여부를 우리는 이해한다:\n' +
      '\n' +
      '* \\(s_{i-L+1}\\geq\\frac{1}{1+i\\exp(-\\tau/d)}\\geq\\frac{1}{1+K\\exp(-\\tau/d)}\\geq\\frac{1}{1+\\epsilon/2}=1-\\frac{\\epsilon/2}{1+\\epsilon/2}\\geq 1-\\epsilon\\\\(j\\neq i-L+1\\), \\[s_{j}\\leq\\exp(-\\tau/d)\\leq\\frac{\\epsilon}{2K}\\eq\\frac{1}{1+K\\exp(-\\tau/d)}\\geq\\frac{1}{1+\\epsilon/2}=1-\\frac{\\epsilon/2}{1+\\epsilon/2}\\geq 1-\\epsilon\\\\(j\\neq i-L+1\\), \\[s_{j}\\\n' +
      '\n' +
      '**청구항:** 일부 \\(\\epsilon\\in(0,1)\\)과 일부 \\(i>L\\)을 고정한다. 그런 다음 \\(\\tau\\geq d\\ln(\\frac{2K}{\\epsilon})\\)에 대해 다음과 같이 유지한다.\n' +
      '\n' +
      '\\[\\|H(\\Psi(x_{1}),\\ldots,\\Psi(x_{i}))-\\Psi(x_{i-L+1})\\|\\leq\\epsilon\\]\n' +
      '\n' +
      '**Proof:** Let \\(\\mathbf{s}\\)는 앞의 청구항에서 정의한 바와 같다. 그리고,\n' +
      '\n' +
      '\\Psi(x_{1}),\\ldots,\\Psi(x_{i}))\\Psi(x_{i}s_{j}\\Psi(x_{j}))-\\Psi(x_{i-L+1})\\frac{ \\frac{ \\\\psi(x_{i-L+1}s_{j}\\leq\\epsilon+(i-1)\\frac{ \\frac{ \\frac{ \\psi(x_{i-L+1})\\right\\psi(x_{i-L+1})\\right\\psi(x_{i-L+1})\\leq i-L+1}s_{j}\\leq 2\\epsilon\\\\[=(1-s_{i-L+1})\\frac{ \\frac{ \\frac{ \\psi(x_{i-L+1})\\right\\psi(x_{i-L+1})\\right\\psi\n' +
      '\n' +
      '이제, \\(\\Phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{D}\\)에 의해 주어진 출력 맵은 \\(\\Phi(\\mathbf{z})=\\arg\\max_{x\\in\\mathbbb{D}\\mathbf{z}\\cdot\\Psi(x)\\)으로 표시된다.\n' +
      '\n' +
      '**청구항:** 만약 \\(\\tau\\geq d\\ln(8Kd)\\)이면, 모든 \\(i>L\\)에 대해 \\(\\Phi(H(\\Psi(x_{1}),\\ldots,\\Psi(x_{i})))=x_{i-L+1}\\을 갖는다.\n' +
      '\n' +
      '**Proof:** Denote \\(\\mathbf{y}_{i}=H(\\Psi(x_{1}),\\ldots,\\Psi(x_{i}))\\). 첫째, 앞의 주장을 이용하여 우리는\n' +
      '\n' +
      '\\mathbf{y}_{i}\\cdot\\Psi(x_{i-L+1}) = (\\mathbf{y}_{i}-\\Psi(x_{i-L+1})\\cdot\\Psi(x_{i-L+1})+\\|\\Psi(x_{i-L+1})\\|\\] \\[\\geq 1-\\|\\mathbf{y}_{i}-\\Psi(x_{i-L+1})\\|\\geq 1-\\frac{1}{4d}\\\\frac{1}{4d}}\n' +
      '\n' +
      '다음으로, 우리가 가지고 있는 모든 \\(j\\neq i-L+1\\)에 대해 관찰하라\n' +
      '\n' +
      'y}_{i}\\cdot\\Psi(x_{j}) = (\\mathbf{y}_{i}-\\Psi(x_{i-L+1})\\cdot\\Psi(x_{j})+\\Psi(x_{j})\\cdot\\Psi(x_{i-L+1})\\] \\[\\leq\\mathbff{y}_{i}-\\Psi(x_{i-L+1})\\|+1-\\frac{1}{d}\\leq1-\\frac{3}{4d}<\\mathbfffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff\n' +
      '\n' +
      '상기 청구항으로부터, 트랜스포머 구축은 자동 회귀 생성의 각 단계에서 올바른 토큰을 출력한다.\n' +
      '\n' +
      '### Lemma 2.4의 증명\n' +
      '\n' +
      'Lemma 2.4의 증명: \\(i<j\\in[L]\\)을 고쳐라. \\(I:=\\{i,\\ldots,i+n\\}\\)과 \\(J:=\\{j,\\ldots,j+n\\}\\)으로 하자. 우리는 먼저 몇 \\(\\mathbf{x}\\) s.t. \\(\\mathbf{x}_{I}=\\mathbf{x}_{J}\\)을 그릴 확률을 경계했다. \\(\\mathbf{x}_{I\\cup J}\\) 선택에는 \\(D^{|I\\cup J}\\)이 있다는 점에 유의한다. 우리는 \\(\\mathbf{x}_{I\\cup J}\\) s.t. \\(\\mathbf{x}_{I}=\\mathbf{x}_{J}\\)에 대한 선택 수를 계산한다. 이 경우 \\(\\mathbf{x}_{I\\cup J}\\)는 \\(\\mathbf{x}_{I\\setminus J}\\)에 의해 결정되므로 \\(D^{|I\\setminus J|}\\)의 선택가능성이 있음을 주목하라. 우리는 결론을 내렸다.\n' +
      '\n' +
      '[\\Pr\\left[\\mathbf{x}_{I}=\\mathbf{x}_{J}\\right]=\\frac{D^{|I\\setminus J|}}{D^{|I\\cup J|}}=D^{|I\\setminus J|-|I\\cup J|}=D^{-n}\\}\n' +
      '\n' +
      '결합결합을 이용하면\n' +
      '\n' +
      '\\mathbf{x}_{i,\\ldots,i+n}=\\mathbf{x}_{j,\\ldots,j+n}\\right]\\leq\\sum_{i<j}\\Pr\\left[\\mathbf{x}_{i,\\ldots,i+n}=\\mathbf{x}_{j,\\ldots,j+n} \\right]<L^{2}D^{-n}\\right]\n' +
      '\n' +
      '## 부록 D 증명 - 하단 경계\n' +
      '\n' +
      '이 절에서는 정리 2.7을 증명한다. 우리는 모든 입력에 대해, 각 반복에서 모델의 출력이 입력을 관찰한 후 모델의 상태의 결정론적 함수임을 보여주는 것으로 시작한다:\n' +
      '\n' +
      '**Lemma D.1**.: _Let \\(H_{u,r}:\\mathbb{D}^{n^{\\prime}\\rightarrow\\mathbb{D}^{n}\\)을 고정 상태 시퀀스 대 시퀀스 모델로 한다. 그리고 모든 \\(\\mathbf{x}\\in\\mathbb{D}^{n^{\\prime}\\)_에 대한 지도 \\(G:\\mathcal{S}\\rightarrow\\mathbb{D}^{n}\\) s.t.\n' +
      '\n' +
      '\\[H_{u,r}(\\mathbf{x})=G\\circ S_{n^{\\prime}}(\\mathbf{x})\\]\n' +
      '\n' +
      '증명: \\(x_{n^{\\prime}+1},\\ldots,x_{n^{\\prime}+n}\\)을 \\(H_{u,r}\\)의 출력으로 하자. 우리는 함수 \\(G_{1},\\ldots,G_{n}\\) s.t. \\(H_{u,r}(x_{1},\\ldots,x_{n^{\\prime}})=G(S_{n^{\\prime}}(x_{1},\\ldots,x_{n}))\\이 존재함을 보여야 한다. 우리는 다음과 같은 재귀적 정의를 내린다:\n' +
      '\n' +
      '*\\(G_{1}(s)=r(s)\\), \\(\\tilde{G}_{1}(s)=u(s,G_{1}(s))\\.\n' +
      '*\\(G_{i}(s)=r(\\tilde{G}_{i-1}(s))\\), \\(\\tilde{G}_{i}(s)=u(\\tilde{G}_{i-1}(s), G_{i}(s))\\.\n' +
      '\n' +
      'Denote \\(s=S_{n^{\\prime}}(x_{1},\\ldots,x_{n^{\\prime}})\\)\\(G_{i}(s)=x_{n^{\\prime}+i}\\)와 \\(\\tilde{G}_{i}(s)=S_{n^{\\prime}+i}(x_{1},\\ldots,x_{n^{\\prime}+i})\\)을 유도함으로써 증명한다.\n' +
      '\n' +
      '* \\(G_{1}(s)=r(s)=R_{n^{\\prime}}(x_{1},\\ldots,x_{n^{\\prime}})=x_{n^{\\prime}+1}\\).\n' +
      '*\\(\\tilde{G}_{1}(s)=u(s,G_{1}(s))=u(s,x_{n^{\\prime}+1})=S_{n^{\\prime}+1}(x_{1}, \\ldots,x_{n^{\\prime}+1})\\)\n' +
      '*\\(G_{i}(s)=r(\\tilde{G}_{i-1}(s))=r(S_{n^{\\prime}+i-1}(x_{1},\\ldots,x_{n^{\\prime}+i-1}))=R_{n^{\\prime}+i-1}(x_{1},\\ldots,x_{n^{\\prime}+i-1})=x_{n^{\\prime}+i}\\)\n' +
      '*\\(\\tilde{G}_{i}(s)=u(\\tilde{G}_{i-1}(s,G_{i}(s)))=u(S_{n^{\\prime}+i-1}(x_{1}, \\ldots,x_{n^{\\prime}+i-1}),x_{n^{\\prime}+i})=S_{n^{\\prime}+i}(x_{1},\\ldots,x_{n^{\\prime}+i})}(x_{1},\\ldots,x_{n^{\\prime}+i})\\)\n' +
      '\n' +
      '따라서 요구되는 것은 다음과 같다.\n' +
      '\n' +
      '이전 렘마를 감안할 때 가능한 상태의 수와 가능한 입력의 수를 비교하여 모델의 오류를 제한했다.\n' +
      '\n' +
      '정리의 증명 2.7.: Lemma D.1로부터, 일부 함수 \\(G:\\mathcal{S}\\rightarrow\\mathbb{D}^{n}\\) s.t.\\(H_{u,r}=G\\circ S_{n^{\\prime}\\)이 존재한다. 각 \\(\\mathbf{x}\\)에 대해 우리는 \\(\\tilde{\\mathbf{x}\\)의 수열을 \\(\\left\\langle{\\rm nos}\\right\\rangle,\\mathbf{x},\\left\\langle{\\rm corv}\\right\\rangle)으로 나타낸다. 자, 다음을 관찰하라.\n' +
      '\n' +
      '{err}_{\\mathcal{D}_{n}}(H_{u,r}(\\tilde{\\mathbf{x}})=\\mathbf{1}\\mathbf{n}\\sum_{u,r}(\\tilde{\\mathbf{x}\\mmathcal{S}\\sum_{n}\\mmathbf{x}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmathbf{n}\\mmath\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>