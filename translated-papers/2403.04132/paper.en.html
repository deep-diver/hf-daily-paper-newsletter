<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\n' +
      '\n' +
      ' Wei-Lin Chiang\n' +
      '\n' +
      '1 **Lianmin Zheng**\n' +
      '\n' +
      '1 **Ying Sheng**\n' +
      '\n' +
      '2 **Anastasios N. Angelopoulos**\n' +
      '\n' +
      '1 **Tianle Li**\n' +
      '\n' +
      '1 **Dacheng Li**\n' +
      '\n' +
      '**Banghua Zhu**\n' +
      '\n' +
      '**Hao Zhang**\n' +
      '\n' +
      '**Michael I. Jordan**\n' +
      '\n' +
      '**Joseph E. Gonzalez**\n' +
      '\n' +
      '**Ion Stoica**\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)UC Berkeley \\({}^{2}\\)Stanford \\({}^{3}\\)UCSD. Correspondence to: Wei-Lin Chiang <weichiang@berkeley.edu>.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at [https://chat.lmsys.org](https://chat.lmsys.org).\n' +
      '\n' +
      'Machine Learning, ICML, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Recent advancements in large language models (LLMs) have significantly expanded their capabilities beyond traditional natural language processing boundaries, addressing a broad array of general tasks (OpenAI, 2023; Gemini et al., 2023; Touvron et al., 2023). These developments underscore the potential of LLMs but also have raised concerns with respect to performance evaluation. Current benchmarks often fail to capture the nuanced and diverse aspects of these models, particularly in assessing their alignment with human preferences in real-world, open-ended tasks.\n' +
      '\n' +
      'To assess the performance of LLMs, the research community has introduced a variety of benchmarks. These benchmarks can be categorized based on two factors: the source of questions (either static or live) and the evaluation metric (either ground truth or human preference). According to these factors, benchmarks can be classified into four categories, as shown in Figure 1. While a range of benchmarks is beneficial, the most prevalent current method for evaluating LLMs remains a static, ground-truth-based evaluation, partly because such evaluations are inexpensive and reproducible.\n' +
      '\n' +
      'However, these static, ground-truth-based benchmarks exhibit several limitations. Firstly, the questions within these benchmarks are not open-ended, hindering the ability to capture the flexible and interactive use found in real-world settings (Zheng et al., 2023). Secondly, the test sets in these benchmarks are static, meaning they can become contaminated over time, which undermines the reliability of the evaluation results (Yang et al., 2023). Furthermore, for many complex tasks, establishing a definitive ground truth is not only challenging but sometimes unattainable. Consequently, current benchmarks fail to adequately address the needs of state-of-the-art LLMs, particularly in evaluating user preferences. Thus, there is an urgent necessity for an open, live evaluation platform based on human preference that can more accurately mirror real-world usage.\n' +
      '\n' +
      'Creating such a benchmark platform entails significant challenges. It requires the collection of live, fresh, and diverse user questions to accurately represent real-world scenarios.\n' +
      '\n' +
      'Figure 1: Classification of LLM benchmarks: We categorize along two dimensions: whether the questions are from a static dataset or a live, fresh source, and whether the evaluation metric relies on ground truth or (approximated) human preferences. MMLU (Hendrycks et al., 2020), HellaSwag (Zellers et al., 2019), GSM-8K (Cobbe et al., 2021), MT-Bench (Zheng et al., 2023), and AlpacaEval (Li et al., 2023) are common examples of static benchmarks. Chatbot Arena is the platform introduced in this paper.\n' +
      '\n' +
      'Additionally, developing scalable, incremental, and efficient ranking systems is essential for evaluating a large number of models. Moreover, ensuring the quality of human evaluations is crucial given the noisy nature of human preferences.\n' +
      '\n' +
      'To this end, we introduce Chatbot Arena, a benchmarking platform for LLMs that features anonymous, randomized battles in a crowdsourced setting. Chatbot Arena is a free website open to all users.1 On this website, a user can ask a question and get answers from two anonymous LLMs. Afterward, the user casts a vote for the model that delivers the preferred response, with the models\' identities revealed only after voting. This crowdsourced method effectively gathers a diverse array of fresh user prompts, accurately reflecting real-world LLM applications. Armed with this data, we employ a suite of powerful statistical techniques, ranging from the statistical model of Bradley and Terry (1952) to the E-values of Vovk and Wang (2021), to estimate the ranking over models as reliably and sample-efficiently as possible. With these tools in hand, we have designed efficient sampling algorithms specifically to select model pairs in a way that accelerates the convergence of rankings while retaining statistical validity.\n' +
      '\n' +
      'Footnote 1: [https://chat.lmsys.org](https://chat.lmsys.org)\n' +
      '\n' +
      'We conduct a thorough analysis of the collected data to ensure the credibility of our platform. We demonstrate that the user-generated questions are sufficiently diverse to encompass a wide range of LLM use cases and are sufficiently challenging to differentiate between models. Furthermore, we confirm that the crowd-sourced votes are highly consistent with expert evaluations.\n' +
      '\n' +
      'We have been running our system since Apr 2023 and have received over 240K votes from about 90K users in over 100 different languages as of Jan 2024. To encourage user engagement, we have made over 50 state-of-the-art models available for free. We also collaborate with leading model developers such as OpenAI, Google, Anthropic, Mistral, Hugging Face, and various universities, incorporating their latest models into our platform. We keep the community engaged by routinely updating the leaderboard, publishing analytical blogs, releasing datasets, and sharing information via tweets. Because of its unique and significant value, our leaderboard has emerged as one of the most referenced in the LLM field and has become a benchmark for the industry. We commit to making our data and code available, ensuring that this platform is open-source and open-accessible.\n' +
      '\n' +
      'We make the following contributions:\n' +
      '\n' +
      '* We build the first large-scale crowd-sourced live LLM evaluation platform with over 1M users visit.2 Footnote 2: The number was estimated by Google Analytics as of March 2024. Note that user visit may not convert to votes as our website also offers “direct chat” mode.\n' +
      '* We conduct an in-depth analysis of the collected data, including prompt diversity, quality, vote quality, and insights on human feedback.\n' +
      '* We will publicly release a human preference dataset with over 100K pairwise votes collected from Chatbot Arena.\n' +
      '* We design an efficient sampling algorithm that actively chooses which model pairs to show, such that our sample efficiency improves, sometimes to a large degree.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '**LLM Benchmarks.** We briefly review the common LLM benchmarks, following the classification presented in Figure 1. The most prevalent benchmarks are static, ground-truth-based ones, typically in the form of multiple-choice questions or question-answering tasks with predefined answers and test cases. These benchmarks encompass a range of topics including language understanding, mathematics, coding, and logical reasoning. Prominent examples in this category are MMLU (Hendrycks et al., 2020), HellaSwag (Zellers et al., 2019), GSM-8K (Cobbe et al., 2021), BigBench (Srivastava et al., 2023), AGIEval (Zhong et al., 2023), and HumanEval (Chen et al., 2021). Benchmarks focusing on safety, such as ToxicChat (Lin et al., 2023), and comprehensive suites like HELM (Liang et al., 2022), also exist. In addition to closed-ended questions, benchmarks can include open-ended questions that are evaluated by human judgment, which can be rated by experts or crowd workers such as Amazon Mechanical Turk (Karpinska et al., 2021; Geng et al., 2023; Wang et al., 2023). The recent trend includes utilizing GPT-4 for approximating human judgment (Chiang and Lee, 2023), with notable instances being MT-Bench (Zheng et al., 2023b) and AlpacaEval (Li et al., 2023). In addition to static benchmarks, live benchmarks that include fresh questions are also available. These questions can be obtained from annual exams or weekly online contests such as Codeforces (Li et al., 2022; Huang et al., 2023). They can also be sourced from human interaction. Some studies have explored using live human interaction for reinforcement learning from human preference (Bai et al., 2022; Ouyang et al., 2022; Touvron et al., 2023). However, these studies are typically limited to specific organizations. In this paper, we introduce Chatbot Arena, the first open, large-scale, and crowdsourced benchmark platform that utilizes live human interaction.\n' +
      '\n' +
      '**Risks of Static Benchmarks.** Static benchmarks have certain issues, including contamination, saturation, overfitting, and a lack of human alignment (Yang et al., 2023; Oren et al., 2023). DynaBench (Kiela et al., 2021) identifies these challenges and recommends the use of a live benchmark that incorporates a human-in-the-loop approach for classical NLP benchmarks. Our system adopts a similar spirit. However, our focus is on chatting with LLMs, and we implement this on a significantly larger user scale.\n' +
      '\n' +
      '**Ranking System.** Ranking systems have been a well-studied topic in statistics. Related topics include probability models (Hunter, 2004; Rao and Kupper, 1967), rank elicitation (Szorenyi et al., 2015; Busa-Fekete et al., 2014a;b), and online experiment design (Chernoff, 1992; Karimi et al., 2021). The Elo rating system has also been used for LLMs (Bai et al., 2022; Boubdir et al., 2023). Contributing to this literature, we introduce techniques for accelerating ranking convergence and detecting abnormalities, specifically applied to large-scale, real-world settings of LLMs.\n' +
      '\n' +
      '**Human Preference Dataset.** Owing to the significance of human preferences, several datasets and analyses exist that incorporate human preferences. These include OpenAssistant (Kopf et al., 2023), HH-RLHF (Bai et al., 2022), LMSYS-Chat-1M (Zheng et al., 2023a), and synthetic approximations of human preferences like UltraFeedback (Cui et al., 2023) and Nectar (Zhu et al., 2023). Our prior data release, LMSYS-Chat-1M (Zheng et al., 2023a), is similarly collected via crowdsourcing. However, LMSYS-Chat-1M comprises solely conversations and lacks human preference data, rendering it unsuitable for direct use in ranking studies. This paper focuses on the analysis of preference data for ranking purposes.\n' +
      '\n' +
      '## 3 Human Preference Data Collection\n' +
      '\n' +
      'In this section, we discuss our interface design to collect human preferences and present summary statistics.\n' +
      '\n' +
      '### Interface\n' +
      '\n' +
      'Chatbot Arena crowd-sources feedback from users for model evaluation. Our goal is to design an ease-of-use interface to reduce friction for users to contribute data. Since we collect feedback from many users, it is difficult to set a consistent grading rubric across different people. Hence, we adopt a pairwise comparison mechanism where users only need to compare two model responses and vote for the better one, instead of requiring users to provide an absolute score.\n' +
      '\n' +
      'In each battle, two anonymous models are sampled. To encourage data diversity, we do not preset any input prompt on the website. Users are free to input any prompt to the two models. We believe this creates incentives for user engagement, particularly given that we offer a free service. It also helps us collect a diverse set of inputs representing real-world usage. After models provide their answers, user compare them side-by-side and vote for the preferred answer. If a user cannot choose in the first turn, the user can continue chatting until identifying a winner. For those who are unsure, we also present two buttons, "tie" or "both are bad." Figure 8 shows a screenshot of our interface. Before using our service, users are required to accept terms of use, which gives us their consent to release the data publicly.\n' +
      '\n' +
      '### Data Statistics\n' +
      '\n' +
      'We began collecting data in April 2023. As of Jan 2024, we have received around 240K votes from over 90K users. Our data involves more than 50 models, including both proprietary models like GPT-4, Claude, and Gemini, as well as open models such as LLaMA and Mistral. These conversations cover more than 100 languages, with 77% being in English, 5% in Chinese, and the remaining languages, such as Russian, German, Spanish, French, and Japanese, each representing less than 2% of the total. Each data point includes multi-turn conversations between the user and two LLMs, and a vote to indicate which model the user prefers. We summarize statistics in Table 1 along with other existing human preference datasets.\n' +
      '\n' +
      'Figure 10 in the Appendix shows the vote count per model. On average, 8K votes are collected for each model. In Figure 2, we select a set of representative models and present their win rate and the number of battles. Note that we employ non-uniform sampling to concentrate votes on model pairs that have similar performance due to higher uncertainty. This helps us reduce the number of votes required to reach stable results. We later develop an adaptive sampling method and demonstrate its effectiveness against random sampling. See Section 5 for further analysis.\n' +
      '\n' +
      'To ensure anonymity, we use keywords to filter out conversations containing model identity such as model name (e.g., GPT, Claude) or companies (e.g., OpenAI, Anthropic). To avoid misuse, we adopt OpenAI moderation API to flag conversations that contain unsafe content. The flagged user requests account for 3% of the total requests. Figure 9 in the Appendix shows the number of valid user votes over time, where we get 1-2K votes per day in recent months and spikes as we introduce new models or leaderboard updates.\n' +
      '\n' +
      '## 4 From Pairwise Comparisons to Rankings\n' +
      '\n' +
      'Our data consists of pairwise comparisons--but how can we use these comparisons to recover a ranking over all \\(M\\) models? This is a well-studied topic in the literature on learning to rank (Liu et al., 2009), and we present our perspective here. We let \\(\\mathcal{A}=\\{(m,m^{\\prime}):m<m^{\\prime}\\text{ and }m,m^{\\prime}\\in[M]\\}\\) denote our comparative data set.\n' +
      '\n' +
      'We consider a sequential setting, where at time \\(t\\in\\mathbb{N}\\), we serve the human a pair of models \\(A_{t}\\in\\mathcal{A}\\) (which we pick), and in turn we observe the human\'s response \\(H_{t}\\in[0,1]\\). As an example, we might have that \\(A_{t}=(1,2)\\) and \\(H_{t}=1\\), indicating that the human prefers model 2 over model 1. In the ensuing text, we will primarily focus on the binary case--where \\(H_{t}\\in\\{0,1\\}\\)--but our approach will generalize to any form of feedback, including the possibility of allowing the human to express different degrees of preference or to say the models are tied.\n' +
      '\n' +
      'One critical goal is to estimate the _win matrix_: \\(\\theta^{*}(a)=\\mathbb{E}[H_{t}\\mid A_{t}=a]\\), for all \\(a\\in\\mathcal{A}\\); see the left panel of Figure 2 for an illustration of the (empirical) win matrix. In the binary case, the \\(a\\) entry in the win matrix corresponds to the probability the human prefers model \\(a_{2}\\) to \\(a_{1}\\) when shown the pair \\(a\\). Finding the win matrix is a relatively straightforward mean-estimation problem; we will provide details in Section 5.\n' +
      '\n' +
      'Formally, consider a _score_\\(s(\\mathbb{P})\\in\\mathbb{R}^{M}\\), where \\(\\mathbb{P}\\) is a joint distribution over \\(A\\) and \\(H\\) (by default, we will target a uniform distribution over \\(\\mathcal{A}\\)). Each model has a true score \\(s(\\mathbb{P})_{m}\\), and better models will have higher scores. In particular, we have the rank of model \\(m\\):\n' +
      '\n' +
      '\\[\\mathrm{rank}(\\mathbb{P})_{m}=1+\\sum_{m^{\\prime}\\in[M]}\\mathds{1}\\left\\{s( \\mathbb{P})_{m^{\\prime}}>s(\\mathbb{P})_{m}\\right\\}. \\tag{1}\\]\n' +
      '\n' +
      'The best model has rank \\(1\\). If there is another model tied for best, they will both get assigned rank \\(1\\).\n' +
      '\n' +
      '**Picking a score.** A standard score function in this setting is the vector of Bradley-Terry (BT) coefficients (Bradley and Terry, 1952). In the Bradley-Terry model, \\(H_{t}\\in\\{0,1\\}\\), and the probability model \\(m\\) beats model \\(m^{\\prime}\\) is modeled via a logistic relationship:\n' +
      '\n' +
      '\\[\\mathbb{P}(H_{t}=1)=\\frac{1}{1+e^{\\xi_{m^{\\prime}}-\\xi_{m}}}, \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\xi\\) is an \\(M\\)-length vector of so-called BT coefficients. Without loss of generality, we take \\(\\xi_{1}=0\\) (since the model is invariant to addition in \\(\\xi\\)). Our goal is to estimate the population Bradley-Terry coefficients, i.e., those that minimize the binary cross-entropy:\n' +
      '\n' +
      '\\[s(\\mathbb{P})=\\operatorname*{argmin}_{\\xi}\\mathbb{E}_{(A,H)\\sim\\mathbb{P}} \\left[\\ell\\left(H,\\frac{1}{1+e^{\\xi_{A_{2}}-\\xi_{A_{1}}}}\\right)\\right], \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\ell\\) is the binary cross-entropy loss, \\(\\ell(h,p)=-(h\\log(p)+(1-h)\\log(1-p))\\).\n' +
      '\n' +
      'Although the BT model technically assumes a parametric form for the model win rates, the seminal results of Huber et al. (1967); White (1982) show that maximum likelihood estimators are still asymptotically normal even when these assumptions _do not_ hold, so long as the so-called "sandwich" covariance matrix is used; see Section 5 for details, and see Appendix B for a nonparametric extension of the Bradley-Terry model. Finally, we remark that previous evolutions of our online interface have reported different ranking scores, such as the Elo score (Elo, 1967) instead of the BT coefficients. We made this change because the BT coefficients are better for the purpose of statistical estimation.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Dataset & \\# Convs & \\# Models & \\# Users & \\# Langs & Avg. \\# Turns per Sample & Avg. \\# Tokens per Prompt & Avg. \\# Tokens per Response \\\\ \\hline Anthropic HH & 338,704 & - & 143 & 1 & 2.3 & 18.9 & 78.9 \\\\ OpenAssistant & 66,497 & - & 13,500 & 35 & - & 36.9 & 214.2 \\\\ \\hline Chatbot Arena (20240121) & 243,329 & 50 & 90,051 & 149 & 1.3 & 94.9 & 269.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Statistics of human preference datasets, including Anthropic HH (Bai et al., 2022), OpenAssistant Conversations (Köpf et al., 2023), and Chatbot Arena (as of 2024/1/21). The tokens are counted by Llam2’s tokenizer. “Conv” = Conversation. “Lang” = Language.\n' +
      '\n' +
      'Figure 2: Win-rate (left) and battle count (right) between a subset of models in Chatbot Arena.\n' +
      '\n' +
      '## 5 Efficient Approximate Ranking\n' +
      '\n' +
      'In Section 4 we described how to calculate the win matrix, score, and rank. Now we describe our estimation procedures.\n' +
      '\n' +
      '**Win matrix estimation.** Estimation of the win matrix is relatively straightforward. Define \\(X_{t}(a)=\\frac{1}{P_{t}(a)}H_{t}\\mathds{1}\\left\\{A_{t}=a\\right\\}\\), where \\(P_{t}(a)\\) is the probability of sampling pair \\(a\\) at time \\(t\\), and \\(X_{t}\\) as the according vector. Then the estimator is\n' +
      '\n' +
      '\\[\\hat{\\theta}_{T}=\\frac{1}{T}\\sum_{t=1}^{T}X_{t}. \\tag{4}\\]\n' +
      '\n' +
      'Note that \\(\\mathbb{E}[X_{t}(a)]=\\theta^{*}(a)\\) for all \\(t\\), and thus \\(\\hat{\\theta}_{T}\\) is an unbiased estimator of \\(\\theta^{*}\\). We will furthermore estimate the covariance matrix as\n' +
      '\n' +
      '\\[\\widehat{\\Sigma}_{T}=\\frac{1}{T}\\sum_{t=1}^{T}(X_{t}-\\hat{\\theta}_{T})(X_{t}- \\hat{\\theta}_{T})^{\\top}. \\tag{5}\\]\n' +
      '\n' +
      'Under the appropriate regularity conditions, we have that\n' +
      '\n' +
      '\\[\\sqrt{T}\\widehat{\\Sigma}^{-1/2}(\\hat{\\theta}-\\theta^{*})\\rightarrow\\mathcal{N }(0,I_{d}), \\tag{6}\\]\n' +
      '\n' +
      'and we construct confidence intervals accordingly. For an understanding of the appropriate regularity conditions, see Durrett (2019), Theorem 8.2.8, where condition (ii) is trivially satisfied so long as \\(P_{t}(a)>\\epsilon>0\\), and condition (i) is implied by the almost-sure convergence of \\(P_{t}(a)\\) to a limiting distribution \\(P(a)\\).\n' +
      '\n' +
      '**Estimating the BT scores.** To estimate the BT coefficients, mirroring (3), we perform (reweighted) maximum likelihood estimation on our data points:\n' +
      '\n' +
      '\\[s(\\hat{\\mathbb{P}})=\\operatorname*{argmin}_{\\xi}\\sum_{t=1}^{T}\\frac{1}{P(A_{ t})}\\ell\\left(H_{t},\\frac{1}{1+e^{\\xi_{A_{t},2}-\\xi_{A_{t,1}}}}\\right), \\tag{7}\\]\n' +
      '\n' +
      'where \\(A_{t}\\sim P\\). We perform the inverse weighting by \\(P(A_{t})\\) because this allows us to target a score with a uniform distribution over \\(A\\).\n' +
      '\n' +
      'To compute confidence intervals on the BT coefficients, we employ two strategies: (1) the pivot bootstrap (DiCiccio and Efron, 1996), and (2) the "sandwich" robust standard errors outlined in Huber et al. (1967) (see also Freedman (2006) for an outline of the necessary technical assumptions). Ultimately, based on the results of a simulation study described in Appendix A, we choose to deploy the sandwich intervals due to their smaller size in large samples.\n' +
      '\n' +
      '**Approximate rankings.** Finally, we report an approximate ranking for each model that accounts for the uncertainty in the estimation of the score. Given an \\(M\\)-dimensional confidence set \\(\\mathcal{C}\\) satisfying\n' +
      '\n' +
      '\\[\\mathbb{P}(s(\\mathbb{P})\\in\\mathcal{C})\\geq 1-\\alpha, \\tag{8}\\]\n' +
      '\n' +
      'we extract an approximate ranking \\(R_{m}=1+\\sum_{m^{\\prime}\\in[M]}\\mathds{1}\\left\\{\\inf\\mathcal{C}_{m^{\\prime}} >\\sup\\mathcal{C}_{m}\\right\\}\\). The uniform validity of \\(\\mathcal{C}\\) directly implies that \\(\\mathbb{P}(\\exists m:R_{m}>\\operatorname{rank}(\\mathbb{P})_{m})\\leq\\alpha\\)--i.e., with high probability, no model\'s performance is understated. A guarantee on the other side--that no model\'s performance is overstated--is possible by interchanging the \\(\\inf\\) and \\(\\sup\\). To get the uniform confidence set, we construct the chi-squared interval implied by the central limit theorem using the sandwich estimate of the variance. In other words, we construct the interval \\(\\{\\xi:T\\left\\|\\hat{V}^{-1/2}(\\hat{\\xi}-\\xi)\\right\\|\\leq\\chi_{1-\\alpha,M-1}^{2}\\), where \\(\\hat{\\xi}\\) is our MLE of the BT coefficients and \\(\\hat{V}_{\\xi}\\) is the sandwich variance of the logistic regression.\n' +
      '\n' +
      '**Active sampling rule.** Our sampling rule was to choose the model pair \\(a\\in\\mathcal{A}\\) proportionally to the reduction in confidence interval size by sampling that pair:\n' +
      '\n' +
      '\\[P_{t}(a)\\propto\\sqrt{\\frac{\\hat{\\Sigma}_{t,a,a}}{|\\{t:A_{t}=a\\}|}}-\\sqrt{\\frac {\\hat{\\Sigma}_{t,a,a}}{|\\{t:A_{t}=a\\}|+1}}. \\tag{9}\\]\n' +
      '\n' +
      '### Detecting Anomalous Users\n' +
      '\n' +
      'On a different note, we take a first step towards identifying anomalous IP addresses in our dataset. In a dataset of \\(U\\) unique IPs, we let \\(\\text{IP}=\\{1,\\dots,U\\}\\) be the set of all IP addresses. Consider a "test" user, outside this database, who gives ratings \\(H_{1}^{\\prime},\\dots,H_{n}^{\\prime}\\) when presented actions \\(A_{1}^{\\prime},\\dots,A_{n}^{\\prime}\\). The idea of our procedure is to compare the distribution of ratings for the new user to the historical distribution of ratings for a given action. We let \\(\\mathcal{H}_{a}=\\{H_{t}:A_{t}=a\\}\\) and every time a user submits a vote, we calculate the following number:\n' +
      '\n' +
      '\\[p_{i}=\\frac{1}{|\\mathcal{H}_{A_{i}^{\\prime}}|+1}\\left(1+\\sum_{h\\in\\mathcal{H}_ {A_{i}^{\\prime}}}\\mathds{1}\\left\\{h\\geq H_{i}^{\\prime}\\right\\}\\right). \\tag{10}\\]\n' +
      '\n' +
      'Under the null hypothesis that \\(\\mathcal{H}_{A_{i}^{\\prime}}\\) is exchangeable with \\(H_{i}^{\\prime}\\), \\(p_{i}\\) is a valid p-value (see Appendix C for a proof). Furthermore, the dependence of these p-values asymptotically is negligible.\n' +
      '\n' +
      'With this p-value in hand, we can test against this null hypothesis sequentially by using Fisher\'s combination test (Fisher, 1928) along with a variant of the Bonferroni correction. In particular, for each user, after their \\(j\\)th vote, we compute \\(M_{j}=-2\\sum\\limits_{i=1}^{j}\\log(p_{i})\\). At 5 randomly chosen values of \\(j\\) between 1 and 100, we identify a user as anomalous if \\(M_{j}\\geq\\chi_{2j,1-\\alpha/5}^{2}\\). (The times are randomly chosen, as to avoid anomalous users strategizing to hack this p-value.) Despite the heuristic application of this procedure, it seems to work well in our small-scale tests reported in Table 5.\n' +
      '\n' +
      '## 6 Data Analysis\n' +
      '\n' +
      'To examine whether Arena\'s crowdsourced data reflects real-world use cases, we conduct topic modeling on the user prompts. We show how effective are these prompts in distinguishing models. Lastly, we validate the vote quality by relabeling data with experts.\n' +
      '\n' +
      '### Topic Modeling on User Prompts\n' +
      '\n' +
      'To study the prompt diversity, we build a topic modeling pipeline with BERTopic3(Grootendorst, 2022). We start with transforming user prompts into representation vectors using OpenAI\'s text embedding model (text-embedding-3-small). To mitigate the curse of dimensionality for data clustering, we employ UMAP (Uniform Manifold Approximation and Projection) (McInnes et al., 2020) to reduce the embedding dimension from 1,536 to 5. We then use the hierarchical density-based clustering algorithm, HDBSCAN, to identify topic clusters with minimum cluster size 32. Finally, to obtain topic labels, we sample 10 prompts from each topic cluster and feed into GPT-4-Turbo for topic summarization.\n' +
      '\n' +
      'Footnote 3: [https://github.com/MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)\n' +
      '\n' +
      'The pipeline identifies 600 clusters covering a wide range of topics including poetry writing, coding, math, and medical queries. We present the top-16 topic clusters in Figure 3. We observe that the largest cluster only accounts for 1% of the entire set and the rest quickly drop to <0.5%, and the similarity between clusters is small, showing a long-tail and diverse distribution. Due to space limit, we present the similarity matrix and cluster hierarchy of top-64 clusters in Figure 11 and 12 in Appendix.\n' +
      '\n' +
      '### Can Arena Prompts Distinguish Models?\n' +
      '\n' +
      'Next, we study how effective are these topic clusters in distinguishing models strengths. Constructing challenging prompts has become increasingly difficult due to LLMs\' fast growing capabilities. For example, open models such as Llama-2-70b-chat can likely answer inquiries about movie or travel recommendation as good as GPT-4, but not in other domains such as reasoning or coding. To demonstrate, we sample 30 prompts from seven topic clusters and compare the performance of Llama-2-70b-chat and GPT-4. To control variables, we factor out user votes and consider LLM-as-judge (Zheng et al., 2023b) to evaluate model response. Results are shown in Table 2, where we see GPT-4 has significantly higher win-rate (up to 97%) in clusters that require coding and reasoning skills. On the other hand, for clusters with less problem-solving tasks, GPT-4 win-rate drops to below 60%. We show examples in Appendix D.1. This result shows models may exhibit varying strengths in different areas, but also highlights some of the topic clusters in Chatbot Arena are effective in differentiate models.\n' +
      '\n' +
      '**Building Challenging Benchmark.** To further demonstrate the prompt quality, we show it is possible to construct a challenging benchmark with crowd-sourced user prompts. To ensure both topic coverage and quality, we first run the topic modeling pipeline and follow a similar procedure in Zheng et al. (2023a) to select challenging questions sampled from each topic cluster. Examples prompts and evaluation procedures can be found in the Appendix D.2 and Appendix D.3, respectively. We observe the selected prompts are highly effective in differentiating models. In Figure 4, we compare Arena bench against a widely used LLM benchmark, MTBench (Zheng et al., 2023b). We can see that Arena Bench effectively reveals a significant gap in performance between proprietary and the strongest open models.\n' +
      '\n' +
      '### Validating Vote Quality\n' +
      '\n' +
      'To assess the quality of crowdsourced votes, we randomly selected 160 battles between GPT-4-Turbo and Llama-2-13B, as well as GPT-4-Turbo and GPT-3.5-Turbo-0613. We then asked experts4 to label their preference per comparison. The experts were given the prompts and answers blindly, and asked to carefully fact-check model\'s answer with external resources like search engine. Manually labeling each\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline Topic Cluster & Win-rate & Size \\\\ \\hline Python Game Programming Challenge & 96.7\\% & 0.2\\% \\\\ C/C++ Process Multi-Threading & 86.7\\% & 0.3\\% \\\\ SQL Query Database Assistance & 73.3\\% & 0.2\\% \\\\ Poetry Writing Prompts & 66.7\\% & 1.1\\% \\\\ Python Coding Basics & 65.0\\% & 0.2\\% \\\\ Linguistic Analysis \\& Wordplay & 58.3\\% & 0.7\\% \\\\ Travel Itinerary Planning & 58.3\\% & 0.4\\% \\\\ Movie Recommendations \\& Ratings & 53.3\\% & 0.2\\% \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: GPT-4-0613’s win-rate against Llama-2-70b-chat on 30 sample prompts from various topic clusters. We use GPT-4-turbo as judge to evaluate model responses in pairwise comparison.\n' +
      '\n' +
      'Figure 3: Similarity matrix of top-16 topic clusters. The number followed by the topic label represents the cluster size in percentage. Note that similarity is computed by cluster’s centroid embeddings, hence diagonals are always one.\n' +
      '\n' +
      'data point took on average 3-5 minutes. For reference, we also use GPT-4 as a judge for pairwise comparisons. The agreement rate between crowd-users, experts, and GPT-4-judge are presented in Table 3. The corresponsing win-rate are shown in Table 4.\n' +
      '\n' +
      'To summarize, we observe high agreement rates (72% to 83%) between Arena crowd-user and experts in both setup. Note that agreement rates between two experts are around similar levels (79.4% and 89.8%). As for the 10%-20% disagreement between experts, it is mostly due to some user prompts don\'t have a ground truth answer. Depending on the preference of the evaluator, sometimes both answers can be argued as being better than the other one, such as the examples in D.4. The gap between crowd-vs-expert agreement rate and expert-vs-expert agreement rate (5%-10%) is mostly attributed to crowd user making mistakes or overlooking factual errors in model\'s response. Overall, the agreement rates presented in Table 3 validate the decent quality of crowd-sourced votes in Chatbot Arena.\n' +
      '\n' +
      '## 7 Experiments\n' +
      '\n' +
      '### Ranking system\n' +
      '\n' +
      '**Computing the rank on real data.** In this section, we report results from our experiments on approximate ranking. For this experiment, we ran a replay of \\(T=213,576\\) historical votes from our online platform and calculate the BT coefficients using our earlier-described estimation algorithm with confidence intervals; see Figure 5 for these intervals (with and without multiplicity correction; the formal notion of approximate ranking technically requires multiplicity correction, but it makes the intervals looser).\n' +
      '\n' +
      '**Evaluating the coverage of the intervals.** A natural follow-up question is whether or not the intervals are doing their job correctly: whether they cover the true BT coefficients with probability at least (and almost exactly) \\(1-\\alpha\\). Of course,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r} \\hline \\hline Baseline & Arena User & Expert 1 & Expert 2 & GPT-4 \\\\ \\hline \\hline Llama-2-13b & 81.2\\% & 89.4\\% & 86.9\\% & 78.8\\% \\\\ GPT-3.5-Turbo & 76.3\\% & 82.5\\% & 89.4\\% & 79.4\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: GPT-4-Turbo’s win-rate across crowd-user, gpt-4 judge, and experts on pairwise battles against Llama-2-13b and GPT-3.5-Turbo-0613.\n' +
      '\n' +
      'Figure 4: Model’s performance between Arena Bench and MT-Bench, showing an increased gap between open and proprietary models. Both uses GPT-4 as judge.\n' +
      '\n' +
      'Figure 5: Intervals for the BT coefficients with and without multiplicity correction. The multiplicity correction, in this case a chi-square CLT interval, is technically required for the purpose of calculating the ranking, because it ensures _all_ scores are simultaneously contained in their intervals (and the ranking is a function of all the scores). However, it induces extra conservatism, so we report both intervals.\n' +
      '\n' +
      'this cannot be evaluated on real data, so we run a simulation. A vector of BT coefficients is drawn, with each coordinate sampled i.i.d. from a distribution \\(\\text{beta}(1/\\gamma,1/\\gamma)\\); we take \\(\\gamma=2\\) in Figure 6 (and we vary \\(\\gamma\\) in Appendix A). Given these coefficients, a dataset is synthesized, and the coverage and average width are computed for each of 20 trials. The results can be seen in Figure 6 for the uncorrected intervals The coverage of the intervals behaves as expected, centering around \\(1-\\alpha\\), regardless of the number of models. Meanwhile, the more models are included, the larger the intervals become.\n' +
      '\n' +
      '**Evaluating the active sampling rule.** Next, we discuss the evaluation of our active sampling rule as Equation (9) for win matrix estimation. We evaluate this sampling rule by taking the best fit BT coefficients to our 213,576 point sized holdout set, and then sampling from that distribution using our active sampling algorithm. The results are displayed in Figure 7. It is hard to tell by looking at plots, but the improvement is substantial: To estimate \\(\\theta^{*}\\) to a precision of 0.2, random needs 6,800 samples and adaptive needs 4,400 samples; meanwhile to estimate the score to a precision of 0.3, random needs 17,200 samples and adaptive needs 16,400 samples. Thus, the random baseline requires 54% and 5% more data to achieve the same level of precision, respectively. One can see from the plots in Figure 7 that these results are not cherry-picked: the sample-efficiency of our method is better at all values on the horizontal axis.\n' +
      '\n' +
      '### Anomalous Users Detection\n' +
      '\n' +
      'We evaluate the outlier detection method in Section 5.1. We construct the evaluation set by manually identifying 25 anomalous users whose inputs are highly repetitive or meaningless (e.g., asking "hi" for 100 times or inputting garbled texts). We randomly sample 25 normal users with at least 50 votes, and inspect their input prompts to ensure no abnormal behaviors. As mentioned in Section 5.1, per user we compute five \\(M_{j}\\) and identify the user as anomalous if \\(M_{j}\\geq\\chi^{2}_{2j,1-\\alpha/5}\\). We present results of two different \\(\\alpha\\) (i.e., the significance level) in Table 5. We find the detection method effective (e.g., reaching 90% true positive and 60-70% true negative rate). We inspect the false negative errors and find those are from users do not always behave abnormally, making them harder to detect.\n' +
      '\n' +
      '## 8 Discussion\n' +
      '\n' +
      '**Limitations.** Although our user base is extensive, we anticipate that it will primarily consist of LLM hobbyists and researchers who are eager to experiment with and evaluate the latest LLMs. This inclination may result in a biased distribution of users. Additionally, despite the wide array of topics encompassed by the prompts discussed in previous sections, the data predominantly comes from our online chat interface. This source might not accurately reflect the real-world usage of LLMs in production environments or specialized domains, potentially leading to a skewed prompt distribution. Moreover, our study concentrates on assessing the helpfulness of LLMs but overlooks their safety aspects. We recognize the possibility and necessity of a parallel\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r} \\hline \\hline \\(\\alpha=0.1\\) & Pred. Positive & Pred. Negative \\\\ \\hline Actual Positive & 13/14 & 12/36 \\\\ Actual Negative & 1/14 & 24/36 \\\\ \\hline \\(\\alpha=0.3\\) & Pred. Positive & Pred. Negative \\\\ \\hline Actual Positive & 21/29 & 4/21 \\\\ Actual Negative & 8/29 & 17/21 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Confusion matrix of different \\(\\alpha\\). “Pred.” means predicted. Positive means anomalous and negative means normal.\n' +
      '\n' +
      'Figure 6: Intervals for the BT coefficients as a function of the number of samples and the number of models \\(M\\).\n' +
      '\n' +
      'Figure 7: Interval widths on the win matrix (upper figure) and on the BT coefficients (lower figure) as a function of the number of samples, for random sampling and also adaptive sampling. Improvements from adaptive sampling can be seen in both cases, although they are more subtle on the scale of the score.\n' +
      '\n' +
      'mechanism to evaluate the safety of these models.\n' +
      '\n' +
      '**Future Directions.** In our future work, we plan to develop comprehensive topic leaderboards and establish a dedicated section for multimodal and agent-based LLMs in more dynamic, gamified settings, catering to more complex tasks. We also believe our approach to detecting harmful users could be improved and made more formally rigorous by using the theory of nonnegative supermartingales and E-values (Howard et al., 2020; Waudby-Smith and Ramdas, 2020; Vovk and Wang, 2021; Ramdas et al., 2023); this would deal with the dependence, but the variants we tried did not perform well in terms of power.\n' +
      '\n' +
      '## 9 Conclusion\n' +
      '\n' +
      'In this paper, we present Chatbot Arena, an open platform for evaluating LLMs through crowdsourced, pairwise human preferences. We conduct an in-depth analysis of the crowdsourced user prompts and preference votes to validate the diversity and quality. We develop an efficient model sampling and ranking algorithm. Our dataset including 100K pairwise preference votes will be released for future research.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'This project is supported by sponsorship from Kaggle, MBZUAI, a16z, Together AI, Anyscale, and HuggingFace. This project is also partly supported by Accenture, AMD, Google, IBM, Intel, Microsoft, Samsung SDS, SAP, Uber, and VMware. The authors would like to thank Siyuan Zhuang for insightful discussion and Tijana Zrnic for helpful feedback on the manuscript.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Bai et al. (2022) Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.\n' +
      '* Boubdir et al. (2023) Boubdir, M., Kim, E., Ermis, B., Hooker, S., and Fadaee, M. Elo uncovered: Robustness and best practices in language model evaluation, 2023.\n' +
      '* Bradley and Terry (1952) Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 39(3/4):324-345, 1952.\n' +
      '* Busa-Fekete et al. (2014) Busa-Fekete, R., Huellermeier, E., and Szorenyi, B. Preference-based rank elicitation using statistical models: The case of mallows. In Xing, E. P. and Jebara, T. (eds.), _Proceedings of the 31st International Conference on Machine Learning_, volume 32 of _Proceedings of Machine Learning Research_, pp. 1071-1079, Bejing, China, 22-24 Jun 2014a. PMLR. URL [https://proceedings.mlr.press/v32/busa-fekete14.html](https://proceedings.mlr.press/v32/busa-fekete14.html).\n' +
      '* Busa-Fekete et al. (2014) Busa-Fekete, R., Huellermeier, E., and Szorenyi, B. Preference-based rank elicitation using statistical models: The case of mallows. In Xing, E. P. and Jebara, T. (eds.), _Proceedings of the 31st International Conference on Machine Learning_, volume 32 of _Proceedings of Machine Learning Research_, pp. 1071-1079, Bejing, China, 22-24 Jun 2014a. PMLR. URL [https://proceedings.mlr.press/v32/busa-fekete14.html](https://proceedings.mlr.press/v32/busa-fekete14.html).\n' +
      '* Busa-Fekete et al. (2014) Busa-Fekete, R., Huellermeier, E., and Szorenyi, B. Preference-based rank elicitation using statistical models: The case of mallows. In Xing, E. P. and Jebara, T. (eds.), _Proceedings of the 31st International Conference on Machine Learning Research_, pp. 1071-1079, Bejing, China, 22-24 Jun 2014b. PMLR. URL [https://proceedings.mlr.press/v32/busa-fekete14.html](https://proceedings.mlr.press/v32/busa-fekete14.html).\n' +
      '* Chen et al. (2021) Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_, 2021.\n' +
      '* Chernoff (1992) Chernoff, H. _Sequential Design of Experiments_, pp. 345-360. Springer New York, New York, NY, 1992. ISBN 978-1-4612-4380-9. doi: 10.1007/978-1-4612-4380-9.27. URL [https://doi.org/10.1007/978-1-4612-4380-9_27](https://doi.org/10.1007/978-1-4612-4380-9_27).\n' +
      '* Chiang and Lee (2023) Chiang, C.-H. and Lee, H.-y. Can large language models be an alternative to human evaluations? In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 15607-15631, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.870. URL [https://aclanthology.org/2023.acl-long.870](https://aclanthology.org/2023.acl-long.870).\n' +
      '* Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.\n' +
      '* Cui et al. (2023) Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z., and Sun, M. Ultrafastback: Boosting language models with high-quality feedback, 2023.\n' +
      '* DiCiccio and Efron (1996) DiCiccio, T. J. and Efron, B. Bootstrap confidence intervals. _Statistical science_, 11(3):189-228, 1996.\n' +
      '* Durrett (2019) Durrett, R. _Probability: theory and examples_, volume 49. Cambridge university press, 2019.\n' +
      '* Elo (1967) Elo, A. E. The proposed uscf rating system, its development, theory, and applications. _Chess Life_, 22(8):242-247, 1967.\n' +
      '* Fisher (1928) Fisher, R. A. _Statistical methods for research workers_. Number 5. Oliver and Boyd, 1928.\n' +
      '* Freedman (2006) Freedman, D. A. On the so-called "huber sandwich estimator"\' and "robust standard errors". _The American Statistician_, 60(4):299-302, 2006.\n' +
      '\n' +
      '* Gemini et al. (2023) Gemini, T., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Geng et al. (2023) Geng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel, P., Levine, S., and Song, D. Koala: A dialogue model for academic research. Blog post, April 2023. URL [https://bair.berkeley.edu/blog/2023/04/03/koala/](https://bair.berkeley.edu/blog/2023/04/03/koala/).\n' +
      '* Grootendorst (2022) Grootendorst, M. Bertopic: Neural topic modeling with a class-based tf-idf procedure. _arXiv preprint arXiv:2203.05794_, 2022.\n' +
      '* Hendrycks et al. (2020) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In _International Conference on Learning Representations_, 2020.\n' +
      '* Howard et al. (2020) Howard, S. R., Ramdas, A., McAuliffe, J., and Sekhon, J. Time-uniform chernoff bounds via nonnegative supermartingales. 2020.\n' +
      '* Huang et al. (2023) Huang, Y., Lin, Z., Liu, X., Gong, Y., Lu, S., Lei, F., Liang, Y., Shen, Y., Lin, C., Duan, N., et al. Competition-level problems are effective llm evaluators. _arXiv preprint arXiv:2312.02143_, 2023.\n' +
      '* Huber et al. (1967) Huber, P. J. et al. The behavior of maximum likelihood estimates under nonstandard conditions. In _Proceedings of the fifth Berkeley symposium on mathematical statistics and probability_, volume 1, pp. 221-233. Berkeley, CA: University of California Press, 1967.\n' +
      '* 406, 2004. doi: 10.1214/aos/1079120141. URL [https://doi.org/10.1214/aos/1079120141](https://doi.org/10.1214/aos/1079120141).\n' +
      '* Karimi et al. (2021) Karimi, M. R., Gurel, N. M., Karlas, B., Rausch, J., Zhang, C., and Krause, A. Online active model selection for pre-trained classifiers. In _International Conference on Artificial Intelligence and Statistics_, pp. 307-315. PMLR, 2021.\n' +
      '* Karpinska et al. (2021) Karpinska, M., Akoury, N., and Iyyer, M. The perils of using Mechanical Turk to evaluate open-ended text generation. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 1265-1285, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 97. URL [https://aclanthology.org/2021.emnlp-main.97](https://aclanthology.org/2021.emnlp-main.97).\n' +
      '* Kiela et al. (2021) Kiela, D., Bartolo, M., Nie, Y., Kaushik, D., Geiger, A., Wu, Z., Vidgen, B., Prasad, G., Singh, A., Ringshia, P., et al. Dynabench: Rethinking benchmarking in nlp. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 4110-4124, 2021.\n' +
      '* Kopf et al. (2023) Kopf, A., Kilcher, Y., von Rutte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., Nagyfi, R., et al. Openassistant conversations-democratizing large language model alignment. _arXiv preprint arXiv:2304.07327_, 2023.\n' +
      '* Langley (2000) Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), _Proceedings of the 17th International Conference on Machine Learning (ICML 2000)_, pp. 1207-1216, Stanford, CA, 2000. Morgan Kaufmann.\n' +
      '* Li et al. (2023) Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval), 2023.\n' +
      '* Li et al. (2022) Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. Competition-level code generation with alpha-code. _Science_, 378(6624):1092-1097, 2022.\n' +
      '* Liang et al. (2022) Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et al. Holistic evaluation of language models. _arXiv preprint arXiv:2211.09110_, 2022.\n' +
      '* Lin et al. (2023) Lin, Z., Wang, Z., Tong, Y., Wang, Y., Guo, Y., Wang, Y., and Shang, J. ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation. In Bouamor, H., Pino, J., and Bali, K. (eds.), _Findings of the Association for Computational Linguistics: EMNLP 2023_, pp. 4694-4702, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp. 311. URL [https://aclanthology.org/2023.findings-emnlp.311](https://aclanthology.org/2023.findings-emnlp.311).\n' +
      '* Liu et al. (2009) Liu, T.-Y. et al. Learning to rank for information retrieval. _Foundations and Trends(r) in Information Retrieval_, 3(3):225-331, 2009.\n' +
      '* McInnes et al. (2020) McInnes, L., Healy, J., and Melville, J. Umap: Uniform manifold approximation and projection for dimension reduction, 2020.\n' +
      '* OpenAI (2023) OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* O\'Hagan et al. (2020)\n' +
      '* Oren et al. (2023) Oren, Y., Meister, N., Chatterji, N., Ladhak, F., and Hashimoto, T. B. Proving test set contamination in black box language models. _arXiv preprint arXiv:2310.17623_, 2023.\n' +
      '* Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022.\n' +
      '* Ramdas et al. (2023) Ramdas, A., Grunwald, P., Vovk, V., and Shafer, G. Game-theoretic statistics and safe anytime-valid inference. _Statistical Science_, 38(4):576-601, 2023.\n' +
      '* Rao & Kupper (1967) Rao, P. V. and Kupper, L. L. Ties in paired-comparison experiments: A generalization of the bradley-terry model. _Journal of the American Statistical Association_, 62(317):194-204, 1967. doi: 10.1080/01621459.1967.10482901.\n' +
      '* Srivastava et al. (2023) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _Transactions on Machine Learning Research_, 2023.\n' +
      '* Szorenyi et al. (2015) Szorenyi, B., Busa-Fekete, R., Paul, A., and Hullermeier, E. Online rank elicitation for blackettluce: A dueling bandits approach. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015. URL [https://proceedings.neurips.cc/paper_files/paper/2015/file/7eacb532570ff6858afd2723755ff790-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/7eacb532570ff6858afd2723755ff790-Paper.pdf).\n' +
      '* Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* Vovk & Wang (2021) Vovk, V. and Wang, R. E-values: Calibration, combination and applications. _The Annals of Statistics_, 49(3):1736-1754, 2021.\n' +
      '* Wang et al. (2023) Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 13484-13508, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754. URL [https://aclanthology.org/2023.acl-long.754](https://aclanthology.org/2023.acl-long.754).\n' +
      '* Waudby-Smith & Ramdas (2020) Waudby-Smith, I. and Ramdas, A. Estimating means of bounded random variables by betting. _arXiv preprint arXiv:2010.09686_, 2020.\n' +
      '* White (1982) White, H. Maximum likelihood estimation of misspecified models. _Econometrica: Journal of the econometric society_, pp. 1-25, 1982.\n' +
      '* Yang et al. (2023) Yang, S., Chiang, W.-L., Zheng, L., Gonzalez, J. E., and Stoica, I. Rethinking benchmark and contamination for language models with rephrased samples. _arXiv preprint arXiv:2311.04850_, 2023.\n' +
      '* Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence? In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pp. 4791-4800, 2019.\n' +
      '* Zheng et al. (2023a) Zheng, L., Chiang, W.-L., Sheng, Y., Li, T., Zhuang, S., Wu, Z., Zhuang, Y., Li, Z., Lin, Z., Xing, E. P., Gonzalez, J. E., Stoica, I., and Zhang, H. Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2023a.\n' +
      '* Zheng et al. (2023b) Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge with MT-bench and chatbot arena. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023b. URL [https://openreview.net/forum?id=uccHPGDlao](https://openreview.net/forum?id=uccHPGDlao).\n' +
      '* Zhong et al. (2023) Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. Agieval: A human-centric benchmark for evaluating foundation models. _arXiv preprint arXiv:2304.06364_, 2023.\n' +
      '* Zhu et al. (2023) Zhu, B., Frick, E., Wu, T., Zhu, H., and Jiao, J. Starling-7b: Improving llm helpfulness & harmlessness with traif, November 2023.\n' +
      '\n' +
      'Figure 8: Screenshot of Chatbot Arena.\n' +
      '\n' +
      'Figure 9: The number of votes over time\n' +
      '\n' +
      'Figure 11: Similarity matrix of top-64 topic clusters.\n' +
      '\n' +
      'Figure 10: The number of votes per model.\n' +
      '\n' +
      'Figure 12: Top-64 clusters visualized in hierarchy. x-axis represents the cosine similarity distance. y-axis shows the topic title per cluster summarized by gpt-4-turbo.\n' +
      '\n' +
      '## Appendix A Confidence Interval Simulation Study\n' +
      '\n' +
      'We conduct a simulation study to evaluate the bootstrap confidence intervals versus the sandwich estimator. To a large extent, both intervals are the same--indeed, their intervals are often identical to the naked eye. Nonetheless, in our experiments, there are some differences. First, in Figure 13, we conduct a replay study using the same 213576 data points mentioned in the main text.\n' +
      '\n' +
      'We also do a suite of experiments in simulation using the same beta generating process as in the main text, with \\(\\gamma=2\\). The result is shown in Figure 14; results are similar across many choices of the parameter \\(\\gamma\\) and the model strength, which indicates that both intervals will have good coverage and width in the practical conditions we would expose them to.\n' +
      '\n' +
      '## Appendix B The Nonparametric Bradley-Terry Model\n' +
      '\n' +
      '**Nonparametric Bradley-Terry.** We next consider a nonparametric extension of the Bradley-Terry (BT) model (Bradley and Terry, 1952) to the case where the ranking is not necessarily transitive. Let \\(\\mathcal{G}(m)\\) denote the set of all _paths_ to the model \\(m\\), i.e.,\n' +
      '\n' +
      '\\[\\mathcal{G}(m)=\\left\\{g\\in\\mathcal{B}^{M-1}:g_{i,1}\\neq g_{j,1},\\ \\forall i\\neq j,\\ \\text{ and}\\ g_{M-1,2}=m\\right\\}, \\tag{11}\\]\n' +
      '\n' +
      'where \\(\\mathcal{B}=\\mathcal{A}\\cup\\{(a_{2},a_{1}):a\\in\\mathcal{A}\\}\\). Each element of \\(\\mathcal{G}(m)\\) is a chain of model pairings that leads to \\(m\\); for example, if \\(m=5\\) and \\(M=6\\), one element of \\(\\mathcal{G}(m)\\) is \\(((1,2),(2,4),(4,3),(3,6),(6,5))\\). Our score function is given by the average\n' +
      '\n' +
      'Figure 14: Synthetic experiment. Coefficients are drawn from the BT-coefficient distribution \\(x\\) on the left. Coverage of the uncorrected intervals is shown in the middle. Line plots of set width are shown on the right, and they almost perfectly match.\n' +
      '\n' +
      'Figure 13: Replay experiment showing the intervals, coverage, and average interval sizes of the bootstrap and of the sandwich intervals. The sandwich intervals, though larger in small samples, are more stable, and in large samples, they actually become smaller. We use the multiplicity corrected version of both intervals, so they both have a coverage of 1. (Coverage here is calculated with respect to the BT coefficient solution on the full dataset, so it is not as meaningful as in the simulation plot below.)\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '## Appendix C Valid P-Value\n' +
      '\n' +
      'Consider the p-value\n' +
      '\n' +
      '\\[p_{i}=\\frac{1}{|\\mathcal{H}_{\\mathcal{A}_{i}^{\\prime}}|+1}\\left(1+\\sum_{h\\in \\mathcal{H}_{i}^{\\prime}}\\mathds{1}\\left\\{h\\geq H_{i}^{\\prime}\\right\\}\\right). \\tag{25}\\]\n' +
      '\n' +
      'We will prove that this p-value is valid, i.e., that \\(\\mathbb{P}(p_{i}\\leq t)\\leq t\\), under the null hypothesis that the vector \\(\\mathcal{H}^{\\prime}=(H_{t}:A_{t}=A_{i}^{\\prime})\\|(H_{i}^{\\prime})\\) is exchangeable, where \\(\\|\\) denotes the concatenation operator. First, notice that we can equivalently write\n' +
      '\n' +
      '\\[p_{i}=\\frac{1}{|\\mathcal{H}^{\\prime}|}\\sum_{h\\in\\mathcal{H}^{\\prime}}\\mathds{1 }\\left\\{h\\geq H_{i}^{\\prime}\\right\\}. \\tag{26}\\]\n' +
      '\n' +
      'We also have that \\(\\mathbb{P}(p_{i}\\leq t)\\leq\\mathbb{P}(p_{i}\\leq\\frac{k}{|\\mathcal{H}^{\\prime}|})\\), where \\(k=\\lfloor t|\\mathcal{H}^{\\prime}|\\rfloor/|\\mathcal{H}^{\\prime}|\\). Then, since the data points are exchangeable, we have that \\(H_{i}^{\\prime}\\) is uniformly distributed among the ranks of \\(\\mathcal{H}^{\\prime}\\), so \\(\\mathbb{P}(p_{i}\\leq\\frac{k}{|\\mathcal{H}^{\\prime}|})\\leq\\frac{k}{|\\mathcal{H }^{\\prime}|}\\leq t\\), completing the proof.\n' +
      '\n' +
      '## Appendix D Sample Prompts\n' +
      '\n' +
      '### Prompt Examples From Different Clusters\n' +
      '\n' +
      '**Example 1:** An example user prompt from the cluster "Python Game Programming Challenge". This prompt distinctively separates GPT-4-0613 performance from Llama-2-70b-chat. The user request "higher quality code" from each assistant. While GPT-4-Turbo provides a correct implementation along with helpful explanation, Llama-2-70b-chat fail to provide any program and provided a less helpful response overall.\n' +
      '\n' +
      '``` <|UserPrompt|> DevelopansowmangamesimilartotheHangmangamesubtdrawasasnowmanfigurestandingonthesnowinsteadofahangman.Italsodownloads(oruseslibrary)thecompleteEnglishdictionarytoselectwordsforthreelevelsofdifficulty.Thinkstep-by-step,takeadeepbreathandchooseahigherqualitycodeforeachstep. <|GPT-4-0613\'sAnswer|> Sure,here\'sstep-by-stepguideonhowtocreateaSnowmangamesimilartoHangman.We\'llusePythonforthisexample. Step1:Importnecessarylibraries "\'python importrandom importurllib.request "\'\' Step2:DownloadtheEnglishdictionary "\'python url="[https://raw.githubusercontent.com/dwyl/english-words/master/words.txt](https://raw.githubusercontent.com/dwyl/english-words/master/words.txt) " urllib.request.urlretrieve(url,\'words.txt\') \'Step3:Loadthewordsfromthedictionaryandcategorizethemintothereelevelsofdifficulty "\'python withopen(\'words.txt\')asword_file:  english_words=word_file.read().split() easy_words=[wordforwordinenglish_wordsif4<=len(word)<=6] medium_words=[wordforwordinenglish_wordsif7<=len(word)<=9] hard_words=[wordforwordinenglish_wordsiflen(word)>=10] \'\'\' Step4:Createfunctiontrandomlyselectawordbasedonthedificulty level "\'python defget_random_word(difficulty):  ifdifficulty==\'easy\':  word=random.choice(easy_words)  elifdifficulty==\'medium\':  word=random.choice(medium_words)  else:  word=random.choice(hard_words)  returnword.upper()...\'Step5:Createfunctiontprintthesnowman\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:19]\n' +
      '\n' +
      '<|Llama-2-70b-chat\'s Answer|> Sure! Here\'s a step-by-step guide to developing a snowman game similar to Hangman, using the complete English dictionary for three levels of difficulty. Step 1: Setting up the project\n' +
      '* Create a new Python project directory and initialize it with Git.\n' +
      '* Install the necessary libraries: + \'numpy\' for generating random numbers + \'nltk\' for accessing the English dictionary + \'pygame\' for creating the game graphics + \'pydub\' for playing sound effects (optional)\n' +
      '* Download the English dictionary dataset (e.g., from Kaggle) and store it in a file named \'dictionary.txt\'. Step 2: Loading the dictionary and generating words\n' +
      '* Use the \'nltk\' library to read the dictionary file and store the words in a list.\n' +
      '* Split the list into three parts: easy, medium, and hard words, based on their length and frequency of use.\n' +
      '* Shuffle the lists to randomize the order of the words. Step 3: Creating the game graphics\n' +
      '* Use \'pygame\' to create a window for the game, with a white background to represent the snow.\n' +
      '* Draw a snowman figure on the background, using black rectangles for the body, hat, and scarf.\n' +
      '* Add buttons for the player to submit their guesses. Step 4: Implementing the game logic\n' +
      '* Create a function to generate a random word from the appropriate difficulty level and display it on the screen with missing letters (e.g \'__________)\'.\n' +
      '* Create a function to check if a guessed letter appears in the word, and update the display accordingly (e.g., ____ X___)\'.\n' +
      '* Create a function to handle incorrect guesses, such as displaying a warning message or playing a sound effect.\n' +
      '* Create a function to win the game, such as displaying a congratulations message or playing a victory sound effect. Step 5: Adding sound effects (optional)\n' +
      '* Use \'pydub\' to play sound effects for various actions in the game, such as guessing a letter correctly or incorrectly.\n' +
      '* Consider using a library like \'pygame_mixer\' to simplify the process of playing sound effects in pygame.\n' +
      '\n' +
      'Step 6: Testing and refining the game\n' +
      '\n' +
      '* Test the game thoroughly to ensure that it works as expected, with no bugs or errors.\n' +
      '* Refine the game mechanics and graphics as needed, based on player feedback or personal preference.\n' +
      '* Consider adding additional features, such as keeping track of the player\'s score or implementing a time limit for each guess.\n' +
      '\n' +
      'Step 7: Packaging and distributing the game\n' +
      '\n' +
      '* Package the game as a standalone executable file using a tool like PyInstaller.\n' +
      '* Distribute the game to friends, family, or online communities, and gather feedback for future improvements.\n' +
      '\n' +
      'This is a high-level overview of the steps involved in creating a snowman game similar to Hangman. Of course, the details will depend on your specific implementation and the tools you choose to use. Good luck with your project!\n' +
      '\n' +
      ' <|GPT-4-Turbo\'s Judgment Verdict|> GPT-4-0613 Wins\n' +
      '\n' +
      '**Program Output:** Below is the terminal output of the snowman game programmed by GPT-4-0613, demonstrating its correctness. It is clear that GPT-4-0613\'s response is more helpful and relevant to the user\'s request.\n' +
      '\n' +
      '>> python snowman.py Choose difficulty (easy, medium, hard): easy The word contains 6 letters. Good job! The word contains that letter. _A______ Guess a letter: e Good job! The word contains that letter. _A______ E Guess a letter: d\n' +
      '\n' +
      ' ( ) Sorry, the word does not contain that letter. _A______ E Guess a letter: g \n' +
      '\n' +
      '**Example 2:** An example user prompt from the cluster "Python Game Programming Challenge". This prompt distinctively separates GPT-4-0613 performance from Llama-2-70b-chat. Due to limited space, the code from each assistant\'s response is omitted.\n' +
      '\n' +
      '```\n' +
      '<!UserPrompt|> Writeapythonfunctionthatprintsachristmastreeintheformofa triangleofasterisks,wheretheonlyinputisize. <!GPT-4-0613\'sCodeOutput|> >>>print_christmas_tree(5) * * *** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** ****\n' +
      '```\n' +
      '\n' +
      '**<!GPT-4-TurboVerdict|>** GPT-4-0613Wins ```\n' +
      '**Example 3:** An example user prompt from the cluster "Movie Recommendations & Rating". Both assistant retrieve the correct movie information and gave an almost identical explanation. Therefore, this prompt cannot distinguish GPT-4-0613\'s performance from Llama-2-70b-chat.\n' +
      '``` <!UserPrompt|> Whichfilmcameoutfirst,BlindShaftorTheMaskOfFuManchu? <!GPT-4-0613\'sAnswer|> TheMaskofFuManchuwasreleasedin1932,andBlindShaftwasreleasedin2003.Therefore,TheMaskofFuManchucameoutfirst. <!Llama-2-70b-chat\'sAnswer|> TheMaskOfFuManchucameoutfirst.Itwasreleasedin1932,whileBlindShaftwasreleasedin2003. <!GPT-4-TurboVerdict|> Tie\n' +
      '\n' +
      '**Example 4**: An example user prompt from the cluster "Travel Itinerary Planning". Both assistant provided an equally helpful itinerary. Therefore, this prompt cannot distinguish GPT-4-0613\'s performance from Llama-2-70b-chat.\n' +
      '\n' +
      '``` <|UserPrompt|> CanyoumakealistofthingsIcoulddowithmygirlfriendinLille, France? <|GPT-4-0613\'sAnswer|> Absolutely,herearesomesuggestionsforactivitiesyouandyourgirlfriend canenjoyinLille,France: 1.VisitthePalaisdesBeaux-Arts:ThisisoneofFrance\'slargestmuseums andit\'srightintheheheheartofLille.IthousescollectionsofEuropean paintings,amedievalandRenaissancecollection,ceramics,French sculpturesandmore. 2.ExploretheVieuxLille:Strollthroughthecharmingoldtownwithits Flemishinfluencedarchitecture,chicboutiques,anddelightfuleateries. 3.VisittheLilleCathedral(Basilique-catedraletNotre-Dame-de-la-Treille) :Thisbeautifulcathedralisamixofmodernandtraditional architecture. 4.EnjoyapicnatheCitadelPark:Thisisagreatplacetorelax,haveapicnic,andenjoytheoutdoors. 5.VisittheLAM(LilleMetropoleMuseedd\'artmoderne,d\'artcontemporainet d\'artbrut):Thismuseumishometoauniquecollectionofmodernandcontemporaryart. 6.TakeaboattourontheDeuleRiver:Thisisaromaticwaytoseethecityanditssurroundings. 7.VisittheMaisonNataledeCharlesdeGauille:ThisisthebirthplaceofCharlesdeGauilleandit\'snowamuseumdedicatedtohislifeandwork. 8.ExploretheMarchedeWazemmes:ThisisoneofthelargestmarketsinFranceandit\'safereplacetobuylocalproducts. 9.Enjoyamalataclocalrestaurant:Lilleisknownforitsdeliciouscusinsine,somakesuretotrysomelocaldishes. 10.VisittheLilleZoo:Thisisasmallbutwell-keptzoothhathousesabout 70differentspecies. 11.TakeapromanticwalkthroughtheJardindesPlantesdeLille:This botanicalgardenisapeacefulplacetoenjoynature. 12.VisittheMuseedeld\'HospiceComtesse:Thismuseumislocatedinadormerhospitalandit\'shometoacollectionofartandartifactsfromtheMiddleAges. 13.EnjoyapperformanceattheLilleOpera:Checkthescheduleandbookticketsforaromaticnightout. 14.TakeadaytriptothenearbycityofBruges:ThischarmingBelgiancityisjustashortrainrideaway. 15.VisittheLePalaisRihour:ThispalacehousestheLilleTouristOfficeandit\'salshometotheSalleduConclave,whichhasabeautifulGothicfireplace. RemembertochecktheopeningtimesandanyCOVID-19restrictionsbeforeyouvisit.Enjoyyourtrip! <|Llama-2-70b-chat\'sAnswer|> Sure!LilleisabeautifulcityinnorthernFrancethatoffersavarietyofactivitiesforcouplestoenjoy.Herearesomeideasforthingsyoucoulddo with your girlfriend in Lille:\n' +
      '\n' +
      '1. Explore the city\'s charming streets and historic landmarks, such as the Grand Place, the Vieille Bourse, and the Cathedral of Lille.\n' +
      '\n' +
      '2. Visit one of the many museums, such as the Palais des Beaux-Arts de Lille, the Museed\'Art Moderne, d\'Art Contemporain et d\'Art Brut, or the Musee de l\'Hospice Comtesse.\n' +
      '\n' +
      '3. Take a stroll along the canal and enjoy the views of the city from the waterfront.\n' +
      '\n' +
      '4. Visit the famous Lille Christmas market, which takes place every year in December and features festive stalls, food, and drinks.\n' +
      '\n' +
      '5. Go shopping at the city\'s many boutiques and galleries, or at the large shopping centers such as the Euralille or the Lille Europe.\n' +
      '\n' +
      '6. Enjoy a romantic thinner at one of Lille\'s many excellent restaurants, such as the Michelin-starred Le Comptoir du Relais or the cozy Le Bistrot d\'Alsace.\n' +
      '\n' +
      '7. Take a day trip to the nearby city of Bruges, Belgium, which is known for its picturesque canals and medieval architecture.\n' +
      '\n' +
      '8. Go to a show or concert at the Lille Opera or the Le Zenith concert hall.\n' +
      '\n' +
      '9. Visit the nearby town of Roubaix, which is famous for its textile industry and its beautiful Art Deco architecture.\n' +
      '\n' +
      '10. Take a walk in the beautiful Parc de la Citadelle, which offers a peaceful escape from the city and features a pond, a botanical garden, and a zoo.\n' +
      '\n' +
      '11. Go on a bike ride or a hike in the surrounding counryside, such as the Parc Naturel Regional Nord-Pas de Calais.\n' +
      '\n' +
      '12. Visit the famous Lille flea market, which takes place every Sunday and offers a wide range of antiques, vintage items, and second-hand goods.\n' +
      '\n' +
      '13. Take a cooking class together and learn how to make some of the region\'s delicious specialties, such as carbonnade flamande (beef stew) or tarte au maroilles (cheese tart).\n' +
      '\n' +
      '14. Go on a wine tasting tour and discover the region\'s excellent wines, such as the Chablis or the Champagne.\n' +
      '\n' +
      '15. Take a hot air balloon ride over the city and enjoy a bird\'s-eye view of Lille\'s beautiful architecture and surroundings.\n' +
      '\n' +
      'These are just a few ideas to get you started. Lille is a vibrant and romantic city that offers many opportunities for couples to have a great time together. Bon voyage!\n' +
      '\n' +
      '<!GPT-4-Turbo Verdict|>\n' +
      '\n' +
      'Tie\n' +
      '\n' +
      '### Arena Bench Prompt\n' +
      '\n' +
      '**Example 1**: An prompt from Arena Bench requiring a very high level of complex problem-solving skill and reasoning while adhering to real-world application.\n' +
      '\n' +
      'Create a flutter app for habit tracking that tracks daily habits for a user. The user should be able create multiple daily tasks that he wants to track. And he should be able to group the habits into a set of groups. One the user completes a task, he is rewarded a number of points per task. The apps should have a page that summarize the total score per group. And the aggregate score of all the groups of habits. This flutter app needs to be compilable for both android and iOS.\n' +
      '\n' +
      '**Example 2**: An prompt from Arena Bench requiring a very high level of complex problem-solving skill and reasoning while adhering to real-world application.\n' +
      '\n' +
      'I want to set up a remote raspberry pi zero, powered by a solar panel with simple wiring. I want to power a small 2W pump, a simple electet microphone, a custom python script running on the raspberry pi that is used to classify audio detected by the microphone. What components will I need to optimise for cost and minimise any electrical work (e.g. soldering)? What size solar panel will I need to power this whole system?\n' +
      '\n' +
      '### Arena Bench System Prompt\n' +
      '\n' +
      'The novel evaluation procedure is as follow: we prompt GPT-4-Turbo with the system prompt displayed below alongside a user prompt, a reference answer, and 2 assistant\'s answers. For reference answer, we present the user prompt with 3 assistants\' answers, GPT-4-Turbo, GPT-4-0314, and Claude-1, to GPT-4-Turbo and ask GPT-4-Turbo to generate an answer to the prompt. To ensure consistent pairwise judgment, we set up GPT-3.5-Turbo-0301 as the baseline answer for all models to be compared against. To avoid positional bias, we conduct two judgments per prompt: the first judgment presents the baseline answer as Assistant A while the second judgment presents the baseline answer as Assistant B. In total, we conduct 700 pairwise comparisons between each model against GPT-3.5-Turbo-0301 across 350 user prompts to calculate a win-rate against the baseline. Then we project the win-rate on a scale from 0 to 10 by assigning wins with a score of 10, ties with a score of 5, and losses with a score of 0. Further, we assign a significant win or loss as 3 wins or 3 losses, respectively, and keeping the other verdicts as a single win, loss, or tie. Finally, we calculate the final score by averaging across the wins, losses, and ties.\n' +
      '\n' +
      '```\n' +
      '<|SystemPrompt|> Pleaseactsanimpartialjudgeandevaluatethequalityoftheresponses providedbytwoAIassistantstotheuserpromptdisplayedbelow.Yourjob istecvaluatewhichassistant\'sanswerisbetter.\n' +
      '``` Whenevaluatingtheassistants\'answ,comparebothassistants\'answ. Youmustidentifyandcorrectanymistakesorinaccurateinformation. ```\n' +
      'Thenconsideriftheassistant\'sanswarehelpful,relevant,andconcise. Helpfulmeanstheanswercorrectlyrespondstothepromptorfollowsthe instructions.Notewhenuserprompthasanyambiguityormorethanone interpretation,itismorehelpfulandappropriatetoaskforclarifications ormoreinformationfromtheuserthanprovidingananswerbasedon assumptions.Relevantmeansallpartsoftheresponsecloselyconnectorareappropriatetowhatisbeingasked.Concisemeanstheresponseisclearandnotverboseorexcessive.\n' +
      '``` Thenconsiderthecreativityandnoveltyoftheassistant\'sanswerswhen needed.Finally,identifyanymissingimportantinformationintheassistants\'answersthatwouldbebeneficialtoincludewhenrespondingtotheuserprompt. ```\n' +
      'Afterprovidingyourexplanation,youmustoutputonlyoneofthefollowingchoicesasyourfinalverdictwithalabel:\n' +
      '```\n' +
      '1.AssistantAisignificantlybetter:[[A>>B]]\n' +
      '2.AssistantAislightlybetter:[[A>B]]\n' +
      '3.Tie,relativelythesame:[[A=B]]\n' +
      '4.AssistantBislightlybetter:[[B>A]]\n' +
      '5.AssistantBisignificantlybetter:[[B>>A]]\n' +
      '6 Exampleoutput:"Myfinalverdictistie:[[A=B]]". ```\n' +
      '\n' +
      '### Arena Battles with No Ground Truth\n' +
      '\n' +
      '**Example 1**: An example of an Arena Battle between GPT-4-Turbo and Llama-2-13B-chat, where depending on different preferences, either answer can be argued as better than the other. While both answers are correct, GPT-4-Turbo\'s answer is concise and appropriate to the format of multiple choice question. On the other hand, Llama-2-13B-chat provides helpful explanation for the multiple choice answer.\n' +
      '\n' +
      '```\n' +
      '<|UserPrompt|>  MultipleChoiceQuestion  Allplantshavebothahaploidandadiploidform.Astheplantchanges  fromoneformtoanother,thisiscalled:  Multiplechoicequestion.  alternativelifecycle  succession  an alternationofgenerations <!GPT-4-Turbo\'sAnswer|>  an alternationofgenerations <!Llama-2-13B-chat\'sAnswer|>  Thecorrectansweris:analternationofgenerations.  Plants,likeotherorganisms,havedifferentlifecyclestages,andthey  canexistintwodifferentforms:haploidanddiploid.Thehaploidform  isasingalestofchromosomes,whilethediploidform istwosetsof  chromosomes.Astheplantchangesfromoneformtoanother,thisiscalled  an alternationofgenerations.Thisprocessinvolvesthealternationof  haploidanddiploidphases,anditisafundamentalaspectofthelifecycle  ofplants.  Alternativelifecycleandsuccessionarenotcorrectdescriptionsofthis  process,astheyrefertodifferentconcepts.Alternativelifecyclerefers  tothedifferentstagesoflifeanorganismmaygothrough,suchaslarva,  pupa,andadultininsects,whilesuccessionreferstotheprocessofone  speciesreplacinganotherinaparticularenvironmentorniche.\n' +
      '```\n' +
      '\n' +
      '**Example 2**: An example of an Arena Battle between GPT-4-Turbo and Llama-2-13B-chat, where depending on different preferences, either answer can be argued as better than the other.\n' +
      '\n' +
      '```\n' +
      '<!UserPrompt|>  WhatarecriticalelementsofaProductMindsetforstartupfounders? <!GPT-4-Turbo\'sAnswer|>  AdoptingaProductMindsetiscrucialforstartupfoundersasithelpsthem  focusoncreatingproductsthatdelivervaluetocustomerswhileachieving  businessobjectives.HerearessomecriticalelementsofaProductMindset  forstartupfounders:\n' +
      '``` 1.CustomerCentricity: -Understandandempathizewithyourcustomers\'needs,problems,and  motivations. ```* Continuously gather and incorporate customer feedback into the product development process.\n' +
      '* Prioritize features and improvements that directly address customer pain points and enhance their experience.\n' +
      '\n' +
      '2. Value Proposition:\n' +
      '* Clearly define the unique value your product offers to the market.\n' +
      '* Align your product\'s features and capabilities with its value proposition to ensure it meets the expectations of your target audience.\n' +
      '\n' +
      '3. Problem-Solving Focus:\n' +
      '* Identify and focus on solving real problems for users rather than just creating features for the sake of innovation.\n' +
      '* Validate assumptions and hypotheses through experimentation and user testing.\n' +
      '\n' +
      '4. Iterative Development:\n' +
      '* Embrace an agile development approach, where you build, measure, and learn in iterative cycles.\n' +
      '* Be open to pivoting or making significant changes based on feedback and market response.\n' +
      '\n' +
      '5. Data-Driven Decision Making:\n' +
      '* Use data to inform decisions about product development, marketing, and growth strategies.\n' +
      '* Establish key performance indicators (KPIs) to measure the success of the product and track progress over time.\n' +
      '\n' +
      '...\n' +
      '\n' +
      'By internalizing these elements, startup founders can develop a strong Product Mindset that guides their decisions and strategies, ultimately leading to a product that customers love and that contributes to the startup\'s success.\n' +
      '\n' +
      '<!Llama-2-13B-chat\'s Answer|>\n' +
      '\n' +
      'A product mindset is a way of thinking and approaching product development that is critical for startup founders. Here are some essential elements of a product mindset for startup founders:\n' +
      '\n' +
      '1. Customer-centricity: A product mindset starts with understanding the customer\'s needs, desires, and pain points. Founders must be obsessed with their customers and be willing to put in the effort to understand their needs.\n' +
      '\n' +
      '2. Iteration and experimentation: A product mindset is all about experimentation and iteration. Founders must be willing to try new things, test hypotheses, and iterate on their product based on customer feedback.\n' +
      '\n' +
      '3. Focus on value: A product mindset is focused on delivering value to the customer. Founders must be able to identify and prioritize the most important features and functionality that deliver the most value to their customers.\n' +
      '\n' +
      '4. Collaboration: A product mindset requires collaboration between cross-functional teams, including engineering, design, and product management. Founders must be able to work effectively with these teams to develop a product that meets customer needs.\n' +
      '\n' +
      '5. Data-driven decision making: A product mindset is data-driven, and founders must be comfortable using data to make informed decisions about their product. This includes analyzing customer feedback, usage data, and other metrics to inform product development.\n' +
      '\n' +
      '...\n' +
      '\n' +
      'By embracing these elements of a product mindset, startup founders can develop a product that meets customer needs, delivers value, and sets their company up for long-terms success.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>