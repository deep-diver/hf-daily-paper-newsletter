<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MVD\\({}^{2}\\): Efficient Multiview 3D Reconstruction for Multiview Diffusion\n' +
      '\n' +
      'XIN-YANG ZHENG\n' +
      '\n' +
      'Work done during internship at Microsoft.\n' +
      '\n' +
      'Tsinghua University, P. R. China\n' +
      '\n' +
      'HAO PAN\n' +
      '\n' +
      'Microsoft Research Asia\n' +
      '\n' +
      'P. R. China\n' +
      '\n' +
      'Yu-XIAO GUO\n' +
      '\n' +
      'Microsoft Research Asia\n' +
      '\n' +
      'P. R. China\n' +
      '\n' +
      'XIN TONG\n' +
      '\n' +
      'Microsoft Research Asia\n' +
      '\n' +
      'P. R. China\n' +
      '\n' +
      '###### Abstract.\n' +
      '\n' +
      'As a promising 3D generation technique, multiview diffusion (MVD) has received a lot of attention due to its advantages in terms of generalizability, quality, and efficiency. By finetuning pretrained large image diffusion models with 3D data, the MVD methods first generate multiple views of a 3D object based on an image or text prompt and then reconstruct 3D shapes with multiview 3D reconstruction. However, the sparse views and inconsistent details in the generated images make 3D reconstruction challenging. We present MVD\\({}^{2}\\), an efficient 3D reconstruction method for multiview diffusion (MVD) images. MVD\\({}^{2}\\) aggregates image features into a 3D feature volume by projection and convolution and then decodes volume features into a 3D mesh. We train MVD\\({}^{2}\\) with 3D shape collections and MVD images prompted by rendered views of 3D shapes. To address the discrepancy between the generated multiview images and ground-truth views of the 3D shapes, we design a simple-yet-efficient _view-dependent_ training scheme. MVD\\({}^{2}\\) improves the 3D generation quality of MVD and is fast and robust to various MVD methods. After training, it can efficiently decode 3D meshes from multiview images within one second. We train MVD\\({}^{2}\\) with Zero-123++ and ObjectVense-LVIS 3D dataset and demonstrate its superior performance in generating 3D models from multiview images generated by different MVD methods, using both synthetic and real images as prompts.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      'Footnote †: Corresponding author\n' +
      '\n' +
      '+\n' +
      '## 1. Introduction\n' +
      '\n' +
      'Recently multiview diffusion (MVD) methods (Liu et al., 2023, 2024, 2024, 2024) have received lots of attention as an emerging 3D generation technique. By finetuning a pre-trained image diffusion model with 3D data, the MVD methods first generate multiview images of a 3D object from text or image prompts and then produce a realistic 3D model from the generated images via multiview 3D reconstruction. Based on the pretrained large image diffusion models, MVD methods have shown remarkable advantages over other 3D generation techniques in terms of generalizability, quality, and efficiency. However, the multiview 3D reconstruction methods used in MVD, such as NeRF (Mildenhall et al., 2020) or NeuS (Wang et al., 2021), are designed for dense and consistent multiview input, which cannot efficiently address the sparse multiview images with inconsistent details generated by MVD methods. As a result, the reconstructed 3D shapes are often blurry and distorted (see Fig. 1). Furthermore, the optimization process of 3D reconstruction is slow.\n' +
      '\n' +
      'Several methods (Liu et al., 2024, 2023; Long et al., 2023; Shi et al., 2023) have been proposed to enhance the multiview consistency of MVD methods. However, these methods do not ensure pixelwise consistency across different views. Some other methods aim to improve the 3D reconstruction quality from MVD images. Zero-1-to-3 (Liu et al., 2023) exploits 2D diffusion priors (Wang et al., 2023) to refine both the 3D geometry and texture through costly optimization. The concurrent One-2-3-45++ (Liu et al., 2023) method employs a two-stage multiview-conditioned 3D diffusion model for 3D shape reconstruction. Although this method achieves better 3D reconstruction results, the discrepancy between the rendered images of 3D data used for training and the images generated by diffusion used for inference limits its performance and the 3D diffusion process remains slow.\n' +
      '\n' +
      'In this paper, we propose MVD\\({}^{2}\\), a multiview 3D reconstruction method that reconstructs 3D shapes from multiview images generated by MVD. By noticing that the multiview images usually contain sufficient information to recover the 3D shape, we design a light-weight neural network that directly maps multiview image features to a 3D feature volume via view projection and 3D convolution, and then outputs a differentiable 3D mesh. We carefully design our network to make it robust to different view configurations of the input images. Once trained, the MVD\\({}^{2}\\) can directly decode 3D shapes\n' +
      '\n' +
      'Figure 1.— _Left_: Existing 3D reconstruction methods, such as NeuS (Wang et al., 2021), struggle to handle the inconsistency in multiview images (in gray background) generated by multiview diffusion, leading to low-quality geometry (upper). Our MVD\\({}^{2}\\) method effectively addresses this challenge and produces more realistic geometry (middle), while being highly efficient (less than 0.5 seconds). The lower row shows our result with texture. _Middle_: Visualization of two more 3D shapes reconstructed from MVD images. _Right_: Diverse 3D shapes reconstructed by our method, with texture mapping.\n' +
      '\n' +
      'from multiview images produced by various MVD methods without optimization.\n' +
      '\n' +
      'Training the MVD\\({}^{2}\\) network is challenging because we do not have access to the true 3D shape that corresponds to each set of inconsistent multiview images. If we use all input views as self-supervision for training, our model will suffer from view-view inconsistency like traditional multiview 3D reconstruction methods. Alternatively, we could train our network with a collection of 3D shapes, following the MVD training procedure. For each 3D shape, we render a set of views and select one view as the prompt of MVD to generate multiview images. Then, we use either the other views or the ground-truth 3D shape as the supervision. However, this training scheme still leads to suboptimal results (shown in Section 4.4) due to the domain gap between the generated images (the underlying 3D shape) and the ground truth images (the training 3D shape).\n' +
      '\n' +
      'By examining the discrepancy between the rendered and generated views, we observe that the discrepancy varies with the views of the generated images. Specifically, the generated view that is closer to the reference view of the prompt image is more consistent with the corresponding ground-truth image. Based on this observation, we propose a _view-dependent training scheme_ that enforces the inferred shape to align with the ground-truth geometry at the prompt view, and to maintain local structural similarity at other views.\n' +
      '\n' +
      'We train MVD\\({}^{2}\\) with MVD images generated by Zero123++ [14] and the Obiverse-LVIS dataset [11], and extensively evaluate our method\'s performance on the unseen multiview images generated by Zero123++ and other MVD methods prompted by rendered images and real images, as well as text. We compare our method with other 3D generation methods and validate the effectiveness of our view-dependent training scheme. The experimental results show that our method significantly enhances the quality and efficiency of 3D shape reconstruction for MVD and exhibits good generalizability for different MVD methods. Fig. 1 gathers a few results for demonstration.\n' +
      '\n' +
      'In summary, we make the following contributions:\n' +
      '\n' +
      '* We specifically address the problem of 3D reconstruction from multiview diffusion images and significantly improve the quality and efficiency of multiview diffusion for 3D generation.\n' +
      '* We identify the challenges of sparsity and inconsistency with MVD images, and propose an efficient lightweight neural network trained with a view-dependent training scheme to resolve these challenges.\n' +
      '* Through extensive evaluations, we show that the reconstruction model works robustly across different MVD models and complements a large family of MVD works.\n' +
      '\n' +
      'We will release our code and model to facilitate future research.\n' +
      '\n' +
      '## 2. Related Work\n' +
      '\n' +
      '_Optimization-based 3D generation._ Numeric recent works [13, 14, 15, 16, 17, 18, 19, 20, 21] use volume rendering methods like NeuS [14] for 3D reconstruction. One-2-3-45 [14] employs SparseNeuS [15] to reconstruct 3D geometry from its 2-stage multiview predictions in one pass; One-2-3-45++ trains a 3D diffusion model to convert MVD images to a signed distance function. Both methods train with ground-truth render images and ignore the inconsistency issue of MVD images.\n' +
      '\n' +
      '## 3. Method\n' +
      '\n' +
      'We first formulate the problem of 3D reconstruction from MVD images (Section 3.1). Then we make observations that reveal unique challenges with this problem (Section 3.2). To address the challenges,we present our training strategy and a lightweight neural network that is designed for MVD images and produces better reconstruction quality with efficiency (Sections 3.3 and 3.4).\n' +
      '\n' +
      '### 3D reconstruction from MVD images\n' +
      '\n' +
      'An MVD model \\(\\mathcal{M}\\) approaches 3D generation by taking an input reference image \\(\\mathbf{I}_{0}\\) at viewpoint \\(\\mathbf{v}_{0}\\) (or a text prompt) as condition, and generating images \\([\\mathbf{I}_{i}]=\\mathcal{M}(\\mathbf{I}_{0})\\) of a target 3D object at novel views \\(\\mathcal{C}=[\\mathbf{v}_{i}]\\). The model is generally adapted from a pretrained large-scale image generation model (_e.g._, stable diffusion [Rombach et al., 2022]), and finetuned on multiview renderings of 3D objects from large-scale datasets [Deitke et al., 2023, 20] to enhance the consistency of generated images. Given these MVD images, a reconstruction algorithm (_e.g._, NeuS and variants [Long et al., 2022; Wang et al., 2021]) is typically applied to obtain the final 3D object. However, due to sparseness and lack of precise consistency of MVD images (Section 3.2), typical multiview 3D reconstruction algorithms do not work well [Long et al., 2023]. To achieve quality and efficiency, we propose to learn a reconstruction model \\(\\text{MVD}^{2}\\) that takes \\([\\mathbf{I}_{i}]\\) as input, and recovers \\(\\mathcal{S}=\\text{MVD}^{2}([\\mathbf{I}_{i}])\\) as output. The problem remains of how to supervise \\(\\mathcal{S}\\), which reveals a unique challenge for MVD reconstruction, as discussed next.\n' +
      '\n' +
      '### The MVD reconstruction dilemma\n' +
      '\n' +
      '_Observations about MVD images._ Despite the differences of MVD models in technical details (Sec. 2), we make the following observations common to their generated images:\n' +
      '\n' +
      '**P1**: Viewpoints of MVD images are known and sparsely scattered.\n' +
      '**P2**: MVD images tend to be consistent in 3D but lack precision.\n' +
      '**P3**: The closer to the input view, the better generated images match the training object.\n' +
      '**P1**: derives from the formulation of MVD. In particular, viewpoints of generated images are specified explicitly, which saves the trouble of pose estimation for 3D reconstruction algorithms. The sparsity of MVD images is a result of limited computational power. MVD models are trained to generate a limited number (_e.g._, 4-16) of images simultaneously with enhanced consistency; beyond that, the memory and computational cost become unbearable [Long et al., 2023; Shi et al., 2024].\n' +
      '\n' +
      '**P2** also follows naturally from MVD approaches. Indeed, while MVD models strive to enhance multiview consistency by diverse techniques of cross-view modulation [Liu et al., 2024; Shi et al., 2023, 2024], there is no guarantee of pixelwise 3D consistency of the generated images.\n' +
      '\n' +
      '**P3** presents an observation that can be confirmed by examining Fig. 2. In this figure, \\(\\mathbf{v}_{0}\\) represents a rendered view of a 3D object used for training, which serves as the input to an MVD model -- Zero-123++ [Shi et al., 2023]. This model generates images from various viewpoints, denoted as \\(\\mathbf{v}_{1}\\) to \\(\\mathbf{v}_{6}\\). It is worth noting that \\(\\mathbf{v}_{1}\\) and \\(\\mathbf{v}_{6}\\) are closer to \\(\\mathbf{v}_{0}\\). We generated two sets of MVD images: MVD-1 and MVD-2. Among these sets, the images at \\(\\mathbf{v}_{1}\\) and \\(\\mathbf{v}_{6}\\) exhibit a higher similarity to the ground-truth images (rendered in the first row) compared to the images at other viewpoints. We also quantitatively validated the observation on 240 3D models (see Table 1), by comparing the image differences between the generated and GT views using PSNR metric.\n' +
      '\n' +
      'Taken together, these observations present the following unique challenge for 3D reconstruction from MVD images.\n' +
      '\n' +
      '_The MVD reconstruction dilemma._ We note there is a dilemma with 3D reconstruction from MVD images. On one hand, there is no ground-truth 3D shape to supervise the training of the reconstruction model, because as noted in **P3**, the MVD images at viewpoints away from the reference view deviate from the training object \\(\\overline{\\mathcal{S}}\\), and to make \\(\\mathcal{S}\\) approach \\(\\overline{\\mathcal{S}}\\) would contradict with the input images \\([\\mathbf{I}_{i}]\\). On the other hand, directly comparing the rendered images with \\([\\mathbf{I}_{i}]\\) would also be problematic, since according to **P2**, \\([\\mathbf{I}_{i}]\\) are not precisely consistent in 3D.\n' +
      '\n' +
      'We validate the dilemma through experiments in Sec. 4.4, where the contradictory situations are shown to produce suboptimal results. To solve this dilemma of input-output mismatch, we use 3D shapes as proxies, construct training inputs through image-prompted MVD models, and design loss functions that avoid the dilemma. Note that while the training relies on image-prompted MVD models, at test time our model naturally extends to processing MVD images of text-prompted models.\n' +
      '\n' +
      '### View-dependent training\n' +
      '\n' +
      'For each object \\(\\overline{\\mathcal{S}}\\), we render it in a random viewpoint as the reference image \\(\\mathbf{I}_{0}\\), and in the specified viewpoints \\([\\mathbf{v}_{i}]\\) the proxy images \\([\\overline{\\mathbf{I}_{i}}]\\). Correspondingly, \\([\\mathbf{I}_{i}]=\\mathcal{M}(\\mathbf{I}_{0})\\) is the set of MVD images to be used as input for MVD\\({}^{2}\\). We formulate the reconstruction loss as follows:\n' +
      '\n' +
      '\\[\\min\\sum_{i=0}^{N}\\text{dis}\\left([\\pi_{i}(\\mathcal{S}),\\overline{\\mathbf{I}_{i}}],\\mathbf{v}_{i}\\right), \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\pi_{i}(\\mathcal{S})\\) is the differentiable rendering of \\(\\mathcal{S}\\) at the viewpoint \\(\\mathbf{v}_{i}\\), and \\(\\text{dis}([\\cdot,\\cdot],\\mathbf{v}_{i})\\) is a view-dependent loss function that quantifies the discrepancy between two images differently according to the viewpoint \\(\\mathbf{v}_{i}\\). Specifically, according to **P3**, at \\(\\mathbf{v}_{0}\\) the reference view, we expect the recovered shape can fully match \\(\\overline{\\mathbf{I}_{0}}\\) in pixelwise details, while at the other viewpoints, we only ask for structural similarity\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & \\(\\mathbf{v}_{1}\\) & \\(\\mathbf{v}_{2}\\) & \\(\\mathbf{v}_{3}\\) & \\(\\mathbf{v}_{4}\\) & \\(\\mathbf{v}_{5}\\) & \\(\\mathbf{v}_{6}\\) \\\\ \\hline\n' +
      '**PSNR\\(\\uparrow\\)** & **25.61** & 23.57 & 23.37 & 23.39 & 23.77 & **25.01** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1. Quantitative evaluation of image differences between generated views and GT views, averaged on 240 objects.\n' +
      '\n' +
      'Figure 2. Inconsistency from training object increases as the viewpoint moves away from the reference image.\n' +
      '\n' +
      'with \\(\\overline{I_{i}},i\\neq 0\\). Therefore, we have\n' +
      '\n' +
      '\\[\\text{dis}([\\mathbf{x},\\mathbf{y}],v_{i})=\\begin{cases}\\mathcal{L}_{pixel}( \\mathbf{x},\\mathbf{y})&\\text{for view }v_{0}\\\\ \\mathcal{L}_{LPIPS}(\\mathbf{x},\\mathbf{y})&\\text{for view }v_{i},i\\neq 0\\end{cases} \\tag{2}\\]\n' +
      '\n' +
      'Here, \\(L_{LPIPS}\\) loss measures perceptual patch similarity between two images using pretrained image backbones (Zhang et al., 2018).\n' +
      '\n' +
      'Disentangled reconstruction of geometry and texture generally leads to better qualities in both geometry and texture (Chen et al., 2023). In this work, we follow this approach and focus on recovering the shape geometry, and apply multiview texture mapping on the reconstructed detailed shapes subsequently (Section 4.1). To supervise the geometry reconstruction, we rewrite the \\(v_{0}\\) pixelwise losses as:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{d}}\\left(\\mathbf{d},\\overline{\\mathbf{d}} \\right) =\\sum_{p}\\frac{\\left\\|\\mathbf{d}(p)-\\overline{\\mathbf{d}}(p) \\right\\|_{1}}{\\overline{\\mathbf{d}}(p)-d_{min}}, \\tag{4}\\] \\[\\mathcal{L}_{\\text{n}}\\left(\\mathbf{n},\\overline{\\mathbf{n}} \\right) =\\sum_{p}1-|\\mathbf{n}(p)\\cdot\\overline{\\mathbf{n}}(p)|,\\] (5) \\[\\mathcal{L}_{\\text{m}}\\left(\\mathbf{m},\\overline{\\mathbf{m}} \\right) =\\left\\|\\mathbf{m}(p)-\\overline{\\mathbf{m}}(p)\\right\\|_{2}^{2} \\tag{3}\\]\n' +
      '\n' +
      'for depth, normal and mask maps \\(\\mathbf{d},\\mathbf{n},\\mathbf{m}\\) respectively. Here \\(p\\) iterates over the pixels, and \\(d_{min}\\) is the minimum depth value of bounding sphere, subtracted to normalize the arbitrary camera translations of MVD models.\n' +
      '\n' +
      'For views other than \\(v_{0}\\), we apply the structure similarity loss on normal maps. Therefore, the total loss is\n' +
      '\n' +
      '\\[\\mathcal{L} =\\ \\ \\lambda_{1}\\mathcal{L}_{\\text{d}}(\\mathbf{d}_{\\mathbf{d}_{ \\mathbf{0}}},\\overline{\\mathbf{d}}_{\\mathbf{a}_{\\mathbf{0}}})+\\lambda_{2} \\mathcal{L}_{\\text{n}}\\left(\\mathbf{n}_{\\mathbf{a}_{\\mathbf{0}}},\\overline{ \\mathbf{n}}_{\\mathbf{a}_{\\mathbf{0}}}\\right)+\\lambda_{3}\\mathcal{L}_{\\text{m }}(\\mathbf{m}_{\\mathbf{v}},\\overline{\\mathbf{m}}_{\\mathbf{a}_{\\mathbf{0}}})\\] \\[\\ \\ \\ \\ +\\lambda_{4}\\sum_{v_{i},i\\neq 0}\\mathcal{L}_{\\text{LPIPS}}( \\mathbf{n}_{\\mathbf{a}_{\\mathbf{i}}},\\overline{\\mathbf{n}}_{\\mathbf{a}_{ \\mathbf{i}}})+\\mathcal{L}_{\\text{reg}} \\tag{6}\\]\n' +
      '\n' +
      'where \\(\\mathcal{L}_{\\text{reg}}\\) is a regularization loss for 3D mesh (Section 3.4). We empirically set \\(\\lambda_{1}=\\lambda_{3}=1.0,\\lambda_{2}=0.2,\\lambda_{4}=0.1\\). Note that while for each training sample only the reference view provides strong pixelwise guidance and the other views regulate shape structures, we have synthesized training samples with random reference views (Sec. 4.1), which ensures that the trained model can recover accurate geometry at arbitrary views.\n' +
      '\n' +
      '### Reconstruction model\n' +
      '\n' +
      'To complement a wide range of MVD models that differ in aspects, including image viewpoints and resolutions, we design MVD\\({}^{2}\\) to be independent of specific MVD models and accommodate diverse sets of sparse images in a generalizable way. In particular, the MVD\\({}^{2}\\) model recovers 3D features by fetching image features according to given viewpoints, and after minimal local transformations that are invariant to image numbers produces high-quality 3D geometry in a mesh representation. We illustrate the overview of our approach in Fig. 3, and expand on the model details next.\n' +
      '\n' +
      '_2D-to-3D feature transformation._ To ensure the independence of specific MVD models, we use a pre-trained DINOv2 model (Qoub et al., 2023) to turn the MVD images \\([\\mathbf{I}_{i}]\\) into 2D feature maps. The original resolution of DiNOv2 feature maps is \\(37\\times 37\\), we upsample the feature maps to \\(64\\times 64\\) resolution, and fabricate them via view-shared 2D convolution. The resulting feature maps are denoted by \\([\\mathbf{F}_{i}\\in\\mathbb{R}^{Dud}]\\). To obtain 3D pointwise features from these 2D feature maps, given a spatial point \\(p\\in G\\) where \\(G\\) is a regular grid of resolution \\(R_{\\text{G}}^{3}\\) tessellating the unit cubic space, we fetch image features for \\(p\\) by \\([\\mathbf{f}_{p,i}=\\mathbf{F}_{i}\\left(\\pi_{i}(p)\\right)]\\) through specified view projection, where \\(\\mathbf{F}(\\cdot)\\) queries the image feature map through bilinear interpolation. To remain invariant to the number and ordering of images, we use average pooling to fuse the image features into a single feature vector, _i.e._, \\([\\mathbf{f}_{p}=\\text{avg}\\left(\\mathbf{f}_{p,i}\\right)]\\), similar to the design of PointNeRF (Yu et al., 2021) in supporting multiviews. The 3D feature grid is then transformed by a small number of 3D convolution layers before extracting the mesh representation. Overall, the network model is lightweight, comprising a few layers and parameters that we find sufficient for reconstruction from the MVD images. It is important to note that the image view at \\(v_{0}\\) (Section 3.3) is solely utilized in the loss computation during training phase, and it is not required by the network for inference.\n' +
      '\n' +
      'Figure 3. Method overview. The MVD model produces a set of images from different viewpoints based on a reference image. MVD\\({}^{2}\\) extracts and averages features from these images for each point in a coarse 3D grid \\(G\\), and interpolates them into a finer grid \\(G^{\\prime}\\), from which the surface mesh is extracted in a differentiable manner. The mesh reconstruction during training is supervised with pixelwise loss (red arrow) against depth/normal/mask maps at the reference view \\(v_{0}\\), and with structural loss (yellow arrow) against normal maps at the other views. The reconstructed mesh can be textured by mapping to MVD images.\n' +
      '\n' +
      '_Shape representation._ We choose triangle meshes as our shape representation for their compactness and suitability as 3D assets, and adopt FlexiCubes (Shen et al., 2023) to produce mesh output with differentiability. At the core of FlexiCubes is to transform a learned feature grid to signed distance and deformation functions, from which the isosurface mesh is extracted by marching cubes. In particular, we use a higher resolution grid \\(G^{\\prime}\\) of shape \\(R^{3}_{G^{\\prime}}\\) as the defining grid of Flexicubes, whose grid point features are interpolated trilinearly from the 3D feature grid \\(G\\). For each grid point \\(p\\in G^{\\prime}\\), we obtain its signed distance value and deformation parameters by learned mappings implemented as shallow MLPs. We adopt the regularizers of Flexicubes to define our \\(\\mathcal{L}_{\\text{reg}}\\), which reduce unnecessary oscillations of both distance fields and surface meshes.\n' +
      '\n' +
      '## 4. Experimental Analysis\n' +
      '\n' +
      'We present a thorough evaluation of \\(\\text{MVD}^{2}\\) in this section. Section 4.1 details the training setup and the evaluation protocol. Section 4.2 shows quantitative and qualitative results, and compares \\(\\text{MVD}^{2}\\) with other methods. Section 4.3 examines the applicability of \\(\\text{MVD}^{2}\\) to other \\(\\text{MVD}\\) models. Section 4.4 explains the rationale behind the training scheme of \\(\\text{MVD}^{2}\\). Section 4.5 discusses the limitations of our approach.\n' +
      '\n' +
      '### Experiment setup\n' +
      '\n' +
      '_Training data preparation._ We adopt Zero-123++ (Shi et al., 2023), a state-of-the-art \\(\\text{MVD}\\) model that is conditioned on an input image and generate six views whose azimuths are relative to the input view while having fixed elevations, to prepare \\(\\text{MVD}\\) images for training. We use Objavverse-LVIS (Deitke et al., 2023), a curated subset of Objavverse with diverse and textured 3D objects, as our 3D training data. For each object, we render three images from random views with a resolution of \\(512\\times 512\\), and for each view image, we generate six groups of multiview images by Zero-123++. The \\(\\text{MVD}\\) images have a resolution of \\(320\\times 320\\). We also use the \\(\\text{N}\\)Wdiffrast library (Laine et al., 2020) to render the depth and normal maps of the 3D object at the same views including views of \\(\\text{MVD}\\) images, with the image resolution of \\(512\\times 512\\).\n' +
      '\n' +
      '_Network setting._ The trainable parameters of \\(\\text{MVD}^{2}\\) consist of four 2D convolutional layers (channel dims: \\([768,512,256,128,32]\\), kernel size: 3) and four 3D convolutional layers (channel dims: \\([32,32,32,32,32]\\), kernel size: 3), each with a residual block structure, and a three-layer \\(\\text{MLP}\\) (channel dims: \\([32,64,64,4]\\)) after interpolating the 3D features, in total 20 M parameters. Correspondingly, we have \\(D_{2d}=128\\), \\(R_{G}=32\\) and \\(R_{G^{\\prime}}=80\\). We trained the model for 100 k steps on eight NVIDIA (16 G) V100 GPUs, with batch size 1 per GPU, taking around 12 hours.\n' +
      '\n' +
      '_Network inference._ \\(\\text{MVD}^{2}\\) takes \\(\\text{MVD}\\) images with known viewpoints as inputs for network inference. It achieves high efficiency: the DINOv2 feature extraction takes 0.2 seconds, and the network\'s forward pass also takes 0.2 seconds with a peak GPU-memory usage of 5.2 GB on an NVIDIA GeForce RTX 3090 GPU. This is much faster than NeuS, which takes 15 minutes.\n' +
      '\n' +
      '_Evaluation protocol._ For evaluating geometry quality of reconstructed shapes, we use the _Google Scan Objects_ (GSO) dataset (Downs et al., 2022) as the test set, since it is not used for training existing \\(\\text{MVD}\\) models and \\(\\text{MVD}^{2}\\). We select 50 diverse objects from GSO for evaluation and comparison. For each object, we render a random view as input for different image-conditioned \\(\\text{MVD}\\) models and image-to-3D models. After 3D reconstruction by our method or other competing methods, we render the object from 32 spatially uniform views, obtaining depth and normal maps. We compute PSNR, SSIM (Structural similarity (Wang et al., 2004)), LPIPS (learned perceptual image patch similarity (Zhang et al., 2018)) metrics of these geometric maps against the ground-truth views rendered from the original 3D object. We also evaluate the Chamfer distance (CD) and Earth mover\'s distance (EMD), using 2048 uniformly sampled points. The subscripts \\(d\\) and \\(n\\) are employed to indicate that the metric is calculated on the depth map and normal map, respectively. To improve readability, the values of CD, EMD, SSIM, and LPIPS are all multiplied by 100. However, this evaluation protocol measures the reconstruction quality only in relation to the reference object, ignoring the diversity of \\(\\text{MVD}\\) images from the reference. Therefore, visual inspection is also essential to assess the shape quality and realism.\n' +
      '\n' +
      '_Texture mapping._ We develop a simple algorithm to convert multiview images to \\(\\text{UV}\\)-map for texture mapping after reconstructing the geometry. The algorithm consists of four steps: (1) _color initialization_: we assign a color to each surface point by selecting the view that has the largest visible area of the projected triangle and using its texture coordinates; (2) _color blending_: we smooth the color transitions by averaging the colors of a small neighborhood around each surface point; (3) _color filling_: we fill in the gaps where no color is assigned due to occlusion by propagating the colors from nearby regions. This algorithm is fast: takes 0.5 seconds for one shape. However, it is only for visualization purposes as shown in Fig. 1 and does not address the inconsistency of \\(\\text{MVD}\\) images or inpaint the occluded regions with better image content. We defer these challenges for future work.\n' +
      '\n' +
      '### Performance evaluation\n' +
      '\n' +
      '_Quantitative evaluation._ Our \\(\\text{MVD}^{2}\\) method outperforms NeuS in recovering 3D geometry from \\(\\text{MVD}\\) images generated by Zero-123++, as shown in the first row of Table 2. The higher SSIM values and lower LIPIPS values indicate that \\(\\text{MVD}^{2}\\) produces geometry that is more structurally similar to GSO data. Fig. 4 shows visual comparisons on two examples. NeuS produces more distorted geometry because it does not account for multiview inconsistency. On the other hand, \\(\\text{MVD}^{2}\\)\'s results are more visually pleasing.\n' +
      '\n' +
      '_Robustness._ The pretrained \\(\\text{MVD}\\) can also reconstruct \\(\\text{MVD}\\) images generated by other \\(\\text{MVD}\\) models using the same viewpoints. We use Stable Zero123 (sta 2023), an enhanced version of Zero-1-to-3 model (Liu et al., 2023) that supports arbitrary viewpoints for view synthesis. We employ the same camera setup as Zero-123++ and generate \\(\\text{MVD}\\) images. We reconstruct 3D shapes from \\(\\text{MVD}\\) images, using both NeuS and \\(\\text{MVD}^{2}\\). The second row of Table 2shows that MVD\\({}^{2}\\) outperforms NeuS significantly in most metrics except CD.\n' +
      '\n' +
      'Comparison with MVD-based methodsWe also compare with other recent MVD methods: SyncDreamer [11], Wonder3D [14], One-2-3-45 [11]. SyncDreamer generates 16 views with uniformly sampled azimuth angles and a fixed elevation angle of 30\\({}^{\\circ}\\), while Wonder3D generates 6 orthogonal side views. We use their default NeuS-based implementation for 3D reconstruction. One-2-3-45 uses Zero-1-to-3 to generate 36 view images and a pretrained SparseNeuS for 3D reconstruction. The quantitative report presented in the third row of Table 2 indicates that they perform significantly worse than Zero-123++ with either our MVD\\({}^{2}\\) or NeuS, and Stable Zero123 with MVD\\({}^{2}\\). We also test a concurrent method -- One-2-3-45++ [11] that generates 3D shapes using MVD-image-conditioned 3D diffusion, where MVD images are generated by an MVD model identical to Zero-123++. Since the code of this method is not released, we only use its commercial web service to generate a few samples for visual evaluation.\n' +
      '\n' +
      'Comparison with image-to-3D methodsWe further compare our approach with recent image-to-3D approaches that do not use multiviews, including Shap-E [13], which employs a conditional diffusion model to generate a 3D implicit function, and LRM [14], which maps the input image to a NeRF by a large transformer model. Since the official implementation of LRM is not available, we use a third-party implementation [11] for comparison. The results are shown in the third row of Table 2. Both LRM and Shap-E are inferior to Zero-123++ with MVD\\({}^{2}\\) and Stable Zero123 with MVD\\({}^{2}\\).\n' +
      '\n' +
      'Visual comparisonWe select a subset of rendering images from the GSO dataset and random images from the Internet to use as inputs to the above compared methods. The reconstructed 3D shapes are compared visually in Figs. 7 and 8. The visualizations demonstrate that our approach produces more visually compelling results with finer geometric details.\n' +
      '\n' +
      'Inference efficiencyOn the single-image-to-3D task, we need 12 seconds to generate six views (in 75 diffusion steps, using Zero-123++) and less than 1 second to produce the final textured mesh via MVD\\({}^{2}\\). This is comparable to Shape-E, which takes 12 seconds to generate the 3D result, but slower than LRM, which only needs 5 seconds. The other methods compared are much slower due to MVD generation, NeuS optimization, or 3D diffusion.\n' +
      '\n' +
      '### Model generalizability\n' +
      '\n' +
      'As MVD\\({}^{2}\\) extracts image features based on view positions and applies mean pooling to combine features from different views, it can accommodate varying numbers and viewpoints, in theory. Hence, we examine whether MVD\\({}^{2}\\) pretrained on Zero-123++\'s output can adapt to MVD images generated by other MVD models, with different view settings.\n' +
      '\n' +
      'Generalizability to image-conditioned MVD modelsWe tested SyncDreamer and Wonder3D with some online images as conditions and used the pretrained MVD\\({}^{2}\\) to reconstruct 3D shapes from the MVD images. As illustrated in Fig. 9, our method can effectively process these inputs and produce visually pleasing shapes.\n' +
      '\n' +
      'Generalizability to text-conditioned MVDWe also apply MVD\\({}^{2}\\) to the four-view images produced by the text-conditioned MVD model, MVDream [11]. As Fig. 9 shows, MVD\\({}^{2}\\) handles MVDream\'s output well and generates convincing shape geometry.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c|c c c c c c c} \\hline \\hline\n' +
      '**MVD Model** & \\(N_{I}\\) & **Recon.** & **CD** & **EMD** & **PSNR\\({}_{d}\\)\\(\\uparrow\\)** & **SSIM\\({}_{d}\\)\\(\\uparrow\\)** & **LPIPS\\({}_{d}\\)\\(\\downarrow\\)** & **PSNR\\({}_{n}\\)\\(\\uparrow\\)** & **SSIM\\({}_{n}\\)** & **LPIPS\\({}_{n}\\)\\(\\downarrow\\)** \\\\ \\hline Zero-123++ [11] & 6 & NeuS & 1.543 & 16.02 & 21.62 & 87.01 & 15.52 & 16.39 & 72.89 & 22.49 \\\\  & 6 & MVD\\({}^{2}\\) & **1.044** & **13.58** & **23.31** & **89.20** & **10.80** & **18.28** & **80.01** & **16.34** \\\\ \\hline Stable Zero123 [12] & 6 & NeuS & 2.146 & 19.95 & 19.91 & 84.37 & 20.31 & 14.91 & 69.65 & 27.24 \\\\  & 6 & MVD\\({}^{2}\\) & 2.631 & 18.66 & 20.98 & 86.88 & 15.13 & 16.34 & 77.04 & 21.38 \\\\ \\hline SyncDreamer [11] & 16 & NeuS & 2.082 & 19.19 & 20.42 & 86.18 & 15.44 & 15.87 & 75.29 & 22.99 \\\\ Wonder3D [14] & 6 & NeuS & 2.347 & 21.54 & 19.68 & 85.45 & 18.10 & 15.30 & 74.59 & 25.50 \\\\ One-2-3-45 [11] & 36 & SparseNeuS & 5.121 & 26.82 & 17.86 & 83.83 & 22.13 & 13.90 & 70.42 & 30.26 \\\\ \\hline Shap-E [13] & - & - & 4.553 & 24.48 & 19.18 & 84.48 & 18.83 & 14.83 & 73.31 & 26.05 \\\\ LRM [14] & - & - & 1.716 & 17.12 & 20.34 & 86.04 & 15.28 & 15.61 & 69.65 & 24.04 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2. Quantitative evaluation of single-view 3D reconstruction on GSO dataset [14], _Recon._ refers to the reconstruction method for MVD images. \\(N_{I}\\) denotes the number of MVD images used for 3D reconstruction.\n' +
      '\n' +
      'Figure 4. 3D reconstruction of Zero123++’s MVD images. The results of NeuS and MVD\\({}^{2}\\) are rendered in blue and cyan tones, respectively, from three different views.\n' +
      '\n' +
      '### Ablation study\n' +
      '\n' +
      'To assess the effectiveness of our training scheme, we conduct an ablation study with five different settings, which differ in whether they use ground-truth images or generated images for loss supervision and whether they apply view-dependent loss or not.\n' +
      '\n' +
      '1. [label=**A0**]\n' +
      '2. The network takes as input the ground-truth images of 3D data rendered at the views specified by Zero-123++. To compute the loss, we render the training 3D objects and the reconstructed mesh from 64 random views, and apply both \\(\\mathcal{L}_{pixel}\\) and \\(\\mathcal{L}_{IPIPS}\\) on all the views.\n' +
      '3. The setup is the same as **A1**, but we follow One-2-3-45++ (Liu et al., 2023b) and add random and small view perturbations when fetching image features during training. We aim to test whether this perturbation can improve the robustness and better handle the inconsistency of MVD images.\n' +
      '4. The setup is the same as **A1**, except that the input MV images are MVD images produced by Zero-123++.\n' +
      '5. The setup is the same as **A3**, except that we only use the images at the reference view \\(v_{0}\\) and six other views in the loss computation.\n' +
      '6. Our default setting, where \\(\\mathcal{L}_{pixel}\\) is applied only to view \\(v_{0}\\).\n' +
      '\n' +
      'As shown in Table 3, the geometry quality improves as the training setup approaches our default setup. Training with ground-truth render images yields the worst results, although view perturbation alleviates the problem to some degree. **A3** and **A4** show that applying \\(\\mathcal{L}_{pixel}\\) to all the views is less effective than our view-dependent training scheme. In Fig. 5, we also present three test cases, where our default setting **A5** clearly avoids floating geometry, reconstructs more detailed geometry such as the empty zone of the alphabet puzzle board (upper), glove\'s fingers (middle), and the witch model\'s arm (lower).\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'Our method relies on the assumption that the input views contain sufficient information of 3D shapes. However, if a large portion of the 3D shape is invisible in all input views, the inferred 3D shape for the unseen part will deteriorate. Fig. 6-a illustrates an example where the views generated by SyncDreamer do not include the teapot bottom. In this case, MVD\\({}^{2}\\) produces a cone-like geometry for the bottom region, which is inconsistent with human expectations.\n' +
      '\n' +
      'Moreover, our method is robust to minor inconsistency of generated images, but it may fail when the generated views are highly discordant. Fig. 6-b shows the reconstructed mesh that has a large visual discrepancy with the inconsistent input MVD images.\n' +
      '\n' +
      'Additionally, our GPU memory budget limits us from using higher grid resolutions (\\(>80^{3}\\)) in mesh generation. Therefore, our approach cannot easily reconstruct thin geometric structures, as shown in Fig. 6-c.\n' +
      '\n' +
      '## 5. Conclusion\n' +
      '\n' +
      'We propose a novel multiview 3D reconstruction method that enhances the 3D reconstruction quality from multiview depth (MVD) images. Our method leverages a carefully designed training scheme that mitigates the discrepancy between the MVD images and the 3D training data. Our MVD\\({}^{2}\\) model, which is pretrained on MVD images from Zero-123++, surpasses previous reconstruction pipelines and other 3D generation methods in terms of quality and efficiency.\n' +
      '\n' +
      'Figure 5. Visualization of three examples reconstructed by different variants of MVD\\({}^{2}\\). Left is the input MVD images.\n' +
      '\n' +
      'Figure 6. Illustration of imperfect and failure reconstruction results. GTs are the 3D objects for reference.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c|c c c c c c c c} \\hline \\hline  & **Training images** & **\\#v** & **VDL** & **CD\\(\\downarrow\\)** & **EMD\\(\\downarrow\\)** & **PSNR\\({}_{d}\\)\\(\\uparrow\\)** & **SSIM\\({}_{d}\\)\\(\\uparrow\\)** & **LPIPS\\({}_{d}\\)\\(\\downarrow\\)** & **PSNR\\({}_{n}\\)\\(\\uparrow\\)** & **SSIM\\({}_{n}\\)\\(\\uparrow\\)** & **LPIPS\\({}_{n}\\)\\(\\downarrow\\)** \\\\ \\hline\n' +
      '**A1** & GT & 64 & ✗ & 4.371 & 28.56 & 17.22 & 81.10 & 25.59 & 13.34 & 72.04 & 32.28 \\\\\n' +
      '**A2** & GT(VP) & 64 & ✗ & 2.172 & 19.95 & 20.83 & 86.76 & 15.32 & 16.29 & 77.47 & 21.55 \\\\\n' +
      '**A3** & Generated & 64 & ✗ & 1.256 & 13.80 & 23.02 & 89.10 & 11.74 & 18.09 & 79.88 & 17.60 \\\\\n' +
      '**A4** & Generated & 7 & ✗ & 1.214 & 14.62 & 23.03 & 89.09 & 11.63 & 18.07 & 79.77 & 17.47 \\\\\n' +
      '**A5** & Generated & 7 & ✓ & **1.044** & **13.58** & **23.31** & **89.20** & **10.80** & **18.28** & **80.01** & **16.34** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3. Ablation study of the training scheme. **VP** stands for viewport perturbation. **#v** indicates denotes the number of ground-truth views that are used in loss supervision, and **VDL** means the use of view-dependent loss. A5 is our final model.\n' +
      '\n' +
      'We conjecture that finetuning our model on the output of other MVD models will further boost their 3D reconstruction quality.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)\n' +
      '* Zero123 (2013) Stable Zero123. [https://huggingface.co/stabilityai/stable-zero123](https://huggingface.co/stabilityai/stable-zero123).\n' +
      '* Chen et al. (2023) Rui Chen, Yongwei Chen, Ningxing Jiao, and Kuii Jia. 2023a. Fantasia3D: Disentangling geometry and appearance for high-quality text-to-30 content creation. In _ICCV_.\n' +
      '* Chen et al. (2023) Yang Chen, Yingwei Pan, Yehao Li, Ting Yao, and Tao Mei. 2023b. Control3D: Towards controllable text-to-3D generation. In _ACM Multimedia_. 1148-1156.\n' +
      '* Chen and Zhang (2019) Zhiqi Chen and Hao Zhang. 2019. Learning implicit fields for generative shape modeling. In _CVPR_. 5939-5948.\n' +
      '* Cheng et al. (2023) Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander G Schwwing, and Liang Yan Gui. 2023. SPfusion: Multimodal 3D shape completion, reconstruction, and generation. In _CVPR_. 4456-4465.\n' +
      '* Li et al. (2022) Thiago L. T. da Silva, Patrico G. L. Pinto, Jeffi Murrugarra-Llerena, and Claudio R. Jung. 2022. 3D scene geometry estimation from \\(360^{\\circ}\\) imagery: A survey. _ACM Comput. Surv._ 55, 4, Article 68 (2022), 39 pages.\n' +
      '* Deike et al. (2021) Matt Deike, Ruoshi Liu, Matthew Wallinford, Houng Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voelnick, Samir Yitohak Garde, et al. 2021a. Obiverse-XL: A universe of 10M + 3D objects. In _NeurIPS_.\n' +
      '* Deike et al. (2023) Matt Deike, Dustin Schwenk, Joris Salvador, Luca Wehus, Oscar Michel, Eli Vanderghul, Ludwig Schmidt, Kiam Hasan, Anirudhchalkom, and Ali Farhadi. 2023b. Oboiverse: A universe of annotated 3D objects. In _CVPR_. 1134-13153.\n' +
      '* Deng et al. (2022) Congye Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Thu Zhou, Leonidas Guibas, Dragram Angiovich, et al. 2022. NextFile-view text synthesis with language-guided diffusion as general image priors. In _CVPR_. 20637-20647.\n' +
      '* Downs et al. (2022) Laura Downs, Anthony Francis, Natee Roenig, Brandon Kunnen, Ryan Hickman, Krista Reynmann, Thomas B McHugh, and Vincent Vanhoucke. 2022. Google scanned objects: A high-quality dataset of 3D scanned household items. In _ICRA_. IEEE. 2553-2560.\n' +
      '* Gao et al. (2022) Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangcue Yin, Daiqing Li, Or Litany, Zan Golei, and Sanja Fidler. 2022. GeStD: A generative model of high quality 3D textured shapes learned from images. _NeurIPS_ 35 (2022), 31841-31854.\n' +
      '* Gupta et al. (2023) Anh Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Oguz. 2023. 3DGen: Triplane latent diffusion for textured mesh generation. arXiv:2303.05371.\n' +
      '* He and Wang (2023) Zeein He and Tengfe Wang. 2023. OpenIR: OM-open-source large reconstruction models. [https://github.com/3DTopia/OpenIR.RM](https://github.com/3DTopia/OpenIR.RM).\n' +
      '* Hong et al. (2024) Yicong Hong, Kai Zhang, Jinxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sankai, Yifung Bui, and Hao Tan. 2024. LKM: Large reconstruction model for single image to 3D. In _ICLR_.\n' +
      '* Huang et al. (2023) Zixuan Huang, Stefan Stojanov, Anh Thai, Varun Jampani, and James M Rehg. 2023. ZeroSHox: Regression-based zero-shot shape reconstruction. arXiv:2312.14198.\n' +
      '* Ding et al. (2023) Moritz Ding, Gregor Kohsis, and Lei Korbel. 2023. Octree Transformer: Autoregressive 3D shape generation in hierarchically structured sequences. In _CVPR_. 2697-2706.\n' +
      '* Jain et al. (2022) Ajay Jain, Ben Mildenhall, Jonathan T. Sharon, Pieter Abbeel, and Ben Poole. 2022. Zero-shot text-guided object generation with dream fields. In _CVPR_. 867-876.\n' +
      '* Houo and Nichol (2023) Hewoo Jun and Alex Nichol. 2023. Shap-E: Generating conditional 3D implicit functions. arXiv:2305.02463.\n' +
      '* Kerbl et al. (2023) Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettalsik. 2023. 3D Gaussian splitting for real-time radiance field rendering. _ACM Trans. Graph._ 42, 4, Article 139 (2023), 14 pages.\n' +
      '* Laine et al. (2020) Samuli Laine, Jame Hellsten, Tero Karras, Yeongho Seel, Jaakko Lehtinen, and Timo Aila. 2020. Modular primitives for high-performance differentiable rendering. _ACM Trans. Graph._ 39, 6 (2020), 1-14.\n' +
      '* Li et al. (2023) Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. 2023. Diffusion-SDF: Text-to-shape via voxelized diffusion. In _CVPR_. 12642-12651.\n' +
      '* Li et al. (2024) Weiyi Li, Bin Chen, Xuehui Cheng, and Ping Tan. 2024. SweetDreamer: Aligning geometric priors in 2D diffusion for consistent Text-to-3D. In _ICLR_.\n' +
      '* Liu et al. (2023) Minghua Liu, Ruosi Shi, Linghao Chen, Zhuowyang Zhang, Chao Xu, Xinyung Wei, Hansheng Chen, Chong Zeng, Jiyuan Gu, and Hao Sun. 2023. One -2-3-45++: Fast single image to 3D objects with consistent multi-view generation and 3D diffusion. arXiv:2311.07885.\n' +
      '* Liu et al. (2023) Minghua Liu, Chao Xu, Haina Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. 2023. One -2-3-45: Any single image to 3D mesh in 45 seconds without per-shape optimization. In _NeurIPS_.\n' +
      '* Liu et al. (2022) Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2022. Zero-1-to-3: Zero-shot one image to 3D object. In _ICCV_.\n' +
      '* Liu et al. (2024) Yuan Liu, Cheng Lin, Zijang Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. 2024. SyncDreamer: Generating multiview-consistent images from a single-view image. In _ICLR_.\n' +
      '* Liu et al. (2023) Yuxin Liu, Minshan Xie, Huanyuan Liu, and Tien-Tian Wong. 2023d. Text-guided text-image by synchronized multi-view diffusion. arXiv:2311.12891.\n' +
      '* Liu et al. (2023) Zhenghee Liu, Peng Dai, Ruihui Li, Xiaojun Qi, and Chi-Wing Fu. 2023a. ISS: Image as testing stone for text-guided 3D shape generation. In _ICLR_.\n' +
      '* Long et al. (2023) Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, and Wenping Wang. 2023. Wonder3D: Single Image to 3D using Cross-Domain Diffusion. arXiv:2310.15008.\n' +
      '* Long et al. (2022) Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and Wenping Wang. 2022. SparseNet: Fast generalizable neural surface reconstruction from sparse views. In _ECCV_. Springer. 210-227.\n' +
      '* Tu et al. (2023) Yuanunun, Lingyang Zhang, Shiwei Li, Tian Fang, David McKinnon, Yanghai Tian, Long Quan, Xun Cao, and Yao Yao. 2023. Direct2.5: Diverse text-to-3D generation via multi-view 2.5D diffusion. arXiv:2311.15980.\n' +
      '* Melas-Kyriani et al. (2023) Luke Melas-Kyriani, Heo Liang, Christian Rupprecht, and Andrea Vedaldi. 2023. Realfusion: 360\\({}^{\\circ}\\) reconstruction of any object from a single image. In _CVPR_. 8446-8455.\n' +
      '* Mildenthal et al. (2020) Ben Mildenthal, Pratul P Srinivasan, Matthew Tarski, Jonathan T Barron, and Ravi Ramamoorthi. 2020. NeoRF: Representing scenes as neural radiance fields for view synthesis. In _ECCV_.\n' +
      '* Mittal et al. (2022) Paritul Mittal, Chen-Chi Cheng, Maneeth Singh, and Shubham Tulsiani. 2022. AutoSDF: Shape priors for 3D completion, reconstruction and generation. In _CVPR_.\n' +
      '* Nash et al. (2020) Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battenga. 2020. PolyGen: An autoregressive generative model of 3D meshes. In _ICML_. PMLR, 7220-7229.\n' +
      '* Ogun et al. (2023) Maxime Ogun, Timothee Darret, Theo Moutakamani, Hyy V. Vo, Marc Szafraniec, Vasil Khalilov, Pierre Fernandez, Daniel Hazira, Francisco Massa, Alaaudeli-Ei Noubly, Russell Horos, P-Yao Huang, Hix Yu, Aussi Sharma, Sheng-Wen Li, Wojciech Gahu, Mike Rabbat, Mido Asaran, Nicolas Ballag, Gabriel Synnase, Isman Misra, Herve Jegou, Julien Mairal, Patrick Labatatat, Armand Joulin, and Piotr Bojanovski. 2023. DING/2: Learning robust visual features without supervision. arXiv:2304.07193.\n' +
      '* Ouyang et al. (2023) Yichen Ouyang, Wenhao Cai, Jiayi Ye, Daepeng Tao, Yibing Zhou, and Gaoang Wang. 2023. Chasing consistency in text-to-3D generation from a single image. arXiv:2309.03599.\n' +
      '* Ouyuyu et al. (2017) Onuyu Ouyuyu, Yisdalslav Voroninski, Ronen Basri, and Amit Singer. 2017. A survey of structure from motion. _Acta Numerica_ 26 (2017), 305-364.\n' +
      '* Poole et al. (2023) Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. 2023. Dreamfusion: Text-to-3D using 2D diffusion. In _ICLR_.\n' +
      '* Puruswahskalam and Naik (2023) Senth Puruswahskalam and Nikik Naik. 2023. ConRad: Image constrained radiance fields for 3D generation from a single image in. In _NeurIPS_.\n' +
      '* Qian et al. (2024) Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandar Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skiorokhodov, Peter Wonka, Sergey Tulyakov, et al. 2024. Maggi-123: One image to high-quality 3D object generation using both 2D and 3D diffusion priors. In _ICLR_.\n' +
      '* Ronn et al. (2022) Robin Ronn, Andreas Bittmann, Dominic Lorenz, Patrick Esser, and Bjorn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In _CVPR_.\n' +
      '* Seitz et al. (2006) Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, and Richard Szeliski. 2006. A comparison and evaluation of multi-view stereo reconstruction algorithms. In _CVPR_. IEEE. 519-528.\n' +
      '* Shen et al. (2021) Tianchang Shen, Jun Gao, Kangcue Yin, Ming-Yu Liu, and Sanja Fidler. 2021. Deep marching* Wang et al. (2023) Zhengyi Wang, Cheng Lu, Yixai Wang, Fan Bao, Chongcuan Li, Hang Su, and Jun Zhu. 2023b. ProfileDreamer: High-fidelity and diverse text-to-3D generation with variational score distillation. In _NeurIPS_.\n' +
      '* Wang et al. (2023) Hahan Weng, Tianyu Yang, Jiannan Wang, Yu Li, Tong Zhang, Cl Chen, and Lei Zhang. 2023. Consistent123: Improve consistency for one image to 3D object synthesis. arXiv:2310.08092.\n' +
      '* Wu et al. (2023) Sangmin Wu, Byenogin Park, Hyojin Go, Jin-Young Kim, and Changjick Kim. 2023. harmonyView: Harmonizing consistency and diversity in one-image-to-3D. arXiv:2312.15980.\n' +
      '* Wu et al. (2023) Chao-Yuan Wu, Justin Johnson, Jhendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. 2023. Multiview compressive coding for 3D reconstruction. In _CVPR_.\n' +
      '* Yan et al. (2021) Xiaojiang Yan, Shizhe Hu, Yiqiao Mao, Yangdong Ye, and Hui Yu. 2021. Deep multi-view learning methods: A review. _Neurocomputing_ 448 (2021), 106-129.\n' +
      '* Yang et al. (2018) Jiayu Yang, Xiang Cheng, Yunlei Duan, Pan Ji, and Hongdong Li. 2018. ConsistNet: Exploring 3D consistency for multi-view images diffusion. arXiv:2310.1043.\n' +
      '* Ye et al. (2023) Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng Wang. 2023. Consistent:1-to-3: Consistent image to 3D view synthesis via geometry-aware diffusion models. In _3DV_.\n' +
      '* Yu et al. (2021) Alex Yu, Vickie Ye, Matthew Tanck, and Angjoo Kanazawa. 2021. pixelNeRF: Neural radiance fields from one few images. In _CVPR_.\n' +
      '* Yu et al. (2023) Wangbo Yu, Li Yuan, Yan-Pei Cao, Xiangjun Gao, Xiaoyu Li, Long Quan, Ying Shan, and Yonghong Tian. 2023. HFI-12: Towards high-fidelity one image to 3D content generation. arXiv:2310.06744.\n' +
      '* Zeng et al. (2023) Bohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng Gao, Jiaming Liu, Huaxia Li, Xu Tang, Jianxhang Liu, and Baochang Zhang. 2023. lPDreamer: Appearance-controllable 3D object generation with image prompts. arXiv:2310.053575.\n' +
      '* Zhang et al. (2022) Biao Zhang, Matthias Niessner, and Peter Wonka. 2022. 3DILG: Irregular latent grids for 3D generative modeling. In _NeurIPS_.\n' +
      '* Zhang et al. (2023) Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 2023. 3DShape2VecSet: A 3D shape representation for neural fields and generative diffusion models. _ACM Trans. Graph._ 42, A Article 92 (2023), 16 pages.\n' +
      '* Zhang et al. (2018) Richard Zhang, Phillib Ioka, Alexand A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_.\n' +
      '* Zheng et al. (2022) Xingwang Zheng, Yang Liu, Pengshuai Wang, and Xin Tong. 2022. SDF-StyleGAN: Implicit SDF-Based StyleGAN for 3D shape generation. _Comput. Graph. Forum_ 41 (2022), 52-63.\n' +
      '* Zheng et al. (2023) Xingwang Zheng, Hao Pan, Pengshuai Wang, Xin Tong, Yang Liu, and Heungyeung Shum. 2023. Locally Attenated SDF diffusion for controllable 3D shape generation. _ACM Trans. Graph._ 42, A Article 91 (2023), 13 pages.\n' +
      '* Yu et al. (2023) Zi-Xin Zhou, Zhipeng Yu, Yuan-Cheng Guo, Yangang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. 2023. Triplane meets Gaussian splitting: Fast and generalizable single-view 3D reconstruction with transformers. arXiv:2312.09147.\n' +
      '\n' +
      'Figure 8: visual comparison of 3D generation conditioned on Internet images. The image background is removed before feeding to different methods.\n' +
      '\n' +
      'Figure 7: Visual comparison of differential approaches in single-view 3D generation on the GSO dataset. For each result, we render two different views for visualization. **Ours** refers to using \\(\\text{MVD}^{2}\\) with Zero-123++. The two rightmost images are rendered views of the referenced 3D object.\n' +
      '\n' +
      'Figure 9: Generalizability test of MVD2. From left to right: prompt image or text, MVD images, the reconstructed shape rendered from four different angles. The MVD images of the first three examples, the next four examples, and the last four examples, are produced using SyncDreamer, Wonder3D, and MVDreamer, respectively.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>