<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MVD\\({}^{2}\\): 다시점 확산을 위한 효율적인 다시점 3차원 재구성\n' +
      '\n' +
      'XIN-YANG ZHENG\n' +
      '\n' +
      '마이크로소프트에서 인턴쉽을 하는 동안 일을 했다.\n' +
      '\n' +
      '중국 칭화대학교\n' +
      '\n' +
      'HAO PAN\n' +
      '\n' +
      '마이크로소프트(주)\n' +
      '\n' +
      '중국 P. R.\n' +
      '\n' +
      'Yu-XIAO GUO\n' +
      '\n' +
      '마이크로소프트(주)\n' +
      '\n' +
      '중국 P. R.\n' +
      '\n' +
      'XIN TONG\n' +
      '\n' +
      '마이크로소프트(주)\n' +
      '\n' +
      '중국 P. R.\n' +
      '\n' +
      '###### Abstract.\n' +
      '\n' +
      '유망한 3D 생성 기법으로서 멀티뷰 확산(multiview diffusion, MVD)은 일반화 가능성, 품질 및 효율성 측면에서 장점으로 인해 많은 주목을 받고 있다. 미리 훈련된 대형 이미지 확산 모델들을 3D 데이터로 미세 조정함으로써, MVD 방법들은 먼저 이미지 또는 텍스트 프롬프트에 기초하여 3D 객체의 다수의 뷰들을 생성한 다음, 멀티뷰 3D 재구성으로 3D 형상들을 재구성한다. 그러나 생성된 영상에서 희소 뷰와 일관되지 않은 세부 정보는 3D 재구성을 어렵게 만든다. 본 논문에서는 다시점 확산 영상을 위한 효율적인 3차원 복원 방법인 MVD({}^{2}\\)를 제안한다. MVD\\({}^{2}\\)는 프로젝션과 컨벌루션에 의해 영상 특징을 3차원 특징 볼륨으로 통합한 후 볼륨 특징을 3차원 메쉬로 디코딩한다. 3차원 형상 집합과 3차원 형상의 렌더링된 뷰에 의해 유도되는 MVD 영상을 이용하여 MVD\\({}^{2}\\)를 학습한다. 생성된 다시점 영상과 3차원 형상의 지상진실 뷰 간의 불일치를 해결하기 위해, 본 논문에서는 간단하면서도 효율적인 _view-dependent_training 기법을 설계한다. MVD({}^{2}\\)는 MVD의 3D 생성 품질을 향상시키고 다양한 MVD 방법에 빠르고 견고하다. 학습 후 1초 이내에 다시점 영상으로부터 3차원 메쉬를 효율적으로 디코딩할 수 있다. 본 논문에서는 Zero-123++와 ObjectVense-LVIS 3D 데이터셋으로 MVD\\({}^{2}\\)를 학습하고, 합성 영상과 실제 영상을 프롬프트로 사용하여 다양한 MVD 방법으로 생성된 다시점 영상으로부터 3D 모델을 생성하는데 우수한 성능을 보인다.\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '각주 † : 교신저자\n' +
      '\n' +
      '+\n' +
      '## 1. Introduction\n' +
      '\n' +
      '최근, 다시점 확산(multiview diffusion, MVD) 방법(Liu et al., 2023, 2024, 2024, 2024)은 새로운 3D 생성 기술로서 많은 주목을 받고 있다. 미리 훈련된 이미지 확산 모델을 3D 데이터로 미세 조정함으로써, MVD 방법은 먼저 텍스트 또는 이미지 프롬프트로부터 3D 객체의 다시점 이미지를 생성한 다음, 생성된 이미지로부터 다시점 3D 재구성을 통해 사실적인 3D 모델을 생성한다. 미리 훈련된 대형 이미지 확산 모델을 기반으로 MVD 방법은 일반화 가능성, 품질 및 효율성 측면에서 다른 3D 생성 기술에 비해 현저한 이점을 보여주었다. 그러나, NeRF(Mildenhall et al., 2020) 또는 NeuS(Wang et al., 2021)와 같은 MVD에서 사용되는 다시점 3D 재구성 방법들은 조밀하고 일관된 다시점 입력을 위해 설계되며, 이는 MVD 방법에 의해 생성된 일관되지 않은 세부사항들을 갖는 희박한 다시점 이미지들을 효율적으로 어드레싱할 수 없다. 결과적으로, 재구성된 3D 형상들은 종종 흐릿하고 왜곡된다(도 1 참조). 또한, 3차원 재구성의 최적화 과정이 느리다.\n' +
      '\n' +
      'MVD 방법들의 멀티뷰 일관성을 향상시키기 위해 여러 방법들(Liu et al., 2024, 2023; Long et al., 2023; Shi et al., 2023)이 제안되었다. 그러나, 이러한 방법들은 상이한 뷰들에 걸쳐 픽셀별 일관성을 보장하지 않는다. 일부 다른 방법들은 MVD 이미지로부터 3D 재구성 품질을 향상시키는 것을 목표로 한다. Zero-1-to-3 (Liu et al., 2023)은 2D 확산 사전(Wang et al., 2023)을 활용하여 값비싼 최적화를 통해 3D 기하학과 텍스처를 모두 정제한다. 동시 One-2-3-45++ (Liu et al., 2023) 방법은 3D 형상 재구성을 위해 2-stage multiview-conditioned 3D 확산 모델을 사용한다. 이 방법은 더 나은 3D 복원 결과를 달성하지만, 훈련에 사용되는 3D 데이터의 렌더링된 이미지와 추론에 사용되는 확산에 의해 생성된 이미지 사이의 불일치는 그 성능을 제한하고 3D 확산 프로세스는 여전히 느리다.\n' +
      '\n' +
      '본 논문에서는 MVD로 생성된 다시점 영상으로부터 3차원 형상을 복원하는 다시점 3차원 복원 방법인 MVD({}^{2}\\)를 제안한다. 다시점 영상은 일반적으로 3차원 형상을 복원할 수 있는 충분한 정보를 포함하고 있음을 인지하여, 시점 투영과 3차원 컨벌루션을 통해 다시점 영상 특징을 3차원 특징 볼륨에 직접 매핑한 후 미분 가능한 3차원 메쉬를 출력하는 경량 신경망을 설계한다. 우리는 입력 이미지의 다양한 뷰 구성에 견고하도록 네트워크를 신중하게 설계한다. 일단 훈련되면, MVD\\({}^{2}\\)는 3D 형상들을 직접 디코딩할 수 있다.\n' +
      '\n' +
      '도 1. - _Left_: NeuS(Wang et al., 2021)와 같은 기존의 3D 재구성 방법들은 멀티뷰 확산에 의해 생성된 멀티뷰 이미지들(회색 배경에서)에서의 불일치를 다루기 위해 고군분투하여, 저-품질 기하학(상부)으로 이어진다. MVD\\({}^{2}\\) 방법은 이 문제를 효과적으로 해결하고 보다 현실적인 기하학(중간)을 생성하면서도 매우 효율적(0.5초 미만)이다. 아래 행은 질감을 가진 우리의 결과를 보여준다. _ Middle_: MVD 영상으로부터 재구성된 2개의 3D 형상의 시각화_ Right_: 텍스쳐 매핑을 이용하여 다양한 3차원 형상을 재구성한다.\n' +
      '\n' +
      '최적화 없이 다양한 MVD 방법에 의해 생성된 다시점 이미지로부터.\n' +
      '\n' +
      'MVD({}^{2}\\) 네트워크를 훈련하는 것은 불일치하는 다시점 영상들의 각 집합에 해당하는 진정한 3차원 형상에 접근할 수 없기 때문에 어려운 일이다. 학습에 모든 입력 뷰를 자기 감독으로 사용할 경우, 본 모델은 전통적인 다시점 3차원 재구성 방법과 같이 뷰-뷰 불일치에 시달리게 된다. 또는 MVD 훈련 절차에 따라 3D 모양의 모음으로 네트워크를 훈련할 수 있다. 각 3D 모양에 대해 뷰 세트를 렌더링하고 MVD의 프롬프트로 하나의 뷰를 선택하여 다시점 이미지를 생성한다. 그런 다음 다른 뷰 또는 지상 진실 3D 모양을 감독으로 사용합니다. 그러나, 이러한 트레이닝 스킴은 생성된 이미지들(기초 3D 형상)과 그라운드 트루스 이미지들(트레이닝 3D 형상) 사이의 도메인 갭으로 인해 여전히 차선의 결과들(섹션 4.4에 도시됨)로 이어진다.\n' +
      '\n' +
      '렌더링된 뷰와 생성된 뷰 사이의 불일치를 조사함으로써, 불일치가 생성된 이미지의 뷰에 따라 달라지는 것을 관찰한다. 구체적으로, 프롬프트 이미지의 참조 뷰에 더 가까운 생성된 뷰는 대응하는 지상-진실 이미지와 더 일치한다. 이러한 관찰을 바탕으로, 우리는 추론된 형상을 프롬프트 뷰에서 지상-진실 기하학과 정렬하고, 다른 뷰에서 국부적인 구조적 유사성을 유지하기 위해 강제하는 _view-의존적 트레이닝 스킴_을 제안한다.\n' +
      '\n' +
      '본 논문에서는 Zero123++[14]에 의해 생성된 MVD 영상과 Obiverse-LVIS 데이터세트[11]를 이용하여 MVD\\({}^{2}\\)를 학습하고, Zero123++에 의해 생성된 미시 다시점 영상과 렌더링된 영상과 실제 영상과 텍스트 영상에 의해 생성된 MVD 방법에 대한 성능을 광범위하게 평가한다. 본 논문에서 제안한 방법을 다른 3차원 생성 방법과 비교하고, 시점 의존 학습 기법의 유효성을 검증한다. 실험 결과는 제안된 방법이 MVD에 대한 3차원 형상 재구성의 품질과 효율성을 크게 향상시키고 다양한 MVD 방법에 대해 우수한 일반화 가능성을 나타냄을 보여준다. 도. 1은 시연을 위한 몇 가지 결과를 수집한다.\n' +
      '\n' +
      '요약하면, 우리는 다음과 같은 기여를 한다:\n' +
      '\n' +
      '* 우리는 특히 다시점 확산 영상으로부터 3차원 재구성의 문제를 해결하고 3차원 생성을 위한 다시점 확산의 품질과 효율성을 크게 향상시킨다.\n' +
      '* MVD 영상과 희소성 및 불일치의 문제를 식별하고, 이러한 문제를 해결하기 위해 뷰 종속 학습 스킴으로 학습된 효율적인 경량 신경망을 제안한다.\n' +
      '* 광범위한 평가를 통해, 우리는 재구성 모델이 상이한 MVD 모델에 걸쳐 강건하게 작동하고 MVD 작업의 큰 패밀리를 보완한다는 것을 보여준다.\n' +
      '\n' +
      '향후 연구를 용이하게 하기 위해 코드와 모델을 공개할 것입니다.\n' +
      '\n' +
      '##2. 관련업무\n' +
      '\n' +
      '기반 3D 생성.__최적화 기반 3D 생성.__ 숫자 최근 작품[13, 14, 15, 16, 17, 18, 19, 20, 21]은 3D 복원을 위해 NeuS[14]와 같은 볼륨 렌더링 방법을 사용한다. One-2-3-45[14]는 SparseNeuS[15]를 이용하여 2단계 다시점 예측으로부터 3차원 기하구조를 하나의 패스로 재구성한다; One-2-3-45++는 3차원 확산 모델을 학습하여 MVD 영상을 부호화된 거리 함수로 변환한다. 두 방법 모두 지상-진실 렌더링 이미지로 훈련하고 MVD 이미지의 불일치 문제를 무시한다.\n' +
      '\n' +
      '## 3. Method\n' +
      '\n' +
      '우리는 먼저 MVD 이미지로부터 3D 복원의 문제를 공식화한다(섹션 3.1). 그런 다음 이 문제(섹션 3.2)와 함께 고유한 문제를 드러내는 관찰을 한다. 이러한 문제를 해결하기 위해, 본 논문에서는 MVD 영상을 위한 훈련 전략과 경량화된 신경망을 제시하고, 효율적인 재구성 품질(섹션 3.3과 3.4)을 제공한다.\n' +
      '\n' +
      'MVD 영상을 이용한###3차원 복원\n' +
      '\n' +
      'MVD 모델\\(\\mathcal{M}\\)은 시점\\(\\mathbf{v}_{0}\\)(또는 텍스트 프롬프트)에서 입력 기준 영상\\(\\mathbf{I}_{0}\\)을 조건으로 하여 3D 생성에 접근하고, 새로운 시점\\(\\mathcal{C}=[\\mathbf{v}_{i}]]에서 대상 3D 객체의 영상\\([\\mathbf{I}_{i}]=\\mathcal{M}(\\mathbf{I}_{0})\\)을 생성한다. 모델은 일반적으로 사전 훈련된 대규모 이미지 생성 모델(_e.g._, 안정적인 확산[Rombach et al., 2022])로부터 적응되고, 생성된 이미지의 일관성을 향상시키기 위해 대규모 데이터세트[Deitke et al., 2023, 20]로부터 3D 객체의 멀티뷰 렌더링 상에서 미세 조정된다. 이러한 MVD 이미지들이 주어지면, 최종 3D 객체를 획득하기 위해 재구성 알고리즘(_e.g._, NeuS 및 변형들[Long et al., 2022; Wang et al., 2021])이 전형적으로 적용된다. 그러나, 희소성과 MVD 이미지의 정확한 일관성의 결여로 인해(섹션 3.2), 전형적인 멀티뷰 3D 재구성 알고리즘은 잘 작동하지 않는다[Long et al., 2023]. 품질과 효율성을 달성하기 위해, 우리는 입력으로서 \\([\\mathbf{I}_{i}]\\)을 취하고 출력으로서 \\(\\mathcal{S}=\\text{MVD}^{2}([\\mathbf{I}_{i}])\\을 복구하는 재구성 모델 \\(\\text{MVD}^{2}\\)을 학습할 것을 제안한다. 다음으로 논의되는 바와 같이 MVD 재구성에 대한 독특한 도전을 드러내는 \\(\\mathcal{S}\\)을 감독하는 방법에 대한 문제가 남아 있다.\n' +
      '\n' +
      'MVD 재구성 딜레마\n' +
      '\n' +
      'MVD 영상들에 대한 관측들.__VD 영상들에 대한 관측들.__ 기술 세부사항에서 MVD 모델의 차이에도 불구하고(Sec. 2) , 우리는 다음의 관찰들을 그들의 생성된 이미지들에 공통으로 만든다:\n' +
      '\n' +
      '**P1**: MVD 이미지의 뷰포인트들이 알려져 있고 희박하게 흩어져 있다.\n' +
      '**P2**: MVD 이미지는 3D로 일관되는 경향이 있지만 정밀도가 부족하다.\n' +
      '**P3**: 입력 뷰에 가까울수록, 더 잘 생성된 이미지들이 트레이닝 오브젝트와 매칭된다.\n' +
      '**P1**: MVD의 제형으로부터 유래한다. 특히, 생성된 영상들의 시점들을 명시적으로 지정함으로써, 3차원 복원 알고리즘들에 대한 포즈 추정의 어려움을 덜어준다. MVD 영상의 희소성은 제한된 계산 능력의 결과이다. MVD 모델들은 향상된 일관성과 동시에 이미지의 제한된 수(_e.g._, 4-16)를 생성하도록 트레이닝된다; 그 이상으로, 메모리 및 계산 비용은 견딜 수 없게 된다[Long et al., 2023; Shi et al., 2024].\n' +
      '\n' +
      '**P2**는 또한 MVD 접근법에서 자연스럽게 따른다. 실제로, MVD 모델들은 교차-시점 변조(cross-view modulation; Liu et al., 2024; Shi et al., 2023, 2024)의 다양한 기법들에 의해 멀티뷰 일관성을 향상시키기 위해 노력하지만, 생성된 이미지들의 픽셀별 3D 일관성의 보장은 없다.\n' +
      '\n' +
      '**P3**는 도 2를 조사하여 확인할 수 있는 관찰을 제시한다. 이 도면에서 \\(\\mathbf{v}_{0}\\)은 훈련에 사용되는 3D 객체의 렌더링된 뷰를 나타내며, 이는 MVD 모델 -- Zero-123++ [Shi et al., 2023]에 대한 입력으로 작용한다. 이 모델은 \\(\\mathbf{v}_{1}\\)에서 \\(\\mathbf{v}_{6}\\)으로 표시되는 다양한 관점에서 이미지를 생성한다. 주목할 점은 \\(\\mathbf{v}_{1}\\)과 \\(\\mathbf{v}_{6}\\)이 \\(\\mathbf{v}_{0}\\)에 더 가깝다는 것이다. MVD-1과 MVD-2의 두 가지 영상을 생성하였는데, 이 중 \\(\\mathbf{v}_{1}\\)과 \\(\\mathbf{v}_{6}\\)의 영상은 다른 시점의 영상에 비해 지상진실영상과 더 높은 유사도를 보인다. 또한 생성된 뷰와 GT 뷰 간의 이미지 차이를 PSNR 메트릭을 사용하여 비교하여 240개의 3D 모델에 대한 관찰을 정량적으로 검증했다(표 1 참조).\n' +
      '\n' +
      '종합하면, 이러한 관찰은 MVD 이미지로부터 3D 복원에 대한 다음과 같은 독특한 도전을 제시한다.\n' +
      '\n' +
      'MVD 재구성 딜레마.___ 우리는 MVD 이미지로부터 3D 복원에 딜레마가 있다는 것을 주목한다. 한편, 재구성 모델의 학습을 감독하기 위한 지상진실 3차원 형상은 존재하지 않는다. 왜냐하면, **P3**에서 언급한 바와 같이, 기준 뷰에서 벗어난 시점의 MVD 영상은 학습 객체\\(\\overline{\\mathcal{S}}\\)에서 벗어나고,\\(\\mathcal{S}\\) 접근법\\(\\overline{\\mathcal{S}}\\)은 입력 영상\\([\\mathbf{I}_{i}]\\)과 모순될 수 있기 때문이다. 한편, 렌더링된 이미지를 \\([\\mathbf{I}_{i}]\\)과 직접 비교하는 것도 문제가 될 수 있는데, 이는 **P2**에 따르면 \\([\\mathbf{I}_{i}]\\)이 3D에서 정확하게 일치하지 않기 때문이다.\n' +
      '\n' +
      '우리는 모순된 상황이 최적이 아닌 결과를 생성하는 것으로 나타나는 4.4항의 실험을 통해 딜레마를 검증한다. 이러한 입출력 불일치의 문제를 해결하기 위해 3D 형상을 프록시로 사용하고, 이미지 프롬프트 MVD 모델을 통해 학습 입력을 구성하며, 딜레마를 피하는 설계 손실 함수를 사용한다. 훈련이 이미지 프롬프트 MVD 모델에 의존하지만 테스트 시간에 우리 모델은 텍스트 프롬프트 모델의 MVD 이미지를 처리하는 데 자연스럽게 확장된다는 점에 유의해야 한다.\n' +
      '\n' +
      '### View-dependent training\n' +
      '\n' +
      '각 객체\\(\\overline{\\mathcal{S}\\)에 대해, 임의의 시점에서 기준 영상\\(\\mathbf{I}_{0}\\)으로 렌더링하고, 지정된 시점에서는 프록시 영상\\([\\mathbf{v}_{i}]\\([\\overline{\\mathbf{I}_{i}]]]으로 렌더링한다. 이에 대응하여 \\([\\mathbf{I}_{i}]=\\mathcal{M}(\\mathbf{I}_{0})\\)는 MVD({}^{2}\\)의 입력으로 사용될 MVD 영상의 집합이다. 우리는 다음과 같이 재구성 손실을 공식화한다:\n' +
      '\n' +
      '\\[\\min\\sum_{i=0}^{N}\\text{dis}\\left([\\pi_{i}(\\mathcal{S}),\\overline{\\mathbf{I}_{i}}],\\mathbf{v}_{i}\\right), \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(\\pi_{i}(\\mathcal{S})\\)는 시점 \\(\\mathbf{v}_{i}\\)에서 \\(\\mathcal{S}\\)의 미분 가능한 렌더링이고, \\(\\text{dis}([\\cdot,\\cdot],\\mathbf{v}_{i})\\)는 시점 \\(\\mathbf{v}_{i}\\)에 따라 두 이미지 사이의 불일치를 다르게 정량화하는 뷰 의존적 손실 함수이다. 구체적으로, **P3**에 따르면, 참조 뷰의 \\(\\mathbf{v}_{0}\\)에서 복구된 모양이 픽셀 단위의 세부 사항에서 \\(\\overline{\\mathbf{I}_{0}\\)과 완전히 일치할 수 있을 것으로 예상되지만, 다른 관점에서는 구조적 유사성만을 요구한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & \\(\\mathbf{v}_{1}\\) & \\(\\mathbf{v}_{2}\\) & \\(\\mathbf{v}_{3}\\) & \\(\\mathbf{v}_{4}\\) & \\(\\mathbf{v}_{5}\\) & \\(\\mathbf{v}_{6}\\) \\\\ \\hline\n' +
      '**PSNR\\(\\uparrow\\)** & **25.61** & 23.57 & 23.37 & 23.39 & 23.77 & **25.01** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1. 생성된 뷰와 GT 뷰 간의 이미지 차이의 정량적 평가, 240개의 객체에 대해 평균을 냈다.\n' +
      '\n' +
      '그림 2. 기준 영상에서 시점이 멀어질수록 훈련 대상으로부터의 불일치가 증가한다.\n' +
      '\n' +
      '는 \\(\\overline{I_{i}},i\\neq 0\\). 따라서 우리는\n' +
      '\n' +
      '[\\mathbf{x},\\mathbf{y}],v_{i})=\\begin{cases}\\mathcal{L}_{pixel}(\\mathbf{x},\\mathbf{y})&\\text{for view }v_{0}\\mathcal{L}_{LPIPS}(\\mathbf{x},\\mathbf{y})&\\text{for view }v_{i},i\\neq 0\\end{cases}\\tag{2}\\text{for view }v_{i},\\mathbf{y}}}\\text{for view }v_{l}_{l}{LPIPS}(\\mathbf{x},\\mathbf{y}}}\\text{for view }v_{i},i\\neq 0\\end{cases}\\tag{2}\\text{for view }\n' +
      '\n' +
      '여기서, \\(L_{LPIPS}\\) 손실은 미리 훈련된 이미지 백본을 사용하여 두 이미지 간의 지각적 패치 유사성을 측정한다(Zhang et al., 2018).\n' +
      '\n' +
      '기하학과 텍스쳐의 분산 재구성은 일반적으로 기하학과 텍스쳐 모두에서 더 나은 품질로 이어진다(Chen et al., 2023). 이 연구에서는 이러한 접근 방식을 따르고 형상 기하학의 복원에 초점을 맞추고, 이후 재구성된 상세 형상에 대한 다시점 텍스처 매핑을 적용한다(섹션 4.1). 지오메트리 재구성을 감독하기 위해 \\(v_{0}\\) 픽셀 단위의 손실을 다음과 같이 다시 쓴다.\n' +
      '\n' +
      '\\mathcal{L}_{\\text{d}}\\left(\\mathbf{d},\\overline{\\mathbf{d}}\\overline{\\mathbf{d}(p)}{\\overline{\\text{n}}\\left(\\mathbf{n},\\overline{\\mathbf{m}}(p)\\cdot\\overline{\\mathbf{n}}(p)\\cdot\\overline{\\mathbf{n}}(p)|,\\m](5)\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\\n' +
      '\n' +
      '깊이, 정규 및 마스크 맵은 각각 \\(\\mathbf{d},\\mathbf{n},\\mathbf{m}\\)이다. 여기서 \\(p\\)은 픽셀에 걸쳐 반복되며, \\(d_{min}\\)은 경계 구의 최소 깊이 값이며, MVD 모델의 임의의 카메라 변환을 정규화하기 위해 차감된다.\n' +
      '\n' +
      '\\(v_{0}\\) 이외의 뷰에 대해서는 구조 유사성 손실을 정규 지도에 적용한다. 따라서, 전체 손실은\n' +
      '\n' +
      '\\text{lambda_{1}(\\mathbf{d}_\\mathbf{0}},\\moverline{\\mathbf{n}_{\\text{n}_{\\mathbf{i}}}(\\mathbf{n}_{\\mathbf{a}_{\\mathbf{i}})},\\moverline{m}_{\\mathbf{n}_{\\mathbf{0}}}\\m\\\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\\n' +
      '\n' +
      '여기서 \\(\\mathcal{L}_{\\text{reg}\\)는 3D 메쉬에 대한 정규화 손실이다(섹션 3.4). 경험적으로 \\(\\lambda_{1}=\\lambda_{3}=1.0,\\lambda_{2}=0.2,\\lambda_{4}=0.1\\)을 설정하였다. 각 학습 샘플에 대해 참조 뷰만이 강한 픽셀별 안내를 제공하고 다른 뷰는 형상 구조를 조절하지만, 우리는 임의의 참조 뷰(Sec. 4.1)로 학습 샘플을 합성했으며, 이는 학습된 모델이 임의의 뷰에서 정확한 기하학을 복원할 수 있음을 보장한다.\n' +
      '\n' +
      '### Reconstruction model\n' +
      '\n' +
      '영상 시점과 해상도를 포함한 다양한 MVD 모델을 보완하기 위해 MVD\\({}^{2}\\)을 특정 MVD 모델과 독립적이고 다양한 희소 이미지 세트를 일반화 가능한 방식으로 수용하도록 설계한다. 특히, MVD\\({}^{2}\\) 모델은 주어진 시점에 따라 이미지 특징을 가져와 3D 특징을 복원하고, 이미지 번호에 불변인 최소한의 국부 변환 후에 메쉬 표현에서 고품질의 3D 기하를 생성한다. 우리는 그림 1에서 접근법의 개요를 설명한다. 3. 다음 모델 세부 정보를 확장합니다.\n' +
      '\n' +
      '__2D-to-3D 특징 변환.__2D-to-3D 특징 변환.__2D-to-3D 특징 변환.__2D-to-3D 특징 변환. 특정 MVD 모델의 독립성을 보장하기 위해 미리 훈련된 DINOv2 모델(Qoub et al., 2023)을 사용하여 MVD 이미지\\([\\mathbf{I}_{i}]\\)를 2D 특징 맵으로 변환한다. DiNOv2 특징맵의 원래 해상도는 \\(37\\times 37\\)이고, 특징맵을 \\(64\\times 64\\) 해상도로 업샘플링하여 뷰 공유 2D 컨벌루션을 통해 제작한다. 결과적인 특징 맵은 \\([\\mathbf{F}_{i}\\in\\mathbb{R}^{Dud}]\\)으로 표시된다. 이 2차원 특징맵으로부터 3차원 점별 특징을 얻기 위해 공간적인 점\\(p\\in G\\)이 해상도\\(R_{\\text{G}}^{3}\\)의 정규 격자인 공간적인 점\\(R_{\\text{G}}^{3}\\)을 주어 단위입방공간을 분할한 후, 지정된 뷰 투영을 통해 \\([\\mathbff{f}_{p,i}=\\mathbff{f}_{i}\\left(\\pi_{i}(p)\\right)]\\)에 대한 이미지 특징을 가져오며, 여기서 \\(\\mathbf{F}(\\cdot)\\)는 이진 보간을 통해 이미지 특징맵을 질의한다. 영상의 수와 순서에 불변성을 유지하기 위해 평균 풀링을 사용하여 다시점 지원에서 PointNeRF(Yu et al., 2021)의 설계와 유사하게 영상 특징을 하나의 특징 벡터인 _i.e., \\([\\mathbf{f}_{p}=\\text{avg}\\left(\\mathbf{f}_{p,i}\\right)]\\으로 융합한다. 이어서, 3D 특징 그리드는 메쉬 표현을 추출하기 전에 적은 수의 3D 컨볼루션 층들에 의해 변환된다. 전반적으로, 네트워크 모델은 MVD 이미지로부터 재구성에 충분한 몇 개의 레이어와 매개변수를 포함하여 가볍다. 학습 단계에서 손실 계산에는 \\(v_{0}\\)(섹션 3.3)의 이미지 뷰가 단독으로 사용되며 추론을 위해 네트워크에 의해 요구되지 않는다는 점에 유의하는 것이 중요하다.\n' +
      '\n' +
      '도 3. 방법 개요. MVD 모델은 기준 이미지에 기초하여 상이한 시점들로부터의 이미지들의 세트를 생성한다. MVD\\({}^{2}\\)는 거친 3차원 격자(G\\)의 각 점에 대해 이들 영상으로부터 특징을 추출하고 평균하여 보다 미세한 격자(G^{\\prime}\\)로 보간하고, 이로부터 표면 메쉬를 미분 가능하게 추출한다. 학습 중 메쉬 재구성은 참조 뷰 \\(v_{0}\\)에서 깊이/정규/마스크 맵에 대한 픽셀별 손실(빨간색 화살표)과 다른 뷰에서 정규 맵에 대한 구조적 손실(노란색 화살표)로 감독된다. 복원된 메시는 MVD 이미지들에 매핑함으로써 텍스처링될 수 있다.\n' +
      '\n' +
      '_Shape representation.___Shape representation. 3D 자산으로서의 적합성과 컴팩트성을 위해 삼각형 메쉬를 형상 표현으로 선택하고 FlexiCubes(Shen et al., 2023)를 채택하여 차별화 가능한 메쉬 출력을 생성한다. FlexiCubes의 핵심은 학습된 특징 그리드를 부호화된 거리와 변형 함수로 변환하는 것이며, 이로부터 등위면 메쉬는 정육면체의 마칭에 의해 추출된다. 특히 3차원 특징격자로부터 격자점 특징을 삼선형 보간한 플렉시큐브의 정의격자로 형상(R^{3}_{G^{\\prime}\\)의 고해상도 격자(G^{\\prime}\\)를 사용한다. 각 격자점\\(p\\in G^{\\prime}\\)에 대해, 얕은 MLP로 구현된 학습된 매핑을 통해 부호 거리 값과 변형 파라미터를 구한다. 본 논문에서는 Flexicubes의 규칙화기를 이용하여 거리장과 표면 메쉬의 불필요한 진동을 줄이는 \\(\\mathcal{L}_{\\text{reg}\\)을 정의한다.\n' +
      '\n' +
      '##4. 실험 분석\n' +
      '\n' +
      '이 절에서 \\(\\text{MVD}^{2}\\)에 대한 철저한 평가를 제시한다. 섹션 4.1은 훈련 설정 및 평가 프로토콜에 대해 자세히 설명한다. 4.2절은 정량적, 정성적 결과를 보여주며, \\(\\text{MVD}^{2}\\)을 다른 방법과 비교한다. 4.3절에서는 \\(\\text{MVD}^{2}\\)의 다른 \\(\\text{MVD}\\) 모델에 대한 적용 가능성을 검토한다. 섹션 4.4는 \\(\\text{MVD}^{2}\\)의 훈련 방식 이면의 근거를 설명한다. 섹션 4.5에서는 접근법의 한계에 대해 설명한다.\n' +
      '\n' +
      '### Experiment setup\n' +
      '\n' +
      '_트레이닝 데이터 준비.__ 본 논문에서는 입력 영상에 대해 방위각이 고정된 상태에서 입력 뷰에 대해 방위각을 갖는 6개의 뷰를 생성하여 훈련용 영상인 Zero-123++(Shi et al., 2023) 모델을 채택하였다. 우리는 다양하고 텍스처링된 3D 객체들을 갖는 Objavverse의 큐레이션된 서브세트인 Objavverse-LVIS(Deitke et al., 2023)를 우리의 3D 트레이닝 데이터로 사용한다. 각 객체에 대해 0-123++의 해상도를 갖는 랜덤 뷰에서 3개의 이미지를 렌더링하고, 각 뷰 이미지에 대해 6개의 그룹의 다시점 이미지를 생성한다. (\\text{MVD}\\) 영상은 320\\(320\\times 320\\)의 해상도를 갖는다. 또한 Laine et al., 2020의\\(\\text{N}\\)Wdiffrast library를 이용하여 \\(512\\times 512\\)의 해상도로 \\(\\text{MVD}\\) 영상의 뷰를 포함한 동일한 뷰에서 3차원 물체의 깊이와 정규 지도를 렌더링한다.\n' +
      '\n' +
      '_네트워크 설정.___네트워크 설정.___ 학습 가능한 매개변수인 \\(\\text{MVD}^{2}\\)는 3D 특징을 보간한 후 4개의 2D 컨볼루션 레이어(채널 dims: \\([768,512,256,128,32]\\), 커널 크기: 3)와 4개의 3D 컨볼루션 레이어(채널 dims: \\([32,32,32,32]\\), 커널 크기: 3)로 구성된다. 이에 대응하여 우리는 \\(D_{2d}=128\\), \\(R_{G}=32\\) 및 \\(R_{G^{\\prime}=80\\)을 갖는다. 우리는 약 12시간 동안 GPU당 배치 크기가 1인 8개의 NVIDIA(16G) V100 GPU에서 100k 단계에 대해 모델을 훈련했다.\n' +
      '\n' +
      '_Network 추론._\\\\ (\\text{MVD}^{2}\\)는 네트워크 추론을 위한 입력으로 알려진 시점을 갖는 \\(\\text{MVD}\\) 영상을 취한다. DINOv2 특징 추출은 0.2초, 네트워크의 순방향 통과는 NVIDIA GeForce RTX 3090 GPU에서 5.2GB의 최대 GPU-메모리 사용으로 0.2초의 높은 효율성을 달성한다. 이것은 15분이 걸리는 NeuS보다 훨씬 빠릅니다.\n' +
      '\n' +
      '__평가 프로토콜.___ 재구성된 도형의 기하학적인 품질을 평가하기 위해 기존의 \\(\\text{MVD}\\) 모델과 \\(\\text{MVD}^{2}\\) 모델의 학습에는 사용되지 않기 때문에 테스트 세트로 _Google Scan Objects_ (GSO) 데이터셋(Downs et al., 2022)을 사용한다. 평가 및 비교를 위해 GSO에서 50개의 다양한 객체를 선택한다. 각 객체에 대해, 서로 다른 이미지-조건 \\(\\text{MVD}\\) 모델과 이미지-대-3D 모델에 대해 랜덤 뷰를 입력으로 렌더링한다. 제안된 방법이나 다른 경쟁적인 방법으로 3차원 복원 후, 32개의 공간적으로 균일한 뷰로부터 객체를 렌더링하여 깊이 및 정규 지도를 얻는다. 우리는 원래의 3D 객체로부터 렌더링된 지상-진실 뷰들에 대해 이들 기하학적 맵들의 PSNR, SSIM(Structural similarity (Wang et al., 2004)), LPIPS(learned perceptual image patch similarity (Zhang et al., 2018)) 메트릭들을 계산한다. 또한 2048개의 균일하게 샘플링된 지점을 사용하여 챔퍼 거리(CD)와 지구 이동자 거리(EMD)를 평가한다. 첨자 \\(d\\)와 \\(n\\)은 깊이 맵과 정규 맵에서 각각 계산됨을 나타내기 위해 사용된다. 가독성을 향상시키기 위해 CD, EMD, SSIM, LPIPS의 값을 모두 100으로 곱하지만, 이 평가 프로토콜은 기준으로부터 \\(\\text{MVD}\\) 영상의 다양성을 무시하고 기준 객체와의 관계에서만 복원 품질을 측정한다. 따라서 형상 품질과 사실성을 평가하기 위해서는 육안 검사도 필수적이다.\n' +
      '\n' +
      '_Texture mapping.___Texture mapping.__ 본 논문에서는 기하구조를 재구성한 후 텍스쳐 매핑을 위해 다시점 영상을 \\(\\text{UV}\\)-맵으로 변환하는 간단한 알고리즘을 개발하였다. 알고리즘은 4단계로 구성된다: (1) _color initialization_: 투영된 삼각형의 가장 큰 가시 영역을 갖는 뷰를 선택하고 텍스처 좌표를 사용하여 각 표면 포인트에 색상을 할당한다; (2) _color blending_: 각 표면 포인트 주변의 작은 이웃의 색상을 평균하여 색상 천이를 매끄럽게 한다; (3) _color filling_: 주변 영역에서 색상을 전파하여 가려짐으로 인해 색상이 할당되지 않은 틈을 채운다. 이 알고리즘은 빠르다: 한 모양에 0.5초가 걸린다. 그러나 이는 그림 1과 같은 시각화 목적으로만 사용된다. 1. 영상의 불일치를 해결하지 못하거나, 더 나은 영상 함량으로 폐색된 영역을 인페인팅하지 못한다. 우리는 향후 작업을 위해 이러한 도전을 연기합니다.\n' +
      '\n' +
      '### Performance evaluation\n' +
      '\n' +
      '정량 평가.__ 표 2의 첫 번째 행에서 볼 수 있듯이, Zero-123++에 의해 생성된 \\(\\text{MVD}^{2}\\) 이미지로부터 \\(\\text{MVD}^{2}\\) 방법이 NeuS보다 3차원 기하구조를 복원하는데 더 우수한 성능을 보였다. SSIM 값이 높고 LIPIPS 값이 낮을수록 \\(\\text{MVD}^{2}\\)은 GSO 데이터와 구조적으로 더 유사한 기하구조를 생성함을 나타낸다. 도. 도 4는 두 가지 예에 대한 시각적 비교를 도시한다. NeuS는 멀티뷰 불일치를 설명하지 않기 때문에 더 왜곡된 기하학을 생성한다. 반면에, \\(\\text{MVD}^{2}\\)의 결과는 시각적으로 더 만족스럽다.\n' +
      '\n' +
      '_Robustness.__ robustness.__ 또한, 미리 학습된 \\(\\text{MVD}\\) 영상은 다른 \\(\\text{MVD}\\) 모델에 의해 생성된 \\(\\text{MVD}\\) 영상을 동일한 시점을 이용하여 복원할 수 있다. 시점 합성을 위해 임의의 시점을 지원하는 Zero-1-to-3 모델(Liu et al., 2023)의 향상된 버전인 Stable Zero123(sta 2023)을 사용한다. Zero-123++와 동일한 카메라 설정을 사용하여 \\(\\text{MVD}\\) 이미지를 생성한다. NeuS와 \\(\\text{MVD}^{2}\\)를 이용하여 \\(\\text{MVD}\\) 영상으로부터 3차원 형상을 재구성한다. 표 2의 두 번째 행은 CD를 제외한 대부분의 메트릭에서 MVD\\({}^{2}\\)이 NeuS보다 크게 우수함을 보여준다.\n' +
      '\n' +
      'MVD 기반 방법과의 비교 또한 SyncDreamer[11], Wonder3D[14], One-2-3-45[11]의 다른 MVD 방법과 비교하였다. SyncDreamer는 균일하게 샘플링된 방위각과 고정된 고도각이 30\\({}^{\\circ}\\)인 16개의 뷰를 생성하고, Wonder3D는 6개의 직교 사이드 뷰를 생성한다. 우리는 3D 복원을 위해 기본 NeuS 기반 구현을 사용한다. One-2-3-45는 Zero-1-to-3을 사용하여 36개의 뷰 이미지를 생성하고 3D 복원을 위해 미리 훈련된 SparseNeuS를 사용한다. 표 2의 세 번째 행에 제시된 정량적 보고서는 MVD\\({}^{2}\\) 또는 NeuS와 MVD\\({}^{2}\\) 및 Stable Zero123보다 훨씬 더 나쁜 성능을 나타냄을 나타낸다. 또한, 0-123++와 동일한 MVD 모델에 의해 MVD 영상이 생성되는 MVD-영상-조건 3D 확산을 이용하여 3D 형상을 생성하는 One-2-3-45++[11]의 동시 방법을 시험한다. 이 방법의 코드는 공개되지 않았기 때문에, 우리는 시각적 평가를 위한 몇 가지 샘플을 생성하기 위해 상용 웹 서비스만 사용한다.\n' +
      '\n' +
      '이미지 대 3D 방법과의 비교 우리는 3D 암시적 함수를 생성하기 위해 조건부 확산 모델을 사용하는 Shap-E[13] 및 대형 변압기 모델에 의해 입력 이미지를 NeRF에 매핑하는 LRM[14]을 포함하여 멀티뷰를 사용하지 않는 최근 이미지 대 3D 접근법과 우리의 접근법을 추가로 비교한다. LRM의 공식 구현이 불가능하기 때문에 비교를 위해 제3자 구현[11]을 사용한다. 그 결과는 표 2의 세 번째 행에 나와 있으며 LRM과 Shap-E는 모두 MVD\\({}^{2}\\)인 Zero-123++와 MVD\\({}^{2}\\)인 Stable Zero123에 비해 열등하다.\n' +
      '\n' +
      '시각적 비교는 GSO 데이터 세트에서 렌더링 이미지의 하위 집합과 인터넷에서 무작위 이미지를 선택하여 위의 비교 방법에 대한 입력으로 사용한다. 복원된 3D 형상들은 그림에서 시각적으로 비교된다. 도 7 및 도 8에 도시된 바와 같이, 시각화는 우리의 접근법이 더 미세한 기하학적 세부사항들로 더 시각적으로 설득력 있는 결과들을 생성한다는 것을 입증한다.\n' +
      '\n' +
      '단일 영상-3D 작업에서 6개의 뷰(75개의 확산 단계, Zero-123++)를 생성하는 데 12초, MVD\\({}^{2}\\)를 통해 최종 텍스처 메쉬를 생성하는 데 1초 미만의 시간이 필요하다. 이는 3D 결과를 생성하는 데 12초가 걸리는 Shape-E와 비슷하지만 5초만 필요한 LRM보다 느리다. 비교된 다른 방법은 MVD 생성, NeuS 최적화 또는 3D 확산으로 인해 훨씬 느리다.\n' +
      '\n' +
      '### Model generalizability\n' +
      '\n' +
      'MVD\\({}^{2}\\)는 뷰 위치에 따라 이미지 특징을 추출하고 평균 풀링을 적용하여 서로 다른 뷰의 특징을 결합하므로 이론상 다양한 수와 시점을 수용할 수 있다. 따라서 본 논문에서는 Zero-123++의 출력으로 미리 학습된 MVD\\({}^{2}\\)가 다른 MVD 모델에 의해 생성된 MVD 영상에 다른 뷰 설정으로 적응할 수 있는지 살펴본다.\n' +
      '\n' +
      '이미지 조절 MVD 모델에 대한 일반화 가능성은 SyncDreamer와 Wonder3D를 일부 온라인 이미지로 테스트하고 미리 훈련된 MVD\\({}^{2}\\)을 사용하여 MVD 이미지에서 3D 모양을 재구성했다. 도 1에 도시된 바와 같다. 도 9에 도시된 바와 같이, 본 방법은 이러한 입력들을 효과적으로 처리하고 시각적으로 즐거운 형상들을 생성할 수 있다.\n' +
      '\n' +
      '텍스트 조건 MVD에 대한 일반화 가능성 또한 텍스트 조건 MVD 모델인 MVDream[11]에 의해 생성된 4시점 영상에 MVD\\({}^{2}\\)을 적용한다. 를 포함하는 것을 특징으로 하는 반도체 소자의 제조 방법. 도 9에 도시된 바와 같이, MVD\\({}^{2}\\)는 MVDream의 출력을 잘 처리하고, 설득력 있는 형상 형상을 생성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c|c c c c c c c} \\hline \\hline\n' +
      '**MVD Model** & \\(N_{I}\\) & **Recon.** & **CD** & **EMD** & **PSNR\\({}_{d}\\)\\(\\uparrow\\)** & **SSIM\\({}_{d}\\)\\(\\uparrow\\)** & **LPIPS\\({}_{d}\\)\\(\\downarrow\\)** & **PSNR\\({}_{n}\\)\\(\\uparrow\\)** & **SSIM\\({}_{n}\\)** & **LPIPS\\({}_{n}\\)\\(\\downarrow\\)** \\\\ \\hline Zero-123++ [11] & 6 & NeuS & 1.543 & 16.02 & 21.62 & 87.01 & 15.52 & 16.39 & 72.89 & 22.49 \\\\  & 6 & MVD\\({}^{2}\\) & **1.044** & **13.58** & **23.31** & **89.20** & **10.80** & **18.28** & **80.01** & **16.34** \\\\ \\hline Stable Zero123 [12] & 6 & NeuS & 2.146 & 19.95 & 19.91 & 84.37 & 20.31 & 14.91 & 69.65 & 27.24 \\\\  & 6 & MVD\\({}^{2}\\) & 2.631 & 18.66 & 20.98 & 86.88 & 15.13 & 16.34 & 77.04 & 21.38 \\\\ \\hline SyncDreamer [11] & 16 & NeuS & 2.082 & 19.19 & 20.42 & 86.18 & 15.44 & 15.87 & 75.29 & 22.99 \\\\ Wonder3D [14] & 6 & NeuS & 2.347 & 21.54 & 19.68 & 85.45 & 18.10 & 15.30 & 74.59 & 25.50 \\\\ One-2-3-45 [11] & 36 & SparseNeuS & 5.121 & 26.82 & 17.86 & 83.83 & 22.13 & 13.90 & 70.42 & 30.26 \\\\ \\hline Shap-E [13] & - & - & 4.553 & 24.48 & 19.18 & 84.48 & 18.83 & 14.83 & 73.31 & 26.05 \\\\ LRM [14] & - & - & 1.716 & 17.12 & 20.34 & 86.04 & 15.28 & 15.61 & 69.65 & 24.04 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2. GSO 데이터셋에 대한 단일 시점 3D 재구성의 정량적 평가 [14], _Recon._ 는 MVD 이미지들에 대한 재구성 방법을 참조한다. \\ (N_{I}\\)는 3D 복원에 사용되는 MVD 영상의 개수를 나타낸다.\n' +
      '\n' +
      '그림 4. Zero123++의 MVD 영상의 3D 재구성. NeuS와 MVD\\({}^{2}\\)의 결과는 세 가지 다른 관점에서 각각 파란색과 청록색 톤으로 렌더링된다.\n' +
      '\n' +
      '### Ablation study\n' +
      '\n' +
      '훈련 방법의 효과를 평가하기 위해 지상진실 영상 또는 생성된 영상을 손실 감독에 사용하는지 여부와 뷰 의존적 손실을 적용하는지 여부에 따라 다른 5가지 설정으로 절제 연구를 수행한다.\n' +
      '\n' +
      '1. [label=**A0**]\n' +
      '2. 네트워크는 Zero-123++로 지정된 뷰에서 렌더링된 3D 데이터의 지상진실 영상을 입력으로 한다. 손실을 계산하기 위해 64개의 임의의 뷰에서 훈련된 3D 객체와 재구성된 메쉬를 렌더링하고, 모든 뷰에 \\(\\mathcal{L}_{pixel}\\)과 \\(\\mathcal{L}_{IPIPS}\\)을 모두 적용한다.\n' +
      '3. Setup은 **A1**과 동일하나, One-2-3-45++(Liu et al., 2023b)을 따르고 트레이닝 중에 이미지 특징을 가져올 때 랜덤하고 작은 뷰 섭동을 추가한다. 우리는 이러한 섭동이 MVD 이미지의 견고성을 개선하고 불일치를 더 잘 처리할 수 있는지 여부를 테스트하는 것을 목표로 한다.\n' +
      '4. 입력 MV 영상이 Zero-123++에 의해 생성된 MVD 영상인 것을 제외하고는 **A1**와 셋업이 동일하다.\n' +
      '5. Setup은 참조 뷰 \\(v_{0}\\)와 손실 계산에서 6개의 다른 뷰에서만 이미지를 사용하는 것을 제외하고는 **A3**와 동일하다.\n' +
      '6. 우리의 기본 설정, 여기서 \\(\\mathcal{L}_{pixel}\\)은 \\(v_{0}\\) 보기에만 적용된다.\n' +
      '\n' +
      '표 3에서 볼 수 있듯이 훈련 설정이 기본 설정에 접근함에 따라 지오메트리 품질이 향상됩니다. 지상-진실 렌더링 이미지를 사용한 훈련은 뷰 섭동이 문제를 어느 정도 완화시키지만 최악의 결과를 산출한다. **A3**와 **A4**는 모든 뷰에 \\(\\mathcal{L}_{pixel}\\)을 적용하는 것이 우리의 뷰 종속 학습 방식보다 덜 효과적임을 보여준다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 5를 참조하면, 기본 설정 **A5**가 플로팅 기하학을 명확하게 피하고 알파벳 퍼즐 보드의 빈 영역(위), 글러브의 손가락(중간), 마녀 모델의 팔(아래)과 같은 보다 상세한 기하학을 재구성하는 세 가지 테스트 케이스도 제시한다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '제안된 방법은 입력 뷰가 3D 도형의 충분한 정보를 포함한다는 가정에 의존한다. 그러나, 3D 형상의 많은 부분이 모든 입력 뷰들에서 보이지 않으면, 보이지 않는 부분에 대한 추론된 3D 형상이 악화될 것이다. 도. 도 6-a는 SyncDreamer에 의해 생성된 뷰들이 찻주전자 하단을 포함하지 않는 예를 도시한다. 이 경우, MVD\\({}^{2}\\)는 바닥 영역에 대한 원뿔 모양의 기하학을 생성하는데, 이는 인간의 예상과 일치하지 않는다.\n' +
      '\n' +
      '또한, 제안된 방법은 생성된 영상의 불일치에 강인하지만, 생성된 뷰가 매우 불일치할 경우 실패할 수 있다. 도. 도 6-b는 불일치하는 입력 MVD 이미지들과 큰 시각적 불일치를 갖는 재구성된 메시를 도시한다.\n' +
      '\n' +
      '또한, GPU 메모리 예산은 메쉬 생성에서 더 높은 그리드 해상도(\\(>80^{3}\\))를 사용하는 것을 제한한다. 따라서 우리의 접근법은 그림 1과 같이 얇은 기하학적 구조를 쉽게 재구성할 수 없다. 6-c.\n' +
      '\n' +
      '## 5. Conclusion\n' +
      '\n' +
      '본 논문에서는 다시점 깊이 영상에서 3차원 복원 품질을 향상시키는 새로운 다시점 3차원 복원 방법을 제안한다. 제안하는 방법은 MVD 영상과 3차원 학습 데이터 간의 불일치를 완화하기 위해 세심하게 설계된 학습 기법을 활용한다. 제안된 MVD 모델({}^{2}\\)은 Zero-123++의 MVD 영상에 미리 훈련된 모델로서 기존의 재구성 파이프라인 및 다른 3D 생성 방법을 능가한다.\n' +
      '\n' +
      '그림 5. MVD\\({}^{2}\\)의 다른 변형으로 재구성된 세 가지 예제의 시각화. 왼쪽은 입력 MVD 이미지입니다.\n' +
      '\n' +
      '그림 6. 불완전 및 고장 재구성 결과의 그림. GT는 참조용 3D 객체이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c|c c c c c c c c} \\hline \\hline  & **Training images** & **\\#v** & **VDL** & **CD\\(\\downarrow\\)** & **EMD\\(\\downarrow\\)** & **PSNR\\({}_{d}\\)\\(\\uparrow\\)** & **SSIM\\({}_{d}\\)\\(\\uparrow\\)** & **LPIPS\\({}_{d}\\)\\(\\downarrow\\)** & **PSNR\\({}_{n}\\)\\(\\uparrow\\)** & **SSIM\\({}_{n}\\)\\(\\uparrow\\)** & **LPIPS\\({}_{n}\\)\\(\\downarrow\\)** \\\\ \\hline\n' +
      '**A1** & GT & 64 & ✗ & 4.371 & 28.56 & 17.22 & 81.10 & 25.59 & 13.34 & 72.04 & 32.28\\\\\n' +
      '**A2** & GT(VP) & 64 & ✗ & 2.172 & 19.95 & 20.83 & 86.76 & 15.32 & 16.29 & 77.47 & 21.55 \\\\\\\n' +
      '**A3** & Generated & 64 & ✗ & 1.256 & 13.80 & 23.02 & 89.10 & 11.74 & 18.09 & 79.88 & 17.60 \\\\\\\n' +
      '**A4** & Generated & 7 & ✗ & 1.214 & 14.62 & 23.03 & 89.09 & 11.63 & 18.07 & 79.77 & 17.47\\\\\n' +
      '**A5** & Generated & 7 & ✓ & **1.044** & **13.58** & **23.31** & **89.20** & **10.80** & **18.28** & **80.01** & **16.34** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3. 훈련 계획의 절제 연구. **VP**는 뷰포트 교란의 약자입니다. **#v**는 손실 감독에서 사용되는 지상-진실 뷰의 수를 나타내고, **VDL**는 뷰 의존적 손실의 사용을 의미한다. A5는 저희의 최종 모델입니다.\n' +
      '\n' +
      '우리는 다른 MVD 모델의 출력에 대한 모델을 미세 조정하면 3D 재구성 품질이 더욱 향상될 것이라고 추측한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)\n' +
      '* Zero123(2013) Stable Zero123. [https://huggingface.co/stabilityai/stable-zero123](https://huggingface.co/stabilityai/stable-zero123)\n' +
      '* Chen et al. (2023) Rui Chen, Yongwei Chen, Ningxing Jiao, and Kuii Jia. 2023a. Fantasia3D: 고품질 텍스트-30 콘텐츠 생성을 위해 기하학과 모양을 분리합니다. _ICCV_에서.\n' +
      '* Chen et al. (2023) Yang Chen, Yingwei Pan, Yehao Li, Ting Yao, and Tao Mei. 2023b. Control3D: 제어 가능한 텍스트-대-3D 생성을 향함. In _ACM Multimedia_. 1148-1156.\n' +
      '* 첸과 장(2019) 쯔치 첸과 하오장. 2019. 생성적 형상 모델링을 위한 암시적 필드 학습. _CVPR_에서. 5939-5948.\n' +
      '* Cheng et al. (2023) Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander G Schwwing, and Liang Yan Gui. 2023. SPfusion: Multimodal 3D shape completion, reconstruction, generation. _CVPR_에서. 4456-4465\n' +
      '* Li et al. (2022) Thiago L. T. da Silva, Patrico G. L. Pinto, Jeffi Murrugarra-Llerena, and Claudio R. 정 2022. \\(360^{\\circ}\\) 이미지로부터의 3D 장면 기하학 추정: 설문조사. _ ACM 컴퓨터. Surv._ 55조 4항 68조 2022항 39쪽\n' +
      '* Deike et al. (2021) Matt Deike, Ruoshi Liu, Matthew Wallinford, Hung Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voelnick, Samir Yitohak Garde, et al. 2021a. 우주-XL: 10M + 3D 물체의 우주. _NeurIPS_에서.\n' +
      '* Deike et al. (2023) Matt Deike, Dustin Schwenk, Joris Salvador, Luca Wehus, Oscar Michel, Eli Vanderghul, Ludwig Schmidt, Kiam Hasan, Anirudhchalkom, and Ali Farhadi. 2023b. Oboiverse: 주석이 달린 3D 물체들의 우주. _CVPR_에서. 1134-13153.\n' +
      '* Deng et al. (2022) Congye Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Thu Zhou, Leonidas Guibas, Dragram Angiovich, et al. 2022. NextFile-view text synthesis with language-guided diffusion as general image priors. _CVPR_에서. 20637-20647.\n' +
      '* Downs et al. (2022) Laura Downs, Anthony Francis, Natee Roenig, Brandon Kunnen, Ryan Hickman, Krista Reynmann, Thomas B McHugh, and Vincent Vanhoucke. 2022. 구글 스캐닝 객체: 3D 스캐닝된 가정용 아이템의 고품질 데이터세트. _ICRA_에서. IEEE. 2553-2560.\n' +
      '* Gao et al. (2022) Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangcue Yin, Daiqing Li, Or Litany, Zan Golei, and Sanja Fidler. 2022. GeStD: 이미지로부터 학습된 고품질 3D 텍스처 형상의 생성 모델. _ NeurIPS_35(2022), 31841-31854.\n' +
      '* Gupta et al. (2023) Anh Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Oguz. 2023. 3DGen: 텍스쳐링된 메시 생성을 위한 트라이플레인 잠재 확산. 2303.05371\n' +
      '* He and Wang (2023) Zeein He and Tengfe Wang. 2023. OpenIR: OM-open-source 대형 재구성 모델. [https://github.com/3DTopia/OpenIR.RM] (https://github.com/3DTopia/OpenIR.RM).\n' +
      '* Hong et al. (2024) Yicong Hong, Kai Zhang, Jinxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sankai, Yifung Bui, and Hao Tan. 2024. LKM: 단일 이미지에 대한 대형 재구성 모델 내지 3D. _ICLR_에서.\n' +
      '* Huang et al. (2023) Zixuan Huang, Stefan Stojanov, Anh Thai, Varun Jampani, and James M Rehg. 2023. ZeroSHox: 회귀 기반 제로 샷 형상 재구성. 2312.14198\n' +
      '* Ding et al. (2023) Moritz Ding, Gregor Kohsis, and Lei Korbel. 2023. Octree Transformer: 계층적으로 구조화된 시퀀스에서의 자기회귀 3D 형상 생성. _CVPR_에서. 2697-2706.\n' +
      '* Jain et al. (2022) Ajay Jain, Ben Mildenhall, Jonathan T. 샤론, 피터 애빌, 벤 풀 2022. Zero-shot text-guided object generation with dream fields. _CVPR_에서. 867-876\n' +
      '* Houo and Nichol(2023) Hewoo Jun and Alex Nichol. 2023. Shap-E: 조건부 3D 암시적 함수 생성. 2305.02463\n' +
      '* Kerbl et al. (2023) Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettalsik. 2023. 실시간 레이디언스 필드 렌더링을 위한 3차원 가우시안 분할_ ACM Trans. Graph.__ 제42조 제4항 제139조 제2023호 14쪽\n' +
      '* Laine et al. (2020) Samuli Laine, Jame Hellsten, Tero Karras, Youngho Seel, Jaakko Lehtinen, and Timo Aila. 2020. Modular Primitives for high performance differentiable rendering. _ ACM Trans. Graph.__ 39, 6(2020), 1-14.\n' +
      '* Li et al.(2023) Muheng Li, Yueqi Duan, Jie Zhou, 및 Jiwen Lu. 2023. 확산-SDF: 복셀화된 확산을 통한 텍스트-투-형상. _CVPR_에서. 12642-12651.\n' +
      '* Li 등(2024) Weiyi Li, Bin Chen, Xuehui Cheng, Ping Tan. 2024. SweetDreamer: 일관된 Text-to-3D를 위해 기하학적 사전들을 2D 확산에서 정렬하는 것. _ICLR_에서.\n' +
      '* Liu et al. (2023) Minghua Liu, Ruosi Shi, Linghao Chen, Zhuowyang Zhang, Chao Xu, Xinyeong Wei, Hansheng Chen, Chong Zeng, Jiyuan Gu, and Hao Sun. 2023. 1 -2-3-45++: 일관된 다시점 생성 및 3D 확산과 함께 3D 객체로의 빠른 단일 이미지. 2311.07885\n' +
      '* Liu et al. (2023) Minghua Liu, Chao Xu, Haina Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. 2023. One -2-3-45: 임의의 단일 이미지 내지 3D 메시를 45초 내에, 형상당 최적화 없이. _NeurIPS_에서.\n' +
      '* Liu et al. (2022) Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2022. Zero-1-to-3: Zero-shot one image to 3D object. _ICCV_에서.\n' +
      '* Liu et al. (2024) Yuan Liu, Cheng Lin, Zijang Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. 2024. SyncDreamer: 단일 시점 영상으로부터 다시점-일관성 영상을 생성하는 단계. _ICLR_에서.\n' +
      '* Liu et al. (2023) Yuxin Liu, Minshan Xie, Huanyuan Liu, and Tien-Tian Wong. 2023d. 동기화된 다중 뷰 확산에 의한 텍스트 유도 텍스트 이미지. 2311.12891\n' +
      '* Liu et al. (2023) Zhenghee Liu, Peng Dai, Ruihui Li, Xiaojun Qi, and Chi-Wing Fu. 2023a. ISS: 텍스트 유도 3D 형상 생성을 위한 테스트 스톤으로서의 이미지. _ICLR_에서.\n' +
      '* Long et al. (2023) Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, and Wenping Wang. 2023. Wonder3D: Cross-Domain Diffusion을 이용한 Single Image to 3D. 2310.15008\n' +
      '* Long et al. (2022) Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and Wenping Wang. 2022. SparseNet: 희소 뷰로부터 빠르게 일반화할 수 있는 신경 표면 재구성. _ECCV_에서. 스프링거 210-227\n' +
      '* Tu et al. (2023) Yuanunun, Lingyang Zhang, Shiwei Li, Tian Fang, David McKinnon, Yanghai Tian, Long Quan, Xun Cao, and Yao Yao. 2023. Direct2.5: 멀티뷰 2.5D 확산을 통한 다양한 텍스트-대-3D 생성. 2311.15980\n' +
      '* Melas-Kyriani et al. (2023) Luke Melas-Kyriani, Heo Liang, Christian Rupprecht, and Andrea Vedaldi. 2023. Realfusion: 360\\({}^{\\circ}\\) reconstruction from any object from single image. _CVPR_에서. 8446-8455\n' +
      '* Mildenthal et al. (2020) Ben Mildenthal, Pratul P Srinivasan, Matthew Tarski, Jonathan T Barron, and Ravi Ramamoorthi. 2020. NeoRF: 장면들을 뷰 합성을 위한 신경 래디언스 필드들로서 표현하는 단계. _ECCV_에서.\n' +
      '* Mittal et al. (2022) Paritul Mittal, Chen-Chi Cheng, Maneeth Singh, and Shubham Tulsiani. 2022. AutoSDF: 3D 완료, 재구성 및 생성을 위한 형상 사전들. _CVPR_에서.\n' +
      '* Nash et al. (2020) Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battenga. 2020. PolyGen: 3D 메쉬의 자기회귀 생성 모델. _ICML_에서. PMLR, 7220-7229.\n' +
      '* Ogun et al. (2023) Maxime Ogun, Timothee Darret, Theo Moutakamani, Hyy V. Vo, Marc Szafraniec, Vasil Khalilov, Pierre Fernandez, Daniel Hazira, Francisco Massa, Alaaudeli-Ei Noubly, Russell Horos, P-Yao Huang, Hix Yu, Aussi Sharma, Sheng-Wen Li, Wojciech Gahu, Mike Rabbat, Mido Asaran, Nicolas Ballag, Gabriel Synnase, Isman Misra, Herve Jegou, Julien Mairal, Patrick Labatat, Armand Joulin, Piotr Bojanovski. 2023. DING/2: 감독 없이 강건한 시각적 특징을 학습하는 것. 2304.07193\n' +
      '* Ouyang et al. (2023) Yichen Ouyang, Wenhao Cai, Jiayi Ye, Daepeng Tao, Yibing Zhou, and Gaoang Wang. 2023. 단일 이미지로부터 텍스트-투-3D 생성에서의 일관성을 추적한다. 2309.03599\n' +
      '* Ouyuyu et al. (2017) Onuyu Ouyuyu, Yisdalslav Voroninski, Ronen Basri, and Amit Singer. 2017. Survey of structure from motion. _ Acta Numerica_26(2017), 305-364.\n' +
      '* Poole et al. (2023) Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. 2023. Dreamfusion: Text-to-3D using 2D diffusion. _ICLR_에서.\n' +
      '* Puruswahskalam and Naik (2023) Senth Puruswahskalam and Nikik Naik. 2023. ConRad: Image constrained radiance field for 3D generation from single image in. _NeurIPS_에서.\n' +
      '* Qian et al. (2024) Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandar Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skiorokhodov, Peter Wonka, Sergey Tulyakov, et al. 2024. Maggi-123: One image to high quality 3D object generation using both 2D and 3D diffusion priors. _ICLR_에서.\n' +
      '* Ronn et al. (2022) Robin Ronn, Andreas Bittmann, Dominic Lorenz, Patrick Esser, and Bjorn Ommer. 2022. 잠재 확산 모델을 이용한 고해상도 영상 합성. _CVPR_에서.\n' +
      '* Seitz et al. (2006) Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, and Richard Szeliski. 2006. A comparison and evaluation of multi-view stereo reconstruction algorithms. _CVPR_에서. IEEE. 519-528\n' +
      '* Shen et al.(2021) Tianchang Shen, Jun Gao, Kangcue Yin, Ming-Yu Liu, and Sanja Fidler. 2021. Deep marching* Wang et al. (2023) Zhengyi Wang, Cheng Lu, Yixai Wang, Fan Bao, Chongcuan Li, Hang Su, and Jun Zhu. 2023b. ProfileDreamer: Variational score distillation를 이용한 높은 충실도와 다양한 text-to-3D 생성. _NeurIPS_에서.\n' +
      '* Wang et al. (2023) Hahan Weng, Tianyu Yang, Jiannan Wang, Yu Li, Tong Zhang, Cl Chen, and Lei Zhang. 2023. Consistent123: 하나의 이미지에 대한 정합성을 3D 객체 합성으로 개선한다. 2310.08092\n' +
      '* Wu et al. (2023) Sangmin Wu, Byenogin Park, Hyojin Go, Jin-Young Kim, and Changjick Kim. 2023. harmonyView: One-image-to-3D에서 일관성과 다양성을 조화시킨다. 2312.15980\n' +
      '* Wu et al. (2023) Chao-Yuan Wu, Justin Johnson, Jhendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. 2023. 3D 복원을 위한 다시점 압축 부호화. _CVPR_에서.\n' +
      '* Yan et al. (2021) Xiaojiang Yan, Shizhe Hu, Yiqiao Mao, Yangdong Ye, and Hui Yu. 2021. 심층 다시점 학습 방법: 리뷰. _ Neurocomputing_ 448 (2021), 106-129.\n' +
      '* Yang et al. (2018) Jiayu Yang, Xiang Cheng, Yunlei Duan, Pan Ji, and Hongdong Li. 2018. ConsistNet: Multi-view 영상 확산에 대한 3D 일관성 탐색. 2310.1043\n' +
      '* Ye et al.(2023) Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng Wang. 2023. Consistent:1-to-3: 기하학 인식 확산 모델을 통한 3D 뷰 합성 일치 이미지. _3DV_에서.\n' +
      '* Yu et al. (2021) Alex Yu, Vickie Ye, Matthew Tanck, and Angjoo Kanazawa. 2021. 픽셀NeRF: 하나의 이미지로부터의 신경 복사 필드. _CVPR_에서.\n' +
      '* Yu et al. (2023) Wangbo Yu, Li Yuan, Yan-Pei Cao, Xiangjun Gao, Xiaoyu Li, Long Quan, Ying Shan, and Yonghong Tian. 2023. HFI-12: 3D 콘텐츠 생성에 대한 높은 충실도의 하나의 이미지를 향한다. 2310.06744\n' +
      '* Zeng et al. (2023) Bohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng Gao, Jiaming Liu, Huaxia Li, Xu Tang, Jianxhang Liu, 및 Baochang Zhang. 2023. lPDreamer: 이미지 프롬프트로 외관 제어 가능한 3D 객체 생성. 2310.053575\n' +
      '* Zhang et al. (2022) Biao Zhang, Matthias Niessner, and Peter Wonka. 2022. 3DILG: 3D 생성 모델링을 위한 불규칙 잠재 그리드. _NeurIPS_에서.\n' +
      '* Zhang et al. (2023) Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 2023. 3DShape2VecSet: 신경계 및 생성 확산 모델에 대한 3D 형상 표현 _ ACM Trans. Graph.__ 42조, A 제92조(2023), 16쪽.\n' +
      '* Zhang et al. (2018) Richard Zhang, Phillib Ioka, Alexand A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. _CVPR_에서.\n' +
      '* Zheng et al. (2022) Xingwang Zheng, Yang Liu, Pengshuai Wang, and Xin Tong. 2022. SDF-StyleGAN: 3D 형상 생성을 위한 implicit SDF-Based StyleGAN; _ 컴퓨터 그래프. Forum_41(2022), 52-63.\n' +
      '* Zheng et al. (2023) Xingwang Zheng, Hao Pan, Pengshuai Wang, Xin Tong, Yang Liu, and Heungyeung Shum. 2023. 제어 가능한 3D 형상 생성을 위한 국부적 감쇠 SDF 확산 _ ACM Trans. Graph.__ 42조, 갑 제91조(2023), 13쪽.\n' +
      '* Yu et al. (2023) Zi-Xin Zhou, Zhipeng Yu, Yuan-Cheng Guo, Yangang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. 2023. Triplane meets Gaussian splitting: Fast and generalizable single-view 3D reconstruction with transformer. 2312.09147\n' +
      '\n' +
      '그림 8: 인터넷 이미지 상에서 조절된 3D 생성의 시각적 비교. 이미지 배경은 다른 방법으로 공급하기 전에 제거됩니다.\n' +
      '\n' +
      '그림 7: GSO 데이터 세트에서 단일 뷰 3D 생성에서 차등 접근법의 시각적 비교. 각 결과에 대해 시각화를 위해 두 개의 다른 뷰를 렌더링합니다. **s**는 Zero-123++와 함께 \\(\\text{MVD}^{2}\\)을 사용하는 것을 말한다. 가장 오른쪽 두 개의 이미지는 참조된 3D 객체의 렌더링된 뷰이다.\n' +
      '\n' +
      '그림 9: MVD2의 일반화 가능성 테스트. 왼쪽에서 오른쪽으로: 프롬프트 이미지 또는 텍스트, MVD 이미지, 4개의 다른 각도에서 렌더링된 재구성된 모양. 처음 세 예, 다음 네 예, 마지막 네 예의 MVD 영상은 SyncDreamer, Wonder3D, MVDreamer를 이용하여 각각 제작된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>