<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 블랙맘바: State-Space 모델을 위한 전문가 혼합\n' +
      '\n' +
      'Quentin Anthony\\({}^{*}\\)\n' +
      '\n' +
      'Yury Tokpanov\\({}^{*}\\)\n' +
      '\n' +
      'Paolo Glorioso\\({}^{*}\\)\n' +
      '\n' +
      'Beren Millidge\\({}^{*}\\)\n' +
      '\n' +
      '{quentin, yury, paolo, beren}@zyphra.com\n' +
      '\n' +
      'Zyphra\n' +
      '\n' +
      '팔로알토 CA\n' +
      '\n' +
      '모든 저자는 이 작업에 동등하게 기여했다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'State-space 모델(SSM)은 최근 대규모 언어 모델링 벤치마크에서 변압기에 대한 경쟁 성능을 보여주면서 시퀀스 길이의 함수로 선형 시간 및 메모리 복잡도를 달성했다. 최근 출시된 SSM 모델인 맘바는 언어 모델링과 긴 시퀀스 처리 작업 모두에서 인상적인 성능을 보여줍니다. 동시에, MoE(Mixture-of-expert) 모델들은 더 큰 메모리 풋프린트를 희생시키면서 추론의 계산 및 지연 비용을 상당히 감소시키면서 현저한 성능을 보여주었다. 이 논문에서 우리는 두 가지 이점을 얻기 위해 Mamba SSM과 MoE를 결합한 새로운 아키텍처인 BlackMamba를 제시한다. 우리는 BlackMamba가 Mamba 및 변압기 기준선 모두에 대해 경쟁적으로 수행되며 추론 및 훈련 FLOP에서 능가함을 보여준다. 우리는 맞춤형 데이터 세트의 300B 토큰에서 340M/1.5B 및 630M/2.8B 블랙맘바 모델을 완전히 훈련하고 오픈 소스한다. 우리는 블랙맘바가 SSM의 선형 복잡성 생성과 MoE의 저렴하고 빠른 추론을 결합하여 SSM과 MoE 아키텍처의 이점을 모두 계승하고 결합한다는 것을 보여준다. 우리는 모든 가중치, 체크포인트 및 추론 코드 오픈 소스를 출시합니다. 1\n' +
      '\n' +
      '각주 1: 추론 코드 at: [https://github.com/Zyphra/BlackMamba](https://github.com/Zyphra/BlackMamba)\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      '디코더 전용 트랜스포머 모델[1, 2]로 구축된 대규모 언어 모델(LLM)의 출현은 이미지 처리[6], 시계열 [7], 강화 학습[8]과 같은 다양한 딥러닝 응용 도메인과 함께 자연 언어 처리[3, 4, 5]에 혁명을 일으켰다. 그러나, 변압기 아키텍처의 강력한 성능과 확장성에도 불구하고, 상당한 단점들이 남아 있다. 최대 표현적이지만 주의 메커니즘은 훈련과 추론 동안 계산적으로 요구되며, 순전히 2차 FLOP와 시퀀스 길이의 메모리를 모두 필요로 한다. 이는 변압기 모델의 컨텍스트 길이를 제한하고, 규모에 따라 자동 회귀 생성을 점점 더 비싸게 하며, 일반적으로 연속 데이터스트림으로부터 진정으로 무제한적인 시퀀스 처리 및 학습을 억제한다.\n' +
      '\n' +
      '이러한 문제점을 개선하기 위해 최근에는 표준 밀집형 주의 변압기 모델에 대한 건축적 대안으로 상당한 노력을 기울이고 있다. 가장 유망한 후보 아키텍처 중 일부는 상태 공간 모델(SSMs) [9, 10] 및 혼합 전문가(MoE) [11, 12, 13]이다. 변압기에 대한 SSM의 주요 실용적 이점은 입력 시퀀스 길이에 대한 선형 계산 복잡도(변압기의 2차 복잡도와 대조적으로)이다. 이것은 이론적으로 SSM이 주어진 FLOP 예산에 대해 변압기보다 훨씬 더 긴 시퀀스를 처리하고 KV 캐시 없이 계산에서 자기회귀 생성 상수를 렌더링할 수 있게 한다. SM의 주목할만한 최근 예는 Mamba[9], RWKV[10], RetNet[14]를 포함하며, 이들 모두는 효율적인 긴 시퀀스 트레이닝 및 추론, CUDA에서의 효율적인 구현, 및 유사한 스케일링 속성을 갖는 변압기에 대한 경쟁 언어 모델링 태스크 성능을 입증한다. 동시에 전문가(MoE) 아키텍처[15, 16, 11, 12]의 혼합물은 유사한 조밀한 모델과 유사한 품질을 달성하는 데 필요한 훈련 및 추론 FLOP를 상당히 감소시킬 수 있는 조밀한 변압기에 비해 새로운 진전이 되었다. MoE 모델들은 \'전문가들\'이 활용되거나 문맥에 의존하지 않는 게이트로의 라우팅 함수에 의존하여, 단일 순방향 패스 상에서 전체 파라미터들의 희소 부분집합만이 활성화될 수 있게 한다. 이 희소성은 모델의 추론 비용과 매개변수 수를 분리하여 더 많은 매개변수와 그에 상응하는 더 큰 메모리 풋프린트의 비용으로 주어진 추론 예산에 대해 훨씬 더 강력한 성능을 가능하게 한다.\n' +
      '\n' +
      '변압기에 비해 이러한 구조적 개선은 그 자체로 강렬하지만, 우리는 그들의 조합이 표준 변압기에 비해 상당히 향상된 언어 모델링 속도와 성능을 가능하게 할 수 있는 자연스러운 다음 단계라고 믿는다. 특히, 우리는 Mamba-MoE 아키텍처가 밀집 변압기에 비해 다음과 같은 개선을 가질 것으로 기대한다:\n' +
      '\n' +
      '* _Mamba_: 훈련 및 추론 모두를 위한 입력 시퀀스 길이에 대한 선형 계산 복잡도. 지속적인 시간과 기억의 자기회귀적 생성\n' +
      '* _MoE_: 등-파라미터 밀집 모델에 가까운 모델 품질을 유지하면서 동등한 더 작은 밀집 기반 모델의 추론 지연 및 훈련 FLOP.\n' +
      '\n' +
      '이 논문에서 우리는 이러한 개선이 달성 가능하고 이를 종합하면 이 두 가지 접근법이 시너지 효과를 발휘하여 강력한 평가 성능을 가진 모델을 생성한다는 것을 입증하기 시작한다(도 8-14). , 계산(도 4) , 레이턴시 이점(도 5 및 도 3) 기존 변압기 모델에 걸쳐 유사한 성능을 위해 FLOP 비용의 일부로 훈련될 수 있다(그림 4). 우리는 훈련 시간과 모델 깊이 전반에 걸쳐 모델에 의해 나타나는 MoE 라우팅 통계를 연구한다. 또한, 수렴까지의 반복 횟수를 크게 줄여 라우팅 속도를 향상시킬 수 있는 새로운 라우팅 싱크혼 알고리즘을 제안한다.\n' +
      '\n' +
      '## II Contributions\n' +
      '\n' +
      '이 작품의 주요 성과는 다음과 같다.\n' +
      '\n' +
      '* 우리는 **BlackMamba**: 무주의 Mamba 블록과 라우팅된 MLP의 교대 조합을 설계, 구현 및 평가한다.\n' +
      '* 우리는 340M/1.5B BlackMamba와 630M/2.8B BlackMamba2의 두 가지 BlackMamba 모델을 훈련하고 오픈 소스한다. 각주 2: 본 논문에서는 \\(X\\) 순방향 통과 파라미터와 \\(Y\\) 총 파라미터를 \\(X/Y\\)으로 하는 MoE 모델을 나타낸다.\n' +
      '* 우리는 블랙맘바가 고밀도 트랜스포머 모델과 유사한 다운스트림 작업 성능을 달성하기 위해 훨씬 적은 훈련 FLOP를 필요로 한다는 것을 입증한다.\n' +
      '* MoE와 같은 라우팅된 희소성 아키텍처와 함께 맘바와 같은 무주의 아키텍처의 조합의 복합 추론 이점을 탐구한다.\n' +
      '\n' +
      '이 논문의 나머지 부분은 다음과 같이 정리된다. 먼저 섹션 IV에서 MoE와 함께 SSM, MoE 및 SSM에 대한 관련 작업에 대한 개요를 제공한다. 그런 다음 섹션 III의 기여를 이해하는 데 필요한 SSM 및 MoE의 기본 개념에 대한 배경을 제공한다. 우리의 아키텍처는 섹션 V에서 설명되고, 그것의 훈련/추론 역학은 섹션 VI에서 탐구된다. 마지막으로 섹션 VII에서 접근법의 의미와 한계를 섹션 VIII에서 이 작업의 결론과 함께 설명한다.\n' +
      '\n' +
      '최종 체크포인트는 아파치 2.0 라이센스가 있는 허깅페이스에서 오픈소싱되며, 중간 트레이닝 체크포인트는 요청 시 이용할 수 있다. 추론 코드는 [https://github.com/Zyphra/BlackMamba](https://github.com/Zyphra/BlackMamba)에서 제공된다.\n' +
      '\n' +
      '## III Background\n' +
      '\n' +
      '### _Transformers_\n' +
      '\n' +
      '트랜스포머 아키텍처 [2]는 언어 모델링에서 예외적으로 강력하고 일관된 성능을 보여주었으며, 거의 모든 다른 시퀀스 처리 작업, 최신 상태로 유지되며 도입 이후 본질적으로 변경되지 않았다. 트랜스포머의 핵심 연산은 자기 주의(self-attention)로, 서로 다른 토큰들의 임베딩들 사이의 도트-곱 유사도들을 정규화하고 출력 벡터에 선형 맵을 수행하기 전에 시퀀스로 2차 올-올 비교를 수행한다. 수학적으로 자기 주의는 다음과 같이 쓰여질 수 있다.\n' +
      '\n' +
      '\\[z=W_{V}x\\sigma(\\frac{1}{\\sqrt{d}}xW_{Q}W_{K}^{T}x\\circM}\\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(\\sigma\\)은 softmax 함수를 나타내고, \\(M\\)은 인과 마스킹과 같은 특정 제약조건을 계산에 강제하는 이진 마스크를 나타내며, \\(T\\)은 전치, \\(\\circ\\)은 요소별 곱셈을 나타낸다. 시퀀스 길이의 2차 비용은 서로 다른 토큰의 임베딩 사이의 유사도 점수의 \\(L\\times L\\) 행렬을 계산하는 \\(xW_{Q}W_{K}^{T}x\\) 항으로 인해 발생한다.\n' +
      '\n' +
      '변압기 모델은 주어진 활성화 함수를 갖는 2-층 MLP로 구성된 다중-층-퍼셉트론(multi-layer-perceptron, MLP) 블록들로 인터리빙된 자기-관심 블록들의 스택으로 구성된다. 따라서 변압기 모델의 레이어는 다음과 같이 기록될 수 있다.\n' +
      '\n' +
      '\\[x_{l+1}=x_{l}+\\text{MLP}(\\text{LN}(x_{l}+\\text{attention}(\\text{LN}(x_{l})))) \\tag{2}\\]\n' +
      '\n' +
      '여기서 LN은 어텐션 및 MLP 블록에 대한 입력을 정규화하는 데 사용되는 레이어노어 연산을 나타낸다.\n' +
      '\n' +
      '도. 1 : 고밀도 변압기, 고밀도 맘바, 변압기-MoE, 및 Mamba-MoE의 구조\n' +
      '\n' +
      '### _Mamba_\n' +
      '\n' +
      '상태 공간 모델(State-space model, SSMs)은 시퀀스 길이에 대해 선형 복잡성을 갖는 시퀀스 모델들의 클래스이다. SSM은 주의 메커니즘보다 RNN 및 CNN 구조와 더 밀접하게 관련되어 있으며, 내재적 잠재 상태\\(h(t)\\in\\mathbb{R}^{N}\\)을 통해 1차원 함수 또는 시퀀스\\(x(t)\\in\\mathbb{R}\\mapsto y(t)\\in\\mathbb{R}\\)를 매핑하는 연속 동적 시스템(수학식 3에 묘사됨)으로부터 영감을 끌어낸다:\n' +
      '\n' +
      '\\[h^{\\prime}(t)=Ah(t)+Bx(t),\\\\\\y(t)=Ch(t)\\tag{3}\\]\n' +
      '\n' +
      '여기서 \'time\'\\(t\\)은 이제 토큰의 시퀀스 위치를 나타낸다. 이와 같은 선형 동역학 시스템은 컨볼루션 또는 연관 스캔을 통해 병렬로 효율적으로 계산될 수 있는 반면, 위에서 제시된 순환 형태는 추론 시간에서 신속한 생성에 활용될 수 있다. Mamba 아키텍처의 근본적인 혁신은 SSM의 \\(A\\), \\(B\\), \\(C\\) 행렬을 선형적으로 입력 의존적으로 만드는 것이다. 즉, 새로운 다이내믹스는 다음과 같이 기입될 수 있다.\n' +
      '\n' +
      '\\[h^{\\prime}(t)=A(x(t))h(t)+B(x(t))x(t),\\\\\\y(t)=C(x(t))h(t)\\tag{4}\\]\n' +
      '\n' +
      '직관적으로, 이것은 SSM의 리커런트 상태에 대한 업데이트들이 프로세싱되는 토큰들에 선택적으로 의존할 수 있게 하고, SSM은 그의 리커런트 상태로부터 특정 정보를 동적으로 저장하거나 제거하기로 결정할 수 있게 한다. 이것은 주의에서 \\(A\\),\\(B\\),\\(C\\) 행렬을 \\(Q\\),\\(K\\),\\(V\\) 행렬과 느슨하게 유사하게 만들고 SSM 블록의 표현력을 크게 증가시키며, 고정된 가중치를 갖는 선형 동역학 시스템에서 기하급수적으로 붕괴되어야 하기 때문에 잠재적으로 컨텍스트가 숨겨진 상태에서 훨씬 더 오래 지속되도록 할 수 있다. 경험적으로, [17]은 이것이 변압기와의 간극의 많은 부분을 닫는 것을 발견했다.\n' +
      '\n' +
      '실용적인 측면에서, SSM의 반복적인 특성은 GPU와 같은 군림하는 고도로 병렬적인 AI 하드웨어에서 채택되는 것을 오랫동안 방지했다. 그러나 최근 Mamba[9] 및 RWKV[10]과 같은 순환 및 상태 공간 모델의 구현은 병렬 스캔 커널을 통해 GPU 하드웨어에 이러한 작업을 효율적으로 매핑하여 잘 최적화된 트랜스포머 모델에 접근하는 효율성을 가진 새로운 아키텍처의 학습을 가능하게 한다.\n' +
      '\n' +
      '맘바에 대한 자세한 내용은 [9] 및 관련 코드베이스뿐만 아니라 맘바 블록의 내부 계산을 자세히 설명하는 부록 C를 참조하십시오.\n' +
      '\n' +
      '### _ Experts_\n' +
      '\n' +
      '전문가(MoE) 모델의 혼합물은 순방향 패스의 모든 파라미터를 활성화하지 않고 대신 특정 MLP _experts_로 토큰을 라우팅함으로써 모델의 추론 비용 및 파라미터 수를 디커플링할 수 있게 한다. 각 전문가는 이론적으로 특정 종류의 입력을 전문화하고, 라우터(작은 신경망)는 각 토큰을 어떤 전문가에게 라우팅할지를 학습한다. 이론적으로, 이는 모델이 훨씬 적은 FLOP에서 매개변수 등가 조밀 모델의 거의 모든 표현성을 유지할 수 있게 한다.\n' +
      '\n' +
      '본 논문에서 따르는 표준 구현 [11]에서 라우터는 토큰에서 전문가 인덱스로의 선형 계층 매핑이며 각 전문가는 단순히 표준 트랜스포머 MLP이다. 토큰이 라우팅되는 전문가를 전문가 확률의 상위 k로 선택하며, 여기서 \\(k\\)은 아키텍처의 하이퍼파라미터이다. MoE 레이어에 대한 입력 토큰이 주어지면 라우터를 통해 확률 분포 \\(p_{i}(x)\\)로 매핑되며, 여기서 \\(i\\)은 전문가에게 레이블을 지정한다. Top-\\(k\\) 확률을 선택할 때, MoE 층 \\(y\\)의 출력은 개략적으로, 다음과 같이 표현될 수 있다.\n' +
      '\n' +
      '\\[y=\\sum_{i\\in\\text{top-}k}c_{i}E_{i}(x) \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(E_{1},E_{2},\\dots\\)는 MLP 전문가를 나타내고,\n' +
      '\n' +
      '\\[E_{i}(x)=W_{\\text{out}}f(W_{\\text{in}}(\\text{LN}(x)) \\tag{6}\\]\n' +
      '\n' +
      '여기서 \\(f\\)은 MLP의 활성화 함수이고, \\(c_{i}\\)은 특정 전문가를 선택하는 라우터가 출력하는 확률인 \\(p_{i}\\)으로 종종 식별되는 계수이다. 라우터를 훈련하기 위한 최적의 방법은 "올바른" 전문가 할당 문제가 미분 불가능하기 때문에 여전히 불확실하며, MoE 모델은 종종 하드웨어 효율성을 위해 서로 다른 전문가 간의 훈련 안정성 및 부하 균형과 함께 어려움을 겪는다. 그럼에도 불구하고, MoE 모델은 고밀도 변압기 모델보다 주어진 계산 예산에 대해 우수한 성능을 달성하는 능력을 입증했다. 마지막으로, MoE 모델의 보고의 복잡성으로 인해, 다른 논문에서 MoE의 순방향 통과 크기, 총 파라미터 또는 둘 다 보고되었기 때문에, 우리는 MoE 모델을 다음과 같이 표현하는 일관된 규칙을 제시한다. MoE 아키텍처 및 일반적인 구현에 대한 자세한 내용은 [16]을 참조하십시오.\n' +
      '\n' +
      '## IV 관련 업무\n' +
      '\n' +
      '### _State-space Models_\n' +
      '\n' +
      '시퀀스 길이에서 변압기의 2차 복잡성은 매우 긴 컨텍스트 추론 및 이해에 대한 일차 병목 현상으로 오랫동안 인식되어 왔다. 최근 연구는 트랜스포머가 관리 가능한 규모로 훈련된 다음 상당히 긴 컨텍스트에서 성공적으로 참조될 수 있도록 컨텍스트 길이 확장[18, 19]의 개념을 개척했지만 FLOP 및 KV 캐시에 필요한 메모리 측면에서 추론 비용은 여전히 상당하다.\n' +
      '\n' +
      '초기 상태 공간 모델은 서열 처리를 위한 컨볼루션[17, 20]과 효율적인 자기 회귀 생성을 위한 재발로 효율적으로 계산할 수 있는 선형 동적 시스템에서 영감을 받았다. 그러나 이러한 모델은 트랜스포머보다 표현력이 현저히 낮고 성능이 우수했다.\n' +
      '\n' +
      '최근 여러 작품[14, 21]은 상태 공간 재귀의 근본적으로 선형적인 성질을 유지하면서 주의의 QKV 행렬과 유사한 입력 종속 게이팅을 사용하여 상태 공간 모델의 표현성을 높이는 것을 목표로 하고 있다. 따라서, 이는 컨볼루션 또는 선택적-스캔을 통한 효율적인 구현이 실제로 변압기 성능에 대한 갭을 실질적으로 닫으면서 유지될 수 있게 한다.\n' +
      '\n' +
      'Mamba[9]는 2.8B 규모까지의 변압기와 비교할 수 있는 강력한 성능과 유망한 스케일링 법칙을 보여주는 이러한 이전 작업에 따라 최근에 출시된 상태 공간 모델이다. Mamba는 맞춤형 선택적 스캔 커널을 통해 효율적인 계산을 유지하면서 SSM 재귀에 대한 입력의 입력 종속 게이팅을 사용한다.\n' +
      '\n' +
      '### _ Experts_\n' +
      '\n' +
      'MoE 모델은 동등한 밀집 모델[11, 12]보다 FLOP당 훈련 및 추론 모두에서 훨씬 더 높은 성능을 달성하는 것으로 입증되었다. 또한, MoE 모델에 대한 스케일링 법칙은 계산, 데이터 및 라우팅되는 전문가의 수에 따라 MoE 성능이 원활하게 향상됨을 보여준다[22]. 이 후자는 추론 비용을 고정시키면서 모델의 능력을 지속적으로 증가시키는 경로를 제공하기 때문에 특히 중요하다.\n' +
      '\n' +
      'MoE 모델은 상당한 가능성을 가지고 있지만 아키텍처는 여전히 많은 단점을 가지고 있다. 전문가 수를 늘리면 매개변수 수와 메모리 비용이 크게 증가하는 반면 많은 작업에서는 MoE 모델이 덜 안정적이고 훈련하기가 더 어렵다고 보고한다. 더욱이, 라우터를 훈련시키기 위한 효과적인 방법들은 전문가에게 라우팅하거나 그렇지 않은 결정은 이산적이고 쉽게 역전파될 수 없기 때문에 여전히 열려 있다. 상대적으로 저사양 GPU에서 실행되는 사용자나 추론을 위해 모델 병렬성을 필요로 하는 단일 GPU에서 제공되는 것보다 메모리 크기가 더 커질 때, 조밀한 대응물에 대한 MoE의 큰 메모리 비용은 특히 중요하다.\n' +
      '\n' +
      '최근 [13]에서는 Llama 2 70B[5]와 경쟁적으로 수행하고, GPT-3.5에 근접한 전문가 모델의 강력한 오픈 소스 혼합물을 발표하면서, 원래의 미스트랄 7B 모델[23]의 순방향 통과 FLOP 비용만을 요구함으로써, 규모 면에서 MoE 모델의 가능성을 입증하고 확고히 했다. 미스트랄 아키텍처는 또한 초기 MoE 작업과 몇 가지 면에서 다르며, 특히 상대적으로 소수의 전문가를 사용하는 설계에서 우리가 또한 활용하고 독립적으로 MoE 모델의 FLOP 및 메모리 비용의 균형을 성공적으로 맞추는 데 유망한 것으로 나타났다.\n' +
      '\n' +
      '전문가들의 혼합을 고려한###_State-space 모델\n' +
      '\n' +
      'State-space 모델과 Mixture of Experts는 모두 추론 언어 모델의 계산 비용을 개선할 수 있는 유망한 아키텍처로 제안되었지만, 그 조합을 규모에서 테스트한 연구는 없다.\n' +
      '\n' +
      '이 연구와 동시에 [24]는 10B 토큰으로 훈련된 25M 모델의 1억 척도의 총 매개변수와 순방향 통과 FLOP에서 매우 작은 맘바-MoE 모델의 성능을 보여준다. 이와는 대조적으로, 우리는 300B 토큰에서 수십억 개의 매개변수 모델을 훈련함으로써 매개변수와 데이터 측면에서 의미 있는 규모에서 이러한 모델의 스케일링 잠재력과 성능을 경험적으로 보여준다. 따라서 본 연구는 상태 공간 모델과 MoE 모델의 조합에 대한 강력한 확장 가능성을 보여주면서 추론에 매우 효율적인 경쟁적이고 사용 가능한 언어 모델을 생성한다.\n' +
      '\n' +
      '## V Design\n' +
      '\n' +
      '### _Architecture_\n' +
      '\n' +
      '표준 트랜스포머 모델 [2]는 인터리브 어텐션과 잔류 스트림을 따라 순차적으로 추가된 MLP 블록으로 구성된다. 단일 변압기 층에 대한 방정식은 수학식 2에 기재되어 있다.\n' +
      '\n' +
      '대부분의 MoE 아키텍처는 단순히 MLP 블록을 라우팅된 전문가 계층으로 대체한다. 우리의 BlackMamba 아키텍처는 단순히 변압기의 MLP 계층을 전문가 계층으로, 주의 계층을 Mamba SSM 계층으로 대체한다(도 1 참조). 따라서 우리 아키텍처의 단일 블록은 다음과 같이 작성될 수 있다.\n' +
      '\n' +
      '\\[x_{l+1}= x_{l}+\\text{MoE}(\\text{LN}(x_{l}+\\text{mamba}(\\text{LN}(x_{l})))) \\tag{7}\\text{\n' +
      '\n' +
      '사용자 지정 데이터 세트에서 300B 토큰에 대해 블랙맘바 340M/1.5B 및 630M/2.8B 모델을 교육했습니다. 우리는 전문가 MLP를 위해 SwiGLU 활성화 함수[25]를 사용했다. 우리는 8명의 전문가와 함께 훈련했으며, 이는 모델의 추론 비용과 메모리 풋프린트 사이의 균형이 잘 맞춰져 있음을 발견했다. 우리는 순차 또는 병렬 [26] 블록이 더 나은 성능을 발휘하는지 테스트했으며 순차에 대한 약간의 이점을 발견했다. [5]에 이어, 우리는 편견 없이 훈련했다. 전문가 라우터는 전문가 간의 부하 균형을 위해 싱크혼 라우팅 기능이 있는 Top-1 라우팅을 사용했다. 우리는 바닐라 싱크혼(부록 F)보다 훨씬 빠르게 수렴하는 싱크혼 알고리즘의 새로운 맞춤형 버전을 활용했다. 우리는 메가트론-LM[27] 분산 훈련 프레임워크를 사용하여 훈련했다. 모델은 bf16 정밀도로 훈련되었다. 모든 추가 모델 아키텍처 및 훈련 하이퍼파라미터는 각각 부록 A와 B에 설명되어 있다.\n' +
      '\n' +
      '### _Dataset_\n' +
      '\n' +
      '블랙맘바를 훈련하기 위해 기존 오픈 소스 데이터 세트의 혼합물로 구성된 맞춤형 데이터 세트를 구성했다. 하위 집합에는 파일[28], 슬림파자마[29], 스타코더[30], PeS2o[31], ProofPile[32]가 포함되었다. 각 데이터 세트에 대한 가중치는 표 I에 제공된다. 토큰은 하위 집합 업웨이에서 샘플링할 확률에 따라 하위 집합 각각에서 교체 없이 샘플링되었다.\n' +
      '\n' +
      '도. 2: BlackMamba의 사전 훈련 데이터 세트에서 데이터 범주의 비율 총 데이터 세트는 1.8조 토큰으로 구성되어 단일 에폭보다 훨씬 적은 시간에 대해 훈련했다. 예비 실험 3은 사전 훈련 단계에 포함될 때 긴 형태의 텍스트와 학업 작업이 자연어 모델링을 향상시키는 것으로 나타나므로 훈련 레시피에서 크게 무게를 잰다. 또한 사전 훈련 단계에서 코드와 수학의 상당 부분을 포함하는 것이 모델의 추론 능력을 의미 있게 향상시킨다는 것을 발견했다. 이 데이터 세트는 필터링되지 않은 웹 데이터에 상대적으로 무겁고 더 작은 하위 집합의 가중화로 인해 많은 중복을 포함하고 있어 모델의 품질을 제한하고 개선의 상당한 여지를 남길 수 있으며 잠재적으로 특정 공통 단편에 대한 과도한 암기를 유발할 수 있다.\n' +
      '\n' +
      '각주 3: 우리는 그러한 실험이 아직 출판하기에 충분히 엄격하지 않으며 향후 작업에 포함될 것이라고 믿는다.\n' +
      '\n' +
      '## VI Results\n' +
      '\n' +
      '공정한 비교를 보장하기 위해 블랙맘바에 대해 보고된 동일한 데이터 세트 및 훈련 하이퍼파라미터를 사용하여 자체 340M 맘바 모델을 훈련했다. 이 맘바 340M 모델은 1152와 34개의 맘바 레이어의 숨겨진 크기를 사용했다. 특히, BlackMamba는 FLOP 학습뿐만 아니라 추론 시간에 동일한 순방향 통과 모델 크기에 대해 동등한 사전 훈련 모델(변압기 및 Mamba 모두)보다 훨씬 더 나은 성능을 보인다. 그림 5에서 초기 1토큰 프롬프트에서 시작하여 주어진 길이의 시퀀스를 자동으로 생성하는 데 걸리는 시간을 시퀀스 길이의 함수로 표시한다. 우리는 Mamba와 MoE 모델의 확립된 지연 이점이 BlackMamaba에서 결합되어 표준 변압기 모델, MoE 변압기 모델 및 순수 Mamba 모델보다 훨씬 더 빠른 추론 시간을 초래한다는 것을 관찰한다. 더욱이, 상기 추론 장점은,\n' +
      '\n' +
      '도. 4: 훈련 FLOP에 걸친 블랙맘바 평균 평가 성능의 비교.\n' +
      '\n' +
      '도. 3: 활성화된 순방향 파라미터에 걸친 블랙맘바 평균 평가 성능의 비교.\n' +
      '\n' +
      '블랙맘바는 더 긴 서열 길이에 따라 증가하여 긴 서열 생성에서 블랙맘바를 매우 경쟁적으로 만든다. 더욱이, 이 그림에는 반영되지 않았지만, 트랜스포머 추론 레이턴시도 선형적으로 증가하지만, 이는 메모리 요구량이 추가로 선형적으로 증가하고 결국 충분한 시퀀스에서 OOM이 되는 KV 캐싱에 기인한다는 것을 인식해야 한다. 대조적으로, Mamba 모델(및 BlackMamba)은 일정한 메모리 풋프린트를 갖는 임의의 길이의 시퀀스를 생성할 수 있다.\n' +
      '\n' +
      '그림 6과 7은 각각 BlackMamba 340M/1.5B 모델과 BlackMamba 630M/2.8B 모델의 각 레이어에서 각 전문가에게 할당된 토큰 카운트를 보여준다. 대부분의 레이어는 개선된 싱크혼 알고리즘에 의해 예상대로 고수준의 전문가 균형을 나타낸다. 그러나 흥미롭게도 두 모델 모두 최종 레이어에서 전문가 불균형으로 명확한 전환을 보여준다(340M/1.5B 모델의 경우 레이어 20, 630M/2.8B 모델의 경우 레이어 25). 이는 나중의 계층에서 증가하는 전문화를 반영하거나 네트워크에서 더 깊게 발달하는 수치적 불안정성을 반영할 수 있다. 이 불균형의 진정한 원인은 아직 알려지지 않았지만, 우리는 또한 유사한 패턴의 불균형이 있지만 안정적인 전문가 할당에 대한 수렴이 이전 MoE 모델에서도 관찰되었음을 주목한다[34].\n' +
      '\n' +
      '표 I에서 우리는 오픈 소스 사전 훈련된 언어 모델 베이스라인 세트에 대한 블랙맘바의 평가 점수를 보고한다. 우리는 자체 모델을 평가한 동일한 버전의 lm-eval(v0.3.0)에서 모든 모델을 *로 재평가했다.\n' +
      '\n' +
      '각주*: [9]와 다른 본 논문에서 비정규화된 HellaSwag 평가 결과를 사용한다.\n' +
      '\n' +
      '부록 E에서는 10k 단계마다 체크포인트에서 훈련하는 동안 모델에 대한 평가 점수를 제공한다. 우리는 일반적으로 훈련 동안 평가 점수에서 비교적 부드럽지만 시끄러운 개선을 발견했다. 평가의 과적합을 방지하기 위해 모델이 교육을 마친 후 평가 점수만 보고 모델 선택에 사용하지 않았다.\n' +
      '\n' +
      '또한 부록 F에서는 수렴 속도를 크게 향상시키는 MoE 라우팅에 사용되는 고전적인 싱크혼 알고리즘에 대한 새로운 초기화를 설명하며, 종종 수렴을 위해 단일 반복만 필요하다. 이는 라우팅된 전문가 계층들에 대해 주목할만한 속도 개선을 제공하고, 정규화된 밸런싱 손실을 갖는 라우터와 유사한 레이턴시를 초래하여, 구현의 훨씬 적은 복잡성을 요구하면서도 우수한 밸런싱 성능을 제공한다.\n' +
      '\n' +
      '마지막으로 부록 C에서는 Mamba Block의 내부 계산에 대한 자세한 수학적 설명을 제공하고 부록 D에서는 Mamba 및 MoE 모델에 대한 매개변수를 계산하고 FLOP를 훈련하기 위한 상세하고 명시적인 공식을 제공하여 커뮤니티가 새로운 SSM 및 MoE 아키텍처를 추가로 개발하고 탐구하는 데 도움이 되기를 바랍니다.\n' +
      '\n' +
      '도. 5 : 밀집형 변압기, 밀집형 맘바, 변압기-MoE 대비 블랙맘바의 발생 지연시간\n' +
      '\n' +
      '## VII Discussion\n' +
      '\n' +
      '이 작업은 추론 및 생성 시간 및 훈련 FLOP 측면에서 매우 경쟁적이고 효율적인 아키텍처를 생성하기 위해 SSM의 최근 발전과 MoE를 결합하는 핵심 개념의 예비 탐색 및 검증이다. 초기 결과는 유망하지만 SSM 및 MoE 구성 요소를 개선하고 조합에 접근하는 최적의 방법에 대한 조사를 위해 많은 작업이 필요하다. 우리는 궁극적으로 유망한 신흥 아키텍처와 이를 병합하고 결합하는 새로운 방법을 탐구함으로써 표준 변압기 레시피에 비해 성능, 효율성 및 속도의 상당한 발전을 얻을 수 있다고 믿는다.\n' +
      '\n' +
      '우리는 우리의 작업이 많은 유익한 방향으로 확장될 수 있다고 믿습니다. 본 논문에서 제시한 평가는 그 범위가 제한적이다. 제로샷 설정에서 표준 순수 언어 모델링 평가에 대한 일반적인 커버리지를 제공하는 반면, 다샷 상황 내 학습 설정에서 모델의 성능은 아직 탐구되지 않았다. 또한, 우리가 명시적으로 조사하지 않은 우리 모델의 행동에는 많은 측면이 있다. 우리는 사실적 정확성, 비속어, 독성 또는 사회적으로 바람직하지 않은 텍스트 생성에 대해 테스트하지 않았다. 마찬가지로, 우리의 훈련 데이터 세트 블렌드는 사회적으로 바람직하지 않은 토큰에 대해 명시적으로 스크래핑되지 않았으며 평가 태스크 4와의 잠재적 중복도 없다. 우리의 데이터 세트는 불완전하지만 목표를 가진 구성 및 구성에 대한 모든 주요 세부 사항을 발표했다.\n' +
      '\n' +
      '도. 6 : 340M/1.5B 블랙맘바의 전문가간 토큰 배포\n' +
      '\n' +
      '도. 7: 630M/2.8B BlackMambaof의 전문가 전반에 걸친 토큰 배포는 사전 훈련 성능 및 모델 행동에 대한 데이터 세트의 영향에 대한 커뮤니티 이해를 돕는다.\n' +
      '\n' +
      '스케일링 법칙의 관점에서, 우리의 모델은 주어진 추론 비용과 FLOP 훈련 예산에 대해 매우 경쟁력이 있지만, 3,000억 토큰에 대해 훈련된 두 개의 모델만으로 데이터와 매개변수 카운트 측면에서 결정적인 스케일링 외삽을 만드는 것은 불가능하다. 또한, 학습 속도의 기본 하이퍼파라미터 튜닝만 수행했기 때문에 많은 학습 하이퍼파라미터가 최적이 아닐 수 있다. 또한, 핵심 아키텍처에 대한 몇 가지 삭제를 수행했지만, 상태 공간 모델과 전문가의 혼합을 결합하는 우수한 방법이 상당한 이점을 제공할 수 있다. 또한, SSM 및 MoE 모델의 매개변수 효율적인 미세 조정을 위한 표준 기술뿐만 아니라 지시 후 및 일반적인 정렬을 위한 잘 확립된 미세 조정 및 RLHF 파이프라인의 효과와 성능은 이러한 모델이 양자화 하에서 수행되는 방식과 마찬가지로 거의 완전히 조사되지 않은 상태로 남아 있다.\n' +
      '\n' +
      '우리의 작업은 또한 최종 모델 아키텍처에 함께 배치될 수 있는 다양한 신경망 구성요소의 모듈성에 대한 흥미로운 질문을 제기한다. 우리는 SSM 블록과 변압기의 MoE 블록을 규모 면에서 경쟁적인 성능으로 결합하는 것이 비교적 간단하다는 것을 보여준다. 그러나, Mamba 및 다른 SSM이 변압기와 마찬가지로 MoE와 동일한 정도의 성능 향상을 보이는지 여부는 불확실하며, 이러한 건축 조각을 결합하는 것이 모델의 내부 표현 및 행동에 동일한 영향을 미치는지 여부도 불확실하다. 또한 블랙맘바에서 라우팅이 보다 고전적인 변압기 MoE 모델과 동일한 기능을 수행하는 범위가 불분명하다.\n' +
      '\n' +
      '## VIII Conclusion\n' +
      '\n' +
      '본 논문에서는 최근 진행된 상태 공간 모델과 전문가의 혼합을 하나의 통합된 아키텍처로 결합한 모델인 블랙맘바를 제안, 구현 및 훈련시켰다. 우리는 우리의 BlackMamba 아키텍처가 추론 비용 및 훈련 플롭 측면에서 강력한 사전 훈련된 LLM 기준선에 대해 매우 경쟁적으로 수행하고, 또한 SSM과 MoE의 감소된 훈련 및 생성 FLOP를 동시에 상속한다는 것을 보여준다. 또한, 블랙맘바가 선형 시간과 메모리 비용으로 빠른 생성이 가능함을 보인다. 우리는 블랙맘바 340M/1.5 및 630M/28억 매개변수 모델과 중간 체크포인트, 추론 코드를 허용 아파치 2.0 라이선스에서 공개하며 광범위한 커뮤니티에 의한 이 새로운 아키텍처의 잠재력에 대한 추가 연구, 실험 및 이해를 가능하게 하고 육성하는 것을 목표로 한다.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      '자이프라 팀은 아담 이브라힘에게 훈련 안정성과 하이퍼파라미터에 대한 유용한 토론과 논평에 감사하고, 앨버트 구는 주 우주 모델에 대한 일반적인 토론에 감사하고 싶다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] D. Bahdanau, K. Cho, and Y. Bengio, "Neural machine translation by jointly learning to align and translate," _arXiv preprint arXiv:1409.0473_, 2014.\n' +
      '* [2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," _Advances in neural information processing systems_, vol. 30, 2017.\n' +
      '* [3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever _et al._, "Language models are unsupervised multitask learners," _OpenAI blog_, vol. 1, no. 8, p. 9, 2019.\n' +
      '* [4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell _et al._, "Language models are few-shot learners," _Advances in neural information processing systems_, vol. 33, pp. 1877-1901, 2020.\n' +
      '* [5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale _et al._, "Llama 2: Open foundation and fine-tuned chat models," _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [6] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly _et al._, "An image is worth 16x16 words: Transformers for image recognition at scale," _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* [7] K. Rasul, A. Ashok, A. R. Williams, A. Khrossani, G. Adamopoulos, R. Bhagwatkar, M. Bilos, H. Ghonia, N. V. Hassen, A. Schneider _et al._, "Lag-llama: Towards foundation models for time series forecasting," _arXiv preprint arXiv:2310.08278_, 2023.\n' +
      '* [8] S. Reed, K. Zohna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg _et al._, "A generalist agent," _arXiv preprint arXiv:2205.06175_, 2022.\n' +
      '* [9] A. Gu and T. Dao, "Mamba: Linear-time sequence modeling with selective state spaces," _arXiv preprint arXiv:2312.00752_, 2023.\n' +
      '* [10] B. Peng, E. Aleaide, O. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K. GV _et al._, "Rwkv: Reinventing rnns for the transformer era," _arXiv preprint arXiv:2305.13048_, 2023.\n' +
      '* [11] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," _The Journal of Machine Learning Research_, vol. 23, no. 1, pp. 5232-5270, 2022.\n' +
      '* [12] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A. Awan, J. Rasley, and Y. He, "Deepsped-more: Advancing mixture-of-experts inference and training to power next-generation ai scale," in _International Conference on Machine Learning_. PMLR, 2022, pp. 18 332-18 346.\n' +
      '* [13] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaptol, D. d. I. Casas, E. B. Hanna, F. Bressand _et al._, "Mirxtl of experts," _arXiv preprint arXiv:2404.04088_, 2024.\n' +
      '* [14] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei, "Retentive network: A successor to transformer for large language models (2023)," _URL [http://arxiv.org/abs/2307.08621](http://arxiv.org/abs/2307.08621) v1_.\n' +
      '* [15] D. Leipkhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, "Gshard: Scaling giant models with conditional computation and automatic handling," _arXiv preprint arXiv:2006.16668_, 2020.\n' +
      '* [16] W. Fedus, J. Dean, and B. Zoph, "A review of sparse expert models in deep learning," _arXiv preprint arXiv:2209.01667_, 2022.\n' +
      '* [17] A. Gu, K. Goel, and C. Re, "Efficiently modeling long sequences with structured state spaces," _arXiv preprint arXiv:2110.00396_, 2021.\n' +
      '* [18] B. Peng, J. Quesselle, H. Fan, and E. Shippole, "Yarm: Efficient context window extension of large language models," _arXiv preprint arXiv:2309.00071_, 2023.\n' +
      '* [19] S. Chen, S. Wong, L. Chen, and Y. Tian, "Extending context window of large language models via positional interpolation," _arXiv preprint arXiv:2306.15595_, 2023.\n' +
      '* [20] M. Poli, S. Massarooli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. Re, "Hyena hierarchy: Towards larger convolutional language models," _arXiv preprint arXiv:2302.10866_, 2023.\n' +
      '* [21] S. Arora, S. Eyuboglu, A. Timalasin, I. Johnson, M. Poli, J. Zou, A. Rudra, and C. Re, "Zoology: Measuring and improving recall in efficient language models," _arXiv preprint arXiv:2312.04927_, 2023.\n' +
      '* [22] A. Clark, D. De Las Casas, A. Guy, A. Mensch, M. Paganini, J. Hoffmann, B. Damoc, B. Hechtman, T. Cai, S. Borgeaud _et al._, "Unified scaling laws for routed language models," in _International Conference on Machine Learning_. PMLR, 2022, pp. 4057-4086.\n' +
      '* [23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. I. Casas, F. Bressand, G. Langley, G. Lample, L. Saulnier _et al._, "Misrtl 7b," _arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* [24] M. Pioro, K. Ciebiera, K. Krolj, J. Ludriejewski, and S. Jaszczur, "Moemabmba: Efficient selective state space models with mixture of experts," _arXiv preprint arXiv:2401.04081_, 2024.\n' +
      '* [25] N. Shazeer, "Glu variants improve transformer," _arXiv preprint arXiv:2002.05202_, 2020.\n' +
      '* [26] B. Wang and A. Komatuszaki, "Gpt-j-6b: A 6 billion parameter autoregressive language model," 2021.\n' +
      '* [27] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-lm: Training multi-billion parameter language models using model parallelism," _arXiv preprint arXiv:1909.08053_, 2019.\n' +
      '* [28] L. Gao, S. Biderman, S. Black, L. Goldling, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima _et al._, "The pile: An 800gb dataset of diverse text for language modeling," _arXiv preprint arXiv:2101.00027_, 2020.\n' +
      '* [29] D. Soboleva, F. Al-Khateeb, R. Myers, J. Steeves, J. Hestness, and N. Dey, "Simpliamaz: A 627b token cleaned and dedupleted version of redjaguima," 7 2023. [Online]. Available: [https://www.corebras.net/blog/simplajama-a-627b-token-cleaned-and-dedupleted-version-of-redpajama](https://www.corebras.net/blog/simplajama-a-627b-token-cleaned-and-dedupleted-version-of-redpajama)\n' +
      '* [30] R. Li, L. B. Allal, Y. Zi, N. Muenninghoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Lim _et al._, "Starcoder: may the source be with you!" _arXiv preprint arXiv:2305.06161_, 2023.\n' +
      '* [31] L. Soldaini and K. Lo, "peS2o (Pretraining Efficiently on S2ORC) Dataset," Allen Institute for AI, Tech. Rep., 2023, oDC-By, [https://github.com/allenai/peseo2o](https://github.com/allenai/peseo2o).\n' +
      '* [32] Z. Azerabyrev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and S. Welleck, "Llemma: An open language model for mathematics," _arXiv preprint arXiv:2310.10631_, 2023.\n' +
      '* [33] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap, "Compressive transformers for long-range sequence modelling," 2019.\n' +
      '* [34] J. He, J. Zhai, T. Antunes, H. Wang, F. Luo, S. Shi, and Q. Li, "Faster-moe: modeling and optimizing training of large-scale dynamic pre-trained models," in _Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming_, 2022, pp. 120-134.\n' +
      '* [35] Y. Elazar, A. Bhagia, I. Magnusson, A. Ravichander, D. Schwenk, A. Suhr, P. Walsh, D. Groeneveld, L. Soldaini, S. Singh, H. Hajishirzi, N. A. Smith, and J. Dodge, "What\'s in my big data?" 2023.\n' +
      '* [36] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPoffi, C. Foster, L. Golding, J. Hsu, A. Le Noac\'h, H. Li, K. McDonell, N. Muenninghoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutwika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, "A framework for few-shot language model evaluation," 12 2023. [Online]. Available: [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)\n' +
      '* [37] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, "Hellaswag: Can an machine really finish your sentence?" 2019.\n' +
      '* [38] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, "Piqa: Reasoning about physical commonsense in natural language," 2019.\n' +
      '* [39] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, "Winogrande: An adversarial winograd schema challenge at scale," 2019.\n' +
      '* [40] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fernandez, "The lambda dataset: Word prediction requiring a broad discourse context," 2016.\n' +
      '* [41] P. Clark, I. Cowbe, O. Etzioni, T. Kh\n' +
      '\n' +
      '### _Model Hyperparameters_\n' +
      '\n' +
      '### _Manba Block Internals_Manba Block Internals_\n' +
      '\n' +
      '이 부록에서는 맘바 블록을 구성하는 핵심 계산에 대한 정밀하고 상세한 설명을 제공한다. 맘바는 최근 대규모 서열 모델링에서 변압기와 경쟁할 수 있는 표현적 반복 모델인 상태 공간 모델에 대한 작업 라인에서 파생된다. 이러한 모델의 재발은 KV 캐시가 없는 생성에 효율적으로 사용될 수 있게 하고 시퀀스 길이에서 FLOP 및 메모리에서 선형적으로 확장되도록 한다. 핵심 통찰력은 중앙 재발을 병렬 GPU 하드웨어에 효율적으로 매핑하기 위해 재발[17] 또는 선택적 스캔[9]을 활용하는 것이다. 이러한 모든 모델의 기초는 다음과 같은 상태-공간 방정식(연속 시간)이다:\n' +
      '\n' +
      '\\[\\frac{dh}{dt} = A\\,h+B\\,x\\tag{8}\\] \\[y=C\\,h\\tag{9}\\]\n' +
      '\n' +
      '고전적인 선형 시간 불변 동적 시스템을 정의합니다. 여기서 \\(h\\)는 한 순간에 시스템의 상태를 나타낸다. \\(h\\) (A\\)는 시간에 따른 \\(h\\)의 \'자연 역학\'을 지배하는 매트릭스를 나타낸다. \\ (x\\)는 시스템에 대한 \'제어\' 입력, 즉 제어기 또는 실험자가 제공한 입력을 의미하고 \\(B\\)는 \\(x\\)이 시스템과 어떻게 상호작용하는지를 제어하는 역학 매트릭스를 의미한다. 마지막으로 관측행렬(C\\)을 통해 관측값(y\\)으로 변환된다.\n' +
      '\n' +
      '맘바 블록은 토큰에 걸친 이 동적 시스템을 하드웨어 효율적인 선택적 스캔으로 구현된 핵심 계산으로 활용한다. Mamba의 혁신은 특히 자기 주의 블록의 \\(Q\\), \\(K\\), \\(V\\) 행렬과 유사한 \\(A\\), \\(B\\), \\(C\\) 행렬을 입력 \\(x\\)의 선형 함수로 만드는 것이다. 이를 넘어, 맘바는 SSM 컴포넌트를 잔차 스트림과 입력의 컨볼루션으로 오가는 선형 투영으로 랩핑하고, 또한 블록에 대한 입력의 투영에 기초하여 SSM의 출력을 게이팅하는 추가 게이팅 투영 경로로 랩핑한다.\n' +
      '\n' +
      '우리는 Mamba 블록 입력\\(x\\), 순환 은닉 상태\\(h\\), 시퀀스 길이를 \\(l\\)으로 나타낸다. 숨겨진 반복 상태 차원을 입력 차원의 일부 요인으로 설정했다.\n' +
      '\n' +
      'Mamba 블록은 반복 상태에 대한 동역학을 정의하는 행렬 \\(A\\), 입력에 대한 투영인 \\(B\\), 출력에 대한 투영인 \\(C\\), 출력에 대한 학습 가능한 바이어스인 행렬 \\(D\\), 이산화 타임스텝 \\(dt\\), 게이팅 벡터 \\(z\\)을 포함한다. 또한 Mamba 블록은 가중치 행렬 \\(W_{x}\\)과 \\(W_{z}\\)과 출력 투영 행렬 \\(W_{y}\\)을 갖는 SSM에 앞서 입력 x와 z의 선형 투영을 수행한다.\n' +
      '\n' +
      '맘바 블록 내부의 계산은 다음과 같이 실행됩니다. 먼저, \\(x\\) 및 \\(z\\) 투영을 계산한다. 이 투영은 시퀀스의 모든 토큰에 대해 독립적으로 발생합니다.\n' +
      '\n' +
      '\\[x=W_{x}\\,x\\tag{10}\\] \\[z=W_{z}\\,z\\tag{11}\\]\n' +
      '\n' +
      '두 번째로, 프로젝션 후에, 맘바 블록은 입력 시퀀스 임베딩들에 걸쳐 1d 컨볼루션(\\(*\\))을 수행한다. 이 컨벌루션은 임베딩 레벨에서 작용하고, 컨벌루션은 토큰 레벨의 시퀀스에서 작용하기 때문에 프로젝션 \\(W_{x}\\)과 병합될 수 없다.\n' +
      '\n' +
      '\\[x_{t}=W_{filter\\_\\_}*x_{t} \\tag{12}\\]\n' +
      '\n' +
      '그런 다음 입력 종속 가중치 \\(B\\), \\(C\\) 및 \\(dt\\)을 계산할 수 있으며, 이는 주의에서 쿼리, 키 및 값 가중치와 유사하다.\n' +
      '\n' +
      '\\[B=W_{B}\\,x\\tag{13}\\] \\[C=W_{C}\\,x\\] (14) \\[dt=W_{D}\\,x\\tag{15}\\]\n' +
      '\n' +
      '행렬 \\(A\\)은 아래의 행렬에 주어진 특별한 초기화로 학습된다. 업데이트는 매개변수화\\(\\ln(A)\\)을 통해 훈련되며, 아마도 \\(A\\)을 양성화하고 안정성을 향상시키기 위해 훈련된 다음 \\(A=\\exp(\\ln(A)\\,)\\)로 계산된다.\n' +
      '\n' +
      '그 다음 SSM 커널에 사용하기 전에 가중치를 이산화한다. B에 대한 이산화는 [9]의 수학식 4를 따르지 않는다는 점에 유의한다.\n' +
      '\n' +
      '\\[dt=\\text{softplus}(dt+dt_{\\text{bias}}) \\tag{17}\\] \\[dA=\\exp(-A\\,dt)\\] (18) \\[dB=B\\,dt\\tag{19}\\]\n' +
      '\n' +
      '그런 다음 ssm의 단일 단계를 수행하여 새로운 반복 상태를 얻는다. 예상대로 \\(dt\\to 0\\)일 때 \\(h^{+}\\to h\\)\n' +
      '\n' +
      '\\[h^{+}=dA\\,h+dB\\,x \\tag{20}\\]\n' +
      '\n' +
      '새로운 순환상태로부터 출력 \\(C\\,h^{+}\\)을 계산할 수 있다. 이 출력은 또한 학습된 게이팅 벡터 z에 의해 게이팅되고 잔류 스트림에 다시 추가되기 전에 최종 출력 투영을 통과한다.\n' +
      '\n' +
      '\\[y=C\\,h^{+}+D\\,x\\tag{21}\\] \\[y=\\text{silu}(z)\\,y\\] (22) \\[y=W_{y}\\,y\\tag{23}\\]\n' +
      '\n' +
      'SSM 블록의 출력은 은닉 상태\\(h^{+}\\)와 출력\\(y\\)이 된다.\n' +
      '\n' +
      '맘바 블록은 두 가지 모드로 작동할 수 있습니다. 첫 번째 모드는 여기에서 설명한 단계를 직접 따르는 순환 방법이다. 이 접근법은 다음 토큰을 예측하기 위해 순환 상태만 사용하기 때문에 단일 단계에 대해 메모리와 계산 비용 모두에서 선형적이다. 두 번째 방법은 [9]에서 소개한 \'선택 스캔\' 연산과 커널을 사용하여 전체 시퀀스에 걸쳐 SSM을 한 번에 실행하는 것이다. 선택적 스캔의 구현에 대한 추가 참조는 [9]를 참조한다.\n' +
      '\n' +
      'Mamba-MoE_에 대한###_계산 파라미터 및 FLOP\n' +
      '\n' +
      '임베딩 차원 \\(D\\), 맘바 내부 상태를 \\(I\\), 순환 상태 차원 \\(H\\), dt 랭크 \\(dt\\) 및 컨볼루션 차원 \\(C\\)으로 나타내자. 우리는 배치 크기\\(B\\)와 서열 길이\\(L\\)을 나타낸다.\n' +
      '\n' +
      '이어서, 맘바 블록 내의 파라미터들의 수는 다음과 같이 계산될 수 있다,\n' +
      '\n' +
      '\\\\\\underbrace{3ID}_{W_{x},W_{z},W_{y}}+2I(\\underbrace{H}_{W_{A},W_{B}}+\\underbrace{dt}_{W_{dt}+\\underbrace{\\frac{C}{2}}_{\\text{conv}})+\\underbrace{I}_{D}+\\underbrace{2D}_{\\text{layernorm} \\tag{25}\\underbrace{H}_{W}{A},W_{z},W_{y}}+2I(\\underbrace{H}_{W}{A},W_{B}}+\\underbrace{dt}_{dt}+\\underbrace{c}{2}}_{\\text{conv}})+\\underbrace{I}_{D}+\\underbrace{2D}_{\\text{layernorm}}\\tag{25}\\underbrace{\n' +
      '\n' +
      'MoE 블록 내의 파라미터들의 수는 다음과 같이 계산될 수 있다.\n' +
      '\n' +
      '\\[\\underbrace{8D^{2}E}_{\\text{experts}}+\\underbrace{DE}_{\\text{router}} \\tag{26}\\]\n' +
      '\n' +
      '여기서 \\(E\\)는 레이어의 전문가 수이다. \\(L\\) 층의 네트워크에는 \\(\\frac{L}{2}\\) Mamba 블록과 \\(\\frac{L}{2}\\) MoE 블록이 있다.\n' +
      '\n' +
      '단일 맘바 블록에 포함된 FLOP의 수를 근사화하기 시작하기 위해 다음과 같이 관찰한다.\n' +
      '\n' +
      '두 행렬(A\\in\\mathcal{R}^{K\\times M}\\)과 \\(B\\in\\mathcal{R}^{M\\times J}\\)이 주어지면, 행렬 곱에 관여하는 총 FLOP는 대략 \\(2KMJ\\)이며, 여기서 \\(2\\)의 인자는 곱셈과 덧셈 연산을 모두 필요로 한다. 다음 계산에서 행렬 곱셈이 모델의 총 FLOP 수를 지배하므로 비선형성, 계층 규범 및 기타 계산을 무시한다고 가정한다.\n' +
      '\n' +
      '먼저, 가중치\\(W_{x}\\),\\(W_{z}\\) 및 \\(W_{y}\\)을 포함하는 투영 연산을 고려한다. 모두 형상\\(I\\times D\\)이므로 이들에 대한 총 FLOP는 \\(6IDLB\\)이다.\n' +
      '\n' +
      '또한 \\(2ICLB\\) FLOP를 필요로 하는 단일 \\(I\\times C\\) 행렬로 다룰 수 있는 컨볼루션도 있다.\n' +
      '\n' +
      '이제 SSM 블록 자체를 살펴보겠습니다. 먼저 입력 종속적인 \\(B\\) 행렬과 \\(C\\) 행렬에 각각 모양 \\(I\\times H\\)을 곱하여 \\(4IH\\) FLOP를 계산한다. \\(A\\) 행렬은 입력에 곱하지 않고 요소 변환 비용 \\(IH\\) FLOP를 거친다. \\(dt\\) 투영은 먼저 차수\\(I\\) FLOP의 원소별 연산을 거친다.\n' +
      '\n' +
      '다음으로, 이산화를 행한다. \\(A\\) 행렬에 \\(dt\\) 벡터를 곱하여 \\(IH\\) FLOP를 구한다. 입력비용 \\(2IH\\) FLOP에 \\(B\\) 행렬을 곱한다. SM 선형 상태공간 단계 자체는 행렬에 곱하여 더하기 때문에 \\(2IH\\) FLOPs가 소요되고, \\(C\\) 행렬을 이용한 출력 투영도 \\(2IH\\) FLOPs가 소요된다. 이 모든 것을 종합하면 다음과 같은 식을 얻을 수 있다.\n' +
      '\n' +
      '\\brace{11H}_{W_{x},W_{z},W_{y},\\text{ SSM}+\\underbrace{4dt}_{text{d proj. discretization}+\\underbrace{1}_{text{dt nonlinearity})+\\underbrace{IH}_{A}\\end{split}\\tag{27}\\underbrace{11H}_{x},W_{z},W_{y},\\text{ SSM}+\\underbrace{4dt}_{text{d proj. discretization}+\\underbrace{1}_{text{dt nonlinearity})+\\underbrace{IH}_{A}\\end{split}\\tag{27}\\underbrace{4dt}\\text{dt}\\text{dt nonlinearity}\n' +
      '\n' +
      'MoE 블록은 \\(E\\) 표준 mlp 블록과 라우터로 구성된다. 각 mlp 블록에 대한 FLOP는 모양(4D\\times D\\)과 행렬당 곱셈과 덧셈의 두 개의 가중치 행렬이 있기 때문에 단순하게 \\(16D^{2}\\)이다. 라우터 비용은 간단히 \\(2DE\\)이다. 이를 종합하면 MoE 블록에 대한 \\(DE(16D+2)\\) FLOP를 얻을 수 있다.\n' +
      '\n' +
      '###_훈련중 평가\n' +
      '\n' +
      '우리는 제로 샷 환경에서 8가지 다양한 평가 태스크로 구성된 제품군에서 블랙맘바를 평가한다. 우리는 EleutherAI 평가 하네스(버전 0.3.0)를 사용한다[36]. 구체적으로, 우리는 HellaSwag[37], PIQA[38], WinoGrande[39], Lambada[40], ARC[41], OpenBookQA[42]에서 모델을 평가한다. 평가들은 매 단계(10,000\\)마다 수행된 모델 체크포인트에서 실행되었다. 우리는 대부분의 평가 메트릭이 최종 값으로 안정되기 전에 훈련 전반에 걸쳐 매끄럽지만 시끄럽게 증가하는 것으로 보인다는 것을 관찰한다. 이는 피티아 모델 제품군[43]의 이전 연구 결과와 대체로 일치하며, 이는 많은 평가 메트릭에서 훈련 전반에 걸쳐 비교적 원활한 개선을 발견한다. 이는 언어 모델의 능력 개발이 원활하게 일어나고 훈련 중에 추적될 수 있고 아마도 미리 예측될 수 있다는 몇 가지 증거를 제공한다. 그러나 두 가지 평가 지표인 위노그랑데와 볼큐는 현재 우리가 이해하지 못하는 이유로 이러한 추세를 위반한다. 우리는 [43]도 관찰한다는 점에 주목한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:12]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>