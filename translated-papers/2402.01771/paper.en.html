<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# BlackMamba: Mixture of Experts for State-Space Models\n' +
      '\n' +
      'Quentin Anthony\\({}^{*}\\)\n' +
      '\n' +
      'Yury Tokpanov\\({}^{*}\\)\n' +
      '\n' +
      'Paolo Glorioso\\({}^{*}\\)\n' +
      '\n' +
      'Beren Millidge\\({}^{*}\\)\n' +
      '\n' +
      '{quentin, yury, paolo, beren}@zyphra.com\n' +
      '\n' +
      'Zyphra\n' +
      '\n' +
      'Palo Alto, CA\n' +
      '\n' +
      'All authors contributed equally to this work\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint. In this paper, we present BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate that BlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. We fully train and open-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a custom dataset. We show that BlackMamba inherits and combines both of the benefits of SSM and MoE architectures, combining linear-complexity generation from SSM with cheap and fast inference from MoE. We release all weights, checkpoints, and inference code open-source. 1\n' +
      '\n' +
      'Footnote 1: Inference code at: [https://github.com/Zyphra/BlackMamba](https://github.com/Zyphra/BlackMamba)\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      'The advent of Large Language Models (LLMs) built from decoder-only transformer models [1, 2] have revolutionized Natural Language Processing (NLP) [3, 4, 5], along with diverse deep learning application domains such as image processing [6], time-series [7], and reinforcement learning [8]. Despite the strong performance and scalability of the transformer architecture, however, there remain significant shortcomings. While maximally expressive, the attention mechanism is computationally demanding both during training and inference, naively requiring both quadratic FLOPs and memory in the sequence length. This limits the context length of transformer models, makes autoregressive generation increasingly expensive with scale, and generally inhibits truly unlimited sequence processing and learning from continual datastreams.\n' +
      '\n' +
      'In order to ameliorate these problems, significant effort has recently been directed towards architectural alternatives to the canonical dense attention transformer model. Some of the most promising candidate architectures are State Space Models (SSMs) [9, 10] and Mixture of Experts (MoE) [11, 12, 13]. The key practical benefit of SSMs over transformers is their linear computational complexity with respect to input sequence length (as opposed to the quadratic complexity of transformers). This theoretically enables SSMs to process vastly longer sequences than transformers for a given FLOP budget, and to render autoregressive generation constant in compute without a KV cache. Notable recent examples of SSMs include Mamba [9], RWKV [10], and RetNet [14], all of which demonstrate efficient long-sequence training and inference, efficient implementations in CUDA, and competitive language modeling task performance to transformers with similar scaling properties. At the same time mixture of expert (MoE) architectures [15, 16, 11, 12] have become an emerging advance over dense transformers which allow for significantly reduced training and inference FLOPs required to achieve comparable quality to a comparable dense model. MoE models allow for only a sparse subset of the total parameters to be activated on a single forward pass, relying on a routing function to gate which \'experts\' are utilized or not depending on the context. This sparsity decouples the inference cost and parameter count of a model, enabling significantly stronger performance for a given inference budget at the cost of many more parameters and a correspondingly greater memory footprint.\n' +
      '\n' +
      'These architectural improvements over transformers are compelling on their own, but we believe that their combination is a natural next step that could enable significantly improved language modelling speed and performance against the canonical transformer. Specifically, we expect a Mamba-MoE architecture would have the following improvements over a dense transformer:\n' +
      '\n' +
      '* _Mamba_: Linear computational complexity with respect to input sequence length for both training and inference. Autoregressive generation in constant time and memory.\n' +
      '* _MoE_: Inference latency and training FLOPs of the equivalent smaller dense base model, while preserving model quality close to an equi-parameter dense model.\n' +
      '\n' +
      'In this paper, we begin to demonstrate that these improvements are achievable and that, when put together, these two approaches synergize to produce a model with compellingevaluation performance (Figs. 8-14), compute (Fig. 4), and latency advantages (Figs. 5 and 3) over existing transformer models and which can be trained at a fraction of the FLOP cost for similar performance (Fig. 4). We study the MoE routing statistics exhibited by our model across training time and across model depth. Additionally, we introduce a novel initialization for our routing Sinkhorn algorithm which significantly reduces the number of iterations required until convergence, thus improving routing speed.\n' +
      '\n' +
      '## II Contributions\n' +
      '\n' +
      'The main achievements of this work are:\n' +
      '\n' +
      '* We design, implement, and evaluate **BlackMamba**: a combination of alternating attention-free Mamba blocks and routed MLPs.\n' +
      '* We train and open-source two BlackMamba Models: 340M/1.5B BlackMamba and 630M/2.8B BlackMamba2. Footnote 2: In this paper, we denote an MoE model with \\(X\\) forward-pass parameters and \\(Y\\) total parameters as \\(X/Y\\).\n' +
      '* We demonstrate that BlackMamba requires significantly fewer training FLOPs to achieve comparable downstream task performance to a dense transformer model.\n' +
      '* We explore the compounding inference benefits of the combination of attention-free architectures such as Mamba along with routed sparsity architectures such as MoE.\n' +
      '\n' +
      'The rest of this paper is organized as follows. We first provide an overview of related works on SSM, MoE, and SSM with MoE in Section IV. We then provide background into the underlying concepts behind SSMs and MoE that are necessary to understand our contributions in Section III. Our architecture is described in Section V, and its training/inference dynamics are explored in Section VI. Finally, we describe the implications and limitations of our approach in Section VII along with our conclusions from this work in Section VIII.\n' +
      '\n' +
      'The final checkpoints are open-sourced on HuggingFace with Apache 2.0 licensing, and intermediate training checkpoints are available upon request. Inference code is provided at [https://github.com/Zyphra/BlackMamba](https://github.com/Zyphra/BlackMamba).\n' +
      '\n' +
      '## III Background\n' +
      '\n' +
      '### _Transformers_\n' +
      '\n' +
      'The transformer architecture [2] has demonstrated exceptionally strong and consistent performance at language modelling, as well as almost all other sequence processing tasks, remaining state-of-the-art and essentially unchanged since its introduction. The core operation of the transformer is self-attention, which performs a quadratic all-to-all comparison of the dot-product similarities between the embeddings of different tokens in a sequence before normalizing it and performing a linear map to an output vector. Mathematically, self-attention can be written as,\n' +
      '\n' +
      '\\[z=W_{V}x\\sigma(\\frac{1}{\\sqrt{d}}xW_{Q}W_{K}^{T}x\\circ M) \\tag{1}\\]\n' +
      '\n' +
      'Where \\(\\sigma\\) denotes the softmax function, \\(M\\) denotes a binary mask which enforces specific constraints, such as causal masking, on the computation, the superscript \\(T\\) denotes transposition, and \\(\\circ\\) denotes element-wise multiplication. The quadratic cost in sequence length is caused by the \\(xW_{Q}W_{K}^{T}x\\) term which computes a \\(L\\times L\\) matrix of similarity scores between the embeddings of different tokens where \\(L\\) is the sequence length.\n' +
      '\n' +
      'The transformer model consists of a stack of self-attention blocks interleaved with multi-layer-perceptron (MLP) blocks which consist of a two-layer MLP with a given activation function. A layer of a transformer model can thus be written as,\n' +
      '\n' +
      '\\[x_{l+1}=x_{l}+\\text{MLP}(\\text{LN}(x_{l}+\\text{attention}(\\text{LN}(x_{l})))) \\tag{2}\\]\n' +
      '\n' +
      'Where LN represents the layernorm operation which is used to normalize the inputs to the attention and MLP blocks.\n' +
      '\n' +
      'Fig. 1: Architecture of dense transformer, dense Mamba, transformer-MoE, and Mamba-MoE\n' +
      '\n' +
      '### _Mamba_\n' +
      '\n' +
      'State-space models (SSMs) are a class of sequence models that possess linear complexity with respect to the sequence length. SSMs are more closely related to RNN and CNN architectures than the attention mechanism, and draw inspiration from a continuous dynamical system (depicted in Equation 3) mapping a 1-dimensional function or sequence \\(x(t)\\in\\mathbb{R}\\mapsto y(t)\\in\\mathbb{R}\\) through an implicit latent state \\(h(t)\\in\\mathbb{R}^{N}\\):\n' +
      '\n' +
      '\\[h^{\\prime}(t)=Ah(t)+Bx(t),\\ \\ \\ y(t)=Ch(t) \\tag{3}\\]\n' +
      '\n' +
      'Where the \'time\' \\(t\\) now represents the sequence position of a token. A linear dynamical system like this can be efficiently computed in parallel via a convolution or associative scan, while the recurrent form presented above can be utilized for rapid generation at inference time. The fundamental innovation of the Mamba architecture is to make the \\(A\\), \\(B\\), and \\(C\\) matrices of the SSM linearly input-dependent. That is, the new dynamics can be written as,\n' +
      '\n' +
      '\\[h^{\\prime}(t)=A(x(t))h(t)+B(x(t))x(t),\\ \\ \\ y(t)=C(x(t))h(t) \\tag{4}\\]\n' +
      '\n' +
      'Intuitively, this enables the updates to the SSM\'s recurrent state to selectively depend upon the tokens being processed, with the SSM being able to decide to store or remove specific information from its recurrent state dynamically. This renders the \\(A\\),\\(B\\),\\(C\\) matrices loosely analogous to the \\(Q\\),\\(K\\),\\(V\\) matrices in attention and significantly increases the expressivity of the SSM block and could potentially enable context to persist much longer in the hidden state than otherwise, since it must exponentially decay in a linear dynamical system with fixed weights. Empirically, [17] found that this closed much of the gap with transformers.\n' +
      '\n' +
      'In practical terms, the recurrent nature of SSMs has long prevented their adoption on the reigning highly-parallel AI hardware like GPUs. However, recent implementations of recurrent and state-space models such as Mamba [9] and RWKV [10] have mapped these operations efficiently to GPU hardware via parallel scan kernels, thus enabling training of such novel architectures with efficiencies approaching that of well-optimized transformer models.\n' +
      '\n' +
      'For more details on Mamba, please see Appendix C which describes in details the internal computations of a Mamba block as well as [9] and its associated codebase.\n' +
      '\n' +
      '### _Mixture of Experts_\n' +
      '\n' +
      'Mixture of Expert (MoE) models allow for the inference cost and number of parameters of a model to be decoupled by not activating all parameters on the forward pass and instead routing tokens to specific MLP _experts_. Each expert theoretically specializes in a certain kind of input, and the router (a small neural network) learns which expert to route each token to. Theoretically, this enables the model to maintain almost all the expressivity of the parameter-equivalent dense model at significantly fewer FLOPs.\n' +
      '\n' +
      'In standard implementations [11], which we follow in this paper, the router is a linear layer mapping from tokens to expert indices, and each expert is simply a standard transformer MLP. The expert that the token is routed to is chosen as the top-k of the expert probabilities, where \\(k\\) is a hyperparameter of the architecture. Given an input token to the MoE layer \\(x\\), this is mapped through the router to a probability distribution \\(p_{i}(x)\\), where \\(i\\) labels the experts. Upon selecting the top-\\(k\\) probabilities, the output of the MoE layer \\(y\\) can be expressed, schematically, as,\n' +
      '\n' +
      '\\[y=\\sum_{i\\in\\text{top-}k}c_{i}E_{i}(x) \\tag{5}\\]\n' +
      '\n' +
      'where \\(E_{1},E_{2},\\dots\\) denote the MLP experts,\n' +
      '\n' +
      '\\[E_{i}(x)=W_{\\text{out}}f(W_{\\text{in}}(\\text{LN}(x)) \\tag{6}\\]\n' +
      '\n' +
      'where \\(f\\) is the activation function of the MLP, and \\(c_{i}\\) are coefficients that are often identified with \\(p_{i}\\), the probability output by the router of choosing a specific expert. The optimal method for training the router is still uncertain since the "correct" expert assignment problem is non-differentiable, and MoE models often struggle with training stability and load-balancing between different experts for hardware efficiency. Nevertheless, MoE models have demonstrated the ability to achieve superior performance for a given compute budget over dense transformer models. Lastly, due to complexity of reporting MoE models, where different papers have reported either the forward pass size of the MoE, the total parameters, or both, we here present a consistent convention of denoting MoE models as: \\((\\text{forward parameters})/(\\text{total parameters})\\). For more details on the MoE architecture and its typical implementation, see [16].\n' +
      '\n' +
      '## IV Related Work\n' +
      '\n' +
      '### _State-space Models_\n' +
      '\n' +
      'The quadratic complexity of transformers in the sequence length has long been recognized as a primary bottleneck to extremely long context reasoning and understanding. While recent work has pioneered the concept of context-length extension [18, 19] allowing transformers to be trained at a manageable scale and then inferenced successfully at a significantly longer context, the inference cost in terms of both FLOPs and the memory required for the KV cache remains substantial.\n' +
      '\n' +
      'Early state-space models were inspired by linear dynamical systems which can be efficiently computed as a convolution [17, 20] for sequence processing and as a recurrence for efficient autoregressive generation. However, such models were noticeably less expressive and performant than transformers.\n' +
      '\n' +
      'A number of recent works [14, 21] has aimed to increase the expressivity of the state-space model by using input-dependent gating, similar to the QKV matrices of attention, while maintaining the fundamentally linear nature of the state-space recursion. This thus enables efficient implementation via convolution or selective-scan to be maintained while substantially closing the gap to transformer performance in practice.\n' +
      '\n' +
      'Mamba [9] is a recently released state-space model in line with these previous works which demonstrates strong performance comparable to transformers up to the 2.8B scale, as well as promising scaling laws. Mamba uses input-dependent gating of the inputs to the SSM recursion while maintaining efficient computation via customized selective scan kernels.\n' +
      '\n' +
      '### _Mixture of Experts_\n' +
      '\n' +
      'MoE models have been demonstrated to achieve significantly higher performance in both training and inference per FLOP than the equivalent dense models [11, 12]. Moreover, scaling laws for MoE models have been put forward [22] which show that MoE performance improves smoothly with compute, data, and the number of experts being routed to. This latter is especially important since it provides a route to continually increasing the capability of the model while holding the inference cost fixed.\n' +
      '\n' +
      'While MoE models hold significant promise, the architecture still retains many drawbacks. Increasing the number of experts increases the parameter count and hence memory cost substantially, while many works report MoE models being less stable and more challenging to train. Moreover, effective methods for training the router are still open, since the decision to route to an expert or not is discrete and cannot be easily backpropagated through. The large memory cost of MoEs relative to their dense counterparts is especially important for users running on relatively low-end GPUs or when the memory size extends beyond that provided by a single GPU necessitating model-parallelism for inference.\n' +
      '\n' +
      'Recently, [13] released a powerful open source mixture of experts model which performs competitively with Llama 2 70B [5] and close to GPT-3.5 in evaluations while requiring only the forward pass FLOP cost of the original Mistral 7B model [23], thus demonstrating and solidifying the promise of MoE models at scale. The Mistral architecture also differs in a few ways from earlier MoE work, especially in its use of relatively few experts, a design which we also utilize and have independently found promising for balancing the FLOP and memory cost of MoE models successfully.\n' +
      '\n' +
      '### _State-space models with Mixture of Experts_\n' +
      '\n' +
      'While both state-space models and Mixture of Experts have been proposed as promising architectures able to improve the computational cost of inferencing language models, no works have ever tested their combination at scale.\n' +
      '\n' +
      'Concurrently with this work, [24] demonstrate the performance of extremely small mamba-MoE models in the hundred-million scale of total parameters and the forward pass FLOPs of a 25M model, trained on \\(<\\)10B tokens. In contrast, we demonstrate empirically the scaling potential and performance of such models at meaningful scales in terms of both parameters and data, by training multi-billion parameter models on 300B tokens. Our work thus demonstrates the strong scaling potential of the combination of state-space models and MoE models while resulting in competitive and usable language models which are extremely efficient for inference.\n' +
      '\n' +
      '## V Design\n' +
      '\n' +
      '### _Architecture_\n' +
      '\n' +
      'A standard transformer model [2] consists of interleaved attention and MLP blocks added in sequence along a residual stream. The equation for a single transformer layer is written in Equation 2.\n' +
      '\n' +
      'Most MoE architectures simply replace the MLP blocks with a routed expert layer. Our BlackMamba architecture simply replaces both the MLP layer in a transformer with an expert layer, and the attention layer with a mamba SSM layer (see Figure 1). A single block of our architecture can thus be written as,\n' +
      '\n' +
      '\\[x_{l+1}= x_{l}+\\text{MoE}(\\text{LN}(x_{l}+\\text{mamba}(\\text{LN}(x_{l})))) \\tag{7}\\]\n' +
      '\n' +
      'We trained BlackMamba 340M/1.5B and 630M/2.8B models for 300B tokens on our custom dataset. We used the SwiGLU activation function [25] for the expert MLPs. We trained with 8 experts, a number that we found balanced well the trade-off between the inference cost and memory footprint of the model. We tested whether sequential or parallel [26] blocks performed better and found a slight advantage for sequential. Following [5], we trained without biases. For the expert router, we used top-1 routing with a Sinkhorn routing function to load-balance between experts. We utilized a novel custom version of the Sinkhorn algorithm which converges substantially faster than vanilla Sinkhorn (Appendix F). We trained using the Megatron-LM [27] distributed training framework. The model was trained in bf16 precision. All further model architectures and training hyperparameters are described in Appendix A and B, respectively.\n' +
      '\n' +
      '### _Dataset_\n' +
      '\n' +
      'To train BlackMamba, we constructed a custom dataset comprised of a mixture of existing open-source datasets. The subsets included: The Pile [28], SlimPajama [29], Starcoder [30], PeS2o [31], and ProofPile [32]. The weights for each dataset is provided in Table I. Tokens were sampled without replacement from each of the subsets according to the probability of sampling from a subset upwei\n' +
      '\n' +
      'Fig. 2: Ratio of data categories in the pretraining dataset of BlackMambaThe total dataset comprised 1.8 trillion tokens and thus we trained for significantly less than a single epoch. Preliminary experiments3 show that long-form text and academic work appears to improve natural language modeling when included in the pretraining phase, so we weigh it heavily in the training recipe. Further, we find that including significant portions of code and math during the pretraining phase meaningfully improves the model\'s reasoning ability. We note that this dataset is comparatively heavy on unfiltered web data and contains many duplicates due to the upweighting of smaller subsets, which may limit the quality of the model and leaves significant room for improvement, as well as potentially causing undue memorization of specific common fragments.\n' +
      '\n' +
      'Footnote 3: We believe that such experiments are not yet rigorous enough for publication, and will be included in future work.\n' +
      '\n' +
      '## VI Results\n' +
      '\n' +
      'To ensure a fair comparison vs Mamba, we trained our own 340M Mamba model with the same dataset and training hyperparameters reported for BlackMamba. This Mamba 340M model used a hidden size of 1152 and 34 mamba layers. Notably, BlackMamba performs significantly better than equivalent pretrained models (both transformer and Mamba) for the same forward pass model size at inference time, as well as training FLOPs. In Figure 5, we plot the time taken to autoregressively generate a sequence of a given length starting from an initial one-token prompt as a function of sequence length. We observe that the established latency benefits of both Mamba and MoE models are combined in BlackMamaba to result in inference times significantly faster than canonical transformer models, MoE transformer models, and pure Mamba models. Moreover, the inference advantage of\n' +
      '\n' +
      'Fig. 4: Comparison of BlackMamba average evaluation performance across training FLOPs.\n' +
      '\n' +
      'Fig. 3: Comparison of BlackMamba average evaluation performance across activated forward parameters.\n' +
      '\n' +
      'BlackMamba increases with greater sequence lengths, making BlackMamba extremely competitive at long sequence generation. Moreover, although not reflected in this Figure, it must be recognized that while the transformer inference latency also increases linearly, this is due to KV caching which has additional linearly increasing memory requirements and would eventually OOM on large enough sequences. By contrast, Mamba models (and BlackMamba) can generate sequences of arbitrary length with a constant memory footprint.\n' +
      '\n' +
      'Figures 6 and 7 illustrate the token counts assigned to each expert in each layer of the BlackMamba 340M/1.5B and the BlackMamba 630M/2.8B models respectively. Most layers display a high degree of expert balance, as expected by our improved Sinkhorn algorithm. Yet, intriguingly, both models show a clear transition towards expert imbalance in the final layers (at layer 20 for the 340M/1.5B model and layer 25 for the 630M/2.8B model). This may reflect increasing specialization in later layers or else reflect numerical instabilities that develop deeper in the network. While the true cause of this imbalance remains unknown, we also note that a similar pattern of imbalance but convergence to a stable expert assignment has also been observed in previous MoE models [34].\n' +
      '\n' +
      'In Table I, we report evaluation scores of BlackMamba against a suite of open-source pretrained language model baselines. We re-evaluated all models on the same version of lm-eval (v0.3.0) that we evaluated our own model on*.\n' +
      '\n' +
      'Footnote *: We use the non-normalized HellaSwag evaluation results in this paper, which differs from those in [9]\n' +
      '\n' +
      'In Appendix E, we provide evaluation scores for our model during training from checkpoints taken every 10k steps. We generally found relatively smooth but noisy improvements in the evaluation scores during training. To prevent overfitting to the evaluations, we only looked at the evaluation scores after the models had finished training and did not use them for model selection.\n' +
      '\n' +
      'Additionally, in Appendix F, we describe a novel initialization for the classical Sinkhorn algorithm used for MoE routing which significantly improves convergence speed of the approach, often requiring only a single iteration for convergence. This provides notable speed improvements for the routed expert layers and results in a similar latency to a router with a regularized balancing loss, providing superior balancing performance while requiring much less complexity of implementation.\n' +
      '\n' +
      'Finally, in Appendix C, we provide a detailed mathematical description of the internal computations of a Mamba Block and in Appendix D, we provide detailed and explicit formulas for computing the parameters and training FLOPs for Mamba and MoE models which we hope aid the community in further developing and exploring novel SSM and MoE architectures.\n' +
      '\n' +
      'Fig. 5: Generation latency of BlackMamba compared to dense transformers, dense mamba, and transformer-MoE\n' +
      '\n' +
      '## VII Discussion\n' +
      '\n' +
      'This work is a preliminary exploration and validation of the core concept of combining together recent advances in SSMs with MoEs to produce a highly competitive and efficient architecture both in terms of inference and generation time and training FLOPs. While initial results are promising, much work needs to be done to improve both the SSM and MoE components as well as investigation of the optimal way to approach their combination. We ultimately believe that by exploring promising emerging architectures architectures and novel ways of merging and combining them, significant advances in performance, efficiency, and speed can be obtained over standard transformer recipes.\n' +
      '\n' +
      'We believe that our work can be extended in many fruitful directions. The evaluations presented in this paper are limited in scope. While we provide general coverage of standard pure language modelling evaluations in the zero-shot setting, the performance of the model in the many-shot in-context-learning setting remains unexplored. Additionally, there are many facets of behaviour of our models which we have not explicitly investigated. We have not tested for factual accuracy, profanity, toxicity, or any other socially undesirable text generation. Similarly, our training dataset blend has not been explicitly scraped for socially undesirable tokens, nor its potential overlap with any evaluation tasks4. Although our dataset remains imperfect, we have released all major details as to its construction and composition with the goal\n' +
      '\n' +
      'Fig. 6: Token distribution across experts in 340M/1.5B BlackMamba\n' +
      '\n' +
      'Fig. 7: Token distribution across experts in 630M/2.8B BlackMambaof aiding community understanding of the effects of dataset on pretraining performance and model behaviours.\n' +
      '\n' +
      'In terms of scaling laws, while our models are highly competitive for a given inference cost and FLOP training budget, it is impossible to make conclusive scaling extrapolations both in terms of data and parameter counts with only two models trained on 300 billion tokens. Additionally, many of our training hyperparameters may be suboptimal as we performed only basic hyperparameter tuning of the learning rate. Additionally, while we performed some ablations on the core architecture, it is possible that a superior method of combining state-space models and mixture of experts would provide significant benefits. Additionally, the efficacy and performance of well-established finetuning and RLHF pipelines for instruction following and general alignment, as well as standard techniques for parameter-efficient-finetuning of SSM and MoE models remains almost completely unexplored, as does how such models perform under quantization.\n' +
      '\n' +
      'Our work also raises interesting questions as to the modularity of different neural network components that can be placed together into a final model architecture. We show that it is relatively straightforward to combine SSM blocks with MoE blocks from transformers at scale with competitive performance. However, whether Mamba and other SSMs show the same degree of improvement in performance with MoE as transformers remains uncertain, as well as whether combining these architectural pieces has the same effect on the internal representations and behaviours of the model. Additionally, it is unclear the extent to which routing serves the same function in BlackMamba as in more classical transformer MoE models.\n' +
      '\n' +
      '## VIII Conclusion\n' +
      '\n' +
      'In this paper, we have proposed, implemented and trained BlackMamba, a model that combines both recent advances in state-space models and mixture-of-experts into a single unified architecture. We demonstrate that our BlackMamba architecture performs highly competitively to strong pretrained LLM baselines in terms of inference cost and training flops, and moreover that it inherits the reduced training and generation FLOPs of both SSMs and MoEs simultaneously. Moreover, we show that BlackMamba is capable of rapid generation with both linear time and memory cost. We release BlackMamba 340M/1.5 and 630M/2.8 billion parameter models and intermediate checkpoints, as well as inference code, under a permissive Apache 2.0 license with the goal of enabling and fostering further study, experimentation, and understanding of the potential of this novel architecture by the broader community.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      'The Zyphra team would like to thank Adam Ibrahim for helpful discussions and comments on training stability and hyperparameters, and Albert Gu for general discussions on state space models.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] D. Bahdanau, K. Cho, and Y. Bengio, "Neural machine translation by jointly learning to align and translate," _arXiv preprint arXiv:1409.0473_, 2014.\n' +
      '* [2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," _Advances in neural information processing systems_, vol. 30, 2017.\n' +
      '* [3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever _et al._, "Language models are unsupervised multitask learners," _OpenAI blog_, vol. 1, no. 8, p. 9, 2019.\n' +
      '* [4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell _et al._, "Language models are few-shot learners," _Advances in neural information processing systems_, vol. 33, pp. 1877-1901, 2020.\n' +
      '* [5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale _et al._, "Llama 2: Open foundation and fine-tuned chat models," _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [6] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly _et al._, "An image is worth 16x16 words: Transformers for image recognition at scale," _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* [7] K. Rasul, A. Ashok, A. R. Williams, A. Khrossani, G. Adamopoulos, R. Bhagwatkar, M. Bilos, H. Ghonia, N. V. Hassen, A. Schneider _et al._, "Lag-llama: Towards foundation models for time series forecasting," _arXiv preprint arXiv:2310.08278_, 2023.\n' +
      '* [8] S. Reed, K. Zohna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg _et al._, "A generalist agent," _arXiv preprint arXiv:2205.06175_, 2022.\n' +
      '* [9] A. Gu and T. Dao, "Mamba: Linear-time sequence modeling with selective state spaces," _arXiv preprint arXiv:2312.00752_, 2023.\n' +
      '* [10] B. Peng, E. Aleaide, O. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K. GV _et al._, "Rwkv: Reinventing rnns for the transformer era," _arXiv preprint arXiv:2305.13048_, 2023.\n' +
      '* [11] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," _The Journal of Machine Learning Research_, vol. 23, no. 1, pp. 5232-5270, 2022.\n' +
      '* [12] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A. Awan, J. Rasley, and Y. He, "Deepsped-more: Advancing mixture-of-experts inference and training to power next-generation ai scale," in _International Conference on Machine Learning_. PMLR, 2022, pp. 18 332-18 346.\n' +
      '* [13] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaptol, D. d. I. Casas, E. B. Hanna, F. Bressand _et al._, "Mirxtl of experts," _arXiv preprint arXiv:2404.04088_, 2024.\n' +
      '* [14] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei, "Retentive network: A successor to transformer for large language models (2023)," _URL [http://arxiv.org/abs/2307.08621](http://arxiv.org/abs/2307.08621) v1_.\n' +
      '* [15] D. Leipkhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, "Gshard: Scaling giant models with conditional computation and automatic handling," _arXiv preprint arXiv:2006.16668_, 2020.\n' +
      '* [16] W. Fedus, J. Dean, and B. Zoph, "A review of sparse expert models in deep learning," _arXiv preprint arXiv:2209.01667_, 2022.\n' +
      '* [17] A. Gu, K. Goel, and C. Re, "Efficiently modeling long sequences with structured state spaces," _arXiv preprint arXiv:2110.00396_, 2021.\n' +
      '* [18] B. Peng, J. Quesselle, H. Fan, and E. Shippole, "Yarm: Efficient context window extension of large language models," _arXiv preprint arXiv:2309.00071_, 2023.\n' +
      '* [19] S. Chen, S. Wong, L. Chen, and Y. Tian, "Extending context window of large language models via positional interpolation," _arXiv preprint arXiv:2306.15595_, 2023.\n' +
      '* [20] M. Poli, S. Massarooli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. Re, "Hyena hierarchy: Towards larger convolutional language models," _arXiv preprint arXiv:2302.10866_, 2023.\n' +
      '* [21] S. Arora, S. Eyuboglu, A. Timalasin, I. Johnson, M. Poli, J. Zou, A. Rudra, and C. Re, "Zoology: Measuring and improving recall in efficient language models," _arXiv preprint arXiv:2312.04927_, 2023.\n' +
      '* [22] A. Clark, D. De Las Casas, A. Guy, A. Mensch, M. Paganini, J. Hoffmann, B. Damoc, B. Hechtman, T. Cai, S. Borgeaud _et al._, "Unified scaling laws for routed language models," in _International Conference on Machine Learning_. PMLR, 2022, pp. 4057-4086.\n' +
      '* [23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. I. Casas, F. Bressand, G. Langley, G. Lample, L. Saulnier _et al._, "Misrtl 7b," _arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* [24] M. Pioro, K. Ciebiera, K. Krolj, J. Ludriejewski, and S. Jaszczur, "Moemabmba: Efficient selective state space models with mixture of experts," _arXiv preprint arXiv:2401.04081_, 2024.\n' +
      '* [25] N. Shazeer, "Glu variants improve transformer," _arXiv preprint arXiv:2002.05202_, 2020.\n' +
      '* [26] B. Wang and A. Komatuszaki, "Gpt-j-6b: A 6 billion parameter autoregressive language model," 2021.\n' +
      '* [27] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-lm: Training multi-billion parameter language models using model parallelism," _arXiv preprint arXiv:1909.08053_, 2019.\n' +
      '* [28] L. Gao, S. Biderman, S. Black, L. Goldling, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima _et al._, "The pile: An 800gb dataset of diverse text for language modeling," _arXiv preprint arXiv:2101.00027_, 2020.\n' +
      '* [29] D. Soboleva, F. Al-Khateeb, R. Myers, J. Steeves, J. Hestness, and N. Dey, "Simpliamaz: A 627b token cleaned and dedupleted version of redjaguima," 7 2023. [Online]. Available: [https://www.corebras.net/blog/simplajama-a-627b-token-cleaned-and-dedupleted-version-of-redpajama](https://www.corebras.net/blog/simplajama-a-627b-token-cleaned-and-dedupleted-version-of-redpajama)\n' +
      '* [30] R. Li, L. B. Allal, Y. Zi, N. Muenninghoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Lim _et al._, "Starcoder: may the source be with you!" _arXiv preprint arXiv:2305.06161_, 2023.\n' +
      '* [31] L. Soldaini and K. Lo, "peS2o (Pretraining Efficiently on S2ORC) Dataset," Allen Institute for AI, Tech. Rep., 2023, oDC-By, [https://github.com/allenai/peseo2o](https://github.com/allenai/peseo2o).\n' +
      '* [32] Z. Azerabyrev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and S. Welleck, "Llemma: An open language model for mathematics," _arXiv preprint arXiv:2310.10631_, 2023.\n' +
      '* [33] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap, "Compressive transformers for long-range sequence modelling," 2019.\n' +
      '* [34] J. He, J. Zhai, T. Antunes, H. Wang, F. Luo, S. Shi, and Q. Li, "Faster-moe: modeling and optimizing training of large-scale dynamic pre-trained models," in _Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming_, 2022, pp. 120-134.\n' +
      '* [35] Y. Elazar, A. Bhagia, I. Magnusson, A. Ravichander, D. Schwenk, A. Suhr, P. Walsh, D. Groeneveld, L. Soldaini, S. Singh, H. Hajishirzi, N. A. Smith, and J. Dodge, "What\'s in my big data?" 2023.\n' +
      '* [36] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPoffi, C. Foster, L. Golding, J. Hsu, A. Le Noac\'h, H. Li, K. McDonell, N. Muenninghoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutwika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, "A framework for few-shot language model evaluation," 12 2023. [Online]. Available: [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)\n' +
      '* [37] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, "Hellaswag: Can an machine really finish your sentence?" 2019.\n' +
      '* [38] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, "Piqa: Reasoning about physical commonsense in natural language," 2019.\n' +
      '* [39] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, "Winogrande: An adversarial winograd schema challenge at scale," 2019.\n' +
      '* [40] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fernandez, "The lambda dataset: Word prediction requiring a broad discourse context," 2016.\n' +
      '* [41] P. Clark, I. Cowbe, O. Etzioni, T. Kh\n' +
      '\n' +
      '### _Model Hyperparameters_\n' +
      '\n' +
      '### _Manba Block Internals_\n' +
      '\n' +
      'In this appendix, we provide a precise and detailed walk-through of the core computations that comprise a Mamba block. Mamba derives from a line of work on state-space models, which are expressive recurrent models which have recently been shown capable of competing with transformers on large scale sequence modelling. The recurrence of these models enables them to be used efficiently for generation without a KV cache and causes them to scale in FLOPs and memory linearly in the sequence length. The core insight is to utilize recurrence [17] or selective scan [9] to efficiently map the central recurrence to parallel GPU hardware. The base of all such models is the following state-space equations (in continuous time):\n' +
      '\n' +
      '\\[\\frac{dh}{dt} =A\\,h+B\\,x \\tag{8}\\] \\[y =C\\,h \\tag{9}\\]\n' +
      '\n' +
      'which define a classical linear time-invariant dynamical system. Here \\(h\\) denotes the state of a system at one instant. \\(A\\) denotes a matrix which governs the \'natural dynamics\' of \\(h\\) over time. \\(x\\) denotes a \'control\' input to the system - i.e. one provided by the controller or experimenter and \\(B\\) denotes a dynamics matrix which controls how \\(x\\) interacts with system. Finally, the states are transformed into \'observations\', denoted \\(y\\), through the observation matrix denoted \\(C\\).\n' +
      '\n' +
      'The Mamba block utilizes this dynamical system across tokens as its core computation implemented as a hardware efficient selective scan. The innovation of Mamba specifically is to make the \\(A\\),\\(B\\),and \\(C\\) matrices a linear function of the input \\(x\\), analogous to the \\(Q\\),\\(K\\),\\(V\\) matrices of a self-attention block. Beyond this, Mamba wraps the SSM component in a linear projection to and from the residual stream and a convolution of the input, as well as an additional gating projection path which gates the output of the SSM based on a projection of the input to the block.\n' +
      '\n' +
      'We denote the input to the Mamba block \\(x\\), the recurrent hidden state \\(h\\), the sequence length as \\(l\\). We set the hidden recurrent state dimension to some factor of the input dimension.\n' +
      '\n' +
      'The Mamba block contains matrices \\(A\\) which defines the dynamics for the recurrent state, \\(B\\) which is the projection for the inputs, \\(C\\) which is the projection to the outputs \\(y\\), the matrix \\(D\\) which is a learnable bias on the output, a discretization timestep \\(dt\\), and a gating vector \\(z\\). The Mamba block also performs a linear projection of the input x and z prior to the SSM with weight matrices \\(W_{x}\\) and \\(W_{z}\\) and an output projection matrix \\(W_{y}\\).\n' +
      '\n' +
      'The computation inside a Mamba block runs as follows. First, the \\(x\\) and \\(z\\) projections are computed. This projection occurs for every token in the sequence independently.\n' +
      '\n' +
      '\\[x =W_{x}\\,x \\tag{10}\\] \\[z =W_{z}\\,z \\tag{11}\\]\n' +
      '\n' +
      'Secondly, after the projection, the Mamba block performs a 1d convolution (\\(*\\)) across the input sequence embeddings. This convolution cannot be merged with the projection \\(W_{x}\\) because this projection acts at the embedding level, and the convolution is acting at the sequence of tokens level.\n' +
      '\n' +
      '\\[x_{t}=W_{filter\\_\\_}*x_{t} \\tag{12}\\]\n' +
      '\n' +
      'The input-dependent \'weights\' \\(B\\), \\(C\\), and \\(dt\\) can then be computed, which are analogous to the Query, Key, and Value weights in attention.\n' +
      '\n' +
      '\\[B =W_{B}\\,x \\tag{13}\\] \\[C =W_{C}\\,x\\] (14) \\[dt =W_{D}\\,x \\tag{15}\\]\n' +
      '\n' +
      'The matrix \\(A\\) is trained with a special initialization given in the matrix below. Note that updates are trained via the parameterization \\(\\ln(A)\\), presumably to make \\(A\\) positive and to improve stability, and then computed as \\(A=\\exp(\\ln(A)\\,)\\).\n' +
      '\n' +
      '\\[A=\\begin{bmatrix}1&2&3&\\cdots\\\\ 1&2&3&\\cdots\\\\ \\vdots&&&\\end{bmatrix} \\tag{16}\\]The weights are then discretized prior to use in the SSM kernel. Note that the discretization for B does not follow Equation 4 in [9].\n' +
      '\n' +
      '\\[dt =\\text{softplus}(dt+dt_{\\text{bias}}) \\tag{17}\\] \\[dA =\\exp(-A\\,dt)\\] (18) \\[dB =B\\,dt \\tag{19}\\]\n' +
      '\n' +
      'A single step of the ssm is then performed to obtain the new recurrent state. Note that \\(h^{+}\\to h\\) when \\(dt\\to 0\\), as expected\n' +
      '\n' +
      '\\[h^{+}=dA\\,h+dB\\,x \\tag{20}\\]\n' +
      '\n' +
      'From the new recurrent state, the output \\(C\\,h^{+}\\) can be computed. This output is also gated by the learnt gating vector z and passed through a final output projection before being addded back into the residual stream.\n' +
      '\n' +
      '\\[y =C\\,h^{+}+D\\,x \\tag{21}\\] \\[y =\\text{silu}(z)\\,y\\] (22) \\[y =W_{y}\\,y \\tag{23}\\]\n' +
      '\n' +
      'The output of the SSM block is then the hidden state \\(h^{+}\\) and the output \\(y\\).\n' +
      '\n' +
      'A Mamba block can operate in two modes. The first mode is the recurrent method, which directly follows the steps described here. This approach is linear in both memory and computational cost for a single step since it only utilizes the recurrent state to predict the next token. The second way is to run the SSM across the whole sequence at once using the\'selective scan\' operation and kernel introduced by [9]. For further reference on the implementation of the selective scan refer to [9].\n' +
      '\n' +
      '### _Computing Parameters and FLOPs for Mamba-MoE_\n' +
      '\n' +
      'Let us denote the embedding dimension \\(D\\), the Mamba inner state as \\(I\\), the recurrent state dimension \\(H\\), the dt rank \\(dt\\) and the convolution dimension \\(C\\). We denote the batch size \\(B\\) and the sequence length \\(L\\).\n' +
      '\n' +
      'The number of parameters in a Mamba block can then be computed as,\n' +
      '\n' +
      '\\[\\underbrace{3ID}_{W_{x},W_{z},W_{y}}+2I(\\underbrace{H}_{W_{A},W_{B}}+ \\underbrace{dt}_{W_{dt}}+\\underbrace{\\frac{C}{2}}_{\\text{conv}})+\\underbrace{ I}_{D}+\\underbrace{2D}_{\\text{layernorm}} \\tag{25}\\]\n' +
      '\n' +
      'The number of parameters in a MoE block can be computed as\n' +
      '\n' +
      '\\[\\underbrace{8D^{2}E}_{\\text{experts}}+\\underbrace{DE}_{\\text{router}} \\tag{26}\\]\n' +
      '\n' +
      'Where \\(E\\) is the number of experts in the layer. For a network of \\(L\\) layers, there are thus \\(\\frac{L}{2}\\) Mamba blocks and \\(\\frac{L}{2}\\) MoE blocks.\n' +
      '\n' +
      'To begin approximating the number of FLOPs involved in a single Mamba block, we make the following observation.\n' +
      '\n' +
      'Given two matrices \\(A\\in\\mathcal{R}^{K\\times M}\\) and \\(B\\in\\mathcal{R}^{M\\times J}\\), then the total FLOPs involved in the matrix product \\(AB\\) is approximately \\(2KMJ\\), where the factor of \\(2\\) arises from the fact that matrix multiplication requires both a multiply and an add operation. In the following calculations, we assume that the matrix multiplications dominate the total FLOP count of the model and hence ignore the nonlinearities, layernorms, and other computations.\n' +
      '\n' +
      'First, let us consider the projection operation involving the weights \\(W_{x}\\),\\(W_{z}\\), and \\(W_{y}\\). All are of shape \\(I\\times D\\) and hence the total FLOPs for these are \\(6IDLB\\).\n' +
      '\n' +
      'There is also the convolution which can be treated as a single \\(I\\times C\\) matrix multiply requiring \\(2ICLB\\) FLOPs.\n' +
      '\n' +
      'Now, we turn to the SSM block itself. We first compute the input-dependent \\(B\\) and \\(C\\) matrices requiring a matrix multiply of shape \\(I\\times H\\) each thus resulting in \\(4IH\\) FLOPs. The \\(A\\) matrix is not multiplied by the input but goes through an elementwise transform costing \\(IH\\) FLOPs. The \\(dt\\) projection first goes through an elementwise operation of order \\(I\\) FLOPs.\n' +
      '\n' +
      'Next, the discretization. The \\(A\\) matrix is multiplied by the \\(dt\\) vector resulting, costing \\(IH\\) FLOPs. The \\(B\\) matrix is multiplied by the input costing \\(2IH\\) FLOPs. The SSM linear state space step itself is just a matrix multiply and add so costs \\(2IH\\) FLOPs, and then the output projection using the \\(C\\) matrix also costs \\(2IH\\) FLOPs. Putting this all together, we obtain the following expression,\n' +
      '\n' +
      '\\[\\begin{split}& BLI(\\underbrace{11H}_{W_{x},W_{z},W_{y},\\text{ SSM}}+\\underbrace{4dt}_{\\text{d proj. discretization}}+\\underbrace{1}_{\\text{dt nonlinearity}})+\\underbrace{IH}_{A}\\end{split} \\tag{27}\\]\n' +
      '\n' +
      'The MoE blocks consist of \\(E\\) standard mlp blocks and a router. The FLOPs for each mlp block is simply \\(16D^{2}\\) since there are two weight matrices of shape \\(4D\\times D\\), and a multiply and add per matrix multiply. The router cost is simply \\(2DE\\). Putting this together, we obtain \\(DE(16D+2)\\) FLOPs for an MoE block.\n' +
      '\n' +
      '### _Evaluations During Training_\n' +
      '\n' +
      'We evaluate BlackMamba on a suite of eight diverse evaluation tasks in the zero-shot setting. We use the EleutherAI evaluation harness (version 0.3.0) [36]. Specifically, we evaluate our models on the HellaSwag [37], PIQA [38], WinoGrande [39], Lambada [40], ARC [41] (both the easy and challenge versions), and OpenBookQA [42]. The evaluations were run on model checkpoints taken every \\(10,000\\) steps. We observe that most evaluation metrics appear to increase smoothly but noisily throughout training, before appearing to plateau towards their final values. This is broadly in line with previous findings in the Pythia model suite [43], which find relatively smooth improvements across training in many of their evaluation metrics. This provides some evidence that the development of capabilities in language models occurs smoothly and can be tracked during training and perhaps predicted ahead of time. Two evaluation metrics, however, WinoGrande and BoolQ, violate this trend for reasons that we do not currently understand. We note that [43] also observe \n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:12]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>