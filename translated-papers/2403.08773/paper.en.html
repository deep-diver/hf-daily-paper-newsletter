<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Veagle: Advancements in Multimodal Representation Learning\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Lately, researchers in artificial intelligence have been really interested in how language and vision come together, giving rise to the development of multimodal models that aim to seamlessly integrate textual and visual information. Multimodal models, an extension of Large Language Models (LLMs), have exhibited remarkable capabilities in addressing a diverse array of tasks, ranging from image captioning and visual question answering (VQA) to visual grounding. While these models have showcased significant advancements, challenges persist in accurately interpreting images and answering the question, a common occurrence in real-world scenarios. This paper introduces a novel approach to enhance the multimodal capabilities of existing models. In response to the limitations observed in current Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs), our proposed model Veagle, incorporates a unique mechanism inspired by the successes and insights of previous works. Veagle leverages a dynamic mechanism to project encoded visual information directly into the language model. This dynamic approach allows for a more nuanced understanding of intricate details present in visual contexts. To validate the effectiveness of Veagle, we conduct comprehensive experiments on benchmark datasets, emphasizing tasks such as visual question answering and image understanding. Our results indicate a improvement of 5-6 % in performance, with Veagle outperforming existing models by a notable margin. The outcomes underscore the model\'s versatility and applicability beyond traditional benchmarks. Furthermore, we make our code and models openly accessible to the research community, fostering collaboration and further exploration in the evolving landscape of multimodal AI. The code repository, along with detailed documentation, can be found at [https://github.com/superagi/Veagle](https://github.com/superagi/Veagle)\n' +
      '\n' +
      'Rajat Chawla*Anmol Gautam Ayush Vatsal Ayush Vatsal Sukrit Chaterjee Mukunda NS+Adarsh Jha Ishaan Bhola SuperAGI\n' +
      '\n' +
      'Footnote †: These authors contributed equally.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'In recent years, the surge of interest in Large Language Models (LLMs) has reshaped the landscape of natural language understanding, a significant surge in the examination and application of Multimodal Large Language Models (MLLMs) has been observed. Allowing models to harness various modalities such as text, images, videos, and voice, MLLMs have become vital in the creation of adaptable all-purpose assistants. Despite their impressive generalization abilities across a wide spectrum of tasks and the development of Vision Language Models (VLMs) which incorporate LLMs with visual understanding competence, contemporary models encounter challenges in interpreting embedded text within images. This limitation is the focal point of this research as images inclusive of text are prevalent in our everyday lives and comprehending such content is imperative for human visual perception.\n' +
      '\n' +
      'Our research presents a new way of doing things by combining learned query embeddings with additional visual assistance. This method uses encoded patch embeddings to deal with the limitations of information that language models typically get from images. As a result, it enhances how well a model can understand and perceive the relationship between text and images. Our model, called Veagle, starts by using a pre-trained vision encoder and language model. We train it in two stages to avoid forgetting what it already knows and make training less complicated, ultimately making the model more effective. We tested the model using standard Visual Question-Answering (VQA) benchmarks and protocols for evaluating images with a lot of text. Our Veagle model significantly improves the understanding and perception of the relationship between text and images, outperforming traditional benchmarks in addressing the challenges of comprehending embedded text within images.\n' +
      '\n' +
      'In this research, we present Veagle, an innovative model that represents a significant leap forward in the field of multimodal learning and interpretation. At the heart of Veagle is the incorporation of an enhanced version of the BLIVA [1] architecture, where cutting-edge components synergize to amplify its capabilities. Notably, we integrate a superior vision abstractor sourced from mPlugOwl[2], enhancing the model\'s visual processing capabilities. This vision abstractor, combined with Q-Former from InstructBLIP[3] and Mistral[4], a Large Language Model (LLM), creates a powerful synergy, resulting in a substantial improvement in the overall accuracy of the model. A crucial aspect of our methodology is the inclusion of a vision encoder, meticulously trained by mPlugOwl[2]. This encoder plays a pivotal role in extracting high-level visual features from images, thereby enabling Veagle to capture essential visual information for accurate interpretation. This vision encoder is trained to extract high-level visual features from images, allowing the model to capture important visual information for accurate interpretation. Veagle distinguishes itself by seamlessly combining Mistral\'s exceptional language understanding with the vision abstractor, resulting in a comprehensive model that effectively integrates both textual and visual information. The proficiency of Mistral in language comprehension significantly enhances Veagle\'s overall performance. Our methodology places strong emphasis on the use of a meticulously curated dataset, carefully selected for both pre-training and fine-tuning stages. This dataset serves as a foundation for shaping the model\'s understanding, ensuring robust generalization across different scenarios. Our results show that Veagle has a better grasp of understanding text within images. This is backed up by its impressive performance on standard Visual Question Answering (VQA) tests. Veagle not only outperforms existing models but also establishes a new benchmark for accuracyand efficiency. In conclusion, Veagle represents a cutting-edge model that not only incorporates advanced components but also benefits from the enriching inclusion of curated open sources data, making it a pioneering solution in the evolving landscape of multimodal AI research.\n' +
      '\n' +
      'The rest of the paper is organized as follows. Section 2 presents the literature review. 3 highlights the proposed architecture and section 4 includes details of the experiments performed and discusses the results. This is followed by the conclusion in Section 5.\n' +
      '\n' +
      '## 2 Literature Survey\n' +
      '\n' +
      'In this section, we delve into the related work on large language models and multimodal large language models.\n' +
      '\n' +
      '### Llm\n' +
      '\n' +
      'Language models (LLMs) have revolutionized the field of natural language processing (NLP), providing capabilities ranging from text prediction to generating coherent and contextually relevant text. In the ever-evolving realm of natural language processing, Large Language Models (LLMs) have undergone a fascinating journey, leaving an indelible mark on the field. The early contributions of trailiblizing models like GPT-2[5] and BERT[6] acted as pillars, demonstrating the immense potential that arises from training on vast web-scale text datasets. These models not only laid the groundwork for Natural Language Processing (NLP) but also served as catalysts for subsequent advancements. Among the notable milestones is the monumental GPT-3[7], a model that not only shattered size records but also showcased unparalleled performance in in tackling intricate challenges. With a staggering 175 billion parameters, GPT-3[7] emerged as a powerhouse, excelling in a diverse array of language tasks. Its introduction prompted a re-examination of the limits of model size and sparked renewed interest in the applications and challenges inherent in handling colossal language models. The journey did not conclude with GPT-3[7], instead, subsequent models like GPT-4[8] and companions like Megaton-turing NLG[9], PalML[10], Gopher[11], Chinchillia[12], OPT[13], and BLOOM[14] emerged, pushing the boundaries even further. These models, each with unique architectures, training methodologies, and applications, contribute to a dynamic tapestry of research in the expansive domain of large language models. This diversity underscores the ongoing efforts to optimize performance, efficiency, and generalization across an array of linguistic tasks. Recent strides in LLMs have been marked by a nuanced focus on refining models to seamlessly align with human instructions and feedback. Pioneering models such as InstructGPT [15], ChatGPT[16], and the latest iteration, GPT-4[8], stand out as exemplars in this regard. They possess the ability to engage in dynamic, contextually rich conversations, skillfully respond to user prompts, and demonstrate proficiency in intricate tasks such as code generation. These subsequent advancements in LLMs led to the emergence of multimodal large language models, which sought to integrate visual information into the text-based language models This emphasis on harmonizing LLMs with human interaction and instruction signifies a pivotal step toward their practical deployment and integration into real-world applications.\n' +
      '\n' +
      '### Multimodal Large Language Models (MLLMs)\n' +
      '\n' +
      'In the dynamic landscape of multimodal language models (MLLMs), a paradigm shift is evident as researchers harness the progress of Large Language Models (LLMs) to transcend traditional linguistic boundaries. Building upon the foundations laid by VisualGPT [17], Frozen [18], Flamingo [19], BLIP2 [20], and other pioneering studies, MLLMs have evolved to profciently tackle an expanding spectrum of vision-language tasks. These tasks include image captioning, visual question answering (VQA), and bounding box generation, showcasing the robust visual grounding capability inherent in MLLMs. Notably, recent endeavors such as IntructBLIP [3], LLVA [21, 22], mPlugOwl [2], and BLIVA actively contribute to diversifying the repertoire of tasks that MLLMs adeptly address. Beyond the conventional scope, ongoing research delves into the realm of multimodal instruction tuning, with endeavors like LLaVA[21], InstructBLIP[3], Otter[23], mPLUG-Owl[2] and LLaVA-1.5[22] pioneering advancements in this domain. Despite the ongoing exploration of model architecture and training pipelines, the landscape remains open for innovative solutions. The integration of multimodal information into language models has brought about significant advancements in their performance, efficiency, and generalization across various linguistic tasks.\n' +
      '\n' +
      '## 3 Proposed Framework\n' +
      '\n' +
      '### Architecture Overview\n' +
      '\n' +
      '#### 3.1.1 Image Encoder\n' +
      '\n' +
      'A visual encoder is a crucial component of a multimodal models. Visual encoders help the model to extract meaningful representations from visual data. This enables the model to understand the semantics and context of the images, which is important for making accurate predictions or generating relevant outputs. In our experiments, we have adopt a vision encoder(ViT-L/14[24]) from mPlugOwl[2]. This encoder is responsible for extracting meaningful representations from the input images. mPlugOwl[2] has used a novel training paradigm that incorporates a trainable visual encoder, while maintaining the pre-trained language model in a frozen state. This approach enables the model to effectively capture both low-level and higher semantic visual information and align it with the pre-trained language. They have utilize the image-caption pairs from sev\n' +
      '\n' +
      'Figure 1: _Veagle Model Architecture: The visual abstractor is responsible for extracting instruction-aware visual features from the output embeddings of the frozen image encoder. Subsequently, these visual features are provided as soft prompts to the frozen Language Model (LLM). The model is then fine-tuned with the language modeling loss to generate the desired response._\n' +
      '\n' +
      'eral datasets, including LAION-400M[25], COYO-700M[26], Conceptual Captions[27] and MSCOCO[28]. model without compromising its performance.\n' +
      '\n' +
      '#### 3.1.2 Visual Abstractor\n' +
      '\n' +
      'A visual abstractor serves as a bridge between the visual encoder and the language decoder, enabling the model to effectively process and utilize visual information alongside text, leading to more powerful and versatile multimodal models. It focuses on extracting essential visual features from the encoded image representations obtained by the image encoder. Large Language Models (LLMs) undergo pretraining primarily on textual corpora, presenting a limitation in their innate ability to process image features extracted from Vision Encoders. Addressing this gap, the introduction of the QFormer module in BLIP-2[20] emerged as a critical intermediary, serving to establish a bridge between Vision Encoder and Language Model. Then came BLIVA[1], a groundbreaking combination of BLIP2[20] and LLaVA[22]. However, a linear projection layer have very limited capability in capturing all the information required for LLM. To overcome the limitations of projection layers in capturing all the necessary information for LLM, we have introduced a multi layer perceptron along with QFormer[20]. In particular, 1 illustrates that our mode generates the embeddings from vision encoder and the output is passed through the projection layer to the Q-former and the second projection layer. The output from the QFormer[20] and Projection layer is concatinated and passed to the LLM which enable better alignment between vision encoders and language models.\n' +
      '\n' +
      '#### 3.1.3 Llm\n' +
      '\n' +
      'At the heart of multimodal large language models is the Large Language Model (LLM), which serves as the keystone. It takes in instructions and aligned image features, processing this information to generate corresponding answers. In our research, we leverage the capabilities of the many different robust open-source large language models ultimately settling on Mistral[4] due to its superior performance. Mistral 7B surpasses the performance of the leading open 13B model (Llama 2[29]) across all benchmarks and outperforms the best released 34B model (Llama 1[29]) specifically in reasoning, mathematics, and code generation tasks. Mistral achieves faster inference through the innovative use of grouped-query attention (GQA) and effectively manages sequences of arbitrary length with reduced inference cost by incorporating sliding window attention (SWA). This combination of advanced techniques positions Mistral 7B as a leading model in the domain, setting new standards for both accuracy and computational efficiency.\n' +
      '\n' +
      '### Training Scheme\n' +
      '\n' +
      'The training scheme consists of two stages: Pretraining and Fine-tuning. Figure 4 illustrate our training paradigm.\n' +
      '\n' +
      '#### 3.2.1 Stage 1: Pre-training\n' +
      '\n' +
      '1. In this crucial pre-training stage, the Large Language Model (LLM) is aligned with a visual encoder using image-text pairs from image captioning datasets, facilitating a comprehensive understanding of visual content. The focus is on training the projection layers, refining the mapping of visual and textual information. Throughout this phase, the Vision Encoder, Q-former, and LLM remain frozen, preserving their pre-existing knowledge for subsequent fine-tuning.\n' +
      '\n' +
      '#### 3.2.2 Stage 2: Finetuning\n' +
      '\n' +
      'Following pre-training, the Large Language Model (LLM) gains familiarity with the visual embedding space, allowing it to generate image descriptions. However, it lacks the ability to understand finer image details and respond effectively to human queries. In this work, we collect publicly available datasets, COCO, TextCaps, VQAv2, OK-VQA, AOK-VQA, GQA, OCR-VQA, TextVQA, VIZWiz and our in-house curated data. During this phase, the Large Language Model (LLM) and Vision Encoder remain in a frozen state, while the remainder of the model undergoes fine-tuning.\n' +
      '\n' +
      '## 4 Experimental Overview\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      'For datasets featuring single-word answers, we adopted an innovative approach by expanding these responses into detailed and nuanced answers utilizing the advanced capabilities of GPT-4[8] and Mistral[30]. This strategic enhancement contributed to the overall effectiveness of our model, ensuring a more robust and comprehensive understanding of various query types. Addressing the challenge of repeated questions present in certain datasets, we took proactive measures to enhance the\n' +
      '\n' +
      'Figure 4: _Overview of Veagle training paradigm_\n' +
      '\n' +
      'Figure 3: _Fine-tuning Loss Insights_\n' +
      '\n' +
      'Figure 2: _Pre-training Loss Insights_\n' +
      '\n' +
      'diversity and quality of our training dataset. By generating various different questions that incorporated a wide range of distinct questions, we effectively mitigated redundancy and enriched the training dataset, thereby fostering improved generalization and performance in handling diverse queries. This meticulous process of dataset augmentation and refinement played a pivotal role in optimizing the overall performance and reliability of our model. The careful compilation, filtering, and augmentation of diverse datasets played a crucial role in maximizing the performance and reliability of our model.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      'Our experimental results demonstrate the effectiveness of our approach, with significantly improved performance across various datasets.\n' +
      '\n' +
      '#### 4.2.1 Baseline vs Proposed Protocol\n' +
      '\n' +
      'We used four advanced baseline models BLIVA[1], instructBLIP[3], mPlugOwl[2], and LLAVA[22] for our analysis. For each of these models, we took an image and a question, input them into the model, and noted down the response it gave. To evaluate the precision of the provided responses, we employed GPT-4[8] as our assessment model. This model categorized the answers into two distinct classifications: either correct or incorrect. The accuracy outcomes corresponding to each dataset for various different models, obtained through the utilization of this evaluation method, are comprehensively presented in Table 1. Our proposed model achieved an impressive level of accuracy when compared to other open sourced baseline models.\n' +
      '\n' +
      '#### 4.2.2 In-House Test Dataset\n' +
      '\n' +
      'To assess how well our model performs in different scenarios and if it generalizes effectively, we created an in-house test dataset. This dataset comprises various types of tasks, including captioning, optical character recognition (OCR), general visual question-answering (VQA), technical VQA, and reasoning VQA. Importantly, our model has never encountered this specific dataset during its training process. Subsequently, we conducted thorough evaluations of all the models using this test dataset, and the outcomes are promising. Detailed results are presented in Table 2.\n' +
      '\n' +
      '#### 4.2.3 Qualitative Annlysis\n' +
      '\n' +
      'In this section, we present the qualitative outcomes derived from our assessment set. This set of evaluations was carefully curated to analyze the model\'s performance on intricate and challenging tasks. The tasks were selected and collected by our team for the purpose of understanding the model\'s effectiveness beyond numerical measures, delving into the nuanced aspects of its performance. Figure 5 is showing the effectiveness of our model. More examples are given in 7\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'In conclusion, the Veagle multi-modal model stands out as a formidable contender, consistently outperforming established benchmarks in diverse domains. Through the strategic fusion of various modules curated from extensive research, Veagle showcases remarkable performance, not only meeting but exceeding the expectations set by existing models. However, our work also reveals areas that still require refinement, emphasizing the on\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline  & **Veagle** & **BLIVA** & **InstrickBLIP** & **mPlugOwl** & **LLAVA** \\\\ \\hline ok,vqa & **49.3** & 43.4 & 30.8 & 34.1 & 46.2 \\\\ ocr,vqa & 48.3 & 38.5 & 32.1 & 61.4 & **67.2** \\\\ scienceQA & **58.1** & 16.1 & 40.2 & 51.8 & 56.5 \\\\ coco,caption & 57.9 & 56.4 & 51.2 & 55.6 & **62.7** \\\\ ai2diagram & **56.3** & 50.8 & 31.9 & 48.5 & 50.9 \\\\ chart,qa & **13.4** & 13.2 & 3.4 & 10.2 & 3.1 \\\\ gua & **44.2** & 28.6 & 40.8 & 33.9 & 43.9 \\\\ text\\_vqa & 22.5 & 23.1 & 20.5 & 32.6 & **37.2** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Performance of the proposed model for different open sourced datasets.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline  & **Veagle** & **BLIVA** & **InstrickBLIP** & **mPlugOwl** & **LLAVA** \\\\ \\hline Test Data & **76.4** & 63.1 & 59.3 & 68.6 & 66.5 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Performance of our proposed model Veagle for our in-house test dataset.\n' +
      '\n' +
      'Figure 5: Qualitative examples produced by our Veagle model showcase a spectrum of its diverse capabilities. These demonstrations include intricate visual scene understanding and reasoning, multi-turn visual conversation, and more.\n' +
      '\n' +
      'going nature of the pursuit for perfection. This acknowledgment underscores the need for further exploration and optimization, recognizing that the path to excellence in multi-modal models like Veagle continues to unfold. As we navigate this landscape, Veagle remains a promising catalyst for future advancements in Vision-Language Models, becoming further investigation and innovation in this dynamic field.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] W. Hu, Y. Xu, Y. Li, W. Li, Z. Chen, and Z. Tu, "Bliva: A simple multimodal llm for better handling of text-rich visual questions," 2023.\n' +
      '* [2] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, C. Li, Y. Xu, H. Chen, J. Tian, Q. Qi, J. Zhang, and F. Huang, "mplug-owl: Modularization empowers large language models with multimodality," 2023.\n' +
      '* [3] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, "Instructblip: Towards general-purpose vision-language models with instruction tuning," 2023.\n' +
      '* [4] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, "Mistral 7b," 2023.\n' +
      '* [5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, "Language models are unsupervised multitask learners," 2019.\n' +
      '* [6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," 2019.\n' +
      '* [7] "Openai: gpt-3: Powerful language models for conersion. openai," 2022.\n' +
      '* [8] OpenAI. ;.: Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji, V. Balcom, P. Baltescu, H. Bao, M. Bavarian, J. Belgum, I. Bello, J. Bredine, G. Bernadt-Shapiro, C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L. Brakman, G. Brockman, T. Brooks, M. Brundage, K. Button, T. Cai, R. Campbell, A. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan, C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen, M. Chen, B. Chess, C. Cho, C. Chu, H. W. Chung, D. Cummings, J. Currier, Y. Dai, C. Decareaux, T. Degry, N. Deutsch, D. Deville, A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecoffet, A. Eleti, T. Eleundou, D. Farhi, L. Fedus, N. Felix, S. P. Fishman, J. Forte, I. Fulford, L. Gao, E. Georges, C. Gibson, V. Goel, T. Gognieni, G. Goh, R. Gontijo-Lopes, J. Gordon, M. Grafstein, S. Gray, R. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han, J. Harris, Y. He, M. Heaton, J. Heidecke, C. Hesse, A. Hickey, W. Hickey, P. Hoeschele, B. Houghton, K. Hsu, S. Hu, X. Hu, J. Huiziang, S. Jain, J. Jang, A. Jiang, R. Jiang, H. Jin, D. Jin, S. Jomoto, B. Honn, H. Jun, T. Kaftan, Lukasz Kaiser, A. Kamali, I. Kanitscheider, N. S. Keskar, T. Khan, L. Kilpatrick, J. W. Kim, C. Kim, Y. Kim, H. Kirchner, J. Kiros, M. Knight, D. Kokotajlo, Lukasz Kondraciuk, A. Kondrich, A. Konstantinidis, K. Kosic, G. Krueger, V. Kuo, M. Lampe, I. Lan, T. Lee, J. Leike, J. Leung, D. Levy, C. M. Li, R. Lim, M. Lin, S. Lin, M. Litwin, T. Lopez, R. Lowe, P. Lue, A. Makanju, K. Malfacini, S. Manning, T. Markov, Y. Markovski, B. Martin, R. Mayer, A. Mayne, B. McGrew, S. M. McKinney, C. McLeavey, P. McMillan, J. McNeil, D. Medina, A. Mehta, J. Menick, L. Metz, A. Mishchenko, P. Mishkin, V. Monaco, E. Morikawa, D. Mossing, T. Mu, M. Murati, O. Murk, D. Mely, A. Nair, R. Nakano, R. Nayak, A. Neelakantan, R. Ngo, H. Noh, L. Ouyang, C. O\'Keefe, J. Pachocki, A. Paino, J. Palermo, A. Pantuliano, G. Parascandolo, J. Parish, E. Parparita, A. Passos, M. Pavlov, A. Peng, A. Perelman, F. de Avila Belbute, B. Peters, M. Petrov, H. P. de Oliveira Pinto, Michael, Pokromy, M. Pokras, Y. Pong, T. Powell, A. Power, B. Power, E. Proehl, R. Puri, A. Radford, J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross, B. Rotsted, H. Roussez, N. Ryder, M. Saltarelli, T. Sanders, S. Santurkar, G. Sastryar, H. Schmidt, D. Schnurr, J. Schulman, D. Selsam, K. Sheppard, T. Sherbakov, J. Shieh, S. Shoker, P. Shyam, S. Sidor, E. Sigler, M. Simens, J. Sitkin, K. Slama, I. Sohl, B. Sokolowsky, Y. Song, N. Staudacher, F. P. Such, N. Summers, I. Sutskever, J. Tang, N. Tezak, M. Thompson, P. Tille, A. Tootoonchian, E. Tseng, P. Tuggle, N. Turley, J. Twork, J. F. C. Uribe, A. Vallone, A. Vijayveriya, C. Voss, C. Wainwright, J. J. Wang, A. Wang, B. Wang, J. Ward, J. Wei, C. Weinmann, A. Welihinda, P. Welinder, J. Weng, L. Weng, M. Wiethoff, D. Willner, C. Winter, S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo, K. Yu, Q. Yuan, W. Zaremba, R. Zellers, C. Zhang, M. Zhang, S. Zhao, T. Zheng, J. Zhuang, W. Zhuk, and B. Zoph, "Gpt-4 technical report," 2023.\n' +
      '* [6] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-lm: Training multi-billion parameter language models using model parallelism," 2020.\n' +
      '* [7] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hell-Belfern, G. Mishra, E. Moreira, M. Omenrick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H. Abrego, J. Ahn, J. Austin, P. Barham, J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng, C. Cherry, C. A. Choquet-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani, S. Dev, J. Devlin, M. Diaz, N. Du, E. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag, X. Garcia, S. Gehrmann, L. Gonzalez, G. Gur-Arri, S. Hand, H. Hashemi, L. Hou, J. Howland, A. Hu, J. Huij, H. Hurwitz, M. Isard, A. Ittychefl, M. Jagielski, W. Jia, K. Kenesad, M. Krikun, S. Kudugunta, C. Lan, K. Lee, B. Lee, E. Li, M. Li, W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu, M. Maggioni, A. Mahendru, J. Mayne, V. Misra, M. Moussaller, Z. Nado, J. Nham, E. Ni, A. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif, B. Richter, P. Riley, A. C. Ros, A. Roy, B. Saeta, R. Samuel, R. Shelby, A. Slone, D. Smilkov, D. R. So, D. Sohn, S. Tokumine, D. Valter, V. Vasudevan, K. Vdrahalli, X. Wang, P. Wang, Z. Wang, T. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. Xue, P. Yin, J. Yu, Q. Zhang, S. Zheng, C. Zheng, W. Zhou, D. Zhou, S. Petrov, and Y. Wu, "Palm 2 technical report," 2023.\n' +
      '* [8] K. A. Wang, D. Maddix, and Y. Wang, "Gopher: Categorical probabilistic forecasting with graph structure via local continuous-time dynamics," 2021.\n' +
      '* [9] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre, "Training compute-optimal large language models," 2022.\n' +
      '* [10] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer, "Opt: Open pre-trained transformer language models," 2022.\n' +
      '* [11] B. Workshop, ; T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagne, A. S. Luccioni, F. Yvon, M. Galle, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammannamachanti, T. Wang, B. Sagot, N. Mueneniboff, A. V. del Moral, O. Rwasse, R. Baowot, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Laurenson, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alffassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emeze, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, D. Radev, E. G. Ponferrada, E. Levkoviz, E. Kim, E. B. Natan, F. D. Toni, G. Dupont, G. Kruszewski, G. Pistilli, H. Elshahar, H. Benyamina, H. Tran, I. Yu, I. Abdulmunm, I. Johnson, I. Gonzalez-Dios, J. de la Rosa, J. Chim, J. Dodge, J. Zhu, J. Chang, J. Frohberg, J. Tobing, J. Bhattacharjee, K. ChK. Lo, L. V. Werna, L. Weber, L. Phan, L. B. allal, L. Tanguy, M. Dey, M. R. Munoz, M. Masoud, M. Grandury, M. Sako, M. Huang, M. Coavoux, M. Singh, M. T.-J. Jiang, M. C. Vu, M. A. Jauhar, M. Ghaleb, N. Subramani, N. Kassner, N. Khamis, O. Nguyen, O. Espejel, O. de Gibert, P. Villegas, P. Henderson, P. Colombo, P. Amuok, Q. Inoest, R. Harliman, R. Bommassani, R. L. Lopez, R. Ribeiro, S. Osei, S. Pyysalo, S. Nagel, S. Bose, S. H. Muhammad, S. Sharma, S. Longrere, S. Nikpoor, S. Silberberg, S. Pai, S. Zink, T. T. Torrent, T. Schick, T. Thrush, V. Danchev, V. Nikoulina, I. Lapiola, V. Lecperq, V. Prabhu, Z. Alayfaei, Z. Talat, A. Raja, B. Heinzerling, C. Si, D. E. Tayar, E. Salesky, S. J. Mielke, W. Y. Lee, A. Sharma, A. Santilli, A. Chaffin, A. Stiegler, D. Datta, E. Szczechla, G. Chibalani, H. Wang, H. Pandey, H. Strobelt, J. A. Fries, J. Rozen, L. Gao, L. Stutzwka, M. S. Bari, M. S. Al-ishabiani, M. Manica, N. Nayak, R. Teahan, S. Albanie, S. Shen, S. Ben-David, S. H. Bach, T. Kim, T. Bers, T. Fevry, T. Neeraj, U. Thakker, V. Raunak, X. Tang, Z.-X. Yong, Z. Sun, S. Brody, Y. Uri, H. Tojarjich, A. Roberts, H. W. Chung, J. Tae, J. Pang, O. Pres, C. Li, D. Narayanan, H. Bourfoune, J. Casper, J. Rasley, M. Ryabinin, M. Mishra, M. Zhang, M. Shoeybi, M. Peyrounette, N. Patry, N. Tazi, O. Sanserive, P. von Platen, P. Cornette, P. F. Lavallee, R. Lacroix, S. Rajbhandari, S. Gandhi, S. Smith, S. Requena, S. Patil, T. Dettmers, A. Baruwa, A. Singh, A. Cheveleva, A.-L. Ligozat, A. Subramonian, A. Neveol, C. Lovering, D. Garrette, D. Tunugunta, E. Reiter, E. Tkatkaeva, E. Voloshina, E. Bogdanov, G. I. Winata, H. Schockopf, J.-C. Kale, J. Novikova, J. Z. Forde, J. Clive, J. Kasai, K. Kawamura, L. Hazan, M. Carpuat, M. Clinciu, N. Kim, N. Cheng, O. Serikov, O. Antverg, O. van der Wal, R. Zhang, R. Zhang, S. Gehrmann, S. Mirkin, S. Pais, T. Shavrina, T. Scialom, T. Yun, T. Limisievicz, V. Rieser, V. Protasov, V. Mikhailov, Y. Pruksakachukun, Y. Belinkov, Z. Bamberger, Z. Kasner, A. Rueda, A. Pestana, A. Feizpour, A. Khan, A. Faranak, A. Santos, A. Hevia, A. Undreaj, A. Aghagol, A. Abdollahi, A. Tammon, A. Hajiosseini, B. Behroozi, B. Ajibade, B. Saxena, C. M. Ferrandis, D. McDuff, D. Contractor, D. Lancaky, D. David, D. Kiela, D. A. Nguyen, E. Tan, E. Baylor, E. Ozonani, F. Mirza, F. Ononiwu, H. Rezanejad, H. Jones, I. Bhattacharyya, I. Solatman, I. Sedenko, I. Nejagdhiol, J. Passmore, J. Setler, J. B. Sanz, L. Dura, M. Samagiao, M. Elbadir, M. Mieskes, M. Gerchick, M. Akindulov, M. McKenna, M. Qiu, M. Ghauri, M. Burynok, N. Abrar, N. Rajani, N. Elkott, N. Fahmy, O. Samuel, R. An, R. Kroman, R. Hao, S. Alizadeh, S. Shubber, S. Wang, S. Roy, S. Viguier, T. Le, T. Oyebade, T. Le, Y. Yang, Z. Nguyen, A. R. Kashyap, A. Palasciano, A. Callahan, A. Shukla, A. Miranda-Escalada, A. Singh, B. Beilharz, B. Wang, C. Brito, C. Zhou, C. Jain, C. Xu, C. Fourrier, D. L. Perinin, D. Molano, D. Yu, E. Manjavacas, F. Barth, F. Fuhrmann, G. Altay, G. Bayrak, G. Burns, H. U. Vrabec, I. Bello, I. Dash, J. Kang, J. Giorgi, J. Golde, J. D. Posada, K. R. Sivaraman, L. Bulchandani, L. Liu, L. Shinzato, M. H. de Bykhovte, M. Takeuchi, M. Pamies, M. A. Castillo, M. Nezhurina, M. Sanger, M. Samwald, M. Cullan, M. Weinberg, M. D. Wolf, M. Mihaljcic, M. Liu, M. Freidak, M. Kang, N. Seelam, N. Dahlberg, N. M. Broad, N. Muellner, P. Fung, P. Haller, R. Chandrasekhar, R. Eisenberg, R. Martin, R. Cannali, R. Su, R. Su, S. Cahyawijaya, S. Garda, S. S. Deshmukh, S. Mishra, S. Kiblawi, S. Ott, S. Sang-aroonsiri, S. Kumar, S. Schweter, S. Bharati, T. Laud, T. Gigant, T. Kainuma, W. Kusa, Y. Labrak, Y. S. Bajaj, Y. Venkatraman, Y. Xu, Y. Xu, Y. Xu, Z. Tan, Z. Xie, Z. Ye, M. Bras, Y. Belkada, and T. Wolf, "Bloom: A 176b-parameter open-access multilingual language model," 2023.\n' +
      '* [18] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe, "Training language models to follow instructions with human feedback," 2022.\n' +
      '* [19] OpenAI, "Tb openai. chatgpt: Optimizing language models for dialogue." 2022.\n' +
      '* [20] J. Chen, H. Guo, K. Yi, B. Li, and M. Elhoseiny, "Visualgpt: Data-efficient adaptation of pretrained language models for image captioning," 2022.\n' +
      '* [21] M. Tsimpoukelli, J. Menick, S. Cabi, S. M. A. Eslami, O. Vinyals, and F. Hill, "Multimodal few-shot learning with frozen language models," 2021.\n' +
      '* [22] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan, "Flamingo: a visual language model for few-shot learning," 2022.\n' +
      '* [23] J. Li, D. Li, S. Savarese, and S. Hoi, "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models," 2023.\n' +
      '* [24] H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Visual instruction tuning," 2023.\n' +
      '* [25] H. Liu, C. Li, Y. Li, and Y. J. Lee, "Improved baselines with visual instruction tuning," 2023.\n' +
      '* [26] B. Li, Y. Zhang, L. Chen, J. Wang, J. Yang, and Z. Liu, "Outer: A multi-modal model with in-context instruction tuning," 2023.\n' +
      '* [27] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, "Learning transferable visual models from natural language supervision," 2021.\n' +
      '* [28] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullins, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki, "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs," 2021.\n' +
      '* [29] M. B. B. P. H. K. S. Lee., "Coyo-700m.," _Journal Name_, vol. Volume, no. Issue, p. Page Range, 2022. [Online]. Available: URL\n' +
      '* [30] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts," 2021.\n' +
      '* [31] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollar, "Microsoft coco: Common objects in context," 2015.\n' +
      '* [32] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Estioglu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerze, M. Khabas, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihalyov, P. Mishra, I. Molybog, Y. Nie, A. Pouton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. Xuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, "Llama 2: Open foundation and fine-tuned chat models," 2023.\n' +
      '* [33] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressard, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulinier, M.-A. Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, "Mixtal of experts," 2024.\n' +
      '\n' +
      '## 7 Appendix\n' +
      '\n' +
      '### GitHub\n' +
      '\n' +
      'The code repository, along with detailed documentation, can be found at [https://github.com/superagi/Veagle](https://github.com/superagi/Veagle).\n' +
      '\n' +
      '### Huggingface\n' +
      '\n' +
      'The Veagle model, along with detailed documentation is available at\n' +
      '\n' +
      '[https://huggingface.co/SuperAGI/Veagle](https://huggingface.co/SuperAGI/Veagle)\n' +
      '\n' +
      '### Training Parameters\n' +
      '\n' +
      '### Cl\n' +
      '\n' +
      'We have used 8 NVIDIA A100 with a batch size of 10 for both pre-training and fine-tuning. For inference 1 NVIDIA A6000 is used.\n' +
      '\n' +
      '### Qualitative Examples\n' +
      '\n' +
      'Sking involves the risk of falling, which can result in injuries such as sprains, strains, fractures, or head injuries. Skiers may collide with other skiers, trees, rocks, or other obstacles, leading to injuries or accidents.\n' +
      '\n' +
      'Adverse weather conditions, such as heavy snowfall, fog, or strong winds, can make skiing more challenging and increase the risk of accidents. Sking at high altitudes can lead to altitude sickness, which can cause symptoms such as headache, dizziness, nausea, and shortness of breath. Prolonged exposure to cold temperatures can lead to frostbite and hypothermia, which can be life-threatening if not treated promptly.\n' +
      '\n' +
      'Figure 6: _Examples generated by our Veagle model exemplify a broad spectrum of its diverse capabilities. These showcases encompass intricate visual scene comprehension and reasoning, multi-turn visual conversation, and various other impressive functionalities._\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c}  & **Epochs** & **Optimizer** & **I-rate** & **Batch size** & **Weight decay** \\\\ \\hline Pre-training & **3** & AdamW & 1e-5 & 8 & 0.05 \\\\ Fine-tuning & **2** & AdamW & 1e-5 & 10 & 0.05 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: _Training parameters in both the stages_Figure 8: _Examples generated by our Veagle model showing its innovative capabilities._\n' +
      '\n' +
      'Figure 7: _From pixels to pitches, our model’s ability to generate poems, ads, and songs from images is a testament to its multi-dimensional creative prowess._\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>