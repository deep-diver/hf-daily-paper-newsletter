<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 독수리: 멀티모달 표현 학습에서의 발전\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근 인공 지능의 연구자들은 언어와 비전이 어떻게 결합하는지에 정말 관심이 있어 텍스트와 시각 정보를 원활하게 통합하는 것을 목표로 하는 멀티모달 모델의 개발을 불러일으켰다. MLM(Large Language Models)의 확장인 멀티모달 모델은 이미지 캡션 및 시각적 질의 응답(VQA)에서 시각적 접지에 이르기까지 다양한 작업을 해결하는 데 놀라운 기능을 발휘했다. 이러한 모델은 상당한 발전을 보여주었지만 이미지를 정확하게 해석하고 실제 시나리오에서 흔히 발생하는 질문에 답하는 데 어려움이 계속된다. 본 논문은 기존 모델의 멀티모달 기능을 향상시키기 위한 새로운 접근 방식을 소개한다. 현재 비전 언어 모델(VLM)과 멀티모달 대형 언어 모델(MLLM)에서 관찰되는 한계에 대응하여, 제안된 모델 베이글은 이전 작업의 성공과 통찰에서 영감을 얻은 고유한 메커니즘을 통합한다. 독수리는 동적 메커니즘을 활용하여 인코딩된 시각적 정보를 언어 모델에 직접 투영한다. 이 동적 접근법은 시각적 맥락에 존재하는 복잡한 세부 사항에 대한 보다 미묘한 이해를 가능하게 한다. 비글의 유효성을 검증하기 위해, 우리는 시각적 질문 응답 및 이미지 이해와 같은 작업을 강조하는 벤치마크 데이터 세트에 대한 포괄적인 실험을 수행한다. 우리의 결과는 비글이 기존 모델보다 눈에 띄는 차이로 성능이 5-6% 향상되었음을 나타낸다. 결과는 전통적인 벤치마크를 넘어 모델의 범용성과 적용 가능성을 강조한다. 또한 코드와 모델을 연구 커뮤니티에 공개적으로 접근 가능하게 하여 멀티모달 AI의 진화하는 환경에서 협업과 추가 탐구를 촉진한다. 코드 저장소는 상세한 문서와 함께 [https://github.com/superagi/Veagle](https://github.com/superagi/Veagle)에서 찾을 수 있다.\n' +
      '\n' +
      'Rajat Chawla*Anmol Gautam Ayush Vatsal Ayush Vatsal Sukrit Chaterjee Mukunda NS+Adarsh Jha Ishan Bhola SuperAGI\n' +
      '\n' +
      '각주 †: 이 작가들은 동등하게 기여했다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 들어, 대규모 언어모델(Large Language Model: LLM)에 대한 관심이 급증하면서 자연언어 이해의 경관이 재편되고, 멀티모달 대규모 언어모델(Multimodal Large Language Model: MLLM)의 검토 및 적용이 크게 급증하고 있다. 모델이 텍스트, 이미지, 비디오 및 음성과 같은 다양한 양식을 활용하도록 허용하는 MLLM은 적응 가능한 다목적 어시스턴트를 만드는 데 필수적이 되었다. 광범위한 작업에 걸친 인상적인 일반화 능력과 시각적 이해 능력을 갖춘 LLM을 통합하는 비전 언어 모델(VLM)의 개발에도 불구하고, 현대 모델은 이미지 내에서 내장된 텍스트를 해석하는 데 어려움을 겪는다. 이러한 한계는 텍스트가 포함된 이미지가 일상 생활에서 널리 퍼져 있고 이러한 내용을 이해하는 것이 인간의 시각 지각에 필수적이기 때문에 이 연구의 초점이다.\n' +
      '\n' +
      '본 연구는 학습된 질의 임베딩과 추가적인 시각적 지원을 결합하여 새로운 작업 방법을 제시한다. 이 방법은 언어 모델이 이미지에서 일반적으로 얻는 정보의 한계를 다루기 위해 인코딩된 패치 임베딩을 사용한다. 결과적으로, 모델이 텍스트와 이미지 사이의 관계를 얼마나 잘 이해하고 인식할 수 있는지를 향상시킨다. 비글이라고 불리는 우리의 모델은 미리 훈련된 비전 인코더와 언어 모델을 사용하는 것으로 시작한다. 이미 알고 있는 것을 잊어버리지 않도록 2단계로 훈련하고 훈련을 덜 복잡하게 만들어 궁극적으로 모델을 더 효과적으로 만듭니다. 텍스트가 많은 이미지를 평가하기 위해 표준 Visual Question-Answering (VQA) 벤치마크와 프로토콜을 사용하여 모델을 테스트했다. 우리의 비글 모델은 텍스트와 이미지 간의 관계에 대한 이해와 인식을 크게 향상시켜 이미지 내에서 내장된 텍스트를 이해하는 데 있어 기존 벤치마크를 능가한다.\n' +
      '\n' +
      '본 연구에서는 멀티모달 학습과 해석 분야에서 유의미한 도약을 나타내는 혁신 모델인 베글(Veagle)을 제시한다. 비글의 핵심은 최첨단 구성 요소가 기능을 증폭하기 위해 시너지 효과를 발휘하는 BLIVA[1] 아키텍처의 향상된 버전을 통합하는 것이다. 특히, 우리는 mPlugOwl[2]에서 조달한 우수한 비전 추상기를 통합하여 모델의 시각적 처리 능력을 향상시킨다. 이 비전 추상기는 InstructBLIP[3]의 Q-Former와 LLM(Large Language Model)인 Mistral[4]과 결합하여 강력한 시너지 효과를 만들어 모델의 전반적인 정확도를 크게 향상시킨다. 우리의 방법론의 중요한 측면은 mPlugOwl[2]에 의해 세심하게 훈련된 비전 인코더를 포함하는 것이다. 이 인코더는 이미지에서 높은 수준의 시각적 특징을 추출하는 데 중추적인 역할을 하여 비글은 정확한 해석을 위해 필수적인 시각적 정보를 캡처할 수 있다. 이 비전 인코더는 이미지에서 높은 수준의 시각적 특징을 추출하도록 훈련되어 모델이 정확한 해석을 위해 중요한 시각적 정보를 캡처할 수 있게 한다. 독수리는 미스트랄의 예외적인 언어 이해를 비전 추상자와 매끄럽게 결합함으로써 자신을 구별하여 텍스트적 정보와 시각적 정보를 모두 효과적으로 통합하는 포괄적인 모델을 도출한다. 언어 이해력에서 미스트랄의 숙련도는 비글의 전반적인 성능을 크게 향상시킨다. 우리의 방법론은 사전 훈련 및 미세 조정 단계 모두에 대해 신중하게 선택된 세심하게 선별된 데이터 세트의 사용에 중점을 둔다. 이 데이터 세트는 모델의 이해도를 형성하는 기초가 되어 다양한 시나리오에 걸쳐 강력한 일반화를 보장합니다. 우리의 결과는 비글이 이미지 내에서 텍스트를 이해하는 데 더 잘 이해한다는 것을 보여준다. 이것은 표준 시각 질문 응답(VQA) 테스트에서 인상적인 성능으로 뒷받침됩니다. 독수리는 기존 모델을 능가할 뿐만 아니라 정확성과 효율성을 위한 새로운 벤치마크를 구축합니다. 결론적으로, 비글은 고급 구성 요소를 통합할 뿐만 아니라 선별된 오픈 소스 데이터의 풍부한 포함으로 인한 이점을 제공하는 최첨단 모델을 나타내며, 멀티모달 AI 연구의 진화하는 환경에서 선구적인 솔루션이 된다.\n' +
      '\n' +
      '나머지 논문은 다음과 같이 정리된다. 2절에서는 문헌 검토를 제시한다. 3은 제안된 아키텍처를 강조하고 섹션 4는 수행된 실험의 세부 사항을 포함하고 결과를 논의한다. 이 후 섹션 5의 결론이 이어진다.\n' +
      '\n' +
      '## 2 문헌 조사\n' +
      '\n' +
      '이 절에서는 대형 언어 모델과 멀티모달 대형 언어 모델에 대한 관련 작업을 조사한다.\n' +
      '\n' +
      '### Llm\n' +
      '\n' +
      '언어 모델(LLM)은 자연어 처리(NLP) 분야에 혁명을 일으켰으며, 텍스트 예측에서 일관성 있고 맥락적으로 관련된 텍스트를 생성하는 것까지 다양한 기능을 제공한다. 자연어 처리의 끊임없이 진화하는 영역에서, LLM(Large Language Models)은 현장에 지울 수 없는 흔적을 남기는 매혹적인 여정을 겪었다. GPT-2[5] 및 BERT[6]과 같은 사전 시각화 모델의 초기 기여는 기둥으로 작용하여 방대한 웹 규모의 텍스트 데이터 세트에 대한 훈련에서 발생하는 엄청난 잠재력을 보여준다. 이 모델들은 자연어 처리(NLP)의 기반을 마련했을 뿐만 아니라 후속 발전을 위한 촉매 역할을 했다. 눈에 띄는 이정표 중에는 크기 기록이 산산조각 났을 뿐만 아니라 복잡한 문제를 해결하는 데 있어 비교할 수 없는 성능을 보여주는 모델인 기념비적인 GPT-3[7]이 있다. 엄청난 1,750억 개의 파라미터로 GPT-3[7]은 다양한 언어 과제에서 탁월하면서 강자로 부상했다. 그것의 도입은 모델 크기의 한계에 대한 재검토를 촉발시켰고 대규모 언어 모델을 처리하는 데 내재된 응용 프로그램과 도전에 대한 새로운 관심을 촉발했다. 그 여정은 GPT-3[7]로 마무리되지 않았고, 대신 GPT-4[8]와 같은 후속 모델과 메가톤 튜링 NLG[9], PalML[10], 고퍼[11], 친칠리아[12], OPT[13], BLOOM[14]와 같은 동반자가 등장하여 경계를 더욱 밀었다. 각각의 독특한 아키텍처, 훈련 방법론 및 응용 프로그램을 가진 이러한 모델은 대규모 언어 모델의 확장 도메인에서 연구의 동적 태피스트리에 기여한다. 이러한 다양성은 다양한 언어 작업에 걸쳐 성능, 효율성 및 일반화를 최적화하려는 지속적인 노력을 강조한다. LLM의 최근 발전은 인간의 지시 및 피드백과 원활하게 정렬하기 위해 모델을 정제하는 데 미묘한 초점으로 표시되었다. InstructGPT[15], ChatGPT[16], 최신 반복인 GPT-4[8]과 같은 개척 모델이 이와 관련하여 모범으로 눈에 띈다. 그들은 역동적이고 맥락적으로 풍부한 대화를 하고, 사용자 프롬프트에 능숙하게 응답하며, 코드 생성과 같은 복잡한 작업에 능숙함을 보여줄 수 있는 능력을 가지고 있다. 이러한 LLM의 후속 발전은 텍스트 기반 언어 모델에 시각적 정보를 통합하려고 하는 멀티모달 대형 언어 모델의 출현으로 이어졌다. LLM을 인간 상호 작용 및 명령과 조화시키는 데 중점을 두는 것은 실제 애플리케이션으로의 실제 배치 및 통합을 향한 중추적인 단계를 의미한다.\n' +
      '\n' +
      '### MLLM(Multimodal Large Language Models)\n' +
      '\n' +
      '멀티모달 언어 모델(MLLMs)의 동적 풍경에서 연구자들은 전통적인 언어 경계를 초월하기 위해 대규모 언어 모델(LLMs)의 진행을 활용함에 따라 패러다임의 전환이 분명하다. VisualGPT[17], 겨울왕국[18], 플라밍고[19], BLIP2[20] 및 기타 선구적인 연구에 의해 기반을 둔 MLLM은 확장되는 비전 언어 과제에 능숙하게 대처하도록 진화했다. 이러한 작업은 이미지 캡션, 시각적 질문 응답(VQA) 및 경계 상자 생성을 포함하여 MLLM에 내재된 강력한 시각적 접지 능력을 보여준다. 특히, IntructBLIP[3], LLVA[21, 22], mPlugOwl[2], BLIVA와 같은 최근의 노력은 MLLM이 능숙하게 다루는 작업의 레퍼토리를 다양화하는 데 적극적으로 기여한다. 기존의 범위를 벗어나, 현재 진행 중인 연구는 LLaVA[21], InstructBLIP[3], Otter[23], mPLUG-Owl[2] 및 LLaVA-1.5[22]와 같은 노력으로 멀티모달 명령어 튜닝의 영역을 탐구한다. 모델 아키텍처 및 훈련 파이프라인에 대한 지속적인 탐색에도 불구하고, 풍경은 혁신적인 솔루션을 위해 여전히 열려 있습니다. 멀티모달 정보를 언어 모델에 통합하는 것은 다양한 언어 작업에 걸쳐 성능, 효율성 및 일반화에 상당한 발전을 가져왔다.\n' +
      '\n' +
      '##3 제안 프레임워크\n' +
      '\n' +
      '### Architecture Overview\n' +
      '\n' +
      '###### 3.1.1 이미지 인코더\n' +
      '\n' +
      '비주얼 인코더는 멀티모달 모델의 중요한 구성 요소이다. 시각 인코더는 모델이 시각 데이터로부터 의미 있는 표현을 추출하도록 돕는다. 이를 통해 모델은 이미지의 의미론과 맥락을 이해할 수 있으며, 이는 정확한 예측을 하거나 관련 출력을 생성하는 데 중요하다. 실험에서는 mPlugOwl[2]의 비젼 인코더(ViT-L/14[24])를 사용하였다. 이 인코더는 입력 영상에서 의미 있는 표현을 추출하는 역할을 한다. mPlugOwl[2]는 사전 훈련된 언어 모델을 동결 상태로 유지하면서 훈련 가능한 시각적 인코더를 통합하는 새로운 훈련 패러다임을 사용했다. 이 접근 방식은 모델이 낮은 수준과 높은 의미론적 시각 정보를 효과적으로 캡처하고 미리 훈련된 언어와 정렬할 수 있게 한다. 그들은 7에서 이미지 캡션 쌍을 활용했다.\n' +
      '\n' +
      '그림 1: _Veagle 모델 아키텍처: 비주얼 추상기는 냉동 이미지 인코더의 출력 임베딩으로부터 명령어 인식 비주얼 특징을 추출하는 역할을 한다. 이어서, 이러한 시각적 특징은 동결 언어 모델(LLM)에 대한 소프트 프롬프트로서 제공된다. 그런 다음 모델을 언어 모델링 손실로 미세 조정하여 원하는 응답을 생성한다._\n' +
      '\n' +
      'LAION-400M[25], COYO-700M[26], Conceptual Captions[27] 및 MSCOCO[28]를 포함한 실제 데이터 세트이다. 성능을 손상시키지 않고 모델링합니다.\n' +
      '\n' +
      '###### 3.1.2 비주얼 추상기\n' +
      '\n' +
      '시각 추상기는 시각 인코더와 언어 디코더 사이의 브리지 역할을 하여 모델이 텍스트와 함께 시각 정보를 효과적으로 처리하고 활용할 수 있게 하여 보다 강력하고 다재다능한 멀티모달 모델로 이어진다. 그것은 이미지 인코더에 의해 획득된 인코딩된 이미지 표현들로부터 필수적인 시각적 특징들을 추출하는 것에 초점을 맞춘다. 대형 언어 모델(LLM)은 주로 텍스트 말뭉치에 대한 사전 훈련을 통해 비전 인코더에서 추출한 이미지 특징을 처리하는 타고난 능력에 한계를 나타낸다. 이러한 간극을 해결하기 위해 BLIP-2[20]에서 QFormer 모듈의 도입은 비전 인코더와 언어 모델 사이의 다리를 구축하는 역할을 하는 중요한 매개체로 등장했다. 그리고 BLIP2[20]와 LLaVA[22]의 획기적인 조합인 BLIVA[1]이 나왔다. 그러나 선형 투영 계층은 LLM에 필요한 모든 정보를 캡처하는 데 매우 제한된 능력을 가지고 있다. LLM에 필요한 모든 정보를 캡처하는 데 있어 투영 레이어의 한계를 극복하기 위해 QFormer[20]와 함께 다중 레이어 퍼셉트론을 도입했다. 특히, 1은 우리의 모드가 비전 인코더로부터 임베딩을 생성하고 출력이 프로젝션 레이어를 통해 Q-포머 및 제2 프로젝션 레이어로 전달되는 것을 예시한다. QFormer[20] 및 Projection 계층의 출력은 연결되고 LLM으로 전달되어 비전 인코더와 언어 모델 간의 더 나은 정렬을 가능하게 한다.\n' +
      '\n' +
      '#### 3.1.3 Llm\n' +
      '\n' +
      '멀티모달 대형 언어 모델의 핵심은 키스톤 역할을 하는 LLM(Large Language Model)이다. 명령어와 정렬된 이미지 기능을 사용하여 해당 답변을 생성하기 위해 이 정보를 처리합니다. 우리의 연구에서 우리는 우수한 성능으로 인해 궁극적으로 미스트랄[4]에 정착하는 다양한 강력한 오픈 소스 대형 언어 모델의 기능을 활용합니다. 미스트랄 7B는 모든 벤치마크에서 선도적인 개방형 13B 모델(Llama 2[29])의 성능을 능가하고 특히 추론, 수학 및 코드 생성 작업에서 가장 잘 출시된 34B 모델(Llama 1[29])보다 우수하다. 미스트랄은 그룹화된 질의어텐션(GQA)의 혁신적인 사용을 통해 더 빠른 추론을 달성하고 슬라이딩 윈도우어텐션(SWA)을 통합함으로써 추론 비용이 감소된 임의의 길이의 시퀀스를 효과적으로 관리한다. 고급 기술의 이러한 조합은 미스트랄 7B를 도메인에서 선도 모델로 위치시켜 정확도와 계산 효율 모두에 대한 새로운 표준을 설정한다.\n' +
      '\n' +
      '### Training Scheme\n' +
      '\n' +
      '훈련 계획은 프리트레이닝과 파인튜닝의 두 단계로 구성된다. 그림 4는 우리의 훈련 패러다임을 보여준다.\n' +
      '\n' +
      '1단계 : 사전교육\n' +
      '\n' +
      '1. 이러한 중요한 사전 훈련 단계에서, LLM(Large Language Model)은 이미지 캡션 데이터 세트로부터 이미지-텍스트 쌍을 사용하여 시각적 인코더와 정렬되어 시각적 콘텐츠에 대한 포괄적인 이해를 용이하게 한다. 초점은 시각적 및 텍스트 정보의 매핑을 정제하여 투영 레이어를 훈련하는 데 있다. 이 단계를 통해 비전 인코더, Q-이전 및 LLM은 동결된 상태로 남아 후속 미세 조정을 위한 기존 지식을 보존한다.\n' +
      '\n' +
      '###### 3.2.2 Stage 2 : 파인튜닝\n' +
      '\n' +
      '사전 학습에 이어, LLM(Large Language Model)은 시각적 임베딩 공간에 익숙해져 이미지 설명을 생성할 수 있다. 그러나, 더 미세한 이미지 세부 사항을 이해하고 인간 질의에 효과적으로 응답할 수 있는 능력이 부족하다. 이 연구에서는 공개 데이터 세트, COCO, TextCaps, VQAv2, OK-VQA, AOK-VQA, GQA, OCR-VQA, TextVQA, VIZWiz 및 자체 큐레이션 데이터를 수집한다. 이 단계 동안, LLM(Large Language Model) 및 비전 인코더는 동결된 상태로 유지되는 반면, 모델의 나머지는 미세 조정을 거친다.\n' +
      '\n' +
      '##4 실험 개요\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '단일 단어 답변을 특징으로 하는 데이터 세트의 경우 GPT-4[8] 및 미스트랄[30]의 고급 기능을 사용하여 이러한 응답을 상세하고 미묘한 응답으로 확장하여 혁신적인 접근법을 채택했다. 이러한 전략적 향상은 본 모델의 전반적인 효과에 기여하여 다양한 쿼리 유형에 대한 보다 강력하고 포괄적인 이해를 보장한다. 특정 데이터 세트에 존재하는 반복 질문의 문제를 해결하기 위해 사전 조치를 취하여 개선했다.\n' +
      '\n' +
      '도 4 : 독수리 훈련 패러다임_개요\n' +
      '\n' +
      '도 3:_미세조정손실 Insights_\n' +
      '\n' +
      '도 2: _사전 훈련 손실 통찰력_\n' +
      '\n' +
      '학습 데이터 세트의 다양성과 품질. 다양한 별개의 질문을 포함하는 다양한 다른 질문을 생성함으로써 중복성을 효과적으로 완화하고 학습 데이터 세트를 강화하여 다양한 쿼리를 처리하는 데 있어 향상된 일반화 및 성능을 촉진한다. 데이터 세트 확대 및 개선의 이러한 세심한 과정은 우리 모델의 전반적인 성능과 신뢰성을 최적화하는 데 중추적인 역할을 했다. 다양한 데이터 세트의 세심한 컴파일, 필터링 및 증강은 모델의 성능과 신뢰성을 최대화하는 데 중요한 역할을 했다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '실험 결과는 다양한 데이터 세트에서 성능이 크게 향상되어 접근법의 효율성을 보여준다.\n' +
      '\n' +
      '4.2.1 기준선 대 제안 프로토콜\n' +
      '\n' +
      '분석을 위해 4개의 고급 기준선 모델 BLIVA[1], 지시BLIP[3], mPlugOwl[2] 및 LLAVA[22]를 사용했다. 이 모델 각각에 대해 이미지와 질문을 촬영하여 모델에 입력하고 주어진 응답을 기록했습니다. 제공된 반응의 정밀도를 평가하기 위해 GPT-4[8]을 평가 모델로 사용했다. 이 모델은 정답 또는 오답의 두 가지 별개의 분류로 답변을 분류했습니다. 이 평가 방법의 활용을 통해 얻은 다양한 다른 모델에 대한 각 데이터 세트에 해당하는 정확도 결과는 표 1에 종합적으로 제시되어 있으며, 제안된 모델은 다른 개방형 소스 베이스라인 모델과 비교할 때 인상적인 수준의 정확도를 달성했다.\n' +
      '\n' +
      '######4.2.2 In-House Test Dataset\n' +
      '\n' +
      '모델이 다양한 시나리오에서 얼마나 잘 수행되고 효과적으로 일반화되는지를 평가하기 위해 사내 테스트 데이터 세트를 만들었다. 이 데이터 세트는 캡션, 광학 문자 인식(OCR), 일반적인 시각적 질문 응답(VQA), 기술적 VQA 및 추론 VQA를 포함한 다양한 유형의 작업을 포함한다. 중요한 것은 우리의 모델이 훈련 과정에서 이 특정 데이터 세트를 만난 적이 없다는 것이다. 그 후, 이 테스트 데이터 세트를 사용하여 모든 모델에 대한 철저한 평가를 수행했으며 결과는 유망하다. 자세한 결과는 표 2에 제시되어 있다.\n' +
      '\n' +
      '######4.2.3 질적연쇄\n' +
      '\n' +
      '이 섹션에서는 평가 세트에서 파생된 정성적 결과를 제시한다. 이 일련의 평가는 복잡하고 도전적인 작업에 대한 모델의 성능을 분석하기 위해 신중하게 선별되었다. 이 작업은 수치 측정을 넘어 모델의 효과를 이해하기 위해 우리 팀이 수행의 미묘한 측면을 파고들기 위해 선택하고 수집했다. 그림 5는 우리 모델의 효과를 보여주고 있다. 더 많은 예가 7에 나와 있다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '결론적으로, 비글 멀티모달 모델은 다양한 영역에서 확립된 벤치마크를 일관되게 능가하는 강력한 경쟁자로 두드러진다. 광범위한 연구에서 선별된 다양한 모듈의 전략적 융합을 통해 비글은 기존 모델이 설정한 기대치를 충족할 뿐만 아니라 능가하는 놀라운 성능을 보여줍니다. 그러나 우리의 작업은 또한 여전히 정련이 필요한 영역을 드러내며 on을 강조한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline  & **Veagle** & **BLIVA** & **InstrickBLIP** & **mPlugOwl** & **LLAVA** \\\\ \\hline ok,vqa & **49.3** & 43.4 & 30.8 & 34.1 & 46.2 \\\\ ocr,vqa & 48.3 & 38.5 & 32.1 & 61.4 & **67.2** \\\\ scienceQA & **58.1** & 16.1 & 40.2 & 51.8 & 56.5 \\\\ coco,caption & 57.9 & 56.4 & 51.2 & 55.6 & **62.7** \\\\ ai2diagram & **56.3** & 50.8 & 31.9 & 48.5 & 50.9 \\\\ chart,qa & **13.4** & 13.2 & 3.4 & 10.2 & 3.1 \\\\ gua & **44.2** & 28.6 & 40.8 & 33.9 & 43.9 \\\\ text\\_vqa & 22.5 & 23.1 & 20.5 & 32.6 & **37.2** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 상이한 오픈소싱 데이터세트에 대한 제안된 모델의 성능.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline  & **Veagle** & **BLIVA** & **InstrickBLIP** & **mPlugOwl** & **LLAVA** \\\\ \\hline Test Data & **76.4** & 63.1 & 59.3 & 68.6 & 66.5 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 사내 테스트 데이터 세트에 대한 제안된 모델 베글의 성능.\n' +
      '\n' +
      '그림 5: 당사의 베글 모델에 의해 생산된 정성적 예는 다양한 기능의 스펙트럼을 보여줍니다. 이러한 시연에는 복잡한 시각적 장면 이해 및 추론, 다중 회전 시각적 대화 등이 포함됩니다.\n' +
      '\n' +
      '완벽함을 추구하는 본성. 이 확인은 비글과 같은 다중 모달 모델의 우수성으로 가는 경로가 계속 전개된다는 것을 인식하면서 추가 탐색 및 최적화의 필요성을 강조한다. 이 풍경을 탐색할 때 비글은 비전 언어 모델의 미래 발전을 위한 유망한 촉매제로 남아 있으며 이 역동적인 분야에서 추가 조사 및 혁신이 되고 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] W. Hu, Y. Xu, Y. Li, W. Li, Z. Chen, and Z. Tu, "Bliva: A simple multimodal llm for better handling of text-rich visual questions," 2023.\n' +
      '* [2] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, C. Li, Y. Xu, H. Chen, J. Tian, Q. Qi, J. Zhang, and F. Huang, "mplug-owl: Modularization empowers large language models with multimodality," 2023.\n' +
      '* [3] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, "Instructblip: Towards general-purpose vision-language models with instruction tuning," 2023.\n' +
      '* [4] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, "Mistral 7b," 2023.\n' +
      '* [5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, "Language models are unsupervised multitask learners," 2019.\n' +
      '* [6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," 2019.\n' +
      '* [7] "Openai: gpt-3: Powerful language models for conersion. openai," 2022.\n' +
      '* [8] OpenAI. ;.: Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji, V. Balcom, P. Baltescu, H. Bao, M. Bavarian, J. Belgum, I. Bello, J. Bredine, G. Bernadt-Shapiro, C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L. Brakman, G. Brockman, T. Brooks, M. Brundage, K. Button, T. Cai, R. Campbell, A. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan, C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen, M. Chen, B. Chess, C. Cho, C. Chu, H. W. Chung, D. Cummings, J. Currier, Y. Dai, C. Decareaux, T. Degry, N. Deutsch, D. Deville, A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecoffet, A. Eleti, T. Eleundou, D. Farhi, L. Fedus, N. Felix, S. P. Fishman, J. Forte, I. Fulford, L. Gao, E. Georges, C. Gibson, V. Goel, T. Gognieni, G. Goh, R. Gontijo-Lopes, J. Gordon, M. Grafstein, S. Gray, R. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han, J. Harris, Y. He, M. Heaton, J. Heidecke, C. Hesse, A. Hickey, W. Hickey, P. Hoeschele, B. Houghton, K. Hsu, S. Hu, X. Hu, J. Huiziang, S. Jain, J. Jang, A. Jiang, R. Jiang, H. Jin, D. Jin, S. Jomoto, B. Honn, H. Jun, T. Kaftan, Lukasz Kaiser, A. Kamali, I. Kanitscheider, N. S. Keskar, T. Khan, L. Kilpatrick, J. W. Kim, C. Kim, Y. Kim, H. Kirchner, J. Kiros, M. Knight, D. Kokotajlo, Lukasz Kondraciuk, A. Kondrich, A. Konstantinidis, K. Kosic, G. Krueger, V. Kuo, M. Lampe, I. Lan, T. Lee, J. Leike, J. Leung, D. Levy, C. M. Li, R. Lim, M. Lin, S. Lin, M. Litwin, T. Lopez, R. Lowe, P. Lue, A. Makanju, K. Malfacini, S. Manning, T. Markov, Y. Markovski, B. Martin, R. Mayer, A. Mayne, B. McGrew, S. M. McKinney, C. McLeavey, P. McMillan, J. McNeil, D. Medina, A. Mehta, J. Menick, L. Metz, A. Mishchenko, P. Mishkin, V. Monaco, E. Morikawa, D. Mossing, T. Mu, M. Murati, O. Murk, D. Mely, A. Nair, R. Nakano, R. Nayak, A. Neelakantan, R. Ngo, H. Noh, L. Ouyang, C. O\'Keefe, J. Pachocki, A. Paino, J. Palermo, A. Pantuliano, G. Parascandolo, J. Parish, E. Parparita, A. Passos, M. Pavlov, A. Peng, A. Perelman, F. de Avila Belbute, B. Peters, M. Petrov, H. P. de Oliveira Pinto, Michael, Pokromy, M. Pokras, Y. Pong, T. Powell, A. Power, B. Power, E. Proehl, R. Puri, A. Radford, J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross, B. Rotsted, H. Roussez, N. Ryder, M. Saltarelli, T. Sanders, S. Santurkar, G. Sastryar, H. Schmidt, D. Schnurr, J. Schulman, D. Selsam, K. Sheppard, T. Sherbakov, J. Shieh, S. Shoker, P. Shyam, S. Sidor, E. Sigler, M. Simens, J. Sitkin, K. Slama, I. Sohl, B. Sokolowsky, Y. Song, N. Staudacher, F. P. Such, N. Summers, I. Sutskever, J. Tang, N. Tezak, M. Thompson, P. Tille, A. Tootoonchian, E. Tseng, P. Tuggle, N. Turley, J. Twork, J. F. C. Uribe, A. Vallone, A. Vijayveriya, C. Voss, C. Wainwright, J. J. Wang, A. Wang, B. Wang, J. Ward, J. Wei, C. Weinmann, A. Welihinda, P. Welinder, J. Weng, L. Weng, M. Wiethoff, D. Willner, C. Winter, S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo, K. Yu, Q. Yuan, W. Zaremba, R. Zellers, C. Zhang, M. Zhang, S. Zhao, T. Zheng, J. Zhuang, W. Zhuk, and B. Zoph, "Gpt-4 technical report," 2023.\n' +
      '* [6] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-lm: Training multi-billion parameter language models using model parallelism," 2020.\n' +
      '* [7] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hell-Belfern, G. Mishra, E. Moreira, M. Omenrick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H. Abrego, J. Ahn, J. Austin, P. Barham, J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng, C. Cherry, C. A. Choquet-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani, S. Dev, J. Devlin, M. Diaz, N. Du, E. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag, X. Garcia, S. Gehrmann, L. Gonzalez, G. Gur-Arri, S. Hand, H. Hashemi, L. Hou, J. Howland, A. Hu, J. Huij, H. Hurwitz, M. Isard, A. Ittychefl, M. Jagielski, W. Jia, K. Kenesad, M. Krikun, S. Kudugunta, C. Lan, K. Lee, B. Lee, E. Li, M. Li, W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu, M. Maggioni, A. Mahendru, J. Mayne, V. Misra, M. Moussaller, Z. Nado, J. Nham, E. Ni, A. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif, B. Richter, P. Riley, A. C. Ros, A. Roy, B. Saeta, R. Samuel, R. Shelby, A. Slone, D. Smilkov, D. R. So, D. Sohn, S. Tokumine, D. Valter, V. Vasudevan, K. Vdrahalli, X. Wang, P. Wang, Z. Wang, T. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. Xue, P. Yin, J. Yu, Q. Zhang, S. Zheng, C. Zheng, W. Zhou, D. Zhou, S. Petrov, and Y. Wu, "Palm 2 technical report," 2023.\n' +
      '* [8] K. A. Wang, D. Maddix, and Y. Wang, "Gopher: Categorical probabilistic forecasting with graph structure via local continuous-time dynamics," 2021.\n' +
      '* [9] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre, "Training compute-optimal large language models," 2022.\n' +
      '* [10] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer, "Opt: Open pre-trained transformer language models," 2022.\n' +
      '* [11] B. Workshop, ; T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagne, A. S. Luccioni, F. Yvon, M. Galle, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammannamachanti, T. Wang, B. Sagot, N. Mueneniboff, A. V. del Moral, O. Rwasse, R. Baowot, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Laurenson, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alffassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emeze, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, D. Radev, E. G. Ponferrada, E. Levkoviz, E. Kim, E. B. Natan, F. D. Toni, G. Dupont, G. Kruszewski, G. Pistilli, H. Elshahar, H. Benyamina, H. Tran, I. Yu, I. Abdulmunm, I. Johnson, I. Gonzalez-Dios, J. de la Rosa, J. Chim, J. Dodge, J. Zhu, J. Chang, J. Frohberg, J. Tobing, J. Bhattacharjee, K. ChK. Lo, L. V. Werna, L. Weber, L. Phan, L. B. allal, L. Tanguy, M. Dey, M. R. Munoz, M. Masoud, M. Grandury, M. Sako, M. Huang, M. Coavoux, M. Singh, M. T.-J. Jiang, M. C. Vu, M. A. Jauhar, M. Ghaleb, N. Subramani, N. Kassner, N. Khamis, O. Nguyen, O. Espejel, O. de Gibert, P. Villegas, P. Henderson, P. Colombo, P. Amuok, Q. Inoest, R. Harliman, R. Bommassani, R. L. Lopez, R. Ribeiro, S. Osei, S. Pyysalo, S. Nagel, S. Bose, S. H. Muhammad, S. Sharma, S. Longrere, S. Nikpoor, S. Silberberg, S. Pai, S. Zink, T. T. Torrent, T. Schick, T. Thrush, V. Danchev, V. Nikoulina, I. Lapiola, V. Lecperq, V. Prabhu, Z. Alayfaei, Z. Talat, A. Raja, B. Heinzerling, C. Si, D. E. Tayar, E. Salesky, S. J. Mielke, W. Y. Lee, A. Sharma, A. Santilli, A. Chaffin, A. Stiegler, D. Datta, E. Szczechla, G. Chibalani, H. Wang, H. Pandey, H. Strobelt, J. A. Fries, J. Rozen, L. Gao, L. Stutzwka, M. S. Bari, M. S. Al-ishabiani, M. Manica, N. Nayak, R. Teahan, S. Albanie, S. Shen, S. Ben-David, S. H. Bach, T. Kim, T. Bers, T. Fevry, T. Neeraj, U. Thakker, V. Raunak, X. Tang, Z.-X. Yong, Z. Sun, S. Brody, Y. Uri, H. Tojarjich, A. Roberts, H. W. Chung, J. Tae, J. Pang, O. Pres, C. Li, D. Narayanan, H. Bourfoune, J. Casper, J. Rasley, M. Ryabinin, M. Mishra, M. Zhang, M. Shoeybi, M. Peyrounette, N. Patry, N. Tazi, O. Sanserive, P. von Platen, P. Cornette, P. F. Lavallee, R. Lacroix, S. Rajbhandari, S. Gandhi, S. Smith, S. Requena, S. Patil, T. Dettmers, A. Baruwa, A. Singh, A. Cheveleva, A.-L. Ligozat, A. Subramonian, A. Neveol, C. Lovering, D. Garrette, D. Tunugunta, E. Reiter, E. Tkatkaeva, E. Voloshina, E. Bogdanov, G. I. Winata, H. Schockopf, J.-C. Kale, J. Novikova, J. Z. Forde, J. Clive, J. Kasai, K. Kawamura, L. Hazan, M. Carpuat, M. Clinciu, N. Kim, N. Cheng, O. Serikov, O. Antverg, O. van der Wal, R. Zhang, R. Zhang, S. Gehrmann, S. Mirkin, S. Pais, T. Shavrina, T. Scialom, T. Yun, T. Limisievicz, V. Rieser, V. Protasov, V. Mikhailov, Y. Pruksakachukun, Y. Belinkov, Z. Bamberger, Z. Kasner, A. Rueda, A. Pestana, A. Feizpour, A. Khan, A. Faranak, A. Santos, A. Hevia, A. Undreaj, A. Aghagol, A. Abdollahi, A. Tammon, A. Hajiosseini, B. Behroozi, B. Ajibade, B. Saxena, C. M. Ferrandis, D. McDuff, D. Contractor, D. Lancaky, D. David, D. Kiela, D. A. Nguyen, E. Tan, E. Baylor, E. Ozonani, F. Mirza, F. Ononiwu, H. Rezanejad, H. Jones, I. Bhattacharyya, I. Solatman, I. Sedenko, I. Nejagdhiol, J. Passmore, J. Setler, J. B. Sanz, L. Dura, M. Samagiao, M. Elbadir, M. Mieskes, M. Gerchick, M. Akindulov, M. McKenna, M. Qiu, M. Ghauri, M. Burynok, N. Abrar, N. Rajani, N. Elkott, N. Fahmy, O. Samuel, R. An, R. Kroman, R. Hao, S. Alizadeh, S. Shubber, S. Wang, S. Roy, S. Viguier, T. Le, T. Oyebade, T. Le, Y. Yang, Z. Nguyen, A. R. Kashyap, A. Palasciano, A. Callahan, A. Shukla, A. Miranda-Escalada, A. Singh, B. Beilharz, B. Wang, C. Brito, C. Zhou, C. Jain, C. Xu, C. Fourrier, D. L. Perinin, D. Molano, D. Yu, E. Manjavacas, F. Barth, F. Fuhrmann, G. Altay, G. Bayrak, G. Burns, H. U. Vrabec, I. Bello, I. Dash, J. Kang, J. Giorgi, J. Golde, J. D. Posada, K. R. Sivaraman, L. Bulchandani, L. Liu, L. Shinzato, M. H. de Bykhovte, M. Takeuchi, M. Pamies, M. A. Castillo, M. Nezhurina, M. Sanger, M. Samwald, M. Cullan, M. Weinberg, M. D. Wolf, M. Mihaljcic, M. Liu, M. Freidak, M. Kang, N. Seelam, N. Dahlberg, N. M. Broad, N. Muellner, P. Fung, P. Haller, R. Chandrasekhar, R. Eisenberg, R. Martin, R. Cannali, R. Su, R. Su, S. Cahyawijaya, S. Garda, S. S. Deshmukh, S. Mishra, S. Kiblawi, S. Ott, S. Sang-aroonsiri, S. Kumar, S. Schweter, S. Bharati, T. Laud, T. Gigant, T. Kainuma, W. Kusa, Y. Labrak, Y. S. Bajaj, Y. Venkatraman, Y. Xu, Y. Xu, Y. Xu, Z. Tan, Z. Xie, Z. Ye, M. Bras, Y. Belkada, and T. Wolf, "Bloom: A 176b-parameter open-access multilingual language model," 2023.\n' +
      '* [18] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe, "Training language models to follow instructions with human feedback," 2022.\n' +
      '* [19] OpenAI, "Tb openai. chatgpt: Optimizing language models for dialogue." 2022.\n' +
      '* [20] J. Chen, H. Guo, K. Yi, B. Li, and M. Elhoseiny, "Visualgpt: Data-efficient adaptation of pretrained language models for image captioning," 2022.\n' +
      '* [21] M. Tsimpoukelli, J. Menick, S. Cabi, S. M. A. Eslami, O. Vinyals, and F. Hill, "Multimodal few-shot learning with frozen language models," 2021.\n' +
      '* [22] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan, "Flamingo: a visual language model for few-shot learning," 2022.\n' +
      '* [23] J. Li, D. Li, S. Savarese, and S. Hoi, "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models," 2023.\n' +
      '* [24] H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Visual instruction tuning," 2023.\n' +
      '* [25] H. Liu, C. Li, Y. Li, and Y. J. Lee, "Improved baselines with visual instruction tuning," 2023.\n' +
      '* [26] B. Li, Y. Zhang, L. Chen, J. Wang, J. Yang, and Z. Liu, "Outer: A multi-modal model with in-context instruction tuning," 2023.\n' +
      '* [27] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, "Learning transferable visual models from natural language supervision," 2021.\n' +
      '* [28] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullins, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki, "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs," 2021.\n' +
      '* [29] M. B. B. P. H. K. S. Lee., "Coyo-700m.," _Journal Name_, vol. Volume, no. Issue, p. Page Range, 2022. [Online]. Available: URL\n' +
      '* [30] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts," 2021.\n' +
      '* [31] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollar, "Microsoft coco: Common objects in context," 2015.\n' +
      '* [32] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Estioglu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerze, M. Khabas, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihalyov, P. Mishra, I. Molybog, Y. Nie, A. Pouton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. Xuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, "Llama 2: Open foundation and fine-tuned chat models," 2023.\n' +
      '* [33] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressard, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulinier, M.-A. Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, "Mixtal of experts," 2024.\n' +
      '\n' +
      '## 7 Appendix\n' +
      '\n' +
      '### GitHub\n' +
      '\n' +
      '자세한 문서와 함께 코드 저장소는 [https://github.com/superagi/Veagle](https://github.com/superagi/Veagle)에서 찾을 수 있다.\n' +
      '\n' +
      '### Huggingface\n' +
      '\n' +
      '자세한 문서와 함께 비글 모델을 사용할 수 있습니다.\n' +
      '\n' +
      '[https://huggingface.co/SuperAGI/Veagle](https://huggingface.co/SuperAGI/Veagle)\n' +
      '\n' +
      '### Training Parameters\n' +
      '\n' +
      '### Cl\n' +
      '\n' +
      '사전 훈련과 미세 조정 모두에 배치 크기가 10인 8개의 NVIDIA A100을 사용했습니다. 추론을 위해 NVIDIA A6000이 사용된다.\n' +
      '\n' +
      '### Qualitative Examples\n' +
      '\n' +
      '스킹은 넘어질 위험을 수반하며, 이는 염좌, 변형, 골절 또는 머리 부상과 같은 부상을 초래할 수 있다. 스키어는 다른 스키어, 나무, 바위 또는 기타 장애물과 충돌하여 부상이나 사고로 이어질 수 있습니다.\n' +
      '\n' +
      '폭설, 안개, 강풍과 같은 기상 악조건은 스키를 더 어렵게 만들고 사고의 위험을 증가시킬 수 있다. 높은 고도에서 운동하는 것은 고산병으로 이어질 수 있으며, 이는 두통, 현기증, 메스꺼움, 호흡 곤란과 같은 증상을 유발할 수 있다. 저온에 장기간 노출되면 동상과 저체온증이 발생할 수 있으며, 이는 신속하게 치료하지 않으면 생명을 위협할 수 있다.\n' +
      '\n' +
      '그림 6: 우리의 베글 모델에 의해 생성된 예는 다양한 기능의 광범위한 스펙트럼을 예시한다. 이러한 쇼케이스는 복잡한 시각적 장면 이해 및 추론, 멀티턴 시각적 대화 및 기타 다양한 인상적인 기능을 포함한다._\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c}  & **Epochs** & **Optimizer** & **I-rate** & **Batch size** & **Weight decay** \\\\ \\hline Pre-training & **3** & AdamW & 1e-5 & 8 & 0.05 \\\\ Fine-tuning & **2** & AdamW & 1e-5 & 10 & 0.05 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 두 단계의 _훈련 매개변수_그림 8: 혁신적인 능력을 보여주는 우리의 베글 모델에 의해 생성된 _예시._\n' +
      '\n' +
      '그림 7: _픽셀에서 음정까지, 이미지에서 시, 광고 및 노래를 생성하는 우리 모델의 능력은 다차원 창의력의 증거이다._\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>