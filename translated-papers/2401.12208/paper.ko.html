<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '재단 모델# CheXagent: 재단의 모델 모델.# CheXagent.\n' +
      '\n' +
      'X-Ray 상호 해석 제1항 X-Ray 사례.\n' +
      '\n' +
      '지히홍 첸\\({}^{1*}\\)는 마야 베르마\\({}^{1*}\\,{}^{1*}\\)의 마그니니 파스칼리\\({}^{1*}\\)이다.\n' +
      '\n' +
      '**Louis Blankemier\\({}^{1}\\) Dave Van Veen\\({}^{1}\\)*\n' +
      '\n' +
      '**Joseph Paul Cohen\\({}^{1\\,{}^{1\\\\)  앤드루 존스턴\\({}^{1\\dagger}\\)  > 에두아르도 폰테스 Reis\\({}^{1\\,{}^{1}\\) ***.\n' +
      '\n' +
      '<{}^{1}\\> <Tanishq Mathew Abraativity\\({}^{1}\\)>*.\n' +
      '\n' +
      '**Akshay S. 커티스 랑그로츠\\({}^{1}\\)**.\n' +
      '\n' +
      '안정적인 AI({}^{1}\\) 스타포드 대학\\({}^{2}\\)가 안정적입니다.\n' +
      '\n' +
      '{zhihongc,mvarma2,jbdel,paschali,akshaysc,langlotz}@stanford.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '흉부 X선(CXR)은 임상 실습에서 가장 자주 수행되는 영상 검사이다. 시력-언어 기반 모델 개발(FM)의 최근 발전은 자동화된 CXR 해석을 수행할 가능성을 높여 임상 의사 결정을 돕고 환자 결과를 개선할 수 있다. 그러나 CXR을 정확하게 해석할 수 있는 FM을 개발하는 것은 의료 이미지 영역에서 대규모 비전-언어 데이터셋의 제한된 가용성, 의료 데이터의 복잡성을 포착할 수 있는 비전 및 언어 인코더의 부족, CXR 해석에 대한 FM의 능력을 벤치마킹하기 위한 평가 프레임워크의 부재(3)로 인해 어렵다. 이 작업에서 우리는 먼저 28개의 공개적으로 이용 가능한 데이터 세트로부터 큐레이션된 _CheXinstruct_ - 대규모 명령어-튜닝 데이터 세트를 도입함으로써 이러한 문제를 해결한다. 그런 다음 CXR을 분석하고 요약할 수 있는 _CheXagent_ - 명령어-튜닝된 FM을 제시한다. CheXagent를 구축하기 위해 방사선 보고서를 파싱하기 위한 임상 대형 언어 모델(LLM), CXR 이미지를 나타내는 비전 인코더 및 비전 및 언어 방식을 설정할 네트워크를 설계한다. 마지막으로, 임상적으로 관련된 8개의 CXR 해석 과제에 걸쳐 FM을 체계적으로 평가하도록 설계된 _CheXbench_ - 새로운 벤치마크를 소개한다. 5명의 전문가 방사선 전문의와 함께 광범위한 정량적 평가와 질적 리뷰는 CheX벤치 작업에 대해 CheXagent가 이전에 개발된 일반 및 의료 도메인 FM을 능가한다는 것을 보여준다. 나아가 모형 투명성을 향상시키기 위해 잠재적인 성과 격차를 부각시키기 위해 성별, 인종 및 연령의 요인에 걸쳐 공정성 평가를 수행한다. 우리의 프로젝트는 [https://stanford-aimi.github.io/chexagenthml] (https://스탄ford-aimi.github.io/chexagenthtml)에 있습니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '연결 모델(FMs)은 최근 다양한 추론 및 이해 과제[9]를 수행할 수 있는 강력한 모델 부류로 부상했다. FM의 증가는 본질적으로 다중 모달 데이터에서 일반적으로 다면적인 질문을 제기해야 하는 복잡한 의료 워크플로우를 재결합할 수 있는 주요 기회를 제공한다. 한 가지 특정 임상 워크플로우는 의료 영상 데이터에 대한 분석이다. 흉부 X선(CXR) 해석의 예를 들어, 가장 흔한 의료 영상 연구 - 미국[33]에서 매년 7,000만 CXR이 수행되었다. 여기서 방사선 전문의는 매일 수백 개의 이미지를 해석하고, 영상 통찰력을 텍스트 설명으로 번역하고, 이미지 결과를 다른 임상의들에게 간결하게 요약하는 한편, 정확도를 극대화하고 편향을 최소화한다. 더욱이, CXR에 대한 환자 대면 작업에는 이러한 생성된 발견에 대한 질문을 명확히 하는 답변이 포함될 수 있다. 이러한 작업의 효율성을 자동화하거나 증가시킬 수 있는 CXR FM은 환자의 만족도와 결과[67; 75; 96]뿐만 아니라 임상적 의사 결정을 실질적으로 향상시킬 수 있다.\n' +
      '\n' +
      '고품질 CXR FM은 비전과 언어의 격차를 해소해야 한다. 시력-언어 FM을 구축하기 위한 대부분의 사전 접근법은 고성능 방식이 이미지 인코더를 전처리된 대형 언어 모델(LLM)과 정렬하는 자연 이미지 설정에 중점을 둔다. 이러한 지시에 따른 복합적 LLM은 다양한 인식과 생성 과제에 걸쳐 우수한 능력을 보여준다. 그러나 의료 영상화를 위한 지도 조정 복합 LLM 개발은 다음과 같은 이유로 어렵다.\n' +
      '\n' +
      '1. _시력 언어 의료 영상 데이터세트_가 부족하다. 명령어 조정 다중 모드 LLM은 명령어, 이미지 및 답변으로 구성된 데이터 삼중선이 있는 대규모 다양한 훈련 데이터 세트를 필요로 한다. 자연 영상 영역의 기존 접근법은 LAION-5B[66]와 같은 데이터세트로부터 수백만 개의 훈련 샘플을 레버리지하는 반면, 이러한 데이터의 가용성은 의료 텍스트의 대형 체인의 전파를 방지하는 환자 프라이버시 우려로 인해 의료 영역에서 심각하게 제한된다. 그 결과, CXR 해석을 위해 개발된 기존의 명령어-튜닝된 멀티모달 LLM은 제한된 명령어 다양성[74; 89]을 가진 작은 데이터셋에 대해 훈련된다.\n' +
      '2.__기존 비전 및 언어 인코더는 의료 데이터의 복잡성을 포착하지 못한다. 다중 모드 LLM은 강력한 이미지와 텍스트 인코더가 필요하다. 그러나 CXR은 자연 영상과 텍스트에서 큰 도메인 이동을 나타내며, 따라서 기존의 전처리된 비전 및 언어 조명은 의료 지식 접지로 투쟁한다.\n' +
      '의학에서 다중 모드 LLM에 대한 엄격한 평가는 3.__ 수행이 어렵다. 다중 모드 LLM은 개방형 응답을 생성하고 사실적 정확성과 완성도에 대한 자유 텍스트 반응을 평가하는 것은 특히 도메인 전문 지식이 필요한 대규모 평가에 매우 중요하다. CXR 해석을 위해 개발된 이전 FM은 주로 시각적 질문 응답(VQA) 또는 텍스트 생성 작업을 사용하여 평가된다. 우리가 아는 한, 우리 지식의 최선을 다해.\n' +
      '\n' +
      '제안된 파이프라인의 그림 1:오버뷰: CheXinstruct은 다양한 CXR 태스크에 걸쳐 지시 조정용 데이터셋 큐레이션, CheXagent는 CXR 해석을 위한 임상 FM, CheXbench는 포괄적인 FM 평가 벤치마크이다. 두 가지 예제 CXR 해석 과제에는 지역 소견 생성 및 개방형 시각 질문 응답(VQA)이 포함된다.\n' +
      '\n' +
      '다양한 임상적으로 관련된 CXR 해석 과제에 걸쳐 다중 모드 LLM의 정량적 및 질적 평가를 위한 기존 벤치마크는 없다.\n' +
      '\n' +
      '이 작업에서 (i) 임상 관련 CXR 해석 태스크 8에 걸쳐 FM을 체계적인 비교가 가능하도록 하는 새로운 벤치마크인 _CheXinstruct_, 명령어 이미지 응답 삼중선이 있는 대규모 명령어-튜닝 데이터셋, (ii) CXR 해석을 위한 명령어-튜닝 다중 모드 LLM인 _CheXagent_ 개발, (iii) 큐레이팅 _CheXBench_에 의한 이러한 문제를 해결한다. 아래에서는 그림에 요약된 주요 기여도를 설명한다. 1, 그것은 능력 있고 강력한 CXR FM을 만드는 데 도움이 될 수 있습니다.\n' +
      '\n' +
      '1. _CheXinstruct_는 CXR을 해석하는 FM의 능력을 향상시키기 위해 설계된 6M 명령어-이미지-응답 삼중선이 있는 명령어-튜닝 데이터세트이다. 우리는 거친 이미지와 미세한 이미지 이해, 질문 응답 및 텍스트 생성을 포함한 카테고리에 걸쳐 34개의 작업과 65개의 고유한 데이터세트로부터 지침을 수집한다.\n' +
      '2. _CheXagent_는 이미지를 분석하고 텍스트를 이해하고 응답을 생성할 수 있는 8B 파라미터를 갖는 명령어-튜닝된 기반 모델이다. CheXagent를 개발하기 위한 우리의 방법론은 방사선 보고서를 이해할 수 있는 임상 LLM(1) 훈련(1), CXR을 읽을 수 있는 비전 인코더(2) 및 (3) 네트워크를 포함하여 비전 및 언어 방식을 모델링한다. 그런 다음 CheXinstruct의 데이터를 사용하여 지시 조정 작업을 수행한다.\n' +
      '3. _CheXbench_는 이미지 인식과 텍스트 이해의 두 가지 평가 축에 걸쳐 FM을 엄격하게 평가하도록 설계된 새로운 벤치마크이다. 7개의 CXR 데이터셋에 걸쳐 8개의 작업을 소개하고, 개폐형 다중 선택 예측과 개방형 텍스트 생성을 사용하여 성능을 평가한다.\n' +
      '\n' +
      '우리는 CheXbench를 사용하여 CheXagent와 6개의 이전 일반 도메인 및 의료 도메인 FM을 비교한다. 6가지 시각적 작업 중 CheXagent의 성능은 일반 영역 FM을 97.5%, 의료 영역 FM은 55.7% 초과한다. 2개의 텍스트 생성 작업에서 CheXagent는 5명의 전문가 방사선사의 자동화된 정량적 메트릭과 질적 메트릭을 통해 평가된 의료 텍스트를 제공한다. 우리는 모델 투명성을 개선하기 위해 성별, 인종 및 연령의 인구통계학적 요인에 걸친 잠재적 모델 편향 및 하이라이트 성능 격차를 추가로 평가한다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '### Foundation Models\n' +
      '\n' +
      '사용 가능한 데이터와 계산 자원의 급증은 단일 일반주의 모델로 광범위한 작업을 해결하는 데 다재다능한 FM의 생성을 가능하게 했다.\n' +
      '\n' +
      '많은 규모의 텍스트 데이터가 온라인으로 쉽게 접근할 수 있었기 때문에 FM의** 언어:** 유의성 있는 지층은 자연어 처리(NLP)에서 처음 나타났다. GPT-3 [10], ChatGPT, GPT-4[54], FLAN-T5[18], Llama-2[76] 77], Mistral 7B[35], PaLM-2[17]과 같은LLM은 프롬프트만을 사용하여 여러 텍스트 기반 작업에서 엑셀되어 임상 텍스트 요약[81]을 위한 인체 전문가를 능가하는 것과 같은 새로운 가능성을 가능하게 한다.\n' +
      '\n' +
      '**Vision:** 비전은 텍스트 대 이미지 생성 과제에 대해 스테이블 디퓨전[63], DALL-E[61; 60] 및 임젠[64]와 같은 FM을 제안하였다. 모델과 같은 부분 데이터베이스 모델(SAM) [38] 및 모델(SEEM) [98]이 복잡한 분할을 수행하기 위해 개발되었다. 시력에서 LLM과 유사한 스케일링을 달성하기 위해 ViT-22B[19], ViT[23]의 스케일업 버전을 대형 비전 인코더로 도입했다. 의료 영역에는 FM[49]을 트레이닝하기 위해 설계된 여러 데이터 세트가 있다.\n' +
      '\n' +
      '**V밀 언어:**은 언어와 비전과 같은 다양한 양식 간의 고유한 연결을 제시하며, 모델 간 감독은 CLIP [59]와 같이 많은 비전-언어 기반 모델(VLM)의 개발에 매우 중요했다. 플라밍고[1], 코카[94], 오웬-VL[5], BLIP[44], LLaVA[47], PaLI-X[14] 및 CogVLM[83]과 같은 여러 다른 대규모 VLM은 텍스트와 이미지를 모두 인식하고 이해할 수 있을 만큼 강력한 모델을 개발하는 데 중점을 두고 도입되었다. 이러한 모델의 성능을 평가하기 위해 멀티모달 대형 언어 모델 평가(MME) [25] 및 SEED-Bench[42]와 같은 벤치마크가 도입되었다. 이러한 벤치마크들은 어떤 데이터 누출도 피하기 위해 수동으로 생성된 명령어-응답 쌍을 사용하여 조심스럽게 큐레이팅된다.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '의료재단 모텔.\n' +
      '\n' +
      '바이오NLP[41], 바이오BERT[40], 바이오BERT[40], PubMedBERT[29], BioGPT[48], Med-PaLM[40] 및 Med-PaLM M[71]과 같은 생물 의학 언어 모델(LM)은 LM이 의학적 질문에 답하는 작업에 경쟁적으로 수행하기 위해 큐레이션 생물의학 체인에 미세 조정될 수 있음을 보여주었다. 구체적으로 메디-PaLM 2[71]는 의료 질문 답변에 대한 의사 수준의 성과에 대한 신속한 진전을 부각시키는 미국 의학 인식 검사(USMLE)의 양식에서 높은 점수를 얻었다. 의학은 본질적으로 여러 가지 방식을 포함하고 광범위한 작업을 다루기 때문에 다중 모달 FM은 이 분야에 특히 적합하다. LLAVA-Med[43], Med-Flamingo[51], Med-PaLM M[78]과 같은 작품은 영상의학, 피부과, 병리 등의 작업에 대한 영상 및 임상 텍스트와 같은 양식에 걸쳐 있는 일반주의적 모델을 제안했다. 라디오FM[88]은 2D 및 3D 영상 작업 모두에 대한 방사선학 기반 모델을 도입했다. XrayGPT[74]는 임상 텍스트를 사용하여 피지컬된 바이쿠나[16]와 메디CLIP[85] 시각적 인코더를 정렬했다. 편리한 버전의 스테이블 디퓨전인 로젠트는 방사선 사진 리포트 텍스트 프롬프트[13; 12]를 사용하여 CXR 이미지를 생성할 수 있다.\n' +
      '\n' +
      '미디어-PaLM M, Med-Flamingo, RadFM 및 LLAVA-Med는 모델이 다양한 이미지 양식의 데이터를 처리하는 데 권한을 부여하는 일반주의적 프레임워크를 확립하는 것을 목표로 한다. 대조적으로, CheXagent는 단일 모델 내에서 CXR과 관련된 여러 작업을 처리하는 데 탁월하도록 설계된다.\n' +
      '\n' +
      '최대 관련 작업.\n' +
      '\n' +
      '우리의 접근법은 다음 작품과 가장 유사하다. (i) BLIP-2[44]: 우리의 모델 아키텍처는 시력-언어 양식의 격차를 해소하기 위해 쿼리밍 트랜스포머(Q 전)를 사용하는 BLIP-2의 구조와 밀접하게 정렬되며, (ii) FLAN[86] 및 다중 구성 [90]: 기존 주석이 달린 데이터 세트에서 파생된 명령어 튜닝 데이터 세트의 개발은 이러한 작품에 의해 영감을 받는다.\n' +
      '\n' +
      '지원: 3 CheXinstruct:\n' +
      '\n' +
      '동기화 PX구조는 그림과 같은 광범위한 작업을 다루고자 한다. 훈련 CXR FM을 지원하기 위한 1(a) 이러한 과제는 (i) CXR을 이해하는 FM의 능력을 향상시키거나 (ii) 임상 의사 결정을 개선할 수 있다.\n' +
      '\n' +
      '디자이너 셰메케스트라는 4단계로 구성되어 있습니다.\n' +
      '\n' +
      '그림 2: CheXinstruct(Left)를 포함하는 데이터 세트 수집 및 작업이다. CXR 비전 인코더와 비전-언어 다리 교육을 통해 임상용 일반 LLM의 적응부터 다양한 CXR 과제(맞아요)에 대한 수업 조정의 마지막 단계에 이르기까지 CheXagent의 4단계 훈련 과정이 있다.\n' +
      '\n' +
      '우리는 CXR FM이 보유해야 할 필수 기술과 역량을 명시하는 1. 카베이트 레벨이다.\n' +
      '2. 태스크 레벨, 식별된 각 능력과 일치하는 광범위한 특정 작업을 요약하여 FM이 CXR의 맥락에서 달성할 수 있는 것에 대한 명확한 프레임워크를 제공한다.\n' +
      '3. 다양한 CXR 데이터 세트를 식별하고 분류하는 다타세트 레벨, 각각 특정 과제와 연관된다. 데이터 세트가 관련되고 지원하려는 과제에 적절하게 일치하도록 하는 데 중점을 둡니다.\n' +
      '4. 인서스 레벨, 여기서 우리는 각 데이터세트 내의 개별 인스턴스를 가장 세분화된 수준에서 정의한다. 각각의 인스턴스는 입력(CXR 이미지 등) 및 그 대응하는 레이블을 포함하며, 이는 FM을 트레이닝하고 평가하기 위한 기본 유닛을 형성한다.\n' +
      '\n' +
      '**T biodiversity**: CheXinstruct는 능력에 따라 5개의 과제 범주로 구성되어 있다.\n' +
      '\n' +
      'CXR에 대한 전반적인 이해, 예를 들어 뷰 분류[36] 및 질병 분류[84; 32; 34; 11; 69;]와 같은 CXR에 대한 전반적인 이해를 정의하는 혐기성 이미지 이해*.\n' +
      '이상 검출 [53; 58], 이상 접지 [8], 이물질 검출 [91]과 같은 CXR에 대한 국소적 이해를 정의하는*-곡물 이미지 이해도이다.\n' +
      'VQA(VQA) [97; 57], 개방형 VQA[7; 4], 차이 VQA[31], 텍스트 QA와 같은 CXR과 관련된 질문에 대응할 수 있는 능력을 정의하는* 질문 수정.\n' +
      '연구 결과에 대한 설명[21; 82; 56], 인상 생성[24], 연구 요약[15], 국소 소견 생성[36]을 포함하여 방사선학 보고서 섹션을 생성하는 능력을 정의하는*교과기이다.\n' +
      '* 미스셀러: 이 범주는 CXR FM에 중요한 잡 능력을, 예를 들어 보고서 평가[93; 50] 및 자연어 설명[37].1].1과 같은 CXR FM에 중요한 잡 능력을 정의한다.\n' +
      '\n' +
      '1: 과제들에 대한 추가 세부 사항은 부록 A에 나와 있다.\n' +
      '\n' +
      '앞서 언급한 과제의 분류로 기존 공개적으로 이용 가능한 데이터 세트를 수집하거나(ii) 기존 데이터 세트의 새로운 라벨로 데이터 세트를 큐레이팅하여 CheXinstruct를 만들었다.\n' +
      '\n' +
      '모든 데이터 세트에 대해, 우리는 별개의 작업을 공식화하기 위해 관련 정보를 도출한다. 모든 데이터 세트는 적용 가능한 경우 해당 공식 분할에 따라 분할된다.\n' +
      '\n' +
      '부츠 2: 데이터세트 소스의 모어 세부 정보는 부록 B에 나와 있다.\n' +
      '\n' +
      '[86]에서 영감을 받은***태스크 지도 창조**를 작성했으며, 각 과제별 10개의 지시 템플릿을 수동으로 작성함으로써 명령어를 생성하며, 지시 템플릿에는 장소 보유자(예: <IMAGE>, <QUESTION>, <OPTIONS>가 포함되어 있으며, 이는 지시 제거 사례를 생성할 때 특정 값으로 대체된다.\n' +
      '\n' +
      '발주 3: 주석자는 방사선 및 컴퓨터 비전(CV)에 있는 연구자들이다.\n' +
      '\n' +
      '마지막으로, 각 CheXinstruct 인스턴스는 이미지(또는 그렇지 않은 경우, 비이미지 기반 작업의 경우), 질문 및 답변으로 구성된 트리플트이다.\n' +
      '\n' +
      '*** 자료 분석*** 자료 분석***의 전체 문항 수를 포함하여 CheXinstruct의 전체 통계를 설명한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r r r} \\hline \\hline \\multirow{2}{*}{**Split**} & \\multirow{2}{*}{**Total**} & \\multicolumn{2}{c}{**Questions**} & \\multicolumn{2}{c}{**Images**} & \\multirow{2}{*}{**Reuse**} \\\\  & & **Unique** & & & **Unique** \\\\ \\hline Train & 6.1M & 1.9M & 3.2 & 9.3M & 1.1M & 8.6 \\\\ Val & 203.3K & 82.9K & 2.5 & 228.9K & 31.7K & 7.2 \\\\ Test & 188.3K & 72.8K & 2.6 & 233.7K & 49.5K & 4.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 1>은 데이터셋(재사용)에서 각 고유한 이미지의 평균 횟수 및 총 및 고유 횟수와 함께 문항 및 이미지의 수를 나타내는 CheXinstruct의 통계이다.\n' +
      '\n' +
      '표 1의 응답 쌍과 CXR은 CheXinstruct에서 6.1M 질문 응답 쌍이 있다. 또한, 하나의 데이터세트로부터의 이미지가 여러 태스크에 걸쳐 사용될 수 있기 때문에, 서로 다른 데이터세트들 중 잠재적인 "오버랩" 문제에 대한 직관을 제공하기 위해 추가 정보를 제시한다.\n' +
      '\n' +
      '부록 4: 데이터 분석에 대한 모음 세부 정보는 부록 C에서 보고된다.\n' +
      '\n' +
      'RXR FM에 이어 4번 체크요\n' +
      '\n' +
      '### Problem Setup\n' +
      '\n' +
      'CheXagent의 목적은 "참고" 이미지 \\(x_{I}\\) 및/또는 "읽기" 텍스트 \\(x_{T}\\)를 생성하고 "응답" \\(y\\)를 생성할 수 있는 모델이다. 이를 위해 비전 인코더 \\(\\mathcal{M}_{v}\\), 비전 언어 브릿지 \\(\\mathcal{M}_{b}\\), 언어 디코더 \\(\\mathcal{M}_{l}\\)의 세 가지 구성 요소를 소개한다. 따라서 원하는 모델의 궁극적인 제형입니다.\n' +
      '\n' +
      '\\[y=\\mathcal{M}_{l}(\\mathcal{M}_{b}(\\mathcal{M}_{v}(x_{I})),x_{T}). \\tag{1}\\]\n' +
      '\n' +
      '### Training CheXagent\n' +
      '\n' +
      '이 하위 섹션에서 그림 1(b)에 도시된 CheXagent의 네 가지 훈련 단계를 제시한다.\n' +
      '\n' +
      '**Stage 0: 임상 LLM** 위험 오픈 소스 생물의학 대형 언어 모델(LLM)은 바이오메가트론[70], GatorTron[92], 바이오GPT[48], 바이오미디어LM5 및 PMC-LLaMA[87]과 같이 존재한다. 이러한 모델은 임상 텍스트보다는 PubMed 중앙(PMC) 기사에 대해 주로 훈련된다. 이러한 격차를 해결하기 위해 일반 도메인 LLM을 적응시켜 임상 LLM 개발에 중점을 둔다. 우리의 시작점은 다양한 벤치마크에 걸쳐 입증된 강력한 추론 능력으로 선택된 Mistral-7B-v0.1 [35]이다.\n' +
      '\n' +
      '폐지 5: [국경신경부진 표면.코/스탄포드-crfm/바이오미디어LM] (국경신경부진 표면.코/스탄포드-crfm/BioMedLM)\n' +
      '\n' +
      '모델을 포괄적인 의료 및 임상 지식으로 추론하기 위해 (i) PMC 기사 추상화, (ii) MIMIC-IV의 방사선학 보고서, (iii) MIMIC-IV 배출 요약, (iv) 위키피디아의 의료 용어, (v) CheX구조에서 CXR의 5가지 별개의 텍스트 소스를 활용했다. 중요하게도, MIMIC-IV 데이터에 대해 데이터 유출을 방지하기 위해 MIMIC-CXR의 검증 및 테스트 세트의 일부인 모든 연구를 꼼꼼하게 배제한다.\n' +
      '\n' +
      '**Stage 1: [45]와 [94]의 작품에서 CXR** 드레이밍 영감에 대한 비전 인코더인 [din][94]에서는 다양한 시각적 사전 학습 목표, 즉 이미지-텍스트 대비(ITC)와 이미지 캡션(IC)을 사용하여 비전 인코더를 훈련시킨다. 우리의 모델 아키텍처는 [45]의 모델을 반영합니다. 훈련 목적을 위해 특히 MIMIC-CXR, PadC흉부 및 BIMCV-코로나19로부터 이미지-텍스트 쌍을 포함하는 데이터 세트를 활용하며 예비 연구에 따르면 [94]의 접근법과 유사한 ITC 및 IC 조합을 사용하는 것이 우리의 특정 맥락에서 향상된 성능을 산출한다.\n' +
      '\n' +
      '**Stage 2: 임상 LLM과 CXR 비전 인코더의 훈련에 따라 비전 언어 다리*** 트레이인, 우리는 가교 모델 \\(\\mathcal{M}_{b}\\) 개발에 중점을 둔다. 이 모델은 시각적 데이터를 해당 언어(의미) 공간에 매핑하도록 설계되었습니다. 훈련 동안 임상 LLM, \\(\\mathcal{M}_{l}\\) 및 CXR 비전 인코더, \\(\\mathcal{M}_{v}\\)를 모두 동결한다. 이 접근법은 이미지-텍스트 정렬 과정 동안 사전 지식의 재앙적 잊힘을 방지하는 데 중요하다. 트레이닝 \\(\\mathcal{M}_{b}\\)을 위해 우리는 학습용 이미지 캡션 목표를 적용하는 페이지 1과 동일한 데이터 세트를 사용한다.\n' +
      '\n' +
      '**Stage 3: 명령 튜닝** Upon 완료 단계 2, CXR 해석에 맞춘 멀티 모달 LLM을 얻는다. 이 단계에서 우리의 초점은 CheXinstruct 프레임워크 내의 다양한 작업에 대한 모델을 훈련하는 것으로 전환된다. 훈련 전에 (i) 평가 목적 전용 특정 작업-다타셋 쌍을 예약하고 (ii) 최적 데이터셋 비율을 결정하여 서로 다른 능력에 걸쳐 균형 잡힌 훈련을 보장하는 두 가지 핵심 측면을 고려한다. 먼저 OpenI, SLAKE 및 SIIM을 포함한 데이터 세트를 격리하여 보다 간소화된 평가 과정(SS5에서 논의된 것)을 용이하게 한다. 두 번째는 각 데이터셋의 품질과 다양성을 주의 깊게 평가하여 데이터셋 비율을 산출하는 것이다. 이 방법은 자동화된 데이터세트 선택에 대한 향후 탐색을 위한 여지를 남기고 밸런싱.6 이 교육은 다음 단어 예측 목적을 사용하여 수행되며 손실 계산은 답변에 국한된다.\n' +
      '\n' +
      '부츠 6: 데이터세트 비율에 대한 모어 세부 정보는 부록 D에서 찾을 수 있다.\n' +
      '\n' +
      '** 구현 디테일** 모델 아키텍처의 경우 비전 인코더에 EVA-CLIP-g [73]과 Qformer에 대한 BERT[22], 가교제의 선형층, LLM에 대한 미스트랄을 사용한다. 트레이닝 가능한 파라미터의 최적화는 다양한 단계에 걸쳐 구조화되어 있는데, (a) 전체 LLM이 스테이지 0에서 훈련되고 (b) 단계 1에서 비전 인코더와 전체 BERT 인코더의 LoRA 파라미터를 모두 훈련하고, (c) 단계 2는 비전-언어 다리를 훈련하는 것을 포함하며, (d) 단계 3에서는 가교체와 전체 LLM을 모두 훈련하는 데 중점을 둔다. 이 모든 단계에 걸친 최적화를 위해 각 단계가 자체 하이퍼 파라미터 7을 갖는 AdamW 최적화기를 사용한다.\n' +
      '\n' +
      '구획 7: 구현에 대한 모음 세부 사항은 부록 E에서 찾을 수 있다.\n' +
      '\n' +
      'CXR에 대한 FM을 평가하기 위한 5 CheXbench:A Bench\n' +
      '\n' +
      '이 절에서는 임상적으로 관련된 8개의 CXR 해석 과제에 걸쳐 FM의 체계적인 비교를 가능하게 하기 위한 평가 벤치마크인 CheXbench를 소개한다.\n' +
      '\n' +
      '### Benchmark Design\n' +
      '\n' +
      'CheXbench는 두 개의 평가 축으로 구성되어 있으며, CXR 해석의 중요한 측면을 평가하기 위해 제작된 이미지 인식 및 텍스트 이해이다. CheXbench 내의 평가는 CheXinstruct 테스트 세트.8의 특정 하위 집합에 대해 수행되며, 다음 섹션에서는 각 평가 축과 관련된 작업을 설명한다.\n' +
      '\n' +
      '부츠 8: 이것은 부록 F에 포괄적으로 자세히 설명되어 있다.\n' +
      '\n' +
      '** 평가 Axis 1 - 이미지 퍼셉션**: 먼저 FM이 CXR의 시각적 함량을 이해하는 능력을 평가하는 것을 목표로 한다. 우리의 목표는 (1) 다양한 데이터 분포로 일반화하는 FM의 능력을 평가하는 것이며 (2)에는 다양한 도전적이고 임상적으로 관련된 작업이 포함된다. 이를 위해 7개의 데이터 세트에 걸쳐 6개의 작업을 도입하며, 특히 잠재적인 데이터 유출을 피하기 위해 CheXagent 훈련에서 3개의 데이터세트(SIIM, SLAKE 및 OpenI)가 완전히 유지되었음을 주목한다. 일반 도메인 FM[42, 26]을 위해 설계된 사전 벤치마크에 따라, 이미지 및 질문이 FM 및 다중 옵션에 제기되는 다중 선택 형식을 사용한다. FM의 개방형 자유텍스트 출력은 평가하기 어렵기 때문에 대신 각 옵션과 관련된 로그 가능성 점수를 계산하고 가장 높은 점수를 가진 옵션을 응답으로 선택한다. 각 작업에 대해 정확도를 보고한다.\n' +
      '\n' +
      '* 뷰 분류(700개 샘플): CXR을 통해 FM은 이미징 뷰를 식별하는 작업을 수행한다. 우리는 4개의 가능한 옵션(AP, PA, 측방향 및 LL)으로 MIMIC-CXR 테스트 세트뿐만 아니라 3개의 가능한 옵션(AP, PA, 측방향 및 LL)으로 CheXpert 테스트 세트에 대한 뷰 분류를 수행한다.\n' +
      '*생물 질환 분류(433개 샘플): CXR 및 질병 라벨을 부착하면 FM은 이미지에 질병이 존재하는지 여부를 결정하는 작업을 수행한다. 우리는 CheXpert 테스트 세트에서 12개의 질병 라벨, RSNA 데이터세트의 1개의 질병 라벨 및 SIIM 데이터세트의 1개의 질병 라벨로 이진 질병 분류를 수행한다. 가능한 두 가지 옵션(예, 없음)이 있습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l l} \\hline \\hline\n' +
      '**Task** & \\multicolumn{3}{c}{**General-domain FMs**} & \\multicolumn{3}{c}{**Medical-domain FMs**} & \\multicolumn{1}{c}{**CheXagent**} \\\\  & & **BL-P2** & **Intertweller LLP** & **XrayCPT** & **MedTraining** & **RadFFM** & **LLAvAv-Med** & **(Ours)** \\\\ \\hline \\multirow{2}{*}{View Classification} & MIMIC-CXR & 28.8 & 25.3 & 24.0 & 25.0 & 28.5 & 23.8 & **97.5** \\\\  & CheXpert & 38.0 & 34.0 & 33.0 & 39.0 & 37.0 & 30.0 & **96.7** \\\\ \\multirow{2}{*}{Binary Disease Classification} & SIM & 53.0 & 54.0 & 50.0 & 50.0 & 50.0 & 49.0 & 64.0 \\\\  & BENA & 50.0 & 60.0 & 50.0 & 50.0 & 50.0 & 44.0 & **81.0** \\\\  & CheXpert & 51.5 & 53.2 & 51.5 & 48.5 & 55.8 & 47.6 & **76.0** \\\\ \\multirow{2}{*}{Single Disease Identification} & OpenI & 40.2 & 40.2 & 45.4 & 39.0 & 42.2 & 43.8 & 47.0 \\\\  & MIMIC-CXR & 25.6 & 22.6 & 24.1 & 25.6 & 27.2 & 26.7 & **30.3** \\\\  & CheXpert & 21.3 & 19.5 & 23.7 & 26.0 & 26.6 & 26.0 & **29.6** \\\\ \\multirow{2}{*}{Multi Disease Identification} & OpenI & 48.5 & 54.4 & 58.7 & 46.1 & 52.8 & 53.9 & 55.6 \\\\  & MIMIC-CXR & 30.0 & 25.3 & 39.0 & 14.7 & 22.3 & 28.7 & **55.3** \\\\  & CheXpert & 4.3 & 6.1 & 3.9 & 7.1 & 23.6 & 2.1 & **52.1** \\\\ \\multirow{2}{*}{Visual Question Answering} & Rad-Restructural & 41.2 & 42.4 & 38.6 & 45.5 & 48.5 & 34.9 & **57.1** \\\\  & SLAKE & 74.3 & 86.4 & 52.4 & 64.8 & 85.0 & 55.5 & 78.1 \\\\ \\hline \\multirow{2}{*}{Image-Text Reasoning} & OpenI & 47.9 & 52.6 & 52.4 & 54.7 & 54.0 & 45.8 & **59.0** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: CheXagent와 일반 도메인 및 의료 도메인 FM을 비교하는 이미지 인식과 관련된 작업에 대한 CheXbench의 평가 결과 1: 여러 CXR 데이터세트 상의 평가 결과. 각 작업에 대해 정확도를 보고한다.\n' +
      '\n' +
      '* 단일 질병 식별(864개 샘플): CXR에 따르면 FM은 이미지에 존재하는 질병을 식별하는 작업을 수행한다. 우리는 CheXpert 테스트 세트(전문가 방사선학자 주석에서 질병 라벨이 얻은 경우)와 OpenI(의료 대상 헤드링(MeSH) 코드에서 질병 라벨이 얻어지는 경우)로 단일 질병 식별을 구현한다. 각 질문과 관련된 4가지 가능한 옵션이 있으며 각 옵션에는 단일 질병 라벨(예: "폐렴")이 포함된다.\n' +
      '* 다중 질병 식별(1387개 샘플): CXR을 통해 FM은 이미지에 존재하는 다중 질병 세트를 식별하는 작업을 수행한다. CheXpert와 OpenI를 사용하여 다중 질병 분류를 구현합니다. 각 질문과 관련된 4가지 가능한 옵션이 있으며 각 옵션에는 다중 질병 세트(예: "폐렴, 흉막 삼출, 심뇌관")가 포함된다.\n' +
      '* 컴퓨터 질문-응답(1319개 샘플): 두 개의 표준 VQA 벤치마크에 걸쳐 FM을 평가하는데, 이는 SLAKE 및 Rad-절단이다. SLAKE는 두 가지 옵션(예와 아니)이 있는 문항으로 구성되어 있으며, Rad-Fi는 두 가지 옵션과 네 가지 옵션 사이에서 질문으로 구성된다.\n' +
      '* Image-Text Reasoning(380개 샘플): CXR을 통해 FM은 이미지에서 질병을 식별하는 작업을 수행한다. 이 과제는 단일 질병 분류 업무와 대조적으로, 각 질문은 위치 또는 중증도를 나타내는 단일 단어(예: "좌측 흉막 유출" 대 "우측 흉막 유출"으로 구별된 두 가지 도전 옵션과 관련이 있다. 오픈I 데이터셋으로 이미지-텍스트 추론을 구현합니다.\n' +
      '\n' +
      '*** 평가 결과 2-텍스트 이해**: CheXagent와 기준선 FM의 텍스트를 생성하고 요약하는 능력을 추가로 평가한다. 이를 위해 자동화 메트릭(ROUGE-L[46], CheXbert-Score[72], BERT-Score[95], RadGraph-Score[20], GPT-4)의 조합과 5명의 방사선 전문의의 인간 전문가 평가를 사용하여 개방형 반응을 평가한다.\n' +
      '\n' +
      '* 파이딩 섹션 세대: 이미지를 통해 FM은 방사선 보고서의 발견 구간을 생성하는 작업을 수행한다. 이 작업은 이상 유무와 같은 이미지의 주요 특징을 식별하는 것을 포함한다. 우리는 MIMIC-CXR으로 연구 구간 생성 작업을 구현한다. 기존 의료 FM은 종종 MIMIC-CXR에서 훈련되기 때문에 개인 데이터 세트의 모델도 평가한다.\n' +
      '* 파이팅 스마마르화: 영상의학 보고서의 연구 섹션에 따르면 FM은 핵심 관찰을 간결한 진술로 요약하는 작업을 수행한다. 이 작업은 이미지를 포함하지 않습니다. 우리는 MIMIC-CXR에 대한 피딩 스마레이팅을 평가한다.\n' +
      '\n' +
      '### Evaluation Results\n' +
      '\n' +
      '우리 연구에서 우리는 CheXbench를 사용하여 이전 연구[42]에서 최첨단 성능을 달성하는 두 개의 일반 도메인 지시-조정 FM, 구성BLIP 및 BLIP2에 대한 CheXagent를 비교한다. 또한 CheXagent와 XrayGPT, MedFlamingo, RadFM 및 LLAVA-Med[74; 51; 43; 88]의 4가지 의료용 FM을 비교한다. 이 비교는 일반 모델과 의료 특정 모델과 관련하여 CheXagent의 성과에 대한 포괄적인 이해를 제공하는 것을 목표로 한다.\n' +
      '\n' +
      '표 2는 평가 축 1. CheXagent의 6개 과제에 대한 결과를 이미지 인식 과제에 걸쳐 우수한 성능을 보여 일반 도메인 FM보다 평균 97.5%, 의료 FM보다 평균 55.7%의 개선을 달성했다. 우리는 CheXagent 공연에 대한 자세한 고장을 제공합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Size**} & \\multicolumn{2}{c}{**MIMIC-CXR**} \\\\  & & **Range-L** \\\\ \\hline Llama-2 & 7B & 20.3 \\\\ Vicuna & 7B & 21.5 \\\\ FLAN-7.5 & 11B & 42.85 \\\\ FLAN-U.2 & 20B & 42.1 \\\\ CheXagent & SB & 40.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 어미 결과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Size**} & \\multicolumn{2}{c}{**Prine Dataset**} & \\multicolumn{2}{c}{**MIMIC-CXR**} \\\\  & & **BERT-S** & **CheXbert-S** & **RadGraph-S** & **BERT-S** & **CheXbert-S** & **RadGraph-S** \\\\ \\hline MedFlamingo & SB & 8.5 & 2.7 & 1.7 & 10.4 & 2.2 & 2.2 \\\\ LLAVA-Med & 8B & 12.5 & 17.0 & 4.2 & 6.2 & 17.5 & 4.0 \\\\ RadFM & 14B & 35.7 & 12.7 & 5.1 & 48.7 & 17.5 & 10.9 \\\\ XmgGPT & SB & 40.1 & 23.4 & 9.0 & 44.0 & 24.2 & 11.2 \\\\ CheXagent & SB & **45.0** & **23.7** & **14.0** & **50.4** & **24.9** & **18.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: CheXagent와 다양한 메트릭을 사용하여 기준선 의료 도메인 FM을 비교하는 소견 생성 과제에 대한 CheXbench의 평가 축 2의 결과.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      'MIMIC-CXR에 대한 요약을 확인한다. 두 작업 모두에 대해 각 방사선사는 태스크 입력뿐만 아니라 A/B 비교(CheXagent _vs_의사)를 포함하는 무작위로 선택된 20개의 샘플 세트를 동일한 세트로 본다. 과제 출력의 출력. 그런 다음 방사선학자들은 완전성, 정확성 및 일치성에 걸쳐 5점 리커트 척도를 사용하여 이러한 출력을 등급화한다.\n' +
      '\n' +
      '그림 4의 연구 결과는 CheXagent가 연구 요약에 대해 의사와 비슷하고 인간 전문가의 텍스트가 발견 생성 측면에서 더 높은 점수를 달성한다는 것을 보여준다. 체자젠트는 정량적 메트릭에서 다른 비전-언어 모델을 능가하지만 이러한 발견은 추가 개선이 CheXagent와 인간 방사선학자 사이의 간격을 폐쇄하는 것을 촉진할 수 있음을 나타낸다.\n' +
      '\n' +
      '우리는 이제 미래 비전 언어 모델의 개선 기회를 더 잘 이해하기 위해 질적 연구 결과에 대해 논의한다. 그림 5에서 보는 바와 같이 의사의 보고서는 보통 과거 환자 연구를 참조하여 시간에 따른 변화를 추적한다. 따라서 단면 보고서에 대해 훈련되는 CheXagent의 능력은 훈련 중 종단 데이터를 통합하여 더욱 향상될 수 있다.\n' +
      '\n' +
      '우리는 또한 모델이 문맥에 관계없이 생성된 텍스트에서 3.5 cm의 균일한 거리 측정을 자주 생성한다는 것을 관찰했다. 이 문제는 CheXagent가 이미지로부터 물리적 거리나 다른 양을 직접 추정하도록 설계되지 않기 때문에 발생한다. 따라서 이러한 한계를 해결하는 것은 비전 언어 모델의 발전을 위한 또 다른 방법을 나타낸다.\n' +
      '\n' +
      '7개의 공정성 평가.\n' +
      '\n' +
      '최근 연구[28, 68]는 방사선학에서 사용되는 AI 모델에서 편향의 존재를 강조하여 다양한 개체군에 걸쳐 공평한 적용에 대한 우려를 제기한다. 공정성 평가를 위해 2명의 전문가 방사선사가 레이블 노이즈(65)를 피하기 위해 주석을 달았다. 총 159명의 독특한 피험자를 "노핀딩"과 "카디메갈리"로 표기했다. 균형 잡힌 검사 세트를 만들기 위해 균형 잡힌 질병 유병률을 보장하기 위해 교체 [27]를 준수합니다.\n' +
      '\n' +
      '그림 4: 방사선 전문의 5명을 대상으로 한 독서자 연구: 톱: CheXagent _vs._vs.__vs._ CheXagent _vs._의 발견 구간을 비교한 연구 설계. 3가지 속성(컬럼)에 걸쳐 두 가지 과제(화살)를 위한 인간 전문가이다. 바텀: 결과. 사람에 비해 CheXagent는 보고서 요약에서 패리티를 달성하는 반면, 결과는 CheXagent 보고 생성과 인간 수준 전문성의 격차를 보여준다.\n' +
      '\n' +
      '그림 5: 보고서 생성에 대한 의사(바닥)에 대한 CheXagent(톱)의 비교이다. 방사선학자 보고서는 종종 CheXagent가 사용할 수 없는 맥락인 과거 연구(푸르플)를 의미한다. 훈련 세트에서 이러한 참조의 존재는 아마도 정확한 보고서(**녹색***)에서 CheXagent의 오류(**red***)에 기여하여 시력-언어 모델에 대한 미래 작업을 동기화한다.\n' +
      '\n' +
      '하위 그룹 표현과 하위 그룹 표현. 우리는 F1-점수의 평균과 표준편차를 계산하기 위해 2000개의 부트스트랩 샘플을 생성한다.\n' +
      '\n' +
      '우리는 "이 흉부 X선에는 "예"와 "아니오"의 가능한 대답이 있는 심장 비대가 포함되어 있다"는 프롬프트로 심장 비대의 검출을 위한 모델 성능을 평가한다. 심결장의 검출은 조기 진단에 중요하며, 심장 질환 [2]의 치료는 인구통계학적 요인 [79]에 기초한 심장 질환 사망률의 상당한 차이를 나타내는 연구로 중요하다. 우리의 연구 결과는 그림 1에 나와 있다. 6개의 차이는 밝혀지지만 F1-점수는 암컷에 비해 수컷이 더 높으며 인종 그룹에 따라 다르며 모델은 블랙 하위 그룹에 대해 가장 잘 수행하고 아시아 하위 그룹에 대해 최악을 수행한다. 이것은 인종 전반에 걸쳐 심장 비대가 제시되는 고유한 차이를 반영할 수 있으며 테스트 세트에 포함된 14명의 흑인과 30명의 독특한 아시아 피험자의 제한된 샘플에 의해 영향을 받을 수 있다. 연령별로 보면, 이 모델은 0-65세 그룹에 비해 65세 연령대에 대해 더 잘 수행되며, 잠재적으로 노인 환자의 심장비대가 더 높은 유병률과 연령 관련 생리학적 차이로 인해 더 잘 수행된다. 이러한 결과는 기존 문헌[28]과 일치하며 의료에 사용되는 AI 모델에서 편향을 완화하는 데 지속적인 노력의 필요성을 강조한다. 이 문제를 해결하기 위한 효과적인 접근법은 더 크고 다양한 데이터 세트를 큐레이팅하고 있으며, 이는 다른 환자 인구 통계에서 더 대표적이고 공평한 모델을 개발하는 데 도움이 될 수 있다.\n' +
      '\n' +
      '## 8 Conclusion\n' +
      '\n' +
      '결론적으로, 우리의 작업은 자동화된 CXR 해석에 대한 진전을 나타낸다. 지시-튜닝 데이터세트인 (i) CheXinstruct, (ii) CheXagent, 8B-파라미터 비전-언어 FM을 소개하고 (iii) CheXbench를 통해 능력을 입증하며 7개 데이터세트 이상의 8개 과제를 포함한 벤치마킹 프레임워크인 (iii) CheXbench를 통해 능력을 보여준다. CheXagent는 일반 및 의료 도메인 LLM에 비해 시각적 인식과 텍스트 생성 작업의 개선을 달성하며 5명의 전문가 방사선 전문의에 의해 검증된다. 또한, 성별, 인종 및 연령에 걸친 공정성 분석에서는 의료 AI의 모델 투명성을 향상시키기 위한 지속적인 노력에 기여한다. CheXinstruct, CheXagent, CheXbench의 공개는 의료 AI 발전에 대한 우리의 의지를 강조할 뿐만 아니라 이 중요한 연구 영역에서 향후 발전을 위한 새로운 벤치마크를 설정한다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'MV는 국방부(NDSEG)의 대학원 교향상과 스탠퍼드대 심히시 장학사 프로그램이 지원한다. AC는 국립보건원(연구원 - R01 HL167974, R01 AR077604, R01 EB002524, R01 AR079431, P41 EB027060 및 계약 75N92020C00008, 75N92020C00021)과 GE 헬스케어와 필립스로부터 연구 지원을 받는다. 이 간행물에 보고된 연구는 부분적으로 75N92020C00008 및 75N92020C00021 계약에 따라 국립보건원 국립생명공학연구소(NIBIB)와 건강 연구 품질청의 #1R18HS028955를 부여함으로써 가능했다. 이 작업은 국립보건원 국립생명공학연구소(NIBIB)가 계약상 75N92020D00021에 따라 가능한 MIDRC(의료영상데이터자원센터)가 부분적으로 지원하고 있으며, 이 작업에 대한 계산 지원을 위해 안정성 AI에 의한 지원을 인정한다.\n' +
      '\n' +
      '잠재 모델 편향을 조사하는 심장 경련 분류에 대한 CheXagent 하위 그룹 성능 평가 그림 6: F1 스코어는 성별, 인종 그룹 및 연령 범주에 따라 다르다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. _Advances in Neural Information Processing Systems_, 35:23716-23736, 2022.\n' +
      '* [2] S. S. Alghamdi, I. Abdelaziz, M. Albadri, S. Alyanbaawi, R. Aljondi, and A. Tajaldeen. Study of cardiomegaly using chest x-ray. _Journal of Radiation Research and Applied Sciences_, 13(1):460-467, 2020.\n' +
      '* [3] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* [4] S. Bae, D. Kyung, J. Ryu, E. Cho, G. Lee, S. Kweon, J. Oh, L. Ji, E. I. Chang, T. Kim, et al. Ehrxqa: A multi-modal question answering dataset for electronic health records with chest x-ray images. _arXiv preprint arXiv:2310.18652_, 2023.\n' +
      '* [5] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.\n' +
      '* [6] S. Bannur, S. Hyland, Q. Liu, F. Perez-Garcia, M. Ilse, D. C. de Castro, B. Boecking, H. Sharma, K. Bouzid, A. Schwaighofer, et al. Ms-cxr-t: Learning to exploit temporal structure for biomedical vision-language processing, 2023.\n' +
      '* [7] A. Ben Abacha, S. A. Hasan, V. V. Datla, D. Demner-Fushman, and H. Muller. Vqa-med: Overview of the medical visual question answering task at imageclef 2019. In _Proceedings of CLEF (Conference and Labs of the Evaluation Forum) 2019 Working Notes_. 9-12 September 2019, 2019.\n' +
      '* [8] B. Boecking, N. Usuyama, S. Bannur, D. C. Castro, A. Schwaighofer, S. Hyland, M. Wetscherek, T. Naumann, A. Nori, J. Alvarez-Valle, et al. Making the most of text semantics to improve biomedical vision-language processing. In _European conference on computer vision_, pages 1-21. Springer, 2022.\n' +
      '* [9] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.\n' +
      '* [10] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [11] A. Bustos, A. Pertusa, J.-M. Salinas, and M. De La Iglesia-Vaya. Padchest: A large chest x-ray image dataset with multi-label annotated reports. _Medical image analysis_, 66:101797, 2020.\n' +
      '* [12] P. Chambon, C. Bluethgen, J.-B. Delbrouck, R. Van der Sluijs, M. Polacin, J. M. Z. Chaves, T. M. Abraham, S. Purohit, C. P. Langlotz, and A. Chaudhari. Roentgen: Vision-language foundation model for chest x-ray generation, 2022. URL [https://arxiv.org/abs/2211.12737](https://arxiv.org/abs/2211.12737).\n' +
      '* [13] P. Chambon, C. Bluethgen, C. P. Langlotz, and A. Chaudhari. Adapting pretrained vision-language foundational models to medical imaging domains, 2022.\n' +
      '* [14] X. Chen, J. Djolonga, P. Padlewski, B. Mustafa, S. Changpinyo, J. Wu, C. R. Ruiz, S. Goodman, X. Wang, Y. Tay, et al. Pali-x: On scaling up a multilingual vision and language model. _arXiv preprint arXiv:2305.18565_, 2023.\n' +
      '* [15] Z. Chen, M. Varma, X. Wan, C. Langlotz, and J.-B. Delbrouck. Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 469-484, Toronto, Canada, July 2023. Association for Computational Linguistics.\n' +
      '\n' +
      '* [16] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgrt quality. _See https://vicuna. lnsys. org (accessed 14 April 2023)_, 2023.\n' +
      '* [17] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n' +
      '* [18] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.\n' +
      '* [19] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. P. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In _International Conference on Machine Learning_, pages 7480-7512. PMLR, 2023.\n' +
      '* [20] J.-B. Delbrouck, P. Chambon, C. Bluethgen, E. Tsai, O. Almusa, and C. Langlotz. Improving the factual correctness of radiology report generation with semantic rewards. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 4348-4360, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.319. URL [https://aclanthology.org/2022.findings-emnlp.319](https://aclanthology.org/2022.findings-emnlp.319).\n' +
      '* [21] D. Demner-Fushman, S. Antani, M. Simpson, and G. R. Thoma. Design and development of a multimodal biomedical information retrieval system. _Journal of Computing Science and Engineering_, 6(2):168-177, 2012.\n' +
      '* [22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, 2019.\n' +
      '* [23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* [24] S. Feng, D. Azzollini, J. S. Kim, C.-K. Jin, S. P. Gordon, J. Yeoh, E. Kim, M. Han, A. Lee, A. Patel, et al. Curation of the candid-ptx dataset with free-text reports. _Radiology: Artificial Intelligence_, 3(6):e210136, 2021.\n' +
      '* [25] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu, W. Lin, J. Yang, X. Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023.\n' +
      '* [26] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, Sept. 2021. URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).\n' +
      '* [27] B. Glocker, C. Jones, M. Bernhardt, and S. Winzeck. Algorithmic encoding of protected characteristics in chest x-ray disease detection models. _Ebiomedicine_, 89, 2023.\n' +
      '* [28] B. Glocker, C. Jones, M. Roschewitz, and S. Winzeck. Risk of bias in chest radiography deep learning foundation models. _Radiology: Artificial Intelligence_, 5(6):e230060, 2023.\n' +
      '* [29] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, and H. Poon. Domain-specific language model pretraining for biomedical natural language processing. _ACM Transactions on Computing for Healthcare (HEALTH)_, 3(1):1-23, 2021.\n' +
      '* [30] G. Holste, S. Wang, A. Jaiswal, Y. Yang, M. Lin, Y. Peng, and A. Wang. Cxr-lt: Multi-label long-tailed classification on chest x-rays. _PhysioNet_, 2023.\n' +
      '\n' +
      '* [31] X. Hu, L. Gu, Q. An, M. Zhang, L. Liu, K. Kobayashi, T. Harada, R. M. Summers, and Y. Zhu. Expert knowledge-aware image difference graph representation learning for difference-aware medical visual question answering. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 4156-4165, 2023.\n' +
      '* [32] J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund, B. Haghgoo, R. Ball, K. Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 590-597, 2019.\n' +
      '* [33] L. Iyeke, R. Moss, R. Hall, J. Wang, L. Sandhu, B. Appold, E. Kalontar, D. Menoudakos, M. Rammarine, S. P. LaVine, et al. Reducing unnecessary \'admission\'chest x-rays: An initiative to minimize low-value care. _Cureus_, 14(10), 2022.\n' +
      '* [34] S. Jaeger, S. Candemir, S. Antani, Y.-X. J. Wang, P.-X. Lu, and G. Thoma. Two public chest x-ray datasets for computer-aided screening of pulmonary diseases. _Quantitative imaging in medicine and surgery_, 4(6):475, 2014.\n' +
      '* [35] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* [36] A. E. Johnson, T. J. Pollard, N. R. Greenbaum, M. P. Lungren, C.-y. Deng, Y. Peng, Z. Lu, R. G. Mark, S. J. Berkowitz, and S. Horng. Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. _arXiv preprint arXiv:1901.07042_, 2019.\n' +
      '* [37] M. Kayser, C. Emde, O.-M. Camburu, G. Parsons, B. Papiez, and T. Lukasiewicz. Explaining chest x-ray pathologies in natural language. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 701-713. Springer, 2022.\n' +
      '* [38] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* [39] J. J. Lau, S. Gayen, A. Ben Abacha, and D. Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. _Scientific data_, 5(1):1-10, 2018.\n' +
      '* [40] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. _Bioinformatics_, 36(4):1234-1240, 2020.\n' +
      '* [41] P. Lewis, M. Ott, J. Du, and V. Stoyanov. Pretrained language models for biomedical and clinical tasks: understanding and extending the state-of-the-art. In _Proceedings of the 3rd Clinical Natural Language Processing Workshop_, pages 146-157, 2020.\n' +
      '* [42] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. _arXiv preprint arXiv:2307.16125_, 2023.\n' +
      '* [43] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. _arXiv preprint arXiv:2306.00890_, 2023.\n' +
      '* [44] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, pages 12888-12900. PMLR, 2022.\n' +
      '* [45] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [46] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pages 74-81, 2004.\n' +
      '* [47] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '\n' +
      '* [48] R. Luo, L. Sun, Y. Xia, T. Qin, S. Zhang, H. Poon, and T.-Y. Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. _Briefings in Bioinformatics_, 23(6): bbac409, 2022.\n' +
      '* [49] X. Mei, Z. Liu, P. M. Robson, B. Marinelli, M. Huang, A. Doshi, A. Jacobi, C. Cao, K. E. Link, T. Yang, et al. Radimagenet: an open radiologic deep learning research dataset for effective transfer learning. _Radiology: Artificial Intelligence_, 4(5):e210315, 2022.\n' +
      '* [50] Y. Miura, Y. Zhang, E. Tsai, C. Langlotz, and D. Jurafsky. Improving factual completeness and consistency of image-to-text radiology report generation. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5288-5304, 2021.\n' +
      '* [51] M. Moor, Q. Huang, S. Wu, M. Yasunaga, C. Zakka, Y. Dalmia, E. P. Reis, P. Rajpurkar, and J. Leskovec. Med-flamingo: a multimodal medical few-shot learner. _arXiv preprint arXiv:2307.15189_, 2023.\n' +
      '* [52] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. _arXiv preprint arXiv:2306.02707_, 2023.\n' +
      '* [53] H. Q. Nguyen, K. Lam, L. T. Le, H. H. Pham, D. Q. Tran, D. B. Nguyen, D. D. Le, C. M. Pham, H. T. Tong, D. H. Dinh, et al. Vindr-cxr: An open dataset of chest x-rays with radiologist\'s annotations. _Scientific Data_, 9(1):429, 2022.\n' +
      '* [54] OpenAI. Gpt-4 technical report, 2023.\n' +
      '* [55] M. Pavlova, T. Tuinstra, H. Aboutalebi, A. Zhao, H. Gunraj, and A. Wong. Covidx ccr-3: a large-scale, open-source benchmark dataset of chest x-ray images for computer-aided covid-19 diagnostics. _arXiv preprint arXiv:2206.03671_, 2022.\n' +
      '* [56] O. Pelka, S. Koitka, J. Ruckert, F. Nensa, and C. M. Friedrich. Radiology objects in context (roco): a multimodal image dataset. In _Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis_, pages 180-189. Springer, 2018.\n' +
      '* [57] C. Pellegrini, M. Keicher, E. Ozsoy, and N. Navab. Rad-restruct: A novel vqa benchmark and method for structured radiology reporting. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 409-419. Springer, 2023.\n' +
      '* [58] H. H. Pham, T. T. Tran, and H. Q. Nguyen. Vindr-pcxr: An open, large-scale pediatric chest x-ray dataset for interpretation of common thoracic diseases. _PhysioNet_, 2022.\n' +
      '* [59] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [60] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.\n' +
      '* [61] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* [62] E. P. Reis, J. P. de Paiva, M. C. da Silva, G. A. Ribeiro, V. F. Paiva, L. Bulgarelli, H. M. Lee, P. V. Santos, V. M. Brito, L. T. Amaral, et al. Brax, brazilian labeled chest x-ray dataset. _Scientific Data_, 9(1):487, 2022.\n' +
      '* [63] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '\n' +
      '* [64] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* [65] A. Saporta, X. Gui, A. Agrawal, A. Pareek, S. Q. Truong, C. D. Nguyen, V.-D. Ngo, J. Seekins, F. G. Blankenberg, A. Y. Ng, et al. Benchmarking saliency methods for chest x-ray interpretation. _Nature Machine Intelligence_, 4(10):867-878, 2022.\n' +
      '* [66] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.\n' +
      '* [67] J. C. Seah, C. H. Tang, Q. D. Buchlak, X. G. Holt, J. B. Wardman, A. Aimoldin, N. Esmaili, H. Ahmad, H. Pham, J. F. Lambert, et al. Effect of a comprehensive deep-learning model on the accuracy of chest x-ray interpretation by radiologists: a retrospective, multireader multicase study. _The Lancet Digital Health_, 3(8):e496-e506, 2021.\n' +
      '* [68] L. Seyred-Kalantari, H. Zhang, M. B. McDermott, I. Y. Chen, and M. Ghassemi. Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations. _Nature medicine_, 27(12):2176-2182, 2021.\n' +
      '* [69] G. Shih, C. C. Wu, S. S. Halabi, M. D. Kohli, L. M. Prevedello, T. S. Cook, A. Sharma, J. K. Amorosa, V. Arteaga, M. Galperin-Aizenberg, et al. Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia. _Radiology: Artificial Intelligence_, 1(1):e180041, 2019.\n' +
      '* [70] H.-C. Shin, Y. Zhang, E. Bakhturina, R. Puri, M. Patwary, M. Shoeybi, and R. Mani. Biomegatron: Larger biomedical domain language model. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 4700-4706, 2020.\n' +
      '* [71] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal, et al. Towards expert-level medical question answering with large language models. _arXiv preprint arXiv:2305.09617_, 2023.\n' +
      '* [72] A. Smit, S. Jain, P. Rajpurkar, A. Pareek, A. Y. Ng, and M. Lungren. Combining automatic labelers and expert annotations for accurate radiology report labeling using bert. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1500-1519, 2020.\n' +
      '* [73] Q. Sun, Y. Fang, L. Wu, X. Wang, and Y. Cao. Eva-clip: Improved training techniques for clip at scale. _arXiv preprint arXiv:2303.15389_, 2023.\n' +
      '* [74] O. Thawkar, A. Shaker, S. S. Mullappilly, H. Cholakkal, R. M. Anwer, S. Khan, J. Laaksonen, and F. S. Khan. Xraygpt: Chest radiographs summarization using medical vision-language models. _arXiv preprint arXiv:2306.07971_, 2023.\n' +
      '* [75] E. Tiu, E. Talius, P. Patel, C. P. Langlotz, A. Y. Ng, and P. Rajpurkar. Expert-level detection of pathologies from unannotated chest x-ray images via self-supervised learning. _Nature Biomedical Engineering_, 6(12):1399-1406, 2022.\n' +
      '* [76] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [77] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [78] T. Tu, S. Azizi, D. Driess, M. Schaekermann, M. Amin, P.-C. Chang, A. Carroll, C. Lau, R. Tanno, I. Ktena, et al. Towards generalist biomedical ai. _arXiv preprint arXiv:2307.14334_, 2023.\n' +
      '\n' +
      '* [79] M. Van Dyke, S. Greer, E. Odom, L. Schieb, A. Vaughan, M. Kramer, and M. Casper. Heart disease death rates among blacks and whites aged 35 years--united states, 1968-2015. _MMWR Surveillance Summaries_, 67(5):1, 2018.\n' +
      '* [80] D. Van Veen, C. Van Uden, M. Attias, A. Pareek, C. Bluethgen, M. Polacin, W. Chiu, J.-B. Delbrouck, J. Zambrano Chaves, C. Langlotz, A. Chaudhari, and J. Pauly. RadAdapt: Radiology report summarization via lightweight domain adaptation of large language models. In D. Demner-fushman, S. Ananiadou, and K. Cohen, editors, _The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks_, pages 449-460, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.bionlp-1.42. URL [https://aclanthology.org/2023.bionlp-1.42](https://aclanthology.org/2023.bionlp-1.42).\n' +
      '* [81] D. Van Veen, C. Van Uden, L. Blankemeier, J.-B. Delbrouck, A. Aali, C. Bluethgen, A. Pareek, M. Polacin, W. Collins, N. Ahuja, et al. Clinical text summarization: Adapting large language models can outperform human experts. _arXiv preprint arXiv:2309.07430_, 2023.\n' +
      '* [82] M. D. L. I. Vaya, J. M. Saborit, J. A. Montell, A. Pertusa, A. Bustos, M. Cazorla, J. Galant, X. Barber, D. Orozco-Beltran, F. Garcia-Garcia, et al. Bimcv covid-19+: a large annotated dataset of rx and ct images from covid-19 patients. _arXiv preprint arXiv:2006.01174_, 2020.\n' +
      '* [83] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al. Cogvlm: Visual expert for pretrained language models. _arXiv preprint arXiv:2311.03079_, 2023.\n' +
      '* [84] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2097-2106, 2017.\n' +
      '* [85] Z. Wang, Z. Wu, D. Agarwal, and J. Sun. Medclip: Contrastive learning from unpaired medical images and text. _arXiv preprint arXiv:2210.10163_, 2022.\n' +
      '* [86] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. In _International Conference on Learning Representations_, 2021.\n' +
      '* [87] C. Wu, W. Lin, X. Zhang, Y. Zhang, Y. Wang, and W. Xie. Pmc-llama: Towards building open-source language models for medicine. _arXiv preprint arXiv:2305.10415_, 2023.\n' +
      '* [88] C. Wu, X. Zhang, Y. Zhang, Y. Wang, and W. Xie. Towards generalist foundation model for radiology. _arXiv preprint arXiv:2308.02463_, 2023.\n' +
      '* [89] S. Xu, L. Yang, C. Kelly, M. Sieniek, T. Kohlberger, M. Ma, W.-H. Weng, A. Kiraly, S. Kazemzadeh, Z. Melamed, et al. Elixr: Towards a general purpose x-ray artificial intelligence system through alignment of large language models and radiology vision encoders. _arXiv preprint arXiv:2308.01317_, 2023.\n' +
      '* [90] Z. Xu, Y. Shen, and L. Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. _arXiv preprint arXiv:2212.10773_, 2022.\n' +
      '* [91] Z. Xue, S. Candemir, S. Antani, L. R. Long, S. Jaeger, D. Demner-Fushman, and G. R. Thoma. Foreign object detection in chest x-rays. In _2015 IEEE international conference on bioinformatics and biomedicine (BIBM)_, pages 956-961. IEEE, 2015.\n' +
      '* [92] X. Yang, A. Chen, N. PourNejatian, H. C. Shin, K. E. Smith, C. Parisien, C. Compas, C. Martin, A. B. Costa, M. G. Flores, et al. A large language model for electronic health records. _NPJ Digital Medicine_, 5(1):194, 2022.\n' +
      '* [93] F. Yu, M. Endo, R. Krishnan, I. Pan, A. Tsai, E. P. Reis, E. K. U. N. Fonseca, H. M. H. Lee, Z. S. H. Abad, A. Y. Ng, et al. Evaluating progress in automatic chest x-ray radiology report generation. _Patterns_, 4(9), 2023.\n' +
      '\n' +
      '* [94] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive captioners are image-text foundation models. _arXiv preprint arXiv:2205.01917_, 2022.\n' +
      '* [95] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. Bertscore: Evaluating text generation with bert. In _International Conference on Learning Representations_, 2019.\n' +
      '* [96] X. Zhang, C. Wu, Y. Zhang, W. Xie, and Y. Wang. Knowledge-enhanced visual-language pre-training on chest radiology images. _Nature Communications_, 14(1):4542, 2023.\n' +
      '* [97] X. Zhang, C. Wu, Z. Zhao, W. Lin, Y. Zhang, Y. Wang, and W. Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. _arXiv preprint arXiv:2305.10415_, 2023.\n' +
      '* [98] X. Zou, J. Yang, H. Zhang, F. Li, L. Li, J. Gao, and Y. J. Lee. Segment everything everywhere all at once. _arXiv preprint arXiv:2304.06718_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      'GPT-4 평가의 프롬프트.\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      '두 명의 AI 보조자가 생성한 방사선 보고에 대한 피드백을 방사선사가 작성한 참조 보고서와 비교하여 요청하고자 합니다.\n' +
      '\n' +
      '[Reference Report]\n' +
      '\n' +
      '[reference]\n' +
      '\n' +
      '[ 참조 보고서 중]][ 참조 보고서 중]]\n' +
      '\n' +
      '[Assistant 1]\n' +
      '\n' +
      '[report1]\n' +
      '\n' +
      '[Assistant 1]\n' +
      '\n' +
      '[Assistant 2]\n' +
      '\n' +
      '[report2]\n' +
      '\n' +
      '[예상 2]의 종료]\n' +
      '\n' +
      '[Requirements]\n' +
      '\n' +
      '1. 보고서 길이는 중요하지 않습니다.\n' +
      '\n' +
      '2. 보고서 양식은 중요하지 않습니다.\n' +
      '\n' +
      '3. 특히 양성 소견(즉, 질병)에 임상 정확도가 중요하다.\n' +
      '\n' +
      '따라서 길이와 스타일 대신 임상 정확도에 집중해 주시기 바랍니다.\n' +
      '\n' +
      '[요청서 종료]\n' +
      '\n' +
      '생성된 보고서의 정확도를 비교하십시오. 보조자 1이 "보다 더 낫다", "더 이상의 말", 또는 "평등한" 보조자 2인지 알려주셔야 합니다. 먼저 생성된 보고서와 참조 보고서를 비교하여 주어진 요구 사항에 따라 어느 것이 더 많은지 분석하십시오. 마지막 선에서는 "보조자 1이 어시스턴트 2보다 낫다", "보조자 1이 어시스턴트 2보다 나쁘다", "저항자 1이 어시스턴트 2와 같다"로부터 단 하나의 레이블만 포함하는 단일 라인을 출력해 주시기 바랍니다.\n' +
      '\n' +
      '그림 8: GPT-4 평가 프롬프트.\n' +
      '\n' +
      '그림 7: CheXbench 이미지 인식 작업의 실시예 : CheXbench는 FM이 흉부 X선을 해석하는 능력을 평가하는 6개의 다중 선택 작업을 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline\n' +
      '**Capability** & **Task** & **Dataset** & **Description** \\\\ \\hline \\multirow{8}{*}{Cune-grained Image Understanding} & \\multicolumn{3}{c}{ConvConv14} \\\\  & \\multicolumn{3}{c}{CNNCapNet} \\\\  & \\multicolumn{3}{c}{ShML-CSR} \\\\  & \\multicolumn{3}{c}{ResNet} \\\\  & \\multicolumn{3}{c}{ResNet} \\\\  & \\multicolumn{3}{c}{ResNet} \\\\  & \\multicolumn{3}{c}{ConvConv14} \\\\  & \\multicolumn{3}{c}{ResNet} \\\\  & \\mul\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Task-Dataset Pair** & **Sampling Ratio** \\\\ \\hline (Named Entity Recognition) RadGraph & 10.00 \\\\ (Abnormality Grounding) VinDr-CXR & 1.00 \\\\ (Abnormality Grounding) VinDr-PCXR & 1.00 \\\\ (Caption Generation) ROCO & 1.00 \\\\ (Close-Ended VQA) PMC-VQA & 1.00 \\\\ (Close-Ended VQA) VQA-RAD & 1.00 \\\\ (Foreign Object Detection) Object-CXR & 1.00 \\\\ (Grounded Captioning) MS-CXR & 1.00 \\\\ (Grounded Diagnosis) MS-CXR & 1.00 \\\\ (Grounded Diagnosis) VinDr-CXR & 1.00 \\\\ (Grounded Diagnosis) VinDr-PCXR & 1.00 \\\\ (Grounded Phase Extraction) MS-CXR & 1.00 \\\\ (Image Classification) COVID-CXR-3 & 1.00 \\\\ (Image Classification) NLM-TB & 1.00 \\\\ (Image Classification) RSNA & 1.00 \\\\ (Impression Generation) Candid & 1.00 \\\\ (Open-Ended VQA) MedVQA-2019 & 1.00 \\\\ (Open-Ended VQA) PMC-VQA & 1.00 \\\\ (Open-Ended VQA) VQA-RAD & 1.00 \\\\ (Phrase Grounding) MS-CXR & 1.00 \\\\ (Pneumothorax Segmentation) Candid & 1.00 \\\\ (Progression Findings Generation) MIMIC-CXR & 1.00 \\\\ (Progression Impression Generation) MIMIC-CXR & 1.00 \\\\ (Text QA) RadQA & 1.00 \\\\ (View Matching) MIMIC-CXR & 0.50 \\\\ (Findings Generation) MIMIC-CXR-Struct & 0.40 \\\\ (Impression Generation) MIMIC-CXR-Struct & 0.30 \\\\ (Natural Language Explanation) MIMIC-NLE & 0.30 \\\\ (Report Generation) BIMC-COVID-D19 & 0.25 \\\\ (Findings Summarization) MIMIC-III & 0.20 \\\\ (Image Classification) ChestXray14 & 0.20 \\\\ (Close-Ended VQA) MIMIC-CXR-VQA & 0.10 \\\\ (Findings Generation) MIMIC-CXR & 0.10 \\\\ (Findings Summarization) MIMIC-CXR & 0.10 \\\\ (Image Classification) Brax & 0.10 \\\\ (Image Classification) CheXpert & 0.10 \\\\ (Image Classification) PadChest & 0.10 \\\\ (Image-Text Matching) ROCO & 0.10 \\\\ (Image-Text Selection) ROCO & 0.10 \\\\ (Impression Generation) MIMIC-CXR & 0.10 \\\\ (Report Generation) PadChest & 0.10 \\\\ (View Classification) MIMIC-CXR & 0.10 \\\\ (Image Classification) MIMIC-CXR & 0.05 \\\\ (Open-Ended VQA) MIMIC-CXR-VQA & 0.05 \\\\ (Local Findings Generation) MIMIC-CXR-Struct & 0.02 \\\\ (Local Impression Generation) MIMIC-CXR-Struct & 0.02 \\\\ (Text Instructions) OpenOrca & 0.02 \\\\ (Difference VQA) MIMIC-Diff-VQA & 0.01 \\\\ (Image Classification) CXR-LT & 0.01 \\\\ (Image-Text Matching) MIMIC-CXR & 0.01 \\\\ (Image-Text Selection) MIMIC-CXR & 0.01 \\\\ (Abnormality Detection) VinDr-CXR & 0.00 \\\\ (Abnormality Detection) VinDr-PCXR & 0.00 \\\\ (Chest Tube Segmentation) Candid & 0.00 \\\\ (Close-Ended VQA) Rad-Restruct & 0.00 \\\\ (Close-Ended VQA) SLAKE & 0.00 \\\\ (Open-Ended VQA) Rad-Restruct & 0.00 \\\\ (Open-Ended VQA) SLAKE & 0.00 \\\\ (Phrase Extraction and Grounding) MS-CXR & 0.00 \\\\ (Pneumothorax Segmentation) SIIM & 0.00 \\\\ (Rib Fracture Segmentation) Candid & 0.00 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: CheXagent의 3단계 훈련에서 각 데이터셋의 샘플링 비율은 표 7이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Configuration** & **Stage 0** & **Stage 1** & **Stage 2** & **Stage 3** \\\\ \\hline ViT init. & - & EVA01-CLIP-g-14-plus & ViT Stage 1 & ViT Stage 2 \\\\ LIM init. & Mistral-7B-v0.1 & - & LLM Stage 0 & LLM Stage 2 \\\\ Qformer init. & - & BERT Base & Qformer Stage 1 & Qformer Stage 2 \\\\ Image resolution & - & 448\\({}^{2}\\) & 448\\({}^{2}\\) & 448\\({}^{2}\\) \\\\ ViT sequence length & - & 1024 & 1024 & 1024 \\\\ LLM sequence length & 2048 & - & 512 & 512 \\\\ Learnable query numbers & - & 128 & 128 & 128 \\\\ Global batch size & 2048 & 256 & 2048 & 512 \\\\ Optimizer & & AdamW & & \\\\ Optimizer hyperparameter & & \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.999\\), \\(eps=1e-8\\) & & \\\\ Peak learning rate & \\(5e-6\\) & \\(1e-4\\) & \\(1e-4\\) & \\(1e-6\\) \\\\ Minimum learning rate & \\(5e-7\\) & \\(1e-5\\) & \\(1e-5\\) & \\(1e-7\\) \\\\ Learning rate schedule & & cosine & & \\\\ Weight decay & & 0.05 & & \\\\ Gradient clip & & 1.0 & & \\\\ Numerical precision & & bf16 & & \\\\ DeepSpeed & ZeRO stage 2 & - & & - & ZeRO stage 2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: CheXagent의 훈련 하이퍼파라미터.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c} \\hline \\hline\n' +
      '**Task** & **Dataset** & **Num. Samples** & **Num. Options** \\\\ \\hline \\multirow{2}{*}{View Classification} & MIMIC-CXR & 400 & 4 \\\\  & CheXpert & 300 & 3 \\\\ \\hline \\multirow{2}{*}{Binary Disease Classification} & SIIM & 100 & 2 \\\\  & RSNA & 100 & 2 \\\\  & CheXpert & 233 & 2 \\\\ \\hline \\multirow{2}{*}{Single Disease Identification} & OpenI & 500 & 4 \\\\  & MIMIC-CXR & 195 & 4 \\\\  & CheXpert & 169 & 4 \\\\ \\hline \\multirow{2}{*}{Multi Disease Identification} & OpenI & 807 & 4 \\\\  & MIMIC-CXR & 300 & 4 \\\\  & CheXpert & 280 & 4 \\\\ \\hline \\multirow{2}{*}{Visual Question Answering} & Rad-Restruct & 899 & 2-4 \\\\  & SLAKE & 420 & 2 \\\\ \\hline Image-Text Reasoning & OpenI & 380 & 2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: CheXbench(평가 Axis 1)의 이미지 인식 과제 통계이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>