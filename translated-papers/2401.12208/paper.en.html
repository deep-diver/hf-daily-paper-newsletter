<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# CheXagent: Towards a Foundation Model\n' +
      '\n' +
      'for Chest X-Ray Interpretation\n' +
      '\n' +
      'Zhihong Chen\\({}^{1*}\\)  Maya Varma\\({}^{1*}\\)  Jean-Benoit Delbrouck\\({}^{1*}\\)  Magdalini Paschali\\({}^{1}\\)\n' +
      '\n' +
      '**Louis Blankemeier\\({}^{1}\\)  Dave Van Veen\\({}^{1}\\)  Jeya Maria Jose Valanarasu\\({}^{1}\\)  Alaa Youssef\\({}^{1}\\)**\n' +
      '\n' +
      '**Joseph Paul Cohen\\({}^{1\\dagger}\\)  Eduardo Pontes Reis\\({}^{1}\\)  Emily B. Tsai\\({}^{1}\\)  Andrew Johnston\\({}^{1}\\)**\n' +
      '\n' +
      '**Cameron Olsen\\({}^{1}\\)  Tanishq Mathew Abraham\\({}^{2}\\)  Sergios Gatidis\\({}^{1}\\)**\n' +
      '\n' +
      '**Akshay S. Chaudhari\\({}^{1}\\)  Curtis Langlotz\\({}^{1}\\)**\n' +
      '\n' +
      '\\({}^{1}\\)Stanford University \\({}^{2}\\)Stability AI\n' +
      '\n' +
      '{zhihongc,mvarma2,jbdel,paschali,akshaysc,langlotz}@stanford.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes. However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation. In this work, we address these challenges by first introducing _CheXinstruct_ - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets. We then present _CheXagent_ - an instruction-tuned FM capable of analyzing and summarizing CXRs. To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities. Finally, we introduce _CheXbench_ - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks. Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities. Our project is at [https://stanford-aimi.github.io/chexagent.html](https://stanford-aimi.github.io/chexagent.html).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Foundation models (FMs) have recently emerged as a powerful class of models capable of performing a diverse range of reasoning and comprehension tasks [9]. The rise of FMs presents a major opportunity to re-imagine complex healthcare workflows that commonly require posing multi-faceted questions from inherently multi-modal data. One particular clinical workflow is the analysis of medical imaging data. Take for example chest X-ray (CXR) interpretation - the most common medical imaging study, with 70+ million CXRs performed annually in the US [33]. Here, radiologists interpret hundreds of images daily, translate imaging insights into textual descriptions, and summarize image findings succinctly to other clinicians, while maximizing accuracy and minimizing bias. Moreover, patient-facing tasks for CXRs might include answering clarifying questions about these generated findings. A CXR FM capable of automating or increasing the efficiency of these tasks can substantially improve clinical decision-making as well as patient satisfaction and outcomes [67; 75; 96].\n' +
      '\n' +
      'High-quality CXR FMs must bridge the gap between vision and language. Most prior approaches for building vision-language FMs focus on natural image settings, where high-performing methods align image encoders with pretrained large language models (LLMs). Such instruction-tuned multimodal LLMs demonstrate superior capabilities across a range of perception and generation tasks. However, developing instruction-tuned multimodal LLMs for medical imaging is challenging for the following reasons:\n' +
      '\n' +
      '1. _There is a lack of vision-language medical imaging datasets_. Instruction-tuned multimodal LLMs require large-scale, diverse training datasets with data triplets consisting of instructions, images, and answers. Whereas existing approaches in the natural image domain leverage millions of training samples from datasets like LAION-5B [66], the availability of such data is severely limited in the medical domain due to patient privacy concerns that prevent dissemination of large corpora of medical text. As a result, existing instruction-tuned multimodal LLMs developed for CXR interpretation are trained on small datasets with limited instruction diversity [74; 89].\n' +
      '2. _Existing vision and language encoders fail to capture the complexities of medical data._ Multimodal LLMs require powerful image and text encoders. However, CXRs represent a large domain shift from natural images and text, and thus, existing pretrained vision and language encoders struggle with medical knowledge grounding.\n' +
      '3. _Performing rigorous evaluations of multimodal LLMs in medicine is challenging._ Multimodal LLMs generate open-ended responses, and evaluating free-text responses for factual correctness and completeness is arduous, particularly for large-scale evaluations that require domain expertise. Previous FMs developed for CXR interpretation are primarily evaluated using visual-question-answer (VQA) or text generation tasks. To the best of our knowledge,\n' +
      '\n' +
      'Figure 1: Overview of the proposed pipeline: CheXinstruct is a curation of datasets for instruction-tuning across various CXR tasks, CheXagent is our clinical FM for CXR interpretation, and CheXbench is our comprehensive FM evaluation benchmark. Two example CXR interpretation tasks include local findings generation and open-ended visual question answering (VQA).\n' +
      '\n' +
      'there are no existing benchmarks for quantitative and qualitative evaluation of multimodal LLMs across diverse, clinically-relevant CXR interpretation tasks.\n' +
      '\n' +
      'In this work, we address these challenges by (i) introducing _CheXinstruct_, a large-scale instruction-tuning dataset with instruction-image-answer triplets, (ii) developing _CheXagent_, an instruction-tuned multimodal LLM for CXR interpretation, and (iii) curating _CheXBench_, a novel benchmark to enable systematic comparisons of FMs across 8 clinically-relevant CXR interpretation tasks. Below we outline our key contributions, also summarized in Fig. 1, that can help create capable and robust CXR FMs:\n' +
      '\n' +
      '1. _CheXinstruct_ is an instruction-tuning dataset with 6M instruction-image-answer triplets designed to improve the ability of FMs to interpret CXRs. We collect instructions from 34 tasks and 65 unique datasets, spanning categories including coarse- and fine-grained image understanding, question answering, and text generation.\n' +
      '2. _CheXagent_ is an instruction-tuned foundation model with 8B parameters capable of analyzing images, understanding text, and generating responses. Our methodology for developing CheXagent includes training (1) a clinical LLM capable of understanding radiology reports, (2) a vision encoder capable of reading CXRs, and (3) a network to bridge the vision and language modalities. We then perform instruction-tuning using data from CheXinstruct.\n' +
      '3. _CheXbench_ is a novel benchmark designed to rigorously evaluate FMs across two evaluation axes: image perception and textual understanding. We introduce 8 tasks across 7 CXR datasets, and we evaluate performance using close-ended multiple-choice predictions as well as open-ended text generation.\n' +
      '\n' +
      'We use CheXbench to compare CheXagent with six prior general-domain and medical-domain FMs. Across six visual tasks, the performance of CheXagent surpasses general-domain FMs by 97.5% and medical-domain FMs by 55.7%. Across two text generation tasks, CheXagent provides medical text evaluated via automated quantitative metrics and qualitative metrics from five expert radiologists. We further provide an evaluation of potential model bias and highlight performance disparities across demographic factors of sex, race and age to improve model transparency.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Foundation Models\n' +
      '\n' +
      'The surge in available data and computational resources has enabled the creation of FMs that are versatile in addressing a wide array of tasks with a single generalist model.\n' +
      '\n' +
      '**Language:** Significant strides in FMs were first seen in natural language processing (NLP) because large-scale text data was easily accessible online. LLMs like GPT-3 [10], ChatGPT, GPT-4 [54], FLAN-T5 [18], Llama-2 [76; 77], Mistral 7B [35], and PaLM-2 [17; 3] excel at multiple text-based tasks using only prompting, enabling new possibilities, such as outperforming human experts for clinical text summarization [81].\n' +
      '\n' +
      '**Vision:** In vision, FMs like Stable Diffusion [63], DALL-E [61; 60], and Imagen [64] were proposed for the task of text-to-image generation. Models Like Segment Anything Model (SAM) [38] and Segment Everything Everywhere Model (SEEM) [98] have been developed to perform in-the-wild segmentation. To achieve LLM-like scaling in vision, ViT-22B [19], a scaled-up version of ViT [23] was introduced as a large vision encoder. In the medical domain, there are several datasets designed for training FMs [49].\n' +
      '\n' +
      '**Vision-Language:** Given the inherent connection between different modalities such as language and vision, inter-modality supervision has been critical in the development of many vision-language foundation models (VLMs), like CLIP [59]. Several other large-scale VLMs like Flamingo [1], Coca [94], Owen-VL [5], BLIP [44], LLaVA [47], PaLI-X [14], and CogVLM [83] have been introduced with a focus on developing models powerful enough to perceive and understand both text and images. To evaluate the performance of these models, benchmarks like Multimodal Large Language Model Evaluation (MME) [25] and SEED-Bench [42] have been introduced. These benchmarks are carefully curated using manually generated instruction-answer pairs to avoid any data leakage.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '### Medical Foundation Models\n' +
      '\n' +
      'Biomedical language models (LMs) like BioNLP [41], BioBERT [40], PubMedBERT [29], BioGPT [48], Med-PaLM [40] and Med-PaLM M [71] have shown that LMs can be fine-tuned on curated biomedical corpora to perform competitively on medical question answering tasks. Specifically, Med-PaLM 2 [71] achieved high scores in questions in the style of the US Medical Licensing Examination (USMLE) highlighting rapid progress towards physician-level performance for medical question answering. Since medicine inherently involves multiple modalities and covers a broad range of tasks, multi-modal FMs are particularly suitable for this field. Works like LLAVA-Med [43], Med-Flamingo [51], and Med-PaLM M [78] have proposed generalist models spanning across modalities like imaging and clinical text for tasks in radiology, dermatology, pathology, etc. RadFM [88] introduced a foundation model for radiology for both 2D and 3D imaging tasks. XrayGPT [74] aligned a MedCLIP [85] visual encoder with Vicuna [16], which was finetuned using clinical text. RoentGen, a fine-tuned version of Stable Diffusion, allows generating CXR images using radiology report text prompts [13; 12].\n' +
      '\n' +
      'Med-PaLM M, Med-Flamingo, RadFM, and LLAVA-Med aim to establish generalist frameworks that empower models to process data from various image modalities. In contrast, CheXagent is designed to excel in handling multiple tasks related to CXRs within a single model.\n' +
      '\n' +
      '### Most Related Work\n' +
      '\n' +
      'Our approach is most similar to the following works. (i) BLIP-2 [44]: our model architecture is closely aligned with that of BLIP-2, which uses a Querying Transformer (QFormer) to bridge the vision-language modality gap; (ii) FLAN [86] and MultiInstruct [90]: our development of instruction tuning datasets derived from existing annotated datasets is inspired by these works.\n' +
      '\n' +
      '## 3 CheXinstruct: Instruction-Tuning Dataset\n' +
      '\n' +
      'MotivationCheXinstruct seeks to cover a broad range of tasks shown in Fig. 1(a) to support training CXR FMs. These tasks can either (i) improve the abilities of FMs to understand CXRs or (ii) improve clinical decision making.\n' +
      '\n' +
      'Design SchemeCheXinstruct is organized into four levels:\n' +
      '\n' +
      'Figure 2: Collection of datasets and tasks comprising CheXinstruct (Left). The four-stage training process of CheXagent, starting from adapting a general LLM for clinical use, through training a CXR vision encoder and a vision-language bridge, to the final stage of instruction tuning on diverse CXR tasks (Right).\n' +
      '\n' +
      '1. Capability Level, where we specify the essential skills and competencies that a CXR FM should possess.\n' +
      '2. Task Level, where we outline a wide array of specific tasks that align with each identified capability, providing a clear framework for what the FM should be able to accomplish in the context of CXRs.\n' +
      '3. Dataset Level, where we identify and categorize various CXR datasets, each associated with a particular task. We focus on ensuring that the datasets are relevant and appropriately matched to the tasks they are intended to support.\n' +
      '4. Instance Level, where we define individual instances within each dataset at the most granular level. Each instance comprises an input (such as a CXR image) and its corresponding labels, forming the basic units for training and evaluating the FM.\n' +
      '\n' +
      '**Tasks**: CheXinstruct consists of five task categories according to their capabilities:\n' +
      '\n' +
      '* Coarse-grained Image Understanding, which defines the overall understanding of CXRs, e.g., view classification [36], and disease classification [84; 32; 62; 55; 30; 6; 34; 11; 69].\n' +
      '* Fine-grained Image Understanding, which defines the localized understanding of CXRs, e.g., abnormality detection [53; 58], abnormality grounding [8], and foreign object detection [91].\n' +
      '* Question Answering, which defines the ability to respond to a question related to CXRs, e.g., close-ended visual question answering (VQA) [97; 57], open-ended VQA [7; 4], difference VQA [31], and text QA.\n' +
      '* Text Generation, which defines the ability to generate radiology report sections, including a description of the findings [21; 82; 56], impression generation [24], findings summarization [15], and local findings generation [36].\n' +
      '* Miscellaneous: This category defines the miscellaneous abilities that are critical for a CXR FM, e.g., report evaluation [93; 50], and natural language explanation [37].1\n' +
      '\n' +
      'Footnote 1: Additional details about the tasks are presented in Appendix A.\n' +
      '\n' +
      'Dataset SourcesWith the aforementioned taxonomy of tasks, we created CheXinstruct by either (i) collecting existing publicly available datasets or (ii) curating a dataset with new labels from existing datasets.\n' +
      '\n' +
      'For every dataset, we derive relevant information to formulate distinct tasks. All the datasets are split following their corresponding official splits if applicable.2\n' +
      '\n' +
      'Footnote 2: More details of the dataset sources are presented in Appendix B.\n' +
      '\n' +
      '**Task Instruction Creation** Inspired by [86], we create instructions by manually writing ten instruction templates for each task.3 Each instruction template contains placeholders (e.g., <IMAGE>, <QUESTION>, and <OPTIONS>), which are replaced with specific values when creating instruction-following instances.\n' +
      '\n' +
      'Footnote 3: The annotators are researchers in radiology and computer vision (CV).\n' +
      '\n' +
      'Finally, each CheXinstruct instance is a triplet consisting of an image (or none, in the case of non-image-based tasks), a question, and an answer.\n' +
      '\n' +
      '**Data Analysis** We describe the overall statistics of CheXinstruct, including the number of questions\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r r r} \\hline \\hline \\multirow{2}{*}{**Split**} & \\multirow{2}{*}{**Total**} & \\multicolumn{2}{c}{**Questions**} & \\multicolumn{2}{c}{**Images**} & \\multirow{2}{*}{**Reuse**} \\\\  & & **Unique** & & & **Unique** \\\\ \\hline Train & 6.1M & 1.9M & 3.2 & 9.3M & 1.1M & 8.6 \\\\ Val & 203.3K & 82.9K & 2.5 & 228.9K & 31.7K & 7.2 \\\\ Test & 188.3K & 72.8K & 2.6 & 233.7K & 49.5K & 4.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: The statistics of CheXinstruct, where the total and unique numbers of questions and images are shown along with the average number of times each unique image is reused in the dataset (reuse).\n' +
      '\n' +
      'answer pairs and CXRs in Table 1. There are 6.1M question-answer pairs in CheXinstruct. Furthermore, since images from one dataset may be used across multiple tasks, we present additional information to provide intuition about the potential "overlap" problem among different datasets.4\n' +
      '\n' +
      'Footnote 4: More details about the data analysis are reported in Appendix C.\n' +
      '\n' +
      '## 4 CheXagent: Instruction-Following CXR FM\n' +
      '\n' +
      '### Problem Setup\n' +
      '\n' +
      'The aim of CheXagent is a model that can "see" images \\(x_{I}\\) and/or "read" text \\(x_{T}\\) and generate "responses" \\(y\\). To this end, we introduce three components of our model: a vision encoder \\(\\mathcal{M}_{v}\\), a vision-language bridge \\(\\mathcal{M}_{b}\\), and a language decoder \\(\\mathcal{M}_{l}\\). Therefore, the ultimate formulation of the desired model is:\n' +
      '\n' +
      '\\[y=\\mathcal{M}_{l}(\\mathcal{M}_{b}(\\mathcal{M}_{v}(x_{I})),x_{T}). \\tag{1}\\]\n' +
      '\n' +
      '### Training CheXagent\n' +
      '\n' +
      'In this subsection, we present the four training stages of CheXagent, which are illustrated in Figure 1(b).\n' +
      '\n' +
      '**Stage 0: Train a clinical LLM** Numerous open-source biomedical large language models (LLMs) exist, such as BioMegatron [70], GatorTron [92], BioGPT [48], BioMedLM5, and PMC-LLaMA [87]. These models are predominantly trained on PubMed Central (PMC) articles rather than clinical texts. To address this gap, we focus on developing a clinical LLM by adapting a general-domain LLM. Our starting point is Mistral-7B-v0.1 [35], chosen for its demonstrated strong reasoning capabilities across various benchmarks.\n' +
      '\n' +
      'Footnote 5: [https://huggingface.co/stanford-crfm/BioMedLM](https://huggingface.co/stanford-crfm/BioMedLM)\n' +
      '\n' +
      'To infuse the model with comprehensive medical and clinical knowledge, we utilize five distinct text sources for training: (i) PMC article abstracts, (ii) radiology reports from MIMIC-IV, (iii) MIMIC-IV discharge summaries, (iv) medical terms from Wikipedia, and (v) CXRs from CheXinstruct. Importantly, for MIMIC-IV data, we meticulously exclude any studies that are part of the validation and test sets of MIMIC-CXR to prevent data leakage.\n' +
      '\n' +
      '**Stage 1: Train a vision encoder for CXR** Drawing inspiration from the works of [45] and [94], we train our vision encoder using a variety of visual pre-training objectives, namely image-text contrastive (ITC) and image captioning (IC). Our model architecture mirrors that of [45]. For training purposes, we utilize datasets comprising image-text pairs, specifically from MIMIC-CXR, PadChest, and BIMCV-COVID-19. Preliminary studies indicate that employing a combination of ITC and IC, akin to the approach in [94], yields enhanced performance in our specific context.\n' +
      '\n' +
      '**Stage 2: Train a vision-language bridge** Following the training of the clinical LLM and the CXR vision encoder, we focus on developing a bridger model, \\(\\mathcal{M}_{b}\\). This model is designed to map visual data to the corresponding language (semantic) space. During training, we keep both the clinical LLM, \\(\\mathcal{M}_{l}\\), and the CXR vision encoder, \\(\\mathcal{M}_{v}\\) frozen. This approach is crucial for preventing catastrophic forgetting of prior knowledge during the image-text alignment process. For training \\(\\mathcal{M}_{b}\\), we employ the same datasets as in Stage 1, applying an image captioning objective for learning.\n' +
      '\n' +
      '**Stage 3: Instruction tuning** Upon completing Stage 2, we obtain a multi-modal LLM tailored for CXR interpretation. In this stage, our focus shifts to training the model on a variety of tasks within the CheXinstruct framework. Prior to training, we consider two key aspects: (i) reserving certain task-dataset pairs exclusively for evaluation purposes, and (ii) determining optimal dataset ratios to ensure balanced training across different capabilities. For the first, we sequester datasets including OpenI, SLAKE, and SIIM, which facilitates a more streamlined evaluation process (as discussed in SS5). For the second, we heuristically establish dataset ratios by carefully assessing the quality and diversity of each dataset. This method leaves room for future exploration into automated dataset selection and balancing.6 This training is conducted using a next-word prediction objective, with the loss computation being limited to answers.\n' +
      '\n' +
      'Footnote 6: More details on dataset ratios can be found in Appendix D.\n' +
      '\n' +
      '**Implementation Details** For model architecture, we use EVA-CLIP-g [73] for the vision encoder and BERT [22] for the Qformer, a linear layer for the bridger, and Mistral for the LLM. The optimization of trainable parameters is structured across various stages: (a) the entire LLM is trained in Stage 0; (b) in Stage 1, we train both the LoRA parameters of the vision encoder and the entire BERT encoder; (c) Stage 2 involves training the vision-language bridge; and (d) in Stage 3, we focus on training both the bridger and the entire LLM. For optimization across all these stages, we employ the AdamW optimizer, with each stage having its own hyper-parameters7.\n' +
      '\n' +
      'Footnote 7: More details on the implementation can be found in Appendix E.\n' +
      '\n' +
      '## 5 CheXbench: A Benchmark for Evaluating FMs on CXR Interpretation\n' +
      '\n' +
      'In this section, we introduce CheXbench, an evaluation benchmark for enabling systematic comparisons of FMs across 8 clinically-relevant CXR interpretation tasks.\n' +
      '\n' +
      '### Benchmark Design\n' +
      '\n' +
      'CheXbench is structured with two evaluation axes, crafted to assess crucial aspects of CXR interpretation: image perception and textual understanding. The evaluations within CheXbench are conducted on a specific subset of the CheXinstruct test set.8 In the following sections, we describe the tasks associated with each evaluation axis.\n' +
      '\n' +
      'Footnote 8: This is comprehensively detailed in Appendix F.\n' +
      '\n' +
      '**Evaluation Axis 1 - Image Perception**: We first aim to evaluate the ability of FMs to understand the visual content of CXRs. Our goals are to (1) evaluate the ability of FMs to generalize to a variety of data distributions and (2) include a range of challenging, clinically-relevant tasks. To this end, we introduce 6 tasks across 7 datasets; in particular, we note that 3 datasets (SIIM, SLAKE, and OpenI) were completely held-out from CheXagent training to avoid any potential data leakage. In line with prior benchmarks designed for general domain FMs [42, 26], we use a multiple-choice format, where an image and a question are posed to the FM and multiple options are considered. Since open-ended, free-text outputs from FMs are challenging to evaluate, we instead compute log-likelihood scores associated with each option; the option with the highest score is then selected as the response. For each task, we report accuracy.\n' +
      '\n' +
      '* View Classification (700 samples): Given a CXR, the FM is tasked with identifying the imaging view. We perform view classification on the CheXpert test set with three possible options (AP, PA, and Lateral) as well as the MIMIC-CXR test set with four possible options (AP, PA, Lateral, and LL).\n' +
      '* Binary Disease Classification (433 samples): Given a CXR and a disease label, the FM is tasked with determining if the disease is present in the image. We perform binary disease classification with twelve disease labels in the CheXpert test set, one disease label in the RSNA dataset, and one disease label in the SIIM dataset. There are two possible options (yes and no).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l l} \\hline \\hline\n' +
      '**Task** & \\multicolumn{3}{c}{**General-domain FMs**} & \\multicolumn{3}{c}{**Medical-domain FMs**} & \\multicolumn{1}{c}{**CheXagent**} \\\\  & & **BL-P2** & **Intertweller LLP** & **XrayCPT** & **MedTraining** & **RadFFM** & **LLAvAv-Med** & **(Ours)** \\\\ \\hline \\multirow{2}{*}{View Classification} & MIMIC-CXR & 28.8 & 25.3 & 24.0 & 25.0 & 28.5 & 23.8 & **97.5** \\\\  & CheXpert & 38.0 & 34.0 & 33.0 & 39.0 & 37.0 & 30.0 & **96.7** \\\\ \\multirow{2}{*}{Binary Disease Classification} & SIM & 53.0 & 54.0 & 50.0 & 50.0 & 50.0 & 49.0 & 64.0 \\\\  & BENA & 50.0 & 60.0 & 50.0 & 50.0 & 50.0 & 44.0 & **81.0** \\\\  & CheXpert & 51.5 & 53.2 & 51.5 & 48.5 & 55.8 & 47.6 & **76.0** \\\\ \\multirow{2}{*}{Single Disease Identification} & OpenI & 40.2 & 40.2 & 45.4 & 39.0 & 42.2 & 43.8 & 47.0 \\\\  & MIMIC-CXR & 25.6 & 22.6 & 24.1 & 25.6 & 27.2 & 26.7 & **30.3** \\\\  & CheXpert & 21.3 & 19.5 & 23.7 & 26.0 & 26.6 & 26.0 & **29.6** \\\\ \\multirow{2}{*}{Multi Disease Identification} & OpenI & 48.5 & 54.4 & 58.7 & 46.1 & 52.8 & 53.9 & 55.6 \\\\  & MIMIC-CXR & 30.0 & 25.3 & 39.0 & 14.7 & 22.3 & 28.7 & **55.3** \\\\  & CheXpert & 4.3 & 6.1 & 3.9 & 7.1 & 23.6 & 2.1 & **52.1** \\\\ \\multirow{2}{*}{Visual Question Answering} & Rad-Restructural & 41.2 & 42.4 & 38.6 & 45.5 & 48.5 & 34.9 & **57.1** \\\\  & SLAKE & 74.3 & 86.4 & 52.4 & 64.8 & 85.0 & 55.5 & 78.1 \\\\ \\hline \\multirow{2}{*}{Image-Text Reasoning} & OpenI & 47.9 & 52.6 & 52.4 & 54.7 & 54.0 & 45.8 & **59.0** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Results of Evaluation Axis 1 of CheXbench for tasks associated with image perception comparing CheXagent with general domain and medical domain FMs on several CXR datasets. For each task, we report accuracy.\n' +
      '\n' +
      '* Single Disease Identification (864 samples): Given a CXR, the FM is tasked with identifying the disease present in the image. We implement single disease identification with the CheXpert test set (where disease labels are obtained from expert radiologist annotations) and OpenI (where disease labels are obtained from Medical Subject Headings (MeSH) codes). There are four possible options associated with each question, and each option includes a single disease label (e.g. "pneumonia").\n' +
      '* Multi-Disease Identification (1387 samples): Given a CXR, the FM is tasked with identifying a set of multiple diseases present in the image. We implement multi-disease classification using CheXpert and OpenI. There are four possible options associated with each question, and each option includes a set of multiple diseases (e.g. "pneumonia, pleural effusion, cardiomegaly").\n' +
      '* Visual-Question-Answering (1319 samples): We evaluate FMs across two standard VQA benchmarks: SLAKE and Rad-Restruct. SLAKE consists of questions with two options (yes and no), and Rad-Restruct consists of questions with between two and four options.\n' +
      '* Image-Text Reasoning (380 samples): Given a CXR, the FM is tasked with identifying the disease in the image. In contrast to the single-disease classification task, this task employs hard negatives; each question is associated with two challenging options, distinguished only by a single word indicating location or severity (e.g. "left-sided pleural effusion" vs. "right-sided pleural effusion"). We implement image-text reasoning with the OpenI dataset.\n' +
      '\n' +
      '**Evaluation Axis 2 - Textual Understanding**: We additionally evaluate the ability of CheXagent and baseline FMs to generate and summarize text. To this end, we introduce the following 2 tasks: we evaluate open-ended responses using a combination of automated metrics (ROUGE-L [46], CheXbert-Score [72], BERT-Score [95], RadGraph-Score [20], and GPT-4) and human expert evaluations from five radiologists.\n' +
      '\n' +
      '* Findings Section Generation: Given an image, the FM is tasked with generating the findings section of the radiology report. This task involves identifying key features of the image, such as the presence of abnormalities. We implement the findings section generation task with MIMIC-CXR. Since existing medical FMs are often trained on MIMIC-CXR, we also evaluate the models on a private dataset.\n' +
      '* Findings Summarization: Given the findings section of a radiology report, the FM is tasked with summarizing the key observations into a concise statement. This task does not include images. We evaluate Findings Summarization on MIMIC-CXR.\n' +
      '\n' +
      '### Evaluation Results\n' +
      '\n' +
      'In our study, we employ CheXbench to compare CheXagent against two general-domain instruction-tuned FMs, InstructBLIP and BLIP2, which achieve state-of-the-art performance in previous research [42]. Additionally, we compare CheXagent with four medical FMs: XrayGPT, MedFlamingo, RadFM, and LLAVA-Med [74; 51; 43; 88]. This comparison aims to provide a comprehensive understanding of CheXagent\'s performance in relation to both general and medical-specific models.\n' +
      '\n' +
      'Table 2 provides results on the six tasks associated with evaluation axis 1. CheXagent demonstrates superior performance across image perception tasks, achieving an average improvement of 97.5% over general-domain FMs and an average improvement of 55.7% over medical FMs. We provide a detailed breakdown of CheXagent performance:\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Size**} & \\multicolumn{2}{c}{**MIMIC-CXR**} \\\\  & & **Range-L** \\\\ \\hline Llama-2 & 7B & 20.3 \\\\ Vicuna & 7B & 21.5 \\\\ FLAN-7.5 & 11B & 42.85 \\\\ FLAN-U.2 & 20B & 42.1 \\\\ CheXagent & SB & 40.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Results of eval.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Size**} & \\multicolumn{2}{c}{**Prine Dataset**} & \\multicolumn{2}{c}{**MIMIC-CXR**} \\\\  & & **BERT-S** & **CheXbert-S** & **RadGraph-S** & **BERT-S** & **CheXbert-S** & **RadGraph-S** \\\\ \\hline MedFlamingo & SB & 8.5 & 2.7 & 1.7 & 10.4 & 2.2 & 2.2 \\\\ LLAVA-Med & 8B & 12.5 & 17.0 & 4.2 & 6.2 & 17.5 & 4.0 \\\\ RadFM & 14B & 35.7 & 12.7 & 5.1 & 48.7 & 17.5 & 10.9 \\\\ XmgGPT & SB & 40.1 & 23.4 & 9.0 & 44.0 & 24.2 & 11.2 \\\\ CheXagent & SB & **45.0** & **23.7** & **14.0** & **50.4** & **24.9** & **18.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Results of evaluation axis 2 of CheXbench for the task of findings generation comparing CheXagent with baseline medical-domain FMs using various metrics.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      'findings summarization on MIMIC-CXR. For both tasks, each radiologist views the same set of 20 randomly selected samples, which contain the task input as well as an A/B comparison (CheXagent _vs._ physician) of the task outputs. Radiologists then grade these outputs using a five-point Likert scale across: completeness, correctness, and conciseness.\n' +
      '\n' +
      'Study results in Figure 4 show that CheXagent is comparable to physicians for findings summarization and that text from human experts achieves higher scores in terms of findings generation. Although CheXagent outperforms other vision-language models in quantitative metrics, these findings indicate that further improvement could facilitate closing the gap between CheXagent and human radiologists.\n' +
      '\n' +
      'We now discuss qualitative findings to better understand the opportunity for future improvement in vision-language models. As shown in Figure 5, physicians\' reports typically reference past patient studies to track changes over time. Thus, the capabilities of CheXagent, which is trained on cross-sectional reports, could be further improved by incorporating longitudinal data during training.\n' +
      '\n' +
      'We also observed that the model frequently produced a uniform distance measurement of 3.5 cm in its generated text, irrespective of context. This issue arises because CheXagent is not designed to directly estimate physical distances or other quantities from images. Thus, addressing this limitation represents yet another avenue for advancement in vision-language models.\n' +
      '\n' +
      '## 7 Fairness Evaluation\n' +
      '\n' +
      'Recent studies [28, 68] highlight the presence of biases in AI models used in radiology, raising concerns about their equitable application across diverse populations. For fairness evaluation, we test CheXagent on a subset of the CheXpert public test set, annotated by two expert radiologists to avoid label noise [65], using frontal view CXRs from individuals self-reporting as Asian, White, or Black. In total we use 159 unique subjects labeled as "No Finding" and "Cardiomegaly". To create a balanced test set, we resample with replacement [27] to ensure balanced disease prevalence\n' +
      '\n' +
      'Figure 4: Reader study with five radiologists. Top: Study design comparing the findings sections of CheXagent _vs._ that of human experts for two tasks (rows) across three attributes (columns). Bottom: Results. Compared to humans, CheXagent achieves parity in report summarization, while the results demonstrate a gap between CheXagent report generation and human-level expertise.\n' +
      '\n' +
      'Figure 5: Comparison of CheXagent (top) against physician (bottom) on report generation. Radiologist reports often refer to past studies (purple), a context not available to CheXagent. The presence of these references in the training set perhaps contributes to CheXagent’s error (**red**) in an otherwise accurate report (**green**), motivating future work for vision-language models.\n' +
      '\n' +
      'and subgroup representation. We generate 2000 bootstrap samples to calculate mean and standard deviation of F1-scores.\n' +
      '\n' +
      'We evaluate model performance for the detection of cardiomegaly with the prompt "Does this chest X-ray contain cardiomegaly" with possible answers "Yes" and "No". Detection of cardiomegaly is crucial for early diagnosis, and treatment of heart conditions [2], with studies revealing significant disparities in heart disease death rates based on demographic factors [79]. Our findings, shown in Fig. 6 reveal disparities; F1-scores are higher for males compared to females and vary across racial groups, with the model performing best for the Black subgroup and worst for the Asian subgroup. This could reflect inherent differences in the presentation of cardiomegaly across races, and could be influenced by the limited samples of 14 Black and 30 unique Asian subjects included in the test set. Age-wise, the model performs better for the 65+ age group compared to the 0-65 group, potentially due to a higher prevalence of cardiomegaly in older patients, and age-related physiological differences. These results are consistent with existing literature [28] and underscore the need for continued efforts in mitigating biases in AI models used in healthcare. An effective approach to address this issue is curating larger and more diverse datasets, which can help in developing models that are more representative and equitable across different patient demographics.\n' +
      '\n' +
      '## 8 Conclusion\n' +
      '\n' +
      'In conclusion, our work represents progress towards automated CXR interpretation. We introduce (i) CheXinstruct, an instruction-tuning dataset, (ii) CheXagent, an 8B-parameter vision-language FM and demonstrate its abilities through (iii) CheXbench, our benchmarking framework including 8 tasks over 7 datasets. CheXagent achieves improvement in visual perception and text generation tasks compared to general- and medical-domain LLMs and is validated by five expert radiologists. Furthermore, our fairness analysis across sex, race, and age contributes to the ongoing efforts to enhance model transparency in healthcare AI. The release of CheXinstruct, CheXagent, and CheXbench to the public domain not only underscores our commitment to advancing medical AI but also sets a new benchmark for future developments in this critical area of research.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'MV is supported by graduate fellowship awards from the Department of Defense (NDSEG) and the Knight-Hennessy Scholars program at Stanford University. AC receives research support from the National Institutes of Health (grants - R01 HL167974, R01 AR077604, R01 EB002524, R01 AR079431, P41 EB027060, and contracts 75N92020C00008, 75N92020C00021); and from GE Healthcare and Philips. Research reported in this publication was made possible in part by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) of the National Institutes of Health under contracts 75N92020C00008 and 75N92020C00021, and by grant #1R18HS028955 from the Agency for Health Research and Quality. This work is supported in part by MIDRC (The Medical Imaging and Data Resource Center), made possible by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) of the National Institutes of Health under contract 75N92020D00021. We acknowledge support by Stability AI in providing computational support for this work.\n' +
      '\n' +
      'Figure 6: Evaluation of CheXagent subgroup performance on cardiomegaly classification investigating potential model biases. F1 Scores vary across sex, racial groups, and age categories.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. _Advances in Neural Information Processing Systems_, 35:23716-23736, 2022.\n' +
      '* [2] S. S. Alghamdi, I. Abdelaziz, M. Albadri, S. Alyanbaawi, R. Aljondi, and A. Tajaldeen. Study of cardiomegaly using chest x-ray. _Journal of Radiation Research and Applied Sciences_, 13(1):460-467, 2020.\n' +
      '* [3] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* [4] S. Bae, D. Kyung, J. Ryu, E. Cho, G. Lee, S. Kweon, J. Oh, L. Ji, E. I. Chang, T. Kim, et al. Ehrxqa: A multi-modal question answering dataset for electronic health records with chest x-ray images. _arXiv preprint arXiv:2310.18652_, 2023.\n' +
      '* [5] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.\n' +
      '* [6] S. Bannur, S. Hyland, Q. Liu, F. Perez-Garcia, M. Ilse, D. C. de Castro, B. Boecking, H. Sharma, K. Bouzid, A. Schwaighofer, et al. Ms-cxr-t: Learning to exploit temporal structure for biomedical vision-language processing, 2023.\n' +
      '* [7] A. Ben Abacha, S. A. Hasan, V. V. Datla, D. Demner-Fushman, and H. Muller. Vqa-med: Overview of the medical visual question answering task at imageclef 2019. In _Proceedings of CLEF (Conference and Labs of the Evaluation Forum) 2019 Working Notes_. 9-12 September 2019, 2019.\n' +
      '* [8] B. Boecking, N. Usuyama, S. Bannur, D. C. Castro, A. Schwaighofer, S. Hyland, M. Wetscherek, T. Naumann, A. Nori, J. Alvarez-Valle, et al. Making the most of text semantics to improve biomedical vision-language processing. In _European conference on computer vision_, pages 1-21. Springer, 2022.\n' +
      '* [9] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.\n' +
      '* [10] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [11] A. Bustos, A. Pertusa, J.-M. Salinas, and M. De La Iglesia-Vaya. Padchest: A large chest x-ray image dataset with multi-label annotated reports. _Medical image analysis_, 66:101797, 2020.\n' +
      '* [12] P. Chambon, C. Bluethgen, J.-B. Delbrouck, R. Van der Sluijs, M. Polacin, J. M. Z. Chaves, T. M. Abraham, S. Purohit, C. P. Langlotz, and A. Chaudhari. Roentgen: Vision-language foundation model for chest x-ray generation, 2022. URL [https://arxiv.org/abs/2211.12737](https://arxiv.org/abs/2211.12737).\n' +
      '* [13] P. Chambon, C. Bluethgen, C. P. Langlotz, and A. Chaudhari. Adapting pretrained vision-language foundational models to medical imaging domains, 2022.\n' +
      '* [14] X. Chen, J. Djolonga, P. Padlewski, B. Mustafa, S. Changpinyo, J. Wu, C. R. Ruiz, S. Goodman, X. Wang, Y. Tay, et al. Pali-x: On scaling up a multilingual vision and language model. _arXiv preprint arXiv:2305.18565_, 2023.\n' +
      '* [15] Z. Chen, M. Varma, X. Wan, C. Langlotz, and J.-B. Delbrouck. Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 469-484, Toronto, Canada, July 2023. Association for Computational Linguistics.\n' +
      '\n' +
      '* [16] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgrt quality. _See https://vicuna. lnsys. org (accessed 14 April 2023)_, 2023.\n' +
      '* [17] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n' +
      '* [18] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.\n' +
      '* [19] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. P. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In _International Conference on Machine Learning_, pages 7480-7512. PMLR, 2023.\n' +
      '* [20] J.-B. Delbrouck, P. Chambon, C. Bluethgen, E. Tsai, O. Almusa, and C. Langlotz. Improving the factual correctness of radiology report generation with semantic rewards. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 4348-4360, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.319. URL [https://aclanthology.org/2022.findings-emnlp.319](https://aclanthology.org/2022.findings-emnlp.319).\n' +
      '* [21] D. Demner-Fushman, S. Antani, M. Simpson, and G. R. Thoma. Design and development of a multimodal biomedical information retrieval system. _Journal of Computing Science and Engineering_, 6(2):168-177, 2012.\n' +
      '* [22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, 2019.\n' +
      '* [23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* [24] S. Feng, D. Azzollini, J. S. Kim, C.-K. Jin, S. P. Gordon, J. Yeoh, E. Kim, M. Han, A. Lee, A. Patel, et al. Curation of the candid-ptx dataset with free-text reports. _Radiology: Artificial Intelligence_, 3(6):e210136, 2021.\n' +
      '* [25] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu, W. Lin, J. Yang, X. Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023.\n' +
      '* [26] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, Sept. 2021. URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).\n' +
      '* [27] B. Glocker, C. Jones, M. Bernhardt, and S. Winzeck. Algorithmic encoding of protected characteristics in chest x-ray disease detection models. _Ebiomedicine_, 89, 2023.\n' +
      '* [28] B. Glocker, C. Jones, M. Roschewitz, and S. Winzeck. Risk of bias in chest radiography deep learning foundation models. _Radiology: Artificial Intelligence_, 5(6):e230060, 2023.\n' +
      '* [29] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, and H. Poon. Domain-specific language model pretraining for biomedical natural language processing. _ACM Transactions on Computing for Healthcare (HEALTH)_, 3(1):1-23, 2021.\n' +
      '* [30] G. Holste, S. Wang, A. Jaiswal, Y. Yang, M. Lin, Y. Peng, and A. Wang. Cxr-lt: Multi-label long-tailed classification on chest x-rays. _PhysioNet_, 2023.\n' +
      '\n' +
      '* [31] X. Hu, L. Gu, Q. An, M. Zhang, L. Liu, K. Kobayashi, T. Harada, R. M. Summers, and Y. Zhu. Expert knowledge-aware image difference graph representation learning for difference-aware medical visual question answering. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 4156-4165, 2023.\n' +
      '* [32] J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund, B. Haghgoo, R. Ball, K. Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 590-597, 2019.\n' +
      '* [33] L. Iyeke, R. Moss, R. Hall, J. Wang, L. Sandhu, B. Appold, E. Kalontar, D. Menoudakos, M. Rammarine, S. P. LaVine, et al. Reducing unnecessary \'admission\'chest x-rays: An initiative to minimize low-value care. _Cureus_, 14(10), 2022.\n' +
      '* [34] S. Jaeger, S. Candemir, S. Antani, Y.-X. J. Wang, P.-X. Lu, and G. Thoma. Two public chest x-ray datasets for computer-aided screening of pulmonary diseases. _Quantitative imaging in medicine and surgery_, 4(6):475, 2014.\n' +
      '* [35] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* [36] A. E. Johnson, T. J. Pollard, N. R. Greenbaum, M. P. Lungren, C.-y. Deng, Y. Peng, Z. Lu, R. G. Mark, S. J. Berkowitz, and S. Horng. Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. _arXiv preprint arXiv:1901.07042_, 2019.\n' +
      '* [37] M. Kayser, C. Emde, O.-M. Camburu, G. Parsons, B. Papiez, and T. Lukasiewicz. Explaining chest x-ray pathologies in natural language. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 701-713. Springer, 2022.\n' +
      '* [38] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* [39] J. J. Lau, S. Gayen, A. Ben Abacha, and D. Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. _Scientific data_, 5(1):1-10, 2018.\n' +
      '* [40] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. _Bioinformatics_, 36(4):1234-1240, 2020.\n' +
      '* [41] P. Lewis, M. Ott, J. Du, and V. Stoyanov. Pretrained language models for biomedical and clinical tasks: understanding and extending the state-of-the-art. In _Proceedings of the 3rd Clinical Natural Language Processing Workshop_, pages 146-157, 2020.\n' +
      '* [42] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. _arXiv preprint arXiv:2307.16125_, 2023.\n' +
      '* [43] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. _arXiv preprint arXiv:2306.00890_, 2023.\n' +
      '* [44] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, pages 12888-12900. PMLR, 2022.\n' +
      '* [45] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [46] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pages 74-81, 2004.\n' +
      '* [47] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '\n' +
      '* [48] R. Luo, L. Sun, Y. Xia, T. Qin, S. Zhang, H. Poon, and T.-Y. Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. _Briefings in Bioinformatics_, 23(6): bbac409, 2022.\n' +
      '* [49] X. Mei, Z. Liu, P. M. Robson, B. Marinelli, M. Huang, A. Doshi, A. Jacobi, C. Cao, K. E. Link, T. Yang, et al. Radimagenet: an open radiologic deep learning research dataset for effective transfer learning. _Radiology: Artificial Intelligence_, 4(5):e210315, 2022.\n' +
      '* [50] Y. Miura, Y. Zhang, E. Tsai, C. Langlotz, and D. Jurafsky. Improving factual completeness and consistency of image-to-text radiology report generation. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5288-5304, 2021.\n' +
      '* [51] M. Moor, Q. Huang, S. Wu, M. Yasunaga, C. Zakka, Y. Dalmia, E. P. Reis, P. Rajpurkar, and J. Leskovec. Med-flamingo: a multimodal medical few-shot learner. _arXiv preprint arXiv:2307.15189_, 2023.\n' +
      '* [52] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. _arXiv preprint arXiv:2306.02707_, 2023.\n' +
      '* [53] H. Q. Nguyen, K. Lam, L. T. Le, H. H. Pham, D. Q. Tran, D. B. Nguyen, D. D. Le, C. M. Pham, H. T. Tong, D. H. Dinh, et al. Vindr-cxr: An open dataset of chest x-rays with radiologist\'s annotations. _Scientific Data_, 9(1):429, 2022.\n' +
      '* [54] OpenAI. Gpt-4 technical report, 2023.\n' +
      '* [55] M. Pavlova, T. Tuinstra, H. Aboutalebi, A. Zhao, H. Gunraj, and A. Wong. Covidx ccr-3: a large-scale, open-source benchmark dataset of chest x-ray images for computer-aided covid-19 diagnostics. _arXiv preprint arXiv:2206.03671_, 2022.\n' +
      '* [56] O. Pelka, S. Koitka, J. Ruckert, F. Nensa, and C. M. Friedrich. Radiology objects in context (roco): a multimodal image dataset. In _Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis_, pages 180-189. Springer, 2018.\n' +
      '* [57] C. Pellegrini, M. Keicher, E. Ozsoy, and N. Navab. Rad-restruct: A novel vqa benchmark and method for structured radiology reporting. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 409-419. Springer, 2023.\n' +
      '* [58] H. H. Pham, T. T. Tran, and H. Q. Nguyen. Vindr-pcxr: An open, large-scale pediatric chest x-ray dataset for interpretation of common thoracic diseases. _PhysioNet_, 2022.\n' +
      '* [59] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [60] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.\n' +
      '* [61] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* [62] E. P. Reis, J. P. de Paiva, M. C. da Silva, G. A. Ribeiro, V. F. Paiva, L. Bulgarelli, H. M. Lee, P. V. Santos, V. M. Brito, L. T. Amaral, et al. Brax, brazilian labeled chest x-ray dataset. _Scientific Data_, 9(1):487, 2022.\n' +
      '* [63] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '\n' +
      '* [64] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* [65] A. Saporta, X. Gui, A. Agrawal, A. Pareek, S. Q. Truong, C. D. Nguyen, V.-D. Ngo, J. Seekins, F. G. Blankenberg, A. Y. Ng, et al. Benchmarking saliency methods for chest x-ray interpretation. _Nature Machine Intelligence_, 4(10):867-878, 2022.\n' +
      '* [66] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.\n' +
      '* [67] J. C. Seah, C. H. Tang, Q. D. Buchlak, X. G. Holt, J. B. Wardman, A. Aimoldin, N. Esmaili, H. Ahmad, H. Pham, J. F. Lambert, et al. Effect of a comprehensive deep-learning model on the accuracy of chest x-ray interpretation by radiologists: a retrospective, multireader multicase study. _The Lancet Digital Health_, 3(8):e496-e506, 2021.\n' +
      '* [68] L. Seyred-Kalantari, H. Zhang, M. B. McDermott, I. Y. Chen, and M. Ghassemi. Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations. _Nature medicine_, 27(12):2176-2182, 2021.\n' +
      '* [69] G. Shih, C. C. Wu, S. S. Halabi, M. D. Kohli, L. M. Prevedello, T. S. Cook, A. Sharma, J. K. Amorosa, V. Arteaga, M. Galperin-Aizenberg, et al. Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia. _Radiology: Artificial Intelligence_, 1(1):e180041, 2019.\n' +
      '* [70] H.-C. Shin, Y. Zhang, E. Bakhturina, R. Puri, M. Patwary, M. Shoeybi, and R. Mani. Biomegatron: Larger biomedical domain language model. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 4700-4706, 2020.\n' +
      '* [71] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal, et al. Towards expert-level medical question answering with large language models. _arXiv preprint arXiv:2305.09617_, 2023.\n' +
      '* [72] A. Smit, S. Jain, P. Rajpurkar, A. Pareek, A. Y. Ng, and M. Lungren. Combining automatic labelers and expert annotations for accurate radiology report labeling using bert. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1500-1519, 2020.\n' +
      '* [73] Q. Sun, Y. Fang, L. Wu, X. Wang, and Y. Cao. Eva-clip: Improved training techniques for clip at scale. _arXiv preprint arXiv:2303.15389_, 2023.\n' +
      '* [74] O. Thawkar, A. Shaker, S. S. Mullappilly, H. Cholakkal, R. M. Anwer, S. Khan, J. Laaksonen, and F. S. Khan. Xraygpt: Chest radiographs summarization using medical vision-language models. _arXiv preprint arXiv:2306.07971_, 2023.\n' +
      '* [75] E. Tiu, E. Talius, P. Patel, C. P. Langlotz, A. Y. Ng, and P. Rajpurkar. Expert-level detection of pathologies from unannotated chest x-ray images via self-supervised learning. _Nature Biomedical Engineering_, 6(12):1399-1406, 2022.\n' +
      '* [76] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [77] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [78] T. Tu, S. Azizi, D. Driess, M. Schaekermann, M. Amin, P.-C. Chang, A. Carroll, C. Lau, R. Tanno, I. Ktena, et al. Towards generalist biomedical ai. _arXiv preprint arXiv:2307.14334_, 2023.\n' +
      '\n' +
      '* [79] M. Van Dyke, S. Greer, E. Odom, L. Schieb, A. Vaughan, M. Kramer, and M. Casper. Heart disease death rates among blacks and whites aged 35 years--united states, 1968-2015. _MMWR Surveillance Summaries_, 67(5):1, 2018.\n' +
      '* [80] D. Van Veen, C. Van Uden, M. Attias, A. Pareek, C. Bluethgen, M. Polacin, W. Chiu, J.-B. Delbrouck, J. Zambrano Chaves, C. Langlotz, A. Chaudhari, and J. Pauly. RadAdapt: Radiology report summarization via lightweight domain adaptation of large language models. In D. Demner-fushman, S. Ananiadou, and K. Cohen, editors, _The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks_, pages 449-460, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.bionlp-1.42. URL [https://aclanthology.org/2023.bionlp-1.42](https://aclanthology.org/2023.bionlp-1.42).\n' +
      '* [81] D. Van Veen, C. Van Uden, L. Blankemeier, J.-B. Delbrouck, A. Aali, C. Bluethgen, A. Pareek, M. Polacin, W. Collins, N. Ahuja, et al. Clinical text summarization: Adapting large language models can outperform human experts. _arXiv preprint arXiv:2309.07430_, 2023.\n' +
      '* [82] M. D. L. I. Vaya, J. M. Saborit, J. A. Montell, A. Pertusa, A. Bustos, M. Cazorla, J. Galant, X. Barber, D. Orozco-Beltran, F. Garcia-Garcia, et al. Bimcv covid-19+: a large annotated dataset of rx and ct images from covid-19 patients. _arXiv preprint arXiv:2006.01174_, 2020.\n' +
      '* [83] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al. Cogvlm: Visual expert for pretrained language models. _arXiv preprint arXiv:2311.03079_, 2023.\n' +
      '* [84] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2097-2106, 2017.\n' +
      '* [85] Z. Wang, Z. Wu, D. Agarwal, and J. Sun. Medclip: Contrastive learning from unpaired medical images and text. _arXiv preprint arXiv:2210.10163_, 2022.\n' +
      '* [86] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. In _International Conference on Learning Representations_, 2021.\n' +
      '* [87] C. Wu, W. Lin, X. Zhang, Y. Zhang, Y. Wang, and W. Xie. Pmc-llama: Towards building open-source language models for medicine. _arXiv preprint arXiv:2305.10415_, 2023.\n' +
      '* [88] C. Wu, X. Zhang, Y. Zhang, Y. Wang, and W. Xie. Towards generalist foundation model for radiology. _arXiv preprint arXiv:2308.02463_, 2023.\n' +
      '* [89] S. Xu, L. Yang, C. Kelly, M. Sieniek, T. Kohlberger, M. Ma, W.-H. Weng, A. Kiraly, S. Kazemzadeh, Z. Melamed, et al. Elixr: Towards a general purpose x-ray artificial intelligence system through alignment of large language models and radiology vision encoders. _arXiv preprint arXiv:2308.01317_, 2023.\n' +
      '* [90] Z. Xu, Y. Shen, and L. Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. _arXiv preprint arXiv:2212.10773_, 2022.\n' +
      '* [91] Z. Xue, S. Candemir, S. Antani, L. R. Long, S. Jaeger, D. Demner-Fushman, and G. R. Thoma. Foreign object detection in chest x-rays. In _2015 IEEE international conference on bioinformatics and biomedicine (BIBM)_, pages 956-961. IEEE, 2015.\n' +
      '* [92] X. Yang, A. Chen, N. PourNejatian, H. C. Shin, K. E. Smith, C. Parisien, C. Compas, C. Martin, A. B. Costa, M. G. Flores, et al. A large language model for electronic health records. _NPJ Digital Medicine_, 5(1):194, 2022.\n' +
      '* [93] F. Yu, M. Endo, R. Krishnan, I. Pan, A. Tsai, E. P. Reis, E. K. U. N. Fonseca, H. M. H. Lee, Z. S. H. Abad, A. Y. Ng, et al. Evaluating progress in automatic chest x-ray radiology report generation. _Patterns_, 4(9), 2023.\n' +
      '\n' +
      '* [94] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive captioners are image-text foundation models. _arXiv preprint arXiv:2205.01917_, 2022.\n' +
      '* [95] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. Bertscore: Evaluating text generation with bert. In _International Conference on Learning Representations_, 2019.\n' +
      '* [96] X. Zhang, C. Wu, Y. Zhang, W. Xie, and Y. Wang. Knowledge-enhanced visual-language pre-training on chest radiology images. _Nature Communications_, 14(1):4542, 2023.\n' +
      '* [97] X. Zhang, C. Wu, Z. Zhao, W. Lin, Y. Zhang, Y. Wang, and W. Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. _arXiv preprint arXiv:2305.10415_, 2023.\n' +
      '* [98] X. Zou, J. Yang, H. Zhang, F. Li, L. Li, J. Gao, and Y. J. Lee. Segment everything everywhere all at once. _arXiv preprint arXiv:2304.06718_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      'The prompt for GPT-4 Evaluation\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      'We would like to request your feedback on the radiology reports generated by two AI assistants by comparing them to the reference report written by radiologists.\n' +
      '\n' +
      '[Reference Report]\n' +
      '\n' +
      '[reference]\n' +
      '\n' +
      '[End of Reference Report]\n' +
      '\n' +
      '[Assistant 1]\n' +
      '\n' +
      '[report1]\n' +
      '\n' +
      '[Assistant 1]\n' +
      '\n' +
      '[Assistant 2]\n' +
      '\n' +
      '[report2]\n' +
      '\n' +
      '[End of Assistant 2]\n' +
      '\n' +
      '[Requirements]\n' +
      '\n' +
      '1. The length of the reports is not important.\n' +
      '\n' +
      '2. The style of the reports is not important.\n' +
      '\n' +
      '3. The clinical accuracy is important especially for positive findings (i.e., diseases).\n' +
      '\n' +
      'Therefore, please focus on clinical accuracy instead of the length and style.\n' +
      '\n' +
      '[End of Requirements]\n' +
      '\n' +
      'Please compare the accuracy of their generated reports. You should tell me whether Assistant 1 is "better than", "worse than", or "equal to" Assistant 2. Please first compare the generated reports with the reference report to analyze which one is more in line with the given requirements. In the last line, please output a single line containing only a single label selecting from "Assistant 1 is better than Assistant 2", "Assistant 1 is worse than Assistant 2", and "Assistant 1 is equal to Assistant 2".\n' +
      '\n' +
      'Figure 8: The prompt for GPT-4 Evaluation.\n' +
      '\n' +
      'Figure 7: Examples of CheXbench image perception tasks: CheXbench includes six multiple-choice tasks that evaluate the ability of FMs to interpret chest X-rays.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline\n' +
      '**Capability** & **Task** & **Dataset** & **Description** \\\\ \\hline \\multirow{8}{*}{Cune-grained Image Understanding} & \\multicolumn{3}{c}{ConvConv14} \\\\  & \\multicolumn{3}{c}{CNNCapNet} \\\\  & \\multicolumn{3}{c}{ShML-CSR} \\\\  & \\multicolumn{3}{c}{ResNet} \\\\  & \\multicolumn{3}{c}{ResNet} \\\\  & \\multicolumn{3}{c}{ResNet} \\\\  & \\multicolumn{3}{c}{ConvConv14} \\\\  & \\multicolumn{3}{c}{ResNet} \\\\  & \\mul\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Task-Dataset Pair** & **Sampling Ratio** \\\\ \\hline (Named Entity Recognition) RadGraph & 10.00 \\\\ (Abnormality Grounding) VinDr-CXR & 1.00 \\\\ (Abnormality Grounding) VinDr-PCXR & 1.00 \\\\ (Caption Generation) ROCO & 1.00 \\\\ (Close-Ended VQA) PMC-VQA & 1.00 \\\\ (Close-Ended VQA) VQA-RAD & 1.00 \\\\ (Foreign Object Detection) Object-CXR & 1.00 \\\\ (Grounded Captioning) MS-CXR & 1.00 \\\\ (Grounded Diagnosis) MS-CXR & 1.00 \\\\ (Grounded Diagnosis) VinDr-CXR & 1.00 \\\\ (Grounded Diagnosis) VinDr-PCXR & 1.00 \\\\ (Grounded Phase Extraction) MS-CXR & 1.00 \\\\ (Image Classification) COVID-CXR-3 & 1.00 \\\\ (Image Classification) NLM-TB & 1.00 \\\\ (Image Classification) RSNA & 1.00 \\\\ (Impression Generation) Candid & 1.00 \\\\ (Open-Ended VQA) MedVQA-2019 & 1.00 \\\\ (Open-Ended VQA) PMC-VQA & 1.00 \\\\ (Open-Ended VQA) VQA-RAD & 1.00 \\\\ (Phrase Grounding) MS-CXR & 1.00 \\\\ (Pneumothorax Segmentation) Candid & 1.00 \\\\ (Progression Findings Generation) MIMIC-CXR & 1.00 \\\\ (Progression Impression Generation) MIMIC-CXR & 1.00 \\\\ (Text QA) RadQA & 1.00 \\\\ (View Matching) MIMIC-CXR & 0.50 \\\\ (Findings Generation) MIMIC-CXR-Struct & 0.40 \\\\ (Impression Generation) MIMIC-CXR-Struct & 0.30 \\\\ (Natural Language Explanation) MIMIC-NLE & 0.30 \\\\ (Report Generation) BIMC-COVID-D19 & 0.25 \\\\ (Findings Summarization) MIMIC-III & 0.20 \\\\ (Image Classification) ChestXray14 & 0.20 \\\\ (Close-Ended VQA) MIMIC-CXR-VQA & 0.10 \\\\ (Findings Generation) MIMIC-CXR & 0.10 \\\\ (Findings Summarization) MIMIC-CXR & 0.10 \\\\ (Image Classification) Brax & 0.10 \\\\ (Image Classification) CheXpert & 0.10 \\\\ (Image Classification) PadChest & 0.10 \\\\ (Image-Text Matching) ROCO & 0.10 \\\\ (Image-Text Selection) ROCO & 0.10 \\\\ (Impression Generation) MIMIC-CXR & 0.10 \\\\ (Report Generation) PadChest & 0.10 \\\\ (View Classification) MIMIC-CXR & 0.10 \\\\ (Image Classification) MIMIC-CXR & 0.05 \\\\ (Open-Ended VQA) MIMIC-CXR-VQA & 0.05 \\\\ (Local Findings Generation) MIMIC-CXR-Struct & 0.02 \\\\ (Local Impression Generation) MIMIC-CXR-Struct & 0.02 \\\\ (Text Instructions) OpenOrca & 0.02 \\\\ (Difference VQA) MIMIC-Diff-VQA & 0.01 \\\\ (Image Classification) CXR-LT & 0.01 \\\\ (Image-Text Matching) MIMIC-CXR & 0.01 \\\\ (Image-Text Selection) MIMIC-CXR & 0.01 \\\\ (Abnormality Detection) VinDr-CXR & 0.00 \\\\ (Abnormality Detection) VinDr-PCXR & 0.00 \\\\ (Chest Tube Segmentation) Candid & 0.00 \\\\ (Close-Ended VQA) Rad-Restruct & 0.00 \\\\ (Close-Ended VQA) SLAKE & 0.00 \\\\ (Open-Ended VQA) Rad-Restruct & 0.00 \\\\ (Open-Ended VQA) SLAKE & 0.00 \\\\ (Phrase Extraction and Grounding) MS-CXR & 0.00 \\\\ (Pneumothorax Segmentation) SIIM & 0.00 \\\\ (Rib Fracture Segmentation) Candid & 0.00 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: The sampling ratio of each dataset in the third-stage training of CheXagent.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Configuration** & **Stage 0** & **Stage 1** & **Stage 2** & **Stage 3** \\\\ \\hline ViT init. & - & EVA01-CLIP-g-14-plus & ViT Stage 1 & ViT Stage 2 \\\\ LIM init. & Mistral-7B-v0.1 & - & LLM Stage 0 & LLM Stage 2 \\\\ Qformer init. & - & BERT Base & Qformer Stage 1 & Qformer Stage 2 \\\\ Image resolution & - & 448\\({}^{2}\\) & 448\\({}^{2}\\) & 448\\({}^{2}\\) \\\\ ViT sequence length & - & 1024 & 1024 & 1024 \\\\ LLM sequence length & 2048 & - & 512 & 512 \\\\ Learnable query numbers & - & 128 & 128 & 128 \\\\ Global batch size & 2048 & 256 & 2048 & 512 \\\\ Optimizer & & AdamW & & \\\\ Optimizer hyperparameter & & \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.999\\), \\(eps=1e-8\\) & & \\\\ Peak learning rate & \\(5e-6\\) & \\(1e-4\\) & \\(1e-4\\) & \\(1e-6\\) \\\\ Minimum learning rate & \\(5e-7\\) & \\(1e-5\\) & \\(1e-5\\) & \\(1e-7\\) \\\\ Learning rate schedule & & cosine & & \\\\ Weight decay & & 0.05 & & \\\\ Gradient clip & & 1.0 & & \\\\ Numerical precision & & bf16 & & \\\\ DeepSpeed & ZeRO stage 2 & - & & - & ZeRO stage 2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Training hyperparameters of CheXagent.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c} \\hline \\hline\n' +
      '**Task** & **Dataset** & **Num. Samples** & **Num. Options** \\\\ \\hline \\multirow{2}{*}{View Classification} & MIMIC-CXR & 400 & 4 \\\\  & CheXpert & 300 & 3 \\\\ \\hline \\multirow{2}{*}{Binary Disease Classification} & SIIM & 100 & 2 \\\\  & RSNA & 100 & 2 \\\\  & CheXpert & 233 & 2 \\\\ \\hline \\multirow{2}{*}{Single Disease Identification} & OpenI & 500 & 4 \\\\  & MIMIC-CXR & 195 & 4 \\\\  & CheXpert & 169 & 4 \\\\ \\hline \\multirow{2}{*}{Multi Disease Identification} & OpenI & 807 & 4 \\\\  & MIMIC-CXR & 300 & 4 \\\\  & CheXpert & 280 & 4 \\\\ \\hline \\multirow{2}{*}{Visual Question Answering} & Rad-Restruct & 899 & 2-4 \\\\  & SLAKE & 420 & 2 \\\\ \\hline Image-Text Reasoning & OpenI & 380 & 2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: Statistics for image perception tasks in CheXbench (Evaluation Axis 1).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>