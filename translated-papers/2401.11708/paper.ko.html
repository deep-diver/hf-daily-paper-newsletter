<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '여행 명창 교과목별 이미지확산.\n' +
      '\n' +
      '멀티모달 LLLM 소비, 계획 및 전환과 함께 측정합니다.\n' +
      '\n' +
      ' 양.\n' +
      '\n' +
      '동일한 기여도 \\({}^{1}\\) 피킹 대학교, 중국\\({}^{2}\\) 스타포드 대학교, 미국\\({}^{2}\\) 피카랩스. 링 양\\(<\\)양링0818@163.com\\(>\\)에 대응한다.\n' +
      '\n' +
      'Zhaochen Yu\n' +
      '\n' +
      '동일한 기여도 \\({}^{1}\\) 피킹 대학교, 중국\\({}^{2}\\) 스타포드 대학교, 미국\\({}^{2}\\) 피카랩스. 링 양\\(<\\)양링0818@163.com\\(>\\)에 대응한다.\n' +
      '\n' +
      'Chenlin Meng\n' +
      '\n' +
      'Minkai Xu\n' +
      '\n' +
      'Stefano Ermon\n' +
      '\n' +
      'Bin Cui\n' +
      '\n' +
      '[https://github.com/YangLing0818/RPG-DiffusionMaster](https://github.com/YangLing0818/RPG-DiffusionMaster)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '확산 모델은 텍스트 대 이미지 생성 및 편집에서 탁월한 성능을 나타낸다. 그러나 기존의 방법은 여러 속성과 관계를 가진 여러 물체를 포함하는 복잡한 텍스트를 처리할 때 종종 도전에 직면한다. 본 논문에서는 텍스트 대 이미지 확산 모델의 구성성을 향상시키기 위해 다중 모드 LLM의 강력한 사슬 추론 능력을 활용하는 _트레이닝이 없는_ 텍스트 대 이미지 생성/편집 프레임워크, 즉 _ 리셉션, 플랜 및 젠티어(RPG)_ 브랜드를 제안한다. 우리의 접근법은 복합 이미지를 생성하는 과정을 하위 영역 내에서 여러 개의 더 간단한 생성 과제로 분해하기 위해 글로벌 플래너로서 MLLM을 사용한다. 지역별 구성 생성을 가능하게 하기 위해 _보완적 지역 확산_를 제안한다. 또한, 우리는 폐쇄 루프 방식으로 제안된 RPG 내에서 텍스트 유도 이미지 생성 및 편집을 통합하여 일반화 능력을 향상시킨다. 광범위한 실험은 특히 다중 범주 객체 구성과 텍스트 이미지 의미 정렬에서 DALL-E 3 및 SDXL을 포함한 RPG가 최첨단 텍스트 대 이미지 확산 모델을 보여준다. 특히, 우리의 RPG 프레임워크는 다양한 MLLM 아키텍처(예: 미니GPT-4) 및 확산 백본(예: 컨트롤Net)과 광범위한 호환성을 나타낸다. 우리의 코드는 [https://github.com/양링0818/RPG-디퓨전마스터](https://github.com/양링0818/양링0818/RPG-디퓨전마스터)에서 사용할 수 있다.\n' +
      '\n' +
      '기계 학습, ICML 기계 학습, ICML 기계 학습, ICML 기계 학습, ICML 머신 학습, ICML 기계 학습, ICML 기계 학습, ICML 컴퓨터 학습, ICML 기계 학습, ICML 기계 학습, ICML 기계 학습\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 확산 모델(Sohl-Dickstein et al, 2015; Dhariwal and Nichol, 2021; Dhariwal and Nichol, 2020; 송 et al., 2023c)은 Imagen(Sahara et al., 2022), DALL-E 2/3(Ramesh et al, 2022; Betker et al., 2022; Betker et al., 2023) 및 SDXL(Podell et al., 2023c)과 같은 텍스트 대 이미지 모델의 합성 결과를 크게 개선했다. 그러나 텍스트 프롬프트와 일치하는 현실 이미지를 합성하는 데 놀라운 능력에도 불구하고 대부분의 확산 모델은 보통 일부 복잡한 프롬프트(펑 et al., 2022; Lian et al., 2023; Liu et al., 2022; Bar-Tal et al., 2023)를 정확하게 따르기 위해 투쟁하며, 이는 모델이 서로 다른 속성과 관계를 가진 객체를 단일 이미지(Huang et al., 2023a)로 구성하도록 요구한다.\n' +
      '\n' +
      '일부 작업은 추가 레이아웃/박스(Li et al., 2023b; Xie et al., 2023e; 양 et al., Qu et al., 2023; Chen et al., 2024; Chen et al., 2023b; Lian et al., 2023b; Lian et al., 2023)를 조건으로 도입하거나 신속한 인식 관심 안내(Feng et al, 2022; Chefer et al, 2023; Chefer et al, 2023)를 활용하여 구성 텍스트 대 이미지 합성을 개선함으로써 이 문제를 해결하기 시작한다. 예를 들어, 구조 확산(Feng et al., 2022)은 확산 모델에서 교차 의도 지도를 조작하여 언어 구조를 유도 생성 과정에 통합한다. GLIGEN(Li et al., 2023b)은 원래의 확산 모델의 가중치를 동결하는 동안 바운딩 박스와 같은 공간적 입력을 통합하기 위해 훈련 가능한 게이팅된 자기 선택 계층을 설계한다.\n' +
      '\n' +
      '그림 1: (a) 텍스트-조건 확산 모델 (Ramesh et al., 2022), (b) 레이아웃/고의 기반 확산 모델 (Feng et al., 2022; Cao et al., 2023), (c) LLM-지상 확산 모델 (Lian et al., 2023) 및 (d) 우리의 RPG 사이의 건축 비교이다.\n' +
      '\n' +
      '그림 2: SDXL(Podell et al., 2023) 및 DALL-E 3(Betker et al., 2023)에 따라 제안된 RPG는 생성된 이미지(** 색 텍스트는 임계 부분**) 내에서 복잡한 및 구성 텍스트 프롬프트를 전달하는 우수한 능력을 나타낸다.\n' +
      '\n' +
      '그림 3: 우리 RPG 프레임워크는 제어넷(Zhang et al, 2023)을 사용하여 더 많은 조건(예: 포즈, 깊이 및 캐니 에지)으로 텍스트 대 이미지 생성을 확장할 수 있다. 원본 제어넷에 비해 RPG는 \'사용자 입력\'을 베이스 프롬프트와 서브프로파트의 조합으로 분해하여 신속한 이해를 크게 개선하고, 지역별 확산 생성(2.2절)을 수행하여 생성된 이미지의 구성 의미 정렬을 더욱 향상시킨다.\n' +
      '\n' +
      '또 다른 잠재적 해결책은 확산 생성을 정제하기 위한 이미지 이해 피드백(황 등, 2023; Xu et al., 2023; 선 et al., 2023; Fang et al., 2023)을 레버리지하는 것이다. 예를 들어, GORS(Huang et al., 2023)는 구성 프롬프트와 고도로 일치하는 생성된 이미지와 전처리된 텍스트 대 이미지 모델을 핀셋링하며, 여기서 핀셋링 손실은 텍스트 이미지 정렬 보상에 의해 가중된다. 자연어 가공에서 인간 피드백(RLHF)(Ouyang et al., 2022; Stiennon et al., 2020)으로부터의 강화 학습에 의해 영감을 받아 이미지 리워드(Xu et al., 2023)는 인간 선호도와 정렬하여 텍스트 대 이미지 모델을 개선하기 위해 범용 보상 모델을 구축한다.\n' +
      '\n' +
      '이러한 방법에 의해 달성되는 일부 개선에도 불구하고, (i) 기존 레이아웃 기반 또는 주의-기반 방법은 거칠고 최적이 아닌 공간 안내만을 제공할 수 있고 중첩된 객체(Cao et al, 2023; Hertz et al, 2022; Lian et al, 2023)를 다루기 위한 투쟁, (ii) 피드백 기반 방법은 고품질의 피드백을 수집하고 추가적인 훈련 비용을 발생시켜야 하는 구성/복합 이미지 생성의 맥락에서 여전히 두 가지 주요 한계가 있다.\n' +
      '\n' +
      '이러한 한계를 해결하기 위해 우리는 확산 모델의 구성성과 제어성을 향상시키기 위해 (다중 모드) LLM의 인상적인 다중 모드 추론 능력을 풀어내는 새로운 _트레이닝 프리_ 텍스트 대 이미지 생성/편집 프레임워크, 즉 _ 재조합, 플랜 및 젠티어(RPG)_를 소개한다. 우리는 RPG에서 세 가지 핵심 전략을 제안한다.\n' +
      '\n' +
      '** 멀티모달 레션*** 우리는 텍스트 프롬프트를 고도로 기술되는 텍스트 프롬프트로 전문적으로 변환하여 확산 모델에서 유익한 증강 프롬프트 이해력과 의미 정렬을 제공한다. 우리는 LLM을 사용하여 텍스트 프롬프트를 별개의 서브프로젝션으로 분해하고, 더 자세한 설명으로 재선택한다. 우리는 MLLM을 사용하여 생성된 이미지와 타겟 프롬프트 사이의 의미 불일치를 식별하기 위해 입력 이미지를 자동으로 재선택한다.\n' +
      '\n' +
      '*** 사상 계획*** 개척 접근법에서 이미지 공간을 보완 하위 영역으로 분할하고 각 하위 영역에 다른 서브 촉진제를 할당하여 구성 생성 작업을 여러 개의 더 간단한 하위 과제로 분해한다. 태스크 지시와 텍스트 내 예를 광범위하게 제작하지만 효율적인 지역 분열을 위해 MLLM(장 등 2023)의 강력한 체인의 추론 능력을 활용합니다. 재선택 중간 결과를 분석하여 후속 이미지 구성에 대한 상세한 합리성 및 정확한 지침을 생성한다.\n' +
      '\n' +
      '*** 보완 지역 융합***는 계획되지 않은 하위 영역과 각각의 프롬프트에 따라 구성 텍스트 대 이미지 생성의 유연성 및 정밀도를 향상시키기 위해 _보완적 지역 확산_을 제안한다. 구체적으로, 우리는 독립적으로 지정된 직사각형 하위 영역 내에서 하위 촉진에 의해 유도되는 이미지 콘텐츠를 생성하고, 이어서 _resize-and-concatenate_ 접근법에서 공간적으로 병합한다. 이 지역별 확산은 서로 겹치는 이미지 콘텐츠의 도전을 효과적으로 다룬다. 또한, 윤곽 기반 지역 확산을 사용하여 편집 작업을 수용할 수 있도록 이 프레임워크를 확장하여 수정을 목표로 하는 일관되지 않은 지역의 정확한 조작을 가능하게 한다.\n' +
      '\n' +
      '이 새로운 RPG 프레임워크는 폐쇄 루프 방식으로 텍스트 유도 이미지 생성 및 편집 작업을 모두 단일화할 수 있다. 우리는 RPG 프레임워크와 그림 1의 이전 작업을 비교하고 주요 기여도를 다음과 같이 요약한다.\n' +
      '\n' +
      '* 우리는 확산 모델의 합성성과 제어성을 최대한 향상시키기 위해 _ 재조합, 계획 및 유전자(RPG)_와 같은 새로운 훈련 없는 텍스트 대 이미지 생성 프레임워크를 제안한다.\n' +
      '* RPG는 MLLM을 _멀티모달 재선택기 및 CoT 플래너_ 둘 다로 사용하여 조향 확산 모델에 대한 보다 유익한 지침을 수정한 최초의 것이다.\n' +
      '* 우리는 구성 이미지 생성 및 정확한 이미지 편집을 위해 MLLM과의 극단적인 협업을 가능하게 하기 위해 _보완적 지역 확산_를 제안한다.\n' +
      '* 우리 RPG 프레임워크는 사용자 친화적이며 다양한 MLLM 아키텍처(예: 미니GPT-4) 및 확산 백본(예: 컨트롤Net)으로 일반화될 수 있다.\n' +
      'SDXL, DALL-E 3 및 이노스트픽스2픽스와 같은 이전 SOTA 방법과* 집중 정성적 및 정량적 비교는 우수한 텍스트 유도 이미지 생성/편집 능력을 보여준다.\n' +
      '\n' +
      '## 2 Method\n' +
      '\n' +
      '확인된 RPG.\n' +
      '\n' +
      '이 섹션에서는 새로운 훈련 없는 프레임워크 - **R**케이션, **P**lan 및 **G***enerate(**RPG*******)를 소개합니다. 우리는 그림 4에 묘사된 바와 같이 텍스트 대 이미지 생성(섹션 2.2)에서 RPG의 세 가지 기본 전략을 설명하며, 구체적으로 다중 엔티티 및 관계를 포함하는 복잡한 텍스트 프롬프트를 고려할 때, 이를 염기 프롬프트 및 고도로 기술되는 서브프로그래프로 분해함으로써 프롬프트(멀티모달) LLM을 _recaption_ 프롬프트로 레버리지(멀티모달)한다. 이어서, 공간 축을 따라 상보적 영역에 분할(서브) 촉진제를 할당하기 위해 멀티모달 CoT 계획을 활용한다. 이러한 과제를 기반으로 독립적으로 이미지 래치들을 생성하고 각 샘플링 단계에서 집계하기 위해 _보완적 지역 확산_를 소개한다.\n' +
      '\n' +
      '우리의 RPG 프레임워크는 2.3절에서 예시된 바와 같이 최소한의 조정으로 텍스트 유도 이미지 편집으로 애플리케이션을 확장함으로써 범용성을 나타내며, 예를 들어 재선택 단계에서 MLLM을 사용하여 짝을 이루는 표적 프롬프트 및 소스 이미지를 분석하여 교차 모달 의미 불일치를 포착하는 유익한 다중 모드 피드백을 초래한다. 복합 CoT 계획에서 우리는 단계별 편집 계획을 생성하고 지역 확산을 위한 _precise 윤곽_을 생성한다. 또한, 2.3절에서 볼 수 있듯이 진행성 자기 감작을 위한 폐쇄 루프 방식으로 RPG 워크플로우 실행 능력을 보여준다. 이 접근법은 정밀한 윤곽 기반 편집과 보완적인 지역 확산 생성을 결합한다.\n' +
      '\n' +
      '### Text-to-image Generation\n' +
      '\n' +
      '프로그래밍(y^{c}\\)은 서로 다른 속성과 관계를 갖는 다수의 엔티티들을 포함하는 복잡한 사용자 프롬프트이다. MLLM을 사용하여 \\(y^{c}\\)의 핵심 문구를 식별하여 표시된 하위 표현을 얻었다.\n' +
      '\n' +
      '>({n}\\subsubseteq y^{c}, \\{y^{i)}}}, \\{y^{i}}_{i=0}^{n} =^{n}=\\{y^{0},y^{1}.\n' +
      '\n' +
      '여기서 \\(n\\)는 핵심 문구의 수를 나타낸다. 사전 훈련된 ** 영상 대 텍스트***(I2T) 캡션 모델을 사용하여 이미지에 대한 설명 프롬프트를 생성하고 고품질 이미지-텍스트 쌍을 갖는 새로운 데이터 세트를 구성하는 DALL-E 3(Betker et al., 2023)에 의해 영감을 받았다. 대조적으로, 우리는 LLM의 인상적인 언어 이해와 추론 능력을 레버리지하고 LLM을 **텍스트 대 텍스트***(T2T) 캡션으로 사용하여 보다 유익한 세부 설명으로 각 서브프로그래프를 추가로 사용한다.\n' +
      '\n' +
      '\\[\\{\\hat{y}^{y}^{y},\\hat{y}^{y}}.\n' +
      '\n' +
      '이와 같이 생성된 이미지의 충실도를 효과적으로 개선하고 신속한 이미지와 이미지 사이의 의미적 불일치를 줄이기 위해 각 서브프로그래핑에 대해 더 조밀하게 미세한 디테일을 생성할 수 있다.\n' +
      '\n' +
      '재선택 하위 촉진에 기초하여, 우리는 확산 모델에 대한 최종 이미지 콘텐츠의 구성을 계획하기 위해 LLM(장 등 2023)의 강력한 멀티모달 체인 추론 능력(CoT)을 레버리지한다. 분명히, 우리는 이미지 공간 \\(H\\tcer W\\)을 여러 _보완 영역_로 나누고, 각각의 증강 하위 촉진자\\(\\hat{y}^{i}\\)를 특정 영역 \\(R^{i}\\)에 할당한다.\n' +
      '\n' +
      'R^{n}\\}\\{R^{i} W, \\{R^{i}}_{i=0}^{n}=^{n}=\\{R^{0},R^{1}.\n' +
      '\n' +
      '의미 있고 정확한 하위 영역을 생산하기 위해서는 (i) 영역 매개변수의 계획을 위한 두 가지 구성 요소를 신중하게 특정해야 하며, (i) 영역 파라미터는 행이 ";"에 의해 분리되고 각 열은 커브(예: "1,1,1")에 의해 분리된 일련의 숫자로 표시된다. 구체적으로, 먼저 ";"를 사용하여 이미지를 다른 행으로 나눈 다음 각 행 내에서 커브를 사용하여 행을 다른 영역으로 분할한다. 더 나은 이해를 위해 그림 2.2를 참조하며, (ii) 영역별 과제 사양은 MLLM을 지시하는데, 우리는 지역 분할 계획을 이유하기 위해 설계되는 일부 텍스트 예시와 함께 MLLM의 CoT 추론을 사용한다. 우리는 여기 설명서 및 문자에 대한 간략화된 템플릿을 제공합니다.\n' +
      '\n' +
      '그림 4: 텍스트 대 이미지 생성을 위한 RPG 프레임워크를 참조하세요.\n' +
      '\n' +
      '그림 5: 지역 분할의 예시적 예.\n' +
      '\n' +
      '수치 정확도를 보장하기 위해 (예컨대, 5개의 사과) 다른 영역에 별도로 할당되며, 두 개체 간의 복잡한 상호 작용을 용이하게 하기 위해(i) 전체로 이 두 개체를 부착하여 특정 개체(Lian et al, 2023)에서 언급된 모순된 생성 결과를 회피할 것이다. (더 긴 우뚝 솟은) 그린 헤어 쌍둥이와 레드 블라우스, 블루 스커트입니다.\n' +
      '\n' +
      '보완적 지역 확산 최근 작품(Liu et al, 2022; Wang et al., 2023; Chefer et al., 2023; Feng et al., 2022)은 구성 생성을 용이하게 하기 위해 교차 의도적인 마스크 또는 레이아웃을 조정했다. 그러나 이러한 접근법은 주로 래치들을 쌓는 것에 의존하여 서로 중첩된 지역에서 갈등과 모호한 결과를 낳는다. 이 문제를 해결하기 위해 그림 6과 같이 지역별 세대 및 이미지 구성을 위한 보완적 지역 확산이라는 새로운 접근법을 소개한다. 중첩되지 않은 상보적인 직사각형 영역을 추출하고 **resize-and-concatenate** 후처리 단계를 적용하여 고품질 구성 생성을 달성합니다. 또한, 각 생성된 영역의 연결을 강화하고 전체 이미지 정합성(제4절에서 세부 절제 연구)을 유지하기 위해 염기 프롬프트와 재선택 서브프로그래프를 결합하여 정합성을 향상시킨다. 이것은 그대로 나타낼 수 있습니다.\n' +
      '\n' +
      '{t}(이하^{f{x}_{t}}^{{i}.^{i} <^{i}_{i} <^{i}_{i} <^{n},t),\\{4}}\n' +
      '\n' +
      'HH(s\\)가 고정된 무작위 종자인 경우 CRD는 보완적인 지역 확산을 위한 약자이다.\n' +
      '\n' +
      '보다 구체적으로, 우리는 기저 프롬프트 \\(y^{\\text{base}}=y^{c}\\)와 재선택 하위 촉진으로 신속한 배치를 구성한다.\n' +
      '\n' +
      '\\[\\text{ 촉진 Batch:}\\quad\\{y^{{\\{{\\{\\{\\{\\{onetext{base}}},\\{\\hat{y}^{i}^{i}_{i=0}^{n}\\}.\n' +
      '\n' +
      '그림 6: ** 보완 지역 세확산***에서 각 샘플링 단계의 입증.\n' +
      '\n' +
      '그런 다음 각 타임스텀에서 프롬프트 배치를 데스토징 네트워크로 전달하고 교차 선택 계층을 조작하여 그림 6과 같이 서로 다른 래트(n+1\\)를 생성한다(\\{\\mathbf{z}^{i}_{t-1}}_{t=0}^{n}\\)를 동시에 생성할 수 있다.\n' +
      '\n' +
      'W_{dot\\psi (W_{dot\\psi,\\hat{y}^{dot\\psi) (\\hathbf{i}.\n' +
      '\n' +
      '이미지 잠재 \\(\\mathbf{z}_{t}\\)가 질의이고 각각의 서브 촉진 \\(y^{i}\\)가 키 및 값으로 작동한다. (W_{Q},W_{K},W_{V},W_{V}\\)는 선형 투영이고 \\(d\\)는 키와 질의의 잠재 투영 차원이다. 그런 다음, 생성된 영역 \\(\\{\\mathbf{z}^{i}_{t-1}\\}_{i=0}^{n}\\)을 재구성하고 연결하여 할당 지역 번호(0~ \\(n\\)) 및 각 비율에 따라 진행한다. 여기서 우리는 각각 재구성된 보완적인 숨겨진 상태를 나타낸다.\n' +
      '\n' +
      '\\[\\mathbf{z}^{i}_{t-1}(h,w)=\\text{Resize}(\\mathbf{z}^{i}_{t-1}), \\tag{7}\\]\n' +
      '\n' +
      '(h,w\\)가 지역의 높이와 폭인 경우. 공간 축을 따라 직접 연결합니다.\n' +
      '\n' +
      '\\[\\mathbf{z}^{\\text{cat}}_{t-1}=\\text{Concatate}(\\{\\{mathbf{z}^{i}_{t-1}_{t-1}(h,w)\\}_{i=0}^ {n})\\[\\mathbf{z}}.\n' +
      '\n' +
      '서로 다른 지역의 경계에서 일관된 전환과 각 지역 내의 엔티티 간의 조화로운 융합을 보장하기 위해 _베이스 래치_\\(\\mathbf{z}^{\\text{ 염기}}_{t-1}\\)와 _concatenated 잠복_\\(\\mathbf{z}^{{t-1}}_{t-1})의 가중 합을 사용하여 최종 변성 출력을 생성한다.\n' +
      '\n' +
      '\\[\\mathbf{z}_{t-1}=\\beta*\\mathbf{z}}^{t 및\\mathbf{z}}}_{t-1}}_{t-1}}_{t-1}(1-\\beta)*\\mathbf{z}}.\n' +
      '\n' +
      '여기서 \\(\\beta\\)는 생성된 이미지의 복잡한 텍스트 프롬프트와의 인간 미학적 인식과 정렬 사이의 적절한 균형을 달성하는 데 사용된다. 보완적 지역 확산은 3.1절에서 평가될 SDXL(Podell et al., 2023), ConPreDiff(양 et al., 2023b), 제어넷(Zhang et al., 2023a)을 포함한 임의의 확산 백본으로 일반화할 수 있다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '교과서에서는 이미지 계산서를 안내합니다.\n' +
      '\n' +
      '우리의 RPG를 선택하면 그림 7과 같이 텍스트 유도 이미지 편집 작업으로 일반화될 수 있으며, 재선택 단계에서는 RPG가 MLLM을 소스 이미지를 재선택하기 위한 자막으로서 채택하고 이미지와 목표 프롬프트 사이의 미세화된 의미 불일치를 식별하는 강력한 추론 능력을 레버리지한다. 입력 이미지 \\(\\mathbf{x}\\)가 목표 프롬프트 \\(y^{\\text{tar}}\\)와 어떻게 일치하는지 직접 분석한다. 구체적으로 \\(\\mathbf{x}\\)와 \\(y^{\\text{tar}}\\)의 핵심 실체를 파악한다.\n' +
      '\n' +
      'r{m}} \\{m\\{i}}(\\mathbf{i}:\\{aq y^{i}} <^{i} <^{i} <^{i}>} <\\{t{i} <^{i}> <\\{{i} <^{m}> <\\{{i}>} <\\{t{i} <^{i}>} <\\{t{i}.\n' +
      '\n' +
      '그런 다음 수치 정확성, 속성 결합 및 객체 관계에 관한 MLLM(예: GPT4(개방AI, 2023), 게미니 Pro(팀 등 2023)을 사용하여 \\(\\{y^{i}_{i = 0}^{i}.0}^{i}_{i=0}^{m}\\) 및 \\(\\{e^{i}.0}\\) 간의 차이를 확인하기 위해 MLLM(예를 들어 GPT4(OpenAI, 2023), 게미니 Pro(팀 et al., 팀 등은 2023)을 사용한다. 결과적인 복합적 이해 피드백은 편집 계획을 이유로 MLLM으로 전달될 것이다.\n' +
      '\n' +
      '그림 7: RPG는 폐쇄 루프 접근법에서 텍스트 유도 이미지 생성 및 편집을 통일한다.\n' +
      '\n' +
      '신속함과 이미지 사이의 캡처된 의미 불일치를 기반으로 에디션을 위한CoT 계획, RPG는 자동 필터링된 고품질 인-컨텍스트 예시로 MLLM의 CoT 추론 능력을 유발하며, 이는 엔티티 누락/복원성, 속성 불일치, 모호한 관계와 같은 단계 편집 케이스를 수동으로 설계했다. 여기서는 RPG에서 이러한 문제를 처리하기 위한 세 가지 주요 편집 작업을 도입하는데, 이는 추가 \\(\\text{ 추가}()\\), 삭제 \\(\\text{Del}()\\), 수정 \\(\\text{Mod}()\\)이다. RPG는 접지 맥락으로 복합 피드백을 받아 일련의 편집 지침을 계획하고 있다. 예를 들어 \\(텍스트{Plan}(y^{\\text{tar}},\\mathbf{x})는 구성된 운영 목록으로 나타낼 수 있다.\n' +
      '\n' +
      '\\[\\{{i},\\mathbf{x}),\\cots,\\text{j}(y^{j},\\mathbf{x}),\\cots,\\text{Mod}(y^ {k},\\mathbf{x})\\}\n' +
      '\n' +
      'HH(i,j,k<=n,\\text{ 길이의})의 경우(y^{\\text{tar}}},x^{0}))=L\\이다. 이와 같이 보다 정확한 결과를 위해 원래의 복잡한 편집 작업을 보다 간단한 편집 과제로 분해할 수 있다.\n' +
      '\n' +
      '콘텐츠 기반 지역 융합은 CoT 계획 편집 지침과 보다 효과적으로 협업하기 위해 보완적인 지역 확산을 텍스트 유도 편집으로 일반화한다. 편집 지도(키릴로프 등, 2023)의 목표 윤곽을 찾아 가리고 확산 기반 인포팅(Rombach et al., 2022)을 적용하여 계획 운영 목록 \\(y^{\\text{tar}}},\\mathbf{x})에 따라 목표 윤곽 영역을 편집한다. 편집용 교차의도 지도 스왑 또는 교체(Hertz et al, 2022; Cao et al., 2023)를 활용하는 전통적인 방법에 비해 CoT 기획으로 구동되는 **mask-and-inpainting*** 방법은 보다 정확하고 복잡한 편집 작업(즉, 추가, 삭제 및 수정)을 가능하게 한다.\n' +
      '\n' +
      '우리의 텍스트 유도 이미지 편집 작업 흐름을 위한 멀티 사운드 에디션은 윤곽 기반 편집과 보완적인 지역 확산 생성을 결합한 폐쇄 루프 자체 반사 텍스트 대 이미지 생성에 적응 가능하다. 우리는 대상 텍스트 프롬프트와 밀접하게 정렬하기 위해 생성된 이미지를 점진적으로 정제하기 위해 MLLM에 의해 제어되는 멀티 그라운드 닫힌 루프 RPG 워크플로우를 수행할 수 있다. 시간 효율성을 고려하여 폐쇄 루프 절차에 갇히지 않도록 최대 라운드를 설정했습니다. 이러한 폐쇄 루프 패러다임을 바탕으로 우리는 RPG에서 텍스트 유도 생성 및 편집을 통일하여 커뮤니티에 보다 실용적인 프레임워크를 제공할 수 있다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '### Text-to-Image Generation\n' +
      '\n' +
      '우리의 RPG는 일반적이고 확장성이 있으며 임의의 MLLM 아키텍처 및 확산 백본을 프레임워크에 통합할 수 있다. 우리의 감각에서\n' +
      '\n' +
      '그림 8: RPG와 SOTA 텍스트 대 이미지 모델(SDXL (Podell et al., 2023), DALL-E 3 (Betker et al., 2023) 및 LLM 지상 확산 모델 LMD+(Lian et al., 2023)의 정성적 비교는 그림 8이다.\n' +
      '\n' +
      '우리는 GPT-4(개방AI, 2023)를 재선택기 및 CoT 플래너로 선택하고 SDXL(Podell et al., 2023)을 염기 확산 백본으로 사용하여 RPG 프레임워크를 구축한다. 분명히, MLLM의 CoT 계획 능력을 유발하기 위해, 우리는 적은 샷 프롬프트를 수행하기 위해 작업 인식 템플릿 및 고품질 인컨텍스트 예를 신중하게 설계한다. Base 프롬프트_ 및 그 가중 하이퍼파라미터 _베이스 비율_가 지역 확산에 중요한 경우 그림 15에서 추가 분석을 제공했으며, 사용자 프롬프트에는 동일한 클래스를 가진 엔티티(예: 2명의 여성, 4명의 소년)가 포함되면 이러한 뚜렷한 동일성을 강조하기 위해 더 높은 염기 비율을 설정해야 한다. 반대로, 사용자 프롬프트가 다른 클래스 이름(예: 세라믹 꽃병 및 유리 꽃병)을 갖는 엔티티들을 포함할 때, 베이스 프롬프트와 서브프로프트의 혼란을 피하기 위해 더 낮은 염기 비율이 필요하다.\n' +
      '\n' +
      '주요 결과는 **(i) 유도 결합***의 세 가지 주요 구성 시나리오에서 이전 SOTA 텍스트 대 이미지 모델 DALL-E 3(Betker et al., 2023), SDXL 및 LMD+(Lian et al., 2023)과 비교된다. 이 시나리오의 각 텍스트 프롬프트는 상이한 엔티티들에 결합하는 다수의 속성들을 갖는다. ***(ii) 내츄릭 Accuracy****. 이 시나리오의 각 텍스트 프롬프트는 동일한 클래스 이름을 공유하는 다수의 엔티티들을 가지며, 각 엔티티의 개수는 2개 이상이어야 한다. **(iii) 복합관계****. 이 시나리오의 각 텍스트 프롬프트는 서로 다른 속성 및 관계(예를 들어, 공간 및 비-스피탈리티)를 갖는 다수의 엔티티들을 갖는다. 표 1에서 입증된 바와 같이, 우리의 RPG는 세 가지 시나리오 모두에서 이전 모델보다 훨씬 우수하며 텍스트 프롬프트와 정렬하는 데 충실도와 정밀도의 놀라운 수준을 달성한다. 우리는 SDXL과 DALL-E 3이 숫자 정확도 및 복잡한 관계와 관련하여 세대 성능이 좋지 않음을 관찰한다. 대조적으로, RPG는 정확한 수의 하위 영역을 효과적으로 계획하고 구성 생성을 달성하기 위해 제안된 보완적 지역 확산을 활용할 수 있다. LLM이 마련한 레이아웃 기반 텍스트-이미지 확산 모델인 LMD+(Lian et al., 2023)에 비해 우리의 텍스트-이미지 확산 모델이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{3}{c}{**Attribute Binding**} & \\multicolumn{3}{c}{**Object Relationship**} & \\multirow{2}{*}{**Complex\\(\\uparrow\\)**} \\\\ \\cline{2-2} \\cline{5-6}  & **Color**\\(\\uparrow\\) & **Shape\\(\\uparrow\\)** & **Texture\\(\\uparrow\\)** & **Spatial\\(\\uparrow\\)** & **Non-Spatial\\(\\uparrow\\)** \\\\ \\hline Stable Diffusion v1.4 (Rombach et al., 2022) & 0.3765 & 0.3576 & 0.4156 & 0.1246 & 0.3079 & 0.3080 \\\\ Stable Diffusion v2 (Rombach et al., 2022) & 0.5065 & 0.4221 & 0.4922 & 0.1342 & 0.3096 & 0.3386 \\\\ Composable Diffusion (Liu et al., 2022) & 0.4063 & 0.3299 & 0.3645 & 0.0800 & 0.2980 & 0.2898 \\\\ Structured Diffusion (Feng et al., 2022) & 0.4990 & 0.4218 & 0.4900 & 0.1386 & 0.3111 & 0.3355 \\\\ Attn-Ext v2 (Chefer et al., 2023) & 0.6400 & 0.4517 & 0.5963 & 0.1455 & 0.3109 & 0.3401 \\\\ GORS (Huang et al., 2023a) & 0.6603 & 0.4785 & 0.6287 & 0.1815 & 0.3193 & 0.3328 \\\\ DALL-E 2 (Ramesh et al., 2022) & 0.5750 & 0.5464 & 0.6374 & 0.1283 & 0.3043 & 0.3696 \\\\ SDXL (Betker et al., 2023) & 0.6369 & 0.5408 & 0.5637 & 0.2032 & 0.3110 & 0.4091 \\\\ PixArt-\\(\\alpha\\)(Chen et al., 2023a) & 0.6886 & 0.5582 & 0.7044 & 0.2082 & 0.3179 & 0.4117 \\\\ ConPreDiff (Yang et al., 2023b) & 0.7019 & 0.5637 & 0.7021 & 0.2362 & 0.3195 & 0.4184 \\\\ \\hline\n' +
      '**RPG (Ours)** & 0.8335 & 0.6801 & 0.8129 & 0.4547 & 0.3462 & 0.5408 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: T2I-CompBench에 대한 평가 결과는 표 1이다. RPG는 속성 결합, 객체 관계 및 복잡한 구성에 관한 최상의 성능을 일관되게 보여준다. 우리는 \\(\\overline{\\text{blue}}\\)에서 가장 좋은 점수와 녹색에서 두 번째 최고 점수를 나타낸다. 기준 데이터는 첸 et al.(2023a)로부터 인용된다.\n' +
      '\n' +
      '그림 9: 위계적 지역 확산의 실증이다. 더 많은 계층으로 융합하면 더 만족스러운 결과를 얻을 수 있다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:10]\n' +
      '\n' +
      '강소픽스2픽스(브룩스 등, 2023)와 마사Ctrl(Cao et al., 2023). 프로그래프2 촉진 및 마사Ctrl은 주로 텍스트-그라운드 교차 선택 스왑 또는 교체를 통해 편집을 수행하며, 구조픽스2픽스는 인간의 지시를 따를 수 있는 모델을 배우는 것을 목표로 한다. 그림 11에 제시된 바와 같이 RPG는 이전 방법보다 더 정밀한 편집 결과를 생성하고, 우리의 마스크 앤 인포팅 편집 전략도 소스 이미지의 의미 구조를 완벽하게 보존할 수 있다.\n' +
      '\n' +
      '멀티 사운드 에디팅은 그림 12에서 RPG 프레임워크로 자기 환제를 평가하기 위해 멀티 그라운드 편집을 수행하며, RPG 기반의 자기 감금이 정밀도를 크게 향상시킬 수 있다고 결론을 내리고 재선택 기반 복합 피드백 및 CoT 기획의 효과를 보여준다. 또한 RPG가 3라운드 내에서 만족스러운 편집 결과를 얻을 수 있다는 것을 알게 되었습니다.\n' +
      '\n' +
      '4개의 모형 분석.\n' +
      '\n' +
      '우리는 재선택에 대한 절제 연구를 수행하며, 그 결과를 그림 13에 보여주고 있는데, 그 결과 재선택 없이 모델이 생성된 이미지에서 일부 핵심 단어를 무시하는 경향이 있음을 관찰한다. 우리의 재선택은 정보성이 높고 더 밀도가 높은 디테일로 이러한 핵심 단어를 설명할 수 있으므로 보다 섬세하고 정밀한 이미지를 생성할 수 있다.\n' +
      '\n' +
      '그림 14에서 입증된 바와 같이 CoT 계획에 대한 절제 연구에서 CoT 계획이 없는 모델이 텍스트 프롬프트로부터 복잡한 관계를 파싱하고 전달하지 못한다는 것을 관찰했다. 대조적으로, 우리의 CoT 계획은 모델이 텍스트 프롬프트로부터 미세화 된 속성과 관계를 더 잘 식별하는 데 도움이 될 수 있으며, 보다 현실적인 계획 구성을 통해 표현할 수 있다.\n' +
      '\n' +
      'RPG에서 베이스 프롭트의 효과, 우리는 이미지 조성물의 정합성을 향상시키기 위해 확산 모델의 염기 프롬프트에서 생성된 잠재력을 레버리지한다. 여기에서 우리는 그림 15에서 이에 대한 더 많은 분석을 수행하며, 결과는 염기 프롬프트의 적절한 비율이 서로 다른 하위 영역의 연관성에 도움이 될 수 있다는 것을 발견하여 보다 자연스러운 구성을 가능하게 한다. 또 다른 발견은 베이스 프롬프트와 지역 프롬프트 사이의 혼란 때문에 과도한 염기 비율이 바람직하지 않은 결과를 초래할 수 있다는 것이다.\n' +
      '\n' +
      '그림 12: RPG 프레임워크로 멀티 그라운드 텍스트 유도 이미지 편집입니다.\n' +
      '\n' +
      '그림 13: RPG 재선택에 대한 구조 연구는 다음과 같다.\n' +
      '\n' +
      '5개의 관련 작업.\n' +
      '\n' +
      '확산 모델(Sohl-Dickstein et al., 2015; 송 및 에르몬, 2019; 송, 에르몬, 2020; 송, 에르몬, 2020)은 생성 모델의 유망한 계층이며 Dhariwal and Nichol(2021)은 생성 적대 네트워크(GAN)(Reed et al, 2016; Creswell et al., 2018)보다 확산 모델의 우수한 이미지 합성 품질을 입증했다. GLIDE(니콜 et al., 2021), 이젠(사아리아 et al., 2022)은 텍스트 프롬프트와 생성된 이미지 사이의 의미 정렬을 개선하기 위해 이미지 샘플링 과정에서 텍스트 유도 이미지 합성, 사전 훈련된 CLIP 모델(라드포드 et al, 2021, 라펠 et al., 2020)을 활용하는 데 중점을 둔다. Latent Diffusion Models(LDMs)(Rombach et al., 2022)은 알고리즘 효율과 화질 균형을 위해 확산 과정을 픽셀 공간에서 잠재 공간으로 이동시킨다. SDXL(Podell et al., 2023) 드림보스(Ruiz et al., 2023), DALL-E 3(Betker et al., 2023)와 같은 텍스트 대 이미지 확산 모델의 최근 발전은 다양한 관점에서 품질과 정렬을 더욱 향상시킨다. 그들의 엄청난 성공에도 불구하고 복잡한 프롬프트로 고 충실도 이미지를 생성하는 것은 여전히 어렵다(Ramesh et al., 2022; Betker et al., 2023; Huang et al., 2023a). 이 문제는 공간적 관계, 속성 결합 및 숫자 인식과 관련된 구성적 설명을 다룰 때 악화된다. 본 논문에서는 MLLM의 강력한 CoT 추론 능력을 텍스트 대 이미지 확산 모델에 통합하여 이 문제를 해결하는 것을 목표로 한다.\n' +
      '\n' +
      '구성 융합 세대 최근 연구는 텍스트 대 이미지 확산 모델의 구성 능력을 향상시키는 것을 목표로 한다. 일부 접근법은 주로 훈련(Li et al., 2023; Avrahami et al., 2023; Zhang et al., 2023a; Zhang et al., 2023; Mou et al., 2023; 양 et al., Huang et al., 2023b;a)에서 확산 모델에 추가 모듈을 도입한다. 예를 들어, 공간적으로 조절된 이미지 생성을 위한 확산 모델 위에 GLIGEN(Li et al., 2023b), ReCo(양 et al., 2023e) 설계 위치 인식 어댑터를 설계한다. T2I-Ad캡터 및 컨트롤넷(Zhang et al., 2023a; Mou et al., 2023)은 의미 구조(Zhang et al., 2023b)를 제어하기 위한 이미지의 일부 고수준의 특징을 지정한다. 그러나 이러한 방법은 추가적인 훈련 및 추론 비용을 초래한다. 훈련 없는 방법은 추론 단계(펑 et al., 2022; Liu et al., 2022; Hertz et al., 2022; Cao et al., 2023; Chen et al, 2024; Chefer et al., 2023) 동안 공간적 또는 의미론적 제약에 따라 잠재 또는 교차 의도 지도를 조작하여 확산 모델을 제거하는 것을 목표로 한다. 복합식 디퓨전(Liu et al, 2022)은 구성 프롬프트를 더 작은 하위 촉진으로 분해하여 별개의 래치들을 생성하고 스코어 함수와 결합시킨다. Chen et al.(2024) 및 Lian et al.(2023)은 바운딩 박스(레이아웃)를 사용하여 구배를 잠재지로 다시 전파하고 모델이 특정 지역으로 교차 의도 지도를 조작할 수 있도록 한다. 다른 방법은 가우시안 낟알(케퍼 et al., 2023)을 적용하거나 언어적 특징(펑 et al., 2022; 라신 et al., 2023)을 통합하여 교차 의도 지도를 조작한다. 그럼에도 불구하고 이러한 조작 기반 방법은 거칠게 제어할 수 있을 뿐이며, 특히 중첩된 객체(Lian et al., 2023; Cao et al., 2023)를 다룰 때 만족스럽지 않은 구성 생성 결과로 이어질 수 있다. 따라서 MLLM이 분쇄한 효과적인 _훈련 없는 상보적 지역 확산 모델_를 도입하여 샘플링 과정에서 더 정확한 제어로 이미지 조성을 점진적으로 정제한다.\n' +
      '\n' +
      '그림 14: RPG에서 CoT 계획에 대한 구조 연구.\n' +
      '\n' +
      'Fig. 15: 상보적 지역 확산의 기반 프롬프트에 대한 구조 연구는 다음과 같다.\n' +
      '\n' +
      '임용세대** 대형 언어 모델(LLM; ChatGPT, 2022; 정관 et al., 2022; Iyer et al., 2022; Syer et al., 2022; Muennighoff et al., 2022; Muennighoff et al., 2022; Zaylor et al; 2022; Tower et al., 2022; Chowhip al., 2022; Touvass et al; Chowh et al., 2022; ly et al., 2022; Ping et al., 2022; ThennOIoff et al., 2022; Muennighoff et al., 2022; Thennoff et al., 2022; Thennoff et al., 2022; Thxt et al., 2022; Theng et al., 2022; Tand et al., 2022; Tand et al., 2022; Th painting et al., 2022; Tand et al., 2022; T stip et al., 2022; Term et al., 2022; Taylor et al., 2022; Taylor et al., 2022; Taylor et al., 2022; Taylor et al., 2022; Taylor et al., 2022; Taylor et al., 2022; Taylor et al., 2022; T ChatGPT(ChatGPT, 2022)와 같은 지도 예들은 지시 튜닝(Ouyang et al, 2022, Li et al, 2023c, Zhang et al., 2023c, Liu et al., 2023)과 같은 기술을 통해 고급 언어 이해력과 추론 기술을 선보였다. 또한, 멀티모달 대형 언어 모델(MLLM), (Koh et al., 2023; Fn et al, 2023; Fot et al, 2023; Fou et al, 2023; Sou et al, 2023; Gupta & Kembhavi, 2023; Gupta & Kembhavi)은 LLM을 비전 모델과 통합하여 이미지 이해, 추론 및 합성을 포함하여 언어 작업에서 비전 작업까지 인상적인 능력을 확장한다. LLM(ChatGPT, 2022; OpenAI, 2023; OpenAI, 2023)과 확산 모델(Ramesh et al, 2022; Betker et al., 2023) 간의 협력은 텍스트 이미지 정렬뿐만 아니라 생성된 이미지의 품질(유 et al, 2023, Chen et al, 2023b, 동 et al, 2023; 우 et al, 2023; Feng et al, 2023)을 크게 향상시킬 수 있다. 예를 들어, GILL(Koh et al., 2023)은 임의로 인터리빙된 이미지와 텍스트 입력들을 조건화하여 일관된 이미지 출력들을 합성할 수 있으며, Emu(Sun et al., 2023b)는 이미지 대 텍스트 및 텍스트 대 이미지 작업 모두에 대한 일반주의 멀티모달 인터페이스로 돋보인다. 최근 LMD(Lian et al., 2023)는 LLM을 활용하여 LLM(Li et al., 2023b)에서 바운딩 박스 레이아웃에 접지된 이미지를 생성하여 확산 모델의 구성 생성을 향상시킨다. 그러나 기존 작품은 주로 LLM을 단순 플러그인 성분으로 확산 모델에 통합하거나, 이미지 조성을 제어하기 위한 레이아웃 생성기로 LLM을 취한다. 대조적으로, MLLM을 사용하여 MLLM이 _region 기반_ 생성 및 편집 과정 모두에서 글로벌 태스크 플래너 역할을 하는 확산 모델에 대한 이미지 구성을 계획한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 복잡하거나 구성적인 텍스트 대 이미지 생성의 문제를 해결하기 위해 SOTA 훈련 없는 프레임워크 RPG, 마스터 확산 모델에 MLLM을 활용하는 것을 제안한다. RPG에서 우리는 설계된 MLLM 기반 재선택기 및 플래너와 협업하기 위해 보완적인 지역 확산 모델을 제안한다. 또한, RPG는 폐쇄 루프 접근법에서 텍스트 유도 이미개 생성 및 편집을 일원화할 수 있으며 MLLM 아키텍처 및 확산 백본으로 일반화할 수 있다. 향후 작업을 위해 보다 복잡한 양식을 입력 조건으로 통합하기 위한 새로운 프레임워크를 지속적으로 개선하고, 이를 보다 현실적인 응용프로그램으로 확장하고자 한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. 아비함, T. 하예, O. 가피니, S. 구파, Y. Taigman, D. 파라케, D. Lischinski, D. 프리드, O. 네, X. 제어 가능한 이미지 생성을 위한 Yin(2023) Spatio-텍스트 표현. 컴퓨터 비전 및 패턴 인식 관련 IEEE/CVF 회의 개최에서 pp. 18370-18380: SS1에 의해 발표되었다.\n' +
      '* O. 바털, L. 야리브, Y. 립맨과 T. 다이켈(2023) 다중확산: 제어된 이미지 생성을 위한 확산 경로를 융합한다. arXiv 프리프린트 arXiv:2302.08113: SS1에 의해 계산된다.\n' +
      '* J. 베커, G. 고, L. 제잉, T. 브룩스, 조 왕, L. 리, L. 오양, 주강, J. 이, Y. 구, 등(2023)은 더 나은 캡션으로 이미지 생성을 개선합니다. 컴퓨터 과학. 벚꽃. com/ Model/dall-e-3. pdf. SS1: SS1로 받았습니다.\n' +
      '* T. 브룩스, A. 힐스스키 및 A. A. Efros(2023) 구조 픽셀2 픽셀: 이미지 편집 지침을 따르도록 학습한다. 컴퓨터 비전 및 패턴 인식 관련 IEEE/CVF 회의 개최에서 pp. 18392-18402: SS1에 의해 발표되었다.\n' +
      '*M. 오, X. >왕, 자. Qi, Y. 산, X. 사이, Y. 정(2023)막: 일관된 이미지 합성 및 편집을 위해 튜닝이 없는 상호 자기 의도 제어입니다. arXiv 프리프린트 arXiv:2304.08465: SS1에 의해 계산된다.\n' +
      '* I. ChatGPT(2022)는 챗봇을 소개합니다. SS1: SS1로 받았습니다.\n' +
      '* H. 체퍼, Y. 알루프, Y. 브링커, L. 볼프, D. Cohen-Or(2023) 인텐-및 제외: 텍스트-이미지 확산 모델에 대한 주의 기반 의미 지침이다. 그래픽(TOG)42(4), pp 1-10에 대한 ACM 거래: SS1에 의해 계산된다.\n' +
      '* J* J. 첸, J. 유, C. Ge, L. 야오, E. 제이, Y. 우, Z. 왕, J Kwok, P. 루오, H. 루, et al.(2023) Pixart-alpha: 광학적 텍스트 대 이미지 합성을 위한 확산 변압기의 빠른 훈련. arXiv 프리프린트 arXiv:2310.00426: SS1에 의해 계산된다.\n' +
      '*M. Chen, I. Laina 및 A. Vedaldi(2024) 훈련 없는 레이아웃 제어를 교차 의도 안내와 함께 한다. 컴퓨터 비전 적용에 관한 IEEE/CVF 동계 회의 개최에서 pp. 5343-5353:SS1이 게시했다.\n' +
      '*W. 헨, I. 스피리도노바, J. 양, J. 가오, C. Li(2023) Llava-상호작용: 이미지 채팅, 분할, 생성 및 편집을 위한 올인원 데모이다. arXiv 프리프린트 arXiv:2311.00571: SS1에 의해 계산된다.\n' +
      '* A. 차우더리, S. 나랑, J. 데블린, M. 보스마, G. 미슈라, A. 로버츠, C. 션튼, S. 바햄, H. W. 정. 게르만, 예를 들어 (2023)팔름: 경로를 이용한 스케일링 언어 모델링이다. 기계학습연구24(240) 저널, pp 1-113:SS1에 의해 작성되었다.\n' +
      '크레스웰, A, 화이트, T, 두모울린, V, 아울쿠마란, K, 스펑푸타, B 및 바레트, A. 유전적 적대관계망. IEEE 신호 처리 잡지_, 35(1):53-65, 2018.\n' +
      '* 다라리왈과 니콜(2021) 다하리왈, P.와 니콜, A. 디퓨전 모델은 이미지 합성에서 그랜스를 꺾었다. 2021년 신경망 정보 처리 시스템_, 34:8780-8794의 발전이다.\n' +
      '*동 등은 (2023) 동, 한, C, 펑, Y, Qi, Z, Ge, Z, 양, J, Zhao, L., 선, H, 저우, H, Wei, H 등 __신자주의적 복합 이해력과 창작. arXiv 프리프린트 arXiv:2309.11499_, 2023.\n' +
      '*판 등은 G, G, 장, Z, 한, J, 루, G, Xu, H 및 리앙, X. 초과된 의미 보상을 가진 텍스트 대 이미지 확산 모델 _는 텍스트 대 이미지 확산 모델을 미세 구성 시멘트로 구현했다. arXiv 프리프린트 arXiv:2305.19599_, 2023.\n' +
      '*펑 등은 (2022) 펑, W, He, X, Fu, T. J, 제파니, V, 아쿨라, A R, 나라야나, P, 바두, S, 왕, X E 및 왕, W. Y는 구성 텍스트 대 이미지 합성을 위한 무정형 구조 확산 지침을 훈련한다. Eleventh 국제 학습 발표회의_, 2022년입니다.\n' +
      '*펑 등은 (2023) 풍, W, 주, W, Fu, T. 대언어 모델을 가진 구성 시각적 계획 및 생성 : 구성적 시각 계획(__j, Jampani, V, Akula, A, He, X, Basu, S, 왕, X E, 왕, W. Y. Layout-gpt)이다. arXiv 프리프린트 arXiv:2305.15393_, 2023.\n' +
      '* Fu et al. (2023) Fu, T. 후, W, 듀, X, 왕, W. Y, 양, Y, 간, Z. 다중 모드 대형 언어 모델을 통해 명령어 기반 이미지 편집을 안내하는 __ 멀티모달 대형 언어 모델을 통해 명령어 기반 이미지 편집을 안내한다. arXiv 프리프린트 arXiv:2309.17102_, 2023.\n' +
      '* Gupta와 Kembhavi (2023) Gupta, T. A. 비주얼 프로그래밍: 훈련 없이 구성 시각적 추론. 컴퓨터 비전 및 패턴 인식_ pp 14953-14962, 2023에 대한 IEEE/CVF 회의의 _검토에서.\n' +
      '* 헤르츠 등은 A(2022) 헤르츠, A, 목다디, R, 테네바움, J, 아헤르만, K, 프릿치, Y 및 코헨-또는 D. 프롭토 촉진 이미지 편집을 교차 주의 제어로 한다. arXiv 프리프린트 arXiv:2208.01626_, 2022.\n' +
      '*호 등은 (2020)호, J, J, Jain, A 및 Abbeel, P. 덴오징 확산 확률 모델. __. 신경 정보 처리 시스템_, 2020:6840-6851의 발전.\n' +
      '*황 등은 (2023a) 황, K, 선, K, Xie, E, Li, Z 및 Liu, X. T2i-compbench: 개방형 구성 텍스트 대 이미지 생성을 위한 포괄적인 벤치마크. __세계 구성 텍스트 대 이미지 생성을 위한 포괄적인 벤치마크. arXiv 프리프린트 arXiv:2307.06350_, 2023a.\n' +
      '*황(2023b) 황, L, Chen, D, Liu, Y, 선전, Y, Zhao, D 및 J. Composer: 합성 가능한 조건을 가진 창의적이고 제어 가능한 이미지 합성. arXiv 프리프린트 arXiv:2302.09778_, 2023b.\n' +
      '* 이이어 등(2022) 이이어, 에스, 린, XV, 파스누루, R, 미하예로프, T, 시미그, D, 유, P, Shuster, K, 왕, T, 류, Q, 쿠우, P. S. 등 일반화의 렌즈를 통한 스칼링 언어 모델 지시 메타 학습. arXiv 프리프린트 arXiv:2212.12017_ 2022년입니다.\n' +
      '* 키릴로프 등은 (2023) 키릴로프, A, 민룬, E, 라비, N, 마오, H, 롤랜드, C, 구스타프슨, L, 샤오, T, 화이트헤드, S, 베르그, A, W. 다른 것. __-Y, 예를 들어. arXiv 프리프린트 arXiv:2304.02643_, 2023.\n' +
      '* 고 등은 (2023) 고, 조, 프리드, D, 살라쿠트디노프, R. 다중 모드 언어 모델들로 이미지를 생성하는 _ __ 및 다중 모드 언어 모델들로 이미지를 생성하는 것이다. arXiv 프리프린트 arXiv:2305.17216_, 2023.\n' +
      '* Li 등은 (2023) Li, J, Li, D, 사바레스, S, 호이, S. 블립-2:부트스트래핑 언어 이미지(Bootstrapping 언어 이미지)는 냉동 이미지 인코더 및 대형 언어 모델들로 사전 훈련된다. arXiv 프리프린트 arXiv:2301.12597_, 2023a.\n' +
      '* Li 등은 (2023) Li, Y, Liu, H, 우, Q, 무, F, 양, J, 가오, J, 리, C, 이, Y. J. 리펜: 오픈셋 접지 텍스트 대 이미지 생성이다. 컴퓨터 비전 및 패턴 인식_, pp 22511-22521, 2023b에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '* 리는 (2023c) Li, Y, 장, C, 유, G, 왕, Z, Fu, B, Lin, G, 선전, C, Chen, L, Wei, Y. 스테이블렐라바: 합성 영상 대화 데이터로 튜닝된 시각적 지시 튜닝. __ 합성 영상 대화 데이터. arXiv 프리프린트 arXiv:2308.10253_, 2023c.\n' +
      '* Lian et al.(2023) Lian, L., Li, B, Yala, A 및 다렐, T. lm-지상 확산: _Llm-지상 확산: 큰 언어 모델을 가진 텍스트-이미지 확산 모델에 대한 신속한 이해도를 강화한다. arXiv 프리프린트 arXiv:2305.13655_, 2023.\n' +
      '* 류 등은 (2023) 류, H, Li, C, 우, Q 및 이, Y. J. 비주얼 지시 튜닝. __. arXiv 프리프린트 arXiv:2304.08485_, 2023.\n' +
      '* Liu et al. (2022) Liu, N, Li, S, Du, Y, 토랄바, A 및 테네바움, J. B. 구성 시각적 생성은 합성 가능한 확산 모델을 가지고 있다. 컴퓨터 비전_, pp 423-439에 관한 _유럽 회의에서 2022년 스프링거.\n' +
      '* 모우 등은 (2023) 모우, C, 왕, X, Xie, L., 장, J, Qi, Z, Shan, Y 및 Qie, X. T2i 어댑터: 텍스트 대 이미지 확산 모델에 대한 더 통제 가능한 능력을 파내기 위한 학습 어댑터. __이미지 확산 모델을 위한 학습 어댑터. arXiv 프리프린트 arXiv:2302.08453_, 2023.\n' +
      '* 무니혼프 등은 (2022) 무니호프, 나, 왕, 투타비카, 루츠, 로버츠, A, 바이데르만, S, 스바오, T, 바리, M., 선, S, 용, Z. M_X, Schoelkopf, H., et al. 크로스클링 일반화는 멀티태스핀셋링을 통한 것이다. arXiv 프리프린트 arXiv:2211.01786_ 2022년.\n' +
      '* 니콜 등은 (2022) 니콜, A, Dhariwal, P, Ramesh, A, Shyam, P, Mishkin, P, McGrew, B, Sutskever, I 및 Chen, M. 글라이드: 광자론적 이미지 생성 및 텍스트 유도 확산 모델 편집과 같은 글라이드는 __의 광자론적 이미지 생성이다. arXiv 프리프린트 arXiv:2112.10741_ 2021.\n' +
      '* OpenAI(2023) OpenAI, R. Gpt-4 기술 보고서입니다. arxiv 2303.08774. _iew. 제_, 2:3, 2023.\n' +
      '*오양(2022) 오양(2022) 오양(주, 우, J, 장, X, 알메시다, D, 알메시다, 웨인라이트, C, 미슈킨, P, 장, C, 아가왈, S, 슬라, K, 레이, A) 등은 인간의 피드백으로 지침을 따르도록 언어 모델을 훈련시킨다. 신경 정보 처리 시스템_, 2022년 35:27730-27744의 정보를 제공한다.\n' +
      '* 판 et al.(2023) 범, X, 동, L, 황, S, 펑, Z, 첸, W 및 위, F. 코스모스-g: 다중 모드 대형 언어 모델과 관련하여 이미지를 촬영한다. arXiv 프리프린트 arXiv:2310.02992_, 2023.\n' +
      '* 포델 등은 (2023) 포델, D, 영어, Z, 레이시, K, 블라트만, A, 도쇼린, T, 뮐러, J, 펜나, J, 람바흐, R. 고해상도 이미지 합성을 위한 잠재 확산 모델 개선: __고해상도 이미지 합성을 위한 잠재 확산 모델 개선. arXiv 프리프린트 arXiv:2307.01952_, 2023.\n' +
      '* Qu et al. (2023) L., 우, S., Fei, H., Nie, L., 추아, T. S. 레이아웃llm-t2i: 텍스트 대 이미지 생성을 위해 llm에서 레이아웃 지침을 수정한다. 멀티미디어_, pp 643-654에 대한 제31회 ACM 국제 회의의 _발표에서 2023년.\n' +
      '* Radford 등은 A(2021) Radford, A, A, 홀리스, C, Ramesh, A, Goh, G, Agarwal, S., Sastry, G, Askell, A, Mishkin, P., Clark, J 등 자연 언어 감독으로부터 시각적 모델을 전달할 수 있다. 머신러닝_, pp. 8748-8763에 관한 _국제회의에서 2021년 PMLR.\n' +
      '* 라펠 등은 (2020) 라펠, C, 샤제르, N, 로버츠, A, 이, K, 나랑, S, 마테나, M, 저우, Y, 리, W 및 류, P. J. 통일된 텍스트-텍스트 변압기로 전이 학습의 한계를 보여준다. 기계학습연구_ 저널 21(1):5485-5551 2020.\n' +
      '* 라메쉬 등은 (2022) 라메쉬, A, 다라리왈, P, 니콜, A, 추, C 및 첸, M. 클립 래치들을 가진 __ 계층적 텍스트-조건 이미지 생성  _ 클립 래치들을 갖는 계층적 텍스트-조건 이미지 생성이다. arXiv 프리프린트 arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* 라신 et al.(2023) 라신, R, Hirsch, E, Glickman, D, Ravfogel, S, 골드버그, Y 및 Chechik, G. Linguistic 결합은 확산 모델에서 주목 지도 정렬을 통한 속성 대응이다. arXiv 프리프린트 arXiv:2306.08877_, 2023.\n' +
      '* Reed et al. (2016) Reed, S, Akata, Z, 옌, X, Logeswaran, L., Schiele, B 및 H. Generative advers 말라리아 텍스트가 이미지 합성에 설명되어 있다. 머신러닝_, pp 1060-1069에 관한 _국제회의에서 2016년 PMLR.\n' +
      '* 람바흐(2022) 루바흐, R, 블라트만, A, 로렌츠, D, 에서, P 및 Ommer, B. 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식_ pp 10684-10695에 대한 IEEE/CVF 회의의 _발표에서 2022년 pp. 10684-10695.\n' +
      '* Ruiz et al. (2023) Ruiz, N, Li, Y, Jampani, V, Pritch, Y, Rubinstein, M 및 Aberman K. 드림보스: 주제 중심 생성을 위한 텍스트 대 이미지 확산 모델입니다. 컴퓨터 비전 및 패턴 인식_ pp. 22500-22510에 대한 IEEE/CVF 회의의 _검토에서 2023.\n' +
      '* 사아리아, C, Chan, W, Saxena, S., Li, L, Whang, J, Denton, E. L., Ghasemipour, K, Gontijo Lintend, R., 카라골 Ayan, B, Salimans, T. 등 언어 이해가 깊은 사진론적 텍스트 대 이미지 확산 모델. 신경정보처리시스템_, 2022년 35:36479-36494의 효과.\n' +
      '* 소릴-디키슈타인 등은 (2015) 소울-디케스타인, J, 웨이스, E, 매서워라노사탄, N 및 강리, S. 비평형 열역학을 이용한 심층 비지도 학습. 머신러닝_, pp. 2256-2265에 관한 _국제회의에서 2015. PMLR.\n' +
      '* 송앤에몬(2019) 송, Y. S. 및 Ermon, S. 데이터 분포의 구배를 추정함으로써 생성 모델링. __ 데이터 분포의 구배를 추정함으로써 생성 모델링이다. 신경 정보 처리 시스템_, 2019년 32.\n' +
      '* 송앤에몬(2020) 송, Y. S. 및 Ermon, S. 점수 기반 생성 모델을 훈련하기 위한 개선 기법 __ 점수 기반 생성 모델 트레이닝을 위한 개선 기술. 신경 정보 처리 시스템_, 2020년 33:12438-12448의 발전이다.\n' +
      '* 송 등은 (2020) 송, Y, 소l-디키슈타인, J, 킹마, D. P, 쿠마르, A, 에르몬, S 및 포올, B. 스코어 기반 생성 모델링을 확률적 차등 방정식을 통한 것이다. arXiv 프리프린트 arXiv:2011.13456_ 2020.\n' +
      '* Stiennon et al.(2020) Stiennon et al.(2020) Stiennon, N., Ouyang, L., 우, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D 및 기독교o, P. F Learning는 인간의 피드백으로 요약한다. 신경 정보 처리 시스템_, 2020년 33:3008-3021의 발전.\n' +
      '*선 등(2023a) 선, J, Fu, D, Hu, Y, 왕, 라신, R., 후안, D.-C, 알론, D., 헤르만, C., 반 스테네키스트, S., 케리샤나, R. arXiv 프리프린트 arXiv:2311.17946_, 2023a.\n' +
      '*선 등은 선(2023b) 선, Q, 유, Q, 코이, 야, 장, F, 장, X, 왕, 야, 가오, H, 류, J, 황, T, 왕, X. 다중성에서의 생성적 척. __ 다중성에서의 생성적 척. _ 다중성에서의 생성적 척. arXiv 프리프린트 arXiv:2307.05222_, 2023b.\n' +
      '* Suris et al.(2023) Suris, D., Menon, S 및 Vondrick, C. Vipergpt: 추론을 위한 피리톤 실행을 통한 비주얼 추론. arXiv 프리프린트 arXiv:2303.08128_, 2023.\n' +
      '* 테일러 등은 (2022) 테일러, R, 카다스, M, 쿠쿠룰, G, 인텔럼, T, 하르트소르네, A, 사라비아, E, 푸울턴, A, 케르케즈, V 및 스토히닉, R. 갈락티카: 과학을 위한 큰 언어 모델. __Galactica: 과학의 큰 언어 모델. arXiv 프리프린트 arXiv:2211.09085_, 2022.\n' +
      '\n' +
      '*[팀 등.2023] 팀, G, 아나일, R, 보르지드, S, 우, Y, 알레이크, J-B, 유, 조루, 스루트, R, 슈알쿠크, J, Dai, A, 하우스, A, Hemini 등 매우 가능한 복합 모델 계열이다. arXiv 프리프린트 arXiv:2312.11805_, 2023.\n' +
      '* [Touvron et al.2023] Touvron, H., Lavril, T., Izacard, G., 마르티넷, X, Lachaux, M. -A, 라크로닉스, T, 로지에어, B, 고달, N, 함크로, E, 아조하, F, 에는 Llama: 오픈 및 효율적인 기반 언어 모델. arXiv 프리프린트 arXiv:2302.13971_, 2023.\n' +
      '*[왕 et al.2023] 왕, R, Chen, Z, Chen, C., Ma, J., 루, H., Lin, X. 확산 모델의 주의 지도 제어가 있는 구성 텍스트 대 이미지 합성 __ 구성 텍스트 대 이미지 합성은 확산 모델의 주의 지도 제어이다. arXiv 프리프린트 arXiv:2305.13921_, 2023.\n' +
      '*[워크숍 등2022] 워크숍, B, 스카오, T. L, 팬, A, 악키, C, 파블릭, E, Ilic, S, Hesslow, D, 카스타그네, R, 루키니, A. S, Yvon, F., A 176b-파라미터 개방형 액세스 다중언어 모델. arXiv 프리프린트 arXiv:2211.05100_ 2022년입니다.\n' +
      '*[Wu et al.2023a] 우, C, Yin, S, Qi, W, 왕, X, 탕, Z 및 Duan, N. 시각적 챗봇:톡킹, 드로잉, 비주얼 기반 모델 편집. __시각적 챗봇: 시각 기반 모델. arXiv 프리프린트 arXiv:2303.04671_, 2023a.\n' +
      '* [Wu et al.2023b] 우, T. H, 리안, L., 곤졸레스, J E, Li, B 및 다렐, T. 자아 보정 llm 제어 확산 모델 _ 자가 보정 llm 제어 확산 모델. _ 자가 보정 llm 제어 확산 모델. arXiv 프리프린트 arXiv:2311.16090_, 2023b.\n' +
      '*[Xie et al.2023] Xie, J, Li, Y, 황, Y, Liu, H, 장, W, 정, Y 및 Shou, M. Z. Boxdiff: 훈련 없는 상자 제한 확산을 가진 텍스트 대 이미지 합성. 컴퓨터 비전_, pp 7452-7461에 대한 IEEE/CVF 국제 회의의 _검토에서 2023.\n' +
      '*[Xu et al.2023] Xu, J, 류, X, 우, Y, 통, Y, Li, Q, Ding, M., 탕, J 및 동, Y. 이미지 생성에 대한 학습 및 인간의 선호도 평가: 텍스트 대 이미지 생성에 대한 학습:_이미지를 상상해 본다. arXiv 프리프린트 arXiv:2304.05977_, 2023.\n' +
      '*[양 et al.2023a] 양, A, 샤오, B, 왕, 장, B, B, Bian, C, Yin, C, Lv, C, 판, D, 왕, D, D, 오픈 대형 언어 모델. __::: 오픈 대형 언어 모델. 바촨 2: D. arXiv 프리프린트 arXiv:2309.10305_, 2023a.\n' +
      '*[양 등.2023b] 양, L, 류, J, 홍, S, 장, Z, 황, Z, Cai, Z, 장, W 및 빈, C.는 컨텍스트 예측으로 확산 기반 이미지 합성을 향상시킨다. Nural 정보 처리 시스템_, 2023b에 대한 _35-seventh 콘퍼런스에서.\n' +
      '*[양] 양, 장, 장, Z, 송, Y, 홍, S, Xu, R, Zhao, Y, 장, W, 코이, B, 양, M. H. 확산 모델: 방법 및 적용에 대한 종합 조사: 방법 및 적용에 대한 종합 조사: __확산 모델. ACM 컴퓨팅 Surveys_, 56(4):1-39, 2023c.\n' +
      '*[양] 양, Z, 리, L, L, 왕, J, Lin, K, 아자라나브, E, 아메드, F, Liu, Z, Liu, C, Zeng, M, 왕, L. Mm-리액션은 __Mm-리액션: 다중 모드 추론 및 작용에 대한 프롭팅 챗봇: 다중 모드 추론 및 작용에 대한 프롭팅 채팅이다. arXiv 프리프린트 arXiv:2303.11381_, 2023d.\n' +
      '*[양 등 알.2023e] 양, Z, 왕, J, 간, Z, 리, L, Lin, K, 우, C, Duan, N, Liu, Z, Liu, C, Z, Z, Z, Z, 성별, M. Reco: 지역 제어 텍스트 대 이미지 생성. 컴퓨터 비전 및 패턴 인식_, pp 14246-14255, 2023e에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '*[유 et al.2023] 유, L, Shi, B, Pasunuru, R, 뮬러, B, 뮬러, B, 골로바, O, 왕, 부, A, 당, B, 카르레르, B, 셰인, S, 예를 들어 자기회귀 다중모달 모델: 재학습 및 지시 튜닝이다. arXiv 프리프린트 arXiv:2309.02591_, 2023.\n' +
      '*[Zeng et al2022] Zeng, A, Liu, X, Du, Z, Wang, Z, Lai, H, Ding, M, 양, Z, Xu, Y, 정, W, 샤, X., An 오픈 이중언어 사전 학습 모델. _130b: An 오픈 이중언어 사전 학습 모델. arXiv 프리프린트 arXiv:2210.02414_, 2022.\n' +
      '* [장 et al.2022] 장, L., 라오, A 및 아크로칼라, M. 텍스트 대 이미지 확산 모델에 조건부 제어를 추가하세요. 컴퓨터 비전_, pp 3836-3847, 2023a에 대한 IEEE/CVF 국제 회의의 _검토에서.\n' +
      '*[장 et al2022] 장, S, 롤러, S, Goyal, N, Artetxe, M, Chen, M, Chen, S, Dewan, C, Diab, M, Li, X, Lin, X.V, PC: 오픈 사전 훈련된 변압기 언어 모델. arXiv 프리프린트 arXiv:2205.01068_, 2022.\n' +
      '* [장 et al.2023b] 장, 장, Y, 비네트, V, 조시, N 및 왕, X. gpt-4._arXiv 프리프린트 arXiv:2305.18583_, 2023b로 제어 가능한 텍스트 대 이미지 생성을 수행한다.\n' +
      '*[장 et al.2023c] 장, Y, 장, R, 구, 주, Y, Lipka, N, 양, D 및 태양, T. 텍스트가 풍부한 이미지 이해를 위해 튜닝된 시각적 명령어를 강화했습니다. _NeurIPS 2023 워크샵에서 조달 및 명령 Following_, 2023c에 대한 작업입니다.\n' +
      '* [장 et al.2023d] 장, Z항, Z항, A, Li, M, Zhao, H, 카리피스, G 및 스놀라, A. 멀티모달 체인 추론 언어 모델에서. arXiv 프리프린트 arXiv:2302.00923_, 2023d.\n' +
      '* [Zhu et al.2023e] Zhu, D, Chen, J., 선전, X, Li, X 및 Elhoseiny, M.M. 미니톤-4: 첨단 대형 언어 모델을 사용한 비전-언어 이해 향상. __학력-4: 첨단 대형 언어 모델을 사용한 비전-언어 이해 arXiv 프리프린트 arXiv:2304.10592_, 2023.\n' +
      '* [Zou et al.2023] Zou, X, Dou, Z. 양, J, 간, Z, Li, L, Li, C, Dai, X, Behl, H, 왕, J, 원, L 등은 픽셀, 이미지 및 언어에 대한 디코딩을 일반화했다. 컴퓨터 비전 및 패턴 인식_ pp 15116-15127, 2023에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>