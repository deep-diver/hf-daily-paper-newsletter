<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# StepCoder: Improve Code Generation\n' +
      '\n' +
      'with Reinforcement Learning from Compiler Feedback\n' +
      '\n' +
      ' Shihan Dou\\({}^{1}\\)\\({}^{\\dagger}\\), Yan Liu\\({}^{1*}\\), Haoxiang Jia\\({}^{2}\\), Limao Xiong\\({}^{1}\\), Enyu Zhou\\({}^{1}\\),\n' +
      '\n' +
      '**Junjie Shan\\({}^{3}\\), Caishuang Huang\\({}^{1}\\), Wei Shen\\({}^{1}\\), Xiaoran Fan\\({}^{1}\\), Zhiheng Xi\\({}^{1}\\), Yuhao Zhou\\({}^{1}\\), Tao Ji\\({}^{1}\\), Rui Zheng\\({}^{1\\dagger}\\), Qi Zhang\\({}^{1\\dagger}\\), Xuanjing Huang\\({}^{1}\\), Tao Gui\\({}^{1\\dagger}\\)**\n' +
      '\n' +
      '\\({}^{1}\\) Fudan NLP Lab, Fudan University, China\n' +
      '\n' +
      '\\({}^{2}\\) Huazhong University of Science and Technology, China\n' +
      '\n' +
      '\\({}^{3}\\) KTH Royal Institute of Technology, Sweden\n' +
      '\n' +
      'Correspondence to: shdou21@m.fudan.edu.cn, {rzhen g20, qz, tqui}@fudan.edu.cn\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce **StepCoder**, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a **C**urriculum of **C**ode **C**ompletion **S**ubtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide **F**ine-**G**rained **O**ptimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Code generation or program synthesis aims to automatically generate source code that adheres to a specified programming requirement, which is typically described in natural language [36; 7]. Recently, with the development of large language models (LLMs), techniques based on LLM [15; 37; 23] have demonstrated impressive ability in code generation. However, challenges persist in aligning these models with complex human requirements [2; 10; 27], indicating a gap that still exists in fully meeting user expectations.\n' +
      '\n' +
      'In this context, learning from compiler feedback exhibits impressive potential to improve the comprehension of complicated human requirements and the quality of generated codes [14]. This feedback from compilation and execution results is instrumental in directly ascertaining the functional correctness of programs [17; 39]. Researchers [20; 33] introduce reinforcement learning (RL) and leverage compiler feedback from unit tests as a reward metric to guide the exploration of the output space of LLMs. The intention is for the policy model to favor actions that yield higher rewards increasingly. Nevertheless, the optimization of LLMs for code generation via RL presents several hurdles. **First**, the increasing complexity of human requirements often results in the generation of longer code sequences, which makes exploration struggle [9; 13]. **Second**, in cases where a single unit test fails to cover the complex code, unexecuted code snippets may emerge that are not relevant to the reward. Rendering optimization based on the entire code sequence is potentially imprecise. Additionally, our analysis reveals quality limitations in existing datasets like APPS [10] for RL training, which impedes accurate learning from compiler feedback through RL.\n' +
      '\n' +
      'To tackle these challenges, we first introduce StepCoder, an innovative framework developed for enhancing code generation through reinforcement learning. StepCoder integrates two key components: **C**urriculum of **C**ode **C**ompletion **S**ubtasks (CCCS) and **F**ine-**G**rained **O**ptimization (FGO). CCCS is designed to alleviate the complexities associated with exploration in code generation, while FGO is designed to provide more precise and effective optimization strategies. Specifically, CCCS employs a step-by-step strategy to break down complex exploration problems (i.e., code generation) into a curriculum of easier sub-tasks (i.e., code completion). As the training progresses, the difficulty of code completion tasks rises by increasingthe portion of code that needs to be completed. Eventually, the aim is for the model to evolve to a stage where it can effectively generate code solely from human requirements, thus fulfilling the original training goal of code generation. On the other hand, the key insight of FGO is that code snippets that are not executed in a unit test do not contribute to the final reward calculation. Therefore, FGO uses a dynamic masking technique to mask unexecuted snippets from unit test evaluations, ensuring that the model is optimized utilizing only the relevant code segments.\n' +
      '\n' +
      'Subsequently, our endeavor involves the development of APPS+, a dataset of superior quality specifically curated for code generation. APPS+ is meticulously designed to exclude code segments that exhibit syntax errors, are irrelevant to the stipulated problem, or fail to produce any output. Additionally, we have taken measures to standardize the format of inputs and outputs in unit tests to guarantee deterministic output comparisons.\n' +
      '\n' +
      'We evaluate the effectiveness of popular LLMs on APPS+. The results reveal that although LLMs show progressive improvements, they face difficulties with complex human requirements. We further evaluate our method on several extensively used benchmarks including MBPP [2] and HumanEval [3]. The experimental results show that StepCoder effectively eases the exploration difficulty in code generation, outperforming other reinforcement learning-based methods in effectiveness. The main contributions of our paper are as follows:\n' +
      '\n' +
      '* We introduce StepCoder, a novelty training method via RL, including CCCS and FGO. CCCS makes exploration easier by breaking down the complicated goals into sub-objectives curriculum. FGO provides fine-grained optimization by only utilizing the executed code in unit tests.\n' +
      '* We constructed APPS+, a high-quality dataset designed for code generation. APPS+ provides a more rigorous evaluation of LLMs\' capabilities and a foundation to introduce reinforcement learning in the training phase.\n' +
      '* Experiments show that StepCoder can improve the exploration efficiency and effectiveness and outperform other methods.\n' +
      '\n' +
      '## 2 Motivation\n' +
      '\n' +
      'In this section, we clearly illustrate the challenges faced by reinforcement learning in code generation using a simplified example from APPS [10], which was widely used for RL training in code generation.\n' +
      '\n' +
      '**Exploration problems of RL in code generation.** Exploration methods play a crucial role in tackling complicated sequence but sparse reward problems [43; 13]. When a policy model explores a trajectory with high returns, it undergoes optimization, making it inclined to take similar actions in the future [41; 28].\n' +
      '\n' +
      'Consider the code shown in Figure 1, aimed at fulfilling a given human requirement. We first collect the conditional statements (CS) that are indicated by the dashed box by analyzing its abstract syntax tree. Conditional statement introduces new independent paths, increasing the complexity of the program [32]. Suppose \\(P_{\\theta}(\\text{CS}_{i})\\) denotes the probability that the policy model with parameter \\(\\theta\\) completes the \\(i\\)-th conditional statement. The probability that the policy model correctly generates this code according to human requirements can be\n' +
      '\n' +
      'Figure 1: The canonical solution of an instance in the APPS dataset. We collect the conditional statements by analyzing their abstract syntax tree, and some conditional statements are highlighted with a grey dashed box. When inputting \\(s=[1\\text{un}10\\,12\\,1\\,5\\,3\\text{n}]\\), only 75% of the code fragment is executed, highlighted with a green background.\n' +
      '\n' +
      'expressed as follows:\n' +
      '\n' +
      '\\[P\\propto P_{o}\\prod_{i=1}^{3}P_{\\theta}(\\text{CS}_{i}), \\tag{1}\\]\n' +
      '\n' +
      'where \\(P_{o}\\) is the probability of other code snippets except the code labeled in the figure. Typically, we initialize the policy model with the SFT model in sequence generation tasks to facilitate easier exploration [26; 45]. However, the limited performance of the SFT model in code generation still leads to the probability \\(P_{\\theta}(\\text{CS}_{i})\\) at low values [33; 27]. The increasing complexity of human requirements in code generation tasks often leads to a corresponding rise in the number of conditional statements. This escalation can result in a substantial decrease in the probability \\(P_{\\theta}(\\text{CS}_{i})\\), potentially leading \\(P\\) to an exponential reduction. Such a scenario exacerbates the challenges associated with exploration in large language models. An alternative approach to facilitate exploration is through reward shaping, a technique where designers _artificially_ introduce rewards more frequently [13]. However, this method encounters a significant limitation in the context of our application. Specifically, in code generation tasks utilizing unit test feedback, rewards can only be obtained after the execution of the completely generated code. Consequently, the exploration of high-return trajectories in tasks with complex sequences and sparse rewards poses a significant challenge in optimizing the policy model.\n' +
      '\n' +
      '**Optimization problems of RL in code generation.** We first introduce the RL fine-tuning process in code generation. Formally, for a learned policy model \\(\\pi_{\\theta}\\) with parameter \\(\\theta\\), we treat the prediction of each token as an _action_\\(a\\) taken by \\(\\pi_{\\theta}\\) according to the history token sequences. The history token sequences can be viewed as the _state_\\(s\\). Given a human requirement \\(x\\), we denote the solution code \\(y\\) generated by \\(\\pi_{\\theta}\\) as an episode, and \\(r(x,y)\\) is the reward function from the compiler based on compilation and execution. Updating the parameters of \\(\\pi_{\\theta}\\) by using gradient policy algorithm [35] can be represented as follows:\n' +
      '\n' +
      '\\[\\max_{\\theta}\\ E_{(x,y)\\sim D_{\\pi_{\\theta}}}[\\sum_{t}A_{\\pi}^{t}\\log(y_{t}|y_ {1:t-1},x;\\theta)] \\tag{2}\\]\n' +
      '\n' +
      'where \\(A_{\\pi}\\) is the advantage computed by the Generalized Advantage Estimator (GAE) [29] from reward \\(r\\), to reduce the variability of predictions.\n' +
      '\n' +
      'In code generation, rewards are contingent upon the correctness of the unit test sample, which is only relevant to the code snippet being executed. For instance, as shown in Figure 1, when the input to the function is \\([1\\text{u}10\\,12\\,1\\,5\\,3\\text{u}]\\), 75% of the code fragment is executed, which is highlighted with a green dashed box. It indicates that some actions in the code are irrelevant to the reward, which leads to inaccurate advantage. Therefore, optimizing the policy model \\(\\pi_{\\theta}\\) with all actions is ineffective by using Equation 2.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      'In this section, we elaborate on the methodological details of StepCoder, which provide an easier exploration and fine-grained optimization for RL in code generation, respectively, as shown in Figure 2.\n' +
      '\n' +
      '### Priliminaries\n' +
      '\n' +
      'Suppose \\(\\mathcal{D}=\\{(x_{i},y_{i},u_{i},e_{i})\\}_{i=0}^{N}\\) is the training dataset for code generation, which \\(x\\), \\(y\\), \\(u\\) denotes the human requirement (i.e., the task description), the canonical solution and the unit test samples, respectively. \\(e_{i}=\\{st_{j},en_{j}\\}_{j=0}^{E_{i}}\\) is a list of conditional statements by automatically analyzing the abstract syntax tree of the canonical solution \\(y_{i}\\), which \\(st\\) and \\(en\\) represent the start position and the end position of the statements, respectively. \\(e\\) is sorted in ascending order based on the start position \\(st\\). For a human requirement \\(x\\), its canonical solution \\(y\\) can be represented as \\(\\{a_{t}\\}_{t=0}^{T}\\). In code generation, given a human requirement \\(x\\), the final states are the set of codes passing the unit tests \\(u\\).\n' +
      '\n' +
      '### StepCoder\n' +
      '\n' +
      'StepCoder integrates two key components: CCCS and FGO. CCCS is designed to break the code generation tasks into a curriculum of the code completion subtasks. It can alleviate the exploration challenge in RL. FGO is specifically designed for code generation tasks to provide fine-grained optimization by computing only the loss of executed code snippets.\n' +
      '\n' +
      '**CCCS.** In code generation, the solution to a complicated human requirement usually involves a long action sequence taken by the policy model. Meanwhile, the feedback from the compiler is delayed and sparse, i.e., the policy model only receives the reward after generating the entire code. In this scenario, exploring is difficult. The core of our method is to break down such a long sequence of exploration problems into a curriculum of short, easily explorable sub-tasks. We simplify code generation to code completion sub-tasks. These sub-tasksare automatically constructed from the canonical solution in the training dataset.\n' +
      '\n' +
      'Consider a human requirement \\(x\\), early in the training phase of CCCS, the starting point \\(s^{*}\\) of exploration is the states near the final states. Specifically, we provide the human requirement \\(x\\) and the front part of the canonical solution \\(x_{p}=\\{a_{i}\\}_{i=0}^{s^{*}}\\), and the policy model is trained to complete the code based on \\(x^{{}^{\\prime}}=(x,x_{p})\\). Let \\(\\hat{y}\\) be the combined sequence of \\(x_{p}\\) and the output trajectory \\(\\tau\\), i.e. \\(\\hat{y}=(x_{p},\\tau)\\). The reward model provides the reward \\(r\\) according to the correctness of the code snippet \\(\\tau\\) with \\(\\hat{y}\\) as input, where we use the same setting as previous approaches [14; 33] as follows:\n' +
      '\n' +
      '\\[r(x^{{}^{\\prime}},\\hat{y})=\\begin{cases}\\begin{aligned} +& 1,\\text{ if }\\hat{y}\\text{ passed all unit tests}\\\\ -& 0.3,\\text{ if }\\hat{y}\\text{ failed any unit test}\\\\ -& 0.6,\\text{ if }\\hat{y}\\text{ happened runtime error}\\\\ -& 1,\\text{ if }\\hat{y}\\text{ happened compile error}.\\end{aligned}\\end{cases} \\tag{3}\\]\n' +
      '\n' +
      'We use the Proximal Policy Optimization (PPO) algorithm [30] to optimize the policy model \\(\\pi_{\\theta}\\) by utilizing the reward \\(r\\) and the trajectory \\(\\tau\\). In the optimization phase, the canonical solution\'s code segment \\(x_{p}\\) used for providing prompts is masked, such that it does not contribute to the gradient for the policy model \\(\\pi_{\\theta}\\) update. CCCS optimizes the policy model \\(\\pi_{\\theta}\\) by maximizing the objection function as follows:\n' +
      '\n' +
      '\\[\\text{Objective}(\\theta) =E_{(x^{{}^{\\prime}},\\hat{y})\\sim D_{\\pi_{\\theta}}}[r(x^{{}^{ \\prime}},\\hat{y})\\] \\[-\\beta\\log(\\pi_{\\theta}(\\hat{y}|x^{{}^{\\prime}}))/\\pi^{\\text{ref}}( \\hat{y}|x^{{}^{\\prime}})] \\tag{4}\\]\n' +
      '\n' +
      'where \\(\\pi^{\\text{ref}}\\) is the reference model in PPO, which is initialized by the SFT model.\n' +
      '\n' +
      'As the training progresses, the starting point \\(s^{*}\\) of exploration gradually moves towards the beginning of the canonical solution. Specifically, we set a threshold \\(\\rho\\) for each training sample. Each time the cumulative correct proportion of code segments generated by \\(\\pi_{\\theta}\\) is greater than \\(\\rho\\), we move the starting point toward the beginning. In the later stages of training, the exploration of our method is equivalent to the exploration process of original reinforcement learning, i.e., \\(s^{*}=0\\), where the policy model generates code using only human requirements as input.\n' +
      '\n' +
      'Figure 2: The overview of our method. In code generation, the environment with sparse and delayed rewards and the complicated human requirement that involves a long sequence make exploration challenging for the Vanilla RL. In CCCS, we break down a complicated exploration problem into a curriculum of sub-tasks. Utilizing a portion of the canonical solution as the prompt enables the LLM to explore starting from simple sequences. The computation of rewards is only relevant for the executed code snippets, and it is imprecise to optimize the LLM with the entire code (i.e., ). In FGO, we mask unexecuted tokens (i.e., ) in unit tests and only compute the loss function using executed tokens (i.e., ) to provide a fine-grained optimization.\n' +
      '\n' +
      'The starting point \\(s^{*}\\) is sampled at the beginning position of the conditional statements to complete the remaining unwritten code segments. Specifically, a program with a greater number of conditional statements results in increased independent paths, leading to a higher logical complexity [32]. This complexity necessitates more frequent sampling to improve the quality of training, while programs with fewer conditional statements need less frequent sampling. This sampling method allows for a balanced and representative sampling of code structures, catering to both complex and simple semantic constructs in the training dataset. To accelerate the training phase, we set the \\(i\\)-th sample\'s number of curricula equal to \\(\\lceil\\sqrt{E_{i}}\\rceil\\), where \\(E_{i}\\) is its number of conditional statements. The \\(i\\)-th sample\'s stride of the training curriculum is \\(\\lceil\\frac{E_{i}}{\\lceil\\sqrt{E_{i}}\\rceil}\\rceil\\) instead of one.\n' +
      '\n' +
      'The key insight of CCCS can be summarized as follows: 1) It is easy to explore from the states near the goal (i.e., final states). 2) Exploring starting from the states distant from the goal is challenging, but it becomes easier when can leverage states that have already learned how to reach the goal.\n' +
      '\n' +
      '**FGO.** The relationship between reward and action in code generation differs from other reinforcement learning tasks such as Atari [25; 19]. In code generation, we can exclude a set of actions irrelevant to computing the rewards in generated code. Specifically, as mentioned in Section 2, for a unit test, the feedback from the compiler relates only to the code snippets being executed. However, in vanilla RL optimization objectives, as shown in Equation 4, all actions of the trajectory are engaged in the computation of the gradient used in the policy update, which is imprecise.\n' +
      '\n' +
      'To improve the precision of optimization, we mask actions (i.e., tokens) that are not executed in unit tests when computing the loss for updating the policy model. The full algorithm of CCCS and FGO is detailed in Algorithm 1.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'In this section, we first introduce APPS+, a high-quality dataset for code generation by manually verifying based on the APPS dataset. Then, we elaborate on the experiment details and the experimental results.\n' +
      '\n' +
      '### Dataset Preprocessing\n' +
      '\n' +
      'Reinforcement learning requires an amount of high-quality training data. During our investigation, we found that among the currently available open-source datasets, only APPS meets this requirement. However, we found there are incorrect instances, such as missing input, output, or canonical solution, canonical solutions that were uncompileable or unexecutable, and discrepancies in execution output.\n' +
      '\n' +
      'To refine the APPS dataset, we excluded instances lacking input, output, or canonical solutions. Then, we standardized the formats of input and output to facilitate the execution and comparison of unit tests. We conducted unit tests and manual analysis for each instance, eliminating those with incomplete or irrelevant code, syntax errors, API misuse, or missing library dependencies. For discrepancies in output, we manually reviewed the problem description, correcting the expected output or eliminating the instance.\n' +
      '\n' +
      'Finally, we construct the APPS+ dataset, containing 7,456 instances. Each instance includes a programming problem description, a canonical solution, a function name, unit tests (i.e., inputs and outputs), and starter code (i.e., the beginning part of the canonical solution). Appendix A illustrates an example from APPS+. The top section of the figure shows the problem description, and the right section presents the canonical solution, unit tests, and metadata. Further details of APPS+ are discussed in Appendix B.1.\n' +
      '\n' +
      '### Experiment Details\n' +
      '\n' +
      '**Benchmarks.** In our study, we initially evaluated our method and baselines on our pre-processed **APPS+** dataset. Moreover, we also evaluate these methods on several widely-used benchmarks in code generation, i.e., **MBPP** (Mostly Basic Programming Problems) [2] and **HumanEval**[3]. We evaluate the MBPP and HumanEval benchmark in a zero-shot learning setting which is the same as previous approaches [14; 33]. In this setting, we fine-tune the models only on the APPS+ dataset and evaluate the code generation performance on MBPP and HumanEval. The detailed description of benchmarks can be found in the Appendix B.1.\n' +
      '\n' +
      '**Baselines.** To verify the effectiveness of Step-Coder and evaluate the performance of LLMs on our APPS+ dataset, we consider a wide range of baselines, including StarCoder [15], WizardCoder[23], DeepSeek-Coder [8], and three versions of CodeLlama (Base, Python, Instruct) [27]. Moreover, we also consider vanilla PPO and two state-of-the-art RL-based approaches, including PPOCoder [33] and RLTF [20]. We carried out experiments applying these methods utilizing the same backbone (i.e., DeepSeek-Coder-Instruct [8]) on the APPS+ dataset to ensure a fair comparison. In addition to demonstrating the necessity and effectiveness of our method, we also supervised fine-tuning DeepSeek-Coder-Instruct [8] on the APPS+ dataset to exclude the effect of training data. The detailed description of these baselines is discussed in Appendix B.2.\n' +
      '\n' +
      '**Implementation Details.** During the SFT phase, we adopt a learning rate set at \\(2e^{-5}\\), conduct training for three epochs, and employ a warm-up period of \\(0.3\\) epochs, with a linear decay to zero. The fine-tuning process was conducted on a device with eight NVIDIA A100 80G GPUs, with the global batch size set to \\(64\\). In the PPO training phase, we employ a learning rate of \\(5e^{-7}\\) for the policy model and \\(1.5e^{-6}\\) for the critic model. For each example, we collect a \\(16\\) roll-out code using nucleus sampling. The sampling temperature is set to \\(0.8\\), top-p is set to \\(0.9\\), and the maximum output token length is set to \\(1024\\). The token-level KL penalty coefficient \\(\\beta\\) is set to \\(0.05\\), with a clip value of \\(0.8\\). In the decoding phase, the temperature and top_p are set to \\(0.2\\) and \\(0.95\\), respectively.\n' +
      '\n' +
      '**Evaluation & Metric.** We conduct the experiments based on Python3.x. Note that we also use Python3.x during the reward collection in RL-based methods. Following prior studies [27; 23; 14], we use **Pass@k**[3] metric to evaluate all the models. Pass@k quantifies the proportion of instances in which at least one of the k-generated code solutions per human requirement successfully passes all unit tests. The prompts used for code generation are listed in Appendix D.\n' +
      '\n' +
      '### Experimental Results on APPS+\n' +
      '\n' +
      'To assess the performance of widely used LLMs and our StepCoder on code generation, we conduct experiments on the APPS+ dataset that we constructed. The experimental results are illustrated in Table 1. The results indicate that RL-based models outperform other language models, including both base models and SFT models. It is reasonable to infer that reinforcement learning can further enhance the quality of code generation by more effectively navigating the model\'s output space, guided by\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c c c|c} \\hline \\hline \\multirow{2}{*}{**Models**} & \\multirow{2}{*}{**Size**} & \\multicolumn{4}{c}{**APPS+**} \\\\  & & **Introductory** & **Interview** & **Competition** & **Overall** \\\\ \\hline \\multicolumn{5}{c}{Base Models} \\\\ \\hline CodeLlama [27] & 13B & 18.7 & 11.0 & 0.0 & 13.0 \\\\ CodeLlama-Python [27] & 13B & 29.0 & 12.3 & 2.9 & 17.9 \\\\ DeepSeek-Coder-Base [8] & 6.7B & 13.0 & 10.3 & 5.0 & 10.9 \\\\ \\hline \\multicolumn{5}{c}{Supervised Fine-tuned Models} \\\\ \\hline StarCoder [15] & 15.6B & 6.3 & 4.1 & 0.7 & 4.7 \\\\ CodeLlama-Instruct [27] & 13B & 33.3 & 11.0 & 1.4 & 18.7 \\\\ WizardCoder-Python-V1.0 [23] & 13B & 39.7 & 15.1 & 4.3 & 23.6 \\\\ DeepSeek-Coder-Instruct [8] & 6.7B & 49.4 & 18.7 & 3.6 & 29.2 \\\\ SFT on APPS+ & 6.7B & **50.1** & **19.0** & **6.4** & **29.8** \\\\ \\hline \\multicolumn{5}{c}{Reinforcement Learning-based Models (Using DeepSeek-Coder-Instruct-6.7B as the backbone)} \\\\ \\hline Vanilla PPO & 6.7B & 53.7 & 20.1 & 5.0 & 31.7 \\\\ PPOCoder [33] & 6.7B & 54.4 & 20.3 & 6.4 & 32.1 \\\\ RLTF [20] & 6.7B & 55.1 & 20.8 & 6.4 & 32.7 \\\\ \\hline \\multicolumn{5}{c}{**StepCoder (Ours)**} & 6.7B & **59.7** & **23.5** & **8.6** & **36.1** \\\\ \\multicolumn{5}{c}{**w/o CCCS**} & 6.7B & 58.7 & 21.7 & 7.1 & 34.6 \\\\ \\multicolumn{5}{c}{**w/o FGO**} & 6.7B & 58.4 & 23.3 & 8.6 & 35.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Results of pass@1 on our proposed APPS+. We compare popular and widely used state-of-the-art methods with our method. To ensure a fair comparison, we apply these RL-based methods using the same base model (i.e., DeepSeek-Coder-Instruct-6.7B [8]) as a backbone on the APPS+ dataset. In addition, We conduct supervised fine-tuning using our APPS+ dataset based on DeepSeek-Coder-Instruct-6.7B to further validate the effectiveness and necessity of our approach.\n' +
      '\n' +
      'compiler feedback.\n' +
      '\n' +
      'Furthermore, our StepCoder surpasses all baseline models including other RL-based approaches, achieving the highest score. Specifically, our approach obtains 59.7%, 23.5% and 8.6% in the \'Introductory\', \'Interview\' and \'Competition\', respectively. Our approach excels in exploring the output space compared to other RL-based methods, achieved by simplifying complex code generation tasks to code completion sub-tasks. Additionally, the FGO process plays a pivotal role in precisely optimizing the policy model. We also found that the performance of StepCoder is better than LLM which supervised fine-tuning on the APPS+ dataset based on the same backbone. The latter did little to improve the pass rate of the generated code compared with the backbone. This also directly demonstrates that the method of using compiler feedback to optimize the model improves the quality of the generated code better than next-token prediction in code generation.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      'To investigate the impact of individual components in StepCoder, we conducted ablation experiments with two variations of our approach, including StepCoder only with CCCS and only with FGO. The experimental results are presented in Table 1. Experimental results demonstrate that both components of our approach improve the quality of the generated code compared to vanilla PPO. CCCS can enhance its performance in addressing Competition-level problems. This improvement is logical, considering that CCCS effectively simplifies the exploration of more complex human requirements. Simultaneously, FGO boosts the pass rate of unit tests by integrating compiler feedback with the relevant executed code snippet.\n' +
      '\n' +
      '### Results on MBPP and HumanEval\n' +
      '\n' +
      'To further demonstrate the effectiveness of our method, we conducted comparative analyses of StepCoder against various approaches using the well-recognized benchmarks MBPP and HumanEval. These models are trained on APPS+ and then evaluated on MBPP and HumanEval. The experimental results are illustrated in Table 2 which shows that StepCoder is superior over all other models on both benchmarks.\n' +
      '\n' +
      'However, there are concerns regarding potential overlaps in the training data between APPS+ and the two benchmarks, which might contribute to an improvement in performance. To address these concerns, we analyze the difference between APPS+ and the benchmarks by calculating the code line overlap ratio of two corresponding canonical solutions following previous work [2; 14]. The findings are presented in Figure 3. This evidence underscores our approach\'s effectiveness in enhancing the quality of generated code and its capability across a broad spectrum of code generation tasks, primarily by improving the exploration problem in reinforcement learning.\n' +
      '\n' +
      'Meanwhile, our findings revealed a significant degradation in the performance of the SFT model on both MBPP and HumanEval benchmarks. Further analysis of the error cases showed that a minority were related to function name errors, while the majority were associated with program correctness errors. This also indicated that SFT on a single dataset may impair the ability to follow instructions and the ability to generalize, thus affecting the performance of code generation on other tasks.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c} \\hline \\hline \\multicolumn{1}{c}{**Models (6.7B)**} & \\multicolumn{1}{c|}{**HumanEval**} & \\multicolumn{1}{c}{**MBPP**} \\\\ \\hline DeepSeek-Coder-Instruct & **78.0** & **64.2** \\\\ SFT on APPS+ & 55.5 & 54.8 \\\\ \\hline Vanilla PPO & 78.0 & 65.0 \\\\ PPOCoder & 76.8 & 63.8 \\\\ RLTF & 76.8 & 65.2 \\\\\n' +
      '**StepCoder (Ours)** & **78.7** & **67.0** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Results of pass@1 on MBPP and HumanEval. We evaluate the LLMsâ€™ performance on code generation in a zero-shot learning setting. In this setting, the models are fine-tuned on our proposed APPS+ dataset and tested for their ability on MBPP and HumanEval.\n' +
      '\n' +
      'Figure 3: Analysis of duplicated lines between APPS+ and the two benchmarks. The overlap of data between APPS+ and them is very small. Only 0.2% and 7.1% had more than half of their lines matched somewhere in MBPP and HumanEval, respectively.\n' +
      '\n' +
      'In contrast, RL-based methods can improve the performance for unseen tasks of code generation.\n' +
      '\n' +
      '### Analysis by Unit Test Results\n' +
      '\n' +
      'We further analyzed the results of cases that did not pass all unit tests, as shown in Figure 4. The results show that our proposed method can effectively reduce the likelihood of compilation errors, which is particularly evident in Interview-level and Competition-level programming problems. However, it was also observed that all LLMs are more prone to runtime errors and failures as compared to compilation errors, albeit StepCoder shows a comparatively lower rate of runtime errors and failures. These results demonstrate that StepCoder is less prone to compilation errors, but still suffers from runtime errors and failure. Consequently, these findings suggest that future research should further concentrate on significantly reducing runtime errors, which could greatly enhance both the quality and the pass rate of the code generated by such models.\n' +
      '\n' +
      '## 5 Related Work\n' +
      '\n' +
      '### Large Language Models for Code Generation\n' +
      '\n' +
      'Recently, LLMs have shown remarkable ability in understanding natural language and code generation by training on large text corpora containing code data. Several pre-trained language models (PLMs) demonstrate significant potential for code generation including CodeGPT [21], PanGu-Coder [4], SantaCoder [1], CodeGeex [44] and Phi-1.5 [16]. In addition, SFT models achieve more competitive performance such as CodeX [3], StarCoder [15], WizardCoder [23], Code Llama Instruct [27], and DeepSeek-Coder [8].\n' +
      '\n' +
      'Reinforcement Learning is a method of learning the optimal policy by exploring the environment and obtaining rewards [41; 34]. Recently, some researchers have introduced RL to LLMs and improved the quality of the generated code by utilizing the unit test feedback to explore the output space of the policy model [33; 20; 14]. For instance, CodeRL [14] leverages signal from unit tests as rewards and utilizes the actor-critic approach [12; 35] to enhance models on code generation. PPOCoder [33] refines CodeRL by employing the PPO algorithm [30] and RLTF [20] provides fine-grained rewards through the error locations, but the reward space is still sparse. However, the exploration of complex tasks in an environment characterized by a sparse reward is challenging. These methods still fall short of effectively using RL to enhance the model\'s performance in code generation.\n' +
      '\n' +
      '### Exploration in Reinforcement Learning\n' +
      '\n' +
      'Exploration is crucial in addressing long sequences and sparse reward problems [9; 13]. In the sequence generation task, researchers improved exploration by initializing the policy model using the SFT model [26; 31]. Our proposed approach incorporates similar methods, but additional methods are necessary to ensure effective exploration. This is particularly evident when facing complex human requirements, where the limited quality of code generated by SFT models makes exploration still challenging [33].\n' +
      '\n' +
      'Other notable methods introduce the Process-Supervised Reward Model to provide step-by-step rewards for complex sequence generation tasks such as mathematical reasoning and code generation [38; 18; 22; 24]. However, these methods require labelling a large preference dataset to train the reward model. Similar to our approach, some methods construct a learning curriculum by initiating each episode from a sequence of progressively more challenging starting states [28; 11; 6]. In contrast to our approach, these methods are designed to address the problem of exploration in other fields, such as gaming and robotic manipulation. Meanwhile, our approach combines software engineering features to dynamically determine the starting states through conditional statements. We also introduce FGO to provide a fine-grained optimization for the policy model by leveraging the coverage information.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'In this paper, we introduce StepCoder, a novelty training framework via RL. StepCoder breaks down\n' +
      '\n' +
      'Figure 4: Analysis by unit test results on APPS+. The results are categorized into CompileError (Reward = -1) and Runtimeerror & Failure (Reward = -0.6 or -0.3).\n' +
      '\n' +
      'complicated exploration problems to reduce the difficulty of exploring environments with sparse rewards while providing fine-grained optimization. In addition, we also construct a high-quality dataset APPS+, specifically for code generation. Experiments indicate that our method can effectively improve the quality of generated code via reinforcement learning compared to other approaches.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]L. Ben Allal, R. Li, D. K. K., C. Mou, C. Akiki, C. Munoz Ferrandis, N. Muenmighoff, M. Mishra, A. Gu, M. Dey, et al. (2023) Santacoder: don\'t reach for the stars!. arXiv preprint arXiv:2301.03988. Cited by: SS1.\n' +
      '* [2]J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dahan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. (2021) Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Cited by: SS1.\n' +
      '* [3]M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. (2021) Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Cited by: SS1.\n' +
      '* [4]F. Christopoulou, G. Lampouras, M. Gritta, G. Zhang, Y. Guo, Z. Li, Q. Zhang, M. Xiao, B. Shen, L. Li, et al. (2022) Pangu-coder: program synthesis with function-level language modeling. arXiv preprint arXiv:2207.11280. Cited by: SS1.\n' +
      '* [5]O. Contributors (2023) OpenCompass: a universal evaluation platform for foundation models. Note: [https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass) Cited by: SS1.\n' +
      '* [6]C. Florensa, D. Held, M. Wulfmeier, M. Zhang, and P. Abbeel (2017) Reverse curriculum generation for reinforcement learning. In Conference on robot learning, pp. 482-495. Cited by: SS1.\n' +
      '* [7]S. Gulwani, O. Polozov, R. Singh, et al. (2017) Program synthesis. Foundations and Trends(r) in Programming Languages4 (1-2), pp. 1-119. Cited by: SS1.\n' +
      '* the rise of code intelligence. Cited by: SS1.\n' +
      '* [9]J. Hao, T. Yang, H. Tang, C. Bai, J. Liu, Z. Meng, P. Liu, and Z. Wang (2023) Exploration in deep reinforcement learning: from single-agent to multiagent domain. IEEE Transactions on Neural Networks and Learning Systems. Cited by: SS1.\n' +
      '* [10]D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt (2021) Measuring coding challenge competence with APPS. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Cited by: SS1.\n' +
      '* [11]I. Hosu and T. Rebedea (2016) Playing atari games with deep reinforcement learning and human checkpoint replay. arXiv preprint arXiv:1607.05077. Cited by: SS1.\n' +
      '* [12]V. Konda and J. Tsitsiklis (1999) Actor-critic algorithms. Advances in neural information processing systems12. Cited by: SS1.\n' +
      '* [13]P. Ladosz, L. Weng, M. Kim, and H. Oh (2022) Exploration in deep reinforcement learning: a survey. Information Fusion85, pp. 1-22. Cited by: SS1.\n' +
      '* [14]H. Le, Y. Wang, A. Deepak Gotmare, S. Savarese, and S. Chu Hong Hoi (2022) Coderl: mastering code generation through pre-trained models and deep reinforcement learning. Advances in Neural Information Processing Systems35, pp. 21314-21328. Cited by: SS1.\n' +
      '* [15]R. Li, L. B. Allal, Y. Zi, N. Muenmighoff, D. Koccetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, et al. (2023) StarCoder: may the source be with you!. arXiv preprint arXiv:2305.06161. Cited by: SS1.\n' +
      '* [16]Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee (2023) Textbooks are all you need ii: phi-1.5 technical report arXiv preprint arXiv:2309.05463. Cited by: SS1.\n' +
      '* [17]Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. (2022) Competition-level code generation with alphacode. Science378 (6624), pp. 1092-1097. Cited by: SS1.\n' +
      '* [18]H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe (2023) Let\'s verify step by step. arXiv preprint arXiv:2305.20050. Cited by: SS1.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* [21] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. _arXiv preprint arXiv:2102.04664_.\n' +
      '* [22] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. _arXiv preprint arXiv:2308.09583_.\n' +
      '* [23] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evol-instruct. _arXiv preprint arXiv:2306.08568_.\n' +
      '* [24] Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang. 2023. Let\'s reward step by step: Step-level reward model as the navigators for reasoning. _arXiv preprint arXiv:2310.10080_.\n' +
      '* [25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533.\n' +
      '* [26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744.\n' +
      '* [27] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. 2023. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_.\n' +
      '* [28] Tim Salimans and Richard Chen. 2018. Learning montezuma\'s revenge from a single demonstration. _arXiv preprint arXiv:1812.03381_.\n' +
      '* [29] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. High-dimensional continuous control using generalized advantage estimation. _arXiv preprint arXiv:1506.02438_.\n' +
      '* [30] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_.\n' +
      '* [31] Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, and Xuan-Jing Huang. 2023. Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 2859-2873.\n' +
      '* [32] Martin Shepperd. 1988. A critique of cyclomatic complexity as a software metric. _Software Engineering Journal_, 3(2):30-36.\n' +
      '* [33] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based code generation using deep reinforcement learning. _arXiv preprint arXiv:2301.13816_.\n' +
      '* [34] Richard S Sutton, Andrew G Barto, et al. 1998. _Introduction to reinforcement learning_, volume 135. MIT press Cambridge.\n' +
      '* [35] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999. Policy gradient methods for reinforcement learning with function approximation. _Advances in neural information processing systems_, 12.\n' +
      '* [36] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: Code generation using transformer. In _Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering_, pages 1433-1443.\n' +
      '* [37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.\n' +
      '* [38] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process-and outcome-based feedback. _arXiv preprint arXiv:2211.14275_.\n' +
      '* [39] Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, and Qun Liu. 2022. Compilable neural code generation with compiler feedback. _arXiv preprint arXiv:2203.05132_.\n' +
      '* [40] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannen Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_.\n' +
      '* [41] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine learning_, 8:229-256.\n' +
      '* [42] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. _arXiv preprint arXiv:2304.12244_.\n' +
      '\n' +
      '* [43] Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Jianye Hao, Zhaopeng Meng, Peng Liu, and Zhen Wang. 2021. Exploration in deep reinforcement learning: a comprehensive survey. _arXiv preprint arXiv:2109.06668_.\n' +
      '* [44] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. 2023. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. _arXiv preprint arXiv:2303.17568_.\n' +
      '* [45] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Yuhao Zhou, Limao Xiong, et al. 2023. Delve into ppo: Implementation matters for stable rhf. In _NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following_.\n' +
      '\n' +
      '## Appendix A Instance of the APPS+ Dataset\n' +
      '\n' +
      'We present an example from our APPS+ dataset, as shown in Figure 5.\n' +
      '\n' +
      '## Appendix B Experiments Setup in Detail\n' +
      '\n' +
      'In this section, we elaborate in detail on the baselines we compare and the implementation details of our method.\n' +
      '\n' +
      '### Benchmarks\n' +
      '\n' +
      '**APPS+.** We construct the new benchmark APPS+ by refining the popular benchmark APPS [10]. APPS+ was categorized into three difficulty levels: Introductory (2,850), Interview (4,020), and Competition (586). The mean length of each problem is 255.3 words, and that of the code is 21.9 lines. On average, each instance is accompanied by three unit tests and includes a \'conditional statement\' attribute representing the start and end position of the statement in the canonical solution. We randomly selected about 25% instances (700 Introductory, 1,000 Interview, and 140 Competition) for the validation dataset and another 25% instances for the test dataset.\n' +
      '\n' +
      '**MBPP.** MBPP [2] is a smaller but common Python code generation benchmark. It contains 974 instances created by crowd-sourcing to an internal pool of crowd workers with basic Python knowledge. The difficulty level of the problems in this dataset is introductory. Most problems are often conveyed in a single sentence of natural language, and each problem consists of a task description, code solution, and three automated test cases. We evaluate LLMs in a zero-shot learning setting which is the same as previous studies [14; 33]. In this setting, we fine-tune models only based on the APPS+ dataset and evaluate them on MBPP.\n' +
      '\n' +
      '**HumanEval.** HumanEval [3] is another extensively used benchmark for evaluating the ability of code generation. It comprises 164 hand-written Python problems that test language comprehension, algorithmic thinking, and basic mathematics. The complexity of these problems is akin to that of simple software interview questions. We also evaluate models on the HumanEval benchmark in a zero-shot learning setting.\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '**StarCoder.** StarCoder [15] is a 15.5B parameter model trained on 80+ programming languages sourced from GitHub, encompassing one trillion tokens. It undergoes fine-tuning specifically for 35 billion Python tokens, enabling its proficiency across a diverse set of coding tasks. With an extended context length of 8K, StarCoder excels particularly in infilling capabilities.\n' +
      '\n' +
      '**CodeLlama.** CodeLlama [27] is a collection of pre-trained and fine-tuned generative text models ranging in scale from 7B to 34B parameters. CodeLlama comes in three variants: **CodeLlama**: base models designed for general code synthesis and understanding; **CodeLlama-Python**: designed specifically to handle the Python programming language; **CodeLlama-Instruct**: for instruction following and safer deployment.\n' +
      '\n' +
      '**WizardCoder.** WizardCoder [23] is fine-tuned by using a complicated dataset which is constructed by adapting the Evol-Instruct [42] on code-related tasks, which is a further improvement of self-instruct method [40]. It has proven to be highly effective in code generation by fine-tuning more complex instruction data.\n' +
      '\n' +
      '**DeepSeek-Coder.** DeepSeek-Coder [8] demonstrates state-of-the-art performance among open-source code models across various programming languages. It encompasses a collection of code language models from 1B to 33B trained from scratch. The training corpus for these models comprises an impressive 2 trillion tokens which is the combination of code and natural languages. Each model is trained to utilize a window size of 16K, and a fill-in-the-blank task is incorporated into the training process, which enhances the models\' capacity to facilitate code completion and infilling tasks.\n' +
      '\n' +
      '**PPOCoder.** PPOCoder [33] initially employs the Proximal Policy Optimization algorithm [30] for code generations. In addition, it integrates dis \n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '```\n' +
      '0: the train dataset \\(\\mathcal{D}=\\{(x_{i},y_{i},u_{i},e_{i}),1\\leq i\\leq n\\}\\), the threshold value \\(\\rho_{t}\\) for curriculum training.\n' +
      '0: the policy model \\(\\pi_{\\theta}\\)\n' +
      '1: Initialize the stride of curriculum \\(s=\\lceil\\frac{E_{i}}{\\lceil\\sqrt{E_{i}}\\rceil}\\rceil\\) for each sample\n' +
      '2: Initialize the current curriculum \\(c=\\lceil\\sqrt{E_{i}}\\rceil-1\\) for each training sample\n' +
      '3: Initialize the pass rate \\(\\rho=0\\) for each training sample\n' +
      '4:while TRUE do\n' +
      '5: Initialize mini-batch \\(\\mathcal{D}_{s}=\\{\\}\\)\n' +
      '6: Get latest policy model \\(\\pi_{\\theta}\\)\n' +
      '7: Sample a mini-batch of size \\(M\\) from \\(\\mathcal{D}\\)\n' +
      '8:for\\(i\\) in \\(0\\), \\(\\cdots\\), \\(M-1\\)do\\(\\triangleright\\) Begin to sample the trajectories\n' +
      '9: Calculate the start position pos \\(=s_{i}*c_{i}\\)\\(\\triangleright\\) CCCS\n' +
      '10: Reorganize the given context \\(x^{{}^{\\prime}}_{i}=x_{i}+y_{i}\\left[:\\text{pos}\\right]\\)\n' +
      '11: Sample trajectory \\(\\hat{y_{i}}\\leftarrow\\pi_{\\theta}(.|x^{{}^{\\prime}}_{i})\\)\n' +
      '12: Compute reward \\(r_{i}\\) using Equation 3\n' +
      '13: Calculate unexecuted snippets\' mask matrix \\(m_{ij}=\\left[1\\text{ if }\\hat{y}^{j}_{i}\\text{ is executed else }0\\right]\\)\\(\\triangleright\\) FGO\n' +
      '14: Add \\(\\{x^{{}^{\\prime}}_{i},\\hat{y_{i}},u_{i},r_{i},s_{i},c_{i},m_{i}\\}\\) to mini-batch \\(\\mathcal{D}_{s}\\)\n' +
      '15:endfor\n' +
      '16:\\(\\theta\\leftarrow\\mathcal{A}(\\theta,\\mathcal{D}_{s})\\)\\(\\triangleright\\) Update the policy model by PPO algorithm\n' +
      '17:for\\(i\\) in \\(0\\), \\(\\cdots\\), \\(M-1\\)do\n' +
      '18:if\\(r_{i}=1\\)then\\(\\triangleright\\) Update pass rate using moving average\n' +
      '19:\\(\\rho_{i}=\\alpha+(1-\\alpha)*\\rho_{i}\\)\n' +
      '20:else\n' +
      '21:\\(\\rho_{i}=(1-\\alpha)*\\rho_{i}\\)\n' +
      '22:endif\n' +
      '23:if\\(\\rho_{i}>\\rho_{t}\\)then\\(\\triangleright\\) Meet the update conditions, proceed to the next stage\n' +
      '24:\\(\\rho_{i}=0\\)\n' +
      '25:\\(c_{i}=min(c_{i}-1,0)\\)\n' +
      '26:endif\n' +
      '27:endfor\n' +
      '28:endwhile\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1** StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>