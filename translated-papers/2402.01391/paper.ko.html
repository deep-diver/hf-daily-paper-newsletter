<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 스텝코더: 코드 생성 개선\n' +
      '\n' +
      '컴파일러 피드백에서 강화 학습으로\n' +
      '\n' +
      ' 시한두\\({}^{1}\\)\\({}^{dagger}\\), 옌류\\({}^{1*}\\), 하옥상지아\\({}^{2}\\), 리마오십구\\({}^{1}\\), 엔유주\\({}^{1}\\)\n' +
      '\n' +
      '준제산\\({}^{3}\\), Caishuang Huang\\({}^{1}\\), Wei Shen\\({}^{1}\\), Xiaoran Fan\\({}^{1}\\), Zhiheng Xi\\({}^{1}\\), Yuhao Zhou\\({}^{1}\\), Tao Ji\\({}^{1}\\), Rui Zheng\\({}^{1\\dagger}\\), Qi Zhang\\({}^{1\\dagger}\\), Xuanjing Huang\\({}^{1\\dagger}\\), Tao Gui\\({}^{1\\dagger}\\), Tao Gui\\({}^{1\\dagger}\\), Ziaoran Fan\\({}^{1}\\), Zhiheng Xi\\({}^{1}\\), Yuhao Zhou\\({}^{1}\\), Tao Zhou\\({}^{1}\\), Qi Zhang\\({}^{\n' +
      '\n' +
      '중국 푸단대학교 푸단 NLP 연구실\n' +
      '\n' +
      '중국 화중과학기술대학교\n' +
      '\n' +
      '({}^{3}\\) KTH Royal Technology Institute, Sweden\n' +
      '\n' +
      '의 대응: shdou21@m.fudan.edu.cn, {rzhen g20, qz, tqui}@fudan.edu.cn\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)의 발전은 코드 생성 분야를 크게 추진했다. 이전 연구에서는 코드 생성 품질을 향상시키기 위해 LLM의 출력 공간을 탐색하기 위해 컴파일러 피드백과 통합 강화 학습(RL)을 수행했다. 그러나 복잡한 인간 요구 사항에 대한 응답으로 LLM에 의해 생성된 긴 코드는 RL 탐사를 도전으로 만든다. 또한, 단위 테스트들이 복잡한 코드를 커버하지 않을 수 있기 때문에, 이러한 미실행 코드 스니펫들을 사용하여 LLMs들을 최적화하는 것은 비효율적이다. 이러한 문제를 해결하기 위해, 우리는 두 가지 주요 구성 요소로 구성된 새로운 코드 생성을 위한 RL 프레임워크인 **StepCoder**를 소개한다: CCCS는 긴 시퀀스 코드 생성 작업을 **C**ode **C** 완성 **S**ubtask의 **C** 커리큘럼으로 분해하여 탐색 문제를 해결하는 반면 FGO는 실행되지 않은 코드 세그먼트를 마스킹하여 모델을 최적화하여 **F**ine-**G**rained **O**ptimization을 제공한다. 또한, 단위 테스트의 정확성을 보장하기 위해 수동으로 검증된 RL 트레이닝을 위한 APPS+ 데이터셋을 추가로 구축한다. 실험 결과는 제안된 방법이 출력 공간을 탐색할 수 있는 능력을 향상시키며, 해당 벤치마크에서 최신 접근 방식보다 우수함을 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '코드 생성 또는 프로그램 합성은 일반적으로 자연 언어로 기술되는 지정된 프로그래밍 요구 사항을 준수하는 소스 코드를 자동으로 생성하는 것을 목표로 한다[36; 7]. 최근 대규모 언어 모델(LLM)의 발달로 LLM[15; 37; 23]을 기반으로 한 기법이 코드 생성에서 인상적인 능력을 발휘하고 있다. 그러나 이러한 모델을 복잡한 인간 요구 사항[2; 10; 27]과 정렬하는 데 어려움이 지속되며, 이는 사용자 기대를 완전히 충족시키는 데 여전히 존재하는 격차를 나타낸다.\n' +
      '\n' +
      '이러한 맥락에서 컴파일러 피드백으로부터의 학습은 복잡한 인간 요구 사항에 대한 이해와 생성된 코드의 품질을 향상시킬 수 있는 인상적인 잠재력을 나타낸다[14]. 이러한 컴파일 및 실행 결과의 피드백은 프로그램의 기능적 정확성을 직접 확인하는 데 도움이 된다[17; 39]. 연구자[20; 33]는 LLM의 출력 공간 탐색을 안내하는 보상 메트릭으로 단위 테스트의 강화 학습(RL) 및 레버리지 컴파일러 피드백을 소개한다. 정책 모델이 점점 더 높은 보상을 산출하는 행동을 선호하기 위한 의도입니다. 그럼에도 불구하고 RL을 통한 코드 생성을 위한 LLM의 최적화는 몇 가지 장애물을 제시한다. **첫째**, 인간 요구 사항의 증가하는 복잡성은 종종 더 긴 코드 시퀀스의 생성을 초래하며, 이는 탐색 투쟁을 만든다[9; 13]. **둘째**, 단일 단위 테스트가 복잡한 코드를 커버하지 못하는 경우 보상과 관련이 없는 실행되지 않은 코드 스니펫이 나타날 수 있다. 전체 코드 시퀀스를 기반으로 하는 렌더링 최적화는 잠재적으로 부정확하다. 또한, RL 학습을 위한 APPS [10]과 같은 기존 데이터 세트의 품질 한계를 발견하여 RL을 통한 컴파일러 피드백으로부터 정확한 학습을 방해한다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해 먼저 강화 학습을 통해 코드 생성을 향상시키기 위해 개발된 혁신적인 프레임워크인 StepCoder를 소개한다. StepCoder는 두 가지 핵심 구성 요소를 통합한다: **C**ode **C**ompletion **S**ubtask (CCCS) 및 **F**ine-**G**rained **O**ptimization (FGO). CCCS는 코드 생성에서 탐색과 관련된 복잡성을 완화하도록 설계된 반면 FGO는 보다 정확하고 효과적인 최적화 전략을 제공하도록 설계되었다. 구체적으로, CCCS는 복잡한 탐색 문제(즉, 코드 생성)를 보다 쉬운 하위 과제(즉, 코드 완성)의 커리큘럼으로 분해하는 단계적 전략을 사용한다. 훈련이 진행됨에 따라, 코드 완성 작업의 난이도는 완료될 필요가 있는 코드의 부분을 증가시킴으로써 상승한다. 결국, 목표는 모델이 인간의 요구 사항만으로 코드를 효과적으로 생성할 수 있는 단계로 진화하여 코드 생성의 원래 훈련 목표를 달성하는 것이다. 반면에 FGO의 핵심 통찰력은 단위 테스트에서 실행되지 않는 코드 스니펫이 최종 보상 계산에 기여하지 않는다는 것이다. 따라서 FGO는 동적 마스킹 기법을 사용하여 단위 테스트 평가에서 실행되지 않은 스니펫을 마스킹하여 모델이 관련 코드 세그먼트만 사용하여 최적화되도록 한다.\n' +
      '\n' +
      '그 후, 우리의 노력은 코드 생성을 위해 특별히 선별된 우수한 품질의 데이터 세트인 APPS+의 개발을 포함한다. APPS+는 구문 오류를 나타내거나 규정된 문제와 무관하거나 출력을 생성하지 못하는 코드 세그먼트를 배제하도록 세심하게 설계된다. 또한 결정론적 출력 비교를 보장하기 위해 단위 테스트에서 입력과 출력의 형식을 표준화하는 조치를 취했다.\n' +
      '\n' +
      'APPS+에 대한 인기 있는 LLM의 효과를 평가한다. 결과는 LLM이 점진적인 개선을 보여주지만 복잡한 인적 요구 사항으로 어려움을 겪는다는 것을 보여준다. 우리는 MBPP [2] 및 HumanEval [3]을 포함하여 광범위하게 사용되는 여러 벤치마크에서 방법을 추가로 평가한다. 실험 결과 StepCoder는 코드 생성의 탐색 난이도를 효과적으로 완화하여 다른 강화 학습 기반 방법보다 효과적이었다. 본 논문의 주요 기여도는 다음과 같다.\n' +
      '\n' +
      '* CCCS와 FGO를 포함하여 RL을 통한 참신성 훈련 방법인 StepCoder를 소개한다. CCCS는 복잡한 목표를 하위목표 교육과정으로 분해하여 탐구를 보다 쉽게 한다. FGO는 단위 테스트에서 실행된 코드만을 활용하여 세밀한 최적화를 제공한다.\n' +
      '* 코드 생성을 위해 설계된 고품질 데이터셋인 APPS+를 구축했습니다. APPS+는 LLM의 능력에 대한 보다 엄격한 평가와 훈련 단계에서 강화 학습을 도입할 수 있는 기반을 제공한다.\n' +
      '* 실험은 StepCoder가 탐색 효율과 효율성을 향상시키고 다른 방법을 능가할 수 있음을 보여준다.\n' +
      '\n' +
      '## 2 Motivation\n' +
      '\n' +
      '이 절에서는 코드 생성에서 RL 훈련에 널리 사용되었던 APPS [10]의 간략화된 예를 사용하여 코드 생성에서 강화 학습이 직면한 과제를 명확하게 설명한다.\n' +
      '\n' +
      '**코드 생성에서 RL의 탐색 문제.** 탐색 방법은 복잡한 순서를 다루지만 희박한 보상 문제를 해결하는 데 중요한 역할을 한다[43; 13]. 정책 모델은 수익률이 높은 궤적을 탐색할 때 최적화를 거치게 되어 향후 유사한 행동을 취하는 경향이 있다[41; 28].\n' +
      '\n' +
      '주어진 인간 요구 사항을 충족하기 위해 그림 1에 표시된 코드를 고려하십시오. 먼저 추상 구문 트리를 분석하여 파선으로 표시된 조건문(CS)을 수집한다. 조건문은 새로운 독립적인 경로를 도입하여 프로그램의 복잡성을 증가시킨다[32]. 만약 \\(P_{\\theta}(\\text{CS}_{i})\\)가 매개변수 \\(\\theta\\)을 갖는 정책 모델이 \\(i\\)번째 조건문을 완성할 확률을 나타낸다고 가정하자. 정책 모델이 인간의 요구 사항에 따라 이 코드를 올바르게 생성할 확률은\n' +
      '\n' +
      '그림 1: APPS 데이터 세트에서 인스턴스의 표준 솔루션. 우리는 추상 구문 트리를 분석하여 조건문을 수집하고 일부 조건문은 회색 대시 상자로 강조 표시된다. (s=[1\\text{un}10\\,12\\,1\\,5\\,3\\text{n}]\\)을 입력할 때, 초록색 배경으로 강조된 코드 조각은 75%만 실행된다.\n' +
      '\n' +
      '다음과 같이 표현된다:\n' +
      '\n' +
      '\\[P\\propto P_{o}\\prod_{i=1}^{3}P_{\\theta}(\\text{CS}_{i}), \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(P_{o}\\)는 그림에 표시된 코드를 제외한 다른 코드 스니펫의 확률이다. 일반적으로, 우리는 더 쉬운 탐색을 용이하게 하기 위해 시퀀스 생성 태스크에서 SFT 모델로 정책 모델을 초기화한다[26; 45]. 그러나 코드 생성에서 SFT 모델의 제한된 성능은 여전히 낮은 값[33; 27]에서 확률\\(P_{\\theta}(\\text{CS}_{i})\\)로 이어진다. 코드 생성 작업에서 인간 요구 사항의 복잡성이 증가하면 종종 조건문의 수가 증가한다. 이러한 증가는 확률\\(P_{\\theta}(\\text{CS}_{i})\\의 상당한 감소를 초래하여 잠재적으로 지수 감소로 이어질 수 있다. 이러한 시나리오는 대규모 언어 모델의 탐색과 관련된 문제를 악화시킨다. 탐구를 용이하게 하기 위한 대안적인 접근은 보상 쉐이핑을 통한 것인데, 이는 설계자들이 인위적으로 보상을 더 자주 도입하는 기법이다[13]. 그러나 이 방법은 응용 프로그램의 맥락에서 상당한 제한에 직면한다. 구체적으로, 단위 테스트 피드백을 활용하는 코드 생성 태스크에서, 리워드는 완전히 생성된 코드의 실행 후에만 획득될 수 있다. 결과적으로, 복잡한 시퀀스 및 희소 보상을 갖는 태스크에서 고수익 궤적의 탐색은 정책 모델을 최적화하는 데 중요한 도전을 제기한다.\n' +
      '\n' +
      '**코드 생성에서 RL의 최적화 문제** 코드 생성에서 RL 미세 조정 과정을 먼저 소개한다. 형식적으로, 매개변수\\(\\theta\\)를 갖는 학습된 정책 모델\\(\\pi_{\\theta}\\)에 대해, 각 토큰의 예측은 이력 토큰 시퀀스에 따라 \\(\\pi_{\\theta}\\)이 취하는 _action_\\(a\\)으로 취급한다. 이력 토큰 시퀀스들은 _state_\\(s\\)로 볼 수 있다. 인간의 요구사항이 주어졌을 때, 우리는 에피소드로 \\(\\pi_{\\theta}\\)에 의해 생성된 솔루션 코드 \\(y\\)을 나타내며, \\(r(x,y)\\)은 컴파일 및 실행을 기반으로 한 컴파일러로부터 보상 함수이다. 기울기 정책 알고리즘을 사용하여 \\(\\pi_{\\theta}\\)의 파라미터들을 업데이트하는 것 [35]은 다음과 같이 나타낼 수 있다:\n' +
      '\n' +
      '\\[\\max_{\\theta}\\E_{(x,y)\\sim D_{\\pi_{\\theta}}[\\sum_{t}A_{\\pi}^{t}\\log(y_{t}|y_{1:t-1},x;\\theta)] \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(A_{\\pi}\\)는 예측의 변동성을 줄이기 위해 보상 \\(r\\)으로부터 일반화된 어드밴티지 추정기(GAE) [29]에 의해 계산된 이점이다.\n' +
      '\n' +
      '코드 생성에서 보상은 실행 중인 코드 조각과 관련된 단위 테스트 샘플의 정확성에 따라 결정된다. 예를 들어, 도 1에 도시된 바와 같이, 함수에 대한 입력이 \\([1\\text{u}10\\,12\\,1\\,5\\,3\\text{u}]\\)일 때, 코드 단편의 75%가 실행되고, 이는 녹색 점선 박스로 강조된다. 코드의 일부 행동이 보상과 무관하여 부정확한 이점을 초래한다는 것을 나타낸다. 따라서 모든 행동으로 정책 모형\\(\\pi_{\\theta}\\)을 최적화하는 것은 식 2를 사용하여 비효율적이다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '이 절에서는 그림 2와 같이 코드 생성에서 RL에 대해 각각 더 쉬운 탐색 및 세밀한 최적화를 제공하는 StepCoder의 방법론적 세부 사항을 자세히 설명한다.\n' +
      '\n' +
      '### Priliminaries\n' +
      '\n' +
      '만약 \\(\\mathcal{D}=\\{(x_{i},y_{i},u_{i},e_{i})\\}_{i=0}^{N}\\)이 코드 생성을 위한 훈련 데이터세트라고 가정하면, \\(x\\), \\(y\\), \\(u\\)은 각각 인간의 요구사항(즉, 작업 설명), 표준 솔루션 및 단위 테스트 샘플을 나타낸다. \\ (e_{i}=\\{st_{j},en_{j}\\}_{j=0}^{E_{i}}\\)은 표준해 \\(y_{i}\\)의 추상 구문 트리를 자동으로 분석하여 조건문의 목록으로, \\(st\\)과 \\(en\\)은 각각 문장의 시작 위치와 끝 위치를 나타낸다. \\\\ (e\\)는 시작 위치 \\(st\\)을 기준으로 오름차순으로 정렬된다. 사람의 요구조건인 \\(x\\)에 대하여, 표준용액 \\(y\\)은 \\(\\{a_{t}\\}_{t=0}^{T}\\)으로 나타낼 수 있다. 코드 생성에서 인간의 요구조건 \\(x\\)이 주어졌을 때, 최종 상태는 단위 테스트를 통과한 코드 집합이다.\n' +
      '\n' +
      '### StepCoder\n' +
      '\n' +
      'StepCoder는 CCCS와 FGO의 두 가지 핵심 구성 요소를 통합한다. CCCS는 코드 생성 작업을 코드 완성 하위 작업의 커리큘럼으로 분할하도록 설계되었다. RL의 탐험 도전을 완화할 수 있습니다. FGO는 실행되는 코드 스니펫의 손실만을 계산하여 세밀한 최적화를 제공하기 위해 코드 생성 작업을 위해 특별히 설계되었다.\n' +
      '\n' +
      '**CCCS.** 코드 생성에서 복잡한 인간 요건에 대한 해결책은 일반적으로 정책 모델에 의해 취해진 긴 액션 시퀀스를 포함한다. 한편, 컴파일러로부터의 피드백은 지연되고 희소하며, 즉 정책 모델은 전체 코드를 생성한 후에만 보상을 수신한다. 이 시나리오에서, 탐험은 어렵다. 우리의 방법의 핵심은 이러한 긴 일련의 탐구 문제를 짧고 쉽게 탐구할 수 있는 하위 과제의 커리큘럼으로 분해하는 것이다. 코드 생성을 코드 완료 하위 작업으로 단순화합니다. 이러한 하위 작업은 훈련 데이터 세트의 표준 솔루션에서 자동으로 구성됩니다.\n' +
      '\n' +
      '인간 요구조건(x\\)을 고려해보면, CCCS의 훈련 단계에서 탐사의 시작점(s^{*}\\)은 최종 상태 근처의 상태이다. 구체적으로, 인간의 요구사항 \\(x\\)과 표준해의 앞부분 \\(x_{p}=\\{a_{i}\\}_{i=0}^{s^{*}}\\)을 제공하고, 정책모델을 학습하여 \\(x^{{}^{\\prime}=(x,x_{p})\\)을 기반으로 코드를 완성한다. \\(\\hat{y}\\)을 \\(x_{p}\\)과 출력궤적 \\(\\tau\\), 즉 \\(\\hat{y}=(x_{p},\\tau)\\의 결합수열이라고 하자. 보상 모델은 코드 스니펫의 정확도에 따라 보상 \\(r\\)을 입력으로서 \\(\\hat{y}\\)으로 제공하며, 여기서 우리는 다음과 같이 이전 접근법 [14; 33]과 동일한 설정을 사용한다:\n' +
      '\n' +
      '\\begin{cases}\\begin{aligned} +& 1,\\text{ if }\\hat{y}\\text{ passed all unit tests}\\\\-& 0.3,\\text{ if }\\hat{y}\\text{ failed any unit test}\\\\-& 0.6,\\text{ if }\\hat{y}\\text{ happened runtime error}\\\\-& 1,\\text{ if }\\hat{y}\\text{ happened compile error}\\end{aligned}\\end{cases}\\tag{3}\\text{\n' +
      '\n' +
      '리워드(r\\)와 궤적(\\tau\\)을 이용하여 정책 모델(\\(\\pi_{\\theta}\\)을 최적화하기 위해 PPO(Proximal Policy Optimization) 알고리즘[30]을 사용한다. 최적화 단계에서는 프롬프트를 제공하기 위해 사용되는 표준해의 코드 세그먼트 \\(x_{p}\\)를 마스킹하여 정책 모델 \\(\\pi_{\\theta}\\) 업데이트의 기울기에 기여하지 않도록 한다. CCCS는 다음과 같이 이의 함수를 최대화하여 정책 모델 \\(\\pi_{\\theta}\\)을 최적화한다.\n' +
      '\n' +
      '\\text{Objective}(\\theta) =E_{(x^{}^{\\prime}},\\hat{y})\\sim D_{\\pi_{\\theta}}[r(x^{}^{prime}},\\hat{y})\\] \\[-\\beta\\log(\\pi_{\\theta}(\\hat{y}|x^{}^{\\prime}}))/\\pi^{text{ref}( \\hat{y}|x^{}^{\\prime}})] \\tag{4}\\\\ta}}[r(x^{}^{prime}},\\hat{y}\\beta\\log(\\pi_{\\theta}(\\hat{y}|x^{}^{\\prime}}}}}/\\pi^{ref}}( \\hat{y}|x^{}^{\\prime}}}}}\n' +
      '\n' +
      '여기서 \\(\\pi^{\\text{ref}}\\)는 SFT 모델에 의해 초기화되는 PPO의 참조 모델이다.\n' +
      '\n' +
      '훈련이 진행됨에 따라 탐사의 시작점\\(s^{*}\\)은 점차 정준해의 시작점으로 이동하게 된다. 구체적으로, 각 훈련 샘플에 대해 임계값 \\(\\rho\\)을 설정한다. 우리는 \\(\\pi_{\\theta}\\)에 의해 생성된 코드 세그먼트의 누적 정확한 비율이 \\(\\rho\\)보다 클 때마다 시작점을 시작점으로 이동시킨다. 훈련의 후반 단계에서, 우리의 방법의 탐색은 원래의 강화 학습의 탐색 과정, 즉 \\(s^{*}=0\\)과 동일하며, 여기서 정책 모델은 인간의 요구 사항만을 입력으로 사용하여 코드를 생성한다.\n' +
      '\n' +
      '그림 2: 우리 방법의 개요. 코드 생성에서 희박하고 지연된 보상이 있는 환경과 긴 시퀀스를 포함하는 복잡한 인간 요구 사항은 바닐라 RL에 대한 탐사를 어렵게 만든다. CCCS에서는 복잡한 탐구 문제를 하위 과제의 커리큘럼으로 분해한다. 표준 솔루션의 일부를 프롬프트로 사용하면 LLM이 간단한 시퀀스부터 탐색할 수 있습니다. 리워드의 계산은 실행된 코드 스니펫에만 관련되며, 전체 코드(즉,)로 LLM을 최적화하는 것은 부정확하다. FGO에서는 단위 테스트에서 실행되지 않은 토큰(즉,)을 마스킹하고 실행된 토큰(즉,)을 사용하여 손실 함수만 계산하여 세밀한 최적화를 제공한다.\n' +
      '\n' +
      '조건문의 시작 위치에서 시작점 \\(s^{*}\\)을 샘플링하여 나머지 미작성 코드 세그먼트를 완성한다. 구체적으로, 조건문의 수가 더 많은 프로그램은 독립적인 경로를 증가시켜 더 높은 논리적 복잡성을 초래한다[32]. 이러한 복잡성은 훈련의 질을 향상시키기 위해 더 빈번한 샘플링을 필요로 하는 반면, 조건문이 적은 프로그램은 덜 빈번한 샘플링을 필요로 한다. 이 샘플링 방법은 코드 구조의 균형 있고 대표적인 샘플링을 가능하게 하여 훈련 데이터 세트에서 복잡하고 단순한 의미 구성 모두를 만족시킨다. 교육 단계를 가속화하기 위해 우리는 \\(i\\)번째 표본의 커리큘럼 수를 \\(\\lceil\\sqrt{E_{i}}\\rceil\\)으로 설정했으며, 여기서 \\(E_{i}\\)은 조건문의 수이다. 교육과정에서의 \\(i\\)번째 표본은 \\(\\lceil\\frac{E_{i}}{\\lceil\\sqrt{E_{i}}\\rceil\\rceil\\) 대신 \\(\\lceil\\frac{E_{i}}{\\lceil\\sqrt{E_{i}\\rceil\\)이다.\n' +
      '\n' +
      'CCCS의 핵심 통찰력은 다음과 같이 요약될 수 있다: 1) 목표 근처의 상태(즉, 최종 상태)로부터 탐색하기 쉽다. 2) 목표에서 멀리 떨어진 상태에서 출발하는 탐구는 어렵지만, 이미 목표에 도달하는 방법을 배운 상태를 활용할 수 있는 때가 더 쉬워진다.\n' +
      '\n' +
      '**FGO.** 코드 생성에서 보상과 액션의 관계는 Atari[25; 19]와 같은 다른 강화 학습 작업과 다르다. 코드 생성에서 우리는 생성된 코드에서 보상을 계산하는 것과 무관한 일련의 동작을 배제할 수 있다. 구체적으로, 섹션 2에서 언급된 바와 같이, 단위 테스트를 위해, 컴파일러로부터의 피드백은 실행되는 코드 스니펫들에만 관련된다. 그러나 바닐라 RL 최적화 목표에서는 수학식 4와 같이 궤적의 모든 액션이 정책 업데이트에 사용되는 그래디언트의 계산에 관여하여 부정확하다.\n' +
      '\n' +
      '최적화의 정밀도를 향상시키기 위해 정책 모델을 업데이트하기 위한 손실을 계산할 때 단위 테스트에서 실행되지 않는 작업(즉, 토큰)을 마스킹한다. CCCS와 FGO의 전체 알고리즘은 알고리즘 1에 자세히 설명되어 있다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '본 절에서는 먼저 APPS 데이터셋을 기반으로 수동으로 검증하여 코드 생성을 위한 고품질 데이터셋인 APPS+를 소개한다. 그런 다음 실험 세부 사항과 실험 결과에 대해 자세히 설명한다.\n' +
      '\n' +
      '### Dataset Preprocessing\n' +
      '\n' +
      '강화 학습은 양질의 학습 데이터를 많이 필요로 한다. 조사하는 동안 현재 사용 가능한 오픈 소스 데이터 세트 중 APPS만 이 요구 사항을 충족한다는 것을 발견했다. 그러나 입력, 출력 또는 표준 솔루션 누락, 컴파일 불가능하거나 실행 불가능한 표준 솔루션, 실행 출력의 불일치와 같은 잘못된 인스턴스가 있음을 발견했다.\n' +
      '\n' +
      'APPS 데이터 세트를 정제하기 위해 입력, 출력 또는 표준 솔루션이 없는 인스턴스를 제외했다. 그런 다음 단위 테스트의 실행 및 비교를 용이하게 하기 위해 입력 및 출력 형식을 표준화했다. 각 인스턴스에 대한 단위 테스트 및 수동 분석을 수행하여 불완전하거나 관련 없는 코드, 구문 오류, API 오용 또는 라이브러리 종속성이 누락된 인스턴스를 제거했다. 출력의 불일치에 대해 문제 설명을 수동으로 검토하여 예상 출력을 수정하거나 인스턴스를 제거했습니다.\n' +
      '\n' +
      '마지막으로 7,456개의 인스턴스를 포함하는 APPS+ 데이터 세트를 구성한다. 각각의 인스턴스는 프로그래밍 문제 설명, 표준 솔루션, 함수 이름, 단위 테스트(즉, 입력 및 출력), 및 스타터 코드(즉, 표준 솔루션의 시작 부분)를 포함한다. 부록 A는 APPS+로부터의 예를 예시한다. 그림의 상단 섹션에는 문제 설명이 표시되고 오른쪽 섹션에는 표준 솔루션, 단위 테스트 및 메타데이터가 표시된다. APPS+에 대한 추가 세부 사항은 부록 B.1에서 논의된다.\n' +
      '\n' +
      '### Experiment Details\n' +
      '\n' +
      '**Benchmark.** 우리 연구에서 우리는 처음에 사전 처리된 **APPS+** 데이터 세트에 대한 방법과 기준을 평가했다. 또한 코드 생성에서 널리 사용되는 벤치마크, 즉 **MBPP**(대부분 기본 프로그래밍 문제) [2] 및 **HumanEval**[3]에 대해서도 이러한 방법을 평가한다. 우리는 이전 접근법[14; 33]과 동일한 제로샷 학습 설정에서 MBPP와 HumanEval 벤치마크를 평가한다. 이 설정에서 우리는 APPS+ 데이터셋에서만 모델을 미세 조정하고 MBPP와 HumanEval에서 코드 생성 성능을 평가한다. 벤치마크에 대한 자세한 설명은 부록 B.1에서 확인할 수 있다.\n' +
      '\n' +
      '**Baselines.** Step-Coder의 효과를 검증하고 APPS+ 데이터셋에서 LLMs의 성능을 평가하기 위해 StarCoder[15], WizardCoder[23], DeepSeek-Coder[8], CodeLlama(Base, Python, Instruct)[27]의 세 가지 버전을 포함하여 광범위한 베이스라인을 고려한다. 또한, 우리는 또한 바닐라 PPO와 PPOCoder[33] 및 RLTF[20]를 포함한 두 가지 최첨단 RL 기반 접근법을 고려한다. 공정한 비교를 보장하기 위해 APPS+ 데이터 세트에서 동일한 백본(즉, DeepSeek-Coder-Instruct[8])을 사용하여 이러한 방법을 적용하는 실험을 수행했다. 우리의 방법의 필요성과 효과를 입증하는 것 외에도, 우리는 훈련 데이터의 효과를 배제하기 위해 APPS+ 데이터 세트에 대해 미세 조정 DeepSeek-Coder-Instruct [8]을 감독했다. 이러한 기준선에 대한 자세한 설명은 부록 B.2에서 논의된다.\n' +
      '\n' +
      '구현 상세.** SFT 단계 동안, 우리는 \\(2e^{-5}\\)으로 설정된 학습 속도를 채택하고, 3개의 에폭에 대한 훈련을 수행하고, 선형 붕괴가 0인 \\(0.3\\) 에폭의 워밍업 기간을 사용한다. 미세 조정 프로세스는 8개의 NVIDIA A100 80G GPU가 있는 장치에서 수행되었으며 전체 배치 크기는 \\(64\\)으로 설정되었다. PPO 교육 단계에서는 정책 모형은 \\(5e^{-7}\\), 비평가 모형은 \\(1.5e^{-6}\\)의 학습률을 사용한다. 각 예제에 대해 핵 샘플링을 사용하여 \\(16\\) 롤아웃 코드를 수집한다. 샘플링 온도는 \\(0.8\\), top-p는 \\(0.9\\), 최대 출력 토큰 길이는 \\(1024\\)으로 설정하였다. 토큰 수준의 KL 패널티 계수 \\(\\beta\\)는 \\(0.05\\)으로 설정되었으며 클립 값은 \\(0.8\\)이다. 디코딩 단계에서, 온도 및 top_p는 각각 \\(0.2\\) 및 \\(0.95\\)으로 설정된다.\n' +
      '\n' +
      '**평가 & Metric.** 우리는 Python3.x를 기반으로 실험을 수행한다. 또한 RL 기반 방법에서 보상 수집 동안 Python3.x를 사용한다는 점에 유의한다. 이전 연구 [27; 23; 14]에 이어 **Pass@k**[3] 메트릭을 사용하여 모든 모델을 평가한다. Pass@k는 인간 요구사항당 k-생성 코드 솔루션 중 적어도 하나가 모든 단위 테스트를 성공적으로 통과한 인스턴스의 비율을 정량화한다. 코드 생성에 사용된 프롬프트는 부록 D에 나열되어 있습니다.\n' +
      '\n' +
      '### APPS+에 대한 실험 결과\n' +
      '\n' +
      '코드 생성에서 널리 사용되는 LLM과 StepCoder의 성능을 평가하기 위해, 우리는 구축한 APPS+ 데이터셋에 대한 실험을 수행한다. 실험 결과는 표 1에 설명되어 있다. 결과는 RL 기반 모델이 기본 모델과 SFT 모델을 모두 포함하여 다른 언어 모델보다 우수하다는 것을 나타낸다. 강화학습이 안내된 모델의 출력 공간을 보다 효과적으로 탐색함으로써 코드 생성의 질을 더욱 높일 수 있다고 추론하는 것이 합리적이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c c c|c} \\hline \\hline \\multirow{2}{*}{**Models**} & \\multirow{2}{*}{**Size**} & \\multicolumn{4}{c}{**APPS+**} \\\\  & & **Introductory** & **Interview** & **Competition** & **Overall** \\\\ \\hline \\multicolumn{5}{c}{Base Models} \\\\ \\hline CodeLlama [27] & 13B & 18.7 & 11.0 & 0.0 & 13.0 \\\\ CodeLlama-Python [27] & 13B & 29.0 & 12.3 & 2.9 & 17.9 \\\\ DeepSeek-Coder-Base [8] & 6.7B & 13.0 & 10.3 & 5.0 & 10.9 \\\\ \\hline \\multicolumn{5}{c}{Supervised Fine-tuned Models} \\\\ \\hline StarCoder [15] & 15.6B & 6.3 & 4.1 & 0.7 & 4.7 \\\\ CodeLlama-Instruct [27] & 13B & 33.3 & 11.0 & 1.4 & 18.7 \\\\ WizardCoder-Python-V1.0 [23] & 13B & 39.7 & 15.1 & 4.3 & 23.6 \\\\ DeepSeek-Coder-Instruct [8] & 6.7B & 49.4 & 18.7 & 3.6 & 29.2 \\\\ SFT on APPS+ & 6.7B & **50.1** & **19.0** & **6.4** & **29.8** \\\\ \\hline \\multicolumn{5}{c}{Reinforcement Learning-based Models (Using DeepSeek-Coder-Instruct-6.7B as the backbone)} \\\\ \\hline Vanilla PPO & 6.7B & 53.7 & 20.1 & 5.0 & 31.7 \\\\ PPOCoder [33] & 6.7B & 54.4 & 20.3 & 6.4 & 32.1 \\\\ RLTF [20] & 6.7B & 55.1 & 20.8 & 6.4 & 32.7 \\\\ \\hline \\multicolumn{5}{c}{**StepCoder (Ours)**} & 6.7B & **59.7** & **23.5** & **8.6** & **36.1** \\\\ \\multicolumn{5}{c}{**w/o CCCS**} & 6.7B & 58.7 & 21.7 & 7.1 & 34.6 \\\\ \\multicolumn{5}{c}{**w/o FGO**} & 6.7B & 58.4 & 23.3 & 8.6 & 35.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 제안된 APPS+에 대한 pass@1의 결과. 우리는 대중적이고 널리 사용되는 최첨단 방법을 우리의 방법과 비교한다. 공정한 비교를 보장하기 위해 APPS+ 데이터 세트의 백본으로 동일한 기본 모델(즉, DeepSeek-Coder-Instruct-6.7B[8])을 사용하는 이러한 RL 기반 방법을 적용한다. 또한 DeepSeek-Coder-Instruct-6.7B를 기반으로 한 APPS+ 데이터셋을 사용하여 감독 미세 조정을 수행하여 접근법의 효과와 필요성을 추가로 검증한다.\n' +
      '\n' +
      'compiler feedback.\n' +
      '\n' +
      '또한, StepCoder는 다른 RL 기반 접근 방식을 포함한 모든 기준 모델을 능가하여 최고 점수를 달성했습니다. 구체적으로 \'입문\', \'인터뷰\', \'경쟁\'에서 각각 59.7%, 23.5%, 8.6%의 결과를 얻었다. 우리의 접근법은 복잡한 코드 생성 작업을 코드 완료 하위 작업으로 단순화함으로써 달성되는 다른 RL 기반 방법과 비교하여 출력 공간을 탐색하는 데 탁월하다. 또한 FGO 프로세스는 정책 모델을 정확하게 최적화하는 데 중추적인 역할을 한다. 또한 동일한 백본을 기반으로 APPS+ 데이터셋에서 Fine-tuning을 감독한 LLM보다 StepCoder의 성능이 우수함을 확인하였다. 후자는 백본에 비해 생성된 코드의 통과율을 개선하는 데 거의 도움이 되지 않았다. 이것은 또한 모델을 최적화하기 위해 컴파일러 피드백을 사용하는 방법이 코드 생성에서 다음 토큰 예측보다 생성된 코드의 품질을 더 잘 향상시킨다는 것을 직접적으로 입증한다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      'StepCoder에서 개별 구성요소의 영향을 조사하기 위해 CCCS만 있는 StepCoder와 FGO만 있는 StepCoder를 포함하여 접근법의 두 가지 변형으로 절제 실험을 수행했다. 실험 결과는 표 1에 제시되어 있으며, 실험 결과는 제안된 방법의 두 구성 요소가 바닐라 PPO에 비해 생성된 코드의 품질을 향상시킨다는 것을 보여준다. CCCS는 경쟁 수준의 문제를 해결하는 데 있어 성능을 향상시킬 수 있다. CCCS가 보다 복잡한 인간 요구 사항에 대한 탐구를 효과적으로 단순화한다는 점을 고려할 때 이러한 개선은 논리적이다. 동시에 FGO는 컴파일러 피드백을 관련 실행된 코드 스니펫과 통합하여 단위 테스트의 통과율을 높인다.\n' +
      '\n' +
      'MBPP와 Humanval에 대한### 결과\n' +
      '\n' +
      '본 논문에서 제안한 방법의 효율성을 입증하기 위해, 잘 알려진 벤치마크인 MBPP와 HumanEval을 사용하여 다양한 접근법에 대한 StepCoder의 비교 분석을 수행하였다. 이러한 모델은 APPS+에 대해 훈련된 다음 MBPP 및 HumanEval에 대해 평가된다. 실험 결과는 표 2에 설명되어 있으며, 이는 StepCoder가 두 벤치마크에서 다른 모든 모델보다 우수함을 보여준다.\n' +
      '\n' +
      '그러나 APPS+와 두 벤치마크 사이의 훈련 데이터에 잠재적인 중복이 우려되어 성능 향상에 기여할 수 있다. 이러한 문제를 해결하기 위해 우리는 이전 작업 [2; 14]에 따라 해당하는 두 표준 솔루션의 코드 라인 중첩 비율을 계산하여 APPS+와 벤치마크 간의 차이를 분석한다. 연구 결과는 그림 3에 제시되어 있으며, 이 증거는 강화 학습에서 탐색 문제를 개선함으로써 광범위한 코드 생성 작업에서 생성된 코드의 품질과 성능을 향상시키는 데 우리의 접근법의 효율성을 강조한다.\n' +
      '\n' +
      '한편, 우리의 연구 결과는 MBPP 및 HumanEval 벤치마크 모두에서 SFT 모델의 성능에서 상당한 저하를 보여주었다. 오류 사례에 대한 추가 분석에서는 소수의 오류가 기능 이름 오류와 관련이 있는 반면 대다수는 프로그램 정확성 오류와 관련이 있는 것으로 나타났다. 이것은 또한 단일 데이터 세트의 SFT가 명령을 따르는 능력과 일반화하는 능력을 손상시켜 다른 작업에서 코드 생성의 성능에 영향을 미칠 수 있음을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c} \\hline \\hline \\multicolumn{1}{c}{**Models (6.7B)**} & \\multicolumn{1}{c|}{**HumanEval**} & \\multicolumn{1}{c}{**MBPP**} \\\\ \\hline DeepSeek-Coder-Instruct & **78.0** & **64.2** \\\\ SFT on APPS+ & 55.5 & 54.8 \\\\ \\hline Vanilla PPO & 78.0 & 65.0 \\\\ PPOCoder & 76.8 & 63.8 \\\\ RLTF & 76.8 & 65.2 \\\\\n' +
      '**StepCoder (Ours)** & **78.7** & **67.0** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: MBPP 및 HumanEval에 대한 pass@1의 결과. 우리는 제로샷 학습 환경에서 코드 생성에 대한 LLM의 성능을 평가한다. 이 설정에서 모델은 제안된 APPS+ 데이터 세트에서 미세 조정되고 MBPP 및 HumanEval에서 능력에 대해 테스트된다.\n' +
      '\n' +
      '그림 3: APPS+와 두 벤치마크 간의 중복 선 분석. APPS+와 APPS+ 사이의 데이터 중첩은 매우 작다. 0.2%와 7.1%만이 MBPP와 휴먼에벌에서 각각 절반 이상의 라인이 일치했다.\n' +
      '\n' +
      '대조적으로, RL 기반 방법은 코드 생성의 보이지 않는 작업에 대한 성능을 향상시킬 수 있다.\n' +
      '\n' +
      '단위시험 결과### 분석\n' +
      '\n' +
      '그림 4와 같이 모든 단위 테스트를 통과하지 못한 사례에 대한 결과를 추가로 분석한 결과, 본 논문에서 제안한 방법은 컴파일 오류 가능성을 효과적으로 줄일 수 있으며, 특히 인터뷰 수준 및 경쟁 수준 프로그래밍 문제에서 두드러진다. 그러나 모든 LLM은 컴파일 오류에 비해 실행 시간 오류 및 실패에 더 취약하지만 StepCoder는 실행 시간 오류 및 실패 비율이 상대적으로 낮다. 이러한 결과는 StepCoder가 컴파일 오류에 덜 취약하지만 여전히 런타임 오류와 실패로 고통받는다는 것을 보여준다. 결과적으로, 이러한 결과는 향후 연구가 런타임 오류를 크게 줄이는 데 더 집중해야 하며, 이는 이러한 모델에 의해 생성된 코드의 품질과 통과율을 크게 향상시킬 수 있음을 시사한다.\n' +
      '\n' +
      '## 5 관련 업무\n' +
      '\n' +
      '### 코드 생성을 위한 대용량 언어 모델\n' +
      '\n' +
      '최근 LLM은 코드 데이터가 포함된 대용량 텍스트 코퍼스를 학습하여 자연어 이해와 코드 생성에 뛰어난 능력을 보이고 있다. 여러 사전 훈련된 언어 모델(PLM)은 CodeGPT[21], PanGu-Coder[4], SantaCoder[1], CodeGeex[44] 및 Phi-1.5[16]를 포함하는 코드 생성에 상당한 잠재력을 보여준다. 또한, SFT 모델은 CodeX[3], StarCoder[15], WizardCoder[23], Code Llama Instruct[27], DeepSeek-Coder[8] 등과 같이 보다 경쟁력 있는 성능을 달성한다.\n' +
      '\n' +
      '강화학습은 환경을 탐색하고 보상을 얻어 최적의 정책을 학습하는 방법이다[41;34]. 최근 일부 연구자들은 LLM에 RL을 도입하고 단위 테스트 피드백을 활용하여 생성된 코드의 품질을 개선하여 정책 모델의 출력 공간을 탐색하였다[33; 20; 14]. 예를 들어 CodeRL [14]는 단위 테스트의 신호를 보상으로 활용하고 행위자 비판 접근법 [12; 35]를 사용하여 코드 생성에 대한 모델을 향상시킨다. PPOCoder[33]는 PPO 알고리즘[30]과 RLTF[20]을 사용하여 CodeRL을 정제하여 오류 위치를 통해 세밀한 보상을 제공하지만 보상 공간은 여전히 희박하다. 그러나 희박한 보상을 특징으로 하는 환경에서 복잡한 과제를 탐색하는 것은 어려운 일이다. 이러한 방법들은 여전히 코드 생성에서 모델의 성능을 향상시키기 위해 RL을 효과적으로 사용하는 데 미치지 못한다.\n' +
      '\n' +
      '강화학습에서의### 탐색\n' +
      '\n' +
      '탐사는 긴 수열과 희박한 보상 문제를 해결하는 데 중요하다[9; 13]. 시퀀스 생성 과제에서 연구자들은 SFT 모형을 이용하여 정책 모형을 초기화하여 탐색을 개선하였다[26; 31]. 제안된 접근 방식은 유사한 방법을 통합하지만 효과적인 탐사를 보장하기 위해서는 추가 방법이 필요하다. 이것은 SFT 모델에 의해 생성된 코드의 제한된 품질이 탐사를 여전히 어렵게 만드는 복잡한 인간 요구 사항에 직면할 때 특히 분명하다[33].\n' +
      '\n' +
      '다른 주목할 만한 방법은 수학적 추론 및 코드 생성과 같은 복잡한 시퀀스 생성 작업에 대한 단계별 보상을 제공하기 위해 프로세스 감독 보상 모델을 도입한다[38; 18; 22; 24]. 그러나, 이러한 방법들은 보상 모델을 트레이닝하기 위해 큰 선호도 데이터세트를 라벨링하는 것을 필요로 한다. 우리의 접근법과 유사하게, 일부 방법들은 점진적으로 더 도전적인 시작 상태들의 시퀀스로부터 각각의 에피소드를 개시함으로써 학습 커리큘럼을 구성한다[28; 11; 6]. 우리의 접근법과 달리 이러한 방법은 게임 및 로봇 조작과 같은 다른 분야의 탐색 문제를 해결하기 위해 설계되었다. 한편, 본 논문에서 제안하는 방법은 소프트웨어 엔지니어링 기능을 결합하여 조건문을 통해 시작 상태를 동적으로 결정한다. 또한 FGO를 도입하여 커버리지 정보를 활용하여 정책 모델에 대한 세분화된 최적화를 제공한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 RL을 통한 새로운 학습 프레임워크인 StepCoder를 소개한다. StepCoder break down\n' +
      '\n' +
      '그림 4: APPS+에 대한 단위 테스트 결과별 분석. 결과는 CompileError(Reward = -1)와 Runtimeerror & Failure(Reward = -0.6 또는 -0.3)로 분류된다.\n' +
      '\n' +
      '복잡한 탐색 문제를 해결하여 희박한 보상으로 환경을 탐색하는 데 어려움을 줄이면서도 세밀한 최적화를 제공합니다. 또한, 특히 코드 생성을 위한 고품질 데이터 세트 APPS+를 구축한다. 실험 결과 본 논문에서 제안한 방법은 강화학습을 통해 생성된 코드의 품질을 다른 방법들에 비해 효과적으로 향상시킬 수 있음을 보였다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]L. 벤 알 Li D. K. K., C. Mou, C. Akiki, C. Munoz Ferrandis, N. Muenmighoff 미쉬라 A구 Dey, et al. (2023) Santacoder: The stars에 도달하지 마세요! ArXiv:2301.03988. 인용: SS1.\n' +
      '*[2]J. 오스틴 A. 오데나 M. 나명 보즈마, H. 미칼레우스키, D. 다한, E. 장, C. 카이, M. 테리, Q Le, et al. (2021) Program synthesis with large language models. ArXiv:2108.07732. 인용: SS1.\n' +
      '*[3]M. 천진욱 Wuan, H. Ponde de Oliveira Pinto, J. Kaplan, H. Edwards, Y. 부르다 Joseph, G. Brockman, et al.(2021) Evaluation large language models trained on code. ArXiv:2107.03374. 인용: SS1.\n' +
      '*[4]F. Christopoulou, G. Lampouras, M. 그릿타장영 곽지 이규 장민 샤오봉선 Li, et al. (2022) Pangu-coder: function-level 언어 모델링과 프로그램 합성. ArXiv:2207.11280. 인용: SS1.\n' +
      '*[5]O. 기여자(2023) OpenCompass: 기반 모델에 대한 보편적인 평가 플랫폼. 참고: [https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass) 인용: SS1.\n' +
      '*[6]C. 플로렌사, D 헬드, M. 울프마이어 Zhang, and P. Abbeel (2017) Reverse curriculum generation for reinforcement learning. In Conference on robot learning, pp. 482-495. Cited by: SS1.\n' +
      '*[7]S. 오굴와니 폴로조프 Singh, et al.(2017) Program synthesis. Foundations and Trends(r) in Programming Languages4(1-2), pp. 1-119. Cited by: SS1.\n' +
      '* 코드 인텔리전스의 상승. 인용: SS1.\n' +
      '*[9]J. 하태 양현당 멍, 류, Z. Wang(2023) Exploration in deep reinforcement learning: one-agent에서 multiagent domain. IEEE Transactions on Neural Networks and Learning Systems. 인용: SS1.\n' +
      '*[10]D. 헨드릭스 바사르트 카다바스 Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt (2021) Measuring coding challenge competence with APPS. The Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Cited by: SS1.\n' +
      '*[11]I. 호수와 T. 리베데아(2016) 심층 강화 학습과 휴먼 체크포인트 리플레이로 아타리 게임을 플레이합니다. ArXiv:1607.05077. 인용: SS1.\n' +
      '*[12]V. Konda and J. Tsitsiklis (1999) Actor-critic algorithm. 신경 정보 처리 시스템 12. 인용: SS1에 의해 발전한다.\n' +
      '*[13]P. Ladosz 왕민 Kim, and H. Oh(2022) Exploration in deep reinforcement learning: a survey. Information Fusion85, pp. 1-22. Cited by: SS1.\n' +
      '*[14]H. 이영 왕A. 디팍 곶메어, S. Savarese, S. 추홍회(2022) 코더: 미리 학습된 모델과 심층 강화 학습을 통한 마스터링 코드 생성. Advances in Neural Information Processing Systems35, pp. 21314-21328. Cited by: SS1.\n' +
      '*[15]R. 이엘비알 지남 Muenmighoff, D. Koccetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, et al. (2023) StarCoder: source is with you. ArXiv:2305.06161. 인용: SS1.\n' +
      '*[16]Y. 이성 부벡 엘단, A 델 조르노, S Gunasekar, and Y. T. Lee (2023) 교과서는 ii: phi-1.5 기술 보고서 arXiv preprint arXiv:2309.05463. 인용: SS1.\n' +
      '*[17]Y. 이정남 Kushman, J. Schrittwieser, R. 르블론드, T Eccles, J. Keeling, F. Kimeno, A. Dal Lago, et al.(2022) Competition-level code generation with alphacode. Science378(6624), pp. 1092-1097. Cited by: SS1.\n' +
      '*[18]H. 라이트먼, V 고사라주 Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe(2023) 단계별로 검증해보자. ArXiv:2305.20050. 인용: SS1.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* [21] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. _arXiv preprint arXiv:2102.04664_.\n' +
      '* [22] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. _arXiv preprint arXiv:2308.09583_.\n' +
      '* [23] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evol-instruct. _arXiv preprint arXiv:2306.08568_.\n' +
      '* [24] Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang. 2023. Let\'s reward step by step: Step-level reward model as the navigators for reasoning. _arXiv preprint arXiv:2310.10080_.\n' +
      '* [25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533.\n' +
      '* [26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744.\n' +
      '* [27] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. 2023. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_.\n' +
      '* [28] Tim Salimans and Richard Chen. 2018. Learning montezuma\'s revenge from a single demonstration. _arXiv preprint arXiv:1812.03381_.\n' +
      '* [29] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. High-dimensional continuous control using generalized advantage estimation. _arXiv preprint arXiv:1506.02438_.\n' +
      '* [30] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_.\n' +
      '* [31] Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, and Xuan-Jing Huang. 2023. Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 2859-2873.\n' +
      '* [32] Martin Shepperd. 1988. A critique of cyclomatic complexity as a software metric. _Software Engineering Journal_, 3(2):30-36.\n' +
      '* [33] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based code generation using deep reinforcement learning. _arXiv preprint arXiv:2301.13816_.\n' +
      '* [34] Richard S Sutton, Andrew G Barto, et al. 1998. _Introduction to reinforcement learning_, volume 135. MIT press Cambridge.\n' +
      '* [35] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999. Policy gradient methods for reinforcement learning with function approximation. _Advances in neural information processing systems_, 12.\n' +
      '* [36] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: Code generation using transformer. In _Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering_, pages 1433-1443.\n' +
      '* [37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.\n' +
      '* [38] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process-and outcome-based feedback. _arXiv preprint arXiv:2211.14275_.\n' +
      '* [39] Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, and Qun Liu. 2022. Compilable neural code generation with compiler feedback. _arXiv preprint arXiv:2203.05132_.\n' +
      '* [40] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannen Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_.\n' +
      '* [41] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine learning_, 8:229-256.\n' +
      '* [42] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. _arXiv preprint arXiv:2304.12244_.\n' +
      '\n' +
      '* [43] Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Jianye Hao, Zhaopeng Meng, Peng Liu, and Zhen Wang. 2021. Exploration in deep reinforcement learning: a comprehensive survey. _arXiv preprint arXiv:2109.06668_.\n' +
      '* [44] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. 2023. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. _arXiv preprint arXiv:2303.17568_.\n' +
      '* [45] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Yuhao Zhou, Limao Xiong, et al. 2023. Delve into ppo: Implementation matters for stable rhf. In _NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following_.\n' +
      '\n' +
      '## APPS+ 데이터셋의 부록 A 인스턴스\n' +
      '\n' +
      '그림 5와 같이 APPS+ 데이터 세트에서 예를 제시한다.\n' +
      '\n' +
      '## 부록 B 세부사항에서의 실험 설정\n' +
      '\n' +
      '이 섹션에서는 비교되는 기준선과 본 방법의 구현 세부 사항에 대해 자세히 설명한다.\n' +
      '\n' +
      '### Benchmarks\n' +
      '\n' +
      '**APPS+.** 인기 벤치마크 APPS를 정제하여 새로운 벤치마크 APPS+를 구성한다[10]. APPS+는 입문(2,850), 면접(4,020), 경쟁(586)의 세 가지 난이도로 분류되었다. 각 문제의 평균 길이는 255.3 단어이며 코드의 길이는 21.9줄이다. 평균적으로 각 인스턴스에는 세 가지 단위 테스트가 수반되며 표준 솔루션에서 문장의 시작 및 끝 위치를 나타내는 \'조건문\' 속성이 포함된다. 검증 데이터셋에 대해 약 25% 인스턴스(입문 700개, 1,000개 인터뷰, 140개 경쟁)를 무작위로 선택하고 테스트 데이터셋에 대해 또 다른 25% 인스턴스를 선택했다.\n' +
      '\n' +
      '**MBPP.**MBPP[2]는 작지만 일반적인 파이썬 코드 생성 벤치마크이다. 그것은 기본적인 파이썬 지식을 가진 크라우드 노동자 내부 풀에 크라우드 소싱에 의해 생성된 974개의 인스턴스를 포함한다. 이 데이터 세트에서 문제의 난이도는 도입이다. 대부분의 문제는 자연어의 한 문장으로 전달되는 경우가 많으며, 각 문제는 과제 기술, 코드 솔루션, 3개의 자동화된 테스트 케이스로 구성된다. 우리는 이전 연구와 동일한 제로샷 학습 설정에서 LLM을 평가한다[14; 33]. 이 설정에서 우리는 APPS+ 데이터 세트를 기반으로만 모델을 미세 조정하고 MBPP에서 평가한다.\n' +
      '\n' +
      '**HumanEval.**HumanEval[3]은 코드 생성 능력을 평가하기 위해 광범위하게 사용되는 또 다른 벤치마크이다. 그것은 언어 이해, 알고리즘 사고, 기초 수학을 시험하는 164개의 손으로 쓴 파이썬 문제로 구성되어 있다. 이러한 문제의 복잡성은 단순한 소프트웨어 인터뷰 질문과 유사하다. 우리는 또한 제로 샷 학습 환경에서 HumanEval 벤치마크에 대한 모델을 평가한다.\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '**StarCoder.** StarCoder[15]는 GitHub에서 조달한 80+ 프로그래밍 언어로 훈련된 15.5B 파라미터 모델로 1조 개의 토큰을 포괄한다. 그것은 350억 파이썬 토큰에 대해 구체적으로 미세 조정을 거쳐 다양한 코딩 작업 세트에 걸쳐 능숙함을 가능하게 한다. 8K의 확장된 컨텍스트 길이로 스타코더는 특히 주입 기능에 탁월합니다.\n' +
      '\n' +
      '**CodeLlama.** CodeLlama[27]는 7B부터 34B 파라미터까지의 스케일 범위에서 미리 훈련되고 미세 조정된 생성 텍스트 모델의 모음이다. CodeLlama는 세 가지 변형으로 제공됩니다: **CodeLlama**: 일반적인 코드 합성 및 이해를 위해 설계된 기본 모델; **CodeLlama-Python**: Python 프로그래밍 언어를 처리하도록 특별히 설계됨; **CodeLlama-Instruct**: 명령어 팔로우 및 보다 안전한 배포를 위해.\n' +
      '\n' +
      '**WizardCoder.**WizardCoder[23]는 코드 관련 태스크에 Evol-Instruct[42]를 적응시켜 구성한 복잡한 데이터셋을 이용하여 미세 조정하는데, 이는 자체 지시 방식의 추가 개선이다[40]. 보다 복잡한 명령어 데이터를 미세 조정함으로써 코드 생성에 매우 효과적인 것으로 입증되었다.\n' +
      '\n' +
      '**DeepSeek-Coder.**DeepSeek-Coder [8]은 다양한 프로그래밍 언어에 걸친 오픈 소스 코드 모델들 사이에서 최첨단 성능을 보여준다. 그것은 처음부터 훈련된 1B부터 33B까지의 코드 언어 모델들의 모음을 포함한다 이러한 모델에 대한 훈련 코퍼스는 코드와 자연 언어의 조합인 인상적인 2조 토큰으로 구성된다. 각 모델은 16K의 윈도우 크기를 활용하도록 훈련되고, 빈칸 채우기 작업이 훈련 프로세스에 통합되어 모델의 용량을 향상시켜 코드 완료 및 채우기 작업을 용이하게 한다.\n' +
      '\n' +
      '**PPOCoder.**PPOCoder[33]는 처음에 코드 세대에 대해 프록시멀 정책 최적화 알고리즘[30]을 사용한다. 또한, dis\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '```\n' +
      '0: 교육과정 교육을 위한 기차 데이터세트 \\(\\mathcal{D}=\\{(x_{i},y_{i},u_{i},e_{i}),1\\leq i\\leq n\\}\\), 임계값 \\(\\rho_{t}\\).\n' +
      '0: 정책 모델\\(\\pi_{\\theta}\\)\n' +
      '1: 각 표본에 대한 커리큘럼의 보폭을 초기화\n' +
      '2: 훈련 샘플별로 현재 교육과정 \\(c=\\lceil\\sqrt{E_{i}}\\rceil-1\\)을 초기화\n' +
      '3: 훈련 샘플별 통과율 \\(\\rho=0\\) 초기화\n' +
      '4: 한편 TRUE do\n' +
      '5: 미니배치 초기화 \\(\\mathcal{D}_{s}=\\{\\}\\)\n' +
      '6: 최신 정책 모델 \\(\\pi_{\\theta}\\)을 얻음\n' +
      '7: \\(\\mathcal{D}\\)에서 \\(M\\) 크기의 미니 배치를 샘플링\n' +
      '8:for\\(i\\ in \\(0\\), \\(\\cdots\\), \\(M-1\\)do\\(\\triangleright\\) 궤적 샘플링을 시작\n' +
      '9: 시작 위치 pos \\(=s_{i}*c_{i}\\)\\(\\triangleright\\) CCCS 계산\n' +
      '10: 주어진 문맥을 재구성한다\\(x^{{}^{\\prime}}_{i}=x_{i}+y_{i}\\left[:\\text{pos}\\right]\\)\n' +
      '11: 샘플 궤적 \\(\\hat{y_{i}}\\leftarrow\\pi_{\\theta}(.|x^{{}^{\\prime}}_{i})\\)\n' +
      '12 : 수학식 3을 이용한 계산 보상 \\(r_{i}\\)\n' +
      '13: 미실행 스니펫의 마스크 매트릭스 \\(m_{ij}=\\left[1\\text{ if }\\hat{y}^{j}_{i}\\text{ is executed else }0\\right]\\)\\(\\triangleright\\) FGO\n' +
      '14: mini-batch \\(\\mathcal{D}_{s}\\)에 \\(\\{x^{{}^{\\prime}}_{i},\\hat{y_{i}},u_{i},r_{i},s_{i},c_{i},m_{i}\\}을 더한다.\n' +
      '15:endfor\n' +
      '16:\\(\\theta\\leftarrow\\mathcal{A}(\\theta,\\mathcal{D}_{s})\\(\\triangleright\\) PPO 알고리즘에 의한 정책 모델 업데이트\n' +
      '17:for\\(i\\ in \\(0\\), \\(\\cdots\\), \\(M-1\\)do\n' +
      '18:if\\(r_{i}=1\\)then\\(\\triangleright\\) 이동평균을 이용한 갱신통과율\n' +
      '19:\\(\\rho_{i}=\\alpha+(1-\\alpha)*\\rho_{i}\\)\n' +
      '20:else\n' +
      '21:\\(\\rho_{i}=(1-\\alpha)*\\rho_{i}\\)\n' +
      '22:endif\n' +
      '23:if\\(\\rho_{i}>\\rho_{t}\\)then\\(\\triangleright\\) 업데이트 조건을 충족하고 다음 단계로 진행\n' +
      '24:\\(\\rho_{i}=0\\)\n' +
      '25:\\(c_{i}=min(c_{i}-1,0)\\)\n' +
      '26:endif\n' +
      '27:endfor\n' +
      '28:endwhile\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 스텝코더 : 컴파일러 피드백으로부터 강화학습을 통한 코드 생성 개선\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>