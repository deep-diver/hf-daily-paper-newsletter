<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Vid2Robot: Cross-Attention Transformer를 이용한 End-to-End Video-conditioned Policy Learning\n' +
      '\n' +
      'Vidhi Jain\\({}^{1,2}\\) Maria Attarian\\({}^{1,3}\\) Nikhil J Joshi\\({}^{1}\\) Ayzaan Wahid\\({}^{1}\\) Danny Driess\\({}^{1}\\) Quan Vuong\\({}^{1}\\) Pannag R Sanketi\\({}^{1}\\)\n' +
      '\n' +
      'Pierre Sermanet\\({}^{1}\\) Stefan Welker\\({}^{1}\\) Christine Chan\\({}^{1}\\) Igor Gilitschenski\\({}^{3}\\) 요나탄 Bisk\\({}^{2}\\) Debidatta Dwibedi\\({}^{1}\\)\n' +
      '\n' +
      '토론토 대학교\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 로봇 시스템은 일반적으로 작업에 대한 텍스트 지침에 의존하지만, 이 작업은 다른 접근법을 탐구한다: 로봇이 인간을 관찰하는 것으로부터 작업을 직접 추론할 수 있는가? 이러한 변화는 인간의 의도를 해독하고 자신의 물리적 제약과 환경 내에서 실행 가능한 행동으로 변환하는 로봇의 능력을 필요로 한다.\n' +
      '\n' +
      '로봇을 위한 새로운 엔드 투 엔드 비디오 기반 학습 프레임워크인 Vid2 로봇을 소개합니다. 조작 작업의 비디오 시연과 현재 시각적 관찰이 주어지면, Vid2로봇은 로봇 동작을 직접 생성한다. 이는 인간 비디오 및 로봇 궤적의 대규모 데이터 세트에 대해 훈련된 통합 표현 모델을 통해 달성된다. 모델은 교차 주의 메커니즘을 활용하여 프롬프트 비디오 기능을 로봇의 현재 상태에 융합하고 관찰된 작업을 모방하는 적절한 동작을 생성한다. 정책 성능을 더욱 향상시키기 위해, 우리는 인간과 로봇 비디오 표현 사이의 정렬을 향상시키는 보조 대조 손실을 제안한다.\n' +
      '\n' +
      '우리는 실제 로봇에 대한 Vid2 로봇을 평가하여 인간 시연 비디오를 사용할 때 다른 비디오 조건 정책에 비해 20% 향상된 성능을 보여준다. 또한, 본 모델은 관측된 움직임을 한 객체에서 다른 객체로 성공적으로 옮기고, 긴 지평선 구성과 같은 새로운 기능을 나타내어 실제 응용 분야에 대한 잠재력을 보여준다. 프로젝트 웹사이트: vid2robot.github.io.\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      '사람들의 일상 생활에 도움을 주는 다용도 로봇을 만드는 길은 이동 중에 새로운 기술을 배워야 합니다. 특정 가정이 어떤 브랜드의 식기세척기를 사용하는지와 같은 작은 선호도부터 집을 청소하는 완전히 다른 방법까지 다양합니다. 알려진 기술의 경우 인간은 단순히 자연어로 의사소통하지만 뉘앙스가 필요하거나 기술이 참신할 때 시연으로 되돌아간다. 예를 들어, 특정 전자레인지가 어떻게 작동하는지 또는 캐비닛을 정리하는 것을 선호하는 방법을 보여줄 수 있습니다. 끊임없는 로봇 배치를 가능하게 하기 위해 로봇은 인간에게 자연스럽게 다가오는 새로운 정책을 학습하기 위한 시연부터 일반화를 위한 동일한 능력이 필요하다.\n' +
      '\n' +
      '인간은 3인칭 시각적 관찰을 바탕으로 다른 인간의 의도를 추론할 수 있다. 우리는 타인의 목표를 암묵적으로 이해하기 위해 사회적 추론과 상식을 사용하는 경우가 많다. 이 능력은 어린이와 성인(예: How-To Video[30])으로 활용되어 작업의 기계적 뉘앙스가 정지 이미지 또는 텍스트[5](예: 반죽 또는 뜨개질 방법)에서 캡처하기 어려운 모든 것을 학습한다. 로봇이 다른 에이전트의 의도를 이해하도록 가르칠 수 있다면 인간과 더 잘 상호작용하고 작업을 더 효율적으로 수행할 수 있다.\n' +
      '\n' +
      '이 작업은 로봇이 비디오 시연을 보며 작업을 수행하는 것을 배우는 시각적 모방 학습에 중점을 둔다. 이 설정은 몇 가지 이점을 제공합니다. 첫째, 로봇이 다른 실시예를 가진 에이전트로부터 학습할 수 있도록 하여 원격 조작 없이 새로운 기술 습득을 가능하게 한다. 둘째, 로봇이 로봇에 위치하지 않더라도 전문가로부터 학습할 수 있습니다. 마지막으로 시각적 모방 학습은 가르치는 데 이상적입니다.\n' +
      '\n' +
      '도. 1: **개요.** Vid2로봇은 비디오-조건 로봇 정책. 인간 시연(위)이 주어지면, Vid2Robot은 태스크 의미론을 인식하고 로봇의 현재 시각적 관찰(왼쪽 아래)에 기초하여 동일한 태스크를 수행한다. 성공적인 궤적은 오른쪽 하단에 제시되어 있다.\n' +
      '\n' +
      '말로 설명하기 어렵거나 불가능한 작업들.\n' +
      '\n' +
      '기존의 다중 작업 로봇 조작 모델(예: RT-1[7], RT-2[8], RT-X[33])은 언어 컨디셔닝을 이용하여 로봇 궤적을 출력한다. 이러한 과제 명세에 대한 텍스트만의 의존성은 로봇이 다의어 및 문맥에 따라 실행이 극적으로 변하는 과제를 다루기 어렵게 만든다. 예를 들어, \'_open_ drawer\', \'_open_ cabinet\', \'_open_ container with lid\' 및 \'_open_ jar with screw cap\'은 동일한 동사를 공유할 수 있지만 각 상호작용에 대해 매우 다른 모터 제어를 필요로 한다. 여기서 에이전트는 정책을 일반화하여서는 안 되는 반면, 유형, 색상 및 모양이 다양한 한 _drawer_에서 다른 _drawer_로 일반화해야 한다. 이러한 이유로, 높은 수준의 계획 접근법들을 위한 프리미티브들을 설계하기 어려운 광범위한 태스크들이 존재한다[25, 2].\n' +
      '\n' +
      '또 다른 일반적인 접근법은 목표 조건 행동 복제 작업에서 최종 목표 이미지를 사용하는 것이었다[31, 24]. 여러 작업 사양들이 환경의 결과적인 상태의 관점에서 정의될 수 있지만, 액션이 수행되는 방식이 명세서의 일부인 다른 것들이 있다. 예를 들어, \'깃발을 잡는다\'와 \'깃발을 흔들다\'는 동일한 최종 목표 이미지를 가질 수 있다. 이러한 모호성은 여러 개의 하위 목표 프레임, 즉 비디오 컨디셔닝의 사용을 통해 해결될 수 있다.\n' +
      '\n' +
      '언어 조건화 정책은 다소 높은 성공률을 달성하는 반면, 비디오 조건화 정책은 이전 작업[21]에서 볼 수 있듯이 성능에서 뒤쳐졌다. 비디오 컨디셔닝을 통한 양호한 성능의 사례들[40]은 제공된 비디오가 제한된 가변성을 갖는 동일한 작업 공간에서 나오도록 요구한다. 관찰을 기반으로 비디오 조건화 정책에 대한 세 가지 주요 과제를 식별한다: (1)_고차원 데이터_: 원시 비디오는 더 많은 계산과 처리를 위한 메모리를 필요로 하는 고차원 데이터이다. 이는 비디오 조건 다중 작업 정책을 대규모로 훈련하기 어렵게 만든다. (2) 태스크 명세에서 _Variability_: 사람들이 동일한 태스크를 수행하는 방법에 상당한 변동이 있을 수 있다. \'컵을 쌓지 않기\'와 같은 작업에 대한 시위는 배경 산만함과 조명 조건의 변화 외에도 시각적으로 독특하고 물리적으로 다양한 컵을 가질 수 있다. 이는 새로운 설정에서 동일한 작업을 수행해야 하는 정책에 대한 작업 사양의 높은 변동성으로 이어집니다. (3)_훈련 데이터의 제한된 가용성_: 인터넷에 레이블이 지정되지 않은 비디오 데이터가 풍부하지만 우리 로봇이 수행할 수 있는 특정 작업에 대해 레이블이 지정된 비디오 데이터 세트를 얻는 것은 어렵다.\n' +
      '\n' +
      '이러한 어려움에도 불구하고 지적한 바와 같이 비디오 조건화 정책 학습은 로봇이 마스터해야 하는 핵심 도전 로봇이다. 따라서 세부적이고 잠재적으로 모호한 언어 프롬프트에 대한 의존도를 줄이기 위해 작업 명세의 또 다른 방법으로 물리적 시각적 시연을 가능하게 하는 것을 목표로 한다. 이를 위해 비디오-컨디셔닝이 적용된 엔드 투 엔드 모델이 로봇에 대한 작업을 지정하는 데 어떻게 사용될 수 있는지 연구한다.\n' +
      '\n' +
      '동영상 시연 형태로 지정된 작업에 신속하게 적응할 수 있는 엔드 투 엔드 시스템을 개발하는 것을 목표로 합니다. 객체만을 위한 비디오 표현과 동사 인식(21) 또는 시뮬레이션(44)에서 학습된 모터 제어(44)와 달리, 본 연구는 실세계 다중 작업 로봇 제어를 위한 종단간 학습된 비디오 표현의 적용 가능성을 보여준다. 본 연구의 주요 기여는 다음과 같다. (1) 로봇 또는 인간 에이전트 실시예(SSII)에 의해 입증된 비디오 작업 사양을 인코딩하기 위한 변압기 기반 정책을 제시한다. (2) 훈련 중 3개의 대조적 보조 손실을 이용하여 프롬프트 비디오와 로봇 비디오 표현 사이의 정렬을 유도한다 (3) 실제 로봇 실험을 통해, 본 논문에서 제안한 비디오 조건화 정책이 인간 프롬프트 비디오의 기준선보다 우수함을 보인다. 게다가, 우리의 정책은 교차 객체 움직임 전달(SSIII)에 더 우수하다.\n' +
      '\n' +
      '## II Approach\n' +
      '\n' +
      '### _Preliminaries_\n' +
      '\n' +
      '우리의 목표는 조작 작업의 _prompt video_를 취하고 비디오에서 시연된 작업을 수행하는 동작을 출력하는 로봇 시스템을 설계하는 것이다. 이 시스템은 (로봇과 다른 설정 또는 실시예를 가질 수 있는) 프롬프트 비디오로부터 기본 태스크를 추론하고, 이어서 추론된 태스크를 달성하기 위해 자신의 환경에서 오브젝트들을 조작할 필요가 있다. 구체적으로, 로봇 상태\\(S_{t}=\\{x_{i}\\}_{i=t-k-1}^{t}\\)가 주어지며, 여기서 \\(x_{i}\\)은 로봇의 카메라 스트림으로부터 시간\\(i\\), \\(k\\)은 최대 과거 프레임 수, \\(t\\)은 현재 시간 단계이다. 우리는 \\(V\\)에서 기본 작업을 추론하고 작업 관련 작업을 예측하는 정책 \\(\\pi(a_{t}|S_{t},V)\\)을 훈련한다. 이 모델을 훈련하려면 페어링된 프롬프트 비디오와 로봇 궤적의 데이터 세트가 필요하다. 아래에서 페어링된 데이터 세트를 만드는 방법에 대해 자세히 논의하겠습니다.\n' +
      '\n' +
      '### _Datasets_\n' +
      '\n' +
      '비디오-조건 로봇 정책을 훈련하기 위해 우리는 동일한 작업을 수행하는 프롬프트 비디오 및 로봇 궤적과 같은 쌍의 데이터 세트가 필요하다. 이 작업에서 우리는 인간과 로봇 모두에 의해 작업이 수행되는 프롬프트 비디오를 탐색한다. 이 데이터 세트를 생성하기 위해 우리는 세 가지 데이터 클래스에 의존한다:\n' +
      '\n' +
      '1. **로봇-로봇**: 동일한 작업의 기존 로봇-로봇 비디오를 페어링한다. 이 페어링의 경우 두 비디오가 다른 설정에서 동일한 작업을 수행하는 경우 일치하는 것으로 간주한다. 우리는 로봇 궤적을 기록할 때 사용되는 자연어 명령어를 기반으로 \'_task_\'를 정의한다. 이러한 명령어들은 전형적으로 \'_place_ 물병 직립\', \'_move_ the coke can to the green chip bag\' 또는 \'_open_ top drawer\'와 같은 명사로 둘러싸인 하나 또는 두 개의 동사로 구성된다. 이 페어링의 목적은 첫째, 이미 레이블이 지정되고 수집된 로봇 궤적 데이터 세트를 활용할 수 있고 둘째, 동일한 작업이 다른 환경에서 시연될 때 로봇이 모방할 수 있도록 하는 것이다.\n' +
      '2. **Hindsight Human-Robot**: 여기서 우리는 로봇 궤적 데이터 세트의 작업 지시를 사용하고 1~5명의 인간 참가자에게 작업을 수행하고 로봇의 관점/시야에서 시연 비디오를 녹화하도록 요청한다. 명령들의 세트는 이전과 동일하지만, 왼손 또는 오른손으로 그리고 무작위화된 로봇 카메라 앵글에서 작업을 수행하는 상이한 인간들로 인해 상당한 실시예 및 속도 가변성이 존재한다. 이것은 약간의 수동적인 노력이 필요하지만 새로운 로봇 궤적을 수집할 필요 없이 로봇 데이터 세트에서 사용 가능한 지침 세트에 대한 정책을 훈련하기 위한 많은 쌍을 이루는 데이터를 제공한다.\n' +
      '3. **Co-located Human-Robot** 이 경우, 인간과 로봇은 동일한 작업공간에서 동일한 작업을 수행한다. 이 접근법을 사용하여 소파가 있는 거실, 화이트보드가 있는 회의실, 장난감 도구가 있는 하드웨어 워크스테이션, 조리대가 있는 주방, 냉장고와 싱크대, 보관 용품 공간 등과 같은 다양한 공간에서 인간 시연과 로봇 궤적을 수집했다.\n' +
      '\n' +
      '우리는 그림 2에서 3개의 데이터 세트 각각에서 짝을 이루는 프롬프트와 로봇 비디오의 예를 보여준다. 볼 수 있듯이, Hindsight Human-Robot 및 Co-located Human-Robot 데이터 세트에서 배경과 산만 객체에 상당한 차이가 있다. 액터가 동일한 모폴로지를 갖는 로봇인 첫 번째 접근법(Robot-Robot)을 프롬프트 비디오에서 인간이 액터인 다른 두 경우와 비교할 때 다른 복잡성이 발생한다.\n' +
      '\n' +
      '모든 데이터 세트를 결합한 후, RT-1[7]과 RT-2[8]에 소개된 작업을 다루는 \\(\\sim\\)100k 로봇 비디오와 \\(\\sim\\)10k 인간 비디오를 가지고 있다. 세 가지 데이터 소스 모두의 비디오가 수집해야 하는 다양한 난이도와 비용을 나타내기 때문에 포함돼요. 기존 로봇 데이터 세트를 페어링하려면 추가 노력이 덜 필요하지만 작업이 수행되는 방법은 다양성이 부족하다. 두 번째 데이터 소스는 인간에게 기존의 로봇 궤적을 모방하도록 요청함으로써 생성된다. 이는 프롬프트 비디오에서 약간의 다양성을 추가하지만 로봇 측의 새로운 작업을 다루지 않습니다. 마지막으로, 추정된 금본위제는 인간과 로봇 모두가 같은 환경에 함께 위치하고 다양한 작업을 수행하는 데이터를 수집하는 것이다. 이는 원격 조작을 통해 수집된 인간과 로봇 궤적 모두 노동이 요구되기 때문에 가장 많은 시간이 소요된다.\n' +
      '\n' +
      '### _Model Architecture_\n' +
      '\n' +
      '우리의 정책은 프롬프트 비디오와 현재 로봇 상태를 입력으로 하고 로봇 동작을 출력한다. 그것은 (1) 프롬프트 비디오 인코더 (2) 로봇 상태 인코더, (3) 상태 프롬프트 인코더 및 (4) 로봇 액션 디코더의 네 개의 모듈로 구성된다. 전체 아키텍처는 그림 3에 설명되어 있으며 각 모듈은 아래에 자세히 설명되어 있다.\n' +
      '\n' +
      '**(1) 프롬프트 비디오 인코더**는 원하는 태스크 의미론을 전달하기 위해 참조로 제공된 비디오 데모를 인코딩한다. 프롬프트 비디오 인코더는 묵시적으로 어떤 작업이 수행되어야 하고 어떻게 수행되어야 하는지를 추론하도록 학습한다. 프롬프트 인코더는 프레임당 이미지 인코더\\(\\phi_{p}\\)(ViT[14])와 Perceiver Resampler[1,19]\\(\\psi_{p}\\)로 구성된다. 프롬프트 인코더 \\(\\psi_{p}(\\phi_{p}(V))=z_{prompt}\\)의 출력은 비디오로부터 태스크 관련 속성들로 정책을 조정하기 위한 d-dimension의 \\(N\\) 토큰들의 집합이다.\n' +
      '\n' +
      '**(2) 로봇 상태 인코더**는 현재 프레임과 마지막 \\(k\\) 프레임을 입력으로 하여 로봇의 현재 상태를 인코딩한다. 이 모듈은 또한 로봇의 객체 및 환경에 대한 정보를 인코딩한다는 점에 유의한다. 이 구조는 prompt encoder, 즉 per-frame Image encoder\\(\\phi_{s}\\)와 Perceiver Resampler\\(\\psi_{s}\\)와 유사하다. 즉시 인코더의 출력과 유사하게 상태 인코더의 출력은 최근 관측의 이력으로부터 잠재 환경과 로봇 상태 정보를 인코딩하는 \\(\\psi_{s}(\\phi_{s}(S_{t}))=z_{state}\\이다.\n' +
      '\n' +
      '우리는 (1)과 (2)에 대해 동일한 영상 부호화기 가중치, 즉 \\(\\phi_{p}\\!=\\!\\phi_{s}\\!=\\!\\phi\\을 사용한다. 영상 부호화기의 역할은 각 프레임에서 공간적인 시각 정보를 포착하는 것이다. Perceiver Resampler는 액션 디코더로 전달되어야 하는 비디오 토큰들의 수를 감소시킬 뿐만 아니라 프레임들에 걸쳐 시간적 학습을 가능하게 하는데 사용된다.\n' +
      '\n' +
      '**(3) 상태-프롬프트 인코더** 상태-프롬프트 인코더는 프롬프트 비디오 인코딩 \\(z_{prompt}\\) 및 로봇 상태 인코딩 \\(z_{state}\\)을 취하고 액션 예측 \\(z_{state|prompt}\\)에 관련된 태스크 인코딩을 출력한다. 모듈은 쿼리들로서 상태 인코딩과 키들 및 값들로서 프롬프트 비디오 인코딩 사이의 교차-어텐딩에 의해 로봇 액션들을 출력하도록 트레이닝된다. 직관적으로, 상태-프롬프트 인코더는 상태 및 프롬프트 정보의 융합을 가능하게 한다. 예를 들어, 프롬프트 비디오가 바구니에서 사과를 줍는 것을 보여주고 현재 상태가 사과, 바나나 및 오렌지를 포함하는 경우, 교차 주의\n' +
      '\n' +
      '도. 2: **데이터셋 생성. (상단 행) 여기서 우리는 로봇을 상단 서랍에 배치하기 위한 로봇-로봇 비디오 쌍을 보여준다. 동일한 작업을 수행하는 기존 로봇-로봇 비디오를 유사하게 페어링합니다. (중간줄) 여기 아래 서랍에서 콜라 캔을 따서 카운터_작업에 놓기 위한 힌두시 휴먼-로봇 페어링 비디오를 보여드립니다. 우리는 로봇 궤적의 작업 지침을 사용하고 인간 참가자에게 작업을 수행하고 로봇의 관점/시야에서 시연 비디오를 녹화하도록 요청한다. (아래 행) 여기서 우리는 도구kit_에 파이프 렌치를 배치하기 위한 Co-located Human-Robot 비디오 쌍을 보여준다. 우리는 동일한 작업 공간에서 인간의 시연과 로봇 원격 조작을 모두 기록합니다. 서로 다른 작업 공간을 사용하여 동일한 작업 지시를 수행할 수 있으므로 결국 시각적으로 다양한 프롬프트 및 로봇 상태 관찰과 쌍을 이룬다. 상태와 프롬프트 인코딩 사이의 §II-B.**에서의 보다 상세한 내용은, 액션 디코딩의 다음 단계에 중요한, 어떤 객체가 스테이트에서 참석할지에 대한 학습을 가능하게 한다. 우리는 상태-프롬프트 인코더의 출력을 프롬프트-인식 상태 토큰이라고 한다.\n' +
      '\n' +
      '**(4) 로봇 액션 디코더** 액션 디코더의 목표는 현재 상태 \\(S_{t}\\)에 대한 액션 벡터 \\(a_{t}\\)을 예측하여 프롬프트 비디오 \\(V_{p}\\)에 나타난 작업을 완료하는 것이다. 액션 디코더는 고정된 액션 위치 토큰[49]을 입력 쿼리로 사용하고 키와 값에 대한 프롬프트 인식 상태 토큰[z_{state|prompt}\\(z_{state|prompt}\\)을 사용하는 트랜스포머 디코더 구조이다. 액션 위치 임베딩의 크기는 \\(N\\times d\\)이고, \\(N\\)은 액션 차원 수이고 \\(d\\)은 트랜스포머 임베딩 차원이다. SSII-D의 액션 벡터에 대한 자세한 내용.\n' +
      '\n' +
      '액션 포지션 임베딩들은 타겟 비닝된 액션 값들을 출력으로서 예측하기 위해 프롬프트-인식 상태 토큰들에 교차-참석한다. 액션 디코더의 각각의 출력 토큰은 모드, 암 및 베이스에 대한 액션 디멘션에 대응한다. 구체적으로, 각각의 토큰 임베딩은 256차원으로 투영되고, 타겟 액션 벡터에 대응하는 빈을 획득하기 위해 상단에 소프트맥스 레이어가 적용된다. 추론 시 다수의 순방향 패스를 필요로 하는 자기회귀적 액션 디코딩을 사용하는 선행 작업 [7, 8]과 달리, ACT [49]와 같이 하나의 순방향 패스 예측을 위해 액션 위치 임베딩을 사용한다. 다음 타임스테프에 대해 하나의 행동을 예측하는 대신 [49, 21]에 설명된 접근법을 따르고 4단계의 예측 지평선으로 정책을 훈련한다. 우리는 실행을 위한 액션 값을 선택하기 위해 항상 가장 높은 확률, 즉 예측된 확률보다 argmax를 갖는 액션 빈을 사용한다.\n' +
      '\n' +
      '**Cross-Attention Layers.** Vid2Robot 아키텍처에서, 우리는 Cross-Attention Transformer 레이어를 광범위하게 사용한다. 이들은 프롬프트 리샘플러, 스테이트 리샘플러, 스테이트 프롬프트 인코더 및 액션 디코더와 같은 모듈들에서 사용된다. 우리는 크로스 어텐션 레이어가 프롬프트 비디오와 로봇 상태 비디오를 모두 처리할 때 높은 수의 토큰과 결과적인 큰 어텐션 매트릭스를 관리하는 데 도움이 된다는 것을 발견했다. 이는 표준 셀프-어텐션 계층들이 동일한 비디오를 프로세싱하기 위해 수십 배 더 많은 메모리를 필요로 할 것이기 때문이다. 예를 들어, ViT-B/16을 사용하는 경우, \\(16\\) 프레임 참조 비디오와 \\(8\\) 프레임 로봇 상태 비디오에 대해 \\(224\\times 224\\) 해상도의 총 비디오 토큰 수는 \\(8\\times 196+16\\times 196=4704\\)이 될 것이다. 이에 대한 완전한 자기 주의 동작은 \\(4704^{2}\\sim 22\\mathrm{M}\\) 엔트리를 갖는 주의 매트릭스로 이어질 것이다. 그러나 64개의 Latent를 가진 두 개의 Perceiver Resampler를 사용하여 8\\times 196\\times 64+16\\times 196\\times 64\\sim.3\\mathrm{M}\\의 주의행렬로 훈련할 수 있었다. 따라서, Vid2Robot에서 크로스 어텐션 레이어는 어텐션 계산을 줄이고 페어링된 비디오로 트레이닝을 가능하게 하는 데 중요한 역할을 한다.\n' +
      '\n' +
      '### _Preprocessing_\n' +
      '\n' +
      '효율적인 훈련을 위해 다양한 길이의 비디오를 처리하기 위해, 우리는 무작위로 \\(N\\!=\\!16\\)을 샘플링한다. 프레임은 항상 첫 번째 프레임과 마지막 프레임을 포함하고 시간이 증가함에 따라 정렬한다. 훈련 중에, 먼저 랜덤 타임스테프를 샘플링하여 로봇 상태\\(S_{t}\\)를 샘플링한다. 그런 다음 앞의 \\(k-1\\) 프레임을 선택하여 총 \\(k\\!=\\!8\\)으로 구성된 로봇 상태 비디오를 생성한다. 이전에는 프레임입니다. 현재 시간 단계 이전에 \\(k-1\\) 미만의 프레임이 있는 경우, 첫 번째 프레임을 반복하여 고정된 크기의 로봇 상태 비디오를 생성한다. 각 프레임 내의 픽셀 값들은 0에서 1 사이에서 정규화된다. 각 프레임은 \\((224,224)\\)으로 리사이징된다. 트레이닝 동안 크롭, 밝기, 대비, 색상 및 채도와 같은 광도 왜곡이 적용된다.\n' +
      '\n' +
      '액션 벡터는 베이스 변위 및 회전뿐만 아니라 모드, 그리퍼 포즈 및 폐쇄성을 나타내는 값으로 구성된다. 각 값은 서로 다른 범위를 가지며, 먼저 0과 1 사이의 값을 스케일링하기 위해 사용하고, 그 값을 각각 256개의 빈으로 이산화한다. 전체적으로 [0, 255] 사이의 값을 갖는 11차원 액션 벡터를 타겟으로 구성한다. 이 연구에서 우리는 기반이 정지 상태로 유지되는 시나리오에서 훈련하고 평가한다.\n' +
      '\n' +
      '### _Training_\n' +
      '\n' +
      '**행동 예측 손실** 행동 복제로 Vid2 로봇을 엔드 투 엔드 훈련합니다. 이 아이디어는 작업 동사와 사물을 인식하기 위해 원시 픽셀에서 비디오 표현을 학습하고 이를 달성하기 위해 모터 제어를 학습하는 것이다. 우리는 \\(N\\!=\\!256\\)으로 토큰화된 동작에 분류 손실을 사용한다. 쓰레기통\n' +
      '\n' +
      '도. 3: **건축. 우리의 모델은 프롬프트 비디오와 로봇의 현재 관찰의 입력 프레임으로 간주하고 이를 프롬프트 비디오와 로봇 상태 토큰 임베딩으로 인코딩한 다음 상태 프롬프트 인코더로 처리하고 현재 타임스텝을 위한 로봇 액션으로 디코딩한다. 현재의 시각적 관찰로 작업을 수행하기 위한 로봇 궤적을 고려할 때 §II-C.**에서 더 자세한 내용은 해당 전문가 액션 \\(x_{t}\\)을 가지고 있다. 액션 예측 손실은 다음과 같이 예측된 액션과 전문가 액션 사이의 크로스 엔트로피이다:\n' +
      '\n' +
      '\\[L_{CE}(a_{t},\\hat{a}_{t})=\\sum_{\\tau}a_{t}\\log\\hat{a}_{t} \\tag{1}\\]\n' +
      '\n' +
      '이것은 그림 3과 같이 모든 모델 매개변수를 훈련시킨다.\n' +
      '\n' +
      '**보조 손실.** 데이터셋 크기가 상당하지만 대형 트랜스포머 기반 모델을 학습하기에는 부족합니다. 훈련 세트에서 행동을 정확하게 예측하여 과적합을 방지하기 위해 신속한 비디오에서 의미 이해에 도움이 되는 학습 기능을 장려하는 세 가지 보조 손실을 추가한다.\n' +
      '\n' +
      '_Video Alignment Loss_: 동일한 작업을 수행하는 프롬프트 비디오와 로봇 비디오 사이의 시간적 정렬을 장려하고자 한다. 신속한 비디오 및 로봇 비디오를 정렬함으로써, 우리는 이미지 인코더가 작업 진행을 예측하는 것과 관련된 특징을 여전히 인코딩하면서 상이한 실시예, 조명, 배경, 뷰 앵글 및 산만 객체에 불변하도록 학습하기를 원한다. 손실 선택은 [17]에서 소개한 시간 주기 일관성 손실이다. 이 손실은 동일한 작업을 수행하는 상이한 에이전트의 비디오에 대해 훈련될 때 작업 진행을 인코딩하는 것으로 나타났다[47]. 이 손실은 훈련 중 프롬프트 \\(V_{p}\\) 및 로봇 \\(V_{r}\\) 비디오의 프레임당 이미지 임베딩에 적용된다. 손실을 적용하기 위해 이미지 인코더에서 공간 차원으로 출력되는 프레임당 임베딩을 평균 풀링하고 2-레이어 MLP의 프로젝터 헤드를 적용한다[10]. 우리는 이것을 그림 4와 같이 프레임당 이미지 임베딩에 _alignment pooling layer_\\(\\Phi\\)라고 부른다. 각 비디오 \\(V_{i}\\)에 대해, 이것은 임베딩 \\(E_{i}=\\{\\Phi(v_{i}^{1}),\\Phi(v_{i}^{2})...,\\Phi(v_{i}^{L_{i}})\\}\\, 여기서 \\(L_{i}\\)은 \\(i^{th}\\)의 길이이다.\n' +
      '\n' +
      '즉시영상과 로봇영상에 대해 각각 부호화(E_{p}\\)와 부호화(E_{r}\\)에 TCC 손실을 적용한다. TCC 손실에 대한 직관적인 아이디어는 \\(E_{p}\\)의 모든 프레임의 표현이 \\(E_{r}\\)으로 일치해야 하고 그 반대도 마찬가지라는 것이다. 이것은 두 단계로 나누어진다. 첫째, \\(E_{p}\\)의 (\\(E_{p}^{t}\\) 프레임 (\\(E_{p}^{t}\\)을 \\(E_{r}\\)으로 계산하고 이것을 \\(\\overrightarrow{E_{pr}^{t}\\)이라 한다.\n' +
      '\n' +
      '{E_{pr}^{t}}=\\sum_{k}^{L}}\\alpha_{k}E_{r}^{k},\\quad\\mathrm{where}\\frac{e^{-\\|E_{i}^{t}-E_{j}^{k}\\|^{2}}{\\sum_{l}^{i}-E_{j}^{k}\\|^{2}}\\end{split}\\tag{2}\\qad\\mathrm{where}\\frac{e^{-\\|E_{i}^{t}-E_{j}^{k}\\|^{2}}\\frac{e^{-\\|E_{i}^{j}}e^{-\\|E_{i}^{t}-E_{j}^{k}\\|^{2}}\n' +
      '\n' +
      '둘째, 새롭게 계산된 소프트 이웃에 대한 대응 프레임을 \\(E_{p}\\)으로 구한다. 이를 [17]에서 _cycle-back_이라 하며, \\(\\overrightarrow{E_{pr}^{t}}\\)을 구하기 위해서는 수학식 2와 유사한 연접 계산을 수반하며, 이는 이상적으로는 \\(t\\), 즉 \\((\\overrightarrow{E_{pr}^{t}-t)^{2}\\)과 같아야 한다. TCC 손실은 프롬프트 및 로봇 비디오 인코딩 및 그 반대, 즉, 모든 프레임 사이의 이러한 평균 제곱 오차를 최소화한다.\n' +
      '\n' +
      '{split} L_{TCC}(E_{p},E_{r})=\\sum_{t\\in V_{p}( \\overrightarrow{E_{pr}^{t}}-t}^{2}\\\\L_{TCC}=\\frac{L_{TCC}(E_{p},E_{r})+L_{TCC}(E_{r},E_{p}}{2}\\end{split}\\tag{3}\\tag{3}}\n' +
      '\n' +
      '_Prompt-Robot Video Contrastive Loss (VVCL)_: 동일한 작업을 수행하는 로봇 또는 프롬프트 비디오에 의해 생성된 프롬프트 토큰들 간에 대비 손실을 적용한다. 이러한 손실은 비디오 토큰으로부터 자기 감독 방식으로만 태스크 의미론을 학습하도록 신속한 인코딩을 유도한다. 여기서 주목할 점은 프롬프트와 로봇 비디오의 초기 페어링이 자연어를 사용하여 이루어졌다는 것이다. 그러나, 자기 감독 손실이 있는 비디오 임베딩에만 해석 손실을 사용함으로써,\n' +
      '\n' +
      '도. 4: **Training Setup.** Vid2Robot이 훈련된 모든 손실과 각 손실이 서로 다른 모듈에 어떻게 연결되는지 보여준다. (1) 메인 액션 예측 손실과 함께, (2) 시간적 비디오 정렬 손실, (3) 동일한 작업을 수행하는 프롬프트와 로봇 비디오 사이의 대조 손실, (4) 언어 임베딩을 갖는 프롬프트/로봇 비디오 사이의 대조 손실의 세 가지 보조 손실을 적용한다. 더 자세한 내용은 §II-E이다.\n' +
      '\n' +
      '우리는 짧은 자연 언어 임베딩 자체에 의해 커버되지 않는 특징들을 인코딩하기를 희망한다. 이러한 특징들의 예들은 물체들에 도달하고, 로봇 암을 회전시키는 것과 같은 유사한 동작들을 포함한다. 우리는 각 비디오에 대한 단일 임베딩을 생성하기 위해 \\(N\\) 프롬프트 토큰에서 피쳐를 병합하기 위해 Attention Pooling 레이어를 사용한다. 우리는 비디오-비디오 쌍 사이의 SigLIP[48] 손실을 적용하여 유사한 동작과 상호작용하는 객체를 포함하는 동일한 작업을 보여주는 비디오가 배치에서 다른 비디오와 떨어져 있는 동안 서로 가깝도록 유도한다. 배치에는 동일한 수의 로봇 비디오와 프롬프트 비디오가 포함됩니다. 우리는 전체 로봇 비디오 임베딩(Z_{robot}\\)과 프롬프트 비디오 임베딩(Z_{prompt}\\)의 배치를 얻기 위해 프롬프트 인코더\\(\\psi_{p}(\\phi(\\cdot))\\)를 사용한다. 우리는 \\(B\\times B\\) 행렬을 얻기 위해 \\(Z_{robot}\\cdot Z_{prompt}^{T}\\을 곱한다. 학습 가능한 온도\\(\\tau\\)와 바이어스\\(b\\)를 추가한 로짓행렬은 \\(\\hat{Y}=(Z_{robot}\\cdot Z_{prompt}^{T})*\\tau+b\\이다. 로봇 동영상과 같은 작업을 수행하는 프롬프트를 양성과 같이 고려하여 대각선을 따라 1, 대각선 이외 쌍에 대해 -1의 레이블, 즉 레이블 행렬 \\(Y=2\\mathrm{I}_{B}-1\\)을 할당한다. SigLIP 손실은 음의 로그우도이며, 구체적으로 \\(\\sigma^{\\prime}(Z_{1},Z_{2})=-\\sum\\log\\sigma(Y\\cdot(Z_{1}\\cdot Z_{2}^{T})*t+b)\\), 여기서 \\(Y=2\\mathrm{I}_{B}-1\\)이다. 그 후, 비디오-비디오 콘트라스트 손실은 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[L_{VVCL}=\\sigma^{\\prime}(Z_{prompt},Z_{robot}) \\tag{4}\\]\n' +
      '\n' +
      'VTCL(VVideo-text Contrastive Loss)_: 프롬프트 토큰(Z_{prompt}\\)과 로봇 비디오(Z_{robot}\\)에 의해 생성된 토큰(Z_{robot}\\) 및 태스크의 텍스트 명령어 임베딩(Z_{text}\\) 사이에 대비 손실을 추가한다. 이는 임베딩 공간의 일부가 프롬프트 및 로봇 비디오에 존재하는 객체 이름 및 동사를 인식하도록 유도한다. 이 손실의 버전은 보조 언어 회귀 손실로 BC-Z[21]에 의해 이전에 적용되었다. 우리는 각 비디오에 대한 단일 임베딩을 생성하기 위해 \\(N\\) 프롬프트 토큰에서 특징을 병합하기 위해 하나의 잠재 쿼리와 함께 Attention Pooling 계층[46]을 사용한다. 배치 내에서, 우리는 비디오와 텍스트 임베딩의 \\(B\\) 쌍을 검색한다. 수학식 4와 유사하게 SigLIP[48] 손실을 적용하여 구한다.\n' +
      '\n' +
      '\\[L_{VTCL}=\\frac{\\sigma^{\\prime}(Z_{prompt},Z_{text})+\\sigma^{\\prime}(Z_{robot},Z_{text})}{2} \\tag{5}\\]\n' +
      '\n' +
      '이것은 모든 비디오가 배치 내의 다른 비디오에 대응하는 텍스트 임베딩과 상이하면서, 그들의 텍스트 기술 임베딩과 유사한 임베딩을 갖도록 장려한다.\n' +
      '\n' +
      '전반적으로, 훈련에 대한 네 가지 손실의 평균을 \\(L=\\frac{1}{4}(L_{CE}+L_{TCC}+L_{VVCL}+L_{VTCL})\\)으로 적용한다.\n' +
      '\n' +
      '### _Implementation_\n' +
      '\n' +
      '우리는 200K 반복에 대해 (Jax에서 구현된) 모델을 훈련시켰다. 준비 단계 2,000과 최종 학습률 1e-6의 코사인 학습률 스케줄로 8e-5의 초기 학습률을 갖는 AdamW 최적화기를 사용하였으며, Prompt와 State Resampler는 64개의 Latent를 갖는 2개의 Perceiver Resampler 레이어를 사용하였다. 상태-프롬프트 인코더와 액션 디코더는 모두 4개의 레이어 딥 크로스-어텐션 트랜스포머이다.\n' +
      '\n' +
      '## III Experiments\n' +
      '\n' +
      '다중 작업 비디오 조건화 정책에 대한 실제 로봇 평가 결과를 제시한다. 우리가 이 작업에서 다루는 핵심 질문 중 하나는 로봇이 조작 작업을 수행하는 인간을 얼마나 잘 모방할 수 있는가이다. 실시예들의 차이 때문에, 인간은 상이한 속도 및 스타일로 조작 태스크들을 수행한다. 우리는 로봇과 인간 비디오를 프롬프트로 사용하는 효과를 연구합니다.\n' +
      '\n' +
      '**Metrics.** 정책으로부터 추론되고 초기 상태 관찰 및 프롬프트 비디오로부터 로봇 상에서 실행되는 액션들의 시퀀스로서 _rollout_를 언급하며, 정책이 종료되거나 또는 가장 낮은 단계들 중 어느 것이든 최대 수의 단계가 취해질 때까지이다. 정책이 프롬프트 비디오에 표시된 작업 명령을 실행할 때 롤아웃에 대한 _success_를 정의합니다. 성공적인 롤아웃은 재설정 또는 복구에 대한 도움 없이 환경에서 연속적으로 수행해야 하는 올바른 조치를 포함합니다. 각 작업 지침에 대해 정책당 많은 롤아웃을 기록합니다. 우리는 모든 롤아웃에 걸쳐 기록된 성공의 평균을 취하고 그것을 그 작업의 성공률이라고 부른다. 작업 전반에 걸친 집계 성공률을 _Overall_Success Rate라고 한다.\n' +
      '\n' +
      '롤아웃에서 초기에 이루어진 실수는 모델의 오프라인 전체 예측 정확도가 높더라도 성공률이 떨어질 수 있다. 예를 들어, 정책이 작업 초기에 물병을 파지하는 동안 오류가 발생하여 도달할 수 없는 위치로 미끄러지면 정책이 다음 단계에 대한 좋은 행동 예측을 가지고 있더라도 롤아웃은 실패로 표시된다. 롤아웃에 대한 부분적인 진행을 기록하기 위해 로봇이 올바른 위치에 _reached_, 올바른 객체를 _grasped_, 올바른 위치에서 객체를 _released_ 및 작업을 올바르게 종료했는지 여부에 주석을 달았다. SSIII-A의 부분 성공 분석에 대한 자세한 내용.\n' +
      '\n' +
      '**평가 설정** 인간 평가자에게 로봇에서의 정책 출시에 대한 성공을 평가해 달라고 요청합니다. 로봇, 조명 조건, 서랍의 상자 및 물체를 변화시켜 정책을 평가한다. 우리는 평가되는 정책이 롤아웃 동안 유사한 초기 객체 구성을 보여주도록 한다. 초기 상태는 주어진 초기 상태에 대해 모든 정책이 평가된 후 무작위화된다. 모든 롤아웃에 대해 훈련 중에 모델에서 볼 수 없는 프롬프트 비디오를 샘플링합니다. 이렇게 하면 정책이 새 프롬프트 비디오에서 작업을 인식할 수 있는지 여부에 대해 평가됩니다.\n' +
      '\n' +
      '**Baselines.** ResNet-18 인코더를 사용하여 비디오 조건화 정책인 BC-Z[21]와 모델을 비교한다. BC-Z[22]는 FiLM[36] 조건화된 ResNet 인코더를 통해 처리되고 로봇 동작을 예측하기 위해 ResNet 기반 정책 네트워크에 공급되는 시연 관찰 쌍을 포함한다. 공정한 비교를 위해 우리는 Vid2Robot 모델을 훈련하는 데 사용되는 것과 동일한 훈련 데이터로 BC-Z 모델을 훈련한다. 우리는 BC-Z에 종료 조치가 없기 때문에 고정된 최대 단계 수에 대해 BC-Z 정책 롤아웃을 실행합니다.\n' +
      '\n' +
      '**핵심 질문 및 결과** 이 작업에서 다음과 같은 질문을 해결합니다:\n' +
      '\n' +
      '1. 비디오-조건화된 정책들이 보이지 않는 비디오에서 태스크를 보여줄 때 얼마나 잘 수행되는가? (도 5, SS Ⅲ-A)2. 신속한 실시예 차이(로봇 v/s 인간)로 인한 성공률의 격차는 무엇인가? (SS III-A)\n' +
      '3. 학습된 움직임 표현을 분산 외 객체 상호작용에 활용할 수 있는가? (SS III-B)\n' +
      '\n' +
      '### _Task-based success_\n' +
      '\n' +
      'Vid2 로봇 모델과 기준 BC-Z를 표 I의 로봇 및 인간 프롬프트 비디오와 비교하며, Vid2 로봇과 BC-Z는 로봇-로봇과 인간-로봇 쌍을 이루는 데이터를 포함하는 동일한 데이터 혼합물에 대해 훈련되었다. 프롬프트 비디오는 훈련 작업의 하위 집합을 포함하지만 비디오 자체는 모델에 대해 새롭습니다. 이 평가에서는 로봇의 현재 상태뿐만 아니라 프롬프트 비디오로부터 태스크 명세를 추론하는 각 모델의 능력을 조사한다.\n' +
      '\n' +
      '실제 로봇에 대한 다양한 설정에서 모델의 성능을 테스트하기 위해 표 I과 같이 8가지 범주의 조작 작업에 대해 평가한다. 구체적으로, 9가지 작업에 대해 평가한다. \'녹 물병 위로\', \'크스바 초콜릿을 코크 캔 근처에 이동\', \'그린 할라페뇨 칩 백을 코크 캔 근처에 이동\', \'그린 쌀 칩 백을 선택\', \'플레이스 코크 캔을 직립\', \'아래 서랍과 카운터에 위치\', \'열린 중간 서랍\', \'닫힌 중간 서랍\', \'사과를 위 서랍에 배치\'이다.\n' +
      '\n' +
      '4명의 평가자가 신속한 비디오 데이터 세트 및 정책 설정(표 I의 행)을 위해 작업당 2번의 롤아웃을 수행하도록 요청하며, 이는 정책의 작업 성공률을 평가하기 위해 작업당 8번의 시도를 수행함을 의미한다. 우리는 과제당 8번의 시도, 즉 \\(9\\!\\times\\!8\\!=\\!72\\)을 통해 9개의 과제에서 행당 전체 성공률을 보고한다. 시련 표 I의 평가에는 모두 \\(72\\!\\times\\!4\\!=\\!288\\)이 필요했다. 진짜 로봇 롤아웃.\n' +
      '\n' +
      '#### Iv-A1 프롬프트 비디오의 실시예 차이로 인한 성공률의 격차는 무엇인가?\n' +
      '\n' +
      '로봇 및 인간 비디오로 프롬프트될 때 모델을 BC-Z와 비교한다. BC-Z는 비교를 위한 강력한 기준선 역할을 한다. 모델 Vid2Robot의 전체 성공률은 인간 프롬프트 비디오에서 BC-Z보다 20% 향상되었으며 로봇 프롬프트 비디오와 비슷합니다. 우리의 훈련 혼합물에는 인간 비디오보다 로봇 궤적에 대한 훈련 샘플이 수십 배 더 많다는 점에 유의해야 한다. 따라서 큰 격차가 없습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c} \\hline \\hline Prompter & Model & _pick_ & _pick-place on_ & _place into_ & _open_ & _close_ & _move near_ & _knock over_ & _place upright_ & Overall \\\\ \\hline \\multirow{3}{*}{Robot} & BC-Z & 75.0\\% & 50.0\\% & **61.5**\\% & 16.7\\% & 66.7\\% & **44.0**\\% & **58.3**\\% & **50.0**\\% & 52.6\\% \\\\  & Vid2Robot & 75.0\\% & **58.8**\\% & 50.0\\% & **91.7\\%** & **100.0**\\% & 33.3\\% & 41.7\\% & 16.7\\% & **54.9\\%** \\\\ \\hline \\multirow{3}{*}{Human} & BC-Z & 50.0\\% & 12.5\\% & 12.5\\% & 0.0\\% & 50.0\\% & 43.8\\% & 12.5\\% & **50.0**\\% & 30.6\\% \\\\  & Vid2Robot & **100.0**\\% & **50.0**\\% & **50.0**\\% & **62.5**\\% & **87.5**\\% & **43.8\\%** & **25.0\\%** & 12.5\\% & **52.8\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: Task Success Rate for Robot and Human prompts.\n' +
      '\n' +
      '도. 5: **정책 롤아웃. 각 행은 왼쪽에서 작업을 수행하는 사람의 프롬프트 비디오를 보여주고 오른쪽에서는 Vid2Robot을 사용하여 해당 성공적인 로봇 롤아웃을 보여준다. 프롬프트가 시각적으로 얼마나 다른지 주목하고, 정책 롤아웃은 다른 조명, 배경, 산만자 객체의 수 및 배치로 기록됩니다.** 로봇 프롬프트 비디오의 성능.** 인간 프롬프트 비디오의 경우, 우리의 모델은 대부분의 작업에서 BC-Z보다 뛰어나며, Vid2로봇이 프롬프트 비디오에서 작업의 의미를 기준선보다 더 잘 포착한다는 것을 보여준다. 우리의 모델은 서랍에서 따기, 카운터 위에 놓기, 서랍을 열거나 닫기 등의 작업에서 큰 여백만큼 능가합니다. 가장 어려운 작업은 업라이트 _lacing upright_와 _knocking over_이다. SSV Fig. 9의 고장 원인을 분석한다.\n' +
      '\n' +
      '비디오 조건화 정책은 보이지 않는 비디오에서 작업을 보여줄 때 얼마나 잘 수행됩니까?\n' +
      '\n' +
      '롤아웃을 성공으로 표시하는 것 외에도 롤아웃당 부분 성공 주석을 기록했다. 그림 6에서, 우리는 우리의 모델이 기준선에 비해 약 8% 더 많은 78%의 정확한 객체에 _reaches_라는 것을 관찰한다. 정책은 때때로 올바른 대상에 도달하지 못하고 대신 주의를 산만하게 한다. 다음으로, _grasping_ 오류는 특히 작고 변형 가능한 물체와 서랍 손잡이 또는 카운터 가장자리와 같은 충돌하기 쉬운 영역에서 발생한다. 여기서 우리의 모델(65%)은 20%의 큰 마진으로 BC-Z(45%)보다 우수하다. 성공적인 파악은 종종 롤아웃에서 가장 어려운 부분이며 성공을 위해 가장 중요합니다. 파악 후 대부분의 작업은 올바른 위치에서 _releasing_을 요구한다. 롤아웃 중 잘못된 릴리스로 인해 두 모델 모두 성공률이 약간 떨어집니다. BC-Z는 고정된 수의 단계를 실행하는 동안, 우리의 정책 Vid2Robot은 언제 종료할지를 예측한다. 우리는 _release_와 _terminate_의 비율이 거의 동일하다는 것을 관찰하며, 이는 우리의 모델에 대해 약 57%이며, 이는 정확한 위치에서 릴리즈한 후 Vid2Robot이 대부분 성공적으로 종료됨을 의미한다.\n' +
      '\n' +
      '### _Cross-object motion transfer_\n' +
      '\n' +
      '우리의 정책과 기준선은 SSII-B에서 논의된 바와 같이 쌍을 이루는 비디오로 훈련되었다. 이는 훈련 데이터가 프롬프트에 표시된 상호 작용 객체가 현재 로봇 관찰에 존재하는 시나리오만을 포함했음을 의미한다. 그러나 만약 우리가 한 물체에 대한 즉각적인 비디오를 제공하고 다른 물체에 대해 테스트한다면 어떨까? 프롬프트 비디오?_ 흥미롭게도, 우리는 우리의 모델이 기차 세트에서 보지 못한 물체에 대해 학습된 조작 동작을 수행한다는 것을 발견했다. 우리는 이 긴급한 행동을 _cross-object motion transfer_라고 부른다.\n' +
      '\n' +
      'Vid2Robot과 BC-Z를 비교하여 크로스 오브젝트 모션 전달 능력을 \'녹물병 오버\', \'녹색 쌀 칩 백 픽\', \'플레이스 코크 캔 직립\', \'아래 서랍과 카운터에서 콜크 캔 픽\', \'사과를 위 서랍에 위치\'의 5가지 프롬프트 비디오와 비교한다. 각 프롬프트 비디오는 로봇의 초기 관찰에서 관련 없는 객체로 평가된다. 평가에 사용되는 오브젝트는 \'_orange\'_, \'green can\'_, \'chips bag\', \'banana\', \'pink piggy soft toy\', \'wrist watch\'_이다. 성공하기 위해 서로 다른 파악이 필요한 상황을 평가하기 위해 다양한 모양, 크기 및 변형성을 갖도록 객체를 선택했다.\n' +
      '\n' +
      '평가 설정은 SSIII-A와 유사하다. 여기서 평가자는 작업을 위해 객체 중 하나를 설정하고 각 모델에 대한 롤아웃을 기록한다. 5개의 태스크에 대한 2개의 모델을 6개의 오브젝트와 비교하여, 모든 평가자는 \\(2\\!\\times\\!5\\!\\times\\!6\\!=\\!60\\)을 실행한다. 롤아웃. 우리는 4명의 평가자와 함께 평가를 반복하므로 표 II에 결과를 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline Model & _pick_ & _place on_ & _place_ & _place_ & _knock_ & \\\\  & _pick_ & _place on_ & _into_ & _upright_ & _over_ & Overall \\\\ \\hline BC-Z & 45.8\\% & 0.0\\% & 29.2\\% & 12.5\\% & 0.0\\% & 17.5\\% \\\\ Vid2Robot & 45.8\\% & **25.0\\%** & **54.2\\%** & **16.7\\%** & **29.2\\%** & **34.2\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: Cross-object motion transfer success.\n' +
      '\n' +
      '도. 6: **BC-Z 및 Vid2Robot.** 우리의 정책 Vid2Robot은 올바른 객체 _reaching_the correct object, _grasping_ it, _releasing_ it at the correct location, then _terminating_the episode correctly 측면에서 BC-Z를 능가한다. BC-Z에는 종료 컨트롤이 없습니다.\n' +
      '\n' +
      '도. 7: **객체간 움직임 전달에 대한 정성적 결과.** _placing coke can upright_의 프롬프트 비디오가 주어지면, 우리는 로봇 앞에 _green can_, _chips bag_, _staplet_ 및 _soft toy_로 정책을 롤아웃한다. 우리는 우리의 모델이 프롬프트 비디오에서 _place upright_의 움직임을 추론하고 다른 객체에 적용할 수 있음을 관찰한다. 녹색 캔의 선택이 다른 대상에 비해 보여준 것처럼 정책에는 실용주의라는 암묵적인 개념이 있다.\n' +
      '\n' +
      '(4{\\times}60=240\\).\n' +
      '\n' +
      'ⅲ-B1 한 물체에 대한 프롬프트 비디오를 제공하고 다른 물체에 대해 테스트할 수 있습니까? 정책이 프롬프트 비디오에 표시된 것과 동일한 동작을 수행합니까?\n' +
      '\n' +
      '그림 7에서 위의 실험 설정을 정성적으로 보여준다. 우리는 \'콜라 캔을 똑바로 놓으라\'는 즉각적인 비디오를 사용한다. 우리는 이 정책이 녹색 캔, 칩 봉지, 스테이플러, 부드러운 장난감과 같은 여러 물체에 \'직립 배치\'의 행동을 전달할 수 있다는 것을 관찰한다. 정책은 신속한 비디오를 준수하고 똑바로 배치하기 위해 칩 백 또는 바나나보다 녹색을 선택할 수 있음을 유의하십시오.\n' +
      '\n' +
      '정량적으로 우리는 표 II의 각 태스크와 같이 BC-Z가 교차=객체 움직임 전달을 테스트할 때 태스크를 성공적으로 완료할 수 없는 경우가 많다는 것을 관찰한다. 대조적으로, 본 모델(34%)은 이 설정에서 BC-Z(17%)보다 더 나은 성능을 수행하고 프롬프트 비디오에 표시된 동작을 수행한다. 이 모델은 BC-Z와 유사하며, 배포 중 아웃-피킹 객체에서 45%의 성공률을 보인다. 더 중요한 것은 서랍에 넣는 작업이 상당한 개선(\\(29\\%\\~54\\%\\))을 보인다는 것이다. 서랍에서 고르고 카운터에 놓고 노크하는 것과 같은 특정 작업의 경우 BC-Z는 전혀 수행할 수 없는 반면 Vid2로봇은 해당 시간의 작업\\(25\\%-29\\%\\)을 완료할 수 있다.\n' +
      '\n' +
      '### _Ablations_\n' +
      '\n' +
      'SSII-E에서는 행동 예측 손실과 세 가지 보조 손실을 제시하였다. 여기서는 이러한 추가 손실 함수가 전체 성공률에 미치는 역할을 분석한다. 우리는 (1) 보조 손실을 사용하지 않는 것과 (2) 보조 언어 손실을 추가하는 것의 영향을 조사한다.\n' +
      '\n' +
      '우리는 SSIII-A에 기술된 것과 유사한 과제, 즉 각 정책을 평가하기 위한 9개의 과제를 고려한다.\n' +
      '\n' +
      '3가지 모델 변형, 즉 원본 Vid2Robot, 비디오 텍스트 대조 손실(CL)이 없는 것과 동작 예측 손실만 있는 것이 있다. 우리는 3명의 인간 평가자에게 각각 2개의 롤아웃으로 각 모델 변형을 실행하도록 요청한다. 전체적으로 그림 8의 <3{\\times}3{\\times}9{\\times}2{=}162\\ 롤아웃 결과를 보고한다. 오차 막대는 각 모델 변형을 사용한 롤아웃에서 보고된 성공에 대한 표준 편차를 나타낸다.\n' +
      '\n' +
      '#### Iii-C1 보조 손실을 사용하지 않는 것의 영향은 무엇인가?\n' +
      '\n' +
      '우리는 행동 예측 손실(45%)만을 사용하는 것에 비해 보조 손실을 통해 표현 제약을 강화함으로써 모델(61%)의 성능이 크게 향상됨을 관찰한다. SSII-E에서 제안된 보조 손실의 중요성을 강조한다.\n' +
      '\n' +
      '#### Iii-C2 보조 언어 손실의 영향은 무엇인가?\n' +
      '\n' +
      'BC-Z는 정책을 조정하기 위한 비디오 표현을 개선하기 위해 언어 표현을 사용하도록 제안하였다. 우리는 비디오 텍스트 CL을 제외한 모든 손실로 훈련된 다른 변형과 정책을 비교한다. 우리는 언어 손실 사용 시 성공률에서 1-2%의 한계 개선만을 관찰한다. 이는 비디오 정렬과 비디오 대비 손실이 성능 향상에 크게 기여함을 의미한다. 우리의 결과는 사전 훈련된 언어 임베딩을 사용하는 보조 손실 없이 효과적인 비디오 표현이 학습될 수 있다는 유망한 증거가 되기를 바란다.\n' +
      '\n' +
      '## IV 관련 업무\n' +
      '\n' +
      '**로봇을 위한 작업 사양** 범용 로봇의 개발은 작업 사양을 효과적으로 접지하는 데 달려 있습니다. 비디오는 물리적 세계에서 무엇을 할 것인지뿐만 아니라 어떻게 할 것인지를 제공하는 조밀한 정보 원천이다. 최근 작업은 작업 사양[4, 23, 40]에 비디오를 사용했다. 또 다른 작업 라인은 비디오를 사용하여 세계 모델을 학습하여 미래의 시각적 관찰을 예측한다[29, 26, 9, 31, 15]. 언어[45, 7, 33, 34], 최종 목표 이미지[24, 6] 및 손으로 그린 입력[43] 등이 작업 명세를 위한 수단으로 제안되었지만, 신속한 비디오로부터의 학습은 이러한 접근법에 상보적이며 배치 시 새로운 조작 기술을 수행하기 위해 훈련된 폴리스의 신속한 적응을 위해 불가피하다.\n' +
      '\n' +
      '**인간 시연에서 학습** 인간이 다양한 작업을 수행하는 비디오가 인터넷을 확산함에 따라 여러 작업은 로봇 학습에 이 정보를 가장 잘 활용하는 방법을 해결하는 것을 목표로 한다. 로봇 대 인간 실시예의 차이는 상당한 도전을 제기하는데, 기존 접근법은 인간의 이미지를 로봇으로 변환하는 것[42]에서 에이전트-진단 표현[3]에 대한 인페인팅까지 다양하다. 많은 선행 연구들은 강화 학습(3, 27, 42)을 위한 보상 함수뿐만 아니라 손 포즈 추정 및 접촉 추적(4, 12, 37), 객체 중심 표현(38, 20)을 위해 기성 모델을 활용하는 것을 제안한다. 다른 방법들[32, 44, 4]은 다운스트림 운동 제어 태스크들의 학습을 가속화하기 위해 이 문제를 시각적 표현 학습으로 주조한다. 이러한 모듈식 학습 솔루션은 제한된 데이터 세트에서 잘 작동하지만 각 구성 요소의 합성 오류가 발생하기 쉬우므로 효율적으로 확장할 수 없다. 목표 조건 모방 학습[11, 41, 18, 13] 및 강화 학습[39, 35]을 위한 종단 간 훈련 접근 방식은 이러한 기술에 대한 유망한 대안이지만 이러한 결과는 시뮬레이션에 크게 제한되고 심-실제 격차로 인해 방해를 받았다. 대조적으로, 우리는 이것을 실제 로봇 평가와 함께 인간 비디오에서 종단 간 대규모 다중 작업 학습으로 다루기로 선택한다.\n' +
      '\n' +
      '**쌍체 시연을 통한 모방** 쌍체 프롬프트 비디오 및 로봇 궤적의 설정은 원샷 시각적 모방 문헌과 가장 유사하다. 많은 이전 작업은 쌍에 대한 액세스를 가정하며, 여기서 첫 번째는 수행할 작업의 시연으로 사용되며 두 번째는 에이전트의 관찰로 사용된다.\n' +
      '\n' +
      '도. 8: Vid2Robot에서 사용되는 보조적 손실에 대한 **절제. 모든 보조 손실(녹색, 왼쪽)이 있는 제안된 접근 방식을 BC-Z(주황색, 중간)에서 원래 제안된 언어 대조 손실이 없는 변형과 보조 손실이 없는 버전(파란색, 오른쪽)과 비교한다. (§III-C)** 일부 초기 작업 [16]은 조작 정책 네트워크를 조정하기 위해 시간적 컨볼루션 및 이웃 주의를 통해 시연 네트워크를 훈련하는 것을 제안했다. [11, 28, 20]과 같은 보다 최근의 접근법에서는 쌍체 시연 및 관찰이 변압기 정책을 훈련하는 데 사용되며, 종종 역동역학 예측[11] 또는 대조적 표현 학습[28]과 같은 추가 제약이 있다. 그러나 이러한 접근법은 시뮬레이션된 작업의 특정 세트에서 크게 평가되며 실제 로봇에서는 비교되지 않는다. 우리의 작업과 가장 유사한 것은 실제 로봇 작업에 대한 평가를 보고하는 BC-Z[22]이다. 우리의 설정은 이러한 종래 기술의 일부와 유사하지만, 우리의 모델 Vid2로봇은 작업을 보여주는 사람을 모방하는 조작 정책을 배우기 위해 대형 이미지 인코더, 교차 주의 레이어 및 대비 보조 손실을 결합한다.\n' +
      '\n' +
      '## V 한계와 미래 방향\n' +
      '\n' +
      'SSIII에서는 이전 작업보다 접근 방식이 개선되었지만 비디오 조건 정책에 대한 성능 차이가 있음을 보여준다. [8]과 같은 언어 조건화 정책은 훈련을 위한 수백 개의 원격 조작 궤적을 가진 알려진 작업 세트에 대해 더 높은 성공을 보여준다. 반면에 우리는 유사한 설정에서 비디오 조건 정책을 평가하는 첫 번째 이정표를 달성한다. 우리는 우리의 작업의 세 가지 한계에 대해 논의하고 향후 방향에 대한 통찰력을 제공한다.\n' +
      '\n' +
      '먼저 정책 출시에 실패한 몇 가지 이유를 정성적으로 조사한다. 그림 9에서 우리는 자체 폐색, 파지 오류 및 산만기의 존재가 롤아웃 동안 고장으로 이어질 수 있는 방법을 보여주는 3가지 예를 설명하고 설명한다. 둘째, 그림 6에서 파지 성공의 현저한 감소를 관찰한다. 로봇 카메라 관찰을 사용하여 상태를 추정하고 암묵적으로 깊이 추정을 학습하는 동안, 폐색이 있거나 로봇 그리퍼가 카메라 시야에서 벗어날 때 종종 불완전하다. 멀티모달 센서 융합으로 상태 정보를 향상시킴으로써 파지 성공률을 향상시킬 수 있다. 셋째, SSII-B와 같이 세 가지 다른 소스에서 수집한 짧은 작업 지시 시범을 고려하며, 모두 5~20초 비디오이다. 온라인에서 긴 수평선 시연 또는 \'야생\' 비디오에서 모델을 테스트하려면 비디오를 위한 효과적인 페어링 전략과 정책을 훈련하기 위한 몇 가지 해당 로봇 궤적이 필요하다.\n' +
      '\n' +
      '## VI Conclusion\n' +
      '\n' +
      '본 논문에서는 비디오 컨디셔닝 스킬 학습을 위한 데이터 수집과 모델링을 위한 새로운 방법을 제시한다. 이러한 기술은 즉시 명백한 객체가 보이지 않을 때 새로운 객체 구성과 더 추상화된 동사 의미로 일반화된다. 우리 모델이 제공하는 기술과 일반성은 로봇이 접근할 수 있는 기술 세트를 넓히고 그렇지 않으면 쉽게 얻을 수 없는 기술을 포함하도록 다른 접근법을 보완한다. 미래의 작업은 이러한 학습된 프리미티브를 활용하여 새로운 작업 계획을 실행할 수 있다. 우리는 우리의 교차 객체 모션 전달 실험이 부트스트래핑 데이터 수집을 위한 새로운 객체와 설정으로 모션을 전달하고 새로운 기술에 빠르게 적응하는 인간-로봇 상호작용을 가능하게 하는 추가 연구를 장려하기를 바란다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '텔레오퍼레이션을 통해 데이터 수집을 지원해준 얀송팡, 그레시아 살라자르, 우츠바 말라, 딕샤 만주나트, 조넬 퀴얌바오, 사라 응우옌, 샹게타 라메쉬, 트란 팜, 사무엘 완, 토마스 잭슨, 조딜린 페랄타, 셀레스트 바라자스, 엘리오 프라도, 로셸 델라 크루즈, 알렉스 루옹, 크리스타 레이만에게 감사드린다. 조넬 키암바오, 그레시아 살라자르, 우타브 말라, 딕샤 만주나스\n' +
      '\n' +
      '도. 9: **정책 롤아웃을 사용한 고장 분석.**(상단) 정책은 그리퍼 포즈를 예측하고 암을 이동시키기 위해 IK 솔버에 의존한다. 때때로, IK 솔루션은 로봇의 카메라 시야를 차단할 수 있습니다. (중간) 파지 실패는 특히 투명하고 변형 가능한 물체들에서 발생한다. (하단) 디스트랙터 오브젝트와 조명 및 배경의 차이는 인식 오류를 유발할 수 있으며, 여기서 정책은 올바른 동작을 수행하지만 잘못된 오브젝트로 수행할 수 있다.\n' +
      '\n' +
      '로봇에 대한 평가로는 사라 응우옌, 상어타 라메쉬, 자스피어 싱; 정책 평가 인프라로는 마이클 안, 안토니 브로한, 키어타나 고팔라크리쉬난; 도움이 되는 토론으로는 수닐 벨케일, 도르사 새디, 첼시 핀, 세르게이 레빈; 글에 대한 철저한 피드백을 위해 조나단 톰슨, 빈센트 바누크. 이 작품은 또한 카네기 멜론 대학의 구글 로보틱스 펀딩의 지원을 받았다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. 하손기 Lenc, A. Mensch, K 밀리컨, M. 레이놀즈, R 링, E. 러더포드, S. 카비태 한진 공성호 사만귀에 몬테이로, J. 메닉, S. Borgeaud, A. Brock, A. Nematadeh, S. 샤리프자데 빈코스키, R 바레이라, 오 빈얼스, A. 지서먼, K. Simonyan(2022) Flamingo: 수 샷 학습을 위한 시각적 언어 모델. 앨리스 H. 오, A. 아가왈, D. 벨그레이브, K. Cho, Editor, Advances in Neural Information Processing Systems, pp. 외부 링크: 문서, 인용 링크: SSII입니다.\n' +
      '*[2]M. 곤잘레스 아레나스 샤오상 싱브이 제인 A. Z. 렌 Q. Vuong, J. 발리, A. Herzog, I. Leal, S. 키르마니, D. 새디, V. 신드환이 Rao, J. Liang, and A. Zeng (2023) How to prompt your robot: a prompt-book for manipulation skills with code as policy. 2차 언어 및 로봇 학습 워크샵: 접지로 언어, 외부 링크: 인용 링크: SSII.\n' +
      '*[3]S. 발란 멘돈카 L. 천욱 Jain, and D. Pathak (2023) Affordances from human video as a versatile representation for robotics. 컴퓨터 비전 및 패턴 인식에서 외부 링크: 인용된 링크: SSII.\n' +
      '*[4]S. 발란 젤러스, R. L. 브라스, J. 가오, Y. 최(2020) PIQA: 자연어로 물리적 상식에 대한 추론. 인공 지능에 관한 34번째 AAAI 회의에서 외부 링크: 인용된 링크: SSII.\n' +
      '*[5]S. 발란 젤러스, R. L. 브라스, J. 가오, Y. 최(2020) PIQA: 자연어로 물리적 상식에 대한 추론. 인공 지능에 관한 34번째 AAAI 회의에서 외부 링크: 인용된 링크: SSII.\n' +
      '*[6]K. Bousmalis, G. Vezzani, D. Rao, C. Devin, A. X. Lee, M. 바우자, T 다브초프 저우, A. 굽타, A. S. 라주, A. 로렌스, C. 판타치, V. 달리바르 잠벨리 머틴스 피버시뷰티, M. 블록질 데닐 배첼러, T 람페, E 파리오토, K 졸나, 세이 리드, 세이 Gomez Colmenarejo, J. Scholz, A. Abdolmaleki, D. Groth, J. Regli, O. O. Sushkov, T. 로호드, 제이 엔리케 첸, Y. Aytar, D. Barker, J. Ortiz, M. A. Riedmiller, J. Tobias Springenberg, R. Hadsell, F. Nori, N. Manfred Otto Heess (2023) Robocat: 로봇 조작을 위한 자기 개선 일반주의자 에이전트. 기계 학습 연구에 대한 트랜잭션에서 외부 링크: 인용된 링크: SSII.\n' +
      '*[7]A. 브로한남 브라운, J. 카르바할, Y. 체보타르, J. 다비스, C. 핀, K. 고팔라크리쉬난 Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. 잭슨 제스월 N.J. 조시, R. 줄리안, D. 칼라쉬니코프, Y. 광일열 이승환 레빈영 루우 Malla, D. Manjunath, I. Mordatch, O. 나첨, C. 파라다, J. 페랄타, E. 페레즈, K. 퍼치, J. 퀴암바오, K. 라오만 류지살라자르 P. 산케티 세이드, J. 싱, S. 손탁케, A. Stone, C. Tan, H. Tran, V. S. 바누크 베가, Q 부엉, F.샤, T. 샤오평수 서태호 Yu, and B. Zitkovich (2022) RT-1: Scale of Real-World Control용 Robotics transformer. ArXiv 프리프린트에서 arXiv:2212.06817, 인용: SSII.\n' +
      '*[8]A. 브로한남 브라운, J. 카르바할, Y. 체보타르, J. 다비스, C. 핀, K. 고팔라크리쉬난 Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. 잭슨 제스월 N.J. 조시, R. 줄리안, D. 칼라쉬니코프, Y. 광일열 이승환 레빈영 루우 Malla, D. Manjunath, I. Mordatch, O. 나첨, C. 파라다, J. 페랄타, E. 페레즈, K. 퍼치, J. 퀴암바오, K. 라오만 류지살라자르 P. 산케티 세이드, J. 싱, S. 손탁케, A. Stone, C. Tan, H. Tran, V. S. 바누크 베가, Q 부엉, F.샤, T. 샤오평수 서태호 Yu, and B. Zitkovich (2022) RT-1: 로보틱스 트랜스포머 for Real-World control at scale. ArXiv 프리프린트에서 arXiv:2212.06817, 인용: SSII.\n' +
      '*[9]A. 브로한남 브라운, J. 카르바할, Y. 체보타르, J. 다비스, C. 핀, K. 고팔라크리쉬난 Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. 잭슨 제스월 N.J. 조시, R. 줄리안, D. 칼라쉬니코프, Y. 광일열 이승환 레빈영 루우 Malla, D. Manjunath, I. Mordatch, O. 나첨, C. 파라다, J. 페랄타, E. 페레즈, K. 퍼치, J. 퀴암바오, K. 라오만 류지살라자르 P. 산케티 세이드, J. 싱, S. 손탁케, A. Stone, C. Tan, H. Tran, V. S. 바누크 베가, Q 부엉, F.샤, T. 샤오평수 서태호 Yu, and B. Zitkovich (2022) RT-1: 로보틱스 트랜스포머 for Real-World control at scale. ArXiv 프리프린트에서 arXiv:2212.06817, 인용: SSII.\n' +
      '*[10]A. 브로한남 브라운, J. 카르바할, Y. 체보타르, J. 다비스, C. 핀, K. 고팔라크리쉬난 Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. 잭슨 제스월 N.J. 조시, R. 줄리안, D. 칼라쉬니코프, Y. 광일열 이승환 레빈영 루우 Malla, D. Manjunath, I. Mordatch, O. 나첨, C. 파라다, J. 페랄타, E. 페레즈, K. 퍼치, J. 퀴암바오, K. 라오만 류지살라자르 P. 산케티 세이드, J. 싱, S. 손탁케, A. Stone, C. Tan, H. Tran, V. S. 바누크 베가, Q 부엉, F.샤, T. 샤오평수 서태호 Yu, and B. Zitkovich (2022) RT-1: 로보틱스 트랜스포머 for Real-World control at scale. ArXiv 프리프린트에서 arXiv:2212.06817, 인용: SSII.\n' +
      '*[11]A. 브로한남 브라운, J. 카르바할, Y. 체보타르, J. 다비스, C. 핀, K. 고팔라크리쉬난 Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. 잭슨 제스월 N.J. 조시, R. 줄리안, D. 칼라쉬니코프, Y. 광일열 이승환 레빈영 루우 Malla, D. Manjunath, I. Mordatch, O. 나첨, C. 파라다, J. 페랄타, E. 페레즈, K. 퍼치, J. 퀴암바오, K. 라오만 류지살라자르 P. 산케티 세이드, J. 싱, S. 손탁케, A. Stone, C. Tan, H. Tran, V. S. 바누크 베가, Q 부엉, F.샤, T. 샤오평수 서태호 Yu, and B. Zitkovich (2022) RT-1: 로보틱스 트랜스포머 for Real-World control at scale. ArXiv 프리프린트에서 arXiv:2212.06817, 인용: SSII.\n' +
      '*[12]A. 브로한남 브라운, J. 카르바할, Y. 체보타르, J. 다비스, C. 핀, K. 고팔라크리쉬난 Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. 잭슨 제스월 N.J. 조시, R. 줄리안, D. 칼라쉬니코프, Y. 광일열 이희희 레빈영 루우 Malla, D. Manjunath, I. Mordatch, O. 나첨, C. 파라다, J. 페랄타, E. 페레즈, K. 퍼치, J. 퀴암바오, K. 라오만 류지살라자르 P. 산케티 세이드, J. 싱, S. 손탁케, A. Stone, C. Tan, H. Tran, V. S. 바누크 베가, Q 부엉, F.샤, T. 샤오평수 서태호 Yu, and B. Zitkovich (2022) RT-1: 로보틱스 트랜스포머 for Real-World control at scale. ArXiv 프리프린트에서 arXiv:2212.06817, 인용: SSII.\n' +
      '*[13]A. 브로한남 브라운, J. 카르바할, Y. 체보타르, J. 다비스, C. 핀, K. 고팔라크리쉬난 Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. 잭슨 제스월 N.J. 조시, R. 줄리안, D. 칼라쉬니코프, Y. 광일열 이희희 레빈영 루우 Malla, D. Manjunath, I. Mordatch, O. 나첨, C. 파라다, J. 페랄타, E. 페레즈, K. 퍼치, J. 퀴암바오, K. 라오만 류지살라자르 P. 산케티 세이드, J. 싱, S. 손탁케, A. Stone, C. Tan, H. Tran, V. S. 바누크 베가, Q 부엉, F.샤, T. 샤오평수 서태호 Yu, and B. Zitkovich (2022) RT-1: 로보틱스 트랜스포머 for Real-World control at scale. ArXiv 프리프린트에서 arXiv:2212.06817, 인용: SSII.\n' +
      '*[14]A. 브로한남 브라운, J. 카르바할, Y. 체보타르, J. 다비스, C. 핀, K. 고팔라크리쉬난 Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. 잭슨 제스월 N.J. 조시, R. 줄리안, D. 칼라쉬니코프, Y. 광일열 이희희 레빈영 루우 Malla, D. Manjunath, I. Mordatch, O. 나첨, C. 파라다, J. 페랄타, E. 페레즈, K. 퍼치, J. 퀴암바오, K. 라오만 류지살라자르 P. 산케티 세이드, J. 싱, S. 손탁케, A. Stone, C. Tan, H. Tran, V. S. 바누크 베가, Q 부엉, F.샤, T. 샤오평수 서태호 Yu, and B. Zitkovich (2022) RT-1: 로보틱스 트랜스포머 for Real-World control at scale. ArXiv 프리프린트에서 arXiv:2212.06817, 인용: SSII.\n' +
      '*[15]A. 브로한남 브라운, J. 카르바할, Y. 체보타르, J. 다비스, C. 핀, K. 고팔라크리쉬난 Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. 잭슨 제스월 N.J. 조시, R. 줄리안, D. 칼라쉬니코프, Y. 광일열 이희희 레빈영 루우 Malla, D. Manjunath, I. Mordatch, O. 나첨, C. 파라다, J. 페랄타, E. 페레즈, K. 퍼치, J. 퀴암바오, K. 라오만 Ryoo, G. Salazar, P. Sanketi, K Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Stefan Welker, Paul Wohan Wahid, Stefan Wu, Fei Xia, Ted Xiao, Peng Xu, Tianhe Yu, Brianna Zitkovich. RT-2: Vision-Language-Action Models은 웹 지식을 로봇 제어로 전달한다. _arXiv preprint arXiv:2307.15818_, 2023.\n' +
      '* [9] Elliot Chane-Sane, Cordelia Schmid, and Ivan Laptev. Goal-conditioned reinforcement learning with imagined subgoals. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 1430-1440. PMLR, 18-24 Jul 2021. URL [https://proceedings.mlr.press/v139/chane-sane21a.html](https://proceedings.mlr.press/v139/chane-sane21a.html).\n' +
      '* [10] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-supervised models are strong semi-supervised learners. _Advances in neural information processing systems_, 33:22243-22255, 2020.\n' +
      '* [11] Sudeep Dasari and Abhinav Gupta. Transformers for one-shot visual imitation. In _Conference on Robot Learning_, pages 2071-2084. PMLR, 2021.\n' +
      '* [12] Eadom Dessalene, Chinmaya Devaraj, Michael Maynord, Cornelia Fermuller, and Yiannis Aloimonos. Forecasting action through contact representations from first person video. _IEEE Trans. Pattern Anal. Mach. Intell._, 45(6):6703-6714, June 2023.\n' +
      '* [13] Yiming Ding, Carlos Florensa, Mariano Phielipp, and Pieter Abbeel. Goal-conditioned imitation learning. In _Proceedings of the 33rd International Conference on Neural Information Processing Systems_, pages 15324-15335. Curran Associates Inc., Red Hook, NY, USA, December 2019.\n' +
      '* [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=YicbFdNTTY](https://openreview.net/forum?id=YicbFdNTTY).\n' +
      '* [15] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL [https://openreview.net/forum?id=b08a5MRcwy](https://openreview.net/forum?id=b08a5MRcwy).\n' +
      '* [16] Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [17] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. Temporal Cycle-Consistency learning. In _Computer Vision and Pattern Recognition_, 2019.\n' +
      '* [18] Oliver Groth, Chia-Man Hung, Andrea Vedaldi, and Ingmar Posner. Goal-Conditioned End-to-End visuomotor control for versatile skill primitives. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 1319-1325. IEEE, May 2021.\n' +
      '* [19] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J Henaff, Matthew Botvinick, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver IO: A general architecture for structured inputs & outputs. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=fILj7Wpl-g](https://openreview.net/forum?id=fILj7Wpl-g).\n' +
      '* [20] Vidhi Jain, Yixin Lin, Eric Undersander, Yonatan Bisk, and Akshara Rai. Transformers are adaptable task planners. In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, _Proceedings of The 6th Conference on Robot Learning_, volume 205 of _Proceedings of Machine Learning Research_, pages 1011-1037. PMLR, 14-18 Dec 2023. URL [https://proceedings.mlr.press/v205/jain23a.html](https://proceedings.mlr.press/v205/jain23a.html).\n' +
      '* [21] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning. In _Proceedings of the 5th Conference on Robot Learning_, pages 991-1002, 2022.\n' +
      '* [22] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. BC-z: Zero-shot task generalization with robotic imitation learning. In _Conference on Robot Learning_, pages 991-1002. PMLR, 2022.\n' +
      '* [23] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. VIMA: General robot manipulation with multimodal prompts. In _Fortieth International Conference on Machine Learning_, 2023.\n' +
      '* [24] Jacob Krantz, Theophile Gervet, Karmesh Yadav, Austin Wang, Chris Paxton, Roozbeh Mottaghi, Dhruv Batra, Jitendra Malik, Stefan Lee, and Devendra Singh Chaplot. Navigating to objects specified by images. April 2023.\n' +
      '* [25] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, 2023.\n' +
      '* [26] Yuxuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. In _2018 IEEE International Conference on Robotics and Automation (ICRA)_, pages 1118-1125. IEEE, May 2018.\n' +
      '\n' +
      '* [27] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. VIP: Towards universal visual reward and representation via Value-Implicit Pre-Training. In _International Conference on Learning Representations_, 2023.\n' +
      '* [28] Zhao Mandi, Fangchen Liu, Kimin Lee, and Pieter Abbeel. Towards more generalizable one-shot visual imitation learning. In _2022 International Conference on Robotics and Automation (ICRA)_, pages 2434-2444. IEEE, 2022.\n' +
      '* [29] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos. 2023.\n' +
      '* [30] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapasawi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In _ICCV_, 2019.\n' +
      '* [31] Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. In _NeurIPS_, July 2018.\n' +
      '* [32] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot manipulation. In _6th Annual Conference on Robot Learning_, 2022. URL [https://openreview.net/forum?id=tGbpgzGyOrI](https://openreview.net/forum?id=tGbpgzGyOrI).\n' +
      '* [33] Open X-Embodiment Collaboration, Abhishek Padalkar, Acorn Pooley, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Animesh Garg, Anthony Brohan, Antonin Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Scholkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue Zhou, Gaurav S Sukhatme, Gautam Salhotra, Ge Yan, Giulio Schiavi, Gregory Kahn, Hao Su, Hao-Shu Fang, Haochen Shi, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Joseph J Lim, Joao Silverio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, Krishnan Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Max Spero, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafaullah, Oier Mees, Oliver Kroemer, Pannag R Sanketi, Paul Wohlhart, Peng Xu, Pierre Sermanet, Priya Sundaresan, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martin-Martin, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Beklhale, Takayuki Osa, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, and Zichen Jeff Cui. Open X-Embodiment: Robotic learning datasets and RT-X models. _arXiv 2310.08864_, 2023.\n' +
      '* [4] Priyam Parashar, Vidhi Jain, Xiaohan Zhang, Jay Vakil, Sam Powers, Yonatan Bisk, and Chris Paxton. Spatial-Language Attention Policies for Efficient Robot Learning. In _Conference on Robot Learning_, 2023. URL [https://arxiv.org/abs/2304.11235](https://arxiv.org/abs/2304.11235).\n' +
      '* [5] Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Abbeel, and Sergey Levine. SFV: reinforcement learning of physical skills from videos. _ACM Trans. Graph._, 37(6):1-14, December 2018.\n' +
      '* [6] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In _Proceedings of the AAAI conference on artificial intelligence_, 2018.\n' +
      '* [7] Vladimir Petrik, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Learning object manipulation skills via approximate state estimation from real videos. In Jens Kober, Fabio Ramos, and Claire Tomlin, editors, _Proceedings of the 2020 Conference on Robot Learning_, volume 155 of _Proceedings of Machine Learning Research_, pages 296-312. PMLR, 2021.\n' +
      '* [8] Soren Pirk, Mohi Khansari, Yunfei Bai, Corey Lynch, and Pierre Sermanet. Online object representations with contrastive learning. 2019.\n' +
      '* [9] Karl Schmeckepper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine, and Chelsea Finn. Reinforcement learning with videos: Combining offline observations with interaction. November 2020.\n' +
      '* [10] Rutav Shah, Roberto Martin-Martin, and Yuke Zhu. MUTEX: Learning unified policies from multimodal task specifications. In _7th Annual Conference on Robot Learning_, 2023.\n' +
      '* [11] Pratyusha Sharma, Deepak Pathak, and Abhinav Gupta.\n' +
      '\n' +
      '비결합 계층 제어기를 통한 3인칭 시각 모방 학습 Adv. 신경 정보요 처리. Syst._ 32, 2019년\n' +
      '* [42] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter Abbeel, and Sergey Levine. AVID: Learning Multi-Stage tasks via Pixel-Level translation of human videos. In _Robotics: Science and Systems (RSS)_, December 2020.\n' +
      '* [43] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, Chelsea Finn, and Karol Hausman. Open-World object manipulation using pre-trained Vision-Language models. March 2023.\n' +
      '* [44] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. _arXiv preprint arXiv:2203.06173_, 2023.\n' +
      '* [45] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, Zsolt Kira, Manolis Savva, Angel Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, and Chris Paxton. HomeRobot: Open-Vocabulary Mobile Manipulation. In _Conference on Robot Learning_, 2023. URL [https://arxiv.org/abs/2306.11565](https://arxiv.org/abs/2306.11565).\n' +
      '* [46] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. _arXiv preprint arXiv:2205. 01917_, 2022.\n' +
      '* [47] Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tomppson, Jeannette Bohg, and Debidatta Dwibedi. Xirl: Cross-embodiment inverse reinforcement learning. In _Conference on Robot Learning_, pages 537-546. PMLR, 2022.\n' +
      '* [48] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In _2023 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 11941-11952, Los Alamitos, CA, USA, oct 2023. IEEE Computer Society. doi: 10.1109/ICCV51070.2023.01100.\n' +
      '* [49] Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning Fine-Grained bimanual manipulation with Low-Cost hardware. In _Robotics: Science and Systems_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>