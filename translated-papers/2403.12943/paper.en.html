<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers\n' +
      '\n' +
      'Vidhi Jain\\({}^{1,2}\\) Maria Attarian\\({}^{1,3}\\) Nikhil J Joshi\\({}^{1}\\) Ayzaan Wahid\\({}^{1}\\) Danny Driess\\({}^{1}\\) Quan Vuong\\({}^{1}\\) Pannag R Sanketi\\({}^{1}\\)\n' +
      '\n' +
      'Pierre Sermanet\\({}^{1}\\) Stefan Welker\\({}^{1}\\) Christine Chan\\({}^{1}\\) Igor Gilitschenski\\({}^{3}\\) Yonatan Bisk\\({}^{2}\\) Debidatta Dwibedi\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Google DeepMind Robotics \\({}^{2}\\)Carnegie Mellon University \\({}^{3}\\)University of Toronto\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'While large-scale robotic systems typically rely on textual instructions for tasks, this work explores a different approach: can robots infer the task directly from observing humans? This shift necessitates the robot\'s ability to decode human intent and translate it into executable actions within its own physical constraints and environment.\n' +
      '\n' +
      'We introduce Vid2Robot, a novel end-to-end video-based learning framework for robots. Given a video demonstration of a manipulation task and current visual observations, Vid2Robot directly produces robot actions. This is achieved through a unified representation model trained on a large dataset of human video and robot trajectory. The model leverages cross-attention mechanisms to fuse prompt video features to the robot\'s current state and generate appropriate actions that mimic the observed task. To further improve policy performance, we propose auxiliary contrastive losses that enhance the alignment between human and robot video representations.\n' +
      '\n' +
      'We evaluate Vid2Robot on real-world robots, demonstrating a 20% improvement in performance compared to other video-conditioned policies when using human demonstration videos. Additionally, our model exhibits emergent capabilities, such as successfully transferring observed motions from one object to another, and long-horizon composition, thus showcasing its potential for real-world applications. Project website: vid2robot.github.io.\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      'The path to creating versatile robots that provide assistance in people\'s daily routines requires them to learn new skills on-the-go. These range from small preferences like which brand of dishwasher a specific household uses to entirely different ways to clean the house. For known skills, humans simply communicate in natural language, but when nuance is required or a skill is novel, we revert to demonstrations. For example, we might show how a particular microwave works or how we prefer our cabinets to be organized. To enable seamless robot deployment, robots need the same ability for generalization from demonstration for learning new policies that comes so naturally to humans.\n' +
      '\n' +
      'Humans can infer the intentions of other humans based on third-person visual observations. Oftentimes, we use social reasoning and common sense to understand others\' goals implicitly. This ability is leveraged both as children and adults (e.g. via How-To videos [30]) for learning anything where the mechanical nuance of the task is hard to capture in still images or text [5] (e.g. how to knead dough or knit). If robots can also be taught to understand the intentions of other agents, it might allow them to better interact with humans and perform tasks more efficiently.\n' +
      '\n' +
      'This work focuses on visual imitation learning, where robots learn to perform tasks by watching video demonstrations. This setup offers several advantages. First, it allows robots to learn from agents with different embodiment, enabling new skill acquisition without tele-operation. Second, it enables robots to learn from experts, even if they are not situated with the robot. Finally, visual imitation learning is ideal for teaching\n' +
      '\n' +
      'Fig. 1: **Overview.** Vid2Robot is a video-conditioned robot policy. Given a human demonstration (top), Vid2Robot recognizes the task semantics and performs the same task based on the robot’s current visual observation (bottom left). A successful trajectory is presented on the bottom right.\n' +
      '\n' +
      'tasks that are difficult or impossible to describe in words.\n' +
      '\n' +
      'Existing multi-task robot manipulation models (e.g. RT-1 [7], RT-2 [8], and RT-X [33]) use language conditioning to output a robot trajectory. This reliance of text alone for task specification makes it difficult for robots to handle polysemy and tasks whose executions vary dramatically based on context. For example, \'_open_ drawer\', \'_open_ cabinet\', \'_open_ container with lid\' and \'_open_ jar with screw cap\' might share the same verb but require very different motor control for each interaction. Here the agent should not generalize its policy, whereas it should generalize from one _drawer_ to others that vary in type, color and shape. For this reason, there are a broad range of tasks for which it is hard to design primitives for high-level planning approaches [25, 2].\n' +
      '\n' +
      'Another common approach has been to use a final goal image in goal-conditioned behavior cloning tasks [31, 24]. While several task specifications can be defined in terms of the resulting state of the environment, there are others for which the manner in which the action is performed is part of the specification. For example, \'hold the flag\' and \'wave the flag\' can have the same final goal image. This ambiguity can be resolved through the use of several sub-goal frames, that is, video conditioning.\n' +
      '\n' +
      'While language conditioned policies achieve somewhat high success rates, video conditioned policies have lagged behind in performance, as shown in prior work [21]. Cases of good performance [40] with video conditioning require the provided video to be from the same workspace with limited variability. Based on observations, we identify three main challenges for video conditioned policies: (1) _High dimensional data_: Raw videos are high dimensional data that require more compute and memory to process. This makes video conditioned multi-task policies difficult to train at scale. (2) _Variability in task specification_: There can be significant variance in how people perform the same task. Demonstrations for a task like \'unstack the cups\' can have both visually distinctive and physically diverse cups, in addition to changes in the background distractors and lighting conditions. This leads to high variability in task specification for a policy that needs to perform the same task in a new setting. (3) _Limited Availability of Training Data_: While there is an abundance of unlabeled video data on the internet, obtaining labeled video datasets for specific tasks that our robots are capable of doing is challenging.\n' +
      '\n' +
      'Despite these challenges, as noted, video conditioned policy learning is a core challenge robots need to master. Therefore, to reduce the reliance on detailed and potentially ambiguous language prompts, we aim to enable physical visual demonstrations as a another way for task specification. To this end, we study how end-to-end models with video-conditioning can used to specify tasks to robot.\n' +
      '\n' +
      'We aim to develop an end-to-end system that enables rapid adaptation to tasks specified in the form of video demonstration. Unlike prior work that either learned representations from videos for only object and verb recognition [21] or learned motor control in simulation [44], our work demonstrates the applicability of end-to-end learned video representations for real-world multi-task robotic control. We present the key contributions of our work as follows: (1) We present a transformer-based policy to encode video task specification, demonstrated by either robot or human agent embodiments (SSII). (2) We encourage alignment between the prompt and robot video representations using three contrastive auxiliary losses during training (SSII-E) (3) Through real robot experiments, we find our video conditioned policy is better than baselines on human prompt videos. Furthermore, our policy is better at cross-object motion transfer (SSIII).\n' +
      '\n' +
      '## II Approach\n' +
      '\n' +
      '### _Preliminaries_\n' +
      '\n' +
      'Our objective is to design a robotic system that takes in a _prompt video_ of a manipulation task and outputs actions that accomplish the task demonstrated in the video. This system needs to infer the underlying task from the prompt video (which might have a different setup or embodiment than the robot) and then manipulate the objects in its own environment to achieve the inferred task. Specifically, we are given a prompt video \\(V\\) and the robot state \\(S_{t}=\\{x_{i}\\}_{i=t-k-1}^{t}\\) where \\(x_{i}\\) is the frame from the robot\'s camera stream at time \\(i\\), \\(k\\) is the maximum number of historical frames, and \\(t\\) is the current time-step. We train a policy \\(\\pi(a_{t}|S_{t},V)\\) that infers the underlying task from \\(V\\) and predicts task relevant action \\(a_{t}\\). To train this model, we need a dataset of paired prompt videos and robot trajectories. We will discuss in detail how to create paired datasets below.\n' +
      '\n' +
      '### _Datasets_\n' +
      '\n' +
      'To train a video-conditioned robot policy we need a dataset of pairs: prompt videos and robot trajectories performing the same task. In this work, we explore prompt videos where the task is performed by both humans and robots. To create this dataset, we rely on three classes of data:\n' +
      '\n' +
      '1. **Robot-Robot**: We pair existing robot-robot videos of the same task. For this pairing we consider two videos to match if they are performing the same task in different settings. We define \'_task_\' based on natural language instructions used when recording robot trajectories. These instructions typically consist of one or two verbs surrounded by nouns, such as \'_place_ water bottle upright\', \'_move_ the coke can to the green chip bag\' or \'_open_ top drawer\'. The objective of this pairing is two-fold: first, to be able to take advantage of an already labeled and collected dataset of robot trajectories and second to ensure robots are able to imitate when the same task is demonstrated in a different environment.\n' +
      '2. **Hindsight Human-Robot**: Here we use the task instructions from the robot trajectories dataset and ask one to five human participants to perform the task and record a demonstration video from the robot\'s perspective/view. The set of instructions are the same as before, but there is a significant embodiment and speed variability due to different humans performing the task with left or right hands and at a randomized robot camera angle. This requires some manual effort but provides us with a lot of paired data for training the policy for the available set of instructions in the robot dataset without having to collect new robot trajectories.\n' +
      '3. **Co-located Human-Robot** In this case, a human and a robot perform the same task in the same workspace. We used this approach to collect human demonstrations and robot trajectories in diverse spaces such as a living space with sofas, a meeting room with whiteboards, hardware workstations with toy tools, a kitchen with a countertop, refrigerator and sink, a storage supplies area, and more.\n' +
      '\n' +
      'We show examples of paired prompt and robot videos from each of the three datasets in Figure 2. As can be seen, there is a considerable difference in the backgrounds and distractor objects in the Hindsight Human-Robot and Co-located Human-Robot datasets. A different complexity arises when comparing the first approach (Robot-Robot) where the actor is a robot with same morphology to the other two cases where the human is the actor in the prompt videos.\n' +
      '\n' +
      'After combining all the datasets, we have \\(\\sim\\)100k robot videos and \\(\\sim\\)10k human videos covering the tasks introduced in RT-1 [7] and RT-2 [8]. We include videos from all three data sources as they represent varying levels of difficulty and expense to collect. Pairing existing robot datasets requires less additional effort but lacks diversity in how the task is done. The second source of data is created by asking humans to mimic existing robot trajectories. While this adds some diversity in prompt videos, it does not cover any new tasks on the robot side. Finally, the presumed gold-standard is to collect data where both humans and robots are co-located in the same environment and perform diverse tasks. This takes the most amount of time as labor is required both of the humans and robot trajectories collected through tele-operation.\n' +
      '\n' +
      '### _Model Architecture_\n' +
      '\n' +
      'Our policy takes as input the prompt video and the current robot state and outputs robot actions. It consists of four modules: (1) prompt video encoder (2) robot state encoder, (3) state-prompt encoder, and (4) robot action decoder. The full architecture is illustrated in Figure 3 and each of the modules are detailed below:\n' +
      '\n' +
      '**(1) Prompt Video Encoder** encodes the video demonstration provided as a reference to convey the desired task semantics. The prompt video encoder implicitly learns to infer what task should be performed and how it needs to be done. The prompt encoder consists of a per-frame Image encoder \\(\\phi_{p}\\) (ViT [14]) followed by a Perceiver Resampler [1, 19]\\(\\psi_{p}\\). The output of the prompt encoder \\(\\psi_{p}(\\phi_{p}(V))=z_{prompt}\\) is a set of \\(N\\) tokens of d-dimension to condition the policy with the task relevant attributes from the video.\n' +
      '\n' +
      '**(2) Robot State Encoder** encodes the current state of the robot given the current frame and last \\(k\\) frames as input. Note that this module also encodes information about the objects and environment of the robot. The architecture is similar to the prompt encoder, that is, a per-frame Image encoder \\(\\phi_{s}\\) followed by a Perceiver Resampler \\(\\psi_{s}\\). Similar to the prompt encoder\'s outputs, the output of the state encoder is \\(\\psi_{s}(\\phi_{s}(S_{t}))=z_{state}\\) that encodes the latent environment and robot state information from the history of recent observations.\n' +
      '\n' +
      'We use the same image encoder weights for both (1) and (2), that is, \\(\\phi_{p}\\!=\\!\\phi_{s}\\!=\\!\\phi\\). The role of the image encoder \\(\\phi\\) is to capture spatial visual information in each frame. The Perceiver Resampler is used to enable temporal learning across frames as well as reduce the number of video tokens that must be passed into the action decoder.\n' +
      '\n' +
      '**(3) State-Prompt Encoder** The state-prompt encoder takes the prompt video encoding \\(z_{prompt}\\) and robot state encoding \\(z_{state}\\) and outputs a task encoding relevant for action prediction \\(z_{state|prompt}\\). The module is trained to output robot actions by cross-attending between the state encoding as queries and the prompt video encoding as keys and values. Intuitively, the state-prompt encoder enables fusion of the state and prompt information. For example, if the prompt video demonstrates picking up of an apple in the basket and the current state contains apple, banana and orange, then the cross attention\n' +
      '\n' +
      'Fig. 2: **Dataset creation. (top row) Here we show a Robot-Robot video pair for _placing the robot into top drawer_. We similarly pair existing robot-robot videos performing the same task. (middle row) Here we show Hindsight Human-Robot paired videos for _picking a coke can from the bottom drawer and placing it on the counter_ task. We use the task instructions from robot trajectories and ask human participants to perform the task and record a demonstration video from robot’s perspective/view. (bottom row) Here we show a Co-located Human-Robot pair of videos for _placing the pipe wrench in the toolkit_. We record both a human demonstration and by a robot teleoperation in a same workspace. Different workspaces can be used to perform the same task instruction, thus, eventually resulting in pairs with visually diverse prompts and robot state observations. More details in §II-B.**between the state and prompt encoding enables learning for which object to attend to in the state, which is crucial for the next step of action decoding. We refer to the output of the state-prompt encoder as prompt-aware state tokens.\n' +
      '\n' +
      '**(4) Robot Action Decoder** The goal of the action decoder is to predict the action vector \\(a_{t}\\) for the current state \\(S_{t}\\) such that it completes the task shown in the prompt video \\(V_{p}\\). The action decoder is a transformer decoder architecture that takes in the fixed action position tokens [49] as input queries and the prompt-aware state tokens \\(z_{state|prompt}\\) for keys and values. The size of the action position embedding is \\(N\\times d\\) where \\(N\\) is the number of action dimensions and \\(d\\) is the transformer embedding dimension. More details on the action vector in SSII-D.\n' +
      '\n' +
      'The action position embeddings cross-attend to the prompt-aware state tokens to predict the target binned action values as output. Each output token of the action decoder corresponds to an action dimension for the mode, arm and base. Specifically, each token embedding is projected to 256 dimensions and a softmax layer is applied on the top to obtain the bin corresponding to the target action vector. Unlike prior work [7, 8] that use autoregressive action decoding that requires multiple forward passes during inference, we use action position embeddings for one forward pass prediction like in ACT [49]. Instead of predicting one action for the next timestep, we follow the approach outlined in [49, 21] and train the policy with a prediction horizon of four steps. We always use the action bin that has the highest probability, i.e. argmax over predicted probabilities, to choose the action value for execution.\n' +
      '\n' +
      '**Cross-Attention Layers.** In the Vid2Robot architecture, we use Cross-Attention Transformer layers extensively. They are used in the following modules: Prompt Resampler, State Resampler, State-Prompt Encoder and Action Decoder. We found Cross-Attention layers are helpful in managing the high number of tokens and the resulting large attention matrices when processing both prompt videos and robot state videos. This is because the standard self-attention layers would require orders of magnitude more memory to process the same video. For example, when using ViT-B/16 the total number of video tokens for a \\(16\\) frame reference video and a \\(8\\) frame robot state video at \\(224\\times 224\\) resolution would be \\(8\\times 196+16\\times 196=4704\\). A full self-attention operation on this would lead to an attention matrix with \\(4704^{2}\\sim 22\\mathrm{M}\\) entries. However, by using two Perceiver Resamplers with 64 latents we were able to train with attention matrices of the size \\(8\\times 196\\times 64+16\\times 196\\times 64\\sim.3\\mathrm{M}\\). Thus, cross attention layers in Vid2Robot play an important role in reducing attention computation and enabling training with paired videos.\n' +
      '\n' +
      '### _Preprocessing_\n' +
      '\n' +
      'To handle the varying lengths of videos for efficient training, we randomly sample \\(N\\!=\\!16\\) frames always including first and last frames and sort them in increasing order of time. During training, we sample a robot state \\(S_{t}\\) by sampling a random timestep first. We then select the preceding \\(k-1\\) frames to create a robot state video comprising of a total of \\(k\\!=\\!8\\) frames before. In case there are less than \\(k-1\\) frames before the current time-step, we repeat the first frame to create a fixed size robot state video. The pixel values in each frame are normalized between 0 to 1. Each frame is resized to \\((224,224)\\). Photometric distortions like cropping, brightness, contrast, hue and saturation are applied during training.\n' +
      '\n' +
      'The action vector consists of values indicating the mode, gripper pose and closedness as well as base displacement and rotation. Each of the values have different ranges, which we first use to scale the values in between 0 and 1. We then discretize the values into 256 bins each. In total, we construct 11-dim action vector as target, each of which has value between [0, 255]. In this study, we train and evaluate in the scenarios where base remains stationary.\n' +
      '\n' +
      '### _Training_\n' +
      '\n' +
      '**Action Prediction Loss** We train Vid2Robot end-to-end with behavior cloning. The idea is to learn video representations from raw pixels to recognize task verb and objects, as well as learn motor control to accomplish it. We use a classification loss on actions that have been tokenized into \\(N\\!=\\!256\\) bins.\n' +
      '\n' +
      'Fig. 3: **Architecture. Our model takes as input frames of the prompt video and the robot’s current observations, encodes those into prompt video and robot state token embeddings, which are then processed through into state-prompt encoder and decoded into a robot action for the current timestep. More details in §II-C.**Given the robot trajectory for performing a task with current visual observations \\(x_{t}\\), we have the corresponding expert action \\(a_{t}\\). The action prediction loss is Cross Entropy between the predicted action and the expert action as:\n' +
      '\n' +
      '\\[L_{CE}(a_{t},\\hat{a}_{t})=\\sum_{\\tau}a_{t}\\log\\hat{a}_{t} \\tag{1}\\]\n' +
      '\n' +
      'This trains all the model parameters, as shown in Fig 3.\n' +
      '\n' +
      '**Auxiliary Losses.** Although our dataset size is substantial, it is insufficient for training large Transformer based models. In order to prevent over-fitting by just predicting actions correctly on the training set, we add three auxiliary losses that encourage learning features that are helpful in understanding semantics in prompt videos.\n' +
      '\n' +
      '_Video Alignment Loss_: We want to encourage temporal alignment between prompt videos and robot videos performing that show the same task. By aligning prompt videos and robot videos, we want the image encoder to learn to be invariant to different embodiments, lighting, backgrounds, view-angles and distractor objects while still encoding features relevant to predicting task progress. Our choice of loss is the temporal-cycle consistency loss introduced in [17]. This loss has been shown to encode task progress when trained on videos of different agents performing the same task [47]. This loss is applied on per-frame image embeddings of the prompt \\(V_{p}\\) and robot \\(V_{r}\\) videos during training. To apply the loss, we average pool the per-frame embeddings output in spatial dimensions from image encoder \\(\\phi\\) and apply a projector head of 2-layer MLP [10]. We call this as _alignment pooling layer_\\(\\Phi\\) on the per-frame image embeddings, as shown in Fig 4. For each video \\(V_{i}\\), this results in a sequence of embeddings \\(E_{i}=\\{\\Phi(v_{i}^{1}),\\Phi(v_{i}^{2})...,\\Phi(v_{i}^{L_{i}})\\}\\), where \\(L_{i}\\) is the length of the \\(i^{th}\\) video.\n' +
      '\n' +
      'We apply TCC loss on encoding \\(E_{p}\\), and \\(E_{r}\\) for prompt and robot video respectively. The intuitive idea of TCC loss is that we want to ensure the representation of every frame of \\(E_{p}\\) should have a correspondence in \\(E_{r}\\) and vice versa. This involves two steps: First, we compute soft neighbor of \\(t^{th}\\) frame of \\(E_{p}\\) (\\(E_{p}^{t}\\) in short) in \\(E_{r}\\) and call it \\(\\overrightarrow{E_{pr}^{t}}\\).\n' +
      '\n' +
      '\\[\\begin{split}\\overrightarrow{E_{pr}^{t}}=\\sum_{k}^{L_{r}}\\alpha_ {k}E_{r}^{k},\\quad\\mathrm{where}\\quad\\alpha_{k}=\\frac{e^{-\\|E_{i}^{t}-E_{j}^{k }\\|^{2}}}{\\sum_{k}^{L_{j}}e^{-\\|E_{i}^{t}-E_{j}^{k}\\|^{2}}}\\end{split} \\tag{2}\\]\n' +
      '\n' +
      'Second, we find the corresponding frame for this newly computed soft-neighbour in \\(E_{p}\\). This is called _cycle-back_ in [17] and it involves similar soft-neighbour computation as in Equation 2 to obtain say \\(\\overrightarrow{E_{pr}^{t}}\\), which ideally should be same as \\(t\\), that is, \\((\\overrightarrow{E_{pr}^{t}}-t)^{2}\\) should be minimized. TCC loss minimizes such mean squared error between all frames for prompt and robot video encodings and vice-versa, that is,\n' +
      '\n' +
      '\\[\\begin{split} L_{TCC}(E_{p},E_{r})=\\sum_{t\\in V_{p}}( \\overrightarrow{E_{pr}^{t}}-t)^{2}\\\\ L_{TCC}=\\frac{L_{TCC}(E_{p},E_{r})+L_{TCC}(E_{r},E_{p})}{2} \\end{split} \\tag{3}\\]\n' +
      '\n' +
      '_Prompt-Robot Video Contrastive Loss (VVCL)_: We apply contrastive loss between prompt tokens produced by robot or prompt video performing the same task. This loss encourages the prompt encodings to learn task semantics from video tokens only in a self-supervised manner. A thing to note here is that the initial pairing of prompt and robot video has been done using natural language. However, by using a constrastive loss only on video embeddings with a self-supervised loss,\n' +
      '\n' +
      'Fig. 4: **Training Setup.** We show all the losses Vid2Robot is trained with and how each loss is connected to its different modules. Along with (1) the main action prediction loss, we apply three auxiliary losses: (2) temporal video alignment loss, (3) a contrastive loss between the prompt and robot video performing the same task, and (4) a contrastive loss between a prompt/robot video with the language embedding. More details in §II-E.\n' +
      '\n' +
      'we hope to encode features not covered by the short natural language embedding itself. Examples of these features include similar motions like reaching for objects, and rotating the robot arm. We use a Attention Pooling layer to merge features from the \\(N\\) prompt tokens to produce a single embedding for each video. We apply the SigLIP [48] loss between video-video pairs to encourage videos showing same task, involving similar motions and interacting objects, to be close to each other while being away from other videos in the batch. A batch contains the same number of robot videos and prompt videos, say \\(B\\). We use the prompt encoder \\(\\psi_{p}(\\phi(\\cdot))\\) to obtain a batch of full robot video embeddings \\(Z_{robot}\\) and prompt video embeddings \\(Z_{prompt}\\), each of size \\(B\\times d\\). We multiply them, \\(Z_{robot}\\cdot Z_{prompt}^{T}\\) to obtain a \\(B\\times B\\) matrix. Adding a learnable temperature \\(\\tau\\) and bias \\(b\\), we have our logit matrix as \\(\\hat{Y}=(Z_{robot}\\cdot Z_{prompt}^{T})*\\tau+b\\). We consider the videos of robot and prompt performing the same task as positives and assign them a label of 1 along the diagonal and -1 for off-diagonal pairs, that is, the label matrix \\(Y=2\\mathrm{I}_{B}-1\\). SigLIP loss is the negative loglikelihood, specifically, \\(\\sigma^{\\prime}(Z_{1},Z_{2})=-\\sum\\log\\sigma(Y\\cdot(Z_{1}\\cdot Z_{2}^{T})*t+b)\\), where \\(Y=2\\mathrm{I}_{B}-1\\). The video-video contrastive loss is then defined as:\n' +
      '\n' +
      '\\[L_{VVCL}=\\sigma^{\\prime}(Z_{prompt},Z_{robot}) \\tag{4}\\]\n' +
      '\n' +
      '_Video-text Contrastive Loss (VTCL)_: We add a contrastive loss between prompt tokens \\(Z_{prompt}\\) and those produced by robot video \\(Z_{robot}\\) and the embedding of the text instructions of the task \\(Z_{text}\\). This encourages a part of the embedding space to be aware of object names and verbs present in the prompt and the robot videos. A version of this loss has been applied before by BC-Z [21] as auxiliary language regression loss. We use an Attention Pooling layer [46] with one latent query to merge features from the \\(N\\) prompt tokens to produce a single embedding for each video. Within a batch, we retrieve \\(B\\) pairs of video and text embeddings. Similar to Equation 4, we apply SigLIP [48] loss to get\n' +
      '\n' +
      '\\[L_{VTCL}=\\frac{\\sigma^{\\prime}(Z_{prompt},Z_{text})+\\sigma^{\\prime}(Z_{robot},Z_{text})}{2} \\tag{5}\\]\n' +
      '\n' +
      'This encourages every video to have similar embeddings to their textual description embeddings, while being different from the text embeddings corresponding to other videos in the batch.\n' +
      '\n' +
      'Overall, we apply the mean of all four losses for training that is \\(L=\\frac{1}{4}(L_{CE}+L_{TCC}+L_{VVCL}+L_{VTCL})\\).\n' +
      '\n' +
      '### _Implementation_\n' +
      '\n' +
      'We trained the model (implemented in Jax) for 200K iterations. We use AdamW optimizer with an initial learning rate of 8e-5 using a cosine learning rate schedule with warmup steps 2,000 and final learning rate of 1e-6. For both the Prompt and State Resamplers, we use 2 Perceiver Resampler layers with 64 latents. Both state-prompt encoder and action decoder are 4 layer deep cross-attention transformers.\n' +
      '\n' +
      '## III Experiments\n' +
      '\n' +
      'We present results with real robot evaluations for our multi-task video-conditioned policy. One of the key questions that we tackle in this work is how well robots can imitate humans performing manipulation tasks. Because of differences in embodiments, humans perform manipulation tasks at a different speed and style. We study the effect of using robot as well as human videos as prompts.\n' +
      '\n' +
      '**Metrics.** We refer to a _rollout_ as a sequence of actions inferred from the policy and executed on the robot from an initial state observation and prompt video, until the policy terminates or a maximum number of steps are taken, whichever is lower. We define _success_ for a rollout when the policy executes the task instruction shown in the prompt video. A successful rollout involves correct actions to be taken successively in the environment, without any assistance for resets or recovery. For each task instruction, we record many rollouts per policy. We take the average of success recorded across all the rollouts and call it the _Success Rate_ for that task. Aggregated success rate across tasks is referred as _Overall_ Success Rate.\n' +
      '\n' +
      'A mistake made early on in a rollout can result in poor success rate, even if the model\'s offline overall prediction accuracy is high. For example, if a policy makes an error while grasping a water bottle early on in the task and it slips to an unreachable location, the rollout will be marked as a failure, even if the policy had good action predictions for the later steps. To record partial progress for a rollout, we annotate whether the robot _reached_ the correct location, _grasped_ the correct object, _released_ the object at the correct location, and _terminated_ the task correctly. More details on partial success analysis in SSIII-A.\n' +
      '\n' +
      '**Evaluation Setup.** We ask human raters to evaluate success for a policy\'s rollout on a robot. We evaluate the policies by varying the robots, lighting conditions, chest of drawers, and objects. We ensure the policies being evaluated are shown similar initial object configurations during rollouts. The initial state is randomized after all policies have been evaluated for a given initial state. For all rollouts, we sample prompt videos that are not seen by the models during training. This ensures that the policies are evaluated for whether they can recognize the task from new prompt videos or not.\n' +
      '\n' +
      '**Baselines.** We compare our model with BC-Z [21], a video conditioned policy using a ResNet-18 encoder. BC-Z [22] involves demonstration-observation pairs processed via a FiLM [36] conditioned ResNet encoder and fed into a ResNet based policy network to predict robot actions. For a fair comparison, we train the BC-Z model with the same training data used to train the Vid2Robot model. We run rollouts of BC-Z policy for a fixed maximum number of steps, as BC-Z doesn\'t have a terminate action.\n' +
      '\n' +
      '**Key Questions and Results** We address the following questions in this work:\n' +
      '\n' +
      '1. How well do video-conditioned policies perform when they are shown a task in an unseen video? (Fig 5, SS III-A)2. What is the gap in success rate due to prompt embodiment difference (robot v/s human)? (SS III-A)\n' +
      '3. Can we leverage the learned motion representations for out-of-distribution object interactions? (SS III-B)\n' +
      '\n' +
      '### _Task-based success_\n' +
      '\n' +
      'We compare the our Vid2Robot model and baseline BC-Z with robot and human prompt videos in Table I. Both Vid2Robot and BC-Z were trained on a same data mixture containing robot-robot and human-robot paired data. Prompt videos cover a subset of the training tasks but the videos themselves are new for the models. In this evaluation, we investigate what each model\'s ability is to infer the task specification from prompt video as well as the current observed state of the robot.\n' +
      '\n' +
      'In order to test the capabilities of the model in different settings on real robot, we evaluate it across eight categories of manipulation tasks as shown in Table I. Specifically, we evaluate for nine tasks: \'knock water bottle over\',\'move rxbar chocolate near coke can\',\'move green jalapeno chip bag near coke can\', \'pick green rice chip bag\', \'place coke can upright\', \'pick coke can from bottom drawer and place on counter\', \'open middle drawer\', \'close middle drawer\', and \'place apple into top drawer\'.\n' +
      '\n' +
      'We ask four evaluators to carry out two rollouts per task for a prompt video dataset and policy setting (a row in Table I), that implies, we have eight trials per task to evaluate a policy\'s task success rate. We report overall success rate per row over nine tasks with eight trials per task, that is, \\(9\\!\\times\\!8\\!=\\!72\\) trials. In total, our evaluations in Table I required \\(72\\!\\times\\!4\\!=\\!288\\) real robot rollouts.\n' +
      '\n' +
      '#### Iv-A1 What is the gap in success rate due to embodiment difference in prompt videos?\n' +
      '\n' +
      'We compare our model with BC-Z when prompted with robot and human videos. BC-Z serves as a strong baseline for our comparisons. The overall success rate of our model Vid2Robot outperforms BC-Z for Human prompt videos by 20%, and is comparable for Robot prompt videos. Note that there is an order of magnitude more training samples for robot trajectories than human videos in our training mixture. Hence, there isn\'t a significant gap\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c} \\hline \\hline Prompter & Model & _pick_ & _pick-place on_ & _place into_ & _open_ & _close_ & _move near_ & _knock over_ & _place upright_ & Overall \\\\ \\hline \\multirow{3}{*}{Robot} & BC-Z & 75.0\\% & 50.0\\% & **61.5**\\% & 16.7\\% & 66.7\\% & **44.0**\\% & **58.3**\\% & **50.0**\\% & 52.6\\% \\\\  & Vid2Robot & 75.0\\% & **58.8**\\% & 50.0\\% & **91.7\\%** & **100.0**\\% & 33.3\\% & 41.7\\% & 16.7\\% & **54.9\\%** \\\\ \\hline \\multirow{3}{*}{Human} & BC-Z & 50.0\\% & 12.5\\% & 12.5\\% & 0.0\\% & 50.0\\% & 43.8\\% & 12.5\\% & **50.0**\\% & 30.6\\% \\\\  & Vid2Robot & **100.0**\\% & **50.0**\\% & **50.0**\\% & **62.5**\\% & **87.5**\\% & **43.8\\%** & **25.0\\%** & 12.5\\% & **52.8\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: Task Success Rate for Robot and Human prompts.\n' +
      '\n' +
      'Fig. 5: **Policy Rollouts. Each row shows a prompt video of a human doing a task on the left, and on the right we show the corresponding successful robot rollouts using Vid2Robot. Note how visually different the prompts are, while the policy rollouts are recorded with different lighting, background, as well as number and placement of the distractor objects.**in performance for robot prompt videos. For human prompt videos, our model outperforms BC-Z in most tasks, showing that Vid2Robot captures the task semantics from prompt videos better than the baseline. Our model outperforms in tasks like picking from drawer and placing on the counter, and opening/closing drawer tasks by a large margin. The most challenging task is _placing upright_ and _knocking over_. We analyze the failure reasons in SSV Fig 9.\n' +
      '\n' +
      '#### V-A2 How well do video-conditioned policies perform when they are shown a task in an unseen video?\n' +
      '\n' +
      'In addition to marking a rollout as a success, we recorded partial success annotations per rollout. In Fig 6, we observe that our model _reaches_ to the correct object 78% of the time, about 8% more as compared to baseline. The policies sometimes fail to reach the correct object and go towards a distractor instead. Next, _grasping_ errors happen, particularly with small and deformable objects and in collision prone areas like drawer handle or counter\'s edge. Here our model (65%) outperforms BC-Z (45%) by a large margin of 20%. A successful grasp often the most difficult part in a rollout, and the most crucial for success. After grasping, most tasks require _releasing_ at a correct location. There is a slight drop in success rate in both models due to incorrect release during the rollouts. While BC-Z runs for a fixed number of steps, our policy Vid2Robot predicts when to terminate. We observe that the rate of _release_ and _terminate_ is almost identical, about 57% for our model, that implies, that after releasing at correct location, Vid2Robot mostly terminates successfully.\n' +
      '\n' +
      '### _Cross-object motion transfer_\n' +
      '\n' +
      'Our policy and baseline were trained with paired videos as discussed in SSII-B. This implies that the training data included only those scenarios where the interaction object shown in prompt is present in the current robot observations. But _what if we provided a prompt video of one object and tested on other objects. Does it do the same motion as shown in the prompt video?_ Interestingly, we found our model to perform learned manipulation actions on objects that it has not seen in train set. We call this emergent behavior as _cross-object motion transfer_.\n' +
      '\n' +
      'We compare Vid2Robot with BC-Z for cross object motion transfer ability with five prompt videos, namely, \'knock water bottle over\', \'pick green rice chip bag\', \'place coke can upright\', \'pick coke can from bottom drawer and place on counter\', and \'place apple into top drawer\'. Each prompt video is evaluated with unrelated objects in robot\'s initial observation. The objects used for evaluation are \'_orange\'_, \'green can\'_, \'chips bag\', \'banana\', \'pink piggy soft toy\', \'wrist watch\'_. We selected objects to have diverse shape, size, and deformability to evaluate situations that require different grasps for success.\n' +
      '\n' +
      'The evaluation setup is similar to SSIII-A. Here the evaluator sets up one of the object for a task and records rollouts for each model. We compare 2 models on 5 tasks with 6 objects, so every evaluator runs \\(2\\!\\times\\!5\\!\\times\\!6\\!=\\!60\\) rollouts. We repeat the evaluation with four raters, thus reporting results in Table II\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline Model & _pick_ & _place on_ & _place_ & _place_ & _knock_ & \\\\  & _pick_ & _place on_ & _into_ & _upright_ & _over_ & Overall \\\\ \\hline BC-Z & 45.8\\% & 0.0\\% & 29.2\\% & 12.5\\% & 0.0\\% & 17.5\\% \\\\ Vid2Robot & 45.8\\% & **25.0\\%** & **54.2\\%** & **16.7\\%** & **29.2\\%** & **34.2\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: Cross-object motion transfer success.\n' +
      '\n' +
      'Fig. 6: **Partial Success Rate for BC-Z and Vid2Robot.** Our policy Vid2Robot outperforms BC-Z in terms of _reaching_ the correct object, _grasping_ it, _releasing_ it at the correct location and then _terminating_ the episode correctly. Note that BC-Z does not have terminate control.\n' +
      '\n' +
      'Fig. 7: **Qualitative results for cross-object motion transfer.** Given a prompt video of _placing coke can upright_, we rollout the policy with a _green can_, _chips bag_, _staplet_ and a _soft toy_ in front of the robot. We observe that our model can infer the motion of _place upright_ in the prompt video and apply it on other objects. There is an implicit notion of pragmatics in the policy as shown by the selection of green can over other objects.\n' +
      '\n' +
      'on a total of \\(4{\\times}60=240\\) rollouts.\n' +
      '\n' +
      'Iii-B1 Can we provide a prompt video of one object and test it on other objects? Does the policy do the same motion as shown in the prompt video?\n' +
      '\n' +
      'In Fig 7, we show the above experimental setup qualitatively. We use a prompt video to \'place coke can upright\'. We observe that the policy is able transfer the action of \'placing upright\' to several objects, like a green can, a chips bag, a stapler, and a soft toy. Note that the policy adheres to the prompt video and chooses green can over chips bag or banana for placing upright.\n' +
      '\n' +
      'Quantitatively, we observe that BC-Z is often unable to successfully complete the tasks when testing cross=object motion transfer, as shown in each task in Table II. In contrast, our model (34%) performs better than BC-Z (17%) in this setting and performs the motion indicated in the prompt video. Our model is comparable to BC-Z with 45% success rate on _picking_ out-of-distribution objects. More importantly, tasks involving placing into drawers demonstrates significant improvement (\\(29\\%\\to 54\\%\\)). For certain tasks like picking from drawers and placing on counters and knocking over, BC-Z is unable to perform at all whereas Vid2Robot is able to complete the task \\(25\\%-29\\%\\) of the time.\n' +
      '\n' +
      '### _Ablations_\n' +
      '\n' +
      'In SSII-E, we presented action prediction loss and three auxiliary losses. Here we analyze the role of these additional loss functions to the overall success rate. We investigate the impact of (1) not using any auxiliary loss, and (2) adding auxiliary language loss.\n' +
      '\n' +
      'We consider the tasks similar to that described in SSIII-A, that is, 9 tasks for evaluating each policy.\n' +
      '\n' +
      'We have 3 model variants, namely, the original Vid2Robot, the one without video-text contrastive loss (CL) and the one with only action prediction loss. We ask 3 human evaluators to run the each model variant with 2 rollouts each. In total, we report results with \\(3{\\times}3{\\times}9{\\times}2{=}162\\) rollouts in Fig 8. The error bars indicate the standard deviation for success reported on rollouts with each model variant.\n' +
      '\n' +
      '#### Iii-C1 What is the impact of not using any auxiliary loss?\n' +
      '\n' +
      'We observe that the performance of our model (61%) is significantly improved by enforcing representation constraints through auxiliary losses, in comparison to using only action prediction loss (45%). It highlights the importance of the proposed auxiliary losses in SSII-E.\n' +
      '\n' +
      '#### Iii-C2 What is the impact of the auxiliary language loss?\n' +
      '\n' +
      'BC-Z proposed to use language representations to improve video representations for conditioning the policy. We compare our policy with another variant trained with all losses but the Video-Text CL. We observe only marginal improvement of 1-2% in success rate when using the language loss. This implies that video alignment and video contrastive loss contribute significantly towards performance improvement. Our results hope to serve as a promising evidence that effective video representations can be learned without auxiliary losses that use pre-trained language embeddings.\n' +
      '\n' +
      '## IV Related Work\n' +
      '\n' +
      '**Task Specifications for Robots** The development of general-purpose robots hinges on effectively grounding task specifications. Videos are a dense source of information that not only provide what to do but also how to do it in physical world. Recent works have used videos for task specification [4, 23, 40]. Another line of work uses videos to learn world models to predict future visual observations [29, 26, 9, 31, 15]. While language [45, 7, 33, 34], final goal images [24, 6], and others like hand-drawn inputs [43] have been proposed as means for task specification, learning from prompt videos is complementary to these approaches and inevitable for rapid adaptation of trained polices to perform new manipulation skills at deployment.\n' +
      '\n' +
      '**Learning from Human Demonstrations** As videos of humans performing various tasks proliferate the internet, several works aim to address how to best leverage this information for robot learning. The difference in robot vs human embodiment poses a significant challenge, for which existing approaches range from translating image of a human into the robot [42] to inpainting for agent-agnostic representations [3]. Many prior works propose to leverage off-the-shelf models for hand pose estimation and contact tracking [4, 12, 37], object-centric representations [38, 20], as well as reward functions for reinforcement learning [3, 27, 42]. Other methods [32, 44, 4] cast this problem into visual representation learning to accelerate learning of downstream motor control tasks. While these modular learning solutions work well in limited datasets, these are prone to compounding error of each of its component, and thus, not efficiently scalable. End-to-end training approaches for goal-conditioned imitation learning [11, 41, 18, 13] and reinforcement learning [39, 35] are promising alternatives to these techniques, but these results have been largely limited in simulation and hindered by sim-to-real gap. In contrast, we choose to tackle this as an end-to-end large multi-task learning from human videos with real robot evaluations.\n' +
      '\n' +
      '**Imitation via Paired Demonstrations** Our setup of paired prompt videos and robot trajectory is most similar to One-Shot Visual Imitation literature. Many prior works assume access to pairs, where the first is used as the demonstration of the task to be performed, and the second as the observation of the agent.\n' +
      '\n' +
      'Fig. 8: **Ablation for auxilliary losses used in Vid2Robot. We compare our proposed approach that has all auxiliary losses (green, left) with a variant without language contrastive loss that was originally proposed in BC-Z (orange, middle) and a version with no auxilliary losses (blue, right). More details in (§III-C)**Some of the early works [16] proposed training a demonstration network via temporal convolution and neighborhood attention to condition a manipulation policy network. In more recent approaches like [11, 28, 20], paired demonstrations and observations are used to train a transformer policy, often with additional constraints like inverse dynamics prediction[11] or contrastive representation learning [28]. However, these approaches are largely evaluated in specific set of simulated tasks, and not compared on real robots. Most similar to our work is BC-Z [22] which reports evaluations with real robot tasks. While our setup is similar to some of this prior art, our model Vid2Robot couples large image encoders, cross-attention layers, and contrastive auxiliary losses to learn a manipulation policy that imitates a human showing a task.\n' +
      '\n' +
      '## V Limitations and Future Directions\n' +
      '\n' +
      'In SSIII, we show that our approach has improved over previous work but there is a gap in performance for video-conditioned policies. Language conditioned policies like [8] shows a higher success for known set of tasks with several hundreds of teleoperation trajectories for training. We, on the other hand, accomplish the first milestone of evaluating the video-conditioned policies in the similar setup. We discuss three limitations of our work and provide insights for future directions here.\n' +
      '\n' +
      'First, we qualitatively investigate some reasons for failure of a policy rollout. In Fig 9, we illustrate and explain 3 examples showing how self occlusion, grasping errors and presence of distractors can lead to failure during any rollout. Second, we observe a significant drop in the grasping success in Fig 6. While we use robot camera observation to estimate the state and implicitly learn depth estimation, it is often incomplete when there is occlusion or when the robot gripper is out of camera view. By enhancing the state information with multimodal sensor fusion, we may improve the grasp success rate. Third, we consider carefully collected short task instruction demonstrations from three different sources as shown in SSII-B, all of which are 5 to 20 seconds videos. To test our models on long horizon demonstrations or \'in-the-wild\' videos online, we need effective pairing strategies for videos and a few corresponding robot trajectories to train the policy.\n' +
      '\n' +
      '## VI Conclusion\n' +
      '\n' +
      'We demonstrate novel methods for both data collection and modeling for video conditioned skill learning. These skills generalize to novel object configurations and more abstracted verb meanings when no immediately obvious object is visible. The skills and generality provided by our model complement other approaches to widen the set of skills that robots have access to, and to include skills not otherwise easily acquired. Future work can leverage these learned primitives to execute novel task plans. We hope our cross-object motion transfer experiments will encourage further research in transferring motion to new objects and settings for bootstrapping data collection, and enabling human-robot interaction with rapid adaptation to new skills.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'We would like to thank Yansong Pang, Grecia Salazar, Utsav Malla, Deeksha Manjunath, Jornell Quiambao, Sarah Nguyen, Sangeetha Ramesh, Tran Pham, Samuel Wan, Tomas Jackson, Jodilyn Peralta, Celeste Barajas, Elio Prado, Rochelle Dela Cruz, Alex Luong and Krista Reymann for supporting data collection via teleoperation. Special thanks to Jornell Quiambao, Grecia Salazar, Utsav Malla, Deeksha Manjunath,\n' +
      '\n' +
      'Fig. 9: **Failure analysis with policy rollouts.** (Top) Policy predicts gripper pose and depends on the IK solver to move the arm. Sometimes, the IK solution can block the robot’s camera view. (Middle) Grasping failures happen, especially with transparent and deformable objects. (Bottom) Distractor objects as well as difference in lighting and background may cause recognition errors, where policy might perform the correct motion but with incorrect object(s).\n' +
      '\n' +
      'Sarah Nguyen, Sangeetha Ramesh, and Jaspiar Singh for evaluations on robot; Michael Ahn, Anthony Brohan and Keerthana Gopalakrishnan for policy evaluation infrastructure; Suneel Belkhale, Dorsa Sadigh, Chelsea Finn, and Sergey Levine for helpful discussions; Jonathan Tompson, and Vincent Vanhouke for thorough feedback on the writing. This work was also supported by Google Robotics Funding at Carnegie Mellon University.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. Menick, S. Borgeaud, A. Brock, A. Nematadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan (2022) Flamingo: a visual language model for few-shot learning. In Alice H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, pp.. External Links: Document, Link Cited by: SSII.\n' +
      '* [2]M. Gonzalez Arenas, T. Xiao, S. Singh, V. Jain, A. Z. Ren, Q. Vuong, J. Varley, A. Herzog, I. Leal, S. Kirmani, D. Sadigh, V. Sindhwani, K. Rao, J. Liang, and A. Zeng (2023) How to prompt your robot: a prompt-book for manipulation skills with code as policies. In 2nd Workshop on Language and Robot Learning: Language as Grounding, External Links: Link Cited by: SSII.\n' +
      '* [3]S. Bahl, R. Mendonca, L. Chen, U. Jain, and D. Pathak (2023) Affordances from human videos as a versatile representation for robotics. In Computer Vision and Pattern Recognition, External Links: Link Cited by: SSII.\n' +
      '* [4]S. Bahl, R. Zellers, R. L. Bras, J. Gao, and Y. Choi (2020) PIQA: Reasoning about Physical Commonsense in Natural Language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, External Links: Link Cited by: SSII.\n' +
      '* [5]S. Bahl, R. Zellers, R. L. Bras, J. Gao, and Y. Choi (2020) PIQA: Reasoning about Physical Commonsense in Natural Language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, External Links: Link Cited by: SSII.\n' +
      '* [6]K. Bousmalis, G. Vezzani, D. Rao, C. Devin, A. X. Lee, M. Bauza, T. Davchev, Y. Zhou, A. Gupta, A. S. Raju, A. Laurens, C. Fantacci, V. Dalibard, M. Zambelli, M. Mertins, R. Pevceivicute, M. Blokzijl, M. Denil, N. Batchelor, T. Lampe, E. Parisotto, K. Zolna, S. E. Reed, S. Gomez Colmenarejo, J. Scholz, A. Abdolmaleki, D. Groth, J. Regli, O. O. Sushkov, T. Rohord, J. Enrique Chen, Y. Aytar, D. Barker, J. Ortiz, M. A. Riedmiller, J. Tobias Springenberg, R. Hadsell, F. Nori, and N. Manfred Otto Heess (2023) Robocat: A self-improving generalist agent for robotic manipulation. In Transactions on Machine Learning Research, External Links: Link Cited by: SSII.\n' +
      '* [7]A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich (2022) RT-1: Robotics transformer for Real-World control at scale. In arXiv preprint arXiv:2212.06817, Cited by: SSII.\n' +
      '* [8]A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich (2022) RT-1: robotics transformer for Real-World control at scale. In arXiv preprint arXiv:2212.06817, Cited by: SSII.\n' +
      '* [9]A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich (2022) RT-1: robotics transformer for Real-World control at scale. In arXiv preprint arXiv:2212.06817, Cited by: SSII.\n' +
      '* [10]A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich (2022) RT-1: robotics transformer for Real-World control at scale. In arXiv preprint arXiv:2212.06817, Cited by: SSII.\n' +
      '* [11]A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich (2022) RT-1: robotics transformer for Real-World control at scale. In arXiv preprint arXiv:2212.06817, Cited by: SSII.\n' +
      '* [12]A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K. Huei Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich (2022) RT-1: robotics transformer for Real-World control at scale. In arXiv preprint arXiv:2212.06817, Cited by: SSII.\n' +
      '* [13]A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K. Huei Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich (2022) RT-1: robotics transformer for Real-World control at scale. In arXiv preprint arXiv:2212.06817, Cited by: SSII.\n' +
      '* [14]A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K. Huei Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich (2022) RT-1: robotics transformer for Real-World control at scale. In arXiv preprint arXiv:2212.06817, Cited by: SSII.\n' +
      '* [15]A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K. Huei Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. In _arXiv preprint arXiv:2307.15818_, 2023.\n' +
      '* [9] Elliot Chane-Sane, Cordelia Schmid, and Ivan Laptev. Goal-conditioned reinforcement learning with imagined subgoals. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 1430-1440. PMLR, 18-24 Jul 2021. URL [https://proceedings.mlr.press/v139/chane-sane21a.html](https://proceedings.mlr.press/v139/chane-sane21a.html).\n' +
      '* [10] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-supervised models are strong semi-supervised learners. _Advances in neural information processing systems_, 33:22243-22255, 2020.\n' +
      '* [11] Sudeep Dasari and Abhinav Gupta. Transformers for one-shot visual imitation. In _Conference on Robot Learning_, pages 2071-2084. PMLR, 2021.\n' +
      '* [12] Eadom Dessalene, Chinmaya Devaraj, Michael Maynord, Cornelia Fermuller, and Yiannis Aloimonos. Forecasting action through contact representations from first person video. _IEEE Trans. Pattern Anal. Mach. Intell._, 45(6):6703-6714, June 2023.\n' +
      '* [13] Yiming Ding, Carlos Florensa, Mariano Phielipp, and Pieter Abbeel. Goal-conditioned imitation learning. In _Proceedings of the 33rd International Conference on Neural Information Processing Systems_, pages 15324-15335. Curran Associates Inc., Red Hook, NY, USA, December 2019.\n' +
      '* [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=YicbFdNTTY](https://openreview.net/forum?id=YicbFdNTTY).\n' +
      '* [15] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL [https://openreview.net/forum?id=b08a5MRcwy](https://openreview.net/forum?id=b08a5MRcwy).\n' +
      '* [16] Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [17] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. Temporal Cycle-Consistency learning. In _Computer Vision and Pattern Recognition_, 2019.\n' +
      '* [18] Oliver Groth, Chia-Man Hung, Andrea Vedaldi, and Ingmar Posner. Goal-Conditioned End-to-End visuomotor control for versatile skill primitives. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 1319-1325. IEEE, May 2021.\n' +
      '* [19] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J Henaff, Matthew Botvinick, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver IO: A general architecture for structured inputs & outputs. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=fILj7Wpl-g](https://openreview.net/forum?id=fILj7Wpl-g).\n' +
      '* [20] Vidhi Jain, Yixin Lin, Eric Undersander, Yonatan Bisk, and Akshara Rai. Transformers are adaptable task planners. In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, _Proceedings of The 6th Conference on Robot Learning_, volume 205 of _Proceedings of Machine Learning Research_, pages 1011-1037. PMLR, 14-18 Dec 2023. URL [https://proceedings.mlr.press/v205/jain23a.html](https://proceedings.mlr.press/v205/jain23a.html).\n' +
      '* [21] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning. In _Proceedings of the 5th Conference on Robot Learning_, pages 991-1002, 2022.\n' +
      '* [22] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. BC-z: Zero-shot task generalization with robotic imitation learning. In _Conference on Robot Learning_, pages 991-1002. PMLR, 2022.\n' +
      '* [23] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. VIMA: General robot manipulation with multimodal prompts. In _Fortieth International Conference on Machine Learning_, 2023.\n' +
      '* [24] Jacob Krantz, Theophile Gervet, Karmesh Yadav, Austin Wang, Chris Paxton, Roozbeh Mottaghi, Dhruv Batra, Jitendra Malik, Stefan Lee, and Devendra Singh Chaplot. Navigating to objects specified by images. April 2023.\n' +
      '* [25] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, 2023.\n' +
      '* [26] Yuxuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. In _2018 IEEE International Conference on Robotics and Automation (ICRA)_, pages 1118-1125. IEEE, May 2018.\n' +
      '\n' +
      '* [27] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. VIP: Towards universal visual reward and representation via Value-Implicit Pre-Training. In _International Conference on Learning Representations_, 2023.\n' +
      '* [28] Zhao Mandi, Fangchen Liu, Kimin Lee, and Pieter Abbeel. Towards more generalizable one-shot visual imitation learning. In _2022 International Conference on Robotics and Automation (ICRA)_, pages 2434-2444. IEEE, 2022.\n' +
      '* [29] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos. 2023.\n' +
      '* [30] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapasawi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In _ICCV_, 2019.\n' +
      '* [31] Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. In _NeurIPS_, July 2018.\n' +
      '* [32] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot manipulation. In _6th Annual Conference on Robot Learning_, 2022. URL [https://openreview.net/forum?id=tGbpgzGyOrI](https://openreview.net/forum?id=tGbpgzGyOrI).\n' +
      '* [33] Open X-Embodiment Collaboration, Abhishek Padalkar, Acorn Pooley, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Animesh Garg, Anthony Brohan, Antonin Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Scholkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue Zhou, Gaurav S Sukhatme, Gautam Salhotra, Ge Yan, Giulio Schiavi, Gregory Kahn, Hao Su, Hao-Shu Fang, Haochen Shi, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Joseph J Lim, Joao Silverio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, Krishnan Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Max Spero, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafaullah, Oier Mees, Oliver Kroemer, Pannag R Sanketi, Paul Wohlhart, Peng Xu, Pierre Sermanet, Priya Sundaresan, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martin-Martin, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Beklhale, Takayuki Osa, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, and Zichen Jeff Cui. Open X-Embodiment: Robotic learning datasets and RT-X models. _arXiv 2310.08864_, 2023.\n' +
      '* [4] Priyam Parashar, Vidhi Jain, Xiaohan Zhang, Jay Vakil, Sam Powers, Yonatan Bisk, and Chris Paxton. Spatial-Language Attention Policies for Efficient Robot Learning. In _Conference on Robot Learning_, 2023. URL [https://arxiv.org/abs/2304.11235](https://arxiv.org/abs/2304.11235).\n' +
      '* [5] Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Abbeel, and Sergey Levine. SFV: reinforcement learning of physical skills from videos. _ACM Trans. Graph._, 37(6):1-14, December 2018.\n' +
      '* [6] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In _Proceedings of the AAAI conference on artificial intelligence_, 2018.\n' +
      '* [7] Vladimir Petrik, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Learning object manipulation skills via approximate state estimation from real videos. In Jens Kober, Fabio Ramos, and Claire Tomlin, editors, _Proceedings of the 2020 Conference on Robot Learning_, volume 155 of _Proceedings of Machine Learning Research_, pages 296-312. PMLR, 2021.\n' +
      '* [8] Soren Pirk, Mohi Khansari, Yunfei Bai, Corey Lynch, and Pierre Sermanet. Online object representations with contrastive learning. 2019.\n' +
      '* [9] Karl Schmeckepper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine, and Chelsea Finn. Reinforcement learning with videos: Combining offline observations with interaction. November 2020.\n' +
      '* [10] Rutav Shah, Roberto Martin-Martin, and Yuke Zhu. MUTEX: Learning unified policies from multimodal task specifications. In _7th Annual Conference on Robot Learning_, 2023.\n' +
      '* [11] Pratyusha Sharma, Deepak Pathak, and Abhinav Gupta.\n' +
      '\n' +
      'Third-Person visual imitation learning via decoupled hierarchical controller. _Adv. Neural Inf. Process. Syst._, 32, 2019.\n' +
      '* [42] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter Abbeel, and Sergey Levine. AVID: Learning Multi-Stage tasks via Pixel-Level translation of human videos. In _Robotics: Science and Systems (RSS)_, December 2020.\n' +
      '* [43] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, Chelsea Finn, and Karol Hausman. Open-World object manipulation using pre-trained Vision-Language models. March 2023.\n' +
      '* [44] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. _arXiv preprint arXiv:2203.06173_, 2023.\n' +
      '* [45] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, Zsolt Kira, Manolis Savva, Angel Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, and Chris Paxton. HomeRobot: Open-Vocabulary Mobile Manipulation. In _Conference on Robot Learning_, 2023. URL [https://arxiv.org/abs/2306.11565](https://arxiv.org/abs/2306.11565).\n' +
      '* [46] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. _arXiv preprint arXiv:2205. 01917_, 2022.\n' +
      '* [47] Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tomppson, Jeannette Bohg, and Debidatta Dwibedi. Xirl: Cross-embodiment inverse reinforcement learning. In _Conference on Robot Learning_, pages 537-546. PMLR, 2022.\n' +
      '* [48] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In _2023 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 11941-11952, Los Alamitos, CA, USA, oct 2023. IEEE Computer Society. doi: 10.1109/ICCV51070.2023.01100.\n' +
      '* [49] Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning Fine-Grained bimanual manipulation with Low-Cost hardware. In _Robotics: Science and Systems_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>