<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# InfiMM-HD: 고해상도 멀티모달 이해에서 도약 전진\n' +
      '\n' +
      ' 류호행\n' +
      '\n' +
      '동등 기여도 \\({}^{1}\\)MAIS & CRIPAC, Institute of Automation, Chinese Academy of Sciences, China. \\({}^{1}\\)MAIS & CRIPAC, Institute of Automation, China Academy of Sciences, China. \\({}^{1}\\)MAIS & CRIPAC, Institute of Automation, China. 중국 베이징 중국과학원 인공지능학부 ({}^{3}\\)ByteDance, Inc. 대응: Haogeng Liu \\(<\\)liuhaogeng22@mails.ucas.ac.cn\\(>\\).\n' +
      '\n' +
      'Quanzeng You\n' +
      '\n' +
      'Xiaotian Han\n' +
      '\n' +
      'Yiqi Wang\n' +
      '\n' +
      'Bohan Zhai\n' +
      '\n' +
      'Yongfei Liu\n' +
      '\n' +
      'Yunzhe Tao\n' +
      '\n' +
      'Huaibo Huang\n' +
      '\n' +
      'Ran He\n' +
      '\n' +
      'Hongxia Yang\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '멀티모달 대형 언어 모델(MLLM)은 최근 상당한 발전을 경험했다. 그럼에도 불구하고 고해상도 이미지 내에서 복잡한 세부 사항을 정확하게 인식하고 이해하는 데 어려움이 있다. 강력한 MLLM의 개발에 필수 불가결한 요소임에도 불구하고 이 영역은 아직 조사되지 않았다. 이 문제를 해결하기 위해, 본 연구는 낮은 계산 오버헤드로 다양한 해상도의 이미지를 처리하기 위해 특별히 설계된 새로운 아키텍처인 InfiMM-HD를 소개한다. 이 혁신은 MLLM을 고해상도 기능으로 확대하는 것을 촉진합니다. InfiMM-HD는 교차 주의 모듈과 시각적 창을 통합하여 계산 비용을 줄입니다. 이 아키텍처 설계를 4단계 훈련 파이프라인과 통합함으로써, 우리의 모델은 효율적이고 비용 효율적으로 향상된 시각적 인식을 달성한다. 경험적 연구는 InfiMM-HD의 견고성과 효율성을 강조하여 관련 분야의 탐색을 위한 새로운 길을 열어준다. 코드 및 모델은 [https://huggingface.co/Infi-MM/infimm-hd](https://huggingface.co/Infi-MM/infimm-hd)를 찾을 수 있다.\n' +
      '\n' +
      '머신러닝, ICML, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Multimodal Large Language Models (MLLMs)의 경관은 미리 훈련된 비전 인코더를 Large Language Models (LLMs)와 통합함으로써 혁신되어 왔다 (Han et al., 2023; Wang et al., 2024; Han et al., 2024), Flamingo (Alayrac et al., 2022), BLIP-2 (Li et al., 2023), LLaVA (Liu et al., 2023), MiniGPT-4 (Zhu et al., 2023) 등의 발전에 의해 예시되는 경향이다. MLLM은 새로운 시각-언어 능력을 나타낼 수 있다(Yang et al., 2023). 예를 들어, 그들은 미리 훈련된 비전 인코더와 LLM에서 결합된 단일 모드 기능을 활용하여 이미지에 따라 코드를 작성하고 이미지의 플롯을 마크다운 형식 테이블로 변환하고 웹 브라우징을 수행할 수 있다.\n' +
      '\n' +
      'MLLM에서 사전 훈련된 비전 인코더와 대규모 언어 모델의 효과적인 통합은 신중하게 설계된 비전 언어 브리징 모듈에 의존한다. 이러한 모듈은 시각적 토큰의 LLM 호환 형식에 대한 변환 및 정렬과 이러한 변환된 토큰의 후속 활용이라는 두 가지 중요한 측면을 다룬다.\n' +
      '\n' +
      '시각 토큰의 변환 및 정렬을 위해, 플라밍고(Alayrac et al., 2022) 및 BLIP-2(Li et al., 2023)와 같은 모델들은 언어 대응물들과의 유연성과 미묘한 정렬을 제공하지만 더 높은 계산 요구 및 잠재적인 정보 손실을 희생시키면서 시각 토큰들을 변환하기 위해 Perceiver-Resampler/Q-이전의 기법들을 사용한다(Cha et al., 2023). 반대로 LLaVA 및 MiniGPT-v2와 같은 모델은 더 간단한 다층 퍼셉트론(MLP) 접근법을 사용하여 다른 경로를 취한다. 이는 계산 복잡성과 학습 가능한 파라미터의 수를 감소시키지만, 시각적 데이터의 완전한 복잡성을 포착하지 못할 수 있다.\n' +
      '\n' +
      '그림 1: InfiMM-HD는 향상된 훈련 파이프라인 및 고해상도 입력 덕분에 다양한 작업에 걸쳐 우수한 성능을 보여주며, 다운스트림 작업에서 최근 방법을 일관되게 능가합니다.\n' +
      '\n' +
      '시각 토큰과 언어 토큰의 통합 단계도 마찬가지로 중요합니다. 플라밍고 스타일의 아키텍처는 교차 주의 메커니즘을 사용하여 토큰 시퀀스 길이를 확장하지 않고 토큰 유형 간의 복잡한 상호 작용을 촉진한다. 이 방법은 계산 부하를 효율적으로 관리한다. 그러나 LLAVA 스타일 모델은 직접 연결 방법을 사용하여 간단하지만 토큰 시퀀스 길이와 계산 복잡도를 증가시킨다.\n' +
      '\n' +
      '서로 다르지만, 두 아키텍처 모두 미리 훈련된 비전 트랜스포머(ViT) 인코더로 인해 낮은 이미지 해상도를 활용했다(Jiang et al., 2023; Radford et al., 2021; He et al., 2021). 낮은 해상도는 기본 이미지 수준의 의미 이해에는 충분하지만 세부 영역 수준의 분석에는 부족하다. 최근의 노력들(Wang et al., 2023; Li et al., 2023; Lin et al., 2023)은 MLLM들이 더 높은-해상도 이미지들을 처리할 수 있게 하는 것을 목표로 한다. 그러나, 주로 계산 요구가 더 큰 이미지에 대한 시퀀스 길이와 관련하여 2차적으로 증가하는 경향이 있기 때문에 상당한 도전이 남아 있다. 예를 들어, 이미지 해상도를 \\(224\\times 224\\)(Dosovitskiy et al., 2021)에서 \\(448\\times 448\\)으로 증가시키면 자기 주의 계산량이 16배가 된다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해 고해상도 이미지 처리를 위해 설계된 혁신적인 MLLM 아키텍처인 InfiMM-HD를 소개한다. InfiMM-HD는 MLLM에서 플라밍고와 LLaVA 스타일 모두의 방법론을 혁신적으로 병합한다. 변환 및 정렬 단계를 위해 LLaVA와 유사한 MLP 기반 접근법을 채택하여 시각적 토큰을 LLM과 호환되는 형식으로 효과적으로 변환하고 정렬한다. 이 전략은 정확한 처리와 계산 효율의 균형을 맞춘다. 통합 단계에서 InfiMM-HD는 플라밍고 스타일의 MLLM을 연상시키는 교차 주의 메커니즘을 활용하여 시각적 토큰 기능을 언어 토큰과 원활하게 통합한다. 이 접근법은 이전에 언급한 바와 같이 더 긴 토큰 시퀀스와 관련된 계산 문제를 완화한다. 특히, 플라밍고 스타일의 MLLM에서 고해상도 이미지 입력 능력에 대한 탐구는 여전히 새로운 연구 영역이지만, InfiMM-HD는 이 영역에서 중요한 선구적인 발전을 나타내며, MLLM 성능을 고해상도 시각적 입력과 향상시키기 위해 두 세계의 최고를 혼합한다.\n' +
      '\n' +
      '사전 훈련된 비전 인코더의 해상도 제약을 극복하기 위해 InfiMM-HD는 4단계로 전략적으로 훈련되어 비전 언어 정렬을 유지하면서 해상도 핸들링을 향상시킨다. 처음에, 모델은 다음과 같이 사전 훈련된다.\n' +
      '\n' +
      '그림 2: InfiMM-HD에 의한 예제 출력으로 세립 시각 지각에서 모델의 능숙함을 강조한다.\n' +
      '\n' +
      ' 효율적인 시각 언어 정렬을 위한 224\\(224\\times 224\\) 해상도의 영상이다. 그 후, 여러 데이터 세트로부터 \\(448\\times 448\\)개의 이미지에 대해 보간된 위치 임베딩을 사용하여 LLM을 동결 상태로 유지하는 사전 훈련을 계속한다. 그런 다음 전체 해상도의 이미지를 사용하여 훈련하고, 2D 위치 임베딩과 여러 하위 이미지에 크롭을 추가하여 \\(448\\times 448\\)의 가장 가까운 배수로 조정한다. 최종 단계에서 모델은 시각적 명령어 미세 조정, 비전 인코더 동결 및 LLM 훈련을 통해 명령어 추적 기능을 향상시킨다. 이러한 구조화된 훈련 접근법은 다양한 입력 해상도에 걸친 모델의 적응성과 성능에 중요하다. 우리 작업의 기여는 다음과 같이 요약할 수 있다.\n' +
      '\n' +
      '* 우리는 변환된 시각적 및 언어 토큰의 강화되고 효율적인 통합을 위해 플라밍고 스타일 교차 주의 메커니즘과 결합된 시각적 토큰 변환 및 정렬을 위해 MLP 기반 접근법을 사용하는 선구적인 MLLM인 InfiMM-HD를 제시한다. **고해상도** 이미지 입력을 원활하게 처리할 수 있도록 독특하게 설계되었습니다.\n' +
      '* 초기 저해상도 프리트레이닝 단계에서부터 지식 주입 및 정렬을 위한 프리트레이닝 단계, 고해상도 채택을 위한 동적 해상도 적응 단계, 마지막으로 시각적 명령어 미세 조정 단계를 거쳐 훈련 비용이 감소된 고해상도 멀티모달 대용량 언어 모델을 효과적으로 달성하는 4단계 훈련 파이프라인을 제시한다.\n' +
      '* 다양한 벤치마크에 걸쳐 수행된 실험은 시지각 영역에서 우리 모델의 놀라운 숙련도를 보여준다. 또한, 포괄적인 절제 연구는 교차 주의 스타일 멀티모달 언어 모델 아키텍처의 맥락에서 우리 설계의 독특한 우월성을 강조한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '대규모 언어 모델(LLM)의 출현은 MLLM의 개발을 촉진했다. 플라밍고(Alayrac et al., 2022)는 시각적 정보를 텍스트 시퀀스로 융합하기 위해 게이티드-크로스 어텐션 메커니즘을 사용하여 미리 훈련된 언어 모델을 MLLM 패러다임에 통합한다. 이에 반해 BLIP-2(Li et al., 2023), MiniGPT4(Zhu et al., 2023), LLaVA(Liu et al., 2023)는 시각 신호를 소프트 토큰으로 변환하고 이를 언어 모델에 직접 통합하는 패러다임 전환을 제안한다. Shikra(Chen et al., 2023)는 참조적 대화에 집중한다. OtterHD(Li et al., 2023) fine-tunes Fuyu-8B(Bavishi et al., 2023)를 지도 지침으로 사용하여 ViT-free MLLM을 가능하게 한다.\n' +
      '\n' +
      '우리가 본 진보에도 불구하고, 몇 가지 문제는 여전히 존재한다. (Zhai et al., 2023)은 시각적 표현과 언어의 불일치가 환각을 유발한다고 지적한다. (Zhang et al., 2023)은 입력 해상도를 향상시키는 것이 MLLM의 광학 문자 인식(OCR) 능력을 상당히 증가시킬 것임을 밝힌다. 점점 더 많은 실험은 현대 시각 인코더에서 정보 병목 현상의 존재를 시사한다(Tong et al., 2024; Zhai et al., 2023). 영상의 해상도는 시각적 처리 능력을 제약하는 중요한 요소로 부각된다. (Tong et al., 2024)의 연구는 현대 MLLM이 특히 시각적 및 텍스트 양식을 효과적으로 정렬하는 데 있어 여전히 체계적인 문제에 직면해 있음을 강조한다.\n' +
      '\n' +
      '그 문제를 해결하기 위해 노력하는 작품들이 있다. Lin 등에 의해 소개된 SPHINX(Lin et al., 2023)는 시각적 신호들과 언어 사이의 연결들을 확립하기 위해 다층 지각(MLP)을 채용한다. 이 모델은 여러 비전 인코더를 활용하여 강력한 시각적 표현을 구성하기 위해 출력 기능을 통합합니다. 고해상도 이미지 입력을 처리하기 위해 SPHINX는 입력된 고해상도 이미지를 더 작은 하위 이미지로 분해한 다음 텍스트 토큰과 직접 시각적 토큰을 연결한다. 이는 LLM(Large Language Model)에 대한 입력 시퀀스 길이 증가와 관련된 한계를 도입한다.\n' +
      '\n' +
      'Monkey 모델(Li et al., 2023)은 공유된 리샘플러를 통합함으로써 이 챌린지를 다룬다. 이 방법은 Flamingo(Alayrac et al., 2022)의 리샘플링 기법을 사용하여 각 입력 서브 이미지를 압축하고 시각적 토큰을 텍스트 시퀀스와 직접 연결하여 입력 이미지 해상도를 \\(1344\\times 896\\)으로 효과적으로 상향 스케일링하는 것을 포함한다. 그러나, 비전 인코더의 원시 출력으로부터 정보를 추출하고 압축하기 위한 인식기 아키텍처 내에서 학습 가능한 쿼리에 대한 의존성은 다양한 응용 시나리오에 걸친 모델의 적응성에 대한 우려를 불러일으킨다.\n' +
      '\n' +
      '우리는 정보 압축 프로세스가 맥락적 명령어와 복잡하게 얽혀 명령어 완성에 필수적인 관련 세부 사항을 식별할 수 있어야 한다고 주장한다. 우리는 교차 주의 메커니즘을 통해 시각과 언어 사이의 연결을 설정하는 InfiMM-HD를 소개한다. 학습 가능한 쿼리에 대한 의존에서 벗어나는 것은 더 광범위한 시나리오 스펙트럼에 걸쳐 모델의 적응성 및 적용 가능성을 보다 상세한 시각 지각에 걸쳐 향상시키는 것을 목표로 한다. 또한, 기존에 제안된 방법보다 적은 비용으로 고해상도 영상을 소비할 수 있다.\n' +
      '\n' +
      '## 3 Methods\n' +
      '\n' +
      '본 절에서는 InfiMM 구조를 소개하고, 저비용으로 MLLM의 입력 영상 해상도를 높이기 위한 훈련 파이프라인을 제안한다. 우리가 아는 한, 우리는 플라밍고 스타일의 아키텍처를 사용하여 HD MLLM을 달성하는 선구자입니다.\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      '제안된 모델은 Vision Transformer Encoder, Gated Cross Attention Module, Large Language Model의 세 가지 구성 요소로 구성된다. 종합적인 아키텍처는 그림 4에서 설명되며, 삽화는 단일 이미지를 보여주지만, 플라밍고의 디자인을 따르는 것은 필수적이지만, 우리의 모듈은 또한 입력으로서 다중 이미지를 다룰 수 있다. 이전 작업(Li et al., 2023b; Wang et al., 2023)에 이어, 비전 트랜스포머에 대해, 우리는 EVA2-CLIP2-E(Sun et al., 2023)를 채용하고, 추출된 비전 특징들로서 마지막 두 번째 층으로부터의 출력을 이용한다. 게이트 교차 주의 모듈은 텍스트 숨겨진 상태를 쿼리로, 비전 기능을 키 및 값으로 활용합니다. 플라밍고(Alayrac et al., 2022)에 소개된 게이팅 방법론과 달리 활성화를 위한 요소별 \\(\\tanh\\) 게이팅 메커니즘을 통합한다. 본 연구의 언어 모델은 Vicuna(Chiang et al., 2023)를 이용하여 인스턴스화된다.\n' +
      '\n' +
      '시각 정보의 효과적인 동화를 보장하기 위해 Gated Cross Attention Module은 Large Language Model의 디코더 레이어 사이에 4개의 레이어마다 전략적으로 삽입된다. 이 결정은 두 층마다 모듈을 삽입하면 게이트 중량의 약 50%가 0에 가까워 교차 주의 모듈을 비효율적으로 만든다는 경험적 관찰에서 비롯된다. 이 모델은 비전 트랜스포머(44억), 대형 언어 모델(129억) 및 게이트 교차 주의 모듈(약 77억)의 세 가지 주요 구성 요소 사이에 복잡하게 할당된 180억 매개변수의 인상적인 집합체를 보여준다.\n' +
      '\n' +
      '이 조사 동안 우리는 LLaVA 스타일의 구조의 기존 패러다임에서 벗어났다. 이러한 편차는 이전 연구에서 입증된 바와 같이 고해상도 이미지와의 타협된 호환성으로 인해 필수적이다(Li et al., 2023c; Lin et al., 2023). 특히, 치수(1344\\times 1344\\)를 갖는 이미지의 처리는 패치 크기(14)를 사용할 때 9217개의 토큰으로 구성된 광범위한 토큰 시퀀스를 생성한다. 최대 32k까지의 시퀀스 길이를 수용할 수 있는 대용량 언어 모델(LLM)의 능력에도 불구하고, 이미지당 9k 토큰의 활용은 멀티모달 언어 모델(MLLM)의 성능, 특히 다중 이미지를 포함하는 시나리오에서 본질적으로 제약을 가한다. 이러한 한계는 결과적이며, 대안적인 건축적 고려의 필요성을 강조한다. 이러한 고려 사항은 현대 언어 모델의 맥락에서 고해상도 이미지 처리에 의해 제기된 문제를 해결하는 것을 목표로 한다. 본 논문에서는 768의 축소된 차원에서의 시각 정보의 통합을 위해 교차 주의 모듈을 채택하였으며, 이 방법은 LLaVA 구조와는 달리 확장 시퀀스를 수용하면서도 계산 비용이 현저히 낮아진다. 한편, 실험을 통해 시각적 정보 추출에 효과적임을 보인다.\n' +
      '\n' +
      '### Training Details\n' +
      '\n' +
      '고해상도 영상을 처리하는 MLLM의 능력을 향상시키기 위한 4단계 훈련 절차를 그림 5와 같이 설정했으며, 이 단계는 PT(Pretraining), CPT(Continue Pretraining), DRA(Dynamic Resolution Adaption), IFT(Instruction Finetuning)로 표시된다.\n' +
      '\n' +
      '**프리트레이닝 스테이지(PT):** 이 스테이지는 주로 초기에\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c} \\hline Task & Dataset & Samples \\\\ \\hline \\multirow{3}{*}{Image Caption} & COCO Caption (Chen et al., 2015) & 205k \\\\  & TextCaps (Sidorov et al., 2020) & 55k \\\\  & VizWiz Caption (Gurari et al., 2020) & 55k \\\\ \\hline \\multirow{4}{*}{General VQA} & VQAV2 (Antol et al., 2015) & 443k \\\\  & OKVQA (Marino et al., 2019) & 9k \\\\  & VizWiz VQA (Gurari et al., 2018) & 20k \\\\  & GQA (Hudson and Manning, 2019) & 471k \\\\  & A-OKQA (Schwenk et al., 2022) & 17k \\\\ \\hline \\multirow{4}{*}{Text-oriented VQA} & TextVQA (Singh et al., 2019) & 34k \\\\  & OCRVQA (Mishra et al., 2019) & 166k \\\\  & STVQA (Biten et al., 2019) & 26k \\\\  & DocVQA (Mathew et al., 2021) & 63k \\\\  & LLaVAR (Zhang et al., 2023) & 16k \\\\ \\hline Region Description & VG (Krishna et al., 2017) & 429k \\\\ \\hline Total & - & 2.00m \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: CPT 및 DRA의 트레이닝 데이터에 대한 상세사항.\n' +
      '\n' +
      '그림 3: LLaVA 665k 데이터 세트로부터의 이미지 크기 분포의 시각화는 일부 고해상도 예제와 혼합된 500-700 사이의 해상도의 우세한 클러스터링을 나타낸다. 교육 중 동적 해상도 활용은 효율적인 리소스 관리를 위한 핵심입니다.\n' +
      '\n' +
      '시각 기능과 언어 기능을 정렬합니다. 이 단계에서는 ViT(Vision Transformer)와 LLM(Large Language Model)이 모두 동결되어 게이트 크로스 어텐션 모듈만 학습 가능하다. 이 단계에서, 모든 이미지는 낮은 훈련 비용을 유지하기 위해 \\(224\\times 224\\)으로 크기가 조정된다.\n' +
      '\n' +
      '**Continue Pretraining Stage (CPT):** 이 단계에서, 우리는 해상도 \\(448\\times 448\\)의 이미지를 처리하기 위해 ViT의 능력을 확장하기 위해 위치 임베딩의 쌍선형 보간을 사용한다. ViT 및 게이티드 크로스 어텐션 모듈은 훈련 가능합니다. 트레이닝 데이터 세트는 주로 이미지 캡션 및 시각적 질문 응답 작업에 중점을 둔다. 교육 데이터 세트에 대한 자세한 정보는 표 1에 나열되어 있다.\n' +
      '\n' +
      '**DRA(Dynamic Resolution Adaption):** 도 3에서, LLaVA-665k 데이터셋에서 이미지의 크기를 예시한 결과들이다(Liu et al., 2023). 데이터세트를 세심하게 검토하면, 모든 이미지가 최대 1344에 이르는 해상도를 나타내지 않는다는 것이 명백해진다. 이미지를 고정된 해상도로 균일하게 리사이징하는 종래의 관행과 대조적으로, 그러한 접근법은 불필요한 계산 비용을 야기하는 반면, 동적 이미지 해상도는 비용 친화적일 수 있다. 동적 해상도 입력을 용이하게 하기 위해, 우리는 개별 서브-이미지에 대해 (Wang and Liu, 2021)에서 제안된 2D 위치 임베딩 방법을 통합한다. 학습 시 입력 영상의 해상도는 448\\(448\\times 448\\)에서 1344\\times 1344\\(1344\\times 1344\\)이다. 이어서, 리사이징된 이미지를 \\(448\\times 448\\)의 서브 이미지로 분할한다. 우리는 또한 \\(448\\ 곱하기 448\\)의 원본 이미지 섬네일을 유지한다. 마지막으로 ViT를 사용하여 각 하위 이미지와 원본 이미지 썸네일에서 특징을 추출하고 직접 연결하여 최종 비전 특징을 형성한다. CPT 단계와 동일한 훈련 데이터 세트를 사용하고 ViT 및 게이티드 크로스 어텐션 모듈을 모두 훈련할 수 있도록 유지한다.\n' +
      '\n' +
      '**IFT(Instruction Finetuning Stage):** 이 최종 단계에서, 우리의 목표는 모델이 고해상도 시지각 능력을 잃지 않고 사용자 지시를 더 잘 따르도록 하는 것이다. 따라서 우리는 ViT를 냉동 상태로 유지하지만 게이트 교차 주의 모듈과 LLM을 훈련할 수 있도록 한다.\n' +
      '\n' +
      '제안된 4단계 학습 파이프라인은 입력 영상의 해상도를 점진적으로 높임과 동시에 학습을 안정화하는 핵심이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '이 섹션에서는 먼저 실험 설정에 대해 논의한다. 그런 다음 InfiMM-HD에서 제안된 모듈의 중요성을 입증하기 위해 InfiMM-HD의 주요 결과와 일련의 절제 연구를 보여준다.\n' +
      '\n' +
      '### Setup\n' +
      '\n' +
      '**Training Dataset.** 프레트레인(PT) 스테이지의 경우, 트레이닝 데이터는 이미지-텍스트 쌍 및 인터리빙된 이미지-텍스트를 모두 포함한다. 이미지-텍스트 쌍 데이터는 LAION-2B(Li et al., 2023), COYO(Byeon et al., 2022), 및 Laion-coco(Schuhmann et al.)로부터 필터링된 140M 샘플들을 포함한다. 인터리빙된 이미지-텍스트 데이터는 트레이닝을 위해 \\(50\\%\\)로 랜덤하게 MMC4(Zhu et al., 2023) 및 OBELISIC(Laurencon et al., 2023)로부터 샘플링된다.\n' +
      '\n' +
      'Continue Pretraining (CPT) 및 Dynamic Resolution Adaption (DRA) 단계에서 사용된 데이터 세트는 표 1에 열거되어 있다. Instruction Finetuning (IFT) 단계에서 LLaVA-665k (Liu et al., 2023), LLaVAR (Zhang et al., 2023), TextVQA (Singh et al., 2019) 및 ScienceQA (Lu et al., 2022)의 데이터 세트를 병합한다. 이 융합은 원래 LLaVA-665k 데이터 세트라는 사실에 의해 동기화된다.\n' +
      '\n' +
      '그림 4: InfiMM-HD의 아키텍처 프레임워크가 개략적으로 설명되며, 여기서 POS(i, j)는 로컬 패치들의 위치 임베딩을 나타내고, (i, j)는 전체 이미지 내의 그들의 위치를 나타낸다. 모델은 다양한 훈련 단계를 통해 진행되며, 각각은 상이한 모듈을 선택적으로 훈련시키는 것을 특징으로 한다. 이러한 전략적 접근에 대한 자세한 설명은 다음 절에서 자세히 설명한다.\n' +
      '\n' +
      '텍스트 지향 샘플이 부족합니다. 결과적으로 이러한 다양한 출처의 추가 데이터를 통합하여 이러한 결함을 보완한다.\n' +
      '\n' +
      'IFT 단계에서는 기본적으로 LLaVA-665k 데이터셋 Liu et al.(2023)을 주로 활용한다. 이와 함께 텍스트VQA, LLAVAR 및 사이언스QA와 같은 추가 데이터 세트를 통합하여 명령어 조정 데이터 세트를 풍부하게 한다.\n' +
      '\n' +
      '**텍스트 지향 데이터 증강** 텍스트 지향 데이터의 부족으로 인해 간단하지만 효과적인 데이터 증강 방법론을 사용합니다. 구체적으로, 이 과정은 장면 관련 인물을 이미지에 통합한 후 해당 질문을 생성하는 것을 포함한다. 이러한 맥락에서 우리는 이미지의 왼쪽 상단과 오른쪽 하단 모서리에 각각 뚜렷한 색상을 가진 두 세트의 문자를 무작위로 도입한다. 생성되는 질문에는 "이미지 좌상단에 어떤 캐릭터가 위치하고 있는가?"와 "이미지 우하단에 위치한 캐릭터의 색상은 무엇인가?"와 "이미지 좌상단에 몇 개의 캐릭터가 존재하는가?"와 같은 질문이 포함된다. 이 데이터 증강 기술은 GQA 데이터 세트에 독점적으로 적용되어 약 100,000개의 질문-답변 쌍을 생성한다는 점에 유의해야 한다. 놀랍게도, 우리는 우리 모델의 텍스트 인식 능력을 향상시키는 데 있어 이 증강 접근법의 효과를 관찰한다. 예는 그림 6에서 찾을 수 있다.\n' +
      '\n' +
      '**Training Details.** 깊은 속도의 Aminabadi et al.(2022)의 활용을 통해 훈련 과정을 용이하게 하였으며, FusedAdam optimizer를 선택하여 최적화 작업을 조정하였다. 실험 구성과 관련된 추가 복잡성은 첨부된 부록 8에 자세히 설명되어 있다.\n' +
      '\n' +
      '**평가.** 다양한 VQA 태스크에 걸쳐 InfiMM-HD를 평가한다. 일반적인 VQA 태스크에 대해서는 OKVQA Marino et al.(2019), VQAV2 Antol et al.(2015), GQA Hudson and Manning (2019), ScienceQA Lu et al.(2022) 등의 벤치마크를 활용한다. 이러한 데이터 세트는 모델에 고급 세부 시지각 기능을 요구하지는 않지만 일반적인 장면을 이해하고 사용자 지침을 따르는 모델의 능력을 효과적으로 측정합니다. 또한, 본 모델의 세밀한 세부 인식 능력을 조사하기 위해 TextVQA Singh et al.(2019), STVQA Biten et al.(2019)을 포함한 텍스트 중심의 VQA 데이터 세트를 통합하고,\n' +
      '\n' +
      '그림 5: InfiMM-HD 훈련 파이프라인의 4단계. 각 단계는 훈련 가능한 모듈, 데이터 세트 및 ViT에 입력된 이미지의 해상도로 특징지어진다. 우리의 실험 결과는 우리의 접근법의 효능을 확인시켜 224x224에서 고해상도 이미지로 점진적으로 전환하는 이점을 보여준다.\n' +
      '\n' +
      '그림 6: 데이터 증강의 일러스트레이션: 이미지의 임의의 영역에 다양한 색상을 갖는 랜덤하게 생성된 문자를 도입한다. 그런 다음 원본 쿼리를 보완하기 위해 해당 질문이 생성됩니다.\n' +
      '\n' +
      '및 OCRVQA(Mishra et al., 2019). MM-VET(Yu et al., 2023), MME(Fu et al., 2023), MMDench(Liu et al., 2023c), InfiMM-Eval(Han et al., 2023), MMMU(Yue et al., 2023) 등 새롭게 도입된 벤치마크를 사용하여 모델의 논리적 추론 능력을 평가한다. 특히, MMMU(Yue et al., 2023)는 대학 수준에서 고급 교과 지식과 의도적인 추론을 요구하는 도전 과제를 제시한다. 이러한 작업은 물리학, 화학, 생물학과 같은 다양한 분야에 걸쳐 있습니다. MM-VET 벤치마크는 모델의 통합 기능을 평가한다.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '본 절에서는 일반적인 VQA와 텍스트 중심의 VQA 과제에 대한 평가 결과를 제시한다.\n' +
      '\n' +
      '표 3은 일반적인 VQA 과제에 대한 결과를 제시한다. OKVQA, GQA 및 VQAv2의 평가 범위는 모델의 시지각 능력에 대한 단순한 평가를 넘어 모델의 사전 지식을 효과적으로 활용하는 능력을 비판적으로 검사하여 모델의 전반적인 기능에 대한 보다 포괄적인 평가를 제공한다는 점을 강조하는 것이 중요하다. 또한, 광범위한 과학 과목을 포괄하는 21,000개의 다중 모드 객관식 질문으로 구성된 ScienceQA(Lu et al., 2022)는 벤치마킹 풍경의 상당한 확장을 나타낸다. 이러한 다양한 작업에서 우리의 모델은 현저한 효과를 보여 상당한 성능 개선을 나타낸다. 우리의 모델은 가장 가까운 경쟁자를 평균 3.88% 능가함으로써 멀티모달 정보를 통합하는 우수한 능력을 보여줄 뿐만 아니라 다양한 질문을 성공적으로 탐색하기 위해 광범위한 사전 지식을 활용하는 능숙함을 보여준다.\n' +
      '\n' +
      '일반적인 VQA 평가 외에도 그림 A에서 입증된 바와 같이 텍스트 기반 데이터 세트인 TextVQA, OCRVQA 및 STVQA에 대한 평가를 통해 모델의 상세한 시각적 인식 능력을 추가로 탐색한다. 정량적 결과는 표 2에 요약되어 있으며, 이러한 결과는 이미지 내에서 복잡한 텍스트 세부 사항을 이해하는 데 있어 제안된 고해상도 모델의 유효성을 강조한다.\n' +
      '\n' +
      '또한 MMMU, MM-Vet, InfiMM-Eval, MMB, MME 및 POPE를 포함하여 최근에 제안된 MLLM 평가 벤치마크에서 InfiMM-HD를 평가한다. 이전 VQA 데이터 세트와 비교하여 이러한 데이터 세트는 MLLM의 보다 포괄적인 평가 측면을 포함하여 보다 복잡한 추론 기능을 필요로 한다. 평가 결과는 표 4에 요약되어 있으며, 각 모델이 고유한 강도와 한계를 나타내는 모든 벤치마크에서 단일 모델이 우수하지 않다는 점은 주목할 만하다. 제안된 모델은 다양한 분야에 걸친 적응성과 역량을 강조하면서 공통 가능한 전반적인 성능을 보여준다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '입력 이미지 해상도가 모델 성능에 미치는 직접적인 영향을 설명하기 위해 절제 연구를 수행했다. 이 조사에서 다양한 해상도 입력이 발생했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l|c|c c c c c} \\hline \\hline Model & LLM & In-house data & OKVQA & IconVQA & GQA & VQAv2 & ScienceQA\\({}_{\\text{img}}\\) \\\\ \\hline Flamingo-80B (Alayrac et al., 2022) & - & ✓ & 50.6 & - & - & 56.3 & - \\\\ Palm-E-12B (Driess et al., 2023) & - & ✓ & 60.1 & - & - & 77.7 & - \\\\ Qwen-VL (Bai et al., 2023) & Qwen-7B & ✓ & 58.6 & - & 59.3 & 79.5 & 67.1 \\\\ Qwen-VL-Chat (Bai et al., 2023) & Qwen-7B & ✓ & 56.6 & - & 57.5 & 78.2 & 68.2 \\\\ CogVLM (Wang et al., 2023) & Vicuna-7B & ✓ & 58.9 & - & - & **83.4** & - \\\\ Monkey (Li et al., 2023c) & Qwen-7B & ✓ & 61.3 & - & 60.7 & 80.3 & 69.4 \\\\ \\hline BLIP-2 (Li et al., 2023b) & Vicuna-13B & \\(\\times\\) & 45.9 & 40.6 & 41.0 & - & - \\\\ Shikra (Chen et al., 2023) & Vicuna-13B & \\(\\times\\) & 47.2 & - & - & 77.4 & - \\\\ mPLUG-Owl2(Ye et al., 2023) & LLaMA2-7B & \\(\\times\\) & 57.7 & - & 56.1 & 79.4 & 68.7 \\\\ LLaVA 1.5 (Liu et al., 2023a) & Vicuna-13B & \\(\\times\\) & - & - & 63.3 & 80.0 & 71.6 \\\\ Sphinx-2K (Lin et al., 2023) & LLaMA2-13B & \\(\\times\\) & 62.6 & 50.5 & 63.1 & 80.7 & 70.6 \\\\ \\hline InfiMM-HD & Vicuna-13B & \\(\\times\\) & **65.5** & **51.3** & **63.5** & 82.0 & **83.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 일반적인 VQA 과제에 대한 결과. 표는 우리의 일반주의 모델의 성능을 독점적으로 제시하며, 다양한 모델에 비해 그 우수성을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l|c|c c c} \\hline \\hline Model & Res & In-house data & TextVQA & OCRVQA & STVQA \\\\ \\hline Qwen-VL-Chatlülülül2023 & 418 \\(\\times\\) 418 & ✓ & 61.5 & **70.5** & - \\\\ Modley (Li et al., 2023c) & 1344 \\(\\times\\) 768 & ✓ & 67.6 & - & 67.7 \\\\ \\hline Unkore Guet et al. (2023) & 2242 \\(\\times\\) 224 & \\(\\times\\) & 40.7 & 34.5 & 30.8 \\\\ DeProfla (Fog et al., 2023) & 2560 \\(\\times\\) 2560 & \\(\\times\\) & 60.2 & 57.2 & 45.5 \\\\ BLIP-2 (Li et al., 2023b) & 2242 \\(\\times\\) 224 & \\(\\times\\) & 40.6 & - & - \\\\ LAVA1.5 (Liu et al., 2023b) & 336 \\(\\times\\) 336 & \\(\\times\\) & 48.5 & - & - \\\\ Sybina-2K (Lin et al., 2023) & 768 \\(\\times\\) 708 & \\(\\times\\) & 61.2 & 67.8 & - \\\\ \\hline InfiMM-HD (all arei-only) & 4 dynamic & \\(\\times\\) & **70.7** & 66.0 & 67.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 텍스트 중심의 시각적 질문 답변(VQA) 과제에 대한 평가 결과. STVQA의 경우, 원숭이는 평가를 위해 열차 세트의 데이터를 무작위로 샘플링한다.\n' +
      '\n' +
      '동일한 네 개의 훈련 단계 집합입니다. 실험적 형평성을 보장하기 위해, 우리는 다중 작업 데이터 세트에서 두 개의 에폭에 의해 DRA 없이 동일한 모델 훈련을 수행한다. 결과는 표 5에 제시되어 있다. 관찰 가능한 패턴은 입력 이미지의 해상도를 높이는 것이 모델의 효율성을 향상시킨다는 것을 시사하며, 특히 TextVQA(Singh et al., 2019) 및 DocVQA(Mathew et al., 2021)와 같은 텍스트 뉘앙스에 대한 이해가 필요한 작업에서 더욱 그렇다. 대조적으로, 일반적인 VQA 작업의 경우 거친 장면 개요는 종종 정확한 질문 응답에 충분하여 모델 효능에 대한 해결의 맥락 의존적 영향을 강조한다.\n' +
      '\n' +
      '직관적으로 입력 영상을 서브 영상으로 크롭함에 따라 공간 정보를 유지하기 위해서는 각 서브 영상에 대한 위치 임베딩을 추가하는 것이 중요하다. 그 영향을 알아보기 위해 2차원 위치 임베딩에 대한 절제 연구를 수행하였으며, 그 결과는 표 6에 나열되어 있으며, 위치 임베딩을 제거하는 것이 모델 성능에 약간의 영향을 미친다는 것을 시사한다. 그러나 DocVQA에서는 분명히 저하에 직면해 있다. 이러한 현상은 DocVQA가 다양한 구성 요소 간의 대응 관계가 중요한 문서를 주로 포함하고 있다는 사실이 공간 정보를 통해 직접적으로 반영되었기 때문일 수 있다.\n' +
      '\n' +
      '우리 모델에서 수신기 리샘플러는 원산지 플라밍고와 비교하여 제거된다. 그 영향을 파악하기 위해 절제 연구가 있는 수신기 리샘플러의 중요성을 조사했다. 표 7에 수신기 리샘플러를 통합한 모델과의 비교가 제시된다. 표에서 알 수 있듯이, 수신기 리샘플러는 정보 병목 현상이 되어 해상도가 증가함에 따라 모델의 성능 향상을 제약한다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '본 연구에서는 고해상도 영상을 효과적으로 처리하기 위해 MLLM을 개선하였다. 결과는 다양한 차원에서 상당한 발전을 나타낸다. 이러한 성과에도 불구하고 일정한 한계는 지속되고 있다. 실제로, 모델은 텍스트 이해에 지향된 과제에서 결함을 보인다. 우리의 지속적인 노력은 전체 모델 성능을 향상시키기 위해 데이터 세트를 보완하면서 보다 효과적인 모달 정렬 전략을 탐색하는 것을 포함한다.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      '본 논문에서는 고해상도의 입력 영상을 처리하기 위해 설계된 Flamingo-style MLLM에 비해 개선된 InfiMM-HD를 제시한다. 우리의 접근 방식은 저차원 공간에서 시각적 정보와 언어 모델을 원활하게 통합하기 위해 교차 주의 메커니즘을 활용한다. 고해상도 이미지와 관련된 가공할 수 있는 계산 요구를 해결하기 위해, 우리는 입력 고해상도 이미지를 더 작은 하위 이미지로 분할하고, 각각은 상대적으로 더 낮은 해상도에 맞게 특별히 조정된 공유 비전 트랜스포머(ViT)를 사용하여 개별 처리를 수행한다. 또한, 4단계 훈련 파이프라인을 구축합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\hline \\hline Resolution & GQA & VQAv2 & OCRVQA & DocVQA & TextVQA \\\\ \\hline dynamic (w/o PE) & 63.3 & 81.6 & 65.4 & 53.0 & 70.3 \\\\ dynamic & 63.5 & 82.0 & 66.0 & 55.1 & 70.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 위치 임베딩에 대한 절제 연구 결과. **w/o PE**는 위치 임베딩을 제거하는 것을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c} \\hline \\hline Model & POPE & MME\\({}^{\\text{P}}\\) & MME\\({}^{\\text{C}}\\) & MMB & MM-VET & InfiMM-Eval & MMMU (val) \\\\ \\hline BLIP-2 (Li et al., 2023b) & 85.3 & 1293.8 & - & - & 22.4 & - & - \\\\ Shikra (Chen et al., 2023) & - & - & - & 58.8 & - & - & - \\\\ LLaVA 1.5 (Liu et al., 2023a) & 85.9 & **1531.3** & 295.4 & 67.7 & 35.4 & 32.62 & 36.4 \\\\ Qwen-VL-Chat (Bai et al., 2023) & - & 1487.5 & **360.7** & 60.6 & - & 37.39 & 35.9 \\\\ Sphinx-2K (Lin et al., 2023) & 87.2 & 1470.6 & 326.8 & 65.9 & **40.2** & - & 32.9 \\\\ \\hline Ours & **87.9** & 1472.3 & 329.4 & **71.6** & 38.9 & **37.42** & **37.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 복잡성이 강화된 MLLM에 대해 복잡하게 설계된 벤치마크에서 얻은 결과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline Configuration & GQA & VQAv2 & DocVQA & TextVQA \\\\ \\hline \\(224\\times 224\\) & 60.7 & 78.7 & 25.6 & 50.0 \\\\ \\(224\\times 224\\) (PC) & 57.7 & 79.0 & 25.2 & 48.9 \\\\ \\hline \\(448\\times 448\\) & 61.3 & 80.5 & 44.9 & 64.1 \\\\ \\(448\\times 448\\) (PC) & 56.9 & 79.5 & 30.7 & 56.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 상이한 해상도로 훈련된 모델에 대한 평가 결과. 여기서 PC는 모델이 perceiver resampler를 가지고 있다는 것을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline Resolution & GQA & VQAv2 & OCRVQA & DocVQA & TextVQA \\\\ \\hline \\(224\\times 224\\) & 60.7 & 78.7 & 57.6 & 25.6 & 50.0 \\\\ \\(448\\times 448\\) & 61.3 & 80.5 & 58.7 & 44.9 & 64.1 \\\\ \\hline dynamic & 63.5 & 82.0 & 66.0 & 55.1 & 70.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 상이한 입력 해상도로 훈련된 모델에 대한 평가 결과. 여기서 동적 모델은 \\(448\\times 448\\)에서 \\(1344\\times 1344\\)까지의 해상도를 지원한다. 추론하는 동안 우리는 해상도를 1344로 제한하지 않는다.\n' +
      '\n' +
      '제안된 모델을 구성하여 낮은 계산 비용이 발생하도록 한다. 따라서 제안된 모델은 포괄적인 설계와 계산 자원 최소화에 중점을 둔 것이 특징이다.\n' +
      '\n' +
      '##6 브로드캐스팅 효과\n' +
      '\n' +
      '우리의 모델은 기능에도 불구하고 부정확한 정보의 생성과 지각 착각에 대한 민감성을 포함하여 문제에 직면할 수 있다. 또한, 많은 기계 학습 모델과 마찬가지로 기본 가치 시스템에 의해 영향을 받는 편향을 나타낼 수 있다. 이러한 잠재적인 문제를 인식하는 것은 그러한 기술의 책임 있고 윤리적인 배치를 보장하는 데 중요하다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: few-shot learning을 위한 시각적 언어 모델. _ 신경 정보 처리 시스템_, 35:23716-23736, 2022에서의 발전.\n' +
      '* Aminabadi et al. (2022) Aminabadi, R. Y., Rajbhandari, S., Zhang, M., Awan, A. A., Li, C., Li, D., Zheng, E., Rasley, J., Smith, S., Ruwase, O., and He, Y. 딥스피드 추론: 2022년 전례 없는 규모로 변압기 모델의 효율적인 추론을 가능하게 한다.\n' +
      '* Antol et al. (2015) Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. Vqa: Visual question answering. International Conference on Computer Vision (ICCV)_, 2015.\n' +
      '* Bai et al. (2023) Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.\n' +
      '* Bavishi et al. (2019) Bavishi, R., Elsen, E., Hawthorne, C., Nye, M., Odena, A., Somani, A., and Tasrlar, S. 멀티모달 모델인 2023. URL[https://www.adept.ai/blog/fuyu-8b](https://www.adept.ai/blog/fuyu-8b)을 소개합니다.\n' +
      '* Biten et al. (2019) Biten, A. F., Tito, R., Mafla, A., Gomez, L., Rusinol, M., Valveny, E., Jawahar, C., and Karatzas, D. Scene text visual question answering. In _Proceedings of the IEEE/CVF international conference on computer vision_, pp. 4291-4301, 2019.\n' +
      '*Byeon et al. (2022) Byeon, M., Park, B., Kim, H., Lee, S., Baek, W., and Kim, S. Coyo-700m: Image-Text pair dataset. [https://github.com/kakaobrain/coyo-dataset] (https://github.com/kakaobrain/coyo-dataset), 2022.\n' +
      '* Cha et al. (2023) Cha, J., Kang, W., Mun, J., and Roh, B. Honeybee: Locality-enhanced projector for multimodal llm, 2023.\n' +
      '* Chen et al. (2023) Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R. Shikra: 멀티모달 llm의 지시적 대화 마법, 2023.\n' +
      '* Chen et al. (2015) Chen, X., Fang, H., Lin, T. - Y., Vedantam, R., Gupta, S., Dollar, P., and Zitnick, C. L. Microsoft coco caption: Data collection and evaluation server, 2015.\n' +
      '* Chiang et al. (2023) Chiang, W. -L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zu, H., Zang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: 90%* chatgtp 품질로 gpt-4를 인상하는 오픈 소스 챗봇, 3월 2023. URL[https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).\n' +
      '* Dosovitskiy et al.(2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. 이미지는 16x16 단어 가치가 있습니다: 2021년 규모에서 이미지 인식을 위한 트랜스포머입니다.\n' +
      '* Driess et al. (2023) Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. Palm-e: embodied multimodal language model, 2023.\n' +
      '* Feng et al. (2023) Feng, H., Liu, Q., Liu, H., Zhou, W., Li, H., and Huang, C. Docpedia: Unleashing the power of large multimodal model in the frequency domain for versatile document understanding, 2023.\n' +
      '* Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., Wu, Y., and Ji, R. Mme: 멀티모달 대형 언어 모델에 대한 종합 평가 벤치마크, 2023.\n' +
      '* Gu et al. (2022) Gu, J., Kuen, J., Morariu, V. I., Zhao, H., Barmpalios, N., Jain, R., Nenkova, A., and Sun, T. 2022년, 문서 이해를 위한 통합 사전 교육 프레임워크.\n' +
      '* Gurari et al. (2018) Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp.3608-3617, 2018.\n' +
      '* Gurari et al. (2020) Gurari, D., Zhao, Y., Zhang, M., and Bhattacharya, N. 시각 장애인이 촬영한 이미지를 캡션하는 중입니다. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVII 16_, pp. 417-434. Springer, 2020.\n' +
      '* Han et al. (2020) Han, X., You, Q., Liu, Y., Chen, W., Zheng, H., Mrini, K., Lin, X., Wang, Y., Zhai, B., Yuan, J., Wang, H., and Yang, H. InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models.\n' +
      '\n' +
      '_arXiv e-prints_, art. arXiv:2311.11567, 11월 2023. doi:10.48550/arXiv.2311.11567.\n' +
      '* Han et al. (2023) Han, X., You, Q., Liu, Y., Chen, W., Zheng, H., Mrini, K., Lin, X., Wang, Y., Zhai, B., Yuan, J., Wang, H., and Yang, H. Infimm-eval: Complex open-ended reasoning evaluation for multi-modal large language models, 2023.\n' +
      '* Han et al. (2024) Han, X., Wang, Y., Zhai, B., You, Q., and Yang, H. Coco is "all" you need for visual instruction fine-tuning, 2024.\n' +
      '* He et al. (2021) He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. 마스크 오토인코더는 2021년 확장 가능한 비전 학습자입니다.\n' +
      '* Hudson & Manning (2019) Hudson, D. A. and Manning, C. D. Gqa: real-world visual reasoning and compositional question answering을 위한 새로운 데이터셋. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 6700-6709, 2019.\n' +
      '* Jiang et al. (2023) Jiang, D., Liu, Y., Liu, S., Zhang, X., Li, J., Xiong, H., and Tian, Q. 클립에서 디노까지: 비주얼 인코더는 2023년 멀티모달 대형 언어 모델로 외친다.\n' +
      '* Krishna et al. (2017) Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L. - J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _ International journal of computer vision_, 123:32-73, 2017.\n' +
      '* Laurencon et al. (2023) Laurencon, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush, A. M., Kiela, D., Cord, M., and Sanh, V. Obelics: open web-scale filtered dataset of interleaved image-text documents, 2023.\n' +
      '* Li 등(2023a) Li, B., Zhang, P., Yang, J., Zhang, Y., Pu, F., and Liu, Z. Otterhd: 고해상도 멀티모달리티 모델, 2023a.\n' +
      '* Li 등(2023b) Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoder and large language models, 2023b.\n' +
      '*Li 등(2023c) Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., and Bai, X. 원숭이: 이미지 해상도와 텍스트 라벨은 대형 멀티모달 모델, 2023c에 중요한 것이다.\n' +
      '* Lin et al. (2023) Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., Han, J., Huang, S., Zhang, Y., He, X., Li, H., and Qiao, Y. 스핑크스: 멀티모달 대형 언어 모델에 대한 가중치, 작업 및 시각적 임베딩의 공동 혼합, 2023.\n' +
      '*Liu et al. (2023a) Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning, 2023a.\n' +
      '* Liu et al. (2023b) Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning, 2023b.\n' +
      '*Liu et al. (2023c) Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., Chen, K., and Lin, D. Mmbbench: Is your multi-modal model is a all-around player?, 2023c.\n' +
      '* Lu et al. (2022) Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K. - W., Zhu, S. - C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chain for science question answering. _The 36th Conference on Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* Marino et al. (2019) Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Ok-vqa: 외부 지식이 필요한 시각적 질문 응답 벤치마크. In _Proceedings of the IEEE/cvf conference on computer vision and pattern recognition_, pp. 3195-3204, 2019.\n' +
      '* Mathew et al. (2021) Mathew, M., Karatzas, D., and Jawahar, C. Docvqa: A dataset for vqa on document images. In _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pp. 2200-2209, 2021.\n' +
      '* Mishra et al. (2019) Mishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A. Ocr-vqa: Visual question answering by reading text in images. In _2019 International conference on document analysis and recognition (ICDAR)_, pp. 947-952. IEEE, 2019.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021.\n' +
      '* Schuhmann et al. (2022) Schuhmann, C., Kopf, A., Vencu, R., Coombes, T., and Beaumont, R. 라이온 코코: 레이노2b-en으로부터 600m 합성 캡션. [https://laino.ai/blog/lain-coco/] (https://laino.ai/blog/lain-coco/).\n' +
      '* Schwenk et al. (2022) Schwenk, D., Khandelwal, A., Clark, C., Marino, K., and Mottaghi, R. A-okvqa: 세계 지식을 이용한 시각적 질문 답변의 벤치마크. In _European Conference on Computer Vision_, pp. 146-162. Springer, 2022.\n' +
      '* Sidorov et al. (2020) Sidorov, O., Hu, R., Rohrbach, M., and Singh, A. Textcaps: a dataset for image captioning with reading comprehension. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pp. 742-758. Springer, 2020.\n' +
      '* Singh et al. (2019) Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. 읽을 수 있는 vqa 모델을 향합니다. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 8317-8326, 2019.\n' +
      '*Sun et al. (2023) Sun, Q., Fang, Y., Wu, L., Wang, X., and Cao, Y. 에바-클립: 스케일에서 클립을 위한 개선된 훈련 기법, 2023.\n' +
      '\n' +
      '* Tong et al. (2024) Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., and Xie, S. 눈을 크게 감고? 멀티모달 llms, 2024의 시각적 단점을 탐구한다.\n' +
      '* Wang et al. (2023) Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., Xu, J., Xu, B., Li, J., Dong, Y., Ding, M., and Tang, J. Cogvlm: Visual expert for prerained language models, 2023.\n' +
      '* Wang et al. (2024) Wang, Y., Chen, W., Han, X., Lin, X., Zhao, H., Liu, Y., Zhai, B., Yuan, J., You, Q., and Yang, H. Exploring ability of multimodal large language models(mllms): a comprehensive survey on emerging trends in multimodal reasoning, 2024.\n' +
      '* Wang & Liu(2021) Wang, Z. 및 류정철 수열 수준 훈련과 함께 심층 신경망을 사용하여 수학 공식 이미지를 라텍스 시퀀스로 변환합니다. _ International Journal on Document Analysis and Recognition (IJDAR)_, 24(1-2):63-75, 2021.\n' +
      '* Yang et al. (2023) Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and Wang, L. lmms의 새벽: 2023년 gpt-4v(ision)를 이용한 예비 탐사.\n' +
      '* Ye 등(2023) Ye, Q., Xu, H., Ye, J., Yan, M., Hu, A., Liu, H., Qian, Q., Zhang, J., Huang, F., and Zhou, J. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, 2023.\n' +
      '* Yu et al. (2023) Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. 음-벳: 통합 기능에 대한 대형 멀티모달 모델 평가, 2023.\n' +
      '* Yue et al. (2023) Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B., Yuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. 음: 2023년 전문가 agi에 대한 대규모 다중 훈련 다중 모드 이해 및 추론 벤치마크.\n' +
      '* Zhai et al. (2023) Zhai, B., Yang, S., Xu, C., Shen, S., Keutzer, K., and Li, M. 할리 스위치: 2023년 대형 시각 언어 모델에서 객체 환각을 제어합니다.\n' +
      '* Zhang et al. (2023) Zhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D., and Sun, T. Llavar: 텍스트가 풍부한 이미지 이해를 위한 향상된 시각적 명령어 튜닝, 2023.\n' +
      '* Zhu et al. (2023a) Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: 고급 대형 언어 모델로 비전 언어 이해력 향상, 2023a.\n' +
      '* Zhu et al. (2023b) Zhu, W., Hessel, J., Awadalla, A., Gadre, S. Y., Dodge, J., Fang, A., Yu, Y., Schmidt, L., Wang, W. Y., and Choi, Y. 멀티모달 C4: 텍스트가 인터리빙된 이미지들의 오픈, 억만 스케일 코퍼스 _ arXiv preprint arXiv:2304.06939_, 2023b.\n' +
      '\n' +
      '트레이닝 구성\n' +
      '\n' +
      '우리는 표 8의 InfiMM-HD의 상세한 훈련 하이퍼 파라미터와 설정을 보고한다. 우리의 모델은 64 NVIDIA A100-SXM-80GB로 훈련되었다. 사전 훈련 단계는 약 72시간입니다. (이미지-텍스트 쌍만 사용할 경우 30시간 소요) 나머지 3단계는 21시간 미만 소요된다.\n' +
      '\n' +
      '## 부록 B 평가 벤치마크의 요약\n' +
      '\n' +
      '우리는 표 9에서 사용한 평가 벤치마크와 해당 메트릭에 대한 자세한 요약을 제공했다. ANLS는 평균 정규화된 레벤슈타인 유사성을 의미한다는 점에 유의한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Configuration & Pretraining & Continue Pretraining & Dynamic Resolution Adaption & Instruction Finetuning \\\\ \\hline ViT init. & EVA2-CLIP2-E (res 224) & EVA2-CLIP2-E (res 448) & ViT from 2nd-stage & ViT from 3rd-stage \\\\ LLM init. & Vicuna-13b & Vicuna-13b & Vicuna-13b & Vicuna-13b \\\\ Gated cross-attention init. & random & InfiMM-HD 1st stage & InfiMM-HD 2nd stage & InfiMM-HD 3rd stage \\\\ Image resolution & 224 & 448 & dynamic(448-1344) & dynamic(448-1344) \\\\ ViT sequence length & 257 & 1024 & 1025 & 1025 \\\\ LLM sequence length & 32 (IT);384 (IIT) & 128 & 128 & 1024 \\\\ Optimizer & \\multicolumn{3}{c}{AdamW} \\\\ Optimizer hyperparameter & \\(\\beta_{1}=0.9,\\beta_{2}=0.95,eps=1e^{-8}\\) & \\multicolumn{3}{c}{\\(\\beta_{1}=0.9,\\beta_{2}=0.999,eps=1e^{-5}\\)} \\\\ Peak learning rate & \\(1e^{-4}\\) & \\(1e^{-5}\\) & \\(1e^{-5}\\) & \\(5e^{-6}\\) \\\\ Minimum learning rate & \\(1e^{-4}\\) & \\(1e^{-6}\\) & \\(1e^{-6}\\) & \\(5e^{-7}\\) \\\\ Learning rate schedule & \\multicolumn{3}{c}{cosine decay} \\\\ Weight decay & \\multicolumn{3}{c}{0.1} \\\\ Gradient clip & \\multicolumn{3}{c}{1.0} \\\\ Training steps & 120k & 8k & 8k & 11k \\\\ warm steps & 6k & 400 & 400 & 550 \\\\ Global batch size & 10240 (IT);768 (IIT) & 256 & 256 & 64 \\\\ Gradient accumulation steps & 2 & 1 & 1 & 2 \\\\ Gradient ACC. & 2 & 1 & 1 & 2 \\\\ Numerical precision & \\multicolumn{3}{c}{bf0at16} \\\\ Gradient checkpointing & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\checkmark\\) & \\(\\times\\) \\\\ Deepspeed Zero Stage & \\multicolumn{3}{c}{2} \\\\ Training resource & \\multicolumn{3}{c}{64 NVIDIA A100-SXM-80GB} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 트레이닝 구성에 대한 상세사항. 두 번째 단계에서는 해상도 448을 지원하기 위해 원점 ViT를 확장하기 위해 쌍선형 보간법을 사용한다. IT는 이미지 텍스트 쌍을 의미한다. 그리고 IIT는 인터리빙된 이미지 텍스트 시퀀스를 의미한다.\n' +
      '\n' +
      '그림 7: TextVQA 샘플의 예, 미묘한 시각적 지각의 복잡한 조사를 예시한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c} \\hline Task & Dataset & Description & Split & Metric \\\\ \\hline \\multirow{4}{*}{General VQA} & VQAV2 & VQA on natural images & test-dev & VQA Score(\\(\\uparrow\\)) \\\\  & OKVQA & VQA on natural images but best world knowledge & val & VQA Score(\\(\\uparrow\\)) \\\\  & IeonQA & Abstract diagram understanding and visual language reasoning & test & EM(\\(\\uparrow\\)) \\\\  & GQA & VQA on scene understanding and reasoning & test-dev & EM(\\(\\uparrow\\)) \\\\  & ScienceQA & Multimodal multi choice VQA on science filed & test & Accuracy(\\(\\uparrow\\)) \\\\ \\hline \\multirow{4}{*}{Text-oriented VQA} & TextVQA & VQA about text in natural scene & val & VQA Score(\\(\\uparrow\\)) \\\\  & OCRVQA & VQA on images of book covers & val & EM(\\(\\uparrow\\)) \\\\  & STVQA & VQA covering reading and reasoning about text & test & ANLS(\\(\\uparrow\\)) \\\\  & DocVQA & VQA on images from documents & test & ANLS(\\(\\uparrow\\)) \\\\ \\hline \\multirow{4}{*}{Other Benchmarks} & MME & Evaluation for MLLM on perception and cognition & Perception and Cognition & Accuracy(\\(\\uparrow\\)) \\\\  & MM-VET & Dialog style VQA on integrated ability & test & GPT-4 score(\\(\\uparrow\\)) \\\\  & MMbench & Comprehensive evaluation with multi choice VQA & test & Accuracy(\\(\\uparrow\\)) score(\\(\\uparrow\\)) \\\\  & POPE & Object hallucination in MLLM & adversarial & F1 score(\\(\\uparrow\\)) \\\\  & InfMM & Complex Open-ended Reasoning & test & GPT-4 score(\\(\\uparrow\\)) \\\\  & MMMU & College-level multi choice VQA & val & Accuracy(\\(\\uparrow\\)) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 테스트 데이터세트에 대한 세부사항.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>