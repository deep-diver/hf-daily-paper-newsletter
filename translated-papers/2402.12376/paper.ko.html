<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# FiT : 확산모델용 유연비전 트랜스포머\n' +
      '\n' +
      ' 루제유\n' +
      '\n' +
      'Zidong Wang\n' +
      '\n' +
      '상하이 인공지능 연구소\n' +
      '\n' +
      '상하이자오동대학교\n' +
      '\n' +
      'Tsinghua University\n' +
      '\n' +
      'Sydney University\n' +
      '\n' +
      '홍콩 대학 대응: Lei Bai <baisanshi@gmail.com>.\n' +
      '\n' +
      'Di Huang\n' +
      '\n' +
      'Chengyue Wu\n' +
      '\n' +
      'Xihui Liu\n' +
      '\n' +
      'Wanli Ouyang\n' +
      '\n' +
      'Lei Bai\n' +
      '\n' +
      '동등기여 상하이 인공지능 연구소\n' +
      '\n' +
      '상하이자오동대학교\n' +
      '\n' +
      'Tsinghua University\n' +
      '\n' +
      'Sydney University\n' +
      '\n' +
      '홍콩 대학 대응: Lei Bai <baisanshi@gmail.com>.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '_Nature는 무한히 resolution-free_이다. 이러한 현실의 맥락에서, 확산 트랜스포머와 같은 기존의 확산 모델은 종종 훈련된 도메인 외부에서 이미지 해상도를 처리할 때 도전에 직면한다. 이러한 한계를 극복하기 위해, 우리는 제한된 해상도와 종횡비를 갖는 이미지를 생성하기 위해 특별히 설계된 트랜스포머 아키텍처인 Flexible Vision Transformer(FiT)를 제시한다. FiT는 이미지를 정적 해상도의 그리드로 인식하는 전통적인 방법과 달리 이미지를 동적 크기의 토큰의 시퀀스로 개념화한다. 이러한 관점은 훈련 및 추론 단계 모두에서 다양한 종횡비에 효과적으로 적응하는 유연한 훈련 전략을 가능하게 하여 해상도 일반화를 촉진하고 이미지 크롭에 의해 유발되는 편향을 제거한다. 세심하게 조정된 네트워크 구조와 훈련 없는 외삽 기술의 통합으로 강화된 FiT는 해상도 외삽 생성에서 현저한 유연성을 나타낸다. 종합 실험은 광범위한 해상도에 걸쳐 FiT의 탁월한 성능을 보여주며, 훈련 해상도 분포 내 및 그 이상의 효과를 보여준다. 저장소는 [https://github.com/whlzy/FiT](https://github.com/whlzy/FiT)에서 사용할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '현재 이미지 생성 모델은 임의 해상도에 걸쳐 일반화하는 데 어려움을 겪고 있다. 디퓨전 트랜스포머(DiT)(Peebles and Xie, 2023) 패밀리는 특정 해상도 범위 내에서 탁월하지만 다양한 해상도의 이미지를 처리할 때 부족하다. 이러한 한계는 DiT가 훈련 과정에서 동적 해상도 이미지를 활용할 수 없어 다양한 토큰 길이 또는 해상도에 효과적으로 적응하는 능력을 저해한다는 사실에서 비롯된다.\n' +
      '\n' +
      '이러한 한계를 극복하기 위해, 우리는 제한된 해상도와 종횡비_에서 이미지를 생성하는 데 능숙한 **Flexible Vision Transformer(FiT)**를 소개한다. 핵심 동기는 이미지 데이터 모델링에 대한 새로운 관점이다: 이미지를 고정된 차원의 정적 그리드로 취급하기보다는 FiT는 이미지를 가변 길이 토큰의 시퀀스로 개념화한다. 이 접근법은 FiT가 시퀀스 길이를 동적으로 조정할 수 있게 하여, 미리 정의된 치수에 의해 제약을 받지 않고 임의의 원하는 해상도에서 이미지의 생성을 용이하게 한다. 가변 길이 토큰 시퀀스를 효율적으로 관리하고 최대 지정된 길이로 패딩함으로써 FiT는 해상도 독립적인 이미지 생성 가능성을 잠금 해제한다. FiT는 **유연한 훈련 파이프라인**, **네트워크 아키텍처** 및 **추론 프로세스**에서 상당한 발전을 통한 이러한 패러다임 변화를 나타낸다.\n' +
      '\n' +
      '**Flexible Training Pipeline.** FiT는 이미지를 토큰들의 시퀀스로서 봄으로써, 트레이닝 동안 원래의 이미지 종횡비를 고유하게 보존한다. 이 독특한 관점은 FiT가 고해상도 이미지를 미리 정의된 최대 토큰 한계 내에 맞도록 적응적으로 크기를 조정할 수 있도록 하여 원래 해상도에 관계없이 이미지가 크롭되거나 불균형적으로 스케일링되지 않도록 한다. 이 방법은 이미지 해상도의 무결성이 도 2에 도시된 바와 같이 유지되는 것을 보장하여, 다양한 해상도에서 고충실도의 이미지를 생성하는 능력을 용이하게 한다. 우리가 아는 한, FiT는 훈련 내내 다양한 이미지 해상도를 유지하는 최초의 변압기 기반 생성 모델이다.\n' +
      '\n' +
      '**네트워크 아키텍처.** FiT 모델은 DiT 아키텍처로부터 진화하지만 해상도 외삽의 한계를 해결한다. 다양한 이미지 크기를 처리하기 위한 하나의 필수적인 네트워크 아키텍처 조정은 길이 외삽을 위한 대형 언어 모델(LLM)을 향상시키는 성공에 영감을 받은 2D Rotary Positional Embedding(RoPE)(Su et al., 2024)의 채택이다(Liu et al., 2023). 또한 기존의 다층 퍼셉트론(MLP) 대신 Swish-Gated Linear Unit(SwiGLU)(Shazeer, 2020)을 소개하고, 유연한 훈련 파이프라인 내에서 패딩 토큰을 효율적으로 관리하기 위해 DiT의 Multi-Head Self-Attention(MHSA)을 Masked MHSA로 대체한다.\n' +
      '\n' +
      '**Inference Process.** 대형 언어 모델들은 임의의 길이의 텍스트를 생성하기 위해 토큰 길이 외삽 기법(Peng et al., 2023; LocalLLaMA)을 채용하지만, FiT에 이들 기술을 직접 적용하는 것은 차선의 결과를 산출한다. 우리는 이러한 기술을 2D RoPE에 맞게 조정하여 해상도와 종횡비의 스펙트럼에 걸쳐 FiT의 성능을 향상시킨다.\n' +
      '\n' +
      '가장 높은 Gflop FiT-XL/2 모델은 _ImageNet-256_(Deng et al., 2009) 데이터셋에서 180만 단계만을 학습한 후, \\(160\\times 320\\), \\(128\\times 384\\), \\(320\\times 320\\), \\(224\\times 448\\), \\(160\\times 480\\)의 해상도에서 모든 최신 CNN 및 변압기 모델보다 상당한 성능 향상을 보였다. FiT-XL/2의 성능은 훈련 없는 해상도 외삽 방법으로 훨씬 더 발전한다. 7백만 단계의 기준 DiT-XL/2 훈련에 비해 FiT-XL/2는 \\(256\\times 256\\)의 해상도에서 약간 지연되지만 다른 모든 해상도에서 상당히 능가한다.\n' +
      '\n' +
      '요약하면, 우리의 기여는 확산 모델에 맞춘 유연한 비전 트랜스포머인 FiT의 새로운 도입에 있으며, 모든 해상도와 종횡비로 이미지를 생성할 수 있다. FiT에서 크롭이 필요 없는 유연한 훈련 파이프라인, 동적 토큰 길이 모델링을 위한 고유한 변압기 아키텍처, 임의 해상도 생성을 위한 훈련 없는 해상도 외삽 방법을 포함한 세 가지 혁신적인 설계 기능을 제시한다. 엄격한 실험은 FiT-XL/2 모델이 다양한 해상도 및 종횡비 설정에서 최첨단 성능을 달성한다는 것을 보여준다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**Diffusion Models.** Denoising Diffusion Probabilistic Models (DPM) (Ho et al., 2020; Saharia et al., 2022; Radford et al., 2021) 및 스코어 기반 모델들 (Hyvarinen and Dayan, 2005; Song et al., 2020)은 이미지 생성 태스크들의 맥락에서 현저한 진전을 나타냈다. DDIM(Denoising Diffusion Implicit Model) Song et al.(2020)은 가속 샘플링 절차를 제공한다. LDMs(Latent Diffusion Models)(Rombach et al., 2022)은 잠재 공간에서의 잡음 과정을 역전시키기 위한 심층 생성 모델 훈련의 새로운 벤치마크를 구축한다.\n' +
      '\n' +
      '도 2: (a) DiT와 (b) FiT 사이의 파이프라인 비교.\n' +
      '\n' +
      'VQ-VAE의 Use(Esser et al., 2021).\n' +
      '\n' +
      '**Transformer Models.** Transformer 모델(Vaswani et al., 2017), 언어(Brown et al., 2020; Chowdhery et al., 2023), 비전(Dosovitskiy et al., 2020), 및 멀티 모달리티(Team et al., 2023)를 포함하지만 이에 제한되지 않는 다양한 분야에서 도메인-특정 아키텍처를 성공적으로 대체하였다. 시지각 연구에서 해결에 초점을 맞춘 대부분의 노력(Touvron et al., 2019, 2021; Liu et al., 2021, 2022)은 고정적이고 낮은 해상도를 사용하여 사전 훈련을 가속화하는 것을 목표로 한다. 한편, NaViT(Dehghani et al., 2023)는 그들의 자연적, \'네이티브\' 해상도에서 이미지를 이용하여 ViT를 훈련시키기 위해 \'Patch n\' Pack\' 기법을 구현한다. 특히, 트랜스포머는 영상을 합성하기 위해 잡음제거 확산 확률 모델(Ho et al., 2020)에서도 탐색되었다. DiT(Peebles and Xie, 2023)는 LDM의 백본으로 비전 트랜스포머를 활용하고 강력한 베이스라인 역할을 할 수 있는 중요한 작업이다. DiT 아키텍처에 기초하여, MDT(Gao et al., 2023)는 훈련 및 추론에서 두 개의 전방-런을 필요로 하는 마스킹된 잠재 모델링 접근법을 도입한다. U-ViT(Bao et al., 2023)는 모든 입력을 토큰으로 취급하고 U-Net 아키텍처를 LDM의 ViT 백본에 통합한다. BifT(Hatamizadeh et al., 2023)는 확산 프로세스의 상이한 스테이지들에 적응하기 위해 시간-의존적 자기-집중 모듈을 DiT 백본에 도입한다. 우리는 위의 방법들의 LDM 패러다임을 따르고 또한 새로운 유연한 이미지 합성 파이프라인을 제안한다.\n' +
      '\n' +
      '**LMs.** RoPE(Rotary Position Embedding)(Su et al., 2024)에서의 길이 외삽은 상대 위치 정보를 절대 위치 임베딩에 통합하는 신규한 위치 임베딩이다. 최근 광범위한 LLM(Large Language Model) 설계에서 지배적인 위치 임베딩이 되고 있다(Chowdhery et al., 2023; Touvron et al., 2023;). RoPE는 시퀀스 길이의 유연성과 같은 귀중한 속성을 즐기지만, 입력 시퀀스가 트레이닝 길이를 초과할 때 성능이 떨어진다. 이 문제를 해결하기 위해 많은 접근법이 제안되었다. PI(Position Interpolation)(Chen et al., 2023)는 원래의 컨텍스트 윈도우 크기와 일치하도록 입력 위치 인덱스들을 선형적으로 하향 스케일링하는 반면, NTK-aware(LocalL-LaMA)는 RoPE의 회전 베이스를 변경한다. YaRN(Yet another RoPE extensioN)(Peng et al., 2023)은 컨텍스트 윈도우를 효율적으로 확장하기 위한 개선된 방법이다. 랜덤PE(Ruoss et al., 2023)는 트레이닝 또는 추론에서 원래 관찰된 것보다 훨씬 더 큰 범위의 위치들로부터 정렬된 위치들의 세트를 서브샘플링한다. xPos(Sun et al., 2022)는 RoPE에 장기 붕괴를 통합하고 더 나은 외삽 성능을 위해 블록 인과적 주의를 사용한다. 우리의 연구는 비전 생성 및 즉석 해상도 외삽 방법에서 RoPE의 구현에 대해 깊이 조사한다.\n' +
      '\n' +
      '##3 확산용 플렉시블 비전 트랜스포머\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      '**1-D RoPE(Rotary Positional Embedding)**(Su et al., 2024)는 절대적 및 상대적 PE를 단일화하는 포지션 임베딩의 유형으로서, LLM들에서 일정 정도의 외삽 능력을 나타낸다. m번째 키와 n번째 질의 벡터가 \\(\\mathbf{q}_{m},\\mathbf{k}_{n}\\in\\mathbb{R}^{|D|}\\)으로 주어지면, 1-D RoPE는 복소 벡터 공간에서 키 또는 질의 벡터에 바이어스를 곱한다:\n' +
      '\n' +
      '\\[f_{q}(\\mathbf{q}_{m},m)=e^{im\\theta}\\mathbf{q}_{m},\\quad f_{k}(\\mathbf{k}_{n},n)=e^{in\\theta}\\mathbff{k}_{n}\\tag{1}\\t.\n' +
      '\n' +
      '여기서 \\(\\theta=\\mathrm{Diag}(\\theta_{1},\\cdots,\\theta_{|D|/2})\\)는 \\(\\theta_{d}=b^{-2d/|D|}\\) 및 회전 베이스 \\(b=10000\\)을 갖는 회전 주파수 매트릭스이다. 실제 공간에서 \\(l=|D|/2\\)이 주어지면 회전 행렬 \\(e^{im\\Theta}\\)은 다음과 같다.\n' +
      '\n' +
      '\\cos m\\theta_{1}&-\\sin m\\theta_{1}&\\cdots&0\\\\theta_{1}&\\cos m\\theta_{1}&\\cdots&0\\\\vdots&\\vdots&\\vdots&\\vdots\\\\\\0&\\cdots&\\cos m\\theta_{l}&\\cos m\\theta_{l}&\\cos m\\theta_{l}\\\\end{bmatrix}\\tag{2}\\cdots&\\cos m\\theta_{l}&\\cos m\\theta_{l}&\\cdots&\\cos m\\theta_{l}&\\tag{2}\\cdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\cos m\\theta_{l}&\\cos m\\theta_{l}&\\cos m\\theta_{l}&\\cdots&\\cos m\\theta_{l}&\\\n' +
      '\n' +
      '1-D RoPE를 갖는 주의 스코어는 다음과 같이 계산된다:\n' +
      '\n' +
      '\\[A_{n}=\\mathrm{Re}(f_{q}(\\mathbf{q}_{m},m),f_{k}(\\mathbf{k}_{n},n)) \\tag{3}\\]\n' +
      '\n' +
      '**NTK-aware Interpolation**(LocalLLaMA)는 LLMs에서 훈련 없는 길이 외삽 기법이다. 최대 학습 길이\\(L_{\\text{train}\\)보다 큰 문맥 길이\\(L_{\\text{test}}\\)을 다루기 위해, 1-D RoPE의 회전 베이스를 다음과 같이 수정한다:\n' +
      '\n' +
      '\\[b^{\\prime}=b\\cdot s^{\\frac{|D|}{|D|-2}, \\tag{4}\\]\n' +
      '\n' +
      '여기서 스케일 팩터 \\(s\\)는 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[s=\\max(\\frac{L_{test}}{L_{train}},1.0). \\tag{5}\\]\n' +
      '\n' +
      '**YaRN(Yet another RoPE extensioN) Interpolation**(Peng et al., 2023)는 치수 \\(d\\)의 비를 \\(r(d)=L_{\\text{train}}/(2\\pi b^{2d/|D|})로 도입하고, 회전 주파수를 다음과 같이 수정한다:\n' +
      '\n' +
      '\\[\\theta_{d}^{\\prime}=(1-\\gamma(r(d))))\\,\\frac{\\theta_{d}}{s}+\\gamma(r(d))\\theta_{d}, \\tag{6}\\.\n' +
      '\n' +
      '여기서 \\(s\\)은 전술한 스케일 팩터이고, \\(\\gamma(r(d))\\)는 여분의 하이퍼-파라미터 \\(\\alpha,\\beta\\)를 갖는 램프 함수이다:\n' +
      '\n' +
      '\\begin{cases}0,\\gamma(r)=\\begin{cases}0,\\text{if }r<\\alpha\\\\1,&\\text{if }r>\\beta\\\\frac{r-\\alpha\\beta\\alpha},&\\text{otherwise}.\\\\\\end{cases}\\tag{7}\\text{if }r<\\alpha\\1,\\text{if \\frac{r-\\alpha\\alpha\\alpha\\\\text{otherwise}\n' +
      '\n' +
      '또한, 1D-RoPE 스케일링 용어를 다음과 같이 통합한다:\n' +
      '\n' +
      '\\frac{1}{\\sqrt{t}f_{q}(\\mathbf{q}_{m},m)=\\frac{1}{\\sqrt{t}f_{q}(\\mathbf{q}_{m},m),f_{k}^{\\prime}(\\mathbf{k}_{n},n)=\\frac{1}{\\sqrt{t}f_{k}(\\mathbf{k}_{n},n), \\tag{8}\\tag{8}}\n' +
      '\n' +
      'where \\(\\frac{1}{\\sqrt{t}}=0.1\\ln(s)+1\\).\n' +
      '\n' +
      '유연한 훈련과 추론 파이프라인\n' +
      '\n' +
      'GPU 하드웨어의 특성에 제약을 받는 현대 딥러닝 모델은 병렬 처리를 위해 데이터를 균일한 모양의 배치로 패킹해야 한다. 이미지 해상도의 다양성으로 인해 그림 1과 같다. 4, DiT는 고정된 해상도 \\(256\\times 256\\)로 이미지를 조정하고 작도한다. 데이터 증강의 수단으로 크기를 조정하고 자르는 것이 일반적인 관행이지만 이 접근법은 입력 데이터에 특정 편향을 도입한다. 이러한 편향은 저해상도로부터 고해상도로의 전환으로부터 블러링 효과들 및 크로핑으로 인해 손실된 정보를 포함하여 모델에 의해 생성된 최종 이미지들에 직접적으로 영향을 미칠 것이다(더 많은 고장 샘플들이 D에서 발견될 수 있다).\n' +
      '\n' +
      '이를 위해 Fig.와 같이 유연한 훈련 및 추론 파이프라인을 제안한다. 3(a,b)_ 전처리 단계에서, 우리는 이미지를 크롭하거나 저해상도 이미지를 더 높은 해상도로 리사이징하는 것을 피한다. 대신, 우리는 고해상도 이미지의 크기를 소정의 최대 해상도 한계인 \\(HW\\leqslant 256^{2}\\)까지만 조정한다. _ 트레이닝 페이즈_에서 FiT는 먼저 미리 트레이닝된 VAE 인코더로 이미지를 잠재 코드로 인코딩한다. 잠재 토큰에 잠재 코드를 패치플라잉함으로써 서로 다른 길이를 갖는 시퀀스를 얻을 수 있다. 이러한 시퀀스를 일괄적으로 패킹하기 위해 패딩 토큰을 사용하여 모든 시퀀스를 최대 토큰 길이\\(L_{max}\\)로 패딩한다. 여기에서 우리는 DiT의 고정된 토큰 길이와 일치하도록 \\(L_{max}=256\\)을 설정했다. 잠재 토큰과 마찬가지로 위치 임베딩도 패킹의 최대 길이로 패딩한다. 마지막으로, 우리는 잡음 제거된 출력 토큰에 대해서만 손실 함수를 계산하는 반면, 다른 모든 패딩 토큰은 폐기한다. _ 추론 phase_에서, 먼저 생성된 이미지의 위치 맵과 가우시안 분포로부터 샘플 잡음 토큰들을 입력으로 정의한다. 디노이징 프로세스의 \\(K\\) 반복을 완료한 후, 미리 정의된 위치 맵에 따라 디노이징된 토큰을 재구성하고 언패치하여 최종 생성된 이미지를 얻는다.\n' +
      '\n' +
      '유연한 비전 트랜스포머 아키텍처\n' +
      '\n' +
      '유연한 훈련 파이프라인을 기반으로, 우리의 목표는 그림 3(c)와 같이 다양한 해상도에 걸쳐 안정적으로 훈련하고 임의의 해상도와 종횡비를 갖는 이미지를 생성할 수 있는 아키텍처를 찾는 것이다. LLM의 몇 가지 중요한 아키텍처 발전에 힘입어 우리는 DiT를 기반으로 한 아키텍처 수정을 탐구하기 위한 일련의 실험을 수행하고 섹션 4.2의 세부 사항을 참조한다.\n' +
      '\n' +
      '**MHSA를 Masked MHSA로 대체하는.** 유연한 트레이닝 파이프라인은 동적 시퀀스들을 배치로 유연하게 패킹하기 위한 패딩 토큰들을 도입한다. 트랜스포머의 정방향 단계에서, 잡음 토큰과 패딩 토큰 사이의 상호 작용을 방지하면서 잡음 토큰 간의 상호 작용을 촉진하는 것이 중요하다. 오리지널 DiT의 MHSA(Multi-Head Self-Attention) 메커니즘은 잡음 토큰과 패딩 토큰을 구별할 수 없다. 이를 위해 Masked MHSA를 사용하여 표준 MHSA를 대체한다. 우리는 Masked Attention을 위해 시퀀스 마스크 \\(M\\)을 사용하는데, 여기서 잡음 토큰은 \\(0\\)의 값을 할당받고, 패딩 토큰은 음의 무한대 (_-inf_)의 값을 할당받는데, 이는 다음과 같이 정의된다:\n' +
      '\n' +
      '[\\text{Masked Attn.}(Q_{i},K_{i},V_{i})=\\text{Softmax}\\left(\\frac{Q_{i}K_{i}^{T}{\\sqrt{d_{k}}+M\\right)V_{i}\\tag{9}\\trac{Q_{i}K_{i}^{T}{\\sqrt{d_{k}}+M\\right)\n' +
      '\n' +
      '여기서 \\(Q_{i}\\), \\(K_{i}\\), \\(V_{i}\\)는 \\(i\\)번째 헤드에 대한 쿼리, 키 및 값 행렬이다.\n' +
      '\n' +
      '**절대 PE를 2D RoPE로 대체.** 절대 위치 임베딩을 갖는 비전 트랜스포머 모델이 훈련 해상도를 넘어 이미지에 잘 일반화되지 못하는 것을 관찰한다. 섹션 4.3 및 4.5에서와 같이 LLM에서 1D-RoPE의 성공에 의해 영감을 받아 길이 외삽(Liu et al., 2023), 우리는 2D-RoPE를 활용하여 해상도를 용이하게 한다.\n' +
      '\n' +
      '도 3: (a) 가요성 트레이닝 파이프라인, (b) 가요성 추론 파이프라인, 및 (c) FiT 블록의 개요.\n' +
      '\n' +
      '비전 트랜스포머 모델의 일반화 형식적으로, 우리는 높이와 너비의 좌표에 대한 1-D RoPE를 별도로 계산한다. 그런 다음 이러한 두 개의 1-D RoPE가 마지막 차원에서 연결된다. 폭과 높이의 2차원 좌표가 \\(\\{(w,h)\\big{|}1\\leqslant w\\leqslant W,1\\leqslant h\\leqslant H\\}\\)으로 주어지면, 2차원 RoPE는 다음과 같이 정의된다.\n' +
      '\n' +
      '{split}&f_{q}(\\mathbf{q}_{m},h_{m},w_{m})=[e^{ih_{m}\\theta}\\mathbf{q}{iw_{m}\\theta}\\mathbf{q}{m}],\\\\\\&f_{k}(\\mathbf{k}_{n},h_{n},w_{n})=[e^{ih_{n}\\theta}\\mathbf{k}\\theta}\\mathbf{k}\\theta}\\mathbf{k}\\text{n}],\\end{split}\\tag{10}\\text{10}\\text{n}\\mathbf{n}\\mathbf{n}\\mathbf{n}\\mathbf{n}\\mathbf{n}\\mathbf{n}\\mathbf{n}\\mathbf{n}\\mathbf{n}\\mathbf{n}\\mathbf\n' +
      '\n' +
      '여기서 \\(\\theta=\\mathrm{Diag}(\\theta_{1},\\cdots,\\theta_{|D|/4})\\) 및 \\(\\parallel\\)은 마지막 차원에서 두 벡터를 연결한 것을 나타낸다. 우리는 1차원 RoPE에서 \\(|D|/2\\)-dimension 부공간과 다른 차원의 일관성을 보장하기 위해 \\(|D|/4\\)-dimension 부공간으로 \\(|D|/4\\)-dimension 공간을 나눈다. 유사하게, 2-D RoPE와의 주의 점수는:\n' +
      '\n' +
      '\\[A_{n}=\\mathrm{Re}\\langle f_{q}(\\mathbf{q}_{m},h_{m},w_{m}),f_{k}(\\mathbf{k}_{n},h_{n},w_{n})\\rangle.\\tag{11}\\\\tag{11}\\\n' +
      '\n' +
      '주목할 점은 2D-RoPE에서 \\(h\\)와 \\(w\\) 사이의 교차항과 주의점수 \\(A_{n}\\)이 없기 때문에 회전주파수를 \\(\\Theta_{h}\\)과 \\(\\Theta_{w}\\)으로 더 분리할 수 있으며, 이는 섹션 3.4에서 논의되고 B에서 더 자세한 내용을 찾을 수 있다.\n' +
      '\n' +
      '**MLP를 SwiGLU로 대체.** LLaMA(Touvron et al., 2023a,b)와 같은 최근의 LLM을 따르고, FFN 내의 MLP를 SwiGLU로 대체하며, 이는 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[\\begin{split}&\\text{SwiGLU}(x,W,V)=\\text{SiLU}(xW)\\otimes(xV) \\\\&\\text{FFN}(x)=\\text{SwiGLU}(x,W_{1},W_{2})W_{3}\\end{split}\\tag{12}\\times(x,W,V)=\\text{SiLU}(x,W,V)=\\text{FFN}(x)=\\text{SwiGLU}(x,W_{1},W_{2})W_{3}\\end{split}\\tag{12}\\times(x,W,V)=\\text{SiLU}(x,W,V)=\\text{SiLU}(x,W,V)=\\text{FFN}(x)=\\text{SwiGLU}(x,W_{1},W_{2})W_{3}\\end{split}\\tag{12}\\text\n' +
      '\n' +
      '여기서 \\(\\otimes\\)은 Hadmard Product를 나타내고, \\(W_{1}\\), \\(W_{2}\\), \\(W_{3}\\)은 bias가 없는 가중치 행렬이며, \\(\\text{SiLU}(x)=x\\otimes\\sigma(x)\\이다. 여기서 우리는 각 FFN 블록에서 우리의 선택으로 SwiGLU를 사용할 것이다.\n' +
      '\n' +
      '훈련 자유 해상도 외삽법\n' +
      '\n' +
      '우리는 추론 해상도를 (\\(H_{\\text{test}\\), \\(W_{\\text{test}\\))로 나타낸다. FiT는 훈련 시 다양한 해상도와 종횡비를 처리할 수 있으므로 훈련 해상도를 \\(L_{\\text{train}}=\\sqrt{L_{\\text{max}}})로 표시한다.\n' +
      '\n' +
      '식 (5)의 스케일 팩터를 \\(s=\\max(\\max(H_{\\text{test}},W_{\\text{test}})/L_{train},1.0)\\으로 변경함으로써, 바닐라 NTK 및 YaRN 구현이라고 하는 2D-RoPE 상의 대규모 언어 모델 외삽에서 위치 보간 방법을 직접 구현할 수 있다. 또한, 디커플링된 2D-RoPE에서 디커플링 속성을 이용하여 비젼 RoPE 보간 방법을 제안한다. 우리는 식 (10)을 다음과 같이 수정한다.\n' +
      '\n' +
      '\\hat{f}_{q}(\\mathbf{q}_{m},h_{m},w_{m})=[e^{ih_{m}\\theta_{m}\\theta_{w}\\mathbf{q}{m}\\mathbf{q}{m}\\mathbf{q}{m}\\mathbf{k}{n}(\\mathbf{k}\\theta_{m}\\mathbf{k}\\theta_{m}\\mathbf{k}\\theta_{m}\\mathbf{k}\\text{m}\\mathbf{k}\\text{m}\\mathbf{k}\\text{m}\\mathbf{k}\\text{m}\\mathbf{k}\\text{m}\\mathbf{k}\\text{m}\\mathbf{k}\\text{m}\\mathbf{k}\\text{m}\\mathbf{k}\\text{m}\\mathbf{k}\n' +
      '\n' +
      '여기서 \\(\\theta_{h}=\\{\\theta_{d}^{h}=b_{h}^{-2d/|D|},1\\leqslant d\\leqslant\\frac{|D|}{2}\\}) 및 \\(\\theta_{w}=\\{\\theta_{d}^{w}=b_{w}^{-2d/|D|},1\\leqslant d\\leqslant\\frac{|D|}{2}\\}\\)은 별도로 계산된다. 따라서, 높이와 너비의 스케일 팩터는 다음과 같이 별도로 정의된다.\n' +
      '\n' +
      '\\[s_{h}=\\max(\\frac{H_{\\text{test}}{L_{\\text{train}},1.0),\\quad s_{w}=\\max(\\frac{W_{\\text{test}}{L_{\\text{train}},1.0). \\tag{14}\\tag{14}}\n' +
      '\n' +
      '**Definition 3.1**.: _The Definition of VisionNTK Interpolation is modification of NTK-aware Interpolation with Equation (13) with the following rotary base._\n' +
      '\n' +
      '\\[b_{h}=b\\cdot s_{h}^{\\frac{|D|}{|D|-2}},\\quad b_{w}=b\\cdot s_{w}^{\\frac{|D|}{|D|-2}, \\tag{15}\\}}\n' +
      '\n' +
      '여기서 \\(b=10000\\)은 식 (1)과 동일하다.\n' +
      '\n' +
      '**Definition 3.2**.: _The Definition of VisionYaRN Interpolation is modification of YaRN Interpolation by Equation (13) with the following rotary frequency._\n' +
      '\n' +
      '{split}&\\theta_{d}^{h}=(1-\\gamma(r(d))\\frac{\\theta_{d}}{s_{h}+ \\gamma(r(d))\\theta_{d}^{w}=(1-\\gamma(r(d))\\frac{\\theta_{d}}{s_{w}+\\gamma(r(d))\\theta_{d}\\theta_{d}\\theta_{d}+\\gamma(r(d))\\theta_{d}\\theta_{d}\\theta_{d},\\end{split}\\tag{16}\\tamma(r(d))\n' +
      '\n' +
      '여기서 \\(\\gamma(r(d))\\)는 식 (6)과 동일하다.\n' +
      '\n' +
      'VisionNTK와 VisionYaRN은 외삽에서 분포에서 벗어난 위치 임베딩의 문제를 완화하기 위해 사용되는 훈련 없는 위치 임베딩 보간 접근법이라는 점에 주목할 필요가 있다. 종횡비가 1과 같을 때 NTK 및 YaRN의 바닐라 구현과 동일하다. 이들은 특히 임의의 종횡비를 갖는 이미지를 생성하는데 효과적이다. 섹션 4.3을 참조한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### FiT Implementation\n' +
      '\n' +
      '우리는 모델 아키텍처, 훈련 세부 사항 및 평가 메트릭을 포함한 FiT의 구현 세부 사항을 제시한다.\n' +
      '\n' +
      '**모델 아키텍처.** 기본 모델 FiT-B 및 xlarge 모델 FiT-XL에 대해 동일한 레이어, 숨겨진 크기 및 주의 헤드를 설정하기 위해 DiT-B 및 DiT-XL을 따른다. 더 작은 패치 크기를 사용할 때 DiT가 더 강한 합성 성능을 나타내므로, FiT-B/2 및 FiT-XL/2로 표시된 패치 크기 p=2를 사용한다. FiT는 이미지/잠재 토큰을 인코딩/디코딩하기 위해 Stable Diffusion(Rombach et al., 2022)에서 제공하는 DiT와 동일한 Off-the-shelf 사전 훈련된 VAE(Esser et al., 2021)를 채택한다. VAE 인코더는 다운샘플링 비율이 \\(1/8\\)이고 특징 채널 치수가 \\(4\\)이다. 크기 \\(160\\times 320\\times 3\\)의 이미지를 크기 \\(20\\times 40\\times 4\\)의 잠재 코드로 인코딩한다. 크기\\(20\\times 40\\times 4\\)의 잠재코드는 길이\\(L=10\\times 20\\times 200\\)의 잠재토큰으로 패치된다.\n' +
      '\n' +
      '**Training details.** we train class-conditional latent FiT models under predetermined maximum resolution limitation, \\(HW\\leqslant 256^{2}\\)(equivalent to token length \\(L\\leq 256\\)), the _ImageNet_(Deng et al., 2009) dataset. 고해상도 영상은 종횡비를 유지하면서 \\(HW\\leqslant 256^{2}\\)의 한계를 만족하도록 축소하였다. 수평 플립 증강을 사용하기 위해 DiT를 따른다. AdamW(Loshchilov and Hutter, 2017)를 사용하여 일정한 학습률\\(1\\times 10^{-4}\\), 무게감퇴가 없고 배치크기가 \\(256\\)인 DiT와 동일한 학습설정을 사용한다. 생성 모델링 문헌의 일반적인 관행에 따라, 우리는 0.9999의 감쇠를 갖는 훈련보다 모델 가중치의 지수 이동 평균(EMA)을 채택하며 모든 결과는 EMA 모델을 사용하여 보고된다. 우리는 DiT와 동일한 확산 하이퍼 파라미터를 유지한다.\n' +
      '\n' +
      '**평가 세부사항 및 메트릭.** 일반적으로 사용되는 일부 메트릭, _i.e._ Fre\'chet Inception Distance(FID)(Heusel et al., 2017), sFID(Nash et al., 2021), Inception Score(IS)(Salimans et al., 2016), 개선된 Precision and Recall(Kynkaniemi et al., 2019)로 모델을 평가한다. 공정한 비교를 위해 DiT를 따라 ADM(Dharwal and Nichol, 2021)의 텐서플로우 평가를 사용하고 250 DDPM 샘플링 단계로 FID-50K를 보고한다. FID는 다양성과 충실도를 모두 측정하기 때문에 주요 척도로 사용된다. 우리는 추가로 IS, sFID, 정밀도 및 리콜을 2차 메트릭으로 보고한다. FiT 아키텍처 실험(섹션 4.2) 및 해상도 외삽 절제 실험(섹션 4.3)의 경우 분류기 없는 안내(Ho 및 Salimans, 2021)를 사용하지 않고 결과를 보고한다.\n' +
      '\n' +
      '**평가 해상도** 고정 종횡비 \\(1:1\\)에 대한 실험을 주로 수행한 이전 작업과 달리 각각 \\(1:1\\), \\(1:2\\) 및 \\(1:3\\)의 서로 다른 종횡비에 대한 실험을 수행했다. 한편, 실험을 학습 분포 내 해상도와 학습 분포 외 해상도로 나눈다. 분포의 해상도는 주로 \\(256\\times 256\\)(1:1), \\(160\\times 320\\)(1:2) 및 \\(128\\times 384\\)(1:3)을 사용하여 평가하며, 각각 \\(256\\), \\(200\\), \\(192\\) 잠재토큰을 사용한다. 모든 토큰 길이는 256보다 작거나 같아 훈련 분포 내에서 각각의 해상도로 이어진다. 분산해소를 위해 주로 \\(320\\times 320\\)(1:1), \\(224\\times 448\\)(1:2) 및 \\(160\\times 480\\)(1:3)을 사용하며, 각각 \\(400\\), \\(392\\), \\(300\\)의 잠재토큰을 사용한다. 모든 토큰 길이가 256보다 커서 학습 분포에서 벗어나는 해상도가 생성됩니다. 이러한 분할을 통해 다양한 해상도와 종횡비에서 FiT의 영상 합성 및 해상도 외삽 능력을 총체적으로 평가한다.\n' +
      '\n' +
      '### FiT Architecture Design\n' +
      '\n' +
      '이 부분에서는 FiT의 아키텍처 설계를 검증하기 위해 절제 연구를 수행한다. 400K 훈련 단계에서 다양한 변형 FiT-B/2 모델의 결과를 보고하고 FID-50k, sFID, IS, Precision, Recall을 평가 지표로 사용한다. 실험은 3가지 분해능, 즉 \\(256\\times 256\\), \\(160\\times 320\\), \\(224\\times 448\\)에서 수행하였다. 이러한 해상도는 서로 다른 종횡비를 포함할 뿐만 아니라 분포 안팎의 해상도를 포함하도록 선택된다.\n' +
      '\n' +
      '**유연한 훈련 vs. 고정 훈련.**_유연 훈련 파이프라인은 var var 전반에 걸친 성능을 상당히 향상시킨다\n' +
      '\n' +
      'ious resolution.__ 이러한 개선은 Tab. 3과 같이 분포 내 해상도뿐만 아니라 훈련 분포 밖의 해상도까지 확대된다. Config A는 유연한 훈련만으로 원래의 DiT-B/2 모델로 \\(256\\times 256\\) 해상도에서 고정 해상도 훈련으로 DiT-B/2에 비해 성능(-1.49 FID)이 약간 향상된다. 구성 A는 유연한 훈련을 통해 상당한 성능 향상을 보여준다. DDiT-B/2에 비해, FID 점수는 해상도 \\(160\\times 320\\)와 \\(224\\times 448\\)에서 각각 \\(40.81\\)과 56.55로 감소하였다.\n' +
      '\n' +
      'SwiGLU vs. MLP.**_SwiGLU는 MLP._swiGLU에 비해 다양한 해상도에 걸쳐 성능이 약간 향상된다. Config B는 MLP를 SwiGLU로 대체하는 FiT-B/2 유연 훈련 모델이다. 구성 A와 비교하여 구성 B는 다양한 해상도에서 주목할 만한 개선을 보여준다. 구체적으로, Tab에서 \\(256\\times 256\\), \\(160\\times 320\\), \\(224\\times 448\\)의 해상도에 대해 Config B는 FID 점수를 1.59, 1.85, 0.21만큼 감소시킨다. 각각 3. 그래서 FiT는 FFN에서 SwiGLU를 사용한다.\n' +
      '\n' +
      '**2D RoPE vs. 절대 PE.**_2D RoPE는 절대 위치 인코딩에 비해 더 큰 효율성을 보여주며 다양한 해상도에 걸쳐 상당한 외삽 능력을 가지고 있다._**_2D RoPE는 절대 위치 인코딩에 비해 더 큰 효율성을 보여준다. Config D는 절대 PE를 2D RoPE로 대체하는 FiT-B/2 유연 학습 모델이다. 학습 분포 내의 해상도, 특히 \\(256\\times 256\\) 및 \\(160\\times 320\\)에 대해 Config D는 탭에서 FID 점수를 6.05, 5.45만큼 감소시킨다. 3. Config A와 비교하였을 때, 훈련분포를 벗어난 해상도의 경우, Config D는 Config A와 비교하여 상당한 외삽 능력(-6.39 FID)을 보인다. Config C는 절대 PE와 2D RoPE를 모두 유지한다. 그러나 Config C와 Config D의 비교에서 우리는 Config C가 더 나쁜 성능을 보이는 것을 관찰한다. 256x256, 160x320, 224x448의 해상도의 경우, Config C는 Config D에 비해 각각 1.82, 1.65, 0.44의 FID 점수를 증가시키므로, 본 구현에서는 위치 임베딩에 2D RoPE만을 사용한다.\n' +
      '\n' +
      '**종합.**_FiT는 원본 DiT._와 비교하여 다양한 해상도 설정에서 유의하고 포괄적인 우월성을 입증한다. FiT는 다양한 구성에 걸쳐 최첨단 성능을 달성했습니다. FiT-B/2는 DiT-B/2에 비해 Tab. 3에서 가장 일반적인 해상도인 \\(256\\times 256\\)에서 FID 점수를 8.47\\만큼 감소시켰다. 또한, FiT-B/2는 해상도인 \\(160\\times 320\\)과 \\(224\\times 448\\)에서 FID 점수를 각각 47.36과 64.43만큼 감소시켰다.\n' +
      '\n' +
      '### FiT Resolution Extrapolation Design\n' +
      '\n' +
      '이 부분에서는 400K 훈련 단계에서 DiT-B/2 모델과 FiT-B/2 모델을 채택하여 세 가지 분포 외 분해능(\\(320\\times 320\\), \\(224\\times 448\\) 및 \\(160\\times 480\\)에 대한 외삽 성능을 평가한다. 직접 외삽은 훈련 분포에서 더 큰 해상도에서 잘 수행되지 않는다. 따라서 우리는 위치 임베딩 보간법에 초점을 맞춘 종합적인 벤치마킹 분석을 수행한다.\n' +
      '\n' +
      '**PI 및 EI.** PI(Position Interpolation) 및 EI(Embedding Interpolation)는 해상도 외삽을 위한 두 가지 기준 위치 임베딩 보간 방법이다. PI는 원래 좌표와 일치하도록 추론 위치 좌표를 선형 하향 스케일링한다. EI는 위치 임베딩의 크기를 조정한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c|c c c c|c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{4}{c|}{320\\(\\times\\)320 (1:1)} & \\multicolumn{4}{c|}{224\\(\\times\\)448 (1:2)} & \\multicolumn{4}{c}{160\\(\\times\\)480 (1:3)} \\\\  & **FID\\(\\downarrow\\)** & **SFD\\(\\downarrow\\)** & **IS\\(\\uparrow\\)** & **Prec.\\(\\uparrow\\)** & **Rec.\\(\\uparrow\\)** & **FID\\(\\downarrow\\)** & **SFD\\(\\downarrow\\)** & **IS\\(\\uparrow\\)** & **Prec.\\(\\uparrow\\)** & **Rec.\\(\\uparrow\\)** & **FID\\(\\downarrow\\)** & **sFID\\(\\downarrow\\)** & **sFID\\(\\downarrow\\)** & **IS\\(\\uparrow\\)** & **Prec.\\(\\uparrow\\)** & **Rec.\\(\\uparrow\\)** \\\\ \\hline DT-B & 95.47 & 108.68 & 18.38 & 0.26 & 0.40 & 109.1 & 110.71 & 14.00 & 0.18 & 0.31 & 143.8 & 122.81 & 8.93 & 0.073 & 0.20 \\\\ DT-B + EI & 81.48 & 62.25 & 20.97 & 0.25 & 0.47 & 133.2 & 72.53 & 11.11 & 0.11 & 0.29 & 160.4 & 93.91 & 7.30 & 0.054 & 0.16 \\\\ DT-B + PI & 72.47 & 54.02 & 24.15 & 0.29 & 0.49 & 133.4 & 70.29 & 11.73 & 0.11 & 0.29 & 156.5 & 93.80 & 7.80 & 0.058 & 0.17 \\\\ \\hline FiT-B & 61.35 & **30.71** & 31.01 & 0.41 & 0.53 & 44.67 & **24.09** & 37.11 & 0.49 & 0.52 & 56.81 & **22.07** & 25.25 & **0.38** & 0.49 \\\\ FiT-B + PI & 65.76 & 65.45 & 29.32 & 0.32 & 0.45 & 175.42 & 114.39 & 8.45 & 0.14 & 0.06 & 224.83 & 123.45 & 5.89 & 0.02 & 0.06 \\\\ FiT-B + YaRN & 44.76 & 38.04 & 44.70 & 0.51 & 0.51 & 82.19 & 75.48 & 29.68 & 0.40 & 0.29 & 104.06 & 27.97 & 20.76 & 0.21 & 0.31 \\\\ FiT-B + NTK & 57.31 & 31.31 & 33.97 & 0.43 & 0.55 & 45.24 & 29.38 & 38.84 & 0.47 & 0.52 & 59.19 & 26.54 & 26.01 & 0.36 & 0.49 \\\\ \\hline FiT-B + **VisionYARN** & **44.76** & 38.04 & **44.70** & **0.51** & 0.51 & **41.92** & 42.79 & **45.87** & **0.50** & 0.48 & 62.84 & 44.82 & **27.84** & 0.36 & 0.42 \\\\ FiT-B + **VisionNTK** & 57.31 & 31.31 & 33.97 & 0.43 & **0.55** & 43.84 & 26.25 & 39.22 & 0.48 & **0.52** & **56.76** & 24.18 & 26.40 & 0.37 & **0.49** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: ImageNet 상의 유통 외 해상도를 갖는 클래스 조건적 이미지 생성 벤치마킹. 400K 훈련 단계에서 FiT-B/2 및 DiT-B/2를 본 실험에 채택하였다. 분류기 없는 지침을 사용하지 않고 메트릭을 계산합니다. YaRN 및 NTK는 이러한 두 가지 방법의 바닐라 구현을 의미한다. 우리의 FiT-B/2는 VisionNTK 및 VisionYARN 방법과 결합하여 더욱 향상될 수 있는 안정적인 외삽 성능을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c c c|c c c c c|c c c c} \\hline \\hline \\multirow{2}{*}{Arch.} & \\multirow{2}{*}{Pos. Embed.} & \\multirow{2}{*}{FFN} & \\multirow{2}{*}{Train} & \\multicolumn{4}{c|}{256\\(\\times\\)256 (4.2)} & \\multicolumn{4}{c|}{160\\(\\times\\)320 (4.3)} & \\multicolumn{4}{c}{224\\(\\times\\)448 (0.0,4)} \\\\  & & **FID\\(\\downarrow\\)** & **sFID\\(\\downarrow\\)** & **IS\\(\\uparrow\\)** & **Prec.\\(\\uparrow\\)** & **Rec.\\(\\uparrow\\)** & **FID\\(\\downarrow\\)** & **sFID\\(\\downarrow\\)** & **IS\\(\\uparrow\\)** & **Prec.\\(\\uparrow\\)** & **Rec.\\(\\uparrow\\)** & **FID\\(\\downarrow\\)** & **sFID\\(\\downarrow\\)** & **IS\\(\\uparrow\\)** & **Prec.\\(\\uparrow\\)** & **Rec.\\(\\uparrow\\)** \\\\ \\hline\n' +
      '**DT-B** & Abs. PE & MLP & Fixed & 44.83 & **8.49** & 32.05 & 0.48 & **0.63** & 91.32 & 66.66 & 14.02 & 0.21 & 0.45 & 109.1 & 110.71 & 14.00 & 0.18 & 0.31 \\\\ \\hline Config A & Abs. PE & MLP & Flexible & 43.34 & 11.11 & 32.32 & 0.48 & 60.1 & 50.51 & 10.36 & 25.26 & 0.42 & 0.60 & 52.55 & 160.58 & 28.69 & 0.42 & **0.58** \\\\ Config B & Abs. PE & SwiGLU & Flexible & 41.75 & 11.53 & 34.55 & 0.49 & 0.61 & 48.66 & 10.68 & 26.76 & 0.41 & 6.60 & 52.34 & 17.73 & 30.01 & 0.41 & 0.57 \\\\ Config C & Abs. PE + 2D RoPE & MLP & Flexible & 39.11 & 0.79 & 36.35 & 0.51 & 0.64 & 1with bilinear interpolation1. Following ViT (Dosovitskiy et al., 2020), EI is used for absolute positional embedding.\n' +
      '\n' +
      '각주 1: torch.nn.functional.interpolate(pe, (h,w), method=“bilinear”)\n' +
      '\n' +
      '**NTK와 YaRN.** 스케일 팩터를 \\(s=\\max(\\max(H_{\\text{test}},W_{\\text{test}})/\\sqrt{256})\\)으로 설정하고, 섹션 3.1에서와 같이 두 방법의 바닐라 구현을 채택한다. YaRN에 대해 식 (7)에서 \\(\\alpha=1,\\beta=32\\)을 설정한다.\n' +
      '\n' +
      '**VisionNTK 및 VisionYaRN.** 이 두 방법은 정의 3.1 및 3.2에 자세히 정의되어 있다. 종횡비가 1과 같을 때, VisionNTK 및 VisionYaRN은 각각 NTK 및 YaRN과 동등하다는 점에 유의한다.\n' +
      '\n' +
      '**분석** 탭에 나와 있습니다 4에서 FiT-B/2는 더 큰 해상도로 직접 외삽할 때 안정적인 성능을 보여준다. PI와 결합하면 세 가지 해상도 모두에서 FiT-B/2의 외삽 성능이 감소한다. YaRN과 결합하면 FID 점수는 \\(320\\times 320\\)에서 16.77\\ 감소하지만 \\(224\\times 448\\) 및 \\(168\\times 480\\)에서의 성능은 감소한다. 우리의 VisionYaRN은 이러한 문제를 해결하여 YaRN에 비해 \\(224\\times 448\\)에서 \\(40.27\\)과 \\(160\\times 480\\)에서 \\(41.22\\)의 FID 점수를 줄인다. NTK 보간법은 안정적인 외삽 성능을 보이지만 \\(224\\times 448\\) 및 \\(160\\times 480\\) 해상도에서 FID 점수가 약간 증가한다. 우리의 VisionNTK 방법은 이 문제를 완화하고 세 가지 해상도 모두에서 직접 외삽 성능을 초과한다. 결론적으로, 우리의 FiT-B/2는 강한 외삽 능력을 가지고 있으며, 이는 VisionYaRN 및 VisionNTK 방법과 결합될 때 더욱 향상될 수 있다.\n' +
      '\n' +
      '그러나 DiT-B/2는 낮은 외삽 능력을 보여준다. PI와 결합하였을 때, FID 점수는 \\(320\\times 320\\) 해상도에서 72.47\\을 얻었으며, 이는 여전히 FiT-B/2에 뒤쳐진다. \\(224\\times 448\\) 및 \\(160\\times 480\\) 해상도에서 PI 및 EI 보간 방법은 외삽 성능을 향상시킬 수 없다.\n' +
      '\n' +
      '### FiT 배포 중 해결 결과\n' +
      '\n' +
      '이전 분석에 이어 최고 Gflops 모델인 FiT-XL/2를 1.8M 단계에 대해 훈련한다. FiT의 성능을 평가하기 위해 3가지 분포 해상도에서 \\(256\\times 256\\), \\(160\\times 320\\), \\(128\\times 384\\)의 실험을 수행하였다. 우리는 그림 1에서 FiT의 샘플을 보여주고, BigGAN(Brock et al., 2018), StyleGAN-XL(Sauer et al., 2022), MaskGIT(Chang et al., 2022), CDM(Ho et al., 2022), U-ViT(Bao et al., 2023), ADM(Dhariwal and Nichol, 2021), LDM(Rombach et al., 2022), MDT(Gao et al., 2023), DiT(Peebles and Xie, 2023)와 비교한다. 4.3절에서 언급한 바와 같이 DiT 모델의 위치 임베딩에 PI(160\\times320\\)와 128\\times384\\) 해상도의 영상을 생성할 때, 학습 가능한 위치 임베딩을 사용하여 U-ViT 모델과 MDT 모델의 위치 임베딩에 EI를 사용한다. ADM과 LDM은 훈련 해상도와 다른 해상도로 영상을 직접 합성할 수 있다.\n' +
      '\n' +
      '탭에 표시된 대로입니다. 1, FiT-XL/2는 U-ViT-H/2-G에 의해 달성된 6.93의 이전 최상의 FID-50K를 \\(160\\times 320\\) 해상도에서 5.74로 감소시키면서 모든 이전 확산 모델보다 우수하다. (128\\times 384\\) 해상도에서 FiT-XL/2는 이전 SOTA FID-50K가 \\(29.67\\)에서 \\(16.81\\)으로 감소하면서 상당한 우월성을 보인다. FiT-XL/2의 FID 점수는 훈련 단계가 더 긴 다른 모델에 비해 \\(256\\times 256\\) 해상도에서 약간 증가한다.\n' +
      '\n' +
      '### FiT Out-Of-Distributed Resolution Results\n' +
      '\n' +
      '본 논문에서 제안한 FiT-XL/2를 U-ViT, ADM, LDM-4, MDT, DiT와 SOTA 클래스 조건 생성 모델(320\\times320\\times, 224\\times 448\\times, 160\\times 480\\times)의 세 가지 다른 분포 외 분해능에 대해 비교 평가한다. PI는 DiT에서, EI는 섹션 4.4에서와 같이 U-ViT 및 MDT에서 채택된다. ADM 및 LDM-4와 같은 U-Net 기반 방법은 분포에서 벗어난 해상도로 이미지를 직접 생성할 수 있다. 표 2에 나타난 바와 같이, FiT-XL/2는 세 가지 해상도 모두에서 가장 우수한 FID-50K 및 IS를 달성하여 뛰어난 외삽 능력을 나타낸다. 다른 메트릭의 관점에서, sFID로서, FiT-XL/2는 경쟁적 성능을 입증한다.\n' +
      '\n' +
      '변압기 백본을 갖는 LDM은 DiT, U-ViT, MDT와 같이 훈련 해상도로 영상을 생성하는데 어려움이 있는 것으로 알려져 있다. 더 심각하게, MDT는 트레이닝 해상도를 넘어 이미지를 생성하는 능력이 거의 없다. 우리는 이것이 학습 가능한 절대 PE와 학습 가능한 상대 PE가 모두 MDT에서 사용되기 때문이라고 추측한다. DiT와 U-ViT는 320x320 해상도에서 각각 \\(9.98\\)과 \\(7.65\\)의 FID 점수를 얻었다. 그러나 종횡비가 1이 아닌 경우에는 \\(224\\times 448\\) 및 \\(160\\times 480\\) 해상도로 인해 생성 성능이 크게 저하된다. Convolution Neural Network의 Local Receptive Field의 장점을 이용하여, ADM과 LDM은 이러한 Out-of-distribution Resolution에서 안정적인 성능을 보인다. 우리의 FiT-XL/2는 이미지 합성에서 변압기의 외삽 능력이 불충분한 문제를 해결한다. FID-50K에서 FiT-XL/2는 320\\times320\\times, 224\\times 448\\times, 160\\times 480\\times의 분해능에서 기존의 SOTA LDM보다 각각 \\(0.82\\), \\(0.65\\), \\(3.52\\) 증가하였다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 연구에서는 임의의 해상도와 종횡비를 유연하게 생성하는 연구를 진행하는데 기여하고자 한다. 본 논문에서는 확산 모델을 위한 Flexible Vision Transformer (FiT)를 제안한다. Flexible Vision Transformer는 임의의 해상도와 종횡비를 갖는 영상을 생성하기 위해 특별히 설계된 유연한 훈련 파이프라인을 갖는 개선된 변압기 구조이다. FiT는 다양한 해상도에서 트랜스포머 기반이든 CNN 기반이든 이전 모든 모델을 능가한다. 해상도 외삽 방법인 VisionNTK를 사용하면 FiT의 성능이 훨씬 향상되었습니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]F. 배승호 니경 허영 Cao, C. Li, H. Su, 및 J. Zhu(2023) 모두는 말할 가치가 있다: 확산 모델을 위한 vit 백본. IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '*[2]A. 브록, J. 도나휴, K. Simonyan (2018) Large scale gan training for high fidelity natural image synthesis. ArXiv:1809.11096. 인용: SS1.\n' +
      '*[3]T. 브라운, B. 만, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) 언어 모델은 소수의 학습자이다. 신경 정보 처리 시스템의 발전 인용: SS1.\n' +
      '*[4]H. 장홍장 Jiang, C. Liu, and W. T. Freeman(2022) Maskgit: masked generatingative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '*[5]S. 천성호 왕락 천영 Tian(2023) 위치 보간을 통해 대형 언어 모델의 컨텍스트 창을 확장한다. ArXiv:2306.15595. 인용: SS1.\n' +
      '*[6]A. 차우더리 나랑, J 데블린, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. (2023) Palm: scaling language modeling with pathways. 기계 학습 연구 저널입니다. 인용: SS1.\n' +
      '*[7]A. 차우더리 나랑, J 데블린, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. (2023) Palm: scaling language modeling with pathways. 기계 학습 연구 저널입니다. 인용: SS1.\n' +
      '*[8]M. 데하니, B. 무스타파, J. 졸롱가, J. 희크, M. 민더러 Caron, A. Steiner, J. Puigecerver, R. Geirhos, I. Alabdulmohsini, et al. (2023) Patch n\'pack: navit, a vision transformer for any aspect ratio and resolution. ArXiv:2307.06304. 인용: SS1.\n' +
      '*[9]J. 등원 동록 소처 이경 리, L. Fei-Fei(2009) Imagenet: 대규모 계층적 이미지 데이터베이스. IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '*[10]P. Dhariwal과 A. Nichol (2021) 확산 모델은 이미지 합성에서 간스를 이겼다. 신경 정보 처리 시스템의 발전 인용: SS1.\n' +
      '*[11]A. L. 도소비츠키 Beyer, A. Kolesnikov, D. Weissenborn, X. 자이태 Unterthiner, M 데하니, G. 민더러, G. 헤이골드, S. Gelly, et al.(2020) 이미지는 16x16 단어들의 가치가 있다: 스케일에서 이미지 인식을 위한 트랜스포머들. ArXiv:2010.11929. 인용: SS1.\n' +
      '*[12]P. 에세르 Rombach, and B. Ommer (2021) Taming transformer for high-resolution image synthesis. IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '*[13]S. 가오평주 쳉과 S. Yan(2023) Masked diffusion transformer는 강한 영상 합성기이다. ArXiv:2303.14389. 인용: SS1.\n' +
      '*[14]A. Hatamizadeh, J. Song, G. Liu, J. Kautz, and A. Vahdat(2023) Difit: 이미지 생성을 위한 확산 비전 트랜스포머. ArXiv:2312.02139. 인용: SS1.\n' +
      '*[15]M. 허셀, H. 램샤워, T. Unterthiner, B. Nessler, S. 2개의 시간-스케일 업데이트 규칙에 의해 훈련된 Hochreiter (2017) Gans는 로컬 내쉬 균형으로 수렴한다. 신경 정보 처리 시스템의 발전 인용: SS1.\n' +
      '*[16]J. 호랑 T 살리만(2021) 분류기 없는 확산 안내. NeurIPS 2021에서 심층 생성 모델 및 다운스트림 응용 프로그램에 대한 워크샵: SS1이 인용합니다.\n' +
      '*[17]J. Ho, A. Jain, and P. Abbeel (2020) Denoising diffusion probability model. 신경 정보 처리 시스템의 발전 인용: SS1.\n' +
      '*[18]J. 호철사하리아 장동준 노루지, T. Salimans (2022) Cascaded diffusion model for high fidelity image generation. 기계 학습 연구 저널입니다. 인용: SS1.\n' +
      '*[19]A. Hyvarinen and P. Dayan (2005) Estimation of non-normalized statistical models by score matching. 기계 학습 연구 저널입니다. 인용: SS1.\n' +
      '*[20]T. 킨카아니에미 카라스 레인과 T Lehtinen (2019) 개선된 정밀도 및 재현율 메트릭은 생성 모델을 평가한다. 신경 정보 처리 시스템의 발전 인용: SS1.\n' +
      '*[21]X. 류현연 장창안 Qiu, and D. Lin (2023) Scaling laws of rope-based extrapolation. ArXiv:2310.05209. 인용: SS1.\n' +
      '*[22]Z. 유영 임영식 조현후 위지 장승 Lin, and B. Guo(2021) Swin transformer: shifted window를 이용한 계층적 비전 트랜스포머. IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '*[23]Z. 류현마오, C. 우, C. 페이히텐호퍼 대럴과 S Xie (2022) 2020년대를 위한 경마장. IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '*[24]L. L. Ma(2020) Ntk 인식 스케일링된 로프는 라마 모델이 미세 조정 및 최소 복잡도 저하 없이 확장(8k+) 컨텍스트 크기를 가질 수 있게 한다. 참고: [https://www.reddit.com/r/LocalLLaMA/](https://www.reddit.com/r/LocalLLaMA/)comments/141z7j5/ntkaware_scaled_rope_allows_llama_models_to_have/. 접속: 2024-2-1\n' +
      '* Loshchilov and Hutter (2017) Loshchilov, I. and Hutter, F. Decoupled Weight decay regularization. _ arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* Nash et al. (2021) Nash, C., Menick, J., Dieleman, S., and Battaglia, P. W. generated images with sparse representations. _ arXiv preprint arXiv:2103.03841_, 2021.\n' +
      '* Peebles and Xie(2023) Peebles, W. 및 Xie, S. 변압기가 있는 확장 가능한 확산 모델입니다. In _IEEE/CVF International Conference on Computer Vision_, 2023.\n' +
      '* Peng et al. (2023) Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarm: Efficient context window extension of large language models. _ arXiv preprint arXiv:2309.00071_, 2023.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. 2021년 기계 학습에 관한 국제 회의에서.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.\n' +
      '* Ruoss et al. (2023) Ruoss, A., Deletang, G., Genewein, T., Grau-Moya, J., Csordas, R., Bennani, M., Legg, S., and Veness, J. Randomized position encodings boost length generalized of Transformers. _ arXiv preprint arXiv:2305.16843_, 2023.\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. _ 신경 정보 처리 시스템_, 2022의 발전.\n' +
      '* Salimans et al. (2016) Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. 갱을 훈련시키기 위한 개선된 기법들. _ 2016년 Neural Information Processing Systems_의 발전\n' +
      '* Sauer et al. (2022) Sauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling stylegan to large diverse datasets. 2022년 _ACM SIGGRAPH 2022 회의 절차에서, 2022.\n' +
      '* Shazeer(2020) Shazeer, N. Glu 변형은 변압기를 개선한다. _ arXiv preprint arXiv:2002.05202_, 2020.\n' +
      '* Song et al. (2020a) Song, J., Meng, C., and Ermon, S. 디노이징 확산 암시적 모델 arXiv preprint arXiv:2010.02502_, 2020a.\n' +
      '* Song et al. (2020b) Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. _ arXiv preprint arXiv:2011.13456_, 2020b.\n' +
      '* Su et al. (2024) Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. 로포머: 회전 위치 매립을 갖는 향상된 트랜스포머. _ 뉴로컴퓨팅_, 2024.\n' +
      '* Sun et al. (2022) Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., and Wei, F. A length-extrapable transformer. _ arXiv preprint arXiv:2212.10554_, 2022.\n' +
      '* Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: Family of highly capable multimodal models. _ arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Touvron et al. (2019) Touvron, H., Vedaldi, A., Douze, M., and Jegou, H. Fixing the train-test resolution discrepancy. _ 신경 정보 처리 시스템_, 2019에서의 발전\n' +
      '* Touvron et al. (2021) Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image transformer & distillation through attention. In _International conference on machine learning_, pp. 10347-10357. PMLR, 2021.\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. - A., Lacroix, T., and et al, B. R. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., and et al, N. B. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023b.\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention all you need. _ 2017년 신경 정보 처리 시스템의 발전\n' +
      '\n' +
      '## 부록 A 실험 설정\n' +
      '\n' +
      '우리는 탭 5에 나열된 모든 모델의 상세한 네트워크 구성과 성능을 제공합니다.\n' +
      '\n' +
      '## 부록 B 2D RoPE 및 디커플링된 2D-RoPE를 사용한 상세 주의 점수.\n' +
      '\n' +
      '2D RoPE는 수학식 10에서 벡터값 복소 함수 \\(f(\\mathbf{x},h_{m},w_{m})\\)를 다음과 같이 정의한다:\n' +
      '\n' +
      '{split}f(\\mathbf{x},h_{m},w_{m})=\\left[(x_{0}+ix_{1})e^{ih_{m}\\theta_{0},(x_{2}+ix_{3})e^{ih_{m}\\theta_{1},\\dots,(x_{d/2-2}+ix_{d/2-1})e^{ih_{m}\\theta_{d/4-1}},\\right.\\end{split}}e^{iw_{m}\\theta_{1}},(x_{d/2}+ix_{d-1})e^{iw_{m}\\theta_{1}},(x_{d/2}+ix_{d-1})e^{iw_{m}\\theta_{1}},(x_{d/2}+ix_{d-1})e^{iw\n' +
      '\n' +
      '식 (11)에서 2D RoPE를 주입한 자기 주의 점수 \\(A_{n}\\)는 다음과 같이 상세히 정의된다:\n' +
      '\n' +
      '\\mathrm{Re}\\langle f_{q}( \\mathbf{q}_{m},h_{m},w_{n},w_{n})\\rangle\\\\\\mathrm{Re}\\left[\\sum_{j=0}^{d/4-1}(q_{m}+iq_{2j+1}k_{2j+1})\\cos((w_{m}-w_{j})\\theta_{j}{m}+q_{2j+1}-q_{2j+1})\\theta_{j}{i(w_{m}+iq_{2j+1})\\theta_{j=0}^{d/4-1}[(q_{j}}k_{j}+q_{2j+1})\\theta_{j=0}^{d/4-1}[(q_{m}-h_{j})\\theta_{j}+(q_{j}\n' +
      '\n' +
      '여기서 폭과 높이의 2차원 좌표는 \\(\\{(w,h)\\Big{|}1\\leqslant w\\leqslant W,1\\leqslant h\\leqslant H\\}\\), \\(q\\)과 \\(k\\)의 첨자는 주의머리 치수를 나타내며 \\(\\theta^{n}=10000^{-2n/d}\\)이다. 2D-RoPE에서 \\(h\\)와 \\(w\\) 사이에는 교차항이 존재하지 않으므로, \\(\\theta_{h}=\\{\\theta_{d}^{h}=b_{h}^{-2d/|D|},1\\leqslant d\\leqslant\\frac{|D|}{2}\\)와 \\(\\theta_{w}=b_{w}^{-2d/|D|},1\\leqslant d\\leqslant\\frac{|D|}\\)의 회전주파수를 더 분리할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline Models & Layers & Dim. & Head Num. & Patch Size & Max Token Length & Training Steps & Batch Size & Learning Rate & FID-50K \\\\ \\hline DiT-B/2 & 12 & 768 & 12 & 2 & 256 & 400K & 256 & \\(1\\times 10^{-4}\\) & 44.83 \\\\ Config A & 12 & 768 & 12 & 2 & 256 & 400K & 256 & \\(1\\times 10^{-4}\\) & 43.34 \\\\ Config B & 12 & 768 & 12 & 2 & 256 & 400K & 256 & \\(1\\times 10^{-4}\\) & 41.75 \\\\ Config C & 12 & 768 & 12 & 2 & 256 & 400K & 256 & \\(1\\times 10^{-4}\\) & 39.11 \\\\ Config D & 12 & 768 & 12 & 2 & 256 & 400K & 256 & \\(1\\times 10^{-4}\\) & 37.29 \\\\ FiT-B/2 & 12 & 768 & 12 & 2 & 256 & 400K & 256 & \\(1\\times 10^{-4}\\) & 36.36 \\\\ \\hline FiT-XL/2-G & 28 & 1152 & 16 & 2 & 256 & 1800K & 256 & \\(1\\times 10^{-4}\\) & 4.27 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 모든 모델의 네트워크 구성 및 성능.\n' +
      '\n' +
      '{split} A_{n}=&\\sum_{j=0}^{d/4-1}[(q_{j}+q_{2j+1})\\theta_{j}^{i(h_{m}+iq_{2j+1})(k_{n})\\theta_{j}^{i(h_{m}+q_{2j+1})\\theta_{j}^{w}(w_{m}+q_{2j+1}k_{2j+1})\\cos((q_{j}{m}+q_{j+1}k_{n})\\theta_{j}^{w})\\mathrm{Re}(\\hat{f}_{j}=0}^{d/4-1}(q_{j}+q_{j+1})\\theta_{j}^{w}{m}+q_{j+1}(q_{j}{j}+q_{j+\n' +
      '\n' +
      '따라서 우리는 식 (13)에서 벡터값 복소함수 \\(\\hat{f}(\\mathbf{x},h_{m},w_{m})\\)을 다음과 같이 재구성할 수 있다.\n' +
      '\n' +
      '{split}\\hat{f}(\\mathbf{x},h_{m},w_{m})=�\\left[(x_{0}+ix_{1})e^{ih_{m}\\theta_{1}}, \\ldots,(x_{d-2}+ix_{d-1})e^{iw_{m}\\theta_{d/4-1}}\\right]^{T}.\\end{split}\\tag{m}\\theta_{m}\\theta_{d/4-1}},\\right.\\\\left(x_{d/2}+ix_{d/2+1})e^{iw_{m}\\theta_{w}},(x_{d/2}+ix_{d/2+1})e^{iw_{m}\\theta_{w}},(x_{d/2}+ix_{d/2+3})e^{iw_{m\n' +
      '\n' +
      '## 부록 C한계 및 향후 업무\n' +
      '\n' +
      '제한된 계산 자원으로 인해 제한된 1800K 단계에 대해서만 FiT-XL/2를 훈련한다. 256x256의 해상도에서 FiT-XL/2의 성능은 DiT-XL/2 모델에 비해 약간 떨어진다. 반면에, 우리는 더 높은 해상도(더 큰 토큰 길이 제한)로 훈련할 때 FiT-XL/2 모델의 생성 능력을 아직 철저히 탐구하지 않았다. 또한, 추가적인 훈련이 필요한 다른 해상도 외삽 방법을 탐구하지 않고, 훈련이 없는 해상도 외삽 기법만을 탐구한다. 우리는 FiT가 이전에 불가능했던 다양한 흥미로운 연구를 가능하게 하고 임의의 해상도와 종횡비를 가진 이미지를 생성하는 데 더 많은 관심을 불러일으킬 것이라고 믿는다.\n' +
      '\n' +
      '## 부록 D 더 많은 모델 샘플\n' +
      '\n' +
      'FiT-XL/2 모델의 샘플을 1.8M(250 DDPM 샘플링 단계와 ft-EMA VAE 디코더로 생성됨)에 대해 훈련된 \\(256\\times 256\\), \\(224\\times 448\\) 및 \\(448\\times 224\\)의 해상도로 보여준다. 도. 도 6은 분류기가 없는 안내 척도 4.0 및 클래스 라벨 "로거헤드 거북이"(33)를 갖는 FiT-XL/2로부터의 미경화 샘플을 도시한다. 도. 도 7은 분류기가 없는 안내 척도 4.0 및 클래스 라벨 "카카투아 갈라리타"(89)를 갖는 FiT-XL/2로부터의 미경화 샘플을 도시한다. 도. 도 8은 분류기가 없는 안내 척도 4.0 및 클래스 라벨 "골든 리트리버"를 갖는 FiT-XL/2로부터의 미경화 샘플을 도시한다(207). 도. 도 9는 분류기가 없는 안내 척도 4.0 및 클래스 라벨 "백색 여우"(279)를 갖는 FiT-XL/2로부터의 미경화 샘플을 도시한다. 도. 도 10은 분류기가 없는 안내 척도 4.0 및 클래스 라벨 "otter"(360)를 갖는 FiT-XL/2로부터의 미경화 샘플을 도시한다. 도. 도 11은 분류기가 없는 안내 척도 4.0 및 클래스 라벨 "화산"(980)을 갖는 FiT-XL/2로부터의 미경화 샘플을 도시한다.\n' +
      '\n' +
      '또한 그림 5와 같이 DiT-XL/2의 일부 고장 샘플을 보여준다.\n' +
      '\n' +
      '도 4: 원본 _ImageNet_(Deng et al., 2009) 데이터세트의 높이/폭 분포.\n' +
      '\n' +
      '그림 5: DiT-XL/2의 미경화 고장 샘플이다.\n' +
      '\n' +
      '그림 8: \\(256\\times 256\\), \\(224\\times 448\\) 및 \\(448\\times 224\\)의 분해능에서 FiT-XL/2 모델의 미경화 샘플이다.\n' +
      '\n' +
      '그림 10: \\(256\\times 256\\), \\(224\\times 448\\) 및 \\(448\\times 224\\)의 분해능에서 FiT-XL/2 모델의 미경화 샘플이다.\n' +
      '\n' +
      '그림 6: \\(256\\times 256\\), \\(224\\times 448\\) 및 \\(448\\times 224\\)의 분해능에서 FiT-XL/2 모델의 미경화 샘플이다.\n' +
      '\n' +
      '그림 7: \\(256\\times 256\\), \\(224\\times 448\\) 및 \\(448\\times 224\\)의 분해능에서 FiT-XL/2 모델의 미경화 샘플이다.\n' +
      '\n' +
      '그림 9: \\(256\\times 256\\), \\(224\\times 448\\) 및 \\(448\\times 224\\)의 분해능에서 FiT-XL/2 모델의 미경화 샘플이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>