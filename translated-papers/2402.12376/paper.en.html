<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# FiT: Flexible Vision Transformer for Diffusion Model\n' +
      '\n' +
      ' Zeyu Lu\n' +
      '\n' +
      'Zidong Wang\n' +
      '\n' +
      'Dianghai Artificial Intelligence Laboratory\n' +
      '\n' +
      'Shanghai Jiao Tong University\n' +
      '\n' +
      'Tsinghua University\n' +
      '\n' +
      'Sydney University\n' +
      '\n' +
      'The University of Hong Kong. Correspondence to: Lei Bai <baisanshi@gmail.com>.\n' +
      '\n' +
      'Di Huang\n' +
      '\n' +
      'Chengyue Wu\n' +
      '\n' +
      'Xihui Liu\n' +
      '\n' +
      'Wanli Ouyang\n' +
      '\n' +
      'Lei Bai\n' +
      '\n' +
      'Equal contribution Shanghai Artificial Intelligence Laboratory\n' +
      '\n' +
      'Shanghai Jiao Tong University\n' +
      '\n' +
      'Tsinghua University\n' +
      '\n' +
      'Sydney University\n' +
      '\n' +
      'The University of Hong Kong. Correspondence to: Lei Bai <baisanshi@gmail.com>.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '_Nature is infinitely resolution-free_. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with _unrestricted resolutions and aspect ratios_. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution. Repository available at [https://github.com/whlzy/FiT](https://github.com/whlzy/FiT).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Current image generation models struggle with generalizing across arbitrary resolutions. The Diffusion Transformer (DiT) (Peebles and Xie, 2023) family, while excelling within certain resolution ranges, falls short when dealing with images of varying resolutions. This limitation stems from the fact that DiT can not utilize dynamic resolution images during its training process, hindering its ability to adapt to different token lengths or resolutions effectively.\n' +
      '\n' +
      'To overcome this limitation, we introduce the **Flexible Vision Transformer (FiT)**, which is adept at generating images at _unrestricted resolutions and aspect ratios_. The key motivation is a novel perspective on image data modeling: rather than treating images as static grids of fixed dimensions, FiT conceptualizes images as sequences of variable-length tokens. This approach allows FiT to dynamically adjust the sequence length, thereby facilitating the generation of images at any desired resolution without being constrained by pre-defined dimensions. By efficiently managing variable-length token sequences and padding them to a maximum specified length, FiT unlocks the potential for resolution-independent image generation. FiT represents this paradigm shift through significant advancements in **flexible training pipeline**, **network architecture**, and **inference processes**.\n' +
      '\n' +
      '**Flexible Training Pipeline.** FiT uniquely preserves the original image aspect ratio during training, by viewing the image as a sequence of tokens. This unique perspective allows FiT to adaptively resize high-resolution images to fit within a predefined maximum token limit, ensuring that no image, regardless of its original resolution, is cropped or disproportionately scaled. This method ensures that the integrity of the image resolution is maintained, as shown in Figure 2, facilitating the ability to generate high-fidelity images at various resolutions. To the best of our knowledge, FiT is the first transformer-based generation model to maintain diverse image resolutions throughout training.\n' +
      '\n' +
      '**Network Architecture.** The FiT model evolves from the DiT architecture but addresses its limitations in resolution extrapolation. One essential network architecture adjustment to handle diverse image sizes is the adoption of 2D Rotary Positional Embedding (RoPE) (Su et al., 2024), inspired by its success in enhancing large language models (LLMs) for length extrapolation (Liu et al., 2023). We also introduce Swish-Gated Linear Unit (SwiGLU) (Shazeer, 2020) in place of the traditional Multilayer Perceptron (MLP) and replace DiT\'s Multi-Head Self-Attention (MHSA) with Masked MHSA to efficiently manage padding tokens within our flexible training pipeline.\n' +
      '\n' +
      '**Inference Process.** While large language models employ token length extrapolation techniques (Peng et al., 2023; LocalLLaMA) for generating text of arbitrary lengths, a direct application of these technologies to FiT yields sub-optimal results. We tailor these techniques for 2D RoPE, thereby enhancing FiT\'s performance across a spectrum of resolutions and aspect ratios.\n' +
      '\n' +
      'Our highest Gflop FiT-XL/2 model, after training for only 1.8 million steps on _ImageNet-256_(Deng et al., 2009) dataset, outperforms all state-of-the-art CNN and transformer models by a significant margin across resolutions of \\(160\\times 320\\), \\(128\\times 384\\), \\(320\\times 320\\), \\(224\\times 448\\), and \\(160\\times 480\\). The performance of FiT-XL/2 significantly advances further with our training-free resolution extrapolation method. Compared to the baseline DiT-XL/2 training for 7 million steps, FiT-XL/2 lags slightly at the resolution of \\(256\\times 256\\) but significantly surpasses it at all other resolutions.\n' +
      '\n' +
      'In summary, our contributions lie in the novel introduction of FiT, a flexible vision transformer tailored for diffusion models, capable of generating images at any resolution and aspect ratio. We present three innovative design features in FiT, including a flexible training pipeline that eliminates the need for cropping, a unique transformer architecture for dynamic token length modeling, and a training-free resolution extrapolation method for arbitrary resolution generation. Strict experiments demonstrate that the FiT-XL/2 model achieves state-of-the-art performance across a variety of resolution and aspect ratio settings.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '**Diffusion Models.** Denoising Diffusion Probabilistic Models (DPM) (Ho et al., 2020; Saharia et al., 2022; Radford et al., 2021) and score-based models (Hyvarinen and Dayan, 2005; Song et al., 2020) have exhibited remarkable progress in the context of image generation tasks. The Denoising Diffusion Implicit Model (DDIM) Song et al. (2020), offers An accelerated sampling procedure. Latent Diffusion Models (LDMs) (Rombach et al., 2022) establishes a new benchmark of training deep generative models to reverse a noise process in the latent space, through the\n' +
      '\n' +
      'Figure 2: Pipeline comparison between (a) DiT and (b) FiT.\n' +
      '\n' +
      'use of VQ-VAE (Esser et al., 2021).\n' +
      '\n' +
      '**Transformer Models.** The Transformer model (Vaswani et al., 2017), has successfully supplanted domain-specific architectures in a variety of fields including, but not limited to, language (Brown et al., 2020; Chowdhery et al., 2023), vision (Dosovitskiy et al., 2020), and multi-modality (Team et al., 2023). In vision perception research, most efforts (Touvron et al., 2019, 2021; Liu et al., 2021, 2022) that focus on resolution are aimed at accelerating pretraining using a fixed, low resolution. On the other hand, NaViT (Dehghani et al., 2023) implements the \'Patch n\' Pack\' technique to train ViT using images at their natural, \'native\' resolution. Notably, transformers have been also explored in the denoising diffusion probabilistic models (Ho et al., 2020) to synthesize images. DiT (Peebles and Xie, 2023) is the seminal work that utilizes a vision transformer as the backbone of LDMs and can serve as a strong baseline. Based on DiT architecture, MDT (Gao et al., 2023) introduces a masked latent modeling approach, which requires two forward-runs in training and inference. U-ViT (Bao et al., 2023) treats all inputs as tokens and incorporates U-Net architectures into the ViT backbone of LDMs. BifT (Hatamizadeh et al., 2023) introduces a time-dependent self-attention module into the DiT backbone to adapt to different stages of the diffusion process. We follow the LDM paradigm of the above methods and further propose a novel flexible image synthesis pipeline.\n' +
      '\n' +
      '**Length Extrapolation in LLMs.** RoPE (Rotary Position Embedding) (Su et al., 2024) is a novel positional embedding that incorporates relative position information into absolute positional embedding. It has recently become the dominant positional embedding in a wide range of LLM (Large Language Model) designs (Chowdhery et al., 2023; Touvron et al., 2023; ). Although RoPE enjoys valuable properties, such as the flexibility of sequence length, its performance drops when the input sequence surpasses the training length. Many approaches have been proposed to solve this issue. PI (Position Interpolation) (Chen et al., 2023) linearly down-scales the input position indices to match the original context window size, while NTK-aware (LocalL-LaMA) changes the rotary base of RoPE. YaRN (Yet another RoPE extensioN) (Peng et al., 2023) is an improved method to efficiently extend the context window. RandomPE (Ruoss et al., 2023) sub-samples an ordered set of positions from a much larger range of positions than originally observed in training or inference. xPos (Sun et al., 2022) incorporates long-term decay into RoPE and uses blockwise causal attention for better extrapolation performance. Our work delves deeply into the implementation of RoPE in vision generation and on-the-fly resolution extrapolation methods.\n' +
      '\n' +
      '## 3 Flexible Vision Transformer for Diffusion\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      '**1-D RoPE (Rotary Positional Embedding)**(Su et al., 2024) is a type of position embedding that unifies absolute and relative PE, exhibiting a certain degree of extrapolation capability in LLMs. Given the m-th key and n-th query vector as \\(\\mathbf{q}_{m},\\mathbf{k}_{n}\\in\\mathbb{R}^{|D|}\\), 1-D RoPE multiplies the bias to the key or query vector in the complex vector space:\n' +
      '\n' +
      '\\[f_{q}(\\mathbf{q}_{m},m)=e^{im\\Theta}\\mathbf{q}_{m},\\quad f_{k}(\\mathbf{k}_{n},n)=e^{in\\Theta}\\mathbf{k}_{n} \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\Theta=\\mathrm{Diag}(\\theta_{1},\\cdots,\\theta_{|D|/2})\\) is rotary frequency matrix with \\(\\theta_{d}=b^{-2d/|D|}\\) and rotary base \\(b=10000\\). In real space, given \\(l=|D|/2\\), the rotary matrix \\(e^{im\\Theta}\\) equals to:\n' +
      '\n' +
      '\\[\\begin{bmatrix}\\cos m\\theta_{1}&-\\sin m\\theta_{1}&\\cdots&0&0\\\\ \\sin m\\theta_{1}&\\cos m\\theta_{1}&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 0&0&\\cdots&\\cos m\\theta_{l}&-\\sin m\\theta_{l}\\\\ 0&0&\\cdots&\\sin m\\theta_{l}&\\cos m\\theta_{l}\\\\ \\end{bmatrix} \\tag{2}\\]\n' +
      '\n' +
      'The attention score with 1-D RoPE is calculated as:\n' +
      '\n' +
      '\\[A_{n}=\\mathrm{Re}(f_{q}(\\mathbf{q}_{m},m),f_{k}(\\mathbf{k}_{n},n)) \\tag{3}\\]\n' +
      '\n' +
      '**NTK-aware Interpolation** (LocalLLaMA) is a training-free length extrapolation technique in LLMs. To handle the larger context length \\(L_{\\text{test}}\\) than maximum training length \\(L_{\\text{train}}\\), it modifies the rotary base of 1-D RoPE as follows:\n' +
      '\n' +
      '\\[b^{\\prime}=b\\cdot s^{\\frac{|D|}{|D|-2}}, \\tag{4}\\]\n' +
      '\n' +
      'where the scale factor \\(s\\) is defined as:\n' +
      '\n' +
      '\\[s=\\max(\\frac{L_{test}}{L_{train}},1.0). \\tag{5}\\]\n' +
      '\n' +
      '**YaRN (Yet another RoPE extensioN) Interpolation**(Peng et al., 2023) introduces the ratio of dimension \\(d\\) as \\(r(d)=L_{\\text{train}}/(2\\pi b^{2d/|D|})\\), and modifies the rotary frequency as:\n' +
      '\n' +
      '\\[\\theta_{d}^{\\prime}=(1-\\gamma(r(d)))\\,\\frac{\\theta_{d}}{s}+\\gamma(r(d))\\theta_ {d}, \\tag{6}\\]\n' +
      '\n' +
      'where \\(s\\) is the aforementioned scale factor, and \\(\\gamma(r(d))\\) is a ramp function with extra hyper-parameters \\(\\alpha,\\beta\\):\n' +
      '\n' +
      '\\[\\gamma(r)=\\begin{cases}0,&\\text{if }r<\\alpha\\\\ 1,&\\text{if }r>\\beta\\\\ \\frac{r-\\alpha}{\\beta-\\alpha},&\\text{otherwise}.\\\\ \\end{cases} \\tag{7}\\]\n' +
      '\n' +
      'Besides, it incorporates a 1D-RoPE scaling term as:\n' +
      '\n' +
      '\\[f_{q}^{\\prime}(\\mathbf{q}_{m},m)=\\frac{1}{\\sqrt{t}}f_{q}(\\mathbf{q}_{m},m),f_ {k}^{\\prime}(\\mathbf{k}_{n},n)=\\frac{1}{\\sqrt{t}}f_{k}(\\mathbf{k}_{n},n), \\tag{8}\\]\n' +
      '\n' +
      'where \\(\\frac{1}{\\sqrt{t}}=0.1\\ln(s)+1\\).\n' +
      '\n' +
      '### Flexible Training and Inference Pipeline\n' +
      '\n' +
      'Modern deep learning models, constrained by the characteristics of GPU hardware, are required to pack data into batches of uniform shape for parallel processing. Due to the diversity in image resolutions, as shown in Fig. 4, DiT resizes and crops the images to a fixed resolution \\(256\\times 256\\). While resizing and cropping as a means of data augmentation is a common practice, this approach introduces certain biases into the input data. These biases will directly affect the final images generated by the model, including blurring effects from the transition from low to high resolution and information lost due to the cropping (more failure samples can be found in D).\n' +
      '\n' +
      'To this end, we propose a flexible training and inference pipeline, as shown in Fig. 3 (a, b). _In the preprocessing phase_, we avoid cropping images or resizing low-resolution images to a higher resolution. Instead, we only resize high-resolution images to a predetermined maximum resolution limit, \\(HW\\leqslant 256^{2}\\). _In the training phase_, FiT first encodes an image into latent codes with a pre-trained VAE encoder. By patchflying latent codes to latent tokens, we can get sequences with different lengths \\(L\\). To pack these sequences into a batch, we pad all these sequences to the maximum token length \\(L_{max}\\) using padding tokens. Here we set \\(L_{max}=256\\) to match the fixed token length of DiT. The same as the latent tokens, we also pad the positional embeddings to the maximum length for packing. Finally, we calculate the loss function only for the denoised output tokens, while discarding all other padding tokens. _In the inference phase_, we firstly define the position map of the generated image and sample noisy tokens from the Gaussian distribution as input. After completing \\(K\\) iterations of the denoising process, we reshape and unpatchfy the denoised tokens according to the predefined position map to get the final generated image.\n' +
      '\n' +
      '### Flexible Vision Transformer Architecture\n' +
      '\n' +
      'Building upon the flexible training pipeline, our goal is to find an architecture that can stably train across various resolutions and generate images with arbitrary resolutions and aspect ratios, as shown in Figure 3 (c). Motivated by some significant architectural advances in LLMs, we conduct a series of experiments to explore architectural modifications based on DiT, see details in Section 4.2.\n' +
      '\n' +
      '**Replacing MHSA with Masked MHSA.** The flexible training pipeline introduces padding tokens for flexibly packing dynamic sequences into a batch. During the forward phase of the transformer, it is crucial to facilitate interactions among noised tokens while preventing any interaction between noised tokens and padding tokens. The Multi-Head Self-Attention (MHSA) mechanism of original DiT is incapable of distinguishing between noised tokens and padding tokens. To this end, we use Masked MHSA to replace the standard MHSA. We utilize the sequence mask \\(M\\) for Masked Attention, where noised tokens are assigned the value of \\(0\\), and padding tokens are assigned the value of negative infinity (_-inf_), which is defined as follows:\n' +
      '\n' +
      '\\[\\text{Masked Attn.}(Q_{i},K_{i},V_{i})=\\text{Softmax}\\left(\\frac{Q_{i}K_{i}^{T} }{\\sqrt{d_{k}}}+M\\right)V_{i} \\tag{9}\\]\n' +
      '\n' +
      'where \\(Q_{i}\\), \\(K_{i}\\), \\(V_{i}\\) are the query, key, and value matrices for the \\(i\\)-th head.\n' +
      '\n' +
      '**Replacing Absolute PE with 2D RoPE.** We observe that vision transformer models with absolute positional embedding fail to generalize well on images beyond the training resolution, as in Sections 4.3 and 4.5. Inspired by the success of 1D-RoPE in LLMs for length extrapolation (Liu et al., 2023), we utilize 2D-RoPE to facilitate the resolution\n' +
      '\n' +
      'Figure 3: Overview of (a) flexible training pipeline, (b) flexible inference pipeline, and (c) FiT block.\n' +
      '\n' +
      'generalization in vision transformer models. Formally, we calculate the 1-D RoPE for the coordinates of height and width separately. Then such two 1-D RoPEs are concatenated in the last dimension. Given 2-D coordinates of width and height as \\(\\{(w,h)\\big{|}1\\leqslant w\\leqslant W,1\\leqslant h\\leqslant H\\}\\), the 2-D RoPE is defined as:\n' +
      '\n' +
      '\\[\\begin{split}& f_{q}(\\mathbf{q}_{m},h_{m},w_{m})=[e^{ih_{m}\\Theta} \\mathbf{q}_{m}\\parallel e^{iw_{m}\\Theta}\\mathbf{q}_{m}],\\\\ & f_{k}(\\mathbf{k}_{n},h_{n},w_{n})=[e^{ih_{n}\\Theta}\\mathbf{k}_{ n}\\parallel e^{iw_{m}\\Theta}\\mathbf{k}_{n}],\\end{split} \\tag{10}\\]\n' +
      '\n' +
      'where \\(\\Theta=\\mathrm{Diag}(\\theta_{1},\\cdots,\\theta_{|D|/4})\\), and \\(\\parallel\\) denotes concatenate two vectors in the last dimension. Note that we divide the \\(|D|\\)-dimension space into \\(|D|/4\\)-dimension subspace to ensure the consistency of dimension, which differs from \\(|D|/2\\)-dimension subspace in 1-D RoPE. Analogously, the attention score with 2-D RoPE is:\n' +
      '\n' +
      '\\[A_{n}=\\mathrm{Re}\\langle f_{q}(\\mathbf{q}_{m},h_{m},w_{m}),f_{k}(\\mathbf{k}_{ n},h_{n},w_{n})\\rangle. \\tag{11}\\]\n' +
      '\n' +
      'It is noteworthy that there is no cross-term between \\(h\\) and \\(w\\) in 2D-RoPE and attention score \\(A_{n}\\), so we can further decouple the rotary frequency as \\(\\Theta_{h}\\) and \\(\\Theta_{w}\\), resulting in the decoupled 2D-RoPE, which will be discussed in Section 3.4 and more details can be found in B.\n' +
      '\n' +
      '**Replacing MLP with SwiGLU.** We follow recent LLMs like LLaMA (Touvron et al., 2023a,b), and replace the MLP in FFN with SwiGLU, which is defined as follows:\n' +
      '\n' +
      '\\[\\begin{split}&\\text{SwiGLU}(x,W,V)=\\text{SiLU}(xW)\\otimes(xV) \\\\ &\\text{FFN}(x)=\\text{SwiGLU}(x,W_{1},W_{2})W_{3}\\end{split} \\tag{12}\\]\n' +
      '\n' +
      'where \\(\\otimes\\) denotes Hadmard Product, \\(W_{1}\\), \\(W_{2}\\), and \\(W_{3}\\) are the weight matrices without bias, \\(\\text{SiLU}(x)=x\\otimes\\sigma(x)\\). Here we will use SwiGLU as our choice in each FFN block.\n' +
      '\n' +
      '### Training Free Resolution Extrapolation\n' +
      '\n' +
      'We denote the inference resolution as (\\(H_{\\text{test}}\\), \\(W_{\\text{test}}\\)). Our FiT can handle various resolutions and aspect ratios during training, so we denote training resolution as \\(L_{\\text{train}}=\\sqrt{L_{\\text{max}}}\\).\n' +
      '\n' +
      'By changing the scale factor in Equation (5) to \\(s=\\max(\\max(H_{\\text{test}},W_{\\text{test}})/L_{train},1.0)\\), we can directly implement the positional interpolation methods in large language model extrapolation on 2D-RoPE, which we call vanilla NTK and YaRN implementation. Furthermore, we propose vision RoPE interpolation methods by using the decoupling attribute in decoupled 2D-RoPE. We modify Equation (10) to:\n' +
      '\n' +
      '\\[\\begin{split}&\\hat{f}_{q}(\\mathbf{q}_{m},h_{m},w_{m})=[e^{ih_{m} \\Theta_{h}}\\mathbf{q}_{m}\\parallel e^{iw_{m}\\Theta_{w}}\\mathbf{q}_{m}],\\\\ &\\hat{f}_{k}(\\mathbf{k}_{n},h_{n},w_{n})=[e^{ih_{n}\\Theta_{h}} \\mathbf{k}_{n}\\parallel e^{iw_{m}\\Theta_{w}}\\mathbf{k}_{n}],\\end{split} \\tag{13}\\]\n' +
      '\n' +
      'where \\(\\Theta_{h}=\\{\\theta_{d}^{h}=b_{h}^{-2d/|D|},1\\leqslant d\\leqslant\\frac{|D|}{2}\\}\\) and \\(\\Theta_{w}=\\{\\theta_{d}^{w}=b_{w}^{-2d/|D|},1\\leqslant d\\leqslant\\frac{|D|}{2}\\}\\) are calculated separately. Accordingly, the scale factor of height and width is defined separately as\n' +
      '\n' +
      '\\[s_{h}=\\max(\\frac{H_{\\text{test}}}{L_{\\text{train}}},1.0),\\quad s_{w}=\\max( \\frac{W_{\\text{test}}}{L_{\\text{train}}},1.0). \\tag{14}\\]\n' +
      '\n' +
      '**Definition 3.1**.: _The Definition of VisionNTK Interpolation is a modification of NTK-aware Interpolation by using Equation (13) with the following rotary base._\n' +
      '\n' +
      '\\[b_{h}=b\\cdot s_{h}^{\\frac{|D|}{|D|-2}},\\quad b_{w}=b\\cdot s_{w}^{\\frac{|D|}{|D| -2}}, \\tag{15}\\]\n' +
      '\n' +
      'where \\(b=10000\\) is the same with Equation (1)\n' +
      '\n' +
      '**Definition 3.2**.: _The Definition of VisionYaRN Interpolation is a modification of YaRN Interpolation by using Equation (13) with the following rotary frequency._\n' +
      '\n' +
      '\\[\\begin{split}&\\theta_{d}^{h}=(1-\\gamma(r(d))\\frac{\\theta_{d}}{s_{h}}+ \\gamma(r(d))\\theta_{d},\\\\ &\\theta_{d}^{w}=(1-\\gamma(r(d))\\frac{\\theta_{d}}{s_{w}}+\\gamma(r(d ))\\theta_{d},\\end{split} \\tag{16}\\]\n' +
      '\n' +
      'where \\(\\gamma(r(d))\\) is the same with Equation (6).\n' +
      '\n' +
      'It is worth noting that VisionNTK and VisionYaRN are training-free positional embedding interpolation approaches, used to alleviate the problem of position embedding out of distribution in extrapolation. When the aspect ratio equals one, they are equivalent to the vanilla implementation of NTK and YaRN. They are especially effective in generating images with arbitrary aspect ratios, see Section 4.3.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### FiT Implementation\n' +
      '\n' +
      'We present the implementation details of FiT, including model architecture, training details, and evaluation metrics.\n' +
      '\n' +
      '**Model architecture.** We follow DiT-B and DiT-XL to set the same layers, hidden size, and attention heads for base model FiT-B and xlarge model FiT-XL. As DiT reveals stronger synthesis performance when using a smaller patch size, we use a patch size p=2, denoted by FiT-B/2 and FiT-XL/2. FiT adopts the same off-the-shelf pre-trained VAE (Esser et al., 2021) as DiT provided by the Stable Diffusion (Rombach et al., 2022) to encode/decode the image/latent tokens. The VAE encoder has a downsampling ratio of \\(1/8\\) and a feature channel dimension of \\(4\\). An image of size \\(160\\times 320\\times 3\\) is encoded into latent codes of size \\(20\\times 40\\times 4\\). The latent codes of size \\(20\\times 40\\times 4\\) are patchified into latent tokens of length \\(L=10\\times 20=200\\).\n' +
      '\n' +
      '**Training details.** We train class-conditional latent FiT models under predetermined maximum resolution limitation, \\(HW\\leqslant 256^{2}\\) (equivalent to token length \\(L\\leq 256\\)), on the _ImageNet_(Deng et al., 2009) dataset. We down-resize the high-resolution images to meet the \\(HW\\leqslant 256^{2}\\) limitation while maintaining the aspect ratio. We follow DiT to use Horizontal Flip Augmentation. We use the same training setting as DiT: a constant learning rate of \\(1\\times 10^{-4}\\) using AdamW (Loshchilov and Hutter, 2017), no weight decay, and a batch size of \\(256\\). Following common practice in the generative modeling literature, we adopt an exponential moving average (EMA) of model weights over training with a decay of 0.9999. All results are reported using the EMA model. We retain the same diffusion hyper-parameters as DiT.\n' +
      '\n' +
      '**Evaluation details and metrics.** We evaluate models with some commonly used metrics, _i.e._ Fre\'chet Inception Distance (FID) (Heusel et al., 2017), sFID (Nash et al., 2021), Inception Score (IS) (Salimans et al., 2016), improved Precision and Recall (Kynkaniemi et al., 2019). For fair comparisons, we follow DiT to use the TensorFlow evaluation from ADM (Dharwal and Nichol, 2021) and report FID-50K with 250 DDPM sampling steps. FID is used as the major metric as it measures both diversity and fidelity. We additionally report IS, sFID, Precision, and Recall as secondary metrics. For FiT architecture experiment (Section 4.2) and resolution extrapolation ablation experiment (Section 4.3), we report the results without using classifier-free guidance (Ho and Salimans, 2021).\n' +
      '\n' +
      '**Evaluation resolution.** Unlike previous work that mainly conducted experiments on a fixed aspect ratio of \\(1:1\\), we conducted experiments on different aspect ratios, which are \\(1:1\\), \\(1:2\\), and \\(1:3\\), respectively. On the other hand, we divide the experiment into resolution within the training distribution and resolution out of the training distribution. For the resolution in distribution, we mainly use \\(256\\times 256\\) (1:1), \\(160\\times 320\\) (1:2), and \\(128\\times 384\\) (1:3) for evaluation, with \\(256\\), \\(200\\), \\(192\\) latent tokens respectively. All token lengths are smaller than or equal to 256, leading to respective resolutions within the training distribution. For the resolution out of distribution, we mainly use \\(320\\times 320\\) (1:1), \\(224\\times 448\\) (1:2), and \\(160\\times 480\\) (1:3) for evaluation, with \\(400\\), \\(392\\), \\(300\\) latent tokens respectively. All token lengths are larger than 256, resulting in the resolutions out of training distribution. Through such division, we holistically evaluate the image synthesis and resolution extrapolation ability of FiT at various resolutions and aspect ratios.\n' +
      '\n' +
      '### FiT Architecture Design\n' +
      '\n' +
      'In this part, we conduct an ablation study to verify the architecture designs in FiT. We report the results of various variant FiT-B/2 models at 400K training steps and use FID-50k, sFID, IS, Precision, and Recall as the evaluation metrics. We conduct experiments at three different resolutions: \\(256\\times 256\\), \\(160\\times 320\\), and \\(224\\times 448\\). These resolutions are chosen to encompass different aspect ratios, as well as to include resolutions both in and out of the distribution.\n' +
      '\n' +
      '**Flexible training vs. Fixed training.**_Flexible training pipeline significantly improves the performance across var\n' +
      '\n' +
      'ious resolutions._ This improvement is evident not only within the in-distribution resolutions but also extends to resolutions out of the training distribution, as shown in Tab. 3. Config A is the original DiT-B/2 model only with flexible training, which slightly improves the performance (-1.49 FID) compared with DiT-B/2 with fixed resolution training at \\(256\\times 256\\) resolution. Config A demonstrates a significant performance improvement through flexible training. Compared to DiT-B/2, FID scores are reduced by \\(40.81\\) and 56.55 at resolutions \\(160\\times 320\\) and \\(224\\times 448\\), respectively.\n' +
      '\n' +
      '**SwiGLU vs. MLP.**_SwiGLU slightly improves the performance across various resolutions, compared to MLP._ Config B is the FiT-B/2 flexible training model replacing MLP with SwiGLU. Compared to Config A, Config B demonstrates notable improvements across various resolutions. Specifically, for resolutions of \\(256\\times 256\\), \\(160\\times 320\\), and \\(224\\times 448\\), Config B reduces the FID scores by 1.59, 1.85, and 0.21 in Tab. 3, respectively. So FiT uses SwiGLU in FFN.\n' +
      '\n' +
      '**2D RoPE vs. Absolute PE.**_2D RoPE demonstrates greater efficiency compared to absolute position encoding, and it possesses significant extrapolation capability across various resolutions._ Config D is the FiT-B/2 flexible training model replacing absolute PE with 2D RoPE. For resolutions within the training distribution, specifically \\(256\\times 256\\) and \\(160\\times 320\\), Config D reduces the FID scores by 6.05, and 5.45 in Tab. 3, compared to Config A. For resolution beyond the training distribution, \\(224\\times 448\\), Config D shows significant extrapolation capability (-6.39 FID) compared to Config A. Config C retains both absolute PE and 2D RoPE. However, in a comparison between Config C and Config D, we observe that Config C performs worse. For resolutions of 256x256, 160x320, and 224x448, Config C increases FID scores of 1.82, 1.65, and 0.44, respectively, compared to Config D. Therefore, only 2D RoPE is used for positional embedding in our implementation.\n' +
      '\n' +
      '**Putting it together.**_FiT demonstrates significant and comprehensive superiority across various resolution settings, compared to original DiT._ FiT has achieved state-of-the-art performance across various configurations. Compared to DiT-B/2, FiT-B/2 reduces the FID score by \\(8.47\\) on the most common resolution of \\(256\\times 256\\) in Tab. 3. Furthermore, FiT-B/2 has made significant performance gains at resolutions of \\(160\\times 320\\) and \\(224\\times 448\\), decreasing the FID scores by 47.36 and 64.43, respectively.\n' +
      '\n' +
      '### FiT Resolution Extrapolation Design\n' +
      '\n' +
      'In this part, we adopt the DiT-B/2 and FiT-B/2 models at 400K training steps to evaluate the extrapolation performance on three out-of-distribution resolutions: \\(320\\times 320\\), \\(224\\times 448\\) and \\(160\\times 480\\). Direct extrapolation does not perform well on larger resolution out of training distribution. So we conduct a comprehensive benchmarking analysis focused on positional embedding interpolation methods.\n' +
      '\n' +
      '**PI and EI.** PI (Position Interpolation) and EI (Embedding Interpolation) are two baseline positional embedding interpolation methods for resolution extrapolation. PI linearly down-scales the inference position coordinates to match the original coordinates. EI resizes the positional embedding\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c|c c c c|c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{4}{c|}{320\\(\\times\\)320 (1:1)} & \\multicolumn{4}{c|}{224\\(\\times\\)448 (1:2)} & \\multicolumn{4}{c}{160\\(\\times\\)480 (1:3)} \\\\  & **FID\\(\\downarrow\\)** & **SFD\\(\\downarrow\\)** & **IS\\(\\uparrow\\)** & **Prec.\\(\\uparrow\\)** & **Rec.\\(\\uparrow\\)** & **FID\\(\\downarrow\\)** & **SFD\\(\\downarrow\\)** & **IS\\(\\uparrow\\)** & **Prec.\\(\\uparrow\\)** & **Rec.\\(\\uparrow\\)** & **FID\\(\\downarrow\\)** & **sFID\\(\\downarrow\\)** & **sFID\\(\\downarrow\\)** & **IS\\(\\uparrow\\)** & **Prec.\\(\\uparrow\\)** & **Rec.\\(\\uparrow\\)** \\\\ \\hline DT-B & 95.47 & 108.68 & 18.38 & 0.26 & 0.40 & 109.1 & 110.71 & 14.00 & 0.18 & 0.31 & 143.8 & 122.81 & 8.93 & 0.073 & 0.20 \\\\ DT-B + EI & 81.48 & 62.25 & 20.97 & 0.25 & 0.47 & 133.2 & 72.53 & 11.11 & 0.11 & 0.29 & 160.4 & 93.91 & 7.30 & 0.054 & 0.16 \\\\ DT-B + PI & 72.47 & 54.02 & 24.15 & 0.29 & 0.49 & 133.4 & 70.29 & 11.73 & 0.11 & 0.29 & 156.5 & 93.80 & 7.80 & 0.058 & 0.17 \\\\ \\hline FiT-B & 61.35 & **30.71** & 31.01 & 0.41 & 0.53 & 44.67 & **24.09** & 37.11 & 0.49 & 0.52 & 56.81 & **22.07** & 25.25 & **0.38** & 0.49 \\\\ FiT-B + PI & 65.76 & 65.45 & 29.32 & 0.32 & 0.45 & 175.42 & 114.39 & 8.45 & 0.14 & 0.06 & 224.83 & 123.45 & 5.89 & 0.02 & 0.06 \\\\ FiT-B + YaRN & 44.76 & 38.04 & 44.70 & 0.51 & 0.51 & 82.19 & 75.48 & 29.68 & 0.40 & 0.29 & 104.06 & 27.97 & 20.76 & 0.21 & 0.31 \\\\ FiT-B + NTK & 57.31 & 31.31 & 33.97 & 0.43 & 0.55 & 45.24 & 29.38 & 38.84 & 0.47 & 0.52 & 59.19 & 26.54 & 26.01 & 0.36 & 0.49 \\\\ \\hline FiT-B + **VisionYARN** & **44.76** & 38.04 & **44.70** & **0.51** & 0.51 & **41.92** & 42.79 & **45.87** & **0.50** & 0.48 & 62.84 & 44.82 & **27.84** & 0.36 & 0.42 \\\\ FiT-B + **VisionNTK** & 57.31 & 31.31 & 33.97 & 0.43 & **0.55** & 43.84 & 26.25 & 39.22 & 0.48 & **0.52** & **56.76** & 24.18 & 26.40 & 0.37 & **0.49** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Benchmarking class-conditional image generation with out-of-distribution resolution on ImageNet. The FiT-B/2 and DiT-B/2 at 400K training steps are adopted in this experiment. Metrics are calculated without using classifier-free guidance. YaRN and NTK mean the vanilla implementation of such two methods. Our FiT-B/2 demonstrates stable extrapolation performance, which can be further improved combined with VisionNTK and VisionYARN methods.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c c c|c c c c c|c c c c} \\hline \\hline \\multirow{2}{*}{Arch.} & \\multirow{2}{*}{Pos. Embed.} & \\multirow{2}{*}{FFN} & \\multirow{2}{*}{Train} & \\multicolumn{4}{c|}{256\\(\\times\\)256 (4.2)} & \\multicolumn{4}{c|}{160\\(\\times\\)320 (4.3)} & \\multicolumn{4}{c}{224\\(\\times\\)448 (0.0,4)} \\\\  & & **FID\\(\\downarrow\\)** & **sFID\\(\\downarrow\\)** & **IS\\(\\uparrow\\)** & **Prec.\\(\\uparrow\\)** & **Rec.\\(\\uparrow\\)** & **FID\\(\\downarrow\\)** & **sFID\\(\\downarrow\\)** & **IS\\(\\uparrow\\)** & **Prec.\\(\\uparrow\\)** & **Rec.\\(\\uparrow\\)** & **FID\\(\\downarrow\\)** & **sFID\\(\\downarrow\\)** & **IS\\(\\uparrow\\)** & **Prec.\\(\\uparrow\\)** & **Rec.\\(\\uparrow\\)** \\\\ \\hline\n' +
      '**DT-B** & Abs. PE & MLP & Fixed & 44.83 & **8.49** & 32.05 & 0.48 & **0.63** & 91.32 & 66.66 & 14.02 & 0.21 & 0.45 & 109.1 & 110.71 & 14.00 & 0.18 & 0.31 \\\\ \\hline Config A & Abs. PE & MLP & Flexible & 43.34 & 11.11 & 32.32 & 0.48 & 60.1 & 50.51 & 10.36 & 25.26 & 0.42 & 0.60 & 52.55 & 160.58 & 28.69 & 0.42 & **0.58** \\\\ Config B & Abs. PE & SwiGLU & Flexible & 41.75 & 11.53 & 34.55 & 0.49 & 0.61 & 48.66 & 10.68 & 26.76 & 0.41 & 6.60 & 52.34 & 17.73 & 30.01 & 0.41 & 0.57 \\\\ Config C & Abs. PE + 2D RoPE & MLP & Flexible & 39.11 & 0.79 & 36.35 & 0.51 & 0.64 & 1with bilinear interpolation1. Following ViT (Dosovitskiy et al., 2020), EI is used for absolute positional embedding.\n' +
      '\n' +
      'Footnote 1: torch.nn.functional.interpolate(pe, (h,w), method=“bilinear”)\n' +
      '\n' +
      '**NTK and YaRN.** We set the scale factor to \\(s=\\max(\\max(H_{\\text{test}},W_{\\text{test}})/\\sqrt{256})\\) and adopt the vanilla implementation of the two methods, as in Section 3.1. For YaRN, we set \\(\\alpha=1,\\beta=32\\) in Equation (7).\n' +
      '\n' +
      '**VisionNTK and VisionYaRN.** These two methods are defined detailedly in Definitions 3.1 and 3.2. Note that when the aspect ratio equals one, the VisionNTK and VisionYaRN are equivalent to NTK and YaRN, respectively.\n' +
      '\n' +
      '**Analysis.** We present in Tab. 4 that our FiT-B/2 shows stable performance when directly extrapolating to larger resolutions. When combined with PI, the extrapolation performance of FiT-B/2 at all three resolutions decreases. When combined with YaRN, the FID score reduces by \\(16.77\\) on \\(320\\times 320\\), but the performance on \\(224\\times 448\\) and \\(168\\times 480\\) descends. Our VisionYaRN solves this dilemma and reduces the FID score by \\(40.27\\) on \\(224\\times 448\\) and by \\(41.22\\) at \\(160\\times 480\\) compared with YaRN. NTK interpolation method demonstrates stable extrapolation performance but increases the FID score slightly at \\(224\\times 448\\) and \\(160\\times 480\\) resolutions. Our VisionNTK method alleviates this problem and exceeds the performance of direct extrapolation at all three resolutions. In conclusion, our FiT-B/2 has a strong extrapolation ability, which can be further enhanced when combined with VisionYaRN and VisionNTK methods.\n' +
      '\n' +
      'However, DiT-B/2 demonstrates poor extrapolation ability. When combined with PI, the FID score achieves \\(72.47\\) at \\(320\\times 320\\) resolution, which still falls behind our FiT-B/2. At \\(224\\times 448\\) and \\(160\\times 480\\) resolutions, PI and EI interpolation methods cannot improve the extrapolation performance.\n' +
      '\n' +
      '### FiT In-Distribution Resolution Results\n' +
      '\n' +
      'Following our former analysis, we train our highest Gflops model, FiT-XL/2, for 1.8M steps. We conduct experiments to evaluate the performance of FiT at three different in distribution resolutions: \\(256\\times 256\\), \\(160\\times 320\\), and \\(128\\times 384\\). We show samples from the FiT in Fig 1, and we compare against some state-of-the-art class-conditional generative models: BigGAN (Brock et al., 2018), StyleGAN-XL (Sauer et al., 2022), MaskGIT (Chang et al., 2022), CDM (Ho et al., 2022), U-ViT (Bao et al., 2023), ADM (Dhariwal and Nichol, 2021), LDM (Rombach et al., 2022), MDT (Gao et al., 2023), and DiT (Peebles and Xie, 2023). When generating images of \\(160\\times 320\\) and \\(128\\times 384\\) resolution, we adopt PI on the positional embedding of the DiT model, as stated in Section 4.3. EI is employed in the positional embedding of U-ViT and MDT models, as they use learnable positional embedding. ADM and LDM can directly synthesize images with resolutions different from the training resolution.\n' +
      '\n' +
      'As shown in Tab. 1, FiT-XL/2 outperforms all prior diffusion models, decreasing the previous best FID-50K of 6.93 achieved by U-ViT-H/2-G to 5.74 at \\(160\\times 320\\) resolution. For \\(128\\times 384\\) resolution, FiT-XL/2 shows significant superiority, decreasing the previous SOTA FID-50K of \\(29.67\\) to \\(16.81\\). The FID score of FiT-XL/2 increases slightly at \\(256\\times 256\\) resolution, compared to other models that have undergone longer training steps.\n' +
      '\n' +
      '### FiT Out-Of-Distribution Resolution Results\n' +
      '\n' +
      'We evaluate our FiT-XL/2 on three different out-of-distribution resolutions: \\(320\\times 320\\), \\(224\\times 448\\), and \\(160\\times 480\\) and compare against some SOTA class-conditional generative models: U-ViT, ADM, LDM-4, MDT, and DiT. PI is employed in DiT, while EI is adopted in U-ViT and MDT, as in Section 4.4. U-Net-based methods, such as ADM and LDM-4 can directly generate images with resolution out of distribution. As shown in Table 2, FiT-XL/2 achieves the best FID-50K and IS, on all three resolutions, indicating its outstanding extrapolation ability. In terms of other metrics, as sFID, FiT-XL/2 demonstrates competitive performance.\n' +
      '\n' +
      'LDMs with transformer backbones are known to have difficulty in generating images out of training resolution, such as DiT, U-ViT, and MDT. More seriously, MDT has almost no ability to generate images beyond the training resolution. We speculate this is because both learnable absolute PE and learnable relative PE are used in MDT. DiT and U-ViT show a certain degree of extrapolation ability and achieve FID scores of \\(9.98\\) and \\(7.65\\) respectively at 320x320 resolution. However, when the aspect ratio is not equal to one, their generation performance drops significantly, as \\(224\\times 448\\) and \\(160\\times 480\\) resolutions. Benefiting from the advantage of the local receptive field of the Convolution Neural Network, ADM and LDM show stable performance at these out-of-distribution resolutions. Our FiT-XL/2 solves the problem of insufficient extrapolation capabilities of the transformer in image synthesis. At \\(320\\times 320\\), \\(224\\times 448\\), and \\(160\\times 480\\) resolutions, FiT-XL/2 exceeds the previous SOTA LDM on FID-50K by \\(0.82\\), \\(0.65\\), and \\(3.52\\) respectively.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'In this work, we aim to contribute to the ongoing research on flexible generating arbitrary resolutions and aspect ratio. We propose Flexible Vision Transformer (FiT) for diffusion model, a refined transformer architecture with flexible training pipeline specifically designed for generating images with arbitrary resolutions and aspect ratios. FiT surpasses all previous models, whether transformer-based or CNN-based, across various resolutions. With our resolution extrapolation method, VisionNTK, the performance of FiT has been significantly enhanced further.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]F. Bao, S. Nie, K. Xue, Y. Cao, C. Li, H. Su, and J. Zhu (2023) All are worth words: a vit backbone for diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '* [2]A. Brock, J. Donahue, and K. Simonyan (2018) Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096. Cited by: SS1.\n' +
      '* [3]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in Neural Information Processing Systems. Cited by: SS1.\n' +
      '* [4]H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman (2022) Maskgit: masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '* [5]S. Chen, S. Wong, L. Chen, and Y. Tian (2023) Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595. Cited by: SS1.\n' +
      '* [6]A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. (2023) Palm: scaling language modeling with pathways. Journal of Machine Learning Research. Cited by: SS1.\n' +
      '* [7]A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. (2023) Palm: scaling language modeling with pathways. Journal of Machine Learning Research. Cited by: SS1.\n' +
      '* [8]M. Dehghani, B. Mustafa, J. Djolonga, J. Heek, M. Minderer, M. Caron, A. Steiner, J. Puigecerver, R. Geirhos, I. Alabdulmohsini, et al. (2023) Patch n\'pack: navit, a vision transformer for any aspect ratio and resolution. arXiv preprint arXiv:2307.06304. Cited by: SS1.\n' +
      '* [9]J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei (2009) Imagenet: a large-scale hierarchical image database. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '* [10]P. Dhariwal and A. Nichol (2021) Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems. Cited by: SS1.\n' +
      '* [11]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, G. Minderer, G. Heigold, S. Gelly, et al. (2020) An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Cited by: SS1.\n' +
      '* [12]P. Esser, R. Rombach, and B. Ommer (2021) Taming transformers for high-resolution image synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '* [13]S. Gao, P. Zhou, M. Cheng, and S. Yan (2023) Masked diffusion transformer is a strong image synthesizer. arXiv preprint arXiv:2303.14389. Cited by: SS1.\n' +
      '* [14]A. Hatamizadeh, J. Song, G. Liu, J. Kautz, and A. Vahdat (2023) Difit: diffusion vision transformers for image generation. arXiv preprint arXiv:2312.02139. Cited by: SS1.\n' +
      '* [15]M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter (2017) Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems. Cited by: SS1.\n' +
      '* [16]J. Ho and T. Salimans (2021) Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, Cited by: SS1.\n' +
      '* [17]J. Ho, A. Jain, and P. Abbeel (2020) Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems. Cited by: SS1.\n' +
      '* [18]J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans (2022) Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research. Cited by: SS1.\n' +
      '* [19]A. Hyvarinen and P. Dayan (2005) Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research. Cited by: SS1.\n' +
      '* [20]T. Kynkaanniemi, T. Karras, S. Laine, and T. Lehtinen (2019) Improved precision and recall metric for assessing generative models. Advances in Neural Information Processing Systems. Cited by: SS1.\n' +
      '* [21]X. Liu, H. Yan, S. Zhang, C. An, X. Qiu, and D. Lin (2023) Scaling laws of rope-based extrapolation. arXiv preprint arXiv:2310.05209. Cited by: SS1.\n' +
      '* [22]Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo (2021) Swin transformer: hierarchical vision transformer using shifted windows. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '* [23]Z. Liu, H. Mao, C. Wu, C. Feichtenhofer, T. Darrell, and S. Xie (2022) A convnet for the 2020s. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '* [24]L. L. Ma (2020) Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation. Note: [https://www.reddit.com/r/LocalLLaMA/](https://www.reddit.com/r/LocalLLaMA/)comments/141z7j5/ntkaware_scaled_rope_allows_llama_models_to_have/. Accessed: 2024-2-1.\n' +
      '* Loshchilov and Hutter (2017) Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* Nash et al. (2021) Nash, C., Menick, J., Dieleman, S., and Battaglia, P. W. Generating images with sparse representations. _arXiv preprint arXiv:2103.03841_, 2021.\n' +
      '* Peebles and Xie (2023) Peebles, W. and Xie, S. Scalable diffusion models with transformers. In _IEEE/CVF International Conference on Computer Vision_, 2023.\n' +
      '* Peng et al. (2023) Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarm: Efficient context window extension of large language models. _arXiv preprint arXiv:2309.00071_, 2023.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, 2021.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.\n' +
      '* Ruoss et al. (2023) Ruoss, A., Deletang, G., Genewein, T., Grau-Moya, J., Csordas, R., Bennani, M., Legg, S., and Veness, J. Randomized positional encodings boost length generalization of transformers. _arXiv preprint arXiv:2305.16843_, 2023.\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* Salimans et al. (2016) Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. _Advances in Neural Information Processing Systems_, 2016.\n' +
      '* Sauer et al. (2022) Sauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling stylegan to large diverse datasets. In _ACM SIGGRAPH 2022 conference proceedings_, 2022.\n' +
      '* Shazeer (2020) Shazeer, N. Glu variants improve transformer. _arXiv preprint arXiv:2002.05202_, 2020.\n' +
      '* Song et al. (2020a) Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020a.\n' +
      '* Song et al. (2020b) Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020b.\n' +
      '* Su et al. (2024) Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 2024.\n' +
      '* Sun et al. (2022) Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X.,, and Wei, F. A length-extrapolatable transformer. _arXiv preprint arXiv:2212.10554_, 2022.\n' +
      '* Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Touvron et al. (2019) Touvron, H., Vedaldi, A., Douze, M., and Jegou, H. Fixing the train-test resolution discrepancy. _Advances in Neural Information Processing Systems_, 2019.\n' +
      '* Touvron et al. (2021) Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image transformers & distillation through attention. In _International conference on machine learning_, pp. 10347-10357. PMLR, 2021.\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., and et al, B. R. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., and et al, N. B. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023b.\n' +
      '* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. _Advances in Neural Information Processing Systems_, 2017.\n' +
      '\n' +
      '## Appendix A Experimentin Setups\n' +
      '\n' +
      'We provide detailed network configurations and performance of all models, which are listed in Tab. 5.\n' +
      '\n' +
      '## Appendix B Detailed Attention Score with 2D RoPE and decoupled 2D-RoPE.\n' +
      '\n' +
      '2D RoPE defines a vector-valued complex function \\(f(\\mathbf{x},h_{m},w_{m})\\) in Equation (10) as follows:\n' +
      '\n' +
      '\\[\\begin{split} f(\\mathbf{x},h_{m},w_{m})=\\left[(x_{0}+ix_{1})e^{ ih_{m}\\theta_{0}},(x_{2}+ix_{3})e^{ih_{m}\\theta_{1}},\\dots,(x_{d/2-2}+ix_{d/2-1})e^{ih_{ m}\\theta_{d/4-1}},\\right.\\\\ \\left.(x_{d/2}+ix_{d/2+1})e^{iw_{m}\\theta_{0}},(x_{d/2+2}+ix_{d/2+ 3})e^{iw_{m}\\theta_{1}},\\dots,(x_{d-2}+ix_{d-1})e^{iw_{m}\\theta_{d/4-1}}\\right] ^{T}.\\end{split} \\tag{17}\\]\n' +
      '\n' +
      'The self-attention score \\(A_{n}\\) injected with 2D RoPE in Equation (11) is detailed defined as follows:\n' +
      '\n' +
      '\\[\\begin{split} A_{n}=&\\mathrm{Re}\\langle f_{q}( \\mathbf{q}_{m},h_{m},w_{m}),f_{k}(\\mathbf{k}_{n},h_{n},w_{n})\\rangle\\\\ =&\\mathrm{Re}\\left[\\sum_{j=0}^{d/4-1}(q_{2j}+iq_{2j+ 1})(k_{2j}-ik_{2j+1})e^{i(h_{m}-h_{n})\\theta_{j}}+\\sum_{j=0}^{d/4-1}(q_{2j}+iq _{2j+1})(k_{2j}-ik_{2j+1})e^{i(w_{m}-w_{n})\\theta_{j}}\\right]\\\\ =&\\sum_{j=0}^{d/4-1}[(q_{2j}k_{2j}+q_{2j+1}k_{2j+1}) \\cos((h_{m}-h_{n})\\theta_{j})+(q_{2j}k_{2j+1}-q_{2j+1}k_{2j})\\sin((h_{m}-h_{n} )\\theta_{j})]+\\\\ &\\sum_{j=0}^{d/4-1}[(q_{2j}k_{2j}+q_{2j+1}k_{2j+1})\\cos((w_{m}- w_{n})\\theta_{j})+(q_{2j}k_{2j+1}-q_{2j+1}k_{2j})\\sin((w_{m}-w_{n})\\theta_{j})], \\end{split} \\tag{18}\\]\n' +
      '\n' +
      'where 2-D coordinates of width and height as \\(\\{(w,h)\\Big{|}1\\leqslant w\\leqslant W,1\\leqslant h\\leqslant H\\}\\), the subscripts of \\(q\\) and \\(k\\) denote the dimensions of the attention head, \\(\\theta^{n}=10000^{-2n/d}\\). There is no cross-term between \\(h\\) and \\(w\\) in 2D-RoPE and attention score \\(A_{n}\\), so we can further decouple the rotary frequency as \\(\\Theta_{h}=\\{\\theta_{d}^{h}=b_{h}^{-2d/|D|},1\\leqslant d\\leqslant\\frac{|D|}{2}\\}\\) and \\(\\Theta_{w}=\\{\\theta_{d}^{w}=b_{w}^{-2d/|D|},1\\leqslant d\\leqslant\\frac{|D|}{2}\\}\\), resulting in the decoupled 2D-RoPE, as follows:\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline Models & Layers & Dim. & Head Num. & Patch Size & Max Token Length & Training Steps & Batch Size & Learning Rate & FID-50K \\\\ \\hline DiT-B/2 & 12 & 768 & 12 & 2 & 256 & 400K & 256 & \\(1\\times 10^{-4}\\) & 44.83 \\\\ Config A & 12 & 768 & 12 & 2 & 256 & 400K & 256 & \\(1\\times 10^{-4}\\) & 43.34 \\\\ Config B & 12 & 768 & 12 & 2 & 256 & 400K & 256 & \\(1\\times 10^{-4}\\) & 41.75 \\\\ Config C & 12 & 768 & 12 & 2 & 256 & 400K & 256 & \\(1\\times 10^{-4}\\) & 39.11 \\\\ Config D & 12 & 768 & 12 & 2 & 256 & 400K & 256 & \\(1\\times 10^{-4}\\) & 37.29 \\\\ FiT-B/2 & 12 & 768 & 12 & 2 & 256 & 400K & 256 & \\(1\\times 10^{-4}\\) & 36.36 \\\\ \\hline FiT-XL/2-G & 28 & 1152 & 16 & 2 & 256 & 1800K & 256 & \\(1\\times 10^{-4}\\) & 4.27 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Network configurations and performance of all models.\n' +
      '\n' +
      '\\[\\begin{split} A_{n}=&\\sum_{j=0}^{d/4-1}[(q_{2j}k_{2j}+q_{2 j+1}k_{2j+1})\\cos((h_{m}-h_{n})\\theta_{j}^{h})+(q_{2j}k_{2j+1}-q_{2j+1}k_{2j})\\sin((h_{m} -h_{n})\\theta_{j}^{h})]+\\\\ &\\sum_{j=0}^{d/4-1}[(q_{2j}k_{2j}+q_{2j+1}k_{2j+1})\\cos((w_{m}-w_ {n})\\theta_{j}^{w})+(q_{2j}k_{2j+1}-q_{2j+1}k_{2j})\\sin((w_{m}-w_{n})\\theta_{j }^{w})]\\\\ =&\\mathrm{Re}\\left[\\sum_{j=0}^{d/4-1}(q_{2j}+iq_{2j+1 })(k_{2j}-ik_{2j+1})e^{i(h_{m}-h_{n})\\theta_{j}^{h}}+\\sum_{j=0}^{d/4-1}(q_{2j} +iq_{2j+1})(k_{2j}-ik_{2j+1})e^{i(w_{m}-w_{n})\\theta_{j}^{w}}\\right]\\\\ =&\\mathrm{Re}(\\hat{f}_{q}(\\mathbf{q}_{m},h_{m},w_{m} ),\\hat{f}_{k}(\\mathbf{k}_{n},h_{n},w_{n})).\\end{split} \\tag{19}\\]\n' +
      '\n' +
      'So we can reformulate the vector-valued complex function \\(\\hat{f}(\\mathbf{x},h_{m},w_{m})\\) in Equation (13) as follows:\n' +
      '\n' +
      '\\[\\begin{split}\\hat{f}(\\mathbf{x},h_{m},w_{m})=&\\left[ (x_{0}+ix_{1})e^{ih_{m}\\theta_{0}^{h}},(x_{2}+ix_{3})e^{ih_{m}\\theta_{1}^{h}}, \\ldots,(x_{d/2-2}+ix_{d/2-1})e^{ih_{m}\\theta_{d/4-1}^{h}},\\right.\\\\ &\\left.(x_{d/2}+ix_{d/2+1})e^{iw_{m}\\theta_{0}^{w}},(x_{d/2+2}+ ix_{d/2+3})e^{iw_{m}\\theta_{1}^{w}},\\ldots,(x_{d-2}+ix_{d-1})e^{iw_{m} \\theta_{d/4-1}^{w}}\\right]^{T}.\\end{split} \\tag{20}\\]\n' +
      '\n' +
      '## Appendix C Limitations and Future Work\n' +
      '\n' +
      'Constrained by limited computational resources, we only train FiT-XL/2 for 1800K steps. At the resolution of 256x256, the performance of FiT-XL/2 is slightly inferior compared to the DiT-XL/2 model. On the other hand, we have not yet thoroughly explored the generative capabilities of the FiT-XL/2 model when training with higher resolutions (larger token length limitation). Additionally, we only explore resolution extrapolation techniques that are training-free, without delving into other resolution extrapolation methods that require additional training. We believe that FiT will enable a range of interesting studies that have been infeasible before and encourage more attention towards generating images with arbitrary resolutions and aspect ratios.\n' +
      '\n' +
      '## Appendix D More Model Samples\n' +
      '\n' +
      'We show samples from our FiT-XL/2 models at resolutions of \\(256\\times 256\\), \\(224\\times 448\\) and \\(448\\times 224\\), trained for 1.8M (generated with 250 DDPM sampling steps and the ft-EMA VAE decoder). Fig. 6 shows uncurated samples from FiT-XL/2 with classifier-free guidance scale 4.0 and class label "loggerhead turtle" (33). Fig. 7 shows uncurated samples from FiT-XL/2 with classifier-free guidance scale 4.0 and class label "Cacatua galerita" (89). Fig. 8 shows uncurated samples from FiT-XL/2 with classifier-free guidance scale 4.0 and class label "golden retriever" (207). Fig. 9 shows uncurated samples from FiT-XL/2 with classifier-free guidance scale 4.0 and class label "white fox" (279). Fig. 10 shows uncurated samples from FiT-XL/2 with classifier-free guidance scale 4.0 and class label "otter" (360). Fig. 11 shows uncurated samples from FiT-XL/2 with classifier-free guidance scale 4.0 and class label "volcano" (980).\n' +
      '\n' +
      'We also show some failure samples from DiT-XL/2, as shown in Fig. 5.\n' +
      '\n' +
      'Figure 4: Height/Width distribution of the original _ImageNet_(Deng et al., 2009) dataset.\n' +
      '\n' +
      'Figure 5: Uncurated failure samples from DiT-XL/2.\n' +
      '\n' +
      'Figure 8: Uncurated samples from FiT-XL/2 models at resolutions of \\(256\\times 256\\), \\(224\\times 448\\) and \\(448\\times 224\\).\n' +
      '\n' +
      'Figure 10: Uncurated samples from FiT-XL/2 models at resolutions of \\(256\\times 256\\), \\(224\\times 448\\) and \\(448\\times 224\\).\n' +
      '\n' +
      'Figure 6: Uncurated samples from FiT-XL/2 models at resolutions of \\(256\\times 256\\), \\(224\\times 448\\) and \\(448\\times 224\\).\n' +
      '\n' +
      'Figure 7: Uncurated samples from FiT-XL/2 models at resolutions of \\(256\\times 256\\), \\(224\\times 448\\) and \\(448\\times 224\\).\n' +
      '\n' +
      'Figure 9: Uncurated samples from FiT-XL/2 models at resolutions of \\(256\\times 256\\), \\(224\\times 448\\) and \\(448\\times 224\\).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>