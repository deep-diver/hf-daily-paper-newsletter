<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Larimar: Large Language Models with Episodic Memory Control\n' +
      '\n' +
      'Payel Das\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Subhajit Chaudhury\n' +
      '\n' +
      '1\n' +
      '\n' +
      ' Elli Nelson\n' +
      '\n' +
      'Igor Melnyk\n' +
      '\n' +
      '**Sarathkrishna Swaminathan**\n' +
      '\n' +
      '**Sihui Dai**\n' +
      '\n' +
      '2\n' +
      '\n' +
      'Aurelie Lozano\n' +
      '\n' +
      'Georgios Kollias\n' +
      '\n' +
      'Vijil Chenthamarakshan\n' +
      '\n' +
      '**Jiri Navratil**\n' +
      '\n' +
      '**Soham Dan**\n' +
      '\n' +
      '**Pin-Yu Chen**\n' +
      '\n' +
      'Equal contribution IBM AI Research Princeton University. Correspondence to: Payel Das \\(<\\)daspa@us.ibm.com\\(>\\).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar\'s memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed--yielding speed-ups of 4-10x depending on the base LLM --as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness.\n' +
      '\n' +
      'Machine Learning, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Pre-trained Large Language Models (LLMs) have achieved impressive performance on various Natural Language Processing (NLP) tasks (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Vaswani et al., 2017), and are often considered as knowledge repositories (Petroni et al., 2019). In order to keep these models fact-relevant, safe, and ethical after deployment - the _knowledge of the LLM needs to be constantly updated_. Thus, it is critical to develop efficient mechanisms to quickly update LLMs so that models can protect privacy, eliminate bias and hallucination, and catch up with new facts. Model editing should remove the undesired, incorrect, or obsolete facts from the LLM\'s "memory", and optionally replace it with the desired outcome. Similarly, the ability to quickly update the LLM can also help with the challenging problem of input _context length generalization_ beyond the training distribution,which is crucial when learning from datasets where longer context instances are rare (Anil et al., 2022; Kazemnejad et al., 2023). A straightforward solution is to fine-tune the model on the corrected/new datasets. Such an approach suffers the risk of overfitting and catastrophic forgetting (Kirkpatrick et al., 2017; Zhu et al., 2020), as the knowledge is implicitly and distributionally encoded across the LLM parameters. Several lines of research have proposed effective and precise LLM editing (for comprehensive surveys on LLM editing, see (Li et al., 2022; Liu et al., 2023; Zhu et al., 2020)), which includes training an external memory model or a hypernetwork model to work alongside with the frozen LLM. Another popular approach is to locate the original fact within LLM features and then do a local parameter update. As shown in Table 1, both lines of methods face scalability problems due to overfitting and the need for retraining or locating for new states, causing a slow-down in editing speed. The high memory needs for storing numerous edits provide a further obstacle in terms of scaling to sequential and batch editing setups. These challenges hinder the application of updating large language models in real-world industrial settings. Further, handling fact editing and selective fact forgetting appear challenging within the same methodological framework even for current state-of-the-art editing methods (Patil et al., 2023), while both new information learning and old information forgetting are intrinsically related to each other in in brain (Dempsey et al., 2022; Autore et al., 2023).\n' +
      '\n' +
      'Humans, in contrast, can very quickly perform knowledge updating and generalization, both of which conform to rapid learning after seeing the first relevant instance. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Consistently, while both semantic and working memory systems struggle with sequential decision making tasks, the episodic memory systems are found to be beneficial (Blundell et al., 2016; Lengyel and Dayan, 2007). The complementary learning systems (CLS) theory (Kumaran et al., 2016) provides rationale for coupling complementary _fast_ (hippocamps) and _slow_ (neocortex) learning systems in brain, former learning from single instances while later modeling the input distribution. The neocortex-hippocamps interactions in brain is known to promote adaptive behavior via memorization and generalization (Sun et al., 2023). Further, it is proposed that the memory consolidation from hippocampus to neocortex is facilitated through the activation synchronized with multiple exact or false replays of the encoded experience in hippocampus - suggesting hippocampus taking the form of a _generative associative network_(Ramirez et al., 2013).\n' +
      '\n' +
      'Inspired by these insights, we propose **Larimar** - a class of LLMs augmented with an external episodic memory controller. We follow the CLS view, where a hippocampal fast-learning system records samples as episodic memory, and a neocortical slow learning system (the LLM) learns summary statistics of the input distribution as semantic memory. Our aim is to treat the episodic memory module as the global storage of the current set of factual updates or edits, and enforce this memory as a condition to the LLM decoder. It is important to learn to update this memory efficiently and accurately, without having to go through any training, as new edits arrive.\n' +
      '\n' +
      'To tackle this, we seek to utilize a hierarchical memory, similar in spirit to the Kanerva Machine (Wu et al., 2018), where the memory writes and reads are interpreted as inference in a generative model. Specifically, we consider the memory model of (Pham et al., 2021), which treats the memory as deterministic, thereby allowing reformulating the Bayesian updates of memory and address proposed in Kanerva Machine as finding least-square solutions to linear systems. Once updated, this fast-learning memory is then used to condition a slow-learning LLM decoder.\n' +
      '\n' +
      'The use of a _global_ memory associated a set of samples and the ability to _fast write_ to memory make this hierarchical memory framework attractive for efficient LLM updating with respect to new knowledge. Implementation-wise, the memory is coupled to the LLM by end-to-end gradient descent on _generic_ data and does not assume access to edits. During inference, the new data is written to memory in one-shot, the updated memory then conditions the LLM decoding to enforce the edited output. We further formalize training-free _selective fact forgetting_ and _information leakage prevention_ operations based on Larimar\'s one-shot memory updating mechanism.\n' +
      '\n' +
      'To our knowledge, this is the first work that proposes and demonstrates online distributed writing to a hierarchical conditional memory model as a solution to test-time adaptation of LLMs to new knowledge. We demonstrate Larimar on single and sequential fact editing tasks on existing benchmarks and compared with baseline methods. Larimar provides accurate and precise editing across these settings, while being up to 10 times faster compared to competitive model editing baselines We further subject Larimar to selective fact forgetting and information leakage prevention and show its efficacy in those tasks. Lastly, we provide a simple recursive search-based solution that enables Larimar\'s memory to generalize to longer input context.\n' +
      '\n' +
      'Our contributions are:\n' +
      '\n' +
      '* Inspired by complementary learning mechanisms in the brain, we propose a class of episodic and adaptable memory-conditioned LLM architectures for test time adaptation in real-time. Our method does not _need_ any time-intensive gradient-based learning or fact tracing within the LLM for performing the edit, providing a faster alternative for LLM updating.\n' +
      '* We demonstrate the utility of this architecture on two relevant and challenging use cases: knowledge editing and input context length generalization. Larimar shows fast and accurate training-free adaptation to new inputs in both scenarios, compared to baseline editing methods and language models.\n' +
      '* We show selective fact forgetting and information leakage prevention using one-shot memory updating.\n' +
      '* We provide a simple means to enable long context generalization in Larimar, based on a recursive search on its memory space.\n' +
      '\n' +
      '## 2 Model architecture\n' +
      '\n' +
      '**Notation**: We define input and output spaces as \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\), respectively. The model comprises an encoder \\(e:\\mathcal{X}\\rightarrow\\mathbb{R}^{C}\\) and a decoder \\(d:\\mathbb{R}^{C}\\rightarrow\\mathcal{Y}\\), linked via an adaptive memory. The encoder outputs in a latent space of dimension \\(C\\). The memory uses \\(K\\) rows to store encoded episodes of length \\(N\\), with initial state \\(\\mathbf{M}_{0}\\in\\mathbb{R}^{K\\times C}\\) and updates through reading and writing weights \\(\\mathbf{W},\\mathbf{W}_{0}\\in\\mathbb{R}^{N\\times K}\\), resulting in updated memory \\(\\mathbf{M}\\).\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      'Given the memory \\(\\mathbf{M}\\), Kanerva Machine aims to maximize the conditional log-likelihood of \\(\\ln p(\\mathbf{X}|\\mathbf{M})\\), where \\(\\mathbf{X}\\) is an exchangeable (order invariant) episode: \\(\\mathbf{X}=\\{x_{1},\\ldots,x_{N}\\}\\), a subset of the input data consisting of \\(N\\) samples. A variational lower bound of this conditional likelihood is optimized, similar to in variational autoencoders (Kingma and Welling, 2013). Consequently, the model learns to compress \\(\\mathbf{X}\\) in a memory \\(\\mathbf{M}\\), which then becomes a distributed associative memory. In practice, \\(\\mathbf{M}\\) is learned on a noisy version of the latent encodings \\(\\mathbf{Z}+\\xi\\) where \\(\\mathbf{Z}=e(\\mathbf{X})\\) for an episode. In the remainder of this study, we use \\(\\mathbf{M}\\) as the posterior memory dependent on an episode \\(\\mathbf{X}\\), whereas \\(\\mathbf{M}_{0}\\) denotes a prior memory. The reading weight matrix, \\(\\mathbf{W}\\), is a random variable to enforce generative ability of the model, for which we use a standard Gaussian prior \\(p(\\mathbf{W})\\sim\\mathcal{N}(0,I_{N\\times K})\\) and posterior \\(q(\\mathbf{W})\\sim\\mathcal{N}(\\overline{\\mathbf{W}},\\sigma_{\\mathbf{W}}^{2} \\cdot I_{N\\times K})\\), where the mean \\(\\overline{\\mathbf{W}}\\) is estimated from each episode and \\(\\sigma_{\\mathbf{W}}\\) is learnable. The memory readouts are obtained as \\(\\mathbf{Z}_{readout}=\\mathbf{W}\\mathbf{M}\\). The overall memory-augmented architecture is depicted in Figure 1.\n' +
      '\n' +
      'During training all the three modules - encoder (\\(e\\)), associative memory (\\(\\mathbf{M}\\)), and decoder (\\(d\\)) - are jointly trained and optimized for an episode \\(\\mathbf{X}\\), using the following loss:\n' +
      '\n' +
      '\\[L= \\mathbb{E}_{\\mathbf{X}\\sim\\text{data}}\\big{(}\\mathbb{E}_{q(\\mathbf{ W})}\\ln p(\\mathbf{X}|\\mathbf{W},\\mathbf{M})\\] \\[+\\alpha\\ln p(d(e(\\mathbf{X}))-\\beta D_{KL}(q(\\mathbf{W})||p( \\mathbf{W}))\\] \\[-\\mathbb{E}_{\\mathbf{X}\\sim\\text{pretrain}}\\ln p(\\mathbf{x}_{i}| \\mathbf{x}_{i-1}..\\mathbf{x}_{1}). \\tag{1}\\]\n' +
      '\n' +
      'The first term is the negative reconstruction loss with memory and \\(\\mathbf{W}\\), a \\(N\\times K\\) matrix. The second is the autoencoder\'s negative reconstruction loss without memory. The third is the KL divergence between prior \\(p(\\mathbf{W})\\) and posterior \\(q(\\mathbf{W})\\). To maintain decoder performance during training, a pretraining data regularization term is added.\n' +
      '\n' +
      '### Memory inference\n' +
      '\n' +
      'Once \\(\\mathbf{M}_{0}\\) is trained via backpropagation, the posterior memory \\(\\mathbf{M}\\) is updated in one-shot by solving a minimization problem as proposed in (Pham et al., 2021), which is \\(\\text{min}_{\\mathbf{M}}||\\mathbf{Z}_{\\text{c}}-\\mathbf{W}_{0}\\mathbf{M}||_{F}^ {2}\\). This minimization problem, which corresponds to solving a linear system of equations, is efficiently done via computing matrix pseudo inverses.\n' +
      '\n' +
      '**Implementation**:\n' +
      '\n' +
      'We employed a BERT large encoder (Devlin et al., 2018) combined with either a GPT2-large (Radford et al., 2019) or a GPTJ-6B decoder and a memory matrix (512x768) for our training experiments, naming the resulting models Larimar-1.3B and Larimar-6B, respectively. Our training data comprised 7.6 million examples constructed by splitting WikiText (Merity et al., 2016) texts to small chunks of 64 tokens. In testing, the Larimar-1.3B model achieved a perplexity of 14.6, while the Larimar-6B model reached 15.9 on 1,000 random WikiText samples, indicating that adding memory barely affects performance. We trained Larimar-6B models for 10 epochs using Adam optimizer, learning rate 5e-6 and batch size 32. For the Larimar-6B\'s training, we used a setup with eight NVIDIA A100-80GB GPUs on a single node, utilizing bfloat16 precision and PyTorch Lightning with the DeepSpeed ZeRO Stage 2 for efficient distributed training.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline\n' +
      '**Editor** & **+Edit Train** & **+Fact Trace** & **Sequential Edit** & **Batch Edit** & **Forgetting/Deletion** & **Time (GPT-2)** & **Time (GPT-J)** \\\\ \\hline ROME & No & Yes & No & No & Yes & 4.8s & 13.9s \\\\ GRACE & Yes & No & Yes & No & No & 13.9s & 19.3s \\\\ Larimar & No & No & Yes & Yes & Yes & 1.1s & 1.7s \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Comparison between different editing methods from the requirement and capability perspective. ROME and GRACE from existing editing methods are shown. The wall clock time for a single edit (averaged over 10 edits) on the CounterFact dataset for ROME (Meng et al., 2022) and GRACE (Hartvigsen et al., 2022) was computed using the EasyEdit (Wang et al., 2023) framework with a single A100 (80G) GPU.\n' +
      '\n' +
      'Figure 1: Larimar Architecture: \\(X\\) and \\(X_{query}\\) respectively denote data input and query, \\(Z\\), \\(Z_{query}\\) and \\(Z_{r}\\) are the latent vectors, and \\(M\\) is the fixed-size memory. \\(W\\) and \\(W_{0}\\) are reading/writing weights to memory. \\(W_{M}\\) interfaces the readout from memory to the decoder.\n' +
      '\n' +
      '## 3 Memory operations\n' +
      '\n' +
      'Write, Read, Generate operationsThe three basic memory operations, write in, read out, and generate, which act upon the \\(\\mathbf{Z}\\) encodings, are cast as in (Pham et al., 2021). See Algorithm 1 for details.\n' +
      '\n' +
      'Sequential Writing and ForgettingGiven an initial set of encodings \\(\\mathbf{Z}_{0}\\) and writing weights \\(\\mathbf{W}_{0}\\), we initialize the memory matrix and key covariance matrix:\n' +
      '\n' +
      '\\[\\mathbf{M}_{0}=\\mathbf{W}_{0}^{\\dagger}\\mathbf{Z}_{0},\\quad\\mathbf{C}_{0}= \\mathbf{W}_{0}^{\\top}\\mathbf{W}_{0} \\tag{2}\\]\n' +
      '\n' +
      'To sequentially update the memory \\(\\mathbf{M}_{i-1}\\), either to add a new set of encodings \\(\\mathbf{Z}_{i}\\) or to forget a previously written set of encodings \\(\\mathbf{Z}_{i}\\), we jointly update the memory matrix and key covariance matrix for \\(i=1,2,...\\) as follows:\n' +
      '\n' +
      '\\[\\mathbf{C}_{i} =\\mathbf{C}_{i-1}+\\alpha_{i}\\mathbf{W}_{i}^{\\top}\\mathbf{W}_{i} \\tag{3}\\] \\[\\mathbf{M}_{i} =\\mathbf{M}_{i-1}+\\alpha_{i}\\mathbf{C}_{i}^{-1}\\mathbf{W}_{i}^{ \\top}(\\mathbf{Z}_{i}-\\mathbf{W}_{i}\\mathbf{M}_{i-1}) \\tag{4}\\]\n' +
      '\n' +
      'When writing new encodings to memory, we use \\(\\alpha_{i}=1\\). When forgetting encodings which were previously written to memory with \\(\\alpha_{i_{write}}=1\\) at any \\(i_{write}<i\\), we use \\(\\alpha_{i}=-1\\). Eq. (4) updates the memory sequentially such that it remains the least-squares solution for the growing sequence of data. Assuming that \\(\\mathbf{M}_{i-1}\\) is the least-squares solution with respect to encodings \\(\\mathbf{Z}_{0:i-1}\\), that is,\n' +
      '\n' +
      '\\[\\mathbf{M}_{i-1}=\\text{argmin}_{\\mathbf{M}}\\sum_{j=0}^{i-1}||\\mathbf{Z}_{j}- \\mathbf{W}_{j}\\mathbf{M}||_{2}^{2}, \\tag{5}\\]\n' +
      '\n' +
      'then Eq. (4) with \\(\\alpha_{i}=1\\) ensures that \\(\\mathbf{M}_{i}\\) likewise is the least-squares solution with respect to \\(\\mathbf{Z}_{0:i}\\)(Meng et al., 2023)). In the case \\(\\alpha_{i}=-1\\) and \\(\\mathbf{Z}_{i}=\\mathbf{Z}_{i_{forget}}\\) for some \\(i_{forget}<i\\), Eq. (4) ensures that \\(\\mathbf{M}_{i}\\) is the least-squares solution with \\(\\mathbf{Z}_{i_{forget}}\\) removed from the data, that is,\n' +
      '\n' +
      '\\[\\mathbf{M}_{i}=\\text{argmin}_{\\mathbf{M}}\\sum_{j=0,j\\neq i_{forget}}^{i-1}|| \\mathbf{Z}_{j}-\\mathbf{W}_{j}\\mathbf{M}||_{2}^{2}, \\tag{6}\\]\n' +
      '\n' +
      'The weights can be computed either (following (Pham et al., 2021)) in terms of the current memory, \\(\\mathbf{W}_{i}=\\mathbf{Z}_{i}\\mathbf{M}_{i-1}^{\\dagger}\\), or in terms of a fixed reference memory, \\(\\mathbf{W}_{i}=\\mathbf{Z}_{i}(\\mathbf{M}^{(\\mathrm{ref})})^{\\dagger}\\). \\(\\mathbf{M}^{(\\mathrm{ref})}\\) remains unchanged across all sequential updates (i.e. is \\(i\\)-independent), is used only during inference, and can (optionally) be constructed using the episode of data encountered during inference. In the event that we wish to remove a given previously written encoding from memory, the fixed nature of \\(\\mathbf{M}^{(\\mathrm{ref})}\\) allows the original writing key \\(\\mathbf{W}_{i_{write}}\\) to be recomputed at a later point in the sequence \\(i_{forget}>i_{write}\\), so that the information can be located in memory and removed.\n' +
      '\n' +
      '```\n' +
      'Functionwrite(\\(\\mathbf{Z}\\)): //\\(\\mathbf{Z}\\) - encoding of the episode to be written to memory (i.e. \\(\\mathbf{Z}=e(\\mathbf{X})\\)) Sample \\(\\xi\\sim\\mathcal{N}(0,\\sigma_{\\xi}^{2}I)\\) Let \\(\\mathbf{Z}_{\\xi}=\\mathbf{Z}+\\xi\\) Compute addressing weight \\(\\mathbf{W}_{0}=\\mathbf{Z}_{\\xi}\\mathbf{M}_{0}^{\\dagger}\\)//\\(\\mathbf{M}_{0}\\) is a learned parameter representing prior memory Compute posterior memory \\(\\mathbf{M}=\\mathbf{W}_{0}^{\\dagger}\\mathbf{Z}_{\\xi}\\)return\\(\\mathbf{M}\\) Functionread(\\(\\mathbf{Z}\\),\\(\\mathbf{M}\\)): //\\(\\mathbf{M}\\) - posterior memory from //\\(\\mathbf{Z}\\) - encoding of the read input (ie. \\(\\mathbf{Z}=e(\\mathbf{X})\\)) Compute mean addressing weight \\(\\overline{\\mathbf{W}}=\\mathbf{Z}\\mathbf{M}^{\\dagger}\\) Sample \\(\\mathbf{W}\\sim\\mathcal{N}(\\overline{\\mathbf{W}},\\sigma_{\\mathbf{W}}^{2}I)\\)//\\(\\sigma_{\\mathbf{W}}\\) is a learned parameter Compute output latent \\(\\mathbf{Z}_{\\text{read}}=\\mathbf{W}\\mathbf{M}\\)return\\(\\mathbf{Z}_{\\text{read}}\\) Functiongenerate(\\(\\mathbf{M}\\)): //\\(\\mathbf{M}\\) is the posterior memory from a previous write Sample \\(\\mathbf{W}\\sim\\mathcal{N}(0,I)\\) Compute output latent \\(\\mathbf{Z}=\\mathbf{W}\\mathbf{M}\\)return\\(\\mathbf{Z}\\)\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1** Basic Memory operations (Pham et al., 2021)\n' +
      '\n' +
      '## 4 Scope Detector\n' +
      '\n' +
      'We also optionally use a scope detection mechanism to detect if the incoming query is close to the facts written in the memory, which is conceptually similar to SERAC (Mitchell et al., 2022). If the query is in-scope, then the corresponding readout from memory is passed to the decoder for memory-conditional decoding otherwise the query is subjected to unconditioned decoding. We consider two different scenarios:\n' +
      '\n' +
      '**External encoding-based scope detector (ESD)**: Sample embeddings are estimated from an external sentence encoder (MiniLM1) trained on 1.1B sentence pairs and with an output space dimensionality of 384. The ESD stores encoded facts as vectors in its own scope storage. At test time, given an encoded input sentence, 1-nearest neighbor cosine similarity is calculated and serves as detection score. Any multi-sentence input is first split into isolated sentences, each of which is processed separately and maximum similarity is taken. Measured on 3800 positive and negative samples from the EasyEdit data set, this ESD model achieves a detection equal-error-rate of 2.9% and an F1 score of 0.974.\n' +
      '\n' +
      'Footnote 1: [https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n' +
      '\n' +
      '**Internal Encoding-based scope detector (ISD)**: Larimarencoder \\(e\\) is used to embed CounterFact samples. The encodings are then used to train a binary scope classifier, where positive samples come from rephrasings of an original fact and negative data correspond to neighboring facts.\n' +
      '\n' +
      '## 5 Results\n' +
      '\n' +
      '### Wall Clock time\n' +
      '\n' +
      'Table 1 presents the wall clock time for each editing method across 10 edits, calculated within the EasyEdit framework (Yao et al., 2023) on a single A100 GPU. Results show that Larimar is 4-10x times faster compared to ROME (Meng et al., 2022) and GRACE (Hartvigsen et al., 2022), two most competitive existing LLM editing baselines. Table 7 in Appendix further provides a edit-time comparison within other existing baselines, as shown in (Yao et al., 2023), establishing Larimar\'s advantage on high-speed editing. Table 1 further lists Larimar\'s abilities to handle edits in a training- or tracing- free manner, enabling high-speed editing, handling selective forgetting, and maintain ability to sequential editing setup.\n' +
      '\n' +
      '### Single Fact editing\n' +
      '\n' +
      'We compare the performance of Larimar against a number of recently proposed knowledge editing approaches on the CounterFact dataset (Meng et al., 2022) designed for testing language models handling of counterfactual edits. It includes 21,919 records to assess if the models can learn new facts rather than simply memorizing the target words. Following other works (Meng et al., 2022; Zheng et al., 2023), we used the first 2000 samples of this dataset and report the average over single fact editing results for Larimar-1.3B and Larimar-6B in Table 2. The performance scores for baselines are from (Meng et al., 2022; Zheng et al., 2023) (see Related Work and Appendix for details on baseline methods). As opposed to training the LLM on edits, or causally tracing the original fact within LLM and updating the relevant parameters to reflect edit, we leverage Larimar\'s one-shot memory update for editing. Wherein, the memory posterior is updated as the edit(s) of interest is written, and then the updated memory is queried. The read-out from the memory then conditions the decoder to output the edit.\n' +
      '\n' +
      'The evaluation metrics used in Table 2 are as follows: _Edit Success_, which is the percent of cases where the edited fact \\((s,r,o^{*})\\), (subject, relation, object) with modified object has higher probability than the one based on the original object \\((s,r,o^{c})\\). Specifically, column \\(S\\) measures percentage of \\(\\mathbb{P}[o^{*}]>\\mathbb{P}[o^{c}]\\) cases, while \\(M\\) is the average of \\(\\mathbb{P}[o^{*}]-\\mathbb{P}[o^{c}]\\) in the logits space of the language model. _Paraphrase_ measures the same performance on \\((s,r,o^{*})\\) but using paraphrased prompts. _Neighborhood_ evaluates the model\'s ability to retain knowledge about the original object but in the context of neighboring subjects \\(s^{\\prime}\\): \\((s^{\\prime},r,o^{c})\\). Here the column \\(S\\) reflects percentage of cases where \\(\\mathbb{P}[o^{c}]>\\mathbb{P}[o^{*}]\\), while \\(M\\) is the average \\(\\mathbb{P}[o^{c}]-\\mathbb{P}[o^{*}]\\).\n' +
      '\n' +
      'As can be seen, when compared to existing editing baselines, Larimar achieves comparable performance in successfully editing new facts, and in the ability to handle neighborhood prompts (on par with ROME, when based on GPT-2XL, and better performance, when based on GPT-J), while there remains room to improve generalization. When compared to existing in-context editing approaches (PROMPT and IKE baselines) (Zheng et al., 2023), Larimar does not need multiple in-context demonstrations of the edit and its paraphrases, as well as of neighboring facts, to the decoder, which are retrieved from a corpus. However, as we show in the Appendix (Table 8), when Larimar has access to one additional paraphrase per fact, by writing it in the memory, the generalization performance increases from 76.5 to 82.8. Note that in this setup the average number of added paraphrase per fact is one and we queried the model with a paraphrased prompt unseen by the memory. Further, ablation experiments in Appendix shows that a scope detector, either trained on Larimar encodings or encodings from an external LLM, helps with better paraphrase generalization and neighborhood specificity. Throughout the paper, Larimar is configured with\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r r r} \\hline \\hline  & \\multicolumn{2}{c}{**Edit Success**} & \\multicolumn{2}{c}{**Paraphrase**} & \\multicolumn{2}{c}{**Neighborhood**} \\\\ \\cline{2-7}\n' +
      '**Editor** & S & M & S & M & S & M \\\\ \\hline GPT-2 XL & 22.2 & -4.8 & 24.7 & -5.0 & **78.1** & **5.0** \\\\ \\hline FT* & **100.0** & 98.8 & 87.9 & 46.6 & 40.4 & -6.2 \\\\ FT+L* & 99.1 & 91.5 & 48.7 & 28.9 & 70.3 & 3.5 \\\\ KN & 28.7 & -3.4 & 28.0 & -3.3 & 72.9 & 3.7 \\\\ KE & 84.3 & 33.9 & 75.4 & 14.6 & 30.9 & -11.0 \\\\ KE-CF & **99.9** & 97.0 & 95.8 & 59.2 & 6.9 & -63.2 \\\\ MEND & 99.1 & 70.9 & 65.4 & 12.2 & 37.9 & -11.6 \\\\ MEND-CF & **100.0** & **99.2** & **97.0** & **65.6** & 5.5 & -69.9 \\\\ ROME & **100.0** & 97.9 & **96.4** & **62.7** & **75.4** & **4.2** \\\\\n' +
      '**Larimar-1.3B** & **100.0** & **99.8** & 41.7 & 0.4 & 74.7 & 1.6 \\\\ \\hline \\hline GPT-J & 16.3 & -7.2 & 18.6 & -7.4 & **83.0** & **7.3** \\\\ \\hline FT & **100.0** & **99.9** & **96.6** & **71.0** & 10.3 & -50.7 \\\\ FT+L & 99.6 & 95.0 & 47.9 & 30.4 & 78.6 & 6.8 \\\\ MEND & 97.4 & 71.5 & 53.6 & 11.0 & 53.9 & -6.0 \\\\ ROME & **99.9** & **99.4** & **99.1** & **74.1** & 78.9 & 5.2 \\\\ PROMPT & 99.7 & 80.9 & 91.0 & 32.9 & 37.9 & -2.8 \\\\ IKE (w/ 32 demonstrations) & **100.0** & 91.7 & 95.2 & 64.5 & 77.0 & **35.2** \\\\ IKE (w/o paraphrases) & **100.0** & - & 73.8 & – & 83.4 & – \\\\ IKE (w/o neighbors) & **100.0** & – & 99.8 & – & 11.5 & – \\\\\n' +
      '**Larimar-6B** & 99.6 & 96.0 & 76.5 & 22.4 & **80.2** & 3.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Single fact edit performance on CounterFact dataset comparing Larimar against baseline. Top two best systems are highlighted. Unlike other methods Larimar uses dynamic memory updates with memory-conditioned decoding, and does not require gradient update on edit samples, as opposed to methods that require training (FT, FT+L, MEND) or tracing plus decoder updating (ROME) on edit samples (ROME), or in-context demonstrations (IKE) of (paraphrased) edits and neighboring samples retrieved from a corpus.\n' +
      '\n' +
      'a scope detector, unless otherwise mentioned. For details, see Appendix.\n' +
      '\n' +
      'We also evaluated Larimar on the ZsRE benchmark (Levy et al., 2017), a QA dataset for relation extraction through reading comprehension, with results displayed in Appendix in Table 12. Performance scores for GPT-2 XL based baselines are cited from (Meng et al., 2022), whereas performance of ROME on GPT-J was independently estimated by us. Unlike the CounterFact evaluation, this assessment uses exact match counts for scoring \\(\\mathbb{I}[(o^{*}=\\text{argmax}_{\\text{o}}\\mathbb{P}[o]]\\). Compared to baselines, Larimar demonstrates effective editing and comparable neighborhood specificity on ZsRE, with slightly lower generalization, maintaining consistent results across GPT-2 and GPT-J decoders, underscoring its model-agnostic editing capabilities.\n' +
      '\n' +
      '### Sequential Fact Editing\n' +
      '\n' +
      'We evaluated Larimar\'s ability to perform sequential editing, following the setup of (Hartvigsen et al., 2022), which tackles the issue of forgetting previous edits after multiple sequential edits. Hartvigsen et. al. introduced a continual editing method that integrates an adaptor to update a codebook of edit key-value pairs with a pre-trained language model (GPT2-XL), showing memory retention during sequential edits. We adapt Larimar to this experimental setup, wherein a subset of 200 facts with 5 rephrasings each is selected from the ZsRE validation dataset for testing. In Larimar, a sequential edit is handled by updating the global memory through Eq. (4), again requiring no gradient-based update on incoming edits. For each edit, the encoding of the rephrased query concatenated with the corresponding answer is written to memory. We assessed Larimar\'s performance, compared to GRACE, using the edit retention rate (ERR), which is the mean F1 score after 1000 sequential edits when querying the memory with the encoded query \\(\\mathbf{Z}_{\\mathrm{query}}\\) for each written fact. Larimar is not finetuned on question-answer data; instead, we write each question-answer pair as a fact in the memory and query the memory with the original question. We report on the Test Retention Rate (TRR) by evaluating Larimar decoder\'s perplexity on 1000 random test samples from wikitext using a separate language model. In comparison, baseline models compute TRR from mean F1 scores from 1000 random samples of NQ data. Results show Larimar\'s comparable ERR performance to GRACE, while preserving its original test set performance. Notably, Larimar-1.3B achieves editing speeds approximately 10 or more times faster than GRACE on GPT-2 XL.\n' +
      '\n' +
      'We also evaluated Larimar\'s generalization to rephrased prompts, again comparing to GRACE. We use both (i) a dataset of 1000 ZsRE facts, each with 10 variations, divided into edit and holdout sets, and (ii) an edit/holdout dataset with more rephrasings and fewer (\\(\\approx 500\\)) ZsRE facts. Our analysis, depicted in Figure 3, examines the mean F1 score on the holdout set against the number of memory writes using the edit set, compared to GRACE on the same datasets.2 As Larimar has no knowledge of upcoming edits, it starts with near-zero F1; in contrast, GRACE has prior knowellge from training on the edit set. As the sequence of edits grows, Larimar surpasses GRACE\'s generalization performance at around 600 edits.\n' +
      '\n' +
      'Footnote 2: We use GRACE with \\(\\epsilon_{\\mathrm{init}}=3.0\\) to edit block 4 of T5 (Hartvigsen et al., 2022).\n' +
      '\n' +
      'In these experiments, we use \\(K=1000\\), setting the memory size proportional to the number of facts to be written. We also checked an alternative method (see Appendix E) for computing the reading and writing weights, which uses a Gaussian convolution to store each encoding \\(\\mathbf{z}\\) in memory location(s) corresponding to the most similar content in a reference memory \\(\\mathbf{M}^{(\\mathrm{ref})}\\), and which we found to perform better than the pseudoinverse method of (Pham et al., 2021) when there are a relatively small number of rephrasings per fact (see Appendix, Figure 5).\n' +
      '\n' +
      '### Selective Forgetting\n' +
      '\n' +
      'The goal of this section is to check if a specific fact can be selectively erased from \\(N\\) facts that are have been written\n' +
      '\n' +
      'Figure 2: Batch editing accuracy on counterfactual dataset. Baseline performances are taken from (Meng et al., 2023). Green: MEMIT, Orange: ROME, Magenta: MEND, Black: Larimar-6B.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r} \\hline \\hline\n' +
      '**Editor** & **Test Retention Rate** & **Edit Retention Rate** \\\\ \\hline MEND & 0.25 & 0.27 \\\\ GRACE & 0.69 & 0.93 \\\\\n' +
      '**Larimar-1.3B** & 14.6* & **0.97** \\\\ Larimar-6B & 15.9* & 0.92 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Sequential editing on ZsRE dataset, showing Larimar does not forget older edits. *We report perplexity on wikitext estimated by a separate language model as Test Retention Rate for Larimar, whereas mean F1 on NQ test set is reported for MEND and GRACE on GPT2-XL (Hartvigsen et al., 2022).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '### Generalization to long input context\n' +
      '\n' +
      'We perform fact recall with long context using data that is not present in the base decoders pretraining corpus. For this purpose, we curated facts from CNN Fast Facts (CNN, 2023) for 2021, 2022 and 2023.\n' +
      '\n' +
      'We divide the input text into \\(T\\) chunks, which is in the range of Larimar\'s training context window, and store each of these chunks in a separate memory \\(M_{i},i=1..T\\). Given a query, we address and read from each of these memories. The readouts from these memories then form the basis of the successive memory, which is then queried and read from again. This process is continued until the number of readout in the final memory is similar to Larimar\'s input training context window. This recursive search in the latent memory space and using readouts to construct new higher-level memory is performed to process the long context with Larimar\'s memory trained on a relative small episode length. The retrieved \\(Z_{r}\\) from the final successor memory is passed to the decoder to predict the response. It should be noted that memory hierarchy is also found in hippocampus and thought to be implicated in learning (Collin et al., 2015).\n' +
      '\n' +
      'Table 6 shows Larimar\'s recall performance does not degrade much with increasing input context length, even compared to some of most competitive baseline LLMs trained with longer training context. We also compare with Supersizing Transformer (Klett and Ahle, 2023) which is a memory-augmented model, however it did not show competitive recall performance because it was not trained to perform memory-conditioned generation. Due to memory processing in the latent space, Larimar is also efficient is terms of number of KV cache token computation compared to baseline methods. Our experiments on 128 facts case shows that the average time required by Larimar to read from memory is 0.36s compared to 1.44s for Mistral7b base model.\n' +
      '\n' +
      'Learning to copy from the context remains an important aspect underlying transformers\' impressive language modeling and other abilities (Devlin et al., 2018; Raffel et al., 2020; Olsson et al., 2022). LLMs with non-attention based architectures, such as state space models, often underperform (Gu et al., 2022; Gu and Dao, 2023) transformers in language modeling, which is at least partly attributed to an inability to copy from the context, as well as an inability to generalize to longer contexts, when compared to transformers (Jelassi et al., 2024). Those investigations have fueled research on hybrid architectures. The results presented here suggest that combining a hierarchical memory model with a generative pretrained transformer, as in Larimar, could be a promising path in that direction. The end-to-end training of the fixed-size latent memory with the decoder in Larimar adds an explicit state to the decoder, writing to which helps controlling the decoding, thus allowing truthful copying from context in a generalized manner. The memory control also provides real-time knowledge editing as well as information leakage prevention. Attending to the memory read-out while decoding uses \\(O(1)\\) memory to predict each token, providing memory and computational benefits.\n' +
      '\n' +
      '## 6 Related work\n' +
      '\n' +
      'Memory-augmented NNsExternal memory augmented neural networks (MANNs) were already proposed in pre-transformer era, with the aim of better learning long-term dependencies in input data (Weston et al., 2014; Graves et al., 2014; Miller et al., 2016) showing enhanced performance in generative tasks, language modeling, long-term planning, and sample-efficient RL, etc. MANNs add a trainable slot-based memory to a recurrent neural net. An attention-based reading mechanism is typically used to compute a weighted average of memory contents. This mechanism is estimated from training data, and thus it remains unclear how they can generalize to new data. Alternatively, Kanerva Machine (Wu et al., 2018), inspired by Kanerva\'s sparse distributed memory model (Kanerva, 1988), views memory as a global latent variable in a generative model and aims to learn a memory dependent data prior and learnable addresses. In this framework, the memory update and read/write are considered as Bayesian inference, i.e., the posterior parameters are updated as new data arrives. KM and its successors (Wu et al., 2018; Ramapuram et al., 2022; Pham et al., 2021) show that these conditional generative memory models offer better performance on image reconstruction, denoising, and generation tasks compared to variational autoencoders (Kingma and Welling, 2013) and memory networks (Bornschein et al., 2017). However, to our knowledge this is the first report on investigating how those models can adapt to LLM and aid in their knowledge updating.\n' +
      '\n' +
      'Transformers struggle with accessing and updating long-term memory (Fan et al., 2021). Efforts to extend input context length for better performance encounter issues integrating inherent model knowledge with external facts, lacking robustness (Li et al., 2022; Liu et al., 2023). Augmenting transformers with external, non-differentiable memory and k-nearest neighbor (kNN) attention has shown promise in improving language modeling by utilizing additional context (Grave et al., 2017; Khandelwal et al., 2019). However, kNN-augmented models face challenges in controlling memory during decoding, leading to difficulties in updating facts due to conflicts between encoded knowledge and real-time information (Li et al., 2022; Liu et al., 2023; Zhu et al., 2020).\n' +
      '\n' +
      'Model EditingFor comprehensive surveys of editing approaches see (Yao et al., 2023; Zhang et al., 2024; Wang et al., 2023b). Editing methods can be broadly categorized into three categories: \'Recognition Phase\', \'Association Phase\' and \'Mastery Phase\' (Zhang et al., 2024). The\'recognition phase\'-targeting methods consider demonstrating right context to help the LLM output correct facts, either via in-context demonstrations of similar examples (Zheng et al., 2023), or training an external model on edits (Mitchell et al., 2022).The \'association phase\' -related editing methods consider merging new knowledge to that of the base LLM, either by patching (adding and training) error-specific neurons (Huang et al., 2023), or by adding a an adaptor storing edit key-value pairs to a specific LLM layer (Hartvigsen et al., 2022). The\'mastery phase\' methods learn to update base LLM\'s own parameters. Examples are regularized finetuning (Zhu et al., 2020) and hypernetwork-based methods (Mitchell et al., 2021; De Cao et al., 2021). Recent works also explore the \'locate-then-edit\' approach: (Meng et al., 2022a;b) first perform a causal tracing to detect which part of hidden states can be attributable to the fact and then do a rank-one update of the corresponding weight parameters to directly write in the updated fact.\n' +
      '\n' +
      'Current model editing approaches, while promising (Yao et al., 2023), face significant limitations, such as high training costs and difficulties in generalizing to new data. These methods often cannot efficiently update Large Language Models (LLMs) due to extensive time and memory requirements (Mitchell et al., 2022). Furthermore, the assumption that knowledge within LLMs is localized has been challenged (Hase et al., 2023), indicating that simple parameter updates may not be effective for comprehensive edits. The performance of LLMs degrades with multiple edits, leading to issues like knowledge forgetting and distortion (Mitchell et al., 2022; Meng et al., 2023; Gupta et al., 2024; Li et al., 2023; Gu et al., 2024). Alternatives like external cache or memory-based editing have been proposed to circumvent direct model modifications, yet challenges in selectively forgetting outdated or sensitive knowledge persist (Ishibashi and Shimodaira, 2023; Patil et al., 2023).\n' +
      '\n' +
      'Different from the above-mentioned works, we present a novel approach to augment Large Language Models (LLMs) with generative memory, enabling dynamic editing and adaptation without retraining. This differs from traditional methods that update LLM parameters (Meng et al., 2022a;b) or external memories (Han et al., 2023; Hartvigsen et al., 2022) and does not require multiple demonstrations for control Zheng et al. (2023).\n' +
      '\n' +
      'Larimar\'s forgetting operation does not use negative examples to fine-tune LLMs for unlearning (Yu et al., 2023). Neither Larimar requires tailored fine-tuning (Eldan and Russinovich, 2023) or inserting extra layers (Chen and Yang, 2023), and is complimentary to in-context unlearning approaches such as (Pawelczyk et al., 2023) for fact forgetting.\n' +
      '\n' +
      '## 7 Conclusions\n' +
      '\n' +
      'In this work, we propose augmenting LLMs with a dynamically updatable and distributed episodic memory as a means to online knowledge adaptation. By exploiting a one-shot memory update mechanism, combined with memory-conditioned decoding, the proposed framework shows _accurate_, _precise_, _robust_, and _significantly faster_ editing performance compared to baselines in single-fact, as well as the challenging sequential editing experiments. We exploit the same memory updating mechanism to enable a _fast and selective_ fact forgetting operation, as well as an effective information deletion mechanism. We also provide a simple approach for handling long input context by recursively reading from Latimar\'s memory space, revealing better fact recall from long input context by Larimar when compared to state-of-the-art LLMs trained with a much larger training context window. The proposed framework thus provides a simple, general, and principled approach to update LLMs in real-time by coupling them with an adaptable episodic memory control.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline Method & Train Context & \\(n_{fact}=64\\) & \\(n_{fact}=96\\) & \\(n_{fact}=128\\) & \\(n_{fact}=256\\) \\\\ \\hline mistral-7b (3-shot) & 8192 & **0.98** / 2655 & **0.96** / 3495 & 0.57 / 4334 & 0.42 / 7417 \\\\ gpt-neox-20b (3-shot) & 2048 & 0.52 / 2366 & 0.36 / 3193 & 0.33 / 4020 & 0.35 / 7231 \\\\ llama2-13b (3-shot) & 4096 & 0.97 / 2755 & 0.66 / 3628 & OOM & OOM \\\\ \\hline \\hline Supersizing Transformer & 2048 & 0.39 / 1462 & 0.39 / 2249 & 0.37 / 3072 & 0.37 / 6201 \\\\ Supersizing Transformer + filtering & 2048 & 0.72 / 1640 & 0.71 / 2375 & 0.70 / 3110 & 0.69 / 5809 \\\\ \\hline Larimar-1.3b & 384/2048 & 0.89 / 1565 & 0.88 / 2276 & **0.88** / 2988 & **0.86** / 5607 \\\\ Larimar-6b & 384/2048 & 0.82 / 1565 & 0.81 / 2276 & 0.81 / 2988 & 0.80 / 5607 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Novel fact addition recall rate on FastFacts. Larimar shows good recall performance and can extrapolate to higher context length trained than it was trained on. Baseline models show good recall on small context but recall degrades significantly for higher context.\n' +
      '\n' +
      '## 8 Broader Impact and Ethical Considerations\n' +
      '\n' +
      'This paper presents work whose goal is to advance the field of machine learning and large language models. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* Petroni et al. (2019) Fabio Petroni, Tim Rocktaschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2463-2473, 2019.\n' +
      '* Anil et al. (2022) Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL [https://openreview.net/forum?id=zSkYVeX7Dc4](https://openreview.net/forum?id=zSkYVeX7Dc4).\n' +
      '* Kazemnejad et al. (2023) Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers, 2023.\n' +
      '* Kirkpatrick et al. (2017) James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _Proceedings of the national academy of sciences_, 114(13):3521-3526, 2017.\n' +
      '* Zhu et al. (2020) Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. Modifying memories in transformer models, 2020.\n' +
      '* Li et al. (2022) Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. Large language models with controllable working memory, 2022.\n' +
      '* Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023.\n' +
      '* Patil et al. (2023) Vaidehi Patil, Peter Hase, and Mohit Bansal. Can sensitive information be deleted from lms? objectives for defending against extraction attacks, 2023.\n' +
      '* Dempsey et al. (2022) William P Dempsey, Zhuowei Du, Anna Nadtochiy, Colton D Smith, Karl Czajkowski, Andrey Andreev, Drew N Robson, Jennifer M Li, Serina Applebaum, Thai V Truong, et al. Regional synapse gain and loss accompany memory formation in larval zebrafish. _Proceedings of the National Academy of Sciences_, 119(3):e2107661119, 2022.\n' +
      '* Autore et al. (2023) Livia Autore, James D O\'Leary, Clara Ortega-de San Luis, and Tomas J Ryan. Adaptive expression of engrams by retroactive interference. _bioRxiv_, pages 2023-03, 2023.\n' +
      '* Blundell et al. (2016) Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo, Jack Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control, 2016.\n' +
      '* Lengyel and Dayan (2007) Mate Lengyel and Peter Dayan. Hippocampal contributions to control: the third way. _Advances in neural information processing systems_, 20, 2007.\n' +
      '* Kumaran et al. (2016) Dharshan Kumaran, Demis Hassabis, and James L McClelland. What learning systems do intelligent agents need? complementary learning systems theory updated. _Trends in cognitive sciences_, 20(7):512-534, 2016.\n' +
      '* Sun et al. (2023) Weinan Sun, Madhu Advani, Nelson Spruston, Andrew Saxe, and James E Fitzgerald. Organizing memories for generalization in complementary learning systems. _Nature neuroscience_, 26(8):1438-1448, 2023.\n' +
      '* Ramirez et al. (2020) Steve Ramirez, Xu Liu, Pei-Ann Lin, Junghyup Suh, Michele Pignatelli, Roger L Redondo, Tomas J Ryan,and Susumu Tonegawa. Creating a false memory in the hippocampus. _Science_, 341(6144):387-391, 2013.\n' +
      '* Wu et al. (2018a) Yan Wu, Greg Wayne, Alex Graves, and Timothy Lillicrap. The kanerva machine: A generative distributed memory, 2018a.\n' +
      '* Pham et al. (2021) Kha Pham, Hung Le, Man Ngo, Truyen Tran, Bao Ho, and Svetha Venkatesh. Generative pseudo-inverse memory. In _International Conference on Learning Representations_, 2021.\n' +
      '* Meng et al. (2022a) Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. _Advances in Neural Information Processing Systems_, 35:17359-17372, 2022a.\n' +
      '* Hartvigsen et al. (2022) Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. Aging with grace: Lifelong model editing with discrete key-value adaptors. _arXiv preprint arXiv:2211.11031_, 2022.\n' +
      '* Wang et al. (2023a) Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, and Huajun Chen. Easyedit: An easy-to-use knowledge editing framework for large language models, 2023a.\n' +
      '* Kingma and Welling (2013) Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.\n' +
      '* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n' +
      '* Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\n' +
      '* Meng et al. (2023) Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a transformer. In _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Mitchell et al. (2022) Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. Memory-based model editing at scale. In _International Conference On Machine Learning, Vol 162_. JMLR-JOURNAL MACHINE LEARNING RESEARCH, 2022.\n' +
      '* Yao et al. (2023) Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. Editing large language models: Problems, methods, and opportunities. _arXiv preprint arXiv:2305.13172_, 2023.\n' +
      '* Zheng et al. (2023) Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. Can we edit factual knowledge by in-context learning?, 2023.\n' +
      '* Levy et al. (2017) Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. _arXiv preprint arXiv:1706.04115_, 2017.\n' +
      '* CNN (2023) CNN. 2023 in review fast facts, 2023. URL: \\(\\backslash\\)[https://www.cnn.com/2023/11/13/us/2023-in-review-fast-facts/index.html](https://www.cnn.com/2023/11/13/us/2023-in-review-fast-facts/index.html).\n' +
      '* Collin et al. (2015) Silvy HP Collin, Branka Milivojevic, and Christian F Doeller. Memory hierarchies map onto the hippocampal long axis in humans. _Nature neuroscience_, 18(11):1562-1564, 2015.\n' +
      '* Klett and Ahle (2023) Phoebe Klett and Thomas Ahle. Supersizing transformers: Going beyond rag with extended minds for llms. The Normal Blog, 2023. URL: [https://blog.normalcomputing.ai/posts/2023-09-12-supersizing-transformers/supersizing-transformers.html](https://blog.normalcomputing.ai/posts/2023-09-12-supersizing-transformers/supersizing-transformers.html).\n' +
      '* Olsson et al. (2022) Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads, 2022.\n' +
      '* Gu et al. (2022) Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces, 2022.\n' +
      '* Gu and Dao (2023) Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.\n' +
      '* Jelassi et al. (2024) Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying, 2024.\n' +
      '* Weston et al. (2014) Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. _arXiv preprint arXiv:1410.3916_, 2014.\n' +
      '* Graves et al. (2014) Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. _arXiv preprint arXiv:1410.5401_, 2014.\n' +
      '* Miller et al. (2016) Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. Key-value memory networks for directly reading documents, 2016.\n' +
      '* Kanerva (1988) Pentti Kanerva. _Sparse distributed memory_. MIT press, 1988.\n' +
      '* Wu et al. (2018) Yan Wu, Greg Wayne, Karol Gregor, and Timothy Lillicrap. Learning attractor dynamics for generative memory, 2018b.\n' +
      '\n' +
      '* Ramapuram et al. (2022) Jason Ramapuram, Yan Wu, and Alexandros Kalousis. Kanerva++: extending the kanerva machine with differentiable, locally block allocated latent memory, 2022.\n' +
      '* Bornschein et al. (2017) Jorg Bornschein, Andriy Mnih, Daniel Zoran, and Danilo J. Rezende. Variational memory addressing in generative models, 2017.\n' +
      '* Fan et al. (2021) Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory, 2021.\n' +
      '* Grave et al. (2017) Edouard Grave, Moustapha Cisse, and Armand Joulin. Unbounded cache model for online language modeling with open vocabulary, 2017.\n' +
      '* Khandelwal et al. (2019) Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. _arXiv preprint arXiv:1911.00172_, 2019.\n' +
      '* Zhang et al. (2024) Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. A comprehensive study of knowledge editing for large language models. _arXiv preprint arXiv:2401.01286_, 2024.\n' +
      '* Wang et al. (2023b) Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. Knowledge editing for large language models: A survey, 2023b.\n' +
      '* Huang et al. (2023) Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. Transformer-patcher: One mistake worth one neuron. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=4oYUGeGBPm](https://openreview.net/forum?id=4oYUGeGBPm).\n' +
      '* Mitchell et al. (2021) Eric Mitchell, Charles Lin, Antoine Bosselt, Chelsea Finn, and Christopher D Manning. Fast model editing at scale. _arXiv preprint arXiv:2110.11309_, 2021.\n' +
      '* De Cao et al. (2021) Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. _arXiv preprint arXiv:2104.08164_, 2021.\n' +
      '* Meng et al. (2022b) Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a transformer. _arXiv preprint arXiv:2210.07229_, 2022b.\n' +
      '* Hase et al. (2023) Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL [https://openreview.net/forum?id=EldbU1Ztbd](https://openreview.net/forum?id=EldbU1Ztbd).\n' +
      '* Gupta et al. (2024) Akshat Gupta, Anurag Rao, and Gopala Anumanchipalli. Model editing at scale leads to gradual and catastrophic forgetting. _arXiv preprint arXiv:2401.07453_, 2024.\n' +
      '* Li et al. (2023) Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, and Huajun Chen. Unveiling the pitfalls of knowledge editing for large language models, 2023.\n' +
      '* Gu et al. (2024) Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, and Nanyun Peng. Model editing can hurt general abilities of large language models, 2024.\n' +
      '* Ishibashi and Shimodaira (2023) Yoichi Ishibashi and Hidetoshi Shimodaira. Knowledge sanitization of large language models, 2023.\n' +
      '* Han et al. (2023) Xiaoqi Han, Ru Li, Hongye Tan, Wang Yuanlong, Qinghua Chai, and Jeff Pan. Improving sequential model editing with fact retrieval. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 11209-11224, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.749. URL [https://aclanthology.org/2023.findings-emnlp.749](https://aclanthology.org/2023.findings-emnlp.749).\n' +
      '* Yu et al. (2023) Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. Unlearning bias in language models by partitioning gradients. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 6032-6048, 2023.\n' +
      '* Eldan and Russinovich (2023) Ronen Eldan and Mark Russinovich. Who\'s harry Potter? approximate unlearning in llms, 2023.\n' +
      '* Chen and Yang (2023) Jiaao Chen and Diyi Yang. Unlearn what you want to forget: Efficient unlearning for llms. _arXiv preprint arXiv:2310.20150_, 2023.\n' +
      '* Pawelczyk et al. (2023) Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. In-context unlearning: Language models as few shot unlearners. _arXiv preprint arXiv:2310.07579_, 2023.\n' +
      '* Dai et al. (2021) Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. _arXiv preprint arXiv:2104.08696_, 2021.\n' +
      '\n' +
      'A Baselines\n' +
      '\n' +
      'FTFine-Tuning (FT) uses Adam optimization with early stopping, focusing on adjusting \\(mlp_{proj}\\) weights in one layer to optimize the training loss.\n' +
      '\n' +
      'FT+LConstrained fine-tuning (FT+L), as in (Zhu et al., 2020), authors apply an \\(L_{\\infty}\\) norm constraint by clamping weights no to exceed \\(\\epsilon\\) range at each gradient step. They chose layer 0 and \\(\\epsilon=5\\times 10^{-4}\\) for GPT-2, and \\(\\epsilon=5\\times 10^{-5}\\) for GPT-J.\n' +
      '\n' +
      'KnThis is a method by (Dai et al., 2021) which selects neurons that are associated with knowledge expression via gradient-based attributions, and then modifies MLP at the rows corresponding to those neurons by adding scaled embedding vectors.\n' +
      '\n' +
      'KeKnowledge editor (KE) (De Cao et al., 2021) learn an LSTM sequence model that uses gradient information to predict rank-1 weight changes to the model. KE-CF / KE-ZsRE is additionally trained model on training set of CounterFact / ZsRE dataset.\n' +
      '\n' +
      'MendModel Editor Networks with Gradient Decomposition (MEND) (Mitchell et al., 2021) learn a rank-1 decomposition of the negative log likelihood gradient with respect to some subset of parameters. Similarly, MEND-CF / MEND-ZsRE is additionally trained model on training set of CounterFact / ZsRE dataset.\n' +
      '\n' +
      'RomeRank-One Model Editing (ROME), proposed by (Meng et al., 2022), treats MLP module as a key-value store. To add a new key-value pair, ROME applies a rank-one modification to the weights of the MLP, adding the new information directly.\n' +
      '\n' +
      'IkeIn-context Knowledge Editing (IKE) (Zheng et al., 2023) defines three types of demonstration formatting templates including copy, update, and retain, which guide model to edit knowledge facts by in-context learning (ICL). The parameters of the model are not updated.\n' +
      '\n' +
      'PromptSimilar to IKE (Zheng et al., 2023) but simply prepends new fact to the LLM prompt. The parameters of the model are also not updated.\n' +
      '\n' +
      'Memit MEMIT aims direct model editing via fact tracing and followed by parameter editing. It is an expanded version of ROME, which enables the editing of large amounts of factual data through the updating of a sequence of MLP layers.\n' +
      '\n' +
      'IclTo compare to In-Context Learning (ICL) as a baseline method in Table 4, we use a prompt which consists of \\(N\\) facts, half of which are marked with a prefix string (e.g. "[UNKNOWN]"), followed by \\(K\\) examples of questions and answers (prior to a final query to the model), half of which correspond to facts marked with the prefix string, which replaces the answer, indicating that the fact should be treated as forgotten.\n' +
      '\n' +
      '### Prompt Example For Forgetting with ICL experiments\n' +
      '\n' +
      'Below, we show an example ICL prompt for \\(N=20\\) and \\(K=6\\), used for Llama2-13B in Table 4:\n' +
      '\n' +
      'Facts:\n' +
      '\n' +
      'Gaston Palewski writes in French\n' +
      '\n' +
      'The domain of work of Hermann Klaatsch is anatomy\n' +
      '\n' +
      '[UNKNOWN] 2 Minute Drill is to debut on ESPN\n' +
      '\n' +
      '[UNKNOWN] Immanuel Wallerstein works in the area of sociology\n' +
      '\n' +
      '[UNKNOWN] Nissan Skyline, created by Nissan\n' +
      '\n' +
      'Feng Fei-fei, a citizen of Taiwan\n' +
      '\n' +
      'Michie Mee works as actor\n' +
      '\n' +
      '[UNKNOWN] Antonio Palomino\'s life ended in Madrid\n' +
      '\n' +
      '[UNKNOWN] Film Forum is within Manhattan\n' +
      '\n' +
      '[UNKNOWN] Phoenix Sky Harbor International Airport is located in Phoenix\n' +
      '\n' +
      'Abraham & Straus formed in Brooklyn\n' +
      '\n' +
      'The domain of activity of Joseph Schumpeter is economics\n' +
      '\n' +
      'George Buza\'s profession is an actor\n' +
      '\n' +
      '[UNKNOWN] Velayudham originated in India\n' +
      '\n' +
      'Sophie Calle, a native French\n' +
      '\n' +
      'In Nunavut, the language spoken is French\n' +
      '\n' +
      '[UNKNOWN] The Wide Country is to debut on NBC\n' +
      '\n' +
      '[UNKNOWN] The mother tongue of Porfiry Ivanov is Russian\n' +
      '\n' +
      'In Kijminki, they understand Finnish\n' +
      '\n' +
      '[UNKNOWN] Pachomius the Great succumbed at Egypt\n' +
      '\n' +
      'Input: The domain of work of Hermann Klaatsch\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r} \\hline \\hline Editor & COUNTERFACT \\\\ \\hline FT & \\(35.94\\) s \\\\ SERAC & \\(5.31\\) s \\\\ CaliNet & \\(1.88\\) s \\\\ T-Patcher & \\(1864.74\\) s \\\\ KE & \\(2.20\\) s \\\\ MEND & \\(0.51\\) s \\\\ KN & \\(225.43\\) s \\\\ ROME & \\(147.2\\) s \\\\ MEMIT & \\(143.2\\) s \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Wall clock time for each edit method for performing 10 edits from CounterFact benchmark, as reported in (Yao et al., 2023).\n' +
      '\n' +
      'In Table 9 and 10 we provide ablation results on Larimar by varying different learning parameters and architectural components of the model and observing performance on CounterFact dataset. In Table 9 the ablation results for GPT-2 XL based model are presented. Here we examined three different training configurations:\n' +
      '\n' +
      '* C1: Episode length 6, observation noise 0.0001, trained for 2 epochs\n' +
      '* C2: Episode length 20, observation noise 0.000001, trained for 4 epochs\n' +
      '* C3: Episode length 16, observation noise 0.000001, trained for 2 epochs\n' +
      '\n' +
      'Note that the model reported in Table 12 in main paper is based on configuration C3. Moreover, we looked at three versions of the Larimar architecture: Original Larimar, Larimar without Scope detector and Larimar without memory. As can be seen, configuration C3 had some edge in performance. The effect of removing scope detector is reflected in drop of the neighborhood score. This is expected since now the model reroutes the prompts from the unconstrained decoder to the memory-constrained one, where the memory influence makes it harder to cover prompts unrelated to in-memory content. On the other hand, removing memory module results in significant decrease in edit success and paraphrasing, as now the model has no knowledge about introduced knowledge facts, at the same time its general language abilities are intact and performing well as reflected in high neighborhood score.\n' +
      '\n' +
      'In Table 10 the ablation results for GPT-J based model represent results for the following five training configurations:\n' +
      '\n' +
      '* C1: Episode length 5, no KL loss, trained for 5 epochs\n' +
      '* C2: Episode length 16, noise level 1e-4, trained for 8 epochs\n' +
      '* C3: Episode length 16, noise level 1e-4, no KL loss, trained for 8 epochs\n' +
      '* C4: Episode length 8, noise level 1e-4, trained for 8 epochs\n' +
      '* C5: Episode length 8, noise level 1e-4, no KL loss, trained for 8 epochs\n' +
      '\n' +
      'Note that the model reported in Table 2 in main paper is based on configuration C1. Similarly as before, we looked at architectural changes which included the removal of scope detector and memory block. We observed that configuration C2 performed the worst, while C1 had overall better performance. Moreover, the experiments again confirmed the benefit of scope detector and the effect of memory unit.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l r r} \\hline \\hline \\multicolumn{1}{l}{**Editor**} & \\multicolumn{2}{c}{**Edit Success**} & \\multicolumn{1}{l}{**Paraphrase**} & \\multicolumn{1}{l}{**Neighborhood**} \\\\ \\hline\n' +
      '**Larimar-6B w/ scope** & 99.6 & 76.5 & 80.2 \\\\\n' +
      '**Larimar-6B+para** & 99.6 & 82.8 & 80.6 \\\\\n' +
      '**Larimar-6B+para, no scope** & 99.6 & 88.7 & 16.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Single fact edit valuation on CounterFact dataset. Larimar-6B base is the baseline which includes only a single fact in the memory and uses in-scope query detector. Larimar-6B +para is the version which adds into the memory on average one additional paraphrased fact.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l r r r r r r} \\hline \\hline \\multirow{3}{*}{**Config**} & \\multirow{3}{*}{**Editor**} & \\multicolumn{6}{c}{**Metrics**} \\\\ \\cline{3-8}  & & & **Edit Success** & & **Paraphrase** & & **Neighbor** \\\\ \\cline{3-8}  & & S & M & S & M & S & M \\\\ \\hline \\multirow{3}{*}{C1} & Larimar & 100.0 & 99.7 & 38.4 & -2.9 & 74.2 & 1.6 \\\\  & No Scope & 100.0 & 99.8 & 37.8 & -3.0 & 22.4 & -34.1 \\\\  & No Memory & 23.3 & -4.4 & 26.5 & -3.5 & 77.7 & 4.7 \\\\ \\hline \\multirow{3}{*}{C2} & Larimar & 100.0 & 99.9 & 35.2 & -3.5 & 75.4 & 2.0 \\\\  & No Scope & 100.0 & 99.9 & 33.1 & -3.6 & 26.2 & -36.2 \\\\  & No Memory & 20.6 & -4.9 & 24.5 & -4.1 & 78.9 & 5.4 \\\\ \\hline \\multirow{3}{*}{C3} & Larimar & 100.0 & 99.8 & 41.9 & 0.4 & 74.8 & 1.6 \\\\  & No Scope & 100.0 & 99.9 & 41.1 & 0.4 & 14.3 & -5.8 \\\\ \\cline{1-1}  & No Memory & 21.6 & -4.8 & 25.4 & -3.8 & 78.4 & 5.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: Ablation results for Larimar-1.3B using CounterFact dataset\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n' +
      '### Sequential editing experiments.\n' +
      '\n' +
      'For the sequential editing experiments reported in Table 3 and Figure 3, we set \\(K=1000\\) and use a fixed reference memory \\(\\mathbf{M}^{(\\mathrm{ref})}\\) (see section 3) to compute reading and writing weights.\n' +
      '\n' +
      'For Table 3, the reference memory is constructed by encoding the prompt for each of the 1000 edits, and placing it in one row of \\(\\mathbf{M}^{(\\mathrm{ref})}\\).\n' +
      '\n' +
      'For Figure 3, the reference memory is constructed by encoding the first prompt for each of the 1000 unique facts (among the several rephrasings in the edit set which are written to memory) and placing it in a single row in \\(\\mathbf{M}^{(\\mathrm{ref})}\\). Thus, when querying memory with an encoded rephrased prompt \\(\\mathbf{z}\\) in Eq. (7), if \\(\\mathbf{z}\\) is closest to the row \\(k\\) in \\(\\mathbf{M}^{(\\mathrm{ref})}\\) corresponding to the same fact, the key vector element \\(w_{k}\\) will be largest for this element, and suppressed for other memory locations. (We use \\(\\alpha=10^{-3}\\) to strongly suppress more distant encodings in the reference memory. Empirically, we found that that the nearest-neighbor encoding picked out by Eq. (7) with small \\(\\alpha\\) is usually the encoded prompt for the same fact, with lower F1 scores occurring mainly in cases where the nearest-neighbor row in \\(\\mathbf{M}^{(\\mathrm{ref})}\\) corresponds to a different fact.) We found that computing reading and writing weights as in (Pham et al., 2021), \\(\\mathbf{w}=\\mathbf{z}(\\mathbf{M}^{(\\mathrm{ref})})^{\\dagger}\\), was not as effective with rephrased facts (Figure 3 and Table 13) unless the number of rephrasings per fact was relatively large.\n' +
      '\n' +
      'When writing to memory, a trailing period is appended to the ground truth label, in order to reduce the likelihood of the model generating additional text. When evaluating the F1 score, we remove (in both target and predicted tokens) the token corresponding to a period (13). We also remove the token 198, which corresponds to the new line character \'\\(\\backslash\\)n\', when it is generated as the last token.\n' +
      '\n' +
      'In Figure 5, we compare different variants of Larimar, on the same task as shown in Figure 3. Relative to the Gaussian convolution method of Eq. (7), computing reading and writing weights with the reference memory matrix pseudoinverse, \\(\\mathbf{w}=\\mathbf{z}(\\mathbf{M}^{(\\mathrm{ref})})^{\\dagger}\\) performed well on a dataset of 511 ZsRE facts and \\(\\approx 20\\) phrasings per fact, but significantly worse on a dataset of \\(1000\\) ZsRE with \\(10\\) phrasings per fact. (We hypothesize that Eq. (7) is more effective at finding a nearby rephrase encoding for the same fact when there are only one or a few paraphrases available in the data.)\n' +
      '\n' +
      'In our fact forgetting experiments (Table 4), we used a simple reference memory where each matrix element is sampled randomly, \\(\\mathbf{M}^{(\\mathrm{ref})}_{ij}\\sim\\mathcal{N}(0,1)\\). We found this choice to be less effective when querying with rephrased prompts - in which case the additional structure of \\(\\mathbf{M}^{(\\mathrm{ref})}\\) described above helps to locate the nearby encoding of a different phrasing of the same fact - but to be sufficient when querying with the same prompts used when writing to memory (as in Table 4). In this case we compute the writing weight using the encoding of the prompt of the fact written to memory, \\(\\mathbf{W}=\\mathbf{Z}_{\\mathrm{prompt}}(\\mathbf{M}^{(\\mathrm{ref})})^{\\dagger}\\) (instead of Eq. (7)), and compute the reading weight in the same way, with the reading prompt differing from the writing prompt in rephrasing experiments.\n' +
      '\n' +
      'Lastly, in our batch editing experiment (Figure 2), we computed writing weights using the encoded prompt, \\(\\mathbf{W}=\\mathbf{Z}_{\\mathrm{prompt}}(\\mathbf{M}^{(\\mathrm{ref})})^{\\dagger}\\), and computed both writing and reading weights with \\(\\mathbf{M}^{(\\mathrm{ref})}\\) set to the memory matrix obtained from Larimar\'s training (although we found a Gaussian random matrix to yield comparable results).\n' +
      '\n' +
      'Throughout these experiments, we use \\(\\sigma_{w}=0\\) and \\(\\xi=0\\).\n' +
      '\n' +
      '## Appendix F Generalization via Rephrase-Augmented Memory\n' +
      '\n' +
      'We also evaluate Larimar-1.3B on generalization to unseen rephrasings, by writing a variable number of seen rephrases of the same fact to memory. After writing \\(N_{reph}\\) rephrasings for each of \\(N_{fact}\\) facts to memory, we estimate recall by querying the model with \\(N_{reph}\\) unseen rephrasings. (As in the sequential editing experiment with rephrase queries, we use a reference memory matrix constructed from the prompt encodings for the facts written to memory.) In Table 13, we show average recall of the ground-truth answer for samples from the ZsRE validation set, revealing generalization to unseen rephrases. Naturally, for facts with more rephrases in memory, recall is higher. We furthermore\n' +
      '\n' +
      'Figure 5: Mean F1 score of Larimar, comparing different choices for computing reading and writing weights – the Gaussian convolution in Eq. (7) and the pseudoinverse method of (Pham et al., 2021) – on held-out sets of unseen rephrasings from ZsRE over a sequence of 3000 edits. (Black curves are shown in Figure 3 in the main text.)\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>