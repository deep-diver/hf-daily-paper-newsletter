<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '크기 멀티모달 모디텔스.\n' +
      '\n' +
      'Rohan Wadhawan*\\({}^{1}\\)  Hritik Bansal*\\({}^{1}\\)  {}^{1}\\  {}^{1}\\ )  ＆moun Peng\\({}^{1}\\) ))  －Hritik Bansal*\\({}^{1}\\.\n' +
      '\n' +
      '캘리포니아 로스앤젤레스의 대학교({}^{1}\\)\n' +
      '\n' +
      'con-textual.github.io\n' +
      '\n' +
      '동일한 기여를 합니다. 연락트 리와단7@cs.ucla.edu, hbansal@ucla.edu.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근의 AI 발전은 이미지(예: 공공 장소에서의 지도 탐색)에서 텍스트와 시각적 콘텐츠에 대한 공동 추론을 포함하는 복잡한 작업을 처리할 수 있는 대규모 복합 모델(LMM)의 개발로 이어졌다. 본 논문은 문맥에 민감한 텍스트가 풍부한 시각적 추론을 수행할 LMM의 능력을 평가하기 위해 명시적으로 설계된 명령어를 포함하는 새로운 벤치마크인 ConTextual를 소개한다. 콘텍스트는 텍스트 요소와 시각적 요소 간의 상호 작용에 대한 더 깊은 이해를 요구하는 다양한 현실 세계 시나리오(예: 시간 읽기, 내비게이션, 쇼핑 등)를 강조한다. 우리의 연구 결과는 컨텍스트에 민감한 텍스트가 풍부한 시각적 추론의 개선을 위한 실질적인 여지를 나타내는 인간 평가를 사용하여 최고의 성능 LMM, GPT-4V(결정) 및 인간 능력 사이의 \\(30.8\\%\\)의 상당한 성능 격차를 보여준다. 특히 GPT-4V는 메모와 인용 해석과 같은 추상적인 범주에 탁월했지만 전체 성과는 여전히 인간에게 뒤처졌다. 인간의 평가 외에도 GPT-4를 사용하여 자동 평가 메트릭을 사용하여 성능 격차의 유사한 경향을 발견했다. 또한 다양한 시각적 맥락에 걸쳐 미세 편성된 평가를 수행하고 LMM 설계에서 향후 발전을 위한 강력한 프레임워크를 제공하는 질적 분석을 제공한다.\n' +
      '\n' +
      '그림 1: GPT-4V, Gemini-Pro-Vision, ShareGPT-4V-7B 및 Humans의 ConTextual 데이터셋에서 성능을 나타낸다. (a) 인간 평가 및 응답 정확도에 대한 자동 GPT-4 기반 평가이다. (b) GPT-4 기반 평가를 사용하여 시각적 맥락의 변화에 따른 성능 변화이다.\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      'AI에서 오랜 목표는 다양한 과제를 해결하기 위해 인간과 상호 작용할 수 있는 복합 보조자를 구축하는 것이다. 이와 관련하여 최근 지도-지시된 대규모 복합 모델(LMM)의 개발은 이미지[1; 2; 4; 5]에 걸쳐 질문 또는 필수 작업으로 제기된 인간 지침에 대응할 수 있는 모델을 생성했다. 당연히 주변의 많은 이미지에는 포괄적인 이해를 위한 신호를 제공하는 텍스트가 있다. 이미지들에서 텍스트와 시각적 컨텍스트 사이의 상호 작용을 이유로 하는 능력은 많은 실제-세계 애플리케이션들을 전력화한다. 예를 들어 시각장애인을 돕기 위한 텍스트가 풍부한 장면(예: 공공공간에서 지도를 탐색하는 것)과 추상적인 텍스트가 풍부한 이미지(예를 들어, 메모)에 대한 창의적인 이해를 해석한다.\n' +
      '\n' +
      '이전 작업에서 TextVQA[6], STVQA[6], ESTVQA[7]와 같은 모델의 텍스트가 풍부한 시각적 추론에 액세스하기 위해 수많은 데이터 세트가 제시되었다. 이러한 데이터 세트는 모델의 OCR 능력에 이미지 내의 텍스트에 _read_ 접속하는 데 초점을 맞추고 있다. 결과적으로, 이러한 데이터 세트는 해당 모델에 질문에 답하기 위해 이미지 내의 시각적 컨텍스트를 캡처하도록 요구하지 않는다. 그림 2(a)에서 ESTVQA 데이터 세트의 몇 가지 예를 강조한다. 여기서 우리는 이미지의 높은 정확도 OCR(예: \'W 63 ST...몬크)\'이 질문(예: \'길거리 번호가 무엇인가?\')에 답하기에 충분한 신호를 가지고 있음을 보여준다. OCR 능력에 액세스하는 것이 중요하지만, 이러한 예는 이미지 내의 텍스트 및 시각적 컨텍스트에 대해 공동으로 이성을 위해 LMM의 고유한 잠재력을 테스트하지 않는다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 **ConText***를 제안하는데, 이는 **506*** LMM 평가를 위한 도전 지침으로 구성된 **C**ontext 민감성 **Text***가 풍부한 시각적 추론 벤치마크이다. 우리는 이미지 내의 텍스트 및 시각적 신호에 대한 컨텍스트에 민감한 관절 추론을 필요로 한다는 제약으로 텍스트가 풍부한 이미지에 대한 다양한 명령어 세트를 생성한다. 그림 2(b)와 같이 데이터 세트의 몇 가지 예를 보여준다. 우리는 이미지 내에서 텍스트 콘텐츠의 OCR을 완벽하게 캡처(예: \'파브먼트 상의 RR 워터...플라인\'(Planes)하여도 명령어(\'어떤 글자가 지상에 물 위에 반영되어 있는가?\')를 응답할 수 없다는 것을 관찰한다. MathVista [8] 및 MMMU [9]와 같은 사전 작업이 대학 수준에서 시력-언어 수학 추론 및 다중 이론(과학, 미술, 역사) 이해를 평가하는 반면, 우리의 벤치마크는 일상-생활 자연 또는 디지털 장면(예: 시간 판독, 쇼핑, 내비게이션)에 대한 텍스트-풍부한 시각적 추론을 테스트하기 위한 개방형 지침을 포함한다. 우리는 표 1의 관련 작품과 비교하여 새로운 기여를 요약한다.\n' +
      '\n' +
      '각 사례는 인간의 필기된 지시(질문 또는 필수 작업)와 인간의 필기된 지상 진실 응답을 포함한다. 우리는 이미지 내의 텍스트로 **eight*** 현실 세계 시나리오를 커버하기 위해 ConTextual 데이터세트를 만듭니다.\n' +
      '\n' +
      '그림 2: 텍스트가 풍부한 시각적 추론을 위한 기존 데이터셋(예: ESTVQA)과 ConTextual 데이터셋의 특성 간의 비교. (a) 이전 데이터 세트의 인스턴스는 LMM의 판독 능력을 크게 테스트한다. 결과적으로 정확한 OCR 검출에 대한 텍스트 전용 추론으로 질문들을 해결할 수 있다. 그러나 시각적 인식과 추론 능력이 향상된 것으로 나타났기 때문에 현대 모델로 더 어려운 시나리오를 평가하고자 한다. (b) 콘텍스트의 인스턴스는 이미지 내의 텍스트와 시각적 내용이 상호 작용하는 컨텍스트를 캡처하는 모델의 능력을 테스트하도록 구성된다. 여기서 검출된 OCR의 텍스트 전용 추론은 잘 수행하기에 충분하지 않다.\n' +
      '\n' +
      ' 타임 리딩_, _shopping_, _navigation_, __navract 장면_, _mobile 애플리케이션_, _webpages_, _infographics_ 및 _miscell 자연적인 장면_. 그림 3의 각 시각적 맥락의 예를 제시하며, 이러한 범주의 다양한 시각적 특성은 모델의 능력에 대한 상세하고 미묘한 평가를 수행할 수 있다(SS3.4).\n' +
      '\n' +
      '13개의 기초 모델(SS3.1)의 추론 능력을 평가하기 위해 ConTextual 데이터셋에 대한 광범위한 실험을 수행한다. 이 세트에는 3개의 증강 LLM 세트(예: GPT-4 [10]가 이미지 OCR, 이미지 레이아웃 및 이미지 캡션), 2개의 독점 LMM(예: GPT-4V[2], Gemini-Pro-Vision[3]) 및 8개의 오픈 LMM(예: LLaVA-1.5 [11], ShareGPT-4V[12], ShareGPT-4V[12]이 포함되어 있다. 또한 선택된 모델 세트(예: 게미니-프로비전, Idefics)에 대한 몇 가지 샷 실험을 수행하여 모델 성능에 대한 인텍스트 예제의 영향을 분석했다. 또한, 우리는 인간 주석자에게 데이터세트 지침에 대한 응답을 쓰기 위해 요청함으로써 인간 기준선을 확립한다. 마지막으로 데이터세트(SS3.2)에서 지상-진실 반응과 관련하여 예측된 반응의 올바른 정도를 평가하기 위해 인간과 자동 평가를 수행한다. 전반적으로 텍스트가 풍부한 시각적 추론(SS3.3)의 진행 상황을 추적하기 위해 포괄적인 모델 및 평가 메트릭 세트를 다룬다.\n' +
      '\n' +
      '무작위로 선택된 280예에 대한 인간 평가를 통해 GPT-4V(결정)가 생성된 응답에 대해 \\(49.3\\%\\) 수용 등급을 달성하는 ConTextual 데이터셋에서 가장 우수한 수행 LMM임을 발견했다(그림 0(a). 그럼에도 불구하고, 성능은 GPT-4V의 능력의 큰 격차를 나타내는 \\(80.1\\%\\)의 인간 기준선 뒤에 있다. 또한, 가장 잘 수행되는 오픈 모델인 ShareGPT-4V-7B가 컨텍스트에 민감한 텍스트가 풍부한 시각적 추론(SS3.3)에 대한 독점 모델 뒤에 열린 모델의 역량이 있음을 나타내는 \\(21.8\\%\\) 등급을 달성한다는 것을 발견했다. 우리의 결과는 ConTextual이 현대 LMM에 대한 도전적인 데이터 세트이고 인간이 그것을 능가한다는 것을 강조한다.\n' +
      '\n' +
      '인간 평가는 규모가 어렵고 비용이 많이 들기 때문에 모든 모델(SS3.3.1)에 대한 완전한 데이터 세트에 대해 자동 평가(예: GPT-4, GPT-4V, BLEURT [14])도 수행한다. 그림 0(b)의 미세 편성 분석과 함께 그림 0(a)의 GPT-4 평가 결과를 제시한다. 우리는 인간이 채우기 위해 큰 격차를 강조하는 \\(22.4\\%\\)에 의해 최고의 수행 모델 GPT-4V를 능가한다는 것을 발견했다. 특히 GPT-4V는 메모와 인용과 같은 시각적 맥락에 대한 이해를 포함하는 추상적인 범주에 대한 인간의 성능을 능가한다.\n' +
      '\n' +
      '마지막으로 모델 예측에 대한 질적 평가를 수행하여 다양한 성능 수준을 보여준다. GPT4V와 Gemini-Vision-Pro는 우수한 성능을 나타내는 반면, 오픈 소스 LMM 및 주입 LLM 접근법은 지연을 나타낸다. 모형 중 GPT4V는 강력한 시각적 인식과 맥락에 민감한 텍스트-비전 추론 능력으로 인해 가장 높은 성능을 보인다. 또한, 시나리오에서 지시에 대한 모델 반응을 정성적으로 분석하는 것은 복각, 이미지에 대한 명령의 접지 부족, 불량한 컨텍스트 민감성 텍스트-비전 추론(SS4)과 관련된 미세 지능화된 시각적 인식과 같은 문제를 식별하는 데 도움이 된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline  & **Ours** & LLaVA[1] & VisTI[15] & TestVQA[6] & STVQA[16] & DUDE[17] & InforgrableVQA[18] & SED [19] \\\\ \\hline Year Referenced & 2024 & 2023 & 2023 & 2019 & 2019 & 2023 & 2021 & 2023 \\\\ \\hline Coms. Text-Rich Visual Reasoning & ✓ & ✗ & ✗ & ✓ & ✓ & ✓ & ✗ & ✗ \\\\ Text in Images & ✓ & ✗ & ✗ & ✓ & ✓ & ✓ & ✗ & ✗ \\\\ \\hline Number of LLVM Models & 13 & 3 & 10 & ✓ & 9 & - & 15 \\\\ Number of Images & 506 & 24 & 574 & 28.4K & 23K & 5K & 5.4K & 19K \\\\ \\hline Diverse Image Sources & ✓ & ✗ & ✓ & ✗ & ✓ & ✓ & ✗ & ✗ \\\\ \\hline Question Instructions & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\\\ Imperative Intrusions & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\\\ \\hline Instruction Gen. by Humans & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ \\\\ Reference Response Gen. by Humans & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ \\\\ \\hline Human Evaluation & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\\\ Automatic Evaluation & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\\\ Human-Aons Eval. Correlation & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\\\ \\hline Human performance & ✗ & ✗ & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ \\\\ Absolute Score to Models & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\\\ Fine-grained Analysis & ✓ & ✓ & ✓ & ✗ & ✓ & ✓ & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 텍스트가 풍부한 시각적 추론에 대한 대규모 멀티모달 모델을 평가하기 위한 관련 작품과의 비교. 우리는 코스트로서 컨텍스트에 민감한 것을 젠트로 축약한다.\n' +
      '\n' +
      '콘교과 Datasetet 2.\n' +
      '\n' +
      '### Collection Guidelines\n' +
      '\n' +
      'LMM의 텍스트 판독 능력을 주로 평가하는 텍스트가 풍부한 이미지에 대한 기존 벤치마크에는 주목할 만한 격차가 있다는 점에 주목한다. 우리의 데이터 세트는 이 갭을 교량하고 이미지 내의 임베디드 텍스트 및 시각적 특징에 대한 LMM의 공동 추론 능력을 테스트하기 위한 평가 프레임워크를 제공한다(그림 2). 우리의 데이터 세트는 다양한 자연 및 디지털 텍스트가 풍부한 시각적 시나리오에 걸쳐 다양한 작업을 포괄하여 강력한 테스트를 가능하게 합니다.\n' +
      '\n' +
      '광범위하게, 우리의 벤치마크는 이러한 주요 데이터 세트 수집 지침을 따른다.\n' +
      '\n' +
      '*이치 샘플은 \\(<이미지,\\ 명령어,\\ 응답>\\) 트리플트로 구성되어 있으며, 이 명령어들은 모델들이 이미지 내의 텍스트 및 시각적 요소에 대한 컨텍스트-민감성 추론을 수행하도록 요구한다. 구체적으로, 검출된 OCR보다 텍스트 기반 추론(예: LLM을 사용하는)에 의해 응답될 수 있는 지침을 만드는 것을 피할 것이다.\n' +
      '\n' +
      '그림 4: 데이터 수집 피라인: (1) \'짧은 목록 이미지 - 설명서 및 자동 방법을 사용하여 주석을 위한 소스 데이터 세트의 이미지를 단락한다. (2) 지도 대응 창작 - 저자를 두 개의 별개의 그룹(녹색 1과 보라색으로 대표되는 그룹 2)으로 전환하여 각 그룹을 4가지 범주에 해당하는 이미지 주석의 책임을 부여한다. (3) 데이터 샘플 발화 - 이전 단계에서 주석이 달린 \\(<이미지, 명령어, 응답>\\) 트리플트를 스루틴화하고 한 그룹은 다른 그룹이 만든 주석을 교차 평가합니다.\n' +
      '\n' +
      '그림 3: 콘텍스트의 8가지 시각적 맥락의 A 샘플은 큰 다중 모드 모델이 따를 수 있어야 한다.\n' +
      '\n' +
      '* 우리는 질문과 필수 작업을 포함한 다양한 지침을 다루는 것을 목표로 합니다. 이를 통해 생성된 데이터셋이 풍부한 다양한 지침을 입증합니다.\n' +
      '* 우리는 다양한 복잡성의 지침을 만드는 것을 목표로 합니다. 구체적으로 특정 텍스트 또는 시각적 요소(예를 들어, 바나나 맛이 있는 음료)를 추출하는 것을 포함하는 추출 지침을 만들 수 있다. 또한 임베디드 정보(예를 들어, 파란색으로 렌더링된 단어 수)에 대한 추론과 관련된 지침을 만들 수 있다.\n' +
      '\n' +
      '이 작업에서 데이터 세트를 실제와 디지털 인간 상호작용을 포함하는 8개의 별개의 시각적 시나리오로 분류하여 분류법을 확립한다. 이러한 시나리오는 쇼핑(예: 식료품, 의류, 가제트, 각종 내비게이션 표지판, 타임(시계 및 달력을 사용하는 시간 및 날짜), 웹 Usage(서로 다른 영역을 통한 시간 및 날짜), 애플리케이션 Usage(교육, 게임, 엔터테인먼트 등을 포괄하는 스마트폰 애플리케이션), 인포그래픽(다양한 주제에 대한 시각적 정보), 인스트럭션(앰프와 같은 추상적인 개념을 보여주는 텍스트 풍부 이미지), 잡종 천연 스켄(무선 인간 상호작용)을 포함한다. 더 자세한 내용은 부록 SSA에서 확인할 수 있습니다.\n' +
      '\n' +
      '### Data Sources\n' +
      '\n' +
      '컨텍스트는 6개의 상이한 소스로부터 제공되는 이미지를 포함한다. 먼저 LAION-5B[20]에서 _Time_, _Shopping_, _Navigation & Abstract_ 범주에 대한 이미지를 얻는다. 구체적으로 CLIP-리코딩 UI [21]를 이용한 키워드 검색을 사용한다. 키워드는 카테고리별 단어(들) + \'텍스트(예를 들어 쇼핑용 의류 텍스트, 내비게이션용 공항 텍스트)로 구성된다. 우리가 사용한 일부 범주별 단어는 쇼핑(예: 식료품, 가구, 가제트, 화장품, 서비스, 옷), 내비게이션(예: 길거리 표지판, 자동차, 버스, 버스, 지하철, 공항, 스테이션, 고속도로, 도로), 시간(예: 클록, 다중 클럭, 디지털 시계, 타임존스, 캘린더, 일정, 시계), Abstr(예: 메모, 인용문, 만화 스트립, 과학 농담, 수학 농담, 인포그래픽 메모)이다.\n' +
      '\n' +
      '둘째, 우리는 27개 카테고리에 걸쳐 9,300개의 안드로이드 앱에서 유래한 66,000개의 별개의 UI 스크린을 포함하는 Rico Dataset[22]의 _앱 애플리케이션 Usage_ 카테고리에 대한 이미지를 소스한다. 셋째, 오픈 웹텍스트 이니셔티브[23]에서 사용할 수 있는 웹사이트 링크를 스크롤하고 _Web Usage_ 범주에 대한 스크린샷을 수집한다. 오픈웹텍스트는 오픈AI의 웹텍스트 데이터세트[24]를 재현하기 위한 오픈소스 노력이다. 마지막으로, 우리는 세 가지 비주얼 질문 검색(VQA [18], 즉 인포그래픽VQA[18], 데이터 세트로부터 이미지를 획득한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline\n' +
      '**Statistic** & **Number** \\\\ \\hline Total number of samples & 506 \\\\ - Images & 506 \\\\ - Instructions & 506 \\\\ - Responses & 506 \\\\ Root verbs in instructions & 79 \\\\ \\hline Visual Scenarios & 8 \\\\ Samples per Scenario & \\\\ - Time & 50 \\\\ - Shopping & 50 \\\\ - Navigation & 50 \\\\ - Abstract & 50 \\\\ - Application Usage & 50 \\\\ - Web Usage & 50 \\\\ - Infographic & 50 \\\\ - Miscellaneous Natural Scenes & 156 \\\\ \\hline Average Instruction Length & 65 \\\\ Average Response Length & 117 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: ConTextualSTVQA[16], ESTVQA[7]의 핵심 통계로 새로운 지도 반응 쌍으로 주석을 진행한다. 원래 이러한 데이터 세트는 주로 텍스트 인식 능력에 초점을 맞춘 질문 및 응답 쌍으로 구성된다. 그러나 그들은 이미지에 대한 컨텍스트에 민감한 명령어를 공식화할 수 있는 기회를 제공한다. 결과적으로, 우리는 이러한 이미지 인스턴스를 재안내하여 인포그래픽VQA 데이터세트로부터 조달된 _인포그래픽_ 및 STVQA 및 ESTVQA 데이터세트로부터 조달된 _미소셀릭 천연 Scenes_의 두 그룹으로 분류한다.\n' +
      '\n' +
      '### Data Annotation\n' +
      '\n' +
      '이미지 소싱 후 그림 4와 같은 3단계 주석 과정을 진행한다.\n' +
      '\n' +
      '1단계: 쇼트리스트 이미지 이 단계에서, 우리는 명령어-반응 쌍 주석을 위한 짧은 리스트 이미지를 나열한다. i_Time_, _Shopping_, _Navigation_ 및 _Abstract_로 분류된 이미지는 주석 적합성을 보장하기 위해 수동 필터링을 거친다. 그러나 _적용 Usage_, _Web Usage_, _Infographic_ 및 _Miscellages 자연 Scenes_의 경우 휴리스틱 기반 필터링을 수행한다. 구체적으로, 우리는 이미지 내의 텍스트를 검출하기 위해 파들OCR[25]을 사용한다. 그 후, 우리는 데이터 세트에 주석이 달린 하위 집합인 단어 수가 가장 많은 상위 500개의 이미지를 선택한다.\n' +
      '\n' +
      '2단계: 지도-대응 창작. 여기서는 저자를 각각 4개의 특정 범주를 주석을 담당하는 그룹 1과 그룹 2의 두 그룹으로 나누었다. 저자는 주석 과정 전반에 걸쳐 제공된 주석 지침을 엄격하게 준수했다. 1111.\n' +
      '\n' +
      '부츠 1: MTurk 작업자가 이 작업을 시간이 많이 소요되어 제한된 예산 내에서 달성하기 어려운 주석으로 이어진다는 것을 관찰한다.\n' +
      '\n' +
      '이 마지막 단계에서는 데이터 삼플 검증 3: 데이터 2에 주석이 달린 각 샘플에 대한 검증 과정을 수행하며, 이를 위해 각 그룹은 다른 그룹이 만든 주석들을 검토하는 작업을 수행한다. 이를 통해 지침을 준수하고 저품질 샘플을 필터링할 수 있습니다. 마지막으로 **506** 인스턴스의 데이터 세트를 사용하여 마무리합니다.\n' +
      '\n' +
      '메타데이터 주석 주입 메타데이터는 벤치마크에 대한 포괄적인 분석과 그에 대한 모델의 성능을 용이하게 한다. 이를 위해 <표 2>와 같이 **ConTextual*** 벤치마크에 대한 통계를 제공하고 있으며, 그림 5와 같이 뿌리 동사와 직명사를 기준으로 각 지시를 시각화하여 추출 유형인지 여부와 수학적 추론(Appendix SSA.1)이 필요한지 여부를 결정하기 위해 각 표본을 주석을 달았다.\n' +
      '\n' +
      '### Data Release\n' +
      '\n' +
      '컨텍스트는 8개의 시각적 범주에 걸쳐 있는 506개의 샘플(표 2 참조)을 포함한다. 모델 개발을 용이하게 하기 위해 506에서 100개의 샘플의 하위 집합을 검증 세트와 함께 참조 응답으로 방출하는 동시에 나머지 406개의 샘플에 대해 숨겨져 있다. 검증 샘플의 분포가 전체 데이터 세트 분포를 밀접하게 반영하는지 확인합니다. 이를 달성하기 위해 우리는 전체 벤치마크와 일치하는 검증 샘플에서 각 범주의 비례 표현을 유지하는 \'피부 천연 스켄\' 범주의 샘플 30개와 나머지 범주의 샘플 10개를 무작위로 선택한다. 본 논문에서는 달리 명시되지 않는 한 모든 결과가 전체 데이터셋에 보고되어 있다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '우리는 콘텍스트를 사용하여 대형 멀티모달 모델(LMM)의 맥락에 민감한 텍스트가 풍부한 시각적 추론 능력에 대한 철저한 정량적이고 질적 평가를 수행한다. 구체적으로, 우리는 10+ 기반 모델을 평가하고 데이터세트 3.1에 대한 인간 성능 기준선을 설정하고 모델 3.2의 예측된 반응을 산출하기 위해 다양한 자동 평가 방법을 활용하며, 이는 SS3.3에서 인간-자동 상관 분석과 함께 모델의 성능에 대한 정량적 결과를 제공하므로 SS3.4에서 시각적 맥락, 시각적 장면 및 작업 복잡성에서 모델의 성능에 대한 정밀한 평가를 수행합니다. 따라서 SS4에서 정성적 분석을 제공한다.\n' +
      '\n' +
      '### Setup\n' +
      '\n' +
      '증강 LLM 이전 작품[26; 27; 28; 29]은 다양한 시각적 도구(예: 이미지 캡션, 객체 검출기 모델)의 감각 정보를 활용하여 큰 언어 모델이 광범위한 시각적 추론 문제를 해결할 수 있음을 보여주었다. 우리의 데이터 세트는 텍스트가 풍부한 시각적 추론에 초점을 맞추고 있기 때문에 강력한 LLM GPT-4가 OCR 정보와 이미지 캡션으로 ConTextual 데이터셋에서 수행할 수 있는 정도를 이해하는 것이 필수적이다. 이를 위해 우리는 (a) 바닐라 OCR으로 촉발된 GPT-4, (b) 레이아웃 인식 OCR 및 (c) 레이아웃 인식 OCR과 이미지 캡션을 결합한 세 가지 설정 하에서 이러한 증강 설정을 연구한다. 이미지로부터 OCR을 추출하기 위한 파들 OCR 라이브러리[25]의 PP-OCRv4 모델, OCR에서 레이아웃 인식 유지를 위한 OCR 텍스트 배열 구현에 영감을 준 LATIN 프롬프트[30] 및 조밀한 이미지 캡션에 대해 ShareGPT-4V-7B를 레버리지한다. 우리는 부록 SSB에서 GPT-4 프롬프트를 제공합니다.\n' +
      '\n' +
      'LMM. 우리는 다른 시각적 추론 벤치마크[31]에서 최첨단 LMM을 달성한 대표적인 독점적 LMM인 GPT-4V[2] 및 Gemini-Pro-Vision[3]를 평가한다. 또한 LLaVA-1.5-13B[11], ShareGPT-4V-7B[12], mPLUG-Owl-v2-7B[32; 33], Qwen-VL-Plus[5], 구조BLIP[4], Idefics-9B[13]를 포함한 광범위한 오픈 LMM을 평가한다. 우리는 향상된 텍스트가 풍부한 시각적 추론을 위해 도입되었기 때문에 LLaVAR[34] 및 BLIVA[35]를 포함한다.\n' +
      '\n' +
      '기본적으로 제로 샷 설정 하에서 위의 모델의 성능을 평가한다. 증강 LLM(GPT-4, 레이아웃 인식 OCR 및 이미지 캡션, 게미니-Pro-Vision 및 Idefics-9B)을 포함한 선택된 모델에 대해 소수의 샷 평가(2샷, 4샷, 8샷)를 수행한다.\n' +
      '\n' +
      '인간은 또한 아마존 기계 투르크를 사용하여 데이터셋에 인간의 성능을 벤치마킹합니다. 자격 테스트를 통과한 선택된 주석을 데이터세트로부터 모든 지시 이미지에 대한 정확한 응답을 작성하도록 요청하였다. 우리는 부록 SSC.1에서 주석 인터페이스의 스크린샷을 제공합니다. 데이터셋에서 인간 예측을 수집하는 데 \\(95달러180\\)를 보냈다.\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      'SS2에서 논의된 바와 같이 ConTextual 데이터셋의 모든 사례는 이미지, 명령어 및 자연어에서의 참조 응답으로 구성된다. 평가의 목표는 주어진 이미지에 대한 지시에 대한 모델 응답의 정확성을 평가하는 것이다.\n' +
      '\n' +
      '인체평가 3.2.1.1의 인적평가#### 3.2.2.1의 인적평가###### 3.2.1의 인적평가########\n' +
      '\n' +
      '예측된 반응에 대한 충실한 평가를 수행하기 위해 아마존 기계 투르크에서 조달한 인간 주석에게 데이터세트로부터 이미지, 명령어 및 참조 응답을 감안할 때 예측된 응답 품질을 평가하도록 요청한다. 먼저 ConTextual 데이터세트로부터 무작위로 데이터셋으로부터 280개의 인스턴스를 샘플한다. 둘째, 레이아웃 인식 OCR 및 이미지 캡션을 가진 LLM(GPT-4), GPT-4V, Gemini-Pro-Vision, LLaVA-1.5-13B, ShareGPT-4V-7B 및 사람에게서 이러한 사례에 대한 모델 반응을 수집한다. 총 1680건의 모델 및 인간의 예측 반응이 있습니다. 셋째, 우리는 모델 정체성을 독립적으로 3명의 인간 주석을 대상으로 밝히지 않고 각각의 모델 반응을 보여준다. 구체적으로, 인간 주석들은 데이터세트로부터의 참조 응답, 명령어 및 이미지를 감안할 때 예측된 응답을 결정하도록 요청된다. 마지막으로, 3명의 주석 중 다수표를 모형 예측에 대한 최종 결정으로 사용하여 응답의 수용 등급(0-100%)을 보고한다. 우리는 부록 C.2에서 주석 인터페이스의 스크린샷을 제공합니다. 우리는 인간 판단을 획득하는 데 \\(1,000\\)를 보냈다.\n' +
      '\n' +
      '3.2.2의 자동평가###### 3.2.2의 자동평가######\n' +
      '\n' +
      '인간의 평가는 금의 기준으로 작용하지만, 비싸고 시간이 많이 들기 때문에 규모가 어렵다. 데이터 세트는 각 지시에 대해 고유하게 참조 응답을 제공하기 때문에 광범위한 참조 유도 자동 평가 방법을 활용한다. 구체적으로, 이는 지시, 참조 응답 및 예측된 응답으로 LLM GPT-4를 프롬프트하는 것(a)을 포함하며, (b)는 이미지, 명령어, 참조 응답 및 예측된 응답으로 LMM GPT-4V를 프롬프트하고, (c) 및 BLEURT [14], Rouge-L [36] 및 BERTS 코어 [37]와 같은 다른 텍스트 생성 방법을 참조 응답과 예측 응답 간의 유사성을 평가하는 Rouge-L[36]와 같은 다른 텍스트 생성 방법을 사용하여 LMM GPT-4V를 지시, 지시, 지시, 지시, 참조 응답과 예측 응답 간의 유사성을 평가하는 Rouge-L[36] 및 BERTS 코어 [37]와 같은 다른 텍스트 생성 방법(Ruge-L[36] 및 BERTS 코어 [37] 및 BERTS 코어 [37]와 같은 다른 텍스트 생성 방법을 통해 LMM GPT-4를 호출한다. 구체적으로 GPT-4 및 GPT-4V는 인간 평가와 동일한 예측된 반응에 대한 판단을 제공하는 것으로 촉발된다. 부록 SSD에서 GPT-4 기반 평가의 프롬프트를 제시한다. 다만, 다른 텍스트 생성 방법은 0-100으로 스케일링된 연속 점수 0-1을 제공한다.\n' +
      '\n' +
      '자동 평가 방법을 통해 전체 데이터 세트에 대한 모든 모델 반응을 평가한다. 그 후, 우리는 자동화된 접근법의 효능을 평가하기 위해 인간 평가에서 동일한 1,680개의 반응을 사용하여 인간과 자동화된 방법 간의 상관 분석을 수행한다(SS3.3.1). 마지막으로 전체 데이터셋(SS3.4)에 대한 모든 모델에 대한 대규모 평가를 위해 인간 판단과 가장 높은 상관관계를 달성하는 GPT-4 자동평가를 활용한다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '표 3에서 인간 및 자동 평가를 사용하여 ConTextual에 대한 증강 LLM, LMM 및 인간의 성능을 비교하며, 인간 평가를 통해 인간은 데이터셋에 대해 가장 잘 수행한다는 것을 알 수 있다(80.1\\%\\). 또한 GPT-4V가 다른 모든 모델과 비교하여 \\(49.3\\%\\)의 최고 수용 등급을 달성한다는 것을 관찰했다. 그러나 이 등급은 인간의 성과와 상당히 거리가 멀며, 이는 인간이 잘하는 동안 우리의 과제가 최첨단 LMM에 상당히 도전적이라는 것을 나타낸다. 우리는 GPT-4V가 텍스트가 풍부한 시각적 추론 능력의 큰 격차를 강조하는 \\(22\\%\\)에 의한 Gemini-Pro-Vision을 능가한다는 것을 발견했다. 또한, 증강 LLM 접근법이 정확한 시각적 인식 없이 데이터세트 인스턴스를 해결할 수 없음을 나타내는 \\(17.2\\%\\)의 매우 낮은 등급을 달성한다는 것을 발견했다. 흥미롭게도 LLaVA-1.5-13B 및 ShareGPT-4V-7B와 같은 개방형 모델은 독점 모델로부터의 능력에 큰 격차가 있음을 나타내는 인간 평가를 통해 불량한 수용 등급을 달성한다는 것을 관찰했다. 이는 사전 억제 데이터의 규모와 품질과 함께 모델 용량의 차이에 기인할 수 있다.\n' +
      '\n' +
      '인간 평가가 확장성이 없기 때문에 전체 데이터 세트에 대한 모델 응답에 대한 자동 평가를 수행한다. 표 3에서 인간 반응의 등급이 GPT-4 및 GPT-4V 평가를 사용하여 GPT-4V의 등급을 \\(22.2\\%\\) 및 \\(23.6\\%\\)만큼 능가한다는 것을 발견했다. 유사한 인간 평가 GPT-4 및 GPT-4V를 사용한 자동 평가는 ConTextual 데이터셋의 인간 성능이 최고의 수행 LMM보다 훨씬 높다는 것을 강조한다. 흥미롭게도 성능 GPT-4V와 Gemini-Pro-Vision의 격차는 \\(7.2\\%\\)이다. 또한, 우리는 여전히 독점 모델의 성능 격차가 크고 LMM을 개방한다. SS3.4에서 다양한 품질 차원을 따라 모델 역량의 격차를 이해하기 위해 미세 학습 평가를 수행한다.\n' +
      '\n' +
      '또한, 우리는 인간에 대한 BLEURT 점수가 가장 높은 반면 GPT-4V는 LMM 중에서 가장 높은 점수를 달성한다는 것을 발견했다. 흥미롭게도 개방형 모델(LLaVA-1.5, ShareGPT-4V)이 Gemini-Pro-Vision보다 더 높은 BLEURT 점수를 달성한다는 것을 발견했다. 우리는 Rouge-L 및 BERTS 코어 기반 자동 평가에서 유사한 반직관적 경향을 관찰한다. 예를 들어, 루지-L과 BERTS코어는 인간 반응이 최고라고 생각했음에도 불구하고 GPT-4V보다 개방형 모델을 더 잘 순위화한다. 이러한 반직관적 관찰은 참조 및 예측된 반응[14]의 어휘 변화 차이에 대한 이러한 방법의 민감도에 기인할 수 있다.\n' +
      '\n' +
      '자동 평가 방법에 대한 상관관계 분석에서는 3.3.1.1###############################.\n' +
      '\n' +
      '우리는 표 4에서 ROC-AUC와 스피어먼 상관관계를 사용하여 후보 자동 메트릭과 인간 판결 간의 상관 관계를 측정하는데, 인간 판단은 표 4에서 ROC-AUC 및 스피어먼 상관 관계에 대한 수용되지 않은 응답에 대해 \'0\'과 \'1\'을 할당하는 금 기준으로 간주된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline GPT-4 w/ Layout-aware OCR + Caption & 17.2 & 22.2 & 17.6 & 41.3 & 22.5 & 53.9 \\\\ GPT-4V [2] & 49.3 & 47.4 & 45.0 & 45.3 & 17.3 & 52.5 \\\\ Gemini-Pro-Vision [3] & 27.8 & 40.2 & 37.1 & 42.5 & 30.1 & 58.4 \\\\ LLaVA-1.5-13B [11] & 17.2 & 20.6 & 17.5 & 43.6 & 21.7 & 54.8 \\\\ ShareGPT-4V-7B [12] & 21.8 & 22.6 & 20.6 & 44.5 & 23.3 & 55.8 \\\\ \\hline Humans & **80.1** & **69.6** & **68.6** & **47.4** & **33.6** & **59.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 다양한 기반 모델(증강 LLM 및 LMM)의 성능 비교 및 ConTextual 데이터세트에서의 인간이다. 인간의 평가, 자동 GPT-4 및 GPT-4V 기반 평가를 사용하여 응답 수용 등급을 보고한다. 또한 BLEURT, Rouge-L 및 BERTS코어를 포함한 표준 텍스트 생성 품질 평가 메트릭을 보고한다. 우리는 인간은 데이터셋에서 기존 모델을 능가한다는 것을 알게 됩니다. 가장 좋은 수행 LMM은 GPT-4V는 \\(30\\%\\)의 인간 성과 뒤에 있다. 컬럼에서 최고의 성능은 **BLACK**에서 강조되는 반면 두 번째 최고의 성능은 **UNDERLINE***에서 강조된다.\n' +
      '\n' +
      '응답을 수용했습니다. GPT-4 기반 평가가 모든 자동 평가 메트릭 중에서 \\(85.9\\)의 가장 높은 ROC-AUC와 \\(0.71\\)의 스피어먼 상관관계를 달성한다는 것을 발견했다. 또한 GPT-4V는 또한 GPT-4에 가까운 인간 판단과 높은 상관관계를 달성한다는 것을 관찰하며, GPT-4 및 GPT-4V의 판단의 차이는 각각의 투입물의 차이에 기인할 수 있다. 구체적으로, GPT-4는 주어진 명령어와 참조 응답에 대한 판단을 기반하는 반면, 입력 이미지에 대한 액세스를 갖는 GPT-4V는 잠재적으로 편향될 수 있다. 이 액세스는 GPT-4V를 유도하여 기준 응답을 간과하고 경우에 따라 판단을 내리기 위한 입력 이미지로부터의 시각적 신호에 의존할 수 있다. 마지막으로, 표준 텍스트 생성 메트릭이 GPT-4 메트릭과 비교하여 불량한 ROC-AUC 및 스피어먼 상관관계를 달성한다는 것을 관찰했다. 이는 GPT-4 평가가 표준 텍스트 생성 메트릭을 능가하는 선행 연구[15]의 결과를 확증한다. 결과적으로 GPT-4를 사용하여 스케일에서 ConTextual 데이터셋에 대한 예측된 반응의 품질을 자동으로 평가한다.\n' +
      '\n' +
      '3.3.2 Few-Shot 세트는 3.3.2 Few-Shot 설정#####.\n' +
      '\n' +
      '여기에서 GPT-4 평가를 사용하여 ConT-4 평가에 대한 기반 모델의 성능을 그림 6의 소수의 샷 설정 아래 비교했으며, 구체적으로 증강 LLM(GPT-4 프롬프트 w/플래시된 w/이미지 캡션), Gemini-Pro-Vision 및 Idefics-9B에 대해 제로샷, 투샷, 4샷 및 8샷 평가를 수행한다. 데이터세트에서 무작위로 문자 내 예를 선택하고 나머지 인스턴스에 대한 모델을 평가한다.\n' +
      '\n' +
      '우리의 실험에서 우리는 모든 모델의 성능이 인텍스트 예에서 크게 변하지 않는다는 것을 발견했다. 구체적으로 제로샷 설정에 비해 2샷 설정에서 게미니-프로비전 응답 수용 등급은 \\(5\\%\\)만큼 감소하며, 2샷에서 8샷으로 단조롭게 증가함을 관측한다. 또한, 우리는 4개의 텍스트 내 예 후에 성능 개선이 Idefics-9B에 대해 정체되었음을 관찰한다. 최근 연구에서는 LMM의 불안정성과 민감도를 소수의 샷 설정[38]에서 강조한다. 예를 들어, 4샷 설정에서 구성BLIP와 같은 모델, 특히 커먼센스 추론이 필요한 작업에서 상당한 정확도 감소가 관찰되었다. 전반적으로, 우리는 소수의 샷 사례를 제공하는 것이 기반 모델에서 컨텍스트에 민감한 텍스트가 풍부한 시각적 추론을 이끌어내지 않는다는 것을 강조한다.\n' +
      '\n' +
      '### Fine-Grained Evaluation\n' +
      '\n' +
      '표 5에서 GPT-4 평가를 사용하여 다양한 시각적 맥락에 걸쳐 광범위한 기반 모델의 미세 변경 성능을 비교했으며, 우리는 GPT-4V가 거의 모든 범주에서 기준선 모델을 능가한다는 것을 발견했다. 우리는 유일한 예외가 웹 사용 및 잡종 자연 장면 맥락이며, 그미니-프로-비전은 리드를 보유하고 있음을 관찰한다. 특히 GPT-4V는 추상적 범주에 대한 추론에 대해 인간을 능가하는 것으로, 많은 회고에 대한 이성과 데이터를 인용하는 것으로 조정되었을 수 있음을 강조한다. 또한, 우리는 모든 모델이 시간 범주에서 가장 고군분투하고 인간이 에이스하는 동안 훈련 데이터에서 배우기 어려울 수 있는 기술임을 관찰한다. 시간 판독 후 데이터 시각화에 대한 추론으로 구성된 인포그래픽 카테고리의 독점적인 LMM은 과소 예측한다. 이전 작업[8; 39]은 기존 LMM이 차트에 대한 추론에 있어서 인간을 과소수행하는 것으로 나타났다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & **GPT-4** & **GPT-4V** & **BLEURT** & **RougeL** & **BERTScore** \\\\ \\hline ROC-AUC & **85.9** & 83.9 & 72.9 & 67.6 & 66.8 \\\\ Spearman Correlation & **0.71** & 0.68 & 0.38 & 0.29 & 0.28 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: ROC-AUC 및 스피어먼 상관 관계를 사용한 인간 및 자동 평가 메트릭의 비교이다. GPT-4 및 GPT-4V 기반 평가가 두 방법 모두에서 인간과 가장 상관관계가 있다는 것을 발견했다.\n' +
      '\n' +
      '그림 6: ConTextual 데이터셋에서 Few-shot 성능을 나타낸 것이다.\n' +
      '\n' +
      '또한 LLaVA-1.5-13B 및 ShareGPT-4V-7B와 같은 최적의 개방형 모델이 시간, 쇼핑, 내비게이션, 앱(모바일) 사용, 웹 사용 및 인포그래픽으로 투쟁하는 동안 추상적이고 자연스러운 장면의 맥락에서만 \\(25\\%\\) 이상의 수용 등급을 달성한다는 것을 관찰했다. 이는 그들의 훈련 데이터에 다양한 시각적 맥락이 없기 때문일 수 있다. 예를 들어, 열린 모델에서 비전-언어 정렬에 사용되는 COCO 데이터셋[40]은 주로 자연 장면을 포함한다. 또한, 우리는 ConTextual 데이터셋 상의 LLaVAR 및 BLIVA 필터와 같은 텍스트가 풍부한 시각적 추론에 대해 구체적으로 도입된 오픈 모델이 관찰된다. 이는 이러한 모델이 이미지 내의 텍스트 내용과 시각적 맥락에 대해 명령어를 공동으로 필요로 할 때 이유할 수 없음을 나타낸다. 부록 SSE에서 추가 미세 편성 평가를 수행합니다. 전반적으로, 우리의 미세 지능 분석에서는 차세대 LMM의 발달에 영감을 줄 기존 모델의 격차를 식별하는 데 도움이 됩니다.\n' +
      '\n' +
      '4개의 정성적 예.\n' +
      '\n' +
      'GPT-4V 대 HumansGPT-4V는 ConTextual에서 잘 수행되지만 여전히 인간 평가에 따라 **30.8%***, GPT-4 분석에서 **22.2%**의 간격을 가지고 있다. 이러한 격차를 더 잘 이해하기 위해 우리는 인간의 성과에 비해 GPT-4V의 결과를 질적으로 분석한다. 예를 들어, 그림 7에서 GPT-4V가 잘못된 답변을 제공하는 경우를 본다. 이 경우, 모델은 우주 발사체-공간 셔틀, 외부 탱크 및 로켓 스루터의 다른 부분의 색상을 식별하도록 요청된다. GPT-4V는 색상 예측의 오류를 만들지만 다이어그램의 정보를 정확하게 유추할 수 있어 정밀한 시각적 인식의 부족을 드러낸다. 공간 셔틀은 흰색이고 로켓의 빨간색 팁을 잘못 가정합니다. 우리는 부록 SSF(그림 25, 29, 33, 34, 48, 51, 52)에서 더 많은 예를 제공하며, GPT-4V의 핵심 문제는 사전 시각 지식(예: 공간 셔틀이 노란색으로 표시되는 경우에도 흰색이라고 가정)에 대한 편향과 결합된 미세곡선 인식에 있음을 강조한다. GPT-4V가 교란된 공통 시각 장애를 가진 버전에 실패하는 이전 작업 [41]에서 유사한 분석이 제시되었다.\n' +
      '\n' +
      'GPT-4V 대. 오픈 LMM 및 증강현실 LLM은 또한 오픈 소스 LMM 및 주입 LLM 접근법에 대한 GPT-4V의 성능을 정성적으로 비교한다. 이를 위해 그림 8에 예시된 예를 사용하여 각각 ShareGPT-4V-7B 및 GPT-4 w/Layout-인식 OCR + Caption을 사용하여 최적의 개방형 소스 LMM 및 발효 LLM 접근법을 분석하는데, 이 예에서 모델은 예시된 세탁기를 9개월 동안 고용하는 비용을 제공하도록 지시하였다. 지시에 정확하게 응답하기 위해서는 먼저 해당 이미지에 어떤 기계가 묘사되어 있는지 이해한 다음 이를 바탕으로 명령어에 언급된 기간에 대한 적절한 비용을 추출해야 한다. 우리는 GPT-4V가 기계를 정확하게 식별하고 기계를 정확하게 식별할 수 있음을 관찰했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c c c c} \\hline Models & Avg. & Time & Shop. & Nav. & Abs. & App. & Web. & Info. & Misc. NS. \\\\ \\hline _Augmented Large Language Models_ & & & & & & & & & & \\\\ \\hline GPT-4 w/ OCR & 15.9 & 4.0 & 10.0 & 14.0 & 30.6 & 8.0 & 16.0 & 28.6 & 16.9 \\\\ GPT-4 w/ Layout-aware OCR & 18.2 & 8.0 & 20.0 & 18.0 & 34.7 & 10.0 & 16.0 & 16.0 & 20.7 \\\\ GPT-4 w/ Layout-aware OCR + Caption & 22.2 & 6.0 & 16.0 & 24.0 & 57.1 & 14.0 & 18.0 & 8.0 & 27.3 \\\\ \\hline _Large Multimodal Models_ & & & & & & & & & \\\\ \\hline GPT-4V [2] & 47.4 & 18.0 & 54.0 & 48.0 & **100.0** & 48.0 & 42.0 & 28.0 & 48.0 \\\\ Gemini-Pro-Vision [3] & 40.2 & 16.0 & 32.7 & 28.6 & 65.3 & 44.9 & 43.8 & 20.0 & 52.8 \\\\ LaAv-1.5B-13B [11] & 20.8 & 4.0 & 10.0 & 18.0 & 44.9 & 16.0 & 26.0 & 4.0 & 29.7 \\\\ ShareGPT-4V-7B [12] & 22.6 & 0.0 & 16.0 & 20.0 & 28.6 & 20.0 & 20.0 & 14.0 & 37.7 \\\\ mPLUG-owl-v2-7B [32] & 18.6 & 4.0 & 8.0 & 24.0 & 32.7 & 20.0 & 10.0 & 12.0 & 26.0 \\\\ Qwen-VL-Plus [5] & 21.8 & 4.0 & 20.0 & 24.0 & 53.1 & 6.0 & 18.0 & 14.0 & 27.3 \\\\ InstruRLIP-Vicuna-7B [4] & 9.7 & 2.0 & 4.0 & 16.0 & 20.0 & 6.0 & 12.0 & 2.1 & 12.0 \\\\ Idefies-9B [13] & 7.7 & 4.0 & 2.0 & 12.0 & 12.0 & 0.0 & 6.0 & 2.0 & 13.3 \\\\ LLaVAR [34] & 14.9 & 10.0 & 16.0 & 6.0 & 44.9 & 8.0 & 10.0 & 6.0 & 16.7 \\\\ BLIVA [35] & 10.3 & 2.0 & 4.0 & 14.0 & 24.5 & 4.0 & 8.0 & 4.0 & 14.7 \\\\ \\hline Humans & **69.6** & **64.0** & **64.0** & **73.5** & 75.6 & **64.0** & **58.0** & **72.0** & **78.0** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: 기반 모델(증강 LLM 및 LMM)의 성능 비교와 GPT-4 평가를 사용한 ConTextual 데이터 세트에 대한 인간이 비교되었다. 우리는 평균 응답 수용 등급인 Avg, Shop, Nav만큼 쇼핑, Abs만큼 Abstract, App로서의 응용 사용, Web Usage, Info와 같은 인포그래픽, NS로서의 피부 자연 장면들을 축약한다. 우리는 GPT-4V가 대부분의 범주에서 모든 모델 기저부를 능가하는 반면 Gemini-Pro-Vision은 웹 사용 및 자연 장면에서 가장 우수하다는 것을 발견했다. 컬럼에서 최고의 성능은 **BLACK**에서 강조되는 반면 두 번째 최고의 성능은 **UNDERLINE**에 의해 강조된다.\n' +
      '\n' +
      '올소는 정확한 양을 확인합니다. 그러나 ShareGPT-4V-7B와 발효 LLM 접근법은 그렇게 하지 못한다.\n' +
      '\n' +
      '오픈 소스 LMM은 시간을 올바르게 결정하지만 올바른 기계를 식별하지 못해 컨텍스트에 민감한 추론과 미세화된 시각적 인식에서 약점을 강조한다. 이 질적 예는 오픈 소스 LMM이 GPT-4V에 열등한 컨텍스트 민감성 추론을 입증한 다른 사례(더 많은 예들에 대한 부록 SS4)와 일치한다. 그들은 또한 주어진 이미지에 지시를 접지하거나 사전 지식에 의존하지 않기 때문에 환각 경향을 더 높게 보여준다. 우리는 더 많은 예를 위해 그림 26,27,30,31,38,47을 참조한다. 이 예에서, 증강 LLM 접근법은 기계를 상단 부하로 잘못 해석하고 LMM과 유사한 결과를 생성한다. 이는 텍스트와 이미지에 대한 이유(더 많은 예들에 대한 부록 SS4를 참조)와 공동으로 설명할 수 없기 때문일 수 있다.\n' +
      '\n' +
      '요약하면, GPT-4V는 ConTextual에서 개방 소스 LMM 및 추출 LLM을 능가하여 우수한 컨텍스트에 민감한 텍스트-비전 추론을 보여준다. 우리는 질적 분석을 통해 환각과 같은 문제를 파악하고, 이미지에 지시를 접지하지 못하고, 좋지 않은 컨텍스트에 민감한 텍스트 실행 추론과 관련된 미세한 시각 인식을 식별하는 데 도움이 된다.\n' +
      '\n' +
      '** 참조 대응:** 공간 셔틀은 노란색으로 묘사되고 로켓은 회색으로 묘사되며 외부 탱크는 빨간색으로 묘사된다.\n' +
      '\n' +
      '** 인간 반응:** 로켓은 회색입니다. 공간 셔틀은 노란색입니다. 외부 탱크는 빨간색입니다.\n' +
      '\n' +
      '***GPT-4V 대응:** 영상에서 우주셔틀의 고체 로켓 부스트기는 빨간색과 가벼운 회색으로 색칠하며, 위쪽에 빨간색과 아래쪽에 회색이 있다. 외부 탱크는 오렌지색입니다. 궤도자로도 알려진 우주셔틀의 본체는 주로 코와 날개 가장자리에 검은색이 있는 흰색이다. 꼬리 섹션에는 약간의 빨간색도 있습니다.\n' +
      '\n' +
      '** 요약된 이유:** 프리 런치에서 모든 구성 요소는 수직입니다. 셔틀이 오를 때 고체 로켓 부스트가 분리됩니다. 셔틀이 공간의 가장자리에 도달하면 외부 탱크가 방출됩니다. 이러한 단계 내내 셔틀의 색상은 일관적이며, 즉 본체에 대한 일반적인 색상인 흰색으로 남아 있다. 외부 탱크는 천연 단열색인 오렌지입니다. 고체 로켓 부스트기는 빨간 상단이 있는 흰색입니다.\n' +
      '\n' +
      '**ShareGPT-4V**는 논리적 추론에도 불구하고 지시에 대한 잘못된 반응을 제공한다. 녹색 사용은 참조와 일치하는 반응을 나타내는 반면, 빨간색은 반응의 오류를 강조한다. 또한, GPT-4V에서 그에 대한 답변에 도달하기 위해 사용하는 근거를 요약하기 위해 _Summarized Reasoning_을 제공한다.\n' +
      '\n' +
      '그림 8: 이 예에서 GPT-4V는 지도에 올바르게 반응한다. 그러나 ShareGPT-4V-7B(최고의 수행 오픈소스 LMM)와 GPT-4 w/레이아웃 인식 OCR+캡션(증강 LLM)은 텍스트와 이미지에 대한 공동 추론이 부족하여 잘못된 반응을 생성한다.\n' +
      '\n' +
      '그림 7: 이 경우 GPT-4V는 논리적 추론에도 불구하고 지시에 대한 잘못된 반응을 제공한다. 녹색 사용은 참조와 일치하는 반응을 나타내는 반면, 빨간색은 반응의 오류를 강조한다. 또한, GPT-4V에서 그에 대한 답변에 도달하기 위해 사용하는 근거를 요약하기 위해 _Summarized Reasoning_을 제공한다.\n' +
      '\n' +
      'Related Work\n' +
      '\n' +
      '최근 텍스트-리치 이미지 이해에 따르면 이미지[42; 43]에서 텍스트와 시각적 요소 간의 상호 작용을 이해하는 데 관심이 높아졌다. 이 분야의 모델의 진행을 추적하기 위해 OCRVQA[44], TextVQA[6], DocVQA[45], STVQA[16], ESTVQA[7]와 같은 여러 데이터 세트가 도입되었다. 이 데이터 세트는 문서 또는 자연 장면 영상에서 텍스트를 정확하게 판독하는 모델의 능력에 크게 중점을 둔다. 또한, 영상에서 텍스트 및 시각적 요소에 대한 모델의 공동 이유 능력을 평가하는 인포그래픽 VQA[18] 및 차트QA[46]과 같은 데이터 세트가 있다. 그러나 특정 영역(예: 인포그래픽)에 초점을 맞추고 질문 응답에 국한된다. 선행 작업[47]은 LMM이 기존 데이터셋에 대해 정확한 OCR을 수행할 수 있는 능력을 평가한다. 이에 비해, 우리는 이미지의 시각적 및 텍스트 요소에 대한 정확한 시각적 인식과 복잡한 추론을 수행하는 LMM의 능력을 테스트하는 것을 목표로 하는 광범위한 시각적 맥락, 명령어 유형(문항 및 필수 작업)을 포함하는 새로운 콘텐셜 데이터셋을 제안한다.\n' +
      '\n' +
      '실세계에 대한 높은 효용 모델을 구축하기 위한 분야의 진행을 평가하기 위해서는 고품질 데이터 세트가 있는 시각적 언어 정의 벤치마크가 필수적이다. 전통적으로 시력-언어 학습은 모델이 주요 대상과 관계를 주로 이해할 필요가 있는 시각 질문 응답 [48; 31] 또는 이미지 자막 [49; 40]과 같은 작업에 초점을 맞추었다. 또한, 수학 기술[8], 세계 지식[9], 학년 학교 과학 다이어그램[50]을 포함한 LMM의 특정 기술을 평가하는 여러 데이터 세트와 벤치마크도 있다. 이에 비해 우리의 콘텐셜 데이터 세트는 실제 시나리오에 대한 컨텍스트에 민감한 텍스트가 풍부한 시각적 추론을 수행하는 대형 멀티모달 모델의 능력을 소개하고 테스트한다. 여기에는 시간 읽기, 공공 공간에서의 내비게이션과 교통, 메모와 인용 이해, 쇼핑 등이 포함된다.\n' +
      '\n' +
      '최근 대형 멀티모달 모달(LMM)으로 불리는 명령어 제거 능력을 가진 비전-언어 생성 모델이 크게 급증하고 있다. 여기에는 GPT-4V[2] 및 Gemini-Pro-Vision[3]과 같은 독점 모델이 포함된다. 이러한 모델은 전통적인 비전 언어 모델에 대한 최첨단 성능을 달성했습니다. 개방 공간에서 모델은 LLaVA[1], mPLUG-Owl[32], OpenFlamingo[51], Idefics[13], LLaMA-Ad캡터[52], Idefics[13]을 포함한다. 또한 LLaVAR[34] 및 BLIVA[35]를 포함한 향상된 텍스트 풍부한 시각적 추론 능력에 초점을 맞춘 LMM 부류가 있다. 이 작업에서 ConTextual 데이터셋에서 LMM의 성능을 비교한다. 우리는 독점 모델의 텍스트가 풍부한 시각적 추론 능력이 오픈 모델보다 훨씬 우수하다는 것을 알게 된다. 우리는 또한 다양한 시각적 맥락에 걸쳐 모델 성능의 격차를 이해하기 위해 미세 편성 분석을 포함한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 연구에서는 대형 멀티모달 모델에서 텍스트가 풍부한 시각적 추론을 평가하기 위한 데이터세트인 ConTextual를 소개한다. 시각적 맥락에서 읽기 기술을 테스트하는 데 주로 초점을 맞춘 사전 노력을 넘어, 모델이 이미지에 제시된 맥락을 포착하기 위해 필요한 처음부터 새롭고 도전적인 지침을 만든다. 또한, 우리는 인간에게 데이터 세트를 해결하고 모델 응답 평가를 위해 인간 주석을 사용하도록 요청한다. 우리의 실험에서, 우리는 현대 LMM(독점 및 개방형 모델)이 인간에게 좋은 동안 데이터 세트에 대해 수행하기 위해 투쟁한다는 것을 발견했다. 또한 모델 역량의 격차를 정확히 파악하기 위해 미세 학습 평가 및 정성적 분석을 수행한다. 요약하면, 우리의 데이터 세트는 중요한 실제 응용 프로그램을 가진 도메인인 텍스트가 풍부한 이미지에 대한 추론 진행 상황을 평가하기 위한 경로를 절단한다.\n' +
      '\n' +
      '## 7 Acknowledgement\n' +
      '\n' +
      '이 자료는 협동조합 협정 HR00112390060에 따른 ECOLE 프로그램이 미국 국방첨단연구사업청(DARPA)과 지원하는 연구를 기반으로 한다. 또한 인간과 인공지능을 위한 UCLA-아마존 과학허브에 감사를 표합니다. Hritik Bansal은 부분적으로 AFOSR MURI 부여 FA9550-22-1-0380에 의해 지원된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '* [2] OpenAI. Gpt-4v(ision) system card, 2023b. [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card), 2023.\n' +
      '* [3] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* [4] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.\n' +
      '* [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.\n' +
      '* [6] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8317-8326, 2019.\n' +
      '* [7] Xinyu Wang, Yuliang Liu, Chunhua Shen, Chun Chet Ng, Canjie Luo, Lianwen Jin, Chee Seng Chan, Anton van den Hengel, and Liangwei Wang. On the general value of evidence, and bilingual scene-text visual question answering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10126-10135, 2020.\n' +
      '* [8] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. _arXiv preprint arXiv:2310.02255_, 2023.\n' +
      '* [9] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. _arXiv preprint arXiv:2311.16502_, 2023.\n' +
      '* [10] OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774, 2023a_, 2023.\n' +
      '* [11] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.\n' +
      '* [12] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.\n' +
      '* [13] HuggingFace. Introducing idefics: An open reproduction of state-of-the-art visual language model. [https://huggingface.co/blog/idefics](https://huggingface.co/blog/idefics), 2023.\n' +
      '* [14] Thibault Sellam, Dipanjan Das, and Ankur P Parikh. Bleurt: Learning robust metrics for text generation. _arXiv preprint arXiv:2004.04696_, 2020.\n' +
      '* [15] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schimdt. Visit-bench: A benchmark for vision-language instruction following inspired by real-world use. _arXiv preprint arXiv:2308.06595_, 2023.\n' +
      '* [16] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marcal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 4291-4301, 2019.\n' +
      '* [17] Jordy Van Landeghem, Ruben Tito, Lukasz Borchmann, Michal Pietruszka, Pawel Joziak, Rafal Powalski, Dawid Jurkiewicz, Mickael Coustaty, Bertrand Anckaert, Ernest Valveny, et al. Document understanding dataset and evaluation (dude). In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 19528-19540, 2023.\n' +
      '\n' +
      '* [18] Minesh Mathew, Viraj Bagal, Ruben Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 1697-1706, 2022.\n' +
      '* [19] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seedbench: Benchmarking multimodal llms with generative comprehension. _arXiv preprint arXiv:2307.16125_, 2023.\n' +
      '* [20] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.\n' +
      '* [21] Romain Beaumont. Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with them. [https://github.com/rom1504/clip-retrieval](https://github.com/rom1504/clip-retrieval), 2022.\n' +
      '* [22] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: A mobile app dataset for building data-driven design applications. In _Proceedings of the 30th annual ACM symposium on user interface software and technology_, pages 845-854, 2017.\n' +
      '* [23] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. [http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus), 2019.\n' +
      '* [24] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n' +
      '* [25] paddlepadle. Paddlecor: Multilingual ocr toolkit based on paddlepaddle. [https://github.com/Paddle/PaddleOCR](https://github.com/Paddle/PaddleOCR), 2023.\n' +
      '* [26] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. _arXiv preprint arXiv:2304.09842_, 2023.\n' +
      '* [27] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. _arXiv preprint arXiv:2303.04671_, 2023.\n' +
      '* [28] Didac Suris, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. _arXiv preprint arXiv:2303.08128_, 2023.\n' +
      '* [29] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14953-14962, 2023.\n' +
      '* [30] Wenjin Wang, Yunhao Li, Yixin Ou, and Yin Zhang. Layout and task aware instruction prompt for zero-shot document image question answering. _arXiv preprint arXiv:2306.00526_, 2023.\n' +
      '* [31] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6904-6913, 2017.\n' +
      '* [32] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. _arXiv preprint arXiv:2304.14178_, 2023.\n' +
      '* [33] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. _arXiv preprint arXiv:2311.04257_, 2023.\n' +
      '* [34] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. _arXiv preprint arXiv:2306.17107_, 2023.\n' +
      '\n' +
      '* [35] Wenbo Hu, Yifan Xu, Y Li, W Li, Z Chen, and Z Tu. Bliva: A simple multimodal llm for better handling of text-rich visual questions. _arXiv preprint arXiv:2308.09936_, 2023.\n' +
      '* [36] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pages 74-81, 2004.\n' +
      '* [37] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. _arXiv preprint arXiv:1904.09675_, 2019.\n' +
      '* [38] Yunxin Li, Longyue Wang, Baotian Hu, Xinyu Chen, Wanqi Zhong, Chenyang Lyu, and Min Zhang. A comprehensive evaluation of gpt-4v on knowledge-intensive visual question answering. _arXiv preprint arXiv:2311.07536_, 2023.\n' +
      '* [39] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. Unichart: A universal vision-language pretrained model for chart comprehension and reasoning. _arXiv preprint arXiv:2305.14761_, 2023.\n' +
      '* [40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.\n' +
      '* [41] Tianrui Guan, Fuxiao Liu, Xiyang Wu Ruiqi Xian Zongxia Li, Xiaoyu Liu Xijun Wang, Lichang Chen Furong Huang Yaser Yacoob, and Dinesh Manocha Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models. _arXiv e-prints_, pages arXiv-2310, 2023.\n' +
      '* [42] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In _International Conference on Machine Learning_, pages 18893-18912. PMLR, 2023.\n' +
      '* [43] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 1192-1200, 2020.\n' +
      '* [44] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In _2019 international conference on document analysis and recognition (ICDAR)_, pages 947-952. IEEE, 2019.\n' +
      '* [45] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pages 2200-2209, 2021.\n' +
      '* [46] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. _arXiv preprint arXiv:2203.10244_, 2022.\n' +
      '* [47] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. _arXiv preprint arXiv:2305.07895_, 2023.\n' +
      '* [48] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In _Proceedings of the IEEE international conference on computer vision_, pages 2425-2433, 2015.\n' +
      '* [49] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3608-3617, 2018.\n' +
      '\n' +
      '* [50] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 235-251. Springer, 2016.\n' +
      '* [51] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. _arXiv preprint arXiv:2308.01390_, 2023.\n' +
      '* [52] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '적용자 B.\n' +
      '\n' +
      '이 섹션에서는 발효 LLM 접근법에 사용된 프롬프트(그림 17, 18, 19)에 사용된 설계에 대해 논의하고 자세히 설명한다. 우리는 사용된 세 가지 뚜렷한 프롬프트 포맷을 설명하며, 각각은 제시된 시각적 정보의 정도가 다르다. 이러한 형태는 이미지의 단순한 OCR, 이미지 내에 나타나는 레이아웃에 배열된 이미지의 OCR, 그리고 포괄적인 이미지 캡션과 함께 레이아웃 형식으로 제시된 OCR을 포함한다. 우리는 이미지를 입력으로 하지 않는 위의 템플릿으로 GPT4를 프롬프트한다. 그러나, 이미지는 참조 목적을 위해 일러스트레이션에 포함된다.\n' +
      '\n' +
      '1.5.5.2.\n' +
      '\n' +
      '공연장.\n' +
      '\n' +
      '우리는 그림 20의 ConTextual 데이터셋 상의 인간 응답을 획득하기 위해 사용되는 사용자 인터페이스의 스크린샷을 제시한다.\n' +
      '\n' +
      '인도적 평가.\n' +
      '\n' +
      '우리는 그림 21에서 인간 평가에 사용되는 사용자 인터페이스의 스크린샷을 제시한다.\n' +
      '\n' +
      '그림 11: _Navigation_ 시각적 시나리오의 실시예 11:\n' +
      '\n' +
      '그림 12: _Abstract_ 시각적 시나리오의 실시예.\n' +
      '\n' +
      '도표 D GPT-4 평가\n' +
      '\n' +
      '적용자 E.\n' +
      '\n' +
      '태스크의 종류.\n' +
      '\n' +
      'GPT-4 평가를 이용하여 Fig. 1(a)의 다양한 유형의 과제와 재단 모델의 성능을 비교한다. 구체적으로, 명령어가 이미지 내의 텍스트 또는 시각적 요소를 추출하도록 할 때 응답의 품질을 평가한다(예를 들어, _List, 해당 삽화가 단일 움직임을 나타내는 운동._). 콘텍스트 데이터셋에는 \\(285\\)와 같은 인스턴스가 있다. 이러한 과제는 복잡한 인식과 추론 능력을 필요로 하지만, 이미지 내에서 이미 제시된 정보 위에 추가적인 동작이 명시적으로 요구되지 않는다. 우리는 인간이 이러한 지침에 대해 \\(75.1\\%\\)를 달성하는 반면 독점 모델 GPT-4V 및 Gemini-Pro-Vision은 각각 \\(38.7\\%\\) 및 \\(40.1\\%\\)를 달성한다는 것을 관찰했다. 이는 인간이 지침에 대응하기 위해 추출해야 하는 핵심 정보를 식별하는 데 매우 능숙함을 나타낸다.\n' +
      '\n' +
      '또한, 모델은 정보 추출을 넘어서야 할 때 응답을 평가하고, 명령어에 대한 수학 추론(예: 여기에 나열된 두 자동차의 총 가격은 얼마인가요?_)을 수행한다. 콘텍스트 데이터셋에는 \\(66\\) 인스턴스가 있습니다. 우리는 인간이 그러한 작업에 대해 \\(62.1\\%\\)를 달성하고 독점 모델 GPT-4V가 \\(27.3\\%\\)를 달성한다는 것을 발견하며 수학 추론의 큰 격차를 다시 강조한다.\n' +
      '\n' +
      '### Visual Scenes\n' +
      '\n' +
      '파운데이션 모델의 성능을 그림 1(b)에서 다양한 시각 장면(예: 자연 장면 및 디지털 장면)과 비교한다. 주로 쇼핑, 내비게이션, 오해가 있습니다. 자연스러운 장면은 자연 장면을 구성하고, 웹 사용, 모바일 사용, 추상, 인포그래픽 및 시간 판독은 디지털 장면을 구성한다. 우리는 인간이 자연 장면과 디지털 장면에서 각각 시각적 장면, 즉 \\(72.5\\%\\)와 \\(66.2\\%\\) 모두에서 가장 높은 성능을 달성한다는 것을 발견했다. 또한 GPT-4V가 자연 및 디지털 장면에서 각각 \\(47.8\\%\\) 및 \\(48.2\\%\\)를 달성한다는 것을 관찰했다. 흥미롭게도 우리는 Gemini-Pro-Vision과 ShareGPT-4V-7B가 Gemini-Pro-Vision 및 ShareGPT-4V-7B보다 자연 장면에서 더 높은 성능을 달성한다는 것을 발견했다.\n' +
      '\n' +
      '그림 14: _Web Usage_ 시각적 시나리오의 실시예이다.\n' +
      '\n' +
      '그림 13: _모바일 Usage_ 시각적 시나리오의 실시예.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:20]\n' +
      '\n' +
      '### Web Usage\n' +
      '\n' +
      '이 섹션은 그림 41, 42, 43, 44와 같이 모델 전반에 걸쳐 _Web Usage_ 시각적 시나리오 및 연구 결과에 대한 질적 분석을 제공한다.\n' +
      '\n' +
      '### Infographic\n' +
      '\n' +
      '이 섹션은 그림 45, 46, 47, 48과 같이 모델 전반에 걸쳐 _인포그래픽_ 시각적 시나리오와 우리의 발견에 대한 질적 분석을 제공한다.\n' +
      '\n' +
      '그림 17: GPT4: OCR을 배치 없이 제공한 8월 LLM에 대한 실시예 프롬프트: GPT4: OCR은 배치에 대해 알고 있는 배치에 대해 그림 17: 레이아웃 없이 제공합니다:\n' +
      '\n' +
      '미소라.\n' +
      '\n' +
      '이 섹션은 그림 49, 50, 51, 52와 같이 모델 전반에 걸쳐 _Miscellwear 천연 Scenes_ 시각적 시나리오와 우리의 연구 결과에 대한 질적 분석을 제공한다.\n' +
      '\n' +
      '그림 18: GPT4: OCR을 레이아웃에서 사용한 8월 LLM에 대한 실시예 프롬프트는 배치 그림 21: 인간 평가의 사용자 인터페이스이다.\n' +
      '\n' +
      '그림 20: 인간 응답 모음의 사용자 인터페이스.\n' +
      '\n' +
      '명령*** 명령:**는 거실과 엉덩이 거실 사이에 가장 유사한 제품을 설명합니다.\n' +
      '\n' +
      '** 참조 대응:** 오랑기-갈색 유색 단종 소파는 리빙룸과 헤프 리빙룸 간에 가장 유사하다.\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      '당신은 이미지에 맞는 지시에 응답하는 전문가 언어 모델인 OCR 이미지옵션GPT입니다. 이미지의 OCR 텍스트와 이미지 에디션을 사용하여 지시에 응답해야 합니다. 보다 구체적으로, 당신은 다음을 받을 것입니다.\n' +
      '\n' +
      '1. An 명령어: 이것은 반응이 필요한 이미지에 대한 질문, 필수 요청 또는 유사한 사항이다.\n' +
      '\n' +
      '2. Layout Aware OCR Text: 이미지에서 추출되어 이미지 내에 나타나는 바와 같이 레이아웃에 배열되는 텍스트는 이미지 내에 나타나는 텍스트들 중 상대적인 위치가 유지되도록 한다.\n' +
      '\n' +
      '3. 이미지 캡션: 이미지의 내용에 대한 자연 언어 설명, 객체, 텍스트 등, 서로의 상호작용 등.\n' +
      '\n' +
      '정답만 하면 됩니다.\n' +
      '\n' +
      '**NOW YOUR TURN:**\n' +
      '\n' +
      '명령 : 이미지에 묘사된 기계 타입 채용 비용을 9개월 동안 제공하세요.\n' +
      '\n' +
      'OCR Text:\n' +
      '\n' +
      'Bedroom\n' +
      '\n' +
      '99달러부터 월 월 $99 / $99 / $\n' +
      '\n' +
      '5가지 품목입니다.\n' +
      '\n' +
      'Living Room\n' +
      '\n' +
      'S59/월\n' +
      '\n' +
      '4가지 품목입니다.\n' +
      '\n' +
      '이미지 캡션: 이미지는 가구점을 위한 광고로 다양한 가구 품목을 보여줍니다. 품목은 격자와 같은 패턴으로 배열되며, 각 항목에는 자체 사각형이 있다. 가구에는 소파, 식탁, 의자, 드레스, 침대 및 캐비닛이 포함되어 있습니다. 가구의 색상은 갈색에서 베이지까지 다양하며 일부 품목은 흑백 악센트가 특징입니다. 이미지의 텍스트는 295~1945년 범위의 가격으로 가구를 구매할 수 있음을 나타내며, 텍스트는 또한 가구가 "LUXURY FURNITURE DESIGN"이라고 언급한다. 광고의 전체 레이아웃과 디자인은 고품질 스타일리시한 가구에 초점을 맞출 것을 제안한다.\n' +
      '\n' +
      'Answer:\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡션 대응: 리빙룸과 헤프리빙룸 사이의 가장 유사한 제품은 품목 수량에 해당한다. 두 거실 옵션 모두 소파, 커피 테이블, 사이드 테이블 각각 4개의 항목을 포함한다. 스타일과 색상은 다르지만 가구들의 상대적 포지셔닝과 종류도 비슷해 보인다.**.\n' +
      '\n' +
      '그림 19: 레이아웃에서 GPT4: OCR이 있는 8월 LLM에 대한 실시예 프롬프트는 배열 + 이미지 활용도 23: 이 태스크에서 _GPT-4V_ 및 _Gemini Pro-Vision_ **c 정확하게 지시에 응답한다. 이는 시각적 인식, 텍스트 인식, 맥락에 민감한 텍스트 구현 이해에 대한 능력에 기인할 수 있다. _ShareGPT4V_ 및 _LLaVA-V-1.5_ 및 _GPT-4 w/레이아웃 인식 OCR+캡션 반응_(증강 LLM 접근법)와 같은 오픈 소스 LMM은 **wrong** 대답을 예측한다.\n' +
      '\n' +
      '그림 22: 무조건 미세 편성 평가 결과.\n' +
      '\n' +
      '***그림 24**: 이 작업에서는 폐쇄 소스 LMM_GPT-4V_ 및 _Gemini Pro-Vision_****가 지시에 응답한다. 이는 시각적 인식, 텍스트 인식, 맥락에 민감한 텍스트-비전 이해의 능력 _에 기인한다고 할 수 있다. GPT-4V_는 또한 응답의 이면에 대한 추론을 올바르게 설명할 수 있다. 오픈 소스 LMM 중 _LLAVA-1.5_**c 정확하게****는 지시에 응답하여 이 경우 컨텍스트에 민감한 텍스트-비전 추론을 수행할 수 있는 능력을 시사한다. 다른 한편으로 _ShareGPT4V_ 및 _GPT-4 w/레이아웃 인식 OCR+캡션 반응_(증강 LLM 접근법)은 **wrong** 대답을 예측한다.\n' +
      '\n' +
      '** 명령:**는 달력의 다른 숫자에 대한 색상 방식을 사용하여 뒤에 있는 아이디어를 설명합니다.\n' +
      '\n' +
      '** 참조 대응:** 달력은 현재 날짜 전 며칠 동안 회색, 현재 날의 노란색 하이라이트, 나머지 일 동안의 파란색 등 세 가지 다른 색상을 사용한다.\n' +
      '\n' +
      '** 인간 반응:** 회색 숫자는 이미 통과한 날짜입니다. 오늘 날짜로 표시된 노란색입니다. 내일 표시된 파란색 색상과 향후 날짜입니다.\n' +
      '\n' +
      '예를 들어,***GPT-4V 반응:*** 캘린더에서 색상 방식의 사용은 사용자가 상이한 유형의 일 또는 이벤트를 빠르게 구별하는 데 도움이 될 수 있다.\n' +
      '\n' +
      '일요일 대. 주간일: 표준 달력은 평일과 주말을 구별하기 위해 색상을 사용하는 경우가 많은데 토요일과 일요일은 월요일부터 금요일까지 눈에 띄기 위해 간혹 색을 띠기도 한다.\n' +
      '\n' +
      '**의 날:**의 현재 날짜는 10번이 주황색인 영상에서 볼 수 있듯이 독특한 색상으로 부각될 수 있다. 이를 통해 사용자는 검색 없이 현재 하루를 빠르게 식별하는 데 도움이 됩니다.\n' +
      '\n' +
      '** 과거, 제시 및 미래 연결: 일부 달력은 색상 구배를 사용하거나 다른 톤을 사용하여 과거, 현재 및 미래 날짜를 구별한다.\n' +
      '\n' +
      '부분적으로 응답을 수정하지만 이 이미지에 근거하지는 않았다.\n' +
      '\n' +
      '** 게미니 프로비전 대응:** 달력에 사용된 색상 방식은 현재 날을 강조하는 것이다. 현재 날이 노란색으로 부각되어 쉽게 식별할 수 있습니다.\n' +
      '\n' +
      '정답.\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응:** 달력의 다른 숫자에 대한 색상 방식의 사용은 몇 가지 목적을 제공한다. 먼저 달력에 시각적인 관심과 심미적 매력을 더해 시각적으로 더 쉽게 읽을 수 있도록 한다. 색상 코딩된 숫자는 또한 전통적인 검정색 또는 회색 수보다 뚜렷한 색상이 더 눈에 띄고 기억에 남을 수 있기 때문에 사용자가 특정 날짜 또는 이벤트를 빠르게 식별하는 데 도움이 될 수 있다.\n' +
      '\n' +
      '이 이미지에 근거하지 않은 응답입니다.\n' +
      '\n' +
      '**LLaVA-1.5-13B 반응:** 달력의 다른 숫자에 대한 색상 방식을 사용하는 이면의 아이디어는 사용자가 특정 날짜 또는 이벤트를 식별 및 구별하기 쉽게 하는 것이다. 이미지에서 10번은 노란색으로 부각되어 그 날의 주목을 받고 있다.\n' +
      '\n' +
      '정정 응답.\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡션 대응: 캘린더에서 서로 다른 숫자에 대한 색상 방식을 사용하는 이면의 아이디어는 제시된 정보의 유형을 명확하게 구별하는 것이다. 이 경우, 청색은 주중일, 검정색은 날짜로 사용한다. 이렇게 하면 사용자는 이 두 가지 유형의 정보를 쉽게 구별하여 가독성과 이해를 높일 수 있다.**\n' +
      '\n' +
      '이 이미지에 근거하지 않은 응답입니다.\n' +
      '\n' +
      '그림 25: 이 작업에서는 모든 모델이 이미지**에 대한 응답을 인용하지 못하며, 가장 좋은 수행 모델은 _GPT4V, Gemini-Vision-Pro & LLaVA-1.5-13B_ 및 _ShareGPT4V & GPT-4 w/Layout 인식 OCR + Caption_이다.\n' +
      '\n' +
      '** 명령:***는 시간 및 분침까지 시간이 얼마나 유지됩니까?\n' +
      '\n' +
      '** 참조 반응:** 시간 및 분침은 묘사된 시간으로부터 5분, 12시에 정렬됩니다.\n' +
      '\n' +
      '인간 반응: 5분:***** 인간 반응.\n' +
      '\n' +
      '***GPT-4V 대응:**에 답하려면 시간 및 분기의 상대 속도를 계산하고 다음에 언제 정렬될지 결정해야 합니다.\n' +
      '\n' +
      '시계의 시침은 12시간마다 풀 사이클을 완료하므로 시간당 30도인 12시간당 360도의 속도로 이동한다. 분손은 60분마다 풀 사이클을 완성하므로 60분당 360도 속도로 움직이며, 분당 6도이다.\n' +
      '\n' +
      '10시에 시침은 300도 표시이고, 분침은 0도 표시입니다. 분손은 한 시간 손으로 "올림"해야 합니다.\n' +
      '\n' +
      '시간마다 시침이 30도 움직이며, 분손이 360도를 움직인다. 상대속도는 속도의 차이이므로 1분손은 시속 360∼30∼330도, 분당 5.5도(1시간 60분)로 시침까지 잡는다. 10시 위치에서 미세한 손이 얼마나 걸려야 하는지 알아봐야 한다. 10시에 두 손 사이에는 300~0 = 300도 차이가 있습니다.\n' +
      '\n' +
      '그 작업을 이미지에 근거하지 않습니다. 복잡하게\n' +
      '\n' +
      '** 게미니 프로비전 대응:** 분침은 분당 6도 움직이며, 시침은 분당 0.5도 움직인다. 그래서 1분의 손이 움직입니다.\n' +
      '\n' +
      '그 작업을 이미지에 근거하지 않습니다. 복잡하게\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응:** 클럭의 시침은 12를 가리키고 있고, 분침은 3을 가리키고 있어 시간, 분침까지 시간이 얼마나 유지되는지 판단하여 손이 정렬되는 시점과 시간의 차이를 계산해야 한다.\n' +
      '\n' +
      '현재 시간: 12:03: 현재 시간: 12:03: 현재 시간:: 현재 시간:: 현재 시간:: 12:03: 12:03:: 현재 시간:: 현재 시간:: 12:03:: 12:03:: 현재 시간::: 12:03:: 현재 시간:::: 12:03:: 현재 시간:::: 12:03:: 현재 시간:::: 12::: 12::: 12:::: 12:: 12::::: 12:: 12::: 12::: 12:: 12::::: 12:::::: 12::::::: 12::::::::::: 12::::::::::::::::::: 12:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n' +
      '\n' +
      '핸드 위치: 12:12.\n' +
      '\n' +
      '손 위치: 3*****\n' +
      '\n' +
      '손이 정렬되는 시간을 찾기 위해 현재 시간에 분수를 추가해야 합니다. 이 경우, 손 정렬 시점이 12:03 + 3 = 12:06일 것이다.\n' +
      '\n' +
      '그래서 1시간 1분 손 정렬까지 3분 남았습니다.\n' +
      '\n' +
      '희생하고 잘못된 추리.\n' +
      '\n' +
      '**LLaVA-1.5-13B 반응:** 시계의 시침은 12이고, 분침은 10**이다. 시간 및 미세한 손이 정렬될 때까지 남은 시간을 결정하기 위해서는 현재 시간과 다음 12의 다수의 차이를 찾아야 하며, 12의 시간 손은 12의 증분으로 움직이기 때문에 다음 12의 배수는 1시에 있을 것이다. 현재 시간(12:10)과 다음 12(1:00)의 시차는 1시간 50분이다. 시간 및 분침 정렬까지 1시간 50분이 남았다는 의미다.\n' +
      '\n' +
      '희생하고 잘못된 추리.\n' +
      '\n' +
      '30분*****GPT-4 w/레이아웃 인식 OCR+ 활용 반응\n' +
      '\n' +
      'Wrong response\n' +
      '\n' +
      '그림 26: 이 태스크에서 닫힌 소스 LMM, _GPT4V & Gemini-Vision-Pro_는 **이 이 이미지**에 과제를 인용하지 못한다. 반면, 오픈 소스 LMM, _LLaVA-1.5-13B & ShareGPT4V-7B_는 **hallucination과 잘못된 추론***를 앓고 있다. 마지막으로, _GPT-4 w/레이아웃 인식 OCR+캡션 반응_(증강 LLM 접근법)은 **wrong*** 반응을 생성한다.\n' +
      '\n' +
      '** 참조 대응:** 이미지에 표시된 기계는 전면 적재 기계이며, 9개월 동안의 임대료는 INR 7200에 해당한다.\n' +
      '\n' +
      '** 인간 반응:** 프론트 로드 기계는 9개월 동안 7200입니다.\n' +
      '\n' +
      '***GPT-4V 대응:** 영상은 다양한 종류의 세탁기를 채용하기 위한 가격표와 함께 전면 부하 세탁기를 표시한다. 9개월 동안의 전면 부하 세탁기 채용은 주어진 가격표에 따라 비용은 INR 7,200입니다.\n' +
      '\n' +
      '*** 게미니 프로비전 대응:*** 게미니 프로비전 대응:*** 9개월 동안 이미지에 묘사된 기계를 고용하는 비용은 **INR 4500입니다.\n' +
      '\n' +
      '컨텍스트에 민감한 텍스트-동작 추론은 부족한다.\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응:** 9개월간 세탁기 채용 비용은 **4500 루피스***입니다.\n' +
      '\n' +
      '컨텍스트에 민감한 텍스트-동작 추론은 부족한다.\n' +
      '\n' +
      '**LLaVA-1.5-13B 반응:** 9개월 동안 이미지에 묘사된 기계 타입 채용 비용은 **60,000 루피스***입니다.\n' +
      '\n' +
      'Hallucination\n' +
      '\n' +
      '9개월 동안 묘사된 기계 유형(대기 도로)을 채용하는 비용은 **4500 단위(제공된 텍스트에서 명시되지 않은 화폐)***************GPT-4 w/레이아웃 인식 OCR+소비 대응이다.\n' +
      '\n' +
      '그림 27: 이 작업에서 _GPT4V_와 별도로 다른 모든 모델은 잘못된 반응을 생성하지 못한다. 이는 GPT4V의 **강한 미세 구성 시각적 인식과 맥락에 민감한 텍스트 실행 추론 능력**에 기인할 수 있다.\n' +
      '\n' +
      '명령*** 명령:**는 거실과 엉덩이 거실 사이에 가장 유사한 제품을 설명합니다.\n' +
      '\n' +
      '** 참조 대응:** 오랑기-갈색 유색 단종 소파는 리빙룸과 헤프 리빙룸 간에 가장 유사하다.\n' +
      '\n' +
      '** 인간 반응:** 두 거실은 오렌지 의자가 비슷합니다.\n' +
      '\n' +
      '***GPT-4V 반응:** "살아있는 방" 세트와 "히프 리빙 방" 세트 사이의 가장 유사한 제품은 **암미터인 것으로 판단된다. 두 세트 모두 유사한 스타일과 디자인을 가진 단일 팔장을 특징으로 하며, 한 세기 중반 근대 미학을 특징으로 하며, 비좁은 다리와 각진 등받이가 있다. \'살아있는 방\' 세트에서는 팔장이 주황색을 띠는 반면, \'히프 리빙방\' 세트에서는 팔장이 비슷한 구조를 가지고 있지만 다른 색상 방식으로 나옵니다. 두 의자는 현대 내부 디자인 내에서 잘 맞는 모던하고 매끄러운 룩을 공유합니다.\n' +
      '\n' +
      '** 게미니 프로비전 대응:** 거실과 엉덩이 거실 ** 소파 사이에 가장 유사한 제품입니다. 두 소파는 모두 파란색이고 모양도 비슷하고.**도 비슷합니다.\n' +
      '\n' +
      '미세곡물 시각지각 부족********, 미세곡물 시각지각 부족******\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응:** 거실과 엉덩이 거실 사이의 가장 유사한 제품은 **couch이다. 거실과 힙 거실 커플 모두 편안함을 제공하고 방의 초점 역할을 할 수 있도록 설계되었습니다.****\n' +
      '\n' +
      '미세곡물 시각지각 부족********, 미세곡물 시각지각 부족******\n' +
      '\n' +
      '**LLAVA-1.5-13B 반응:** 거실과 고관절 거실 사이의 가장 유사한 제품은 **couch이다. 두 거실 모두 소파를 중심 가구로서 특징으로 하여 휴식과 사회화를 위한 편안한 좌석 공간을 제공합니다. 두 방의 커플은 디자인이 비슷하고 같은 목적을 가지고 있어 두 생활 공간 사이에서 가장 비슷한 품목이 되도록 하였다.**.\n' +
      '\n' +
      '미세곡물 시각지각 부족********, 미세곡물 시각지각 부족******\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡션 대응:** 리빙룸과 헤프리빙룸 사이의 가장 유사한 제품은 품목 수량에 해당한다. ** 두 거실 옵션 모두 소파, 커피 테이블 및 사이드 테이블 각각에 4개의 항목을 포함한다. 스타일과 색상은 다르지만 가구들의 상대적 포지셔닝과 종류도 비슷해 보인다.**.\n' +
      '\n' +
      '동시 문자동작 추론*****의 부족.\n' +
      '\n' +
      '그림 28: 이 작업에서 _GPT4V_와 별도로 다른 모든 모델은 잘못된 반응을 생성한다. 이는 GPT4V의 **강한 미세 구성 시각적 인식과 맥락에 민감한 텍스트 실행 추론 능력**에 기인할 수 있다. 또한, _GPT-4 w/레이아웃 인식 OCR+캡션 대응_(증강 LLM 접근법)은 제공된 조밀한 이미지 정보를 텍스트로 하여 올바르게 이유할 수 없음을 보여 관절 비전 및 텍스트 추론의 필요성을 강조한다.\n' +
      '\n' +
      '** 참조 대응:** 그림에는 4명의 잠재적 관광객, 1명의 관광 가이드, 1명의 버스 드라이버가 그려져 있다. 여행 가이드가 1인당 2달러에서 5달러 사이인 것을 감안하면 4명의 관광객에서 얻을 수 있는 가장 높은 금액은 20달러다.\n' +
      '\n' +
      '** 인간 반응:**에는 이미지에 표시된 관광객 4명이 있다. 5 x 4 = 20달러예요.\n' +
      '\n' +
      '***GPT-4V 대응:**을 계산하여 여행 가이드가 이 이미지에서 보여지는 관광객으로부터 하루 최대 금액을 계산할 수 있으며, 관광객 수를 세고 1인당 하루 최대 5달러까지 곱해야 한다. 이미지에**는 5명의 관광객이 보인다. 따라서 계산은 5명의 관광객 x $5/관광자/일 = $25/일이다. 따라서, 여행 가이드가 하루 이 이미지에서 보여지는 관광객들로부터 만들 수 있는 최대 금액은 25달러입니다.\n' +
      '\n' +
      '미세곡물 시각지각 부족********, 미세곡물 시각지각 부족******\n' +
      '\n' +
      '** Gemini Pro-Vision대응:** 영상은 5명의 관광객과 여행 가이드 비용은 1인당 하루 2-5달러이다. 그래서 여행 가이드를 만들 수 있는 최대 금액은 하루 $25입니다.\n' +
      '\n' +
      '초과한 시각적 인식이 부족합니다.\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응:** 투어 가이드는 이미지에 표시된 관광객들로부터 하루 최대 25달러를 만들 수 있다. **는 이미지가 옆에 서 있는 5명의 관광객을 보여주고 있고, 관광 가이드가 주황색 깃발을 들고 있기 때문이다. 여행 가이드의 최대 수익은 그룹 내 5명의 관광객이 있는 것을 고려할 때 1인당 하루 25달러이다.\n' +
      '\n' +
      '초과한 시각적 인식이 부족합니다.\n' +
      '\n' +
      '***LLaVA-1.5-13B 대응:**는 관광 가이드가 관광객으로부터 만들 수 있는 최대 비용을 계산하려면 관광객 수와 1인당 비용을 결정해야 한다. 이미지에**는 버스 앞에 5명이 서 있고, 투어 가이드는 1인당 하루 25달러 5달러를 제공하고 있다. 5명이므로 여행 가이드가 할 수 있는 최대 금액은 하루 25 x 5 =125달러입니다.\n' +
      '\n' +
      '초과한 시각적 인식이 부족합니다.\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡션 대응:** 이 이미지에서 보여지는 5명의 관광객으로부터 여행 가이드가 만들 수 있는 최대 돈은 하루 25달러(1인당 5달러)이다.\n' +
      '\n' +
      '초과한 시각적 인식이 부족합니다.\n' +
      '\n' +
      '그림 29: 이 과제에서 모든 모델은 잘못된 반응을 생성한다. 이것은 이 작업에 대한 **미세-곡물 시각적 인식***의 부족으로 인한 것일 수 있다.\n' +
      '\n' +
      '명령어*** 명령:**는 한 손으로 수직으로 카메라의 기능을 논의합니다.\n' +
      '\n' +
      '** 참조 대응:** 콘니카 미니올타는 한 손으로 수직으로 열리는 카메라, 특히 빅 미니 BM-201 모델로 가격은 130달러,113달러,102달러 또는 IDR 1,893,775달러입니다.\n' +
      '\n' +
      '** 인간 반응:** 세로로 한 손으로 들고 있는 카메라는 빅 미니이며 가격은 130달러입니다.\n' +
      '\n' +
      '**GPT-4V 반응:** 영상은 오른쪽 상단 모서리에 세로로 하나의 카메라를 들고 있는 사람과 다양한 카메라의 콜라주로 보인다. 수직적으로 유지된 카메라에 초점을 맞추고 있기 때문에, 여기에 하나의 손으로 수직 방향으로 카메라를 사용하기 위한 몇 가지 일반적인 특징과 고려 사항이 있습니다.\n' +
      '\n' +
      '** 게미니 프로비전 대응:** 한 손으로 수직으로 열리는 카메라는 후지필림 인맥스 미니 7s입니다. 완벽한 컴팩트하고 가벼운 즉석 카메라입니다.\n' +
      '\n' +
      'Wrong response\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응:** 한 손으로 수직으로 열리는 카메라는 칸톤 EOS 5D 마크 IV입니다. 이 카메라는 알려져 있습니다.\n' +
      '\n' +
      'Hallucination\n' +
      '\n' +
      '**LLAVA-1.5-13B 반응:** 싱글 핸드로 수직으로 진행 중인 카메라는 전형적으로 컴팩트하거나 점 및 소호트 카메라입니다.\n' +
      '\n' +
      'Hallucination\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡션 대응:** 영상은 한 손으로 수직으로 들고 있는 카메라의 특징을 특정하지 않는다. 그러나 이 사용을 위해 설계된 카메라는 일반적으로 가볍고 컴팩트할 것입니다. 합작 텍스트-동작 추론 및 필요\n' +
      '\n' +
      '그림 30: 이 작업에서 _Gemini-Vision-Pro_, **hallucinate**를 제외한 모든 모델이 있다. 모두 이 과제를 해결하기 위해 **의 시각적 지식 부족**를 보여준다.\n' +
      '\n' +
      '명령*** 명령:**는 삼각형의 징후를 감지합니다.\n' +
      '\n' +
      '** 참조 대응:** 트랑가 모양의 징후는 바른 큐브, 레프트 커브, 크로스 로드, 스피드 브릭스, 학교입니다.\n' +
      '\n' +
      '** 인간 반응:** 삼각형의 징후는 오른쪽 큐브, 레프트 커브, 크로스 로드, 스피드 브릭스이며, 왼쪽에서 오른쪽까지, 상방으로 한다. 두 번째 행, 마지막 이미지는 삼각이며 학교를 위한 것입니다.\n' +
      '\n' +
      '**GPT-4V 대응:** 화상의 삼각형의 징후는 1과 같으며, 1행의 두 번째 표지는 "RIGHT CURVE. 2"의 아이콘으로 상단 줄의 세 번째 부호는 "SCHOS ROAD"를 나타내며, 세 번째 부호는 두 개의 행에 "RIGHT CURVE. 3"을 나타낸다.\n' +
      '\n' +
      '*** 게미니 Pro-Vision대응:***는 **6** 삼각형의 징후가 있는데, 1. 우곡선 2. 4. 점수곡선 3. 크로스 도로 4. 스피드 차단기는 부분적으로 정확한 반응과 환각이다.\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응:** 영상에서 삼각형 모양의 징후는 "노 파킹" 표지판과 "노 엔트리" 표지판이다. 잘못된 대응과 함께 해악.\n' +
      '\n' +
      '영상 속 삼각형의 징후는 경고 표지이며, 이는 일반적으로 운전자와 보행자에게 중요한 정보나 지시를 전달하는 데 사용된다...** 한라시핑...\n' +
      '\n' +
      '이미지의 삼각형의 징후는 정확한 반응과 함께 **1**GPT-4 w/Layout OCR + Caption 응답: LNTRY 12. SCHOER AHEAD.\n' +
      '\n' +
      '그림 31: 이 작업 _GPT4V_는 **c 보정*** 반응을 생성하며, _Gemini-Vision-Pro_는 마이너 환각****로 **부분 정답을 생성한다. 다른 모든 모델에서 모델 **hallucinates**는 잘못된 답변 또는 둘 다를 생성한다.\n' +
      '\n' +
      '** 참조 대응:** 왼쪽 차선에서 주행할 때 방향 및 출구의 옵션에는 각각 남역 또는 북콘코드(NH 온 국도 93호선)로 이어지는 국도 93호선 또는 출구 24 A-B에서 남 퀘시 24C 출구가 포함된다.\n' +
      '\n' +
      '*** 인간 반응:** 24A-B S. 북 93, 콘코드 NH 재생, 북 93, 콘코드 NH 재생, 북 93.\n' +
      '\n' +
      '***GPT-4V 대응:**에서 왼쪽으로 주행하고 이미지 속 사이니지를 기반으로 잠재적 목적지를 고려하고 출항하는 경우 여기 있습니다.\n' +
      '\n' +
      '24C: 목적지가 퀘시라면 이 출구를 가시면 "EXIT ONLY" 차선이 됩니다.\n' +
      '\n' +
      '이 출: 24A 남역, 24B북 93, 콘코드 NH-B: 24A-B.\n' +
      '\n' +
      '** 게미니 Pro-비전 반응:** 티이크 엑킷 24C는 퀘시입니다. NH 코코드를 위해 남역과 북을 위해 24A-B번 출구를 하세요.\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응:** While 왼쪽에는 몇 가지 목적지가 있고 고려해야 할 출구가 있다.\n' +
      '\n' +
      '**에는 퀘시, 사우스 보스턴 로건 공항, 콘코드 NH, S역이 포함됩니다. 또한 24A, 24B, 93, 90의 출구가 있으며, 이를 통해 운전자에게 다양한 위치에 도달하여 도시를 효과적으로 탐색할 수 있는 기회를 제공한다. 유감해.\n' +
      '\n' +
      '**LLaVA-1.5-13B 대응: 왼쪽에 주행하는 동안 몇 가지 목적지가 있으며 드라이버가 가져갈 수 있습니다. 여기에는 엑킷 24A-B: 이 출구는 남역으로 향하는 드라이버, 출구 24C: 이 출구는 퀘시, 출구 24D로 향하는 드라이버, 이 출구는 사우스 보스턴으로 향하는 드라이버, 출구 24F, 이 출구는 로건 공항으로 향하는 드라이버를 위한 것이다. 이들 출구는 고속도로 육교 아래에 위치하며, 운전자는 원하는 목적지를 기준으로 적절한 출구를 선택할 수 있다. 허허비****\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡션 대응: 왼쪽에서 주행 중 취할 수 있는 목적지와 출구는 남 93, 콘코드 NH, 남보스턴 로건 공항이다. 허허비****\n' +
      '\n' +
      '그림 32: 이 작업에서 폐쇄 소스 LMM _GPT-4V_ 및 _Gemini Pro-Vision_****가 지시에 응답한다. 이는 **시각적 인식, 텍스트 인식, 맥락에 민감한 텍스트-비전 이해**에 대한 능력에 기인할 수 있다. 다른 모든 모델 **힐루시네이트**의 반응.\n' +
      '\n' +
      '** 명령:**는 로켓, 우주 셔틀, 외부 탱크의 색상을 분류합니다.\n' +
      '\n' +
      '** 참조 대응:** 공간 셔틀은 노란색으로 묘사되고 로켓은 회색으로 묘사되며 외부 탱크는 빨간색으로 묘사된다.\n' +
      '\n' +
      '** 인간 반응:** 로켓은 회색입니다. 공간 셔틀은 노란색입니다. 외부 탱크는 빨간색입니다.\n' +
      '\n' +
      '***GPT-4V 대응:** 영상에서 **스페이스 셔틀의 고체 로켓 부스트기는 빨간색과 가벼운 회색으로 색칠하며, 위쪽에 빨간색과 아래쪽에 회색이 있다. 외부 탱크는 오렌지색입니다. 궤도자로도 알려진 우주셔틀의 본체는 주로 코와 날개 가장자리에 검은색이 있는 흰색이다. 꼬리 섹션에도 약간 빨간색이 있습니다.** 영상은 사전 런치부터 착륙까지 우주 셔틀 비행 시퀀스의 다양한 단계를 묘사합니다.\n' +
      '\n' +
      'Hallucinations\n' +
      '\n' +
      '** 게미니 프로비전 대응:** 로켓은 빨간색과 흰색, 우주 셔틀은 흰색, 외부 탱크는 주황색이다. 유감해.\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응:*** **rocket는 적색**이고 공간 셔틀은 노란색이며 외부 탱크는 주황색이다. 후당에 대한 반응을 정확하게 설정합니다.\n' +
      '\n' +
      '**LLaVA-1.5-13B 반응:*** **rocket는 적색**이고 공간 셔틀은 노란색이며 외부 탱크는 흰색이다. 후당에 대한 반응을 정확하게 설정합니다.\n' +
      '\n' +
      '**GPT-4 w/레이아웃 인식 OCR+캡션 대응:***로켓은 빨간색과 노란색입니다. 공간 셔틀은 노란색이고 빨간색입니다. 이미지는 외부 탱크의 색상을 명시하지 않았다. 유감해.\n' +
      '\n' +
      '그림 33: 이 과제에서 모든 모델 **hallucinate**는 주어진 이미지 및 미세 편성된 시각적 인식에 대한 응답을 접지하고 사전 시각적 지식을 사용하여 설정되지 않기 때문이다. *****, 개방형 소스 LMM, _LLAVA-1.5-13B_ & _ShareGPT-4V-7B_는 부분적으로 정확한 반응을 제공하여 이전에 시각적 지식에 대한 의존도가 감소했음을 시사하며, 이는 아마도 보지 않았기 때문일 수 있다.\n' +
      '\n' +
      '** 명령:** 피크는 노랑과 파랗게 날아가는 새가 등장하는 항공사를 바로 앞뒤로 아웃시킨다.\n' +
      '\n' +
      '** 참조 대응:** 새 로고가 있는 항공사 바로 앞에 있는 항공사는 스리랑카 항공이고 그 뒤에 위치한 항공은 에어 프랑스입니다.\n' +
      '\n' +
      '** 인간 대응:** 에어프랑스와 스리랑카입니다.\n' +
      '\n' +
      '\'루프산사\' 대응, 즉 스리랑칸항공기 바로 뒤편에는 \'루파산사\'의 간신함이 특징이며, 다크파란 꼬리가 날아가는 비행기, \'콘도르항공\'은 \'SriLankan 항공\'이다.\n' +
      '\n' +
      '다진 시각 인식*****\n' +
      '\n' +
      '전방: 스리랑카*** 게미니 프로-비전 대응: 스리랑카.\n' +
      '\n' +
      '미세 편성 시각 인식****\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응: 비행 새가 있는 항공사 앞에 있는 항공사는 콘도르와 스리랑칸이다. 날아오는 새가 있는 항공사의 뒤에 있는 항공사는 루프트산사****입니다.\n' +
      '\n' +
      '미세 편성 시각 인식****\n' +
      '\n' +
      '**LLaVA-1.5-13B 대응: 이미지를 보면 비행새 로고가 있는 항공사는 스리랑카 항공이다. 앞의 항공사는 컨도르이고, 그 뒤의 항공사는 에어프랑스***이다.\n' +
      '\n' +
      '미세 편성 시각 인식****\n' +
      '\n' +
      '****GPT-4 w/레이아웃 인식 OCR+캡션 대응: 비행 새인 콘도어가 등장하는 바로 앞뒤의 항공사는 각각 Srilankan과 Lufthansa이다.\n' +
      '\n' +
      '미세 편성 시각 인식****\n' +
      '\n' +
      '그림 34: 이 작업에서 _all 모델_은 **wrong*** 반응을 생성하며, 이는 **-곡물 시각 인식***의 부족 때문이다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:37]\n' +
      '\n' +
      '** 참조 대응:** 이 4판 코믹에서 변호사(세날)는 피고(킬러 고래)에게 자신을 확인하라고 요구하지만 고래가 "킬러"라고 인정하는 것은 범행을 고백하는 것을 의미하기 때문에 주저한다. 최종 패널에서 국방변호사(돌핀)는 "이 침몰을 통해 유머를 불러일으키는 대상"을 흥분시킨다.\n' +
      '\n' +
      '** 인간 반응:** 스트립에 묘사된 고래는 "킬러" 휘일이다.\n' +
      '\n' +
      '**GPT-4V 대응: 이 만화 스트립은 인류형주의, 등장인물은 인간의 옷을 입은 동물이며 본질적으로 원뿔적인 법원 재판에 참여한다는 몇 가지 이유로 특히 웃긴 것으로 간주될 수 있다. 시각적 유머: 동물의 표현과 신체 언어는 인간의 궁중 드라마를 모방하기 때문에 원만한 효과를 더한다. 아이러니: 고래가 분명히 말할 수 없기 때문에 유머러스하고 아이러니하게도, 법률 절차에 혼자 참여할 수 없기 때문에 모든 상황이 터무니없이 웃기게 만든다.\n' +
      '\n' +
      '컨텍스트에 민감한 텍스트-동작 추론은 부족한다.\n' +
      '\n' +
      '*** 게미니 프로비전 대응: 만화 스트립은 고래가 종종 지능적이고 관절적인 생명체로 볼 수 있다는 사실에 활약하기 때문에 웃기지만, 이 경우 고래는 그렇게 할 수 없다.\n' +
      '\n' +
      '컨텍스트에 민감한 텍스트-동작 추론은 부족한다.\n' +
      '\n' +
      '**ShareGPT-4V-7B 대응: 시청자는 이 만화 스트립이 유머러스하고 예상치 못한 시나리오가 특징입니다. 만화 스트립에는 한 사람과 씰이 대화로 묘사되어 있으며, 인장은 의자에 앉아 잠깐을 들고 있다. 인장을 양복을 입고 잠깐을 들고 있는 만화 스트립의 예상치 못한 반전은 시청자가 즐겁게 찾는 유머와 유머감을 자아낸다.\n' +
      '\n' +
      '컨텍스트에 민감한 텍스트-동작 추론은 부족한다.\n' +
      '\n' +
      '**LLaVA-1.5-13B 반응: 시청자는 이 만화 스트립이 독특한 예상치 못한 요소의 조합이 특징입니다. 스트립에는 정장에 밀봉이 있는 코트룸 장면과 증인 상자에 돌고래, 동그랗게 고래가 있다. 이러한 이례적이고 황당한 시나리오는 풍자 또는 패러디의 한 형태로 볼 수 있는 심각한 주제에 대해 유머러스하고 예상치 못한 반전을 만들어낸다.**.\n' +
      '\n' +
      '컨텍스트에 민감한 텍스트-동작 추론은 부족한다.\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡션 대응: 시청자는 이 만화 스트립이 사정의 황당하고 겸손한 것, 즉 법적 담론을 가진 사람과 물고기로 인해 가장 재미있게 볼 수 있는 것을 찾을 수 있다. 특히 물고기가 수염을 자라며 한 패널에서 정장을 입기 때문에 궁중 장면에서 물고기를 상상하는 것은 이례적이고 유머러스하다. 이 깜짝 요소와 등장인물과 상황 사이의 부조화는 이러한 상황을 시청자에게 웃기게 만들 수 있다.\n' +
      '\n' +
      '합리적 텍스트-동작 추론 및 공동 텍스트-동작 추론 부족 부족\n' +
      '\n' +
      '그림 36: 이 작업에서 _all 모델_은 **wrong*** 반응을 생성하며, 이는 컨텍스트에 민감한 텍스트-비전 추론***의 부족 때문이다. 구체적으로, 증강 LLM 접근법의 경우, 그것은 **-공동 텍스트-비전 추론의 부족이다.\n' +
      '\n' +
      '***그림 37**: 이 작업 _GPT4V_는 **c 보정*** 반응을 생성하고, _all 다른 모델_**hallucinate***는 미세 제조 시각 인식 능력이 없기 때문일 수 있다.\n' +
      '\n' +
      '*** <그림 38**: 이 작업에서 닫힌 소스 LMM, _GPT4V & Gemini-Vision-Pro_는 **이 이 이미지**에 과제를 인용하지 못한다. 반면, 오픈 소스 LMM, _LLaVA-1.5-13B & ShareGPT4V-7B_는 환각 및 잘못된 추론**에 시달린다. 특히, _GPT-4 w/레이아웃 인식 OCR+캡처 반응_(증강 LLM 접근법)은 **부분적으로 정확한*** 반응을 생성한다. 추론을 위해 GPT4에 제공된 시각적 정보를 분석하는 데, 우리는 캡션에는 거짓으로 설정된 "덴디 프렌드 레이싱"에 대한 정보가 포함되어 있다. 캡션은 _ShareGPT-4V-7B_를 사용하여 생성되지만, 이 과제를 고려할 때 답을 복원한다.\n' +
      '\n' +
      '** 구조:**는 낭만과 낭만 II의 일러스트레이션의 차이는 무엇인가?\n' +
      '\n' +
      '** 참조 대응:** "로맨스" 로고는 흔들리지 않은 내부를 가진 심장이 특징인 반면 "로맨스 II" 로고는 내부에 흰색 음영으로 채워진 심장을 보여준다.\n' +
      '\n' +
      '** 인간 반응:** 낭만 II는 심장에 채워져 있지 않다.\n' +
      '\n' +
      '이미지 내 "로맨스"와 "로맨스 II"의 예는 모두 심장*****GPT-4V 반응:** "로맨스"와 "로맨스 II"를 특징으로 한다.\n' +
      '\n' +
      '사랑과 애정의 널리 인식되는 상징인**icona. 두 사람의 차이는 단순히 수치적인 것으로 보이며, 언어 학습 앱 "단순히 타갈로그를 배우면 된다"에서 로맨스와 관련된 지속 또는 제2 세트의 구절을 나타낼 수 있으며 아이콘 자체의 시각적 차이는 없으며, 이는 구별이 어떤 시각적 또는 주제적 변화보다는 내용적으로 있음을 시사한다. 추론.\n' +
      '\n' +
      '** Gemini Pro-Vision대응:** 둘의 차이점은 첫 번째 것이 한 마음을 가진 반면 두 번째 것은 두 가지 마음을 가지고 있다는 것이다. 잘해.\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응:** 낭만과 낭만 II의 삽화의 차이는 심장 기호의 설계에 있다. 첫 번째 일러스트에서 심장 기호는 단순하고 2차원적인 형상으로 묘사된다. 대조적으로, 두 번째 삽화인 낭만 II의 심장 기호는 더 상세하고 복잡하게 설계된다.\n' +
      '\n' +
      '부분적으로 정확한 응답은 부분적으로 수정되지만 이미지에 완전히 접지되지 않았다.\n' +
      '\n' +
      '**LLaVA-1.5-13B 반응:** 앱 내 낭만과 낭만 II의 예는 시각적 표현 측면에서 다르다. 로맨스는 심장 기호로 대표되는 반면, 로맨스 II는 그 위에 플러스 부호를 가진 심장으로 표현된다. 잘해.\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡션 대응:** 영상은 "로맨스"와 "로맨스 II"에 대한 뚜렷한 삽화를 제공하지 않는다. 화상 캡션 - 로즈에는 로맨스와 관련된 하나의 아이콘만이 설명되어 있다. 지시는 다른 요소를 언급할 수 있지만 제공된 OCR 텍스트와 이미지 자막을 기반으로 "로맨스"와 "로맨스 II"를 구분할 정보가 충분하지 않다. 공동 텍스트 식별 추론은\n' +
      '\n' +
      '그림 39: 이 작업에서 닫힌 소스 LMM, _GPT4V_는 잘못된 추론으로 인해 **wrong 반응**를 생성한다. 이 이미지***. __to **의 태스크를 이 이미지***에 인용한다. Gemini-Vision-Pro \\(\\&\\) LLaVA-1.5-13B 환각***의 반응에서. _memini-Vision-Pro \\(\\&\\) LLaVA-1.5-13B 환각***. GPT-4 w/레이아웃 인식 OCR+캡션 대응_(증강 LLM 접근법) **는 아마도 공동 텍스트-비전 추론이 없기 때문에 잘못된 답변**를 생성한다. 그러나 _ShareGPT4V-7B_는 **부분적으로 정확한 반응**를 생성하지만 이미지에 완전히 접지할 수 없다.\n' +
      '\n' +
      '** 명령:**는 모든 그라피티 스티커에 의해 형성된 단어를 나열합니다.\n' +
      '\n' +
      '** Lol, Love, Thgul, Wtf, 웰, 온, 온, 핫, 다크, 도프, Dope** 참조 반응\n' +
      '\n' +
      '** 인간 대응:** 단어란 LOL, 사랑, 추트, WTF, UGH, 멋진, 오스, HoT, Dope이다.\n' +
      '\n' +
      '**GPT-4V 반응:** 그래피티 스티커는 각각 다른 단어를 가지고 있습니다. 내가 식별할 수 있는 것부터, 단어들은 LOVE, THANKS, **WIZ**, FISH, AWESOME, HOT, DOPE이다.\n' +
      '\n' +
      '올바른 응답과 함께 함성이 있습니다.\n' +
      '\n' +
      '홈, 오토, 윷, 우트 러브 등 여행 대응**제미니 프로비전 대응. 감사합니다.\n' +
      '\n' +
      '올바른 응답과 함께 함성이 있습니다.\n' +
      '\n' +
      '**ShareGPT-4V-7B대응: 사랑, 오대박, 향, 페이스북, 구글, 안드로이드 베암****.\n' +
      '\n' +
      '올바른 응답과 함께 함성이 있습니다.\n' +
      '\n' +
      '**LLaVA-1.5-13B 반응:** 모든 등급 스티커에 의해 형성된 단어들은 "LOVE"이다.\n' +
      '\n' +
      '정답.\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡션 대응: 그래피티 스티커가 형성한 단어들은 "GRAFONSNCINER", "UTH", "AVESOS"이다.\n' +
      '\n' +
      'Hallucinations\n' +
      '\n' +
      '그림 40: 이 작업에서 모든 모델은 스타일리즘 텍스트를 인식하는 다양한 능력을 보여주는데, _GPT4V_가 최고이고 놀랍게도 _GPT-4 w/레이아웃 인식 OCR+캡션 대응_(증강 LLM 접근)는 최악이다. 이는 다양한 스타일을 이해할 수 있는 개방형 텍스트 인식 모델의 필요성을 강조한다.\n' +
      '\n' +
      '** 명령:** 초상화가 도로에 사람이 있는 나라입니다.\n' +
      '\n' +
      '** 쿠바 쿠바 반응** 참조 반응\n' +
      '\n' +
      '** 쿠바의 인간 반응.** 쿠바의 인간 반응\n' +
      '\n' +
      '***GPT-4V 대응:** 도로에 사람이 있는 이미지는 "물질 세계: 쿠바"와 관련이 있다.\n' +
      '\n' +
      '밀대응: 부탄*****제미니 프로-비전 반응: 부탄**Vemini Pro-VVV.\n' +
      '\n' +
      'Wrong response\n' +
      '\n' +
      '**ShareGPT-4V-7B Response: Mexico**\n' +
      '\n' +
      'Wrong response\n' +
      '\n' +
      '**LLaVA-1.5-13B Response: Bhutan**\n' +
      '\n' +
      'Wrong response\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡션 대응: 이미지는 어느 국가의 초상화가 도로에 사람 그룹을 가지고 있는지에 대한 구체적인 정보를 제공하지 않는다.\n' +
      '\n' +
      '공동 텍스트-동작 추론은 부족합니다.\n' +
      '\n' +
      '그림 41: 이 작업 _GPT4V_는 **c 보정** 반응을 생성하고 _all 다른 모델_**hallucinate***는 미세 제조 시각 인식 능력이 없기 때문일 수 있다. 구체적으로, _GPT-4 w/레이아웃 인식 OCR+캡션 대응_(증강 LLM 접근법)는 이 작업에 대응하는 공동 텍스트-비전 추론 능력이 부족하다.\n' +
      '\n' +
      '**Abstract:** <사이언스>라는 제목을 가진 이야기 <사이언스: 메타어의 기괴한 정렬처럼 보였는데, 다른 두 스토리가 가지고 있는 잠금 표지판이 없기 때문이다.\n' +
      '\n' +
      '** 인간 반응:**는 메타어의 기괴한 정렬처럼 보였다. 다른 것이었습니다.\n' +
      '\n' +
      '**GPT-4V 대응 : 죄송하지만, 국가지리학과 같은 외부 웹사이트에 가입하지 않고도 어떤 특정 기사를 이용할 수 있는지에 대한 실시간 또는 현재 정보를 제공할 수 없습니다. 어떤 이야기가 무료로 접근할 수 있는지 직접 웹사이트를 방문해야 합니다.***.\n' +
      '\n' +
      '명령어 이해 및 접지 부족**** 이미지입니다.\n' +
      '\n' +
      '*** 게미니 프로비전 대응: "메이커들의 기이한 정렬처럼 보였다"는 제목의 이야기는 구독 없이 읽힐 수 있다.***.\n' +
      '\n' +
      '**ShareGPT-4V-7B대응: "여기 산불이 어떻게 시작되고 중단될지"라는 이야기는 오늘 픽 아래 구독 없이 읽을 수 있다.***.\n' +
      '\n' +
      '컨텍스트에 민감한 텍스트-동작 추론은 부족한다.\n' +
      '\n' +
      '***LLaVA-1.5-13B 대응: 오늘 픽에 따르면 "산불이 어떻게 시작되는지, 어떻게 멈출 것인지" 이야기는 구독 없이 읽을 수 있다. 이 이야기는 완전한 접근을 위해 사용할 수 있는 것이지만 다른 이야기는 국가 지질학적***에 구독을 필요로 한다.\n' +
      '\n' +
      '컨텍스트에 민감한 텍스트-동작 추론은 부족한다.\n' +
      '\n' +
      '오늘 피스 아래 구독 없이 읽을 수 있는 \'고향 OCR+옵션 대응\'이라는 범주 아래 \'**GPT-4 w/Layout OCR+캡션 대응\'은 \'사이언스&문화\'라는 범주 아래 \'산불이 어떻게 시작됐는지\', \'하와이 왕국의 보석인 라히나에서 잃어버린 것은 역사&문화\'라는 범주 아래 \'아랍 사막 상인들\'의 기괴한 정렬처럼 보였다.\n' +
      '\n' +
      '정답으로 응답하십시오.\n' +
      '\n' +
      '그림 42: 이 작업 _Gemini-Vision-Pro_에서는 **c 보정*** 반응을 생성한다. _Gemini-Vision-Pro_가 생성한다. GPT4V_는 과제를 이해하지 못하고, 이를 부여 이미지에 근거시킨다. 오픈 소스 LMM, _LLaVA-1.5-13B & ShareGPT4V-7B_는 문맥에 민감한 텍스트-비전 추론이 없기 때문에 잘못된 반응을 생성한다. GPT-4 w/레이아웃 인식 OCR+캡션 반응_(증강 LLM 접근법)는 정확한 반응을 생성하는 것과 함께 환각된다.\n' +
      '\n' +
      '***그림 43**: 이 작업 _GPT4V_는 응답을 생성하지 않는다. 다른 모든 모델은 강력한 텍스트 인식 능력의 부족으로 인해 잘못된 응답을 생성한다. 구체적으로, _GPT-4 w/레이아웃 인식 OCR+캡션 대응_(증강 LLM 접근법)는 과제에 대응하기 위한 공동 텍스트-비전 추론 능력이 부족하다.\n' +
      '\n' +
      '***그림 44**: 이 작업에서 _all 모델_**hallucinate***는 주어진 이미지에 대한 접지력이 없기 때문이다.\n' +
      '\n' +
      '명령어** 인스트럭션:**는 눈 기호에 대응하는 메시지는 무엇입니까?\n' +
      '\n' +
      '** 참조 대응:**는 조사원의 61%가 외식할 관중이 없다고 믿는다.\n' +
      '\n' +
      '** 인간 대응:** 61%의 응답자는 식사를 할 곳이 없다는 것을 알고 싶어한다.\n' +
      '\n' +
      '***GPT-4V 대응:** 눈 기호는 "식사의 61%가 그들이 가고 있는 사람들이 없을 것이라는 것을 아는 것이 매우 중요하다고 느낀다"는 메시지에 해당하며, 이는 코로나19 대유행 상황에서 식사 장소에서의 예상 군중 수준에 대한 지식을 다시 외식과 위로하는 요인으로 상당수의 식사를 우선시한다는 것을 나타낸다.\n' +
      '\n' +
      '** 게미니 프로비전 대응:** 눈 기호에 해당하는 메시지는 "그들이 가는 곳에서 인파가 없을 것임을 안다"는 것이다. 조사 설문조사에서 응답하는 숙박가의 61%가 이 중61%가 이 중61%가 이에 대해 응답한다고 응답 응답의 61%가 응답자 중61%가 응답한다고 응답했다.\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응:** 눈 기호에 해당하는 메시지는 "더 높은 팁을 남길 용의가 있다"며, 이 진술은 상당한 수의 저녁 식사를 일반적으로 15-20%인 표준 팁보다 더 큰 팁을 남길 의향이 있음을 시사한다.\n' +
      '\n' +
      'Wrong response\n' +
      '\n' +
      '**ILaVA-1.5-13B 반응:** 눈 기호에 해당하는 메시지는 식사자가 식음료 품목에 대해 더 많은 비용을 지불할 의향이 있다는 것이다. 이는 고객이 양질의 외식 경험에 관심을 가지고 있으며 식사에 더 많은 돈을 투자할 의향이 있음을 나타낸다. 눈 기호는 다른 기호들과 함께 고객이 중요시하는 식사 선호도와 경험의 다양한 측면을 나타낸다.\n' +
      '\n' +
      'Hallucinations\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡쳐 대응:** 아이 기호에 해당하는 메시지는 "다시는 공공공간에서 편안한 모임을 느끼기 위해서는 마스크를 쓰고 손 소독제를 사용하는 사람이 매우 중요하다"는 점과 63% 합의로 "어디에 가는지 알 수 없다"는 것, 61% 합의로 "코로나19 백신 전달"도 61% 합의에 이른다.\n' +
      '\n' +
      '올바른 반응과 함께 해룩하다.\n' +
      '\n' +
      '그림 45: 이 작업에서 폐쇄 소스 LMM_GPT-4V_ 및 _Gemini Pro-Vision_****는 지시에 응답한다. 이는 **시각적 인식, 텍스트 인식, 맥락에 민감한 텍스트-비전 이해**에 대한 능력에 기인할 수 있다. 다른 모든 모델 **힐루시네이트**의 반응.\n' +
      '\n' +
      '** 지도:**는 자신의 도장에 인형이 있는 국가에서 오바이에 대한 자신감과 자신감이 없는 응답자의 백분율 차이를 계산합니다.\n' +
      '\n' +
      '** 참조 대응:** 국가는 폴란드입니다. 승인자 비율은 47명이고 자신감이 없는 비율은 38명으로 두 사람의 차이는 9%이다.\n' +
      '\n' +
      '** 인간 대응:** 국가는 폴란드이며 오바마에 대한 확신을 갖고 있지 않은 응답자의 차이는 9%이다.\n' +
      '\n' +
      '***GPT-4V 대응:** 인형에 인형이 있는 나라를 찾기 위해 먼저 해당 국가를 파악하고 그 나라로부터 오바마에 대한 자신감과 자신감이 없는 응답자의 비율 차이를 계산하겠습니다. 검사 결과 인형이 있는 도장으로 대표되는 나라는 폴란드입니다. 제공된 자료에 따르면 응답자의 47%가 오바마의 세계 업무에서 올바른 일을 할 수 있는 능력에 대한 자신감이나 확신을 많이 가지고 있다. 38%의 응답자는 오바마의 세계 업무에서 올바른 일을 할 수 있는 능력에 대해 너무 자신감이 많거나 전혀 자신감을 갖지 못한다. 차이를 계산하기 위해 47%(신뢰) - 38%(자신감 없음) = 9%로 더 큰 비율에서 더 작은 비율을 뺄 수 있으며, 따라서 폴란드에서 오바마에 대한 자신감과 자신감이 없는 응답자의 백분율 차이는 9%이다.\n' +
      '\n' +
      '인형 스탬프(나이지리아)가 있는 국가의 경우 85%의 응답자가 오바마에 대한 확신을 가지고 있으며 13%는 한라시밍에 대한 확신을 갖지 않는다.\n' +
      '\n' +
      '**ShareGPT-4V-7B 대응:** 오바마에 대한 신뢰 비율이 가장 높은 국가는 영국으로 응답자의 80%가 신뢰를 갖고 있다. 반면 오바마에 대한 신뢰 비율이 가장 낮은 국가는 파키스탄으로 응답자의 20%만이 신뢰를 갖고 있다. 양국 간의 백분율 차이는 60%이다. 유감해.\n' +
      '\n' +
      '**LLaVA-1.5-13B 대응:** 이미지가 인형을 소지한 나라는 미국입니다. 오바마에 대한 자신감이 있는 응답자의 비율은 63%인 반면 오바마에 대한 신뢰가 없는 응답자의 비율은 37%이다. 두 비율의 차이는 63 - 37 = 26%이다. 유감해.\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡션 대응:**에 제공된 이미지는 인형이 있는 특정 나라를 도장에 보여주는 정보를 포함하지 않는다. 유감해.\n' +
      '\n' +
      '그림 46: 이 작업에서 _GPT-4V는 지도에 올바르게 응답한다. 이것은 **math 및 컨텍스트에 민감한 텍스트-비전 이해**에 대한 능력에 기인할 수 있다. 다른 모든 모델 **힐루시네이트**의 반응.\n' +
      '\n' +
      '** 명령:** 하이라이트는 양성 테스트의 비율이 30%보다 큰 상태를 보여줍니다.\n' +
      '\n' +
      '** WA 및 MS*WA 및 MS** 참조 반응:** WA 및 MS*** 참조 반응\n' +
      '\n' +
      '** 워싱턴과 미시시피:*** 인간 반응.\n' +
      '\n' +
      '**GPT-4V 대응: 제공된 이미지에 따라 양성 테스트의 비율이 30% 이상인 상태를 강조해야 한다면, 그래픽 편집 도구를 사용하여 이러한 상태에 주의를 기울일 수 있다. 일반적으로 이러한 유형의 히트맵 스케일에 어두운 색조로 표시됩니다. 이미지를 보았듯이 양성률이 30% 이상 높은 그늘을 나타내는 상태가 나타나지 않지만 화면 표시와 화질의 변화로 인해 정확한 정보를 위해 소스 데이터 또는 보다 상세한 맵 버전을 사용하여 확인을 부탁드립니다.**\n' +
      '\n' +
      '***는 이미지**에 대한 응답을 접지하지 않는다.\n' +
      '\n' +
      '*** 게미니 프로비전 대응 : 양성 테스트의 비율이 30% 이상인 국가는 부분적인 정확한 반응**와 함께 루이지애나, 앨라배마, 조지아, 미시시피 한라시이다.\n' +
      '\n' +
      '양성 시험의 비율이 30% 이상인 뉴욕, 뉴저지, 펜실베이니아, 메릴랜드주, 버지니아주, 델라웨어주, 델라웨어주, 델라웨어주, 코네티컷주, 오하이오, 인디애나주, 일리노이주, 미네소타주, 위스콘신주, 위스콘신주, 사우스다코타주, 네브래스카주, 노스다코타주, 웨스트버지니아주, 켄터키주, 켄터키주, 테네시주, 앨라배마주, 앨라배마주, 앨라배마주, 노스다코타주, 노스다코타주, 쿠덴시아주, 네브래스카주, 노버시아주, 네브래스카주, 쿠에다코타주, 켄터키주, 켄터키주, 켄터키주, 켄터키주, 켄터키주, 켄터키주, 켄터키주, 테네시주, 켄터키주, 테네시주, 테네시주, 테네시주, 테네시주, 테네시주, 테네시주, 테네시주, 테네시주, 테네시주, 테네시, 플로리다주, 부분적인 정확한 반응***Hallucing과 함께\n' +
      '\n' +
      '**LLaVA-1.5-13B 반응: 이미지에서는 양성 테스트의 비율이 30% 이상인 여러 상태가 있다. 이 주에는 캘리포니아: 31.7%, 텍사스: 30.6%, 플로리다: 30.5%, 플로리다: 30.4%, 펜실베이니아: 30.3%****가 포함된다.\n' +
      '\n' +
      '**Hallucinations**\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡션 대응: 이미지는 30% 이상의 양성 테스트의 백분율을 갖는 상태에 대한 특정 정보를 제공하지 않는다.\n' +
      '\n' +
      '동시 문자 작용 추론 능력*****의 부족.\n' +
      '\n' +
      '그림 47: 이 태스크에서 닫힌 소스 LMM, _GPT4V_**도에는 이미지에 대한 반응이 접지되지 않는다. 헤미니-비전-프로, ShareGPT-4V-7B & LLaVA-1.5-13Bhallucinate_의 반응에서. GPT-4 w/레이아웃 인식 OCR+캡션 대응_(증강 LLM 접근법) **는 아마도 공동 텍스트-비전 추론이 없기 때문에 잘못된 답변**를 생성한다.\n' +
      '\n' +
      '** 구조:**는 부어버/디립 카테고리에서 몇 가지 생산 방법을 사용합니까?\n' +
      '\n' +
      '좌표 반응****\n' +
      '\n' +
      '** 인간 반응:**는 붓기/달리기 범주에 10가지 방법이 있다.\n' +
      '\n' +
      '***GPT-4V 반응:** 이 커피 차트의 부는/달림 범주에서 사용된 6가지 생산 방법이 있는 것으로 보인다. 이러한 방법은 오토 디립, 코네, 플랫 보텀, 아로프레, 우드네크, 화학 웜 위롱 응답과 같다.\n' +
      '\n' +
      '** Gemini Pro-Vision 반응:**에는 부어버/디립 카테고리에 나열된 8가지 생산 방법이 있다. 정답.\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응:**는 부어버/디립 부문에서는 프랑스 언론 및 드립 커피 메이커의 두 가지 생산 방법이 사용된다. 정답.\n' +
      '\n' +
      '**LLaVA-1.5-13B 반응:**는 부어버/디립 카테고리에서 사용되는 세 가지 생산 방법이 있다. 정답.\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡처 대응:** OCR 텍스트는 쏟아짐/디프 카테고리에 따라 사용되는 생산 방법 수에 대한 구체적인 정보를 제공하지 않는다. 공동 텍스트-동작 추론은 부족합니다.\n' +
      '\n' +
      '그림 48: 이 과제에서 _all LMMs_는 미세 학습 시각 인식 부족과 수학 추론 불량으로 인해 **wrong*** 반응을 생성한다. __wrong*** 반응을 생성한다. GPT-4 w/레이아웃 인식 OCR+캡션 대응_(증강 LLM 접근법) **는 아마도 공동 텍스트-비전 추론이 없기 때문에 잘못된 답변**를 생성한다.\n' +
      '\n' +
      '** 명령:**는 스위치의 현재 위치를 변경하는 것이 악기를 활성화할지에 대한 의견을 제시합니다.\n' +
      '\n' +
      '** 참조 반응:** 스위치를 온 위치로 돌리기에도 불구하고, 연결된 전원 케이블이 없기 때문에 시스템이 구동되는 상태를 유지할 것이다.\n' +
      '\n' +
      '** 인간 반응:**는 그렇지 않습니다. 작업에 기기를 꽂아야 합니다.\n' +
      '\n' +
      '**GPT-4V 반응:** 영상은 **\'OFF\' 위치에 전력 스위치가 있는 전자 장치의 백 패널을 보여준다. 스위치의 현재 위치를 \'OFF\'에서 \'ON\'으로 변경하는 것은 전력원에 적절히 연결되어 있고, 악기 자체에 오작동이 없다는 점에서 도구를 활성화할 가능성이 높다.\n' +
      '\n' +
      'Wrong response\n' +
      '\n' +
      '** 게미니 프로비전 대응:** \'ON\' 위치로 전환하면 악기가 힘듭니다.\n' +
      '\n' +
      'Wrong response\n' +
      '\n' +
      '**ShareGPT-4V-7B 반응:** 이미지에 따라 스위치의 현재 위치를 변경하는 것이 악기를 활성화할 것인지 확인할 수 없다. 스위치는 현재 오프 위치에 있고, 악기는 사용하지 않습니다. 스위치의 위치를 변경하면 기구가 활성화되는지 확인하기 위해서는 스위치의 움직임에 대한 악기의 반응을 관찰해야 한다.\n' +
      '\n' +
      'Wrong response\n' +
      '\n' +
      '\'**LLaVA-1.5-13B 반응:**\' 영상에서 스위치는 현재 "오프" 위치에 있으며, 이는 도구가 활성화되지 않음을 의미한다. 악기를 활성화하려면 스위치를 "온" 위치로 이동해야 합니다. 이를 통해 전기 전류가 장치를 통해 흐를 수 있어 제대로 기능할 수 있습니다.\n' +
      '\n' +
      'Wrong response\n' +
      '\n' +
      '***GPT-4 w/레이아웃 인식 OCR+캡션 대응:**는 제공된 OCR 텍스트 또는 스위치에 관한 이미지 캡션에서 이용 가능한 시각적 정보가 충분하지 않아 명령어가 모호하다. 따라서 스위치의 현재 위치를 변경하는 것이 악기를 활성화할 것인지에 대한 의견을 제시할 수는 없다.\n' +
      '\n' +
      '공동 텍스트-동작 추론은 부족합니다.\n' +
      '\n' +
      '그림 49: 이 작업에서 _GPT4V_는 **c 정확하고** 반응을 생성하는 반면, _all 다른 LMMs_는 미세 제조 시각 인식의 부족으로 **wrong**** 반응을 생성한다. GPT-4 w/레이아웃 인식 OCR+캡션 대응_(증강 LLM 접근법) **는 아마도 공동 텍스트-비전 추론이 없기 때문에 잘못된 답변**를 생성한다.\n' +
      '\n' +
      '그림 50: 이 작업에서 _GPT-4V_ 및 _Gemini Pro-Vision_ **c 정확하게***는 지시에 응답한다. 이는 텍스트 인식에 대한 능력, 맥락에 민감한 텍스트 구현 이해에 기인한다고 할 수 있다. _ShareGPT4V_ 및 _LLaVA-V-1.5_ 및 _GPT-4 w/레이아웃 인식 OCR+캡션 반응_(증강 LLM 접근법)와 같은 오픈 소스 LMM은 **wrong** 대답을 예측한다.\n' +
      '\n' +
      '***그림 51**: 이 과제에서 _all LMMs_는 미세 편성된 시각적 인식과 맥락에 민감한 텍스트-비전 추론의 부족으로 인해 잘못된 답을 생성한다. _all LMMs_는 잘못된 답변을 생성한다. GPT-4 w/레이아웃 인식 OCR+캡션 대응_(증강 LLM 접근법) **는 아마도 공동 텍스트-비전 추론이 없기 때문에 잘못된 답변**를 생성한다.\n' +
      '\n' +
      '그림 52: 이 작업에서 _GPT4V_ & _LLaVA-1.5-13B_홀쿠네이트인 반면, _Gemini-Vision-Pro & ShareGPT-4V-7B_는 잘못된 반응을 생성한다. LMM의 잘못된 예측은 아마도 미세한 시각 인식의 부족으로 인한 것일 수 있다. __ GPT-4 w/레이아웃 인식 OCR+캡션 대응_(증강 LLM 접근법) **는 아마도 공동 텍스트-비전 추론이 없기 때문에 잘못된 답변**를 생성한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>