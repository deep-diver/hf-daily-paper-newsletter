<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# LocalMamba: Visual State Space Model with Windowed Selective Scan\n' +
      '\n' +
      'Tao Huang\n' +
      '\n' +
      '1School of Computer Science, Faculty of Engineering, The University of Sydney 1\n' +
      '\n' +
      'Xiaohuan Pei\n' +
      '\n' +
      '1School of Computer Science, Faculty of Engineering, The University of Sydney 1\n' +
      '\n' +
      'Shan You\n' +
      '\n' +
      '2SenseTime Research 2\n' +
      '\n' +
      'Fei Wang\n' +
      '\n' +
      '3University of Science and Technology of China3\n' +
      '\n' +
      'Chen Qian\n' +
      '\n' +
      '2SenseTime Research 2\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      '1School of Computer Science, Faculty of Engineering, The University of Sydney 1\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Recent advancements in state space models, notably Mamba, have demonstrated significant progress in modeling long sequences for tasks like language understanding. Yet, their application in vision tasks has not markedly surpassed the performance of traditional Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). This paper posits that the key to enhancing Vision Mamba (ViM) lies in optimizing scan directions for sequence modeling. Traditional ViM approaches, which flatten spatial tokens, overlook the preservation of local 2D dependencies, thereby elongating the distance between adjacent tokens. We introduce a novel local scanning strategy that divides images into distinct windows, effectively capturing local dependencies while maintaining a global perspective. Additionally, acknowledging the varying preferences for scan patterns across different network layers, we propose a dynamic method to independently search for the optimal scan choices for each layer, substantially improving performance. Extensive experiments across both plain and hierarchical models underscore our approach\'s superiority in effectively capturing image representations. For example, our model significantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs. Code is available at: [https://github.com/hunto/LocalMamba](https://github.com/hunto/LocalMamba).\n' +
      '\n' +
      'Keywords:Generic vision model Image recognition State space model\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Structured State Space Models (SSMs) have recently gained prominence as a versatile architecture in sequence modeling, heralding a new era of balancing computational efficiency and model versatility [12, 13, 9, 35]. These models synthesize the best attributes of Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), drawing inspiration from the foundational principles of classical state space models [23]. Characterized by their computational efficiency, SSMs exhibit linear or near-linear scaling complexity with sequencelength, making them particularly suited for handling long sequences. Following the success of Mamba [9], a novel variant that incorporates selective scanning (S6), there has been a surge in applying SSMs to a wide range of vision tasks. These applications extend from developing generic foundation models [32, 60] to advancing fields in image segmentation [30, 34, 40, 54] and synthesis [14], demonstrating the model\'s adaptability and potential in visual domain.\n' +
      '\n' +
      'Typically these vision studies need to transform 2D images into 1D sequences for SSM-based processing, and then integrate the original SSM structure of Mamba into their foundational models for specific tasks. Nevertheless, they have only shown modest improvements over traditional CNNs [20, 24, 41, 42, 52] and Vision Transformers (ViTs) [3, 3, 7, 46]. This modest advancement underscores a significant challenge: the non-causal nature of 2D spatial pattern in images is inherently at odds with the causal processing framework of SSMs. As illustrated in Figure 1, traditional methods that flatten spatial data into 1D tokens disrupt the natural local 2D dependencies, weakening the model\'s ability to accurately interpret spatial relationships. Although V Mamba [32] introduces a 2D scanning technique to address this by scanning images in both horizontal and vertical directions, it still struggles with maintaining the proximity of originally adjacent tokens within the scanned sequences, which is critical for effective local representation modeling.\n' +
      '\n' +
      'In this work, we introduce a novel approach to improve local representation within Vision Mamba (ViM) by segmenting the image into multiple distinct local windows. Each window is scanned individually before conducting a traversal\n' +
      '\n' +
      'Figure 1: Illustration of scan methods. (a) and (b): Previous methods Vim [60] and V Mamba [32] traverse the entire row or column axis, resulting in significant distances for capturing dependencies between neighboring pixels within the same semantic region (_e.g._, the left eye in the image). (c) We introduce a novel scan method that partitions tokens into distinct windows, facilitating traversal within each window (window size is \\(3\\times 3\\) here). This approach enhances the ability to capture local dependencies.\n' +
      '\n' +
      'Figure 2: By extending the original scan with our local scan mechanism, our method significantly improves the ImageNet accuracies of Vim [60] while keeping similar FLOPs.\n' +
      '\n' +
      ' across windows, ensuring that tokens within the same 2D semantic region are processed closely together. This method significantly boosts the model\'s capability to capture details among local regions, with the experimental results validated in Figure 2. We design our foundational block by integrating both traditional global scanning directions and our novel local scanning technique, empowering the model to assimilate comprehensive global and nuanced local information. Furthermore, to better aggregate features from these diverse scanning processes, we propose a spatial and channel attention module, SCAttn, engineered to discern and emphasize valuable information while filtering out redundancy.\n' +
      '\n' +
      'Acknowledging the distinct impact of scanning directions on feature representation (for instance, a local scan with a window size of 3 excels in capturing smaller objects or details, whereas a window size of 7 is better suited for larger objects), we introduce a direction search method for selecting optimal scanning directions. This variability is especially pronounced across different layers and network depths. Inspired by DARTS [29], we proceed from discrete selection to a continuous domain, represented by a learnable factor, to incorporate multiple scanning directions within a single network. After the training of this network, the most effective scanning directions are determined by identifying those with the highest assigned probabilities.\n' +
      '\n' +
      'Our developed models, LocalVim and LocalV Mamba, incorporate both plain and hierarchical structures, resulting in notable enhancements over prior methods. Key contributions of this study include:\n' +
      '\n' +
      '1. We introduce a novel scanning methodology for SSMs that includes localized scanning within distinct windows, significantly enhancing our models\' ability to capture detailed local information in conjunction with global context.\n' +
      '2. We develop a method for searching scanning directions across different network layers, enabling us to identify and apply the most effective scanning combinations, thus improving network performance.\n' +
      '3. We present two model variants, designed with plain and hierarchical structures. Through extensive experimentation on image classification, object detection, and semantic segmentation tasks, we demonstrate that our models achieve significant improvements over previous works. For example, on semantic segmentation task, with a similar amount of parameters, our LocalVim-S outperforms Vim-S by a large margin of 1.5 on mIoU (SS).\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Generic Vision Backbone Design\n' +
      '\n' +
      'The last decade has witnessed transformative advancements in computer vision, primarily driven by the evolution of deep neural networks and the emergence of foundational generic models. Initially, Convolutional Neural Networks (CNNs) [17, 20, 24, 39, 41, 42, 50, 52] marked a significant milestone in visual model architecture, setting the stage for complex image recognition and analysis tasks.\n' +
      '\n' +
      'Among these works, ResNet [20], with a cornerstone residual connection technique, is one of the most popular model that is widely used in broad field of vision tasks; MobileNet [21, 41] series lead the design of light-weight models with the utilization of depth-wise convolutions. However, the introduction of the Vision Transformer (ViT) [7] marked a paradigm shift, challenging the supremacy of CNNs in the domain. ViTs revolutionize the approach to image processing by segmenting images into a series of sequential patches and leveraging the self-attention mechanism, a core component of Transformer architectures [48], to extract features. This novel methodology highlighted the untapped potential of Transformers in visual tasks, sparking a surge of research aimed at refining their architecture design [45] and training methodologies [18, 31, 46, 47, 53], boosting computational efficiency [3, 22, 33, 49], and extending their application scope [58, 25, 26, 38, 44, 26, 8]. Building on the success of long-sequence modeling with Mamba [9], a variant of State Space Models (SSMs), some innovative models such as Vim [60] and V Mamba [32] have been introduced in visual tasks, namely Vision Mamba. These models adapt the Mamba framework to serve as a versatile backbone for vision applications, demonstrating superior efficiency and accuracy over traditional CNNs and ViTs in high-resolution images.\n' +
      '\n' +
      '### State Space Models\n' +
      '\n' +
      'State Space Models (SSMs) [11, 13, 16, 27, 37], represent a paradigm in architecture designed for sequence-to-sequence transformation, adept at managing long dependency tokens. Despite initial challenges in training, owing to their computational and memory intensity, recent advancements [9, 10, 11, 16, 43] have significantly ameliorated these issues, positioning deep SSMs as formidable competitors against CNNs and Transformers. Particularly, S4 [11] introduced an efficient Normal Plus Low-Rank (NPLR) representation, leveraging the Woodbury identity for expedited matrix inversion, thus streamlining the convolution kernel computation. Building on this, Mamba [9] further refined SSMs by incorporating an input-specific parameterization alongside a scalable, hardware-optimized computation approach, achieving unprecedented efficiency and simplicity in processing extensive sequences across languages and genomics.\n' +
      '\n' +
      'The advent of S4ND [36] marked the initial foray of SSM blocks into visual tasks, adeptly handling visual data as continuous signals across 1D, 2D, and 3D domains. Subsequently, taking inspiration of the success of Mamba models, Vmamba [32] and Vim [60] expanded into generic vision tasks, addressing the directional sensitivity challenge in SSMs by proposing bi-directional scan and cross-scan mechanisms. Leveraging Mamba\'s foundation in generic models, new methodologies have been developed for visual tasks, such as image segmentation [30, 34, 40, 54] and image synthetic [14], showcasing the adaptability and effectiveness of visual Mamba models in addressing complex vision challenges.\n' +
      '\n' +
      '## 3 Preliminaries\n' +
      '\n' +
      '### State Space Models\n' +
      '\n' +
      'Structured State Space Models (SSMs) represent a class of sequence models within deep learning, characterized by their ability to map a one-dimensional sequence \\(x(t)\\in\\mathbb{R}^{L}\\) to \\(y(t)\\in\\mathbb{R}^{L}\\) via an intermediate latent state \\(h(t)\\in\\mathbb{R}^{N}\\):\n' +
      '\n' +
      '\\[\\begin{split} h^{\\prime}(t)&=\\mathbf{A}h(t)+\\mathbf{B}x(t), \\\\ y(t)&=\\mathbf{C}h(t),\\end{split} \\tag{1}\\]\n' +
      '\n' +
      'where the system matrices \\(\\mathbf{A}\\in\\mathbb{R}^{N\\times N}\\), \\(\\mathbf{B}\\in\\mathbb{R}^{N\\times 1}\\), and \\(\\mathbf{C}\\in\\mathbb{R}^{N\\times 1}\\) govern the dynamics and output mapping, respectively.\n' +
      '\n' +
      '**Discretization.** For practical implementation, the continuous system described by Equation 1 is discretized using a zero-order hold assumption4, effectively converting continuous-time parameters (\\(\\mathbf{A}\\), \\(\\mathbf{B}\\)) to their discrete counterparts (\\(\\mathbf{\\overline{A}}\\), \\(\\mathbf{\\overline{B}}\\)) over a specified sampling timescale \\(\\mathbf{\\Delta}\\in\\mathbb{R}\\)\\(>0\\):\n' +
      '\n' +
      'Footnote 4: This assumption holds the value of \\(x\\) constant over a sample interval \\(\\Delta\\).\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{\\overline{A}}&=e^{\\mathbf{\\Delta A}} \\\\ \\mathbf{\\overline{B}}&=(\\mathbf{\\Delta A})^{-1}(e^{\\mathbf{ \\Delta A}}-\\mathbf{I})\\cdot\\mathbf{\\Delta B}.\\end{split} \\tag{2}\\]\n' +
      '\n' +
      'This leads to a discretized model formulation as follows:\n' +
      '\n' +
      '\\[\\begin{split} h_{t}&=\\mathbf{\\overline{A}}h_{t-1}+\\mathbf{ \\overline{B}}x_{t},\\\\ y_{t}&=\\mathbf{C}h_{t}.\\end{split} \\tag{3}\\]\n' +
      '\n' +
      'For computational efficiency, the iterative process delineated in Equation 3 can be expedited through parallel computation, employing a global convolution operation:\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{y}&=\\mathbf{x}\\oplus\\mathbf{\\overline{K}} \\\\ \\text{with}&\\mathbf{\\overline{K}}=(\\mathbf{C}\\mathbf{\\overline{B}}, \\mathbf{C}\\mathbf{\\overline{A}}\\mathbf{\\overline{B}},...,\\mathbf{C}\\mathbf{\\overline{A}}^{L-1}\\mathbf{ \\overline{B}}),\\end{split} \\tag{4}\\]\n' +
      '\n' +
      'where represents the convolution operation, and \\(\\mathbf{\\overline{K}}\\in\\mathbb{R}^{L}\\) serves as the kernel of the SSM. This approach leverages the convolution to synthesize the outputs across the sequence simultaneously, enhancing computational efficiency and scalability.\n' +
      '\n' +
      '### Selective State Space Models\n' +
      '\n' +
      'Traditional State Space Models (SSMs), often referred to as S4, have achieved linear time complexity. However, their ability to capture sequence context is inherently constrained by static parameterization. To address this limitation,Selective State Space Models (termed Mamba) [9] introduce a dynamic and selective mechanism for the interactions between sequential states. Unlike conventional SSMs that utilize constant transition parameters \\((\\overline{\\mathbf{A}},\\overline{\\mathbf{B}})\\), Mamba models employ input-dependent parameters, enabling a richer, sequence-aware parameterization. Specifically, Mamba models calculate the parameters \\(\\mathbf{B}\\in\\mathbb{R}^{B\\times L\\times N}\\), \\(\\mathbf{C}\\in\\mathbb{R}^{B\\times L\\times N}\\), and \\(\\mathbf{\\Delta}\\in\\mathbb{R}^{B\\times L\\times D}\\) directly from the input sequence \\(\\mathbf{x}\\in\\mathbb{R}^{B\\times L\\times D}\\).\n' +
      '\n' +
      'The Mamba models, leveraging selective SSMs, not only achieve linear scalability in sequence length but also deliver competitive performance in language modeling tasks. This success has inspired subsequent applications in vision tasks, with studies proposing the integration of Mamba into foundational vision models. Vim [60], adopts a ViT-like architecture, incorporating bi-directional Mamba blocks in lieu of traditional transformer blocks. V Mamba [32] introduces a novel 2D selective scanning technique to scan images in both horizontal and vertical orientations, and constructs a hierarchical model akin to the Swin Transformer [33]. Our research extends these initial explorations, focusing on optimizing the S6 adaptation for vision tasks, where we achieve improved performance outcomes.\n' +
      '\n' +
      '## 4 Methodology\n' +
      '\n' +
      'This section delineates core components of our LocalMamba, beginning with the local scan mechanism designed to enhance the model\'s ability to dig fine-grained details from images. Subsequently, we introduce the scan direction search algorithm, an innovative approach that identifies optimal scanning sequences across different layers, thereby ensuring a harmonious integration of global and local visual cues. The final part of this section illustrates the deployment of the LocalMamba framework within both simple plain architecture and complex hierarchical architecture, showcasing its versatility and effectiveness in diverse settings.\n' +
      '\n' +
      'Figure 3: (a) Structure of the LocalVim model. (b) Illustration of the proposed spatial and channel attention module (SCAttn).\n' +
      '\n' +
      '### Local Scan for Visual Representations\n' +
      '\n' +
      'Our method employs the selective scan mechanism, S6, which has shown exceptional performance in handling 1D causal sequential data. This mechanism processes inputs causally, effectively capturing vital information within the scanned segments, akin to language modeling where understanding the dependencies between sequential words is essential. However, the inherent non-causal nature of 2D spatial data in images poses a significant challenge to this causal processing approach. Traditional strategies that flatten spatial tokens compromise the integrity of local 2D dependencies, thereby diminishing the model\'s capacity to effectively discern spatial relationships. For instance, as depicted in Figure 1 (a) and (b), the flattening approach utilized in Vim [60] disrupts these local dependencies, significantly increasing the distance between vertically adjacent tokens and hampering the model\'s ability to capture local nuances. While VMDA [32] attempts to address this by scanning images in both horizontal and vertical directions, it still falls short of comprehensively processing the spatial regions in a single scan.\n' +
      '\n' +
      'To address this limitation, we introduce a novel approach for scanning images locally. By dividing images into multiple distinct local windows, our method ensures a closer arrangement of relevant local tokens, enhancing the capture of local dependencies. This technique is depicted in Figure 1 (c), contrasting our approach with prior methods that fail to preserve spatial coherence.\n' +
      '\n' +
      'While our method excels at capturing local dependencies effectively within each region, it also acknowledges the significance of global context. To this end, we construct our foundational block by integrating the selective scan mechanism across four directions: the original (a) and (c) directions, along with their flipped counterparts facilitating scanning from tail to head (the flipped directions are adopted in both Vim and VMDA for better modeling of non-causal image tokens). This multifaceted approach ensures a comprehensive analysis within each selective scan block, striking a balance between local detail and global perspective.\n' +
      '\n' +
      'As illustrated in Figure 3, our block processes each input image feature through four distinct selective scan branches. These branches independently capture relevant information, which is subsequently merged into a unified feature output. To enhance the integration of diverse features and eliminate extraneous information, we introduce a spatial and channel attention module before merging. As shown in Figure 2(b), this module adaptively weights the channels and tokens within the features of each branch, comprising two key components: a channel attention branch and a spatial attention branch. The channel attention branch aggregates global representations by averaging the input features across the spatial dimension, subsequently applying a linear transformation to determine channel weights. Conversely, the spatial attention mechanism assesses token-wise significance by augmenting each token\'s features with global representations, enabling a nuanced, importance-weighted feature extraction.\n' +
      '\n' +
      '**Remark.** While some ViT variants, such as the Swin Transformer [33], propose the division of images into smaller windows, the local scan in our LocalMamba is distinct both in purpose and effect. The windowed self-attention in ViTs primarily addresses the computational efficiency of global self-attention, albeit at the expense of some global attention capabilities. Conversely, our local scan mechanism aims to rearrange token positions to enhance the modelling of local region dependencies in visual Mamba, while the global understanding capability is retained as the entire image is still aggregated and processed by SSM.\n' +
      '\n' +
      '### Searching for Adaptive Scan\n' +
      '\n' +
      'The efficacy of the Structured State Space Model (SSM) in capturing image representations varies across different scan directions. Achieving optimal performance intuitively suggests employing multiple scans across various directions, similar to our previously discussed 4-branch local selective scan block. However, this approach substantially increases computational demands. To address this, we introduce a strategy to efficiently select the most suitable scan directions for each layer, thereby optimizing performance without incurring excessive computational costs. This method involves searching for the optimal scanning configurations for each layer, ensuring a tailored and efficient representation modeling.\n' +
      '\n' +
      '**Search space.** To tailor the scanning process for each layer, we introduce a diverse set \\(\\mathcal{S}\\) of 8 candidate scan directions. These include horizontal and vertical scans (both standard and flipped), alongside local scans with window sizes of 2 and 7 (also both standard and flipped). For a consistent computational budget as previous models, we select 4 out of these 8 directions for each layer. This approach results in a substantial search space of \\((C_{8}^{4})^{K}\\), with \\(K\\) representing the total number of blocks.\n' +
      '\n' +
      'Building upon the principles of DARTS [29], our method applies a differentiable search mechanism for scan directions, employing continuous relaxation to navigate the categorical choices. This approach transforms the discrete selection\n' +
      '\n' +
      'Figure 4: Visualization of the searched directions of our models. The visualization of LocalVLambda-S is in Section A.2.\n' +
      '\n' +
      'process into a continuous domain, allowing for the use of softmax probabilities to represent the selection of scan directions:\n' +
      '\n' +
      '\\[\\mathbf{y}^{(l)}=\\sum_{s\\in\\mathcal{S}}\\frac{\\exp(\\alpha_{s}^{(l)})}{\\sum_{s^{\\prime} \\in\\mathcal{S}}\\exp(\\alpha_{s^{\\prime}}^{(l)})}\\text{SSM}_{s}(\\mathbf{x}^{(l)}), \\tag{5}\\]\n' +
      '\n' +
      'where \\(\\mathbf{\\alpha}^{(l)}\\) denotes a set of learnable parameters for each layer \\(l\\), reflecting the softmax probabilities over all potential scan directions.\n' +
      '\n' +
      'We construct the entire search space as an over-parameterized network, allowing us to simultaneously optimize the network parameters and the architecture variables \\(\\mathbf{\\alpha}\\), following standard training protocols. Upon completion of the training, we derive the optimal direction options by selecting the four directions with the highest softmax probabilities. We visualize the searched directions of our models in Figure 4. For detailed analysis of the search results, see Section 5.5.\n' +
      '\n' +
      '**Scalability of direction search.** Our current approach aggregates all scan directions for selection in training, aptly serving models with a moderate range of options. For instance, a model featuring 20 blocks and 128 directions per block requires 28 GB of GPU memory, indicating scalability limits for extensive choices. To mitigate memory consumption in scenarios with a vast selection array, techniques such as single-path sampling [15, 57], binary approximation [1], and partial-channel usage [55] present viable solutions. We leave the investigation of more adaptive direction strategies and advanced search techniques to future endeavors.\n' +
      '\n' +
      '### Architecture Variants\n' +
      '\n' +
      'To thoroughly assess our methodology\'s effectiveness, we introduce architecture variants grounded in both plain [60] and hierarchical [32] structures, named LocalVim and LocalVamba, respectively. The configurations of these architectures are detailed in Table 1. Specifically, in LocalVim, the standard SSM block is substituted with our LocalVim block, as depicted in Figure 3. Considering the original Vim block comprises two scanning directions (horizontal and flipped\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c} \\hline Model & \\#Dims & \\#Blocks & Params & FLOPs \\\\ \\hline LocalVim-T & 192 & 20 & 8M & 1.5G \\\\ LocalVim-S & 384 & 20 & 28M & 4.8G \\\\ \\hline LocalVAMaba-T & [96, 192, 384, 768] & [2, 2, 9, 2] & 26M & 5.7G \\\\ LocalVAMaba-S & [96, 192, 384, 768] & [2, 2, 27, 2] & 50M & 11.4G \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Architecture variants. We follow the original structure designs of Vim and V Mamba, where Vim uses a plain structure with a patch embedding of stride 16, while V Mamba constructs a hierarchical structures with SSM stages on strides 4, 8, 16, and 32.\n' +
      '\n' +
      'horizontal), and our LocalVim introduces four scanning directions, thereby increasing the computational overhead. To maintain similar computation budgets, we adjust the number of Vim blocks from 24 to 20. For LocalVAMaba, which inherently has four scanning directions akin to our model, we directly replace the blocks without changing the structural configurations.\n' +
      '\n' +
      '**Computational cost analysis.** Our LocalMamba block is efficient and effective, with only a marginal increase on computation cost. The scanning mechanism, which involves merely repositioning tokens, incurs no additional computational cost in terms of FLOPs. Furthermore, the SCAttn module, designed for efficient aggregation of varied information across scans, is exceptionally streamlined. It leverages linear layers to reduce the token dimension by a factor of \\(1/r\\), thereafter generating attention weights across both spatial and channel dimensions, with \\(r\\) set to 8 for all models. For instance, our LocalVAMaba-T model, which replaces the VAMaba block with our LocalMamba block, only increases the FLOPs of VAMava-T from 5.6G to 5.7G.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      'This section outlines our experimental evaluation, starting with the ImageNet classification task, followed by transferring the trained models to various downstream tasks, including object detection and semantic segmentation.\n' +
      '\n' +
      '### ImageNet Classification\n' +
      '\n' +
      '#### 5.1.1 Training strategies.\n' +
      '\n' +
      'We train the models on ImageNet-1K dataset [6] and evaluate the performance on ImageNet-1K validation set. Following previous works [32, 33, 46, 60], we train our models for 300 epochs with a base batch size of 1024 and an AdamW optimizer, a cosine annealing learning rate schedule is adopted with initial value \\(10^{-3}\\) and 20-epoch warmup. For training data augmentation, we use random cropping, AutoAugment [5] with policy _rand-m9-mstd0.5_, and random erasing of pixels with a probability of 0.25 on each image, then a MixUp strategy with ratio 0.2 is adopted in each batch. An exponential moving average on model is adopted with decay rate 0.9999.\n' +
      '\n' +
      '**Scan direction search.** For the supernet training, we curtail the number of epochs to 100 while maintaining the other hyper-parameters consistent with standard ImageNet training. The embedding dimension for the supernet in LocalVim variants is set to 128, with search operations conducted identically on LocalVim-T and LocalVim-S due to their uniform layer structure. For LocalV-Mamba variants, including LocalVAMaba-T and LocalVAMaba-S, the initial embedding dimension is minimized to 32 to facilitate the search process.\n' +
      '\n' +
      '**Results.** Our results, summarized in Table 2, illustrate significant accuracy enhancements over traditional CNNs and ViT methodologies. Notably, LocalVim-T achieves a 76.2% accuracy rate with 1.5G FLOPs, surpassing the DeiT-Ti, which records a 72.2% accuracy. In hierarchical structures, LocalVAMaba-T\'s 82.7% accuracy outperforms Swin-T by 1.4%. Moreover, compared to our s. The proposed method is the seminal contributions, Vim and VMamba, our approach registers substantial gains; for instance, LocalVim-T and LocalVAMaba-T exceed Vim-Ti and VMamba-T by 2.7% and 0.5% in accuracy, respectively. Additionally, to validate the local scan\'s effectiveness, we conducted additional experiments on models without the scan direction search, delineated in Section 4.1, marked with \\({}^{*}\\) in the table. Incorporating merely our local scans into the original Vim framework, LocalVim-T\\({}^{*}\\) surpasses Vim-Ti by 2.7%, while the complete methodology further elevates accuracy by 0.4%. These findings affirm the pivotal role of scan directions in visual SSMs, evidencing our local scan approach\'s capability to enhance local dependency capture effectively.\n' +
      '\n' +
      '### Object Detection\n' +
      '\n' +
      '**Training strategies.** We validate our performance on object detection using MSCOCO 2017 dataset [28] and MMDetection library [2]. For LocalVAMaba series, we follow previous works [32, 33] to train object detection and instance segmentation tasks with Mask-RCNN detector [19]. The training strategies include 1\\(\\times\\) setting of 12 training epochs and 3\\(\\times\\) setting with 36 training epochs\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c} \\hline \\hline Method & Image size & Params (M) & FLOPs (G) & Top-1 ACC (\\%) \\\\ \\hline RegNetY-4G [39] & \\(224^{2}\\) & 21 & 4.0 & 80.0 \\\\ RegNetY-8G [39] & \\(224^{2}\\) & 39 & 8.0 & 81.7 \\\\ RegNetY-16G [39] & \\(224^{2}\\) & 84 & 16.0 & 82.9 \\\\ \\hline ViT-B/16 [7] & \\(384^{2}\\) & 86 & 55.4 & 77.9 \\\\ ViT-L/16 [7] & \\(384^{2}\\) & 307 & 190.7 & 76.5 \\\\ \\hline DeiT-Ti [46] & \\(224^{2}\\) & 6 & 1.3 & 72.2 \\\\ DeiT-S [46] & \\(224^{2}\\) & 22 & 4.6 & 79.8 \\\\ DeiT-B [46] & \\(224^{2}\\) & 86 & 17.5 & 81.8 \\\\ \\hline Swin-T [33] & \\(224^{2}\\) & 29 & 4.5 & 81.3 \\\\ Swin-S [33] & \\(224^{2}\\) & 50 & 8.7 & 83.0 \\\\ Swin-B [33] & \\(224^{2}\\) & 88 & 15.4 & 83.5 \\\\ \\hline Vim-Ti [60] & \\(224^{2}\\) & 7 & 1.5 & 73.1 \\\\ Vim-S [60] & \\(224^{2}\\) & 26 & 5.1 & 80.3 \\\\ \\hline LocalVim-T\\({}^{*}\\) & \\(224^{2}\\) & 8 & 1.5 & 75.8 \\\\ LocalVim-T & \\(224^{2}\\) & 8 & 1.5 & 76.2 \\\\ LocalVim-S\\({}^{*}\\) & \\(224^{2}\\) & 28 & 4.8 & 81.0 \\\\ LocalVim-S & \\(224^{2}\\) & 28 & 4.8 & 81.2 \\\\ \\hline VMamba-T [32] & \\(224^{2}\\) & 22 & 5.6 & 82.2 \\\\ VMamba-S [32] & \\(224^{2}\\) & 44 & 11.2 & 83.5 \\\\ VMamba-B [32] & \\(224^{2}\\) & 75 & 18 & 83.7 \\\\ \\hline LocalVMamba-T & \\(224^{2}\\) & 26 & 5.7 & 82.7 \\\\ LocalVMamba-S & \\(224^{2}\\) & 50 & 11.4 & 83.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Comparison of different backbones on ImageNet-1K classification. \\(*\\): Our model without scan direction search.\n' +
      '\n' +
      'and multi-scale data augmentations. While for LocalVim, we follow Vim [60] to use Cascade Mask R-CNN with ViTDet [26] as the detector.\n' +
      '\n' +
      '**Results.** We summarize our results on LocalVMamba in comparisons to other backbones in Table 3. We can see that, our LocalVMamba outperforms VMamba consistently on all the model variants. And compared to other architectures, CNNs and ViTs, we obtain significant superiority. For example, our LocalVMamba-T obtains \\(46.7\\) box AP and \\(42.2\\) mask AP, improves Swin-T by large margins of \\(4.0\\) and \\(2.9\\), respectively. For quantitative comparisons to Vim, please refer to supplementary material.\n' +
      '\n' +
      '### Semantic Segmentation\n' +
      '\n' +
      '**Training strategies.** Following [32, 33, 60], we train UperNet [51] with our backbones on ADE20K [59] dataset. The models are trained with a total batch size of \\(16\\) with \\(512\\times 512\\) inputs, an AdamW optimizer is adopted with weight decay \\(0.01\\). We use a Poly learning rate schedule, which decays \\(160\\)K iterations with an initial learning rate of \\(6\\times 10^{-5}\\). Note that Vim did not report the FLOPs and mIoU (MS) and release the code for segmentation, so we implement our LocalVim following the ViT example configuration in MMSegmentation [4].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c c|c c} \\hline \\multicolumn{8}{c}{**Mask R-CNN 1\\(\\times\\) schedule**} \\\\ \\hline Backbone & Params & FLOPs & AP\\({}^{\\text{b}}\\) & AP\\({}^{\\text{b}}_{50}\\) & AP\\({}^{\\text{b}}_{75}\\) & AP\\({}^{\\text{m}}\\) & AP\\({}^{\\text{m}}_{50}\\) & AP\\({}^{\\text{m}}_{75}\\) \\\\ \\hline ResNet-50 & 44M & 260G & 38.2 & 58.8 & 41.4 & 34.7 & 55.7 & 37.2 \\\\ Swin-T & 48M & 267G & 42.7 & 65.2 & 46.8 & 39.3 & 62.2 & 42.2 \\\\ ConvNeXt-T & 48M & 262G & 44.2 & 66.6 & 48.3 & 40.1 & 63.3 & 42.8 \\\\ ViT-Adapter-S & 48M & 403G & 44.7 & 65.8 & 48.3 & 39.9 & 62.5 & 42.8 \\\\ VMamba-T & 42M & 286G & 46.5 & 68.5 & 50.7 & 42.1 & 65.5 & 45.3 \\\\ LocalVMamba-T & 45M & 291G & 46.7 & 68.7 & 50.8 & 42.2 & 65.7 & 45.5 \\\\ \\hline ResNet-101 & 63M & 336G & 38.2 & 58.8 & 41.4 & 34.7 & 55.7 & 37.2 \\\\ Swin-S & 69M & 354G & 44.8 & 66.6 & 48.9 & 40.9 & 63.2 & 44.2 \\\\ ConvNeXt-S & 70M & 348G & 45.4 & 67.9 & 50.0 & 41.8 & 65.2 & 45.1 \\\\ VMamba-S & 64M & 400G & 48.2 & 69.7 & 52.5 & 43.0 & 66.6 & 46.4 \\\\ \\hline LocalVMamba-S & 69N & 414G & 48.4 & 69.9 & 52.7 & 43.2 & 66.7 & 46.5 \\\\ \\hline \\multicolumn{8}{c}{**Mask R-CNN 3\\(\\times\\) MS schedule**} \\\\ \\hline Swin-T & 48M & 267G & 46.0 & 68.1 & 50.3 & 41.6 & 65.1 & 44.9 \\\\ ConvNeXt-T & 48M & 262G & 46.2 & 67.9 & 50.8 & 41.7 & 65.0 & 44.9 \\\\ ViT-Adapter-S & 48M & 403G & 48.2 & 69.7 & 52.5 & 42.8 & 66.4 & 45.9 \\\\ VMamba-T & 42M & 286G & 48.5 & 69.9 & 52.9 & 43.2 & 66.8 & 46.3 \\\\ \\hline LocalVMamba-T & 45M & 291G & 48.7 & 70.1 & 53.0 & 43.4 & 67.0 & 46.4 \\\\ \\hline Swin-S & 69M & 354G & 48.2 & 69.8 & 52.8 & 43.2 & 67.0 & 46.1 \\\\ ConvNeXt-S & 70M & 348G & 47.9 & 70.0 & 52.7 & 42.9 & 66.9 & 46.2 \\\\ VMamba-S & 64M & 400G & 49.7 & 70.4 & 54.2 & 44.0 & 67.6 & 47.3 \\\\ \\hline LocalVMamba-S & 69M & 414G & 49.9 & 70.5 & 54.4 & 44.1 & 67.8 & 47.4 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Object detection and instance segmentation results on COCO _val_ set.\n' +
      '\n' +
      '**Results.** We report the results of both LocalVim and LocalVMamba in Table 4. On LocalVim, we achieve significant improvements over the baseline Vim-Ti. For example, with a similar amount of parameters, our LocalVim-S outperforms Vim-S by 1.5 on mIoU (SS). While on LocalVMamba, we achieve significant improvements over the VMamba baseline; _e.g._, our LocalVMamba-T achieves a remarkable mIoU (MS) of 49.1, surpassing VMamba-T by 0.8. Compared to CNNs and ViTs, our improvements are more obvious. The results demonstrate the efficacy of the global representation of SSMs in dense prediction tasks.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**Effect of local scan.** The impact of our local scan technique is assessed, with experiments detailed in Table 5. Substituting Vim-T\'s traditional horizontal scan with our local scan yielded a 1% performance boost over the baseline. A combination of scan directions under a constrained FLOP budget in LocalVim-T\\({}^{*}\\) led to an additional 1.1% accuracy increase. These results underscore the varied impacts of scanning with different window sizes (considering the horizontal scan as a local scan with a window size of \\(14\\times 14\\)) on image recognition, and an amalgamation of these scans enhances performance further.\n' +
      '\n' +
      '**Effect of SCAttn.** In Table 5, the incorporation of SCAttn into the final LocalVim block facilitated an additional improvement of 0.6%, validating the effectiveness of strategically combining various scan directions. This underscores SCAttn\'s role in enhancing performance by adaptively merging scan directions.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c} \\hline \\hline Backbone & Image size & Params (M) & FLOPs (G) & mIoU (SS) & mIoU (MS) \\\\ \\hline DeiT-Ti & \\(512^{2}\\) & 11 & - & 39.2 & - \\\\ Vim-Ti & \\(512^{2}\\) & 13 & - & 40.2 & - \\\\ LocalVim-T & \\(512^{2}\\) & 36 & 181 & 43.4 & 44.4 \\\\ \\hline ResNet-50 & \\(512^{2}\\) & 67 & 953 & 42.1 & 42.8 \\\\ DeiT-S + MLN & \\(512^{2}\\) & 58 & 1217 & 43.8 & 45.1 \\\\ Swin-T & \\(512^{2}\\) & 60 & 945 & 44.4 & 45.8 \\\\ Vim-S & \\(512^{2}\\) & 46 & - & 44.9 & - \\\\ \\hline LocalVim-S & \\(512^{2}\\) & 58 & 297 & 46.4 & 47.5 \\\\ VMamba-T & \\(512^{2}\\) & 55 & 964 & 47.3 & 48.3 \\\\ LocalVMamba-T & \\(512^{2}\\) & 57 & 970 & 47.9 & 49.1 \\\\ \\hline ResNet-101 & \\(512^{2}\\) & 85 & 1030 & 42.9 & 44.0 \\\\ DeiT-B + MLN & \\(512^{2}\\) & 144 & 2007 & 45.5 & 47.2 \\\\ Swin-S & \\(512^{2}\\) & 81 & 1039 & 47.6 & 49.5 \\\\ VMamba-S & \\(512^{2}\\) & 76 & 1081 & 49.5 & 50.5 \\\\ LocalVMamba-S & \\(512^{2}\\) & 81 & 1095 & 50.0 & 51.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Results of semantic segmentation on ADE20K using UperNet [51]. We measure the mIoU with single-scale (SS) and multi-scale (MS) testings on the _val_ set. The FLOPs are measured with an input size of \\(512\\times 2048\\). -: Vim [60] did not report the FLOPs and mIOU (MS). MLN: multi-level neck.\n' +
      '\n' +
      '**Effect of scan direction search.** Our empirical evaluation, as depicted in Table 2, confirms the significant benefits derived from the scan direction search strategy in the final LocalVim models. These models exhibit marked improvements over versions that merely amalgamate horizontal scans, local scans with a window size of \\(2\\times 2\\), and their mirrored counterparts. For instance, LocalVim-T exhibits a \\(0.4\\%\\) enhancement over LocalVim-T\\({}^{*}\\). This performance gain can be attributed to the methodological selection of scan combinations at each layer, offering a diverse set of options to optimize model efficacy.\n' +
      '\n' +
      '### Visualization of Searched Scan Directions\n' +
      '\n' +
      'Figure 4 presents visualizations of the scanned directions obtained in our models. Observations suggest that within the plain architecture of LocalVim, there is a predilection for employing local scans in both the initial and terminal segments, with intermediate layers favoring global horizontal and vertical scans. Notably, the \\(2\\times 2\\) local scans tend to concentrate towards the network\'s tail, whereas larger \\(7\\times 7\\) scans are prominent towards the network\'s inception. Conversely, the hierarchical structure of LocalVamba exhibits a greater inclination towards local scans compared to LocalVim, with a preference for \\(7\\times 7\\) scans over \\(2\\times 2\\) scans.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'In this paper, we introduce LocalMamba, an innovative approach to visual state space models that significantly enhances the capture of local dependencies within images while maintaining global contextual understanding. Our method leverages windowed selective scanning and scan direction search to significantly improve upon existing models. Extensive experiments across various datasets and tasks have demonstrated the superiority of LocalMamba over traditional CNNs and ViTs, establishing new benchmarks for image classification, object detection, and semantic segmentation. Our findings underscore the importance of scanning mechanisms in visual state space model and open new avenues for research in efficient and effective state space modeling. Future work will explore the scalability of our approach to more complex and diverse visual tasks, as well as the potential integration of more advanced scanning strategies.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c} \\hline Model & Horizontal scan & Local scan & SCAttn & ACC \\\\ \\hline Vim-T & ✓ & & & 73.1 \\\\ Vim-T w/ local scan & & ✓ & & 74.1 \\\\ LocalVim-T\\({}^{*}\\) w/o SCAttn & ✓ & ✓ & & 75.2 \\\\ \\hline LocalVim-T\\({}^{*}\\) & ✓ & ✓ & ✓ & 75.8 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Ablation study of local scan with LocalVim-T\\({}^{*}\\) (no scan direction search, \\(2\\times 2\\) window size) on ImageNet.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on target task and hardware. arXiv preprint arXiv:1812.00332 (2018)\n' +
      '* [2] Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., et al.: Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019)\n' +
      '* [3] Chu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. Advances in Neural Information Processing Systems **34**, 9355-9366 (2021)\n' +
      '* [4] Contributors, M.: MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. [https://github.com/open-mmlab/mmsegmentation](https://github.com/open-mmlab/mmsegmentation) (2020)\n' +
      '* [5] Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmentation strategies from data. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 113-123 (2019)\n' +
      '* [6] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248-255. Ieee (2009)\n' +
      '* [7] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2021), [https://openreview.net/forum?id=YicbFdNTly](https://openreview.net/forum?id=YicbFdNTly) 2,\n' +
      '* [8] Fang, Y., Liao, B., Wang, X., Fang, J., Qi, J., Wu, R., Niu, J., Liu, W.: You only look at one sequence: Rethinking transformer in vision through object detection. Advances in Neural Information Processing Systems **34**, 26183-26197 (2021)\n' +
      '* [9] Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023)\n' +
      '* [10] Gu, A., Goel, K., Gupta, A., Re, C.: On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems **35**, 35971-35983 (2022)\n' +
      '* [11] Gu, A., Goel, K., Re, C.: Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396 (2021)\n' +
      '* [12] Gu, A., Goel, K., Re, C.: Efficiently modeling long sequences with structured state spaces. In: International Conference on Learning Representations (2022), [https://openreview.net/forum?id=uYLFoz1v1AC](https://openreview.net/forum?id=uYLFoz1v1AC) 1\n' +
      '* [13] Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., Re, C.: Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems **34**, 572-585 (2021)\n' +
      '* [14] Guo, H., Li, J., Dai, T., Ouyang, Z., Ren, X., Xia, S.T.: Mambair: A simple baseline for image restoration with state-space model. arXiv preprint arXiv:2402.15648 (2024)\n' +
      '* [15] Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y., Sun, J.: Single path one-shot neural architecture search with uniform sampling. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVI 16. pp. 544-560. Springer (2020)\n' +
      '* [16] Gupta, A., Gu, A., Berant, J.: Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems **35**, 22982-22994 (2022)* [17] Han, K., Wang, Y., Xu, C., Guo, J., Xu, C., Wu, E., Tian, Q.: Ghostnets on heterogeneous devices via cheap operations. International Journal of Computer Vision **130**(4), 1050-1069 (2022)\n' +
      '* [18] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 16000-16009 (2022)\n' +
      '* [19] He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask r-cnn. In: Proceedings of the IEEE international conference on computer vision. pp. 2961-2969 (2017)\n' +
      '* [20] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)\n' +
      '* [21] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 (2017)\n' +
      '* [22] Huang, T., Huang, L., You, S., Wang, F., Qian, C., Xu, C.: Lightvit: Towards light-weight convolution-free vision transformers. arXiv preprint arXiv:2207.05557 (2022)\n' +
      '* [23] Kalman, R.E.: A new approach to linear filtering and prediction problems (1960)\n' +
      '* [24] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems **25** (2012)\n' +
      '* [25] Lee, K., Chang, H., Jiang, L., Zhang, H., Tu, Z., Liu, C.: Vitgan: Training gans with vision transformers. arXiv preprint arXiv:2107.04589 (2021)\n' +
      '* [26] Li, Y., Mao, H., Girshick, R., He, K.: Exploring plain vision transformer backbones for object detection. In: European Conference on Computer Vision. pp. 280-296. Springer (2022)\n' +
      '* [27] Li, Y., Cai, T., Zhang, Y., Chen, D., Dey, D.: What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298 (2022)\n' +
      '* [28] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference on computer vision. pp. 740-755. Springer (2014)\n' +
      '* [29] Liu, H., Simonyan, K., Yang, Y.: DARTS: Differentiable architecture search. In: International Conference on Learning Representations (2019), [https://openreview.net/forum?id=S1eYHoC5FX](https://openreview.net/forum?id=S1eYHoC5FX)\n' +
      '* [30] Liu, J., Yang, H., Zhou, H.Y., Xi, Y., Yu, L., Yu, Y., Liang, Y., Shi, G., Zhang, S., Zheng, H., et al.: Swin-umamba: Mamba-based unet with imagenet-based pre-training. arXiv preprint arXiv:2402.03302 (2024)\n' +
      '* [31] Liu, J., Liu, B., Zhou, H., Li, H., Liu, Y.: Tokenmix: Rethinking image mixing for data augmentation in vision transformers. In: European Conference on Computer Vision. pp. 455-471. Springer (2022)\n' +
      '* [32] Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 (2024)\n' +
      '* [33] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 10012-10022 (2021)\n' +
      '* [34] Ma, J., Li, F., Wang, B.: U-mamba: Enhancing long-range dependency for biomedical image segmentation. arXiv preprint arXiv:2401.04722 (2024)\n' +
      '* [35] Mehta, H., Gupta, A., Cutkosky, A., Neyshabur, B.: Long range language modeling via gated state spaces. In: The Eleventh International Conference on Learning Representations (2023), [https://openreview.net/forum?id=5MKYYCDva](https://openreview.net/forum?id=5MKYYCDva)\n' +
      '* [36] Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao, T., Baccus, S., Re, C.: S4nd: Modeling images and videos as multidimensional signals with state spaces. Advances in neural information processing systems **35**, 2846-2861 (2022)\n' +
      '* [37] Orvieto, A., Smith, S.L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., De, S.: Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349 (2023)\n' +
      '* [38] Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4195-4205 (2023)\n' +
      '* [39] Radosavovic, I., Kosaraju, R.P., Girshick, R., He, K., Dollar, P.: Designing network design spaces. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10428-10436 (2020)\n' +
      '* [40] Ruan, J., Xiang, S.: Vm-unet: Vision mamba unet for medical image segmentation. arXiv preprint arXiv:2402.02491 (2024)\n' +
      '* [41] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted residuals and linear bottlenecks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4510-4520 (2018)\n' +
      '* [42] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)\n' +
      '* [43] Smith, J.T., Warrington, A., Linderman, S.W.: Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933 (2022)\n' +
      '* [44] Strudel, R., Garcia, R., Laptev, I., Schmid, C.: Segmenter: Transformer for semantic segmentation. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 7262-7272 (2021)\n' +
      '* [45] Su, X., You, S., Xie, J., Zheng, M., Wang, F., Qian, C., Zhang, C., Wang, X., Xu, C.: Vitas: Vision transformer architecture search. In: European Conference on Computer Vision. pp. 139-157. Springer Nature Switzerland Cham (2022)\n' +
      '* [46] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training data-efficient image transformers & distillation through attention. In: International conference on machine learning. pp. 10347-10357. PMLR (2021)\n' +
      '* [47] Touvron, H., Cord, M., Jegou, H.: Deit iii: Revenge of the vit. In: European Conference on Computer Vision. pp. 516-533. Springer (2022)\n' +
      '* [48] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems **30** (2017)\n' +
      '* [49] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 568-578 (2021)\n' +
      '* [50] Wang, Y., Xu, C., Xu, C., Xu, C., Tao, D.: Learning versatile filters for efficient convolutional neural networks. Advances in Neural Information Processing Systems **31** (2018)\n' +
      '* [51] Xiao, T., Liu, Y., Zhou, B., Jiang, Y., Sun, J.: Unified perceptual parsing for scene understanding. In: Proceedings of the European conference on computer vision (ECCV). pp. 418-434 (2018)\n' +
      '* [52] Xie, S., Girshick, R., Dollar, P., Tu, Z., He, K.: Aggregated residual transformations for deep neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1492-1500 (2017)* [53] Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., Hu, H.: Simmim: A simple framework for masked image modeling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9653-9663 (2022)\n' +
      '* [54] Xing, Z., Ye, T., Yang, Y., Liu, G., Zhu, L.: Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation. arXiv preprint arXiv:2401.13560 (2024)\n' +
      '* [55] Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Partial channel connections for memory-efficient architecture search. In: International Conference on Learning Representations (2020), [https://openreview.net/forum?id=BJlS634tPr](https://openreview.net/forum?id=BJlS634tPr) 9\n' +
      '* [56] Yang, S., Wang, X., Li, Y., Fang, Y., Fang, J., Liu, W., Zhao, X., Shan, Y.: Temporally efficient vision transformer for video instance segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2885-2895 (2022)\n' +
      '* [57] You, S., Huang, T., Yang, M., Wang, F., Qian, C., Zhang, C.: Greedynas: Towards fast one-shot nas with greedy supernet. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1999-2008 (2020)\n' +
      '* [58] Zhang, B., Gu, S., Zhang, B., Bao, J., Chen, D., Wen, F., Wang, Y., Guo, B.: Styleswin: Transformer-based gan for high-resolution image generation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 11304-11314 (2022)\n' +
      '* [59] Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.: Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision **127**(3), 302-321 (2019)\n' +
      '* [60] Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417 (2024)\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      '### Visualization of Searched Directions on LocalVMamba-S\n' +
      '\n' +
      'We visualize the searched directions of LocalVMamba-S in Figure 5. In this model, with 27 layers in stage 3, more \\(7\\times 7\\) local scans are preferred compared to LocalVMamba-T.\n' +
      '\n' +
      '### Discussions\n' +
      '\n' +
      '**Potential negative impact.** Investigating the effects of the proposed model requires large consumptions on computation resources, which can potentially raise the environmental concerns.\n' +
      '\n' +
      '**Limitations.** The visual state space models with linear-time complexity to the sequence length, show significant improvements especially on large-resolution downstream tasks compared to previous CNNs and ViTs architectures. Nonetheless, the computational framework of SSMs is inherently more intricate than that of convolution and self-attention mechanisms, complicating the efficient execution of parallel computations. Current deep learning frameworks also exhibit limited capability in accelerating SSM computations as efficiently as they do for more established architectures. On a positive note, ongoing efforts in projects such as VMamba5[32] are aimed at enhancing the computational efficiency of selective SSM operations. These initiatives have already realized notable advancements in speed, as evidenced by the improvements over the original implementations documented in Mamba [9].\n' +
      '\n' +
      'Footnote 5: Project page: [https://github.com/MzeroMiko/VMamba](https://github.com/MzeroMiko/VMamba).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>