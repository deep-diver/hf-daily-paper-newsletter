<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# LocalMamba: 윈도우 선택 스캔을 이용한 시각 상태 공간 모델\n' +
      '\n' +
      'Tao Huang\n' +
      '\n' +
      '시드니대학교 컴퓨터학부\n' +
      '\n' +
      'Xiaohuan Pei\n' +
      '\n' +
      '시드니대학교 컴퓨터학부\n' +
      '\n' +
      'Shan You\n' +
      '\n' +
      '2enseTime Research 2\n' +
      '\n' +
      'Fei Wang\n' +
      '\n' +
      '중국과학기술3대학\n' +
      '\n' +
      'Chen Qian\n' +
      '\n' +
      '2enseTime Research 2\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      '시드니대학교 컴퓨터학부\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근 국가 공간 모델, 특히 맘바의 발전은 언어 이해와 같은 작업에 대해 긴 시퀀스를 모델링하는 데 상당한 진전을 보여주었다. 그러나, 비전 태스크에서의 그들의 적용은 전통적인 합성곱 신경망(CNN) 및 비전 트랜스포머(ViT)의 성능을 현저하게 능가하지 못했다. 본 논문은 비전맘바(Vision Mamba, ViM)를 향상시키기 위한 핵심이 시퀀스 모델링을 위한 스캔 방향 최적화에 있다고 가정한다. 공간 토큰을 평평하게 하는 전통적인 ViM 접근법은 로컬 2D 종속성의 보존을 간과하여 인접한 토큰 간의 거리를 연장한다. 우리는 이미지를 별개의 창으로 분할하여 전역적 관점을 유지하면서 국소 의존성을 효과적으로 포착하는 새로운 국소 스캐닝 전략을 소개한다. 또한, 서로 다른 네트워크 계층에 걸쳐 스캔 패턴에 대한 다양한 선호도를 인정하고, 각 계층에 대한 최적의 스캔 선택을 독립적으로 검색하여 성능을 실질적으로 향상시키는 동적 방법을 제안한다. 평범한 모델과 계층적 모델 모두에 걸친 광범위한 실험은 이미지 표현을 효과적으로 캡처하는 데 있어 우리의 접근법의 우월성을 강조한다. 예를 들어, 우리의 모델은 동일한 1.5G FLOP를 사용하여 이미지넷에서 Vim-Ti보다 3.1% 크게 능가한다. 코드는 [https://github.com/hunto/LocalMamba](https://github.com/hunto/LocalMamba)에서 이용 가능하다.\n' +
      '\n' +
      '키워드: 일반 비전 모델 이미지 인식 상태 공간 모델\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '구조화된 상태 공간 모델(Structured State Space Models, SSM)은 최근 시퀀스 모델링의 다용도 아키텍처로 주목받고 있으며, 계산 효율과 모델 범용성의 균형을 맞추는 새로운 시대를 예고하고 있다[12, 13, 9, 35]. 이 모델들은 순환신경망(RNNs)과 합성곱신경망(CNNs)의 가장 좋은 속성을 합성하여 고전적 상태공간 모델의 기본 원리로부터 영감을 얻는다[23]. 계산 효율성을 특징으로 하는 SSM은 서열 길이를 갖는 선형 또는 거의 선형 스케일링 복잡성을 나타내어 긴 서열을 처리하는 데 특히 적합하다. 선택적 스캐닝(S6)을 통합한 새로운 변형인 맘바[9]의 성공에 이어, 광범위한 비전 작업에 SSM을 적용하는 것이 급증했다. 이러한 응용은 일반적인 기초 모델[32, 60] 개발에서 이미지 분할[30, 34, 40, 54] 및 합성[14] 분야의 발전까지 확장되어 모델의 적응성과 시각적 도메인에서의 잠재력을 보여준다.\n' +
      '\n' +
      '일반적으로 이러한 비전 연구는 SSM 기반 처리를 위해 2D 이미지를 1D 시퀀스로 변환한 다음 특정 작업을 위한 기본 모델에 맘바의 원래 SSM 구조를 통합해야 한다. 그럼에도 불구하고, 그들은 전통적인 CNN[20, 24, 41, 42, 52] 및 비전 트랜스포머(ViTs)[3, 3, 7, 46]에 비해 약간의 개선만을 보여주었다. 이러한 완만한 발전은 중요한 도전을 강조한다: 이미지에서 2D 공간 패턴의 비인과적 특성은 본질적으로 SSM의 인과적 처리 프레임워크와 상충한다. 그림 1에서 알 수 있듯이 공간 데이터를 1D 토큰으로 평평하게 만드는 전통적인 방법은 자연스러운 국부적 2D 의존성을 방해하여 공간 관계를 정확하게 해석하는 모델의 능력을 약화시킨다. V Mamba[32]는 수평 및 수직 방향 모두에서 이미지를 스캐닝함으로써 이를 해결하기 위해 2D 스캐닝 기술을 도입하지만, 여전히 스캐닝된 시퀀스 내에서 원래 인접한 토큰의 근접성을 유지하는 데 어려움을 겪고 있으며, 이는 효과적인 로컬 표현 모델링에 중요하다.\n' +
      '\n' +
      '본 논문에서는 영상을 여러 개의 별개의 로컬 윈도우로 분할함으로써 비전맘바(Vision Mamba, ViM) 내의 로컬 표현을 개선하는 새로운 방법을 소개한다. 각 창은 횡단하기 전에 개별적으로 스캔됩니다.\n' +
      '\n' +
      '도 1: 스캔 방법의 일러스트레이션. (a) 및 (b): 이전 방법들 Vim[60] 및 VMamba[32]는 전체 행 또는 열 축을 횡단하여, 동일한 의미 영역(_e.g._, 이미지 내의 왼쪽 눈) 내의 이웃하는 픽셀들 사이의 의존성을 포착하기 위한 상당한 거리들을 초래한다. (c) 우리는 토큰들을 별개의 윈도우들로 분할하고, 각각의 윈도우 내에서 탐색을 용이하게 하는 새로운 스캔 방법을 소개한다(윈도우 크기는 여기서 \\(3\\times 3\\). 이 접근 방식은 로컬 종속성을 캡처하는 기능을 향상시킵니다.\n' +
      '\n' +
      '그림 2: 로컬 스캔 메커니즘으로 원래 스캔을 확장함으로써, 본 방법은 유사한 FLOP를 유지하면서 Vim[60]의 이미지넷 정확도를 크게 향상시킨다.\n' +
      '\n' +
      ' 윈도우들을 가로질러, 동일한 2D 의미 영역 내의 토큰들이 밀접하게 함께 처리되도록 보장한다. 이 방법은 지역 간 세부 정보를 포착할 수 있는 모델의 능력을 크게 향상시키며, 실험 결과는 그림 2에서 검증되었다. 우리는 전통적인 전역 스캐닝 방향과 새로운 지역 스캐닝 기법을 통합하여 기반 블록을 설계함으로써 포괄적인 전역 및 미묘한 지역 정보를 동화할 수 있는 모델을 강화한다. 또한, 이러한 다양한 스캐닝 프로세스로부터 더 나은 특징을 수집하기 위해, 중복성을 필터링하면서 가치 있는 정보를 식별 및 강조하도록 설계된 공간 및 채널 주의 모듈 SCAttn을 제안한다.\n' +
      '\n' +
      '특징 표현에 대한 스캐닝 방향의 뚜렷한 영향(예를 들어, 윈도우 크기가 3인 로컬 스캔은 더 작은 객체 또는 세부 사항을 캡처하는 데 탁월하지만, 윈도우 크기가 7인 로컬 스캔은 더 큰 객체에 더 적합함)을 인정하여 최적의 스캐닝 방향을 선택하기 위한 방향 검색 방법을 소개한다. 이러한 변동성은 특히 다른 계층 및 네트워크 깊이에서 두드러집니다. DARTS[29]에서 영감을 얻은 우리는 단일 네트워크 내에서 여러 스캐닝 방향을 통합하기 위해 이산 선택에서 학습 가능한 요소로 표시되는 연속 영역으로 진행한다. 이 네트워크의 트레이닝 후에, 가장 효과적인 스캐닝 방향들은 가장 높은 할당된 확률들을 갖는 방향들을 식별함으로써 결정된다.\n' +
      '\n' +
      '개발된 모델인 LocalVim 및 LocalV Mamba는 일반 구조와 계층적 구조를 모두 통합하여 이전 방법에 비해 눈에 띄는 향상을 가져왔다. 본 연구의 주요 기여도는 다음과 같다.\n' +
      '\n' +
      '1. 우리는 SSM을 위한 새로운 스캐닝 방법론을 소개하고, 별개의 윈도우 내에서 국부적 스캐닝을 포함하여, 글로벌 컨텍스트와 함께 상세한 로컬 정보를 캡처하는 우리의 모델의 능력을 상당히 향상시킨다.\n' +
      '2. 서로 다른 네트워크 계층에서 스캐닝 방향을 탐색할 수 있는 방법을 개발하여 가장 효과적인 스캐닝 조합을 식별하고 적용할 수 있게 하여 네트워크 성능을 향상시킨다.\n' +
      '3. 본 논문에서는 일반 구조 및 계층 구조로 설계된 두 가지 모델 변형을 제시한다. 이미지 분류, 객체 검출 및 의미론적 분할 작업에 대한 광범위한 실험을 통해 제안된 모델이 이전 작업보다 상당한 개선을 달성함을 입증한다. 예를 들어, 유사한 양의 파라미터를 갖는 시맨틱 세분화 태스크에서, 우리의 LocalVim-S는 mIoU(SS) 상에서 1.5의 큰 마진만큼 Vim-S를 능가한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '일반 시각 백본 디자인\n' +
      '\n' +
      '지난 10년 동안 컴퓨터 비전의 변혁적 발전은 주로 심층 신경망의 진화와 기초 일반 모델의 출현에 의해 이루어졌다. 초기에는 CNN(Convolutional Neural Networks)[17, 20, 24, 39, 41, 42, 50, 52]가 시각적 모델 아키텍처에서 중요한 이정표를 표시하여 복잡한 이미지 인식 및 분석 작업의 단계를 설정했다.\n' +
      '\n' +
      '이 중 초석 잔차 연결 기법을 적용한 ResNet[20]은 광범위한 비전 작업 분야에서 널리 사용되는 모델 중 하나이며, MobileNet[21, 41] 시리즈는 깊이-와이즈 컨볼루션을 활용하여 경량 모델의 설계를 주도한다. 그러나 비전 트랜스포머(ViT) [7]의 도입은 도메인에서 CNN의 패권에 도전하면서 패러다임의 변화를 표시했다. ViT는 이미지를 일련의 순차적 패치로 분할하고 트랜스포머 아키텍처의 핵심 구성요소인 자체 주의 메커니즘을 활용하여 특징을 추출함으로써 이미지 처리에 대한 접근 방식을 혁신한다[48]. 이 새로운 방법론은 시각적 작업에서 트랜스포머의 미개발된 잠재력을 강조하여 아키텍처 설계[45] 및 교육 방법론[18, 31, 46, 47, 53]을 개선하고 계산 효율성을 높이고 적용 범위를 확장하기 위한 연구의 급증을 촉발했다[3, 22, 33, 49], [58, 25, 26, 38, 44, 26, 8]. 상태 공간 모델(SSM)의 변형인 Mamba[9]를 사용한 긴 시퀀스 모델링의 성공을 기반으로 Vim[60] 및 V Mamba[32]와 같은 일부 혁신적인 모델이 시각적 작업, 즉 Vision Mamba에 도입되었다. 이 모델은 Mamba 프레임워크를 적용하여 비전 애플리케이션을 위한 다목적 백본 역할을 하며 고해상도 이미지에서 기존 CNN 및 ViT보다 우수한 효율성과 정확성을 보여줍니다.\n' +
      '\n' +
      'State Space Models\n' +
      '\n' +
      '상태 공간 모델(State Space Models; SSMs) [11, 13, 16, 27, 37]은 긴 종속성 토큰을 관리하는 데 능숙한 시퀀스 간 변환을 위해 설계된 아키텍처의 패러다임을 나타낸다. 훈련의 초기 도전에도 불구하고 계산 및 기억 강도 때문에 최근의 발전[9, 10, 11, 16, 43]은 이러한 문제를 크게 개선하여 심층 SSM을 CNN 및 트랜스포머에 대한 강력한 경쟁자로 포지셔닝했다. 특히, S4[11]은 효율적인 Normal Plus Low-Rank (NPLR) 표현을 도입하여 빠른 행렬 역산을 위해 Woodbury 아이덴티티를 활용하여 컨볼루션 커널 계산을 능률화 하였다. 이를 기반으로 맘바[9]는 확장 가능하고 하드웨어 최적화된 계산 접근법과 함께 입력 특정 매개변수화를 통합하여 SSM을 더욱 세분화하여 언어 및 유전체학 전반에 걸쳐 광범위한 시퀀스를 처리하는 데 전례 없는 효율성과 단순성을 달성했다.\n' +
      '\n' +
      'S4ND[36]의 출현은 SSM 블록을 시각적 작업으로 초기 추진하여 시각적 데이터를 1D, 2D 및 3D 도메인에 걸쳐 연속적인 신호로 능숙하게 처리했다. 그 후, Mamba 모델의 성공을 바탕으로 Vmamba[32] 및 Vim[60]은 양방향 스캔 및 교차 스캔 메커니즘을 제안함으로써 SSM의 방향성 민감도 도전을 해결하면서 일반적인 비전 작업으로 확장되었다. 일반적인 모델에서 맘바의 기초를 활용하여 이미지 분할[30, 34, 40, 54] 및 이미지 합성[14]과 같은 시각적 작업에 대한 새로운 방법론을 개발하여 복잡한 시각 문제를 해결하는 데 시각적 맘바 모델의 적응성과 효과를 보여준다.\n' +
      '\n' +
      '## 3 Preliminaries\n' +
      '\n' +
      'State Space Models\n' +
      '\n' +
      '구조화된 상태 공간 모델(SSM)은 심층 학습 내의 시퀀스 모델 클래스를 나타내며, 1차원 시퀀스\\(x(t)\\in\\mathbb{R}^{L}\\)를 중간 잠재 상태\\(h(t)\\in\\mathbb{R}^{N}\\)을 통해 \\(y(t)\\in\\mathbb{R}^{L}\\)에 매핑하는 능력을 특징으로 한다.\n' +
      '\n' +
      '\\[\\begin{split}h^{\\prime}(t)&=\\mathbf{A}h(t)+\\mathbf{B}x(t), \\\\y(t)&=\\mathbf{C}h(t),\\end{split}\\tag{1}\\]\n' +
      '\n' +
      '여기서 시스템 행렬 \\(\\mathbf{A}\\in\\mathbb{R}^{N\\times N}\\), \\(\\mathbf{B}\\in\\mathbb{R}^{N\\times 1}\\), \\(\\mathbf{C}\\in\\mathbb{R}^{N\\times 1}\\)은 각각 동역학 및 출력 매핑을 지배한다.\n' +
      '\n' +
      '**discretization.** 실제 구현을 위해, 수학식 1에 의해 설명된 연속 시스템은 제로-차 홀드 가정4를 사용하여 이산화되며, 연속-시간 파라미터들(\\(\\mathbf{A}\\), \\(\\mathbf{B}\\))을 그들의 이산 대응물들(\\(\\mathbf{\\overline{A}\\), \\(\\mathbf{\\overline{B}\\))로 지정된 샘플링 시간스케일 \\(\\mathbf{\\Delta}\\in\\mathbb{R}\\)\\(>0\\):\n' +
      '\n' +
      '각주 4: 이 가정은 표본 간격 \\(\\Delta\\)에 걸쳐 \\(x\\) 상수의 값을 유지한다.\n' +
      '\n' +
      '\\cdot\\mathbf{\\overline{A}}&=e^{\\begin{split}\\mathbf{\\overline{B}}&=(\\mathbf{\\delta A}}^{-1}(e^{\\mathbf{\\Delta A}}-\\mathbf{I}}\\cdot\\mathbf{\\delta B}.\\end{split}\\tag{2}\\\n' +
      '\n' +
      '이것은 다음과 같은 이산화된 모델 제형으로 이어진다:\n' +
      '\n' +
      '\\mathbf{\\overline{A}}h_{t-1}+\\mathbf{\\overline{B}}x_{t},\\\\y_{t}&=\\mathbf{C}h_{t}.\\end{split}\\tag{3}\\\n' +
      '\n' +
      '계산 효율을 위해, 수학식 3에 묘사된 반복 프로세스는 전역 컨볼루션 연산을 채용하는 병렬 계산을 통해 가속화될 수 있다:\n' +
      '\n' +
      '\\mathbf{x}\\oplus\\mathbf{y}&=\\mathbf\\text{with}&\\mathbf{\\overline{K}=(\\mathbf{C}\\mathbf{\\overline{B}}, \\mathbf{C}\\mathbf{\\overline{A}\\mathbf{\\overline{B}},...,\\mathbf{C}\\mathbf{\\overline{A}}^{L-1}\\mathbf{ \\overline{B}},\\end{split}\\tag{4}}}\n' +
      '\n' +
      '여기서 컨벌루션 연산은 \\(\\mathbf{\\overline{K}\\in\\mathbb{R}^{L}\\)이 SSM의 커널 역할을 한다. 이 접근법은 컨볼루션(convolution)을 활용하여 시퀀스에 걸친 출력을 동시에 합성함으로써 계산 효율과 확장성을 향상시킨다.\n' +
      '\n' +
      '선택적 상태공간 모델\n' +
      '\n' +
      '종종 S4로 지칭되는 전통적인 상태 공간 모델(SSM)은 선형 시간 복잡도를 달성했다. 그러나 시퀀스 컨텍스트를 캡처하는 능력은 본질적으로 정적 매개변수화에 의해 제한된다. 이러한 한계를 해결하기 위해 선택적 상태 공간 모델(테밍된 맘바) [9]는 순차적 상태 간의 상호 작용에 대한 동적 및 선택적 메커니즘을 도입한다. Mamba 모델은 상수 전이 매개변수\\((\\overline{\\mathbf{A}},\\overline{\\mathbf{B}})를 사용하는 기존의 SSM과 달리 입력 종속 매개변수를 사용하여 더 풍부한 시퀀스 인식 매개변수화를 가능하게 한다. 구체적으로, Mamba 모델은 입력 시퀀스 \\(\\mathbf{B}\\in\\mathbb{R}^{B\\times L\\times D}\\), \\(\\mathbf{C}\\in\\mathbb{R}^{B\\times L\\times N}\\), \\(\\mathbf{C}\\in\\mathbb{R}^{B\\times N}\\) 및 \\(\\mathbf{\\Delta}\\in\\mathbb{R}^{B\\times L\\times D}\\)을 직접 입력 시퀀스 \\(\\mathbf{x}\\in\\mathbb{R}^{B\\times L\\times D}\\)으로부터 매개변수를 계산한다.\n' +
      '\n' +
      'Mamba 모델은 선택적 SSM을 활용하여 시퀀스 길이의 선형 확장성을 달성할 뿐만 아니라 언어 모델링 작업에서 경쟁력 있는 성능을 제공한다. 이러한 성공은 Mamba를 기초 비전 모델에 통합하는 것을 제안하는 연구와 함께 비전 작업에서 후속 응용 프로그램에 영감을 주었다. Vim[60]은 전통적인 변압기 블록 대신 양방향 맘바 블록을 통합하는 ViT 유사 아키텍처를 채택한다. V Mamba[32]는 수평 및 수직 방향 모두에서 이미지를 스캔하는 새로운 2D 선택적 스캔 기술을 도입하고, Swin Transformer[33]와 유사한 계층적 모델을 구성한다. 우리의 연구는 개선된 성능 결과를 달성하는 시각 작업에 대한 S6 적응을 최적화하는 데 중점을 두고 이러한 초기 탐색을 확장한다.\n' +
      '\n' +
      '## 4 Methodology\n' +
      '\n' +
      '이 섹션에서는 이미지에서 세밀한 세부 정보를 캐는 모델의 능력을 향상시키기 위해 설계된 로컬 스캔 메커니즘으로 시작하여 로컬맘바의 핵심 구성 요소를 설명한다. 이어서, 서로 다른 레이어에 걸쳐 최적의 스캐닝 시퀀스를 식별하는 혁신적인 접근 방법인 스캔 방향 탐색 알고리즘을 도입하여 글로벌 및 로컬 비주얼 큐의 조화로운 통합을 보장한다. 이 섹션의 마지막 부분은 간단한 일반 아키텍처와 복잡한 계층 아키텍처 모두에서 로컬맘바 프레임워크의 배치를 보여주며 다양한 환경에서 범용성과 효율성을 보여준다.\n' +
      '\n' +
      '도 3: (a) LocalVim 모델의 구조. (b) 제안된 공간 및 채널 주의 모듈(SCAttn)의 일러스트레이션.\n' +
      '\n' +
      '시각적 표현을 위한### 로컬 스캔\n' +
      '\n' +
      '이 방법은 선택적 스캔 메커니즘인 S6을 사용하여 1D 인과 순차 데이터를 처리하는 데 탁월한 성능을 보여주었다. 이 메커니즘은 순차적인 단어 간의 종속성을 이해하는 데 필수적인 언어 모델링과 유사하게 스캔된 세그먼트 내에서 중요한 정보를 효과적으로 캡처하는 인과적으로 입력을 처리한다. 그러나, 이미지 내의 2D 공간 데이터의 고유한 비인과적 특성은 이러한 인과적 처리 접근법에 상당한 도전을 제기한다. 공간 토큰을 평평하게 만드는 전통적인 전략은 로컬 2D 종속성의 무결성을 손상시켜 공간 관계를 효과적으로 식별할 수 있는 모델의 능력을 감소시킨다. 예를 들어, 그림 1(a) 및 (b)에 묘사된 바와 같이, Vim[60]에서 사용되는 평탄화 접근법은 이러한 로컬 종속성을 방해하여 수직으로 인접한 토큰 사이의 거리를 크게 증가시키고 로컬 뉘앙스를 캡처하는 모델의 능력을 방해한다. VMDA[32]는 수평 및 수직 방향 모두에서 이미지를 스캔하여 이를 해결하려고 시도하지만, 여전히 단일 스캔에서 공간 영역을 종합적으로 처리하는 데 미치지 못한다.\n' +
      '\n' +
      '이러한 한계를 해결하기 위해 우리는 국부적으로 이미지를 스캔하는 새로운 접근법을 소개한다. 이미지들을 다수의 별개의 로컬 윈도우들로 분할함으로써, 본 방법은 관련 로컬 토큰들의 더 가까운 배열을 보장하여 로컬 종속성들의 캡처(capture)를 향상시킨다. 이 기술은 그림 1(c)에 나와 있으며, 우리의 접근법과 공간적 일관성을 보존하지 못하는 이전 방법을 대조한다.\n' +
      '\n' +
      '우리의 방법은 각 지역 내에서 지역 의존성을 효과적으로 포착하는 데 탁월하지만 글로벌 컨텍스트의 중요성을 인정한다. 이를 위해, 기존의 (a) 방향과 (c) 방향의 선택 스캔 메커니즘을 통합하고, 꼬리부터 머리까지의 스캔을 용이하게 하는 플립된 대응물과 함께, 비인과적 이미지 토큰의 더 나은 모델링을 위해 Vim과 VMDA 모두에서 플립된 방향을 채택하여 기본 블록을 구성한다. 이러한 다각적인 접근법은 각 선택적 스캔 블록 내에서 포괄적인 분석을 보장하여 로컬 세부 사항과 글로벌 관점 간의 균형을 이룬다.\n' +
      '\n' +
      '그림 3에 도시된 바와 같이, 우리의 블록은 4개의 별개의 선택적 스캔 분기를 통해 각각의 입력 이미지 특징을 처리한다. 이러한 브랜치는 독립적으로 관련 정보를 캡처하고, 이는 후속적으로 통합된 피쳐 출력으로 병합된다. 다양한 기능의 통합을 강화하고 외부 정보를 제거하기 위해 병합하기 전에 공간 및 채널 주의 모듈을 소개한다. 도 2의 (b)에 도시된 바와 같이, 이 모듈은 채널 어텐션 브랜치와 공간 어텐션 브랜치의 두 개의 주요 컴포넌트들을 포함하는, 각 브랜치의 특징들 내에서 채널들 및 토큰들에 적응적으로 가중치를 부여한다. 채널 어텐션 브랜치는 공간 차원에 걸쳐 입력 피처들을 평균화함으로써 전역 표현들을 집계하고, 이어서 채널 가중치들을 결정하기 위해 선형 변환을 적용한다. 반대로, 공간 주의 메커니즘은 각 토큰의 특징을 전역 표현으로 증강하여 토큰별 유의성을 평가하여 미묘한 중요도 가중치 특징 추출을 가능하게 한다.\n' +
      '\n' +
      '**Remark.** Swin Transformer[33]와 같은 일부 ViT 변종이 이미지를 더 작은 창으로 분할할 것을 제안하지만, 로컬맘바의 로컬 스캔은 목적과 효과 모두에서 구별된다. ViT의 창형 자기 주의는 일부 글로벌 주의 능력을 희생하더라도 주로 글로벌 자기 주의의 계산 효율성을 다룬다. 반대로, 우리의 로컬 스캔 메커니즘은 시각적 맘바의 로컬 영역 의존성의 모델링을 향상시키기 위해 토큰 위치를 재배열하는 것을 목표로 하는 반면, 전체 이미지가 SSM에 의해 여전히 집계되고 처리됨에 따라 글로벌 이해 능력은 유지된다.\n' +
      '\n' +
      '### 적응적 스캔을 위한 검색\n' +
      '\n' +
      '이미지 표현들을 캡처하는 데 있어서 구조화된 상태 공간 모델(SSM)의 효능은 상이한 스캔 방향들에 걸쳐 변한다. 최적의 성능을 달성하는 것은 이전에 논의된 4-분기 로컬 선택적 스캔 블록과 유사하게 다양한 방향에 걸쳐 다중 스캔을 사용하는 것을 직관적으로 제안한다. 그러나, 이러한 접근법은 계산 요구를 실질적으로 증가시킨다. 이를 해결하기 위해 각 레이어에 가장 적합한 스캔 방향을 효율적으로 선택하여 과도한 계산 비용을 발생시키지 않고 성능을 최적화하는 전략을 소개한다. 이 방법은 각 레이어에 대한 최적의 스캐닝 구성을 검색하여 맞춤화되고 효율적인 표현 모델링을 보장하는 것을 포함한다.\n' +
      '\n' +
      '**Search space.** 각 레이어에 대한 스캐닝 과정을 맞추기 위해 8개의 후보 스캔 방향의 다양한 집합 \\(\\mathcal{S}\\)을 소개한다. 여기에는 수평 및 수직 스캔(표준 및 뒤집기 모두)과 창 크기가 2 및 7인 로컬 스캔(표준 및 뒤집기 모두)이 포함됩니다. 이전 모델과 같은 일관된 계산 예산을 위해 각 레이어에 대해 이 8개 방향 중 4개를 선택한다. 이 방법은 전체 블록 수를 나타내는 \\((C_{8}^{4})^{K}\\)의 실질적인 탐색 공간을 생성한다.\n' +
      '\n' +
      'DARTS의 원리를 기반으로 [29] 본 방법은 범주형 선택을 탐색하기 위해 연속 이완을 사용하여 스캔 방향에 대해 미분 가능한 검색 메커니즘을 적용한다. 이 접근 방식은 이산 선택을 변환합니다.\n' +
      '\n' +
      '그림 4: 우리 모델의 탐색된 방향을 시각화합니다. LocalVLambda-S의 시각화는 섹션 A.2에 있다.\n' +
      '\n' +
      '연속 도메인 내로 프로세스하여, 스캔 방향들의 선택을 나타내기 위해 소프트맥스 확률들의 사용을 허용한다:\n' +
      '\n' +
      '\\sum_{s\\in\\mathcal{S}\\frac{\\exp(\\alpha_{s}^{(l)}}{\\sum_{s^{\\prime}\\mathcal{S}}\\exp(\\alpha_{s^{\\prime}}^{(l)}}\\text{SSM}_{s}(\\mathbf{x}^{(l)}), \\tag{5}\\text{\n' +
      '\n' +
      '여기서 \\(\\mathbf{\\alpha}^{(l)}\\)는 모든 잠재적 스캔 방향에 대한 소프트맥스 확률을 반영하는 각 레이어 \\(l\\)에 대한 학습 가능한 매개변수 세트를 나타낸다.\n' +
      '\n' +
      '우리는 전체 탐색 공간을 오버파라미터화된 네트워크로 구성하여, 표준 훈련 프로토콜에 따라 네트워크 파라미터와 아키텍처 변수를 동시에 최적화할 수 있다. 훈련이 완료되면 소프트맥스 확률이 가장 높은 4개의 방향을 선택하여 최적의 방향 옵션을 도출한다. 검색된 모델의 방향을 그림 4에서 시각화합니다. 검색 결과에 대한 자세한 분석은 섹션 5.5를 참조하십시오.\n' +
      '\n' +
      '**방향 검색의 확장성** 현재 접근 방식은 훈련에서 선택을 위해 모든 스캔 방향을 집계하여 적당한 범위의 옵션을 가진 모델을 적절하게 제공합니다. 예를 들어, 블록당 20개의 블록과 128개의 방향을 특징으로 하는 모델은 28GB의 GPU 메모리를 필요로 하며, 이는 광범위한 선택에 대한 확장성 한계를 나타낸다. 방대한 선택 배열을 가진 시나리오에서 메모리 소비를 완화하기 위해 단일 경로 샘플링[15, 57], 이진 근사[1] 및 부분 채널 사용[55]과 같은 기술이 실행 가능한 솔루션을 제시한다. 우리는 더 적응적인 방향 전략과 고급 검색 기술에 대한 조사를 향후 노력에 맡긴다.\n' +
      '\n' +
      '### Architecture Variants\n' +
      '\n' +
      '방법론의 효율성을 철저히 평가하기 위해 각각 LocalVim 및 LocalVamba로 명명된 일반 [60] 및 계층 [32] 구조 모두에 기반을 둔 아키텍처 변형을 소개한다. 이러한 아키텍처의 구성은 표 1에 상세히 설명되어 있다. 구체적으로, LocalVim에서, 표준 SSM 블록은 도 3에 도시된 바와 같이, 우리의 LocalVim 블록으로 대체된다. 원래의 Vim 블록이 두 개의 스캐닝 방향(수평 및 플립)을 포함하는 것을 고려하면\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c} \\hline Model & \\#Dims & \\#Blocks & Params & FLOPs \\\\ \\hline LocalVim-T & 192 & 20 & 8M & 1.5G \\\\ LocalVim-S & 384 & 20 & 28M & 4.8G \\\\ \\hline LocalVAMaba-T & [96, 192, 384, 768] & [2, 2, 9, 2] & 26M & 5.7G \\\\ LocalVAMaba-S & [96, 192, 384, 768] & [2, 2, 27, 2] & 50M & 11.4G \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 아키텍처 변형들. 우리는 Vim과 V Mamba의 원래 구조 설계를 따르며, 여기서 Vim은 스트라이드 16의 패치 임베딩을 갖는 플레인 구조를 사용하는 반면 V Mamba는 스트라이드 4, 8, 16 및 32에 SSM 단계를 갖는 계층적 구조를 구성한다.\n' +
      '\n' +
      '수평) 및 LocalVim은 네 가지 스캐닝 방향을 도입하여 계산 오버헤드를 증가시킨다. 본 논문에서는 유사한 계산량을 유지하기 위해 Vim 블록의 수를 24개에서 20개로 조정하였으며, LocalVAMaba는 본 논문에서 제안한 모델과 유사한 4개의 스캐닝 방향을 가지고 있어 구조적 구성을 변경하지 않고 블록을 직접 교체하였다.\n' +
      '\n' +
      '**계산 비용 분석** 우리의 LocalMamba 블록은 계산 비용의 미미한 증가만으로 효율적이고 효과적이다. 토큰의 재배치만을 포함하는 스캐닝 메커니즘은 FLOP 측면에서 추가적인 계산 비용을 발생시키지 않는다. 또한, 스캔에 걸쳐 다양한 정보의 효율적인 집계를 위해 설계된 SCAttn 모듈은 예외적으로 간소화된다. 선형 레이어를 활용하여 토큰 차원을 \\(1/r\\)의 비율로 축소한 후, 모든 모델에 대해 \\(r\\)을 8로 설정하여 공간 차원과 채널 차원에 걸쳐 주의 가중치를 생성한다. 예를 들어, VAMaba 블록을 LocalMamba 블록으로 대체하는 LocalVAMaba-T 모델은 VAMava-T의 FLOP만 5.6G에서 5.7G로 증가시킨다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '이 섹션에서는 이미지넷 분류 작업을 시작으로 훈련된 모델을 객체 탐지 및 의미 분할을 포함한 다양한 다운스트림 작업으로 이전하여 실험 평가를 요약한다.\n' +
      '\n' +
      '### ImageNet Classification\n' +
      '\n' +
      '#### 5.1.1 훈련 전략.\n' +
      '\n' +
      '우리는 ImageNet-1K 데이터셋[6]에서 모델을 학습하고 ImageNet-1K 유효성 검사 세트에서 성능을 평가한다. 기존 연구들[32, 33, 46, 60]에 이어서, 기본 배치 크기가 1024이고 AdamW 최적화기를 사용하여 300개의 에폭에 대한 모델을 학습하고, 코사인 어닐링 학습 속도 스케쥴을 초기값 \\(10^{-3}\\)과 20-에폭 워밍업으로 채택한다. 학습 데이터 증강을 위해 랜덤 크로핑, 정책 _rand-m9-mstd0.5_를 갖는 AutoAugment[5] 및 각 이미지에 0.25의 확률로 픽셀을 랜덤 소거한 후, 각 배치에서 비율 0.2의 MixUp 전략을 채택한다. 모델의 지수 이동 평균은 붕괴율 0.9999로 채택되었다.\n' +
      '\n' +
      '**스캔 방향 검색.** 슈퍼넷 트레이닝의 경우, 표준 이미지넷 트레이닝과 일치하는 다른 하이퍼 파라미터를 유지하면서 에폭의 수를 100으로 줄인다. 로컬Vim 변형에서 슈퍼넷에 대한 임베딩 차원은 128로 설정되며, 검색 동작은 균일한 레이어 구조로 인해 로컬Vim-T 및 로컬Vim-S에서 동일하게 수행된다. LocalVAMaba-T 및 LocalVAMaba-S를 포함하는 LocalV-Mamba 변이체의 경우, 초기 임베딩 차원이 32로 최소화되어 검색 프로세스를 용이하게 한다.\n' +
      '\n' +
      '**결과.** 표 2에 요약된 우리의 결과는 전통적인 CNN 및 ViT 방법론에 비해 상당한 정확도 향상을 보여준다. 특히 LocalVim-T는 1.5G FLOP로 76.2%의 정확도를 달성하여 72.2%의 정확도를 기록하는 DeiT-Ti를 능가한다. 계층 구조에서 LocalVAMaba-T의 82.7% 정확도는 Swin-T보다 1.4% 더 우수하다. 게다가, 우리의 s와 비교된다. 제안된 방법은 Vim과 VMamba라는 중요한 기여도이며, 본 논문에서 제안하는 방법은 LocalVim-T와 LocalVAMaba-T가 Vim-Ti와 VMamba-T를 각각 2.7%와 0.5%의 정확도로 능가하는 상당한 이득을 기록한다. 또한, 로컬 스캔의 유효성을 검증하기 위해 표에서 \\({}^{*}\\)으로 표시된 섹션 4.1에 설명된 스캔 방향 검색이 없는 모델에 대해 추가 실험을 수행했다. 로컬 스캔만을 원래의 Vim 프레임워크에 통합하면 로컬Vim-T\\({}^{*}\\)이 Vim-Ti를 2.7% 능가하는 반면 완전한 방법론은 정확도를 0.4% 더 높인다. 이러한 결과는 시각적 SSM에서 스캔 방향의 중추적인 역할을 확인시켜 로컬 의존성 캡처를 효과적으로 향상시키는 로컬 스캔 접근법의 능력을 입증한다.\n' +
      '\n' +
      '### Object Detection\n' +
      '\n' +
      '**훈련 전략** MSCOCO 2017 데이터세트 [28] 및 MMDetection 라이브러리 [2]를 사용하여 객체 검출에 대한 성능을 검증한다. LocalVAMaba 시리즈의 경우, Mask-RCNN 검출기로 객체 검출 및 인스턴스 분할 작업을 훈련하기 위해 이전 작업[32, 33]을 따른다[19]. 훈련 전략에는 12개의 훈련 에폭의 1\\(\\times\\) 설정과 36개의 훈련 에폭의 3\\(\\times\\) 설정이 포함된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c} \\hline \\hline Method & Image size & Params (M) & FLOPs (G) & Top-1 ACC (\\%) \\\\ \\hline RegNetY-4G [39] & \\(224^{2}\\) & 21 & 4.0 & 80.0 \\\\ RegNetY-8G [39] & \\(224^{2}\\) & 39 & 8.0 & 81.7 \\\\ RegNetY-16G [39] & \\(224^{2}\\) & 84 & 16.0 & 82.9 \\\\ \\hline ViT-B/16 [7] & \\(384^{2}\\) & 86 & 55.4 & 77.9 \\\\ ViT-L/16 [7] & \\(384^{2}\\) & 307 & 190.7 & 76.5 \\\\ \\hline DeiT-Ti [46] & \\(224^{2}\\) & 6 & 1.3 & 72.2 \\\\ DeiT-S [46] & \\(224^{2}\\) & 22 & 4.6 & 79.8 \\\\ DeiT-B [46] & \\(224^{2}\\) & 86 & 17.5 & 81.8 \\\\ \\hline Swin-T [33] & \\(224^{2}\\) & 29 & 4.5 & 81.3 \\\\ Swin-S [33] & \\(224^{2}\\) & 50 & 8.7 & 83.0 \\\\ Swin-B [33] & \\(224^{2}\\) & 88 & 15.4 & 83.5 \\\\ \\hline Vim-Ti [60] & \\(224^{2}\\) & 7 & 1.5 & 73.1 \\\\ Vim-S [60] & \\(224^{2}\\) & 26 & 5.1 & 80.3 \\\\ \\hline LocalVim-T\\({}^{*}\\) & \\(224^{2}\\) & 8 & 1.5 & 75.8 \\\\ LocalVim-T & \\(224^{2}\\) & 8 & 1.5 & 76.2 \\\\ LocalVim-S\\({}^{*}\\) & \\(224^{2}\\) & 28 & 4.8 & 81.0 \\\\ LocalVim-S & \\(224^{2}\\) & 28 & 4.8 & 81.2 \\\\ \\hline VMamba-T [32] & \\(224^{2}\\) & 22 & 5.6 & 82.2 \\\\ VMamba-S [32] & \\(224^{2}\\) & 44 & 11.2 & 83.5 \\\\ VMamba-B [32] & \\(224^{2}\\) & 75 & 18 & 83.7 \\\\ \\hline LocalVMamba-T & \\(224^{2}\\) & 26 & 5.7 & 82.7 \\\\ LocalVMamba-S & \\(224^{2}\\) & 50 & 11.4 & 83.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: ImageNet-1K 분류상 서로 다른 등뼈의 비교. \\ (*\\): 스캔 방향 탐색이 없는 우리의 모델.\n' +
      '\n' +
      '및 다중 스케일 데이터 증강. LocalVim의 경우 Vim[60]을 따라 ViTDet[26]을 디텍터로 사용하는 캐스케이드 마스크 R-CNN을 사용한다.\n' +
      '\n' +
      '**결과.** 표 3의 다른 백본과의 비교에서 LocalVMamba에 대한 결과를 요약한다. 우리는 우리의 LocalVMamba가 모든 모델 변형에서 VMamba를 일관되게 능가한다는 것을 알 수 있다. 그리고 다른 아키텍처인 CNN 및 ViT와 비교하여 상당한 우위를 확보한다. 예를 들어, LocalVMamba-T는 \\(46.7\\) 박스 AP와 \\(42.2\\) 마스크 AP를 얻었고, Swin-T는 각각 \\(4.0\\)과 \\(2.9\\)의 큰 마진으로 개선되었다. Vim과의 정량적 비교는 보충 자료를 참고하시기 바랍니다.\n' +
      '\n' +
      '### Semantic Segmentation\n' +
      '\n' +
      '**훈련 전략** [32, 33, 60] 다음에 ADE20K [59] 데이터 세트에서 등뼈로 UperNet [51]을 훈련한다. 모델은 총 배치크기가 \\(512\\times 512\\)일 때 \\(16\\)으로 학습되며, AdamW 최적화기는 중량감소 \\(0.01\\)으로 채택된다. 우리는 초기 학습률이 \\(6\\times 10^{-5}\\)인 \\(160\\)K 반복을 소멸시키는 Poly learning rate schedule을 사용한다. Vim은 FLOPs 및 mIoU(MS)를 보고하지 않았고 세그먼트화를 위한 코드를 릴리스했으므로, MMS 세그먼트화 [4]에서 ViT 예제 구성에 따라 우리의 LocalVim을 구현한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c c|c c} \\hline \\multicolumn{8}{c}{**Mask R-CNN 1\\(\\times\\) schedule**} \\\\ \\hline Backbone & Params & FLOPs & AP\\({}^{\\text{b}}\\) & AP\\({}^{\\text{b}}_{50}\\) & AP\\({}^{\\text{b}}_{75}\\) & AP\\({}^{\\text{m}}\\) & AP\\({}^{\\text{m}}_{50}\\) & AP\\({}^{\\text{m}}_{75}\\) \\\\ \\hline ResNet-50 & 44M & 260G & 38.2 & 58.8 & 41.4 & 34.7 & 55.7 & 37.2 \\\\ Swin-T & 48M & 267G & 42.7 & 65.2 & 46.8 & 39.3 & 62.2 & 42.2 \\\\ ConvNeXt-T & 48M & 262G & 44.2 & 66.6 & 48.3 & 40.1 & 63.3 & 42.8 \\\\ ViT-Adapter-S & 48M & 403G & 44.7 & 65.8 & 48.3 & 39.9 & 62.5 & 42.8 \\\\ VMamba-T & 42M & 286G & 46.5 & 68.5 & 50.7 & 42.1 & 65.5 & 45.3 \\\\ LocalVMamba-T & 45M & 291G & 46.7 & 68.7 & 50.8 & 42.2 & 65.7 & 45.5 \\\\ \\hline ResNet-101 & 63M & 336G & 38.2 & 58.8 & 41.4 & 34.7 & 55.7 & 37.2 \\\\ Swin-S & 69M & 354G & 44.8 & 66.6 & 48.9 & 40.9 & 63.2 & 44.2 \\\\ ConvNeXt-S & 70M & 348G & 45.4 & 67.9 & 50.0 & 41.8 & 65.2 & 45.1 \\\\ VMamba-S & 64M & 400G & 48.2 & 69.7 & 52.5 & 43.0 & 66.6 & 46.4 \\\\ \\hline LocalVMamba-S & 69N & 414G & 48.4 & 69.9 & 52.7 & 43.2 & 66.7 & 46.5 \\\\ \\hline \\multicolumn{8}{c}{**Mask R-CNN 3\\(\\times\\) MS schedule**} \\\\ \\hline Swin-T & 48M & 267G & 46.0 & 68.1 & 50.3 & 41.6 & 65.1 & 44.9 \\\\ ConvNeXt-T & 48M & 262G & 46.2 & 67.9 & 50.8 & 41.7 & 65.0 & 44.9 \\\\ ViT-Adapter-S & 48M & 403G & 48.2 & 69.7 & 52.5 & 42.8 & 66.4 & 45.9 \\\\ VMamba-T & 42M & 286G & 48.5 & 69.9 & 52.9 & 43.2 & 66.8 & 46.3 \\\\ \\hline LocalVMamba-T & 45M & 291G & 48.7 & 70.1 & 53.0 & 43.4 & 67.0 & 46.4 \\\\ \\hline Swin-S & 69M & 354G & 48.2 & 69.8 & 52.8 & 43.2 & 67.0 & 46.1 \\\\ ConvNeXt-S & 70M & 348G & 47.9 & 70.0 & 52.7 & 42.9 & 66.9 & 46.2 \\\\ VMamba-S & 64M & 400G & 49.7 & 70.4 & 54.2 & 44.0 & 67.6 & 47.3 \\\\ \\hline LocalVMamba-S & 69M & 414G & 49.9 & 70.5 & 54.4 & 44.1 & 67.8 & 47.4 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: COCO _val_set에 대한 객체 검출 및 인스턴스 분할 결과.\n' +
      '\n' +
      '**결과.** 표 4의 LocalVim과 LocalVMamba의 결과를 보고한다. LocalVim에 대해, 우리는 기준선 Vim-Ti에 비해 상당한 개선을 달성한다. 예를 들어, 유사한 양의 파라미터로, 우리의 LocalVim-S는 mIoU(SS)에서 Vim-S를 1.5만큼 능가한다. LocalVMamba에서는 VMamba 기준선에 비해 상당한 개선을 이루는데, 예를 들어, 우리의 LocalVMamba-T는 VMamba-T를 0.8로 능가하는 49.1의 놀라운 mIoU(MS)를 달성한다. CNN 및 ViT에 비해 우리의 개선은 더 분명하다. 결과는 조밀한 예측 작업에서 SSM의 글로벌 표현의 유효성을 입증한다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**로컬 스캔의 효과** 표 5에 자세히 설명된 실험으로 로컬 스캔 기술의 영향을 평가한다. Vim-T의 전통적인 수평 스캔을 로컬 스캔으로 대체하면 기준선에 비해 1%의 성능 향상이 나타났다. LocalVim-T\\({}^{*}\\)에서 제한된 FLOP 예산 하에서 스캔 방향의 조합은 추가로 1.1%의 정확도를 증가시켰다. 이러한 결과는 윈도우 크기를 달리한 스캔(수평 스캔을 윈도우 크기가 \\(14\\times 14\\)인 로컬 스캔으로 간주)이 이미지 인식에 미치는 다양한 영향을 강조하며, 이러한 스캔의 병합은 성능을 더욱 향상시킨다.\n' +
      '\n' +
      '**SCAttn의 효과.** 표 5에서 SCAttn을 최종 LocalVim 블록에 통합하는 것은 0.6%의 추가 개선을 촉진하여 다양한 스캔 방향을 전략적으로 결합하는 효과를 검증했다. 이는 스캔 방향을 적응적으로 병합하여 성능을 향상시키는 SCAttn의 역할을 강조한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c} \\hline \\hline Backbone & Image size & Params (M) & FLOPs (G) & mIoU (SS) & mIoU (MS) \\\\ \\hline DeiT-Ti & \\(512^{2}\\) & 11 & - & 39.2 & - \\\\ Vim-Ti & \\(512^{2}\\) & 13 & - & 40.2 & - \\\\ LocalVim-T & \\(512^{2}\\) & 36 & 181 & 43.4 & 44.4 \\\\ \\hline ResNet-50 & \\(512^{2}\\) & 67 & 953 & 42.1 & 42.8 \\\\ DeiT-S + MLN & \\(512^{2}\\) & 58 & 1217 & 43.8 & 45.1 \\\\ Swin-T & \\(512^{2}\\) & 60 & 945 & 44.4 & 45.8 \\\\ Vim-S & \\(512^{2}\\) & 46 & - & 44.9 & - \\\\ \\hline LocalVim-S & \\(512^{2}\\) & 58 & 297 & 46.4 & 47.5 \\\\ VMamba-T & \\(512^{2}\\) & 55 & 964 & 47.3 & 48.3 \\\\ LocalVMamba-T & \\(512^{2}\\) & 57 & 970 & 47.9 & 49.1 \\\\ \\hline ResNet-101 & \\(512^{2}\\) & 85 & 1030 & 42.9 & 44.0 \\\\ DeiT-B + MLN & \\(512^{2}\\) & 144 & 2007 & 45.5 & 47.2 \\\\ Swin-S & \\(512^{2}\\) & 81 & 1039 & 47.6 & 49.5 \\\\ VMamba-S & \\(512^{2}\\) & 76 & 1081 & 49.5 & 50.5 \\\\ LocalVMamba-S & \\(512^{2}\\) & 81 & 1095 & 50.0 & 51.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: UperNet을 이용한 ADE20K에 대한 의미론적 분할 결과[51]. 우리는 _val_ 집합에서 단일 스케일(SS) 및 다중 스케일(MS) 테스트로 mIoU를 측정한다. FLOP는 입력크기가 \\(512\\times 2048\\)일 때 측정되었다. -: Vim[60]은 FLOPs 및 mIOU(MS)를 보고하지 않았다. MLN: 멀티 레벨 넥.\n' +
      '\n' +
      '**스캔 방향 탐색의 효과.** 표 2에 표시된 바와 같이 우리의 경험적 평가는 최종 로컬빔 모델에서 스캔 방향 탐색 전략에서 파생된 상당한 이점을 확인한다. 이 모델은 수평 스캔, 윈도우 크기가 \\(2\\times 2\\)인 로컬 스캔 및 미러링된 버전에 비해 현저한 개선을 보여준다. 예를 들어, LocalVim-T는 LocalVim-T({}^{*}\\)에 비해 \\(0.4\\%\\)의 향상을 보인다. 이러한 성능 이득은 각 계층에서 스캔 조합의 방법론적 선택에 기인할 수 있으며, 모델 효능을 최적화하기 위한 다양한 옵션 세트를 제공한다.\n' +
      '\n' +
      '### 검색 검색 방향 시각화\n' +
      '\n' +
      '그림 4는 우리 모델에서 얻은 스캔된 방향의 시각화를 보여준다. 관찰에 따르면 LocalVim의 일반 아키텍처 내에서 초기 세그먼트와 말단 세그먼트 모두에 로컬 스캔을 사용하기 위한 선호가 있으며 중간 계층은 글로벌 수평 및 수직 스캔을 선호한다. 특히, \\(2\\times 2\\) 로컬 스캔은 네트워크의 꼬리에 집중되는 경향이 있는 반면, 더 큰 \\(7\\times 7\\) 스캔은 네트워크의 시작에 두드러진다. 반대로, LocalVamba의 계층적 구조는 LocalVim에 비해 로컬 스캔에 대한 경향이 더 크며, \\(2\\times 2\\) 스캔보다 \\(7\\times 7\\) 스캔을 선호한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 글로벌 맥락적 이해를 유지하면서 이미지 내의 로컬 의존성을 캡처하는 것을 상당히 향상시키는 시각적 상태 공간 모델에 대한 혁신적인 접근 방법인 LocalMamba를 소개한다. 제안된 방법은 윈도우형 선택적 스캔과 스캔 방향 검색을 활용하여 기존 모델을 크게 개선한다. 다양한 데이터 세트와 작업에 걸친 광범위한 실험은 전통적인 CNN 및 ViT에 비해 로컬맘바의 우수성을 입증하여 이미지 분류, 객체 탐지 및 의미론적 세그먼테이션에 대한 새로운 벤치마크를 수립했다. 우리의 연구 결과는 시각적 상태 공간 모델에서 스캐닝 메커니즘의 중요성을 강조하고 효율적이고 효과적인 상태 공간 모델링 연구를 위한 새로운 길을 열어준다. 향후 작업은 보다 복잡하고 다양한 시각적 작업에 대한 접근 방식의 확장성과 보다 고급화된 스캐닝 전략의 잠재적 통합을 탐구할 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c} \\hline Model & Horizontal scan & Local scan & SCAttn & ACC \\\\ \\hline Vim-T & ✓ & & & 73.1 \\\\ Vim-T w/ local scan & & ✓ & & 74.1 \\\\ LocalVim-T\\({}^{*}\\) w/o SCAttn & ✓ & ✓ & & 75.2 \\\\ \\hline LocalVim-T\\({}^{*}\\) & ✓ & ✓ & ✓ & 75.8 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: ImageNet 상에서 LocalVim-T\\({}^{*}\\)(스캔 방향 탐색 없음, \\(2\\times 2\\) 윈도우 크기)을 이용한 국소 스캔의 절제 연구.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on target task and hardware. arXiv preprint arXiv:1812.00332 (2018)\n' +
      '* [2] Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., et al.: Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019)\n' +
      '* [3] Chu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. Advances in Neural Information Processing Systems **34**, 9355-9366 (2021)\n' +
      '* [4] Contributors, M.: MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. [https://github.com/open-mmlab/mmsegmentation](https://github.com/open-mmlab/mmsegmentation) (2020)\n' +
      '* [5] Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmentation strategies from data. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 113-123 (2019)\n' +
      '* [6] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248-255. Ieee (2009)\n' +
      '* [7] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2021), [https://openreview.net/forum?id=YicbFdNTly](https://openreview.net/forum?id=YicbFdNTly) 2,\n' +
      '* [8] Fang, Y., Liao, B., Wang, X., Fang, J., Qi, J., Wu, R., Niu, J., Liu, W.: You only look at one sequence: Rethinking transformer in vision through object detection. Advances in Neural Information Processing Systems **34**, 26183-26197 (2021)\n' +
      '* [9] Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023)\n' +
      '* [10] Gu, A., Goel, K., Gupta, A., Re, C.: On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems **35**, 35971-35983 (2022)\n' +
      '* [11] Gu, A., Goel, K., Re, C.: Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396 (2021)\n' +
      '* [12] Gu, A., Goel, K., Re, C.: Efficiently modeling long sequences with structured state spaces. In: International Conference on Learning Representations (2022), [https://openreview.net/forum?id=uYLFoz1v1AC](https://openreview.net/forum?id=uYLFoz1v1AC) 1\n' +
      '* [13] Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., Re, C.: Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems **34**, 572-585 (2021)\n' +
      '* [14] Guo, H., Li, J., Dai, T., Ouyang, Z., Ren, X., Xia, S.T.: Mambair: A simple baseline for image restoration with state-space model. arXiv preprint arXiv:2402.15648 (2024)\n' +
      '* [15] Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y., Sun, J.: Single path one-shot neural architecture search with uniform sampling. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVI 16. pp. 544-560. Springer (2020)\n' +
      '* [16] Gupta, A., Gu, A., Berant, J.: Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems **35**, 22982-22994 (2022)* [17] Han, K., Wang, Y., Xu, C., Guo, J., Xu, C., Wu, E., Tian, Q.: Ghostnets on heterogeneous devices via cheap operations. International Journal of Computer Vision **130**(4), 1050-1069 (2022)\n' +
      '* [18] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 16000-16009 (2022)\n' +
      '* [19] He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask r-cnn. In: Proceedings of the IEEE international conference on computer vision. pp. 2961-2969 (2017)\n' +
      '* [20] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)\n' +
      '* [21] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 (2017)\n' +
      '* [22] Huang, T., Huang, L., You, S., Wang, F., Qian, C., Xu, C.: Lightvit: Towards light-weight convolution-free vision transformers. arXiv preprint arXiv:2207.05557 (2022)\n' +
      '* [23] Kalman, R.E.: A new approach to linear filtering and prediction problems (1960)\n' +
      '* [24] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems **25** (2012)\n' +
      '* [25] Lee, K., Chang, H., Jiang, L., Zhang, H., Tu, Z., Liu, C.: Vitgan: Training gans with vision transformers. arXiv preprint arXiv:2107.04589 (2021)\n' +
      '* [26] Li, Y., Mao, H., Girshick, R., He, K.: Exploring plain vision transformer backbones for object detection. In: European Conference on Computer Vision. pp. 280-296. Springer (2022)\n' +
      '* [27] Li, Y., Cai, T., Zhang, Y., Chen, D., Dey, D.: What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298 (2022)\n' +
      '* [28] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference on computer vision. pp. 740-755. Springer (2014)\n' +
      '* [29] Liu, H., Simonyan, K., Yang, Y.: DARTS: Differentiable architecture search. In: International Conference on Learning Representations (2019), [https://openreview.net/forum?id=S1eYHoC5FX](https://openreview.net/forum?id=S1eYHoC5FX)\n' +
      '* [30] Liu, J., Yang, H., Zhou, H.Y., Xi, Y., Yu, L., Yu, Y., Liang, Y., Shi, G., Zhang, S., Zheng, H., et al.: Swin-umamba: Mamba-based unet with imagenet-based pre-training. arXiv preprint arXiv:2402.03302 (2024)\n' +
      '* [31] Liu, J., Liu, B., Zhou, H., Li, H., Liu, Y.: Tokenmix: Rethinking image mixing for data augmentation in vision transformers. In: European Conference on Computer Vision. pp. 455-471. Springer (2022)\n' +
      '* [32] Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 (2024)\n' +
      '* [33] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 10012-10022 (2021)\n' +
      '* [34] Ma, J., Li, F., Wang, B.: U-mamba: Enhancing long-range dependency for biomedical image segmentation. arXiv preprint arXiv:2401.04722 (2024)\n' +
      '* [35] Mehta, H., Gupta, A., Cutkosky, A., Neyshabur, B.: Long range language modeling via gated state spaces. In: The Eleventh International Conference on Learning Representations (2023), [https://openreview.net/forum?id=5MKYYCDva](https://openreview.net/forum?id=5MKYYCDva)\n' +
      '* [36] Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao, T., Baccus, S., Re, C.: S4nd: Modeling images and videos as multidimensional signals with state spaces. Advances in neural information processing systems **35**, 2846-2861 (2022)\n' +
      '* [37] Orvieto, A., Smith, S.L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., De, S.: Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349 (2023)\n' +
      '* [38] Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4195-4205 (2023)\n' +
      '* [39] Radosavovic, I., Kosaraju, R.P., Girshick, R., He, K., Dollar, P.: Designing network design spaces. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10428-10436 (2020)\n' +
      '* [40] Ruan, J., Xiang, S.: Vm-unet: Vision mamba unet for medical image segmentation. arXiv preprint arXiv:2402.02491 (2024)\n' +
      '* [41] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted residuals and linear bottlenecks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4510-4520 (2018)\n' +
      '* [42] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)\n' +
      '* [43] Smith, J.T., Warrington, A., Linderman, S.W.: Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933 (2022)\n' +
      '* [44] Strudel, R., Garcia, R., Laptev, I., Schmid, C.: Segmenter: Transformer for semantic segmentation. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 7262-7272 (2021)\n' +
      '* [45] Su, X., You, S., Xie, J., Zheng, M., Wang, F., Qian, C., Zhang, C., Wang, X., Xu, C.: Vitas: Vision transformer architecture search. In: European Conference on Computer Vision. pp. 139-157. Springer Nature Switzerland Cham (2022)\n' +
      '* [46] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training data-efficient image transformers & distillation through attention. In: International conference on machine learning. pp. 10347-10357. PMLR (2021)\n' +
      '* [47] Touvron, H., Cord, M., Jegou, H.: Deit iii: Revenge of the vit. In: European Conference on Computer Vision. pp. 516-533. Springer (2022)\n' +
      '* [48] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems **30** (2017)\n' +
      '* [49] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 568-578 (2021)\n' +
      '* [50] Wang, Y., Xu, C., Xu, C., Xu, C., Tao, D.: Learning versatile filters for efficient convolutional neural networks. Advances in Neural Information Processing Systems **31** (2018)\n' +
      '* [51] Xiao, T., Liu, Y., Zhou, B., Jiang, Y., Sun, J.: Unified perceptual parsing for scene understanding. In: Proceedings of the European conference on computer vision (ECCV). pp. 418-434 (2018)\n' +
      '* [52] Xie, S., Girshick, R., Dollar, P., Tu, Z., He, K.: Aggregated residual transformations for deep neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1492-1500 (2017)* [53] Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., Hu, H.: Simmim: A simple framework for masked image modeling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9653-9663 (2022)\n' +
      '* [54] Xing, Z., Ye, T., Yang, Y., Liu, G., Zhu, L.: Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation. arXiv preprint arXiv:2401.13560 (2024)\n' +
      '* [55] Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Partial channel connections for memory-efficient architecture search. In: International Conference on Learning Representations (2020), [https://openreview.net/forum?id=BJlS634tPr](https://openreview.net/forum?id=BJlS634tPr) 9\n' +
      '* [56] Yang, S., Wang, X., Li, Y., Fang, Y., Fang, J., Liu, W., Zhao, X., Shan, Y.: Temporally efficient vision transformer for video instance segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2885-2895 (2022)\n' +
      '* [57] You, S., Huang, T., Yang, M., Wang, F., Qian, C., Zhang, C.: Greedynas: Towards fast one-shot nas with greedy supernet. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1999-2008 (2020)\n' +
      '* [58] Zhang, B., Gu, S., Zhang, B., Bao, J., Chen, D., Wen, F., Wang, Y., Guo, B.: Styleswin: Transformer-based gan for high-resolution image generation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 11304-11314 (2022)\n' +
      '* [59] Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.: Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision **127**(3), 302-321 (2019)\n' +
      '* [60] Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417 (2024)\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      'LocalVMamba-S에서 탐색된 방향 시각화\n' +
      '\n' +
      '우리는 그림 5에서 LocalVMamba-S의 탐색된 방향을 시각화하는데, 이 모델에서는 3단계에서 27개의 레이어로 구성된 로컬VMamba-T에 비해 더 많은 \\(7\\times 7\\)의 로컬 스캔이 선호된다.\n' +
      '\n' +
      '### Discussions\n' +
      '\n' +
      '**잠재적인 부정적인 영향.** 제안된 모델의 효과를 조사하려면 계산 리소스에 대한 많은 소비가 필요하며, 이는 잠재적으로 환경 문제를 제기할 수 있다.\n' +
      '\n' +
      '**Limitations.** 시퀀스 길이에 대한 선형-시간 복잡성을 갖는 시각적 상태 공간 모델들은, 특히 이전의 CNN들 및 ViTs 아키텍처들과 비교하여 큰-해상도 다운스트림 태스크들에서 상당한 개선들을 보여준다. 그럼에도 불구하고, SSM의 계산 프레임워크는 본질적으로 컨볼루션 및 자기 주의 메커니즘보다 더 복잡하여 병렬 계산의 효율적인 실행을 복잡하게 한다. 현재의 딥 러닝 프레임워크는 또한 더 확립된 아키텍처에 대해 보다 효율적으로 SSM 계산을 가속화하는 데 있어서 제한된 능력을 나타낸다. 긍정적인 측면에서 VMamba5[32]와 같은 프로젝트의 지속적인 노력은 선택적 SSM 연산의 계산 효율성을 높이는 것을 목표로 한다. 이러한 이니셔티브는 이미 Mamba [9]에 문서화된 원래 구현에 대한 개선으로 입증된 바와 같이 속도의 주목할만한 발전을 실현했습니다.\n' +
      '\n' +
      '각주 5: 프로젝트 페이지: [https://github.com/MzeroMiko/VMamba](https://github.com/MzeroMiko/VMamba)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>