<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'GPTVQ라고 불리는 트레이닝 양자화. 이 방법은 빠른 비균일 및 벡터 양자화(VQ)를 허용하여, 이전의 최신 기술에 비해 성능-크기 트레이드-오프를 상당히 개선한다.\n' +
      '\n' +
      '이 작업의 기여도는 다음과 같다:\n' +
      '\n' +
      '* 우리의 분석 및 실험 결과는 양자화의 차원을 증가시키는 것이 많은 LLMs에 대해 개선된 정확도 대 모델 크기 트레이드-오프를 제공한다는 것을 보여준다.\n' +
      '* 훈련 후 VQ 압축을 위한 빠르고 정확한 알고리즘을 제안한다. 제안된 알고리즘은 70B 파라미터 모델에 대해 3~11시간의 실제 실행 시간을 갖는 반면, 광범위한 LLM에서 SOTA 크기 대 정확도 절충을 달성함을 보여준다.\n' +
      '* 모바일 CPU에서 VQ 압축 해제를 구현하고 벤치마킹하였다. VQ가 상당한 메모리 풋프린트 감소로 이어지는 반면, 온-디바이스 타이밍은 또한 그것이 4-비트 정수 베이스라인에 비해 개선된 레이턴시로 이어진다는 것을 입증한다.\n' +
      '\n' +
      '## 2 Motivation\n' +
      '\n' +
      '신경망 양자화는 일반적으로 모델 풋프린트, 데이터 전송 및 계산 요구 사항을 줄이기 위해 사용된다. 모델을 양자화함으로써, 훈련에 일반적으로 사용되는 높은 비트-폭 부동 소수점 가중치들 및 활성화들은 더 적은 비트들로 표현되는 더 낮은 정밀도 값들로 표현될 수 있다. 8비트 이하로 양자화하면 모델에 _양자화 잡음_를 도입하는 비용으로 풋프린트, 데이터 전송 및 계산 병목 현상이 크게 감소하여 정확도가 잠재적으로 떨어진다. 이 섹션에서는 균일 스칼라 양자화, 불균일 스칼라 양자화에 대한 간략한 개요를 제공하고 벡터 양자화를 도입하며, 이들 각각은 양자화에서 점진적으로 더 많은 유연성을 제공한다. 그런 다음 이러한 방법이 (비균일) 기본 분포의 표현 정확도를 향상시키는 방법을 설명하고 압축과 정확도 사이의 개선된 트레이드 오프를 산출할 수 있다. 마지막으로, 벡터 양자화의 어려움과 현재 접근법의 한계에 대해 다룬다.\n' +
      '\n' +
      '### 양자화 격자의 종류와 유연성에 관한 연구\n' +
      '\n' +
      '대칭형 균일 양자화기는 원래의 부동 소수점 벡터 \\(\\textbf{x}\\in\\mathbb{R}^{D}\\)을 \\(\\textbf{x}\\approx~{s\\textbf{x}_{int}\\)으로 근사하고, 여기서 \\(\\textbf{x}_{int}\\)의 각 요소는 \\(b\\)비트의 정수 값이고 \\(s\\)은 **x**의 구성 요소에 공유되는 더 높은 정밀도의 양자화 스케일이다.\n' +
      '\n' +
      '이전 섹션에서 제시된 바와 같은 비-균일 양자화 균일 양자화는 효율적이지만, 표현 가능한 포인트들이 단독으로 등거리 이격될 수 있기 때문에 매우 유연하지 않다. 보다 유연한 양자화 방법은 부호책 양자화를 이용한 비균일 양자화로서, 부호책 \\(C:C=\\{c_{1},c_{2},\\dots,c_{k}\\}\\)에서 부동 소수점 수를 임의의 스칼라 중심점으로 이산화하는 방법이다. *x**에서의 각각의 고정밀 값은 중심 \\(c_{j}\\)의 인덱스 \\(j\\)으로 표현된다. 각 인덱스는 \\(\\lceil\\log_{2}k\\rceil\\) 비트를 사용하여 저장할 수 있다. 이 기법은 \\(\\log_{2}k\\)가 **x**의 원소들의 원래 비트폭보다 작도록 \\(k\\)을 선택하여 가중치 텐서를 압축하는데 사용될 수 있다. 코드북 자체에는 오버헤드가 발생하므로 섹션 2.2와 섹션 3.2에서 더 자세히 논의할 것이다.\n' +
      '\n' +
      '비균일 양자화에서 벡터 양자화는 이전 단락에서 소개된 바와 같이, **x**의 각 스칼라 값이 개별적으로 양자화된다고 가정한다. 그러나 코드북의 중심점에 대해 더 높은 차원을 선택함으로써 보다 유연한 양자화기를 구성할 수 있다. 이 경우 \\(C\\)의 각 중심은 \\(d\\) 값을 인코딩하는데, 예를 들어 \\(d=2\\)의 값 쌍과 **x**의 각 \\(d\\) 값은 단일 인덱스로 \\(C_{d}\\)으로 표현되며, 여기서 우리는 차원 요소가 있는 코드북을 나타내기 위해 \\(C_{d}\\)을 사용한다. 이 기술을 벡터 양자화(VQ: Vector Quantization)라고 한다(Gersho & Gray, 2012). \\(D\\)차원 벡터를 여러 개의 \\(d\\)차원 서브 벡터, \\(d<D\\)로 분할하고, 각각 개별적으로 색인을 \\(C_{d}\\)으로 표현하는 경우도 종종 곱 양자화(Stock et al., 2019)라 한다.\n' +
      '\n' +
      '고차원 내에서의 정확도 향상 불균일하게 분포된 데이터가 불균일 양자화기에 의해 더 정확하게 표현될 수 있다는 것은 잘 알려진 사실이다. 코드북의 차원을 증가시킬 때, 즉 VQ를 통해 그리드의 유연성이 증가한다. 이에 대한 시각적 표현은 그림 1에 주어진다. 본 예에서, 원안의 각 값을 3비트 표현(즉, \\(d=2\\)으로 VQ의 경우 6비트)으로 양자화하는 경우, 점들의 수는 동일하게 유지되는, 즉 \\(2^{6}=64\\)을 볼 수 있지만 중심점의 분포는 기본 분포와 더 밀접하게 일치하여 표현의 정확도를 높일 수 있다.\n' +
      '\n' +
      '도 2: Llama-v2 7B 가중치에 대한 치수에 따른 양자화 SQNR. 신호 대 잡음비는 양자화 그리드의 추가적인 유연성으로 인해 양자화 차원성에 따라 증가한다.\n' +
      '\n' +
      '표현의 정확도는 코드북의 차원성이 증가할 수록 증가한다. 그림 2에서 더 높은 \\(d\\)의 표현 정확도의 향상을 볼 수 있다. 여기서 우리는 LLama-v2 7B의 가중치를 균일 양자화, 불균일 양자화, 2차원과 4차원 벡터 양자화로 압축하는 효과를 플롯한다. y축에서 우리는 원래 가중치와 양자화된 가중치 사이의 신호 대 양자화 잡음비(SQNR)를 표시하는데, 여기서 더 높으면 더 좋다. 공정한 비교를 위해, 코드북 오버헤드가 각각의 양자화 방법에 대해 가중치당 0.25b와 항상 동일하다는 것을 보장한다. 즉, 개선된 SQNR은 우리의 표현에 더 많은 비트를 사용함으로써 사소하게 야기되지 않는다. 우리는 차원성이 증가함에 따라 SQNR도 크게 향상된다는 것을 분명히 알 수 있다.\n' +
      '\n' +
      '### 벡터 양자화의 과제\n' +
      '\n' +
      '코드북 크기(Codebook size)는 VQ 코드북(C_{d}\\)을 저장 및 전송해야 하고, 가중치에 대한 인덱스 할당이 필요하기 때문에 표현 정확도의 향상은 비용이 많이 든다. 텐서에 대한 코드북의 크기는 \\(k\\times d\\)에 비례하며, 여기서 \\(k\\)은 코드북당 중심수이고 \\(d\\)은 VQ차원이다. 우리가 가중치 텐서를 압축하기 위해 VQ를 사용하는 것을 목표로 한다면, 우리는 네트워크에서 가중치 텐서의 정확도와 크기 사이의 좋은 절충점을 찾는 데 이러한 오버헤드를 고려해야 한다.\n' +
      '\n' +
      '이 작업의 나머지 부분에서는 각 개별 가중치에 대해 저장된 인덱스 비트 수를 나타내기 위해 차원_(\\(b\\))당 _bits를 사용한다. 이는 차원이 \\(d\\)인 VQ의 경우 전체 인덱스 비트 수는 \\(d\\times b\\)이고 코드북의 중심 수는 \\(k=2^{d\\times b}\\)임을 의미한다.\n' +
      '\n' +
      '중심점 및 할당 설정 벡터 양자화를 적용하기 위해서는 대표 중심점의 코드북을 찾고, 각 가중치에 대한 중심점에 대한 할당을 수행해야 한다. 이를 달성하기 위한 많은 방법들이 있지만, 실용적이고 인기 있는 접근법은 k-Means 알고리즘이다(Han et al., 2015). 그러나 신경망 가중치의 경우 가중치에 대한 클러스터링만으로는 충분한 정확도를 산출하지 못할 수 있다. 결과들을 개선하기 위해, 몇몇 저자들(Stock et al., 2019; Martinez et al., 2021)은 층 재구성 에러를 그들의 최적화에 포함시키고, 이는 모델 효율 문헌에서 결과들을 상당히 개선하는 것으로 나타난 기법이다(He et al., 2017; Zhang et al., 2016; Nagel et al., 2020).\n' +
      '\n' +
      '그럼에도 불구하고, 우리는 표 1에서 볼 수 있는 바와 같이, k-Means 단독, 또는 레이어 입력 데이터가 포함된 k-Means가 LLamav2-7B(Touvron et al., 2023b) 상에서 충분히 수행적이지 않다는 것을 발견한다. 이 실험에서 우리는 VQ를 가중치들의 그룹들에 적용하는데, 여기서 가중치들의 각 그룹들은 자신의 코드북을 갖는다. 각 설정마다 오버헤드가 동일하도록 각 가중치 그룹의 크기를 선택한다. 데이터가 포함될 때 결과가 개선되지만 특히 2비트 및 3비트 VQ의 경우 복잡성의 증가가 허용할 수 없을 정도로 크게 유지된다는 것을 알 수 있다. 레이어 입력 데이터를 포함하면 결과가 향상되지만 (Stock et al., 2019; Martinez et al., 2021)과 같은 방법의 저자는 이것만으로는 만족스러운 성능을 얻을 수 없으며, 그들의 알고리즘에 종단간 미세 조정 단계를 포함한다는 점에 주목한다. 불행히도, 현대 LLM의 크기는 많은 실무자들에게 단대단 미세 조정을 엄청나게 비싸게 만든다. 훈련 후 양자화를 위한 빠르고 확장 가능한 방법을 목표로 하여, 양자화 시 활성화(activation)를 고려한 정확하고, 상당히 큰 크기의 언어 모델에 적용하기에 효율적이고 확장 가능한 방법을 찾기 시작했다.\n' +
      '\n' +
      '## 3 Gptvq\n' +
      '\n' +
      '이 섹션에서는 LLM을 효율적이고 정확하게 벡터 양자화하기 위한 새로운 방법을 소개한다. 앞 절에서 언급한 바와 같이 VQ를 대상으로 하는 기존의 방법은 LLM 크기의 모델에 축척하지 않는다. 대신, 계층 출력 복원 MSE의 Hessian으로부터의 정보를 사용하여 나머지 (비양자화된) 가중치에 대한 업데이트와 함께 열별 양자화를 인터리빙하는 GPTQ(Frantar et al., 2022)라는 최근 균일 양자화 방법을 기반으로 한다. 이 방법은 최대 수천억 개의 매개변수로 LLM을 균일하게 양자화하는 데 탁월한 성능을 제공하는 것으로 나타났다.\n' +
      '\n' +
      '우리는 먼저 GPTQ에 대한 간략한 설명을 제시한다. 그런 다음, GPTQ를 VQ로 확장하고 정확한 초기화를 위해 (Stock et al., 2019)의 아이디어를 통합하는 GPTVQ 방법을 제시한다. 마지막으로, 우리는 크기 대 크기를 개선하기 위한 몇 가지 새로운 기술을 제시한다. 결과 양자화된 모델의 정확성 절충.\n' +
      '\n' +
      '### Background: GPTQ\n' +
      '\n' +
      '섹션 2.1에 기술된 바와 같이, 양자화는 양자화 잡음을 도입한다. 많은 문헌들이 방법과 함께 존재한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Setting & With input data & Perplexity \\\\ \\hline FP16 & n/a & 5.47 \\\\ \\hline\n' +
      '2 bits per dim & No & 1.3e3 \\\\  & Yes & 948 \\\\ \\hline\n' +
      '3 bits per dim & No & 8.23 \\\\  & Yes & 6.95 \\\\ \\hline\n' +
      '4 bits per dim & No & 5.97 \\\\  & Yes & 5.78 \\\\ \\hline \\hline Uniform 3 bit & Yes & 6.03 \\\\ Uniform 4 bit & Yes & 5.74 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: (데이터가 포함되지 않은) k-Means를 사용하여 Llamav2-7B에 대한 **2D VQ.** 양자화 잡음이 모델 정확도에 미치는 영향을 완화하기 위해, 최근 조사를 위해 (Nagel et al., 2021; Gholami et al., 2022) 참조. PTQ(Post-training quantization) 접근법들은 값비싼 QAT(Quantization-aware Training)에 의존할 필요 없이, 미리 훈련된 네트워크들에 대한 양자화 잡음의 역효과를 완화하는 것을 목표로 한다. AdaRound(Nagel et al., 2020)에 의해 소개된 PTQ에서의 대중적이고 효과적인 접근법은 전체 네트워크의 손실에 대한 근사치로서 레이어의 출력 에러를 최소화하기 위해 가중치를 수정하는 것이다:\n' +
      '\n' +
      '\\mathbb{E}\\left[\\theta+\\epsilon]\\mathcal{L}(\\theta+\\epsilon)-\\mathcal{L}(\\theta\\right]\\approx\\sum_{\\ell}||\\mathbf{W}^{\\ell}\\mathbf{X}^{\\ell}-\\mathbf{\\widehat{W}^{\\ell}\\mathbf{X}^{\\ell}||_{F}^{2}, \\tag{1}\\approx\\sum_{\\ell}||\\mathbf{W}^{\\ell}\\mathbf{X}^{\\ell}||_{F}^{2}, \\tag{1}\\approx\\sum_{\\ell}||\\mathbf{W}^{\\ell}\\mathbf{X}^{\\ell}\\widehat{W}^{\\ell}\\mathbf{X}^{\\ell}||_{F}^{2},\\tag{1}\\approx\\sum_\n' +
      '\n' +
      '여기서 \\(\\mathbf{W}^{\\ell}\\)는 레이어에 대한 가중치이고, \\(\\mathbf{\\widehat{W}^{\\ell}=\\mathbf{W}^{\\ell}+\\epsilon^{\\ell}\\)는 이 가중치 텐서에 대한 (양자화된) 근사치이고, \\(\\mathbf{X}^{\\ell}\\)는 캘리브레이션 데이터 세트로부터 레이어에 대한 입력 데이터를 나타내며, \\(N\\)의 차원성 \\(R\\)의 개별 데이터 포인트를 컬럼을 따라 나타낸다.\n' +
      '\n' +
      'GPTQ는 수학식 1의 Hessian을 사용하는 최적 뇌 양자화(OBQ; Frantar and Alistarh (2022))를 따른다. 이 Hessian은 \\(\\mathbf{H}^{(\\ell)}=\\mathbf{X}^{(\\ell)}\\mathbf{X}^{(\\ell)T}\\으로 효율적으로 계산될 수 있다. OBQ와 마찬가지로 GPTQ는 \\(\\mathbf{W}^{(\\ell)}\\)에서 가중치를 양자화하여 도입된 헤시안 가중 오차를 최소화하는 것을 목표로 한다:\n' +
      '\n' +
      '\\[E=\\sum_{q}|E_{q}|_{2}^{2};\\hskip 14.226378ptE_{q}=\\frac{\\mathbf{W}_{:,q}- \\text{quant}(\\mathbf{W}_{:,q}}{\\left[\\mathbf{H}^{-1}\\right]_{qq}}. \\tag{2}\\t}\n' +
      '\n' +
      'GPTQ는 다음과 같은 방식으로 OBQ를 확장한다. 먼저, GPTQ는 열의 모든 가중치를 왼쪽에서 오른쪽으로 병렬로 양자화함으로써 \\(\\mathbf{H}^{(\\ell)}\\)이 \\(\\mathbf{W}^{(\\ell)}\\)의 모든 행에 공유된다는 사실을 이용한다. 이것은 상이한 행들에 대한 독립적인 헤시안 업데이트들의 필요성을 제거한다. 컬럼 \\(q\\)을 양자화한 후, 나머지 (양자화되지 않은) 컬럼 \\(q^{\\prime}>q\\)은 레이어의 출력에서 컬럼 \\(q\\)을 양자화하여 도입된 오차를 흡수하는 헤시안 기반 업데이트 규칙 \\(\\delta\\)으로 수정한다:\n' +
      '\n' +
      '\\[\\delta=-\\frac{\\mathbf{W}_{:,q}-\\text{quant}(\\mathbf{W}_{:,q}}{\\left[\\mathbf{H}^{-1}\\right]_{qq}}\\mathbf{H}_{:,(q+1):}\\tag{3}\\tag{3}}}{\\left[\\mathbf{H}^{-1}\\right]\n' +
      '\n' +
      'GPTQ는 데이터 전송을 줄이기 위해 \\(q\\) 열이 존재하는 \\(B\\) 열의 작은 블록에만 수학식 3의 업데이트를 적용한다. 블록 \\(B\\)의 바깥쪽 열을 갱신하기 위해, 블록 \\(B\\)의 열을 처리하는 동안 수학식 2의 오차 \\(E_{q}\\)를 누적하고, 블록 \\(B\\)의 모든 열을 처리한 후 블록 \\(B\\)의 바깥쪽 모든 열을 한 번에 적용한다. 마지막으로, GPTQ는 역 헤시안 \\(\\mathbf{H}^{-1}\\)의 촐레스키 분해를 사용하며, 이는 OBQ의 역 헤시안 행 및 열 제거 연산에 대한 보다 수치적으로 안정적인 대안을 소개한다.\n' +
      '\n' +
      '### GPTVQ 방식\n' +
      '\n' +
      'GPTVQ 방법은 불균일 및 벡터 양자화를 위해 GPTQ 방법을 일반화한다.\n' +
      '\n' +
      'GPTQ 프레임워크에 따라 첫 번째 열부터 탐욕스러운 방식으로 가중치 텐서의 양자화를 수행한다. 이 방법의 상세는 알고리즘 1에 제시되어 있다. VQ dimensionality \\(d\\)을 주어 한 번에 \\(d\\) 열을 양자화 한다. 스칼라 양자화의 경우, 단일 컬럼의 최적 헤시안 가중 양자화는 가장 가까운 값으로 반올림함으로써 달성되었다. 그러나 벡터 양자화의 경우, 각 \\(d\\) 좌표의 오차가 다르게 가중되어 가장 가까운 중심점을 선택하는 것이 차선책일 수 있다. 데이터 포인트\\(\\mathbf{x}^{(i)}\\)와 대응하는 역 서브-헤시안\\(\\mathbf{H}^{(i)}\\)에 대한 최적 할당 \\(j\\)을 선택하기 위해 다음의 규칙이 사용된다:\n' +
      '\n' +
      '[j=\\arg\\min_{m}\\left(\\mathbf{x}-\\mathbf{c}^{(m)}\\right)^{T}\\mathbf{H}^{(i)}\\left(\\mathbf{x}-\\mathbf{c}^{(m)}\\right) \\tag{4}\\right)\n' +
      '\n' +
      '\\(d\\) 열의 양자화를 수행한 후, 갱신 규칙 3을 이용하여 나머지 가중치를 갱신하고, 갱신된 가중치를 \\(d\\) 좌표에 따라 누적하여 하나의 연산으로 나머지 가중치에 적용한다.\n' +
      '\n' +
      '양자화 오류를 최소화하기 위해 계층마다 여러 개의 코드북을 사용한다. 각 코드북은 가중치들의 _group_에 할당된다(알고리즘 1 참조).\n' +
      '\n' +
      '코드북 초기화는 가중치 그룹에 대한 코드북을 초기화하기 위해 EM 알고리즘의 다음 변형을 제안한다. \\(d\\)차원 벡터(\\mathbf{x}^{(i)}\\)의 집합이 주어졌을 때, 우리의 목표는 \\(k\\) 중심 벡터(\\(\\mathbf{c}^{(m)}\\)와 그에 상응하는 할당의 집합 \\(I_{m}\\)을 찾는 것이다. 목적은 다음과 같은 헤시안 가중 거리 함수의 합이다:\n' +
      '\n' +
      '\\min_{\\mathbf{I},\\mathbf{c}^{(0)},\\dots,(k)}\\sum_{m=0}^{k}\\sum_{i\\in I_{m}}\\left(\\mathbf{x}^{(m)}\\mathbf{c}^{(i)}\\mathbff{H}^{(i)}\\left( \\mathbf{x}^{(m)}\\mathbf{c}^{(m)}\\right), \\tag{5}\\t\n' +
      '\n' +
      '여기서 \\(\\mathbf{H}^{(i)}\\)는 데이터 포인트 \\(\\mathbf{x}^{i}\\)에 대응하는 역 헤시안(Inverse Hessian)의 \\(d\\times d\\) 서브세트이다. 예를 들어, 2D 벡터 양자화를 위해, 이들 행렬들은 열 쌍들 사이에서 공유된다. 동일성과 같은 \\(\\mathbf{H}^{(i)}\\)의 경우 클러스터링 방법은 K-means와 동일하다. 목표는 다음과 같이 E-단계 및 M-단계를 사용하여 최소화될 수 있다.\n' +
      '\n' +
      '**E-step**: 목적 4를 최소화하는 각 미양자화 \\(d\\)-dimensionl 벡터 \\(\\mathbf{x}^{(i)}\\)에 대한 할당 \\(j\\)을 구하고, 이 거리 함수를 이용하여 데이터 인식 손실을 기반으로 최적의 중심점을 할당한다.\n' +
      '\n' +
      '**M-step**: 최소로 하는 중심값 \\(\\mathbf{c}^{(m)}\\)을 구한다\n' +
      '\n' +
      '[\\mathbf{c}^{(m)}=\\arg\\min_{\\mathbf{c}^{(m)}}\\sum_{i\\in I_{m}}\\left(\\mathbf{x}^{(i)}-\\mathbf{c}^{(m)}\\right)\\mathbff{H}^{(i)}\\left(\\mathbf{x}^{(i)}-\\mathbf{c}^{(m)}}^{(m)}\\right)\\mathbff{c}^{(m)}}\\tag{6}\\t}\n' +
      '\n' +
      '이 목적은 2차 형태 w.r.t\\(\\mathbf{c}^{(m)}\\)이다. 최적값은 \\(\\mathbf{c}^{(m)}=\\left(\\sum_{i\\in I_{m}}\\mathbf{H}^{(i)}\\right)^{+}\\left(\\sum_{i\\in I_{m}}\\mathbfff{H}^{(i)}\\mathbff{x}^{(i)}\\right)\\)의 닫힌 형태로 계산되며, 여기서 \\((\\cdot)^{+}\\은 무어-펜로즈 슈도인버스이다. 알고리즘 1의 라인 15에 대한 벡터 양자화 연산 동안, 우리는 수학식 4에 정의된 할당 단계를 또한 사용한다. 실제적으로, 역 헤시안 대각선이나 풀 \\(d\\)-딤 역 서브 헤시안 사이의 성능 차이를 찾을 수 없다.\n' +
      '\n' +
      '블록 단위의 데이터 정규화는 벡터 양자화의 오차를 낮추기 위해 코드북 초기화 전의 데이터에 블록 단위의 데이터 정규화를 적용한다. 새로운 코드북에 해당하는 각 그룹에 대해 가중치 부행렬 행렬 \\(\\mathbf{W}_{i}\\odot\\mathbf{S}_{i}\\)의 요소별 나눗셈 \\(\\mathbf{W}_{i}\\)을 해당 척도 \\(\\mathbf{S}_{i}\\)으로 수행한다. 축척은 16, 32 또는 64의 블록 크기에 대해 \\(\\mathbf{W}_{i}\\)의 모든 하위 행에 대해 블록 단위로 계산된다.\n' +
      '\n' +
      '블록들의 집합(sub-rows) \\(\\mathbf{w}^{(i)}\\)이 주어지면, 이들 각각에 대한 스케일 \\(s^{(i)}\\(s^{(i)}=\\max_{j}|w_{j}^{(i)}|\\)으로 계산된다. 오버헤드를 최소화하기 위해, 스케일들은 4-비트 정수로 양자화된다. 우리는 가중치에서 여러 크기의 크기를 캡처하기 위해 로그 스케일로 양자화를 수행하는 것이 유용하다는 것을 발견했다. 양자화된 척도는 \\(s_{int}^{(i)}=\\lceil\\frac{\\log_{2}[s^{(i)}]-z}{a}\\rfloor a\\으로 계산되며, 여기서 \\(a\\)는 가중치 그룹 간에 공유되는 양자화 척도이다. 단위스케일링에 해당하는 로그공간에서 0을 정확하게 표현하기 위해 부동소수점 오프셋 \\(z\\)을 사용한다. 실제로 \\(z\\)의 값은 \\(\\mathbf{W}\\)의 열 내에서 공유되므로 무시할 수 있는 오버헤드를 갖는다. 마지막으로 축척된 부행은 \\(\\mathbf{w}\\cdot 2^{-a(s_{int}-s_{0})}\\)로 정규화된다. 여기서 \\(s_{0}=\\log_{2}(z)\\). 스케일링된 데이터는 코드북 초기화에 사용된다. 역 스케일링은 VQ 디코딩 단계에서 적용된다.\n' +
      '\n' +
      '전체 모델 크기의 척도로서 값당 총 비트를 계산하고, \\(\\log_{2}(k)+kdb_{c}/l+b_{s}/N_{s}\\)으로 주어진 값당 _비트를 계산하며, 여기서 \\(k\\)은 중심점 수, d는 \\(VQ\\) 차원 수, \\(b_{c}\\)은 코드북 비트 폭, \\(l\\)은 코드북당 가중치 수, \\(b_{s}\\)은 스케일 비트 폭, \\(N_{s}\\)은 스케일링 블록 크기이다. 우리는 값 \\(k\\) s.t. \\(\\log_{2}(k)\\)는 정수이다.\n' +
      '\n' +
      '### Additional steps\n' +
      '\n' +
      '알고리즘 1의 절차가 완료된 후, 우리는 모델 크기 대 복잡성 상충 관계를 더욱 개선하기 위해 여러 단계를 수행한다. 이들 각 단계를 이하에 설명한다.\n' +
      '\n' +
      '코드북 업데이트는 _codebook update_를 통해 출력 복원 오류를 더 줄일 수 있음을 발견했다. 알고리즘 1의 15행에서 \\(\\mathbf{Q}\\)는 \\(\\mathbf{C}\\)의 요소로부터 점진적으로 구성된다는 것을 기억하라. 이 구조는 \\(\\mathbf{C}\\)의 값들의 룩업을 구성하기 때문에, 층별 목표는 여전히 w.r.t\\(\\mathbf{C}\\)로 최소화될 수 있다. 목적은 2차 프로그램이고 볼록하다:\n' +
      '\n' +
      '\\[\\min_{\\mathbf{c}_{0},\\dots,\\mathbf{C}_{N}}||\\mathbf{W}\\mathbf{X}-\\mathbf{Q}\\mathbf{X}||_{F}^{2},\\tag{7}\\tag{\n' +
      '\n' +
      '여기서 \\(\\mathbf{Q}(\\mathbf{C}_{0},\\dots,\\mathbf{C}_{N})\\)는 중심으로부터 양자화된 가중치들을 재구성하는 룩업 연산이다. 이 목표는 닫힌 형태로 최소화될 수 있지만 경사 하강이 상당히 빠르고 동등하게 좋은 솔루션을 산출한다는 것을 발견했다. \\(\\mathbf{Q}\\) w.r.t.\\(\\mathbf{C}\\)의 기울기는 단순히 룩업 연산만을 포함하는 \\(Q\\)을 구성하는 것으로 정의될 수 있다. 각 GD 단계에서 \\(\\mathbf{C}\\)의 값을 갱신하고, \\(\\mathbf{Q}\\)의 새로운 값을 사용하여 \\(\\mathbf{C}\\)의 값을 재구성하여 과제를 고정한다.\n' +
      '\n' +
      '실제 시나리오에서 코드북 양자화는 8비트로 양자화될 필요가 있다. 추가 후처리 단계로, 대칭 최소-최대 양자화를 사용하여 각 가중치 그룹에 대한 코드북을 부호화된 8비트 정수로 양자화한다.\n' +
      '\n' +
      '추가적인 코드북 압축은 코드북 텐서 \\(\\mathbf{C}\\)의 랭크를 줄임으로써 개선된 모델 크기 대 복잡도 절충을 달성한다. 단일 텐서에 대해 \\(\\mathbf{C}\\)은 모양 \\(N_{G}\\times k\\times d\\)을 가지며, 여기서 \\(N_{G}\\)은 해당 가중치 텐서의 그룹 수, \\(k\\)은 코드북당 중심 개수, \\(d\\)은 VQ 차원 \\(d\\geq 1\\)이다. 우리는 먼저 세 번째 차원을 따라 첫 번째 값으로 \\(\\mathbf{C}\\)의 두 번째 차원을 정렬하고, 그에 따라 \\(\\mathbf{I}\\)의 지수를 재할당한다. 그 다음, 세 번째 차원을 따라 모든 \\(N_{G}\\times k\\) 행렬에 대해 SVD를 수행하여 \\(\\mathbf{U}_{i}\\), \\(\\mathbf{\\Sigma}_{i}\\) 및 \\(\\mathbf{V}_{i}\\), \\(i=1\\cdots d\\), \\(N_{G}\\times k\\), \\(k\\times k\\) 및 \\(k\\times k\\)의 행렬에 대해 각각 SVD를 수행한다. 우리는 \\(\\mathbf{\\Sigma}\\)을 \\(\\mathbf{U}^{\\prime}=\\mathbf{U}\\mathbf{\\Sigma}\\)으로 \\(\\mathbf{U}^{\\prime}\\)으로 접고, 이 행렬의 순위를 \\(k\\)으로 줄여서 \\(N_{G}\\times k\\times k\\times \\(\\mathbf{U}^{\\prime\\prime}\\)의 행렬을 얻는다. 또한 이에 따라 \\(\\mathbf{V}\\)의 순위를 감소시켜 \\(k\\times k\\) 행렬 \\(\\mathbf{V}^{\\prime}\\)을 얻는다. 그 다음, 방정식 7의 손실에 대해 기울기 하강(GD)을 수행하지만, 코드북 텐서 인자\\(\\mathbf{U}^{\\prime\\prime}\\) 및 \\(\\mathbf{V}^{\\prime}\\)에 대해 수행한다. 각 GD 단계에서 \\(\\widehat{\\mathbf{C}\\)는 \\(\\widehat{\\mathbf{C}=\\mathbf{U}^{\\prime\\prime\\mathbf{V}^{\\prime T}\\)으로 생성되며, 앞서 설명한 나머지 코드북 업 절차를 따른다. 마지막으로, 코드북 텐서 인자\\(\\mathbf{U}^{\\prime\\prime\\\\)만이 양자화되는데, 이는 \\(\\mathbf{V}^{\\prime\\)의 오버헤드가 거의 없기 때문이다. 추론 과정에서 구성 후 코드북당 \\(\\widehat{\\mathbf{C}}\\)을 양자화한다. 실제로, 우리는 이 단계를 1d VQ에만 적용하는데, 이는 더 큰 \\(d\\)에 대해 거의 영향을 미치지 않는다는 것을 발견했다.\n' +
      '\n' +
      '##4 실험 및 결과\n' +
      '\n' +
      '이 섹션에서는 GPTVQ를 평가하고 균일 양자화 기준선 방법과 비교하여 1, 2 및 4차원에서 벡터 양자화의 성능을 비교한다.\n' +
      '\n' +
      'ModelsWe는 Llama-1(Touvron et al., 2023), Llama-2(Touvron et al., 2023) 뿐만 아니라 Mistral-7B-v0.1(Jiang et al., 2023) 및 Mistral-MoE-8x7B-v0.1(Jiang et al., 2024)을 사용한다. 또한, BLOOM-560M(Workshop et al., 2022)에 대한 단일 절제를 실행한다.\n' +
      '\n' +
      'DatasetsWe follow Shao et al.(2023) and use the WikiText2 (Merity et al., 2016) training set as the calibration dataset for all our experiment. 우리는 위키텍스트2 검증세트에 대한 토큰 퍼플렉시티와 제로샷 언어 태스크인 PIQA(Bisk et al., 2020), ARC-easy and ARC-challenge(Clark et al., 2018), BoolQ(Clark et al., 2019), HellaSwag(Zellers et al., 2019), WinoGrande(Keisuke et al., 2019)에 대한 모델을 평가한다. WikiText2를 제외한 모든 데이터 세트에 대해 우리는 평가를 실행하기 위해 LLM-평가-harness(Gao et al., 2023)를 사용한다.\n' +
      '\n' +
      '기준은 GPTVQ와 그룹 크기가 다른 다양한 균일 양자화 방법을 비교하고, 모두 동일한 비트-퍼-밸류(bpv) 오버헤드를 갖도록 한다. 우리는 LLMs: GPTQ(Frantar et al., 2022), AWQ(Lin et al., 2023), 및 OmniQuant(Shao et al., 2023)를 목표로 하는 Round-to-Nearest(RTN) 및 몇몇 최근의 최신 PTQ 접근법들을 포함한다.\n' +
      '\n' +
      '주요 결과 표 2는 GPTVQ에 대한 주요 결과를 포함한다. 이 표에서는 PIQA, BoolQ, ARC-easy, ARC-challenge, HellaSwag 및 WinoGrande 작업에 대해 위키텍스트 2 당혹감과 제로 샷 작업 점수에 대한 평균을 보고한다. 이 표에서 우리는 모든 Llama-v2 모델, Mistral-7B-v0.1 및 Mistral-8x7B-v0.1에 대한 결과를 보고한다. 더 자세한 결과는 부록 A에 포함된다; 표 5는 제로 샷 작업에 대한 개별 점수를 포함하고 표 4는 모든 Llama-v1 모델에 대한 WikiText2 복잡성을 포함하고, 4 비트 양자화를 사용한 추가 실험을 포함한다.\n' +
      '\n' +
      '이러한 표에서 GPTVQ를 이용한 비균일 양자화는 일반적으로 균일한 PTQ 방법에 비해 향상된 결과를 얻을 수 있음을 알 수 있다. 이 갭은 낮은 비트폭 및 매우 큰 모델에 대해 특히 커진다. 예를 들어, Llamav2-70B 상의 GPTVQ 2D와 옴니퀀트 W2@g128을 비교하며, 여기서 거의 2개의 복잡성 포인트의 개선이 달성된다. 또한 거의 모든 경우에 2D VQ가 1D VQ보다 우수하며 4D VQ로 훨씬 더 큰 개선이 달성된다.\n' +
      '\n' +
      '### GPTVQ hyperparameters\n' +
      '\n' +
      '모든 실험에서 우리는 위키텍스트 트레이닝 세트를 우리의 방법을 위한 교정 데이터로 사용한다. (Frantar et al., 2022)에 이어서, 우리는 각각 2048개의 토큰의 128개의 시퀀스를 샘플링한다. 제안된 방법은 EM 초기화 방법, EM 반복 횟수, 동일한 코드북을 공유하는 가중치 블록의 가중치 수, 각 블록의 열의 수 등 여러 개의 하이퍼파라미터를 가진다. 또한, 블록 크기 증가, 코드북 양자화, 또는 코드북에 대한 SVD 수행 등 다양한 경로를 통해 코드북 오버헤드를 낮출 수 있다. 주요 결과에서 우리는 다음과 같은 하이퍼파라미터 설정을 사용한다: 우리는 \'마할라노비스\' 방법에 의해 발견된 중심체로 EM 초기화를 시드하고(섹션 4.3 참조), 코드북 중심체를 초기화하기 위해 100번의 반복 동안 EM을 실행한다. 각 가중치 그룹은 256개의 열에 걸쳐 있으며, 예를 들어 1024개의 가중치 그룹은 4행 256열이다. 알고리즘 1의 절차를 실행한 후 25번의 반복에 대해 3.3에서 설명한 대로 코드북을 업데이트하고 기본적으로 코드북 값을 나타내기 위해 8비트 균일 양자화를 사용한다. 섹션 4.3에서 이러한 하이퍼파라미터 각각의 선택에 대한 절제를 수행한다. 섹션 3.2에 도입된 블록별 데이터 정규화를 적용하면 대부분 최종 성능이 향상된다는 점에 주목한다. 그러나, 어떤 경우, 특히 인덱스당 2비트를 갖는 1D VQ의 경우, 그것은 성능에 해를 끼치고 그러한 경우, 우리는 그것을 적용하지 않았다.\n' +
      '\n' +
      '섹션 2.2에 설명된 바와 같이 코드북 오버헤드는 VQ 코드북은 무시할 수 없는 오버헤드를 도입한다. 드물게 다루어지는 점은 균일한 양자화의 양자화 스케일도 저장 및 전송될 필요가 있고, 오버헤드가 발생한다는 것이다. 이 스케일의 오버헤드는, 채널당 또는 텐서당 양자화에 대해서는 무시할 수 있지만, LLMs에 대한 낮은 비트폭 양자화에 종종 적용되는 바와 같이, 더 작은 블록 사이즈로의 양자화에 대해서는 현저해진다(Rouhani et al., 2023; Frantar et al., 2022; Lin et al., 2023; Shao et al., 2023). 예를 들어 128개의 가중치 그룹에 대해, 16 비트 스케일은 값당 \\(16/128=0.125\\) 비트의 오버헤드를 도입한다. 실험에서 주어진 VQ 차원과 비트폭에 대해 특정 목표 오버헤드가 달성되도록 그룹 크기를 선택한다. 예를 들어, 값 타겟당 2.125 비트를 갖는 2D VQ를 고려한다. 2D VQ의 코드북 오버헤드는 \\(2\\times 2^{2\\times 2}\\times 8=256\\) 비트이며, 이는 각 그룹이 값 목표당 2.125 비트를 만족하기 위해 2048개의 가중치를 포함해야 함을 의미한다.\n' +
      '\n' +
      '(Shao et al., 2023)에 제시된 베이스라인 결과와 비교하기 위해, 우리는 값당 0.125 또는 0.25 비트의 오버헤드에 대응하는 그룹 크기 및 코드북 비트 폭의 조합을 선택한다. 이들 설정은 (Shao et al., 2023)에서 사용된 바와 같이, 각각 128 또는 64 가중치의 그룹 크기를 갖는 균일한 양자화에 대응한다.\n' +
      '\n' +
      '### 데이터 전송 속도 비교\n' +
      '\n' +
      'VQ가 데이터 전송 지연에 미치는 영향을 설명하기 위해, 우리는 VQ 압축 가중치를 효율적으로 디코딩하기 위해 Arm@ CPU에 최적화된 커널을 개발했다. 우리의 구현은 Arm@ TBL 명령의 변형을 사용한다. TBL 명령은 룩업 테이블(LUT)에서 값들을 룩업하기 위해 사용될 수 있고, (최대) 5 비트의 인덱스를 8 비트 정수 값으로 변환하기 위해 사용될 수 있다. 1보다 높은 치수의 VQ는 다수의 LUT들 및 대응하는 TBL 명령어들을 사용함으로써 구현될 수 있다. 예를 들어, 인덱스당 2비트를 갖는 2D VQ는 2개의 LUT로 변환되며, 각각의 VQ 딤에 대해 하나씩이고, 각각은 4비트 인덱스를 8비트 값으로 매핑한다.\n' +
      '\n' +
      '본 논문에서는 Snapdragon@ technology1을 이용하여 4비트 정수 베이스라인과 비교하여 초당 전송 및 디코딩된 가중치를 측정하고 상대 속도를 보고한다. 2차원 벡터 양자화된 데이터 텐서에 대한 데이터 전송 지연 시간을 2비트 또는 2.5비트, 즉 인덱스당 각각 4비트 또는 5비트로 측정한다. TBL 명령어 수의 두 배가 필요하기 때문에 인덱스당 비트폭이 더 높은 설정은 고려하지 않습니다.\n' +
      '\n' +
      '각주 1: 스냅드래곤은 퀄컴 테크놀로지스의 제품이다. 및/또는 그 자회사들.\n' +
      '\n' +
      '표 3은 본 실험의 결과를 나타낸다. 이 표에서 우리는 큰 풋프린트 감소 외에도 VQ가 4비트 정수 기준선에 비해 데이터 전송 대기 시간을 감소시킨다는 것을 보여준다. 마지막으로 동일한 장치에서 Llamav2-7B에 대한 하나의 LLM 생성 실험을 실행합니다. 본 실험에서는 MatMul 연산과 1차원 VQ 복호 커널을 통합하였다.\n' +
      '\n' +
      '### 하이퍼모수 선택에 대한 분석\n' +
      '\n' +
      '좋은 종자 중심 세트에서 EM 초기화를 시작하는 EM 초기화는 최종 GPTVQ 천공에 중요하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c c c c} \\hline \\hline  & & \\multicolumn{6}{c}{WikiText2 perplexity \\(\\downarrow\\)} & \\multicolumn{6}{c}{Zeroshot avg acc. \\(\\uparrow\\)} \\\\  & & L2-7B & L2-13B & L2-70B & M-7B & 8x7B & L2-7B & L2-13B & M-7B & 8x7B \\\\ \\hline FP16 & & 5.47 & 4.88 & 3.31 & 5.25 & 3.84 & 70.47 & 73.22 & 75.69 & 75.93 \\\\ \\hline \\multirow{7}{*}{2.125 bvp} & RTN & 4.263 & 122.08 & 27.27 & 1.4e3 & 4.3e3 & 36.94 & 42.06 & 37.75 & 38.29 \\\\  & GPTQ & 36.77 & 28.14 & 6.74 & 15.68 & 14.17 & 41.44 & 46.56 & 41.93 & 44.54 \\\\  & AWQ & 2.265 & 1.2e5 & - & - & - & - & - & - & - \\\\  & OmniQuant & 11.06 & 8.26 & 6.55 & - & - & - & - & - & - \\\\  & **GPTVQ 10 (ours)** & 11.57 & **7.34** & **5.00** & **15.03** & **8.11** & **47.51** & **60.82** & **44.85** & **57.54** \\\\  & **GPTVQ 2D (ours)** & **8.23** & **6.50** & **4.64** & **10.28** & **6.37** & **57.24** & **64.46** & **57.25** & **64.50** \\\\ \\hline \\multirow{7}{*}{2.25 bvp} & RTN & 431.97 & 26.22 & 10.31 & 71.52 & 155.82 & 42.40 & 46.41 & 44.79 & 46.86 \\\\  & GPTQ & 20.85 & 22.44 & NAN & 14.24 & 10.07 & 47.51 & 54.16 & 51.76 & 48.78 \\\\  & AWQ & 2.1e5 & 1.2e5 & - & - & - & - & - & - & - \\\\  & OmniQuant & 9.62 & 7.56 & 6.11 & - & - & - & - & - & - \\\\  & **GPTVQ 10 (ours)** & 10.08 & **7.17** & **4.82** & **9.56** & **8.06** & **51.95** & **61.48** & **55.82** & **57.12** \\\\  & **GPTVQ 2D (ours)** & **7.97** & **6.47** & **4.61** & **10.11** & **6.23** & **59.08** & **64.85** & **56.14** & **63.92** \\\\  & **GPTVQ 4D (ours)** & **7.22** & **6.08** & **4.39** & **7.16** & **5.55** & **61.49** & **66.17** & **64.44** & **66.43** \\\\ \\hline \\multirow{7}{*}{3.125 bvp} & RTN & 6.66 & 5.51 & 3.97 & 6.15 & 5.18 & 67.25 & 70.75 & 71.79 & 72.40 \\\\  & GPTQ & 6.29 & 5.42 & 3.85 & 5.83 & 4.71 & 66.16 & 71.44 & 72.24 & 72.73 \\\\ \\cline{1-1}  & AWQ & 6.24 & 5.32 & - & - & - & - & - & - & - \\\\ \\cline{1-1}  & OmniQuant & 6.03 & 5.28 & 3.78 & - & - & - & - & - & - \\\\ \\cline{1-1}  & **GPTVQ 10 (ours)** & **5.98** & **5.17** & **3.62** & **5.76** & **4.59** & **67.61** & **71.59** & 71.56 & **72.75** \\\\ \\cline{1-1}  & GPTVQ 2D (ours)** & **5.82** & **5.10** & **3.55** & **5.51** & **4.30** & **67.88** & **71.76** & **73.56** & **74.36** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **Llama-v2, Mistral, Mistral-MoE Models**의 가중치 전용 양자화 결과. We report WikiText2 perplexity and average zero-shot accuracy; \'L2\'로 표시된 모델은 Llama-v2, M은 Mistral, 8x7B는 Mistral-MoE 8x7B를 나타낸다. 굵게 표시된 숫자는 SOTA이거나 이를 능가하며 밑줄이 그어진 숫자는 적어도 하나의 VQ 변형과 동등하거나 능가한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Setting & BPV \\(\\downarrow\\) & Relative \\(\\downarrow\\) & Relative \\(\\downarrow\\) \\\\  & footprint & latency \\\\ \\hline INT4 & 4 & 1.00\\(\\times\\) & 1.00\\(\\times\\) \\\\ INT8 & 8 & 2.00\\(\\times\\) & 1.93\\(\\times\\) \\\\ \\hline\n' +
      '2D 2.5B @ 512 & 3 & 0.75\\(\\times\\ & 0.98\\(\\times\\) \\\\\n' +
      '2D 2.5B @ 2048 & 2.25 & 0.56\\(\\times\\) & 0.96\\(\\times\\) \\\\\n' +
      '2D 2B @ 1024 & 2.25 & 0.56\\(\\times\\) & 0.87\\(\\times\\) \\\\ \\hline Llamav2-7B & 3.5 & 0.88\\(\\times\\) & 0.96\\(\\times\\) \\\\\n' +
      '1D 3B @ 128 & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: **벡터 양자화된 데이터 전송 및 디코딩의 **모델 풋프린트 및 레이턴시.**mance. 본 논문에서는 EM 초기화를 위한 seed centroids를 찾기 위해 k-Means++ (Arthur and Vassilvitskii, 2007)를 _Mahalanobis initialization_로 명명하는 빠르고 효과적인 초기화 방법과 비교한다. 후자의 방법에서, 우리는 다음과 같이 \\(N\\)\\(d\\)차원 점 \\(\\mathbf{X}\\)의 행렬에 대해 EM을 초기화한다. 먼저, \\(\\mathbf{X}\\)의 평균까지의 마할라노비스 거리 \\(a\\)에 의해 모든 점을 정렬한다. \\(a^{(i)}=(x^{(i)}-\\mu)^{T}\\mathbf{\\Sigma}^{-1}(x^{(i)}-\\mu)\\), \\(\\mu\\)은 데이터의 평균이고 \\(\\mathbf{\\Sigma}\\)의 공분산 행렬이다. 그 다음, \\(k\\)점을 샘플링하기 위해 정렬된 목록에서 \\(\\lfloor\\frac{k-1}{N}\\rceil\\)으로 균등하게 이격된 \\(k\\)점을 샘플링한다. 이론적으로는 정당하지 않지만, 직관적으로 이 방법은 점들이 대표 거리에서 샘플링되는 것을 보장한다. 표 6은 EM 초기화를 위한 양호한 시드 값을 찾는 다른 방법에 대한 GPTVQ 이후의 복잡성을 나타낸다. 여기에서 마할라노비스 초기화가 상당히 증가된 속도로 k-Means++와 비교 가능하게 수행됨을 알 수 있다.\n' +
      '\n' +
      'EM 반복은 GPTVQ의 복잡성의 최종에 대한 EM 초기화 반복 수의 영향을 탐구한다. 표 7은 최대 100번의 반복에도 결과가 약간 개선되므로 100번의 반복을 기본값으로 사용한다는 것을 보여준다.\n' +
      '\n' +
      '섹션 4.1에서 언급한 바와 같이 코드북 오버헤드는 특정 오버헤드를 목표로 하는 그룹 크기를 결정한다. 그러나, 코드북들을 더 낮은 비트폭들로 양자화하는 경우, 또는 섹션 3.3에 설명된 바와 같이 코드북 압축이 적용되는 경우, 그룹 크기는 동일한 오버헤드를 달성하기 위해 비례적으로 감소될 수 있다.\n' +
      '\n' +
      '본 논문에서는 코드북을 16비트로 유지하는 방법, 코드북을 8비트로 양자화하여 블록 크기를 절반으로 줄이는 방법, 코드북을 16비트로 유지하는 방법, 코드북의 랭크를 원래 랭크의 50%로 줄이고 블록 크기를 절반으로 줄이는 방법 중 어떤 방법이 가장 좋은 결과를 얻을 수 있는지 평가한다.\n' +
      '\n' +
      '표 8에서 이러한 실험의 결과는 전반적으로 코드북을 8비트로 양자화하는 것이 일반적으로 약간 개선된 결과를 산출한다는 것을 보여준다.\n' +
      '\n' +
      '표 9에서 코드북 업데이트는 섹션 3.3에 설명된 대로 코드북 업데이트를 포함한 효과에 대한 삭제를 포함한다. 모든 경우에 알고리즘 1을 실행한 후 코드북 업데이트는 적당히 증가된(여전히 합리적이지만) 실행 시간을 희생시키면서 최종 복잡성을 개선한다는 것을 발견했다. 따라서 모든 교육 실행에서 코드북 업데이트를 포함합니다.\n' +
      '\n' +
      '메서드 런타임 우리의 메서드는 대용량 언어 모델을 효율적으로 처리할 수 있다. GPTVQ의 정확한 실행 시간은 모델, 양자화 설정(그룹 크기, 비트 폭, vq-차원) 및 여러 하이퍼 파라미터(EM 반복, 코드북 업데이트 반복)에 따라 달라진다. 현실적인 실행 시간을 표시하려면 단일 H100에서 라마브2-7B는 30분에서 1시간 사이에 걸리는 반면 라마브2-70B는 3시간에서 11시간 사이에 걸린다.\n' +
      '\n' +
      '블록별 데이터 정규화의 효과 섹션 3.2에 설명된 입력 데이터 정규화를 적용하는 것이 최종 성능에 어떤 영향을 미치는지 조사한다. 표 10은 양자화된 모델의 복잡성이 스케일링 블록 크기에 어떻게 의존하는지를 보여준다. 또한, 스케일링을 적용하거나 적용하지 않은 동일한 오버헤드의 구성에 대해 복잡성을 비교했으며 결과는 표 11을 참조한다. 전반적으로, 우리는 스케일링이 많은 경우 결과를 개선하지만, 때로는 특히 인덱스당 2비트를 갖는 1D VQ의 경우 복잡성 증가로 이어진다는 것을 안다.\n' +
      '\n' +
      '##5 관련사항\n' +
      '\n' +
      '벡터 양자화는 CNN 가중치 압축을 위한 벡터 양자화를 사용하여 제안된 다수의 작업들(Gong et al., 2014; Martinez et al., 2021; Fan et al., 2020; Stock et al., 2019; Wu et al., 2016; Martinez et al., 2021; Cho et al., 2021). 가장 일반적인 접근법은 컨볼루션 또는 완전히 연결된 레이어의 가중치를 매트릭스로 재구성한 다음 컬럼에 직접 K-평균 군집링을 적용하는 것이다. 일반적으로 클러스터링은 스칼라 또는 차원 4 이상의 벡터에 적용된다. 일부 작업은 양자화된 가중치의 데이터 인식 최적화를 고려한다. 대부분의 경우, 중심체 및 할당을 업데이트하기 위해 EM 알고리즘의 변형이 사용된다(Stock et al., 2019; Gong et al., 2014). 대안적인 접근법은 네트워크 정확도를 회복하기 위해 원래의 손실 함수와 함께 SGD를 사용하여 미세 조정을 가능하게 하는 미분 가능한 K-means 공식을 사용하는 것이다(Cho et al., 2021; Fan et al., 2020; Tang et al., 2023).\n' +
      '\n' +
      '최근 LLM에 대한 DNN 양자화 접근법을 적용하는 LLLM 양자화는 종종 상당한 계산 문제를 제기한다. 따라서, 균일한 훈련 후 양자화 방법조차도 그들의 확장성을 향상시키기 위해 재방문이 필요했다(Frantar et al., 2022). 벡터 양자화 방법은 계산 복잡도가 더 높기 때문에 LLM 가중치 압축에 사용하면 계산 요구 사항이 훨씬 더 엄격해진다. 우리의 작업과 가장 유사한 것은 접근법이다(Deng et al., 2024). 이 방법은 코드북들을 업데이트하기 위해 그래디언트 기반 계층 민감성을 사용하고 정확도를 부분적으로 회복하기 위해 감소된 복잡도 LoRA 기반 접근법(Hu et al., 2021)을 사용한다.\n' +
      '\n' +
      '헤시안 기반 압축 방법들 몇몇 고전적인 연구들은 정확한 비구조적 프루닝(LeCun et al., 1989; Hassibi et al., 1993)을 위해 신경망 손실 함수의 2차 근사치를 사용하는 것을 제안한다. 보다 최근의 논문들의 라인은 PTQ(Singh and Alistarh, 2020; Frantar and Alistarh, 2022; Frantar et al., 2022)를 위한 이러한 방법들의 패밀리를 확장한다.\n' +
      '\n' +
      '## 6 Conclusions\n' +
      '\n' +
      '이 연구에서 우리는 하나 이상의 차원에서의 벡터 양자화가 양자화된 모델 정확도를 점진적으로 향상시킨다는 것을 보여주었다. 본 논문에서는 VQ를 이용한 대규모 네트워크의 훈련 후 양자화를 위한 빠른 방법을 소개한다. 이 방법은 광범위한 LLM 및 제로 샷 작업에서 SOTA 모델 크기 대 정확도 절충을 달성한다. 마지막으로, 우리는 VQ가 압축 방법으로 균일한 양자화에 대한 HW 실현 가능한 대안을 제시하여 동일한 정확도로 초당 증가된 토큰을 생성하거나 초당 고정된 토큰에 대해 더 높은 정확도를 제공한다는 것을 보여주었다.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      '우리는 유용한 토론에 대해 아미르 사이드에게 감사를 표하고 싶다.\n' +
      '\n' +
      'Impact\n' +
      '\n' +
      '효율성 우리의 방법은 모델을 더 효율적으로 만드는 데 사용될 수 있다. 벡터 양자화된 네트워크를 지원하는 하드웨어 및 소프트웨어 스택이 주어지면, 사용자는 주어진 에너지 버짓에 대해 더 많은 추론을 실행하거나, 고정된 추론 작업에 필요한 에너지를 감소시킬 수 있다.\n' +
      '\n' +
      '신경망의 추론 비용을 줄이는 민주화는 일반적으로 더 많은 실무자들이 모델을 배치할 수 있게 하고 딥 러닝의 민주화를 증가시킨다. 우리의 방법 자체는 매우 큰 네트워크에서도 소비자 등급의 하드웨어에서 실행될 수 있을 만큼 충분히 효율적이다.\n' +
      '\n' +
      '바이어스(bias)는 모델 프루닝이 신경망에서 바이어스를 증가시킬 수 있는 것으로 나타났지만(Iofinova et al., 2023), 이것이 양자화에 대한 경우인지 그리고 어느 정도까지, 그리고 이것이 큰 언어 모델에 어떻게 적용되는지는 미개척된 주제이다. 이 주제에 대한 조사는 이 논문의 범위를 벗어났지만, 우리는 우리의 방법이 양자화된 모델에 미묘한 편향을 도입할 수 있음을 인정한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 기술 보고서. _ arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Arthur and Vassilvitskii (2007) Arthur, D. and Vassilvitskii, S. K-means++는 세심한 파종술의 장점입니다. In _Proceedings of the 18 annual ACM-SIAM symposium on Discrete algorithms_, pp. 1027-1035, 2007.\n' +
      '* Bisk et al. (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piga: Reasoning about physical commonsense in natural language. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pp. 7432-7439, 2020.\n' +
      '* Cho et al. (2021) Cho, M., Vahid, K. A., Adya, S., and Rastegari, M. Dkm: 신경망 압축을 위한 미분 k-means 클러스터링 계층_ arXiv preprint arXiv:2108.12659_, 2021.\n' +
      '* Clark et al. (2019) Clark, C., Lee, K., Chang, M. - W., Kwiatkowski, T., Collins, M., and Toutanova, K. 불크: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구하는 것. 2019년 _NAACL_에서\n' +
      '* Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 질문에 답하는 걸 해결했다고 생각해? try arc, the ai2 reasoning challenge. _ arXiv:1803.05457v1_, 2018.\n' +
      '* Deng et al. (2024) Deng, J., Li, S., Wang, C., Gu, H., Shen, H., and Huang, K. LLM-codebook for extreme compression of large language models, 2024. URL[https://openreview.net/forum?id=nMDWsXPUVL](https://openreview.net/forum?id=nMDWsXPUVL).\n' +
      '* Fan et al. (2020) Fan, A., Stock, P., Graham, B., Grave, E., Gribonval, R., Jegou, H., and Joulin, A. Training with quantization noise for extreme model compression. _ arXiv preprint arXiv:2004.07320_, 2020.\n' +
      '* Frantar and Alistarh (2022) Frantar, E. and Alistarh, D. Optimal brain compression: 정확한 post-training quantization and pruning을 위한 framework. _ 신경 정보 처리 시스템_, 35:4475-4488, 2022에서의 발전.\n' +
      '* Frantar et al. (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pre-trained transformer. _ ArXiv:2210.17323_, 2022.\n' +
      '* Gao et al. (2023) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noac\'h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. URL[https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)\n' +
      '* Gersho and Gray (2012) Gersho, A. and Gray, R. M. _Vector quantization and signal compression_, volume 159. Springer Science & Business Media, 2012.\n' +
      '* Gholami et al. (2022) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., and Keutzer, K. 효율적인 신경망 추론을 위한 양자화 방법에 대한 조사. In _Low-Power Computer Vision_, pp. 291-326. Chapman and Hall/CRC, 2022.\n' +
      '* Gong et al. (2014) Gong, Y., Liu, L., Yang, M., and Bourdev, L. 벡터 양자화를 이용하여 심층 컨볼루션 네트워크를 압축하는 단계 _ arXiv preprint arXiv:1412.6115_, 2014.\n' +
      '* Han et al. (2018) Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning,trained quantization and huffman coding. _ arXiv preprint arXiv:1510.00149_, 2015.\n' +
      '* Hassibi et al. (1993) Hassibi, B., Stork, D. G., and Wolff, G. J. Optimal brain surgeon and general network pruning. In _IEEE international conference on neural networks_, pp. 293-299. IEEE, 1993.\n' +
      '* He et al. (2017) He, Y., Zhang, X., and Sun, J. Channel pruning for accelerating very deep neural networks. In _Proceedings of the IEEE International Conference on Computer Vision_, pp. 1389-1397, 2017.\n' +
      '* Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: 대형 언어 모델의 낮은 랭크 적응. _ arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* Iofinova et al. (2023) Iofinova, E., Peste, A., and Alistarh, D. Bias in pruned vision models: Indepth analysis and countermeasures. _ IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR_, pp. 24364-24373, 2023.\n' +
      '* Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. _ arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mistral of experts. _ arXiv preprint arXiv:2401.04088_, 2024.\n' +
      '* Keisuke et al. (2019) Keisuke, S., Ronan, L. B., Chandra, B., and Yejin, C. Winogrande: An adversarial winograd schema challenge at scale. 2019년\n' +
      '* LeCun et al. (1989) LeCun, Y., Denker, J., and Solla, S. 최적의 뇌 손상이야 신경 정보 처리 시스템_, 2, 1989의 발전.\n' +
      '* Lin et al. (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S. Awq: llm 압축 및 가속을 위한 활성화 인식 가중치 양자화. _ arXiv preprint arXiv:2306.00978_, 2023.\n' +
      '* Martinez et al. (2021) Martinez, J., Shewakramani, J., Liu, T. W., Barsan, I. A., Zeng, W., and Urtasun, R. 순수, 양자화 및 미세 조정: 신경망의 효율적인 압축. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 15699-15708, 2021.\n' +
      '* Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. 포인터 센티넬 혼합 모델 2016년\n' +
      '* Nagel et al. (2020) Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. 위로요, 아래로요? 훈련 후 양자화를 위한 적응적 반올림. In _International Conference on Machine Learning_, pp. 7197-7206. PMLR, 2020.\n' +
      '* Nagel et al. (2021) Nagel, M., Fournarakis, M., Amjad, R. A., Bondarenko, Y., Van Baalen, M., and Blankevoort, T. 신경망 양자화에 관한 백서 arXiv preprint arXiv:2106.08295_, 2021.\n' +
      '* Rouhani et al. (2023) Rouhani, B., Zhao, R., Elango, V., Shafipour, R., Hall, M., Mesmakhorsoshahi, M., More, A., Melnick, L., Golub, M., Varatkar, G., Shao, L., Kolhe, G., Melts, D., Klar, J., L\'Heureux, R., Perry, M., Burger, D., Chung, E., Deng, Z., and Naumov, M. 공유된 마이크로 컴포넌트들과 함께, 약간의 이동은 먼 길을 간다. In _Proceedings of the 50th Annual International Symposium on Computer Architecture_, pp. Article No. 83, 2023.\n' +
      '* Roziere et al. (2023) Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code lama: Open foundation models for code. _ arXiv preprint arXiv:2308.12950_, 2023.\n' +
      '* Shao et al. (2023) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang, K., Gao, P., Qiao, Y., and Luo, P. Omniquant: Omnidirectionally calibrated quantization for large language models. _ arXiv preprint arXiv:2308.13137_, 2023.\n' +
      '* Singh & Alistarh (2020) Singh, S. P. and Alistarh, D. Woodfisher: neural network compression을 위한 효율적인 2차 근사치 _ 신경 정보 처리 시스템_, 33:18098-18109, 2020에서의 발전.\n' +
      '* Stock et al. (2019) Stock, P., Joulin, A., Gribonval, R., Graham, B., and Jegou, H. And the bit goes down: Revisiting the quantization of neural networks. _ ArXiv preprint arXiv:1907.05686_, 2019.\n' +
      '* Tang 등(2023) Tang, X., Wang, Y., Cao, T., Zhang, L. L., Chen, Q., Cai, D., Liu, Y., and Yang, M. Lut-nn: 중심 학습 및 테이블 룩업을 이용한 Empower 효율적인 신경망 추론. In _Proceedings of the 29th Annual International Conference on Mobile Computing and Networking_, pp. 1-15, 2023.\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. - A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023b.\n' +
      '* Tu et al. (2024) Tu, T., Palepu, A., Schaekermann, M., Saab, K., Freyberg, J., Tanno, R., Wang, A., Li, B., Amin, M., Tomasev, N., et al. arXiv preprint arXiv:2401.05654_, 2024.\n' +
      '\n' +
      '*(2016) Workshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A. S., Yvon, F., et al. Bloom: A 176b-parameter open-access multilingual language model _ ARXiv 프리프린트 arXiv:2211.05100_, 2022.\n' +
      '*(2016) Wu, J., Leng, C., Wang, Y., Hu, Q., and Cheng, J. Quantized convolutional neural networks for mobile devices. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 4820-4828, 2016.\n' +
      '*(2016) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. 헬라스바그: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, 2019.\n' +
      '*(2016) Zhang, X., Zou, J., He, K., and Sun, J. Accelerating very deep convolutional networks for classification and detection. _ IEEE transactions on pattern analysis and machine intelligence_, 38(10):1943-1955, 2016.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:12]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c} \\hline \\hline \\(\\uparrow\\) & \\#Bits & Method & PIQA & ARC-e & Arc-c & BoolQ & HellaSwag & Winogrande & **Avg.** \\\\ \\hline \\multirow{8}{*}{Llama-v2-7B} & FP16 & 79.11 & 74.58 & 46.25 & 77.74 & 75.99 & 69.14 & 70.47 \\\\ \\cline{2-10}  & RTN & 51.09 & 27.95 & 25.00 & 41.13 & 26.57 & 49.88 & 36.94 \\\\\n' +
      '2.125 bpv & GPTQ & 54.84 & 30.64 & 25.09 & 53.43 & 33.09 & 51.54 & 41.44\\\\\n' +
      '**(W2@g128** & **62.95** & **40.28** & **22.61** & **44.63** & **52.72** & **47.51**\\\\\\& **47.72** & *VQ-2D** & **70.73** & *58.08** & *31.48** & *63.73** & **63.93** & **57.24** & 58.76 & 36.83 & 24.83 & 41.93 & 42.40 \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n' +
      '2.25 bpv & GPTQ & 60.83 & 39.02 & 25.17 & 45.49 & 47.61 \\\\\\** & **64.95** & **56.83** & **51.95** & *63.71 & 67.64 & **63.69** & **63.09** & **63.09** & **63.09** & **63.09**\n' +
      '3.125 bpv & GPTQ & 77.37 & 68.14 & 40.70 & 71.04 & 72.50 & 67.25 & 66.16\\\\\n' +
      '**(W3@g128)** & **VQ-1D** & **77.64** & **70.12** & **42.15** & **75.90** & **71.42** & **68.43** & **67.61** \\\\  & **VQ-2D** & **77.64** & **72.73** & **43.69** & **71.65** & **72.71** & **67.64** & **67.68** \\\\ \\hline \\multirow{8}{*}{Llama-v2-13B} & FP16 & 80.52 & 77.53 & 49.23 & 80.52 & 79.38 & 72.14 & 73.22 \\\\ \\cline{2-10}  & RTN & 58.43 & 32.32 & 25.51 & 47.86 & 39.40 & 48.86 & 42.06 \\\\\n' +
      '2.125 bpv & GPTQ & 59.52 & 40.15 & 27.65 & 57.06 & 41.56 & 53.43 & 46.56\\\\\n' +
      '**&*VQ-1D**&*72.74**&*63.75**&*65.54**&*61.60**&*65.43**&*60.82**&*VQ-2D**&*75.19**&*68.27**&*39.51**&*70.67**&*65.66**&*63.74**&*63.75**&*65.54**&*61.62&58.88&54.16\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n' +
      '**VQ-1D** & **72.91** & **65.32** & **66.48** & **65.11** & **61.48** & *VQ-2D** & *74.92** & **39.51** & *59.95** & **67.36** & **64.38** & **68.17** & **68.19** & *75\\\\\\cline{2-10}\n' +
      '3.125 bpv & GPTQ & 79.33 & 75.84 & 47.01 & 78.90 & 77.16 & 70.40 & 71.44\\\\\n' +
      '**(W3@g128)** & **VQ-1D** & **78.78** & **75.85** & **47.35** & **79.36** & **76.57** & **71.90** & **71.59** \\\\  & **VQ-2D** & **79.43** & **75.29** & **48.12** & **78.99** & **76.96** & **71.74** & **71.76** \\\\ \\hline \\multirow{8}{*}{Mistral-7B} & FP16 & 82.10 & 79.59 & 53.92 & 83.58 & 81.07 & 73.88 & 75.69 \\\\ \\cline{2-10}  & RTN & 53.05 & 29.24 & 26.62 & 38.56 & 29.26 & 49.57 & 37.75 \\\\\n' +
      '2.125 bpv & GPTQ & 57.73 & 35.65 & 26.62 & 46.06 & 41.93\\\\cline{1-1} & **58.71** & **38.85** & **23.89** & **37.40** & **50.83** & RTN & 60.72 & 38.47 & 44.10 & 46.47 & 46.10 & **55.09** & **68.09** & **68.09** & **68.09\n' +
      '2.25 bpv & GPTQ & 65.83 & 46.21 & 30.20 & 62.11 & 50.64 & 55.56 & 51.76\\\\\n' +
      '**(W2@g64** & **VQ-1D** & **66.27** & *57.85** & *33.35** & *70.58** & *51.53** & *55.41** & *55.82**\n' +
      '**(W2@g64)** & **VQ-2D** & **68.01** & **59.85** & **33.53** & **51.40** & **58.01** & **56.14** \\\\\\ & **VQ-4D** & **72.80** & **69.28** & **40.02** & **63.03** & **66.54** & **64.44** & 74.62 & 48.46 & 68.19 & 71.79\\\\\\ cir{2-10}\n' +
      '3.125 bpv & GPTQ & 79.82 & 75.51 & 49.40 & 81.22 & 77.34 & 70.17 & 72.24\\\\\\\n' +
      '3.125 bpv & **VQ-1D** & **79.76** & **75.04** & **47.53** & *79.69** & *75.91** & *71.43** & *71.56**\n' +
      '**(W3@g128)** & **VQ-2D** & **80.41** & **77.23** & **49.57** & **82.72** & **78.52** & **72.93** & **73.56** \\\\ \\hline \\multirow{8}{*}{Mixtral-8x7B} & FP16 & 83.46 & 73.74 & 55.89 & 84.74 & 82.45 & 75.30 & 75.93 \\\\ \\cline{1-1} \\cline{2-10}  & RTN & 51.90 & 27.27 &\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:14]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline \\(d\\) & \\(b\\) & gs & \\multicolumn{5}{c}{Scaling BS} \\\\  & & & None & 128 & 64 & 32 & 16 & 8 \\\\ \\hline \\multirow{2}{*}{1} & 2 & 512 & 14.01 & 16.74 & 2744.9 & 480.8 & 15.36 & 13.79 \\\\ \\cline{2-7}  & 3 & 1024 & 6.02 & 5.97 & 6.00 & 5.87 & 5.82 & 5.72 \\\\ \\hline \\multirow{2}{*}{2} & 2 & 2048 & 8.23 & 8.38 & 8.04 & 7.97 & 7.56 & 6.89 \\\\ \\cline{2-7}  & 3 & 8192 & 5.91 & 5.82 & 5.78 & 5.73 & 5.74 & 5.66 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: ** 스케일링 블록 크기가 Llamav2-7B에 대한 복잡도에 미치는 영향. \\ (d\\)**: VQ-dimension; \\(b\\): 차원당 VQ 비트폭; gs: 블록 크기; 코드북은 8비트로 양자화된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\hline \\(d\\) & \\(b\\) & gs & Scale & Llamav2-7B & Llamav2-13B & Mistral-7B & Mistral-8x7B \\\\ \\hline \\multirow{2}{*}{1} & 2 & 256 & N & 14.01 & 7.34 & 15.03 & 8.56 \\\\  & & 512 & Y & 171.29 & 7.44 & 87.60 & 8.11 \\\\ \\cline{2-7}  & 3 & 512 & N & 5.98 & 5.21 & 5.76 & 4.60 \\\\ \\cline{2-7}  & & 1024 & Y & 6.01 & 5.17 & 5.77 & 4.59 \\\\ \\hline \\multirow{2}{*}{2} & 2 & 2048 & N & 8.23 & 6.69 & 10.98 & 6.73 \\\\  & & 4096 & Y & 8.49 & 6.50 & 10.28 & 6.37 \\\\ \\cline{2-7}  & 3 & 8192 & N & 5.91 & 5.19 & 8.63 & 4.52 \\\\ \\cline{2-7}  & 16384 & Y & 5.56 & 5.11 & 5.53 & 4.30 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: **스케일링이 다른 모델에 대한 복잡성에 미치는 영향**. 스케일링이 있거나 없는 동일한 오버헤드를 갖는 구성들이 고려된다. \\ (d\\): VQ-dimension; \\(b\\): 차원당 VQ 비트폭; gs: 블록 크기; 코드북은 8비트로 양자화되는 것으로 가정한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline \\(d\\) & \\(b\\) & gs & Update & PPL & Runtime (s) \\\\ \\hline \\multirow{4}{*}{1} & 2 & 512 & \\begin{tabular}{c} N \\\\ Y \\\\ \\end{tabular} & 43.14 & 625 \\\\  & 14.02 & 1857 \\\\ \\cline{2-7}  & 3 & 1024 & \\begin{tabular}{c} N \\\\ Y \\\\ \\end{tabular} & 6.01 & 712 \\\\ \\cline{2-7}  & 2 & 2048 & \\begin{tabular}{c} N \\\\ Y \\\\ \\end{tabular} & 8.64 & 723 \\\\  & 8.21 & 1335 \\\\ \\cline{2-7}  & 3 & 8192 &\n' +
      '\\begin{tabular}{c} N \\\\ Y \\\\ \\end{tabular} & 5.93 & 1585 \\\\ \\cline{2-7}  & 5.88 & 2195 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: **코드북 미세조정이 Llamav2-7B에 대한 최종 PPL에 미치는 영향.**\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>