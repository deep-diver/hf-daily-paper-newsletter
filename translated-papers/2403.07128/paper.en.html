<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# FAX: Scalable and Differentiable Federated Primitives in JAX\n' +
      '\n' +
      ' Keith Rush, Zachary Charles & Zachary Garrett\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      'Seattle, WA USA\n' +
      '\n' +
      '{krush,zachcharles,zachgarrett}@google.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'We present FAX, a JAX-based library designed to support large-scale distributed and federated computations in both data center and cross-device applications. FAX leverages JAX\'s sharding mechanisms to enable native targeting of TPUs and state-of-the-art JAX runtimes, including Pathways (Barham et al., 2022). FAX embeds building blocks for federated computations as primitives in JAX. This enables three key benefits. First, FAX computations can be translated to XLA HLO. Second, FAX provides a full implementation of federated automatic differentiation (Rush et al., 2023), greatly simplifying the expression of federated computations. Last, FAX computations can be interpreted out to existing production cross-device federated compute systems. We show that FAX provides an easily programmable, performant, and scalable framework for federated computations in the data center. FAX is available at [https://github.com/google-research/google-research/tree/master/fax](https://github.com/google-research/google-research/tree/master/fax).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'The ability to scale abstractly written compute-intensive programs across large distributed compute environments is a key factor in the success of modern machine learning (ML). This is crucial for ML computations involving large language models, which are often too large to fit on a single compute node. Another key facet of modern ML software is the general ease with which computations can be written and optimized. Techniques such as automatic differentiation (AD) and just-in-time (JIT) compilation have enabled frameworks such as PyTorch (Paszke et al., 2019), TensorFlow (Abadi et al., 2016), and JAX (Bradbury et al., 2018) to scale to larger and more complex ML workloads.\n' +
      '\n' +
      'In this work we describe a software library, FAX, that brings these benefits - sharding, easy-to-use JIT compilation, and AD - to the types of computations used in _federated learning_ (FL). FL is a distributed learning paradigm in which clients collaborate on an ML task without sharing data. This means that computations for FL, which we refer to as _federated computations_, often involve many clients training models in parallel with periodic synchronization (McMahan et al., 2017).\n' +
      '\n' +
      'While FL applications may involve on-device clients (e.g. cross-device FL (Kairouz et al., 2021)), performant data center software is still crucial. First, it can accelerate FL research, which is performed almost entirely in the data center. Second, FL algorithms can be used in data centers for specific learning objectives. For example, FL algorithms can meta-learn (Finn et al., 2017) over heterogeneous data sources (Jiang et al., 2019; Charles et al., 2023) or can be combined with differential privacy to obtain group-level privacy guarantees (McMahan et al., 2018). Last, production FL can involve using data center simulation to determine which algorithms to translate into production. This process is often complex, and mismatches in data center and production system expectations can be challenging (Minec et al., 2023). An ideal authoring surface for federated computations provides several features simultaneously: performant and scalable data center computation, easy and extensible algorithm expression, and automated translation to production federated systems.\n' +
      '\n' +
      'Our contributions.We present FAX, a library for defining scalable distributed and federated computations in the data center. FAX embeds a federated programming model (Section 2) into JAX via JAX\'s Primitive mechanism (Section 3). This allows FAX to use powerful features like sharding and JIT-compilation to XLA. For example, FAX can shard computations across clients, model, and within-client data simultaneously across physical and logical meshes of devices. FAX can also leverage advances in distributed data-center training like GSPMD (Xu et al., 2021) and Pathways (Bahram et al., 2022). JAX\'s Primitive mechanism also enables forward- and reverse-mode differentiation. FAX leverages this to provide a full implementation of the Federated Automatic Differentiation (federated AD) framework (Rush et al., 2023). This allows users to differentiate through federated computations while preserving information about data location.\n' +
      '\n' +
      'We highlight the benefits above empirically in Section 4, demonstrating that FAX enables efficient and scalable federated training of language models. Finally, in Section 5 we show that FAX computations can be interpreted out to Python-independent computation graphs usable by production FL systems such as those discussed by Bonawitz et al. (2019) and Paulik et al. (2021). FAX\'s design ensures that functional transformations like jax.grad preserve FAX\'s ability to be lowered to XLA HLO or staged out to production FL systems.\n' +
      '\n' +
      'Federated learning and beyond.While we use FL to anchor our discussion of system design, implementation, and even the API of FAX, FAX can be used to express, shard, and run a wide range of ML computations in the data center. This includes many parallel and distributed algorithms including FedAvg (McMahan et al., 2017), FedOpt (Reddi et al., 2020), branch-train-merge (Li et al., 2022), DiLoCo (Douillard et al., 2023), and PAPA (Jolicoeur-Martineau et al., 2023). FAX is useful for a wide array of computations, including algorithms that do not require data minimization, or do not operate over heterogeneous data.\n' +
      '\n' +
      'Related work.Many frameworks targeting federated learning predate FAX. Without intending to be exhaustive, these frameworks include at least: TensorFlow-Federated (Ingerman and Ostrowski, 2019); PySytf (Ziller et al., 2021); FedJAX (Ro et al., 2021); FedScale (Lai et al., 2022); FedML (He et al., 2020); Flower (Beutel et al., 2020); FLUTE (Dimitriadis et al., 2022); FL_Pytorch (Burlachenko et al., 2021); FATE (Liu et al., 2021); and still others. These frameworks target diverse audiences and applications, from optimization researchers to companies wishing to collaboratively train a model without sharing data.\n' +
      '\n' +
      'While each of these frameworks has their own strengths, the motivation for FAX was anchored in three simultaneous desires, which to the best of our knowledge are not simultaneously satisfied by any of these frameworks: (1) scalable data center performance on large models (by contemporary standards, we mean models that can take tens or hundreds of gigabytes of memory to even load); (2) an implementation of federated AD; (3) a path to production systems running federated computations on mobile devices.\n' +
      '\n' +
      '## 2 System Design\n' +
      '\n' +
      'FAX is designed with two key ideas in mind. First, most federated computations can be built from subroutines already integral to distributed ML workloads, an observation drawn from the programming model of TensorFlow Federated (Ingerman and Ostrowski, 2019). What separates FL from ML is an accounting of where data lives and which subroutines require privacy safeguards. Second, we can implement federated AD (Rush et al., 2023) by combining standard AD techniques (e.g. computation graphs (Bauer, 1974)) with this data location accounting. We defer to (Charles et al., 2022; Rush et al., 2023) for in-depth discussions of federated programming models and data location accounting.\n' +
      '\n' +
      'To do this accounting, FAX operates on _federated values_. These are values coupled with a data location, called the value\'s _placement_. We consider two placements: Clients-placed, and server-placed. We assume there are potentially many clients participating in a given computation, but only a single server.1 We denote a federated value by \\(\\textit{v@P}\\) where \\(P\\in\\textit{v@P}\\) is the number of clients in the database. We denote the number of clients in the database as \\(\\textit{v@P}\\).\n' +
      '\n' +
      '\\(\\{C,S\\}\\) denotes clients and server placement, respectively. While \\(v@S\\) is a singleton, \\(v@C\\) is multi-valued; given a set of clients \\(M\\), \\(v@C\\) is shorthand for the set \\(\\{v_{i}:i\\in M\\}\\).\n' +
      '\n' +
      'Federated computations are functions whose inputs and outputs are federated values. While these can be arbitrary functions, we focus on an important subclass inspired by map-reduce frameworks. Rush et al. (2023) consider federated computations that can be built from the following federated computations, which we refer to as **federated building blocks** :\n' +
      '\n' +
      '1. federated_broadcast: Broadcasts a server-placed value to all clients. Thus, \\(\\texttt{federated\\_broadcast}(x@S)=x@C\\).\n' +
      '2. federated_map: Applies a non-federated function \\(f\\) to all values at a given placement, preserving their placement. Thus, \\(\\texttt{federated\\_map}(f,x@P)=f(x)@P\\).\n' +
      '3. federated_sum: Sums a clients-placed value, returning a server-placed value. Thus, \\(\\texttt{federated\\_sum}(x@C)=\\left(\\sum_{i\\in M}x_{i}\\right)@S\\).\n' +
      '\n' +
      'The class of federated computations built from these building blocks includes most federated algorithms of interest, including FedAvg (McMahan et al., 2017). Rush et al. (2023) extend AD to this class. They show that if a federated computation \\(f:x@S\\to y@S\\) is built from these building blocks, then \\(\\nabla f:x@S\\to dy_{/d}x@S\\) can be computed using the same building blocks. Moreover, this can be done in a programmatic manner by coupling a non-federated AD with careful placement manipulations, yielding federated AD.\n' +
      '\n' +
      'This leads to our key observation: If we embed the building blocks above into JAX in a suitable manner, then we can (1) lower federated computations to data structures accepted by performant data center runtimes, (2) implement federated AD by appropriately delegating to JAX\'s AD, and (3) preserve data location information to enable translation to production FL systems. FAX does just this, embedding the building blocks into JAX in a JIT-compatible manner. FAX also provides implementations of Jacobian-vector and vector-Jacobian products of the federated building blocks. This allows FAX to perform forward- and reverse-mode AD on federated computations by delegating to JAX\'s forward- and reverse-mode AD.\n' +
      '\n' +
      '**Authoring surface.** FAX code is almost entirely JAX code, with two general exceptions. First, there are the federated building blocks above. Second, FAX code must specify how many clients there are during the invocation of the computation. To see this, consider the code in Snippet 1, which simply broadcasts a value to all clients, has clients double the value, and takes a sum over clients.\n' +
      '\n' +
      '```\n' +
      'defbroadcast_double_and_sum(x): y=fax.federated_broadcast(x)#Sendxtoallclients. z=fax.federated_map(lambdaa:2*a,y)#Eachclientcomputes2*x. returnfax.federated_sum(z)#Sumoverclients,returns2*num_clients*x.\n' +
      '```\n' +
      '\n' +
      'Snippet 1: An incomplete FAX program. The program must know the number of clients to correctly compute the desired result.\n' +
      '\n' +
      'To compute the result, FAX needs to know how many clients there are. The user has to give this information to the FAX programs. For example, Snippet 2 modifies Snippet 1 to include explicit information about the number of clients.\n' +
      '\n' +
      '```\n' +
      'defbroadcast_double_and_sum(x): y=fax.federated_broadcast(x)#Sendxtoall3clients. z=fax.federated_map(lambdaa:2*a,y)#Eachclientcomputes2*x. returnfax.federated_sum(z)#Sumoverclients,returns6*x.\n' +
      '```\n' +
      '\n' +
      'Snippet 2: A basic FAX program, with a decorator specifying the number of clients.\n' +
      '\n' +
      '## 3 Implementation\n' +
      '\n' +
      'We now discuss FAX\'s implementation in JAX, in particular how it represents federated values and implements federated computations on said values. We also discuss how we ensure FAX computations are effectively sharded across data center runtimes, and how FAX can implement federated AD. While we focus on the federated programming model above, we note FAX\'s lower-level implementation can be used for much more general distributed and even hierarchical processing and sharding patterns.\n' +
      '\n' +
      'Federated values.We first represent federated arrays. FAX represents these as arrays with an extra dimension indicating their placement. The first dimension of an array is simply the cardinality of its placement. Thus, a server-placed array has an extra leading axis of cardinality \\(1\\), while a clients-placed array has an extra leading axis of cardinality equal to the number of clients. Given an \\((n+1)\\)-dimensional array \\(x\\), the \\(i\\)-th component \\(x[i,...]\\) is the \\(n\\)-dimensional array held by the \\(i\\)-th client. Figure 2 gives an example of this representation.\n' +
      '\n' +
      'All JAX values are essentially represented as structures whose leaf nodes are arrays, which FAX carries forward. A federated structure is a structure of placed arrays. We note that a federated structure does not allow mixed placements, so all leaf arrays must have a leading axis of the same size. For example, Figure 2 gives an example of a clients-placed structure with multiple leaf arrays, all of which have the same leading dimension.\n' +
      '\n' +
      'Federated computations.Since federated values are represented as JAX arrays, federated computations defined via FAX operate on JAX arrays. Other goals of FAX, like scalability, data center performance, and the ability to implement federated AD, inform how FAX operates on arrays. We address these simultaneously by leveraging JAX\'s Primitive mechanism.\n' +
      '\n' +
      'Briefly, FAX defines the federated building blocks above at decorator-installation time. These building blocks are processed _symbolically_ by functional transformations in JAX. FAX registers the behavior of these operators under the action of these transformations, providing JAX with the necessary information to (1) lower FAX-defined functions wholesale to XLA HLO, (2) shard intermediate tensors in a maximally efficient manner, and (3) transform JAX functions containing FAX code under operations including JIT compilation and differentiation.\n' +
      '\n' +
      'Given the representation of federated values above, we can implement the federated building blocks via straightforward array operations:\n' +
      '\n' +
      '1. federated_broadcast: Tile an array over its leading axis.\n' +
      '2. federated_map: Apply a function point-wise over an array\'s leading axis.\n' +
      '3. federated_sum: Sum an array over its leading axis.\n' +
      '\n' +
      'We extend these to structures of federated arrays by applying them leaf-wise. FAX registers these implementations with JAX lowering logic. This ensures that FAX code is entirely replaced by JAX code by the time JAX dispatches logic to an XLA runtime. Other building blocks can be added to FAX by registering primitives in a similar fashion, or by defining them in terms of the building blocks above. For example, FAX provides a federated_mean symbol which takes an average across clients, which lowers to two calls to federated_sum.\n' +
      '\n' +
      'Partitioning FAX computations.By registering the primitives above, we ensure that compilers like GSPMD (Xu et al., 2021) can partition FAX computations across devices. However, we want to ensure the partitioned computations are as efficient as possible. While many federated computations are trivially parallelizable (e.g. federated_map), compilers may not be able to detect this usage pattern and generate efficient code. However, we only need to focus on how the building blocks above are sharded by compilers. Once this is done, we are free to compose with model- and data-parallelism provided by various JAX libraries.\n' +
      '\n' +
      'As noted by Xu et al. (2021) and Lepikhin et al. (2020), internal sharding annotations can dramatically affect the performance of a compiler targeting distributed execution. FAX uses static and dynamic sharding constraints to ensure that after compilation, the resulting computation will run efficiently in the data center. As we will see in Section 4, without these annotations, compilers like GSPMD do not optimally shard FAX computations, especially as the number of clients increases.\n' +
      '\n' +
      'Federated automatic differentiation.The last benefit of embedding federated building blocks as JAX primitives is that it gives us a straightforward way to implement federated AD. We only need to define the action of vector-Jacobian products (VJPs) and Jacobian-vector products (JVPs) on these primitives. Rush et al. (2023) discuss how to compute these products, and show that their computation does not require any new federated building blocks. That is, the JVPs and VJPs of these primitives can be expressed in terms of the same set of primitives. With the JVPs and VJPs, we can now entirely rely on JAX\'s AD to do forward- and reverse-mode AD on computations involving these primitives. For more details, see Section 5.\n' +
      '\n' +
      '## 4 Scalability and Efficiency\n' +
      '\n' +
      'We now present numerical evidence of the scalability and efficiency of FAX. We perform multiple rounds of FedAvg on transformer language models with 350 million (350M), 1 billion (1B), and 8 billion (8B) parameters. We use a causal language modeling loss and a sequence length of 512. Each client in a cohort performs 4 local SGD steps on their own data with a batch size of 8. We train on the FedCCNews dataset via Dataset Grouper (Charles et al., 2023). To describe the scale of the experiments, Table 1 contains the maximum number of tokens processed and model parameters updated per round for each model.\n' +
      '\n' +
      'Figure 3: A high-level depiction of FAX building blocks operating on placed arrays.\n' +
      '\n' +
      'For all experiments, we shard the training computation over some number of TPUv2s. The total number of TPU chips is always a multiple of the cohort size \\(M\\). For 350M, 1B, and 8B models we use \\(M\\), \\(4M\\), and \\(8M\\) chips in total, respectively. This means that if we double the cohort size, we also double the number of TPU chips used. We fully shard the computations across the \\(M\\) clients, and additionally do model parallelism for the 1B and 8B models. For all experiments, our FAX computations are first compiled using GSPMD (Xu et al., 2021) and then delegated to the Pathways runtime (Barham et al., 2022).\n' +
      '\n' +
      'Weak scaling.The _weak scaling_ of a system refers to how its compute time changes as the workload and compute resources scale simultaneously. Generally, modern ML systems attempt to obtain near-constant weak scaling performance.2 For FAX, we fix the model size and number of local SGD steps computed per client, and vary the cohort size in order to vary the workload size. As discussed above, we scale the number of TPU chips used in our simulations linearly with respect to the cohort size.\n' +
      '\n' +
      'Footnote 2: Constant performance is generally impossible due to overhead such as synchronization costs.\n' +
      '\n' +
      'Figure 4 shows how training time of FAX-based FedAvg scales as the cohort size and number of TPU chips increase, across a range model sizes. FAX-exhibits near-constant runtime for a fixed model size, even up to cohort sizes of 128 or 512. This is highly non-trivial. Because FedAvg involves parallel model training across cohorts, and for multiple steps per client, the per-round workload size (in terms of total floating point operations) is at least as large as \\(4\\times\\)(model size)\\(\\times\\)(cohort size). To see this, note that in each round, each client updates its own local model 4 times. As shown in Table 1, the largest workload for each model size involves updating over 1 trillion model parameters per round.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline\n' +
      '**Model Size** & **Cohort Size** & **Tokens per Round** & **FLOPs per Round** \\\\ \\hline\n' +
      '350M & 2048 & \\(3.355\\times 10^{7}\\) & \\(2.293\\times 10^{13}\\) \\\\\n' +
      '1B & 512 & \\(8.389\\times 10^{6}\\) & \\(1.638\\times 10^{13}\\) \\\\\n' +
      '8B & 128 & \\(2.097\\times 10^{6}\\) & \\(3.277\\times 10^{13}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Maximum cohort size, number of tokens processed, and total floating point operations (FLOPs) when training with FedAvg, for each model size. For simplicity, we only present FLOPs associated with the forward pass, using the approximation that a forward pass on a model of size \\(M\\) uses \\(M\\) FLOPs.\n' +
      '\n' +
      'JIT compilation alone is not enough.ML research often involves writing custom training loops. For FL research, this is often done by writing a double for-loop that iterates over clients in a cohort, and over the batches held by each client. The outer loop here has no data dependency, meaning that the values returned by iterations of this loop are not processed as inputs to the next iteration. One might therefore imagine that a sufficiently advanced compiler could detect this fact, and parallelize client training when possible (e.g. within resource constraints in the data center environment).\n' +
      '\n' +
      'This can be a difficult task for a compiler. To illustrate this difficulty, we implemented a double for loop in place of FAX-based training (looping over clients, and over each client\'s data). For both programs, we JIT-compiled the program, and provided identical input and output shardings to GSPMD and the XLA compiler stack. Though this stack is quite advanced and used to train many of the largest contemporary ML models, it does not recover the performance of FAX from this for-loop implementation. Indeed, round runtime scales linearly with cohort size, as expected, rather than remaining constant, indicating an inability to use the increased resource scale allocated to the experiment.\n' +
      '\n' +
      'GSPMD alone is not enough.A better way to parallelize federated computations than the for-loop approach above is to implement FAX\'s building blocks and use a compiler like GSPMD (Xu et al., 2021) to do automated sharding of the program. This leads to the question: Do we need FAX\'s internal sharding annotations to obtain weak scaling behavior, or can GSPMD alone fully and efficiently parallelize FAX computations? Given the relatively simple nature of FedAvg\'s parallel processing patterns (heavily parallelized model training with infrequent synchronization), one might expect that isolating federated building blocks as primitives with specially-designed sharding annotations is unnecessarily complex.\n' +
      '\n' +
      'To test this, we took a FAX-based implementation of FedAvg and removed all of FAX\'s internal sharding annotations at function-tracing time, denoting this FAX-NS (FAX with no sharding). We then re-ran the simulations in Fig. 4. The results in Fig. 6 show that at present, these explicit sharding annotations play a crucial role in ensuring FAX\'s weak-scaling behavior. FAX-NS computation times increased sublinearly but significantly faster than FAX computation times. Moreover, FAX-NS exhibited memory footprint scaling issues. We found that for sufficiently large model or cohort sizes, FAX-NS eventually ran out of high-bandwidth memory. In particular, this occurred for the 1B model with a cohort size of 512 and for the 8B model with all tested cohort sizes; that is, at 2 clients and beyond.\n' +
      '\n' +
      'Figure 6: Total training time for 100 rounds of FedAvg for the 1B model with various cohort sizes. We implement FedAvg using FAX with and without (FAX-NS) FAX’s sharding annotations. The red X represents the point at which the FAX-NS could not be sharded without triggering out of memory errors.\n' +
      '\n' +
      '## 5 Interpreting FAX to Production Platforms\n' +
      '\n' +
      'While data center performance is the primary goal of FAX, we wish to preserve the optionality to translate FAX computations into artifacts interpretable by production FL systems. For example, if federated_sum is only captured as a jnp.sum, then it may be difficult to tell whether this sum is intended to be _within_ a placement (e.g. taking a sum of values placed at the server) or between placements (e.g. taking a sum across clients and transferring the result). Below, we discuss how FAX\'s implementation enables computing program representations that can be automatically translated to production platforms.\n' +
      '\n' +
      'Preserving placement information.Recall from above that we implement federated building blocks as JAX primitives, and build FAX computations out of these primitives. This has one last key benefit, crucial to integrating with downstream production systems: the ability to preserve information about the federated placement of values, and how they are communicated across a system.\n' +
      '\n' +
      'JAX\'s Primitive mechanism allows users to inject new symbols into JAX itself, defining how these symbols behave under JAX\'s functional transformations like jax.vmap and jax.grad. These primitives are preserved in JAX\'s intermediate data structure, the jaxpr, which is usually lowered into XLA HLO. By using a custom interpreter, we can instead generate and consume jaxprs. This custom interpreter can use special behavior when it encounters the FAX-defined symbols injected via the Primitive mechanism. This preserves information about data placement and device communication patterns, allowing us to translate jaxprs into computations that can be run by a production FL platform.\n' +
      '\n' +
      'An example jaxpr is illustrative. In Snippet 3, we define a FAX program for computing a "federated loss", wherein the server broadcasts a model to all clients, each client computes the model\'s loss on its own data, and the server receives the average loss.\n' +
      '\n' +
      '```\n' +
      '@fax.fax_program(placements={"clients":1}) deffederated_loss(model, cohort): broadcast_model=fax.federated_broadcast(model)#Broadcastamodeltoclients client_losses=fax.federated_map_clients(#Clientslocallycomputelosses loss_fn,(broadcast_model, cohort)) returnfax.federated_mean(client_losses)#Sendtheaveragelosstotheserver\n' +
      '```\n' +
      '\n' +
      'Snippet 3: Computing a federated loss function using a FAX program.\n' +
      '\n' +
      'To obtain a jaxpr representing this processing pattern, we provide the concrete shape and type of arguments. In our example, the model is a float vector of shape [2], the cohort is a float vector of shape [1, 2], so there is only a single client. The loss_fn function is a simplified linear regression loss, used only for illustrative purposes, defined by\n' +
      '\n' +
      '\\[\\texttt{loss\\_fn}(x,y):=\\frac{1}{2}(\\langle x,y\\rangle-1)^{2}.\\]\n' +
      '\n' +
      'Given this information, JAX and FAX can generate a jaxpr representing Snippet 3. The result is in Snippet 4. The key takeaway is that this jaxpr preserves the FAX-defined primitives representing cross-machine communication, broadcast_clients and mean_from_clients, both of which are primitives registered by FAX in JAX. We can trace through the arguments in the jaxpr to see that the computation operates by (1) broadcasting a value to the clients, (2) calculating loss_fn using the broadcast values, (3) taking a mean over said clients. Since FL does not allow clients to communicate directly, the result of this mean operation is unambiguous: it must yield a server-placed value.\n' +
      '\n' +
      'Interpreting the jaxpr to a production FL system like TensorFlow Federated (Ingerman and Ostrowski, 2019) is now straightforward: all cross-machine communication is explicit, and the processing in-between communication is entirely local and can be extracted into standalone functions executed locally by devices in the system.\n' +
      '\n' +
      '```\n' +
      '{lambda;a:f32[2]b:f32[1,2].let c:f32[1,2]=broadcast_clientsa d:f32[1]=dot_general[ dimension_numbers=(([1], [1]), [[0], [0])) preferred_element_type=float32 ]c b e:f32[1]=subd1.0 f:f32[1]=integer_pow[y=2]e g:f32[1]=mul0.5f h:f32[1]=mean_from_clientsg in(h,)}\n' +
      '```\n' +
      '\n' +
      'Integrating federated AD.As discussed in Section 2, the Primitive mechanism allows FAX to to specify the behavior of federated building blocks under JAX\'s functional transformations, including computing forward- and reverse-mode Jacobians (jax.jacfwd and jax.jacrev). This allows FAX to fully implement the forward- and reverse-mode federated AD algorithms presented in (Rush et al., 2023).\n' +
      '\n' +
      'Rush et al. (2023) note that the federated building blocks in Section 2 form a closed set under forward- and reverse-mode automatic differentiation. For example, forward- and reverse-mode Jacobians of federated_broadcast can be computed via federated_broadcast and federated_sum, respectively. FAX can therefore implement federated AD without additional primitives. This means that the jaxpr of FAX computations that use federated AD will contain JAX\'s standard AD symbols, along with FAX\'s primitive set. This ensures that computations using federated AD are still interpretable to production FL systems.\n' +
      '\n' +
      'For example, Snippet 5 gives the jaxpr of jax.grad(federated_loss), the reverse-mode derivative of the federated loss computation above. Again, we see that information about communication in the system is preserved. The jaxpr contains the primitives broadcast_clients, mean_from_clients, and sum_from_clients, and which just as above, can be used by a custom interpreter to translate the jaxpr into a production system.\n' +
      '\n' +
      '```\n' +
      '{lambda;a:f32[2]b:f32[1,2].let c:f32[1,2]=broadcast_clientsa d:f32[1]=dot_general[ dimension_numbers=(([1], [1]), [[0], [0])) preferred_element_type=float32 ]c b e:f32[1]=subd1.0 f:f32[1]=integer_pow[y=2]e g:f32[1]=integer_pow[y=1]e h:f32[1]=mul12.0g i:f32[1]=mul0.5f _:f32[1]=mean_from_clientsi j:f32[1]=broadcast_clients1.0 k:f32[1]=div1.0 l:f32[1]=mul0.5k m:f32[1]=mul1h n:f32[1,2]=dot_general[ dimension_numbers=(([], [0], [0])) preferred_element_type=float32 ]m b o:f32[2]=sum_from_clientsn in(o,)}\n' +
      '```\n' +
      '\n' +
      'Snippet 5: jaxpr generated for grad_fn=jax.grad(federated_loss).\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      'Why federated AD?While features like scalability and efficiency are self-explanatory, the reader may be interested in _why_ we wish to implement federated AD, especially given the care required to interpret to production systems. In short, federated AD makes expressing efficient algorithms easier (Rush et al., 2023). By way of analogy, AD has made the development of sophisticated neural network architectures significantly easier. Libraries can define the conceptually simpler forward-pass, and rely on AD to perform backpropagation. The result is often faster and less error-prone than hand-implemented gradient computations (Baydin et al., 2018).\n' +
      '\n' +
      'FL algorithm development can see similar benefits. For example, Snippet 3 contains a FAX program used to compute the average loss over clients. By simply calling jax.grad(federated_loss), we immediately get a FAX program that computes the average gradient over clients. With this, we can take an optimization step (e.g. using a library such as Optax (DeepMind et al., 2020)) at the server, and immediately get the FedSGD algorithm from (McMahan et al., 2017). Snippet 6 depicts exactly that, defining the entire FedSGD algorithm by pairing jax.grad with a gradient update.\n' +
      '\n' +
      '```\n' +
      '@fax.fax_program(placements=("clients": 3)) deffed.sgd_step(model, cohort, opt_state): server_grad=jax.grad(federated_loss)(model, cohort)#Computeagradient updates,opt_state=optimizer.update(server_grad, opt_state) model=optax.apply_updates(model, updates)#Applythemodelupdate returnmodel,opt_state\n' +
      '```\n' +
      '\n' +
      'Snippet 6: Implementing FedSGD via federated AD.\n' +
      '\n' +
      'Federated AD enables algorithms like federated hypergradient descent, and algorithms that learn how to weight averages across clients (Rush et al., 2023). Federated AD also preserves compatibility with privacy-preserving mechanisms such as differential privacy and secure aggregation (Bonawitz et al., 2017). While not all FL algorithms can be expressed directly via federated AD (including FedAvg, see (Charles and Rush, 2022)), we believe an easy-to-use implementation of federated AD can accelerate algorithm development and research in FL.\n' +
      '\n' +
      'Beyond federation.Despite the repeated references to FL, FAX and federated AD can be used for more general parallel and distributed ML, in or outside of the data center. Just as federated AD aids FL algorithm development, we believe it can aid in distributed algorithm development writ large. For example, it can be used to differentiate through any algorithm involving communication between server and clients. This opens the door to things like self-tuning distributed algorithms, distributed learning across modular components, and model architectures whose forward passes involve explicit communication across locations.\n' +
      '\n' +
      'Conclusion.By pairing federated AD with an easy-to-use front-end via JAX, performant federated building block implementations, and useful sharding information, we hope to accelerate research on distributed and parallel ML. Future work includes (1) generalizations of federated AD to non-linear communication primitives, (2) extensions of FAX to more general data placements, including hierarchical data placements, and (3) mature FAX interpreters for specific production systems.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'The authors would like to thank Nicole Mitchell, Sean Augenstein, and Brendan McMahan for their valuable feedback on the manuscript. We would like to further thank Nicole and Sean for their further help in stress-testing and developing federated algorithms in FAX. We would also like to thank Yash Katariya, Hyeontaek Lim, Sharad Vikram, Roy Frostig, and Matthew Johnson for helpful discussions on JAX. Finally, we would like to thank Xiao Yu for help with interpreting FAX to production platforms.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Abadi et al. (2016) Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: A system for large-scale machine learning. In Kimberly Keeton and Timothy Roscoe, editors, _12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016_, pages 265-283. USENIX Association, 2016. URL [https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi](https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi).\n' +
      '* September 1, 2022_. mlsys.org, 2022. URL [https://proceedings.mlsys.org/paper/2022/hash/98dece83da5fb0395e163467c9dae521b-Abstract.html](https://proceedings.mlsys.org/paper/2022/hash/98dece83da5fb0395e163467c9dae521b-Abstract.html).\n' +
      '* Bauer (1974) Friedrich L Bauer. Computational graphs and rounding error. _SIAM Journal on Numerical Analysis_, 11(1):87-96, 1974.\n' +
      '* Baydin et al. (2018) Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. _Journal of Machine Learning Research_, 18(153):1-43, 2018. URL [http://jmlr.org/papers/v18/17-468.html](http://jmlr.org/papers/v18/17-468.html).\n' +
      '* Beutel et al. (2020) Daniel J. Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan Parcollet, and Nicholas D. Lane. Flower: A friendly federated learning research framework. _CoRR_, abs/2007.14390, 2020. URL [https://arxiv.org/abs/2007.14390](https://arxiv.org/abs/2007.14390).\n' +
      '* April 2, 2019_. mlsys.org, 2019. URL [https://proceedings.mlsys.org/book/271.pdf](https://proceedings.mlsys.org/book/271.pdf).\n' +
      '* Bonawitz et al. (2017) Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcodone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-preserving machine learning. In _proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security_, pages 1175-1191, 2017.\n' +
      '* Bradbury et al. (2018) James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL [http://github.com/google/jax](http://github.com/google/jax).\n' +
      '* Burlachenko et al. (2021) Konstantin Burlachenko, Samuel Horvath, and Peter Richtarik. FL_PyTorch: Optimization research simulator for federated learning. In _Proceedings of the 2nd ACM International Workshop on Distributed Machine Learning_, pages 1-7, 2021.\n' +
      '* Charles and Rush (2022) Zachary Charles and Keith Rush. Iterated vector fields and conservatism, with applications to federated learning. In Sanjoy Dasgupta and Nika Haghtalab, editors, _Proceedings of The 33rd International Conference on Algorithmic Learning Theory_, volume 167 of _Proceedings of Machine Learning Research_, pages 130-147. PMLR, 29 Mar-01 Apr 2022. URL [https://proceedings.mlr.press/v167/charles22a.html](https://proceedings.mlr.press/v167/charles22a.html).\n' +
      '* Charles et al. (2022) Zachary Charles, Kallista Bonawitz, Stanislav Chiknavaryan, Brendan McMahan, et al. Federated select: A primitive for communication-and memory-efficient federated learning. _arXiv preprint arXiv:2208.09432_, 2022.\n' +
      '* Chen et al. (2019)Zachary Charles, Nicole Mitchell, Krishna Pillutla, Michael Reneer, and Zachary Garrett. Towards federated foundation models: Scalable dataset pipelines for group-structured learning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, _Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023_, 2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/662bb54dc59eaac8e7c0d3fc6a0add-Abstract-Datasets_and_Benchmarks.html](http://papers.nips.cc/paper_files/paper/2023/hash/662bb54dc59eaac8e7c0d3fc6a0add-Abstract-Datasets_and_Benchmarks.html).\n' +
      '* Mind et al. (2020) DeepMind, Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Antoine Dedieu, Claudio Fantacci, Jonathan Godwin, Chris Jones, Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven Kapturovski, Thomas Keck, Iuri Kemaev, Michael King, Markus Kunesch, Lena Martens, Hamza Merzic, Vladimir Mikulik, Tamara Norman, George Papamakarios, John Quan, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Laurent Sartran, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan, Milos Stanojevic, Wojciech Stokowiec, Luyu Wang, Guangyao Zhou, and Fabio Viola. The DeepMind JAX Ecosystem, 2020. URL [http://github.com/google-deepmind](http://github.com/google-deepmind).\n' +
      '* Dimitriadis et al. (2022) Dimitrios Dimitriadis, Mirian Hipolito Garcia, Daniel Madrigal Diaz, Andre Manoel, and Robert Sim. FLUTE: A scalable, extensible framework for high-performance federated learning simulations. _CoRR_, abs/2203.13789, 2022. doi: 10.48550/ARXIV.2203.13789. URL [https://doi.org/10.48550/arXiv.2203.13789](https://doi.org/10.48550/arXiv.2203.13789).\n' +
      '* Douillard et al. (2023) Arthur Douillard, Qixuang Feng, Andrei A. Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc\'Aurelio Ranzato, Arthur Szlam, and Jiajun Shen. Diloco: Distributed low-communication training of language models. _CoRR_, abs/2311.08105, 2023. doi: 10.48550/ARXIV.2311.08105. URL [https://doi.org/10.48550/arXiv.2311.08105](https://doi.org/10.48550/arXiv.2311.08105).\n' +
      '* Finn et al. (2017) Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pages 1126-1135. PMLR, 2017.\n' +
      '* He et al. (2020) Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr. Fedml: A research library and benchmark for federated machine learning, 07 2020.\n' +
      '* Ingerman and Ostrowski (2019) Alex Ingerman and Krzysztof Ostrowski. Introducing Tensorflow Federated, Mar 2019. URL [https://blog.tensorflow.org/2019/03/introducing-tensorflow-federated.html](https://blog.tensorflow.org/2019/03/introducing-tensorflow-federated.html).\n' +
      '* Jiang et al. (2019) Yihan Jiang, Jakub Konecny, Keith Rush, and Sreeram Kannan. Improving federated learning personalization via model agnostic meta learning. _CoRR_, abs/1909.12488, 2019. URL [http://arxiv.org/abs/1909.12488](http://arxiv.org/abs/1909.12488).\n' +
      '* Jolicoeur-Martineau et al. (2023) Alexia Jolicoeur-Martineau, Emy Gervais, Kilian Fatras, Yan Zhang, and Simon Lacoste-Julien. PopulAtion Parameter Averaging (PAPA). _arXiv preprint arXiv:2304.03094_, 2023.\n' +
      '* Kairouz et al. (2021) Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and trends(r) in machine learning_, 14(1-2):1-210, 2021.\n' +
      '* Lai et al. (2022) Fan Lai, Yinwei Dai, Sanjay S. Singapuram, Jiachen Liu, Xiangfeng Zhu, Harsha V. Madhyastha, and Mosharaf Chowdhury. FedScale: Benchmarking model and system performance of federated learning at scale. In _International Conference on Machine Learning (ICML)_, 2022.\n' +
      '* Lepikhin et al. (2020) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. _CoRR_, abs/2006.16668, 2020. URL [https://arxiv.org/abs/2006.16668](https://arxiv.org/abs/2006.16668).\n' +
      '* Liu et al. (2019)Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and Luke Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models. _arXiv preprint arXiv:2208.03306_, 2022.\n' +
      '* Liu et al. (2021) Yang Liu, Tao Fan, Tianjian Chen, Qian Xu, and Qiang Yang. FATE: an industrial grade platform for collaborative learning with data protection. _Journal of Machine Learning Research_, 22(226):1-6, 2021. URL [http://jmlr.org/papers/v22/20-815.html](http://jmlr.org/papers/v22/20-815.html).\n' +
      '* McMahan et al. (2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguiera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Aarti Singh and Xiaojin (Jerry) Zhu, editors, _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA_, volume 54 of _Proceedings of Machine Learning Research_, pages 1273-1282. PMLR, 2017. URL [http://proceedings.mlr.press/v54/ncmahan17a.html](http://proceedings.mlr.press/v54/ncmahan17a.html).\n' +
      '* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL [https://openreview.net/forum?id=BJ0hF120b](https://openreview.net/forum?id=BJ0hF120b).\n' +
      '* 16, 2023_, 2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/42c40aff7814e9796266e12053b1c610-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/42c40aff7814e9796266e12053b1c610-Abstract-Conference.html).\n' +
      '* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 8024-8035, 2019. URL [https://proceedings.neurips.cc/paper/2019/hash/bdbca288fe7f92f2bfa9f7012727740-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/bdbca288fe7f92f2bfa9f7012727740-Abstract.html).\n' +
      '* Paulik et al. (2021) Matthias Paulik, Matt Seigel, Henry Mason, Dominic Telaar, Joris Kluivers, Rogier C. van Dalen, Chi Wai Lau, Luke Carlson, Filip Granqvist, Chris Vandevelde, Sudeep Agarwal, Julien Freudiger, Andrew Byde, Abhishek Bhowmick, Gaurav Kapoor, Si Beaumont, Aine Cahill, Dominic Hughes, Omid Javidbakht, Fei Dong, Rehan Rishi, and Stanley Hung. Federated evaluation and tuning for on-device personalization: System design & applications. _CoRR_, abs/2102.08503, 2021. URL [https://arxiv.org/abs/2102.08503](https://arxiv.org/abs/2102.08503).\n' +
      '* Reddi et al. (2020) Sashank J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In _International Conference on Learning Representations_, 2020.\n' +
      '* Ro et al. (2021) Jae Hun Ro, Ananda Theertha Suresh, and Ke Wu. FedJAX: Federated learning simulation with JAX. _arXiv preprint arXiv:2108.02117_, 2021.\n' +
      '* Rush et al. (2023) Keith Rush, Zachary Charles, and Zachary Garrett. Federated automatic differentiation. _arXiv preprint arXiv:2301.07806_, 2023.\n' +
      '* Xu et al. (2021) Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake A. Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen. GSPMD: general and scalable parallelization for ML computation graphs. _CoRR_, abs/2105.04663, 2021. URL [https://arxiv.org/abs/2105.04663](https://arxiv.org/abs/2105.04663).\n' +
      '* Xu et al. (2021)Alexander Ziller, Andrew Trask, Antonio Lopardo, Benjamin Szymkow, Bobby Wagner, Emma Bluemke, Jean-Mickael Nounahon, Jonathan Passerat-Palmbach, Kritika Prakash, Nick Rose, et al. Pysyft: A library for easy federated learning. _Federated learning systems: Towards next-generation AI_, pages 111-139, 2021.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>