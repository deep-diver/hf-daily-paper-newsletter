<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# ShortGPT: 대규모 언어 모델의 레이어가 예상보다 더 중복됨\n' +
      '\n' +
      ' 신남성\\({}^{1}\\), 명규수\\({}^{1}\\), 칭유장\\({}^{2}\\), 빙닝왕\\({}^{1}\\)\n' +
      '\n' +
      'HHongyu Lin\\({}^{2}\\), Yaojie Lu\\({}^{2}\\), Xianpei Han\\({}^{2}\\), Weipeng Chen\\({}^{1}\\)**\n' +
      '\n' +
      '(주)바이촨\n' +
      '\n' +
      '({}^{2}\\) 중국 정보처리연구소\n' +
      '\n' +
      '중국과학원 소프트웨어연구소\n' +
      '\n' +
      'daniel@baichuan-inc.com\n' +
      '\n' +
      '{zhangqingyu2024,hongyu,yaojie,xianpei}@iscas.ac.cn\n' +
      '\n' +
      'Corresponding author\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)의 성능이 계속 향상됨에 따라, 현재 LLM은 수십억 또는 심지어 수조 개의 매개 변수를 포함하는 등 그 크기가 크게 증가했다. 그러나 본 연구에서는 LLM의 많은 계층이 높은 유사성을 나타내고 일부 계층은 네트워크 기능에서 무시할 수 있는 역할을 한다는 것을 발견했다. 이러한 관찰을 바탕으로 LLM에서 각 계층의 유의성을 측정하기 위해 블록 영향(Block Influence, BI)이라는 메트릭을 정의한다. 그런 다음 BI 점수를 기반으로 LLM의 중복 레이어를 직접 삭제하는 레이어 제거라는 간단한 가지치기 접근법을 제안한다. 실험을 통해 ShortGPT라고 불리는 본 논문에서 제안한 방법이 기존의 SOTA(state-of-the-art) 방법보다 모델 가지치기에서 우수한 성능을 보임을 확인하였다. 더욱이, ShortGPT는 양자화-유사 방법들과 직교하여, 파라미터들 및 계산들의 추가적인 감소를 가능하게 한다. 보다 복잡한 가지치기 기술과 달리 간단한 레이어 제거를 통해 더 나은 결과를 얻을 수 있는 능력은 모델 아키텍처에서 고수준의 중복성을 시사한다.\n' +
      '\n' +
      '그림 1: 25% 매개변수를 다른 방법으로 가지치기하여 MMLU에 대한 결과.\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      '대규모 언어 모델(LLM) 분야는 최근 LLM이 다양한 영역에서 인상적인 성능을 달성하는 등 급속한 발전을 목격했다. 선행 연구인 Kaplan et al. (2020); Hoffmann et al. (2022)에서 확인된 스케일링 법칙에 의해, 현재 LLM 연구는 성능을 향상시키기 위해 모델 파라미터를 일관되게 증가시키는 경향이 있다. 그러나 수십억 또는 심지어 수조 개의 매개변수를 포함하는 결과적인 대규모 모델은 엄격한 하드웨어 요구 사항을 제기하여 실제 배포 및 사용을 방해한다.\n' +
      '\n' +
      '대규모 훈련된 모델을 배포하기 위한 까다로운 하드웨어 요구 사항을 완화하기 위해, 많은 연구자들이 추론 비용을 감소시키는 모델 압축 기술 Zhu et al.(2023)에 초점을 맞추고 있다. 모델 압축 방법은 크게 양자화 Liu et al. (2021); Gholami et al. (2022); Dettmers et al. (2022, 2024) 및 pruning LeCun et al. (1989); Han et al. (2015). 양자화 방법은 가중치 및 활성화를 정량화하여 정밀도를 낮춘다. 그러나, 양자화의 가속 이점은 하드웨어 지원에 의존하며, 때때로 양호한 성능을 유지하기 위해 추가적인 미세 조정을 필요로 한다. 대조적으로, 가지치기 방법은 전체 매개변수 수를 줄이기 위해 중복 모델 매개변수를 제거한다. 프루닝은 재훈련 없이 훈련된 모델에 직접 적용될 수 있으며, 일반적으로 양자화 접근법보다 하드웨어에 더 친화적이다.\n' +
      '\n' +
      '최근 모델 프루닝에 기반한 대규모 언어 모델 압축 방법에 상당한 진전이 있었지만, 기존의 방법은 비교적 복잡하도록 설계되는 경우가 많다. 일부는 그라디언트 정보 Zhang et al.(2023)의 사용을 요구하거나 폭 Ashkbooks et al.(2024)에서의 압축에만 집중하여, 실제 응용을 위해 지나치게 정교하게 렌더링한다. 따라서, 대형 언어 모델에 대해 특별히 맞춤화된 간단하고 효율적인 모델 가지치기 접근법을 탐구할 필요가 있다.\n' +
      '\n' +
      '효과적인 모델 가지치기 기술을 식별하려면 모델 중복 Huang et al.(2021); Dalvi et al.(2020)을 연구해야 한다. 모델 중복성에 대한 이전의 연구는 일반적으로 합성곱 신경망(CNN) 또는 작은 크기의 트랜스포머와 같은 비교적 작은 모델에 초점을 맞추었다. LLM에서 모델 프루닝의 경우, 대부분의 이전 작업은 각 매개변수 텐서 내의 리던던시를 조사하는 텐서별 리던던시 분석에 집중되었다. 그러나, 본 논문의 핵심 발견은 LLM들이 계층 레벨에서 상당한 리던던시를 나타내어, 다운스트림 태스크 성능에 실질적으로 영향을 미치지 않으면서 전체 계층들을 간단하게 제거할 수 있다는 것이다. 예를 들어, LLaMA 2-13B 모델에서 마지막 10개 레이어(전체 40개 레이어의 25%)를 삭제할 때, MMLU 벤치마크 Hendrycks et al.(2020)에 대한 결과는 55.0에서 52.22(95% retention)로 떨어질 뿐이다. 또한, 마지막 22개 층(전체 40개 층의 55%)을 제거하여 56억 개의 매개변수 모델을 생성함으로써, 우리는 여전히 LLaMA 2-7B 모델보다 우수한 미세 조정 없이 MMLU에서 47.19의 점수를 얻을 수 있다.\n' +
      '\n' +
      '본 논문에서는 LLM의 모델링 과정에서 은닉 상태 변환을 측정하는 블록 영향(Block Influence, BI)의 렌즈를 통해 계층별 중복성을 분석하는 것을 제안한다. 우리는 BI가 LLM에서 계층의 중요성을 나타내는 더 적절한 지표임을 발견했으며, BI로 중복 계층을 삭제하기만 하면 모델 가지치기를 수행할 수 있다. 특정 층을 제거하는 이 간단한 동작은 이전의 보다 복잡한 가지치기 방법을 훨씬 능가한다. 우리의 연구 결과는 현재 LLM 아키텍처에서 상당한 중복성을 강조하고 향후 더 효율적인 LLM 교육을 위한 기회를 조명한다.\n' +
      '\n' +
      '본 논문의 주요 기여는 다음과 같이 요약된다.\n' +
      '\n' +
      '*큰 언어 모델(LLM)에서 중복성을 분석하고 계층 수준에서 상당한 중복성을 나타냄을 발견한다. 이 통찰력은 단순히 중복 레이어를 제거하여 LLM을 제거하도록 영감을 줍니다.\n' +
      '* 계층 중요도의 효과적인 지표로서 블록 영향력(Block Influence, BI)이라는 메트릭을 제안한다. 정량적 분석을 통해 LLM이 깊이(층)와 너비(층 내 매개변수) 모두에서 중복성을 가지고 있음을 보여준다.\n' +
      '* BI 메트릭을 기반으로 BI 점수가 낮은 레이어를 제거하여 간단하면서도 효과적인 가지치기 전략을 제안한다. 실험 결과, 본 논문에서 제안한 방법은 92%의 성능을 유지하면서 기존의 최신 기법보다 약 25%의 파라미터와 연산량을 줄일 수 있었다.\n' +
      '\n' +
      '* 더 나아가, 우리는 우리의 레이어 프루닝 접근법이 양자화 방법과 직교한다는 것을 입증하며, 이는 LLM의 배치 오버헤드를 더욱 감소시키기 위해 양자화 기술과 조합될 수 있다는 것을 의미한다.\n' +
      '\n' +
      '## 2 Methodology\n' +
      '\n' +
      '이 섹션에서는 LLM에 대한 레이어 삭제 접근법의 방법론적 프레임워크를 제시하여 사용된 기본 원칙과 기술을 설명한다. 먼저 LLaMA 2와 Baichuan 2와 같은 기존의 LLM에 존재하는 계층 중복 문제를 정량화하고, LLM 추론 시 각 계층별 은닉 상태의 변환을 평가하기 위해 블록 영향(Block Influence, BI)이라는 메트릭을 소개한다. BI를 기반으로 LLM에 레이어 삭제를 적용하여 예측 정확도나 언어 능력을 손상시키지 않고 추론 비용을 줄인다.\n' +
      '\n' +
      '### Layer redundancy\n' +
      '\n' +
      '전류에서 우세한 LLMs는 주로 Transformer Vaswani et al.(2017)에 기초한다. 트랜스포머 아키텍처는 일반적으로 서로 적층된 여러 개의 잔류 층으로 구성된 주의 메커니즘을 기반으로 한다. 트랜스포머는 시퀀스 대 시퀀스 매핑이며, 이는 가능한\n' +
      '\n' +
      '그림 2: 우리의 방법: 먼저, 우리는 계층들을 그들의 BI 점수(Sec. 2.2에 도입됨)로 정렬한 다음, 계층들을 오름차순으로 삭제한다. 모든 결과는 LLama2-7B-Base를 기반으로 한다.\n' +
      '\n' +
      '(y=f(X,\\theta)\\)로 정의되며, 여기서 \\(X\\in R^{T\\times n}\\), \\(Y\\in R^{T\\times n}\\), \\(T\\)은 수열의 길이, \\(n\\)은 어휘 크기, \\(\\theta\\)은 학습 가능한 매개변수이다. L층 변압기의 형식적 표현은 다음과 같다:\n' +
      '\n' +
      '\\mathbf{A}_{i}}\\mathbf{W}_{\\text{emb}}\\] \\text{ATTN}(\\text{LN}(\\mathbf{X}_{i}}))+\\mathbf{X}_{i}\\\\mathbf{X}_{i+1}=\\text{FFN}(\\text{LN}(\\mathbf{A}_{i+1}}\\mathbf{Y}=\\mathbf{X}_{L}\\mathbf{W}_{\\text{head}}\\mathbf{A}_{i+1}}}\\tag{1}\\mathbf{A}_{i+1}}\\mathbf{A}_{L}\\mathbf{W}_{\\text{head}}\\mathbf{A}_{i+1}}\\mathbf{A}_{i+1}}\\mathbf{A}_{L}\\mathbf{W}_{\\text{1}\\\n' +
      '\n' +
      '여기서, \\(\\mathbf{W}_{\\text{emb}}\\in R^{n\\times d}\\)은 단어 임베딩 행렬이고, \\(\\mathbf{W}_{\\text{head}}\\in R^{d\\times n}\\)은 변압기의 출력 투영 행렬이며, 이는 때때로 \\(\\mathbf{W}_{\\text{emb}\\) 차우더리 등(2023), \\(d\\)은 변압기의 숨겨진 딤이다. **ATTN**은 주의층을 의미하고 **FFN**은 피드-포워드층을 의미하며, \\(\\mathbf{X}_{i}\\in R^{T\\times d}\\)는 \\(i^{th}\\) 층의 숨겨진 상태를 의미한다. 변압기 구조가 여러 개의 동일한 층으로 구성되어 있다는 점을 감안할 때, 이러한 동일한 층의 기능과 역할 간의 차이와 연결은 무엇인지에 대한 자연스러운 질문이 있다. 기존 연구에서는 트랜스포머가 이전 계층인 Hasan 등(2021)에서 특정 의미역량을 가지고 있음을 발견하였다. 본 연구에서는 변압기의 층 사이에 상당한 수준의 중복성을 발견한다.\n' +
      '\n' +
      '**Layer Redundancy**: 중복성이 높은 네트워크는 일부 중복 레이어를 포함해야 하며, 이는 네트워크의 최종 성능에 최소한의 영향을 미친다. 이는 네트워크 내의 다른 층들과 비교하여 균질화된 기능들을 갖는 이들 층들에 기인할 수 있다. 우리는 특정 계층을 생략함으로써 현재 LLM에서 높은 수준의 중복성을 발견했다.\n' +
      '\n' +
      '우리는 많은 경우에 네트워크의 \\(Y\\) 예측은 크게 변하지 않는다는 것을 관찰했다. 이 현상은 변압기의 층에 걸쳐 상당한 수준의 중복성을 보여준다. 도 3은 Llama2-7B-Base Touvron et al. (2023), English based LLMs, Baichuan2-7B-Base Yang et al. (2023)에서 특정 레이어를 생략한 Perplexity와 MMLU Hendrycks et al. (2020) 점수를 나타낸 것으로, 주로 중국어에 집중되어 있다. 더 많은 벤치마크 결과를 보려면 섹션 3을 참조하세요. 그림 3은 일부 레이어가 LLM에서 중요한 역할을 하지 않는다는 것을 보여줍니다. LLM에서 특정 층을 떨어뜨리는 것은 최종 결과에 최소한의 영향을 미칠 수 있으며, 당혹감에 대해서도 마찬가지이다. 더욱이, 이러한 리던던시는 주로 네트워크의 중간에서 나중의 계층들에서 나타나며, 초기 계층들 및 최종 계층은 종종 더 중요하다.\n' +
      '\n' +
      '### Layer importance\n' +
      '\n' +
      '이전 섹션에 요약된 바와 같이, LLM들의 층들은 상이한 층들에 걸쳐 다양한 정도의 리던던시를 갖는 리던던시를 나타낸다. 특히, 더 깊은 층들은 더 높은 레벨의 리던던시를 입증한다. 이러한 중복 레이어를 제거하기 위해서는 레이어의 중요도를 측정하기 위한 모델 고유 메트릭이 필요하다. 직관적인 방법은 더 큰 크기가 더 활성화된 뉴런을 의미하기 때문에 각 층의 출력 크기를 중요도의 척도로 사용하는 것이다. Samragh et al.(2023)에서, 그들은 층들의 중요도를 측정하기 위해 상대적인 크기 \\(||\\frac{f(x)}{x+f(x)}||\\)를 사용할 것을 제안했다. 특정 층의 영향을 특성화하기 위해. 변압기 블록이 은닉 상태를 더 많이 변화시킬수록 이에 더 큰 영향을 미친다는 가정하에 새로운 메트릭인 Block Influence(BI)를 정의하였다.\n' +
      '\n' +
      '도 3: 단일층을 제거하는 성능.\n' +
      '\n' +
      'layer는. 도 4에 도시된 바와 같이, \\(i^{th}\\) 블록들의 BI 점수는 다음과 같이 계산될 수 있다:\n' +
      '\n' +
      'b{E}_{X,t}\\frac{X_{i,t}^{T}X_{i+1,t}}{||X_{i,t}||_{2}||X_{ i+1,t}||_{2}}, \\tag{2}\\\n' +
      '\n' +
      '여기서 \\(X_{i,t}\\)는 \\(X_{i}\\)의 \\(t^{th}\\) 열을 의미한다. 우리의 경험적 증거는 BI가 층의 중요성을 효과적으로 반영한다는 것을 뒷받침한다. 그림 8은 이러한 서로 다른 메트릭을 자세히 보여준다.\n' +
      '\n' +
      '### Layer Removal\n' +
      '\n' +
      '우리가 제안하는 방법은 간단히 ShortGPT라고 하는 레이어 제거이다. BI 점수를 기반으로 LLM의 특정 레이어를 삭제합니다. 그림 2는 우리의 가지치기 방법을 보여준다. 먼저, PG19 Rae et al.(2019)과 같은 라벨링되지 않은 텍스트 샘플들의 집합인 캘리브레이션 세트를 구성한다. 그런 다음 이러한 샘플에 대한 추론 동안 각 레이어의 숨겨진 상태를 수집한다. 다음으로 수집된 은닉 상태를 기반으로 BI 점수를 계산한다. 마지막으로, BI에 따라 계층들을 오름차순으로 정렬하고, 중요도가 작은 계층들을 삭제한다. 삭제할 레이어의 수는 속도와 성능을 상쇄하기 위해 달라질 수 있습니다. 계층 제거 설정에 대한 자세한 내용은 부록 B에서 확인할 수 있습니다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '#### 3.1.1 Models\n' +
      '\n' +
      '이 방법의 유효성을 검증하기 위해 Llama2-7B Touvron et al.(2023), Llama2-13B, Baichuan2-7B 및 Baichuan2-13B를 포함한 기존의 인기 있는 오픈 소스 언어 모델에 대한 실험을 수행했다. 모두 디코더 전용 트랜스포머 아키텍처를 기반으로 한 대형 언어 모델입니다. LLaMA 2는 2조 개 이상의 토큰에 대해 교육을 받았다. 베이촨 시리즈는 주로 중국어로 훈련되었으며 130억 모델은 RoPE Su et al.(2024) 위치 임베딩을 ALBi Press et al.(2021)로 대체했다.\n' +
      '\n' +
      '#### 3.1.2 Benchmarks\n' +
      '\n' +
      '프루닝 전후의 대형 언어 모델의 능력 변화를 종합적으로 평가하기 위해 대형 모델을 평가하기 위해 가장 일반적으로 사용되는 Benchmark MMLU Hendrycks et al.(2020), CMMLU Li et al.(2024)에 대한 평가를 수행하였다. 또한, 보다 넓은 데이터셋을 평가하기 위해 LaCo Yang et al.(2024)을 따랐다.\n' +
      '\n' +
      '그림 4: 블록 영향력(BI) 메트릭의 그림입니다. 시각적 간결성에 대한 레이어 규범을 생략합니다.\n' +
      '\n' +
      '**MMLU**Hendrycks et al. (2020)은 제로-샷 및 소수-샷 설정에서 모델을 구체적으로 평가함으로써 사전 훈련 동안 획득된 지식을 측정하는 것을 목표로 하는 벤치마크이다. 이는 벤치마크를 우리가 인간을 평가하는 방식과 더 어렵고 유사하게 만든다. 이 벤치마크는 STEM, 인문, 사회과학 등 57개 과목을 대상으로 한다. 어려움은 초보자부터 고급 전문가 수준까지 다양하며 세계 지식과 문제 해결 능력을 테스트합니다.\n' +
      '\n' +
      '**CMMLU**Li 등(2024)은 LLM의 고급 지식 및 추론 능력을 중국어 및 문화의 맥락에서 평가하기 위해 특별히 설계된 포괄적인 중국어 평가 데이터세트이다. CMMLU는 초등학교에서 대학교 또는 전문직에 이르기까지 67개의 주제를 다루고 있다. 자연과학뿐만 아니라 인문사회과학까지 포함하여 중국적 특성을 가진 내용도 많이 포함하고 있다.\n' +
      '\n' +
      '**CMNLI**Xu et al.(2020)은 중국어 이해도 평가 벤치마크의 일부이다. XNLI와 MNLI의 두 부분으로 구성되어 있다. **HellaSwag (HeSw)**Zellers et al. (2019)는 인간에게는 사소한 질문이지만 최첨단 모델에 특히 어려운 상식 NLI를 평가하기 위한 도전적인 데이터 세트이다. **PIQA**Bisk 등(2020)은 일별 시나리오에 초점을 맞춘 다선택 질의응답 데이터셋이다. 이 데이터 세트는 일상적인 시나리오를 통해 실제 물리적 세계의 법칙에 대한 모델의 파악을 탐구한다. **CHID**Zheng 등(2019)은 주로 후보 단어의 선택과 숙어의 표현에 초점을 맞춘 숙어 클로즈 테스트 데이터셋이다. **CoQA**Reddy et al.(2019)은 127000개 이상의 질문 및 그에 대응하는 답변을 포함하는 대화식 질문-응답 태스크에 사용되는 대규모 데이터세트이다. **BoolQ**Clark 등(2019)은 예/아니오 질문의 15942개의 예를 포함하는 질문-응답 데이터세트이다. 이러한 문제들은 자연적으로 발생한다 - 그것들은 조용하고 구속받지 않는 환경에서 발생한다. **Race**Lai et al.(2017)은 중국 영어시험에서 수집된 대규모 독해 데이터셋으로 중학생과 고등학생을 대상으로 설계되었다. **XSumHasan et al.(2021)은 추상적인 단일 문서 요약 시스템을 평가하는 데 사용된다. 목표는 그 기사가 무엇에 관한 것인지에 대한 짧고 한 문장짜리 새로운 요약을 만드는 것이다. **C3**Sun et al.(2020)은 객관식 문항, 중국어 능력 시험의 읽기 자료, 중국어 민족 시험으로 구성된 객관식 기계독해 데이터셋이다. **PG19**Rae et al. (2019)는 언어 모델링의 효과를 테스트하기 위해 사용되는 책으로부터 긴 문서 데이터세트이다.\n' +
      '\n' +
      '#### 3.1.3 Baselines\n' +
      '\n' +
      '본 논문에서 제안한 방법의 효율성을 평가하기 위해 LaCo Yang et al. (2024)에 이어 대규모 언어 모델에 대한 몇 가지 구조화된 프루닝 방법을 비교하였다. 제안하는 방법은 계층 중요도와 복잡도 계산을 위해 PG19를 사용한다.\n' +
      '\n' +
      '**LLMPru.**Ma et al. (2024)는 기울기 정보에 기초하여 비-임계 결합 구조들을 선택적으로 제거하는 구조적 프루닝을 채택하여, LLM의 기능성의 대부분을 최대로 보존한다.\n' +
      '\n' +
      '**SliceGPT**Ashkboos et al. (2024)는 각 가중치 행렬을 더 작은 행렬로 대체하여 네트워크의 임베딩 차원을 줄이는 훈련 후 희박화 기법이다.\n' +
      '\n' +
      '**LaCo**Yang et al.(2024)은 다중 레이어 병합에 기반한 대용량 언어 모델에 대한 프루닝 방법이다.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '제안된 방법의 유효성을 검증하기 위해, 우리는 대규모 언어 모델 평가에 일반적으로 사용되는 벤치마크 및 기준선 기술에 대한 비교 실험을 수행했다. 현재 구조화된 가지치기 방법을 고려할 때 일반적으로 매개변수를 30% 이하로 줄이며, 약 1/4의 매개변수로 실험을 수행했다. 실험 결과는 표 1에 나와 있으며, 다른 매개변수 감소 비율을 탐색하는 추가 실험은 후속 섹션에서 논의될 것이다.\n' +
      '\n' +
      '그 결과, 본 논문에서 제안한 방법에 의해 프루닝된 모델의 성능은 추론, 언어 이해, 지식 유지 및 검사 수행과 같은 대부분의 대형 언어 모델의 능력을 가능한 한 최대로 유지하면서 기준 방법의 성능을 크게 능가하는 것으로 나타났다. 또한, 레이어 수를 줄이는 접근법(ShortGPT/LaCo)이 임베딩 차원을 줄이는 방법(LLM)보다 우수하다는 점에 주목한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '본 논문에서 제안한 중요도를 기반으로 레이어를 삭제하여 1~32개의 레이어로 모델을 생성하였다. 그림 5의 결과는 가지치기 비율이 증가함에 따라 모델의 성능이 감소함을 나타낸다. 그러나, MMLU 점수는 특정 계층에서 현저하게 떨어지며, 이는 매우 중요한 역할을 하는 네트워크 내의 특정 특수 계층의 존재를 암시할 수 있다.\n' +
      '\n' +
      '#Depth Redundancy vs width Redundancy\n' +
      '\n' +
      '앞의 섹션에서는 깊이(계층) 측면에서 대규모 언어 모델(LLM)의 중복성을 분석하고 탐구했다. 그러나, 우리는 또한 특히 주의 헤드에 초점을 맞춘 폭 측면에서 중복성을 조사했다. 이 방법은 다중 헤드 어텐션 메커니즘에서 각 헤드를 순차적으로 제거하고 어텐션 출력 투영의 매개변수를 조정하여 어텐션 블록의 출력 모양이 변경되지 않도록 하는 것을 포함한다. FFN(피드 포워드 네트워크)은 변경되지 않은 상태로 유지됩니다. 섹션 2.1의 접근법과 유사하게 각 헤드의 제거가 네트워크의 최종 성능에 미치는 영향을 관찰한다. 그림 6은 각 헤드가 제거된 후 Perplexity와 MMLU 점수의 변화를 보여준다. 결과는 LLM이 깊이에서의 중복성에 필적하는 폭에서 높은 정도의 중복성을 나타냄을 나타낸다. 그러나 이러한 폭 중복성은 식별 가능한 패턴을 따르지 않으며 다른 모델에 따라 다르다. 우리는 이 변화가 머리 사이의 대칭에 기인할 수 있다고 가정한다.\n' +
      '\n' +
      '비변속기 LLM에 대한 중복성\n' +
      '\n' +
      '본 논문에서는 트랜스포머의 구조에 따른 중복성 여부를 심층적으로 분석한다. 인기 LLM의 대다수가 트랜스포머 구조를 기반으로 한다는 점을 감안할 때. 우리는 Peng et al. (2023)에서 제안한 RWKV-7B 모델을 트랜스포머 아키텍처에서 실제로 어느 정도 동일한 레이어로 구성된 강력한 경쟁자로 선정했다. 2.1의 방법론을 사용하여 이 모델을 분석했다. 그림 7은 RWKV-7B 2 모델의 중복성을 보여준다. 이 그림을 통해 RWKV-7B 모델도 높은 수준의 중복성을 보이는 것을 관찰할 수 있다. 이것은 중복성이 LLM에 걸쳐 보편적이라는 것을 시사할 수 있다.\n' +
      '\n' +
      '각주 2: [https://huggingface.co/BlinkDL/rwkv-5-world](https://huggingface.co/BlinkDL/rwkv-5-world]에서 rwkv-v5-world-7B를 사용한다.\n' +
      '\n' +
      '### Importance metric\n' +
      '\n' +
      '우리의 방법의 가장 중요한 개념은 계층들을 중요도별로 순위를 매기고 덜 중요한 계층들을 제거하는 것이다. 중요도 메트릭의 정의는 결과에 지대한 영향을 미친다. 이 섹션에서는 다양한 중요한 메트릭을 정의하고 비교한다:\n' +
      '\n' +
      '그림 6: 너비의 중복, 모델에서 단일 헤드를 제거합니다. x축은 헤드 id입니다.\n' +
      '\n' +
      '도면층 변환 함수.\n' +
      '\n' +
      '**BI**: 이전 섹션 4.4에서 언급한 BI.\n' +
      '\n' +
      '그림 8은 다양한 메트릭을 보여줍니다. 우리는 LLM 네트워크에서 더 얕은 층이 더 깊은 층보다 더 중요하다는 것을 관찰한다. 그림 5는 다른 메트릭으로 레이어를 제거한 결과를 보여주며, 제안한 BI가 다른 메트릭보다 우수함을 보여준다. Samragh et al.(2023)의 방법은 경쟁이 심하여 상대값도 어느 정도 중요도를 반영할 수 있음을 나타낸다. MMLU 벤치마크만 고려할 때 숨겨진 상태 규범은 좋은 척도로 보이지만, 텍스트 생성과 관련된 작업에 상당한 영향을 미치는 당혹감이 상대적으로 열악하다는 점에 주목할 필요가 있다. 이는 또한 현재 LLMs 평가 방법의 한계를 나타낸다.\n' +
      '\n' +
      '### 양자화에 대한 직교\n' +
      '\n' +
      '이 섹션에서, 우리는 우리의 방법이 양자화 방법과 직교한다는 것을 보여준다. 우리는 GPTQ 알고리즘에 의해 양자화된 모델 3에 우리의 방법을 적용한다. 표 2는 우리의 방법이 양자화 유사 방법과 호환되어 풋프린트를 더 줄일 수 있음을 보여준다.\n' +
      '\n' +
      '도 8: 상이한 중요도 메트릭들.\n' +
      '\n' +
      '도 7: RWKV-7B 상의 단일 층들을 제거하는, RWKV 상의 리던던시.\n' +
      '\n' +
      '## 5 Limitation\n' +
      '\n' +
      '비록 우리의 방법이 현재의 가지치기 방법에 비해 강력한 경쟁력을 보여주지만, 일부 벤치마크는 심지어 특정 레이어를 삭제하는 것이 최종 결과에 영향을 미치지 않는다는 것을 암시하지만, 이것은 레이어 제거가 단점 없이 온다는 것을 의미하지는 않는다. 벤치마킹의 한계로 인해 현재 평가는 층 제거가 모델 성능에 미치는 영향을 완전히 포착하지 못할 수 있다. 우리의 실험은 층 제거의 효과가 객관식 작업에 비해 생성 작업에 훨씬 더 두드러진다는 것을 보여준다. GSM8K Cobbe 등(2021) 및 HumanEval Chen 등(2021)과 같은 벤치마크에서, 층의 25%를 제거하는 것은 종종 심각한 성능 저하로 이어지고, 점수는 0에 근접한다. 그럼에도 불구하고 본 연구는 모델들이 레이어 제거 후에도 강한 의미적 이해와 처리 능력을 유지하고 있음을 증명한다. 훈련 후 기술은 진행 중인 연구의 영역인 성능 손실을 잠재적으로 더 완화할 수 있다.\n' +
      '\n' +
      '##6 관련 사항\n' +
      '\n' +
      '대형 언어 모델의 추론 비용을 줄이고 실제 응용도를 높이기 위해 최근 압축 모델에 대한 연구가 많이 진행되고 있는데, 이는 모델 프루닝, 지식 증류, 양자화, 저순위 인수분해의 네 가지 범주로 분류할 수 있다.\n' +
      '\n' +
      '**모델 프루닝:**모델 프루닝 LeCun 등(1989); Han 등(2015)은 모델을 압축하기 위해 모델 리던던시 모듈들을 감소시키는 고전적이고 효과적인 방법이다. 모델 가지치기 방법은 주로 비구조적 가지치기 및 구조화된 가지치기를 포함한다. 비구조 프루닝은 SparseGPT Frantar 및 Alistarh (2023) 및 LoRAPtune Zhang et al. (2023)과 같은 내부 구조를 고려하지 않고 특정 파라미터를 제거함으로써 LLM을 단순화한다. 그러나 이 방법은 전체 LLM 구조를 무시하여 불규칙한 희소 모델 구성을 초래한다. 또 다른 더 실용적인 접근법은 구조화된 프루닝이고, GUMSyed et al.(2023)은 디코더-전용 LLM들을 위한 몇몇 구조화된 프루닝 방법들의 분석을 행한다. LLM-Pruner Ma et al.(2024)은 기울기 정보에 따라 비임계 구조를 선택적으로 제거한다. ShearedLLaMA Xia et al.(2023)은 표적화된 구조화된 프루닝 및 동적 배치 로딩을 채용한다. LaCo Yang et al.(2024)은 모델을 압축하기 위해 레이어 병합을 사용하였다. 제안된 방법은 기존의 방법에 비해 간단하고 효율적인 구조의 가지치기 방법이다.\n' +
      '\n' +
      '**지식 증류:**지식 증류(KD) 힌튼 등(2015); Gou 등(2021)은 모델을 압축하는 또 다른 방법으로서, 여기서 더 큰 교사 네트워크는 더 작은 학생 네트워크에 지식을 제공한다. 소형 모델을 미세 조정하기 위해 LLM API에 의한 신속한 응답 쌍을 사용하면 약속된 결과를 얻을 수 있다(2022). 또한, 투명한 교사 모델에 접근할 수 있을 때, 그 결과가 더 향상될 수 있다. MiniLLM Gu et al.(2023)은 역컬백-라이블러 발산을 사용하여 학생 모델이 교사 분포의 저확률 영역을 과대평가하는 것을 방지한다. DistILLM Ko et al.(2024)은 학생 생성 출력을 활용하는 데 효율성을 높이기 위해 설계된 적응형 오프 정책 접근법을 사용한다. 그러나, 모델 프루닝에 비해, 이 방법은 종종 더 높은 계산 자원을 필요로 한다.\n' +
      '\n' +
      '**양자화:**양자화 Liu et al. (2021); Gholami et al. (2022); Dettmers et al. (2022, 2024)는 모델 압축 분야에서 널리 받아들여지는 기술로서, 딥러닝 모델의 저장 및 계산 비용을 상당히 절약할 수 있다. 전통적인 모델은 일반적으로 부동 소수점 수로 저장되지만 양자화는 이들을 정수 또는 다른 이산 형태로 변환한다. LUTGEMM Park et al.(2022)은 가중치만을 정량화하고 BCQ 포맷을 사용하여 LLM에서 행렬 곱셈을 최적화한다. SPQR Dettmers et al.(2023)은 비정상적인 가중치를 식별 및 분리하여 더 높은 정확도로 저장하고, 다른 모든 가중치를 3-4비트로 압축한다. 모델 프루닝 방법과 양자화 방법은 직교하며, 이는 프루닝된 모델에 기초한 정량화가 모델을 더 압축할 수 있음을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|} \\hline Model & Ratio/Layer & Perplexity & MMLU \\\\ \\hline \\hline Baseline & 0\\%/32 & 8.4999 & 37.99 \\\\ \\hline ShortGPT & 27.1\\%/23 & 42.6951 & 36.69 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: Llama2-7B-Base-GPTQ에 대한 층 제거 결과.\n' +
      '\n' +
      '**Low-rank factorization:** Low-rank decomposition Cheng et al. (2017); Povey et al. (2018)은 주어진 가중치 매트릭스를 상당히 더 낮은 차원을 갖는 두 개 이상의 더 작은 매트릭스로 분해하여 근사화하는 것을 목표로 하는 모델 압축 기술이다. TensorGPT Xu et al. (2023)은 낮은 랭크 텐서 포맷으로 큰 임베딩들을 저장하여 LLMs들의 공간 복잡도를 감소시키고 에지 디바이스들에서 이용가능하게 한다. 최근 SliceGPT Ashkboos et al. (2024)는 은닉 상태의 행렬 인수분해에 기반한 모델의 구조화된 압축을 구현하고 새로운 행렬을 원점 네트워크의 매개변수로 흡수하였다.\n' +
      '\n' +
      '**모델 중복성:** 연구자들은 비선형 모델인 캐치폴과 모건(1997)에서 상당한 중복성을 오랫동안 알아챘다. 최근에는 변압기 모델 구조가 널리 적용되고 있으며, 연구자들도 그 중복성에 대해 연구하고 있다. Bian et al.(2021)에서 연구자들은 주의 메커니즘에서 중복성을 분석했는데, 주의 헤드 사이에서 명확하고 유사한 중복 패턴(클러스터 구조)이 관찰된다. 달비 등(2020)에서 연구자들은 BERT Devlin 등(2018)과 XLNet Yang 등(2019)의 두 가지 사전 훈련된 모델을 해부하여 표현 수준과 보다 세밀한 뉴런 수준에서 얼마나 많은 중복성을 나타내는지를 연구한다. 그러나, 디코더 전용 구조들에 기초한 현재의 대형 언어 모델들에서의 리던던시는 여전히 탐색될 필요가 있다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '이 연구는 계층 중복성과 주의 엔트로피로 정의되는 "중요도" 메트릭을 기반으로 하는 대규모 언어 모델(LLM)을 가지치기하는 새로운 접근법을 도입했다. 우리의 연구는 LLM에서 상당한 정도의 계층별 중복성을 보여주며, 이는 특정 계층이 전체 네트워크 기능에 최소한으로 기여하므로 모델 성능을 실질적으로 손상시키지 않고 제거될 수 있음을 나타낸다. 각 층의 계산된 중요도에 의해 유도되는 간단한 층 제거 전략을 사용함으로써 모델의 매개변수 수와 계산 요구량을 약 25% 감소시키면서 LLM 성능의 최대 95%를 유지할 수 있음을 입증했다. 이 성과는 이전의 최첨단 가지치기 방법을 능가할 뿐만 아니라 LLM 배치 전략에서 추가 최적화의 가능성을 강조한다.\n' +
      '\n' +
      '우리의 연구 결과는 LLM에 내재된 중복성이 너비 기반보다는 깊이 기반임을 시사하며, 신경망의 구조적 효율성에 대한 향후 연구를 위한 방법을 강조한다. 또한, 단순성과 효율성을 특징으로 하는 우리의 가지치기 접근법은 양자화와 같은 다른 압축 기술과 호환되어 중요하고 다재다능한 모델 크기 감소를 위한 복합 경로를 제공한다.\n' +
      '\n' +
      '우리의 연구의 의미는 학문적 관심을 넘어 자원이 제한된 환경에서 고급 LLM을 배포하기 위한 실용적인 솔루션을 제공한다. 광범위한 재교육이 필요 없이 보다 효율적인 모델 아키텍처를 가능하게 함으로써, 당사의 가지치기 방법은 다양한 플랫폼 및 장치에 걸쳐 최첨단 AI 기능에 대한 광범위한 액세스를 용이하게 합니다.\n' +
      '\n' +
      '결론적으로, 계층 중복성에 대한 조사와 중요도 기반 가지치기 전략의 개발은 대규모 언어 모델의 최적화에서 의미 있는 발전을 나타낸다. 정교한 AI 도구에 대한 수요가 계속 증가함에 따라 우리와 같은 접근법은 이러한 기술을 보다 접근 가능하고 지속 가능하게 만드는 데 중요한 역할을 할 것이다. 향후 연구는 모델 중복성에 대한 이해를 개선하고 신경망 모델의 효율성을 높이기 위한 추가 방법을 탐색하는 데 중점을 둘 것이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Ashkboos et al. (2024) Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. 2024. Slicegpt: 행과 열을 삭제하여 대언어 모델을 압축한다. _ arXiv preprint arXiv:2401.15024_.\n' +
      '* Bian et al. (2021) Yuchen Bian, Jiaji Huang, Xingyu Cai, Jiahong Yuan, and Kenneth Church. 2021. 주의 중복성: 종합적 연구. _Proceedings of the 2021 conference of the north American chapter of the association for computational linguistics: Human language technologies_, pages 930-945.\n' +
      '* Bisk et al. (2020) 요나탄 비스크, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piga: Reasoning about physical commonsense in natural language. 인공지능에 관한 AAAI 컨퍼런스의 _Proceedings_에서, 페이지 7432-7439.\n' +
      '\n' +
      '에드워드 캐치폴과 바이런 JT 모건 1997. 파라미터 중복성 검출__ Biometrika_, 84(1):187-196.\n' +
      '* Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. 코드로 훈련된 대형 언어 모델들을 평가한다. _ arXiv preprint arXiv:2107.03374_.\n' +
      '*Cheng et al. (2017) Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2017. Survey of model compression and acceleration for deep neural networks. _ arXiv preprint arXiv:1710.09282_.\n' +
      '* Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. _ Journal of Machine Learning Research_, 24(240):1-113.\n' +
      '* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구합니다. _Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics Association: Human Language Technologies, Volume 1(Long and Short Papers)_, pages 2924-2936.\n' +
      '* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. 수학 단어 문제를 해결하기 위한 훈련 검증자 __ arXiv preprint arXiv:2110.14168_.\n' +
      '* Dalvi et al. (2020) Fahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yoatan Belinkov. 2020. 사전 훈련된 변압기 모델의 중복성 분석.\n' +
      '* Dettmers et al.(2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Llm. int8(): 스케일에서 트랜스포머에 대한 8비트 행렬 곱셈; _ arXiv preprint arXiv:2208.07339_.\n' +
      '* Dettmers et al.(2024) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora : 양자화된 llms의 효율적인 finetuning. _ 신경 정보 처리 시스템_, 36의 발전.\n' +
      '*Dettmers et al. (2023) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuzmedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. 2023. Spqr: 근거리 무손실 llm 가중치 압축을 위한 희소 양자화된 표현 _ arXiv preprint arXiv:2306.03078_.\n' +
      '* Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. _ arXiv preprint arXiv:1810.04805_.\n' +
      '* Frantar and Alistarh (2023) Ellias Frantar and Dan Alistarh. 2023. 대규모 언어 모델들이 원샷으로 정확하게 가지치기될 수 있다. _ arXiv preprint arXiv:2301.00774_.\n' +
      '* Gholami et al. (2022) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. 2022. 효율적인 신경망 추론을 위한 양자화 방법들에 대한 조사. _Low-Power Computer Vision_에서, 페이지 291-326. Chapman 및 Hall/CRC.\n' +
      '* Gou et al. (2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. 지식 증류: 설문조사. _ International Journal of Computer Vision_, 129:1789-1819.\n' +
      '* Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. 대용량 언어 모델의 지식 증류. _ arXiv preprint arXiv:2306.08543_.\n' +
      '* Han et al. (2015) Song Han, Jeff Pool, John Tran, and William Dally. 2015. learning both weight and connections for efficient neural network. _ 신경 정보 처리 시스템들_, 28에서의 진보들.\n' +
      '* Hasan et al. (2021) Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M Sohel Rahman, and Rifat Shahriyar. 2021. Xl-sum: 44개 언어에 대한 대규모 다국어 추상 요약. _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pages 4693-4703.\n' +
      '* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. 대용량 멀티태스크 언어 이해도 측정. _ arXiv preprint arXiv:2009.03300_.\n' +
      '* Hovy et al. (2020) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in neural network. _ arXiv preprint arXiv:1503.02531_.\n' +
      '* Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. 레이, 오리올 빈날스 로랑 시프레 2022. 컴퓨팅-최적 대형 언어 모델 트레이닝.\n' +
      '* Huang et al. (2021) Feiqing Huang, Yuefeng Si, Yao Zheng, and Guodong Li. 2021. 압축된 컨볼루션 신경망에 대한 모델 중복성의 새로운 척도.\n' +
      '* Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. 신경 언어 모델에 대한 스케일링 법칙.\n' +
      '* 고 등(2024) 고종우, 김숭륜, 천이천, 윤세영. 2024. 증류기: 대용량 언어 모델에 대한 유선형 증류를 향하여 _ arXiv preprint arXiv:2402.03898_.\n' +
      '* Lai et al. (2017) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: 검사로부터의 대규모 읽기 이해 데이터세트. _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 785-794.\n' +
      '* LeCun et al. (1989) Yann LeCun, John Denker, and Sara Solla. 1989. 최적의 뇌 손상. _ 신경 처리 시스템_, 2의 발전.\n' +
      '* Li et al. (2024) Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2024. Cmmlu: 중국어로 된 방대한 멀티태스킹 언어 이해도 측정.\n' +
      '* Li et al. (2022) Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al. 2022. 설명 큰 언어 모델로부터의 설명은 작은 추론자를 더 잘 만든다. _ arXiv preprint arXiv:2210.06726_.\n' +
      '* Liu et al. (2021) Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. 2021. 비전 트랜스포머를 위한 훈련 후 양자화. _ 신경 정보 처리 시스템_, 34:28092-28103의 발전.\n' +
      '* Ma et al. (2024) Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2024. Llm-pruner: 대형 언어 모델의 구조적 pruning 상에서. _ 신경 정보 처리 시스템들_, 36에서의 진보들.\n' +
      '* 박등(2022) 건호박, 배성박, 세정권, 김병욱, 이영주, 동수. 2022. nuqmm: 대규모 생성 언어 모델의 효율적인 추론을 위한 양자화된 matmul. _ arXiv preprint arXiv:2206.09557_.\n' +
      '* Peng et al. (2023) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. 2023. Rwkv: Reinventing rnns for the transformer era. _ arXiv preprint arXiv:2305.13048_.\n' +
      '* Povey et al. (2018) Daniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and Sanjeev Khudanpur. 2018. Semi-orthogonal low-rank matrix factorization for deep neural networks. _Interspeech_에서, 페이지 3743-3747.\n' +
      '* Press et al. (2021) Ofir Press, Noah A Smith, and Mike Lewis. 2021. 열차단축, 시험장: 선형편향을 갖는 주의는 입력길이 외삽을 가능하게 한다. _ arXiv preprint arXiv:2108.12409_.\n' +
      '* Rae et al. (2019) Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. 2019. 장거리 시퀀스 모델링을 위한 압축 변압기. _International Conference on Learning Representations_.\n' +
      '* Reddy et al. (2019) Siva Reddy, Danqi Chen, and Christopher D Manning. 2019. Coqa: 대화식 질문 답변 챌린지 _ The Association for Computational Linguistics_, 7:249-266의 트랜잭션.\n' +
      '* Samragh et al. (2023) Mohammad Samragh, Mehrdad Farajtabar, Sachin Mehta, Raviteja Vemulapalli, Fartash Faghri, Devang Naik, Oncel Tuzel, and Mohammad Rastegari. 2023. 가중치 서브클로닝: 더 큰 사전 훈련된 것을 사용하는 변압기의 직접 초기화.\n' +
      '\n' +
      '젠린 수, 무타다 아흐메드, 유루, 성펑 판, 원보, 윤펑 류 등이다. 2024. 로포머: 회전 위치 매립을 구비한 개량형 트랜스포머. _ Neurocomputing_, 568:127063.\n' +
      '* Sun et al. (2020) Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. 2020. 도전적인 중국어 기계독해력을 위한 사전 지식 조사. _ Computational Linguistics_, 8:141-155의 거래.\n' +
      '* Syed et al. (2023) Aaquib Syed, Phillip Huang Guo, and Vijaykaarti Sundarapandiyan. 2023. 프룬 앤 튜닝: 대용량 언어 모델에 대한 효율적인 프루닝 기법 개선 _ Arxiv_.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. 주의력만 있으면 됩니다 _ 신경 정보 처리 시스템_, 30의 발전.\n' +
      '* Xia et al. (2023) Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2023. 전단 라마: 구조화된 프루닝을 통한 언어 모델 사전 트레이닝 가속화. _ arXiv preprint arXiv:2310.06694_.\n' +
      '* Xu et al. (2020) Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, et al. 2020. Clue: 중국어 이해도 평가 벤치마크. _Proceedings of the 28th International Conference on Computational Linguistics_, pages 4762-4772.\n' +
      '* Xu et al. (2023) Mingxue Xu, Yao Lei Xu, and Danilo P Mandic. 2023. 텐서grpt: 텐서-트레인 분해에 기초한 l lms 내의 임베딩 층의 효율적인 압축. _ arXiv preprint arXiv:2307.00526_.\n' +
      '* Yang et al. (2023) Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. 2023. Baichuan 2: Open large scale language models. _ arXiv preprint arXiv:2309.10305_.\n' +
      '* Yang et al. (2024) Yifei Yang, Zouying Cao, and Hai Zhao. 2024. Laco: 레이어 붕괴를 통한 대형 언어 모델 가지치기.\n' +
      '* Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. _ 신경 정보 처리 시스템들_, 32에서의 진보들.\n' +
      '* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, 요나탄 Bisk, Ali Farhadi, and Yejin Choi. 2019년. 헬라스바그: 기계가 정말로 당신의 문장을 끝낼 수 있나요? _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4791-4800.\n' +
      '* Zhang et al. (2023) Mingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, et al. 2023. Pruning meets lowrank parameter-efficient fine-tuning. _ arXiv preprint arXiv:2305.18403_.\n' +
      '* Zheng et al. (2019) Chujie Zheng, Minlie Huang, and Aixin Sun. 2019. Chid: Cloze test를 위한 대규모 중국어 관용구 데이터셋. _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 778-787.\n' +
      '* Zhu et al.(2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023. 큰 언어 모델에 대한 모델 압축에 대한 조사.\n' +
      '\n' +
      '## 부록 계층 제거를 위한 세부 전략\n' +
      '\n' +
      '## 제거 레이어에 대한 부록 B 벤치마크 설정\n' +
      '\n' +
      '벤치마킹된 모델에서 특정 레이어 제거 구성을 설명하기 전에 구조 수정에 대한 간략한 개요가 보장된다. 처음에 32개의 층으로 설계된 Llama-2-7B 및 Baichuan-2-7B 모델은 전체 층 프레임워크의 28% 감소에 해당하는 9개의 층을 절제함으로써 간소화되었다. 원래 40개 층으로 구성된 13B 변형은 10개 층의 감소를 경험하여 25% 감소를 나타냈다. 계층 제거를 위한 선택 기준은 상승 블록 영향(BI) 값을 기반으로 하여 최소화를 위한 데이터 기반 접근 방식을 보장했다. 이 전제는 벤치마크 구성을 뒷받침합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r} \\hline \\hline Model & Removed Layers \\\\ \\hline Llama-2-7B & 27, 26, 25, 28, 24, 29, 23, 21, 22 \\\\ Llama-2-13B & 33, 31, 32, 30, 29, 34, 28, 35, 27, 26 \\\\ Baichuan-2-7B & 26, 27, 25, 28, 24, 29, 23, 22, 30 \\\\ Baichuan-2-13B & 32, 31, 33, 30, 34, 29, 28, 35, 27, 26 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 벤치마크 모델에 대한 제거 레이어 설정\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Strategy & Description \\\\ \\hline Sequential & Layers are removed sequentially from the beginning of the model. The process starts with layer 0 and progressively includes more layers for removal (e.g., \\{0\\}, \\{0, 1\\}, \\dots\\)). \\\\ Reverse-order & This strategy involves starting from the model’s final layer and progressively removing layers in reverse order (e.g., \\{-1\\}, \\{-1, -2\\}, \\dots\\). \\\\ Relative Magnitude & Layers are removed in ascending order based on their Relative Magnitude values. The removal process accumulates layers from those with the smallest to the largest values, mirroring the sequential strategy’s accumulation method. \\\\ BI (Block Influence) & Follows a similar accumulation approach as the Sequential strategy, but layers are ordered and removed according to their BI values, starting from the lowest and moving to the highest. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 모델의 레이어 제거 전략.\n' +
      '\n' +
      '## 바이촨2계열 모델에서의 부록 C층 제거\n' +
      '\n' +
      '## HumanEval상의 부록 D층 제거.\n' +
      '\n' +
      '또한 층 제거는 생성 작업에서 모델의 성능에 상당한 영향을 미친다는 것을 발견했다. 구체적으로, 코드 생성 모델의 성능을 평가하기 위해 사용된 데이터셋인 HumanEval Chen et al.(2021)의 Baichuan2-13B 모델에 대한 실험을 수행하였다. 그 결과를 그림 10에 나타내었다. 10개의 층을 제거한 후 HumanEval에서의 성능은 거의 완전히 저하되었다. 우리는 그것이 생성 작업의 상당한 누적 오류 때문일 수 있다고 추측한다.\n' +
      '\n' +
      '그림 10: 생성 태스크에 대한 성능 저하.\n' +
      '\n' +
      '그림 9: Baichuan2-시리즈 모델에 대한 다른 메트릭에 의한 프루닝.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>