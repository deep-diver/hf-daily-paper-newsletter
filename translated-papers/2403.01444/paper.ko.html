<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'DGStream: 사진-실감 자유시점 비디오의 효율적인 스트리밍을 위한 3D 가우시안들의 온더플라이 트레이닝\n' +
      '\n' +
      '지아카이 선, 한자오, 광위안 리, 잔지장, 레이자오, 웨이싱\n' +
      '\n' +
      'Zhejiang University\n' +
      '\n' +
      '{csjk, csjh, cslgy, cszzj, cszhl, wxing}@zju.edu.cn\n' +
      '\n' +
      '[https://sjojok.github.io/3dgstream](https://sjojok.github.io/3dgstream)\n' +
      '\n' +
      'Corresponding authors.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '멀티뷰 비디오로부터 동적 장면들의 사진-실감 자유 시점 비디오들(FVV)을 구성하는 것은 도전적인 노력으로 남아 있다. 현재의 신경망 렌더링 기술에 의해 달성된 현저한 진보에도 불구하고, 이러한 방법들은 일반적으로 오프라인 트레이닝을 위해 완전한 비디오 시퀀스를 필요로 하고 실시간 렌더링이 가능하지 않다. 이러한 제약을 해결하기 위해, 본 논문에서는 실세계 동적 장면들의 효율적인 FVV 스트리밍을 위해 설계된 3DGStream을 소개한다. 이 방법은 12초 이내에 프레임당 빠른 복원과 200 FPS에서 실시간 렌더링을 수행한다. 구체적으로 3D 가우시안(3DGs)을 활용하여 장면을 표현한다. 프레임당 3DG를 직접 최적화하는 순진한 접근법 대신, 우리는 3DG의 번역 및 회전을 모델링하기 위해 컴팩트 신경 변환 캐시(NTC)를 사용하여 각 FVV 프레임에 필요한 훈련 시간과 저장을 현저하게 줄인다. 또한, 동적 장면에서의 새로운 객체를 처리하기 위한 적응적 3DG 추가 전략을 제안한다. 실험 결과, 3DGStream은 렌더링 속도, 화질, 학습 시간, 모델 저장 측면에서 최첨단의 방법과 비교하여 경쟁적인 성능을 달성하였다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '여러 뷰에서 알려진 포즈 카메라 세트에 의해 캡처된 비디오로부터 자유 뷰 포인트 비디오(FVV)를 구성하는 것은 컴퓨터 비전 및 그래픽의 도메인 내에서 프론티어 도전으로 남아 있다. VR/AR/XR 영역에서 이 작업의 잠재적 가치와 적용 전망은 많은 연구를 끌어들였다. 전통적인 접근법은 주로 동적 그래픽 프리미티브를 명시적으로 재구성하는 기하학 기반 방법[15, 17]과 보간을 통해 새로운 뷰를 얻는 이미지 기반 방법[7, 75]의 두 가지 범주로 나뉜다. 그러나, 이러한 종래의 방법들은 복잡한 기하학적 구조 및 외관을 특징으로 하는 실세계 장면들을 다루기 위해 고군분투한다.\n' +
      '\n' +
      '최근, 신경 복사 필드(NeRFs) [36]은 3D 체적 표현으로서 새로운 뷰를 합성하는 강력한 능력으로 인해 상당한 관심을 받고 있다. NeRF 유사 작업[19, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 44, 45, 46, 48, 60, 67]의 연속은 동적 장면에서 FVV를 구성하는 발전을 더욱 촉진했다. 그럼에도 불구하고, NeRF와 같은 FVV 구축 방법의 대다수는 두 가지 주요 한계에 직면했다: (1) 일반적으로 시간이 많이 걸리는 오프라인 교육을 위해 완전한 비디오 시퀀스를 필요로 하며, 이는 동적 장면을 재생할 수 있지만 스트리밍할 수 없음을 의미하며, (2) 일반적으로 실시간 렌더링을 달성하지 못하여 실제 적용을 방해한다.\n' +
      '\n' +
      '최근, Kerbl[26]은 3D Gaussians(3DGs)를 이용한 실시간 복사 필드 렌더링을 달성하여 정적 장면에서의 새로운 뷰의 즉각적인 합성을 가능하게 한다.\n' +
      '\n' +
      '도 1: **N3DV 데이터세트의 _flame stack_ scene에 대한 비교[31]. 트레이닝 시간, 필수 저장 및 PSNR은 전체 비디오에 대한 평균으로 계산된다. 우리의 방법은 빠른 온라인 훈련과 실시간 렌더링의 능력으로 눈에 띄며, 모델 저장과 이미지 품질 모두에서 경쟁력이 있다.**\n' +
      '\n' +
      '몇 분간의 훈련으로. 이러한 돌파구에서 착안하여 동적 장면의 FVV(Free-Viewpoint Videos)를 구성하기 위해 3DG를 활용하는 방법인 3DGStream을 제안한다. 구체적으로, 먼저 타임스텝 0에서 다시점 프레임에서 초기 3DG를 학습한 후, 각 타임스텝 \\(i\\)에 대해 이전 타임스텝 \\(i-1\\)의 3DG를 초기화로 사용하여 2단 파이프라인으로 전달한다. (1) 단계 1에서, 우리는 3DG의 변환을 모델링하기 위해 신경망 변환 캐시(NTC)를 훈련한다. (2) 단계 2에서는 적응형 3DG 추가 전략을 사용하여 이러한 객체 근처에 프레임별 추가 3DG를 산란하여 새로운 객체를 처리하고 주기적인 분할 및 가지치기와 함께 최적화한다. 2단계 파이프라인이 끝나면 NTC에 의해 변환된 3DG와 현재 타임스텝 \\(i\\)에서 렌더링하기 위해 추가 3DG를 모두 사용하며, 이후 타임스텝의 초기화를 위해 전자만 이월한다. 이 설계는 각 프레임에 대한 모든 3DG가 아니라 프레임별 NTC 및 프레임별 추가만 저장하면 되므로 FVV에 대한 저장 요구 사항을 크게 줄인다.\n' +
      '\n' +
      '3DGStream은 실시간 메가픽셀 해상도에서 사진 실시간 FVV를 렌더링할 수 있으며, 예외적으로 빠른 프레임당 훈련 속도와 제한된 모델 저장 요구 사항을 자랑한다. 도면에 도시된 바와 같다. 도 1 및 도 2를 참조하면, 프레임 당 스크래치로부터 트레이닝하는 정적 재구성 방법 및 완전한 비디오 시퀀스에 걸쳐 오프라인 트레이닝을 필요로 하는 동적 재구성 방법과 비교하여, 우리의 접근법은 트레이닝 속도와 렌더링 속도 모두에서 탁월하여 이미지 품질 및 모델 저장에서 경쟁력을 유지한다. 또한, 본 방법은 모든 관련 측면에서 동일한 작업을 해결하는 최첨단 기술인 StreamRF[29]보다 우수하다.\n' +
      '\n' +
      '요약하자면, 우리의 기여는 다음과 같습니다.\n' +
      '\n' +
      '* 우리는 3DGStream을 제안한다. 3DGStream은 비디오 스트림에 대한 실시간 렌더링 가능한 FVV를 온더플라이로 구성하기 위한 방법으로, 전체 비디오 시퀀스에 대한 긴 오프라인 트레이닝의 필요성을 제거한다.\n' +
      '* 동적 장면 내에서 떠오르는 객체를 다루기 위해 적응형 3DG 추가 전략과 함께 3DG의 변환을 모델링하기 위해 NTC를 활용한다. 이 조합은 제한된 성능 오버헤드로 장면 변경을 수용하는 3DG의 세심한 조작을 허용한다.\n' +
      '* 3DGStream의 렌더링 품질, 훈련 시간 및 필수 저장 공간에서의 경쟁 우위를 입증하고 기존의 최첨단 동적 장면 재구성 방법에 비해 우수한 렌더링 속도를 입증하기 위해 광범위한 실험을 수행했다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '정적 장면을 위한 새로운 시각 합성\n' +
      '\n' +
      '정적 장면들의 이미지 세트로부터 새로운 뷰들을 합성하는 것은 컴퓨터 비전 및 그래픽의 도메인들에서 시간적으로 유서 깊은 문제이다. Lumagraph[8, 22] 또는 Light-Field[10, 16, 28, 50]과 같은 전통적인 방법들은 보간을 통해 새로운 뷰 합성을 달성한다. 최근, 신경 복사 필드(NeRF) [36]은 다층 퍼셉트론(MLP)을 사용하여 복사 필드를 표현함으로써 사실적 합성 결과를 달성했다. 후속 일련의 작업은 훈련 속도 가속[12, 13, 20, 25, 40, 52], 실시간 렌더링[14, 21, 23, 47, 72], 도전 장면[2, 3, 4, 35, 37, 56] 또는 희소 입력[11, 41, 53, 63, 66, 69, 73]에서 합성 품질을 향상시키는 등 다양한 측면에서 NeRF의 성능을 향상시킨다. 바닐라 NeRF는 값비싼 볼륨 렌더링을 사용하기 때문에 렌더링을 위해 신경망 쿼리를 필요로 하기 때문에 후속 접근 방식은 훈련 시간, 렌더링 속도, 모델 저장, 이미지 품질 및 적용 가능성에서 상충 관계에 직면했다. 이러한 문제를 해결하기 위해 Kerbl _et al._[26]은 3DG와 미분 가능한 점 기반 렌더링을 통합하는 3D 가우스 스플래팅(3DG-S)을 제안한다. 3DG-S는 적당한 저장 요구 사항으로 짧은 훈련 기간 후에 대규모 무제한 장면에서 실시간 고충실도 뷰 합성을 가능하게 한다. 이 작업에서 영감을 받아 동적 장면의 FVV를 구성하는 작업으로 그 적용을 확장한다. 한 단계 더 나아가 효율적인 FVV 스트리밍을 달성하기 위해 현장 훈련 프레임워크를 설계한다.\n' +
      '\n' +
      '### 동적 장면의 자유 시점 비디오\n' +
      '\n' +
      '동적 장면의 비디오 세트로부터 FVV를 구성하는 것은 컴퓨터 비전 및 그래픽 영역에서 더 도전적이고 적용 가능한 작업이다. 이 작업을 해결하기 위한 이전의 시도는 동적 프리미티브[15, 17]의 구성을 중심으로 피벗되거나 보간[7, 75]에 의존한다. 함께\n' +
      '\n' +
      '그림 2: **N3DV 데이터셋**[31]에서 우리의 방법과 다른 방법의 비교. \\ (\\square\\)은 프레임당 스크래치로부터의 트레이닝을 나타내고, \\(\\triangle\\)은 완전한 비디오 시퀀스에 대한 오프라인 트레이닝을 나타내며, \\(\\bigcirc\\)은 비디오 스트림에 대한 온라인 트레이닝을 나타낸다. 온라인 교육을 달성하는 동안, 우리의 방법은 렌더링 속도와 전체 교육 시간 모두에서 최첨단 성능에 도달한다.\n' +
      '\n' +
      '정적 장면에 대한 새로운 뷰 합성에서 NeRF 유사 방법의 성공, 일련의 작업 [1, 9, 19, 29, 30, 31, 32, 33, 34, 42, 44, 45, 46, 48, 51, 55, 57, 59, 61, 62, 68, 74] 동적 장면에서 FVV를 구성하기 위해 NeRF를 사용하려고 시도한다. 이러한 작업은 일반적으로 사전 구동, 흐름 기반, 경사 기반, 시공간 입력을 사용하는 작업 및 프레임당 훈련의 5가지 유형으로 분류할 수 있다.\n' +
      '\n' +
      '**Prior-driven** 방법 [27, 30, 62, 74, 68, 74]은 파라메트릭 모델을 활용하거나 인간-포즈 골격과 같은 추가 이전을 통합하여 특정 동적 객체, _예: 인간의 재구성에 대한 성능을 강화한다. 그러나 그들의 적용은 제한적이며 더 넓은 장면으로 일반화할 수 없다.\n' +
      '\n' +
      '**흐름 기반** 방법 [32, 33]은 주로 단안 비디오로부터 FVV를 구성하는 데 중점을 둔다. 연속 프레임에서 3D 점의 대응 관계를 추정함으로써 인상적인 결과를 얻을 수 있다. 그럼에도 불구하고 복잡한 동적 장면에서 단안 재구성의 내재적 비포즈성은 깊이, 광학 흐름 및 모션 분할 마스크와 같은 보충적 전조를 자주 요구한다.\n' +
      '\n' +
      '**Warp-based** 방법 [1, 42, 44, 46, 51, 55, 61]은 장면의 역학이 정적 구조의 변형으로부터 발생한다는 가정이다. 이러한 기술은 각 프레임의 복사 필드를 하나 또는 여러 표준 프레임에 왜곡하여 주목할만한 결과를 달성한다. 그러나 그들이 의존하는 강한 가정은 종종 위상 변화를 처리하는 것을 방지한다.\n' +
      '\n' +
      '*시공간 입력**[9, 19, 31, 45, 57, 58, 48]을 사용하는 방법은 시간 차원을 추가하여 복사 필드를 향상시켜 시공간 좌표를 사용하여 복사 필드를 쿼리할 수 있다. 이러한 기법들은 동적 장면들에서 새로운 시점들을 합성하는 놀라운 능력을 보여주지만, 얽힌 장면 파라미터들은 다운스트림 애플리케이션들에 대한 그들의 적응성을 제한할 수 있다.\n' +
      '\n' +
      '**프레임당 훈련** 방법 [29, 34, 59]은 또한 우리가 채택한 패러다임인 프레임당 훈련을 활용하여 온라인에서의 장면 변화에 적응한다. 구체적으로, StreamRF[29]는 장면 표현을 위해 Plenoxels[20]을 사용하며, 협대역 튜닝 및 차분 기반 압축과 같은 기술을 통해 최소한의 저장 요구 사항으로 신속한 현장 훈련을 달성한다. ReRF[59]는 장면 표현을 위해 DVGO[52]를 사용하고 프레임 간 불일치를 모델링하기 위해 프레임별로 모션 그리드 및 잔여 그리드 프레임을 최적화하여 고품질의 FVV 스트리밍 및 렌더링을 가능하게 한다. Dynamic3DG[34]는 단순화된 3DG를 최적화하고 동적 장면에서 고품질 신규 뷰 합성을 위해 물리적 기반 이전을 통합한다.\n' +
      '\n' +
      '앞서 언급한 작업 중 FVV를 스트리밍할 수 있는 것은 NeRF-Player[51], ReRF[59], StreamRF[29], Dynamic3DG[34]뿐이다. NeRF 플레이어는 분해 모듈과 특징 스트리밍 모듈을 통해 FVV 스트리밍을 달성하지만 사전 학습된 모델만 스트리밍할 수 있다. ReRF와 Dynamic3DG는 전경 마스크를 장착하고 몇 개의 객체만 포함하는 내향 장면을 처리하는 것으로 제한되므로 각 프레임을 훈련하는 데 몇 분이 필요하다. 스트림RF는 압축된 모델 저장소로 도전적인 실제 동적 장면에서 고충실도 FVV를 구성하기 위해 각 프레임의 훈련에 몇 초만 요구함으로써 두드러진다. 그러나 렌더링 속도는 부족합니다. 대조적으로, 우리의 접근법은 200 FPS에서 실시간 렌더링을 달성하면서 훈련 속도, 모델 스토리지 및 이미지 품질에서 StreamRF와 일치하거나 능가한다.\n' +
      '\n' +
      '도 3: **3DGStream의 개요. 멀티뷰 비디오 스트림들의 세트가 주어지면, 3DGStream은 캡처된 동적 장면의 고품질 FVV 스트림을 온 더 플라이로 구성하는 것을 목표로 한다. 초기에는 타임스텝(0\\)에서 장면을 표현하기 위해 3DG 집합을 최적화한다. 각각의 후속 타임스텝 \\(i\\)에 대해, 타임스텝 \\(i-1\\)의 3DG를 초기화로 사용한 다음, 단계 1: 신경망 변환 캐시(NTC)를 훈련하여 3DG의 번역 및 회전을 모델링한다. 훈련 후 NTC는 3DG를 변환하여 현재 타임스텝에서 다음 타임스텝과 다음 단계를 준비한다. 2단계: 우리는 잠재적인 위치에서 프레임별 추가 3DG를 산란하고 주기적 분할 및 가지치기와 함께 최적화한다. 2단계 프로세스가 끝난 후, 변환 및 추가 3DG 모두 현재 타임스텝 i에서 렌더링하는 데 사용되며, 변환된 것만이 다음 타임스텝으로 전달된다.**\n' +
      '\n' +
      '### Concurrent Works\n' +
      '\n' +
      '동적 3DG를 제외한 여러 동시 작업은 동적 장면을 표현하기 위해 3DG-S를 확장했다. 변형 가능한 3DG[70]는 MLP를 사용하여 3DG의 변형을 모델링하고, [65]는 keplane 기반 인코더를 도입하여 변형 질의의 효율성을 향상시킨다. 한편, [18, 71]은 동적 장면 표현을 위해 3DG를 4DG 프리미티브로 들어올린다. 그러나 이러한 접근 방식은 오프라인 재구성에 국한되고 스트리밍 가능한 기능이 부족한 반면, 우리의 작업은 온라인 교육 패러다임으로 FFV의 효율적인 스트리밍을 달성하는 것을 목표로 한다.\n' +
      '\n' +
      '##3 배경 : 3D Gaussian Splatting\n' +
      '\n' +
      '3D Gaussian Splatting (3DG-S) [26]은 명시적 장면 표현으로서 이방성 3D Gaussians을 사용한다. 빠르게 차별화할 수 있는 래스터라이저와 페어링된 3DG는 몇 분간의 훈련만으로 실시간 새로운 뷰 합성을 달성합니다.\n' +
      '\n' +
      '장면 표현으로서의###3D 가우시안\n' +
      '\n' +
      '3DG는 점 (_i.e_., 평균) \\(\\mu\\)을 중심으로 하는 공분산 행렬 \\(\\시그마\\)에 의해 정의된다:\n' +
      '\n' +
      '\\[G(x;\\mu,\\Sigma)=e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}. \\tag{1}\\]\n' +
      '\n' +
      '최적화 과정에서 양의 반정의성을 보장하기 위해 공분산 행렬 \\(\\Sigma\\)을 회전 행렬 \\(R\\)과 스케일링 행렬 \\(S\\)으로 분해한다:\n' +
      '\n' +
      '\\[\\Sigma=RSS^{T}R^{T}. \\tag{2}\\]\n' +
      '\n' +
      '회전은 편리하게 단위 쿼터니온으로 표현되며, 스케일링은 3D 벡터를 사용한다. 또한, 각 3DG는 \\(\\alpha\\)-블렌딩(Eq)에 사용되는 불투명도 값 \\(\\alpha\\)과 함께 시점 의존 색상을 나타내는 구면 고조파(SH) 계수 세트를 포함한다. (4)).\n' +
      '\n' +
      '미분 래스터화를 위한### 스플래팅\n' +
      '\n' +
      '새로운 뷰 합성을 위해, 3DG-S[26]는 3DG를 2D 가우시안(2DG) 스플래트[76]로 투영한다:\n' +
      '\n' +
      '\\[\\Sigma^{\\prime}=JW\\Sigma W^{T}J^{T}. \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(\\Sigma^{\\prime}\\)는 카메라 좌표의 공분산 행렬이다. \\ (J\\)은 사영 변환의 아핀 근사치의 자코비안이고, \\(W\\)은 뷰잉 변환 행렬이다. 우리는 \\(\\Sigma^{\\prime}\\)의 세 번째 행과 세 번째 열을 건너뛰어 \\(\\Sigma_{2d}\\)으로 표시된 \\(2\\times 2\\) 행렬을 유도할 수 있다. 또한, 3DG의 평균인 \\(\\mu\\)을 이미지 공간에 투영하면 2D 평균인 \\(\\mu_{2d}\\)이 된다. 결과적으로 영상공간에서 2DG를 \\(G_{2d}(x;\\mu_{2d},\\Sigma_{2d})\\으로 정의할 수 있다.\n' +
      '\n' +
      '(\\(\\Sigma^{\\prime}\\)을 사용하여, 픽셀과 겹쳐지는 \\(N\\) 순서점들을 블렌딩함으로써 픽셀의 색상 \\(C\\)을 계산할 수 있다:\n' +
      '\n' +
      '[C=\\sum_{i\\in N}c_{i}\\alpha^{\\prime}_{i}\\prod_{j=1}^{i-1}(1-\\alpha^{\\prime}_{j}). \\tag{4}\\\n' +
      '\n' +
      '여기서, \\(c_{i}\\)는 \\(i\\)번째 3DG의 시점 의존적 색상을 나타낸다. \\(c_{i}\\) (\\alpha^{\\prime}_{i}\\)는 \\(i\\)번째 3DG\\(G\\)의 불투명도 \\(\\alpha_{i}\\)에 해당하는 2DG\\(G_{2d}\\)의 평가를 곱하여 결정된다.\n' +
      '\n' +
      '맞춤형 CUDA 커널과 결합된 고도로 최적화된 래스터화 파이프라인을 활용하면 3DG-S의 훈련 및 렌더링이 현저하게 빠르다. 예를 들어, 메가픽셀 규모의 실제 세계 장면의 경우, 단지 몇 분 동안의 최적화는 3DG가 100 FPS를 능가하는 광-실감적 시각적 품질 및 렌더링 속도를 달성할 수 있게 한다.\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      '3DGStream은 프레임당 트레이닝 패러다임을 이용하여 다시점 비디오 스트림으로부터 광실감형 FVV 스트림을 온더플라이로 구성한다. 타임스텝 0에서 3DG[26]를 학습하여 프로세스를 시작하고, 후속 타임스텝을 위해 이전 타임스텝의 3DG를 초기화로 사용하여 2단계 파이프라인으로 전달한다. 먼저, 신경망 변환 캐시(Neural Transformation Cache, NTC)를 학습하여 각 3DG의 변환을 모델링한다. 훈련이 끝나면 3DG를 변환하고 변환된 3DG를 다음 타임스텝으로 운반합니다. 둘째(Sec. 4.2) 새로운 객체를 처리하기 위해 적응형 3DG 추가 전략을 사용한다. 각 FVV 프레임에 대해 변환된 3DG와 추가 3DG를 모두 사용하여 현재 타임스텝에서 뷰를 렌더링하는 반면 후자는 다음 타임스텝으로 전달되지 않는다. 모든 3DG가 아닌 각 후속 타임스텝에 대해 NTC 및 추가 3DG의 매개변수를 훈련하고 저장하기만 하면 된다. 우리는 그림 3에서 접근법의 개요를 묘사한다.\n' +
      '\n' +
      '### 신경변환 캐쉬\n' +
      '\n' +
      'NTC의 경우 3DG의 변환을 모델링하기 위해 컴팩트하고 효율적이며 적응적인 구조를 찾는다. 모델 스토리지를 줄이려면 소형화가 필수적입니다. 효율성은 훈련 및 추론 속도를 향상시킵니다. 적응성은 모델이 동적 영역에 더 집중하도록 합니다. 추가적으로, 구조물이 물체의 이웃하는 부분들이 유사한 동작을 갖는 경향과 같은 동적 장면들 [5, 24, 54]의 특정 사전들을 고려할 수 있다면 유익할 것이다.\n' +
      '\n' +
      '신경 복사 캐싱[39] 및 I-NGP[40]에서 영감을 받아 NTC로 얕은 완전 융합 MLP[38]와 결합된 다중 해상도 해시 인코딩을 사용한다. 특히, I-NGP에 이어 다중 해상도 복셀 그리드를 사용하여 장면을 표현한다. 각 해상도의 복셀 그리드는 \\(d\\)차원의 학습 가능한 특징 벡터를 저장하는 해시 테이블에 매핑된다. 주어진 3차원 위치 \\(x\\in\\mathbb{R}^{3}\\)에 대해, 해상도 \\(l\\)에서 해시 인코딩을 \\(h(x;l)\\in\\mathbb{R}^{d}\\)으로 표기하는 것은 주변 격자의 여덟 모서리에 해당하는 특징 벡터들의 선형 보간법이다. 결과적으로, 다중 해상도 해시 인코딩 \\(h(x)=[h(x;0),h(x;1),...,h(x;L-1)]\\in\\mathbb{R}^{Ld}\\, 여기서 \\(L\\)은 해상도 레벨의 수를 나타낸다. 다중 해상도 해시 인코딩은 NTC:**Compactness**: 해싱은 전체 장면을 인코딩하는 데 필요한 저장 공간을 효과적으로 줄입니다.\n' +
      '**효율**: 해시 테이블 룩업은 \\(O(1)\\)에서 작동하며, 현대 GPU와 호환성이 높습니다.\n' +
      '***적응성**: 해시 충돌은 더 미세한 해상도의 해시 테이블에서 발생하며, 더 큰 기울기를 갖는 영역-우리의 맥락에서 동적 영역을 나타냄-이 최적화를 구동할 수 있게 한다.\n' +
      '* **Priors**: 선형 보간법과 복셀-그리드 구조의 조합은 변환의 국부적인 평활성을 보장한다. 또한, 다중 해상도 접근법은 전역 및 지역 정보를 능숙하게 병합한다. 또한, 최소 오버헤드로 NTC의 성능을 향상시키기 위해 고도로 최적화된 얕은 완전 융합 MLP를 활용한다[38]. 이것은 해시 인코딩을 7차원 출력으로 매핑한다: 처음 3차원은 3DG의 번역을 나타내고, 나머지 차원은 쿼터니언을 사용하여 3DG의 회전을 나타낸다. MLP와 결합된 다중 해상도 해시 인코딩이 주어지면 NTC는 다음과 같이 공식화된다. \\[d\\mu,dq=MLP(h(\\mu)),\\](5 여기서 \\(\\mu\\)은 입력 3DG의 평균을 나타낸다. 우리는 \\(d\\mu\\)와 \\(dq\\)을 기반으로 3DG를 변환한다. 구체적으로, 변환된 3DG들의 다음의 파라미터들이 다음과 같이 주어진다:\n' +
      '\n' +
      '**Mean**: \\(\\mu^{\\prime}=\\mu+d\\mu\\), 여기서 \\(\\mu^{\\prime}\\)은 새로운 평균이고 \\(+\\)은 벡터 덧셈을 나타낸다.\n' +
      '**Rotation**: \\(q^{\\prime}=norm(q)\\times norm(dq)\\), 여기서 \\(q^{\\prime}\\)은 새로운 로테이션, \\(\\times\\)은 쿼터니언 곱셈, \\(norm\\)은 정규화를 나타낸다.\n' +
      '***SH 계수**: 3DG를 회전할 때 SH 계수도 3DG의 회전과 일치하도록 조정해야 한다. SH의 회전 불변성을 활용하여 SH를 업데이트하기 위해 SH 회전을 직접 사용한다. 자세한 내용은 보충자료(Suppl.)를 참고하시기 바랍니다.\n' +
      '\n' +
      '스테이지 1에서는 NTC에 의해 이전 프레임에서 3DG를 변환한 후 렌더링한다. NTC의 파라미터는 렌더링된 이미지와 그라운드 트루스 사이의 손실에 의해 최적화된다. 3DG-S [26] 다음에, 손실 함수는 D-SSIM 항과 결합된 \\(L_{1}\\)이다:\n' +
      '\n' +
      '\\[L=(1-\\lambda)L_{1}+\\lambdaL_{D-SSIM}, \\tag{6}\\]\n' +
      '\n' +
      '여기서 \\(\\lambda=0.2\\). 트레이닝 프로세스 동안, 이전 프레임으로부터의 3DG들은 동결된 상태로 유지되고 어떠한 업데이트도 겪지 않는다는 것에 유의해야 한다. 이는 NTC에 대한 입력이 일관성을 유지한다는 것을 의미한다.\n' +
      '\n' +
      '또한 훈련 안정성을 보장하기 위해 준비 매개변수로 NTC를 초기화한다. 상기 워밍업 동안 사용되는 손실은 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[L_{warm-up}=||d\\mu||_{1}-cos^{2}(norm(dq),Q), \\tag{7}\\]\n' +
      '\n' +
      '여기서 \\(Q\\)은 아이덴티티 쿼터니언이다. 첫 번째 항은 \\(L_{1}\\) 규범을 사용하여 추정된 병진동이 0에 근접하도록 하는 반면, 두 번째 항인 코사인 유사성은 추정된 회전이 회전이 없음을 보장한다. 그러나 단위 쿼터니언의 이중 커버링 특성을 고려하여 코사인 유사도의 제곱을 사용한다. 각 장면에 대해 초기 3DG의 노이즈 증강 수단을 입력으로 사용하여 타임스텝 0에서 훈련 후에만 워밍업을 실행한다. 3000번의 훈련 반복(대략 20초) 후에, 파라미터들이 저장되고 다음의 모든 타임스테프들에 대한 NTC들을 초기화하는 데 사용된다.\n' +
      '\n' +
      '##### : 적응형 3DG 추가\n' +
      '\n' +
      '3DG 변환에만 의존하는 것은 실제 동적 장면의 상당 부분을 적절하게 커버하며, 번역은 후속 타임스텝에서 폐색 및 소멸을 효과적으로 관리한다. 그러나 이 접근법은 화염이나 연기와 같은 일시적인 물체와 같은 초기 프레임에 존재하지 않는 물체와 병에서 쏟아지는 액체와 같은 새로운 지속성 물체와 마주할 때 필터링한다. 3DG는 구조화되지 않은 명시적 표현이기 때문에 이러한 새로운 객체를 모델링하기 위해 새로운 3DG를 추가하는 것이 필수적이다. 모델 저장 요구 사항 및 훈련 복잡성과 관련된 제약을 고려할 때, 시간이 지남에 따라 3DG가 축적되기 때문에 광범위한 수의 추가 3DG를 생성하거나 후속 프레임에서 사용할 수 없다. 따라서 이러한 새로운 객체를 정확하게 모델링하고 현재 타임스텝에서 장면의 완성도를 향상시키기 위해 제한된 수의 프레임별 3DG를 신속하게 생성하는 전략이 필요하다.\n' +
      '\n' +
      '첫째, 새로운 물체의 위치를 확인해야 합니다. 3DG-S[26]에서 영감을 얻은 3DG의 뷰 공간 위치 기울기를 주요 지표로 인식했다. 우리는 새로운 물체에 대해 근접한 3DG가 큰 뷰 공간 위치 기울기를 나타냄을 관찰했다. 이는 3DG를 변형시켜 새롭게 등장하는 \'가면화\'를 시도하는 최적화 덕분이다. 그러나 1단계에서 3DG의 색상이 업데이트되는 것을 방지하기 때문에 이 시도는 부족합니다. 그럼에도 불구하고, 그들은 여전히 큰 뷰 공간 위치 기울기와 함께 적절한 위치로 변환된다.\n' +
      '\n' +
      '앞서 언급한 관찰에 기초하여, 우리는 이러한 고계조 영역 주위에 추가 3DG를 도입하는 것이 적절하다고 생각한다. 또한 새로운 객체가 나타날 수 있는 모든 잠재적 위치를 철저히 포착하기 위해 **적응** 3DG 산란 전략을 채택한다. 구체적으로, 단계 1의 최종 훈련 시기 동안 시점 공간 위치 기울기를 추적한다. 이 단계가 끝나면, 시점 공간 위치 기울기의 평균 크기가 상대적으로 낮은 임계값 \\(\\tau_{grad}=0.00015\\)을 초과하는 3DG를 선택한다. 각각의 선택된 3DG에 대해, 추가 3DG의 위치는 \\(X\\sim\\mathcal{N}(\\mu,2\\Sigma)\\)에서 샘플링되며, 여기서 \\(\\mu\\) 및 \\(\\Sigma\\)은 선택된 3DG의 평균 및 공분산 행렬이다. 추가 3DG의 다른 속성에 대한 가정을 피하지만 SH 계수와 스케일링 벡터의 부적절한 초기화는 이러한 매개변수를 조정하는 것보다 불투명도를 줄이기 위한 최적화 선호도를 초래하는 경향이 있다. 이로 인해 추가 3DG가 빠르게 투명해져 떠오르는 물체를 포착하지 못하게 된다. 이 문제를 완화하기 위해, 이들 3DG의 SH 계수 및 스케일링 벡터는 선택된 것에서 유도되며, 회전은 동일성 쿼터니언 q = [1, 0, 0, 0]으로 설정되고 불투명도는 0.1에서 초기화된다. 산란 후, 3DG는 동일한 손실 함수(Eq)를 사용하여 최적화를 거친다. (6) 단계 1로서, 추가적인 3DG들의 파라미터들만이 최적화되는 반면, 변환된 3DG들의 파라미터들은 고정된 채로 유지된다는 것에 유의한다.\n' +
      '\n' +
      '지역 최소값을 보호하고 추가 3DG의 수를 관리하기 위해 **적응** 3DG 수량 제어 전략을 구현한다. 구체적으로, Stage 2에서는 불투명도 값에 대해 상대적으로 높은 임계값 \\(\\tau_{\\alpha}=0.01\\)을 설정하였다. 각 훈련시기가 끝날 때, 뷰 공간 위치 기울기가 \\(\\tau_{grad}\\)을 초과하는 3DG에 대해, 우리는 과소 재구성된 영역을 다루기 위해 근처에 추가 3DG를 산란한다. 이러한 추가 3DG는 원래 3DG로부터 회전 및 SH 계수를 상속하지만, 이들의 스케일링은 Kerbl 등이 기술한 \'분할\' 동작을 미러링하여 원래의 80%로 조정된다[26]. 그 후, 불투명도 값이 \\(\\tau_{\\alpha}\\) 이하인 추가 3DG를 폐기하여 3DG 양의 성장을 억제했다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '우리는 N3DV 데이터세트[31]와 Meet Room 데이터세트[29]의 두 가지 실제 동적 장면 데이터세트에 대한 실험을 수행한다.\n' +
      '\n' +
      '**N3DV 데이터셋**[31]은 21개 카메라의 다시점 시스템을 이용하여 촬영되며, 2704\\(\\times\\)2028 및 30 FPS의 해상도로 기록된 동적 장면을 포함한다. 이전 작업[9, 29, 31, 48, 57]에 이어 동영상을 2배 다운샘플링하고 [31]에서 제공한 훈련 및 검증 카메라 분할을 따른다.\n' +
      '\n' +
      '**Meet Room dataset**[29]은 13-카메라 멀티뷰 시스템을 사용하여 캡처되며, 1280\\(\\times\\)720 및 30 FPS의 해상도로 기록된 동적 장면을 포함한다. [29]에 이어 교육을 위해 13개의 뷰를 활용하고 테스트를 위해 1개를 예약했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l c c c|c} \\hline \\hline \\multirow{2}{*}{Category} & \\multirow{2}{*}{Method} & PSNR\\(\\uparrow\\) & Storage\\(\\downarrow\\) & Train\\(\\downarrow\\) & Render\\(\\uparrow\\) \\\\  & (dB) & (MB) & (mins) & (FPS) \\\\ \\hline \\multirow{3}{*}{Static} & Phenoxels [20] & 30.77 & 4106 & 23 & 8.3 & ✓ \\\\  & JNGP [40] & 28.62 & 48.2 & 1.3 & 2.9 & ✓ \\\\  & 3DG-S [26] & 32.08 & 47.1 & 8.3 & 390 & ✓ \\\\ \\hline \\multirow{3}{*}{Offline} & DyNeRF [31] & 29.58\\({}^{\\dagger}\\) & 0.1 & 260 & 0.02 & \\(\\times\\) \\\\  & NeRFPaper [51] & 30.69 & 17.1 & 1.2 & 0.05 & ✓ \\\\  & HasPlane [9] & 31.00 & 0.8 & 2.4 & 0.21 & \\(\\times\\) \\\\  & K-Planes [48] & 31.63 & 1.0 & 0.8 & 0.15 & \\(\\times\\) \\\\  & HyperRecel [11] & 31.10 & 1.2 & 1.8 & 2.00 & \\(\\times\\) \\\\  & MixVoxels [57] & 30.80 & 1.7 & 0.27 & 16.7 & \\(\\times\\) \\\\ \\hline \\multirow{2}{*}{Online} & StreamRF [29] & 30.68 & 17.7/31.4\\({}^{\\star}\\) & 0.25 & 8.3 & ✓ \\\\  & Ours & 31.67 & 7.67/8.4\\({}^{\\star}\\) & 0.20 & 215 & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: N3DV 데이터 세트의 **정량적 비교**. 각 장면에 대해 전체 300 프레임 동안 훈련 시간, 필요한 저장 및 PSNR을 평균화한다. \\ ({}^{\\dagger}\\)DyNeRF[31] only report metrics on the _flame salmon_ scene. \\\\ 초기 모델을 고려할 때 ({}^{\\star}\\)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline \\multirow{2}{*}{Method} & PSNR\\(\\uparrow\\) & Storage\\(\\downarrow\\) & Train\\(\\downarrow\\) & Render\\(\\uparrow\\) \\\\  & (dB) & (MB) & (mins) & (FPS) \\\\ \\hline \\multirow{2}{*}{Plenoxels [20]} & 27.15 & 1015 & 14 & 10 \\\\  & I-NGP [40] & 28.10 & 48.2 & 1.1 & 4.1 \\\\  & 3DG-S [26] & 31.31 & 21.1 & 2.6 & 571 \\\\ \\hline \\multirow{2}{*}{StreamRF [29]} & 26.72 & 5.7/9.0\\({}^{\\star}\\) & 0.17 & 10 \\\\ \\cline{2-5}  & 30.79 & 4.0/4.1\\({}^{\\star}\\) & 0.10 & 288 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 회의실 데이터 세트의 **정량적 비교**. 트레이닝 시간, 필요한 저장 및 PSNR은 전체 300 프레임에 걸쳐 평균화된다는 점에 유의한다. \\ 초기 모델을 고려할 때 ({}^{\\star}\\)\n' +
      '\n' +
      '도 4: Meet Room 데이터셋의 _discussion_ 장면과 N3DV 데이터셋의 _sear streak_ 장면에 대한 **Qualitative 비교**.\n' +
      '\n' +
      '### Implementation\n' +
      '\n' +
      '3D Gaussian Splitting (3DG-S) 코드에서 3DGStream을 구현하고, tiny-cuda-nn (38)을 사용하여 신경망 변환 캐쉬(NTC)를 구현한다. 초기 3DG의 학습을 위해 3DG-S의 기본 설정을 기반으로 N3DV 데이터 세트의 학습 속도를 미세 조정하고 이를 Meet Room 데이터 세트에 적용한다. 모든 장면에 대해, 단계 1에서 150번의 반복에 대해 NTC를 훈련하고, 단계 2에서 100번의 반복에 대해 추가 3DG를 훈련한다. Suppl을 참조하라. 더 자세한 내용을 위해.\n' +
      '\n' +
      '### Comparisons\n' +
      '\n' +
      '정량적 비교 우리의 정량적 분석은 N3DV 데이터세트 및 Meet Room 데이터세트에 대한 3DGStream을 벤치마킹하여 다양한 대표적인 방법과 비교하는 것을 포함한다. 우리는 플래녹셀[20], I-NGP[40], 3DG-S[26]을 빠른 정적 장면 재구성 방법의 대표자로 사용하여 각 프레임에 대해 처음부터 교육한다. 스트림RF[29], 동적3DG[34], 및 ReRF[60]은 동적 장면에서의 온라인 트레이닝을 위해 설계된다. 전경 마스크가 필요하고 객체가 적은 장면에 국한되는 Dynamic3DG 및 ReRF의 한계와 프레임당 미세 수준의 훈련 시간으로 인해 N3DV 및 MeetRoom 데이터 세트에 대한 적응성과 훈련 가능성으로 인해 온라인 훈련 방법의 대표자로 선택된 StreamRF를 선택한다. 3DGStream의 경쟁력 있는 영상 품질을 입증하기 위해 DyN-eRF[31], NeRFplayer[51], HexPlane[9], K-Planes[48], HyperRecel[1], MixVoxels[57]의 각 논문에서 N3DV 데이터 세트에 대해 보고된 정량적 결과와 비교를 도출하였으며, 이는 모두 전체 비디오 시퀀스에 대한 오프라인 훈련을 통해 동적 장면을 재구성하는 방법이다.\n' +
      '\n' +
      '탭에서 도 1을 참조하면, N3DV 데이터 셋의 모든 장면들에 대한 평균 렌더링 속도, 트레이닝 시간, 필요한 저장 및 피크 신호 대 잡음비(peak signal-to-noise ratio, PSNR)를 제시한다. 각 장면에 대해 후자의 세 가지 메트릭은 전체 300 프레임에 대한 평균으로 계산된다. 또한, 우리는 Suppl의 N3DV 데이터 세트 내의 모든 장면에 대한 비교 분석을 제공한다. 본 방법의 일반성을 입증하기 위해 스트림RF[29]에 의해 도입된 MeetRoom 데이터 세트에 대한 실험을 수행하고 Plenoxels[20], I-NGP[40], 3DG-S[26], StreamRF[29]에 대해 정량적 비교를 수행했다. 결과는 탭에 나와 있다. 2. 탭에 나와 있다. 도 1 및 도 2를 참조하면, 본 방법은 빠른 온라인 훈련과 실시간 렌더링을 통해 우수성을 입증한다\n' +
      '\n' +
      '그림 5: **3DG의 변환을 모델링하기 위한 다양한 접근법의 비교.** 동일한 초기 3DG를 사용하여 _플레임 연어_ 비디오의 두 번째 프레임에서 수행된다.\n' +
      '\n' +
      '그림 6: ** _flame salmon_ scene에 대한 다른 접근법의 비교.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c} \\hline \\hline Variant & PSNR\\(\\uparrow\\) (dB) & \\#Additional 3DGs\\(\\downarrow\\) \\\\ \\hline Baseline & 28.39 & **0** \\\\ Rnd. Spawn & 28.39 & 971.9 \\\\ _w/o_ Quant. Ctrl. & 28.43 & 8710.8 \\\\ Full Model & 28.42 & 477.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: _flame salmon_ scene**에 대한 Adaptive 3DG Addition strategy의 **Ablation study. 메트릭은 전체 시퀀스에 걸쳐 평균화된다.\n' +
      '\n' +
      '그림 7: _flame stack_ scene과 _coffee martini_ scene에 대해 수행된 ablation study**의 **Quantitative results.\n' +
      '\n' +
      '모델 스토리지 및 이미지 품질 측면에서 경쟁 우위를 동시에 유지합니다. 또한, FVV를 스트리밍할 수 있는 방법 중 본 모델은 최소한의 모델 저장이 필요하다.\n' +
      '\n' +
      '정성적 비교.___ 우리의 접근법은 탭에 설명된 대로 주로 온라인 FVV 구축의 효율성을 향상시키는 것을 목표로 한다. 도 1 및 도 2를 참조하면, 여전히 경쟁력 있는 화질을 달성한다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 4를 참조하여, N3DV 데이터세트 [31] 및 Meet Room 데이터세트 [29]의 장면에 걸쳐 I-NGP [40], 하이퍼릴 [1], StreamRF [29]와 정성적 비교를 제시하며, 얼굴, 손, 집게와 같은 동적 객체뿐만 아니라 레이블 및 조각상과 같은 복잡한 객체를 특히 강조한다. 우리의 방법은 복잡한 물체를 재구성하는 능력을 희생하지 않고 장면의 역학을 충실하게 포착한다는 것은 분명하다. 자세한 비디오 결과는 프로젝트 페이지를 참조하십시오.\n' +
      '\n' +
      '### Evaluations\n' +
      '\n' +
      'Cache.___Neural Transformation Cache. NTC의 효과를 보여주기 위해 N3DV 데이터 세트의 _플레임 연어_ 비디오 내에서 첫 번째 프레임에서 두 번째 프레임까지 3DG의 변환을 모델링하기 위해 별개의 접근법을 사용한다. 도. 도 5는 다중 해상도 해시 인코딩(_w/o_Hash enc.) 없이, MLP가 변환을 효과적으로 모델링하는 데 어려움에 직면함을 보여준다. 추가적으로, 웜업(_w/o_ Warm-up) 없이, 수렴을 위해 더 많은 반복이 필요하다. 또한, 이전 프레임의 3DG(Direct Opt)의 직접 최적화와 비교하더라도 NTC는 온-파 성능을 보여준다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 6을 참조하면, 첫 번째 프레임(_i.e_., Frame 0)을 제외한 전체 _flame salmon_video에 걸쳐 적용된 서로 다른 접근 방식의 결과를 제시한다. _ w/o_Hash enc. 그리고 _w/o_ Warm-up. 빠르게 수렴할 수 없어 시퀀스가 진행됨에 따라 오류가 누적됩니다. 다이렉트 옵트 가장 좋은 결과를 얻지만 부풀려진 보관 비용을 지불해야 합니다. 대조적으로 NTC를 활용하면 모든 3DG를 절약할 필요가 없어 스토리지 오버헤드가 상당히 낮아 유사한 결과를 얻을 수 있다.\n' +
      '\n' +
      '_Adaptive 3DG Addition.___Adaptive 3DG Addition.___ 탭 도 3은 _flame salmon_ 장면에 대해 수행된 절제 연구의 정량적 결과를 제시하고 있으며, 더 많은 결과는 Suppl에 제시되어 있다. 단계 2가 없는 기본 모델과 무작위로 산란된 3DG(Rnd. Spawn)의 집합 종균 전략과 동등한 양으로 둘 다 신흥 개체를 포착하지 못한다. 수량 제어 전략이 없는 변형(_w/o_Quant. Ctrl) 새로운 객체를 모델링하기 위한 관리이지만 훨씬 더 많은 수의 추가 3DG가 필요하다. 대조적으로, 우리의 전체 모델은 3DG의 최소 추가를 사용하여 신흥 객체를 능숙하게 재구성한다. 절제 연구는 그림 1에 나와 있다. 도 7은 적응형 3DG 추가 전략의 효과를 질적으로 보여주며, 냄비에 있는 커피, 개의 혀, 화염과 같이 초기 프레임에 존재하지 않는 물체를 재구성하는 능력을 강조한다.\n' +
      '\n' +
      '_Real-time Rendering. 3DG-S[26]에 이어서, 렌더링 속도를 측정하기 위해 SIBR 프레임워크[6]을 사용한다. 필요한 모든 리소스가 GPU에 로드되면 접근법의 추가 오버헤드는 주로 NTC를 쿼리하고 3DG를 변환하는 데 걸리는 시간이다. 탭에 자세히 나와 있습니다. 도 4를 참조하면, 본 논문에서 제안하는 방법은 고속 NTC 질의를 용이하게 하는 다중 해상도 해시 인코딩과 완전 융합된 MLP[38]의 효율성을 제공한다. 특히, 가장 시간이 많이 걸리는 단계는 SH 회전이다. 그러나 SH 회전은 재구성 품질에 최소한의 영향을 미치며, 이는 SH 계수가 아닌 대체 메커니즘(예: 객체를 둘러싼 다양한 색상의 작은 3DG)을 통해 뷰 의존적 색상을 모델링하는 3DG에 기인할 수 있다. 그럼에도 불구하고, 우리는 이론적 건전성을 위해 SH 회전을 유지한다.\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      '초기 프레임에서 3DG-S[26]의 품질은 3DGStream에 매우 중요하다. 따라서 초기 포인트 클라우드에 대한 의존도가 높은 등 3DG-S의 한계를 계승한다. 도 1에 도시된 바와 같다. 7, COLMAP의 [49] 먼 풍경을 재구성할 수 없기 때문에 창 너머에 명백한 인공물이 있다. 따라서 우리의 방법은 향후 3DG-S로의 향상으로 직접적인 이점을 얻을 수 있다. 또한, 효율적인 현장 훈련을 위해, 1단계에서 과감한 움직임의 모델링과 2단계에서 복잡한 부상 물체의 모델링을 제한하는 훈련 반복 횟수를 제한한다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '본 논문에서는 효율적인 Free-Viewpoint 비디오 스트리밍을 위한 새로운 방법인 3DGStream을 제안한다. 3DG-S [26]을 기반으로 객체의 움직임을 포착하기 위해 효과적인 신경망 변환 캐시를 사용한다. 또한, 동적 장면에서의 새로운 객체들을 정확하게 모델링하기 위한 Adaptive 3DG Addition 전략을 제안한다. 3DGStream의 2단계 파이프라인은 비디오 스트림에서 동적 장면들의 온더 플라이 재구성을 가능하게 한다. 3DGStream은 광실감적인 화질을 보장하면서도, 중간 정도의 저장이 필요한 메가픽셀 해상도에서 온더플라이 트레이닝(\\(\\sim\\)10s per-frame)과 실시간 렌더링(\\(\\sim\\)200FPS)을 달성한다. 광범위한 실험을 통해 3DGStream의 효율과 효율성을 입증한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c} \\hline \\hline Step & Overhead (ms) & FPS \\\\ \\hline Render _w/o_ NTC & 2.56 & 390 \\\\ + Query NTC & 0.62 & \\\\ + Transformation & 0.02 & \\\\ + SH Rotation & 1.46 & \\\\ \\hline Total & 4.66 & 215 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 메가픽셀 해상도에서 _flame salmon_ 장면에 대한 **렌더링 프로파일링**. _flame salmon_은 우리 실험에서 모든 장면을 렌더링하는 데 가장 시간이 많이 소요된다는 점에 유의해야 한다.\n' +
      '\n' +
      '## 8 Acknowledgement\n' +
      '\n' +
      '이 작업은 저장성 프로그램(2022C01222, 2023C03199, 2023C03201), 중국 국가 프로그램(62172365, 2021YFF0900604, 19ZDA197), 닝보 과학 기술 계획 프로젝트(022Z167, 2023Z137), 뇌 과학 및 뇌-기계 통합을 위한 MOE 프론티어 과학 센터(저장 대학)가 부분적으로 지원했다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew O\'Toole, and Changil Kim. Hyperreel: High-fidelity 6-dof video with ray-conditioned sampling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16610-16620, 2023.\n' +
      '* [2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5855-5864, 2021.\n' +
      '* [3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5470-5479, 2022.\n' +
      '* [4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. _ICCV_, 2023.\n' +
      '* [5] Michael J Black and Paul Anandan. The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields. _Computer vision and image understanding_, 63(1):75-104, 1996.\n' +
      '* [6] Sebastien Bonopera, Jerome Esnault, Siddhant Prakash, Simon Rodriguez, Theo Thonat, Mehdi Benadel, Gaurav Chaurasia, Julien Philip, and George Drettakis. sibr: A system for image based rendering, 2020.\n' +
      '* [7] Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erickson, Peter Hedman, Matthew Duvall, Jason Dourgarian, Jay Busch, Matt Whalen, and Paul Debevec. Immersive light field video with a layered mesh representation. _ACM Transactions on Graphics (TOG)_, 39(4):86-1, 2020.\n' +
      '* [8] Chris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen. Unstructured lumigraph rendering. In _SIGGRAPH_, pages 425-432, 2001.\n' +
      '* [9] Ang Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. _CVPR_, 2023.\n' +
      '* [10] Jin-Xiang Chai, Xin Tong, Shing-Chow Chan, and Heung-Yeung Shum. Plenoptic sampling. In _Proceedings of the 27th annual conference on Computer graphics and interactive techniques_, pages 307-318, 2000.\n' +
      '* [11] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 14124-14133, 2021.\n' +
      '* [12] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In _European Conference on Computer Vision (ECCV)_, 2022.\n' +
      '* [13] Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su, and Andreas Geiger. Dictionary fields: Learning a neural basis decomposition. _ACM Trans. Graph._, 2023.\n' +
      '* [14] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In _The Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [15] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Eveseev, David Calabrese, Hugues Hoppe, Adam Kirk, and Steve Sullivan. High-quality streamable free-viewpoint video. _ACM Transactions on Graphics (TOG)_, 34(4):69, 2015.\n' +
      '* [16] Abe Davis, Marc Levoy, and Fredo Durand. Unstructured light fields. _Comput. Graph. Forum_, 31(2pt1):305-314, 2012.\n' +
      '* [17] Mingsong Dou, Philip Davidson, Sean Ryan Fanello, Sameh Khamis, Adarsh Kowdle, Christoph Rhemann, Vladimir Tankovich, and Shahram Izadi. Motion2fusion: Real-time volumetric performance capture. _ACM Trans. Graph._, 36(6):246:1-246:16, 2017.\n' +
      '* [18] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, and Baoquan Chen. 4d gaussian splatting: Towards efficient novel view synthesis for dynamic scenes. _arXiv preprint arXiv:2402.03307_, 2024.\n' +
      '* [19] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias Niessner, and Qi Tian. Fast dynamic radiance fields with time-aware neural voxels. In _SIGGRAPH Asia 2022 Conference Papers_, 2022.\n' +
      '* [20] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5501-5510, 2022.\n' +
      '* [21] Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 14346-14355, 2021.\n' +
      '* [22] Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. The lumigraph. In _Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques_, page 43-54, New York, NY, USA, 1996. Association for Computing Machinery.\n' +
      '* [23] Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. In _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 5855-5864, 2021.\n' +
      '* [24] Berthold KP Horn and Brian G Schunck. Determining optical flow. _Artificial intelligence_, 17(1-3):185-203, 1981.\n' +
      '* [25] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao, Xiao Liu, and Yuewen Ma. Tri-miprf: Tri-mip representationfor efficient anti-aliasing neural radiance fields. In ICCV, Cited by: SS2.\n' +
      '*[26]B. 커블, G. 코파나, T. Leimkuhler, and G. Drettakis (2023) 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics42 (4). 인용: SS2.\n' +
      '*[27]T. Kirschstein 건성 기벤하인, T 월터, M. Niessner (2023) Nersemble: 인간 머리들의 멀티뷰 복사 필드 재구성. ArXiv:2305.03027. 인용: SS2.\n' +
      '*[28]M. Levoy and P. Hanrahan (1996) Light field rendering. In SIGGRAPH, pp. 31-42. Cited by: SS2.\n' +
      '*[29]L. 이종욱 신진 왕락 Shen, and P. Tan (2022) Streaming radiance field for 3d video synthesis. NeurIPS에서 인용: SS2.\n' +
      '*[30]R. 이종완 보민 Zollhofer, J. Gall, A. Kanazawa, and C. Lassner (2022) Tava: 템플릿이 없는 애니메이션 볼륨 배우들. 유럽 Conference on Computer Vision, pp. 419-436. Cited by: SS2.\n' +
      '*[31]T. 이명 슬라베바 S. 졸호퍼 그린씨래스너씨 김태원 슈미트 러브그로브 고세일 Newcombe, et al.(2022) Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5521-5531. Cited by: SS2.\n' +
      '*[32]Z. 이성 니클라우스 순진하고, O. Wang (2021) Neural scene flow fields for space-time view synthesis of dynamic scene. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6498-6508. Cited by: SS2.\n' +
      '*[33]Z. Lee G. Wang F. Cole, R 터커, N. Snavely(2023) Dynibar: neural dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS2.\n' +
      '*[34]J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan(2024) Dynamic 3d Gaussians: Tracking by persistent dynamic view synthesis. 3DV에서 인용: SS2.\n' +
      '*[35]R. N. 마틴-브루엘라 Radwan, M. S. M. Sajjadi, J. T. Barron, A. Dosovitskiy, and D. Duckworth(2021) NeRF in the Wild: neural radiance fields for nonstrained photo collection. CVPR에서 인용됨: SS2.\n' +
      '*[36]B. 밀덴홀, P. P. 스리니바산, M. Tancik, J. T. Barron, R 라마모오르티, R. Ng(2020) Nerf: 장면들을 뷰 합성을 위한 신경 래디언스 필드들로서 표현하는 것. 유럽 컴퓨터 비전 컨퍼런스에서, pp. 405-421. 인용: SS2.\n' +
      '*[37]B. 밀덴홀, P. 헤드먼, R. Martin-Brualla, P. P. Srinivasan, and J. T. Barron (2022) Nerf in the dark: high dynamic range view synthesis from noisy raw image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16190-16199. Cited by: SS2.\n' +
      '*[38]B. 밀덴홀, P. 헤드먼, R. Martin-Brualla, P. P. Srinivasan, and J. T. Barron (2022) Nerf in the dark: high dynamic range view synthesis from noisy raw image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16190-16199. Cited by: SS2.\n' +
      '*[39]T. Muller, F. Rousselle, J. Novak, and A. Keller (2021) Real-time neural radiance caching for path tracing. ACM Transactions on Graphics (TOG)40 (4), pp. 1-16. Cited by: SS2.\n' +
      '*[40]T. Muller, A. Evans, C. Schied, and A. Keller (2022) Instant neural graphics primitives with multiresolution hash encoding. ACM Trans. Graph.41(4), pp. 102:1-102:15. Cited by: SS2.\n' +
      '*[41]M. Niemeyer, J. T. Barron, B. Mildenhall, M. 사자디, A. 가이거, N. Radwan(2022) Regnerf: 희소 입력으로부터 뷰 합성을 위해 신경 복사 필드를 정규화한다. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5480-5490. Cited by: SS2.\n' +
      '*[42]K. 박욱 신하정배론 Bouaziz, D. B. Goldman, S. M. Seitz, and R. Martin-Brualla (2021) Nerfies: 변형 가능한 신경 복사 필드. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 5865-5874. Cited by: SS2.\n' +
      '*[43]K. 박욱 신하정배론 Bouaziz, D. B. Goldman, S. M. Seitz, and R. Martin-Brualla (2021) Nerfies: 변형 가능한 신경 복사 필드. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5865-5874. Cited by: SS2.\n' +
      '*[44]K. 박욱 신하정배론 Bouaziz, D. B. Goldman, S. M. Seitz, and R. Martin-Brualla (2021) Nerfies: 변형 가능한 신경 복사 필드. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5865-5874. Cited by: SS2.\n' +
      '*[45]K. 박욱 신하정배론 Bouaziz, D. B. Goldman, S. M. Seitz, and R. Martin-Brualla (2021) Nerfies: 변형 가능한 신경 복사 필드. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5865-5874. Cited by: SS2.\n' +
      '*[46]K. 박욱 신하 P. Hedman, J. T. Barron, S. 부아지즈, D. B. 골드만, R. Martin-Brualla, and S. M. Seitz (2021) Hypernerf: a higher-dimensional representation for topologically varying neural radiance fields. ACM Trans. Graph.40 (6). 인용: SS2.\n' +
      '*[47]S. 박민 손승 장영철 강(2023) 시간 보간은 동적 신경 복사 필드를 위해 필요한 전부이다. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4212-4221. Cited by: SS2.\n' +
      '*[48]A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer(2021) D-nerf: neural radiance fields for dynamic scene. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10318-10327. Cited by: SS2.\n' +
      '*[49]C. 라이저 Szeliski, D. Verbin, P. Srinivasan, B. Mildenhall, A. Geiger, J. Barron, and P. Hedman (2023) Merf: unbounded scene에서 실시간 뷰 합성을 위한 메모리 효율적인 래디언스 필드. ACM Transactions on Graphics (TOG)42 (4), pp. 1-12. Cited by: SS2.\n' +
      '*[50]S. Fridovich-Keil and G. Meanti (2022) Frederik Rahbaek Warburg, Benjamin Recht, and A. Kanazawa. K-평면: 공간, 시간 및 외관의 명시적인 복사 필드입니다. CVPR에서 인용됨: SS2.\n' +
      '*[51]J. R. Schonberger and J. Frahm(2016) Structure-from-motion을 재방문하였다. 컴퓨터 비전 및 패턴 인식 회의(CVPR)에서 인용: SS2.\n' +
      '*[52]H. 쉔과 L. 그는 (1999) 동심원 모자이크로 렌더링한다. In Proceedings of the 26th annual conference on Computer graphics and interactive techniques, pp. 299-306. Cited by: SS2.\n' +
      '*[53]L. 송아천 이종욱 천락 천진원 Xu, 및 A. Geiger (2021) Nerf\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '* [75] C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele, Simon Winder, and Richard Szeliski. High-quality video view interpolation using a layered representation. _ACM transactions on graphics (TOG)_, 23(3):600-608, 2004.\n' +
      '* [76] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In _Proceedings Visualization, 2001. VIS\'01._, pages 29-538. IEEE, 2001.\n' +
      '\n' +
      '* [001] Thank you to all reviewers for your insightful feedback. Below please find the responses to your specific comments\n' +
      '* [002] **(Fmyh) 1 (a)** Yes. **(b)** The ablation studies in Figs. 6 and 7 show NTC\'s effectiveness. The ability of NTC to model transformation is on par with Direct Opt., a variant akin to Baseline A but optimizing all attributes. The "Storage" entry in Tabs. 1 and 2 of our method closely matches NTC\'s storage cost, as it occupies a large share according to Tab. 5. Meanwhile, the storage cost of Baseline A, equivalent to 3DGs with only means and rotations, is nearly \\(\\frac{T}{23}\\) that of full 3DGs and higher than NTC, as can be calculated from Tabs. 1 and 2. For Baseline B, Figs. 6 and 7 show that a pure MLP struggles to model transformation. **(c)** "Noise-augmented means" comprises initial means, noise-perturbed ones, and random samples. Since initial 3DGs move over time, using only initial means for warm-up is insufficient for NTC to meet starting conditions (outputting near-zero translations and rotations close to the identity quaternion, as described in L323-L326) in subsequent timesteps. Thus, We adopt this design to enhance NTC\'s robustness.\n' +
      '*[0.5] **(Fmyh)2(a,b)** 각 프레임에는 두 가지 유형의 3DG가 필요하다. 부가는 이전 프레임과 무관하며, 후속 프레임(L063-L066, L243-L244, L343-L350)에 사용되지 않으며, 이는 시간이 지남에 따라 축적되지 않는다는 것을 의미한다. 따라서, 이전 프레임에 비해, 3DG의 수는 프레임별 추가에 따라 증가하거나 감소할 수 있다. 추가 3DG는 탭에서 볼 수 있듯이 프레임당 총 저장 비용과 비교할 때 상대적으로 작은 저장 비용과 작은 저장 변동으로 인해 데이터 IO에 스트레스를 주지 않는다. 3, 탭. 도 5 및 도 6을 참조하여 설명한다. 도 9에 도시된 바와 같이, 3DG들을 렌더링하는 것은 가시성을 인식하기 때문에, 명시적인 3DG 모션은 최적화 동안 보이지 않는 장소들로 이동함으로써 카메라의 층 외부의 이동 또는 폐색으로 인한 물체의 소멸을 처리할 수 있다. 최적화 과정에서 처리할 수 있는 사라지는 객체와 달리 새로운 객체에는 모델링을 위해 추가 3DG가 필요하기 때문에 3DG를 추가하는 데 중점을 둔다. **(c)** 온라인 교육에서 후속 프레임 정보를 요구하지 않는 모든 방법이 실행 가능한 대안이 될 수 있다고 생각한다. **(d)** 제안 감사합니다. 우리는 수정된 버전으로 시각화를 제공할 것입니다. **(e)** 우리의 의도는 하이퍼파라미터들이 실험적으로 결정된, 기존의 것들과 가까운 적절한 위치들에서 새로운 3DG들을 생성하는 것이다.\n' +
      '*[0.5] **(Fmyh) 3** 제안 감사합니다. 우리는 수용 시 시청자 친화적인 비디오 결과를 추가로 공개할 것이다.\n' +
      '*[0.5] **(Fmyh)4(a)** No. 표현은: 초기 3DG들, 프레임별 NTC들, 및 프레임별 **프레임-특정** 추가 3DG들을 포함하며, 이들은 현재 프레임에 대해 **배타적으로** 채용된다. **(b)** NTC의 스토리지 비용은 프레임당 일관되며, 추가 3DG를 위한 스토리지만 변경되며, 이는 탭에서 볼 수 있듯이 최소이다. 도 5 및 그림 9. 탭에 "추가 3DG" 항목이 나와 있다. 3은 그들의 수량을 나타낸다. **(c)** 아니요, 2(a,b)에서 논의된 바와 같이, 변환 3DG의 크고 고정된 수에 비해 추가적인 3DG의 수가 무시할 수 있기 때문에 훈련/렌더링 속도에 영향을 미치지 않을 것이다. **(Fmyh) 5, 6** 제안 감사합니다. 탭을 업데이트하겠습니다. 4 및 개정판에 관련 연구를 추가한다. 제안해 주셔서 감사합니다. 우리는 개정판에 동시 작업에 대한 논의를 통합할 것이다. **(p5r1) Effectiveness.** Tab. 2 in the Supp. 매트 다른 장면에 대한 절제 연구를 보여주며, 이를 통해 탭을 도출할 수 있다. 도 3(본문)은 실제로 모든 장면 중 가장 작은 PSNR 이득을 보여준다. 탭 3가지 특징은 N3DV 데이터 세트에서 자주 평가되는 장면이기 때문에 _flame salmon_에 대한 결과이며 "추가 3DGs" 항목은 평균화에 장면에 따라 부적합한 **(p6r)**이다. 이 단계의 효과는 PSNR 이득보다 새로운 객체를 모델링하는 데 더 잘 반영된다. 도. 9 8은 정밀한 신흥 객체 **(p6r)** 모델링을 통해 향상된 비주얼을 강조합니다. 탭을 업데이트하겠습니다. 수정된 버전 3. **(p5r1) 비교.** L181-L184 및 L438-L442에서 언급한 바와 같이, Dynamic3DG 및 ReRF는 몇 가지 이유로 비교에서 제외된다: (1) N3DV 및 MeetRoom 데이터 세트에 대한 결과를 보고하지 않으며 공개된 **(p6r)** 코드는 이러한 데이터 세트에서 쉽게 실행되지 않는다. (2) 우리는 복잡한 실제 장면에서 FVV를 구성하는 것을 목표로 하는 반면, **(p6r)** 두 방법 모두 객체와 전경 마스크가 거의 없는 합성 또는 틈새 데이터 세트를 배치하며 이러한 데이터 세트는 접근법의 적용 가능성을 보여주지 못한다. (3) 각각의 데이터 세트에서도, 둘 다 프레임당 미세 수준의 트레이닝 시간이 소요되어 효율성의 결여를 나타낸다. 대조적으로, 본 방법은 도전적인 실제 데이터 세트에 대해 2단계 프레임당 훈련을 달성한다. 이러한 요소를 감안할 때, 이는 우리의 직접적인 경쟁자가 아니며, StreamRF 및 우리와 동일한 프레임당 훈련되고 스트리밍 가능한 표현 범주에 속하기 때문에 섹션 2에서 논의된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>