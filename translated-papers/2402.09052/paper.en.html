<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects\n' +
      '\n' +
      'Yutaro Yamada\n' +
      '\n' +
      'Khyathi Chandu\n' +
      '\n' +
      'Yuchen Lin\n' +
      '\n' +
      'Jack Hessel\n' +
      '\n' +
      'Ilker Yildirim\n' +
      '\n' +
      'Yejin Choi\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Diffusion-based image generation models such as DALL-E 3 and Stable Diffusion-XL demonstrate remarkable capabilities in generating images with realistic and unique compositions. Yet, these models are not robust in precisely reasoning about physical and spatial configurations of objects, especially when instructed with unconventional, thereby out-of-distribution descriptions, such as _"a chair with five legs"_. In this paper, we propose a language agent with chain-of-3D-thoughts (L3GO), an inference-time approach that can reason about part-based 3D mesh generation of unconventional objects that current data-driven diffusion models struggle with. More concretely, we use large language models as agents to compose a desired object via trial-and-error within the 3D simulation environment. To facilitate our investigation, we develop a new benchmark, **Unconventionally Feasible Objects (UFO)**, as well as SimpleBlenv, a wrapper environment built on top of Blender1 where language agents can build and compose atomic building blocks via API calls. Human and automatic GPT-4V evaluations show that our approach surpasses the standard GPT-4 and other language agents (e.g., ReAct and Reflexion) for 3D mesh generation on ShapeNet. Moreover, when tested on our UFO benchmark, our approach outperforms other state-of-the-art text-to-2D image and text-to-3D models based on human evaluation.\n' +
      '\n' +
      'Footnote 1: [https://www.blender.org/](https://www.blender.org/)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'AI applications that generate 2D images (Betker et al.; Saharia et al., 2022; Podell et al., 2023) and 3D models (Jun and Nichol, 2023; Lin et al., 2023b) from text instructions have opened up significant possibilities for creators. However, these tools lack precise output controls as they often produce unexpected or _"hallucinatory"_ results (Saharia et al., 2022) not loyal to the input prompts. Additionally, early versions of Stable Diffusion (Rombach et al., 2022) had difficulty in combining multiple concepts in one image or would mix up different attributes. Previous efforts have improved performance on object-attribute attachment, missing objects, etc. by steering attention layers (Feng et al., 2022; Chefer et al., 2023; Rassin et al., 2023), or, by training larger models with detailed captions on a vast scale (StableDiffusion-XL (SDXL) (Podell et al., 2023) and DALL-E-3 (Betker et al.).) However, even the most performant diffusion model, DALL-E 3, still fails to generate objects that require precise 3D spatial understanding like "a chair with five legs" (Figure 1). This difficulty persists even after repeated attempts to adjust DALL-E-3\'s outputs with human feedback directly, e.g., "The chair you generated has seven legs. Please make a chair with exactly five legs."\n' +
      '\n' +
      'We posit that the sophisticated text-based reasoning abilities inherent in LLMs can compensate for shortcomings in the 3D spatial comprehension of text-to-2D image and text-to-3D models. We present L3GO, an inference agent capable\n' +
      '\n' +
      'Figure 1: We compare one of the state-of-the-art text-to-image models (DALL-E 3) with our LLM-based approach (L3GO). We perform five iterations of DALL-E 3 generation with human feedback but DALL-E 3 does not strictly follow the prompt. L3GO creates a chair with the correct number of legs.\n' +
      '\n' +
      'of iteratively soliciting feedback from LLMs to integrate corrections and enhance the precision of rendering a 3D mesh used as a skeleton to generate 2D image.\n' +
      '\n' +
      'We conduct our experiments within Blender --a widely acclaimed 3D modeling software. We create and release an environment called SimpleBlenv, based on Blender, to systematically evaluate text-to-3D mesh generation performance of LLM agents. State of the art LLMs such as GPT-4 (Bubeck et al., 2023), despite being trained on only text has decent spatial reasoning capabilities. Figure 2 shows mixed results when GPT-4 is prompted to write a Python script that can run in Blender to create 3D meshes of basic objects solely based on the object name. On the one hand, text-only GPT-4 demonstrates surprising proficiency in creating simple 3D objects like lamps (2a) comprising of three basic shapes. However, as object complexity increases to more than four parts (ex: four legs of a table) or complex objects like airplane (2b, 2c), GPT4\'s success in perfectly assembling them is limited.\n' +
      '\n' +
      'Our L3GO agent bridges these gaps by breaking down the constructing complex objects by employing a more structured, part-by-part approach into: (1) identifying relevant parts specifications and critiquing them, (2) identifying spatial specifications and placement, (3) running the current action to critique the spatial placement and completion. This setup iteratively seeks feedback from SimpleBlenv and the specifications and critiques are generated from LLM. Finally, we render the mesh into an image, and feed it into ControlNet (Zhang et al., 2023) with Canny edge detection (Canny, 1986) to generate a textured and more natural looking image. We conduct human evaluations to compare the performance of LLM-based mesh creation using 13 popular object categories from ShapeNet. L3GO outperforms basic GPT-4, ReAct-B, and Reflexion-B according to both human and auto evaluation. We also show that mesh quality evaluation using GPT-4V (OpenAI, 2023) yields a metric with high correlation to human judgement. Finally, we introduce Unconventionally Feasible Objects, named UFO with unconventional yet feasible objects. We show that L3GO surpasses current state-of-the-art text-to-2D image and text-to-3D mesh models on UFO. Collectively, our findings indicate the promising role of integrating language agents in diffusion model pipelines, particularly for constructing objects with specific attribute requirements in the future applications of generative AI.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Spatial Understanding of Language ModelsNumerous studies have delved into the spatial comprehension capabilities of language models. Janner et al. (2018) explored the spatial reasoning of LSTM (Hochreiter and Schmidhuber, 1997) in a simulated environment with agent actions and rewards, though their 2D grid world environment is notably simpler than the 3D modeling context considered in our work. Abdou et al. (2021) and Patel and Pavlick (2022) demonstrated that language models develop internal representations for directional information. Additionally, (Mirzaee et al., 2021) introduced a question-answering benchmark for spatial reasoning based on language description. It is reasonable to assert that LLMs exhibit spatial reasoning capabilities and can be effectively prompted to offer feedback on spatial constructions.\n' +
      '\n' +
      'Large Language Models as AgentsLLM agents (Ge et al., 2023; Park et al., 2023; Shen et al., 2023; Gupta and Kembhavi, 2023; Wu et al., 2023; Yin et al., 2023) represent a new category of artificial intelligence systems built upon large models. These agents are capable of acting, reasoning, and interacting with external environments. Although LLMs has limited executable skills on their own, when integrated with external APIs and knowledge sources, they can tackle a wide range of tasks (Schick et al., 2023). An iterative approach has shown to be beneficial to solve natural language processing tasks, as evidenced by ReAct (Yao et al., 2022) and embodied tasks applied to games (Wang et al., 2023; Lin et al., 2023), web navigation (Yao et al., 2022), and robot navigation (Huang et al., 2022). Our approach, which utilizes LLMs for creating 3D meshes, contributes to the expanding research in this developing field.\n' +
      '\n' +
      'Text to 3D modelsA growing number of studies are exploring how to adapt pre-trained text-to-2D image diffusion models for text-to-3D generation (Poole et al., 2022; Lin et al., 2023; Wang et al., 2023), suggesting that similar challenges are found in text-to-3D models. Meanwhile, LLM based approaches introduce a new perspective to text-to-3D mesh creation, potentially offering ways to address issues with out-of-distribution samples.\n' +
      '\n' +
      'Figure 2: GPT-4 tries to construct three types of objects from ShapeNet by writing Python scripts in Blender. It can successfully create simple items like lamps, but faces challenges with more complex objects such as tables and airplanes.\n' +
      '\n' +
      '## 3 L3GO framework\n' +
      '\n' +
      'The main challenge with generating entire 3D objects in one go is that it often leads to compounding spatial inaccuracies. We propose decomposing the creation of a 3D mesh into distinct parts and placing each component step by step. We name this approach L3GO, an agent that can collect the feedback and execute the action from a chain of 3D thoughts in a simple 3D environment. This approach transforms a singular attempt at object construction into iterative feedback collection and correction processes, enabling the integration of feedback from the Blender environment. Our framework borrows ideas from previous work of LLM-based reasoning and acting (Yao et al., 2022; Wang et al., 2023), but has been adopted for 3D mesh construction in a practical 3D modeling software.\n' +
      '\n' +
      '### SimpleBlenv\n' +
      '\n' +
      'We introduce SimpleBlenv, an environment built on top of Blender, where agents can easily submit action commands and receive environmental feedback, such as bounding box information and placement errors. We plan to release the code for the environment.\n' +
      '\n' +
      'Action space:In Blender, professional 3D designers have the ability to create complex 3D models, with a nearly limitless range of actions at their disposal. Although nearly every action a user can perform in Blender\'s UI has a corresponding Python API, we choose to focus on five basic shape primitive APIs for the sake of simplicity. These APIs are: primitive_cube_add, primitive_cylinder_add, primitive_cone_add, primitive_uv_sphere_add, and primitive_torus_add. As their names imply, these are used for adding cubes, cylinders, cones, spheres, and toruses, respectively.\n' +
      '\n' +
      'These API functions come with a complex set of arguments. To make it easier for LLMs to use these functions, we wrap each function with a wrapper that only requires a few key parameters, such as scale, location, and radius. Consequently, the range of actions available to our L3GO agent includes different settings for size, position, radius, and so on, for each shape-creating command. An illustration of this can be seen in the following example:\n' +
      '\n' +
      '```\n' +
      'defcreate_cube(name,location,scale): bpy.ops.mesh.primitive_cube_add( size=1,location=location) cube=bpy.context.object cube.name=name cube.scale=scale returncube\n' +
      '```\n' +
      '\n' +
      'For a detailed list of all the action wrapper APIs we created, refer to the Appendix. Despite using only five basic shape APIs, agents can create objects in many ways thanks to the flexibility in scaling, positioning, and adjusting the radius, among other controls.\n' +
      '\n' +
      'Observations and Environment FeedbackWe maintain a state space representation as a list of object parts that have been created so far, including their size in terms of x, y, z-axis and location in the global coordinate. Regarding environment feedback, after the agent selects an action, we execute the action in Blender thereby creating a mesh in the virtual space. From this, we can (a) extract information such as the bounding box dimensions of the object parts, and (b) check if the part built by the agent is intersecting with any other parts or if there is an unnecessary gap between the parts (e.g. see Figure 4.) We have built a set of functions to directly gather this information from Blender. This feedback is then relayed to the L3GO agent as text messages before it takes its next action.\n' +
      '\n' +
      '### L3GO: LLM-based 3D Generation of Objects\n' +
      '\n' +
      'In this section, we introduce L3GO, an LLM agent specifically designed for 3D mesh creation from text. L3GO is comprised of six components, each powered by a language model that either functions as a _generator_ or a _critic_. The schematic diagram in Figure 3 is shown for a visual overview.\n' +
      '\n' +
      'Part Specifications Generator:L3GO first prompts the LLM to identify the most pivotal part of the object. This pivotal part makes it easier to attach subsequent components. For instance, starting with the seat of a chair is practical because it is straightforward to add legs and a backrest to it, simplifying the coordinate calculations for the other parts. After naming the part, the agent uses a size generator to determine its reasonable dimensions in terms of width, depth, and height, corresponding to the x, y, and z axes.\n' +
      '\n' +
      'Part Specifications Critic:Once a part name is proposed, it undergoes a review by the Part Specifications Critic. This step is crucial to avoid ambiguity, which can confuse the agent later. For example, if "leg" is proposed while creating a chair, the agent cannot know its exact placement without a spatial descriptor like "front right leg". The Part Specifications Critic\'s role is to identify and correct such vague descriptions, allowing the Part Specifications Generator to revise its suggestion accordingly. The process moves forward only after the Part Specifications Critic\'s approval.\n' +
      '\n' +
      'Spatial Specifications Generator:After establishing the part name and size, the model considers the spatial requirements of the part, given what has already been constructed. (For the first part, we simply position it at the center.) Theagent begins by selecting the most appropriate base part to attach the new component to, then determine the spatial relationship between them. For instance, if constructing an airplane with the fuselage as the base and the left wing as the new part, a typical spatial requirement would be to attach the wing to the middle of the fuselage\'s left side.\n' +
      '\n' +
      'Coordinate Calculator:Based on the spatial requirements and the base part\'s position, this component calculates the new part\'s center coordinate. Accuracy here is crucial, as even minor misalignments can impact the overall correctness. To ensure precision, the agent is given access to a python execution environment: while using the LLM only to generate Python code for calculating the position of the new part. This approach is similar to the one described in Gao et al. (2023). To increase reliability, the process is repeated three times and determine the x, y, and z coordinates based on a majority vote from these repetitions (with ties broken arbitrarily)\n' +
      '\n' +
      'Run action:After determining the size and spatial position, the agent asks an LLM to decide on the part\'s shape, choosing from cubes, cylinders, cones, spheres, and toruses. Then, the agent writes a valid Python script for Blender, specifying the size, position, and shape type. Finally, the agent runs a command to generate the part\'s mesh in Blender. This code is executed in Blender in a headless mode, and the environment provides important feedback, such as the bounding boxes of each generated part, which is used in the next module.\n' +
      '\n' +
      'Spatial Critic:After running the Blender code, two final spatial correctness checks are conducted: a _continuity check_, which tells the agent if the newly created part is disconnected from the existing parts, and a _total overlap check_ with existing parts, which tells the agent if a newly created part is entirely contained within an existing part. If either issue arises, the process returns to the spatial requirement generation stage and the agent adjusts accordingly. See examples of spatial errors in Figure 4.\n' +
      '\n' +
      'Completion Critic:The final step is to determine whether the construction of a 3D mesh for an object is completed. To do this, this critic is provided with the name of the object being built and the list of its parts that have already been constructed to an LLM to make a binary decision of completion. If the critic predicts that it is incomplete, we start the next iteration with the Part Specifications Generator. If the task is completed, we proceed to generating a more natural-looking image using ControlNet.\n' +
      '\n' +
      'ControlNet for 3D Meshes \\(\\rightarrow\\) 2D ImagesAfter the L3GO agent finishes the creation of a 3D mesh, we render the object into gray-scaled image. We then feed this image to ControlNet with Canny edge detection to produce a more realistic looking image.\n' +
      '\n' +
      'Note that as L3GO is text-based, it does not use visual\n' +
      '\n' +
      'Figure 4: Two types of error feedback we provide in SimpleBlenv: (a) The newly added cuboid (in orange) is completely inside the base cylinder. (b) There is unnecessary spatial gap between the newly added cuboid and the base cylinder.\n' +
      '\n' +
      'Figure 3: (Top): SimpleBlenv, a wrapper environment on top of Blender, where LLM can construct a 3D mesh by using atomic building blocks. (Bottom): Schematic diagram of L3GO.\n' +
      '\n' +
      'information. Therefore, all communication must be text-based, including defining spatial orientations like which direction is the front or back when asking the model to create an object. We set these spatial assumptions in the prompts in advance to guide the construction process. Unless specified otherwise, we use GPT-4 as our base LLM in L3GO in our experiments.\n' +
      '\n' +
      '## 4 Experiments on ShapeNet\n' +
      '\n' +
      'In this section, we detail the baseline methods (SS4.1) to compare text-to-2D image and text-to-3D mesh generation to compare with our L3GO. We demonstrate the effectiveness of LLM-based methods in the generation of simple 3D objects, specifically focused on 13 well-known categories sourced from ShapeNet. For automatic evaluation, we use GPT-4V as evaluator for recognizing 3D meshes (SS4.2) of simple objects. We also show that human assessments and GPT-4V\'s automated evaluations (SS4.2) are well correlated.\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      'Given the absence of pre-existing LLM agents designed to work in Blender, we chose a range of algorithms that serve as baseline references. Originally intended for natural language processing tasks, we have adapted these baselines to function within the Blender environment, ensuring they align with our experimental framework.\n' +
      '\n' +
      'ReAct-BReAct (Yao et al., 2022) is a framework designed for implementing a language model based agent. In this framework, the agent outputs its thought process before taking any action. The observations gathered from the external environment following this action inform the subsequent steps. We implement ReAct in the SimpleBlenv setting, utilizing the environment feedback, observations, and action space outlined in Section 3. To differentiate it from the text version of ReAct, we refer to our implementation as ReAct-Blender, or ReAct-B for short.\n' +
      '\n' +
      'Reflexion-BReflexion (Shinn et al., 2023) builds upon the ReAct framework by adding an extra step of reflection. In ReAct, the agent outlines its reasoning, takes an action, and then receives feedback from the environment. Reflexion goes a step further - after receiving environment feedback, the agent engages in reflection on the action taken and its results, to inform its next move. In our setup, at the end of every iteration, we consider the current object part, previously built parts, and the current environment feedback. We then ask the agent to reflect on the size and placement of the object part it has just built. After the agent shares its thoughts, we prompt it to decide whether to redo the current part or move on. If the agent chooses to redo, its reflective insights are used in the next step.\n' +
      '\n' +
      'Gpt-4For a less structured approach, we use GPT-4 to generate the entire Python code needed to create an object in one single attempt. The prompt we used for GPT-4 was adapted from a Github repository (gd3kr, 2023), which, to our knowledge, was the first to present the open-source project that uses GPT-4 to control Blender. For the full prompt, refer to the Appendix.\n' +
      '\n' +
      'Dataset: ShapeNet-13To assess the basic mesh generation ability, we use 13 categories of ShapeNet: [\'airplane\', \'bench\', \'cabinet\', \'car\', \'chair\', \'display\', \'lamp\', \'loudspeaker\', \'rifle\',\'sofa\', \'table\', \'telephone\', \'watercraft\'], as introduced by (Choy et al., 2016).\n' +
      '\n' +
      '### Automatic Evaluation via GPT-4V\n' +
      '\n' +
      'To streamline our evaluation process, we propose using GPT-4V to assess the performance of mesh construction. For each object category, we generate 10 meshes from GPT-4, ReAct-B, Reflexion-B, and L3GO. After the agent finishes mesh construction, we render the object from 10 different views by rotating the camera around the object at the same height. This results in 10 images per mesh. we then feed 10 images to GPT-4V all at once, and use the following prompt: \'_What object do you see in these images? Answer with a single object name. Your answer must be one of the following options: [airplane, bench, cabinet, car, chair, display, lamp, loudspeaker, rifle, sofa, table, telephone, watercraft]"_.\n' +
      '\n' +
      'Table 1 presents the average accuracy across different object categories. It is evident that structured methods, including ReAct-B, Reflexion-B, and L3GO, surpass GPT-4 in performance. Notably, among these structured approaches, L3GO proves to be the most effective. Delving into the results for each object, as detailed in Figure 5, it becomes clear that L3GO particularly excels in constructing complex objects like airplanes and rifles.\n' +
      '\n' +
      'Correlation with human evaluationsWe recruit human participants to assess whether the evaluation of GPT-4V aligns with human judgment. For each mesh, we use 10 images from 10 different angles (the same images as above for GPT-4V evaluation), and ask a human participant to classify these images into one the 13 object categories. We\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & Human & L3GO & ReAct-B & Reflexion-B & GPT-4 \\\\ \\hline GPT-4V & 0.877 & **0.6** & 0.423 & 0.4 & 0.346 \\\\ Human & 0.894 & **0.584** & 0.385 & 0.403 & 0.445 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Mean accuracy of different LLM-based agents on ShapeNet-13, evaluated by GPT-4V (top row) and humans (bottom row); each cell is an average over 130 trials. ‘Human’ in the column names refers to the original ShapeNet meshes, designed by humans, which can be considered as the upper bound. We see that L3GO outperforms other GPT-4-based agents (e.g. ReAct-B, Reflexion-B, and unmodified GPT-4).\n' +
      '\n' +
      'collect 130 human responses by showing meshes generated by GPT-4, and L3GO, as well as the original ShapeNet meshes, totaling in 390 human responses. The category-by-category results are shown in Figure 6. We can see an overall pattern, where the original ShapeNet has high accuracy, L3GO outperforms GPT-4, except for a few cases like "lamp", "bench", and "loudspeaker".\n' +
      '\n' +
      'We also gather 390 responses from GPT-4V using the same set of images. To benchmark GPT-4V against other top vision-language models, we also obtain 390 responses each from BLIP-2 and InstructBLIP. Regarding human-to-human correlation evaluation, four participants were asked to classify all 390 images. However, we exclude the data from the participant whose responses most often differed from the other three. We then calculate the Cohen\'s Kappa score to measure the correlation between these models and three human evaluators, averaging these correlations, as shown in Table 3. Our findings indicate that GPT-4V\'s responses align most closely with human judgments, though it is important to note that even among humans, agreement was not perfect.\n' +
      '\n' +
      '## 5 Experiments on UFO: Constructing Unconventionally Feasible Objects\n' +
      '\n' +
      'Our previous experiments show that L3GO can accurately construct a simple object from ShapeNet around 60% of the time. However, modern diffusion based models can nearly always generate an image of a given ShapeNet object. This is in part because there are many possible valid instantiations of, e.g., "car" or "bench". So: is there any potential practical advantage to using a method like L3GO?\n' +
      '\n' +
      'To illustrate the potential advantages of LLMs in spatial construction, we introduce a benchmark that requires more precise spatial understanding. Inspired by DrawBench (Saharia et al., 2022) and PariPrompts (Yu et al., 2022) which are collections of prompts that systematically evaluate text-to-image models, we introduce UFO: a set of 50 difficult prompts that 1) require precise spatial understanding to construct; and 2) are unusual, in the sense that they are less likely to occur during text-only pre-training, e.g., "a chair with one armrest".\n' +
      '\n' +
      'The prompts in UFO span 9 object categories, and each prompt is a combination of a common object with varied characteristics such as "a chair with five legs", "a mug with two handles" and so on. The full prompt list is shown in the Appendix. We focus on everyday 3D objects to help us isolate the model\'s performance in accurately interpreting prompts from its inherent ability to create unconventional objects. By using simple-to-assemble items such as sofas, chairs, lamps, and tables, we can better discern whether any shortcomings are due to the model\'s prompt following from its object creation skills.\n' +
      '\n' +
      'BaselinesWe compare our LLM-based approach with latest text-to-2D and text-to-3D methods such as DALL-E 3 (Betker et al., ), Stable Diffusion XL (SDXL) (Podell et al., 2023), and Shap-E (Jun and Nichol, 2023). DALL-E 3 uses descriptive synthetic captions to improve prompt following of DALL-E 2 (Ramesh et al., 2022), the previous version of OpenAI\'s text-to-image diffusion model. Stable Diffusion XL is an open-sourced text-to-image diffusion model. Shap-E (Jun and Nichol, 2023) is a text-to-3D model that generates\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & GPT-4V & InstructBLIP & BLIP-2 & Human \\\\ \\hline Human & \\(\\textbf{0.512}_{(0.028)}\\) & \\(0.344_{(0.016)}\\) & \\(0.341_{(0.012)}\\) & \\(0.569_{(0.020)}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: The Cohen’s Kappa correlation between evaluations based on models and human judgement. We report the average and standard deviation calculated from three independent human evaluators.\n' +
      '\n' +
      'Figure 5: GPT-4V evaluation of L3GO, ReAct-B, Reflexion-B, and GPT-4 on ShapeNet-13. ’Human’ refers to original ShapeNet meshes that were designed by humans. For complex objects such as airplanes and rifles, L3GO performs better than others.\n' +
      '\n' +
      'Figure 6: Human evaluation of L3GO, ReAct-B, Reflexion-B, and GPT-4 on ShapeNet-13. ’Human’ refers to the original human-designed ShapeNet meshes. We observed a pattern similar to that in GPT-4V’s evaluation.\n' +
      '\n' +
      'the parameters of implicit function for 3D models which then can be rendered. Since DALL-E 3 automatically rewrites the prompt for safety reasons and adds more detail, (and it is not possible to disable this feature at the moment) we add "I NEED to test how the tool works with extremely simple prompts. DO NOT add any detail, just use it AS-IS:" to our prompt as recommended by OpenAI 2.\n' +
      '\n' +
      'Footnote 2: [https://platform.openai.com/docs/guides/images](https://platform.openai.com/docs/guides/images)\n' +
      '\n' +
      'Experiment proceduresWe again utilize the judgements of human participants to evaluate the output of our models on UFO. For each given prompt, we generate 10 random objects from one model and another 10 from a different model. A participant is then asked to judge which set of images better matches the provided text caption. If they believe that neither set accurately represents the caption, they can choose "no preference." For each experiment, we recruit 10 human evaluators. Additionally, we include 4 attention check questions on top of the 50 total questions. Any evaluator who does not correctly answer all the attention check questions is excluded from our analysis. For models that create 3D objects, we render 10 images from various angles by rotating the camera at a constant height. These 10 images are then compiled into a rotating GIF.\n' +
      '\n' +
      'ResultsThe results are shown in Figure 7. L3GO outperforms the other LLM agents (e.g. ReAct-B, Reflexion-B) and the state-of-the-art text-to-image models (DALL-E-3 and SDXL) and text-to-3D model (Shap-E) in terms of human preference. Example generated images are shown in Figure 8. DALL-E-3, SDXL and Shap-E produce images that do not perfectly follow the specific prompt instructions. While SDXL is able to generate a desk with three legs, an additional chair that is not asked in the prompt is generated. DALL-E-3 seems to completely ignore the specific requirements of prompts. In contrast, while their designs are not perfect, language model-based agents are capable of constructing chairs with the right number of legs. These results suggest that structured reasoning may serve as a viable strategy to mitigate the challenges posed by insufficient training data.\n' +
      '\n' +
      'Effect of background and texture on evaluation for UFOWe look into how the background and texture differences in images created by text-to-image models and LLM-based methods might affect human evaluations. To test this, we change prompt styles with these text-to-image models. For DALL-E-3, we use "[object name] Make sure the background is black, and the object is a gray-colored 3D shape." For Stable Diffusion XL, we use "[object name], black background, gray-colored 3D shape." Additionally, we alter the guidance scale for Stable Diffusion XL, which determines how closely the diffusion model follows the text prompts. In both scenarios, we observe that L3GO outperforms text-to-image models in terms of human preference, as illustrated in Figure 9. We also conducted an initial test to determine if GPT-4V could serve as an evaluator for UFO. However, we observed that in over 20% of the cases, GPT-4V refuses to provide an answer, which might be related to the characteristics of the images generated. We refer to Table 4 in Appendix for more details.\n' +
      '\n' +
      '## 6 Ablation studies\n' +
      '\n' +
      'We ablate three 3 system design choices to see which component most impacts the overall performance for 3D mesh generation. We use our automatic evaluation via GPT-4V to compare the performance in 3 ablations: 1) without spatial critic, 2) without program-based coordinate calculator, and 3) choice of LLMs. For each setting, we generate 10 objects per category, and render 10 images from different angles. For 1) and 2), the mean accuracy across the 13 categories of ShapeNet (evaluated by GPT-4V) are 0.515 and 0.585, respectively. In comparison, L3GO achieves a higher score of 0.6. While the average scores for L3GO and without coordinate calculator achieve similar scores, the latter scores 0 in both the cabinet and car categories. In contrast, when L3GO employs program-based coordinate calculation, it achieves scores of 0.5 and 0.4 for these categories, respectively.\n' +
      '\n' +
      'Open-sourced LLMsWe explore the use of open-source LLMs in place of GPT-4. For this purpose, we use Mixtral\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline \\multicolumn{1}{c}{Mixtral-8\\%7B} & w/o spatial critic & w/o program-based & L3GO \\\\ \\hline\n' +
      '0.138 & 0.515 & 0.585 & **0.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Ablation studies. We evaluate the performance based on ShapeNet’s 13 categories using GPT-4V as an evaluator; each cell is an average over 130 trials. ‘w/o spatial critic/program-based’ refers to L3GO based on GPT-4 without spatial critic and without program-based coordinate calculation module. ‘Mixtral-8x7B’ refers to ReAct-B based on Mixtral-8X7B instead of GPT-4.\n' +
      '\n' +
      'Figure 7: Human preference of L3GO vs. DALL-E-3, SDXL, Shap-E, ReAct-B, and Reflexion-B on UFO.\n' +
      '\n' +
      '8x7B, a sparse mixture-of-experts model (Jiang et al., 2024), that is known to either match or surpass the performance of Llama-2 70B and GPT-3.5 in most benchmark tests. We carry out experiments using ReAct-B and ShapeNet-13, with GPT-4V serving as our evaluation tool. While the accuracy for the ReAct-B(Mixtral-8x7B) for most shape categories ranged between 0 and 0.1, the categories of sofas, lamps, and tables achieved higher scores of 0.3, 0.3, and 0.7, respectively. This is likely due to their simpler shapes and easier recognizability. The average accuracy score is 0.138. This result is significantly lower than the 0.423 accuracy achieved by ReAct-B(GPT-4), as indicated in Table 1. This indicates that the task of constructing mesh objects, which demands a precise understanding of 3D space, still needs the reasoning abilities found at the level of GPT-4.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'We introduced L3GO, a language agent designed to generate 3D objects from text instructions through an API we developed for Blender, a 3D modeling software. Our evaluation using 13 largest object categories from ShapeNet shows that L3GO\'s superior capabilities in comparison to other models such as GPT-4, ReAct, and Reflexion. Additionally, we devised UFO, a set of challenges aimed at testing the ability of generative AI models in creating common objects with unconventional characteristics. The performance of L3GO marks a significant advancement in the application range of language models. For instance, diffusion models could be further improved with unconventional data generated by structured prompting. Moreover, analyzing how language models process spatial information with internal model representations may yield valuable insights into understanding and improving their 3D modeling abilities.\n' +
      '\n' +
      'Impact StatementOur research indicates the vast potential for integrating language models with 3D modeling, potentially revolutionizing design processes and the creation of digital environments. This convergence aims at making generative AI tools more intuitive and capable of supporting creative endeavors. With L3GO, we begin to tap into the untapped possibilities in this domain, setting the stage for extensive future exploration and development.\n' +
      '\n' +
      'The positive societal impacts of our work could be substantial, particularly in design, engineering, and the arts,\n' +
      '\n' +
      'Figure 8: Example generated images based on UFO. The LLM-based approaches (ReAct-B, Reflexion-B, and L3GO) successfully create the desired objects, while some of the most advanced text-to-image and text-to-3D models (DALL-E 3, Stable Diffusion XL, and Shap-E) still struggle to follow the prompt perfectly.\n' +
      '\n' +
      'Figure 9: Human preference of L3GO vs. DALL-E-3 and SDXL on UFO, where we attempt to make the background of generated images to be simple gray shading. We also vary guidance scales for SDXL to see if better prompt following improves the performance, denoted as ‘guide=15, 30’. For DALL-E-3, we use ‘quality=hd’ option for enhanced detail.\n' +
      '\n' +
      'by enabling the visualization and prototyping of ideas that were previously difficult or impossible to achieve. Furthermore, our approach could enhance educational tools, making complex concepts more accessible through interactive and visually intuitive representations. However, we must also consider the ethical implications of advancing image generation technology, such as the potential for creating misleading or harmful content. It underscores the necessity for ongoing research into mechanisms that ensure these powerful tools are used responsibly and ethically. We advocate for a balanced approach that emphasizes innovation alongside responsibility and ethical considerations.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Abdou et al. (2021) Abdou, M., Kulmizev, A., Hershcovich, D., Frank, S., Pavlick, E., and Sogaard, A. Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color. In _Proceedings of the 25th Conference on Computational Natural Language Learning_, pp. 109-132, Online, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.conll-1.9.\n' +
      '* Betker et al. (2023) Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., Manassra, W., Dhariwal, P., Chu, C., Jiao, Y., and Ramesh, A. Improving Image Generation with Better Captions. _[https://openai.com/dall-e-3_](https://openai.com/dall-e-3_).\n' +
      '* Bubeck et al. (2023) Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang, Y. Sparks of Artificial General Intelligence: Early experiments with GPT-4, April 2023.\n' +
      '* Canny (1986) Canny, J. A Computational Approach to Edge Detection. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, PAMI-8(6):679-698, November 1986. ISSN 1939-3539. doi: 10.1109/TPAMI.1986.4767851.\n' +
      '* Chefer et al. (2003) Chefer, H., Alaluf, Y., Vinker, Y., Wolf, L., and Cohen-Or, D. Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models. _ACM Transactions on Graphics_, 42(4):148:1-148:10, July 2023. ISSN 0730-0301. doi: 10.1145/3592116.\n' +
      '* Choy et al. (2016) Choy, C., Xu, D., Gwak, J., Chen, K., and Savarese, S. 3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction. In _Proceedings of the European Conference on Computer Vision (ECCV)_, volume 9912, pp. 644, October 2016. ISBN 978-3-319-46483-1. doi: 10.1007/978-3-319-46484-8_38.\n' +
      '* Feng et al. (2022) Feng, W., He, X., Fu, T.-J., Jampani, V., Akula, A. R., Narayana, P., Basu, S., Wang, X. E., and Wang, W. Y. Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis. In _The Eleventh International Conference on Learning Representations_, September 2022.\n' +
      '* Gao et al. (2023) Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. PAL: Program-aided Language Models. In _Proceedings of the 40th International Conference on Machine Learning_, pp. 10764-10799. PMLR, July 2023.\n' +
      '* gd3kr (2023) gd3kr. BlenderGPT. [https://github.com/gd3kr/BlenderGPT](https://github.com/gd3kr/BlenderGPT), 2023.\n' +
      '* Ge et al. (2023) Ge, Y., Hua, W., Mei, K., Ji, J., Tan, J., Xu, S., Li, Z., and Zhang, Y. OpenAGI: When LLM Meets Domain Experts. In _Thirty-Seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, November 2023.\n' +
      '* Gupta & Kembhavi (2023) Gupta, T. and Kembhavi, A. Visual Programming: Compositional Visual Reasoning Without Training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 14953-14962, 2023.\n' +
      '* Hochreiter & Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. Long Short-Term Memory. _Neural Computation_, 9, 1997.\n' +
      '* Huang et al. (2022) Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Brown, N., Jackson, T., Luu, L., Levine, S., Hausman, K., and Ichter, B. Inner Monologue: Embodied Reasoning through Planning with Language Models. In _Conference on Robot Learning_, 2022. doi: 10.48550/ARXIV.2207.05608.\n' +
      '* Janner et al. (2018) Janner, M., Narasimhan, K., and Barzilay, R. Representation Learning for Grounded Spatial Reasoning. _Transactions of the Association for Computational Linguistics_, 6:49-61, 2018. doi: 10.1162/tacl_a_00004.\n' +
      '* Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral of Experts, January 2024.\n' +
      '* Jun & Nichol (2023) Jun, H. and Nichol, A. Shap-E: Generating Conditional 3D Implicit Functions, May 2023.\n' +
      '* Lin et al. (2018) Lin, B. Y., Fu, Y., Yang, K., Brahman, F., Huang, S., Bhagavatula, C., Ammanabrolu, P., Choi, Y., and Ren, X. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023a. URL [https://openreview.net/forum?id=R2k3GP1HN7](https://openreview.net/forum?id=R2k3GP1HN7).\n' +
      '\n' +
      '* Lin et al. (2023) Lin, C.-H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.-Y., and Lin, T.-Y. Magic3D: High-Resolution Text-to-3D Content Creation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 300-309, 2023b.\n' +
      '* Mirzaee et al. (2021) Mirzaee, R., Rajaby Faghihi, H., Ning, Q., and Kordjamshidi, P. SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning. In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y. (eds.), _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 4582-4598, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.364.\n' +
      '* OpenAI (2023) OpenAI. GPT-4V(ision) technical work and authors, 2023.\n' +
      '* Park et al. (2023) Park, J. S., O\'Brien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative Agents: Interactive Simulacra of Human Behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, UIST \'23, pp. 1-22, New York, NY, USA, October 2023. Association for Computing Machinery. ISBN 9798400701320. doi: 10.1145/3586183.3606763.\n' +
      '* Patel & Pavlick (2022) Patel, R. and Pavlick, E. Mapping Language Models to Grounded Conceptual Spaces. In _International Conference on Learning Representations_, January 2022.\n' +
      '* Podell et al. (2023) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis, July 2023.\n' +
      '* Poole et al. (2022) Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dream-Fusion: Text-to-3D using 2D Diffusion. In _The Eleventh International Conference on Learning Representations_, September 2022.\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical Text-Conditional Image Generation with CLIP Latents, April 2022.\n' +
      '* Rassin et al. (2023) Rassin, R., Hirsch, E., Glickman, D., Ravfogel, S., Goldberg, Y., and Chechik, G. Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment. In _Thirty-Seventh Conference on Neural Information Processing Systems_, November 2023.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-Resolution Image Synthesis With Latent Diffusion Models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 10684-10695, 2022.\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Gontijo-Lopes, R., Ayan, B. K., Salimans, T., Ho, J., Fleet, D. J., and Norouzi, M. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. In _Advances in Neural Information Processing Systems_, May 2022.\n' +
      '* Schick et al. (2023) Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language Models Can Teach Themselves to Use Tools. In _Thirty-Seventh Conference on Neural Information Processing Systems_, November 2023.\n' +
      '* Shen et al. (2023) Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. In _Thirty-Seventh Conference on Neural Information Processing Systems_, November 2023.\n' +
      '* Shinn et al. (2023) Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. In _Thirty-Seventh Conference on Neural Information Processing Systems_, November 2023.\n' +
      '* Wang et al. (2023a) Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An Open-Ended Embodied Agent with Large Language Models. In _NeurIPS 2023 Foundation Models for Decision Making Workshop_, November 2023a.\n' +
      '* Wang et al. (2022b) Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., and Zhu, J. ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation. In _Thirty-Seventh Conference on Neural Information Processing Systems_, November 2023b.\n' +
      '* Wu et al. (2023) Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., and Duan, N. Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models. 2023. doi: 10.48550/ARXIV.2303.04671.\n' +
      '* Yao et al. (2022) Yao, S., Chen, H., Yang, J., and Narasimhan, K. WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. _Advances in Neural Information Processing Systems_, 35:20744-20757, December 2022a.\n' +
      '* Yao et al. (2022b) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. ReAct: Synergizing Reasoning and Acting in Language Models. In _The Eleventh International Conference on Learning Representations_, September 2022b.\n' +
      '\n' +
      '* Yin et al. (2023) Yin, D., Brahman, F., Ravichander, A., Chandu, K. R., Chang, K.-W., Choi, Y., and Lin, B. Y. Lumso: Learning agents with unified data, modular design, and open-source llms. _ArXiv_, abs/2311.05657, 2023. URL [https://api.semanticscholar.org/CorpusID:265128672](https://api.semanticscholar.org/CorpusID:265128672).\n' +
      '* Yu et al. (2022) Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., and Wu, Y. Scaling Autoregressive Models for Content-Rich Text-to-Image Generation. _Transactions on Machine Learning Research_, August 2022. ISSN 2835-8856.\n' +
      '* Zhang et al. (2023) Zhang, L., Rao, A., and Agrawala, M. Adding Conditional Control to Text-to-Image Diffusion Models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 3836-3847, 2023.\n' +
      '\n' +
      '## Appendix A Algorithm\n' +
      '\n' +
      'The pseudo algorithm of L3GO is given in Figure 10.\n' +
      '\n' +
      '## Appendix B GPT-4V as an evaluator for UFO\n' +
      '\n' +
      'We also conducted an initial test where we used GPT-4V to evaluate generated images using prompts from UFO. This experiment involved showing GPT-4V two sets of 10 images from two models, similar to our human study, and asking GPT-4V which set of images more accurately represent the corresponding text caption. We ask it choose either the \'top row\', \'bottom row\', or \'no preference\', along with providing a reason for its choice. The images were generated using L3GO and DALL-E-3. We carried out this experiment twice. In both cases, for more than 20% of the prompts (13 and 18 out of 50), the response we received was "I cannot assist with this request". Due to this, we decided to rely on human evaluators instead of GPT-4V for our main experiments.\n' +
      '\n' +
      '## Appendix C Details for human evaluation\n' +
      '\n' +
      'We recruit participants through Prolific.com, an online platform used by researchers to conduct behavioral studies and gather participants. The requirement for participants was English fluency, as the study\'s materials were all in English. The study was made accessible to all qualified participants on the platform. We ensure $15 hourly wage.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & L3GO & DALL-E-3 & No pref & Refuse & No images \\\\ \\hline\n' +
      '1st trial & 12 & 8 & 9 & 18 & 3 \\\\\n' +
      '2nd trial & 14 & 12 & 11 & 13 & 0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: GPT-4V’s evaluation of UFO, where GPT-4V compares the image generation by L3GO and DALL-E-3. “Refuse” indicates instances where GPT-4V declined to respond, for example by saying “I cannot assist with this request.” “No images” refers to cases where GPT-4V incorrectly perceived the images as being invisible.\n' +
      '\n' +
      'Figure 10: Pseudo algorithm of L3GO.\n' +
      '\n' +
      'At the beginning of the experiment, we give participants an online form detailing the experiment\'s aims, a summary, the methodology, confidentiality assurances, and the voluntary nature of their involvement. The participants are assured of the confidentiality of their responses and their right to withdraw at any time. Participants are asked to acknowledge their understanding of the provided information and their consent to participate by clicking the "start" button. The experiment commences only after this button was clicked.\n' +
      '\n' +
      'Despite efforts to minimize bias by keeping participant criteria to a minimum, there remains an inherent bias towards English-speaking individuals with access to computers and the internet. We note that this bias also tends to reflect characteristics of the WEIRD (Western, Educated, Industrial, Rich, Democracies) demographic.\n' +
      '\n' +
      '## Appendix D Additional details for experiments\n' +
      '\n' +
      'Wrapper APIs for action commands in SimpleBlenvare shown in Figure 11.\n' +
      '\n' +
      'Example environment feedback from SimpleBlenvis shown in Figure 12.\n' +
      '\n' +
      'Full prompt for the baseline GPT-4is shown in Figure 13.\n' +
      '\n' +
      'The list of prompts in UFOachair with two legs, a chair with three legs, a chair with six legs, a chair with seven legs, a chair with one armrest, a chair with three armrests, a chair with two backrests, a chair with three backrests, a stool with two legs, a stool with five legs, a stool with six legs, a stool with one armrest, a stool with three armrests, a stool with two backrests, a stool with three backrests, a desk with two legs, a desk with three legs, a desk with five legs, a desk with six legs, a desk with seven legs, a table with two legs, a table with three legs, a table with five legs, a table with six legs, a table with seven legs, a pair of eyeglasses with one round lens and one square lens, a pair of eyeglasses with one round lens and one triangle lens, a pair of eyeglasses with one square lens and one triangle lens, a pair of eyeglasses with three lenses, a pair of eyeglasses with four lenses, a pair of eyeglasses with five lenses, a sofa with one leg, a sofa with two legs, a sofa with three legs, a sofa with five legs, a sofa with legs that are longer than its backrest, a lamp with two legs, a lamp with five legs, a bottle whose lid is twice as wide as its mouth, a bottle with a lid that is three times wider than its mouth, a bottle with a lid that is four times wider than its mouth, a mug with two handles, a mug with three handles, a mug with four handles, a mug with five handles\n' +
      '\n' +
      '### Additional generated examples\n' +
      '\n' +
      'are shown in Figure 14 and 15.\n' +
      '\n' +
      'The code is uploaded in the following url.5\n' +
      '\n' +
      'Footnote 5: [https://u.pcloud.link/publink/show?code=k24e55022VnyJfJO8VHyivUkfVdKhb5B8SEk](https://u.pcloud.link/publink/show?code=k24e55022VnyJfJO8VHyivUkfVdKhb5B8SEk)\n' +
      '\n' +
      '## Appendix E Applications and Future Work\n' +
      '\n' +
      '### Creative Combinations of Object Parts\n' +
      '\n' +
      'In UFO, we explored how our approach could adapt to unique variations in common object characteristics, such as creating a chair with an atypical number of legs. While legs are a standard feature of chairs, our focus was on experimenting with unconventional leg counts. In this section, we extended this question to see if L3GO can handle even more creative combinations, like integrating components not typically associated with the object, for instance, a chair with wings.\n' +
      '\n' +
      'Displayed in Figure 16 are instances of these creative prompts and their resulting models. Even though the forms for "wings" and "umbrella" are basic, such as rectangles and elongated spheres with thin cylinders, L3GO adeptly figures out that "wings" should attach to the sides of the chair, and an "umbrella" should be positioned on top. (See Appendix for more examples.) Our results demonstrate that L3GO can successfully interpret and construct these unusual designs.\n' +
      '\n' +
      '### Language interface for complex softwares\n' +
      '\n' +
      'Most 3D modeling software programs have a steep learning curve. Creating sophisticated 3D models often involves a sequence of operations using complex interfaces, which can be challenging for non-experts to navigate. Our method can be used as a language interface for these complex software programs, making them more accessible to beginners. As illustrated in Figure 17, a user can input a prompt, and L3GO will generate an initial draft of the object. While our approach is still in its early stages and needs further development for practical use, we hope our research inspires future advancements.\n' +
      '\n' +
      'Figure 11: Full list of action wrapper APIs for SimpleBlenv.\n' +
      '\n' +
      '## Appendix F Limitations\n' +
      '\n' +
      'The quality of 3D mesh objects generated using LLM-based methods lag behind those produced by diffusion-based text-to-3D methods, and far from being on par with human standards. Moreover, creating simple objects takes roughly 3 to 5 minutes, while more complex ones can take 10 to 20 minutes, depending on the number of retry operations needed after the feedback. We believe that the future work should explore more efficient LLM-based approaches to create 3D mesh as this is a promising direction to better control the eventual 3D object generation.\n' +
      '\n' +
      'Figure 14: Additional generated examples on UFO.\n' +
      '\n' +
      'Figure 12: An example environment feedback from SimpleBlenv.\n' +
      '\n' +
      'Figure 13: The full prompt for the baseline GPT-4 used in our experiments. This prompt is taken from Blender-GPT\\({}^{4}\\), which is the first open-source code that uses GPT-4 inside Blender to our knowledge.\n' +
      '\n' +
      'Figure 16: Because Large Language Models can interpret creative prompts, L3GO can create unique items such as a chair with wings or a chair that has an umbrella attached.\n' +
      '\n' +
      'Figure 15: Additional generated examples of L3GO on ShapeNet.\n' +
      '\n' +
      'Figure 17: The Blender user interface can be daunting for newcomers. An LLM agent can be helpful by creating a draft of an object to assist beginners.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>