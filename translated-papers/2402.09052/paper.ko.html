<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# L3GO: 비전통적 객체 생성을 위한 Chain-of-3D-Thoughts를 갖는 언어 에이전트\n' +
      '\n' +
      'Yutaro Yamada\n' +
      '\n' +
      'Khyathi Chandu\n' +
      '\n' +
      'Yuchen Lin\n' +
      '\n' +
      'Jack Hessel\n' +
      '\n' +
      'Ilker Yildirim\n' +
      '\n' +
      'Yejin Choi\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'DALL-E 3 및 Stable Diffusion-XL과 같은 확산 기반 이미지 생성 모델은 사실적이고 독특한 구도를 가진 이미지를 생성하는 데 현저한 능력을 보여준다. 그러나 이러한 모델은 특히 비전통적으로 지시될 때 물체의 물리적 및 공간적 구성에 대한 정확한 추론에 견고하지 않으며, 따라서 5개의 다리가 있는 의자 "_"와 같은 유통 외 설명에 견고하지 않다. 본 논문에서는 현재 데이터 기반 확산 모델들이 어려움을 겪고 있는 비전통적 객체들의 부분 기반 3차원 메쉬 생성을 추론할 수 있는 추론 시간 접근 방법인 L3GO(chain-of-3D-thoughts)를 갖는 언어 에이전트를 제안한다. 보다 구체적으로, 우리는 3D 시뮬레이션 환경에서 시행착오를 통해 원하는 객체를 구성하기 위해 큰 언어 모델을 에이전트로 사용한다. 우리의 조사를 용이하게 하기 위해, 우리는 언어 에이전트가 API 호출을 통해 원자 구성 블록을 구축하고 구성할 수 있는 Blender1 위에 구축된 래퍼 환경인 SimpleBlenv뿐만 아니라 새로운 벤치마크인 ** 비전통적인 실현 가능한 객체(UFO)**를 개발한다. 인간 및 자동 GPT-4V 평가는 우리의 접근 방식이 ShapeNet에서 3D 메쉬 생성을 위한 표준 GPT-4 및 기타 언어 에이전트(예: ReAct 및 Reflexion)를 능가한다는 것을 보여준다. 또한, UFO 벤치마크에서 테스트했을 때, 우리의 접근법은 인간 평가를 기반으로 하는 다른 최신 텍스트 대 2D 이미지 및 텍스트 대 3D 모델보다 우수하다.\n' +
      '\n' +
      '각주 1: [https://www.blender.org/](https://www.blender.org/)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '텍스트 명령어로부터 2D 이미지를 생성하는 AI 애플리케이션(Betker et al.; Saharia et al., 2022; Podell et al., 2023) 및 3D 모델(Jun and Nichol, 2023; Lin et al., 2023b)은 크리에이터에게 상당한 가능성을 열어주었다. 그러나, 이러한 툴들은 종종 입력 프롬프트들에 충성하지 않은 예기치 않은 또는 _"Hallucinatory"_ 결과들(Saharia et al., 2022)을 생성하기 때문에 정확한 출력 컨트롤이 부족하다. 또한, 초기 버전의 Stable Diffusion(Rombach et al., 2022)은 하나의 이미지에서 여러 개념을 결합하는 데 어려움이 있거나 다른 속성을 혼합할 것이다. 이전의 노력은 객체-속성 부착, 누락된 객체 등에 대한 성능을 향상시켰다. 조향 주의 층들(Feng et al., 2022; Chefer et al., 2023; Rassin et al., 2023)에 의해, 또는 방대한 스케일에서 상세한 캡션들을 갖는 더 큰 모델들을 트레이닝함으로써(StableDiffusion-XL(SDXL)(Podell et al., 2023) 및 DALL-E-3(Betker et al.)). 그러나, 가장 성능이 좋은 확산 모델인 DALL-E 3조차도 여전히 "5개의 다리를 가진 의자"와 같은 정밀한 3D 공간 이해를 필요로 하는 객체들을 생성하지 못한다(도 1). 이러한 어려움은 인간의 피드백으로 DALL-E-3의 출력을 직접 조정하려는 반복 시도 후에도 지속되는데, 예를 들어 "당신이 생성한 의자는 7개의 다리를 가지고 있습니다. 정확히 5개의 다리를 가진 의자를 만들어 주세요."와 같은 것이다.\n' +
      '\n' +
      '우리는 LLM에 내재된 정교한 텍스트 기반 추론 능력이 텍스트 대 2D 이미지와 텍스트 대 3D 모델의 3D 공간 이해의 단점을 보완할 수 있다고 가정한다. 우리는 추론 에이전트가 가능한 L3GO를 제시한다.\n' +
      '\n' +
      '그림 1: 최신 텍스트 대 이미지 모델 중 하나(DALL-E 3)와 LLM 기반 접근법(L3GO)을 비교한다. 우리는 인간 피드백으로 DALL-E 3 세대의 5회 반복을 수행하지만 DALL-E 3은 프롬프트를 엄격하게 따르지 않는다. L3GO는 정확한 수의 다리를 가진 의자를 만듭니다.\n' +
      '\n' +
      '보정을 통합하고 2D 이미지를 생성하기 위해 골격으로 사용되는 3D 메시를 렌더링하는 정밀도를 향상시키기 위해 LLM으로부터 피드백을 반복적으로 요청한다.\n' +
      '\n' +
      '우리는 널리 호평을 받는 3D 모델링 소프트웨어인 블렌더 내에서 실험을 수행한다. LLM 에이전트의 텍스트-투-3D 메쉬 생성 성능을 체계적으로 평가하기 위해 블렌더를 기반으로 SimpleBlenv라는 환경을 만들고 출시한다. 텍스트에 대해서만 트레이닝됨에도 불구하고 GPT-4(Bubeck et al., 2023)와 같은 최신 LLM들은 괜찮은 공간 추론 능력들을 갖는다. 그림 2는 GPT-4가 객체 이름만을 기반으로 기본 객체의 3D 메쉬를 생성하기 위해 블렌더에서 실행할 수 있는 파이썬 스크립트를 작성하라는 메시지를 받았을 때 혼합된 결과를 보여준다. 한편, 텍스트 전용 GPT-4는 세 가지 기본 모양으로 구성된 램프(2a)와 같은 간단한 3D 물체를 만드는 놀라운 능숙함을 보여준다. 그러나 객체 복잡도가 4개 이상(ex: 테이블의 네 다리) 또는 비행기(2b, 2c)와 같은 복잡한 객체까지 증가함에 따라 GPT4의 완벽한 조립 성공은 제한적이다.\n' +
      '\n' +
      '우리의 L3GO 에이전트는 (1) 관련 부품 사양을 식별하고 이를 비판하는 것, (2) 공간 사양 및 배치를 식별하는 것, (3) 공간 배치 및 완성을 비판하기 위한 현재 작업을 실행함으로써 복잡한 객체를 구성함으로써 이러한 격차를 해소한다. 이 설정은 SimpleBlenv의 피드백을 반복적으로 찾고 사양 및 비평은 LLM에서 생성된다. 마지막으로, 메쉬를 이미지로 렌더링하고, 이를 Canny edge detection(Canny, 1986)과 함께 ControlNet(Zhang et al., 2023)에 공급하여 질감 있고 보다 자연스러운 모습의 이미지를 생성한다. 우리는 ShapeNet의 13개의 인기 객체 카테고리를 사용하여 LLM 기반 메쉬 생성의 성능을 비교하기 위해 인간 평가를 수행한다. L3GO는 인간 및 자동 평가 모두에서 기본 GPT-4, ReAct-B 및 반사-B를 능가한다. 또한, GPT-4V(OpenAI, 2023)를 이용한 메쉬 품질 평가는 인간의 판단과 높은 상관 관계를 갖는 메트릭을 산출함을 보인다. 마지막으로, 비전통적이면서도 실현 가능한 객체들을 갖는 UFO라는 비전통적 실현 가능한 객체들을 소개한다. 우리는 L3GO가 UFO에서 현재 최첨단 텍스트 투 2D 이미지와 텍스트 투 3D 메쉬 모델을 능가한다는 것을 보여준다. 종합적으로, 우리의 연구 결과는 확산 모델 파이프라인에서 언어 에이전트를 통합하는 유망한 역할, 특히 생성 AI의 향후 응용에서 특정 속성 요구 사항을 가진 객체를 구성하는 데 중요한 역할을 나타낸다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '언어 모델의 공간 이해 많은 연구가 언어 모델의 공간 이해 능력을 탐구했다. Janner et al. (2018)은 에이전트 액션과 보상이 있는 시뮬레이션 환경에서 LSTM(Hochreiter and Schmidhuber, 1997)의 공간 추론을 탐구했지만, 그들의 2D 그리드 세계 환경은 우리의 작업에서 고려된 3D 모델링 컨텍스트보다 특히 간단하다. Abdou et al. (2021) 및 Patel and Pavlick (2022)은 언어 모델들이 방향성 정보에 대한 내부 표현들을 개발한다는 것을 입증하였다. 추가적으로, (Mirzaee et al., 2021)은 언어 설명에 기초한 공간 추론을 위한 질문-답변 벤치마크를 도입하였다. LLM이 공간 추론 능력을 나타내고 공간 구성에 대한 피드백을 제공하도록 효과적으로 촉구될 수 있다고 주장하는 것이 합리적이다.\n' +
      '\n' +
      '에이전트LLM 에이전트로서의 대형 언어 모델(Ge et al., 2023; Park et al., 2023; Shen et al., 2023; Gupta and Kembhavi, 2023; Wu et al., 2023; Yin et al., 2023)은 대형 모델에 기초하여 구축된 인공 지능 시스템의 새로운 카테고리를 나타낸다. 이러한 에이전트는 외부 환경과 행동, 추론 및 상호 작용할 수 있다. LLM은 자체적으로 제한된 실행 능력을 가지고 있지만, 외부 API 및 지식 소스와 통합될 때, 광범위한 태스크를 해결할 수 있다(Schick et al., 2023). 반복적 접근은 ReAct(Yao et al., 2022)에 의해 입증되고 게임에 적용되는 구체화된 작업(Wang et al., 2023; Lin et al., 2023), 웹 네비게이션(Yao et al., 2022), 및 로봇 네비게이션(Huang et al., 2022)에 의해 입증되는 바와 같이, 자연 언어 처리 작업을 해결하는 데 유익한 것으로 나타났다. 3D 메쉬를 생성하기 위해 LLM을 사용하는 우리의 접근법은 이 개발 분야에서 연구를 확장하는 데 기여한다.\n' +
      '\n' +
      '텍스트 대 3D 모델에 대한 많은 연구가 텍스트 대 3D 생성을 위해 미리 훈련된 텍스트 대 2D 이미지 확산 모델을 어떻게 적응시킬지를 탐색하고 있다(Poole et al., 2022; Lin et al., 2023; Wang et al., 2023). 이는 텍스트 대 3D 모델에서도 유사한 과제가 발견됨을 시사한다. 한편, LLM 기반 접근법은 텍스트 대 3D 메쉬 생성에 대한 새로운 관점을 도입하여 잠재적으로 유통 외 샘플 문제를 해결할 수 있는 방법을 제공한다.\n' +
      '\n' +
      '그림 2: GPT-4는 블렌더에서 파이썬 스크립트를 작성함으로써 ShapeNet으로부터 세 가지 유형의 객체를 구성하려고 한다. 램프와 같은 간단한 아이템을 성공적으로 만들 수 있지만 테이블, 비행기 등 더 복잡한 오브젝트와 함께 도전에 직면해 있습니다.\n' +
      '\n' +
      '## 3 3GO 프레임워크\n' +
      '\n' +
      '전체 3D 객체를 한 번에 생성하는 데 가장 큰 문제는 공간 부정확성을 복잡하게 만드는 경우가 많다는 것이다. 우리는 3D 메쉬의 생성을 별개의 부분으로 분해하고 각 구성요소를 단계별로 배치하는 것을 제안한다. 우리는 간단한 3D 환경에서 피드백을 수집하고 3D 생각의 사슬로부터 액션을 실행할 수 있는 에이전트인 L3GO라는 접근 방식을 명명한다. 이 접근법은 객체 구성에 대한 단일 시도를 반복적인 피드백 수집 및 수정 프로세스로 변환하여 블렌더 환경으로부터의 피드백의 통합을 가능하게 한다. 우리의 프레임워크는 LLM 기반 추론 및 연기의 이전 작업에서 아이디어를 차용하지만(Yao et al., 2022; Wang et al., 2023), 실용적인 3D 모델링 소프트웨어에서 3D 메쉬 구성에 채택되었다.\n' +
      '\n' +
      '### SimpleBlenv\n' +
      '\n' +
      '에이전트가 쉽게 액션 명령을 제출하고 바운딩 박스 정보, 배치 오류 등 환경 피드백을 받을 수 있는 블렌더 위에 구축된 환경 SimpleBlenv를 소개합니다. 환경을 위한 코드를 공개할 계획입니다.\n' +
      '\n' +
      '액션 공간: 블렌더에서 전문 3D 디자이너는 복잡한 3D 모델을 생성할 수 있으며 거의 무한한 범위의 액션을 마음대로 사용할 수 있습니다. 블렌더의 UI에서 사용자가 수행할 수 있는 거의 모든 액션에는 해당하는 파이썬 API가 있지만, 단순화를 위해 5가지 기본 모양 프리미티브 API에 초점을 맞추기로 선택한다. 이들 API는 primitive_cube_add, primitive_cylinder_add, primitive_cone_add, primitive_uv_sphere_add 및 primitive_torus_add이다. 이름에서 알 수 있듯이, 이들은 각각 정육면체, 원통, 원뿔, 구, 원환체를 추가하는 데 사용된다.\n' +
      '\n' +
      '이러한 API 함수는 복잡한 인수 집합과 함께 제공됩니다. LLM이 이러한 기능을 더 쉽게 사용할 수 있도록 축척, 위치 및 반경과 같은 몇 가지 주요 매개변수만 필요한 랩퍼로 각 기능을 랩핑한다. 결과적으로 L3GO 에이전트가 사용할 수 있는 작업 범위에는 모양 작성 명령마다 크기, 위치, 반지름 등에 대한 다양한 설정이 포함됩니다. 이에 대한 예시는 다음의 예에서 볼 수 있다:\n' +
      '\n' +
      '```\n' +
      'defcreate_cube(name,location,scale):bpy.ops.mesh.primitive_cube_add(size=1,location=location) cube=bpy.context.object cube.name=name cube.scale=scale returncube\n' +
      '```\n' +
      '\n' +
      '우리가 만든 모든 액션 래퍼 API의 자세한 목록은 부록을 참조하십시오. 5개의 기본 모양 API만 사용함에도 불구하고 에이전트는 다른 컨트롤 중에서 크기 조정, 위치 지정 및 반지름 조정에 대한 유연성 덕분에 다양한 방식으로 객체를 생성할 수 있다.\n' +
      '\n' +
      '관찰 및 환경 피드백 우리는 전역 좌표에서 x, y, z 축 및 위치 측면에서 크기를 포함하여 지금까지 생성된 객체 부품의 목록으로 상태 공간 표현을 유지한다. 환경 피드백과 관련하여 에이전트가 액션을 선택한 후 블렌더에서 액션을 실행하여 가상 공간에 메쉬를 생성한다. 이로부터, 우리는 (a) 객체 부품들의 바운딩 박스 치수들과 같은 정보를 추출할 수 있고, (b) 에이전트에 의해 구축된 부품이 임의의 다른 부품들과 교차하는지 또는 부품들 사이에 불필요한 갭이 존재하는지를 체크할 수 있다(예를 들어, 도 4 참조). 우리는 블렌더로부터 이 정보를 직접 수집하기 위한 기능들의 세트를 구축했다. 그런 다음 이 피드백은 다음 작업을 수행하기 전에 문자 메시지로 L3GO 에이전트에 릴레이됩니다.\n' +
      '\n' +
      '### L3GO: LLM 기반의 객체 3D 생성\n' +
      '\n' +
      '본 절에서는 텍스트로부터 3D 메쉬 생성을 위해 특별히 설계된 LLM 에이전트인 L3GO를 소개한다. L3GO는 6개의 구성 요소로 구성되며, 각각은 _generator_ 또는 _critic_로 기능하는 언어 모델에 의해 구동된다. 시각적 개요를 위해 그림 3의 개략도가 표시된다.\n' +
      '\n' +
      '요소 사양 생성기:L3GO는 먼저 LLM이 객체의 가장 중추적인 부분을 식별하도록 프롬프트한다. 이 중추적인 부분은 후속 구성 요소를 더 쉽게 부착할 수 있게 합니다. 예를 들어, 의자의 좌석부터 시작하는 것은 다리와 등받이를 추가하는 것이 간단하기 때문에 실용적이며, 다른 부분에 대한 좌표 계산을 단순화한다. 부품의 이름을 지정한 후 에이전트는 크기 생성기를 사용하여 x, y 및 z 축에 해당하는 너비, 깊이 및 높이 측면에서 합리적인 치수를 결정한다.\n' +
      '\n' +
      '부품 사양 비평: 부품 이름이 제안되면 부품 사양 비평의 검토를 거칩니다. 이 단계는 모호성을 피하기 위해 중요하며, 이는 나중에 에이전트를 혼란스럽게 할 수 있다. 예를 들어 의자를 만들면서 "다리"가 제안되면 에이전트는 "앞쪽 오른쪽 다리"와 같은 공간 기술자가 없으면 정확한 배치를 알 수 없다. 부품 사양 비평가의 역할은 이러한 모호한 설명을 식별하고 수정하여 부품 사양 생성기가 그에 따라 제안을 수정할 수 있도록 하는 것이다. 프로세스는 부품 사양 비평가의 승인 후에만 진행됩니다.\n' +
      '\n' +
      '공간 사양 생성기: 요소 이름과 크기를 설정한 후 모델은 이미 구성된 요소를 고려하여 요소의 공간 요구 사항을 고려합니다. (첫 번째 부분의 경우, 우리는 단지 중심에 위치시킨다.) 에이전트는 새로운 컴포넌트를 부착할 가장 적절한 베이스 부분을 선택하는 것으로 시작하여, 그들 사이의 공간적 관계를 결정한다. 예를 들어 동체를 기단으로 하고 왼쪽 날개를 새로운 부분으로 하여 비행기를 건설하는 경우, 동체의 왼쪽 측면 중앙에 날개를 부착하는 것이 전형적인 공간적 요건일 것이다.\n' +
      '\n' +
      '좌표 계산기: 공간 요구 사항과 기준 요소의 위치를 기반으로 이 구성요소는 새 요소의 중심 좌표를 계산합니다. 경미한 오정렬도 전반적인 정확성에 영향을 미칠 수 있기 때문에 여기에서 정확성은 중요하다. 정밀성을 보장하기 위해 에이전트는 파이썬 실행 환경에 액세스할 수 있으며, LLM을 사용하여 새 부품의 위치를 계산하기 위한 파이썬 코드를 생성할 때만 사용합니다. 이 접근법은 Gao 등(2023)에 기술된 것과 유사하다. 신뢰성을 높이기 위해, 프로세스는 세 번 반복되고 (임의로 넥타이가 끊어진) 이러한 반복으로부터 다수결에 기초하여 x, y, z 좌표를 결정한다.\n' +
      '\n' +
      '실행 액션: 크기와 공간 위치를 결정한 후 에이전트는 LLM에 큐브, 실린더, 원뿔, 구 및 원환체 중에서 선택하여 부품의 모양을 결정하도록 요청한다. 그런 다음 에이전트는 크기, 위치 및 모양 유형을 지정하는 블렌더에 대해 유효한 파이썬 스크립트를 작성합니다. 마지막으로 에이전트는 믹서기에서 부품의 메쉬를 생성하기 위한 명령을 실행합니다. 이 코드는 헤드리스 모드에서 블렌더에서 실행되며, 환경은 다음 모듈에서 사용되는 각 생성된 부품의 바운딩 박스와 같은 중요한 피드백을 제공한다.\n' +
      '\n' +
      'Spatial Critic:Blender 코드를 실행한 후, 새롭게 생성된 파트가 기존 파트와 연결이 끊겼는지 에이전트에게 알려주는 _continuity check_와 새롭게 생성된 파트가 완전히 기존 파트에 포함되어 있는지 에이전트에게 알려주는 _total overlap check_ 두 가지 최종 공간 정확성 검사가 수행된다. 어느 하나의 이슈가 발생하면, 프로세스는 공간 요구사항 생성 단계로 되돌아가고 에이전트는 그에 따라 조정한다. 그림 4의 공간 오차 예제를 참조하십시오.\n' +
      '\n' +
      'Completion Critic: 최종 단계는 객체에 대한 3D 메쉬의 구성이 완료되었는지 여부를 결정하는 것이다. 이를 위해, 이 비평가는 구축되는 대상의 이름과 이미 구축되어 있는 그 부분의 목록을 LLM에 제공하여 이항 완성의 결정을 내린다. 비평가에서 불완전하다고 예측하면 부품 사양 생성기로 다음 반복을 시작합니다. 작업이 완료되면 ControlNet을 사용하여 보다 자연스러운 이미지 생성을 진행합니다.\n' +
      '\n' +
      '3D Meshes\\(\\rightarrow\\) 2D 영상을 위한 ControlNet은 L3GO 에이전트가 3D 메쉬를 생성한 후, 객체를 그레이 스케일의 영상으로 렌더링한다. 그런 다음 이 이미지를 캐니 에지 검출과 함께 컨트롤넷에 공급하여 보다 사실적으로 보이는 이미지를 생성한다.\n' +
      '\n' +
      'L3GO는 텍스트 기반이므로 비주얼을 사용하지 않는다는 점에 유의하십시오.\n' +
      '\n' +
      '그림 4: SimpleBlenv에서 제공하는 두 가지 유형의 오류 피드백: (a) 새로 추가된 큐보이드(오렌지색)는 기본 실린더 내부에 완전히 있다. (b) 새로 추가된 큐보이드와 베이스 실린더 사이에는 불필요한 공간적인 간격이 존재한다.\n' +
      '\n' +
      '도 3: (상단): 블렌더의 상부에 래퍼 환경인 SimpleBlenv, 여기서 LLM은 원자 빌딩 블록을 사용하여 3D 메시를 구성할 수 있다. (하) : L3GO의 모식도.\n' +
      '\n' +
      '정보. 따라서 모델에 객체를 만들 것을 요청할 때 어떤 방향이 전면인지 후면인지와 같은 공간 방향을 정의하는 것을 포함하여 모든 통신은 텍스트 기반이어야 한다. 우리는 시공 과정을 안내하기 위해 프롬프트에 이러한 공간 가정을 미리 설정했다. 달리 명시되지 않는 한, 우리는 실험에서 L3GO에서 GPT-4를 기본 LLM으로 사용한다.\n' +
      '\n' +
      '##4 ShapeNet에 대한 실험\n' +
      '\n' +
      '이 섹션에서는 텍스트 대 2D 이미지와 텍스트 대 3D 메쉬 생성을 비교하여 L3GO와 비교하는 기준선 방법(SS4.1)을 자세히 설명한다. 본 논문에서는 ShapeNet에서 잘 알려진 13개의 카테고리에 초점을 맞추어 간단한 3D 객체 생성에서 LLM 기반 방법의 효율성을 입증한다. 자동 평가를 위해 단순 객체의 3차원 메쉬(SS4.2)를 인식하기 위한 평가자로 GPT-4V를 사용한다. 또한 인간 평가와 GPT-4V의 자동화된 평가(SS4.2)가 잘 연관되어 있음을 보여준다.\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '블렌더에서 작동하도록 설계된 기존 LLM 에이전트가 없기 때문에 기준 참조 역할을 하는 다양한 알고리즘을 선택했다. 원래 자연어 처리 작업을 위해 고안된 이 기준선을 블렌더 환경 내에서 작동하도록 조정하여 실험 프레임워크와 일치하도록 했다.\n' +
      '\n' +
      'ReAct-BReAct(Yao et al., 2022)는 언어 모델 기반 에이전트를 구현하기 위해 설계된 프레임워크이다. 이 프레임워크에서 에이전트는 어떤 행동을 취하기 전에 자신의 사고 과정을 출력한다. 이 작업을 수행한 후 외부 환경에서 수집한 관찰은 후속 단계를 알려줍니다. 섹션 3에 요약된 환경 피드백, 관찰 및 작업 공간을 활용하여 SimpleBlenv 설정에서 ReAct를 구현합니다. ReAct의 텍스트 버전과 구별하기 위해 구현을 ReAct-Blender 또는 ReAct-B라고 약칭합니다.\n' +
      '\n' +
      'Reflexion-BReflexion(Shinn et al., 2023)은 반사의 추가 단계를 추가하여 ReAct 프레임워크를 기반으로 한다. ReAct에서 에이전트는 자신의 추론을 요약하고 행동을 취한 다음 환경으로부터 피드백을 받는다. 반사는 한 걸음 더 나아간다 - 환경 피드백을 받은 후, 에이전트는 다음 움직임을 알리기 위해 취한 행동과 결과에 대한 반성에 관여한다. 설정에서 모든 반복이 끝나면 현재 객체 부분, 이전에 구축된 부분 및 현재 환경 피드백을 고려한다. 그런 다음 에이전트가 방금 만든 객체 부분의 크기와 배치를 반영하도록 요청합니다. 에이전트가 자신의 생각을 공유한 후, 우리는 현재 부분을 다시 할 것인지 아니면 계속 할 것인지 결정하도록 촉구한다. 에이전트가 다시 실행하기로 선택하면 다음 단계에서 해당 에이전트의 반영 통찰력이 사용됩니다.\n' +
      '\n' +
      'Gpt-4 덜 구조화된 접근법의 경우 GPT-4를 사용하여 한 번의 시도에서 객체를 생성하는 데 필요한 전체 파이썬 코드를 생성한다. GPT-4에 사용한 프롬프트는 Github 저장소(gd3kr, 2023)에서 채택되었으며, 이는 우리가 아는 한 GPT-4를 사용하여 블렌더를 제어하는 오픈 소스 프로젝트를 처음으로 제시했다. 전체 프롬프트는 부록을 참조하십시오.\n' +
      '\n' +
      'Dataset: ShapeNet-13 기본 메쉬 생성 능력을 평가하기 위해 ShapeNet의 13가지 범주를 사용한다: [Choy et al., 2016]에 의해 소개된 바와 같이 [\'airplane\', \'bench\', \'cabinet\', \'car\', \'chair\', \'display\', \'lamp\', \'loudspeaker\', \'rifle\',\'sofa\', \'table\', \'telephone\', \'watercraft\'.\n' +
      '\n' +
      '### GPT-4V를 이용한 자동평가\n' +
      '\n' +
      '평가 프로세스를 간소화하기 위해 GPT-4V를 사용하여 메쉬 구성의 성능을 평가하는 것을 제안한다. 각 객체 범주에 대해 GPT-4, ReAct-B, Reflexion-B 및 L3GO에서 10개의 메쉬를 생성한다. 에이전트가 메쉬 구성을 마친 후, 동일한 높이에서 객체를 중심으로 카메라를 회전시켜 10개의 다른 뷰에서 객체를 렌더링한다. 따라서 메쉬당 10개의 이미지가 생성됩니다. 그런 다음 한 번에 10개의 이미지를 GPT-4V에 공급하고 다음과 같은 프롬프트를 사용합니다. \'_이 이미지에서 어떤 객체가 보입니까? 단일 개체 이름으로 응답합니다. 대답은 다음 옵션 중 하나여야 합니다. [비행기, 벤치, 캐비닛, 자동차, 의자, 디스플레이, 램프, 확성기, 소총, 소파, 테이블, 전화, 수상함]_.\n' +
      '\n' +
      '표 1은 서로 다른 객체 범주에 걸친 평균 정확도를 나타낸다. ReAct-B, Reflexion-B 및 L3GO를 포함한 구조화된 방법이 성능에서 GPT-4를 능가한다는 것은 분명하다. 특히, 이러한 구조화된 접근법 중 L3GO가 가장 효과적인 것으로 입증된다. 그림 5에 자세히 설명된 바와 같이 각 물체에 대한 결과를 살펴보면 L3GO가 특히 비행기, 소총과 같은 복잡한 물체를 구성하는 데 탁월하다는 것이 분명해진다.\n' +
      '\n' +
      '인간 평가와의 상관 관계 GPT-4V의 평가가 인간 판단과 일치하는지 여부를 평가하기 위해 인간 참가자를 모집한다. 각 메쉬에 대해 10개의 서로 다른 각도에서 10개의 이미지(GPT-4V 평가를 위해 위와 동일한 이미지)를 사용하고 인간 참가자에게 이러한 이미지를 13개의 객체 범주 중 하나로 분류하도록 요청한다. 우리\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & Human & L3GO & ReAct-B & Reflexion-B & GPT-4 \\\\ \\hline GPT-4V & 0.877 & **0.6** & 0.423 & 0.4 & 0.346 \\\\ Human & 0.894 & **0.584** & 0.385 & 0.403 & 0.445 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: GPT-4V(상행) 및 인간(하행)에 의해 평가된 ShapeNet-13 상의 상이한 LLM 기반 에이전트의 평균 정확도; 각 셀은 평균 130회 이상의 시험이다. 컬럼 네임에서 \'인간\'은 인간이 설계한 오리지널 ShapeNet mesh를 의미하며, 이를 상한으로 간주할 수 있다. 우리는 L3GO가 다른 GPT-4 기반 에이전트(예: ReAct-B, Reflexion-B 및 수정되지 않은 GPT-4)를 능가한다는 것을 안다.\n' +
      '\n' +
      'GPT-4 및 L3GO에 의해 생성된 메시뿐만 아니라 원래의 ShapeNet 메시를 보여줌으로써 130개의 인간 응답을 수집하여 총 390개의 인간 응답을 수집한다. 카테고리별 결과는 그림 6과 같다. 전체 패턴을 볼 수 있는데, 원래 ShapeNet이 "램프", "벤치", "확성기"와 같은 몇 가지 경우를 제외하고 L3GO가 GPT-4보다 높은 정확도를 가지고 있다.\n' +
      '\n' +
      '또한 동일한 이미지 세트를 사용하여 GPT-4V에서 390개의 응답을 수집한다. GPT-4V를 다른 상위 비전 언어 모델에 대해 벤치마킹하기 위해 BLIP-2 및 InstructBLIP에서 각각 390개의 응답을 얻는다. 인간 대 인간 상관관계 평가와 관련하여 4명의 참가자에게 390개의 이미지를 모두 분류하도록 요청했다. 그러나 응답이 다른 세 가지와 가장 자주 다른 참가자에서 데이터를 제외한다. 그런 다음 이러한 모델과 세 명의 인간 평가자 간의 상관 관계를 측정하기 위해 코헨의 카파 점수를 계산하여 표 3과 같이 이러한 상관 관계를 평균화한다. 우리의 연구 결과는 GPT-4V의 반응이 인간 간의 일치도 완벽하지 않다는 점에 유의하는 것이 중요하지만 인간의 판단과 가장 밀접하게 일치한다는 것을 나타낸다.\n' +
      '\n' +
      '## 5 UFO에 대한 실험: 비전통적으로 실현 가능한 객체 구성\n' +
      '\n' +
      '우리의 이전 실험은 L3GO가 ShapeNet으로부터 약 60%의 시간 동안 간단한 객체를 정확하게 구성할 수 있음을 보여준다. 그러나, 현대 확산 기반 모델들은 주어진 ShapeNet 객체의 이미지를 거의 항상 생성할 수 있다. 이것은 부분적으로, 예를 들어 "자동차" 또는 "벤치"의 많은 가능한 유효한 인스턴스들이 있기 때문이다. 그렇다면: L3GO와 같은 방법을 사용하는 데 잠재적인 실용적인 이점이 있습니까?\n' +
      '\n' +
      '공간 구성에서 LLM의 잠재적인 이점을 설명하기 위해 보다 정확한 공간 이해가 필요한 벤치마크를 소개한다. 텍스트-이미지 모델을 체계적으로 평가하는 프롬프트 모음인 DrawBench(Saharia et al., 2022)와 PariPrompts(Yu et al., 2022)에서 영감을 얻은 UFO: 1) 구성하기 위해 정확한 공간 이해가 필요한 50개의 어려운 프롬프트 세트, 2) 텍스트 전용 사전 훈련, 예를 들어 "팔걸이가 하나인 의자"에서 발생할 가능성이 낮다는 점에서 이례적이다.\n' +
      '\n' +
      'UFO의 프롬프트는 9개의 객체 범주에 걸쳐 있으며 각 프롬프트는 "5개의 다리가 있는 의자", "2개의 손잡이가 있는 머그" 등과 같은 다양한 특성을 가진 공통 객체의 조합이다. 전체 프롬프트 목록은 부록에 나와 있습니다. 우리는 일상적인 3D 객체에 초점을 맞추고, 비전통적인 객체를 생성하는 고유한 능력으로부터 프롬프트를 정확하게 해석하는 데 있어 모델의 성능을 분리하는 데 도움이 된다. 소파, 의자, 램프 및 테이블과 같은 조립하기 쉬운 항목을 사용하여 모델의 객체 생성 기술에서 후속하는 프롬프트로 인해 부족한 점이 있는지 더 잘 식별할 수 있습니다.\n' +
      '\n' +
      '기준선 우리는 LLM 기반 접근 방식을 DALL-E 3(Betker et al., ), Stable Diffusion XL(SDXL) (Podell et al., 2023), Shap-E(Jun and Nichol, 2023)와 같은 최신 텍스트 대 2D 및 텍스트 대 3D 방법과 비교한다. DALL-E 3은 OpenAI의 텍스트-이미지 확산 모델의 이전 버전인 DALL-E 2(Ramesh et al., 2022)의 프롬프트 후속을 개선하기 위해 기술 합성 캡션을 사용한다. 안정적인 확산 XL은 오픈 소스 텍스트 대 이미지 확산 모델이다. Shap-E(Jun and Nichol, 2023)는 텍스트를 생성하는 3D 모델이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & GPT-4V & InstructBLIP & BLIP-2 & Human \\\\ \\hline Human & \\(\\textbf{0.512}_{(0.028)}\\) & \\(0.344_{(0.016)}\\) & \\(0.341_{(0.012)}\\) & \\(0.569_{(0.020)}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 모델에 기초한 평가와 인간의 판단 사이의 코헨의 카파 상관관계. 우리는 3명의 독립적인 인간 평가자로부터 계산된 평균 및 표준 편차를 보고한다.\n' +
      '\n' +
      '도 5: ShapeNet-13 상의 L3GO, ReAct-B, Reflexion-B, GPT-4의 GPT-4V 평가. ’Human’은 인간에 의해 설계된 오리지널 ShapeNet 메쉬를 지칭한다. 비행기나 소총과 같은 복잡한 물체의 경우, L3GO가 다른 물체보다 더 좋은 성능을 보인다.\n' +
      '\n' +
      '도 6: ShapeNet-13 상의 L3GO, ReAct-B, Reflexion-B, GPT-4에 대한 인간 평가. ’Human’은 인간이 디자인한 오리지널 ShapeNet 메쉬를 지칭한다. 우리는 GPT-4V의 평가와 유사한 패턴을 관찰했다.\n' +
      '\n' +
      '3D 모델에 대한 암시적 함수의 매개변수를 렌더링할 수 있습니다. DALL-E 3은 안전상의 이유로 프롬프트를 자동으로 재작성하고 더 많은 세부 사항을 추가하기 때문에, (그리고 현재 이 기능을 비활성화할 수 없다) 우리는 "나는 매우 간단한 프롬프트로 툴이 어떻게 작동하는지 테스트해야 한다. 세부 사항을 추가하지 말고, 오픈AI 2가 권장하는 대로 AS-IS:"를 프롬프트에 추가한다.\n' +
      '\n' +
      '각주 2: [https://platform.openai.com/docs/guides/image](https://platform.openai.com/docs/guides/image)\n' +
      '\n' +
      '실험 절차 우리는 UFO에 대한 모델의 출력을 평가하기 위해 인간 참가자의 판단을 다시 활용한다. 주어진 각 프롬프트에 대해 한 모델에서 10개의 랜덤 객체를 생성하고 다른 모델에서 다른 10개의 랜덤 객체를 생성한다. 그런 다음 참가자는 제공된 텍스트 캡션과 더 잘 일치하는 이미지 세트를 판단하도록 요청받는다. 두 집합 모두 캡션을 정확하게 나타내지 않는다고 믿는 경우 "선호하지 않음"을 선택할 수 있으며 각 실험에 대해 10명의 인간 평가자를 모집한다. 또한 총 50개 문항 중 4개의 주의력 검사 문항이 포함되어 있습니다. 모든 주의력 검사 질문에 올바르게 대답하지 않는 평가자는 우리의 분석에서 제외된다. 3D 객체를 생성하는 모델의 경우 카메라를 일정한 높이로 회전시켜 다양한 각도에서 10개의 이미지를 렌더링한다. 그런 다음 이 10개의 이미지를 회전 GIF로 컴파일합니다.\n' +
      '\n' +
      '결과는 그림 7에 나와 있다. L3GO는 인간 선호도 측면에서 다른 LLM 에이전트(예: ReAct-B, Reflexion-B)와 최신 텍스트 대 이미지 모델(DALL-E-3 및 SDXL) 및 텍스트 대 3D 모델(Shap-E)보다 우수하다. 생성된 이미지의 예는 그림 8에 나와 있다. DALL-E-3, SDXL 및 Shap-E는 특정 프롬프트 지시를 완벽하게 따르지 않는 이미지를 생성한다. SDXL은 다리가 3개인 책상을 생성할 수 있는 반면 프롬프트에서 묻지 않는 의자가 추가로 생성된다. DALL-E-3는 프롬프트의 특정 요구 사항을 완전히 무시하는 것 같다. 대조적으로, 그들의 디자인은 완벽하지 않지만 언어 모델 기반 에이전트는 적절한 수의 다리를 가진 의자를 구성할 수 있다. 이러한 결과는 구조화된 추론이 불충분한 훈련 데이터에 의해 제기된 문제를 완화하기 위한 실행 가능한 전략 역할을 할 수 있음을 시사한다.\n' +
      '\n' +
      'UFO에 대한 평가에 대한 배경 및 질감의 영향 텍스트-이미지 모델 및 LLM 기반 방법에 의해 생성된 이미지의 배경 및 질감 차이가 인간 평가에 어떤 영향을 미칠 수 있는지 살펴본다. 이를 테스트하기 위해 이러한 텍스트 대 이미지 모델로 프롬프트 스타일을 변경합니다. DALL-E-3의 경우 "[객체명] 배경이 검은색이고 객체가 회색색 3D 모양임 확인"을 사용한다. 안정한 확산 XL의 경우 "[객체명], 검은색 배경, 회색색 3D 모양임 확인"을 사용한다. 또한 확산 모델이 텍스트 프롬프트를 얼마나 밀접하게 따르는지 결정하는 안정한 확산 XL의 유도 척도를 변경한다. 두 시나리오에서, 우리는 그림 9와 같이 L3GO가 인간 선호도 측면에서 텍스트 대 이미지 모델보다 우수함을 관찰하고 GPT-4V가 UFO에 대한 평가자 역할을 할 수 있는지 확인하기 위해 초기 테스트를 수행했다. 그러나 20% 이상의 사례에서 GPT-4V가 답변을 제공하지 않는 것으로 관찰되었으며, 이는 생성된 이미지의 특성과 관련이 있을 수 있다. 자세한 내용은 부록의 표 4를 참조한다.\n' +
      '\n' +
      '## 6 Ablation Study\n' +
      '\n' +
      '우리는 3D 메쉬 생성을 위한 전체 성능에 가장 큰 영향을 미치는 구성 요소를 확인하기 위해 3개의 시스템 설계 선택을 삭제한다. 이를 위해 GPT-4V를 이용한 자동평가를 통해 1) 공간평론기 없음, 2) 프로그램 기반 좌표계산기 없음, 3) LLM의 선택 등 3가지 성능비교를 하였다. 각 설정에 대해 카테고리당 10개의 객체를 생성하고 서로 다른 각도에서 10개의 이미지를 렌더링한다. 1) 및 2)의 경우 ShapeNet의 13개 범주(GPT-4V에 의해 평가됨)에 걸친 평균 정확도는 각각 0.515 및 0.585이다. 이와 비교하여, L3GO는 0.6의 더 높은 점수를 달성한다. L3GO에 대한 평균 점수와 좌표 계산기가 없는 평균 점수는 유사한 점수를 달성하지만, 후자는 캐비닛 및 자동차 범주 모두에서 0점을 달성한다. 대조적으로, L3GO는 프로그램 기반 좌표 계산을 사용할 때, 이들 카테고리에 대해 각각 0.5 및 0.4의 점수를 달성한다.\n' +
      '\n' +
      '오픈 소스 LLMs GPT-4 대신 오픈 소스 LLM의 사용을 탐색합니다. 이를 위해 믹스트럴을 사용합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline \\multicolumn{1}{c}{Mixtral-8\\%7B} & w/o spatial critic & w/o program-based & L3GO \\\\ \\hline\n' +
      '0.138 & 0.515 & 0.585 & **0.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 절제 연구. 우리는 GPT-4V를 평가자로 사용하여 ShapeNet의 13개 범주를 기반으로 성능을 평가한다. 각 셀은 평균 130번의 시도이다. \'w/o 공간 평론가/프로그램 기반\'은 공간 평론가가 없고 프로그램 기반 좌표 계산 모듈이 없는 GPT-4 기반의 L3GO를 의미한다. ‘Mixtral-8x7B’는 GPT-4 대신 Mixtral-8X7B를 기준으로 하는 ReAct-B를 의미한다.\n' +
      '\n' +
      '도 7: L3GO 대 인간 선호도. DALL-E-3, SDXL, Shap-E, ReAct-B, Reflexion-B on UFO.\n' +
      '\n' +
      '대부분의 벤치마크 테스트에서 Llama-2 70B 및 GPT-3.5의 성능과 일치하거나 능가하는 것으로 알려진 희박한 혼합-전문가 모델(Jiang et al., 2024)인 8x7B이다. 우리는 GPT-4V를 평가 도구로 사용하여 ReAct-B와 ShapeNet-13을 사용하여 실험을 수행한다. 대부분의 형상 범주에서 ReAct-B(믹스트랄-8x7B)의 정확도는 0에서 0.1 사이인 반면 소파, 램프 및 표 범주는 각각 0.3, 0.3 및 0.7의 더 높은 점수를 달성했다. 이것은 더 단순한 모양과 더 쉬운 인식 가능성 때문일 수 있다. 평균 정확도 점수는 0.138으로 표 1과 같이 ReAct-B(GPT-4)가 달성한 정확도 0.423보다 현저히 낮다. 이는 3차원 공간에 대한 정확한 이해를 요구하는 메쉬 객체를 구성하는 작업이 여전히 GPT-4 수준에서 발견되는 추론 능력을 필요로 한다는 것을 나타낸다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '3D 모델링 소프트웨어인 블렌더를 위해 개발한 API를 통해 텍스트 명령어로부터 3D 객체를 생성하도록 설계된 언어 에이전트인 L3GO를 도입했다. 쉐이프넷의 13개의 가장 큰 객체 범주를 사용한 평가는 L3GO가 GPT-4, ReAct 및 Reflexion과 같은 다른 모델에 비해 우수한 능력을 보여준다. 또한, 우리는 비전통적인 특성을 가진 공통 객체를 생성하는 생성 AI 모델의 능력을 테스트하기 위한 일련의 도전 과제인 UFO를 고안했다. L3GO의 성능은 언어 모델의 적용 범위에서 상당한 발전을 나타낸다. 예를 들어, 확산 모델은 구조화된 프롬프트에 의해 생성된 비전통적인 데이터로 더욱 개선될 수 있다. 또한 언어 모델이 내부 모델 표현으로 공간 정보를 처리하는 방법을 분석하면 3D 모델링 능력을 이해하고 개선하는 데 귀중한 통찰력을 얻을 수 있다.\n' +
      '\n' +
      '영향 진술 우리의 연구는 언어 모델을 3D 모델링과 통합하고 잠재적으로 디자인 프로세스를 혁신하고 디지털 환경을 만들 수 있는 방대한 잠재력을 나타낸다. 이러한 융합은 생성 AI 도구를 보다 직관적이고 창의적인 노력을 지원할 수 있도록 하는 것을 목표로 한다. L3GO를 통해 우리는 이 영역에서 미개발된 가능성을 활용하기 시작하여 광범위한 미래 탐색 및 개발의 단계를 설정한다.\n' +
      '\n' +
      '우리 작품의 긍정적인 사회적 영향은 특히 디자인, 엔지니어링 및 예술 분야에서 상당할 수 있습니다.\n' +
      '\n' +
      '도 8: UFO에 기초하여 이미지를 생성한 예제. LLM 기반 접근법(ReAct-B, Reflexion-B, L3GO)은 원하는 객체를 성공적으로 생성하는 반면, 가장 진보된 텍스트 대 이미지 및 텍스트 대 3D 모델(DALL-E 3, 안정적인 확산 XL, Shap-E)은 여전히 프롬프트를 완벽하게 따르기 위해 고군분투한다.\n' +
      '\n' +
      '도 9: L3GO 대 인간 선호도. UFO 상의 DLL-E-3와 SDXL은 생성된 이미지의 배경을 단순한 회색 음영으로 만들려고 시도한다. 또한 SDXL에 대한 지침 척도를 변경하여 더 나은 프롬프트 팔로우가 \'가이드=15, 30\'으로 표시된 성능을 향상시키는지 확인한다. DALL-E-3의 경우, 향상된 디테일을 위해 \'quality=hd\' 옵션을 사용한다.\n' +
      '\n' +
      '이전에 달성하기 어렵거나 불가능했던 아이디어의 시각화 및 프로토타이핑을 가능하게 합니다. 또한, 우리의 접근법은 상호작용적이고 시각적으로 직관적인 표현을 통해 복잡한 개념을 더 쉽게 접근할 수 있도록 교육 도구를 향상시킬 수 있다. 그러나 오해의 소지가 있거나 유해한 콘텐츠를 만들 수 있는 가능성과 같은 이미지 생성 기술의 진보의 윤리적 의미도 고려해야 한다. 이러한 강력한 도구가 책임감 있고 윤리적으로 사용되도록 하는 메커니즘에 대한 지속적인 연구의 필요성을 강조한다. 우리는 책임과 윤리적 고려와 함께 혁신을 강조하는 균형 잡힌 접근을 옹호한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Abdou et al. (2021) Abdou, M., Kulmizev, A., Hershcovich, D., Frank, S., Pavlick, E., and Sogaard, A. Can Language Models Encode Perceptual Structure With With Grounding? 색채에 관한 사례 연구 In _Proceedings of the 25th Conference on Computational Natural Language Learning_, pp. 109-132, Online, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.conll-1.9.\n' +
      '* Betker et al. (2023) Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., Manassra, W., Dhariwal, P., Chu, C., Jiao, Y., and Ramesh, A. Improving Image Generation with Better Captions. _[https://openai.com/dall-e-3_] (https://openai.com/dall-e-3_).\n' +
      '* Bubeck et al. (2023) Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang, Y. 인공지능의 불꽃: 2023년 4월 GPT-4를 사용한 초기 실험.\n' +
      '* Canny(1986) Canny, J. A Computational Approach to Edge Detection. _ IEEE Transactions on Pattern Analysis and Machine Intelligence_, PAMI-8(6):679-698, November 1986. ISSN 1939-3539. doi: 10.1109/TPAMI.1986.4767851.\n' +
      '* Chefer et al. (2003) Chefer, H., Alaluf, Y., Vinker, Y., Wolf, L., and Cohen-Or, D. Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models. _ ACM Transactions on Graphics_, 42(4):148:1-148:10, July 2023. ISSN 0730-0301. doi: 10.1145/3592116.\n' +
      '* Choy et al. (2016) Choy, C., Xu, D., Gwak, J., Chen, K., and Savarese, S. 3D-R2N2: 단일 및 다시점 3D 객체 재구성을 위한 통합 접근법. In _Proceedings of the European Conference on Computer Vision (ECCV)_, volume 9912, pp. 644, October 2016. ISBN 978-3-319-46483-1. doi: 10.1007/978-3-319-46484-8_38.\n' +
      '* Feng et al. (2022) Feng, W., He, X., Fu, T. - J., Jampani, V., Akula, A. R., Narayana, P., Basu, S., Wang, X. E., and Wang, W. Y. Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis. _The eleventh International Conference on Learning Representations_, 2022년 9월.\n' +
      '* Gao et al. (2023) Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. PAL: Program-aided Language Models. In _Proceedings of the 40th International Conference on Machine Learning_, pp. 10764-10799. PMLR, July 2023.\n' +
      '* gd3kr(2023) gd3kr. BlenderGPT. [https://github.com/gd3kr/BlenderGPT] (https://github.com/gd3kr/BlenderGPT), 2023.\n' +
      '* Ge et al. (2023) Ge, Y., Hua, W., Mei, K., Ji, J., Tan, J., Xu, S., Li, Z., and Zhang, Y. OpenAGI: LLM이 도메인 전문가를 만날 때. 2023년 11월 _30-7차 신경 정보 처리 시스템 데이터 세트 및 벤치마크 Track_ 회의에서.\n' +
      '* Gupta & Kembhavi (2023) Gupta, T. And Kembhavi, A. Visual Programming: Compositional Visual Reasoning without training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 14953-14962, 2023.\n' +
      '* Hochreiter & Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. Long Short-Term Memory. _ Neural Computation_, 9, 1997\n' +
      '* Huang et al. (2022) Huang, W., Xia, F., Xiao, T., Chan, H., Liorence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Brown, N., Jackson, T., Luu, L., Levine, S., Hausman, K., and Ichter, B. Inner Monologue: Embodied Reasoning through Planning with Language Models. _Conference on Robot Learning_, 2022. doi: 10.48550/ARXIV.2207.05608.\n' +
      '* Janner et al. (2018) Janner, M., Narasimhan, K., and Barzilay, R. 근접 공간추론을 위한 표현학습 The Association for Computational Linguistics_, 6:49-61, 2018. doi: 10.1162/tacl_a_00004.\n' +
      '* Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M. - A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral of Experts, 2024년 1월.\n' +
      '* Jun & Nichol(2023) Jun, H. and Nichol, A. Shap-E: Generating Conditional 3D Implicit Functions, May 2023.\n' +
      '* Lin et al. (2018) Lin, B. Y., Fu, Y., Yang, K., Brahman, F., Huang, S., Bhagavatula, C., Ammanabrolu, P., Choi, Y., and Ren, X. Swiftsage: 복잡한 상호 작용 작업을 위한 빠르고 느린 사고를 가진 생성 에이전트. 30-7차 신경 정보 처리 시스템 회의, 2023a. URL[https://openreview.net/forum?id=R2k3GP1HN7](https://openreview.net/forum?id=R2k3GP1HN7).\n' +
      '\n' +
      '* Lin et al. (2023) Lin, C.-H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M. - Y., and Lin, T. -Y. 매직3D: 고해상도 텍스트-3D 콘텐츠 생성. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 300-309, 2023b.\n' +
      '* Mirzaee et al. (2021) Mirzaee, R., Rajaby Faghihi, H., Ning, Q., and Kordjamshidi, P. SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning. Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y. (eds.), _Proceedings of the 2021 Conference of the North American chapter of the Computational Linguistics: Human Language Technologies_, pp. 4582-4598, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.364.\n' +
      '* OpenAI(2023) OpenAI. GPT-4V(비전) 기술 작업 및 저자, 2023.\n' +
      '* Park et al. (2023) Park, J. S., O\'Brien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative Agents: Interactive Simulacra of Human Behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, UIST \'23, pp. 1-22, New York, NY, USA, October 2023. Association for Computing Machinery. ISBN 9798400701320. doi: 10.1145/3586183.3606763.\n' +
      '* Patel & Pavlick(2022) Patel, R. 그리고 Pavlick, E. 언어 모델을 접지된 개념 공간으로 매핑합니다. _International Conference on Learning Representations_, 2022년 1월.\n' +
      '* Podell et al. (2023) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. SDXL: 2023년 7월 고해상도 영상 합성을 위한 잠재 확산 모델 개선\n' +
      '* Poole et al. (2022) Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dream-Fusion: Text-to-3D using 2D Diffusion. _The eleventh International Conference on Learning Representations_, 2022년 9월.\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. 2022년 4월 CLIP Latents를 이용한 계층적 텍스트 조건 이미지 생성\n' +
      '* Rassin et al.(2023) Rassin, R., Hirsch, E., Glickman, D., Ravfogel, S., Goldberg, Y., and Chechik, G. Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment. 2023년 11월, 30-7차 신경 정보 처리 시스템 회의에서.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-Resolution Image Synthesis With Latent Diffusion Models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 10684-10695, 2022.\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Gontijo-Lopes, R., Ayan, B. K., Salimans, T., Ho, J., Fleet, D. J., and Norouzi, M. 딥 언어 이해를 통한 사실적 텍스트-이미지 확산 모델 _Advances in Neural Information Processing Systems_, May 2022.\n' +
      '* Schick et al. (2023) Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. 도구형성기: 언어 모델은 스스로 도구를 사용하도록 가르칠 수 있습니다. 2023년 11월, 30-7차 신경 정보 처리 시스템 회의에서.\n' +
      '* Shen et al.(2023) Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. 포옹 GPT: ChatGPT와 포옹 페이스의 친구로 AI 작업을 해결합니다. 2023년 11월, 30-7차 신경 정보 처리 시스템 회의에서.\n' +
      '* Shinn et al. (2023) Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. 반사: 언어 강화 학습을 하는 언어 에이전트. 2023년 11월, 30-7차 신경 정보 처리 시스템 회의에서.\n' +
      '* Wang et al. (2023a) Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An Open-Ended Embodied Agent with Large Language Models. _NeurIPS 2023 Foundation Models for Decision Making Workshop_, November 2023a.\n' +
      '* Wang et al. (2022b) Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., and Zhu, J. ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation. 2023b년 11월, 30-7차 신경 정보 처리 시스템 회의에서.\n' +
      '* Wu et al. (2023) Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., and Duan, N. 시각적 채팅PT: 시각적 기초 모델로 대화, 그리기 및 편집 2023. doi: 10.48550/ARXIV.2303.04671.\n' +
      '* Yao et al. (2022) Yao, S., Chen, H., Yang, J., and Narasimhan, K. 웹샵: 그라운드 언어 에이전트와 확장 가능한 실세계 웹 상호 작용을 중심으로 2022a년 12월, 신경망 정보 처리 시스템_, 35:20744-20757의 발전.\n' +
      '* Yao et al. (2022b) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. 재작업: 언어 모델에서 추론 및 작업을 동기화합니다. _The Eleventh International Conference on Learning Representations_, September 2022b.\n' +
      '\n' +
      '* Yin et al. (2023) Yin, D., Brahman, F., Ravichander, A., Chandu, K. R., Chang, K. - W., Choi, Y., and Lin, B. Y. Lumso: Learning agents with unified data, modular design, and open-source llms. _ ArXiv_, abs/2311.05657, 2023. URL[https://api.semanticscholar.org/CorpusID:265128672](https://api.semanticscholar.org/CorpusID:265128672).\n' +
      '* Yu et al. (2022) Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., and Wu, Y. 내용이 풍부한 텍스트-이미지 생성을 위한 자기회귀 모델의 스케일링 IMT2000 3GPP - 기계 학습 연구, 2022년 8월. ISSN 2835-8856\n' +
      '* Zhang et al. (2023) Zhang, L., Rao, A., and Agrawala, M. 텍스트 대 이미지 확산 모델에 조건부 제어 추가 In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 3836-3847, 2023.\n' +
      '\n' +
      '## 부록 알고리즘\n' +
      '\n' +
      'L3GO의 의사 알고리즘은 그림 10에 나와 있다.\n' +
      '\n' +
      '## UFO 평가자로서의 부록 B GPT-4V\n' +
      '\n' +
      '또한 UFO의 프롬프트를 사용하여 생성된 이미지를 평가하기 위해 GPT-4V를 사용하는 초기 테스트를 수행했다. 이 실험은 인간 연구와 유사한 두 모델의 10개 이미지 중 GPT-4V 두 세트를 보여주고 GPT-4V에 해당하는 텍스트 캡션을 더 정확하게 나타내는 이미지 세트를 묻는 것을 포함했다. 선택 이유를 제시하면서 \'상행\', \'하행\', \'선호하지 않음\' 중 하나를 선택하도록 한다. L3GO와 DALL-E-3을 이용하여 영상을 생성하였으며, 실험을 2회 수행하였다. 두 경우 모두 프롬프트의 20% 이상(50개 중 13개 및 18개)에 대해 우리가 받은 응답은 "이 요청을 도울 수 없다"였다. 이로 인해 우리는 주요 실험을 위해 GPT-4V 대신 인간 평가자에 의존하기로 결정했다.\n' +
      '\n' +
      '## 부록 C 인간 평가 세부사항\n' +
      '\n' +
      '연구자가 행동 연구를 수행하고 참가자를 모으기 위해 사용하는 온라인 플랫폼 Prolific.com을 통해 참가자를 모집합니다. 연구 자료가 모두 영어로 되어 있었기 때문에 참가자들의 요구 사항은 영어 유창성이었다. 이 연구는 플랫폼의 모든 자격을 갖춘 참가자가 접근할 수 있도록 했다. 우리는 시간당 15달러의 임금을 보장한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & L3GO & DALL-E-3 & No pref & Refuse & No images \\\\ \\hline\n' +
      '제 1 시험 & 12 & 8 & 9 & 18 & 3\\\\\n' +
      '2nd trial & 14 & 12 & 11 & 13 & 0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: GPT-4V가 UFO에 대한 평가로서, GPT-4V는 L3GO와 DALL-E-3에 의한 이미지 생성을 비교한다. "거절"은 예를 들어 "이 요청을 도울 수 없다"는 말로 GPT-4V가 응답을 거부한 경우를 나타낸다. "이미지 없음"은 GPT-4V가 이미지를 보이지 않는 것으로 잘못 인식한 경우를 나타낸다.\n' +
      '\n' +
      '도 10: L3GO의 Pseudo 알고리즘.\n' +
      '\n' +
      '실험 초기에는 참가자에게 실험의 목적, 요약, 방법론, 기밀 보장 및 참여의 자발적 특성을 자세히 설명하는 온라인 양식을 제공한다. 참가자는 응답의 기밀성과 언제든지 철회할 수 있는 권리를 보장받는다. 참가자는 "시작" 버튼을 클릭하여 제공된 정보에 대한 이해와 참가 동의를 인정하도록 요청받는다. 이 단추를 클릭한 후에만 실험이 시작됩니다.\n' +
      '\n' +
      '참가자 기준을 최소한으로 유지하여 편견을 최소화하려는 노력에도 불구하고 컴퓨터와 인터넷에 액세스할 수 있는 영어권 개인에 대한 고유한 편견이 남아 있다. 우리는 이러한 편향이 또한 WEIRD(서양, 교육, 산업, 부자, 민주주의) 인구통계학적 특성을 반영하는 경향이 있다는 점에 주목한다.\n' +
      '\n' +
      '## 부록 D 실험을 위한 추가 세부사항\n' +
      '\n' +
      '그림 11에 표시된 SimpleBlenvare에서 동작 명령에 대한 래퍼 API.\n' +
      '\n' +
      '도 12에 도시된 SimpleBlenvis로부터의 예시적인 환경 피드백.\n' +
      '\n' +
      '기준선 GPT-4에 대한 전체 프롬프트는 그림 13에 나와 있다.\n' +
      '\n' +
      'UFOachair의 프롬프트 목록은 2개의 다리가 있는 의자, 6개의 다리가 있는 의자, 1개의 팔받침이 있는 의자, 3개의 다리가 있는 의자, 5개의 다리가 있는 의자, 6개의 다리가 있는 의자, 1개의 다리가 있는 의자, 1개의 팔받침이 있는 의자, 3개의 다리가 있는 의자, 3개의 다리가 있는 의자, 5개의 다리가 있는 의자, 3개의 다리가 있는 의자, 3개의 다리가 있는 의자, 3개의 다리가 있는 의자, 3개의 다리가 있는 의자, 3개의 다리가 있는 책상, 5개의 다리가 있는 책상, 5개의 다리가 있는 책상, 1개의 다리가 있는 책상, 1개의 다리가 있는 책상, 1개의 다리가 있는 책상, 1개의 다리가 있는 책상, 1개의 다리가 있는 책상, 1개의 다리가 있는 책상, 1개의 다리가 있는 책상, 1개의 다리가 있는 책상, 1개의 다리가 있는 책상, 1개의 다리가 있는 의자, 1개의 다리가 있는 의자, 1개의 다리가 있는 의자, 1개의 다리가 있는 의자, 1개의 다리가 있는 의자, 1개의 다리가 있는 의자, 1개의 다리가 있는 의자, 1개의 다리가 있는 의자, 1개의 다리가 있는 의자, 1개의 다리가 있는 의자, 1개의 다리가 있는 의자, 1개의 다리가 있는 의자, 1개의 다리가 있는 의자, 1개의 다리가 있는\n' +
      '\n' +
      '### 추가로 생성된 예제\n' +
      '\n' +
      '는 도 14 및 도 15에 도시되어 있다.\n' +
      '\n' +
      '코드는 다음 url.5에 업로드됩니다.\n' +
      '\n' +
      '각주 5: [https://u.pcloud.link/publink/show?code=k24e55022VnyJfJO8VHyivUkfVdKhb5B8SEk](https://u.pcloud.link/publink/show?code=k24e55022VnyJfJO8VHyivUkfVdKhb5B8SEk)\n' +
      '\n' +
      '## 부록 E 응용 프로그램 및 향후 작업\n' +
      '\n' +
      '### 객체 부분의 창의적 조합\n' +
      '\n' +
      'UFO에서 우리는 우리의 접근법이 비정형적인 수의 다리를 가진 의자를 만드는 것과 같은 일반적인 물체 특성의 독특한 변화에 어떻게 적응할 수 있는지 탐구했다. 다리는 의자의 표준 기능이지만, 우리의 초점은 비전통적인 다리 수를 실험하는 것이었다. 이 섹션에서는 L3GO가 대상과 일반적으로 연관되지 않는 구성 요소, 예를 들어 날개가 있는 의자를 통합하는 것과 같이 훨씬 더 창의적인 조합을 처리할 수 있는지 확인하기 위해 이 질문을 확장했다.\n' +
      '\n' +
      '그림 16에 표시된 것은 이러한 창의적 프롬프트와 그 결과 모델의 사례이다. "날개"와 "우산"의 형태는 직사각형과 얇은 원통이 있는 길쭉한 구와 같이 기본이지만 L3GO는 "날개"가 의자 측면에 부착되어야 하고 "우산"이 위에 위치되어야 한다는 것을 능숙하게 파악한다. (더 많은 예는 부록 참조.) 우리의 결과는 L3GO가 이러한 특이한 디자인을 성공적으로 해석하고 구성할 수 있음을 보여준다.\n' +
      '\n' +
      '복잡한 소프트웨어를 위한### 언어 인터페이스\n' +
      '\n' +
      '대부분의 3D 모델링 소프트웨어 프로그램은 학습 곡선이 가파르다. 정교한 3D 모델들을 생성하는 것은 종종 복잡한 인터페이스들을 사용하는 일련의 동작들을 수반하며, 이는 비전문가들이 탐색하기 어려울 수 있다. 우리의 방법은 이러한 복잡한 소프트웨어 프로그램을 위한 언어 인터페이스로 사용되어 초보자에게 더 쉽게 접근할 수 있다. 도 17에 예시된 바와 같이, 사용자는 프롬프트를 입력할 수 있고, L3GO는 객체의 초기 드래프트를 생성할 것이다. 우리의 접근 방식은 아직 초기 단계이며 실용화를 위한 추가 개발이 필요하지만, 우리의 연구가 향후 발전에 영감을 주기를 바란다.\n' +
      '\n' +
      '도 11: SimpleBlenv에 대한 액션 래퍼 API들의 전체 리스트.\n' +
      '\n' +
      '## 부록 F 한계\n' +
      '\n' +
      'LLM 기반 방법을 사용하여 생성된 3D 메쉬 객체의 품질은 확산 기반 텍스트 대 3D 방법으로 생성된 것보다 뒤처지며 인간 표준과 동등하지는 않다. 또한 간단한 개체를 만드는 데 약 3~5분이 걸리는 반면, 피드백 후 필요한 재시도 작업의 수에 따라 더 복잡한 개체를 만드는 데 10~20분이 걸릴 수 있다. 향후 연구는 3차원 메쉬를 생성하기 위한 보다 효율적인 LLM 기반 접근 방식을 탐색해야 하며, 이는 궁극적인 3차원 객체 생성을 더 잘 제어할 수 있는 유망한 방향이라고 생각한다.\n' +
      '\n' +
      '그림 14: UFO에 대한 추가 생성 예제.\n' +
      '\n' +
      '도 12: SimpleBlenv로부터의 예시적인 환경 피드백.\n' +
      '\n' +
      '그림 13: 실험에 사용된 기준 GPT-4에 대한 전체 프롬프트. 이 프롬프트는 Blender-GPT({}^{4}\\)에서 가져온 것으로, 이는 Blender 내부에서 GPT-4를 사용하는 최초의 오픈 소스 코드이다.\n' +
      '\n' +
      '그림 16: 대형 언어 모델은 창의적인 프롬프트를 해석할 수 있기 때문에, L3GO는 날개가 달린 의자 또는 우산이 달린 의자와 같은 독특한 아이템을 만들 수 있다.\n' +
      '\n' +
      '그림 15: ShapeNet에서 L3GO의 추가 생성 예.\n' +
      '\n' +
      '도 17: 블렌더 사용자 인터페이스는 신참자들에게 벅찰 수 있다. LLM 에이전트는 초보자를 지원하기 위해 객체의 초안을 작성함으로써 도움이 될 수 있다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>