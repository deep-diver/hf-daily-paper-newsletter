<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '10억 달러 규모의 3D 스케이프 모델 메이크-A-샤프: 10억 규모의 3D 플러프 모델.\n' +
      '\n' +
      'Ka-Hei Hui\\({}^{*}{}^{1}\\)\n' +
      '\n' +
      'Aditya Sanghi\\({}^{*}{}^{2}\\)\n' +
      '\n' +
      'Arianna Rampini\\({}^{2}\\)\n' +
      '\n' +
      '카말 라히미 만록산\\({}^{2}\\)\n' +
      '\n' +
      'Zhengzhe Liu\\({}^{1}\\)\n' +
      '\n' +
      'Hooman Shayani\\({}^{2}\\)\n' +
      '\n' +
      'Chi-Wing Fu\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{*}\\) 이 저자는 동등하게 기여했다.\n' +
      '\n' +
      '중국 홍콩대학({}^{1}}\\)은 중국 홍콩대학({}^{1}\\)이다.\n' +
      '\n' +
      '자동화 AI 랩({}^{2}\\)\n' +
      '\n' +
      '[https://edward1997104.github.io/make-a-shape/](https://edward1997104.github.io/make-a-shape/)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '자연어와 이미지를 위한 대형 생성 모델을 훈련하는 데 상당한 진전이 있었다. 그러나 3D 생성 모델의 발전은 비효율적이고 비압축적이며 덜 표현적인 표현과 함께 훈련에 대한 실질적인 자원 요구로 인해 방해를 받는다. 본 논문은 10백만 개의 공개적으로 이용 가능한 형태를 활용할 수 있는 방대한 규모의 효율적인 교육을 위해 설계된 새로운 3D 생성 모델인 메이크-A-샤페를 소개한다. 기술적으로, 우리는 먼저 계수 관계를 효율적으로 활용하기 위해 서브밴드 계수 필터링 방식을 형성함으로써 형태를 컴팩트하게 인코딩하기 위해 웨이브렛-트리 표현을 혁신한다. 그런 다음 하위 밴드 계수 패킹 방식을 저해상도 그리드에서 표현을 레이아웃하기 위해 고안함으로써 확산 모델에 의해 표현 일반화할 수 있게 한다. 또한, 우리는 거친 웨이브릿 계수를 생성하기 위해 효과적으로 학습하도록 모델을 훈련시키기 위한 서브밴드 적응 훈련 전략을 도출한다. 마지막으로, 다양한 양식, _e._g._, _싱글/멀티뷰 이미지, 포인트 클라우드 및 저해상도 복셀에서 모양을 생성할 수 있도록 추가 입력 조건에 의해 제어할 프레임워크를 확장한다. 광범위한 실험 세트에서 우리는 광범위한 양식에 대해 무조건적인 생성, 형상 완료 및 조건부 생성과 같은 다양한 애플리케이션을 보여준다. 우리의 접근법은 고품질 결과를 전달하는 데 있어 예술의 상태를 능가할 뿐만 아니라 몇 초 이내에 효율적으로 형태를 생성하며, 종종 대부분의 조건에 대해 단 2초 만에 이를 달성한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대규모 생성 모델은 점점 더 현실적인 출력을 생성할 수 있게 되었다[72, 74, 75, 101] 이러한 성공은 대규모 3D 생성 모델 개발이 가능한지 여부에 대한 문제로 이어졌으며, 이는 잠재적으로 흥미로운 출현 특성을 나타내고 다양한 응용 프로그램을 용이하게 할 수 있다. 그러나 대부분의 현재 3D 생성 모델은 품질이 제한되어 있으며 작은 3D 데이터 세트[6, 18, 28, 32, 56, 82, 100, 105]에 초점을 맞추거나 단일 조건[25, 33, 40, 43, 62, 98]을 허용하는 데 뒤처졌다.\n' +
      '\n' +
      '그림 2와 비교하여 3D에서 큰 생성 모델을 훈련할 수 있는데, 메이크-A-샤프는 다양한 입력 양식에 대한 다양한 형태를 생성할 수 있으며, 단일 뷰 이미지(화살 1 & 2), 멀티뷰 이미지(화살 3 & 4), 포인트 구름(화살 5 & 6), 복셀(화살 7 & 8), 불완전한 입력(위행)이다. 7번과 8번 행의 복셀 분해능은 각각 \\(16^{3}\\) 및 \\(32^{3}\\)이다. 상위 8개의 행에서 홀수 열은 입력을 나타내는 반면, 심지어 열은 생성된 형상을 보여준다. 마지막 행에서 열 1과 열 4는 부분 입력을 나타내는 반면 나머지 열은 다양한 완성된 형상을 보여준다.\n' +
      '\n' +
      '2D 이미지는 몇 가지 중요한 문제에 직면해 있습니다. 먼저, 3D에서 추가 공간 차원을 갖는 것은 신경망을 모델링하기 위해 필요한 입력 변수의 수를 실질적으로 증가시켜 더욱 많은 네트워크 파라미터를 생성한다. 이는 U-Net 기반 확산 모델[24, 83, 84]에서 특히 분명하며, 이는 GPU가 처리하기에 너무 큰 메모리 집약적 특징 맵을 생성하여 트레이닝 시간[26]을 연장시킨다. 둘째, 생성 모델을 3D로 스케일링하면 2D 이미지와 존재하지 않는 데이터 처리 복잡성이 소개된다. 대형 모델을 훈련하기 위한 대부분의 저장 및 데이터 처리는 AWS 또는 아저레와 같은 클라우드 서비스에서 이루어진다. 3D 데이터는 각 훈련 반복에서 데이터를 다운로드하기 위해 저장 및 시간 비용을 증가시킨다. 셋째, 3D 모양을 나타내는 방법은 여러 가지가 있다. 효율적인 훈련을 위해 좋은 재현 콤팩트성을 유지하면서 어떤 것이 높은 재현 품질을 달성하는지는 불분명하다.\n' +
      '\n' +
      '최근 3D 형태에 대한 대규모 생성 모델은 두 가지 주요 전략을 통해 이러한 문제를 해결한다. 첫 번째는 모델이 처리해야 하는 입력 변수의 수를 줄이기 위해 손실 입력 표현을 사용한다. 그러나 중요한 세부 사항을 생략하는 것을 희생하여 형태를 충실히 포착하지 못했다. 이 전략의 주요 예는 포인트 클라우드를 입력으로 활용하는 포인트-E[62]와 잠재 벡터를 사용하는 Shap-E[33]이다. 그들의 절충은 그림 3과 표 1에서 분명하며, 지상-진리 서명 거리 함수를 재구성할 때 상당한 세부 손실이 종종 관찰된다(SDF). 두 번째 전략은 기하학 [25, 40, 43, 98]을 나타내기 위해 멀티뷰 이미지를 사용한다. 이 접근법에서 생성 네트워크는 학습을 용이하게 하기 위해 지상-진술 멀티뷰 이미지와 비교하기 위해 생성된 형상의 이미지를 생성하기 위해 서로 다른 렌더링을 사용한다. 이러한 방법은 일반적으로 손실 계산을 위해 서로 다른 사용할 수 있는 렌더링을 사용하기 때문에 광범위한 훈련 시간이 필요하며, 이는 느리고 하나의 훈련 예에서 전체 기하학을 포착하지 않을 수 있다. 우리의 프레임워크는 평균적으로 덜 강력한 GPU(A10G 대 A100)를 사용했음에도 불구하고 이러한 방법보다 하루에 2배에서 6배 더 많은 훈련 형태를 처리한다. 표 2에 자세히 설명된 바와 같이.\n' +
      '\n' +
      '이러한 문제는 표현적이고 컴팩트하며 효율적으로 학습할 수 있는 적절한 3D 표현이 없기 때문에 발생한다. 본 연구에서는 대규모 모델 학습을 위한 3D 형상을 인코딩하기 위해 설계된 새로운 3D 표현인 _파플렛-트리 표현_을 소개한다. 이 표현은 고해상도 SDF 그리드에 웨이브렛 분해를 사용하여 거친 계수 서브밴드 및 다중 다단계 세부 계수 서브밴드를 생성한다. 효율적인 생성을 위해 (i) _subband 계수 필터링__subband 계수 모두 정보를 식별하고 정확하게 유지할 수 있는 큰 기술의 계열을 설계하며, 이는(i) _subband 계수 필터링_은 정보 집약적인 세부 계수를 통해 더 많은 형태 및 세부 모델을 식별하고 유지할 수 있도록, 우리의 표현은 더 다양한 모양에 참여할 수 있다. 또한 점 구름, 복셀, 이미지 등 탄력적인 입력 조건을 수용하기 위해 다양한 컨디셔닝 메커니즘을 구축합니다. 따라서, 우리의 새로운 표현은 컴팩트하지만 대부분의 모양 정보를 충실히 유지할 수 있으며 수백만 개 이상의 3D 모양으로 큰 생성 모델의 효과적인 교육을 용이하게 할 수 있다.\n' +
      '\n' +
      '위의 기술적 기여로 우리는 최소한의 손실로 형태를 인코딩할 수 있는 현저하게 _발현_인 표현을 생성할 수 있으며, 예를 들어 \\(256^{3}\\) 그리드는 약 1초 내에 웨이플릿-트리 표현으로 생물적으로 인코딩될 수 있지만 IoU는 99.56%이다. 동시에, 우리의 표현은 _compact_이며, 이는 적은 수의 입력 변수를 특징으로 한다. 이것은 거의 다입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c} \\hline \\hline Representation & IOU & Input Variables & Extra Network & Process Time \\\\ \\hline Ground-truth SDF (\\(256^{3}\\)) & 1.0 & 1677216 & No & \\(-\\) \\\\ Point-E [62] & 0.8642 & 12288 & Yes & \\(\\sim\\)1 second \\\\ Shap-E [33] & 0.8576 & 1048576 & Yes & \\(\\sim\\)5 minutes \\\\ Coarse Component [28] & 0.9531 & 97336 & No & \\(\\sim\\)1 second \\\\ Wavelet tree (ours) & 0.9956 & 1129528 & No & \\(\\sim\\)1 second \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 교차로 오버 연합(IOU) 및 처리 시간 측면에서 GSO 데이터세트 [15]에 대한 서로 다른 3D 표현을 비교하는 것이다. "추출 네트워크"는 SDF를 얻기 위해 여러 네트워크를 트레이닝할 필요성을 의미하며, "프로세스 타임"은 하나의 표현에서 SDF로 전환하는 데 필요한 시간을 의미하며, "입력 가변"은 각 표현에서 채택된 부동 소수점 수를 보고한다. 우리의 표현은 Shap-E[33]와 유사한 매개변수 수를 가지고 있지만 추가 네트워크가 필요하지 않고 더 빠른 전환을 달성한다는 점은 주목할 만하다.\n' +
      '\n' +
      '그림 3: (b)포인트-E[62], (c) Shap-E[33], (d) 거친 계수 \\(C_{0}\\)[28], (e) 우리의 웨이브렛-트리 표현과 같은 다양한 방법을 사용하여 형상(a)의 SDF를 재구성한다. 우리의 접근(e)은 형태의 구조와 세부 사항을 보다 충실히 재구성할 수 있다.\n' +
      '\n' +
      '잠재 벡터[33]과 같은 손실 표현에 대한 아킨은 오토인코더의 추가 훈련을 필요로 하지 않고 이를 달성하지만, 표 1과 같이 품질이 높을수록 우리의 표현은 _효율적인_로 효율적인 스트리밍 및 훈련을 가능하게 한다. 예를 들어, 정교한 압축된 \\(256^{3}\\) SDF 그리드를 스트리밍 및 로딩하는 것은 266 밀리초인 반면, 우리의 표현은 동일한 공정에 대해 184 밀리초만 소요된다. 데이터 로딩 시간의 44.5% 감소는 대규모 모델 학습에 중요하다.\n' +
      '\n' +
      '전반적으로 생성 모델은 효과적으로 훈련될 수 있어 표 2에 보고된 기존 방법과 비교하여 빠른 추론과 _few 초_만 복용하여 고품질 형태를 생성할 수 있으며 제안된 생성 프레임워크 _Make-A-Shape_라고 명명할 수 있다. 이 프레임워크는 광범위한 객체 카테고리에 걸쳐 1천만 3D 형상을 포함하는 광범위한 데이터세트 상에서 상이한 입력 조건에서 무조건 생성 모델 및 확장 모델의 훈련을 용이하게 한다. 그림 1과 2에 도시된 바와 같이 다양한 그럴듯한 모양을 성공적으로 생성한다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '**신경사탑 발표*** 최근 몇 년 동안 딥러닝을 다양한 3D 표현과 통합하는 데 중요한 연구가 있었다. [54, 95]와 같은 초기 작업은 3D 컨볼루션 네트워크의 사용을 통해 다양한 작업에 대한 체적 표현에 초점을 맞췄다. 다중 뷰 CNN은 또한 처음 3D 형태를 여러 이미지로 렌더링함으로써 [66, 86] 탐색되었으며, 그 동안 2D CNN이 다운스트림 응용 분야에 사용하기 위해 적용된다.\n' +
      '\n' +
      '이어서 포인트넷[67]에 포인트 클라우드에 대한 딥러닝 방법을 도입하여 이후 [68, 90]에서 볼 수 있듯이 컨볼루션과 같은 추가적인 유도적 편향을 채택하였다. 신경망은 또한 서명된 거리 함수들(SDF들) 또는 점유 필드들 중 하나를 예측함으로써 3D 형상을 나타내기 위해 사용될 수 있다. 일반적으로 신경 암묵적 표현으로 알려져 있는 이러한 표현은 탐구 대상[6, 56, 63]이었다. [21, 53, 61, 88] 등의 작품에서는 메스와 같은 다른 명시적 표상이 탐구되어 왔다. 또 다른 널리 퍼진 3D 표현인 경계 표현(BREP)은 판별 및 생성 응용에 대해 [31, 32, 38, 93]과 같은 연구에서 최근에야 조사되었다.\n' +
      '\n' +
      '최근 일부 작품[28, 47]은 SDF 신호를 멀티 스케일 웨이플릿 계수로 분해하기 위해 웨이브렛의 사용을 조사했다. 그러나 이러한 방법은 형상 충실도를 희생하지만 학습 효율을 높이기 위해 고주파 세부 정보를 필터링한다. 이 작품에서 우리는 웨이플릿-트리 표현으로 알려진 새로운 표현을 소개한다. 우리는 조밀한 세부 계수와 정보가 풍부한 세부 계수를 모두 컴팩트하지만 거의 손실 없이 3D 모양을 인코딩한다고 생각한다. 우리가 소개할 다양한 기술에 의해 달성될 수 있는 우리의 표현은 고품질 형상 생성을 가능하게 하는 반면 수백만 개 이상의 형상을 포함하는 큰 3D 데이터에 걸쳐 확장성을 위한 컴팩트함이 남아 있다.\n' +
      '\n' +
      '***3D 제네릭 모델*** 3D 생성 모델 분야의 초기 노력은 주로 제네릭 애버버러네트웍스(GAN) [19, 92]에 집중되어 있다. 이어서, 오토인코더를 훈련하고 GAN을 사용하여 이러한 오토인코더의 잠재 공간을 처리하여 점 구름[1] 및 암묵적 표현[6, 29, 106]과 같은 표현에 대한 생성 모델을 가능하게 한다. 보다 최근의 연구[2, 18, 79]는 다양한 렌더링이 있는 GAN을 통합했으며, 여기서 렌더링된 여러 견해가 손실 신호로 사용된다. 또한 흐름[35, 76, 100]과 별거 오토인코더(VAE)-기반 생성 모델[60]의 정상화에 초점이 맞춰져 있다. 자가회귀 모델은 3D 생성 모델링에서 인기를 얻었으며 광범위하게 탐구되었으며 [8, 59, 61, 77, 87, 103, 99]였다.\n' +
      '\n' +
      '최근 양질의 이미지 생성을 위한 확산 모델의 발전으로 3D 맥락에 대한 확산 모델에 대한 관심도 크게 나타났다. 대부분의 접근 방식은 확산 모델을 잠재 공간에 활용하기 전에 트라이플레인[10, 64, 82], 암묵적인 형태[9, 41, 104] 및 포인트 클라우드[33, 102]와 같은 3D 표현에서 Vector-양자화된-VAE(VQ-VAE)를 먼저 훈련시킨다. 3D 표현에 대한 직접적인 훈련은 덜 탐색되었다. 일부 최근 연구는 점 구름[52, 62, 108], 복셀[107], 신경파렛 계수[28, 47]에 초점을 맞추고 있다. 우리의 작업은 3D 표현에서 직접 확산 모델을 사용하여 VQ-VAE와 관련된 정보 손실을 회피한다. 우리 웨이브렛-트리 표현을 형성하는 것 외에도 확산 가능한 형태로 변환하거나 확산 모델에 의해 효과적으로 생성 가능한 방식을 제안한다. 우리의 접근법은 높은 해상도에서 큰 효율성을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c} \\hline \\hline Method & Inference time & \\# Training shapes in 1 day / GPU \\\\ \\hline Point-E [62] & \\(\\sim 31\\) sec & – \\\\ Shape-E [33] & \\(\\sim 6\\) sec & – \\\\ One-2-3-45 [43] & \\(\\sim 4\\) sec & \\(\\sim 50\\)Å (A10G) \\\\ DMV3D [98] & \\(\\sim 30\\) sec & \\(\\sim 110\\)Å (A100) \\\\ Instant3D [40] & \\(\\sim 20\\) sec & \\(\\sim 98\\)Å (A100) \\\\ LRM [25] & \\(\\sim 5\\) sec & \\(\\sim 74\\)Å (A100) \\\\ \\hline Ours (single-view 10 liter.) & \\(\\sim 2\\) sec & \\\\ Ours (single-view 100 liter.) & \\(\\sim 8\\) sec & \\(\\sim 290\\)Å (A10G) \\\\ \\hline Ours (multi-view 10 liter.) & \\(\\sim 2\\) sec & \\(\\sim 250\\)Å (A10G) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 최첨단 방법과 효율성 비교. 4-6 행에 대한 결과는 각각 동시 작품인 DMV3D[98], 인판트3D[40], LRM[25]에서 조달된다. 단일 뷰의 경우, 10 및 100 반복(문) 모두에 대한 추론 시간을 제시하며, 후자는 절제 연구에 따라 품질에 대한 최적의 하이퍼파라미터이다. 멀티 뷰의 경우 10번의 반복이 최적으로 식별된다. 훈련 시간은 서로 다른 수의 GPU를 사용하기 때문에 1일 동안 처리할 수 있는 훈련 모양의 수로 훈련 속도를 사용된 GPU의 수로 나눈 값이다. 포인트-E[62] 및 Shap-E[33]에는 훈련 시간을 사용할 수 없다.\n' +
      '\n' +
      '[107]과 비교하여 [52, 62, 108]보다 더 깨끗한 매니폴드 표면을 생성하고 [28]보다 훨씬 더 자세한 내용을 캡처한다.\n' +
      '\n' +
      '***조건 3D 모델** 3D 조건에서 조건부 모델을 두 그룹으로 분류할 수 있다. 제1 그룹은 3D 장면 또는 객체를 최적화하기 위해 Stable Diffusion[74] 또는 Imagen[75]와 같은 큰 2D 조건부 이미지 생성 모델을 배포한다. 이 방법들은 3D 형상을 생성하여 상이한 이용가능한 렌더러를 사용하여 이미지로 변환함으로써, 이미지가 다수의 이미지와 비교되거나 큰 텍스트 대 3D 생성 모델의 분포와 정렬될 수 있다. 이 지역의 초기 탐색은 [30, 57, 65]에서 볼 수 있듯이 텍스트 대 3D를 중심으로 이루어졌다. 이 접근법은 나중에 이미지[13, 55, 96] 및 다중 뷰 이미지[12, 44, 69, 81]를 포함하도록 확장되었다. 최근의 방법은 스케치[58]와 같은 추가 조건도 통합했다. 전반적으로, 이 접근법은 실제로 응용을 제한하는 값비싼 최적화를 의도치 않게 필요로 한다.\n' +
      '\n' +
      '두 번째 방법 그룹은 조건과 페어링되거나 제로 샷 방식으로 사용되는 데이터로 조건부 생성 모델을 훈련하는 데 중점을 둔다. 포커스 조건부 생성 모델은 포인트 클라우드[103, 105], 이미지[33, 62, 103, 105], 저해상도 복셀[5, 7], 스케치[17, 20, 37, 51] 및 텍스트[33, 62]와 같은 다양한 조건을 탐색한다. 보다 최근에는 텍스트[46, 76, 77, 97]와 스케치[78]를 중심으로 제로샷 방식이 인기를 끌었다. 이 작업에서 우리의 주요 초점은 크고 쌍을 이루는 조건부 생성 모델을 훈련하는 데 있다. 이 모델은 장면 최적화의 필요성을 없애기 때문에 빠른 생성을 제공합니다. 우리의 접근법은 또한 다양한 조건, 점 구름, 저해상도 복셀 및 이미지의 쉽게 통합할 수 있도록 한다. 게다가, 그것은 무조건적인 애플리케이션과 형상 완료와 같은 제로 샷 작업을 모두 가능하게 한다.\n' +
      '\n' +
      '## 3 Overview\n' +
      '\n' +
      '그림 4는 수백만 개의 3D 형태에 대한 효율적인 교육이 가능한 대규모 3D 생성 모델을 만들기 위해 설계된 형상 생성 프레임워크에 대한 개요를 제공한다. 3D 데이터의 복잡성은 특히 형태 생성의 질과 훈련 속도를 모두 최적화할 필요성을 고려할 때 이 규모에서 효율적인 훈련을 매우 어렵게 만든다. 우리의 접근법은 4~7절에서 자세히 설명하는 4개의 주요 구성 요소를 포함한다.\n' +
      '\n' +
      '__(i) _Wavelet-tree 표현.__Wavelet-tree 표현.__Wavelet-tree 표현.__Wavelet-tree 표현. 우리는 먼저 대규모 형상 훈련을 지원하기 위해 컴팩트하고 효율적이고 표현적인 3D 표현을 공식화한다. 중요하게도, 우리는 먼저 각 형태를 고해상도 잘린 사인 거리 필드(TSDF)로 인코딩하고 TSDF를 멀티 스케일 웨이플릿 계수로 분해한다. 우리는 웨이브렛 트리 표현에서 충실하면서도 콤팩트한 표현을 가능하게 하여 계수 간의 관계를 악용하는 서브밴드 계수 필터링 절차를 설계하여 정보가 풍부한 파동 성분(크게, 세부 사항)을 유지할 수 있다.\n' +
      '\n' +
      '그림 4: 생성 접근법의 고려. (a) A 형상은 먼저 잘린 사인 거리 필드(TSDF)로 인코딩된 다음, 웨이블릿 트리 구조에서 멀티 스케일 웨이플릿 계수들로 분해된다. 우리는 계수 간의 관계를 활용하고 정보가 풍부한 계수를 추출하여 웨이브렛-트리 표현을 구축하기 위해 _subband 계수 필터링_ 절차를 설계한다. (b) 우리는 우리의 웨이브렛-트리 표현을 관리 가능한 공간 분해능의 규칙적인 격자 구조로 재배열하기 위해 _subband 계수 패킹_ 체계를 제안하므로, 우리는 재현을 효과적으로 생성하기 위해 데노징 확산 모델을 채택할 수 있다. 물론, 우리는 서로 다른 서브 밴드에서 형상 정보를 효과적으로 균형을 잡고 세부 계수 유출을 해결하기 위해 _subband 적응 훈련_ 전략을 수립한다. 따라서 수백만 개의 3D 모양으로 모델을 효율적으로 훈련할 수 있습니다. (d) 우리의 프레임워크는 다양한 양식의 조건으로 확장될 수 있다.\n' +
      '\n' +
      '효율적인 저장 및 스트리밍을 위해 3D 형상의 제품입니다.\n' +
      '\n' +
      '(ii) _Diffusible Whatlet-tree 발표___Diffusible Wavelet-tree 제시.__Diffusible Wavelet-tree 설명.__Diffusibleaylet-tree 설명 다음으로, 우리는 웨이브렛 트리 표현을 확산 모델과 더 호환되는 포맷으로 변환한다. 우리의 표현은 콤팩트한 형상 인코딩을 달성하지만 불규칙한 형식은 효과적인 형상 학습과 생성을 방해한다. 이는 계수를 형상 생성을 위한 관리 가능한 공간 분해능의 규칙적인 그리드로 재배열하기 위해 서브밴드 계수 패킹 방식을 설계하도록 동기를 부여한다.\n' +
      '\n' +
      '__(iii) _ 하위 밴드 적응 훈련 전략.__ 하위 밴드 적응 훈련 전략.__ 하위 밴드 적응 훈련 전략. 또한, 확산성 웨이브렛 트리 표현에서 모델을 훈련시키기 위한 방법을 이용한다. 일반적으로 모양 정보는 서브밴드 및 스케일에 따라 다르며, 상세 계수는 매우 드물지만 모양 세부 정보가 풍부하다. 따라서 표준 균일한 Mean Squared Error(MSE) 손실로 훈련하면 세부 사항에 대한 모델 붕괴 또는 비효율적인 학습이 발생할 수 있다. 이를 해결하기 위해 다양한 서브밴드의 계수에 선택적으로 초점을 맞춘 _subband 적응 훈련_ 전략을 소개한다. 이 접근법은 훈련 중에 조대한 하위 밴드에 대한 형상 정보의 효과적인 균형을 가능하게 하고 모델의 구조적 측면과 세부적인 측면을 모두 학습하도록 장려한다.\n' +
      '\n' +
      '조건부 시대를 위한__(iv) _확장._조건부 시대를 위한 연장._(iv) _확장. 마지막으로 무조건적인 세대를 넘어 단일/멀티뷰 이미지, 복셀, 포인트 클라우드와 같은 조건에 따라 조건부 형상 생성을 지원하는 방법을 확장합니다. 본질적으로, 우리는 지정된 조건을 잠재 벡터로 인코딩한 다음 집합적으로 여러 메커니즘을 사용하여 이러한 벡터를 생성 네트워크에 주입한다.\n' +
      '\n' +
      '4개의 웨이플릿-트리 공유 제시.\n' +
      '\n' +
      '우리의 표현을 구축하기 위해 3D 형태를 \\(256^{3}\\)의 해상도로 잘린 서명 거리 함수(TSDF)로 변환한다. 우리는 웨이브렛 변환1을 사용하여 TSDF를 거친 계수 \\(C_{0}\\) 및 3개의 세부 계수 \\(\\{D_{0},D_{1},D_{2}\\}\\) 세트로 분해한다. 이러한 계수를 얻는 과정은 먼저 TSDF를 \\(C_{2}\\) 및 관련 세부 계수 \\(D_{2}=D_{2}^{LH},D_{2}^{HL},D_{2}^{HH}\\)로 전환하는 것을 포함한다. 그런 다음,\\(D_{1} <{1}{1}> 및 \\(C_{1}{1}^{HL}, D_{1}^{HL},D_{{H},^{1}},^{HH})를 \\(C_{2}}, D_{{H}}, D_{{1}}, D_{{H}})로 분해한 다음, \\)를 \\(C_{1}441},D_{1}^{H}},D_{1},{1},D_{{H},{1},{1},D_{1},^{H}},D_{1},^{1},^{H}},{1},{H},{1},{H}}, D_{1},^{H}}, D_{1},^{H}}, D_{{H}},^{H}},^{H}})로 분해하며, 이어서 \\) 이 과정은 그림 5에 나와 있으며 단순성은 2D 삽화를 사용한 방법을 제시하지만 실제 계산은 각 분해에서 세부 계수의 7 _subband 부피_(3 _subband 이미지_)로 3D에서 수행된다. 세부 계수에는 고주파 정보가 포함되어 있다는 점에 유의하는 것이 중요하다. 또한, 이 표현은 손실되지 않으며 역파지 변환을 통해 TSDF로 생물학적 변환될 수 있다.\n' +
      '\n' +
      '[28]에서 접근 방식을 올리면 6분 8분으로 생물신앙파렛을 사용한다.\n' +
      '\n' +
      'Wavelet-tree 및 효율적인 관계는 [28]에서 제안된 신경 파랑자 표현에 따라 구축되며, 우리는 3D 표현을 위한 파랑자 계수 간의 관계를 활용할 것을 제안한다. 일반적으로 \\(C_{0}\\)의 각 거친 계수와 _Self_로 알려진 \\(D_{0}\\)의 관련 세부 계수는 역파지 변환을 통해 \\(C_{1}\\)에서 해당 계수를 재구성한다. 이 _부모-자녀 관계_ 관계는 \\(D_{0}\\)와 \\(D_{1}\\) 사이에서 확장되며, 이와 같이 그림 6의 \\(D_{0}\\)에서 \\(D_{1}\\)로 이어지는 화살표로 보여지는 바와 같이 동일한 부모를 공유하는 계수는 _핀덤_이라고 한다. \\(C_{0}\\)에서 계수의 모든 후손을 집계함으로써 \\(C_{0}\\)의 계수가 뿌리 역할을 하는 _파팔 계수 트리_ 또는 단순히 웨이플릿 트리를 구성할 수 있다. 이 개념은 그림 6에 더 설명되어 있다.\n' +
      '\n' +
      '관찰: 웨이브렛 계수에 대한 4가지 주목할만한 관찰을 확인했으며, 우리는 웨이브렛 계수에 대한 4가지 주목할 만한 관찰을 확인했다.\n' +
      '\n' +
      '1. 계수 크기가 임계치보다 작으면(하위 밴드에서 가장 큰 계수의 1/32) 어린이는 작은 크기를 가질 가능성이 높다. 작은 크기는 모양에 대한 낮은 기여도를 의미하므로 작은 크기는 형태가 낮은 기여도를 의미한다.\n' +
      '\n' +
      '그림 5: 입력 형상의 웨이브렛 분해는 최근에 조밀한 계수 \\(C_{i}\\)로 TSDF를 나타내고 세부 계수 \\(\\{D_{i}^{LH},D_{i}^{HL},D_{i}^{HH}\\}\\)를 나타낸다. 3D 경우 각각의 분해에서 세부 계수의 7개의 하위 밴드가 있을 것이다.\n' +
      '\n' +
      '그림 6: 부모-자녀 관계의 과뷰. C_{0}\\(C\\)의 각 계수에 대한 파이버 트리가 뿌리로 형성되며, 코스터 레벨 계수는 부모로서 그리고 더 미세한 수준의 계수는 어린이로서 형성된다.\n' +
      '\n' +
      '계수는 형태에 거의 영향을 미치지 않는다. 우리는 1,000개의 무작위 형상의 \\(D_{0}\\) 하위 밴드에서 이 관찰을 실증적으로 연구했으며 계수의 \\(96.1\\%\\) 이상이 이 가설을 충족한다는 것을 발견했다.\n' +
      '2. 형제 계수의 값은 양의 상관관계가 있다. 우리는 1,000개의 무작위 형상으로 모든 형제 계수 쌍 간의 상관 계수를 평가했으며 0.35의 양의 상관 값을 발견했다.\n' +
      'C_{0}\\(C_{0}\\)의 3가지 비효율체는 대부분 비-저자가며 평균 크기는 \\(2.2\\)인 반면, \\(D_{0}\\)의 평균 상세 계수의 크기는 0에 훨씬 가깝으며 이는 \\(C_{0}\\)가 대부분의 형상 정보를 포함하고 있음을 의미한다.\n' +
      '4. \\(D_{2}\\)의 최대 계수는 미미하다. 역 웨이브렛 변환에서 지퍼로 실증적으로 설정함으로써, 우리는 \\(99.64\\%\\) IoU 정확도로 1,000개의 무작위 형태에 대해 충실히 TSDF를 재구성할 수 있다.\n' +
      '\n' +
      '관찰에 기초하여, 우리는 대표성을 구축할 때 정보가 풍부한 계수를 찾고 포장하기 위해 서브 밴드 계수 필터링 절차를 설계한다. 먼저, 우리는 모든 계수를 \\(C_{0}\\)에 보관하고, 표현에서 _coarse 구성요소_로 삼고, 관찰(iii) 및 (iv)에 따라 \\(D_{2}\\)의 모든 계수를 제외한다. 둘째, \\(C_{0}\\)만으로는 세부 사항을 포착하기에 불충분하며 그림 3(d) 대 그림과 비교한다. >아유. 우리는 \\(D_{0}\\) 및 \\(D_{1}\\)의 계수가 필요하다. 그러나 단순히 \\(D_{0}\\) 및 \\(D_{1}\\)의 모든 계수를 포함하여 부피가 큰 표현으로 이어질 것이다. 따라서 우리는 \\(D_{0}\\) 및 \\(D_{1}\\)에서 계수 관계를 사용하기 위해 관찰(i) 및 (ii)에 따라 세부 사항을 유지할 수 있는 컴팩트한 표현을 목표로 한다.\n' +
      '\n' +
      '절차적으로는 \\(D_{0}\\)의 서브밴드가 동일한 해상도를 공유하고 양의 상관관계가 있기 때문에 관찰(iii)에서 분석된 바와 같이 \\(D_{0}\\)의 모든 서브밴드의 계수 위치를 함께 조사하였다. 각 계수 위치에 대해 \\(D_{0}^{LH}\\), \\(D_{0}^{HL}\\) 및 \\(D_{0}^{HH}\\)의 형제 계수를 조사하여 크기가 가장 큰 수를 선택했다. 우리는 그 계수 위치에 대한 _정보_의 측정치로서 그 크기를 고려한다. 다음으로 정보 함량이 가장 높은 상위 K 계수 위치(왼쪽 그림 7 참조)를 필터링하고 \\(D_{0}\\)에서 자녀의 계수 값과 함께 위치 좌표 및 연관 계수 값을 \\(D_{1}\\)에 저장한다. 이는 오른쪽 그림 7과 같이 웨이브렛 트리 표현에서 _RS 구성 요소_를 형성한다. 거친 성분, 즉 \\(C_{0}\\)와 함께 우리는 웨이브렛 트리 표현을 구성한다. I\\(D_{2}\\)의 모든 계수를 배제하고 \\(D_{0}\\)에서 상위 K 계수를 선택함에도 불구하고, 우리의 표현은 99.56%의 인상적인 평균 IOU를 효과적으로 달성할 수 있다.\n' +
      '\n' +
      '수백만 개의 3D 모양을 효율적으로 처리하기 위해 각 모양을 인코딩하기 위해 웨이브렛 트리 표현을 사용하여 결과를 클라우드에 저장한다. 이는 일회성 전처리 단계를 구성한다. 이 측면은 도입에서 지적한 바와 같이 데이터 스트리밍과 로딩 모두에서 44.5% 감소하기 때문에 대규모 학습에 특히 중요하다. 더욱이, 우리는 그들이 발생하는 감압 오버헤드로 인해 보다 정교한 압축 기법을 일부러 사용하는 것을 피하며, 이는 모델 훈련을 상당히 늦출 수 있다.\n' +
      '\n' +
      '5 Diffusible Whatlet-tree 공유.\n' +
      '\n' +
      '다음으로 확산 기반 생성 모델에 의해 효과적으로 훈련되고 생성될 수 있는 표현을 개발한다. 이 확산 모델은 마르코프 체인으로서 생성 과정을 형성하는 DDPM 프레임워크[24]를 기반으로 한다. 이 사슬은 (i) 순방향 과정을 포함하며, 이는 결국 \\(T\\) 시간 단계보다 노이즈를 데이터 샘플 \\(x_{0}\\)로 확장적으로 도입하여 \\(p(x_{T})\\심 N(0,I)\\로 표시되는 단위 가우시안 분포로 변환하며, (ii) 역과정은 시끄러운 샘플에서 소음을 점진적으로 제거하는 것을 특징으로 한다. 우리의 접근법에서 발전기 네트워크는 \\(f_{\\ta}(x_{t},t)\\simeq x_{0}\\)로 표현되는 시끄러운 샘플 \\(x_{t}\\)에서 직접 원래의 확산 목표 \\(x_{0}\\)를 예측하도록 설계되었다. 이를 달성하기 위해 평균 제곱 손실 목표를 사용한다.\n' +
      '\n' +
      '당사의 웨이브렛-트리 표현은 데이터 스트리밍에 컴팩트하고 효율적이면서도 훈련 중 특정 도전에 직면합니다. 이 표현은 그리드로 구조화된 거친 성분, \\(C_{0}\\) 및 세 개의 불규칙한 어레이를 포함하는 디테일 성분으로 구성된다. 세부 구성 요소는 그림 7(오른쪽)과 같이 \\(D_{0}\\) 및 \\(D_{1}\\)에서 파생된다. 이 표현을 확산 대상으로 직접 처리하고 2분장 네트워크를 사용하여 거친 디테일 컴포넌트를 예측하는 것이 간단한 접근법이다. 그러나 여러 목적을 균형을 이루면서 세부 계수 위치를 정확하게 예측하기 어렵다. 우리는 이러한 접근이 융합과 투쟁하고 모델 훈련의 붕괴로 이어진다는 것을 경험적으로 관찰했다.\n' +
      '\n' +
      '우리가 시도한 또 다른 접근법은 추출된 코를 평평하게 하는 것이다.\n' +
      '\n' +
      '그림 7: 우리 대표성의 세부 구성 요소입니다. 우리는 공간 위치와 함께 노란색 상자로 표시된 \\(D_{0}\\) 및 \\(D_{1}\\)로부터 정보 계수 및 팩을 추출하여 표현의 세부 구성요소를 형성한다.\n' +
      '\n' +
      '자세한 구성요소의 비리를 피하기 위해 대표성의 비효율성. 그림 8(좌표)에서 알 수 있듯이, 먼저 왼쪽 상단에 거친 성분 \\(C_{0}\\)을 배열한 다음, 추출한 세부 계수를 각 서브밴드의 각 위치에 배치하여 \\(D_{0}\\) 및 \\(D_{1}\\)를 연속적으로 배치하여 나머지 위치를 zeros로 남겨두면서 각 서브밴드의 각각의 위치에 정렬하여 \\(C_{0}\\)한다. 이와 같이, 입력 표현은 DDPM을 모델링하기 위한 일반 2D 그리드가 된다. 그러나 이 표현은 공간적으로 매우 크다. 기존의 확산 모델에 의해 널리 채택된 현재의 U-Net 아키텍처는 GPU 메모리 집약적 특징맵을 생성하여 메모리 외 문제로 이어져 계산 강도가 낮아 가속기 [26]의 활용도가 저하될 수 있다. 결과적으로 모델 훈련은 여전히 견딜 수 없습니다.\n' +
      '\n' +
      '이러한 과제를 해결하기 위해 우리는 효율적인 2D 이미지 생성 [26]에 대한 최근 연구에서 영감을 이끌어낸다. 우리의 동기는 계수 간의 관계를 강조하는 관찰(iii)에 의해 더욱 강화된다. 이 통찰을 통해 유사한 구조(D_{0}\\)와 그림 6의 \\(D_{1}\\)를 나타내는 형제 하위 밴드 계수를 효과적으로 포장할 수 있다. 이 아이들은 그림 8(오른쪽)에서 볼 수 있듯이 \\(1차시 1\\t 4\\) 형식으로 재구성되어 채널 차원에 통합된다. 이 접근법에서 결과적 표현(확산 모델 대상)은 공간 해상도가 감소했지만 채널 수가 증가한 격자 구조를 채택한다. 따라서 이것은 메모리 집약적 특징 맵의 사용을 우회하고 메모리 외 문제를 피하고 수백만 개의 3D 모양에 대한 훈련 시 보다 효율적인 계산을 가능하게 한다. 우리의 3D 표현에서 이 전략을 사용하면 거의 같은 네트워크 아키텍처에 적용될 때 약 64x로 추정되는 입방계 속도 및 GPU 메모리 사용량의 상당한 감소가 발생할 수 있다.\n' +
      '\n' +
      '6개의 하위 밴드 A.\n' +
      '\n' +
      '이 단계에서 우리의 목표는 확산 가능한 웨이브렛 트리 표현에 대한 확산 모델을 효과적으로 훈련시키는 것이다. 우리의 모델에 대한 주요 과제는 표현의 거친 요소와 상세한 구성 요소를 모두 능숙하게 생성할 수 있도록 하는 것이다. 이를 위한 명백한 선택은 계수 집합 \\(X\\)에 대한 표준 평균 제곱 오차(MSE) 손실을 사용하는 것이다.\n' +
      '\n' +
      '지노지노{1}{|X|}}\\_{|f_{t}}(x_{t},t)-x_{0})\\[L_{text{E}}}.\n' +
      '\n' +
      'i\\(x_{t}\\)가 시간 단계 \\(t\\)에서 \\(x_{0}\\)의 무지도 계수이다. 실증적으로, 우리는 모든 계수에 MSE를 순하게 적용하는 것이 절제 연구에서 입증된 바와 같이 훈련 중 상당한 품질 저하를 초래한다는 것을 관찰했다. 우리는 이것을 두 가지 주요 요인에 속합니다. 첫째, 서로 다른 규모에 걸쳐 계수의 수에 불균형이 있다. 2D 경우 \\(|C_{0}|:|D_{0}|:|D_{0}|:|D_{1}|\\)의 비율은 \\(1:3:12\\)이고, 3D 경우에는 \\(1:7:56\\)이다. 결과적으로 균일한 MSE 손실을 사용하는 것은 코어 형상 정보가 \\(D_{0}\\) 및 \\(D_{1}\\)보다 \\(C_{0}\\)에서 더 조밀하게 표현되었음에도 불구하고 미세 상세 계수를 불균형적으로 강조하는 경향이 있다. 둘째, 세부 계수의 대부분은 제로에 가깝고, 형상에 대한 최대 고주파 정보를 포함하는 크기 또는 _정보_가 거의 없다. 따라서 이러한 계수에 걸쳐 균일하게 손실을 샘플링하면 고감도 세부 계수 수의 불균형으로 인해 최적이 아닌 훈련 메커니즘이 발생할 수 있다.\n' +
      '\n' +
      '문제를 해결하기 위한 초기 접근법은 \\(C_{0}\\), \\(D_{0}\\), \\(D_{1}\\)에 대한 3개의 개별 MSE 손실을 정의하는 다음 이러한 손실을 동일한 가중치와 결합하는 것을 포함한다. 그러나 이 접근법은 세부 서브밴드의 유출로 인한 불균형 문제를 해결하지 않고 여전히 세부 계수에 대한 손실을 균일하게 처리한다. 이 감독은 절제 연구에서 관찰된 하위 결합 성능에 의해 경험적으로 입증된다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해 서브밴드 적응 훈련 전략을 개발한다. 이 접근법은 다른 나머지 세부 계수에 대한 균형 잡힌 고려를 유지하면서 높은 크기 세부 계수에 더 효과적으로 초점을 맞출 수 있도록 특별히 설계되었다. 이를 통해 완전히 간과되지 않도록 합니다. 구체적으로, \\(D_{0}\\)의 각 서브밴드에 대해 \\(D_{0}^{LH}\\)라고 말하고, 먼저 \\(D_{0}^{LH}\\)에서 가장 큰 크기의 계수를 구한다. 우리는 이러한 계수를 크기 값(v/32\\)으로 보고 이러한 계수를 중요하게 간주하고 공간(D_{0}^{LH}\\)보다 큰 \\(D_{0}^{LH}\\)에서 모든 계수를 기록하고 그 크기 값으로 기록한다.\n' +
      '\n' +
      '그림 8: Diffusible 웨이플렛 표현. 먼저, 우리는 웨이브렛 트리 표현(좌표)에서 계수를 포장하고 평탄화한다. 관찰(iii) 후, 우리는 공간적 해상도(오른쪽)를 줄이기 위해 형제 계수를 채널 단위로 연결한다. 여기에서 \\(C_{0}\\)의 각 계수와 \\(D_{0}\\)의 세 자녀와 \\(D_{1}\\)의 재혼 후예(1\\ 1\\t 4\\)를 연결한다.\n' +
      '\n' +
      '좌표 세트 \\(P_{0}^{LH}\\)에 할당한다. 유사하게, 우리는 \\(D_{0}^{HL}\\) 및 \\(D_{0}^{HH}\\)에 대한 좌표 세트 \\(P_{0}^{HL}\\)를 획득할 수 있다. 동생계수가 긍정적으로 연관되어 있기 때문에 세 좌표 세트를 좌표 집합 \\(P_{0}\\)으로 조합하여 중요한 세부계수의 공간적 위치를 기록하고 있다.\n' +
      '\n' +
      '한편, \\(P_{0}^{\\prime}\\)를 \\(D_{0}\\) 서브밴드의 공간 영역에서 \\(P_{0}\\)에 대한 좌표 집합 보체로 정의한다. 그런 다음 \\(P_{0}\\)와 \\(P_{0}\\)를 사용하여 훈련 손실을 교육 손실(P_{0}\\)과 \\(P_{0}^{\\prime}\\)로 공식화할 수 있다.\n' +
      '\n' +
      '}} (D[P_{{D})\\Big{{} (R[P_{{D]\\text{MSE} (C_{{{{})\\Big{{(C_{{{{})\n' +
      '\n' +
      'H\\(D\\)는 \\(\\{D_{0},D_{1}\\}\\)의 서브밴드이고, \\(D[P]\\)는 \\(P\\)의 위치의 \\(D\\) 계수를 나타내고, \\(D[P_{0}^{\\prime}]\\)는 \\(D[P]\\)의 계수 중 일부를 무작위로 클릭하는 함수이다. 중요하게도, \\(R\\)가 선택한 계수의 수는 \\(|P_{0}|\\)이므로 동일한 수의 계수로 손실에서 마지막 두 항을 균형을 맞출 수 있다. 무작위 샘플링을 사용하면 훈련에서 완전히 무시하지 않고 덜 중요한 작은 계수를 효과적으로 규칙화할 수 있다는 점에 유의한다. 또한, \\(P_{0}\\)는 \\(D_{0}\\)의 전체 도메인(단독 \\(\\심 7.4\\%\\)보다 훨씬 적은 계수를 포함하므로 모델 훈련은 세부 사항에 참석하면서 중요한 정보에 효과적으로 집중할 수 있다.\n' +
      '\n' +
      '손실 계산을 위해 \\(P_{0}\\) 및 \\(P_{0}^{\\prime}\\)를 불규칙한 크기의 좌표 세트로 저장하고 생성 대상 및 네트워크 예측을 처리하기 위해 슬라이싱을 사용할 수 있다. 그러나 그렇게 하는 것은 다양한 데이터 샘플에 대한 별개의 계산이 필요하며 훈련 속도를 확보하기 위한 코드 편집 기능을 활용하는 것을 방지하기 때문에 매우 비효율적이다.\n' +
      '\n' +
      '모델 트레이닝 효율을 향상시키기 위해, 우리는 고정된 크기의 이진 마스크를 사용하여 좌표 세트를 나타낼 것을 제안하며, 여기서 하나의 값은 선택된 대로 연관된 위치를 나타낸다. 그런 다음 MSE 손실은 생성 목표와 네트워크 예측을 모두 마스킹하여 효율적으로 계산할 수 있다. 이 접근법은 불규칙한 작업의 필요성을 없애고 훈련을 가속화하기 위해 PyTorch에서 코드 편집을 효율적으로 사용할 수 있도록 한다.\n' +
      '\n' +
      '조건부세대 7배.\n' +
      '\n' +
      '>아니, 이거. 우리의 프레임워크는 다재다능하며 다양한 양식에 걸쳐 조건부 생성을 수용하기 위해 무조건적인 세대를 넘어 확장될 수 있다. 이를 달성하기 위해 주어진 조건을 잠재 벡터의 시퀀스로 전환하는 각 모달리티에 대한 상이한 인코더를 채택한다. 이어서, 이들 벡터들은 복수의 컨디셔닝 메커니즘을 이용하여 발전기에 주입된다. 또한 조건부 환경에서 더 큰 효과를 실증적으로 입증한 분류기가 없는 안내 메커니즘[23]을 사용한다.\n' +
      '\n' +
      '***컨디션 라텐트 Vectors** 우리는 모든 입력 조건을 일부러 _조건 잠재 벡터_라고 부르는 잠재 벡터의 시퀀스로 전환하여 컨디셔닝 프레임워크의 일반성을 보존한다. 이 접근법은 각 모달리티에 대한 확산 모델에 새로운 특정 조건 메커니즘을 고안해야 하는 필요성을 제거함으로써 우리의 프레임워크가 다양한 양식에 걸쳐 원활하게 기능할 수 있게 한다. 다양한 방식에 대한 인코더는 아래에 설명되어 있습니다.\n' +
      '\n' +
      '1. [label=()]\n' +
      '_2._단일뷰 이미지._단일뷰 이미지._단일뷰 이미지._단일뷰 이미지._단일뷰 이미지. 3D 모델의 렌더링된 이미지를 감안할 때, 우리는 이미지를 처리하기 위해 미리 학습된 CLIP L-14 이미지 인코더[70]를 사용한다. 그런 다음 이 인코더의 풀링 레이어 직전에 추출된 잠재 벡터를 조건부 잠재 벡터로 사용한다.\n' +
      '__3._멀티뷰 이미지._멀티뷰 이미지.__멀티뷰 이미지.__멀티뷰 이미지. 각각 55개의 사전 정의된 카메라 포즈(무작위로 선택된) 중 하나에서 렌더링된 3D 모델의 4개의 이미지를 제공합니다. 조건부 잠재 벡터를 생성하기 위해 먼저 CLIP L-14 이미지 인코더를 사용하여 각각의 렌더링된 이미지를 개별적으로 처리하여 이미지 잠재 벡터를 생성한다. 카메라 포즈들을 고려하여, 우리는 각각 하나의 카메라 포즈에 대응하는 55개의 훈련 가능한 카메라 잠재 벡터를 유지하고 CLIP 이미지 인코더에 의해 인코딩된 잠재 벡터의 차원을 매칭한다. 각각의 인코딩된 이미지 잠재 벡터에 대해, 우리는 이미지의 카메라 포즈에 기초하여 대응하는 훈련 가능한 카메라 잠재 벡터를 검색한다. 그런 다음 이 카메라 벡터는 요소별 방식으로 시퀀스의 각 이미지 잠재 벡터에 추가된다. 마지막으로, 잠재 벡터의 4개의 처리된 서열이 연결되어 조건부 잠재 벡터를 형성한다.\n' +
      '__4._3D 포인트 클라우드__3D 포인트 클라우드__4. _3D 포인트 클라우드.__3D 포인트 클라우드. 우리는 3개의 멀티 플레이어 퍼셉트론(MLP) 층을 사용하여 주어진 포인트 클라우드를 포인트넷[67]과 같은 특징 벡터로 먼저 변환한다. 그런 다음 PMA 블록을 사용하여 응집된 벡터들은 세트 트랜스포머 계층[39]을 형성함으로써 조건 역할을 하는 잠재 벡터의 세크네이스를 생성한다.\n' +
      '\n' +
      '그림 9: 우리 발전기 네트워크 진행적으로 병목 특징 부피(중간)에 대한 입력 계수를 감소시킨다. 이 부피는 변성 계수를 예측하기 위해 업샘플링을 위한 주의 레이어 및 디컨볼루션을 거친다. 조건 잠재 벡터를 사용할 수 있다면, 우리는 이러한 벡터를 동시에 변환하고 입력 없음 계수(녹색 화살표)와 연결(i)하고(ii) 컨볼루션 및 디컨볼루션 블록(블루 화살표)을 컨디셔닝하고(iii) 병목 부피(붉은 화살표)와 교차 표시)하는 아키텍처의 세 위치에서 채택한다.\n' +
      '\n' +
      '_Voxels__Voxels__._Voxels__4. _Voxels.__Voxels.__Voxels.__Voxels._4. _Voxels. 우리는 입력 3D 복셀을 3D 특징 부피로 점진적으로 다운샘플링하기 위해 2개의 3D 컨볼루션 레이어를 사용한다. 이 부피는 이후에 평탄화되어 원하는 조건부 잠재 벡터를 형성한다.\n' +
      '\n' +
      '***네트워크 건축***그림 9는 발전기의 네트워크 구조를 보여준다. 노란색 상자가 강조하는 본점은 U-ViT 아키텍처[26]를 채택하고 있다. 네트워크는 그림 9의 중간 부분에서 볼 수 있듯이 특징 병목 부피로 비식별 계수를 다운샘플링하기 위해 다수의 ResNet 컨볼루션 레이어를 사용하는데, 이 단계를 통해 볼륨에 일련의 주의 레이어를 적용한다. 그런 다음 부피는 다양한 디컨볼루션 층을 사용하여 업스케일링되어 변성 계수를 생성한다. 우리 구조의 주요 특징은 합성곱과 디컨볼루션 블록 사이의 학습 가능한 스킵-연결을 포함하는 것이며, 이는 안정성을 높이고 보다 효과적인 정보 공유[27]를 촉진하는 것으로 확인되었다.\n' +
      '\n' +
      '더욱이 조건 잠재 벡터를 사용할 수 있을 때 그림 9의 하단에 묘사된 U-ViT 아키텍처의 세 가지 별개의 위치에서 생성 네트워크에 통합하는데, 이러한 잠재 벡터는 MLP 층과 풀링 층을 통해 처리되어 단일 잠복 벡터(그림 9의 왼쪽 부분에서 녹색 화살표로 높이라이트)를 생성한다. 이 벡터는 이후 입력 노이즈 계수의 추가 채널로 연결된다. 둘째, 유사한 과정에 따라 조건 잠재 벡터를 다른 잠재 벡터로 변환한다. 그러나 이 벡터는 그룹 정규화 레이어[14]의 아핀 파라미터를 변조함으로써 컨볼루션 및 디컨볼루션 레이어를 조건화하는 데 사용된다. 이 통합은 그림 9의 파란색 화살표로 표시되며, 이는 병목 부피를 조건화하기 위해 요소별 방식으로 조건 잠재 벡터에 추가 위치 인코딩이 적용된다. 그런 다음 이러한 벡터는 그림 9의 적색 화살표로 표시된 바와 같이 병목 부피와 교차 의도 작업에서 사용된다.\n' +
      '\n' +
      '## 8 Results\n' +
      '\n' +
      '이 절에서는 실험 설정을 제시한 후 다양한 입력 조건을 사용하여 얻은 정량적 결과와 질적 결과를 모두 제시함으로써 시작한다. 또한, 우리는 생성 모델이 추가적인 훈련 없이 완료 작업을 형성하는 데 적응 가능하다는 것을 보여준다. 마지막으로, 우리는 프레임워크에 대한 포괄적인 분석과 남용을 제시한다.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      'SMAL[109], SMAL[109], SMAL[73], SMAM[73], SMA3D[94], ABC[36], Fusion 360[91], 3D-FUTURE[16], 빌딩넷[80], FG3D[42], FG3D[42], ABO[85], ABO[71], Objaverse [12] 및 오자버스[12]의 두 하위 집합으로 구성된 새로운 대규모 데이터 세트[12]로 구성된 SMLP[42], 3D[42], pUTURE[42], ABD[42], ABD[42], ABO[85], ABO[85], ABO[85], ABO[85], ABO[11], ABO[12] 및 2 하위 집합으로 구성된 3D[85], ABO[12] 및 GitHub. 캐드 모델(ABC 및 퓨전 360), 가구(ShapeNet, 3D-FUTURE, 모델넷, FG3D 및 ABO), 가구(SMAL 및 모델넷, FG3D), 얼굴(COMA), 집(건물넷 및 하우스3D), 일부에는 특정 객체 클래스, _e가 포함되어 있다. 이를 넘어, 오자버스 및 오자버스XL 데이터셋은 인터넷으로부터 수집된 일반적인 오브젝트를 포괄하여 전술한 카테고리를 커버할 뿐만 아니라 보다 다양한 범위의 객체를 제공한다. 데이터 분할을 위해 각각의 서브-다타셋을 형상의 \\(98\\%\\)로 구성된 학습 세트와 나머지 \\(2\\%\\)를 포함하는 테스트 세트의 두 부분으로 무작위로 나누었다. 그런 다음 각 서브-다타세트의 각각의 기차 및 테스트 세트를 결합하여 최종 열차와 테스트 세트를 컴파일했다.\n' +
      '\n' +
      '데이터세트 내의 각 형상에 대해 모델 학습을 위한 TSDF 및 그 웨이브렛 트리 표현을 생성한다. 반면에 조건부 생성 작업에 대한 다양한 추가 투입물을 준비합니다. 이미지 입력에 대해 [33]에서 제공하는 스크립트를 사용하여 미리 정의된 55개의 카메라 포즈들을 무작위로 샘플링하고 이러한 포즈에 따라 각 객체에 대해 55개의 이미지를 렌더링했다. 복셀 입력을 위해 각 3D 오브젝트에 대해 다른 해상도(16^{3}\\) 및 \\(32^{3}\\))에서 두 세트의 복셀을 준비하고 각 해상도에 대해 별도의 모델을 훈련했다. 마지막으로, 포인트 클라우드 입력을 생성하기 위해 각 3D 형태의 표면에서 무작위로 25,000점을 샘플링했다.\n' +
      '\n' +
      '*** 훈련 디테일** 우리는 학습률 1e-4, 배치 크기 96으로 아담 옵티미저[34]를 사용하여 형상 모델을 훈련하여 훈련 안정화를 위해 기존 2D 대규모 확산 모델[74]에 따라 모델 업데이트에서 부패율 0.9999로 지수 이동 평균을 사용한다. 우리의 모델은 입력 조건에 따라 2M-4M 반복이 있는 48\\(표본) A10G로 훈련된다.\n' +
      '\n' +
      '*** 평가 Dataset*** 정성적 평가를 위해 컴파일된 대규모 데이터 세트의 엔세닝 테스트 세트의 입력을 기반으로 시각적 결과를 제공합니다. 정량적 평가를 위해 메트릭 계산을 위한 두 가지 평가 세트를 준비한다. 특히, 각 서브-다타셋의 테스트 세트에서 무작위로 50개의 형상을 선택하여 메트릭을 컴퓨팅하기 위한 첫 번째 평가 세트를 형성한다. 우리는 이 데이터 세트를 나머지 종이 전체에 걸쳐 "우리 발" 데이터세트라고 한다. 또한 구글 스칸디드 오브제(GSO) 데이터 세트를 사용하여 우리의 모델이 이 데이터 세트에 대해 훈련되지 않았다는 점에 주목하여 방법을 위한 교차 영역 일반화 능력을 평가하기 위한 추가 평가 세트를 만든다. 구글 스칸디드 오브제(GSO) 데이터셋의 하위 집합이 원-2345[43]에 설정된 평가 집합으로 사용되었지만, 보다 포괄적인 평가를 보장하기 위해 이 데이터셋의 모든 물체를 연구에 포함했다는 점에 유의하시기 바랍니다.\n' +
      '\n' +
      '*** 평가 측정** 조건 작업에서 이미지 대 3D 생성 과제에 대한 표준 비교는 우리의 방법이 포인트-E[62], Shap-E[33], 원-2-3-45 [43]의 세 가지 주요 생성 모델을 능가한다는 것을 보여준다. 우리의 단일 뷰 모델은 이러한 기저부에 비해 더 정확한 모양을 생성하고 다중 뷰 모델은 추가 뷰 정보로 형상 충실도를 더욱 향상시킨다.\n' +
      '\n' +
      '두 가지 메트릭을 사용하여 생성된 모양과 관련된 지상-진열 형태 사이의 유사성을 비교함으로써 성능은 (i) 교차로 오버연합 (IoU)과 복셀화된 부피 간의 결합으로의 교차로 부피 비율을 측정하는 교차로 및 (ii) 라이트 필드 거리 (LFD) [4]와 다양한 관점에서 렌더링된 두 이미지 세트 간의 유사성을 평가한다.\n' +
      '\n' +
      '무조건의 과제를 위해 프로체트 인셉션 거리(FID) [22]를 사용하여 세대 성능을 평가하기 위해 [104]에서 설명한 방법을 채택한다. 특히, 우리는 우리 발 세트에서 각 모양에 대한 이미지를 만듭니다. 그 후, 우리는 생성된 등가 크기의 형상 세트에 동일한 렌더링 프로세스를 적용한다. 그런 다음 렌더링된 각 이미지에 대한 특징 벡터를 계산하고 두 세트 사이의 이러한 특징 벡터의 분포의 차이를 최종 메트릭으로 평가한다.\n' +
      '\n' +
      '대용량 확인 모델.\n' +
      '\n' +
      '이 실험에서 우리는 다른 큰 이미지 대 3D 생성 모델과 방법을 대조한다. 우리의 분석에서는 단일 뷰와 멀티 뷰의 두 가지 별개의 모델 설정을 포함한다. 단일 뷰 모델은 단일 이미지를 사용하여 우리의 웨이브렛 생성 모델에 대한 입력 역할을 한다. 여러 이미지들이 접근할 수 있는 경우, 우리의 멀티 뷰 모델은 재생된다. 이 모델은 카메라 파라미터와 함께 4개의 이미지를 조건으로 사용한다.\n' +
      '\n' +
      '표 3의 정량적 결과와 그림 10의 질적 비교 결과를 제시하며 표 3과 같이 단일 뷰 모델은 IoU 및 LFD 메트릭 모두에서 상당한 마진으로 모든 기준선 모델을 상당히 능가한다. 회전에 민감하지 않은 메트릭인 LFD가 우리의 결과가 생성된 모양과 지반진 모양 사이의 정렬에 의존하지 않는다는 것을 나타내는 것은 주목할 만하다. 또한 그림 10은 우리의 방법이 기저부에 비해 글로벌 구조뿐만 아니라 미세한 지역 세부 사항뿐만 아니라 더 복잡한 기하 패턴을 포착한다는 것을 보여준다. 이것은 그림 10의 7-8열에서 특히 분명하여 우리의 방법이 다른 기저장보다 선의 기하 패턴을 더 정확하게 포착할 수 있음을 보여준다. 또한, 행 3-4는 우리의 모델이 모양의 거친 코트 패턴을 효과적으로 포착하는 방법을 보여줌으로써 우위를 더욱 보여준다. 마지막으로, 우리는 우리의 방법으로 생성된 기하학이 다른 기준선 방법에 의해 생성된 것과 대조적으로 깨끗하고 매끄러운 표면을 특징으로 한다는 것을 강조하고 싶다.\n' +
      '\n' +
      '반면에, 우리의 모델은 추가 3개의 견해를 제공할 때 결과의 상당한 개선을 나타낸다. 표 3에 표시된 바와 같이 LFD와 IoU 메트릭 모두 생성 모델이 더 많은 글로벌 형상을 포착한다는 것을 보여준다. 이 개선은 네트워크가 복수의 뷰 형태로 더 많은 정보에 액세스할 수 있기 때문에 예상되지만, 4개의 이미지는 여전히 형태를 완전히 재구성하기 위한 제한된 뷰 세트를 구성한다. 더욱이, 다중 견해는 그림 10열 3-4 및 7-8에서 입증된 바와 같이 지역 기하학적 세부 사항을 더 잘 포착하는 데 도움이 되며, 이는 단일 관점에서 다중 견해를 생성할 수 있는 능력이 있는 제로123[44] 및 제로123-XL[12]와 같은 최근 발전을 언급할 가치가 있다. 이 발전은 잠재적으로 제로123 또는 제로123-XL을 사용하여 단일 뷰를 멀티뷰로 변환할 수 있기 때문에 사용 가능한 단일 뷰만으로 다중 뷰 모델이 효과적으로 동작하게 하고, 그 후 우리의 모델을 적용할 수 있다. 그러나 이러한 가능성을 탐구하는 것은 향후 연구의 주제로 남아 있다.\n' +
      '\n' +
      '우리는 이 성공을 두 가지 주요 요인에 속합니다. 먼저, 우리는 포인트-E 및 Shape-E와 비교하여 표 1에서 논의된 바와 같이 웨이브렛-트리 표현이 거의 손실되지 않는다는 것을 추정한다. 이러한 특성은 확산 모델을 사용하여 재현의 상단을 훨씬 쉽게 캡처할 수 있다. 또한, 우리의 적응 훈련 체계는 우리의 순을 가능하게 한다.\n' +
      '\n' +
      '그림 11: 우리 모델은 단일 입력 이미지로부터 다양한 결과를 생성할 수 있는 능력을 보여주며, 이는 눈에 보이는 부분과 정확하게 유사한 반면 비선 영역에서 다양성을 제공한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c} \\hline \\hline \\multirow{2}{*}{_Method_} & \\multicolumn{2}{c}{GSO Dataset} & \\multicolumn{2}{c}{Our Val Dataset} \\\\  & LFD \\(\\downarrow\\) & IoU \\(\\uparrow\\) & LFD \\(\\downarrow\\) & IoU \\(\\uparrow\\) \\\\ \\hline Point-E [62] & 5018.73 & 0.1948 & 6181.97 & 0.2154 \\\\ Shap-E [33] & 3824.48 & 0.3488 & 4858.92 & 0.2656 \\\\ One-2-3-45 [43] & 4397.18 & 0.4159 & 5094.11 & 0.2900 \\\\ Ours (Single view) & **3406.61** & **0.5004** & **4071.33** & **0.4285** \\\\ \\hline Ours (Multi view) & 1890.85 & 0.7460 & 2217.25 & 0.6707 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 이미지 대 3D 과제에 대한 정량적 평가는 단일 뷰 모델이 기저선을 조절하여 가장 높은 IoU 및 가장 낮은 LFD 메트릭을 달성한다는 것을 보여준다. 추가 정보를 통합하면 멀티뷰 모델은 성능을 더욱 향상시킵니다.\n' +
      '\n' +
      '세부 계수 서브밴드의 정보가 풍부한 계수를 모델링하기 위해 효율적으로 학습함으로써 보다 지역적인 세부 사항과 기하 패턴을 캡처하기 위한 작업을 수행한다.\n' +
      '\n' +
      '### Image-to-3D Generation\n' +
      '\n' +
      '이 섹션에서는 밸브 세트의 단일 뷰 컨디셔닝에 대한 추가 정성적 결과를 제시한다. 그림 12에 묘사된 바와 같이, 우리의 방법은 다양한 범주에서 객체를 생성할 수 있다는 것이 분명하다. 여기에는 1행의 나사와 같은 CAD 물체, 5행의 의자와 같은 가구, 4행의 집에서 예시한 건물들이 포함되며, 우리의 방법은 인간(4행과 6행), 동물(3행의 개) 및 식물(크리스마스 나무와 버섯에 의해 열화됨)을 포함한 유기적인 형상을 효과적으로 나타낸다.\n' +
      '\n' +
      '생성 모델인 우리의 모델은 단일 뷰 이미지를 사용하여 그림 11에서 입증된 바와 같이 주어진 조건에 대해 여러 변형을 생성할 수 있다. 우리의 접근법에 의해 충실히 재구성될 수 있는 가시적인 영역 외에도 보이지 않는 부분을 더 상상적으로 재구성한다. 예를 들어, 그림 11, 2, 4에서 CAD 객체의 상단은 보이지 않아 모델을 상상하게 한다. 가시 부품에 부착하고 보이지 않는 부분을 창의적으로 해석하는 사이의 이러한 절충은 분류기가 없는 유도 가중치를 조정함으로써 더 탐색될 수 있다. 우리는 향후 작업을 위해 이 측면을 남깁니다.\n' +
      '\n' +
      '우리는 또한 그림 13의 다중뷰 접근법에 대한 추가 결과를 제시하며, 이는 우리의 방법이 다양한 범주에 걸쳐 물체를 생성할 수 있음을 보여준다. 더욱이, 입력 이미지와의 객체들의 눈에 띄는 정렬이 존재하며, 이는 단일 뷰 접근법에 비해 더 두드러진다. 우리의 방법은 또한 유전이 가능하다.\n' +
      '\n' +
      '그림 12: 우리의 단일 뷰 조건 생성 모델은 매우 다양한 형태를 생성한다. 우리의 모델은 나사, 의자 및 자동차와 같은 CAD 물체뿐만 아니라 인간, 동물 및 식물과 같은 유기적인 형태를 동시에 생성한다.\n' +
      '\n' +
      'CAD 예들에 의해 입증된 바와 같이, 매우 복잡한 토폴로지와 복잡한 기하학들로 객체를 대체(1-2열에서의 제2 오브젝트 및 5-6열 내의 제1 오브젝트 참조)한다.\n' +
      '\n' +
      '### Point-to-3D Generation\n' +
      '\n' +
      '이 실험에서 우리의 목표는 포인트 클라우드를 입력으로 하고 기하학에 따라 TSDF를 생성하는 것이다. 포인트넷[67]과 세트트랜스포머[39]로부터의 PMA 블록을 포함하는 당사의 인코더는 다양한 개수의 포인트를 처리하는 데 부착된다. 이러한 다용도성은 25,000점으로 모델을 훈련시키는 동시에 테스트하는 동안 임의의 수의 포인트를 입력할 수 있는 유연성을 제공한다.\n' +
      '\n' +
      '표 4에 자세히 설명된 바와 같이 생성 품질이 다양한 포인트 세트에 의해 어떻게 영향을 받는지 평가하기 위한 절제 연구를 수행하고 있으며, 우리의 연구 결과는 점 수의 증가가 밸브 세트의 개선된 IoU 결과로 이어진다는 것을 보여준다. 특히, 5,000점 이하의 희소 포인트 클라우드를 사용하더라도 우리의 모델은 합리적인 IoU를 달성합니다.\n' +
      '\n' +
      '이 분석에서는 그림 14에도 시각적으로 표시되며, 여기에서는 2열에서 알 수 있듯이 더 적은 수의 포인트를 사용할 때 특정 세부 사항이 손실된다는 것을 관찰하지만 일반적으로 더 적은 점에서도 우리의 방법이 잘 수행된다는 것을 언급할 가치가 있다. 우리는 또한 그림 15에 추가 시각적 결과를 제시하며, 이 결과는 우리의 방법이 다양한 유형의 대상에 걸쳐 일관되게 잘 수행되고 주로 점 수에 대한 견고성을 나타낸다는 것을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c} \\hline \\hline \\multirow{2}{*}{Metrics} & \\multicolumn{4}{c}{Number of Points} \\\\  & 2500 & 5000 & 10000 & 25000 \\\\ \\hline LFD \\(\\downarrow\\) & 1857.84 & 1472.02 & 1397.39 & 1368.90 \\\\ IoU \\(\\uparrow\\) & 0.7595 & 0.8338 & 0.8493 & 0.8535 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 정량적 평가는 우리 모델의 성능이 투입값의 수에 크게 영향을 받지 않는다는 것을 보여준다. 5000점의 입력에도 25000점 입력에서 훈련되었지만 합리적인 재구성을 관리한다.\n' +
      '\n' +
      '그림 13: 우리의 멀티 뷰 조건 생성 모델은 다뷰 정보를 활용하여 처음 두 행의 CAD 객체들에 의해 예시된 복잡한 토폴로지와 다양하고 일관된 형상을 생성할 수 있다.\n' +
      '\n' +
      '### Voxel-to-3D Generation\n' +
      '\n' +
      '다음으로, 기하학에 따라 설계된 거리 함수 기능을 출력하는 모델에 대한 입력으로 저해상도 복셀의 사용을 탐색한다. 우리는 \\(16^{3}\\) 및 \\(32^{3}\\)의 서로 다른 복셀 해상도에서 두 가지 별개의 모델을 훈련했다. 두 모델 모두 동일한 인코더를 사용하는데, 이는 2개의 3D 컨볼루션 블록을 사용하여 입력을 조건부 잠재 벡터로 다운샘플링한다.\n' +
      '\n' +
      '두 결의에 대한 정성적 결과는 그림 16에 나타나 있는데, 이 분석에서는 크게 세 가지 점을 관측한다. 먼저, 우리의 방법은 부드럽고 깨끗한 표면을 성공적으로 만듭니다. 둘째, 복셀 \\(16^{3}\\)과 복셀 \\(3행의 인간 예시에서 볼 수 있는 것)에 대한 일부 예들에 존재하는 모호성에도 불구하고, 본 방법은 그럴듯한 형상을 생성한다(32^{3}\\). 마지막으로, 우리의 방법은 또한 이합체(인간 예에서 3열에서 입증됨) 및 장면(1열 및 7열에서 방 예시에 도시된 것)과 잘 수행한다.\n' +
      '\n' +
      '우리는 저해상도 복셀을 메쉬로 전환하기 위한 전통적인 접근법과 방법을 추가로 비교한다. 바젤의 경우 먼저 가장 가까운 이웃과 트릴린나르 보간과 같은 보간 기술을 사용하고 메서를 도출하기 위해 행진 큐브[49]를 사용한다. 중요하게도, 우리의 접근법은 이 과제를 해결하기 위한 최초의 대규모 생성 모델이다. 이 비교의 양적 및 질적 결과는 표 5와 그림 17에 나와 있으며, 기준 방법 중 3중 보간은 직관적으로 합리적인 이웃과 가장 가까운 이웃을 능가한다는 것이 분명하다. 우리의 방법은 IoU 및 LFD 메트릭 모두에서 이러한 전통적인 방법을 쉽게 능가한다.\n' +
      '\n' +
      '3D 인버터 보증.\n' +
      '\n' +
      '또한, 훈련된 무조건 생성 모델은 제로 샷 방식으로 완료 작업에 사용할 수 있습니다. 이러한 맥락에서, 목적은 부분 입력 형상의 다수의 변형을 생성하는 것이다. 형태를 감안할 때, 우리는 먼저 메쉬 상의 정점 집합으로 구성된 영역을 식별하여 재생 및 후속적으로 폐기하는 것을 목표로 한다. 메쉬의 나머지 부분은 그림 18(좌측 열)과 같이 부분 입력으로 유지된다. 이어서, 이 부분 형상을 TSDF로 변환한 후 격자 구조를 갖는 확산성 파동체 표현으로 변환한다. 이전 선택된 재생 영역을 기반으로 누락된 영역을 식별하는 바이너리 마스크를 구성하여 완성한다. 선택 마스크를 사용하여 훈련된 무조건 모델을 사용하여 이 마스킹된 지역의 웨이브렛 계수를 재생하기 위해 [50]에 설명된 접근법을 채택한다.\n' +
      '\n' +
      '그림 18은 완료 연령의 시각적 예를 보여준다.\n' +
      '\n' +
      '그림 14: 입력 포인트의 수를 기준으로 예상 비교는 사슴 뿔이나 의자 다리처럼 얇은 구조를 강력하게 생성하는 모델의 능력을 강조하며 합리적인 수의 포인트(\\(\\geq 5000\\)를 가지고 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c} \\hline \\hline Setting & Methods & LFD \\(\\downarrow\\) & IoU \\(\\uparrow\\) \\\\ \\hline \\multirow{3}{*}{Voxel (\\(16^{3}\\))} & Ours & 2266.41 & 0.687 \\\\  & Nearest & 6408.82 & 0.2331 \\\\  & Trilinear & 6132.99 & 0.2373 \\\\ \\hline \\multirow{3}{*}{Voxel (\\(32^{3}\\))} & Ours & 1580.98 & 0.7942 \\\\  & Nearest & 3970.49 & 0.4677 \\\\ \\cline{1-1}  & Trilinear & 3682.83 & 0.4719 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 우리의 방법은 전통적인 복셀 업샘플링 기술, 특히 가장 가까운 이웃 업샘플링 및 트릴린나르 보간과 정량적으로 비교되며 메쉬 추출을 위해 큐브[49]를 행진한다. 우리의 생성 모델은 라이트 필드 거리(LFD)와 교차로 오버 연합(IoU) 메트릭 모두에서 이 두 기저부를 크게 능가한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '규모 매개변수 또는 안내 중량은 \\(w\\)로 표시된다. 이 매개변수는 투입 조건에 대한 세대의 충실도와 생성된 출력의 다양성 사이의 교역을 관리하는 데 중요한 역할을 한다.\n' +
      '\n' +
      '다양한 조건부 모델에 의해 생성된 샘플의 품질에 대한 안내 가중치 매개변수의 영향을 조사하기 위한 실험을 수행하였다. 지도 가중치 매개변수는 1.0에서 5.0까지의 선형 진행에서 체계적으로 조정되었으며 효율적인 평가를 위해 모든 실험에 걸쳐 100의 추론 타임스팟이 일관되게 사용되었음을 주목하는 것이 중요하다. 본 연구의 결과는 <표 6>에 제시되어 있다.\n' +
      '\n' +
      '실증적으로 우리는 \\(2.0\\)의 지도 중량이 대부분의 조건부 생성 작업에 최적임을 관찰한다. 그러나 모델을 포인트 클라우드 데이터에 조건화할 때 \\(1.0\\)의 더 낮은 안내 중량은 더 나은 결과를 산출한다. 이는 일반적으로 안내 가중치에 대한 더 큰 값을 필요로 하는 텍스트 대 이미지 시나리오와 대조된다. 우리는 이러한 차이가 이미지 및 포인트 구름과 같이 우리가 사용하는 입력 조건의 특성상 더 많은 정보를 포함하고 있으므로 텍스트 기반 입력들에 비해 다양한 샘플들을 생성하기 더 어려워진다고 의심한다. 우리는 이러한 최적 값을 나머지 실험에서 모든 후속 추론에 대해 고정된 하이퍼파라미터로 채택하고 질적 결과의 생성을 채택한다.\n' +
      '\n' +
      '시간 단계 분석.**. 더 나아가, 우리는 또한 노력 시간 단계 분석.***에서 해결 시간 단계 분석.\n' +
      '\n' +
      '그림 16: 우리의 복셀-조건 생성 모델은 다양한 그럴듯한 기하 패턴을 상상적으로 도입하고 저해상도 입력에서 고품질 출력을 생성하는 데 탁월하다. 이는 초기 입력에서 사용할 수 없는 크라운에 구멍이 생성됨으로써 예시된다.\n' +
      '\n' +
      '조건부 모델 및 무조건 모델 모두에 대한 추론 시간표에 대한 자세한 분석을 참조하세요. 구체적으로, 우리는 위와 동일한 설정 하에서 생성 모델을 평가하지만 다양한 타임스팟, 즉 10, 100, 500 및 1000으로 평가한다.\n' +
      '\n' +
      '표 7은 추론 중 다양한 시간 단계를 사용하여 다양한 생성 모델에 대한 정량적 결과를 제시한다. 구체적으로, 우리는 작은 시간 단계(10)가 멀티뷰 이미지, 복셀 및 포인트 클라우드와 같은 최소한의 모호성을 가진 조건에 충분하다는 것을 경험적으로 발견했다. 불확실성이 높아짐에 따라 만족스러운 표본 품질을 달성하기 위한 필요한 시간 단계도 증가한다. 조건이 없는 무조건 모델의 경우 최적 시간 단계는 최대값(1000)이다. 안내 가중치와 유사하게 최적의 시간 단계를 모든 실험에서 사용되는 초매개변수라고 생각한다.\n' +
      '\n' +
      '적응 훈련 전략의***** 이 실험에서 우리는 다중 뷰 모델을 사용하고 초기에 제안된 표현을 표현의 거친 구성요소만을 사용하여 얻은 생성 결과와 비교한다. 이 접근법은 생성 대상으로 채택되었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multirow{2}{*}{Metrics} & \\multicolumn{4}{c}{Inference Time step (t)} \\\\  & & 10 & 100 & 500 & 1000 \\\\ \\hline \\multirow{2}{*}{Single-view} & LFD \\(\\downarrow\\) & 4312.23 & **4071.33** & 4136.14 & 4113.10 \\\\  & IoU \\(\\uparrow\\) & **0.4477** & 0.4285 & 0.4186 & 0.4144 \\\\ \\hline \\multirow{2}{*}{Multi-view} & LFD \\(\\downarrow\\) & **2217.25** & 2310.30 & 2369.15 & 2394.17 \\\\  & IoU \\(\\uparrow\\) & **0.6707** & 0.6595 & 0.6514 & 0.6445 \\\\ \\hline \\multirow{2}{*}{Voxels (\\(32^{3}\\))} & LFD \\(\\downarrow\\) & **1580.98** & 1683.95 & 1744.48 & 1763.91 \\\\  & IoU \\(\\uparrow\\) & **0.7943** & 0.7771 & 0.7700 & 0.7667 \\\\ \\hline \\multirow{2}{*}{Voxels (\\(16^{3}\\))} & LFD \\(\\downarrow\\) & **2266.41** & 2347.04 & 2375.89 & 2373.42 \\\\  & IoU \\(\\uparrow\\) & **0.6870** & 0.6726 & 0.6620 & 0.6616 \\\\ \\hline \\multirow{2}{*}{Point Cloud} & LFD \\(\\downarrow\\) & **1368.90** & 1429.37 & 1457.89 & 1468.91 \\\\  & IoU \\(\\uparrow\\) & **0.8535** & 0.8380 & 0.8283 & 0.8287 \\\\ \\hline Unconditional & FID \\(\\downarrow\\) & 371.32 & 85.25 & 74.60 & **68.54** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 추론 시간 단계가 다른 생성 모델의 성능을 정량적으로 평가한다.\n' +
      '\n' +
      '그림 17: 가장 가까운 이웃 업샘플링 및 트릴린나르 보간법을 사용하여 보간에서 생성된 중간체와 비교하여 우리 생성 결과는 특히 더 부드러운 표면을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c} \\hline \\hline \\multirow{2}{*}{Settings} & \\multicolumn{2}{c}{Metrics} \\\\  & LFD \\(\\downarrow\\) & IoU \\(\\uparrow\\) \\\\ \\hline Coarse component only [28] & 2855.41 & 0.5919 \\\\ \\hline Ours (MSE) & 3191.49 & 0.5474 \\\\ Ours (subband-based MSE) & 2824.28 & 0.5898 \\\\ \\hline Ours & **2611.60** & **0.6105** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 각 서브밴드에 대해 직접 또는 별도로 사용 중인 메안 스쿼드 에러(MSE)는 더 높은 이론적 표현 능력에도 불구하고 거친 성분[28]만을 사용하는 것에 비해 성능이 더 악화되었다. 대조적으로, 우리의 하위 밴드 적응 훈련 전략은 라이트 필드 거리(LFD)와 교차로 오버 연합(IoU) 메트릭 모두에서 상당한 개선을 가져왔다.\n' +
      '\n' +
      '그림 18: 부분 입력(좌표)을 제공했을 때, 우리의 무조건 생성 모델은 결측 지역을 일관성 있고 의미 있는 방식으로 완성한다. 또한 완성된 형상의 여러 변형을 생성할 수 있으며, 그 중 다수는 원래의 입력(오른쪽)과 크게 달라 학습한 다양한 형상 분포를 강조한다.\n' +
      '\n' +
      '[28]. 이것은 표 8의 첫 번째 행에 나와 있으며, 더 나아가 채택된 하위 밴드 적응 훈련 전략의 효과를 설명하기 위해 두 가지 다른 기준선 훈련 목표와 비교합니다. 먼저, 우리는 확산 가능한 표현에 대한 모든 계수에 걸쳐 균일하게 직접 MSE(평균 Squared Error) 손실을 적용한다. 이 접근법은 고전적인 확산 모델의 목표와 동일하다. 둘째, 우리는 \\(C_{0}\\), \\(D_{0}\\) 및 \\(D_{1}\\)에 대해 3개의 개별 MSE(평균 Squared Error) 손실을 구성하는 방법으로 전략을 대조한 다음 직접 합산한다. 이러한 실험 각각에 대해 별도의 모델이 필요하며 계산 자원 비용이 증가한다는 점에 유의하는 것이 중요하다. 또한 융합은 최대 몇 주까지 소요될 수 있으므로 750k 반복에서 조기 중단을 실행하고 이 단계에서 결과를 비교하기로 결정했다.\n' +
      '\n' +
      '표 8에서 거친 성분만을 사용하면 여전히 그럴듯한 결과를 얻을 수 있음을 관찰할 수 있다. 이 발견은 [28]에서 보고된 발견과 일치한다. 그러나 모든 계수에 걸쳐 MSE 손실을 균일하게 적용하면 표현 능력의 상한이 상당히 높음에도 불구하고 성능 저하가 발생한다. 우리는 손실 계산을 분리하려는 초기 시도가 훈련된 발전기의 성능을 향상시킨다는 것을 관찰한다. 그러나 결과의 품질은 거친 성분만을 사용하여 생성된 것과 유사하다. 제안된 서브밴드 적응 훈련 계획을 채택함으로써 [28]의 접근법에 비해 상당한 개선을 달성한다.\n' +
      '\n' +
      '9개의 임장 및 미래 사업을 수행합니다.\n' +
      '\n' +
      '우리의 접근법은 다음과 같은 한계를 나타내는데, (i) 무조건 모델은 다양한 하위 분류에서 다양한 형태를 생성할 수 있으며 샘플링 동안 다른 범주에 걸쳐 객체들의 균형 잡힌 표현을 보장할 수 없다. 따라서, 학습된 3D 형상 분포는 CAD 모델의 불균형적 표현에서 명백하게 본질적으로 불균형된다. 우리는 물체 카테고리를 주석하기 위해 ChatGPT와 같은 큰 제로 샷 언어 모델을 활용할 수 있으며, 이러한 카테고리에 따라 학습 데이터의 균형을 맞추기 위해 다양한 데이터 증강 방법의 적용이 가능하다. (ii) 데이터 세트의 이종 혼합으로 훈련된 우리 생성 네트워크는 카테고리 라벨을 추가 조건으로 활용하지 않는다. 따라서 무조건적인 모델은 때때로 시사할 수 없는 모양을 생성하거나 출력에 노이즈를 도입할 수 있다. 이러한 이상을 식별하고 완화하는 것은 향후 연구를 위한 강력한 방향을 나타낸다. 특히 이용 가능한 대규모 3D 데이터셋의 맥락에서 생성된 3D 형상의 시각적 그럴듯함을 평가하기 위한 데이터 기반 메트릭의 개발을 고려하는 것이 특히 흥미롭다. (iii) 현재 우리의 주요 초점은 3D 기하학의 직접적인 생성에 있다. 미래 탐구를 위한 흥미로운 방법은 계산적으로 표현된 최적화에 의존하지 않고 이를 달성하는 것을 목표로 기하학에서 질감을 함께 생성하는 것을 포함한다.\n' +
      '\n' +
      '## 10 Conclusion\n' +
      '\n' +
      '요약하면, 이 논문은 2초 이내에 고급 3D 모양을 인상적으로 생산할 수 있는 수백만 개 이상의 공개적으로 이용 가능한 3D 형상의 방대한 데이터셋에서 학습된 새로운 3D 생성 프레임워크인 메이크-A-Shape를 제시한다. 우리의 접근법의 중심은 새로운 기술의 가족을 도입하는 것이다. 여기에는 최소 정보 손실로 \\(256^{3}\\) SDF를 효과적으로 암호화하는 컴팩트하고 표현적이며 효율적인 웨이브렛 트리 표현을 구성하는 데 도움이 되는 서브밴드 계수 필터링 방식이 포함된다. 그런 다음, 우리는 하위 밴드 계수 패킹 방식을 사용하여 확산 기반 생성 모델에 의한 웨이브렛-트리 표현을 추가로 모델링하고 거친 세부 계수 및 희소 세부 계수 모두에 효과적으로 참여할 수 있는 모델 교육을 달성하기 위해 서브 밴드 적응 훈련 전략을 도출한다. 또한 다양한 모달리티의 선택적 조건 입력을 얻기 위해 Make-A-Shape를 확장합니다.\n' +
      '\n' +
      '우리의 광범위한 실험은 단일/멀티뷰 이미지, 포인트 클라우드 및 저해상도 복셀을 포함한 다양한 도전 조건에 걸쳐 고품질 3D 형태를 합성하는 데 있어 모델의 우월성을 보여주지만, 모두 훈련 중 최소한의 자원 요구 사항을 필요로 한다. 놀랍게도, 우리의 모델은 기존의 기저부를 정량적으로 능가할 뿐만 아니라 부분 형상 완료와 같은 제로 샷 응용 프로그램을 보여준다. 우리는 우리의 작업이 대규모 3D 모델 교육을 가능하게 하기 위해 다른 3D 표현에서 향후 연구의 길을 열어줄 것이라고 믿는다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and generative models for 3d point clouds. In _International conference on machine learning_, pages 40-49. PMLR, 2018.\n' +
      '* [2] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16123-16133, 2022.\n' +
      '* [3] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. _arXiv preprint arXiv:1512.03012_, 2015.\n' +
      '* [4] Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming Ouhyoung. On visual similarity based 3d model retrieval. In _Computer graphics forum_, pages 223-232. Wiley Online Library, 2003.\n' +
      '* [5] Qimin Chen, Zhiqin Chen, Hang Zhou, and Hao Zhang. Shaddr: Real-time example-based geometry and texture generation via 3d shape detailization and differentiable rendering. _arXiv preprint arXiv:2306.04889_, 2023.\n' +
      '* [6] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5939-5948, 2019.\n' +
      '* [7] Zhiqin Chen, Vladimir G Kim, Matthew Fisher, Noam Aigerman, Hao Zhang, and Siddhartha Chaudhuri. Decorgan: 3d shape detailization by conditional refinement. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 15740-15749, 2021.\n' +
      '* [8] An-Chieh Cheng, Xueting Li, Sifei Liu, Min Sun, and Ming-Hsuan Yang. Autoregressive 3d shape generation via canonical mapping. In _European Conference on Computer Vision_, pages 89-104. Springer, 2022.\n' +
      '* [9] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal 3d shape completion, reconstruction, and generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4456-4465, 2023.\n' +
      '* [10] Gene Chou, Yuval Bahat, and Felix Heide. Diffusion-sdf: Conditional generative modeling of signed distance functions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2262-2272, 2023.\n' +
      '* [11] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 21126-21136, 2022.\n' +
      '* [12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13142-13153, 2023.\n' +
      '* [13] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al. Nerdi: Single-view nerf synthesis with language-guided diffusion as general image priors. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 20637-20647, 2023.\n' +
      '* [14] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _NeurIPS_, 34:8780-8794, 2021.\n' +
      '* [15] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items. In _2022 International Conference on Robotics and Automation (ICRA)_, pages 2553-2560. IEEE, 2022.\n' +
      '* [16] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. _International Journal of Computer Vision_, 129:3313-3337, 2021.\n' +
      '* [17] Chenjian Gao, Qian Yu, Lu Sheng, Yi-Zhe Song, and Dong Xu. Sketchsampler: Sketch-based 3d reconstruction via view-dependent depth sampling. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part I_, pages 464-479. Springer, 2022.\n' +
      '* [18] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned from images. _Advances In Neural Information Processing Systems_, 35:31841-31854, 2022.\n' +
      '* [19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _Advances in neural information processing systems_, 27, 2014.\n' +
      '* [20] Benoit Guillard, Edoardo Remelli, Pierre Yvernay, and Pascal Fua. Sketch2mesh: Reconstructing and editing 3d shapes from sketches. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13023-13032, 2021.\n' +
      '* [21] Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and Daniel Cohen-Or. Meshcnn: a network with an edge. _ACM Transactions on Graphics (ToG)_, 38(4):1-12, 2019.\n' +
      '* [22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [25] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. _arXiv preprint arXiv:2311.04400_, 2023.\n' +
      '* [26] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. _arXiv preprint arXiv:2301.11093_, 2023.\n' +
      '* [27] Zhongzhan Huang, Pan Zhou, Shuicheng Yan, and Liang Lin. Scalelong: Towards more stable training of diffusion model via scaling network long skip connection. _arXiv preprint arXiv:2310.13545_, 2023.\n' +
      '* [28] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural wavelet-domain diffusion for 3D shape generation. In _ACM SIGGRAPH Asia_, pages 1-9, 2022.\n' +
      '* [29] Moritz Ibing, Isaak Lim, and Leif Kobbelt. 3d shape generation with grid-based implicit functions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13559-13568, 2021.\n' +
      '* [30] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 867-876, 2022.\n' +
      '***[31] 푸데프 쿠마르 자야만, 아디야 상히, 조셉 감버른, 카를 DD 윌리스, 토머스 다비, 호만 샤야니, 니겔 모리스 등이다. Uv-net: 경계 표현으로부터의 학습. 컴퓨터 비전 및 패턴 인식_ 페이지 11703-11712에 대한 IEEE/CVF 회의의 _발표에서 2021년 페이지 11703-11712.\n' +
      '* [32] Pradeep Kumar Jayaraman, Joseph G Lambbourne, Nishkrit Desai, Karl DD Willis, Aditya Sanghi, and Nigel JW Morris. Solidgen: An autoregressive model for direct b-rep synthesis. _arXiv preprint arXiv:2203.13944_, 2022.\n' +
      '* [33] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3D implicit functions. _arXiv preprint arXiv:2305.02463_, 2023.\n' +
      '* [34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.\n' +
      '* [35] Roman Klokov, Edmond Boyer, and Jakob Verbeek. Discrete point flow networks for efficient point cloud generation. In _European Conference on Computer Vision_, pages 694-710. Springer, 2020.\n' +
      '* [36] Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, and Daniele Panozzo. Abc: A big cad model dataset for geometric deep learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9601-9611, 2019.\n' +
      '* [37] Di Kong, Qiang Wang, and Yonggang Qi. A diffusion-refinement model for sketch-to-point modeling. In _Proceedings of the Asian Conference on Computer Vision_, pages 1522-1538, 2022.\n' +
      '* [38] Joseph G Lambourne, Karl DD Willis, Pradeep Kumar Jayaraman, Aditya Sanghi, Peter Meltzer, and Hooman Shayani. Brepnet: A topological message passing system for solid models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12773-12782, 2021.\n' +
      '* [39] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In _International conference on machine learning_, pages 3744-3753. PMLR, 2019.\n' +
      '* [40] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. _arXiv preprint arXiv:2311.06214_, 2023.\n' +
      '* [41] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-sdf: Text-to-shape via voxelized diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12642-12651, 2023.\n' +
      '* [42] Yang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng, and Matthias Niessner. 4dcomplete: Non-rigid motion estimation beyond the observable surface. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 12706-12716, 2021.\n' +
      '* [43] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. _arXiv preprint arXiv:2306.16928_, 2023.\n' +
      '* [44] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9298-9309, 2023.\n' +
      '* [45] Xinhai Liu, Zhizhong Han, Yu-Shen Liu, and Matthias Zwicker. Fine-grained 3d shape classification with hierarchical part-view attention. _IEEE Transactions on Image Processing_, 30:1744-1758, 2021.\n' +
      '* [46] Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, and Chi-Wing Fu. Iss: Image as setting stone for text-guided 3d shape generation. _arXiv preprint arXiv:2209.04145_, 2022.\n' +
      '* [47] Zhengzhe Liu, Jingyu Hu, Ka-Hei Hui, Xiaojuan Qi, Daniel Cohen-Or, and Chi-Wing Fu. Exim: A hybrid explicit-implicit representation for text-guided 3d shape generation. _ACM Transactions on Graphics (TOG)_, 42(6):1-12, 2023.\n' +
      '* [48] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A skinned multi-person linear model. _ACM Trans. Graphics (Proc. SIGGRAPH Asia)_, 34(6):248:1-248:16, 2015.\n' +
      '* [49] William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface construction algorithm. In _Seminal graphics: pioneering efforts that shaped the field_, pages 347-353. 1998.\n' +
      '* [50] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repairt: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11461-11471, 2022.\n' +
      '* [51] Zhaoliang Lun, Matheus Gadelha, Evangelos Kalogerakis, Subhransu Maji, and Rui Wang. 3d shape reconstruction from sketches via multi-view convolutional networks. In _2017 International Conference on 3D Vision (3DV)_, pages 67-77. IEEE, 2017.\n' +
      '* [52] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2837-2845, 2021.\n' +
      '* [53] Jonathan Masci, Davide Boscaini, Michael Bronstein, and Pierre Vandergheynst. Geodesic convolutional neural networks on riemannian manifolds. In _Proceedings of the IEEE international conference on computer vision workshops_, pages 37-45, 2015.\n' +
      '* [54] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In _2015 IEEE/RSJ international conference on intelligent robots and systems (IROS)_, pages 922-928. IEEE, 2015.\n' +
      '* [55] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruction of any object from a single image. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8446-8455, 2023.\n' +
      '\n' +
      '* [56] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4460-4470, 2019.\n' +
      '* [57] Oscar Michel, Roi Bar-On, Richard Liu, Sague Benaim, and Rana Hanocka. Text2mesh: Text-driven neural stylization for meshes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13492-13502, 2022.\n' +
      '* [58] Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or, and Ali Mahdavi-Amiri. Sked: Sketch-guided text-based 3d editing. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 14607-14619, 2023.\n' +
      '* [59] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. Autosdf: Shape priors for 3d completion, reconstruction and generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 306-315, 2022.\n' +
      '* [60] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra, and Leonidas J Guibas. Structurenet: Hierarchical graph networks for 3d shape generation. _arXiv preprint arXiv:1908.00575_, 2019.\n' +
      '* [61] Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia. Polygen: An autoregressive generative model of 3d meshes. In _International conference on machine learning_, pages 7220-7229. PMLR, 2020.\n' +
      '* [62] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for generating 3d point clouds from complex prompts. _arXiv preprint arXiv:2212.08751_, 2022.\n' +
      '* [63] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 165-174, 2019.\n' +
      '* [64] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16_, pages 523-540. Springer, 2020.\n' +
      '* [65] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. _arXiv preprint arXiv:2209.14988_, 2022.\n' +
      '* [66] Charles R Qi, Hao Su, Matthias Niessner, Angela Dai, Mengyuan Yan, and Leonidas J Guibas. Volumetric and multi-view cnns for object classification on 3d data. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5648-5656, 2016.\n' +
      '* [67] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 652-660, 2017.\n' +
      '* [68] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [69] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. _arXiv preprint arXiv:2306.17843_, 2023.\n' +
      '* [70] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [71] Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, et al. Infinite photorealistic worlds using procedural generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12630-12641, 2023.\n' +
      '* [72] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.\n' +
      '* [73] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and Michael J. Black. Generating 3D faces using convolutional mesh autoencoders. In _European Conference on Computer Vision (ECCV)_, pages 725-741, 2018.\n' +
      '* [74] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '* [75] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* [76] Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malekshan. CLIP-Forge: Towards zero-shot text-to-shape generation. In _CVPR_, pages 18603-18613, 2022.\n' +
      '* [77] Aditya Sanghi, Rao Fu, Vivian Liu, Karl DD Willis, Hooman Shayani, Amir H Khasahmadi, Srinath Sridhar, and Daniel Ritchie. Clip-sculptor: Zero-shot generation of high-fidelity and diverse shapes from natural language. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18339-18348, 2023.\n' +
      '* [78] Aditya Sanghi, Pradeep Kumar Jayaraman, Arianna Rampini, Joseph Lambourne, Hooman Shayani, Evan Atherton, and Saeid Asgari Taghanaki. Sketch-a-shape:Zero-shot sketch-to-3d shape generation. _arXiv preprint arXiv:2307.03869_, 2023.\n' +
      '* [79] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao, and Andreas Geiger. Voxgraf: Fast 3d-aware image synthesis with sparse voxel grids. _Advances in Neural Information Processing Systems_, 35:33999-34011, 2022.\n' +
      '* [80] Pratheba Selvaraju, Mohamed Nabail, Marios Loizou, Maria Masioukova, Melinos Averkiou, Andreas Andreou, Siddhartha Chaudhuri, and Evangelos Kalogerakis. Buildingnet: Learning to label 3d buildings. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 10397-10407, 2021.\n' +
      '* [81] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. _arXiv preprint arXiv:2308.16512_, 2023.\n' +
      '* [82] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3D neural field generation using triplane diffusion. In _CVPR_, pages 20875-20886, 2023.\n' +
      '* [83] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pages 2256-2265. PMLR, 2015.\n' +
      '* [84] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* [85] Stefan Stojanov, Anh Thai, and James M Rehg. Using shape to categorize: Low-shot learning with an explicit shape bias. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1798-1808, 2021.\n' +
      '* [86] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In _Proceedings of the IEEE international conference on computer vision_, pages 945-953, 2015.\n' +
      '* [87] Yongbin Sun, Yue Wang, Ziwei Liu, Joshua Siegel, and Sanjay Sarma. Pointgrow: Autoregressively learned point cloud generation with self-attention. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 61-70, 2020.\n' +
      '* [88] Nitika Verma, Edmond Boyer, and Jakob Verbeek. Feastnet: Feature-steered graph convolutions for 3d shape analysis. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2598-2606, 2018.\n' +
      '* [89] Kashi Venkatesh Vishwanath, Diwaker Gupta, Amin Vahdat, and Ken Yocum. Modelnet: Towards a datacenter emulation environment. In _2009 IEEE Ninth International Conference on Peer-to-Peer Computing_, pages 81-82. IEEE, 2009.\n' +
      '* [90] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. _ACM Transactions on Graphics (tog)_, 38(5):1-12, 2019.\n' +
      '* [91] Karl DD Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao Du, Joseph G Lambourne, Armando Solar-Lezama, and Wojciech Matusik. Fusion 360 gallery: A dataset and environment for programmatic cad construction from human design sequences. _ACM Transactions on Graphics (TOG)_, 40(4):1-24, 2021.\n' +
      '* [92] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. _Advances in neural information processing systems_, 29, 2016.\n' +
      '* [93] Rundi Wu, Chang Xiao, and Changxi Zheng. Deepcad: A deep generative network for computer-aided design models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 6772-6782, 2021.\n' +
      '* [94] Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian. Building generalizable agents with a realistic and rich 3d environment. _arXiv preprint arXiv:1801.02209_, 2018.\n' +
      '* [95] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1912-1920, 2015.\n' +
      '* [96] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. Neurallift-360: Lifting an in-the-wild 2d photo to a 3d object with 360 views. _arXiv e-prints_, pages arXiv-2211, 2022.\n' +
      '* [97] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot text-to-3d synthesis using 3d shape prior and text-to-image diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 20908-20918, 2023.\n' +
      '* [98] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. _arXiv preprint arXiv:2311.09217_, 2023.\n' +
      '* [99] Xingguang Yan, Liqiang Lin, Niloy J Mitra, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Shapeformer: Transformer-based shape completion via sparse representation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6239-6249, 2022.\n' +
      '* [100] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. Pointflow: 3d point cloud generation with continuous normalizing flows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 4541-4550, 2019.\n' +
      '* [101] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. _arXiv preprint arXiv:2206.10789_, 2(3):5, 2022.\n' +
      '* [102] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3d shape generation. _arXiv preprint arXiv:2210.06978_, 2022.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '신경정보처리시스템_, 2022년 35:21871-21885.\n' +
      '* [104] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape representation for neural fields and generative diffusion models. _arXiv preprint arXiv:2301.11445_, 2023.\n' +
      '* [105] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3DShape2VecSet: A 3D shape representation for neural fields and generative diffusion models. 42(4), 2023.\n' +
      '* [106] Xinyang Zheng, Yang Liu, Pengshuai Wang, and Xin Tong. Sdf-stylegan: Implicit sdf-based stylegan for 3d shape generation. In _Computer Graphics Forum_, pages 52-63. Wiley Online Library, 2022.\n' +
      '* [107] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, and Heung-Yeung Shum. Locally attentional sdf diffusion for controllable 3d shape generation. _arXiv preprint arXiv:2305.04461_, 2023.\n' +
      '* [108] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5826-5835, 2021.\n' +
      '* [109] Qingnan Zhou and Alec Jacobson. Thingi10k: A dataset of 10,000 3d-printing models. _arXiv preprint arXiv:1605.04797_, 2016.\n' +
      '* [110] Silvia Zuffi, Angjoo Kanazawa, David Jacobs, and Michael J. Black. 3D menagerie: Modeling the 3D shape and pose of animals. In _IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2017.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>