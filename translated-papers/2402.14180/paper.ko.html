<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 선형 변압기는 다재다능한 상황 학습자\n' +
      '\n' +
      'Max Vladymyrov\n' +
      '\n' +
      '요한스 폰 오스왈드\n' +
      '\n' +
      'Mark Sandler\n' +
      '\n' +
      'Rong Ge\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근의 연구는 변압기, 특히 선형 주의 모델이 순방향 추론 단계 동안 컨텍스트 내에서 제공된 데이터에 대해 암묵적으로 기울기 감소 유사 알고리즘을 실행한다는 것을 입증했다. 그러나, 더 복잡한 문제들을 다루는 그들의 능력은 아직 밝혀지지 않았다. 본 논문에서는 임의의 선형 변압기가 암시적 선형 모델을 유지함을 증명하고, 선조건 경사 하강의 변형을 수행하는 것으로 해석될 수 있다. 또한 훈련 데이터가 다양한 수준의 노이즈로 손상되는 어려운 시나리오에서 선형 변압기의 사용을 조사한다. 놀랍게도, 우리는 이 문제에 대해 선형 변압기가 복잡하고 매우 효과적인 최적화 알고리즘을 발견하여 많은 합리적인 기준선에서 능가하거나 일치한다는 것을 보여준다. 본 논문에서는 이 알고리즘을 역공학하여 잡음레벨에 기반한 운동량 및 적응 리스케일링을 통합한 새로운 접근방법임을 보인다. 우리의 연구 결과는 선형 변압기도 정교한 최적화 전략을 발견하는 놀라운 능력을 가지고 있음을 보여준다.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '트랜스포머 아키텍처(Vaswani et al., 2017)는 기계 학습, 자연 언어 처리, 컴퓨터 비전 및 그 너머에 걸친 돌파구를 주도하는 분야에 혁명을 일으켰다. 강력한 기초 모델의 백본이 되었다(Anil et al., 2023; Achiam et al., 2023; Team et al., 2023; Jiang et al., 2023). 그러나 그들의 광범위한 성공에도 불구하고, 그들의 성과를 이끄는 메커니즘은 여전히 활발한 연구 영역으로 남아 있다. 그들의 성공의 핵심 요소는 명시적인 파라미터 업데이트 없이 입력 시퀀스 자체 내에서 제공된 정보에 기초하여 예측을 수행하는 트랜스포머의 출현 능력인 상황 내 학습(ICL, Brown 등, 2020)에 기인한다.\n' +
      '\n' +
      '최근, 몇몇 논문들(Garg et al., 2022; Akyurek et al., 2022; von Oswald et al., 2023)은 ICL이 입력 컨텍스트(일명 mesa-optimization Hubinger et al., 2019) 상에서 발생하는 변압기의 암시적 메타-optimization에 의해 부분적으로 설명될 수 있다고 제안했다. 그들은 선형 회귀 작업에 대해 훈련된 선형 자기 주의 층을 갖는 변압기(일명 선형 변압기)가 구배 기반 최적화를 내부적으로 구현할 수 있음을 보여주었다.\n' +
      '\n' +
      '구체적으로, von Oswald et al. (2023)은 선형 변압기가 그레디언트 하강 알고리즘(그들이 GD\\({}^{++}\\)이라 칭함)과 유사한 알고리즘의 반복을 실행할 수 있음을 입증했으며, 각각의 어텐션 레이어는 알고리즘의 한 단계를 나타낸다. 추후, Ahn et al. (2023); Zhang et al. (2023)은 학습된 해는 사전조건화된 GD의 형태이고, 이 해는 1층 선형 변압기에 최적임을 보여주는 이러한 거동을 추가로 특성화하였다.\n' +
      '\n' +
      '이 논문에서는 선형 회귀 문제에 대해 훈련된 선형 변압기에 대해 계속 연구한다. 우리는 _any_ 선형 변압기가 암시적 선형 모델을 유지한다는 것을 증명한다. 일부 제한에서 실행되는 알고리즘은 운동량 유사 거동을 갖는 사전 조건 구배 하강의 복잡한 변형으로 해석될 수 있다.\n' +
      '\n' +
      '선형 모델을 유지하는 것은 (데이터에 관계없이) 제한적인 것처럼 보일 수 있지만, 그럼에도 불구하고 우리는 선형 변압기가 강력한 최적화 알고리즘을 발견할 수 있다는 것을 발견한다. 첫 번째 예로서, GD({}^{++}\\)의 경우, 프리컨디셔너가 2차 최적화 알고리즘을 생성함을 증명한다.\n' +
      '\n' +
      '또한, 선형 변압기가 훨씬 더 강력하고 복잡한 알고리즘을 발견하도록 훈련될 수 있음을 입증한다. 우리는 다양한 잡음 레벨1(Bai et al., 2023에 의해 영감을 받은)을 갖는 혼합 선형 회귀를 고려하기 위해 문제 공식화를 수정했다. 이것은 입력에서 다양한 수준의 소음을 고려해야 하기 때문에 명백한 폐쇄형 솔루션이 없는 더 어렵고 자명하지 않은 문제이다.\n' +
      '\n' +
      '각주 1: 구글 리서치\\({}^{2}\\)듀크 대학교. 대응: Max Vladymyrov \\(<\\)mxv@google.com\\(>\\)\n' +
      '\n' +
      '두 가지 다른 잡음 분산 분포(균일 및 범주형)를 사용한 실험은 선형 변압기의 현저한 유연성을 보여준다. 이러한 환경에서 선형 변압기를 훈련시키는 것은 GD\\({}^{++}\\) 뿐만 아니라 리지 회귀의 정확한 폐쇄형 해로부터 유도된 다양한 기저선들을 능가하는 알고리즘으로 이어진다. 우리는 이 결과가 대각 가중치 행렬로 선형 변환기를 훈련할 때에도 성립한다는 것을 발견한다.\n' +
      '\n' +
      '세부적인 분석을 통해 학습된 알고리즘을 역공학하여 운동량 유사 항과 잡음 수준에 기반한 적응적 재스케일링을 포함한 GD\\({}^{++}\\)의 주요 차이점을 밝혀냈다.\n' +
      '\n' +
      '본 연구 결과는 변압기 무게의 역공학을 통해 새로운 고성능 알고리즘이 직접 발견된 연구 발전에 기여한다. 이 연구는 주의 기반 모델의 암시적 학습 능력에 대한 이해를 확장하고 문맥 내 학습자로서 단순한 선형 변압기의 놀라운 범용성을 강조한다. 우리는 변압기가 최적화 및 기계 학습에서 일반적으로 최첨단 알고리즘을 발전시킬 수 있는 효과적인 알고리즘을 발견할 가능성이 있음을 보여준다.\n' +
      '\n' +
      '## 2 Preliminaries\n' +
      '\n' +
      '이 절에서는 선형 변압기에 대한 표기, 데이터 및 우리가 고려하는 문제 유형을 소개한다.\n' +
      '\n' +
      '### 선형 변압기와 상황 내 학습\n' +
      '\n' +
      '주어진 입력 시퀀스 \\(e_{1},e_{2},...,e_{n}\\in\\mathbb{R}^{d}\\), 선형 자기 집중 계층에서 단일 헤드는 일반적으로 4개의 행렬, 키 \\(W_{K}\\), 질의 \\(W_{Q}\\), 값 \\(W_{V}\\) 및 투영 \\(W_{P}\\)에 의해 매개변수화된다. 위치 \\(i\\)에서 비인과계층의 출력은 \\(e_{i}+\\Delta e_{i}\\)이고, 여기서 \\(\\Delta e_{i}\\)는 다음과 같이 계산된다.\n' +
      '\n' +
      '\\[\\Delta e_{i}=W_{P}\\left(\\sum_{j=1}^{n}\\langle W_{Q}e_{i},W_{K}e_{j}\\rangle W_{V}e_{j}\\right)\\tag{1}\\\\langle\n' +
      '\n' +
      '등가적으로, 우리는 매개변수 \\(P=W_{P}W_{V}\\)와 \\(Q=W_{K}^{\\top}W_{Q}\\)을 사용할 수 있고, 방정식은 다음과 같다.\n' +
      '\n' +
      '\\[\\Delta e_{i}=\\sum_{j=1}^{n}(e_{j}^{\\top}Qe_{i})Pe_{j}. \\tag{2}\\}\n' +
      '\n' +
      '만약 우리가 여러 개의 헤드를 가지고 있다면 \\((P_{1},Q_{1}),(P_{2},Q_{2}),...,(P_{h},Q_{h})\\), 효과는 단지 모든 헤드의 합이다.\n' +
      '\n' +
      '\\[\\Delta e_{i}=\\sum_{k=1}^{H}\\sum_{j=1}^{n}(e_{j}^{\\top}Q_{k}e_{i})P_{k}e_{j}. \\tag{3}\\}\n' +
      '\n' +
      '우리는 \\(\\theta=\\{Q_{k}^{l},P_{k}^{l}\\}_{H,L}\\)에 의해 매개변수화된 \\(L\\) 선형 자기 주의 층으로 구성된 다층 신경망으로 _linear transformer_를 정의한다. 핵심 메커니즘을 분리하기 위해 MLP 및 LayerNorm 구성요소를 제외한 단순화된 디코더 전용 아키텍처를 고려한다. 이 아키텍처는 이전 작업에서도 사용되었다(von Oswald et al., 2023a; Ahn et al., 2023).\n' +
      '\n' +
      '선형 변압기의 두 가지 버전을 고려한다: 전체 행렬로 표현된 변압기 매개변수와 대각 행렬로만 제한된 Diag.\n' +
      '\n' +
      'Von Oswald et al.(2023a)에 의해 영감을 받아, 본 논문에서는 데이터가 토큰들의 시퀀스로서 제공되는 회귀 문제를 고려한다. 각 토큰 \\(e_{i}=(x_{i},y_{i})\\in\\mathbb{R}^{d+1}\\)은 특징 벡터 \\(x_{i}\\in\\mathbb{R}^{d}\\)와 그에 상응하는 출력 \\(y_{i}\\in\\mathbb{R}\\)으로 구성된다. 또한, 질의 토큰 \\(e_{n+1}=(x_{t},0)\\)을 시퀀스에 추가한다. 여기서 \\(x_{t}\\in\\mathbb{R}^{d}\\)은 테스트 데이터를 나타낸다. 인컨텍스트 학습의 목표는 테스트 데이터 \\(x_{t}\\)에 대한 \\(y_{t}\\)을 예측하는 것이다. 우리는 질의 토큰을 무시하도록 시퀀스의 첫 번째 \\(n\\) 토큰에만 집중하도록 주의를 제한한다.\n' +
      '\n' +
      '우리는 \\(x_{i}^{l},y_{i}^{l})\\)을 사용하여 층 \\(l\\)에서 변압기의 출력에서 \\(i\\)번째 토큰을 나타낸다. 초기 레이어는 단순히 입력이다. \\((x_{i}^{0},y_{i}^{0})=(x_{i},y_{i})\\). 매개변수가 \\(\\theta\\)인 모델의 경우, 마지막 레이어에서 최종 토큰의 마지막 좌표의 음수 2를 \\(\\hat{y}_{\\theta}(\\{e_{1},...,e_{n}\\},e_{n+1})=-y_{n+1}^{L}\\)로 하여 예측값을 읽었다.\n' +
      '\n' +
      '각주 2: 실제 예측은 von Oswald et al.(2023a)과 유사하게 \\(-y_{n+1}^{l}\\)으로 설정했는데, 이는 선형 변압기가 \\(-y_{t}\\)을 예측하는 것이 더 쉽기 때문이다.\n' +
      '\n' +
      '또한 논문 전체에서 사용할 다음 표기법을 정의해 보자.\n' +
      '\n' +
      '\\quad\\lambda=\\sum_{i=1}x_{i}(x_{i})^{\\top};\\quad\\lambda=\\sum_{i=1}y_{i}x_{i};\\quad\\lambda=\\sum_{i=1}^{n}(y_{i})^{2}\\]\\[\\Sigma^{l}=\\sum_{i=1}x_{i}^{l}(x_{i}^{l};\\quad\\lambda^{l}=\\sum_{i=1}^{n}(y_{i}^{l})^{l};\\quad\\lambda^{l}=\\sum_{i=1}^{n}(y_{i}^{l})^{2}\\\\sigma^{l}=\\sum_{i=1}y_{i}^{l};\\quad\\lambda^{l}=\\sum_{i=1\n' +
      '\n' +
      '########## 잡음 회귀 모델\n' +
      '\n' +
      '모델 문제로는 잡음 선형 회귀 모델에서 생성된 데이터를 고려한다. 각 입력 시퀀스 \\(\\tau\\)에 대해 접지-진점 가중치 벡터 \\(w_{\\tau}\\sim N(0,I)\\)을 샘플링하고 \\(n\\)의 데이터 포인트를 \\(x_{i}\\sim N(0,I)\\)과 \\(y_{i}=\\langle w_{\\tau},x_{i}\\rangle+\\xi_{i}\\), 잡음 \\(\\xi_{i}\\sim N(0,\\sigma_{\\tau}^{2})으로 생성한다.\n' +
      '\n' +
      '각 시퀀스는 서로 다른 지면-진리 가중치 벡터\\(w_{\\tau}\\)를 가질 수 있지만, 시퀀스의 모든 데이터 포인트는 동일한 \\(w_{\\tau}\\)과 \\(\\sigma_{\\tau}\\)을 공유한다. 질의는 \\(x_{t}\\sim N(0,I)\\)와 \\(y_{t}=\\langle w_{\\tau},x_{t}\\rangle\\)으로 생성된다. (잡음이 독립적이므로, 우리가 \\(y_{q}\\)에 잡음을 포함하건 말건 최종 목적에 대한 가산 상수일 뿐이다.\n' +
      '\n' +
      '우리는 추가로 보통 최소 제곱(OLS) 손실을 다음과 같이 정의합니다.\n' +
      '\n' +
      '\\[L_{\\text{OLS}}(w)=\\sum_{i=1}^{n}\\left(y_{i}-\\langle w,x_{i}\\rangle\\right)^{2}. \\tag{4}\\]\n' +
      '\n' +
      'OLS 해는 잔차가 \\(r_{i}:=y_{i}-\\langle w^{*},x_{i}\\rangle)인 \\(w^{*}:=\\Sigma^{-1}\\alpha\\)이다.\n' +
      '\n' +
      '잡음(\\sigma_{\\tau}\\)이 존재하는 경우, 일반적으로 \\(w^{*}\\)는 Ground truth \\(w_{\\tau}\\)과 같지 않다. _known_noise level \\(\\sigma_{\\tau}\\)에 대해, \\(w_{\\tau}\\)에 대한 가장 좋은 추정량은 융선 회귀에 의해 제공된다:\n' +
      '\n' +
      '\\[L_{\\text{RR}}(w)=\\sum_{i=1}^{n}\\left(y_{i}-\\langle w,x_{i}\\rangle\\right)^{2}+\\sigma_{\\tau}^{2}\\|w\\|^{2}, \\tag{5}\\]\n' +
      '\n' +
      '(w_{\\sigma^{2}}^{*}:=\\left(\\Sigma+\\sigma_{\\tau}^{2}I\\right)^{-1}\\alpha\\). 물론, 실제로는 잡음의 분산이 알려져 있지 않고 데이터로부터 추정되어야 한다.\n' +
      '\n' +
      '### 고정 vs. 혼합 잡음 분산 문제\n' +
      '\n' +
      '우리는 잡음 선형 회귀 프레임워크 내에서 두 가지 다른 문제를 고려한다.\n' +
      '\n' +
      '고정 잡음 분산.이 시나리오에서 분산\\(\\sigma_{\\tau}\\)은 모든 훈련 데이터에 대해 일정하게 유지된다. 여기서, 상기 인컨텍스트 손실은:\n' +
      '\n' +
      '[L(\\theta)=\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}w_{\\tau}\\sim N(0,I)\\\\x_{i}\\sim N(0,I)\\xi_{i}\\sim N(0,\\sigma_{\\tau}^{2})\\end{subarray}}\\left[(\\hat{y}_{\\theta}(\\hat{y}_{\\theta}(\\{e_{1},...,e_{n}\\,e_{n+1})-y_{t}^{2}\\right], \\tag{6}\\c}w_{\\tau}\\sim N(0,I)\\xi_{i}\\sim N(0,\\sigma_{\\tau}^{2})\\end{subarray}\\left[(\\hat{y}_{\\theta}(\\hat{y}_{\\theta}(\\{e_{1},...,e_{n\n' +
      '\n' +
      '여기서 \\(e_{i}=(x_{i},y_{i})\\) 및 \\(y_{i}=\\langle w_{\\tau},x_{i}\\rangle+\\xi_{i}\\). 이 문제는 처음에 Garg et al.(2022)에 의해 탐구되었다. 이후, von Oswald et al.(2023)은 선형 변압기(6)가 GD\\({}^{++}\\)라고 불리는 경사 하강 용액의 형태로 수렴한다는 것을 입증하였다. 우리는 이것을 나중에 자세히 정의한다.\n' +
      '\n' +
      '혼합 잡음 분산.이 경우, 잡음 분산\\(\\sigma_{\\tau}\\)은 각 시퀀스에 대한 일부 고정 분포\\(p(\\sigma_{\\tau})\\)로부터 도출된다. 상기 인컨텍스트 학습 손실은:\n' +
      '\n' +
      '[L(\\theta)=\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}w_{\\tau}\\sim N(0,I)\\\\x_{i}\\sim N(0,I)\\xi_{i}\\sim N(0,\\sigma_{\\tau}^{2})\\\\sigma_{\\tau}\\sim p(\\sigma_{\\tau})\\end{subarray}}\\left[(\\hat{y}_{\\theta}(\\hat{y}_{\\theta}(\\{e_{1},...,e_{n}\\,e_{n+1})-y_{t}}^{2}\\right]. \\tag{7}\\c}x_{i}\\sim N(0,I)\\xi_{i}\\sim N(0,\\sigma_{\\tau}^{2})\\sigma_{\\tau}\\sim p(\\sigma_{\\tau})\\end{subarray\n' +
      '\n' +
      '이 시나리오는 잡음 분포를 변화시키기 위해 모델이 \\(w_{\\tau}\\)을 예측해야 하기 때문에 복잡성을 추가하며, 최적의 해는 일종의 잡음 추정을 포함할 가능성이 있다.\n' +
      '\n' +
      '놀랍게도, 이 간단한 수정은 풀 및 다이아그 선형 변압기가 GD\\({}^{++}\\)보다 훨씬 더 나은 솔루션으로 수렴하는 것으로 이어진다. 실제로, GD\\({}^{++}\\)는 잡음 분산 추정을 모델링하지 못하고, 대신 모든 입력 데이터에 걸쳐 단일 잡음 분산 추정으로 해석될 수 있는 해로 수렴한다.\n' +
      '\n' +
      '##3 관련 사항\n' +
      '\n' +
      'In-context Learning as Gradient Descent 우리의 작업은 In-context Learning을 입증한 작업 라인에서 영감을 받아 Gradient descent(Akyurek et al., 2022; von Oswald et al., 2023)로 볼 수 있다. 1층 선형 변압기의 경우, 여러 작업(Zhang et al., 2023; Mahankali et al., 2023; Ahn et al., 2023)이 최적의 파라미터와 훈련 동역학을 특성화하였다. 보다 최근의 연구는 아이디어를 자동 회귀 모델(Li et al., 2023; von Oswald et al., 2023) 및 비선형 모델(Cheng et al., 2023)로 확장했다. Fu et al. (2023)은 변압기가 선형 데이터에 대해 2차 뉴턴 방법과 유사하게 수행한다는 것을 발견했으며, 이를 위해 정리 5.1에서 그럴듯한 설명을 제공한다.\n' +
      '\n' +
      'LLMs에서의 문맥내 학습은 또한 미리 훈련된 LLMs에서 문맥내 학습이 어떻게 작용하는지를 연구하는 많은 작업들이 있다(Kossen et al., 2023; Wei et al., 2023; Hendel et al., 2023; Shen et al., 2023). 이러한 모델의 복잡성으로 인해 맥락 내 학습에 대한 정확한 메커니즘은 여전히 주요 개방형 문제이다. 여러 작품들(Olsson et al., 2022; Chan et al., 2022; Akyurek et al., 2024)은 복제, 토큰 번역 및 패턴 매칭과 같은 간단한 문맥 내 학습 태스크를 위한 중요한 메커니즘으로서 유도 헤드를 식별했다.\n' +
      '\n' +
      '선형 모델의 설정 이외의 변압기를 훈련시키기 위한 다른 이론들, 여러 다른 작업들(Garg et al., 2022; Tarzanagh et al., 2023; Li et al., 2023; Huang et al., 2023; Tian et al., 2023; b)은 상이한 데이터 및 모델 가정 하에서 변압기의 최적화를 고려하였다. (Wen et al., 2023)은 변압기에 의해 수행되는 "알고리즘"을 매우 강한 제약 없이 해석하기 어려울 수 있음을 보여주었다.\n' +
      '\n' +
      '혼합 선형 모델은 여러 연구에서 변압기가 선형 모델의 혼합물에서 좋은 성능을 달성할 수 있음을 관찰했다(Bai et al., 2023; Pathak et al., 2023; Yadlowsky et al., 2023). 이러한 연구 결과는 변압기가 다양한 모델 선택 기법을 구현할 수 있음을 보여주지만, 본 연구의 결과는 선형 변압기가 훈련 과정에서 많은 하이퍼파라미터를 튜닝한 흥미로운 최적화 알고리즘을 발견함으로써 이러한 문제를 해결한다는 것을 보여준다. 이러한 전략은 모델 선택을 하는 전통적인 방식과 상당히 다르다. 트랜스포머는 또한 많은 상이한 셋업에서 강력한 알고리즘을 구현할 수 있는 것으로 알려져 있다(Guo et al., 2023; Giannou et al., 2023).\n' +
      '\n' +
      '선형 및 커널형 변압기의 변압기 구조에 대한 주요 제약 조건은 길이\\(N\\)의 시퀀스에 대해 \\(O(N^{2})\\)의 시간이 소요되는 반면 선형 변압기의 경우 이를 \\(O(N)\\)로 개선할 수 있다는 것이다. Mirchandani et al.(2023)은 선형 변압기도 많은 작업에 상당히 강력하다는 것을 보여주었다. 다른 작업들(Katharopoulos et al., 2020; Wang et al., 2020; Schlag et al., 2021; Choromanski et al., 2020)은 커널/랜덤 특징과 유사한 아이디어를 사용하여 실행 시간을 거의 선형으로 개선하면서도 많은 성능을 잃지 않는다.\n' +
      '\n' +
      '##4 선형 변압기는 선형 모델을 유지\n' +
      '\n' +
      '비선형성이 있는 대형 변압기는 복잡한 모델을 나타낼 수 있지만, \\(l\\)번째 층 출력은 항상 잠재 계수(및 아마도 비선형) 계수를 갖는 입력의 선형 함수라는 의미에서 선형 변압기는 입력에 기초한 선형 모델을 유지하는 데 제한된다는 것을 보여준다.\n' +
      '\n' +
      '**정리 4.1**: \\(l\\)번째 층에서 선형 변압기의 출력이 \\((x_{1}^{l},y_{1}^{l}), (x_{2}^{l},y_{2}^{l}),..., (x_{n}^{l},y_{n}^{l}), (x_{t}^{l},y_{t}^{l}))\\이라고 가정하면, 행렬 \\(M^{l}\\), 벡터 \\(u^{l},w^{l}\\) 및 스칼라 \\(a^{l}\\)이 존재하므로,\n' +
      '\n' +
      '\\[x_{t}^{l+1} = M^{l}x_{i}+y_{i}u^{l},\\] \\[x_{t}^{l+1} = M^{l}x_{t},\\] \\[y_{t}^{l+1} = a^{l}y_{i}-\\langle w^{l},x_{i}\\rangle,\\] \\[y_{t}^{l+1} = -\\langle w^{l},x_{t}\\rangle.\\]\n' +
      '\n' +
      '이 정리는 선형변압기의 출력이 항상 잠재중량 \\(a^{l}\\)과 \\(w^{l}\\)의 선형조합으로 설명될 수 있음을 의미한다. 이것은 행렬 \\(M^{l}\\), 벡터 \\(u^{l},w^{l}\\) 및 수 \\(a^{l}\\)이 선형임을 의미하지 않는다. 사실 그것들은 상당히 복잡할 수 있는데, 그것은 우리가 아래에서 특징짓는다:\n' +
      '\n' +
      '**Lemma 4.2**. : _정리 4.1의 설정에서,\n' +
      '\n' +
      '{cc}A^{l}&b^{l}\\(c^{l})^{\\top}&d^{l}\\end{array}\\right):=\\] \\[\\sum_{k=1}^{h}\\left[P_{k}^{l}\\sum_{j=1}^{n}\\left(\\left(\\begin{array}[]{c}x_{j}^{l}\\y_{j}}^{l}\\end{array}\\right)(x_{j}^{l}}^{l}\\right)(y_{j}^{l}\\right)Q_{k}^{l}\\right]\n' +
      '\n' +
      '다음 공식_1을 사용하여 모든 층에 대해 행렬 \\(M^{l}\\), 벡터 \\(u^{l},w^{l}\\) 및 숫자 \\(a^{l}\\)을 재귀적으로 계산할 수 있다.\n' +
      '\n' +
      '[M^{l+1} = (I+A^{l})M^{l}+b^{l}(w^{l})^{\\top}\\] \\[u^{l})u^{l}+a^{l}b^{l}\\] \\[a^{l+1} = (1+d^{l})a^{l}+\\langle c^{l},u^{l}\\rangle\\] \\[w^{l})w^{l}-(M^{l})^{\\top}c^{l},\\\\\n' +
      '\n' +
      '_with the init. condition \\(a^{0}=1,w^{0}=0,M^{0}=I,u^{0}=0\\)_\n' +
      '\n' +
      '매개 변수에 대한 업데이트가 복잡하고 비선형적이라는 것을 알 수 있습니다. 이를 통해 선형 변압기는 창의적이고 강력한 알고리즘을 구현할 수 있으며, 나중에 섹션 5에서 볼 수 있듯이 실제로 \\(P\\) 및 \\(Q\\) 행렬을 대각선으로 제한하더라도 선형 변압기는 여전히 매우 유연하다. 이 경우의 업데이트들은 보다 친숙한 형태로 더 단순화될 수 있다:\n' +
      '\n' +
      '**Lemma 4.3**.: _ 대각 파라미터(9)를 갖는 정리 4.1의 설정에서, 파라미터 \\(u^{l},w^{l}\\)는_\n' +
      '\n' +
      '(I-\\Lambda^{l})u^{l}+\\Gamma^{l}\\Sigma\\left(a^{l}w^{l}-w^{l} \\right);\\] \\[w^{l+1} =(1+s^{l})w^{l}-\\Pi^{l}\\Sigma(a^{l}w^{l}-w^{l})-\\Phi^{l}u^{l}.\\\n' +
      '\n' +
      '(\\Lambda^{l},\\Gamma^{l},s^{l},\\Pi^{l},\\Phi^{l}\\)은 Lemma 4.2._emma에서 \\(M^{l},u^{l},a^{l},w^{l}\\)에 의존하는 행렬과 숫자이다.\n' +
      '\n' +
      '주목할 점은 \\(\\Sigma\\left(a^{l}w^{*}-w^{l}\\right)\\)는 선형모델 \\(f(w^{l})=\\sum_{i=1}^{n}(a^{l}y_{i}-\\langle w^{l},x_{i}\\rangle)^{2}\\)의 기울기에 비례한다는 것이다. 이것은 업데이트들을 운동량을 갖는 경사 하강과 유사하게 만든다:\n' +
      '\n' +
      '\\[u^{l+1}=(1-\\beta)u^{l}+\\nabla f(w^{l});w^{l+1}=w^{l}-\\eta u^{l}.\\\n' +
      '\n' +
      '물론, Lemma 4.3의 공식은 \\(\\beta\\)와 \\(\\eta\\)의 위치에 있는 행렬과 \\(w\\)의 갱신에 대한 기울기 항을 포함하여 여전히 훨씬 더 복잡하다.\n' +
      '\n' +
      '##5 대각선 주의행렬의 검정력\n' +
      '\n' +
      '이전 섹션에서는 선형 변압기가 항상 선형 모델을 유지한다는 점에서 선형 변압기가 제한적이라는 것을 보았다. 그러나, 이것이 반드시 그들이 몇몇 흥미로운 맥락 내 학습 문제들을 해결할 수 없다는 것을 의미하지는 않는다. 경험적으로, 선형 변압기는 혼합 잡음 분산(7)으로 선형 회귀를 매우 정확하게 해결할 수 있다는 것을 발견했다. 놀랍게도, 선형 변압기의 \\(Q\\) 및 \\(P\\) 행렬 (3)이 대각선으로 구속될 때에도 최종 손실은 현저하게 일정하게 유지된다. 이 섹션에서 우리는 이 특별한 경우를 연구하고 그것들이 왜 그렇게 잘 작동하는지 이해할 것이다.\n' +
      '\n' +
      '\\(x\\)의 원소들은 순열 불변이기 때문에 대각 파라미터화는 각 주의 헤드를 단지 네 개의 파라미터로 감소시킨다:\n' +
      '\n' +
      '[P_{k}^{l}=\\left(\\begin{array}{cc}p_{x,k}^{l}I&0\\\\0&p_{y,k}^{l}\\end{array}\\right);\\quad Q_{k}^{l}=\\left(\\begin{array}{cc}q_{x,k}^{l}I&0\\\\0&q_{y,k}^{l}\\end{array}\\right); \\tag{8}\\w}\n' +
      '\n' +
      '상기 선형 변압기(3)를 사용하여 추가로 재패러메트리하는 것이 유용할 것이다:\n' +
      '\n' +
      '\\sum_{H}p_{x,k}^{l}q_{x,k}^{l},\\quad w_{xy}^{l}=\\sum_{H}p_{x,k}^{l}q_{y,k}^{l},\\tag{9}\\\\[w_{yx}^{l}=\\sum_{H}p_{y,k}^{l}q_{x,k}^{l},\\quad w_{y}^{l}=\\sum_{H}p_{y,k}^{l}q_{y,k}^{l},\\quad w_{y}^{l}=\\sum_{H}p_{y,k}^{l}q_{y,k}^{l}=\\sum_{H}p_{y,k}^{l}q_{y,k}^{l},\\quad w_{y}^{l}=\\sum_{H}p_{y,\n' +
      '\n' +
      '이것은 다음의 대각선 레이어 업데이트로 이어진다:\n' +
      '\n' +
      'l}x_{i}^{l}+w_{i}^{l}\\alpha^{l}\\l}=x_{t}^{l}+w_{xx}^{l}\\sigma^{l}^{l}y_{t}^{l}^{l}\\alpha^{l},x_{i}^{l}^{l}y_{t}^{l}\\langle\\alpha^{l},x_{t}^{l}^{l}\\langle+w_{y}^{l}\\lambda^{l}\n' +
      '\n' +
      '본질적으로, 4개의 변수 \\(w_{xx}^{l}\\), \\(w_{xy}^{l}\\), \\(w_{yx}^{l}\\), \\(w_{yy}^{l}\\)은 레이어에 걸쳐 데이터와 레이블 간의 정보 흐름을 나타낸다. 예를 들어, \\(w_{xx}^{l}\\)은 \\(x^{l}\\)에서 \\(x^{l+1}\\)으로 얼마나 많은 정보가 흐르는지를 측정하고, \\(w_{yx}^{l}\\)에서 \\(y^{l+1}\\)으로 얼마나 많은 정보가 흐르는지를 측정한다. 모형은 항상 이러한 4개의 변수에 의해 캡처될 수 있기 때문에 많은 헤드를 갖는 것은 표현력을 크게 증가시키지 않는다. 헤드가 한 개만 있는 경우 방정식 \\(w_{xx}^{l}w_{yy}^{l}=w_{xy}^{l}w_{yx}^{l}\\)은 항상 참이지만, 두 개 이상의 헤드를 가진 모델은 이러한 제한을 갖지 않는다. 그러나 경험적으로 한 개의 헤드를 가진 모델조차도 상당히 강력합니다.\n' +
      '\n' +
      '### Gd\\({}^{++}\\) 및 최소 제곱 풀기\n' +
      '\n' +
      'Von Oswald et al. (2023a)에 소개된 GD\\({}^{++}\\)는 고정 잡음 분산 문제에 대해 훈련된 선형 변압기를 나타낸다. 이것은 대각선 선형 변압기의 변형으로 모든 헤드가 \\(q_{y,k}^{l}=0\\)을 만족한다. 동역학은 \\(w_{xx}^{l}\\)과 \\(w_{yx}^{l}\\)에 의해서만 영향을 받으며, 더 간단한 업데이트로 이어진다:\n' +
      '\n' +
      '{split}& x_{i}^{l+1}=\\left(I+w_{xx}^{l}\\Sigma^{l}\\right)x_{i}^{l}\\\\\\& y_{i}^{l+1}=y_{i}^{l}+w_{yx}^{l}\\langle\\alpha^{l},x_{i}^{l}\\rangle.\\end{split}\\tag{11}\\\\times\n' +
      '\n' +
      '\\(x\\)에 대한 업데이트는 사전 조건화 역할을 하는 반면, \\(y\\)에 대한 업데이트는 현재 데이터에 기초한 기울기 하강 단계일 뿐이다.\n' +
      '\n' +
      'An et al.(2023)에 의한 기존의 분석은 GD\\({}^{++}\\)에 대한 빠른 수렴 속도를 산출하지 못했지만, 우리는 이것이 실제로 최소 제곱 문제(4)에 대한 2차 최적화 알고리즘임을 보여준다:\n' +
      '\n' +
      '**정리 5.1**.: _Given \\((x_{1},y_{1}),...,(x_{n},y_{n}),(x_{t},0)\\) 여기서 \\(\\Sigma\\)은 조건수 \\(\\kappa=\\nu/\\mu\\)의 범위 \\([\\nu,\\mu]\\)의 고유값을 갖는다. (w^{*}\\)을 최소자승문제(4)의 최적해라고 하고, \\(l=O(\\log\\kappa+\\log\\log1/\\epsilon) 단계에서 정확도로 \\(\\hat{y}-\\langle x_{t},w^{*}\\rangle|\\leq\\epsilon\\|x_{t}\\|\\w^{*}\\|\\)을 출력하는 GD\\({}^{++}\\) 알고리즘에 대한 하이퍼파라미터가 존재하도록 한다. 특히 이 작업을 해결할 수 있는 \\(l\\)-레이어 선형 변압기가 있음을 의미한다._\n' +
      '\n' +
      '일반적으로 \\(O(\\log\\log 1/\\epsilon)\\)의 수렴 속도는 뉴턴의 방법과 같은 2차 알고리즘에 의해서만 달성된다.\n' +
      '\n' +
      '### 이해 \\(w_{yy}\\) : 적응적 리스케일링\n' +
      '\n' +
      '레이어가 \\(w_{yy}^{l}\\neq 0\\)만 있으면 재스케일링 효과가 있다. 스케일링 양은 모델 선택 설정에서 추가된 노이즈의 양과 관련이 있다. 이 계층에 대한 업데이트 규칙은 다음과 같다:\n' +
      '\n' +
      '\\[y_{i}^{l+1}=\\left(1+w_{yy}^{l}\\lambda^{l}\\right)y_{i}^{l}.\\]\n' +
      '\n' +
      '우리가 볼 수 있듯이, 이것은 \\(\\lambda^{l}\\)에 의존하는 요인에 의해 \\(y\\)마다 재조정된다. (w_{yy}^{l}<0\\)일 때, 이는 이전 층에서 \\(y\\)의 규준에 기초하여 출력이 축소되는 결과를 초래한다. 이는 혼합 잡음 분산 문제에 도움이 될 수 있는데, 능선 회귀 해는 최소 제곱 해를 잡음 수준에 의존하는 요인으로 스케일링하기 때문이다.\n' +
      '\n' +
      '구체적으로, \\(\\Sigma\\approx\\mathbb{E}[\\Sigma]=nI\\)을 가정하면, 능선회귀해는 \\(w_{\\sigma^{2}}^{*}\\approx\\frac{n}{n+\\sigma^{2}}w^{*}\\)이 되며, 이는 정확히 OLS해의 축소된 버전이다. 또한 잡음이 클수록 스케일링 팩터가 작아져 음의 \\(w_{yy}\\)의 거동과 일치하였다.\n' +
      '\n' +
      '### 이해 \\(w_{xy}\\): 단계별 크기 조정\n' +
      '\n' +
      '대각선 모형의 마지막 항인 \\(w_{xy}\\)은 더 복잡한 영향을 미친다. 이것은 \\(x\\)-좌표만 변경하므로 \\(y\\)에 즉각적인 영향을 미치지 않는다. 이 과정이 \\(y\\)에 어떤 영향을 미치는지 이해하기 위해, 첫 번째 단계는 \\(w_{xy}\\neq 0\\)만 있고 두 번째 단계는 \\(w_{yx}\\neq 0\\)만 있는 단순화된 2단계 과정을 고려한다. 이 경우, 첫 번째 레이어는 \\(x_{i}\\)를 다음과 같이 업데이트할 것이다.\n' +
      '\n' +
      '\\[\\begin{split} x_{i}^{1}&=x_{i}+y_{i}w_{i}\\underset{j=1}{\\overset{n}{\\sum}y_{i}\\underset{j=1}{\\overset{n}{\\sum}(\\langle w^{*},x_{j}\\rangle+r_{j})x_{j}\\\\&=x_{i}+w_{i}\\Sigma w^{*}\\\\\\&=(\\langle w^{*},x_{i}\\rangle+r_{i})\\Sigma w^{*}\\\\\\&=(I+w_{xy}\\Sigma w^{*}(w^{*}))x_{i}+w_{xyproblem (7). 단일 헤드 선형 변압기 모델의 세 가지 유형을 평가한다:\n' +
      '\n' +
      '* 풀. 전체 매개변수 행렬을 훈련합니다.\n' +
      '* Diag. 대각 매개변수 행렬(10)을 훈련합니다.\n' +
      '* GD\\({}^{++}\\). (11)에 정의된 훨씬 더 제한된 대각선 변형입니다.\n' +
      '\n' +
      '각 실험을 위해 각 선형 변압기의 층수(\\(1\\)에서 \\(7\\))를 변화시키면서 \\(0.0001\\)의 학습률과 \\(2\\,048\\)의 배치 크기를 갖는 \\(200\\,000\\) 반복에 대한 Adam 최적화기를 사용하여 훈련한다. 특히 많은 계층의 경우 안정성 문제를 방지하기 위해 학습률을 조정해야 하는 경우도 있었다. 우리는 다른 훈련 종자를 사용한 \\(5\\) 실행 중 가장 좋은 결과를 보고한다. 우리는 \\(D=10\\)차원에서 \\(N=20\\)의 문맥 내 예제를 사용했다. 우리는 \\(100\\,000\\)의 새로운 시퀀스를 사용하여 알고리즘을 평가했다.\n' +
      '\n' +
      '우리는 조정된 평가 손실을 주요 성능 메트릭으로 사용한다. 예측 변수의 손실에서 오라클 손실을 빼서 계산됩니다. 오라클 손실은 잡음 분산(\\sigma_{\\tau}\\)이 알려져 있다고 가정할 때 리지 회귀 손실(5)의 닫힌 형태의 해이다. 조정된 평가 손실을 통해 다양한 노이즈 분산에 걸쳐 직접 모델 성능 비교가 가능합니다. 이것은 더 높은 잡음이 모델 예측을 상당히 저하시키기 때문에 중요하다. 우리의 조정은 가법 상수에 의해 손실만 수정하기 때문에 모델의 최적화 프로세스에 영향을 미치지 않는다.\n' +
      '\n' +
      '기준선 추정.선형 변압기를 능선 회귀 문제에 대한 닫힌 형태의 해(5)에 대해 평가하였다. 우리는 다음과 같은 방법을 사용하여 잡음 분산\\(\\sigma_{\\tau}\\)을 추정했다.\n' +
      '\n' +
      '* _ Constant Ridge Regression(ConstRR)_ 잡음 분산은 모든 시퀀스에 대해 단일 스칼라 값을 사용하여 추정되며, 각 혼합 분산 문제에 대해 별도로 조정된다.\n' +
      '* _Adaptive Ridge Regression (AdARR)_ 비편향 추정기(Cherkassky & Ma, 2003)\\(\\sigma_{\\text{est}}^{2}=\\frac{1}{n-d}\\sum_{j=1}^{n}(y_{j}-\\hat{y}_{j})^{2}\\)를 통해 잡음 분산을 추정하며, 여기서 \\(\\hat{y}_{j}\\)은 닫힌 형태에서 발견되는 보통 최소 제곱(4)에 대한 해를 나타낸다.\n' +
      '*_Tuned Adaptive Ridge Regression (TunedRR) 위와 동일하지만 노이즈가 추정된 후 평가 손실을 최소화하기 위해 두 가지 추가 매개변수를 조정했다. threshold value for the estimated variance, (2) the multiplicative adjustment to the noise estimator. 이 값은 각 문제에 대해 별도로 조정됩니다.\n' +
      '\n' +
      '그림 1: 층수가 다른 모델에 걸쳐 잡음이 있는 선형 회귀 문제에 대한 문맥 내 학습 성능과 \\(\\sigma_{\\tau}\\sim U(0,\\sigma_{max})\\)에 대한 \\(\\sigma_{max}\\) 각각의 마커는 주어진 수의 레이어들을 갖는 별도로 트레이닝된 모델에 대응한다. 대각선 주의 가중치(Diag)가 있는 모델은 전체 주의 가중치(Full)가 있는 모델과 일치합니다. 고정 잡음(GD\\({}^{++}\\))에 특화된 모델들은 일정한 잡음(ConstRR)을 갖는 Ridge Regression 솔루션과 유사하게 성능이 좋지 않다. 기준선 중 튜닝된 정확한 Ridge Regression Solution(TunedRR)만이 선형 변압기와 비슷하다.\n' +
      '\n' +
      '그림 2: 선형 변압기 모델은 혼합 잡음 분산 \\(\\sigma_{\\tau}\\sim U(0,5)\\)을 갖는 데이터에 대해 훈련될 때 층당 오차가 일관되게 감소하는 것을 보여준다. 오차 막대는 \\(5\\) 훈련 종자에 대한 분산을 측정한다.\n' +
      '\n' +
      '위의 모든 기준선은 닫힌 형식의 비반복 솔루션인 능선 회귀를 기반으로 합니다. 따라서 행렬 역산에 접근할 수 없는 선형 변압기에 비해 알고리즘적인 이점이 있다. 이러한 기준선은 엄격하게 동등한 비교가 아닌 상한을 설정하여 가능한 최고의 성능을 측정하는 데 도움이 됩니다.\n' +
      '\n' +
      '우리의 방법과 더 충실한 비교는 행렬 역산을 사용하지 않는 AdaRR의 반복 버전일 것이다. 대신 경사 하강법을 사용하여 잡음과 융선 회귀에 대한 해를 추정할 수 있다. 그러나 실제로 이 기울기 하강 추정기는 \\(\\approx 100\\) 반복 후에만 AdaRR로 수렴한다. 대조적으로, 선형 변압기는 일반적으로 \\(10\\) 이하의 층에 수렴한다.\n' +
      '\n' +
      '우리는 \\(\\sigma_{\\tau}\\)의 분포에 대한 두 가지 선택을 고려한다:\n' +
      '\n' +
      '* _Uniform._\\ (\\sigma_{\\tau}\\sim U(0,\\sigma_{max})\\)는 \\(\\sigma_{max}\\)으로 경계를 이루는 균일한 분포로부터 유도되었다. 우리는 0에서 7까지의 범위의 \\(\\sigma_{max}\\) 시나리오를 여러 번 시도했다.\n' +
      '* _Categorical._\\ (\\sigma_{\\tau}\\in S\\)는 이산집합 \\(S\\)에서 선택된다. 우리는 \\(S=\\{1,3\\}\\)과 \\(S=\\{1,3,5\\}\\)을 시험하였다.\n' +
      '\n' +
      '제안된 방법은 범주형 분산 선택만을 고려한 Bai et al.(2023)이 연구한 문제를 일반화하고 두 개의 \\(\\sigma_{\\tau}\\) 값으로만 실험을 보여준다.\n' +
      '\n' +
      '균일한 잡음 분산. 우리는 균일한 잡음 분산으로 시작한다. 도. 도 1은 다양한 레이어의 수와 잡음 임계값 \\(\\sigma_{max}\\)으로 훈련된 서로 다른 모델의 성능을 보여준다. 특히 Full과 Diag는 서로 다른 층수와 서로 다른 \\(\\sigma_{max}\\)에서 비슷한 성능을 보인다. 반면에 GD\\({}^{++}\\)는 ConstRR 기준선의 성능에 근접하여 더 높은 값으로 수렴한다.\n' +
      '\n' +
      '\\(\\sigma_{max}\\)가 증가함에 따라 선형 변압기는 기준선에 비해 분명한 이점을 보인다. 4개의 층에서, 그들은 \\(\\sigma_{max}=4\\) 이상에서는 폐쇄형 용액 AdaRR을 능가한다. \\(5\\) 이상의 레이어를 가진 모델은 TunedRR의 성능과 일치하거나 초과합니다.\n' +
      '\n' +
      '본 논문에서는 혼합 잡음 분산 문제를 학습한 선형 변압기가 반복 알고리즘을 수행한다는 추가 가설을 검증하였으며, 각 계층은 예측 \\(y^{l}_{n+1}\\)으로 하나의 반복에 해당한다. 최종 레이어의 예측(y^{L}_{n+1}\\)에 대해서만 손실을 최소화하면서도, 각 중간 예측(y^{L}_{n+1}\\)에 대해서도 손실을 추적한다. 도. 도 2는 \\(\\sigma_{max}=5\\)의 균일한 혼합 잡음 분산 문제에 대해 트레이닝된, 층들의 수가 상이한 모델들에 대한 이러한 중간 반복들을 예시한다.\n' +
      '\n' +
      'GD\\({}^{++}\\)은 두 번째 층 이후에 각 층 이후에 손실이 점진적으로 감소한다는 것을 관찰한다 (놀랍게도, 첫 번째 층은 2개 이상의 층을 가진 모델에서 예측을 개선하지 않는다). Diag와 Full은 더 불규칙하게 행동하며 첫 번째 층과 마지막 층에서 극적인 손실 감소가 발생한다. 중간 레이어는 예측을 개선하지만 일관되게 개선하지는 않습니다. 이 동작은 이 모델들이 마지막 레이어에서 예측을 하기 전에 예측의 \\(x\\) 성분에 정보를 축적하고 있을 수 있음을 시사한다.\n' +
      '\n' +
      '상단부. 3은 7층 모델과 기준선의 성능에 대한 자세한 관점을 제공합니다. 여기에서 우리는 노이즈 분산 범위의 분산별 프로파일을 계산했다.\n' +
      '\n' +
      '그림 3: 균일 잡음 분산에 대한 모델 행동의 Per-variance profile \\(\\sigma_{\\tau}\\sim U(0,\\sigma_{max})\\)_ 상위 2개의 행: \\(\\sigma_{max}\\)이 변하는 7-레이어 모델들_ 맨 아래 행:_ 레이어 수가 다양한 고정 \\(\\sigma_{max}=5\\) 모델입니다. 배포 내 노이즈는 회색으로 음영 처리됩니다.\n' +
      '\n' +
      '0 내지 \\(\\sigma_{max}+1\\). 우리는 GD({}^{++}\\)의 성능이 떨어지는 것은 전체 잡음 분산 범위에 걸쳐 잘 추정할 수 없기 때문임을 알 수 있다. 성능은 ConstRR과 밀접하게 일치하며, 이는 후드 아래의 GD\\({}^{++}\\)도 모든 데이터에 대해 단일 상수 분산을 추정할 수 있음을 시사한다.\n' +
      '\n' +
      'AdaRR은 노이즈가 없는 문제를 완벽하게 추정하지만 노이즈 분산이 증가함에 따라 더 많은 어려움을 겪는다. 튜닝된 RR은 튜닝 가능한 파라미터에 \\(\\sigma_{max}\\)을 포함시킴으로써 추정량을 약간 향상시킨다. 그러나 그 예측은 중간 범위에서 어려움을 겪는다. Full과 Diag는 매우 유사하며 잡음 분산의 전체 스펙트럼에서 좋은 성능을 보인다. 동등성을 확정적으로 확인하거나 부인하기 위해서는 더 많은 연구가 필요하지만 이러한 모델은 유사한 성능에도 불구하고 실제로 동일하지 않다고 믿는다.\n' +
      '\n' +
      '그림 1의 하단에 있습니다. 도 3은 잡음 분산을 \\(\\sigma_{max}=5\\)로 고정하였고, 레이어가 다른 모델에 대해 퍼-분산 프로파일을 보였다. Full 및 Diag에 대한 2-layer 모델은 GD\\({}^{++}\\)와 유사하게 동작하며, 중간에서 단일 잡음 분산만을 모델링한다. 그러나, 결과는 3개 이상의 층에 대한 전체 잡음 스펙트럼에 걸쳐 빠르게 개선된다. 대조적으로, GD\\({}^{++}\\)는 차선의 해로 빠르게 수렴한다.\n' +
      '\n' +
      '범주형 노이즈 분산. 도 4는 범주형 잡음 분산 \\(\\sigma_{\\tau}\\in\\{1,3\\}\\)에 대한 Diag 모델과 Full 모델의 현저한 차이를 보여준다. 이것은 나쁜 국소 최소값에서 비롯되거나 이 문제에 대한 모델 간의 근본적인 차이를 제안할 수 있다. 흥미로운 사실은 분산별 프로파일링에서 Diag는 훈련에 사용되지 않는 분산에 대해 더 잘 외삽하는 반면 Full은 분포 내 오류가 낮음에도 불구하고 보이지 않는 분산에서 더 나쁜 성능을 발휘한다는 것을 알 수 있다.\n' +
      '\n' +
      '\\(\\sigma_{\\tau}\\in\\{1,3,5\\}\\)의 경우 그림 하단의 분산당 프로파일을 조사한다. 4는 행동의 차이를 보여준다. Full은 대각선 모델보다 더 많은 변동을 갖는 더 복잡한 분산당 프로파일을 나타내어 더 큰 표현 능력을 시사한다. 놀랍게도, 그것은 Diag에 비해 더 나은 손실 결과로 해석되지 않았다.\n' +
      '\n' +
      '쉬운 비교를 위해 부록 표 1의 모든 방법과 기준선의 결과도 요약한다.\n' +
      '\n' +
      '## 7 Conclusions\n' +
      '\n' +
      '우리는 선형 변압기도 어려운 상황 내 학습 문제를 처리할 수 있는 놀라운 능력을 가지고 있음을 보여준다. 모든 선형 변압기가 입력 데이터의 암시적 선형 모델을 유지한다는 것을 증명한다. 특정 제한 하에서, 이 모델은 운동량-유사 거동을 갖는 사전조건 구배 하강의 복잡한 변형으로 해석될 수 있다.\n' +
      '\n' +
      '노이즈 분산이 알려지지 않은 노이즈 선형 회귀 문제에 대해 훈련할 때 선형 변압기는 표준 기준선을 능가하고 노이즈 인식 단계 크기 조정 및 노이즈 수준에 기반한 재스케일링을 통합하는 정교한 최적화 알고리즘을 발견한다.\n' +
      '\n' +
      '우리의 연구 결과는 올바른 문제에 노출되었을 때 새로운 최적화 알고리즘을 발견하는 선형 변압기의 놀라운 능력을 강조한다. 이것은 변압기를 사용한 자동화된 알고리즘 발견 또는 가능한 일반화와 같은 향후 연구를 위한 흥미로운 가능성을 열어준다.\n' +
      '\n' +
      '그림 4: 조건부 잡음 분산 \\(\\sigma_{\\tau}\\in\\{1,3\\}\\)과 \\(\\sigma_{\\tau}\\in\\{1,3,5\\}\\)에 대해 층수가 다른 모델에 걸친 잡음 선형 회귀 문제에 대한 문맥 내 학습 성능 상단 행:_ 다양한 개수의 레이어를 가진 모델들에 대한 손실 및 7개의 레이어를 가진 모델들에 대한 퍼-분산 프로파일 _ 하단 행:_ 다른 도면층 수에 걸친 모델의 분산 프로파일입니다. 배포 내 노이즈는 회색으로 음영 처리됩니다.\n' +
      '\n' +
      '다른 문제들 선형 회귀를 넘어, 우리는 우리의 연구가 최적화 알고리즘을 배우고 표현하는 변압기의 능력에 대한 추가 탐구에 영감을 주길 바란다.\n' +
      '\n' +
      '우리의 연구는 선형 변압기와 같은 겉보기에 단순한 모델조차도 그들이 암묵적으로 학습하는 최적화 알고리즘에서 놀라운 복잡성을 구현할 수 있음을 강조한다. 더 많은 작업이 필요하지만, 우리는 우리의 논문이 맥락 내 학습의 이면에 있는 메커니즘을 이해하는 데 기여할 수 있기를 바란다.\n' +
      '\n' +
      '## 8 브로드캐스팅 효과\n' +
      '\n' +
      '본 논문은 머신러닝 분야의 발전을 목표로 하는 작업을 제시한다. 우리의 작업에는 많은 잠재적인 사회적 결과가 있으며, 우리가 특별히 강조해야 한다고 느끼는 것은 없다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, D., Almeida, J., Altenschmidt, S., Anadkat, S., et al. Gpt-4 기술 보고서. _ arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Ahn et al. (2023) Ahn, K., Cheng, X., Daneshmand, H., and Sra, S. 트랜스포머는 상황 내 학습을 위해 사전 조건화된 경사 하강을 구현하는 것을 학습한다. _ arXiv preprint arXiv:2306.00297_, 2023.\n' +
      '* Akyurek et al. (2022) Akyurek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. what learning algorithm is in-context learning? investigations with linear models. _ ARXiv 프리프린트 arXiv:2211.15661_, 2022.\n' +
      '* Akyurek et al. (2024) Akyurek, E., Wang, B., Kim, Y., and Andreas, J. In-Context 언어 학습: Architecture and algorithms. _ arXiv preprint arXiv:2401.12973_, 2024.\n' +
      '* Anil et al. (2023) Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. Palm 2 technical report. _ arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* Bai et al. (2023) Bai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. 통계학자로서의 트랜스포머: 상황 내 알고리즘 선택으로 상황 내 학습을 제공할 수 있다. _ arXiv preprint arXiv:2306.04637_, 2023.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models is few-shot learners. _ 신경 정보 처리 시스템_, 33:1877-1901, 2020의 발전.\n' +
      '* Chan et al. (2022) Chan, S., Santoro, A., Lampinen, A., Wang, J., Singh, A., Richemond, P., McClelland, J., and Hill, F. Data Distributional properties drive emergent in-context learning in transformers. _ 신경 정보 처리 시스템_, 35:18878-18891, 2022에서의 발전.\n' +
      '* Cheng et al.(2023) Cheng, X., Chen, Y., and Sra, S. 트랜스포머는 문맥에서 비선형 함수를 학습하기 위해 함수 경사 하강법을 구현한다. _ arXiv preprint arXiv:2312.06528_, 2023.\n' +
      '* Cherkassky & Ma (2003) Cherkassky, V. 마영 회귀 분석을 위한 모형 선택 비교 Neural computation_, 15(7):1691-1714, 2003.\n' +
      '* Choromanski et al. (2020) Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. _ arXiv preprint arXiv:2009.14794_, 2020.\n' +
      '* Fu et al. (2023) Fu, D., Chen, T. -Q., Jia, R., and Sharan, V. 트랜스포머는 상황 내 학습을 위한 고차 최적화 방법을 학습한다: 선형 모델을 사용한 연구. _ arXiv preprint arXiv:2310.17086_, 2023.\n' +
      '* Garg et al. (2022) Garg, S., Tsipras, D., Liang, P. S., and Valiant, G. Transformers learn in-context? 단순 함수 클래스 사례 연구. _ 신경 정보 처리 시스템_, 35:30583-30598, 2022에서의 발전.\n' +
      '* Giannou et al. (2023) Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped Transformers as programmable computer. _ arXiv preprint arXiv:2301.13196_, 2023.\n' +
      '* Guo et al. (2023) Guo, T., Hu, W., Mei, S., Wang, H., Xiong, C., Savarese, S., and Bai, Y. 변압기는 간단한 기능을 넘어 문맥에서 어떻게 학습하나요? 표상 학습을 위한 사례 연구. _ arXiv preprint arXiv:2310.10616_, 2023.\n' +
      '* Hendel et al. (2023) Hendel, R., Geva, M., and Globerson, A. In-context learning creates task vector. _ arXiv preprint arXiv:2310.15916_, 2023.\n' +
      '* Huang et al.(2023) Huang, Y., Cheng, Y., and Liang, Y. Context convergence of transformer. _ arXiv preprint arXiv:2310.05249_, 2023.\n' +
      '* Hubinger et al. (2019) Hubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S. 고급 기계 학습 시스템에서 학습된 최적화의 위험. _ ArXiv preprint arXiv:1906.01820_, 2019.\n' +
      '* Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. _ arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* Katharopoulos et al. (2020) Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformer with linear attention. In _International conference on machine learning_, pp. 5156-5165. PMLR, 2020.\n' +
      '\n' +
      'Kossen, J., Rainforth, T., and Gal, Y. 대형 언어 모델에서의 문맥 내 학습은 레이블 관계를 학습하지만 종래의 학습은 아니다. _ arXiv preprint arXiv:2307.12375_, 2023.\n' +
      '* Li et al. (2023) Li, Y., Ildaz, M. E., Papailiopoulos, D., and Oymak, S. 알고리즘으로서의 트랜스포머: 상황 내 학습에서의 일반화 및 안정성. In _International Conference on Machine Learning_, pp. 19565-19594. PMLR, 2023.\n' +
      '* Mahankali et al. (2023) Mahankali, A., Hashimoto, T. B., and Ma, T. 경사 하강의 한 단계는 선형 자기 주의의 한 층을 갖는 최적의 상황 내 학습기이다. _ arXiv preprint arXiv:2307.03576_, 2023.\n' +
      '* Mirchandani et al. (2023) Mirchandani, S., Xia, F., Florence, P., Ichter, B., Driess, D., Arenas, M. G., Rao, K., Sadigh, D., and Zeng, A. Large language models as general pattern machines. _ arXiv preprint arXiv:2307.04721_, 2023.\n' +
      '* Olsson et al. (2022) Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., et al. In-context learning and induction head. _ ArXiv:2209.11895_, 2022.\n' +
      '* Pathak et al. (2023) Pathak, R., Sen, R., Kong, W., and Das, A. Transformers can optim learn regression mixture models. _ arXiv preprint arXiv:2311.08362_, 2023.\n' +
      '* Schlag et al. (2021) Schlag, I., Irie, K., and Schmidhuber, J. Linear Transformers is secretly fast weight programmers. In _International Conference on Machine Learning_, pp. 9355-9366. PMLR, 2021.\n' +
      '* Shen et al. (2023) Shen, L., Mishra, A., and Khashabi, D. Do prerained transformers really learn in-context by gradient descent? _ arXiv preprint arXiv:2310.08540_, 2023.\n' +
      '*Tarzanagh et al. (2023) Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. 어텐션 메커니즘에서 최대 마진 토큰 선택 30-7차 신경 정보 처리 시스템 회의에서_, 2023.\n' +
      '* Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: Family of highly capable multimodal models. _ arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Tian et al. (2023a) Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: 1층 변압기에서 훈련 역학 및 토큰 구성에 대한 이해 _ arXiv preprint arXiv:2305.16380_, 2023a.\n' +
      '* Tian et al. (2023b) Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. Joma: mlp와 주의의 관절 역학을 통해 다층 변압기를 디미스티파잉한다. _NeurIPS 2023에서 현대 기계 학습의 수학에 대한 워크숍_, 2023b.\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention all you need. _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* von Oswald et al. (2023a) von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. 트랜스포머는 기울기 하강에 의해 문맥 내에서 학습한다. In _International Conference on Machine Learning_, pp. 35151-35174. PMLR, 2023a.\n' +
      '* von Oswald et al. (2023b) von Oswald, J., Niklasson, E., Schlegel, M., Kobayashi, S., Zucchet, N., Scherrer, N., Miller, N., Sandler, M., Vladymyrov, M., Pascanu, R., et al. Uncovering mesa-optimization algorithms in transformer. _ arXiv preprint arXiv:2309.05858_, 2023b.\n' +
      '* Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. _ arXiv preprint arXiv:2006.04768_, 2020.\n' +
      '* Wei et al. (2023) Wei, J., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., Chen, X., Liu, H., Huang, D., Zhou, D., et al. Larger language models do in-context learning differently. _ arXiv preprint arXiv:2303.03846_, 2023.\n' +
      '* Wen et al. (2023) Wen, K., Li, Y., Liu, B., and Risteski, A. Transformers are uninterpretable with myopic methods: a case study with bounded dyck grammars. 30-7차 신경 정보 처리 시스템 회의에서_, 2023.\n' +
      '* Yadlowsky et al. (2023) Yadlowsky, S., Doshi, L., and Tripuraneni, N. 데이터 혼합물 사전 훈련은 변압기 모델에서 좁은 모델 선택 기능을 가능하게 합니다. _ arXiv preprint arXiv:2311.00871_, 2023.\n' +
      '* Zhang et al. (2023) Zhang, R., Frei, S., and Bartlett, P. L. Trained Transformers learn linear models in-context. _ arXiv preprint arXiv:2306.09927_, 2023.\n' +
      '\n' +
      '## 부록 A 섹션 4에서 누락된 증명\n' +
      '\n' +
      '우리는 먼저 정리 4.1에 대한 증명을 제시하고, 그 과정에서 정리 4.1이 Lemma를 기반으로 한 귀납법으로부터 즉시 뒤따르는 것처럼 Lemma 4.2도 증명할 것이다.\n' +
      '\n' +
      '증거를요 귀납법으로 하는 겁니다 \\(l=0\\)에서 \\(a^{(0)}=1,w^{(0)}=0,M^{(0)}=I,u^{(0)}=0\\)을 설정할 수 있음을 쉽게 확인할 수 있다.\n' +
      '\n' +
      '일부 층 \\(l\\)에 대해 이것이 참이라고 가정하면, 층 \\(l\\)의 가중치 \\(P_{1}^{l},Q_{1}^{l}),...,(P_{k}^{l},Q_{k}^{l})\\)이 \\(l+1\\)의 출력에서 \\(k\\) 헤드에 대해 있다면, 다음과 같다.\n' +
      '\n' +
      '{c}x_{i}^{l+1}\\y_{i}^{l}\\end{array}\\right)=\\left(\\begin{array}{c}x_{i}^{l}\\y_{i}^{l}\\end{array}\\right)+\\sum_{k=1}^{p}\\left[P_{k}^{l}\\sum_{j=1}^{n}\\left(\\begin{array}{c}x_{j}\\y_{j}^{l}\\end{array}\\right)(x_{j}^{l}\\y_{i}\\end{array}\\right)Q_{k}^{l}\\right}\\begin{array}{c}x_{i}^{i}^{l}\\end{array}\\right}\n' +
      '\n' +
      '단지 \\(y_{n+1}=0\\)을 둠으로써 \\(i=n+1\\)에 대해서도 동일한 방정식이 참임을 주목하라. 중간 행렬이 다음의 구조를 갖도록 하자:\n' +
      '\n' +
      '[\\left(\\begin{array}{cc}A&b\\c^{\\top}&d\\end{array}\\right):=\\sum_{k=1}^{p}\\left[P_{k}^{l}\\sum_{j=1}^{n}\\left(\\left(\\begin{array}{c}x_{j}^{l}\\y_{j}^{l}\\end{array}\\right)((x_{j}^{l}}^{l}},y_{j}^{l}\\right)Q_{k}^{l}}\\right],\\w}\n' +
      '\n' +
      '그런 다음 Lemma 4.2에서와 같이 다음 레이어의 매개변수를 선택할 수 있습니다.\n' +
      '\n' +
      '[M^{l+1} = (I+A)M^{l}+b(w^{l})^{\\top}\\] \\[u^{l}1} = (I+A)u^{l}+a^{l}b\\] \\[a^{l+1} = (1+d)a^{l}+\\langle c,u^{l}\\rangle\\] \\[w^{l+1} = (1+d)w^{l}-(M^{l})^{\\top}c.\\]\n' +
      '\n' +
      '이 선택이 (12)를 만족한다는 것을 확인할 수 있다.\n' +
      '\n' +
      '다음으로 Lemma 4.3을 증명한다. 이 lemma는 실제로 Lemma 4.2의 결과이다. 먼저 알려지지 않은 행렬 \\(\\Lambda^{l},\\Gamma^{l},\\Pi^{l},\\Phi^{l})을 명시적으로 설명하는 보다 상세한 버전을 제공한다:\n' +
      '\n' +
      '**Lemma A.1**.: _the setup of The theorem 4.1 with diagonal parameters (9), one can recursively compute matrices \\(u^{l},w^{l}\\) using the following formula_\n' +
      '\n' +
      '(1+w^{l}_{xy}(a^{l})^{l}_{xx}\\Sigma^{l}\\left(M^{l}+a^{l}u^{l}(w^{*})^{\\top}\\l})\\Sigma \\left(a^{l}_{l}_{l}(M^{l})^{\\top}w^{l}(m^{l})^{\\top}w^{l}(m^{l})^{\\top}w^{l}(m^{l})\\Sigma \\\\[-a^{l}{l}w^{l}{l}{l}(m^{l})^{\\top}u^{l},\\]\n' +
      '\n' +
      '\\(\\rho=\\sum_{i=1}^{n}r_{i}^{2}\\) 및 초기 조건 \\(a^{0}=1,w^{0}=0,M^{0}=I,u^{0}=0\\)\n' +
      '\n' +
      '증거: 첫째, 특정 대각선 경우에 대해 Lemma 4.2에서 나타난 다음 행렬을 계산한다:\n' +
      '\n' +
      '{cc}A^{l}&b^{l}\\end{array}\\right =\\sum_{k=1}^{p}\\left[P^{k}^{l}\\sum_{j=1}^{n}\\left(\\left(\\begin{array}{c}x_{j}\\y_{j}^{l}\\end{array}\\right)(x_{j}^{l})^{top,y_{j}^{l}\\alpha^{l}\\alpha^{l}\\l}\\alpha^{l}\\alpha^{l}\\w^{l}\\w^{l}\\w^{l}\\w^{l}\\w^{l}\\w^{l}\\w^{l}\\w^{c}x_{j}\\w^{l}\\w^{l}\\w^{l}\\w^{l}\\w^{c}x_{j}\\w^{l}\\w^{l}\\w^{l 이는 \\(A^{l}=w_{xx}^{l}\\Sigma^{l}\\), \\(b^{l}=w_{xy}^{l}\\alpha^{l}\\), \\(c^{l}=w_{yx}^{l}\\alpha^{l}\\) 및 \\(d^{l}=w_{yy}^{l}\\lambda^{l}\\임을 의미한다. 다음에 \\(\\alpha^{l}\\)을 다시 쓴다.\n' +
      '\n' +
      '[=\\sum_{l}y_{i}-\\langle w^{l},x_{i}\\rangle(M^{l}r^{l}+\\langle a^{l}-w^{l},x_{i}\\rangle(M^{l}r^{l}+\\langle a^{l}-w^{l},x_{i}\\rho u^{l}\\] \\[=\\sum_{i=1}^{l}w^{l}(w^{*}))\n' +
      '\n' +
      '여기서 첫 번째 단계는 정리 4.1에 의해, 두 번째 단계는 \\(y_{i}\\)을 \\(\\langle w^{*},x_{i}\\rangle+r_{i}\\으로 대체하고, 세 번째 단계는 교차항을 제거하기 위해 \\(\\sum_{i=1}^{n}r_{i}x_{i}=0\\)이라는 사실을 사용한다.\n' +
      '\n' +
      '나머지 증명은 단지 \\(\\alpha^{l}\\)의 공식을 Lemma 4.2에 대입한다.\n' +
      '\n' +
      '이제 Lemma A.1은 \\(\\Lambda^{l}=-w_{xy}^{l}(a^{l})^{2}\\rho)I-w_{xx}^{l}\\Sigma^{l},\\Gamma^{l}=a^{l}w_{xy}^{l}left(M^{l}+a^{l}u^{l}(w^{*})^{\\top}\\), \\(s^{l}=tw_{yy}\\lambda^{l}(M^{l})^{l}(M^{l})^{l}(a^{l})^{l}(w^{*})^{l}) 및 \\(\\Phi^{l}=a^{l}^{l}^{l}^{l}(M^{l})^{l}(m^{l})^{l}(m^{l})^{l}(m^{l})^{\n' +
      '\n' +
      '## 부록 B 정리 5.1에 대한 증명 생략\n' +
      '\n' +
      '이 절에서는 최소자승 문제를 매우 높은 정확도로 해결하는 GD({}^{++}\\) 알고리즘에 대한 하이퍼파라미터를 찾아 정리 5.1을 증명한다. 구축의 첫 번째 단계는 반복적으로 데이터\\(x_{i}\\)의 조건을 더 잘 맞추고 마지막 단계는 기울기 하강의 단일 단계이다. 증명은 몇 가지 리마를 기반으로 하며, 먼저 데이터가 매우 잘 조절된 경우 1단계 기울기 하강이 문제를 정확하게 해결한다는 것을 관찰한다.\n' +
      '\n' +
      '**Lemma B.1**.: _Given \\((x_{1},y_{1}),...,(x_{n},y_{n})\\) 여기서 \\(\\Sigma:=\\sum_{i=1}^{n}x_{i}x_{i}^{\\top}\\)은 \\(1\\)과 \\(1+\\epsilon\\) 사이의 고유값을 갖는다. \\(w^{*}:=\\arg\\min_{w}\\sum_{i=1}^{n}(y_{i}-\\langle w,x_{i}\\rangle)^{2}\\)을 최적의 최소자승 해로 하고, \\(\\hat{w}=\\sum_{i=1}^{n}y_{i}x_{i}\\)은 \\(\\|\\hat{w}-w^{*}\\|\\leq\\epsilon\\|w^{*}\\|\\)을 만족한다.\n' +
      '\n' +
      '증명: \\(y_{i}=\\langle x_{i},w^{*}\\rangle+r_{i}\\)을 쓸 수 있다. 우리는 \\(w^{*}\\)이 최적해라는 사실에 의해 \\(r_{i}\\)이 \\(\\sum_{i=1}^{n}r_{i}x_{i}=0\\)을 만족한다는 것을 알 수 있다. 따라서 \\(\\hat{w}=\\sum_{i=1}^{n}y_{i}x_{i}=\\sum_{i=1}^{n}\\langle x_{i},w^{*}\\rangle x_{i}=\\Sigma w^{*}\\. 이 의미는\n' +
      '\n' +
      '\\[\\|\\hat{w}-w^{*}\\|=\\|(\\Sigma-I)w^{*}\\|\\leq\\|\\Sigma-I\\|\\|w^{*}\\|\\leq\\epsilon\\|w^ {*}\\|.\\]\n' +
      '\n' +
      '다음으로 GD({}^{++}\\)의 사전조건화 단계만을 적용하여 매우 빠르게 잘 조절된 \\(x\\) 행렬을 얻을 수 있음을 보인다. \\(\\Sigma\\) 행렬은 \\(\\Sigma\\leftarrow(I-\\gamma\\Sigma)\\Sigma(I-\\gamma\\Sigma)\\으로 갱신되므로 원래의 \\(\\Sigma\\) 행렬에서 \\(\\lambda\\)의 고유값은 \\(\\lambda(1-\\gamma\\lambda)^{2}\\)이 된다. 다음 리마는 이 변환이 조건 수를 줄이는 데 효과적임을 보여준다.\n' +
      '\n' +
      '**Lemma B.2**.: _Suppose \\(\\nu/\\mu=\\kappa\\geq 1.1\\), 그리고 \\(\\gamma\\nu=1/3\\)을 선택할 수 있도록 보편적인 상수 \\(c<1\\)이 존재한다.\n' +
      '\n' +
      '\\\\frac{\\max_{\\lambda\\in[\\nu,\\mu]}\\lambda(1-\\gamma\\lambda)^{2}{\\min_{\\lambda\\in[\\nu,\\mu]}\\lambda(1-\\gamma\\lambda)^{2}\\leq c\\kappa.\\\n' +
      '\n' +
      '반면에 \\(\\nu/\\mu=\\kappa\\leq 1+\\epsilon\\)에서 \\(\\epsilon\\leq 0.1\\)을 선택한다면 \\(\\gamma\\nu=1/3\\)은 \\(\\gamma\\nu=1/3\\)을 의미한다.\n' +
      '\n' +
      '\\[\\frac{\\max_{\\lambda\\in[\\nu,\\mu]}\\lambda(1-\\gamma\\lambda)^{2}{\\min_{\\lambda\\in[\\nu,\\mu]}\\lambda(1-\\gamma\\lambda)^{2}}\\leq 1+2\\epsilon^{2}.\\ 첫 번째 주장은 작은 상수일 때까지 모든 단계에서 상수 인자만큼 조건 수를 줄일 수 있음을 보여준다. 두 번째 주장은 조건수가 작으면(\\(1+\\epsilon\\)) 각 반복이 1에 훨씬 더 가까워질 수 있음을 보여준다(\\(1+O(\\epsilon^{2}))\n' +
      '\n' +
      '이제 레마를 증명해야죠\n' +
      '\n' +
      '첫째, \\(f(x)=x(1-\\gamma x)^{2}\\)이 \\(\\gamma\\nu=1/3\\)일 때 \\(x\\in[0,\\nu]\\)에 대해 단조롭게 감소하지 않는 함수(f(x)=x(1-\\gamma x)(1-3\\gamma x)^{2}\\)가 항상 음수임을 알 수 있다. 따라서, max는 \\(x=\\nu\\)에서 항상 달성되고 min은 \\(x=\\mu\\)에서 항상 달성된다. 따라서 새로운 비율은\n' +
      '\n' +
      '\\[\\frac{\\nu(1-\\gamma\\nu)^{2}}{\\mu(1-\\gamma\\mu)^{2}}=\\kappa\\frac{4/9}{(1-1/3 \\kappa)^{2}}.\\]\n' +
      '\n' +
      '\\(\\kappa\\geq 1.1\\)의 비가 \\(\\frac{4/9}{(1-1/3\\kappa)^{2}}\\)일 때, \\(\\frac{4/9}{(1-1/3.3)^{2}}\\)은 항상 \\(\\frac{4/9}{(1-1/3.3)^{2}}\\)이하이다.\n' +
      '\n' +
      '\\(\\kappa=1+\\epsilon<1.1\\)일 때 RHS를 \\(\\epsilon\\)으로 표기할 수 있다.\n' +
      '\n' +
      '\\[\\frac{\\nu(1-\\gamma\\nu)^{2}}{\\mu(1-\\gamma\\mu)^{2}}=\\kappa\\frac{4/9}{(1-1/3 \\kappa)^{2}}=(1+\\epsilon)(1+\\frac{1}{2}(1-\\frac{1}{1+\\epsilon}))^{-2}.\\]\n' +
      '\n' +
      '주의할 점은 \\(\\gamma\\)의 신중한 선택에 의해 RHS는 다음과 같은 테일러 확장을 갖는다.\n' +
      '\n' +
      '\\[(1+\\epsilon)(1+\\frac{1}{2}(1-\\frac{1}{1+\\epsilon}))^{-2}=1+\\frac{3\\epsilon^{2 }}{4}-\\frac{5\\epsilon^{3}}{4}+O(\\epsilon^{4}).\\]\n' +
      '\n' +
      '그리고 RHS는 \\(\\epsilon<0.1\\)일 때 항상 \\(2\\epsilon^{2}\\)으로 상한이 됨을 확인할 수 있다.\n' +
      '\n' +
      '두 개의 레마로 우리는 이제 주요 정리를 증명할 준비가 되었다:\n' +
      '\n' +
      '증명: Lemma B.2에 의해 우리는 \\(\\log\\kappa+\\log\\log1/\\epsilon)\\) 반복에서 \\(\\kappa\\)을 Lemma B.2의 방법으로 할당함으로써 \\(x\\)의 조건수를 \\(\\kappa^{\\prime}\\leq 1+\\epsilon/2\\kappa\\)으로 줄일 수 있음을 알 수 있다.\n' +
      '\n' +
      '이러한 반복을 수행한 후 공분산 행렬을 \\(\\sigma^{\\prime}\\)으로 하고, 고유값에 대해 \\(\\nu^{\\prime},\\mu^{\\prime}\\)을 상한과 하한으로 한다. 데이터\\(x_{i}\\)를 새로운 데이터\\(x_{i}^{\\prime}=Mx_{i}\\)로 변환하였다. \\(M=A\\Sigma^{-1/2}\\)이라 하자. 그리고 \\(M^{\\prime}=AA^{\\top}\\)이 \\(\\sqrt{\\mu^{\\prime}\\)과 \\(\\sqrt{\\nu^{\\prime}\\) 사이의 특이값을 갖는 행렬임을 알 수 있다. 최적해((w^{\\star})^{\\prime}=M^{-\\top}w^{\\star}\\)는 최대(\\sqrt{\\nu}/\\sqrt{\\mu^{\\prime}\\|w^{\\star}\\|\\)의 규격을 갖는다. 따라서 Lemma B.1에 의해 \\(\\hat{w}=\\sum_{i=1}^{n}\\frac{1}{\\mu^{\\prime}y_{i}x_{i}\\)이 \\(\\\\hat{w}-(w^{\\star})^{\\prime}\\\\\\leq(\\kappa^{\\prime}-1)\\sqrt{\\nu}/\\sqrt{\\mu^{ \\sqrt{\\mu^{\\star}\\\\\\sat{w}-(w^{\\star})^{\\prime}x_{i}\\)을 만족하는 1단계 기울기 단계를 알 수 있다. 테스트 데이터\\(x_{t}\\)도 \\(x_{t}^{\\prime}=A\\Sigma^{-1/2}x_{t}\\으로 변환되고, 알고리즘은 \\(\\langle\\hat{w},x_{t}^{\\prime\\rangle\\)을 출력하므로, 오차는 최대\\(\\sqrt{\\nu}\\|w^{\\star}\\|*\\|x_{t}^{\\prime\\\\leq(\\kappa^{\\prime}-1)\\sqrt{\\kappa^{\\prime\\\\sqrt{\\kappa^{\\prime\\\\x_{t}\\|w^{\\star}\\\\sqrt{\\kappa^{\\prime\\\\sqrt{\\kappa^{\\prime\\\\sqrt{\\kappa^{\\prime\\\\sqrt{\\kappa^{\\prime\\\\x_{t}\\|\\\\sqrt{\\kappa^{\\prime\\\\sqr \\(\\kappa^{\\prime}\\)의 선택에 의해 RHS가 기껏해야 \\(\\epsilon\\|w^{\\star}\\|\\|x_{t}\\|\\)임을 확인할 수 있다.\n' +
      '\n' +
      '## 부록 C 더 실험\n' +
      '\n' +
      '여기에서 본문에 도달하지 못한 추가 실험의 결과를 제공한다.\n' +
      '\n' +
      '도. 도 5는 조정되지 않은 손실의 예를 도시한다. 분명히, 이러한 방식으로 다양한 잡음 레벨들에 걸친 방법들을 비교하는 것은 사실상 불가능하다.\n' +
      '\n' +
      '도. 도 6은 변화하는 깊이의 네트워크의 중간 예측들의 퍼-분산 프로파일을 도시한다. GD({}^{++}\\)는 GD 기반 알고리즘의 전형적인 동작을 보여주는 것으로 보인다: 초기 반복은 더 높은 잡음을 모델링하고(초기 정지와 유사), 점점 더 낮은 잡음 예측으로 수렴한다. 디아그는 처음에 이 패터를 나타내지만, 특히 더 낮은 잡음 범위에 대해 극적으로 개선된다. 흥미롭게도 Full은 반대 경향을 나타내며 먼저 저잡음 예측을 개선하고 특히 마지막 레이어에서 더 높은 잡음 예측 정확도의 감소가 뒤따른다.\n' +
      '\n' +
      '마지막으로, 표 1은 다양한 혼합 잡음 분산 모델에 걸친 실험에 대한 포괄적인 수치 결과를 제시한다. 각 모델 변형(열로 표시됨)에 대해 가장 성능이 좋은 결과가 굵게 강조 표시됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c||c|c|c|c|c|c|c||c|c|c|} \\hline Method & \\multicolumn{8}{|c||}{Uniform \\(\\sigma_{\\tau}\\sim(0,\\sigma_{max})\\)} & \\multicolumn{2}{|c|}{Categorical \\(\\sigma_{\\tau}\\in S\\)} \\\\ \\cline{2-10}  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & \\{1,3\\} & \\{1,3,5\\} \\\\ \\hline \\hline \\multicolumn{10}{|c|}{1 layer} \\\\ \\hline GD\\({}^{++}\\) & 1.768 & 1.639 & 1.396 & 1.175 & 1.015 & 0.907 & 0.841 & 0.806 & 1.007 & 0.819 \\\\ \\hline Diag & 1.767 & 1.639 & 1.396 & 1.175 & 1.015 & 0.906 & 0.841 & 0.806 & 1.007 & 0.819 \\\\ \\hline Full & 1.768 & 1.640 & 1.397 & 1.176 & 1.016 & 0.907 & 0.842 & 0.806 & 1.008 & 0.820 \\\\ \\hline \\hline \\multicolumn{10}{|c|}{2 layers} \\\\ \\hline GD\\({}^{++}\\) & 0.341 & 0.295 & 0.243 & 0.265 & 0.347 & 0.366 & 0.440 & 0.530 & 0.305 & 0.427 \\\\ \\hline Diag & 0.265 & 0.214 & 0.173 & 0.188 & 0.219 & 0.242 & 0.254 & 0.259 & 0.201 & 0.246 \\\\ \\hline Full & 0.264 & 0.215 & 0.173 & 0.188 & 0.220 & 0.245 & 0.259 & 0.263 & 0.202 & 0.276 \\\\ \\hline \\hline \\multicolumn{10}{|c|}{3 layers} \\\\ \\hline GD\\({}^{++}\\) & 0.019 & 0.021 & 0.071 & 0.161 & 0.259 & 0.344 & 0.454 & 0.530 & 0.222 & 0.422 \\\\ \\hline Diag & 0.013 & 0.015 & 0.048 & 0.087 & 0.109 & 0.118 & 0.121 & 0.123 & 0.098 & 0.119 \\\\ \\hline Full & 0.012 & 0.015 & 0.049 & 0.075 & 0.101 & 0.117 & 0.124 & 0.127 & 0.076 & 0.113 \\\\ \\hline \\hline \\multicolumn{10}{|c|}{4 layers} \\\\ \\hline GD\\({}^{++}\\) & 9.91e-05 & 0.014 & 0.066 & 0.160 & 0.258 & 0.344 & 0.454 & 0.530 & 0.222 & 0.422 \\\\ \\hline Diag & 1.19e-04 & 0.006 & 0.024 & 0.041 & 0.050 & 0.059 & 0.065 & 0.073 & 0.043 & 0.062 \\\\ \\hline Full & 1.63e-04 & 0.005 & 0.021 & 0.038 & 0.052 & 0.065 & 0.068 & 0.076 & 0.032 & 0.061 \\\\ \\hline \\hline \\multicolumn{10}{|c|}{5 layers} \\\\ \\hline GD\\({}^{++}\\) & 1.14e-07 & 0.014 & 0.066 & 0.161 & 0.265 & 0.344 & 0.454 & 0.530 & 0.222 & 0.422 \\\\ \\hline Diag & 1.81e-07 & 0.004 & 0.016 & 0.029 & 0.041 & 0.051 & 0.058 & 0.062 & 0.026 & 0.051 \\\\ \\hline Full & 1.79e-07 & **0.002** & 0.015 & 0.026 & 0.038 & 0.048 & 0.059 & 0.065 & 0.016 & 0.048 \\\\ \\hline \\hline \\multicolumn{10}{|c|}{6 layers} \\\\ \\hline GD\\({}^{++}\\) & 2.37e-10 & 0.009 & 0.066 & 0.161 & 0.265 & 0.344 & 0.454 & 0.530 & 0.222 & 0.422 \\\\ \\hline Diag & 2.57e-10 & 0.003 & 0.014 & 0.028 & 0.040 & 0.048 & 0.054 & 0.059 & 0.020 & 0.047 \\\\ \\hline Full & 2.71e-10 & **0.002** & 0.014 & 0.025 & 0.036 & 0.044 & 0.052 & 0.059 & 0.011 & 0.043 \\\\ \\hline \\hline \\multicolumn{10}{|c|}{7 layers} \\\\ \\hline GD\\({}^{++}\\) & 2.65e-12 & 0.009 & 0.066 & 0.161 & 0.265 & 0.344 & 0.454 & 0.530 & 0.222 & 0.422 \\\\ \\hline Diag & 2.50e-12 & **0.002** & 0.014 & 0.027 & 0.040 & 0.047 & 0.052 & 0.059 & 0.018 & 0.046 \\\\ \\hline Full & 2.50e-1Figure 6: Layer by layer prediction quality for different models with \\(\\sigma_{\\tau}\\sim U(0,5)\\). The error bars measure std over \\(5\\) training seeds.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>