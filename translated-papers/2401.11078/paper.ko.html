<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '실현적인 3D 아비타 확산.\n' +
      '\n' +
      '({}^{*}\\)1, 지웨이 주안({}^{**}\\)1, 자쿠준 주안({}^{**}\\)1, 구이준 주원({}^{**}\\)1.\n' +
      '\n' +
      '미국 이노페락테크놀로지, 중국 서클라케대, 중국 이노푸락테크놀로지 2개 연구센터, 미국, 중국 이노페크테크놀로지, 2개 서클레이크대, 미국, 미국, 미국, 중국, 중국, 중국, 중국, 중국, 2개, 미국, 미국, 미국, 중국, 중국, 중국, 미국, 중국, 중국, 중국, 중국, 중국, 중국, 중국, 중국, 중국, 중국, 미국 연구센터, 2개, 2개, 1, 2개, 1, 2개, 1, 2개, 2개, 1, 2개, 1, 2개, 1, 2개, 2개, 1, 2개, 2개, 1, 2개, 2개, 1, 2개, 2개, 2개, 2개, 1, 2개, 2개, 1, 2개, 2개, 2개, 2개, 1, 2개, 2개, 2개, 2개, 2개, 2개, 1, 2개, 2개, 2개, 2개, 2개, 2개, 2개, 2개, 2개, 2개, 2개, 2개, 2개,\n' +
      '\n' +
      '{mingyuan.zhou,rakib.hyder,ziwei.xuan}@innopeaktech.com, guojjunq@gmail.com\n' +
      '\n' +
      'Equal contribution.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근 3D 아바타 세대의 발전은 상당한 관심을 얻었습니다. 이러한 돌파구는 가상 경험과 실제 경험 사이의 격차를 좁히는 보다 현실적인 애니메이션 가능한 아바타를 생산하는 것을 목표로 한다. 기존의 대부분의 작품들은 3D 아바타를 생성하는 확산 모델을 안내하기 위해 다른 렌더링기와 텍스트 조건과 결합된 스코어 교차 샘플링(SDS) 손실을 사용한다. 그러나 SDS는 종종 얼굴 디테일이 거의 없는 과도한 결과를 생성하여 조상 샘플링과 비교하여 다양성이 부족하다. 한편, 다른 작품들은 단일 이미지로부터 3D 아바타를 생성하는데, 여기서 원치 않는 조명 효과, 시각 조회 및 열등한 화질의 난제가 정렬된 완전한 질감으로 3D 얼굴 메세지를 안정적으로 재구성하기 어렵게 만든다. 본 논문에서는 기하학의 충실도가 향상된 울트리아바타라는 새로운 3D 아바타 생성 접근법과 원치 않는 조명 없이 물리적으로 기반 렌더링(PBR) 텍스처의 우수한 품질을 제안한다. 이를 위해 제안된 접근법은 확산 색 추출 모델과 진정성 유도 질감 확산 모델을 제시한다. 전자는 원치 않는 조명 효과를 제거하여 다양한 조명 조건에서 생성된 아바타를 렌더링할 수 있도록 진정한 확산 색상을 드러낸다. 후자는 다양한 얼굴-정체성 특징 및 세부 정보를 3D 메쉬 기하학과 더 잘 정렬하도록 PBR 텍스처를 생성하기 위한 두 가지 구배 기반 어이스트링을 따른다. 우리는 실험에서 최첨단 방법을 큰 차이로 능가하는 제안된 방법의 효과와 견고성을 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '3D 얼굴 아바타를 생성하는 것은 컴퓨터 비전 및 컴퓨터 그래픽의 커뮤니티에 상당한 관심을 가지고 있다. 최근의 딥러닝 발전은 AI 생성 아바타의 현실성을 크게 향상시켰다. 클라우드(SfM)[65]는 사용자 또는 특히 텍스트 프롬프트에서 생성된 단일 뷰와 같이 다양한 각도로 캡처된 여러 이미지로부터 아바타 생성을 촉진하여 다뷰 스스테로(MVS)(66])와 같은 현실적인 3D 재구성을 입증한다.\n' +
      '\n' +
      '그림 1: **UtrAvatar***입니다. 우리의 방법은 다양한 렌더링 엔진과 호환되는 PBR 텍스처가 있는 현실적인 애니메이션 가능한 3D 아바타를 생성하기 위해 입력으로 텍스트 프롬프트 또는 단일 이미지를 취하는데, 우리 세대는 광범위한 다양성, 고품질 및 우수한 충실도를 초래한다.\n' +
      '\n' +
      '제한된 가시성, 원치 않는 조명 효과 및 열등한 화질로 인한 것이다.\n' +
      '\n' +
      '이전 작품은 단일 뷰 이미지에 포함된 이용 가능한 정보를 활용하여 이러한 과제를 극복하고자 하였다. 예를 들어 [24, 46, 79]는 랜드마크 손실 및 광학적 손실을 최소화하여 3D 형태 모형(3DMM) 모델의 파라미터를 추정하는 데 초점을 맞추었고, 기타 접근법은 3DMM 매개변수 [20, 25, 64, 80]을 예측하기 위해 자체 수정 네트워크를 훈련시킨다. 이러한 방법은 교합 및 조명 조건에 민감하여 민감도 3DMM 매개변수 또는 불량한 품질 질감의 생성으로 이어진다. 더욱이, 많은 기존 작품[20, 25, 79, 80]은 얼굴 질감을 생성하기 위해 조립식 질감 기준으로 [31]에 의존한다. 이러한 질감은 조명 파라미터와 공동으로 재구성되는 경우가 많지만, 진정한 얼굴 색상과 피부 디테일이 기본 텍스처 기반에서 누락되어 복구될 수 없다. 또는 다른 작품[8, 10, 37, 73]은 신경 방사 렌더링 필드(NeRF)를 사용하여 3D 아바타를 생성하지만 3D 아바타의 메쉬 기반 애니메이션에 대해 계산적으로 요구되고 적합하지는 않다. 그들은 또한 이전에 관측되지 않은 관점에서 렌더링될 때 광현실주의가 부족할 수 있다.\n' +
      '\n' +
      '3D 아바타를 위해 설계된 생성 모델[29, 30, 43, 53, 61, 75]은 일관된 3D 메쉬 및 질감을 생성할 것을 약속하는 것으로 나타났다. 그러나 이러한 모델은 진정한 얼굴 색상에 대한 접근을 방지하고 확산 질감을 악화시킬 수 있는 원치 않는 조명 효과를 설명하지 않는다. 한편, 일부 작업은 SDS 손실 [56, 75, 76]을 사용하여 렌더링된 뷰와 확산 모델에 의해 생성된 텍스처를 정렬하여 3D 아바타를 훈련시킨다. SDS는 기저 확산 모델에서 샘플링된 원래 2D 이미지와 비교하여 피부 및 얼굴 디테일의 다양성이 부족한 과잉 괴사된 결과를 초래할 수 있다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해 확산된 색상과 피부 세부 사항에서 더 현실적인 3D 애니메이션 아바타를 만드는 새로운 접근법을 제안한다. 먼저, 우리의 접근법은 텍스트 프롬프트 또는 단일 얼굴 이미지를 입력으로 취할 수 있다. 텍스트 프롬프트는 얼굴 이미지를 생성하기 위해 일반 확산 모델에 공급되거나 단일 얼굴 이미지도 프레임워크에 입력될 수 있다. 단일 영상에서 캡처된 색상과 조명을 분리하는 것은 본질적으로 어려운 것으로 잘 알려져 있다. 원치 않는 조명에 오염되지 않은 고품질 질감을 얻기 위해, 우리의 핵심 관찰은 확산 모델에서 자기 의도 블록이 실제로 조명 효과를 포착한다는 것이며, 이는 입력 이미지의 질감으로부터 조명을 견고하게 제거하기 위해 확산 색상 추출(DCE) 모델을 제안함으로써 진정한 확산 색상을 드러낼 수 있다는 것이다.\n' +
      '\n' +
      '또한 3D 얼굴 메쉬와 일치하는 고품질 완전 안면 질감을 생성할 수 있는 진정성 유도 질감 확산 모델(AGT-DM)을 학습시킬 것을 제안한다. 생성된 3D 아바타 - 광학적 지침 및 분류기가 없는 확산 샘플링 프로세스에 추가된 에지 지침을 향상시키기 위해 두 가지 구배 검색이 제시된다. 이것은 관찰된 뷰와 관찰되지 않은 뷰에 걸쳐 안면 질감의 더 미묘한 고주파 디테일로 생성된 3D 아바타의 다양성을 개선할 수 있다.\n' +
      '\n' +
      '우리 작업의 주요 기여는 아래에 요약되어 있습니다.\n' +
      '\n' +
      '* 우리는 자기 의도적 특징과 조명 효과 간의 관계를 드러냄으로써 단일 영상에서 조명 효과를 제거하여 확산색을 추출하는 새로운 모델을 제안할 수 있게 한다. 우리의 실험은 이것이 정사각형 스포트라이트와 그림자를 제거하는 것과 같은 작업에 적합한 강력하고 효과적인 접근법이다.\n' +
      '* 우리는 PBR 텍스처를 생성하기 위해 진정성 유도 확산 모델을 도입한다. 감수성 조명 효과 없이 3D 메쉬와 잘 어울리는 고품질 완전 텍스처를 제공할 수 있습니다. 샘플링 과정은 각 동일성에 고유한 얼굴 디테일을 유지하기 위해 두 가지 구배 기반 어메니티를 따르며, 이는 개선된 세대 다양성에 기여한다.\n' +
      '* 우리는 제안된 DCE 모델과 AGT-DM에 따라 새로운 3D 아바타 생성 프레임워크 울트리아바타를 구축한다. 우리의 실험은 진정한 색상과 날카로운 디테일로 고품질의 다양한 3D 아바타를 보여준다(그림 1 참조).\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '**이미지-아바타 세대:**에 따르면 아바타 생성은 주로 복잡하고 비용이 많이 드는 스캐닝 세트에 의존하여 확장성을 제한했다[7, 13, 33, 48]. 이 과제는 단일 사진[20, 25, 80] 또는 비디오[16, 28, 32, 79]와 같은 공통 이미지 입력을 사용하는 방향으로 전환되었다. 또한, 3D 헤드의 표현은 메쉬 기반 매개변수 모델[46, 79]에서 NeRF[8, 37, 73]과 같은 유체 신경 암묵 함수에 이르기까지 다양화되었다. 첨단 신경망, 특히 유전적 적대네트웍스(GAN) [40]의 도입은 3D 속성들을 이들 생성 모델[17, 21, 52]에 직접 임베딩하고 생성 모델을 3D 아바타[29, 30, 43, 44]를 생성하기 전에 사용하게 하였다. 일부 기술[10, 23]은 또한 단일 이미지로부터 엄격한 아바타를 생성할 수 있다. 그럼에도 불구하고 이러한 기존 방법은 메쉬 및 텍스처 생성을 위한 제공된 이미지에 의존하며, 종종 메시 생성의 오류와 이미지 상의 종방향 하이라이트 및 그림자와 같은 조명으로 인한 확산 텍스처 생성의 문제를 접한다. 제안된 방법은 이러한 문제를 엄격하게 다루고 완화하여 보다 일관되고 정확한 결과를 보장합니다.\n' +
      '\n' +
      '**Text-to-3D 세대:** Text-to-3D 생성은 텍스트 대 이미지 모델의 성공을 기반으로 하는 인기 있는 연구 주제[51, 58, 59, 60]이다. 드림퓨전[56], 매직3D[47], 라텐트-NeRF[49], 아비타르CLIP[36], 클라핀[9], 로빈[70] 등 3D 생성을 안내하는 텍스트 프롬프트를 사용한다. 이러한 접근법의 대부분은 확산 모델에 의해 생성된 이미지와 3D 객체 간의 일관성을 유지하기 위해 SDS 손실을 사용한다. 그러나 SDS 손실은 세대의 다양성을 크게 제한한다. 우리의 접근법은 대규모 데이터에 대해 훈련된 확산 모델에서 강력한 이미지 생성 능력을 채택하여 다양성을 촉진한다. 동시에, SDS 손실에 의존하지 않고 텍스트 프롬프트와 생성된 아바타 사이의 높은 정도의 충실도를 보장한다.\n' +
      '\n' +
      '안내된 확산 모델: 확산 모델의 두드러진 특징은 샘플링 과정을 테일러 출력에 안내함으로써 달성된 훈련 후 적응성에 있다. 유도 확산의 개념은 이미지 초해상도 [18, 27, 63], 색상화[19, 62], 디버링[18, 71], 스타일 전달 [26, 41, 42]과 같은 작업을 포함하는 다양한 응용 분야에서 광범위하게 탐구되었다. 최근 연구에 따르면 확산 U-Net의 중간 특징은 생성된 이미지[12, 34, 42, 57, 69]의 구조와 내용에 대한 정보가 풍부하다. 우리는 주의 기능이 이미지 내의 조명을 나타낼 수 있음을 발견하고 주어진 이미지로부터 확산색을 추출하는 방법을 제안한다. 또한 생성된 아바타의 진정성과 사실성을 보장하기 위해 두 개의 어메니티를 통합했습니다.\n' +
      '\n' +
      '방법 3.\n' +
      '\n' +
      '우리의 프레임워크에 대한 개요는 그림 2에 나와 있다. 우리는 얼굴 이미지를 입력으로 취하거나 확산 모델을 가진 아바타의 뷰 \\(I\\)를 생성하기 위해 텍스트 프롬프트를 사용한다. 그런 다음 생성된 이미지에서 원치 않는 조명을 제거하여 확산색을 회복하기 위한 DCE 모델을 소개합니다. 이 과정은 사각형, 그림자 등의 조명 효과에 의해 악화되지 않고 고품질 질감을 생성하는 것이 핵심이다. 이것은 또한 생성된 아바타를 새로운 조명 조건에서 적절하게 렌더링할 수 있도록 보장합니다. 우리는 3D 얼굴 모델(예: 3DMM 모델)을 적용하여 결과 확산 얼굴 이미지와 정렬된 메쉬를 생성한다. 마지막으로, 우리는 확산 색상, 정상, 사각형 및 거칠기 질감을 포함하여 PBR 질감을 생성하기 위해 여러 복호기로 AGT-DM을 훈련한다. 이 완전한 PBR 텍스처 세트는 3D 메쉬와 잘 정렬될 수 있으며 개별 동일성에 고유한 얼굴 세부 정보를 보존할 수 있다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '확산 모델은 잡음을 점진적으로 제거함으로써 조건 \\(y\\)로 랜덤 노이즈를 선명한 이미지로 무한히 변환하도록 학습한다. 이 모델은 두 가지 필수 프로세스를 기반으로 합니다. 정방향 과정은 투명한 이미지 \\(x_{0}\\)로 시작하여 잡음을 증분적으로 도입하여 시끄러운 이미지 \\(x_{T}\\)에서 도태하고, 후방 공정은 \\(x_{T}\\)에서 노이즈를 점진적으로 제거하여 투명 이미지 \\(x_{0}\\)를 복원한다. 안정적인 확산(SD) 모델[55, 60]은 이미지 \\(x\\)를 잠재 표현으로 인코딩하여 잠재 공간 \\(z=E(x)\\) 내에서 동작한다. 최종 변성 RGB 이미지는 \\(x_{0}=D(z_{0})\\를 통해 잠재 영상을 디코딩하여 얻는다. 순차적인 데모를 수행하기 위해 네트워크 \\(\\epsilon_{\\theta}\\)는 목적 함수를 따라 각 시간 단계 \\(t\\)에서 노이즈를 예측하기 위해 엄격하게 훈련된다.\n' +
      '\n' +
      '그림 2: ** 울트라아바타의 오버뷰** 첫 번째는 텍스트 프롬프트를 일반 확산 모델(SDXL[55])에 공급하여 얼굴 이미지를 생성한다. 또는, 얼굴 이미지는 또한 우리의 프레임워크에 직접 입력될 수 있다. 둘째, 우리의 DCE 모델은 조명을 제거하여 확산 색상 \\(I_{d}\\)을 추출하기 위해 얼굴 이미지를 취한다. 그런 다음 \\(I_{d}\\)를 메쉬 발생기 및 에지 검출기로 통과시켜 3D 메쉬, 카메라 파라미터 및 에지 이미지를 생성한다. 이러한 예측된 파라미터들을 통해, 초기 텍스처 및 대응하는 가시성 마스크는 텍스처 매핑에 의해 생성될 수 있다. 마지막으로, 우리는 마스킹된 초기 텍스처를 AGT-DM에 입력하여 PBR 텍스처를 생성한다. 생성된 메쉬와 PBR 텍스처를 이용한 재조명 결과가 여기에 나와 있다.\n' +
      '\n' +
      '\\[\\min_{\\mathbb{E}\\ \\min_{\\theta}_{z\\athbb{E}_{z\\ason E(x),t,\\epsilon\\\\mathcal{N}(0,1)}||\\\\\\epuffon- \\epsilon_{\\theta}(z_{t},t},y))||_{2} \\{1}<{z_{t.\n' +
      '\n' +
      '아이티(\\tau(\\cdot)\\가 입력 조건 \\(y\\)에 대한 컨디셔닝 인코더인 경우, 텍스트 임베딩과 같은 \\(z_{t}\\)는 시간 단계 \\(t\\)에서 시끄러운 잠복 샘플을 나타낸다. SD의 노이즈 예측 모델은 그림 4에 묘사된 바와 같이 잔류 블록, 자기 의도 블록 및 교차 의도 블록으로 구성된 U-Net 아키텍처를 기반으로 하며, 데노징 단계 \\(t\\), 이전 \\((l-1)\\(f_{t}^{l}\\)로부터의 피처 \\(\\_{t}1}\\)를 잔차 블록으로 중계하여 레코딩(f_{t}^{l}\\)을 생성한다. 자기 선택 블록 내에서, 잔차 연결을 통한 결합 피처 \\((\\phi_{t}^{l-1}^{l-1}+f_{t}^{l}\\)는 질의 피처 \\(q_{t}^{l}\\), 키 피처 \\(k_{t}^{l}\\) 및 값 피처 \\(v_{t}^{l}^{l}\\)에 투영된다. 상기 리피처 \\(f_{t}^{l}\\)는 생성된 이미지의 콘텐츠에 기여하고, 관심 특징은 이미지 편집[34, 42, 57, 69]에서 일반적으로 사용되는 전체 구조 레이아웃에 기여하는 실질적인 정보를 보유한다.\n' +
      '\n' +
      '확산 모델은 조건부 샘플 생성을 위한 역 과정에 영향을 미치기 위해 지침을 사용하는 중추적인 특징을 가지고 있다. 일반적으로 분류기 지침은 별개의 분류기를 활용하여 점수 기반 모델에 적용될 수 있다. [35]는 조건화된 노이즈 예측 \\(z_{t},t,\\,\\varnothing)를 서로 외삽하기 위해 분류기 없는 지시 기술(z_{t},t,\\varnothing)을 도입하고 분류기 없는 노이즈 예측 \\(z_{t},t,\\varnothing)을 소개한다.\n' +
      '\n' +
      '\\[\\bar{\\epsilon}_{\\epsilon}_{\\theta}(z_{t},t,\\tau)(y))=\\omega\\epsilon_{\\theta}(z_{t,\\t){\\theta}(z_{t)\\epsilon_{\\theta}(z_{t}:z_{t},t,\\varnidalta})(z_{t},t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t,\\t).{t.\n' +
      '\n' +
      '아이티는 널 텍스트의 임베딩이고 \\(\\omega\\)는 안내 척도이다.\n' +
      '\n' +
      '확산 철수를 통한 착색율을 제공합니다.\n' +
      '\n' +
      '우리의 접근법은 사용자에 의해 제공되거나 텍스트 프롬프트로부터 확산 모델[55, 60]에 의해 생성된 얼굴 이미지 \\(I\\)로부터 3D 아바타를 생성한다. 이 두 사례의 치료를 단일화하기 위해, DDIM 반전 [22, 68]은 시간 단계 \\(z_{T}^{l}\\)에서 잠재 노이즈(z_{T}^{l}\\)를 초래하는 이미지에 적용되며, 이후 후진 과정을 통해 원본 이미지 \\(I\\)를 재구성한다. 이것은 확산 모델에서 일련의 특징을 생성한다.\n' +
      '\n' +
      '주어진 이미지 \\(I\\)는 사용자 제공 또는 SD 생성에도 불구하고 일반적으로 제거하기 어려운 그림자, 종방향 하이라이트 및 조명 효과를 포함한다. 재현 가능하고 애니메이션이 가능한 3D 아바타를 만들기 위해서는 일반적으로 도전적인 작업인 이미지에서 제거된 이러한 조명 효과로 확산된 텍스처 맵이 필요하다. 이를 위해 자기 의도적 특징과 이미지 내 조명 효과 간의 관계를 드러내는 핵심 관찰을 하고, 조명을 제거하기 위한 DCE 모형을 소개한다.\n' +
      '\n' +
      '먼저 각 층에서 \\(f_{t}^{l}\\)의 특징에는 [67, 69]에서 논의된 바와 같이 RGB 세부 정보가 포함되어 있다는 점에 주목한다. 자기 의도적 특징 \\(q_{t}^{l}\\) 및 \\(k_{t}^{l}\\)는 유사한 값을 나타내는 유사한 영역을 갖는 이미지 레이아웃을 반영한다. 이 외에도 우리의 발견은 이러한 자기 선택 특징의 변화 \\(q_{t}^{l}\\)와 \\(k_{t}^{l}\\)가 실제로 의미 영역 내에서 음영, 그림자 및 종방향 하이라이트와 같은 조명 효과로 인한 변화를 반영한다는 것이다. 이것은 그림 3에 나와 있으며, 이것은 이해하기 어렵지 않다. 얼굴 이미지 상의 픽셀을 고려할 때, 그것의 질의 특징들은 그 색상이 관련 픽셀들로부터 검색될 수 있도록 동일한 얼굴 부분으로부터의 키 피처들과 정렬되어야 한다. 이미지에 추가된 조명에 의해, 쿼리 특징들은 조명 효과에 의한 변이와 동일하게 달라져야 한다. 이와 같이 조명 패턴에 대응하여 라이트된 색상을 올바르게 검색할 수 있으며, 그림자는 주변 그림자 픽셀의 색상에 기여하는 반면 하이라이트는 인근 강조 픽셀의 색상에 기여할 수 있다.\n' +
      '\n' +
      '조명 효과를 제거하기 위해서는 자기 이해력(쿼리 및 키)의 변화를 제거할 필요가 있다.\n' +
      '\n' +
      '그림 3: ** 피처화** 복합 조명 조건에서 PBR 텍스처가 있는 고품질 데이터를 이미지 \\(I\\)에 렌더링하고 그에 상응하는 근거 진실 확산 색상 이미지를 렌더링한다. 우리는 \\(I\\)를 DCE 모델에 입력하여 결과 \\(I_{d}\\)를 생성했다. 아이티(S\\)는 시맨틱 마스크입니다. 우리는 이러한 이미지에 DDIM 반전 및 샘플링을 적용하고 특징을 추출한다. 특징을 시각화하기 위해 추출된 특징에 PCA를 적용하여 처음 세 가지 주요 구성 요소를 확인한다. 여기에 표시된 주의 특징 및 리피처는 모두 시간 단계 \\(101\\)에서 U-Net의 업샘플링 레이어에서 \\(8\\)-제 레이어에서 나온다. 추출된 \\(I\\)의 질의 및 주요 특징으로부터 우리는 조명을 명확하게 시각화할 수 있다. 결과 \\(I_{d}\\)의 색상 및 추출된 질의 및 주요 특징은 그라운드 진리 영상에서 나온 것과 밀접하게 일치하며, 이는 우리의 방법이 조명을 효과적으로 제거하는 것을 보여준다. 모든 리뷰티는 조명을 너무 많이 제시하지 않습니다. 우리는 또한 이 세 이미지의 색상 분포를 보여주며, 이는 결과 \\(I_{d}\\)가 그림자와 종말점을 제거할 수 있어 지반 진리와 유사한 분포를 만들 수 있음을 보여준다.\n' +
      '\n' +
      '여전히 이러한 특징을 의미 구조와 정렬하고 있다. 그림. 4는 아이디어를 요약합니다. 구체적으로, 먼저 이미지 \\(I\\)에 대한 의미 마스크 \\(S\\)를 생성하기 위해 얼굴 파싱 모델을 선택한다. 시맨틱 마스크는 설계로 의미 구조와 완벽하게 일치하고 의미 영역 내에서 차이가 없기 때문에 위의 두 가지 요건을 충족한다. 그런 다음 시간 단계 \\(T\\)에서 DDIM을 \\(z_{T}^{S}\\)로 반전시키고, 원래 \\(I\\)의 \\(q_{t}^{l}\\) 및 \\(k_{t}^{l}\\)를 추가로 교체하기 위해 \\(z_{t}^{l}\\)를 위해 \\(k_{t}^{l}\\)와 \\(k_{t}^{l}^{l}\\)를 추가로 대체하기 위해 \\(k_{t}^{t}^{l}\\) 및 \\(k_{t}^{l}^{l}\\)를 추가로 대체하기 위해 \\(k_{t}^{l}^{l}\\)을 위해 \\(k_{t}^{l}\\)을 위해 \\(k_{t}^{t}^{l}\\)을 위해 \\)을 위해 \\(I\\)을 위해 \\(k_{t}^{t}^{l}\\ 시맨틱 마스크는 의미 영역 내에서 균일한 값을 가지므로 결과적인 자기 의도 특징은 조명 효과를 포함하지 않는 것으로 가정된다(그림 참조). 3), 얼굴 세부 사항은 여전히 원래 이미지(I\\)의\\(f_{t}^{l}\\)에 보관된다. 따라서 질의 및 주요 특징(q_{t}^{l}\\)과 \\(k_{t}^{l}\\)를 자기 의도 블록의 의미 마스크로 대체함으로써, 우리는 \\(I\\)로부터의 조명 효과를 제거하고 \\(z_{T}^{I}\\) 생성에 사용되는 잠재 노이즈(z_{T}^{I}\\)로부터 시작하는 후방 과정을 통해 확산색을 유지할 수 있다.\n' +
      '\n' +
      '이 접근법은 얼굴 이미지 이외의 더 일반적인 이미지에서 조명 효과를 제거하기 위해 적용될 수 있으며 부록에서 더 많은 결과를 보여준다.\n' +
      '\n' +
      '3D 아바타 마.\n' +
      '\n' +
      '우리는 FLAME [46] 모델을 3D 아바타의 기하학 표현으로 사용한다. FLAME는 3D 헤드 템플릿 모델로 \\(33,000\\) 이상의 스캔에서 훈련된다. 정체성 형상에 대한 매개변수 \\(\\beta\\), 표정 \\(\\psi\\) 및 포즈 파라미터 \\(\\theta\\)가 특징이다. 이러한 매개변수와 함께. FLAME는 머리, 목, 안구를 포함한 5023개의 꼭짓점과 9976개의 얼굴로 구성된 메쉬 \\(M(\\beta,\\psi,\\theta)\\)를 생성한다. 중성 얼굴 모양을 정확하게 추정하고 발현, 조명 및 카메라에 강건한 확산 이미지(I_{d}\\)에서 FLAME 모델의 형상 코드 \\(\\beta^{*}\\)를 추정하기 위해 MICA[80]를 채택한다. 우리는 또한 발현 코드 \\(\\psi^{*}\\)를 얻기 위해 EMOCA[20]를 적용하여 파라미터 \\(\\theta^{*}\\) 및 카메라 파라미터 \\(c^{*}\\)를 생성하며, 이는 후속 3D 애니메이션/주행 애플리케이션에 사용된다. FLAME 질감 기반을 결합한 EMOCA에서 생성된 색상 텍스처를 사용하지 않습니다. 진정한 얼굴 색상을 정확하게 제시할 수 없고 피부 디테일이 부족하고 AGT-DM으로 아래에 도출할 수 있는 확산 색상, 정상 맵, 사구체 및 거칠기 질감과 같은 PBR 세부 정보가 포함되어 있지 않다.\n' +
      '\n' +
      '정확함.\n' +
      '\n' +
      '현재 추정된 메쉬 \\(M(\\beta^{*},\\psi^{*},\\theta^{*})\\), 카메라 파라미터 \\(c^{*}\\) 및 조명 없는 얼굴 이미지 \\(I_{d}\\)를 감안할 때 후자를 메쉬에 텍스쳐 매핑한 다음 획득한 메쉬 질감을 초기 텍스처 UV 맵 \\(I_{m}\\)에 투영할 수 있다. I_{d}\\(I_{d}\\)는 얼굴의 단일 뷰에 불과하기 때문에, 결과 \\(I_{m}\\)는 확산 색상의 불완전한 UV 텍스처 맵이며, 우리는 \\(V\\)를 사용하여 UV 좌표에서 가시적인 마스크를 나타낸다. UV 텍스처 맵은 또한 EMOCA에 의한 추정된 얼굴 포즈, 발현 및 카메라 포즈의 오류로 인해 메쉬와 완벽하게 정렬되지 않을 수 있다.\n' +
      '\n' +
      '위의 과제를 해결하기 위해 부분적으로 관찰된 질감 UV 맵 \\(I_{m}\\)을 전체 UV 좌표로 돌릴 수 있는 AGT-DM을 훈련시키고, 2) 질감 맵과 UV 좌표 간의 정렬을 향상시킨다. 더욱이, 모델은 주어진 \\(I_{m}\\) 및 \\(V\\)로부터 정상, 사각형 및 거칠기 맵을 포함하여 확산된 색상 질감을 넘어 더 많은 PBR 세부 정보를 출력할 수 있다. 우리는 광계 및 가장자리 세부 정보를 기반으로 두 개의 안내 신호로 출력 PBR 결과를 향상시켜 AGT-DM에서 미묘한 주름과 기공과 같은 정체성과 더 많은 얼굴 특징을 회수할 수 있도록 할 것이다.\n' +
      '\n' +
      '이를 위해 3D 스캔의 전체 고품질 메세지에 확산 색상, 정상 맵, 사구체 및 거칠기 질감을 포함한 여러 유형의 PBR 텍스처 맵(4K 및 8K)과 함께 3D 얼굴 스캔으로 구성된 온라인 3DScan 데이터세트[1]을 사용한다. 우리는 이 데이터셋(부록의 세부 사항)을 트레이닝 데이터셋으로 처리하고 이를 사용하여 인포팅 SD 모델을 훈련시키며, 원본 SD의 U-넷은 데이터세트로부터의 지상 신뢰 확산 UV 맵 위에 핀셋된다. PBR 텍스처를 생성하기 위해, SD 인코더는 동결되고, 확산 텍스쳐에 대한 SD 디코더는 PBR 디코더 \\(D_{d}\\)가 직접 계승되는 것을 제외하고는, PBR 텍스쳐의 종류별 데이터셋 위에 복사되고 핀셋된다.\n' +
      '\n' +
      '그림 4: **DCE 모델** 입력 이미지 \\(I\\)를 얼굴 파싱 모델에 공급하여 의미형 마스크(S\\)를 생성한다. 우리는 초기 노이즈 \\(z_{T}^{I}\\) 및 \\(z_{T}^{I}\\)를 얻기 위해 DDIM 반전(S\\)을 적용한 다음, 점진적으로 \\(z_{T}^{I}\\) 및 \\(z_{T}^{I}\\)를 변성시켜 레플리콘 및 주의 기능을 별도로 추출하고 보존한다(z_{T}^{S}\\). 마지막으로, 우리는 점진적으로 \\(z_{T}^{I}\\)를 한 번 더 변성시켜 특정 계층( Sec에서 논의된 바와 같이)의 저장에서 재판매 및 주의 기능을 복사한다. 표집 중 \\(\\hat{z}_{0}^{l}\\), 최종 결과 \\(I_{d}\\)는 \\(\\hat{z}_{0}^{l}\\)에서 생성될 것이다.\n' +
      '\n' +
      '원래 SD 디코더에서. 그런 다음 핀셋화된 SD 모델을 사용하여 다른 PBR 텍스처와 함께 마스킹된 확산 색상 맵 \\(V\\odot I_{m}\\)을 모델링할 수 있다. 트레이닝 데이터셋은 이상적으로 메세지와 텍스쳐 디테일을 정렬했기 때문에 생성된 인포팅 확산 모델은 출력 PBR 텍스처와 메쉬 간의 정렬과 얼굴 디테일을 개선할 수 있다. 잠복에 따른 시끄러운 마스킹 질감 잠재 \\(Z_{N}\\)을 분해하면 탈색은 \\(I_{m}\\)의 오문을 부패시키고 확산 샘플링을 통해 인포팅 확산 모델을 보정할 수 있기 때문에 질감과 중간 사이에 더 정확한 정렬이 생성될 수 있다.\n' +
      '\n' +
      '더 많은 얼굴 디테일로 PBR 텍스처를 더욱 향상시키기 위해 두 가지 에너지 기능을 사용하여 인핑 확산 모델의 샘플링 과정을 안내합니다. 첫 번째는 다음과 같은 에너지 기능을 가진 광측정 유도 \\(G_{P}\\)로서 다음과 같은 에너지 기능을 가지고 있다.\n' +
      '\n' +
      'R(M)}(\\beta^{d}),\\peta^{d}(\\beta^{d}), V.{d}(\\beta^{d}), FTC_{d},\\d{d})\n' +
      '\n' +
      'E\\(V_{d}\\)는 그림과 같이 렌더링된 얼굴의 가시적인 부분에 걸친 마스크이다. (R(\\cdot)\\)는 확산 시간 단계 \\(t\\)에서 메시 \\(M\\), 확산 색상 텍스처 맵 \\(D_{d}(z_{t})\\의 현재 추정치를 기반으로 아바타 얼굴의 차등 렌더이며, \\(L_{lpips}(.)\\)는 지각 손실 함수(LPIPS[77])이다. 이 광측정 에너지의 최소화는 렌더링된 이미지와 원본 이미지를 정렬할 것이다.\n' +
      '\n' +
      '두 번째는 다음과 같은 에지 에너지 기능을 가진 엣지 안내입니다.\n' +
      '\n' +
      'MS(G_{d})는\\(C(M(\\beta^{*},\\peta^{*}), D(z_{t}),^{2}(I_{t})\n' +
      '\n' +
      '아이티(C(\\cdot)\\이 있는 곳은 캐니 에지 검출 기능[15]이다. 가장자리는 그림과 같이 고주파 세부 정보를 포함한다. 2, 이 지침은 이미지 \\(I_{d}\\)에 주름, 프리클, 기공, 몰, 흉터와 같은 얼굴 디테일을 유지하는 데 도움이 되어 생성된 아바타가 높은 충실도로 더 현실감 있게 보일 것이다.\n' +
      '\n' +
      '아래의 무기능 안내 샘플링에 에너지 기능의 구배를 통해 두 편을 통합한다.\n' +
      '\n' +
      'SS_{t}}(z_{t},t,\\ga_{t})\\omega_{t}}(z_{t},t,\\ga_{t})(1-\\omega)\\epsilon_{\\theta}(z_{t)\\epsilon_{\\theta}(z_\\t+(1-\\omega)\\tapapsilon_{t})\\t}}(z_{t)\\t}(1-\\t+(1-\\t)\\t +\\t+(1-\\t)\\telt:\\t+(1-\\t)\\tor.{t)\\tutepsilon_\\t+(1-\\t)\\tylt)\\tμM(y)){t)\\tapt)\\trigsilon_\\t+(1-\\t)\\tapt)\\tapapsilon_\\t 및\\t)\\t surgery(I)\\tapt)\\tapapsilon_\\t)\\tapsilon_\\\n' +
      '\n' +
      '우리는 그림 6의 효과를 보여준다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '셋과 바젤린스.\n' +
      '\n' +
      '실험 집합은 SDXL[55]을 텍스트 대 이미지 생성 모델로 사용했다. 얼굴 파싱 마스크인 \\(S\\)를 생성하기 위해 전처리된 BiSeNet[4, 74]을 사용했다. 우리의 DCE 모듈에서 표준 SD-2.1 염기 모델을 사용하고 20개의 시간 단계로 DDIM 샘플링을 적용한다. 우리는\\(4^{th}\\)에서 \\(I\\)에서 추출한 U-net에서 \\(11^{th}\\) 업샘플링 계층을 보존하고 \\(I\\)의 DDIM 샘플링에 주입하고 \\(4^{th}\\)에서 \\(9^{th}\\)에서 추출된 \\(9^{th}\\) 업샘플링 레이어에서 \\(9^{th}\\) 업샘플링 레이어에서 \\(I\\)의 질의 및 주요 특징을 \\(I\\)에서 \\(9^{th}\\) 업샘플링 레이어에서 \\(I\\)에서 \\(9^{th}\\) 업샘플링 레이어에서 \\(9^{th}\\) 업샘플링 레이어에서 \\(9^{th}\\) 업샘플링 레이어에서 \\(9^{th}\\) 업샘플링 레이어에서 \\(9^{th}\\) 업샘플링 레이어에서 \\(9^{th}\\) 업샘플링 레이어에서 추출된 \\(9^{th}\\) 업샘플링 레이어에서 \\(9 우리는 마지막 몇 층으로 주입하는 것을 발견하면 때때로 정체를 약간 바꾸기 때문에 모든 층에서 질의 및 주요 특징을 주입하지 않기로 선택한다.\n' +
      '\n' +
      'AGT-DM의 경우 주석이 달린 3DScan 스토어 데이터 세트에 SD-2.1 염기 모델에서 U-Net을 통합하여 확산 색상 텍스처 맵을 생성한다. 우리는 FLAME UV 맵을 생성하기 위해 핀셋링 동안 텍스트 프롬프트에 "UV 맵의 지도"를 부착한다. 우리는 정상, 사각형 및 거칠기 지도를 출력하기 위해 세 개의 디코더를 훈련시킵니다. 더 많은 훈련 세부 사항은 부록에 나와 있습니다.\n' +
      '\n' +
      'AGT-DM에서 우리는\\(T=200\\), \\(N=90\\), \\(\\omega=7.5\\), \\(\\omega_{p}=0.1\\), \\(\\omega_{photo}=0.4\\), \\(\\omega_{lpips}=0.6\\) 및 \\(\\omega_{lpips}=0.05\\)를 사용한다. 초기 \\(T-N\\) 데노징 단계에서 우리의 접근법은 가시성 마스크를 사용하여 [11]에서 설명한 방법과 유사한 잠재 공간 인포팅 기술을 채택한다. 최종 \\(N\\) 단계 동안 제안된 광계 및 엣지 어시스트를 적용하여 오문을 수정하고 관찰된 얼굴 영역과 관측되지 않은 영역 사이의 일관된 통합을 보장한다. 추론 후, 우리는 결과 잠재 코드를 정상, 사각형 및 거칠기 디코더로 통과시켜 해당 PBR 텍스처 맵을 얻는다. 그런 다음 2K 해상도 질감을 얻기 위해 전처리된 Stable Diffusion 슈퍼-resoltuion 네트워크[60]로 텍스처를 통과합니다.\n' +
      '\n' +
      '바젤린은 텍스트 대 아바타 생성(Latent3d [14], CLIP매트릭스[38], Text2Mesh[50], CLIPFace[9], 드림Face[76] 및 이미지 대 아바타 생성(FlameTex[24], Pano헤드[8])에 대한 다양한 최첨단 접근법에 대한 비교를 보여준다. 비교에 대한 자세한 내용은 부록에 포함되어 있습니다.\n' +
      '\n' +
      '결과 및 토론.\n' +
      '\n' +
      '우리는 그림 1에서 사실적인 아바타를 생성한 텍스트/이미지를 보여준다. 1과 5. 노트, AGT-DM에 대한 훈련 데이터에 해당 이미지가 없습니다. 생성된 결과는 주어진 텍스트 프롬프트/이미지로 충실도를 유지하는 풍부한 텍스처를 보여준다. 또한, 우리의 DCE 모델과 확산된 색상 질감과 PBR 세부 정보를 추출하는 AGT-DM의 능력으로 인해 모든 조명 조건에서 재발하는 아바타를 올바르게 렌더링할 수 있습니다. AGT-DM은 관찰된 영역과 관찰되지 않은 영역에 걸쳐 일관성을 구현하기 때문에 렌더링된 아바타는 가시적인 유물 없이 서로 다른 각도에서 동등하게 사실적으로 보인다.\n' +
      '\n' +
      '비교를 위해 성능 분석을 위해 무작위로 40개의 텍스트 프롬프트를 선택하여 다양한 연령 그룹, 민족 및 성별에 걸쳐 포괄적인 표현을 보장하고 다양한 유명인을 포함한다. 드림페이스와 울트리아바타의 경우, 우리는 생성된 메서를 5가지 다른 조명 조건에서 50가지 각도에서 렌더링한다. 파노 헤드의 경우 동일한 40개의 텍스트 프롬프트를 사용하여 SDXL에 의해 생성된 200개의 이미지를 제공합니다. 초음파 아바타는 단일 Nvidia A6000 GPU에서 2분 이내에 텍스트 프롬프트로부터 고품질 안면 자산을 생성할 수 있다.\n' +
      '\n' +
      '표준 생성 모델 메트릭 FID 및 KID를 사용하여 렌더링된 이미지의 지각 품질을 평가한다. CLIPFace와 유사하게 마스킹된 FFHQ 이미지[39](배경, 눈 및 입 내부 없음)와 관련하여 이 두 메트릭을 모두 근거 진리로 평가한다. 텍스트 대 아바타 생성을 위해 텍스트 프롬프트와 렌더링된 이미지 간의 유사성을 측정하기 위해 CLIP 점수를 추가로 계산했다. 우리는 두 가지 다른 CLIP 변이체, 즉 \'ViT-B/16\' 및 \'ViT-L/14\'의 평균 점수를 보고한다.\n' +
      '\n' +
      '표 1의 텍스트 대 아바타 생성 접근법 중 드림페이스는 텍스트와 생성된 아바타 간의 유사성을 유지하는 데 매우 잘 작용한다. 그러나 드림페이스가 생성한 아바타들은 현실성과 다양성이 부족하다. 우리의 제안된 울트리아바타는 지각 품질 측면에서 드림페이스보다 훨씬 더 나은 성능을 발휘한다(더 많은 결과는 부록에 나와 있다). 또한 그림 1에 나와 있다. 7, 우리는 드림페이스가 도전적인 프롬프트(예: 큰 코, 흔치 않은 유명인)로부터 아바타를 생성하지 못한다는 것을 보여준다. 꿈페이스의 결과는 여러 번의 실행에서 가장 좋은 출력을 나타낸다는 점에 유의하는 것이 중요하다. 우리의 울트리아바타는 또한 표 1에 보고된 바와 같이 지각 품질 및 CLIP 점수 측면에서 다른 텍스트 대 아바타 접근법을 능가하며, 파노 헤드는 이미지 대 아바타 세대의 과제에서 앞 뷰를 렌더링하는 데 인상적인 성능을 얻을 수 있다. 그러나 파노헤드 효과는 간혹 전처리 단계의 정확도에 크게 좌우되는 전처리 단계의 정확도에 크게 의존한다.\n' +
      '\n' +
      '그림 5: ** 랜덤 아이덴티티와 연예인**를 생성하는 결과는 텍스트 프롬프트를 일반 SDXL에 입력하여 2D 얼굴 이미지를 생성한다. 우리의 결과는 중재와 잘 정렬되어 높은 충실도를 나타내고 정체성과 얼굴 디테일을 유지하는 재구성된 고품질 PBR 텍스처를 보여준다. 우리 세대의 품질을 설명하기 위해 다양한 환경 맵에서 각 3D 아바타를 재발시킵니다.\n' +
      '\n' +
      'AGT-DM(I_{d}\\)에 의한 이미지 \\(I_{d}\\)의 AGT-DM(I_{d}\\)의 결과 생성 시나리오의 ** 분석에서는 \\(G_{P}\\) 및 \\(G_{E}\\)가 없는 것, \\(G_{P}\\)만이 있는 것, 그리고 \\(G_{P}\\) 및 \\(G_{E}\\)와 \\(G_{E}\\)이 없는 것, 그리고 \\(G_{E}\\)와 \\(G_{E}\\(G_{E}\\)와 \\(G_{E}\\(G_{E}\\)와 \\(G_{E}\\(G_{E}\\)와 \\(G_{E}\\)와 \\(G_{E}\\)와 \\(G_{E}\\)와 \\(G_{E}\\)와 \\(G_{E}\\)와 \\(G_{E}\\)이 있는 것 이 두 명의 어메니스를 통해 정체성과 얼굴 디테일이 효과적으로 유지되고 있음을 분명히 보여준다.\n' +
      '\n' +
      '정확한 추정을 제공할 수 있습니다. 또한, NeRF 기반 파노헤드 접근법은 재발에 한계가 있다. 다뷰 렌더링 능력을 고려할 때, 울트리아바타는 표 1과 같이 이미지 대 아바타 과제에 파노 헤드를 능가한다.\n' +
      '\n' +
      '또한 GPT-4V(비전) [5, 6]를 사용하여 텍스트 대 아바타 성능 평가를 자동화합니다. GPT-4V는 비전-언어 과제[72, 78]에서 인간과 유사한 평가 능력을 인정받는다. 우리는 5점 리커트 척도로 모델을 평가한다. 평가 기준은 사진-현실주의, 인공 최소화, 피부 질 질, 텍스트 프롬프트 정렬, 이미지의 전반적인 초점 및 선명도를 포함한다. 그림과 같이. 8, 울트리아바타는 인간 얼굴과 유사한 기능을 생성하는 우수한 능력을 보여준다. 드림페이스와 파노헤드보다 인공물을 크게 줄이고 선명도와 초점을 높일 뿐만 아니라 텍스트 촉진 정렬에서 높은 수준의 광현실주의와 충실도를 유지한다.\n' +
      '\n' +
      '### Ablation Studies.\n' +
      '\n' +
      '그림에서. 6, 우리는 다양한 주장이 AGT-DM 성능에 미치는 영향을 보여준다. 광계 안내는 생성된 텍스쳐와 소스 이미지 사이의 유사도를 구현한다. 또한, 에지 안내는 생성된 색상 질감의 세부 정보를 향상시킨다.\n' +
      '\n' +
      '도메인 세대 내 초음파 아바타는 애니메이션 캐릭터, 만화 캐릭터 및 기타 비인간 캐릭터의 이미지/프로그래밍으로부터 아바타를 생성할 수 있다. 우리는 그림 9에 몇 가지 결과를 보여주었다.\n' +
      '\n' +
      '생성된 아바타가 메쉬 기반 모델이기 때문에 표현과 포즈를 변경하여 생성된 아바타를 분리시킬 수 있습니다. 우리는 또한 AGT-DM에서 텍스트 프롬프트를 사용하여 일부 텍스처 편집을 수행할 수 있다. 우리는 애니메이션과 편집 결과를 부록에 포함했다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '텍스트 프롬프트 또는 단일 이미지로부터 3D 아바타 생성에 대한 새로운 접근법을 도입했다. 우리의 방법의 핵심은 소스 이미지에서 원치 않는 조명 효과를 제거하기 위해 설계된 DCE 모델이며, 아바타의 PBR 세부 사항을 유지하기 위해 광계 및 에지 신호에 의해 유도되는 텍스처 생성 모델이다. 다른 SOTA 접근법과 비교하여 이 방법을 통해 현실적인, 더 높은 품질, 우수한 충실도 및 보다 광범위한 다양성을 높이는 3D 아바타를 생성할 수 있음을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|c|c|c|} \\hline Method & FID \\(\\downarrow\\) & KID \\(\\downarrow\\) & CLIP Score \\(\\uparrow\\) \\\\ \\hline DreamFace [76] & 76.70 & 0.061 & 0.291 \\(\\pm\\) 0.020 \\\\ ClipFace\\({}^{*}\\)[9] & 80.34 & 0.032 & 0.251 \\(\\pm\\) 0.059 \\\\ Latent3d\\({}^{*}\\)[14] & 205.27 & 0.260 & 0.227 \\(\\pm\\) 0.041 \\\\ ClipMatrix\\({}^{*}\\)[38] & 198.34 & 0.180 & 0.243 \\(\\pm\\) 0.049 \\\\ Text2Mesh\\({}^{*}\\)[50] & 219.59 & 0.185 & 0.264 \\(\\pm\\) 0.044 \\\\ \\hline FlameTex\\({}^{*}\\)[24] & 88.95 & 0.053 & - \\\\ PanoHead [8] & 48.64 & 0.039 & - \\\\ \\hline UltraAvatar (Ours) & **45.50** & **0.029** & **0.301 \\(\\pm\\) 0.023** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: FID, KID 및 CLIP 점수 메트릭에 기반한 방법의 비교, \\({}^{*}\\) 결과는 CLIPFace의 결과이다.\n' +
      '\n' +
      'GPT-4V에 의한 [그림 8: ** 정성적 평가]** 우리 프레임워크는 전반적으로 더 나은 성능을 가지고 있다.\n' +
      '\n' +
      '그림 7: ** 드림페이스와의 비교** 우리 결과는 특히 극단적인 프롬프트에 대해 드림페이스보다 텍스트 프롬프트와 더 나은 정렬을 달성한다.\n' +
      '\n' +
      '도메인 외 아바타 세대의 그림 9: ** 결과는** 우리 프레임워크는 유통되지 않은 애니메이션 캐릭터 또는 비인간 아바타를 생성할 수 있는 능력을 가지고 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] 3Dscan store. Note: [https://www.3dscanstore.com/](https://www.3dscanstore.com/).\n' +
      '* [2] Wrap4d. Note: [https://www.russian3dscanner.com/wrap4d/](https://www.russian3dscanner.com/wrap4d/) Cited by: SS1.\n' +
      '* [3] Hyperhuman. Note: [https://hyperhuman.deemos.com/](https://hyperhuman.deemos.com/) Cited by: SS1.\n' +
      '* [4] Using modified BiSeNet for face parsing in PyTorch. Note: [https://github.com/zlrunning/face-parsing.PyTorch](https://github.com/zlrunning/face-parsing.PyTorch) Cited by: SS1.\n' +
      '* [5] ChatGPT can now see, hear, and speak. Note: [https://openai.com/blog/chatgpt-can-now-see-heear-and-speak](https://openai.com/blog/chatgpt-can-now-see-heear-and-speak) Cited by: SS1.\n' +
      '* [6] GPT-4V(sion) system card. Note: [https://cdn.openai.com/papers/GPTV_System_Card.pdf](https://cdn.openai.com/papers/GPTV_System_Card.pdf) Cited by: SS1.\n' +
      '* [7] Oleg Alexander, Mike Rogers, William Lambeth, Jen-Yuan Chiang, Wan-Chun Ma, Chuan-Chang Wang, and Paul Debevec. The Digital Emily Project: Achieving a Photorealistic Digital Actor. IEEE Computer Graphics and Applications30 (4), pp. 20-31. Cited by: SS1.\n' +
      '* [8] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y Ogras, and Linjie Luo. PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360\\({}^{\\circ}\\). In CVPR, pp. 20950-20959. Cited by: SS1.\n' +
      '* [9] Shivangi Aneja, Justus Thies, Angela Dai, and Matthias Niessner. ClipFace: Text-guided Editing of Textured 3D Morphable Models. In ACM SIGGRAPH 2023 Conference Proceedings, pp. 1-11. Cited by: SS1.\n' +
      '* [10] ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli Shechtman, and Zhixin Shu. RigNeRF: Fully Controllable Neural 3D Portraits. In CVPR, pp. 20364-20373. Cited by: SS1.\n' +
      '* [11] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended Latent Diffusion. ACM TOG42 (4), pp. 1-11. Cited by: SS1.\n' +
      '* [12] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. SpaText: Spatio-Textual Representation for Controllable Image Generation. In CVPR, pp. 18370-18380. Cited by: SS1.\n' +
      '* [13] George Borshukov and John P Lewis. Realistic Human Face Rendering for "The Matrix Reloaded". In ACM Siggraph 2005 Courses, pp. 13-es. Cited by: SS1.\n' +
      '* [14] Zehranaz Canfes, M Furkan Atasoy, Alara Dirik, and Pinar Yanardag. Text and Image Guided 3D Avatar Generation and Manipulation. In CVPR, pp. 4421-4431. Cited by: SS1.\n' +
      '* [15] John Canny. A Computational Approach to Edge Detection. IEEE TPAMI (6), pp. 679-698. Cited by: SS1.\n' +
      '* [16] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz, Michael Zollhoefer, Shun-Suke Saito, Stephen Lombardi, Shih-En Wei, Danielle Belko, Shoou-I Yu, et al. Authentic volumetric avatars from a phone scan. ACM TOG41 (4), pp. 1-19. Cited by: SS1.\n' +
      '* [17] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient Geometry-aware 3D Generative Adversarial Networks. In CVPR, pp. 16123-16133. Cited by: SS1.\n' +
      '* [18] Hyungjin Chung, Jeongsol Kim, Michael Thompson McCann, Marc Louis Klasky, and Jong Chul Ye. Diffusion Posterior Sampling for General Noisy Inverse Problems. In ICLR, Cited by: SS1.\n' +
      '* [19] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving Diffusion Models for Inverse Problems using Manifold Constraints. NeurIPS35, pp. 25683-25696. Cited by: SS1.\n' +
      '* [20] Radek Danceek, Michael J Black, and Timo Bolkart. EMOCA: Emotion Driven Monocular Face Capture and Animation. In CVPR, pp. 20311-20322. Cited by: SS1.\n' +
      '* [21] Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong. GRAN: Generative Radiance Manifolds for 3D-Aware Image Generation. In CVPR, pp. 10673-10683. Cited by: SS1.\n' +
      '* [22] Prafulla Dhariwal and Alexander Nichol. Diffusion Models Beat GANs on Image Synthesis. NeurIPS34, pp. 8780-8794. Cited by: SS1.\n' +
      '* [23] Zheng Ding, Xuaner Zhang, Zhihao Xia, Lars Jebe, Zhuowen Tu, and Xiuming Zhang. DiffusionRig: Learning Personalized Priors for Facial Appearance Editing. In CVPR, pp. 12736-12746. Cited by: SS1.\n' +
      '* [24] Haven Feng. Photometric FLAME fitting. Note: [https://github.com/HavenFeng/photometric_optimization](https://github.com/HavenFeng/photometric_optimization) Cited by: SS1.\n' +
      '* [25] Yao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart. Learning an Animatable Detailed 3D Face Model from InThe-Wild Images. ACM TOG40 (4), pp. 1-13. Cited by: SS1.\n' +
      '* [26] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. In ICLR, Cited by: SS1.\n' +
      '* [27] Sicheng Gao, Xuhui Liu, Bohan Zeng, Sheng Xu, Yanjing Li, Xiaoyan Luo, Jianzhuang Liu, Xiantong Zhen, and Baochang Zhang. Implicit Diffusion Models for Continuous Super-Resolution. In CVPR, pp. 10021-10030. Cited by: SS1.\n' +
      '* [28] Xuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong, Yudong Guo, and Juyong Zhang. Reconstructing Personalized Semantic Facial NeRF Models from Monocular Video. ACM TOG41 (6), pp. 1-12. Cited by: SS1.\n' +
      '* [29] Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos Zafeiriou. GANFIT: Generative Adversarial Network Fitting for High Fidelity 3D Face Reconstruction. In CVPR, pp. 1155-1164. Cited by: SS1.\n' +
      '* [30] Baris Gecer, Alexandros Lattas, Stylianos Ploumpis, Jiankang Deng, Athanasios Papaioannou, Stylianos Moschoglou, and Stefanos Zafeiriou. Synthesizing Coupled 3D Face Modalities by Trunk-Branch Generative Adversarial Networks. In ECCV, pp. 415-433. Cited by: SS1.\n' +
      '* An 오픈 프레임워크입니다. 2018년 제13차 IEEE 국제 자동페이스&제스처 인식 컨퍼런스(FG 2018)에서, pp. 75-82: SS1이 발표했다.\n' +
      '* [32] Philip-William Grassal, Malte Pinzler, Titus Leistner, Carsten Rother, Matthias Niessner, and Justus Thies. Neural Head Avatars from Monocular RGB Videos. In CVPR, pp. 18653-18664. Cited by: SS1.\n' +
      '*[33]*[33] Kaiwen Guo, psychiatric Lincoln, 필립 데이비슨, 제이 부흐, Xueming 유, 마트 휘웬, 거프 하비, 세르히오 오르츠-에콜라노, 노짓 판다노, 제이슨 두르가리아 등 사실적인 재조명으로 인간의 재조명: 재조명 공연 캡처. ACM TOG_, 38(6):1-19, 2019.\n' +
      '* [34] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-Prompt Image Editing with Cross-Attention Control. In _ICLR_, 2022.\n' +
      '* [35] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion Guidance. In _NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications_, 2021.\n' +
      '* [36] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars. _ACM TOG_, 41(4):1-19, 2022.\n' +
      '* [37] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juyong Zhang. HeadNeRF: A Real-time NeRF-based Parametric Head Model. In _CVPR_, pages 20374-20384, 2022.\n' +
      '* [38] Nikolay Jetchev. ClipMatrix: Text-controlled Creation of 3D Textured Meshes. _arXiv preprint arXiv:2109.12922_, 2021.\n' +
      '* [39] Tero Karras, Samuli Laine, and Timo Aila. A Style-Based Generator Architecture for Generative Adversarial Networks. In _CVPR_, pages 4401-4410, 2019.\n' +
      '* [40] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and Improving the Image Quality of StyleGAN. In _CVPR_, pages 8110-8119, 2020.\n' +
      '* [41] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In _CVPR_, pages 1931-1941, 2023.\n' +
      '* [42] Ghiyun Kwon and Jong Chul Ye. Diffusion-based Image Translation using Disentangled Style and Content Representation. In _ICLR_, 2022.\n' +
      '* [43] Alexandros Lattas, Stylianos Moschoglou, Baris Gecer, Stylianos Ploumpis, Vasileios Triantafyllou, Abhijeet Ghosh, and Stefanos Zafeiriou. AvatarMe: Realistically Renderable 3D Facial Reconstruction "in-the-wild". In _CVPR_, pages 760-769, 2020.\n' +
      '* [44] Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Baris Gecer, Jiankang Deng, and Stefanos Zafeiriou. FitMe: Deep Photorealistic 3D Morphable Model Avatars. In _CVPR_, pages 8629-8640, 2023.\n' +
      '* [45] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. 2023.\n' +
      '* [46] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier Romero. Learning a model of facial shape and expression from 4D scans. _ACM TOG_, 36(6), 2017.\n' +
      '* [47] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-Resolution Text-to-3D Content Creation. In _CVPR_, pages 300-309, 2023.\n' +
      '* [48] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser Sheikh. Deep Appearance Models for Face Rendering. _ACM TOG_, 37(4):1-13, 2018.\n' +
      '* [49] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures. In _CVPR_, pages 12663-12673, 2023.\n' +
      '* [50] Oscar Michel, Roi Bar-On, Richard Liu, Sagei Benaim, and Rana Hanocka. Text2Mesh: Text-Driven Neural Stylization for Meshes. In _CVPR_, pages 13492-13502, 2022.\n' +
      '* [51] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. In _Int. Conf. Machine Learn._, pages 16784-16804. PMLR, 2022.\n' +
      '* [52] Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman. StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation. In _CVPR_, pages 13503-13513, 2022.\n' +
      '* [53] Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos Moschoglou, and Stefanos Zafeiriou. Relightfly: Reslighable 3d faces from a single image via diffusion models. _arXiv preprint arXiv:2305.06077_, 2023.\n' +
      '* [54] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter. A 3d face model for pose and illumination invariant face recognition. In _2009 sixth IEEE international conference on advanced video and signal based surveillance_, pages 296-301. Ieee, 2009.\n' +
      '* [55] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [56] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D Diffusion. In _ICLR_, 2022.\n' +
      '* [57] Konpat Preechakul, Nattanat Chathee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion Autoencoders: Toward a Meaningful and Decodable Representation. In _CVPR_, pages 10619-10629, 2022.\n' +
      '* [58] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-Shot Text-to-Image Generation. In _Int. Conf. Machine Learn._, pages 8821-8831. PMLR, 2021.\n' +
      '* [59] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* [60] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. In _CVPR_, pages 10684-10695, 2022.\n' +
      '* [61] Will Rowan, Patrik Huber, Nick Pears, and Andrew Keeling. Text2Face: A Multi-Modal 3D Face Model. _arXiv preprint arXiv:2303.02688_, 2023.\n' +
      '*[62]*[62] 체트완 사하라리아, 윌리엄 찬, 후이웬 창, 크리스 이, 조나단 호, 팀 살림산, 데이비드 플레인, 모하마드 노루지 등이 있다. 팔렛: 이미지 대 이미지 디확산 모델. _ACM SIGGRAPH 2022 콘퍼런스 프로그램_, 2022년 1-10페이지입니다.\n' +
      '* [63] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image Super-Resolution via Iterative Refinement. _IEEE TPAMI_, 45(4):4713-4726, 2022.\n' +
      '* [64] Soubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael J Black. Learning to Regress 3D Face Shape and Expression from an Image without 3D Supervision. In _CVPR_, pages 7763-7772, 2019.\n' +
      '* [65] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-Motion Revisited. In _CVPR_, pages 4104-4113, 2016.\n' +
      '* [66] Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, and Richard Szeliski. A Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms. In _CVPR_, pages 519-528. IEEE, 2006.\n' +
      '* [67] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. FreeU: Free Lunch in Diffusion U-Net. _arXiv preprint arXiv:2309.11497_, 2023.\n' +
      '* [68] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _ICLR_, 2021.\n' +
      '* [69] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation. In _CVPR_, pages 1921-1930, 2023.\n' +
      '* [70] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion. In _CVPR_, pages 4563-4573, 2023.\n' +
      '* [71] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G Dimakis, and Peyman Milanfar. Delburring via Stochastic Refinement. In _CVPR_, pages 16293-16303, 2022.\n' +
      '* [72] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision), 2023.\n' +
      '* [73] Yu Yin, Kamran Ghasedi, HsiangTao Wu, Jiaolong Yang, Xin Tong, and Yun Fu. NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real Image Animation. In _CVPR_, pages 8539-8548, 2023.\n' +
      '* [74] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation. In _ECCV_, pages 325-341, 2018.\n' +
      '* [75] Hao Zhang, Yao Feng, Peter Kults, Yandong Wen, Justus Thies, and Michael J Black. Text-Guided Generation and Editing of Compositional 3D Avatars. _arXiv preprint arXiv:2309.07125_, 2023.\n' +
      '* [76] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and Jingyi Yu. DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance. _ACM TOG_, 42(4), 2023.\n' +
      '* [77] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 586-595, 2018.\n' +
      '* [78] Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, and Linda Ruth Petzold. GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks, 2023.\n' +
      '* [79] Yufeng Zheng, Victoria Fernandez Abrevaya, Marcel C Buhler, Xu Chen, Michael J Black, and Otmar Hilliges. I M Avatar: Implicit Morphable Head Avatars from Videos. In _CVPR_, pages 13545-13555, 2022.\n' +
      '* [80] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Towards Metrical Reconstruction of Human Faces. In _ECCV_, pages 250-269. Springer, 2022.\n' +
      '\n' +
      '부록 A\n' +
      '\n' +
      '### Dataset\n' +
      '\n' +
      '다양한 피부 색상, 피부 톤, 성별, 연령 및 민족을 포함하는 [1]의 188개의 초고품질 상업적 데이터 샘플을 포함하는 3DScan 데이터 세트를 사용한다. 각 정체에 대해 데이터는 고해상도의 확산, 정상, 거칠기, 사각형 질감과 함께 고품질 3D 헤드 및 안구 메쉬를 포함한다(4K 및 8K). 특히, 제공된 질감은 근거 진리와 동일하고 확산 맵은 조명 효과를 포함하지 않는다. 데이터세트로부터 FLAME 메쉬 포맷으로 모든 3D 메쉬를 등록하고 모든 텍스처 맵과 상업용 Wrap4D[2]를 통해 FLAME UV 매핑을 정렬한다. 우리는 피부색, 성별, 민족성, 연령의 개별적인 정체성 속성을 바탕으로 데이터셋을 주석을 둔다.\n' +
      '\n' +
      'PBR 추출기 훈련.\n' +
      '\n' +
      '우리는 데이터 세트를 사용하여 정상, 종방향 및 거칠기 텍스처 추정을 위해 별도의 디코더를 훈련시킨다. 우리는 확산 텍스쳐에 대해 SD-2.1 염기 모델에서 가변 오토인코더(VAE)를 직접 적용하고 인코더를 동결하고 입력으로서 확산 텍스쳐를 취하고 데이터셋 위에 다른 맵을 생성하기 위해 핀셋 3 개별 디코더를 생성한다. D_{n}}(E_{r})는 정상, 종량 및 거칠기}}(E_{n,\\}}), \\(D_{n,{r}}), \\(D_{r}})는 정상, 종량 및 거칠기}}(D_{n,{r}}), \\(D_{r}}(D_{n,{n,{r}}(D_{n,{n)이고,\\}(D_{r}}(D_{n,\\,{n,\\,{n,\\,{n,\\,{n,\\,{n,{r},\\,{n,\\,{n,\\,{n,\\,{n,\\,{n,\\,{r},\\,{n,\\,{n,\\,{r},\\,{n,\\,{r},\\,{r})는 정상,\\,{n,\\,{r},\\,\\,{r},\\,\\,{r},\\,\\,{r})\n' +
      '\n' +
      '### Inpainting\n' +
      '\n' +
      '우리는 확산 변성 과정의 첫 번째 \\(T-N) 단계에서 잠복 실화를 수행한다. 우리는 가시성 마스크 \\(V\\)를 잠재 가시성 마스크 \\(V^{*}\\)로 다운샘플링하고 \\(I_{m}\\)를 잠재 코드 \\(z^{m}=E(I_{m})\\로 인코딩한다. 각 변성 단계에서 [11]과 유사하게 \\(z_{t}^{*}=V^{*}}\\odot(z^{m}+\\epsilon_{t}})+(1-V^{**}})\\odot z_{t}\\)를 업데이트하여 인징을 적용하며, 여기서 \\(z_{t}\\)는 변성된 잠재이고 \\(z_{t_{t}\\)은 시간 단계(t\\)는 시간 단계(t\\)의 잡음이다.\n' +
      '\n' +
      '이미지가 입력으로 사용될 때 BLIP-2 [45]를 사용하여 결국 AGT-DM에 공급될 자막을 생성한다. 중성 얼굴 메쉬 생성을 위해 발현 및 포즈 파라미터를 0으로 설정했다.\n' +
      '\n' +
      '부록 B 평가 내역.\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '**Latent3d**[14]는 텍스트 또는 이미지 기반 프롬프트를 사용하여 3D 모델의 모양과 질감을 변화시킨다. CLIP와 3D GAN을 서로 다른 렌더링기와 결합하여 특정 속성 조작에 대한 입력 잠재 코드를 조정하면서 다른 속성을 변하지 않는다.\n' +
      '\n' +
      '**CLIP매트릭스***[38]은 CLIP 텍스트 임베딩을 기록하여 텍스트 프롬프트에 의해 제어되는 고해상도 관절형 3D 메서를 생성한다.\n' +
      '\n' +
      '***Text2Mesh**[50]은 텍스트 프롬프트에 기초하여 3D 메쉬를 스타일 편집을 위한 뉴럴 스타일 필드 네트워크 및 CLIP 모델을 사용하여 스타일링한다. 사전 훈련된 모델이나 전문 데이터 세트에 의존하지 않습니다.\n' +
      '\n' +
      '**CLIPFace**[9]는 텍스트를 사용하여 3D 얼굴의 표현과 외관을 제어한다. 3D 모델과 생성 모델을 결합하여 표현, 질감, 관절된 얼굴을 적대적 훈련과 서로 다른 렌더링으로 만든다.\n' +
      '\n' +
      '**DreamFace***[76]은 CG 파이프라인과 호환되는 개인화, 애니메이션이 가능한 3D 얼굴 자산을 생성하도록 설계된 진보적인 텍스트 유도 방법으로 사용자가 특정 형상, 질감, 세부 애니메이션으로 얼굴을 맞춤화할 수 있다. 드림페이스는 공개적으로 구현이 없기 때문에 웹사이트 UI[3]를 사용하여 모든 PBR 텍스처와 함께 메세지를 생성하고 다운로드했다.\n' +
      '\n' +
      '**FlameTex***[24]는 FFHQ 데이터세트로부터 무작위로 선택된 1500개의 이미지를 사용하여 개발된 FLAME 모델에 맞춘 PCA 기반 텍스트링 모델로, 기본 텍스처는 바젤 페이스 모델[54]에서 나온 것이다.\n' +
      '\n' +
      '***파노헤드**[8]은 비정형 영상에서 전체 인간 헤드의 뷰-지속 360\\({}^{\\ 회로}\\) 이미지를 만든다. 새로운 3D GAN 훈련과 특징 얽힘 해상도 기술을 사용하여 단일 이미지로부터 아바타를 만듭니다.\n' +
      '\n' +
      '### Qualitative Comparison\n' +
      '\n' +
      '우리는 그림 1의 질적 시각화를 위한 정량적 비교 실험에서 하나의 조명 조건에서 울트리아바타, 드림페이스 및 파노헤드에서 각각 생성된 8개의 샘플을 제시한다. 10. 비교 실험에 사용된 40개의 프롬프트에서 해당하는 8개의 프롬프트가 있다. 우리는 렌더링을 위해 유니리얼 엔진을 사용합니다. 각 방법에 대한 세 가지 관점(전경, 좌경 -45도 각도, 우경 45도 각도)에서 결과를 표시한다. 또한, 입력 프롬프트에서 생성된 파노헤드 결과의 중간 이미지는 그들의 입력과 우리의 입력이다. 이에 비해 울트리아바타는 더 높은 품질 결과를 전달하고 입력된 텍스트와 생성된 아바타 사이의 보다 정확한 정렬을 달성한다. 파노 헤드는 만족스러운 결과를 제공하지만 낮은 해상도로 줌, 더 나아가 애니메이션성 아바타를 생산할 수 없을 때 가장자리와 경계를 따라 많은 유물이 있다.\n' +
      '\n' +
      'GPT4-V## 평가 GPT4-V#### 평가.\n' +
      '\n' +
      '최근 출시된 GPT-4V(시온)[5, 6]는 이미지 [72, 78]에 대한 뛰어난 인간 정렬을 가진 효과적인 평가 도구로 인식되고 있다. 우리는 생성된 아바타의 렌더링된 이미지를 정성적으로 비율을 얻기 위해 GPT-4V를 레버리지한다. 우리는 광실현, 인공 최소화, 피부 텍스 10: ** 정성적 비교**의 다섯 가지 기준에 기초하여 GPT-4V 수행 평가를 요청하며, 드림페이스 및 파노헤드와 관련된 정량적 비교 실험의 일부 결과를 보여준다. 초아바타는 더 높은 품질, 더 큰 다양성, 더 나은 충실도 결과를 생성하며, 최첨단 방법을 분명히 능가한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>