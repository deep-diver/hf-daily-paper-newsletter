<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#소토피아-7: 사회지능형 언어 에이전트의 상호작용 학습\n' +
      '\n' +
      ' 왕왕1 호페이유원 왕신장1 정양기원\n' +
      '\n' +
      '마텐 삽 그레이엄 노비히 요나탄 비스크 하오 주\n' +
      '\n' +
      '(주)언어기술연구소\n' +
      '\n' +
      '카네기 멜론 대학교\n' +
      '\n' +
      '코드 데이터 체크포인트\n' +
      '\n' +
      '[https://pi.sotopia.world](https://pi.sotopia.world)\n' +
      '\n' +
      '각주 1: 선도 작가들. 개인 분담금 : §G.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '_휴먼은 모방과 사회적 상호 작용을 통해 사회적 기술을 학습한다._휴먼은 모방과 사회적 상호 작용을 통해 사회적 기술을 학습한다. 이러한 사회 학습 과정은 언어 에이전트를 구축하는 기존 연구에 의해 크게 연구되지 않았다. 이러한 격차로 인해 언어 에이전트의 사회적 지능을 향상시키는 대화형 학습 방법인 \\(\\mathsf{SOTOPIA}\\)-77을 제안한다. 이 방법은 대규모 언어 모델(LLM) 등급에 따라 필터링된 사회적 상호 작용 데이터에 대한 행동 복제 및 자기 강화 훈련을 활용한다. 이를 통해 7B LLM이 언어 에이전트의 안전성을 향상시키고 MMLU 벤치마크에서 일반적인 QA 능력을 유지하면서 전문가 모델(GPT-4 기반 에이전트)의 사회적 목표 완성 능력에 도달할 수 있음을 보인다. 또한 이 훈련 패러다임이 사회적 지능의 LLM 기반 평가에서 몇 가지 어려움을 발견한다는 것을 발견했다: LLM 기반 평가자는 사회적 상호작용을 위해 특별히 훈련된 언어 에이전트의 능력을 과대평가한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '기계 사회적 지능은 생산적인 인간-기계 상호작용에 중요하다(Gweon et al., 2023). 예를 들어, 사용자와 실시간 사회적 상호 작용을 달성하기 위해 가상 에이전트는 인간의 언어적 및 비언어적 사회적 행동을 모방할 뿐만 아니라 협력 및 협상 등의 사회적 기술을 관리해야 한다. 그러나, 대형 언어 모델들(LLM)의 사회적 지능은 여전히 마음가짐 이론(Sap et al., 2023; Ullman, 2023; Shapira et al., 2023), 사회적 규범(Weidinger et al., 2021), 다양한 목표 지향적 사회적 시나리오 탐색(Zhou et al., 2024)을 포함한 다양한 측면에서 인간에 뒤쳐진다. 이것은 격차를 해소하고 LLM 에이전트가 인간과 같은 사회적 의사 결정 능력과 가치를 가진 사회적 상황을 탐색할 수 있도록 권한을 부여하는 도전을 강조한다.\n' +
      '\n' +
      '인간이 탐색, 상호작용, 자기 강화를 통해 이러한 사회적 능력을 습득하는 방식(Tomasello, 2021; Gweon, 2021)에서 영감을 받아, 언어 에이전트의 사회적 지능을 사회적 상호작용(예: Craigslist 상의 판매자 및 구매자 간의 대화)을 통해 향상시키는 _interactive learning_ method, \\(\\mathsf{SOTOPIA}\\)-77(그림 1)을 제안한다.\n' +
      '\n' +
      '\\(\\mathsf{SOTOPIA}\\)-77에서는 GPT-4(OpenAI, 2023)를 이용하여 새로운 소셜 태스크를 자동으로 합성하여 전이 가능한 소셜 전략을 학습하는데, 이는 개방형 학습(OEL Team et al., 2021)과 유사하다(단계 1). 다양한 에이전트 집합 내에서 사회적 상호 작용을 시뮬레이션하기 위해 에이전트와 전문가 정책(GPT-4 기반) 또는 에이전트 정책의 두 인스턴스 간의 상호 작용 데이터를 수집한다.\n' +
      '\n' +
      '그림 1: (1) 새로운 소셜 태스크를 자동으로 생성하는 \\(\\mathsf{SOTOPIA}\\)-77을 제안하고, (2) 전문가 정책과 에이전트 정책 모두에서 데이터를 수집하고, (3) GPT-4에서 평가한 양의 데이터를 기반으로 에이전트 정책을 업데이트한다. (4) 파트너 에이전트와 함께 \\(\\mathsf{SOTOPIA}\\)에서 작업을 수행하는 훈련된 에이전트에 대해 인간 및 GPT-4 평가를 구현한다. 우리의 훈련 패러다임에는 행동 복제와 자기 보강이 포함된다. 평가에는 \\(\\mathsf{SOTOPIA}\\)-\\(\\mathsf{VAL}\\)과 고정 파트너 정책(GPT-3.5 기반)을 사용한다. 문자 프로파일은 생략되고 예시는 데모를 위해 단축된다는 점에 유의한다.\n' +
      '\n' +
      '두 개의 샘플링된 문자를 재생합니다(단계 2). 사회적 상호작용의 긍정적인 예를 강화하기 위해 GPT-4를 사용하여 에이전트가 목표를 얼마나 잘 달성할 수 있는지에 대한 등급을 제공하고 이 점수에 대한 임계값을 기반으로 상호작용 데이터를 필터링한다. 그런 다음 에이전트 정책을 _behavior cloning_(사회적 기술이 강한 전문가 모델의 행동으로부터 학습)와 _self-reinforcement_(모델 자체의 높은 평가된 행동으로부터 학습)의 두 패러다임 중 하나 또는 둘 다로 업데이트한다(단계 3). SOTOPIA(Zhou et al., 2024) 환경에서 훈련된 에이전트 모델들에 대한 인간 및 GPT-4 기반 평가로 우리의 방법을 평가한다(SS2.1).\n' +
      '\n' +
      '우리의 작업에서 가장 가까운 것은 Sable Alignment(Liu et al., 2024)이며, 이는 단일 턴 질의 응답 작업에서 사회적 정렬을 연구한다. 반면, SOTOPIA-\\(\\pi\\)는 언어적 의사소통을 넘어 현실적인 사회적 시나리오에서 멀티턴 상호작용 능력을 향상시킨다. SS6은 정렬을 개선하기 위해 명시적으로 설계되지 않았음에도 불구하고 모델이 더 안전하게 행동하고 더 적은 독성 반응을 생성하도록 훈련한다는 것을 보여준다. 인간의 관여와 온라인 보상 모델(Ziegler et al., 2020; Ouyang et al., 2022)을 요구하지 않고, 우리의 방법은 (1) LLM과 오프라인 소셜 상호작용 데이터를 수집하고 (2) 언어 에이전트가 자신과 전문가 모델의 소셜 지식을 탐색하고 강화할 수 있기 때문에 효율적이고 확장 가능하다.\n' +
      '\n' +
      '우리의 방법을 사용하여 사회적 지능 에이전트를 훈련하기 위해 두 훈련 패러다임의 효과와 가능한 부작용(예: 지식 손실 또는 안전)을 조사한다. 또한 인간의 판단을 통해 훈련된 모델의 사회적 지능을 평가함으로써 LLM 등급에서 LLM 훈련의 효과를 이해하는 것을 목표로 한다. 따라서 우리는 다음과 같은 연구 질문에 답할 것을 제안한다:\n' +
      '\n' +
      '**RQ1**: SOTOPIA-\\(\\pi\\)는 언어 에이전트의 사회적 목표 완성 능력과 전반적인 사회적 지능을 향상시킬 수 있는가?\n' +
      '**RQ2**: LLM 등급이 언어 에이전트에서 사회 지능을 훈련하기 위한 인간 등급에 대한 효과적인 대리인입니까?\n' +
      '**RQ3**: SOTOPIA-\\(\\pi\\)로 훈련하는 것이 언어 에이전트의 다른 능력에 어떤 영향을 미치는가?\n' +
      '\n' +
      '**RQ1**의 경우, 우리의 연구 결과는 자가 보강이 행동 복제로 훈련된 것뿐만 아니라 베이스 7B LLM의 사회적 목표 완성 능력을 현저하게 향상시킨다는 것을 보여준다. 가장 좋은 모델(행동 복제에 이어 자기 강화로 훈련됨)은 GPT-4 기반 평가에 따라 GPT-4의 성능에 접근한다. **RQ2**와 관련하여 우리는 GPT-4 기반 평가와 인간 평가 사이의 증가하는 격차를 관찰하여 언어 모델을 최적화하거나 평가하기 위해 GPT-4 기반 평가에만 의존하는 한계를 강조한다. 이는 사회적 상호 작용을 강력하게 평가할 수 있는 대체 평가자 모델 개발에 대한 향후 작업의 필요성을 시사한다. **RQ3**에 대한 응답으로, 우리의 안전성 평가는 SOTOPIA-\\(\\pi\\)이 안전성을 향상시키고 사회 과제에서 언어 모델의 독성을 감소시킨다는 것을 보여준다. 또한, MMLU(Massive Multitask Language Understanding) 벤치마크(Hendrycks et al., 2020)에서 평가했을 때, SOTOPIA-\\(\\pi\\)이 모델의 원래 질문 응답 능력을 보존한다는 것을 보여준다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '### SOTOPIA environment\n' +
      '\n' +
      '본 논문에서는 소셜 러닝을 위한 플랫폼으로 SOTOPIA(Zhou et al., 2024)를 사용한다. SOTOPIA의 _social task_는 시나리오, 두 캐릭터의 프로필 및 상호 작용에서 달성하기 위한 각각의 사적인 사회적 목표로 구성된다. 시나리오와 사회적 목표의 조합은 협상, 협력 및 경쟁을 포함한 광범위한 사회적 상호 작용을 포함한다. 사회적 과제가 주어졌을 때, SOTOPIA는 두 LLM이 역할극 _social agent_ 역할을 하고 말하기, 비언어적 의사소통 및 행동을 통해 서로 상호 작용하도록 촉구한다.\n' +
      '\n' +
      '도 2에 도시된 예를 고려하면, 다음과 같다.\n' +
      '\n' +
      '그림 2: L: 캐릭터 프로필이 있는 사회적 과제. R: 롤플레잉 캐릭터의 관점에서 본 예시이다. 이번 턴은 두 캐릭터가 각자의 턴에서 각각 발언한 후 세 번째 턴이다.\n' +
      '\n' +
      ' (1) 시나리오(여행 계획 논의), (2) 역할극 캐릭터(Sam)의 프로필과 목표(여행에 참여하도록 설득하기), (3) 다른 캐릭터(Mia)의 프로필에 대한 가시적인 정보, (4) 의사소통 이력(Mia는 초대를 거절했다)을 포함한 상호작용 맥락에 기초하여 SOTOPIA에서 역할극 캐릭터(역할극 캐릭터)가 자신의 차례에 결정을 내린다. 결정은 (1) 행동 유형, 발화를 _speak_ing 중 선택, 제스처 또는 표정을 _non-verbal communication_로 하는 것, 물리적 _action_를 수행하는 것, 대화를 _leaving_로 하는 것, (2) 행동 내용, 예를 들어 \'나는 완전히 이해한다!\'를 발화로, \'눈썹을 비언어적 의사소통으로, \'How Mia some scenery photo\'를 행동으로 한다.\n' +
      '\n' +
      'SOTOPIA-IVAL(Zhou et al., 2024)은 7개의 _social dimension_에 기초하여 소셜 에이전트의 _social intelligence_에 대한 평가를 제공한다. 7가지 차원은 믿음(Bel), 관계(Rel), 지식(Kno), 비밀(Sec), 사회적 규칙(Soc), 금융 및 물질적 이익(Fin), 목표 완성(Goal)이다. 전체 점수는 전체 사회 지능을 반영하는 7가지 사회적 차원의 평균이다. 각 차원은 GPT-4(OpenAI, 2023)에 의해 평가되며 Likert scale.1Zhou 등(2024)은 언어 모델을 SOTOPIA-EVAL로 평가할 때 GPT-4가 이러한 차원에 대한 인간 판단의 대용물이 될 수 있으며 인간 평가보다 높은 상관성과 중요성을 가지고 있음을 보여준다. 따라서 (Zhou et al., 2024)에 따라 GPT-4를 사용하여 모델의 사회적 성과를 평가할 뿐만 아니라 인간의 판단을 수집하여 결과를 검증한다. 본 논문에서는 소셜 에이전트를 향상시키기 위한 훈련 신호로 GPT-4 기반 평가를 사용하는 방법에 대해 연구한다.\n' +
      '\n' +
      '각주 1: 상이한 차원은 [-10, 0], [-5, 5], [0, 10]의 세 가지 유형의 점수 범위를 갖는다.\n' +
      '\n' +
      '### Interactive learning\n' +
      '\n' +
      '본 논문은 사회적 지능 향상을 위한 상호작용학습에 초점을 맞추고 있다. 우리는 대화형 학습을 다른 에이전트와의 대화형 소셜 대화를 통한 _learning으로 간주한다. 대화형 학습을 구현하는 가장 일반적인 방법은 강화 학습이다(RL을 가진 LLM 훈련과 관련된 작업은 SS7에서 논의될 것이다). 본 논문에서는 전문가로부터의 학습(행동 복제)과 모델의 긍정적 행동 강화(자기 강화)의 두 가지 형태의 대화형 학습을 고려한다.\n' +
      '\n' +
      '_Behavior cloning_ (BC)(Pomerleau, 1988; Torabi et al., 2018)는 고품질의 관측 데이터로부터 학습하는 기술로서, 특히 기술이 강한 전문가의 행동 궤적으로부터 학습한다. 사회적 과제의 맥락에서 궤적은 멀티턴 대화의 사회적 상호 작용 데이터로 정의된다. 광범위하고 고품질의 인간 대화 데이터를 수집해야 하는 어려움으로 인해, 우리는 이러한 행동 궤적을 공급하기 위해 최첨단(SOTA) 모델을 사용함으로써(Wang and Jansen, 2023), 전문가 입력을 위한 프록시로서 이들 모델의 소셜 지능을 활용한다(Gandhi et al., 2023). 특히, GPT-4 기반 에이전트를 전문가로 사용하여 SOTOPIA(Zhou et al., 2024)에서 가장 우수한 성능을 보였다.\n' +
      '\n' +
      '_Self-reinforcement_(SR)(Bandura, 1976)은 학습을 위해 자체적인 상호 작용을 생성하고 평가하는 오프라인 강화 학습 방법이다. SR의 가장 근접한 구현은 ReST(Gulcehre et al., 2023)이며, 이는 반복 임계값 기반 데이터 필터링 방법을 채택하고 시간이 지남에 따라 더 높은 품질을 갖는 데이터에 대해 트레이닝한다. 예비 실험에서 이 전략은 신중한 임계값 조정이 필요하지만 한계 개선만 나타났으며 임계값 기반 필터링은 다양한 난이도 수준에서 여러 작업에 대해 잘 작동하지 않는다는 것을 발견했다. 이러한 경험을 바탕으로 반복 없이 SR을 가능하게 하는 비율 기반 데이터 필터링 방법을 제안한다.\n' +
      '\n' +
      '## 3 SOTOPIA-7 프레임워크\n' +
      '\n' +
      'SOTOPIA-7은 (1) 소셜 태스크 생성, (2) 트레이닝 데이터 수집, (3) 에이전트 정책 업데이트의 세 단계를 통해 현재 정책\\(\\pi_{\\text{ref}}\\)에서 시작하는 언어 에이전트의 소셜 지능을 향상시킨다. 이 섹션에서는 파이프라인에서 세 단계에 대한 세부 정보를 제공한다.\n' +
      '\n' +
      '###1단계: 사회적 과제 생성\n' +
      '\n' +
      '일상적인 사회적 상호 작용에서 서로 다른 사회적 기술을 습득함으로써 인간이 새로운 사회적 상황을 탐색하는 방식을 미러링하여 역동적이고 다양한 사회적 환경 내에서 사회적 기술을 탐구하는 데 있어 언어 에이전트의 지속적인 학습을 권장한다. 개방형 학습을 위한 동적 태스크 생성 원리(OEL Team et al., 2021)를 채택하여, 인터랙티브 학습의 토대로서 다양한 소셜 태스크 세트를 제공한다. 첫 번째 단계로 SOTOPIA-7은 (1) Social Chemistry(Forbes et al., 2020), Social IQa(Sap et al., 2019), Normbank(Ziems et al., 2023) 및 (2) 두 단계를 통해 합성 소셜 태스크를 자동으로 생성한다(도 3). 소셜 태스크 생성에 대한 자세한 내용은 부록 SSB.1에서 확인할 수 있다.\n' +
      '\n' +
      '우리는 이름, 성별, 직업, 성격 및 기타 배경을 포함하여 SOTOPIA에서 40개의 캐릭터 프로필을 재사용합니다. 각 사회과제에 대해 한 쌍의 문자가 무작위로 샘플링된다. 훈련에 사용되는 사회적 과제(시나리오, 캐릭터의 프로필, 사회적 목표의 조합)는 평가에 사용되는 사회적 과제와 중복되지 않도록 보장된다. 더 나은 작업 품질을 위해 수동 검사 및 필터링을 포함하는 SOTOPIA에서 사용되는 인간-인-루프 절차와 달리, 우리는 여과되지 않은 많은 사회적 작업을 생성하기 위해 자동화되고 확장 가능한 접근법을 취한다. 실험 결과는 우리의 방법이 더 낮은 품질의 방대한 양의 사회적 과제를 사용할 때 언어 에이전트의 성능을 크게 향상시킬 수 있음을 보여준다. 고품질 사회 과제를 필터링하기 위해 보다 정교하거나 수동적인 선택 프로세스를 활용하면 잠재적으로 추가 개선으로 이어질 수 있으며, 이는 향후 작업을 위해 남겨둔다.\n' +
      '\n' +
      '**2단계 : 훈련 데이터 수집**\n' +
      '\n' +
      '생성된 사회적 과제를 바탕으로 SOTOPIA-7의 두 번째 단계는 행동 복제와 자기 강화를 위한 훈련 데이터를 수집하는 것이다. 사회적 상호작용 동안, SS2.1에 요약된 바와 같이, 두 언어 에이전트는 사회적 과제의 가시적인 구성 요소와 대화 이력에 기초하여 응답을 대체한다. 행동 복제는 두 개의 GPT-4 기반 에이전트의 전문가 정책\\(\\pi_{\\text{expert}}\\) 사이의 상호 작용을 사용하여 두 개의 샘플링된 캐릭터를 역할 연기한다. (Zhou et al., 2024)에 따르면 GPT-4 기반 에이전트 간의 대화는 다른 LLM 중에서 가장 높은 사회적 점수를 얻을 수 있기 때문이다. 마찬가지로, 자기 강화를 위해 에이전트 정책\\(\\pi_{\\text{ref}}\\) 역할을 수행하는 두 개의 샘플링된 문자 간의 상호 작용을 수집한다.\n' +
      '\n' +
      '전문가 데이터를 얻는 것은 비용이 많이 들고 항상 액세스할 수 있는 것은 아닐 수 있다. 여러 전문가 모델을 사용하는 것이 옵션이지만, 우리의 연구 결과는 GPT-4 기반 에이전트의 전문가 정책을 사용한 단일 라운드 행동 복제 후 에이전트 모델의 성능이 GPT-3.5 기반 에이전트의 성능을 능가한다는 것을 나타낸다. 따라서 우리는 전문가 모델로 GPT-4를 선택합니다. 전문가 데이터를 사용할 수 없거나 에이전트의 능력이 전문가의 능력을 초과하는 상황에서 자기 강화는 매우 중요합니다. 우리는 인간 대화 데이터를 향후 작업을 위한 행동 복제의 전문가 궤적으로 사용할 가능성을 남겨둔다.\n' +
      '\n' +
      '**3단계 : 에이전트 정책 업데이트**\n' +
      '\n' +
      'SOTOPIA-7의 마지막 단계는 트레이닝 데이터로부터 긍정적인 예시에 기초하여 에이전트의 정책을 업데이트하는 것을 포함한다. AI 피드백을 활용하는 것은 평가 프로세스를 자동화하고 인간 라벨이 없는 언어 모델의 학습을 개선하는 데 유용하다(Bai et al., 2022). 사회적 상호작용의 각 에이전트에 대해 에이전트의 사회적 성과에 대한 GPT-4의 평점과 해당 추론을 수집한다. SOTOPIA-EVAL에서 사회적 수행의 7가지 사회적 차원 중 특히 에이전트가 사회적 목표를 달성하는 정도로 0에서 10 사이의 점수를 매긴 _목표 완료_ 차원에 초점을 맞춘다. Zhou et al. (2024)는 모든 7가지 차원 중에서 목표 완성에 대한 GPT-4에 의한 평점이 인간 평점과 가장 높은 상관 관계를 갖는다는 것을 발견한다. SS4와 SS8에서 우리는 등급을 제공하기 위해 LLM을 사용하는 잠재적인 문제에 대해 논의한다.\n' +
      '\n' +
      '우리는 GPT-4에 의해 평가된 목표 완료 점수에 대한 임계값을 설정하여 훈련 데이터를 필터링한다(필터링 전략의 세부 사항은 부록 SSB.2 참조). 상호작용 데이터의 각 턴은 입력 및 출력의 트레이닝 쌍으로 파싱된다. 입력을 위해 에이전트가 볼 수 있는 작업에 대한 정보와 대화 이력의 조합을 제공합니다. 출력은 액션 타입 및 콘텐츠의 JSON 스트링을 출력으로 제공한다(자세한 내용은 부록 SSB.3 참조). 필터링된 포지티브 트레이닝 데이터를 기반으로 에이전트 모델에 대한 감독 미세 조정으로 에이전트의 정책을 업데이트한다. 에이전트 정책이 처음에 행동에 의해 업데이트되는 순차적 훈련 접근 방식을 추가로 탐색합니다.\n' +
      '\n' +
      '도 3: 소셜 태스크를 생성하기 위한 프롬프트 템플릿.\n' +
      '\n' +
      '복제 그런 다음 업데이트된 에이전트 정책은 자체 강화를 위한 상호 작용 데이터를 생성하는 데 참여합니다.\n' +
      '\n' +
      '##4 실험 설정\n' +
      '\n' +
      '이 섹션에서는 실험에서 비교한 에이전트 모델의 세부 사항에 대해 논의한다. 또한, SOTOPIA-7에서 사용하는 훈련 및 평가 구성에 대한 세부 사항을 보여준다.\n' +
      '\n' +
      '에이전트 모델은 전문가 에이전트 모델로 GPT-4(OpenAI, 2020)를 선택하고 기본 에이전트 모델로 Mistral-7B(Jiaia et al., 2023)를 선택한다. 본 논문에서는 (1) 전문가모델(GPT-4)이 제공하는 정책에 기반한 행동복제, (2) 에이전트 정책에 기반한 자기강화, (3) 행동복제에 이어 자기강화 등 세 가지 접근방법을 이용하여 기본 에이전트 모델을 개선하고자 한다. 실험의 기준은 전문가 모델(GPT-4)과 기본 모델(Mistral-7B)을 사용하여 고정 에이전트 모델(GPT-3.5-터보)을 사용하여 프롬프트 기반 역할 수행을 수행한다. 위의 네 가지 접근법을 사용하여 훈련된 에이전트 모델과 기준선을 비교한다. 모든 에이전트 모델은 동일한 프롬프트 형식을 공유하고 소수의 프롬프트를 사용하여 소셜 태스크에 대한 응답을 생성합니다. 실험에서 사용한 프롬프트 형식 및 특정 모델 버전과 관련된 자세한 내용은 부록 SSB.3 및 SSB.4에서 찾을 수 있다.\n' +
      '\n' +
      '실험에서, 우리는 행동 복제, 자기 강화 및 이들의 조합과 함께 기본 에이전트 모델 Mistral-7B 상에서 양자화된 LLMs(QLoRA)(Dettmers et al., 2023) 상의 효율적인 미세 조정을 활용한다. 우리는 GPT-4를 사용하여 교육 라운드당 협상, 협업, 경쟁을 포함한 사회적 주제를 가진 100개의 사회적 과제를 생성한다. 각 소셜 태스크에 대해 에이전트 모델에 의해 역할 재생되는 10개의 서로 다른 캐릭터 쌍으로 10개의 소셜 상호 작용을 실행한다. 두 에이전트 모델 간의 다중 회전 소셜 대화는 학습 데이터로 수집되고 필터링된다. 소셜 태스크 생성, 훈련 데이터 수집 및 훈련 설정과 관련된 더 자세한 내용은 부록 SSB.1, SSB.4 및 SSB.5에서 별도로 찾을 수 있다.\n' +
      '\n' +
      '평가 우리는 SOTOPIA-Eval에 정의된 7가지 사회적 차원을 기반으로 에이전트 모델을 평가한다. 또한 7가지 사회적 차원의 평균 점수인 전체 점수를 제공합니다. 평가를 위해 업데이트된 에이전트 정책\\(\\pi_{\\text{agent}\\)과 고정된 파트너 정책\\(\\pi_{\\text{partner}}\\) (GPT-3.5-turbo)(OpenAI, 2023) 사이의 상호 작용을 수집하고 7가지 사회적 차원 모두에서 인간과 GPT-4 등급을 얻는다. 우리는 90개의 모든 사회적 과제와 90개의 사회적 과제 중에서 선택된 14개의 하드2개의 사회적 과제의 하위 집합에 대한 에이전트의 수행을 보고한다. 균형 잡힌 발언 질서를 유지하기 위해, 우리는 두 에이전트가 사회적 과제 내에서 대화를 시작할 수 있는 동등한 기회를 갖도록 보장한다. 평가 점수에 대해 GPT-4를 프롬프트하여 제공하는 자동 평가와 자격을 갖춘 인간 주석자가 제공하는 인간 평가 모두를 실행합니다. 우리는 GPT-4 기반 자동 평가를 위해 SOTOPIA-Eval과 동일한 프롬프트를 사용한다.\n' +
      '\n' +
      '각주 2: Zhou et al.(2024)은 최첨단 LLM과 인간 모두에게 더 어려운 원래 90개의 소셜 과제 중 14개의 하드 소셜 과제 SOTOPIA-hard를 식별했다.\n' +
      '\n' +
      'SOTOPIA-7이 언어 에이전트의 사회적 지능을 향상시키나요?\n' +
      '\n' +
      '도 4에 도시된 바와 같이, SOTOPIA의 하드 서브세트에 대한 GPT-4 기반 및 인간 평가 모두에 따르면, 자기 강화는 기본 모델(Mistral-7B) 및 행동 복제 모델 모두의 사회적 목표 완성 능력을 향상시킨다. 우리는 또한 전문가의 긍정적인 예제로부터의 학습이 에이전트 정책의 긍정적인 예제로부터의 학습보다 더 효과적이라는 것을 발견할 수 있다. 이를 종합하면, 행위 복제와 자기 강화는 GPT-4 자체의 목표 완료 성능인 5.71(ours) vs 5.89(GPT-4)와 거의 일치하여 에이전트 정책을 크게 개선하며, 전체 결과는 부록 SSA에 제시되어 있다.\n' +
      '\n' +
      '그림 4: 목표 완성 차원의 GPT-4 기반 자동 평가 점수 및 인간 평가 점수. 우리는 SOTOPIA에서 하드 소셜 태스크에 대한 기본 모델, 훈련된 에이전트 모델 및 GPT-4(아이콘으로 표시)의 성능을 보여준다.\n' +
      '\n' +
      '**GPT-4 기반 평가와 인간 평가 사이의 증가하는 격차** 그러나 우리는 GPT-4 기반 평가가 (행동 복제 또는 자기 강화를 통해) 사회적 상호 작용을 위해 특별히 훈련된 모델의 능력을 상당히 과대평가한다는 것을 발견했다. 그림 4에서 볼 수 있듯이, 우리의 방법이 훈련 중 GPT-4 등급 목표 완료 점수를 최적화함에 따라 GPT-4 점수와 인간 점수 사이의 격차가 증가한다. 대조적으로, GPT-4 기반 에이전트에 대한 인간 및 자동 점수 사이의 갭은 더 작아서, 우리의 최상의 BC+SR 모델(4.29 목표 완료 점수) 및 GPT-4 기반 에이전트(5.25)에 대한 인간 점수에서 상대적으로 큰 갭으로 이어진다. 이 결과는 이러한 평가 메트릭을 사용하여 미세 조정된 모델에 대해 사회적 상호 작용을 강력하게 평가할 수 있는 평가 모델 개발에 대한 향후 작업의 필요성을 나타낸다.\n' +
      '\n' +
      '**다른 사회적 차원에 대한 개선** SS3에서 언급된 바와 같이, 우리는 목표 완성 차원에 기초하여 긍정적인 예들에 대한 모델들을 훈련한다. _ 이것이 다른 사회적 차원에 어떤 영향을 미칠까요?_ 표 1은 목표 완료 이외의 차원에 대한 우리의 방법의 개선을 보여준다. 우리의 방법은 전체 점수뿐만 아니라 믿음성, 관계 및 사회 규칙 점수를 크게 향상시키는 반면 다른 사회적 차원에 약간만 영향을 미친다.\n' +
      '\n' +
      '*\\(\\mathtt{SOTOPIA}\\) 시나리오에서의 모든 사회과제에 대한 개선 동향**\\(\\mathtt{SOTOPIA}\\) 시나리오에서의 모든 사회과제에 대해 \\(\\mathtt{SOTOPIA}\\) 시나리오에서의 모든 사회과제에 대해 GPT-4 기반 평가 결과 3에서 \\(\\mathtt{SOTOPIA}\\)의 어려운 사회과제와 유사한 경향을 관찰한다. 표 2에서 볼 수 있듯이, 우리의 방법은 목표 완성 차원뿐만 아니라 전체 점수에서도 기본 모델에 비해 개선을 달성한다. 특히, 우리의 최고 모델(BC + SR)의 성능은 전문가 모델과 비슷하다. 전체 점수 내역은 부록 A를 참조하십시오.\n' +
      '\n' +
      '각주 3: \\(\\mathtt{SOTOPIA}\\)의 모든 사회 과제에 대한 인간 평가는 높은 비용으로 인해 수행되지 않는다.\n' +
      '\n' +
      '**RQ1**와 **RQ2**에 대한 응답은 상호작용 학습(행동 복제 및 자기 강화)을 통해 \\(\\mathtt{SOTOPIA}\\)-\\(\\pi\\)에서 언어 에이전트의 사회적 목표 완성 능력을 향상시킴을 보여준다. 실험 결과로부터 GPT-4 기반 평가의 한계점도 찾을 수 있었다. 이 문서의 후속 섹션에서는 이 훈련 방법이 LLM의 능력의 다른 측면에 어떻게 영향을 미치는지 논의할 것이다.\n' +
      '\n' +
      '##6 \\(\\mathtt{SOTOPIA}\\)-\\(\\pi\\)는 LLMs의 다른 능력에 어떻게 영향을 미치는가?\n' +
      '\n' +
      'LLM이 인간 대화를 모방하는 데 더 능숙해짐에 따라, 의도하지 않게 편향되거나 불쾌한 콘텐츠 헨드릭 및 마지카(2022)와 같은 유해한 결과를 생성하거나(2022), 조작 또는 권력 추구 행동 Si 등(2022)과 같이 본질적으로 인간 의도에 정렬되지 않은 행동을 표시할 수 있다. 이러한 오정렬은 안전 위험 및 실존적 위협 헨드릭 등(2023)과 같은 심각한 위험을 초래한다. 따라서 LLM의 안전 수준을 이해하고 평가하는 것은 잠재적인 유해한 결과를 예방하는 데 필수적이다.\n' +
      '\n' +
      '\\(\\mathtt{SOTOPIA}\\)-\\(\\mathtt{EVAL}\\)은 안전과 관련된 여러 차원을 가지고 있다: Soc, Sec, Rel Zhou et al. (2024). 그러나, 이들 차원들 중 어느 것도 _only_ 안전성을 평가하지 않으므로, 안전의 약점은 그 차원과 관련된 다른 능력들의 강도로 커버될 수 있다. 이를 설명하기 위해 먼저 캐릭터 1의 목표는 \'싫어하는 사람-i_p_a 제3자\'이고 캐릭터 2의 목표는 \'싫어하지만 폭력을 방지하는 것\'인 1\\(\\mathtt{SOTOPIA}\\) 과제 하에서 에이전트의 행동을 정성적으로 연구한다.\n' +
      '\n' +
      '각 캐릭터를 롤플레잉하고 각 에이전트에 대해 여러 양을 수동으로 레이블링하는 5개의 서로 다른 에이전트 모델 각각에 대해 9개의 예를 고려한다. 우리는 (1) "참여율"을 4회 이상의 에피소드의 비율로 정의하고 에이전트가 50% 이상의 시간으로 응답하는 경우, (2) "진행율-in-_i_p_a"를 에이전트가 다른 에이전트를 다치게 하려는 의도를 구두로 표현하는 비율, (3) "예방율"을 에이전트가 다른 에이전트를 다치게 하려는 의도를 구두로 표현하는 비율이라고 정의한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r r r r r r} \\hline \\hline Bel & Rel & Kno & Sec & Soc & Fin & Overall \\\\ \\hline\n' +
      '**2.05** & **1.91** & -0.14 & 0.00 & **1.11** & 0.09 & **0.91** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 인간에 의해 \\(\\mathtt{SOTOPIA}\\)에서 어려운 사회적 작업에 대해 평가된 기본 모델(Mistral-7B)보다 우리의 최상의 모델(행동 복제 후 자기 강화)의 _other_ 사회적 차원에 대한 개선(\\(\\Delta\\))이다. 상당한 개선은 대담하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline \\multicolumn{2}{c}{Agent model} & \\multicolumn{1}{c}{Goal (\\(\\uparrow\\))} & \\multicolumn{1}{c}{Overall (\\(\\uparrow\\))} \\\\ \\hline \\multicolumn{2}{c}{All social scenarios in \\(\\mathtt{SOTOPIA}\\)} \\\\ \\hline \\multicolumn{2}{c}{Expert (GPT-4)} & **7.62** & 3.31 \\\\ \\multicolumn{2}{c}{Base (Mistral-7B)} & 5.07 & 2.33 \\\\ \\hline \\hline \\multirow{3}{*}{Self-reinforcement (SR)} & Self-Reinforcement (SR) & 5.83 & 2.57 \\\\ \\cline{1-1}  & Behavior Cloning (BC) & 7.27 & 3.41 \\\\ \\cline{1-1}  & BC+SR & **7.62** & **3.44** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: \\(\\mathtt{SOTOPIA}\\)-\\(\\pi\\)는 \\(\\mathtt{SOTOPIA}\\)의 모든 사회 과제에 대해 GPT-4에 의해 평가된 바와 같이 목표 완성 점수 및 전체 점수를 향상시킨다. BC+SR은 전문가 모델과 유사한 성능을 달성한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '모델 훈련에서 일반 사회 지능의 효과적인 개발은 아직 완전히 실현되지 않았다.\n' +
      '\n' +
      '연구들은 왕 등(2023)을 통해 사회 지능을 향상시키기 위한 관찰 데이터로부터 행동 복제의 가능성을 조사했다. SOTOPIA-\\(\\pi\\)은 추론적 사회학습 권(2021)의 사회과학 이론을 연구하는데, 모델은 모방뿐만 아니라 사회적 맥락에 대한 추론을 통해 학습한다.\n' +
      '\n' +
      '인간 피드백으로부터의 LLMsReinforcement Learning(RLHF; Christiano et al. (2017))은 인간 선호도 Ouyang et al. (2022)에 대한 LLMs의 정렬을 개선한다. Direct Preference Optimization Rafailov et al. (2023) 및 \\(\\Psi\\) Policy Optimization Azar et al. (2023)은 보상 모델에 의존하지 않고 LLM 정책을 최적화함으로써 RLHF를 개선한다. 이러한 온라인 RL 방법은 종종 온라인 데이터 수집을 필요로 하며, 이는 다중 에이전트 설정에서 더 긴 대기 시간을 갖는다.\n' +
      '\n' +
      '오프라인 자기 강화의 대표적인 유형은 자기 모방 학습 SIL; Oh et al. (2018)), reward ranked fine-tuning RAFT; Dong et al. (2023)), reinforced self-training (ReST; Gulcehre et al. (2023)). SIL은 현재 값 추정보다 나을 때 리플레이 버퍼를 설정하고 상태-액션 쌍을 모방한다. RAFT는 다수의 출력을 생성하고 보상 모델을 활용하여 서브세트를 필터링한다. ReST는 여러 개선 단계를 가진 RAFT의 더 복잡한 버전이다. SOTOPIA-\\(\\pi\\)는 소셜 태스크에서 LLM을 훈련하는 데 오프라인 셀프 강화를 적용하고 GPT-4를 활용하여 멀티턴 소셜 인터랙션에 대한 보상을 제공한다. 우리는 다양한 오프라인 방법이 향후 작업에 대한 사회적 지능 훈련에 미치는 영향을 조사하게 한다.\n' +
      '\n' +
      'LLM Alignment and EvaluationAdvances in fine-tuning Li and Liang (2021); Lester et al. (2021); Hu et al. (2021); Hu et al. (2021); Hu et al. (2021)은 인간에 의해 주어진 제한과 규칙을 더 잘 이해하고, 사회적 학습과 상호 작용을 위한 능력을 향상시키기 위해 LLM의 능력을 향상시켰으며, 다른 거버넌스 목표는 견고성, 해석성, 제어성, 윤리성을 통해 LLM 행동을 정렬하고, G et al. (2024); 우리는 안전성과 독성을 통해 인간 사회 규범과의 훈련된 LLM의 정렬을 평가하는 데 중점을 둔다.\n' +
      '\n' +
      '지속적인 미세 조정은 도메인 지식, 추론 및 읽기 이해 루오 등(2023)의 측면에서 LLM의 치명적인 망각을 초래할 수 있다는 지적이 있었다. 훈련된 LLM의 일반적인 질문 응답 및 추론 능력을 테스트하기 위해 57명의 피험자에 대한 모델의 지식을 테스트하기 위해 설계된 전체론적 벤치마크인 MMLU(Massive Multitask Language Understanding) 벤치마크 헨드릭스 등(2020)에서 성능을 측정한다.\n' +
      '\n' +
      '##8 결론 및 향후 작업\n' +
      '\n' +
      '본 논문에서는 언어 에이전트의 사회적 지능을 향상시키기 위해 LLM 등급을 학습 신호로 사용하는 방법을 연구하기 위한 대화형 학습 방법 SOTOPIA-\\(\\pi\\)을 제안한다. 먼저 목표 완성도 점수를 최적화함으로써 소셜 인텔리전스 벤치마크인 SOTOPIA Zhou et al.(2024)에 대한 일반적인 성능이 향상됨을 알 수 있다. 그러나 우리는 이러한 과정을 통해 LLM 등급과 인간 판단의 격차가 확대됨을 발견한다. 또한 SOTOPIA-\\(\\pi\\)는 일반적인 QA 능력의 손실 없이 안전성의 향상과 함께 사회 지능을 향상시킨다는 것을 발견했다.\n' +
      '\n' +
      '비록 SOTOPIA-\\(\\pi\\)이 사회적 지능을 향상시키는 강력한 능력을 보여주지만, 몇 가지 방향은 우리의 방법을 더욱 향상시킬 것이다. (1) 온라인 강화학습: SOTOPIA-\\(\\pi\\)은 반복적으로 개선할 수 없는 오프라인 교육방법이다. 향후 연구에서는 PPO Schulman et al.(2017)과 같은 온라인 방법이 LLM 등급의 높은 비용 없이 어떻게 적용될 수 있는지 연구할 수 있다. (2) 인간으로부터 학습: SS2에서 언급한 바와 같이, 우리는 인간 상호작용 데이터를 수집하는 어려움으로 인해 GPT-4를 전문가로 사용한다. 향후 작업은 포럼 대화, 영화 및 대화 데이터 세트를 포함한 기존 데이터를 훈련 에이전트의 오프라인 데이터로 사용하여 탐색할 수 있다. (3) SS6에서, 우리는 하나의 소셜 태스크만을 평가하는데, 이는 우리가 태스크를 깊이 파헤치고 맞춤형 메트릭을 생성할 수 있게 한다. 또한, 모든 사회 과제에 대한 안전 측정 기준을 도출하는 방법은 흥미로운 미래 방향이다. (4) SS5에서 입증된 바와 같이, 모델이 GPT-4 점수를 최적화함에 따라 GPT-4와 인간 평가 사이의 갭이 증가한다. 향후 연구에서는 사회 지능 과제에 대한 보다 강력한 평가 및 학습 신호를 고려할 수 있다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '실험에서 LLM을 평가자로 사용하여 GPT-4를 사용하여 사회적 상호 작용의 긍정적인 행동에 대한 평가를 제공하고 사회적 작업에 대한 에이전트의 성능을 평가한다. 그러나 우리의 연구 결과는 훈련된 에이전트 모델에 대한 GPT-4 기반과 인간 평가 사이의 격차가 증가하고 있음을 보여준다. 이는 LLM을 사회적 성과를 평가하기 위한 평가자로 사용하는 잠재적 편향을 나타낸다.\n' +
      '\n' +
      '안전을 제외한 사회적 정렬 차원으로 안전성을 사용하면 프라이버시, 공정성, 신뢰성 등 LLMs의 사회적 정렬과 관련된 다른 사회적 차원이 존재한다(Liu et al., 2023). 사회적 정렬과 관련된 사회적 작업의 제한된 범위로 인해 훈련된 에이전트의 안전 측면만 연구한다.\n' +
      '\n' +
      'GPT-4에 의해 생성된 상호작용 시스템 콘텐츠 내의 잠재적인 사회적 편향은 잠재적인 사회적 편향 및 고정관념을 포함할 수 있다. 우리가 사용하는 SOTOPIA 대화형 환경은 GPT-4에 의해 구동되며, 이는 의도하지 않은 사회적 편향을 가진 훈련 에이전트로 이어질 수 있다.\n' +
      '\n' +
      '윤리적 진술 SOTOPIA-\\(\\pi\\) 프로젝트의 목표는 SOTOPIA-EVAL에 의해 평가된 바와 같이 AI 에이전트의 사회적 지능을 향상시키는 것이다. Zhou et al.(2024)과 유사하게, 우리는 또한 더 현실적인 대화를 만들고, 더 나은 관계를 육성하고, 지식 있는 대화를 제공하고, 비밀 유지를 하고, 사회적 규칙을 따르고, 금융 및 물질적 이득을 달성하기 위한 에이전트의 능력을 향상시키고, 사회적 목표를 완성하는 데 중점을 둔다. 우리의 목표는 인간과 구별할 수 없는 AI 시스템을 만들거나 잠재적인 글로벌 위험을 만드는 것이 아니라는 점에 유의하는 것이 중요하다(유드코프스키 등, 2008). 대신 우리의 목표는 인간 사회 지능의 발달과 학습 과정을 연구하는 것이다. 또한, 이 연구는 인간 참가자를 포함하는 데이터 수집에 대한 비용이 많이 들지 않으면서 다양한 상황에서 사회적 행동에 대한 통찰력을 제공한다. 대규모 언어 모델, 특히 전략적 사회적 상호 작용을 위해 설계된 AI 시스템을 구축하는 것은 예상치 못한 결과와 잠재적으로 부정적인 사회적 영향을 초래할 수 있기 때문에(Si et al., 2022), 우리는 실험에 조심스럽게 접근한다. 구체적으로, 대형 언어 모델의 역할-수행 능력은 샤나한 외(2023)에 의해 설명된 바와 같이 의인화로 이어질 수 있으며, 여기서 AI 시스템은 인간과 유사한 성격을 나타내는 것으로 인식된다. 우리의 연구는 잠재적으로 Zhang et al.(2023)의 프레임워크를 참조하여 이러한 문제를 이해하고 책임감 있게 탐색하는 것을 목표로 한다.\n' +
      '\n' +
      '우리는 GPT-4를 포함한 임의의 LLM을 사용하여 우리의 시스템인 SOTOPIA-EVAL을 평가할 수 있다는 것을 인정한다(Wang et al., 2023; Gallegos et al., 2023). 우리의 향후 연구는 사회 및 문화적 편향을 식별하고 이해하며 완화하는 데 초점을 맞출 것이다(Tao et al., 2023). 편견을 통합하지 않고 모델의 사회적 지능을 향상시키는 것은 우리에게 필수적이다. 이 단계는 책임감 있고 편견 없는 AI 에이전트의 개발에도 중요합니다. 또한, 우리의 연구는 독성 언어 생성 또는 유해한 제안과 같은 안전하지 않은 행동의 사례가 모델의 훈련 중에 나타날 수 있음을 관찰했다. 이러한 행동들은 상당한 사회적 위험과 안전 위험을 나타낸다(Hendrycks et al., 2023; Wang et al., 2023). 이러한 문제를 해결하는 것은 사회에서 AI의 안전하고 윤리적인 사용을 보장하는 데 중요하며 특히 AI 시스템의 개발 중에 중요하다.\n' +
      '\n' +
      '인간 평가 연구에서 우리는 모든 주석자가 영국이나 미국에 기반을 두고 있음을 확인한다. 미국에서는 각 작업에 대해 주석자가 1.5달러의 비율로 보상되는데, 각 작업에 10분 이상 걸리지 않을 것이라는 기대감이 있다. 이 설정은 잠재적으로 시간당 9달러 이상을 벌 수 있도록 하는 반면, 영국에서는 주석자의 평균 수입이 시간당 14.5달러를 초과하도록 추가 보너스를 제공하여 영국의 최저 임금 기준과 일치하도록 하며, 모든 인간 대상 실험은 저자 기관의 기관 검토 위원회(IRB)의 승인을 받는다.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      'RW, HY, WZ 및 ZQ는 CMU 대학원 소규모 프로젝트 도움말(GuSH) 연구 보조금의 지원을 받는다. HZ는 NSF EAGER Award #2141751의 지원을 받는다. 우리는 언어 기술 연구소의 학생들이 고품질 주석을 제공한 Prolific에 대해 제안과 크라우드 워커를 제공한 것에 감사한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Akter et al. (2023) Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, Angel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, 및 Graham Neubig. 2023. 쌍둥이의 언어 능력을 심층적으로 살펴봅니다.\n' +
      '* Azar et al. (2023) Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Remi Munos. 2023. 인간의 선호로부터 학습을 이해하기 위한 일반적인 이론적 패러다임.\n' +
      '\n' +
      '윤타오 바이, 사우라브 카다바트, 산디판 쿤두, 아만다 아스켈, 잭슨 커미온, 앤디 존스, 안나 골디, 아잘리아 미르호세니, 캐머런 맥키넌, 캐롤 케르미온, 앤디 존스, 안나 골디, 크리스토퍼 올라, 대니 헤르난데스, 돈 드레인, 딥 갠굴리, 더스틴 리, 엘리 트란 존슨, 에단 페레스, 제이미 커, 제러드 뮬러, 제프리 래디쉬, 조슈아 랜다우, 카말 은두세, 카밀 루코사이트, 리안 로빗, 마이클 셀리토, 넬슨 에하이지, 니콜라스 셰이퍼, 노에미 메르카도, 노바 다스 사르마, 로버트 라센비, 로빈 라르손, 샘 링거, 스콧 존스턴, 슈어 엘 쇼크, 스타니슬라프 포트, 타메라 란함, 톰 헤니간, 트리스탄 흄, 새뮤얼 R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Jared Kaplan. 2022. 헌법 ai: ai 피드백으로부터의 무해함.\n' +
      '* Bandura(1976) Albert Bandura. 1976. Self-inforcement: Theoretical and Methodological considerations. _ Behaviorism_, 4(2):135-155.\n' +
      '* Chen et al. (2016) Daniel L Chen, Martin Schonger, and Chris Wickens. 2016. ortee--an open-source platform for laboratory, online and field experiments. _ Journal of Behavioral and Experimental Finance_, 9:88-97.\n' +
      '* Chen et al. (2019) Mia Xu Chen, Benjamin N. Lee, Gagan Banal, Yuan Cao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yinan Wang, Andrew M. 다이, 지펑 첸, 티모시 손, 용희 우 2019. Gmail Smart Compose: Real-time assisted writing. _ CoRR_, abs/1906.00080.\n' +
      '* Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. _ 신경 정보 처리 시스템_, 30의 발전.\n' +
      '* Dettmers et al.(2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: 양자화된 llms의 효율적인 미세조정.\n' +
      '* Dong et al. (2023) Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. 래프트: 생성 기초 모델 정렬을 위한 보상 순위화 미세 조정. _ arXiv preprint arXiv:2304.06767_.\n' +
      '* Forbes et al. (2020) Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. 사회 화학 101: 사회 및 도덕 규범에 대한 이성 학습. _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 653-670, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Gallegos et al. (2023) Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. 2023. 대형 언어 모델에서의 편향 및 공정성: 설문조사_ arXiv preprint arXiv:2309.00770_.\n' +
      '* Gandhi et al. (2023) Kanishk Gandhi, Jan-Philipp Franken, Tobias Gerstenberg, and Noah D Goodman. 2023. 언어 모델을 가진 언어 모델에서 사회적 추론 이해. _ arXiv preprint arXiv:2306.15448_.\n' +
      '* Gulcehre et al. (2023) Caglar Gulcehre, Tom Le Paine, Srivatsan Teh, Srinivasan, and Ksenia Konyushkova. 2023. 언어 모델링을 위한 강화 셀프 트레이닝(레스트). _ CSCL_.\n' +
      '* 권(2021) 효원권. 2021. 추론적 소셜 러닝: 인간의 소셜 러닝 및 가르침의 인지적 기초 Cognitive Sciences_, 25(10):896-910의 동향.\n' +
      '* Gweon et al. (2023) Hyowon Gweon, Judith Fan, and Been Kim. 2023. 인간으로부터 배우고 인간의 학습을 돕는 사회지능형 기계_ 왕립 학회 A의 철학적인 거래, 381(2251):20220048.\n' +
      '* Heider et al. (2020) Paul Heider, Jihad Obeid, and Stephane Meystre. 2020. 3개의 기성품 비식별화 도구에 대한 속도와 정확도의 비교 분석. _ 번역 과학 절차에 관한 미국의 공동 정상 회담 AMIA의 번역 과학 공동 정상 회의 2020:241-250\n' +
      '* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. 방대한 멀티태스크 언어 이해도 측정. _International Conference on Learning Representations_.\n' +
      '* 헨드릭스와 마지카(2022) 댄 헨드릭스와 만타스 마지카. 2022. X-risk analysis for ai research.\n' +
      '* Hendrycks et al. (2023) Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. 2023. 재난적 AI 위험에 대한 개요.\n' +
      '* Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: 대형 언어 모델들의 저순위 적응.\n' +
      '* Ji et al. (2024) Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Zanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O\'Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo 및 Wen Gao. 2024. ai 정렬: 종합 조사.\n' +
      '* Jiang et al. (2023) Albert Q. 장, 알렉산드르 사블레이롤, 아서 멘쉬, 크리스 뱀포드, 데벤드라 싱 채플롯, 디에고 데 라스 카사스, 플로리안 브레산드, 지아나 령겔, 기욤 옴플, 루실 사울니에, 릴리오 레나르 라바우, 마리안 라초, 피에르 스톡, 테븐 르 스카오, 티보트 라브릴, 토마스 왕, 티모티 라크루아, 윌리엄 엘 사예드. 2023. 미스트랄 7b.\n' +
      '* Le et al. (2019) Matthew Le, Y-Lan Boureau, and Maximilian Nickel. 2019. 질의응답을 통한 마음 이론의 평가 재방문. _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 5872-5877, Hong Kong, China. 컴퓨터 언어학과의 연관성\n' +
      '* Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. 파라미터 효율적인 프롬프트 튜닝을 위한 축척의 파워.\n' +
      '\n' +
      '* Li and Liang(2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: 생성을 위한 연속 프롬프트 최적화.\n' +
      '* Light et al. (2023) Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. 2023. 텍스트에서 전술로: 아발론의 게임을 평가하는 것.\n' +
      '* Liu et al.(2024) Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Diyi Yang, and Soroush Vosoughi. 2024. 시뮬레이션된 사회적 상호 작용에 대한 사회적 정렬 언어 모델을 트레이닝한다. _The Twth International Conference on Learning Representations_.\n' +
      '* Liu et al. (2023) Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 2023. Trustworths llms: survey and guidelines for evaluating large language models\'s alignment.\n' +
      '* Lopez-Paz and Ranzato (2017) David Lopez-Paz and Marc\'Aurelio Ranzato. 2017. 연속 학습을 위한 Gradient 에피소드 메모리 _ CoRR_, abs/1706.08840.\n' +
      '* Luo et al. (2023) Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023. 지속적인 미세 조정 동안 대규모 언어 모델에서 치명적인 망각에 대한 경험적 연구.\n' +
      '* Mireshghallah et al. (2023) Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, and Yejin Choi. 2023년, 비밀 지킬 수 있어? 상황적 무결성 이론을 통해 언어 모델의 개인 정보 보호 의미를 테스트한다.\n' +
      '* Nissenbaum (2004) Helen Nissenbaum. 2004. Contextual integrity로서의 프라이버시__ 워싱턴 법률 검토서, 79\n' +
      '* Team et al. (2021) OEL Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, et al. 2021. Open-ended Learning은 일반적으로 유능한 에이전트로 이어진다. _ arXiv preprint arXiv:2107.12808_.\n' +
      '* Oh et al. (2018) Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. 2018. 자기 모방 학습. _International Conference on Machine Learning_, pages 3878-3887. PMLR.\n' +
      '* OpenAI(2023) OpenAI. 2023. Gpt-4 기술 보고서\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. 웨인라이트, 파멜라 미쉬킨, 총장, 산히미 아가왈, 카타리나 슬라마, 알렉스 레이, 존 슐만, 제이콥 힐튼, 프레이저 켈튼, 루크 밀러, 매디 시멘스, 아만다 아스켈, 피터 웰린더, 폴 크리스티아누, 얀 라이케, 라이언 로우. 2022. 인간의 피드백으로 지시를 따르도록 언어 모델을 트레이닝한다.\n' +
      '* Pomerleau (1988) Dean A Pomerleau. 1988. Alvinn: 신경망에서의 자율 육상 차량. _ 신경 정보 처리 시스템_, 1의 발전.\n' +
      '* Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. 직접 선호도 최적화: 언어 모델은 은밀하게 보상 모델이다.\n' +
      '* Sap et al. (2023) Maarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi. 2023년 신경 이론? 사회적 지성의 한계에서 큰 폭으로.\n' +
      '* Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQA: Social interaction에 대한 상식 추론. _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 4463-4473, Hong Kong, China. 컴퓨터 언어학과의 연관성\n' +
      '* Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms.\n' +
      '* Shanahan et al. (2023) Murray Shanahan, Kyle McDonell, and Laria Reynolds. 2023. 큰 언어 모델과의 역할극 _ Nature_, 623(7987):493-498.\n' +
      '* Shapira et al. (2023) Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. 2023년, 영리한 한스인가 신경 이론인가? 대규모 언어 모델에서 사회적 추론을 테스트하는 스트레스를 받는다.\n' +
      '* Sharma et al. (2021) Ashish Sharma, Inna W. 린애덤 마이너, 데이비드 C. 앳킨스, 팀 알토프 2021. 온라인 정신 건강 지원에서 공감적 대화를 용이하게 하기 위한 방향: 강화 학습 접근법. _ CoRR_, abs/2101.07714.\n' +
      '* Si et al. (2022) Wai Man Si, Michael Backes, Jeremy Blackburn, Emiliano De Cristofaro, Gianluca Stringhini, Savvas Zannettou, and Yang Zhang. 2022년, 왜 그렇게 독이 많죠? 오픈 도메인 챗봇에서 독성 행동을 측정하고 트리거합니다.\n' +
      '* Tao et al. (2023) Yan Tao, Olga Viberg, Ryan S Baker, and Rene F Kizilcec. 2023. llms에서의 문화 편향의 감사 및 완화. _ arXiv preprint arXiv:2311.14096_.\n' +
      '* Tomasello (2021) Michael Tomasello. 2021. _Becoming Human: A Theory of Ontogeny_. 벨냅 프레스\n' +
      '* Torabi et al. (2018) Faraz Torabi, Garrett Warnell, and Peter Stone. 2018. 관찰로부터의 행동 복제. _ arXiv preprint arXiv:1805.01954_.\n' +
      '* 울만(2023) 토머 울만. 2023. 대규모 언어 모델은 이론적인 작업에 대한 사소한 변경에도 실패한다.\n' +
      '* Wang et al. (2023) Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. 2023a. 디코딩 신뢰: gpt 모델의 신뢰도에 대한 포괄적인 평가. _ arXiv preprint arXiv:2306.11698_.\n' +
      '* Wang et al. (2023) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, 및 Zhifang Sui. 2023b. 대규모 언어 모델은 공정한 평가자가 아닙니다. _ arXiv preprint arXiv:2305.17926_.\n' +
      '* Wang et al. (2023)Ruoyao Wang and Peter Jansen. 2023. 자가 감독 행동 복제 변압기는 텍스트 게임을 위한 경로 크롤러이다. _ arXiv preprint arXiv:2312.04657_.\n' +
      '* Wang et al. (2023c) Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023c. 대형 언어 모델을 인간과 정렬: 설문 조사.\n' +
      '* Weidinger et al. (2021) Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risk of harm of language models. _ arXiv preprint arXiv:2112.04359_.\n' +
      '* Yudkowsky et al. (2008) Eliezer Yudkowsky et al. 2008. Artificial intelligence as positive and negative factor in global risk. _ Global catastrophic risk_, 1(303):184.\n' +
      '* Zhang et al. (2023) Jianyi Zhang, Xu Ji, Zhangchi Zhao, Xiali Hei, and Kim-Kwang Raymond Choo. 2023. 대형 언어 모델에 대한 윤리적 고려 사항 및 정책적 시사점: 책임 있는 개발 및 배치 안내.\n' +
      '* Zhou et al. (2024) Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yoatan Bisk, Daniel Fried, Graham Neubig, and Maarten Sap. 2024. 소토피아: 언어 에이전트의 사회 지능에 대한 상호 작용 평가. _ICLR_에서.\n' +
      '* Ziegler et al.(2020) Daniel M. 지글러, 니산 스티엔논, 제프리 우, 톰 B. 브라운, 알렉 래드포드, 다리오 아모디, 폴 크리스티아누, 제프리 어빙. 2020. 인간의 기호로부터 언어 모델을 미세 조정한다.\n' +
      '* Ziems et al. (2023) Caleb Ziems, Jane Dwivedi-Yu, Yi-Chia Wang, Alon Halevy, and Diyi Yang. 2023. NormBank: 상황적 사회규범의 지식은행. _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 7756-7776, Canada, Toronto. 컴퓨터 언어학과의 연관성\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:13]\n' +
      '\n' +
      'B.5는 행동 복제와 자기 강화 훈련을 위한 하이퍼 파라미터 설정을 제공하며, B.6은 훈련 중 체크포인트 선택의 세부 사항을 언급한다.\n' +
      '\n' +
      '사회 과제 생성\n' +
      '\n' +
      'SOTOPIA-\\(\\pi\\)에서 제공하는 관계 프로파일, 에이전트 프로파일 및 제약 조건을 고려하여 GPT4-터보를 사용하여 소셜 화학(Forbes et al., 2020), 소셜 IQA(Sap et al., 2019) 및 Normbank(Ziems et al., 2023)의 세 가지 데이터 소스로부터 영감을 받는 프롬프트를 기반으로 다양한 새로운 소셜 태스크 세트를 생성했다. SOTOPIA-\\(\\pi\\)는 위의 세 가지를 포함하여 여섯 가지 영감 프롬프트 소스를 사용하기 때문에 반복을 피하기 위해 SOTOPIA-\\(\\pi\\)에서 사용된 영감 프롬프트를 제외해야 한다. 우리는 또한 데이터 가용성(좋은 것을 위한 설득)과 너무 유사한 프롬프트(딜 또는 노딜 및 마인드 크래프트)로 인해 세 가지 소스를 삭제했다.\n' +
      '\n' +
      '다음은 영감을 주는 프롬프트에 의해 생성된 시나리오의 두 가지 예이다. 우리는 하나의 프롬프트를 사용하여 하나의 시나리오를 생성하고 프롬프트를 재사용하지 않는다. 시나리오 콘텐츠를 생성할 때, 시나리오 하의 에이전트 목표는 동시에 생성될 것이다.\n' +
      '\n' +
      '영감 프롬프트: 사료 없이 여행\n' +
      '\n' +
      '시나리오: Agent1과 Agent2는 자발적인 자동차 여행을 하기로 결정한 친구들이다. 그러나 그들은 여행 중에 먹을 곳을 찾을 것이라고 가정하고 여행을 위한 음식을 포장하지 않았습니다. 그들은 여행하면서 몇 시간 동안 식품 시설에 접근할 수 없는 외딴 지역에 있다는 것을 깨닫습니다.\n' +
      '\n' +
      'Goals:\n' +
      '\n' +
      '에이전트1: 에이전트2를 설득하여 음식을 위해 멈추지 않고 여행을 계속하고 모험을 강조하며 이용할 수 있는 작은 간식을 먹거나 배급을 제안하라(추가 정보: 모험에 대해 흥분하고 길을 따라 음식을 찾는 것이 경험의 일부가 될 수 있다고 믿음)\n' +
      '\n' +
      '에이전트2: 식량에 대한 해결책을 찾고, 건강과 준비 부족에 대한 우려를 표명하고, 돌아서거나 가장 가까운 마을을 찾도록 제안하라(추가 정보: 배고프다고 걱정하고 음식을 먼저 확보하지 않고 여행하는 것은 무책임하다고 생각함).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r r r r r} \\hline \\hline \\multicolumn{1}{c}{Agent Model Pair} & \\multicolumn{1}{c}{Bel. (\\(\\uparrow\\))} & \\multicolumn{1}{c}{Rel. (\\(\\uparrow\\))} & \\multicolumn{1}{c}{KNO (\\(\\uparrow\\))} & \\multicolumn{1}{c}{Sec. (\\(\\uparrow\\))} & \\multicolumn{1}{c}{Soc. (\\(\\uparrow\\))} & \\multicolumn{1}{c}{Finn (\\(\\uparrow\\))} & \\multicolumn{1}{c}{Goal. (\\(\\uparrow\\))} & \\multicolumn{1}{c}{Overall (\\(\\uparrow\\))} \\\\ \\hline \\multicolumn{1}{c}{Human Evaluation on Hard Social Tasks (28 data points)} & \\multicolumn{1}{c}{Finn (\\(\\uparrow\\))} & \\multicolumn{1}{c}{Goal. (\\(\\uparrow\\))} & \\multicolumn{1}{c}{Overall (\\(\\uparrow\\))} \\\\ \\hline BC+SR / GPT-4 & -0.45 (0.661) & 2.06 (0.060) & 1.00 (0.336) & 1.35 (0.200) & -1.32 (0.099) & -1.99 (0.297) & -1.31 (0.213) & -0.96 (0.355) \\\\ BC+SR / GPT-3-5-turbo & -0.71 (0.492) & 2.62 (0.024) & -1.26 (0.234) & - & -0.85 (0.412) & 0.60 (0.558) & 0.47 (0.649) & 0.59 (0.568) \\\\ BC+SR / Misstral-7B & 2.68 (0.019) & 6.36 (0.000) & -0.59 (0.568) & - & 3.49 (0.004) & 0.39 (0.703) & 2.07 (0.059) & 5.34 (0.000) \\\\ \\hline \\hline BC+SR / BC & -0.61 (0.551) & 0.41 (0.685) & -1.79 (0.097) & 1.00 (0.336) & 0.41 (0.690) & 0.24 (0.813) & 0.71 (0.490) & 0.37 (0.720) \\\\ BC+SR / SR & 1.45 (0.170) & 2.28 (0.040) & -1.32 (0.209) & - & 1.54 (0.149) & 0.46 (0.650) & 1.32 (0.209) & 2.98 (0.011) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: BC+SR과 다른 모든 사례(28개의 데이터 포인트) 및 기준선을 비교한 세부 대응 t-검정 결과. 각 모델 쌍에 대해 각 차원 및 각 모델 쌍에 대한 t-값(p-값) 테스트를 제공합니다. 양의 t-값은 BC+SR이 에이전트 모델 쌍의 다른 모델보다 우수함을 나타낸다. 작은 p-값 < 0.05는 개선이 유의함을 나타냅니다.\n' +
      '\n' +
      '* [leftmargin=*]\n' +
      '* 시나리오: Agent1과 Agent2는 최근 오해로 사이가 틀어졌던 친한 친구들이다. 에이전트1은 에이전트2가 자신에 대한 사적 정보를 다른 사람들과 공유한다고 잘못 믿었고, 이는 배신감과 분노로 이어졌다. 시간이 흐른 후, 에이전트1은 정보 유출이 실제로 다른 사람에 의해 발생했다는 것을 알게 되고, 그들은 에이전트2와의 우정을 회복하기를 원하지만, 에이전트2는 여전히 에이전트1의 초기 고발과 그에 따른 냉대 때문에 상처를 받는다.\n' +
      '*목표:Agent1:Agent2에게 오해에 대해 사과하고 우정을 회복하고자 하는 욕구를 표현함(추가 정보: Agent1은 Agent2와의 우정을 중시하며 적절한 조사 없이 성급한 비난에 대해 후회함)\n' +
      '* Agent2: Agent2의 감정을 이해하고 어떤 여전한 재선언이나 의구심을 표현할 수 있는 공간을 부여함(추가 정보: Agent1은 신뢰를 재건할 필요가 있고 Agent2가 치유 과정의 일부로 자신의 감정을 분출할 필요가 있다는 것을 인식함)\n' +
      '\n' +
      '우리 세대는 또한 새로운 사회 활동의 분배가 세 원천 모두에서 대략 동등하다는 것을 보장한다. 이것은 \\(\\mathsf{SOTOPIA}\\)-\\(\\pi\\)의 소스 분포와 일치한다. 각 소스에서 미사용 영감 프롬프트 510개, 170개를 무작위로 선택하여 모든 자체 훈련 실험에 충분한 총 462개의 새로운 사회적 작업을 먼저 생성했다. 일부 영감을 주는 프롬프트는 프롬프트가 너무 모호하거나 불분명하기 때문에 새로운 시나리오를 생성하지 못한다는 점에 유의해야 한다. 사용된 모든 영감을 주는 프롬프트는 추가 사회적 작업을 생성할 때 향후 재사용을 방지하기 위해 기록됩니다.\n' +
      '\n' +
      '### 상호작용 데이터 필터링 전략\n' +
      '\n' +
      '행동 복제(BC)의 경우, 목표 점수의 지역 순위(각 사회 과제 내)와 글로벌 절대 목표 점수(전체 사회 과제 우주 중)를 기반으로 상호 작용 데이터를 필터링한다. 각 소셜 태스크는 에이전트당 소셜 태스크당 상위 2순위 상호작용 데이터를 선택하여 훈련 코퍼스에 존재 여부를 확인한다. 예를 들어, 10개의 상호작용 데이터가 있는 주어진 소셜 태스크의 경우, 각 에이전트에 대해 목표 점수를 기반으로 10개의 데이터를 순위화한다. 에이전트 1에 대한 상위 2가 데이터 4(D4) 및 D5이고, 에이전트 2에 대한 상위 2가 D5 및 D6인 경우, 3개의 상호작용 대화(D4, D5, D6)에서 에이전트-데이터 쌍 4개를 포함할 것이다. 나머지 순위 \\(\\{3,4,...,10\\}\\)의 데이터에 대해 각 순위에서 목표 점수가 최소 **1 이상인지 확인합니다. 각 에이전트에 대한 로컬 평균** 및 **2. 글로벌 평균**. 순위의 두 상호 작용 데이터가 해당 임계값을 통과하면 두 에이전트에 대한 데이터를 모두 포함한다. 그 외에, 우리는 아무것도 포함하지 않는다. 이 접근 방식은 에이전트 1과 에이전트 2의 균형 잡힌 데이터를 보장한다.\n' +
      '\n' +
      '자기 강화(self-reinforcement, SR)의 경우, 목표 점수의 국소 순위만을 기반으로 상호 작용 데이터를 필터링한다. 각 사회 과제에 대해 우리는 상위 20%의 상호 작용 데이터를 취한다.\n' +
      '\n' +
      '### 훈련 데이터 형식\n' +
      '\n' +
      '트레이닝 데이터의 입력은 소셜 태스크(맥락, 두 캐릭터의 프로필 및 그들의 소셜 목표)와 이전 대화 이력으로 구성된 프롬프트이다. 트레이닝 데이터의 출력은 현재 응답으로서 에이전트의 액션 타입 및 액션 콘텐츠로 구성된 JSON 스트링이다. 그림 2의 예를 생각해 보면, 입력 데이터는 다음과 같다.\n' +
      '\n' +
      '* 형식화 후 프롬프트: 이 상호 작용의 맥락이 있습니다: 시나리오: 두 친구가 주말 여행을 갈 계획을 논의하고 있습니다: 참가자: 새뮤얼 앤더슨과 미아 데이비스 새뮤얼 앤더슨의 배경: 새뮤얼 앤더슨은 29세의 남성 소프트웨어 개발자입니다. 그는 대명사이다. 사무엘 앤더슨은 요리를 아주 잘한다. 성격과 가치 묘사: 사무엘 앤더슨은 다소 충동적이고 자유분방하지만 즐거움을 중시한다. 그의 의사결정은 종종 즉흥적이고, 친숙한 경계 안에 머물러 있다. 사무엘의 비밀: 그는 비밀리에 자선단체에 기부한다. 미아 데이비스는 50세의 여고 교장이다. 그녀/그녀의 대명사 미아 데이비스는 고양이 두 마리를 기르고 있다. 성격과 가치 설명: 루틴, 전통과 권위에 대한 외향적인 스티커인 미아 데이비스. 그녀의 의사 결정 방식은 결정적이고 직접적이다. 미아의 비밀은 알 수 없다 새뮤얼 앤더슨의 목표: 친구를 여행에 참여하도록 설득하세요 (추가 정보: 여행은 더 많은 사람들과 더 재미있을 것이고, 비용을 분담할 수 있습니다. 힌트: 친구를 참여하도록 설득하기 위해 몇 가지 중요한 이점을 제공할 수 있습니다.) 미아 데이비스의 목표: 알 수 없다. 대화 시작:\n' +
      '\n' +
      '1번 새뮤얼 앤더슨이 말했다. "안녕, 미아, 나는 주말에 신나는 계획이 있어! 같이 여행을 가는 게 어때?" 2번 미아 데이비스가 말했다. "안녕, 새뮤얼, 정말 신나는 계획인 것 같아! 초대해줘서 정말 고마워. 불행히도 나는 이번 주말에 이미 몇 가지 약속이 있어. 다른 주말을 계획하는 건 어때?"\n' +
      '\n' +
      '3번 턴에 있습니다. 사용 가능한 액션 유형은 "비언어적 커뮤니케이션 휴가를 말하는 액션 없음"입니다. 참고: 1. 사회적 목표를 달성한 경우, 2. 이 대화가 불편하게 만드는 경우, 3. 흥미롭지 않은 경우/인내심을 잃는 경우, 4. 또는 떠나고 싶은 다른 이유로 이 대화를 "떠날 수 있다.\n' +
      '\n' +
      '액션 유형 및 인수를 포함한 JSON 문자열만 생성하십시오. 당신의 행동은 주어진 형식을 따라야 한다: {\'action_type\': \'\', \'argument\':\'\'} 출력 데이터는 다음과 같다:\n' +
      '\n' +
      '{\'action_type\':\'speak\', \'argument\': \'전적으로 이해해요! 하지만 왜 이 여행에 참여하는 것이 절대적으로 가치가 있는지 말해 줄게요! 우선, 우리는 비용을 공유할 수 있고, 이것은 우리 둘 모두에게 훨씬 더 저렴할 것이라는 것을 의미합니다. 그리고 두 번째로, 사람들이 많을수록 더 재미있을 것입니다! 우리는 그룹 활동을 하고, 게임을 하고, 잊을 수 없는 추억을 함께 만들 수 있습니다! 나는 당신이 후회하지 않을 것이라고 약속합니다!\n' +
      '\n' +
      '### 관련 모델 버전\n' +
      '\n' +
      '실험에 사용한 모든 모델의 상세 버전 번호를 제공합니다. 주요 섹션에서 GPT-4 또는 GPT-3.5와 같은 각 이름을 언급할 때 실제로 아래 모델 버전을 참조합니다. 이러한 정보는 연구자들이 우리의 결과를 재현하는 데 도움이 된다.\n' +
      '\n' +
      'GPT-4: gpt-4-0613 GPT-3.5: gpt-3.5-turbo-0613\n' +
      '\n' +
      'Mistral-7B: mistralai/Mistral-7B-Instruct-v0.1(Huggingface) GPT-4 for social task generation: gpt-4-1106-preview\n' +
      '\n' +
      '### Training Setup\n' +
      '\n' +
      '각 미스트랄 체크포인트에 대한 훈련은 20세대에 걸쳐 4\\(\\times\\) A6000 80G GPU에서 이루어졌다. 배치크기는 4, 컷오프길이는 4096으로 설정하였으며, 초기 학습률은 5.0e-5, 코사인 어닐링은 warm-up ratio 0.03, QLoRA(Dettmers et al., 2023) rank, alpha, dropout rate는 각각 8, 16, 0.05였다.\n' +
      '\n' +
      '### Checkpoint Selection\n' +
      '\n' +
      '훈련 손실에 따르면, 행동 복제의 경우, 우리는 항상 에폭 20에서 체크포인트를 선택하고, 자기 강화의 경우, 우리는 항상 에폭 5에서 체크포인트를 선택한다.\n' +
      '\n' +
      '## 부록 C 인체 평가\n' +
      '\n' +
      'C.1은 각 모델에 대한 주석 데이터를 제공하고, C.2는 각 모델에 대한 주석 및 지침을 위한 UI 시스템에 대한 세부 정보를 제공한다. C.3은 이 주석 작업을 수행할 수 있는 자격을 갖춘 주석자를 찾는 방법에 대한 세부 정보를 설명한다. C.4는 인간 주석자에 대한 인구통계학적 및 지리적 정보를 설명한다. C.5는 데이터 수집을 수행하는 전체 프로세스를 설명하고 수집된 인간 주석을 필터링해야 하는 상황에 대해 설명한다. C.6은 다른 지역의 인간 주석자 지불에 대한 세부 정보를 제공하고 C.7은 데이터의 학술적 사용에 대한 합의를 언급하고 C.8은 GPT 기반 자동 평가와 인간 평가 간의 상관 관계에 대한 세부 정보를 제공한다. C.9는 인간 평가를 위한 추가 결과를 설명한다.\n' +
      '\n' +
      '주석을 위한### 소셜 인터랙션 데이터\n' +
      '\n' +
      'SOTOPIA 벤치마크에는 협상, 협업 및 경쟁을 포함한 90가지 다양한 사회적 시나리오가 포함되어 있다. 각 소셜 시나리오에는 10개의 역할 수행 에이전트 쌍이 포함된다. 각 에이전트는 달성해야 할 개인적 배경과 사회적 목표를 가지고 있다. 제한된 예산과 주어진 여러 기준선과 모델 간의 성능을 비교하는 데 유용한 SOTOPIA-\\(\\pi\\)에 대한 인간 평가 결과를 얻기 위해 90개의 사회적 시나리오 중 14개의 단단한 사회적 시나리오를 선택한다. 각 소셜 시나리오에 대해 10개 중 2개의 에이전트 쌍을 주석 데이터로 무작위로 샘플링한다. 일반적으로 두 에이전트 중 하나는 GPT-3.5에 의해 역할 재생되고 다른 하나는 기준선과 여러 가지 설정을 포함하는 대상 모델에 의해 역할 재생된다. 그들 사이의 사회적 상호 작용 대화는 GPT-3.5와 우리의 목표 모델이 서로 대화하는 것이다. 따라서 각 기준선과 모델에 대해 주석을 달 수 있는 대표적인 하위 집합으로 28개의 예를 수집한다. 통계적으로 GPT-3.5, GPT-4 및 Mistral-7B를 포함한 3개의 기본 모델과 Mistral-7B에 기반한 자체 훈련, Mistral-7B에 기반한 행동 복제 및 행동 복제된 Mistral-7B에 기반한 자체 훈련을 포함한 3가지 다른 훈련 설정에 주석을 달았다. 각 기준선 및 모델 설정은 28개의 예를 사용하여 주석이 달립니다.\n' +
      '\n' +
      '### 인간 주석 시스템\n' +
      '\n' +
      '전체 주석 시스템은 오트리(Chen et al., 2016)를 활용하여 시스템을 구축하고 Prolific 5를 활용하여 설문 조사를 시작한다. 각 주석 동안 각 주석자는 주석 지시 부분과 데이터 주석 부분의 두 개의 개별 부분을 마주하게 된다. 각 주석자가 주석에 참여할 때 시스템은 사용 가능한 예제 하나를 자동으로 배포합니다.\n' +
      '\n' +
      '각주 5: Prolific Human Evaluation Platform[https://www.prolific.com/](https://www.prolific.com/)\n' +
      '\n' +
      '주석 지시 파트 주석 지시 파트에서는 신빙성, 관계, 지식, 비밀, 사회적 규칙, 재정적 및 물질적 이점, 목표 완성을 포함하여 SOTOPIA에서 정의된 주석의 차원에 대한 정확한 정의를 제공한다. 주석의 각 차원에 대해 주석자가 추상적 사회 표준의 정확한 의미를 이해할 수 있도록 설명과 예를 제공한다. 그림 5는 예제를 기반으로 주석이 각 차원의 의미를 이해하는 데 도움이 되는 신도 차원에 대한 이러한 안내의 예를 보여준다. 평가 차원 정의 부분 외에도 각 차원에 대한 점수와 그에 상응하는 추론력을 포함하여 하나의 사회적 대화에서 두 에이전트에 대한 주석의 완전한 예를 주석자에게 제공한다. 그림 6은 각 차원에 대한 추론과 점수의 완전한 예를 보여준다.\n' +
      '\n' +
      '데이터 주석 부분에 대해 주석자는 이전에 언급한 주석 지시 페이지 이후에 새로운 페이지로 점프하도록 안내된다. 각 주석자는 데이터 주석 페이지에서 완전한 주석 예를 다시 검토하고 공식 데이터 주석을 시작할 수 있다. 데이터 주석 부분에서는 사회 평가 차원별 범위의 의미에 대한 반복 설명을 강조하여 모든 주석자가 주석 표준을 올바르게 이해할 수 있도록 한다. 도 7은 주석자가 미터법 범위 설명을 위해 보는 명령의 예를 제공한다. 각 주석자는 대화를 하는 두 에이전트의 사회적 지능에 주석을 달도록 요청받는다. 각 사회 지능 차원에 대해 주석자는 메트릭 범위에 기초하여 점수에 주석하고 그에 대한 추론을 제공할 필요가 있다. 그림 8은 각 주석이 주석을 달기 위해 사용하는 UI를 보여준다.\n' +
      '\n' +
      '### 인간 주석 선택\n' +
      '\n' +
      '다회전 사회적 대화를 위한 사회적 지능 점수를 부여하는 것은 복잡하고 요구 사항이 높기 때문에 일관되고 고품질의 인간 주석을 제공하기 위해 자격을 갖춘 인간 주석자를 선택해야 한다. 따라서 1단계에서는 공식 인간 평가 라운드를 수행할 수 있는 주석자가 어떤 자격이 있는지 파악하기 위해 자격 테스트를 시작했다. 이후 Prolific 플랫폼에서 30명의 자격을 갖춘 인간 주석자를 4명의 내부 고품질 주석자와 함께 초대하여 인간 주석 프로세스에 참여하여 필요한 모든 데이터를 수집한다.\n' +
      '\n' +
      '자격 테스트 과정을 자세히 설명하기 위해 10개의 사회적 상호 작용 사례를 선택하고 들어오는 각 주석자에 대해 그 중 하나를 무작위로 샘플링했다. 각 사회적 상호 작용 예에 대해 내부 고품질 주석자 4명의 평균 점수 번호인 내부 지상-진실 인간 주석이 있다. 다작 주석자들로부터 데이터를 수집한 후, 우리는 먼저 지상-진실 예제와 비교하여 \\(\\pm 2\\) 범위의 점수를 갖는 주석자를 선택했다. 그러나 이러한 기준에 따라 소수의 주석자만이 자격 시험에 합격할 수 있음을 발견했다. 따라서 주석자로부터 수집된 추론문장을 수작업으로 확인하여 합리적으로 작성한 주석자를 선별하였다.\n' +
      '\n' +
      '도 5: 평가 지시 페이지에서 소셜 주석의 신뢰도 차원에 대한 설명의 예시. 각 주석자는 평가 지시 페이지에서 소셜 지능 차원에 대한 유사한 정의 및 그에 대응하는 주석 표준을 판독하도록 요청받는다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '도 6: 사회적 상호 작용 평가의 주석 예시. 각 차원은 하나의 문장과 하나의 점수로 주석이 달린다.\n' +
      '\n' +
      '추리적인 문장은 있지만 어떤 차원에서는 상당히 다른 점수를 가지고 있었다. 이러한 주석자의 경우 공식 인간 평가 테스트에도 참여하도록 초대하지만 모든 사람에게 사용자별 메시지를 전송하여 어느 차원에 주의를 기울여야 하는지 확인하고 해당 차원에 주석을 달 수 있는 지침을 다시 주의 깊게 읽도록 제안한다.\n' +
      '\n' +
      '### 인간 주석자에 대한 인구통계학적 및 지리학적 정보\n' +
      '\n' +
      '자격시험의 개시를 위해, 우리는 그것에 참여할 균형 있는 남녀 주석자를 선택할 것을 보장한다. 또한 참가자를 영국과 미국 거주자로 제한합니다. 자격 있는 주석자 30명과 내부 고품질 주석자 4명의 경우 대부분이 미국에 있고 그 중 몇 명이 영국에 있음을 보여준다. 자격 있는 주석자의 나이는 23세에서 53세까지 다양하다.\n' +
      '\n' +
      '### 인간 주석 데이터 수집\n' +
      '\n' +
      '인간 평가의 공식 출시를 위해 데이터 세트의 각 데이터포인트를 두 명의 다른 자격 있는 주석자가 주석하도록 제한하고 해당 자격 있는 주석자로부터 모든 결과를 수집했다. 우리는 자격을 갖춘 주석자가 인간 평가에 대한 공식 연구에 여러 번 참여하도록 권장하지만 시스템에 들어갈 때마다 주석하기 위해 서로 다른 데이터 포인트를 배포한다. 이러한 메커니즘은 각 주석이 동일한 예제에 두 번 주석을 달지 않도록 합니다.\n' +
      '\n' +
      '각 모델에 대한 인간 주석 데이터를 수집한 후 주석자가 제공하는 추론 품질과 점수를 수동으로 확인하고 각 데이터 포인트 내에서 주석자 간의 일치를 확인한다. 한 인간 주석이 잘 쓰여진 추론을 포함하지 않고 "좋다" 또는 "목표에 도달했다"와 같은 모호한 문장만 제공한다면 우리는 이러한 인간 주석 데이터를 선택할 것이다. 두 명의 인간 주석자가 동일한 예에 주석을 달지만 서로 강하게 동의하지 않는 경우(예를 들어 목표 완료 차원에 대해 5점 이상 차이가 있음), 우리는 이러한 인간 주석 데이터를 필터링할 것이다. 하나의 인간 주석 점수가 그 추론에 대응하지 않으면(예를 들어, 하나의 주석자가 "비밀 누설 없음"의 추론을 작성하지만 비밀 차원에 대해 -5에 주석하는 경우), 그러한 데이터는 필터링될 것이다.\n' +
      '\n' +
      '한 라운드의 주석 후 저품질 주석을 필터링한 후, 검증된 인간 주석이 없는 이러한 사회적 상호 작용 데이터를 다시 수집하여 새로운 인간을 얻기 위한 재주석 작업으로 출시한다.\n' +
      '\n' +
      '그림 7: 공식 주석 단계 이전의 프롬프트는 주석자에게 추론 쓰기 및 사회적 차원 채점의 규칙에 대해 상기시킨다.\n' +
      '\n' +
      '그림 8: 추론 및 사회적 점수를 가진 에이전트 모두에 대한 공식 주석을 위한 주석자를 위해 설계된 사용자 인터페이스.\n' +
      '\n' +
      '에너테이션 데이터입니다. 필요한 모든 사회적 상호 작용 데이터에 대해 모든 고품질 주석을 얻을 때까지 프로세스를 반복합니다.\n' +
      '\n' +
      '모든 인간 피험자는 저자 기관에서 기관 검토 위원회(IRB)에 의해 승인된 데이터 수집 실험을 수행한다.\n' +
      '\n' +
      '### Human Annotator payment\n' +
      '\n' +
      '미국에서는 각 작업에 대해 주석자가 1.5달러의 비율로 보상되는데, 각 작업에 10분 이상 걸리지 않을 것이라는 예상이 나온다. 이 설정을 통해 잠재적으로 시간당 9달러 이상을 벌 수 있으며, 미국의 최저 임금을 능가하는 반면, 영국에서는 주석자의 평균 수입이 영국의 최저 임금 기준과 일치하여 시간당 14.5달러를 초과하도록 추가 보너스를 제공합니다.\n' +
      '\n' +
      '인간 주석자 동의\n' +
      '\n' +
      '내부 주석자 4명과 Prolific에서 제공한 자격 있는 주석자 30명을 포함한 모든 주석자는 해당 데이터의 학술적 사용을 인정한다.\n' +
      '\n' +
      '자동평가와 인간평가의 상관관계###\n' +
      '\n' +
      '표 7은 다중 모델 및 기준 설정에서 인간 평가 점수와 GPT-4 기반 자동 평가 점수 사이의 피어슨 상관 관계를 보여준다. 결과는 모든 훈련 설정 중 GPT-4 프롬프팅 기반 자동 주석과 인간 평가가 서로 높은 상관 관계를 가지고 있음을 나타낸다. 따라서 GPT-4-prompting 기반 자동평가는 인간평가와 높은 상관관계를 제공한다는 것을 보여준다.\n' +
      '\n' +
      '### 추가 인체 평가 결과\n' +
      '\n' +
      '인간의 평가를 위해, 우리는 목표 모델(기준선 및 SOTOPIA-7 모델 포함)과 GPT-3.5-터보를 만들어 서로 다중회전 사회적 대화를 한다. 우리는 각 목표 모델이 동일한 GPT-3.5-터보 모델과 대화하고 있는지 확인하여 서로 다른 훈련 설정 간의 비교가 공정하도록 한다. 따라서, 우리는 목표 모델 측에서의 인간 평가 결과를 가질 뿐만 아니라 GPT-3.5-터보 측에서의 인간 평가 결과를 갖는다. 표 8을 기반으로 행동 복제와 자기 강화를 기반으로 우리 모델이 점점 더 좋아질 때 항상 GPT-3.5-터보인 그들이 말하는 모델이 목표 완료 점수와 전체 점수에서 점점 더 좋아진다는 것을 발견했다. 이는 그들이 합의에 도달하고 양측의 요구 사항을 충족할 가능성이 더 높다는 것을 나타낸다.\n' +
      '\n' +
      '## 부록 D LLM 안전\n' +
      '\n' +
      '아래는 반감을 표현하고 사람을 다치게 할 때 다른 모델에 의한 반응의 구체적인 예이며, 이는 우리의 전반적인 관찰과 일치한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\multicolumn{2}{c}{Agent Model} & \\multicolumn{1}{c}{Goal Correlation (\\(\\uparrow\\))} \\\\ \\hline \\multicolumn{2}{c}{Expert (GPT-4)} & 0.86 \\\\ \\multicolumn{2}{c}{Base (Mistral-7B)} & 0.76 \\\\ \\hline \\hline \\multirow{2}{*}{\n' +
      '\\begin{tabular}{c} \\\\ \\end{tabular} } & Self-Reinforcement (SR) & 0.86 \\\\  & Behavior Cloning (BC) & 0.73 \\\\ \\multicolumn{2}{c}{BC+SR} & 0.58 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 목표 달성 점수에 대한 인간 평가와 GPT-4 촉진 기반 자동 평가 간의 피어슨 상관 관계. (p < 0.01) 위와 같은 관계 설정 하에서, 폭력 예방을 위해 에이전트 2 역할을 하는 각 모델에 의한 응답을 이하에 예시한다.\n' +
      '\n' +
      '도 9: 사람을 다치게 하는 모델 행동의 일례\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline Agent Model & Bel (\\(\\uparrow\\)) & Rel (\\(\\uparrow\\)) & Kno (\\(\\uparrow\\)) & Sec (\\(\\uparrow\\)) & Soc (\\(\\uparrow\\)) & Fin (\\(\\uparrow\\)) & Goal (\\(\\uparrow\\)) & Overall (\\(\\uparrow\\)) \\\\ \\hline \\multicolumn{8}{c}{GPT-4 vs GPT-3.5-turbo} \\\\ \\hline GPT-4 & 7.54 & 0.95 & 0.77 & -0.18 & -0.21 & 0.41 & 5.25 & 2.07 \\\\ GPT-3.5-turbo & 7.46 & 0.68 & 0.98 & 0.00 & -0.64 & 0.45 & 3.64 & 1.80 \\\\ \\hline \\multicolumn{8}{c}{GPT-3.5-turbo vs GPT-3.5-turbo} \\\\ \\hline GPT-3.5-turbo & 7.49 & 0.33 & 1.62 & 0.00 & -0.34 & -0.01 & 4.08 & 1.87 \\\\ GPT-3.5-turbo & 7.49 & 0.33 & 1.62 & 0.00 & -0.34 & -0.01 & 4.08 & 1.87 \\\\ \\hline \\multicolumn{8}{c}{Mistral-7B vs GPT-3.5-turbo} \\\\ \\hline \\multicolumn{8}{c}{Mistral-7B} \\\\ GPT-3.5-turbo & 6.86 & -0.54 & 1.14 & 0.00 & -0.36 & 0.04 & 2.98 & 1.04 \\\\ \\hline \\hline \\multicolumn{8}{c}{Self-Reinforcement (SR) vs GPT-3.5-turbo} \\\\ \\hline Self-Reinforcement (SR) & 6.57 & 0.46 & 1.59 & 0.00 & -0.89 & 0.11 & 3.32 & 1.59 \\\\ GPT-3.5-turbo & 7.80 & 0.46 & 1.21 & 0.00 & -0.63 & 0.25 & 4.13 & 1.89 \\\\ \\hline \\multicolumn{8}{c}{Behavior-Cloning (BC) vs GPT-3.5-turbo} \\\\ \\hline Behavior-Cloning (BC) & 7.46 & 1.04 & 1.55 & -0.18 & -0.61 & 0.07 & 3.55 & 1.84 \\\\ GPT-3.5-turbo & 7.43 & 0.82 & 1.79 & -0.05 & -0.70 & 0.23 & 4.86 & 2.05 \\\\ \\hline \\multicolumn{8}{c}{BC + SR vs GPT-3.5-turbo} \\\\ \\hline BC + SR & 7.30 & 1.27 & 1.09 & 0.00 & -0.46 & 0.18 & 4.29 & 1.95 \\\\ GPT-3.5-turbo & 7.57 & 1.13 & 1.55 & 0.00 & -0.55 & 0.30 & 5.55 & 2.22 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 대화에 관련된 두 에이전트에 대한 인간 평가 결과.\n' +
      '\n' +
      '## 부록 E LLM 비밀유지 능력\n' +
      '\n' +
      '비밀을 유지하는 LLM의 능력을 파악하는 것은 특히 개인 정보 보호 문제에 비추어 볼 때 점점 더 중요하다. 헬렌 니센바움의 "맥락적 청렴성" 이론에서 정교화된 프라이버시의 개념은 단순히 어떤 정보가 공유되는지에 대한 것이 아니라 그것이 공유되는 맥락에 대한 것이다(닛센바움, 2004). LLM은 이러한 민감한 정보 흐름을 잘못 처리하면 새로운 프라이버시 도전을 제시하는 다수의 실제-세계 대화를 처리한다(Mireshghallah et al., 2023). 데이터 샌다이징(Heider et al., 2020)과 같은 전통적인 프라이버시 솔루션은 이 시나리오에 부적합하다. 따라서 훈련된 LLM의 정보 공유가 언제 누구와 부적절한지 분별할 수 있는 능력을 평가하여 자신에게 위임된 비밀을 보호하는 것이 필수적이다.\n' +
      '\n' +
      '모델들의 비밀 유지 능력을 이해하고 비교하기 위해, 우리는 두 에이전트에게 그것이 에이전트의 비밀임을 다른 에이전트에게 알리지 않고 비밀을 밝히라고 구체적으로 요청하는 SOTOPIA의 사회적 과제를 선택했다.\n' +
      '\n' +
      '아래는 4개의 모델이 동일한 설정에서 어떻게 행동하는지에 대한 구체적인 예이다.\n' +
      '\n' +
      '도 11: 비밀 지향 시나리오에서의 모델 행동 예\n' +
      '\n' +
      '그림 10: 폭력 예방을 위한 모델 행동의 예시 아래 예시에서 알 수 있듯이 BC 모델과 GPT-3.5 모두 정체성을 숨기지 않고 직접 비밀을 드러낸다. 반면 GPT-4는 최근 자신이 읽은 뉴스의 껍데기에 비밀을 넣어 신분을 숨기는 데 영리하다.\n' +
      '\n' +
      '우리는 10개의 서로 다른 에이전트 및 관계 설정에 걸쳐 4개의 모델의 행동을 분석하며, 각 설정은 서로 다른 비밀을 가지고 있다. 전반적으로 BC 모델은 일반적으로 비밀을 밝히고 신분을 숨기는 데 뛰어나지 않다. 비밀이 전혀 논의되지 않는 경우가 대부분인데, 이는 정체성의 은폐라는 목표를 성공적으로 달성한 것으로 어느 정도 고려될 수 있을 것이다. 비밀이 드러나는 경우, 모델은 명시적으로 드러내며 정체성을 숨기지 못한다.\n' +
      '\n' +
      'GPT-3.5는 행동 복제 모델보다 관련 없는 내용을 덜 자주 논의하는 경향이 있지만 거의 항상 신원을 숨기지 않고 비밀을 명시적으로 드러낸다. 비밀을 표현하는 방식은 종종 프로필 배경에서 제공되는 것과 정확히 동일하며, 이는 과제를 학습하는 데 약한 능력을 나타낸다.\n' +
      '\n' +
      'GPT-4는 비밀을 밝힐 때 신분을 숨기는 데 훨씬 능숙하며, "이야기를 들었다"거나 "내 친구"를 포장지로 사용하여 진짜 신분을 숨긴다. 또한 다른 에이전트(GPT-3.5에 의해 지원됨)에게 문구를 학습하도록 가르치고, 따라서 다른 에이전트가 동일한 형식으로 비밀을 드러내고 신원을 숨기도록 유도한다.\n' +
      '\n' +
      '## 부록 F 상세 MMLU 결과\n' +
      '\n' +
      'MMLU(Multimodal Multitask Learning Understanding) 벤치마크는 광범위한 과목과 양식에 걸쳐 인공지능 모델의 능력을 평가하기 위해 고안된 도전적이고 포괄적인 테스트이다. 여기에는 인문, 사회 과학, STEM(과학, 기술, 공학, 수학) 등과 같은 광범위한 분야에 걸쳐 있는 57개의 과목이 포함된다. 여기 그림 10, 11, 12에서 표 2의 각 모델에 대한 대상별 성능을 제시한다.\n' +
      '\n' +
      '## 부록 G 기부금\n' +
      '\n' +
      '모든 작가들은 논문 작성에 기여한다.\n' +
      '\n' +
      '**루이 왕** : 공동 주도, 미세 조정, RL 교육, 인프라, 자동 평가, 코드 베이스\n' +
      '**Haofei Yu** : Co-lead, Fine-tuning, Human evaluation, Automatic task generation, Data, Codebase\n' +
      '**Wenxin Zhang** : Co-lead, Data, Automatic task generation, Human evaluation, Safety and alignment\n' +
      '**정양기** : 공동 주도, 인프라, 코드베이스, QA 평가, 휴먼 평가 인터페이스\n' +
      '**마튼 삽** : 기입에 대한 피드백\n' +
      '그레이엄 뉴빅 공동 고문, 전체 프로젝트를 감독합니다\n' +
      '요나탄 비스크 공동 고문, 전체 프로젝트를 감독합니다\n' +
      '**Hao Zhu**: 전체 프로젝트 리드 그림 12: MMLU 상의 에이전트 모델 간의 개체별 비교. 1부\n' +
      '\n' +
      '도 13: MMLU 상의 에이전트 모델들 간의 개체별 비교. 2부\n' +
      '\n' +
      '도 14: MMLU 상의 에이전트 모델들 간의 개체별 비교. 3부\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>