<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'LLM과 함께 비디오 생성을 위한 3D 비디오 토큰화기, 이의 적용 가능성은 긴 토큰 시퀀스(예를 들어, 2.2s 클립에 대한 1280 토큰)의 사용으로 인해 짧은 비디오 클립으로 제한된다. 긴 비디오를 이해하거나 생성할 때 LLM에 과도한 수의 토큰을 입력하는 것은 계산 자원 측면에서 허용되지 않는 것으로 간주된다.\n' +
      '\n' +
      '본 연구는 비디오 언어 사전 훈련의 한계를 해결하기 위해, 비디오를 키 프레임과 시간 움직임으로 분해하는 효율적인 비디오 표현을 탐색한다. 우리의 동기는 비디오 데이터 자체의 자연스러운 특성에 기반합니다. 도 1에 예시된 바와 같이, 비디오는 전형적으로 여러 샷들로 분할되며, 여기서 각 샷 내의 비디오 프레임들은 종종 실질적인 정보 중복성을 나타낸다. 이러한 모든 프레임을 토큰으로 인코딩하여 LLM의 생성 사전 훈련에 통합하는 것은 불필요하다. 이 사실은 우리가 각 비디오를 교대로 키프레임과 움직임 벡터로 분해하도록 강력하게 자극하는데, 전자는 1차 시각적 의미론을 캡슐화하고 후자는 시간에 따른 해당 키프레임의 동적 진화를 묘사한다. 이러한 분해된 표현에는 몇 가지 이점이 있다: (1) 3D 인코더를 사용하여 연속적인 비디오 프레임을 처리하는 것과 비교하여, 단일 키프레임과 모션 벡터의 조합은 비디오 시간 역학을 나타내기 위해 더 적은 토큰을 필요로 하며, 이는 대규모 사전-트레이닝에 더 효율적이다. (2) 모델은 기성 이미지 전용 LLM으로부터 획득된 시각적 지식을 계승할 수 있고, 처음부터 학습하지 않고 시간 정보를 모델링하는 데에만 집중할 수 있다.\n' +
      '\n' +
      '위의 동기부여를 바탕으로 LLM이 통합된 프레임워크에서 비디오 콘텐츠를 이해하고 생성할 수 있도록 효과적으로 권한을 부여하는 새로운 멀티모달 사전 훈련 방식인 **Video-LaVIT(L**anguage-**V**Ision **T**ransformer)를 제시한다. 구체적으로 Video-LaVIT는 비디오 모달리티를 처리하기 위해 _tokenizer_와 _detokenizer_의 두 가지 핵심 구성 요소를 통합한다. 비디오 토큰화기는 연속 비디오 데이터를 외국어와 유사한 컴팩트한 이산 토큰들의 시퀀스로 변환하는 것을 목표로 하며, 여기서 키프레임들은 확립된 이미지 토큰화기를 활용하여 처리된다(Jin et al., 2024). 시간적 모션들을 호환 가능한 이산 포맷으로 변환하기 위해, 시공간 모션 인코더가 고안된다. 추출된 움직임 벡터에 포함된 시변 상황 정보를 캡처하여 복잡한 동작을 비디오에서 이해하는 LLM의 능력을 크게 향상시킬 수 있다. 비디오 디토키나이저는 LLM들에 의해 생성된 이산화 비디오 토큰을 원래의 연속적인 픽셀 공간으로 다시 매핑하는 것을 담당한다. 트레이닝 동안, 비디오는 교번하는 이산 비주얼-모션 토큰 시퀀스로서 표현되며, 따라서 상이한 모달리티들과 함께 동일한 다음-토큰 예측 목표 하에서 최적화될 수 있다. 비디오는 본질적으로 시계열이기 때문에, 이 공동 자기회귀 사전 훈련은 상이한 비디오 클립들의 순차적인 관계들을 학습하는 데 기여한다. 우리는 Video-LaVIT가 더 이상의 미세 조정 없이 이해와 생성 작업 모두에서 유망한 결과를 달성하는 멀티모달 일반주의자 역할을 할 수 있음을 발견했다. 이 작업의 주요 기여는 다음과 같이 요약된다.\n' +
      '\n' +
      '* 우리는 LLM의 통일된 이해와 생성 능력의 한계를 비디오로 밀어내는 멀티모달 사전 훈련 방법인 Video-LaVIT를 소개한다.\n' +
      '* 비디오에서 시각적 및 시간적 정보를 효율적으로 모델링하기 위해 Video-LaVIT는 키프레임들 및 움직임 벡터들의 분해된 표현들에 대해 동작하는 새로운 비디오 토큰화기와 디토키나이저를 통합한다.\n' +
      '* 종합 정량적 및 정성적 실험을 통해 Video-LaVIT가 이미지 및 비디오 이해에서 텍스트-비디오 및 이미지-비디오 생성에 이르기까지 매우 경쟁력 있는 성능을 달성함을 입증한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**시각 언어 사전 교육** 비전-언어 모델의 대조적 학습을 위해 대규모 이미지-텍스트 쌍을 사용하는 성공(Radford et al., 2021)에 이어, 시각적 및 언어 데이터가 자기회귀 프로세스 하에서 공동으로 모델링되는 생성 사전 훈련에서 유사한 아이디어가 이용되었다. 실제로, 이는 일반적으로 교차-어텐션(Alayrac et al., 2022), Q-이전(Li et al., 2023), 또는 선형 투영(Liu et al., 2023)과 같은 중간 모듈을 통해 미리 훈련된 LLMs(Raffel et al., 2020; Brown et al., 2020; Touvron et al., 2023)에 시각적 이미지 입력을 적응시킴으로써 달성된다. CM3Leon(Yu et al., 2023) 및 LaVIT(Jin et al., 2024)와 같은 보다 최근의 접근법들은 통합된 다음 토큰 예측 목표를 형성하기 위해 이산 시각적 토큰화기들(van den Oord et al., 2017; Esser et al., 2021)의 사용을 옹호한다. 그러나, 이러한 방법들은 주로 이미지-텍스트 데이터에 집중되어 있으며, 상당히 높은 계산 비용으로 인해 비디오로 직접 확장될 수 없다.\n' +
      '\n' +
      '**비디오 이해 및 생성** 위의 사전 훈련 프레임워크에서 동영상을 통일함으로써, 마스킹된(Yang et al., 2022) 및 자기회귀 언어 모델(Li et al., 2023; Zhang et al., 2023; Maaz et al., 2023)로 동영상 이해에 현저한 진전이 있었다. 그러나, 비디오 생성을 위해, 주류 접근법들은 여전히 확산 모델들(Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Ho et al., 2020)에 기초하고 있으며, 이는 더 나은 시간적 일관성을 갖는 기존의 이미지 사전 트레이닝된 모델들을 향상시킨다(Ho et al., 2022; Singer et al., 2023; Blattmann et al., 2023; Esser et al., 2023; Blattmann et al., 2023). 한편, 언어 모델 기반 대응물들(Yan et al., 2021; Hong et al., 2023; Kondratyuk et al., 2023)은 제한된 컨텍스트 윈도우들 및 계산 자원들로 비디오 시간 역학을 효율적으로 인코딩해야 하는 중요한 도전에 직면한다. 이에 대해, 본 연구는 LLM 기반 비디오 이해 및 생성의 효능을 향상시키기 위해 비디오 모델링(Zhang et al., 2016; Wang et al., 2023; Shen et al., 2024)에서 고전적이고 효과적인 큐인 모션 벡터를 활용한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '본 연구는 대용량 언어 모델(Large Language Models, LLM)의 예외적인 모델링 능력을 활용하여 비디오 촬영장비의 학습을 용이하게 하는 효과적인 사전 훈련 프레임워크를 제시하는 것을 목적으로 한다. 이 목표를 달성하기 위해 우리는 통합된 이산 형태로 모든 모달리티를 표현할 수 있는 비디오 _tokenizer_(섹션 3.1)와 생성된 이산 토큰을 연속 픽셀 공간에 다시 매핑하는 비디오 _detokenizer_(섹션 3.2)의 두 가지 핵심 디자인을 강조한다. 이 두 가지 주요 구성 요소를 적용한 Video-LaVIT는 통합된 자기회귀 학습 패러다임(섹션 3.3)을 통해 최적화되어 다양한 멀티모달 콘텐츠를 동시에 이해하고 생성할 수 있다.\n' +
      '\n' +
      '### Video Tokenization\n' +
      '\n' +
      'LLM들에 대한 입력들로서 트리밍되지 않은 비디오를 인코딩하기 위해, 지배적인 접근법들(Lin et al., 2023; Li et al., 2023)은 주로 오리지널 비디오를 일련의 프레임들로 균일하게 다운샘플링한다. 이어서, 미리 트레이닝된 ViT 인코더(Radford et al., 2021; Fang et al., 2023)가 채용되어 이들 프레임을 별도로 인코딩하고 비디오 표현으로서 프레임-레벨 임베딩의 시퀀스를 생성한다. 이 간단한 방법은 프레임 간의 시간 역학 모델링을 무시하여 비디오에서 발생하는 액션 및 카메라 전환을 이해하는 능력을 방해한다. 매우 최근의 3D 비디오 인코더들의 활용(Kondratyuk et al., 2023)이 시간적 정보의 인코딩을 가능하게 하지만, 그것은 단지 짧은 비디오 클립들에만 적용되고 불가피하게 토큰들의 실질적인 확산(예를 들어, 하나의 2.2s 클립에 대해 1280개의 토큰들)을 산출함으로써, 많은 계산 오버헤드를 초래한다.\n' +
      '\n' +
      '**동작 인식 비디오 분해** 위와 같은 문제를 고려하여, 제안된 비디오 토큰화기는 시간 역학을 비디오 표현에 효율적으로 통합하고자 한다. 동일한 샷에서 캡처된 비디오 클립은 단일 키프레임을 통해 주요 의미를 전달할 수 있는 반면, 후속 프레임들은 그 키프레임을 기반으로 한 시간적 진화만을 보여준다. 이 속성은 키프레임 및 시간적 모션에 대해 분해된 비디오 토큰화에 힘을 준다. 키프레임을 위해 LaVIT(Jin et al., 2024)의 기성 이미지 토큰화기를 사용하여 학습된 비주얼 코드북과 사전 지식을 처음부터 교육 없이 계승한다. 시간적 모션 정보를 인코딩하기 위해, 일반적인 대안은 인접한 프레임들 사이의 손으로 조작된 조밀한 광학 흐름을 계산하는 것이다(Beauchemin and Barron, 1995). 비디오에서 객체 모션의 세밀한 묘사를 제공함에도 불구하고, 값비싼 계산은 사전 훈련 동안 대규모 비디오 데이터로 스케일링하는 데 적합하지 않게 만든다. 따라서, 압축 비디오 디코딩 과정에서 CPU(Wu et al., 2018)에서 고속으로 직접 추출될 수 있는 움직임 벡터에 의존한다.\n' +
      '\n' +
      '도 2에 예시된 바와 같이, 키프레임 및 움직임 정보를 추출하기 위해 MPEG-4(Le Gall, 1991) 압축 기법을 채용한다. 간결함을 위해, MPEG-4에서의 I-프레임들은 토큰화를 필요로 하는 키프레임들로서 고려된다. 보다 정교한(그러나 값비싼) 키프레임 선택 방식도 고려될 수 있지만, 이 작업의 주요 초점은 아니다. 형식적으로 각 비디오 프레임은 \\(16\\times 16\\)개의 비중첩 매크로블록으로 분할된다. 가장 좋은 매크로블록을 찾아 \\(t\\)번째 프레임의 움직임 벡터 \\(\\vec{m}\\)를 추정한다.\n' +
      '\n' +
      '그림 2: 각 비디오-텍스트 쌍에 대해, Video-LaVIT는 효율적인 토큰화를 위해 비디오를 키프레임 및 모션 벡터로 분해한다. 토키나이저는 원래의 입력들을 최대로 재구성함으로써 학습된다(예를 들어, 모션토키나이저는 우측에 도시된다). 마지막으로, 인코딩된 토큰은 텍스트 토큰과 연결되어 멀티모달 시퀀스를 형성하여 LLM(왼쪽)의 통합된 생성 사전 훈련을 허용한다.\n' +
      '\n' +
      '인접한 프레임간의 대응 \\(I_{t}\\)과 \\(I_{t-1}\\) :\n' +
      '\n' +
      '\\[\\vec{m}(p,q)=\\arg\\min_{i,j}\\|I_{t}(p,q)-I_{t-1}(p-i,q-j)\\|, \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(I(p,q)\\)는 위치 \\((p,q)\\)에서 매크로블록의 픽셀값을 나타내고, \\((i,j)\\)는 두 매크로블록의 중심 사이의 좌표 오프셋이다. 그런 다음, 비디오 클립을 키프레임(I_{0}\\in\\mathbb{R}^{H\\times W\\times 3})과 움직임 벡터(M\\in\\mathbb{R}^{T\\times\\frac{\\mu_text{fc}}\\times\\frac{\\mu_text{fc}}}\\times 2}\\times 2}으로 분해할 수 있다.\n' +
      '\n' +
      '**모션 벡터 토큰화** 움직임 벡터들을 외국어와 같은 이산 토큰들의 시퀀스로 변환하기 위해, 우리는 VQ-VAE 아키텍처를 기반으로 모션-특정 토큰화기를 개발한다(van den Oord et al., 2017). 시공간 인코더\\(f_{\\mathcal{E}\\), 학습 가능한 코드북\\(\\mathcal{C}=\\{c_{k}\\}_{k=1}^{K}\\), 디코더\\(f_{\\mathcal{D}\\)을 포함한다. 인코더 \\(f_{\\mathcal{E}}\\)는 \\(T\\) 프레임 중 문맥 움직임 정보를 융합하기 위해 공간 및 시간 주의 계층으로 구성된 \\(L\\) 스택형 트랜스포머 블록을 갖는다. 움직임 벡터\\(M\\in\\mathbb{R}^{T\\times\\frac{\\mu\\text{fc}}\\times\\frac{1}{\\mu\\text{fc}}\\times 2}\\times 1D 잠재 임베딩 시퀀스\\(\\hat{Z}\\in\\mathbb{R}^{N\\times d}\\)으로 매핑한다. 각 임베딩 벡터 \\(\\hat{z}\\in\\mathbb{R}^{d}\\)는 벡터 양자화기 \\(Q\\)에 의해 토큰화되어 \\(\\mathcal{C}\\)에서 가장 가까운 코드에 할당된다:\n' +
      '\n' +
      '\\[z_{i}=\\arg\\min_{j}\\|l_{2}(\\hat{z}_{i})-l_{2}(c_{j})\\|_{2}, \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(l_{2}\\)는 \\(L_{2}\\) 정규화를 나타낸다. 디코더 \\(f_{\\mathcal{D}\\)는 인코더와 유사한 구조를 가지며 이산 움직임 코드 \\(\\{z_{i}\\}_{i=1}^{N}\\)을 원래의 움직임 벡터로 다시 매핑해야 한다. 전체 모션 토큰화기는 재구성 품질을 최적화함으로써 업데이트될 수 있다. 훈련 중 코드북 붕괴를 방지하기 위해 Yu et al.(2022)을 따라 움직임 임베딩 \\(\\hat{Z}\\)을 양자화 전에 저차원 공간으로 투영하고 지수 이동 평균(EMA) 업데이트를 사용한다. 모션토키나이저에 대한 자세한 내용은 부록 A.1에서 확인할 수 있다. 마지막으로 비디오는 교대 \\(\\langle visual,motion,...\\rangle\\)으로 토큰화된다. 생성 사전 훈련 동안 LLM의 감독 신호 역할을 하는 코드. 이러한 인자화된 토큰화는 시간적 모션 정보를 효율적으로 캡처하면서 하나의 비디오 샷에서의 프레임간 중복성을 상당히 감소시킨다.\n' +
      '\n' +
      '### Video Detokenization\n' +
      '\n' +
      'Video-LaVIT의 비디오 디토키저는 비디오 생성을 위해 원래의 연속 픽셀 공간으로 다시 변환하는 역할을 한다. 이산형 토큰에서 고차원 비디오 공간으로 직접 매핑을 학습하는 데 어려움을 고려할 때, 우리는 키프레임이 비주얼 토큰에 기초하여 초기에 복구되는 순차적 디코딩 전략을 취한다. 이어서, 키프레임 및 모션 토큰 둘 다를 조건으로 취함으로써 후속 프레임들이 디코딩된다. 비디오 생성 품질을 향상시키는 데 있어 이 전략의 효능은 또한 최근 작업에 의해 검증되었다(Girdhar et al., 2023).\n' +
      '\n' +
      '구체적으로, 키프레임 및 비디오 디토키나이저는 모두 조건부 디노이징 U-Net(Rombach et al., 2022)을 사용한다. LaVIT(Jin et al., 2024)와 유사하게, 키프레임 U-Net \\(g_{I}\\)은 가우스 잡음으로부터 시각적 세부사항들을 채우기 위한 조건들로서 이미지 시맨틱스를 포함하는 재구성된 시각적 특징들을 취한다. 여기서는 주로 새롭게 제안된 비디오 디토키나이저 \\(g_{V}\\)에 초점을 맞춘다. 도 3(a)에 예시된 바와 같이, 블랫만 등(2023, 2023)에 이어, 공간 모듈들 뒤에 시간적 컨볼루션 및 어텐션 층들을 삽입함으로써 원래의 2D U-Net 아키텍처의 3D 변형이다.\n' +
      '\n' +
      '**Enhanced Motion Conditioning** 비디오 디토키나이저 \\(g_{V}\\)의 목적은 움직임 벡터의 안내를 엄격하게 준수함으로써 키프레임에 이어지는 \\(T\\) 프레임의 복구를 용이하게 하는 것이다. 이를 위해 우리는 \\(g_{V}\\)의 두 가지 운동 조건을 강조한다. 샘플링된 비디오 클립의 움직임 벡터\\(M\\in\\mathbb{R}^{T\\times\\frac{\\mu_text{fc}}}\\times\\frac{\\mu_text{fc}}}\\times\\frac{\\mu_text{fc}}\\times2}\\times가 주어지면,\n' +
      '\n' +
      '도 3: Video-LaVIT에서의 비디오 디토큰화를 위한 일러스트레이션. (a) 하나의 키프레임 및 후속 모션 벡터를 사용하여 원래의 비디오 클립을 재구성하는 것을 목표로 하는 비디오 디토키나이저를 위한 트레이닝 파이프라인. (b) 긴 비디오 디코딩을 위한 자기회귀 추론.\n' +
      '\n' +
      '우리는 U-Net 입력의 공간 모양과 일치하는지 확인하기 위해 최근접 이웃 보간법을 채택한다. 또한, VAE로부터 키프레임의 잠재상태\\(\\hat{I}\\)를 시간축을 따라 \\(T\\)회 반복하여 시각적 컨디셔닝을 형성한다. 움직임 벡터 \\(M\\), 키프레임 잠재 \\(\\hat{I}\\) 및 잡음 비디오 프레임을 \\(g_{V}\\)에 입력조건으로 채널별로 연결한다. 또한, 3D U-Net 블록에서 공간 및 시간 교차 주의 레이어를 통해 모션 피쳐 임베딩을 통해 직접 입력 컨디셔닝을 제외하고 컨디셔닝을 향상시킨다. 여기서, 모션 특징은 잠재적인 정보 손실을 줄이기 위해 다운샘플 계층을 제외한 \\(f_{\\mathcal{E}}\\)와 유사한 아키텍처를 갖는 컨디셔닝 인코더에서 비롯된다. 비디오 디토키나이저 \\(g_{V}\\)의 파라미터는 비디오 트레이닝 데이터세트 \\(\\mathcal{D}\\) 상에서 다음의 EDM 트레이닝 목적(Karras et al., 2022)을 최소화함으로써 업데이트된다:\n' +
      '\n' +
      '\\[\\mathbb{E}_{(X_{0},\\hat{I},\\hat{M})\\sim\\mathcal{D},\\sigma,n}\\left[\\lambda_{\\sigma}||g_{V}(X_{0}+n,\\sigma,\\hat{I},M)-X_{0}||\\right], \\tag{3}\\tag{3}\\sigma}\\left[\\lambda_{\\sigma}||g_{V}(X_{0}+n,\\sigma,\\hat{I},M)-X_{0}||\\right]\n' +
      '\n' +
      '여기서 \\(\\sigma\\sim p(\\sigma)\\)는 훈련 중 잡음 수준이고, \\(n\\sim\\mathcal{N}(n;0,\\sigma^{2})\\)는 비디오 샘플에 첨가된 랜덤 잡음이며, \\(\\lambda_{\\sigma}\\)은 손실 가중 함수이다. 추론에서 LLM에 의해 계산된 \\(\\langle visual,motion\\rangle\\) 토큰은 먼저 해당 토큰타이저에 의해 시각적 특징과 움직임 벡터로 매핑된다. 재구성된 시각적 특징들은 키프레임을 생성하기 위해 \\(g_{I}\\)에 공급되고, 이 키프레임은 비디오 클립을 디코딩하기 위해 \\(g_{V}\\)을 위한 조건들로서 후속적으로 재구성된 움직임 벡터들과 결합된다(도 3의 (b) 참조).\n' +
      '\n' +
      '** 롱 비디오 디코딩** 비디오는 여러 개의 교번하는 \\(\\langle visual,motion\\rangle\\) 시퀀스로 표현되므로, LLM의 자기회귀적 사전학습을 통해 서로 다른 비디오 조각들 간의 상호의존성을 효과적으로 학습할 수 있다. 따라서 Video-LaVIT는 여러 개의 클립을 프로그래시브 디코딩하여 더 긴 비디오의 생성을 자연스럽게 지원한다. 그러나, 별도의 디코딩은 상이한 클립들 사이에서 일부 미세한 시각적 세부사항들에 불일치를 가져올 것이다(도 5 참조). 이를 완화하기 위해, 비디오 클립의 키프레임 \\(I_{r}\\)을 디코딩할 때 명시적인 잡음 제약 조건을 통합한다. 도 3(b)에 예시된 바와 같이, DDIM 샘플링(Song et al., 2020) 프로세스\\(\\Delta T\\) 시간을 반전시킴으로써 이전에 생성된 클립으로부터 마지막 프레임\\(I_{r-1}\\)을 중간 잡음 상태\\(x_{\\Delta T}\\)으로 되돌린다. 각각의 반전 단계는 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\[x_{t+1}=\\sqrt{\\frac{\\alpha_{t+1}{\\alpha_{t}}}x_{t+\\left(\\sqrt{\\frac{1}{\\alpha_{t+1}}-1}-\\sqrt{\\frac{1}{\\alpha_{t}-1}\\right)g_{I}(x_{t},t,\\hat{I}), \\tag{4}\\left(\\sqrt{\\frac{1}{\\alpha_{t+1}}-1}-\\sqrt{\\frac{1}{\\alpha_{t}-1}\\right)g_{I}(x_{t},t,\\hat{I}), \\tag{4}\\left(\\sqrt{\\frac{1}{\\alpha_{t+1}}}x_{t}+\\left(\\sqrt{\\frac{1}{\\alpha_{t}}-1}\n' +
      '\n' +
      '여기서 \\(\\alpha_{t}\\)는 잡음 레벨이고, \\(\\hat{I}\\)은 시각적 특징 조건이다. 이후 키프레임(I_{r}\\)에 대한 잡음제거 루프에서 역잡음 상태\\(x_{\\Delta T}\\)를 초기 잡음으로 간주한다. 도 5에 예시된 바와 같이, 이러한 잡음 제약을 추가하는 것은 비디오 클립들 사이의 시간적 일관성을 향상시킬 수 있다.\n' +
      '\n' +
      '### 통합 생성 모델링\n' +
      '\n' +
      '개발된 분해된 비디오 토큰화 전략을 기반으로 모든 양식(비디오, 이미지 및 텍스트)을 LLM에 공급되는 1D 이산 토큰으로 무차별적으로 처리할 수 있다. LaVIT(Jin et al., 2024)에 이어, 특수 토큰들(예를 들어, 모션 모달리티에 대한 [MOV] 및 [/MOV])이 입력 데이터 내의 모달리티들을 미분하기 위한 시각적 및 모션 토큰 시퀀스의 시작 및 끝에 삽입된다. 또한 사전 학습 시 입력 시퀀스로 \\([\\text{video}(\\text{image}), \\text{text}]\\)와 \\([\\text{text},\\text{video}(\\text{image})]\\)을 형성하기 위해 멀티모달 데이터 쌍의 순서를 교환한다. 형식적으로 멀티모달 시퀀스 \\(y=(y_{1},y_{2},..,y_{S})\\)이 주어지면 Video-LaVIT는 LLM으로부터 성공적인 생성 언어 모델링 패러다임을 계승하여 각 토큰 \\(y_{i}\\)의 가능성을 자기회귀 방식으로 직접 극대화한다:\n' +
      '\n' +
      '\\[p(y)=\\sum_{y\\in\\mathcal{D}}\\sum_{i=1}^{S}\\log P_{\\theta}(y_{i}|y_{<i}). \\tag{5}\\\\\n' +
      '\n' +
      '사전 훈련 후 Video-LaVIT는 모든 모달리티에서 멀티모달 이해와 데이터 생성을 모두 달성하기 위한 멀티모달 일반주의자의 역할을 할 수 있다.\n' +
      '\n' +
      '**모델 트레이닝.** Video-LaVIT는 대규모 멀티모달 코퍼스에서 3단계 트레이닝 절차를 거친다. 각 단계의 목적은 다음과 같이 요약할 수 있다. i) Tokenizer와 Detokenizer Training. 이 단계는 대응하는 텍스트 캡션 없이 순수 비디오 데이터만을 필요로 한다. 후속 생성 사전 훈련을 안내하는 감독 신호 역할을 하는 소형 비디오 토큰을 생산할 뿐만 아니라 원본 비디오의 정확한 재구성을 용이하게 하는 것을 목표로 한다. ii) 생성적 사전 훈련. 단계-2는 모델이 LLM 내에서 통합된 생성 모델링을 통해 서로 다른 양식의 데이터 간의 상호 상관 관계를 학습할 수 있도록 한다. iii) 명령어 튜닝. 획득한 지식을 완전히 풀기 위해 마지막 단계는 다양한 멀티모달 작업을 수행할 수 있는 지시 추적 능력을 더욱 향상시킨다. 각 단계에 대한 모델 아키텍처 및 훈련 데이터에 대한 자세한 내용은 부록 A.1에 나와 있다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Multimodal Understanding\n' +
      '\n' +
      '분해된 비디오 표현으로 Video-LaVIT는 자연스럽게 비디오와 이미지를 모두 이해할 수 있다. 여기에서 일반적으로 사용되는 11개의 이미지 및 비디오 벤치마크에 대한 멀티모달 이해 능력을 보여준다.\n' +
      '\n' +
      '**Image Understanding.** 표 1은 널리 사용되는 8개의 이미지 질문 응답 및 멀티모달 벤치마크에 걸친 광범위한 비교를 제시한다: VQA v2(Goyal et al., 2017), GQA(Hudson and Manning, 2019), VizWiz(Gurari et al., 2018), ScienceQA-IMG(Lu et al., 2022), MME(Fu et al., 2023), MMBench(Liu et al., 2023), SEED(Li et al., 2023), MM-Vet(Yu et al., 2023). 우리의 모델은 사전 훈련 지식을 이미지 이해 과제에 성공적으로 일반화하고 최상의 전체 성능을 제공한다. 구체적으로, 동일한 명령어 데이터세트와 베이스로\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '텍스트-비디오 생성기, 우리의 방법은 훨씬 더 큰 데이터에 대해 트레이닝된 3D 비디오 토큰화기를 사용하는 최근 동시 작업 비디오 시인(Kondratyuk et al., 2023)을 능가하면서 일관되게 CogVideo(Hong et al., 2023)를 능가한다. 이것은 당사의 토큰타이저 디자인의 우수성을 분명히 입증합니다.\n' +
      '\n' +
      '### Qualitative Results\n' +
      '\n' +
      '이 섹션에서는 Video-LaVIT에 의해 생성된 비디오를 텍스트 및 이미지 조건 모두에서 최첨단 결과와 비교한다. 또한 긴 비디오를 생성할 수 있는 특별한 기능을 제공합니다. 보다 많은 시각화 예가 부록 B.1에 나와 있다.\n' +
      '\n' +
      '텍스트-투-비디오 및 이미지-투-비디오 생성 결과는 그림 4에서 시각화되었다. 텍스트-투-비디오 생성의 경우, 본 방법은 이미지와 통합된 사전 훈련 프레임워크 덕분에 폐쇄 소스 모델 Gen-2(Runaway, 2023)에서 그리 멀지 않은 시각적 품질을 생성할 수 있다. 한편 Video-LaVIT는 (왼쪽 상단 예에서) 더 나은 동작을 추론하고 텍스트 프롬프트(두 경우 모두)에 기초하여 예술적 터치를 추가하는 것과 같은 추론 능력에 유리하다. 이미지-투-비디오 생성을 위해, 본 방법은 코히어런트 및 고도로 심미적인 비디오 클립들(하단-좌측 예) 모두를 생성하는데 있어서 최신 모델 SVD(Blattmann et al., 2023)와 비교된다. 또한, 분해된 비디오 표현은 비디오 디코더가 비교적 어려운 합성 이미지 프롬프트(우하단 예)가 주어진 보다 현저하고 생생한 움직임을 생성할 수 있게 한다.\n' +
      '\n' +
      '더 나아가, 우리의 자기회귀 모델은 그림 5와 같이 자연스럽게 긴 비디오 생성으로 확장될 수 있다. 제안된 명시적 잡음 제약 덕분에 de\n' +
      '\n' +
      '도 4: Gen-2(Runaway, 2023) 및 SVD-XT(Blattmann et al., 2023)와의 Text-to-video(상단) 및 image-to-video(하단) 생성 비교. 텍스트 프롬프트는 Emu Video(Girdhar et al., 2023) 및 SVD로부터 온다. I2V 세대는 가장 왼쪽 프레임에서 조정된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{Data size} & \\multirow{2}{*}{Public data} & \\multicolumn{2}{c}{MSR-VTT} & \\multicolumn{2}{c}{UCF-101} \\\\ \\cline{3-8}  & & & CLIPSIM (\\(\\uparrow\\)) & FVD (\\(\\downarrow\\)) & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & FVD (\\(\\downarrow\\)) \\\\ \\hline CogVideo (Hong et al., 2023) & 5.4M & ✓ & 0.2631 & 1294 & 23.59 & 25.27 & 701.59 \\\\ Video LDM (Blattmann et al., 2023) & 10M & ✓ & 0.2929 & - & - & 33.45 & 550.61 \\\\ VideoComposer (Wang et al., 2023) & 10M & ✓ & 0.2932 & 580 & - & - & - \\\\ InternVid (Wang et al., 2024) & 28M & ✓ & 0.2951 & - & - & 21.04 & 616.51 \\\\ Make-A-Video (Singer et al., 2023) & 20M & ✓ & **0.3049** & - & 13.17 & 33.00 & 367.23 \\\\ VideoPoet (Kondratyuk et al., 2023) & 270M & \\(\\times\\) & **0.3049** & 213 & - & 38.44 & 355.00 \\\\ PYCo (Ge et al., 2023) & 22.5M & \\(\\times\\) & - & - & **9.73** & **47.76** & 355.19 \\\\ SVD (Blattmann et al., 2023) & 152M & \\(\\times\\) & - & - & - & **242.02** \\\\ \\hline Video-LaVIT & 10M & ✓ & 0.3010 & **169.51** & 11.80 & 37.96 & 274.96 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: Zero-shot text-to-video 생성 성능. Video-LaVIT는 더 많은 독점 데이터에 대해 훈련된 최신 모델에 대해 경쟁적인 결과를 제공하며, 데이터 크기는 훈련 비디오 클립의 수 측면에서 보고된다. 그 다음으로 좋은 결과는 밑줄이 그어져 있다.\n' +
      '\n' +
      '연속 비디오 클립들을 코딩함으로써, 디코딩된 클립들 사이의 시간적 일관성이 크게 개선된다. 대조적으로, 각각의 비디오 클립을 개별적으로 디코딩하는 것은 상이한 클립들의 비디오 프레임들 중 세밀한 시각적 세부사항들의 비간섭성을 초래할 것이다(도 5의 하단을 참조).\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '이 절에서는 모션 토큰화와 다양한 모션 토큰 길이의 영향을 조사합니다. 제한된 공간으로 인해 제안된 향상된 모션 컨디셔닝 전략에 대한 삭제는 부록 B.1에 제공된다.\n' +
      '\n' +
      '**모션 토큰화의 효과** 비디오 사전 학습에서 모션 토큰화의 유효성을 검증하기 위해, 모션 토큰 없이 균일하게 샘플링된 \\(16\\) 프레임을 별도로 토큰화하여 비디오를 나타내는 베이스라인을 설계한다. 비디오 합성을 평가할 때 먼저 동일한 키프레임 U-Net을 사용하여 생성된 비주얼 토큰에서 키프레임을 생성한 다음 이미지 대 비디오 모델 SVD-XT에 입력하여 비디오 샘플을 생성한다. 표 4에 나타난 바와 같이, 모션 벡터를 시간 정보의 표현으로 통합함으로써 Video-LaVIT는 비디오 이해 및 생성 작업 모두에서 상당히 더 나은 결과를 달성한다. 특히 UCF-101 벤치마크에서 텍스트 대 비디오 생성을 위해서는 모션 안내 없이 텍스트 프롬프트에 의해 기술된 정확한 비디오 콘텐츠를 생성하는 것이 어렵다.\n' +
      '\n' +
      '**토큰 길이의 효과** 또한 시간적 움직임 정보를 부호화할 때 서로 다른 움직임 토큰 길이의 영향을 탐색한다. 자세한 결과는 표 5에 보고되었으며, 매우 적은 수로 높은 이해와 생성 성능을 얻을 수 있음을 관찰할 수 있다. 더 많은 토큰 번호는 명백한 모션 없이 비디오를 인코딩할 때 표현 중복으로 이어질 수 있고 더 많은 중복 모션 토큰 ID를 가져와 LLM의 다음 토큰 예측 학습 패러다임을 덜 효과적으로 만든다. 더 적은 모션 토큰들을 사용하는 것은 또한 LLM의 동일한 컨텍스트 길이 하에서 입력 조건들로서 더 많은 비디오 클립들을 허용하며, 이는 긴 비디오 이해에 유용하다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 비디오, 이미지, 언어의 통합적 이해와 생성을 LLM에 부여하는 멀티모달 생성 사전 훈련 방법인 Video-LaVIT를 소개한다. 이 방법의 핵심은 영상 전용 멀티모달 LLM의 시각적 지식을 재사용하면서 시간 정보를 보다 효과적으로 모델링할 수 있는 비디오 분해 기법이다. 분해된 키프레임들과 모션 벡터들은 통합된 생성 사전 트레이닝을 위해 LLM들에 적응하도록 효율적으로 토큰화될 수 있다. 또한, 잘 설계된 디토키나이저를 사용하여, 우리의 모델은 긴 비디오를 포함하는 멀티모달 생성을 유연하게 지원한다. 마지막으로 Video-LaVIT의 이해와 생성 능력은 광범위한 양적, 질적 결과를 통해 검증된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multirow{2}{*}{Method} & MSVD & ActivityNet & \\multicolumn{2}{c}{UCF-101} \\\\ \\cline{2-5}  & Accuracy & Accuracy & IS (\\(\\uparrow\\)) & FVD (\\(\\downarrow\\)) \\\\ \\hline w/o motion & 67.3 & 47.4 & 29.56 & 442.80 \\\\ w/ motion & **73.5** & **50.2** & **37.96** & **274.96** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 제로-샷 비디오 이해(왼쪽) 및 생성(오른쪽)에서 제안된 모션 토큰화 전략의 제거.\n' +
      '\n' +
      '도 5: _“캐리비안의 맑고 맑은 바다를 우아하게 항해하는 날렵한 요트의 360샷”_를 가진 긴 비디오 생성 예. 상위 두 행은 시간 일관성을 향상시키기 위해 식 (4)의 잡음 제약을 사용하는 반면, 하위 행은 그렇지 않다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multirow{2}{*}{Method} & MSVD & ActivityNet & \\multicolumn{2}{c}{UCF-101} \\\\ \\cline{2-5}  & Accuracy & Accuracy & IS (\\(\\uparrow\\)) & FVD (\\(\\downarrow\\)) \\\\ \\hline \\(N=256\\) & 69.2 & 48.8 & 37.57 & 281.24 \\\\ \\(N=64\\) & **73.5** & **50.2** & **37.96** & **274.96** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 제로-샷 비디오 이해(왼쪽) 및 생성(오른쪽)에서 모션 토큰의 수(\\(N\\)로 표시됨)의 제거.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Aghajany et al. (2022) Aghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu, H., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis, M., et al. CM3: causal masked multimodal model of the internet. _ arXiv preprint arXiv:2201.07520_, 2022.\n' +
      '* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: few-shot learning을 위한 시각적 언어 모델. In _NeurIPS_, pp. 23716-23736, 2022.\n' +
      '* Awadalla et al. (2023) Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa, S., et al. OpenFlamingo: a open-source framework for training large autoregressive vision-language models. _ arXiv preprint arXiv:2308.01390_, 2023.\n' +
      '*백등(2019) 백, Y., Lee, B., Han, D., Yun, S., and Lee, H. Character region awareness for text detection. In _CVPR_, pp. 9365-9374, 2019.\n' +
      '* Bain et al. (2021) Bain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen in time: A joint video and image encoder for end-to-end retrieval. In _ICCV_, pp. 1728-1738, 2021.\n' +
      '* Beauchemin & Barron (1995) Beauchemin, S. S. and Barron, J. L. optical flow의 계산. _ ACM Computing Surveys_, 27(3):433-466, 1995.\n' +
      '* Blattmann et al. (2023a) Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al. Stable video diffusion: latent video diffusion models to large datasets. _ arXiv preprint arXiv:2311.15127_, 2023a.\n' +
      '* Blattmann et al. (2023b) Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. 잠복기 정렬: 잠재 확산 모델과 고해상도 비디오 합성 _CVPR_에서, pp. 22563-22575, 2023b.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models is few-shot learners. In _NeurIPS_, pp. 1877-1901, 2020.\n' +
      '* Carreira & Zisserman (2017) Carreira, J. and Zisserman, A. Quo vadis, action recognition? 새로운 모델과 동역학 데이터 세트입니다. _CVPR_에서, pp. 6299-6308, 2017.\n' +
      '* Changpinyo et al. (2021) Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. 컨셉 12M: 롱테일 비주얼 컨셉을 인식하기 위해 웹 스케일 이미지-텍스트 사전 트레이닝을 푸시한다. _CVPR_, pp. 3558-3568, 2021.\n' +
      '* Chen & Dolan (2011) Chen, D. and Dolan, W. B. 패러프레이즈 평가를 위한 고도의 병렬 데이터를 수집하는 단계. In _ACL_, pp. 190-200, 2011.\n' +
      '* Dai et al. (2023) Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. InstructBLIP: 명령어 튜닝이 있는 범용 비전 언어 모델을 향합니다. _NeurIPS_, 2023.\n' +
      '* Dong 등(2024) Dong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao, L., Sun, J., Zhou, H., Wei, H., et al. DreamLLM: Synergistic multimodal comprehension and creation. 2024년 _ICLR_에서\n' +
      '* Esser et al. (2021) Esser, P., Rombach, R., and Ommer, B. Taming transformer for high-resolution image synthesis. _CVPR_, pp. 12873-12883, 2021.\n' +
      '* Esser et al. (2023) Esser, P., Chiu, J., Atighehchian, P., Grasskog, J., and Germanidis, A. Structure and content-guided video synthesis with diffusion models. In _ICCV_, pp. 7346-7356, 2023.\n' +
      '* Fang et al. (2023) Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., and Cao, Y. EVA: 스케일에서 마스킹된 시각적 표현 학습의 한계를 탐구한다. _CVPR_, pp. 19358-19369, 2023.\n' +
      '* Feng et al. (2024) Feng, W., Zhu, W., Fu, T. -j., Jampani, V., Akula, A., He, X., Basu, S., Wang, X. E., and Wang, W. Y. LayoutGPT: Compositional visual planning and generation with large language models. 2024년 _ICLR_에서\n' +
      '* Fu et al. (2023) Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu, Z., Lin, W., Yang, J., Zheng, X., et al. MME: 멀티모달 대형 언어 모델들에 대한 종합 평가 벤치마크. _ arXiv preprint arXiv:2306.13394_, 2023.\n' +
      '* Ge et al. (2023) Ge, S., Nah, S., Liu, G., Poon, T., Tao, A., Catanzaro, B., Jacobs, D., Huang, J.-B., Liu, M. - Y, Balaji, Y. 자신의 상관 관계를 보존합니다: 비디오 확산 모델에 대한 노이즈 이전입니다. _ICCV_에서, pp. 22930-22941, 2023.\n' +
      '* Gemini Team (2023) Gemini Team, G. Gemini: 고도로 유능한 멀티모달 모델의 가족. _ arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Girdhar et al. (2023) Girdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S. S., Shah, A., Yin, X., Parikh, D., and Misra, I. Emu video: Factorizing text-to-video generation by explicit image conditioning. _ arXiv preprint arXiv:2311.10709_, 2023.\n' +
      '* Goyal et al. (2017) Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. _CVPR_, pp. 6904-6913, 2017에서.\n' +
      '* Gurari et al. (2018) Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. VizWiz grand challenge: Answering visual questions from blind people. In _CVPR_, pp.3608-3617, 2018.\n' +
      '\n' +
      '* Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. 2021년 _ICLR_에서.\n' +
      '* Heusel et al. (2017) Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. 2개의 시간 척도 업데이트 규칙에 의해 훈련된GAN은 로컬 내쉬 균형으로 수렴한다. In _NeurIPS_, pp. 6629-6640, 2017.\n' +
      '* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In _NeurIPS_, pp. 6840-6851, 2020.\n' +
      '* Ho et al. (2022) Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. In _NeurIPS_, pp. 8633-8646, 2022.\n' +
      '* Hong et al. (2023) Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. CogVideo: Large-scale pretraining for text-to-video generation via transformer. 2023년 _ICLR_에서\n' +
      '* Hudson & Manning (2019) Hudson, D. A. and Manning, C. D. GQA: real-world visual reasoning and compositional question answering을 위한 새로운 데이터셋. In _CVPR_, pp. 6700-6709, 2019.\n' +
      '* Jin et al. (2024) Jin, Y., Xu, K., Xu, K., Chen, L., Liao, C., Tan, J., Huang, Q., Chen, B., Lei, C., Liu, A., et al. Unified language-vision preraining in LLM with dynamic discrete visual tokenization. 2024년 _ICLR_에서\n' +
      '* Karras et al. (2022) Karras, T., Aittala, M., Aila, T., and Laine, S. 확산 기반 생성 모델의 설계 공간을 설명한다. In _NeurIPS_, pp. 26565-26577, 2022.\n' +
      '* Kay et al. (2017) Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al. The Kinetics human action video dataset. _ ArXiv:1705.06950_, 2017.\n' +
      '* Kondratyuk et al. (2023) Kondratyuk et al. (2023) Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Hornung, R., Adam, H., Akbari, H., Alon, Y., Birodkar, V., et al. VideoPoot: Zero-shot 비디오 생성을 위한 큰 언어 모델. _ arXiv preprint arXiv:2312.14125_, 2023.\n' +
      '* Kynkaanniemi et al. (2023) Kynkaanniemi, T., Karras, T., Aittala, M., Aila, T., and Lehtinen, J. The role of ImageNet classes in Frechet Inception distance. 2023년 _ICLR_에서\n' +
      '* Le Gall(1991) Le Gall, D. MPEG : 멀티미디어 응용을 위한 동영상 압축 표준. _ ACM_, 34(4):46-58, 1991의 통신.\n' +
      '* Li 등(2023a) Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y. SEED-Bench: 생성적 이해도를 가진 멀티모달 LLMs를 벤치마킹한다. _ arXiv preprint arXiv:2307.16125_, 2023a.\n' +
      '* Li 등 (2022) Li, D., Li, J., and Hoi, S. BLIP-확산: 제어 가능한 텍스트 대 이미지 생성 및 편집을 위한 사전 훈련된 피사체 표현. _NeurIPS_, 2023b.\n' +
      '* Li 등 (2022) Li, J., Li, D., Xiong, C., and Hoi, S. BLIP: 통일된 시각 언어 이해 및 생성을 위한 언어 이미지 사전 훈련. _ICML_, pp. 12888-12900, 2022에서.\n' +
      '* Li et al. (1974) Li, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: 냉동 이미지 인코더 및 대형 언어 모델로 부트스트래핑 언어-이미지 사전 트레이닝. _ICML_, pp. 19730-19742, 2023c.\n' +
      '*Li 등(2023d) Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., and Qiao, Y. VideoChat: 채팅 중심의 비디오 이해. _ arXiv preprint arXiv:2305.06355_, 2023d.\n' +
      '* Li et al. (2023e) Li, Y., Wang, C., and Jia, J. LLaMA-VID: An image is worth 2 tokens in large language models. _ arXiv preprint arXiv:2311.17043_, 2023e.\n' +
      '* Lian et al. (2023) Lian, L., Li, B., Yala, A., and Darrell, T. LLM-grounded diffusion: 텍스트-대-이미지 확산 모델에 대한 신속한 이해도를 큰 언어 모델로 향상시킨다. _ arXiv preprint arXiv:2305.13655_, 2023.\n' +
      '* Lin et al. (2023) Lin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., and Yuan, L. Video-LLaVA: 투영 전에 정렬에 의해 통합된 시각적 표현을 학습하는 단계 _ arXiv preprint arXiv:2311.10122_, 2023.\n' +
      '* Lin et al. (2014) Lin, T. -Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft COCO: Common objects in context. _ECCV_, pp. 740-755, 2014.\n' +
      '* Liu et al. (2023a) Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning. _ arXiv preprint arXiv:2310.03744_, 2023a.\n' +
      '*Liu et al. (2023b) Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. _NeurIPS_, 2023b.\n' +
      '* Liu et al. (2023c) Liu, S., Cheng, H., Liu, H., Zhang, H., Li, F., Ren, T., Zou, X., Yang, J., Su, H., Zhu, J., et al. LLaVA-Plus: Learning to use tools for creating multimodal agents. _ arXiv preprint arXiv:2311.05437_, 2023c.\n' +
      '*Liu et al. (2023d) Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. MMBench: your multi-modal model is all-around player? _ arXiv preprint arXiv:2307.06281_, 2023d.\n' +
      '* Lu et al. (2022) Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K. - W., Zhu, S. - C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chain for science question answering. In _NeurIPS_, pp. 2507-2521, 2022.\n' +
      '\n' +
      '* Maaz et al. (2023) Maaz, M., Rasheed, H., Khan, S., and Khan, F. S. Video-ChatGPT: Towards detailed video understanding via large vision and language models. _ arXiv preprint arXiv:2306.05424_, 2023.\n' +
      '* OpenAI(2023a) OpenAI. GPT-4 기술 보고서입니다 arXiv preprint arXiv:2303.08774_, 2023a.\n' +
      '* OpenAI(2023b) OpenAI. GPT-4V(ision) 시스템 카드. [https://openai.com/research/gpt-4v-system-card] (https://openai.com/research/gpt-4v-system-card), 2023b.\n' +
      '* Ordonez et al. (2011) Ordonez, V., Kulkarni, G., and Berg, T. L. Im2Text: 100만장의 캡션 사진을 이용하여 이미지를 기술하는 것. In _NeurIPS_, pp. 1143-1151, 2011.\n' +
      '* Podell et al. (2024) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. SDXL: 고해상도 이미지 합성을 위한 잠재 확산 모델 개선. 2024년 _ICLR_에서\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _ICML_, pp.8748-8763, 2021.\n' +
      '* Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limit of transfer learning with unified text-to-text transformer. _ JMLR_, 21(1):5485-5551, 2020.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. _CVPR_, pp. 10684-10695, 2022.\n' +
      '* Runaway(2023) Runaway. Gen-2. [https://research.runwayml.com/gen2](https://research.runwayml.com/gen2), 2023.\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. _NeurIPS_, pp. 36479-36494, 2022.\n' +
      '* Saito et al. (2020) Saito, M., Saito, S., Koyama, M., and Kobayashi, S. 기차는 희소하게, 조밀하게 생성: 고해상도 시간 GAN의 메모리-효율적인 비감독 트레이닝. _ IJCV_, 128(10-11):2586-2606, 2020.\n' +
      '* Sharma et al. (2018) Sharma, P., Ding, N., Goodman, S., and Soricut, R. 개념 캡션: 자동 이미지 캡션을 위한 세척, 하이퍼니밍, 이미지 알트 텍스트 데이터 세트. In _ACL_, pp. 2556-2565, 2018.\n' +
      '* Shen et al. (2024) Shen, C., Gan, Y., Chen, C., Zhu, X., Cheng, L., and Wang, J. Decouple content and motion for conditional image-to-video generation. 2024년, _AAAI_에서\n' +
      '* Singer et al. (2023) Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., et al. Make-A-Video: Text-to-video data without text-video generation. 2023년 _ICLR_에서\n' +
      '* Sohl-Dickstein et al. (2015) Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. 평형 열역학을 이용한 심층 비지도 학습 In _ICML_, pp. 2256-2265, 2015.\n' +
      '* Song et al. (2020) Song, J., Meng, C., and Ermon, S. 확산 암시적 모델의 잡음 제거 2020년 _ICLR_에서\n' +
      '*Song & Ermon(2019) Song, Y. 및 Ermon, S. 데이터 분포의 기울기를 추정하여 생성 모델링 In _NeurIPS_, pp.11918-11930, 2019.\n' +
      '* Soomro et al. (2012) Soomro, K., Zamir, A. R., and Shah, M. 야생의 비디오로부터 101개의 인간 행동 클래스의 데이터세트. _ Computer Vision_, 2(11), 2012 연구센터.\n' +
      '*Sun et al. (2024) Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., and Wang, X. Emu: 멀티모달리티에서 생성적 사전 훈련. 2024년 _ICLR_에서\n' +
      '* Together Computer(2023) Together Computer. RedPajama: Open dataset for training large language models, 2023. URL[https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. - A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. LLaMA: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023b.\n' +
      '* Tran et al. (2015) Tran, D., Bourdev, L., Fergus, R., Torresani, L., and Paluri, M. 3D 컨볼루션 네트워크로 시공간 특징을 학습합니다. _ICCV_, pp. 4489-4497, 2015.\n' +
      '* Unterthiner et al. (2018) Unterthiner, T., Van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., and Gelly, S. 비디오의 정확한 생성 모델에 대한: 새로운 메트릭 & 도전. _ arXiv preprint arXiv:1812.01717_, 2018.\n' +
      '* van den Oord et al. (2017) van den Oord, A., Vinyals, O., and Kavukcuoglu, K. 신경 이산 표현 학습. In _NeurIPS_, pp. 6309-6318, 2017.\n' +
      '* Villegas et al. (2023) Villegas, R., Babaeizadeh, M., Kindermans, P.-J., Moraldo, H., Zhang, H., Saffar, M. T., Castro, S., Kunze, J., and Erhan, D. Phenaki: Variable length video generation from open domain textual description. 2023년 _ICLR_에서\n' +
      '\n' +
      '* Wang et al. (2023) Wang, F.-Y., Chen, W., Song, G., Ye, H.-J., Liu, Y., and Li, H. Gen-L-Video: Multi-text to long video generation via temporal co-denoising. _ arXiv preprint arXiv:2305.18264_, 2023a.\n' +
      '* Wang et al. (2023b) Wang, X., Yuan, H., Zhang, S., Chen, D., Wang, J., Zhang, Y., Shen, Y., Zhao, D., and Zhou, J. VideoComposer: Compositional video synthesis with motion controllability. _NeurIPS_, 2023b.\n' +
      '* Wang et al. (2024) Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Chen, X., Wang, Y., Luo, P., Liu, Z., et al. InternVid: 멀티모달 이해 및 생성을 위한 대규모 비디오-텍스트 데이터세트. 2024년 _ICLR_에서\n' +
      '* Wu et al. (2021) Wu, C., Huang, L., Zhang, Q., Li, B., Ji, L., Yang, F., Sapiro, G., and Duan, N. GODIVA: 자연 기술로부터 오픈 도메인 비디오를 생성하는 단계; _ arXiv preprint arXiv:2104.14806_, 2021.\n' +
      '* Wu et al. (2018) Wu, C.-Y., Zaheer, M., Hu, H., Manmatha, R., Smola, A. J., and Krahenbuhl, P. Compressed video action recognition. In _CVPR_, pp. 6026-6035, 2018.\n' +
      '* Xu et al.(2016) Xu, J., Mei, T., Yao, T., and Rui, Y. MSR-VTT: 브리징 비디오 및 언어를 위한 대규모 비디오 기술 데이터세트. _CVPR_, pp. 5288-5296, 2016.\n' +
      '* Yan et al. (2021) Yan, W., Zhang, Y., Abbeel, P., and Srinivas, A. VideoGPT: Video generation using VQ-VAE and transformer. _ arXiv preprint arXiv:2104.10157_, 2021.\n' +
      '* Yang et al. (2022) Yang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C. Zero-shot video question answering via frozen bidirectional language models. In _NeurIPS_, pp. 124-141, 2022.\n' +
      '* Yu et al. (2022) Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., and Wu, Y. 개선된 VQGAN을 이용한 벡터 양자화 영상 모델링 2022년 _ICLR_에서\n' +
      '* Yu et al. (2023a) Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O., Wang, T., Babu, A., Tang, B., Karrer, B., Sheynin, S., et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning. _ arXiv preprint arXiv:2309.02591_, 2023a.\n' +
      '* Yu et al. (2023b) Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. MM-Vet: 통합 기능에 대한 대규모 멀티모달 모델 평가 arXiv preprint arXiv:2308.02490_, 2023b.\n' +
      '* Yu et al. (2019) Yu, Z., Xu, D., Yu, J., Yu, T., Zhao, Z., Zhuang, Y., and Tao, D. ActivityNet-QA: 질문 응답을 통해 복잡한 웹 비디오를 이해하기 위한 데이터세트. In _AAAI_, pp. 9127-9134, 2019.\n' +
      '* Zeng et al. (2023) Zeng, Y., Wei, G., Zheng, J., Zou, J., Wei, Y., Zhang, Y., and Li, H. Make pixel dance: High-dynamic video generation. _ arXiv preprint arXiv:2311.10982_, 2023.\n' +
      '* Zhang et al. (2016) Zhang, B., Wang, L., Wang, Z., Qiao, Y., and Wang, H. Enhanced Motion Vector CNNs를 이용한 실시간 동작 인식. _CVPR_, pp. 2718-2726, 2016.\n' +
      '* Zhang et al.(2023) Zhang, H., Li, X., and Bing, L. Video-LLaMA: 비디오 이해를 위한 명령어-조정된 시청각 언어 모델. _ arXiv preprint arXiv:2306.02858_, 2023.\n' +
      '\n' +
      '## 부록 A 실험 설정\n' +
      '\n' +
      '### 모델 구현 세부사항\n' +
      '\n' +
      '**비디오 토큰화기** 비디오 키프레임을 평균 90개의 비주얼 토큰으로 변환하기 위해 LaVIT(Jin et al., 2024)로부터의 기성 비주얼 토큰화기를 채용하며, 이는 EVA-CLIP(Fang et al., 2023)의 ViT-G/14를 비주얼 인코더로서 활용하기 위해 대부분의 기존 MLLM에 따른다. 시각 코드북 크기는 16384로 설정되어 있습니다. 자세한 내용은 원본 용지를 참고하시기 바랍니다. 학습 및 추론 과정에서 영상과 키프레임은 입력으로 224\\(\\times\\)224 해상도로 크기가 조정된다.\n' +
      '\n' +
      '모션 토큰화는 원본 비디오를 6 fps로 다운샘플링한 후 24개의 프레임을 비디오 클립으로 사용하여 모션 벡터 \\(M\\)을 계산한다. 이를 다시 해당 동영상의 가로와 세로로 나누어 \\([-1,1]\\)의 범위 내의 값을 정규화한다. 모션토키나이저에 입력되기 전에, 모션벡터\\(M\\)은 16\\(\\times\\)16의 해상도로 크기가 조정되어 최종 입력 텐서 형상은 \\(B\\times 24\\times 16\\times 16\\times 2\\)이 된다. 제안된 모션토너마이저의 인코더(f_{\\mathcal{E}}\\)와 디코더(f_{\\mathcal{D}}\\)는 모두 512개의 은닉 상태와 8개의 주의 헤드를 갖는 12개의 트랜스포머 블록을 갖는다. 각 블록은 공간, 시간 주의 및 피드 포워드 계층으로 구성된다. 어텐션 계산 전에, 모션 입력은 공간 및 시간 레이어에 대해 각각 \\([(B*24)\\times(16*16)\\times D]\\) 및 \\([(B*16*16)\\times 24\\times D]\\)으로 재형성된다. 본 논문에서는 [\\(\\times\\)2,\\(\\times\\)2,\\(\\times\\)2,\\(\\times\\)2] 다운샘플링을 수행하기 위해 인코더 블록 뒤에 3D 평균 풀링 레이어를 삽입한 후 256개의 이산 모션 토큰으로 양자화한다. 64개의 모션 토큰이 있는 버전의 경우, 모션 임베딩의 공간 형상도 양자화 전에 다운샘플링될 것이다. 디코더(f_{\\mathcal{D}}\\)는 학습시 원래의 입력 움직임 벡터를 복원하기 위해 대칭적인 업샘플링 레이어를 포함한다. 학습된 모션 코드북의 크기를 1024로 설정하고, 모션 코드북의 학습 안정성을 향상시키기 위해 가중치 \\(0.995\\)의 지수 이동 평균(EMA) 업데이트를 활용한다. 양자화 전에, 움직임 임베딩들은 Yu 등의 경험에 따라, 코드북 사용을 개선하기 위해 저차원 공간으로 투영된다(2022).\n' +
      '\n' +
      '**비디오 디토키저** 비디오 디토키저를 훈련하는 동안 6 fps로 다운샘플링된 비디오에서 24개의 연속 프레임을 무작위로 샘플링했다. 모션 컨디셔닝 인코더는 입력 비디오 프레임과 동일한 시간 차원을 유지하기 위해 다운샘플 레이어를 제거한 것을 제외하고는 \\(f_{\\mathcal{E}}\\)과 동일한 트랜스포머 아키텍처(12 블록)를 갖는다. 이 전략은 인코딩 동안 모션 정보의 압축을 감소시키고 3D U-Net에서 잡음제거될 각각의 프레임에 대해 명시적인 안내를 제공한다. 사용된 3D U-Net의 상세한 아키텍처는 Blattmann et al.(2023b, 2023a)과 동일한 구현들을 따른다. 디토키나이저를 위한 EDM-프리컨디셔닝 최적화 동안, \\(\\log\\sigma\\)의 분포는 \\(\\mathcal{N}(1.0,1.2^{2})\\)으로 설정되어 더 높은 잡음 레벨을 유도하며, 이는 고해상도 생성에 효과적인 것으로 밝혀졌다 (Girdhar et al., 2023). 3D U-Net에서 모션 컨디셔닝 인코더, 입력 인코딩 레이어 및 모든 크로스 어텐션 레이어를 처음부터 트레이닝하고, SVD img2vid-xt에서 다른 가중치들을 초기화한다(Blattmann et al., 2023a). 디토키저(detokenizer)는 계산 복잡도를 줄이기 위해 먼저 50k 단계의 해상도 384\\(\\times\\)384로 학습한 후, 다른 10k 단계의 해상도 768\\(\\times\\)768 또는 1024\\(\\times\\)576의 두 가지 해상도로 미세 조정한다.\n' +
      '\n' +
      '**Language Model** Llama 2 7B(Touvron et al., 2023b)를 생성 사전 훈련을 위한 디폴트 큰 언어 모델로 활용한다. 언어 모델의 가중치는 이미지 도메인에 대한 이해와 생성을 지원하기 위해 학습된 시각적 사전 지식을 보존하기 위해 LaVIT(Jin et al., 2024)로부터 초기화된다. 사전 학습 동안 이미지-텍스트, 비디오-텍스트 쌍 및 텍스트 데이터를 하나의 배치로 혼합하여 최종 멀티모달 입력 시퀀스를 형성한다.\n' +
      '\n' +
      '### Pre-training Data\n' +
      '\n' +
      'Video-LaVIT에서 사용하는 학습 데이터셋은 공개된 이미지와 비디오 데이터셋으로만 구성되어 있다. 다음에서는 각 훈련 단계에서 데이터 세트 사용에 대한 자세한 설명을 제시한다.\n' +
      '\n' +
      '**단계 1:** 비디오 토큰타이저 및 디토키나이저는 WebVid-10M(Bain et al., 2021) 상에서 트레이닝되며, 이는 스톡 비디오 사이트들로부터 스크래핑된 1000만 비디오-텍스트 쌍들을 포함하는 오픈-소스 비디오-텍스트 데이터세트이다. 토코나이저와 디토키나이저는 모두 텍스트 데이터에 의존하지 않기 때문에 이 단계에서는 순수 비디오 데이터만 사용한다. WebVid-10M의 공통 워터마크 때문에, 비디오 디토키나이저의 트레이닝 동안, 우리는 생성된 비디오들에서 워터마크를 제거하기 위해 InterVid-14M-미학의 서브세트를 통합한다(Wang et al., 2024). 또한 PixelDance(Zeng et al., 2023)에서도 유용한 것으로 나타났다. 구체적으로, 먼저 가장 높은 미적 점수를 갖는 4s-10s 비디오 클립의 서브세트를 선택한 다음, CRAFT(백 등, 2019)를 적용함에 있어서 SVD(Blattmann 등, 2023a)를 따라 원하지 않는 텍스트가 있는 비디오들을 필터링한다. 그 결과 약 300k의 공개적으로 이용 가능한 비디오 클립이 포함되어 있다. **300k 비디오 서브세트가 생성된 비디오의 미감을 향상시키기 위해 비디오 디토키나이저의 트레이닝 동안에만 사용된다는 것에 주목하여, 모든 실험에서 보고된 결과는 WebVid-10M 데이터세트만을 사용하는 체크포인트에서 테스트된다.**\n' +
      '\n' +
      '**단계 2:** 언어 모델은 WebVid-10M (Bainet al., 2021); Conceptual Caption Sharma et al. (2018); Changpinyo et al. (2021), SBU Ordonez et al. (2011), 및 BLIP-Capfilt Li et al. (2022)의 93M 샘플들을 포함하는 비디오, 이미지 및 텍스트 데이터의 혼합물 상에서 사전 트레이닝된다. 또한, LLaMA를 처음부터 훈련하기 위해 원본 데이터와 같은 오픈 소스 데이터인 RedPajama Together Computer(2023)의 영어 텍스트 코퍼스를 사용한다. 사전 훈련 동안 영어 텍스트 코퍼스를 포함하는 목적은 좋은 멀티모달 능력을 획득하면서 LLM의 이미 학습된 언어 이해 능력(예를 들어, MMLU Hendrycks et al. (2021))을 보존하는 것이다.\n' +
      '\n' +
      '**단계 3:** 공정한 비교를 위해, 우리는 이 단계 동안 기존의 작업 Lin et al.(2023); Li et al.(2023)과 동일한 명령어 튜닝 데이터세트를 사용한다. LLaVA v1.5 Liu 등(2023)으로부터의 665k 이미지-텍스트 명령 데이터세트 및 Video-ChatGPT Maaz 등(2023)으로부터의 100k 비디오-텍스트 명령 데이터세트를 포함한다.\n' +
      '\n' +
      '### Training Settings\n' +
      '\n' +
      'Video-LaVIT에서 비디오 토큰다이저, 디토키나이저, 언어 모델에 대한 상세한 트레이닝 하이퍼-파라미터 설정은 표 6에 보고되어 있다. LLaVA v1.5 Liu 등(2023)과 동일한 명령어 튜닝 설정을 채택한다.\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      '**Image Understanding**은 8개의 인기 있는 이미지 질의 응답 및 멀티모달 벤치마크: VQA v2 Goyal et al. (2017), GQA Hudson and Manning (2019), VizWiz Gurari et al. (2018), ScienceQA-IMG Lu et al. (2022), MME Fu et al. (2023), MMBench Liu et al. (2023), SEED Li et al. (2023), MM-Vet Yu et al. (2023). 질문 응답 데이터 세트의 경우 LLaVA-1.5 Liu et al.(2023)과 동일한 프롬프트를 사용하고 널리 사용되는 VQA 정확도를 평가 메트릭으로 채택한다.\n' +
      '\n' +
      '**비디오 질문 답변** 3개의 공통 데이터 세트가 고려된다: MSVD-QA Chen and Dolan (2011), MSRVTT-QA Xu et al.(2016) 및 ActivityNet-QA Yu et al.(2019). 모델 정확도를 평가하기 위해 GPT-3.5 어시스턴트 마즈 등(2023)이 사용되며, 이는 또한 0 내지 5 범위의 상대 점수를 출력한다.\n' +
      '\n' +
      '**텍스트 대 이미지 생성** 우리는 MS-COCO Lin et al.(2014)의 검증 세트를 채택하고 30K 샘플을 무작위로 선택한다. 생성된 영상의 품질은 FID(Frechet Inception distance) Heusel et al.(2017)에 의해 평가되며, 이는 미리 훈련된 Inception V3 모델의 특징 공간에서 그라운드 트루스까지의 자신의 Frechet distance를 계산한다.\n' +
      '\n' +
      '**Text-to-Video Generation**는 MSR-VTT Xu et al.(2016) 및 UCF-101 Soomro et al.(2012)에서 측정된다. MSR-VTT의 경우, 2990개의 비디오를 모두 사용하고 각 비디오에 대해 하나의 캡션을 샘플링하여 2990개의 비디오-텍스트 쌍을 생성하며, UCF-101의 경우, 클래스당 20개의 비디오를 샘플링하고 PYoCo Ge et al.(2023)을 따라 각 클래스에 대한 프롬프트를 큐레이션하여 2020개의 비디오-텍스트 쌍을 생성한다. 그들의 평가 지표는 아래에 자세히 설명되어 있습니다.\n' +
      '\n' +
      '* CLIP 유사성(CLIPSIM) Wu 등(2021)은 비디오-텍스트 쌍들 사이의 의미론적 유사성을 측정한다. 우리는 224\\(\\times\\)224 크기의 비디오 프레임과 그들의 대응하는 캡션들 사이의 CLIP 스코어들을 계산하기 위해 ViT-B/16 Radford et al. (2021)을 사용하는 데 있어서 Phenaki Villegas et al. (2023) 및 VideoPoot Kondratyuk et al. (2023)을 따른다. 상기 최종 점수는,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Configuration & Language Model & Tokenizer & Detokenizer \\\\ \\hline LLM init & LaVIT-7B & - & - \\\\ Optimizer & AdamW & AdamW & AdamW \\\\ Optimizer Hyperparameters & \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.95\\), \\(\\epsilon=1e^{-6}\\) & \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.99\\), \\(\\epsilon=1e^{-6}\\) & \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.99\\), \\(\\epsilon=1e^{-6}\\) \\\\ Global batch size & 2048 & 512 & 128 \\\\ Peak learning rate of LLM & 2e-5 & - & - \\\\ Peak learning rate of other Part & 5e-5 & 1e-4 & 5e-5 \\\\ Learning rate schedule & Cosine & Cosine & Cosine \\\\ Training Steps & 30K & 100K & 60K \\\\ Warm-up steps & 2k & 5K & 3K \\\\ Weight decay & 0.1 & 0.001 & 0.001 \\\\ Gradient clipping & 1.0 & 1.0 & 1.0 \\\\ Input sequence to LLM & 2048 & - & - \\\\ Numerical precision & bfloat16 & bfloat16 & bfloat16 \\\\ GPU Usage & 128 NVIDIA A100 & 64 NVIDIA A100 & 64 NVIDIA A100 \\\\ Framework & Megatron & DeepSpeed & DeepSpeed \\\\ Training Time & 60h & 10h & 48h \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 생성된 모든 비디오 프레임에 걸쳐 Video-LaVITaveraged의 상세한 트레이닝 하이퍼파라미터.\n' +
      '* Frechet 비디오 거리(FVD)(Unterthiner et al., 2018)는 Kinetics-400에서 미리 훈련된 I3D 액션 분류 모델(Carreira and Zisserman, 2017)의 특징 공간에서 생성된 비디오와 실제 비디오 사이의 Frechet 거리를 평가한다(Kay et al., 2017).\n' +
      '* Frechet Inception 거리(FID)(Heusel et al., 2017)는 생성된 비디오 프레임과 실제 비디오 프레임 사이의 Frechet 거리를 측정한다. PYoCo(Ge et al., 2023)에 이어서, 우리는 프레임 특징들을 추출하기 위해 ViT-B/32 모델(Kynkaanniemi et al., 2023)을 사용한다. 최종 결과는 모든 비디오 프레임에 걸쳐 평균화된다.\n' +
      '* 인셉션 스코어(IS)(Saito et al., 2020)는 생성된 비디오 프레임들의 분포를 평가한다. 우리는 개시 점수의 비디오 버전을 계산하기 위해 UCF-101에서 미세 조정된 C3D 모델(Tran et al., 2015)을 채용한다. 모델은 각 비디오의 중앙 16프레임을 입력으로 한다.\n' +
      '\n' +
      '다른 논문의 평가 프로토콜에는 약간의 차이가 있다. 우리는 프로토콜을 대부분의 상위 방법과 동일하거나 유사하게 유지하려고 노력했다.\n' +
      '\n' +
      '## 부록 B 추가 결과\n' +
      '\n' +
      '### Multimodal Generation\n' +
      '\n' +
      '이 섹션에서는 본 논문의 기존 비교를 보완하여 멀티모달 생성에 대한 설계의 효과를 입증하기 위한 추가 정성적 결과와 절제 연구를 제공한다.\n' +
      '\n' +
      '**텍스트 대 이미지 생성** 도 6은 Video-LaVIT와 SDXL 사이의 텍스트-대-이미지 생성의 비교를 예시한다(Podell et al., 2024). 전반적으로, 우리의 방법은 더 나은 언어 이해 및 추론 능력을 가지면서도 경쟁력 있는 시각적 품질을 달성한다. 예를 들어, UFO 앞에 있는 젊은 여성의 왼쪽 상단 사례에서 본 방법은 텍스트 프롬프트에서 "날카로운 초점"의 세부 사항을 캡처하면서 여성의 높은 미적 헤드샷을 생성한다. 그리고 사과 그림의 왼쪽 하단 예에서, 우리의 모델은 우리가 채택한 LLM 기반 생성 접근법의 더 나은 논리적 추론 능력 덕분에 두 개의 녹색 사과를 그리기 위해 "둘 다 빨간색이고 둘 다 녹색"이라는 프롬프트에서 성공적으로 추론한다.\n' +
      '\n' +
      '**텍스트 투 비디오 생성** 도 7은 Video-LaVIT를 폐쇄 소스 모델 Gen-2(Runaway, 2023)와 비교한다. 보이는 바와 같이, 본 모델은 일반적으로 Gen-2에 필적하는 고품질 비디오를 생성하며, 이는 텍스트 프롬프트에서 "이끼와 많은 꽃" 및 "가을"과 같은 세부 사항을 성공적으로 캡처하고 Gen-2와 매우 유사한 결과를 산출하는 마지막 두 예에서 특히 분명하며, 처음 두 비교는 본 모델의 유리한 프롬프트 추종 능력을 보여준다. "달리기"라는 키워드가 있는 첫 번째 경우, 본 모델은 캐빈을 향해 상당한 카메라 움직임을 생성하는 반면 Gen-2의 움직임은 상대적으로 미묘한 차이를 보인다. 두 번째 경우, 우리의 모델은 "강렬한 전투"의 의미에 따라 모든 선박에 불이 붙는 것과 같은 예술적 세부 사항과 함께 프롬프트가 지정된 대로 여러 개의 "해적선"을 올바르게 표시한다. 이러한 결과는 신속한 후속 기능에서 통일된 비디오 언어 사전 훈련의 이점을 뒷받침한다.\n' +
      '\n' +
      '**이미지 대 비디오 생성** 도 8은 비디오-LaVIT와 오픈 소스 모델 SVD(Blattmann et al., 2023)의 비교를 제시하며, 둘 다 합성 이미지 프롬프트에 컨디셔닝된다. 보이지 않는 테스트 사례들로 이동하여, 본 방법은 움직임과 관련된 지식을 새로운 시각적 입력으로 더 잘 전달할 수 있는 분해된 비디오 표현 덕분에 자연스러운 동작과 정제된 동작을 모두 특징으로 하는 비디오 클립을 생성한다. 예를 들어, 중간 케이스에서, 우리의 생성된 염소는 머리를 부드럽게 내리고 생각하는 사람처럼 깜박이는 반면, SVD로 제작된 비디오의 염소는 거의 움직이지 않았다. 영상 프롬프트가 테디가 오토바이를 타고 있는 것을 보여주는 하부 케이스에서, 생성된 전체 비디오는 오토바이를 타고 있는 사람과 매우 자연스럽고 유사하게 보이는 반면, SVD는 오토바이가 타이어가 가리키는 방향(물리적으로 잘못된 것)과 다른 방향으로 이동하는 시나리오를 지속적으로 생성한다. 전반적으로, 본 모델은 디커플드 비주얼 모션 토큰화 및 LLM 사전 트레이닝을 포함하여 우수한 이미지 대 비디오 생성 성능을 보여준다.\n' +
      '\n' +
      '**긴 비디오 생성**은 그림 9에 나와 있다. 연속적인 비디오 클립들을 디코딩할 때 잡음을 명시적으로 제한함으로써, 우리의 모델은 긴 비디오 생성 동안 높은 시간적 일관성을 제공할 수 있다. 예를 들어, 처음 두 경우에, 개와 지프차는 매우 일관된 시각적 세부 사항으로 서로 다른 클립에 걸쳐 동일한 동일성을 유지한다. 큰 카메라 움직임을 특징으로 하는 마지막 예에서, 이동 궤적은 텍스트 프롬프트에 따라 캐빈에 접근함에 따라 일관성을 유지한다. 이 예들은 모두 긴 비디오 생성의 우리의 상당히 좋은 품질을 보여준다. 생성된 모든 비디오는 [https://anonymous-icml-2024.github.io/video-lavit](https://anonymous-icml-2024.github.io/video-lavit)에서 제공된다는 점에 유의한다.\n' +
      '\n' +
      '**절제 연구** 본 논문에서는 3D 비디오 U-Net(g_{V}\\)을 학습하기 위해 모션 입력 조건과 모션 특징 조건인 향상된 컨디셔닝을 적용한다. 그림 10에서 제안된 향상된 모션 컨디셔닝(EMC) 전략이 비디오 디코딩에 미치는 영향을 설명한다. 변형 "w/o EMC"는 입력 조건으로서 모션 벡터만을 활용한다. EMC를 사용하는 것과 비교할 때, 원본 입력 비디오의 움직임을 복구할 수 없다. 예를 들어, "열차"와 "생선"은 표시된 비디오 샘플에서 거의 움직이지 않았으며, 이는 제안된 컨디셔닝 전략의 효과를 보여준다.\n' +
      '\n' +
      '### Multimodal Understanding\n' +
      '\n' +
      '본 절에서는 영상 및 영상 이해를 위한 Video-LaVIT의 질적 결과를 제시한다. 먼저, 표 7은 GPT-4(OpenAI, 2023a)의 유명 테스트 예를 이용하여 이미지 질의 응답에서 우리의 성능을 보여준다. 보이는 바와 같이, 우리의 모델은 많은 정확한 세부 사항(예: SUV인 차량의 유형)과 심지어 친절한 안전 경고까지 갖춘 합리적인 답변을 생성한다. 이에 비해, 최근의 멀티모달 LLMs 중 하나인 LLaVA(Liu et al., 2023b)는 다소 부정확한 세부사항(차종을 "미니반 또는 밴"으로 오인함)과 함께 대략적인 정답을 생성한다.\n' +
      '\n' +
      '비디오 질문 응답을 위해, 표 8 및 표 9는 비디오-ChatGPT로부터의 비디오 클립들에 기초하여 우리의 방법을 Video-LLaVA(Lin et al., 2023) 및 Video-ChatGPT(Maaz et al., 2023)와 비교한다. 동영상이 재미있는 이유를 설명하라는 표 8의 첫 번째 예에서, 우리의 모델은 비교된 비디오 언어 모델 중 가장 간결한 답을 얻음과 동시에 다른 모델이 언급하지 않은 두드러진 점을 포함한다. 반면에 표 8의 다음 예는 후자의 두 모델이 비디오에 기반이 없는 과도하게 상세한 액션 설명을 생성하는 경향이 있기 때문에 비디오-LLaVA 및 비디오-ChatGPT보다 우리의 방법이 더 적은 환각을 생성한다는 것을 보여준다. 그리고 마지막으로 표 9의 예에서, 우리의 모델은 간결함과 도덕적 교훈을 모두 가진 아름다운 동화를 생산함으로써 지시 프롬프트를 따른다("사랑은 모두를 정복할 수 있다") 요약하자면, 우리의 방법은 여러 벤치마크에 대한 이전의 정량적 비교에 따라 다양한 테스트 사례에 걸쳐 합리적으로 좋은 다중 모드 이해 능력을 보여준다.\n' +
      '\n' +
      '## 부록 C 제한사항\n' +
      '\n' +
      '제안된 모델은 제한된 컨텍스트 창(4096)으로 인해 100대 이상의 매우 긴 비디오를 직접 처리할 수 없다. 가능한 해결책은 더 적은 수의 비디오 키프레임을 샘플링하는 것이지만, 일부 미묘한 모션은 리샘플링 후에 캡처되지 않을 수 있다. 이 문제를 해결하기 위해 움직임 벡터의 분해능은 인간의 행동과 같은 매우 미세한 움직임을 모델링하는 병목 현상이 될 수 있다. 반면에, 일반적인 우려는 우리의 훈련 비용이 여전히 너무 높아서 웹 스케일 비디오 데이터를 확장할 수 없으며, 이는 비디오에서 공간적 및 시간적 중복성의 공동 활용을 통해 추가 최적화가 필요할 수 있다는 것이다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '도 7: 기본 파라미터를 이용한 Gen-2(Runaway, 2023)와의 텍스트 대 비디오 생성 비교. Prompts는 VideoPoet(Kondratyuk et al., 2023) 및 PixelDance(Zeng et al., 2023)로부터 왔다. 우리의 모델은 텍스트 프롬프트를 더 잘 따르면서 유사하게 높은 시각적 품질(아래 두 가지 경우)을 제공한다(첫 번째 예에서는 "달리기"를, 두 번째 예에서는 "해적선"을 포함한다).\n' +
      '\n' +
      '도 8: 안정-비디오-확산-img2vid-xt 버전을 사용하는 SVD(Blattmann et al., 2023a)와의 이미지-대-비디오 생성 비교. 세대는 가장 왼쪽 프레임에서 조정됩니다. 우리의 모델은 물리적 규칙을 위반하지 않으면서(예를 들어, 두 번째 마지막 행에서 오토바이가 타이어 방향으로 움직이지 않고) 더 정교한 동물 동작을 생성할 수 있다(상위 두 가지 경우를 참조).\n' +
      '\n' +
      '도 9: 긴 비디오 생성 예. Prompts는 Gen-L-Video(Wang et al., 2023) 및 VideoPoot(Kondratyuk et al., 2023)이다. 생성된 비디오는 제안된 명시적 노이즈 제약 덕분에 서로 다른 디코딩된 클립에서도 시간적으로 일관성이 있다.\n' +
      '\n' +
      '도 10: 비디오 재구성을 위한 강화된 모션 컨디셔닝(EMC)의 절제 연구. 마지막 행(즉, w/o EMC)은 3D U-Net을 트레이닝하기 위한 입력 조건으로서 모션 벡터들만을 나타낸다. 예시된 바와 같이, EMC를 통합하면 재구성된 비디오의 충실도를 향상시킬 수 있다. 비디오 샘플은 WebVid-10M(Bain et al., 2021)으로부터 채취된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline User & What is unusual about this image? \\\\ LLaVA (Liu et al., 2023b) & The unusual aspect of this image is a man ironing clothes on the back of a minivan or van. This is not a typical place to perform this activity, as one would usually iron clothes in a more stationary and safe location, such as a home, using a regular ironing board. The scene depicted in the image is peculiar as it involves a makeshift ironing setup on a vehicle, which can be both unsafe and unconventional. Additionally, it is not clear how the man is able to maintain balance and stability while ironing clothes in such an unstable environment. [_Start a new conversation, and clear the history_] \\\\ User & What’s happening in the scene? \\\\ LLaVA (Liu et al., 2023b) & The image depicts a man standing on top of a yellow SUV in a busy city street. He is holding a portable ladder, seemingly ironing clothes while standing on the vehicle. Around the scene, there are other cars, a traffic light, and a fire hydrant. Additionally, there are two other people in the vicinity, one close to the man on the car and another further away. The man on the ladder appears to be attracting attention due to his unconventional choice of ironing his clothes on top of a moving car. The city street around him is bustling with activity, adding to the unique nature of the scene. \\\\ \\hline User & What is unusual about this image? \\\\ GPT-4 (OpenAI, 2023a) & The unusual thing about this image is that a man is ironing clothes on an ironing board attached to the roof of a moving taxi. \\\\ \\hline User & Can you explain this meme in detail? \\\\ BLIP-2 (Li et al., 2023c) & a man is sitting on the back of a yellow cab \\\\ \\hline User & Can you explain this meme in detail? \\\\ OpenFlamingo (Awadalla et al., 2023) & The man is drying his clothes on the hood of his car. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: LLaVA(Liu et al., 2023b), GPT-4(OpenAI, 2023a), BLIP-2(Li et al., 2023c), OpenFlamingo(Awadalla et al., 2023)와의 이미지 질의 응답 비교. 우리 모델은 다른 멀티모달 LLM과 마찬가지로 정확하게 응답하지만 차량 유형(밴이나 미니밴 대신 SUV)과 같은 세부 정보가 누락되지 않습니다. 표 형식은 GPT-4를 따른다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{113.8pt} p{284.5pt}} \\hline \\hline User & Why is this video funny? \\\\ Video-LaVIT & The video is funny because the baby is wearing glasses and trying to read a book, but she is not able to do so. The baby’s attempts to read the book are adaptable and amusing to watch. \\\\ \\hline User & Why is this video funny? \\\\ Video-LLaVA (Lin et al., 2023) & Why is this video funny? \\\\ Video-LLaVIT (Maaz et al., 2023) & Why is this video funny? \\\\ Video-ChatGPT (Maaz et al., 2023) & The video is funny because it shows a baby girl who is learning to read. She is sitting on a bed and looking at a book, and she is trying to read it. The baby girl is wearing glasses, and she is holding the book with both hands. The baby girl is trying to read the book, and her actions are adorable and endearing. The video captures the innocence and curiosity of a young child as she learns to read, which is a joyful and amusing sight for the viewer. \\\\ \\hline User & What does the woman use to split the logs and how does she do it? \\\\ Video-LaVIT & The woman uses an ax to split the logs. She picks up the ax and swings it down with force, splitting the logs in half. \\\\ \\hline User & What does the woman use to split the logs and how does she do it? \\\\ Video-LLaVA (Lin et al., 2023) & The woman uses an ax to split the logs. She holds the ax in her hand and swings it down onto the log. She repeats this process several times, splitting the log into smaller pieces. \\\\ \\hline User & What does the woman use to split the logs and how does she do it? \\\\ Video-ChatGPT (Maaz et al., 2023) & The woman uses a large ax to split the logs. She starts by placing the axes on the log and then swings it down with a powerful motion. The axe slices through the log with ease, and the woman continues to swing it until the log is split in half. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: Video-LLaVA(Lin et al., 2023) 및 Video-ChatGPT(Maaz et al., 2023)와의 Video question answering 비교. 비디오 예들은 Video-ChatGPT에서 나온 것이다. 첫 번째 예에서, 우리의 모델은 비디오가 재미있는 이유에 대한 두드러진 설명을 직접적으로 지적한다. 두 번째 예에서, 우리의 모델은 비디오에 대해 환각을 보지 않고 정확하게 반응한다(여자는 도끼를 한 번만 휘둘렀다).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{142.3pt} p{142.3pt}} \\hline \\hline User & Create a short fairy tale with a moral lesson inspired by the video. \\\\ Video-LAVIT & Once upon a time, there was a beautiful princess who lived in a grand castle. She was known for her kindness and compassion towards others. One day, a handsome prince vasited her castle and fell deeply in love with her. However, the princess was hesitant to accept his love due to her fear of losing her independence and being trapped in a loveless marriage. The prince, determined to win her heart, gave her a beautiful glass slipper as a gift. The princess tried it on and danced with the prince, realizing that true love transcends all boundaries. From that day on, the princess and the prince lived happily ever after, proving that love can conquer all. \\\\ \\hline User & Create a short fairy tale with a moral lesson inspired by the video. \\\\ Video-CLAVA (Lin et al., 2023) & Once upon a time, in a kingdom far away, there lived a beautiful princess named Rose. She was known for her kindness and beauty, and everyone loved her. One day, a handsome prince named Leo came to visit the kingdom. He was fascinated by Rose’s beauty and decided to stay in the kingdom. However, Rose’s jealous sister, Lily, did not like the attention Leo was giving her sister. She tried to harm Rose, but Leo saved her and they fell in love. The kingdom celebrated their love and they lived happily ever after. The moral of the story is that true love conquers all and that jealousy can lead to harmful consequences. \\\\ \\hline User & Create a short fairy tale with a moral lesson inspired by the video. \\\\ Video-ChatGPT (Maaz et al., 2023) & Sure, here’s a short fairy tale with a moral lesson inspired by the video: Once upon a time, there was a beautiful princess named Cinderella who lived in a castle with her cruel stepmother and two stepsisters. She was known for her kindness and her love for animals, especially birds. One day, while she was out in the garden, she met a handsome prince who was lost in the woods. He was in need of help, and Cinderella offered to guide him back to the castle. As they walked together, the prince noticed that Cinderella’s stepsisters were cruel to her and her and stepmother was always mean to her. He asked her about her life and how she managed to stay so kind and gentle despite the hardships she faced. Cinderella told him that she found happiness in the simple things in life, such as the beauty of nature and the love of animals. The prince was touched by her story and decided to help her in any way he could. He introduced her to the king and queen, who were impressed by her kindness and grace. They arranged for her to attend a ball at the castle, where she met the prince once again. They fell in love and got married, living happily ever after. The moral of the story is that even in difficult situations, one should find happiness in the simple things in life and never lose sight of their values and kindness. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: Video-LLAVA(Lin et al., 2023) 및 Video-ChatGPT(Maaz et al., 2023)와의 Video question answering 비교. 비디오 예들은 Video-ChatGPT에서 나온 것이다. 이 예에서 볼 수 있듯이 우리 모델은 러브 스토리인 이 비디오의 기본 컨텍스트를 캡처합니다. 사랑이 지배할 것임을 암시하는 동화는 다른 모델들과 마찬가지로 언급되지 않은 제3자(질투하는 자매나 계모)를 소개하지 않아도 자연스럽게 영상에서 영감을 받는다. “아름다운 유리구두”라는 문구를 사용하는 것은 이야기에 멋진 디테일을 더한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>