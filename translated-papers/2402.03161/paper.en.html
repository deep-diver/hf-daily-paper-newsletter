<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'a 3D video tokenizer for video generation with LLM, its applicability is constrained to short video clips due to the use of long token sequences (e.g., 1280 tokens for a 2.2s clip). When it comes to understanding or generating long videos, inputting excessive numbers of tokens into LLMs is deemed unacceptable in terms of computational resources.\n' +
      '\n' +
      'This work addresses the limitation in video-language pre-training by exploring an efficient video representation that decomposes video into keyframes and temporal motions. Our motivation is built upon the natural characteristics of video data itself. As illustrated in Figure 1, a video is typically divided into several shots, where video frames within each shot often exhibit substantial information redundancy. It is superfluous to encode all of these frames as tokens and incorporate them into the generative pre-training of LLMs. This fact strongly spurs us to decompose each video into alternating keyframes and motion vectors, where the former encapsulate the primary visual semantics and the latter depict the dynamic evolution of its corresponding keyframe over time. There are several benefits to such decomposed representation: (1) Compared to processing consecutive video frames utilizing 3D encoders, the combination of a single keyframe and motion vectors requires fewer tokens to represent video temporal dynamics, which is more efficient for large-scale pre-training. (2) The model can inherit the acquired visual knowledge from an off-the-shelf image-only LLM and focus solely on modeling temporal information without learning from scratch.\n' +
      '\n' +
      'Based on the above motivations, we present **Video-LaVIT (L**anguage-**V**Ision **T**ransformer), a new multimodal pre-training approach that effectively empowers LLMs to comprehend and generate video content in a unified framework. Specifically, Video-LaVIT incorporates two core components: a _tokenizer_ and a _detokenizer_ to handle video modality. The video tokenizer aims to transform the continuous video data into a sequence of compact discrete tokens akin to a foreign language, where the keyframes are processed by utilizing an established image tokenizer (Jin et al., 2024). For converting the temporal motions into the compatible discrete format, a spatiotemporal motion encoder is devised. It can capture the time-varying contextual information contained in extracted motion vectors, thereby significantly enhancing LLMs\' ability to comprehend the intricate actions in video. The video detokenizer is responsible for mapping the discretized video token generated by LLMs back into its original continuous pixel space. During training, video is represented as an alternating discrete visual-motion token sequence, and thus can be optimized under the same next-token prediction objective together with different modalities. Since video is inherently a time series, this joint autoregressive pre-training contributes to learning the sequential relationships of different video clips. We found that Video-LaVIT, is capable of serving as a multimodal generalist to achieve promising results in both understanding and generation tasks without further fine-tuning. The key contributions of this work are summarized as:\n' +
      '\n' +
      '* We introduce Video-LaVIT, a multimodal pre-training method that pushes the limit of LLMs\' unified understanding and generation capability towards video.\n' +
      '* To efficiently model visual and temporal information in video, Video-LaVIT incorporates a novel video tokenizer and detokenizer that operates on the decomposed representations of keyframes and motion vectors.\n' +
      '* Comprehensive quantitative and qualitative experiments demonstrate that Video-LaVIT achieves very competitive performance ranging from image and video comprehension to text-to-video and image-to-video generation.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '**Vision-language pre-training**. Following the success of using large-scale image-text pairs for contrastive learning of vision-language models (Radford et al., 2021), a similar idea has been exploited in generative pre-training, where visual and language data are jointly modeled under an autoregressive process. In practice, this is typically achieved by adapting visual image inputs to pre-trained LLMs (Raffel et al., 2020; Brown et al., 2020; Touvron et al., 2023) via an intermediate module like cross-attention (Alayrac et al., 2022), Q-Former (Li et al., 2023), or linear projection (Liu et al., 2023). More recent approaches such as CM3Leon (Yu et al., 2023) and LaVIT (Jin et al., 2024) advocate the use of discrete visual tokenizers (van den Oord et al., 2017; Esser et al., 2021) to form a unified next token prediction objective. However, these methods are primarily focused on image-text data and cannot be directly extended to videos due to the significantly higher computational cost.\n' +
      '\n' +
      '**Video understanding and generation**. By unifying videos in the above pre-training framework, remarkable progress has been made in video comprehension with masked (Yang et al., 2022) and autoregressive language models (Li et al., 2023; Zhang et al., 2023; Maaz et al., 2023). However, for video generation, the mainstream approaches are still based on diffusion models (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Ho et al., 2020), which enhance existing image pre-trained models with better temporal consistency (Ho et al., 2022; Singer et al., 2023; Blattmann et al., 2023; Esser et al., 2023; Blattmann et al., 2023). Language model based counterparts (Yan et al., 2021; Hong et al., 2023; Kondratyuk et al., 2023), on the other hand, face the critical challenge of efficiently encoding video temporal dynamics with limited context windows and computational resources. In response, our work leverages motion vectors, a classic and effective cue in video modeling (Zhang et al., 2016; Wang et al., 2023; Shen et al., 2024), for improving the efficacy of LLM-based video comprehension and generation.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      'This work aims to present an effective pre-training framework that harnesses the exceptional modeling capability of Large Language Models (LLMs) to facilitate the learning of video modality. In pursuit of this goal, we highlight two core designs: a video _tokenizer_ (Section 3.1) which allows for the representation of all modalities in a unified discrete form, and a video _detokenizer_ (Section 3.2) to map the generated discrete tokens back to the continuous pixel space. Coped with these two main components, Video-LaVIT can be optimized through a unified autoregressive training paradigm (Section 3.3), enabling it to simultaneously comprehend and generate various multimodal content.\n' +
      '\n' +
      '### Video Tokenization\n' +
      '\n' +
      'To encode an untrimmed video as inputs to LLMs, the prevailing approaches (Lin et al., 2023; Li et al., 2023) mainly uniformly downsample the original video into a series of frames. Then, a pre-trained ViT encoder (Radford et al., 2021; Fang et al., 2023) is employed to separately encode these frames and produce a sequence of frame-level embeddings as the video representation. This straightforward way disregards the modeling of temporal dynamics between frames, thus impeding the capacity to understand the actions and camera transitions occurring in the video. While the utilization of 3D video encoders the very recent (Kondratyuk et al., 2023) enables the encoding of temporal information, it only applies to short video clips and inevitably yields a substantial proliferation of tokens (e.g., 1280 tokens for one 2.2s clip), resulting in a heavy computational overhead.\n' +
      '\n' +
      '**Motion-aware Video Decomposition**. Given the above concerns, our proposed video tokenizer seeks to integrate temporal dynamics into the video representations efficiently. We observe that a video clip captured in the same shot can convey its primary semantics through a single keyframe, while the subsequent frames only illustrate the temporal evolvement based on that keyframe. This property empowers the decomposed video tokenization for keyframe and temporal motion. For the keyframe, we employ an off-the-shelf image tokenizer from LaVIT (Jin et al., 2024) to inherit the learned visual codebook and prior knowledge without training from scratch. For encoding temporal motion information, a common alternative is to calculate hand-crafted dense optical flow between adjacent frames (Beauchemin and Barron, 1995). Despite providing a fine-grained depiction of object motions in videos, the expensive computations render it unsuitable for scaling to large-scale video data during pre-training. Hence, we resort to motion vectors, which can be directly extracted at high speed on the CPU (Wu et al., 2018) during the compressed video decoding process.\n' +
      '\n' +
      'As illustrated in Figure 2, we employ the MPEG-4 (Le Gall, 1991) compression technique to extract keyframe and motion information. For simplicity, the I-frames in MPEG-4 are considered as the keyframes requiring tokenization. More sophisticated (but expensive) keyframe selection schemes can also be considered, but are not the main focus of this work. Formally, each video frame is partitioned into \\(16\\times 16\\) non-overlapping macroblocks. Motion vectors \\(\\vec{m}\\) of the \\(t\\)-th frame are estimated by finding the best macroblock\n' +
      '\n' +
      'Figure 2: For each video-text pair, Video-LaVIT decomposes the video into keyframes and motion vectors for efficient tokenization. The tokenizers are learned by maximally reconstructing original inputs (e.g., the motion tokenizer is shown on the right). Finally, the encoded tokens are concatenated with text tokens to form a multimodal sequence, allowing for unified generative pre-training of the LLM (left).\n' +
      '\n' +
      'correspondence between adjacent frames \\(I_{t}\\) and \\(I_{t-1}\\) :\n' +
      '\n' +
      '\\[\\vec{m}(p,q)=\\arg\\min_{i,j}\\|I_{t}(p,q)-I_{t-1}(p-i,q-j)\\|, \\tag{1}\\]\n' +
      '\n' +
      'where \\(I(p,q)\\) indicates the pixel values of the macroblock at location \\((p,q)\\), and \\((i,j)\\) is the coordinate offset between the center of two macroblocks. Then, a video clip can be decomposed into a keyframe \\(I_{0}\\in\\mathbb{R}^{H\\times W\\times 3}\\) and the motion vectors \\(M\\in\\mathbb{R}^{T\\times\\frac{\\mu_{\\text{fc}}}{\\mu_{\\text{fc}}}\\times\\frac{\\mu_{ \\text{fc}}}{\\mu_{\\text{fc}}}\\times 2}\\) of its subsequent \\(T\\) frames.\n' +
      '\n' +
      '**Motion Vector Tokenization**. To transform the motion vectors into a sequence of discrete tokens like a foreign language, we develop a motion-specific tokenizer based on the VQ-VAE architecture (van den Oord et al., 2017). It includes a spatiotemporal encoder \\(f_{\\mathcal{E}}\\), a learnable codebook \\(\\mathcal{C}=\\{c_{k}\\}_{k=1}^{K}\\), and a decoder \\(f_{\\mathcal{D}}\\). The encoder \\(f_{\\mathcal{E}}\\) has \\(L\\) stacked transformer blocks consisting of spatial and temporal attention layers to fuse the contextual motion information among the \\(T\\) frames. It maps the motion vectors \\(M\\in\\mathbb{R}^{T\\times\\frac{\\mu_{\\text{fc}}}{\\mu_{\\text{fc}}}\\times\\frac{1} {\\mu_{\\text{fc}}}\\times 2}\\) into a 1D latent embedding sequence \\(\\hat{Z}\\in\\mathbb{R}^{N\\times d}\\). Each embedding vector \\(\\hat{z}\\in\\mathbb{R}^{d}\\) is then tokenized by a vector quantizer \\(Q\\), which assigns it to the closest code in \\(\\mathcal{C}\\):\n' +
      '\n' +
      '\\[z_{i}=\\arg\\min_{j}\\|l_{2}(\\hat{z}_{i})-l_{2}(c_{j})\\|_{2}, \\tag{2}\\]\n' +
      '\n' +
      'where \\(l_{2}\\) indicates the \\(L_{2}\\) normalization. The decoder \\(f_{\\mathcal{D}}\\) has a similar structure to the encoder and is obliged to map the discrete motion codes \\(\\{z_{i}\\}_{i=1}^{N}\\) back to the original motion vectors. The whole motion tokenizer can be updated by optimizing the reconstruction quality. To prevent codebook collapse during training, we follow Yu et al. (2022) to project the motion embeddings \\(\\hat{Z}\\) into a low-dimensional space before quantization and use exponential moving average (EMA) updates. More details about the motion tokenizer can be found in Appendix A.1. Finally, a video is tokenized into alternating \\(\\langle visual,motion,...\\rangle\\) codes that serve as the supervision signals in LLMs during generative pre-training. Such a factorized tokenization significantly reduces the inter-frame redundancy in one video shot while efficiently capturing the temporal motion information.\n' +
      '\n' +
      '### Video Detokenization\n' +
      '\n' +
      'The video detokenizer of Video-LaVIT is in charge of converting them back into the original continuous pixel space for video generation. Considering the challenge in learning a direct mapping from discrete tokens to the high-dimensional video space, we take a sequential decoding strategy, wherein the keyframe is initially recovered based on the visual token. The subsequent frames are then decoded by taking both the keyframe and motion tokens as the conditions. The efficacy of this strategy in enhancing video generation quality has also been validated by recent work (Girdhar et al., 2023).\n' +
      '\n' +
      'Specifically, the keyframe and video detokenizers both use conditional denoising U-Net (Rombach et al., 2022). Similar to LaVIT (Jin et al., 2024), the keyframe U-Net \\(g_{I}\\) takes the reconstructed visual features that contain image semantics as conditions to infill visual details from a Gaussian noise. Here, we primarily focus on the newly proposed video detokenizer \\(g_{V}\\). As illustrated in Figure 3(a), it is a 3D variant of the original 2D U-Net architecture by inserting temporal convolution and attention layers after the spatial modules, following Blattmann et al. (2023, 2023).\n' +
      '\n' +
      '**Enhanced Motion Conditioning**. The objective of the video detokenizer \\(g_{V}\\) is to rigorously adhere to the guidance of the motion vectors, thereby facilitating the recovery of \\(T\\) frames following the keyframe. To this end, we highlight two different forms of motion conditions in \\(g_{V}\\). Given the motion vectors \\(M\\in\\mathbb{R}^{T\\times\\frac{\\mu_{\\text{fc}}}{\\mu_{\\text{fc}}}\\times\\frac{\\mu_ {\\text{fc}}}{\\mu_{\\text{fc}}}\\times 2}\\) of a sampled video clip,\n' +
      '\n' +
      'Figure 3: Illustrations for video detokenization in Video-LaVIT. (a) Training pipeline for the video detokenizer, which aims to reconstruct the original video clip using one keyframe and the subsequent motion vectors. (b) Autoregressive inference for long video decoding.\n' +
      '\n' +
      'we adopt the nearest neighbor interpolation to ensure that it matches the spatial shape of the U-Net input. Also, the latent state \\(\\hat{I}\\) of the keyframe from the VAE is repeated \\(T\\) times along the temporal axis to form visual conditioning. The motion vector \\(M\\), the keyframe latent \\(\\hat{I}\\), and the noisy video frames are concatenated channel-wise as the input condition to \\(g_{V}\\). Except for direct input conditioning, we also enhance conditioning with motion feature embedding via the spatial and temporal cross-attention layers in the 3D U-Net blocks. Here, the motion features are from a conditioning encoder that has a similar architecture to \\(f_{\\mathcal{E}}\\) excluding the downsample layer to reduce potential information loss. The parameters of the video detokenizer \\(g_{V}\\) are updated by minimizing the following EDM training objective (Karras et al., 2022) on a video training dataset \\(\\mathcal{D}\\):\n' +
      '\n' +
      '\\[\\mathbb{E}_{(X_{0},\\hat{I},\\hat{M})\\sim\\mathcal{D},\\sigma,n}\\left[\\lambda_{ \\sigma}||g_{V}(X_{0}+n,\\sigma,\\hat{I},M)-X_{0}||\\right], \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\sigma\\sim p(\\sigma)\\) is the noise level during training, \\(n\\sim\\mathcal{N}(n;0,\\sigma^{2})\\) is a random noise added to video sample \\(X_{0}\\), and \\(\\lambda_{\\sigma}\\) is loss weighting function. At inference, the \\(\\langle visual,motion\\rangle\\) tokens yielded by LLM are first mapped into visual features and motion vectors by their corresponding tokenizers. The reconstructed visual features are fed into \\(g_{I}\\) to generate a keyframe, which is subsequently combined with reconstructed motion vectors to serve as conditions for \\(g_{V}\\) to decode the video clip (See Figure 3(b)).\n' +
      '\n' +
      '**Long Video Decoding**. Since a video is expressed as multiple alternating \\(\\langle visual,motion\\rangle\\) sequences, the interdependencies among different video fragments can be effectively learned by autoregressive pre-training of LLMs. Hence, Video-LaVIT naturally supports the generation of longer videos by progressive decoding multiple clips. However, separate decoding will bring inconsistencies in some fine-grained visual details among different clips (See Figure 5). To mitigate this, we incorporate an explicit noise constraint when decoding the keyframe \\(I_{r}\\) of a video clip. As illustrated in Figure 3(b), we reverse its last frame \\(I_{r-1}\\) from the previously generated clip into an intermediate noisy state \\(x_{\\Delta T}\\) by reversing the DDIM sampling (Song et al., 2020) process \\(\\Delta T\\) times. Each inversion step is formulated by:\n' +
      '\n' +
      '\\[x_{t+1}=\\sqrt{\\frac{\\alpha_{t+1}}{\\alpha_{t}}}x_{t}+\\left(\\sqrt{\\frac{1}{ \\alpha_{t+1}}-1}-\\sqrt{\\frac{1}{\\alpha_{t}}-1}\\right)g_{I}(x_{t},t,\\hat{I}), \\tag{4}\\]\n' +
      '\n' +
      'where \\(\\alpha_{t}\\) is the noise level, \\(\\hat{I}\\) is the visual feature condition. The reversed noisy state \\(x_{\\Delta T}\\) is then considered as the initial noise in the denoising loop for keyframe \\(I_{r}\\). As illustrated in Figure 5, adding this noise constraint can improve the temporal consistency between video clips.\n' +
      '\n' +
      '### Unified Generative Modeling\n' +
      '\n' +
      'Based on the developed decomposed video tokenization strategy, it is feasible to indiscriminately treat all the modalities (video, image, and text) as 1D discrete tokens fed into LLMs. Following LaVIT (Jin et al., 2024), special tokens (e.g., [MOV] and [/MOV] for motion modality) are inserted at the beginning and end of the visual and motion token sequence for differentiating modalities in the input data. During pre-training, we also exchange the order of multimodal data pairs to form both \\([\\text{video}(\\text{image}),\\text{text}]\\) and \\([\\text{text},\\text{video}(\\text{image})]\\) as input sequences. Formally, given a multimodal sequence \\(y=(y_{1},y_{2},..,y_{S})\\), Video-LaVIT inherits the successful generative language modeling paradigm from LLM to directly maximize the likelihood of each token \\(y_{i}\\) in an autoregressive manner:\n' +
      '\n' +
      '\\[p(y)=\\sum_{y\\in\\mathcal{D}}\\sum_{i=1}^{S}\\log P_{\\theta}(y_{i}|y_{<i}). \\tag{5}\\]\n' +
      '\n' +
      'After pre-training, Video-LaVIT is capable of serving as a multimodal generalist to achieve both multimodal comprehension and generation of data in any modality.\n' +
      '\n' +
      '**Model Training.** Video-LaVIT undergoes a three-stage training procedure on the large-scale multimodal corpora. The purpose of each stage can be summarized as follows: i) Tokenizer and Detokenizer Training. This stage requires only pure video data without corresponding textual captions. It aims to produce compact video tokens that serve as supervision signals to guide the subsequent generative pre-training, as well as to facilitate an accurate reconstruction of the original videos. ii) Generative Pre-training. Stage-2 empowers the model to learn the inter-correlation among the data of different modalities via unified generative modeling within the LLM. iii) Instruction Tuning. To fully unleash the acquired knowledge, the last stage further improves the instruction-following ability to accomplish various multimodal tasks. More details about the model architectures and training data for each stage are provided in Appendix A.1.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Multimodal Understanding\n' +
      '\n' +
      'With the decomposed video representation, Video-LaVIT is naturally capable of understanding both videos and images. Here, we demonstrate its multimodal understanding capability on 11 commonly used image and video benchmarks.\n' +
      '\n' +
      '**Image Understanding.** Table 1 presents an extensive comparison across eight widely used image question answering and multimodal benchmarks: VQA v2 (Goyal et al., 2017), GQA (Hudson and Manning, 2019), VizWiz (Gurari et al., 2018), ScienceQA-IMG (Lu et al., 2022), MME (Fu et al., 2023), MMBench (Liu et al., 2023), SEED (Li et al., 2023), MM-Vet (Yu et al., 2023). Our model successfully generalizes the pre-training knowledge to image comprehension tasks and provides the best overall performance. Specifically, with the same instruction dataset and the base \n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      'text-to-video generators, our method consistently outscores CogVideo (Hong et al., 2023), while surpassing the recent concurrent work VideoPoet (Kondratyuk et al., 2023), which uses a 3D video tokenizer trained on the much larger data. This clearly validates the superiority of our tokenizer design.\n' +
      '\n' +
      '### Qualitative Results\n' +
      '\n' +
      'This section compares videos created by Video-LaVIT with state-of-the-art results under both text and image conditions. It also presents our special ability to generate long videos. More visualization examples are provided in Appendix B.1.\n' +
      '\n' +
      'The text-to-video and image-to-video generation results are visualized in Figure 4. For text-to-video generation, our method can produce visual quality not much far from the closed-source model Gen-2 (Runaway, 2023), thanks to the unified pre-training framework with images. Meanwhile, Video-LaVIT is advantageous in reasoning abilities, such as inferring better motion (in the top-left example) and adding artistic touches based on the text prompt (as in both cases). For image-to-video generation, our method is comparable to the state-of-the-art model SVD (Blattmann et al., 2023) in generating both coherent and highly aesthetic video clips (the bottom-left example). In addition, the decomposed video representation enables the video decoder to produce more salient and vivid movements given a relatively difficult synthetic image prompt (the bottom-right example).\n' +
      '\n' +
      'Furthermore, our autoregressive model can be naturally extended to long video generation, as shown in Figure 5. Thanks to the proposed explicit noise constraint when de\n' +
      '\n' +
      'Figure 4: Text-to-video (top) and image-to-video (bottom) generation comparison with Gen-2 (Runaway, 2023) and SVD-XT (Blattmann et al., 2023). Text prompts are from Emu Video (Girdhar et al., 2023) and SVD. The I2V generation is conditioned on the leftmost frame.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{Data size} & \\multirow{2}{*}{Public data} & \\multicolumn{2}{c}{MSR-VTT} & \\multicolumn{2}{c}{UCF-101} \\\\ \\cline{3-8}  & & & CLIPSIM (\\(\\uparrow\\)) & FVD (\\(\\downarrow\\)) & FID (\\(\\downarrow\\)) & IS (\\(\\uparrow\\)) & FVD (\\(\\downarrow\\)) \\\\ \\hline CogVideo (Hong et al., 2023) & 5.4M & ✓ & 0.2631 & 1294 & 23.59 & 25.27 & 701.59 \\\\ Video LDM (Blattmann et al., 2023) & 10M & ✓ & 0.2929 & - & - & 33.45 & 550.61 \\\\ VideoComposer (Wang et al., 2023) & 10M & ✓ & 0.2932 & 580 & - & - & - \\\\ InternVid (Wang et al., 2024) & 28M & ✓ & 0.2951 & - & - & 21.04 & 616.51 \\\\ Make-A-Video (Singer et al., 2023) & 20M & ✓ & **0.3049** & - & 13.17 & 33.00 & 367.23 \\\\ VideoPoet (Kondratyuk et al., 2023) & 270M & \\(\\times\\) & **0.3049** & 213 & - & 38.44 & 355.00 \\\\ PYCo (Ge et al., 2023) & 22.5M & \\(\\times\\) & - & - & **9.73** & **47.76** & 355.19 \\\\ SVD (Blattmann et al., 2023) & 152M & \\(\\times\\) & - & - & - & **242.02** \\\\ \\hline Video-LaVIT & 10M & ✓ & 0.3010 & **169.51** & 11.80 & 37.96 & 274.96 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Zero-shot text-to-video generation performance. Video-LaVIT delivers competitive results against state-of-the-art models trained on more proprietary data, with data size reported in terms of the number of training video clips. The next best results are underlined.\n' +
      '\n' +
      'coding consecutive video clips, the temporal consistency between decoded clips is greatly improved. In contrast, decoding each video clip separately will result in the incoherence of fine-grained visual details among the video frames of different clips (See the bottom of Figure 5).\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'This section investigates the impact of motion tokenization and different motion token lengths. Due to limited space, the ablation for proposed enhanced motion conditioning strategy is provided in Appendix B.1.\n' +
      '\n' +
      '**Effect of Motion Tokenization**. To validate the effectiveness of motion tokenization in video pre-training, we design a baseline that represents a video by separately tokenizing \\(16\\) uniformly-sampled frames without any motion tokens. When evaluating video synthesis, we first employ the same keyframe U-Net to yield a keyframe from generated visual tokens and then input it to an image-to-video model SVD-XT to generate a video sample. As shown in Table 4, by incorporating motion vectors as the representation of temporal information, Video-LaVIT achieves significantly better results on both video comprehension and generation tasks. Especially for text-to-video generation on the UCF-101 benchmark, it is difficult to generate accurate video content described by text prompts without motion guidance.\n' +
      '\n' +
      '**Effect of Token Length**. We also explore the influence of different motion token lengths when encoding temporal motion information. The detailed results are reported in Table 5. It can be observed that a very small number suffice to yield high understanding and generation performance. More token numbers may lead to representation redundancy and bring more duplicate motion token IDs when encoding videos without obvious motions, rendering the next-token prediction learning paradigm of LLM less effective. Using fewer motion tokens also allows for more video clips as input conditions under the same context length of LLM, which is useful for long video understanding.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'This paper introduces Video-LaVIT, a multimodal generative pre-training method that empowers LLMs with unified comprehension and generation of videos, images, and language. At the core of our method is a video decomposition scheme that allows for more effective modeling of temporal information while reusing visual knowledge from image-only multimodal LLMs. The decomposed keyframes and motion vectors can be efficiently tokenized to be adapted to LLMs for unified generative pre-training. Further, with a well-designed detokenizer, our model flexibly supports multimodal generation inclusive of long videos. Finally, the understanding and generative capabilities of Video-LaVIT are verified by extensive quantitative and qualitative results.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multirow{2}{*}{Method} & MSVD & ActivityNet & \\multicolumn{2}{c}{UCF-101} \\\\ \\cline{2-5}  & Accuracy & Accuracy & IS (\\(\\uparrow\\)) & FVD (\\(\\downarrow\\)) \\\\ \\hline w/o motion & 67.3 & 47.4 & 29.56 & 442.80 \\\\ w/ motion & **73.5** & **50.2** & **37.96** & **274.96** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Ablation of proposed motion tokenization strategy in zero-shot video understanding (left) and generation (right).\n' +
      '\n' +
      'Figure 5: Long video generation example with _“a 360 shot of a sleek yacht sailing gracefully through the crystal-clear waters of the Caribbean”_. The top two rows use the noise constraint in Equation (4) to improve temporal consistency, while the bottom row does not.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multirow{2}{*}{Method} & MSVD & ActivityNet & \\multicolumn{2}{c}{UCF-101} \\\\ \\cline{2-5}  & Accuracy & Accuracy & IS (\\(\\uparrow\\)) & FVD (\\(\\downarrow\\)) \\\\ \\hline \\(N=256\\) & 69.2 & 48.8 & 37.57 & 281.24 \\\\ \\(N=64\\) & **73.5** & **50.2** & **37.96** & **274.96** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Ablation of the number of motion tokens (denoted by \\(N\\)) in zero-shot video understanding (left) and generation (right).\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Aghajany et al. (2022) Aghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu, H., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis, M., et al. CM3: A causal masked multimodal model of the internet. _arXiv preprint arXiv:2201.07520_, 2022.\n' +
      '* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: A visual language model for few-shot learning. In _NeurIPS_, pp. 23716-23736, 2022.\n' +
      '* Awadalla et al. (2023) Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa, S., et al. OpenFlamingo: An open-source framework for training large autoregressive vision-language models. _arXiv preprint arXiv:2308.01390_, 2023.\n' +
      '* Baek et al. (2019) Baek, Y., Lee, B., Han, D., Yun, S., and Lee, H. Character region awareness for text detection. In _CVPR_, pp. 9365-9374, 2019.\n' +
      '* Bain et al. (2021) Bain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen in time: A joint video and image encoder for end-to-end retrieval. In _ICCV_, pp. 1728-1738, 2021.\n' +
      '* Beauchemin & Barron (1995) Beauchemin, S. S. and Barron, J. L. The computation of optical flow. _ACM Computing Surveys_, 27(3):433-466, 1995.\n' +
      '* Blattmann et al. (2023a) Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. _arXiv preprint arXiv:2311.15127_, 2023a.\n' +
      '* Blattmann et al. (2023b) Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. Align your latents: High-resolution video synthesis with latent diffusion models. In _CVPR_, pp. 22563-22575, 2023b.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In _NeurIPS_, pp. 1877-1901, 2020.\n' +
      '* Carreira & Zisserman (2017) Carreira, J. and Zisserman, A. Quo vadis, action recognition? a new model and the kinetics dataset. In _CVPR_, pp. 6299-6308, 2017.\n' +
      '* Changpinyo et al. (2021) Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In _CVPR_, pp. 3558-3568, 2021.\n' +
      '* Chen & Dolan (2011) Chen, D. and Dolan, W. B. Collecting highly parallel data for paraphrase evaluation. In _ACL_, pp. 190-200, 2011.\n' +
      '* Dai et al. (2023) Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In _NeurIPS_, 2023.\n' +
      '* Dong et al. (2024) Dong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao, L., Sun, J., Zhou, H., Wei, H., et al. DreamLLM: Synergistic multimodal comprehension and creation. In _ICLR_, 2024.\n' +
      '* Esser et al. (2021) Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In _CVPR_, pp. 12873-12883, 2021.\n' +
      '* Esser et al. (2023) Esser, P., Chiu, J., Atighehchian, P., Grasskog, J., and Germanidis, A. Structure and content-guided video synthesis with diffusion models. In _ICCV_, pp. 7346-7356, 2023.\n' +
      '* Fang et al. (2023) Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., and Cao, Y. EVA: Exploring the limits of masked visual representation learning at scale. In _CVPR_, pp. 19358-19369, 2023.\n' +
      '* Feng et al. (2024) Feng, W., Zhu, W., Fu, T.-j., Jampani, V., Akula, A., He, X., Basu, S., Wang, X. E., and Wang, W. Y. LayoutGPT: Compositional visual planning and generation with large language models. In _ICLR_, 2024.\n' +
      '* Fu et al. (2023) Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu, Z., Lin, W., Yang, J., Zheng, X., et al. MME: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023.\n' +
      '* Ge et al. (2023) Ge, S., Nah, S., Liu, G., Poon, T., Tao, A., Catanzaro, B., Jacobs, D., Huang, J.-B., Liu, M.-Y., and Balaji, Y. Preserve your own correlation: A noise prior for video diffusion models. In _ICCV_, pp. 22930-22941, 2023.\n' +
      '* Gemini Team (2023) Gemini Team, G. Gemini: A family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Girdhar et al. (2023) Girdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S. S., Shah, A., Yin, X., Parikh, D., and Misra, I. Emu video: Factorizing text-to-video generation by explicit image conditioning. _arXiv preprint arXiv:2311.10709_, 2023.\n' +
      '* Goyal et al. (2017) Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In _CVPR_, pp. 6904-6913, 2017.\n' +
      '* Gurari et al. (2018) Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. VizWiz grand challenge: Answering visual questions from blind people. In _CVPR_, pp. 3608-3617, 2018.\n' +
      '\n' +
      '* Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In _ICLR_, 2021.\n' +
      '* Heusel et al. (2017) Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In _NeurIPS_, pp. 6629-6640, 2017.\n' +
      '* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In _NeurIPS_, pp. 6840-6851, 2020.\n' +
      '* Ho et al. (2022) Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. In _NeurIPS_, pp. 8633-8646, 2022.\n' +
      '* Hong et al. (2023) Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. CogVideo: Large-scale pretraining for text-to-video generation via transformers. In _ICLR_, 2023.\n' +
      '* Hudson & Manning (2019) Hudson, D. A. and Manning, C. D. GQA: A new dataset for real-world visual reasoning and compositional question answering. In _CVPR_, pp. 6700-6709, 2019.\n' +
      '* Jin et al. (2024) Jin, Y., Xu, K., Xu, K., Chen, L., Liao, C., Tan, J., Huang, Q., Chen, B., Lei, C., Liu, A., et al. Unified language-vision pretraining in LLM with dynamic discrete visual tokenization. In _ICLR_, 2024.\n' +
      '* Karras et al. (2022) Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. In _NeurIPS_, pp. 26565-26577, 2022.\n' +
      '* Kay et al. (2017) Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al. The Kinetics human action video dataset. _arXiv preprint arXiv:1705.06950_, 2017.\n' +
      '* Kondratyuk et al. (2023) Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Hornung, R., Adam, H., Akbari, H., Alon, Y., Birodkar, V., et al. VideoPoot: A large language model for zero-shot video generation. _arXiv preprint arXiv:2312.14125_, 2023.\n' +
      '* Kynkaanniemi et al. (2023) Kynkaanniemi, T., Karras, T., Aittala, M., Aila, T., and Lehtinen, J. The role of ImageNet classes in Frechet Inception distance. In _ICLR_, 2023.\n' +
      '* Le Gall (1991) Le Gall, D. MPEG: A video compression standard for multimedia applications. _Communications of the ACM_, 34(4):46-58, 1991.\n' +
      '* Li et al. (2023a) Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y. SEED-Bench: Benchmarking multimodal LLMs with generative comprehension. _arXiv preprint arXiv:2307.16125_, 2023a.\n' +
      '* Li et al. (2022) Li, D., Li, J., and Hoi, S. BLIP-Diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. In _NeurIPS_, 2023b.\n' +
      '* Li et al. (2022) Li, J., Li, D., Xiong, C., and Hoi, S. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _ICML_, pp. 12888-12900, 2022.\n' +
      '* Li et al. (1974) Li, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _ICML_, pp. 19730-19742, 2023c.\n' +
      '* Li et al. (2023d) Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., and Qiao, Y. VideoChat: Chat-centric video understanding. _arXiv preprint arXiv:2305.06355_, 2023d.\n' +
      '* Li et al. (2023e) Li, Y., Wang, C., and Jia, J. LLaMA-VID: An image is worth 2 tokens in large language models. _arXiv preprint arXiv:2311.17043_, 2023e.\n' +
      '* Lian et al. (2023) Lian, L., Li, B., Yala, A., and Darrell, T. LLM-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. _arXiv preprint arXiv:2305.13655_, 2023.\n' +
      '* Lin et al. (2023) Lin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., and Yuan, L. Video-LLaVA: Learning united visual representation by alignment before projection. _arXiv preprint arXiv:2311.10122_, 2023.\n' +
      '* Lin et al. (2014) Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft COCO: Common objects in context. In _ECCV_, pp. 740-755, 2014.\n' +
      '* Liu et al. (2023a) Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023a.\n' +
      '* Liu et al. (2023b) Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. In _NeurIPS_, 2023b.\n' +
      '* Liu et al. (2023c) Liu, S., Cheng, H., Liu, H., Zhang, H., Li, F., Ren, T., Zou, X., Yang, J., Su, H., Zhu, J., et al. LLaVA-Plus: Learning to use tools for creating multimodal agents. _arXiv preprint arXiv:2311.05437_, 2023c.\n' +
      '* Liu et al. (2023d) Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. MMBench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281_, 2023d.\n' +
      '* Lu et al. (2022) Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. In _NeurIPS_, pp. 2507-2521, 2022.\n' +
      '\n' +
      '* Maaz et al. (2023) Maaz, M., Rasheed, H., Khan, S., and Khan, F. S. Video-ChatGPT: Towards detailed video understanding via large vision and language models. _arXiv preprint arXiv:2306.05424_, 2023.\n' +
      '* OpenAI (2023a) OpenAI. GPT-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023a.\n' +
      '* OpenAI (2023b) OpenAI. GPT-4V(ision) system card. [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card), 2023b.\n' +
      '* Ordonez et al. (2011) Ordonez, V., Kulkarni, G., and Berg, T. L. Im2Text: Describing images using 1 million captioned photographs. In _NeurIPS_, pp. 1143-1151, 2011.\n' +
      '* Podell et al. (2024) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. SDXL: Improving latent diffusion models for high-resolution image synthesis. In _ICLR_, 2024.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _ICML_, pp. 8748-8763, 2021.\n' +
      '* Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. _JMLR_, 21(1):5485-5551, 2020.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _CVPR_, pp. 10684-10695, 2022.\n' +
      '* Runaway (2023) Runaway. Gen-2. [https://research.runwayml.com/gen2](https://research.runwayml.com/gen2), 2023.\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. In _NeurIPS_, pp. 36479-36494, 2022.\n' +
      '* Saito et al. (2020) Saito, M., Saito, S., Koyama, M., and Kobayashi, S. Train sparsely, generate densely: Memory-efficient unsupervised training of high-resolution temporal GAN. _IJCV_, 128(10-11):2586-2606, 2020.\n' +
      '* Sharma et al. (2018) Sharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _ACL_, pp. 2556-2565, 2018.\n' +
      '* Shen et al. (2024) Shen, C., Gan, Y., Chen, C., Zhu, X., Cheng, L., and Wang, J. Decouple content and motion for conditional image-to-video generation. In _AAAI_, 2024.\n' +
      '* Singer et al. (2023) Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., et al. Make-A-Video: Text-to-video generation without text-video data. In _ICLR_, 2023.\n' +
      '* Sohl-Dickstein et al. (2015) Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, pp. 2256-2265, 2015.\n' +
      '* Song et al. (2020) Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. In _ICLR_, 2020.\n' +
      '* Song & Ermon (2019) Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. In _NeurIPS_, pp. 11918-11930, 2019.\n' +
      '* Soomro et al. (2012) Soomro, K., Zamir, A. R., and Shah, M. A dataset of 101 human action classes from videos in the wild. _Center for Research in Computer Vision_, 2(11), 2012.\n' +
      '* Sun et al. (2024) Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., and Wang, X. Emu: Generative pretraining in multimodality. In _ICLR_, 2024.\n' +
      '* Together Computer (2023) Together Computer. RedPajama: an open dataset for training large language models, 2023. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. LLaMA: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023b.\n' +
      '* Tran et al. (2015) Tran, D., Bourdev, L., Fergus, R., Torresani, L., and Paluri, M. Learning spatiotemporal features with 3D convolutional networks. In _ICCV_, pp. 4489-4497, 2015.\n' +
      '* Unterthiner et al. (2018) Unterthiner, T., Van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., and Gelly, S. Towards accurate generative models of video: A new metric & challenges. _arXiv preprint arXiv:1812.01717_, 2018.\n' +
      '* van den Oord et al. (2017) van den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. In _NeurIPS_, pp. 6309-6318, 2017.\n' +
      '* Villegas et al. (2023) Villegas, R., Babaeizadeh, M., Kindermans, P.-J., Moraldo, H., Zhang, H., Saffar, M. T., Castro, S., Kunze, J., and Erhan, D. Phenaki: Variable length video generation from open domain textual descriptions. In _ICLR_, 2023.\n' +
      '\n' +
      '* Wang et al. (2023) Wang, F.-Y., Chen, W., Song, G., Ye, H.-J., Liu, Y., and Li, H. Gen-L-Video: Multi-text to long video generation via temporal co-denoising. _arXiv preprint arXiv:2305.18264_, 2023a.\n' +
      '* Wang et al. (2023b) Wang, X., Yuan, H., Zhang, S., Chen, D., Wang, J., Zhang, Y., Shen, Y., Zhao, D., and Zhou, J. VideoComposer: Compositional video synthesis with motion controllability. In _NeurIPS_, 2023b.\n' +
      '* Wang et al. (2024) Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Chen, X., Wang, Y., Luo, P., Liu, Z., et al. InternVid: A large-scale video-text dataset for multimodal understanding and generation. In _ICLR_, 2024.\n' +
      '* Wu et al. (2021) Wu, C., Huang, L., Zhang, Q., Li, B., Ji, L., Yang, F., Sapiro, G., and Duan, N. GODIVA: Generating open-domain videos from natural descriptions. _arXiv preprint arXiv:2104.14806_, 2021.\n' +
      '* Wu et al. (2018) Wu, C.-Y., Zaheer, M., Hu, H., Manmatha, R., Smola, A. J., and Krahenbuhl, P. Compressed video action recognition. In _CVPR_, pp. 6026-6035, 2018.\n' +
      '* Xu et al. (2016) Xu, J., Mei, T., Yao, T., and Rui, Y. MSR-VTT: A large video description dataset for bridging video and language. In _CVPR_, pp. 5288-5296, 2016.\n' +
      '* Yan et al. (2021) Yan, W., Zhang, Y., Abbeel, P., and Srinivas, A. VideoGPT: Video generation using VQ-VAE and transformers. _arXiv preprint arXiv:2104.10157_, 2021.\n' +
      '* Yang et al. (2022) Yang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C. Zero-shot video question answering via frozen bidirectional language models. In _NeurIPS_, pp. 124-141, 2022.\n' +
      '* Yu et al. (2022) Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., and Wu, Y. Vector-quantized image modeling with improved VQGAN. In _ICLR_, 2022.\n' +
      '* Yu et al. (2023a) Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O., Wang, T., Babu, A., Tang, B., Karrer, B., Sheynin, S., et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning. _arXiv preprint arXiv:2309.02591_, 2023a.\n' +
      '* Yu et al. (2023b) Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. MM-Vet: Evaluating large multi-modal models for integrated capabilities. _arXiv preprint arXiv:2308.02490_, 2023b.\n' +
      '* Yu et al. (2019) Yu, Z., Xu, D., Yu, J., Yu, T., Zhao, Z., Zhuang, Y., and Tao, D. ActivityNet-QA: A dataset for understanding complex web videos via question answering. In _AAAI_, pp. 9127-9134, 2019.\n' +
      '* Zeng et al. (2023) Zeng, Y., Wei, G., Zheng, J., Zou, J., Wei, Y., Zhang, Y., and Li, H. Make pixels dance: High-dynamic video generation. _arXiv preprint arXiv:2311.10982_, 2023.\n' +
      '* Zhang et al. (2016) Zhang, B., Wang, L., Wang, Z., Qiao, Y., and Wang, H. Real-time action recognition with enhanced motion vector CNNs. In _CVPR_, pp. 2718-2726, 2016.\n' +
      '* Zhang et al. (2023) Zhang, H., Li, X., and Bing, L. Video-LLaMA: An instruction-tuned audio-visual language model for video understanding. _arXiv preprint arXiv:2306.02858_, 2023.\n' +
      '\n' +
      '## Appendix A Experimental Settings\n' +
      '\n' +
      '### Model Implementation Details\n' +
      '\n' +
      '**Video Tokenizer** We employ the off-the-shelf visual tokenizer from LaVIT (Jin et al., 2024) to transform the video keyframe into 90 visual tokens on average, which follows most existing MLLMs to utilize the ViT-G/14 of EVA-CLIP (Fang et al., 2023) as the visual encoder. The visual codebook size is set to 16384. Please refer to the original paper for more details. During training and inference, images and keyframes are resized to 224\\(\\times\\)224 resolution as input.\n' +
      '\n' +
      'As for motion tokenization, we downsample the original videos at 6 fps and then take 24 consecutive frames as a video clip to compute the motion vector \\(M\\). It is further divided by the width and height of the corresponding video to normalize the value within the range of \\([-1,1]\\). Before feeding into the motion tokenizer, the motion vector \\(M\\) is resized to a resolution of 16\\(\\times\\)16, resulting in the final input tensor shape being \\(B\\times 24\\times 16\\times 16\\times 2\\). The encoder \\(f_{\\mathcal{E}}\\) and decoder \\(f_{\\mathcal{D}}\\) in our motion tokenizer both have \\(L=12\\) transformer blocks with 512 hidden states and 8 attention heads. Each block consists of spatial, temporal attention, and feed-forward layers. Before the attention computation, the motion input is reshaped into \\([(B*24)\\times(16*16)\\times D]\\) and \\([(B*16*16)\\times 24\\times D]\\) for the spatial and temporal layers, respectively. We insert the 3D average pooling layers after the \\([3,6,9,12]\\) encoder blocks to conduct [\\(\\times\\)2, \\(\\times\\)2, \\(\\times\\)2] downsampling on the motion embeddings, which will then be quantized into 256 discrete motion tokens. For the version with 64 motion tokens, the spatial shape of the motion embeddings will also be downsampled \\(\\times\\)2 before quantization. The decoder \\(f_{\\mathcal{D}}\\) includes symmetrical upsampling layers to recover the original input motion vector during training. The size of learned motion codebook is set to 1024. To improve the training stability of the motion codebook, we leverage exponential moving average (EMA) updates with a weight of \\(0.995\\). Before quantization, the motion embeddings are projected into a low-dimensional space to improve the codebook usage, following the experience of Yu et al. (2022).\n' +
      '\n' +
      '**Video Detokenizer** During training of the video detokenizer, we randomly sampled 24 consecutive frames from videos downsampled at 6 fps. The motion conditioning encoder has the same transformer architecture (12 blocks) as \\(f_{\\mathcal{E}}\\), except that the downsample layers are removed to keep the same temporal dimension with the input video frames. This strategy reduces the compression of motion information during encoding and provides explicit guidance for each frame to be denoised in the 3D U-Net. The detailed architecture of the 3D U-Net employed follows the same implementations as Blattmann et al. (2023b, 2023a). During the EDM-preconditioning optimization for the detokenizer, the distribution of \\(\\log\\sigma\\) is set to \\(\\mathcal{N}(1.0,1.2^{2})\\) to encourage a higher noise level, which is found effective for the high-resolution generation (Girdhar et al., 2023). We train the motion conditioning encoder, the input encoding layer, and all the cross-attention layers in the 3D U-Net from scratch and initialize the other weights from the SVD img2vid-xt (Blattmann et al., 2023a). To reduce the computational complexity, the detokenizer is first trained with a resolution 384 \\(\\times\\) 384 for 50k steps, and then further fine-tuned at two types of resolutions: 768 \\(\\times\\) 768 or 1024 \\(\\times\\) 576 for another 10k steps.\n' +
      '\n' +
      '**Language Model** We utilize Llama 2 7B (Touvron et al., 2023b) as the default large language model for the generative pre-training. The weight of the language model is initialized from LaVIT (Jin et al., 2024) to preserve the learned visual prior knowledge to support the comprehension and generation for the image domain. During pre-training, we mix the image-text, video-text pairs, and textual data in one batch to form the final multimodal input sequence.\n' +
      '\n' +
      '### Pre-training Data\n' +
      '\n' +
      'The training dataset used by Video-LaVIT only consists of publicly available image and video datasets. In the following, we present a detailed elaboration of the dataset usage at each training stage.\n' +
      '\n' +
      '**Stage 1:** The video tokenizer and detokenizer are trained on the WebVid-10M (Bain et al., 2021), which is an open-source video-text dataset containing 10 million video-text pairs scraped from the stock footage sites. Since both our tokenizer and detokenizer do not rely on textual data, we only employ pure video data at this stage. Due to the common watermarks in WebVid-10M, during the training of the video detokenizer, we incorporate a subset of InterVid-14M-aesthetics (Wang et al., 2024) to remove watermarks in the generated videos. It has also been shown useful in PixelDance (Zeng et al., 2023). Specifically, we first select a subset of 4s-10s video clips with the highest aesthetic scores and then follow SVD (Blattmann et al., 2023a) in applying CRAFT (Baek et al., 2019) to filter out those videos with unwanted written text. The result contains about 300k publicly available video clips. **Noting that the 300k video subset is only used during the training of the video detokenizer to improve the aesthetics of the generated videos, the results reported in all the experiments are tested on the checkpoint that uses only the WebVid-10M dataset.**\n' +
      '\n' +
      '**Stage 2:** The language model is pre-trained on a mixture of video, image and text data, including WebVid-10M (Bainet al., 2021); 93M samples from Conceptual Caption Sharma et al. (2018); Changpinyo et al. (2021), SBU Ordonez et al. (2011), and BLIP-Capfilt Li et al. (2022). Moreover, we also employ the English text corpus from RedPajama Together Computer (2023), which is open-source data like the original one to train LLaMA from scratch. The purpose of including the English text corpus during pre-training is to preserve the already learned language understanding ability of LLM (e.g., the performance on linguistic benchmarks like MMLU Hendrycks et al. (2021)) while acquiring good multimodal capabilities.\n' +
      '\n' +
      '**Stage 3:** For a fair comparison, we employ the same instruction tuning dataset as the existing works Lin et al. (2023); Li et al. (2023) during this stage. It includes a 665k image-text instruction dataset from LLaVA v1.5 Liu et al. (2023) and a 100k video-text instruction dataset from Video-ChatGPT Maaz et al. (2023).\n' +
      '\n' +
      '### Training Settings\n' +
      '\n' +
      'The detailed training hyper-parameter settings for the video tokenizer, detokenizer, and language model in Video-LaVIT are reported in Table 6. We adopt the same instruction tuning setting as LLaVA v1.5 Liu et al. (2023).\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      '**Image Understanding** is evaluated using eight popular image question answering and multimodal benchmarks: VQA v2 Goyal et al. (2017), GQA Hudson and Manning (2019), VizWiz Gurari et al. (2018), ScienceQA-IMG Lu et al. (2022), MME Fu et al. (2023), MMBench Liu et al. (2023), SEED Li et al. (2023), MM-Vet Yu et al. (2023). For question-answering datasets, we use the same prompts as in LLaVA-1.5 Liu et al. (2023), and adopt the widely used VQA accuracy as the evaluation metric.\n' +
      '\n' +
      '**Video Question Answering**. Three common datasets are considered: MSVD-QA Chen and Dolan (2011), MSRVTT-QA Xu et al. (2016) and ActivityNet-QA Yu et al. (2019). To assess model accuracy, a GPT-3.5 assistant Maaz et al. (2023) is employed, which also produces outputs a relative score ranging from 0 to 5.\n' +
      '\n' +
      '**Text-to-Image Generation**. We adopt the validation set of MS-COCO Lin et al. (2014) and randomly select 30K samples. The quality of the generated images is evaluated by Frechet Inception distance (FID) Heusel et al. (2017), which computes its Frechet distance to the ground truth in the feature space of a pre-trained Inception V3 model.\n' +
      '\n' +
      '**Text-to-Video Generation** is measured on MSR-VTT Xu et al. (2016) and UCF-101 Soomro et al. (2012). For MSR-VTT, we use all 2990 videos and sample one caption for each video, resulting in 2990 video-text pairs; for UCF-101, we sample 20 videos per class and follow PYoCo Ge et al. (2023) to curate prompts for each class, producing 2020 video-text pairs. Their evaluation metrics are detailed below.\n' +
      '\n' +
      '* CLIP similarity (CLIPSIM) Wu et al. (2021) measures the semantic similarity between video-text pairs. We follow Phenaki Villegas et al. (2023) and VideoPoot Kondratyuk et al. (2023) in using a ViT-B/16 Radford et al. (2021) to compute CLIP scores between 224\\(\\times\\)224 sized video frames and their corresponding captions. The final score is\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Configuration & Language Model & Tokenizer & Detokenizer \\\\ \\hline LLM init & LaVIT-7B & - & - \\\\ Optimizer & AdamW & AdamW & AdamW \\\\ Optimizer Hyperparameters & \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.95\\), \\(\\epsilon=1e^{-6}\\) & \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.99\\), \\(\\epsilon=1e^{-6}\\) & \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.99\\), \\(\\epsilon=1e^{-6}\\) \\\\ Global batch size & 2048 & 512 & 128 \\\\ Peak learning rate of LLM & 2e-5 & - & - \\\\ Peak learning rate of other Part & 5e-5 & 1e-4 & 5e-5 \\\\ Learning rate schedule & Cosine & Cosine & Cosine \\\\ Training Steps & 30K & 100K & 60K \\\\ Warm-up steps & 2k & 5K & 3K \\\\ Weight decay & 0.1 & 0.001 & 0.001 \\\\ Gradient clipping & 1.0 & 1.0 & 1.0 \\\\ Input sequence to LLM & 2048 & - & - \\\\ Numerical precision & bfloat16 & bfloat16 & bfloat16 \\\\ GPU Usage & 128 NVIDIA A100 & 64 NVIDIA A100 & 64 NVIDIA A100 \\\\ Framework & Megatron & DeepSpeed & DeepSpeed \\\\ Training Time & 60h & 10h & 48h \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: The detailed training hyperparameters of Video-LaVITaveraged over all generated video frames.\n' +
      '* Frechet video distance (FVD) (Unterthiner et al., 2018) evaluates the Frechet distance between generated and real videos in the feature space of an I3D action classification model (Carreira and Zisserman, 2017) pre-trained on Kinetics-400 (Kay et al., 2017).\n' +
      '* Frechet Inception distance (FID) (Heusel et al., 2017) measures the Frechet distance between generated and real video frames. Following PYoCo (Ge et al., 2023), we use a ViT-B/32 model (Kynkaanniemi et al., 2023) to extract the frame features. The final result is averaged over all video frames.\n' +
      '* Inception score (IS) (Saito et al., 2020) evaluates the distribution of our generated video frames. We employ a C3D model (Tran et al., 2015) fine-tuned on UCF-101 to calculate a video version of the inception score. The model takes the central 16 frames of each video as the input.\n' +
      '\n' +
      'Note that there are slight variations in the evaluation protocols of different papers. We have sought to keep our protocol the same as or similar to most of the top-ranked methods.\n' +
      '\n' +
      '## Appendix B Additional Results\n' +
      '\n' +
      '### Multimodal Generation\n' +
      '\n' +
      'This section provides additional qualitative results and an ablation study to demonstrate the effectiveness of our design for multimodal generation, complementing the existing comparisons in the main paper.\n' +
      '\n' +
      '**Text-to-Image Generation**. Figure 6 illustrates the comparison of text-to-image generation between Video-LaVIT and SDXL (Podell et al., 2024). Overall, our method achieves competitive visual quality while having better language understanding and reasoning capabilities. For example, in the top-left case of a young woman in front of a UFO, our method produces highly aesthetic headshots of the woman, while capturing the detail of "sharp focus" in the text prompt. And in the bottom-left example of apple painting, our model successfully infers from the prompt "neither is red and both are green" to draw two green apples, thanks to the better logical reasoning ability of the LLM-based generation approach we adopted.\n' +
      '\n' +
      '**Text-to-Video Generation**. Figure 7 compares Video-LaVIT to a closed-source model Gen-2 (Runaway, 2023). As can be seen, our model produces high-quality videos that are generally comparable to Gen-2, which is especially evident in the last two examples where it successfully captures details such as "moss and many flowers" and "autumn" in the text prompt and yields very similar results to Gen-2. Moreover, the first two comparisons demonstrate a favorable prompt following ability of our model. In the first case with the keyword "running", our model produces significant camera motion toward the cabin, while the movement in Gen-2 is relatively nuanced. In the second case, our model correctly displays multiple "pirate ships" as the prompt specified, with artistic details such as all the ships being on fire, according to the implication of "intense battle". These results support the benefits of unified video-language pre-training in prompt following capabilities.\n' +
      '\n' +
      '**Image-to-video generation**. Figure 8 presents a comparison of Video-LaVIT with the open-source model SVD (Blattmann et al., 2023), both conditioned on synthetic image prompts. Moving to some unseen test cases, our method produces video clips featuring both natural and refined motions, thanks to the decomposed video representation that can better transfer motion-related knowledge to new visual inputs. For example, in the middle case, our generated goat smoothly lowers its head and blinks as if it were a human to think, while the goat in the video produced by SVD hardly moved. In the bottom case, where the image prompt shows a teddy is riding a motorcycle, our generated full video looks very natural and similar to a human riding a motorcycle, while SVD constantly produces a scenario where the motorcycle is moving a different direction from where its tire is pointing (which is physically wrong). Overall, our model demonstrates superior image-to-video generation performance with the inclusion of decoupled visual-motion tokenization and LLM pre-training.\n' +
      '\n' +
      '**Long Video Generation** is showcased in Figure 9. By explicitly constraining the noise when decoding successive video clips, our model can provide a high temporal consistency during long video generation. For example, in the first two cases, the dog and the jeep car maintain the same identity across different clips with highly coherent visual details. In the last example which features large camera movement, the moving trajectory remains consistent as it approaches the cabin according to the text prompt. These examples all illustrate our reasonably good quality of long video generation. Note that all the generated videos are provided at [https://anonymous-icml-2024.github.io/video-lavit](https://anonymous-icml-2024.github.io/video-lavit).\n' +
      '\n' +
      '**Ablation Study**. To rigorously reconstruct original video content, we employ the enhanced conditioning: motion input condition and motion feature condition for training the 3D video U-Net \\(g_{V}\\). We illustrate the effect of proposed enhanced motion conditioning (EMC) strategy on video decoding in Figure 10. The variant "w/o EMC" only leverages motion vectors as the input condition. Compared with using EMC, it is incapable of recovering the motion of original input videos. For example, the "train" and "fish" barely moved in the shown video samples, which demonstrates the effectiveness of our proposed conditioning strategy.\n' +
      '\n' +
      '### Multimodal Understanding\n' +
      '\n' +
      'This section presents qualitative results of Video-LaVIT for image and video understanding. First, Table 7 showcases our performance in image question answering using the famous test example from GPT-4 (OpenAI, 2023a). As can be seen, our model produces a reasonable answer with a good number of correct details (e.g. the type of the vehicle being SUV) and even a friendly safety warning. In comparison, one of the recent multimodal LLMs, LLaVA (Liu et al., 2023b), produces a roughly correct answer with some inaccurate detail (mistaking the vehicle type as "minivan or van").\n' +
      '\n' +
      'For video question answering, Tables 8 and 9 compares our method to Video-LLaVA (Lin et al., 2023) and Video-ChatGPT (Maaz et al., 2023) based on the video clips from Video-ChatGPT. In the first example of Table 8 which asks to explain why a video is funny, our model yields the most concise answer among the video-language models compared, and at the same time contains a salient point that the other models failed to mention. The next example in Table 8, on the other hand, shows that our method produces fewer hallucinations than Video-LLaVA and Video-ChatGPT, as the latter two models tend to generate overly detailed action descriptions that have no basis in the video. And lastly, in the example of Table 9, our model follows the instruction prompt by producing a beautiful fairy tale with both conciseness and a moral lesson ("love can conquer all"). To summarize, our method demonstrates reasonably good multimodal understanding capabilities across different test cases, in line with the previous quantitative comparison on multiple benchmarks.\n' +
      '\n' +
      '## Appendix C Limitations\n' +
      '\n' +
      'Our proposed model cannot directly process very long videos of more than 100s due to its limited context window (4096). While a possible solution is to sample fewer video keyframes, some nuanced motions may not be captured after resampling. To compound this problem, the resolution of motion vectors could be a bottleneck for modeling very fine-grained motions, such as human actions. On the other hand, a general concern is that our training cost is still too high to scale to web-scale video data, which may require further optimization through joint exploitation of spatial and temporal redundancies in video.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      'Figure 7: Text-to-video generation comparison with Gen-2 (Runaway, 2023) using default parameters. Prompts are from VideoPoet (Kondratyuk et al., 2023) and PixelDance (Zeng et al., 2023). Our model provides a similarly high visual quality (in the bottom two cases) while following the text prompt better (including ”running” in the first example and ”pirate ships” in the second examples).\n' +
      '\n' +
      'Figure 8: Image-to-video generation comparison with SVD (Blattmann et al., 2023a) using the stable-video-diffusion-img2vid-xt version. The generation is conditioned on the leftmost frame. Our model can produce more sophisticated animal motions (see the top two cases) while not violating the physical rules (e.g., in the second last row, the motorcycle is not moving in the direction of its tire).\n' +
      '\n' +
      'Figure 9: Long video generation examples. Prompts are Gen-L-Video (Wang et al., 2023) and VideoPoot (Kondratyuk et al., 2023). Our generated videos are temporally coherent even across different decoded clips, thanks to our proposed explicit noise constraint.\n' +
      '\n' +
      'Figure 10: Ablation study of enhanced motion conditioning (EMC) for video reconstruction. The last row (i.e., w/o EMC) indicates only motion vectors as the input condition for training 3D U-Net. As illustrated, incorporating EMC can improve the fidelity of the reconstructed videos. The video samples are taken from WebVid-10M (Bain et al., 2021).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline User & What is unusual about this image? \\\\ LLaVA (Liu et al., 2023b) & The unusual aspect of this image is a man ironing clothes on the back of a minivan or van. This is not a typical place to perform this activity, as one would usually iron clothes in a more stationary and safe location, such as a home, using a regular ironing board. The scene depicted in the image is peculiar as it involves a makeshift ironing setup on a vehicle, which can be both unsafe and unconventional. Additionally, it is not clear how the man is able to maintain balance and stability while ironing clothes in such an unstable environment. [_Start a new conversation, and clear the history_] \\\\ User & What’s happening in the scene? \\\\ LLaVA (Liu et al., 2023b) & The image depicts a man standing on top of a yellow SUV in a busy city street. He is holding a portable ladder, seemingly ironing clothes while standing on the vehicle. Around the scene, there are other cars, a traffic light, and a fire hydrant. Additionally, there are two other people in the vicinity, one close to the man on the car and another further away. The man on the ladder appears to be attracting attention due to his unconventional choice of ironing his clothes on top of a moving car. The city street around him is bustling with activity, adding to the unique nature of the scene. \\\\ \\hline User & What is unusual about this image? \\\\ GPT-4 (OpenAI, 2023a) & The unusual thing about this image is that a man is ironing clothes on an ironing board attached to the roof of a moving taxi. \\\\ \\hline User & Can you explain this meme in detail? \\\\ BLIP-2 (Li et al., 2023c) & a man is sitting on the back of a yellow cab \\\\ \\hline User & Can you explain this meme in detail? \\\\ OpenFlamingo (Awadalla et al., 2023) & The man is drying his clothes on the hood of his car. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Image question answering comparison with LLaVA (Liu et al., 2023b), GPT-4 (OpenAI, 2023a), BLIP-2 (Li et al., 2023c), and OpenFlamingo (Awadalla et al., 2023). Our model answers correctly like the other multimodal LLMs, but without missing details such as the vehicle type (it is an SUV instead of a van or minivan). The table format follows GPT-4.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{113.8pt} p{284.5pt}} \\hline \\hline User & Why is this video funny? \\\\ Video-LaVIT & The video is funny because the baby is wearing glasses and trying to read a book, but she is not able to do so. The baby’s attempts to read the book are adaptable and amusing to watch. \\\\ \\hline User & Why is this video funny? \\\\ Video-LLaVA (Lin et al., 2023) & Why is this video funny? \\\\ Video-LLaVIT (Maaz et al., 2023) & Why is this video funny? \\\\ Video-ChatGPT (Maaz et al., 2023) & The video is funny because it shows a baby girl who is learning to read. She is sitting on a bed and looking at a book, and she is trying to read it. The baby girl is wearing glasses, and she is holding the book with both hands. The baby girl is trying to read the book, and her actions are adorable and endearing. The video captures the innocence and curiosity of a young child as she learns to read, which is a joyful and amusing sight for the viewer. \\\\ \\hline User & What does the woman use to split the logs and how does she do it? \\\\ Video-LaVIT & The woman uses an ax to split the logs. She picks up the ax and swings it down with force, splitting the logs in half. \\\\ \\hline User & What does the woman use to split the logs and how does she do it? \\\\ Video-LLaVA (Lin et al., 2023) & The woman uses an ax to split the logs. She holds the ax in her hand and swings it down onto the log. She repeats this process several times, splitting the log into smaller pieces. \\\\ \\hline User & What does the woman use to split the logs and how does she do it? \\\\ Video-ChatGPT (Maaz et al., 2023) & The woman uses a large ax to split the logs. She starts by placing the axes on the log and then swings it down with a powerful motion. The axe slices through the log with ease, and the woman continues to swing it until the log is split in half. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Video question answering comparison with Video-LLaVA (Lin et al., 2023) and Video-ChatGPT (Maaz et al., 2023). Video examples are from Video-ChatGPT. In the first example, our model directly points to a salient explanation for why the video is funny. In the second example, our model responses accurately without hallucinating about the video (the woman only swung the ax once).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{142.3pt} p{142.3pt}} \\hline \\hline User & Create a short fairy tale with a moral lesson inspired by the video. \\\\ Video-LAVIT & Once upon a time, there was a beautiful princess who lived in a grand castle. She was known for her kindness and compassion towards others. One day, a handsome prince vasited her castle and fell deeply in love with her. However, the princess was hesitant to accept his love due to her fear of losing her independence and being trapped in a loveless marriage. The prince, determined to win her heart, gave her a beautiful glass slipper as a gift. The princess tried it on and danced with the prince, realizing that true love transcends all boundaries. From that day on, the princess and the prince lived happily ever after, proving that love can conquer all. \\\\ \\hline User & Create a short fairy tale with a moral lesson inspired by the video. \\\\ Video-CLAVA (Lin et al., 2023) & Once upon a time, in a kingdom far away, there lived a beautiful princess named Rose. She was known for her kindness and beauty, and everyone loved her. One day, a handsome prince named Leo came to visit the kingdom. He was fascinated by Rose’s beauty and decided to stay in the kingdom. However, Rose’s jealous sister, Lily, did not like the attention Leo was giving her sister. She tried to harm Rose, but Leo saved her and they fell in love. The kingdom celebrated their love and they lived happily ever after. The moral of the story is that true love conquers all and that jealousy can lead to harmful consequences. \\\\ \\hline User & Create a short fairy tale with a moral lesson inspired by the video. \\\\ Video-ChatGPT (Maaz et al., 2023) & Sure, here’s a short fairy tale with a moral lesson inspired by the video: Once upon a time, there was a beautiful princess named Cinderella who lived in a castle with her cruel stepmother and two stepsisters. She was known for her kindness and her love for animals, especially birds. One day, while she was out in the garden, she met a handsome prince who was lost in the woods. He was in need of help, and Cinderella offered to guide him back to the castle. As they walked together, the prince noticed that Cinderella’s stepsisters were cruel to her and her and stepmother was always mean to her. He asked her about her life and how she managed to stay so kind and gentle despite the hardships she faced. Cinderella told him that she found happiness in the simple things in life, such as the beauty of nature and the love of animals. The prince was touched by her story and decided to help her in any way he could. He introduced her to the king and queen, who were impressed by her kindness and grace. They arranged for her to attend a ball at the castle, where she met the prince once again. They fell in love and got married, living happily ever after. The moral of the story is that even in difficult situations, one should find happiness in the simple things in life and never lose sight of their values and kindness. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: Video question answering comparison with Video-LLAVA (Lin et al., 2023) and Video-ChatGPT (Maaz et al., 2023). Video examples are from Video-ChatGPT. As shown in this example, our model captures the underlying context of this video, which is a love story. The fairy tale told, which implies that love will prevail, is naturally inspired by the video without introducing an unmentioned third party (jealous sister or stepmother) as in the other models. The use of the phrase “beautiful glass slipper” adds a nice detail to the story.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>