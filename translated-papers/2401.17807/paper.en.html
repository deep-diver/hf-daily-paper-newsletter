<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Advances in 3D Generation: A Survey\n' +
      '\n' +
      'Xiaoyu Li1\n' +
      '\n' +
      'Qi Zhang1\n' +
      '\n' +
      'Di Kang1\n' +
      '\n' +
      'Weihao Cheng2\n' +
      '\n' +
      'Yiming Gao2\n' +
      '\n' +
      'Jingbo Zhang3\n' +
      '\n' +
      'Zhihao Liang4\n' +
      '\n' +
      'Jing Liao3\n' +
      '\n' +
      'Yan-Pei Cao1,2\n' +
      '\n' +
      'Ying Shan1,2\n' +
      '\n' +
      '1Tencent AI Lab 2ARC Lab, Tencent PCG 3City University of Hong Kong 4South China University of Technology\n' +
      '\n' +
      '*Equal contribution. \\({}^{\\dagger}\\)Corresponding author.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Generating 3D models lies at the core of computer graphics and has been the focus of decades of research. With the emergence of advanced neural representations and generative models, the field of 3D content generation is developing rapidly, enabling the creation of increasingly high-quality and diverse 3D models. The rapid growth of this field makes it difficult to stay abreast of all recent developments. In this survey, we aim to introduce the fundamental methodologies of 3D generation methods and establish a structured roadmap, encompassing 3D representation, generation methods, datasets, and corresponding applications. Specifically, we introduce the 3D representations that serve as the backbone for 3D generation. Furthermore, we provide a comprehensive overview of the rapidly growing literature on generation methods, categorized by the type of algorithmic paradigms, including feedforward generation, optimization-based generation, procedural generation, and generative novel view synthesis. Lastly, we discuss available datasets, applications, and open challenges. We hope this survey will help readers explore this exciting topic and foster further advancements in the field of 3D content generation.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Automatically generating 3D models using algorithms has long been a significant task in computer vision and graphics. This task has garnered considerable interest due to its broad applications in video games, movies, virtual characters, and immersive experiences, which typically require a wealth of 3D assets. Recently, the success of neural representations, particularly Neural Radiance Fields (NeRFs) [17, 20, 21, 22], and generative models such as diffusion models [14, 23], has led to remarkable advancements in 3D content generation.\n' +
      '\n' +
      'In the realm of 2D content generation, recent advancements in generative models have steadily enhanced the capacity for image generation and editing, leading to increasingly diverse and high-quality results. Pioneering research on generative adversarial networks (GANs) [GPAM\\({}^{*}\\)14, AQW19], variational autoencoders (VAEs) [KPHL17, PGH\\({}^{*}\\)16, KW13], and autoregressive models [RWC*19, BMR*20] has demonstrated impressive outcomes. Furthermore, the advent of generative artificial intelligence (AI) and diffusion models [HJA20, ND21, SCS\\({}^{*}\\)22] signifies a paradigm shift in image manipulation techniques, such as Stable Diffusion [RBL*22a], Imagen [SCS\\({}^{*}\\)22], Midjourney [Mid], or DALL-E 3 [Ope]. These generative AI models enable the creation and editing of photorealistic or stylized images, or even videos [CZC\\({}^{*}\\)24, HSG\\({}^{*}\\)22, SPH\\({}^{*}\\)23, GNL\\({}^{*}\\)23], using minimal input like text prompts. As a result, they often generate imaginative content that transcends the boundaries of the real world, pushing the limits of creativity and artistic expression. Owing to their "emergent" capabilities, these models have redefined the limits of what is achievable in content generation, expanding the horizons of creativity and artistic expression.\n' +
      '\n' +
      'The demand to extend 2D content generation into 3D space is becoming increasingly crucial for applications in generating 3D assets or creating immersive experiences, particularly with the rapid development of the metaverse. The transition from 2D to 3D content generation, however, is not merely a technological evolution. It is primarily a response to the demands of modern applications that necessitate a more intricate replication of the physical world, which 2D representations often fail to provide. This shift highlights the limitations of 2D content in applications that require a comprehensive understanding of spatial relationships and depth perception.\n' +
      '\n' +
      'As the significance of 3D content becomes increasingly evident, there has been a surge in research efforts dedicated to this domain. However, the transition from 2D to 3D content generation is not a straightforward extension of existing 2D methodologies. Instead, it involves tackling unique challenges and re-evaluating data representation, formulation, and underlying generative models to effectively address the complexities of 3D space. For instance, it is not obvious how to integrate the 3D scene representations into 2D generative models to handle higher dimensions, as required for 3D generation. Unlike images or videos which can be easily collected from the web, 3D assets are relatively scarce. Furthermore, evaluating the quality of generated 3D models presents additional challenges, as it is necessary to develop better formulations for objective functions, particularly when considering multi-view consistency in 3D space. These complexities demand innovative approaches and novel solutions to bridge the gap between 2D and 3D content generation.\n' +
      '\n' +
      'While not as prominently featured as its 2D counterpart, 3D content generation has been steadily progressing with a series of notable achievements. The representative examples shown in Fig. 1 demonstrate significant improvements in both quality and diversity, transitioning from early methods like 3D-GAN [WZX\\({}^{*}\\)16] to recent approaches like Instant3D [LTZ\\({}^{*}\\)23]. Therefore, This survey paper seeks to systematically explore the rapid advancements and multifaceted developments in 3D content generation. We present a structured overview and comprehensive roadmap of the many recent works focusing on 3D representations, 3D generation methods, datasets, and applications of 3D content generation, and to outline open challenges.\n' +
      '\n' +
      'Fig. 2 presents an overview of this survey. We first discuss the scope and related work of this survey in Sec. 2. In the following sections, we examine the core methodologies that form the foundation of 3D content generation. Sec. 3 introduces the primary scene representations and their corresponding rendering functions used in 3D content generation. Sec. 4 explores a wide variety of 3D generation methods, which can be divided into four categories based on their algorithmic methodologies: feedforward generation, optimization-based generation, procedural generation, and generative novel view synthesis. An evolutionary tree of these methodologies is also depicted to illustrate their primary branch. As data accumulation plays a vital role in ensuring the success of deep learning models, we present related datasets employed for training 3D generation methods. In the end, we include a brief discussion on related applications, such as 3D human and face generation, outline open challenges, and conclude this survey. We hope this survey offers a systematic summary of 3D generation that could inspire subsequent work for interested readers.\n' +
      '\n' +
      'In this work, we present a comprehensive survey on 3D generation, with two main contributions:\n' +
      '\n' +
      '* Given the recent surge in contributions based on generative models in the field of 3D vision, we provide a comprehensive and timely literature review of 3D content generation, aiming to offer readers a rapid understanding of the 3D generation framework and its underlying principles.\n' +
      '* We propose a multi-perspective categorization of 3D generation methods, aiming to assist researchers working on 3D content generation in specific domains to quickly identify relevant works and facilitate a better understanding of the related techniques.\n' +
      '\n' +
      '## 2Scope of This Survey\n' +
      '\n' +
      'In this survey, we concentrate on the techniques for the generation of 3D models and their related datasets and applications. Specifically, we first give a short introduction to the scene representation. Our focus then shifts to the integration of these representations and the generative models. Then, we provide a comprehensive overview of the prominent methodologies of generation methods. We also explore the related datasets and cutting-edge applications such as 3D human generation, 3D face generation, and 3D editing, all of which are enhanced by these techniques.\n' +
      '\n' +
      'This survey is dedicated to systematically summarizing and categorizing 3D generation methods, along with the related datasets and applications. The surveyed papers are mostly published in major computer vision and computer graphics conferences/journals as well as some preprints released on arXiv in 2023. While it\'s challenging to exhaust all methods related to 3D generation, we hope to include as many major branches of 3D generation as possible. We do not delve into detailed explanations for each branch, instead, we typically introduce some representative works within it to explain its paradigm. The details of each branch can be found in the related work section of these cited papers.\n' +
      '\n' +
      '**Related Survey.** Neural reconstruction and rendering with scene representations are closely related to 3D generation. However, we consider these topics to be outside the purview of this report. For a comprehensive discussion on neural rendering, we direct readers to [17], [18], and for a broader examination of other neural representations, we recommend [19, 20]. Our primary focus is on exploring techniques that generate 3D models. Therefore, this review does not encompass research on generation methods for 2D images within the realm of visual computing. For further information on a specific generation method, readers can refer to [16] (VAEs), [14] (GANs), [21] (GANs), [22] (Diffusion) and [15] (Transformers) for a more detailed understanding. There are also some surveys related to 3D generation that have their own focuses such as 3D-aware image synthesis [23], 3D generative models [24], Text-to-3D [16] and deep learning for 3D point clouds [21]. In this survey, we give a comprehensive analysis of different 3D generation methods.\n' +
      '\n' +
      '## 3 Neural Scene Representations\n' +
      '\n' +
      'In the domain of 3D AI-generated content, adopting a suitable representation of 3D models is essential. The generation process typically involves a scene representation and a differentiable rendering algorithm for creating 3D models and rendering 2D images. Conversely, the created 3D models or 2D images could be supervised in the reconstruction domain or image domain, as illustrated in Fig. 3. Some methods directly supervise the 3D models of the scene representation, while others render the scene representation into images and supervise the resulting renderings. In the following, we broadly classify the scene representations into three groups: explicit scene representations (Section 3.1), implicit representations (Section 3.2), and hybrid representations (Section 3.3). Note that, the rendering methods (_e.g._ ray casting, volume rendering, rasterization, _etc_), which should be differentiable to optimize the scene representations from various inputs, are also introduced.\n' +
      '\n' +
      '### Explicit Representations\n' +
      '\n' +
      'Explicit scene representations serve as a fundamental module in computer graphics and vision, as they offer a comprehensive means of describing 3D scenes. By depicting scenes as an assembly of basic primitives, including point-like primitives, triangle-based meshes, and advanced parametric surfaces, these representations can create detailed and accurate visualizations of various environments and objects.\n' +
      '\n' +
      '#### 3.1.1 Point Clouds\n' +
      '\n' +
      'A point cloud is a collection of elements in Euclidean space, representing discrete points with addition attributes (_e.g._ colors and normals) in three-dimensional space. In addition to simple points, which can be considered infinitesimally small surface patches, oriented point clouds with a radius (surfels) can also be used [23]. Surfels are used in computer graphics for rendering\n' +
      '\n' +
      'Figure 2: Overview of this survey, including 3D representations, 3D generation methods, datasets and applications. Specifically, we introduce the 3D representations that serve as the backbone for 3D generation. Furthermore, we provide a comprehensive overview of the rapidly growing literature on generation methods, categorized by the type of algorithmic paradigms, including feedforward generation, optimization-based generation, procedural generation, and generative novel view synthesis. Finally, we provide a brief discussion on popular datasets and available applications.\n' +
      '\n' +
      'point clouds (called splitting), which are differentiable [19, 17] and allow researchers to define differentiable rendering pipelines to adjust point cloud positions and features, such as radius or color. Techniques like Neural Point-based Rendering [17, 18, 19], SynSin [20], Pulsar [16, 15] and ADOP [14] leverage learnable features to store information about the surface appearance and shape, enabling more accurate and detailed rendering results. Several other methods, such as FVS [13], SVS [14], and FWD-Transformer [15], also employ learnable features to improve the rendering quality. These methods typically embed features into point clouds and warp them to target views to decode color values, allowing for more accurate and detailed reconstructions of the scene.\n' +
      '\n' +
      'By incorporating point cloud-based differentiable renderers into the 3D generation process, researchers can leverage the benefits of point clouds while maintaining compatibility with gradient-based optimization techniques. This process can be generally categorized into two different ways: point splitting which blends the discrete samples with some local deterministic blurring kernels [16, 17, 18], and conventional point renderer [17, 18, 19]. These methods facilitate the generation and manipulation of 3D point cloud models while maintaining differentiability, which is essential for training and optimizing neural networks in 3D generation tasks.\n' +
      '\n' +
      '#### 3.1.2 Meshes\n' +
      '\n' +
      'By connecting multiple vertices with edges, more complex geometric structures (_e.g._ wireframes and meshes) can be formed [19]. These structures can then be further refined by using polygons, typically triangles or quadrilaterals, to create realistic representations of objects [16]. Meshes provide a versatile and efficient means of representing intricate shapes and structures, as they can be easily manipulated and rendered by computer algorithms. The majority of graphic editing toolchains utilize triangle meshes. This type of representation is indispensable for any digital content creation (DCC) pipeline, given its wide acceptance and compatibility. To align seamlessly with these pipelines, neural networks can be strategically trained to predict discrete vertex locations [16, 17]. This ability allows for the direct importation of these locations into any DCC pipeline, facilitating a smooth\n' +
      '\n' +
      'Figure 3: Neural scene representations used for 3D generation, including explicit, implicit, and hybrid representations. The 3D generation involves the use of scene representations and a differentiable rendering algorithm to create 3D models or render 2D images. On the flip side, these 3D models or 2D images can function as the reconstruction domain or image domain, overseeing the 3D generation of scene representations.\n' +
      '\n' +
      'and efficient workflow. In contrast to predicting discrete textures, continuous texture methods optimized via neural networks are proposed, such as texture fields [19] and NeRF-Tex [20]. In this way, it could provide a more refined and detailed texture, enhancing the overall quality and realism of the generated 2D models.\n' +
      '\n' +
      'Integrating mesh representation into 3D generation requires the use of mesh-based differentiable rendering methods, which enable meshes to be rasterized in a manner that is compatible with gradient-based optimization. Several such techniques have been proposed, including OpenDR [1], neural mesh renderer [18], Paparazzi [19], and Soft Rasterizer [19]. Additionally, general-purpose physically based renderers like Mitsuba [2] and Taichi [18] support mesh-based differentiable rendering through automatic differentiation.\n' +
      '\n' +
      '#### 3.1.3 Multi-layer Representations\n' +
      '\n' +
      'The use of multiple semi-transparent colored layers for representing scenes has been a popular and successful scheme in real-time novel view synthesis [21]. Using Layered Depth Image (LDI) representation [14] is a notable example, extending traditional depth maps by incorporating multiple layers of depth maps, each with associated color values. Several methods [22, 19, 20] have drawn inspiration from the LDI representation and employed deep learning advancements to create networks capable of predicting LDIs. In addition to LDIs, Stereomagnification [21] initially introduced the multiple image (MPI) representation. It describes scenes using multiple front-parallel semi-transparent layers, including colors and opacity, at fixed depth ranges through plane sweep volumes. With the help of volume rendering and homography projection, the novel view could be synthesized in real-time. Building on Stereomagnification [21], various methods [22, 23, 24, 25] have adopted the MPI representation to enhance rendering quality. The multi-layer representation has been further expanded to accommodate wider fields of view in [20], [23, 24] by substituting planes with spheres. As research in this domain continues to evolve, we can expect further advancements in these methods, leading to more efficient and effective 3D generation techniques for real-time rendering.\n' +
      '\n' +
      '### Implicit Representations\n' +
      '\n' +
      'Implicit representations have become the scene representation of choice for problems in view synthesis or shape reconstruction, as well as many other applications across computer graphics and vision. Unlike explicit scene representations that usually focus on object surfaces, implicit representations could define the entire volume of a 3D object, and use volume rendering for image synthesis. These representations utilize mathematical functions, such as radiance fields [21] or signed distance fields [23, 24], to describe the properties of a 3D space.\n' +
      '\n' +
      '#### 3.2.1 Neural Radiance Fields\n' +
      '\n' +
      'Neural Radiance Fields (NeRFs) [21] have gained prominence as a favored scene representation method for a wide range of applications. Fundamentally, NeRFs introduce a novel representation of 3D scenes or geometries. Rather than utilizing point clouds and meshes, NeRFs depict the scene as a continuous volume. This approach involves obtaining volumetric parameters, such as view-dependent radiance and volume density, by querying an implicit neural network. This innovative representation offers a more fluid and adaptable way to capture the intricacies of 3D scenes, paving the way for enhanced rendering and modeling techniques.\n' +
      '\n' +
      'Specifically, NeRF [21] represents the scene with a continuous volumetric radiance field, which utilizes MLPs to map the position \\(\\mathbf{x}\\) and view direction \\(\\mathbf{r}\\) to a density \\(\\mathbf{\\sigma}\\) and color \\(\\mathbf{c}\\). To render a pixel\'s color, NeRF casts a single ray \\(\\mathbf{r}(t)=\\mathbf{\\phi}+t\\mathbf{d}\\) and evaluates a series of points \\(\\{t_{i}\\}\\) along the ray. The evaluated \\(\\{(\\mathbf{\\sigma}_{i},\\mathbf{c}_{i})\\}\\) at the sampled points are accumulated into the color \\(C(\\mathbf{r})\\) of the pixel via volume rendering [20]:\n' +
      '\n' +
      '\\[C(r)=\\sum_{i}T_{i}\\alpha_{i}\\mathbf{c}_{i},\\quad\\text{where }T_{i}=\\exp\\left(- \\sum_{k=0}^{i-1}\\sigma_{k}\\delta_{k}\\right), \\tag{1}\\]\n' +
      '\n' +
      'and \\(\\alpha_{i}=1-\\exp(-\\sigma_{i}\\delta_{k})\\) indicates the opacity of the sampled point. Accumulated transmittance \\(T_{i}\\) quantifies the probability of the ray traveling from \\(t_{0}\\) to \\(t_{i}\\) without encountering other particles, and \\(\\delta_{i}=t_{i}-t_{i-1}\\) denotes the distance between adjacent samples.\n' +
      '\n' +
      'NeRFs [21, 22, 23, 24, 21], BMV*22, VHM*22, LWC*23] have seen widespread success in problems such as edition [21, ZLLD21, CZL*22, YSL*22], joint optimization of cameras [23, WKN*21, CCW*23, TRMT23], inverse rendering [21, SDZ*21, BB*21, ZSD*21, ZZW*23, LZF*23], generalization [22, WWG*21, CXZ*21, LFS*21, JLF22, HZF*23b], acceleration [21, GK+21, ZZZ*23b], and free-viewpoint video [21, LSZ*22, PCPMMN21]. Apart from the above applications, NeRF-based representation can also be used for digit avatar generation, such as face and body reenactment [23, GCL*21, LHR*21, WCS*22, HPX*22]. NeRFs have been extend to various fields such as robotics [21, ZKW*23, ACC*22], tomography [24, ZLZ*22], image processing [21, MLL*22b, HZF*23a], and astronomy [22].\n' +
      '\n' +
      '#### 3.2.2 Neural Implicit Surfaces\n' +
      '\n' +
      'Within the scope of shape reconstruction, a neural network processes a 3D coordinate as input and generates a scalar value, which usually signifies the signed distance to the surface. This method is particularly effective in filling in missing information and generating smooth, continuous surfaces. The implicit surface representation defines the scene\'s surface as a learnable function \\(f\\) that specifies the signed distance \\(f(\\mathbf{x})\\) from each point to the surface. The fundamental surface can then be extracted from the zero-level set, \\(S=\\{\\mathbf{x}\\in\\mathbb{R}^{3}|f(\\mathbf{x})=0\\}\\), providing a flexible and efficient way to reconstruct complex 3D shapes. Implicit surface representations offer numerous advantages, as they eliminate the need to define mesh templates. As a result, they can represent objects with unknown or changing topology in dynamic scenarios. Specifically, implicit surface representations recover signed distance fields for shape modeling using MLPs with coordinate inputs. These initial proposals sparked widespread enthusiasm and led to various improvements focusing on different aspects, such as enhancing training schemes [21, 22], VAK*20, ZML*22], leveraging global-local context [21, EGO*20, ZPL*22], adopting specific parameterizations [19, 20, 21], and employing spatial partitions [14, 20, 22, 23].\n' +
      '\n' +
      'NeuS [24] and VolSDF [25] extend the basic NeRF formulation by integrating an SDF into volume rendering, which defines a function to map the signed distance to density \\(\\sigma\\). It attains a locally maximal value at surface intersection points. Specifically, accumulated transmittance \\(T(t)\\) along the ray \\(\\mathbf{r}(t)=\\mathbf{o}+t\\mathbf{d}\\) is formulated as a sigmoid function: \\(T(t)=\\Phi(f(t))=(1+e^{sf(t)})^{-1}\\), where \\(s\\) and \\(f(t)\\) refer to a learnable parameter and the signed distance function of points at \\(\\mathbf{r}(t)\\), respectively. Discrete opacity values \\(\\alpha_{i}\\) can then be derived as:\n' +
      '\n' +
      '\\[\\alpha_{i}=\\max\\left(\\frac{\\Phi_{\\mathbf{x}}\\left(f(t_{i})\\right)-\\Phi_{ \\mathbf{x}}\\left(f(t_{i+1})\\right)}{\\Phi_{\\mathbf{x}}\\left(f(t_{i})\\right)},0 \\right). \\tag{2}\\]\n' +
      '\n' +
      'NeuS employs volume rendering to recover the underlying SDF based on Eqs. (1) and (2). The SDF is optimized by minimizing the photometric loss between the rendering results and ground-truth images.\n' +
      '\n' +
      'Building upon NeuS and VolSDF, NeuralWarp [26], GeoNeuS [27], MonoSDF [25] leverage prior geometry information from MVS methods. IRON [28], MII [29], and WildLight [28] apply high-fidelity shape reconstruction via SDF for inverse rendering. HF-NeuS [30] and PET-Neus [31] integrate additional displacement networks to fit the high-frequency details. LoD-NeuS [27] adaptively encodes Level of Detail (LoD) features for shape reconstruction.\n' +
      '\n' +
      '### Hybrid Representations\n' +
      '\n' +
      'Implicit representations have indeed demonstrated impressive results in various applications as mentioned above. However, most of the current implicit methods rely on regression to NeRF or SDF values, which may limit their ability to benefit from explicit supervision on the target views or surfaces. Explicit representation could impose useful constraints during training and improve the user experience. To capitalize on the complementary benefits of both representations, researchers have begun exploring hybrid representations. These involve scene representations (either explicit or implicit) that embed features utilizing rendering algorithms for view synthesis.\n' +
      '\n' +
      '#### 3.3.1 Voxel Grids\n' +
      '\n' +
      'Early work [24, 25] depicting 3D shapes using voxels, which store coarse occupancy (inside/outside) values on a regular grid. This approach enabled powerful convolutional neural networks to operate natively and produce impressive results in 3D reconstruction and synthesis [23, 24, 25]. These methods usually use explicit voxel grids as the 3D representation. Recently, to address the slow training and rendering speeds of implicit representations, the 3D voxel-based embedding methods [19, 25, 26] have been proposed. These methods encode the spatial information of the scene and decode the features more efficiently. Moreover, Instant-NGP [26] introduces the multi-level voxel grids encoded implicitly via the hash function for each level. It facilitates rapid optimization and rendering while maintaining a compact model. These advancements in 3D shape representation and processing techniques have significantly enhanced the efficiency and effectiveness of 3D generation applications.\n' +
      '\n' +
      '#### 3.3.2 Tri-plane\n' +
      '\n' +
      'Tri-plane representation is an alternative approach to using voxel grids for embedding features in 3D shape representation and neural rendering. The main idea behind this method is to decompose a 3D volume into three orthogonal planes (e.g., XY, XZ, and YZ planes) and represent the features of the 3D shape on these planes. Specifically, TensoRF [27] achieves similar model compression and acceleration by replacing each voxel grid with a tensor decomposition into planes and vectors. Tri-planes are efficient and capable of scaling with the surface area rather than volume and naturally integrate with expressive, fine-tuned 2D generative architectures. In the generative setting, EG3D [28] proposes a spatial decomposition into three planes whose values are added together to represent a 3D volume. NFD [26] introduces diffusion on 3D scenes, utilizing 2D diffusion model backbones and having built-in tri-plane representation.\n' +
      '\n' +
      '#### 3.3.3 Hybrid Surface Representation\n' +
      '\n' +
      'DMTet, a recent development cited in [26], is a hybrid three-dimensional surface representation that combines both explicit and implicit forms to create a versatile and efficient model. It segments the 3D space into dense tetrahedra, thereby forming an explicit partition. By integrating explicit and implicit representations, DMTet can be optimized more efficiently and transformed seamlessly into explicit structures like mesh representations. During the generation process, DMTet can be differentiably converted into a mesh, which enables swift high-resolution multi-view rendering. This innovative approach offers significant improvements in terms of efficiency and versatility in 3D modeling and rendering.\n' +
      '\n' +
      '## 4 Generation Methods\n' +
      '\n' +
      'In the past few years, the rapid development of generative models in 2D image synthesis, such as generative adversarial networks (GANs) [18, 25], variational autoencoders (VAEs) [17, 19, 26], autoregressive models [27, 28], diffusion models [16, 29, 28], _etc._, has led to their extension and combination with these scene representations for 3D generation. Tab. 1 shows well-known examples of 3D generation using generative models and scene representations. These methods may use different scene representations in the generation space, where the representation is generated by the generative models, and the reconstruction space, where the output is represented. For example, AutoSDF [26] uses a transformer-based autoregressive model to learn a feature voxel grid and decode this representation to SDF for reconstruction. EG3D [28] employs GANs to generate samples in latent space and introduces a tri-plane representation for rendering the output. SSDNeRF [26] uses the diffusion model to generate tri-plane features and decode them to NeRF for rendering. By leveraging the advantages of neural scene representations and generative models, these approaches have demonstrated remarkable potential in generating realistic and intricate 3D models while maintaining view consistency.\n' +
      '\n' +
      'In this section, we explore a large variety of 3D generation methods which are organized into four categories based on their algorithmic paradigms: Feedforward Generation (Sec. 4.1), generating results in a forward pass; Optimization-Based Generation (Sec. 4.2), necessitating a test-time optimization for each generation; Procedural Generation (Sec. 4.3), creating 3D models from sets of rules; and Generative Novel View Synthesis (Sec. 4.4), synthesizing multi-view images rather than an explicit 3D representation for 3D generation. An evolutionary tree of 3D generation methods is depicted in Fig. 4, which illustrates the primary branch of generation techniques, along with associated work and subsequent developments. A comprehensive analysis will be discussed in the subsequent subsection.\n' +
      '\n' +
      '### Feedforward Generation\n' +
      '\n' +
      'A primary technical approach for generation methods is feedforward generation, which can directly produce 3D representations using generative models. In this section, we explore these methods based on their generative models as shown in Fig. 5, which include generative adversarial networks (GANs), diffusion Models, autoregressive models, variational autoencoders (VAEs) and normalizing flows.\n' +
      '\n' +
      '#### 4.1.1 Generative Adversarial Networks\n' +
      '\n' +
      'Generative Adversarial Networks (GANs) [1] have demonstrated remarkable outcomes in image synthesis tasks, consisting of a generator \\(G(\\cdot)\\) and a discriminator \\(D(\\cdot)\\). The generator network \\(G\\) produces synthetic data by accepting latent code as input, while the discriminator network \\(D\\) differentiates between generated data from \\(G\\) and real data. Throughout the training optimization process, the generator \\(G\\) and discriminator \\(D\\) are jointly optimized, guiding the generator to create synthetic data as realistic as real data.\n' +
      '\n' +
      'Building on the impressive results achieved by GANs in 2D image synthesis, researchers have begun to explore the application of these models to 3D generation tasks. The core idea is to marry GANs with various 3D representations, such as point clouds (l-GAN/r-GAN [1], tree-GAN [14]), voxel grids (3D-GAN [15], Z-GAN [16]), meshes (MeshGAN [17]), or SDF (SurfGen [11], SDF-StyleGAN [18]). In this context, the 3D generation process can be viewed as a series of adversarial steps, where the generator learns to create realistic 3D data from input latent codes, and the discriminator differentiates between generated data and real data. By iteratively optimizing the generator and discriminator networks, GANs learn to generate 3D data that closely resembles the realism of actual data.\n' +
      '\n' +
      'For 3D object generation, prior GAN methodologies, such as l-GAN [1], 3D-GAN [15], and Multi-chart Generation [1], directly utilize explicit 3D object representation of real data to instruct generator networks. Their discriminators employ 3D representation as supervision, directing the gener\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline Method & Generative Model & Generation Space & Reconstruction Space & Rendering & Supervision & Condition \\\\ \\hline PointFlow [16] & Normalizing Flow & Latent Code & Point Cloud & - & 3D & Uncon \\\\\n' +
      '3dAE [17] & VAE & Latent Code & Point Cloud & - & 3D & Uncon \\\\ SDM-NET [14] & VAE & Latent Code & Mesh & - & 3D & Uncon \\\\ \\hline AutoSDF [16] & Autoregressive & Voxel & SDF & - & 3D & Uncon. \\\\ PolyGen [15] & Autoregressive & Polygon & Mesh & - & 3D & Uncon./Label/Image \\\\ PointGrow [14] & Autoregressive & Point & Point Cloud & - & 3D & Uncon./Label/Image \\\\ \\hline EG3D [17] & GAN & Latent Code & Tri-plane & Mixed Rendering & 2D & Uncon. \\\\ GIRAFFE [18] & GAN & Latent Code & NeRF & Mixed Rendering & 2D & Uncon. \\\\ BlockGAN [15] & GAN & Latent Code & Voxel Grid & Network Rendering & 2D & Uncon. \\\\ gDNA [17] & GAN & Latent Code & Occupancy Field & Surface Rendering & 2D\\&3D & Uncon. \\\\ SurfGen [11] & GAN & Latent Code & SDF & - & 3D & Uncon. \\\\ Free-GAN [15] & GAN & Latent Code & Point Cloud & - & 3D & Uncon. \\\\ \\hline HoloDiffusion [14] & Diffusion & Voxel & NeRF & Volume Rendering & 2D & Image \\\\ SSDNeRF [16] & Diffusion & Tri-plane & NeRF & Volume Rendering & 2D & Uncon./Image \\\\\n' +
      '3DShape2VecSet [15] & Diffusion & Latent Set & SDF & - & 3D & Uncon./Text/Image \\\\ Point-E [18] & Diffusion & Point & Point Cloud & - & 3D & Text \\\\\n' +
      '3DGen [14] & Diffusion & Tri-plane & Mesh & - & 3D & Text/Image \\\\ DreamFusion [15] & Diffusion & - & NeRF & Volume Rendering & SDS & Text \\\\ Make-h-3D [15] & Diffusion & - & Point Cloud & Network Rendering & SDS & Image \\\\ Zero-1to-3 [15] & Diffusion & Pixel & - & - & 2D & Image \\\\ MDFrame [14] & Diffusion & Pixel & - & - & 2D & Image \\\\ DMV2D [16] & Diffusion & Pixel & Tri-plane & Volume Rendering & 2D & Text/Image \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Some examples of 3D generation methods. We first divide the methods according to the generative models and their corresponding representations in generation space. The representations in the reconstruction space determine how the 3D objects are formatted and rendered. We also list the main supervision and conditions of these methods. For the 2D supervision, a rendering technique is utilized to generate the images.\n' +
      '\n' +
      'Figure 4: The evolutionary tree of 3D generation illustrates the primary branch of generation methods and their developments in recent years. Specifically, we provide a comprehensive overview of the rapidly growing literature on generation methods, categorized by the type of algorithmic paradigms, including feedforward generation, optimization-based generation, procedural generation, and generative novel view synthesis.\n' +
      '\n' +
      'ator to produce synthetic data that closely resembles the realism of actual data. During training, specialized generators generate corresponding supervisory 3D representations, such as point clouds, voxel grids, and meshes. Some studies, like SurfGen [11], have progressed further to generate intermediate implicit representations and then convert them to corresponding 3D representations instead of directly generating explicit ones, achieving superior performance. In particular, the generator of 1-GAN [1], 3D-GAN [21], and Multi-chart Generation [2] generate the position of point cloud, voxel grid, and mesh directly, respectively, taking latent code as input. SurfGen [11] generates implicit representation and then extracts explicit 3D representation.\n' +
      '\n' +
      'In addition to GANs that directly generate various 3D representations, researchers have suggested incorporating 2D supervision through differentiable rendering to guide 3D generation, which is commonly referred to as 3D-Aware GAN. Given the abundance of 2D images, GANs can better understand the implicit relationship between 2D and 3D data than relying solely on 3D supervision. In this approach, the generator of GANs generates rendered 2D images from implicit or explicit 3D representation. Then the discriminators distinguish between rendered 2D images and real 2D images to guide the training of the generator.\n' +
      '\n' +
      'Specifically, HoloGAN [20] first learns a 3D representation of 3D features, which is then projected to 2D features by the camera pose. These 2D feature maps are then rendered to generate the final images. BlockGAN [20] extends it to generate 3D features of both background and foreground objects and combine them into 3D features for the whole scene. In addition, PrGAN [2] and PlatonicGAN [15] employ an explicit voxel grid structure to represent 3D shapes and use a render layer to create images. Other methods like DIB-R [19], ConvMesh [20], Textured3DGAN [3] and GET3D [21] propose GAN frameworks for generating triangle meshes and textures using only 2D supervision.\n' +
      '\n' +
      'Building upon representations such as NeRFs, GRAF [14] proposes generative radiance fields utilizing adversarial frameworks and achieves controllable image synthesis at high resolutions. pi-GAN [20] introduces SIREN-based implicit GANs with FiLM conditioning to further improve image quality and view consistency. GIRAFFE [20] represents scenes as compositional generative neural feature fields to model multi-object scenes. Furthermore, EG3D [20] first proposes a hybrid explicit-implicit tri-plane representation that is both efficient and expressive and has been widely adopted in many following works.\n' +
      '\n' +
      '#### 4.1.2 Diffusion Models\n' +
      '\n' +
      'Diffusion models [15, 16] are a class of generative models that learn to generate data samples by simulating a diffusion process. The key idea behind diffusion models is to transform the original data distribution into a simpler distribution, such as Gaussian, through a series of noise-driven steps called the forward process. The model then learns to reverse this process, known as the backward process, to generate new samples that resemble the original data distribution. The forward process can be thought of as gradually adding noise to the original data until it reaches the target distribution. The backward process, on the other hand, involves iteratively denoising the samples from the distribution to generate the final output. By learning this denoising process, diffusion models can effectively capture the underlying structure and patterns of the data, allowing them to generate high-quality and diverse samples.\n' +
      '\n' +
      'Building on the impressive results achieved by diffusion models in generating 2D images, researchers have begun to explore the applications of these models to 3D generation tasks. The core idea is to marry denoising diffusion models with various 3D representations. In this context, the 3D generation process can be viewed as a series of denoising steps, reversing the diffusion process from input 3D data to Gaussian noise. The diffusion models learn to generate 3D data from this noisy distribution through denoising.\n' +
      '\n' +
      'Figure 5: Exemplary feedforward 3D generation models. We showcase several representative pipelines of feedforward 3D generation models, including (a) generative adversarial networks, (b) diffusion models, (c) autoregressive models, (d) variational autoencoders and (e) normalizing flows.\n' +
      '\n' +
      'Specifically, Cai et al. [2] build upon a denoising score-matching framework to learn distributions for point cloud generation. PVD [15] combines the benefits of both point-based and voxel-based representations for 3D generation. The model learns a diffusion process that transforms point clouds into voxel grids and vice versa, effectively capturing the underlying structure and patterns of the 3D data. Similarly, DPM [14] focuses on learning a denoising process for point cloud data by iterative denoising the noisy point cloud samples. Following the advancements made by PVD [15] and DPM [14], LION [13] builds upon the idea of denoising point clouds and introduces the concept of denoising in the latent space of point clouds, which is analogous to the shift in 2D image generation from denoising pixels to denoising latent space representations. To generate point clouds from text prompts, Point-E [12] initially employs the GLIDE model [12] to generate text-conditional synthetic views, followed by the production of a point cloud using a diffusion model conditioned on the generated image. By training the model on a large-scale 3D dataset, it achieves remarkable generalization capabilities.\n' +
      '\n' +
      'In addition to point clouds, MeshDiffusion [10], Tetrahedral Diffusion Models [11], and SLIDE [13] explore the application of diffusion models to mesh generation. MeshDiffusion [10] adopts the DMTet representation [21] for meshes and optimizes the model by treating the optimization of signed distance functions as a denoising process. Tetrahedral Diffusion Models [11] extends diffusion models to tetrahedral meshes, learning displacement vectors and signed distance values on the tetrahedral grid through denoising. SLIDE [13] explores diffusion models on sparse latent points for mesh generation.\n' +
      '\n' +
      'Apart from applying diffusion operations on explicit 3D representations, some works focus on performing the diffusion process on implicit representations. SSDNeRF [20], DiffRF [22] and Shap-E [23] operate on 3D radiance fields, while SDF-Diffusion [21], LAS-Diffusion [15], Neural Wavelet-domain Diffusion [16], One-2-3-45++ [21], SDFusion [22] and 3D-LDM [23] focus on signed distance fields representations. Specifically, Diffusion-SDF [20] utilizes a voxel-shaped SDF representation to generate high-quality and continuous 3D shapes. 3D-LDM [23] creates neural implicit representations of SDFs by initially using a diffusion model to generate the latent space of an auto-decoder. Subsequently, the latent space is decoded into SDFs to acquire 3D shapes. Moreover, Rodin [13] and Shue et al. [20] adopt tri-plane as the representation and optimize the tri-plane features using diffusion methods. Shue et al. [20] generates 3D shapes using occupancy networks, while Rodin [13] obtains 3D shapes through volumetric rendering.\n' +
      '\n' +
      'These approaches showcase the versatility of diffusion models in managing various 3D representations, including both explicit and implicit forms. By tailoring the denoising process to different representation types, diffusion models can effectively capture the underlying structure and patterns of 3D data, leading to improved generation quality and diversity. As research in this area continues to advance, it is expected that diffusion models will play a crucial role in pushing the boundaries of 3D shape generation across a wide range of applications.\n' +
      '\n' +
      '#### 4.1.3 Autoregressive Models\n' +
      '\n' +
      'A 3D object can be represented as a joint probability of the occurrences of multiple 3D elements:\n' +
      '\n' +
      '\\[p(x_{0},x_{1},...,x_{n}), \\tag{3}\\]\n' +
      '\n' +
      'where \\(x_{i}\\) is the \\(i\\)-th element which can be the coordinate of a point or a voxel. A joint probability with a large number of random variables is usually hard to learn and estimate. However, one can factorize it into a product of conditional probabilities:\n' +
      '\n' +
      '\\[p(x_{0},x_{1},...,x_{n})=p(x_{0})\\prod_{i=1}^{n}p(x_{i}|x_{<i}), \\tag{4}\\]\n' +
      '\n' +
      'which enables learning conditional probabilities and estimating the joint probability via sampling. Autoregressive models for data generation are a type of models that specify the current output depending on their previous outputs. Assuming that the elements \\(x_{0}\\), \\(x_{1}\\),..., \\(x_{n}\\) form an ordered sequence, a model can be trained by providing it with previous inputs \\(x_{0}\\),... \\(x_{i-1}\\) and supervising it to fit the probability of the outcome \\(x_{i}\\):\n' +
      '\n' +
      '\\[p(x_{i}|x_{<i})=f(x_{0},...,x_{i-1}), \\tag{5}\\]\n' +
      '\n' +
      'the conditional probabilities are learned by the model function \\(f\\). This training process is often called teacher forcing. The model can be then used to autoregressively generate the elements step-by-step:\n' +
      '\n' +
      '\\[x_{i}=\\text{argmax }p(x|x_{<i}). \\tag{6}\\]\n' +
      '\n' +
      'State-of-the-art generative models such as GPTs [12, 13] are autoregressive generators with Transformer networks as the model function. They achieve great success in generating natural languages and images. In 3D generation, several studies have been conducted based on autoregressive models. In this section, we discuss some notable examples of employing autoregressive models for 3D generation.\n' +
      '\n' +
      'PointGrow [23] generates point clouds using an autoregressive network with self-attention context awareness operations in a point-by-point manner. Given its previously generated points, PointGrow reforms the points by axes and passes them into three branches. Each branch takes the inputs to predict a coordinate value of one axis. The model can also condition an embedding vector to generate point clouds, which can be a class category or an image. Inspired by the network from PointGrow, PolyGen [23] generates 3D meshes with two transformer-based networks, one for vertices and one for faces. The vertex transformer autoregressively generates the next vertex coordinate based on previous vertices. The face transformer takes all the output vertices as context to generate faces. PolyGen can condition on a context of object classes or images, which are cross-attended by the transformer networks.\n' +
      '\n' +
      'Recently, AutoSDF [20] generates 3D shapes represented by volumetric truncated-signed distance function (T-SDF). AutoSDF learns a quantized codebook regarding local regions of T-SDFs using VQ-VAE. The shapes are then presented by the codebook tokens and learned by a transformer-based network in a non-sequential autoregressive manner. In detail, given previous tokens at arbitrary locations and a query location, the network predicts the token that is queried. AutoSDF is capable of completing shapes and generating shapes based on images or text. Concurrently with AutoSDF, ShapeFormer [14] generates surfaces of 3D shapes based on incomplete and noisy point clouds. A compact 3D representation called vector quantized deep implicit function (VQDIF) is used to represent shapes using a feature sequence of discrete variables. ShapeFormer first encodes an input point cloud into a partial feature sequence. It then uses a transformer-based network to autoregressively sample out the complete sequence. Finally, it decodes the sequence to a deep implicit function from which the complete object surface can be extracted. Instead of learning in 3D volumetric space, Luo et al.proposes an improved auto-regressive model (ImAM) to learn discrete representation in a one-dimensional space to enhance the efficient learning of 3D shape generation. The method first encodes 3D shapes of volumetric grids into three axis-aligned planes. It uses a coupling network to further project the planes into a latent vector, where vector quantization is performed for discrete tokens. ImAM adopts a vanilla transformer to autoregressively learn the tokens with tractable orders. The generated tokens are decoded to occupancy values via a network by sampling spatial locations. ImAM can switch from unconditional generation to conditional generation by concatenating various conditions, such as point clouds, categories, and images.\n' +
      '\n' +
      '#### 4.1.4 Variational Autoencoders\n' +
      '\n' +
      'Variational autoencoders (VAEs) [15] are probabilistic generative models that consist of two neural network components: the encoder and decoder. The encoder maps the input data point to a latent space that corresponds to the parameters of a variational distribution. In this way, the encoder can produce multiple different samples that all come from the same distribution. The decoder maps from the latent space to the input space, to produce or generate data points. Both networks are typically trained together with the usage of the reparameterization trick, although the variance of the noise model can be learned separately. VAEs have also been explored in 3D generation [16, 17, 18, 19, 20].\n' +
      '\n' +
      'Brock et al. trains variational autoencoders directly for voxels using 3D ConvNet, while SDM-Net [19] focuses on the generation of structured meshes composed of deformable parts. The method uses one VAE network to model parts and another to model the whole object. The follow-up work TM-Net [19] could generate texture maps of meshes in a part-aware manner. Other representations like point clouds [16] and NeRFs [18] are also explored in variational autoencoders. Owing to the reconstruction-focused objective of VAEs, their training is considerably more stable than that of GANs. However, VAEs tend to produce more blurred results compared to GANs.\n' +
      '\n' +
      '#### 4.1.5 Normalizing Flows\n' +
      '\n' +
      'Normalizing flow models consist of a series of invertible transformations that map a simple distribution, such as Gaussian, to a target distribution, which represents the data to generation. These transformations are carefully designed to be differentiable and invertible, allowing one to compute the likelihood of the data under the model and optimize the model parameters using gradient-based optimization techniques.\n' +
      '\n' +
      'In 3D generation, PointFlow [18] learns a distribution of shapes and a distribution of points using continuous normalizing flows. This approach allows for the sampling of shapes, followed by the sampling of an arbitrary number of points from a given shape. Discrete PointFlow (DPF) network [18] improves PointFlow by replacing continuous normalizing flows with discrete normalizing flows, which reduces the training and sampling time. SoftFlow [19] is a framework for training normalizing flows on the manifold. It estimates a conditional distribution of the perturbed input data instead of learning the data distribution directly. SoftFlow alleviates the difficulty of forming thin structures for flow-based models.\n' +
      '\n' +
      '### Optimization-based Generation\n' +
      '\n' +
      'Optimization-based generation is employed to generate 3D models using runtime optimization. These methods usually leverage pre-trained multimodal networks to optimize 3D models based on user-specified prompts. The key lies in achieving alignment between the given prompts and the generated content while maintaining high fidelity and diversity. In this section, we primarily examine optimization-based generation methods that use texts and images, based on the types of prompts provided by users.\n' +
      '\n' +
      '#### 4.2.1 Text-to-3D\n' +
      '\n' +
      'Language serves as the primary means of human communication and describing scenes, and researchers are dedicated to exploring the potential of text-based generation methods. These methods typically align the text with the images obtained through the differentiable rendering techniques, thereby guiding the generation of 3D content based on the text prompts. Given a fixed surface, TANGO [19] uses CLIP [17] to supervise differentiable physical-based rendering (PBR) images and obtain texture maps that align with the specified text prompt. Inspired by the success of NeRF [17] and diffusion models in modeling 3D static scenes and text-to-image tasks respectively, Dream-Fusion [19] (as shown in Fig. 6) combines the volumetric\n' +
      '\n' +
      'Figure 6: Results of text-guided 3D generation by DreamFusion [19] using SDS loss. \\(*\\) denotes a DSLR photo, \\(\\dagger\\) denotes a zoomed out DSLR photo.\n' +
      '\n' +
      'representation used in NeRF with the proposed Score Distillation Sampling (SDS) loss to achieve high-fidelity 3D content generation. SDS loss converts rendering error minimization into probability density distillation and enables 2D diffusion priors to optimize 3D representations (e.g., volumetric representation and triangle mesh) via image parameterization (e.g., differentiable rendering). As a concurrent work concurrent with SDS, Score Jacobian Chaining (SJC) [WDL\\({}^{*}\\)23] interprets predictions from pre-trained diffusion models as a score function of the data log-likelihood, similarly enabling 2D diffusion priors to optimize 3D representations via score matching. Based on DreamFusion, Magic3D [LGT\\({}^{*}\\)23] introduces a coarse-to-fine manner and extracts the underlying geometry of the volume as a mesh. It then combines differentiable neural rendering and SDS to refine the extracted mesh. Magic3D is capable of exporting high-quality textured meshes and seamlessly embedding them into the traditional graphics pipeline. Also as a two-stage method, Fantasia3D further combines DMTet [SGY\\({}^{*}\\)21] and SDS in the first geometry modeling stage to explicitly optimize surface. In the second stage, it introduces the PBR material model and disentangle texture and environment illumination. ProlificDreamer [WLW\\({}^{*}\\)23] presents variational score distillation (VSD) to boost text-to-3D generation. VSD adopts particles to model the distribution of 3D scenes and derive a gradient-based optimization scheme from the Wasserstein gradient flow, narrowing the gap between the rendering results distribution of the modeling distribution and pre-trained diffusion distribution. Benefiting from the optimization of scene distribution rather than a single scene, VSD overcomes the over-saturated and over-smoothed results produced by SDS and improves diversities. MVDream [SWY\\({}^{*}\\)23] further fine-tunes a multi-view diffusion model and introduces multi-view consistent 3D priors, overcoming multi-face and content-drift problems. Text-to-3D has garnered significant attention recently, in addition to these, many other methods [ZZ23, LCT23, MRP\\({}^{*}\\)23a] have been proposed in this field.\n' +
      '\n' +
      '#### 4.2.2 Image-to-3D\n' +
      '\n' +
      'As the primary way to describe the visual effects of scenes, images can more intuitively describe the details and appearance of scenes at a finer-grained than language. Recent works thus are motivated to explore the image-to-3D techniques, which reconstruct remarkable and high-fidelity 3D models from specified images. These methods strive to maintain the appearance of the specified images and optimized 3D contents while introducing reasonable geometric priors. Similar to the text-to-3D methods, several image-to-3D methods leverage the volumetric representation used in NeRF to represent the target 3D scenes, which natively introduces multi-view consistency. NeuralLift-360 [XJW\\({}^{*}\\)23] uses estimated monocular depth and CLIP-guided diffusion prior to regularizing the geometry and appearance optimization respectively, achieving lift of a single image to a 3D scene represented by a NeRF. RealFusion [MKLRV23] and NeRDi [DJQ\\({}^{*}\\)23] leverage textual inversion [GAA\\({}^{*}\\)22] to extract text embeddings to condition a pre-trained image diffusion model [RBL\\({}^{*}\\)22b], and combine use the score distillation loss to optimize the volumetric representation. Based on Magic3D [LGT\\({}^{*}\\)23] that employs a coarse-to-fine framework as mentioned above, Magic123 [QMH\\({}^{*}\\)23] additionally introduces 3D priors from a pre-trained viewpoint-conditioned diffusion model Zero-1-to-3 [LWVH\\({}^{*}\\)23] in two optimization stage, yielding textured meshes that match the specified images. As another two-stage image-to-3D method, Make-it-3D [TWZ\\({}^{*}\\)23] enhances texture and geometric structure in the fine stage, producing high-quality textured point clouds as final results. Subsequent works [SZS\\({}^{*}\\)23, YYC\\({}^{*}\\)23] have been consistently proposed to enhance the previous results. Recently, 3D Gaussian Splatting (3DGS) [KKLD23] has emerged as a promising modeling as well as a real-time rendering technique. Based on 3DGS, DreamGaussian [TRZ\\({}^{*}\\)23] presents an efficient two-stage framework for both text-driven and image-driven 3D generation. In the first stage, DreamGaussian leverages SDS loss (i.e. 2D diffusion priors [LWVH\\({}^{*}\\)23] and CLIP-guided diffusion priors [PJBM23]) to generate target objects represented by 3D Gaussians. Then DreamGaussian extracts textured mesh from the optimized 3D Gaussians by querying the local density and refines textures in the UV space. For a better understanding of readers to various image-to-3D methods, we evaluate the performance of some open-source state-of-the-art methods. Tab. 2 shows the quantitative comparison of image-to-3D methods on surface reconstruction. We summarize the Chamfer distance and volume IoU as the metrics to evaluate the quality of surface reconstruction. Tab. 3 demonstrates the quantitative comparison of image-to-3D methods on novel view synthesis. We report the CLIP-Similarity, PSNR, and LPIPS as the metrics to evaluate the quality of view synthesis.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & Chamfer Distance \\(\\downarrow\\) & Volume IoU \\(\\uparrow\\) \\\\ \\hline RealFusion [MKLRV23] & 0.0819 & 0.2741 \\\\ Magic123 [QMH\\({}^{*}\\)23] & 0.0516 & 0.4528 \\\\ Make-it-3D [TWZ\\({}^{*}\\)23] & 0.0732 & 0.2937 \\\\ One-2-3-45 [LXJ\\({}^{*}\\)23] & 0.0629 & 0.4086 \\\\ Point-E [END\\({}^{*}\\)22] & 0.0426 & 0.2875 \\\\ Shap-E [DJ23] & 0.0436 & 0.3584 \\\\ Zero-1-to-3 [LWVH\\({}^{*}\\)23] & 0.0339 & 0.5035 \\\\ SyncDreamer [LLZ\\({}^{*}\\)23] & 0.0261 & 0.5421 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Quantitative comparison of image-to-3D methods on surface reconstruction. We summarize the Chamfer distance and volume IoU as the metrics to evaluate the quality of surface reconstruction.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Method & CLIP-Similarity \\(\\uparrow\\) & PSNR \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) \\\\ \\hline RealFusion [MKLRV23] & 0.735 & 20.216 & 0.197 \\\\ Magic123 [QMH\\({}^{*}\\)23] & 0.747 & 25.637 & 0.062 \\\\ Make-it-3D [TWZ\\({}^{*}\\)23] & 0.839 & 20.010 & 0.119 \\\\ One-2-3-45 [LXJ\\({}^{*}\\)23] & 0.788 & 23.159 & 0.096 \\\\ Zero-1-to-3 [LWVH\\({}^{*}\\)23] & 0.759 & 25.386 & 0.068 \\\\ SyncDreamer [LLZ\\({}^{*}\\)23] & 0.837 & 25.896 & 0.059 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Quantitative comparison of image-to-3D methods on novel view synthesis. We report the CLIP-Similarity, PSNR, and LPIPS as the metrics to evaluate the quality of view synthesis.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      'sentation vis VQGAN to obtain an abstract latent space for training transformers. While ViewFormer [14] also uses a two-stage training consisting of a Vector Quantized Variational Autoencoder (VQ-VAE) codebook and a transformer model. And [15] employs an encoder-decoder model based on transformers to learn an implicit representation.\n' +
      '\n' +
      'On the other hand, generative adversarial networks could produce high-quality results in image synthesis and consequently are applied to novel view synthesis [16, 17, 18, 19, 20, 21, 22]. Some methods [15, 20, 21] maintain a 3D point cloud as the representation, which could be projected onto novel views followed by a GAN to hallucinate the missing regions and synthesize the output image. While [20] and [21] focus on long-range view generation from a single view with adversarial training. At an earlier stage of deep learning methods when the auto-encoders and variational autoencoders begin to be explored, it is also used to synthesize the novel views [16, 21, 22].\n' +
      '\n' +
      'In summary, generative novel view synthesis can be regarded as a subset of image synthesis techniques and continues to evolve alongside advancements in image synthesis methods. Besides the generative models typically included, determining how to integrate information from the input view as a condition for synthesizing the novel view is the primary issue these methods are concerned with.\n' +
      '\n' +
      '## 5 Datasets for 3D Generation\n' +
      '\n' +
      'With the rapid development of technology, the ways of data acquisition and storage become more feasible and affordable, resulting in an exponential increase in the amount of available data. As data accumulates, the paradigm for problem-solving gradually shifts from data-driven to model-driven approaches, which in turn contributes to the growth of "Big Data" and "AIGC". Nowadays, data plays a crucial role in ensuring the success of algorithms. A well-curated dataset can significantly enhance a model\'s robustness and performance. On the contrary, noisy and flawed data may cause model bias that requires considerable effort in algorithm design to rectify. In this section, we will go over the common data used for 3D generation. Depending on the methods employed, it usually includes 3D data (Section 5.1), multi-view image data (Section 5.2), and single-view image data (Section 5.3), which are also summarized in Tab. 4.\n' +
      '\n' +
      '### Learning from 3D Data\n' +
      '\n' +
      '3D data could be collected by RGB-D sensors and other technology for scanning and reconstruction. Apart from 3D generation, 3D data is also widely used for other tasks like helping improve classical 2D vision task performance by data synthesis, environment simulation for training embodied AI agents, 3D object understanding, etc. One popular and frequently used 3D model database in the early stage is The Princeton Shape Benchmark [23]. It contains about 1800 polygonal models collected from the World Wide Web. While [16] constructs a special rig that contains a 3D digitizer, a turntable, and a pair of cameras mounted on a sled that can move along a bent rail to capture the kit object models database. To evaluate the algorithms to detect and estimate the objects in the image given 3D models, [20] introduces a dataset of 3D IKEA models obtained from Google Warehouse. Some 3D model databases are presented for tasks like robotic manipulation [16, 21], 3D shape retrieval [20], 3D shape modeling from a single image [21]. BigBIRD [23] presents a large-scale dataset of 3D object instances that also includes multi-view images and depths, camera pose information, and segmented objects for each image.\n' +
      '\n' +
      'However, those datasets are very small and only contain hundreds or thousands of objects. Collecting, organizing, and labeling larger datasets in computer vision and graphics communities is needed for data-driven methods of 3D content. To address this, ShapeNet [14] is introduced to build a large-scale repository of 3D CAD models of objects. The core of ShapeNet covers 55 common object categories with about 51,300 models that are manually verified category and alignment annotations. Thingi10K [18] collects 10,000 3D printing models from an online repository Thingiverse. While PhotoShape [22] produces 11,000 photorealis\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Dataset & Type & Year & Samples & Category \\\\ \\hline ShapeNet [14] & 3D data & 2015 & 51K & objects \\\\ Thingi10K [18] & 3D data & 2016 & 10K & objects \\\\\n' +
      '3D-Future [14] & 3D data & 2020 & 10K & furniture \\\\ GSO [21] & 3D Data & 2022 & 1K & household items \\\\ Objectives [13] & 3D data & 2022 & 800K & objects \\\\ OmniObject [14] & 3D data & 2023 & 6K & objects \\\\ Objareser-XL [20] & 3D Data & 2023 & 10.2M & objects \\\\ \\hline ScanNet [14] & multi-view images & 2017 & 1.5K (2.5M images) & indoor scenes \\\\ CO3D [16] & multi-view images & 2021 & 19K (1.5M images) & objects \\\\ MVImgNet [14] & multi-view images & 2023 & 219K (6.5M images) & objects \\\\ \\hline DeepFashion [20] & single-view images & 2016 & 800K & clothes \\\\ FFHQ [21] & single-view images & 2018 & 70K & human faces \\\\ AFHQ [14] & single-view images & 2019 & 15K & animal faces \\\\ SHHQ [20] & single-view images & 2022 & 40K & human bodies \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Selected datasets commonly used for 3D generation.\n' +
      '\n' +
      'tic, reliable 3D shapes based on online data. Other datasets such as 3D-Future [14], ABO [15][16], GSO [17] and OmniObject3D [18] try to improve the texture quality but only contain thousands of models. Recently, Obiayverse [19] presents a large-scale corpus of 3D objects that contains over 800K 3D assets for research in the field of AI and makes a step toward a large-scale 3D dataset. Obiayverse-XL [15] further extends Obiayverse to a larger 3D dataset of 10.2M unique objects from a diverse set of sources. These large-scale 3D datasets have the potential to facilitate large-scale training and boost the performance of 3D generation.\n' +
      '\n' +
      '### Learning from Multi-view Images\n' +
      '\n' +
      '3D objects have been traditionally created through manual 3D modeling, object scanning, conversion of CAD models, or combinations of these techniques [16]. These techniques may only produce synthetic data or real-world data of specific objects with limited reconstruction accuracy. Therefore, some datasets directly provide multi-view images in the wild which are also widely used in many 3D generation methods. ScanNet [20] introduces an RGB-D video dataset containing 2.5M views in 1513 scenes and Obijectron [1] contains object-centric short videos and includes 4 million images in 14,819 annotated videos, of which only a limited number cover the full 360 degrees. CO3D [17] extends the dataset from [18] and increases the size to nearly 19,000 videos capturing objects from 50 MS-COCO categories, which has been widely used in the training and evaluations of novel view synthesis and 3D generation or reconstruction methods. Recently, MVImgNet [19] presents a large-scale dataset of multi-view images that collects 6.5 million frames from 219,188 videos by shooting videos of real-world objects in human daily life. Other lines of work provide the multi-view dataset in small-scale RGB-D videos [15, 16] compared with these works, large-scale synthetic videos [21], or egocentric videos [22]. A large-scale dataset is still a remarkable trend for deep learning methods, especially for generation tasks.\n' +
      '\n' +
      '### Learning from Single-view Images\n' +
      '\n' +
      '3D generation methods usually rely on multi-view images or 3D ground truth to supervise the reconstruction and generation of 3D representation. Synthesizing high-quality multi-view images or 3D shapes using only collections of single-view images is a challenging problem. Benefiting from the unsupervised training of generative adversarial networks, 3D-aware GANs are introduced that could learn 3D representations in an unsupervised way from natural images. Therefore, several single-view image datasets are proposed and commonly used for these 3D generation methods. Although many large-scale image datasets have been presented for 2D generation, it is hard to directly use them for 3D generation due to the high uncertainty of this problem. Normally, these image datasets only contain a specific category or domain. FFHQ [1], a real-world human face dataset consisting of 70,000 high-quality images at 1024\\({}^{2}\\) resolution, and AFHQ [14], an animal face dataset consisting of 15,000 high-quality images at 512\\({}^{2}\\) resolution, are introduced for 2D image synthesis and used a lot for 3D generation based on 3D-aware GANs. In the domain of the human body, SHHQ [10] and DeepFashion [15] have been adopted for 3D human generation. In terms of objects, many methods [19, 20, 21, 22] render synthetic single-view datasets using several major object categories of ShapeNet. While GRAF [19] renders 150k Chairs from Photoshapes [23]. Moreover, CelebA [15] and Cats [18] datasets are also commonly used to train the models like HoloGAN [21] and pi-GAN [20]. Since the single-view images are easy to obtain, these methods could collect their own dataset for the tasks.\n' +
      '\n' +
      '## 6 Applications\n' +
      '\n' +
      'In this section, we introduce various 3D generation tasks (Sec. 6.1-6.3) and closely related 3D editing tasks (Sec. 6.4). The generation tasks are divided into three categories, including 3D human generation (Sec. 6.1), 3D face generation (Sec. 6.2), and generic object and scene generation (Sec. 6.3).\n' +
      '\n' +
      '### 3D Human Generation\n' +
      '\n' +
      'With the emergence of the metaverse and the advancements in virtual 3D social interaction, the field of 3D human digitization and generation has gained significant attention in recent years. Different from general 3D generation methods that focus on category-free rigid objects with sample geometric structures [19], LXZ* [21], most 3D human generation methods aim to tackle the complexities of articulated pose changes and intricate geometric details of clothing. Tab. 5 presents a compilation of notable 3D human body generation methods in recent years, organized according to the input conditions and the output format of the generated 3D human bodies. Some results of these methods are shown in Fig. 8. Specifically, in terms of the input condition, current 3D human body generation methods can be categorized based on the driving factors including latent features randomly sampled from a pre-defined latent space [18], CJS* [20], HCL* [21], a single reference image [19, 17, 18], CIV* [22], HYX* [23], ZLZ* [21], or text prompts [16, 17, 18]. According to the form of the final output, these methods can be classified into two categories: textless shape generation [19,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Methods & Input Condition & Output Texture \\\\ \\hline ICON [19] & Single-Image & ✗ \\\\ ECON [19] & Single-Image & ✗ \\\\ gDNA [18] & Latent & ✗ \\\\ Chupa [18] & Text/Latent & ✗ \\\\ ELICIT [18] & Single-Image & ✓ \\\\ TeCH [18] & Single-Image & ✓ \\\\ Get3DHuman [18] & Latent & ✓ \\\\ EVA3D [18] & Latent & ✓ \\\\ AvatarCraft [18] & Text & ✓ \\\\ DreamHuman [18] & Text & ✓ \\\\ TADA [18] & Text & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Recent 3D human generation techniques and their corresponding input-output formats.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '### 3D Face Generation\n' +
      '\n' +
      'One essential characteristic of 3D face generation tasks is to generate high-quality human face images that can be viewed from different viewpoints. Popular tasks can be loosely classified into three major categories, including personalized head avatar creation (e.g. 3D talking head generation), neural implicit 3D morphable models (3DMMs), and generative 3D face models, which are shown in Fig. 9 and Fig. 10.\n' +
      '\n' +
      '**Personalized head avatar creation** aims at creating an animatable avatar that can be viewed from different viewpoints of the target person, which has broad applications such as talking head generation. Most of the existing methods take as input a sequence of video frames (i.e. monocular video) [PSB\\({}^{*}\\)21, GTZN21, GPL\\({}^{*}\\)22, ZAB\\({}^{*}\\)22, ZBT23, ZYW\\({}^{*}\\)23, BTH\\({}^{*}\\)23, GZX\\({}^{*}\\)22]. Although convenient, the viewing angles of these avatars are limited in a relatively small range (i.e. near frontal) and their quality is not always satisfactory due to limited data. In contrast, another stream of works [LSSS18, MSS\\({}^{*}\\)21, LSS\\({}^{*}\\)21, WKC\\({}^{*}\\)23, KQG\\({}^{*}\\)23] aims at creating a very high-quality digital human that can be viewed from larger angles (e.g. side view). These methods usually require high-quality synchronized multi-view images under even illumination. However, both streams rely heavily on implicit or hybrid neural representations and neural rendering techniques. The quality and animation accuracy of the generated talking head video are usually measured with PSNR, SSIM, and LPIPS metrics.\n' +
      '\n' +
      '**Neural implicit 3DMMs.** Traditional 3D morphable face models (3DMMs) assume a predefined template mesh (i.g. fixed topology) for the geometry and have explored various modeling methods including linear models (e.g. PCA-based 3DMMs) and non-linear models (e.g. network-based 3DMMs). A comprehensive survey of these methods has been discussed in [EST\\({}^{*}\\)20]. Recently, thanks to the rapid advances in implicit neural representations (INRs), several neural implicit 3DMMs utilizing INRs for face modeling emerges [YTB\\({}^{*}\\)21, ZYHC22, GKG\\({}^{*}\\)23] since continuous implicit neural representations do not face discretization error and can theoretically modeling infinite details. Indeed, NPHM [GKG\\({}^{*}\\)23] can generate more subtle expressions unseen in previous mesh-based 3DMMs. What\'s more, neural implicit 3DMMs can potentially model hair better since the complexity of different hairstyles varies drastically, which imposes a great challenge for fixed topology mesh-based traditional 3DMMs.\n' +
      '\n' +
      '**Generative 3D face models.** One key difference from 2D generative face models (e.g. StyleGAN [KLA19, KLA\\({}^{*}\\)20]) is that 3D face models can synthesize multi-view consistent images (i.e. novel views) of the same target (identity and clothes). Early attempts towards this direction include HoloCAN [NPLT\\({}^{*}\\)19] and PlatonicGAN [HMR19b], which are both voxel-based methods and can only generate images in limited resolution. Quickly, methods [SLNG20, NG21, CMK\\({}^{*}\\)21b, OELS\\({}^{*}\\)22, GLWT22, CLC\\({}^{*}\\)22] utilizing neural radiance fields are proposed to increase the image resolution. For example, EG3D [CLC\\({}^{*}\\)22] proposes a hybrid tri-plane representation, which strikes a good trade-off to effectively address the memory and rendering inefficiency faced by previous generative 3D GANs and can produce high-quality images with good multi-view consistency.\n' +
      '\n' +
      'Thanks to the success of various 3D GANs, many down\n' +
      '\n' +
      'Figure 10: Representative 3D face generation tasks. Images adapted from NHA [GPL\\({}^{*}\\)22], NPHM [GKG\\({}^{*}\\)23], and EG3D [CLC\\({}^{*}\\)22].\n' +
      '\n' +
      'Figure 9: Representative applications and methods of 3D face generation.\n' +
      '\n' +
      'stream applications (e.g. editing, talking head generation) are enabled or become less data-hungry, including 3D consistent editing [14], SWW\\({}^{*}\\)23, SWZ\\({}^{*}\\)22, SWS\\({}^{*}\\)22, LFLSY\\({}^{*}\\)23, JCL\\({}^{*}\\)22], 3D talking head generation [15], CSJ\\({}^{*}\\)23, WDY\\({}^{*}\\)22], etc.\n' +
      '\n' +
      '### General Scene Generation\n' +
      '\n' +
      'Different from 3D human and face generation, which can use existing prior knowledge such as SMPL and 3DMM, general scene generation methods are more based on the similarity of semantics or categories to design a 3D model generation framework. Based on the differences in generation results, as shown in Fig. 11 and Tab. 6, we further subdivide general scene generation into object-centered asset generation and outward-facing scene generation.\n' +
      '\n' +
      '#### 6.3.1 Object-Centered Asset Generation\n' +
      '\n' +
      'The field of object-centered asset generation has seen significant advancements in recent years, with a focus on both textureless shape generation and textured asset generation. For the textureless shape generation, early works use GAN-based networks to learn a mapping from latent space to 3D object space based on specific categories of 3D data, such as 3D-GAN [16], HoloGAN [17], and PlatonicGAN [18]. However, limited by the generation capabilities of GANs, these methods can only generate rough 3D assets of specific categories. To improve the quality of generated results, SingleShapeGen [19] leverages a pyramid of generators to generate 3D assets in a coarse to fine manner. Given the remarkable achievements of diffusion models in image generation, researchers are directing their attention towards the application of diffusion extensions in the realm of 3D generation. Thus, subsequent methods [14, 15, 16, 17, 18] explore the use of diffusion processes for 3D shape generation from random noise. In addition to these latent-based methods, another important research direction is text-driven 3D asset generation [19, 18]. For example, 3D-LDM [17], SDFusion [16], and Diffusion-SDF [15] achieve text-to-3D shape generation by designing the diffusion process in 3D feature space. Due to such methods requiring 3D datasets to train the diffusion-based 3D generators, they are limited to the training data in terms of the categories and diversity of generated results. By contrast, CLIP-Forge [16], CLIP-Sculptor [18], and Michelangel [17] directly employ the prior of the pre-trained CLIP model to constrain the 3D generation process, effectively improving the generalization of the method and the diversity of generation results. Unlike the above latent-conditioned or text-driven 3D generation methods, to generate 3D assets with expected shapes, there are some works [18, 19] explore image or sketch-conditioned generation.\n' +
      '\n' +
      'In comparison to textureless 3D shape generation, textured 3D asset generation not only produces realistic geometric structures but also captures intricate texture details. For example, HoloGAN [17], GET3D [14], and EG3D [18] employ GAN-based 3D generators conditioned on latent vectors to produce category-specific textured 3D assets. By contrast, text-driven 3D generation methods rely on the prior knowledge of pre-trained large-scale text-image models to enable category-free 3D asset generation. For instance, CLIP-Mesh [19], Dream Fields [19], and PureCLIPNeRF [18] employ the prior of CLIP model to constrain the optimization process and achieve text-driven 3D generation. Furthermore, DreamFusion [19] and SJC [17] propose a score distillation sampling (SDS) method to achieve 3D constraint which priors extracted from pre-trained 2D diffusion models. Then, some methods further improve the SDS-based 3D generation process in terms of generation quality, multi-face problem, and optimization efficiency, such as Magic3D [16], Latent-NeRF [20], Fantasia3D [18], DreamBooth3D [21], HiFA [22], ATT3D [17], ProfilcDreamer [19], IT3D [18], DreamGANisation [20], and CAD [17]. On the other hand, distinct from text-driven 3D generation, single-image-conditioned 3D generation is also a significant research direction [18], CSJ\\({}^{*}\\)23, MKLRV23, CGC\\({}^{*}\\)23, WLY\\({}^{*}\\)23, KDJ\\({}^{*}\\)23].\n' +
      '\n' +
      '#### 6.3.2 Outward-Facing Scene Generation\n' +
      '\n' +
      'Early scene generation methods often require specific scene data for training to obtain category-specific scene generators, such as GAUDI [1] and the work of Xiang _et al._[19], or implement a single scene reconstruction based on the input image, such as PixelSynth [16] and Worldsheet [18]. However, these methods are either limited by the quality of the generation or by the extensibility of the scene. With the rise of diffusion models in image inpainting, various methods are beginning to use the scene completion capabilities of diffusion models to implement scene generation tasks [18], HCO\\({}^{*}\\)23, ZLW\\({}^{*}\\)23]. Recently, SceneScape [1], Text2Room [19], Text2NeRF [17], and LucidDreamer [16] propose progressive inpainting and updating strategies for generating realistic 3D scenes using pre-trained diffusion models. SceneScape and Text2Room utilize explicit polygon meshes as their\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Methods & Type & Condition & \n' +
      '\\begin{tabular}{c} Texture \\\\ Generation \\\\ \\end{tabular} \\\\ \\hline PVD [15] & Object-Centered & Latent & ✗ \\\\ NPD [18] & Object-Centered & Latent & ✗ \\\\ Point-E [18] & Object-Centered & Text & ✗ \\\\ Diffusion-SDF [15] & Object-Centered & Text & ✗ \\\\ Deep3DSketch+ [18] & Object-Centered & Sketch & ✗ \\\\ Zero-1-to-3 [18] & Object-Centered & Single-Image & ✓ \\\\ Make-It-3D [18] & Object-Centered & Single-Image & ✓ \\\\ GET3D [18] & Object-Centered & Latent & ✓ \\\\ EG3D [18] & Object-Centered & Latent & ✓ \\\\ CLIP-Mesh [19] & Object-Centered & Text & ✓ \\\\ DreamFusion [19] & Object-Centered & Text & ✓ \\\\ ProfilcDreamer [18] & Object-Centered & Text & ✓ \\\\ PixelSynth [16] & Outward-Facing & Single-Image & ✓ \\\\ DiffcDreamer [18] & Outward-Facing & Single-Image & ✓ \\\\ Xiang _et al._[19] & Outward-Facing & Latent & ✓ \\\\ CC3D [18] & Outward-Facing & Layout & ✓ \\\\ Text2Room [19] & Outward-Facing & Text & ✓ \\\\ Text2NeRF [17] & Outward-Facing & Text & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Applications of general scene generation methods.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      'local editing types include appearance manipulation [YBZ\\({}^{*}\\)22, ZWL\\({}^{*}\\)23], geometry deformation [JKK\\({}^{*}\\)23, PYL\\({}^{*}\\)22, YSL\\({}^{*}\\)22, TLYCS22], object-/semantic-level duplication/deletion and moving/removing [YZX\\({}^{*}\\)21, WL\\({}^{*}\\)22, KMS22, WWL\\({}^{*}\\)23]. For example, NeuMesh [YBZ\\({}^{*}\\)22] supports several kinds of texture manipulation including swapping, filling, and painting since they distil a NeRF scene into a mesh-based neural representation. NeRF-Shop [JKK\\({}^{*}\\)23] and CageNeRF [PYL\\({}^{*}\\)22] transform/deform the volume bounded by a mesh cage, resulting in moved or deformed/articulated object. SINE [BZY\\({}^{*}\\)23] updates both the NeRF geometry and the appearance with geometry prior and semantic (image feature) texture prior as regularizations.\n' +
      '\n' +
      'Another line of works (e.g. ObjectNeRF [YZX\\({}^{*}\\)21], ObjectSDF [WLC\\({}^{*}\\)22], DFF [KMS22]) focus on automatically decomposing the scene into individual objects or semantic parts during reconstruction, which is made possible by utilizing extra 2D image understanding networks (e.g. instance segmentation), and support subsequent object-level manipulations such as re-coloring, removal, displacement, duplication.\n' +
      '\n' +
      'Recently, it is possible to create new textures and/or content only according to text description in the existing 3D scenes due to the success of large-scale text-to-image models (e.g. Stable Diffusion [RBL\\({}^{*}\\)22a]). For example, instruct-NeRF2NeRF [HTE\\({}^{*}\\)23] iteratively updates the reference dataset images modified by a dedicated diffusion model [BHE23] and the NeRF model. DreamEditor [ZWL\\({}^{*}\\)23] performs local updates on the region located by text attention guided by score distillation sampling [PJBM23]. Focal-Dreamer [LDS\\({}^{*}\\)23] creates new geometries (objects) in the specified empty spaces according to the text input. SKED [MPS\\({}^{*}\\)23] supports both creating new objects and modifying the existing part located in the region specified by the provided multi-view sketches.\n' +
      '\n' +
      '## 7 Open Challenges\n' +
      '\n' +
      'The quality and diversity of 3D generation results have experienced significant progress due to advancements in generative models, 3D representations, and algorithmic paradigms. Considerable attention has been drawn to 3D generation recently as a result of the success achieved by large-scale models in natural language processing and image generation. However, numerous challenges remain before the generated 3D models can meet the high industrial standards required for video games, movies, or immersive digital content in VR/AR. In this section, we will explore some of the open challenges and potential future directions in this field.\n' +
      '\n' +
      '**Evaluation.** Quantifying the quality of generated 3D models objectively is an important and not widely explored problem. Using metrics such as PSNR, SSIM, and F-Score to evaluate rendering and reconstruction results requires ground truth data on the one hand, but on the other hand, it can not comprehensively reflect the quality and diversity of the generated content. In addition, user studies are usually time-consuming, and the study results tend to be influenced by the bias and number of surveyed users. Metrics that capture both the quality and diversity of the results like FID can be applied to 3D data, but may not be always aligned with 3D domain and human preferences. Better metrics to judge the results objectively in terms of generation quality, diversity, and matching degree with the conditions still need further exploration.\n' +
      '\n' +
      '**Dataset.** Unlike language or 2D image data which can be easily captured and collected, 3D assets often require 3D artists or designers to spend a significant amount of time using professional software to create. Moreover, due to the different usage scenarios and creators\' personal styles, these 3D assets may differ greatly in scale, quality, and style, increasing the complexity of 3D data. Specific rules are needed to normalize this diverse 3D data, making it more suitable for generation methods. A large-scale, high-quality 3D dataset is still highly desirable in 3D generation. Meanwhile, exploring how to utilize extensive 2D data for 3D generation could also be a potential solution to address the scarcity of 3D data.\n' +
      '\n' +
      '**Representation.** Representation is an essential part of the 3D generation, as we discuss various representations and the associated methods in Sec. 3. Implicit representation is able to model complex geometric topology efficiently but faces challenges with slow optimization; explicit representation facilitates rapid optimization convergence but struggles to encapsulate complex topology and demands substantial storage resources; Hybrid representation attempts to consider the trade-off between these two, but there are\n' +
      '\n' +
      'Figure 12: Representative 3D editing tasks. Images adapted from ARF [ZKB\\({}^{*}\\)22], Text2Mesh [MBOL\\({}^{*}\\)22], NeRFShop [JKK\\({}^{*}\\)23], SKED [MPS\\({}^{*}\\)23], and DreamEditor [ZWL\\({}^{*}\\)23].\n' +
      '\n' +
      'still shortcomings. In general, we are motivated to develop a representation that balances optimization efficiency, geometric topology flexibility, and resource usage.\n' +
      '\n' +
      '**Controllability.** The purpose of the 3D generation technique is to generate a large amount of user-friendly, high-quality, and diverse 3D content in a cheap and controllable way. However, embedding the generated 3D content into practical applications remains a challenge: most methods [4, 15] rely on volume rendering or neural rendering, and fail to generate content suitable for rasterization graphics pipeline. As for methods [16, 17] that generate the content represented by polygons, they do not take into account layout (e.g. the rectangular plane of a table can be represented by two triangles) and high-quality UV unwrapping and the generated textures also face some issues such as black shadows. These problems make the generated content unfavorable for artist-friendly interaction and editing. Furthermore, the style of generated content is still limited by training datasets. Furthermore, the establishment of comprehensive toolchains is a crucial aspect of the practical implementation of 3D generation. In modern workflows, artists use tools (e.g. LookDev) to harmonize 3D content by examining and contrasting the righting results of their materials across various lighting conditions. Concurrently, modern Digital Content Creation (DCC) software offers extensive and fine-grained content editing capabilities. It is promising to unify 3D content produced through diverse methods and establish tool chains that encompass abundant editing capabilities.\n' +
      '\n' +
      '**Large-scale Model.** Recently, the popularity of large-scale models has gradually affected the field of 3D generation. Researchers are no longer satisfied with using distillation scores that use large-scale image models as the priors to optimize 3D content, but directly train large-scale 3D models. MeshGPT [2] follows large language models and adopts a sequence-based approach to autoregressively generate sequences of triangles in the generated mesh. MeshGPT takes into account layout information and generates compact and sharp meshes that match the style created by artists. Since MeshGPT is a decoder-only transformer, compared with the optimization-based generation, it gets rid of inefficient multi-step sequential optimization, achieving rapid generation. Despite this, MeshGPT\'s performance is still limited by training datasets and can only generate regular furniture objects. But there is no doubt that large-scale 3D generation models have great potential worth exploring.\n' +
      '\n' +
      '## 8 Conclusion\n' +
      '\n' +
      'In this work, we present a comprehensive survey on 3D generation, encompassing four main aspects: 3D representations, generation methods, datasets, and various applications. We begin by introducing the 3D representation, which serves as the backbone and determines the characteristics of the generated results. Next, we summarize and categorize a wide range of generation methods, creating an evolutionary tree to visualize their branches and developments. Finally, we provide an overview of related datasets, applications, and open challenges in this field. The realm of 3D generation is currently witnessing explosive growth and development, with new work emerging every week or even daily. We hope this survey offers a systematic summary that could inspire subsequent work for interested readers.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [ACC\\({}^{*}\\)22]Adamkiewicz M., Chen T., Caccavale A., Gardner R., Culbertson P., Bohg J., Schwager M.: Vision-only robot navigation in neural radiance world. _IEEE Robotics and Automation Letters 7_, 2 (2022), 4606-4613.\n' +
      '* [ADMG18]Achlioptas P., Diamanti O., Mitliagkas I., Guibas L.: Learning representations and generative models for 3d point clouds. In _International conference on machine learning_ (2018), PMLR, pp. 40-49.\n' +
      '* [ALG\\({}^{*}\\)20]Attal B., Ling S., Gokaslan A., Richardt C., Tomp-Kink J.: Matryodshka: Real-time 6d video view synthesis using multi-sphere images. In _European Conference on Computer Vision_ (2020), Springer, pp. 441-459.\n' +
      '* [APMTM19]Alldieck T., Pons-Moll G., Theobalt C., Magnor M.: Tex2shape: Detailed full human body geometry from a single image. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2019), pp. 2293-2303.\n' +
      '* [AQW19]Abmal R., Qin Y., Wonka P.: Image2stylegan: How to embed images into the stylegan latent space? In _Proceedings of the IEEE/CVF international conference on computer vision_ (2019), pp. 4432-4441.\n' +
      '* [ASK\\({}^{*}\\)20]Aliev K.-A., Sevastopolsky A., Kolos M., Ulyanov D., Lempitsky V.: Neural point-based graphics. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXII 16_ (2020), Springer, pp. 696-712.\n' +
      '* [AST\\({}^{*}\\)23]AlBahar B., Saito S., Tseng H.-Y., Kim C., Kopf J., Huang J.-B.: Single-image 3d human digitization with shape-guided diffusion. In _SIGGRAPH Asia 2023 Conference Papers_ (2023), pp. 1-11.\n' +
      '* [ATDN23]Aneja S., Thies J., Dai A., Niessner M.: ClipFace: Text-guided editing of textured 3d morphable models. In _ACM SIGGRAPH 2023 Conference Proceedings_ (2023).\n' +
      '* [AYS\\({}^{*}\\)23]Abdal R., Yiew W., Shi Z., Xu Y., Po R., Kuang Z., Chen Q., Yeung D.-Y., Wetzstein G.: Gaussian shell maps for efficient 3d human generation. _arXiv preprint arXiv:2311.17857_ (2023).\n' +
      '* [AZA\\({}^{*}\\)21]Ahmadyan A., Zhang L., Ablavatski A., Wei J., Grundmann M.: Objectron: A large scale dataset of object-centric videos in the wild with pose annotations. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2021), pp. 7822-7831.\n' +
      '* [AZS22]Alldieck T., Zanfir M., Sminchisescu C.: Photoorealistic monocular 3d reconstruction of humans wearing clothing. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 1506-1515.\n' +
      '* [BBJ\\({}^{*}\\)21]Boss M., Braun R., Jampani V., Barron J. T., Liu C., Lensch H.: Nerd: Neural reflectance decomposition from image collections. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 12684-12694.\n' +
      '* [BFO\\({}^{*}\\)20]Broxton M., Flynn J., Overbeck R., Erickson D., Hebman D., Duvali M., Dotozarian J., Busch J., Whalen M., Debevec P.: Immersive light field video with a layered mesh representation. _ACM Transactions on Graphics (TOG) 39_, 4 (2020), 86-1.\n' +
      '* [BFW\\({}^{*}\\)23]Bai Y., Fan Y., Wang X., Zhang Y., Sun J., Yuan C., Shan Y.: High-fidelity facial avatar reconstruction from monocular video with generative priors. In _CVPR_ (2023).\n' +
      '\n' +
      '* [BGA*22]Bautista M. A., Guo P., Abnar S., Talabott W., Toshev A., Chen Z., Dinh L., Zhai S., Goh H., Ulbricht D., et al.: Gaudi: A neural architect for immersive 3d scene generation. _Advances in Neural Information Processing Systems 35_ (2022), 25102-25116.\n' +
      '* [BGP*22]Baatz H., Granskog J., Papas M., Rousselle F., Novak J.: Nerf-tex: Neural reflectance field textures. In _Computer Graphics Forum_ (2022), vol. 41, Wiley Online Library, pp. 287-301.\n' +
      '* [BHE23]Brooks T., Holynski A., Efros A. A.: InstructPix2Pix: Learning to follow image editing instructions. In _CVPR_ (2023).\n' +
      '* [BHMK*18]Ben-Hamid u., Maron H., Kezurer I., Avineri G., Lipman Y.: Multi-chart generative surface modeling. _ACM Transactions on Graphics (TOG) 37_, 6 (2018), 1-15.\n' +
      '* [BKP*10]Botsch M., Kobbelt L., Pauly M., Alliez P., Levy B.: _Polygon mesh processing_. CRC press, 2010.\n' +
      '* [BKY*22]Bergman A., Kellnhofer P., Yifan W., Chan E., Lindell D., Weetzstein G.: Generative neural articulated radiance fields. _Advances in Neural Information Processing Systems 35_ (2022), 19900-19916.\n' +
      '* [BLW16]Brock A., Lim T., Weston N.: Generative and discriminative voxel modeling with convolutional neural networks. _arXiv preprint arXiv:1608.04236_ (2016).\n' +
      '* [BMR*20]Brown T., Mann B., Ryder N., Subbiah M., Kaplan J. D., Dhariwal P., Neelakantan A., Shyam P., Sastry G., Askell A., et al.: Language models are few-shot learners. _NeurIPS_ (2020).\n' +
      '* [BMD*21]Barron J. T., Mildenhall B., Tancik M., Hedman P., Martin-Brualla R., Srinivasan P. P.: Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 5855-5864.\n' +
      '* [BMV*22]Barron J. T., Mildenhall B., Verbin D., Srinivasan P. P., Hedman P.: Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 5470-5479.\n' +
      '* [BNT21]Burov A., Niessner M., Thies J.: Dynamic surface function networks for clothed human bodies. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 10754-10764.\n' +
      '* [BPP*23]Bahmani S., Park J. J., Paschalidou D., Yan X., Wetzstein G., Guibas L., Tagliascach A.: Cc3d: Layout-conditioned generation of compositional 3d scenes. _arXiv preprint arXiv:2303.12074_ (2023).\n' +
      '* [BSKG22]Ben-Shabar Y., Koneputuogdage C. H., Gould S.: Digs: Divergence guided shape implicit neural representation for oriented point clouds. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 19323-19332.\n' +
      '* [BTH*23]Bai Z., Tan F., Huang Z., Sarkar K., Tang D., Qiu D., Meka A., Du R., Dou M., Orts-Escolano S., et al.: Learning personalized high quality volumetric head avatars from monocular rgb videos. In _CVPR_ (2023).\n' +
      '* [BZY*23]Bao C., Zhang Y., Yang B., Fan T., Yang Z., Bao H., Zhang G., Cui Z.: SINE: Semantic-driven image-based nef editing with prior-guided editing field. In _CVPR_ (2023).\n' +
      '* [CBZ*19]Cheng S., Bronstein M., Zhou Y., Kotsia I., Pantic M., Zafehiou S.: Mesbrian: Non-linear 3d morphable models of faces. _arXiv preprint arXiv:1903.10384_ (2019).\n' +
      '* [CCH*23]Cao Y., Cao Y.-P., Han K., Shan Y., Wong K.-Y. K.: Dreamavatar: Text-and-shape guided 3d human avatar generation via diffusion models. _arXiv preprint arXiv:2304.00916_ (2023).\n' +
      '* [CCJJ23]Chen R., Chen Y., Jiao N., Jia K.: Fantasia3D: Disentangling geometry and appearance for high-quality text-to-3d content creation. In _ICCV_ (2023).\n' +
      '* [CCP*23]Cai S., Chan E. R., Peng S., Shahbazi M., Obukhov A., Van Gool L., Wetzstein G.: Diffreramer: Towards consistent unsupervised single-view scene extrapolation with conditional diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2023), pp. 2139-2150.\n' +
      '* [CCS*19]Chen K., Choy C. B., Savva M., Chang A. X., Funkhouser T., Savarese S.: Text2shape: Generating shapes from natural language by learning joint embeddings. In _Computer Vision-ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2-6, 2018, Revised Selected Papers, Part III 14_ (2019), Springer, pp. 100-116.\n' +
      '* [CCW*23]Chen Y., Chen X., Wang X., Zhang Q., Guo Y., Shan Y., Wang F.: Local-to-global registration for bundle-adjusting neural radiance fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 8264-8273.\n' +
      '* [CFG*15]Chang A. X., Funkhouser T., Guibas L., Hanrahan P., Huang Q., Li Z., Savarese S., Savva M., Song S., Su H., et al.: Shapenet: An information-rich 3d model repository. _arXiv preprint arXiv:1512.03012_ (2015).\n' +
      '* [CFZ*23]Chen T., Fu C., Zang Y., Zhu L., Zhang J., Mao P., Sun L.: Deep3dketch+: Rapid 3d modeling from single free-hand sketches. In _International Conference on Multimedia Modeling_ (2023), Springer, pp. 16-28.\n' +
      '* [CGC**23]Chen H., Gu J., Chen A., Tian W., Tu Z., Liu L., Su H.: Single-stage diffusion nerf: A unified approach to 3d generation and reconstruction. _arXiv preprint arXiv:2304.06714_ (2023).\n' +
      '* [CGD*22]Collins J., Goel S., Deng K., Luthra A., Xu L., Gundogdu E., Zhang X., Vicente T. F. Y., Dideriksen T., Arora H., et al.: Abo: Dataset and benchmarks for real-world 3d object understanding. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 21126-21136.\n' +
      '* [CGT*19]Choi I., Gallo O., Troccoli A., Kim M. H., Kautz J.: Extreme view synthesis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2019), pp. 7781-7790.\n' +
      '* [CHB*23]Chen X., Huang J., Bin Y., Yu L., Liao Y.: Veri3d: Generative vertex-based radiance fields for 3d controllable human image synthesis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2023), pp. 8986-8997.\n' +
      '* [CHIS23]Crotoru F.-a., Hondur V., Ionescu R. T., Shah M.: Diffusion models in vision: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_ (2023).\n' +
      '* [CJS*22]Chen X., Jiang T., Song J., Yang J., Black M. J., Geiger A., Hillings O.: gdna: Towards generative detailed neural avatars. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 20427-20437.\n' +
      '* [CLC*22]Chan E. R., Lin C. Z., Chan M. A., Nagano K., Pan B., De Mello S., Gallo O., Guibas L. J., Tremblay J., Khamis S., et al.: Efficient geometry-aware 3d generative adversarial networks. In _CVPR_ (2022).\n' +
      '* [CLL23]Cheng Z., Li J., Li H.: Wildlight: In-the-wild inverse rendering with a flashlight. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 4305-4314.\n' +
      '* [CLN*23]Chung J., Lee S., Nam H., Lee J., Lee K. M.: Lucid-dreamer: Domain-free generation of 3d gaussian splatting scenes. _arXiv preprint arXiv:2311.13384_ (2023).\n' +
      '* [CLT*23]Cheng Y.-C., Lee H.-Y., Tulyakov S., Schwing A. G., Gui L.-Y.: Sofusion: Multimodal 3d shape completion, reconstruction, and generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 4456-4465.\n' +
      '* [CMA*22]Choi H., Moon G., Armando M., Leroy V., Lee K. M., Rogez G.: Mononhr: Monocular neural human renderer. In _2022 International Conference on 3D Vision (3DV)_ (2022), IEEE, pp. 242-251.\n' +
      '\n' +
      '* [CMK\\({}^{*}\\)21a] Chan E. R., Monteiro M., Kellnhofer P., Wu J., Wetzstein G.: pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2021), pp. 5799-5809.\n' +
      '* [CMK\\({}^{*}\\)21b] Chan E. R., Monteiro M., Kellnhofer P., Wu J., Wetzstein G.: pi-GAN: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In _CVPR_ (2021).\n' +
      '* [CNC\\({}^{*}\\)23] Chan E. R., Nagano K., Chan M. A., Bergman A. W., Park J. J., Levy A., Mittala M., De Mello S., Karras T., Wetzstein G.: Generative novel view synthesis with 3d-aware diffusion models. _arXiv preprint arXiv:2304.02602_ (2023).\n' +
      '* [CPA\\({}^{*}\\)21] Corona E., Pumarola A., Alenya G., Pons-Moll G., Moreno-Noguer F.: Explicit: Topology-aware generative model for clotped people. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2021), pp. 11875-11885.\n' +
      '* [CPB\\({}^{*}\\)20] Choutas V., Pavlakos G., Bolkart T., Tzionas D., Black M. J.: Monocular expressive body regression through body-driven attention. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part X 16_ (2020), Springer, pp. 20-40.\n' +
      '* [CRJ22] Cao A., Rockwell C., Johnson J.: Fwd: Real-time novel view synthesis with forward warping and depth. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 15713-15724.\n' +
      '* [CSH19] Chen X., Song J., Hilliges O.: Monocular neural image based rendering with continuous view control. In _Proceedings of the IEEE/CVF international conference on computer vision_ (2019), pp. 4090-4100.\n' +
      '* [CSL\\({}^{*}\\)23] Chen D. Z., Siddiqui Y., Lee H.-Y., Tulyakov S., Niessner M.: TextText: Text-driven texture synthesis via diffusion models. In _ICCV_ (2023).\n' +
      '* [CTZ20] Chen Z., Tagliasacchi A., Zhang H.: Bsp-net: Generating compact meshes via binary space partitioning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp. 45-54.\n' +
      '* [CUYH20] Choi Y., Uh Y., Yoo J., Ha J.-W.: Stargan v2: Diverse image synthesis for multiple domains. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2020), pp. 8188-8197.\n' +
      '* [CWS\\({}^{*}\\)15] Calli B., Walsman A., Singh A., Srinivasa S., Abbeel P., Dollar A. M.: Benchmarking in manipulation research: The ycb object and model set and benchmarking protocols. _arXiv preprint arXiv:1502.03143_ (2015).\n' +
      '* [CX\\({}^{*}\\)23] Chao Y.-W., Xiang Y., et al.: Fewsol: A dataset for few-shot object learning in robotic environments. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_ (2023), IEEE, pp. 9140-9146.\n' +
      '* [CXG\\({}^{*}\\)16] Choy C. B., Xu D., Gwak J., Chen K., Savarese S.: 3d-2m2: A unified approach for single and multi-view 3d object reconstruction. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VII 14_ (2016), Springer, pp. 628-644.\n' +
      '* [CXG\\({}^{*}\\)22] Chen A., Xu Z., Geiger A., Yu J., Su H.: TensoRF: Tensorial radiance fields. In _European Conference on Computer Vision_ (2022).\n' +
      '* [CXZ\\({}^{*}\\)21] Chen A., Xu Z., Zhao F., Zhang X., Xiang F., Yu J., Su H.: MVSNeRF: Fast generalizable radiance field reconstruction from multi-view stereo. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14124-14133.\n' +
      '* [CYAE\\({}^{*}\\)20] Cai R., Yang G., Averbuch-Elor H., Hao Z., Belongie S., Snavely N., Hariharan B.: Learning gradient fields for shape generation. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16_ (2020), Springer, pp. 364-381.\n' +
      '* [CYL\\({}^{*}\\)22] Chen Y., Yuan Q., Li Z., Liu Y., Wang W., Xie C., Wen X., Yu Q.: UPST-NeRF: Universal photorealistic style transfer of neural radiance fields for 3d scene. In _arXiv preprint arXiv:2208.07059_ (2022).\n' +
      '* [CYW\\({}^{*}\\)23] Cheng X., Yang T., Wang J., Li Y., Zhang L., Zhang J., Yuan L.: Progressive3D: Progressively local editing for text-to-3d content creation with complex semantic prompts. _arXiv preprint arXiv:2310.11784_ (2023).\n' +
      '* [CZ19] Chen Z., Zhang H.: Learning implicit fields for generative shape modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2019), pp. 5939-5948.\n' +
      '* [CZC\\({}^{*}\\)24] Chen H., Zhang Y., Cun X., Xia M., Wang X., Weng C., Shan Y.: Videocrafer2: Overcoming data limitations for high-quality video diffusion models, 2024. arXiv:2401.09047.\n' +
      '* [CZL\\({}^{*}\\)22] Chen X., Zhang Q., Li X., Chen Y., Feng Y., Wang X., Wang J.: Hallucinated neural radiance fields in the wild. In _CVPR_ (2022), pp. 12943-12952.\n' +
      '* [CZY\\({}^{*}\\)23] Chen Y., Zhang C., Yang X., Cai Z., Yu G., Yang L., Lin G.: I3d: Improved text-to-3d generation with explicit view synthesis. _arXiv preprint arXiv:2308.11473_ (2023).\n' +
      '* [DBD\\({}^{*}\\)22] Darmon F., Bascle B., Devaux J.-C., Monasse P., Aubry M.: Improving neural implicit surfaces geometry with patch warping. In _CVPR_ (2022).\n' +
      '* [DCS\\({}^{*}\\)17] Dai A., Chang A. X., Savva M., Halber M., Funkhouser T., Niessner M.: Scannet: Richly-annotated 3d reconstructions of indoor scenes. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2017), pp. 5828-5839.\n' +
      '* [DFK\\({}^{*}\\)22] Downs L., Francis A., Koenig N., Kinman B., Hickman R., Reymann K., McHugh T. B., Vanhoucke V.: Google scanned objects: A high-quality dataset of 3d scanned household items. In _2022 International Conference on Robotics and Automation (ICRA)_ (2022), IEEE, pp. 2553-2560.\n' +
      '* [DJQ\\({}^{*}\\)23] Deng C., Jiang C., Qi C. R., Yan X., Zhou Y., Guibas L., Anguelov D., et al.: Nerdl: Single-view nerf synthesis with language-guided diffusion as general image priors. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 20637-20647.\n' +
      '* [DLW\\({}^{*}\\)23] Deitke M., Liu R., Nallingford M., Ngo H., Michel O., Kusupati A., Fan A., Laforte C., Voleti V., Gadre S. Y., et al.: Obiayverse-xl: A universe of 10m+ 3d objects. _arXiv preprint arXiv:2307.05663_ (2023).\n' +
      '* [Doe16] Doersch C.: Tutorial on variational autoencoders. _arXiv preprint arXiv:1606.05908_ (2016).\n' +
      '* [DRB\\({}^{*}\\)18] Dai A., Ritchie D., Bokeloh M., Reed S., Sturm J., Niessner M.: Scannet: Large-scale scene completion and semantic segmentation for 3d scans. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_ (2018), pp. 4578-4587.\n' +
      '* [DSS\\({}^{*}\\)23] Deitke M., Schwenk D., Salvador J., Weihs L., Michel O., Vanderbilt E., Schmidt L., Ehsani K., Kembhavi A., Farhadi A.: Obiayverse: A universe of annotated 3d objects. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 13142-13153.\n' +
      '* [DZL\\({}^{*}\\)20] Dai P., Zhang Y., Li Z., Liu S., Zeng B.: Neural point cloud rendering via multi-plane projection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp. 7830-7839.\n' +
      '* [DZW\\({}^{*}\\)20] Duan Y., Zhu H., Wang H., Yi L., Nevatia R., Guibas L. J.: Curriculum deepsdf. In _European Conference on Computer Vision_ (2020), Springer, pp. 51-67.\n' +
      '* [DZY\\({}^{*}\\)21] Du Y., Zhang Y., Yu H.-X., Tenenbaum J. B., Wu J.: Neural radiance flow for 4d view synthesis and video processing. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), IEEE Computer Society, pp. 14304-14314.\n' +
      '\n' +
      '* [EGO\\({}^{*}\\)20]Erler P., Guerrero P., Ohrhallinger S., Mitra N. J., Wimmer M.: Points2surf learning implicit surfaces from point clouds. In _European Conference on Computer Vision_ (2020), Springer, pp. 108-124.\n' +
      '* [EMS\\({}^{*}\\)23]Erkoc Z., Ma F., Shan Q., Niessner M., Dai A.: Hyperdiffusion: Generating implicit neural fields with weight-space diffusion. _arXiv preprint arXiv:2303.17015_ (2023).\n' +
      '* [EST\\({}^{*}\\)20]Egger B., Smith W. A., Tewari A., Wuher S., Zollhofer M., Beeler T., Bernard F., Bolkart T., Kortylewski A., Romdhani S., et al.: 3d morphable face models--past, present, and future. _ACM Trans. Graph._ (2020).\n' +
      '* [FAKD23]Fridman R., Abecasis A., Kasten Y., Dekel T.: Secesnexer: Text-driven consistent scene generation. _arXiv preprint arXiv:2302.01133_ (2023).\n' +
      '* [FBD\\({}^{*}\\)19]Flynn J., Broxton M., Debevec P., DuVall M., Fyffe G., Overbeck R., Snavely N., Tucker R.: Decepview: View synthesis with learned gradient descent. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2019), pp. 2367-2376.\n' +
      '* [FJG\\({}^{*}\\)21]Fu H., Jia R., Gao L., Gong M., Zhao B., Maybank S., Tao D.: 3d-future: 3d furniture shape with texture. _International Journal of Computer Vision 129_ (2021), 3313-3337.\n' +
      '* [FJW\\({}^{*}\\)22]Fan Z., Jiang Y., Wang P., Gong X., Xu D., Wang Z.: Unified implicit neural stylization. In _ECCV_ (2022), Springer.\n' +
      '* [FKYT\\({}^{*}\\)22]Fridovich-Keil S., Yu A., Tancik M., Chen Q., Recht B., Kanazawa A.: Plenoxels: Radiance fields without neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 5501-5510.\n' +
      '* [FLJ\\({}^{*}\\)22]Fu J., Li S., Jiang Y., Lin K.-Y., Qian C., Loy C. C., Wu W., Liu Z.: Stylegan-human: A data-centric odyssey of human generation. In _European Conference on Computer Vision_ (2022), Springer, pp. 1-19.\n' +
      '* [FKOT22]Fu Q., Xu Q., Ong Y. S., Tao W.: Geo-NeuS: Geometry-consistent neural implicit surfaces learning for multi-view reconstruction. _Advances in Neural Information Processing Systems 35_ (2022), 3403-3416.\n' +
      '* [GAA\\({}^{*}\\)22]Gal R., Alaluf Y., Atzmon Y., Patashnik O., Bermano A. H., Chechik G., Cohen-Or D.: An image is worth one word: Personalizing text-to-image generation using textual inversion. _arXiv preprint arXiv:2208.01618_ (2022).\n' +
      '* [GCL\\({}^{*}\\)21]Guo Y., Chen K., Liang S., Liu Y.-J., Bao H., Zhang J.: Ad-erf: Audio driven neural radiance fields for talking head synthesis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 5784-5794.\n' +
      '* [GCS\\({}^{*}\\)20]Genova K., Cole F., Sud A., Sarma A., Funkhouser T.: Local deep implicit functions for 3d shape. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp. 4857-4866.\n' +
      '* [GCV\\({}^{*}\\)19]Genova K., Cole F., Vlasic D., Sarma A., Freeman W. T., Funkhouser T.: Learning shape templates with structured implicit functions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2019), pp. 7154-7164.\n' +
      '* [GEB16]Gatys L. A., Ecker A. S., Bethge M.: Image style transfer using convolutional neural networks. In _CVPR_ (2016).\n' +
      '* [GII\\({}^{*}\\)21]Grigorev A., Isakov K., Iainna A., Bashirov R., Zakhari I., Vakhitov A., Lempitsky V.: Stylepeople: A generative model of fullbody human avatars. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 5151-5160.\n' +
      '* [GKG\\({}^{*}\\)23]Giebenhain S., Kirschenstein T., Georgopoulos M., Runz M., Agapito L., Niessner M.: Learning neural parametric head models. In _CVPR_ (2023).\n' +
      '* [GKJ\\({}^{*}\\)21]Garbin S. J., Kowalski M., Johnson M., Shotton J.: Valentin J.: Fastnerf: High-fidelity neural rendering at 200fps. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14346-14355.\n' +
      '* [GLWT22]Gu J., Liu L., Wang P., Theobalt C.: StyleNeRF: A style-based 3d-aware generator for high-resolution image synthesis. In _Int. Conf. Learn. Represent._ (2022).\n' +
      '* [GLZ\\({}^{*}\\)23]Gao X., Li X., Zhang C., Zhang Q., Cao Y., Shan Y., Quan L.: Context-human: Free-view rendering of human from a single image with texture-consistent synthesis. _arXiv preprint arXiv:2311.17123_ (2023).\n' +
      '* [GMW17]Gadelha M., Maji S., Wang R.: 3d shape induction from 2d views of multiple objects. In _2017 International Conference on 3D Vision (3DV)_ (2017), IEEE, pp. 402-411.\n' +
      '* [GNL\\({}^{*}\\)23]Ge S., Nah S., Liu G., Poon T., Tao A., Catanzaro B., Jacobs D., Huang J.-B., Liu M.-Y., Balaji Y.: Preserve your own correlation: A noise prior for video diffusion models. In _ICCV_ (2023).\n' +
      '* [GPAM\\({}^{*}\\)14]Goodfellow I., Pouget-Abadie J., Mirza M., Xu B., Warde-Farley D., Ozair S., Courville A., Bengio Y.: Generative adversarial nets. _Advances in neural information processing systems 27_ (2014).\n' +
      '* [GPL\\({}^{*}\\)22]Grassal P.-W., Prinzler M., Leistner T., Rother C., Niessner M., Thies J.: Neural head avatars from monocular RGB videos. In _CVPR_ (2022).\n' +
      '* [GSW\\({}^{*}\\)21]Gui J., Sun Z., Wen Y., Tao D., Ye J.: A review on generative adversarial networks: Algorithms, theory, and applications. _IEEE transactions on knowledge and data engineering 35_, 4 (2021), 3313-3332.\n' +
      '* [GSW\\({}^{*}\\)22]Gao J., Shen T., Wang Z., Chen W., Yin K., Li D., Litany O., Gojicc Z., Fidler S.: GET3D: A generative model of high quality 3d textured shapes learned from images. In _NeurIPS_ (2022).\n' +
      '* [GTZN21]Gafni G., Thies J., Zollhofer M., Niessner M.: Dynamic neural radiance fields for monocular 4D facial avatar reconstruction. In _CVPR_ (2021).\n' +
      '* [GWH\\({}^{*}\\)20]Guo Y., Wang H., Hu Q., Liu H., Liu L., Bennamoun M.: Deep learning for 3d point clouds: A survey. _IEEE transactions on pattern analysis and machine intelligence 43_, 12 (2020), 4338-4364.\n' +
      '* [GWY\\({}^{*}\\)21]Gao L., Wu T., Yuan Y.-J., Lin M.-X., Lai Y.-K., Zhang H.: Tm-net: Deep generative networks for textured meshes. _ACM Transactions on Graphics (TOG) 40_, 6 (2021), 1-15.\n' +
      '* [GXN\\({}^{*}\\)23]Gupta A., Xiong W., Nie Y., Jones I., Oguz B.: 3dgen: Trilinear latent diffusion for textured mesh generation. _arXiv preprint arXiv:2303.05371_ (2023).\n' +
      '* [GYW\\({}^{*}\\)19a]Gao L., Yang J., Wu T., Yuan Y.-J., Fu H., Lai Y.-K., Zhang H.: Sdm-net: Deep generative network for structured deformable mesh. _ACM Transactions on Graphics (TOG) 38_, 6 (2019), 1-15.\n' +
      '* [GYW\\({}^{*}\\)19b]Gao L., Yang J., Wu T., Yuan Y.-J., Fu H., Lai Y.-K., Zhang H.: Sdm-net: Deep generative network for structured deformable mesh. _ACM Transactions on Graphics (TOG) 38_, 6 (2019), 1-15.\n' +
      '* [GZX\\({}^{*}\\)22]Gao X., Zhong C., Xiang J., Hong Y., Guo Y., Zhang J.: Reconstructing personalized semantic realr model from monocular video. _ACM Trans. Graph._ (2022).\n' +
      '* [HB17]Huang X., Belongie S.: Arbitrary style transfer in real-time with adaptive instance normalization. In _ICCV_ (2017).\n' +
      '* [HCL\\({}^{*}\\)22]Hong F., Chen Z., Lan Y., Pan L., Liu Z.: Eva3d: Compositional 3d human generation from 2d image collections. _arXiv preprint arXiv:2210.04888_ (2022).\n' +
      '* [HCO\\({}^{*}\\)23]Holein L., Cao A., Owens A., Johnson J., Niessner M.: Text2room: Extracting textured 3d meshes from 2d text-to-image models. _arXiv preprint arXiv:2303.11989_ (2023).\n' +
      '\n' +
      '* [HHP\\({}^{*}\\)23]Hu S., Hong F., Pan L., Mei H., Yang L., Liu Z.: Sherf: Generalizable human nerf from a single image. _arXiv preprint arXiv:2303.12791_ (2023).\n' +
      '* [HHY\\({}^{*}\\)22]Huang Y.-H., He Y., Yuan Y.-J., Lai Y.-K., Gao L.: Style2defined: consistent 3d scene stylization as stylized nerf via 2d-3d mutual learning. In _CVPR_ (2022).\n' +
      '* [HJA20]Ho J., Jain A., Abbeel P.: Denoising diffusion probabilistic models. _Advances in neural information processing systems 33_ (2020), 6840-6851.\n' +
      '* [HLA\\({}^{*}\\)19]Hu Y., Li T.-M., Anderson L., Ragan-Kelley J., Durand F.: Taichi: a language for high-performance computation on spatially sparse data structures. _ACM Transactions on Graphics (TOG) 38_, 6 (2019), 1-16.\n' +
      '* [HLHF22]Hui K.-H., Li R., Hu J., Fu C.-W.: Neural wavelet-domain diffusion for 3d shape generation. In _SIGGRAPH Asia 2022 Conference Papers_ (2022), pp. 1-9.\n' +
      '* [HMR19a]Henzler P., Mitra N. J., Ritschel T.: Escaping plato\'s cave: 3D shape from adversarial rendering. In _ICCV_ (2019).\n' +
      '* [HMR19b]Henzler P., Mitra N. J., Ritschel T.: Escaping plato\'s cave: 3d shape from adversarial rendering. In _ICCV_ (2019).\n' +
      '* [HPX\\({}^{*}\\)22]Hong Y., Peng B., Xiao H., Liu L., Zhang J.: Head-NeRF: A real-time nerf-based parametric head model. In _CVPR_ (2022).\n' +
      '* [HRBP21]Hu R., Ravi N., Berg A. C., Pathak D.: Worldsheet: Wrapping the world in a 3d sheet for view synthesis from a single image. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 12528-12537.\n' +
      '* [HRL\\({}^{*}\\)21]Henzler P., Reizenstein J., Labatut P., Shapovalov R., Ritschel T., Vedaldi A., Novotny D.: Unsupervised learning of 3d object categories from videos in the wild. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 4700-4709.\n' +
      '* [HSG\\({}^{*}\\)22]Ho J., Salimans T., Gritsenko A., Chan W., Norouzi M., Fleet D. J.: Video diffusion models. In _NeurIPS_ (2022).\n' +
      '* [HSZ\\({}^{*}\\)23]Huang X., Shao R., Zhang Q., Zhang H., Feng Y., Liu Y., Wang Q.: Humannorm: Learning normal diffusion model for high-quality and realistic 3d human generation. _arXiv preprint arXiv:2301.01406_ (2023).\n' +
      '* [HTE\\({}^{*}\\)23]Haque A., Tancik M., Efros A. A., Holynski A., Kanazawa A.: Instruct-NeRF2NeRF: Editing 3d scenes with instructions. In _ICCV_ (2023).\n' +
      '* [HTS\\({}^{*}\\)21]Huang H.-P., Tseng H.-Y., Saini S., Singh M., Yang M.-H.: Learning to stylize novel views. In _ICCV_ (2021).\n' +
      '* [HWZ\\({}^{*}\\)23]Huang Y., Wang J., Zeng A., Cao H., Qi X., Shi Y., Zha Z.-J., Zhang L.: Dreamwalkitz: Make a scene with complex 3d animatable avatars. _arXiv preprint arXiv:2305.12529_ (2023).\n' +
      '* [HYL\\({}^{*}\\)23]Huang Y., Yi H., Liu W., Wang H., Wu B., Wang W., Lin B., Zhang D., Cai D.: One-shot implicit animatable avatars with model-based priors. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2023), pp. 8974-8985.\n' +
      '* [HYX\\({}^{*}\\)23]Huang Y., Yi H., Xiu Y., Liao T., Tang J., Cai D., Thies J.: Tech: Text-guided reconstruction of lifelike clothed humans. _arXiv preprint arXiv:2308.08545_ (2023).\n' +
      '* [HZF\\({}^{*}\\)22]Huang X., Zhang Q., Feng Y., Li H., Wang X., Wang Q.: Hdr-nerf: High dynamic range neural radiance fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 18398-18408.\n' +
      '* [HZF\\({}^{*}\\)23]Huang X., Zhang Q., Feng Y., Li H., Wang Q.: Inverting the imaging process by learning an implicit camera model. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 21456-21465.\n' +
      '* [HZF\\({}^{*}\\)23b]Huang X., Zhang Q., Feng Y., Li X., Wang X., Wang Q.: Local implicit ray function for generalizable radiance field representation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 97-107.\n' +
      '* [HZP\\({}^{*}\\)22]Hong F., Zhang M., Pan L., Cai Z., Yang L., Liu Z.: Avatarclip: Zero-shot text-driven generation and animation of 3d avatars. _arXiv preprint arXiv:2205.08535_ (2022).\n' +
      '* [ID18]Inafutdinov E., Dosovitskiy A.: Unsupervised learning of shape and pose with differentiable point clouds. _Advances in neural information processing systems 31_ (2018).\n' +
      '* [JCL\\({}^{*}\\)22]Jiang K., Chen S.-Y., Liu F.-L., Fu H., Gao L.: NeRF-FaceFighting: Disentangled face editing in neural radiance fields. In _SIGGRAPH Asia Conference Papers_ (2022), 18, 19\n' +
      '* [JJW\\({}^{*}\\)23]Jiang S., Jiang H., Wang Z., Luo H., Chen W., Xu L.: Humangen: Generating human radiance fields with explicit priors. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 12543-12554.\n' +
      '* [JKK\\({}^{*}\\)23]Jambon C., Kerbl B., Kopanas G., Diolatzis S., Drettakis G., Leimkohler T.: Isrfshop: Interactive editing of neural radiance fields. In _Proceedings of the ACM on Computer Graphics and Interactive Techniques_ (2023).\n' +
      '* [JLF22]Johari M. M., Lepoittevin Y., Fleuret F.: GeoNeRF: Generalizing nerf with geometry priors. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 18365-18375.\n' +
      '* [JMB\\({}^{*}\\)22]Jah A., Mildenhall B., Barron J. T., Abbeel P., Poole B.: Zero-shot text-guided object generation with dream fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 867-876.\n' +
      '* [JN23]Jun H., Nichol A.: Shap-e: Generating conditional 3d implicit functions. _arXiv preprint arXiv:2305.02463_ (2023).\n' +
      '* [JWZ\\({}^{*}\\)23]Jiang R., Wang C., Zhang J., Chai M., He M., Chen D., Liao J.: Avatarra: Transforming text into neural human avatars with parameterized shape and pose control. _arXiv preprint arXiv:2303.17606_ (2023).\n' +
      '* [KZ\\({}^{*}\\)23]Kolotouros N., Alldieck T., Zanfir A., Bazavan E. G., Fieraru M., Sminchisescu C.: Dreamhuman: Animatable 3d avatars from text. _arXiv preprint arXiv:2306.09329_ (2023).\n' +
      '* [KBM\\({}^{*}\\)20]Kato H., Beker D., Morariu M., Ando T., Matsuoka T., Kehl W., Gabon A.: Differentiable rendering: A survey. _arXiv preprint arXiv:2006.12057_ (2020).\n' +
      '* [KBV20]Klokov R., Boyer E., Verbeek J.: Discrete point flow networks for efficient point cloud generation. In _European Conference on Computer Vision_ (2020), Springer, pp. 694-710.\n' +
      '* [KDJ\\({}^{*}\\)23]Kwak J.-g., Dong E., Jin Y., Ko H., Mahajan S., Yi K. M.: Vivid-1-to-3: Novel view synthesis with video diffusion models. _arXiv preprint arXiv:2312.01305_ (2023).\n' +
      '* [KDSB22]Kulhanek J., Derner E., Sattler T., Babuska R.: Viewformer: Next-free neural rendering from few images using transformers. In _European Conference on Computer Vision_ (2022), Springer, pp. 198-216.\n' +
      '* [KFH\\({}^{*}\\)22]Kerr J., Fu L., Huang H., Aigal Y., Tancik M., Ichnowski J., Kanazawa A., Goldberg K.: Evo-nerf: Evolving nerf for sequential robot grasping of transparent objects. In _6th Annual Conference on Robot Learning_ (2022).\n' +
      '* [KKL\\({}^{*}\\)23]Kim B., Kwon P., Lee K., Lee M., Han S., Kim D., Joo H.: Chupa: Carving 3d clothed humans from skinned shape priors using 2d diffusion probabilistic models. _arXiv preprint arXiv:2305.11870_ (2023).\n' +
      '* [KKLD23]Kerbl B., Kopanas G., Leimkohler T., Drettakis G.: 3d gaussian splatting for real-time radiance field rendering. _ACM Transactions on Graphics (TOG) 42_, 4 (2023), 1-14.\n' +
      '\n' +
      '* [KKR18]Knyaz V. A., Kniaz V. V., Remondino F.: Image-to-voxel model translation with conditional adversarial networks. In _Proceedings of the European Conference on Computer Vision (ECCV) Workshops_ (2018), pp. 0-0.\n' +
      '* [KLA19]Karras T., Laine S., Aila T.: A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2019), pp. 4401-4410.\n' +
      '* [KLA*20]Karras T., Laine S., Aittala M., Hellsten J., Lehtinen J., Aila T.: Analyzing and improving the image quality of StyleGAN. In _CVPR_ (2020), 16, 17\n' +
      '* [KLK*20]Kim H., Lee H., Kang W. H., Lee J. Y., Kim N. S.: Software: Probabilistic framework for normalizing flow on manifolds. _Advances in Neural Information Processing Systems 33_ (2020), 16388-16397.\n' +
      '* [KLY*21]Koh J. Y., Lee H., Yang Y., Baldridge J., Anderson P.: Pathdreamer: A world model for indoor navigation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14738-14748.\n' +
      '* [KMS22]Kobayashi S., Matsumoto E., Sitzmann V.: Decomposing nerf for editing via feature field distillation. In _NeurIPS_ (2022).\n' +
      '* [KNH*22]Khan S., Naseer M., Hayat M., Zamir S. W., Khan F. S., Shah M.: Transformers in vision: A survey. _ACM computing surveys (CSUR) 54_, 108 (2022), 1-4.\n' +
      '* [KPHL17]Kusner M. J., Paige B., Hernandez-Lobato J. M.: Grammar variational autoencoder. In _International conference on machine learning_ (2017), PMLR, pp. 1945-1954.\n' +
      '* [KPLD21]Kopanas G., Philip J., Leimkohler T., Drettakis G.: Point-based neural rendering with per-view optimization. In _Computer Graphics Forum_ (2021), vol. 40, Wiley Online Library, pp. 29-43.\n' +
      '* [KPWS22]Kalischek N., Peters T., Wegner J. D., Schindler K.: Tetrahedral diffusion models for 3d shape generation. _arXiv preprint arXiv:2211.13220_ (2022).\n' +
      '* [KOG*23]Kirschstein T., Qian S., Giebenhain S., Walter T., Niessner M.: NeResp: Multiple-view radiance field reconstruction of human heads. _ACM Trans. Graph._ (2023), 17\n' +
      '* [KSZ*21]Kosiorek A. R., Strathmann H., Zoran D., Moreno P., Schneider R., Morka S., Rezende D. J.: Nerf-vae: A geometry aware 3d scene generative model. In _ICML_ (2021).\n' +
      '* [KUH18]Kato H., Ushiku Y., Harada T.: Neural 3d mesh renderer. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2018), pp. 3907-3916.\n' +
      '* [KVNN23]Karnewar A., Vedaldi A., Novotny D., Mitra N. J.: Holddiffusion: Training a 3d diffusion model using 2d images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 18423-18433.\n' +
      '* [KW13]Kingma D. P., Welling M.: Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_ (2013).\n' +
      '* [KWKT15]Kulkarni T. D., Whitney W. F., Kohli P., Tenenbaum J.: Deep convolutional inverse graphics network. _Advances in neural information processing systems 28_ (2015).\n' +
      '* [KXD12]Kasper A., Xue Z., Dillmann R.: The kit object models database: An object model database for object recognition, localization and manipulation in service robotics. _The International Journal of Robotics Research 31_, 8 (2012), 927-934.\n' +
      '* [KYL21]Kim J., Yoo J., Lee J., Hong S.: Setvae: Learning hierarchical composition for generative modeling of set-structured data. In _CVPR_ (2021).\n' +
      '* [LB14]Loper M. M., Black M. J.: Opendr: An approximate differentiable renderer. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13_ (2014), Springer, pp. 154-169.\n' +
      '* [LBRF11]Lai K., Bo L., Ren X., Fox D.: A large-scale hierarchical multi-view rgb-d object dataset. In _2011 IEEE international conference on robotics and automation_ (2011), IEEE, pp. 1817-1824.\n' +
      '* [LC22]Lee H.-H., Chang A. X.: Understanding pure clip guidance for voxel grid metfs. _arXiv preprint arXiv:2209.15172_ (2022).\n' +
      '* [LCCT23]Li W., Chen R., Chen X., Tan P.: Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. _arXiv preprint arXiv:2310.02596_ (2023).\n' +
      '* [LDS*23]Li Y., Dou Y., Shi Y., Lei Y., Chen X., Zhang Y., Zhou P., Ni B.: FocalDreamer: Text-driven 3d editing via focal-fusion assembly. _arXiv preprint arXiv:2308.10608_ (2023).\n' +
      '* [LDZL23]Li M., Duan Y., Zhou J., Lu J.: Diffusion-sdf: Text-to-shape via voxelized diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 12642-12651.\n' +
      '* [LFB*23]Liu Z., Feng Y., Black M. J., Nowrouzezahrai D., Paull L., Liu W.: Meshdiffusion: Score-based generative 3d mesh modeling. _arXiv preprint arXiv:2308.08133_ (2023).\n' +
      '* [LFLSY*23]Lin G., Feng-Lin L., Shu-Yu C., Kaiwen J., Chumpeng L., Lai Y., Hongbo F.: SketchFaceseRF: Sketch-based facial generation and editing in neural radiance fields. _ACM Trans. Graph._ (2023).\n' +
      '* [LFS*21]Li J., Feng Z., She Q., Ding H., Wang C., Lee G. H.: Mine: Towards continuous depth mpi with nerf for novel view synthesis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 12578-12588.\n' +
      '* [LGL*23]Long X., Guo Y.-C., Lin C., Liu Y., Dou Z., Liu L., Ma Y., Zhang S.-H., Habermann M., Theobalt C., et al.: Won-dcf3: Single image to 3d using cross-domain diffusion. _arXiv preprint arXiv:2310.15008_ (2023).\n' +
      '* [LGF*23]Lin C.-H., Gao J., Tang L., Takikawa T., Zeng X., Huang X., Kreis K., Fidler S., Liu M.-Y., Lin T.-Y.: Magic3d: High-resolution text-to-3d content creation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 300-309.\n' +
      '* [LGZ*20]Liu L., Gu J., Zaw Lin K., Chua T.-S., Theobalt C.: Neural sparse voxel fields. _Advances in Neural Information Processing Systems 33_ (2020), 15651-15663.\n' +
      '* [LH21]Luo S., Hu W.: Diffusion probabilistic models for 3d point cloud generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 2837-2845.\n' +
      '* [LHC*23]Lin Y., Han H., Gong C., Xu Z., Zhang Y., Li X.: Consistent123: One image to highly consistent 3d asset using case-aware diffusion priors. _arXiv preprint arXiv:2309.17261_ (2023).\n' +
      '* [LHR*21]Liu L., Habermann M., Rudney V., Sarkar K., Gu J., Theobalt C.: Neural actor: Neural free-view synthesis of human actors with pose control. _ACM transactions on graphics (TOG) 40_, 6 (2021), 1-16.\n' +
      '* [Lin68]Lindenmayer A.: Mathematical models for cellular interactions in development i. filaments with one-sided inputs. _Journal of theoretical biology 18_, 3 (1968), 280-299.\n' +
      '* [LKL18]Lin C.-H., Kong C., Lucey S.: Learning efficient point cloud generation for dense 3d object reconstruction. In _proceedings of the AAAI Conference on Artificial Intelligence_ (2018), vol. 32.\n' +
      '* [LLCL19]Liu S., Li T., Chen W., Li H.: Soft rasterizer: A differentiable renderer for image-based 3d reasoning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2019), pp. 7708-7717.\n' +
      '* [LLF*23]Li Y., Lin Z.-H., Forsyth D., Huang J.-B., Wang S.: ClimateNeRF: Physically-based neural rendering for extreme climate synthesis. In _ICCV_ (2023).\n' +
      '* [LLL*14]Li B., Lu Y., Li C., Godil A., Schreck T., Aono M., Chen Q., Chowdhury N. K., Fang B., Furuya T., et al.:Shrec\'14 track: Large scale comprehensive 3d shape retrieval. In _Eurographics Workshop on 3D Object Retrieval_ (2014), vol. 2.\n' +
      '* [LLQ*16]Liu Z., Luo P., Qiu S., Wang X., Tang X.: Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2016), pp. 1096-1104.\n' +
      '* [LLWT15]Liu Z., Luo P., Wang X., Tang X.: Deep learning face attributes in the wild. In _Proceedings of the IEEE international conference on computer vision_ (2015), pp. 3730-3738.\n' +
      '* [LLZ*23]Liu Y., Lin C., Zeng Z., Long X., Liu L., Komura T., Wang W.: Syncreamer: Generating multiview-consistent images from a single-view image. _arXiv preprint arXiv:2309.03453_ (2023).\n' +
      '* [LLZl21]Luo A., Li T., Zhang W.-H., Lee T. S.: Surfgen: Adversarial 3d shape synthesis with explicit surface discriminators. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 16238-16248.\n' +
      '* [LMTL21]Lin C.-H., Ma W.-C., Torralba A., Lucey S.: Barf: Bundle-adjusting neural radiance fields. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 5741-5751.\n' +
      '* [LMY*23]Li Y., Ma C., Yan Y., Zhu W., Yang X.: 3d-aware face swapping. In _CVPR_ (2023).\n' +
      '* [LPT13]Lim J. J., Pirsiavash H., Torralba A.: Parsing ikea objects: Fine pose estimation. In _Proceedings of the IEEE international conference on computer vision_ (2013), pp. 2992-2999.\n' +
      '* [LSC*22]Levis A., Srinivasan P. P., Chael A. A., Ng R., Bouman K. L.: Gravitationally lensed black hole emission tomography. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 19841-19850.\n' +
      '* [LSMG20]Liao Y., Schwarz K., Mescheder L., Geiger A.: Towards unsupervised learning of generative models for 3d controllable image synthesis. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2020), pp. 5871-5880.\n' +
      '* [LSS*21]Lombardi S., Simon T., Schwartz G., Zollhofer M., Sheikh Y., Saragih J.: Mixture of volumetric primitives for efficient neural rendering. _ACM Trans. Graph._ (2021).\n' +
      '* [LSSS18]Lombardi S., Saragih J., Simon T., Sheikh Y.: Deep appearance models for face rendering. _ACM Trans. Graph._ (2018).\n' +
      '* [LSZ*22]Li T., Slavcheva M., Zollhofer M., Green S., Lassner C., Kim C., Schmidt T., Lovegrove S., Goesele M., Newcombe R., et al.: Neural 3d video synthesis from multi-view video. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 5521-5531.\n' +
      '* [LTJ18]Liu H.-T., Tao M., Jacobson A.: Paparazzi: surface editing by way of multi-view image processing. _ACM Trans. Graph._ 37, 6 (2018), 221-.\n' +
      '* [LTJ*21]Liu A., Tucker R., Jampani V., Makadia A., Snavely N., Kanazawa A.: Infinite nature: Perpethul view generation of natural scenes from a single image. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14458-14467.\n' +
      '* [LTZ*23]Li J., Tan H., Zhang K., Xu Z., Luan F., Xu Y., Hong Y., Sunkavalli K., Shakhinarovich G., Bi S.: Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. _arXiv preprint arXiv:2311.06214_ (2023).\n' +
      '* [LWA*23]Lyu Z., Wang J., An Y., Zhang Y., Lin D., Dai B.: Controllable mesh generation through sparse latent point diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 271-280.\n' +
      '* [LWC*23]Li Z., Wang Q., Cole F., Tucker R., Snavely N.: Dynibar: Neural dynamic image-based rendering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 4273-4284.\n' +
      '* [LWQZ22]Liu Z., Wang Y., Qi X., Fu C.-W.: Towards implicit text-guided 3d shape generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 17896-17906.\n' +
      '* [LWSK22]Li Z., Wang Q., Snavely N., Kanazawa A.: Infinitentane-zero: Learning perpetual view generation of natural scenes from single images. In _European Conference on Computer Vision_ (2022), Springer, pp. 515-534.\n' +
      '* [LWVH*23]Liu R., Wu R., Van Hoorick B., Tokmakov P., Zakharov S., Vondrick C.: Zero-1-to-3: Zero-shot one image to 3d object. _arXiv preprint arXiv:2303.11328_ (2023).\n' +
      '* [LXC*21]Li J., Xu C., Chen Z., Bian S., Yang L., Lu C.: Hybrid: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2021), pp. 3383-3393.\n' +
      '* [LXJ*23]Liu M., Xu C., Jin H., Chen L., Xu Z., Su H., et al.: One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. _arXiv preprint arXiv:2306.16928_ (2023).\n' +
      '* [LXM*20]Liu K.-E., Xu Z., Milchenhal I., Srinivasan P. P., Hold-Geoffroy Y., DiVerdi S., Sun Q., Sunkavalli K., Ramamoorthi R.: Deep multi depth panoramas for view synthesis. In _European Conference on Computer Vision_ (2020), Springer, pp. 328-344.\n' +
      '* [LXZ*23]Lorraine J., Xie K., Zeng X., Lin C.-H., Takikawa T., Sharp N., Lin T.-Y., Liu M.-Y., Fidler S., Lucas J.: An3d: Amortized text-to-3d object synthesis. _arXiv preprint arXiv:2306.07349_ (2023).\n' +
      '* [LYX*24]Liao T., Yi H., Xiu Y., Tang J., Huang Y., Thies J., Black M. J.: TADA! text to animatable digital avatars. In _3DV_ (2024).\n' +
      '* [LZ21]Lassner C., Zollhofer M.: Pulsar: Efficient sphere-based neural rendering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 1440-1449.\n' +
      '* [LZF*23]Liang Z., Zhang Q., Feng Y., Shan Y., Jia K.: Gs-ir: 3d gaussian splitting for inverse rendering. _arXiv preprint arXiv:2311.16473_ (2023).\n' +
      '* [LZJ*22]Lei J., Zhang Y., Jia K., et al.: TANGO: Text-driven photorealistic and robust 3d stylization via lighting decomposition. In _NeurIPS_ (2022).\n' +
      '* [LZT*23]Liu X., Zhan X., Tang J., Shan Y., Zeng G., Lin D., Liu X., Liu Z.: Humangaussian: Text-driven 3d human generation with gaussian splitting. _arXiv preprint arXiv:2311.17061_ (2023).\n' +
      '* [LZW*23]Li C., Zhang C., Waghawase A., Lee L.-H., Rameau F., Yang Y., Bae S.-H., Hong C. S.: Generative ai meets 3d: A survey on text-to-3d in age era. _arXiv preprint arXiv:2305.06131_ (2023).\n' +
      '* [Man67]Mandelbrot B.: How long is the coast of britain? statistical self-similarity and fractional dimension. _science 156_, 3775 (1967), 636-638.\n' +
      '* [Max95]Max N.: Optical models for direct volume rendering. _IEEE Transactions on Visualization and Computer Graphics 1_, 2 (195), 99-108.\n' +
      '* [MBOL*22]Michel O., Bar-On R., Liu R., Benaim S., Hanocka R.: Text2mesh: Text-driven neural stylization for meshes. In _CVPR_ (2022).\n' +
      '* [MBRS*21]Martin-Brudalla R., Radwan N., Sajjadi M. S., Barron J. T., Dosovitskiy A., Duckworth D.: Nerf in the wild: Neural radiance fields for unconstrained photo collections. In _CVPR_ (2021), pp. 7210-7219.\n' +
      '* [MCL20]Morrison D., Corke P., Leitner J.: Egad! an evolved grasping analysis dataset for diversity and reproducibility in robotic manipulation. _IEEE Robotics and Automation Letters 5_, 3 (2020), 4368-4375.\n' +
      '* [MCL20]Morrison D., Corke P., Leitner J.: Egad! an evolved grasping analysis dataset for diversity and reproducibility in robotic manipulation. _IEEE Robotics and Automation Letters 5_, 3 (2020), 4368-4375.\n' +
      '\n' +
      '* [MCST22a]Mittal P., Cheng Y.-C., Singh M., Tulsiani S.: Autosdf: Shape priors for 3d completion, reconstruction and generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 306-315.\n' +
      '* [MCST22b]Mittal P., Cheng Y.-C., Singh M., Tulsiani S.: Autosdf: Shape priors for 3d completion, reconstruction and generation. In _CVPR_ (2022).\n' +
      '* [MESK22]Muller T., Evans A., Schied C., Keller A.: Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG) 41_, 4 (2022), 1-15.\n' +
      '* [Mid]Midjourney: Midjourney. [https://www.midjourney.com/](https://www.midjourney.com/).\n' +
      '* [MKLRV23]Melas-Kyriazi L., Laina I., Rupprecht C., Vedaldi A.: Realfusion: 360deg reconstruction of any object from a single image. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 8446-8455.\n' +
      '* [MKXP22]Mohammad Khalid N., Xie T., Bellovsky E., Popa T.: Clip-mesh: Generating textured meshes from text using pre-trained image-text models. In _SIGGRAPH Asia 2022 conference papers_ (2022), pp. 1-8.\n' +
      '* [MLL*22a]Ma L., Li X., Liao J., Wang X., Zhang Q., Wang J., Sande P. V.: Neural parameterization for dynamic human head editing. _ACM Transactions on Graphics (TOG) 41_, 6 (2022), 1-15.\n' +
      '* [MLL*22b]Ma L., Li X., Liao J., Zhang Q., Wang X., Wang J., Sande P. V.: Deblur-nerf: Neural radiance fields from blurry images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 12861-12870.\n' +
      '* [MM82]Mandelbrot B. B., Manelbrot B. B.: _The fractal geometry of nature_, vol. 1, W1 freen New York, 1982.\n' +
      '* [MPS*23]Mikaeli A., Perel O., Safaee M., Cohen-Or D., Mahdavi-Amiri A.: SKED: Sketch-guided text-based 3d editing. In _ICCV_ (2023).\n' +
      '* [MRP*23a]Metzer G., Richardson E., Patashnik O., Giryes R., Cohen-Or D.: Latent-nref for shape-guided generation of 3d shapes and textures. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 12663-12673.\n' +
      '* [MRP*23b]Metzer G., Richardson E., Patashnik O., Giryes R., Cohen-Or D.: Latent-NeRF for shape-guided generation of 3d shapes and textures. In _CVPR_ (2023).\n' +
      '* [MS15]Maturana D., Scherer S.: Voxnet A 3d convolutional neural network for real-time object recognition. In _2015 IEEE/RSJ international conference on intelligent robots and systems (IROS)_ (2015), IEEE, pp. 922-928.\n' +
      '* [MSOC*19]Mildenhall B., Srinivasan P. P., Ortiz-Cayon R., Kalantari N. K., Ramamoorthi R., Ng R., Kar A.: Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. _ACM Transactions on Graphics (TOG) 38_, 4 (2019), 1-4.\n' +
      '* [MSP*23]Muller N., Siddiqui Y., Porzi L., Bulo S. R., Kontschieder P., Niessner M.: Diffff: Rendering-guided 3d radiance field diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 4328-4338.\n' +
      '* [MSS*21]Ma S., Simon T., Saraghi J., Wang D., Li Y., De la Torre F., Sheikh Y.: Pixel codec avatars. In _CVPR_ (2021).\n' +
      '* [MST*20]Mildenhall B., Srinivasan P. P., Tancik M., Barron J. T., Ramamoorthi R., Ng R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In _European conference on computer vision_ (2020), Springer, pp. 405-421.\n' +
      '* [MYR*20]Ma Q., Yang J., Ranish A., Pujades S., Pons-Moll G., Tang S., Black M. J.: Learning to dress 3d people in generative clothing. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp. 6469-6478.\n' +
      '* [MZS*23]Ma Y., Zhang X., Sun X., Ji J., Wang H., Jiang G., Zhuang W., Ji R.: X-Mesh: Towards fast and accurate text-driven 3d stylization via dynamic textual guidance. In _ICCV_ (2023).\n' +
      '* [ND21]Nichol A. Q., Dhariwal P.: Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_ (2021), PMLR, pp. 8162-8171.\n' +
      '* [NDR*21]Nichol A., Dhariwal P., Ramesh A., Shyam P., Mishra P., McGrew B., Sutskever I., Chen M.: Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_ (2021).\n' +
      '* [NDVZJ19]Nimier-David M., Vicini D., Zelner T., Jakob W.: Mitsuba 2: A retargetable forward and inverse renderer. _ACM Transactions on Graphics (TOG) 38_, 6 (2019), 1-17.\n' +
      '* [Neu66]Neumann J. V.: Theory of self-reproducing automata. _Edited by Arthur W. Burks_ (1966).\n' +
      '* [NG21]Niemeyer M., Geiger A.: GIRAFFE: Representing scenes as compositional generative neural feature fields. In _CVPR_ (2021).\n' +
      '* [NGEB20a]Nash C., Ganin Y., Eslami S. A., Battaglia P.: Polygen: An autoregressive generative model of 3d meshes. In _International conference on machine learning_ (2020), PMLR, pp. 7220-7229.\n' +
      '* [NGEB20b]Nash C., Ganin Y., Eslami S. A., Battaglia P.: Polygen: An autoregressive generative model of 3d meshes. In _ICML_ (2020).\n' +
      '* [NJD*22]Nichol A., Jun H., Dhariwal P., Mishkin P., Chen M.: Point-e: A system for generating 3d point clouds from complex prompts. _arXiv preprint arXiv:2212.08757_ (2021).\n' +
      '* [NKR*22]Nam G., Khlip M., Rodriguez A., Tono A., Zhou L., Guerrero P.: 3d-ldm: Neural implicit 3d shape generation with latent diffusion models. _arXiv preprint arXiv:2212.00842_ (2022).\n' +
      '* [NPLT*19]Nguyen-Phuoc T., Li C., Theis L., Richardt C., Yang Y.-L.: HoloGAN: Unsupervised learning of 3d representations from natural images. In _ICCV Workshop_ (2019).\n' +
      '* [NPRM*20]Nguyen-Phuoc T. H., Richardt C., Mai L., Yang Y., Mitra A.: Blockgan: Learning 3d object-aware scene representations from unlabelled images. _Advances in neural information processing systems 33_ (2020), 6767-6778.\n' +
      '* [NSLH22]Noguchi A., Sun X., Lin S., Harada T.: Unsupervised learning of efficient geometry-aware neural articulated representations. In _European Conference on Computer Vision_ (2022), Springer, pp. 597-614.\n' +
      '* [OBB20]Osman A. A., Bolkart T., Black M. J.: Star: Sparse trained articulated human body regressor. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V 16_ (2020), Springer, pp. 598-613.\n' +
      '* [OELS*22]Oen-El R., Luo X., Shan M., Shechtman E., Park J. J., Kemelmacher-Shilzerman I.: StyleSDF: High-resolution 3d-consistent image and geometry generation. In _CVPR_ (2022).\n' +
      '* [OMN*19]Occhsle M., Mescheder L., Niemeyer M., Strauss T., Geiger A.: Texture fields: Learning texture representations in function space. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2019), pp. 4531-4540.\n' +
      '* [Ope]OpenAI: Dall-e 3. [https://openai.com/dall-e-3](https://openai.com/dall-e-3).\n' +
      '* [PCPMMN21]Pumarola A., Corona E., Pons-Moll G., Moreno-Noguer F.: D-nerf: Neural radiance fields for dynamic scenes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 10318-10327.\n' +
      '* [PDW*21]Peng S., Dong J., Wang Q., Zhang S., Shuai Q., Zhou X., Bao H.: Animatable neural radiance fields for modeling dynamic human bodies. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14314-14323.\n' +
      '* [Per85]Perlin K.: An image synthesizer. _ACM Siggraph Computer Graphics 19_, 3 (1985), 287-296.\n' +
      '* [Per02]Perlin K.: Improving noise. In _Proceedings of the 29th annual conference on Computer graphics and interactive techniques_ (2002), pp. 681-682.\n' +
      '\n' +
      '* [PFS*19]Park J. J., Florence P., Straub J., Newcombe R., Lovegrove S.: Deepsdf: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2019), pp. 165-174.\n' +
      '* [PGH*16]Pu Y., Gan Z., Henao R., Yuan X., Li C., Stevens A., Carin L.: Variational autoencoder for deep learning of images, labels and captions. _Advances in neural information processing systems 29_ (2016).\n' +
      '* [PJBM23]Poole B., Jain A., Barron J. T., Mildenhall B.: DreamFusion: Text-to-3d using 2d diffusion. In _Int. Conf. Learn. Represent._ (2023).\n' +
      '* [PKHL21]Pavllo D., Kohler J., Hofmann T., Lucchi A.: Learning generative models of textured 3d meshes from real-world images. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 13879-13889.\n' +
      '* [PRFS18]Park K., Rematas K., Farhadi A., Seitz S. M.: Photodshape: Photorealistic materials for large-scale shape collections. _arXiv preprint arXiv:1809.09761_ (2018).\n' +
      '* [PSB*21]Park K., Sinha U., Barron J. T., Bouaziz S., Goldman D. B., Seitz S. M., Martin-Revlalta R.: Nerfies: Deformable neural radiance fields. In _ICCV_ (2021).\n' +
      '* [PSH*20]Pavllo D., Spins G., Hofmann T., Moens M.-F., Lucchi A.: Convolutional generation of textured 3d meshes. _Advances in Neural Information Processing Systems 33_ (2020), 870-882.\n' +
      '* [PYG*23]Po R., Yifan W., Golyanik V., Aberman K., Barron J. T., Bermano A. H., Chan E. R., Dekel T., Holynski A., Kanazawa A., et al.: State of the art on diffusion models for visual computing. _arXiv preprint arXiv:2310.07204_ (2023).\n' +
      '* [PYL*22]Peng Y., Yan Y., Liu S., Cheng Y., Guan S., Pan B., Zhai G., Yang X.: CageNeRF: Cage-based neural radiance field for generalized 3d deformation and animation. In _NeurIPS_ (2022).\n' +
      '* [PZ17]Penner E., Zhang L.: Soft 3d reconstruction for view synthesis. _ACM Transactions on Graphics (TOG) 36_, 6 (2017), 1-11.\n' +
      '* [PZYBG00]Pfister H., Zwicker M., Van Baar J., Gross M.: Surfies: Surface elements as rendering primitives. In _Proceedings of the 27th annual conference on computer graphics and interactive techniques_ (2000), pp. 335-342.\n' +
      '* [QMH*23]Qian G., Mai J., Hamdi A., Ren J., Siarohin A., Li B., Lee H.-Y., Skorokhodhodov I., Wonka P., Tulkavko S., et al.: Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. _arXiv preprint arXiv:2306.17843_ (2023).\n' +
      '* [RALB22]Rakhimov R., Ardelean A.-T., Lempitsky V., Burnaev E.: Npbg++: Accelerating neural point-based graphics. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 15969-15979.\n' +
      '* [RBL*22]Rombach R., Blattmann A., Lorenz D., Esser P., Ommer B.: High-resolution image synthesis with latent diffusion models. In _CVPR_ (2022).\n' +
      '* [RBL*22b]Rombach R., Blattmann A., Lorenz D., Esser P., Ommer B.: High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2022), pp. 10684-10695.\n' +
      '* [REO21]Rombach R., Esser P., Ommer B.: Geometry-free view synthesis: Transformers and no 3d priors. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14356-14366.\n' +
      '* [RFJ21]Rockwell C., Fouhey D. F., Johnson J.: Pixelsynth: Generating a 3d-consistent experience from a single image. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14104-14113.\n' +
      '* [RFS22]Ruckert D., Franke L., Stamminger M.: Adop: Approximate differentiable one-pixel point rendering. _ACM Transactions on Graphics (ToG) 41_, 4 (2022), 1-14.\n' +
      '* [RK20]Riegler G., Koltun V.: Free view synthesis. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIX 16_ (2020), Springer, pp. 623-640.\n' +
      '* [RK21]Riegler G., Koltun V.: Stable view synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 12216-12225.\n' +
      '* [RKH*21a]Radford A., Kim J. W., Hallacy C., Ramesh A., Goh G., Agarwal S., Sastry G., Askell A., Mishkin P., Clark J., et al.: Learning transferable visual models from natural language supervision. In _ICML_ (2021).\n' +
      '* [RKH*21b]Radford A., Kim J. W., Hallacy C., Ramesh A., Goh G., Agarwal S., Sastry G., Askell A., Mishkin P., Clark J., et al.: Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_ (2021), PMLR, pp. 8748-8763.\n' +
      '* [RKP*23]Rai A., Kaza S., Poole B., Niemeyer M., Ruiz N., Milendaill B., Zada S., Aberman K., Rubinstein M., Barron J., et al.: Dreambooth3d: Subject-driven text-to-3d generation. _arXiv preprint arXiv:2303.13508_ (2023).\n' +
      '* [RPLG21]Reiser C., Peng S., Liao Y., Geiger A.: Kilonperf: Speeding up neural radiance fields with thousands of tiny mlps. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14335-14345.\n' +
      '* [RROG18]Roveri R., Rahmann L., Oztireli C., Gross M.: A network architecture for point cloud classification via automatic depth images generation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_ (2018), pp. 4176-4184.\n' +
      '* [RSH*21]Reizenstein J., Shapovalov R., Henzler P., Sbordone L., Labarut P., Novotny D.: Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 10901-10911.\n' +
      '* [RWC*19]Radford A., Wu J., Child R., Luan D., Amodei D., Sutskever I., et al.: Language models are unsupervised multitask learners. _Op health 1_, 8 (2019), 9.\n' +
      '* [RWL*22]Ruckert D., Wang Y., Li R., Idoughi R., Heidrich W.: Neat: Neural adaptive tomography. _ACM Transactions on Graphics (TOG) 41_, 4 (2022), 1-13.\n' +
      '* [SAA*23]Sidiqiqi Y., Alliegro A., Artemov A., Tommasi T., Sirigatti D., Rosov V., Dai A., Niessner M.: Meshpft: Generating triangle meshes with decoder-only transformers. _arXiv preprint arXiv:2311.15475_ (2023).\n' +
      '* [SCL*22]Sanghi A., Chu H., Lambourne J. G., Wang Y., Cheng C.-Y., Fumero M., Malekshan K. R.: Clip-forge: Towards zero-shot text-to-shape generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 18603-18613.\n' +
      '* [SCP*23]Shue J. R., Chan E. R., Po R., Ankner Z., Wu J., Wetzstein G.: 3d neural field generation using triplane diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 20875-20886.\n' +
      '* [SCS*22]Saharia C., Chan W., Saxena S., Li L., Whang J., Denton E. L., Ghasempotu K., Gontijo Lopes R., Karagoi, ayan B., Salihans T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems 35_ (2022), 36479-36494.\n' +
      '* [SCZ*23]Shi R., Chen H., Zhang Z., Liu M., Xu C., Wei X., Chen L., Zeng C., Su H.: Zero123++: a single image to consistent multi-view diffusion base model. _arXiv preprint arXiv:2310.15110_ (2023).\n' +
      '* [SDZ*21]Shinivasan P. P., Deng B., Zhang X., Tancik M., Mildenhall B., Barron J. T.: Nerv: Neural reflectance and visibility fields for relighting and view synthesis. In _Proceedings of the _IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 7495-7504.\n' +
      '* [SFL*23]Sanghi A., Fu R., Liu V., Willis K. D., Shayani H., Khasahmadi A. H., Sridhar S., Ritchie D.: Clip-sculptor: Zero-shot generation of high-fidelity and diverse shapes from natural language. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 18339-18348.\n' +
      '* [SGHS98]Shade J., Gortler S., He L.-w., Szeliski R.: Layered depth images. In _Proceedings of the 25th annual conference on Computer graphics and interactive techniques_ (1998), pp. 231-242.\n' +
      '* [SGY*21]Shen T., Gao J., Yin K., Liu M.-Y., Fidler S.: Deep marching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis. _Advances in Neural Information Processing Systems 34_ (2021), 6087-6101.\n' +
      '* [SHG*22]Shrestha R., Hu S., Gou M., Liu Z., Tan P.: A real world dataset for multi-view 3d reconstruction. In _European Conference on Computer Vision_ (2022), Springer, pp. 56-73.\n' +
      '* [SHN*19]Saito S., Huang Z., Natsume R., Morishima S., Kanazawa A., Li H.: Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In _Proceedings of the IEEE/CVF international conference on computer vision_ (2019), pp. 2304-2314.\n' +
      '* [SKJ23]Shim J., Kang C., Joo K.: Diffusion-based signed distance fields for 3d shape generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 20887-20897.\n' +
      '* [SLNG20]Schwarz K., Liao Y., Niemeyer M., Geiger A.: GRAF: Generative radiance fields for 3d-aware image synthesis. In _NeurIPS_ (2020).\n' +
      '* [SMKF04]Shilane P., Min P., Kazhdan M., Funkhouser T.: The princeton shape benchmark. In _Proceedings Shape Modeling Applications_, 2004. (2004), IEEE, pp. 167-178.\n' +
      '* [SMP*22]Sajhadi M. S., Meyer H., pot E., Bergmann U., Greff K., Radwan N., Vora S., Lucic M., Duckworth D., Dosovitskiy A., et al.: Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 6229-6238.\n' +
      '* [SPH*23]Singer U., Polyak A., Hayes T., Yin X., An J., Zhang S., Hu Q., Yang H., Ashual O., Gafni O., et al.: Make-a-video: Text-to-video generation without text-video data. In _Int. Conf. Learn. Represent._ (2023).\n' +
      '* [SPK19]Shu D. W., Park S. W., Kwon J.: 3d point cloud generative adversarial network based on tree structured graph convolutions. In _Proceedings of the IEEE/CVF international conference on computer vision_ (2019), pp. 3859-3868.\n' +
      '* [SPX*22]Shi Z., Peng S., Xu Y., Geiger A., Liao Y., Shen Y.: Deep generative models on 3d representations: A survey. _arXiv preprint arXiv:2210.15663_ (2022).\n' +
      '* [SSS7]Shirman L. A., Sequin C. H.: Local surface interpolation with beier patches. _Computer Aided Geometric Design 4_, 4 (1987), 279-295.\n' +
      '* [SSC22]Sun C., Sun M., Chen H.-T.: Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 5459-5469.\n' +
      '* [SSKH20]Shih M.-L., Su S.-Y., Kopf J., Huang J.-B.: 3d photography using context-aware layered depth inpainting. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp. 8028-8038.\n' +
      '* [SSN*14]Singh A., Sha J., Narayan K. S., Achim T., Abbeel P.: Bigbird: A large-scale 3d database of object instances. In _2014 IEEE international conference on robotics and automation (ICRA)_ (2014), IEEE, pp. 509-516.\n' +
      '* [SSN*22]Schwarz K., Sauer A., Niemeyer M., Liao Y., Geiger A.: Voxgraf: Fast 3d-aware image synthesis with sparse voxel grids. _Advances in Neural Information Processing Systems 35_ (2022), 33999-34011.\n' +
      '* [STB*19]Srinivasan P. P., Tucker R., Barron J. T., Ramamoorthi R., Ng R., Snavely N.: Pushing the boundaries of view extrapolation with multiplane images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2019), pp. 175-184.\n' +
      '* [SWL*20a]Sus Y., Wang Y., Liu Z., Siegel J., Sarma S.: Pointgrow: Autoregressively learned point cloud generation with self-attention. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_ (2020), pp. 61-70.\n' +
      '* [SWL*20b]Sun Y., Wang Y., Liu Z., Siegel J., Sarma S.: Pointgrow: Autoregressively learned point cloud generation with self-attention. In _WACV_ (2020).\n' +
      '* [SWS*22]Sun J., Wang X., Shi Y., Wang L., Wang J., Liu Y.: IDE-3D: Interactive disentangled editing for high-resolution 3d-aware portrait synthesis. _ACM Trans. Graph._ (2022).\n' +
      '* [SWW*23]Sun J., Wang X., Wang L., Li X., Zhang Y., Zhang H., Liu Y.: Next3D: Generative neural texture rasterization for 3d-aware head avstars. In _CVPR_ (2023).\n' +
      '* [SWY*23]Shi Y., Wang P., Ye J., Long M., Li K., Yang X.: Mvdream: Multi-view diffusion for 3d generation. _arXiv preprint arXiv:2308.16512_ (2023).\n' +
      '* [SWZ*18]Sun X., Wu J., Zhang X., Zhang Z., Zhang C., Xue T., Tenenbaum J. B., Freeman W. T.: Pix3d: Dataset and methods for single-image 3d shape modeling. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2018), pp. 2974-2983.\n' +
      '* [SWZ*22]Sun J., Wang X., Zhang Y., Li X., Zhang Q., Liu Y., Wang J.: Fenerf: Face editing in neural radiance fields. In _CVPR_ (2022).\n' +
      '* [SZS*23]Sun J., Zhang B., Shao R., Wang L., Liu W., Xie Z., Liu Y.: Dreamerfstd: Hierarchical 3d generation with bootstrapped diffusion prior. _arXiv preprint arXiv:2310.16818_ (2023).\n' +
      '* [TDB16]Tatarchenko M., Dosovitskiy A., Brox T.: Multi-view 3d models from single images with a convolutional network. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VII 14_ (2016), Springer, pp. 322-337.\n' +
      '* [TFT*20]Tewah A., Fried O., Thies J., Sitzmann V., Lombardi S., Sunkavalli K., Martin-Brualla R., Simon T., Saragih J., Niessner M., et al.: State of the art on neural rendering. In _Computer Graphics Forum_ (2020), vol. 39, Wiley Online Library, pp. 701-727.\n' +
      '* [TLK*23]Tseng H.-Y., Li Q., Kim C., Alsisan S., Huang J.-B., Kopf J.: Consistent view synthesis with pose-guided diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 16773-16783.\n' +
      '* [TLY*21]Takikawa T., Litalien J., Yin K., Kreis K., Loop C., Nowrouzezahrai D., Jacobson A., McGuire M., Fidler S.: Neural geometric level of detail: Real-time rendering with implicit 3d shapes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 11358-11367.\n' +
      '* [TLYCS22]Tseng W.-C., Liao H.-J., Yen-Chen L., Sun M.: CLA-NeRF: Category-level articulated neural radiance field. In _International Conference on Robotics and Automation (ICRA)_ (2022).\n' +
      '* [TME*22]Tremblay J., Meshry M., Evans A., Kautz J., Keller A., Khamis S., Muller T., Loop C., Morrica N., Nagano K., et al.: Ritru: A ray-traced multi-view synthetic dataset for novel view synthesis. _arXiv preprint arXiv:2205.07058_ (2022).\n' +
      '* [TRMT23]Truong P., Rakotosaona M.-J., Manhardt F., Tombari F.: Sparf: Neural radiance fields from sparse and noisy poses. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 4190-4200.\n' +
      '\n' +
      '* [TRZ*23]Tang J., Ren J., Zhou H., Liu Z., Zeng G.: Dream-gaussian: Generative gaussian splatting for efficient 3d content creation. _arXiv preprint arXiv:2309.16653_ (2023).\n' +
      '* [ITG*20]Tretschk E., Tewari A., Golyanik V., Zollhofer M., Stoll C., Theobalt C.: Patchlets: Patch-based generalizable deep implicit 3d shape representations. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVI 16_ (2020), Springer, pp. 293-309.\n' +
      '* [ITM*22]Tewari A., Thies J., Mildenhall B., Srinivasan P., Tretschk E., Yigan W., Lassner C., Sitzmann V., Martin-Ruulak R., Lombardi S., et al.: Advances in neural rendering. In _Computer Graphics Forum_ (2022), vol. 41, Wiley Online Library, pp. 703-735.\n' +
      '* [TWZ*23]Tang J., Wang T., Zhang B., Zhang T., Yi R., Ma L., Chen D.: Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior. _arXiv preprint arXiv:2303.14184_ (2023).\n' +
      '* [TYC*23]Tewari A., Yin T., Cazenavette G., Rezchikov S., Tenenbaum J. B., Durand F., Freeman W. T., Sitzmann V.: Diffusion with forward models: Solving stochastic inverse problems without direct supervision. _arXiv preprint arXiv:2306.11719_ (2023).\n' +
      '* [TZN19]Thies J., Zollhofer M., Niessner M.: Deferred neural rendering: Image synthesis using neural textures. _Acm Transactions on Graphics (TOG) 38_, 4 (2019).\n' +
      '* [VHM*22]Verbin D., Hedman P., Mildenhall B., Zickler T., Barron J. T., Srinivasan P. P.: Ref-nerf: Structured view-dependent appearance for neural radiance fields. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_ (2022), IEEE, pp. 5481-5490.\n' +
      '* [VN*51]Von Neumann J., et al.: The general and logical theory of automata. _1951_ (1951), 1-41.\n' +
      '* [WCH*22]Wang C., Chai M., He M., Chen D., Liao J.: CLIP-NeRF: Text-and-image driven manipulation of neural radiance fields. In _CVPR_ (2022).\n' +
      '* [WCMB*22]Watson D., Chan W., Martin-Bruulak R., Ho J., Tagliaschi A., Norouzi M.: Novel view synthesis with diffusion models. _arXiv preprint arXiv:2210.04628_ (2022).\n' +
      '* [WCS*22]Weng C.-Y., Curless B., Srinivasan P. P., Barron J. T., Kemelmacher-Shilzerman I.: Humannerf: Free-viewpoint rendering of moving people from monocular video. In _Proceedings of the IEEE/CVF conference on computer vision and pattern Recognition_ (2022), pp. 16210-16220.\n' +
      '* [WDL*23]Wang H., Du X., Li J., Yeh R. A., Shakhinarovich G.: Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 12619-12629.\n' +
      '* [WDY*22]Wu Y., Deng Y., Yang J., Wei F., Chen Q., Tong X.: AniFaceGAN: Animatchable 3d-aware face image generation for video avatars. In _NeurIPS_ (2022).\n' +
      '* [WGS21]Wiles O., Gkoxari G., Szeliski R., Johnson J.: Synsin: End-to-end view synthesis from a single image. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp. 7467-7477.\n' +
      '* [WJWC*23]Wang C., Jiang R., Chai M., He M., Chen D., Liao J.: NeRF-art: Text-driven neural radiance fields stylization. _IEEE Trans. Vis. Comput. Graph._ (2023).\n' +
      '* [WKC*23]Wang C., Kang D., Cao Y., Bao L., Shan Y., Zhang S.-H.: Neural point-based volumetric avatar: Surface-guided neural points for efficient and photorealistic volumetric head avatar. In _ACM SIGGRAPH Asia 2023 Conference Proceedings_ (2023).\n' +
      '* [WLC*22]Wu Q., Liu X., Chen Y., Li K., Zheng C., Cai J., Zheng J.: Object-compositional neural implicit surfaces. In _ECCV_ (2022).\n' +
      '* [WLG*23]Wang M., Liu Y.-S., Gao Y., Shi K., Fang Y., Han Z.: Lp-dfi. Learning local pattern-specific deep implicit function for 3d objects and scenes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 21856-21865.\n' +
      '* [WLL*21]Wang P., Liu L., Liu Y., Theobalt C., Komura T., Wang W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. _Advances in Neural Information Processing Systems 34_ (2021), 27171-27183.\n' +
      '* [WLW*23]Wang Z., Lu C., Wang Y., Bao F., Li C., Su H., Zhu J.: Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. _arXiv preprint arXiv:2305.16213_ (2023).\n' +
      '* [WLY*23]Wu T., Li Z., Yang S., Zhang P., Pan X., Wang J., Lin D., Liu Z.: Hyperdreamer: Hyper-realistic 3d content generation and editing from a single image. In _SIGGRAPH Asia 2023 Conference Papers_ (2023).\n' +
      '* [Wol83]Wolfram S.: Statistical mechanics of cellular automata. _Reviews of modern physics 55_, 3 (1983), 601.\n' +
      '* [WPH*23]Wan Z., Paschalidou D., Huang I., Liu H., Shen B., Xiang X., Liao J., Guibas L.: Cad: Photorealistic 3d generation via adversarial distillation. _arXiv preprint arXiv:2312.06663_ (2023).\n' +
      '* [WSK*15]Wu Z., Song S., Khosla A., Yu F., Zhang L., Tang X., Xiao J.: 3d shapenets: A deep representation for volumetric shapes. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2015), pp. 1912-1920.\n' +
      '* [WSW22]Wang Y., Skorokhodov I., Wonka P.: HF-NeuS: Improved surface reconstruction using high-frequency details. _Advances in Neural Information Processing Systems_ (2022).\n' +
      '* [WSW23]Wang Y., Skorokhodov I., Wonka P.: PET-NeuS: Positional encoding triplicates for neural surfaces. In _CVPR_ (2023).\n' +
      '* [WWG*21]Wang Q., Wang Z., Genova K., Srinivasan P. P., Zhou H., Barron J. T., Martin-Bruulak R., Snavely N., Funkhouser T.: IBRNet: Learning multi-view image-based rendering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 4690-4699.\n' +
      '* [WWL*23]Wu Q., Wang K., Li K., Zheng J., Cai J.: ObjectSDF++: Improved object-compositional neural implicit surfaces. In _ICCV_ (2013).\n' +
      '* [WWK*21]Wang Z., Wu S., Xie W., Chen M., Prisacariu V. A.: Nerf-: Neural radiance fields without known camera parameters. _arXiv preprint arXiv:2102.07064_ (2021).\n' +
      '* [WZ22]Wu R., Zheng C.: Learning to generate 3d shapes from a single example. _arXiv preprint arXiv:2208.02946_ (2022).\n' +
      '* [WZF*23]Wu T., Zhang J., Fu X., Wang Y., Ren J., Pan L., Wu, Yang L., Wang J., Qian C., et al.: Omnibject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 803-814.\n' +
      '* [WZX*16]Wu J., Zhang C., Xue T., Freeman B., Tenenbaum J.: Learning a probabilistic latent space of object shapes via 3d generative adversarial modeling. _Advances in neural information processing systems_ (2016).\n' +
      '* [WZZ*23]Wang T., Zhang B., Zhang T., Gu S., Bao J., Baltrusaitis T., Shen J., Chen D., Wen F., Chen Q., et al.: RODIN: A generative model for sculpting 3d digital avatars using diffusion. In _CVPR_ (2023).\n' +
      '* [JYW*23]Xu D., Jiang Y., Wang P., Fan Z., Wang Y., Wang Z.: Neurallift-360: Lifting an in-the-wild 2d photo to a 3d object with 360deg views. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 4479-4489.\n' +
      '* [KKJ*23]Xiong Z., Kang D., Jin D., Chen W., Bao L., Cui S., Han X.: Get3dhuman: Lifting stylegan-human into a 3d generative model using pixel-aligned reconstruction priors. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2023), pp. 9287-9297.\n' +
      '* [XJS\\({}^{*}\\)23]Xu H., Song G., Jiang Z., Zhang J., Shi Y., Liu J., Ma W., Feng J., Luo L.: OmniAvaatar: Geometry-guided controllable 3d head synthesis. In _CVPR_ (2023), 18, 19\n' +
      '* [XITL\\({}^{*}\\)23]Xu Y., Tan H., Luan F., Bi S., Wang P., Li J., Shi Z., Sunkavalli K., Weitzstein G., Xu Z., et al.: Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. _arXiv preprint arXiv:2311.09217_ (2023).\n' +
      '* [XTS\\({}^{*}\\)22]Xie Y., Takikawa T., Saito S., Litany O., Yan S., Khan N., Tombaari P., Tompkin J., Sitzmann V., Sridhar S.: Neural fields in visual computing and beyond. In _Computer Graphics Forum_ (2022), vol. 41, Wiley Online Library, pp. 641-676.\n' +
      '* [XWC\\({}^{*}\\)19]Xu Q., Wang W., Ceylan D., Mech R., Neumann U.: Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. _Advances in neural information processing systems 32_ (2019).\n' +
      '* [XX23]Xia W., Xue J.-H.: A survey on deep generative 3d-aware image synthesis. _ACM Computing Surveys 56_, 4 (2023), 1-34.\n' +
      '* [XXB\\({}^{*}\\)23]Xu Y., Yifan W., Bergman A. W., Chai M., Zhou B., Weitzstein G.: Efficient 3d articulated human generation with layered surface volumes. _arXiv preprint arXiv:2307.05462_ (2023).\n' +
      '* [XXC\\({}^{*}\\)23]Xiu Y., Yang J., Cao X., Tziomas D., Black M. J.: Eco: Explicit clothed humans optimized via normal integration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 512-523.\n' +
      '* [XYHT23]Xiang J., Yang J., Huang B., Tong X.: 3d-aware image generation using 2d diffusion models. _arXiv preprint arXiv:2303.17905_ (2023).\n' +
      '* [XYTB22]Xiu Y., Yang J., Tziomas D., Black M. J.: Icon: Implicit clothed humans obtained from normals. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_ (2022), IEEE, pp. 13286-13296.\n' +
      '* [YAK\\({}^{*}\\)20]Yifan W., Aigerman N., Kim V. G., Chaudhuri S., Sorkine-Hornung O.: Neural cages for detail-preserving 3d deformations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp. 75-83.\n' +
      '* [YBZ\\({}^{*}\\)22]Yang B., Bao C., Zeng J., Bao H., Zhang Y., Cui Z., Zhang G.: NeuMesh: Learning disentangled neural mesh-based implicit field for geometry and texture editing. In _ECCV_ (2022), Springer.\n' +
      '* [YGKL19]Yariv L., Gu J., Kasten Y., Lipman Y.: Volume rendering of neural implicit surfaces. _Advances in Neural Information Processing Systems 34_ (2021), 4805-4815.\n' +
      '* [YGMG23]Yoo P., Guo J., Matsuo Y., Gu S. S.: Dreamsparse: Escaping from plato\'s cave with 2d frozen diffusion model given sparse views. _CoRR_ (2023).\n' +
      '* [YHH\\({}^{*}\\)19a]Yang G., Huang X., Hao Z., Liu M.-Y., Belongie S., Hariharan B.: Pointflow: 3d point cloud generation with continuous normalizing flows. In _Proceedings of the IEEE/CVF international conference on computer vision_ (2019), pp. 4541-4550.\n' +
      '* [YHH\\({}^{*}\\)19b]Yang G., Huang X., Hao Z., Liu M.-Y., Belongie S., Hariharan B.: Pointflow: 3d point cloud generation with continuous normalizing flows. In _Proceedings of the IEEE/CVF international conference on computer vision_ (2019), pp. 4541-4550.\n' +
      '* [YLM\\({}^{*}\\)22]Yan X., Lin L., Mitra N. J., Lischinski D., Cohen-Or D., Huang H.: Shaperformer: Transformer-based shape completion via sparse representation. In _CVPR_ (2022).\n' +
      '* [YLWD22]Yang Z., Li S., Wu W., Dai B.: 3dhungan: Towards photo-realistic 3d-aware human image generation. _arXiv preprint arXiv:2212.07378_ (2022).\n' +
      '* [YLX\\({}^{*}\\)23]Yang X., Luo Y., Xiu Y., Wang W., Xu H., Fan Z.: D-if: Uncertainty-aware human digitization via implicit distribution field. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2023), pp. 9122-9132.\n' +
      '* [YPN\\({}^{*}\\)22]Yu Z., Peng S., Niemeyer M., Sattler T., Geiger A.: Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. _Advances in neural information processing systems 35_ (2022), 25018-25032.\n' +
      '* [YRSh21]Yifan W., Rahmann L., Sorkine-hornung O.: Geometry-consistent neural shape representation with implicit displacement fields. In _International Conference on Learning Representations_ (2021).\n' +
      '* [YSL\\({}^{*}\\)22]Yuan Y.-J., Sun Y.-T., Lai Y.-K., Ma Y., Jia R., Gao L.: NeRF-Editing: geometry editing of neural radiance fields. In _CVPR_ (2022).\n' +
      '* [YSW\\({}^{*}\\)19]Yifan W., Serena F., Wu S., Oztireli C., Sorkine-Hornung O.: Differentiable surface splitting for point-based geometry processing. _ACM Transactions on Graphics (TOG) 38_, 6 (2019), 1-14.\n' +
      '* [YTB\\({}^{*}\\)21]Yennamandra T., Tewari A., Bernard F., Seidel H.-P., Elgharib M., Cremers D., Theobalt C.: 13DMM: Deep implicit 3d morphable model of human heads. In _CVPR_ (2021).\n' +
      '* [YXZ\\({}^{*}\\)23]Yu X., Xu M., Zhang Y., Liu H., Ye C., Wu Y., Yan Z., Zhu C., Xiong Z., Liang T., et al.: Mvimgnet: A large-scale dataset of multi-view images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 9150-9161.\n' +
      '* [YYC\\({}^{*}\\)23]Yu W., Yuan L., Cao Y.-P., Gao X., Li X., Quan L., Shan Y., Tian Y.: Hifi-123: Towards high-fidelity one image to 3d content generation. _arXiv preprint arXiv:2310.06744_ (2023).\n' +
      '* [YYTK21]Yu A., Ye V., Tancik M., Kanazawa A.: pixelnerf: Neural radiance fields from one or few images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 4578-4587.\n' +
      '* [YZX\\({}^{*}\\)21]Yang B., Zhang Y., Xu Y., Li Y., Zhou H., Bao H., Zhang G., Cui Z.: Learning object-compositional neural radiance field for editable scene rendering. In _ICCV_ (2021).\n' +
      '* [ZAB\\({}^{*}\\)22]Zheng Y., Abrevara V. F., Bohler M. C., Chen X., Black M. J., Hilligos O.: IMavatar: Implicit morphable head avatars from videos. In _CVPR_ (2022).\n' +
      '* [ZBT23]Zielonka W., Bolkart T., Thies J.: Instant volumetric head avatars. In _CVPR_ (2023).\n' +
      '* [ZCY\\({}^{*}\\)23]Zhang H., Chen B., Yang H., Qu L., Wang X., Chen L., Long C., Zhu F., Du K., Zheng M.: Avatarverse: High-quality \\(\\&\\) stable 3d avatar creation from text and pose. _arXiv preprint arXiv:2308.03610_ (2023).\n' +
      '* [ZDW21]Zhou L., Du Y., Wu J.: 3d shape generation and completion through point-voxel diffusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 5826-5835.\n' +
      '* [ZJ16]Zhou Q., Jacobson A.: Thingi10k: A dataset of 10,000 3d-printing models. _arXiv preprint arXiv:1605.04797_ (2016).\n' +
      '* [ZJY\\({}^{*}\\)22]Zhang J., Jiang Z., Yang D., Xu H., Shi Y., Song G., Xu Z., Wang X., Feng J.: Avatargen: a 3d generative model for animatable human avatars. In _European Conference on Computer Vision_ (2022), Springer, pp. 668-685.\n' +
      '* [ZKB\\({}^{*}\\)22]Zhang K., Kolkin N., Bi S., Luan F., Xu Z., Shechtman E., Sawaley N.: ARF: Artistic radiance fields. In _ECCV_ (2022), Springer.\n' +
      '* [ZKW\\({}^{*}\\)23]Zhou A., Kim M. J., Wang L., Florence P., Finn C.: Nerf in the palm of your hand: Corrective augmentation for robotics via novel-view synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 17907-17917.\n' +
      '* [ZLC\\({}^{*}\\)23]Zhao Z., Liu W., Chen X., Zeng X., Wang R., Cheng P., Fu B., Chen T., Yu G., Gao S.: Michelangelo: Conditional 3dshape generation based on shape-image-text aligned latent representation. _arXiv preprint arXiv:2306.17115_ (2023).\n' +
      '* [ZLLD21]Zhi S., Laidlow T., Leutenegger S., Davison A. J.: In-place scene labelling and understanding with implicit scene representation. In _ICCV_ (2021), pp. 15838-15847.\n' +
      '* [ZLLS22]Zhang K., Luan F., Li Z., Snavely N.: Iron: Inverse rendering by optimizing neural sdfs and materials from photometric images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 5565-5574.\n' +
      '* [ZLW*21]Zhang K., Luan F., Wang Q., Bala K., Snavely N.: Physg: Inverse rendering with spherical gaussians for physics-based material editing and relighting. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 5453-5462.\n' +
      '* [ZLW*22]Zhang J., Li X., Wan Z., Wang C., Liao J.: Fotherl: Few-shot dynamic neural radiance fields for face reconstruction and expression editing. In _SIGGRAPH Asia 2022 Conference Papers_ (2022), pp. 1-9.\n' +
      '* [ZLW*23]Zhang J., Li X., Wan Z., Wang C., Liao J.: Text2nerf: Text-driven 3d scene generation with neural radiance fields. _arXiv preprint arXiv:2305.11588_ (2023).\n' +
      '* [ZLM72]Zheng X., Liu Y., Wang P., Tong X.: Sdf-stylegan: Implicit sdf-based stylegan for 3d shape generation. In _Computer Graphics Forum_ (2022), vol. 41, Wiley Online Library, pp. 52-63.\n' +
      '* [ZLZ+22]Zhu H., Liu Z., Zhou Y., Ma Z., Cao X.: DNF: Diffractive neural field for lensless microscopic imaging. _Optics Express 30_, 11 (2022), 18168-18178.\n' +
      '* [ZLZ+23]Zhang J., Li X., Zhang Q., Cao Y., Shan Y., Liao J.: Hummer: Single image to 3d human generation via reference-guided diffusion. _arXiv preprint arXiv:2311.16961_ (2023).\n' +
      '* [ZML*22]Zhou J., Ma B., Liu Y.-S., Fang Y., Han Z.: Learning consistency-aware unsigned distance functions progressively from raw point clouds. _Advances in Neural Information Processing Systems 35_ (2022), 16481-16494.\n' +
      '* [ZPL*22]Zhu Z., Peng S., Larsson V., Xu W., Bao H., Cui Z., Oswald M. R., Pollefeys M.: Nice-slam: Neural implicit scalable encoding for slam. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 12786-12796.\n' +
      '* [ZPVBG02]Zwicker M., Pfister H., Van Baar J., Gross M.: Ewa splatting. _IEEE Transactions on Visualization and Computer Graphics_ 8, 3 (2002), 223-238.\n' +
      '* [ZPW*23]Zheng X.-Y., Pan H., Wang P.-S., Tong X., Liu Y., Shu H.-Y.: Locally attentional sdf diffusion for controllable 3d shape generation. _arXiv preprint arXiv:2305.04461_ (2023).\n' +
      '* [ZQL*23]Zhang L., Qiu Q., Lin H., Zhang Q., Shi C., Yang W., Shi Y., Yang S., Xu L., Yu J.: DreamFace: Progressive generation of animatable 3d faces under text guidance. _ACM Trans. Graph._ (2023).\n' +
      '* [ZSD*21]Zhang X., Srinivasan P. P., Deng B., Debevec P., Freeman W. T., Barron J. T.: Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. _ACM Transactions on Graphics (ToG) 40_, 6 (2021), 1-18.\n' +
      '* [ZSH*22]Zhang Y., Sun J., He X., Fu H., Jia R., Zhou X.: Modeling indirect illumination for inverse rendering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 18643-18652.\n' +
      '* [ZST08]Zhang W., Sun J., Tang X.: Cat head detection-how to effectively exploit shape and texture features. In _Computer Vision-ECCV\'2008: 10th European Conference on Computer Vision, Marseille, France, October 12-18, 2008, Proceedings, Part IV 10_ (2008), Springer, pp. 802-816.\n' +
      '* [ZTF*18]Zhou T., Tucker R., Flynn J., Fyffe G., Snavely N.: Stereo magnification: Learning view synthesis using multiplane images. _arXiv preprint arXiv:1805.09817_ (2018).\n' +
      '* [ZTNW23]Zhang B., Tang J., Niessner M., Wonka P.: 3dshape2vecset: A 3d shape representation for neural fields and generative diffusion models. _arXiv preprint arXiv:2301.11445_ (2023).\n' +
      '* [ZTS*16]Zhou T., Tulsiani S., Sun W., Malik J., Efros A. A.: View synthesis by appearance flow. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_ (2016), Springer, pp. 286-301.\n' +
      '* [ZVW*22]Zeng X., Vahdat A., Williams F., Gojcic Z., Litany, O., Fidler S., Kreis K.: Lion: Latent point diffusion models for 3d shape generation. _arXiv preprint arXiv:2210.06978_ (2022).\n' +
      '* [ZWL*23]Zhuang J., Wang C., Liu L., Lin L., Li G.: Dreameditor: Text-driven 3d scene editing with neural fields. In _SIGGRAPH Asia Conference Papers_ (2023).\n' +
      '* [ZXA*23]Zhu C., Xiao F., Alvarado A., Babaei Y., Hu J., El-Mohri H., Culatana S., Sumbaly R., Yan Z.: Egoobjects: A large-scale egocentric dataset for fine-grained object understanding. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2023), pp. 20110-20120.\n' +
      '* [ZYHC22]Zheng M., Yang H., Huang D., Chen L.: ImFace: A nonlinear 3d morphable face model with implicit neural representations. In _CVPR_ (2022).\n' +
      '* [ZYLD21]Zheng Z., Yu T., Liu Y., Dai Q.: Pamir: Parametric model-conditioned implicit representation for image-based human reconstruction. _IEEE transactions on pattern analysis and machine intelligence 44_, 6 (2021), 3170-3184.\n' +
      '* [ZYW*23]Zheng Y., Yifan W., Wetzstein G., Black M. J., Hilliges O.: PointAvatar: Deformable point-based head avatars from videos. In _CVPR_ (2023).\n' +
      '* [ZZZ3]Zhu J., Zhuang P.: Hifa: High-fidelity text-to-3d with advanced diffusion guidance. _arXiv preprint arXiv:2305.18766_ (2023).\n' +
      '* [ZZF*23]Zhuang Y., Zhang Q., Feng Y., Zhu H., Yao Y., Li X., Cao Y.-P., Shan Y., Cao X.: Anti-aliased neural implicit surfaces with encoding level of detail. _arXiv preprint arXiv:2309.10336_ (2023).\n' +
      '* [ZZK*20]Zamorski M., Zieba M., Klukowski P., Nowak R., Kurach K., Stokowiec W., Trzcinski T.: Adversarial autoencoders for compact representations of 3d point clouds. _Computer Vision and Image Understanding 193_ (2020), 102921.\n' +
      '* [ZZW*23]Zhuang Y., Zhang Q., Wang X., Zhu H., Feng Y., Li X., Shan Y., Cao X.: Neai: A pre-convoluted representation for plug-and-play neural ambient illumination. _arXiv preprint arXiv:2304.08757_ (2023).\n' +
      '* [ZZZ*18]Zhu J.-Y., Zhang Z., Zhang C., Wu J., Torralba A., Tenebraum J., Freeman B.: Visual object networks: Image generation with disentangled 3d representations. _Advances in neural information processing systems 31_ (2018).\n' +
      '* [ZZZ*23a]Zhang J., Zhang X., Zhang H., Liew J. H., Zhang C., Yang Y., Feng J.: Avatarstudio: High-fidelity and animatable 3d avatar creation from text. _arXiv preprint arXiv:2311.17917_ (2023).\n' +
      '* [ZZZ*23b]Zhu J., Zhu H., Zhang Q., Zhu F., Ma Z., Cao X.: Pyramid nerf: Frequency guided fast radiance field optimization. _International Journal of Computer Vision_ (2023), 1-16.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>