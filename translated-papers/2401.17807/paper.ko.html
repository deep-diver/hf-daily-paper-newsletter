<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 3D 세대의 발전: 설문조사\n' +
      '\n' +
      'Xiaoyu Li1\n' +
      '\n' +
      'Qi Zhang1\n' +
      '\n' +
      'Di Kang1\n' +
      '\n' +
      'Weihao Cheng2\n' +
      '\n' +
      'Yiming Gao2\n' +
      '\n' +
      'Jingbo Zhang3\n' +
      '\n' +
      'Zhihao Liang4\n' +
      '\n' +
      'Jing Liao3\n' +
      '\n' +
      'Yan-Pei Cao1,2\n' +
      '\n' +
      'Ying Shan1,2\n' +
      '\n' +
      '홍콩 4남중국공대 텐센트 PCG 3시티대학 1텐센트 AI Lab 2ARC Lab\n' +
      '\n' +
      '*동일 기여도. \\ ({}^{\\dagger}\\) 교신저자.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '3D 모델을 생성하는 것은 컴퓨터 그래픽의 핵심에 있으며 수십 년간의 연구의 초점이 되어 왔다. 고급 신경 표현 및 생성 모델의 출현으로 3D 콘텐츠 생성 분야가 빠르게 발전하고 있어 점점 더 고품질이고 다양한 3D 모델의 생성이 가능하게 되었다. 이 분야의 급속한 성장은 최근의 모든 발전과 보조를 맞추기 어렵게 만든다. 본 연구에서는 3D 생성 방법의 기본 방법론을 소개하고 3D 표현, 생성 방법, 데이터 세트 및 해당 응용 프로그램을 포함하는 구조화된 로드맵을 수립하는 것을 목표로 한다. 구체적으로, 3D 생성을 위한 백본 역할을 하는 3D 표현을 소개한다. 또한, 피드포워드 생성, 최적화 기반 생성, 절차 생성 및 생성 신규 뷰 합성을 포함하여 알고리즘 패러다임의 유형에 따라 분류되는 생성 방법에 대한 빠르게 성장하는 문헌에 대한 포괄적인 개요를 제공한다. 마지막으로 사용 가능한 데이터 세트, 애플리케이션 및 오픈 챌린지에 대해 논의합니다. 이 설문 조사를 통해 독자들이 이 흥미로운 주제를 탐색하고 3D 콘텐츠 생성 분야의 추가 발전을 촉진하는 데 도움이 되기를 바랍니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '알고리즘을 사용하여 3D 모델을 자동으로 생성하는 것은 오랫동안 컴퓨터 비전 및 그래픽에서 중요한 작업이었다. 이 작업은 일반적으로 풍부한 3D 자산을 필요로 하는 비디오 게임, 영화, 가상 캐릭터 및 몰입형 경험에 대한 광범위한 응용으로 인해 상당한 관심을 받았다. 최근 신경 표현, 특히 신경 복사 필드(NeRF)[17, 20, 21, 22], 확산 모델[14, 23]과 같은 생성 모델의 성공은 3D 콘텐츠 생성에서 현저한 발전을 가져왔다.\n' +
      '\n' +
      '2D 콘텐츠 생성의 영역에서 최근 생성 모델의 발전은 이미지 생성 및 편집의 능력을 꾸준히 향상시켜 점점 더 다양하고 고품질의 결과를 가져왔다. 생성적 적대 네트워크(GANs) [GPAM\\({}^{*}\\)14, AQW19], 변량 자동 인코더(VAEs) [KPHL17, PGH\\({}^{*}\\)16, KW13], 자기 회귀 모델 [RWC*19, BMR*20]에 대한 선구적인 연구가 인상적인 결과를 보여주었다. 또한, 생성 인공지능(AI)과 확산 모델[HJA20, ND21, SCS\\({}^{*}\\)22]의 출현은 안정적인 확산[RBL*22a], Imagen[SCS\\({}^{*}\\)22], Midjourney[Mid], DALL-E3[Ope]와 같은 이미지 조작 기술의 패러다임 변화를 의미한다. 이러한 생성 AI 모델은 텍스트 프롬프트와 같은 최소한의 입력을 사용하여 사실적 또는 양식화된 이미지, 또는 비디오 [CZC\\({}^{*}\\)24, HSG\\({}^{*}\\)22, SPH\\({}^{*}\\)23, GNL\\({}^{*}\\)23]의 생성 및 편집을 가능하게 한다. 그 결과 그들은 창의성과 예술적 표현의 한계를 밀어붙이면서 현실 세계의 경계를 초월한 상상적 콘텐츠를 생성하는 경우가 많다. 이 모델들은 "신흥" 능력으로 인해 콘텐츠 생성에서 달성할 수 있는 것의 한계를 재정의하여 창의성과 예술적 표현의 지평을 확장했다.\n' +
      '\n' +
      '2D 콘텐츠 생성을 3D 공간으로 확장하려는 요구는 특히 메타버스의 급속한 발전과 함께 3D 자산을 생성하거나 몰입형 경험을 생성하는 애플리케이션에 점점 더 중요해지고 있다. 그러나 2D에서 3D 콘텐츠 생성으로의 전환은 단순한 기술적 진화가 아니다. 그것은 주로 2D 표현이 종종 제공하지 못하는 물리적 세계의 보다 복잡한 복제를 필요로 하는 현대 응용 프로그램의 요구에 대한 응답이다. 이러한 변화는 공간 관계와 깊이 인식에 대한 포괄적인 이해가 필요한 애플리케이션에서 2D 콘텐츠의 한계를 강조한다.\n' +
      '\n' +
      '3D 콘텐츠의 중요성이 점점 분명해지면서 이 영역에 전념하는 연구 노력이 급증했다. 그러나, 2D에서 3D 콘텐츠 생성으로의 전환은 기존의 2D 방법론의 직접적인 확장은 아니다. 대신, 3D 공간의 복잡성을 효과적으로 해결하기 위해 고유한 문제를 해결하고 데이터 표현, 공식화 및 기본 생성 모델을 재평가하는 것을 포함한다. 예를 들어, 3D 생성에 요구되는 바와 같이, 더 높은 차원을 다루기 위해 3D 장면 표현들을 2D 생성 모델들에 통합하는 방법은 명백하지 않다. 웹에서 쉽게 수집할 수 있는 이미지나 동영상과 달리 3D 자산은 상대적으로 부족하다. 또한, 생성된 3D 모델의 품질을 평가하는 것은 특히 3D 공간에서의 다시점 일관성을 고려할 때 목적 함수를 위한 더 나은 공식을 개발하는 것이 필요하기 때문에 추가적인 과제를 제시한다. 이러한 복잡성은 2D와 3D 콘텐츠 생성 사이의 격차를 해소하기 위한 혁신적인 접근법과 새로운 솔루션을 요구한다.\n' +
      '\n' +
      '2D 상대방만큼 두드러지게 등장하지는 않지만, 3D 콘텐츠 생성은 일련의 주목할 만한 성과와 함께 꾸준히 진행되고 있다. 도 1에 나타낸 대표적인 예이다. 1은 3D-GAN[WZX\\({}^{*}\\)16]과 같은 초기 방법에서 Instant3D[LTZ\\({}^{*}\\)23]과 같은 최근 접근법으로 전환하면서 품질과 다양성 모두에서 상당한 개선을 보여준다. 이에 본 연구는 3D 콘텐츠 제작의 급속한 발전과 다각적인 발전을 체계적으로 탐색하고자 한다. 본 논문에서는 3D 콘텐츠 생성의 3D 표현, 3D 생성 방법, 데이터 세트 및 응용 프로그램에 초점을 맞춘 최근의 많은 작업들에 대한 구조화된 개요와 포괄적인 로드맵을 제시하고, 오픈 챌린지의 개요를 제시한다.\n' +
      '\n' +
      '도. 2는 이 조사의 개요를 제시한다. 먼저 2절에서 이 조사의 범위와 관련 작업에 대해 논의하고, 다음 절에서는 3D 콘텐츠 생성의 기초를 형성하는 핵심 방법론을 살펴본다. Sec. 도 3은 3D 콘텐츠 생성에 사용되는 주요 장면 표현들 및 그들의 대응하는 렌더링 기능들을 소개한다. Sec. 4는 매우 다양한 3D 생성 방법을 탐색하는데, 이 방법은 피드포워드 생성, 최적화 기반 생성, 절차적 생성, 생성적 신규 뷰 합성의 알고리즘 방법론을 기반으로 네 가지 범주로 나눌 수 있다. 이러한 방법론의 진화 트리도 주요 가지를 설명하기 위해 묘사된다. 데이터 축적은 딥러닝 모델의 성공을 보장하는 데 중요한 역할을 하기 때문에, 우리는 3D 생성 방법을 훈련하기 위해 사용되는 관련 데이터 세트를 제시한다. 마지막으로, 3D 인간 및 얼굴 생성과 같은 관련 응용 프로그램에 대한 간략한 논의, 개방형 도전의 개요 및 이 설문 조사를 마무리한다. 우리는 이 조사가 관심 있는 독자들에게 후속 작업에 영감을 줄 수 있는 3D 세대에 대한 체계적인 요약을 제공하기를 바란다.\n' +
      '\n' +
      '이 작업에서는 크게 두 가지 기여와 함께 3D 세대에 대한 포괄적인 조사를 제시한다.\n' +
      '\n' +
      '* 최근 3D 비전 분야의 생성 모델을 기반으로 한 기여도가 급증함에 따라 3D 콘텐츠 생성에 대한 종합적이고 시기적절한 문헌 검토를 제공하여 독자들에게 3D 생성 프레임워크와 그 기본 원칙에 대한 신속한 이해를 제공하고자 한다.\n' +
      '* 특정 도메인에서 3D 콘텐츠 생성을 연구하는 연구자들이 관련 작업을 신속하게 식별하고 관련 기술에 대한 더 나은 이해를 돕기 위해 3D 생성 방법의 다중 관점 범주화를 제안한다.\n' +
      '\n' +
      '##2본 조사의 범위\n' +
      '\n' +
      '본 연구에서는 3차원 모델의 생성과 관련 데이터 세트 및 응용 분야에 초점을 맞춘다. 구체적으로 먼저 장면 재현에 대한 간략한 소개를 한다. 그런 다음 우리의 초점은 이러한 표현과 생성 모델의 통합으로 이동한다. 그런 다음 생성 방법의 두드러진 방법론에 대한 포괄적인 개요를 제공한다. 또한 3D 인간 생성, 3D 얼굴 생성 및 3D 편집과 같은 관련 데이터 세트와 최첨단 응용 프로그램을 탐색하며, 이 모든 기술은 이러한 기술에 의해 향상된다.\n' +
      '\n' +
      '이 조사는 관련 데이터 세트 및 응용 프로그램과 함께 3D 생성 방법을 체계적으로 요약하고 분류하는 데 전념한다. 조사된 논문들은 대부분 주요 컴퓨터 비전 및 컴퓨터 그래픽 컨퍼런스/저널과 2023년 ArXiv에서 발표된 일부 프리프린트에 게재되어 있으며, 3D 세대와 관련된 모든 방법을 소진하기는 어렵지만, 가능한 한 많은 3D 세대의 주요 분야가 포함되기를 바란다. 우리는 각 지점에 대한 자세한 설명을 조사하지 않고 일반적으로 그 안에 몇 가지 대표적인 작품을 도입하여 패러다임을 설명한다. 각 지점의 세부 사항은 이들 인용 논문의 관련 업무 부분에서 확인할 수 있다.\n' +
      '\n' +
      '**관련 조사.** 신경 재구성 및 장면 표현을 사용한 렌더링은 3D 생성과 밀접한 관련이 있다. 그러나 이러한 주제는 이 보고서의 범위를 벗어난 것으로 간주한다. 신경 렌더링에 대한 포괄적인 논의를 위해 독자를 [17], [18]로 안내하고 다른 신경 표현에 대한 광범위한 검토를 위해 [19, 20]을 권장한다. 우리의 주요 초점은 3D 모델을 생성하는 기술을 탐색하는 것입니다. 따라서 본 리뷰는 비주얼 컴퓨팅의 영역 내에서 2D 이미지의 생성 방법에 대한 연구를 포함하지 않는다. 특정 생성 방법에 대한 추가 정보를 위해, 독자는 보다 상세한 이해를 위해 [16] (VAEs), [14] (GANs), [21] (GANs), [22] (Diffusion) 및 [15] (Transformers)를 참조할 수 있다. 또한 3D 포인트 클라우드에 대한 3D 인식 이미지 합성[23], 3D 생성 모델[24], Text-to-3D[16] 및 딥 러닝(deep learning)과 같은 자체 초점을 갖는 3D 생성과 관련된 설문 조사도 있다. 이 조사에서는 다양한 3D 생성 방법에 대한 포괄적인 분석을 제공한다.\n' +
      '\n' +
      '##3 신경 장면 표현\n' +
      '\n' +
      '3D AI 생성 콘텐츠의 영역에서는 3D 모델의 적절한 표현을 채택하는 것이 필수적이다. 생성 프로세스는 전형적으로 3D 모델들을 생성하고 2D 이미지들을 렌더링하기 위한 장면 표현 및 미분가능한 렌더링 알고리즘을 포함한다. 반대로, 생성된 3D 모델들 또는 2D 이미지들은 도 3에 예시된 바와 같이 재구성 도메인 또는 이미지 도메인에서 감독될 수 있다. 일부 방법들은 장면 표현의 3D 모델들을 직접 감독하는 반면, 다른 방법들은 장면 표현을 이미지들로 렌더링하고 결과적인 렌더링들을 감독한다. 이하에서는 장면 표현을 명시적 장면 표현(섹션 3.1), 암시적 표현(섹션 3.2), 하이브리드 표현(섹션 3.3)의 세 그룹으로 크게 분류한다. 다양한 입력들로부터 장면 표현들을 최적화하기 위해 미분가능해야 하는 렌더링 방법들(_e.g._ ray casting, volume rendering, rasterization, _etc_)이 또한 도입된다는 점에 유의한다.\n' +
      '\n' +
      '### Explicit Representations\n' +
      '\n' +
      '명시적 장면 표현은 3D 장면을 묘사하는 포괄적인 수단을 제공하기 때문에 컴퓨터 그래픽과 비전에서 기본 모듈 역할을 한다. 점형 프리미티브, 삼각형 기반 메시 및 고급 파라메트릭 표면을 포함하는 기본 프리미티브의 어셈블리로서 장면을 묘사함으로써, 이러한 표현들은 다양한 환경들 및 객체들의 상세하고 정확한 시각화를 생성할 수 있다.\n' +
      '\n' +
      '###### 3.1.1 포인트 클라우드\n' +
      '\n' +
      '점 구름은 3차원 공간에서 덧셈 속성(_예: 색상 및 정규)을 갖는 이산점을 나타내는 유클리드 공간의 원소 집합이다. 무한히 작은 표면 패치로 간주될 수 있는 단순한 점 외에도 반지름(서펠)을 갖는 배향된 점 구름도 사용될 수 있다[23]. 서펠은 렌더링용 컴퓨터 그래픽에 사용됩니다.\n' +
      '\n' +
      '그림 2: 3D 표현, 3D 생성 방법, 데이터 세트 및 응용 프로그램을 포함한 이 조사의 개요. 구체적으로, 3D 생성을 위한 백본 역할을 하는 3D 표현을 소개한다. 또한, 피드포워드 생성, 최적화 기반 생성, 절차 생성 및 생성 신규 뷰 합성을 포함하여 알고리즘 패러다임의 유형에 따라 분류되는 생성 방법에 대한 빠르게 성장하는 문헌에 대한 포괄적인 개요를 제공한다. 마지막으로 인기 있는 데이터 세트와 사용 가능한 애플리케이션에 대한 간략한 토론을 제공합니다.\n' +
      '\n' +
      '점 구름(분할이라고 함)은 미분 가능한 [19, 17]이며 연구자가 반지름 또는 색상과 같은 점 구름 위치 및 특징을 조정하기 위해 미분 가능한 렌더링 파이프라인을 정의할 수 있게 한다. 신경점 기반 렌더링[17, 18, 19], SynSin[20], Pulsar[16, 15] 및 ADOP[14]와 같은 기술은 학습 가능한 특징을 활용하여 표면 모양과 모양에 대한 정보를 저장하여 보다 정확하고 상세한 렌더링 결과를 가능하게 한다. FVS[13], SVS[14], 및 FWD-트랜스포머[15]와 같은 몇몇 다른 방법들 또한 렌더링 품질을 향상시키기 위해 학습가능한 특징들을 채용한다. 이러한 방법들은 전형적으로 포인트 클라우드들에 특징들을 내장하고 컬러 값들을 디코딩하기 위해 타겟 뷰들로 워핑하여, 장면의 보다 정확하고 상세한 재구성을 허용한다.\n' +
      '\n' +
      '점 구름 기반 미분 가능한 렌더러를 3D 생성 프로세스에 통합함으로써 연구자들은 기울기 기반 최적화 기술과의 호환성을 유지하면서 점 구름의 이점을 활용할 수 있다. 이 프로세스는 일반적으로 두 가지 다른 방식으로 분류될 수 있는데, 이산 샘플을 일부 국부 결정론적 블러링 커널[16, 17, 18]과 기존의 포인트 렌더러[17, 18, 19]와 혼합하는 포인트 분할이다. 이러한 방법은 3D 생성 작업에서 신경망을 훈련하고 최적화하는 데 필수적인 미분 가능성을 유지하면서 3D 포인트 클라우드 모델의 생성 및 조작을 용이하게 한다.\n' +
      '\n' +
      '#### 3.1.2 Meshes\n' +
      '\n' +
      '여러 꼭짓점을 에지와 연결함으로써, 보다 복잡한 기하학적 구조들(_예를 들어, 와이어 프레임들 및 메쉬들)이 형성될 수 있다[19]. 그런 다음, 이러한 구조들은 객체들의 사실적 표현들을 생성하기 위해 다각형들, 전형적으로 삼각형들 또는 사각형들을 사용함으로써 더욱 정제될 수 있다[16]. 메쉬는 복잡한 모양과 구조를 컴퓨터 알고리즘에 의해 쉽게 조작하고 렌더링할 수 있기 때문에 다양하고 효율적인 표현 수단을 제공한다. 대부분의 그래픽 편집 도구 체인은 삼각형 메쉬를 사용합니다. 이러한 유형의 표현은 광범위한 수용과 호환성을 고려할 때 모든 디지털 콘텐츠 생성(DCC) 파이프라인에 필수적이다. 이러한 파이프라인들과 매끄럽게 정렬하기 위해, 신경망들은 이산 정점 위치들을 예측하도록 전략적으로 훈련될 수 있다[16, 17]. 이러한 기능을 통해 이러한 위치를 모든 DCC 파이프라인으로 직접 수입할 수 있어 원활한 작업이 가능하다.\n' +
      '\n' +
      '그림 3: 명시적, 암시적 및 하이브리드 표현을 포함하는 3D 생성에 사용되는 신경 장면 표현. 3D 생성은 3D 모델들을 생성하거나 2D 이미지들을 렌더링하기 위해 장면 표현들 및 미분가능한 렌더링 알고리즘의 사용을 포함한다. 편면에서, 이러한 3D 모델들 또는 2D 이미지들은 장면 표현들의 3D 생성을 감독하면서 재구성 도메인 또는 이미지 도메인으로서 기능할 수 있다.\n' +
      '\n' +
      '및 효율적인 워크플로우. 이산 텍스처를 예측하는 것과는 대조적으로, 텍스처 필드 [19] 및 NeRF-Tex [20]과 같이 신경망을 통해 최적화된 연속 텍스처 방법이 제안된다. 이를 통해 보다 정교하고 세밀한 텍스처를 제공하여 생성된 2D 모델의 전반적인 품질과 사실성을 높일 수 있다.\n' +
      '\n' +
      '메쉬 표현을 3D 생성에 통합하는 것은 메쉬 기반 미분 가능한 렌더링 방법의 사용을 필요로 하며, 이는 메쉬가 그래디언트 기반 최적화와 호환되는 방식으로 래스터화될 수 있게 한다. OpenDR[1], 신경 메시 렌더러[18], 파파라치[19], 및 소프트 래스터라이저[19]를 포함하는 몇몇 그러한 기술들이 제안되었다. 추가적으로, Mitsuba[2] 및 Taichi[18]와 같은 범용 물리 기반 렌더러는 자동 분화를 통해 메시 기반 미분 가능 렌더링을 지원한다.\n' +
      '\n' +
      '###### 3.1.3 다층 표현\n' +
      '\n' +
      '장면들을 표현하기 위한 다수의 반투명 컬러 층들의 사용은 실시간 신규 뷰 합성에서 대중적이고 성공적인 계획이었다[21]. LDI(Layered Depth Image) 표현을 사용하는 [14]는 주목할 만한 예이며, 각각의 연관된 컬러 값들을 갖는 깊이 맵들의 다수의 층들을 통합함으로써 전통적인 깊이 맵들을 확장한다. 여러 방법[22, 19, 20]은 LDI 표현에서 영감을 얻었고 딥 러닝 발전을 사용하여 LDI를 예측할 수 있는 네트워크를 생성했다. LDI 외에도 Stereomagnification[21]은 처음에 다중 이미지(MPI) 표현을 도입하였다. 평면 스윕 볼륨을 통해 고정된 깊이 범위에서 색상 및 불투명도를 포함하는 다수의 전방-병렬 반투명 레이어를 사용하는 장면을 설명한다. 볼륨 렌더링과 호모그래피 투영의 도움으로 새로운 뷰를 실시간으로 합성할 수 있었다. Stereomagnification[21]을 기반으로, 다양한 방법들[22, 23, 24, 25]은 렌더링 품질을 향상시키기 위해 MPI 표현을 채택하였다. 다층 표현은 평면을 구로 대체함으로써 [20], [23, 24]에서 더 넓은 시야를 수용하도록 더욱 확장되었다. 이 영역에 대한 연구가 계속 발전함에 따라 이러한 방법의 추가 발전을 기대할 수 있으며, 이는 실시간 렌더링을 위한 보다 효율적이고 효과적인 3D 생성 기술로 이어진다.\n' +
      '\n' +
      '### Implicit Representations\n' +
      '\n' +
      '암시적 표현은 컴퓨터 그래픽 및 비전에 걸친 많은 다른 애플리케이션들뿐만 아니라 뷰 합성 또는 형상 재구성에서의 문제들에 대한 선택의 장면 표현이 되었다. 일반적으로 객체 표면에 초점을 맞추는 명시적 장면 표현과 달리 암시적 표현은 3D 객체의 전체 볼륨을 정의하고 이미지 합성을 위해 볼륨 렌더링을 사용할 수 있다. 이러한 표현들은 3D 공간의 속성들을 기술하기 위해 래디언스 필드들[21] 또는 서명된 거리 필드들[23, 24]과 같은 수학적 함수들을 이용한다.\n' +
      '\n' +
      '###### 3.2.1 신경 복사 필드\n' +
      '\n' +
      'Neural Radiance Fields (NeRFs) [21]은 광범위한 응용 분야에서 선호되는 장면 표현 방법으로 주목을 받고 있다. 근본적으로, NeRF는 3D 장면 또는 기하학의 새로운 표현을 소개한다. NeRF는 점 구름과 메쉬를 활용하기보다는 장면을 연속적인 볼륨으로 묘사한다. 이 접근법은 암시적 신경망을 질의함으로써 뷰-의존적 광도 및 체적 밀도와 같은 체적 파라미터들을 획득하는 것을 포함한다. 이 혁신적인 표현은 3D 장면의 복잡성을 포착하는 더 유동적이고 적응 가능한 방법을 제공하여 향상된 렌더링 및 모델링 기술의 길을 열어준다.\n' +
      '\n' +
      '특히, NeRF[21]은 MLP를 이용하여 위치\\(\\mathbf{x}\\) 및 뷰 방향\\(\\mathbf{r}\\)을 밀도\\(\\mathbf{\\sigma}\\) 및 색상\\(\\mathbf{c}\\)에 매핑하는 연속적인 체적 복사 필드를 갖는 장면을 나타낸다. NeRF는 픽셀의 색상을 렌더링하기 위해 단일 광선\\(\\mathbf{r}(t)=\\mathbf{\\phi}+t\\mathbf{d}\\)을 캐스팅하고 광선을 따라 일련의 점\\(\\{t_{i}\\}\\}\\)을 평가한다. 샘플링된 포인트들에서 평가된 \\(\\mathbf{\\sigma}_{i},\\mathbf{c}_{i})\\}\\)은 볼륨 렌더링을 통해 픽셀의 컬러 \\(C(\\mathbf{r})\\)에 축적된다:\n' +
      '\n' +
      '\\[C(r)=\\sum_{i}T_{i}\\alpha_{i}\\mathbf{c}_{i},\\quad\\text{여기서 }T_{i}=\\exp\\left(- \\sum_{k=0}^{i-1}\\sigma_{k}\\delta_{k}\\right), \\tag{1}\\text{여기서 }T_{i}=\\exp\\left(- \\sum_{k=0}^{i-1}\\sigma_{k}\\delta_{k}\\right)\n' +
      '\n' +
      '그리고 \\(\\alpha_{i}=1-\\exp(-\\sigma_{i}\\delta_{k})\\)는 샘플링된 지점의 불투명도를 나타낸다. 누적투과율 \\(T_{i}\\)은 다른 입자와 만나지 않고 \\(t_{0}\\)에서 \\(t_{i}\\)으로 진행하는 광선의 확률을 정량화하고 \\(\\delta_{i}=t_{i}-t_{i-1}\\)은 인접한 샘플 사이의 거리를 나타낸다.\n' +
      '\n' +
      'NeRFs[21, 22, 23, 24, 21], BMV*22, VHM*22, LWC*23]는 에디션 [21, ZLLD21, CZL*22, YSL*22], 카메라의 조인트 최적화 [23, WKN*21, CCW*23, TRMT23], 역 렌더링 [21, SDZ*21, BB*21, ZSD*21, ZZW*23, LZF*23], 일반화 [22, WWG*21, CXZ*21, LFS*21, JLF22, HZF*23b], 가속 [21, GK+21, ZZZZ*23b], 자유 시점 비디오 [21, LSZ*22, PCPMMN21]과 같은 문제에서 광범위한 성공을 거두었다. 상기 애플리케이션과는 별도로, NeRF-기반 표현은 얼굴 및 신체 재연과 같은 디지트 아바타 생성을 위해 또한 사용될 수 있다[23, GCL*21, LHR*21, WCS*22, HPX*22]. NeRF는 로봇 공학[21, ZKW*23, ACC*22], 단층 촬영[24, ZLZ*22], 이미지 처리[21, MLL*22b, HZF*23a], 천문학[22] 등 다양한 분야로 확장되었다.\n' +
      '\n' +
      '###### 3.2.2 신경 암묵면\n' +
      '\n' +
      '형상 재구성의 범위 내에서, 신경망은 3D 좌표를 입력으로 처리하고 스칼라 값을 생성하며, 이는 보통 표면까지의 서명된 거리를 나타낸다. 이 방법은 누락된 정보를 채우고 매끄럽고 연속적인 표면을 생성하는 데 특히 효과적이다. 암시적 표면 표현은 장면 표면을 각 지점에서 표면까지의 부호 거리 \\(f(\\mathbf{x})\\)를 지정하는 학습 가능한 함수 \\(f\\)로 정의한다. 그리고 0-레벨 집합인 \\(S=\\{\\mathbf{x}\\in\\mathbb{R}^{3}|f(\\mathbf{x})=0\\}\\)로부터 기본표면을 추출할 수 있어 복잡한 3차원 형상을 복원하는데 유연하고 효율적인 방법을 제공한다. 암시적 표면 표현은 메쉬 템플릿을 정의할 필요가 없기 때문에 수많은 이점을 제공한다. 결과적으로 동적 시나리오에서 토폴로지를 알 수 없거나 변화하는 객체를 나타낼 수 있다. 구체적으로, 암시적 표면 표현은 좌표 입력이 있는 MLP를 사용하여 형상 모델링을 위한 서명된 거리 필드를 복구한다. 이러한 초기 제안은 광범위한 열정을 촉발하고 훈련 계획[21, 22], VAK*20, ZML*22], 글로벌-로컬 컨텍스트[21, EGO*20, ZPL*22], 특정 매개변수 채택[19, 20, 21], 공간 파티션 채택[14, 20, 22, 23]과 같은 다양한 측면에 초점을 맞춘 다양한 개선으로 이어졌다.\n' +
      '\n' +
      'NeuS[24]와 VolSDF[25]는 SDF를 볼륨 렌더링에 통합하여 기본 NeRF 공식을 확장하며, 이는 서명된 거리를 밀도\\(\\sigma\\)에 매핑하는 함수를 정의한다. 지표면 교차점에서 국부적으로 최대값에 도달합니다. 구체적으로, 광선 \\(\\mathbf{r}(t)=\\mathbf{o}+t\\mathbf{d}\\)에 따른 누적 투과율 \\(T(t)\\)은 시그모이드 함수로서 공식화된다: \\(T(t)=\\Phi(f(t))=(1+e^{sf(t)})^{-1}\\), 여기서 \\(s\\)과 \\(f(t)\\)는 각각 \\(\\mathbf{r}(t)\\)에서 점들의 학습 가능한 파라미터와 부호화된 거리 함수를 의미한다. 이산 불투명도 값 \\(\\alpha_{i}\\)은 다음에 다음과 같이 유도될 수 있다:\n' +
      '\n' +
      '\\frac{\\Phi_{\\mathbf{x}}\\left(f(t_{i})\\right)-\\Phi_{\\mathbf{x}}\\left(f(t_{i+1})\\right)}{\\Phi_{\\mathbf{x}}\\left(f(t_{i})\\right)\\left(f(t_{i})\\right)\\tag{2}\\right)\n' +
      '\n' +
      'NeuS는 Eqs를 기반으로 기본 SDF를 복구하기 위해 볼륨 렌더링을 사용한다. (1) 및 (2). SDF는 렌더링 결과와 지상-진실 영상 사이의 측광 손실을 최소화함으로써 최적화된다.\n' +
      '\n' +
      'NeuS 및 VolSDF를 기반으로 하는 NeuralWarp[26], GeoNeuS[27], MonoSDF[25]는 MVS 메소드에서 이전 지오메트리 정보를 활용합니다. IRON[28], MII[29], 와일드라이트[28]은 역렌더링을 위해 SDF를 통한 고충실도 형상 재구성을 적용한다. HF-NeuS[30] 및 PET-Neus[31]은 고주파 세부 사항에 맞게 추가 변위 네트워크를 통합한다. LoD-NeuS[27]는 형태 재구성을 위해 LoD(Level of Detail) 특징들을 적응적으로 인코딩한다.\n' +
      '\n' +
      '### Hybrid Representations\n' +
      '\n' +
      '암시적 표현은 위에서 언급한 다양한 응용 프로그램에서 실제로 인상적인 결과를 보여주었다. 그러나 현재 암시적 방법의 대부분은 NeRF 또는 SDF 값에 대한 회귀에 의존하며, 이는 대상 뷰 또는 표면에 대한 명시적 감독으로부터 이익을 얻을 수 있는 능력을 제한할 수 있다. 명시적 표현은 훈련 동안 유용한 제약들을 부과하고 사용자 경험을 개선할 수 있다. 두 표상의 보완적 이점을 활용하기 위해 연구자들은 하이브리드 표상을 탐구하기 시작했다. 여기에는 뷰 합성을 위한 렌더링 알고리즘을 사용하는 피쳐를 내장하는 장면 표현(명시적 또는 암시적)이 포함된다.\n' +
      '\n' +
      '###### 3.3.1 복셀 그리드\n' +
      '\n' +
      '정규 그리드에서 거친 점유율(내부/외부) 값을 저장하는 복셀을 사용하여 3D 모양을 묘사하는 초기 작업[24, 25]. 이 접근법을 통해 강력한 컨볼루션 신경망이 독창적으로 작동하고 3D 재구성 및 합성에서 인상적인 결과를 생성할 수 있었다[23, 24, 25]. 이러한 방법은 일반적으로 명시적 복셀 그리드를 3D 표현으로 사용한다. 최근에는 암시적 표현의 느린 학습 및 렌더링 속도를 해결하기 위해 3D 복셀 기반 임베딩 방법[19, 25, 26]이 제안되었다. 이러한 방법들은 장면의 공간 정보를 인코딩하고 특징을 보다 효율적으로 디코딩한다. 또한, Instant-NGP [26]은 각 레벨에 대한 해시 함수를 통해 암묵적으로 인코딩된 다중 레벨 복셀 그리드를 도입한다. 컴팩트한 모델을 유지하면서 신속한 최적화 및 렌더링을 용이하게 합니다. 이러한 3D 형상 표현 및 처리 기술의 발전은 3D 생성 애플리케이션의 효율성과 효율성을 크게 향상시켰다.\n' +
      '\n' +
      '#### 3.3.2 Tri-plane\n' +
      '\n' +
      '삼면 표현은 3D 형상 표현 및 신경 렌더링에 특징을 임베딩하기 위해 복셀 그리드를 사용하는 대안적인 접근법이다. 이 방법의 주요 아이디어는 3D 볼륨을 3개의 직교 평면들(예를 들어, XY, XZ, 및 YZ 평면들)로 분해하고 이들 평면들 상에서 3D 형상의 특징들을 나타내는 것이다. 구체적으로, TensoRF [27]은 각각의 복셀 그리드를 텐서 분해로 평면 및 벡터로 대체함으로써 유사한 모델 압축 및 가속을 달성한다. 트리플레인들은 효율적이고 부피보다는 표면적으로 스케일링할 수 있으며 표현적이고 미세 조정된 2D 생성 아키텍처와 자연스럽게 통합된다. 생성 설정에서 EG3D[28]은 3D 볼륨을 나타내기 위해 값이 함께 추가된 3개의 평면으로 공간 분해를 제안한다. NFD[26]은 2차원 확산 모델 백본(backbones)을 활용하여 3차원 장면에서의 확산을 도입하고, 3-평면 표현을 내장한다.\n' +
      '\n' +
      '###### 3.3.3 하이브리드 곡면 표현\n' +
      '\n' +
      '[26]에서 인용한 최근 개발인 DMTet은 다재다능하고 효율적인 모델을 만들기 위해 명시적 형태와 암시적 형태를 모두 결합한 하이브리드 3차원 표면 표현이다. 그것은 3D 공간을 조밀한 사면체로 분할하여 명시적인 칸막이를 형성한다. 명시적 표현과 암시적 표현을 통합하여 DMTet을 보다 효율적으로 최적화하고 메쉬 표현과 같은 명시적 구조로 매끄럽게 변환할 수 있다. 생성 프로세스 동안, DMTet는 메쉬로 차별적으로 변환될 수 있고, 이는 신속한 고해상도 멀티뷰 렌더링을 가능하게 한다. 이 혁신적인 접근 방식은 3D 모델링 및 렌더링에서 효율성과 범용성 측면에서 상당한 개선을 제공한다.\n' +
      '\n' +
      '##4 생성 방법\n' +
      '\n' +
      '지난 몇 년 동안, 생성적 적대 네트워크(GANs)[18, 25], 변량 자동 인코더(VAEs)[17, 19, 26], 자기 회귀 모델[27, 28], 확산 모델[16, 29, 28], _etc._ 3D 생성을 위해 이러한 장면 표현과 확장 및 조합으로 이어졌다. 탭 도 1은 생성 모델들 및 장면 표현들을 사용하는 3D 생성의 잘 알려진 예들을 도시한다. 이들 방법들은 생성 공간에서 상이한 장면 표현들을 사용할 수 있으며, 여기서 표현은 생성 모델들에 의해 생성되고, 여기서 출력은 표현된다. 예를 들어, AutoSDF[26]는 트랜스포머 기반 자기회귀 모델을 사용하여 특징 복셀 그리드를 학습하고 재구성을 위해 이 표현을 SDF에 디코딩한다. EG3D[28]은 GAN을 사용하여 잠재 공간에서 샘플을 생성하고 출력을 렌더링하기 위한 3면 표현을 도입한다. SSDNeRF[26]은 확산 모델을 사용하여 3-평면 특징들을 생성하고 렌더링하기 위해 NeRF로 디코딩한다. 신경 장면 표현과 생성 모델의 장점을 활용하여 이러한 접근 방식은 뷰 일관성을 유지하면서 사실적이고 복잡한 3D 모델을 생성하는 데 놀라운 잠재력을 보여주었다.\n' +
      '\n' +
      '이 섹션에서는 알고리즘 패러다임에 따라 네 가지 범주로 구성된 다양한 3D 생성 방법을 탐구한다: 피드포워드 생성(Sec. 4.1), 포워드 패스로 결과를 생성하는 단계; 최적화 기반 생성(Sec. 4.2), 각 세대에 대한 테스트 시간 최적화를 필요로 하는 단계; 절차적 생성(Sec. 4.3), 규칙 집합으로부터 3D 모델을 생성하는 단계; 생성적 새로운 뷰 합성(Sec. 4.4), 3D 생성을 위한 명시적인 3D 표현보다는 다시점 영상을 합성하는 단계. 3D 생성 방법의 진화 트리가 그림 1에 나와 있다. 4는 관련 작업 및 후속 개발과 함께 생성 기술의 주요 분기를 보여준다. 종합적인 분석은 후속 하위 섹션에서 논의될 것이다.\n' +
      '\n' +
      '### Feedforward Generation\n' +
      '\n' +
      '생성 방법에 대한 주요 기술적 접근법은 생성 모델을 사용하여 3D 표현을 직접 생성할 수 있는 피드포워드 생성이다. 이 섹션에서는 그림 1과 같이 생성 모델을 기반으로 이러한 방법을 탐구한다. 5는 생성적 적대 네트워크(GAN), 확산 모델, 자기 회귀 모델, 가변 자동 인코더(VAE) 및 정규화 흐름을 포함한다.\n' +
      '\n' +
      '######4.1.1 생성적 적대적 네트워크\n' +
      '\n' +
      '생성적 적대 네트워크(Generative Adversarial Networks, GANs) [1]은 생성기\\(G(\\cdot)\\)와 판별기\\(D(\\cdot)\\)로 구성된 이미지 합성 작업에서 놀라운 결과를 보여주었다. 생성자 네트워크\\(G\\)는 잠재 코드를 입력으로 받아 합성 데이터를 생성하는 반면, 판별자 네트워크\\(D\\)는 생성된 데이터와 실제 데이터를 구별한다. 학습 최적화 과정을 통해 생성기와 판별기를 공동으로 최적화하여 생성기가 실제 데이터만큼 사실적인 합성 데이터를 생성할 수 있도록 안내한다.\n' +
      '\n' +
      '2D 이미지 합성에서 GAN이 달성한 인상적인 결과를 기반으로 연구자들은 이러한 모델을 3D 생성 작업에 적용하는 것을 탐구하기 시작했다. 핵심 아이디어는 포인트 클라우드(l-GAN/r-GAN[1], 트리-GAN[14]), 복셀 그리드(3D-GAN[15], Z-GAN[16]), 메시(MeshGAN[17]), 또는 SDF(SurfGen[11], SDF-StyleGAN[18])와 같은 다양한 3D 표현들로 GAN들과 결혼하는 것이다. 이러한 맥락에서 3D 생성 과정은 일련의 적대적 단계로 볼 수 있는데, 생성기는 입력 잠재 코드로부터 사실적인 3D 데이터를 생성하도록 학습하고 판별기는 생성된 데이터와 실제 데이터를 구별한다. 생성기 및 판별기 네트워크를 반복적으로 최적화함으로써, GAN은 실제 데이터의 현실성과 매우 유사한 3D 데이터를 생성하는 것을 학습한다.\n' +
      '\n' +
      '3D 객체 생성을 위해, l-GAN[1], 3D-GAN[15], 및 멀티차트 생성[1]과 같은 이전의 GAN 방법론들은 실제 데이터의 명시적인 3D 객체 표현을 직접 활용하여 생성기 네트워크들을 지시한다. 그들의 차별자들은 3D 표현을 감독으로 사용하고 장군을 감독한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline Method & Generative Model & Generation Space & Reconstruction Space & Rendering & Supervision & Condition \\\\ \\hline PointFlow [16] & Normalizing Flow & Latent Code & Point Cloud & - & 3D & Uncon \\\\\n' +
      '3dAE [17] & VAE & Latent Code & Point Cloud & - & 3D & Uncon \\\\ SDM-NET [14] & VAE & Latent Code & Mesh & - & 3D & Uncon \\\\ \\hline AutoSDF [16] & Autoregressive & Voxel & SDF & - & 3D & Uncon. \\\\ PolyGen [15] & Autoregressive & Polygon & Mesh & - & 3D & Uncon./Label/Image \\\\ PointGrow [14] & Autoregressive & Point & Point Cloud & - & 3D & Uncon./Label/Image \\\\ \\hline EG3D [17] & GAN & Latent Code & Tri-plane & Mixed Rendering & 2D & Uncon. \\\\ GIRAFFE [18] & GAN & Latent Code & NeRF & Mixed Rendering & 2D & Uncon. \\\\ BlockGAN [15] & GAN & Latent Code & Voxel Grid & Network Rendering & 2D & Uncon. \\\\ gDNA [17] & GAN & Latent Code & Occupancy Field & Surface Rendering & 2D\\&3D & Uncon. \\\\ SurfGen [11] & GAN & Latent Code & SDF & - & 3D & Uncon. \\\\ Free-GAN [15] & GAN & Latent Code & Point Cloud & - & 3D & Uncon. \\\\ \\hline HoloDiffusion [14] & Diffusion & Voxel & NeRF & Volume Rendering & 2D & Image \\\\ SSDNeRF [16] & Diffusion & Tri-plane & NeRF & Volume Rendering & 2D & Uncon./Image \\\\\n' +
      '3DShape2VecSet [15 & Diffusion & Latent Set & SDF & - & 3D & Uncon./Text/Image \\\\ Point-E [18 & Diffusion & Point & Point Cloud & - & 3D & Text \\\\\\\n' +
      '3DGen [14] & Diffusion & Tri-plane & Mesh & - & 3D & Text/Image \\\\ DreamFusion [15] & Diffusion & - & NeRF & Volume Rendering & SDS & Text \\\\ Make-h-3D [15] & Diffusion & - & Point Cloud & Network Rendering & SDS & Image \\\\ Zero-1to-3 [15] & Diffusion & Pixel & - & - & 2D & Image \\\\ MDFrame [14] & Diffusion & Pixel & - & - & 2D & Image \\\\ DMV2D [16] & Diffusion & Pixel & Tri-plane & Volume Rendering & 2D & Text/Image \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 3D 생성 방법의 일부 예. 먼저 생성공간에서 생성모델과 대응표상에 따라 방법을 나눈다. 복원 공간 내의 표현들은 3D 객체들이 어떻게 포맷되고 렌더링되는지를 결정한다. 또한 이러한 방법의 주요 감독 및 조건을 나열합니다. 2차원 감리를 위해 렌더링 기법을 이용하여 영상을 생성한다.\n' +
      '\n' +
      '그림 4: 3D 세대의 진화 트리는 최근 몇 년 동안 세대 방법과 그 발전의 주요 가지를 보여준다. 구체적으로, 피드포워드 생성, 최적화 기반 생성, 절차적 생성 및 생성적 신규 뷰 합성을 포함하여 알고리즘 패러다임의 유형에 따라 분류되는 생성 방법에 대한 빠르게 성장하는 문헌에 대한 포괄적인 개요를 제공한다.\n' +
      '\n' +
      '실제 데이터의 사실성과 매우 유사한 합성 데이터를 생성하는 생성자. 트레이닝 동안, 특수 생성기는 포인트 클라우드, 복셀 그리드 및 메쉬와 같은 대응하는 감독 3D 표현을 생성한다. 서프젠[11]과 같은 일부 연구는 중간 암시적 표현을 생성한 다음 명시적 표현을 직접 생성하는 대신 해당 3D 표현으로 변환하여 우수한 성능을 달성하도록 더 발전했다. 특히, 1-GAN[1], 3D-GAN[21], Multi-chart Generation[2]의 생성기는 잠재 코드를 입력으로 하여 포인트 클라우드, 복셀 그리드, 메시의 위치를 각각 직접 생성한다. SurfGen[11]은 암시적 표현을 생성한 후, 명시적 3D 표현을 추출한다.\n' +
      '\n' +
      '다양한 3D 표현을 직접 생성하는 GAN 외에도 연구자들은 일반적으로 3D 인식 GAN이라고 하는 3D 생성을 안내하기 위해 미분 가능한 렌더링을 통해 2D 감독을 통합하는 것을 제안했다. 2D 이미지의 풍부함을 감안할 때 GAN은 3D 감독에만 의존하는 것보다 2D 데이터와 3D 데이터 사이의 암시적 관계를 더 잘 이해할 수 있다. 이 접근법에서, GAN들의 생성기는 암시적 또는 명시적 3D 표현으로부터 렌더링된 2D 이미지들을 생성한다. 그런 다음 판별기는 렌더링된 2D 이미지와 실제 2D 이미지를 구별하여 생성기의 훈련을 안내한다.\n' +
      '\n' +
      '구체적으로, 홀로GAN[20]은 먼저 3D 피처들의 3D 표현을 학습하고, 그 후 카메라 포즈에 의해 2D 피처들에 투영된다. 이어서, 이들 2D 피처 맵들은 최종 이미지들을 생성하기 위해 렌더링된다. BlockGAN[20]은 이를 확장하여 배경 객체와 전경 객체 모두의 3D 특징을 생성하고 전체 장면에 대한 3D 특징으로 결합한다. 또한 PrGAN[2] 및 PlatonicGAN[15]은 명시적인 복셀 그리드 구조를 채용하여 3D 형상을 표현하고 렌더 레이어를 이용하여 이미지를 생성한다. DIB-R[19], ConvMesh[20], Textured3DGAN[3] 및 GET3D[21]과 같은 다른 방법들은 2D 감독만을 사용하여 삼각형 메쉬들 및 텍스처들을 생성하기 위한 GAN 프레임워크들을 제안한다.\n' +
      '\n' +
      'NeRF와 같은 표현을 기반으로 GRAF[14]는 적대적 프레임워크를 사용하여 생성 복사 필드를 제안하고 높은 해상도에서 제어 가능한 이미지 합성을 달성한다. pi-GAN[20]은 이미지 품질 및 뷰 일관성을 더욱 향상시키기 위해 FiLM 컨디셔닝을 갖는 SIREN 기반 암시적 GAN들을 도입한다. GIRAFFE[20]은 다중 객체 장면을 모델링하기 위한 합성 생성 신경망 특징 필드로서 장면을 나타낸다. 또한, EG3D[20]은 먼저 효율적이고 표현적이며 많은 후속 작업에서 널리 채택된 하이브리드 명시적-암시적 삼면 표현을 제안한다.\n' +
      '\n' +
      '######4.1.2 확산 모델\n' +
      '\n' +
      '확산 모델 [15, 16]은 확산 프로세스를 시뮬레이션하여 데이터 샘플을 생성하는 것을 학습하는 생성 모델의 클래스이다. 확산 모델 이면의 핵심 아이디어는 정방향 프로세스라고 불리는 일련의 잡음 구동 단계를 통해 원래 데이터 분포를 가우스처럼 더 간단한 분포로 변환하는 것이다. 그런 다음 모델은 백워드 프로세스로 알려진 이 프로세스를 반전시켜 원래 데이터 분포와 유사한 새로운 샘플을 생성하도록 학습한다. 순방향 과정은 목표 분포에 도달할 때까지 원본 데이터에 점차적으로 노이즈를 추가하는 것으로 생각할 수 있다. 반면, 역방향 프로세스는 최종 출력을 생성하기 위해 분포로부터 샘플들을 반복적으로 잡음제거하는 것을 포함한다. 이 노이즈 제거 프로세스를 학습함으로써 확산 모델은 데이터의 기본 구조와 패턴을 효과적으로 캡처하여 고품질 및 다양한 샘플을 생성할 수 있다.\n' +
      '\n' +
      '2D 이미지 생성에서 확산 모델이 달성한 인상적인 결과를 바탕으로 연구자들은 이러한 모델의 3D 생성 작업에 대한 적용을 탐구하기 시작했다. 핵심 아이디어는 다양한 3D 표현과 함께 노이즈 제거 확산 모델과 결혼하는 것이다. 이러한 맥락에서, 3D 생성 프로세스는 입력 3D 데이터에서 가우시안 노이즈로 확산 프로세스를 반전시키는 일련의 잡음 제거 단계로 볼 수 있다. 확산 모델들은 잡음 제거를 통해 이 잡음 분포로부터 3D 데이터를 생성하는 것을 학습한다.\n' +
      '\n' +
      '도 5: 예시적인 피드포워드 3D 생성 모델. 우리는 (a) 생성적 적대 네트워크, (b) 확산 모델, (c) 자기 회귀 모델, (d) 가변 자동 인코더 및 (e) 정규화 흐름을 포함하여 피드포워드 3D 생성 모델의 몇 가지 대표적인 파이프라인을 보여준다.\n' +
      '\n' +
      '구체적으로, Cai et al. [2]는 포인트 클라우드 생성을 위한 분포를 학습하기 위해 잡음 제거 점수-매칭 프레임워크를 구축한다. PVD[15]는 3D 생성을 위한 점 기반 표현과 복셀 기반 표현 모두의 이점을 결합한다. 모델은 포인트 클라우드를 복셀 그리드로 변환하는 확산 프로세스를 학습하여 3D 데이터의 기본 구조와 패턴을 효과적으로 캡처한다. 유사하게, DPM[14]은 잡음이 많은 포인트 클라우드 샘플들을 반복적으로 잡음제거함으로써 포인트 클라우드 데이터에 대한 잡음제거 프로세스를 학습하는 것에 초점을 맞춘다. PVD[15]와 DPM[14]의 발전에 따라, LION[13]은 포인트 클라우드의 잡음 제거 개념을 기반으로 하고 포인트 클라우드의 잠재 공간에서 잡음 제거 개념을 도입하는데, 이는 2D 이미지 생성이 잡음 제거 픽셀에서 잡음 제거 잠재 공간 표현으로 이동하는 것과 유사하다. 텍스트 프롬프트로부터 포인트 클라우드를 생성하기 위해, 포인트-E[12]는 처음에 텍스트-조건부 합성 뷰를 생성하기 위해 GLIDE 모델[12]을 채용하고, 이어서 생성된 이미지 상에 컨디셔닝된 확산 모델을 사용하여 포인트 클라우드를 생성한다. 대규모 3D 데이터 세트에서 모델을 훈련함으로써 놀라운 일반화 기능을 달성한다.\n' +
      '\n' +
      '점 구름 외에도 메쉬 확산[10], 사면체 확산 모델[11], SLIDE[13]은 메쉬 생성에 확산 모델의 적용을 탐구한다. MeshDiffusion[10]은 메쉬에 대한 DMTet 표현[21]을 채택하고 부호 거리 함수의 최적화를 잡음 제거 과정으로 처리하여 모델을 최적화한다. 사면체 확산 모델 [11]은 확산 모델을 사면체 메쉬로 확장하고, 디노이징을 통해 사면체 그리드 상의 변위 벡터 및 서명된 거리 값을 학습한다. SLIDE[13]은 메쉬 생성을 위한 희박한 잠재점에 대한 확산 모델을 탐색한다.\n' +
      '\n' +
      '명시적 3D 표현에서 확산 연산을 적용하는 것과는 별도로, 일부 작업은 암시적 표현에서 확산 처리를 수행하는 데 중점을 둔다. SSDNeRF[20], DiffRF[22] 및 Shap-E[23]은 3D 복사 필드들에서 동작하는 반면, SDF-확산[21], LAS-확산[15], 신경 웨이브렛-도메인 확산[16], One-2-3-45++[21], SDFusion[22] 및 3D-LDM[23]은 서명된 거리 필드 표현들에 초점을 맞춘다. 구체적으로, Diffusion-SDF[20]는 복셀 형태의 SDF 표현을 활용하여 고품질의 연속적인 3D 형상을 생성한다. 3D-LDM[23]은 초기에 오토 디코더의 잠재 공간을 생성하기 위해 확산 모델을 사용함으로써 SDF들의 신경 암시적 표현들을 생성한다. 이어서, 잠재 공간은 SDF들로 디코딩되어 3D 형상들을 획득한다. 더욱이, 로댕[13] 및 슈 등[20]은 트리-평면을 표현으로서 채택하고 확산 방법들을 사용하여 트리-평면 특징들을 최적화한다. Shue et al. [20]은 점유 네트워크들을 사용하여 3D 형상들을 생성하는 반면, Rodin [13]은 체적 렌더링을 통해 3D 형상들을 획득한다.\n' +
      '\n' +
      '이러한 접근법은 명시적 형태와 암시적 형태를 포함한 다양한 3D 표현을 관리하는 데 있어 확산 모델의 다양성을 보여준다. 디노이징 프로세스를 다양한 표현 유형에 맞게 조정함으로써 확산 모델은 3D 데이터의 기본 구조와 패턴을 효과적으로 캡처하여 생성 품질과 다양성을 향상시킬 수 있다. 이 분야의 연구가 계속 진행됨에 따라 확산 모델은 광범위한 응용 분야에 걸쳐 3D 형상 생성의 경계를 밀어내는 데 중요한 역할을 할 것으로 예상된다.\n' +
      '\n' +
      '회귀모형 4.1.3 자기회귀모형\n' +
      '\n' +
      '3D 오브젝트는 다수의 3D 엘리먼트들의 발생들의 조인트 확률로서 표현될 수 있다:\n' +
      '\n' +
      '\\[p(x_{0},x_{1},...,x_{n}), \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(x_{i}\\)는 점이나 복셀의 좌표가 될 수 있는 \\(i\\)번째 원소이다. 확률 변수가 많은 결합 확률은 보통 학습과 추정이 어렵다. 그러나 조건부 확률의 곱으로 인수분해할 수 있습니다.\n' +
      '\n' +
      '\\[p(x_{0},x_{1},...,x_{n})=p(x_{0})\\prod_{i=1}^{n}p(x_{i}|x_{<i}), \\tag{4}\\]\n' +
      '\n' +
      '이를 통해 조건부 확률을 학습하고 샘플링을 통해 결합 확률을 추정할 수 있다. 데이터 생성을 위한 자기 회귀 모델은 이전 출력에 따라 현재 출력을 지정하는 모델의 유형이다. 요소 \\(x_{0}\\), \\(x_{1}\\),..., \\(x_{n}\\)이 순서화된 시퀀스를 형성한다고 가정하면, 이전 입력 \\(x_{0}\\),... (x_{i-1}\\) 및 결과 확률 \\(x_{i}\\)에 맞도록 감독하는 단계:\n' +
      '\n' +
      '\\[p(x_{i}|x_{<i})=f(x_{0},...,x_{i-1}), \\tag{5}\\]\n' +
      '\n' +
      '조건부 확률들은 모델 함수 \\(f\\)에 의해 학습된다. 이러한 연수 과정을 교사 강제라고 부르는 경우가 많다. 그런 다음 모델을 사용하여 요소를 단계별로 자동으로 생성할 수 있다.\n' +
      '\n' +
      '\\[x_{i}=\\text{argmax }p(x|x_{<i}). \\tag{6}\\\n' +
      '\n' +
      'GPT[12, 13]과 같은 최신 생성 모델은 트랜스포머 네트워크를 모델 함수로 하는 자기회귀 생성기이다. 그들은 자연 언어와 이미지를 생성하는 데 큰 성공을 거두었습니다. 3D 세대에서는 자기회귀모형을 기반으로 여러 연구가 진행되어 왔다. 이 섹션에서는 3D 생성을 위해 자기회귀 모델을 사용하는 몇 가지 주목할 만한 예에 대해 논의한다.\n' +
      '\n' +
      'PointGrow[23]은 포인트-바이-포인트 방식으로 자기-관심 상황 인식 동작들을 갖는 자기회귀 네트워크를 사용하여 포인트 클라우드를 생성한다. 이전에 생성된 점을 감안할 때 포인트그로우는 점을 축으로 수정하여 세 가지 지점으로 전달한다. 각 분기는 입력을 사용하여 한 축의 좌표 값을 예측합니다. 모델은 또한 클래스 카테고리 또는 이미지일 수 있는 포인트 클라우드를 생성하기 위해 임베딩 벡터를 조건화할 수 있다. PointGrow의 네트워크에서 영감을 얻은 PolyGen[23]은 두 개의 트랜스포머 기반 네트워크와 3D 메쉬를 생성하는데, 하나는 정점이고 하나는 면이다. 버텍스 변환기는 이전 버텍스들을 기반으로 다음 버텍스 좌표를 자동으로 생성한다. 면 변환기는 모든 출력 정점을 컨텍스트로 사용하여 면을 생성합니다. PolyGen은 변압기 네트워크에 의해 교차되는 객체 클래스 또는 이미지의 컨텍스트를 조건화할 수 있다.\n' +
      '\n' +
      '최근 AutoSDF[20]는 T-SDF(Volumetric truncated-signed distance function)로 표현되는 3차원 형상을 생성한다. AutoSDF는 VQ-VAE를 이용하여 T-SDF의 국부 영역에 관한 양자화된 코드북을 학습한다. 그런 다음 모양은 코드북 토큰에 의해 제시되고 비순차 자기회귀 방식으로 변압기 기반 네트워크에 의해 학습된다. 구체적으로, 임의의 위치들 및 질의 위치에서 이전 토큰들이 주어지면, 네트워크는 질의되는 토큰을 예측한다. AutoSDF는 이미지나 텍스트를 기반으로 모양을 완성하고 모양을 생성할 수 있다. AutoSDF와 동시에 ShapeFormer[14]는 불완전하고 잡음이 많은 포인트 클라우드를 기반으로 3D 모양의 표면을 생성한다. 벡터 양자화된 심층 암묵 함수(VQDIF)라고 불리는 컴팩트한 3D 표현은 이산 변수들의 특징 시퀀스를 사용하여 형상들을 표현하는데 사용된다. Shape former는 먼저 입력 포인트 클라우드를 부분 특징 시퀀스로 인코딩한다. 그런 다음 변압기 기반 네트워크를 사용하여 전체 시퀀스를 자동으로 샘플링합니다. 마지막으로, 완전한 객체 표면이 추출될 수 있는 깊은 암시적 함수로 시퀀스를 디코딩한다. Luo 등은 3D 체적 공간에서 학습하는 대신, 3D 형상 생성의 효율적인 학습을 향상시키기 위해 1차원 공간에서 이산 표현을 학습하는 개선된 자동 회귀 모델(ImAM)을 제안한다. 이 방법은 먼저 부피 그리드의 3D 형상을 3축 정렬 평면으로 인코딩한다. 그것은 결합 네트워크를 사용하여 평면들을 잠재 벡터로 더 투영하고, 여기서 벡터 양자화는 이산 토큰들에 대해 수행된다. ImAM은 다루기 쉬운 주문으로 토큰을 자동으로 학습하기 위해 바닐라 변압기를 채택한다. 생성된 토큰들은 공간적 위치들을 샘플링함으로써 네트워크를 통해 점유 값들로 디코딩된다. ImAM은 포인트 클라우드, 카테고리, 이미지 등 다양한 조건을 연결하여 무조건 생성에서 조건부 생성으로 전환할 수 있다.\n' +
      '\n' +
      '######4.1.4 변종 오토인코더\n' +
      '\n' +
      'VAE(Variational Autoencoder) [15]는 두 개의 신경망 구성 요소인 인코더와 디코더로 구성된 확률적 생성 모델이다. 인코더는 입력 데이터 포인트를 변분 분포의 파라미터들에 대응하는 잠재 공간에 맵핑한다. 이러한 방식으로, 인코더는 모두 동일한 분포로부터 오는 다수의 상이한 샘플들을 생성할 수 있다. 디코더는 잠재 공간으로부터 입력 공간으로 매핑하여, 데이터 포인트들을 생성하거나 생성한다. 노이즈 모델의 분산은 별도로 학습될 수 있지만, 두 네트워크 모두 일반적으로 재매개변수화 트릭의 사용과 함께 학습된다. VAE는 또한 3D 세대[16, 17, 18, 19, 20]에서 탐구되었다.\n' +
      '\n' +
      'Brock 등은 3D ConvNet을 사용하여 복셀에 대해 직접 변량 오토인코더를 훈련시키는 반면, SDM-Net[19]은 변형 가능한 부품으로 구성된 구조화된 메시의 생성에 중점을 둔다. 이 방법은 한 VAE 네트워크를 사용하여 요소를 모델링하고 다른 VAE 네트워크를 사용하여 전체 객체를 모델링한다. 후속 작업 TM-Net[19]은 부분 인식 방식으로 메쉬의 텍스처 맵을 생성할 수 있다. 점 구름[16] 및 NeRF[18]과 같은 다른 표현도 변분 자동 인코더에서 탐색된다. VAE의 재구성 초점 목표 때문에 이들의 훈련은 GAN보다 훨씬 더 안정적이다. 그러나 VAE는 GAN에 비해 더 흐릿한 결과를 생성하는 경향이 있다.\n' +
      '\n' +
      '######4.1.5 정규화 흐름\n' +
      '\n' +
      '정규화 흐름 모델은 가우시안과 같은 단순 분포를 표적 분포에 매핑하는 일련의 가역 변환으로 구성되며, 이는 데이터를 생성으로 나타낸다. 이러한 변환은 미분 가능하고 반전 가능하도록 신중하게 설계되어 모델에서 데이터의 가능성을 계산하고 구배 기반 최적화 기술을 사용하여 모델 매개변수를 최적화할 수 있다.\n' +
      '\n' +
      '3D 생성에서 PointFlow[18]은 연속적인 정규화 흐름을 이용하여 모양의 분포와 점의 분포를 학습한다. 이 접근법은 형상들의 샘플링을 허용하고, 이어서 주어진 형상으로부터 임의의 수의 점들의 샘플링을 허용한다. 이산 포인트플로우(DPF) 네트워크[18]는 연속 정규화 플로우를 이산 정규화 플로우로 대체함으로써 포인트플로우를 개선하며, 이는 트레이닝 및 샘플링 시간을 감소시킨다. SoftFlow[19]는 매니폴드 상의 흐름을 정규화하는 훈련을 위한 프레임워크이다. 데이터 분포를 직접 학습하는 대신 교란된 입력 데이터의 조건부 분포를 추정한다. SoftFlow는 유동 기반 모델의 얇은 구조를 형성하는 어려움을 완화한다.\n' +
      '\n' +
      '### Optimization-based Generation\n' +
      '\n' +
      '최적화 기반 생성은 런타임 최적화를 사용하여 3D 모델을 생성하는 데 사용된다. 이러한 방법들은 일반적으로 미리 훈련된 멀티모달 네트워크를 활용하여 사용자 지정 프롬프트를 기반으로 3D 모델을 최적화한다. 핵심은 높은 충실도와 다양성을 유지하면서 주어진 프롬프트와 생성된 콘텐츠 사이의 정렬을 달성하는 데 있다. 본 절에서는 주로 사용자가 제공하는 프롬프트의 유형을 기반으로 텍스트와 이미지를 사용하는 최적화 기반 생성 방법을 살펴본다.\n' +
      '\n' +
      '#### 4.2.1 Text-to-3D\n' +
      '\n' +
      '언어는 인간의 의사소통과 장면 서술의 주요 수단으로 작용하며, 연구자들은 텍스트 기반 생성 방법의 잠재력을 탐구하는 데 전념한다. 이러한 방법들은 전형적으로 텍스트를 미분가능한 렌더링 기법들을 통해 획득된 이미지들과 정렬시킴으로써, 텍스트 프롬프트들에 기초하여 3D 콘텐츠의 생성을 안내한다. 고정된 표면이 주어지면, TANGO[19]는 CLIP[17]을 사용하여 미분가능한 물리적-기반 렌더링(PBR) 이미지들을 감독하고 지정된 텍스트 프롬프트와 정렬되는 텍스처 맵들을 획득한다. 3D 정적 장면 및 텍스트-이미지 태스크를 각각 모델링하는 데 NeRF[17] 및 확산 모델의 성공에 영감을 받아, Dream-Fusion[19] (도 6에 도시된 바와 같이) 부피를 결합하다.\n' +
      '\n' +
      '도 6: SDS loss를 이용한 DreamFusion [19]에 의한 텍스트 유도 3D 생성 결과. \\ (*\\)는 DSLR 사진을 나타내고, \\(\\dagger\\)는 줌 아웃된 DSLR 사진을 나타낸다.\n' +
      '\n' +
      'representation in NeRF with the proposed Score Distillation Sampling (SDS) loss to achieve high-fidelity 3D content generation. SDS 손실은 렌더링 오류 최소화를 확률 밀도 증류로 변환하고 2D 확산 사전이 이미지 매개변수화(예: 미분 가능한 렌더링)를 통해 3D 표현(예: 부피 표현 및 삼각형 메쉬)을 최적화할 수 있게 한다. SDS와 동시 작업으로서 스코어 자코비안 체인(Score Jacobian Chaining, SJC) [WDL\\({}^{*}\\)23]은 사전 훈련된 확산 모델의 예측을 데이터 로그 우도의 점수 함수로 해석하며, 유사하게 2D 확산 사전이 점수 매칭을 통해 3D 표현을 최적화할 수 있게 한다. DreamFusion을 기반으로 Magic3D[LGT\\({}^{*}\\)23]은 coarse-to-fine 방식을 도입하여 볼륨의 기본 형상을 메쉬로 추출한다. 그런 다음 미분 가능한 신경망 렌더링과 SDS를 결합하여 추출된 메쉬를 정제한다. 매직3D는 고품질 텍스처 메쉬를 내보내고 전통적인 그래픽 파이프라인에 매끄럽게 삽입할 수 있습니다. 또한 2단계 방법으로 Fantasia3D는 첫 번째 기하 모델링 단계에서 DMTet [SGY\\({}^{*}\\)21]과 SDS를 결합하여 표면을 명시적으로 최적화한다. 두 번째 단계에서는 PBR 재료 모델과 디엔탱글 텍스처 및 환경 조명을 소개한다. ProlificDreamer [WLW\\({}^{*}\\)23]은 텍스트-3D 생성을 증가시키기 위해 가변 점수 증류(variational score distillation, VSD)를 제시한다. VSD는 파티클을 이용하여 3차원 장면의 분포를 모델링하고 Wasserstein 기울기 흐름으로부터 기울기 기반 최적화 기법을 유도하여 모델링 분포의 렌더링 결과 분포와 사전 학습된 확산 분포 사이의 간격을 좁힌다. VSD는 단일 장면이 아닌 장면 분포의 최적화로 인해 SDS에 의해 생성된 과포화 및 과 평활화된 결과를 극복하고 다양성을 향상시킨다. MVDream[SWY\\({}^{*}\\)23]은 멀티뷰 확산 모델을 더 미세 조정하고, 멀티뷰 일관성 있는 3D 이전을 도입하여 멀티페이스 및 콘텐츠 드리프트 문제를 극복한다. 텍스트-투-3D(Text-to-3D)는 최근 많은 주목을 받고 있으며, 이 외에도 많은 다른 방법[ZZ23, LCT23, MRP\\({}^{*}\\)23a]이 이 분야에서 제안되었다.\n' +
      '\n' +
      '#### 4.2.2 Image-to-3D\n' +
      '\n' +
      '장면의 시각적 효과를 설명하는 주요 방법으로 이미지는 언어보다 세밀하게 장면의 세부 사항과 모양을 보다 직관적으로 설명할 수 있다. 따라서, 최근 연구는 특정 이미지로부터 현저하고 충실도가 높은 3D 모델을 재구성하는 이미지-투-3D 기술을 탐구하는 데 동기를 부여한다. 이러한 방법은 합리적인 기하학 전개를 도입하면서 지정된 이미지의 모양과 최적화된 3D 콘텐츠를 유지하기 위해 노력한다. 텍스트-대-3D 방법과 유사하게, 여러 이미지-대-3D 방법은 타겟 3D 장면을 나타내기 위해 NeRF에서 사용되는 체적 표현을 활용하며, 이는 기본적으로 멀티 뷰 일관성을 도입한다. NeuralLift-360 [XJW\\({}^{*}\\)23]은 기하학 및 외형 최적화를 각각 정규화하기 전에 추정된 단안 깊이와 CLIP 유도 확산을 사용하여 NeRF로 표현되는 3차원 장면으로 단일 이미지의 양력을 달성한다. RealFusion[MKLRV23]과 NeRDi[DJQ\\({}^{*}\\)23]은 텍스트 역산[GAA\\({}^{*}\\)22]을 이용하여 텍스트 임베딩을 추출하여 미리 학습된 이미지 확산 모델[RBL\\({}^{*}\\)22b]을 조건화하고, 이를 결합하여 스코어 증류 손실을 이용하여 체적 표현을 최적화한다. Magic3D [LGT\\({}^{*}\\)23]을 기반으로 coarse-to-fine framework를 적용한 Magic123 [QMH\\({}^{*}\\)23]은 미리 학습된 시점-조건 확산 모델인 Zero-1-to-3 [LWVH\\({}^{*}\\)23]의 3D 전원을 두 가지 최적화 단계에서 추가로 도입하여 지정된 이미지와 일치하는 텍스처 메쉬를 생성한다. 또 다른 2단계 이미지-to-3D 방법으로 Make-it-3D[TWZ\\({}^{*}\\)23]은 미세한 단계에서 질감과 기하학적 구조를 향상시켜 고품질의 질감 포인트 클라우드를 최종 결과로 생성한다. 이후의 [SZS\\({}^{*}\\)23, YYC\\({}^{*}\\)23]이 지속적으로 제안되어 그 결과를 향상시켰다. 최근 3DGS (3D Gaussian Splatting) [KKLD23]은 실시간 렌더링 기술뿐만 아니라 유망한 모델링으로 부상하고 있다. 3DGS를 기반으로 DreamGaussian [TRZ\\({}^{*}\\)23]은 텍스트 기반 3D 생성과 이미지 기반 3D 생성을 위한 효율적인 2단계 프레임워크를 제시한다. 첫 번째 단계에서 DreamGaussian은 SDS 손실(2D 확산 사전[LWVH\\({}^{*}\\)23]과 CLIP 유도 확산 사전[PJBM23])을 평균하여 3D 가우시안(Gaussians)으로 표현되는 표적 객체를 생성한다. 그런 다음 드림가우시안(DreamGaussian)은 최적화된 3D 가우시안으로부터 국부 밀도를 질의하여 텍스처 메쉬를 추출하고 UV 공간에서 텍스처를 정제한다. 다양한 이미지-투-3D 방법에 대한 독자들의 이해를 돕기 위해, 우리는 일부 오픈 소스 최신 방법의 성능을 평가한다. 탭 도 2는 표면 재구성에 대한 이미지-대-3D 방법의 정량적 비교를 도시한다. 표면 재구성의 품질을 평가하기 위한 메트릭으로 챔퍼 거리와 부피 IoU를 요약한다. 탭 도 3은 새로운 뷰 합성에 대한 이미지 대 3D 방법의 정량적 비교를 보여준다. 뷰 합성의 품질을 평가하기 위한 메트릭으로 CLIP 유사성, PSNR 및 LPIPS를 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & Chamfer Distance \\(\\downarrow\\) & Volume IoU \\(\\uparrow\\) \\\\ \\hline RealFusion [MKLRV23] & 0.0819 & 0.2741 \\\\ Magic123 [QMH\\({}^{*}\\)23] & 0.0516 & 0.4528 \\\\ Make-it-3D [TWZ\\({}^{*}\\)23] & 0.0732 & 0.2937 \\\\ One-2-3-45 [LXJ\\({}^{*}\\)23] & 0.0629 & 0.4086 \\\\ Point-E [END\\({}^{*}\\)22] & 0.0426 & 0.2875 \\\\ Shap-E [DJ23] & 0.0436 & 0.3584 \\\\ Zero-1-to-3 [LWVH\\({}^{*}\\)23] & 0.0339 & 0.5035 \\\\ SyncDreamer [LLZ\\({}^{*}\\)23] & 0.0261 & 0.5421 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 표면 재구성에 대한 이미지-대-3D 방법의 정량적 비교. 표면 재구성의 품질을 평가하기 위한 메트릭으로 챔퍼 거리와 부피 IoU를 요약한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Method & CLIP-Similarity \\(\\uparrow\\) & PSNR \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) \\\\ \\hline RealFusion [MKLRV23] & 0.735 & 20.216 & 0.197 \\\\ Magic123 [QMH\\({}^{*}\\)23] & 0.747 & 25.637 & 0.062 \\\\ Make-it-3D [TWZ\\({}^{*}\\)23] & 0.839 & 20.010 & 0.119 \\\\ One-2-3-45 [LXJ\\({}^{*}\\)23] & 0.788 & 23.159 & 0.096 \\\\ Zero-1-to-3 [LWVH\\({}^{*}\\)23] & 0.759 & 25.386 & 0.068 \\\\ SyncDreamer [LLZ\\({}^{*}\\)23] & 0.837 & 25.896 & 0.059 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 새로운 뷰 합성에 대한 이미지 대 3D 방법의 정량적 비교. 뷰 합성의 품질을 평가하기 위한 메트릭으로 CLIP 유사성, PSNR 및 LPIPS를 보고한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '트랜스포머를 훈련하기 위한 추상적인 잠재 공간을 얻기 위해 VQGAN에 대한 문장이다. 한편 ViewFormer[14]는 VQ-VAE(Vector Quantized Variational Autoencoder) 코드북과 트랜스포머 모델로 구성된 2단계 학습도 사용한다. 그리고 [15]는 암시적 표현을 학습하기 위해 트랜스포머에 기초한 인코더-디코더 모델을 채용한다.\n' +
      '\n' +
      '반면에 생성적 적대 네트워크는 이미지 합성에서 고품질 결과를 생성할 수 있으며 결과적으로 새로운 뷰 합성에 적용된다[16, 17, 18, 19, 20, 21, 22]. 일부 방법[15, 20, 21]은 3D 포인트 클라우드를 표현으로 유지하며, 이는 누락된 영역을 환각하고 출력 이미지를 합성하기 위해 GAN이 뒤따르는 새로운 뷰에 투영될 수 있다. [20]과 [21]은 적대적 훈련으로 단일 뷰에서 장거리 뷰 생성에 초점을 맞춘다. 자동 인코더와 변량 자동 인코더가 탐구되기 시작하는 딥 러닝 방법의 초기 단계에서 새로운 견해[16, 21, 22]를 합성하는 데에도 사용된다.\n' +
      '\n' +
      '요약하면, 생성적 신규 뷰 합성은 이미지 합성 기술의 하위 집합으로 간주될 수 있으며 이미지 합성 방법의 발전과 함께 계속 발전한다. 일반적으로 포함된 생성 모델 외에도 새로운 뷰를 합성하기 위한 조건으로 입력 뷰에서 정보를 통합하는 방법을 결정하는 것이 이러한 방법과 관련된 주요 문제이다.\n' +
      '\n' +
      '## 5 3D 생성을 위한 데이터셋\n' +
      '\n' +
      '기술의 급속한 발전으로 데이터 획득 및 저장 방법이 더 실현 가능하고 저렴해짐에 따라 사용 가능한 데이터 양이 기하급수적으로 증가한다. 데이터가 축적되면서 문제 해결을 위한 패러다임이 점차 데이터 중심 접근 방식에서 모델 중심 접근 방식으로 전환되고, 이는 결국 "빅데이터"와 "AIGC"의 성장에 기여한다. 오늘날 데이터는 알고리즘의 성공을 보장하는 데 중요한 역할을 한다. 잘 정제된 데이터 세트는 모델의 견고성과 성능을 크게 향상시킬 수 있다. 반대로 잡음이 많고 결함이 있는 데이터는 알고리즘 설계에 상당한 노력을 필요로 하는 모델 편향을 유발할 수 있다. 이 절에서는 3D 생성에 사용되는 공통 데이터에 대해 살펴보겠습니다. 사용된 방법에 따라 일반적으로 3D 데이터(섹션 5.1), 다중 뷰 이미지 데이터(섹션 5.2), 단일 뷰 이미지 데이터(섹션 5.3)를 포함하며, 이는 탭 4에도 요약되어 있다.\n' +
      '\n' +
      '3차원 데이터를 이용한### 학습\n' +
      '\n' +
      '3D 데이터는 RGB-D 센서 및 스캔 및 재구성을 위한 기타 기술에 의해 수집될 수 있다. 3D 생성 외에도 3D 데이터는 데이터 합성에 의한 고전적인 2D 비전 작업 성능 향상, 체화된 AI 에이전트 훈련을 위한 환경 시뮬레이션, 3D 객체 이해 등과 같은 다른 작업에도 널리 사용된다. 초기 단계에서 인기 있고 자주 사용되는 3D 모델 데이터베이스 중 하나는 프린스턴 모양 벤치마크[23]이다. 그것은 월드 와이드 웹에서 수집된 약 1800개의 다각형 모델을 포함하고 있다. [16]이 키트 객체 모델 데이터베이스를 캡처하기 위해 구부러진 레일을 따라 이동할 수 있는 썰매에 장착된 3D 디지타이저, 턴테이블 및 한 쌍의 카메라를 포함하는 특수 리그를 구성하는 동안. 3D 모델이 주어진 이미지에서 객체를 탐지하고 추정하는 알고리즘을 평가하기 위해 [20]은 구글 웨어하우스에서 얻은 3D IKEA 모델의 데이터 세트를 소개한다. 일부 3D 모델 데이터베이스는 로봇 조작[16, 21], 3D 형상 검색[20], 단일 이미지로부터의 3D 형상 모델링[21]과 같은 작업에 대해 제시된다. BigBIRD[23]은 다시점 영상 및 깊이, 카메라 포즈 정보 및 각 영상에 대한 분할된 객체도 포함하는 3D 객체 인스턴스의 대규모 데이터 세트를 제시한다.\n' +
      '\n' +
      '그러나 이러한 데이터 세트는 매우 작으며 수백 또는 수천 개의 개체만 포함한다. 3D 콘텐츠의 데이터 기반 방법을 위해서는 컴퓨터 비전 및 그래픽 커뮤니티에서 더 큰 데이터 세트를 수집, 조직 및 레이블링하는 것이 필요하다. 이를 해결하기 위해 ShapeNet[14]을 도입하여 객체의 3D CAD 모델의 대규모 저장소를 구축한다. 쉐이프넷의 코어는 55개의 공통 객체 카테고리를 커버하며, 수동으로 검증된 카테고리 및 정렬 주석이 있는 약 51,300개의 모델을 포함한다. Thingi10K[18]은 온라인 저장소 Thingiverse로부터 10,000개의 3D 프린팅 모델을 수집한다. 포토샵[22]이 11,000개의 포토레알리스를 생산하는 동안\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Dataset & Type & Year & Samples & Category \\\\ \\hline ShapeNet [14] & 3D data & 2015 & 51K & objects \\\\ Thingi10K [18] & 3D data & 2016 & 10K & objects \\\\\n' +
      '3D-Future [14] & 3D data & 2020 & 10K & furniture \\\\ GSO [21] & 3D Data & 2022 & 1K & household items \\\\ Objectives [13] & 3D data & 2022 & 800K & objects \\\\ OmniObject [14] & 3D data & 2023 & 6K & objects \\\\ Objareser-XL [20] & 3D Data & 2023 & 10.2M & objects \\\\ \\hline ScanNet [14] & multi-view images & 2017 & 1.5K (2.5M images) & indoor scenes \\\\ CO3D [16] & multi-view images & 2021 & 19K (1.5M images) & objects \\\\ MVImgNet [14] & multi-view images & 2023 & 219K (6.5M images) & objects \\\\ \\hline DeepFashion [20] & single-view images & 2016 & 800K & clothes \\\\ FFHQ [21] & single-view images & 2018 & 70K & human faces \\\\ AFHQ [14] & single-view images & 2019 & 15K & animal faces \\\\ SHHQ [20] & single-view images & 2022 & 40K & human bodies \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 3D 생성에 일반적으로 사용되는 선택된 데이터 세트.\n' +
      '\n' +
      '온라인 데이터를 기반으로 한 신뢰할 수 있는 3D 모양입니다. 3D-Future[14], ABO[15][16], GSO[17] 및 OmniObject3D[18]와 같은 다른 데이터 세트는 텍스처 품질을 개선하려고 시도하지만 수천 개의 모델만 포함한다. 최근, Obiayverse[19]는 AI 분야의 연구를 위해 800K 이상의 3D 자산을 포함하고 대규모 3D 데이터셋으로 한 걸음 나아가는 3D 객체의 대규모 코퍼스를 제시하고 있다. Obiayverse-XL[15]는 다양한 소스 세트로부터 10.2M 고유 객체들의 더 큰 3D 데이터세트로 Obiayverse를 더 확장한다. 이러한 대규모 3D 데이터 세트는 대규모 훈련을 용이하게 하고 3D 생성의 성능을 높일 수 있는 잠재력을 가지고 있다.\n' +
      '\n' +
      '다시점 영상에서 학습하는###\n' +
      '\n' +
      '3D 오브젝트는 전통적으로 수동 3D 모델링, 오브젝트 스캐닝, CAD 모델의 변환 또는 이러한 기술의 조합을 통해 생성되었다[16]. 이러한 기술들은 제한된 재구성 정확도를 갖는 특정 객체들의 합성 데이터 또는 실세계 데이터만을 생성할 수 있다. 따라서, 일부 데이터 세트는 많은 3D 생성 방법에서도 널리 사용되는 야생에서 다시점 영상을 직접 제공한다. ScanNet[20]은 1513개의 장면에서 2.5M 뷰를 포함하는 RGB-D 비디오 데이터 세트를 소개하고 Obijectron[1]은 객체 중심의 짧은 비디오를 포함하고 14,819개의 주석이 달린 비디오에서 400만 개의 이미지를 포함하며, 이 중 제한된 수만이 전체 360도를 커버한다. CO3D[17]은 [18]에서 데이터 세트를 확장하고 새로운 뷰 합성 및 3D 생성 또는 재구성 방법의 훈련 및 평가에 널리 사용되는 50개의 MS-COCO 카테고리로부터 객체를 캡처하는 거의 19,000개의 비디오로 크기를 증가시킨다. 최근 MVImgNet[19]은 인간의 일상생활에서 실세계 사물의 동영상을 촬영하여 219,188개의 동영상에서 650만 프레임을 수집하는 대규모 다시점 영상 데이터셋을 제시하고 있다. 다른 작업 라인은 이러한 작업, 대규모 합성 비디오[21] 또는 자기중심적 비디오[22]와 비교하여 소규모 RGB-D 비디오[15, 16]에서 멀티뷰 데이터세트를 제공한다. 대규모 데이터 세트는 딥 러닝 방법, 특히 생성 작업에 대해 여전히 주목할만한 추세입니다.\n' +
      '\n' +
      'Single-view 영상으로부터 학습\n' +
      '\n' +
      '3D 생성 방법은 일반적으로 3D 표현의 재구성 및 생성을 감독하기 위해 다시점 이미지 또는 3D 그라운드 트루스에 의존한다. 단일 시점 영상의 집합만을 이용하여 고품질의 다시점 영상 또는 3차원 형상을 합성하는 것은 어려운 문제이다. 생성적 적대 네트워크의 비감독 훈련에 의해, 자연 이미지로부터 비감독 방식으로 3D 표현을 학습할 수 있는 3D 인식 GAN이 도입된다. 따라서, 이러한 3D 생성 방법에는 여러 개의 단일 시점 영상 데이터 세트가 제안되고 일반적으로 사용된다. 2D 생성을 위해 많은 대규모 이미지 데이터 세트가 제시되었지만 이 문제의 불확실성이 높기 때문에 3D 생성에 직접 사용하기 어렵다. 일반적으로 이러한 이미지 데이터 세트에는 특정 범주 또는 도메인만 포함됩니다. 1024\\({}^{2}\\) 해상도에서 70,000개의 고품질 이미지로 구성된 실제 인간 얼굴 데이터세트인 FFHQ[1], 512\\({}^{2}\\) 해상도에서 15,000개의 고품질 이미지로 구성된 동물 얼굴 데이터세트인 AFHQ[14]를 2D 이미지 합성에 도입하여 3D 인식 GAN 기반 3D 생성에 많이 사용하였다. 인체의 영역에서 3D 인간 세대를 위해 SHHQ[10]와 DeepFashion[15]가 채택되었다. 객체 측면에서 많은 방법[19, 20, 21, 22]은 ShapeNet의 여러 주요 객체 범주를 사용하여 합성 단일 뷰 데이터 세트를 렌더링한다. GRAF[19]가 포토쉐이프[23]에서 150k 의자를 렌더링하는 동안. 또한 CelebA[15] 및 Cats[18] 데이터 세트는 홀로GAN[21] 및 pi-GAN[20]과 같은 모델을 훈련하는 데 일반적으로 사용된다. 단일 시점 이미지는 쉽게 얻을 수 있기 때문에 이러한 방법은 작업에 대한 자체 데이터 세트를 수집할 수 있다.\n' +
      '\n' +
      '## 6 Applications\n' +
      '\n' +
      '본 절에서는 다양한 3D 생성 작업(Sec. 6.1-6.3)과 밀접하게 관련된 3D 편집 작업(Sec. 6.4)을 소개한다. 생성 작업은 3D 인간 생성(Sec. 6.1), 3D 얼굴 생성(Sec. 6.2), 일반 객체 및 장면 생성(Sec. 6.3) 등 세 가지 범주로 나뉜다.\n' +
      '\n' +
      '##3D 인간 세대\n' +
      '\n' +
      '메타버스의 출현과 가상 3D 사회적 상호작용의 발전으로 최근 3D 인간 디지털화 및 생성 분야가 큰 주목을 받고 있다. 샘플 기하학적 구조를 갖는 카테고리 없는 강성 객체에 초점을 맞추는 일반적인 3D 생성 방법[19], LXZ*[21], 대부분의 3D 인간 생성 방법은 관절형 포즈 변화의 복잡성과 의류의 복잡한 기하학적 세부 사항을 해결하는 것을 목표로 한다. 탭 도 5는 생성된 3차원 인체의 입력 조건 및 출력 형식에 따라 정리된, 최근 주목할 만한 3차원 인체 생성 방법의 편찬을 제시한다. 이러한 방법의 일부 결과는 도 8에 도시된다. 구체적으로, 입력 조건의 관점에서, 현재의 3D 인체 생성 방법은 미리 정의된 잠재 공간[18], CJS*[20], HCL*[21], 단일 참조 이미지[19, 17, 18], CIV*[22], HYX*[23], ZLZ*[21], 또는 텍스트 프롬프트[16, 17, 18]로부터 랜덤하게 샘플링된 잠재 특징을 포함하는 구동 인자에 기초하여 분류될 수 있다. 최종 출력의 형태에 따라, 이러한 방법들은 텍스트리스 형상 생성[19],\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Methods & Input Condition & Output Texture \\\\ \\hline ICON [19] & Single-Image & ✗ \\\\ ECON [19] & Single-Image & ✗ \\\\ gDNA [18] & Latent & ✗ \\\\ Chupa [18] & Text/Latent & ✗ \\\\ ELICIT [18] & Single-Image & ✓ \\\\ TeCH [18] & Single-Image & ✓ \\\\ Get3DHuman [18] & Latent & ✓ \\\\ EVA3D [18] & Latent & ✓ \\\\ AvatarCraft [18] & Text & ✓ \\\\ DreamHuman [18] & Text & ✓ \\\\ TADA [18] & Text & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 최근 3D 인간 생성 기술 및 그에 대응하는 입력-출력 포맷.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '##3D 얼굴 생성\n' +
      '\n' +
      '3D 얼굴 생성 태스크의 본질적인 특징 중 하나는 서로 다른 관점에서 볼 수 있는 고품질의 인간 얼굴 이미지를 생성하는 것이다. 인기 있는 작업은 개인화된 머리 아바타 생성(예: 3D 대화 머리 생성), 신경 암시적 3D 변형 모델(3DMM) 및 생성 3D 얼굴 모델을 포함한 세 가지 주요 범주로 느슨하게 분류될 수 있다. 도 9 및 도 9를 참조하여 설명한다. 10.\n' +
      '\n' +
      '**개인화된 머리 아바타 생성**은 말하는 머리 생성과 같은 광범위한 응용을 갖는, 대상 인물의 상이한 관점에서 볼 수 있는 애니메이션 가능한 아바타를 생성하는 것을 목표로 한다. 기존의 방법들은 대부분 비디오 프레임(단안 비디오)[PSB\\({}^{*}\\)21, GTZN21, GPL\\({}^{*}\\)22, ZAB\\({}^{*}\\)22, ZBT23, ZYW\\({}^{*}\\)23, BTH\\({}^{*}\\)23, GZX\\({}^{*}\\)22를 입력으로 한다. 편리하지만, 이들 아바타의 시야각은 비교적 작은 범위(즉, 정면 부근)에서 제한되고, 그들의 품질은 제한된 데이터로 인해 항상 만족스러운 것은 아니다. 이와는 대조적으로, 다른 작품들[LSSS18, MSS\\({}^{*}\\)21, LSS\\({}^{*}\\)21, WKC\\({}^{*}\\)23, KQG\\({}^{*}\\)23]은 보다 큰 각도에서 볼 수 있는 매우 고품질의 디지털 인간을 만드는 것을 목표로 한다. 이러한 방법들은 일반적으로 균일한 조명 하에서 고품질의 동기화된 다시점 영상을 필요로 한다. 그러나, 두 스트림 모두 암시적 또는 하이브리드 신경 표현 및 신경 렌더링 기술에 크게 의존한다. 생성된 대화 헤드 비디오의 품질 및 애니메이션 정확도는 일반적으로 PSNR, SSIM 및 LPIPS 메트릭으로 측정된다.\n' +
      '\n' +
      '**신경 암묵적 3DMMs.** 전통적인 3D 형태 가능 얼굴 모델(3DMMs)은 기하학에 대해 미리 정의된 템플릿 메시(i.g. 고정 토폴로지)를 가정하고 선형 모델(e.g. PCA 기반 3DMMs) 및 비선형 모델(e.g. 네트워크 기반 3DMMs)을 포함한 다양한 모델링 방법을 탐색했다. 이 방법들에 대한 종합적인 조사는 [EST\\({}^{*}\\)20]에서 논의되었다. 최근, 묵시적 신경 표현(INR)의 급속한 발전에 힘입어, 연속적인 묵시적 신경 표현이 얼굴 이산화 오류가 없고 이론적으로 무한한 세부 사항을 모델링할 수 있기 때문에 얼굴 모델링에 INR을 활용하는 여러 개의 신경 묵시적 3DMM이 등장한다[YTB\\({}^{*}\\)21, ZYHC22, GKG\\({}^{*}\\)23]. 실제로, NPHM [GKG\\({}^{*}\\)23]은 이전의 메쉬 기반 3DMM에서 보이지 않는 더 미묘한 표현을 생성할 수 있다. 게다가, 신경 암시적 3DMM은 다양한 헤어스타일의 복잡성이 크게 다르기 때문에 잠재적으로 모발을 더 잘 모델링할 수 있으며, 이는 고정 토폴로지 메쉬 기반 전통적인 3DMM에 큰 도전을 부과한다.\n' +
      '\n' +
      '**생성적 3D 얼굴 모델.**2D 생성적 얼굴 모델(예: StyleGAN[KLA19, KLA\\({}^{*}\\)20])과의 한 가지 주요 차이점은 3D 얼굴 모델은 동일한 타겟(아이덴티티 및 의상)의 멀티 뷰 일관된 이미지(즉, 신규 뷰)를 합성할 수 있다는 것이다. 이 방향을 향한 초기 시도에는 홀로칸[NPLT\\({}^{*}\\)19]과 플라톤GAN[HMR19b]이 있으며, 이는 복셀 기반 방법이며 제한된 해상도로만 이미지를 생성할 수 있다. 신속하게 영상 해상도를 높이기 위해 신경 복사 필드를 이용하는 방법[SLNG20, NG21, CMK\\({}^{*}\\)21b, OELS\\({}^{*}\\)22, GLWT22, CLC\\({}^{*}\\)22]을 제안하였다. 예를 들어, EG3D[CLC\\({}^{*}\\)22]는 기존의 생성 3D GAN들이 직면하는 메모리 및 렌더링 비효율성을 효과적으로 해결하기 위해 좋은 트레이드 오프를 갖는 하이브리드 3-평면 표현을 제안하고, 좋은 멀티-뷰 일관성을 갖는 고품질 이미지를 생성할 수 있다.\n' +
      '\n' +
      '다양한 3D GAN의 성공 덕분에 많은 사람들이 쓰러졌다.\n' +
      '\n' +
      '그림 10: 대표 3D 얼굴 생성 작업. NHA[GPL\\({}^{*}\\)22], NPHM[GKG\\({}^{*}\\)23] 및 EG3D[CLC\\({}^{*}\\)22]로부터 적응된 이미지.\n' +
      '\n' +
      '그림 9: 3D 얼굴 생성의 대표적인 응용 및 방법.\n' +
      '\n' +
      '스트림 애플리케이션(예: 편집, 대화 헤드 생성)은 3D 일관된 편집[14], SWW\\({}^{*}\\)23, SWZ\\({}^{*}\\)22, SWS\\({}^{*}\\)22, LFLSY\\({}^{*}\\)23, JCL\\({}^{*}\\)22], 3D 대화 헤드 생성[15], CSJ\\({}^{*}\\)23, WDY\\({}^{*}\\)22 등을 포함하여 데이터 굶주림이 적거나 활성화된다.\n' +
      '\n' +
      '일반 장면 생성\n' +
      '\n' +
      '기존의 SMPL, 3DMM과 같은 사전 지식을 사용할 수 있는 3D 인간 및 얼굴 생성과는 달리, 일반적인 장면 생성 방법은 3D 모델 생성 프레임워크를 설계하기 위해 의미론이나 카테고리의 유사성에 더 기초한다. 그림 1과 같이 생성 결과의 차이를 기반으로 한다. 11 및 탭. 도 6을 참조하면, 일반 장면 생성을 객체 중심 자산 생성과 외향 장면 생성으로 더 세분화한다.\n' +
      '\n' +
      '객체 중심 자산 생성 6.3.1\n' +
      '\n' +
      '객체 중심 자산 생성 분야는 최근 질감 없는 형태 생성과 질감 있는 자산 생성 모두에 중점을 두고 상당한 발전을 보였다. 질감 없는 형상 생성을 위해 초기 작업은 GAN 기반 네트워크를 사용하여 3D-GAN[16], HoloGAN[17], PlatonicGAN[18]과 같은 3D 데이터의 특정 범주를 기반으로 잠재 공간에서 3D 객체 공간으로 매핑을 학습한다. 그러나, GAN들의 생성 능력들에 의해 제한되는, 이러한 방법들은 특정 카테고리들의 대략적인 3D 자산들을 생성할 수 있을 뿐이다. 생성된 결과의 품질을 향상시키기 위해 SingleShapeGen[19]은 생성기의 피라미드를 활용하여 거칠고 미세한 방식으로 3D 자산을 생성한다. 이미지 생성에서 확산 모델의 놀라운 성과를 감안할 때 연구자들은 3D 생성 영역에서 확산 확장을 적용하는 데 주의를 기울이고 있다. 따라서, 후속 방법 [14, 15, 16, 17, 18]은 랜덤 노이즈로부터 3D 형상 생성을 위한 확산 프로세스의 사용을 탐구한다. 이러한 잠재 기반 방법 외에도 또 다른 중요한 연구 방향은 텍스트 기반 3D 자산 생성[19, 18]이다. 예를 들어, 3D-LDM[17], SDFusion[16], Diffusion-SDF[15]는 3D 특징 공간에서 확산 과정을 설계하여 텍스트 대 3D 형태 생성을 달성한다. 확산 기반 3D 발전기를 훈련하기 위해 3D 데이터 세트가 필요한 이러한 방법으로 인해 생성된 결과의 범주 및 다양성 측면에서 훈련 데이터로 제한된다. 이에 반해 CLIP-Forge[16], CLIP-Sculptor[18], Michelangel[17]은 사전에 학습된 CLIP 모델의 선행을 직접 채용하여 3D 생성 과정을 제약함으로써 방법의 일반화와 생성 결과의 다양성을 효과적으로 개선하였다. 위의 잠재 조건 또는 텍스트 중심의 3D 생성 방법과 달리, 예상되는 형상을 갖는 3D 자산을 생성하기 위해 이미지 또는 스케치 조건 생성을 탐색하는 작업[18, 19]이 있다.\n' +
      '\n' +
      '텍스처가 없는 3D 형상 생성과 비교하여 텍스처링된 3D 자산 생성은 사실적인 기하학적 구조를 생성할 뿐만 아니라 복잡한 텍스처 디테일을 캡처한다. 예를 들어, HoloGAN[17], GET3D[14], 및 EG3D[18]은 카테고리-특정 텍스처링된 3D 자산들을 생성하기 위해 잠재 벡터들에 조건화된 GAN-기반 3D 생성기들을 채용한다. 대조적으로, 텍스트-구동 3D 생성 방법들은 카테고리-프리 3D 자산 생성을 가능하게 하기 위해 사전 훈련된 대규모 텍스트-이미지 모델들의 사전 지식에 의존한다. 예를 들어, CLIP-Mesh[19], Dream Fields[19], 및 PureCLIPNeRF[18]은 최적화 프로세스를 제약하고 텍스트-구동 3D 생성을 달성하기 위해 CLIP 모델 이전의 것을 채용한다. 또한, DreamFusion[19]과 SJC[17]은 사전 학습된 2D 확산 모델로부터 사전 추출된 3D 제약 조건을 달성하기 위한 스코어 증류 샘플링(score distillation sampling, SDS) 방법을 제안한다. 그런 다음 일부 방법은 SDS 기반 3D 생성 프로세스를 매직3D[16], Latent-NeRF[20], Fantasia3D[18], DreamBooth3D[21], HiFA[22], ATT3D[17], ProfilcDreamer[19], IT3D[18], DreamGANisation[20], CAD[17]와 같이 생성 품질, 다중면 문제 및 최적화 효율 측면에서 더욱 개선한다. 한편, 텍스트 기반 3D 생성과 구별되는 단일 영상 조건 3D 생성은 또한 중요한 연구 방향[18], CSJ\\({}^{*}\\)23, MKLRV23, CGC\\({}^{*}\\)23, WLY\\({}^{*}\\)23, KDJ\\({}^{*}\\)23이다.\n' +
      '\n' +
      '외향 장면 생성 6.3.2\n' +
      '\n' +
      '초기 장면 생성 방법들은 종종 GAUDI[1] 및 Xiang_et al._[19]의 작업과 같은 카테고리-특정 장면 생성기를 획득하거나, PixelSynth[16] 및 Worldsheet[18]와 같은 입력 이미지에 기초하여 단일 장면 재구성을 구현하기 위해 트레이닝을 위한 특정 장면 데이터를 필요로 한다. 그러나, 이러한 방법들은 생성의 품질 또는 장면의 확장성에 의해 제한된다. 영상 인페인팅에서 확산 모델의 등장으로 확산 모델의 장면 완성 능력을 이용하여 장면 생성 작업 [18], HCO\\({}^{*}\\)23, ZLW\\({}^{*}\\)23을 구현하기 위한 다양한 방법들이 등장하고 있다. 최근 SceneScape[1], Text2Room[19], Text2NeRF[17], LucidDreamer[16]은 사전 학습된 확산 모델을 이용하여 사실적인 3D 장면을 생성하기 위한 점진적 인페인팅 및 업데이트 전략을 제안한다. SceneScape 및 Text2 Room의 명시적 폴리곤 메쉬 활용\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Methods & Type & Condition &\n' +
      '\\begin{tabular}{c} Texture \\\\ Generation \\\\ \\end{tabular} \\\\ \\hline PVD [15] & Object-Centered & Latent & ✗ \\\\ NPD [18] & Object-Centered & Latent & ✗ \\\\ Point-E [18] & Object-Centered & Text & ✗ \\\\ Diffusion-SDF [15] & Object-Centered & Text & ✗ \\\\ Deep3DSketch+ [18] & Object-Centered & Sketch & ✗ \\\\ Zero-1-to-3 [18] & Object-Centered & Single-Image & ✓ \\\\ Make-It-3D [18] & Object-Centered & Single-Image & ✓ \\\\ GET3D [18] & Object-Centered & Latent & ✓ \\\\ EG3D [18] & Object-Centered & Latent & ✓ \\\\ CLIP-Mesh [19] & Object-Centered & Text & ✓ \\\\ DreamFusion [19] & Object-Centered & Text & ✓ \\\\ ProfilcDreamer [18] & Object-Centered & Text & ✓ \\\\ PixelSynth [16] & Outward-Facing & Single-Image & ✓ \\\\ DiffcDreamer [18] & Outward-Facing & Single-Image & ✓ \\\\ Xiang _et al._[19] & Outward-Facing & Latent & ✓ \\\\ CC3D [18] & Outward-Facing & Layout & ✓ \\\\ Text2Room [19] & Outward-Facing & Text & ✓ \\\\ Text2NeRF [17] & Outward-Facing & Text & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 일반적인 장면 생성 방법의 응용예.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      '로컬 편집 유형에는 외형 조작[YBZ\\({}^{*}\\)22, ZWL\\({}^{*}\\)23], 기하 변형[JKK\\({}^{*}\\)23, PYL\\({}^{*}\\)22, YSL\\({}^{*}\\)22, TLYCS22], 객체-/의미 수준 중복/삭제 및 이동/제거[YZX\\({}^{*}\\)21, WL\\({}^{*}\\)22, KMS22, WWL\\({}^{*}\\)23]이 있다. 예를 들어, NeuMesh[YBZ\\({}^{*}\\)22]는 NeRF 장면을 메쉬 기반 신경 표현으로 확장하기 때문에 스와핑, 채우기, 페인팅을 포함한 여러 종류의 텍스처 조작을 지원한다. NeRF-Shop[JKK\\({}^{*}\\)23]과 CageNeRF[PYL\\({}^{*}\\)22]는 메쉬 케이지로 둘러싸인 볼륨을 변환/변형시켜 이동 또는 변형/관절화된 객체를 생성한다. SINE[BZY\\({}^{*}\\)23]은 NeRF 기하학과 외형을 기하학 사전 및 의미론적(이미지 특징) 텍스처로 정규화하여 업데이트한다.\n' +
      '\n' +
      '다른 작업 라인(예: ObjectNeRF[YZX\\({}^{*}\\)21], ObjectSDF[WLC\\({}^{*}\\)22], DFF[KMS22])은 재구성 동안 장면을 개별 객체 또는 의미 부분으로 자동 분해하는 데 초점을 맞추고, 이는 추가 2D 이미지 이해 네트워크(예: 인스턴스 분할)를 활용하여 가능하며, 재컬러링, 제거, 변위, 중복과 같은 후속 객체 수준 조작을 지원한다.\n' +
      '\n' +
      '최근, 대규모의 텍스트-이미지 모델(예: Stable Diffusion [RBL\\({}^{*}\\)22a])의 성공으로 인해 기존의 3D 장면들에서 텍스트 설명에 따라 새로운 텍스처 및/또는 콘텐츠를 생성하는 것이 가능하다. 예를 들어, 지시-NeRF2NeRF[HTE\\({}^{*}\\)23]는 전용 확산 모델[BHE23] 및 NeRF 모델에 의해 수정된 참조 데이터세트 이미지들을 반복적으로 업데이트한다. DreamEditor [ZWL\\({}^{*}\\)23]은 스코어 증류 샘플링[PJBM23]에 의해 안내된 텍스트 어텐션에 의해 위치된 영역에 대한 로컬 업데이트를 수행한다. Focal-Dreamer[LDS\\({}^{*}\\)23]은 텍스트 입력에 따라 지정된 빈 공간에 새로운 기하학(객체)을 생성한다. SKED [MPS\\({}^{*}\\)23]은 새로운 객체를 생성하고, 제공된 다시점 스케치에 의해 지정된 영역에 위치한 기존 부분을 수정하는 것을 모두 지원한다.\n' +
      '\n' +
      '## 7 열린 도전\n' +
      '\n' +
      '3D 생성 결과의 품질과 다양성은 생성 모델, 3D 표현 및 알고리즘 패러다임의 발전으로 인해 상당한 진전을 경험했다. 최근 자연어 처리 및 이미지 생성에서 대규모 모델이 달성한 성공으로 인해 3D 생성에 상당한 관심이 집중되고 있다. 그러나 생성된 3D 모델이 VR/AR에서 비디오 게임, 영화 또는 몰입형 디지털 콘텐츠에 필요한 높은 산업 표준을 충족하기 전에 수많은 과제가 남아 있다. 이 절에서는 이 분야의 몇 가지 열린 도전과 잠재적인 미래 방향을 탐색할 것이다.\n' +
      '\n' +
      '**평가.** 생성된 3D 모델의 품질을 객관적으로 정량화하는 것은 중요하고 널리 탐구되지 않은 문제이다. 렌더링 및 재구성 결과를 평가하기 위해 PSNR, SSIM, F-Score 등의 메트릭을 사용하면 한편으로는 그라운드 트루스 데이터가 필요하지만, 다른 한편으로는 생성된 콘텐츠의 품질과 다양성을 종합적으로 반영할 수 없다. 또한, 사용자 연구는 일반적으로 시간이 많이 소요되며, 연구 결과는 조사된 사용자의 편향과 수에 영향을 받는 경향이 있다. FID와 같은 결과의 품질과 다양성을 모두 캡처하는 메트릭은 3D 데이터에 적용될 수 있지만, 항상 3D 도메인 및 인간 선호도와 정렬되지는 않을 수 있다. 생성 품질, 다양성 및 조건과의 일치 정도 측면에서 결과를 객관적으로 판단하기 위한 더 나은 메트릭은 여전히 추가 조사가 필요하다.\n' +
      '\n' +
      '**Dataset.** 쉽게 캡처되고 수집될 수 있는 언어 또는 2D 이미지 데이터와 달리, 3D 자산은 종종 3D 아티스트 또는 디자이너가 전문 소프트웨어를 사용하여 상당한 시간을 소비해야 한다. 또한, 다양한 사용 시나리오와 크리에이터의 개인 스타일로 인해 이러한 3D 자산은 규모, 품질 및 스타일이 크게 다를 수 있으므로 3D 데이터의 복잡성이 증가한다. 이 다양한 3D 데이터를 정규화하기 위해서는 구체적인 규칙이 필요하므로 생성 방법에 더 적합하다. 대규모 고품질 3D 데이터 세트는 3D 생성에서 여전히 매우 바람직하다. 한편, 3D 생성을 위해 광범위한 2D 데이터를 활용하는 방법을 탐색하는 것도 3D 데이터의 희소성을 해결하는 잠재적인 해결책이 될 수 있다.\n' +
      '\n' +
      '**Representation.** Representation은 Sec. 3에서 다양한 표현 및 관련 방법에 대해 논의하는 바와 같이 3D 생성의 필수적인 부분이다. 암시적 표현은 복잡한 기하학적 토폴로지를 효율적으로 모델링할 수 있지만 느린 최적화의 어려움에 직면한다; 명시적 표현은 빠른 최적화 수렴을 촉진하지만 복잡한 토폴로지를 캡슐화하기 위한 투쟁을 촉진하고 실질적인 스토리지 리소스를 요구하며; 하이브리드 표현은 이들 둘 사이의 트레이드-오프(trade-off)를 고려하기 위해 시도되지만, 이 두 가지 방법 중 하나가 있다.\n' +
      '\n' +
      '도 12: 대표적인 3D 편집 작업. ARF[ZKB\\({}^{*}\\)22], Text2Mesh[MBOL\\({}^{*}\\)22], NeRFShop[JKK\\({}^{*}\\)23], SKED[MPS\\({}^{*}\\)23], DreamEditor[ZWL\\({}^{*}\\)23]에서 각색된 이미지.\n' +
      '\n' +
      '여전히 부족한 점. 일반적으로 최적화 효율성, 기하학적 토폴로지 유연성, 자원 사용의 균형을 맞추는 표현을 개발하고자 한다.\n' +
      '\n' +
      '**제어 가능.** 3D 생성 기법의 목적은 값싸고 제어 가능한 방식으로 사용자 친화적이고, 고품질이며, 다양한 3D 콘텐츠를 대량으로 생성하는 것이다. 그러나, 생성된 3D 콘텐츠를 실제 애플리케이션들에 임베딩하는 것은 과제로 남아 있다: 대부분의 방법들 [4, 15]은 볼륨 렌더링 또는 신경 렌더링에 의존하며, 래스터화 그래픽 파이프라인에 적합한 콘텐츠를 생성하지 못한다. 다각형으로 표현된 콘텐츠를 생성하는 방법 [16, 17]에 대해서는 레이아웃(예를 들어, 테이블의 직사각형 평면은 두 개의 삼각형으로 표현될 수 있음) 및 고품질 UV 언랩핑(UV unwrapping)을 고려하지 않으며 생성된 텍스처는 또한 검은색 그림자와 같은 일부 문제에 직면한다. 이러한 문제는 생성된 콘텐츠를 아티스트 친화적인 상호 작용과 편집에 불리하게 만든다. 또한, 생성된 콘텐츠의 스타일은 여전히 트레이닝 데이터 세트에 의해 제한된다. 또한, 포괄적인 도구 사슬의 구축은 3D 세대의 실질적인 구현의 중요한 측면이다. 현대 워크플로우에서 아티스트는 다양한 조명 조건에 걸쳐 재료의 올바른 결과를 조사하고 대조함으로써 3D 콘텐츠를 조화시키기 위해 도구(예: LookDev)를 사용한다. 동시에, 현대 디지털 콘텐츠 생성(DCC) 소프트웨어는 광범위하고 세분화된 콘텐츠 편집 기능을 제공합니다. 다양한 방법을 통해 제작된 3D 콘텐츠를 통일하고 풍부한 편집 역량을 아우르는 툴 체인을 구축하는 것이 유망하다.\n' +
      '\n' +
      '**대규모 모델.** 최근 대규모 모델의 인기가 점차 3D 세대 분야에 영향을 미치고 있다. 연구자들은 더 이상 3D 콘텐츠를 최적화하기 위해 대규모 이미지 모델을 전조로 사용하는 증류 점수를 사용하는 것에 만족하지 않고 대규모 3D 모델을 직접 훈련한다. MeshGPT[2]는 큰 언어 모델을 따르고 생성된 메쉬에서 삼각형의 시퀀스를 자동으로 생성하기 위해 시퀀스 기반 접근법을 채택한다. MeshGPT는 레이아웃 정보를 고려하여 아티스트가 만든 스타일에 맞는 컴팩트하고 선명한 메쉬를 생성합니다. MeshGPT는 디코더 전용 변압기이기 때문에 최적화 기반 생성에 비해 비효율적인 다단계 순차 최적화를 제거하여 빠른 생성을 달성한다. 그럼에도 불구하고 MeshGPT의 성능은 학습 데이터셋에 의해 여전히 제한적이며 일반 가구 객체만 생성할 수 있다. 그러나 대규모 3D 생성 모델이 탐험할 가치가 있다는 것은 의심의 여지가 없다.\n' +
      '\n' +
      '## 8 Conclusion\n' +
      '\n' +
      '본 연구에서는 3D 표현, 생성 방법, 데이터 세트 및 다양한 응용의 네 가지 주요 측면을 포함하는 3D 생성에 대한 포괄적인 조사를 제시한다. 우리는 백본 역할을 하고 생성된 결과의 특성을 결정하는 3D 표현을 도입하는 것으로 시작한다. 다음으로 광범위한 생성 방법을 요약하고 분류하여 진화 트리를 생성하여 분기 및 개발을 시각화한다. 마지막으로 관련 데이터 세트, 응용 프로그램 및 이 분야의 개방형 문제에 대한 개요를 제공한다. 3D 세대의 영역은 현재 폭발적인 성장과 발전을 목격하고 있으며, 매주 또는 심지어 매일 새로운 작업이 등장한다. 우리는 이 조사가 관심 있는 독자들에게 후속 작업에 영감을 줄 수 있는 체계적인 요약을 제공하기를 바란다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[ACC\\({}^{*}\\)22]Adamkiewicz M., Chen T., Caccavale A., Gardner R., Culbertson P., Bohg J., Schwager M.: Vision-only robot navigation in neural radiance world. _ IEEE Robotics and Automation Letters 7_, 2(2022), 4606-4613.\n' +
      '*[ADMG18]Achlioptas P., Diamanti O., Mitliagkas I., Guibas L.: Learning representations and Generative models for 3d point cloud. In _International conference on machine learning_ (2018), PMLR, pp.40-49.\n' +
      '*[ALG\\({}^{*}\\)20]Attal B., Ling S., Gokaslan A., Richardt C., Tomp-Kink J.: Matryodshka: Real-time 6d video view synthesis using multi-sphere images. In _European Conference on Computer Vision_ (2020), Springer, pp. 441-459.\n' +
      '*[APMTM19]Alldieck T., Pons-Moll G., Theobalt C., Magnor M.: Tex2shape: 단일 이미지로부터 상세한 전체 인체 기하학. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2019), pp. 2293-2303.\n' +
      '*[AQW19]Abmal R., Qin Y., Wonka P.: Image2stylegan: Stylegan 잠재 공간에 이미지를 삽입하는 방법. In _Proceedings of the IEEE/CVF international conference on computer vision_(2019), pp.4432-4441.\n' +
      '*[ASK\\({}^{*}\\)20]Aliev K. - A., Sevastopolsky A., Kolos M., Ulyanov D., Lempitsky V.: Neural Point-based graphics. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXII 16_(2020), Springer, pp. 696-712.\n' +
      '*[AST\\({}^{*}\\)23]AlBahar B., Saito S., Tseng H.-Y., Kim C., Kopf J., Huang J.-B.: 형상 유도 확산을 갖는 단일 이미지 3d 인간 디지털화. _SIGGRAPH Asia 2023 Conference Papers_(2023), pp. 1-11.\n' +
      '* [ATDN23]Aneja S., Thies J., Dai A., Niessner M.: ClipFace: Textured 3d morphable model의 text-guided editing. In _ACM SIGGRAPH 2023 Conference Proceedings_(2023).\n' +
      '*[AYS\\({}^{*}\\)23]Abdal R., Yiew W., Shi Z., Xu Y., Po R., Kuang Z., Chen Q., Young D.-Y., Wetzstein G.: Gaussian shell map for efficient 3d human generation. _ arXiv preprint arXiv:2311.17857_(2023).\n' +
      '*[AZA\\({}^{*}\\)21]Ahmadyan A., Zhang L., Ablavatski A., Wei J., Grundmann M.: Objectron: 포즈 주석이 있는 야생에서 객체 중심 비디오의 대규모 데이터 세트. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2021), pp.7822-7831.\n' +
      '*[AZS22]Alldieck T., Zanfir M., Sminchisescu C.: 옷을 입은 인간의 광실체 단안 3d 재구성. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 1506-1515.\n' +
      '*[BBJ\\({}^{*}\\)21]Boss M., Braun R., Jampani V., Barron J. T., Liu C., Lensch H.: Nerd: Neural reflectance decomposition from image collection. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 12684-12694.\n' +
      '*[BFO\\({}^{*}\\)20]Broxton M., Flynn J., Overbeck R., Erickson D., Hebman D., Duvali M., Dotozarian J., Busch J., Whalen M., Debevec P.: layered mesh representation을 갖는 Immersive light field video. _ ACM Transactions on Graphics (TOG) 39_, 4 (2020), 86-1.\n' +
      '*[BFW\\({}^{*}\\)23]Bai Y., Fan Y., Wang X., Zhang Y., Sun J., Yuan C., Shan Y.: 생성 전과가 있는 단안 비디오로부터 고 충실도 얼굴 아바타 재구성. _CVPR_(2023)에서.\n' +
      '\n' +
      '*[BGA*22]Bautista M. A., Guo P., Abnar S., Talabott W., Toshev A., Chen Z., Dinh L., Zhai S., Goh H., Ulbricht D., et al.: Gaudi: 몰입형 3d 장면 생성을 위한 신경 아키텍쳐. _ 신경 정보 처리 시스템(35_(2022), 25102-25116)에서의 발전.\n' +
      '*[BGP*22]Baatz H., Granskog J., Papas M., Rousselle F., Novak J.: Nerf-tex: Neural reflectance field textureures. In _Computer Graphics Forum_(2022), vol. 41, Wiley Online Library, pp. 287-301.\n' +
      '*[BHE23]Brooks T., Holynski A., Efros A. A.: InstructPix2Pix: 이미지 편집 지시를 따르는 학습. _CVPR_(2023)에서.\n' +
      '*[BHMK*18]Ben-Hamid u., Maron H., Kezurer I., Avineri G., Lipman Y.: Multi-chart Generative Surface Modeling. _ ACM Transactions on Graphics (TOG) 37_, 6 (2018), 1-15.\n' +
      '*[BKP*10]Botsch M., Kobbelt L., Pauly M., Alliez P., Levy B.: _Polygon mesh processing_. CRC 프레스 2010년\n' +
      '*[BKY*22]Bergman A., Kellnhofer P., Yifan W., Chan E., Lindell D., Weetzstein G.: Generative neural articulated radiance fields. _ 신경 정보 처리 시스템(35_(2022), 19900-19916의 발전.\n' +
      '*[BLW16]Brock A., Lim T., Weston N.: 합성곱 신경망을 이용한 생성 및 판별 복셀 모델링_ arXiv preprint arXiv:1608.04236_(2016).\n' +
      '*[BMR*20]Brown T., Mann B., Ryder N., Subbiah M., Kaplan J. D., Dhariwal P., Neelakantan A., Shyam P., Sastry G., Askell A., et al.: 언어 모델은 소수의 학습자를 포함한다. _ NeurIPS_(2020).\n' +
      '*[BMD*21]Barron J. T., Mildenhall B., Tancik M., Hedman P., Martin-Brualla R., Srinivasan P. P. P.: Mip-nerf: anti-aliasing neural radiance field에 대한 멀티스케일 표현. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 5855-5864.\n' +
      '*[BMV*22]Barron J. T., Mildenhall B., Verbin D., Srinivasan P. P., Hedman P.: Mip-nerf 360: Unbounded anti-aliased neural radiance field. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 5470-5479.\n' +
      '*[BNT21]Burov A., Niessner M., Thies J.: 옷 입은 인체를 위한 동적 표면 기능 네트워크. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 10754-10764.\n' +
      '*[BPP*23]Bahmani S., Park J. J., Paschalidou D., Yan X., Wetzstein G., Guibas L., Tagliascach A.: Cc3d: Layout-conditioned generation of compositional 3d scenes. _ arXiv preprint arXiv:2303.12074_(2023).\n' +
      '*[BSKG22]Ben-Shabar Y., Koneputuogdage C. H., Gould S.: Digs: Divergence guided shape implicit neural representation for oriented point cloud. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 19323-19332.\n' +
      '*[BTH*23]Bai Z., Tan F., Huang Z., Sarkar K., Tang D., Qiu D., Meka A., Du R., Dou M., Orts-Escolano S., et al. : 단안 rgb 비디오로부터 개인화된 고품질 체적 헤드 아바타를 학습. _CVPR_(2023)에서.\n' +
      '*[BZY*23]Bao C., Zhang Y., Yang B., Fan T., Yang Z., Bao H., Zhang G., Cui Z.: SINE: Semantic-driven image-based nef editing with prior-guided editing field. _CVPR_(2023)에서.\n' +
      '*[CBZ*19]Cheng S., Bronstein M., Zhou Y., Kotsia I., Pantic M., Zafehiou S.: Mesbrian: Nonlinear 3d morphable model of face. _ arXiv preprint arXiv:1903.10384_(2019).\n' +
      '*[CCH*23]Cao Y., Cao Y. - P., Han K., Shan Y., Wong K. - Y. K.: Dreamavatar: Text-and shape guided 3d human avatar generation via diffusion models. _ arXiv preprint arXiv:2304.00916_(2023).\n' +
      '*[CCJJ23]Chen R., Chen Y., Jiao N., Jia K.: Fantasia3D: Disentangling geometry and appearance for high-quality text-to-3d content creation. _ICCV_(2023)에서.\n' +
      '* [CCP*23]Cai S., Chan E. R., Peng S., Shahbazi M., Obukhov A., Van Gool L., Wetzstein G.: Diffreramer: 조건부 확산 모델을 사용한 일관된 감독되지 않은 단일 뷰 장면 외삽을 향한. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2023), pp. 2139-2150.\n' +
      '*[CCS*19]Chen K., Choy C. B., Savva M., Chang A. X., Funkhouser T., Savarese S.:Text2shape: joint embedding을 학습하여 자연 언어로부터 모양을 생성한다. In _Computer Vision-ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2-6, 2018, Revised Selected Papers, Part III 14_(2019), Springer, pp. 100-116.\n' +
      '*[CCW*23]Chen Y., Chen X., Wang X., Zhang Q., Guo Y., Shan Y., Wang F.: bundle-adjusting neural radiance field에 대한 Local-to-global registration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 8264-8273.\n' +
      '*[CFG*15]Chang A. X., Funkhouser T., Guibas L., Hanrahan P., Huang Q., Li Z., Savarese S., Savva M., Song S., Su H., et al.: Shapenet: An information-rich 3d model repository. _ arXiv preprint arXiv:1512.03012_(2015).\n' +
      '*[CFZ*23]Chen T., Fu C., Zang Y., Zhu L., Zhang J., Mao P., Sun L.: Deep3dketch+: 단일 자유손 스케치로부터의 신속한 3d 모델링. In _International Conference on Multimedia Modeling_ (2023), Springer, pp. 16-28.\n' +
      '*[CGC**23]Chen H., Gu J., Chen A., Tian W., Tu Z., Liu L., Su H.: Single-stage diffusion nerf: Unified approach to 3d generation and reconstruction. _ arXiv preprint arXiv:2304.06714_(2023).\n' +
      '*[CGD*22]Collins J., Goel S., Deng K., Luthra A., Xu L., Gundogdu E., Zhang X., Vicente T. F. Y., Dideriksen T., Arora H., et al.: Abo: Dataset and benchmarks for real-world 3d object understanding. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 21126-21136.\n' +
      '*[CGT*19]Choi I., Gallo O., Troccoli A., Kim M. H., Kautz J.: Extreme view synthesis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2019), pp. 7781-7790.\n' +
      '*[CHB*23]Chen X., Huang J., Bin Y., Yu L., Liao Y.: Veri3d: Generative vertex-based radiance field for 3d controllable human image synthesis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2023), pp. 8986-8997.\n' +
      '* [CHIS23]Crotoru F.-a., Hondur V., Ionescu R. T., Shah M.: Diffusion model in vision:A survey. _ IEEE Transactions on Pattern Analysis and Machine Intelligence_ (2023).\n' +
      '*[CJS*22]Chen X., Jiang T., Song J., Yang J., Black M. J., Geiger A., Hillings O.: gdna: Towards generatingative detailed neural avatars. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 20427-20437.\n' +
      '*[CLC*22]Chan E. R., Lin C. Z., Chan M. A., Nagano K., Pan B., De Mello S., Gallo O., Guibas L. J., Tremblay J., Khamis S., et al.: Efficient geometry-aware 3d generative adversarial networks. _CVPR_(2022)에서.\n' +
      '*[CLL23]Cheng Z., Li J., Li H.: Wildlight: In-the-wild inverse rendering with a flashlight. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 4305-4314.\n' +
      '*[CLN*23]Chung J., Lee S., Nam H., Lee J., Lee K. M.: Lucid-dreamer: Domain-free generation of 3d gaussian splatting scenes. _ arXiv preprint arXiv:2311.13384_(2023).\n' +
      '*[CLT*23]Cheng Y. - C., Lee H.-Y., Tulyakov S., Schwing A. G., Gui L. - Y.: 소퓨전: 멀티모달 3d 형상 완성, 재구성 및 생성. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 4456-4465.\n' +
      '*[CMA*22]Choi H., Moon G., Armando M., Leroy V., Lee K. M., Rogez G.: Mononhr: Monocular neural human renderer. In _2022 International Conference on 3D Vision(3DV)_(2022), IEEE, pp. 242-251.\n' +
      '\n' +
      '*[CMK\\({}^{*}\\)21a] Chan E. R., Monteiro M., Kellnhofer P., Wu J., Wetzstein G.: pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2021), pp. 5799-5809.\n' +
      '*[CMK\\({}^{*}\\)21b] Chan E. R., Monteiro M., Kellnhofer P., Wu J., Wetzstein G.: pi-GAN: Periodic implicit generative adversarial networks for 3d-aware image synthesis. _CVPR_(2021)에서.\n' +
      '*[CNC\\({}^{*}\\)23] Chan E. R., Nagano K., Chan M. A., Bergman A. W., Park J. J., Levy A., Mittala M., De Mello S., Karras T., Wetzstein G.: Generative novel view synthesis with 3d-aware diffusion models. _ arXiv preprint arXiv:2304.02602_(2023).\n' +
      '*[CPA\\({}^{*}\\)21] 코로나 E., Pumarola A., Alenya G., Pons-Moll G., Moreno-Noguer F.: Explicit: Topology-aware Generative Model for clotped people. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2021), pp.11875-11885.\n' +
      '*[CPB\\({}^{*}\\)20] Choutas V., Pavlakos G., Bolkart T., Tzionas D., Black M. J.: Body-driven attention을 통한 단안 표현체 회귀. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part X 16_(2020), Springer, pp. 20-40.\n' +
      '*[CRJ22] Cao A., Rockwell C., Johnson J.: Fwd: Forward Warping 및 depth를 갖는 실시간 신규 뷰 합성. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 15713-15724.\n' +
      '*[CSH19] Chen X., Song J., Hilliges O.: 연속 뷰 제어를 갖는 단안 신경 이미지 기반 렌더링. In _Proceedings of the IEEE/CVF international conference on computer vision_(2019), pp. 4090-4100.\n' +
      '*[CSL\\({}^{*}\\)23] Chen D. Z., Siddiqui Y., Lee H.-Y., Tulyakov S., Niessner M.:TextText:Text-driven texture synthesis via diffusion models. _ICCV_(2023)에서.\n' +
      '*[CTZ20] Chen Z., Tagliasacchi A., Zhang H.: Bsp-net: 이진 공간 분할을 통해 콤팩트 메시를 생성한다. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp.45-54.\n' +
      '*[CUYH20] Choi Y., Uh Y., Yoo J., Ha J.-W.: Stargan v2: 여러 도메인에 대한 다양한 이미지 합성. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2020), pp. 8188-8197.\n' +
      '*[CWS\\({}^{*}\\)15] Calli B., Walsman A., Singh A., Srinivasa S., Abbeel P., Dollar A. M.: Benchmarking in manipulation research: ycb object and model set and benchmarking protocol. _ arXiv preprint arXiv:1502.03143_(2015).\n' +
      '*[CX\\({}^{*}\\)23] Chao Y. - W., Xiang Y., et al.: Fewsol: A dataset for few-shot object learning in robotic environments. _2023 IEEE International Conference on Robotics and Automation (ICRA)_ (2023), IEEE, pp. 9140-9146.\n' +
      '*[CXG\\({}^{*}\\)16] Choy C. B., Xu D., Gwak J., Chen K., Savarese S.: 3d-2m2: 단일 및 다시점 3d 객체 재구성을 위한 통일된 접근법. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VII 14_(2016), Springer, pp. 628-644.\n' +
      '*[CXG\\({}^{*}\\)22] Chen A., Xu Z., Geiger A., Yu J., Su H.: TensoRF: Tensorial radiance field. In _European Conference on Computer Vision_ (2022).\n' +
      '*[CXZ\\({}^{*}\\)21] Chen A., Xu Z., Zhao F., Zhang X., Xiang F., Yu J., Su H.: MVSNeRF: Fast generalizable radiance field reconstruction from multi-view stereo. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14124-14133.\n' +
      '*[CYAE\\({}^{*}\\)20] Cai R., Yang G., Averbuch-Elor H., Hao Z., Belongie S., Snavely N., Hariharan B.: 형상 생성을 위한 기울기 필드 학습. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16_(2020), Springer, pp. 364-381.\n' +
      '*[CYL\\({}^{*}\\)22] Chen Y., Yuan Q., Li Z., Liu Y., Wang W., Xie C., Wen X., Yu Q.: UPST-NeRF: Universal photoreistic style transfer of neural radiance fields for 3d scene. _arXiv preprint arXiv:2208.07059_(2022).\n' +
      '*[CYW\\({}^{*}\\)23] Cheng X., Yang T., Wang J., Li Y., Zhang L., Zhang J., Yuan L.: Progressive3D: 복잡한 의미 프롬프트를 갖는 텍스트-투-3d 콘텐츠 생성을 위한 점진적인 로컬 편집. _ arXiv preprint arXiv:2310.11784_(2023).\n' +
      '*[CZ19] Chen Z., Zhang H.: 생성 모양 모델링을 위한 암시적 필드 학습. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2019), pp. 5939-5948.\n' +
      '*[CZC\\({}^{*}\\)24] Chen H., Zhang Y., Cun X., Xia M., Wang X., Weng C., Shan Y.: Videocrafer2: Overcoming data limitations for high-quality video diffusion models, 2024. arXiv:2401.09047.\n' +
      '*[CZL\\({}^{*}\\)22] Chen X., Zhang Q., Li X., Chen Y., Feng Y., Wang X., Wang J.: Hallucinated neural radiance field in the wild. _CVPR_(2022)에서, pp. 12943-12952.\n' +
      '*[CZY\\({}^{*}\\)23] Chen Y., Zhang C., Yang X., Cai Z., Yu G., Yang L., Lin G.: I3d: explicit view synthesis with improved text-to-3d generation. _ arXiv preprint arXiv:2308.11473_(2023).\n' +
      '*[DBD\\({}^{*}\\)22] Darmon F., Bascle B., Devaux J.-C., Monasse P., Aubry M.: 패치 와핑으로 신경 암시적 표면 기하학을 개선한다. _CVPR_(2022)에서.\n' +
      '*[DCS\\({}^{*}\\)17] Dai A., Chang A. X., Savva M., Halber M., Funkhouser T., Niessner M.: Scannet: Richly-annotated 3d reconstructions of indoor scene. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2017), pp. 5828-5839.\n' +
      '*[DFK\\({}^{*}\\)22] Downs L., Francis A., Koenig N., Kinman B., Hickman R., Reymann K., McHugh T. B., Vanhoucke V.: Google scanned objects: 3d scanned household items의 고품질 데이터셋. In _2022 International Conference on Robotics and Automation (ICRA)_ (2022), IEEE, pp. 2553-2560.\n' +
      '*[DJQ\\({}^{*}\\)23] Deng C., Jiang C., Qi C. R., Yan X., Zhou Y., Guibas L., Anguelov D., et al.: Nerdl: Single-view nerf synthesis with language-guided diffusion as general image priors. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 20637-20647.\n' +
      '*[DLW\\({}^{*}\\)23] Deitke M., Liu R., Nallingford M., Ngo H., Michel O., Kusupati A., Fan A., Laforte C., Voleti V., Gadre S. Y., et al.: Obiayverse-xl: 10m+ 3d 객체들의 우주. _ arXiv preprint arXiv:2307.05663_(2023).\n' +
      '*[Doe16]Doersch C.: 튜토리얼 on variational autoencoders. _ arXiv preprint arXiv:1606.05908_(2016).\n' +
      '*[DRB\\({}^{*}\\)18] Dai A., Ritchie D., Bokeloh M., Reed S., Sturm J., Niessner M.: Scannet: Large-scale scene completion and semantic segmentation for 3d scan. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_ (2018), pp.4578-4587.\n' +
      '*[DSS\\({}^{*}\\)23] Deitke M., Schwenk D., Salvador J., Weihs L., Michel O., Vanderbilt E., Schmidt L., Ehsani K., Kembhavi A., Farhadi A.: Obiayverse: A universe of annotated 3d objects. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp.13142-13153.\n' +
      '*[DZL\\({}^{*}\\)20] Dai P., Zhang Y., Li Z., Liu S., Zeng B. : 다중 평면 투영을 통한 신경점 구름 렌더링. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp.7830-7839.\n' +
      '*[DZW\\({}^{*}\\)20] Duan Y., Zhu H., Wang H., Yi L., Nevatia R., Guibas L. J.: Curriculum deepsdf. _European Conference on Computer Vision_ (2020), Springer, pp.51-67.\n' +
      '*[DZY\\({}^{*}\\)21] Du Y., Zhang Y., Yu H.-X., Tenenbaum J. B., Wu J.: Neural Radiance flow for 4d view synthesis and video processing. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), IEEE Computer Society, pp. 14304-14314.\n' +
      '\n' +
      '*[EGO\\({}^{*}\\)20]Erler P., Guerrero P., Ohrhallinger S., Mitra N. J., Wimmer M.: Points2surf learning implicit surface from point cloud. In _European Conference on Computer Vision_ (2020), Springer, pp. 108-124.\n' +
      '*[EMS\\({}^{*}\\)23]Erkoc Z., Ma F., Shan Q., Niessner M., Dai A: Hyperdiffusion: Weight-space diffusion을 갖는 암시적 신경장 생성. _ arXiv preprint arXiv:2303.17015_(2023).\n' +
      '*[EST\\({}^{*}\\)20]Egger B., Smith W. A., Tewari A., Wuher S., Zollhofer M., Beeler T., Bernard F., Bolkart T., Kortylewski A., Romdhani S., et al.: 3d morphable face models--past, present, and future. _ ACM Trans. Graph.__ (2020).\n' +
      '*[FAKD23]Fridman R., Abecasis A., Kasten Y., Dekel T.: Secesnexer: Text-driven consistent scene generation. _ arXiv preprint arXiv:2302.01133_(2023).\n' +
      '*[FBD\\({}^{*}\\)19]Flynn J., Broxton M., Debevec P., DuVall M., Fyffe G., Overbeck R., Snavely N., Tucker R.: Decepview: learned gradient descent를 이용한 view synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2019), pp. 2367-2376.\n' +
      '*[FJG\\({}^{*}\\)21]Fu H., Jia R., Gao L., Gong M., Zhao B., Maybank S., Tao D: 3d-future: 3d furniture shape with texture. _ International Journal of Computer Vision 129_(2021), 3313-3337.\n' +
      '*[FJW\\({}^{*}\\)22]Fan Z., Jiang Y., Wang P., Gong X., Xu D., Wang Z.: Unified implicit neural stylization. _ECCV_(2022)에서, Springer.\n' +
      '*[FKYT\\({}^{*}\\)22]Fridovich-Keil S., Yu A., Tancik M., Chen Q., Recht B., Kanazawa A.: Plenoxels: Radiance field without neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 5501-5510.\n' +
      '*[FLJ\\({}^{*}\\)22]Fu J., Li S., Jiang Y., Lin K. - Y., Qian C., Loy C. C., Wu W., Liu Z.: Stylegan-human: A data-centric odyssey of human generation. _European Conference on Computer Vision_ (2022), Springer, pp. 1-19.\n' +
      '*[FKOT22]Fu Q., Xu Q., Ong Y. S., Tao W.: Geo-NeuS: Geometry-consistent neural implicit surfaces learning for multi-view reconstruction. _ 신경 정보 처리 시스템(35_(2022), 3403-3416의 발전.\n' +
      '*[GAA\\({}^{*}\\)22]Gal R., Alaluf Y., Atzmon Y., Patashnik O., Bermano A. H., Chechik G., Cohen-Or D.: 이미지는 한 단어 가치가 있다: 텍스트 역산을 사용하여 텍스트-이미지 생성을 개인화한다. _ arXiv preprint arXiv:2208.01618_(2022).\n' +
      '*[GCL\\({}^{*}\\)21]Guo Y., Chen K., Liang S., Liu Y. - J., Bao H., Zhang J.: Ad-erf: 음성 헤드 합성을 위한 오디오 구동 신경 복사 필드. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 5784-5794.\n' +
      '*[GCS\\({}^{*}\\)20]Genova K., Cole F., Sud A., Sarma A., Funkhouser T.: Local deep implicit functions for 3d shape. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp. 4857-4866.\n' +
      '*[GCV\\({}^{*}\\)19]Genova K., Cole F., Vlasic D., Sarma A., Freeman W. T., Funkhouser T.: 구조화된 함수가 있는 학습 모양 템플릿. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2019), pp. 7154-7164.\n' +
      '*[GEB16]Gatys L. A., Ecker A. S., Bethge M. : 합성곱 신경망을 이용한 이미지 스타일 전달. _CVPR_(2016)에서.\n' +
      '*[GII\\({}^{*}\\)21]Grigorev A., Isakov K., Iainna A., Bashirov R., Zakhari I., Vakhitov A., Lempitsky V.: Stylepeople: Fullbody human avatars의 생성 모델. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 5151-5160.\n' +
      '*[GKG\\({}^{*}\\)23]Giebenhain S., Kirschenstein T., Georgopoulos M., Runz M., Agapito L., Niessner M.: learning neural parametric head model. _CVPR_(2023)에서.\n' +
      '*[GKJ\\({}^{*}\\)21]Garbin S. J., Kowalski M., Johnson M., Shotton J.: Valentin J.: Fastnerf: High-fidelity neural rendering at 200fps. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14346-14355.\n' +
      '*[GLWT22]Gu J., Liu L., Wang P., Theobalt C.: StyleNeRF: A style-based 3d-aware generator for high-resolution image synthesis. 인트로. Conf. 배워 Represented._ (2022).\n' +
      '*[GLZ\\({}^{*}\\)23]Gao X., Li X., Zhang C., Zhang Q., Cao Y., Shan Y., Quan L.: Context-human: 텍스쳐-일관된 합성을 갖는 단일 이미지로부터 인간의 자유-뷰 렌더링. _ arXiv preprint arXiv:2311.17123_ (2023).\n' +
      '*[GMW17]Gadelha M., Maji S., Wang R.: 3d shape induction from 2d view of multiple objects. In _2017 International Conference on 3D Vision (3DV)_ (2017), IEEE, pp. 402-411.\n' +
      '*[GNL\\({}^{*}\\)23]Ge S., Nah S., Liu G., Poon T., Tao A., Catanzaro B., Jacobs D., Huang J.-B., Liu M. - Y., Balaji Y.: 자신의 상관 관계를 보존함: 비디오 확산 모델에 대한 잡음 사전. _ICCV_(2023)에서.\n' +
      '*[GPAM\\({}^{*}\\)14]Goodfellow I., Pouget-Abadie J., Mirza M., Xu B., Warde-Farley D., Ozair S., Courville A., Bengio Y.: Generative Adversarial net. _ 신경 정보 처리 시스템들(27_(2014))에서의 발전들.\n' +
      '*[GPL\\({}^{*}\\)22]Grassal P.-W., Prinzler M., Leistner T., Rother C., Niessner M., Thies J.: Neural head avatars from monocular RGB video. _CVPR_(2022)에서.\n' +
      '*[GSW\\({}^{*}\\)21]Gui J., Sun Z., Wen Y., Tao D., Ye J.: A review on generative adversarial networks: Algorithm, theory, and applications. _ IEEE 트랜잭션들은 지식 및 데이터 엔지니어링 35_, 4(2021), 3313-3332에 관한 것이다.\n' +
      '*[GSW\\({}^{*}\\)22]Gao J., Shen T., Wang Z., Chen W., Yin K., Li D., Litany O., Gojicc Z., Fidler S.: GET3D: 이미지로부터 학습된 고품질 3d 텍스처 형상의 생성 모델. _NeurIPS_(2022)에서.\n' +
      '* [GTZN21]Gafni G., Thies J., Zollhofer M., Niessner M.: Dynamic neural radiance fields for monocular 4D facial avatar reconstruction. _CVPR_(2021)에서.\n' +
      '*[GWH\\({}^{*}\\)20]Guo Y., Wang H., Hu Q., Liu H., Liu L., Bennamoun M.: Deep Learning for 3d point cloud: A survey. _ IEEE transaction on pattern analysis and machine intelligence 43_, 12(2020), 4338-4364.\n' +
      '*[GWY\\({}^{*}\\)21]Gao L., Wu T., Yuan Y. - J., Lin M. -X, 레이용 - K., Zhang H.: Tm-net: Textured meshes에 대한 심층 생성 네트워크; _ ACM Transactions on Graphics (TOG) 40_, 6 (2021), 1-15.\n' +
      '*[GXN\\({}^{*}\\)23]Gupta A., Xiong W., Nie Y., Jones I., Oguz B.: 3dgen: Trilinear latent diffusion for textured mesh generation. _ arXiv preprint arXiv:2303.05371_(2023).\n' +
      '*[GYW\\({}^{*}\\)19a]Gao L., Yang J., Wu T., Yuan Y. - J., Fu H., Lai Y. - K., Zhang H.: Sdm-net: Deep Generative Network for structured deformable mesh. _ ACM Transactions on Graphics (TOG) 38_, 6 (2019), 1-15.\n' +
      '*[GYW\\({}^{*}\\)19b]Gao L., Yang J., Wu T., Yuan Y. - J., Fu H., Lai Y. - K., Zhang H.: Sdm-net: Deep Generative Network for structured deformable mesh. _ ACM Transactions on Graphics (TOG) 38_, 6 (2019), 1-15.\n' +
      '*[GZX\\({}^{*}\\)22]Gao X., Zhong C., Xiang J., Hong Y., Guo Y., Zhang J.: 단안 비디오로부터 개인화된 의미론적 리얼러 모델을 재구성한다. _ ACM Trans. Graph.__ (2022).\n' +
      '*[HB17]Huang X., Belongie S.: 적응적 인스턴스 정규화와 함께 실시간 임의 스타일 전송. _ICCV_(2017)에서.\n' +
      '*[HCL\\({}^{*}\\)22]Hong F., Chen Z., Lan Y., Pan L., Liu Z.: Eva3d: Compositional 3d human generation from 2d image collection. _ arXiv preprint arXiv:2210.04888_(2022).\n' +
      '*[HCO\\({}^{*}\\)23]Holein L., Cao A., Owens A., Johnson J., Niessner M.: Text2room: 2d text-to-image 모델들로부터 텍스처링된 3d 메쉬들을 추출하는 단계; _ arXiv preprint arXiv:2303.11989_(2023).\n' +
      '\n' +
      '*[HHP\\({}^{*}}\\)23]Hu S., Hong F., Pan L., Mei H., Yang L., Liu Z.: Sherf: Generalizable human nerf. _ arXiv preprint arXiv:2303.12791_(2023).\n' +
      '*[HHY\\({}^{*}\\)22]Huang Y. - H., He Y., Yuan Y. - J, 레이용 - K., Gao L.: Style2defined: 일관된 3d scene stylization as stylized nerf via 2d-3d mutual learning. _CVPR_(2022)에서.\n' +
      '*[HJA20]Ho J., Jain A., Abbeel P.: Denoising diffusion probability models. _ 신경 정보 프로세싱 시스템들(33_(2020), 6840-6851의 진보들.\n' +
      '*[HLA\\({}^{*}\\)19]Hu Y., Li T. - M., Anderson L., Ragan-Kelley J., Durand F.: Taichi: a language for highperformance computation for spatially sparse data structures. _ ACM Transactions on Graphics (TOG) 38_, 6 (2019), 1-16.\n' +
      '*[HLHF22]Hui K. - H., Li R., Hu J., Fu C.-W.: 3d 형상 생성을 위한 신경 웨이블릿-도메인 확산. _SIGGRAPH Asia 2022 Conference Papers_(2022), pp. 1-9.\n' +
      '*[HMR19a]Henzler P., Mitra N. J., Ritschel T.: Escaping plato\'s cave: 3D shape from adversarial rendering. In _ICCV_(2019).\n' +
      '*[HMR19b]Henzler P., Mitra N. J., Ritschel T.: Escaping plato\'s cave: 3d shape from adversarial rendering. In _ICCV_(2019).\n' +
      '*[HPX\\({}^{*}\\)22]Hong Y., Peng B., Xiao H., Liu L., Zhang J.: Head-NeRF: A real-time nerf-based parametric head model. _CVPR_(2022)에서.\n' +
      '*[HRBP21]Hu R., Ravi N., Berg A. C., Pathak D.: Worldsheet: 단일 이미지로부터의 뷰 합성을 위해 3d 시트에 세계를 랩핑한다. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp.12528-12537.\n' +
      '*[HRL\\({}^{*}\\)21]Henzler P., Reizenstein J., Labatut P., Shapovalov R., Ritschel T., Vedaldi A., Novotny D.: Unsupervised learning of 3d object categories from video from wild. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 4700-4709.\n' +
      '*[HSG\\({}^{*}\\)22]Ho J., Salimans T., Gritsenko A., Chan W., Norouzi M., Fleet D. J.: Video diffusion models. _NeurIPS_(2022)에서.\n' +
      '*[HSZ\\({}^{*}\\)23]Huang X., Shao R., Zhang Q., Zhang H., Feng Y., Liu Y., Wang Q.: Humannorm: learning normal diffusion model for high quality and realistic 3d human generation. _ arXiv preprint arXiv:2301.01406_(2023).\n' +
      '*[HTE\\({}^{*}\\)23]Haque A., Tancik M., Efros A. A., Holynski A., Kanazawa A.: Instruct-NeRF2NeRF: Editing 3d scenes with instructions. _ICCV_(2023)에서.\n' +
      '*[HTS\\({}^{*}\\)21]Huang H.-P., Tseng H.-Y., Saini S., Singh M., Yang M. - H. : 참신한 견해를 양식화하는 법을 배우는 것. _ICCV_(2021)에서.\n' +
      '*[HWZ\\({}^{*}\\)23]Huang Y., Wang J., Zeng A., Cao H., Qi X., Shi Y., Zha Z. - J., Zhang L.: Dreamwalkitz: 복잡한 3d animatable avatars로 장면을 만들어라. _ arXiv preprint arXiv:2305.12529_(2023).\n' +
      '*[HYL\\({}^{*}\\)23]Huang Y., Yi H., Liu W., Wang H., Wu B., Wang W., Lin B., Zhang D., Cai D.: One-shot implicit animatable avatars with model-based priors. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2023), pp. 8974-8985.\n' +
      '*[HYX\\({}^{*}\\)23]Huang Y., Yi H., Xiu Y., Liao T., Tang J., Cai D., Thies J: Tech: Text-guided reconstruction of lifeelike clothed humans. _ arXiv preprint arXiv:2308.08545_(2023).\n' +
      '*[HZF\\({}^{*}\\)22]Huang X., Zhang Q., Feng Y., Li H., Wang X., Wang Q.: Hdr-nerf: High dynamic range neural radiance field. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 18398-18408.\n' +
      '*[HZF\\({}^{*}\\)23]Huang X., Zhang Q., Feng Y., Li H., Wang Q. : 암시적 카메라 모델을 학습하여 촬상 과정을 반전시킨다. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 21456-21465.\n' +
      '*[HZF\\({}^{*}\\)23b]Huang X., Zhang Q., Feng Y., Li X., Wang X., Wang Q.: Generalizable radiance field representation을 위한 Local implicit ray function. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 97-107.\n' +
      '*[HZP\\({}^{*}\\)22]Hong F., Zhang M., Pan L., Cai Z., Yang L., Liu Z., Avatarclip: Zero-shot text-driven generation and animation of 3d avatars. _ arXiv preprint arXiv:2205.08535_(2022).\n' +
      '*[ID18]Inafutdinov E., Dosovitskiy A.: Unsupervised learning of shape and pose with differentiable point cloud. _ 신경 정보 처리 시스템들(31_(2018)의 진보들.\n' +
      '*[JCL\\({}^{*}\\)22]Jiang K., Chen S. - Y., Liu F.-L., Fu H., Gao L.: NeRF-FaceFighting: Disentangled face editing in neural radiance fields. In _SIGGRAPH Asia Conference Papers_ (2022), 18, 19\n' +
      '*[JJW\\({}^{*}\\)23]Jiang S., Jiang H., Wang Z., Luo H., Chen W., Xu L.: Humangen: 명시적인 전과를 가진 인간 광채 필드를 생성한다. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp.12543-12554.\n' +
      '*[JKK\\({}^{*}\\)23]Jambon C., Kerbl B., Kopanas G., Diolatzis S., Drettakis G., Leimkohler T.: Isrfshop: Interactive editing of neural radiance field. In _Proceedings of the ACM on Computer Graphics and Interactive Techniques_ (2023).\n' +
      '*[JLF22]Johari M. M., Lepoittevin Y., Fleuret F.: GeoNeRF: Generalizing nerf with geometry priors. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 18365-18375.\n' +
      '*[JMB\\({}^{*}\\)22]Jah A., Mildenhall B., Barron J. T., Abbeel P., Poole B.: Zero-shot text-guided object generation with dream field. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 867-876.\n' +
      '*[JN23]Jun H., Nichol A.: Shap-e: generating conditional 3d implicit functions. _ arXiv preprint arXiv:2305.02463_(2023).\n' +
      '*[JWZ\\({}^{*}\\)23]Jiang R., Wang C., Zhang J., Chai M., He M., Chen D., Liao J.: Avatarra: 파라미터화된 형상 및 포즈 제어를 갖는 신경 인간 아바타로의 텍스트 변환 arXiv preprint arXiv:2303.17606_(2023).\n' +
      '*[KZ\\({}^{*}\\)23]Kolotouros N., Alldieck T., Zanfir A., Bazavan E. G., Fieraru M., Sminchisescu C.: Dreamhuman: Animatable 3d avatars from text. _ arXiv preprint arXiv:2306.09329_(2023).\n' +
      '*[KBM\\({}^{*}\\)20]Kato H., Beker D., Morariu M., Ando T., Matsuoka T., Kehl W., Gabon A.: Differentiable rendering: A survey. _ arXiv preprint arXiv:2006.12057_(2020).\n' +
      '*[KBV20]Klokov R., Boyer E., Verbeek J.: 효율적인 포인트 클라우드 생성을 위한 이산 포인트 플로우 네트워크. _European Conference on Computer Vision_ (2020), Springer, pp. 694-710.\n' +
      '*[KDJ\\({}^{*}\\)23]Kwak J.-g., Dong E., Jin Y., Ko H., Mahajan S., Yi K. M.: Vivid-1-to-3: 비디오 확산 모델과의 새로운 뷰 합성. _ arXiv preprint arXiv:2312.01305_ (2023).\n' +
      '*[KDSB22]Kulhanek J., Derner E., Sattler T., Babuska R.: Viewformer: Transformer를 이용한 소수의 이미지로부터의 Next-free neural rendering. _European Conference on Computer Vision_ (2022), Springer, pp. 198-216.\n' +
      '*[KFH\\({}^{*}\\)22]Kerr J., Fu L., Huang H., Aigal Y., Tancik M., Ichnowski J., Kanazawa A., Goldberg K.: Evo-nerf: Evolving nerf for sequential robot grasping of transparent object. 제6회 연례 로봇 학습 컨퍼런스_(2022).\n' +
      '*[KKL\\({}^{*}\\)23]Kim B., Kwon P., Lee K., Lee M., Han S., Kim D., Joo H.: Chupa: Carving 3d clving human from skinned shape priors using 2d diffusion probability models. _ arXiv preprint arXiv:2305.11870_(2023).\n' +
      '*[KKLD23]Kerbl B., Kopanas G., Leimkohler T., Drettakis G.: 3d gaussian splatting for real time radiance field rendering. _ ACM Transactions on Graphics (TOG) 42_, 4 (2023), 1-14.\n' +
      '\n' +
      '*[KKR18]Knyaz V. A., Kniaz V. V., Remondino F.: Image-to-voxel model translation with conditional adversarial networks. In _Proceedings of the European Conference on Computer Vision(ECCV) Workshops_(2018), pp.0-0.\n' +
      '*[KLA19]Karras T., Laine S., Aila T.: 생성적 적대 네트워크를 위한 스타일 기반 생성기 아키텍처. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2019), pp. 4401-4410.\n' +
      '*[KLA*20]Karras T., Laine S., Aittala M., Hellsten J., Lehtinen J., Aila T.: Analyzing and improving image quality of StyleGAN. _CVPR_ (2020)에서 16, 17\n' +
      '*[KLK*20]Kim H., Lee H., Kang W. H., Lee J. Y., Kim N. S.: Software: Probabilistic framework for normalizing flow on manifolds. _ 신경 정보 처리 시스템(33_(2020), 16388-16397)에서의 발전.\n' +
      '*[KLY*21]Koh J. Y., Lee H., Yang Y., Baldridge J., Anderson P.: Pathdreamer: A world model for indoor navigation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14738-14748.\n' +
      '*[KMS22]Kobayashi S., Matsumoto E., Sitzmann V.: feature field distillation를 통한 편집을 위한 너프를 분해. _NeurIPS_(2022)에서.\n' +
      '*[KNH*22]Khan S., Naseer M., Hayat M., Zamir S. W., Khan F. S., Shah M: Transformers in vision: Survey. _ ACM 컴퓨팅 조사(CSUR) 54_, 108(2022), 1-4.\n' +
      '*[KPHL17]Kusner M. J., Paige B., Hernandez-Lobato J. M.: Grammar variational autoencoder. In _International conference on machine learning_ (2017), PMLR, pp. 1945-1954.\n' +
      '*[KPLD21]Kopanas G., Philip J., Leimkohler T., Drettakis G.: Point-based neural rendering with per-view optimization. In _Computer Graphics Forum_(2021), vol. 40, Wiley Online Library, pp. 29-43.\n' +
      '*[KPWS22]Kalischek N., Peters T., Wegner J. D., Schindler K.: 3차원 형상 생성을 위한 사면체 확산 모델. _ arXiv preprint arXiv:2211.13220_(2022).\n' +
      '*[KOG*23]Kirschstein T., Qian S., Giebenhain S., Walter T., Niessner M.: NeResp: Multipleview radiance field reconstruction of human head. _ ACM Trans. Graph.__ (2023), 17\n' +
      '*[KSZ*21]Kosiorek A. R., Strathmann H., Zoran D., Moreno P., Schneider R., Morka S., Rezende D. J.: Nerf-vae: A geometry aware 3d scene generationative model. _ICML_(2021)에서.\n' +
      '*[KUH18]Kato H., Ushiku Y., Harada T.: Neural 3d mesh renderer. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2018), pp. 3907-3916.\n' +
      '*[KVNN23]Karnewar A., Vedaldi A., Novotny D., Mitra N. J.: Holddiffusion: 2d 이미지를 이용하여 3d 확산 모델을 학습시킨다. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 18423-18433.\n' +
      '* [KW13]Kingma D. P., Welling M.: Auto-encoding variational bayes _ arXiv preprint arXiv:1312.6114_(2013).\n' +
      '*[KWKT15]Kulkarni T. D., Whitney W. F., Kohli P., Tenenbaum J.: Deep convolutional inverse graphics network. _ 신경 정보 처리 시스템들(28_(2015))에서의 발전들.\n' +
      '*[KXD12]Kasper A., Xue Z., Dillmann R.: The kit object models database: object recognition, localization and manipulation in service robotics. _ The International Journal of Robotics Research 31_, 8(2012), 927-934.\n' +
      '*[KYL21]Kim J., Yoo J., Lee J., Hong S.: Setvae: Learning hierarchical composition for generatingative modeling of set-structured data. _CVPR_(2021)에서.\n' +
      '*[LB14]Loper M. M., Black M. J.:Opendr: 근사 미분 가능한 렌더러. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13_(2014), Springer, pp. 154-169.\n' +
      '*[LBRF11]Lai K., Bo L., Ren X., Fox D.: 대규모 계층적 다시점 rgb-d 객체 데이터세트. In _2011 IEEE international conference on robotics and automation_ (2011), IEEE, pp. 1817-1824.\n' +
      '*[LC22]Lee H.-H., Chang A. X. : 복셀 그리드 metfs에 대한 순수 클립 안내를 이해한다. _ arXiv preprint arXiv:2209.15172_(2022).\n' +
      '* [LCCT23]Li W., Chen R., Chen X., Tan P.: Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. _ arXiv preprint arXiv:2310.02596_(2023).\n' +
      '*[LDS*23]Li Y., Dou Y., Shi Y., Lei Y., Chen X., Zhang Y., Zhou P., Ni B.: FocalDreamer: Text-driven 3d editing via focal-fusion assembly. _ arXiv preprint arXiv:2308.10608_(2023).\n' +
      '*[LDZL23]Li M., Duan Y., Zhou J., Lu J.: Diffusion-sdf: Text-to-shape via voxelized diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 12642-12651.\n' +
      '*[LFB*23]Liu Z., Feng Y., Black M. J., Nowrouzezahrai D., Paull L., Liu W.: Meshdiffusion: Score-based generative 3d mesh modeling. _ arXiv preprint arXiv:2308.08133_(2023).\n' +
      '*[LFLSY*23]Lin G., Feng-Lin L., Shu-Yu C., Kaiwen J., Chumpeng L., Lai Y., Hongbo F.: SketchFaceseRF: Sketch based facial generation and editing in neural radiance fields. _ ACM Trans. Graph.__ (2023).\n' +
      '*[LFS*21]Li J., Feng Z., She Q., Ding H., Wang C., Lee G. H.: Mine: Tow the continuous depth mpi with nerf for novel view synthesis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 12578-12588.\n' +
      '*[LGL*23]Long X., Guo Y. - C., Lin C., Liu Y., Dou Z., Liu L., Ma Y., Zhang S. - H., Habermann M., Theobalt C., et al.: Won-dcf3: single image to 3d using cross-domain diffusion. _ arXiv preprint arXiv:2310.15008_(2023).\n' +
      '*[LGF*23]Lin C.-H., Gao J., Tang L., Takikawa T., Zeng X., Huang X., Kreis K., Fidler S., Liu M. - Y., Lin T. - Y.: Magic3d: High-resolution text-to-3d content creation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 300-309.\n' +
      '*[LGZ*20]Liu L., Gu J., Zaw Lin K., Chua T. -S., Theobalt C.: Neural sparse voxel fields. _ 신경 정보 처리 시스템(33_(2020), 15651-15663의 발전.\n' +
      '*[LH21]Luo S., Hu W.: 3d 포인트 클라우드 생성을 위한 확산 확률 모델. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 2837-2845.\n' +
      '*[LHC*23]Lin Y., Han H., Gong C., Xu Z., Zhang Y., Li X.: Consistent123: one image to high consistent 3d asset using case-aware diffusion priors. _ arXiv preprint arXiv:2309.17261_(2023).\n' +
      '*[LHR*21]Liu L., Habermann M., Rudney V., Sarkar K., Gu J., Theobalt C.: Neural actor: Neural free-view synthesis of human actors with pose control. _ ACM transaction on graphics(TOG) 40_, 6(2021), 1-16.\n' +
      '*[Lin68]Lindenmayer A.: cell interaction in development i. filament with one-sided input. _ Journal of theoretical biology 18_, 3 (1968), 280-299.\n' +
      '*[LKL18]Lin C.-H., Kong C., Lucey S.: learning efficient point cloud generation for dense 3d object reconstruction. The _proceedings of the AAAI Conference on Artificial Intelligence_ (2018), vol. 32.\n' +
      '*[LLCL19]Liu S., Li T., Chen W., Li H.: Soft rasterizer: a differentiable renderer for image-based 3d reasoning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2019), pp. 7708-7717.\n' +
      '*[LLF*23]Li Y., Lin Z. -H., Forsyth D., Huang J.-B., Wang S.: ClimateNeRF: Physically-based neural rendering for extreme climate synthesis. _ICCV_(2023)에서.\n' +
      '*[LLL*14]Li B., Lu Y., Li C., Godil A., Schreck T., Aono M., Chen Q., Chowdhury N. K., Fang B., Furuya T., et al.:Shrec\'14 track: Large scale comprehensive 3d shape retrieval. _Eurographics Workshop on 3D Object Retrieval_ (2014), vol. 2.\n' +
      '*[LLQ*16]Liu Z., Luo P., Qiu S., Wang X., Tang X.: Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2016), pp. 1096-1104.\n' +
      '*[LLWT15]Liu Z., Luo P., Wang X., Tang X.: Deep Learning face attributes in the wild. In _Proceedings of the IEEE international conference on computer vision_ (2015), pp. 3730-3738.\n' +
      '*[LLZ*23]Liu Y., Lin C., Zeng Z., Long X., Liu L., Komura T., Wang W.: Syncreamer: 단일 시점 영상으로부터 다시점-일관성 영상을 생성하는 단계; _ arXiv preprint arXiv:2309.03453_(2023).\n' +
      '*[LLZl21]Luo A., Li T., Zhang W. - H., Lee T. S.: Surfgen: 명시적 표면 판별기를 사용한 적대적 3d 형상 합성. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 16238-16248.\n' +
      '*[LMTL21]Lin C.-H., Ma W. - C., Torralba A., Lucey S.: Barf: Bundle-adjusting neural radiance field. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 5741-5751.\n' +
      '*[LMY*23]Li Y., Ma C., Yan Y., Zhu W., Yang X.: 3d-aware face swapping. _CVPR_(2023)에서.\n' +
      '*[LPT13]Lim J. J., Pirsiavash H., Torralba A.: Parsing ikea object: Fine pose estimation. In _Proceedings of the IEEE international conference on computer vision_ (2013), pp. 2992-2999.\n' +
      '*[LSC*22]Levis A., Srinivasan P. P., Chael A. A., Ng R., Bouman K. L.: 중력 렌즈형 블랙홀 방출 단층 촬영. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 19841-19850.\n' +
      '*[LSMG20]Liao Y., Schwarz K., Mescheder L., Geiger A.: 3d 제어 가능한 이미지 합성을 위한 생성 모델의 비지도 학습을 향함. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2020), pp. 5871-5880.\n' +
      '*[LSS*21]Lombardi S., Simon T., Schwartz G., Zollhofer M., Sheikh Y., Saragih J.: 효율적인 신경 렌더링을 위한 체적 프리미티브들의 혼합물. _ ACM Trans. Graph.__ (2021).\n' +
      '*[LSSS18]Lombardi S., Saragih J., Simon T., Sheikh Y. : 얼굴 렌더링을 위한 Deep appearance models. _ ACM Trans. Graph.__ (2018).\n' +
      '*[LSZ*22]Li T., Slavcheva M., Zollhofer M., Green S., Lassner C., Kim C., Schmidt T., Lovegrove S., Goesele M., Newcombe R., et al.: Neural 3d video synthesis from multi-view video. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 5521-5531.\n' +
      '*[LTJ18]Liu H.-T., Tao M., Jacobson A.: Paparazzi: multi-view 이미지 프로세싱에 의한 표면 편집. _ ACM Trans. Graph.__ 37, 6(2018), 221-.\n' +
      '*[LTJ*21]Liu A., Tucker R., Jampani V., Makadia A., Snavely N., Kanazawa A.: Infinite nature: Perpethul view generation of natural scene from single image. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14458-14467.\n' +
      '*[LTZ*23]Li J., Tan H., Zhang K., Xu Z., Luan F., Xu Y., Hong Y., Sunkavalli K., Shakhinarovich G., Bi S., Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. _ arXiv preprint arXiv:2311.06214_(2023).\n' +
      '*[LWA*23]Lyu Z., Wang J., An Y., Zhang Y., Lin D., Dai B. : 희소 잠재점 확산 모델을 통한 제어 가능한 메쉬 생성. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp.271-280.\n' +
      '*[LWC*23]Li Z., Wang Q., Cole F., Tucker R., Snavely N.: Dynibar: Neural dynamic image-based rendering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 4273-4284.\n' +
      '*[LWQZ22]Liu Z., Wang Y., Qi X., Fu C.-W.: 암시적 텍스트 유도 3d 형상 생성을 향함. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 17896-17906.\n' +
      '*[LWSK22]Li Z., Wang Q., Snavely N., Kanazawa A.: Infinitentane-zero: Learning permanent view generation of natural scene from single image. In _European Conference on Computer Vision_ (2022), Springer, pp. 515-534.\n' +
      '*[LWVH*23]Liu R., Wu R., Van Hoorick B., Tokmakov P., Zakharov S., Vondrick C.: Zero-1-to-3: Zero-shot one image to 3d object. _ arXiv preprint arXiv:2303.11328_(2023).\n' +
      '*[LXC*21]Li J., Xu C., Chen Z., Bian S., Yang L., Lu C.: Hybrid: 3d 인간 포즈 및 형상 추정을 위한 하이브리드 분석-신경 역기구학 솔루션. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2021), pp.3383-3393.\n' +
      '*[LXJ*23]Liu M., Xu C., Jin H., Chen L., Xu Z., Su H., et al.: One-2-3-45: 임의의 단일 이미지 내지 3d 메쉬 in 45 seconds without per-shape optimization. _ arXiv preprint arXiv:2306.16928_(2023).\n' +
      '*[LXM*20]Liu K. -E., Xu Z., Milchenhal I., Srinivasan P. P., Hold-Geoffroy Y., DiVerdi S., Sun Q., Sunkavalli K., Ramamoorthi R.: 뷰 합성을 위한 딥 멀티 깊이 파노라마. In _European Conference on Computer Vision_ (2020), Springer, pp. 328-344.\n' +
      '*[LXZ*23]Lorraine J., Xie K., Zeng X., Lin C.-H., Takikawa T., Sharp N., Lin T. - Y., 류민 - Y., Fidler S., Lucas J.: An3d: Amortized text-to-3d object synthesis. _ arXiv preprint arXiv:2306.07349_(2023).\n' +
      '*[LYX*24]Liao T., Yi H., Xiu Y., Tang J., Huang Y., Thies J., Black M. J.: TADA! 텍스트를 애니메이션 가능한 디지털 아바타에 제공합니다. _3DV_(2024)에서.\n' +
      '*[LZ21]Lassner C., Zollhofer M.: Pulsar: Efficient sphere-based neural rendering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 1440-1449.\n' +
      '*[LZF*23]Liang Z., Zhang Q., Feng Y., Shan Y., Jia K.: Gs-ir: 3d gaussian splitting for inverse rendering. _ arXiv preprint arXiv:2311.16473_(2023).\n' +
      '*[LZJ*22]Lei J., Zhang Y., Jia K., et al.: TANGO: Text-driven photorealistic and robust 3d stylization via lighting decomposition. _NeurIPS_(2022)에서.\n' +
      '*[LZT*23]Liu X., Zhan X., Tang J., Shan Y., Zeng G., Lin D., Liu X., Liu Z., Liu Z.: Humangaussian: text-driven 3d human generation with gaussian splitting. _ arXiv preprint arXiv:2311.17061_(2023).\n' +
      '*[LZW*23]Li C., Zhang C., Waghawase A., Lee L. - H., Rameau F., Yang Y., Bae S. - H., Hong C. S.: Generative ai meets 3d: Survey on text-to-3d in age era. _ arXiv preprint arXiv:2305.06131_(2023).\n' +
      '* [Man67] Mandelbrot B. : 영국 해안은 얼마나 길죠? 통계적 자기 유사성 및 소수 차원. _ science 156_, 3775 (1967), 636-638.\n' +
      '*[Max95]Max N. : 직접 볼륨 렌더링을 위한 광학 모델들 _ IEEE Transactions on Visualization and Computer Graphics 1_, 2(195), 99-108.\n' +
      '*[MBOL*22]Michel O., Bar-On R., Liu R., Benaim S., Hanocka R.: Text2mesh: Text-driven neural stylization for meshes. _CVPR_(2022)에서.\n' +
      '*[MBRS*21]Martin-Brudalla R., Radwan N., Sajjadi M. S., Barron J. T., Dosovitskiy A., Duckworth D.: Nerf in the wild: Neural radiance fields for nonstrained photo collection. _CVPR_(2021), pp. 7210-7219.\n' +
      '*[MCL20]Morrison D., Corke P., Leitner J.: Egad! 로봇 조작의 다양성과 재현성에 대한 진화된 파지 분석 데이터 세트. _ IEEE Robotics and Automation Letters 5_, 3(2020), 4368-4375.\n' +
      '*[MCL20]Morrison D., Corke P., Leitner J.: Egad! 로봇 조작의 다양성과 재현성에 대한 진화된 파지 분석 데이터 세트. _ IEEE Robotics and Automation Letters 5_, 3(2020), 4368-4375.\n' +
      '\n' +
      '*[MCST22a]Mittal P., Cheng Y. - C., Singh M., Tulsiani S.: Autosdf: Shape priors for 3d completion, reconstruction and generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 306-315.\n' +
      '*[MCST22b]Mittal P., Cheng Y. - C., Singh M., Tulsiani S.: Autosdf: Shape priors for 3d completion, reconstruction and generation. _CVPR_(2022)에서.\n' +
      '*[MESK22]Muller T., Evans A., Schied C., Keller A.: Instant neural graphics primitives with multiresolution hash encoding. _ ACM Transactions on Graphics(ToG) 41_, 4(2022), 1-15.\n' +
      '*[Mid]Midjourney: Midjourney. [https://www.midjourney.com/] (https://www.midjourney.com/).\n' +
      '*[MKLRV23]Melas-Kyriazi L., Laina I., Rupprecht C., Vedaldi A.: Realfusion: 360deg reconstruction of any object from a single image. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 8446-8455.\n' +
      '*[MKXP22]Mohammad Khalid N., Xie T., Bellovsky E., Popa T.: Clip-mesh: 미리 훈련된 이미지-텍스트 모델을 사용하여 텍스트로부터 텍스처링된 메시를 생성하는 단계. _SIGGRAPH Asia 2022 conference papers_(2022), pp. 1-8.\n' +
      '*[MLL*22a]Ma L., Li X., Liao J., Wang X., Zhang Q., Wang J., Sande P. V. : 동적 인간 머리 편집을 위한 신경 파라미터화 _ ACM Transactions on Graphics (TOG) 41_, 6 (2022), 1-15.\n' +
      '*[MLL*22b]Ma L., Li X., Liao J., Zhang Q., Wang X., Wang J., Sande P. V. : Deblur-nerf : 블러 이미지로부터의 신경 복사 필드. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp.12861-12870.\n' +
      '*[MM82]Mandelbrot B. B., Manelbrot B. B.: _The fractal geometry of nature_, vol. 1, W1 freen New York, 1982.\n' +
      '*[MPS*23]Mikaeli A., Perel O., Safaee M., Cohen-Or D., Mahdavi-Amiri A.: SKED: Sketch-guided text-based 3d editing. _ICCV_(2023)에서.\n' +
      '*[MRP*23a]Metzer G., Richardson E., Patashnik O., Giryes R., Cohen-Or D.: Latent-nref for shape-guided generation of 3d shape and textureures. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 12663-12673.\n' +
      '*[MRP*23b]Metzer G., Richardson E., Patashnik O., Giryes R., Cohen-Or D.: Latent-NeRF for shape-guided generation of 3d shape and textureures. _CVPR_(2023)에서.\n' +
      '*[MS15]Maturana D., Scherer S.: Voxnet A 3d convolutional neural network for real-time object recognition. In _2015 IEEE/RSJ international conference on intelligent robots and systems (IROS)_ (2015), IEEE, pp. 922-928.\n' +
      '*[MSOC*19]Mildenhall B., Srinivasan P. P., Ortiz-Cayon R., Kalantari N. K., Ramamoorthi R., Ng R., Kar A.: Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. _ ACM Transactions on Graphics (TOG) 38_, 4 (2019), 1-4.\n' +
      '*[MSP*23]Muller N., Siddiqui Y., Porzi L., Bulo S. R., Kontschieder P., Niessner M.: Diffff: Rendering-guided 3d radiance field diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 4328-4338.\n' +
      '*[MSS*21]Ma S., Simon T., Saraghi J., Wang D., Li Y., De la Torre F., Sheikh Y. : 픽셀 코덱 아바타. _CVPR_(2021)에서.\n' +
      '*[MST*20]Mildenhall B., Srinivasan P. P., Tancik M., Barron J. T., Ramamoorthi R., Ng R.: Nerf: 장면들을 뷰 합성을 위한 신경 복사 필드들로서 표현하는 단계. _European conference on computer vision_(2020), Springer, pp. 405-421.\n' +
      '*[MYR*20]Ma Q., Yang J., Ranish A., Pujades S., Pons-Moll G., Tang S., Black M. J.: 3d 사람들에게 생성 의복을 입는 것을 배우는 것. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp. 6469-6478.\n' +
      '*[MZS*23]Ma Y., Zhang X., Sun X., Ji J., Wang H., Jiang G., Zhuang W., Ji R.: X-Mesh: Towards fast and accurate text-driven 3d stylization via dynamic textual guidance. _ICCV_(2023)에서.\n' +
      '*[ND21]Nichol A. Q., Dhariwal P.: 개선된 잡음 제거 확산 확률 모델. In _International Conference on Machine Learning_ (2021), PMLR, pp. 8162-8171.\n' +
      '*[NDR*21]Nichol A., Dhariwal P., Ramesh A., Shyam P., Mishra P., McGrew B., Sutskever I., Chen M.: Glide: 텍스트 유도 확산 모델을 사용한 실사 이미지 생성 및 편집을 향함. _ arXiv preprint arXiv:2112.10741_(2021).\n' +
      '*[NDVZJ19]Nimier-David M., Vicini D., Zelner T., Jakob W.: Mitsuba 2: 리타겟팅 가능한 순방향 및 역방향 렌더러. _ ACM Transactions on Graphics (TOG) 38_, 6 (2019), 1-17.\n' +
      '*[Neu66]Neumann J. V.: Self-reproducing automata의 이론_ 아서 W에 의해 편집되었다. Burks_(1966).\n' +
      '*[NG21]Niemeyer M., Geiger A.: GIRAFFE: 장면들을 구성 생성 신경 특징 필드들로서 표현하는 것. _CVPR_(2021)에서.\n' +
      '*[NGEB20a]Nash C., Ganin Y., Eslami S. A., Battaglia P.: Polygen: 3d meshes의 자기회귀 생성 모델. In _International conference on machine learning_ (2020), PMLR, pp. 7220-7229.\n' +
      '*[NGEB20b]Nash C., Ganin Y., Eslami S. A., Battaglia P.: Polygen: 3d meshes의 자기회귀 생성 모델. In _ICML_(2020).\n' +
      '*[NJD*22]Nichol A., Jun H., Dhariwal P., Mishkin P., Chen M.: Point-e: 복잡한 프롬프트로부터 3d 포인트 클라우드를 생성하는 시스템. _ arXiv preprint arXiv:2212.08757_(2021).\n' +
      '*[NKR*22]Nam G., Khlip M., Rodriguez A., Tono A., Zhou L., Guerrero P.: 3d-ldm: Neural implicit 3d shape generation with latent diffusion models. _ arXiv preprint arXiv:2212.00842_(2022).\n' +
      '*[NPLT*19]Nguyen-Phuoc T., Li C., Theis L., Richardt C., Yang Y. - L.: 홀로GAN: 자연 이미지들로부터의 3d 표현들의 비지도 학습. In _ICCV Workshop_ (2019).\n' +
      '* [NPRM*20]Nguyen-Phuoc T. H., Richardt C., Mai L., Yang Y., Mitra A.: Blockgan: Learning 3d object-aware scene representations from unlabelelled images. _ 신경 정보 처리 시스템들(33_(2020), 6767-6778)에서의 진보들.\n' +
      '*[NSLH22]Noguchi A., Sun X., Lin S., Harada T.: Unsupervised learning of efficient geometry-aware neural articulated representations. _European Conference on Computer Vision_ (2022), Springer, pp. 597-614.\n' +
      '*[OBB20]Osman A. A., Bolkart T., Black M. J.: Star: Sparse trained articulated human body regressor. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V 16_(2020), Springer, pp. 598-613.\n' +
      '*[OELS*22]Oen-El R., Luo X., Shan M., Shechtman E., Park J. J., Kemelmacher-Shilzerman I.: StyleSDF: High-resolution 3d-consistent image and geometry generation. _CVPR_(2022)에서.\n' +
      '*[OMN*19]Occhsle M., Mescheder L., Niemeyer M., Strauss T., Geiger A.: Texture fields: Learning texture representations in function space. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2019), pp.4531-4540.\n' +
      '*[Ope]OpenAI: Dall-e 3. [https://openai.com/dall-e-3](https://openai.com/dall-e-3)\n' +
      '*[PCPMMN21]Pumarola A., Corona E., Pons-Moll G., Moreno-Noguer F.: D-nerf: Neural radiance fields for dynamic scene. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 10318-10327.\n' +
      '*[PDW*21]Peng S., Dong J., Wang Q., Zhang S., Shuai Q., Zhou X., Bao H.: Animatable neural radiance fields for modeling dynamic human bodies. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14314-14323.\n' +
      '*[Per85]Perlin K. : 영상 합성기 _ ACM Siggraph Computer Graphics 19_, 3(1985), 287-296.\n' +
      '*[Per02]Perlin K. : 잡음 개선. _Proceedings of the 29th annual conference on Computer graphics and interactive techniques_ (2002), pp. 681-682.\n' +
      '\n' +
      '*[PFS*19]Park J. J., Florence P., Straub J., Newcombe R., Lovegrove S.: Deepsdf: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2019), pp. 165-174.\n' +
      '*[PGH*16]Pu Y., Gan Z., Henao R., Yuan X., Li C., Stevens A., Carin L.: 이미지, 라벨 및 캡션의 딥 러닝을 위한 Variational Autoencoder _ 신경 정보 처리 시스템들(29_(2016))에서의 발전들.\n' +
      '*[PJBM23]Poole B., Jain A., Barron J. T., Mildenhall B.: DreamFusion:Text-to-3d using 2d diffusion. 인트로. Conf. 배워 Represented._ (2023).\n' +
      '*[PKHL21]Pavllo D., Kohler J., Hofmann T., Lucchi A.: learning generatingative models of textureured 3d meshes from real-world images. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 13879-13889.\n' +
      '*[PRFS18]Park K., Rematas K., Farhadi A., Seitz S. M.: Photodshape:Photorealistic materials for large scale shape collection. _ arXiv preprint arXiv:1809.09761_(2018).\n' +
      '*[PSB*21]Park K., Sinha U., Barron J. T., Bouaziz S., Goldman D. B., Seitz S. M., Martin-Revlalta R.: Nerfies: Deformable neural radiance field. _ICCV_(2021)에서.\n' +
      '*[PSH*20]Pavllo D., Spins G., Hofmann T., Moens M. - F., Lucchi A.: Convolutional generation of textureured 3d meshes. _ 신경 정보 처리 시스템 33_(2020), 870-882에서의 발전.\n' +
      '*[PYG*23]Po R., Yifan W., Golyanik V., Aberman K., Barron J. T., Bermano A. H., Chan E. R., Dekel T., Holynski A., Kanazawa A., et al.: State of the art on diffusion models for visual computing. _ arXiv preprint arXiv:2310.07204_(2023).\n' +
      '*[PYL*22]Peng Y., Yan Y., Liu S., Cheng Y., Guan S., Pan B., Zhai G., Yang X.: CageNeRF: Cage-based neural radiance field for generalized 3d deformation and animation. _NeurIPS_(2022)에서.\n' +
      '*[PZ17]Penner E., Zhang L. : 뷰 합성을 위한 소프트 3d 재구성 _ ACM Transactions on Graphics (TOG) 36_, 6 (2017), 1-11.\n' +
      '*[PZYBG00]Pfister H., Zwicker M., Van Baar J., Gross M.:Surfies:Surface elements as rendering primitives. _Proceedings of the 27th annual conference on computer graphics and interactive techniques_ (2000), pp. 335-342.\n' +
      '*[QMH*23]Qian G., Mai J., Hamdi A., Ren J., Siarohin A., Li B., Lee H.-Y., Skorokhodhodov I., Wonka P., Tulkavko S., et al.: Magic123: One image to high quality 3d object generation using both 2d and 3d diffusion priors. _ arXiv preprint arXiv:2306.17843_(2023).\n' +
      '*[RALB22]Rakhimov R., Ardelean A.-T., Lempitsky V., Burnaev E.: Npbg++: Accelerating neural point-based graphics. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 15969-15979.\n' +
      '*[RBL*22]Rombach R., Blattmann A., Lorenz D., Esser P., Ommer B.: 잠재 확산 모델을 사용한 고해상도 이미지 합성. _CVPR_(2022)에서.\n' +
      '*[RBL*22b]Rombach R., Blattmann A., Lorenz D., Esser P., Ommer B.: 잠재 확산 모델을 사용한 고해상도 이미지 합성. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (2022), pp. 10684-10695.\n' +
      '*[REO21]Rombach R., Esser P., Ommer B.: Geometry-free view synthesis: Transformers and no 3d priors. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14356-14366.\n' +
      '*[RFJ21]Rockwell C., Fouhey D. F., Johnson J.: Pixelsynth: 단일 이미지로부터 3d-일관된 경험을 생성하는 단계. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14104-14113.\n' +
      '*[RFS22]Ruckert D., Franke L., Stamminger M.: Adop: 근사 미분 가능한 1-픽셀 포인트 렌더링. _ ACM Transactions on Graphics(ToG) 41_, 4(2022), 1-14.\n' +
      '*[RK20]Riegler G., Koltun V.: Free view synthesis. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIX 16_(2020), Springer, pp. 623-640.\n' +
      '*[RK21]Riegler G., Koltun V. : 안정한 뷰 합성. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 12216-12225.\n' +
      '*[RKH*21a]Radford A., Kim J. W., Hallacy C., Ramesh A., Goh G., Agarwal S., Sastry G., Askell A., Mishkin P., Clark J., et al.: learning transferable visual models from natural language supervision. _ICML_(2021)에서.\n' +
      '*[RKH*21b]Radford A., Kim J. W., Hallacy C., Ramesh A., Goh G., Agarwal S., Sastry G., Askell A., Mishkin P., Clark J., et al.: learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_ (2021), PMLR, pp.8748-8763.\n' +
      '*[RKP*23]Rai A., Kaza S., Poole B., Niemeyer M., Ruiz N., Milendaill B., Zada S., Aberman K., Rubinstein M., Barron J., et al.: Dreambooth3d: Subject-driven text-to-3d generation. _ arXiv preprint arXiv:2303.13508_(2023).\n' +
      '*[RPLG21]Reiser C., Peng S., Liao Y., Geiger A.:Kilonperf:Speeding neural radiance field with thousands of tiny mlps. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 14335-14345.\n' +
      '*[RROG18]Roveri R., Rahmann L., Oztireli C., Gross M.: 자동 깊이 이미지 생성을 통한 포인트 클라우드 분류를 위한 네트워크 아키텍처. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_ (2018), pp. 4176-4184.\n' +
      '*[RSH*21]Reizenstein J., Shapovalov R., Henzler P., Sbordone L., Labarut P., Novotny D.: Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 10901-10911.\n' +
      '*[RWC*19]Radford A., Wu J., Child R., Luan D., Amodei D., Sutskever I., et al.: 언어 모델들은 비감독 멀티태스크 학습자들 _ Op health 1_, 8(2019), 9.\n' +
      '*[RWL*22]Ruckert D., Wang Y., Li R., Idoughi R., Heidrich W.:Neat: Neural adaptive tomography. _ ACM Transactions on Graphics (TOG) 41_, 4 (2022), 1-13.\n' +
      '*[SAA*23]Sidiqiqi Y., Alliegro A., Artemov A., Tommasi T., Sirigatti D., Rosov V., Dai A., Niessner M.: Meshpft: 디코더 전용 변압기와 삼각형 메쉬를 생성하는 단계; _ arXiv preprint arXiv:2311.15475_ (2023).\n' +
      '*[SCL*22]Sanghi A., Chu H., Lambourne J. G., Wang Y., Cheng C.-Y., Fumero M., Malekshan K. R.: Clip-forge: Towards zero-shot text-to-shape generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 18603-18613.\n' +
      '*[SCP*23]Shue J. R., Chan E. R., Po R., Ankner Z., Wu J., Wetzstein G.: 3d neural field generation using triplane diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 20875-20886.\n' +
      '*[SCS*22]Saharia C., Chan W., Saxena S., Li L., Whang J., Denton E. L., Ghasempotu K., Gontijo Lopes R., Karagoi, ayan B., Salihans T., et al., Photorealistic text-to-image diffusion models with deep language understanding. _ 신경 정보 처리 시스템(35_(2022), 36479-36494의 발전.\n' +
      '*[SCZ*23]Shi R., Chen H., Zhang Z., Liu M., Xu C., Wei X., Chen L., Zeng C., Su H.: Zero123++: 단일 이미지 내지 일관된 다시점 확산 기저 모델. _ arXiv preprint arXiv:2310.15110_(2023).\n' +
      '*[SDZ*21]Shinivasan P. P., Deng B., Zhang X., Tancik M., Mildenhall B., Barron J. T., Nerv: Neural reflectance and visibility field for relighting and view synthesis. In _Proceedings of the _IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 7495-7504.\n' +
      '*[SFL*23]Sanghi A., Fu R., Liu V., Willis K. D., Shayani H., Khasahmadi A. H., Sridhar S., Ritchie D.: Clip-sculptor: Zero-shot generation of high-fidelity and various shape from natural language. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 18339-18348.\n' +
      '*[SGHS98]Shade J., Gortler S., He L. -w., Szeliski R.: Layered depth image. _Proceedings of the 25th annual conference on Computer graphics and interactive techniques_ (1998), pp. 231-242.\n' +
      '*[SGY*21]Shen T., Gao J., Yin K., Liu M. -Y., Fidler S.: Deep marching tetrahedra: Hybrid representation for high-resolution 3d shape synthesis. _ 신경 정보 처리 시스템 34_(2021), 6087-6101에서의 발전.\n' +
      '*[SHG*22]Shrestha R., Hu S., Gou M., Liu Z., Tan P.: Multi-view 3d reconstruction을 위한 실세계 데이터셋. _European Conference on Computer Vision_ (2022), Springer, pp. 56-73.\n' +
      '*[SHN*19]Saito S., Huang Z., Natsume R., Morishima S., Kanazawa A., Li H.: Pifu: Pixel-aligned implicit function for high-resolution closed human digitization. In _Proceedings of the IEEE/CVF international conference on computer vision_(2019), pp. 2304-2314.\n' +
      '*[SKJ23]Shim J., Kang C., Joo K.: Diffusion-based signed distance fields for 3d shape generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 20887-20897.\n' +
      '*[SLNG20]Schwarz K., Liao Y., Niemeyer M., Geiger A.: GRAF: Generative radiance field for 3d-aware image synthesis. In _NeurIPS_(2020).\n' +
      '*[SMKF04]Shilane P., Min P., Kazhdan M., Funkhouser T.: The Princeton shape benchmark. In _Proceedings Shape Modeling Applications_, 2004. (2004), IEEE, pp. 167-178.\n' +
      '*[SMP*22]Sajhadi M. S., Meyer H., pot E., Bergmann U., Greff K., Radwan N., Vora S., Lucic M., Duckworth D., Dosovitskiy A., et al.: Scene representation transformer: Set-latent scene representation을 통한 Geometry-free novel view synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 6229-6238.\n' +
      '*[SPH*23]Singer U., Polyak A., Hayes T., Yin X., An J., Zhang S., Hu Q., Yang H., Ashual O., Gafni O., et al.: Make-a-video: Text-to-video generation without text-video data. 인트로. Conf. 배워 Represented._ (2023).\n' +
      '*[SPK19]Shu D. W., Park S. W., Kwon J.: 3d point cloud generatedative adversarial network based on tree structured graph convolutions. In _Proceedings of the IEEE/CVF international conference on computer vision_ (2019), pp. 3859-3868.\n' +
      '*[SPX*22]Shi Z., Peng S., Xu Y., Geiger A., Liao Y., Shen Y.: Deep Generative Model on 3d representations: A survey. _ arXiv preprint arXiv:2210.15663_(2022).\n' +
      '*[SSS7]Shirman L. A., Sequin C. H.: Beier patch를 갖는 국부 표면 보간. _ Computer Aided Geometric Design 4_, 4(1987), 279-295.\n' +
      '*[SSC22]Sun C., Sun M., Chen H.-T.: Direct voxel grid optimization: Super-fast convergence for radiance field reconstruction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 5459-5469.\n' +
      '*[SSKH20]Shih M. - L, Su S. - Y., Kopf J., Huang J.-B.: context-aware layered depth inpainting을 이용한 3d 사진. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp. 8028-8038.\n' +
      '*[SSN*14]Singh A., Sha J., Narayan K. S., Achim T., Abbeel P.: Bigbird: A large-scale 3d database of object instances. In _2014 IEEE international conference on robotics and automation (ICRA)_ (2014), IEEE, pp.509-516.\n' +
      '*[SSN*22]Schwarz K., Sauer A., Niemeyer M., Liao Y., Geiger A.: Voxgraf: Fast 3d-aware image synthesis with sparse voxel grid. _ 신경 정보 처리 시스템(35_(2022), 33999-34011의 발전.\n' +
      '*[STB*19]Srinivasan P. P., Tucker R., Barron J. T., Ramamoorthi R., Ng R., Snavely N.: 멀티판 이미지로 뷰 외삽의 경계를 밀어내는 단계. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2019), pp. 175-184.\n' +
      '*[SWL*20a]Sus Y., Wang Y., Liu Z., Siegel J., Sarma S.: Pointgrow:Autoregressively learned point cloud generation with self-attention. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_ (2020), pp. 61-70.\n' +
      '*[SWL*20b]Sun Y., Wang Y., Liu Z., Siegel J., Sarma S.: Pointgrow: Autoregressively learned point cloud generation with self-attention. _WACV_(2020)에서.\n' +
      '*[SWS*22]Sun J., Wang X., Shi Y., Wang L., Wang J., Liu Y.: IDE-3D: Interactive disentangled editing for high resolution 3d-aware portrait synthesis. _ ACM Trans. Graph.__ (2022).\n' +
      '*[SWW*23]Sun J., Wang X., Wang L., Li X., Zhang Y., Zhang H., Liu Y.: Next3D: Generative neural texture rasterization for 3d-aware head avstars. _CVPR_(2023)에서.\n' +
      '*[SWY*23]Shi Y., Wang P., Ye J., Long M., Li K., Yang X.: Mvdream: Multi-view diffusion for 3d generation. _ arXiv preprint arXiv:2308.16512_(2023).\n' +
      '*[SWZ*18]Sun X., Wu J., Zhang X., Zhang Z., Zue C., Xue T., Tenenbaum J. B., Freeman W. T.: Pix3d: Dataset and methods for single-image 3d shape modeling. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2018), pp. 2974-2983.\n' +
      '*[SWZ*22]Sun J., Wang X., Zhang Y., Li X., Zhang Q., Liu Y., Wang J.: Fenerf: Face editing in neural radiance fields. _CVPR_(2022)에서.\n' +
      '*[SZS*23]Sun J., Zhang B., Shao R., Wang L., Liu W., Xie Z., Liu Y.: Dreamerfstd: Streamical 3d generation with bootstrapped diffusion prior. _ arXiv preprint arXiv:2310.16818_(2023).\n' +
      '* [TDB16]Tatarchenko M., Dosovitskiy A., Brox T.: 컨볼루션 네트워크를 갖는 단일 이미지로부터 멀티뷰 3d 모델. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VII 14_(2016), Springer, pp. 322-337.\n' +
      '*[TFT*20]Tewah A., Fried O., Thies J., Sitzmann V., Lombardi S., Sunkavalli K., Martin-Brualla R., Simon T., Saragih J., Niessner M., et al.: the State of the art on neural rendering. In _Computer Graphics Forum_(2020), vol. 39, Wiley Online Library, pp. 701-727.\n' +
      '*[TLK*23]Tseng H.-Y., Li Q., Kim C., Alsisan S., Huang J.-B., Kopf J.: 포즈 유도 확산 모델과의 일관된 뷰 합성. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 16773-16783.\n' +
      '*[TLY*21]Takikawa T., Litalien J., Yin K., Kreis K., Loop C., Nowrouzezahrai D., Jacobson A., McGuire M., Fidler S.: Neural geometric level of detail: Real-time rendering with implicit 3d shape. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 11358-11367.\n' +
      '*[TLYCS22]Tseng W. -C., Liao H.-J., Yen-Chen L., Sun M.: CLA-NeRF: Category-level articulated neural radiance field. In _International Conference on Robotics and Automation (ICRA)_ (2022).\n' +
      '*[TME*22]Tremblay J., Meshry M., Evans A., Kautz J., Keller A., Khamis S., Muller T., Loop C., Morrica N., Nagano K., et al.: Ritru: A ray-traced multi-view 합성 dataset for novel view synthesis. _ arXiv preprint arXiv:2205.07058_(2022).\n' +
      '* [TRMT23]Truong P., Rakotosaona M. - J., Manhardt F., Tombari F.: Sparf: 희박하고 시끄러운 포즈로부터의 신경 복사 필드. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 4190-4200.\n' +
      '\n' +
      '*[TRZ*23]Tang J., Ren J., Zhou H., Liu Z., Zeng G.: Dream-gaussian: Generative gaussian splatting for efficient 3d content creation. _ arXiv preprint arXiv:2309.16653_(2023).\n' +
      '*[ITG*20]Tretschk E., Tewari A., Golyanik V., Zollhofer M., Stoll C., Theobalt C.: Patchlets: Patch-based generalizable deep implicit 3d shape representation. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVI 16_(2020), Springer, pp. 293-309.\n' +
      '*[ITM*22]Tewari A., Thies J., Mildenhall B., Srinivasan P., Tretschk E., Yigan W., Lassner C., Sitzmann V., Martin-Ruulak R., Lombardi S., et al.: Advances in neural rendering. In _Computer Graphics Forum_(2022), vol. 41, Wiley Online Library, pp. 703-735.\n' +
      '*[TWZ*23]Tang J., Wang T., Zhang B., Zhang T., Yi R., Ma L., Chen D.: Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior. _ arXiv preprint arXiv:2303.14184_(2023).\n' +
      '*[TYC*23]Tewari A., Yin T., Cazenavette G., Rezchikov S., Tenenbaum J. B., Durand F., Freeman W. T., Sitzmann V.: Diffusion with forward models: Solving stochastic inverse problems without direct supervision. _ arXiv preprint arXiv:2306.11719_(2023).\n' +
      '*[TZN19]Thies J., Zollhofer M., Niessner M.: Deferred neural rendering: Image synthesis using neural textureures. _ Acm Transactions on Graphics (TOG) 38_, 4 (2019).\n' +
      '*[VHM*22]Verbin D., Hedman P., Mildenhall B., Zickler T., Barron J. T., Srinivasan P. P.: Ref-nerf: Structured view-dependent appearance for neural radiance field. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_ (2022), IEEE, pp. 5481-5490.\n' +
      '*[VN*51]Von Neumann J., et al.: The general and logical theory of automata. _ 1951_(1951), 1-41.\n' +
      '*[WCH*22]Wang C., Chai M., He M., Chen D., Liao J.: CLIP-NeRF: Text-and-image driven manipulation of neural radiance fields. _CVPR_(2022)에서.\n' +
      '*[WCMB*22]Watson D., Chan W., Martin-Bruulak R., Ho J., Tagliaschi A., Norouzi M.: 확산 모델과의 새로운 뷰 합성. _ arXiv preprint arXiv:2210.04628_(2022).\n' +
      '*[WCS*22]Weng C.-Y., Curless B., Srinivasan P. P., Barron J. T., Kemelmacher-Shilzerman I.: Humannerf: Free-viewpoint rendering of moving people from monocular video. In _Proceedings of the IEEE/CVF conference on computer vision and pattern Recognition_ (2022), pp. 16210-16220.\n' +
      '*[WDL*23]Wang H., Du X., Li J., Yeh R. A., Shakhinarovich G.: Score jacobian chaining: Lifting prerained 2d diffusion model for 3d generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 12619-12629.\n' +
      '*[WDY*22]Wu Y., Deng Y., Yang J., Wei F., Chen Q., Tong X.: AniFaceGAN: Animatchable 3d-aware face image generation for video avatars. _NeurIPS_(2022)에서.\n' +
      '*[WGS21]Wiles O., Gkoxari G., Szeliski R., Johnson J.: Synsin: End-to-end view synthesis from a single image. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp. 7467-7477.\n' +
      '*[WJWC*23]Wang C., Jiang R., Chai M., He M., Chen D., Liao J.: NeRF-art: Text-driven neural radiance fields stylization. _ IEEE Trans. Vis. 컴퓨터 그래프._ (2023).\n' +
      '*[WKC*23]Wang C., Kang D., Cao Y., Bao L., Shan Y., Zhang S. - H.: Neural Point-based volumetric avatar: Surface-guided neural points for efficiently and photorealistic volumetric head avatar. In _ACM SIGGRAPH Asia 2023 Conference Proceedings_ (2023).\n' +
      '*[WLC*22]Wu Q., Liu X., Chen Y., Li K., Zheng C., Cai J., Zheng J.: Object-compositional neural implicit surfaces. In _ECCV_(2022).\n' +
      '*[WLG*23]Wang M., 류영 -S., Gao Y., Shi K., Fang Y., Han Z.: Lp-dfi. 3D 객체 및 장면에 대한 로컬 패턴별 심층 암시적 함수 학습. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 21856-21865.\n' +
      '*[WLL*21]Wang P., Liu L., Liu Y., Theobalt C., Komura T., Wang W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. _ 신경 정보 처리 시스템(34_(2021), 27171-27183의 발전.\n' +
      '*[WLW*23]Wang Z., Lu C., Wang Y., Bao F., Li C., Su H., Zhu J.: Prolificdreamer: Highfidelity and diverse text-to-3d generation with variational score distillation. _ arXiv preprint arXiv:2305.16213_ (2023).\n' +
      '*[WLY*23]Wu T., Li Z., Yang S., Zhang P., Pan X., Wang J., Lin D., Liu Z.: Hyperdreamer: Hyper-realistic 3d content generation and editing from a single image. In _SIGGRAPH Asia 2023 Conference Papers_ (2023).\n' +
      '*[Wol83]Wolfram S. : Cellular Automata의 통계역학. _ 현대 물리학의 리뷰 55_, 3 (1983), 601.\n' +
      '*[WPH*23]Wan Z., Paschalidou D., Huang I., Liu H., Shen B., Xiang X., Liao J., Guibas L.: Cad: Adversarial distillation를 통한 Photorealistic 3d generation. _ arXiv preprint arXiv:2312.06663_(2023).\n' +
      '*[WSK*15]Wu Z., Song S., Khosla A., Yu F., Zhang L., Tang X., Xiao J.: 3d shapenets: a deep representation for volumetric shape. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2015), pp. 1912-1920.\n' +
      '*[WSW22]Wang Y., Skorokhodov I., Wonka P.: HF-NeuS:high-frequency details를 이용한 개선된 표면 재구성. _ 신경 정보 처리 시스템_(2022)의 발전.\n' +
      '*[WSW23]Wang Y., Skorokhodov I., Wonka P.: PET-NeuS: Positional encoding triplicates for neural surfaces. _CVPR_(2023)에서.\n' +
      '*[WWG*21]Wang Q., Wang Z., Genova K., Srinivasan P. P., Zhou H., Barron J. T., Martin-Bruulak R., Snavely N., Funkhouser T.: IBRNet: Learning multi-view image-based rendering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 4690-4699.\n' +
      '*[WWL*23]Wu Q., Wang K., Li K., Zheng J., Cai J.: ObjectSDF++: 개선된 객체-구성 신경 암시적 표면. _ICCV_(2013)에서.\n' +
      '*[WWK*21]Wang Z., Wu S., Xie W., Chen M., Prisacariu V. A.: Nerf-: Neural Radiance fields without known camera parameters. _ arXiv preprint arXiv:2102.07064_(2021).\n' +
      '*[WZ22]Wu R., Zheng C. : 단일 예제로부터 3d 형상을 생성하는 학습. _ arXiv preprint arXiv:2208.02946_(2022).\n' +
      '*[WZF*23]Wu T., Zhang J., Fu X., Wang Y., Ren J., Pan L., Wu, Yang L., Wang J., Qian C., et al.: Omnibject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 803-814.\n' +
      '*[WZX*16]Wu J., Zhang C., Xue T., Freeman B., Tenenbaum J.: 3d 생성적 적대적 모델링을 통한 객체 형상의 확률적 잠재 공간 학습. _ 신경 정보 처리 시스템들의 발전들_(2016).\n' +
      '*[WZZ*23]Wang T., Zhang B., Zhang T., Gu S., Bao J., Baltrusaitis T., Shen J., Chen D., Wen F., Chen Q., et al.: RODIN: 확산을 이용하여 3d 디지털 아바타를 조각하기 위한 생성 모델. _CVPR_(2023)에서.\n' +
      '*[JYW*23]Xu D., Jiang Y., Wang P., Fan Z., Wang Y., Wang Z.: Neurallift-360: in-the-wild 2d 사진을 360deg 뷰로 3d 객체에 올리는 단계. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 4479-4489.\n' +
      '*[KKJ*23]Xiong Z., Kang D., Jin D., Chen W., Bao L., Cui S., Han X.: Get3dhuman: Lifting stylegan-human in the 3d generative model using pixel-aligned reconstruction priors. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2023), pp. 9287-9297.\n' +
      '*[XJS\\({}^{*}\\)23]Xu H., Song G., Jiang Z., Zhang J., Shi Y., Liu J., Ma W., Feng J., Luo L.: OmniAvaatar: Geometry-guided controllable 3d head synthesis. _CVPR_ (2023), 18, 19\n' +
      '*[XITL\\({}^{*}\\)23]Xu Y., Tan H., Luan F., Bi S., Wang P., Li J., Shi Z., Sunkavalli K., Weitzstein G., Xu Z., et al.: Dmv3d: 3d 대형 재구성 모델을 이용한 잡음제거 멀티뷰 확산. _ arXiv preprint arXiv:2311.09217_(2023).\n' +
      '*[XTS\\({}^{*}\\)22]Xie Y., Takikawa T., Saito S., Litany O., Yan S., Khan N., Tombaari P., Tompkin J., Sitzmann V., Sridhar S.: Neural fields in visual computing and beyond. In _Computer Graphics Forum_(2022), vol. 41, Wiley Online Library, pp. 641-676.\n' +
      '*[XWC\\({}^{*}\\)19]Xu Q., Wang W., Ceylan D., Mech R., Neumann U:Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. _ 신경 정보 처리 시스템들(32_(2019)에서의 발전들.\n' +
      '*[XX23]Xia W., Xue J.-H.: deep generative 3d-aware image synthesis에 대한 survey. _ ACM Computing Surveys 56_, 4 (2023), 1-34.\n' +
      '*[XXB\\({}^{*}\\)23]Xu Y., Yifan W., Bergman A. W., Chai M., Zhou B., Weitzstein G.: 층상 표면 부피를 갖는 효율적인 3d 관절 인간 세대. _ arXiv preprint arXiv:2307.05462_(2023).\n' +
      '*[XXC\\({}^{*}\\)23]Xiu Y., Yang J., Cao X., Tziomas D., Black M. J.: Eco: Explicit clothed humans optimized via normal integration. IEEE/CVF Conference on Computer Vision and Pattern Recognition_(2023)의 _Proceedings, pp. 512-523.\n' +
      '*[XYHT23]Xiang J., Yang J., Huang B., Tong X.: 2d 확산 모델을 이용한 3d-aware image generation. _ arXiv preprint arXiv:2303.17905_(2023).\n' +
      '*[XYTB22]Xiu Y., Yang J., Tziomas D., Black M. J.: Icon: Implicit clothed humans obtained from normalals. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_ (2022), IEEE, pp.13286-13296.\n' +
      '*[YAK\\({}^{*}\\)20]Yifan W., Aigerman N., Kim V. G., Chaudhuri S., Sorkine-Hornung O.: Neural cage for detail-preserving 3d deformation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2020), pp. 75-83.\n' +
      '*[YBZ\\({}^{*}\\)22]Yang B., Bao C., Zeng J., Bao H., Zhang Y., Cui Z., Zhang G.: NeuMesh: Learning disentangled neural mesh-based implicit field for geometry and texture editing. _ECCV_(2022)에서, Springer.\n' +
      '*[YGKL19]Yariv L., Gu J., Kasten Y., Lipman Y. : 신경 암시적 표면의 볼륨 렌더링. _ 신경 정보 처리 시스템 34_(2021), 4805-4815에서의 발전.\n' +
      '*[YGMG23]Yoo P., Guo J., Matsuo Y., Gu S. S.: Dreamsparse: sparse view 주어진 2d frozen diffusion model with plato\'s cave에서 탈출. _ CoRR_(2023).\n' +
      '*[YHH\\({}^{*}\\)19a]Yang G., Huang X., Hao Z., Liu M. - Y., Belongie S., Hariharan B.: Pointflow: 연속 정규화 흐름을 갖는 3d 포인트 클라우드 생성. In _Proceedings of the IEEE/CVF international conference on computer vision_(2019), pp.4541-4550.\n' +
      '*[YHH\\({}^{*}\\)19b]Yang G., Huang X., Hao Z., Liu M. - Y., Belongie S., Hariharan B.: Pointflow: 연속 정규화 흐름을 갖는 3d 포인트 클라우드 생성. In _Proceedings of the IEEE/CVF international conference on computer vision_(2019), pp.4541-4550.\n' +
      '*[YLM\\({}^{*}\\)22]Yan X., Lin L., Mitra N. J., Lischinski D., Cohen-Or D., Huang H.: Shaperformer: Transformer-based shape completion via sparse representation. _CVPR_(2022)에서.\n' +
      '*[YLWD22]Yang Z., Li S., Wu W., Dai B.: 3dhungan: photo-realistic 3d-aware human image generation. _ arXiv preprint arXiv:2212.07378_(2022).\n' +
      '*[YLX\\({}^{*}\\)23]Yang X., Luo Y., Xiu Y., Wang W., Xu H., Fan Z.: D-if: Uncertainty-aware human digitization via implicit distribution field. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2023), pp. 9122-9132.\n' +
      '*[YPN\\({}^{*}\\)22]Yu Z., Peng S., Niemeyer M., Sattler T., Geiger A.: Monosdf: Exploration monocular geometric cues for neural implicit surface reconstruction. _ 신경 정보 프로세싱 시스템들(35_(2022), 25018 내지 25032)에서의 진보들.\n' +
      '*[YRSh21]Yifan W., Rahmann L., Sorkine-hornung O.: 암시적 변위 필드를 갖는 기하-일관성 신경 형상 표현. In _International Conference on Learning Representations_ (2021).\n' +
      '*[YSL\\({}^{*}\\)22]Yuan Y. - J, Sun Y. - T, 레이용 - K., Ma Y., Jia R., Gao L.: NeRF-Editing: Geometry editing of neural radiance fields. _CVPR_(2022)에서.\n' +
      '*[YSW\\({}^{*}\\)19]Yifan W., Serena F., Wu S., Oztireli C., Sorkine-Hornung O.: 점 기반 기하 처리를 위한 미분 가능한 표면 분할. _ ACM Transactions on Graphics (TOG) 38_, 6 (2019), 1-14.\n' +
      '*[YTB\\({}^{*}\\)21]Yennamandra T., Tewari A., Bernard F., Seidel H.-P., Elgharib M., Cremers D., Theobalt C.:13DMM: Deep implicit 3d morphable model of human head. _CVPR_(2021)에서.\n' +
      '*[YXZ\\({}^{*}\\)23]Yu X., Xu M., Zhang Y., Liu H., Ye C., Wu Y., Yan Z., Zhu C., Xiong Z., Liang T., et al.: Mvimgnet: 멀티뷰 이미지들의 대규모 데이터세트. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 9150-9161.\n' +
      '*[YYC\\({}^{*}\\)23]Yu W., Yuan L., Cao Y. - P., Gao X., Li X., Quan L., Shan Y., Tian Y.: Hifi-123: highfidelity one image to the 3d content generation. _ arXiv preprint arXiv:2310.06744_(2023).\n' +
      '*[YYTK21]Yu A., Ye V., Tancik M., Kanazawa A.:pixelnerf: neural radiance field from one or few images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp.4578-4587.\n' +
      '*[YZX\\({}^{*}\\)21]Yang B., Zhang Y., Xu Y., Li Y., Zhou H., Bao H., Zhang G., Cui Z.: Learning object-compositional neural radiance field for editingable scene rendering. _ICCV_(2021)에서.\n' +
      '*[ZAB\\({}^{*}\\)22]Zheng Y., Abrevara V. F., Bohler M. C., Chen X., Black M. J., Hilligos O.: IMavatar: Implicit morphable head avatars from video. _CVPR_(2022)에서.\n' +
      '*[ZBT23]Zielonka W., Bolkart T., Thies J.: Instant volumetric head avatars. _CVPR_(2023)에서.\n' +
      '*[ZCY\\({}^{*}\\)23]Zhang H., Chen B., Yang H., Qu L., Wang X., Chen L., Long C., Zhu F., Du K., Zheng M.: Avatarverse: High-quality \\(\\&\\) stable 3d avatar creation from text and pose. _ arXiv preprint arXiv:2308.03610_(2023).\n' +
      '*[ZDW21]Zhou L., Du Y., Wu J.: 포인트-복셀 확산을 통한 3d 형상 생성 및 완성. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2021), pp. 5826-5835.\n' +
      '*[ZJ16]Zhou Q., Jacobson A.: Thingi10k: 10,000개의 3d-프린팅 모델의 데이터세트. _ arXiv preprint arXiv:1605.04797_(2016).\n' +
      '*[ZJY\\({}^{*}\\)22]Zhang J., Jiang Z., Yang D., Xu H., Shi Y., Song G., Xu Z., Wang X., Feng J.: Avatargen: 3d Generative model for animatable human avatars. In _European Conference on Computer Vision_ (2022), Springer, pp. 668-685.\n' +
      '*[ZKB\\({}^{*}\\)22]Zhang K., Kolkin N., Bi S., Luan F., Xu Z., Shechtman E., Sawaley N.: ARF: Artistic radiance field. _ECCV_(2022)에서, Springer.\n' +
      '*[ZKW\\({}^{*}\\)23]Zhou A., Kim M. J., Wang L., Florence P., Finn C.: Nerf in your palm in your palm: Corrective augmentation via robotics via novel-view synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), pp. 17907-17917.\n' +
      '*[ZLC\\({}^{*}\\)23]Zhao Z., Liu W., Chen X., Zeng X., Wang R., Cheng P., Fu B., Chen T., Yu G., Gao S., Michelangelo: Conditional 3dshape generation based on shape-image-text aligned latent representation. _ arXiv preprint arXiv:2306.17115_(2023).\n' +
      '*[ZLLD21]Zhi S., Laidlow T., Leutenegger S., Davison A. J.: In-place scene labeling and understanding with implicit scene representation. _ICCV_(2021)에서, pp. 15838-15847.\n' +
      '*[ZLLS22]Zhang K., Luan F., Li Z., Snavely N.: Iron: Inverse rendering by optimizing neural sdfs and materials from photometric images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 5565-5574.\n' +
      '*[ZLW*21]Zhang K., Luan F., Wang Q., Bala K., Snavely N.: Physg: Inverse rendering with spherical gaussians for physics-based material editing and relighting. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2021), pp. 5453-5462.\n' +
      '*[ZLW*22]Zhang J., Li X., Wan Z., Wang C., Liao J.: Fotherl: Few-shot dynamic neural radiance field for face reconstruction and expression editing. _SIGGRAPH Asia 2022 Conference Papers_(2022), pp. 1-9.\n' +
      '*[ZLW*23]Zhang J., Li X., Wan Z., Wang C., Liao J.: Text2nerf: Text-driven 3d scene generation with neural radiance fields. _ arXiv preprint arXiv:2305.11588_(2023).\n' +
      '*[ZLM72]Zheng X., Liu Y., Wang P., Tong X.: Sdf-stylegan: Implicit sdf-based stylegan for 3d shape generation. In _Computer Graphics Forum_(2022), vol. 41, Wiley Online Library, pp. 52-63.\n' +
      '* [ZLZ+22]Zhu H., Liu Z., Zhou Y., Ma Z., Cao X.: DNF: 회절신경계 for lensless microscopic imaging. _ Optics Express 30_, 11(2022), 18168-18178.\n' +
      '*[ZLZ+23]Zhang J., Li X., Zhang Q., Cao Y., Shan Y., Liao J.: Hummer: 기준 유도 확산을 통한 단일 이미지 내지 3d 인간 세대 _ arXiv preprint arXiv:2311.16961_(2023).\n' +
      '*[ZML*22]Zhou J., Ma B., Liu Y. - S., Fang Y., Han Z.: Learning consistency-aware unsigned distance functions progressively from raw point cloud. _ 신경 정보 처리 시스템(35_(2022), 16481-16494)에서의 발전.\n' +
      '*[ZPL*22]Zhu Z., Peng S., Larsson V., Xu W., Bao H., Cui Z., Oswald M. R., Pollefeys M.: Nice-slam: Neural implicit scalable encoding for slam. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 12786-12796.\n' +
      '*[ZPVBG02]Zwicker M., Pfister H., Van Baar J., Gross M.:Ewa splatting. _ IEEE Transactions on Visualization and Computer Graphics_ 8, 3(2002), 223-238.\n' +
      '*[ZPW*23]Zheng X. -Y., Pan H., Wang P.-S., Tong X., Liu Y., Shu H.-Y.: 제어가능한 3d 형상 생성을 위한 국부 주의 sdf 확산 _ arXiv preprint arXiv:2305.04461_(2023).\n' +
      '*[ZQL*23]Zhang L., Qiu Q., Lin H., Zhang Q., Shi C., Yang W., Shi Y., Yang S., Xu L., Yu J.: DreamFace: Progressive generation of animatable 3d face under text guidance. _ ACM Trans. Graph.__ (2023).\n' +
      '*[ZSD*21]Zhang X., Srinivasan P. P., Deng B., Debevec P., Freeman W. T., Barron J. T.: Nerfactor: Neural factorization of shape and reflectance under unknown illumination. _ ACM Transactions on Graphics(ToG) 40_, 6(2021), 1-18.\n' +
      '*[ZSH*22]Zhang Y., Sun J., He X., Fu H., Jia R., Zhou X. : 역렌더링을 위한 간접 조명 모델링. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), pp. 18643-18652.\n' +
      '*[ZST08]Zhang W., Sun J., Tang X.: Cat head detection-how to effectively exploit shape and texture features. In _Computer Vision-ECCV\'2008: 10th European Conference on Computer Vision, Marseille, France, October 12-18, 2008, Proceedings, Part IV 10_(2008), Springer, pp. 802-816.\n' +
      '*[ZTF*18]Zhou T., Tucker R., Flynn J., Fyffe G., Snavely N.: Stereo magnification: Learning view synthesis using multiplane images. _ arXiv preprint arXiv:1805.09817_(2018).\n' +
      '*[ZTNW23]Zhang B., Tang J., Niessner M., Wonka P.: 3dshape2vecset: 신경장 및 생성 확산 모델에 대한 3d 형상 표현 _ arXiv preprint arXiv:2301.11445_(2023).\n' +
      '*[ZTS*16]Zhou T., Tulsiani S., Sun W., Malik J., Efros A. A.: View synthesis by appearance flow. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_(2016), Springer, pp. 286-301.\n' +
      '*[ZVW*22]Zeng X., Vahdat A., Williams F., Gojcic Z., Litany, O., Fidler S., Kreis K.: Lion: Latent point diffusion models for 3d shape generation. _ arXiv preprint arXiv:2210.06978_(2022).\n' +
      '*[ZWL*23]Zhuang J., Wang C., Liu L., Lin L., Li G.: Dreameditor: Text-driven 3d scene editing with neural fields. In _SIGGRAPH Asia Conference Papers_ (2023).\n' +
      '*[ZXA*23]Zhu C., Xiao F., Alvarado A., Babaei Y., Hu J., El-Mohri H., Culatana S., Sumbaly R., Yan Z.: Egoobjects: 미세 입자 객체 이해를 위한 대규모 자기 중심 데이터세트. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_ (2023), pp. 20110-20120.\n' +
      '*[ZYHC22]Zheng M., Yang H., Huang D., Chen L.: ImFace:Implicit neural representation을 갖는 비선형 3d morphable face model. _CVPR_(2022)에서.\n' +
      '*[ZYLD21]Zheng Z., Yu T., Liu Y., Dai Q.: Pamir: Parametric model-conditioned implicit representation for image-based human reconstruction. _ IEEE 트랜잭션들은 패턴 분석 및 머신 인텔리전스 44_, 6(2021), 3170-3184에 관한 것이다.\n' +
      '*[ZYW*23]Zheng Y., Yifan W., Wetzstein G., Black M. J., Hilliges O.: PointAvatar: Deformable point-based head avatars from video. _CVPR_(2023)에서.\n' +
      '*[ZZZ3]Zhu J., Zhuang P.: Hifa: Highfidelity text-to-3d with advanced diffusion guidance. _ arXiv preprint arXiv:2305.18766_(2023).\n' +
      '*[ZZF*23]Zhuang Y., Zhang Q., Feng Y., Zhu H., Yao Y., Li X., Cao Y. - P., Shan Y., Cao X.: Antialaliased neural implicit surfaces with encoding level of detail. _ arXiv preprint arXiv:2309.10336_(2023).\n' +
      '*[ZZK*20]Zamorski M., Zieba M., Klukowski P., Nowak R., Kurach K., Stokowiec W., Trzcinski T.: Adversarial autoencoder for compact representations of 3d point cloud. _ Computer Vision and Image Understanding 193_(2020), 102921.\n' +
      '*[ZZW*23]Zhuang Y., Zhang Q., Wang X., Zhu H., Feng Y., Li X., Shan Y., Cao X.: Neai: plug-and-play neural ambient illumination에 대한 pre-convoluted representation. _ arXiv preprint arXiv:2304.08757_(2023).\n' +
      '*[ZZZ*18]Zhu J.-Y., Zhang Z., Zhang C., Wu J., Torralba A., Tenebraum J., Freeman B.: Visual object networks: Disentangled 3d representations를 갖는 이미지 생성 _ 신경 정보 처리 시스템들(31_(2018)의 진보들.\n' +
      '*[ZZZ*23a]Zhang J., Zhang X., Zhang H., Liew J. H., Zhang C., Yang Y., Feng J.: Avatarstudio: High-fidelity and animatable 3d avatar creation from text. _ arXiv preprint arXiv:2311.17917_(2023).\n' +
      '*[ZZZ*23b]Zhu J., Zhu H., Zhang Q., Zhu F., Ma Z., Cao X.: Pyramid nerf: Frequency guided fast radiance field optimization. _ International Journal of Computer Vision_ (2023), 1-16.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>