<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# FusChat: Knowledge Fusion of Chat Models\n' +
      '\n' +
      ' Fanqi Wan, Ziyi Yang, Longguang Zhong, Xiaojun Quan, Xinting Huang, Wei Bi\n' +
      '\n' +
      'School of Computer Science and Engineering, Sun Yat-sen University, China\n' +
      '\n' +
      '{wanfq,yangzy39,zhonglg5}@mail2.sysu.edu.cn, quanxj3@mail.sysu.edu.cn\n' +
      '\n' +
      '{timxthuang,victoriabi}@tencent.com\n' +
      '\n' +
      'Corresponding authors.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, FusLLM introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the FusELM framework to realize the fusion of chat LLMs, resulting in FusChat. FusChat comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs to derive multiple target LLMs of identical structure and size via lightweight fine-tuning. Then, these target LLMs are merged within the parameter space, wherein we propose a novel method for determining the merging weights based on the variation ratio of parameter matrices before and after fine-tuning. We validate our approach using three prominent chat LLMs with diverse architectures and scales, namely NH2-Mixtral-8x7B, NH2-Solar-10.7B, and OpenChat-3.5-7B. Experimental results spanning various chat domains demonstrate the superiority of FusChat-7B across a broad spectrum of chat LLMs at 7B and 34B scales, even surpassing GPT-3.5 (March) and approaching Mixtral-8x7B-Instruct. Our code, model weights, and data are openly accessible at [https://github.com/fanqiwan/FuseLLM](https://github.com/fanqiwan/FuseLLM).\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      'Large language models (LLMs) such as GPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023) series have demonstrated remarkable success across a wide range of natural language processing (NLP) tasks. It has become an imperative requirement for individuals or corporations to build their proprietary LLMs. However, the computational resources and time costs associated with LLM development remain prohibitively high for most entities. Despite the structural and functional differences among LLMs, they often exhibit similar capabilities across various tasks. Therefore, moving beyond the traditional approach of training a LLM from scratch, an alternative option is to combine existing LLMs into a new, more powerful one, which is termed _knowledge fusion of LLMs_ by Wan et al. (2024). If successful, this fusion not only reduces the initial training costs but also enables the combined model to leverage the strengths of multiple LLMs.\n' +
      '\n' +
      'The endeavor to integrate the capabilities of multiple models has been a long-standing pursuit. For example, ensemble methods (Littlestone and Warmuth, 1994; Jiang et al., 2023) directly aggregate the outputs of different models to enhance prediction performance and robustness. However, this approach requires maintaining multiple trained models and executing each during inference, which is inefficient for LLMs due to their substantial memory and inference time requirements. Another approach is to directly merge several neural networks into a single network through arithmetic operations in the parameter space (Gupta et al., 2020). This approach typically assumes uniform network architectures and seeks to merge the parameters of different neural networks either through manual merging weights (Wortsman et al., 2022; Yadav et al., 2023) or by automatically obtaining merging weights based on model gradients or representations of additional data (Matena and Raffel, 2022; Jin et al., 2022). Recently, FuseLLM (Wan et al., 2024) introduced a new paradigm for integrating the capabilities of multiple LLMs. This approach externalizes the knowledge of multiple source LLMs using their generated probability distribution matrices and transfers their collective knowledge into a target LLM through lightweight continual training. Consequently, FuseLLM facilitates the fusion of multiple pre-trained LLMs with distinct architectures into a unified LLM.\n' +
      '\n' +
      'In this study, we extend the framework of FuseLLM to fuse multiple chat LLMs with diverse architectures and scales, leading to the development of FuseChat, which comprises two main stages. Firstly, it conducts knowledge fusion for source LLMs with varying structures and scales to derive multiple target LLMs of identical structure and size. To this end, FuseChat follows the idea of FuseLLM but adopts a pairwise knowledge fusion strategy. Secondly, these target LLMs are merged within the parameter space to incorporate the collective knowledge and respective advantages from source LLMs. For merging, we introduce VaRM (**V**ariation **R**atio **M**erge), a novel method for determining the merging weights based on the variation ratio of parameter matrices before and after fine-tuning. In contrast to previous approaches, VaRM enables the automatic allocation of distinct weights to each parameter matrix based on the variation ratio of updates during fine-tuning. This facilitates merging LLMs with fine-grained weights without requiring additional training efforts.\n' +
      '\n' +
      'FuseChat offers superior scalability compared to FuseLLM. Firstly, while FuseLLM limits its exploration to LLMs of the same size as the target LLM, FuseChat delves into the fusion of source chat LLMs with varying sizes. This broader scope allows for greater adaptability to diverse model configurations and requirements. Secondly, the framework of FuseLLM does not seamlessly support the inclusion of new source LLMs as it requires the combination of distribution matrices from all source LLMs during continual training. In contrast, integrating a new source LLM at any scale in FuseChat is plug-and-play, requiring only obtaining a target LLM from the new source LLM and merging it with the existing version of FuseChat. Given the frequent updates of chat LLMs in the open-source community2, FuseChat appears to be more promising for the fusion of chat models.\n' +
      '\n' +
      'Footnote 2: There are 7300+ chat LLMs available on HuggingFace as of drafting this report.\n' +
      '\n' +
      'To empirically demonstrate the effectiveness of FuseChat, we implement FuseChat using three representative open-source chat LLMs for fusion: NH2-Mixtral-8x7B (Jiang et al., 2024), NH2-Solar-10.7B (Kim et al., 2023), and OpenChat-3.5-7B (Wang et al., 2023). Experimental results on MT-Bench (Zheng et al., 2023), a cutting-edge benchmark consisting of eight different domains to assess chat LLMs\' multi-turn dialogue ability, confirm that FuseChat outperforms all the source LLMs and fine-tuned baselines at 7B and 10.7B scales, even approaching the 8x7B MoE source LLM. Moreover, among all the merging methods, the proposed VaRM achieves the best performance, indicating the efficacy of merging weights based on the variation ratio of updates.\n' +
      '\n' +
      ' Related Work\n' +
      '\n' +
      'Model FusionThe fusion of capabilities from diverse models has been a long-standing objective, with existing approaches mainly falling into three categories. Firstly, the traditional technique of model _ensemble_ combines the outputs of multiple models to enhance overall system performance (Littlestone and Warmuth, 1994; Sagi and Rokach, 2018). Note that this technique doesn\'t involve the explicit merging of multiple models into a new one. Common methods for model ensemble typically employ weighted averaging (Littlestone and Warmuth, 1994) or majority voting (Monteith et al., 2011) to consolidate predictions from various models. Recently, Jiang et al. (2023) introduced an ensemble framework designed to leverage the diverse strengths of multiple open-source LLMs. This framework first employs a pairwise comparison method to detect subtle distinctions among candidate outputs. Then, it combines the top-ranked candidates to produce an enhanced output.\n' +
      '\n' +
      'Secondly, _model merging_ presents another approach that facilitates model fusion within the parameter space. Wortsman et al. (2022) combined multiple models, obtained through different strategies or configurations, through a linear weighted average of parameters, resulting in enhanced overall performance. Likewise, Shoemake (1985) and Ilharco et al. (2022) integrated the capabilities of distinct models by employing spherical linear interpolation and task arithmetic to merge model parameters. To avoid redundant parameter interference, Yadav et al. (2023) and Yu et al. (2023) suggested pruning low-amplitude varying parameter values before model merging. Furthermore, Matena and Raffel (2022) and Jin et al. (2022) incorporated supplementary data to compute merging weights based on model gradients or representations, eliminating the need for hyperparameter tuning.\n' +
      '\n' +
      'Lastly, FuseLLM (Wan et al., 2024) presents a new paradigm for knowledge fusion of multiple LLMs, which leverages the probabilities distribution matrices generated from source LLMs to transfer the collective knowledge and respective advantages into a target LLM. In comparison to the model ensemble method, which requires the parallel deployment of multiple models, and the model merging approach, which is generally limited to models with identical architectures, FuseLLM supports the fusion of multiple source LLMs with different architectures into a target LLM.\n' +
      '\n' +
      'Knowledge DistillationKnowledge distillation (Hinton et al., 2015), initially proposed for model compression, involves training a student model under the guidance of one or more teacher models. In the NLP community, knowledge distillation has been widely applied to text classification tasks. These applications include training the student model to replicate the teacher\'s output distribution (Sanh et al., 2019; Turc et al., 2019), as well as features (Sun et al., 2019; Jiao et al., 2020) and relations (Wang et al., 2020) derived from intermediate layers of the teacher model. In the realm of text generation, the conventional approach focuses on minimizing the KL divergence between the student and teacher generation distributions. This is achieved by using the teacher\'s probability distributions at each time step as supervision (Khanuja et al., 2021; Gu et al., 2023; Agarwal et al., 2023) or by directly training on the teacher\'s generated texts (Peng et al., 2023; Xu et al., 2023).\n' +
      '\n' +
      '## 3 Knowledge Fusion of Chat Models\n' +
      '\n' +
      'The core concept of FuseChat comprises two stages. Firstly, it externalizes and transfers the knowledge and capabilities inherent in source chat LLMs to multiple target LLMs of the same structure and size. Secondly, these target LLMs are incorporated into a final fused LLM through model merging. An overview of FuseChat is presented in Figure 1.\n' +
      '\n' +
      'Specifically, considering \\(K\\) source chat LLMs \\(\\{\\mathcal{M}^{s}_{i}\\}_{i=1}^{K}\\) with varying architectures and scales, FuseChat first specifies a source LLM \\(\\mathcal{M}^{s}_{v}\\) as the _pivot_ and then applies pairwise knowledge fusion for the pivot and each of the rest LLMs, obtaining \\((K-1)\\) target LLMs \\(\\{\\mathcal{M}^{t}_{j}\\}_{j=1}^{K-1}\\) which share the same architecture and initial parameters as the pivot LLM. To perform the pairwise knowledge fusion, FuseChat prompts these source LLMs using a compact and representative training dataset \\(\\mathcal{D}\\) to showcase their inherent knowledge by predicting the next token. The resulting probabilistic distribution matrices are then utilized to perform pairwise knowledge fusion through lightweight fine-tuning as FuseLLM (Wan et al., 2024). After that, the (\\(K-1\\)) target LLMs are combined in the parameter space using a specific merging method to yield the fused LLM \\(\\mathcal{M}^{f}\\). To incorporate fine-grained advantages of source LLMs, we introduce VARM (**V**ariation **R**atio **M**erge) to determine the merging weights based on the variation ratio of parameter matrices before and after fine-tuning.\n' +
      '\n' +
      'In the following sections, we provide a brief introduction to the preliminaries, followed by a detailed description of the pairwise knowledge fusion and model merging in FuseChat.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      'Let us consider a text sequence \\(q\\) of length \\(N\\), which is sampled from the training dataset \\(\\mathcal{D}\\). The sequence preceding the \\(i\\)th token is represented by \\(t_{<i}=(t_{1},t_{2},\\ldots,t_{i-1})\\). The causal language modeling (CLM) objective for training a language model parameterized by \\(\\theta\\) is defined as minimizing the negative log-likelihood:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{CLM}}=-\\mathbb{E}_{q\\sim\\mathcal{D}}\\left[\\sum_{i}\\log p_{ \\theta}(t_{i}|t_{<i})\\right], \\tag{1}\\]\n' +
      '\n' +
      'where \\(p_{\\theta}(t_{i}|t_{<i})\\) is the model\'s predicted probability for the \\(i\\)th token given the preceding tokens.\n' +
      '\n' +
      'To facilitate the fine-tuning of chat LLMs, wherein the text sequence \\(q\\) often consists of a multiple-turn dialogue between a user and an assistant, we follow previous works (Chiang et al., 2023; Wan et al., 2023) and mask the user instructions when calculating the training loss \\(\\mathcal{L}_{\\text{CLM}}\\).\n' +
      '\n' +
      'The above objective decomposes sequence likelihood into token-level cross-entropy losses, comparing each token\'s predicted distribution to its one-hot representation. To provide a more generalized perspective, we reframe this token-level view into a sequential distribution format. Specifically, for the text sequence \\(q\\), we aggregate token-level predictions to form a probabilistic distribution matrix, \\(\\mathbf{P}_{q}^{\\theta}\\in\\mathbb{R}^{N\\times V}\\), where the \\(i\\)-th row represents the distribution predicted by the model for the \\(i\\)th token over the vocabulary of size \\(V\\). The CLM objective can then be interpreted as reducing the discrepancy between \\(\\mathbf{P}_{q}^{\\theta}\\) and the one-hot label matrix, \\(\\mathbf{O}_{q}\\in\\{0,1\\}^{N\\times V}\\), where each row is a one-hot representation of the corresponding gold token. Formally, the CLM objective is transformed into the following representation:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{CLM}}=-\\mathbb{E}_{q\\sim\\mathcal{D}}\\left[\\mathbb{D}( \\mathbf{P}_{q}^{\\theta},\\mathbf{O}_{q})\\right], \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\mathbb{D}(\\cdot,\\cdot)\\) represents the discrepancy function between two matrices, and it is equivalent to Eq. 1 when implemented using the KL divergence.\n' +
      '\n' +
      '### Pairwise Knowledge Fusion\n' +
      '\n' +
      'Taking this perspective on a language model, we follow Wan et al. (2024) and assume that the probabilistic distribution matrix reflects certain inherent knowledge of the language model in understanding the text. Consequently, different probabilistic distribution matrices for the same text, originating from various LLMs, can be used to represent the diverse knowledge embedded within these models. Based on this assumption, the proposed FuseChat externalizes the knowledge of source LLMs through probabilistic modeling and performs pairwise knowledge fusion by fine-tuning target LLMs using the generated distribution matrices of the source LLMs.\n' +
      '\n' +
      'Specifically, for each text sample \\(q\\) in the training dataset \\(\\mathcal{D}\\), we first apply the provided \\(K\\) source LLMs to obtain a set of probabilistic distribution matrices, denoted as \\(\\{\\mathbf{P}_{q}^{\\theta_{j}}\\}_{j=1}^{K}\\), where \\(\\theta_{j}\\) represents\n' +
      '\n' +
      'Figure 1: Illustration of FuseLLM and the proposed FuseChat. Distinct animal icons symbolize diverse LLMs, where species and sizes correspond to distinct architectures and scales. FuseChat extends FuseLLM and provides enhanced scalability and flexibility for the fusion of multiple chat LLMs.\n' +
      '\n' +
      'the parameters of the \\(j\\)th source LLM. Note that these source LLMs may employ different tokenizers, and token alignment is often necessary for proper mapping of probabilistic distribution matrices (Fu et al., 2023; Wan et al., 2024). Utilizing these matrices, we externalize the knowledge from individual models into a unified space, essentially creating unified probabilistic representations over the text.\n' +
      '\n' +
      'Then, pairwise knowledge fusion is conducted between the pivot LLM and each of the rest source LLMs. To achieve this, we denote the probabilistic distribution matrix generated by the pivot LLM as \\(\\mathbf{P}_{q}^{\\theta_{v}}\\) and obtain a set \\(\\{\\mathbf{P}_{q}^{j}\\}_{j=1}^{K-1}\\) of fused matrices as follows:\n' +
      '\n' +
      '\\[\\mathbf{P}_{q}^{j}=\\mathbb{F}\\text{usion}(\\mathbf{P}_{q}^{\\theta_{v}}, \\mathbf{P}_{q}^{\\theta_{j}})|_{v\\neq j}, \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\mathbb{F}\\text{usion}(\\cdot)\\) represents the function that fuses two matrices, and the resulting matrix \\(\\mathbf{P}_{q}^{j}\\) is seen as a representation of the collective knowledge and distinctive strengths of two source LLMs. Among various fusion strategies, this work employs minimum edit distance (MinED) (Wan et al., 2024), which empirically performs the best in both FuseLLM and FuseChat.\n' +
      '\n' +
      'After that, we enforce alignment between the prediction of each target LLM \\(\\mathcal{M}_{j}^{t}\\) and the corresponding fused representation matrices \\(\\mathbf{P}_{q}^{j}\\). We use \\(\\mathbf{Q}_{q}^{\\theta_{j}}\\) to represent the output distribution matrix of the target LLM \\(\\mathcal{M}_{j}^{t}\\) for text \\(q\\), and then define the fusion objective for each target LLM as follows:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{Fusion}}=-\\mathbb{E}_{q\\sim\\mathcal{D}}\\left[\\mathbb{D}( \\mathbf{Q}_{q}^{\\theta_{j}},\\mathbf{P}_{q}^{j})\\right]. \\tag{4}\\]\n' +
      '\n' +
      'The overall training objective for each target LLM consists of a weighted combination of the causal language modeling objective \\(\\mathcal{L}_{\\text{CLM}}\\) and the fusion objective \\(\\mathcal{L}_{\\text{Fusion}}\\) as follows:\n' +
      '\n' +
      '\\[\\mathcal{L}=\\lambda\\mathcal{L}_{\\text{CLM}}+(1-\\lambda)\\mathcal{L}_{\\text{ Fusion}}. \\tag{5}\\]\n' +
      '\n' +
      '### Model Merging\n' +
      '\n' +
      'Given that the fused target LLMs \\(\\{\\mathcal{M}_{j}^{t}\\}_{j=1}^{K-1}\\) share identical architecture and scale while possessing diverse advantages and capabilities learned from the source LLMs, which can be further integrated in the parameters space (Wortsman et al., 2022) to obtain the final fused LLM \\(\\mathcal{M}^{f}\\):\n' +
      '\n' +
      '\\[\\mathcal{M}^{f}=\\mathbb{M}\\text{erge}(\\{\\mathcal{M}_{1}^{t},\\mathcal{M}_{2}^{ t},...,\\mathcal{M}_{K-1}^{t}\\}), \\tag{6}\\]\n' +
      '\n' +
      'where \\(\\mathbb{M}\\text{erge}(\\cdot)\\) denotes the function that merges multiple target LLMs into a final LLM that combines collective knowledge and distinctive strengths of these target LLMs.\n' +
      '\n' +
      'To enhance the adaptability of FuseChat, it is essential to maintain the simplicity of the \\(\\mathbb{M}\\text{erge}\\) function. Firstly, it should be capable of automatically computing the merging weights, eliminating the need for intricate hyperparameter tuning. Secondly, the merging procedure should not require the incorporation of additional data for the calculation of model gradients or representations.\n' +
      '\n' +
      'Since the parameters of the target LLMs continuously evolve to align their generated distribution matrices with the corresponding source LLMs, we propose **V**ariation **R**atio **M**erge (VARM) to utilize the variation ratio of parameters before and after fine-tuning each target LLM as an indicator of knowledge updates, determining its importance in the \\(\\mathbb{M}\\text{erge}\\) function:\n' +
      '\n' +
      '\\[W_{j,m}=\\frac{\\mathbb{E}_{m}\\Delta\\theta_{j,m}^{2}}{\\sum_{j}^{K-1}\\mathbb{E} _{m}\\Delta\\theta_{j,m}^{2}}. \\tag{7}\\]\n' +
      '\n' +
      'where \\(W_{j,m}\\) represents the merging weight for the parameter unit \\(\\theta_{j,m}\\) (e.g., a matrix) in the target LLM \\(\\mathcal{M}_{j}^{t}\\), while \\(\\mathbb{E}_{m}\\Delta\\theta_{j,m}^{2}\\) denotes the average squared variation of parameters in the unit \\(\\theta_{j,m}\\).\n' +
      '\n' +
      'In our preliminary explorations, we have investigated several alternative approaches to determining the weights. These include replacing the square operation with the absolute operation or using softmax. However, the results indicate that none of these alternatives outperforms the current method.\n' +
      '\n' +
      'In this work, we define the parameter unit for model merging at the matrix level3. This approach enables the automatic allocation of distinct merging weights to each parameter matrix, thereby facilitating the integration of fine-grained advantages from multiple target LLMs into the fused LLM.\n' +
      '\n' +
      '### Discussions\n' +
      '\n' +
      'The reasons why FuseChat does not directly follow FuseLLM to fuse multiple source LLMs of different structures and scales are twofold. Firstly, directly fusing all the source LLMs proves to be difficult, as evidenced by the results of OpenChat-3.5-7B Multi in Table 1. Instead, FuseChat adopts a fuse-then-merge strategy, wherein the fusing stage employs pairwise knowledge fusion between the pivot LLM and other source LLMs, reducing the difficulty of model fusion. Secondly, FuseChat offers superior scalability compared to FuseLLM. The framework of FuseLLM requires the combination of distribution matrices from all source LLMs during continual training, which does not seamlessly support the inclusion of new source LLMs. In contrast, FuseChat supports plug-and-play integration of a new source LLM at any scale, requiring only obtaining a target LLM by fusing the new source LLM and the pivot, and then merging it with the existing version of FuseChat.\n' +
      '\n' +
      'Moreover, the concept of knowledge fusion adopted by both FuseChat and FuseLLM shares a fundamentally similar purpose with other related topics, such as traditional model ensemble and merging techniques, as well as the recently prominent topic of mixture of experts (MoEs), because they all aim to leverage the strengths of multiple models (experts). While model ensemble and MoEs require loading multiple models (experts) during inference, which have higher memory requirements, weight merging is limited to models with identical architectures. In contrast, knowledge fusion supports the integration of multiple LLMs with diverse architectures into a single LLM without any additional memory requirement, making it appealing in terms of both flexibility and efficiency.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'In our experiments, we consider a challenging scenario for the fusion of chat LLMs, where the source LLMs exhibit minimal similarities in architectures and scales. Specifically, we conduct experiments with three representative open-source chat LLMs as the source LLMs, including NH2-Mixtral-8x7B4 (Jiang et al., 2024), NH2-Solar-10.7B5 (Kim et al., 2023), and OpenChat-3.5-7B6 (Wang et al., 2023). As for the pivot LLM, which also serves as the starting point for target LLMs, we opt for OpenChat-3.5-7B due to its balanced scale and performance. We then apply pairwise knowledge fusion as introduced in Section 3.2 to obtain two target LLMs OpenChat-3.5-7B Mixtral and OpenChat-3.5-7B Solar. Finally, we merge OpenChat-3.5-7B Mixtral and OpenChat-3.5-7B Solar by our VaRM method (Section 3.3) to obtain the final FuseChat-7B. To assess the performance of FuseChat-7B, we conduct experiments on MT-Bench7(Zheng et al., 2023), a benchmark specifically designed to evaluate chat LLMs\' capabilities in multi-turn dialogues across various domains.\n' +
      '\n' +
      'Footnote 4: [https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO)\n' +
      '\n' +
      'Footnote 5: [https://huggingface.co/NousResearch/Nous-Hermes-2-SOLAR-10.7B](https://huggingface.co/NousResearch/Nous-Hermes-2-SOLAR-10.7B)\n' +
      '\n' +
      'Footnote 6: [https://huggingface.co/openchat/openchat_3.5](https://huggingface.co/openchat/openchat_3.5)\n' +
      '\n' +
      'Footnote 7: [https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge)\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      'Training DatasetTo acquire the advantages of source LLMs during knowledge fusion, while mitigating catastrophic forgetting, we curated a high-quality training dataset named FuseChat Mixture from two sources. Firstly, 50% of our training data is sampled from the dataset used by OpenChat8. Secondly, we collected the remaining training samples, unseen by OpenChat, from open-source communities. These two sources resulted in a collection of around 95,000 dialogues across various domains. Further details of FuseChat Mixture can be found in Appendix A.\n' +
      '\n' +
      'Footnote 8: [https://huggingface.co/openchat/openchat_3.5#dataset-details](https://huggingface.co/openchat/openchat_3.5#dataset-details)\n' +
      '\n' +
      'Training DetailsIn all experiments, we train the OpenChat-3.5-7B using a batch size of 128 and a maximum length of 2048 on a single node with 8x40GB NVIDIA A100 GPUs for three epochs, which takes approximately 7 hours. The model is optimized using the AdamW (Loshchilov and Hutter, 2017) optimizer with \\(\\beta_{1}=0.9\\) and \\(\\beta_{2}=0.999\\), with gradient clipping set to 1.0 and weight decay to 0.0. A cosine learning rate schedule is employed, with a maximum learning rate of 5e-6 and a warmup ratio of 0.03. We empirically set the combination weight \\(\\lambda\\) in Eq. 5 to 0.9. Our training framework is implemented based on the HuggingFace Transformers (Wolf et al., 2020).\n' +
      '\n' +
      'EvaluationWe evaluate FuseChat on MT-Bench, which comprises 80 multi-turn dialogues spanning _writing_, _roleplay_, _reasoning_, _math_, _coding_, _stem_, and _humanities_ domains. We adhere to the default configuration of Zheng et al. (2023) and use GPT-49 (gpt-4-0613) as the evaluator for the generated responses, setting the temperature to 0.0 to ensure replicability. The evaluation score ranges from 1 to 10, with 1 denoting the poorest quality and 10 denoting the best.\n' +
      '\n' +
      'Footnote 9: [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)\n' +
      '\n' +
      'BaselinesIn our experiments, we compare our FuseChat with three categories of baselines. (i) _Closed-source LLMs_: GPT-4 (March), GPT-3.5 (March), and Claude-1.0. (ii) _Source LLMs_: NH2-Mixtral-8x7B, NH2-Solar-10.7B, and OpenChat-3.5-7B. (iii) _Fine-tuned target LLMs_: OpenChat-3.5-7B CLM, which is fine-tuned using only the casual language modeling objective; OpenChat-3.5-7B Multi, which is fine-tuned using the fusion of distributions generated from multiple source LLMs (Wan et al., 2024); OpenChat-3.5-7B Mixtral, which is the corresponding target LLM obtained by fusing OpenChat-3.5-7B and NH2-Mixtral-8x7B; OpenChat-3.5-7B Solar, which is the corresponding target LLM obtained by fusing OpenChat-3.5-7B and NH2-Solar-10.7B. We also evaluate the performance of FuseChat by comparing different merging methods to obtain the _fused LLMs_, including FuseChat-7B Linear(Wortsman et al., 2022), FuseChat-7B SLERP(Shoemake, 1985), FuseChat-7B TA(Ilharco et al., 2022), FuseChat-7B TIES(Yadav et al., 2023), FuseChat-7B DARE(Yu et al., 2023b), and our FuseChat-7B VARM.\n' +
      '\n' +
      '### Overall Results\n' +
      '\n' +
      'In Table 1, we present the overall results of FuseChat compared to baselines of different scales and categories across various domains of MT-Bench. Our observations are as follows. First, we note distinct performance among the three source LLMs across all domains, with OpenChat-3.5-7B exhibiting balanced performance despite its smaller scale. Second, after fine-tuning using the casual language model objective on our high-quality training dataset, the resulting model (OpenChat-3.5-7B CLM) achieves an increased average performance from 7.79 to 7.95, although this improvement is relatively modest and inconsistent across distinct domains. Third, in the category of fine-tuned target LLMs, OpenChat-3.5-7B Multi achieves a relative performance gain of 1.38% over OpenChat-3.5-7B CLM. Notably, OpenChat-3.5-7B Mixtral and OpenChat-3.5-7B Solar, two target LLMs obtained by pairwise knowledge fusion, outperform OpenChat-3.5-7B Multi. Moreover, these target LLMs demonstrate individual strengths in different domains, providing a foundation for subsequent integration into a more powerful LLM. For instance, OpenChat-3.5-7B Mixtral excels in the _reasoning_ domain, surpassing OpenChat-3.5-7B CLM by an average of 12.58%, while OpenChat-3.5-7B Solar achieves the highest scores in both the _extraction_ and _STEM_ domains, with 8.70% and 9.53% relative performance enhancements, respectively.\n' +
      '\n' +
      'The final fused LLM FuseChat-7B is obtained by merging OpenChat-3.5-7B Mixtral and OpenChat-3.5-7B Solar in the parameter space, where various merging methods are explored. It is observed that FuseChat-7B with SLERP, TA, and our VARM outperform all the fine-tuned target LLMs, showcasing FuseChat\'s ability to integrate the unique strengths and collective capabilities of different target LLMs. In contrast, merging methods such as Linear and DARE tend to result in degraded performance. Since the target LLMs exhibit varying parameter variations, designing fine-grained merging weights is crucial for effectively combining their respective advantages. Therefore, methods like Linear, which involves manual weight assignment, and DARE, which eliminates a subset of model parameters before merging, are deemed inappropriate for FuseChat.\n' +
      '\n' +
      'We further demonstrate that FuseChat-7B with VARM consistently outperforms all other merging methods, achieving an average evaluation score of 8.22. This score not only surpasses GPT-3.5 (March)\'s score of 7.94, but also approaches the score of the current state-of-the-art (SOTA) open-source chat LLM, NH2-Mixtral-8X7B, which stands at 8.33. This confirms the effectiveness of the proposed VARM method in utilizing the variation ratio of each parameter matrix to allocate different merging weights, thereby blending updated knowledge at a fine-grained matrix level.\n' +
      '\n' +
      '### Merging Granularities in VARM\n' +
      '\n' +
      'Since the merging granularity of the parameter unit \\(\\theta_{j,m}\\) in Eq. 7 can be adaptively adjusted, we investigate its influence on the final performance of FuseChat-7B VARM.\n' +
      '\n' +
      'In Table 2, we observe a consistent improvement in average performance when transitioning the granularity of merging weights from model level to layer level, and then to matrix level. This suggests that the assignment of fine-grained merging weights is effective for integrating knowledge from multiple target LLMs. However, when the granularity is reduced to the parameter level, we observe a notable decline in performance. This may be attributed to extreme merging weights assigned to specific parameters, which disrupts correlations among other parameters.\n' +
      '\n' +
      'We further investigate the impact of varying merging granularities on the performance of different dialogue turns. Figure 2 illustrates that as the merging granularity progressively decreases from the model level to the layer level and then to the matrix level, the performance of the first turn first experiences enhancement and then declines, while the performance of the second turn exhibits the opposite trend. Despite this fluctuation, there is a consistent improvement in overall performance. These findings suggest that VARM at the matrix granularity adeptly captures the complex dynamics among multiple dialogue turns, thereby leading to optimal overall performance.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c|c} \\hline \\hline\n' +
      '**Models** & **Writing** & **Roleplay** & **Reasoning** & **Math** & **Coding** & **Extraction** & **STEM** & **Hummities** & **Avg.** \\\\ \\hline \\hline \\multicolumn{10}{c}{_Closed-aware LIDMs_} \\\\ \\hline GPT-4 (March) & 9.65 & 8.50 & 9.00 & 6.80 & 8.55 & 9.38 & 9.70 & 9.95 & 8.96 \\\\ GPT-3.5 (March) & 9.20 & 8.40 & 5.65 & 6.30 & 6.90 & 8.85 & 8.70 & 9.55 & 7.94 \\\\ Clande-1.0 & 9.50 & 8.50 & 5.95 & 4.80 & 6.25 & 8.80 & 9.70 & 9.70 & 7.90 \\\\ \\hline \\multicolumn{10}{c}{_Source LIDMs_} \\\\ \\hline NL2:Mixture-87/B & 9.70 & 9.05 & 6.63 & 6.75 & 6.00 & 8.80 & 9.70 & 9.95 & 8.33 \\\\ NL2:Solar-10.7B & 9.50 & 8.40 & 7.35 & 5.55 & 4.00 & 7.85 & 8.80 & 9.80 & 7.66 \\\\ OpenChat-3.5-7B & 9.00 & 7.85 & 7.75 & 5.30 & 5.50 & 8.45 & 8.55 & 9.90 & 7.79 \\\\ \\hline \\multicolumn{10}{c}{_Tine-tuned LIDMs_} \\\\ \\hline OpenChat-3.5-7B CLM & 8.80 & 8.60 & 7.55 & 5.88 & 5.45 & 8.05 & 9.45 & 9.85 & 7.95 \\\\ OpenChat-3.5-7B Multi & 9.35 & 8.38 & 8.40 & 5.35 & 5.15 & 8.70 & 9.28 & 9.50 & 8.06 \\\\ (+2.55\\%) & (+11.26\\%) & (+0.01\\%) & (+0.19\\%) & (+5.09\\%) & (+8.07\\%) & (+1.80\\%) & (+0.51\\%) & (+1.38\\%) \\\\ \\hline OpenChat-3.5-7B Mixat & 9.35 & 8.35 & **8.50** & 5.80 & 5.10 & 8.25 & 9.38 & 9.50 & 8.08 \\\\ (+6.25\\%) & (+2.91\\%) & (+12.58\\%) & (+3.66\\%) & (+6.42\\%) & (+2.48\\%) & (+0.74\\%) & (+0.51\\%) & (+1.64\\%) \\\\ \\hline \\multicolumn{10}{c}{_Fused LIDMs_} \\\\ \\hline \\hline \\multicolumn{10}{c}{_Fused LIDMs_} \\\\ \\hline \\hline \\multicolumn{10}{c}{FuseChat-7B Linear} & 9.18 & 8.23 & 8.00 & 5.78 & 5.35 & 8.50 & 9.43 & 9.78 & 8.05 \\\\ (+3.23\\%) & (+3.40\\%) & (+5.96\\%) & (+7.10\\%) & (+1.83\\%) & (+5.99\\%) & (+0.21\\%) & (+0.71\\%) & (+1.26\\%) \\\\ \\hline \\multicolumn{10}{c}{_FuseChat-7B SLERP_} \\\\ \\hline \\hline \\multicolumn{10}{c}{_FuseChat-7B SLERP_} \\\\ \\hline \\hline \\multicolumn{10}{c}{_FuseChat-7B TA_} \\\\ \\hline \\hline \\multicolumn{10}{c}{_FuseChat-7B TA_} \\\\ \\hline \\hline \\multicolumn{10}{c}{_FuseChat-7B TIES_} \\\\ \\hline \\hline \\multicolumn{10}{c}{_FuseChat-7B DARE_} \\\\ \\hline \\hline \\multicolumn{10}{c}{_FuseChat-7B DARE_} \\\\ \\hline \\hline \\multicolumn{10}{c}{_FuseChat-7B VARM_} \\\\ \\hline \\hline \\multicolumn{10}{c}{_FuseChat-7B VARM_} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Overall results of the proposed FuseChat compared to baselines of different scales and categories across various domains of MT-Bench. Percentages indicate the rate of improvement (in blue)/decrease (in red) compared to OpenChat-3.5-7B CLM.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c|c} \\hline \\hline\n' +
      '**Granularity** & **Writing** & **Roleplay** & **Reasoning** & **Math** & **Coding** & **Extraction** & **STEM** & **Hummities** & **Avg.** \\\\ \\hline \\hline Model & 8.93 & 8.58 & **8.20** & 5.73 & 5.45 & **8.55** & **9.45** & **9.90** & 8.10 \\\\ Layer & **9.43** & 8.73 & 8.00 & **6.15** & 5.25 & **8.55** & 9.18 & **9.90** & 8.15 \\\\ Matrix & 9.20 & 8.63 & 8.00 & **6.15** & **6.15** & 8.50 & 9.30 & 9.85 & **8.22** \\\\ Parameter & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Results of FuseChat-7B VARM with VARM adopting different merging granularities of parameter inputs across various domains of MT-Bench.\n' +
      '\n' +
      'Figure 2: Performance of FuseChat-7B VARM by using varying merging granularities of parameters groups on different dialogue turns in MT-Bench.\n' +
      '\n' +
      'Conclusion\n' +
      '\n' +
      'In this work, we propose an extended framework of FuseLLM to integrate the collective knowledge and individual strengths of multiple structure and scale-varied chat LLMs into a more powerful chat LLM, resulting in FuseChat. FuseChat adopts a fuse-then-merge strategy with two main stages. Firstly, it undertakes pairwise knowledge fusion for source LLMs to derive multiple target LLMs of identical structure and size via lightweight fine-tuning. Then, these target LLMs are merged within the parameter space, wherein we propose a novel method VaRM for determining the merging weights based on the variation ratio of parameter matrices before and after fine-tuning. Experimental results spanning various chat domains demonstrate the superiority of FuseChat across different model scales, even surpassing GPT-3.5 (March) and approaching Mixtral-8x7B-Instruct.\n' +
      '\n' +
      'Moreover, we argue that the concept of knowledge fusion adopted by both FuseChat and FuseLLM shares a fundamentally similar purpose with other related topics, such as the recently popular topic of mixture of experts (MoEs), because they all aim to leverage the strengths of multiple models (experts). However, while MoEs require loading multiple experts during inference, which has higher memory requirements, knowledge fusion supports the integration of multiple LLMs with diverse architectures into a single LLM without any additional memory requirement, making it more memory-efficient. To the best of our knowledge, MoEs typically employ more than six experts, while FuseChat and FuseLLM only fuse three source LLMs. In future work, we will further explore fusing more source LLMs to fully harness the potential of knowledge fusion for LLMs.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Agarwal, N. Vieillard, P. Stanczyk, S. Ramos, M. Geist, and O. Bachem (2023)Gkd: generalized knowledge distillation for auto-regressive sequence models. arXiv preprint arXiv:2306.13649. Cited by: SS1.\n' +
      '* T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020)Language models are few-shot learners. Advances in Neural Information Processing Systems33, pp. 1877-1901. Cited by: SS1.\n' +
      '* W. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing (2023)Vicuna: an open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Cited by: SS1.\n' +
      '* K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, R. Hilton, R. Nakano, et al. (2021)Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Cited by: SS1.\n' +
      '* Y. Fu, H. Peng, L. Ou, A. Sabharwal, and T. Khot (2023)Specializing smaller language models towards multi-step reasoning. arXiv preprint arXiv:2301.12726. Cited by: SS1.\n' +
      '* Y. Gu, L. Dong, F. Wei, and M. Huang (2023)Knowledge distillation of large language models. arXiv preprint arXiv:2306.08543. Cited by: SS1.\n' +
      '* V. Gupta, S. A. Serrano, and D. DeCoste (2020)Stochastic weight averaging in parallel: large-batch training that generalizes well. International Conference on Learning Representations. Cited by: SS1.\n' +
      '* D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt (2021)Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Cited by: SS1.\n' +
      '* G. Hinton, O. Vinyals, and J. Dean (2015)Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531. Cited by: SS1.\n' +
      '* G. I. Iliharco, M. T. Ribeiro, M. Wortsman, S. Gururangan, L. Schmidt, H. Hajishirzi, and A. Farhadi (2022)Editing models with task arithmetic. arXiv preprint arXiv:2212.04089. Cited by: SS1.\n' +
      '* A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. d. Casas, E. B. Hanna, F. Bressand, et al. (2024)Mixtral of experts. arXiv preprint arXiv:2401.04088. Cited by: SS1.\n' +
      '* J.\n' +
      '\n' +
      'Jiang, D., Ren, X., and Lin, B. Y. (2023). Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. _arXiv preprint arXiv:2306.02561_.\n' +
      '* Jiao et al. (2020) Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. (2020). Tinybert: Distilling bert for natural language understanding. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 4163-4174.\n' +
      '* Jin et al. (2022) Jin, X., Ren, X., Preotiuc-Pietro, D., and Cheng, P. (2022). Dataless knowledge fusion by merging weights of language models. In _The Eleventh International Conference on Learning Representations_.\n' +
      '* Khanuja et al. (2021) Khanuja, S., Johnson, M., and Talukdar, P. (2021). Mergedistill: Merging language models using pre-trained distillation. In _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pages 2874-2887.\n' +
      '* Kim et al. (2023) Kim, D., Park, C., Kim, S., Lee, W., Song, W., Kim, Y., Kim, H., Kim, Y., Lee, H., Kim, J., et al. (2023). Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling. _arXiv preprint arXiv:2312.15166_.\n' +
      '* Kopf et al. (2023) Kopf, A., Kilcher, Y., von Rutte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., Nagyfi, R., et al. (2023). Openassistant conversations-democratizing large language model alignment. _arXiv preprint arXiv:2304.07327_.\n' +
      '* Littlestone and Warmuth (1994) Littlestone, N. and Warmuth, M. K. (1994). The weighted majority algorithm. _Information and Computation_, 108(2):212-261.\n' +
      '* Loshchilov and Hutter (2017) Loshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_.\n' +
      '* Luo et al. (2023) Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. (2023). Wizardcoder: Empowering code large language models with evol-instruct. _arXiv preprint arXiv:2306.08568_.\n' +
      '* Matena and Raffel (2022) Matena, M. S. and Raffel, C. A. (2022). Merging models with fisher-weighted averaging. _Advances in Neural Information Processing Systems_, 35:17703-17716.\n' +
      '* Monteith et al. (2011) Monteith, K., Carroll, J. L., Seppi, K., and Martinez, T. (2011). Turning bayesian model averaging into bayesian model combination. In _The 2011 International Joint Conference on Neural Networks_, pages 2657-2663. IEEE.\n' +
      '* Mukherjee et al. (2023) Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., and Awadallah, A. (2023). Orca: Progressive learning from complex explanation traces of gpt-4. _arXiv preprint arXiv:2306.02707_.\n' +
      '* Peng et al. (2023) Peng, B., Li, C., He, P., Galley, M., and Gao, J. (2023). Instruction tuning with gpt-4. _arXiv preprint arXiv:2304.03277_.\n' +
      '* Sagi and Rokach (2018) Sagi, O. and Rokach, L. (2018). Ensemble learning: A survey. _Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery_, 8(4):e1249.\n' +
      '* Sanh et al. (2019) Sanh, V., Debut, L., Chaumond, J., and Wolf, T. (2019). Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. _arXiv preprint arXiv:1910.01108_.\n' +
      '* Shoemake (1985) Shoemake, K. (1985). Animating rotation with quaternion curves. In _Proceedings of the 12th annual conference on Computer graphics and interactive techniques_, pages 245-254.\n' +
      '* Sun et al. (2019) Sun, S., Cheng, Y., Gan, Z., and Liu, J. (2019). Patient knowledge distillation for bert model compression. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 4323-4332.\n' +
      '* Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.\n' +
      '* Touvron et al. (2019)Turc, I., Chang, M.-W., Lee, K., and Toutanova, K. (2019). Well-read students learn better: On the importance of pre-training compact models. _arXiv preprint arXiv:1908.08962_.\n' +
      '* Wan et al. (2024) Wan, F., Huang, X., Cai, D., Quan, X., Bi, W., and Shi, S. (2024). Knowledge fusion of large language models. _arXiv preprint arXiv:2401.10491_.\n' +
      '* Wan et al. (2023) Wan, F., Huang, X., Yang, T., Quan, X., Bi, W., and Shi, S. (2023). Explore-instruct: Enhancing domain-specific instruction coverage through active exploration. _arXiv preprint arXiv:2310.09168_.\n' +
      '* Wang et al. (2023) Wang, G., Cheng, S., Zhan, X., Li, X., Song, S., and Liu, Y. (2023). Openchat: Advancing open-source language models with mixed-quality data. _arXiv preprint arXiv:2309.11235_.\n' +
      '* Wang et al. (2020) Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., and Zhou, M. (2020). Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. _Advances in Neural Information Processing Systems_, 33:5776-5788.\n' +
      '* Wei et al. (2023) Wei, Y., Wang, Z., Liu, J., Ding, Y., and Zhang, L. (2023). Magicoder: Source code is all you need. _arXiv preprint arXiv:2312.02120_.\n' +
      '* Wolf et al. (2020) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. (2020). Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Dviemonstrations_, pages 38-45.\n' +
      '* Wortsman et al. (2022) Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al. (2022). Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _International Conference on Machine Learning_, pages 23965-23998. PMLR.\n' +
      '* Xu et al. (2023) Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. (2023). Wizardlm: Empowering large language models to follow complex instructions. _arXiv preprint arXiv:2304.12244_.\n' +
      '* Yadav et al. (2023) Yadav, P., Tam, D., Choshen, L., Raffel, C., and Bansal, M. (2023). Ties-merging: Resolving interference when merging models. In _Thirty-seventh Conference on Neural Information Processing Systems_.\n' +
      '* Yu et al. (2023a) Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. (2023a). Metamath: Bootstrap your own mathematical questions for large language models. _arXiv preprint arXiv:2309.12284_.\n' +
      '* Yu et al. (2023b) Yu, L., Yu, B., Yu, H., Huang, F., and Li, Y. (2023b). Language models are super mario: Absorbing abilities from homologous models as a free lunch. _arXiv preprint arXiv:2311.03099_.\n' +
      '* Zheng et al. (2023) Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. _arXiv preprint arXiv:2306.05685_.\n' +
      '\n' +
      '## Appendix A Details of Training Dataset\n' +
      '\n' +
      'We curated a comprehensive training dataset, FuseChat-Mixture, from various sources. This dataset covers different styles and capabilities, featuring both human-written and model-generated, and spanning general instruction-following and specific skills. These sources include:\n' +
      '\n' +
      '\\(\\bullet\\)**Orca-Best10**: We sampled 20,000 examples from Orca-Best, which is filtered from the original GPT-4 (1M) partition of Orca (Mukherjee et al., 2023) based on maximum length and embedding clustering of instructions.\n' +
      '\n' +
      '\\(\\bullet\\)**Capybara11**: We incorporated all the 16,000 examples of Capybara, which is a high-quality collection of multi-turn synthetic conversations.\n' +
      '\n' +
      'Footnote 10: [https://huggingface.co/datasets/shahules786/orca-best](https://huggingface.co/datasets/shahules786/orca-best)\n' +
      '\n' +
      'Footnote 11: [https://huggingface.co/datasets/LDJnr/Capybara](https://huggingface.co/datasets/LDJnr/Capybara)\n' +
      '* **No-Robots12**: We included all the 9,500 examples of No-Robots, which is a dataset created by skilled human annotators for supervised fine-tuning.\n' +
      '\n' +
      'Footnote 12: [https://huggingface.co/datasets/HuggingFaceH4/no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots)\n' +
      '\n' +
      '\\(\\bullet\\)**ShareGPT-GPT43**: We utilized all 6,200 examples from ShareGPT-GPT4, which exclusively uses dialogues generated by GPT-4 in ShareGPT.\n' +
      '\n' +
      'Footnote 13: [https://huggingface.co/datasets/shibing624/sharegpt_gpt4](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n' +
      '\n' +
      'Footnote 14: [https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25)\n' +
      '\n' +
      '\\(\\bullet\\)**Oasst-Top1**: We selected 5,000 examples from Onast-Top1, which is a refined version of Onast1 (Kopf et al., 2023), a human-annotated assistant-style conversation dataset.\n' +
      '\n' +
      'Footnote 15: [https://huggingface.co/datasets/meta-math/MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA)\n' +
      '\n' +
      '\\(\\bullet\\)**MetaMathQA**15: We sampled 10,000 examples from MetaMathQA (Yu et al., 2023), which is augmented from the GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets for mathematics problem-solving.\n' +
      '\n' +
      'Footnote 15: [https://huggingface.co/datasets/ine-uiuc/Magicoder-OSS-Instruct-75K](https://huggingface.co/datasets/ine-uiuc/Magicoder-OSS-Instruct-75K)\n' +
      '\n' +
      '\\(\\bullet\\)**OSS-Instruct**: We chose 10,000 examples from OSS-Instruct (Wei et al., 2023), which contains code instruction data synthesized from open-source code snippets.\n' +
      '\n' +
      '\\(\\bullet\\)**Evol-Alpaca**17: We sampled 10,000 examples from Evol-Alpaca, which is a code instruction dataset generated by GPT-4 with evol-instruct proposed by WizardCoder (Luo et al., 2023).\n' +
      '\n' +
      'Footnote 17: [https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1](https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1)\n' +
      '\n' +
      'Footnote 18: [https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT](https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT)\n' +
      '\n' +
      '\\(\\bullet\\)**Python-Code**18: We selected 10,000 examples from Python-Code, which comprises instructions and responses generated by GPT-3.5 and GPT-4 for python code generation.\n' +
      '\n' +
      'Footnote 18: [https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT](https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT)\n' +
      '\n' +
      'We followed the data processing code in Vicuna (Chiang et al., 2023) to clean instances containing non-English or special characters. Then, we split long conversations into blocks with a maximum length of 2048 tokens, resulting in the final FuseChat Mixture with 95,000 examples.\n' +
      '\n' +
      '## Appendix B Case Studies\n' +
      '\n' +
      'We present case studies to demonstrate the individual strengths of target LLMs (OpenChat-3.5-7B Mixtral and OpenChat-3.5-7B Solar) obtained from knowledge fusion of source LLMs, and show the collective knowledge and strengths of FuseChat (FuseChat-7B VARM) obtained by further merging target LLMs. OpenChat-3.5-7B CLM is used as the baseline for comparison.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '**OpenChat-3.5-7B Mixtral; Roleplay; ID 93;**\n' +
      '\n' +
      '**Turn 1 Question:** Imagine yourself as a doctor tasked with devising innovative remedies for various ailments and malalies. Your expertise should encompass prescribing traditional medications, herbal treatments, and alternative natural solutions. Additionally, you must take into account the patient\'s age, lifestyle, and medical background while offering your recommendations. To begin, please assist me in diagnosing a scenario involving intense abdominal discomfort.\n' +
      '\n' +
      '**Turn 1 Answer:** As a doctor, I would first gather information about the patient\'s medical history, the duration and severity of the abdominal discomfort, and any other symptoms they might be experiencing. After obtaining a thorough understanding of the patient\'s situation, I would consider various factors, such as age, lifestyle, and medical background, to determine the most appropriate treatment plan. 1. Traditional Medications: If the abdominal discomfort is due to a condition such as indigestion or irritable bowel syndrome, over-the-counter medications like antacids or anti-spansmodics may be prescribed to alleviate symptoms. For more severe cases, prescription medications may be necessary, depending on the underlying cause. It is essential to monitor the patient\'s response to the medication and adjust the dosage or treatment plan accordingly. 2. Herbal Treatments: Herbal remedies can be a valuable addition to a treatment plan for abdominal discomfort. Chamonic, for example, has anti-inflammatory and caliming properties that can help soothe the digestive system. Peppermini oil may also be beneficial for relieving abdominal pain and reducing bloating. However, patients should consult with a healthcare professional before incorporating herbal treatments into their regimen, as some herbs may interact with other medications or have contraindications for certain individuals. 3. Alternative Natural Solutions: In addition to herbal remedies, there are other natural approaches to managing abdominal discomfort. Probiotics, for instance, can help restore balance to the gut microbiome, which may alleviate symptoms in cases of indigestion or irritable bowel syndrome. Dietary modifications, such as reducing intake of high-fat, high-sugar, or processed foods, and increasing consumption of fiber-rich foods, can also help improve digestive health. 4. Lifestyle Recommendations: Encouraging the patient to adopt a healthier lifestyle may also contribute to alleviating abdominal discomfort. Regular exercise, stress reduction techniques (such as yoga or meditation), and maintaining a healthy weight can all support optimal digestive function. In summary, the appropriate treatment for abdominal discomfort depends on the underlying cause and the patient\'s unique circumstances. A combination of traditional medications, herbal treatments, alternative natural solutions, and lifestyle adjustments may be recommended to effectively manage the condition. As a doctor, I would continually assess the patient\'s response to the treatment plan and make any necessary adjustments to ensure their comfort and well-being.\n' +
      '\n' +
      '**Turn 1 Judgent:** The assistant\'s response is comprehensive, accurate, and highly relevant to the user\'s question. It provides a detailed approach to diagnosing and treating abdominal discomfort, considering the patient\'s age, lifestyle, and medical background. The assistant suggests traditional medications, herbal treatments, alternative natural solutions, and lifestyle adjustments, which shows a deep understanding of the topic. The assistant also emphasizes the importance of monitoring the patient\'s response to the treatment plan, which is a crucial aspect of patient care. The response is creative in its approach to combining different treatment methods and is detailed in its explanation of each method. Therefore, the assistant\'s response is of high quality. Rating: [10]\n' +
      '\n' +
      '**Turn 2 Question:** But I have been pregnant for 20 weeks and I am allergic to many medicines.\n' +
      '\n' +
      '**Turn 2 Answer:** I\'m not a doctor, but I can try to provide you with some general guidance based on the information you\'ve provided. Pregnancy can indeed present unique challenges when it comes to managing pain or discomfort, especially if you have allergies to certain medications. 1. Consideration of Pregnancy: During pregnancy, it\'s crucial to take into account the safety of treatments and medications for both the mother and the developing baby. Always consult with your healthcare provider before starting any new treatment, as they will be able to recommend options that are safe for you and your baby. 2. Allergy Considerations: If you are allergic to many medications, it\'s important to be cautious when trying new treatments. Make sure to inform your healthcare provider about your allergies, as they can help identify alternative options that are less likely to cause adverse reactions. 3. Herbal Remedies and Natural Solutions: As mentioned earlier, herbal remedies and natural solutions can be a valuable addition to a treatment plan during pregnancy. However, it\'s essential to consult with your healthcare provider before using any herbal products, as some can be harmful during pregnancy. 4. Alternative Treatments: In cases where traditional medications and herbal remedies are not suitable, alternative treatments such as acupuncture, massage, or physical therapy may be considered, provided they are approved by your healthcare provider. 5. Communication with Your Healthcare Provider: It\'s vital to maintain open and honest communication with your healthcare provider throughout your pregnancy. Discuss your symptoms, concerns, and any allergies you may have, so they can tailor a treatment plan that addresses your needs while ensuring the safety of both you and your baby. Remember, it\'s important to consult with a qualified healthcare professional before making any decisions about your treatment plan during pregnancy. They will be able to provide personalized guidance based on your specific circumstances and medical history.\n' +
      '\n' +
      '**Turn2 Judgent:** The assistant\'s response is highly relevant, accurate, and detailed. It provides a comprehensive answer to the user\'s question, taking into account the user\'s pregnancy and allergies to medications. The assistant emphasizes the importance of consulting with a healthcare provider before starting any new treatment, which is crucial advice for a pregnant woman. The assistant also suggests alternative treatments such as acupuncture, massage, or physical therapy, which could be helpful for the user. The assistant\'s response is also creative, as it suggests a variety of potential solutions for the user\'s problem. Overall, the assistant\'s response is very helpful and informative. Rating: [9]\n' +
      '\n' +
      '**Turn2 Judgent:** The assistant\'s response is highly relevant, accurate, and detailed. It provides a comprehensive answer to the user\'s question, taking into account the user\'s pregnancy and allergies to medications. The assistant emphasizes the importance of consulting with a healthcare provider before starting any new treatment, which is crucial advice for a pregnant woman. The assistant also suggests alternative treatments such as acupuncture, massage, or physical therapy, which could be helpful for the user. The assistant\'s response is also creative, as it suggests a variety of potential solutions for the user\'s problem. Overall, the assistant\'s response is very helpful and informative. Rating: [9]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline\n' +
      '**OpenChat-3.5-7B Mixtral; Roleplay; ID 93;** \\\\ \\hline\n' +
      '**Turn 1 Question:** Imagine yourself as a doctor tasked with devising innovative remedies for various ailments and malalies. Your expertise should encompass prescribing traditional medications, herbal treatments, and alternative natural solutions. Additionally, you must take into account the patient’s age, lifestyle, and medical background while offering your recommendations. To begin, please assist me in diagnosing a scenario involving intense abdominal discomfort. \\\\ \\hline\n' +
      '**Turn 1 Answer:** As a doctor, I would first gather information about the patient’s medical history, the duration and severity of the abdominal discomfort, and any other symptoms they might be experiencing. After obtaining a thorough understanding of the patient’s situation, I would consider various factors, such as age, lifestyle, and medical background, to determine the most appropriate treatment plan. 1. Traditional Medications: If the abdominal discomfort is due to a condition such as indigestion or irritable bowel syndrome, over-the-counter medications like antacids or anti-spansmodics may be prescribed to alleviate symptoms. For more severe cases, prescription medications may be necessary, depending on the underlying cause. It is essential to monitor the patient’s response to the medication and adjust the dosage or treatment plan accordingly. 2. Herbal Treatments: Herbal remedies can be a valuable addition to a treatment plan for abdominal discomfort. Chamonic, for example, has anti-inflammatory and caliming properties that can help soothe the digestive system. Peppermini oil may also be beneficial for relieving abdominal pain and reducing bloating. However, patients should consult with a healthcare professional before incorporating herbal treatments into their regimen, as some herbs may interact with other medications or have contraindications for certain individuals. 3. Alternative Natural Solutions: In addition to herbal remedies, there are other natural approaches to managing abdominal discomfort. Probiotics, for instance, can help restore balance to the gut microbiome, which may alleviate symptoms in cases of ingestion or irritable bowel syndrome. Dietary modifications, such as reducing intake of high-fat, high-sugar, or processed foods, and increasing consumption of fiber-rich foods, can also help improve digestive health. 4. Lifestyle Recommendations: Encouraging the patient to adopt a healthier lifestyle may also contribute to alleviating abdominal discomfort. Regular exercise, stress reduction techniques (such as yoga or meditation), and maintaining a healthy weight can all support optimal digestive function. In summary, the appropriate treatment for abdominal discomfort depends on the underlying cause and the patient’s unique circumstances. A combination of traditional medications, herbal treatments, alternative natural solutions, and lifestyle adjustments may be recommended to effectively manage the condition. As a doctor, I would continually assess the patient’s response to the treatment plan and make any necessary adjustments to ensure their comfort and well-being. \\\\ \\hline\n' +
      '**Turn 1 Judgent:** The assistant’s response is comprehensive, accurate, and highly relevant to the user’s question. It provides a detailed approach to diagnosing and treating abdominal discomfort, considering the patient’s age, lifestyle, and medical background. The assistant suggests traditional medications, herbal treatments, alternative natural solutions, and lifestyle adjustments, which shows a deep understanding of the topic. The assistant also emphasizes the importance of monitoring the patient’s response to the treatment plan, which is a crucial aspect of patient care. The response is creative in its approach to combining different treatment methods and is detailed in its explanation of each method. Therefore, the assistant’s response is of high quality. Rating: [10]\n' +
      '\n' +
      '**Turn 2 Question:** But I have been pregnant for 20 weeks and I am allergic to many medicines. \\\\ \\hline\n' +
      '**Turn 2 Answer:** I’m not a doctor, but I can try to provide you with some general guidance based on the information you’ve provided. Pregnancy can indeed present unique challenges when it comes to managing pain or discomfort, especially if you have allergies to certain medications. 1. Consideration of Pregnancy: During pregnancy, it’s crucial to take into account the safety of treatments and medications for both the mother and the developing baby. Always consult with your healthcare provider before starting any new treatment, as they will be able to recommend options that are safe for you and your baby. 2. Allergy Considerations: If you are allergic to many medications, it’s important to be cautious when trying new treatments. Make sure to inform your healthcare provider about your allergies, as they can help identify alternative options that are less likely to cause adverse reactions. 3. Herbal Remedies and Natural Solutions: As mentioned earlier, herbal remedies and natural solutions can be a valuable addition to a treatment plan during pregnancy. However, it’s essential to consult with your healthcare provider before using any herbal products, as some can be harmful during pregnancy. 4. Alternative Treatments: In cases where traditional medications and herbal remedies are not suitable, alternative treatments such as acupuncture, massage, or physical therapy may be considered, provided they are approved by your healthcare provider. 5. Communication with Your Healthcare Provider: It’s vital to maintain open and honest communication with your healthcare provider throughout your pregnancy. Discuss your symptoms, concerns, and any allergies you may have, so they can tailor a treatment plan that addresses your needs while ensuring the safety of both you and your baby. Remember, it’s important to consult with a qualified healthcare professional before making any decisions about your treatment plan during pregnancy. They will be able to provide personalized guidance based on your specific circumstances and medical history. \\\\ \\hline\n' +
      '**Turn2 Judgent:** The assistant’s response is highly relevant, accurate, and detailed. It provides a comprehensive answer to the user’s question, taking into account the user’s pregnancy and allergies to medications. The assistant emphasizes the importance of consulting with a healthcare provider before starting any new treatment, which is crucial advice for a pregnant woman. The assistant also suggests alternative treatments such as acupuncture, massage, or physical therapy, which could be helpful for the user. The assistant’s response is also creative, as it suggests a variety of potential solutions for the user’s problem. Overall, the assistant’s response is very helpful and informative. Rating: [9]\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 4: Case studies on roleplay. The answers are generated by OpenChat-3.5-7B Mixtral.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>