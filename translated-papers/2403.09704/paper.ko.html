<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 정렬 스튜디오: 대규모 언어 모델을 특정 문맥 규정에 정렬\n' +
      '\n' +
      'Swapnaja Achintalwar, Ioana Baldini, Djallel Bouneffouf, Joan Byamugisha, Maria Chang, Pierre Dognin, Eitan Farchi, Ndivhuwo Makondo, Aleksandra Mojsilovic, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Inkit Padhi, Orna Raz, Jesus Rios, Prasanna Sattigeri, Moninder Singh, Siphiwe Thwala, Rosario A Uceda-Sosa, Kush R. 바쉬니\n' +
      '\n' +
      '저자는 IBM 리서치와 함께 있습니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대형 언어 모델의 정렬은 일반적으로 사용 사례 및 컨텍스트에 걸쳐 공통적이거나 보편적으로 이해되는 행동을 추가하거나 제어하기 위해 모델 제공자에 의해 수행된다. 대조적으로, 이 기사에서는 애플리케이션 개발자가 특정 가치, 사회 규범, 법률 및 기타 규정에 따라 모델을 조정하고 잠재적으로 상충되는 요구 사항 사이를 맥락에서 조정할 수 있도록 하는 접근 방식과 아키텍처를 제시한다. 이러한 정렬 스튜디오 아키텍처의 세 가지 주요 구성 요소, 즉 언어 모델의 동작을 제어하기 위해 함께 작동하는 프레임, 강사 및 감사인을 배치한다. 우리는 기업의 내부 대면 기업 챗봇을 비즈니스 수행 지침에 정렬하는 실행 사례를 통해 이 접근 방식을 설명한다.\n' +
      '\n' +
      ' AI 정렬, 지식 표현, 미세 조정, 레드 티밍\n' +
      '\n' +
      '## Introduction\n' +
      '\n' +
      '사전 훈련된 대형 언어 모델(LLM)은 일반적으로 모델 제공자에 의해 조정되어 지시를 따르고 사용자와 유용한 대화를 수행하는 능력과 같은 상이한 능력을 그들에게 부여한다. 많은 모델 제공자는 무해성의 정의에 따라 LLM을 무해하게 만들기 위해 정렬이라고 하는 추가 조정을 수행한다. 이러한 LLM의 \'문명화\'와 \'인간화\' 단계는 기반 모델의 사전 훈련보다 모델의 행동을 통제하는데 결정적인 역할을 한다. 모델 제공자가 예방하려는 해악은 기존 벤치마크 및 평가 데이터 세트를 가진 증오, 악의, 배제, 비속어 및 독성과 같은 위험 분류학에서 발견되는 일반적인 것이다.\n' +
      '\n' +
      '그럼에도 불구하고, 우리는 일반적인 우려에 대한 그러한 정렬이 포괄적일 수 있다고 믿지 않으며 정렬의 모든 치수가 항상 반드시 바람직하다고 생각하지 않는다. 상황은 중요하지 모든 산업, 부문, 관할권, 문화 및 유스 케이스는 _common_ 분류법에서 _not_ 캡처되는 고유한 _particular_ 원하는 행동을 가지고 있다. 그 예들은 수없이 많다. 의료 응용 프로그램에서 개발자는 LLM이 신체 부위의 이름을 욕설로 취급하기를 원하지 않을 수 있다. 입력들이 불쾌한 언어로 묶인 고객 불만 처리 애플리케이션에서, 개발자들은 시스템이 계속 응답하기를 원할 수 있다. 식료품점의 챗봇은 독성 식품 품목에 대한 언급을 자제하라는 추가 요구 사항이 있을 수 있고 은행의 챗봇은 경쟁사의 브랜드나 제품에 대한 언급을 자제하라는 추가 요구 사항이 있을 수 있다. 법은 사회주의 핵심 가치를 반영하기 위해 모든 생성 내용을 요구하는 중국에서와 같이 특정 LLM 행동을 요구할 수 있다. 조직은 반드시 지켜야 할 LLM의 어조와 성격에 대한 스타일 가이드를 가질 수 있다. 기업은 존중받을 영업 행위를 명시하는 지침이 있을 수 있다. 이러한 모든 예는 맥락에 따라 유효한 원하는 동작이지만 일반적인 문제에 대해 모델 제공자가 수행한 정렬에는 나타나지 않는다.1\n' +
      '\n' +
      '각주 1: 우리는 저자로서 LLM을 나열된 모든 예에 정렬하는 데 동의하지 않을 수 있다. 그러나 그것이 요점입니다: 우리의 개인적인 가치는 보편적이지 않으며 우리는 그것을 최종 커뮤니티에 부과해서는 안 됩니다. 우리는 가능성에 대해 넓은 구멍을 제공하는 정신으로 예를 들었다.\n' +
      '\n' +
      '규제의 원천은 법률뿐만 아니라 사회통념, 시장의 요구, 기술적 제약 등 여러 가지가 있다[1]. 관련 요건들은 사용 사례에 기초하여 상당히 독특하고 맥락적일 수 있다. 따라서 LLM을 평가할 기존 벤치마크가 없습니다. 또한 다른 규정이 경쟁적이거나 상충할 수 있습니다. [2]와 같은 높은 수준의 일반 진술과 달리 "독성, 인종 차별, 성차별 또는 다른 형태의 신체적 또는 사회적 피해를 나타내는 반응을 선택하지 말라"는 특정 규정은 상당히 상세할 수 있다. 우리가 일하는 IBM에서는 상세한 비즈니스 수행 지침이 있으며 이를 LLM 동작을 정렬할 수 있는 특정 규정 세트의 실행 사례로 사용할 것입니다.\n' +
      '\n' +
      '도. 1: 프레임, 강사 및 감사관의 세 가지 구성 요소가 있는 정렬 스튜디오의 스타일화된 묘사입니다.\n' +
      '\n' +
      '다양한 특정 상황 규정을 준수하는 것은 고객에게 더 나은 서비스를 제공하는 것부터 기소를 피하는 것까지 많은 비즈니스 이점을 가지고 있다. 아마도 가장 큰 이점은 LLM을 모델 배포자와 최종 사용자 커뮤니티의 가치에 정통하게 만드는 것이다. 개인화 혹은 커스터마이징[3], 일종의 조종 가능한 다원주의[4], 모델 제공자와 권한의 권력과 특권을 해체하고 공동체에게 \'시민\'과 \'인간\'이 무엇인지에 대한 발언권을 갖게 하는 탈식민주의 방식이다. 중요하게도, 애플리케이션 개발자는 모델이 _open_인 경우에만 모델 제공자에 의해 수행된 공통 정렬을 넘어 LLM을 추가로 정렬할 수 있다. 또한, 정렬 기술은 애플리케이션 개발자의 수단을 넘어 확장되기 때문에 너무 비용이 많이 들거나 부담이 되지 않아야 한다.\n' +
      '\n' +
      'LLM의 동작을 범용적이지 않은 값과 요구 사항으로 사용자 정의하려면 _Alignment Studio_라는 도구를 사용해야 합니다. 그 출발점은 자연어 정책 문서에 주어진 일련의 규정으로, 이미 심의 및 채택된 법률, 산업 표준 또는 기업 지침이 될 수 있으며, 툴링은 원칙적이고 투명하며 감사할 수 있으며 신중한 정렬 접근 방식을 허용한다.\n' +
      '\n' +
      '각주 2: 향후 작업에서는 우화나 민속과 같은 다른 형태의 가치 사양으로 확장할 것이다.\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 1, Alignment Studio의 첫 번째 구성 요소는 \'Framers\'로 명명되며, LLM에 원하는 행동을 주입하고 성공 여부를 평가하는 다운스트림 작업을 가능하게 하는 형태로 명령어 데이터 및 시나리오 데이터를 생성하기 위해 여러 지식 엔지니어링 및 생성 AI 기술을 적용한다. 두 번째 구성요소인 \'강사\'는 프레임어의 출력을 사용하여 모델을 적절하게 미세 조정합니다. 중요한 것은, 강사 컴포넌트는 또한 경쟁 가치 또는 규정의 조정을 허용한다. 세 번째 구성요소인 \'감사인\'은 인간 및 자동화된 벤치마킹과 레드 티밍의 조합을 사용하여 미세 조정 모델이 원하는 행동을 학습했는지 여부를 평가한다. 입안자, 강사 및 감사인은 지속적인 발전의 순환을 형성한다. 입안자는 반드시 개발 라이프사이클의 첫 번째 단계일 필요는 없으며, 예를 들어 테스트 기반 개발 접근 방식은 감사인으로부터 시작할 수 있다. 감사인은 또한 정렬의 한계(예: 원칙에 대한 정렬 오류)를 정량화할 수 있으며, 이는 데이터 증강 또는 모델 편집과 같은 교정 접근법을 제안할 수 있다. 정책 문서를 시작으로 Alignment Studio를 위한 대표적인 소프트웨어 아키텍처가 그림 2에 나와 있다.\n' +
      '\n' +
      '## 정렬 스튜디오와 거버넌스\n' +
      '\n' +
      '대체로 LLM을 특정 규정 세트에 정렬하는 목적은 그 행동을 통제하거나 통제하는 것이다. 이러한 의미에서 우리는 건물의 냉난방 시스템과 같은 피드백 제어 시스템으로 Alignment Studio를 상상할 수 있다. 사용자는 원하는 온도 스케줄을 설정하고, 온도 조절기 컨트롤러는 노와 에어컨을 켜고 끄고 원하는 온도에 맞추려고 하며, 센서는 원하는 온도와 비교되는 현재 온도를 측정한다. 차이는 제어기로 피드백됩니다. 원하는 행동과 측정된 행동은 비슷해야 한다. 상황은 전반적으로 중요하다. 저녁 행사나 휴일이 있는 경우 다른 일정을 원할 수 있습니다. 내부 최소 온도(수도관이 터지지 않도록) 또는 공기 조화기가 활성화될 수 있는 외부 최소 온도(압축기가 파손되지 않도록)를 유지하는 것과 같은 기술적 요건이 있을 수 있다. 환경적인 이유로 겨울에 건물을 상당히 시원하게 유지하고, 그런 식으로 좋아하는 방문 손님을 만족시키기 위해 상당히 따뜻하게 유지하는 등 상충되는 가치가 있을 수 있다.\n' +
      '\n' +
      '정렬 스튜디오는 AI 거버넌스를 위해 이러한 모든 부분을 수행하는 기술을 제공하지만 스칼라 값 온도보다 더 복잡한 값, 행동 및 규정을 제공한다. 규제는 종종 그 의도에서 상당히 구체적이지만(예를 들어, 직원은 경쟁자를 위해 일하지 않을 수 있음), 종종 문서에 주어진 것(예를 들어, 위의 경우 경쟁자를 구성하는 사람)을 넘어서는 지식을 요구한다. 정확성, 충실성 등을 크게 강조한 모형의 반응을 제약하기 위한 규정도 필수적이다. (규정에 위배되는) 오답을 하는 것은 용납할 수 없다. 더 중요한 것은, 정답을 제공하기 위해 추가 정보가 필요한 경우, 모델은 항상 질문에 대한 응답을 생성하기보다는 그렇게 말해야 한다. 이것은 환각과 같은 알려진 LLM 문제를 예방한다.\n' +
      '\n' +
      '정렬 스튜디오가 제공하는 AI 거버넌스의 또 다른 중요한 차원은 상충될 수 있는 행동 중에서 투명하고 통제 가능하게 선택하는 것이다. Lazar는 이러한 거버넌스의 필요성을 요약한다 [6]: "LLM 행동을 조정하는 것은 실제로 최종 사용자를 통제하는 문제이며 오용을 방지하기 위한 알고리즘 보호를 개발하는 것이다. 이 알고리즘 거버넌스가 명시적이거나 직접적인 통제가 없는 LLM에 의해 만들어진 불가해한 절충에 의존한다면, 통치 권력은 명백한 불법적이고 정당하지 않다."\n' +
      '\n' +
      '### Running Example\n' +
      '\n' +
      '이 글에서 우리는 정렬 스튜디오를 설명하기 위해 다음과 같은 실행 예를 사용할 것이다. LLM은 IBM 직원이 일반적인 질문을 하고 조언을 받고 제안을 받을 수 있는 애플리케이션에 주입됩니다. LLM은 IBM 비즈니스 행동 지침(BCGs)에 문서화된 IBM의 기업 정책과 일치합니다.3은 78개의 단락이 있는 8개의 섹션으로 분할된 약 11,500개의 단어가 있는 46페이지 문서입니다. 콘텐츠는 실행 가능한 306개의 개별 정책을 통합하는 _topic-paragraph_, _question-answer_ 및 call-out _block_와 같은 상이한 형태로 표현된다. 우리는 원시 텍스트에서 모델을 직접 미세 조정할 수 있고 이것이 모델의 어휘와 텍스트에서 일반적인 패턴을 가르칠 수 있지만 모델이 BCG 정책에 대해 스스로 배울 수 있는 신호는 충분하지 않다. 설명서의 명확성을 위해 기본 지시 추적 모델에 대한 정렬만 보여주고 강력한 시스템의 일부인 다른 종과 휘파람은 제외한다. 중요한 것은 우리가 상상하고 있는 내부 챗봇 애플리케이션은 BCG에 대한 사실이나 지식을 검색할 수 있는 인터페이스일 뿐만 아니라 BCG 정책을 다양한 주제에 대한 응답의 제약으로 사용하는 범용 질의 응답 서비스라는 것이다.\n' +
      '\n' +
      '## Framers\n' +
      '\n' +
      'Framers 모듈은 사용자가 애플리케이션(또는 도메인)에 필수적인 것으로 간주하는 지식을 식별하여 LLM 모델의 사용자 정의 및 결과의 검증을 위해 성문화될 수 있다. 한마디로, Framers _frames_ 문제 공간은 시스템의 나머지 부분에 의해 라인 아래로 레버리지될 수 있다.\n' +
      '\n' +
      '실행 중인 예에서 이것은 IBM BCG의 구조와 내용을 활용하여 모델 정렬에 적합한 미세 조정 데이터를 생성한다는 것을 의미합니다. 위에서 언급한 바와 같이 정책 문서로 언어 모델을 직접 미세 조정하면 정책 관련 어휘가 부여되지만 문맥 관련 정책 정보로 응답하거나 정책 준수를 평가할 수 있는 능력은 부여되지 않는다.\n' +
      '\n' +
      '따라서 우리는 주어진 상황에 대한 관련 정책을 식별하거나 시나리오의 준수 여부를 포함하여 사용자가 필요로 하는 작업의 유형에 모델을 정렬하기 위해 다양한 작업에 대한 정책 관련 명령의 예로 구성된 _instruction_ 스타일 데이터[7, 8]와 _scenario_ 스타일 데이터(아래에서 논의됨)를 생성한다. 수동으로 충분한 학습 데이터를 생성하는 것은 비용이 많이 들기 때문에, 우리는 두 스타일 모두에서 일부 _seed_ 데이터를 생성하고 LLM을 사용하여 이 데이터 세트를 증강하기 위한 합성 데이터를 생성하는 하이브리드 접근법을 채택한다. 이 두 데이터 세트 모두 BCG 문서에서 단락을 추출하고 자급자족하는 원자 정책을 필요로 한다. 또한 정책과 관련된 질문과 답변을 포함하는 정책 교육 자료와 같은 다른 데이터 소스를 사용할 수 있습니다. 이는 정렬 결과를 검증하기 위한 고품질 데이터 소스가 될 것입니다.\n' +
      '\n' +
      '첫 번째 데이터 스타일에 대해, 우리는 (a) 문서 내의 토픽 및 단락에 대응하는 _topic-paragraph_, (b) 문서에 제공된 질문 및 답변에 대응하는 _question-answer_, 및 (c) 정책 시나리오를 강조하는 콜-아웃 블록에 대응하는 _block_의 세 가지 유형의 시드 데이터를 추출한다. 이들은 _summarization_와 _question-answering_의 두 가지 다른 작업 명령에만 해당한다. 우리가 가지고 있는 소량의 종자 데이터와 이 두 가지 지시 유형만으로는 모델이 일반화될 수 없다. 따라서 우리는 다른 LLM에 이 시드 데이터를 기반으로 _synthetic_ 데이터를 생성하도록 촉구한다. 우리는 LLM이 단지 두 가지 시드 작업에서 시작하여 다양한 작업 지침을 생성하는 데 능숙하다는 것을 발견했다. 이는 몇 가지 문맥 내 예제를 사용하여 _LLama2-70B_, _Falcon-180B_ 및 _Mixtrtal-8\\(\\times\\)7B_와 같은 강력한 LLM에서도 마찬가지이다. 우리가 만든 100,000개의 합성 예제에서 모델을 훈련하기 위해 약 76,000개의 예제를 남겨두고 잘못된 형식의 예제를 필터링하고 검증/테스트 데이터로 작은 부분을 보류한다. 명령 스타일 데이터를 생성하는 묘사는 그림 3에 나와 있다.\n' +
      '\n' +
      '시나리오 스타일 데이터의 경우 IBM BCG의 소수의 정책에 대해 준수, 위반 또는 모호한(결정을 내리기 위해 추가 정보가 필요한) 실제 상황을 수동으로 생성하는 것으로 시작합니다. 예를 들어, 이 정책의 경우, IBM의 기밀 및 독점 정보를 유지하는 것은 귀하의 책임입니다. 준수 시나리오(픽션)는 IBM이 관련된 거래에 대해 좋은 친구로부터 요청받았지만, 그녀는 그것이 기밀이라는 것을 알았습니다. 그래서 그녀는 정중히 대답하기를 거절했다. 비호환 시나리오는 유사하게 생성될 수 있고, 이러한 대조 시나리오는 모델을 세밀한 방식으로 정책에 정렬하는 데 사용될 수 있다. 시나리오에 대한 수동 데이터 생성은 비용이 많이 들 수 있으므로 LLM을 적절하게 프롬프트하여 문서의 모든 정책에 대해 많은 양의 합성 시나리오를 생성하기 위해 이 _seed_ 데이터를 활용합니다. 이 시나리오 데이터세트는 _classification_ 태스크를 위한 추가 데이터로 사용될 수 있지만 정책 위반 탐지기를 생성하는 데에도 사용될 수 있다.\n' +
      '\n' +
      'LLM에 합성 데이터를 생성하도록 요청하면 일반적으로 좋은 품질과 합리적인 다양성에 대한 지식이 생성되지만 데이터 세트의 커버리지, 따라서 대상 LLM의 전반적인 신뢰성에 대한 확약이 없다. 튜닝 및 검증 모두에 대한 도메인의 커버리지를 보장하는 방법은 상보적 상징 기술, 즉 지식 그래프를 사용하는 것이다.\n' +
      '\n' +
      '도. 2: 정책 문서로 시작하는 정렬 스튜디오 소프트웨어 아키텍처의 실현. 엔드 투 엔드 소프트웨어 테스트 및 설명서가 권장되지만 구현에는 모든 구성 요소가 포함될 필요는 없습니다.\n' +
      '\n' +
      '특히, 관계에 대한 추론을 가진 존재론. 온톨로지는 알고리즘적이고 추적 가능한 방식으로 구조화된 사실 정보를 제공하는 반면 LLM은 고급 자연 언어 이해와 생성을 제공한다. 다행히도 위키다타[9]와 ConceptNet과 같은 온톨로지로 성문화된 열린 지식의 큰 몸체가 있다. 이 지식을 사용하여 데이터를 체계적으로 생성하여 철저한 도메인 어휘로 시나리오를 큐합니다.\n' +
      '\n' +
      'BCG는 조직(공급자, 경쟁자 및 정부 주체와 같은), 부서(예를 들어, 법률, 인력 및 회계), 자산(제품, 시설, 시스템 및 지적 재산과 같은), 사람(예를 들어, IBM 직원, 정부 공무원 및 가족 구성체)과 같은 IBM 내부 및 외부의 주요 구조적 위계를 포함한다. 또한 BCG는 다른 엔티티가 무엇인지, 그리고 그들이 어떻게 조직되고 관련되는지, 예를 들어 정부 엔티티를 구성하는 것, 직원이 관리자와 어떻게 관련되는지, 그리고 데이터 프로세서가 처리하는 정보의 종류를 명확히 한다.\n' +
      '\n' +
      '이 정보는 BCG 사용 사례에 대한 도메인별 온톨로지를 구축하는 데 적용될 수 있으며, 이 온톨로지는 이해관계자(사람, 기업, 정부 기관 및 부서와 같은 조직), 지정학적 실체, 직업 및 정보 기술 제품 및 서비스와 같은 위키다타에서 자동으로 추출된 실체 및 관계를 보완할 수 있다. 우리는 관계(개체보다 크기가 작은)의 의미에 기초하여 상속 계층 및 보조 개체(예: 위치)를 추출한다. 이러한 존재론적 구조는 직원과 조직(직원이 일하는 곳), 조직과 경쟁자 사이의 관계, \'일\'이 직원과 경쟁자 사이의 허용 관계인지 여부, 그렇다면 어떻게 하는지 명시적으로 명시함으로써 모호한 진술(직원이 경쟁자를 위해 일하지 않을 수 있음)을 명확히 하는데 사용될 수 있다. 또한, 온톨로지의 모든 용어와 관계가 생성된 데이터셋에서 적어도 한 번 이상 나타날 것으로 예상하므로, 온톨로지 구조는 생성된 데이터셋의 적절한 적용 범위를 확인하는 데 활용될 수 있다.\n' +
      '\n' +
      '## IInstructors\n' +
      '\n' +
      '교수자를 통해, 정렬을 위해 원하는 값 및 행동을 주입시키는 것은 명령 데이터와 인간 안내를 사용하여 수행된다. 이는 일반적으로 원하는 행동의 고품질 시연에서 감독 미세 조정(SFT)을 사용하고 및/또는 LLM 행동보다 선호도를 평가하는 보상을 최적화하기 위해 강화 학습 미세 조정(RLFT)을 사용하여 달성된다. BCG의 사용 사례의 경우, 이러한 알고리즘은 LLM이 Framers 컴포넌트에 의해 생성된 명령 및 시나리오 데이터를 통해 문서에 표현된 다양한 암시적 값/행위를 따르도록 돕는다.\n' +
      '\n' +
      '규제 문서는 일반적으로 LLM이 정렬해야 하는 여러 개의 충돌하는 값 또는 원하는 행동을 반영하므로 이러한 값 및 행동의 집계를 위한 기술이 필요하다. 강사는 선호도 데이터와 이진 레이블 모두에서 보상 모델을 학습할 수 있습니다. 이러한 보상은 LLM 출력이 사용 사례에서 고려되는 각 개별 값 및 원하는 행동과 얼마나 잘 일치하는지 평가한다. 강사는 LLM이 따르기를 원하는 가치 또는 원칙의 도출과 가능한 충돌을 해결하기 위한 상대적 중요성을 허용한다. 그런 다음 RLFT를 사용하여 상대적 중요도에 따라 LLM을 이러한 값과 정렬한다. 강사는 또한 LLM이 사용되는 컨텍스트로부터 각 값의 상대적 중요성을 추론할 수 있게 한다[10]. 마지막으로, 낮은 자원 체제에서 미세 조정은 매개변수 효율적인 최적화 전략을 필요로 한다. 이러한 전략은 (Q)LoRA[11, 12]를 통해 (양자화된) 하위 적응을 사용하여 통합된다.\n' +
      '\n' +
      '도. 3: 정책 문서로부터 명령어 스타일 시드 데이터를 생성하고(왼쪽), 이를 사용하여 수-샷 설정(오른쪽)에서 LLM을 사용하여 합성 데이터를 생성한다.\n' +
      '\n' +
      '## Auditors\n' +
      '\n' +
      '감사인 구성 요소는 프레이머의 데이터와 강사의 메서드가 특정 상황 규정을 포함하여 원하는 모든 기준과 관련하여 잘 수행되는 모델을 생성했는지 확인하는 작업을 수행합니다. 일반적으로 모델 평가는 세 가지 축을 따라 분류될 수 있다:\n' +
      '\n' +
      '1. **** 시: 모델 또는 애플리케이션의 개발 라이프사이클의 어느 순간에 평가가 수행되는지(예를 들어, 모델이 일반적인 바람직한 능력 및/또는 특정 규정과 관련하여 능력을 보장하도록 훈련하는 동안; 훈련 후에, 일단 모델 체크포인트가 충분히 수행 가능한 것으로 간주되면, 모델이 훈련 중에 체크하기에는 너무 비용이 많이 드는 기준을 충족하는지 여부를 확립하기 위해; 전개 후에, 예상치 못한 또는 설명되지 않은 행동에 직면하지 않도록 하기 위해);\n' +
      '2. **What**: 평가 동안 어떤 유형의 데이터가 사용되는지(예를 들어, 범용 능력을 테스트하기 위한 확립된 벤치마크, 일반적인 인간 선호를 테스트하기 위한 범용 정렬 데이터, 특정 원하는 기준에 대한 준수를 보장하기 위해 수작업으로 조작된 도메인-특정 데이터)\n' +
      '3. **How**: 감사 또는 평가 방법론이 어떻게 수행되고 누구에 의해 수행되는지(예를 들어, 잘 정의된 벤치마크에 기초한 자동화된 평가, 모델의 인간-인-루프 레드-티밍, 또는 둘 모두의 조합).\n' +
      '\n' +
      '특정 상황에 대한 규정에 대한 모델에 대한 체계적인 평가는 특정 규정을 포괄하는 일반적인 벤치마크가 존재할 가능성이 낮기 때문에 특별히 조작된 데이터를 필요로 한다. 따라서 도메인별 평가는 두 단계로 수행된다. 먼저, 테스트 케이스의 작고 선별된 데이터 세트에 대한 정렬에 대해 모델을 평가한다. 그런 다음, 정렬된 모델에서 잠재적인 결함을 발견하기 위해 레드-티밍 [13]이 활용된다. 이 레드-티밍 단계는 정렬된 모델의 후속 반복을 위해 라이프사이클에 걸쳐 사용될 수 있는 데이터 세트를 동적으로 확장하는 것을 돕는다.\n' +
      '\n' +
      '### Red-Teaming\n' +
      '\n' +
      '우리는 특정 상황 규정 준수를 위한 레드 티밍이 두 모델에서 생성된 출력을 나란히 비교할 때 특히 효과적이라는 것을 발견했다. 이 두 가지 모델을 감안할 때 레드팀 구성원은 관심 있는 규정 또는 정책을 준수하는지 테스트하는 프롬프트를 작성하도록 요청받는다. 빨간 팀 구성원은 충실성 및 완전성과 같은 다양한 차원에 따라 응답을 등급화하여 이진 등급이 충분한 신호를 제공하지 않을 때마다 출력 품질에 대한 자세한 설명을 제공한다. 이 레드-티밍 동안 수집된 데이터는 다음과 같이 정렬된 모델을 개선하는 데 대한 추가 통찰력을 개발하는 데 사용될 수 있다:\n' +
      '\n' +
      '1. 정렬 및 비정렬 모델 둘 다 정렬된 응답을 생성하는데, 대응하는 테스트 케이스는 "간단한" 예로 간주되며, 여기서 입력은 적절하고 정확한 답변을 제공하기 위해 특정 규정에 대한 정렬이 요구될 정도로 충분히 어렵지 않다.\n' +
      '2. 정렬되지 않은 모델은 정렬된 모델보다 더 나은 응답을 제공한다: 이러한 상황은 충돌하는 값 및/또는 열악한 품질 명령 데이터가 정렬된 모델의 유용성에 부정적인 영향을 미쳤을 수 있음을 나타낸다.\n' +
      '3. 정렬되지 않은 모델은 테스트되는 특정 규정을 따르지 않는 반면, 정렬된 모델은 그러합니다: 이것들이 훌륭한 예입니다! 쿼리에 정확하게 응답하기 위해 특정 정책에 대한 특정 정렬의 필요성을 보여줍니다.\n' +
      '4. 정렬 및 비정렬 모델 둘 다 테스트되는 특정 규정을 따르지 않는다: 이는 "가장 어려운" 예이며, 이는 정렬 모델이 특정 규정을 학습하기 위해 더 많은 또는 더 나은 데이터가 필요함을 나타낸다.\n' +
      '\n' +
      '감사인의 주요 우선순위는 초기 정렬을 검증하고 기준선을 설정하는 것입니다. 그런 다음 기준선을 사용하여 LLM 모델의 원하는 동작을 지속적으로 제어하고 보장한다. 실제로, 감사인의 주요 측면은 평가가 _never_ 완료되었다는 것이다: 그것은 전개된 모델 또는 애플리케이션이 사용자 대면하는 한 지속되는 동적 활동이다.\n' +
      '\n' +
      '## Summary Demonstration\n' +
      '\n' +
      '우리는 IBM 화강암 모델[14]을 IBM BCG에 정렬하여 정렬 스튜디오를 시연합니다. Framers 섹션에 설명된 방법론을 사용하여 시드 _instruction_ 스타일 데이터를 만들고 SFT를 사용하여 모델을 정렬한다. 또한, RAG[15]를 사용하여 정렬되지 않은 모델과 정렬된 모델의 공정한 비교를 가능하게 하고 충실도를 향상시킨다. 사용자 인터페이스(User Interface, UI)를 개발하여 (a) 정렬된 모델이 응답을 얻도록 유도하고, (b) 정렬된 모델과 정렬되지 않은 모델의 응답을 비교하고, (c) 레드-티밍을 수행할 수 있는 능력과 함께 정렬되지 않은 모델과 정렬된 모델의 응답 간의 사용자 선호도를 포착하였다. UI는 그림 1에 나와 있다. 4는 위의 각 목표에 해당하는 세 개의 탭을 포함합니다. UI를 사용하여 정렬되지 않은 모델과 정렬된 모델의 응답을 소프트웨어 구매에 대한 사용자 프롬프트에 비교하는 예를 보여준다. 정렬된 모델은 정책에 더 충실한 응답을 생성한다.\n' +
      '\n' +
      '또 다른 예로, 다음과 같은 프롬프트를 고려하십시오: IBM에서의 작업과 오픈 소스 개발자 자원 봉사 사이에 시간을 할당하는 데 도움이 되는 계획을 만들고 싶습니다. 조심해야 하는 것은?_ 정렬되지 않은 모델은 응답을 생성합니다: - IBM에서의 작업 성능에 영향을 미칠 수 있으므로 오픈 소스 프로젝트에 너무 많은 시간을 투자하는 것을 주의해야 합니다._ 정렬된 모델이 응답하는 동안 IBM의 오픈 소스 참여 지침을 참조하고 해당 지침을 따르십시오. 정렬되지 않은 모델 반응은 합리적이지만 도움이 되지도 않고 충실하지도 않다. 정렬된 모델은 협의가 필요한 지침을 명확하고 간결하게 명시한다.\n' +
      '\n' +
      '마지막으로 프롬프트를 고려하십시오: 내 동생은 생성 AI 스타트업 회사를 가지고 있습니다. IBM의 새로운 벤처펀드에 대한 내부자 정보는 어떻게 보내드리면 될까요? 비정렬 모델의 반응은: _합리적인 투자자가 회사의 증권을 사고 팔거나 보유하는 데 영향을 미칠 수 있는 일반 대중이 이용할 수 없는 정보를 내부 정보로 간주한다. 증권의 가격에 작은 영향만 미칠지라도 정보는 물질적일 수 있다._ 정렬된 모델이 응답: _없음. IBM의 재무 성과, 인수, 매각 또는 기타 중요한 활동에 대한 내부 정보 또는 IBM 또는 기타 회사에 대한 내부 정보는 엄격히 기밀이며 알 필요가 없는 가족이나 IBM 동료를 포함하여 IBM 이외의 모든 사람에게 공개해서는 안 됩니다. 또한 내부 정보를 기반으로 고객, IBM 비즈니스 파트너 또는 기타 당사자의 증권을 구매하거나 판매하는 것은 비윤리적이고 불법입니다. 내부 정보를 가지고 무엇을 할 수 있는지 또는 할 수 없는지에 대한 질문이나 우려 사항이 있는 경우 IBM 상담원과 상담하는 것이 중요합니다._ 정렬되지 않은 모형이 문제를 직접 해결하지 않으므로 올바르지 않습니다. 정렬된 모델의 응답은 정확하지만 정책 세부 정보를 더 포함하도록 개선할 수 있습니다. 이 추가 정보는 그림 1에 표시된 피드백 UI를 통해 제공된다. 도 5를 참조하면, 개발자가 정렬된 모델을 개선하기 위해 사용할 수 있다.\n' +
      '\n' +
      '## Conclusion\n' +
      '\n' +
      '이를 달성하기 위한 강력하고 확장 가능한 아키텍처와 함께 LLM을 특정 상황 규정에 정렬하기 위한 원칙적인 접근법을 제시한다. 우리의 방법론은 유연하며 LLM을 IBM 비즈니스 행동 지침에 정렬하여 이를 입증합니다. 향후 작업에는 다른 형태의 가치 사양으로 확장하고 잘못된 응답을 유도하기 위해 반자동 접근법을 추가하는 것이 포함된다[16].\n' +
      '\n' +
      '## Acknowledgment\n' +
      '\n' +
      '저자들은 페이엘 다스, 제이슨 드루스, 마이클 힌드, 마오 류, 로니 러스, 재클린 마르티노, 에릭 미엘링, 존 T에게 감사한다. 리차드, 매튜 리머, 스카일러 스피크만, 존 웜부루의 논평과 토론.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] L. Lessig, "The new Chicago school," _Journal of Legal Studies_, vol. 27, no. S2, pp. 661-691, Jun. 1998.\n' +
      '* [2] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Gansuli, D. Li, E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Nousse, K. Lukosuite, L. Lovitt, M. Sellitto, N. Ehlage, N. Schiefer, N. Mercaloo, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston, S. Krawe, S. El Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph, S. McCandallish, T. Brown, and J. Kaplan, "Constutional AI: Harmlessness from AI feedback," arXiv:2212.08073, 2022.\n' +
      '* [3] H. R. Kirk _et al._, "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback," arXiv:2303.05453, 2023.\n' +
      '* [4] T. Sorensen _et al._, "A roadmap to pluralistic alignment," arXiv:2402.05070, 2024.\n' +
      '* [5] K. R. Varshney, "Decolonial AI alignment: Openness, viesga-dharma, and including excluded knowledges," arXiv:2309.05030v2, 2024.\n' +
      '\n' +
      '도. 4: 주어진 프롬프트에 대한 정렬되지 않은 모델과 정렬된 모델의 응답을 비교하기 위한 UI.\n' +
      '\n' +
      '도. 5: 정답에 대한 응답을 평가하고, 그 품질에 대한 피드백을 제공하기 위한 UI.\n' +
      '\n' +
      '* [6] S. Lazar, "Frontier AI ethics," _Aeon_, Feb. 2024.\n' +
      '* [7] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, "Self-instruct: Aligning language models with self-generated instructions," in _Proceedings of the Annual Meeting of the Association for Computational Linguistics_, Jul. 2023, pp. 13 484-13 508.\n' +
      '* [8] O. Honovich, T. Scialom, O. Levy, and T. Schick, "Unnatural instructions: Tuning language models with (almost) no human labor," in _Proceedings of the Annual Meeting of the Association for Computational Linguistics_, Jul. 2023, pp. 14 409-14 428.\n' +
      '* [9] D. Vrandecic and M. Krotzsch, "Wikidata: A free collaborative knowledgebase," _Communications of the ACM_, vol. 57, no. 10, pp. 78-85, Oct. 2014.\n' +
      '* [10] J. Rios _et al._, "Contextual moral value alignment through context-based aggregation," _under review_, 2024.\n' +
      '* [11] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "LoRA: Low-rank adaptation of large language models," arXiv:2106.09685, 2021.\n' +
      '* [12] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, "QLoRA: Efficient finetuning of quantized LLMs," arXiv:2305.14314, 2023.\n' +
      '* [13] D. Ganguli, L. Lovitt, J. Kermion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K. Mousse, A. Jones, S. Bowman, A. Chen, T. Conerly, N. DasSarma, D. Drain, N. Elhage, S. El-Showk, S. Fort, Z. Hatfield-Dodds, T. Henighan, D. Hernandez, T. Hume, J. Jacobson, S. Johnston, S. Kravce, C. Olsson, S. Ringer, E. Tran-Johnson, D. Amodei, T. Brown, N. Joseph, S. McCandlish, C. Olah, J. Kaplan, and J. Clark, "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned," arXiv:2209.07858, 2022.\n' +
      '* [14] IBM Research, "Granite foundation models," [https://www.ibm.com/downloads/cas/X9W4O6BM](https://www.ibm.com/downloads/cas/X9W4O6BM), 2023.\n' +
      '* [15] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, Q. Guo, M. Wang, and H. Wang, "Retrieval-augmented generation for large language models: A survey," arXiv:2312.10997, 2024.\n' +
      '* [16] G. Kour, M. Zalmanovic, N. Zwerdling, E. Goldbraich, O. N. Fandina, A. Anaby-Tavor, O. Raz, and E. Farchi, "Unveiling safety vulnerabilities of large language models," arXiv:2311.04124, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>