<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# CRM: Convolutional Reconstruction Model을 이용한 단일 이미지 대 3D Textured Mesh\n' +
      '\n' +
      'Zhengyi Wang\n' +
      '\n' +
      '1학과 of Comp. Sci. & 테크, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University,\n' +
      '\n' +
      '중국 런민대학교 2가올링 인공지능학부, 베이징 빅데이터 관리 및 분석방법 핵심연구실, 중국 베이징;\n' +
      '\n' +
      '중국 베이징 3성슈.\n' +
      '\n' +
      'Yikai Wang\n' +
      '\n' +
      '1학과 of Comp. Sci. & 테크, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University,\n' +
      '\n' +
      '중국 런민대학교 2가올링 인공지능학부, 베이징 빅데이터 관리 및 분석방법 핵심연구실, 중국 베이징;\n' +
      '\n' +
      '중국 베이징 3성슈.\n' +
      '\n' +
      'Yifei Chen\n' +
      '\n' +
      '1학과 of Comp. Sci. & 테크, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University,\n' +
      '\n' +
      '중국 런민대학교 2가올링 인공지능학부, 베이징 빅데이터 관리 및 분석방법 핵심연구실, 중국 베이징;\n' +
      '\n' +
      '중국 베이징 3성슈.\n' +
      '\n' +
      'Chendong Xiang\n' +
      '\n' +
      '1학과 of Comp. Sci. & 테크, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University,\n' +
      '\n' +
      '중국 런민대학교 2가올링 인공지능학부, 베이징 빅데이터 관리 및 분석방법 핵심연구실, 중국 베이징;\n' +
      '\n' +
      '중국 베이징 3성슈.\n' +
      '\n' +
      'Shuo Chen\n' +
      '\n' +
      '1학과 of Comp. Sci. & 테크, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University,\n' +
      '\n' +
      '중국 런민대학교 2가올링 인공지능학부, 베이징 빅데이터 관리 및 분석방법 핵심연구실, 중국 베이징;\n' +
      '\n' +
      '중국 베이징 3성슈.\n' +
      '\n' +
      'Dajiang Yu\n' +
      '\n' +
      '1학과 of Comp. Sci. & 테크, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University,\n' +
      '\n' +
      '중국 런민대학교 2가올링 인공지능학부, 베이징 빅데이터 관리 및 분석방법 핵심연구실, 중국 베이징;\n' +
      '\n' +
      '중국 베이징 3성슈.\n' +
      '\n' +
      'Chongxuan Li\n' +
      '\n' +
      '중국 런민대학교 2가올링 인공지능학부, 베이징 빅데이터 관리 및 분석방법 핵심연구실, 중국 베이징;\n' +
      '\n' +
      '중국 베이징 3성슈.\n' +
      '\n' +
      'Hang Su\n' +
      '\n' +
      '1학과 of Comp. Sci. & 테크, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University,\n' +
      '\n' +
      '중국 런민대학교 2가올링 인공지능학부, 베이징 빅데이터 관리 및 분석방법 핵심연구실, 중국 베이징;\n' +
      '\n' +
      '중국 베이징 3성슈.\n' +
      '\n' +
      'Jun Zhu\n' +
      '\n' +
      '교신저자.교신저자.1과 of Comp. Sci. & 테크, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University,\n' +
      '\n' +
      '중국 런민대학교 2가올링 인공지능학부, 베이징 빅데이터 관리 및 분석방법 핵심연구실, 중국 베이징;\n' +
      '\n' +
      '중국 베이징 3성슈.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'LRM(Large Reconstruction Model) [18]과 같은 피드-포워드 3D 생성 모델은 탁월한 생성 속도를 보여주었다. 그러나, 트랜스포머 기반 방법은 트라이플레인 구성요소의 기하학적 사전을 아키텍처에서 활용하지 않으며, 종종 3D 데이터의 제한된 크기를 고려할 때 차선의 품질로 이어진다.\n' +
      '\n' +
      '그림 1: **CRM**은 **10초 안에 단일 이미지로부터 고충실도 텍스처 메쉬를 생성한다.\n' +
      '\n' +
      '느린 훈련 본 연구에서는 고충실도 피드포워드 단일 이미지-3D 생성 모델인 컨볼루션 재구성 모델(Convolutional Reconstruction Model, CRM)을 제시한다. 희소 3차원 데이터에 의해 야기되는 한계를 인식하고, 기하학적 사전들을 네트워크 설계에 통합할 필요성을 강조한다. CRM은 삼평면의 가시화가 6개의 정사영상의 공간대응을 나타낸다는 주요 관찰을 기반으로 한다. 먼저, 단일 입력 영상으로부터 6개의 정사 뷰 영상을 생성한 후, 이들 영상을 컨볼루션 U-Net에 공급하여 높은 픽셀 수준의 정렬 능력과 상당한 대역폭을 활용하여 고해상도 삼평면을 생성한다. CRM은 기하학적 표현으로 플렉시큐브를 추가로 사용하여 텍스처 메쉬에서 직접 엔드 투 엔드 최적화를 용이하게 한다. 전반적으로, 본 모델은 테스트 시간 최적화 없이 단 10초 만에 이미지에서 높은 충실도의 텍스처 메쉬를 제공합니다.\n' +
      '\n' +
      '키워드:3D 생성 텍스처 메쉬 확산 모델\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 몇 년 동안 생성 모델은 데이터 크기의 빠른 성장에 크게 기인하여 상당한 발전을 목격했다. 트랜스포머 [55]는 특히 언어 [3], 이미지 [1, 37] 및 비디오 생성 [2]를 포함한 다양한 도메인에 걸쳐 고성능 결과를 달성했다. 그러나 3D 생성의 영역은 독특한 과제를 제시한다. 다른 모달의 데이터가 풍부한 것과 달리 3D 데이터는 비교적 부족하다. 3D 데이터의 생성은 전문화된 전문성과 상당한 시간을 필요로 하여, 가장 큰 3D 데이터 세트, 즉 Objavverse[12, 13]가 50억 개의 이미지를 포함하는 Laion[45]와 같은 이미지 데이터 세트보다 훨씬 작은 수백만 개의 3D 콘텐츠만을 포함하는 상황으로 이어진다.\n' +
      '\n' +
      '그럼에도 불구하고, 최근의 개발들은 단일 또는 멀티 뷰 이미지들로부터 피드-포워드 방식으로 3D 콘텐츠를 생성하기 위한 LRM[18]과 같은 일부 트랜스포머 기반 방법들[18, 21, 59, 63, 70]을 도입하였다. 이러한 모델들 중에서, 트라이플레인(triplane)은 최소한의 메모리 비용으로 고해상도 3D 결과를 생성하는 효율성으로 인해 인기 있는 컴포넌트로 부상했다. 그러나 트라이플레인 패치 생성을 위한 변압기 기반 네트워크에 대한 의존도는 트라이플레인 개념에 내재된 기하학적 사전값을 활용하지 못하여 품질 및 충실도 측면에서 최적이 아닌 결과를 가져오며, 훈련 시간이 길다.\n' +
      '\n' +
      '본 논문에서는 이러한 문제를 해결하기 위해 빠른 학습뿐만 아니라 높은 생성 품질을 갖는 새로운 컨벌루션 재구성 모델(Convolutional Reconstruction Model, CRM)을 제안한다. 제한된 3D 콘텐츠 양을 감안할 때 CRM은 아키텍처 설계에서 기하학적 사전 사항을 탐색하는 것이 유익하다는 핵심 가설을 기반으로 한다. 즉, 3면 [5, 6, 47]의 시각화를 통해 3면이 그림 2와 같이 입력 6개의 정사 영상의 공간적 대응성을 나타내는 것을 관찰할 수 있다. 입력 영상의 실루엣과 질감은 3면 구조와 자연스러운 정렬을 갖는다. 이는 (1) 6개의 정사영상을 입력영상으로 사용하여 임의로 선택한 다른 포즈 대신 3차원 입체영상을 재구성하고, (2) U-Net 컨볼루션 네트워크를 사용하여 입력과 출력 사이의 강한 픽셀 레벨 정렬을 탐색하여 입력영상을 롤아웃된 3차원 입체영상에 매핑하도록 한다. 또한, U-Net의 상당한 대역폭 용량은 6개의 정사 이미지를 3평면으로 직접 변환할 수 있어 매우 상세한 결과를 얻을 수 있다. 또한 재구성 네트워크에 표준좌표지도(CCM: Canonical Coordinate Map)를 추가하여 모델의 공간관계와 기하학에 대한 이해를 풍부하게 한다.\n' +
      '\n' +
      '단일 영상으로부터 3D 생성 작업을 위해, 6개의 정사영상과 CCM을 직접 이용할 수 없기 때문에, 입력 영상에 조건화된 다시점 확산 모델을 학습하여 6개의 정사영상을 생성하고, 생성된 6개의 정사영상에 조건화된 CCM을 생성한다. 두 확산 모델은 Objaverse 데이터셋의 필터링된 버전에 대해 학습된다[13]. 품질과 견고성을 더욱 향상시키기 위해, 우리는 Zero-SNR[26], 랜덤 크기 조정 및 윤곽 증강을 포함한 멀티뷰 확산 모델에 대한 훈련 개선을 구현한다.\n' +
      '\n' +
      '마지막으로 고품질 텍스처 메쉬를 직접 최적화하는 것이 어렵기 때문에 구배 기반 메쉬 최적화를 무효화하기 위해 형상 표현으로 플렉시큐브[46]를 채택한다. 이것은 NeRF[35] 또는 Gaussian Splatting[19]과 같은 대체 3D 표현을 사용하는 이전 작업 [18, 70]과 다르다. 이러한 방법은 상세한 시각화를 생성할 수 있지만 텍스처 메쉬[51]를 얻기 위한 추가 절차 단계를 포함하는 경우가 많다. 우리의 설계를 통해 질감 있는 메쉬를 최종 출력으로 엔드 투 엔드 방식으로 CRM을 훈련할 수 있으며, 우리의 접근법은 더 간단한 추론 파이프라인과 더 나은 메쉬 품질을 가지고 있다. 전반적으로, 우리의 방법은 그림 1과 같이 10초 이내에 고충실도 텍스처 메쉬를 생성할 수 있다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '### 3D 생성을 위한 스코어 증류\n' +
      '\n' +
      'DreamFusion[38]은 Score Jacobian Chaining[56]이라고도 하는 SDS(Score Distillation Sampling)라는 기술을 제안한다. 대규모 이미지 확산 모델[43, 44]을 활용하여 특정 프롬프트 또는 이미지와 정렬하기 위해 3D 모델을 반복적으로 정제한다. 따라서 3D 데이터셋에 대한 학습 없이 3D 콘텐츠를 생성할 수 있다. 이 라인을 따라 ProlificDreamer[62]는 SDS의 과포화 문제를 크게 완화하고 다양성을 향상시키는 원칙적인 변량 프레임워크인 Variational Score Distillation(VSD)을 제안한다. Zero123[30], MVDream[48], ImageDream[58] 및 기타 많은 [40, 22, 41]은 3D 데이터에 미세 조정된 확산 모델을 사용하여 결과를 개선하고 다중 얼굴 문제를 완화한다[33, 39]. 상각 점수 증류를 탐색합니다. 다른 많은 작업[7, 8, 9, 20, 23, 24, 25, 50, 52, 54, 61, 64, 69]은 속도 또는 품질에서 결과를 많이 개선한다. 그러나, 스코어 증류에 기초한 방법들은 단일 물체를 생성하기 위해 보통 몇 분에서 몇 시간이 걸리며, 이는 계산적으로 비싸다.\n' +
      '\n' +
      '희소 뷰 재구성을 이용한###3D 생성\n' +
      '\n' +
      '몇 가지 접근 방법은 다시점 일관 영상을 생성하고, 희소 뷰 재구성을 이용하여 3차원 콘텐츠를 생성하는 것을 목표로 한다. 예를 들어 SyncDreamer[31]은 다시점 일관성 있는 영상을 생성한 후 재구성을 위해 NeuS[57]을 사용한다. Wonder3D[32]는 교차 도메인 확산으로 결과를 개선한다. Direct2.5[34]는 2.5D 확산으로 결과를 개선한다. 그러나 이러한 방법의 한 가지 일반적인 문제는 희소 뷰를 사용한 재구성을 위한 테스트 시간 최적화가 필요하다는 것이며, 이는 추가 컴퓨팅으로 이어져 최종 품질을 손상시킬 수 있다.\n' +
      '\n' +
      '### Feed-forward 3D 생성 모델\n' +
      '\n' +
      '일부 작업은 피드 포워드 모델[16, 10, 4, 15, 65, 68]을 사용하여 3D 객체를 생성하려고 한다. 피드 포워드 방법은 위에서 언급한 두 가지 유형의 방법에 비해 훨씬 더 빠른 생성 속도를 보여준다. 최근에 더 큰 3D 데이터세트 Objaverse[13]에 대해 훈련된 작품들이 있다. One-2-3-45[29]는 멀티뷰 이미지들을 생성한 후, 이미지들을 네트워크로 공급하여 3D 객체를 얻는다. LRM 시리즈 [18, 59, 63, 21]은 트랜스포머 기반 아키텍처로 생성된 결과의 품질을 향상시킨다. TGS[70] 및 LGM[51]은 기하학적 표현으로서 가우시안 스플래팅[19]을 사용한다. 다른 기법들로 결과를 개선하는 많은 다른 작품들[53, 67, 28]도 있다. 이러한 발전에도 불구하고 네트워크 아키텍처 또는 지오메트리 표현에는 개선의 여지가 남아 있다. 우리의 접근법은 전략적으로 설계된 아키텍처를 가진 네트워크와 메쉬를 최종 출력으로 직접 생성하는 엔드 투 엔드 트레이닝 접근법을 활용한다.\n' +
      '\n' +
      '그림 2: 우리의 주요 동기 중 하나는 3평면이 입력 6개의 정사 이미지와 강한 공간 정렬을 공유한다는 것이다. (a) 입력 형상의 6개의 정사 화상. (b) 6개의 직교 CCM. (c) 우리의 U-Net이 출력한 3면(모든 채널의 평균값)은 입력 영상과 공간적으로 정렬된다. (d) 합성곱 재구성 모델에 의한 텍스처링된 메시 출력.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '이 섹션에서는 본 방법(도 3에 도시됨)의 세부 설계를 설명한다. 단일 입력 영상이 주어졌을 때, 본 모델은 먼저 다시점 확산 모델(Sec. 3.1)을 이용하여 6개의 정사영상과 정준좌표지도(CCM)를 생성한다. 그런 다음 이미지와 CCM에서 3D 텍스처 메쉬를 재구성하기 위해 **컨볼루션 재구성 모델**(CRM, Sec. 3.2)를 개발한다.\n' +
      '\n' +
      '### 다시점 확산 모델\n' +
      '\n' +
      '먼저 단일 입력 영상으로부터 6개의 정사시점 영상을 생성하기 위한 다시점 확산 모델의 설계에 대해 설명한다. 일반적으로 비용이 많이 드는 처음부터 훈련하는 대신, 단일 영상으로부터 다시점 영상 생성 작업을 위한 고성능 확산 모델인 ImageDream [58]의 체크포인트를 이용하여 확산 모델을 초기화한다. 오리지널 이미지드림에서는 4개의 뷰 생성을 지원합니다. 우리는 두 가지 관점(위아래)을 더 추가하여 6개의 관점을 포함하도록 확장한다. 우리는 생성된 6개의 뷰에 조건화된 다른 확산 모델을 사용하여 표준 좌표 맵을 생성한다. 조건부 RGB 이미지는 노이즈 표준 좌표 맵과 연결된다. 또한 이미지드림 체크포인트에서 초기화됩니다. 두 확산 모델은 Objavverse [13] 데이터 세트에서 미세 조정된다.\n' +
      '\n' +
      '결과의 품질과 견고성을 더욱 향상시키기 위해 몇 가지 개선 사항을 소개한다. **(1) Zero-SNR Training.** [26]에서 언급한 대로 Zero-SNR 트릭을 사용한다. 이는 샘플링 동안의 초기 가우시안 노이즈와 가장 시끄러운 트레이닝 샘플 사이의 불일치로 인한 문제를 완화할 수 있다. **(2) 랜덤 리사이징.** 순진한 구현은 모델이 전체 이미지를 차지하는 객체를 생성하는 경향이 있게 할 것이다. 이를 완화하기 위해 훈련할 때 객체의 크기를 임의로 조정합니다. **(3) Contour Augmentation.**.\n' +
      '\n' +
      '그림 3: 우리 방법의 전체 파이프라인. 입력 영상은 다시점 영상 확산 모델에 입력되어 6개의 정사 영상을 생성한다. 그런 다음 다른 확산 모델을 사용하여 6개의 이미지에 조건화된 CCM을 생성한다. CCM과 함께 6개의 이미지는 최종 텍스처 메쉬를 재구성하기 위해 CRM에 전송된다. 전체 추론 과정은 A800 GPU에서 약 10초가 소요된다. *4초는 U-Net 포워드(0.1s 미만), UV 텍스처 및 파일 I/O에 대한 표면 포인트들을 질의하는 것을 포함한다.\n' +
      '\n' +
      '입력 뷰의 등고선. 모델을 등고선에 둔감하게 만들기 위해 훈련 중에 등고선 색상을 무작위로 변경한다.\n' +
      '\n' +
      '### Convolutional Reconstruction Model\n' +
      '\n' +
      '이제 컨볼루션 재구성 모델(CRM)의 세부 아키텍처를 소개하기 위해 이동한다. 그림 1에 요약된 대로. 도 4에 도시된 바와 같이, 입력 6개의 이미지들 및 CCM들이 주어지면, 컨볼루션 U-Net은 CCM들과 함께 입력 이미지들을 롤-아웃 트라이플레인에 매핑하기 위해 사용된다. 그리고 나서, 롤-아웃된 삼평면은 삼평면으로 재형상화된다. 작은 다층 인식(MLP)은 트라이플레인 특징들을 SDF 값들, 텍스처 컬러 및 플렉시큐브 파라미터들로 디코딩하기 위해 사용된다. 마지막으로, 이 값들은 듀얼 마칭 큐브에 의해 텍스처 메쉬를 얻는 데 사용된다. 아래에서는 CRM의 주요 구성 요소에 대해 자세히 설명한다.\n' +
      '\n' +
      '###### 3.2.1 삼면 표현\n' +
      '\n' +
      '3차원 표현으로 삼평면을 선택하는데, 이는 2차원 계산의 소모로 높은 해상도의 3차원 결과를 얻을 수 있기 때문이다. 각 질의 격자 셀을 축 정렬 직교 평면(\\(xy\\), \\(xz\\), \\(yz\\) 평면)으로 투영한 후 각 평면에서 특징을 집계한다. 그리고 2개의 은닉층을 갖는 3개의 작은 MLP에 의해 특징을 디코딩하여 변형, 색상 및 플렉시큐브 가중치와 함께 SDF 값을 얻는다. 또한, 서로 다른 평면들의 불필요한 얽힘을 피하기 위해, 우리는 롤-아웃 트라이플레인[60]을 사용한다.\n' +
      '\n' +
      '####3.2.2 정준좌표지도(CCM)\n' +
      '\n' +
      '또한 추가 지오메트리 정보를 포함하는 CCM을 입력으로 추가한다[22]. 이는 3D 객체를 예측하기 위해 일반적으로 순수한 RGB 이미지를 입력으로 사용하는 이전 작업과 다르다[18]. 순수한 RGB 이미지를 사용하면 정확한 기하학을 예측하는 것이 매우 어려워지고 때로는 기하학이 저하됩니다(Sec. 4.3의 세부 사항). 형식적으로 CCM은 정준 공간에서 각 점의 좌표이다. 정준 공간의 좌표를 나타내는 \\([0,1]\\) 이내의 값을 갖는 3개의 채널을 포함한다.\n' +
      '\n' +
      '###### 3.2.3 UNet 기반 Convolutional Network\n' +
      '\n' +
      '우리의 핵심 통찰력은 그림 2와 같이 3평면이 입력된 6개의 정사 이미지와 CCM과 공간적으로 정렬된다는 것이다. 롤아웃된 3평면과 일치하도록 6개의 이미지와 CCM은 유사한 방식으로 배열된다. 우리는 6개의 이미지와 CCM을 \\(256\\times 256\\)의 해상도로 렌더링한다. 그들은 두 개의 그룹으로 나뉘며, 각 그룹은 세 개의 이미지를 가지고 있다. 그런 다음 네 그룹의 이미지를 결합하여 각 그룹의 해상도가 \\(256\\times 768\\)인 네 개의 더 큰 이미지를 생성하여 공간 정렬이 가능하다. 이 네 개의 그룹을 연결하여 12채널 입력을 형성한다. 다음으로, 컨볼루션 U-Net은 출력 삼평면을 생성하기 위해 이 입력을 처리한다.\n' +
      '\n' +
      '변압기 기반 방법[18, 21, 63, 70]에 비해 U-shape 설계는 입력 정보를 보존하는 데 더 큰 대역폭을 가지고 있어 매우 상세한 3면 특징과 최종적으로 정교한 텍스처 메쉬로 이어진다. 또한, 합성곱 신경망은 3평면의 공간 대응 이전에 기하학을 충분히 활용하고, 6개의 정사 영상을 입력함으로써 융합을 크게 체결하고 훈련을 안정화시킨다. 우리의 모델은 훈련의 매우 초기 단계(처음부터 약 20분의 훈련)에서 합리적인 재구성 결과를 얻을 수 있다. 또한, 우리의 모델은 훨씬 더 작은 배치 크기 32(1024의 배치 크기를 사용하는 변압기 기반 LRM에 비해)로 훈련될 수 있으며, 이는 우리의 모든 실험이 8 GPU 카드 기계에서 수행될 수 있게 한다. 재구성 모델의 전체 훈련 비용은 LRM보다 1/8에 불과하다. 자세한 내용은 실험에 나와 있다(Sec. 4.1 참조).\n' +
      '\n' +
      '######4.2.2 플렉시큐브 형상\n' +
      '\n' +
      '기존의 일반적인 3D 생성 방법은 대부분 NeRF[35] 또는 Gaussian splatting[19]를 기하 표현으로 채택하는데, 이는 등지면을 추출하기 위해 Marching Cubes(MC)와 같은 추가 절차에 의존하며 위상 모호성을 겪고 고충실도의 기하 세부 사항을 표현하는데 어려움을 겪는다. 이 작업에서 우리는 기하학적 표현으로 플렉시큐브[46]를 사용한다. 훈련 중에 이중 행진 큐브[36]를 통해 그리드의 피쳐에서 메쉬를 얻을 수 있습니다. 특징들은 SDF 값들, 변형 및 가중치들을 포함한다. 텍스처는 표면에서의 컬러를 질의함으로써 얻어진다. 플렉시큐브는 텍스처 메쉬를 최종 출력으로 사용하여 종단 간 방식으로 재구성 모델을 훈련할 수 있게 한다.\n' +
      '\n' +
      '######4.2.3 손실함수\n' +
      '\n' +
      '마지막으로, CRM 모델을 학습하기 위해, LRM[18]과 유사한 텍스처를 위해 렌더링된 이미지에 대해 MSE 손실\\(\\mathcal{L}_{\\textsc{mse}\\)과 LPIPS 손실[66]\\(\\mathcal{L}_{\\textsc{LPIPS}\\)의 조합을 사용한다. 지오메트리를 더욱 향상시키기 위해 깊이 맵도 포함합니다.\n' +
      '\n' +
      '도 4: CRM의 트레이닝 파이프라인과 함께 아키텍처. 우리는 3D 메쉬를 6개의 정사 이미지와 CCM으로 렌더링한다. 그런 다음 이미지와 CCM을 연결하여 U-Net에 공급한다. 출력 3평면은 플렉시큐브의 특징 그리드를 형성하기 위해 작은 MLP 네트워크에 의해 디코딩되고, 텍스처링된 메시는 듀얼 마칭 큐브에 의해 얻어진다. 훈련하는 동안, 우리는 감독을 위해 GT 메쉬와 재구성 메쉬로부터 컬러 이미지, 깊이 맵 및 마스크를 렌더링한다.\n' +
      '\n' +
      '그리고 감독용 마스크[16]. 상기 전체 손실 함수는,\n' +
      '\n' +
      '\\mathcal{L}_{\\text{MSE}(\\boldsymbol{x}^{\\text{GT}})+\\lambda_{LPIPS}(\\boldsymbol{x}^{\\text{mask}},\\end{split}\\mathcal{L}_{\\text{MSE}(\\boldsymbol{x}^{\\text{mask}}),\\lambda_{mask}}\\mathcal{L}_{\\text{mask}}(\\boldsymbol{x}^{\\text{mask})+\\lambda_{mask}}\n' +
      '\n' +
      '여기서 \\(\\boldsymbol{x}\\), \\(\\boldsymbol{x}_{\\text{depth}\\) 및 \\(\\boldsymbol{x}_{\\text{sil}\\)은 각각 재구성 텍스처 메쉬로부터 렌더링된 RGB 이미지, 깊이 맵 및 마스크를 나타낸다. 그리고 \\(\\boldsymbol{x}^{\\text{GT}}\\), \\(\\boldsymbol{x}^{\\text{GT}}_{\\text{depth}\\) 및 \\(\\boldsymbol{x}^{\\text{GT}}_{\\text{mask}\\)는 지상진실 텍스쳐 메쉬로부터 렌더링된다. \\\\ (\\mathcal{L}_{\\text{reg}}\\)는 Flexicubes[46]에 소개된 메쉬 품질의 규칙화기이다. (\\lambda_{\\text{LPIPS}\\), \\(\\lambda_{\\text{depth}\\), \\(\\lambda_{\\text{mask}\\) 및 \\(\\lambda_{\\text{reg}\\)은 각 손실의 균형을 이루는 계수이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setting\n' +
      '\n' +
      '#### 4.1.1 Dataset\n' +
      '\n' +
      '우리는 Objaverse [13] 데이터 세트를 필터링하고, 장면 레벨 객체들과 저품질 메쉬들을 제거하고, 트레이닝 세트로서 약 376k의 유효한 고품질 객체들을 얻는다. 포함된 SyncDreamer[31]에서 렌더링된 이미지를 재사용한다.\n' +
      '\n' +
      '그림 5: 기준선과의 질적 비교. 모델은 더 나은 지오메트리 및 텍스처로 높은 충실도 결과를 생성합니다.\n' +
      '\n' +
      '각 형상당 16개의 영상을 256\\(256\\times 256\\)의 해상도로 렌더링하고, 동일한 조명과 해상도로 6개의 정사영상과 CCM을 추가로 렌더링한다.\n' +
      '\n' +
      '######4.1.3 네트워크 아키텍처\n' +
      '\n' +
      '복원 모델은 약 300M 매개 변수를 포함한다. U-Net은 \\([64,128,128,256,256,512,512]\\) 채널들을 포함하고, 해상도 \\([32,16,8]\\)에서 주의 블록들을 포함한다. 우리는 플렉시큐브 그리드 크기를 80으로 설정합니다.\n' +
      '\n' +
      '1.4 구현 세부사항\n' +
      '\n' +
      '재구성 모델은 8개의 NVIDIA A800 80GB GPU 카드에서 6일 동안 \\(110k\\) 반복으로 훈련되었다. 모델은 배치 크기 32(반복당 32개 모양)로 훈련되었다. 각 반복에서 감독을 위해 각 모양에 대해 총 16개의 이미지 중 8개의 뷰를 무작위로 샘플링했다. 학습률\\(1e-4\\)을 갖는 Adam optimizer를 사용하였다. 각 손실의 균형을 이루는 계수는 \\(\\lambda_{\\text{\\tiny LPIPS}=0.1\\), \\(\\lambda_{\\text{\\tiny depth}=0.5\\), \\(\\lambda_{\\text{\\tiny max}=0.5\\) 및 \\(\\lambda_{\\text{\\tiny reg}=0.005\\)으로 설정하였다. 생성된 다시점 영상에서 작은 불일치에 대한 강인성을 향상시키기 위해 학습과 추론에서 작은 가우시안 잡음을 입력으로 도입하였다.\n' +
      '\n' +
      '6개의 정사 영상과 CCM에 대한 확산 모델은 8개의 NVIDIA A800 80GB GPU 카드에서 2일 동안 \\(10k\\) 반복으로 훈련되었다. 구배축적은 12단계로 설정하였으며, 총 배치크기는 1536이었으며, 학습률\\(5e-5\\)을 갖는 Adam optimizer를 사용하였다. 샘플링하는 동안 두 확산은 DDIM[49]을 사용하여 50단계로 샘플링되었다.\n' +
      '\n' +
      '###기준선 비교\n' +
      '\n' +
      '######4.2.1 질적 결과\n' +
      '\n' +
      '제안된 방법의 유효성을 검증하기 위해, 원더3d[32], SynC Dreamer[31], Magic123[40], One-2-3-45[29] 및 OpenLRM[17]을 포함한 이전 연구와 결과를 정성적으로 비교한다. LRM[18]은 오픈소싱이 아니므로 비교를 위해 LRM의 오픈소싱 구현인 OpenLRM[17]을 사용한다. 다른 기준선의 경우 공식 코드와 체크포인트를 사용합니다. 테스트를 위한 입력 이미지는 GSO[14] 데이터셋에서 웹에서 다운로드한 것과 텍스트-이미지 확산 모델에 의해 생성된 것 중 두 가지를 선택한다. 그 결과는 그림 5에 나와 있으며, 그림에서 본 논문에서 제안한 방법은 다른 모든 기준선보다 질감과 형상이 더 좋은 3D 텍스처 메쉬를 생성함을 알 수 있다. 이는 재구성 모델이 입력 6개의 정사영상과 출력 삼평면의 공간 정렬을 충분히 활용하기 때문이다. 또한, 우리의 모델은 대부분의 기준선보다 훨씬 빠른 10초 만에 생성할 수 있습니다. 우리의 방법은 최종 출력으로서 텍스처링된 메시를 갖는 종단간 방식으로 트레이닝되며, 따라서 [51]에서와 같이 메시로 변환하기 위한 시간 소모적인 후처리를 회피한다.\n' +
      '\n' +
      '또한, 생성된 메쉬를 이전 작업 LRM[18]과 동시 작업 LGM[51]과 비교하여 시각화한다. LRM은 오픈 소스가 아니므로 프로젝트 페이지에서 메쉬를 사용하며 결과는 그림 6에 나와 있으며 그림에서 더 나은 질감을 가지고 있음을 알 수 있다. 또한 제안된 방법은 LRM보다 더 부드러운 기하 구조와 LGM보다 더 나은 기하 세부 정보를 가지고 있다.\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 7을 참조하면, 본 논문에서 제안한 방법으로 단일 영상으로부터 생성된 고충실도 텍스처 메쉬의 결과를 보다 잘 보여준다.\n' +
      '\n' +
      '######4.2.2 정량적 결과\n' +
      '\n' +
      '선행 연구 [32]에 기초하여, 우리는 훈련 데이터 세트에 포함되지 않은 Google Scanned Objects (GSO) 데이터 세트 [14]를 사용하여 우리의 방법을 평가한다. 우리는 무작위로 30개의 도형을 선택하고, 평가를 위한 입력으로 \\(256\\times 256\\) 크기의 단일 이미지를 렌더링한다. 생성된 메쉬가 그라운드 트루스 메쉬와 정확하게 정렬되도록 하기 위해, 우리는 그들의 자세를 조심스럽게 조정하고 \\([-0.5,0.5]\\) 상자에 맞도록 스케일을 조정한다. 메쉬 기하학 평가를 위해 재구성된 메쉬와 Ground truth 메쉬 사이의 기하학 유사성을 측정하는 Chamfer Distance (CD), Volumn IoU 및 F-Score (임계값 0.05, One-2-3-45 [29])를 보고한다. 그 결과는 표 1과 같으며, 표로부터 제안된 방법이 모든 기준선들을 능가하는 것을 알 수 있으며, 이는 기하학적 품질에 대한 우리의 방법의 유효성을 입증한다.\n' +
      '\n' +
      '또한 메쉬 텍스처를 평가하기 위해 생성된 메쉬와 지상-진실 메쉬에 대해 각각 0, 15, 30도의 고도각에서 24개의 이미지를 512(512배 512\\)의 해상도로 렌더링한다. 각 고도에 대해 8개의 이미지가 전체 360도 회전을 중심으로 고르게 분포되어 있습니다. 그리고 PSNR, SSIM, LPIPS, Clip-Similarity를 이용하여 재구성된 메쉬와 원래의 그라운드 트루스 메쉬 사이의 외관의 유사성을 측정하였다. 결과를 표 2에 나타낸다. 생성된 것을 나타낸다\n' +
      '\n' +
      '그림 6: LRM [18] 및 LGM [51]과의 질적 비교. 우리의 모델은 상세한 질감과 매끄러운 기하학으로 높은 충실도 결과를 생성한다.\n' +
      '\n' +
      '본 논문에서 제안한 모델의 질감 메쉬는 외관상 모든 기준선의 메쉬를 능가하여 질감 품질에 대한 방법의 효율성을 보여준다.\n' +
      '\n' +
      '또한, 다중 뷰 확산 모델에 대한 단일 이미지의 유효성을 평가하기 위한 실험을 수행한다. 생성된 다시점 영상과 Ground truth 다시점 영상의 유사도를 측정하기 위해 PSNR, SSIM, LPIPS를 사용한다. 이 분석을 위해 각 모델에서 생성된 이미지(왼쪽, 오른쪽, 앞, 뒤)를 비교하여 네 개의 뷰를 사용하여 배경색을 회색(값 128)으로 설정한다. 우리는 SyncDreamer[31]와 Wonder3D[32]를 비교한다. 이 평가 결과는 표 3에 기록되어 있으며, 표에서 우리의 방법이 모든 기준선보다 우수하다는 것을 알 수 있다.\n' +
      '\n' +
      '### 절제 연구 및 분석\n' +
      '\n' +
      '조기훈련단계의 재구성 결과\n' +
      '\n' +
      'CRM의 장점은 훈련하기가 쉽다는 것입니다. 사실, 우리는 CRM이 훈련의 매우 초기 단계에서 합리적인 결과를 보여주기 시작한다는 것을 발견했다. 결과를 도 8에 나타낸다. 겨우 280회 반복(단 20분의 훈련)으로도 결과가 좋다.\n' +
      '\n' +
      '그림 7: 단일 이미지에서 본 방법의 생성된 결과의 추가 결과.\n' +
      '\n' +
      '우리는 빠른 수렴이 아키텍처 설계 이전의 강한 기하학으로부터 기인한다고 추측한다.\n' +
      '\n' +
      'CRM의 훈련시간 4.2.1\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 9에서는 본 방법(재구성 모델만)과 LRM[18]과 LGM[51]의 두 가지 기준선 사이의 훈련 비용을 비교한다. 우리는 NVIDIA A100/A800 GPU 카드 사용량에 훈련 일수를 곱하여 훈련 비용을 측정한다. 우리의 모델은 두 개의 기준선보다 훨씬 적은 훈련 시간이 소요됨을 알 수 있다. 이 모델은 입력 6개의 정사영상/CCM과 3면 사이의 공간대응을 활용하기 때문에 훈련을 더 쉽게 할 수 있는 강력한 사전 역할을 하기 때문이다.\n' +
      '\n' +
      '입력 CCM의 중요도 4.2.2\n' +
      '\n' +
      '입력 영상에 연결된 CCM의 중요도를 살펴본다. 비교를 위해 CCM 없이 6개의 RGB 영상만을 입력으로 하는 재구성 모델을 학습한다. 그 결과를 도에 나타낸다. 10. CCM 입력 없이 기하학의 결과가 많이 열화됨을 알 수 있다. CCM은 특히 지오메트리가 복잡한 경우 모델에 중요한 지오메트리 정보를 제공하기 때문이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Method & Chamfer Dist.\\(\\downarrow\\) & Vol. IoU\\(\\uparrow\\) & F-Sco. (\\%)\\(\\uparrow\\) \\\\ \\hline One-2-3-45 [29] & 0.0172 & 0.4463 & 72.19 \\\\ SyncDreamer [31] & 0.0140 & 0.3900 & 75.74 \\\\ Wonder3D [32] & 0.0186 & 0.4398 & 76.75 \\\\ Magic123 [40] & 0.0188 & 0.3714 & 60.66 \\\\ TGS [70] & 0.0172 & 0.2982 & 65.17 \\\\ OpenLRM [17; 18] & 0.0168 & 0.3774 & 63.22 \\\\ LGM [51] & 0.0117 & 0.4685 & 68.69 \\\\ Ours & **0.0094** & **0.6131** & **79.38** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 단일 이미지와 3D 텍스처 메쉬 생성에 대한 기준선 간의 기하학적 품질에 대한 정량적 비교. 우리는 GSO 데이터 세트에 대한 Chamfer Distance, Volumn IoU 및 F-score의 메트릭을 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) & Clip-Sim\\(\\uparrow\\) \\\\ \\hline One-2-3-45 [29] & 13.93 & 0.8084 & 0.2625 & 79.83 \\\\ SyncDreamer [31] & 14.00 & 0.8165 & 0.2591 & 82.76 \\\\ Wonder3D [32] & 13.31 & 0.8121 & 0.2554 & 83.70 \\\\ Magic123 [40] & 12.69 & 0.7984 & 0.2442 & 85.16 \\\\ OpenLRM [17; 18] & 14.30 & 0.8294 & 0.2276 & 84.20 \\\\ LGM [70] & 13.28 & 0.7946 & 0.2560 & 85.20 \\\\ Ours & **16.22** & **0.8381** & **0.2143** & **87.55** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 단일 이미지에 대한 기준선과 3D 텍스처 메쉬 생성에 대한 우리의 방법 간의 텍스처 품질에 대한 정량적 비교. 우리는 GSO 데이터 세트에서 PSNR, SSIM, LPIPS 및 Clip [42] 유사성의 메트릭을 보고한다.\n' +
      '\n' +
      '다시점 확산 설계 4.2.3\n' +
      '\n' +
      '본 논문에서는 다시점 확산 모델 설계의 효용성을 검토한다. 순진하게 기준선에서 시작해서\n' +
      '\n' +
      '그림 8: 훈련 초기 단계에서 보이지 않는 샘플에 대한 재구성 결과.\n' +
      '\n' +
      '그림 10: 입력 이미지에 연결된 CCM은 우리 모델에 유익하다. (a) CCM을 제공하지 않고, 모델은 합리적이지만 그다지 좋지 않은 기하학을 출력한다. (b) 훨씬 더 나은 기하학으로 CCM 입력과 함께 우리의 전체 모델을 사용하여 형상을 재구성했다. (c) 동일한 포즈로부터 렌더링된 그라운드 진리 메시.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Method & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline SyncDreamer [31] & 20.30 & 0.7804 & 0.2932 \\\\ Wonder3D [32] & 23.76 & 0.8127 & 0.2210 \\\\ Ours & **29.36** & **0.8721** & **0.1354** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 다중 뷰 확산 모델의 새로운 뷰 합성을 위한 우리의 방법과 기준선 간의 정량적 비교. 우리는 GSO 데이터 세트에 대한 PSNR, SSIM 및 LPIPS의 메트릭을 보고한다.\n' +
      '\n' +
      '2개의 추가 뷰로 미리 훈련된 ImageDream 모델을 미세 조정하고, 제안된 기법을 훈련에 순차적으로 추가한다. 본 논문에서는 GSO의 부분집합에 대해 PSNR, SSIM, LPIPS 메트릭을 이용하여 생성된 새로운 뷰 영상과 지상진실 영상의 유사도를 비교하여 그 결과를 살펴본다. 결과를 표 4에 나타내었다. Zero-SNR 트릭과 랜덤 리사이징이 모두 유익함을 알 수 있다. 등고선 증강은 정량적 메트릭을 개선하지 않는다는 점에 유의한다. 그러나, 우리는 이 트릭이 모델을 와일드 입력 이미지들에서 더 강건하게 만든다는 것을 발견한다(도 11).\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 단일 영상으로부터 고품질의 3차원 모델을 생성하기 위한 컨볼루션 재구성 모델(Convolutional Reconstruction Model, CRM)을 제안한다. 제안된 방법은 입력 영상과 출력 삼면 사이의 공간 관계를 효과적으로 활용하여 기존의 변압기 기반 방법[18]에 비해 훨씬 적은 훈련 비용으로 텍스처 메쉬를 개선한다. 모델은 엔드 투 엔드 트레이닝 기반으로 작동하여 텍스처 메쉬를 직접 출력합니다. 전반적으로, 우리의 방법은 단 10초 만에 상세한 텍스처 메쉬를 생성할 수 있다.\n' +
      '\n' +
      '**제한.** 우리의 방법은 10초 내에 고 충실도의 텍스처 메쉬를 생성할 수 있지만, 여전히 몇 가지 제한 사항이 있다. ImageDream[58]의 한계로서, 입력 영상이 표고가 크거나 FoV가 다른 경우, 결과는 때때로 만족스럽지 않다. 또한, 보장하기가 매우 어렵습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Method & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline ImageDream (6 view) & 28.99 & 0.8565 & 0.1497 \\\\ + Zero-SNR & 29.13 & 0.8598 & 0.1498 \\\\ + Random Resizing & **29.36** & **0.8721** & **0.1354** \\\\ + Contour Augmentation & 28.92 & 0.8681 & 0.1444 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 새로운 뷰 합성에 대한 멀티뷰 확산 설계에 대한 절제 연구. 우리는 GSO 데이터 세트에 대한 PSNR, SSIM 및 LPIPS의 메트릭을 보고한다.\n' +
      '\n' +
      '도 11: 윤곽 증강의 시연. (a) 입력 이미지가 주어지면, (b) 기성품 분할 모델은 때때로 불완전한 결과들을 제공한다. (c) 윤곽 증강 없이, 예측된 백뷰 색상은 윤곽에 민감하다. (d) 윤곽 증강을 통해, 모델은 합리적인 결과를 예측한다. (e) 훈련 중에 입력 영상을 증강하는 방법을 보여준다.\n' +
      '\n' +
      '멀티뷰 확산 모델은 항상 완전히 일관된 결과를 생성하고, 일관되지 않은 이미지는 3D 결과를 열화시킬 수 있다. 마지막으로, 제한된 컴퓨팅 자원으로 인해 플렉시큐브 그리드 크기는 80에 불과하여 매우 상세한 형상을 표현할 수 없다.\n' +
      '\n' +
      '**잠재적 부정적 영향** 다른 많은 생성된 모델과 유사하게 CRM이 악성 또는 가짜 3D 콘텐츠를 생성하는 데 사용될 수 있으므로 추가 주의가 필요할 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Bao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., Zhu, J.: All are worth words: A vit backbone for diffusion models. In: CVPR (2023)\n' +
      '* [2] Brooks, T., Peebles, B., Homes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., Ramesh, A.: Video generation models as world simulators (2024), [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)\n' +
      '* [3] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners (2020)\n' +
      '* [4] Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo, O., Guibas, L.J., Tremblay, J., Khamis, S., et al.: Efficient geometry-aware 3d generative adversarial networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16123-16133 (2022)\n' +
      '* [5] Chen, A., Xu, Z., Geiger, A., Yu, J., Su, H.: Tensorf: Tensorial radiance fields. In: European Conference on Computer Vision. pp. 333-350. Springer (2022)\n' +
      '* [6] Chen, H., Gu, J., Chen, A., Tian, W., Tu, Z., Liu, L., Su, H.: Single-stage diffusion nerf: A unified approach to 3d generation and reconstruction. arXiv preprint arXiv:2304.06714 (2023)\n' +
      '* [7] Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873 (2023)\n' +
      '* [8] Chen, Y., Zhang, C., Yang, X., Cai, Z., Yu, G., Yang, L., Lin, G.: It3d: Improved text-to-3d generation with explicit view synthesis (2023)\n' +
      '* [9] Chen, Z., Wang, F., Liu, H.: Text-to-3d using gaussian splatting. arXiv preprint arXiv:2309.16585 (2023)\n' +
      '* [10] Cheng, Y.C., Lee, H.Y., Tuyakov, S., Schwing, A., Gui, L.: SDFusion: Multimodal 3d shape completion, reconstruction, and generation. In: CVPR (2023)\n' +
      '* [11] Decatur, D., Lang, I., Aberman, K., Hanocka, R.: 3d paintbrush: Local stylization of 3d shapes with cascaded score distillation. arXiv preprint arXiv:2311.09571 (2023)\n' +
      '* [12] Deitke, M., Liu, R., Wallingford, M., Ngo, H., Michel, O., Kusupati, A., Fan, A., Laforte, C., Voleti, V., Gadre, S.Y., et al.: Objaverse-xl: A universe of 10m+ 3d objects. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [13] Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3d objects. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13142-13153 (2023)* [14] Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K., McHugh, T.B., Vanhoucke, V.: Google scanned objects: A high-quality dataset of 3d scanned household items. In: 2022 International Conference on Robotics and Automation (ICRA). pp. 2553-2560. IEEE (2022)\n' +
      '* [15] Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Gojcic, Z., Fidler, S.: Get3d: A generative model of high quality 3d textured shapes learned from images. Advances In Neural Information Processing Systems **35**, 31841-31854 (2022)\n' +
      '* [16] Gupta, A., Xiong, W., Nie, Y., Jones, I., Oguz, B.: 3dgen: Triplane latent diffusion for textured mesh generation. arXiv preprint arXiv:2303.05371 (2023)\n' +
      '* [17] He, Z., Wang, T.: Openlrm: Open-source large reconstruction models. [https://github.com/3DTopia/OpenLRM](https://github.com/3DTopia/OpenLRM) (2023)\n' +
      '* [18] Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400 (2023)\n' +
      '* [19] Kerbl, B., Kopanas, G., Leimkuhler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics **42**(4) (2023)\n' +
      '* [20] Kim, S., Lee, K., Choi, J.S., Jeong, J., Sohn, K., Shin, J.: Collaborative score distillation for consistent visual synthesis (2023)\n' +
      '* [21] Li, J., Tan, H., Zhang, K., Xu, Z., Luan, F., Xu, Y., Hong, Y., Sunkavalli, K., Shakhnarovich, G., Bi, S.: Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214 (2023)\n' +
      '* [22] Li, W., Chen, R., Chen, X., Tan, P.: Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. arXiv preprint arXiv:2310.02596 (2023)\n' +
      '* [23] Li, Y., Dou, Y., Shi, Y., Lei, Y., Chen, X., Zhang, Y., Zhou, P., Ni, B.: Focal-dreamer: Text-driven 3d editing via focal-fusion assembly (2023)\n' +
      '* [24] Liang, Y., Yang, X., Lin, J., Li, H., Xu, X., Chen, Y.: Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching (2023)\n' +
      '* [25] Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 300-309 (2023)\n' +
      '* [26] Lin, S., Liu, B., Li, J., Yang, X.: Common diffusion noise schedules and sample steps are flawed. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 5404-5411 (2024)\n' +
      '* [27] Liu, F., Wu, D., Wei, Y., Rao, Y., Duan, Y.: Sherpa3d: Boosting high-fidelity text-to-3d generation via coarse 3d prior (2023)\n' +
      '* [28] Liu, M., Shi, R., Chen, L., Zhang, Z., Xu, C., Wei, X., Chen, H., Zeng, C., Gu, J., Su, H.: One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885 (2023)\n' +
      '* [29] Liu, M., Xu, C., Jin, H., Chen, L., Xu, Z., Su, H., et al.: One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. arXiv preprint arXiv:2306.16928 (2023)\n' +
      '* [30] Liu, R., Wu, R., Hoorick, B.V., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object (2023)\n' +
      '* [31] Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: Syncdreamer: Generating multiview-consistent images from a single-view image. arXiv preprint arXiv:2309.03453 (2023)* [32] Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H., Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008 (2023)\n' +
      '* [33] Lorraine, J., Xie, K., Zeng, X., Lin, C.H., Takikawa, T., Sharp, N., Lin, T.Y., Liu, M.Y., Fidler, S., Lucas, J.: Att3d: Amortized text-to-3d object synthesis (2023)\n' +
      '* [34] Lu, Y., Zhang, J., Li, S., Fang, T., McKinnon, D., Tsin, Y., Quan, L., Cao, X., Yao, Y.: Direct2.5: Diverse text-to-3d generation via multi-view 2.5d diffusion (2023)\n' +
      '* [35] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM **65**(1), 99-106 (2021)\n' +
      '* [36] Nielson, G.M.: Dual marching cubes. In: IEEE visualization 2004. pp. 489-496. IEEE (2004)\n' +
      '* [37] Peebles, W., Xie, S.: Scalable diffusion models with transformers (2023)\n' +
      '* [38] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022)\n' +
      '* [39] Qian, G., Cao, J., Siarohin, A., Kant, Y., Wang, C., Vasilkovsky, M., Lee, H.Y., Fang, Y., Skorokhodov, I., Zhuang, P., Gilitschenski, I., Ren, J., Ghanem, B., Aberman, K., Tulyakov, S.: Atom: Amortized text-to-mesh using 2d diffusion (2024)\n' +
      '* [40] Qian, G., Mai, J., Hamdi, A., Ren, J., Siarohin, A., Li, B., Lee, H.Y., Skorokhodov, I., Wonka, P., Tulyakov, S., Ghanem, B.: Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors (2023)\n' +
      '* [41] Qiu, L., Chen, G., Gu, X., Zuo, Q., Xu, M., Wu, Y., Yuan, W., Dong, Z., Bo, L., Han, X.: Richdreamer: A generalizable normal-depth diffusion model for detail richness in text-to-3d (2023)\n' +
      '* [42] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [43] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [44] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems **35**, 36479-36494 (2022)\n' +
      '* [45] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: Laion-5b: An open large-scale dataset for training next generation image-text models (2022)\n' +
      '* [46] Shen, T., Munkberg, J., Hasselgren, J., Yin, K., Wang, Z., Chen, W., Gojcic, Z., Fidler, S., Sharp, N., Gao, J.: Flexible isosurface extraction for gradient-based mesh optimization. ACM Transactions on Graphics (TOG) **42**(4), 1-16 (2023)\n' +
      '* [47] Shi, R., Wei, X., Wang, C., Su, H.: Zerorf: Fast sparse view 360 {\\(\\backslash\\)deg} reconstruction with zero pretraining. arXiv preprint arXiv:2312.09249 (2023)\n' +
      '* [48] Shi, Y., Wang, P., Ye, J., Long, M., Li, K., Yang, X.: Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512 (2023)\n' +
      '* [49] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020)\n' +
      '* [50] Sun, J., Zhang, B., Shao, R., Wang, L., Liu, W., Xie, Z., Liu, Y.: Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior (2023)\n' +
      '*[*[51] Tang, J., Chen, Z., Chen, X., Wang, T., Zeng, G., Liu, Z.: Lgm: Large multi-view gaussian model for high-resolution 3d content creation(2024)\n' +
      '* [52] Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)\n' +
      '* [53] Tochilkin, D., Pankratz, D., Liu, Z., Huang, Z.,, Letts, A., Li, Y., Liang, D., Laforte, C., Jampani, V., Cao, Y.P.: Triposr: Fast 3d object reconstruction from a single image. arXiv preprint arXiv:2403.02151 (2024)\n' +
      '* [54] Tsalicoglou, C., Manhardt, F., Tonioni, A., Niemeyer, M., Tombari, F.: Textmesh: Generation of realistic 3d meshes from text prompts. arXiv preprint arXiv:2304.12439 (2023)\n' +
      '* [55] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need (2023)\n' +
      '* [56] Wang, H., Du, X., Li, J., Yeh, R.A., Shakhnarovich, G.: Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation (2022)\n' +
      '* [57] Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction (2023)\n' +
      '* [58] Wang, P., Shi, Y.: Imagedream: Image-prompt multi-view diffusion for 3d generation. arXiv preprint arXiv:2312.02201 (2023)\n' +
      '* [59] Wang, P., Tan, H., Bi, S., Xu, Y., Luan, F., Sunkavalli, K., Wang, W., Xu, Z., Zhang, K.: Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024 (2023)\n' +
      '* [60] Wang, T., Zhang, B., Zhang, T., Gu, S., Bao, J., Baltrusaitis, T., Shen, J., Chen, D., Wen, F., Chen, Q., et al.: Rodin: A generative model for sculpting 3d digital avatars using diffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4563-4573 (2023)\n' +
      '* [61] Wang, X., Wang, Y., Ye, J., Wang, Z., Sun, F., Liu, P., Wang, L., Sun, K., Wang, X., He, B.: Animatabledreamer: Text-guided non-rigid 3d model generation and reconstruction with canonical score distillation\n' +
      '* [62] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), [https://openreview.net/forum?id=ppJuFSOAnM](https://openreview.net/forum?id=ppJuFSOAnM)\n' +
      '* [63] Xu, Y., Tan, H., Luan, F., Bi, S., Wang, P., Li, J., Shi, Z., Sunkavalli, K., Wetzstein, G., Xu, Z., et al.: Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. arXiv preprint arXiv:2311.09217 (2023)\n' +
      '* [64] Yu, X., Guo, Y.C., Li, Y., Liang, D., Zhang, S.H., Qi, X.: Text-to-3d with classifier score distillation (2023)\n' +
      '* [65] Zeng, X., Vahdat, A., Williams, F., Gojcic, Z., Litany, O., Fidler, S., Kreis, K.: Lion: Latent point diffusion models for 3d shape generation (2022)\n' +
      '* [66] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric (2018)\n' +
      '* [67] Zheng, X.Y., Pan, H., Guo, Y.X., Tong, X., Liu, Y.: Mvd\\({}^{2}\\): Efficient multiview 3d reconstruction for multiview diffusion (2024)\n' +
      '* [68] Zheng, X.Y., Pan, H., Wang, P.S., Tong, X., Liu, Y., Shum, H.Y.: Locally attentional sdf diffusion for controllable 3d shape generation. arXiv preprint arXiv:2305.04461 (2023)\n' +
      '* [69] Zhu, J., Zhuang, P.: Hifa: High-fidelity text-to-3d generation with advanced diffusion guidance (2023)\n' +
      '*[*[70] Zou, Z.X., Yu, Z., Guo, Y.C., Li, Y., Liang, D., Cao, Y.P., Zhang, S.H.: Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformer. arXiv preprint arXiv:2312.09147 (2023)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>