<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# LongAlign: Long Context Alignment를 위한 레시피\n' +
      '\n' +
      '대언어 모델\n' +
      '\n' +
      ' Yushi Bai\\({}^{\\dagger\\dagger}\\), Xin Lv\\({}^{\\lx@sectionsign}\\), Jiajie Zhang\\({}^{\\dagger\\dagger}\\), Yuze He\\({}^{\\dagger}\\), Ji Qi\\({}^{\\dagger\\dagger}\\)\n' +
      '\n' +
      '*Lei Hou\\({}^{\\ddagger}\\), Jie Tang\\({}^{\\ddagger}\\), Yuxiao Dong\\({}^{\\ddagger}\\), Juanzi Li\\({}^{\\ddagger}\\)**\n' +
      '\n' +
      '싱화대학교 ({}^{\\lx@sectionsign}\\)Zhipu.AI\n' +
      '\n' +
      'YB, JZ, JQ가 Zhipu.AI에서 인턴을 할 때 작업을 수행했습니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '긴 컨텍스트를 효과적으로 처리하기 위해 큰 언어 모델을 확장하려면 비슷한 길이의 입력 시퀀스에 대한 세부 조정이 필요하다. 이를 해결하기 위해 긴 컨텍스트 정렬을 위한 명령어 데이터, 훈련 및 평가의 레시피인 LongAlign을 제시한다. 먼저, Self-Instruct를 이용하여 long instruction-following dataset을 구성한다. 데이터 다양성을 보장하기 위해 다양한 긴 컨텍스트 소스에서 광범위한 작업을 다룬다. 둘째, 다양한 길이 분포를 가진 데이터에 대한 감독 미세 조정을 가속화하기 위해 패킹 및 정렬 배치 전략을 채택한다. 또한, 패킹 훈련 동안 서로 다른 시퀀스에 걸쳐 손실에 대한 기여도의 균형을 맞추기 위해 손실 가중치 방법을 개발한다. 셋째, 10k-100k 길이의 질의어에 대한 명령어 추종 능력을 평가하기 위한 LongBench-Chat 벤치마크를 소개한다. 실험은 LongAlign이 긴 컨텍스트 작업에서 LLM에 대한 기존 레시피를 최대 30%까지 능가하는 동시에 짧고 일반적인 작업을 처리하는 능숙함을 유지한다는 것을 보여준다. 코드, 데이터 및 긴 정렬 모델은 [https://github.com/THUDM/LongAlign](https://github.com/THUDM/LongAlign)에서 오픈 소스된다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '큰 컨텍스트 윈도우를 갖는 큰 언어 모델(LLM)은 요약, 긴 텍스트 및 코드에 대한 질문 응답과 같은 작업을 용이하게 한다(Bai et al., 2023). 중요하게는, 이들은 평생 대화 및 복잡한 에이전트 시나리오에 대한 기초 지원을 형성할 수 있다(Xiao et al., 2023; Liu et al., 2023). Long-context LLM들을 구축하기 위한 기존의 작업들은 주로 컨텍스트 확장(Chen et al., 2023; Xiong et al., 2023; Peng et al., 2023), 즉, 포지션 인코딩 확장 및 롱 텍스트에 대한 지속적인 트레이닝에 초점을 맞춘다.\n' +
      '\n' +
      '이 작업에서는 대신 긴 컨텍스트 정렬의 관점, 즉 긴 사용자 프롬프트를 처리하도록 LLM을 미세 조정하도록 지시하는 데 중점을 둔다. 그러나 해결하기 위해서는 몇 가지 과제가 필요하다. 첫째, 지도 미세 조정(Supervised Fine-tuning, SFT)을 위한 긴 명령어 추적 데이터 세트가 없고, 더 나아가 이러한 데이터를 구성하기 위한 방법이 부족하다. 둘째, 긴 컨텍스트 데이터의 다양한 길이 분포는 다중 GPU 설정에서 전통적인 배치 방법의 훈련 효율을 크게 감소시키는데, 이는 GPU가 더 긴 입력을 처리하는 것이 작업을 완료할 때까지 더 짧은 입력을 처리하는 것이 유휴 상태를 유지해야 하기 때문이다. 셋째, 실제 질의에 대한 LLM의 긴 컨텍스트 용량을 평가하기 위한 강력한 벤치마크가 필요하다.\n' +
      '\n' +
      '이를 해결하기 위해 데이터, 효율적인 교육 및 평가를 포함하는 **LongAlign** 레시피를 각각 제시한다. _ Data-wise_, 다양한 long instruction-following dataset을 구성하기 위해 9개의 소스로부터 long sequence를 수집하고 Self-Instruct(Wang et al., 2022)를 이용하여 8k-64k 길이의 10k instruction data를 생성한다.\n' +
      '\n' +
      '고르지 못한 배치하에서의 비효율성을 해결하기 위해, 우리는 불균일한 배치하에서의 비효율성을 해결하기 위해 시퀀스들을 패킹하는 패킹 전략(Krell et al., 2021)을 채택한다.\n' +
      '\n' +
      '그림 1: 길이1에서 10k-100k의 실제 질의를 포함하는 LongBench-Chat에 대한 테스트 결과.\n' +
      '\n' +
      'GPU에 배포하기 전에 최대 길이까지 함께 사용할 수 있습니다. 그러나 다른 수의 서열을 포함하는 팩이 최종 손실 계산에서 동일한 가중치가 할당되기 때문에 이 패킹 훈련 동안 손실 평균의 편향을 확인했다. 이러한 편향을 완화하기 위해 서로 다른 시퀀스에 걸쳐 손실에 대한 기여의 균형을 맞추기 위한 손실 가중치 전략을 제안한다. 또한, 배치 내 유휴 시간을 줄이기 위해 유사한 길이의 시퀀스를 그룹화하는 정렬된 배치 방법을 소개한다.\n' +
      '\n' +
      '_평가-wise_, 우리는 박사학위 학생들이 주석을 달 수 있는 10k-100k 길이의 개방형 질문을 절충하는 벤치마크인 LongBench-Chat를 개발한다. 그것은 긴 문맥에 대한 추론, 코딩, 요약, 다국어 번역과 같은 수업 추적 능력의 다양한 측면을 다룬다. GPT-4(OpenAI, 2023b)는 주석이 달린 지상 진실과 몇 개의 샷 채점 사례를 기반으로 기계 생성 응답을 채점하는 데 사용된다.\n' +
      '\n' +
      '광범위한 실험을 통해 LongAlign이 모델을 효과적으로 정렬하여 최대 64k 토큰 길이의 컨텍스트를 처리하면서도 일반 태스크의 성능을 저하 없이 유지할 수 있음을 보인다. 또한 다음과 같은 결과를 얻었다.\n' +
      '\n' +
      '데이터 양과 다양성의 영향**: 긴 명령 데이터의 양과 다양성 모두 정렬된 모델의 긴 컨텍스트 처리 능력에 상당한 영향을 미쳐 최종 성능에 최대 30%까지 영향을 미친다.\n' +
      '장문 데이터의 이점**: 장문 데이터의 양은 장문 태스크의 성능에 긍정적인 영향을 미치지만 모델의 단문 처리 능력에는 영향을 미치지 않는다.\n' +
      '훈련 전략의 효과**: 채택된 패킹 및 정렬된 배치 전략은 성능 손상 없이 훈련을 100% 이상 가속화할 수 있다. 또한, 제안된 손실 가중 기법은 긴 컨텍스트 성능을 10% 향상시킨다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**긴 컨텍스트 스케일링** 긴 컨텍스트 스케일링은 긴 컨텍스트 태스크들을 지원하기 위해 기존의 LLM들의 제한된 컨텍스트 길이를 확장하는 것을 목표로 한다(Xiong et al., 2023). 긴 컨텍스트 스케일링을 위한 현재 방법은 더 긴 시퀀스에 대해 미세 조정 또는 지속적인 훈련을 필요로 하는 방법과 그렇지 않은 방법의 두 가지 범주로 나눌 수 있다. 미세 조정을 필요로 하지 않는 방법들은 종종 슬라이딩 윈도우 어텐션(Han et al., 2023; Xiao et al., 2023) 또는 이웃 토큰 압축(Jiang et al., 2023; Zhang et al., 2024; Jin et al., 2024)과 같은 기법들을 채용하여 긴 컨텍스트들에 대한 어텐션 계산에서 위치 O.O.D. 문제를 처리한다. 이러한 방법은 플러그 앤 플레이 방식으로 LLM의 컨텍스트 길이를 확장할 수 있지만 여전히 미세 조정된 접근법의 성능과 일치할 수 없다. 긴 컨텍스트 스케일링을 위한 두드러진 미세 조정 접근법들(Chen et al., 2023; Peng et al., 2023; Xiong et al., 2023; Chen et al., 2023; Zhu et al., 2023; Fu et al., 2023)은 전형적으로 더 긴 시퀀스들에 대한 위치 인코딩 확장 및 연속적인 프리트레이닝을 수반한다.\n' +
      '\n' +
      'LLM Alignment** 긴 컨텍스트 스케일링의 이전 단계들에 이어서, 또한 모델이 채팅 인터페이스에서 다양한 사용자 요청들과 상호작용할 수 있다는 것을 보장하기 위해 명령-후속 데이터와 정렬하는 것이 필수적이다(Wang et al., 2023). 종종 감독 미세-튜닝 또는 명령어-튜닝으로 지칭되는 이 단계는 짧은 컨텍스트 시나리오들에서 광범위하게 연구되었다(Wang et al., 2022; Taori et al., 2023; Wang et al., 2023; Tunstall et al., 2023). 그러나 긴 시퀀스의 도입은 데이터, 훈련 방법 및 정렬을 위한 평가 측면에서 독특한 과제를 제시한다. Xiong et al. (2023)은 짧은 명령어 데이터를 연결함으로써 긴 명령어 데이터를 생성하는 것을 제안하지만, 그들의 데이터세트 및 모델 가중치는 오픈소싱되지 않는다. 한편, Chen et al. (2023b)은 LongAlpaca-12k의 긴 수업 데이터를 사용가능하고 효율적인 미세 조정을 위해 LoRA (Hu et al., 2022)를 채용했지만, 데이터 및 훈련 방법론의 영향에 대한 심층적인 논의와 비교 분석이 부족하다. 본 연구는 광범위한 작업에서 데이터 튜닝, 훈련 방법 및 정렬된 모델을 평가하여 긴 컨텍스트에서 완전한 주의력으로 감독(전체 매개변수) 미세 조정을 위한 최적의 솔루션을 찾는 것을 목표로 한다.\n' +
      '\n' +
      '## 3 LongAlign\n' +
      '\n' +
      '이 절에서는 데이터 구축 프로세스, 훈련 방법 및 평가 벤치마크를 포함하는 LongAlign의 방법론에 대해 논의한다.\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      '대형 언어 모델은 고품질 명령어 쌍 \\(x\\) 및 응답 \\(y\\)(Ouyang et al., 2022; Chung et al., 2022)에서 지도 미세 조정을 통해 정렬을 학습할 수 있다. 훈련 중에, 명령어와 응답은 전형적으로 시퀀스 \\([x,y]\\)를 형성하기 위해 연결되고, 그 후 확률 \\(P_{\\pi}(y|x)\\을 최대화하기 위해 자동 회귀 언어 모델 \\(\\pi\\)을 통해 처리된다. 손실은 언어 모델링 손실과 유사하지만, \\(y\\)(타겟 토큰들)에서 토큰들과 연관된 손실만을 설명한다:\n' +
      '\n' +
      '[x,y]=-\\sum_{i=1}^{|y|}\\log P_{\\pi}(y_{i}\\,|\\,[x,y_{<i}]). \\tag{1}\\\n' +
      '\n' +
      '### Dataset Construction\n' +
      '\n' +
      '긴 명령어 데이터는 전형적으로, 자료에 기초한 요약, 추론 또는 컴퓨팅을 필요로 하는 태스크 질의를 수반하는, 책, 광범위한 문서, 또는 긴 코드와 같은 긴 컨텍스트 자료를 수반한다. 건축하는 동안 우리는 먼저 책, 백과사전, 학술 논문, 코드 등을 다루는 9가지 다양한 출처에서 긴 기사와 문서를 수집한다. 그런 다음 그림 2와 같이 주어진 긴 문맥에 따라 태스크와 답변을 생성하기 위해 Claude 2.1(Anthropic, 2023)을 사용한다. 생성된 태스크의 다양한 범위를 육성하기 위해 요약, 정보 추출, 추론 등의 태스크 유형 설명을 프롬프트에 통합한다. 이 방법론을 사용하여 10k 길이의 텍스트에 대한 작업과 답변을 만들어 총 10k개의 감독 데이터 인스턴스를 산출하며 그 중 10%는 중국어로 표시된다. 이들 데이터의 길이는 한자에 대한 더 높은 압축률로 인해 ChatGLM 토큰타이저(Zeng et al., 2023)에 의해 측정된 8k 내지 64k의 범위이다. 프롬프트 및 데이터 구성 과정에 대한 자세한 내용은 부록 A에서 확인할 수 있다.\n' +
      '\n' +
      '### 효율적인 롱컨텍스트 트레이닝\n' +
      '\n' +
      '모델이 SFT 이후 긴 텍스트와 짧은 텍스트(일반적 능력)를 모두 처리할 수 있는 능력을 유지하도록 하기 위해 긴 명령어 데이터를 훈련용 일반 명령어 데이터 세트와 혼합한다. 비교적 적은 양의 긴 명령 데이터를 갖는 많은 양의 일반 짧은 데이터의 혼합물은 롱-테일 데이터 길이 분포를 초래한다. 그림 3에서 보는 바와 같이, 대부분의 데이터는 0-8k 길이 범위에 속하는 반면, 나머지 데이터는 8k-64k 길이 구간에 상당히 고르게 분포되어 있다. 이 분포에서 훈련 중에 데이터 배치에는 일반적으로 대부분 짧은 데이터가 포함되어 있지만 이러한 배치에는 훨씬 더 많은 계산 시간이 필요한 몇 개의 더 긴 텍스트도 포함되어 있어 상당한 유휴 시간이 발생한다. 이러한 유휴 시간을 최소화하기 위해, 가장 효과적인 접근법은 각 배치 내에서 보다 균일한 길이와 계산 시간을 보장하는 방식으로 데이터를 연결하거나 정렬하는 것이다. 이를 염두에 두고 우리는 포장 및 정렬된 배치의 두 가지 훈련 방법을 탐구한다.\n' +
      '\n' +
      '짐 싸고 있어 그것은 최대 길이에 도달할 때까지 다양한 길이의 데이터를 함께 연결하는 것을 포함한다. 그 후, 길이가 일반적으로 최대 길이에 가까운 결과 패킹된 데이터는 멀티-GPU에서 배치 및 처리된다. 이 접근법은, 도 3의 우측 상단에 묘사된 바와 같이, 각각의 배치 내의 유휴 시간을 효과적으로 최소화한다. 추가적으로, 자기-어텐션 계산 동안 동일한 팩 내의 상이한 시퀀스들 간의 교차-오염을 방지하기 위해, 상이한 시퀀스들의 시작 및 종료 위치들을 포함하는 리스트를 전달하고 플래시 어텐션 2로부터의 플래시_attn_varlen_func을 활용한다(Dao et al., 2022; Dao, 2023). 이는 블록 대각 어텐션의 효율적인 계산을 지원한다(보다 상세한 내용은 부록 B를 참조). 기존의 2D 어텐션 마스크 사용에 비해 계산량과 IO 시간이 적게 소요된다.\n' +
      '\n' +
      '그러나 패킹 전략이 더 긴 시퀀스 및 더 많은 타겟 토큰을 포함하는 시퀀스에 대한 편향을 초래한다는 것을 알 수 있다. 이는 각각 최종 손실에 동등하게 기여하는 상이한 팩들이 상이한 수의 타겟 토큰들을 갖는 상이한 수의 시퀀스들을 포함하기 때문이다. 결과적으로, 각 배치에 대한 평균 손실을 계산할 때, 더 적은 시퀀스(일반적으로 더 긴 시퀀스)를 갖는 팩 내의 시퀀스 또는 더 많은 타겟 토큰을 포함하는 시퀀스는 최종 손실에 더 큰 영향을 미친다. 형식적으로는 \\(P_{i-1},P_{i})\\의 지수와 함께 \\(P_{0}=1,P_{K}=M+1\\)으로 구성된 \\(K\\) 팩에 패킹된 \\(M\\) 시퀀스를 고려한다. \\(L_{i}\\)는 \\(i\\)번째 시퀀스에서 \\(N_{i}\\) 타겟 토큰에 대한 손실의 총 합을 나타낸다. 각자 무게를 잰다면\n' +
      '\n' +
      '도 2 : 데이터 구축예.\n' +
      '\n' +
      '똑같이, 손실이 있어야 한다.\n' +
      '\n' +
      '\\[\\mathcal{L}=\\frac{1}{M}\\sum_{i=1}^{M}\\frac{L_{i}}{N_{i}}, \\tag{2}\\]\n' +
      '\n' +
      '상기 패킹 하에서 계산된 손실은,\n' +
      '\n' +
      '\\[\\mathcal{L}^{\\prime}=\\frac{1}{K}\\sum_{k=1}^{K}(\\sum_{i=P_{k-1}}}^{P_{k}-1}L_{i}/\\sum_{i=P_{k-1}}}^{P_{k}-1}N_{i}\\neq\\mathcal{L}. \\tag{3}\\tag{3}\\tag{3}}}\n' +
      '\n' +
      'Eq와 비교하여. 2에서, 이것은 손실에서 수열\\(j\\)에 \\((N_{j}/\\sum_{i=P_{k-1}}^{k-1}N_{i})\\)의 가중치를 할당하는 것과 동일하며, 즉 더 많은 타겟 토큰이 있는 수열과 더 작은 팩에서 수열을 선호한다. 이 부등식을 해결하기 위해, 우리는 \\(i\\)번째 시퀀스에서 손실을 \\(K/(N_{i}M)\\)으로 스케일링하고 대신 각 팩에서 스케일링된 손실의 합을 취하여 Eq와 동일한 손실을 초래한다. 2:\n' +
      '\n' +
      '[\\mathcal{L}^{\\prime}=\\frac{1}{K}\\sum_{k=1}^{K}(\\sum_{i=P_{k-1}}^{P_{k}-1}\\frac{L_{i}K}{N_{i}M})=\\frac{1}{K}\\sum_{i=1}^{M}\\frac{L_{i}K}{N_{i}M}=\\mathcal{L}. \\tag{4}\\frac{L}{i}\\frac{L}{i}\\frac{K}\\sum_{i}}{N_{i}M}=\\mathcal{L}.\n' +
      '\n' +
      '우리의 실험 섹션에서 입증된 바와 같이 손실 가중 전략은 다운스트림 작업에서 10% 개선을 초래한다.\n' +
      '\n' +
      '분류된 배치야 우리는 또한 훈련을 위한 효율적인 정렬된 배치 전략을 고려한다(도 3의 우측 하단). 각 배치 내의 시퀀스가 유사한 길이인지 확인하기 위해 데이터를 길이별로 정렬하고 반복 없이 각 배치에 대해 무작위 연속 데이터 그룹을 선택한다. 그러나 이 전략은 배치가 모든 긴 서열 또는 모든 짧은 서열로 구성된 다른 배치에 걸친 데이터 분포의 편향을 필연적으로 도입한다. 이것은 SGD 최적화에 잠재적으로 재앙이 될 수 있다. 실험에서 정렬된 배치가 성능에 눈에 띄는 부정적인 영향 없이 프로세스를 상당히 가속화하는 것을 관찰한다. 이것은 큰 구배 축적 단계의 사용과 최적화기의 강력한 적응성에 기인할 수 있다.\n' +
      '\n' +
      '### LongBench-Chat\n' +
      '\n' +
      'LLMs의 긴 컨텍스트 이해도를 평가하기 위한 기존의 벤치마크(An et al., 2023; Bai et al., 2023; Li et al., 2023)가 있지만, 그들은 긴 컨텍스트 하에서 그들의 명령어 추종 능력을 평가하는 데 중점을 두지 않는다. 또한, 평가를 위한 자동 메트릭에 의존하는 것은 정렬된 모델의 더 길고 다양한 출력의 평가, 그리고 그들의 반응이 인간의 선호도와 어떻게 정렬되는지를 제한한다.\n' +
      '\n' +
      '이를 위해 LongBench-Chat(LongBench-Chat)을 제안한다. LongBench-Chat은 문서 QA, 요약, 코딩과 같은 다양한 주요 사용자 집약적 시나리오를 포함하는 10k에서 100k 길이의 50개의 긴 컨텍스트 실세계 질의를 포함한다. 영어로 40개, 중국어로 10개의 과제로 구성되어 있습니다. 평가가 긴 맥락 지시를 따르는 모델의 능력을 진정으로 반영하기 위해 사전 훈련 동안 모델에 의해 보고 암기되었을 가능성이 있는 인기 있는 긴 텍스트를 사용하는 것을 피한다. 우리는 또한 모델이 긴 텍스트를 읽지 않고 대답할 수 있는 질문을 제기하는 것을 피한다.\n' +
      '\n' +
      '평가를 위해 LLM을 평가자로 사용하는 효과를 보여준 이전 작업(Bai et al., 2023; Zheng et al., 2023; Ke et al., 2023)에 이어 GPT-4(OpenAI, 2023)를 사용하여 주어진 인간 주석이 달린 참조 답변과 각 질문에 대한 몇 개의 샷 채점 예를 기반으로 모델의 응답을 1-10으로 채점한다. 현재 긴 컨텍스트 입력 하에서 응답의 품질을 평가할 수 있는 모델이 없기 때문에 우리는 짧은 쿼리(긴 문서 없이)를 평가자에게 전달할 뿐이다. 평가자가 근거 진실 및 소수의 채점 사례만을 기반으로 정보에 입각한 판단을 내릴 수 있도록 하기 위해 "선행 텍스트를 기반으로 시를 작성"과 같은 지나치게 개방적인 질문을 피한다.\n' +
      '\n' +
      'LongBench-Chat에서 GPT-4를 평가자로 사용하는 것의 신뢰성을 검증하기 위해 hu를 수행한다.\n' +
      '\n' +
      '그림 3: 긴 꼬리 데이터 길이 분포 하에서, 패킹 또는 정렬된 배칭은 유휴 시간을 감소시키고 트레이닝 프로세스를 고속화할 수 있다. 패킹 중에 손실 가중은 시퀀스에 걸친 손실 기여도의 균형을 맞추기 위해 필요하다.\n' +
      '\n' +
      '남성 평가 연구(부록 C에 더 자세한 내용) <표 1>에서 GPT-4의 평가는 참조된 답변만을 포함하는 제로샷 프롬프트(zero-shot prompting)를 사용한 평가와 추가 소수의 샷 스코어링 예제를 사용한 평가 사이의 상관 관계를 크라우드소싱된 인간 판단과 비교하여 제시하고, 첫 번째 열에서 주석자 간 상관 관계를 보여주며, GPT-4의 인간 주석과의 상관 관계는 일치할 뿐만 아니라 인간 주석자 간의 일치 수준을 능가하여 LongBench-Chat에서 이러한 메트릭의 신뢰성을 증명한다. 우리는 GPT-4+_Few-shot_를 사용하여 얻은 전체 평균 점수(1-10)가 인간 전문가가 제공한 점수와 평균 0.1 이하 차이가 있음을 추가로 발견한다. 또한, 우리는 응답 길이에 대한 GPT-4의 점수에서 상당한 편향을 관찰하지 못하며, 실제로 지나치게 긴 응답에도 불이익을 준다.\n' +
      '\n' +
      '리더보드! 그림 1은 현재 긴 컨텍스트(16k+) 명령어 미세 조정 모델(채팅 모델) 및 LongBench-Chat에서 LongAlign으로 훈련된 가장 유능한 모델의 테스트 결과를 보고한다. API 기반 상용 모델들: GPT-4-1106-preview(OpenAI, 2023a)(GPT-4 Turbo), GLM-4-128k2, 및 Claude-2.1(Anthropic, 2023); 뿐만 아니라 오픈 소스 모델들: InternLM2-7b-200k, InternLM2-20b-200k(Team, 2023), ChatGLM3-6B-32k(Du et al., 2022; Zeng et al., 2023), Vicuna-7b-v1.5-16k(Zheng et al., 2023), Orion-14b-LongChat(Chen et al., 2024), LongChat-7b-v1.5-32k(Li et al., 2023a), 및 Mixtral-8x7b-Instruct-v0.2(Jiang et al., 2024). 모델의 컨텍스트 창을 초과하는 입력에 대해 중간 절단을 사용합니다. 우리의 평가 결과는 현재 오픈 소스 모델의 성능이 여전히 상업 모델에 비해 상당히 뒤처져 있음을 보여주며, 이는 부분적으로 이러한 모델 간의 규모 차이에 기인한다. 또한, 컨텍스트 길이가 32k 이하인 모델은 LongBench-Chat에서 성능이 떨어지는 경향이 있으며, 이는 이러한 긴 작업을 완료하기 위해 더 긴 컨텍스트 창이 필요함을 나타낸다.\n' +
      '\n' +
      '각주 2: [https://open.bigmodel.cn/pricing](https://open.bigmodel.cn/pricing)\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '이 절에서는 일련의 실험을 통해 다음과 같은 연구 질문에 답하는 것을 목표로 한다. **RQ1**. SFT 동안, 긴 명령어 데이터의 양과 다양성은 다운스트림 태스크에서 모델의 성능에 어떻게 영향을 미치는가?\n' +
      '\n' +
      'RQ2** 훈련 동안 긴 명령어 데이터를 통합하는 것이 짧은 컨텍스트 시나리오에서 모델의 일반적인 능력 및 그들의 명령어-추종/대화 능력에 영향을 미치는지 여부.\n' +
      '\n' +
      'RQ3** 패킹 및 정렬 배칭 훈련 방법이 훈련 효율성과 모델의 최종 성능에 미치는 영향.\n' +
      '\n' +
      '또한 모델 크기와 컨텍스트 길이에 대한 LongAlign의 확장성 및 긴 컨텍스트 정렬에 대한 학습 곡선에 대한 논의를 통합한다.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '**데이터** 모델의 일반적인 능력과 짧은 명령어들에 대한 숙련도를 유지하기 위해, 우리는 ShareGPT (Chiang et al., 2023) (빈 어시스턴트 응답은 필터링됨)를 우리의 트레이닝 데이터에서 짧은 명령어 데이터의 소스로서 활용한다. 모델 학습에 대한 긴 수업 데이터의 다양한 측면의 영향을 비교하기 위해, 우리는 다음 네 가지 긴 수업 데이터 스위트를 실험에 통합한다. Sec 3.2에서의 절차에 따라 구축된 LongAlign 데이터의 \'_LongAlign-0k_\', \'_LongAlign-5k_\', 및 \'_LongAlign-10k_\': 0, 5k, 및 10k 인스턴스; LongAlpaca 데이터세트로부터의 \'_LongAlpaca-12k_\': 12k 데이터(Chen et al., 2023b). LongAlpaca는 9k 길이의 QA 데이터와 3k 길이의 QA 데이터를 포함하며, 여기서 긴 QA 데이터는 학술 논문과 책에만 기초하여 생성되므로 LongAlign 데이터에 비해 더 적은 다양성을 제공한다. 이 데이터 세트를 사용하여 긴 지시 데이터의 다양성이 모델 학습에 미치는 영향을 비교한다.\n' +
      '\n' +
      '모델 우리는 ChatGLM3-6B(Du et al., 2022; Zeng et al., 2023), Llama-2-7B, 및 Llama-2-13B(Touvron et al., 2023)(모든 기본 모델)의 세 가지 모델 변형을 포함한다. 8k 및 4k 컨텍스트 창이 주어지면 먼저 컨텍스트 확장을 수행하여 컨텍스트 창을 64k로 확장하면 ChatGLM3-6B-64k, Llama-2-7B-64k 및 Llama-2-13B-64k가 생성된다. 이것은 RoPE 포지션 인코딩(Su et al., 2024)의 기본 주파수 \\(b\\)를 200배(10,000에서 2,000,000으로) 확장하고 총 100억 토큰3에 대해 길이가 64k 미만인 사전 트레이닝 데이터에 대한 지속적인 트레이닝을 포함한다.\n' +
      '\n' +
      '각주 3: Fu 등(2023)에서 제안된 바와 같이, 10B 토큰에 대한 연속 트레이닝은 컨텍스트 확장에 충분하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & Human & GPT-4 & GPT-4+_Few-shot_ \\\\ \\hline Spearman (\\(\\rho\\)) & 0.817 & 0.788 & **0.844** \\\\ Kendall (\\(\\tau\\)) & 0.694 & 0.656 & **0.716** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 주석자 간 상관관계; GPT-4(w/ 및 w/o Few-shot)와 인간 간의 상관관계.\n' +
      '\n' +
      '훈련 모든 모델은 8xA800 80G GPU 및 DeepSpeed+ZeRO3+CPU 오프로딩으로 트레이닝된다(Rasley et al., 2020). 모델들은 GPU 메모리 오버플로우 없이 64k 토큰들의 최대 길이로 트레이닝될 수 있다. 결과적으로, 우리는 훈련 데이터의 최대 길이를 64k로 설정했으며, 이 길이를 초과하는 데이터는 오른쪽에서 잘렸다. 패킹 훈련의 경우, 각 팩은 평균 12개의 시퀀스로 구성되어 있으며, 총 배치 크기를 8로 설정하여 전체 배치 크기를 96으로 하고, 공정한 비교를 위해 배치 크기를 8로 설정하고, 다른 비포장 훈련 방법의 경우 구배 누적 단계를 12로 한다. 우리는 훈련 데이터(약 1500-2000 단계)에서 2개의 에포크를 훈련한다.\n' +
      '\n' +
      '평가** 우리는 긴 컨텍스트 태스크와 짧은 컨텍스트 태스크를 모두 평가에 포함한다. 긴 시나리오와 짧은 시나리오 모두에서 우리는 교수-추종 능력과 대화 능력을 평가하는 작업과 일반적인 능력을 평가하는 작업을 고려한다. 본 논문에서 제안하는 LongBench-Chat을 사용하여 긴 문맥 정렬 능력을 평가하고 LongBench(Bai et al., 2023)를 사용하여 일반적인 긴 문맥 이해 능력을 테스트한다. LongBench는 이중언어, 다중작업의 긴 문맥 벤치마크이며, 단일-Doc QA, Multi-Doc QA, 요약의 세 가지 유형의 작업에 대해 평가를 수행한다. 정렬된 모델은 일반적으로 더 긴 응답을 생성하므로, 원래 메트릭(ROUGE, F1)을 사용하여 모델의 응답을 채점하는 대신 GPT-4를 사용하여 LongBench에 대한 기초 응답과의 정렬을 기반으로 모델의 출력을 평가한다. 짧은 컨텍스트 태스크의 경우, 우리는 짧은 지시를 따르는 모델들의 능력을 측정하기 위해 다중 턴 채팅 벤치마크인 MT-벤치(Zheng et al., 2023)를 사용한다. 또한 ARC(Clark et al., 2018), HellaSwag(Zellers et al., 2019), Truthful QA(Lin et al., 2022) 및 MMLU(Hendrycks et al., 2021)를 포함한 Open LLM Leaderboard(Beeching et al., 2023)의 일반적인 작업에 대해 평가한다. 우리는 Open LLM Leaderboard의 평가 설정을 따르고 이러한 작업에 대한 평가를 위해 lm-평가-harness 프레임워크(Gao et al., 2023)를 활용한다. 가장 안정적인 평가 결과를 보장하기 위해 GPT-4를 사용하여 LongBench-Chat 및 MT-Bench에서 두 번 점수를 매기고 이 점수를 평균하여 최종 점수를 얻는다.\n' +
      '\n' +
      '### 데이터의 영향\n' +
      '\n' +
      'ShareGPT 데이터를 이용하여 ChatGLM3-6B-64k에서 SFT를 수행한다. _LongAlign-Ok_를 제외한 모든 모델들은 손실 가중과 함께 보다 효율적인 패킹 전략을 사용하여 트레이닝된다. 평가 결과는 표 2에 보고되어 있다. LongBench-Chat 및 MT-Bench의 경우, 보고된 결과는 모든 테스트 인스턴스에 걸쳐 GPT-4의 등급(1-10)에 대해 평균화되고, 다른 데이터 세트에 대한 결과는 0-100 사이에서 정규화된다. 또한, "Needle in A HayStack" 실험4(결과 시각화 도 4)를 수행하여, 길이가 1k-60k 사이에서 변화하는 긴 컨텍스트 내에서 10개의 상이한 위치로부터의 정보를 활용하는 모델의 능력을 테스트한다. 구체적으로, 이 작업은 긴 컨텍스트 윈도우의 중간(지정된 깊이 퍼센트에 위치됨)에 삽입된 사실(\'needle\')을 모델이 검색하도록 요청한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c|c c c c} \\hline \\hline\n' +
      '** 훈련 데이터** &\\멀티컬럼{4}{c|}{** 긴 작업**} &\\멀티컬럼{4}{c}{** 짧은 작업**} \\\\\\cline{2-10}\n' +
      '**(Long)** & **LongBench-Chat** & **S-Doc QA** & **M-Doc QA** & **Summ** & **MT-Bench** & **ARC** & **HellaSwag** & **TruthfulQA** & **MMLU** \\\\ \\hline _LongAlign-Ok_ & 3.73 & 58.7 & 41.1 & 38.4 & 5.34 & 50.3 & 74.7 & 51.6 & 45.5 \\\\ _LongAlign-Sk_ & 5.97 & 61.8 & 42.1 & 42.0 & 5.51 & 50.3 & 75.1 & 52.5 & 46.6 \\\\ _LongAlign-10k_ & **6.21** & 64.0 & 44.4 & 44.2 & 5.5 & 50.5 & 74.9 & 52.5 & 45.5 \\\\ _LongAlpaca-12k_ & 4.46 & 65.8 & 45.6 & 44.1 & 4.93 & 51.5 & 75.4 & 53.2 & 47.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 상이한 양 및 유형의 긴 명령 데이터에 대한 트레이닝 후의 ChatGLM3-6B-64k의 성능.\n' +
      '\n' +
      '그림 4: ShareGPT와 혼합된 긴 데이터의 다른 스위트에 대해 훈련된 ChatGLM3-6B-64k의 1k-60k 바늘 테스트 성능.\n' +
      '\n' +
      '(\'헤이스택\'). 우리는 데이터의 영향에 대한 주요 연구 결과를 다음과 같이 요약한다.\n' +
      '\n' +
      '**1. 더 긴 명령어 데이터는 긴 태스크에서의 성능을 향상시키고, 짧은 태스크에서의 성능을 손상시키지 않는다**. _LongAlign-0k_, _LongAlign-5k_ 및 _LongAlign-10k_의 성능을 비교하면, 긴 명령어 데이터의 양이 증가함에 따라 모든 긴 태스크에 걸쳐 모델의 성능이 일관되게 향상됨을 관찰한다. 한편, 흥미롭게도 짧은 작업에 대한 성능은 짧은 지시만으로 훈련될 때와 비교할 수 있다. 추가적으로, 긴 태스크들(특히 LongBench-Chat 상에서)에서 _LongAlign-0k_의 열등한 성능을 고려할 때, 이는 또한 단지 베이스 모델 상에서 컨텍스트 확장을 수행하는 것이 다운스트림 긴 태스크들에서 양호한 성능을 보장하기에 불충분하다는 것을 나타낸다. SFT 동안 다양한 길이를 포함하는 상당한 양의 긴 데이터를 통합할 필요가 있다. 또한 니들 테스트 결과는 더 긴 데이터가 긴 텍스트 내에서 서로 다른 위치의 정보를 활용하는 모델의 능력을 향상시켜 모델의 검색 오류를 감소시킨다는 것을 시사한다.\n' +
      '\n' +
      '긴 명령어 데이터의 다양성은 모델의 명령어 추종 능력에 유익하다. _ LongAlign-10k_는 _LongAlpaca-12k_에 비해 길고 짧은 명령어 후속 작업(LongBench-Chat 및 MTBench)에서 훨씬 더 나은 결과를 보여준다. 한편, _LongAlpaca-12k_는 LongBench에서 _LongAlign-10k_보다 약간 우수하다. 이는 주로 2WikiMQA(Ho et al., 2020) 및 NarrativeQA(Kocisky et al., 2018) 데이터 세트에서 우수한 성능으로 인해 위키피디아와 소설을 기반으로 하며 LongAlpaca-12k의 명령어 데이터 소스와 더 유사하다.\n' +
      '\n' +
      '### 훈련방법의 영향\n' +
      '\n' +
      '본 논문에서는 ChatGLM3-6B-64k와 Llama-2-6B-64k의 학습 방법을 비교하였으며, 학습 효율에 미치는 영향과 다운스트림 태스크 성능에 미치는 영향을 평가하였다. 5개의 모델은 모두 _LongAlign-10k_로 학습되었다. 그림 5는 각 방법에 필요한 훈련 시간을 비교한 것이다. <표 3>은 하류 과제에 대한 성과를 제시하고 있다. 본 연구의 결과는 다음과 같다.\n' +
      '\n' +
      '각주 5: 나이브 배칭 및 정렬된 배칭은 기울기 축적의 사용으로 인해 패킹에 비해 GPU 메모리를 더 많이 소비한다. GPU 메모리 오버플로가 발생하지 않도록 이 두 가지 방법으로 ChatGLM의 모든 데이터를 56k 길이로 잘라낸다.\n' +
      '\n' +
      '**1. 포장 및 정렬 배칭은 우수한 성능을 발휘하면서 훈련 효율을 배가시킨다**. 그림 5에서 패킹 및 정렬된 배치의 훈련 효율이 비슷하다는 것을 알 수 있으며, 둘 다 순진한 배치에서 필요한 시간의 절반 미만을 필요로 한다. 또한 표 3에 따르면 두 가지 효율적인 방법으로 훈련된 모델은 긴 작업과 짧은 작업 모두에서 순진한 배치로 훈련된 모델과 비교할 수 있다. 우리는 또한 이 두 훈련 방법의 효과가 모델에 따라 다르다는 것을 발견했다.\n' +
      '\n' +
      '도 5: 상이한 트레이닝 방법 하에서 8xA800 80G GPU들에 대한 트레이닝 시간(hrs)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c c c c} \\hline \\hline \\multirow{2}{*}{**Training Method**} & \\multicolumn{4}{c|}{**Long Tasks**} & \\multicolumn{4}{c}{**Short Tasks**} \\\\ \\cline{2-10}  & **LongBench-Chat** & **S-Doc QA** & **M-Doc QA** & **Summ** & **MT-Bench** & **ARC** & **HellaSwag** & **TruthfulQA** & **MMLU** \\\\ \\hline _ChatGLM3-6B-64k_ & & & & & & & & & \\\\ Nalve batching & 5.87 & 65.4 & 45.0 & 44.8 & 5.61 & 50.7 & 74.7 & 52.8 & 46.0 \\\\ Sorted batching & 5.4 & 66.2 & 46.3 & 43.7 & 5.76 & 51.3 & 74.8 & 51.9 & 46.3 \\\\ Packing & 5.76 & 65.0 & 45.1 & 42.8 & 5.64 & 50.9 & 74.8 & 50.5 & 47.2 \\\\ Packing-loss weighting & 6.21 & 64.0 & 44.4 & 44.2 & 5.5 & 50.5 & 74.9 & 52.5 & 45.5 \\\\ \\hline _Llama-2-7B-64k_ & & & & & & & & & \\\\ Nalve batching & 5.95 & 62.8 & 42.7 & 41.6 & 5.52 & 48.9 & 74.8 & 45.3 & 43.6 \\\\ Sorted batching & 6.38 & 63.4 & 42.2 & 41.3 & 5.51 & 49.5 & 74.8 & 48.0 & 44.3 \\\\ Packing & 5.89 & 61.7 & 40.4 & 42.0 & 5.58 & 48.1 & 74.9 & 46.1 & 43.9 \\\\ Packing-loss weighting & 6.10 & 60.8 & 41.3 & 43.1 & 5.60 & 48.4 & 74.5 & 47.4 & 43.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 상이한 훈련 방법 하에서 ChatGLM3-6B-64k 및 Llama-2-7B-64k의 성능.\n' +
      '\n' +
      '예를 들어 패킹+손실 가중치를 사용하여 ChatGLM3-6B에서 훈련된 모델은 LongBench-Chat에서 훨씬 더 나은 성능을 나타내는 반면 정렬된 배칭은 Llama-2-7B에서 가장 좋은 성능을 나타낸다.\n' +
      '\n' +
      '**2. 손실 가중치는 패킹 트레이닝을 위한 긴 지시 태스크에 대한 성능을 상당히 향상시킨다**. 패킹 훈련 시 손실 가중 전략이 있는 모델과 없는 모델의 성능을 비교함으로써 손실 가중 전략을 통합하면 LongBench-Chat(약 5%\\(\\sim\\)10%)에서 성능이 크게 향상되지만 다른 작업의 성능에 최소 및 가변적인 영향을 미친다는 것을 알 수 있다. 우리는 이것이 주로 SFT 데이터에서 손실 가중치 없이, 상이한 긴 명령 데이터가 손실에 가변적으로 기여하기 때문이라고 믿는다 - 더 긴 데이터는 손실에 더 많이 기여하는 경향이 있다(식 3 참조). 이러한 부자연스러운 가중 편향은 종종 모델 훈련에 해로우며, 잠재적으로 훈련 불안정성을 초래하여 최적의 학습 궤적에서 이탈한다.\n' +
      '\n' +
      '### Discussion\n' +
      '\n' +
      '**LongAlign**의 확장성 우리는 LongAlign 프레임워크에서 **더 큰 모델 크기**와 **더 긴 컨텍스트 창**의 두 가지 스케일링 방향을 탐색한다. 이를 위해 두 가지 효율적인 훈련 방법으로 _LongAlign-10k_ dataset을 사용하여 Llama-2-13B-64k를 미세 조정하며, 평가 결과는 표 4와 같다. 7B 척도 모델에 비해 13B 모델은 LongBench-Chat에서 10%의 개선을 보여 오픈소싱 모델(도 1의 LongAlign-13B-64k) 중 새로운 기록을 세웠다. 이것은 우리의 정렬 방법이 대규모 모델에 효과적으로 확장된다는 것을 나타낸다. 또한 인간 주석으로 최대 128k 길이의 SFT 데이터를 구성하고 손실 가중이 있는 패킹 훈련을 사용하여 128k 컨텍스트 윈도우에서 ChatGLM3-6B를 성공적으로 정렬하여 ChatGLM3-6B-128k(그림 1에 표시된 성능)를 생성했다.\n' +
      '\n' +
      '**긴 작업 대 짧은 작업의 곡선 학습** 긴 문맥과 짧은 문맥에서 정렬의 학습 과정을 비교하기 위해 모델 학습 중 긴 및 짧은 명령어 후속 작업(LongBench-Chat 및 MT-Bench에서 각각)에 대한 상대적 성능 곡선을 그림 6에 제시하여 학습 단계의 수에 따라 성능이 어떻게 달라지는지 보여준다. 우리는 지수 이동 평균을 사용하여 원래의 성능 곡선(점선)을 평활화하고 실선으로 표시한다. 우리는 두 학습 곡선의 경향이 놀라울 정도로 유사하다는 것을 관찰한다. 둘 다 0-500 단계 사이에서 빠른 개선을 보이고, 그 다음에는 느린 상승이 뒤따르고, 1000 단계 후에 안정화된다. 이것은 긴 정렬과 짧은 정렬 사이의 더 깊은 연결을 의미할 수 있다. 그들은 공유 잠재 요인에 의해 공동으로 결정될 수 있으며, 이는 모델이 긴 지침과 짧은 지침 모두에 동시에 정렬되도록 훈련 중에 최적화된다.\n' +
      '\n' +
      '부록 D에서는 다양한 LongAlign-tuned model on out-of-distribution(OOD) 긴 컨텍스트 질의, 즉 긴 컨텍스트 SFT 데이터에서 모델들이 접하지 못한 질의에 대한 사례 분석을 제공한다. LongAlign으로 훈련된 모델은 연구 논문에 대한 리뷰를 작성하는 것과 같은 OOD 긴 컨텍스트 쿼리로 일반화할 수 있으며 대규모 모델은 더 강력한 일반화 기능을 가지고 있음을 발견했다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문은 데이터의 범위, 훈련 방법, 평가의 범위에서 긴 맥락 정렬을 위한 모범 사례를 찾는 것을 목표로 한다. 제안된 LongAlign은 Self-Instruct를 이용하여 다양한 긴 명령어 데이터를 구성하고, 손실 가중이나 정렬된 배칭과 함께 패킹으로 모델을 효율적으로 미세 조정한다. 또한 신뢰할 수 있도록 LongBench-Chat을 소개합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\hline \\hline _Llama-2-13B-64k_ & **LongBench-Chat** & **S-Doc QA** & **M-Doc QA** & **Summ** & **MT-Bench** \\\\ \\hline Packing+loss weighting & 6.79 & 68.0 & 40.3 & 43.6 & 6.12 \\\\ Sorted batching & 7.02 & 66.1 & 43.9 & 45.3 & 6.02 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 스케일링 업: LLama-2-13B 상의 LongAlign.\n' +
      '\n' +
      '그림 6: ChatGLM3-6B-64k의 훈련 과정 전반에 걸쳐 길고 짧은 작업에 대한 상대적 성능.\n' +
      '\n' +
      '실제적인 긴 맥락 상호작용에 대한 LLM의 지시-추종 능력에 대한 평가. 제어된 실험을 통해 데이터의 양과 다양성, 올바른 훈련 방법이 최종 성능에 중요하다는 것을 알 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: 긴 컨텍스트 언어 모델에 대한 표준화된 평가 기관. 외부 링크: 2308.14508 인용: SS1.\n' +
      '* Anthropic(2023) Anthropic: introduced claude 2.1. External Links: Link Cited by: SS1.\n' +
      '*Y. 배진 Lv, J. Zhang, H. Lyu, J. Tang, Z. 황진 듀, 엑스 류아정 허영 동, 제탕, 제리 2023a. 롱벤치: 긴 맥락 이해를 위한 이중언어, 멀티태스크 벤치마크. ArXiv:2308.14508. External Links:2308.14508 Cited by: SS1.\n' +
      '*Y. 배정영 조욱 엘브이 그, X 왕진유 정영 샤오, H. Lyu, et al. 2023b. 평가자로서의 언어 모델을 가진 벤치마킹 기반 모델. ArXiv:2306.04181. External Links:2306.04181 Cited by: SS1.\n' +
      '* E. Beeching, C. Fourrier, N. 하빕 한남 N. 램버트 오자니 산세비에로 툰스톨, T 울프(2023) 오픈 LLM 리더보드. 외부 링크: 인용된 링크: SS1입니다.\n' +
      '* D. Chen, Y. 황철 이영 이영 류현판 서덕장 장규 Han(2024)Orion-14b: 오픈소스 다국어 대용량 언어 모델. arXiv preprint arXiv:2401.12246. External Links:2401.12246 Cited by: SS1.\n' +
      '* S. 천성호 왕락 천영 Tian(2023a) 위치 보간을 통해 대형 언어 모델의 컨텍스트 윈도우를 확장한다. arXiv preprint arXiv:2306.15595. External Links:2306.15595 Cited by: SS1.\n' +
      '*Y. 천성호 기안홍당 라이자 류승 Han, and J. Jia(2023b)Longlora: efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307. External Links:2309.12307 Cited by: SS1.\n' +
      '*W. 장종 이종욱 임영식 성진 우현장 정승 장영 Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing(2023)Vicuna: 90%* chatgpt 품질의 gpt-4를 인상하는 오픈 소스 챗봇. 외부 링크: 2309.12307 인용: SS1.\n' +
      '*H.W.정룡 허승 롱프레, B. 조프, Y 테이원 페더스 리진 왕민 데하니, S. Brahma, et al.(2022)Scaling instruction-finetuned language models. ArXiv:2210.11416. 인용: SS1.\n' +
      '* P. Clark, I. Cowhey, O. 에치오니, T. Khot, A. Sabharwal, C. Schoenick, O. 타피오르(2018) 질의응답을 해결했다고 생각하십니까? try arc, the ai2 reasoning challenge. ARXiv 프리프린트 arXiv:1803.05457. External Links:1803.05457 Cited by: SS1.\n' +
      '*T. Dao(2023)FlashAttention-2: 더 나은 병렬성과 작업 분할로 더 빠른 주의력. 외부 링크: 2309.12307 인용: SS1.\n' +
      '*T. Dao(2023)FlashAttention-2: 더 나은 병렬성과 작업 분할로 더 빠른 주의력. 외부 링크: 2309.12307 인용: SS1.\n' +
      '*Z. 두영 기안 류민 딩종주 Yang, and J. Tang(2022)GIM: general language model preraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 320-335. External Links: 2309.12307 Cited by: SS1.\n' +
      '*Y. 후, 엑스 니우, 엑스 유룡 판다용 Kim, and H. Peng (2023) 이해 데이터는 컨텍스트 스케일링에 영향을 미친다. 야오푸의 노션 외부 링크: 2309.12307 인용: SS1.\n' +
      '* L. 가오성 비더만 블랙 L. 골딩, T 호피, C. 포스터, J. Phang, H. He, A. Thite, N. Nabeshima, et al. (2020)The pile: a 800gb dataset of various text for language modeling. ArXiv:2101.00027. 인용: SS1.\n' +
      '* L. 가오, J. Tow, B. Abbasi, S. 비더만 블랙, A. 디포피 C. 포스터, L. 골딩, J. Hsu, A. Le Noac\'h, H. Li, K. 맥도넬 Muennighoff, C. Ociepa, J. Phang, L. 레이놀즈, H. 쇤코프, A. 스코론, L. 수타위카 E. Tang A. Thite B. Wang K. Wang, and A. Zou (2023)A framework for few-shot language model evaluation. 외부 링크: 2309.12307 인용: SS1.\n' +
      '* C. Han, Q. 왕욱 시온 천현지, S. Wang (2023)Lm-infinite: 간단한 on-the-fly length 일반화를 위한 대규모 언어 모델. ArXiv:2308.16137. External Links:2308.16137 Cited by: SS1.\n' +
      '* D. 헨드릭스, C. 번즈, S. 바사르트, A. 주 Mazeika, D. Song, J. Steinhardt (2021)는 대규모 멀티태스크 언어 이해도를 측정한다. 학습 표상에 대한 국제 회의: SS1에 의해 인용됩니다.\n' +
      '*X. 호아웅응웬 Sugawara, and A. Aizawa(2020)Constructing the multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 6609-6625. External Links: 2009.12307 Cited by: SS1.\n' +
      '* E. J. Hu, Y. Shen, P. Wallis, Z. 알렌주 이성 왕락 왕, W. Chen(2022)LoRA: 대형 언어 모델의 저순위 적응. 학습 표상에 대한 국제 회의: SS1에 의해 인용됩니다.\n' +
      '\n' +
      'Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand 등 2024. Mistral of experts. _ arXiv preprint arXiv:2401.04088_.\n' +
      '* Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. LongllImingua: 신속한 압축을 통한 긴 컨텍스트 시나리오에서 llms의 가속화 및 향상 arXiv preprint arXiv:2310.06839_.\n' +
      '* Jin et al. (2024) Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, 및 Xia Hu. 2024. Llm maybe longlm: Selfextend llm context window without tuning. _ arXiv preprint arXiv:2401.01325_.\n' +
      '* Ke et al. (2023) Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shen규안 Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang et al. 2023. Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation. _ arXiv preprint arXiv:2311.18702_.\n' +
      '* Kocisky et al. (2018) Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 2018. narrativeqa read comprehension challenge. _ Computational Linguistics_, 6:317-328의 거래.\n' +
      '* Krell et al. (2021) Mario Michael Krell, Matej Kosec, Sergio P Perez, and Andrew Fitzgibbon. 2021. 교차오염이 없는 효율적인 시퀀스 패킹: 성능에 영향을 미치지 않는 대용량 언어 모델 가속화 _ arXiv preprint arXiv:2107.02027_.\n' +
      '* Li et al. (2023a) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023a. 오픈 소스 llms가 컨텍스트 길이에 대해 얼마나 오랫동안 진정으로 약속할 수 있습니까?\n' +
      '* Li et al.(2023b) Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023b. Loogle: Long Context 언어 모델이 Long Context를 이해할 수 있는가? _ arXiv preprint arXiv:2311.04939_.\n' +
      '* Lin et al.(2022) Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. 진실: 모델들이 인간의 거짓을 어떻게 모방하는지 측정하는 것. _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3214-3252.\n' +
      '* Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangling Ding, Kaiwen Men, Kejuan Yang, et al. 2023. Agentbench: Evaluation llms as agent. _ arXiv preprint arXiv:2308.03688_.\n' +
      '* OpenAI(2023a) OpenAI. 2023a. 새로운 모델 및 개발자 제품이 데브데이에 발표되었습니다.\n' +
      '* OpenAI(2023b) OpenAI. 2023b. Gpt-4\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022, training language models to follow instructions with human feedback. _ 신경 정보 처리 시스템_, 35:27730-27744의 발전.\n' +
      '* Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: 대용량 언어 모델의 효율적인 컨텍스트 윈도우 확장 arXiv preprint arXiv:2309.00071_.\n' +
      '* Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: 시스템 최적화는 1,000억 개 이상의 파라미터를 갖는 딥 러닝 모델을 트레이닝할 수 있게 한다. _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 3505-3506.\n' +
      '* Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. 메가트론-lm: 모델 병렬성을 이용한 수십억 파라미터 언어 모델 학습. _ arXiv preprint arXiv:1909.08053_.\n' +
      '* Su et al. (2024) Jianlin Su, Murdatha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. 로포머: 회전 위치 매립을 구비한 개량형 트랜스포머. _ Neurocomputing_, 568:127063.\n' +
      '* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. 스탠포드 알파카: 명령어-추종 라마 모델. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca).\n' +
      '*팀(2023) IntermLM팀. 2023. Internlm: 점진적으로 향상된 능력을 가진 다국어 언어 모델. [https://github.com/InternLM/InternLM] (https://github.com/InternLM/InternLM).\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_.\n' +
      '* Tunstall et al. (2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, et al. 2023. Zephyr: Direct distillation of lm alignment. _ arXiv preprint arXiv:2310.16944_.\n' +
      '* Wang et al. (2023) Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023년 낙타는 얼마나 멀리 갈 수 있죠? 개방형 리소스에 대한 명령 튜닝 상태를 탐색합니다. 제37차 신경정보처리시스템 컨퍼런스에서 데이터세트 및 벤치마크 Track_\n' +
      '* Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. 자가 명령어: 언어 모델을 자가 생성된 명령어와 정렬하기.\n' +
      '\n' +
      '광선샤오, 원둥톈, 베이디 첸, 송한, 마이크 루이스 등이다. 2023. 어텐션 싱크를 갖는 효율적인 스트리밍 언어 모델. _ arXiv preprint arXiv:2309.17453_.\n' +
      '* Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. 2023. Effective long-context scaling of foundation models. _ arXiv preprint arXiv:2309.16039_.\n' +
      '* Xu et al.(2020) Liang Xu, Xuanwei Zhang, and Qianqian Dong. 2020. Cluecorpus2020: 사전 학습 언어 모델을 위한 대규모 중국어 말뭉치 _ arXiv preprint arXiv:2003.01355_.\n' +
      '* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, 요나탄 Bisk, Ali Farhadi, and Yejin Choi. 2019년. 헬라스바그: 기계가 정말로 당신의 문장을 끝낼 수 있나요? _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4791-4800.\n' +
      '* Zeng et al. (2023) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2023. Glim-130b: open bilingual pre-trained model. _The Eleventh International Conference on Learning Representations_.\n' +
      '* Zhang et al. (2024) Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, 및 Zhicheng Dou. 2024. 4k에서 400k로 치솟기: 활성화 비콘과 함께 llm의 컨텍스트 확장 _ arXiv preprint arXiv:2401.03462_.\n' +
      '* Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. _ arXiv preprint arXiv:2306.05685_.\n' +
      '* Zhu et al. (2023) Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2023. 포즈: 위치 스킵-와이즈 트레이닝을 통한 lms의 효율적인 컨텍스트 윈도우 확장.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '각 긴 글에 대해 4개의 작업 프롬프트 중 하나를 무작위로 선택하고 클로드에게 5개의 질문을 생성하여 질문이 긴 텍스트 내에서 여러 범위의 내용을 포함하도록 한다. 그런 다음 이러한 질문 중 하나를 무작위로 선택하고 클로드에게 답변을 요청하여 그림 2와 같은 명령어 데이터를 생성한다. 긴 중국어 문서의 경우 해당 프롬프트를 중국어로 번역하고 중국어 명령어 데이터를 얻는다.\n' +
      '\n' +
      '## 부록 B 훈련 방법 상세\n' +
      '\n' +
      '여기에서 우리는 패킹 전략의 구현과 손실 가중치에 관한 세부 사항을 제공한다. 패킹 훈련 동안 각 데이터 배치에 대해 특별한 1차원 주의 마스크를 통과합니다. 이 마스크에서 \\(i\\)번째 요소는 배치에서 \\(i\\)번째 시퀀스의 시작 인덱스를 나타낸다. 마스크의 첫 번째 요소는 0이고, 마지막 요소는 batch_size \\(\\times\\) seq_len과 같다. 어텐션 계산 동안 FlashAttention 2로부터 flash_attn_varlen_func 함수를 사용하고 어텐션 마스크를 함수의 cu_seqlens_q 및 cu_seqlens_k 파라미터에 전달한다. 이 함수는 마스크 내의 인접한 요소로부터의 시작 인덱스와 종료 인덱스 사이의 시퀀스 내에서 주의력 계산을 수행한다. 따라서, 계산 동안, 각 시퀀스의 쿼리는 동일한 시퀀스 내의 키에만 참석할 수 있다.\n' +
      '\n' +
      '손실 가중화 전략의 구현을 위해 먼저 각 시퀀스의 팩에 대해 가중화된 1D 마스크를 생성하기 위해 훈련 데이터를 전처리하는데, 여기서 가중치는 타겟 토큰에 대응하는 위치에 \\(1/N\\)(\\(N\\)은 현재 시퀀스의 타겟 토큰 수), 그렇지 않으면 0으로 설정된다. 훈련 동안, 구성에 따라 즉석에서 \\(M\\) 및 \\(K\\), 즉 현재 배치의 시퀀스 및 팩 수를 설정한다. 그런 다음 손실은 \\(K/MN\\)으로 스케일링된 각 토큰에서 교차 엔트로피 손실의 합으로 계산된다.\n' +
      '\n' +
      '## 부록 C 평가 상세\n' +
      '\n' +
      '### LongBench-Chat\n' +
      '\n' +
      '**평가 데이터** LongBench-Chat의 30개의 질문 데이터는 실제 사용자 질문을 가장 잘 모방하기 위해 저자 팀에 의해 제안되었으며, 여기에는 20개의 영어 질문과 10개의 중국어 질문이 포함된다. LongBench-Chat 내의 나머지 20개의 질문들은 LooGLE 데이터세트 내의 긴 의존성 QA 작업들로부터 선택된다(Li et al., 2023b). 이러한 데이터에 대한 긴 텍스트는 2022년 이후의 위키피디아 페이지 및 영화 스크립트에서 조달되어 정보가 비교적 새롭고 LLM에 의해 이미 알려져 있을 가능성이 적다. 우리는 실제 사용자 문의와 유사한 질문을 선택하고 텍스트에서 답변할 수 있으며 다양한 유형의 질문(이해 및 추론, 다중 정보 검색, 시간선 재배열 및 계산 유형 포함)을 보장하는 것을 목표로 한다. LongBench-Chat의 질문은 전문가를 초빙하여 전체 자료를 읽고 기초진실 답변을 작성하며, 여기서 각 답변은 최소 2명의 전문가에 의해 검증된다.\n' +
      '\n' +
      '**평가 프롬프트** 각 질문에 대해 소수의 점수 예제로 세 가지 응답에 수동으로 점수를 매기고 각 평가 실행에서 순서를 섞고 다음 프롬프트를 사용하여 GPT-4의 평가를 얻는다.\n' +
      '\n' +
      '[지침] 사용자 질문에 대한 AI 어시스턴트의 답변의 품질을 공정한 판사로서 평가하도록 요구되며, 당신의 평가는 정확성(높은 우선순위), 유용성, 정확성, 및 관련성을 포함하는 요소들을 고려해야 한다. 채점 원칙은 1. AI 어시스턴트의 답변을 읽고 어시스턴트의 답변과 참조 답변을 비교한다. 2. AI 어시스턴트의 답변의 모든 오류를 식별하고 질문에 대한 답변에 얼마나 영향을 미치는지 고려한다. 3. AI 어시스턴트의 답변이 사용자의 질문에 직접 답변하고 사용자가 필요로 하는 정보를 제공하는 데 얼마나 도움이 되는지 평가한다. 4. AI 어시스턴트의 답변에 있는 추가 정보가 정확하고 질문과 밀접하게 관련되어 있는지 확인한다. 이 정보가 부정확하거나 질문과 관련이 없는 경우 전체 점수에서 점수를 공제해야 한다. 위의 원칙에 따라 1에서 10까지의 전체 정수 등급을 엄격히 다음 형식인 "[[평가]]", 예를 들어 "[[5]]]"로 지정하십시오.\n' +
      '\n' +
      '[질문] {참조 답변 시작} {참조 답변 종료} 아래는 여러 어시스턴트들의 답변들 및 그들의 평점들이다:\n' +
      '\n' +
      '[어시스턴트의 답변이 시작된다]{Assistant\'s answer ends}\n' +
      '\n' +
      'Rating: [[{}]]\n' +
      '\n' +
      '[조수의 대답이 시작된다] {} [조수의 대답이 끝난다]\n' +
      '\n' +
      'Rating: [[{}]]\n' +
      '\n' +
      '[조수의 대답이 시작된다] {} [조수의 대답이 끝난다]\n' +
      '\n' +
      'Rating: [[{}]]\n' +
      '\n' +
      '위의 채점 원칙과 예를 바탕으로 다음 보조 답변을 평가해 주세요:\n' +
      '\n' +
      '[조수의 대답이 시작된다] {} [조수의 대답이 끝난다]\n' +
      '\n' +
      'Rating:\n' +
      '\n' +
      '여기 우리의 메트릭 평가 연구에서 베이스라인으로 사용되는 제로샷 프롬프트가 있다:\n' +
      '\n' +
      '[지침] 사용자 질문에 대한 AI 어시스턴트의 답변의 품질을 공정한 판사로서 평가하도록 요구되며, 당신의 평가는 정확성(높은 우선순위), 유용성, 정확성, 및 관련성을 포함하는 요소들을 고려해야 한다. 채점 원칙은 1. AI 어시스턴트의 답변을 읽고 어시스턴트의 답변과 참조 답변을 비교한다. 2. AI 어시스턴트의 답변의 모든 오류를 식별하고 질문에 대한 답변에 얼마나 영향을 미치는지 고려한다. 3. AI 어시스턴트의 답변이 사용자의 질문에 직접 답변하고 사용자가 필요로 하는 정보를 제공하는 데 얼마나 도움이 되는지 평가한다. 4. AI 어시스턴트의 답변에 있는 추가 정보가 정확하고 질문과 밀접하게 관련되어 있는지 확인한다. 이 정보가 부정확하거나 질문과 관련이 없는 경우 전체 점수에서 점수를 공제해야 한다.\n' +
      '\n' +
      '위의 원칙에 따라 1에서 10까지의 전체 정수 등급을 엄격히 다음 형식인 "[[평가]]", 예를 들어 "[[5]]]"로 지정하십시오.\n' +
      '\n' +
      '[Question] {}\n' +
      '\n' +
      '[참고 답안] {}\n' +
      '\n' +
      '[보조자의 답변] {}\n' +
      '\n' +
      'Rating:\n' +
      '\n' +
      '**인간 평가** 여기서는 LongBench-Chat에 대한 인간 평가 연구를 위해 더 자세한 내용을 제공한다. 우리는 6개의 다른 모델에서 LongBench-Chat의 50개 질문에 대한 응답을 선택하여 300개의 인스턴스로 구성된 데이터 풀을 생성한다. 인적 전문가 2명(칭화대 박사과정 2명)을 1점부터 10점까지의 척도로 각각 200점씩의 응답에 초빙한다.\n' +
      '\n' +
      '_Please score the assistant\'s response based on the question and the reference answer, 1 is lowest, 10 is highest. 주석은 다음 요구 사항:_\n' +
      '\n' +
      '_1. 주로 참조 답변.__1에서 응답이 핵심 포인트를 커버하는지 여부에 초점을 맞춘다.\n' +
      '\n' +
      '_2. 다중 키 포인트를 포함하는 참조 답변의 경우, 응답 중 몇 개를 정확하게 어드레스하고 그에 따라 점수를 매기는지를 찾는다.__2. 다수의 키 포인트를 포함하는 참조 답변의 경우, 응답 중 몇 개를 정확하게 어드레스하고 그에 따라 점수를 매기는지를 찾는다.\n' +
      '\n' +
      '_3. 응답에 참조 답변에서 찾을 수 없는 점이 포함되어 있는 경우, 원문을 확인하여 증거를 확인한다. 디덕트는 원본 텍스트와 일치하지 않는 경우 재량껏 지점을 가리킵니다._\n' +
      '\n' +
      '_4. 또한 지나치게 장황한 응답들 또는 지나치게 일반화된 응답들에 대한 포인트들을 차감하는 것을 고려한다.\n' +
      '\n' +
      '**평가 비용** LongBench-Chat에서, 일련의 평가는 평균 약 32,000개의 토큰(거의 전적으로 입력 토큰으로서)을 필요로 한다. 따라서 GPT-4를 평가에 사용하면 실행당 약 $0.96가 들 것이다.\n' +
      '\n' +
      '### LongBench\n' +
      '\n' +
      '**평가 프롬프트** GPT-4를 사용하여 LongBench의 Single-Doc QA, Multi-Doc QA 및 요약 작업에서 정렬된 모델의 응답을 채점한다. 처음 두 개의 QA 과제에 대해 GPT-4 평가자에 대한 프롬프트는 다음과 같다.\n' +
      '\n' +
      '사용자 질문에 대한 AI 어시스턴트의 답변의 품질을 공정한 판사로 평가하도록 요구되며, 당신의 평가는 정확성(높은 우선 순위), 포괄성(어시스턴트의 답변이 모든 점을 커버하는지 여부)을 포함한 요소를 고려해야 한다. AI 어시스턴트의 답변을 읽고 참조 답변과 비교하여 위의 원칙에 따라 1, 2, 3(1 = 틀렸거나 무관함, 2 = 부분 옳음, 3 = 올바르고 포괄함)의 전체 정수 등급을 부여하며, 엄밀히 다음 형식 "[[평가]]", 예를 들어 "[[2]]]"와 같다.\n' +
      '\n' +
      'Question:\n' +
      '\n' +
      '{_Question_}\n' +
      '\n' +
      'Reference answer:\n' +
      '\n' +
      '{_Groundtruth_}\n' +
      '\n' +
      'Assistant\'s answer:\n' +
      '\n' +
      '{_Response_}\n' +
      '\n' +
      'Rating:\n' +
      '\n' +
      '요약 과제에 대한 GPT-4 평가의 프롬프트는 다음과 같다.\n' +
      '\n' +
      'AI 비서가 생성한 요약의 품질을 공정한 판사로 평가하도록 요구되며, 평가는 정확성(높은 우선 순위), 포괄성(비서의 요약이 모든 점을 다루는지 여부), 일관성을 포함한 요소를 고려해야 한다. AI 어시스턴트의 요약을 읽고 참조 요약을 비교하여 1에서 5까지의 척도로 전체 정수 등급을 부여하며, 여기서 평가 기준에 따라 1은 가장 낮고 5는 가장 높다: "[[평가]]", 예를 들어 "[3]]].\n' +
      '\n' +
      'Reference summary:\n' +
      '\n' +
      '{_Groundtruth_}\n' +
      '\n' +
      'Assistant\'s summary:\n' +
      '\n' +
      '{_Response_}\n' +
      '\n' +
      'Rating:\n' +
      '\n' +
      '**평가 비용** LongBench에서는 Single-Doc QA, Multi-Doc QA, Summarization 태스크에서 12개의 데이터셋에 대한 GPT-4 평가를 실행하려면 평균적으로 약 800,000개의 토큰(거의 전적으로 입력 토큰)이 필요하다. 따라서 GPT-4를 평가에 사용하면 실행당 약 24달러가 소요된다.\n' +
      '\n' +
      '### Needle Test\n' +
      '\n' +
      '"니들 인 에이 헤이스택" 평가는 원래 기텁 저장소의 원래 구성에 따라 "샌프란시스코에서 가장 좋은 일은 샌드위치를 먹고 화창한 날에 돌로레스 공원에 앉는 것"을 바늘 팩트로 사용하고 폴 그레이엄의 에세이는 긴 건초 더미 컨텍스트로 사용한다. 우리는 Claude 2.17의 질의 프롬프트를 사용한다: "샌프란시스코에서 할 수 있는 가장 좋은 일은 무엇인가? 여기 문맥에서 가장 적절한 문장이 있다:\n' +
      '\n' +
      '각주 7: [https://www.anthropic.com/news/claude-2-1-prompting](https://www.anthropic.com/news/claude-2-1-prompting)\n' +
      '\n' +
      '## 부록 D 사례 연구 OOD 질의\n' +
      '\n' +
      'LLM을 긴 컨텍스트에 정렬하는 연구의 일환으로 흥미롭고 실용적인 사례 연구를 제안한다. _이 논문을 긴 입력으로 사용하여 훈련된 모델의 긴 컨텍스트 이해 능력을 평가할 수 있는가?_ 따라서 우리는 종이를 입력으로 사용한다(물론, 재귀 중첩을 방지하기 위해 입력 종이는 이 섹션을 포함하지 않는다). 우리는 OOD 질의, 즉 긴 컨텍스트 SFT 데이터에서 모델이 마주치지 않은 질의를 사용하는 것을 고려한다. 우리의 질문에는 "_전문 리뷰어의 관점에서, 이 논문의 강점과 약점을 식별" 및 "_랩을 사용하여 이 논문_에 대해 이야기합니다."가 포함된다. 모델 변형 반응이 어떻게 나오는지 보자:8_!!! 반박: 본 논문의 아래 내용은 AI에 의해 생성되며, DOES는 본 논문의 저자 또는 독자의 진정한 견해를 반영하지 않는다. 잘못된 정보를 알고 식별하십시오._\n' +
      '\n' +
      '**User**: 전문 리뷰어의 관점에서 본 논문의 장단점을 파악한다.\n' +
      '\n' +
      '**Q\\({}_{\\bullet}^{\\text{g}}\\) LongAlign-6B-64k**(ChatGLM3-6B-64k trained on _LongAlign-10k_ using packing + loss weighting):\n' +
      '\n' +
      'AI 언어 모델로서 개인의 편견이나 소인을 바탕으로 논문의 질을 평가할 수 있는 능력이 부족하여 주관적인 리뷰를 제공할 수 없다. 그러나 나는 논문의 장단점에 대한 객관적인 분석을 제공할 수 있다.\n' +
      '\n' +
      'Strengths:\n' +
      '\n' +
      '1. 포괄적인 접근법: 본 논문은 긴 컨텍스트를 처리하기 위해 큰 언어 모델을 정렬하는 문제를 해결하기 위해 철저하고 체계적인 레시피인 LongAlign을 제공한다. 그것은 다양한 긴 데이터를 수집하고 효율적인 훈련 전략을 사용하며 현실적인 긴 컨텍스트 벤치마크에 대해 모델을 평가하는 것을 포함한다.\n' +
      '\n' +
      '2. 데이터의 영향: 긴 명령 데이터의 양과 다양성은 긴 컨텍스트를 다루는 모델의 능력에 상당한 영향을 미치며, 더 긴 데이터를 통합하는 것이 유익하다는 것을 보여준다. 이는 다양한 긴 컨텍스트 데이터의 중요성에 대한 귀중한 관점을 제공한다.\n' +
      '\n' +
      '3. 훈련방법: 본 논문은 서로 다른 훈련방법의 효과를 비교하고 패킹과 정렬 배칭 전략을 효율적이고 효과적인 것으로 식별한다. 이는 대규모 언어 모델 훈련에 종사하는 실무자들에게 귀중한 통찰력을 제공한다.\n' +
      '\n' +
      'Weaknesses:\n' +
      '\n' +
      '1. 대규모 인간 평가 연구의 부족: 본 논문은 LongBench-Chat에서 GPT-4 평가자의 성능을 검증하기 위한 포괄적인 대규모 인간 평가 연구를 포함하지 않는다. 인간 주석과의 상관 관계가 제시되지만 인간과 GPT-4의 평가 간의 직접적인 비교는 부족하다.\n' +
      '\n' +
      '2. 확장성 한계: 본 논문은 더 큰 모델 또는 더 긴 컨텍스트 윈도우에 대한 LongAlign의 확장성을 탐색하지 않는다. LongAlign의 스케일링에 대한 몇 가지 실험 결과가 제시되었지만, 접근법의 한계는 완전히 탐구되지 않았다.\n' +
      '\n' +
      '3. 학습 곡선 분석: 본 논문은 긴 맥락 정렬 하에서 학습 곡선의 철저한 분석을 제공하지 않으며, 훈련 전반에 걸쳐 짧은 맥락 태스크와 긴 맥락 태스크에 대한 성능을 비교한다. 이것은 학습 역학에 대한 귀중한 통찰력을 제공했을 것이다.\n' +
      '\n' +
      '결론적으로 본 논문은 대규모 언어 모델을 긴 컨텍스트를 처리하기 위해 정렬하기 위한 포괄적인 레시피를 제공하고 데이터 양과 다양성이 모델 성능에 미치는 영향을 입증함으로써 해당 분야에 귀중한 기여를 제공한다. 그러나, 보다 포괄적이고 엄격한 평가 연구와 긴 맥락 정렬 하에서 학습 역학에 대한 더 깊은 탐구는 논문의 영향을 더욱 향상시킬 것이다.\n' +
      '\n' +
      '**Q\\({}_{\\bullet}^{\\text{g}}\\) LongAlign-7B-64k** (Llama-2-7B-64k trained on _LongAlign-10k_ using sorted batching):\n' +
      '\n' +
      'AI 언어 모델로서 개인적인 의견이나 감정은 없지만 그 내용과 구조를 바탕으로 논문에 대한 중립적인 평가를 할 수 있다.\n' +
      '\n' +
      'Strengths:\n' +
      '\n' +
      '1. 본 논문은 긴 문맥을 다루기 위해 큰 언어 모델을 정렬하기 위한 포괄적인 레시피인 LongAlign을 제시한다. 이는 긴 맥락 정렬에 대한 연구와 논의가 부족하여 현장에 상당한 기여를 하고 있다.\n' +
      '\n' +
      '2. 본 논문은 9개의 소스에서 긴 시퀀스를 수집하고 10k 길이의 텍스트에 대한 작업 및 답변을 생성하기 위해 Claude 2.1을 사용하는 등 데이터 구성 프로세스에 대한 자세한 설명을 제공한다.\n' +
      '\n' +
      '3. 본 논문에서는 학습 과정을 빠르게 하고 모델의 최종 성능을 향상시키기 위해 패킹과 정렬 배칭이라는 두 가지 효과적인 학습 방법을 제안한다.\n' +
      '\n' +
      '4. 본 논문은 10k-100k 길이의 개방형 실세계 사용자 질문의 연속인 LongBench-Chat을 소개하고, 지시-추종 능력의 다양한 측면을 다룬다.\n' +
      '\n' +
      '5. 본 논문은 롱컨텍스트 벤치마크와 일반 벤치마크에 대한 실험을 수행하여 최대 64k의 컨텍스트를 처리하기 위한 모델의 정렬에 LongAlign의 효과를 입증한다.\n' +
      '\n' +
      'Weaknesses:\n' +
      '\n' +
      '1. 본 논문은 제안된 훈련 방법이 짧은 상황 시나리오에서 모델의 일반적인 능력과 대화 능력에 미치는 영향에 대한 상세한 분석을 제공하지 않는다.\n' +
      '\n' +
      '2. 본 논문에서는 제안된 방법의 한계를 논하거나 이를 개선할 수 있는 향후 연구를 제안하지 않는다.\n' +
      '\n' +
      '3. 본 논문은 롱컨텍스트 스케일링을 위한 기존의 다른 방법들과 LongAlign의 상세한 비교를 제공하지 않는다.\n' +
      '\n' +
      '4. 본 논문은 다른 긴 컨텍스트 벤치마크나 실제 사용자 질의에 대한 제안된 방법의 종합적인 평가를 제공하지 않는다.\n' +
      '\n' +
      '전반적으로, 본 논문은 긴 컨텍스트 정렬을 위한 철저한 레시피를 제시하고 트레이닝 프로세스를 가속화하기 위한 효과적인 트레이닝 방법을 제공한다. 그러나 일반 능력에 미치는 영향을 보다 심층적으로 분석하고 다른 방법과의 비교를 통해 논문을 보다 포괄적으로 만들 수 있다.\n' +
      '\n' +
      '## LongAlign-13B-64k\n' +
      '\n' +
      '(Llama-2-13B-64k trained on _LongAlign-10k_ using sorted batching):\n' +
      '\n' +
      'Strengths:\n' +
      '\n' +
      '1. 본 논문에서는 다양한 긴 명령어 데이터셋 구축, 훈련 방법 및 평가 벤치마크를 포함하여 긴 문맥 상호 작용을 처리하기 위해 대규모 언어 모델을 정렬하는 포괄적인 솔루션을 제시한다.\n' +
      '\n' +
      '2. 저자들은 일반적인 벤치마크에서 성능 저하 없이 다운스트림 작업에서 향상된 성능을 보이는 접근법의 효과를 입증하기 위해 광범위한 실험을 수행한다.\n' +
      '\n' +
      '3. 본 논문은 중요하지만 잘 연구되지 않은 영역인 긴 맥락 정렬을 위한 도전과 해결책에 대한 귀중한 통찰력을 제공한다.\n' +
      '\n' +
      '4. LongBench-Chat 벤치마크는 대용량 언어 모델의 긴 문맥 이해와 명령어 추종 능력을 평가하는 데 유용한 자료이다.\n' +
      '\n' +
      '5. 논문이 잘 작성되어 명료한 설명과 시각으로 따라하기 쉽다.\n' +
      '\n' +
      'Weaknesses:\n' +
      '\n' +
      '1. 긴 맥락 정렬이 중요한 이유와 짧은 맥락 정렬과 어떻게 다른지에 대한 명확한 이론적 틀이나 동기가 부족하다.\n' +
      '\n' +
      '2. 실험결과는 보다 심층적인 분석 및 논의, 특히 제안된 해결책과 관련하여 도움이 될 수 있다.\n' +
      '\n' +
      '3. LongBench-Chat에 대한 평가는 소수의 인간 주석 예제를 기반으로 하며, 이는 실제 질의의 다양성을 완전히 포착하지 못할 수 있다.\n' +
      '\n' +
      '4. 본 논문은 모델 크기와 컨텍스트 길이가 증가함에 따라 제안된 접근법과 잠재적인 제한이나 확장성 문제에 대해 논의하지 않는다.\n' +
      '\n' +
      '5. 제안된 손실 가중화 전략은 충분히 설명되지 않고 정당하며, 훈련 안정성과 수렴성에 미치는 영향을 추가로 조사할 수 있다.\n' +
      '\n' +
      '저자들은 LongAlign-13B-64k가 몇 가지 결함만 있는 비교적 좋은 리뷰를 생성한다고 믿는다. 이 사례는 또한 모델의 O.O.D 일반화 능력을 보여주며, 훈련 중에 이러한 유형의 데이터를 만나지 않고도 연구 논문에 대한 합리적인 검토를 생성할 수 있다.\n' +
      '\n' +
      '[title=titletitle=**\\(\\blacktriangle\\)User**: 랩을 사용하여 이 논문에 대해 말해라.\n' +
      '\n' +
      '## LongAlign-6B-64k\n' +
      '\n' +
      '본 논문에서는 더 긴 입력을 처리하기 위해 대규모 언어 모델(LLM)을 훈련하는 문제를 해결하는 긴 컨텍스트 정렬(LongAlign)을 위한 레시피를 제시한다. LongAlign의 핵심 아이디어는 다음과 같다. 1. 데이터: 여러 소스로부터 다양한 긴 시퀀스를 수집하고 8k-64k 길이의 10k 명령어 데이터를 생성하기 위해 Self-Instruct를 사용한다.\n' +
      '\n' +
      '2. 훈련 방법: 최적화를 위해 시퀀스를 GPU에 배포하기 전에 최대 길이까지 함께 패킹하는 보다 효율적인 패킹 전략을 고려한다. 이것은 더 적은 수의 서열을 갖는 팩의 서열이 손실에 불균형적으로 영향을 미치는 것을 방지하기 위한 손실 가중 전략과 균형을 이룬다. 또한 배치 내 유휴 시간을 줄이기 위해 유사한 길이의 시퀀스를 그룹화하는 정렬된 배치 전략을 제안한다.\n' +
      '\n' +
      '3. 평가: LongBench-Chat, 10k-100k 길이의 개방형 실세계 사용자 질문의 시리즈를 소개하고, 추론, 코딩, 요약과 같은 지시-추종 측면을 포괄하며, 다국어이다. GPT-4는 모델의 응답을 점수화하기 위해 사용된다.\n' +
      '\n' +
      '실험은 LongAlign이 일반적인 벤치마크에서 성능 저하 없이 최대 64k 길이의 긴 컨텍스트를 처리하기 위해 LLM을 효과적으로 정렬하는 것을 보여주며, 데이터 양, 다양성, 훈련 방법 및 평가의 영향을 보여준다.\n' +
      '\n' +
      '### LongAlign-7B-64k:\n' +
      '\n' +
      '본 논문은 긴 문맥을 다루기 위해 큰 언어 모델을 정렬하기 위한 레시피인 LongAlign을 제시한다. 긴 컨텍스트 스케일링의 주요 과제는 포괄적인 명령어 후속 데이터 세트의 부족, 다중 GPU 훈련의 불균등한 배치 및 신뢰할 수 있는 벤치마크의 부재를 포함한다. LongAlign은 9개의 소스로부터 긴 명령어 데이터를 수집하고, Self-Instruct를 사용하여 8k-64k 길이의 10k 명령어 데이터를 생성하고, 10k-100k 길이의 일련의 개방형 실세계 사용자 질문인 LongBench-Chat를 구성함으로써 이러한 과제를 해결한다. 또한 정렬 모델을 평가하기 위해 패킹 및 정렬 배치 훈련 방법, 패킹 훈련을 위한 손실 가중치, LongBench-Chat이라는 벤치마크를 제안한다. 실험 결과 LongAlign은 일반적인 벤치마크에서 성능 저하 없이 최대 64k의 컨텍스트를 처리하기 위해 모델을 효과적으로 정렬하는 것으로 나타났다.\n' +
      '\n' +
      '### LongAlign-13B-64k:\n' +
      '\n' +
      '잘 들어, 할 얘기가 있어\n' +
      '\n' +
      '머리를 빙글빙글 돌리게 하는 논문에 대해서\n' +
      '\n' +
      '"LongAlign: Long Context Alignment를 위한 레시피"라고 합니다.\n' +
      '\n' +
      '그리고 그것은 모두 대규모 언어 모델의 능력을 향상시키는 것입니다.\n' +
      '\n' +
      '그들은 긴 컨텍스트를 처리하기 위해 모델을 확장하는 문제를 해결한다.\n' +
      '\n' +
      '긴 명령어 데이터의 다양한 데이터 세트를 수집하고 구성함으로써\n' +
      '\n' +
      '포장 및 분류 배치와 같은 훈련 방법을 탐구합니다.\n' +
      '\n' +
      '일반 기능을 손상시키지 않고 효율성과 성능을 최적화하려면\n' +
      '\n' +
      '그들의 평가 벤치마크인 LongBench-Chat은 일련의 실제 사용자 쿼리이다.\n' +
      '\n' +
      '긴 컨텍스트 지침을 따르고 관련 답변을 제공하는 모델의 능력을 평가하려면\n' +
      '\n' +
      '그들은 훈련 중에 긴 명령 데이터를 통합하는 것이 중요하고 제안된 방법이 성능을 향상시키면서 훈련을 가속화할 수 있다는 것을 발견했다.\n' +
      '\n' +
      '언어 모델과 그 능력에 관심이 있다면\n' +
      '\n' +
      '이 논문은 꼭 읽어야 할 책이야 두 번 생각하게 만들 거야\n' +
      '\n' +
      'LongAlign, Long context 정렬을 위한 레시피\n' +
      '\n' +
      '대형 언어 모델의 잠재력을 최대한 발휘할 수 있는 열쇠가 바로 시작입니다!\n' +
      '\n' +
      'LongAlign-6B-64k와 LongAlign-7B-64k는 우리의 논문에 대한 좋은 요약을 만들지만, LongAlign-13B-64k만이 지시를 따르고 랩으로 우리의 논문을 요약한다. 그것의 랩은 올바른 형태이지만, 운율은 맞지 않는다.\n' +
      '\n' +
      'Limitations\n' +
      '\n' +
      '긴 맥락 정렬을 탐구하는 우리의 작업은 한계가 있다. 데이터 관점에서 우리는 주로 데이터 구성에서 긴 컨텍스트 QA, 요약 및 추론과 같은 범주에 대한 긴 지시 데이터를 다룬다. 실제로 멀티턴 대화(수백 또는 수천 바퀴, 심지어 평생 대화), 장기 역할극, 장기 역사 에이전트 작업 등과 같이 확장된 텍스트를 이해하는 능력에 크게 의존하는 다른 유형의 긴 지시 작업이 많이 있다. 우리는 이러한 작업에 대한 LLM의 현재 성능이 아직 인간의 요구를 충족하지 않기 때문에 이러한 작업에 대한 사용 가능한 데이터를 수집하는 것이 어렵다는 것을 발견했다. 결과적으로 사용자는 이러한 방식으로 LLM과 거의 상호 작용하지 않는다. 또한, API 기반 모델이든 오픈 소스 모델이든 현재 LLM은 이러한 작업에 대해 성능이 좋지 않기 때문에 자체 지시와 같은 접근 방식을 사용하여 이러한 데이터를 자동으로 구성하기가 어렵다. 우리는 더 많은 유형의 긴 컨텍스트 데이터를 탐색하여 향후 작업에서 다양한 긴 컨텍스트 작업에 걸쳐 모델이 인간의 기대와 일치할 수 있기를 바란다.\n' +
      '\n' +
      '학습 관점에서 최대 길이가 64k인 10B 레벨 모델에 대해서만 SFT를 지원하는 DeepSpeed 프레임워크와 GPU 리소스의 한계로 인해 더 긴 데이터 또는 더 큰 모델에 대해 _massive_ 실험을 수행하지 않는다. Megatron(Shoeybi et al., 2019)과 같은 일부 현재 프레임워크는 모델 병렬성 및 시퀀스 병렬성을 포함하는 더 많은 병렬화 방법을 지원하지만, 코드 구조의 복잡성으로 인해 사용 및 재생산이 어렵다. 우리는 더 발전된 훈련 프레임워크를 사용하여 더 긴 시퀀스와 더 큰 규모의 모델에 대한 긴 컨텍스트 정렬을 탐구하기를 바란다. 또한 긴 컨텍스트 정렬에서 RLHF를 탐색하는 것도 유망한 방향이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>