<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# LLM 에이전트의 매우 장기적인 대화 기억 평가\n' +
      '\n' +
      '**아디아샤 마하라나1***동호 리2***세르게이 툰야코프3***\n' +
      '\n' +
      '**Mohit Bansal1\\({}^{\\dagger}\\)** **Francesco Barbieri\\({}^{\\dagger}\\)** **Yuwei Fang3\\({}^{\\dagger}\\)**\n' +
      '\n' +
      '캘리포니아주 서던힐1대학 노스캐롤라이나대학\n' +
      '\n' +
      '각주 1: 사용 가능한 코드 및 데이터\n' +
      '\n' +
      '[https://snap-research.github.io/locomo](https://snap-research.github.io/locomo)\n' +
      '\n' +
      '각주 2: 동등한 조언.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '장기 오픈 도메인 대화의 기존 작업은 5개 이하의 채팅 세션에 걸쳐 있는 컨텍스트 내에서 모델 응답을 평가하는 데 중점을 둔다. 롱컨텍스트 대형 언어 모델(LLM) 및 검색 증강 생성(RAG) 기술의 발전에도 불구하고, _very_ 장기 대화에서 그들의 효능은 아직 탐구되지 않았다. 본 논문에서는 이러한 문제점을 해결하기 위해 LLM 기반 에이전트 아키텍처를 활용하고 페르소나 및 시간 이벤트 그래프에 대화문을 접음으로써 고품질의 _very_ 장기 대화문을 생성하는 머신-휴먼 파이프라인을 소개한다. 또한, 각 에이전트는 이미지를 공유하고 반응할 수 있는 기능을 갖추고 있다. 생성된 대화는 이벤트 그래프에 대한 장거리 일관성과 접지를 위해 인간 주석자에 의해 검증되고 편집된다. 이 파이프라인을 사용하여 최대 35개 세션에 걸쳐 각각 300번의 턴과 avg의 9K 토큰을 포함하는 _very_ 장기 대화 데이터 집합인 LoCoMo를 수집한다. LoCoMo를 기반으로 질문 응답, 이벤트 요약 및 멀티모달 대화 생성 작업을 포함하는 모델의 장기 기억 측정을 위한 포괄적인 평가 벤치마크를 제시한다. 우리의 실험 결과는 LLM이 긴 대화를 이해하고 대화 내에서 장거리 시간적 및 인과적 역학을 이해하는 데 어려움을 나타낸다. Long-context LLMs 또는 RAG와 같은 전략을 사용하는 것은 개선을 제공할 수 있지만 이러한 모델은 여전히 인간 성능에 상당히 뒤처진다.1\n' +
      '\n' +
      '각주 1: 사용 가능한 코드 및 데이터\n' +
      '\n' +
      '[https://snap-research.github.io/locomo](https://snap-research.github.io/locomo)\n' +
      '\n' +
      '각주 2: 동등한 조언.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '확장된 컨텍스트(Bertsch et al., 2024; Xiao et al., 2023)에 대한 LLM에 기초한 대화 모델의 최근 발전에도 불구하고, 검색 증강 생성(RAG) 기술의 통합(Shuster et al., 2021; Ram et al., 2023; Shi et al., 2023)은 여전히 매우 긴 대화를 처리하는데 있어서 그들의 효능에 대한 철저한 평가가 필요하다. 실제로, 장기 오픈 도메인 대화에서 연구는 5개의 채팅 세션에 걸쳐 \\(\\sim\\)1K 토큰과 같은 제한된 컨텍스트 내에서 모델 응답을 평가하는 데 집중했다(Xu et al., 2022; Jang et al., 2023; Zhang et al., 2023). 이 장기적인 평가는 과거 상호 작용의 주요 정보를 기억할 수 있는 매력적인 챗봇을 정제하여 공감적이고 일관적이며 유용한 반응을 생성하는 데 중요하다.\n' +
      '\n' +
      '도 1: LoCoMo.**의 **예: 다이얼로그들은 화자들의 페르소나들에 의해 조종되고 대응하는 이벤트들, 예를 들어, 조안나의 응답들은 그녀의 애완동물 알레르기와 일치한다. 네이트의 경우, 이벤트 _got a new dog_ 다음에 이웃의 dog_와 _playdate가 뒤따르며, 장기 기억을 보여준다. 멀티모달 대화 상자는 이미지 공유 및 이미지 응답 동작으로 활성화됩니다.\n' +
      '\n' +
      '이를 위해 우리는 먼저 LLM 기반 생성 에이전트를 사용하여 대화를 생성한 다음 인간 주석자에게 대화의 장기적인 불일치를 수정하도록 요청하는 인간 기계 파이프라인을 통해 수집된 실제 온라인 상호 작용을 밀접하게 미러링하는 매우 장기적인 개방형 도메인 다중 모드 대화의 첫 번째 연구를 제시한다. 구체적으로, 현실 세계 대화가 집단 기억(Assmann and Czaplicka, 1995; Hirst and Manier, 2008), 개별 관점(Hirst et al., 2018), 외부 영향(Hirst and Echterhoff, 2012) 및 화자의 고유한 페르소나(Pruitt and Grudin, 2003; Cooper, 1999; Zhou et al., 2020; Shum et al., 2020)의 복합적인 혼합이라는 이해를 바탕으로, 우리는 LLM 에이전트를 기반으로 하는 _very long-term_ dialogues를 다음과 같은 특징으로 생성한다: (1) 고유한 페르소나(SS3.1); (2) 그들의 삶에서 인과적으로 연동된 사건들의 타임라인(SS3.2); 및 (3) 대화 이력(Park et al. (2023)에서와 같이) 및 이미지에 송신하거나 반응하는 _image sharing & image reaction_ behavior(SS3.3)에 기초하여 응답하는 _reflect & response_ mechanism. 마지막으로, 인간 주석자는 대화의 장거리 불일치를 수정하고, 관련 없는 이미지를 제거하고, 대화의 이벤트 접지를 확인한다(SS3.4). 이 파이프라인으로, 우리는 최대 35 세션에 걸쳐 각각 300 턴 및 avg 상의 9K 토큰으로 구성된 50 _very long-term_ dialogues의 데이터세트인 LoCoMo를 생성한다(도 1 및 표 1 참조).\n' +
      '\n' +
      '개방형 도메인 대화에서 대화 에이전트를 평가하기 위한 종래의 접근법은 과거 대화 이력에 기초하여 에이전트 응답을 직접 평가하는 것을 포함한다. 흔히, 지면 진실과 에이전트 반응 사이의 어휘 중복(Papineni et al., 2002) 및 의미 중복(Zhang et al., 2019)을 채용하거나, 또는 일관성(Ghazarian et al., 2022), 모순(Nie et al., 2021; Welleck et al., 2019), 및 에이전트 반응의 공감(Zhang et al., 2021, 2022)을 채용한다. 그러나 이러한 평가 지표는 에이전트의 장기적 맥락에 대한 이해도를 직접 평가하기에 적합하지 않다.\n' +
      '\n' +
      '본 연구에서는 에이전트의 인간 숙련도를 평가하기 위한 총체적인 평가 프레임워크를 제시한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline\n' +
      '**Dataset** & **Avg. turns** & **Avg. sessions** & **Avg. tokens** & **Time Interval** & **Multimodal** & **Collection** \\\\  & **per conv.** & **per conv.** & **per conv.** & & & \\\\ \\hline MPChat (Ahn et al., 2023) & 2.8 & 1 & 53.3 & - & ✓ & Reddit \\\\ MMDiolog (Feng et al., 2023) & 4.6 & 1 & 72.5 & - & ✓ & Social media \\\\ Daily Dialog (Li et al., 2017) & 7.9 & 1 & 114.7 & - & ✗ & Crowdsourcing \\\\ SODA (Kim et al., 2023) & 7.6 & 1 & 122.4 & - & ✗ & LLM-generated \\\\ MSC (Xu et al., 2022) (train; 1-4 sessions) & 53.3 & 4 & 1,225.9 & few days & ✗ & Crowdsourcing \\\\ Conversation Chronicles (Jang et al., 2023) & 58.5 & 5 & 1,054.7 & few hours - years & ✗ & LLM-generated \\\\\n' +
      '**LoCoMo (ours)** & 304.9 & 19.3 & 9,209.2 & few months & ✓ & LLM-gen. + crowdsourc. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **기존 대화 데이터셋 대비 LoCoMo**의 통계량. LoCoMo에서 대화의 평균 길이는 MSC(Xu et al., 2022)의 9배이며, 6배 더 많은 턴과 4배 더 많은 세션(평균)에 걸쳐 분포된다.\n' +
      '\n' +
      '그림 2: **평가 프레임워크 개요. 우리는 매우 장기적인 대화에서 모델의 이해도를 평가하기 위해 질문 응답, 이벤트 요약 및 멀티모달 대화 생성의 세 가지 과제를 제안한다.\n' +
      '\n' +
      '장기적인 컨텍스트 내에서 노화 및 대응(도 2 참조) 첫째, 에이전트는 관련 정보를 미래의 응답에 통합하기 위해 과거의 컨텍스트를 올바르게 "리콜"해야 한다. 우리는 _question answering_ (QA) task (SS4.1)를 통해 그들의 _memory_에 대한 직접적인 검사를 제시한다. 우리는 질문을 단일 홉, 다중 홉, 시간적, 상식 또는 세계 지식, 적대적 등 다양한 관점에서 기억을 평가하기 위해 다섯 가지 별개의 추론 유형으로 분류한다. 둘째, 에이전트는 또한 공감적 및 관련 반응을 생성하기 위해 대화에서 장거리 인과적 및 시간적 연결을 인식할 필요가 있다. 본 논문에서는 SSS4.2를 이용한 인과적 이해와 시간적 이해의 측정 방법을 제안한다. 이 작업에서는 각 LLM 화자에 연결된 이벤트 그래프가 정답 역할을 하며, 모델은 대화 이력에서 이 정보를 추출하는 작업을 수행한다. 셋째, 대화 에이전트는 진행 중인 내러티브와 일치하는 응답을 생성하기 위해 과거 대화에서 회상된 관련 컨텍스트를 활용할 필요가 있다. 우리는 _multi-modal dialog generation_ task(SS4.3)를 통해 이 능력을 평가한다.\n' +
      '\n' +
      '본 논문에서는 명령어 기반 LLMs, Long-context LLMs, RAG 기법(SS5)을 사용하여 LoCoMo 벤치마크에 대한 광범위한 실험 결과를 제시한다. 우리의 연구 결과는 다음과 같다.\n' +
      '\n' +
      '(1) Long-context LLMs 및 RAG는 QA 태스크에서 효과를 입증하여 LLMs의 \'emory\' 능력을 개선(22-66% 범위의 개선)하지만 여전히 인간 수준(56% 이하), 특히 시간적 추론(73% 이하)에 크게 뒤처진다;\n' +
      '\n' +
      '(2) Long-context LLMs는 QA 과제에서 적대적 질문과 관련하여 상당한 어려움을 보여 기본 모델보다 83% 낮은 성능을 보여준다. 특히 잘못된 스피커에 대한 대화 또는 이벤트가 누락되는 경향이 있습니다. 더욱이, 그들은 _event 그래프 요약_에서 열등한 성능을 보이며, 기본 모델에 14% 뒤처져 전체 대화 내에서 사실 요소를 파악할 수 있지만 컨텍스트를 정확하게 이해하지 못한다는 것을 나타낸다; 및\n' +
      '\n' +
      '(3) RAG는 짧은 문맥 LLM의 정확성과 넓은 문맥 LLM의 광범위한 이해를 결합한 균형 잡힌 절충안을 제공하며, 대화들이 각 화자의 삶과 페르소나에 대한 주장(_observations_)의 데이터베이스로 변환될 때 특히 잘한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      'Long-term Dialogue.Recent approaches involves retrieving historical context from range of previous dialogues and reasoning over retrieved segment over temporal order Lee et al. (2023); Lu et al. (2023); Zhong et al. (2023); Liang et al. (2023) and/or using events to scaffold the dialog Jing et al. (2023); Zhang et al. (2023). 이러한 프레임워크의 몇 가지 한계는 다음과 같다. (1) 검색 모델은 일반적으로 이러한 대화에서 특정하기보다는 의미적 유사성에 초점을 맞춘 작업에 대해 훈련되기 때문에 검색의 정확도가 손상될 수 있다. 추가적으로, 실제-세계 대화들은 종종 공동-참조들 및 누락된 내용(_i.e.,_Anaphora) Anantha et al. (2021)을 특징으로 하며, 이는 검색 프로세스 Mallen et al. (2023); Gao et al. (2023); Liu et al. (2023); (2) 특히 모델이 검색된 데이터 Liu et al. (2024); (3) 시간 간격들에 대한 추론은 문제들을 제시한다. 예를 들어, 시스템이 과거 이벤트들에 대해 응답하는 방식은 마지막 대화 Zhang et al.(2023); Jang et al.(2023) 이후 경과한 시간의 양에 따라 달라질 수 있다. 따라서 장기적인 대화 생성에 대한 접근법의 효과를 정확하게 평가하기 위해서는 체계적인 평가 프레임워크뿐만 아니라 상당한 길이의 대화를 갖는 것이 필수적이다. 본 논문에서는 검색 증강과 이벤트 그래프를 기반으로 한 장기 대화 생성 파이프라인을 설계하고, 장기 대화 에이전트를 평가하기 위한 프레임워크를 제안한다.\n' +
      '\n' +
      '멀티모달 대화.멀티모달 대화는 주로 이미지 기반 대화와 이미지 공유 대화의 두 가지 유형의 작업으로 구성된다. 이미지 기반 대화 태스크는 Antol et al. (2015); Das et al. (2017); Kottur et al. (2019) 또는 특정 이미지 Mostafazadeh et al. (2017); Shuster et al. (2020); Meng et al. (2020); Zheng et al. (2022). 반대로, 이미지 공유 대화 태스크는 제공된 대화 컨텍스트 Zang et al. (2021); Feng et al. (2023); Lee et al. (2023). 이미지 공유 대화 작업에서 얻은 방법을 사용하여 이미지 기반 대화 작업으로 평가되는 멀티모달 대화 상자를 만듭니다.\n' +
      '\n' +
      '합성 평가 벤치마크.인간 생성 데이터의 부족에 직면하고 LLM이 인간 수준 주석의 품질에 접근하고 있음을 관찰하는 것; He et al.(2023); Lee et al.(2023)은 이러한 개발로부터 영감을 얻는 연구가 급증했다. 결과적으로, 많은 연구가 LLM을 활용하여 일상적인 사회적 상호작용에서 반응을 평가하기 위한 대규모 대화 벤치마크를 보완하거나 합성하기 시작했으며(2023), 다중 모드 환경에서 반응을 검사하고(2023), 특정 페르소나 잔다기 등과 일치하는 반응을 평가하기 시작했다(2023). 우리는 LLM을 활용하여 데이터를 만들지만 인간 검증 및 편집으로 높은 품질을 보장합니다.\n' +
      '\n' +
      '##3 LoCoMo용 생성 파이프라인\n' +
      '\n' +
      'LoCoMo에 대한 생성 파이프라인의 개요는 그림 3과 같다. 우리는 각각 LLM(\\mathcal{M}\\)(_i.gpt-3.5-turbo)으로 초기화된 \\(\\mathcal{L}_{1}\\)과 \\(\\mathcal{L}_{2}\\)의 두 가상 에이전트를 생성한다. 이를 위해 각 에이전트에 고유한 페르소나 문장\\(p\\)을 할당하여 각 에이전트의 대화체들(SS3.1)에 고유한 성격의 통합을 보장한다. 실제 경험을 반영하기 위해 각 에이전트에 대한 시간 이벤트 그래프 \\(\\mathcal{G}\\)를 생성하는데, 이는 실제 생활 이벤트의 시퀀스(SS3.2)를 보여준다. LLM 에이전트 아키텍쳐 Park 등(2023)은 각 에이전트 \\(\\mathcal{L}_{i}\\)에 대해 활용되어, 대화 이력을 효과적으로 암기하고 진행 중인 대화(SS3.3)에 반영할 수 있다. 또한, 각 에이전트\\(\\mathcal{L}_{i}\\)은 일관성 있는 이미지를 공유할 수 있어 멀티모달 대화 양상을 향상시킬 수 있다. 마지막으로, 인간 주석자는 생성된 데이터를 수동으로 필터링하고 정제하는 작업을 수행한다(SS3.4).\n' +
      '\n' +
      '### Persona\n' +
      '\n' +
      'MSC 데이터세트 Xu et al.(2022)에서 4~5개의 문장을 포함하는 초기 페르소나 문장 \\(p_{c}\\)을 선택하고, 이를 전체 페르소나 문장 \\(p\\)으로 확장하기 위해 gpt-3.5-turbo를 \\(\\mathcal{M}\\)으로 사용한다(부록 A.1의 예제 및 프롬프트 세부사항 참조). 생성된 진술은 전형적으로 다음과 같은 요소 Gao 등(2023) 중 하나 이상에 관한 세부사항을 포함한다 : 목표, 과거 경험, 일상 습관, 및 대인 관계뿐만 아니라 개인의 이름, 나이, 및 성별.\n' +
      '\n' +
      '### 시간 이벤트 그래프\n' +
      '\n' +
      '대화에서 각 에이전트의 실제 경험을 활용하기 위해 각 에이전트에 대해 \\(\\mathcal{G}\\)로 표시된 시간 이벤트 그래프를 구성한다. 이 그래프는 사건(e_{i}\\)으로 구성된 \\(\\mathcal{G}\\)을 지정된 페르소나 \\(p\\)에 \\(\\mathcal{M}\\)(즉, text-davinci-003)의 조건을 적용하여 생성된다. 각 사건\\(e_{i}\\)은 발생일\\(t_{i}\\)과 연관된다. \\ (\\mathcal{G}\\)는 사건들 사이의 인과관계를 설명하는 인과관계 \\(l=(e_{i},e_{j})\\)을 포함하고, 사건들의 자연스러운 연계를 개인의 삶에 반영한다. 각 \\(\\mathcal{G}\\)에 대해, 우리는 추론 시간과 시간 및 인과 관계의 일관성 사이의 균형을 이루는 반복 프로세스에서 6개월에서 12개월의 시간 프레임에 걸쳐 최대 25개의 이벤트를 생성한다. 초기에, \\(k=3\\) 이벤트들의 작은 배치가 생성되고, 이어서 \\(k\\) 이벤트들의 후속 배치를 생성하기 위해 입력 프롬프트로서 반복적으로 사용된다. 부록 A.2의 세부사항을 참조하십시오.\n' +
      '\n' +
      '가상 에이전트 아키텍처\n' +
      '\n' +
      '모든 에이전트\\(\\mathcal{L}_{i}\\)는 생성 에이전트 아키텍처 Park et al.(2023)의 모듈을 통합한다. 에이전트는 (1) _reflect & respond_와 (2) _image sharing & image reaction_의 두 가지 기능을 가진다. 에이전트는 대화의 문맥 내에서 현명하고 적절하게 _image sharing & image reaction_ function을 채용하면서 _reflect & response_ function을 주로 사용하도록 요청받는다.\n' +
      '\n' +
      'Reflect & Respond.각 에이전트가 _reflect and response_를 하기 위한 기본적인 과정은 단기 및 장기 기억의 개념을 포함한다. 추론 과정에서 에이전트\\(\\mathcal{L}_{i}\\)는 단기 기억과 장기 기억 모두에 대한 반응을 조건화하며, 인간이 최근 대화를 기억하는 방법과 병행하면서 장기 기억에서 증류된 중요한 경험을 회상한다. 각 세션 \\(k\\)에 이어서, 각 에이전트는 요약 \\(w_{k}\\)을 생성하도록 요청받고, 이 요약은 단기 \\(\\mathcal{H}_{s}\\)에 저장된다. 이 요약 \\(w_{k}\\)은 가장 최근의 세션 대화 이력 \\(h_{k}\\)과 이전의 요약 \\(w_{k-1}\\in\\mathcal{H}_{l}\\)에 대한 조건화 \\(\\mathcal{M}\\)에 의해 생성된다. 세션\\(k\\) 내의 각 턴\\(j\\)에 대해 대화의 한 턴\\(h_{k_{j}}\\)을 관찰\\(o_{k_{j}}\\)으로 변환한 후 장기 기억\\(\\mathcal{H}_{l}\\)에 저장한다. 그리고 에이전트\\(\\mathcal{L}_{i}\\)은 날짜\\(t_{k+1}^{s}\\)에 대한 응답(k+1\\)을 가장 최근의 요약\\(w_{k}\\), 검색된 관련 관측치\\(o\\in\\mathcal{H}_{s}\\), 현재 세션에서 진행 중인 대화 이력\\(h_{k+1}\\) 및 페르소나 진술\\(p\\)에 기초하여 생성한다. 마지막 세션과 현재 세션 사이에 발생하는 \\(\\{e\\in\\mathcal{G}\\,|\\,t_{k}^{s}\\,<\\,t_{i}^{e}\\,<\\,t_{k+1}^{s}\\,\\}\\)에서 이벤트의 부분집합에 대한 에이전트의 반응을 추가적으로 조절함으로써 대화에서 장기적인 시간적 서사를 유도한다. 부록 A.2.1의 세부사항을 참조하십시오.\n' +
      '\n' +
      'Image Sharing & Image Reaction.The _image sharing & image reaction_ functions integrated to add multi-modal dimension of the long-term dialogues.2 The _image sharing_ function is called when the agent decides to send a image. 이 과정은 (1) \\(\\mathcal{M}\\)을 사용하여 의도된 이미지에 대한 캡션 \\(c\\)을 생성하는 단계; (2) \\(\\mathcal{M}\\)을 사용하여 캡션 \\(c\\)을 관련 키워드 \\(w\\)으로 변환하는 단계; (3) 웹 검색을 통해 이미지를 찾기 위해 키워드 \\(k\\)을 사용하는 단계; (4) 선택된 \\(이미지 \\(이미지 \\)을 공유하는 단계를 포함한다. 반대로, _image reaction_ function은 다른 에이전트로부터 이미지를 수신하면 트리거되며 수반된다: (1) 수신된 이미지에 대한 캡션 \\(c\\) 생성; (2) \\(\\mathcal{M}\\)을 사용하여 응답으로 수신된 이미지에 대한 반응을 생성한다(부록 A.2.1 참조).\n' +
      '\n' +
      '각주 2: 이미지 캡션도 장기 메모리에 저장됩니다.\n' +
      '\n' +
      '각주 3: [https://pypi.org/project/icrawler/](https://pypi.org/project/icrawler/)\n' +
      '\n' +
      '각주 4: 캡션 모델로 BLIP-2(Li et al., 2023b)를 사용한다.\n' +
      '\n' +
      '### 인간 검증 및 편집\n' +
      '\n' +
      '결론 단계에서 인간 주석자는 (1) 장기적인 불일치를 제거하기 위해 대화를 편집하고, (2) 관련 없는 이미지를 제거하거나 대체하며, (3) 이벤트 그래프와 대화 내용 간의 정렬을 검증하고 편집하는 작업을 수행한다. 전반적으로 주석이 대화상자의 거의 15%를 편집하고 제거하거나 대치하는 것을 관찰했다. LLM 생성 데이터 세트에 19% 이미지가 있습니다. 부록 A.3의 일부 편집 예제를 참조하십시오.\n' +
      '\n' +
      '##4 LoCoMo 평가 벤치마크\n' +
      '\n' +
      '섹션 3에서 생성된 다이얼로그를 기반으로 _long-term memory_의 정확도를 평가하기 위해 세 가지 태스크로 구성된 평가 벤치마크(도 2 참조)를 소개한다. 부록의 표 5의 데이터 세트 및 평가 벤치마크 통계를 참조하십시오.\n' +
      '\n' +
      '질문 응답 작업\n' +
      '\n' +
      '대화 에이전트는 이전 대화를 기억하기 위해 _memory_를 보유할 것으로 예상되며, 이는 향후 대화에서 더 매력적인 응답을 생성하기 위해 반영된다. 이_memory_에 대한 포괄적인 평가를 위해, 다섯 가지 별개의 추론 범주로 나누어진 질문-응답 태스크를 소개한다: (1) **Single-hop** 질문은 단일 세션을 기반으로 답변을 필요로 한다; (2) **Multi-hop** 질문은 여러 다른 세션으로부터의 정보를 합성해야 한다; (3) **Temporal reasoning** 질문은 시간 추론을 통해 답변할 수 있고 대화 내에서 시간 관련 데이터 단서를 캡처할 수 있다; (4) **Open-domain knowledge** 질문은 화자의 제공된 정보와 상식 또는 세계 사실과 같은 외부 지식을 통합함으로써 답변할 수 있다; (5) **Adversarial** 질문은 에이전트가 응답할 수 없는 것으로 올바르게 식별할 것이라는 기대와 함께 에이전트가 오답을 제공하도록 설계된다.\n' +
      '\n' +
      '각 범주에 대해 예측된 진리 답과 실제 진리 답의 정규화에 따라 정확한 일치에 대한 F1 점수를 계산한다. 그러나 자동화된 메트릭으로 긴 형태의 답변을 평가하는 것은 종종 과제를 제시한다(Xu et al., 2023). LLM은 다양한 형식으로 패러프레이즈된 응답을 생성하는 경향이 있어 정확한 일치 평가를 복잡하게 만든다. 작업에서 평가를 단순화하기 위해 QA 주석의 답변이 가능한 한 대화에서 직접 추출되도록 합니다. LLM에 가능한 경우 대화에서 정확한 단어를 복제하도록 지시하고 예측을 평가하기 위해 F1 부분 일치 메트릭을 사용한다. 각각의 QA 샘플은 또한 답변을 포함하는 대화 로그 내의 턴 ID들로 주석이 달린다. 우리는 RAG 모델에 대한 정확한 컨텍스트 검색의 정확성을 보고한다.\n' +
      '\n' +
      '그림 3: **LoCoMo에 대한 생성 파이프라인의 개요. 각 LLM 에이전트는 파일에 별개의 페르소나 및 인과적으로 연결된 이벤트의 타임라인이 할당됩니다. 에이전트에는 대화 상자 생성을 위한 관련 이력을 검색할 수 있는 메모리 및 반사 모듈이 장착되어 있으며 이미지 공유 및 이미지 반응 동작(왼쪽)에도 사용할 수 있습니다. 생성된 대화는 인간 주석자에 의해 편집되어 장거리 일관성(오른쪽)을 유지한다.**\n' +
      '\n' +
      '### 이벤트 요약 작업\n' +
      '\n' +
      '이 대화는 페르소나 문장(p\\)에 LLM을 컨디셔닝하여 구성한 시간 이벤트 그래프 \\(\\mathcal{G}\\)를 기반으로 생성되며, 이는 개인의 삶에서 사건의 연대순을 반영한다. 대화 에이전트는 \\(\\mathcal{G}\\)에서 인과관계와 사건의 순서를 이해할 뿐만 아니라 필요에 따라 이러한 사건들을 재계산할 수 있을 것으로 기대된다. 에이전트의 이벤트 동역학 파악을 평가하기 위해 에이전트가 지정된 시간 내에 이벤트를 요약하기 위해 도전하는 이벤트 요약 태스크를 소개하고 에이전트의 요약과 \\(\\mathcal{G}\\)의 이벤트를 비교한다. LoCoMo에서의 사건들은 기존의 연구 논문 Li et al.(2023), 영화 대본 Chen et al.(2022), 책 Krycsinski et al.(2022), 이메일 Zhang et al.(2021) 등의 요약 벤치마크와 대조적으로, 대화들에 존재하는 시간적 및 인과적 공동 참조들로 인해 요약하기 어려운 생활 사건들의 조밀하게 주석이 달린 목록들이다.\n' +
      '\n' +
      'BLEU Papineni et al. (2002) 및 ROGUE Lin (2004)과 같은 전통적인 메트릭은 요약에서 사실적인 정확성을 강조하기 때문에 참조와 생성된 요약 사이의 어휘 유사성에 초점을 맞추지 않는다. 이러한 맥락에서, 우리는 참조와 가설을 모두 원자적 사실로 분해하여 생성된 텍스트의 사실성을 평가하는 방법인 FactScore Min et al.(2023)을 채용한다. (1) \\(\\mathcal{G}\\)에 해당하는 내용 내에서 원자적 사실의 수를 카운트하여 요약된 내용의 _precision_를 측정하는 메트릭을 적용하고, (2) \\(\\mathcal{G}\\)의 원자적 사실이 내용 내에서 얼마나 포괄적으로 표현되는지를 결정하여 요약된 내용의 _recall_를 측정한다. 계산된 정밀도와 재현율로부터 도출된 F1 점수를 제시한다.\n' +
      '\n' +
      '### 멀티모달 대화 생성 작업\n' +
      '\n' +
      '데이터세트의 대화는 특정 페르소나 \\(p\\)와 그에 맞는 사건 \\(\\mathcal{G}\\)에 고정되어 있다. 대화의 주제는 몇 주 또는 몇 달에 걸쳐 이전 대화에서 소개된 이벤트에서 진화한다. 이 구조는 대화 에이전트가 시간이 지남에 따라 일관된 페르소나와 지속적인 내러티브를 유지할 수 있는지 여부를 평가할 수 있게 한다. 예를 들어, 만약 화자가 최근에 부상을 입었다면, 다음 대화는 모험적인 활동에 참여하기 보다는 회복하는 것에 초점을 맞출 것이다. 예측된 다중 모달 대화 상자가 데이터 세트의 그라운드 트루스 다중 모달 대화 상자와 얼마나 밀접하게 정렬되는지 측정하여 이러한 일관성을 평가하며, 다른 NLG 메트릭 외에도 MMRevance Feng 등(2023)을 통해 이 정렬을 정량화한다.\n' +
      '\n' +
      '## 5 실험 설정\n' +
      '\n' +
      '질문-답변 및 이벤트 요약 작업을 위해 LoCoMo의 이미지를 그들의 캡션 Li 등(2023)으로 대체하고, 이미지 캡션과 인터리빙된 텍스트 전용 다이얼로그를 추론하기 위해 최첨단 LLM을 사용한다. 다중 모드 대화 상자 생성 작업에만 이미지를 직접 사용합니다. 부록 C의 추가 세부 정보를 참조하십시오.\n' +
      '\n' +
      '질문 응답.우리는 (1) **베이스** 초기 대화들이 생략된 제한된 컨텍스트 길이들로 동작하는 LLMs들, 즉 Mistral-7B Jiang et al.(2023), LLama-70B-chat Touvron et al.(2023), gpt-3.5-turbo 5, gpt-4-turbo 6; (2) 확장된 컨텍스트 윈도우를 갖는 **Long-context** LLMs들, 즉 gpt-3.5-turbo-16k; (3) **검색-증강 생성(RAG)**은 대화 이력, 관찰(화자에 대한 주장; SS3.3, 도 9 참조), 또는 세션-레벨 요약(SS3.3, 도 8 참조)을 포함한다. 리트리버로는 DRAGON Lin et al.(2023)을, 리더로는 gpt-3.5-turbo-16k를 사용한다.\n' +
      '\n' +
      '각주 5: [https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5)\n' +
      '\n' +
      '각주 6: [https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)\n' +
      '\n' +
      '이벤트 요약.우리는 질문-답변 태스크에서 **Base** 및 **Long-context** 설정을 사용하는 실험을 제시하지만, 요약은 특정 부분을 검색하는 것이 아니라 전체 대화의 포괄적인 이해가 필요하기 때문에 RAG를 포함하는 것을 삼간다. 우리는 점진적 요약(incremental summarization)을 구현하는데, 즉 선행 세션들의 요약을 반복적으로 생성한 다음 그 요약을 기초로 하여 후속 세션 Chang 등을 요약한다(2023).\n' +
      '\n' +
      '다중-모달 대화 생성.우리는 훈련 데이터에 대한 자동화된 파이프라인(인간 필터링 없이; SS3)을 사용하여 50개의 대화를 생성하고 MiniGPT-5 Zheng et al. (2023): (1) 사전 대화 턴에서만 **Base** 열차; (2) 사전 대화 턴에서 **+ 요약** 열차 및 진행 중인 대화의 전역 요약; (3) 사전 대화 턴에서 **+ 관찰** 열차 및 대화 이력에서 검색된 관찰. 각 런은 MMDialog Feng 등(2023)에서 조정된 MiniGPT-5 체크포인트 피니트로 초기화된다.\n' +
      '\n' +
      '## 6 실험 결과\n' +
      '\n' +
      '우리는 질문 응답(SS6.1), 이벤트 그래프 요약(SS6.2), 다중 모달 대화 생성(SS6.3)을 위한 모든 기본 방법의 포괄적인 성능을 평가하고 분석한다.\n' +
      '\n' +
      '질문 응답 작업\n' +
      '\n' +
      '표 2와 표 3은 질문 응답 과제에 대한 수행 결과를 제시하고 있다. 맥락 길이가 제한된 **(1) LLM은 잘린 컨텍스트 창으로 인해 극도로 긴 대화를 이해하는 데 어려움을 겪는다. gpt-4-터보가 전체 점수가 32.4인 최고 성능 모델로 부상했음에도 불구하고 인간 벤치마크인 87.9보다 특히 뒤떨어지며, **(2) 긴 컨텍스트 LLM은 더 긴 내러티브를 이해할 수 있지만 환각을 일으키기 쉽다. gpt-3.5-turbo-16k는 다른 접근 방식보다 성능이 우수하지만, 적대적 질문에 대한 성능은 Llama-2-Chat을 사용한 22.1%, 4K 컨텍스트 창을 사용한 GPT-4-turbo를 사용한 70.2%에 비해 2.1%로 떨어진다. 이는 LLM이 긴 컨텍스트를 받을 때 환각을 생성하는 데 쉽게 오도될 수 있음을 나타내며, **(3) RAG는 대화가 관찰로 저장될 때 효과적이다. 순수 대화 로그 대신 입력이 상위 5개의 관련 관찰일 때 gpt-3.5-터보로 눈에 띄는 5% 개선이 있다. 이러한 개선은 검색된 관찰의 수가 증가함에 따라 흔들리며, 이는 모델이 컨텍스트를 정확하게 활용하기 위해 검색된 컨텍스트에서 신호 대 잡음(SNR) 비율을 줄이는 것이 중요함을 시사한다. 반대로 세션 요약을 컨텍스트로 사용하면 높은 재현율 7에도 불구하고 성능이 크게 향상되지 않으며, 이는 대화 상자를 요약으로 변환하는 동안 정보가 손실되기 때문일 수 있다.\n' +
      '\n' +
      '각주 7: 요약 기반 RAG 모델들에 대해, 리콜 정확도는 관련 세션(들)의 요약을 검색하는 것에 기초한다.\n' +
      '\n' +
      '흥미로운 발견은 **시간 추론과 열린 도메인 지식 질문이 가장 어려운 시나리오**라는 것이다.\n' +
      '\n' +
      '(1) LLMs는 대화 내에서 시간 개념을 이해하는 데 어려움을 겪는데, 이는 LLMs에 대한 시간 추론 능력에 초점을 맞춘 다른 단일 턴 기반 벤치마크의 결과와 일치한다[23].\n' +
      '\n' +
      '(2) LLM은 오픈 도메인 지식에 어려움을 겪고 RAG 설정에서 저하된다. 이는 특정 오픈 도메인 지식이 모델의 매개변수 내에 포함될 수 있지만 부정확한 검색에서 부적절한 컨텍스트를 도입하면 성능이 저하될 수 있음을 시사한다[13].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Category**} & \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Context Length**} & \\multicolumn{6}{c}{**Answer Prediction (F1)**} \\\\ \\cline{4-9}  & & & Single Hop & Multi Hop & Temporal & Open Domain & Adversarial & **Overall** \\\\ \\hline \\multirow{2}{*}{\\begin{tabular}{c} Human \\\\ \\end{tabular} } & Human & - & 95.1 & 85.8 & 92.6 & 75.4 & 89.4 & 87.9 \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} Base \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} Mistral-Instruct-7B \\\\ Llama-2-Chat-70B \\\\ GPT-3.5-turbo \\\\ GPT-4-turbo \\\\ GPT-3.5-turbo-16K \\\\ \\end{tabular} } & 8K & 10.2 & 12.8 & 16.1 & 19.5 & 17.0 & 13.9 \\\\  & & 4,096 & 19.7 & 14.4 & 13.3 & 15.9 & 22.1 & 17.9 \\\\  & & 4,096 & **29.9** & 23.3 & **17.5** & **29.5** & 12.8 & 22.4 \\\\  & & 4,096 & 23.4 & **23.4** & 10.4 & 24.6 & **70.2** & **32.1** \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} Long context \\\\ \\end{tabular} } & \\multirow{2}{*}{\n' +
      '\\begin{tabular}{c} GPT-3.5-turbo-16K \\\\ \\end{tabular} } & 4K & 31.7 & 25.4 & 16.8 & 27.6 & **13.1** & 24.1 \\\\  & & **8K** & 38.8 & 31.2 & 21.0 & 35.0 & 8.4 & 25.2 \\\\ \\cline{1-1}  & & 12K & 51.1 & 40.4 & **25.0** & 36.5 & 6.4 & 33.5 \\\\ \\cline{1-1}  & & **16K** & **56.4** & **42.0** & 20.3 & **37.2** & 2.1 & **37.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: ** Base 및 Long-context 모델의 질의 응답 성능. 최적의 성능은 굵게 표시됩니다. 결과는 답변 예측을 위해 F1-점수를 기반으로 하며, 더 높으면 더 좋다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Retrieval Unit**} & \\multirow{2}{*}{**top-B**} & \\multicolumn{6}{c}{**Answer Prediction (F1 score)**} & \\multicolumn{6}{c}{**Recall Accuracy (R@\\(k\\))**} \\\\ \\cline{3-11}  & & Single & Multi & Temporal & Open & Adver- & **Overall** & Single & Multi & Temporal & Open & Adver- & **Overall** \\\\ \\cline{2-11}  & & Hop & Hop & Domain & -sarial & Hop & Hop & Hop & Domain & -sarial & \\\\ \\hline \\multirow{2}{*}{\\begin{tabular}{c} None \\\\ \\end{tabular} } & - & 29.9 & 23.3 & 17.5 & 29.5 & 12.8 & 22.4 & - & - & - & - & - & - \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} Dialog \\\\ \\end{tabular} } & 10 & 42.9 & 19.4 & 21.3 & 35.8 & 31.9 & 31.7 & 66.2 & 34.4 & 89.2 & 38.5 & 45.7 & 58.8 \\\\  & 10 & 46.3 & 26.8 & 24.8 & 37.5 & **29.8** & 34.6 & 72.8 & 247.4 & 97.3 & 53.8 & 54.3 & 67.5 \\\\  & 25 & 48.1 & 36.1 & **26.2** & **43.4** & 23.4 & **35.8** & 87.5 & 64.1 & **97.3** & **67.9** & **69.1** & 79.9 \\\\  & 50 & **50.9** & **37.2** & 24.6 & 38.3 & 17.0 & 34.8 & **90.4** & **75.8** & 97.3 & 67.9 & **77.7** & **84.8** \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} Observation \\\\ \\end{tabular} } & 5 & 44.3 & 30.6 & 41.9 & 40.2 & **44.7** & **41.4** & 52.9 & 40.1 & 81.1 & 38.5 & 29.8 & 49.6 \\\\  & 10 & 42.2 & 30.5 & **42.1** & 41.9 & 36.2 & 38.8 & 57.4 & 53.1 & **83.8** & 46.2 & 41.5 & 57.1 \\\\  & 25 & **44.6** & 33.2 & 41.8 & 41.9 & 27.7 & 38.0 & **71.3** & 63.8 & 83.8 & 66.7 & 45.7 & 66.0 \\\\  & 50 & 44.0 & **34.5** & 41.1 & **41.9** & 27.7 & 37.8 & 72.8 & **73.2** & 83.8 & **74.4** & **56.4** & **71.1** \\\\ \\hline \\multirow{4}{*}{\n' +
      '\\begin{tabular}{c} Summary \\\\ \\end{tabular} } & 2 & 34.6 & 15.7 & 26.9 & 26.5 & 36.2 & 29.9 & 68.4 & 39.6 & 56.8 & 50.0 & 73.4 & 61.5 \\\\  & **36.6** & **16.6** & **31.0** & **34.7** & 38.3 & **32.8** & 81.6 & 57.0 & 70.3 & 60.3 & 86.2 & 75.1 \\\\ \\cline{1-1}  & 10 & 34.5 & 14.7 & 29.3 & 31.6 & **40.4** & 31.5 & **93.4** & **82.3** & **91.9** & **80.8** & **94.7** & **90.7** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: ** RAG 기반 GPT-3.5-turbo-16k의 질문 응답 성능. 최적의 성능은 굵게 표시됩니다. 결과는 답변 예측에 대한 F1-score 메트릭과 회상 정확도에 대한 recall@\\(k\\)을 기반으로 하며, 높을수록 좋다.**\n' +
      '\n' +
      '### 이벤트 요약 작업\n' +
      '\n' +
      '표 4는 이벤트 요약 작업에 대한 결과를 제시한다. *gpt-3.5-터보와 함께 점진적 요약의 사용은 회상과 F1 점수 모두에서 가장 높은 성능**로 이어진다. gpt-4-turbo는 gpt-3.5-turbo에 비해 정밀도가 5.3% 향상되었지만 회상 측면에서는 그렇지 않다. 이벤트 요약 작업은 다수의 세션에서 화자에 의해 논의된 이벤트들 사이의 시간적 및 인과적 연결들을 이해하기 위해 장거리 의존성을 요구한다(도 7 참조). 예상과 달리 **긴 컨텍스트 모델은 더 큰 컨텍스트 창에 의해 촉진되는 확장 범위 추론 능력에도 불구하고 기본 모델**을 능가하지 않는다. gpt-3.5-turbo-16k는 4K 컨텍스트 윈도우를 갖는 gpt-3.5-turbo에 비해 정밀도(3.0%) 및 재현율(8.7%) 모두에서 감소를 나타낸다. 이는 **long-context 모델이 그들의 컨텍스트를 적절하게 활용하는 데 능숙하지 않을 수 있음을 시사하며, 이는 또한 LoCoMo의 QA 작업뿐만 아니라 Li 등(2023)의 유사한 발견과 일치한다. ROUGE 및 FactScore 메트릭의 측면에서 상용 모델(gpt-4-터보, gpt-3.5-터보)은 오픈 소스 메트릭을 크게 능가한다. 그럼에도 불구하고, 이 과제에 대한 성과를 향상시킬 수 있는 상당한 범위가 남아 있다.\n' +
      '\n' +
      '예측된 요약에 대한 수동 분석에서, 우리는 (1) 모델이 긴 대화에서 시간적 및/또는 인과적 연결을 하지 못하기 때문에 이벤트에서 누락된 정보**의 5가지 광범위한 범주 이벤트 요약 오류를 식별한다. (2) 모델은 대화에 존재하지 않거나 동일한 세션에서 다른 이벤트의 일부인 추가 세부 정보를 패드한다. (3) 유머 또는 빈정거림과 같은 대화 단서의 ** 오해로 인한 오류는 대화의 이해에 있어 독특한 문제이며, (4) 부정확한 ** 화자 속성** 및 (5) 의미 없는 대화는 ** 의미 있는** 이벤트로 잘못 간주된다. 부록의 표 7의 예를 참조하십시오.\n' +
      '\n' +
      '### 멀티모달 대화 생성 작업\n' +
      '\n' +
      '그림 4는 다중 모드 대화 생성에서 다양한 MiniGPT-5 훈련 변형의 효과를 보여준다. 컨텍스트를 훈련에 통합하면 성능이 향상되며, 컨텍스트로 관찰을 포함하면 결과가 크게 향상된다. 예를 들어, 도 4a에서, 검색된 관찰들은 비디오 게임 토너먼트들에서 화자의 경험에 관한 정보를 포함하고, 이는 대화 및 이미지들의 예측으로 이어진다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Category**} & \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Context Length**} & \\multicolumn{3}{c}{**ROGUE**} & \\multicolumn{3}{c}{**FactScore**} \\\\ \\cline{3-8}  & & & & **ROGUE-1** & **ROGUE-2** & **ROGUE-L** & **Precision** & **Recall** & **F1** \\\\ \\hline \\multirow{3}{*}{Base} & \\multicolumn{2}{c}{Mistral-Instruct-7B} & 8K & 29.4 & 7.2 & 14.1 & 27.1 & 19.8 & 23.0 \\\\  & \\multicolumn{2}{c}{Llama-2-Chat-7B6} & 4,096 & 28.1 & 9.3 & 14.8 & 36.3 & 22.7 & 28.3 \\\\  & \\multicolumn{2}{c}{GPT-4-turbo} & 4,096 & 38.8 & 11.4 & 20.6 & **51.6** & 41.8 & 45.1 \\\\  & \\multicolumn{2}{c}{GPT-3.5-turbo} & 4,096 & **41.1** & **13.5** & **20.9** & 45.3 & **46.5** & **45.9** \\\\ \\hline Long context & GPT-3.5-turbo-16K & 16K & 36.2 & 8.5 & 16.4 & 42.3 & 37.8 & 39.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: Base 및 Long-context 모델의 **Event 요약 성능. 최적의 성능은 **bold**로 표시됩니다. 결과는 ROUGE 및 FactScore(Min et al., 2023) 메트릭에 기초하고; 더 높은 것이 더 좋다.**\n' +
      '\n' +
      '그림 4: **MiniGPT-5**의 멀티모달 다이얼로그 생성 성능. (A) 검색된 컨텍스트로서 관찰이 있거나 없는 MiniGPT5를 사용하여 예측된 멀티모달 다이얼로그의 예, (B) 다이얼로그 이력의 길이에 따른 MM-관련성 스코어의 변동, 및 (C) RAG 기반 MiniGPT-5 방법의 비교.\n' +
      '\n' +
      '그것은 화자의 페르소나에 더 충실하다. 이 관찰은 QA 작업의 초기 결과와도 일치한다(표 3 참조). 또한, 우리는 대화 이력의 길이가 증가함에 따라 MM-관련성 점수가 떨어지는 것을 관찰한다(도 4b 참조). 검색 강화 세대는 MM 관련성의 하락을 어느 정도 완화한다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '우리는 최대 35개 세션에 걸쳐 각각 300번의 턴과 9K 토큰을 포함하는 50개의 고품질 매우 긴 대화 데이터 집합인 LoCoMo를 수집하기 위한 인간-기계 파이프라인을 개발하고, 긴 대화에서 모델의 숙련도를 평가하는 세 가지 작업으로 구성된 평가 프레임워크를 제안한다. 우리의 실험은 LLM이 대화 상자 내에서 장기적인 내러티브를 이해하는 데 어려움을 겪고 화자가 논의한 사건 간의 시간적 및 인과적 연결을 그리지 못한다는 것을 보여준다.\n' +
      '\n' +
      '## 8 Limitations\n' +
      '\n' +
      '하이브리드 인간-기계 생성 데이터.우리의 데이터 세트는 주로 LLM에 의해 생성된 텍스트에서 조달된다. 우리는 매우 장기적인 실제 대화를 대규모로 수집하는 물류 및 법적 복잡성을 피하기 위해 시간 집약적인 수동 데이터 수집의 인기 있는 대안으로 빠르게 등장한 이 방법을 추구했다(김 외, 2023; 장 외, 2023). 우리는 인간 주석자가 생성된 대화를 확인하고 편집하도록 함으로써 데이터 세트가 실제 상호 작용을 최대한 반영하도록 한다. 그러나 이 데이터 세트가 실제 온라인 대화의 뉘앙스를 완전히 반영하지 않을 수 있음을 인정한다.\n' +
      '\n' +
      '본 논문에서 제안하는 데이터셋의 이미지는 웹에서 제공되기 때문에 개인 사진(예: 외모, 가정 환경, 사람 및 애완동물 등)에 주로 나타나는 시각적 장기 일관성을 입증하지 못한다. 따라서 OCR이 필요한 경우를 제외하고는 데이터의 손실 없이 캡션으로 대체할 수 있음을 알 수 있다. 그럼에도 불구하고 우리의 작업은 매우 장기적인 대화의 복합적 측면에 대한 연구를 향한 첫걸음이다.\n' +
      '\n' +
      '언어. 장기 대화를 생성하기 위한 LLM 기반 파이프라인은 영어 전용으로 개발되었다. 그러나 파이프라인은 해당 언어와 프롬프트의 적절한 번역에 능숙한 LLM을 사용하여 다른 언어와 함께 작동하도록 만들 수 있다.\n' +
      '\n' +
      '폐쇄 소스 LLMs. 우리는 대화 생성 파이프라인에서 최신 LLM을 사용하여 가능한 한 현실적인 대화 데이터 세트를 생성한다. 불행히도, 이는 합성 대화를 생성하는 많은 동시 작업과 유사하게 유료 API를 통해 이용 가능한 가장 강력한 상업용 LLM을 사용하는 것을 의미했다(Zhong et al., 2023; Lu et al., 2023). 우리는 우리의 생성 파이프라인 코드가 향후 최첨단 오픈 소스 LLM과 효과적으로 작동하도록 만들 수 있기를 희망하여 공개적으로 사용할 수 있도록 할 것이다.\n' +
      '\n' +
      '긴 형태의 NLG.LLM에 대한 평가는 짧은 문구로 대답하라는 프롬프트가 있는 경우에도 장황한 답변을 생성하기 쉽다. 이것은 LLMs에 의해 제공된 답변의 정확성을 평가하는 데 어려움을 야기하고 NLP 문헌에 널리 문서화되었다(Chang et al., 2023; Xu et al., 2023; Krishna et al., 2023). 우리의 평가 프레임워크는 LLM으로 실험할 때 동일한 문제를 겪는다.\n' +
      '\n' +
      '## 9 Broader Impact\n' +
      '\n' +
      '우리는 장기적인 대화의 생성을 위해 Park et al.(2023)에 소개된 생성 에이전트의 프레임워크를 채택하고 개선한다. 결과적으로, Park et al.(2023)에 의해 요약된 생성 에이전트의 윤리적 우려는 우리의 작업에도 적용되며, 특히 우리의 프레임워크의 목표는 대화를 가능한 한 사실적으로 만드는 것이기 때문이다.\n' +
      '\n' +
      '특히, 우리의 프레임워크에서 시간적 사건 그래프에 의해 가능하게 된 현실적 삶을 가진 인간으로서 포즈를 취할 수 있는 대화 에이전트는 사용자가 삶에 부정적인 영향을 미칠 수 있는 그러한 에이전트와 친사회적 관계를 형성할 수 있는 위험을 제기한다. 작업에서 언급된 생성 프레임워크의 실제 배포에는 항상 대화 상자의 출처에 대한 거부권이 있어야 합니다.\n' +
      '\n' +
      '둘째, 대화 상에 조건화된 이미지들을 생성하기 위해 멀티모달 LLM들(Zheng et al., 2023)을 사용하는 것은 특히 대화 에이전트가 거짓 정보 또는 위험한 의견들을 퍼트리는 것으로 강요될 수 있는 경우, 잘못된 정보 및 사회적 편향의 전파로 이어질 수 있다.\n' +
      '\n' +
      '셋째, 생성 에이전트를 사용하여 프로세스를 실제 인간으로 대체하는 것은 유혹적이며, 특히 1년 이상 동안 인간 간의 실제 상호 작용을 수집하는 것과 같은 특정 목표를 위해 인간과 작업하는 데 상당한 어려움이 있을 때 그렇다. 인간에게 가시적인 영향을 미치는 실제 결정을 내리는 데 결과가 사용될 수 있는 연구에서 그러한 대체물이 이루어지지 않도록 주의해야 한다. 우리의 작업은 매우 장기적인 대화에서 모델 이해에 대한 연구일 뿐이다. 우리는 이 연구를 기반으로 실제 정책에 대한 권장 사항을 만들지 않으며 이러한 권장 사항을 만들지 않도록 프레임워크의 잠재적 사용자에게 조언한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Larincke, and D. Parikh (2015)Vqa: 시각적 질문 응답. In Proceedings of the IEEE international conference on computer vision, pp. 2425-2433. Cited by: SS1.\n' +
      '* J. Assmann and J. Czaplicka (1995)collective memory and cultural identity. New german critique65, pp. 125-133. Cited by: SS1.\n' +
      '* A. Bertsch, U. Alon, G. Neubig, M. Gormley (2024)Unlimformer: 무제한 길이 입력이 있는 장거리 변압기. 신경 정보 처리 시스템36의 발전. 인용: SS1.\n' +
      '*Y. 장경 로태 고얄, 엠. Iyyer (2023)Booookscore: lms 시대의 책장 요약에 대한 체계적인 탐색. 제12차 국제 학습 표상 회의에서 SS1이 인용했다.\n' +
      '* M. 천진 추승 와이즈먼과 K Gimpel(2022)Summscreen: 추상적인 각본 요약용 데이터셋. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8602-8615. Cited by: SS1.\n' +
      '*Y. 천성호 기안홍당 라이자 류승 Han, and J. Jia (2023)Longlora: long-context large language models의 효율적인 미세 조정. 제12차 국제 학습 표상 회의에서 SS1이 인용했다.\n' +
      '* A. Cooper (1999) 모방자들이 망명을 운영하고 있다. 스프링거 인용: SS1.\n' +
      '*T. 다오대복 Ermon, A. Rudra, and C. Re(2022)Flashattention: io-awareness와 함께 빠르고 기억력이 효율적인 정확한 주의력. The Advances in Neural Information Processing Systems35, pp. 16344-16359. Cited by: SS1.\n' +
      '* A. Das, S. 코투르 Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh, and D. Batra(2017)Visual dialog. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 326-335. Cited by: SS1.\n' +
      '* J. Feng, Q. 선철수 양찬타오, 동자오, Q. Lin(2023)MDialog: 멀티모달 오픈 도메인 대화를 향한 대규모 멀티턴 대화 데이터세트. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, pp. 7348-7363. Cited by: SS1.\n' +
      '* S. 가오보르지스 오동애 카노, 와카키, Y. Mitsufuji, and A. Bosseltu (2023)PeaCoK: 일관되고 매력적인 내러티브를 위한 상식적 지식. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, pp. 6569-6591. Cited by: SS1.\n' +
      '*T. 가오, H. Yen, J. Yu, 및 D. Chen(2023)은 큰 언어 모델들이 인용들을 갖는 텍스트를 생성할 수 있게 한다. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore, pp. 6465-6488. Cited by: SS1.\n' +
      '* S. 가자리안, N. Wen, A. Galstyan, N. Peng(2022)Deam: amr 기반 의미 조작을 이용한 대화 일관성 평가. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 771-785. Cited by: SS1.\n' +
      '*X. 그, 지 임영식 공아진, H. Zhang, C. Lin, J. Jiao, S. M. Yiu, N. 두안원 Chen, et al.(2023)Annollm: 대규모 언어 모델을 더 나은 크라우드소싱 주석이 되도록 만드는 것. ArXiv:2303.16854. 인용: SS1.\n' +
      '*W. 허스트와 G. 에크터호프(2012)는 대화에서 기억한다: 기억의 사회적 공유와 재편성. 연간 리뷰 of psychology63, pp. 55-79. Cited by: SS1.\n' +
      '*W. 허스트와 D. 매니어(2008)는 집단 기억의 심리를 향한다. Memory16(3), pp. 183-200. Cited by: SS1.\n' +
      '*W. Hirst, J. K. Yamashiro, and A. Coman (2018) Collective memory from psychological perspective. Trends in cognitive sciences22(5), pp. 438-451. Cited by: SS1.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '제2회 G. Echterhoff (2012) 기억: 사회적 공유와 기억의 재구성. 연간 리뷰 of psychology63, pp. 55-79. Cited by: SS1.\n' +
      '페가 잔다기, 상하이 성, 샤이니 바이, 제이 푸자라, 하킴 시다메드. 2023. 큰 언어 모델을 가진 Faithful 페르소나 기반 대화 데이터셋 생성. _ arXiv preprint arXiv:2312.10007_.\n' +
      '* 장등(2023) 장지형, 민영부, 김현훈. 2023. 대화 연대: 다중 세션 대화에서 다양한 시간적, 관계적 역학을 향한다. _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 13584-13606, Singapore. 컴퓨터 언어학과의 연관성\n' +
      '* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. _ arXiv preprint arXiv:2310.06825_.\n' +
      '* Kim et al. (2023) Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, and Yejin Choi. 2023. SODA: 사회적 상식 맥락화와 함께 수백만 규모의 대화 증류. _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 12930-12949, Singapore. 컴퓨터 언어학과의 연관성\n' +
      '* Kottur et al. (2019) Satwik Kottur, Jose M. F. Moura, Devi Parikh, Dhruv Batra, and Marcus Rohrbach. 2019. CLEVR-dialog: 시각적 대화에서 다중 라운드 추론을 위한 진단 데이터세트. [Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics Association for Human Language Technologies, Volume 1(Long and Short Papers)_, pages 582-595, Minneapolis, Minnesota. 컴퓨터 언어학과의 연관성\n' +
      '* Krishna et al. (2023) Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. Longeval: Long-form 요약에서 신의성실의 인간 평가를 위한 지침. _Proceedings of the 17th Conference of the European chapter of the Computational Linguistics_, pages 1642-1661.\n' +
      '* Krycinski et al. (2022) Wojciech Krycinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022. Booksum: Long-form narrative 요약용 데이터셋 모음. _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 6536-6558.\n' +
      '* Lee et al. (2023a) Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen White, and Sujay Jauhar. 2023a. 대형 언어 모델을 더 나은 데이터 생성자로 만듭니다. _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 15349-15360, Singapore. 컴퓨터 언어학과의 연관성\n' +
      '* Lee et al. (2023b) Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos, and Kang욱 Lee. 2023b. 롱 오픈 도메인 대화를 위한 챗봇 모듈로 LLM을 호출했다. _Findings of the Association for Computational Linguistics: ACL 2023_, pages 4536-4554, Canada, Toronto. 컴퓨터 언어학과의 연관성\n' +
      '* 이등(2023c) 이영준, 고병수, 김한규, 종환현, 최호진 2023c. 대화: 고품질 다중 모드 대화 데이터 세트를 생성하기 위한 자동화된 파이프라인입니다. In _NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following_.\n' +
      '* Li et al.(2023a) Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023a. Loogle: Long Context 언어 모델이 Long Context를 이해할 수 있는가? _ arXiv preprint arXiv:2311.04939_.\n' +
      '* Li et al. (2023b) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: 냉동 이미지 인코더 및 대형 언어 모델을 사용한 부트스트래핑 언어-이미지 사전 훈련. In _International Conference on Machine Learning_.\n' +
      '* Li 등(2017) Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, Shuzi Niu. 2017. Dailydialog: 수동으로 레이블이 지정된 다중 회전 대화 데이터 세트. _Proceedings of the E8h International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 986-995.\n' +
      '* Liang et al. (2023) Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023. 자체 제어 메모리 시스템을 갖는 대규모 언어 모델들에 대한 무한 길이 입력 용량을 풀기 _ arXiv preprint arXiv:2304.13343_.\n' +
      '* Lin(2004) Chin-Yew Lin. 2004. ROUGE: 요약의 자동 평가를 위한 패키지. _텍스트 요약 분기 아웃_에서 스페인 바르셀로나의 74-81페이지입니다. 컴퓨터 언어학과의 연관성\n' +
      '* Lin et al. (2023) Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023. 드래곤 훈련 방법: 일반화 가능한 고밀도 검색을 위한 다양한 증강. _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 6385-6400, Singapore. 컴퓨터 언어학과의 연관성\n' +
      '* Liu et al. (2023) Nelson Liu, Tianyi Zhang, and Percy Liang. 2023. 생성 검색 엔진에서의 검증 가능성 평가. _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 7001-7025, Singapore. 컴퓨터 언어학과의 연관성\n' +
      '* Liu et al. (2024) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in Middle: How Language Models Use Long Contexts. _ Computational Linguistics_, 12:157-173의 거래.\n' +
      '* Lu et al. (2023) Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yunsheng Wu. 2023. 메모챗: llms를 조정하여 일관된 장거리 오픈 도메인 대화를 위한 메모를 사용한다. _ arXiv preprint arXiv:2308.08239_.\n' +
      '\n' +
      '알렉스 맬런, 아카리 아사이, 빅터 중, 라자르시 다스, 다니엘 카샤비, 한나네 하지시르지. 2023. 언어 모델을 신뢰하지 않는 경우: 파라메트릭 및 비모수 메모리의 유효성 조사. _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 9802-9822, Canada, Toronto. 컴퓨터 언어학과의 연관성\n' +
      '* Meng et al. (2020) Yuxian Meng, Shuhe Wang, Qinghong Han, Xiaofei Sun, Fei Wu, Rui Yan, and Jiwei Li. 2020. Openvidial: 시각적 컨텍스트를 갖는 대규모의 오픈 도메인 대화 데이터세트. _ arXiv preprint arXiv:2012.15015_.\n' +
      '* Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 12076-12100, Singapore. 컴퓨터 언어학과의 연관성\n' +
      '* Mostafazadeh et al. (2017) Nasrin Mostafazadeh, Chris Brockett, Bill Dolan, Michel Galley, Jianfeng Gao, Georgios Spithourakis, and Lucy Vanderwende. 2017. Image-grounded conversation: Multimodal context for natural question and response generation. _Proceedings of the E8h International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 462-472, Taipei, Taiwan. 아시아 자연 언어 처리 연합.\n' +
      '* Nie et al. (2021) Yixin Nie, Mary Williamson, Mohit Bansal, Douwe Kiela, and Jason Weston. 2021. 나는 물고기, 특히 돌고래를 좋아한다: 대화 모델링에서 모순을 해결한다. _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 1699-1713.\n' +
      '* Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: 기계 번역의 자동 평가 방법. _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_, pages 311-318, Philadelphia, Pennsylvania, USA. 컴퓨터 언어학과의 연관성\n' +
      '* Park et al. (2023) Joon Sung Park, Joseph O\'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. 번스틴 2023. 생성 에이전트: 인간 행동의 상호작용 시뮬라크라. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, UIST \'23, New York, NY, USA. 전산기계협회\n' +
      '* Pruitt and Grudin (2003) John Pruitt and Jonathan Grudin. 2003. Personnas: 연습과 이론. 2003년 사용자 경험 디자인 회의의 _Proceedings_, 페이지 1-15.\n' +
      '* Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shohham. 2023. 문맥 내 검색-증강 언어 모델 _ The Association for Computational Linguistics_, 11:1316-1331의 트랜잭션.\n' +
      '* Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjun Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. 리플러그: 검색-증강된 블랙박스 언어 모델들 _ arXiv preprint arXiv:2301.12652_.\n' +
      '* Shum et al. (2020) Michael Shum, Stephan Zheng, Wojciech Kryscinski, Caiming Xiong, and Richard Socher. 2020. Sketchfill-a-R: Persona-grounded chit-chat 생성 프레임워크. 대화형 AI를 위한 자연어 처리에 관한 제2 워크숍의 _Proceedings_, 페이지 118-131, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Shuster et al. (2020) Kurt Shuster, Samuel Humeau, Antoine Bordes, and Jason Weston. 2020. 이미지-채팅: 접지된 대화 참여. _Proceedings of the 58th Annual Meeting for Computational Linguistics_, pages 2414-2429, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Shuster et al. (2021) Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. 검색 증강은 대화에서의 환각을 감소시킨다. _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 3784-3803.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_.\n' +
      '* Wang and Zhao (2023) Yuqing Wang and Yun Zhao. 2023. 트램: 대용량 언어 모델에 대한 벤치마킹 시간적 추론_ arXiv preprint arXiv:2310.00835_.\n' +
      '* Welleck et al. (2019) Sean Welleck, Jason Weston, Arthur Szlam, and KyungHyun Cho. 2019. 대화 자연어 추론. _Proceedings of the 57th Annual Meeting of the Computational Linguistics_, pages 3731-3741, Florence, Italy. 컴퓨터 언어학과의 연관성\n' +
      '* Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. 트랜스포머: 최첨단 자연어 처리. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Xiao et al.(2023) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. 어텐션 싱크를 갖는 효율적인 스트리밍 언어 모델. _ arXiv preprint arXiv:2309.17453_.\n' +
      '\n' +
      '방위안 슈, 요샤오 송, 모히트 아이이어, 최은솔. 2023. Long-form 질의응답을 위한 평가들에 대한 비판적 평가. _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3225-3245, Canada, Toronto. 컴퓨터 언어학과의 연관성\n' +
      '* Xu et al. (2022) Jing Xu, Arthur Szlam, and Jason Weston. 2022년 금붕어 기억 이상 장기 개방 영역 대화 _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5180-5197.\n' +
      '* Zang et al. (2021) Xiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song, Hao Zhang, 및 Jindong Chen. 2021. PhotoChat: 공동 이미지-텍스트 모델링을 위한 사진 공유 행동을 갖는 인간-인간 대화 데이터세트. _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 6142-6152, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Zhang et al. (2021a) Chen Zhang, Yiming Chen, Luis Fernando D\'Haro, Yan Zhang, Thomas Friedrichs, Grandee Lee, and Haizhou Li. 2021a. Dynaeval: 전환과 대화 수준 평가를 통합합니다. _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 5676-5689.\n' +
      '* Zhang et al. (2022) Chen Zhang, Luis Fernando D\'Haro, Qiquan Zhang, Thomas Friedrichs, and Haizhou Li. 2022. Finedeval: Fine-grained automatic dialogue-level evaluation. _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 3336-3355.\n' +
      '* Zhang et al. (2023) Qiang Zhang, Jason Naradowsky, and Yusuke Miyao. 2023. 개선된 장기 대화 생성을 위한 대화들 간의 격차에 유의한다. _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 10735-10762, Singapore. 컴퓨터 언어학과의 연관성\n' +
      '* Zhang et al. (2021b) Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, and Mohit Bansal. 2021b. 이메일섬: 추상적인 전자 메일 스레드 요약입니다. _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 6895-6909.\n' +
      '* Zhang et al. (2019) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Bert로 텍스트 생성을 평가한다. _International Conference on Learning Representations_.\n' +
      '* Zheng et al. (2023) Kaizhi Zheng, Xuehai He, and Xin Eric Wang. 2023. Minigpt-5: Generative Vokens를 통한 Interleaved vision-and-language generation. _ arXiv preprint arXiv:2310.02239_.\n' +
      '* Zheng et al. (2022) Yinhe Zheng, Guanyi Chen, Xin Liu, and Jian Sun. 2022. MMChat: 소셜 미디어 상의 멀티모달 채팅 데이터세트. _Proceedings of the Th13 Language Resources and Evaluation Conference_, pages 5778-5786, Marseille, France. 유럽 언어 자원 협회\n' +
      '* Zhong et al. (2023) Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. 2023. 메모리 뱅크 : 장기 메모리를 갖는 대용량 언어 모델 향상 _ arXiv preprint arXiv:2305.10250_.\n' +
      '* Zhou et al. (2020) Li Zhou, Jianfeng Gao, Di Li, and Heung-Yeung Shum. 2020. Empathetic social chatbot인 xiaoice의 설계 및 구현. _ Computational Linguistics_, 46(1):53-93.\n' +
      '\n' +
      '## 부록, 부록\n' +
      '\n' +
      '부록은 다음과 같이 구성된다:\n' +
      '\n' +
      '**섹션 A**: LoCoMo 데이터세트에 대한 생성 파이프라인의 상세.\n' +
      '\n' +
      '**섹션 B**: LoCoMo 데이터 세트의 통계, 데이터 릴리스에 대한 라이센스 및 주석자 세부 정보.\n' +
      '\n' +
      '**섹션 C**: 실험 설정 및 구현 세부사항.\n' +
      '\n' +
      '**섹션 D**: LoCoMo 벤치마크에 대한 평가의 추가 결과.\n' +
      '\n' +
      '## 부록 LoCoMo를 위한 생성 파이프라인\n' +
      '\n' +
      '### Persona\n' +
      '\n' +
      '우리는 각 에이전트 \\(\\mathcal{L}_{i}\\)에 고유한 페르소나 문장 \\(p\\)을 부여한다. 이를 위해 MSC 데이터세트 Xu et al.(2022)에서 각각 4~5개의 문장을 포함하는 초기 페르소나 문장 \\(p_{c}\\)의 범위를 선택한다. 본 논문에서는 gpt-3.5-turbo를 \\(\\mathcal{M}\\)으로 사용하여 전체 페르소나 문장 \\(p\\), 조건화 \\(\\mathcal{M}\\)으로 확장한다. MSC 데이터세트 Xu et al.(2022)의 화자 속성의 짧은 목록을 완전한 페르소나 요약으로 변환하기 위해 사용된 프롬프트는 그림 5에 제시되어 있다. 또한 프롬프트와 함께 문맥내 시연으로서 화자 속성 \\(\\rightarrow\\) 페르소나 요약의 단일 예를 사용한다. LoCoMo 데이터 세트에서 화자의 다양성을 보여주는 페르소나의 작은 선택은 그림 5에 나와 있다.\n' +
      '\n' +
      '### 시간 이벤트 그래프\n' +
      '\n' +
      'Sec. 3.2에 요약된 바와 같이, 우리는 주어진 페르소나 요약에 기초하여 인과적으로 연결된 이벤트들로 구성된 이벤트 그래프들을 생성하기 위해 반복 프로세스를 사용한다. 이벤트 그래프의 구성, 이벤트의 성격 및 이벤트 간의 인과 관계를 설명하기 위한 기본 프롬프트는 그림 6에 나와 있다. 먼저, 기본 프롬프트는 주어진 성격과 관련된 세 개의 독립적인 이벤트를 생성하기 위해 이벤트 그래프 초기화를 위한 프롬프트와 함께 사용된다. 그런 다음, 기본 프롬프트는 그래프에 이미 존재하는 이벤트들 중 하나 이상에 의해 야기되는 이벤트들을 계속 생성하기 위해 이벤트들의 반복적인 생성을 위한 프롬프트와 결합된다. 페르소나 및 이에 대응하는 시간 이벤트 그래프의 예를 도 7에서 참조한다. 예에서, 잭은 호텔 매니저가 되기를 열망한다. 결과적으로, 그는 7월에 호텔 관리 코스에 등록하고, 3개월 후에 소셜 미디어에 그 코스에 대한 흥분을 표현한다. 비슷한 맥락에서, 그의 게임에 대한 열정은 유명한 게임 회사의 초청으로 귀결된다.\n' +
      '\n' +
      '#### a.2.1 가상 에이전트 아키텍처\n' +
      '\n' +
      '섹션 3.3에 요약된 바와 같이, 우리의 생성 파이프라인에서의 가상 에이전트는 _Reflect & response_Park 등(2023)과 _Image sharing & response_의 두 가지 메커니즘으로 구성된다.\n' +
      '\n' +
      '**Reflect & Response.** 이 메커니즘은 단기 및 장기 기억의 조합에 걸쳐 작동한다. 단기 기억은 이전 세션으로부터의 요약에 조건되는 세션의 요약이다. 요약을 생성하기 위해 우리의 파이프라인에서 LLM들에 주어진 프롬프트, 그리고 생성된 요약의 예를 도 8에 참조한다. 장기 기억은 화자의 페르소나 및 삶에 대한 본질적으로 주장적인 진술인 각 화자에 대한 _관찰_의 데이터베이스이다. 관찰을 생성하기 위해 파이프라인에서 LLM에 제공된 프롬프트와 대화에서 추출된 관찰의 예를 참조하세요. 그림 9에서, 실제로 대화는 각 회전에 대한 턴 ID로 주석이 달리고 모델은 또한 각 관찰에 직접 기여하는 턴 ID를 표시하도록 지시됩니다. 이를 통해 실험에 사용된 RAG 기반 모델의 컨텍스트로 관찰을 사용할 때 증거를 추적할 수 있다(섹션 5 참조).\n' +
      '\n' +
      '**이미지 공유 및 응답.** 이미지 공유 및 이미지-응답 동작을 구현하기 위한 프롬프트를 그림 10에서 참조하십시오.\n' +
      '\n' +
      '### Human Filtering\n' +
      '\n' +
      '인간 주석자들은 다음의 시나리오들에서 LLM-생성 대화들을 편집하도록 지시된다:\n' +
      '\n' +
      '* 현재 대화 또는 대화와 관련이 없는 경우 이미지를 제거합니다.\n' +
      '* 이미지에 대한 컨텍스트를 현재 화자의 다이얼로그에 추가하는데, 만약 그것이 그것들에 의해 논의되지 않지만 후속 화자가 이미지에 반응하였다.\n' +
      '* 이미지를 쿼리할 때 사용했던 캡션이 일치하지 않으면 이미지를 교체합니다.\n' +
      '\n' +
      '도 5: **페르소나 진술(\\(p\\)) 생성에 대한 프롬프트 및 LoCoMo에서의 페르소나의 예. 대화 생성 파이프라인(상단)의 가상 에이전트에 대한 초기 페르소나(\\(p_{c}\\))로부터 확장된 페르소나 진술(\\(p\\))을 생성하고 LoCoMo 데이터 세트에 존재하는 페르소나 진술의 예를 선택하는 데 사용되는 프롬프트.**\n' +
      '\n' +
      '* 다이얼로그에 존재하는 정보가 이전 또는 이후 턴에서 언급된 것(또는 이미지를 통해 공유됨)과 불일치할 때 다이얼로그를 편집합니다.\n' +
      '* 대화의 세부사항들이 세션에 대한 이벤트에서 주어진 것들과 일치하도록 다이얼로그를 편집한다.\n' +
      '* 대화에 나타나지 않으면 이벤트 그래프에서 모든 이벤트를 제거합니다.\n' +
      '\n' +
      '그림의 일부 편집의 예를 참조하십시오. 11.\n' +
      '\n' +
      '## 부록 B 데이터세트\n' +
      '\n' +
      '### Dataset Statistics\n' +
      '\n' +
      '표 5의 상단 패널에 있는 LoCoMo 데이터 세트의 대화 통계의 분석을 참조하라. 또한 표 5의 하단 패널에 있는 평가 벤치마크에서 주석의 통계의 분석을 참조하라.\n' +
      '\n' +
      '### Dataset License\n' +
      '\n' +
      'LoCoMo 데이터 세트는 CC BY-NC 4.0 DEED 라이선스.8에 따라 릴리스됩니다.\n' +
      '\n' +
      '각주 8: [https://creativecommons.org/licenses/by-nc/4.0/](https://creativecommons.org/licenses/by-nc/4.0/)\n' +
      '\n' +
      '### Annotator Details\n' +
      '\n' +
      'LoCoMo 데이터 세트에 작업한 주석자는 사내 주석자였으며 이러한 정보의 기밀 특성으로 인해 개체군 통계를 얻을 수 없었다.\n' +
      '\n' +
      '## 부록 C 실험 설정\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      'LoCoMo 데이터셋의 대화는 자연어 대화와 고차 추론과 멀티모달 공동참조 해결이 필요한 이미지로 각각 구성된다. 초기 연구에서 우리는 다중 모드 공동 참조를 관찰했다.\n' +
      '\n' +
      '도 6: **시간 이벤트 그래프 생성을 위한 프롬프트.** 대화 생성 파이프라인(상단)에서 LLM에 대한 완전한 페르소나 및 LoCoMo 데이터세트에 존재하는 페르소나의 예를 생성하는 데 사용되는 프롬프트.\n' +
      '\n' +
      '그림 7: **Temporal Event Graph \\(\\mathcal{G}\\) Creation.** 각 이벤트는 지정된 페르소나 \\(p\\)에 따라 생성되고 이벤트 간의 인과관계 \\(l\\)은 이들 간의 인과관계를 설명하기 위해 묘사된다.\n' +
      '\n' +
      'LoCoMo 내의 이미지들을 BLIP-2 Li 등을 사용하여 생성된 그들의 캡션들로 대체하고(2023), 이미지 캡션들로 인터리빙된 자연 언어 텍스트 위에 추론하기 위해 최첨단 LLM들을 사용함으로써, 해상도가 효과적으로 수행될 수 있다. 따라서 질의 응답 및 이벤트 요약 작업을 위한 실험은 LLM을 사용하여 수행된다. 우리는 멀티모달 대화 생성 태스크에 대한 실험에만 이미지를 직접 사용한다.\n' +
      '\n' +
      '질문 응답.우리는 세 가지 별개의 방법론을 사용하여 실험을 수행한다: (1) **Base**는 제한된 컨텍스트 내에서 작업을 직접 수행하기 위해 LLM을 사용하는 것을 포함한다. 작업 설명은 대화 히스토리 뒤에 나옵니다. 제한된 컨텍스트 윈도우 크기를 수용하기 위해, 이전의 다이얼로그는 생략된다; (2) **긴-컨텍스트**는 모델을 가능한 한 많은 대화 컨텍스트에 노출시키기 위해 확장된 컨텍스트 윈도우를 갖는 LLM을 채용한다; (3) **검색-증강 생성(RAG)**은 대화 히스토리, 관찰 또는 세션-레벨 요약의 데이터베이스로부터 관련 컨텍스트를 검색하는 것을 포함한다. _ Observations_는 SS3.3에 기술된 바와 같이 대화 이력으로부터 추출된 각 화자에 대한 어써션이며, 도 9의 예를 참조한다. 세션-레벨 _summaries_는 각 세션에서 일어나는 대화의 간결한 요약이며, 도 8의 예를 참조한다.\n' +
      '\n' +
      '검색 모델은 DRAGON Lin et al.(2023)을 사용한다. _Base_에서, 우리는 Mistral-7B Jiang et al. (2023), LLama-70B-chat Touvron et al. (2023), gpt-3.5-turbo 9, 및 gpt-4-turbo 10을 이용한다. _Long-context_ 및 _RAG_에 대한 실제 시나리오에서의 유효성을 평가하기 위해, 우리는 _Long-context_ 및 _RAG_에 대한 변형들을 사용하여 비교들을 도출한다.\n' +
      '\n' +
      '도 8: **대화 요약을 생성하기 위한 프롬프트. 이전 세션들로부터의 요약 및 현재 세션의 원시 대화 로그들(상부)을 컨디셔닝함으로써 현재 세션에 대한 요약을 반복적으로 생성하기 위해 사용되는 프롬프트; 및 LoCoMo 데이터세트로부터의 세션의 프롬프트 및 대응하는 출력 요약을 위한 입력들의 예.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Conversation Statistics & \\# Counts \\\\ \\hline Total. \\# conversations \\(h\\). & 50 \\\\ Avg. \\# sessions \\(k\\). in conversation \\(h\\) & 19.3 \\\\ Avg. \\# turns \\(j\\). in session \\(k\\) & 15.8 \\\\ \\hline Avg. \\# tokens. conversation \\(h\\) & 9.209.2 \\\\ Avg. \\# tokens. dialogue \\(h_{s_{j}}\\) of turn \\(j\\) in session \\(k\\) & 30.2 \\\\ Avg. \\# tokens. observation \\(o_{k_{j}}\\) of turn \\(j\\) in session \\(k\\) & 18.2 \\\\ Avg. \\# tokens. summary \\(w_{\\text{x}}\\) of session \\(k\\) & 127.4 \\\\ \\hline \\multicolumn{3}{l}{QA Benchmark Statistics} \\\\ \\hline \\# questions. single-hop retrieval & 2,705 (36\\%) \\\\ \\# questions. multi-hop retrieval & 1,104 (14.6\\%) \\\\ \\# questions. temporal reasoning & 1,547 (20.6\\%) \\\\ \\# questions. open domain knowledge & 285 (3.9\\%) \\\\ \\# questions. adversarial & 1,871 (24.9\\%) \\\\ \\multicolumn{3}{l}{**Total. \\# questions.**} \\\\ \\hline \\multicolumn{3}{l}{Event Summarization Statistics} \\\\ \\hline Avg. \\# ground truth events. in conversation \\(h\\) & 24.2 \\\\ Avg. \\# tokens. event summary & 896.5 \\\\ \\hline \\multicolumn{3}{l}{Multi-modal Dialogue Generation Statistics} \\\\ \\hline Avg. \\# images. in conversation \\(h\\) & 32.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: **대화 통계 및 해당 벤치마크**gpt-3.5-터보. 우리는 긴 컨텍스트 미세 조정 오픈 소스 모델 Chen et al. (2023) 또는 슬라이딩 윈도우 Bertsch et al. (2024)의 성능을 보고하지 않는다; Dao et al. (2022)는 다른 오픈 소스 모델에 내재된 가변성과 더 짧은 컨텍스트에서 그들의 능력의 잠재적인 감소로 인해 보고하지 않는다.\n' +
      '\n' +
      '이벤트 요약.우리는 두 가지 별개의 구성으로 수행된 실험을 제시한다. 우리는 질의 응답 태스크에서 **Base**와 **Long-context** 설정을 모두 사용하지만, 요약은 특정 부분을 검색하는 것이 아니라 전체 대화의 포괄적인 이해가 필요하기 때문에 RAG를 포함하는 것을 자제했다. 질문 답하기 과제와 비교하여 우리의 접근법에서 주목할 만한 차이점은 맥락을 다루는 데 있다. 구체적으로, 선행 세션의 요약을 생성하는 반복 프로세스를 채용하고, 그 요약을 기초로 하여 후속 세션 Chang 등에 대한 요약을 생성한다(2023). 또한 입력 및 출력의 단일 컨텍스트 시연을 사용하여 요약에 대한 중요한 생활 이벤트만 선택하는 모델을 안내한다.\n' +
      '\n' +
      '다중-모달 대화 생성.다중-모달 대화 생성을 평가하기 위해, 우리는 SS3에 자세히 설명된 바와 같이 (인간 필터링 없이) 자동화된 파이프라인을 사용하여 생성된 50개의 대화들에 대해 MiniGPT-5 Zheng et al. (2023)을 훈련시킨다. (1) 선행 대화 턴들 상의 **Base** 열차들; (2) 선행 대화 턴들 상의 **+ 요약** 열차들 및 진행 중인 대화의 글로벌 요약열차들; (3) 선행 대화 턴들 상의 **+ 관찰** 열차들 및 대화 이력으로부터 검색된 관련 관찰열차들. 이러한 각 모델에 대해 MMDi-alog 데이터세트 Feng 등(2023)에서 사전 훈련된 MiniGPT-5 체크포인트로 시작했다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'LoCoMo 벤치마크 평가를 위해 2024년 1월 현재 OpenAI API와 Huggingface Wolf et al. (2020)을 사용하였으며, 구체적인 설정으로 \\(온도\\)는 0, \\(top_{p}\\)는 1로 설정하였다. RAG 기반 모델, MiniGPT-5 훈련 및 추론을 포함한 모든 실험은 FP32가 포함된 Nvidia A6000 서버에서 수행되었으며, 각 모델에 대한 단일 추론 실행 결과를 보고한다. MiniGPT-5의 경우 원래 코드베이스에서 권장되는 하이퍼 파라미터를 사용하고 단일 A6000 GPU에서 약 30시간이 소요되는 10 에폭 동안 모델을 훈련했다.\n' +
      '\n' +
      'BLEU11의 기본 구현을 사용하고,\n' +
      '\n' +
      '도 9: **대화로부터 관찰을 생성하기 위한 프롬프트. 대화(상부)로부터 관찰들을 생성하는 데 사용되는 프롬프트; 및 상기 프롬프트에 대한 입력들의 예 및 LoCoMo 데이터세트로부터의 세션에 대한 대응하는 출력 관찰들.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:18]\n' +
      '\n' +
      '## Appendix A\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c} \\hline \\hline\n' +
      '**Error Type** & **Explanation** & **Ground truth event _or_ relevant dialogs** & **Predicted event** \\\\ \\hline Missing information & Key details about event are omitted because the model fails to make causal and temporal connections over a long conversation. & Joanna submits her third screenplay on loss, identity, and \\\\  & & connection to a film contest & \\\\ \\hline Hallucination & Non-existent details or details from a different event are padded onto an event & _N_: "The gaming party was a great success!" & Nate’s wegan ice cream is a huge \\\\  & & _N_:... said they’d want to do it again next month!" & success and people want to do it \\\\  & & _N_: On another note, I made vegan ice cream...’ & again next month. \\\\ \\hline Misunder-standing of dialog cues & e.g., model confuses a light-hearted statement from a speaker as serious statement & _J_: " these trails that made me feel like writing a drama.’ & Nate considers writing his own \\\\  & & _N_:... go together. Maybe \\(\\Gamma\\)II start to think of a drama & drama screenplay. \\\\ \\hline Speaker & Event is attributed to the wrong speaker & myself and write a steeply...’ & \\\\  & & _J_: "Haha, now that would be something!." & \\\\ \\hline Saliency & Unimportant interactions in the conversation & _N_: Hey Joanna, what’s been up since we last chatted? & Joanna invites Nate to her home to \\\\  & are considered significant by model & How’s it going? & try her dairy-free ice cream recipe. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: **LLM 생성 이벤트 요약의 오류 분류.** LLM에 의해 생성된 이벤트 요약에서 5가지 유형의 오류가 주로 발생한다. 예제는 gpt-3.5-터보의 예측을 기반으로 한다.\n' +
      '\n' +
      '도 11: **주석이 만든 편집의 예.** 인간 주석이 LLM-생성 대화에서 편집을 하도록 지시되어 관련 없는 것을 제거한다. 우리의 대화 생성 파이프라인(상단)에서 LLM에 대한 완전한 페르소나를 생성하는 데 사용되는 프롬프트 및 LoCoMo 데이터세트에 존재하는 페르소나의 예.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>