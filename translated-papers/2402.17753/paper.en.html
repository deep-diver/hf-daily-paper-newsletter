<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Evaluating Very Long-Term Conversational Memory of LLM Agents\n' +
      '\n' +
      '**Adyasha Maharana1** **Dong-Ho Lee2** **Sergey Tulyakov3**\n' +
      '\n' +
      '**Mohit Bansal1\\({}^{\\dagger}\\)** **Francesco Barbieri\\({}^{\\dagger}\\)** **Yuwei Fang3\\({}^{\\dagger}\\)**\n' +
      '\n' +
      'University of North Carolina, Chapel Hill1 University of Southern California2 Snap Inc.3\n' +
      '\n' +
      'Footnote 1: Code and data to be available at\n' +
      '\n' +
      '[https://snap-research.github.io/locomo](https://snap-research.github.io/locomo)\n' +
      '\n' +
      'Footnote 2: Equal advising.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in _very_ long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, _very_ long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of _very_ long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.1\n' +
      '\n' +
      'Footnote 1: Code and data to be available at\n' +
      '\n' +
      '[https://snap-research.github.io/locomo](https://snap-research.github.io/locomo)\n' +
      '\n' +
      'Footnote 2: Equal advising.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Despite recent advancements in dialogue models based on LLMs for extended contexts (Bertsch et al., 2024; Xiao et al., 2023), as well as the integration of retrieval augmented generation (RAG) techniques (Shuster et al., 2021; Ram et al., 2023; Shi et al., 2023), there is still a need for thorough evaluation of their efficacy in handling very long conversations. Indeed, studies in long-term open-domain dialogues have concentrated on assessing model responses within limited contexts e.g., \\(\\sim\\)1K tokens over five chat sessions (Xu et al., 2022; Jang et al., 2023; Zhang et al., 2023). This long term evaluation is crucial for refining engaging chat-bots capable of remembering key information from past interactions, to generate empathetic, consistent, and useful responses.\n' +
      '\n' +
      'Figure 1: **An example in LoCoMo.** Dialogs are steered by the speakers’ personas and corresponding events e.g., Joanna’s responses are consistent with her pet allergies. For Nate, the event _got a new dog_ is followed by a _playdate with neighbor’s dog_, showcasing long-term memory. Multimodal dialog is enabled with image-sharing and image-response behaviors.\n' +
      '\n' +
      'To this end, we present the first study of very long-term open-domain multi-modal dialogues, closely mirroring real-world online interactions, collected via a human-machine pipeline where we first use LLM-based generative agents to generate conversations and then ask human annotators to fix any long-term inconsistencies in the conversations. Specifically, drawing on the understanding that real-world conversations are a complex blend of collective memories (Assmann and Czaplicka, 1995; Hirst and Manier, 2008), individual viewpoints (Hirst et al., 2018), external influences (Hirst and Echterhoff, 2012), and the unique persona of the speakers (Pruitt and Grudin, 2003; Cooper, 1999; Zhou et al., 2020; Shum et al., 2020), we create _very long-term_ dialogues based on LLM agent with the following features: (1) a unique persona (SS3.1); (2) a timeline of causally interlinked events in their lives (SS3.2); and (3) _reflect & response_ mechanism to respond based on dialogue history (like in Park et al. (2023)) and _image sharing & image reaction_ behavior which sends or reacts to images (SS3.3). Finally, human annotators fix long-range inconsistencies in dialogues, remove irrelevant images, and verify the grounding of dialogs to events (SS3.4). With this pipeline, we create LoCoMo, a dataset of 50 _very long-term_ dialogues, each consisting of 300 turns and 9K tokens on avg., over up to 35 sessions (see Figure 1 and Table 1).\n' +
      '\n' +
      'Conventional approaches for evaluating conversational agents in open-domain dialogues involves directly evaluating the agent response based on past dialogue history. It often employs lexical overlap (Papineni et al., 2002) and semantic overlap (Zhang et al., 2019) between ground truth and the agent response, or consistency (Ghazarian et al., 2022), contradiction (Nie et al., 2021; Welleck et al., 2019), and empathy (Zhang et al., 2021, 2022) of the agent response. However, these evaluation metrics are not well-suited for directly assessing the agent\'s comprehension of long-term contexts.\n' +
      '\n' +
      'In this study, we present a holistic evaluation framework to assess an agent\'s proficiency in man\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline\n' +
      '**Dataset** & **Avg. turns** & **Avg. sessions** & **Avg. tokens** & **Time Interval** & **Multimodal** & **Collection** \\\\  & **per conv.** & **per conv.** & **per conv.** & & & \\\\ \\hline MPChat (Ahn et al., 2023) & 2.8 & 1 & 53.3 & - & ✓ & Reddit \\\\ MMDiolog (Feng et al., 2023) & 4.6 & 1 & 72.5 & - & ✓ & Social media \\\\ Daily Dialog (Li et al., 2017) & 7.9 & 1 & 114.7 & - & ✗ & Crowdsourcing \\\\ SODA (Kim et al., 2023) & 7.6 & 1 & 122.4 & - & ✗ & LLM-generated \\\\ MSC (Xu et al., 2022) (train; 1-4 sessions) & 53.3 & 4 & 1,225.9 & few days & ✗ & Crowdsourcing \\\\ Conversation Chronicles (Jang et al., 2023) & 58.5 & 5 & 1,054.7 & few hours - years & ✗ & LLM-generated \\\\\n' +
      '**LoCoMo (ours)** & 304.9 & 19.3 & 9,209.2 & few months & ✓ & LLM-gen. + crowdsourc. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Statistics of LoCoMo** compared to existing dialog datasets. The average length of a conversation in LoCoMo is 9x that of MSC (Xu et al., 2022), distributed over 6x more turns and 4x more sessions (on average).\n' +
      '\n' +
      'Figure 2: **Overview of our evaluation framework. We propose three tasks: question answering, event summarization and multimodal dialog generation to evaluate models’ comprehension in very long-term dialogues.**\n' +
      '\n' +
      'aging and responding within long-term contexts (see Figure 2). First, agents need to "recall" past context correctly to integrate relevant information into future responses. We present a direct examination of their _memory_ via a _question answering_ (QA) task (SS4.1). We classify questions into five distinct reasoning types to evaluate memory from multiple perspectives: single-hop, multi-hop, temporal, commonsense or world knowledge, and adversarial. Second, agents also need to recognize long-range causal and temporal connections in the dialogues to generate empathetic and relevant responses. We propose a measurement of their causal and temporal understanding with an _event graph summarization_ task (SS4.2). In this task, the event graphs linked to each LLM speaker serve as the correct answers, and models are tasked with extracting this information from the conversation history. Third, conversational agents need to utilize relevant context recalled from past conversations to generate responses that are consistent with the ongoing narrative. We assess this ability via the _multi-modal dialog generation_ task (SS4.3).\n' +
      '\n' +
      'We present extensive experimental results on the LoCoMo benchmark using instruction-based LLMs, long-context LLMs, and RAG techniques (SS5). Our findings include:\n' +
      '\n' +
      '(1) Long-context LLMs and RAG demonstrate effectiveness in QA tasks, improving\'memory\' capabilities of LLMs (with improvements ranging from 22-66%), but still significantly lag behind human levels (by 56%), especially in temporal reasoning, (by 73%);\n' +
      '\n' +
      '(2) long-context LLMs demonstrate significant difficulty with adversarial questions in the QA task, showing a performance that is 83% lower than the base model. They are especially prone to missing dialogs or events to the wrong speaker. Moreover, they show poor performance on _event graph summarization_, lagging behind the base model by 14%, indicating that they may grasp the factual elements within the entire conversation but do not accurately comprehend the context; and\n' +
      '\n' +
      '(3) RAG offers a balanced compromise, combining the accuracy of short-context LLMs with the extensive comprehension of wide-context LLMs, and does particularly well when dialogues are transformed into a database of assertions (_observations_) about each speaker\'s life and persona.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Long-term Dialogue.Recent approaches involve retrieving historical context from a range of previous dialogues and reasoning over retrieved segments in a temporal order Lee et al. (2023); Lu et al. (2023); Zhong et al. (2023); Liang et al. (2023) and/or using events to scaffold the dialog Jing et al. (2023); Zhang et al. (2023) to enable consistency in long-term conversations. Some limitations of such frameworks are: (1) The accuracy of retrieval can be compromised, as the retrieval model is generally trained on tasks focusing on semantic similarity rather than specifically on such dialogues. Additionally, real-world dialogues often feature co-references and missing content (_i.e.,_ anaphora) Anantha et al. (2021), which further complicate the retrieval process Mallen et al. (2023); Gao et al. (2023); Liu et al. (2023); (2) Challenges arise in reasoning over retrieved documents, especially when the model struggles to identify the correct context among the retrieved data Liu et al. (2024); (3) Reasoning over time intervals presents challenges. For example, the way a system responds about past events can vary depending on the amount of time that has passed since the last conversation Zhang et al. (2023); Jang et al. (2023). Therefore, it is essential to have conversations of considerable length, as well as a systematic evaluation framework, to accurately assess the effectiveness of approaches to long-term dialogue generation. We design a long-term conversation generation pipeline based on retrieval augmentation and events graphs and propose a framework for evaluating long-term dialog agents.\n' +
      '\n' +
      'Multi-modal Dialogue.Multi-modal dialogue primarily consists of two types of tasks: image-grounded dialogue and image-sharing dialogue. The image-grounded dialogue task is centered around responding to questions Antol et al. (2015); Das et al. (2017); Kottur et al. (2019) or creating natural conversations related to specific images Mostafazadeh et al. (2017); Shuster et al. (2020); Meng et al. (2020); Zheng et al. (2022). Conversely, the image-sharing dialogue task focuses on selecting images that semantically align with the provided dialogue context Zang et al. (2021); Feng et al. (2023); Lee et al. (2023). We use a method from the image-sharing dialogue task to create multimodal dialogs which are then evaluated as an image-grounded dialogue task.\n' +
      '\n' +
      'Synthetic Evaluation Benchmark.Faced with a shortage of human-generated data and observing that LLMs are approaching the quality of human-level annotations He et al. (2023); Lee et al. (2023), there has been a surge in research drawing inspiration from this development. Consequently, numerous studies have started utilizing LLMs to augment or synthesize large-scale dialogue benchmarks for assessing responses in everyday social interactions Kim et al. (2023), examining responses in multi-modal environment Feng et al. (2023), and evaluating responses that align with specific persona Jandaghi et al. (2023). We leverage LLMs to create data but ensure its high quality with human verification and editing.\n' +
      '\n' +
      '## 3 Generative Pipeline for LoCoMo\n' +
      '\n' +
      'An overview of our generative pipeline for LoCoMo is shown in Figure 3. We create two virtual agents, named \\(\\mathcal{L}_{1}\\) and \\(\\mathcal{L}_{2}\\), each initialized with a LLM \\(\\mathcal{M}\\) (_i.e.,_ gpt-3.5-turbo). To start, unique persona statements \\(p\\) are assigned to each agent \\(\\mathcal{L}_{i}\\), ensuring the integration of distinct personalities into their dialogues (SS3.1). To mirror real-life experiences, we create a temporal event graph \\(\\mathcal{G}\\) for each agent, which illustrates a realistic sequence of life events (SS3.2). The LLM agent architecture Park et al. (2023) is utilized for each agent \\(\\mathcal{L}_{i}\\), enabling them to effectively memorize and reflect conversation history into ongoing dialogues (SS3.3). Further, each agent \\(\\mathcal{L}_{i}\\) can share coherent images, thereby enhancing the multi-modal dialogue aspect. Finally, human annotators are tasked with manually filtering and refining the generated data (SS3.4).\n' +
      '\n' +
      '### Persona\n' +
      '\n' +
      'We select an initial persona statement \\(p_{c}\\) from the MSC dataset Xu et al. (2022), encompassing 4 to 5 sentences, and employ gpt-3.5-turbo as \\(\\mathcal{M}\\) to expand these into full persona statement \\(p\\) (See examples and prompt details in Appendix A.1). The generated statements typically include details about one or more of the following elements Gao et al. (2023): objectives, past experiences, daily habits, and interpersonal relationships, as well as name, age, and gender of the individual.\n' +
      '\n' +
      '### Temporal Event Graph\n' +
      '\n' +
      'To utilize the real-life experiences of each agent in the conversation, we construct a temporal event graph, labeled as \\(\\mathcal{G}\\), for each agent. This graph \\(\\mathcal{G}\\), consisting of events \\(e_{i}\\), is produced by applying the condition of \\(\\mathcal{M}\\) (i.e., text-davinci-003) on a designated persona \\(p\\). Each event \\(e_{i}\\) is associated with a date of occurrence \\(t_{i}\\). \\(\\mathcal{G}\\) includes causal connections \\(l=(e_{i},e_{j})\\) that illustrate the causal relationships among events \\(e_{i}\\in\\mathcal{G}\\) and reflect a natural succession of events in an individual\'s life. For each \\(\\mathcal{G}\\), we create up to 25 events, spread across a time frame of 6 to 12 months, in an iterative process that balances between inference time and the coherence of temporal and causal connections in the timeline. Initially, a small batch of \\(k=3\\) events is generated, which is then used iteratively as input prompt to create the subsequent batch of \\(k\\) events. See details in Appendix A.2.\n' +
      '\n' +
      '### Virtual Agent Architecture\n' +
      '\n' +
      'Every agent \\(\\mathcal{L}_{i}\\) incorporates modules from generative agent architecture Park et al. (2023). The agent has two functions: (1) _reflect & respond_; and (2) _image sharing & image reaction_. The agent is asked to primarily use the _reflect & respond_ function while employing _image sharing & image reaction_ function judiciously and appropriately within the context of the conversation.\n' +
      '\n' +
      'Reflect & Respond.The fundamental process for each agent to _reflect and respond_ involves the concept of short-term and long-term memory. During inference, agent \\(\\mathcal{L}_{i}\\) conditions its responses on both short and long-term memories, paralleling how humans remember recent conversations while also recalling distilled important experiences from long-term memory. Following each session \\(k\\), each agent is asked to produce a summary \\(w_{k}\\) that is then stored in the short-term \\(\\mathcal{H}_{s}\\). This summary \\(w_{k}\\) is generated by conditioning \\(\\mathcal{M}\\) on both the most recent session conversation history \\(h_{k}\\) and the preceding summary \\(w_{k-1}\\in\\mathcal{H}_{l}\\). For each turn \\(j\\) within session \\(k\\), a single turn of the conversation \\(h_{k_{j}}\\) is transformed into an observation \\(o_{k_{j}}\\) and then stored in the long-term memory \\(\\mathcal{H}_{l}\\). Then, agent \\(\\mathcal{L}_{i}\\) generates a response in session \\(k+1\\) on the date \\(t_{k+1}^{s}\\) by basing it on the latest summary \\(w_{k}\\), reflections based on the retrieved relevant observations \\(o\\in\\mathcal{H}_{s}\\), the ongoing conversation history in the current session \\(h_{k+1}\\) and persona statement \\(p\\). Long-term temporal narratives are induced in the conversation by additionally conditioning the agent\'s response on the subset of events in \\(\\mathcal{G}\\) that occur between the last and current session i.e. \\(\\{e\\in\\mathcal{G}\\,|\\,t_{k}^{s}\\,<\\,t_{i}^{e}\\,<\\,t_{k+1}^{s}\\,\\}\\). See details in Appendix A.2.1.\n' +
      '\n' +
      'Image Sharing & Image Reaction.The _image sharing & image reaction_ functions are integrated to add a multi-modal dimension to the long-term dialogues.2 The _image sharing_ function is called when the agent decides to send an image. This process includes: (1) Generate a caption \\(c\\) for the intended image using \\(\\mathcal{M}\\); (2) Convert the caption \\(c\\) into relevant keywords \\(w\\) using \\(\\mathcal{M}\\); (3) Use the keywords \\(k\\) to find an image through web search \\(WEB(k)\\)3; (4) Share the chosen \\(image\\). Conversely, the _image reaction_ function is triggered upon receiving an image from another agent and entails: (1) Generate caption \\(c\\) for the received image4; (2) Generate a reaction for the received image in response using \\(\\mathcal{M}\\) (See Appendix A.2.1).\n' +
      '\n' +
      'Footnote 2: Image captions are also saved to long-term memory.\n' +
      '\n' +
      'Footnote 3: [https://pypi.org/project/icrawler/](https://pypi.org/project/icrawler/)\n' +
      '\n' +
      'Footnote 4: We use BLIP-2 (Li et al., 2023b) as the captioning model.\n' +
      '\n' +
      '### Human Verification & Editing\n' +
      '\n' +
      'In the concluding phase, human annotators are tasked with (1) editing the dialogue to eliminate long-term inconsistencies, (2) removing or substituting irrelevant images, and (3) verifying and editing for alignment between event graphs and the content of the conversations. Overall, we observed that annotators edited nearly 15% of the dialog turns and removed or substituted approx. 19% images present in the LLM-generated dataset. See examples of some edits in Appendix A.3.\n' +
      '\n' +
      '## 4 LoCoMo Evaluation Benchmark\n' +
      '\n' +
      'Based on the dialogues generated in section 3, we introduce an evaluation benchmark (see Figure 2) composed of three tasks to assess the accuracy of _long-term memory_. See statistics of the dataset and evaluation benchmark in Table 5 in the Appendix.\n' +
      '\n' +
      '### Question Answering Task\n' +
      '\n' +
      'A conversational agent is expected to possess a _memory_ to remember previous dialogues, reflecting it to create more engaging responses in future conversations. For a comprehensive assessment of this _memory_, we introduce a question-answering task divided into five distinct reasoning categories: (1) **Single-hop** questions require answers based on a single session; (2) **Multi-hop** questions require synthesizing information from multiple different sessions; (3) **Temporal reasoning** questions can be answered through temporal reasoning and capturing time-related data cues within the conversation; (4) **Open-domain knowledge** questions can be answered by integrating a speaker\'s provided information with external knowledge such as commonsense or world facts; (5) **Adversarial** questions are designed to trick the agent into providing wrong answers, with the expectation that the agent will correctly identify them as unanswerable.\n' +
      '\n' +
      'For each category, we calculate the F1 score for exact matches, following the normalization of both the predicted and the actual ground truth answers. However, evaluating long-form answers with automated metrics often presents challenges (Xu et al., 2023). LLMs tend to produce paraphrased responses in varied formats, complicating exact match evaluation. To simplify evaluation in our task, we ensure that answers in our QA annotations are directly taken from the conversations as much as possible. We instruct the LLMs to replicate the exact wording in the conversation when feasible and employ the F1 partial match metric for evaluating the predictions. Each QA sample is also annotated with the turn IDs in the conversation logs that contain the answer. We report the accuracy of retrieving the correct context for RAG models.\n' +
      '\n' +
      'Figure 3: **Overview of the generative pipeline for LoCoMo. Each LLM agent is assigned a distinct persona and a timeline of causally connected events in their file. The agent is equipped with a memory and reflection module to retrieve relevant history for dialog generation and is also enabled for image-sharing and image-reaction behaviors (left). The generated conversations are edited by human annotators to maintain long-range consistency (right).**\n' +
      '\n' +
      '### Event Summarization Task\n' +
      '\n' +
      'The conversation is generated based on a temporal event graph \\(\\mathcal{G}\\) which is constructed by conditioning an LLM on a persona statement \\(p\\), reflecting the chronological sequence of events in an individual\'s life. A conversational agent is expected to not only comprehend the causal connections and the sequence of events in \\(\\mathcal{G}\\) but also to recount these events as required. To evaluate the agent\'s grasp of event dynamics, we introduce the event summarization task which challenges the agent to summarize the events within a designated timeframe and compares the agent\'s summary with events in \\(\\mathcal{G}\\). The events in LoCoMo are densely annotated lists of life events that are hard to summarize due to temporal and causal coreferences present in the dialogues, in contrast to existing summarization benchmarks of research papers Li et al. (2023), movie scripts Chen et al. (2022), books Krycsinski et al. (2022), emails Zhang et al. (2021) etc.\n' +
      '\n' +
      'Traditional metrics like BLEU Papineni et al. (2002) and ROGUE Lin (2004) focus on lexical similarity between the reference and generated summaries, not meeting our needs as we emphasize factual accuracy in summarization. In this context, we employ FactScore Min et al. (2023), a method that evaluates the factuality of generated text by decomposing both the reference and hypothesis into atomic facts. We adapt the metric to measure (1) _precision_ of the summarized content by counting the number of atomic facts within the content that correspond with those in \\(\\mathcal{G}\\); (2) _recall_ of the summarized content by determining how comprehensively the atomic facts of \\(\\mathcal{G}\\) are represented within the content. We present the F1 score, derived from the calculated precision and recall.\n' +
      '\n' +
      '### Multi-Modal Dialogue Generation Task\n' +
      '\n' +
      'The conversations in our dataset are anchored to specific personas \\(p\\) and corresponding events \\(\\mathcal{G}\\) tailored to \\(p\\). The topics in conversations evolve from events that were introduced in earlier dialogues, spanning weeks or months. This structure allows for an assessment of whether conversational agents can sustain a coherent persona and a continuous narrative over time. For example, if a speaker recently had an injury, the next conversations would likely focus on them recuperating, rather than engaging in adventurous activities. We assess such consistency by measuring how closely the predicted multi-modal dialogues align with the ground truth multi-modal dialogues in our dataset, quantifying this alignment through MMRevance Feng et al. (2023), in addition to other NLG metrics.\n' +
      '\n' +
      '## 5 Experimental Setup\n' +
      '\n' +
      'For the question-answering and event summarization tasks, we replace images in LoCoMo with their captions Li et al. (2023), and use state-of-art LLMs to reason over text-only dialogues interleaved with image captions. We use images directly for the multimodal dialog generation task only. See additional details in Appendix C.\n' +
      '\n' +
      'Question Answering.We evaluate three types of models: (1) **Base** LLMs operating with constrained context lengths where earlier dialogues are omitted i.e., Mistral-7B Jiang et al. (2023), LLama-70B-chat Touvron et al. (2023), gpt-3.5-turbo 5, and gpt-4-turbo 6; (2) **Long-context** LLMs with an extended context window i.e., gpt-3.5-turbo-16k; (3) **Retrieval-augmented Generation (RAG)** involves retrieving relevant context from a database of dialog history, observations (assertions about speakers; see SS3.3, Figure 9), or session-level summaries (see SS3.3, Figure 8). We employ DRAGON Lin et al. (2023) as retriever and gpt-3.5-turbo-16k as reader.\n' +
      '\n' +
      'Footnote 5: [https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5)\n' +
      '\n' +
      'Footnote 6: [https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)\n' +
      '\n' +
      'Event Summarization.We present experiments using **Base** and **Long-context** setups from the question-answering task, but refrain from including RAG since summarization requires a comprehensive understanding of the entire dialogue, rather than just retrieving a specific portion. We implement incremental summarization i.e., iteratively create a summary of a preceding sessions and then use that summary as a basis to summarize the subsequent sessions Chang et al. (2023).\n' +
      '\n' +
      'Multi-modal Dialogue Generation.We generate 50 conversations using our automated pipeline (without human filtering; SS3) for training data and train three versions of MiniGPT-5 Zheng et al. (2023): (1) **Base** trains on prior dialogue turns only; (2) **+ summary** trains on prior dialogue turns and a global summary of the ongoing conversation; (3) **+ observation** trains on prior dialogue turns and observations retrieved from conversation history. Each run is initialized with a MiniGPT-5 checkpoint finetuned on MMDialog Feng et al. (2023).\n' +
      '\n' +
      '## 6 Experimental Results\n' +
      '\n' +
      'We evaluate and analyze the comprehensive performance of all baseline methods for question answering (SS6.1), event graph summarization (SS6.2), and multi-modal dialogue generation (SS6.3).\n' +
      '\n' +
      '### Question Answering Task\n' +
      '\n' +
      'Tables 2 and 3 present the performance results for the question answering task. We find that: **(1) LLMs with limited context length face challenges in understanding extremely long conversations** due to truncated context windows. Despite gpt-4-turbo emerging as the top-performing model with an overall score of 32.4, it notably lags behind the human benchmark of 87.9; **(2) long-context LLMs can comprehend longer narratives, yet they are prone to generating hallucinations**. gpt-3.5-turbo-16k outperforms other approaches, but its performance on adversarial questions drops to a mere 2.1%, as compared to 22.1% using Llama-2-Chat and 70.2% using GPT-4-turbo with 4K context windows. This indicates that LLMs can be easily misled into generating hallucinations when they are subjected to long contexts; **(3) RAG is effective when conversations are stored as observations**. There is a noticeable 5% improvement with gpt-3.5-turbo when the input is top 5 relevant observations instead of pure conversation logs. This improvement falters with an increase in the number of retrieved observations, suggesting that it is important to reduce the signal-to-noise (SNR) ratio in retrieved contexts for models to utilize the context accurately. Conversely, using session summaries as context does not significantly improve the performance despite high recall accuracies7, likely due to loss of information during the conversion of dialogs to summaries.\n' +
      '\n' +
      'Footnote 7: For summary-based RAG models, the recall accuracy is based on retrieving the summary of the relevant session(s).\n' +
      '\n' +
      'The interesting finding is that **time reasoning and open-domain knowledge questions are the most challenging scenarios**.\n' +
      '\n' +
      '(1) LLMs face challenges in understanding time concepts within dialogues, which is consistent with findings from other single-turn-based benchmarks focused on temporal reasoning capabilities for LLMs [23].\n' +
      '\n' +
      '(2) LLMs struggle with open-domain knowledge and degrade in the RAG setting. This suggests that while certain open-domain knowledge may be embedded within the model\'s parameters, introducing improper context from inaccurate retrieval can lead to a decline in performance [13].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Category**} & \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Context Length**} & \\multicolumn{6}{c}{**Answer Prediction (F1)**} \\\\ \\cline{4-9}  & & & Single Hop & Multi Hop & Temporal & Open Domain & Adversarial & **Overall** \\\\ \\hline \\multirow{2}{*}{\\begin{tabular}{c} Human \\\\ \\end{tabular} } & Human & - & 95.1 & 85.8 & 92.6 & 75.4 & 89.4 & 87.9 \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} Base \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} Mistral-Instruct-7B \\\\ Llama-2-Chat-70B \\\\ GPT-3.5-turbo \\\\ GPT-4-turbo \\\\ GPT-3.5-turbo-16K \\\\ \\end{tabular} } & 8K & 10.2 & 12.8 & 16.1 & 19.5 & 17.0 & 13.9 \\\\  & & 4,096 & 19.7 & 14.4 & 13.3 & 15.9 & 22.1 & 17.9 \\\\  & & 4,096 & **29.9** & 23.3 & **17.5** & **29.5** & 12.8 & 22.4 \\\\  & & 4,096 & 23.4 & **23.4** & 10.4 & 24.6 & **70.2** & **32.1** \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} Long context \\\\ \\end{tabular} } & \\multirow{2}{*}{\n' +
      '\\begin{tabular}{c} GPT-3.5-turbo-16K \\\\ \\end{tabular} } & 4K & 31.7 & 25.4 & 16.8 & 27.6 & **13.1** & 24.1 \\\\  & & **8K** & 38.8 & 31.2 & 21.0 & 35.0 & 8.4 & 25.2 \\\\ \\cline{1-1}  & & 12K & 51.1 & 40.4 & **25.0** & 36.5 & 6.4 & 33.5 \\\\ \\cline{1-1}  & & **16K** & **56.4** & **42.0** & 20.3 & **37.2** & 2.1 & **37.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Question answering performance of Base and Long-context models. Optimal performance is in bold. Results are based on F1-score for answer prediction; higher is better.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Retrieval Unit**} & \\multirow{2}{*}{**top-B**} & \\multicolumn{6}{c}{**Answer Prediction (F1 score)**} & \\multicolumn{6}{c}{**Recall Accuracy (R@\\(k\\))**} \\\\ \\cline{3-11}  & & Single & Multi & Temporal & Open & Adver- & **Overall** & Single & Multi & Temporal & Open & Adver- & **Overall** \\\\ \\cline{2-11}  & & Hop & Hop & Domain & -sarial & Hop & Hop & Hop & Domain & -sarial & \\\\ \\hline \\multirow{2}{*}{\\begin{tabular}{c} None \\\\ \\end{tabular} } & - & 29.9 & 23.3 & 17.5 & 29.5 & 12.8 & 22.4 & - & - & - & - & - & - \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} Dialog \\\\ \\end{tabular} } & 10 & 42.9 & 19.4 & 21.3 & 35.8 & 31.9 & 31.7 & 66.2 & 34.4 & 89.2 & 38.5 & 45.7 & 58.8 \\\\  & 10 & 46.3 & 26.8 & 24.8 & 37.5 & **29.8** & 34.6 & 72.8 & 247.4 & 97.3 & 53.8 & 54.3 & 67.5 \\\\  & 25 & 48.1 & 36.1 & **26.2** & **43.4** & 23.4 & **35.8** & 87.5 & 64.1 & **97.3** & **67.9** & **69.1** & 79.9 \\\\  & 50 & **50.9** & **37.2** & 24.6 & 38.3 & 17.0 & 34.8 & **90.4** & **75.8** & 97.3 & 67.9 & **77.7** & **84.8** \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} Observation \\\\ \\end{tabular} } & 5 & 44.3 & 30.6 & 41.9 & 40.2 & **44.7** & **41.4** & 52.9 & 40.1 & 81.1 & 38.5 & 29.8 & 49.6 \\\\  & 10 & 42.2 & 30.5 & **42.1** & 41.9 & 36.2 & 38.8 & 57.4 & 53.1 & **83.8** & 46.2 & 41.5 & 57.1 \\\\  & 25 & **44.6** & 33.2 & 41.8 & 41.9 & 27.7 & 38.0 & **71.3** & 63.8 & 83.8 & 66.7 & 45.7 & 66.0 \\\\  & 50 & 44.0 & **34.5** & 41.1 & **41.9** & 27.7 & 37.8 & 72.8 & **73.2** & 83.8 & **74.4** & **56.4** & **71.1** \\\\ \\hline \\multirow{4}{*}{\n' +
      '\\begin{tabular}{c} Summary \\\\ \\end{tabular} } & 2 & 34.6 & 15.7 & 26.9 & 26.5 & 36.2 & 29.9 & 68.4 & 39.6 & 56.8 & 50.0 & 73.4 & 61.5 \\\\  & **36.6** & **16.6** & **31.0** & **34.7** & 38.3 & **32.8** & 81.6 & 57.0 & 70.3 & 60.3 & 86.2 & 75.1 \\\\ \\cline{1-1}  & 10 & 34.5 & 14.7 & 29.3 & 31.6 & **40.4** & 31.5 & **93.4** & **82.3** & **91.9** & **80.8** & **94.7** & **90.7** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **Question answering performance of RAG-based GPT-3.5-turbo-16k. Optimal performance is in bold. Results are based on F1-score metric for answer prediction and recall@\\(k\\) for recall accuracy; higher is better.**\n' +
      '\n' +
      '### Event Summarization Task\n' +
      '\n' +
      'Table 4 presents results for the event summarization task. The use of incremental summarization with **gpt-3.5-turbo leads to the highest performance** in both recall and F1 score. While gpt-4-turbo records a 5.3% improvement in precision over with gpt-3.5-turbo, it does not fare as well in terms of recall. The event summarization task requires long-range dependency to understand the temporal and causal connections between the events discussed by the speaker in multiple sessions (see Figure 7). Contrary to expectations, the **long-context model does not surpass the base model**, despite its capability for extended-range reasoning facilitated by a larger context window. gpt-3.5-turbo-16k exhibits a decline in both precision (by 3.0%) and recall (by 8.7%) compared to gpt-3.5-turbo which has a 4K context window. This suggests that **long-context models may not be proficient at utilizing their context appropriately**, which also aligns with similar findings in Li et al. (2023) as well as the QA task in LoCoMo. In terms of both the ROUGE and FactScore metrics, commercial models (gpt-4-turbo, gpt-3.5-turbo) significantly outshine their open-source counterparts. Nonetheless, there remains considerable scope for improving performance on this task.\n' +
      '\n' +
      'From a manual analysis of predicted summaries, we identify five broad categories of event summarization errors made by LLMs: (1) **missing information** in events because the model fails to make temporal and/or causal connections over a lengthy conversation; (2) **hallucinations** i.e., models pad extra details that are either not present in the conversation or are part of a different event in the same session; (3) errors from **misunderstanding of dialog cues** such as humor or sarcasm is a distinctive issue with comprehension of dialogs; (4) inaccurate **speaker attributions**; and (5) insignificant dialogs that are wrongly considered as **salient** events. See examples in Table 7 in the Appendix.\n' +
      '\n' +
      '### Multi-Modal Dialog Generation Task\n' +
      '\n' +
      'Figure 4 illustrates the effectiveness of various MiniGPT-5 training variants in multi-modal dialogue generation. Incorporating context into training enhances performance, with the inclusion of observation as context yielding significantly improved results. For instance, in Figure 4A, the retrieved observations contain information about the speaker\'s experience in video game tournaments, which leads to the prediction of dialog and images\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Category**} & \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Context Length**} & \\multicolumn{3}{c}{**ROGUE**} & \\multicolumn{3}{c}{**FactScore**} \\\\ \\cline{3-8}  & & & & **ROGUE-1** & **ROGUE-2** & **ROGUE-L** & **Precision** & **Recall** & **F1** \\\\ \\hline \\multirow{3}{*}{Base} & \\multicolumn{2}{c}{Mistral-Instruct-7B} & 8K & 29.4 & 7.2 & 14.1 & 27.1 & 19.8 & 23.0 \\\\  & \\multicolumn{2}{c}{Llama-2-Chat-7B6} & 4,096 & 28.1 & 9.3 & 14.8 & 36.3 & 22.7 & 28.3 \\\\  & \\multicolumn{2}{c}{GPT-4-turbo} & 4,096 & 38.8 & 11.4 & 20.6 & **51.6** & 41.8 & 45.1 \\\\  & \\multicolumn{2}{c}{GPT-3.5-turbo} & 4,096 & **41.1** & **13.5** & **20.9** & 45.3 & **46.5** & **45.9** \\\\ \\hline Long context & GPT-3.5-turbo-16K & 16K & 36.2 & 8.5 & 16.4 & 42.3 & 37.8 & 39.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: **Event summarization performance of Base and Long-context models. The optimal performance is shown in **bold**. Results are based on ROUGE and FactScore (Min et al., 2023) metrics; higher is better.**\n' +
      '\n' +
      'Figure 4: **Multimodal dialog generation performance of MiniGPT-5**. (A) an example of multimodal dialog predicted using MiniGPT5 with and without observation as retrieved context, (B) Variation of MM-Relevance score with length of dialog history, and (C) comparison of RAG-based MiniGPT-5 methods.\n' +
      '\n' +
      'that are more faithful to the speaker\'s persona. This observation is consistent with earlier findings from the QA task as well (see Table 3). Also, we observe that the MM-Relevance score drops with an increase in the length of dialog history (see Figure 4B). Retrieval-augmented generation alleviates the drop in MM-Relevance to some extent.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'We develop a human-machine pipeline to collect LoCoMo, a dataset of 50 high-quality very long conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions, and propose an evaluation framework consisting of three tasks that evaluate models\' proficiency in long conversations. Our experiments show that LLMs struggle to comprehend long-term narratives within the dialog and fail to draw temporal and causal connections between events discussed by speakers.\n' +
      '\n' +
      '## 8 Limitations\n' +
      '\n' +
      'Hybrid human-machine generated data.Our dataset is sourced primarily from text generated by LLMs. We pursued this method, which has quickly emerged as a popular alternative to time-intensive manual data collection (Kim et al., 2023; Jang et al., 2023), to avoid the logistical and legal complexities of collecting very long-term real-world conversations at scale. We ensure that the dataset mirrors real-world interactions as much as possible by having human annotators verify and edit the generated conversations. However, we acknowledge that this dataset may not fully reflect the nuances of real-world online conversations.\n' +
      '\n' +
      'Limited exploration of multimodal behavior.Since the images in our dataset are sourced from the web, they do not demonstrate the visual long-term consistencies that are usually exhibited in personal photos (e.g., appearance, home environment, people and pets, etc.). Consequently, we find that the images in our dataset can be replaced with their captions without much loss of information, except for cases where OCR is required. Nevertheless, our work is a first step toward research into the multimodal aspect of very long-term conversations.\n' +
      '\n' +
      'Language.Our LLM-based pipeline for generating long-term conversations has been developed for the English language only. However, our pipeline can be made to work with any other language using an LLM that is proficient at that language and appropriate translations of our prompts.\n' +
      '\n' +
      'Closed-source LLMs.We use state-of-the-art LLMs in our dialog generation pipeline to create a dialog dataset that is as realistic as possible. Unfortunately, this meant employing the strongest commercial LLMs available through a paid API, similar to many concurrent works that generate synthetic conversations (Zhong et al., 2023; Lu et al., 2023). We will make the code for our generative pipeline publicly available in the hope that it can be made to work effectively with state-of-the-art open-source LLMs in the future.\n' +
      '\n' +
      'Evaluation of long-form NLG.LLMs are prone to generating verbose answers even when prompted to answer in short phrases. This creates challenges in evaluating the correctness of answers provided by LLMs and has been widely documented in NLP literature (Chang et al., 2023; Xu et al., 2023; Krishna et al., 2023). Our evaluation framework suffers from the same challenges when used for experimenting with LLMs.\n' +
      '\n' +
      '## 9 Broader Impacts\n' +
      '\n' +
      'We adopt and improve a framework of generative agents introduced in Park et al. (2023) for the generation of long-term conversations. Consequently, the ethical concerns of generative agents outlined by Park et al. (2023) apply to our work as well, especially since the goal of our framework is to make the conversations as realistic as possible.\n' +
      '\n' +
      'Specifically, conversational agents that can pose as human beings with a realistic life, as enabled by the temporal event graphs in our framework, pose the risk that users may form parasocial relationships with such agents that may affect their lives adversely. We recommend that any practical deployment of the generative frameworks mentioned in our work be always prefaced with a disclaimer about the source of the dialogs.\n' +
      '\n' +
      'Second, the use of multimodal LLMs (Zheng et al., 2023) to generate images conditioned on dialog can lead to the propagation of misinformation and social biases, especially if the conversational agent can be coerced into parroting false information or dangerous opinions.\n' +
      '\n' +
      'Third, it is tempting to use generative agents to substitute real humans for a process, especially when there are significant challenges in working with humans for a particular goal e.g., collecting real-world interactions between humans over a year or more. Care must be taken to ensure that such substitutes are not made in studies whose outcomes may be used to make real-world decisions with tangible impacts on humans. Our work is merely a study of model comprehension in very long-term conversations. We do not make any recommendations for real-world policies based on this study and advise potential users of our framework to avoid making such recommendations as well.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Larincke, and D. Parikh (2015)Vqa: visual question answering. In Proceedings of the IEEE international conference on computer vision, pp. 2425-2433. Cited by: SS1.\n' +
      '* J. Assmann and J. Czaplicka (1995)Collective memory and cultural identity. New german critique65, pp. 125-133. Cited by: SS1.\n' +
      '* A. Bertsch, U. Alon, G. Neubig, and M. Gormley (2024)Unlimformer: long-range transformers with unlimited length input. Advances in Neural Information Processing Systems36. Cited by: SS1.\n' +
      '* Y. Chang, K. Lo, T. Goyal, and M. Iyyer (2023)Booookscore: a systematic exploration of book-length summarization in the era of lms. In The Twelfth International Conference on Learning Representations, Cited by: SS1.\n' +
      '* M. Chen, Z. Chu, S. Wiseman, and K. Gimpel (2022)Summscreen: a dataset for abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8602-8615. Cited by: SS1.\n' +
      '* Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, and J. Jia (2023)Longlora: efficient fine-tuning of long-context large language models. In The Twelfth International Conference on Learning Representations, Cited by: SS1.\n' +
      '* A. Cooper (1999)The imates are running the asylum. Springer. Cited by: SS1.\n' +
      '* T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Re (2022)Flashattention: fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems35, pp. 16344-16359. Cited by: SS1.\n' +
      '* A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh, and D. Batra (2017)Visual dialog. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 326-335. Cited by: SS1.\n' +
      '* J. Feng, Q. Sun, C. Xu, P. Zhao, Y. Yang, C. Tao, D. Zhao, and Q. Lin (2023)MDialog: a large-scale multi-turn dialogue dataset towards multi-modal open-domain conversation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, pp. 7348-7363. Cited by: SS1.\n' +
      '* S. Gao, B. Borges, S. Oh, D. Bayazit, S. Kanno, H. Wakaki, Y. Mitsufuji, and A. Bosseltu (2023)PeaCoK: persona commonsense knowledge for consistent and engaging narratives. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, pp. 6569-6591. Cited by: SS1.\n' +
      '* T. Gao, H. Yen, J. Yu, and D. Chen (2023)Enabling large language models to generate text with citations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore, pp. 6465-6488. Cited by: SS1.\n' +
      '* S. Ghazarian, N. Wen, A. Galstyan, and N. Peng (2022)Deam: dialogue coherence evaluation using amr-based semantic manipulations. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 771-785. Cited by: SS1.\n' +
      '* X. He, Z. Lin, Y. Gong, A. Jin, H. Zhang, C. Lin, J. Jiao, S. M. Yiu, N. Duan, W. Chen, et al. (2023)Annollm: making large language models to be better crowdsourced annotators. arXiv preprint arXiv:2303.16854. Cited by: SS1.\n' +
      '* W. Hirst and G. Echterhoff (2012)Remembering in conversations: the social sharing and reshaping of memories. Annual review of psychology63, pp. 55-79. Cited by: SS1.\n' +
      '* W. Hirst and D. Manier (2008)Towards a psychology of collective memory. Memory16 (3), pp. 183-200. Cited by: SS1.\n' +
      '* W. Hirst, J. K. Yamashiro, and A. Coman (2018)Collective memory from a psychological perspective. Trends in cognitive sciences22 (5), pp. 438-451. Cited by: SS1.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      'nd G. Echterhoff (2012)Remembering in conversations: the social sharing and reshaping of memories. Annual review of psychology63, pp. 55-79. Cited by: SS1.\n' +
      'Pegah Jandaghi, XiangHai Sheng, Xinyi Bai, Jay Pujara, and Hakim Sidahmed. 2023. Faithful persona-based conversational dataset generation with large language models. _arXiv preprint arXiv:2312.10007_.\n' +
      '* Jang et al. (2023) Jihyoung Jang, MINEeong Boo, and Hyounghun Kim. 2023. Conversation chronicles: Towards diverse temporal and relational dynamics in multi-session conversations. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 13584-13606, Singapore. Association for Computational Linguistics.\n' +
      '* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. _arXiv preprint arXiv:2310.06825_.\n' +
      '* Kim et al. (2023) Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, and Yejin Choi. 2023. SODA: Million-scale dialogue distillation with social commonsense contextualization. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 12930-12949, Singapore. Association for Computational Linguistics.\n' +
      '* Kottur et al. (2019) Satwik Kottur, Jose M. F. Moura, Devi Parikh, Dhruv Batra, and Marcus Rohrbach. 2019. CLEVR-dialog: A diagnostic dataset for multi-round reasoning in visual dialog. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 582-595, Minneapolis, Minnesota. Association for Computational Linguistics.\n' +
      '* Krishna et al. (2023) Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. Longeval: Guidelines for human evaluation of faithfulness in long-form summarization. In _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics_, pages 1642-1661.\n' +
      '* Krycinski et al. (2022) Wojciech Krycinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022. Booksum: A collection of datasets for long-form narrative summarization. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 6536-6558.\n' +
      '* Lee et al. (2023a) Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen White, and Sujay Jauhar. 2023a. Making large language models better data creators. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 15349-15360, Singapore. Association for Computational Linguistics.\n' +
      '* Lee et al. (2023b) Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos, and Kangwook Lee. 2023b. Prompted LLMs as chatbot modules for long open-domain conversation. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 4536-4554, Toronto, Canada. Association for Computational Linguistics.\n' +
      '* Lee et al. (2023c) Young-Jun Lee, Byungsoo Ko, Han-Gyu Kim, Jonghwan Hyeon, and Ho-Jin Choi. 2023c. Dialogcc: An automated pipeline for creating high-quality multi-modal dialogue datasets. In _NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following_.\n' +
      '* Li et al. (2023a) Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023a. Loogle: Can long-context language models understand long contexts? _arXiv preprint arXiv:2311.04939_.\n' +
      '* Li et al. (2023b) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In _International Conference on Machine Learning_.\n' +
      '* Li et al. (2017) Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset. In _Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 986-995.\n' +
      '* Liang et al. (2023) Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. _arXiv preprint arXiv:2304.13343_.\n' +
      '* Lin (2004) Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In _Text Summarization Branches Out_, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.\n' +
      '* Lin et al. (2023) Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023. How to train your dragon: Diverse augmentation towards generalizable dense retrieval. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 6385-6400, Singapore. Association for Computational Linguistics.\n' +
      '* Liu et al. (2023) Nelson Liu, Tianyi Zhang, and Percy Liang. 2023. Evaluating verifiability in generative search engines. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 7001-7025, Singapore. Association for Computational Linguistics.\n' +
      '* Liu et al. (2024) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the Middle: How Language Models Use Long Contexts. _Transactions of the Association for Computational Linguistics_, 12:157-173.\n' +
      '* Lu et al. (2023) Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yunsheng Wu. 2023. Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. _arXiv preprint arXiv:2308.08239_.\n' +
      '\n' +
      'Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 9802-9822, Toronto, Canada. Association for Computational Linguistics.\n' +
      '* Meng et al. (2020) Yuxian Meng, Shuhe Wang, Qinghong Han, Xiaofei Sun, Fei Wu, Rui Yan, and Jiwei Li. 2020. Openvidial: A large-scale, open-domain dialogue dataset with visual contexts. _arXiv preprint arXiv:2012.15015_.\n' +
      '* Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 12076-12100, Singapore. Association for Computational Linguistics.\n' +
      '* Mostafazadeh et al. (2017) Nasrin Mostafazadeh, Chris Brockett, Bill Dolan, Michel Galley, Jianfeng Gao, Georgios Spithourakis, and Lucy Vanderwende. 2017. Image-grounded conversations: Multimodal context for natural question and response generation. In _Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 462-472, Taipei, Taiwan. Asian Federation of Natural Language Processing.\n' +
      '* Nie et al. (2021) Yixin Nie, Mary Williamson, Mohit Bansal, Douwe Kiela, and Jason Weston. 2021. I like fish, especially dolphins: Addressing contradictions in dialogue modeling. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 1699-1713.\n' +
      '* Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\n' +
      '* Park et al. (2023) Joon Sung Park, Joseph O\'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, UIST \'23, New York, NY, USA. Association for Computing Machinery.\n' +
      '* Pruitt and Grudin (2003) John Pruitt and Jonathan Grudin. 2003. Personnas: practice and theory. In _Proceedings of the 2003 conference on Designing for user experiences_, pages 1-15.\n' +
      '* Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. _Transactions of the Association for Computational Linguistics_, 11:1316-1331.\n' +
      '* Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented black-box language models. _arXiv preprint arXiv:2301.12652_.\n' +
      '* Shum et al. (2020) Michael Shum, Stephan Zheng, Wojciech Kryscinski, Caiming Xiong, and Richard Socher. 2020. Sketchfill-a-R: A persona-grounded chit-chat generation framework. In _Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI_, pages 118-131, Online. Association for Computational Linguistics.\n' +
      '* Shuster et al. (2020) Kurt Shuster, Samuel Humeau, Antoine Bordes, and Jason Weston. 2020. Image-chat: Engaging grounded conversations. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 2414-2429, Online. Association for Computational Linguistics.\n' +
      '* Shuster et al. (2021) Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 3784-3803.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.\n' +
      '* Wang and Zhao (2023) Yuqing Wang and Yun Zhao. 2023. Tram: Benchmarking temporal reasoning for large language models. _arXiv preprint arXiv:2310.00835_.\n' +
      '* Welleck et al. (2019) Sean Welleck, Jason Weston, Arthur Szlam, and Kyunghyun Cho. 2019. Dialogue natural language inference. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 3731-3741, Florence, Italy. Association for Computational Linguistics.\n' +
      '* Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online. Association for Computational Linguistics.\n' +
      '* Xiao et al. (2023) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks. _arXiv preprint arXiv:2309.17453_.\n' +
      '\n' +
      'Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023. A critical evaluation of evaluations for long-form question answering. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3225-3245, Toronto, Canada. Association for Computational Linguistics.\n' +
      '* Xu et al. (2022) Jing Xu, Arthur Szlam, and Jason Weston. 2022. Beyond goldfish memory: Long-term open-domain conversation. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5180-5197.\n' +
      '* Zang et al. (2021) Xiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song, Hao Zhang, and Jindong Chen. 2021. PhotoChat: A human-human dialogue dataset with photo sharing behavior for joint image-text modeling. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 6142-6152, Online. Association for Computational Linguistics.\n' +
      '* Zhang et al. (2021a) Chen Zhang, Yiming Chen, Luis Fernando D\'Haro, Yan Zhang, Thomas Friedrichs, Grandee Lee, and Haizhou Li. 2021a. Dynaeval: Unifying turn and dialogue level evaluation. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 5676-5689.\n' +
      '* Zhang et al. (2022) Chen Zhang, Luis Fernando D\'Haro, Qiquan Zhang, Thomas Friedrichs, and Haizhou Li. 2022. Finedeval: Fine-grained automatic dialogue-level evaluation. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 3336-3355.\n' +
      '* Zhang et al. (2023) Qiang Zhang, Jason Naradowsky, and Yusuke Miyao. 2023. Mind the gap between conversations for improved long-term dialogue generation. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 10735-10762, Singapore. Association for Computational Linguistics.\n' +
      '* Zhang et al. (2021b) Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, and Mohit Bansal. 2021b. Emailsum: Abstractive email thread summarization. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 6895-6909.\n' +
      '* Zhang et al. (2019) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. In _International Conference on Learning Representations_.\n' +
      '* Zheng et al. (2023) Kaizhi Zheng, Xuehai He, and Xin Eric Wang. 2023. Minigpt-5: Interleaved vision-and-language generation via generative vokens. _arXiv preprint arXiv:2310.02239_.\n' +
      '* Zheng et al. (2022) Yinhe Zheng, Guanyi Chen, Xin Liu, and Jian Sun. 2022. MMChat: Multi-modal chat dataset on social media. In _Proceedings of the Thirteenth Language Resources and Evaluation Conference_, pages 5778-5786, Marseille, France. European Language Resources Association.\n' +
      '* Zhong et al. (2023) Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. 2023. Memorybank: Enhancing large language models with long-term memory. _arXiv preprint arXiv:2305.10250_.\n' +
      '* Zhou et al. (2020) Li Zhou, Jianfeng Gao, Di Li, and Heung-Yeung Shum. 2020. The design and implementation of xiaoice, an empathetic social chatbot. _Computational Linguistics_, 46(1):53-93.\n' +
      '\n' +
      '## Appendix A Appendix\n' +
      '\n' +
      'The appendix is organized as follows:\n' +
      '\n' +
      '**Section A**: Details of generative pipeline for the LoCoMo dataset.\n' +
      '\n' +
      '**Section B**: Statistics of LoCoMo dataset, license for data release and annotator details.\n' +
      '\n' +
      '**Section C**: Experimental setup and implementation details.\n' +
      '\n' +
      '**Section D**: Additional results from evaluation on the LoCoMo benchmark.\n' +
      '\n' +
      '## Appendix A Generative Pipeline for LoCoMo\n' +
      '\n' +
      '### Persona\n' +
      '\n' +
      'We assign unique persona statement \\(p\\) to each agent \\(\\mathcal{L}_{i}\\). For this, we select a range of initial persona statements \\(p_{c}\\) from the MSC dataset Xu et al. (2022), each encompassing 4 to 5 sentences. We employ gpt-3.5-turbo as \\(\\mathcal{M}\\) to expand these into full persona statement \\(p\\), conditioning \\(\\mathcal{M}\\) on the chosen statements \\(p_{c}\\). The prompt used for converting a short list of speaker attributes from the MSC dataset Xu et al. (2022) into a complete persona summary is presented in Fig. 5. We also use a single example of speaker attribute \\(\\rightarrow\\) persona summary as an in-context demonstration along with the prompt. A small selection of personas showcasing the diversity of speakers in the LoCoMo dataset is demonstrated in Fig. 5.\n' +
      '\n' +
      '### Temporal Event Graph\n' +
      '\n' +
      'As outlined in Sec. 3.2, we use an iterative process for generating event graphs consisting of causally connected events based on a given persona summary. The base prompt for describing the constitution of the event graph, the nature of events and causal connections between events is shown in Fig. 6. First, the base prompt is used along with the prompt for event graph initialization to generate three independent events relevant to a given personality. Then, the base prompt is combined with the prompt for the iterative generation of events to continue generating events that are caused by one or more of the events that are already present in the graph. See an example of a persona and the corresponding temporal event graph in Fig. 7. In the example, Jack aspires to be a hotel manager. Consequently, he enrolls in a hotel management course in July, and after three months, he expresses his excitement about the course on social media. In a similar vein, his passion for gaming results in an invitation from a well-known gaming company.\n' +
      '\n' +
      '#### a.2.1 Virtual Agent Architecture\n' +
      '\n' +
      'As outlined in Section 3.3, the virtual agents in our generative pipelines are composed of two mechanisms, _Reflect & respond_Park et al. (2023) and _Image sharing & response_.\n' +
      '\n' +
      '**Reflect & respond.** This mechanism operates over a combination of short-term and long-term memory. The short-term memory is a summary of a session that is conditioned on the summary from a previous session. See the prompt given to LLMs in our pipeline for generating summaries, and an example of a generated summary, in Fig. 8. The long-term memory is a database of _observations_ about each speaker, that are essentially assertive statements about the speaker\'s persona and life. See the prompt given to LLMs in our pipeline for generating observations, and an example of observations extracted from a conversation, in Fig. 9. In practice, the conversation is annotated with turn IDs for each turn, and the model is also instructed to indicate the turn IDs that directly contribute to each observation. This allows us to keep track of the evidence when using observations as the context for RAG-based models used in our experiments (see Section 5).\n' +
      '\n' +
      '**Image sharing & response.** See prompts for implementing image-sharing and image-response behaviors in Figure 10.\n' +
      '\n' +
      '### Human Filtering\n' +
      '\n' +
      'Human annotators are instructed to edit the LLM-generated conversations in the following scenarios:\n' +
      '\n' +
      '* Remove an image if it is not relevant to the current dialog or the conversation.\n' +
      '* Add context about an image to the current speaker\'s dialog if it is not discussed by them but the subsequent speaker has reacted to the image.\n' +
      '* Replace an image if it does not match the caption that was used to query for images.\n' +
      '\n' +
      'Figure 5: **Prompt for persona statement (\\(p\\)) generation and examples of personas in LoCoMo. The prompt used to generate expanded persona statements (\\(p\\)) from initial personas (\\(p_{c}\\)) for the virtual agents in our conversation generation pipeline (top) and select examples of persona statements present in the LoCoMo dataset.**\n' +
      '\n' +
      '* Edit the dialog when the information present in the dialog is inconsistent with something said (or shared through an image) in earlier or later turns.\n' +
      '* Edit the dialog to ensure that the details in the conversation are consistent with those given in the event for the session.\n' +
      '* Remove any events from the event graph if they do not appear in the conversation.\n' +
      '\n' +
      'See an example of some edits in Fig. 11.\n' +
      '\n' +
      '## Appendix B Dataset\n' +
      '\n' +
      '### Dataset Statistics\n' +
      '\n' +
      'See a breakdown of the statistics of the conversations in the LoCoMo dataset in the top panel of Table 5. Also, see a breakdown of the statistics of the annotations in the evaluation benchmark in the bottom panel of Table 5.\n' +
      '\n' +
      '### Dataset License\n' +
      '\n' +
      'The LoCoMo dataset will be released under the CC BY-NC 4.0 DEED license.8\n' +
      '\n' +
      'Footnote 8: [https://creativecommons.org/licenses/by-nc/4.0/](https://creativecommons.org/licenses/by-nc/4.0/)\n' +
      '\n' +
      '### Annotator Details\n' +
      '\n' +
      'The annotators who worked on the LoCoMo dataset were in-house annotators and we were unable to obtain their demographics due to the confidential nature of such information.\n' +
      '\n' +
      '## Appendix C Experimental Setup\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      'The conversations in the LoCoMo dataset are composed of natural language dialogs and images that require higher-order reasoning and multimodal coreference resolution, respectively. From initial studies, we observed that multimodal coreference\n' +
      '\n' +
      'Figure 6: **Prompts for temporal event graph generation.** The prompt used to generate complete personas for the LLMs in our conversation generation pipeline (top) and examples of personas present in the LoCoMo dataset.\n' +
      '\n' +
      'Figure 7: **Temporal Event Graph \\(\\mathcal{G}\\) Creation.** Each event is generated in accordance with the specified persona \\(p\\) and causal connections \\(l\\) between events are depicted to illustrate the casual relationships among them.\n' +
      '\n' +
      'resolution can be performed effectively by replacing images in LoCoMo with their captions generated using BLIP-2 Li et al. (2023), and using state-of-art LLMs to reason over natural language text interleaved with image captions. Hence, our experiments for the question answering and event summarization tasks are conducted using LLMs. We use the images directly only for experiments on the multimodal dialog generation task.\n' +
      '\n' +
      'Question Answering.We carry out experiments using three distinct methodologies: (1) **Base** involves utilizing LLMs to directly conduct the task within a constrained context. The task description comes after the dialogue history. To accommodate the restricted context window size, earlier dialogues are omitted; (2) **Long-context** employs LLMs with an extended context window to expose the models to as much dialogue context as possible; (3) **Retrieval-augmented Generation (RAG)** involves retrieving relevant context from a database of dialog history, observations, or session-level summaries. _Observations_ are assertions about each speaker extracted from the dialog history as described in SS3.3, see an example in Figure 9. Session-level _summaries_ are concise summaries of the conversation that takes place in each session, see an example in Figure 8.\n' +
      '\n' +
      'For the retrieval model, we employ DRAGON Lin et al. (2023). In the _Base_, we utilize Mistral-7B Jiang et al. (2023), LLama-70B-chat Touvron et al. (2023), gpt-3.5-turbo 9, and gpt-4-turbo 10. To assess the effectiveness in practical scenarios for _Long-context_ and _RAG_, we draw comparisons using variants of\n' +
      '\n' +
      'Figure 8: **Prompt for generating conversation summaries. The prompt used to iteratively generate a summary for the current session by conditioning on summary from preceding sessions and the raw conversation logs of the current session (top); and an example of inputs for the prompt and corresponding output summary of a session from the LoCoMo dataset.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Conversation Statistics & \\# Counts \\\\ \\hline Total. \\# conversations \\(h\\). & 50 \\\\ Avg. \\# sessions \\(k\\). in conversation \\(h\\) & 19.3 \\\\ Avg. \\# turns \\(j\\). in session \\(k\\) & 15.8 \\\\ \\hline Avg. \\# tokens. conversation \\(h\\) & 9.209.2 \\\\ Avg. \\# tokens. dialogue \\(h_{s_{j}}\\) of turn \\(j\\) in session \\(k\\) & 30.2 \\\\ Avg. \\# tokens. observation \\(o_{k_{j}}\\) of turn \\(j\\) in session \\(k\\) & 18.2 \\\\ Avg. \\# tokens. summary \\(w_{\\text{x}}\\) of session \\(k\\) & 127.4 \\\\ \\hline \\multicolumn{3}{l}{QA Benchmark Statistics} \\\\ \\hline \\# questions. single-hop retrieval & 2,705 (36\\%) \\\\ \\# questions. multi-hop retrieval & 1,104 (14.6\\%) \\\\ \\# questions. temporal reasoning & 1,547 (20.6\\%) \\\\ \\# questions. open domain knowledge & 285 (3.9\\%) \\\\ \\# questions. adversarial & 1,871 (24.9\\%) \\\\ \\multicolumn{3}{l}{**Total. \\# questions.**} \\\\ \\hline \\multicolumn{3}{l}{Event Summarization Statistics} \\\\ \\hline Avg. \\# ground truth events. in conversation \\(h\\) & 24.2 \\\\ Avg. \\# tokens. event summary & 896.5 \\\\ \\hline \\multicolumn{3}{l}{Multi-modal Dialogue Generation Statistics} \\\\ \\hline Avg. \\# images. in conversation \\(h\\) & 32.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: **Dataset Statistics of conversation and corresponding benchmark**gpt-3.5-turbo. We do not report the performance of long-context fine-tuned open-source models Chen et al. (2023) or those utilizing sliding window Bertsch et al. (2024); Dao et al. (2022) due to the variability inherent across different open-source models and the potential reduction in their capability on shorter context.\n' +
      '\n' +
      'Event Summarization.We present experiments conducted in two distinct configurations. We use both the **Base** and **Long-context** setups from the question answering task, but we refrained from including RAG since summarization requires a comprehensive understanding of the entire dialogue, rather than just retrieving a specific portion. A notable distinction in our approach, compared to the question-answering task, lies in our handling of the context. Specifically, we employ an iterative process of creating a summary of a preceding session and then use that summary as a basis to generate the summary for the subsequent session Chang et al. (2023). Further, we use a single in-context demonstration of input and output to guide the model toward selecting only significant life events for the summary.\n' +
      '\n' +
      'Multi-modal Dialogue Generation.For evaluating multi-modal dialogue generation, we train MiniGPT-5 Zheng et al. (2023) on 50 conversations generated using our automated pipeline (without human filtering) as detailed in SS3. Three distinct versions of the model were developed, each with varying training data: (1) **Base** trains on preceding dialogue turns; (2) **+ summary** trains on both prior dialogue turns and a global summary of the ongoing conversation; (3) **+ observation** trains on both preceding dialogue turns and relevant observations retrieved from the conversation history. For each of these models, we started with a MiniGPT-5 checkpoint pretrained on the MMDi-alog dataset Feng et al. (2023).\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'We use OpenAI API and Huggingface Wolf et al. (2020), as of January 2024, with specific settings of \\(temperature\\) set to 0 and \\(top_{p}\\) set to 1 for evaluation of the LoCoMo benchmark. All experiments, including those for RAG-based models, MiniGPT-5 training, and inference, are conducted on an Nvidia A6000 server with FP32. We report results from a single inference run for each model in our experiments. For MiniGPT-5, we used the hyper-parameters recommended in the original codebase and trained our models for 10 epochs, which took approximately 30 hours on a single A6000 GPU.\n' +
      '\n' +
      'We use the default implementations of BLEU11,\n' +
      '\n' +
      'Figure 9: **Prompts for generating observations from conversations. The prompt used to generate observations from a conversation (top); and an example of inputs for the prompt and corresponding output observations for a session from the LoCoMo dataset.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:18]\n' +
      '\n' +
      '## Appendix A\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c} \\hline \\hline\n' +
      '**Error Type** & **Explanation** & **Ground truth event _or_ relevant dialogs** & **Predicted event** \\\\ \\hline Missing information & Key details about event are omitted because the model fails to make causal and temporal connections over a long conversation. & Joanna submits her third screenplay on loss, identity, and \\\\  & & connection to a film contest & \\\\ \\hline Hallucination & Non-existent details or details from a different event are padded onto an event & _N_: "The gaming party was a great success!" & Nate’s wegan ice cream is a huge \\\\  & & _N_:... said they’d want to do it again next month!" & success and people want to do it \\\\  & & _N_: On another note, I made vegan ice cream...’ & again next month. \\\\ \\hline Misunder-standing of dialog cues & e.g., model confuses a light-hearted statement from a speaker as serious statement & _J_: " these trails that made me feel like writing a drama.’ & Nate considers writing his own \\\\  & & _N_:... go together. Maybe \\(\\Gamma\\)II start to think of a drama & drama screenplay. \\\\ \\hline Speaker & Event is attributed to the wrong speaker & myself and write a steeply...’ & \\\\  & & _J_: "Haha, now that would be something!." & \\\\ \\hline Saliency & Unimportant interactions in the conversation & _N_: Hey Joanna, what’s been up since we last chatted? & Joanna invites Nate to her home to \\\\  & are considered significant by model & How’s it going? & try her dairy-free ice cream recipe. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: **Taxonomy of errors in LLM-generated event summaries.** Five types of errors predominantly occur in the event summaries generated by LLMs. Examples are based on predictions from gpt-3.5-turbo.\n' +
      '\n' +
      'Figure 11: **Example of edits made by annotators.** Human annotators are instructed to make edits in the LLM-generated conversations to remove irrelevant The prompt used to generate complete personas for the LLMs in our conversation generation pipeline (top) and examples of personas present in the LoCoMo dataset.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>