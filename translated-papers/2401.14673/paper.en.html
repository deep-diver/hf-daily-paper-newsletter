<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Generative Expressive Robot Behaviors\n' +
      '\n' +
      'using Large Language Models\n' +
      '\n' +
      ' Karthik Mahadevan\n' +
      '\n' +
      'University of Toronto\n' +
      '\n' +
      'karthikm@dgp.toronto.edu Jonathan Chien\n' +
      '\n' +
      'Google Deepmind\n' +
      '\n' +
      'chienj@google.com Noah Brown\n' +
      '\n' +
      'Google Deepmind noahbrown@google.com\n' +
      '\n' +
      ' Zhuo Xu\n' +
      '\n' +
      'Google Deepmind zhuoxu@google.com Carolina Parada\n' +
      '\n' +
      'Google Deepmind carolinap@google.com Fei Xia\n' +
      '\n' +
      'Google Deepmind xiafei@google.com Andy Zeng\n' +
      '\n' +
      'Google Deepmind andyzeng@google.com Leila Takayama\n' +
      '\n' +
      'Hoku Labs takayama@hokulabs.com Dorsa Sadigh\n' +
      '\n' +
      'Google Deepmind dorsas@google.com\n' +
      '\n' +
      '###### Abstract.\n' +
      '\n' +
      'People employ expressive behaviors to effectively communicate and coordinate their actions with others, such as nodding to acknowledge a person glancing at them or saying "_excuse me_" to pass people in a busy corridor. We would like robots to also demonstrate expressive behaviors in human-robot interaction. Prior work proposes rule-based methods that struggle to scale to new communication modalities or social situations, while data-driven methods require specialized datasets for each social situation the robot is used in. We propose to leverage the rich social context available from large language models (LLMs) and their ability to generate motion based on instructions or user preferences, to generate _expressive robot motion_ that is _adaptable_ and _composable_, building upon each other. Our approach utilizes few-shot chain-of-thought prompting to translate human language instructions into parametrized control code using the robot\'s available and learned skills. Through user studies and simulation experiments, we demonstrate that our approach produces behaviors that users found to be competent and easy to understand. Supplementary material can be found at [https://generative-expensive-motion.github.io/](https://generative-expensive-motion.github.io/).\n' +
      '\n' +
      'Generative expressive robot behaviors, in-context learning, language corrections 2024 acmcopyright 11-14, 2024, Boulder, CO, USA 2024 2024 2024 2024 2024\n' +
      '\n' +
      'Generative expressive robot behaviors, in-context learning, language corrections 2024\n' +
      '\n' +
      '## 1. Introduction\n' +
      '\n' +
      'People employ a wide range of expressive behaviors to effectively interact with others on a daily basis. For instance, a person walking by an acquaintance may briefly glance at them and not to acknowledge their presence. A person might apologetically say, "excuse me" to squeeze through a tight hallway, where a group of people are conversing. In much the same manner, we would like robots to also demonstrate expressive behaviors when interacting with people. Robots that don\'t have expressive capabilities will need to re-plan their paths to avoid the crowded hallway. On the other hand, robots that have expressive capabilities might actually be able to persuade the group of people to make room for them to squeeze by, thereby improving the robot\'s efficiency in getting its job done.\n' +
      '\n' +
      'Prior work has demonstrated the value of expressive robot behaviors, and explored approaches for generating behaviors for various\n' +
      '\n' +
      'Figure 1. We present Generative Expressive Motion (GenEM), a new approach to autonomously generate expressive robot behaviors. GenEM takes a desired expressive behavior (or a social context) as language instructions, reasons about human social norms, and generates control code for a robot using pre-existing robot skills and learned expressive behaviors. Iterative feedback can quickly modify the behavior according to user preferences. Here, the \\({}^{*}\\) symbols denote frozen large language models.\n' +
      '\n' +
      'purposes and contexts, including general-purpose use (Kirshik Mahadevan et al., 2019), manipulation settings, where transparency is important (Kirshik Mahadevan et al., 2019), and everyday scenarios where social norms must be observed (such as interacting with a receptionist) (Kirshik et al., 2019). Approaches can be rule- or template-based (Kirshik et al., 2019; Mahadevan et al., 2019; Mahadevan et al., 2019), which often rely on a rigid template or a set of rules to generate behaviors. This often leads to robot behaviors that can be expressive, but do not scale to new modalities or variations of human preferences. On the other hand, data-driven techniques offer the promise of flexibility and the ability to adapt to variations. Prior work have studied data-driven techniques that generate expressive motion (Kirshikik et al., 2019), but these methods also have their shortcomings as they often need specialized datasets for each social interaction where a particular behavior is used (e.g., for affective robot movements (Kirshik et al., 2019; Kirshik et al., 2019)).\n' +
      '\n' +
      'Our goal is to enable robots to generate expressive behavior that is flexible: behaviors that can _adapt_ to different human preferences, and be _composed_ of simpler behaviors. Recent work show that large language models (LLMs) can synthesize code to control virtual (Kirshik et al., 2019) and embodied agents (Kirshik et al., 2019; Mahadevan et al., 2019), help design reward functions (Kirshik et al., 2019; Mahadevan et al., 2019), enable social and common-sense reasoning (Kirshik et al., 2019), or perform control and sequential decision making tasks through in-context learning (Kirshik et al., 2019; Mahadevan et al., 2019; Mahadevan et al., 2019) by providing a sequence of desirable inputs, and outputs in the prompt. Our key insight is to tap into the rich social context available from LLMs to generate adaptable and composable expressive behavior. For instance, an LLM has enough context to realize that it is polite to make an eye contact when greeting someone. In addition, LLMs enable the use of corrective language such as "bend your arm a bit more!" and the ability to generate motion in response to such instructions. This makes LLMs a useful framework for autonomously generating expressive behavior that flexibly respond to and learn from human feedback in human-robot interaction settings.\n' +
      '\n' +
      'Leveraging the power and flexibility provided by LLMs, we propose a new approach, Generative Expressive Motion (GenEM), for autonomously generating expressive robot behaviors. GenEM uses few-shot prompting and takes a desired expressive behavior (or a social context) as language instructions, performs social reasoning (akin to chain-of-thought (Kirshik et al., 2019)), and finally generates control code for a robot using available robot APIs. GenEM can produce multimodal behaviors that utilize the robot\'s available affordances (e.g., speech, body movement, and other visual features such as light strips) to effectively express the robot\'s intent. One of the key benefits of GenEM is that it responds to live human feedback - adapting to iterative corrections and generating new expressive behaviors by composing the existing ones.\n' +
      '\n' +
      'In a set of online user studies, we compared behaviors generated on a mobile robot using two variations of GenEM, with and without user feedback (a non-expert in HRI behavior design), to a set of behaviors designed by a professional character animator (or the _oracle animator_). We show that behaviors generated by GenEM and further adapted with user feedback were positively perceived by users, and in some cases better perceived than the oracle behaviors.\n' +
      '\n' +
      'In additional experiments with the mobile robot and a simulated quadruped, we show that GenEM: (1) performs better than a version where language instructions are directly translated into code, (2) allows for the generation of behaviors that are agnostic to embodiment, (3) allows for the generation of composable behaviors that build on simpler expressive behaviors, and finally, (4) adapt to different types of user feedback.\n' +
      '\n' +
      '## 2. Related Work\n' +
      '\n' +
      '**Expressive Behavior Generation.** Researchers have made significant efforts towards generating socially acceptable behavior for both robots and virtual humans. These can largely categorized into _rule-based_, _template-based_, and _data-driven_(Kirshik et al., 2019) behavior generation approaches. We define rule-based approaches as those that require a formalized set of rules and operations (typically provided by a person) which are used to generate subsequent robot behavior.\n' +
      '\n' +
      'Rule-based approaches enable behavior generation through formalized sets of rules and operations (Kirshik et al., 2019). Some methods include interfaces that lets users manually specify interaction rules and logic (Kirshik et al., 2019; Mahadevan et al., 2019; Mahadevan et al., 2019; Mahadevan et al., 2019). Other methods work by observing and modelling humans (Kirshik et al., 2019; Mahadevan et al., 2019; Mahadevan et al., 2019; Mahadevan et al., 2019). Despite their use, rule-based approaches face several issues, including limited expressivity in the generated behavior due to the requirement of formal rules, and the reduced ability to produce multimodal behaviors as the number of modalities increases (Kirshik et al., 2019). Template-based methods formulate generic templates for interaction by learning from traces of interaction data (Kirshik et al., 2019; Mahadevan et al., 2019). Templates can translate few examples of human traces into reusable programs through program synthesis (Kirshik et al., 2019; Mahadevan et al., 2019). Traces can be collected by observing humans interacting (Kirshik et al., 2019; Mahadevan et al., 2019), or through approaches such as sketching (Kirshik et al., 2019) or tangibles on a tabletop (Kirshik et al., 2019). Overall, prior rule- and template-based methods enforce strong constraints to enable behavior generation but are limited in their expressivity. In contrast, GenEM enables increased expressivity in the initial behavior generation as well as iterative improvements through live user feedback.\n' +
      '\n' +
      'On the other hand, data-driven approaches produce behaviors using models trained on data. Some methods learn _interaction logic_ through data and use this to produce multimodal behaviors via classical machine learning methods (Kirshik et al., 2019; Mahadevan et al., 2019; Mahadevan et al., 2019). Other methods train on hand-crafted examples through generative models (Kirshik et al., 2019; Mahadevan et al., 2019). For instance, predicting when to use backchanneling behaviors (i.e., providing feedback during conversation such as by nodding) has been learned through batch reinforcement learning (Kirshik et al., 2019) and recurrent neural networks (Kirshik et al., 2019). Lastly, recent work has investigated how to learn cost functions for a target emotion from user feedback (Kirshik et al., 2019), or even learn an emotive latent space to model many emotions (Kirshik et al., 2019). However, these approaches are data inefficient and require specialized datasets per behavior to be generated, while GenEM is able to produce a variety of expressive behaviors with a few examples through in-context learning.\n' +
      '\n' +
      '**LLMs for Robot Planning and Control.** Recent work has achieved great success by leveraging LLMs in downstream robotics tasks specifically by providing sequences of desirable input-output pairs in context (Kirshik et al., 2019; Mahadevan et al., 2019; Mahadevan et al., 2019). In addition, LLMs have been used for long-horizon task planning (Kirshik et al., 2019; Mahadevan et al., 2019), and can react to environmental and human feedback (Kirshik et al., 2019). LLMs have been leveraged for designing reward functions for training reinforcement learning agents (Kirshik et al., 2019; Mahadevan et al., 2019). Research has also shown that LLMs can enable social and commonsense reasoning (Kirshik et al., 2019) as well as infer user preferences by summarizing interactions with humans (Kirshik et al., 2019). Most relevant to our approach are prior work where LLMs synthesize code to control virtual (Kirshik et al., 2019)and robotic agents (Levy and Goyal, 2017; Goyal, 2017) by using existing APIs to compose more complex robot behavior as programs. We are also encouraged by work demonstrating that language can be used to correct robot manipulation behaviors online (Beng et al., 2019). Taken together, we propose to leverage the rich social context available from LLMs, and their ability to adapt to user instructions, to generate expressive robot behaviors. To our knowledge, LLMs have not previously been used to generate expressive robot behaviors that adapt to user feedback.\n' +
      '\n' +
      '## 3. Generative Expressive Motion\n' +
      '\n' +
      '**Problem Statement.** We aim to tackle the problem of expressive behavior generation that is both adaptive to user feedback and composable so that more complex behaviors can build on simpler behaviors. Formally, we define being _expressive_ as the distance between some expert expressive trajectory that could be generated by an animator (or demonstrated) \\(\\tau_{\\text{expert}}\\) and a robot trajectory \\(\\tau\\). \\(\\text{dist}(\\tau,\\tau_{\\text{expert}})\\) can be any desirable distance metric between the two trajectories, e.g., dynamic time warping (DTW). GenEM aims to minimize this distance \\(d^{*}=\\min\\text{dist}(\\tau,\\tau_{\\text{expert}})\\).\n' +
      '\n' +
      'Our approach (Figure 2) uses several LLMs in a modular fashion so that each _LLM agent_ plays a distinct role. Later, we demonstrate through experiments that a modular approach yields better quality of behaviors compared to an end-to-end approach. GenEM takes user language instructions \\(l_{in}\\in L\\) as input and outputs a robot policy \\(\\pi_{\\theta}\\), which is in the form of a parameterized code. Human iterative feedback \\(f_{i}\\in L\\) can be used to update the policy \\(\\pi_{\\theta}\\). The policy parameters get updated one step at a time given the feedback \\(f_{i}\\), where \\(i\\in\\{1,\\dots,K\\}\\). The policy can be instantiated from some initial state \\(s_{0}\\in S\\) to produce trajectories \\(\\tau=\\{s_{0},a_{0},\\dots,a_{N-1},s_{N}\\}\\) or instantiations of expressive robot behavior. Below we describe one sample iteration with human feedback \\(f_{i}\\). Please refer to **Appendix A** for full prompts.\n' +
      '\n' +
      '**Expressive Instruction Following.** The input to our approach is a language instruction \\(l_{in}\\in L\\), which can either be a description of a social context where the robot needs to perform an expressive behavior by following social norms (e.g., "A person walking by waves at you.") _or_ an instruction that describing an expressive behavior to be generated (e.g., "Nod your head"). The input prompt is of the form \\(u=[h_{pre},l_{in}]\\) where \\(h_{pre}\\) is the prompt prefix that adds context about the role of the LLM and includes few-shot examples. The output of the LLM call is a string of the form \\(h=[h_{cot},h_{exp}]\\) consisting of Chain-of-Thought reasoning \\(h_{cot}\\)(Levy and Goyal, 2017) and the human expressive motion \\(h_{exp}\\) in response to the instruction. For example, for \\(l_{in}=\\)_"Acknowledge a person walking by. You cannot speak."_, the _Expressive Instruction Following_ module would output \\(h_{exp}=\\)_Make eye contact with the person. Smile or not to acknowledge their presence._ Examples of \\(h_{cot}\\) could be: _"The person is passing by and it\'s polite to acknowledge their presence. Since I cannot speak, I need to use non-verbal communication. A nod or a smile is a universal sign of acknowledgement."_\n' +
      '\n' +
      '**From Human Expressive Motion to Robot Expressive Motion.** In the next step, we use an LLM to translate human expressive motion \\(h\\) to robot expressive motion \\(r\\). The prompt takes the form \\(u=[r_{pre},l_{in},h,r_{i-1_{opt}},f_{i-1_{opt}}]\\) where \\(r_{pre}\\) is the prompt prefix setting context for the LLM, contains few-shot examples, and describes the robot\'s capabilities some of which are pre-defined (e.g., the ability to speak or move its head) and others which are learned from previous interactions (e.g., nodding or approaching a person). Optionally, the prompt can include the response from a previous step \\(r_{i-1}\\) and response to user iterative feedback from a previous step \\(f_{i-1}\\). The output is of the form \\(r=[r_{cot},r_{exp}]\\) consisting of the LLM\'s reasoning and the procedure to create expressive robot motion. An example response \\(r_{exp}\\) could include: _"I) Use the head\'s pan and tilt capabilities to face the person who is walking by. 2) Use the light strip to display a pre-programmed pattern that mimics a smile or nod."_. An example of \\(r_{cot}\\) could be: _"The robot can use its head\'s pan and tilt capabilities to make \'eye contact" with the person. The robot can use its light strip to mimic a smile or nod."_.\n' +
      '\n' +
      '**Translating Robot Expressive Motion to Code.** In the following step, we use an LLM to translate the step-by-step procedure of how to produce expressive robot motion into executable code. We propose a skill library in a similar fashion to that of Voyager (Voyager, 2017) containing existing robot skill primitives, and parametrized robot code \\(\\pi_{\\theta}\\) representing previously learned expressive motions. To facilitate this, the prompt encourages modular code generation by providing examples where small, reusable functions with docstrings and named arguments are used to generate more complex functions that describe an expressive behavior. To generate code, the prompt to\n' +
      '\n' +
      'Figure 2. Generative Expressive Motion. Given a language instruction \\(l_{in}\\), the _Expressive Instruction Following_ module reasons about the social norms and outputs how a human might express this behavior (\\(h\\)). This is translated into a procedure for robot expressive behavior using a prompt describing the robot’s pre-existing capabilities (\\(r_{pre}\\)) and any learned expressive behaviors. Then, the procedure is used to generate parametrized robot code \\(c\\) that can be executed. The user can provide iterative feedback \\(f_{i}\\) on the behavior which is processed to determine whether to re-run the robot behavior module first followed by the code generation module or just the code generation module. Note: \\({}^{*}\\) shown on top of all the gray modules denotes them as frozen LLMs.\n' +
      '\n' +
      'the LLM takes the form \\(u=[c_{pre},l_{in},h_{exp},r_{exp,i-1_{opt}},c_{i-1_{opt}},\\hat{f_{i-1}},\\)\\(r_{exp}]\\). Here, \\(c_{pre}\\) provides context about its role as a code generating agent to the LLM, includes the robot\'s current skill library, and contains few-shot examples. Optionally, the expressive robot motion \\(r_{exp,i-1}\\), and code \\(\\hat{c_{i-1}}\\) from a previous step can be provided as well as LLM output \\(\\hat{f_{i-1}}\\) responding to the user feedback \\(\\hat{f_{i-1}}\\). The output \\(c\\) is parametrized robot code representing the policy \\(\\pi_{\\theta}\\) for the expressive behavior (see Figure 2 for sample output). Later, the generated code can be incorporated into the robot\'s skill library to utilize in future expressive behavior generations.\n' +
      '\n' +
      '**Propagating Human Feedback.** In the final (optional) step, we use an LLM to update the generated expressive behavior in response to human feedback \\(\\hat{f_{i}}\\) if the user is not satisfied with the generated behavior. The prompt is of the form \\(u=[\\hat{f_{pre}},l_{in},r_{exp},c,f_{i}]\\), where \\(\\hat{f_{pre}}\\) provides context to LLM, and includes both the procedure for expressive robot motion \\(r_{exp}\\) and the generated code \\(c\\). The output is of the form \\(f=[\\hat{f_{cont}},\\hat{f_{i}}]\\) and includes the LLM\'s reasoning and the changes \\(\\hat{f_{i}}\\) needed to improve the current expressive motion based on human feedback. The output also classifies whether the changes require an iterative call to modify the procedure for generating the robot\'s expressive behavior \\(r\\) and then translating it to code \\(c\\), \\(or\\) just modifying the generated code \\(c\\).\n' +
      '\n' +
      'For example, the user could state \\(\\hat{f_{i}}=\\)_"When you first see the person, nod at them."_, and the output \\(\\hat{f_{i}}\\) could be: _"[Change: What robot should do]...As soon as the robot sees the person, it should not at them. After nodding, the robot can use its light strip to display a pre-programmed pattern that mimics a smile or nod..."_. As an example, \\(f_{cont}\\) could state: _" The feedback suggests that the robot\'s action of acknowledging the person was not correct. This implies that the robot should not at the person when it first sees them."_\n' +
      '\n' +
      '## 4. User Studies\n' +
      '\n' +
      'We conducted two user studies to assess whether our approach, GenEM, can be used to generate expressive behaviors that are perceivable by people. We generated two versions of behaviors: GenEM, and GenEM with iterative Feedback (or _GenEM++_). In both studies, all comparisons were made against behaviors designed by a professional animator and implemented by a software developer, which we term the _oracle animator_. In the _first study_, our goal was to assess whether behaviors that are generated using GenEM and GenEM++ would be perceived similarly to the behaviors created using the oracle animator. In the _second study_, we attempted to generate behaviors using GenEM and GenEM++ that were similar to the behaviors created using the oracle animator. Both studies aim to demonstrate that our approach is _adaptable_ to human feedback.\n' +
      '\n' +
      '**Behaviors.** All behaviors were generated on a mobile robot platform (please see website 1 for full clips). The robot has several capabilities that can be used to generate behaviors through existing APIs, including a head that can pan and tilt, a base that can translate, rotate, and navigate from point to point, a light strip that can display different colors and patterns, and finally, a speech module that can generate utterances and nonverbal effects. To enable the comparison of behaviors produced in the three conditions - oracle animator, GenEM, and GenEM++, we recorded video clips of each behavior (see Figure 3). To ensure consistency across conditions, behaviors in each condition were recorded in the same physical locations under similar lighting conditions. The GenEM and GenEM++ behaviors were generated by sampling OpenAI\'s GPT-4 APIs for text completion (Shen et al., 2019) (gpt-4-0613) with the temperature set to 0.\n' +
      '\n' +
      '**Study Procedure.** After providing informed consent, participants completed an online survey to evaluate the robot\'s expressive behaviors in both studies. The survey is divided into three sections (one per behavior condition) and clips within each condition randomly appeared. To minimize ordering effects, a Balanced Latin Square design (3 x 3) was used. For each behavior in each condition, participants watched an unlabeled video clip 1, and then answered questions. All participants received remuneration after the study.\n' +
      '\n' +
      '**Measures.** In both studies, participants completed a survey to assess each behavior, answering three 7-point Likert scale questions assessing their confidence on their understanding of the behavior, the difficulty in understanding what the robot is doing, and the competency of the robot\'s behavior. Participants also provided an open-ended response describing what behavior they _believed_ the robot was attempting to express.\n' +
      '\n' +
      '**Analysis.** One-way repeated-measures ANOVA were performed on the data with post-hoc pairwise comparisons where there were significant differences with Bonferroni corrections applied. When reporting comparisons between conditions, we define _instances_ as pairwise significant conditions for at least one of the three Likert-scale questions asked about a behavior.\n' +
      '\n' +
      'Footnote 1: [https://generative-expensive-motion.github.io/](https://generative-expensive-motion.github.io/)\n' +
      '\n' +
      '### Study 1: Benchmarking Generative Expressive Motion\n' +
      '\n' +
      'To determine whether our approach produces expressive behaviors that people can perceive, we conducted a within-subjects user study with thirty participants (16 women, 14 men), aged 18 to 60 (18-25: 3, 26-30: 9, 31-40: 9, 41-50: 7, 51-60: 2). One participant did not complete the entire survey and their data was omitted.\n' +
      '\n' +
      '**Behaviors.** We generated ten expressive behaviors (see Figure 3) ranging in complexity: _Nod_, shake head (_Shake_), wake up (_Wake_), excuse me (_Excuse_), recoverable mistake (_Recoverable_), unrecoverable mistake (_Unrecoverable_), acknowledge person walking by (_Acknowledge_), follow person (_Follow_), approach person (_Approach_) and pay attention to person (_Attention_). The input included a one-line instruction (e.g., _Respond to a person saying_, _"Come here. You cannot speak."_).\n' +
      '\n' +
      '**Conditions.** The oracle animator condition consisted of professionally animated behaviors that were implemented on the robot through scripting. To create the GenEM behaviors, we sampled our approach five times to generate five versions of each behavior. Since the behaviors were sampled with a temperature of 0, they shared significant overlap with small variations amongst them (due to non-determinism in GPT-4 output; please see **Appendix C** for samples generated using the same prompt). Then, six participants experienced in working with the robot were asked to rank them. The best variation for each behavior was included as part of the GenEM behaviors. To generate the GenEM++ behaviors, we recruited one participant experienced in using the robot (but inexperienced in HRI behavior design) and asked them to provide feedback on the best rated version of each behavior. Feedback was used to iteratively modify the expressive behavior until the participant was satisfied with the result, or upon reaching the maximum number of feedback rounds (n = 10). We note that although participants rated the behaviors in the studies, the behavior generation is personalized to the user who provided the initial feedback, which may not reflect the preferences of all potential users (e.g., study participants).\n' +
      '\n' +
      '**Hypotheses.** We hypothesized that the perception of the GenEM++ behaviors would not differ significantly from the oracle animator behaviors (**H1**). We also hypothesized that the GenEM behaviors would be less well-received compared to the GenEM++ and the oracle animator behaviors (**H2**).\n' +
      '\n' +
      '**Quantitative Findings.** Figure 4 summarizes participants\' responses to the survey questions for each behavior. The results show that the GenEM++ behaviors were worse than the oracle animator behaviors in 2/10 instances (_Shake_ and _Follow_). In contrast, the GenEM++ behaviors received higher scores than the oracle animator behaviors in 2/10 instances (_Excuse_ and _Approach_). Hence, **H1 is** supported by our data - the GenEM++ behaviors were well received and the oracle animator behaviors were not significantly better received than the GenEM++ behaviors.\n' +
      '\n' +
      'The GenEM behaviors were worse received compared to the oracle animator behaviors in 2/10 instances (_Acknowledge Walk_ and _Follow_) whereas the GenEM behaviors were better received than the oracle animator behaviors in 2/10 instances (_Excuse_ and _Approach_). This was surprising because user feedback was not incorporated into the behavior generation in this condition. Besides 1/10 instances (_Shake_), there were no significant differences in the perceptions of the GenEM and GenEM++ behaviors. Hence, we did not find support for **H2**. We performed equivalence tests (equivalence bound: +/- 0.5 Likert points) but did not find any sets of behaviors to be equivalent. Overall, the results support the finding that GenEM (even with an untrained user providing feedback) produces expressive robot behaviors that users found to be competent and easy to understand.\n' +
      '\n' +
      '### Study 2: Mimicking the Oracle Animator\n' +
      '\n' +
      'We conducted an additional within-subjects user study with twenty four participants (21 men, 2 women, 1 prefer not to say), aged 18-60 (18-25: 4, 26-30: 3, 31-40: 12, 41-50: 4, 51-60: 1) to assess whether using GenEM to generate behaviors that resembled the oracle animator would be perceived differently. One participant did not complete the entire survey and their data was omitted.\n' +
      '\n' +
      '**Behaviors.** We generated ten expressive behaviors ranging in complexity, with eight overlapping 2 behaviors from the first study (see Figure 3): nod (_Nod_), shake head (_Shake_), wake up (_Wake_), excuse me (_Excuse_), recoverable mistake (_Recoverable_), unrecoverable mistake (_Unrecoverable_), acknowledge person walking by (_Acknowledge Walking_), acknowledge person stopping by (_Acknowledge Stop_), follow person (_Follow_), and teaching session (_Teach_). Behaviors that were different from the first study were chosen to add further complexity - e.g., longer single-turn interactions such as _teaching_, that started with a person walking up a robot, teaching it a lesson, and lastly the robot acknowledging that it understood the person\'s instructions. Unlike in the first study, the prompts were more varied and sometimes included additional descriptions such as for the more complex behaviors (see **Appendix B** for full prompts for each behavior). To generate each GenEM behavior, we sampled our approach ten times after which an experimenter selected the version that appeared most similar to the equivalent oracle animator behavior when deployed on the robot. To create each GenEM++ behavior, an experimenter refined the GenEM behavior through iterative feedback until it appeared similar to the equivalent oracle animator behavior or after exceeding the maximum number of feedback rounds (n = 10) 1.\n' +
      '\n' +
      'Footnote 2: Some behaviors in the second study differ from the first study as they are too complex to express as a single line instruction which we maintained for consistency in the first study. Instead, in the first study, these complex behaviors were broken down into simpler behaviors (e.g., teaching is equivalent to approaching and paying attention).\n' +
      '\n' +
      '**Hypotheses.** We hypothesized that user perceptions of the GenEM++ behaviors would not significantly differ when compared to the oracle animator behaviors (**H3**). We also suppose that the behaviors in the GenEM condition would be perceived as worse than the GenEM++ and oracle animator behaviors (**H4**).\n' +
      '\n' +
      '**Quantitative Findings.** The results of the study are summarized in Figure 5. They show that the GenEM++ behaviors were worse received than the oracle animator behaviors in 2/10 instances (_Acknowledge Walk_ and _Follow_) whereas the GenEM++ behaviors were more positively received than the oracle animator in 2/10 instances (_Excuse_ and _Teach_). Hence, our hypothesis is supported by the data\n' +
      '\n' +
      'Figure 3. Behaviors tested in the two user studies where the behaviors labelled in green denote those unique to the first study and behaviors labelled in blue denote those unique to the second study. The remaining behaviors (8) were common among the two studies.\n' +
      '\n' +
      '**(H3)** - the GenEM++ behaviors well received and the oracle animator behaviors were not significantly better perceived. When comparing the oracle animator behaviors and GenEM behaviors, there were 4/10 instances where the GenEM behaviors were worse received (_Wake_, _Acknowledge Walk_, _Acknowledge Stop_, and _Follow_), and 1/10 instances where the GenEM behaviors were more positively rated (_Excuse_). As with the first study, it is somewhat surprising that the GenEM behaviors were better received than the baselines in one instance; although they resemble them, they do not capture all the nuances present in the oracle animator behaviors since user feedback is not provided. Lastly, the GenEM behaviors were rated worse than the GenEM++ behaviors in 2/10 instances (_Wake_ and _Teach_) whereas there were 0/10 instances where the reverse was true. Hence, we did not find support for the last hypothesis **(H4).** Upon performing equivalence tests (equivalence bound: +/- 0.5 Likert points), we did not find any sets of behaviors to be equivalent. Overall, the findings suggest that expressive robot behaviors produced using our approach (with user feedback) were found competent and easy to understand by users.\n' +
      '\n' +
      '## 5. Experiments\n' +
      '\n' +
      'We conducted a set of experiments to carefully study different aspects of GenEM. This includes ablations to understand the impact of our prompting structure and the modular calls to different LLMs versus an end-to-end approach. Further, through an experiment, we demonstrate that GenEM can produce modular and composable behaviors, i.e., behaviors that build on top of each other. The behaviors were generated by sampling OpenAI\'s GPT-4 APIs for text\n' +
      '\n' +
      'Figure 4. Plots showing participants’ survey responses to three questions about each behavior (of 10) in each condition (of 3) in the 1st user study. Bars at the top denote significant differences, where (*) denotes p<.05 and (**) denotes p<.001. Error bars represent standard error. The first plot shows the average score for each question across conditions. The arrows reflect the direction in which better scores lie.\n' +
      '\n' +
      'Figure 5. Plots showing participants’ survey responses to three questions about each behavior (of 10) in each condition (of 3) in the 2nd user study. Bars at the top denote significant differences, where (*) denotes p<.05 and (**) denotes p<.001. Error bars represent standard error. The first plot shows the average score for each question across conditions. The arrows reflect the direction in which better scores lie.\n' +
      '\n' +
      'completion [32] (gpt-4-0613) with the temperature set to 0. In addition to our user study and experiments on the mobile manipulator, we conducted further experiments using a quadruped simulated in Gazebo/Unity via ROS (see Figure 6).\n' +
      '\n' +
      '**Ablations.** We performed ablations to compare GenEM to an end-to-end approach that takes language instructions and makes one call to an LLM to generate an expressive behavior. The ablations were performed using existing APIs for the mobile robot. The behaviors examined were identical to the first user study along with the prompts. Each prompt was sampled five times to generate behaviors and executed on the robot to verify _correctness_. Further, an experimenter examined the code to check whether the behavior code incorporated reasoning to account for human social norms. The results for code correctness and social norm appropriateness are shown in Table 1. Overall, our approach produced higher success rates compared to the ablated variation where no successful runs were generated for 2 behaviors - _Excuse_ and _Follow_. For the _Excuse_ behavior, the robot must check the user\'s distance and signal to a person that they are in its way. However, for the ablated variation, the distance was never checked in the attempts. For the _Follow_ behavior, the code called functions that were not previously defined, and used the wrong input parameter type when calling robot APIs, resulting in zero successful attempts. Further, nearly all generated functions were missing docstrings and named arguments, which could make it difficult to use them in a modular fashion for more complex behaviors (despite providing few-shot code examples).\n' +
      '\n' +
      'We qualitatively observed that behaviors generated by GenEM reflected social norms, particularly for more complex behaviors, and looked similar for simpler behaviors. For instance, the _Excuse_ behavior generated by GenEM used the speech module to say, _"Excuse me"_. For the _Attention_ behavior, the ablated variations looked at the person, turned on the light strip, and then turned it off, whereas the GenEM variations also incorporated periodic nodding to mimic "active listening". For the _Approach_ behavior, the GenEM variations always incorporated a nod before moving towards the person while the ablated variations never used nodding; instead lights were used in two instances.\n' +
      '\n' +
      '**Cross-Embolment Behavior Generation.** We sampled the same prompts in the first user study five times per behavior using API for a simulated Spot robot. The results, summarized in Table 2, show that we were able to generate most expressive behaviors using the same prompts using a different robot platform with its own affordances and APIs. However, some generated behaviors such as _Approach_ included variations where the robot navigated to the human\'s location instead of a safe distance near them, which would be considered a social norm mismatch (possibly due to the lack of a distance threshold parameter in the translate API), while some did not account for the human (e.g., the robot rotating an arbitrary angle instead of towards the human for _Attention_). Overall, the success rates hint at the generality of our approach to differing robot embodiments.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & \\multicolumn{2}{c}{**GenEM**} & \\multicolumn{2}{c}{**Ablated**} \\\\  & _Execution_ & _Norms_ & _Execution_ & _Norms_ \\\\ \\hline Nod & 5 & 0 & 5 & 2 \\\\ Shake & 5 & 0 & 5 & 2 \\\\ Wake & **4** & 2 & 3 & 0 \\\\ Excuse & 5 & 3 & 0 & - \\\\ Recoverable & 3 & 0 & 5 & 1 \\\\ Unrecoverable & 5 & 0 & 5 & 0 \\\\ Acknowledge & 5 & 1 & 5 & 0 \\\\ Follow & **3** & 1 & 0 & - \\\\ Approach & 5 & 1 & 5 & 3 \\\\ Attention & **4** & 0 & 1 & 0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1. Ablations on the mobile robot platform showing the successful attempts of behavior generation when sampling each prompt five times to compare our approach (without feedback) against a variation without the _Expressive Instruction Following_ module and subsequently the module translating human expressive motion to robot expressive motion. The _Execution_ column indicates the number of successful attempts (_5_). The _Norms_ column indicates the number of attempts where social norms were not appropriately followed (coded by the experimenter).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & _Execution_ & _Norms_ \\\\ \\hline Nod & 5 & 0 \\\\ Shake & 5 & 0 \\\\ Wake & 5 & 0 \\\\ Excuse & 3 & 0 \\\\ Recoverable & 5 & 2 \\\\ Unrecoverable & 4 & 0 \\\\ Acknowledge & 4 & 1 \\\\ Follow & 2 & 2 \\\\ Approach & 5 & 5 \\\\ Attention & 1 & 0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2. Behaviors generated on the quadruped in simulation showing successful attempts of behavior generation when sampling each prompt five times. The _Execution_ column indicates the number of successful attempts (_5_). The _Norms_ column indicates the number of attempts where social norms were not properly observed (coded by the experimenter).\n' +
      '\n' +
      'Figure 6. Quadruped simulated in Gazebo performing the _Recoverable mistake_ behavior (top) and _Unrecoverable mistake_ (bottom) generated by GenEM prior to feedback. After making a recoverable mistake, the robot demonstrates it made a mistake by turning away, lowering its legs, and flashing red lights to convey regret but then returns to its initial position and flashes a green light. In contrast, an unrecoverable mistake causes the robot to lower its height, display red lights for a brief period, and bow forwards and maintains this pose.\n' +
      '\n' +
      '**Composing Complex Expressive Behaviors.** In the user studies, all behaviors were generated from scratch using few-shot examples and existing robot APIs. We attempted to generate more complex behaviors using a set of learned expressive behaviors from previous interactions -- these skills (represented as functions with docstrings) were appended to the prompts describing the robot\'s capabilities (step 2 of our approach) as well as the robot\'s API (step 3 of our approach). The learned behaviors used in the prompt were: _nodding, making eye contact, blinking the light strip, looking around,_ and _shaking_. We prompted GenEM to generate three behaviors, varying in complexity: _Acknowledge Walk, Approach_, and expressing confusion (_Confusion_). All of these behaviors were generated on the quadruped without providing feedback, using instructions that contained a single line description of the desired behavior. We sampled GenEM five times to assess the frequency with which learned behaviors would be included in the outputted program. To assess success, an experimenter checked whether the generated code utilized a combination of robot APIs and learned APIs (see Table 3). For the approach behavior, it was surprising to note that the _nod head_ behavior was never utilized whereas blinking lights were always used. For expressing confusion, it was surprising that 4/5 instances generated code for looking around, but only 1/5 instances used the existing _looking around_ behavior.\n' +
      '\n' +
      '**Adaptability to Human Feedback.** In the user studies, feedback had some effect on the perception of the generated behaviors. Further, we qualitatively observed that feedback could steer the behavior generation in different ways. We studied this in an experiment where we generated three behaviors from the two prior studies: _Excuse, Approach_, and _Acknowledge Stop_. Each behavior was generated using a single-line description as before, and without any learned robot APIs. We attempted to modify the generated behavior through four types of feedback: (1) adding an action and enforcing that it must occur before another action, (2) swapping the order of the actions, (3) making a behavior repeat itself (loops), and (4) removing an existing capability without providing an alternative (e.g., removing the light strip as a capability after producing a behavior that uses the light strip). Overall, the results (see Table 4) suggest that it is possible to modify the behavior according to the type of feedback provided, though removing capabilities lead to calling undefined functions more often.\n' +
      '\n' +
      '## 6. Discussion\n' +
      '\n' +
      '**Summary.** In this work, we proposed an approach, GenEM, to generate and modify expressive robot motions using large language models by translating user language instructions to robot code. Through user studies and experiments, we have shown that our framework can quickly produce expressive behaviors by way of in-context learning and few-shot prompting. This reduces the need for curated datasets to generate specific robot behaviors or carefully crafted rules as in prior work. In the user studies, we demonstrated that participants found the behaviors generated using GenEM with user feedback competent and easy to understand, and in some cases perceived significantly more positively than the behaviors created by an expert animator. We have also shown that our approach is _adaptable_ to varying types of user feedback, and that more complex behaviors can be _composed_ by combining simpler, learned behaviors. Together, they form the basis for the rapid creation of expressive robot behaviors conditioned on human preferences.\n' +
      '\n' +
      '**Limitations and Future Work.** Despite the promise of our approach, there are a few shortcomings. Our user studies were conducted online through recorded video clips, and although this is a valid methodology (Han et al., 2018; Wang et al., 2019), it may not reflect how participants would react when in the physical proximity of the robot (Wang et al., 2019). Hence, further studies involving interactions with the robot should be pursued. Some inherent limitations of current LLMs should be noted, including small context windows and the necessity for text input.\n' +
      '\n' +
      'In our work, we only evaluate single-turn behaviors (e.g., acknowledugging a passerby), but there are opportunities to generate behaviors that are multi-turn and involve back-and-forth interaction between the human and the robot. Future work should also explore generating motion with a larger action space such as by including the manipulator and gripper. Although we have shown that our approach can adapt to user feedback and their preferences, there is currently no mechanism to learn user preferences over a longer period. In reality, we expect that users will exhibit individual differences in their preferences about the behaviors they expect robots to demonstrate in a given situation. Hence, learning preferences in-context (Wang et al., 2019) may be a powerful mechanism to refine expressive behaviors.\n' +
      '\n' +
      'Despite these limitations, we believe our approach presents a flexible framework for generating adaptable and composable expressive motion through the power of large language models. We hope that this inspires future efforts towards expressive behavior generation for robots to more effectively interact with people.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & _Insert_ & _Swap_ & _Loop_ & _Remove_ \\\\  & _actions_ & _actions_ & _actions_ & _capability_ \\\\ \\hline Excuse & 4 & 5 & 5 & 5 \\\\ Approach & 4 & 5 & 5 & 3 \\\\ Acknowledge Stop & 5 & 5 & 4 & 3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4. Success rates (out of 5 attempts) when providing different types of feedback to behaviors generated using GenEM, where: _Insert actions_ request a new action be added ahead of other actions, _Swap actions_ request to swap the order of existing actions, _Loop actions_ request to add loops to repeat actions, and _Remove capability_ requests to swap an existing action with an alternate one.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & _Eye_ & _Blinking_ & _Look_ & _Shake_ & _Nod_ \\\\  & _contact_ & _lights_ & _around_ & _head_ & _head_ \\\\ \\hline Acknowledge Walk & 5 & - & - & - & 5 \\\\ Approach & 4 & 5 & - & - & 0 \\\\ Confusion & - & 4 & 1 & 5 & - \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3. Number of times (out of 5 attempts) where previously-learned behaviors (columns) are used when composing new behaviors (rows) using GenEM. Dashes indicate that the given learned behavior API is not provided when prompting the creation of the new behavior.\n' +
      '\n' +
      '###### Acknowledgements.\n' +
      '\n' +
      ' We thank Doug Dooley for providing animations for the baseline robot behaviors, and Edward Lee for helpful discussions on the system. We thank Rishi Krishnan, Diego Reyes, Sphurti More, April Zitkovich, and Rosario Jauregui for their help with robot access and troubleshooting, and Justice Carbajal, Jodilyn Peralta, and Jonathan Vela for providing support with video recording. Lastly, we thank Ben Jyenis and the UX research team for coordinating the user studies and data collection efforts.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)\n' +
      '* A. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al. (2023)Do as I can, Not as Is Say: grounding language in robotic affordances. In Conference on Robot Learning, PMLR, pp. 287-318. Cited by: SS1.\n' +
      '* A. A. A. Tapas (2013)A model for synthesizing a combined verbal and nonverbal behavior based on personality traits in human-robot interaction. In 2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 325-332. Cited by: SS1.\n' +
      '* N. Bergstrom, T. Kanda, T. Miyashita, H. Ishiguro, and N. Hagita (2008)Modeling of natural human-robot encounters in: 2008 ieee/ij international conference on intelligent robots and systems. Cited by: SS1.\n' +
      '* N. Buchina, S. Kamel, and E. Barakova (2016)Design and evaluation of an end-user friendly tool for robot programming. In 2016 55th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), pp. 185-191. Cited by: SS1.\n' +
      '* M. J. Chung, J. Huang, L. Takayama, T. Lau, and M. Cakmak (2016)Iterative design of a system for programming socially interactive service robots. In Social Robots 8th International Conference, ICSR 2016, Kansas City, MO, USA, November 1-2, 2016 Proceedings A S. Springer, pp. 919-929. Cited by: SS1.\n' +
      '* Y. Cui, S. Karmchetti, B. Palletti, N. Shivakumar, P. Liang, and D. Sadigh (2023)No, to the Right: online language corrections for robotic manipulation via shared autonomy. In Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction, pp. 93-101. Cited by: SS1.\n' +
      '* P. David, M. Cakmak, A. Sauppe, A. Albarghouthi, and B. Mutlu (2022)Interaction Templates: a Data-Driven Approach for Authoring Robot programs. In PLATAT, External Links: Link Cited by: SS1.\n' +
      '* R. Desai, F. Anderson, J. Matejka, S. Coros, J. McCann, G. Fitzmurice, and T. Grossman (2019)Geppetto: enabling semantic design of expressive robot behaviors. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1-14. Cited by: SS1.\n' +
      '* M. Doering, D. F. Glas, and H. Ishiguro (2019)Modeling interaction for robot imitation learning of human social behavior. IEEE Transactions on Human-Machine Systems49 (3), pp. 219-231. Cited by: SS1.\n' +
      '* Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui (2022)A survey for in-context learning. arXiv preprint arXiv:2301.02034. Cited by: SS1.\n' +
      '* P. Ferrarelli, M. Lizaro, and L. locchi (2018)Design of robot teaching assistants through multi-modal human-robot interactions. In Robotics in Education: Latent Results and Developments, pp. 274-286. Cited by: SS1.\n' +
      '* G. Hoffman and W. Jyl (2014)Designing robots with movement in mind. Journal of Human-Robot Interaction3 (1), pp. 91-122. Cited by: SS1.\n' +
      '* C. Huang and B. Mutlu (2013)The repertoire of robot behavior: enabling robots to achieve interaction goals through social behavior. Journal of Human-Robot Interaction2 (2), pp. 80-102. Cited by: SS1.\n' +
      '* C. Huang and B. Mutlu (2014)Learning-based modeling of multimodal behaviors for humanlike robots. In Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction, pp. 57-64. Cited by: SS1.\n' +
      '* W. Huang, F. Xia, Y. Maris, Y. Zhang, J. Jiang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. (2023)Inner Monologae: embodied reasoning through planning with language models. In Conference on Robot Learning, pp. 1769-1782. Cited by: SS1.\n' +
      '* N. Hussain, E. Erzin, T. M. Sezgin, and Y. Yemez (2022)Training socially engaging robots: modeling backchannel behaviors with both reinforcement learning. IEEE Transactions on Affective Computing13 (4), pp. 1840-1853. Cited by: SS1.\n' +
      '* Y. Kato, T. Kanda, and H. Ishiguro (2015)May I help you? design of human-like police approaching behavior. In Proceedings of the Tenth Annual ACMIEEE International Conference on Human-Robot Interaction, pp. 35-42. Cited by: SS1.\n' +
      '* A. Kubota, E. I. Peterson, V. Rajendren, H. Kress-Gaif, and L. D. Riek (2020)Jessie: synthesizing social robot behaviors for personalized neurorehabilitation and beyond. In Proceedings of the 2020 ACM/IEEE international conference on human-robot interaction, pp. 121-130. Cited by: SS1.\n' +
      '* M. Kwon, S. H. Huang, and A. D. Dragan (2018)Expressing robot incapability. In Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, pp. 87-95. Cited by: SS1.\n' +
      '* M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh (2023)Reward design with language models. In International Conference on Learning Representations (ICLR), Cited by: SS1.\n' +
      '* N. Leonardi, M. Manca, F. Paternio, and C. Santoro (2019)Triggeration programming for personalising humanoid robot behavior. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1-13. Cited by: SS1.\n' +
      '* Z. Li, C. Cummings, and K. Sreenath (2020)Animated cassie: a dynamic relatable robotic character. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 379-374. Cited by: SS1.\n' +
      '* J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng (2023)Code as policies: language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 9493-9500. Cited by: SS1.\n' +
      '* K. Lin, C. Raja, T. Marone, P. Bruno, and J. Bohg (2023)Text2020. Cited by: SS1.\n' +
      '* P. Liu, D. G. Glas, T. Kanda, and H. Ishiguro (2016)Data-driven rith: learning social behaviors by example from human-human interaction. IEEE Transactions on Robotics32 (4), pp. 988-1008. Cited by: SS1.\n' +
      '* M. Marronegen, A. Lim, T. S. Dahl, and N. Hennion (2019)Generating robotic contionnelly language with variational autoencoders. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACIT), pp. 543-551. Cited by: SS1.\n' +
      '* S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer (2022)Beithining the role of demonstrations: what makes in-context learning work. arXiv preprint arXiv:2202.12837. Cited by: SS1.\n' +
      '* S. Mirachandani, F. Xia, P. Florence, B. Ichter, D. Driess, M. A. Arenas, K. Rao, D. Sadigh, and A. Zeng (2023)Large language models as general pattern machines. In Proceedings of the 7th Conference on Robot Learning (CoRL), Cited by: SS1.\n' +
      '* M. Murray, N. Walker, A. Nanavati, P. Alves-Oliveira, N. Filippov, M. Sampe, B. Mutlu, and M. Cakmak (2022)Learning backnancing behaviors for a social robot via data augmentation from human-human conversations. In Conference on robot learning, PMLR, pp. 513-525. Cited by: SS1.\n' +
      '* O. Ozsa (2023)GFT-tberai. Note: arXiv:2308.08774 [cs.CL] Cited by: SS1.\n' +
      '* N. Orzbayev, A. Aly, A. Sandrybaudin, and B. Mutlu (2023)Data-driven communicative behaviour generation: a survey. ACM Transactions on Human-Robot Interaction. Cited by: SS1.\n' +
      '* D. Porfirio, L. Fisher, A. Sauppe, A. Albarghouthi, and B. Mutlu (2019)Bodystorming human-robot interactions. In proceedings of the 32nd annual ACM symposium on user interface software and technology, pp. 479-491. Cited by: SS1.\n' +
      '* D. Porfirio, A. Sauppe, A. Albarghouthi, and B. Mutlu (2018)Authoring and verifying human-robot interactions. In Proceedings of the 31st annual symposium on user interface software and technology, pp. 75-86. Cited by: SS1.\n' +
      '* D. Porfirio, A. Sauppe, A. Albarghouthi, and B. Mutlu (2020)Transforming robot programs based on social context. In Proceedings of the 2020 CHI conference on human factors in computing systems, pp. 1-12. Cited by: SS1.\n' +
      '* D. Porfirio, L. Stegner, M. Cakmak, A. Sauppe, A. Albarghouthi, and B. Mutlu (2023)Sjecting robot programs on the rife. In Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction, pp. 584-593. Cited by: SS1.\n' +
      '* D. Porfirio, L. Stegner, M. Cakmak, A. Sauppe, A. Albarghouthi, and B. Mutlu (2021)Figator: a tabletop authoring environment for human-robot interaction. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-15. Cited by: SS1.\n' +
      '* I. Singh, V. Blukis, A. Moussavi, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomson, and A. Garg (2023)Program: generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (CRA), pp. 11523-11530. Cited by: SS1.\n' +
      '* A. Sipathy, A. Boho, Z. Kousli, K. Sreenath, D. S. Brown, and A. Dragan (2022)Teaching robots to span the space of functional expressive motion. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 13406-13413. Cited by: SS1.\n' +
      '* M. Suguitan, M. Bretan, and G. Hoffman (2019)Affective robot movement generation using cyclegrams. In 2019 19th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 534-535. Cited by: SS1.\n' +
      '* M. Suguitan, R. Gomez, and G. Hoffman (2020)MoveAE: modifying affective robot movements using classifying variational autoencoders. In Proceedings of the 2020 ACM/IEEE international conference on human-robot interaction,481-489.\n' +
      '* Takayama et al. (2011) Leila Takayama, Doug Dooley, and Wendy Ju. 2011. Expressing thought: improving robot readability with animation principles. In _Proceedings of the 6th international conference on Human-robot interaction_. 69-76.\n' +
      '* Wang et al. (2023) Guanzhiu Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Aminx Anandkumar. 2023. Voyager: An open-ended embodied agent with large language models. _arXiv preprint arXiv:2305.16291_ (2023).\n' +
      '* Wei et al. (2022) Jason Wei, Xuehui Wang, Dale Schurmanns, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Demy Zhou, Lei. 2022. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_ 35 (2022), 24824-24837.\n' +
      '* Woods et al. (2006) Sarah Woods, Michael Walters, Kheng Lee Koay, and Kerstin Dautenhahn. 2006. Comparing human robot interaction scenarios using live and video based methods: towards a novel methodological approach. In _9th IEEE International Workshop on Advanced Motion Control_, 2006. IEEE, 750-755.\n' +
      '* Wu et al. (2023) Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Syvomn Rusnikev, and Thomas Funkhouser. 2023. Tidy: Post personalized Robot Assistance with Large Language Models. In _2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_. 3546-3553. [https://doi.org/10.1109/IROS.355522.2023.1034577](https://doi.org/10.1109/IROS.355522.2023.1034577)\n' +
      '* Yu et al. (2023) Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasencleer, Jan Humplik, Brian Leit, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Litese, Dorsa Sadigh, Jie Tan, Yuval Tassa, and Fei Xia. 2023. Language to Rewards for Robotic Skill Synthesis. In _Proceedings of the 7th Conference on Robot Learning (CoRL)_.\n' +
      '* Zhou and Dragan (2018) Allan Zhou and Anna D Dragan. 2018. Cost functions for robot motion style. In _2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_. IEEE, 3632-3639.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>