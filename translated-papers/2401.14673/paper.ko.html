<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '경쟁력 있는\n' +
      '\n' +
      '대용량 모델입니다.\n' +
      '\n' +
      ' 식식 마하데반반.\n' +
      '\n' +
      '토론토의 대학의 대학입니다.\n' +
      '\n' +
      '토론토에 따르면, 지나탄 치넨 치케니켐@dgp. 토리쿠는 조나탄 치켄치크m@dgp.\n' +
      '\n' +
      'Google Deepmind\n' +
      '\n' +
      '노아 브라운 브라운치옌지@google.com 노아 브라운.\n' +
      '\n' +
      'H.com.com.\n' +
      '\n' +
      ' 후후고 주우주오 주조.\n' +
      '\n' +
      '캐롤라이나 파르다 파라다 구글 딥언 지후옥수@google.닷컴 캐롤라이나 파라다 구글 딥언 지후옥수@google.\n' +
      '\n' +
      '3.5.com Fei 샤샤샤 구글 딥틱 카롤린ap@google.com Fei 샤샤샤 구글 딥리티 카롤린ap@google.\n' +
      '\n' +
      '\'구글 딥리티 xiafei@google.\'.com 앤디 겐 겐 구글 딥리티 xiafei@google.\n' +
      '\n' +
      '루라 다카야마마야마야마야마 구글 딥틱과 분석@google.com. 레이라 다카야마마야마 구글 딥틱과 분석@google.com.\n' +
      '\n' +
      '코쿠랩스 타카야마@hokulabs.com Dorsa Sadighu Labs takayama@hokulabs.com Dorsa Sadighu Labs takayama@hokulabs.\n' +
      '\n' +
      'com.com.com.\n' +
      '\n' +
      '###### Abstract.\n' +
      '\n' +
      '사람들은 자신의 행동을 효과적으로 소통하고 조율하기 위해 표현적 행동을 사용하는데, 그 속에서 활짝 웃는 사람을 인정하거나 바쁜 복도에서 사람을 통과시키기 위해 "_excuse me_"라고 말하도록 고개를 끄덕인다. 로봇이 인간-로봇 상호작용에서도 표현 행동을 보여 주고 싶습니다. 선행 작업은 새로운 의사소통 방식이나 사회적 상황에 대해 스케일링하기 위해 어려움을 겪는 규칙 기반 방법을 제안하는 반면, 데이터 기반 방법은 로봇에 사용되는 각 사회적 상황에 대한 전문 데이터 세트를 필요로 한다. 우리는 큰 언어 모델(LLM)에서 이용 가능한 풍부한 사회적 컨텍스트와 명령어 또는 사용자 선호도에 기초하여 움직임을 생성하는 능력을 활용하여 서로 쌓아가는 _적응 가능한_ 및 _압축 가능한_인 _발현 로봇 모션_을 생성할 것을 제안한다. 우리의 접근법은 인간의 언어 지침을 로봇의 사용 가능하고 학습된 기술을 사용하여 모수화된 제어 코드로 번역하는 소수의 샷 체인 프롬프트 프롬프트를 사용한다. 사용자 연구와 시뮬레이션 실험을 통해 우리의 접근법이 사용자들이 유능하고 이해하기 쉬운 것으로 밝혀진 행동을 생성한다는 것을 보여준다. 보충 물질은 [클론성-발현-선택적-운동. 지투비오/](전기적 나노 생성-발현-선택적-운동. 지투비오/]에서 찾을 수 있다.\n' +
      '\n' +
      '언어 보정 2024 2024 2024 2024 2024 2024 2024 2024\n' +
      '\n' +
      '언어 수정 2024 생성 표현 로봇 행동 인맥스 러닝입니다.\n' +
      '\n' +
      '## 1. Introduction\n' +
      '\n' +
      '사람들은 매일 다른 사람들과 효과적으로 상호 작용하기 위해 광범위한 표현 행동을 사용한다. 예를 들어 지인이 걷는 사람은 그들을 간단히 보고 자신의 존재를 인정하지 않을 수 있다. 사람 한 명이 대화 중인 팽팽한 복도를 통해 짜내기 위해 "나를 방해하지 말라"고 사과할 수 있다. 많은 방식으로, 우리는 로봇도 사람과 상호 작용할 때 표현적 행동을 보여 주고 싶습니다. 표현 능력이 없는 로봇은 북적이는 복도를 피하기 위해 길을 다시 계획해야 할 것이다. 한편, 표현력이 있는 로봇은 실제로 사람들의 집단을 설득하여 그들이 짜낼 수 있는 여지를 만들 수 있게 함으로써, 그 작업을 수행할 수 있는 로봇의 효율성을 향상시킬 수 있을 것이다.\n' +
      '\n' +
      '선행 작업은 표현 로봇 행동의 가치를 입증하고, 다양한 작업을 위한 행동 생성을 위한 접근 방식을 탐색했다.\n' +
      '\n' +
      '그림 1. 자율적으로 표현 로봇 행동을 생성하는 새로운 접근 방식인 유전체 억제 모션(유전자EM)을 제시한다. 유전자EM은 원하는 표현 행동(또는 사회적 맥락)을 언어 지시, 인간 사회 규범에 대한 이유, 기존의 로봇 기술을 이용하여 로봇에 대한 제어 코드를 생성하고, 학습된 표현 행동을 한다. 경험적 피드백은 사용자 선호도에 따라 빠르게 행동을 수정할 수 있다. 여기서 \\({}^{*}\\) 기호는 냉동 대형 언어 모델을 나타낸다.\n' +
      '\n' +
      '일반 목적 사용(키리식 마하데반 등 2019년), 투명성이 중요한 조작 설정(키리식 마하데반 등, 2019년), 사회 규범을 준수해야 하는 일상 시나리오(수신자와의 상호작용 등)를 포함한 정화 및 맥락이다. 적용은 규칙 또는 템플릿 기반(키르시크 등 2019, 마하데반 등 2019, 마하데반 등은 2019)일 수 있으며, 이는 종종 경직된 템플릿 또는 행동을 생성하기 위한 일련의 규칙에 의존한다. 이것은 종종 표현적일 수 있지만 인간의 선호의 새로운 양식이나 변형으로 스케일링되지 않는 로봇 행동으로 이어진다. 반면, 데이터 중심 기술은 유연성의 약속과 변화에 적응할 수 있는 능력을 제공한다. 선행 작업은 표현 운동(키르시크크 등 2019)을 생성하는 데이터 기반 기술을 연구했지만, 이러한 방법들은 특정 행동이 사용되는 사회적 상호작용(예: 정의적 로봇 움직임들(키르시크 등, 2019; 키르시크 등)에 대한 전문 데이터 세트가 종종 필요하기 때문에 단점을 가지고 있다.\n' +
      '\n' +
      '우리의 목표는 로봇이 다양한 인간 선호도에 대해 _adapt_할 수 있는 행동, 그리고 더 단순한 행동의 _comaction_가 될 수 있는 표현 행동을 생성할 수 있도록 하는 것이다. Kirshik et al., 2019; 마하데반 et al., 2019) 및 체화된 에이전트(Kirshik et al., 2019; 마하데반 et al., 2019; 마하데반 et al., 2019)는 가상(키르식 et al., 마하데반 et al., 2019)를 제어하기 위해 코드를 합성할 수 있으며, 사회·공통의 추론(Kirshik et al., 마하데반 et al., 마하데반 et al., 2019)을 제어하거나 사회·공통의 추론(Kirshik et al., 마하데반 et al., 마하데반 et al., 마하데반 et al., 마하데반 et al., 마하데반 et al., 마하데반 et al., 마하데반 et al., 마하데반 et al., 마하데반 et al., 마하데반 et al., 마하데반 et al., 마하데반 et al., 마하데반 et al., 2019), 사회·공통의 추론(Kirshik et al., 마하데반 et al., 마하데반 et al., 2019)을 가능하게 하거나, 사회·공통의 추론(Kir 우리의 핵심 통찰력은 적응 가능하고 복합 가능한 표현 행동을 생성하기 위해 LLM에서 사용할 수 있는 풍부한 사회적 맥락으로 바꾸는 것이다. 예를 들어, LLM은 누군가를 맞이할 때 눈 접촉을 하는 것이 예의라는 것을 깨닫기에 충분한 맥락을 가지고 있다. 또한 LLM은 "팔을 조금 더 보내라!"와 같은 교정 언어의 사용과 그러한 지시에 대응하여 동작을 생성하는 능력을 가능하게 한다. 이것은 LLM이 인간-로봇 상호작용 환경에서 인간 피드백에 유연하게 반응하고 학습하는 표현 행동을 자율적으로 생성하는 데 유용한 프레임워크를 만든다.\n' +
      '\n' +
      'LLM이 제공하는 힘과 유연성을 추적하여 자율적으로 표현 로봇 행동을 생성하기 위해 새로운 접근 방식인 유전체 억제 모션(유전자EM)을 제안한다. 유전자EM은 소수의 샷 프롬프트를 사용하고 원하는 표현 행동(또는 사회적 맥락)을 언어 지시로 취하고, 사회적 추론(키르시크 et al., 2019)을 수행하고, 최종적으로 이용 가능한 로봇 API를 사용하여 로봇에 대한 제어 코드를 생성한다. 유전자EM은 로봇의 사용 가능한 어포던스(예: 음성, 신체 움직임 및 빛 스트립과 같은 기타 시각적 특징)를 사용하여 로봇의 의도를 효과적으로 표현하는 다중 모드 행동을 생성할 수 있다. 유전자EM의 주요 이점 중 하나는 살아있는 인간 피드백에 반응하고 반복적 수정에 적응하고 기존의 것을 구성함으로써 새로운 표현 행동을 생성한다는 것이다.\n' +
      '\n' +
      '온라인 사용자 연구 세트에서는 사용자 피드백(HRI 행동 설계에서 비전문가)이 있거나 없는 2개의 유전체EM을 사용하여 이동 로봇에서 생성된 행동을 전문 캐릭터 애니메이션기(또는 _oracle 애니메이션ator_)가 설계한 일련의 행동과 비교했다. 우리는 GenEM에 의해 생성되고 사용자 피드백으로 추가로 적응된 행동이 사용자에 의해 긍정적으로 인식되었으며 일부 경우에는 오라클 행동보다 더 잘 인식되었음을 보여준다.\n' +
      '\n' +
      '모바일 로봇과 시뮬레이션된 사분면에 대한 추가 실험에서 유전체EM:(1)이 언어 명령어가 코드로 직접 번역되는 버전보다 더 잘 수행된다는 것을 보여주었고, (2)는 구체화된 행동 생성을 허용하며, (3)은 보다 단순한 표현 행동에 기반하는 합성 가능한 행동 생성을 허용하고, 마지막으로 (4)는 다양한 유형의 사용자 피드백에 적응한다.\n' +
      '\n' +
      '## 2. 업무 관련 작업 작업## 2.\n' +
      '\n' +
      '**-압축 행동 세대는 로봇과 가상 인간 모두에 대해 사회적으로 허용 가능한 행동을 생성하는 데 상당한 노력을 기울였다.** 연구원이 있다. 이들은 크게 _rule 기반_, _템플레이트 기반_ 및 _data 구동_(키르시크 등 2019) 행동 생성 접근법으로 분류할 수 있다. 규칙 기반 접근법을 후속 로봇 행동을 생성하는 데 사용되는 공식화된 규칙 및 작업 세트(사람에 의해 수정적으로 제공됨)가 필요한 접근법으로 정의한다.\n' +
      '\n' +
      '룰 기반 접근법은 공식화된 규칙 및 작업 세트(키르시크 등, 2019)를 통해 행동 생성을 가능하게 한다. 일부 방법에는 사용자가 수동으로 상호작용 규칙과 논리를 지정할 수 있는 인터페이스(키르시크 등, 2019), 마하데반 등은 2019, 마하데반 등은 2019, 마하데반 등은 2019)가 포함된다. 다른 방법은 인간을 관찰하고 모델링함으로써 작동한다(키르시크 등 2019, 마하데반 등은 2019, 마하데반 등은 2019, 마하데반 등은 2019). 이들의 사용에도 불구하고 규칙 기반 접근법은 형식적 규칙의 요건으로 인해 생성된 행동의 제한된 표현성과 모달리티의 수가 증가함에 따라 다중 모드 행동을 생성하는 능력 감소(키르시크 등 2019)를 포함하여 여러 문제에 직면한다. 템플릿 기반 방법은 상호 작용 데이터의 흔적(키르시크 등, 2019, 마하데반 등, 2019)에서 학습함으로써 상호 작용을 위한 일반 템플릿을 공식화한다. 예시적인 내용은 프로그램 합성을 통해 인간 흔적의 몇 가지 예를 재사용 가능한 프로그램으로 번역할 수 있다(키르시크 등, 2019, 마하데반 등). 트라우스는 인간 상호 작용(키르시크 등 2019, 마하데반 등)을 관찰하거나 테이블탑(키르시크 등 2019)에서 스케치(키르시크 등 2019)나 탱글과 같은 접근 방식을 통해 수집할 수 있다. 전반적으로, 사전 규칙 및 템플릿 기반 방법은 행동 생성을 가능하게 하기 위해 강력한 제약을 시행하지만 표현성이 제한된다. 대조적으로, GenEM은 라이브 사용자 피드백을 통해 반복적 개선뿐만 아니라 초기 행동 생성에서의 표현력 증가를 가능하게 한다.\n' +
      '\n' +
      '반면, 데이터 중심 접근법은 데이터에 대해 훈련된 모델을 사용하여 행동을 생성한다. 일부 방법은 데이터를 통해 _상호작용 논리_를 학습하고 이를 사용하여 고전적 기계 학습 방법(키르시크 등, 2019, 마하데반 등, 2019, 마하데반 등)을 통해 다중 모드 행동을 생성한다. 다른 방법은 생성 모델(키르시크 등 2019, 마하데반 등 2019)을 통해 핸드메이드 사례를 훈련시킨다. 예를 들어, 배치 강화 학습(키리식 등 대화 중 피드백 제공)과 재발성 신경망(키리식 등 2019)을 통해 백 채널화 행동(즉, 노드화 등)을 언제 사용할지 예측한다. 마지막으로 최근 작업은 사용자 피드백(키리식 등 2019)으로부터 목표 감정에 대한 비용 기능 학습 방법(Kirshik et al., 2019)을 조사하거나, 심지어 많은 감정(키리식 et al., 2019)을 모델링하기 위한 이모형 잠재 공간을 배우기도 했다. 그러나 이러한 접근법은 데이터가 비효율적이며 행동당 특수 데이터 세트가 생성되어야 하는 반면, GenEM은 텍스트 내 학습을 통해 몇 가지 예시로 다양한 표현 행동을 생성할 수 있다.\n' +
      '\n' +
      '로봇 기획 및 통제를 위한**LLM***Lent 작업은 상황(키르시크 등 2019, 마하데반 등 2019, 마하데반 등)에서 바람직한 입출력 쌍의 서열을 제공함으로써 특히 하류 로봇 공학 작업에서 LLM을 활용함으로써 큰 성공을 거두었다. 또한 LLM은 장기수평 과제 계획(키르시크 등 2019, 마하데반 등 2019)에 사용되어 환경 및 인간 피드백(키르시크 등 2019)에 반응할 수 있다. LLM은 강화 학습 에이전트(Kirshik et al., 2019; 마하데반 et al., 2019)에 대한 보상 기능을 설계하기 위해 활용되었다. 연구에 따르면 LLM은 사회적, 커먼센스 추론(키리식 등 2019)을 가능하게 하고, 인간과의 상호 작용(키리식 등, 2019)을 요약하여 사용자 선호도를 유추할 수 있다. 우리의 접근법과 관련된 대부분은 LLM이 기존의 API를 사용하여 더 복잡한 로봇 행동을 프로그램으로 구성함으로써 가상(키리식 등 알, 2019) 및 로봇 에이전트(Levy and Goyal, 2017, Goyal, 2017)를 제어하기 위해 코드를 합성하는 이전 작업이다. 우리는 또한 언어가 온라인으로 로봇 조작 행위를 교정하는 데 사용될 수 있음을 보여주는 작업(Beng et al., 2019)에 의해 권장된다. 종합하면, 우리는 LLM에서 사용할 수 있는 풍부한 사회적 맥락과 표현 로봇 행동을 생성하기 위해 사용자 지침에 적응할 수 있는 능력을 레버리지할 것을 제안한다. 알고 있는 범위 내에서는, LLM은 이전에 사용자 피드백에 적응하는 표현 로봇 행동을 생성하는 데 사용되지 않았다.\n' +
      '\n' +
      '진보적인 억제 모션 3. 유전학적 억제 모션## 3.\n' +
      '\n' +
      '**문제 진술*** 우리는 보다 복잡한 행동이 더 간단한 행동에 구축할 수 있도록 사용자 피드백에 적응적이고 합성 가능한 표현 행동 생성 문제를 해결하는 것을 목표로 한다. 형식적으로, 우리는 _발현_을 애니메이트(또는 입증됨) \\(\\tau_{\\text{expert}}\\)와 로봇 궤적 \\(\\tau\\)에 의해 생성될 수 있는 일부 전문가 표현 궤적 사이의 거리로 정의한다. (텍스트{dist},\\tau,\\tau_{\\text{expert}})\\)는 동적 시간 와핑(DTW)과 같은 두 궤적 사이의 바람직한 거리 메트릭일 수 있다. 유전자EM은 이 거리 \\(d^{*}=\\min\\text{dist} (\\tau,\\tau_{\\text{expert}})\\를 최소화하는 것을 목표로 한다.\n' +
      '\n' +
      '우리의 접근 방식(그림 2)은 모듈식 방식으로 여러 LLM을 사용하여 각 _LLM 에이전트_가 뚜렷한 역할을 한다. 나중에, 우리는 모듈식 접근법이 엔드 투 엔드 접근법에 비해 더 나은 행동 품질을 산출한다는 실험을 통해 입증한다. 유전자EM은 사용자 언어 명령어 \\(l_{in}\\in L\\)를 입력으로 하여 파라미터화된 코드 형태인 로봇 정책 \\(\\pi_{\\theta}\\)을 출력한다. 인간 반복 피드백 \\(f_{i}\\in L\\)을 사용하여 정책 \\(\\pi_{\\theta}\\)를 업데이트할 수 있다. 정책 파라미터는 \\(i\\in\\{1,\\dots,K\\}\\)가 있는 피드백(f_{i}\\)을 감안할 때 한 단계 업데이트된다. 이 정책은 일부 초기 상태(s_{0}\\in S\\)에서 인스턴스화하여 궤적(\\tau=\\{s_{0},a_{0},a_{0},\\dots,a_{N-1},s_{N}\\}\\)을 생성하거나 표현 로봇 행동의 순간화를 생성할 수 있다. 아래에서는 인간 피드백 \\(f_{i}\\)로 하나의 샘플 반복을 설명한다. 전체 프롬프트에 대해 ** 부록 A**를 참조하십시오.\n' +
      '\n' +
      '** 확장 지침** 우리의 접근 방식에 대한 입력은 언어 지시 \\(l_{in}\\in L\\)이며, 이는 로봇이 사회적 규범(예: "파동으로 걷는 사람")에 따라 표현적 행동을 수행해야 하는 사회적 맥락(예: "파동으로 걷는 사람") _or_가 생성되어야 하는 표현적 행동을 설명하는 지시일 수 있다. 입력 프롬프트는 \\(u=[h_{pre},l_{in}]\\)의 형태이며, 여기서 \\(h_{pre}\\)는 LLM의 역할에 대한 맥락을 추가하고 몇 가지 샷 예를 포함하는 프롬프트 프리픽스이다. LLM 콜의 출력은 지시에 따라 사상 추론 \\(h_{cot},h_{cot},h_{exp}]\\)와 인간 표현 운동 \\(h_{exp}\\, 2017)로 구성된 형태 \\(h=[h_{cot},h_{exp}]\\)의 문자열이다. 예를 들어, \\(l_{in}=\\)_"걷는 사람의 경우, 말을 할 수 없다._, _항체 명령어 Following_ 모듈은 \\(h_{exp}=\\)_al 눈 접촉을 출력한다. __미연하거나 존재를 인정하지 않는 것이 아니다.__미연하거나 존재를 인정하지 않는다. (h_{cot}\\)의 예로는 __)가 지나가고 있고 자신의 존재를 인정하는 것이 예의라 할 수 있다.\n' +
      '\n' +
      '인간 억제 모션에서 로봇 억제 모션*** 다음 단계에서 LLM을 사용하여 인간 표현 모션 \\(h\\)를 로봇 표현 모션 \\(r\\)로 번역한다. 프로브는 형태 \\(u=[r_{pre}, a_{i},h,r_{i-1_{i-1_{opt}},f_{i-1_{opt}}]\\)를 취하는데, 여기서 \\(r_{pre}})는 LLM에 대한 프롬픽스 설정 맥락이며, 몇 가지 샷 예시를 포함하고 있으며, 그 중 일부는 사전 정의(예: 헤드를 말하거나 움직일 수 있는 능력)인 로봇의 능력(예를 들어, 노드딩 또는 사람에게 접근하는 능력)을 설명한다. 선택적으로, 프롬프트는 이전 단계 \\(r_{i-1}\\)로부터의 응답 및 이전 단계 \\(f_{i-1}\\)로부터의 사용자 반복 피드백에 대한 응답을 포함할 수 있다. 출력은 LLM의 추론과 표현 로봇 운동을 만드는 절차로 구성된 형태 \\(r=[r_{cot},r_{exp}]\\)이다. 예시적 반응 \\(r_{exp}\\)은 _"I)가 머리의 팬과 기울어진 능력을 사용하여 2로 걷는 사람을 마주할 수 있으며, 광 스트립을 사용하여 미소와 결절을 모방한 사전 프로그램 패턴을 표시할 수 있다._. (r_{cot}\\)의 예는 _"로봇이 머리 팬과 틸트 기능을 사용하여 사람과의 \'눈 접촉\'을 만들 수 있으며 로봇은 가벼운 스트립을 사용하여 웃거나 결절을 모방할 수 있다._\n' +
      '\n' +
      '코드에 대한** 트랜스포팅 로봇 억제 모션*** 다음 단계에서 LLM을 사용하여 표현 로봇 운동을 어떻게 생산해야 하는지에 대한 단계별 절차를 실행 가능한 코드로 번역한다. 기존 로봇 기술 프리미티브를 포함하는 보이저(보이저, 2017)와 유사한 방식으로 기술 라이브러리를 제안하며, 이전에 학습된 표현 동작을 나타내는 모수화된 로봇 코드 \\(\\pi_{\\theta}\\)를 제안한다. 이를 용이하게 하기 위해 프롬프트는 도클링과 함께 작고 재사용 가능한 기능을 사용하고 주장이라고 명명되는 예를 제공하여 표현 행동을 설명하는 보다 복잡한 기능을 생성함으로써 모듈식 코드 생성을 장려한다. 코드를 생성하기 위해, 프롬프트는 코드를 생성하기 위해 코드를 생성하라는 요청을 받았습니다.\n' +
      '\n' +
      '그림 2. 유전 억제 모션. 언어 지시 \\(l_{in}\\)를 감안할 때, 사회 규범에 대한 _항체 명령어 Following_ 모듈 이유를 감안할 때, 인간이 이러한 행동을 어떻게 표현할 수 있는지(\\(h\\) 출력한다. 이는 로봇의 기존 능력(r_{pre}\\)) 및 학습된 임의의 표현 행동을 설명하는 프롬프트를 사용하여 로봇 표현 행동을 위한 절차로 번역된다. 그런 다음, 절차를 사용하여 실행될 수 있는 매개변수화된 로봇 코드 \\(c\\)를 생성한다. 사용자는 먼저 로봇 행동 모듈을 재운영할지, 코드 생성 모듈이 뒤따르는지를 결정하기 위해 처리된 거동에 대한 반복 피드백 \\(f_{i}\\)을 제공할 수 있다. 모든 회색 모듈 위에 표시된 \\({}^{*}\\)는 냉동 LLM으로 표시한다.\n' +
      '\n' +
      'LLM은 형태 \\(u=[c_{pre},l_{pre},l_{in},h_{exp},r_{exp},r_{exp,i-1_{opt}},c_{i-1_{opt}},\\hat{f_{i-1}},\\)\\(r_{exp}]\\)를 취한다. 여기서, \\(c_{pre}\\)는 LLM에 코드 생성 에이전트로서의 역할에 대한 맥락을 제공하며, 로봇의 현재 기술 라이브러리를 포함하고, 몇 가지 샷 예시를 포함한다. 선택적으로, 표현 로봇 모션 \\(r_{exp,i-1}\\) 및 이전 단계로부터의 코드 \\(\\hat{c_{i-1}\\)뿐만 아니라 사용자 피드백 \\(\\hat{f_{i-1}\\)에 반응하는 LLM 출력 \\(\\hat{f_{i-1}}\\)도 제공될 수 있다. 출력 \\(c\\)는 표현 행동에 대한 정책 \\(\\pi_{\\theta}\\)을 나타내는 매개변수화된 로봇 코드(표본 출력에 대한 그림 2 참조)이다. 이후 생성된 코드를 로봇의 기술 라이브러리에 통합하여 미래 표현 행동 세대에 활용할 수 있다.\n' +
      '\n' +
      '***Propagating 인간 피드백*** 최종(optional) 단계에서 LLM을 사용하여 사용자가 생성된 행동에 만족하지 않는 경우 인간 피드백 \\(\\hat{f_{i}}\\)에 응답하여 생성된 표현 행동을 업데이트한다. 프로브는 형태 \\(u=[\\hat{f_{pre}},l_{in},r_{exp},c,f_{i}]\\)이며, 여기서 \\(\\hat{f_{pre}}}\\)는 LLM에 맥락을 제공하고, 표현 로봇 모션 \\(r_{exp}\\) 및 생성된 코드 \\(c\\)를 모두 포함한다. 출력은 형태 \\(f=[\\hat{f_{cont}},\\hat{f_{i}}]\\)이며, LLM의 추론 및 인간 피드백에 기초한 현재 표현 운동을 개선하기 위해 필요한 변화(\\hat{f_{i}}\\)를 포함한다. 출력은 또한 변화가 로봇의 표현 행동(r\\)을 생성하기 위한 절차를 수정한 다음 이를 코딩(c\\), \\(또는\\)로 번역하기 위해 반복적 전화가 필요한지 여부를 분류한다(c\\).\n' +
      '\n' +
      '예를 들어, 사용자는 먼저 사람을 볼 때 \\(\\hat{f_{i}}=\\)_에 고개를 끄덕일 수 있고, 출력 \\(\\hat{f_{i}}\\)는 _"[변화: 어떤 로봇이 해야 하는지]가 될 수 있으며, 로봇이 사람을 보는 순간, 광 스트립을 사용하여 미소를 모방하거나 결절을 모방하는 사전 프로그램 패턴을 표시할 수 있다. 예를 들어 \\(f_{cont}\\)는 상태일 수 있다: _"의 피드백은 로봇이 사람을 인정하는 행위가 옳지 않다는 것을 시사하며, 이는 로봇이 먼저 볼 때 사람에게서는 안 된다는 것을 의미한다.__을 의미한다.\n' +
      '\n' +
      '## 4. 기계연구.\n' +
      '\n' +
      '우리의 접근 방식인 GenEM이 사람들이 인식할 수 있는 표현 행동을 생성하는 데 사용될 수 있는지 여부를 평가하기 위해 두 가지 사용자 연구를 수행했다. 우리는 GenEM과 반복적 Feedback(또는 _GenEM++_)을 갖는 GenEM의 두 가지 버전의 행동을 생성했다. 두 연구 모두에서 모든 비교는 전문 애니메이션자가 설계하고 소프트웨어 개발자에 의해 구현되었으며, 이는 우리가 _oracle 애니메이션ator_라고 한다. i_첫 번째 연구_에서 우리의 목표는 지니EM 및 GenEM++를 사용하여 생성된 행동이 오라클 애니메이션기를 사용하여 생성된 행동과 유사하게 인식되는지 여부를 평가하는 것이었다. i_2차 연구_에서 oracle 애니메이션을 사용하여 생성된 행동과 유사한 GenEM 및 GenEM++를 사용하여 행동을 생성하려고 시도했다. 두 연구 모두 우리의 접근법이 인간 피드백에 _적응 가능한_임을 입증하는 것을 목표로 한다.\n' +
      '\n' +
      '행동자인**** 모든 행동은 모바일 로봇 플랫폼(전체 클립용 웹사이트 1 참조)에서 생성되었다. 로봇은 팬과 틸트가 가능한 헤드와 포인트에서 회전하여 탐색할 수 있는 베이스, 다양한 색상과 패턴을 표시할 수 있는 라이트 스트립, 마지막으로 발화 및 비언어 효과를 생성할 수 있는 스피치 모듈을 포함하여 기존 API를 통해 행동을 생성하는 데 사용할 수 있는 여러 기능을 가지고 있다. 오라클 애니메이트, GenEM 및 GenEM++의 세 가지 조건에서 생성된 행동의 비교를 가능하게 하기 위해 각 행동의 비디오 클립(그림 3 참조)을 기록했다. 조건에 걸친 일관성을 보장하기 위해 유사한 조명 조건에서 각 조건의 행동을 동일한 물리적 위치에 기록했다. 유전자EM 및 GenEM++ 행동은 0으로 설정된 온도로 텍스트 완료(Shen et al., 2019)(gpt-4-0613)를 위해 OpenAI의 GPT-4 API를 샘플링하여 생성되었다.\n' +
      '\n' +
      '** 조사 절차*** 사전 동의를 제공한 후 참가자는 두 연구에서 로봇의 표현 행동을 평가하기 위해 온라인 조사를 완료했다. 설문조사는 3개의 구간(행동 조건당 1개)으로 나뉘며 각 조건 내의 클립이 무작위로 나타났다. 주문 효과를 최소화하기 위해 균형 있는 라틴 스퀘어 디자인(3 x 3)을 사용하였다. 각 조건의 각 행동에 대해 참가자들은 표지되지 않은 비디오 클립 1을 본 후 질문에 응답했다. 모든 참가자는 연구 후 보수를 받았다.\n' +
      '\n' +
      '*** 조치** 두 연구 모두 참여자가 각 행동을 평가하기 위한 조사를 완료하고, 행동 이해에 대한 자신감을 평가하는 3개의 7점 리커트 척도 질문, 로봇이 무엇을 하고 있는지 이해하는 어려움, 로봇의 행동 역량에 응답했다. 참가자들은 또한 자신이 어떤 행동을 표현하고 있는지 기술하는 개방형 응답을 제공했다.\n' +
      '\n' +
      '*** 분석*** 원방향 반복 측정 분산 분석에서는 본페로니 수정과 유의한 차이가 있는 사후 쌍별 비교를 사용하여 데이터에 대해 수행되었다. 조건 간의 비교를 보고할 때, 우리는 _instances_를 행동에 대해 묻는 세 가지 Likert 규모 질문 중 적어도 하나에 대해 쌍별 중요한 조건으로 정의한다.\n' +
      '\n' +
      '폐경 1: [국경 분리-표현-선택적-운동-기투비오/](부근적-발현-선택적-움직임-기투비오/)\n' +
      '\n' +
      '실험 1.\n' +
      '\n' +
      '우리의 접근법이 사람들을 인식할 수 있는 표현 행동을 생성하는지 여부를 결정하기 위해 18~60세(18-25:3, 26-30:9, 31-40:9, 41-50:7, 51-60:2) 30명의 참가자를 대상으로 피험자 내 사용자 연구를 수행했다. 한 참가자는 전체 조사를 완료하지 않았고 데이터는 생략했다.\n' +
      '\n' +
      '** 행동자는 _Nod_, 쉐이크 헤드(_Shake_), 웨이크 업(_Wake_), 변명 나(_Excuse_), 회복 불가능한 실수(_ 제공되지 않는 실수), (_Acknowledge_), 후속 사람(_aturlow_), 접근 사람(_Approach_) 및 사람(_Approach_)에 대한 주의를 기울이는 것(<그림 3> 참조)의 복잡도에서 10개의 표현 행동(<그림 3>을 생성했다. 입력에는 1선 지시(예를 들어, _응답자가_, _\'에 와라._말할 수 없다)가 포함되었다.\n' +
      '\n' +
      '조건**** 오라클 애니메이션 조건은 스크립팅을 통해 로봇에 구현된 전문적으로 애니메이션화된 행동으로 구성되었다. 유전자EM 행동을 만들기 위해 우리는 각 행동의 5가지 버전을 생성하기 위해 접근법을 5회 샘플링했다. 행동은 0의 온도로 샘플링되었기 때문에 (GPT-4 출력에서 비결정성으로 인해) 이들 사이의 작은 변형과 상당한 중첩을 공유했으며, 동일한 프롬프트를 사용하여 생성된 샘플에 대해 ** 부록 C**를 참조하십시오. 그런 다음 로봇과 함께 일하는 경험이 있는 6명의 참가자에게 순위를 매길 것을 요청했다. 각 행동에 대한 최상의 변화는 GenEM 행동의 일부로 포함되었다. 유전자EM++ 행동을 생성하기 위해 로봇(그러나 HRI 행동 설계에서 경험이 없는)을 사용하여 경험한 한 참가자를 모집하고 각 행동의 최상의 평가 버전에 대한 피드백을 제공하도록 요청했다. 피드백은 참여자가 결과에 만족하거나 최대 피드백 라운드 수(n = 10)에 도달할 때까지 표현 행동을 반복적으로 수정하는 데 사용되었다. 참가자들이 연구에서 행동을 평가했지만, 행동 생성은 초기 피드백을 제공한 사용자에게 개인화되어 모든 잠재적 사용자(예: 연구 참가자)의 선호도를 반영하지 못할 수 있다는 점에 주목한다.\n' +
      '\n' +
      '** 부인** 우리는 GenEM++ 행동에 대한 인식이 오라클 애니메이트 행동(**H1***)과 크게 다르지 않을 것이라고 가정했다. 우리는 또한 GenEM 행동이 GenEM++ 및 oracle 애니메이션자 행동(**H2***)에 비해 덜 호평을 받을 것이라고 가정했다.\n' +
      '\n' +
      '** 정량적 추천*** 그림 4는 각 행동에 대한 설문 질문에 대한 참가자의 반응을 요약한 것이다. 결과는 2/10예(_Shake_ 및 _435low_)에서 지니EM++ 행동이 오라클 애니메이트 행동보다 더 나쁘다는 것을 보여준다. 대조적으로, GenEM++ 행동은 2/10개의 경우(_Excuse_ 및 _Approach_)에서 오라클 애니메이션 행동보다 더 높은 점수를 받았다. 따라서 **H1은 데이터에 의해 지원되는*** - GenEM++ 행동이 잘 수신되었고 oracle 애니메이션자 행동은 GenEM++ 행동보다 유의하게 잘 수신되지 않았다.\n' +
      '\n' +
      '유전자EM 행동은 2/10건(_Acknowledge Walk_ 및 _435low_)에서 오라클 애니메이션 행동보다 더 나쁜 반면, 유전체EM 행동은 2/10건(_Excuse_ 및 _Approach_)에서 오라클 애니메이션 행동보다 더 잘 받았다. 이는 사용자 피드백이 이 조건에서 행동 생성에 통합되지 않았기 때문에 놀라운 일이었다. 1/10건(_Shake_) 외에도 GenEM 및 GenEM++ 행동에 대한 인식에는 유의한 차이가 없었다. 따라서 **H2**에 대한 지원을 찾지 못했다. 우리는 동등성 테스트(평등성 결합: +/- 0.5 Likert 포인트)를 수행했지만 동등한 행동 세트를 찾지 못했다. 전반적으로 결과는 제네EM(피드백을 제공하는 훈련되지 않은 사용자 배제)이 사용자들이 유능하고 이해하기 쉬운 것으로 밝혀진 표현 로봇 행동을 생성한다는 발견을 뒷받침한다.\n' +
      '\n' +
      '### 연구 2: Oracle Animator를 모방한다.\n' +
      '\n' +
      '18-60세(18-25:4, 26-30:3, 31-40:12, 41-50:4, 51-60:1)에 20명의 참가자를 대상으로 추가 피험자 내 사용자 연구를 수행하여 지니EM을 사용하여 오라클 애니메이션기와 유사한 행동을 생성할지 여부를 다르게 평가했습니다. 한 참가자는 전체 조사를 완료하지 않았고 데이터는 생략했다.\n' +
      '\n' +
      '1차 연구(<그림 3>)에서 8개의 중복된 2개의 행동, 노드(_Nod_), 쉐이크 헤드(_Shake_), 웨이크 업(_Wake_), 핑메(_Excuse_), 회복 불가능한 실수(_ 제공되지 않을 수 없는 실수), (_Acknowledge Wop_), 후속 인물(_loadlow_) 및 가르침(_Teach_)로 복잡도 범위의 10개의 표현 행동을 생성했다. 첫 번째 연구와 다른 행동은 추가 복잡성을 추가하기 위해 선택되었으며, 예를 들어 로봇을 걷는 사람과 시작하여 교훈으로 가르치고 로봇이 사람의 지시를 이해했음을 인정하는 _teaching_와 같은 더 긴 단일 시간 상호작용을 추가했다. 첫 번째 연구와 달리 프롬프트는 더 다양했으며 때로는 더 복잡한 행동(각 행동에 대한 완전한 프롬프트에 대해 **Appendix B** 참조)과 같은 추가 설명을 포함했다. 각 GenEM 거동을 생성하기 위해 10회 접근법을 샘플링한 후 실험자가 로봇에 배치될 때 등가 오라클 애니메이션 거동과 가장 유사한 것으로 나타난 버전을 선택했다. 각 GenEM++ 거동을 생성하기 위해 실험자는 등가 오라클 애니메이트 행동과 유사하거나 최대 피드백 라운드 수(n = 10) 1을 초과하는 후 반복 피드백을 통해 GenEM 행동을 정제하였다.\n' +
      '\n' +
      '두 번째 연구의 일부 행동은 첫 번째 연구에서 일관성을 유지하기 위해 유지되는 단일 라인 지시로 표현하기에는 너무 복잡하기 때문에 첫 번째 연구와 다르다. 대신, 첫 번째 연구에서는 이러한 복잡한 행동을 더 간단한 행동(예를 들어, 가르침은 접근하고 주목하는 것과 동등함)으로 분해하였다.\n' +
      '\n' +
      '** 부인** 우리는 지니EM++ 행동에 대한 사용자 인식이 오라클 애니메이트 행동(**H3***)과 비교할 때 크게 다르지 않을 것이라고 가정했다. 우리는 또한 GenEM 조건의 행동이 GenEM++ 및 오라클 애니메이트 행동(**H4***)보다 더 나쁜 것으로 인식될 것이라고 가정한다.\n' +
      '\n' +
      '** 정량적 피터** 연구의 결과는 그림 5에 요약되었으며, 젠EM++ 행동이 2/10예(_Acknowledge Walk_ 및 _910low_)에서 오라클 애니메이션 행동보다 더 나쁜 것으로 나타난 반면, 유전체EM++ 행동은 2/10예(_Excuse_ 및 _Teach_)에서 오라클 애니메이션자보다 더 긍정적으로 수신되었음을 보여준다. 따라서 우리의 가설은 데이터에 의해 뒷받침된다.\n' +
      '\n' +
      '그림 3은 녹색으로 표지된 행동이 첫 번째 연구에 고유한 행동 및 파란색으로 표지된 행동을 나타내는 두 가지 사용자 연구에서 테스트한 행동자는 두 번째 연구에 고유한 행동을 나타낸다. 나머지 행동(8)은 두 연구 중 공통적이었다.\n' +
      '\n' +
      '***(H3)** - GenEM++ 행동이 잘 전달되었고 오라클 애니메이션 행동은 유의하게 더 잘 인식되지 않았다. 오라클 애니메이션 행동 및 젠EM 행동을 비교할 때, 제네EM 행동이 더 나쁜 경우(_Wake_, _Acknowledge Walk_, _Acknowledge Stop_, _435low_)가 4/10건, GenEM 행동이 더 긍정적으로 평가되는 경우(_Excuse__Excuse_) 1/10건이었다. 첫 번째 연구와 마찬가지로 유전체EM 행동이 한 사례에서 기저장보다 더 잘 수신되었다는 것은 다소 놀라운 일이며, 그것과 유사하지만 사용자 피드백이 제공되지 않기 때문에 오라클 애니메이션자 행동에 존재하는 모든 뉘앙스를 포착하지 않는다. 마지막으로, 유전체EM 행동은 2/10예(_Wake_ 및 _Teach_)에서 GenEM++ 행동보다 더 나쁜 평가를 받은 반면, 역이 진정한 경우는 0/10예였다. 따라서 우리는 마지막 가설 **(H4.*)에 대한 지지를 찾지 못했다(동일성 결합: +/- 0.5 Likert 포인트). 동등한 행동 세트를 찾지 못했다. 전반적으로, 연구 결과는 우리의 접근(사용자 피드백이 있는)을 사용하여 생성된 표현 로봇 행동이 사용자에게 유능하고 이해하기 쉽다는 것을 시사한다.\n' +
      '\n' +
      '## 5. Experiments\n' +
      '\n' +
      '우리는 GenEM의 다양한 측면을 신중하게 연구하기 위해 일련의 실험을 수행했다. 여기에는 프롬프트 구조의 영향과 엔드 투 엔드 접근법에 대한 다른 LLM에 대한 모듈식 호출이 이해되도록 타협이 포함된다. 또한 실험을 통해 유전체EM이 모듈적이고 복합적인 행동, 즉 서로 위에 형성된 행동을 생성할 수 있음을 보여준다. 텍스트에 대한 OpenAI의 GPT-4 API를 샘플링하여 행동을 생성했다.\n' +
      '\n' +
      '그림 4. 1차 사용자 연구에서 각 조건(3번)에서 각 행동(10번)에 대한 세 가지 질문에 대한 참가자의 설문 응답을 보여준다. 상단의 막대는 상당한 차이를 나타내며, 여기서 (*)는 p<.05 및 (**)는 p<.001 에르러 막대를 나타낸다. 첫 번째 도표는 조건 전반에 걸쳐 각 문항에 대한 평균 점수를 보여준다. 화살표는 더 나은 점수가 거짓말을 하는 방향을 반영한다.\n' +
      '\n' +
      '그림 5. 2차 사용자 연구에서 각 조건(3번)에서 각 행동(10번)에 대한 세 가지 질문에 대한 참가자의 설문 응답을 보여준다. 상단의 막대는 상당한 차이를 나타내며, 여기서 (*)는 p<.05 및 (**)는 p<.001 에르러 막대를 나타낸다. 첫 번째 도표는 조건 전반에 걸쳐 각 문항에 대한 평균 점수를 보여준다. 화살표는 더 나은 점수가 거짓말을 하는 방향을 반영한다.\n' +
      '\n' +
      '온도가 0으로 설정된 완전 [32] (gpt-4-0613). 사용자 연구 및 이동 조작기에 대한 실험 외에도 ROS를 통한 가제보/유니티에서 4중 시뮬레이션된 실험을 사용하여 추가 실험을 수행했다.\n' +
      '\n' +
      '***Ablation*** 우리는 유전체EM을 언어 지침을 취하고 하나의 전화를 LLM에 호출하여 표현 행동을 생성하는 엔드 투 엔드 접근법과 비교하기 위해 ablation을 수행했다. 이동 로봇용 기존 API를 사용하여 연마 작업을 수행했다. 조사된 행동은 프롬프트와 함께 첫 번째 사용자 연구와 동일했다. 각 프롬프트를 5회 샘플링하여 행동을 생성하고 로봇에서 실행하여 _c 보정_을 확인했다. 또한 실험자는 행동 코드가 인간의 사회 규범을 설명하기 위해 추론을 통합했는지 확인하기 위해 코드를 조사했다. 코드 정확성과 사회적 규범 적절성에 대한 결과는 표 1에 나와 있으며, 우리의 접근법은 2개의 행동 - _Excuse_ 및 _435low_에 대해 성공적인 실행이 발생하지 않은 절제된 변이에 비해 더 높은 성공률을 생성했다. i_Excuse_행동을 위해서는 로봇이 사용자의 거리 및 신호를 자신의 방법으로 확인하여야 한다. 그러나 절제된 변이에 대해서는 시도에서 거리를 확인한 적이 없다. i_후속low_행동에 대해서는 기존에 정의되지 않은 기능이라 불리는 코드가 로봇 API를 호출할 때 잘못된 입력 파라미터 유형을 사용하여 성공 시도가 0이었다. 또한, 생성된 거의 모든 기능이 도스트링이 누락되어 주장이라고 명명되었으며, 이는 모듈러 방식으로 더 복잡한 행동을 사용하기 어려울 수 있다(몇 가지 샷 코드 예제 제공에도 불구하고).\n' +
      '\n' +
      '우리는 GenEM에 의해 생성된 행동이 사회 규범, 특히 보다 복잡한 행동에 대해 반영되고, 보다 간단한 행동에 대해서도 유사하게 나타났음을 질적으로 관찰했다. 예를 들어, GenEM에 의해 생성된 _Excuse_ 거동은 스피치 모듈을 사용하여 _"실례 me"_을 말하였다. i_ 예측할_ 행동에 대해 절제된 변이는 사람을 보고 빛 띠를 켜고 꺼낸 반면, GenEM 변이는 "적극적인 청취"를 모방하기 위해 주기적인 결절도 통합했다. i_Approach_ 행동의 경우, 유전체EM 변형은 항상 결절을 사용하지 않는 동안 사람 쪽으로 이동하기 전에 결절을 통합했으며 대신 두 가지 경우에 조명을 사용했다.\n' +
      '\n' +
      '***C 교차 상징 행동 세대*** 시뮬레이션된 Spot 로봇에 대한 API를 사용하여 행동당 5회 동일한 프롬프트를 첫 번째 사용자 연구에서 샘플링했다. 표 2에 요약된 결과는 자신의 어포던스 및 API로 다른 로봇 플랫폼을 사용하여 동일한 프롬프트를 사용하여 가장 표현적인 행동을 생성할 수 있음을 보여준다. 그러나 _Approach_와 같은 일부 생성된 행동에는 로봇이 그 근처에서 안전한 거리 대신 인간의 위치로 항해하는 변화가 포함되었으며, 이는 사회적 규범 불일치로 간주될 수 있는 변화(번역 API에서 거리 임계 매개변수가 없기 때문)인 반면 일부는 인간(예를 들어, 로봇은 _ complementary_에 대해 사람 대신 임의의 각도를 회전하는 것)을 설명하지 않았다. 전반적으로, 성공률은 다른 로봇 실시예들에 대한 접근법의 일반성을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & \\multicolumn{2}{c}{**GenEM**} & \\multicolumn{2}{c}{**Ablated**} \\\\  & _Execution_ & _Norms_ & _Execution_ & _Norms_ \\\\ \\hline Nod & 5 & 0 & 5 & 2 \\\\ Shake & 5 & 0 & 5 & 2 \\\\ Wake & **4** & 2 & 3 & 0 \\\\ Excuse & 5 & 3 & 0 & - \\\\ Recoverable & 3 & 0 & 5 & 1 \\\\ Unrecoverable & 5 & 0 & 5 & 0 \\\\ Acknowledge & 5 & 1 & 5 & 0 \\\\ Follow & **3** & 1 & 0 & - \\\\ Approach & 5 & 1 & 5 & 3 \\\\ Attention & **4** & 0 & 1 & 0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1은 각 샘플링 시 행동 생성의 성공적인 시도를 보여주는 이동 로봇 플랫폼에 대한 분류는 _항체 명령어 Following_ 모듈이 없는 변화에 대한 접근(피드백 없이)과 후속적으로 인간 표현 운동을 로봇 표현 운동으로 번역하는 모듈을 비교하는 것을 5회 촉발했다. i_Ex실행_ 컬럼은 성공적인 시도 수를 나타낸다(_5_). i_Norms_ 컬럼은 사회적 규범이 적절하게 뒤따르지 않은 시도(실험자에 의해 코딩)의 수를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & _Execution_ & _Norms_ \\\\ \\hline Nod & 5 & 0 \\\\ Shake & 5 & 0 \\\\ Wake & 5 & 0 \\\\ Excuse & 3 & 0 \\\\ Recoverable & 5 & 2 \\\\ Unrecoverable & 4 & 0 \\\\ Acknowledge & 4 & 1 \\\\ Follow & 2 & 2 \\\\ Approach & 5 & 5 \\\\ Attention & 1 & 0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2는 시뮬레이션에서 4중으로 생성된 행동자는 각 프롬프트 5회 샘플링 시 행동 생성의 성공적인 시도를 보여준다. i_Ex실행_ 컬럼은 성공적인 시도 수를 나타낸다(_5_). i_Norms_ 컬럼은 사회적 규범이 제대로 관찰되지 않은 시도(실험자에 의해 코딩)의 수를 나타낸다.\n' +
      '\n' +
      '그림 6. 쿼드럽은 피드백 전에 GenEM에 의해 생성된 _재조합성 실수_행동(톱) 및 _보복성 실수_(바닥)를 수행하는 가제보에서 시뮬레이션되었다. 이 로봇은 회복 가능한 실수를 한 뒤 눈을 돌리고 다리를 낮추고 붉은 조명을 뒤집어 안타까움을 전한 뒤 초기 위치로 돌아가 초록빛을 끄며 실수를 한 것을 보여준다. 대조적으로, 밝혀질 수 없는 실수가 로봇이 높이를 낮추고, 짧은 기간 동안 적색 조명을 표시하며, 이 포즈를 숙이고 유지한다.\n' +
      '\n' +
      '** 결합 복합 억제 행동*** 사용자 연구에서 몇 가지 샷 예와 기존 로봇 API를 사용하여 처음부터 모든 행동을 생성했다. 우리는 이전 상호작용에서 학습된 표현 행동 세트를 사용하여 보다 복잡한 행동을 생성하려고 시도했으며, 이러한 기술(단축과 함께 기능)은 로봇의 능력(접근법의 2단계) 및 로봇의 API(접근법의 3단계)를 설명하는 프롬프트에 지정됐다. 프로젝션에서 사용된 학습된 행동은 _nodding, 눈 접촉, 광 스트립 비추기, 둘레,_ 및 _shaking_였다. 우리는 젠EM이 _Acknowledge Walk, Approach_ 및 혼란(_Confusion_)의 복잡성으로 변하는 세 가지 행동을 생성하도록 촉발했다. 이러한 모든 행동은 원하는 행동에 대한 단일 라인 설명을 포함하는 지침을 사용하여 피드백을 제공하지 않고 사중에서 생성되었다. 학습된 행동이 출력 프로그램에 포함된 빈도를 평가하기 위해 젠EM을 5회 샘플링했다. 성공을 평가하기 위해, 실험자는 생성된 코드가 로봇 API의 조합을 사용하고 API를 학습했는지 여부를 확인했다(표 3 참조). 접근행동은 _nod 헤드_행동을 사용하지 않은 반면 깜박임 조명을 항상 사용했다는 점에 주목하는 것이 놀랍다. 혼동을 표현하기 위해 4/5예가 주변의 탐색을 위한 코드를 생성했다는 것은 놀라운 일이었지만, 기존의 _ 보이지 않는 약_행동을 사용한 경우는 1/5예에 불과했다.\n' +
      '\n' +
      '인간 피드백에 대한 적응 가능성**** 사용자 연구에서 피드백은 생성된 행동에 대한 인식에 어느 정도 영향을 미쳤다. 또한, 피드백이 행동 생성을 다른 방식으로 제거할 수 있음을 질적으로 관찰했다. 우리는 _Excuse, Approach_ 및 _Acknowledge Stop_의 두 가지 이전 연구에서 세 가지 행동을 생성한 실험에서 이를 연구했다. 각 행동은 이전과 같이 단일 라인 설명을 사용하여 생성되었으며 학습된 로봇 API가 없었다. 우리는 (1)의 4가지 유형의 피드백을 통해 생성된 행동을 수정하고, 다른 행동 이전에 발생해야 한다는 것을 구현하고, (2) 행위의 순서를 스와핑하고, (3) 행동 반복 자체(루프)를 만드는 것, (4)는 대안(예: 광 스트립을 사용하는 행동을 만든 후 능력으로서 광 스트립을 제거하는 것)을 제공하지 않고 기존 능력을 제거하려고 했다. 전반적으로, 결과(표 4 참조)는 능력을 제거하면 정의되지 않은 기능을 더 자주 호출하게 되지만 제공된 피드백 유형에 따라 행동을 수정할 수 있음을 시사한다.\n' +
      '\n' +
      '## 6. Discussion\n' +
      '\n' +
      '*** 요약*** 이 작업에서 사용자 언어 지침을 로봇 코드로 번역하여 대형 언어 모델을 사용하여 표현 로봇 움직임을 생성하고 수정하는 접근 방식, GenEM을 제안했다. 사용자 연구와 실험을 통해, 우리는 우리의 틀이 텍스트 내 학습과 소수의 샷 프롬프트 방식으로 빠르게 표현 행동을 생성할 수 있음을 보여주었다. 이는 사전 작업에서와 같이 특정 로봇 행동 또는 신중하게 제작된 규칙을 생성하기 위해 큐레이션된 데이터셋의 필요성을 감소시킨다. 사용자 연구에서 참가자들은 사용자 피드백 능력이 있고 이해하기 쉬운 GenEM을 사용하여 생성된 행동을 발견했으며 일부 경우에는 전문가 애니메이션자가 만든 행동보다 훨씬 더 긍정적으로 인식했음을 입증했다. 우리는 또한 우리의 접근법이 다양한 유형의 사용자 피드백에 _적응 가능한_이고 더 복잡한 행동이 단순하고 학습된 행동을 결합하여 _comaction_일 수 있음을 보여주었다. 이들은 함께 인간의 선호에 따라 조건화된 표현 로봇 행동의 급속한 생성을 위한 기초를 형성한다.\n' +
      '\n' +
      '** 제한 및 미래 작업** 접근법의 약속에도 불구하고 몇 가지 단점이 있다. 우리의 사용자 연구는 녹화된 비디오를 통해 온라인으로 수행되었으며, 이는 유효한 방법론이지만(한 et al., 2018; 왕 et al., 2019) 로봇(왕 et al.,Wang et al., 2019)의 물리적 근접성에서 참여자들이 어떻게 반응할지 반영할 수 없다. 따라서 로봇과의 상호 작용을 포함하는 추가 연구를 추구해야 한다. 작은 컨텍스트 창과 텍스트 입력의 필요성을 포함하여 현재 LLM의 일부 고유한 한계를 유의해야 한다.\n' +
      '\n' +
      '우리 작품에서 우리는 단회전 행동(예: 행인을 인정)만을 평가하지만, 멀티턴이고 인간과 로봇 사이의 백 앤 포크 상호작용을 포함하는 행동을 생성할 기회가 있다. 향후 작업은 또한 조작기와 그리퍼를 포함하여 더 큰 액션 공간으로 동작 생성 동작을 탐색해야 한다. 우리의 접근법이 사용자 피드백과 그들의 선호도에 적응할 수 있음을 보여주었지만, 현재 더 긴 기간 동안 사용자 선호도를 배울 메커니즘은 없다. 실제로 사용자는 주어진 상황에서 로봇이 발휘되기를 기대하는 행동에 대한 개인의 선호 차이를 보일 것으로 기대한다. 따라서 학습 선호도 인텍스트(왕 등 2019)는 표현 행동을 다듬는 강력한 메커니즘일 수 있다.\n' +
      '\n' +
      '이러한 한계에도 불구하고 우리의 접근법은 큰 언어 모델의 힘을 통해 적응 가능하고 복합 가능한 표현 운동을 생성하기 위한 유연한 프레임워크를 제시한다고 믿는다. 이는 로봇이 사람과 보다 효과적으로 상호 작용할 수 있도록 표현적 행동 생성에 대한 미래의 노력에 영감을 주길 바란다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & _Insert_ & _Swap_ & _Loop_ & _Remove_ \\\\  & _actions_ & _actions_ & _actions_ & _capability_ \\\\ \\hline Excuse & 4 & 5 & 5 & 5 \\\\ Approach & 4 & 5 & 5 & 3 \\\\ Acknowledge Stop & 5 & 5 & 4 & 3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4의 성공률(5시도의 출현)은 젠EM을 사용하여 생성된 행동에 대한 다양한 유형의 피드백을 제공할 때, 즉 _Insert 액션_가 다른 액션보다 먼저 새로운 행동을 요구하며, _Swap 액션_는 기존 액션의 순서를 스왑하도록 요청하며, _Loop 액션_요청은 루프를 반복 액션에 추가하도록 요청하며, _loove 능력__above 능력_는 대체 액션과 기존 액션을 스왑할 것을 요청한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & _Eye_ & _Blinking_ & _Look_ & _Shake_ & _Nod_ \\\\  & _contact_ & _lights_ & _around_ & _head_ & _head_ \\\\ \\hline Acknowledge Walk & 5 & - & - & - & 5 \\\\ Approach & 4 & 5 & - & - & 0 \\\\ Confusion & - & 4 & 1 & 5 & - \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '유전자EM을 사용하여 새로운 행동(화살)을 구성할 때 이전에 학습된 행동(컬럼)이 사용되는 시간(5회 시도)의 수는 표 3. 충돌은 새로운 행동의 생성을 촉발할 때 주어진 학습된 행동 API가 제공되지 않음을 나타낸다.\n' +
      '\n' +
      '###### Acknowledgements.\n' +
      '\n' +
      ' 기준 로봇 행동에 애니메이션을 제공해 주신 더그 도올리, 시스템에 대한 유용한 논의를 위해 에드워드 이씨에게 감사드린다. 로봇 접근과 문제 제기에 도움을 주신 리시 크리슈난, 디에고 레예스, 스푸티 모레, 4월 지트코비치, 로사리오 자우레게이, 동영상 녹화를 지원해 주신 카바잘, 조덴 페랄타, 조나단 벨라 법무에게 감사드린다. 마지막으로 사용자 연구와 데이터 수집 노력을 조율해 주신 벤 제니스와 UX 연구팀에 감사드립니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)\n' +
      '* A. 안, A. 브한, N. 브라운, Y. 치즈봇, O. Cortes, B. 데이비드, C. Finn, C. Fu, K. 고팔라쿠리슈난 K. 하우즈맨, 즉 내가 할 수 있는 것처럼 (2023)도, 로봇 어포던스에서 언어를 접지하는 것은 아니다. 로봇 학습 회의에서 PMLR, pp 287-318: SS1이 게시했다.\n' +
      '* A. A. A. Tapas(2013)A 모델은 인간-로봇 상호작용에서 성격 특성에 기초한 결합 언어 및 비언어적 행동을 합성하기 위한 모델이다. 2013년 제8차 ACM/IEEE 인간-로봇 상호 작용 국제회의(HRI)에서 pp. 325-332: SS1에 의해 체결되었다.\n' +
      '*N. 베르스트롬, T. 칸다, T. 미야시타, H. 이시구로 및 N. 2008년 지능형 로봇 및 시스템에 관한 국제 컨퍼런스 ieee/ij에서 자연 인간봇의 하기타(2008) 모델링. SS1: SS1로 받았습니다.\n' +
      '*N. 부키나, S. 카멜과 E. 바라코바(2016)는 로봇 프로그래밍을 위한 최종 사용자 친화적인 도구의 설계 및 평가를 수행한다. 2016년 제55차 IEEE 국제 로봇 및 인간 상호 작용 커뮤니케이션 심포지엄(RO-MAN)에서 pp. 185-191: SS1로 지정되었다.\n' +
      '* M J J. 정, J. 휴앙, L. 타카야마, T. 루, M. 사회적 상호작용 서비스 로봇을 프로그래밍하기 위한 시스템의 카막(2016) 평가 설계입니다. 사회 로봇 8차 국제회의에서 ICSR 2016, 캔자스 시티, 미국 MO, 11월 1-2, 2016년 프로이딩 A S. 식기, pp. 919-929.: SS1.\n' +
      '* Y. 의, S. 카메치티, 버크홀데리아 파레티, N. 시바키아마르, P. 리앙, D. 사디고(2023)는 공유자치를 통한 로봇 조작을 위한 온라인 언어 수정의 적절성. 인간-로봇 상호 작용에 관한 2023 ACM/IEEE 국제 회의의 개최에서 pp. 93-101: SS1에 의해 체결되었다.\n' +
      '* P. 데이비드, M. Cak막, A. Sauppe, A. 알바구티 및 B. Mutlu(2022) 상호 작용 테이터: 로보트 프로그램을 승인하기 위한 데이터-디벤 앱라우치이다. PLATAT에서 외부 링크: SS1에 의해 링크되었습니다.\n' +
      '* R. 데다이, F. 앤더슨, J. 메이트조카, S. 코코스, J. McCann, G. 피츠무리스 및 T. 크로스만(2019) 거페토: 표현 로봇 행동의 의미 설계를 가능하게 한다. 컴퓨팅 시스템에서의 인간 팩터 관련 2019 CHI 회의 개최에서 pp. 1-14.: SS1에 의해 발표되었다.\n' +
      '*M. 인간 사회적 행동의 로봇 모방 학습을 위한 대화, D. F. 글래스 및 H. 이시구로(2019) 모델링 상호 작용을 한다. IEEE 전환은 인간-기계 시스템49(3), pp 219-231에 대한 상호 작용: SS1에 의해 계산된다.\n' +
      '* Q. 동, L. 리, D. Dai, C. Z정. 와, B. 창, X. 선, J.Xu, Z. 의(2022)는 텍스트 내 학습을 위한 설문 조사를 한다. arXiv 프리프린트 arXiv:2301.02034: SS1에 의해 계산된다.\n' +
      '* P. 페라렐리, M. 리자로, L. 로봇을 가르치는 로치(2018) 설계는 멀티 모달 인간-로봇 상호작용을 통해 보조자를 가르친다. 교육용 로봇 공학에서 라틴 결과 및 개발, pp. 274-286.: SS1이 적용한다.\n' +
      '* G. 호프만과 W. 자일(2014)은 움직임을 염두에 두고 로봇을 디자인한다. 인간-로봇 인터액션3(1), pp 91-122의 저널: SS1에 의해 계산된다.\n' +
      '* C. 황 및 B. Mutlu (2013) 로봇 행동의 레퍼토리는 로봇이 사회적 행동을 통해 상호 작용 목표를 달성할 수 있게 한다. 인간-로봇 인터액션2(2), pp. 80-102의 저널: SS1에 의해 계산된다.\n' +
      '* C. 황 및 B. Mutlu(2014)는 인간 유사 로봇을 위한 다중 모드 행동의 기반 모델링을 학습한다. 2014 ACM/IEEE 국제 인간 로봇 상호 작용에 관한 회의의 개최에서 pp. 57-64: SS1에 의해 발표되었다.\n' +
      '*W. 후앙, F. 샤, Y. 마리스, Y. 장, 장, 장, P. 피렌체, A. Zeng, J. 톰슨, I. Mordatch, Y. 치즈봇, 예를 들어 알(2023) 인너 모노로그라: 언어 모델과의 계획을 통해 추론을 구현했다. 로봇 학습 회의에서 pp 1769-1782는: SS1로 작성되었다.\n' +
      '*N. Hussain, E. Erzin, T. M. 세진 및 Y. 사회적으로 참여하는 로봇을 훈련시키는 Yemez(2022): 두 강화 학습으로 백채널 행동을 모델링한다. IEEE 거래 결함 컴퓨터화13(4), pp 1840-1853.: SS1에 의해 계산된다.\n' +
      '* Y. 가토, T. 칸다, H. 이시구로(2015)를 도와드릴까요? 인간과 같은 경찰의 설계는 행동에 접근한다. 인간-로봇 상호 작용에 관한 제15회 연례 ACMIEEE 국제 회의의 개최에서 pp. 35-42: SS1에 의해 발표되었다.\n' +
      '* A. 쿠보타, E. I. 피터슨, V. 라젠덴렌, H. Kress-Gaif 및 L. D. Riek (2020)Jessie: 개인화된 신경 재활을 위한 사회적 로봇 행동을 합성한다. 인간 로봇 상호 작용에 관한 2020 ACM/IEEE 국제 회의의 개최에서 pp. 121-130: SS1에 의해 발표되었다.\n' +
      '*M. 권, S. H. 황, A. D. Dragan (2018) 로봇 무능력 표현. 2018 ACM/IEEE 인간-로봇 상호 작용에 관한 국제 회의의 개최에서 pp. 87-95.: SS1에 의해 발표되었다.\n' +
      '*M. 권, S. M. Xie, K. 불라드, D. 사디이(2023) 리워드 디자인으로 언어 모델이 있습니다. 국제 학습 발표회의(ICLR)에서는 SS1이 수여했다.\n' +
      '*N. 레너디, M. 만카, F. 파테니오 및 C. 산토로(2019)는 인간형 로봇 행동을 개인화하기 위한 환경 프로그래밍이다. 컴퓨팅 시스템에서의 인간 팩터 관련 2019 CHI 회의 개최에서 pp. 1-13: SS1에 의해 발표되었다.\n' +
      'Z*Z. Li, C. 커밍스, K. 스크린(2020) 수정 카세트: 동적 재발 가능한 로봇 캐릭터입니다. 2020년 IEEE/RSJ 지능형 로봇 및 시스템(IROS) 국제 회의에서 pp 379-374: SS1이 게시했다.\n' +
      '* J. 리앙, W. 황, F. 샤, P. Xu, K. Hausman, B. Ichter, P. Florence 및 A. Zeng(2023)Code는 체화된 제어를 위한 언어 모델 프로그램으로서 정책이다. 19 2023년 IEEE 로보틱스 및 오토메이션 국제회의(ICRA)에서 pp. 9493-9500: SS1이 발표했다.\n' +
      '* K. 린, C. 라자, T. Marone, P. Bruno 및 J Bohg(2023)Text2020.: SS1에 의해 계산되었습니다.\n' +
      '* P. 류, D. 글라스, T. 칸다와 H. 이시구로(2016) 데이터 중심 리스는 인간-인간 상호 작용에서 예시적으로 사회적 행동을 배우는 것이다. IEEE 거래 로보틱스32 (4), pp 988-1008.: SS1에 의해 계산된다.\n' +
      '*M. 마리오네겐, A. 임, T. S. 다올 및 N. Hennion(2019)은 다양한 자동 암호화기를 사용하여 로봇 경합 언어를 생성합니다. 2019 제8차 보호 컴퓨팅 및 지능형 상호 작용에 관한 국제 회의(ACIT)에서 pp 543-551: SS1이 발표했다.\n' +
      'S*S. 민, X. 유, A. 홀츠만, M. 아테텍스, M. 루이스, H. 하지시르지 및 L. 제트렘요어(2022)는 시연의 역할을 정의하는데, 이는 교수 내 학습이 작동하는 것이다. arXiv 프리프린트 arXiv:2202.12837: SS1에 의해 계산된다.\n' +
      'S*S. 미라차단다니, F. 샤, P. 피렌체, B. Ichter, D. Driess, M. A. 아레나, K. Rao, D. Sadigh, A. Zeng(2023) 대형 언어 모델은 일반적인 패턴 기계입니다. 제7차 로봇 학습 컨퍼런스(CoRL) 개최에서 SS1: SS1로 작성되었다.\n' +
      '*M. 머레이, N. 와커, A. 나노바티, P. 알베스-올리브라, N. 필리핀, M. 삼페, 바실러스 Mutlu 및 M. 카막(2022)은 인간-인간 대화로부터 데이터 증강을 통해 사회 로봇에 대한 백터링 행동을 학습한다. 로봇 학습 컨퍼런스에서는 PMLR, pp 513-525: SS1로 작성되었다.\n' +
      '* O. Ozsa(2023)GFT-tberai. : arXiv:2308.08774[cs.CL]: SS1에 의해 계산된다.\n' +
      '*N. Orzbayev, Aly, A. 샌드리바우딘 및 B. Mutlu(2023) 데이터 주도 통신 행동 생성 조사이다. ACM 인간-로봇 상호 작용에 대한 거래. SS1: SS1로 받았습니다.\n' +
      '* D. 포피리오, L. 수산, A. Sauppe, A. 알바구티 및 B. Mutlu (2019)바디스토밍 인간-로봇 상호작용. 사용자 인터페이스 소프트웨어 및 기술에 대한 제32회 연간 ACM 심포지엄의 절차에서 pp. 479-491:SS1이 게시했다.\n' +
      '* D. 포피리오, A. 사부테, A. 알바르구티 및 B. Mutlu(2018)는 인간-로봇 상호작용을 지지하고 검증한다. 사용자 인터페이스 소프트웨어 및 기술에 대한 제31회 연례 심포지엄의 진행에서 pp. 75-86.: SS1이 게시했다.\n' +
      '* D. 포피리오, A. 사부테, A. 알바르구티 및 B. Mutlu(2020)는 사회적 맥락을 기반으로 로봇 프로그램을 변환한다. 컴퓨팅 시스템의 인간 요인에 대한 2020년 CHI 회의의 진행에서 pp. 1-12: SS1에 의해 발표되었다.\n' +
      '* D. 포피리오, L. 스텝너, M. 각막, A. 사부테, A. 알바르구티 및 B. Mutlu(2023) 스푸딩 로봇 프로그램을 루프에 배치한다. 인간-로봇 상호 작용에 관한 2023 ACM/IEEE 국제 회의의 개최에서 pp. 584-593:SS1에 의해 체결되었다.\n' +
      '* D. 포피리오, L. 스텝너, M. 크막, A. Sauppe, A. 알바구티 및 B. Mutlu(2021) 그림자: 인간-로봇 상호작용을 위한 탁상 저작 환경이다. 컴퓨터 시스템에서의 인간 팩터들에 대한 2021년 CHI 회의 개최에서 pp. 1-15.: SS1에 의해 발표되었다.\n' +
      '* I. 신기, V. 블라우키스, A. 무스사비, A. 고열, D. Xu, J. 트렘블레이, D. 폭스, J. 톰슨 및 A. 가르그(2023) 프로그램: 대형 언어 모델을 사용하여 위치 로봇 작업 계획을 생성한다. 19 2023년 IEEE 로보틱스 및 자동화에 관한 국제 회의(CRA)에서 pp 11523-11530: SS1이 게시했다.\n' +
      '* A. 시병증, A. 보호. Z. 케리, K. 스크린, D. S. 브라운 및 A. Dragan(2022) 테스팅 로봇은 기능적 표현 운동의 공간에 걸쳐 있다. 2022년 IEEE/RSJ 지능형 로봇 및 시스템(IROS) 국제 회의에서 pp 13406-13413: SS1이 발표했다.\n' +
      '*M. 수기탄, M. 브레탄과 G. 호프만(2019) 효과 로봇 이동 생성은 사이코그램을 사용한다. 2019년 제19차 ACM/IEEE 인간-로봇 상호 작용 국제회의(HRI)에서 pp 534-535: SS1이 발표했다.\n' +
      '*M. 수기탄, R. Gomez와 G. Hoffman(2020)MoveAE: 가변 자동 암호화기를 분류하여 정의적 로봇 움직임을 수정한다. 인간 로봇 상호 작용에 관한 2020 ACM/IEEE 국제 회의의 개최에서 481-489.\n' +
      '* 다카야마 등 (2011) 레이라 다카야마, 더그 도롤리, 웜디 주. 2011년 익스프링 생각: 애니메이션 원리로 로봇 가독성을 향상시킵니다. 인간봇 상호 작용에 관한 제6차 국제 회의의 _검토. 69-76.\n' +
      '* 왕 등은 왕(2023) 과안지쿠 왕, 유키 시, 윤판 장, 아주 만들카르, 차와이 샤오, 유케 주, 린시 판, 아민크스 안드쿠마르. 2023. 보이저: 큰 언어 모델을 가진 개방형 구현 에이전트. __개방형 체화된 에이전트. arXiv 프리프린트 arXiv:2305.16291_(2023)\n' +
      '*웨이 등은 (2022) 제이슨웨이, 주휘왕, 데일 슈르만렌스, 마트엔 보스마, 페아샤, 에드치, 퀀코 비 르, 데미 저우, 르. 고유한 프롬프트는 __2022. 체인 프롬프트는 대형 언어 모델에서 추론을 이끌어낸다. 신경 정보 처리 시스템_35(2022), 24824-24837의 발전이다.\n' +
      '* 우즈 등은 알리우즈(2006)와 마이클 월터스, 청 이고이, 케르스틴 다텐하렌 등이 있다. 2006년은 살아있는 방법과 비디오 기반 방법을 사용하여 인간 로봇 상호 작용 시나리오를 비교하는 것: 새로운 방법론적 접근에 대한 것이다. 2006년 제9회 IEEE 첨단 모션 제어_, IEEE, 750-755에 관한 국제 워크숍에서.\n' +
      '* 우 등은 (2023) 지미우, 라카 안토니바, 아담칸, 마린 레퍼트, 앤디쫑, 슈란 송, 저네트 보, 시보먼 러나케프, 토머스 벙커 등이 있다. 2023. Tidy: 대형 언어 모델과의 포스트 개인 맞춤형 로봇 지원입니다. i_2023 IEEE/RSJ 국제 지능형 로봇 및 시스템(IROS)_에 관한 회의. (https://doi/10.1109/IROS.1109/IROS.355522.2023.2023.1034577)](https://doi/10.1109/IROS.355522.2023.2023.1034577) [https://doi.org/10.1109/IROS.355522)](https://doi.org/IROS.1109/IROS.355522.2023.2023.2023.264.355522.2023.2023.2023.264.2023.2023.2023.2023.0022.2023.2023.2023.264.355522)\n' +
      '만세 곤잘라디, 추앙헤르마니, 서안 쿠르마니, 하노-티엔 루이스치앙, 톰 에레즈, 레너드 하센클러, 얀 홉플라크, 얀 후미플릭, 테드 샤오, 펑쉬우, 앤디탱, 앤디 장, 니콜라스 루트, 지카르, 지르다 탄, 유발 타사, 피샤. 2023. 로보틱 스킬 합성 언어. 제7차 로봇 학습 컨퍼런스(CoRL)_검토에서.\n' +
      '* 저우, 드라간(2018) 앨런 저우, 안나 드라간. 2018년 로봇 모션 스타일에 대한 비용 기능입니다. i_2018 IEEE/RSJ 국제 지능형 로봇 및 시스템(IROS)_에 관한 회의. IEEE, 3632-3639.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>