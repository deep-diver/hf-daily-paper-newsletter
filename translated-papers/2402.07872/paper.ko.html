<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# PIVOT: VLMs에 대한 행동가능한 지식의 반복적 시각적 프롬프트\n' +
      '\n' +
      'Soroush Nasiriany123\n' +
      '\n' +
      'Fei Xia12\n' +
      '\n' +
      'Wenhao Yu12\n' +
      '\n' +
      'Ted Xiao12\n' +
      '\n' +
      'Jacky Liang1\n' +
      '\n' +
      'Ishita Dasgupta1\n' +
      '\n' +
      'Annie Xie2\n' +
      '\n' +
      'Danny Driess1\n' +
      '\n' +
      'Ayzaan Wahid1\n' +
      '\n' +
      'Zhuo Xu1\n' +
      '\n' +
      'Quan Vuong1\n' +
      '\n' +
      'Tingnan Zhang1\n' +
      '\n' +
      '이창위\n' +
      '\n' +
      'Kuang-Huei Lee1\n' +
      '\n' +
      'Peng Xu1\n' +
      '\n' +
      'Sean Kirmani1\n' +
      '\n' +
      'Yuke Zhu3\n' +
      '\n' +
      'Andy Zeng1\n' +
      '\n' +
      'Karol Hausman1\n' +
      '\n' +
      'Nicolas Heess1\n' +
      '\n' +
      'Chelsea Finn1\n' +
      '\n' +
      'Sergey Levine1\n' +
      '\n' +
      'Brian Ichter12\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '시각 언어 모델(VLMs)은 논리적 추론에서 시각적 이해에 이르기까지 다양한 작업에 걸쳐 인상적인 기능을 보여주었다. 이것은 예를 들어 로봇 제어와 같은 세계와의 더 풍부한 상호 작용을 위한 문을 열어준다. 그러나 VLM은 텍스트 출력만을 생성하는 반면, 로봇 제어 및 다른 공간 태스크는 연속적인 좌표, 동작 또는 궤적을 출력하는 것을 요구한다. VLM이 작업별 데이터에 대한 미세 조정 없이 이러한 설정을 처리할 수 있도록 하려면 어떻게 해야 합니까?\n' +
      '\n' +
      '본 논문에서는 반복적 시각적 질의응답으로 태스크를 주조하는 PIVOT(Prompting with Iterative Visual Optimization)라고 불리는 VLM에 대한 새로운 시각적 프롬프트 접근 방식을 제안한다. 각각의 반복에서, 이미지는 VLM이 참조할 수 있는 제안들(예를 들어, 후보 로봇 액션들, 로컬화들, 또는 궤적들)의 시각적 표현으로 주석이 달린다. 그런 다음 VLM은 작업에 가장 적합한 것을 선택한다. 이러한 제안은 반복적으로 정제되어 VLM이 궁극적으로 사용 가능한 최상의 답변에 집중하도록 한다. 본 논문에서는 PIVOT를 이용하여 실제 로봇 내비게이션, 영상에서의 실제 조작, 시뮬레이션에서 뒤따르는 지시, 위치 추정과 같은 추가적인 공간 추론 작업에 대해 연구한다. 우리는 아마도 놀랍게도 우리의 접근법이 로봇 훈련 데이터, 다양한 환경에서의 내비게이션 및 기타 능력 없이 로봇 시스템의 제로 샷 제어를 가능하게 한다는 것을 발견한다. 현재 성능은 완벽과는 거리가 멀지만, 우리의 작업은 이 새로운 체제의 잠재력과 한계를 강조하고 로봇 및 공간 추론 영역에서 인터넷 규모 VLM에 대한 유망한 접근법을 보여준다.\n' +
      '\n' +
      '1Google DeepMind, 2Stanford University, 3 Texas University at Austin\n' +
      '\n' +
      '에 대응: {soroush, xiafei, magicmelon, tedxiao, ichter}@google.com\n' +
      '\n' +
      '웹사이트: pivot-prompt.github.io and HuggingFace: [https://huggingface.co/space/pivot-prompt/pivot-prompt-demo](https://huggingface.co/space/pivot-prompt-demo]\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대규모 언어 모델(LLM)은 코드 생성부터 질문 응답, 심지어 논리적 추론까지 광범위한 실제 문제를 해결할 수 있음을 보여주었다[5, 3, 52]. LLM을 다중 모달 입력으로 확장하여 강력한 시각 언어 모델(VLM)을 생성하면 훨씬 풍부한 시각 양식[16, 37, 2, 9]을 처리하는 모델이 가능하므로 자연 언어뿐만 아니라 물리적 세계와도 직접 상호 작용할 수 있다. 그러나 대부분의 VLM은 여전히 _output_textual 답변만 가능하므로 이러한 상호 작용을 고수준의 질문 답변으로 제한하는 것으로 판단된다. 많은 현실 세계의 문제들은 본질적으로 공간적이다: 로봇 팔의 궤적을 제어하는 것, 이동 로봇을 위한 웨이포인트를 선택하는 것, 테이블 상의 객체들을 재배열하는 방법을 선택하는 것, 또는 심지어 이미지 내의 키포인트들을 로컬화하는 것. VLM은 이러한 유형의 구체화, 물리적 및 공간적 문제를 해결하기 위해 적응될 수 있는가? 도메인 내 추가 훈련 데이터 없이 제로 샷을 할 수 있습니까? 이 연구에서는 이를 가능하게 하기 위한 반복 프롬프트 방법을 제안하고 VLM을 사용한 제로 샷 로봇 제어 및 공간 추론의 한계와 가능성을 연구한다.\n' +
      '\n' +
      '제안된 방법은 간단한 통찰력을 기반으로 한다: VLM은 정확한 공간 출력을 직접 생성하기 위해 고군분투하지만, 거친 선택의 이산 세트 중에서 쉽게 선택할 수 있으며, 이는 다음 반복에서 더 정확한 선택을 제공하기 위해 이 세트를_refine_에 사용할 수 있다. 반복 절차의 각 반복에서, 우리는 제안 분포로부터 도출된 후보 제안들(즉, Yang et al. [59]에서와 같이 번호가 매겨진 키포인트들)로 이미지에 주석을 달고, VLM에 그들이 원하는 작업을 수행하는 정도를 순위화하도록 요청한다. 그런 다음 이 제안 분포를 수정하고 출력 공간의 더 나은 영역 주위에 클러스터링된 새로운 후보 제안을 생성하고 이 절차를 반복한다. 이러한 최적화 접근법으로, 전체 루프는 교차 엔트로피 방법[11]과 유사한 반복적 최적화로 볼 수 있으며, 각 단계는 별도의 훈련 없이 현재의 VLM들과 호환되는 시각적 질문으로서 프레이밍된다. 그림 1과 이 작업 전반에 걸쳐 우리는 후보에 번호가 매겨진 화살표가 있는 실행 예로 로봇 제어를 사용한다.\n' +
      '\n' +
      'VLM에서 공간 출력을 추출하는 방법을 갖추고 로봇 항법, 객체 파악 및 재배열, 시뮬레이션된 로봇 벤치마크에서 언어 지침, 키포인트 로컬화를 통한 비로봇 공간 추론 등 다양한 도메인에서 제로 샷 VLM 추론의 한계와 잠재력을 연구한다. 이 모든 도메인에서 우리는 수정이나 미세 조정 없이 최첨단 비전 언어 모델, 즉 GPT-4[37] 및 쌍둥이자리[17]를 사용한다는 점에 유의하는 것이 중요하다. 우리의 목표는 반드시 가능한 최고의 로봇 제어 또는 키포인트 위치 지정 기술을 개발하는 것이 아니라 그러한 모델의 한계와 잠재력을 연구하는 것이다. 향후 VLM의 개선이 실제 작업에 대한 추가적인 양적 증가로 이어질 것으로 기대한다. 이러한 설정에서 VLM의 제로 샷 성능은 완벽과는 거리가 멀지만, _any_ 로봇 데이터, 복잡한 프롬프트 설계, 코드 생성 또는 기타 특수 도구 없이 제로 샷에서 로봇을 제어하는 능력은 매우 유연하고 일반화할 수 있는 시스템을 얻는 일반적인 방법을 제공한다.\n' +
      '\n' +
      '따라서 우리의 주요 기여는 VLM을 사용한 시각적 촉진 및 반복적인 최적화, 저수준의 로봇 제어 및 기타 공간 작업에 대한 적용, 이러한 제로 샷 공간 추론을 위한 VLM의 잠재력과 한계에 대한 경험적 분석을 위한 접근법이다. 우리는 다양한 로봇 시스템과 일반적인 시각 기반 시각 질의 응답 작업에 접근 방식을 적용하고 이 접근 방식이 성공하고 실패하는 상황의 종류를 평가한다. 우리의 현재 결과는 자연스럽게 현재의 최첨단 VLM에 특정되지만, 우리는 더 크고 더 많은 성능의 VLM으로 성능이 향상된다는 것을 발견했다. 따라서 VLM 기능이 시간이 지남에 따라 계속 개선됨에 따라 제안된 접근법이 차례로 개선되기를 기대한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**VLM을 사용한 시각적 주석**VLM의 능력이 증가함에 따라 시각적 주석을 이해하는 능력[60, 46, 57, 65], 이러한 능력[6, 56]을 개선하는 것뿐만 아니라 인식 또는 의사 결정 작업[18, 59, 53, 26, 33]에 활용하는 능력에 대한 관심이 증가하고 있다. Shtedritski et al. [46]은 CLIP [40]과 같은 VLM이 특정 시각적 주석을 인식할 수 있음을 식별한다. Yang et al. [60]은 GPT-4 모델에 대해 보다 포괄적인 분석을 수행하고 복잡한 시각적 주석을 이해하는 능력을 입증한다. Yang et al. [59]는 이러한 모델이 객체 마스크 및 숫자로 입력 이미지에 주석을 달아 시각적 추론 작업을 해결할 수 있는 방법을 보여준다. 웹 내비게이션 작업[26, 57, 65]에도 시각적 프롬프트 방법을 적용하여 인상적인 제로 샷 성능을 얻었다. 우리의 작업은 이러한 작업을 기반으로 한다: 주어진 대로 제안을 받아들이거나 별도의 인식 시스템으로 제안을 생성하는 대신 PIVOT는 무작위로 제안을 생성하지만 반복적인 개선을 통해 분포를 조정한다. 결과적으로, 우리는 여러 번의 반복을 통해 비교적 정확한 출력을 얻을 수 있으며, VLM 자체 외에 별도의 인식 시스템이나 다른 모델이 전혀 필요하지 않다.\n' +
      '\n' +
      '**신속한 최적화.** LLMs[5] 내의 컨텍스트 학습에서 소수의 샷의 출현은 프롬프트에서 많은 돌파구로 이어졌다. 자연스럽게 신속한 최적화는 구배[29, 28]와 함께 또는 구배 없이, 예를 들어 인간 공학[27]과 함께 또는 언어 공간[66]에서의 자동 최적화를 통해 유망한 접근법으로 등장했다. 이러한 자동 접근법은 우리의 작업과 가장 관련이 있으며 언어 모델 피드백[39], 답변 점수[66, 58, 55] 및 환경 피드백[49]이 LLM 및 VLM의 출력을 크게 향상시킬 수 있음을 보여주었다. 이러한 이전 방법과 우리의 주요 차이점은 반복 프롬프트가 정제 단계에 걸쳐 시각적 주석을 변경하여 _visual_ 입력의 정제화를 사용한다는 것이다. 우리는 고정된 프롬프트를 식별하기 위해 오프라인이 아닌 특정 쿼리에 대해 "온라인" 프롬프트를 최적화하며, 반복적인 절차가 더 정확한 공간 출력으로 이어진다는 것을 보여준다.\n' +
      '\n' +
      '**로봇 추론 및 제어를 위한 기초 모델.** 최근 몇 년 동안 기초 모델은 고수준 추론에서 저수준 제어에 이르기까지 로봇 공학에서 인상적인 결과를 보여주었다[13, 19]. 많은 초기 작업은 LLM과 언어 출력이 잘 맞는 로봇 추론 및 계획 체제를 조사했다[21, 64, 1, 22, 34, 41, 47, 32, 31, 51, 8]. 토대 모델을 적용하여 작업을 제어하기 위해 몇 가지 유망한 접근법이 등장했다. 한 가지 작업은 기초 모델이 선택한 하위 목표가 항법[12, 44, 7, 20, 43, 14] 및 조작[10, 45]을 위한 정책에 투입하는 효과적인 추상화임을 보여주었다. 통제에 효과적인 것으로 밝혀진 또 다른 추상화는 LLM 생성 보상이며, 이는 시뮬레이션 [23, 62, 35] 내에서 최적화될 수 있다. 다른 사람들은 제어 및 지각 프리미티브를 통해 실행될 수 있는 코드를 직접 작성하기 위해 LLM을 작성하는 코드를 조사했다[30, 48, 54]. 단순 도메인에서 소수의 샷 프롬프트 언어 모델도 [36, 50] 제어가 가능한 것으로 나타난 반면, 미세 조정 기반 모델은 훨씬 더 유능한 VLM 기반 컨트롤러[4, 45, 25, 42, 15, 38]를 산출했다. 이러한 작업과 달리, 우리는 VLM이 여러 실제 로봇 플랫폼의 낮은 수준의 제어에 어떻게 적용될 수 있는지 보여준다.\n' +
      '\n' +
      '##3 반복적 시각 최적화를 통한 프롬프트\n' +
      '\n' +
      '이 작업이 고려하는 과제의 종류는 자연어로 된 과제의 설명(\\ell\\in\\mathcal{L}\\)과 이미지 관찰(I\\in\\mathbb{R}^{H\\times W\\times 3}\\)을 주어진 집합 \\(\\mathcal{A}\\)으로부터 값 \\(a\\in\\mathcal{A}\\)을 생성함으로써 해결되어야 한다. 이 집합 \\(\\mathcal{A}\\)은 예를 들어 연속 좌표, 3차원 공간 위치, 로봇 제어 동작 또는 궤적을 포함할 수 있다. [\\(\\mathcal{A}\\)이 로봇의 행동 집합일 때, 이것은 행동을 방출하는 정책\\(\\pi(\\cdot|\\ell,I)\\)을 찾는 것에 해당한다. 대부분의 실험은 로봇 동작에 대한 제어 정책을 찾는 데 중점을 둔다. 따라서 이하에서는 이러한 사용 사례를 염두에 두고 PIVOT의 방법을 제시한다. 그러나, PIVOT는 VLM으로부터 (연속) 출력을 생성하는 일반적인 알고리즘이다.\n' +
      '\n' +
      '이미지 주석을 통한 로봇 동작에 VLM을 접지하는###\n' +
      '\n' +
      '본 논문에서는 VQA(Visual Question Answering) 문제로 정책(\\(\\pi\\)을 생성하는 문제를 프레이밍하는 방법을 제안한다. 본 연구에서 사용하는 VLMs 클래스는 이미지\\(I\\)와 텍스트 프리픽스\\(w_{p}\\)을 입력으로 하여 텍스트 완성도의 분포\\(P_{\\text{VLM}}(\\cdot|w_{p},I)\\)를 생성한다. 이 인터페이스를 활용하여 정책을 도출하는 것은 (연속적인) 공간\\(\\mathcal{A}\\)에서 어떤 행동이 텍스트 완성으로 표현될 수 있는지에 대한 도전을 제기한다.\n' +
      '\n' +
      '이 작업의 핵심 아이디어는 낮은 수준의 동작을 VLM의 _시각 언어_, 즉 이미지와 텍스트의 조합으로 들어 올려 일반적인 시각 언어 작업의 훈련 분포에 더 가깝게 하는 것이다. 이를 위해, 우리는 _visual prompt mapping_\n' +
      '\n' +
      '\\[\\big{(}\\hat{I},w_{1:M}\\big{)}=\\Omega(I,a_{1:M}) \\tag{1}\\]\n' +
      '\n' +
      '이미지 관찰(I\\)과 후보 액션들의 집합 \\(a_{1:M}\\), \\(a_{j}\\in\\mathcal{A}\\)을 주석이 달린 이미지 \\(\\hat{I}\\) 및 그들의 대응하는 텍스트 라벨 \\(w_{1:M}\\)으로 변환하는 것으로, 여기서 \\(w_{j}\\)은 이미지 공간에서 \\(a_{j}\\을 나타내는 주석을 지칭한다. 예를 들어, 그림 1에 시각화된 바와 같다. 도 1을 참조하면, 카메라 매트릭스들을 이용하여 3D 위치를 이미지 공간에 투영하고, 이 투영된 위치에 시각적 마커를 그릴 수 있다. 이 마커를 텍스트 참조, 예를 들어 숫자로 레이블링하면 결과적으로 VLM이 자연 입력 공간, 즉 이미지 및 텍스트에서 쿼리될 뿐만 아니라 마커 레이블을 참조하는 텍스트를 생성함으로써 자연 출력 공간의 공간 개념을 참조할 수 있다. 섹션 4.4에서 매핑(1)의 다양한 선택을 조사하고 성능에 미치는 영향을 줄인다.\n' +
      '\n' +
      '반복적인 시각 최적화를 통한 프롬프트\n' +
      '\n' +
      '영상공간에서의 연속적인 로봇동작과 공간개념을 텍스트 레이블로 표현하면 VLM\\(P_{\\text{VLM}}\\)을 질의하여 작업을 해결하는 데 있어 동작이 유망할지를 판단할 수 있다. 따라서 우리는 최적화 문제를 해결하는 것으로 정책 \\(\\pi\\)을 얻는 것을 볼 수 있다.\n' +
      '\n' +
      '\\[\\max_{a\\in\\mathcal{A},w}\\;P_{\\text{VLM}}(w\\;\\big{|}\\;\\hat{I},\\ell)\\quad\\text{ s.t.}\\quad\\big{(}\\hat{I},w\\big{}=\\Omega(I,a). \\tag{2}\\\n' +
      '\n' +
      '직관적으로, 우리는 VLM이 매핑 \\(\\Omega\\)을 적용한 후 대응하는 레이블 \\(w\\)을 선택할 액션 \\(a\\)을 찾는 것을 목표로 한다. (2)를 해결하기 위해 반복적 알고리즘을 제안하며, 이를 반복적 시각 최적화로 프롬프팅이라고 한다. 각 반복 \\(i\\)에서 알고리즘은 먼저 분포 \\(P_{\\mathcal{A}^{(i)}}\\)로부터 후보 액션들의 집합 \\(a_{1:M}^{(i)}\\)을 샘플링한다 (그림 2(a)). 그런 다음 이러한 후보 액션들은 주석이 달린 이미지를 생성하는 이미지\\(I\\)와 관련된 액션 레이블 \\(w_{1:M}^{(i)}\\)에 매핑된다(도 2(b)). 그런 다음 레이블 \\(w_{1:M}^{(i)}\\)에 대한 객관식 질문에 VLM에 질문하여 후보 액션 중 가장 유망한 것을 선택한다(그림 2(c)). 이것은 우리가 새로운 분포\\(P_{\\mathcal{A}^{(i+1)}}\\)에 맞는 최선의 행동 집합으로 이어진다(그림 2(d)). 이 과정은 수렴하거나 최대 단계 수 \\(N\\)에 도달할 때까지 반복된다. 알고리즘 1과 그림 2는 이 과정을 시각화한다.\n' +
      '\n' +
      '병렬 호출을 이용한 강인한 PIVOT\n' +
      '\n' +
      'VLM은 실수를 하여 PIVOT가 최적이 아닌 영역에서 액션을 선택하도록 할 수 있다. PIVOT의 강인성을 향상시키기 위해 병렬 호출 전략을 사용하여 먼저 \\(E\\) 병렬 PIVOT 인스턴스를 실행하고 \\(E\\) 후보 동작을 얻는다. 그런 다음 선택된 후보를 집계하여 최종 액션 출력을 식별한다. 서로 다른 PIVOT 인스턴스로부터 후보 액션들을 집계하기 위해, 1) \\(E\\) 액션 후보들로부터 새로운 액션 분포를 피팅하고 피팅된 액션 분포를 반환한다, 2) VLM에 다시 질의하여 \\(E\\) 액션들 중에서 단일 베스트 액션을 선택한다. 병렬 호출을 채택함으로써 최적화 프로세스에서 PIVOT의 견고성을 효과적으로 개선하고 국부 최소값을 완화할 수 있음을 발견한다.\n' +
      '\n' +
      '```\n' +
      '1:Given: image \\(I\\), instruction \\(\\ell\\), action space \\(\\mathcal{A}\\), max iterations \\(N\\), sample 수 \\(M\\)\n' +
      '2:Initialize:\\(\\mathcal{A}^{(0)}=\\mathcal{A}\\), \\(i=0\\)\n' +
      '3:while\\(i<N\\)do\n' +
      '4: \\(P_{\\mathcal{A}^{(0)}}\\)로부터의 샘플 동작 \\(a_{1:M}\\)\n' +
      '5: 이미지 공간으로의 프로젝트 액션 및 텍스트 레이블 \\(\\left(\\hat{I},\\omega_{1:M}\\right)=\\Omega(I,a_{1:M})\\)\n' +
      '6: 가장 유망한 행동을 결정하기 위한 질의 VLM\\(P_{\\text{VLM}}(w\\mid\\hat{I},\\ell)\\)\n' +
      '7: 최선의 행동에 대한 적합 분포\\(P_{\\mathcal{A}^{(n+1)}}\\)\n' +
      '8 : 증분 반복 \\(i\\gets i+1\\)\n' +
      '9:endwhile\n' +
      '10: 리턴: VLM 베스트 액션으로부터의 액션\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 반복적 시각 최적화를 통한 프롬프트\n' +
      '\n' +
      '### PIVOT Implementation\n' +
      '\n' +
      '본 논문에서 제안한 방법은 여러 개의 답변이 동시에 이미지 상에서 가시화될 수 있는 한 모든 유형의 답변에 대해 VLM에 질의하는 데 사용될 수 있다. 도 1에서 시각화된 바와 같이, 시각적 프롬프트 매핑 \\(\\Omega\\)에 대해, 우리는 실시예가 보이지 않는 경우 로봇 또는 이미지의 중심에서 나오는 화살표로 동작을 표현한다. 3D 문제의 경우 화살표의 색상과 레이블의 크기는 앞뒤로 움직임을 나타냅니다. 우리는 끝에 동그라미 친 숫자 레이블로 이러한 작업에 레이블을 지정합니다.\n' +
      '\n' +
      '그림 2: 반복적 시각 최적화를 통한 프롬프트는 (a) 동작 분포로부터 동작을 반복적으로 샘플링하고 (i)}\\(\\mathcal{A}^{(i)}\\), (b) 이미지 공간에 투영하고 각 샘플에 주석을 달며 (c) 최상의 동작을 VLM에 질의하고 (d) 선택된 동작에 분포를 피팅하여 \\(\\mathcal{A}^{(i+1)}\\)을 형성함으로써 로봇 제어 정책을 생성한다. (e) 설정된 반복 횟수 후에, 선택된 베스트 액션이 실행된다.\n' +
      '\n' +
      '화살표. 달리 언급되지 않는 한, 본 명세서에서 사용된 VLM은 GPT-4V[37]였다. 텍스트 프롬프트 \\(w_{p}\\)를 생성하기 위해 VLM에 문제를 통해 추론에 대한 사고의 사슬을 사용하도록 프롬프트한 다음 상위 몇 개의 레이블을 요약한다. 알고리즘 1의 분포\\(P_{\\mathcal{A}}\\)는 등방성 가우시안들로 근사화된다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '우리는 시각 운동 로봇 제어 및 시각적으로 접지된(예: 공간) VQA에 대한 PIVOT의 능력과 한계를 조사한다. 우리의 주요 예는 (a) 미세한 시각적 접지를 필요로 하고 (b) 행동은 언어로 표현하기가 어려울 수 있으며 (c) 미리 훈련된 VLM 내에 저장된 지식으로부터 이익을 얻는 시각적 일반화에 의해 종종 병목 현상이 발생하기 때문에 제어를 위한 행동 선택을 포함한다. 우리는 접근법의 강점과 약점을 모두 이해하는 것을 목표로 하며, (i) 이러한 한계를 식별하고 (ii) 스케일링을 통해 그리고 기본 기반 모델을 개선하여 이러한 한계를 완화할 수 있는 방법을 이해하는 것이 이 작업의 주요 기여라고 믿는다. 구체적으로, 우리는 질문에 답하기 위해 노력한다:\n' +
      '\n' +
      '1. PIVOT는 로봇 제어 작업에서 어떻게 수행되나요?\n' +
      '2. PIVOT는 객체 참조 태스크에 대해 어떻게 수행하는가?\n' +
      '3. PIVOT(textual prompting, visual prompting, iterative optimization)의 다른 구성요소들이 성능에 미치는 영향은 무엇인가?\n' +
      '4. 현재 VLM을 갖는 PIVOT의 한계는 무엇인가?\n' +
      '5. PIVOT는 VLM 성능과 함께 어떻게 스케일링되는가?\n' +
      '\n' +
      '로보트 실험장치\n' +
      '\n' +
      '다음 로봇 실시예에 걸쳐 PIVOT를 평가하며, 이는 그림 3에서 시각화되고 부록 A에 자세히 설명되어 있다.\n' +
      '\n' +
      '* 네비게이션(2D 액션 공간, 도 3(a) 및 조작 작업(4D 엔드 이펙터 상대 직교 \\((x,y,z)\\) 및 바이너리 그리퍼 액션 공간, 도 3(b) 모두를 위한 헤드 장착형 카메라를 구비한 모바일 조작기.\n' +
      '* 손목 장착형 카메라와 4D 액션 공간(end-effector relative Cartesian \\((x,y,z)\\) 및 그리퍼를 구비한 프랑카 암. 결과는 부록 F에 나와 있다.\n' +
      '* RAVENS[63] 시뮬레이터로서, 오버헤드 카메라와 픽 앤 플레이스 픽셀 액션 공간을 구비한다. 결과는 부록 E에 나와 있다.\n' +
      '\n' +
      '도 3 | 우리는 (a) 내비게이션 및 (b) 조작을 위한 모바일 조작기, (c) 단일 프랑카 암 조작, 및 (d) 테이블탑 픽 앤 플레이스[63]를 포함하는 여러 로봇 실시예에 대한 PIVOT를 평가한다.\n' +
      '\n' +
      '### 현실세계의 제로샷 로봇제어\n' +
      '\n' +
      '우리의 첫 번째 실제 로봇 실험 세트는 모바일 조작기 내비게이션과 조작, 그리고 프랑카 조작으로 제로 샷 로봇 제어를 수행하는 PIVOT의 능력을 평가한다. 이러한 로봇은 제어 설정(항법 및 조작), 카메라 뷰(1인칭 및 3인칭) 및 액션 공간 차원 측면에서 다양하기 때문에 PIVOT의 유연성을 강조한다. 예를 들어, 도 4는 반복 프로세스를 밟을 때 PIVOT 및 액션 샘플(이미지 상에 투영됨)의 몇 가지 정성적 롤아웃을 도시한다. 최적화 후에, 선택된 액션들은 임의의 모델 미세 조정 없이, 타겟 객체들 및 관심 영역들(입력 언어 명령들에 가장 관련됨) 상에 더 정확하게 위치된다는 점에 유의한다. 목표 지향적 탐색 작업의 경우, 모바일 조작기가 목표 목적지(PIVOT에 언어 입력으로 제공됨)에 도달할 수 있는지 여부에 대한 성공률을 측정하여 PIVOT를 정량적으로 평가한다. 조작을 위해, 세 가지 메트릭(i) 로봇 엔드 이펙터가 관련 객체에 도달하는지 여부( 도달), (ii) 성공적인 종료(단계) 이전의 액션 단계 수를 통한 효율성, (iii) 로봇이 관련 객체를 파악하는 성공률(파지 - 적용 가능한 경우)을 통해 성능을 평가한다.\n' +
      '\n' +
      '네비게이션 및 조작 태스크 둘 다에 대한 결과(표 1 및 표 2에 도시됨)는 (i) PIVOT가 두 도메인에 대해 제로 아닌 태스크 성공을 가능하게 하고, (ii) 병렬 호출이 (성공률 측면에서) 성능(평균 액션 단계 수를 감소시킴으로써) 및 효율성을 향상시킨다는 것을 입증하고, (iii)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline  & No Iteration & 3 Iterations & No Iteration & 3 Iterations \\\\ Task & No Parallel & No Parallel & 3 Parallel & 3 Parallel \\\\ \\hline Go to orange table with tissue box & 25\\% & 50\\% & **75\\%** & **75\\%** \\\\ Go to wooden bench without hitting obstacle & 25\\% & 50\\% & **75\\%** & 50\\% \\\\ Go to the darker room & 25\\% & 50\\% & 75\\% & **100\\%** \\\\ Help me find a place to sit and write & 75\\% & 50\\% & **100\\%** & 75\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 도 3(a)의 이동 조작기에서의 항법 성공률. 우리는 반복과 병렬 호출이 성능을 향상시킨다는 것을 관찰한다.\n' +
      '\n' +
      '도 4: (a) 실제-세계 내비게이션 태스크 상의 예시적인 롤아웃. 우리는 샘플을 생성하기 위해 세 개의 병렬 호출을 사용한다. (b) 3번의 반복을 갖는 PIVOT에 의해 선택된 액션들이 매 단계에서 직접 실행되는, 실제-세계 조작 태스크 상의 예시적인 롤아웃. PIVOT는 로봇 액션의 견고성 및 정밀도를 향상시켜 단계 2. (c) RefCOCO 질문에 대한 예시적인 롤아웃과 같은 교정 동작을 가능하게 한다.\n' +
      '\n' +
      'PIVOT 반복 횟수를 늘리면 성능도 향상됩니다. 부록 F 및 E는 도 3의 (b)에 도시된 이동 조작기에 대한 표 2 | 조작 결과를 제시하며, 여기서 "Reach"는 로봇이 관련 객체에 성공적으로 도달한 속도를 나타내고, "Steps"는 단계의 수를 나타내고, "Grasp"는 로봇이 관련 객체를 성공적으로 파지한 속도(작업에 적용 가능한 경우)를 나타낸다. 우리는 모든 접근법이 0이 아닌 성공을 달성할 수 있지만 반복 및 병렬 호출은 정책의 성능과 효율성을 향상시킨다는 것을 관찰한다.\n' +
      '\n' +
      '### Zero-shot Visual Grounding\n' +
      '\n' +
      '로봇 제어 작업 외에도 정밀하고 견고한 시각적 접지를 평가하는 RefCOCO[61]의 기준 위치 지정 작업에 대한 PIVOT도 조사한다. 이를 위해 RefCOCO 테스트A 분할의 1000개 예제의 무작위 하위 집합에서 3 라운드의 PIVOT로 GPT-4V를 평가한다. 우리는 추가 반복에 비해 약간의 개선으로 첫 번째 반복에서도 강력한 성능을 발견한다. 사용된 프롬프트는 부록 H에 있고 결과는 그림 5와 그림 4의 예에 있다.\n' +
      '\n' +
      '우리는 허깅페이스에서 몇 가지 시연 이미지와 새로운 이미지와 질문을 업로드할 수 있는 기능을 제공하는 대화형 데모를 제공합니다.\n' +
      '\n' +
      '### 오프라인 성능 및 제안\n' +
      '\n' +
      '본 절에서는 오프라인 평가를 통해 PIVOT의 각 요소(텍스트 프롬프트, 시각적 프롬프트, 반복 최적화)를 살펴 실제 로봇에 대한 실행이 필요 없이 철저한 평가가 가능하도록 한다. 이를 위해 시연 데이터를 참조로 사용하여 PIVOT에서 계산된 액션이 지상-진실 전문가 액션과 얼마나 유사한지 계산한다.\n' +
      '\n' +
      '조작 영역의 경우 RT-X 데이터세트[38]로부터 기준 로봇 액션을 구하고, 카메라 프레임 내 두 액션의 코사인 유사도를 메트릭으로 계산한다. 이 메트릭은 VLM 선택이 인간 시연과 "정렬"되는 방법을 측정한다. 예를 들어, 2차원 공간에서 0.5 코사인 유사도는 \\(\\arccos(0.5)=60^{\\circ}\\)에 해당한다. 우리의 액션은 선택된 데카르트 액션 방향을 따라 최대 델타를 실행할 수 있기 때문에, 우리는 이 메트릭이 다른 메트릭들, 예를 들어 평균 제곱 오차보다 더 유익하다는 것을 발견했다. 항법 도메인의 경우 항법 로그에서 인간 레이블 데이터 세트를 사용하고 선택된 액션과 카메라 프레임의 관심 지점 사이의 정규화된 L2 거리를 메트릭으로 계산한다. 각 오프라인 데이터셋에 대한 자세한 내용은 부록 D와 B에서 확인할 수 있다.\n' +
      '\n' +
      '** 텍스트 프롬프트.** 다양한 텍스트 프롬프트의 효과를 이해하기 위해 부록 D에 보고된 숫자로 여러 디자인 선택을 실험한다. 우리는 제로 샷, 소 샷, 사고 체인 및 직접 프롬프트의 역할을 조사한다. 소 샷 직접 프롬프트가 가깝고 더 토큰 효율적이지만 제로 샷 사고 체인이 가장 잘 수행된다는 것을 발견했다. 또한 영상, 프리앰블, 태스크의 순서에 대해 실험한다. 프리앰블, 이미지, 태스크를 찾는 것은 적은 여백에도 불구하고 가장 좋은 성능을 보인다.\n' +
      '\n' +
      '**시각적 프롬프트.**시각적 프롬프트의 스타일의 양상은 색상, 크기, 음영 및 모양과 같은 선행 작업 [59, 46]에서 검토되었다. 여기에서 우리는 PIVOT의 중심인 샘플 수와 시각적 프롬프트 자체의 중요성을 조사한다. 샘플 수에 대한 절제는 그림 7에 나와 있는데, 여기서 흥미로운 경향을 주목한다: 더 많은 샘플이 더 나은 초기 답으로 이어지지만 더 나쁜 최적화로 이어진다. 직관적으로, 많은 수의 샘플들은 초기 답변에 대해 양호한 커버리지를 지원하지만, 너무 많은 샘플들로 인해, 정답 주위의 이미지 영역이 혼잡해지고 폐색(occlusion)과 함께 상당한 문제들을 야기한다. 우리의 작업을 위해 우리는 분배 범위와 충분한 시각적 선명도를 유지하는 데 가장 적합한 10개의 샘플을 발견했다.\n' +
      '\n' +
      '시각적 프롬프트 자체의 필요성을 이해하기 위해 우리는 VLM이 로봇 동작에 매핑되는 언어 동작의 하위 집합에서 선택하는 언어 전용 기준선과 비교한다. 조작 태스크에 대해, VLM은 이미지와 태스크를 부여받고 이동 "우", "좌", "상", "하" 중에서 선택한다. 유사한 내비게이션 벤치마크는 부록 B에 설명되어 있다. 우리는 그림 7과 그림 6에서 PIVOT가 텍스트보다 큰 여백만큼 성능이 우수함을 알 수 있다. 여기에서 우리는 우리의 초점이 제로 샷 이해에 있기 때문에 훈련이나 미세 조정이 필요한 학습된 접근법과 비교하지 않는다는 점에 주목한다. 우리는 이러한 많은 접근법이 이러한 작업에 대한 배포에서 잘 수행되지만 배포 작업 이외의 작업에 대한 일반화는 제한적일 것이라고 믿는다.\n' +
      '\n' +
      '**반복 최적화.** 반복 최적화 프로세스의 효과를 이해하기 위해 반복 및 병렬 호출 수를 줄인다. 도 5, 도 6 및 도 7에서, 반복을 증가시키면 성능이 향상되고, 병렬 호출을 증가시키면 성능이 향상되며, 결정적으로 수행되는 것을 발견한다\n' +
      '\n' +
      '도 7 | 코사인 유사도를 갖는 조작 작업에 대한 오프라인 평가 결과(높을수록 좋다)\n' +
      '\n' +
      '둘 다 최고를 한다. 이것은 위의 온라인 평가 결과에 영향을 미친다.\n' +
      '\n' +
      '### Scaling\n' +
      '\n' +
      '우리는 모바일 매니퓰레이터 오프라인 평가(그림 8의 PIVOT와 시연 데이터 지상 진실 사이의 코사인 유사성 및 L2 오차 측면에서 측정된 결과)에서 PIVOT가 다양한 크기의 VLM에 걸쳐 확장된다는 것을 관찰한다. 특히, 우리는 a에서 d로 레이블이 지정된 모델 [17]의 제미니 계열의 4가지 크기를 사용하여 PIVOT를 점진적으로 더 많은 매개변수와 비교한다. 우리는 각 모델 크기에 따라 성능이 단조롭게 증가한다는 것을 발견했다. 여전히 상당한 한계와 능력 격차가 있지만, 우리는 이 스케일링을 PIVOT가 모델 크기와 능력이 증가하는 차세대 기반 모델을 활용할 수 있다는 유망한 신호로 본다[17].\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '이 작업에서 우리는 최첨단 VLM과 제로샷 기능을 사용하여 PIVOT를 평가한다. 우리는 기본 모델이 시각적 주석 분포로 표현되는 로봇 제어 또는 물리적 추론을 위한 도메인 내 데이터에 대해 훈련되지 않았다는 점에 주목한다. 정확한 고장 모드는 특정 기본 VLM에 특이적일 수 있지만 광범위한 제한 영역을 반영할 수 있는 경향을 계속 관찰한다. 개선된 일반 시각 추론 능력을 가진 미래 VLM도 시각 주석 및 로봇 추론 능력이 향상될 것으로 예상하며, 현재 최첨단 VLM에 대한 PIVOT의 일반적인 한계는 향후 작업을 위한 흥미로운 열린 영역을 가리키는 잠재적인 위험과 능력 격차를 강조하는 역할을 할 수 있다.\n' +
      '\n' +
      '**3D 이해** VLM은 2D 이미지를 시각적 입력으로만 취하는 반면, 원칙적으로 PIVOT를 통해 적용된 이미지 주석 및 변환도 3D 쿼리를 나타낼 수 있다. 색상 및 라벨 크기를 사용하여 주석의 일부로 깊이 값을 표현하는 것을 조사했지만(그리고 프리앰블 프롬프트 내에서 매핑되는 것을 설명했지만), 테스트한 VLM 중 어느 것도 깊이에 따라 동작을 안정적으로 선택할 수 없음을 관찰했다. 이를 넘어 회전과 같은 고차원 공간으로 일반화하는 것은 추가적인 과제까지 제기한다. 우리는 더 복잡한 시각적(예: 깊이의 환상을 주기 위한 음영)이 이러한 문제 중 일부를 해결할 수 있다고 믿지만 궁극적으로 기본 VLM에서 3D 훈련 데이터의 부족은 병목 현상으로 남아 있다. 로봇 특정 데이터 또는 깊이 이미지로 훈련하면 이러한 문제를 완화할 수 있다.\n' +
      '\n' +
      '**상호작용 및 세밀한 제어** 폐쇄-루프 비쥬오모터 작업들(예를 들어, 1인칭 내비게이션 작업들, 또는 손-장착형 카메라들을 갖는 조작 작업에 대해) 동안, 이미지들은 종종 폐색의 양을 증가시키는 것에 의해 특징지어질 수 있으며, 여기서 관심 객체들은 카메라들이 너무 가까우면 더 이상 보이지 않을 수 있다. 이는 PIVOT 및 의사 결정을 위한 VLM에 영향을 미치며, 예를 들어, 언제 파지할지, 물체를 들어올릴지, 또는 물체를 올바른 측면에서 접근하여 밀어낼지를 결정한다. 이는 그림 9에서 시각화되어 궤적에 대한 오류가 표시된다. 이 오류들은\n' +
      '\n' +
      '그림 8: 제미니 모델[17] 크기에 걸친 첫 번째 반복 시각적 자극 성능의 스케일링 결과는 PIVOT가 개선된 VLM으로 잘 스케일링됨을 보여준다. 왼쪽 및 중앙 그림은 조작(객체 픽업, 한 객체를 다른 객체 옆에 이동)이고 오른쪽 그림은 탐색입니다.\n' +
      '\n' +
      '두 폐색, 이미지의 해상도, 그러나 아마도 더 결정적으로 유사한 상호 작용으로부터의 트레이닝 데이터의 부족의 결과. 이 경우, 구체화된 데이터 또는 비디오 데이터에 대한 트레이닝이 해결책일 수 있다.\n' +
      '\n' +
      '**탐욕 행동** 반복적 최적화가 많은 단순 오류를 완화하지만, 기본 VLM은 종종 다단계 의사 결정 작업에 대한 탐욕스럽고 근시안적인 행동을 표시한다는 것을 발견한다. 예를 들어, "사과를 바나나로 옮겨라"라는 과제를 고려할 때, VLM은 사과가 먼저 아닌 바로 바나나로 접근하는 것을 추천할 수 있다. 우리는 이러한 실수가 더 유능한 VLM 또는 미세 조정을 통해 또는 예를 들어 미래의 생성된 동작을 안내하기 위해 VLM에 대한 입력 컨텍스트로서 동작의 히스토리를 갖는 몇 개의 샷 프롬프트를 통해 제공되는 더 많은 도메인 내 예를 통해 감소할 수 있다고 믿는다.\n' +
      '\n' +
      '**시각-언어 연결 추론 오류** VLM의 전반적인 사고 과정이 합리적이지만 사고 과정을 잘못된 화살표로 확률적으로 연결한다는 것을 발견한다. 이 문제는 자기 회귀 디코딩의 도전으로 보이며, 일단 숫자가 디코딩되면 VLM은 부정확하더라도 이를 정당화해야 하며, 따라서 그렇지 않으면 합리적인 사고 과정을 환각한다. 이러한 오류 중 많은 부분이 PIVOT의 최적화 프로세스를 통해 개선되지만 강력한 최적화로 인한 도구로 추가 개선이 이루어질 수 있다고 믿는다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'PIVOT는 공간 추론 제로샷을 위해 VLM을 활용하는 유망한 단계를 제시하고 전통적으로 도전적인 문제(예: 저수준 로봇 제어)를 비전 문제로 던질 수 있는 새로운 기회를 제안한다. PIVOT는 VLM 제로 샷으로 공간적으로 접지된 연속 값을 출력해야 하는 로봇 암을 제어하는 것과 같은 작업에 사용될 수 있다. 이것은 이미지 공간에서 공간 개념을 표현한 다음 VLM을 프롬프트하여 반복적으로 정제함으로써 가능하다. 반복 최적화에 기반을 둔 PIVOT는 다른 샘플링 초기화 절차, 최적화 알고리즘 또는 검색 기반 전략의 이점을 제공한다. 또한, 본 명세서에서 성능(예를 들어, 3D 이해 및 상호 작용)을 제한하는 현재 최첨단 모델의 몇 가지 한계를 확인했다. 따라서 이러한 영역을 나타내는 데이터 세트를 추가하면 작업 특정 데이터를 직접 미세 조정하는 것과 함께 향후 작업에 대한 흥미로운 방법이 제공된다. 그러나 더 중요한 것은 VLM의 능력이 시간이 지남에 따라 향상될 것으로 예상하므로 스케일링 실험에서 조사한 바와 같이 PIVOT의 제로 샷 성능도 향상될 가능성이 있다. 우리는 이 작업이 인터넷 규모의 일반 비전 언어 과제를 동일한 입력 공간에서 표현함으로써 현실 세계의 물리적 문제와 통일하려는 시도로 볼 수 있다고 본다. 대부분의 실험은 로봇 공학에 초점을 맞추지만 알고리즘은 일반적으로 VLM으로 연속 값을 출력해야 하는 문제에 적용될 수 있다.\n' +
      '\n' +
      '그림 9: 물체를 집어 다른 물체 근처로 이동시키는 궤적 "가까이 이동"에 대한 PIVOT 성능. 초기에 성능은 높지만 로봇이 파지 및 리프트에 접근함에 따라 감소한다(물체가 가려지고 VLM이 파지하는 미묘함을 이해하지 못함). 파악 후 다른 객체로 이동함에 따라 성능은 증가하지만 접근함에 따라 다시 감소한다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '카니시카 라오, 지 탄, 캐롤라이나 파라다, 제임스 해리슨, 닉 스튜어트, 조나단 톰슨에게 도움이 되는 토론과 논문에 대한 피드백을 주셔서 감사합니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., et al. Do as i can, not as i say: Grounding language in robotic affordances. _arXiv preprint arXiv:2204.01691_, 2022.\n' +
      '* [2] Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. _Advances in Neural Information Processing Systems_, 35:23716-23736, 2022.\n' +
      '* [3] Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. _arXiv preprint arXiv:2108.07732_, 2021.\n' +
      '* [4] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. _arXiv preprint arXiv:2307.15818_, 2023.\n' +
      '* [5] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [6] Cai, M., Liu, H., Mustikowela, S. K., Meyer, G. P., Chai, Y., Park, D., and Lee, Y. J. Making large multimodal models understand arbitrary visual prompts. _arXiv preprint arXiv:2312.00784_, 2023.\n' +
      '* [7] Chen, B., Xia, F., Ichter, B., Rao, K., Gopalakrishnan, K., Ryoo, M. S., Stone, A., and Kappler, D. Open-vocabulary queryable scene representations for real world planning. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pp. 11509-11522. IEEE, 2023.\n' +
      '* [8] Chen, B., Xu, Z., Kirmani, S., Ichter, B., Driess, D., Florence, P., Sadigh, D., Guibas, L., and Xia, F. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. _arXiv preprint arXiv:2401.12168_, 2024.\n' +
      '* [9] Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J., Ruiz, C. R., Goodman, S., Wang, X., Tay, Y., et al. Pali-x: On scaling up a multilingual vision and language model. _arXiv preprint arXiv:2305.18565_, 2023.\n' +
      '* [10] Cui, Y., Niekum, S., Gupta, A., Kumar, V., and Rajeswaran, A. Can foundation models perform zero-shot task specification for robot manipulation? In _Learning for Dynamics and Control Conference_, pp. 893-905. PMLR, 2022.\n' +
      '* [11] De Boer, P.-T., Kroese, D. P., Mannor, S., and Rubinstein, R. Y. A tutorial on the cross-entropy method. _Annals of operations research_, 134:19-67, 2005.\n' +
      '* [12] Dorbala, V. S., Sigurdsson, G., Piramuthu, R., Thomason, J., and Sukhatme, G. S. Clip-nav: Using clip for zero-shot vision-and-language navigation. _arXiv preprint arXiv:2211.16649_, 2022.\n' +
      '* [13] Firoozi, R., Tucker, J., Tian, S., Majumdar, A., Sun, J., Liu, W., Zhu, Y., Song, S., Kapoor, A., Hausman, K., et al. Foundation models in robotics: Applications, challenges, and the future. _arXiv preprint arXiv:2312.07843_, 2023.\n' +
      '* [14] Gadre, S. Y., Wortsman, M., Ilharco, G., Schmidt, L., and Song, S. Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 23171-23181, 2023.\n' +
      '* [15] Gao, J., Sarkar, B., Xia, F., Xiao, T., Wu, J., Ichter, B., Majumdar, A., and Sadigh, D. Physically grounded vision-language models for robotic manipulation. _arXiv preprint arXiv:2309.02561_, 2023.\n' +
      '* [16] Gemini, T., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* [17] Gemini Team, G. Gemini: A family of highly capable multimodal models. Technical report, Google, 2023. URL [https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf).\n' +
      '\n' +
      '* [18] Gu, J., Kirmani, S., Wohlhart, P., Lu, Y., Arenas, M. G., Rao, K., Yu, W., Fu, C., Gopalakrishnan, K., Xu, Z., et al. Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. _arXiv preprint arXiv:2311.01977_, 2023.\n' +
      '* [19] Hu, Y., Xie, Q., Jain, V., Francis, J., Patrikar, J., Keetha, N., Kim, S., Xie, Y., Zhang, T., Zhao, Z., et al. Toward general-purpose robots via foundation models: A survey and meta-analysis. _arXiv preprint arXiv:2312.08782_, 2023.\n' +
      '* [20] Huang, C., Mees, O., Zeng, A., and Burgard, W. Visual language maps for robot navigation. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pp. 10608-10615. IEEE, 2023.\n' +
      '* [21] Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In _International Conference on Machine Learning_, pp. 9118-9147. PMLR, 2022.\n' +
      '* [22] Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., et al. Inner monologue: Embodied reasoning through planning with language models. _arXiv preprint arXiv:2207.05608_, 2022.\n' +
      '* [23] Huang, W., Wang, C., Zhang, R., Li, Y., Wu, J., and Fei-Fei, L. Voxposer: Composable 3d value maps for robotic manipulation with language models. _arXiv preprint arXiv:2307.05973_, 2023.\n' +
      '* [24] Itseez. Open source computer vision library. [https://github.com/itseez/opencv](https://github.com/itseez/opencv), 2015.\n' +
      '* [25] Jiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L., Anandkumar, A., Zhu, Y., and Fan, L. Vima: General robot manipulation with multimodal prompts. _arXiv_, 2022.\n' +
      '* [26] Koh, J. Y., Lo, R., Jang, L., Duvvur, V., Lim, M. C., Huang, P.-Y., Neubig, G., Zhou, S., Salakhutdinov, R., and Fried, D. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. _arXiv preprint arXiv:2401.13649_, 2024.\n' +
      '* [27] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. _Advances in neural information processing systems_, 35:22199-22213, 2022.\n' +
      '* [28] Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. _arXiv preprint arXiv:2104.08691_, 2021.\n' +
      '* [29] Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. _arXiv preprint arXiv:2101.00190_, 2021.\n' +
      '* [30] Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., Florence, P., and Zeng, A. Code as policies: Language model programs for embodied control. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pp. 9493-9500. IEEE, 2023.\n' +
      '* [31] Lin, K., Agia, C., Migimatsu, T., Pavone, M., and Bohg, J. Text2motion: From natural language instructions to feasible plans. _arXiv preprint arXiv:2303.12153_, 2023.\n' +
      '* [32] Liu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J., and Stone, P. Llm+ p: Empowering large language models with optimal planning proficiency. _arXiv preprint arXiv:2304.11477_, 2023.\n' +
      '* [33] Liu, D., Dong, X., Zhang, R., Luo, X., Gao, P., Huang, X., Gong, Y., and Wang, Z. 3daxiesprompts: Unleashing the 3d spatial task capabilities of gpt-4v. _arXiv preprint arXiv:2312.09738_, 2023.\n' +
      '* [34] Liu, Z., Bahety, A., and Song, S. Reflect: Summarizing robot experiences for failure explanation and correction. _arXiv preprint arXiv:2306.15724_, 2023.\n' +
      '* [35] Ma, Y. J., Liang, W., Wang, G., Huang, D.-A., Bastani, O., Jayaraman, D., Zhu, Y., Fan, L., and Anandkumar, A. Eureka: Human-level reward design via coding large language models. _arXiv preprint arXiv:2310.12931_, 2023.\n' +
      '* [36] Mirchandani, S., Xia, F., Florence, P., Ichter, B., Driess, D., Arenas, M. G., Rao, K., Sadigh, D., and Zeng, A. Large language models as general pattern machines. _arXiv preprint arXiv:2307.04721_, 2023.\n' +
      '* [37] OpenAI. Gpt-4v(ision) system card. Technical report, OpenAI, 2023. URL [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card).\n' +
      '* [38] Padalkar, A., Pooley, A., Jain, A., Bewley, A., Herzog, A., Irpan, A., Khazatsky, A., Rai, A., Singh, A., Brohan, A., et al. Open x-embodiment: Robotic learning datasets and rt-x models. _arXiv preprint arXiv:2310.08864_, 2023.\n' +
      '* [39] Pryzant, R., Iter, D., Li, J., Lee, Y. T., Zhu, C., and Zeng, M. Automatic prompt optimization with" gradient descent" and beam search. _arXiv preprint arXiv:2305.03495_, 2023.\n' +
      '\n' +
      '* [40] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* [41] Raman, S. S., Cohen, V., Rosen, E., Idrees, I., Paulius, D., and Tellex, S. Planning with large language models via corrective re-prompting. In _NeurIPS 2022 Foundation Models for Decision Making Workshop_, 2022.\n' +
      '* [42] Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., et al. A generalist agent. _arXiv preprint arXiv:2205.06175_, 2022.\n' +
      '* [43] Shah, D., Equi, M. R., Osinski, B., Xia, F., Ichter, B., and Levine, S. Navigation with large language models: Semantic guesswork as a heuristic for planning. In _Conference on Robot Learning_, pp. 2683-2699. PMLR, 2023.\n' +
      '* [44] Shah, D., Osinski, B., Levine, S., et al. Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. In _Conference on Robot Learning_, pp. 492-504. PMLR, 2023.\n' +
      '* [45] Shridhar, M., Manuelli, L., and Fox, D. Cliport: What and where pathways for robotic manipulation. In _Conference on Robot Learning_, pp. 894-906. PMLR, 2022.\n' +
      '* [46] Shredritski, A., Rupprecht, C., and Vedaldi, A. What does clip know about a red circle? visual prompt engineering for vlms. _arXiv preprint arXiv:2304.06712_, 2023.\n' +
      '* [47] Silver, T., Dan, S., Srinivas, K., Tenenbaum, J. B., Kaelbling, L. P., and Katz, M. Generalized planning in pddl domains with pretrained large language models. _arXiv preprint arXiv:2305.11014_, 2023.\n' +
      '* [48] Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D., Thomason, J., and Garg, A. Progprompt: Generating situated robot task plans using large language models. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pp. 11523-11530. IEEE, 2023.\n' +
      '* [49] Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An open-ended embodied agent with large language models. _arXiv preprint arXiv:2305.16291_, 2023.\n' +
      '* [50] Wang, Y.-J., Zhang, B., Chen, J., and Sreenath, K. Prompt a robot to walk with large language models. _arXiv preprint arXiv:2309.09969_, 2023.\n' +
      '* [51] Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. _arXiv preprint arXiv:2302.01560_, 2023.\n' +
      '* [52] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.\n' +
      '* [53] Wen, L., Yang, X., Fu, D., Wang, X., Cai, P., Li, X., Ma, T., Li, Y., Xu, L., Shang, D., et al. On the road with gpt-4v (sision): Early explorations of visual-language model on autonomous driving. _arXiv preprint arXiv:2311.05332_, 2023.\n' +
      '* [54] Wu, J., Antonova, R., Kan, A., Lepert, M., Zeng, A., Song, S., Bohg, J., Rusinkiewicz, S., and Funkhouser, T. Tidybot: Personalized robot assistance with large language models. _arXiv preprint arXiv:2305.05658_, 2023.\n' +
      '* [55] Xu, H., Chen, Y., Du, Y., Shao, N., Wang, Y., Li, H., and Yang, Z. Gps: Genetic prompt search for efficient few-shot learning. _arXiv preprint arXiv:2210.17041_, 2022.\n' +
      '* [56] Xu, J., Zhou, X., Yan, S., Gu, X., Arnab, A., Sun, C., Wang, X., and Schmid, C. Pixel aligned language models. _arXiv preprint arXiv:2312.09237_, 2023.\n' +
      '* [57] Yan, A., Yang, Z., Zhu, W., Lin, K., Li, L., Wang, J., Yang, J., Zhong, Y., McAuley, J., Gao, J., et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. _arXiv preprint arXiv:2311.07562_, 2023.\n' +
      '* [58] Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., and Chen, X. Large language models as optimizers. _arXiv preprint arXiv:2309.03409_, 2023.\n' +
      '* [59] Yang, J., Zhang, H., Li, F., Zou, X., Li, C., and Gao, J. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. _arXiv preprint arXiv:2310.11441_, 2023.\n' +
      '* [60] Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and Wang, L. The dawn of lmms: Preliminary explorations with gpt-4v (sision). _arXiv preprint arXiv:2309.17421_, 9(1):1, 2023.\n' +
      '\n' +
      '* [61] Yu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L. Modeling context in referring expressions. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14_, pp. 69-85. Springer, 2016.\n' +
      '* [62] Yu, W., Gileadi, N., Fu, C., Kirmani, S., Lee, K.-H., Arenas, M. G., Chiang, H.-T. L., Erez, T., Hasenclever, L., Humplik, J., et al. Language to rewards for robotic skill synthesis. _arXiv preprint arXiv:2306.08647_, 2023.\n' +
      '* [63] Zeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M., Armstrong, T., Krasin, I., Duong, D., Sindhwani, V., et al. Transporter networks: Rearranging the visual world for robotic manipulation. In _Conference on Robot Learning_, pp. 726-747. PMLR, 2021.\n' +
      '* [64] Zeng, A., Attarian, M., Ichter, B., Choromanski, K., Wong, A., Welker, S., Tombari, F., Purohit, A., Ryoo, M., Sindhwani, V., et al. Socratic models: Composing zero-shot multimodal reasoning with language. _arXiv preprint arXiv:2204.00598_, 2022.\n' +
      '* [65] Zheng, B., Gou, B., Kil, J., Sun, H., and Su, Y. Gpt-4v (sion) is a generalist web agent, if grounded. _arXiv preprint arXiv:2401.01614_, 2024.\n' +
      '* [66] Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large language models are human-level prompt engineers. _arXiv preprint arXiv:2211.01910_, 2022.\n' +
      '\n' +
      '## 부록 A 로봇 실시예\n' +
      '\n' +
      '**모바일 매니퓰레이터 내비게이션.** 도 3의 (a)에 도시된 바와 같이, 우리는 내비게이션 작업을 위해 모바일 매니퓰레이터 플랫폼을 사용한다. 고정 헤드 카메라의 이미지를 사용하고 이미지의 하단 중심에서 유래한 화살표로 이미지에 주석을 달아서 2D 액션 공간을 표현합니다. PIVOT는 픽셀 공간에서 후보 행동을 식별한 후, 로봇에서 온보드 깊이 카메라를 사용하여 3D 표적 위치에 매핑하고 로봇이 목표물을 향해 이동하도록 명령한다(최대 거리 1.0m). 실제 로봇과 오프라인 데이터 세트 모두에서 PIVOT를 평가한다. 실제 로봇 평가를 위해 관심 객체(예: 사과 찾기) 또는 간접 지시(예: 낮잠을 잘 장소를 찾기)를 통해 로봇이 지정된 목표 위치에 도달할 것으로 예상되는 4가지 시나리오를 설계했다. 오프라인 평가를 위해 지면 진리 표적이 라벨링된 이전 로봇 항법 데이터에서 60개의 예제 데이터 세트를 만들었다. 작업 및 데이터 세트에 대한 자세한 내용은 부록 B에서 확인할 수 있습니다.\n' +
      '\n' +
      '**모바일 매니퓰레이터 조작.** 도 3의 (b)에 도시된 바와 같이, 우리는 조작 작업을 위해 모바일 매니퓰레이터 플랫폼을 사용한다. 고정 헤드 카메라의 영상을 이용하여 카메라 프레임의 엔드 이펙터에서 생성된 화살표로 영상에 주석을 달았는데, 이때 각 화살표는 3차원 상대 데카르트 엔드 이펙터 위치((x,y,z))를 나타낸다. z 차원 높이를 처리하기 위해 두 가지 설정을 연구합니다. 높이는 색상 등급(빨간색에서 파란색 스펙트럼)을 통해 표시되는 설정과 암이 고정된 높이 작업만 사용하는 설정입니다. 그리퍼 닫기 동작은 시각적 주석으로 표시되지 않고 텍스트 프롬프트를 통해 표시됩니다. 엔드 이펙터는 회전 자유도가 있지만 4.6절에서 설명한 대로 시각적 프롬프트로 표현하기 어렵기 때문에 이를 수정하고 실제 로봇과 오프라인 데이터 세트 모두에서 PIVOT를 평가한다. 실제 로봇 평가를 위해 의미 추론과 동작 추론을 결합해야 하는 세 가지 테이블탑 조작 작업에 대해 연구한다. 성공 기준은 성공에 도달하는 이진 객체, 성공적인 궤적 도달을 위해 취한 단계 수, 적용 가능한 경우 성공 파악으로 구성된다. 오프라인 평가를 위해 RT-X 모바일 매니퓰레이터 데이터 세트의 시연 데이터를 사용한다[38]. 대부분의 오프라인 평가를 위해 픽 데모 10회, 상호 작용을 위해 데모 근처 이동 30회를 샘플링했으며 결과에 대한 자세한 내용은 부록 D 섹션에서 확인할 수 있다.\n' +
      '\n' +
      '**Franka.** 도 3의 (c)에 도시된 바와 같이, 우리는 조작을 위해 Franka를 사용한다. 손목 장착된 카메라에서 얻은 이미지를 사용하여 카메라 프레임의 중심에서 유래한 화살표로 이미지에 주석을 달며, 각 화살표는 빨간색에서 파란색까지의 색상 스펙트럼으로 \\(x,y,z,\\) 차원이 캡처되는 3D 상대 데카르트 엔드 이펙터 위치 \\((x,y,z,\\))를 나타낸다. 각 태스크에 대해 5개의 객체가 있는 선택 태스크와 배치 태스크를 모두 검토한다. 결과에 대한 자세한 내용은 부록 F 절에서 확인할 수 있다.\n' +
      '\n' +
      '**RAVENS[63].** 도 3의 (d)를 참조하면, 픽 앤 플레이스 조작을 위해 RAVENS 시뮬레이션 도메인을 사용한다. 우리는 오버헤드 카메라의 이미지를 사용하고 Zeng 등의 액션 표현에 따라 픽 앤 플레이스 위치로 이미지에 주석을 달았다[63]. 이 액션 공간은 우리가 더 높은 수준의 액션 표현을 평가할 수 있게 해준다. 결과에 대한 자세한 내용은 부록 E 절에서 확인할 수 있다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '표에서 볼 수 있듯이, VLM 출력을 견고히 하기 위해 병렬 호출을 사용함으로써 우리는 한 번만(0 병렬) VLM을 실행하고 여러 번 반복할 때 PIVOT를 실행하는 것보다 상당한 개선이 또한 작업의 정확도를 향상시킨다. 그러나, 병렬 호출 또는 반복 횟수를 더 증가시키는 것은 현저하게 더 나은 성능을 달성하지 못했다.\n' +
      '\n' +
      '우리는 이미지 공간의 이유와 이미지 주석의 이유인 제안된 접근 방식을 주석이 달린 이미지가 없는 텍스트의 추론과 비교했다. 이 텍스트 기반 베이스라인에서, 우리는 동일한 이미지 및 내비게이션 쿼리를 VLM에 제공하지만, 우리는 VLM에 이미지가 동일한 크기의 영역의 3행 3열로 분할된다고 상상하고 그 영역들 중 하나의 영역(예를 들어, "왼쪽 상단", "중간 하단")의 이름을 출력하도록 요청한다. 그런 다음 선택된 영역의 중심과 지면 진리 목표점 사이의 거리를 계산한다. 이 텍스트 기준선과 반복 최적화를 수행하지 않는다는 점을 감안할 때 1회 반복 및 0병렬로 PIVOT와 결과를 비교한다. 표 4의 결과를 참조하십시오. GPT-4V의 경우 텍스트 기준선은 모든 작업에 걸쳐 더 높은 평균 및 표준 편차를 발생시킵니다.\n' +
      '\n' +
      '## 부록 C 모바일 매니퓰레이터 조작 온라인 평가\n' +
      '\n' +
      '4.2절에 설명된 실제 조작 전문가에 대한 정량적 평가 시험 외에도 그림 11에서 추가 평가 롤아웃을 보여준다. 정성적으로 PIVOT는 불완전한 깊이 인식 또는 행동 정밀도 도전으로 인해 발생할 수 있는 것과 같은 행동 예측의 부정확성에서 회복할 수 있음을 발견했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Method & In-View & Semantic & Out-of-View \\\\ \\hline Image & \\(0.21\\pm 0.002\\) & \\(0.23\\pm 0.012\\) & \\(0.44\\pm 0.04\\) \\\\ Text & \\(0.26\\pm 0.15\\) & \\(0.35\\pm 0.14\\) & \\(0.46\\pm 0.31\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 이미지 주석을 사용한 추론 대. 텍스트를 사용하여 L2 손실로 측정한 오프라인 평가입니다.\n' +
      '\n' +
      '## 부록 D 모바일 매니퓰레이터 조작 오프라인 평가\n' +
      '\n' +
      '섹션 A에 설명된 오프라인 모바일 조작기 데이터 세트를 사용하여 본 명세서에서 텍스트 프롬프트를 추가로 삭제한다. 그림 13에서 우리는 제로 샷과 소 샷 프롬프트의 성능과 사고 사슬[52] 및 직접 프롬프트의 성능을 고려한다. 우리는 일반적으로 제로 샷 사고 체인이 가장 잘 수행되지만 소수의 샷 직접 프롬프트가 유사하게 수행되며 훨씬 더 토큰 효율적이라는 것을 발견했다. 그림 14에서 우리는 프롬프트의 순서가 성능에 미치는 영향을 고려한다. 프롬프트의 구별되는 요소는 프리앰블(하이 레벨 목표를 기술함), 작업(로봇이 수행하려고 시도하는 특정 작업을 기술함), 및 이미지이다. 이러한 프롬프트의 예는 부록 D에서 볼 수 있다. 우리는 순서들 간의 성능의 작은 변화를 발견하며, 프리앰블, 이미지 및 태스크가 가장 높은 성능을 초래한다. 우리는 이 순서가 훈련 혼합물을 가장 밀접하게 반영한다고 가정한다.\n' +
      '\n' +
      '그림 1에 설명된 방법의 한계를 설명하기 위해. 9 더 나은, 우리는 그림에서 모바일 조작기 조작의 두 가지 에피소드를 오프라인 평가로 시각화한다. 12. 그림에 따르면, 어디로 이동해야 하는지가 분명한 에피소드의 시작 부분에서, 우리의 방법은 정확한 예측을 생성하는 경향이 있는 반면, 상호작용이 있는 에피소드 중간에, 우리의 방법은 올바른 행동을 생성하기 위해 고군분투한다.\n' +
      '\n' +
      '## 부록 E RAVENS 온라인 시뮬레이션 평가\n' +
      '\n' +
      '우리는 로봇이 지정된 과일을 따서 지정된 그릇에 넣어야 하는 일련의 평가 작업을 만듭니다. 장면에 과일 세 개(바나나, 딸기, 배)와 색깔이 다른 그릇 세 개(파란색, 녹색, 노란색)가 있다. 각 태스크는 "{fruit}를 골라 {color} bowl에 배치"하는 형태를 취한다. 태스크 목표가 주어지면, 소스 오브젝트와 타겟 오브젝트를 파싱하고, 독립적으로 VLM에 이 두 오브젝트에 대응하는 픽 및 플레이스 위치를 각각 획득하도록 프롬프트한다. 사용하는 프롬프트는 부록 H를 참조하십시오. 그림 15에서 우리는 5개의 무작위 인스턴스에 대한 평가를 보고한다. 여기에서 우리는 시각적 프롬프트의 각 반복에 걸쳐 지상 진리 선택 및 장소 위치와 관련된 오류를 구체적으로 보고한다. 우리는 오류가 일반적으로 처음 몇 번의 반복에서 감소하고 결국 수렴한다는 것을 안다. 대부분의 설정에서 선택한 선택 및 위치 위치가 원하는 개체에 가깝지만 VLM은 한 작업에서 작업을 성공적으로 실행할 수 있는 점을 정확하게 선택할 수 있는 기능이 부족합니다.\n' +
      '\n' +
      '그림 12: 모바일 조작기 조작 오프라인 평가의 두 에피소드. 이 방법은 화살표 주석에 따라 합리적인 작업을 생성할 수 있음을 보여준다.\n' +
      '\n' +
      '그림 13: 소수 샷 대 소수 샷의 절제. 0-shot and CoT vs. 조작 도메인에 대한 직접 성능. 가장 성능이 좋은 조합은 제로샷 CoT입니다. 그러나 직접 모델은 훨씬 적은 출력 토큰으로 유사한 성능을 달성할 수 있으므로 토큰 효율이 더 높다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:21]\n' +
      '\n' +
      '## 부록 G 시각 주석 민감도\n' +
      '\n' +
      '시각적 주석을 이해하는 데 있어 현대 VLM의 흥미로운 편향과 한계를 찾는 이전 작업에서 영감을 받아 다양한 유형의 화살표 주석을 이해하는 최첨단 VLM의 능력을 분석한다[46, 59, 60]. 흰색 배경에 오버레이된 다양한 스타일의 CV2[24] 화살표의 장난감 데이터 세트 하나와 실제 로봇 공학 장면에 오버레이된 다양한 스타일의 객체 참조 화살표의 보다 현실적인 데이터 세트 두 개의 합성 데이터 세트를 생성한다. 데이터 세트는 화살표 색상, 화살표 두께 및 상대 화살촉 크기와 같은 매개변수를 조정합니다. 첫 번째 데이터세트에서는 VLM을 질의하여 화살표의 방향을 분류하며, 이는 스타일러스가 VLM의 절대 화살표 방향을 이해하는 능력에 미치는 영향을 연구한다. 두 번째 데이터세트에서는 VLM을 질의하여 복수의 객체 중 지정된 객체를 가리키는 화살표를 선택하고, VLM은 스타일러스가 VLM의 상대적 화살표 방향과 객체 중심 화살표 방향을 이해하는 능력에 미치는 영향을 연구한다. 두 번째 데이터 세트에는 다양한 객체가 있는 장면이 포함되어 있으며, "Easy"(플레이트, 상자, 큐브), "Medium"(컵, 가방, 머그잔), "Hard"(행거, 장난감), "Very Hard"(브러시, 편심 객체)로 분류한다.\n' +
      '\n' +
      '도 11 | 시맨틱 추론과 액션 이해의 조합이 요구되는 현실 세계 모바일 조작기 테이블탑 조작 시나리오 상에서 PIVOT를 평가하는 것. 실제 모바일 매니퓰레이터에서 3번의 최적화 반복을 사용하여 (a) "오렌지를 움직여 과일로 대표되는 웃는 얼굴을 완성한다", (b) "마커를 사용하여 파란색 도로를 따라 선을 추적한다", (c) "올바른 종이에 들고 있는 물체를 정렬한다"에 대한 유망한 성공을 본다.\n' +
      '\n' +
      '도 14 | 모바일 조작 도메인 상의 프리앰블, 이미지 및 태스크의 순서의 제거. 우리는 효과가 미미하지만 이미지를 프롬프트의 끝에 더 가깝게 두는 것이 유익하다는 것을 발견했다. P, I, T는 프리앰블을 의미하고, 이미지 및 태스크 디스크립션이 뒤따르는 것을 의미하고, I, P, T는 이미지 및 프리앰블 및 태스크 디스크립션이 뒤따르는 것을 의미한다.\n' +
      '\n' +
      '도 11 | 시맨틱 추론과 액션 이해의 조합이 요구되는 현실 세계 모바일 조작기 테이블탑 조작 시나리오 상에서 PIVOT를 평가하는 것. 실제 모바일 매니퓰레이터에서 3번의 최적화 반복을 사용하여 (a) "오렌지를 움직여 과일로 대표되는 웃는 얼굴을 완성한다", (b) "마커를 사용하여 파란색 도로를 따라 선을 추적한다", (c) "올바른 종이에 들고 있는 물체를 정렬한다"에 대한 유망한 성공을 본다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:24]\n' +
      '\n' +
      '도 17 | 시각적 주석 화살표 스타일을 이해하기 위해 VLM의 견고성을 연구하는 절차적으로 생성된 데이터 세트의 예. (a) 공백 배경에서 단일 화살표의 절대 방향 이해에 초점을 맞춘다. (b) 현실적 장면에서의 객체-상대 화살표 이해에 초점을 맞춘다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c|c c c|c c c} \\hline \\hline  & \\multicolumn{3}{c}{Arrow Thickness} & \\multicolumn{3}{c}{Arrowhead Size} & \\multicolumn{3}{c}{Target Object} \\\\ \\cline{2-10} Color & 2 & 4 & 6 & 0.1 & 0.3 & 0.5 & Easy & Medium & Hard & Very Hard \\\\ \\hline red & 42\\% & 33\\% & 33\\% & 50\\% & 33\\% & 25\\% & 44\\% & 100\\% & 0\\% & 0\\% \\\\ orange & 25\\% & 25\\% & 25\\% & 25\\% & 25\\% & 25\\% & 0\\% & 100\\% & 0\\% & 0\\% \\\\ yellow & 67\\% & 58\\% & 50\\% & 83\\% & 58\\% & 33\\% & 100\\% & 33\\% & 56\\% & 44\\% \\\\ green & 50\\% & 58\\% & 50\\% & 83\\% & 58\\% & 33\\% & 100\\% & 33\\% & 56\\% & 44\\% \\\\ blue & 42\\% & 36\\% & 33\\% & 36\\% & 50\\% & 25\\% & 100\\% & 33\\% & 22\\% & 0\\% \\\\ purple & 33\\% & 50\\% & 50\\% & 58\\% & 58\\% & 17\\% & 89\\% & 22\\% & 56\\% & 11\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 객체-참조 화살표 데이터세트 상의 VLM들의 시각적 주석 화살표 견고성. 두께, 크기 및 절대 방향이 다른 다양한 색상의 화살표에 대해 지정된 객체를 가리키는 화살표를 올바르게 선택할 때 GPT-4V의 견고성을 평가한다.\n' +
      '\n' +
      '그림 17: 시각적 주석 화살표 스타일을 이해하기 위해 VLM의 견고성을 연구하는 절차적으로 생성된 데이터 세트의 예. (a) 공백 배경에서 단일 화살표의 절대 방향 이해에 초점을 맞춘다. (b) 현실적 장면에서의 객체-상대 화살표 이해에 초점을 맞춘다.\n' +
      '\n' +
      '## 부록 H 프롬프트\n' +
      '\n' +
      '### RefCOCO prompt\n' +
      '\n' +
      '당신의 목표는 이 장면에서 OBJECT를 찾는 것입니다. 나는 번호가 매겨진 원으로 이미지에 주석을 달았다. OBJECT와 가장 겹치는 숫자 3개를 선택합니다. 중첩된 점이 없는 경우 점을 선택하지 마십시오. 당신은 이 게임에서 5번째 세계 챔피언이다. 왜 그 점들을 선택했는지에 대한 한 문장 분석을 해라. 이 형식의 json 파일로 끝에 답변을 제공합니다. {"포인트": []}\n' +
      '\n' +
      '### Navigation prompt\n' +
      '\n' +
      '나는 물체를 넘어갈 수 없는 바퀴 달린 로봇이다. 이게 바로 내가 지금 보고 있는 이미지야. 나는 번호를 매긴 원으로 그것에 주석을 달았다. 각 숫자는 내가 따를 수 있는 일반적인 방향을 나타낸다. 이제 당신은 세계 5대 챔피언의 항해사이고 당신의 임무는 내가 어떤 원을 선택해야 하는지 말해주는 것이다. {INSTRUCTION} {X} 최상의 후보 번호를 선택합니다. 객체를 통과하는 경로를 선택하지 마십시오. 분석을 건너뛰고 끝에 있는 답변을 이 형식의 json 파일로 제공합니다. {"포인트": []}\n' +
      '\n' +
      '### RAVENS prompt\n' +
      '\n' +
      '{OBJECT}에 가장 가까운 숫자 마커는? 최종 답변을 \'최종 답변\'으로 추론하고 가장 가까운 마커 번호 목록이 뒤따른다.\n' +
      '\n' +
      '### 조작 온라인 평가 프롬프트\n' +
      '\n' +
      '#### Direct\n' +
      '\n' +
      '로봇은 몇 개의 화살표를 따라 작업해야 하는가?\n' +
      '\n' +
      '규칙: - 물건을 정리하려고 하는 책상 앞에 있는 로봇의 이미지를 보고 있습니다. 로봇에는 노란색 손가락이 있는 팔과 그리퍼가 있습니다. - 이미지의 화살표는 로봇이 취할 수 있는 동작을 나타낸다. - 적색 화살표는 팔을 카메라로부터 더 멀리 이동시키고, 청색 화살표는 팔을 카메라 쪽으로 더 가깝게 이동시킨다. - 더 작은 원들은 카메라로부터 더 멀리 떨어져 있고, 따라서 팔을 더 멀리 이동하고, 더 큰 원들은 더 가깝고 따라서 팔을 뒤로 이동시킨다. - 로봇은 로봇 그리퍼가 물체에 근접하고 그리퍼 손가락이 물체를 안정적으로 둘러싸는 경우에만 물체를 잡거나 이동할 수 있음 - 즉각적인 다음 동작을 나타내는 후보 화살표 리스트(0.3초)로 답을 끝내야 한다. 즉각적인 다음 단계 사이에 미래의 조치를 고려하지 마십시오. - 다수의 화살표가 취할 양호한 즉각적인 조치를 나타내는 경우, 최악에서 최고로 순위가 매겨진 모든 후보를 복귀시킨다. - 경험상 일반적인 규칙은 1-4명의 후보를 반환하는 것이다. 지시: 과제를 통해 먼저 추론하고 마지막에 올바른 행동 선택(들)을 "화살표: [<숫자>, <숫자]" 형식으로 요약한다. 과제: 과제\n' +
      '\n' +
      '### 조작 오프라인 평가 프롬프트\n' +
      '요약: 화살표는 로봇이 취할 수 있는 작업입니다. 빨간색은 팔을 앞으로 (카메라에서 멀리) 이동시키고, 파란색은 팔을 뒤로 (카메라 쪽으로) 이동시킨다. 더 작은 원들은 카메라로부터 더 멀리 떨어져서 팔을 앞으로 이동시키고, 더 큰 원들은 더 가깝고 따라서 팔을 뒤로 이동시킨다. 다른 어떤 것도 출력하지 마세요, 형식에 대한 직접 답변, 화살표: [<number>, <number> 등]. IMG, Task: 로봇이 흰색 코트 걸이를 고르기 위해 따르는 가장 좋은 화살표는 무엇인가요?\n' +
      '\n' +
      'CoT\n' +
      '\n' +
      '요약: 화살표는 로봇이 취할 수 있는 작업입니다. 작업을 통한 이유 먼저 그리고 마지막에 올바른 액션 선택(들)을 포맷, 화살표: [<숫자>, <숫자> 등으로 요약한다. 설명: 로봇은 그리퍼가 물체 주위에 있고 물체 위에 닫혀 있는 경우에만 물체를 잡거나 이동할 수 있다. 빨간색은 팔을 앞으로 (카메라에서 멀리) 이동시키고, 파란색은 팔을 뒤로 (카메라 쪽으로) 이동시킨다. 더 작은 원들은 카메라로부터 더 멀리 떨어져서 팔을 앞으로 이동시키고, 더 큰 원들은 더 가깝고 따라서 팔을 뒤로 이동시킨다. 이 요약을 포함해야 합니다. IMG, 과제: 로봇이 캣닙 장난감을 고르기 위해 따르는 가장 좋은 화살표는 무엇인가요?\n' +
      '\n' +
      'Few-shot Direct\n' +
      '\n' +
      '요약: (위와 같은) IMG, 태스크: 화이트보드에 있는 글쓰기를 지웁니다. 화살표: [5, 10], IMG, Task: 아이스 커피 캔을 집어라. 화살표: [1], IMG, 과제: 스트링 치즈를 집어라. 화살표: [8, 15, 3, 13], IMG, Task: 흰색 코트 걸이를 선택합니다.\n' +
      '\n' +
      'Few-shot CoT\n' +
      '\n' +
      '요약: (위와 같은) IMG, 태스크: 화이트보드에 있는 글쓰기를 지웁니다. 로봇이 지우개를 들고 있으므로 화이트보드의 마커 위로 이동해야 합니다. 다음 화살표는 유망해 보인다. 5. 이 화살표는 지우개를 쓰기 위로 이동시키고 카메라에서 멀어지게 하여 화이트보드를 향하게 한다. 10. 이 화살표는 또한 지우개를 쓰기 위로 이동하고 훨씬 더 작은 원(및 더 빨간색)을 가지며 따라서 화이트보드를 향해 더 많이 이동한다. 화살표: [5, 10], IMG, Task: 화살표: [5, 10], IMG, Task: 화살표: [8, 15, 3, 13], IMG, Task: 오레오를 고른다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>