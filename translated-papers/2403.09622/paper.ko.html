<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Glyph-ByT5: 정확한 시각적 텍스트 렌더링을 위한 맞춤형 텍스트 인코더\n' +
      '\n' +
      ' Zeyu Liu\\({}^{\\dagger}\\) Weicong Liu\\({}^{\\dagger}\\) Zhanhao Liang\\({}^{\\dagger}\\) Chong Luo Ji Li Gao Huang Yuhui Yuan\\({}^{\\dagger\\ddagger}\\)\n' +
      '\n' +
      '마이크로소프트에서의 코어 기여도\\({}^{\\dagger}\\({}^{\\dagger}\\)프로젝트의 리드\n' +
      '\n' +
      '싱화대학교 북경대학 마이크로소프트 연구\n' +
      '\n' +
      '[https://glyph-byt5.github.io](https://glyph-byt5.github.io)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '시각적 텍스트 렌더링은 텍스트 인코더 결핍에 있는 핵심 문제와 함께 현대 텍스트 대 이미지 생성 모델에 근본적인 문제를 제기한다. 정확한 텍스트 렌더링을 달성하기 위해 텍스트 인코더에 대한 두 가지 중요한 요구 사항인 문자 인식 및 글리프와의 정렬을 식별한다. 우리의 솔루션은 세심하게 선별된 쌍을 이루는 글리프 텍스트 데이터 세트를 사용하여 문자 인식 ByT5 인코더를 미세 조정함으로써 일련의 맞춤형 텍스트 인코더인 글리프-ByT5를 만드는 것을 포함한다. Glyph-ByT5를 SDXL과 통합하는 효과적인 방법을 제시한다.\n' +
      '\n' +
      '그림 1: 자동 다중 라인 레이아웃 계획(1\\({}^{\\text{st}\\) 행), 텍스트가 풍부한 디자인 이미지(2\\({}^{\\text{nd}\\) 행), 장면 텍스트가 있는 오픈 도메인 이미지(3\\({}^{\\text{rd}\\) 행)를 사용하여 문단 렌더링 기능을 보여준다.\n' +
      '\n' +
      '를 포함하는 것을 특징으로 하는 디자인 이미지 생성을 위한 Glyph-SDXL 모델 생성 방법. 이는 본 논문에서 제안한 디자인 이미지 벤치마크에서 텍스트 렌더링 정확도를 크게 향상시켜 \\(20\\%\\)이하에서 거의 \\(90\\%\\)으로 개선한다. 주목할 만한 것은 글립-SDXL의 텍스트 단락 렌더링에 대한 새로운 발견 능력으로 자동 다중 라인 레이아웃으로 수십에서 수백 개의 문자에 대한 높은 철자 정확도를 달성한다는 것이다. 마지막으로, 시각적 텍스트를 특징으로 하는 고품질, 사실적 이미지의 작은 집합으로 글리프-SDXL을 미세 조정함으로써, 오픈 도메인 실제 이미지에서 장면 텍스트 렌더링 능력의 상당한 향상을 보여준다. 이러한 강력한 결과는 다양하고 도전적인 작업을 위해 맞춤형 텍스트 인코더를 설계할 때 추가 탐구를 권장하는 것을 목표로 한다._\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '확산 모델은 이미지 생성을 위한 주요 접근법으로 등장했다. DALL-E3[3, 19] 및 Stable Diffusion 시리즈[22, 24]와 같이 주목할 만한 기여는 사용자 프롬프트에 응답하여 고품질 이미지를 생성하는 데 현저한 숙련도를 보여준다. 그러나, 다양한 이미지 생성 애플리케이션에서 중요한 요소인 시각적 텍스트를 정확하게 렌더링하는 능력에 상당한 제한이 지속된다. 이러한 응용 프로그램은 포스터, 카드 및 브로셔에 대한 디자인 이미지를 생산하는 것부터 도로 표지판, 광고판 또는 텍스트가 들어 있는 티셔츠에서 발견되는 장면 텍스트를 특징으로 하는 실제 이미지를 합성하는 것까지 다양하다. 정확한 텍스트 렌더링 정확도를 달성하는 과제는 이러한 중요한 도메인에서 이미지 생성 모델의 실제 배치를 방해했다.\n' +
      '\n' +
      '시각적 텍스트 렌더링 성능을 저해하는 주요 과제는 텍스트 인코더의 한계에 있다고 가정한다. 시각적 신호와 정렬하도록 훈련된 널리 사용되는 CLIP 텍스트 인코더는 주로 이미지 세부 사항을 파헤치기보다는 이미지 개념을 파악하는 데 중점을 둔다. 반대로, 언어에 대한 포괄적인 이해를 위해 설계된 일반적으로 채택되는 T5 텍스트 인코더는 시각적 신호와의 정렬이 부족하다. 우리는 문자 수준의 정보를 인코딩하고 시각적 텍스트 신호 또는 글리프와 정렬할 수 있는 텍스트 인코더가 시각적 텍스트 렌더링에서 높은 정확도를 달성하기 위해 필수적이라고 주장한다. 캐릭터 인식 ByT5 인코더[16]에서 영감을 끌어내는 우리의 접근법은 시각적 텍스트 또는 글리프와 더 잘 정렬되도록 사용자 정의하는 것을 목표로 한다.\n' +
      '\n' +
      '원하는 문자 인식 및 글리프 정렬 텍스트 인코더를 구성하기 위해 쌍을 이루는 텍스트-글리프 데이터를 사용하여 ByT5 모델에 기반한 미세 조정 접근법을 사용한다. 주요 과제는 그래픽 렌더링을 기반으로 사실상 무제한 페어링 데이터를 생성할 수 있는 확장 가능한 파이프라인을 구축함으로써 극복하는 고품질 페어링 텍스트 글리프 데이터의 부족에서 발생한다. 또한, [16]에서 논의된 바와 같이 시각적 텍스트 렌더링에서 흔히 발생하는 다양한 오류 유형을 다루는 텍스트 인코더의 문자 인식을 향상시키기 위해 글리프 증강 전략을 통합한다. 세심하게 조작된 데이터 세트를 활용하고 혁신적인 상자 수준의 대조 손실을 사용하여 ByT5를 글리프 생성을 위한 일련의 맞춤형 텍스트 인코더인 글리프-ByT5로 효율적으로 미세 조정한다.\n' +
      '\n' +
      '철저한 훈련 시, Glyph-ByT5는 효율적인 영역별 교차 주의 메커니즘을 사용하여 SDXL 모델에 매끄럽게 통합되어 원래 확산 모델의 텍스트 렌더링 성능을 크게 향상시킨다. 그 결과 Glyph-SDXL 모델은 표 1과 같이 텍스트가 풍부한 디자인 이미지 생성에서 다른 최신 모델보다 뛰어난 철자 정확도를 보여주며, 제한된 장면 텍스트 이미지 세트를 사용하여 Glyph-SDXL을 미세 조정함으로써 장면 텍스트 이미지 생성의 숙련도를 크게 강화했다. 예제는 그림 1에 나와 있다. 도 1은 정제된 모델이 원래 모델의 이미지 생성 능력에서 지각 가능한 저하 없이 텍스트 단락을 장면 텍스트로 능숙하게 렌더링한다는 것을 입증한다.\n' +
      '\n' +
      '본 연구는 맞춤형 텍스트 인코더의 학습과 적절한 정보 주입 메커니즘의 구현을 통해 오픈 도메인 이미지 생성기를 뛰어난 시각적 텍스트 렌더러로 변환할 수 있음을 보여준다. 수십 자에서 수백 자 사이의 텍스트 단락을 제시하면, 우리의 미세 조정 확산 모델은 다중 라인 레이아웃의 완전 자동화된 처리로 지정된 영역 내에서 렌더링을 위한 높은 철자 정확도를 달성한다. 본질적으로 이 작업은 세 가지 뚜렷하지만 보완적인 방식으로 기여한다. 먼저 문자 인식, 글리프 정렬 텍스트 인코더, 글리프를 학습합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c c c c} \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{**HPperm**} & \\multirow{2}{*}{**Size-aware**} & \\multirow{2}{*}{**Glyph-aligned**} & \\multicolumn{3}{c}{**Precision (\\%)**} \\\\ \\cline{5-8}  & & & \\(\\pm\\)20.0mm \\(\\pm\\)50.0mm \\(\\pm\\)50.0mm \\(\\pm\\)50.0mm \\(\\pm\\)100.0mm \\\\ \\hline SDXL & \\(\\pm\\)40.0mm \\(\\pm\\)40.0mm & 8174 & ✗ & 81.72 & 20.98 & 18.23 & 19.17 \\\\ + TS-L & \\(\\pm\\)30.000 & ✗ & ✗ & 48.46 & 44.90 & 34.56 & 26.09 \\\\ + HyT5-S & \\(\\pm\\)20.000 & ✓ & ✗ & 60.52 & 52.79 & 50.11 & 42.05 \\\\ + Glyph-byT5-S & \\(\\pm\\)20.000 & ✓ & ✓ & 92.56 & 90.38 & 87.16 & 83.17 \\\\ + Glyph-byT5-S & \\(\\pm\\)20.000 & ✓ & ✓ & **93.89** & **93.67** & **91.45** & **89.17** \\\\ \\hline DMPylB & \\(\\pm\\)30.000 & 4.18 & ✗ & 17.63 & 17.17 & 16.42 & 13.00 \\\\ DALL-E3 & \\(\\pm\\)40.000 & ✗ & ✗ & 23.23 & 21.59 & 20.1 & 15.81 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 다양한 수의 문자에 걸쳐 SDXL을 기반으로 한 접근법으로 달성된 개선된 결과를 보여줌으로써 비교적 공정한 비교를 위해 T5-Large 및 ByT5-Small의 인코더를 선택한다. 텍스트 인코더 구성 요소에 대한 매개 변수의 수만 두 번째 열에 표시합니다. 성능은 서로 다른 텍스트 길이 범위에서 각 모델의 단어 수준 정밀도 평가를 통해 입증된다. _ Char-aware_: 문자 인식 텍스트 인코더를 사용함 _ Glyph-align_: glyph-alignment pre-training. 또한 각 문자 번호 범위 내에서 250개의 프롬프트가 있는 1,000개의 프롬프트로 구성된 벤치마크에서 DeepFloyd-IF 및 DALL-E3의 성능을 보고한다. 기본적으로 단어 수준에서 모든 정밀도 점수를 계산합니다. 위 첨자 \'1M\'은 100만 개의 훈련 쌍을 사용하는 반면 앞의 4개의 행은 기본적으로 500K를 사용한다.\n' +
      '\n' +
      'ByT5는 정확한 시각적 텍스트 렌더링 문제의 핵심 해결책이다. 둘째, 효율적인 영역별 교차 주의 메커니즘을 통해 글리프-ByT5를 SDXL에 통합하는 강력한 설계 이미지 생성기인 글리프-SDXL의 구조와 훈련에 대해 자세히 설명한다. 마지막으로 글리프-SDXL을 장면 텍스트 이미지 생성기로 미세 조정할 수 있는 가능성을 보여주며, 뛰어난 시각적 텍스트 렌더링 기능을 갖춘 포괄적인 오픈 도메인 이미지 생성기를 개발할 수 있는 기반을 마련한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '시각 텍스트 렌더링\n' +
      '\n' +
      '읽을 수 있고 시각적으로 일관된 텍스트를 렌더링하는 것은 확산 기반 이미지 생성 모델에 대해 잘 알려져 있는 제한과 중요한 문제를 제기한다. Stable Diffusion 3[10] 및 Ideogram 1.01과 같은 특정 현대 오픈 도메인 이미지 생성 모델은 시각적 텍스트 렌더링 성능을 향상시키기 위해 상당한 노력을 기울였다는 점에 주목할 필요가 있다. 그러나 렌더링된 텍스트의 철자 정확도는 여전히 만족스럽지 못하다. 반대로, GlyphControl, GlyphDraw 및 TextDiffuser 시리즈[6, 7, 16, 17, 29]와 같은 시각적 텍스트 렌더링에 초점을 맞춘 노력이 있었다. 이러한 노력이 철자 정확도에서 상당한 개선을 보여주었지만, 여전히 약 20자 미만의 단일 단어 또는 텍스트 라인을 렌더링하는 데 집중하고 있다는 점에 주목하는 것은 실망스럽다. 본 연구에서는 정확한 시각적 텍스트 렌더링 문제, 특히 100자 이상의 텍스트 콘텐츠를 다룰 때 이 영역에서 야심찬 목표를 제시하는 것을 목표로 한다.\n' +
      '\n' +
      '각주 1: [https://about.ideogram.ai/1.0](https://about.ideogram.ai/1.0)\n' +
      '\n' +
      '맞춤형 텍스트 인코더\n' +
      '\n' +
      '텍스트 지향 확산 모델을 훈련시키고, 상이한 방식으로 독창적인 CLIP 인코더를 맞춤화된 텍스트 인코더로 교체 또는 증강하기 위한 몇 가지 최근의 노력[5, 12, 32]이 행해지고 있다. 그러나 이러한 방법은 이전 방법과 마찬가지로 특정 길이의 텍스트 시퀀스를 처리하는 것으로 제한되며 UDiffText[32]는 12자 이하의 시퀀스를 지원한다. 이와는 대조적으로, 우리의 방법론은 100자 이상의 텍스트 시퀀스를 생성하는 능력으로 구별되며, 예외적으로 높은 정확도를 달성하면서 거의 \\(90\\%\\)의 단어 수준 정확도에 도달한다. 이 상당한 진전은 이전 방법의 단점을 해결하여 텍스트 생성 작업에서 더 넓은 적용 가능성과 향상된 성능을 제공한다. 또 다른 밀접하게 관련된 작업은 Counting-aware CLIP[21]이며, 이는 전문화된 이미지-텍스트 카운팅 데이터세트와 카운팅-초점 손실 함수로 원래의 CLIP 텍스트 인코더를 향상시킨다. 그러나 이 접근법의 중요한 한계는 데이터 세트의 확장성이 부족하다는 것이다. 그들은 원본 텍스트 인코더와 확산 모델을 처음부터 교체하고 훈련하기로 선택한 반면, 데이터 구축 파이프라인은 확장 가능하며 효율성을 개선하기 위해 GlyphByT5를 원본 텍스트 인코더와 통합하는 것을 우선시한다.\n' +
      '\n' +
      '**우리의 기여** 우리의 작업은 대부분의 현재 텍스트 대 이미지 생성 모델에서 하나의 중요한 제한이 텍스트 인코더에 있음을 식별하는 이전에 언급한 연구의 통찰력과 일치한다. 우리 작업의 주요 기여는 글리프 렌더링 작업을 체계적으로 해결하기 위한 효과적인 전략을 제시하는 데 있다. 먼저 그래픽 렌더링을 활용하여 확장 가능하고 정확한 글립 텍스트 데이터를 생성하는 것이 고품질의 글립 정렬된 문자 인식 텍스트 인코더를 훈련하는 데 중요하다는 것을 입증한다. 그런 다음 글리프-ByT5 텍스트 인코더와 SDXL에서 사용되는 원본 CLIP 텍스트 인코더를 통합하는 간단하면서도 강력한 방법을 소개한다. 또한, 설계-대-장면 정렬 미세 조정을 수행하여 제안된 방법이 장면 텍스트 생성에 어떻게 적용될 수 있는지 설명한다. 확장성 있는 고품질 데이터에 대해 맞춤형 텍스트 인코더를 교육하는 것이 공간 인식 및 수리 능력과 같은 근본적인 한계를 극복하기 위한 유망한 방법을 나타낼 것으로 기대한다.\n' +
      '\n' +
      '##3. 접근\n' +
      '\n' +
      '맞춤형 글리프 정렬 문자 인식 텍스트 인코더인 글리프-ByT5의 세부 사항을 설명하는 것으로 시작하며, 이는 쌍을 이루는 글리프 이미지와 텍스트 지침의 실질적인 데이터 세트를 사용하여 훈련된다. 다음으로 디자인 텍스트 렌더링 작업을 위한 SDXL 모델과 통합할 때 Glyph-ByT5가 시각적 텍스트 렌더링 정확도를 크게 향상시키는 방법을 보여준다. 마지막으로, 정확한 장면 텍스트 생성을 위한 Glyph-SDXL의 적응을 가능하게 하는 디자인 대 장면 정렬을 위한 간단하면서도 효과적인 접근법을 소개한다.\n' +
      '\n' +
      '### Glyph-ByT5: 디자인 텍스트 생성을 위한 맞춤형 Glyph-Aligned 문자 인식 텍스트 인코더\n' +
      '\n' +
      '텍스트 렌더링의 부정확성에 기여하는 핵심 요소는 현대 확산 모델에서 텍스트 인코더의 고유한 한계, 특히 글리프 이미지의 해석에 관한 것이다. 원래의 CLIP 텍스트 인코더는, 예를 들어, 개념적 레벨에서 광범위한 시각적-언어 의미 정렬을 위해 맞춤화되는 반면, T5/ByT5 텍스트 인코더는 심층 언어 이해에 초점을 맞춘다. 그러나, 최근 연구에서 T5/ByT5 텍스트 인코더가 시각적 텍스트 렌더링 작업에 유리하다는 것을 보여주지만, 글리프 이미지 해석을 위해 명시적으로 미세 조정되지 않는다. 이러한 맞춤형 텍스트 인코더 설계의 부족은 다양한 애플리케이션에서 덜 정확한 텍스트 렌더링을 초래할 수 있다.\n' +
      '\n' +
      '기존의 텍스트 인코더(CLIP 텍스트 인코더 또는 T5/ByT5 텍스트 인코더와 같은)와 글리프 이미지 사이의 격차를 해소하기 위해, 우리는 일련의 글리프 정렬 문자 인식 텍스트 인코더, 즉 글리프-ByT5를 트레이닝하기 위한 혁신적인 글리프 정렬 방법론을 제안한다. 본 연구는 글리프 이미지와 텍스트 사이의 격차를 조정하도록 특별히 설계된 일련의 글리프 인식 텍스트 인코더를 트레이닝하는 데 중점을 둔다. LiT 프레임워크에서 영감을 끌어내는[30], 우리의 전략은 사전 훈련된 이미지 모델을 동결 상태로 유지하면서 텍스트 모델을 독점적으로 미세 조정하는 것을 포함한다. 이 접근법은 텍스트 인코더들이 적응하도록 효과적으로 강요하고, 이미 트레이닝된 이미지 모델로부터 추출된 시각적 글리프 표현들 내에 인코딩된 풍부한 정보를 식별하기 위해 학습한다. 비전 인코더 구성 요소의 경우 사전 훈련된 CLIP 비전 인코더 또는 DINOv2 모델을 선택하여 시각적 데이터를 처리하는 데 고급 기능을 활용합니다. 또한 장면 텍스트 인식 또는 기타 작업에 특별히 맞춤화된 비전 인코더를 사용하는 것의 영향을 탐색하고, 미래의 연구 방법으로 시각적 텍스트 렌더링을 위한 보다 진보된 비전 인코더의 개발 및 훈련을 고려한다.\n' +
      '\n' +
      '*Scalable and Accurate Glyph-Text Dataset** 사용자 정의 글라이프 인식 텍스트 인코더의 학습을 가능하게 하기 위해, 먼저, 대략 백만 쌍의 합성 데이터(\\{\\mathrm{l}_{\\mathrm{glyph},\\mathrm{T}_{\\mathrm{text}\\})로 구성된 \\(\\mathcal{D}\\)으로 표시된 고품질 글라이프 텍스트 데이터 세트를 생성한다. 이 데이터 세트는 [13]에 의해 최근 작업에 도입된 개선된 그래픽 렌더로 개발되었다. 크롤링된 그래픽 디자인 이미지에서 발견되는 원래 타이포그래픽 속성(글꼴 유형, 색상, 크기, 위치 및 기타 포함)을 기반으로 초기 글리프 이미지 세트를 구성한다. 우리는 말뭉치에서 샘플링된 무작위 텍스트로 단어를 대체하여 글리프 이미지 세트를 풍부하게 하는 데 사용할 수 있는 큰 텍스트 말뭉치를 컴파일한다. 또한 각 텍스트 상자 내에서 글꼴 유형과 색상을 무작위로 수정하여 데이터 세트를 추가로 확대한다. 글립 텍스트 데이터세트 \\(\\mathcal{D}\\)는 거의 \\(\\sim 512\\)의 서로 다른 글꼴 유형과 \\(\\sim 100\\)의 서로 다른 글꼴 색상을 포함한다. 글리프 정렬 텍스트 인코더가 시각적 텍스트의 차이에만 초점을 맞추도록 하기 위해 우리는 기본적으로 검은색 배경을 사용한다.\n' +
      '\n' +
      '도 2의 (a)에 도시된 글리프 이미지에 대응하는 글리프 프롬프트의 예를 예시하면 다음과 같다 : {Text"oh! the places you\'ll go!" in [font-color-39], [font-type-90]. Text"Happy Graduation Kim!" in [font-color-19] [font-type-181]}. 이 과정에서 특수 토큰은 폰트 색상과 유형을 나타내기 위해 활용된다. Glyph-ByT5 텍스트 인코더에 입력하기 전에, 우리는 토큰 \'[font-color-39]\'와 같은 특수 토큰을 강화 코드북의 일련의 전역 임베딩으로 대체하여 프롬프트 텍스트를 전처리한다. 우리는 글리프 텍스트 데이터 세트를 100K에서 500K로 확장하고 최대 1M까지 세 가지 다른 규모로 실험을 수행했다. 미래에는 더 많은 컴퓨팅 리소스에 액세스할 수 있는 100M까지 확장하면서 데이터 세트를 크게 확장하는 것을 목표로 합니다.\n' +
      '\n' +
      '문단-글리프-텍스트 데이터 생성**맞춤형 텍스트 인코더의 작은 글꼴의 생성 품질과 문단 수준 레이아웃 계획 능력을 향상시키기 위해 \\(\\mathcal{D}^{\\mathrm{paragraph}}\\)으로 표시된 조밀하고 작은 문단 수준 글리프-텍스트 데이터 세트를 추가로 컴파일했다.\n' +
      '\n' +
      '우리는 \'단락\'을 일반적으로 10개 이상의 단어 또는 100자 이상으로 구성된 한 줄 내에서 수용할 수 없는 텍스트 내용의 블록으로 정의한다. 문단 글라이프 렌더링 작업은 매우 높은 단어 수준 철자 정확성뿐만 아니라 지정된 상자 영역 내에서 단어 수준 및 라인 수준 레이아웃의 세심한 계획을 요구하기 때문에 더 큰 문제를 제기한다. 이 데이터 세트는 100,000쌍의 합성 데이터\\(\\{\\mathrm{l}_{\\mathrm{glyph},\\mathrm{T}_{\\mathrm{text}\\})로 구성된다. 실증분석 결과 좋은 결과를 얻었다.\n' +
      '\n' +
      '그림 3: 문단-글리프 텍스트 데이터 세트에서 문단 시각적 텍스트로 예제 이미지를 보여줍니다. 왼쪽에서 오른쪽으로 단어의 #: 55, 64, 52, 46, 34, 35, 40, 43; 문자의 #: 443, 442, 416, 318, 247, 267, 282, 302.\n' +
      '\n' +
      '도 2: 글리프 증강의 스킴을 예시한다. 원래의 문양. (b) 캐릭터 교체(Happy \\(\\rightarrow\\) Hdpppy). (c) 문자 반복(Happy \\(\\rightarrow\\) Happpppy). (d) 캐릭터 드롭(Happy \\(\\rightarrow\\) Happy). (e) 문자 추가(졸업 \\(\\rightarrow\\) 졸업). (f) 단어 치환(Graduation \\(\\rightarrow\\) Gauatikn). (g) 단어 반복(Kim \\(\\rightarrow\\) Kim Kim). (h) 워드 드롭(Happy Graduation Kim \\(\\rightarrow\\) Graduation).\n' +
      '\n' +
      '초기 훈련된 모델을 \\(\\mathcal{D}\\)으로 조정하면, \\(\\mathcal{D}^{\\mathrm{paragraph}\\)을 사용하여 작은 크기와 문단 수준의 시각적 텍스트를 렌더링하는 성능이 현저하게 향상된다.\n' +
      '\n' +
      '문단 수준의 레이아웃 계획을 위한 능력은 간단하지 않으며, 확산 모델이 크기나 종횡비에 관계없이 주어진 텍스트 박스에 따라 효과적으로 다중 라인 배열을 계획하고 라인 또는 워드 간격을 조정할 수 있음을 경험적으로 보여준다. 그림 3에서 문단 글리프 텍스트 데이터의 예제 이미지를 표시하며, 각 이미지에는 100자 이상의 텍스트 상자가 하나 이상 포함되어 있음을 보여준다. 일부 이미지는 심지어 400자까지 도달하며 적당한 간격을 가진 여러 줄로 배열된다. 또한 100K, 500K 및 1M 글리프 텍스트 쌍으로 구성된 문단 글리프 텍스트 데이터 세트의 세 가지 척도를 구성한다.\n' +
      '\n' +
      '**글리프 증강** 서로 다른 글리프-텍스트 쌍만을 부정적인 샘플로 간주하는 기존의 CLIP 모델과 달리 -\\(10\\) 이상의 문자로 구성된 여러 단어 또는 심지어 단락에 의해 야기되는 상대적으로 높은 수준의 차이만을 모델링함 - 우리는 간단하면서도 효과적인 문자 수준 및 단어 수준 글리프 증강 방식을 제안한다. 이 접근법은 더 유익한 음성 샘플을 구성하여 훈련 효율성을 크게 향상시킨다.\n' +
      '\n' +
      '제안된 문자-레벨 및 단어-레벨 증강 기법은 기본적으로 문자-레벨 및 단어-레벨 모두에서 _glyph replacement_, _glyph repeat_, _glyph drop_ 및 _glyph add_를 포함하는 4가지 다른 글리프 증강 전략의 조합으로 구성된다. 이 증분을 \\(\\mathsf{l}_{\\mathrm{glyph}\\)과 \\(\\mathsf{T}_{\\mathrm{text}\\)에 적용하여 일관성을 보장한다. 그림 2는 이러한 증강 전략을 가진 몇 가지 대표적인 예를 보여준다. 또한 각 샘플에 대해 유익한 음성 샘플의 다른 비율을 구성하는 효과를 조사한다. 우리는 이러한 증강을 각 텍스트 상자에 독립적으로 적용한다. 우리는 보충 자료의 전체 글리프 텍스트 데이터 세트와 단락 글리프 텍스트 데이터 세트에 걸쳐 텍스트 상자, 단어 및 문자 수에 대한 통계를 제시한다.\n' +
      '\n' +
      '**글리프 텍스트 인코더** 각 문자의 텍스트 특징을 효율적으로 캡처하기 위해 글리프-CLIP의 기본 텍스트 인코더로 문자 인식 ByT5[28] 인코더를 선택했다. 오리지널 ByT5 모델은 더 가벼운 디코더와 쌍을 이루는 견고하고 무거운 인코더를 특징으로 합니다. ByT5 인코더는 [27]에서 언급한 바와 같이 mC4 텍스트 코퍼스로부터 공식 사전 훈련된 체크포인트를 사용하여 초기화된다.\n' +
      '\n' +
      '또한, 텍스트 인코더를 더 작은 크기에서 더 큰 크기로 스케일링하는 것의 영향을 탐구한다. 여기에는 ByT5-Small(217M 매개변수), ByT5-Base(415M 매개변수) 및 ByT5-Large(864M 매개변수)와 같은 다양한 ByT5 모델의 성능 향상을 검토하는 평가가 포함된다. 원본 ByT5 시리즈와 구별하기 위해 이러한 텍스트 인코더를 글리프-ByT5라고 하며, 이는 글리프 이미지와 해당 텍스트 프롬프트 사이의 격차를 줄이는 데 특화된 초점을 나타낸다.\n' +
      '\n' +
      '**Glyph Vision Encoder** 비주얼 인코더의 탐색을 위해 CLIP [23], 또는 DINOv2 [9, 20], 또는 비주얼 텍스트 인식 작업에 맞춘 변형 [1, 31]에서 파생된 비주얼 임베딩을 사용하여 영향을 분석했다. 우리의 관찰은 DINOv2가 최고의 성능을 산출한다는 것을 보여주었다. CLIP의 시각적 임베딩은 서로 다른 글꼴 유형을 구별하기 위해 고군분투했다는 점도 주목했다. 이 발견은 [8, 34]에서 논의한 바와 같이 최근 연구 노력과 일치하며, 이는 DINOv2가 신원 정보를 보존하는 데 탁월함을 보여준다. 결과적으로, DINOv2는 우리의 주요 시각적 인코더로 선택되었다. 또한, 시각적 인코더를 더 작은 크기에서 더 큰 크기로 스케일링하는 것이 성능에 미치는 영향을 조사했다. 여기에는 ViT-B/14(86M 매개변수), ViT-L/14(300M 매개변수) 및 ViT-g/14(1.1B 매개변수)와 같은 변화를 평가하여 다양한 스케일의 위에서 언급한 3개의 ByT5 텍스트 인코더와 정렬하는 것이 포함되었다.\n' +
      '\n' +
      '**박스 레벨 대비 손실** 전체 이미지에 대비 손실을 적용하는 기존의 CLIP와 달리 각 텍스트 박스와 해당 텍스트 프롬프트를 인스턴스로 처리하는 박스 레벨 대비 손실을 적용하는 것을 제안한다. 텍스트 상자 내의 문자 또는 단어의 수를 기반으로 단어 텍스트 상자, 문장 텍스트 상자 또는 단락 텍스트 상자로 분류할 수 있다. 따라서 상자 수준의 대조 손실은 다양한 수준의 입도에서 텍스트를 글리프 이미지와 정렬할 수 있다. 이 정렬은 문단 수준 레이아웃 계획을 위한 기능을 획득하는 데 사용자 정의된 텍스트 인코더를 지원합니다. 우리는 수학 공식을 다음과 같이 설명한다:\n' +
      '\n' +
      '{L}_{mathrm}}=-\\frac{1}{2\\sum_{i}}^{n}|}\\mathcal{N}{i}|}\\sum_{i=1}^{|\\mathcal{B}_{i}|}\\sum_{j=1}^{|\\mathcal{B}_{i}|}(\\log\\frac{e^{t\\mathbf{x}_{i}^{j}\\mathbf{x}_{i}^{j}}{Z_{y}}, \\tag{1}\\frac{e^{t\\mathbf{x}_{i}^{j}}{Z_{y}}}\n' +
      '\n' +
      '여기서 \\(\\mathcal{N}=\\{(\\mathsf{l}_{1},\\mathsf{T}_{1}), (\\mathsf{l}_{2},\\mathsf{T}_{2}), \\ldots\\}\\)는 동일한 배치 내의 모든 이미지-텍스트 쌍을 나타내며, 여기서 \\(i\\)번째 이미지-텍스트 쌍은 \\(|\\mathcal{B}_{i}|\\) 박스-서브-텍스트 쌍으로 구성된다. \\(i\\)번째 이미지-텍스트 쌍 \\((\\mathsf{l}_{i},\\mathsf{T}_{i})\\)에서 \\(j\\)번째 박스의 박스 임베딩과 서브 텍스트 임베딩을 다음과 같이 계산한다. \\(\\mathsf{x}_{i}^{j}=\\mathsf{ROIAlign}(\\frac{f(\\mathsf{l}_{i})}{\\|f(\\mathsf{l}_{i})\\|_{2},\\mathrm}_{i}^{j})\\(\\mathsf{y}^{j}=\\frac{g(\\mathsf{t}_{i}^{j})\\|g(\\mathsf{l}_{i}}^{j}) 및 \\(\\mathsf{y}_{i}^{j}=\\frac{g(\\mathsf{t}_{i}^{j})\\(\\mathsf{l}_{i} (f(\\cdot)\\)와 \\(g(\\cdot)\\)는 각각 비주얼 인코더와 텍스트 인코더를 나타낸다. 우리는 \\(Z_{x}=\\sum_{k=1}^{|\\mathcal{N}|}\\sum_{l=1}^{|\\mathcal{B}_{k}|}e^{t\\mathbff{x}_{i}\\mathbf{cdot}_{k}^{j}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}\\mathbf{x}\n' +
      '\n' +
      'Glyph Augmentation:_Glyph Augmentation을 기반으로 하는_Hard-negative Contrastive Loss:_ 우리는 추가적으로 우리의 glyph augmentation으로 생성된 hard-negative 샘플에 대한 contrastive loss를 계산하며 수학적 공식은 다음과 같다:\n' +
      '\n' +
      'mmathbff{x}_{i}^{i}^{i}}\\cdot\\mathbf{y}^{i}}\\cdot\\mathbf{y}^{i}}\\cdot\\mathbf{y}^{i}}}{Z_{y}^{prime}} 여기서, \\(\\mathcal{G}\\)는 박스 \\(\\mathbf{x}_{i}^{j}\\)와 서브 텍스트 \\(\\mathbf{y}_{i}^{j}\\)에 기초한 증강된 훈련 데이터를 나타낸다. 절제 실험에서 증강된 데이터 포인트 수의 변화에 따른 영향을 조사한다.\n' +
      '\n' +
      '이 두 가지 손실 즉, \\(\\mathcal{L}_{\\mathrm{box}+\\mathcal{L}_{\\mathrm{hard}}\\)을 결합하여 글리프 정렬 사전 훈련 과정을 용이하게 한다. 또한, 본 논문에서 제안한 기법이 절제 실험에서 이미지 수준의 대비 손실보다 우수하다는 것을 실험적으로 증명하였다. 우리는 우수한 성능을 두 가지 주요 요인, 즉 훨씬 더 많은 수의 효과적인 훈련 샘플의 가용성과 보다 정확한 시각적 텍스트 정보를 제공하는 박스 수준의 시각적 특징으로 돌린다. 이러한 주장은 두 개의 이전 연구 [4, 33]의 결과에 의해 확증된다. 그림 4는 글리프-ByT5의 완전한 프레임워크를 나타내며 이전에 언급한 중요한 구성요소를 통합하는 글리프 정렬 사전 훈련 프로세스를 보여준다.\n' +
      '\n' +
      '### 글리프-SDXL: 디자인 이미지 생성을 위한 글리프-ByT5와 함께 SDXL을 증강\n' +
      '\n' +
      '디자인 이미지에서 정확한 텍스트 콘텐츠를 생성하고 각 텍스트 박스 내에서 시각적 단락 레이아웃을 계획하는 데 우리의 접근법의 유효성을 검증하기 위해 글리프-ByT5를 최신 오픈 소스 텍스트-이미지 생성 모델인 SDXL[22]과 통합했다. 주된 과제는 우리의 맞춤형 텍스트 인코더를 기존 인코더와 통합하여 원래 성능을 손상시키지 않고 둘 다의 장점을 활용하는 것이다. 또 다른 과제는 일관된 배경 이미지 레이어에서 렌더링된 디자인-텍스트 생성 모델을 트레이닝하기 위한 고품질 그래픽 디자인 데이터세트의 부족이다.\n' +
      '\n' +
      '위에서 언급한 두 가지 문제를 해결하기 위해 먼저 대상 타이포그래피 박스 내에서 맞춤형 텍스트 인코더에 인코딩된 글리프 지식과 타이포그래피 박스 외부의 영역에서 원본 텍스트 인코더에 의해 전달되는 사전 지식을 원활하게 융합하는 영역별 다중 헤드 교차 주의 메커니즘을 소개한다. 또한, 정확한 시각적 텍스트 렌더링을 위해 글리프-SDXL 생성 모델을 훈련하기 위해 고품질 그래픽 디자인 데이터 세트를 구축한다. 이 두 가지 중추적인 기여에 대한 자세한 논의는 후속 섹션에서 제공된다.\n' +
      '\n' +
      '**영역별 다중-헤드 크로스-어텐션** 원본 다중-헤드 크로스-어텐션은 텍스트-공간의 풍부한 의미 정보를 이미지-공간 내의 상이한 위치들로 매핑하는 것을 담당하는 핵심 컴포넌트이다. 즉, 서로 다른 레이어와 타임 스텝에 걸쳐 멀티 헤드 크로스 어텐션을 지속적으로 적용하여 어디에서 어떤 객체를 생성하는지를 결정한다.\n' +
      '\n' +
      '영역별 다중 헤드 교차 주의의 세부 프레임워크는 그림 4의 오른쪽에 표시된다. 영역별 다중 헤드 교차 주의 메커니즘에서 먼저 입력 픽셀 임베딩(Query)을 여러 그룹으로 분할한다. 이 그룹들은 사용자가 지정하거나 GPT-4의 계획 능력을 활용하여 자동으로 예측할 수 있는 목표 텍스트 박스에 해당하며, 동시에 텍스트 프롬프트(Key-Value)를 전역 프롬프트와 글리프 특정 프롬프트의 여러 그룹을 포함하는 해당 하위 섹션으로 나눈다. 그런 다음 대상 텍스트 박스 내의 픽셀 임베딩이 Glyph-ByT5로 추출된 글리프 텍스트 임베딩에만 참석하도록 특별히 지시한다. 마찬가지로 텍스트 박스 외부의 픽셀 임베딩은 원본 두 CLIP 텍스트 인코더로 추출된 전역 프롬프트 임베딩에만 참석하도록 지시한다.\n' +
      '\n' +
      'Glyph-ByT5의 출력 임베딩 공간과 원래의 SDXL 임베딩 공간 사이의 간격을 좁히기 위해 경량 매퍼, 즉 ByT5-to-SDXL 매퍼를 소개한다. 이 매퍼에는 4개의 ByT5 트랜스포머 인코더 레이어가 장착되어 있으며, 각각은 랜덤 가중치로 초기화되며, 미리 훈련된 Glyph-ByT5 텍스트 인코더의 출력에 적용된다. 효율성을 위해 픽셀 임베딩과 다중 텍스트 인코더 임베딩 사이의 매핑 관계를 보장하는 마스크로 어텐션 맵을 변조함으로써 위에서 언급한 영역별 다중 헤드 교차 어텐션을 구현한다. 확산 모델 내에서 문자 인식 텍스트 인코더를 정제하는 것이 성능을 크게 향상시킬 수 있음을 강조하는 이전 연구 [16]에 따라 훈련 중 글리프-ByT5 텍스트 인코더와 ByT5-to-SDXL 매핑기의 가중치를 미세 조정한다.\n' +
      '\n' +
      '**디자인-텍스트 생성을 위한 시각적 디자인 데이터세트** 디자인-텍스트 렌더링 성능의 성능에 액세스하기 위해 신뢰성 있는 태스크를 선택하는 것이 중요하다. 이 작업은 텍스트 집약적 생성 작업의 가장 대표적인 시나리오 중 하나이기 때문에 디자인 이미지 생성을 선택한다. 따라서 우리는 먼저 [13]에 이어 많은 그래픽 디자인 웹사이트에서 크롤링하여 각 이미지에 렌더링된 조밀한 문단 수준의 시각적 텍스트로 고품질 시각적 디자인 이미지 데이터 세트를 구축한다. 이 작업은 조밀한 시각적 텍스트의 생성뿐만 아니라 시각적으로 매력적인 배경 이미지를 필요로 하기 때문에 두 가지 중요한 과제를 제시한다. 우리는 또한 100K, 500K 및 1M의 크기를 포함하는 세 가지 버전의 그래픽 디자인 데이터 세트를 만들었으며, 여기서 LLama2-13B[25]를 기반으로 한 LLaVA[15]를 사용하여 각 그래픽 디자인 이미지에 대한 세부 캡션을 생성하고 원시 데이터에서 지면 진실 글리프 텍스트를 쉽게 액세스할 수 있다. 또한 그래픽 디자인 이미지가 글리프 정렬 사전 교육에 사용되는 글리프 텍스트 이미지와 동일한 타이포그래피를 공유하는 경우가 거의 없도록 데이터 클리닝을 수행했다.\n' +
      '\n' +
      '**Glyph-SDXL** 위 구축된 디자인 텍스트 데이터셋에서 Glyph-SDXL을 학습한다. SDXL의 고유한 기능을 보존하기 위해 UNet 아키텍처와 이중 CLIP 텍스트 인코더를 모두 포함하는 전체 모델의 가중치를 잠급니다. 먼저, UNet 컴포넌트에 LoRA[11] 모듈을 포괄적으로 구현한다. 둘째, 글리프-ByT5 텍스트 인코더의 글리프 인식 능력과 두 개의 원본 CLIP 텍스트 인코더의 강력한 강도를 통합하기 위해 설계된 영역별 다중 텍스트 인코더 융합 메커니즘을 소개한다. 이 접근법은 각 텍스트 인코더의 고유한 특징을 상승시켜 시각적 텍스트 렌더링 성능을 향상시키는 것을 목표로 한다. 구현 시, 영역별 다중 헤드 크로스 어텐션으로 기존의 다중 헤드 크로스 어텐션 모듈을 수정하기만 하면 된다.\n' +
      '\n' +
      '우리는 보조 자료의 접근 방식과 전통적인 타이포그래피 렌더링 도구의 차이점을 자세히 설명한다. 당사의 맞춤형 글리프-ByT5는 완전 확산 기반 모델의 기능을 활용하면서 기존 도구의 렌더링 정확성과 일치합니다. 이를 통해 표준 렌더링 도구의 기능을 넘어서는 장면 텍스트 생성 작업을 처리할 수 있다.\n' +
      '\n' +
      '### Design-to-Scene Alignment: Scene-Text 생성을 위한 Finetuning Glyph-SDXL\n' +
      '\n' +
      '기존의 그래픽 디자인 이미지를 중심으로 훈련된 Glyph-SDXL은 일관된 레이아웃을 유지하는 장면 텍스트를 제작하는데 어려움을 겪는다. 게다가, 우리는 모델의 원래 숙련도를 약간 약화시키는 \'언어 드리프트\'로 알려진 현상을 발견했다. 이러한 문제를 해결하고 우수한 장면 텍스트 생성 모델의 생성을 용이하게 하기 위해, 우리는 하이브리드 디자인 대 장면 정렬 데이터 세트의 개발을 제안한다. 이 데이터 세트는 TextSeg[26]의 4,000개의 장면 텍스트 및 디자인 텍스트 이미지, SDXL을 사용하여 생성된 4,000개의 합성 이미지 및 4,000개의 디자인 이미지의 세 가지 유형의 고품질 데이터를 결합한다. 우리는 \\(2\\) 시대에 대한 하이브리드 설계-장면 정렬 데이터 세트에서 글리프-SDXL을 간단히 미세 조정한다. 세 가지 공개 벤치마크에 걸쳐 본 방법의 장면 텍스트 렌더링 능력에 대한 철저한 평가를 수행하고 이전 최신 방법에 비해 상당한 성능 향상을 보고한다. 원래 Glyph-SDXL과 구별하기 위해 설계 대 장면 정렬 데이터 세트의 미세 조정 버전을 Glyph-SDXL-씬으로 지정한다. 또한, 각 부분 집합이 보조 자료에 자세히 설명된 대로 일관성 있는 레이아웃, 정확한 텍스트 렌더링 및 시각적 품질의 세 가지 결합 목적에 유용함을 보여준다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '본 논문에서 제안하는 방법은 그래픽 디자인 이미지에서 정확한 디자인 텍스트를 생성할 수 있는 능력을 평가하는데, 이는 종종 수많은 문단 수준의 텍스트 상자와 실제 이미지 내의 장면 텍스트를 포함한다. 문단 수준의 시각적 텍스트 렌더링의 평가를 용이하게 하기 위해 VisualParagraphy 벤치마크를 개발했다. 이 벤치마크는 다양한 종횡비와 스케일의 경계 상자 내에 있는 다중 라인 시각적 텍스트를 포함한다.\n' +
      '\n' +
      '설계-텍스트 생성 작업에서 제안된 방법을 상용 제품과 DALL-E와 같은 가장 진보된 시각적 텍스트 렌더링 기법과 비교한다. 객관적인 OCR 메트릭을 보고하고 다른 측면에서 시각적 품질을 평가하기 위해 주관적인 사용자 연구를 수행한다. 장면-텍스트 생성 태스크를 위해 3개의 공개 벤치마크에 걸쳐 대표적인 모델인 GlyphControl[29] 및 TextDiffuser-2[7]과 제안한 방법을 비교한다.\n' +
      '\n' +
      '또한, 우리는 접근 방식 내에서 각 구성 요소의 효과를 연구하고 교차 주의 맵을 시각화하여 맞춤형 텍스트 인코더가 확산 모델 이전에 글리프를 제공할 수 있음을 입증하기 위해 철저한 절제 실험을 수행한다. 교육 설정을 자세히 설명하고 보충 자료에 추가 비교 결과를 제공합니다.\n' +
      '\n' +
      '### Metrics\n' +
      '\n' +
      '대부분의 실험에서 우리는 비교를 제외하고 대소문자에 민감한 단어 수준 정밀도를 보고하지 않는다.\n' +
      '\n' +
      '도 4: 글리프 정렬 사전 훈련 프레임워크 및 영역별 멀티헤드 교차 주의 모듈을 예시하는 것\n' +
      '\n' +
      '글리프컨트롤과 텍스트디퓨저를 포함하는 교도소. 이러한 경우 사례 진단 메트릭 및 이미지 수준 메트릭을 보고함으로써 원래 방법론과 일치합니다. 예를 들어, 표 2에 표시된 바와 같이, Case-Recall은 대문자와 소문자를 구별하기 위한 대소문자 민감 메트릭으로 사용된다. 반대로 다른 모든 메트릭은 대/소문자를 구분하지 않습니다. 정확도[IMG]는 이미지 수준의 정확도를 나타내기 위해 사용되며, 이는 양호한 평가를 달성하기 위해 전체 이미지 내의 모든 시각적 단어의 정확한 철자에 의존한다. 또한 글리프컨트롤의 OCR 정확도 메트릭과 텍스트디퓨저의 리콜 메트릭 간의 직접적인 대응 관계를 확인했다. 그 결과 SimpleBench와 CreativeBench 모두에 대한 메트릭 보고의 일관성을 보장하기 위해 Recall을 주요 메트릭으로 선택하여 접근 방식을 통합했다.\n' +
      '\n' +
      '### VisualParagraphy Benchmark\n' +
      '\n' +
      '설계문 생성 작업의 벤치마크를 구축하였으며, 난이도가 다른 다양한 문자 수를 포괄하는 설계문 프롬프트를 대략 \\(\\sim 1,000\\)개 축적하고, 20자 미만의 렌더링, 20~50자 렌더링, 50~100자 렌더링, 100자 이상의 렌더링에 대한 벤치마크를 구축하였다. 보충 자료의 프롬프트의 몇 가지 대표적인 예를 제공합니다. 상업용 제품인 DALL-E3와 비교할 때 약 1,000개의 디자인 텍스트 프롬프트를 사용하는 반면, 기본적으로 효율성을 위해 약 400개의 디자인 텍스트 프롬프트의 더 작은 하위 집합이 모든 후속 절제 실험에 사용된다.\n' +
      '\n' +
      '상용제품 DALL-E3와의 비교\n' +
      '\n' +
      '우리는 비주얼 텍스트 렌더링 작업에서 가장 강력한 상용 제품, 즉 VisualParagraphy 벤치마크에서 DALL-E3와 접근 방식을 비교한다. 우리는 시각적 심미성, 레이아웃 품질, 타이포그래피 정확성의 세 가지 중요한 측면에서 결과를 평가하기 위해 사용자 연구를 수행했다. 디자인 배경을 가진 10명의 사용자를 대상으로 1에서 5까지의 척도로 생성된 이미지를 평가하였으며, 사용자 연구 결과는 그림 6에 나와 있으며, 다른 두 가지 측면에서 사용자가 타이포그래피에 대해 상당히 높은 점수를 주고 약간 낮은 점수를 주는 것을 볼 수 있다. 또한, 그림 5에서 대표적인 정성적 시각적 비교 결과를 보여주며, 본 논문에서 제안한 방법은 디자인-텍스트 렌더링 작업에서 상당한 이점을 보여준다.\n' +
      '\n' +
      '최신 기술과의 비교\n' +
      '\n' +
      '우리의 최우선 목표는 시각적 텍스트 생성 모델의 광범위한 적용 가능성을 확인하는 것이었다. 이를 위해 본 연구의 방법론을 텍스트디퓨저[6], 텍스트디퓨저-2[7] 및 글리프컨트롤[29]과 같은 이전 연구에서 요약된 대표적인 장면-텍스트 렌더링 벤치마크에 적용하여 얻은 결과를 주의 깊게 자세히 설명했다.\n' +
      '\n' +
      '그림 5: 질적 비교 결과. 첫 번째 행과 두 번째 행에서 각각 Glyph-SDXL과 DALL-E3로 생성된 결과를 보여준다.\n' +
      '\n' +
      '도 6: 사용자 연구 결과. 점수가 높을수록 더 좋은 결과를 의미한다.\n' +
      '\n' +
      '여기에는 MARIO-Eval, SimpleBench 및 CreativeBench와 같은 벤치마크에 대한 포괄적인 테스트가 포함됩니다. 비교 결과는 표 2에 요약되어 있으며, 이러한 비교 결과에 따르면 글리프-SDXL-씬은 이 세 벤치마크에서 상당한 마진으로 이전 최신 기술을 훨씬 능가하는 것이 분명하다. 제안된 방법의 모든 결과는 제로샷 성능을 나타낸다.\n' +
      '\n' +
      'DALL-E3의 타이포그래피 편집\n' +
      '\n' +
      '우리는 Glyph-SDXL이 보충 자료의 SDEdit[18]에 이어 DALL-E3에 의해 생성된 이미지에서 타이포그래피를 편집할 수 있음을 보여준다.\n' +
      '\n' +
      '### Ablation Experiments\n' +
      '\n' +
      '우리는 처음에 글리프 정렬 사전 훈련을 수행한 다음 그래픽 디자인 벤치마크에서 글리프-SDXL 모델을 훈련하여 모든 절제 연구를 수행한다. 또한, 모든 절제는 명시되지 않는 한 Glyph-ByT5 및 Glyph-SDXL 모델에 대해 각각 \\(100\\)K 글리프 이미지-텍스트 쌍에 대해 수행된다.\n' +
      '\n' +
      '**사전 훈련된 비주얼 인코더 초이스** CLIP 비주얼 인코더[23], DINOV2[9], ViTSTR[1], CLIP4STR 비주얼 인코더[31]의 네 가지 다른 사전 훈련된 비주얼 인코더를 선택하는 효과를 연구한다. 우리는 표 3의 상세한 비교 결과를 보고하며, 특히 DINOV2를 사전 훈련된 시각적 인코더로 사용할 때만 정확한 글꼴 유형과 색상 제어가 발생하는 것을 관찰한다.\n' +
      '\n' +
      '**Loss Design** 서로 다른 훈련 손실 설계를 선택하는 효과를 연구하고 상세한 비교 결과를 표 4에 보고한다. 제안된 박스-레벨 대비 손실이 양호한 성능을 달성함을 분명히 한다.\n' +
      '\n' +
      '**글리프 증강** 글리프 정렬 사전 훈련 동안 글리프 증강의 효과를 연구한다. 표 5에 나타낸 바와 같이, 글리프 증강은 비증강 설정에 비해 현저한 개선을 제공하며, 약 1:16에서 최고조에 달한다. 특히, 우리는 또한 폰트-타입 및 폰트-타입을 관찰한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\multirow{2}{*}{Text encoder} & \\multirow{2}{*}{\\#Params} & \\multicolumn{4}{c}{Precision (\\%)} \\\\ \\cline{3-6}  & & \\(\\leq\\)20 chars & \\(\\leq\\)20-50 chars & \\(\\leq\\)50-100 chars & \\(\\geq\\)100 chars \\\\ \\hline Glyph-ByT5-S & \\(292\\)M & \\(84.54\\) & \\(84.56\\) & \\(\\mathbf{79.89}\\) & \\(73.29\\) \\\\ Glyph-ByT5-B & \\(510\\)M & \\(\\mathbf{87.10}\\) & \\(\\mathbf{84.93}\\) & \\(78.72\\) & \\(72.81\\) \\\\ Glyph-ByT5-L & \\(963\\)M & \\(87.07\\) & \\(82.87\\) & \\(79.12\\) & \\(\\mathbf{73.72}\\) \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: Glyph-SDXL 내의 ByT5-to-SDXL 매퍼의 효과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c} \\multirow{2}{*}{Method} & \\multicolumn{4}{c|}{Precision (\\%)} \\\\ \\cline{2-7}  & \\(\\leq\\)20 chars & \\(\\leq\\)20-50 chars & \\(\\leq\\)50-100 chars & \\(\\geq\\)100 chars \\\\ \\hline DINOV2 VIT-B/14 + reg & \\(\\mathbf{84.54}\\) & \\(\\mathbf{84.56}\\) & \\(\\mathbf{79.89}\\) & \\(\\mathbf{73.29}\\) \\\\ CLIP VIT-B/16 & \\(77.17\\) & \\(74.78\\) & \\(74.94\\) & \\(66.34\\) \\\\ VITSTR & \\(79.29\\) & \\(78.2\\) & \\(75.35\\) & \\(68.49\\) \\\\ CLIP4STR VIT-B/16 & \\(80.38\\) & \\(79.12\\) & \\(77.08\\) & \\(69.24\\) \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: SimpleBench, CreativeBench, MARIO-Eval의 비교.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c} \\multirow{2}{*}{Loss design} & \\multicolumn{4}{c}{Precision (\\%)} \\\\ \\cline{2-7}  & \\(\\leq\\)20 chars & \\(\\leq\\)20-50 chars & \\(\\leq\\)50-100 chars & \\(\\geq\\)100 chars \\\\ \\hline IL-CL & \\(83.13\\) & \\(81.83\\) & \\(77.15\\) & \\(69.42\\) \\\\ BL-CL & \\(\\mathbf{84.54}\\) & \\(\\mathbf{84.56}\\) & \\(\\mathbf{79.89}\\) & \\(\\mathbf{73.29}\\) \\\\ IL-CL + BL-CL & \\(83.86\\) & \\(82.08\\) & \\(78.07\\) & \\(70.54\\) \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 상이한 손실 설계를 사용하는 것의 효과. IL-CL: 이미지 수준의 대비 손실. BL-CL: 박스 레벨 대비 손실.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c c|c c c} \\multirow{2}{*}{Visual encoder} & \\multicolumn{4}{c|}{Precision (\\%)} \\\\ \\cline{2-10}  & & \\(\\leq\\)20 chars & \\(\\leq\\)20-50 chars & \\(\\leq\\)50-100 chars & \\(\\geq\\)100 chars \\\\ \\hline DINOV2 VIT-B/14 + reg & \\(\\mathbf{84.54}\\) & \\(\\mathbf{84.56}\\) & \\(\\mathbf{79.89}\\) & \\(\\mathbf{73.29}\\) \\\\ CLIP VIT-B/16 & \\(77.17\\) & \\(74.78\\) & \\(74.94\\) & \\(66.34\\) \\\\ VITSTR & \\(79.29\\) & \\(78.2\\) & \\(75.35\\) & \\(68.49\\) \\\\ CLIP4STR VIT-B/16 & \\(80.38\\) & \\(79.12\\) & \\(77.08\\) & \\(69.24\\) \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 상이한 사전 훈련된 시각적 인코더를 사용하는 것의 효과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\multirow{2}{*}{Glyph huge-Text} & \\multicolumn{4}{c}{Precision (\\%)} \\\\ \\cline{2-7}  & \\(\\leq\\)20 chars & \\(\\leq\\)20-50 chars & \\(\\leq\\)50-100 chars & \\(\\geq\\)100 chars \\\\ \\hline None & \\(78.93\\) & \\(78.35\\) & \\(74.0\\) & \\(65.40\\) \\\\\n' +
      '1:8 & \\(81.15\\) & \\(80.45\\) & \\(77.03\\) & \\(70.03\\) \\\\(81.15\\) & \\(80.45\\) & \\(77.03\\) & \\(70.03\\)\n' +
      '1:16 & \\(\\mathbf{84.54}\\) & \\(84.56\\) & \\(\\mathbf{79.89}\\) & \\(\\mathbf{73.29}\\) \\\\\n' +
      '1:32 & \\(83.24\\) & \\(\\mathbf{85.02}\\) & \\(78.92\\) & \\(색 조절은 비율이 1:16에 도달하거나 초과할 때만 발생하며, 또한 그 효과를 나타낸다.\n' +
      '\n' +
      '**Mapper, Scaling Glyph-Text Dataset 및 Text Encoder Size, More** 표 6은 ByT5-to-SDXL 용지를 사용하여 간격을 정렬하는 것의 중요성을 보여준다. 표 7 및 표 8은 글리프-텍스트 데이터세트 크기 및 텍스트 인코더 크기를 스케일링하는 것의 이점을 검증한다. 우리는 보충 재료에서 Glyph-SDXL-장면에 대한 더 많은 절제 실험을 제공한다.\n' +
      '\n' +
      '정성적 분석** 시각적 텍스트 렌더링 작업에서 글리프-ByT5가 어떻게 뛰어난지에 대한 더 깊은 이해를 얻기 위해 글리프 텍스트 프롬프트와 렌더링된 이미지 사이의 교차 주의 맵을 추가로 시각화하여 그림 8의 예를 제공한다. 이 시각화는 확산 모델이 글리프-ByT5 텍스트 인코더 내에 인코딩된 이전 글리프 정렬을 효과적으로 활용함을 확인한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문은 확산 모델을 이용한 정확한 시각적 텍스트 렌더링을 위한 Glyph-ByT5 텍스트 인코더의 설계 및 트레이닝을 제시한다. 이 노력의 핵심은 확장 가능한 고품질 글리프 텍스트 데이터 세트의 생성과 글리프 텍스트 정렬을 위한 사전 훈련 기술의 구현이라는 두 가지 핵심 개발이다. 이러한 중요한 발전은 글라이프 이미지와 텍스트 프롬프트 사이의 격차를 효율적으로 해소하여 텍스트가 풍부한 디자인 이미지와 장면 텍스트가 있는 오픈 도메인 이미지 모두에 대한 정확한 텍스트 생성을 촉진한다. 제안된 Glyph-SDXL 모델에 의해 달성된 강력한 성능은 전문 텍스트 인코더의 개발이 확산 모델과 관련된 일부 근본적인 문제를 극복하기 위한 유망한 방법을 나타냄을 시사하며, 이는 도메인에서 상당한 경향을 나타낸다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Rowel Atienza. Vision transformer for fast and efficient scene text recognition. In _International Conference on Document Analysis and Recognition_, pages 319-334. Springer, 2021.\n' +
      '* [2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. _ACM Transactions on Graphics (TOG)_, 42(4):1-11, 2023.\n' +
      '* [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. 2023.\n' +
      '* [4] Ioana Bica, Anastasijia Ilic, Matthias Bauer, Goker Erdogan, Matko Bosnjak, Christos Kaplanis, Alexey A Gritsenko, Matthias Minderer, Charles Blundell, Razvan Pascanu, et al. Improving fine-grained understanding in image-text pre-training. _arXiv preprint arXiv:2401.09865_, 2024.\n' +
      '* [5] Haoxing Chen, Zhuoer Xu, Zhangxuan Gu, Jun Lan, Xing Zheng, Yaohui Li, Changhua Meng, Huijia Zhu, and Weiqiang Wang. Diffute: Universal text editing diffusion model.\n' +
      '* [6] Jingye Chen, Yunqu Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdifffuser: Diffusion models as text painters. _arXiv preprint arXiv:2305.10855_, 2023.\n' +
      '* [7] Jingye Chen, Yunqu Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdifffuser-2: Unleashing the power of language models for text rendering. _arXiv preprint arXiv:2311.16465_, 2023.\n' +
      '* [8] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Arydoor: Zero-shot object-level image customization. _arXiv preprint arXiv:2307.09481_, 2023.\n' +
      '* [9] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers, 2023.\n' +
      '* [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024.\n' +
      '* [11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [12] Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian Price, and Shiyu Chang. Improving diffusion models for scene text editing with dual encoders. _arXiv preprint arXiv:2304.05568_, 2023.\n' +
      '* [13] Peidong Jia, Chenxuan Li, Zeyu Liu, Yichao Shen, Xingru Chen, Yuhui Yuan, Yinglin Zheng, Dong Chen, Ji Li, Xiaodong Xie, et al. Cole: A hierarchical generation framework for graphic design. _arXiv preprint arXiv:2311.16974_, 2023.\n' +
      '\n' +
      '그림 8: Glyph-SDXL 모델 내에서 교차 주의 지도의 시각화. 텍스트 박스 내의 모든 픽셀과 선택된 문자 사이의 히트 맵을 \'INTERNATIONAL CAT DAY\'와 \'2022년 8월 8일\'로 보여준다.\n' +
      '\n' +
      '* [14] DeepFloyd Lab. Deepfloyd if. [https://github.com/deep-floyd/IF](https://github.com/deep-floyd/IF), 2023.\n' +
      '* [15] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2024.\n' +
      '* [16] Rosanne Liu, Daniel H Garrette, Chiwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, R. J. Mical, Mohammad Norouzi, and Noah Constant. Character-aware models improve visual text rendering. In _Annual Meeting of the Association for Computational Linguistics_, 2022.\n' +
      '* [17] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, and Xiaodong Lin. Glyphdraw: Learning to draw chinese characters in image synthesis models coherently. _arXiv preprint arXiv:2303.17870_, 2023.\n' +
      '* [18] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. _arXiv preprint arXiv:2108.01073_, 2021.\n' +
      '* [19] OpenAI. Dall-e 3 system card. 2023.\n' +
      '* [20] Maxime Quabu, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.\n' +
      '* [21] Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. Teaching clip to count to ten. _arXiv preprint arXiv:2302.12066_, 2023.\n' +
      '* [22] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [24] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '* [25] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [26] Xingqian Xu, Zhifei Zhang, Zhaowen Wang, Brian Price, Zhonghao Wang, and Humphrey Shi. Rethinking text segmentation: A novel dataset and a text-specific refinement approach. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12045-12055, 2021.\n' +
      '* [27] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. _arXiv preprint arXiv:2010.11934_, 2020.\n' +
      '* [28] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. Byt5: Towards a token-free future with pre-trained byte-to-byte models. _Transactions of the Association for Computational Linguistics_, 10:291-306, 2022.\n' +
      '* [29] Yukawa Yang, Dongnan Gu, Yuhui Yuan, Haisong Ding, Han Hu, and Kai Chen. Glyphcontrol: Glyph conditional control for visual text generation, 2023.\n' +
      '* [30] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18123-18133, 2022.\n' +
      '* [31] Shuai Zhao, Xiaohan Wang, Linchao Zhu, and Yi Yang. Clip4str: A simple baseline for scene text recognition with pre-trained vision-language model. _arXiv preprint arXiv:2305.14014_, 2023.\n' +
      '* [32] Yiming Zhao and Zhouhui Lian. Udifftext: A unified framework for high-quality text synthesis in arbitrary images via character-aware diffusion models. _arXiv preprint arXiv:2312.04884_, 2023.\n' +
      '* [33] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16793-16803, 2022.\n' +
      '* [34] Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, and Tong Sun. Customization assistant for text-to-image generation. _arXiv preprint arXiv:2312.03045_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:12]\n' +
      '\n' +
      '## 다른 텍스트 인코더 융합 방식에 대한 부록 E 삭제\n' +
      '\n' +
      '다른 텍스트 인코더의 텍스트 임베딩을 연결하는 기본 접근법의 효과에 의문을 제기할 수 있다. 비교 결과는 표 11에 자세히 설명되어 있으며, 경험적으로 텍스트 인코더 간의 상당한 불일치로 인해 이 기준선이 크게 성능이 저하됨을 알 수 있다.\n' +
      '\n' +
      '## 부록 F 글꼴형 블렌딩\n' +
      '\n' +
      '글리프-SDXL 모델의 외삽 능력을 입증하기 위해 다른 글꼴 유형을 혼합하여 보이지 않는 새로운 글꼴 유형을 생성하는 효과를 설명한다. 도 17에 예시된 바와 같이, 우리는 이탤릭한 _Brightwall-Italic_ 폰트 타입의 임베딩을 독특한 _Creepster-Regular_ 폰트 타입으로 보간하여 이탤릭하고 _Creepster-Regular_의 특수 효과를 포함하는 새로운 블렌디드 폰트 타입을 생성한다.\n' +
      '\n' +
      '도 11: **저희 Glyph-SDXL을 기반으로 한 타이포그래피 편집 결과 예시도. DALL-E3에 의해 생성된 원본 이미지와 Glyph-SDXL에 의해 편집된 이미지는 각각 1, 3, 5행, 2, 4, 6행에 예시되어 있다.**\n' +
      '\n' +
      '도 10: 영역별 SDEdit 파이프라인을 예시한다.\n' +
      '\n' +
      '도 9: Glyph-Text Dataset 및 Paragraph-Glyph-Text Dataset For Glyph-Text Dataset의 통계를 도시한 도면: (a) 텍스트 박스 #, (b) 단어 #, (c) 문자 #. 문단-Glyph-Text Dataset에 대해: (d)#의 텍스트 박스들, (e)#의 단어들, (f)#의 문자들. 문단 글립 텍스트의 글립 이미지는 훨씬 더 많은 수의 단어와 문자로 구성되어 있음을 알 수 있다.\n' +
      '\n' +
      '## 부록 G 상세 프롬프트 리스트\n' +
      '\n' +
      '표 12에서 그림 1과 그림 5에 표시된 생성된 이미지에 대한 자세한 프롬프트를 보여준다.\n' +
      '\n' +
      '## GPT-4를 이용한 부록 H 타이포그래피 레이아웃 계획\n' +
      '\n' +
      '대상 텍스트 박스와 같이 수동으로 제공되는 타이포그래피 레이아웃에 대한 의존도를 줄이기 위해 GPT-4의 시각적 계획 기능을 활용하여 레이아웃 생성을 자동화하고 이러한 레이아웃을 기반으로 이미지를 표시한다. 또한, TextDiffuser-2의 LLM의 레이아웃 예측 기능을 사용하여 타겟 텍스트 박스를 결정하고, 이러한 예측에 따라 Glyph-SDXL 모델을 구현하여 그림 18과 같이 시각적 텍스트 이미지를 생성한다. 그 결과 GPT-4의 레이아웃 기능이 TextDiffuser-2의 레이아웃 LLM의 레이아웃 성능보다 훨씬 더 신뢰할 수 있음을 나타낸다.\n' +
      '\n' +
      '또한, 도 19에서 GPT-4와 마주치는 몇 가지 전형적인 실패 사례들을 제시한다. 특히, GPT-4는 모든 텍스트 박스들을 동일한 열(열 1 & 2), 텍스트 박스들을 하나의 코너(열 3 & 4)로 균일하게 분배하거나, "Happy" 및 "Father"을 함께 배치하는 것과 같은 텍스트 의미론에 의해 암시되는 레이아웃 제약들을 간과하는 경향이 있다(열 5 & 6).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      '도 14: **디자인-장면 미세조정 과정에서 그래픽 디자인 이미지를 통합한 영향을 나타낸 도면.** 순으로 표시됨: 첫 번째 행에 Glyph-SDXL, 두 번째 행에 그래픽 디자인 이미지가 없는 Glyph-SDXL-Scene 미세조정, 마지막으로 마지막 행에 그래픽 디자인 이미지를 활용한 Glyph-SDXL-Scene으로 생성된 이미지이다.\n' +
      '\n' +
      '그림 12: 영역별 SDEdit에 대한 \\(t_{0}\\)의 다른 선택의 영향.\n' +
      '\n' +
      '그림 13: 영역별 SDEdit에 대한 \\(t_{1}\\)의 다른 선택의 영향.\n' +
      '\n' +
      '도 16: **장면-텍스트 이미지를 디자인-대-장면 미세 조정 프로세스에 통합하는 것의 영향을 예시한다.** 순서로 디스플레이됨은: 첫 번째 행에 Glyph-SDXL, 두 번째 행에 TextSeg 이미지가 없는 Glyph-SDXL-Scene 미세 조정, 마지막으로 마지막 행에 TextSeg 이미지를 활용하는 Glyph-SDXL-Scene에 의해 생성된 이미지이다.\n' +
      '\n' +
      '도 15: 설계-장면 미세 조정 프로세스에서 SDXL-생성 이미지를 통합하는 것의 영향을 예시한다.** 순서로 디스플레이된다: 첫 번째 행에서 원래의 SDXL, 두 번째 행에서 Glyph-SDXL, 세 번째 행에서 SDXL-생성 이미지가 없이 Glyph-SDXL-Scene 미세 조정, 마지막으로 마지막 행에서 SDXL-생성 이미지를 활용하는 Glyph-SDXL-Scene.\n' +
      '\n' +
      '도 17: 폰트 타입 블렌딩의 효과를 예시한다.\n' +
      '\n' +
      '도 18: GPT-4(Row 1 & 2) 및 TextDiffuser-2(Row 3 & 4)를 통합하여 생성된 이미지 및 레이아웃 예제(TextDiffuser-2는 폰트 타입 및 색상을 계획하기 위해 훈련되지 않으므로, GPT-4에서 권장하는 스타일을 각각 계획 이전으로 사용한다.\n' +
      '\n' +
      '그림 19: GPT-4 레이아웃 계획 실패 사례. GPT 설계자와 인간 설계자로 예측된 레이아웃을 사용하여 생성된 결과는 Col 1, 3, 5 및 Col 2, 4, 6에서 각각 나타난다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>