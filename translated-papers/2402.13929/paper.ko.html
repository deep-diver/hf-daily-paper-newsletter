<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# SDXL-Lightning: 점진적 적대적 확산 증류\n' +
      '\n' +
      ' 샤천린안란왕샤오양\n' +
      '\n' +
      'ByteDance Inc.\n' +
      '\n' +
      '{peterlin, anran.wang, yangxiao.0}@bytedance.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문에서는 SDXL을 기반으로 한 1단계/소단계 1024px 텍스트-이미지 생성에서 새로운 최신 기술을 달성하는 확산 증류 방법을 제안한다. 우리의 방법은 품질과 모드 적용 범위 사이의 균형을 달성하기 위해 점진적 증류와 적대적 증류를 결합한다. 본 논문에서는 이론적 분석, 판별기 설계, 모델 공식화 및 훈련 기법에 대해 논의한다. 우리는 LoRA와 전체 UNet 가중치로 증류된 SDXL-번개 모델을 오픈 소스한다.\n' +
      '\n' +
      '+\n' +
      '각주 †: 모델: [https://huggingface.co/ByteDance/SDXL-Lightning](https://huggingface.co/ByteDance/SDXL-Lightning)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '확산 모델[67, 14, 63]은 텍스트-대-이미지[5, 43, 44, 49, 51, 53, 79], 텍스트-대-비디오[2, 3, 10, 13, 62, 80], 및 이미지-대-비디오[2], _etc_와 같은 광범위한 애플리케이션들을 최첨단의 결과를 달성한 생성 모델들의 상승 클래스이다. 그러나 확산 모델의 반복 생성 과정은 느리고 계산적으로 확장적이다. 고품질 샘플을 더 빨리 생성하는 방법은 활발하게 연구되고 있는 분야이며 우리 작업의 주요 초점입니다.\n' +
      '\n' +
      '개념적으로, 생성은 데이터와 잡음 분포 사이에서 샘플을 점진적으로 수송하는 흐름을 포함한다. 확산 모델은 이 흐름의 임의의 위치에서 기울기를 예측하도록 학습한다. 세대는 단순히 흐름을 통해 예측된 기울기를 따라 노이즈 분포에서 데이터 분포로 샘플을 운반하는 것이다. 흐름이 복잡하고 곡선적이기 때문에 세대는 한 번에 작은 걸음을 내딛어야 한다. 형식적으로 흐름은 상미분 방정식(ODE)[67]로 표현된다. 실제로 고품질 데이터 샘플을 생성하려면 50개 이상의 추론 단계가 필요하다.\n' +
      '\n' +
      '추론 단계의 수를 줄이기 위한 다양한 접근법이 연구되었다. 이전 작업은 흐름의 곡선 특성을 설명하기 위해 더 나은 ODE 솔버를 제안했다[19, 30, 34, 35, 64, 78]. 다른 사람들은 흐름을 더 똑바로 만들기 위한 제형을 제안했다[29, 31]. 그럼에도 불구하고 이러한 접근법은 일반적으로 여전히 20개 이상의 추론 단계를 필요로 한다.\n' +
      '\n' +
      '반면에 모델 증류[22, 32, 36, 37, 54, 58, 65, 66, 73, 75]는 10개의 추론 단계에서 고품질 샘플을 달성할 수 있다. 현재 흐름 위치에서 기울기를 예측하는 대신 모델을 변경하여 훨씬 더 멀리 다음 흐름 위치를 직접 예측한다. 기존의 방법은 4 또는 8개의 추론 단계를 사용하여 좋은 결과를 얻을 수 있지만, 품질은 여전히 1 또는 2개의 추론 단계를 사용하여 생산-허용되지 않는다. 우리의 방법은 모델 증류 우산에 속하며 기존 방법에 비해 훨씬 우수한 품질을 달성한다.\n' +
      '\n' +
      '우리의 방법은 진행성[54]과 적대적 증류(58)의 두 세계 중 최고를 결합한다. 점진적 증류는 증류된 모델이 동일한 ODE 흐름을 따르고 원래 모델과 동일한 모드 적용 범위를 갖는다는 것을 보장한다. 그러나, 평균 제곱 오차(MSE) 손실을 갖는 점진적 증류는 8개의 추론 단계에서 흐릿한 결과를 생성하며, 본 논문에서는 이론적 분석을 제공한다. 이 문제를 완화하기 위해 증류의 모든 단계에서 적대적 손실을 사용하여 품질과 모드 적용 범위 사이의 균형을 맞춘다. 또한 점진적 증류는 다단계 샘플링에 대한 추가적인 이점인 _i.e_를 가져오며, 본 모델은 다른 증류 접근법[58, 66, 75]에 의해 매번 ODE 종점으로 점프하는 대신 ODE 궤적의 다음 위치를 예측한다. 이것은 원래의 모델 동작을 더 잘 보존하고 LoRA 모듈 [16] 및 제어 플러그인 [76, 10, 74]과의 더 나은 호환성을 용이하게 한다.\n' +
      '\n' +
      '또한, 본 논문에서는 혁신적인 판별기 설계, 손실 목표 및 안정적인 훈련 기법을 제안한다. 구체적으로, 사전 학습된 확산 UNet 인코더를 판별기 백본으로 사용하고 잠재 공간에서 완전히 동작한다. 우리는 샘플 품질과 모드 적용 범위를 상쇄하기 위해 두 가지 적대적 손실 목표를 제안한다. 우리는 확산 스케줄과 출력 공식의 함의를 조사한다. 우리는 적대적 훈련을 안정시키기 위한 기술에 대해 논의한다.\n' +
      '\n' +
      '우리의 증류 방법은 1024px 해상도에서 1단계/소단계 생성을 지원하는 새로운 최첨단 SDXL 모델을 생성한다. 우리는 SDXL-번개로 증류 모델을 오픈 소스합니다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '### Diffusion Model\n' +
      '\n' +
      '정방향 확산 프로세스[14]는 데이터 분포로부터 가우시안 잡음 분포로 샘플들을 점진적으로 변환한다. 데이터 샘플 \\(x_{0}\\), 잡음 \\(\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})\\) 및 시간 \\(t\\sim\\mathcal{U}(1,T)\\)이 주어졌다. 순방향 함수는 다음과 같이 정의되며, \\(\\bar{\\alpha}_{t}\\)는 수동으로 정의된 스케줄[14]로 정의된다:\n' +
      '\n' +
      '\\[x_{t}=\\mathbf{forward}(x_{0},\\epsilon,t)\\equiv\\sqrt{\\bar{\\alpha}_{t}}x_{0}+\\sqrt{1-\\bar{\\alpha}_{t}\\epsilon\\tag{1}\\.\n' +
      '\n' +
      '뉴럴 네트워크 \\(f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}\\)는 흐름의 임의의 위치 \\(x_{t}\\)에서 기울기 필드 \\(u_{t}\\)을 예측하도록 훈련된다. 네트워크는 시간 \\(t\\) 및 선택적으로 다른 조건 \\(c\\):\n' +
      '\n' +
      '\\[\\hat{u}_{t}=f(x_{t},t,c) \\tag{2}\\]\n' +
      '\n' +
      '많은 선행 작업들이 잡음 예측을 수행하기 위해 네트워크를 공식화한다[14], _i.e_. \\ (\\hat{\\epsilon}=f(x_{t},t,c)\\). 우리는 예측값을 \\(\\hat{x}_{0}\\) 공간으로 변환하기 위해 변환 함수 \\(\\mathbf{x}\\)을 사용할 수 있다:\n' +
      '\n' +
      '\\hat{x}_{0}=\\mathbf{x}(x_{t},\\hat{\\epsilon},t)\\equiv(x_{t}-\\sqrt{1-\\bar{\\alpha}_{t}\\hat{\\epsilon})/\\sqrt{\\bar{\\alpha}_{t}\\tag{3}\\t.\n' +
      '\n' +
      '대안적으로, 네트워크는 데이터 샘플 예측[14], _i.e_. \\ (\\hat{x}_{0}=f(x_{t},t,c)\\). 예측을 \\(\\hat{\\epsilon}\\) 공간으로 변환하기 위해 변환 함수 \\(\\boldsymbol{\\epsilon}\\)을 사용할 수 있다.\n' +
      '\n' +
      '\\[\\hat{\\epsilon}=\\boldsymbol{\\epsilon}(x_{t},\\hat{x}_{0},t)\\equiv(x_{t}-\\sqrt{\\bar{\\alpha}_{t}}\\hat{x}_{0})/\\sqrt{1-\\bar{\\alpha}_{t}\\tag{4}\\t.\n' +
      '\n' +
      '제형에 관계없이 네트워크는 본질적으로 구배\\(\\hat{u}_{t}\\)를 예측한다. 임의의 위치 \\(x_{t}\\)에서 기울기 \\(u_{t}\\)이 주어지면, 우리는 흐름을 따라 샘플을 이동할 수 있다:\n' +
      '\n' +
      '\\mathbf{move}(x_{t},u_{t},t,t^{\\prime}&=\\mathbf{move}(x_{t},u_{t},t,t^{\\prime})\\\\&\\equiv\\mathbf{forward}(\\mathbf{x}(x_{t},u_{t},t),\\boldsymbol{\\epsilon}(x_{t},u_{t},t),t^{\\prime}\\end{split}\\tag{5}\\mathbf{move}(x_{t},u_{t},t,t^{\\prime}}&=\\mathbf{move}(x_{t},u_{t},t,t^{\\prime})\\mathbf{move}(x_{t},u_{t},t,t^{\\prime})\\mathbf{move}(x_{t},u_{t},t\n' +
      '\n' +
      '생성 과정은 샘플 \\(x_{T}\\sim\\mathcal{N}(0,\\mathbf{I})\\)을 \\(t=T\\)에서 \\(t=0\\)의 작은 단계로 한 번에 이동시킨다.\n' +
      '\n' +
      '### 잠재 확산 모델\n' +
      '\n' +
      '데이터 공간에서 샘플을 직접 생성하는 대신에, 잠재 확산 모델들(LDMs)(51)은 데이터를 보다 컴팩트한 잠재 공간으로 인코딩하는 Variational Autoencoder(VAE)(25)를 먼저 트레이닝할 것을 제안한다. 확산 모델들은 잠재 코드들을 생성하도록 트레이닝되고, 이들은 VAE 디코더를 통과하여 최종 데이터 샘플을 생성한다.\n' +
      '\n' +
      '잠재 확산 모델은 계산 효율로 인해 고해상도 이미지 및 비디오 생성에 널리 채택된다. SDXL[44]는 128px 잠재 공간으로부터 1024px 해상도 영상을 생성할 수 있는 최첨단 텍스트-이미지 생성 모델이다.\n' +
      '\n' +
      '### Progressive Distillation\n' +
      '\n' +
      '점진 증류[54]는 교사가 여러 단계를 수행한 것처럼 다음 흐름 위치를 가리키는 방향을 예측하도록 학생을 훈련시킨다.\n' +
      '\n' +
      '구체적으로, 데이터세트로부터 데이터 \\(x_{0},c\\)와 잡음 \\(\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})\\)이 주어지면 임의의 타임스텝 \\(t\\):\n' +
      '\n' +
      '\\[x_{t}=\\mathbf{forward}(x_{0},\\epsilon,t) \\tag{6}\\]\n' +
      '\n' +
      '우리는 \\(x_{t-ns}\\)을 유도하기 위해 \\(n\\) 추론 단계를 수행하기 위해 냉동 교사 네트워크 \\(f_{\\mathrm{teacher}}\\)을 사용한다.\n' +
      '\n' +
      '(x_{t},u_{t},t,t-s)\\](8) \\[u_{t-s} =f_{\\mathrm{teacher}(x_{t-s},u_{t-s},t-s,t-2s} =f_{\\mathrm{move}(x_{t-s},t_{t-1)s},t-s,t-2s)\\](10) \\[u_{t-2s} =f_{\\mathrm{move}(x_{t-s},n-1)s,t-s,t-2s},t\\!-\\!1\\!},t\\!-\\!(11) \\[t-t-s} =f_mathbf{move}(x_{t-s},t-t-s},t-t-s)\\!\n' +
      '\n' +
      '그런 다음, 학생 네트워크 \\(f_{\\mathrm{student}}\\)를 학습하여 \\(x_{t}\\)에서 \\(x_{t-ns}\\)까지 직접 가리키는 방향 필드 \\(\\hat{u}_{t}\\)을 예측한다.\n' +
      '\n' +
      '\\[\\hat{u}_{t} =f_{\\mathrm{student}}(x_{t},t,c) \\tag{14}\\] \\[\\hat{x}_{t-ns} =\\mathbf{move}(x_{t},\\hat{u}_{t},t,t-ns) \\tag{15}\\]\n' +
      '\n' +
      '원래 작업은 MSE 손실[54]을 사용한다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathrm{mse}}=\\|\\hat{x}_{t-ns}-x_{t-ns}\\|_{2}^{2} \\tag{16}\\]\n' +
      '\n' +
      '학생 모형이 수렴하면 교사 모형으로 활용되며 증류 과정이 반복된다. 이론적으로는 1단계 생성 모델을 생성할 수 있지만 실제로는 모델이 흐릿한 결과를 생성한다. 우리는 3.1절에서 이 문제를 분석한다.\n' +
      '\n' +
      '### Adversarial Distillation\n' +
      '\n' +
      '적대적 훈련은 실제 샘플에서 생성된 샘플을 식별하는 것을 목표로 하는 판별기 네트워크와 판별기를 속이는 것을 목표로 하는 생성기 네트워크 사이의 최소 최적화(minimax optimization)를 포함한다. 원래 생성 네트워크의 독립형 클래스인 생성적 적대 네트워크(GANs, Generative Adversarial Networks) [8]로 제안되었지만 모드 붕괴 및 불안정성 등의 문제를 겪는다. 최근 연구에 따르면 적대적 목표는 확산 훈련[72] 및 증류[58, 73]에 통합될 수 있다.\n' +
      '\n' +
      'SDXL-Turbo[58]은 적대적 확산 증류를 사용하는 최신이고 가장 인기 있는 오픈 소스 모델이다. 사전 훈련된 이미지 인코더 DINOv2[41]를 판별기 백본으로 사용하여 훈련을 가속화하는 것은 선행 작업 [56, 57]을 따른다. 그러나, 이것은 몇 가지 한계를 가져온다. 첫째, 기성 비전 인코더를 사용하면 잠재 공간 대신 픽셀 공간에서 작동해야 하므로 계산, 메모리 소비 및 훈련 시간이 크게 증가하여 고해상도 증류가 비실용적이다. 이는 SDXL-터보가 최대 512px 해상도만을 지원하는 이유일 가능성이 높다. 둘째, 기성용 비전 인코더는 \\(t=0\\)에서만 작동한다. 증류 모델은 ODE 끝점 \\(x_{0}\\)으로 점프하도록 훈련되어야 하지만, 1단계 추론을 위한 품질이 충분하지 않기 때문에, 다중 단계 추론을 위해 랜덤 잡음이 다시 추가된다. 이러한 다단계 추론은 모델 동작을 상당히 변경하여 기존 LoRA 모듈 [16] 및 제어 플러그인 [74, 76, 10]과 호환성이 떨어진다. 셋째, 기성품 인코더는 다른 데이터셋(애니메이션, 라인아트, _etc._)을 찾기 어려울 수 있다. 및 모달리티(video, audio, _etc._). 이는 증류법의 일반화 가능성을 감소시킨다. 마지막으로 적대적 목적만으로는 모델이 동일한 ODE 흐름을 따르도록 강제하지 않으므로 모드 적용 범위가 강제되지 않는다.\n' +
      '\n' +
      '이 방법은 확산 모델의 U-Net 인코더를 판별기 백본으로 사용한다. 이를 통해 고해상도 모델을 위한 잠재 공간에서 효율적으로 증류할 수 있고 모든 타임스텝에서 차별을 지원하며 모든 데이터 세트와 모달리티에 일반화할 수 있다. 우리의 방법은 또한 나중에 섹션 3.2 및 3.4에서 논의된 바와 같이 품질과 모드 커버리지 사이의 절충을 제어할 수 있다.\n' +
      '\n' +
      '##### 기타 증류방법\n' +
      '\n' +
      '우리는 다른 증류 방법과 비교하여 접근법의 장점에 대해 간략하게 논의한다.\n' +
      '\n' +
      '일관성 모델(CM) [65, 66]은 또한 모든 추론 단계에서 ODE 종점으로 점프할 것을 요구한다. 이는 LoRA 모듈 및 플러그인과의 호환성을 감소시키는 다단계 샘플링에 대한 큰 모델 동작 변화를 야기한다. 이 방법은 SDXL[36, 37]에 적용되었지만 8단계에서는 생성 품질이 좋지 않다. 일관성 궤적 모델(CTM) [22]는 적대적 손실을 추가하고 임의의 흐름 위치에 점프를 지원하지만 적대적 훈련은 증류 중 대신 증류 후 적용되며 대규모 텍스트 대 이미지 모델에는 이 방법이 적용되지 않았다.\n' +
      '\n' +
      'Rectified Flow(RF) [31, 32]는 결정론적 데이터와 잡음 쌍으로 반복적으로 훈련하여 흐름을 직선화한다. 그러나 몇 단계 발전 품질은 여전히 좋지 않습니다. 또한, 모델은 증류 동안 특정 데이터와 노이즈 쌍만 보았기 때문에 더 이상 임의의 노이즈와 데이터 쌍을 지원하지 않는다. 이것은 SDEdit[38]과 같은 이미지 편집 기능에 영향을 미칩니다.\n' +
      '\n' +
      '점수 증류 샘플링(SDS)[45]은 적대적 훈련을 안정화하기 위해 SDXL-터보[58]에서 사용되었지만 그 효과는 미미하며 증류 방법만으로는 사용할 수 없다. 변동 점수 증류(VSD) [70]은 최근 확산 증류[75]에 사용되고 있다. 그러나 증류 과정에서 음의 분포에 대한 추가 점수 모델을 훈련해야 하며, 적대적 훈련의 판별자와 마찬가지로 훈련 안정성에 부정적인 영향을 미칠 수 있는 동적 훈련 목표도 포함한다. 비교를 위한 오픈 소스 모델은 없으며, 우리의 예비 실험은 우리의 방법이 더 나은 품질을 달성한다는 것을 발견했다.\n' +
      '\n' +
      '### LoRA\n' +
      '\n' +
      'Low-Rank Adaptation (LoRA) [16]은 효율적인 미세 조정 기법이다. 그것은 단지 소수의 추가적인 파라미터들을 모델에 훈련시키고, 기존의 텍스트-이미지 모델들에 대한 스타일화 모듈들을 훈련시키는 데 특히 인기 있게 되었다.\n' +
      '\n' +
      'LCM-LoRA[37]는 모델 증류가 LoRA 모듈로서 또한 훈련될 수 있다는 것을 보여주는 최초의 것이다. 이를 통해 최소한의 매개변수 변경을 보장하고 기존 생태계에 편리하게 연결할 수 있습니다.\n' +
      '\n' +
      '우리의 작업은 이 접근법에서 영감을 얻었으며 편리한 플러그 앤 플레이를 위한 LoRA와 훨씬 더 나은 품질을 위한 전체 모델로 증류된 모델을 제공합니다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### 왜 MSE 실패로 증류하는가\n' +
      '\n' +
      '학습된 ODE 흐름은 forward schedule[29, 31], loss function[27], model capacity에 의해 결정된다. 유한한 훈련 샘플이 주어지면 기본 데이터 분포가 모호하다. 최대우도 추정(MLE)은 관측된 표본에만 짝수 확률을 할당하고 다른 곳에서는 0을 할당하는 분포이다. 모형이 무한한 용량을 갖는다면, 관측된 샘플을 항상 생산하고 새로운 데이터를 생성하지 않기 위해 이 최대 우도 추정 및 과잉 적합의 흐름을 학습할 것이다. 실제로 확산 모델은 신경망이 완벽한 학습자가 아니기 때문에 새로운 데이터를 생성할 수 있다.\n' +
      '\n' +
      '모델을 다단계 세대로 사용하면 적층되며 더 복잡한 분포를 근사화하기 위해 립시츠 상수가 더 높고 비선형성이 더 높다. 그러나 모델을 몇 단계 세대로 사용하면 더 이상 동일한 분포를 근사할 수 있는 용량이 동일하지 않습니다. 이는 확산 모델이 초기 잡음[9]의 작은 변화에도 불구하고 결과의 매우 급격한 변화를 가질 수 있음을 입증하지만, 증류 모델은 훨씬 더 부드러운 잠재 횡단을 갖는다.\n' +
      '\n' +
      '이것은 MSE 손실이 있는 증류가 흐릿한 결과를 생성하는 이유를 설명한다. 학생 모델은 단순히 선생님과 매칭할 수 있는 능력을 가지고 있지 않다. 우리는 L1 및 지각 손실[27, 77]과 같은 다른 거리 메트릭도 바람직하지 않은 결과를 생성한다는 것을 발견했다. 반면에 우리는 적대적 목적이 이 문제를 완화하는 데 효과적이라는 것을 발견한다.\n' +
      '\n' +
      '### Adversarial Objective\n' +
      '\n' +
      '식 (16)과 같이 학생 예측 \\(\\hat{x}_{t-ns}\\)과 교사 예측 \\(x_{t-ns}\\) 사이의 MSE 손실을 사용하는 대신 적대적 판별기를 사용한다. 구체적으로, 판별자 \\(D:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\in[0,1]\\)은 주어진 조건 \\(x_{t}\\)과 \\(c\\)이 주어졌을 때 학생과는 반대로 교사로부터 \\(x_{t-ns}\\)이 생성될 확률을 계산한다.\n' +
      '\n' +
      '\\[D(x_{t},x_{t-ns},t,t-ns,c) \\tag{17}\\]\n' +
      '\n' +
      '우리는 비포화 적대적 손실[8]을 사용하고 판별기와 학생 모델을 교대로 훈련한다. 이것은 학생 예측 \\(\\hat{x}_{t-ns}\\)이 교사 예측 \\(x_{t-ns}\\)에 더 가깝도록 장려한다:\n' +
      '\n' +
      '\\[p=D(x_{t},x_{t-ns},t,t-ns,c) \\tag{18}\\] \\[\\hat{p} =D(x_{t},\\hat{x}_{t-ns},t,t-ns,c)\\] (19) \\[\\mathcal{L}_{D} = -\\log(p)-\\log(1-\\hat{p})\\] (20) \\[\\mathcal{L}_{G} = -\\log(\\hat{p}) \\tag{21}\\]\n' +
      '\n' +
      'ODE 유동을 보존하기 위해서는 \\(x_{t}\\)의 조건이 중요하다. 이는 교사의 \\(x_{t-ns}\\) 생성이 \\(x_{t}\\)으로부터 결정론적이기 때문이다. 판별기는 \\(x_{t-ns}\\)와 \\(x_{t}\\)을 함께 제공함으로써, 판별기는 기본 ODE 흐름을 학습하고 학생도 같은 흐름을 따라 판별기를 속여야 한다.\n' +
      '\n' +
      '우리의 제형은 처음부터 훈련하는 대신 증류에 사용하는 것을 제외하고는 이전 작업[72]과 매우 유사하다. 이 접근법은 ODE 흐름만 보존하고 증류에 사용할 때 모드 적용 범위를 보장한다는 점에 유의해야 한다.\n' +
      '\n' +
      '### Discriminator Design\n' +
      '\n' +
      '이전 작업 [27]은 미리 훈련된 확산 모델의 U-Net [52] 인코더가 비전 백본으로 사용될 수 있음을 보여주었다. 이러한 사전 훈련된 백본은 대상 데이터셋에서 사전 훈련되고 잠재 공간에서 직접 작동하며 모든 타임스텝에서 잡음 입력을 지원하며 텍스트 조건을 지원하기 때문에 판별기에 매우 적합하다.\n' +
      '\n' +
      '본 논문에서는 판별기 백본으로 미리 학습된 SDXL 모델의 인코더와 중간 블록을 복사한다. 공유 백본 \\(d\\)을 통해 \\(x_{t-ns}\\)과 \\(x_{t}\\)을 독립적으로 통과시키고, 중간 블록 뒤에 숨겨진 특징들을 채널 차원으로 연결하여 예측 헤드로 전달한다. 예측 헤드는 단순하게 구성된다\n' +
      '\n' +
      '그림 1: 용량이 다른 모델에 의해 학습된 여러 가능한 흐름의 그림. 몇 단계 세대에 대한 증류된 학생 모델은 교사 모델과 일치하는 능력이 동일하지 않아 MSE 손실과 함께 흐릿한 결과를 초래한다.\n' +
      '\n' +
      '2의 스트라이드를 갖는 \\(4\\times 4\\) 컨볼루션, 32개의 그룹을 갖는 그룹 정규화 [71] 및 공간 차원을 더 감소시키기 위한 SiLU 활성화 [11, 48] 레이어의 블록. 출력은 단일 값으로 투영되고 시그모이드\\(\\sigma(\\cdot)\\)로 \\([0,1]\\) 범위로 클램핑된다. 그들은 함께 완전한 판별자 \\(D\\):\n' +
      '\n' +
      '{split} D&(x_{t},x_{t-ns},t,t-ns,c)\\\\\\mathrm{head}\\Big{(}d(x_{t-ns},t-ns,c),d(x_{t},t,c)\\Big{}\\bigg{}\\end{split}\\tag{22}\\\\mathrm{head}\\Big{(}d(x_{t-ns},t-ns,c),d(x_{t},t,c)\\Bigg{}\\end{split}\\tag{22}\\mathrm{head}\\Big{(}d(x_{t-ns},t,c)\\mathrm{head}\\Big{(x_{t-ns},t,t,c)\\Bigg{}\\end{split}\\tag{22}\\mathrm{head}\\Big{(x_{t-ns},t,t,c)\\mathrm{head}\\Big{(x_{t-ns\n' +
      '\n' +
      '백본은 미리 훈련된 가중치로 초기화되며 백본을 동결하지 않고 전체 판별기를 훈련한다. 우리는 확장 R1 정규화[39]나 L2 주의 집중으로 전환할 필요 없이 훈련이 안정적이라고 생각한다[17, 23]. 추가 안정화 기술은 섹션 3.7에서 논의된다.\n' +
      '\n' +
      '### 모드 커버리지를 완화합니다\n' +
      '\n' +
      '위의 적대적 목표는 예상이 날카롭고 흐름을 보존하도록 장려하지만, 이는 3.1절에서 논의한 바와 같이 학생이 교사와 완벽하게 일치할 수 있는 충분한 능력을 가지고 있지 않다는 사실을 변경하지 않으며 MSE 목표와 함께 흐릿한 결과를 나타낸다. 적대적 목적을 가지고, 그것은 "Janus"의 유물들을 보여준다\n' +
      '\n' +
      '그림 2에 도시된 바와 같이, 교사 모델은 때때로 인접한 잡음 입력에 대해 급격한 레이아웃 변화를 생성할 수 있지만, 학생 모델은 그러한 급격한 변화를 만들기 위한 동일한 용량을 갖지 않는다. 결과적으로, 적대적 손실은 첨예함과 레이아웃을 보존하기 위해 필요한 의미적 정확성을 희생시키며, 결합된 머리와 몸체를 특징으로 하는 아티팩트를 나타낸다.\n' +
      '\n' +
      '인간의 선호에 의한 모드 커버리지보다 의미적 정확성이 더 중요하다. 따라서 본연의 적대적 목표를 가지고 훈련한 후 흐름 보존 요구 사항을 완화한다. 구체적으로, 우리는 \\(x_{t}\\)의 조건 없이 모델을 추가로 미세화한다:\n' +
      '\n' +
      '{split}D^{\\prime}&(x_{t-ns},t-ns,c)\\\\\\mathrm{head}\\Big{(}d(x_{t-ns},t-ns,c)\\Big{}\\bigg{}\\end{split}\\tag{23}\\mathrm{head}\\Big{(}d(x_{t-ns},t-ns,c)\\Big{}\\bigg{}\\end{split}\\tag{23}\\mathrm{head}\\Big{(}d(x_{t-ns},t-ns,c)\\equiv\\sigma\\bigg{}}\n' +
      '\n' +
      '우리는 이 목적을 이용한 미세조정이 "Janus" 유물을 제거하는 데 효과적이라는 것을 발견했으며, 여전히 실제적으로 원래의 흐름을 상당 부분 보존하고 있다. 따라서 점진적 증류의 모든 단계에서 먼저 조건부 목표로 훈련한 다음 이 무조건적 목표로 최종 조정한다. 무조건적인 목표는 샘플당 품질에만 영향을 미치기 때문에 우리는 품질을 추가로 유지하고 오류 축적을 완화하기 위해 1단계 및 2단계 모델을 증류하는 스킵 수준 교사를 사용한다.\n' +
      '\n' +
      '### 일정 수정\n' +
      '\n' +
      '이전 작업 [26]은 공통 확산 스케줄에 결함이 있음을 보여줍니다. 구체적으로, 훈련 시 일정은 \\(t=T\\)에서 순수한 잡음에 도달하지 않지만 추론 시 순수한 잡음이 주어져 불일치가 발생한다. 불행히도 SDXL은 이 결함이 있는 일정을 사용합니다. 그 효과는 많은 수의 추론 단계에서 덜 분명하지만 몇 단계 세대에는 특히 해롭다.\n' +
      '\n' +
      '이 문제를 피할 수 있는 해키한 방법은 훈련 중에 \\(t=T\\)에서 모델 입력으로 순수 잡음 \\(\\epsilon\\)을 하드 스왑하는 것이다. 이러한 방법으로 모델은 \\(t=T\\)에서 입력으로 순수한 잡음을 기대하도록 학습되며, 우리는 여전히 특이점을 피하기 위해 추론에서 오래된 \\(\\bar{\\alpha}\\) 스케줄과 함께 식 (3)을 사용한다. 그것은 기존의 소프트웨어 생태계로 샘플링 절차에 최소한의 변화를 초래한다[69]. 이 접근법은 또한 SDXL-Turbo[58]에 의해 사용된다.\n' +
      '\n' +
      '\\begin{split}&\\mathbf{Forward}(x_{0},\\epsilon,t)\\begin{cases}\\mathbf{forward}(x_{0},\\epsilon,t),&\\text{when }t<T\\\\epsilon,&\\text{when }t=T\\end{cases}\\end{split}\\tag{24}\\text{when }t<T\\\\epsilon,\\text{when }t=T\\end{cases}\\end{split}\\tag{24}\\mathbf{forward}(x_{0},\\epsilon,t)\n' +
      '\n' +
      '### Distillation Procedure\n' +
      '\n' +
      '먼저 MSE 손실로 128단계에서 32단계로 직접 증류를 수행한다. 우리는 MSE가 초기 단계에 충분하다는 것을 발견한다. 또한 이 단계에서만 분류기 없는 안내(CFG) [15]를 적용한다. 부정적인 프롬프트 없이 6의 안내 척도를 사용합니다.\n' +
      '\n' +
      '그런 다음, 이 순서로 스텝 카운트를 증류하기 위해 적대적 손실을 사용하는 것으로 전환한다: \\(32\\~8\\~4\\~2\\~1\\). 각 단계에서 먼저 ODE 흐름을 보존하기 위해 섹션 3.2와 같은 조건부 목적으로 훈련한 다음 모드 커버리지를 완화하기 위해 섹션 3.4와 같은 무조건적 목적으로 훈련한다.\n' +
      '\n' +
      '각 단계에서 먼저 두 가지 목표를 사용하여 LoRA로 훈련한 다음 LoRA를 병합하고 무조건적인 목표와 함께 전체 UNet을 더 훈련한다. 우리는 전체 UNet을 미세 조정하면 더 나은 성능을 얻을 수 있는 반면 LoRA 모듈은 다른 기본 모델에 사용할 수 있음을 발견했다. 우리의 LoRA 설정은 LCM-LoRA [37]과 동일하며, 입력 및 출력 컨볼루션과 공유 시간 임베딩 선형 레이어를 제외한 모든 컨볼루션 및 선형 가중치에 대해 순위 64를 사용한다. 우리는 차별자에게 LoRA를 사용하지 않는다. 우리는 각 단계에서 판별자를 다시 초기화한다.\n' +
      '\n' +
      '그림 2: "Janus" 유물은 학생 네트워크가 교사의 갑작스러운 변화에 부합할 수 있는 능력이 없을 때 나타난다. 이 문제는 모드 커버리지 요건을 완화함으로써 완화될 수 있다.\n' +
      '\n' +
      '우리는 LAION [59] 및 COYO [4] 데이터 세트의 하위 집합에 모델을 증류한다. 미학적 점수가 5.5 이상인 1024px 이상의 이미지를 선택하고 라플라시안 필터를 사용하여 선명도에 따라 이미지를 필터링하고 텍스트 프롬프트를 정리한다. 증류는 정사각형 종횡비에 대해 수행되지만, 추론 시간에 다른 종횡비에 대해 잘 일반화되는 것을 발견했다.\n' +
      '\n' +
      '64 A100 80G GPU에 걸쳐 배치 크기 512를 사용합니다. MSE 손실이 있는 첫 번째 \\(128\\~32\\) 단계에서는 Adam \\(\\beta_{1}=0.9,\\beta_{2}=0.999\\)으로 학습률 1e-5를 사용한다. 적대적 손실이 있는 나머지 단계의 경우 학생과 판별자 네트워크 모두에 대해 LoRA가 있는 학습률 1e-6과 LoRA가 없는 5e-7을 사용한다. 애덤 최적화기 [24]는 중량감소가 없는 선행 연구 [17, 21]에 이어 \\(\\beta_{1}=0,\\beta_{2}=0.99\\)을 사용한다. 메모리 풋프린트를 줄이기 위해 기울기 축적, VAE 슬라이싱, BF16 혼합 정밀도[40], 플래시 주의[6, 7] 및 제로 리던던시 최적화기[47]를 사용한다.\n' +
      '\n' +
      '안전한 훈련 기술\n' +
      '\n' +
      '1단계 및 2단계 증류에서는 훈련을 안정화하기 위해 추가 기술을 사용한다.\n' +
      '\n' +
      '다중 타임스탬프에서의 훈련 학생 네트워크\n' +
      '\n' +
      '우리는 완전한 이미지 생성을 위해 타임스텝 {1000}에서 1단계 모델, 타임스텝 {500, 1000}에서 2단계 모델만 훈련하면 되지만, 더 많은 타임스텝 {250, 500, 750, 1000}에서 훈련하면 안정성이 향상된다. 추가적인 이점으로서, 이것은 우리 모델들이 아래 예시된 바와 같이 상이한 타임스텝들에서 SDEdit[38]을 지원할 수 있게 한다:\n' +
      '\n' +
      '다중 타임스텝에서의 열차 판별기 3.7.2\n' +
      '\n' +
      '우리는 위의 판별기 공식을 사용하여 1단계 모델을 훈련하는 것이 매우 불안정하다는 것을 발견했다. 근본적인 이유는 판별기가 미리 훈련된 확산 UNet 인코더를 백본으로 사용하지만, 확산 인코더는 낮은 타임스텝에서 높은 주파수 세부사항과 높은 타임스텝에서 낮은 주파수 구조에만 집중하도록 훈련되기 때문이다. 1단계 세대 동안 학생 네트워크는 \\(\\hat{x}_{0}\\)을 직접 예측한다. 판별기 백본에 \\(\\hat{x}_{0}\\)과 \\(t=0\\)을 전달하면 이미지 구조를 평가할 수 없다. 이것은 나쁜 모양과 심지어 발산을 가진 이미지로 이어진다.\n' +
      '\n' +
      '우리의 해결책은 교사가 예측한 \\(x_{0}\\)와 학생이 예측한 \\(\\hat{x}_{0}\\)에 무작위로 {10, 250, 500, 750}의 시간을 추가하는 것이다. 이러한 방식으로 판별기는 고주파 세부 사항과 저주파 구조 모두에 대한 예측을 비판할 수 있다.\n' +
      '\n' +
      '구체적으로, 우리는 먼저 1:1:1의 균일한 가중치를 갖는 \\(t*\\leftarrow\\{10,250,500,750\\}\\)을 그리고 새로운 잡음 \\(\\epsilon*\\sim\\mathcal{N}(0,\\mathbf{I})을 샘플링한다. 그런 다음 판별기를 통과하기 전에 노이즈를 적용합니다:\n' +
      '\n' +
      '\\[D(x_{t},\\mathbf{forward}(\\hat{x}_{0},\\epsilon*,t*),t,t*,c) \\tag{25}\\]\n' +
      '\n' +
      '\\[D^{\\prime}(\\mathbf{forward}(\\hat{x}_{0},\\epsilon*,t*),t*,c) \\tag{26}\\]\n' +
      '\n' +
      '모델이 안정적으로 훈련된 후, 타임스테이프 가중치를 5:1:1:1로 변경함으로써, 디테일을 더욱 향상시키고 잡음 아티팩트를 제거한다.\n' +
      '\n' +
      '이 안정화 기술은 또한 분포 갭을 브리징하는 렌즈[39], 판별기 증강[20], 및 다중 스케일 판별기[18]로부터 볼 수 있다는 점에 유의한다.\n' +
      '\n' +
      '###### 3.7.3 \\(x_{0}\\) 예측으로의 전환\n' +
      '\n' +
      '우리는 \\(\\epsilon\\)-예측 공식을 갖는 1단계 모델이 수치적 불안정성으로 인해 잡음 아티팩트를 생성하는 경향이 있음을 발견했다. 우리는 1단계 모델을 \\(x_{0}\\)-예측으로 변경하고 문제를 해결한다.\n' +
      '\n' +
      '구체적으로, 우리는 네트워크를 복사하고 식 (3)에 정의된 변환 함수 \\(\\mathbf{x}\\)을 통해 예측된 \\(\\hat{\\epsilon}\\)을 \\(\\hat{x}_{0}\\)으로 변환한다. 우리는 MSE를 사용하여 온라인 모델을 \\(x_{0}\\)-예측으로 점진적으로 안내한다.\n' +
      '\n' +
      '\\[\\hat{\\epsilon} =f_{\\mathrm{frozen}}(x_{t},t,c) \\tag{27}\\] \\[\\hat{x}_{0} =f_{\\mathrm{online}(x_{t},t,c)\\] (28) \\[\\mathcal{L}_{\\mathrm{convert} =\\|\\hat{x}_{0}-\\mathbf{x}(x_{t},\\hat{\\epsilon},t)\\|_{2}^{2} \\tag{29}\\]\n' +
      '\n' +
      '변환 후, 1단계 모델은 \\(x_{0}\\)-예측 공식에서 적대적 목표로 훈련되는 반면, 교사 모델은 여전히 \\(\\epsilon\\)-예측 공식에서 작동한다. 실질적인 제형 변경으로 인해 1단계 생성을 위한 LoRA를 제공하지 않습니다.\n' +
      '\n' +
      '## 4 Evaluation\n' +
      '\n' +
      '### Specification Comparison\n' +
      '\n' +
      '표 1은 다른 모델에 비해 증류된 모델의 사양을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multirow{2}{*}{Method} & Steps & \\multirow{2}{*}{Resolution} & CFG & Offer \\\\  & Needed & & Free & LoRA \\\\ \\hline SDXL [44] & 25+ & 1024px & No & - \\\\ LCM [36, 37] & 4+ & 1024px & Yes\\&No & Yes \\\\ Turbo [58] & 1+ & 512px & Yes & No \\\\\n' +
      '**Ours** & **1+** & **1024px** & **Yes** & **Yes** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 모델 사양. 우리의 방법은 고품질 샘플을 생산하기 위해 가장 적은 양의 단계를 필요로 한다.\n' +
      '\n' +
      '그림 3: 질적 비교. 이곳의 모델은 LoRA 대신 완전히 훈련되었습니다. 모델은 512px 해상도만 지원하기 때문에 SDXL-터보를 제외하고 각 프롬프트에 대해 동일한 초기 노이즈가 제공된다. 우리의 모델은 모든 프롬프트에서 최상의 결과를 생성하며, 원래 모델의 스타일과 레이아웃을 가장 잘 보존합니다. 참고 1: 원본 SDXL과 우리는 오일러 샘플러를 사용하는 반면 다른 방법은 기본 샘플러를 사용한다. 참고 2: 일부 방법은 추론 시 분류기 없는 안내(CFG)를 필요로 하며, 이는 함수 평가(NFE)의 수를 배가시키고 계산을 배가시킨다. (전체 해상도로 보려면 확대하십시오.)\n' +
      '\n' +
      '### Qualitative Comparison\n' +
      '\n' +
      '그림 3은 SDXL-터보 [58] 및 LCM [36]과 같은 다른 오픈 소스 증류 모델과 우리의 방법을 비교한다. 우리의 방법은 전반적인 품질과 세부 사항에서 상당히 더 우수하다. 우리의 방법은 또한 원래 모델의 스타일과 레이아웃을 보존하는 데 상당히 더 우수하다. 또한, 본 논문에서 제안한 4-step과 8-step 모델이 기존의 SDXL 모델보다 32단계 성능이 우수함을 보인다. 이것은 우리의 점진적인 증류가 128단계부터 시작되기 때문입니다.\n' +
      '\n' +
      '그림 4는 LoRA 모델을 완전히 훈련된 모델과 비교한다. 우리는 완전히 훈련된 모델이 더 나은 구조와 세부 사항을 가지고 있다는 것을 발견했다. 이것은 8단계 모델에서는 덜 두드러지지만 2단계 모델에서는 더 관찰 가능하다.\n' +
      '\n' +
      '### Quantitative Comparison\n' +
      '\n' +
      '표 2는 FID(Frechet Inception Distance) [42, 12] 및 CLIP 점수 [46]을 나타낸다. 관례에 따라 COCO [28] 검증 데이터 세트에서 처음 10K 프롬프트를 사용하여 이미지를 생성한다. FID 메트릭은 COCO로부터의 대응하는 지상 진실 이미지에 대해 계산된다.\n' +
      '\n' +
      'FID는 일반적으로 InceptionV3 네트워크에 대해 전체 이미지의 크기를 299px로 조정함으로써 계산된다[68]. 이것은 높은 수준의 샘플 품질과 다양성만을 평가합니다. 이 메트릭은 우리의 모델이 다른 증류 기술과 유사한 성능을 달성한다는 것을 보여준다. 모든 증류 방법은 다양성의 감소로 인해 원래 SDXL에 비해 FID가 더 나쁘다.\n' +
      '\n' +
      '또한 고해상도 세부 정보를 평가하기 위해 이미지 패치에서 FID를 계산할 것을 제안한다. 구체적으로, 우리는 모든 이미지의 299px 중심 크롭 패치에서 FID를 계산한다. 터보의 경우 공정한 비교를 위해 작물 전에 512px를 1024px로 조정한다. 이 메트릭은 우리 모델이 다른 방법에 비해 훨씬 더 나은 고해상도 세부 정보를 가지고 있음을 보여줍니다. 또한 메트릭은 우리 모델이 origi에 비해 고해상도 세부 정보가 더 우수하다는 것을 보여줍니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & Steps & \\begin{tabular}{c} FID \\(\\downarrow\\) \\\\ (Whole) \\\\ \\end{tabular} & \\begin{tabular}{c} FID \\(\\downarrow\\) \\\\ (Patch) \\\\ \\end{tabular} & CLIP \\(\\uparrow\\) \\\\ \\hline \\hline \\multirow{3}{*}{\\begin{tabular}{l} DXL [44] \\\\ \\end{tabular} } & 32 & 18.49 & 35.89 & 26.48 \\\\ \\cline{2-5}  & 1 & 80.01 & 158.90 & 23.65 \\\\ \\cline{2-5}  & 4 & 21.85 & 42.53 & 26.09 \\\\ \\cline{2-5}  & 4 & 21.50 & 40.38 & 26.18 \\\\ \\hline \\hline \\multirow{3}{*}{\\begin{tabular}{l} Ours \\\\ \\end{tabular} } & 1 & 23.71 & 43.69 & 26.36 \\\\ \\cline{2-5}  & 4 & 22.58 & 42.65 & 26.18 \\\\ \\hline \\hline \\multirow{3}{*}{\\begin{tabular}{l} Ours \\\\ \\end{tabular} } & 1 & 22.61 & **41.53** & 26.02 \\\\ \\cline{2-5}  & 2 & 23.11 & **35.12** & 25.98 \\\\ \\cline{1-1} \\cline{2-5}  & 4 & 22.30 & **33.52** & 26.07 \\\\ \\cline{1-1} \\cline{2-5}  & 8 & 21.43 & **33.55** & 25.86 \\\\ \\hline \\hline \\multirow{3}{*}{\n' +
      '\\begin{tabular}{l} Ours-LoRA \\\\ \\end{tabular} } & 2 & 23.39 & **40.54** & 26.18 \\\\ \\cline{1-1} \\cline{2-5}  & 4 & 23.01 & **34.10** & 26.04 \\\\ \\cline{1-1} \\cline{2-5}  & 8 & 22.30 & **33.92** & 25.77 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 정량적 비교. FID-Whole은 높은 수준의 다양성과 품질을 반영한다. FID-패치는 고해상도 디테일을 반영합니다. CLIP 점수는 텍스트 정렬을 반영합니다. 우리의 모델은 다양성과 텍스트 정렬에서 유사한 성능을 유지하면서 훨씬 더 나은 고해상도 세부 정보를 가지고 있습니다.\n' +
      '\n' +
      '그림 4: 완전 훈련 모델과 LoRA 훈련 모델 간의 비교. 풀 UNet을 훈련하는 것은 약간 더 나은 세부 사항을 가지고 있지만, 우리의 LoRA 모델도 매우 고품질입니다. 참고로 우리는 1단계 LoRA를 제공하지 않습니다. (전체 해상도에서 보려면 확대하십시오.) 당사의 증류는 128단계부터 시작되므로 32단계에 대한 SDXL 모델입니다. 또한 추론 단계의 수가 감소함에 따라 품질이 저하됨을 보여준다.\n' +
      '\n' +
      'CLIP 점수는 우리의 방법이 다른 방법과 비교하여 유사한 텍스트 정렬 성능을 달성한다는 것을 보여준다.\n' +
      '\n' +
      '## 5 Ablation\n' +
      '\n' +
      '### 다른 베이스 모델에 LoRA 적용\n' +
      '\n' +
      '그림 5는 우리의 증류 LoRA 모델이 다른 기본 모델에 적용될 수 있음을 보여준다. 구체적으로 타사 만화 [55], 애니메이션 [1], 사실적인 [50] 기본 모델에 대해 테스트합니다. 당사의 증류 LoRA는 새로운 베이스 모델의 스타일과 레이아웃을 크게 유지할 수 있습니다.\n' +
      '\n' +
      '양상비가 다른### 추론\n' +
      '\n' +
      '그림 6은 증류가 정사각형 이미지에서만 수행되었음에도 불구하고 모델이 대부분 다른 분해능과 종횡비로 추론할 수 있는 능력을 유지할 수 있음을 보여준다. 그러나 1단계 및 2단계 세대를 수행할 때 점점 더 많은 불량 사례가 발생하는 것을 알 수 있습니다. 이는 다중 종횡비로 증류함으로써 개선될 수 있으며, 이는 향후 개선을 위해 남겨둔다.\n' +
      '\n' +
      '###ControlNet과의 호환성\n' +
      '\n' +
      '도 7은 우리의 모델들이 ControlNet[76]과 호환된다는 것을 보여준다. 캐니 에지 [60] 및 깊이 [61] 컨트롤넷에서 테스트합니다. 우리는 우리의 모델이 추론 단계의 수가 감소함에 따라 약간의 품질 저하와 함께 조건을 올바르게 따르는 것을 관찰한다.\n' +
      '\n' +
      '## 6 Limitation\n' +
      '\n' +
      '다중 추론 단계 설정을 지원하는 단일 증류 체크포인트를 갖는 다른 방법 [36, 37, 58]과 달리, 본 방법은 각 해당 추론 단계 설정에 대해 별도의 체크포인트를 생성한다. 추론 단계의 수가 고정되어 있을 때 이것은 일반적으로 생산에서 문제가 되지 않는다. 추론 단계의 수가 유연해야 하는 경우, LoRA 모듈은 체크포인트 전환 문제를 완화할 수 있다.\n' +
      '\n' +
      '제안하는 방법은 교사 모델과 동일한 구조의 증류된 학생 모델을 생성한다. 그러나 우리는 UNet 아키텍처가 1단계 생성에 최적이 아니라고 믿습니다. 각 UNet 계층에서 특징맵을 검사하고, 대부분의 생성은 디코더에 의해 수행됨을 알 수 있다. 우리는 이 문제를 미래의 개선에 맡긴다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '요약하자면, 우리는 새로운 점진적 적대 확산 증류 방법으로 인한 최첨단 1단계/소단계 텍스트-이미지 생성 모델인 SDXL-번개를 제시했다. 우리의 평가에서, 우리는 우리의 모델이 이전 작업들에 비해 우수한 이미지 품질을 생산한다는 것을 발견했다. 우리는 생성 AI 연구를 발전시키기 위해 모델을 오픈소싱하고 있습니다.\n' +
      '\n' +
      '그림 5: 우리의 증류 LoRA는 다른 기본 모델, 만화 [55], 애니메이션 [1], 사실적인 [50] 기본 모델에 적용될 수 있다.\n' +
      '\n' +
      '그림 6: 우리의 모델은 정사각형 이미지에서만 훈련되지만 여전히 다른 종횡비를 생성할 수 있다. 예제 이미지는 4단계 모델에 의해 생성된 1:2 종횡비 720\\(\\times\\)1440px이다.\n' +
      '\n' +
      '도 7: 우리의 모델들은 ControlNet[76]과 호환된다. 표시된 예는 캐니 에지 및 깊이에 조건화된 생성이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]AAM-XL Anime Mix. [https://civitai.com/models/269232.9] (https://civitai.com/models/269232.9)\n' +
      '* [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023.\n' +
      '* [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models, 2023.\n' +
      '* [4] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. [https://github.com/kakaobrain/coyo-dataset](https://github.com/kakaobrain/coyo-dataset), 2022.\n' +
      '* [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-\\(\\alpha\\): Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023.\n' +
      '* [6] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.\n' +
      '* [7] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022.\n' +
      '* [8] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.\n' +
      '* [9] Jiayi Guo, Xingqian Xu, Yifan Pu, Zanlin Ni, Chaofei Wang, Manushee Vasu, Shiji Song, Gao Huang, and Humphrey Shi. Smooth diffusion: Crafting smooth latent spaces in diffusion models, 2023.\n' +
      '* [10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning, 2023.\n' +
      '* [11] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023.\n' +
      '* [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 6626-6637, 2017.\n' +
      '* [13] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models, 2022.\n' +
      '* [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle, Marc\'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.\n' +
      '* [15] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022.\n' +
      '* [16] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.\n' +
      '* [17] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis, 2023.\n' +
      '* [18] Animesh Karnewar and Oliver Wang. MSG-GAN: multi-scale gradients for generative adversarial networks. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020_, pages 7796-7805. IEEE, 2020.\n' +
      '* [19] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models, 2022.\n' +
      '* [20] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. In Hugo Larochelle, Marc\'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.\n' +
      '* [21] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020_, pages 8107-8116. IEEE, 2020.\n' +
      '* [22] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion, 2023.\n' +
      '* [23] Hyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of self-attention. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 5562-5571. PMLR, 2021.\n' +
      '* [24] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015.\n' +
      '* [25] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun, editors, _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings_, 2014.\n' +
      '\n' +
      '* [26] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed, 2023.\n' +
      '* [27] Shanchuan Lin and Xiao Yang. Diffusion model with perceptual loss, 2024.\n' +
      '* [28] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context, 2015.\n' +
      '* [29] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023.\n' +
      '* [30] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.\n' +
      '* [31] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022.\n' +
      '* [32] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. Instaffow: One step is enough for high-quality diffusion-based text-to-image generation, 2023.\n' +
      '* [33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019.\n' +
      '* [34] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps, 2022.\n' +
      '* [35] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023.\n' +
      '* [36] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference, 2023.\n' +
      '* [37] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: A universal stable-diffusion acceleration module, 2023.\n' +
      '* [38] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.\n' +
      '* [39] Lars M. Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 3478-3487. PMLR, 2018.\n' +
      '* 5월 3일, 2018, Conference Track Proceedings_. 2018년 오픈 리뷰 닷넷\n' +
      '* [41] Maxime Oquab, Timothee Darect, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khialkov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaeddin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023.\n' +
      '* [42] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation, 2022.\n' +
      '* [43] Pablo Pernias, Dominic Rampas, Mats L. Richter, Christopher J. Pal, and Marc Aubreville. Wuerstchen: An efficient architecture for large-scale text-to-image diffusion models, 2023.\n' +
      '* [44] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\n' +
      '* [45] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion, 2022.\n' +
      '* [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR, 2021.\n' +
      '* [47] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models, 2020.\n' +
      '* [48] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2017.\n' +
      '* [49] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022.\n' +
      '* [50] RealVisXL V4.0. [https://civitai.com/models/139562](https://civitai.com/models/139562).\n' +
      '* [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022.\n' +
      '* [52] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015.\n' +
      '* [53] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022.\n' +
      '\n' +
      '* [54] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.\n' +
      '* [55] Samaritan 3D Cartoon V4. [https://civitai.com/mmodels/81270.9](https://civitai.com/mmodels/81270.9)\n' +
      '* [56] Axel Sauer, Kashyap Chitta, Jens Muller, and Andreas Geiger. Projected gans converge faster. In Marc\'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 17480-17492, 2021.\n' +
      '* [57] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis, 2023.\n' +
      '* [58] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation, 2023.\n' +
      '* [59] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022.\n' +
      '* [60] SDXL-ControlNet Canny. [https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0.9](https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0.9)\n' +
      '* [61] SDXL-ControlNet Depth. [https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0.9](https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0.9)\n' +
      '* [62] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Garfini, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data, 2022.\n' +
      '* [63] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis R. Bach and David M. Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015_, volume 37 of _JMLR Workshop and Conference Proceedings_, pages 2256-2265. JMLR.org, 2015.\n' +
      '* [64] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.\n' +
      '* [65] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models, 2023.\n' +
      '* [66] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models, 2023.\n' +
      '* [67] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.\n' +
      '* [68] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016_, pages 2818-2826. IEEE Computer Society, 2016.\n' +
      '* [69] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. [https://github.com/huggingface/diffusers](https://github.com/huggingface/diffusers), 2022.\n' +
      '* [70] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation, 2023.\n' +
      '* [71] Yuxin Wu and Kaiming He. Group normalization, 2018.\n' +
      '* [72] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.\n' +
      '* [73] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generation via diffusion gans, 2023.\n' +
      '* [74] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-to-image diffusion models, 2023.\n' +
      '* [75] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T. Freeman, and Taesung Park. One-step diffusion with distribution matching distillation, 2023.\n' +
      '* [76] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023.\n' +
      '* [77] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018_, pages 586-595. IEEE Computer Society, 2018.\n' +
      '* [78] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor-corrector framework for fast sampling of diffusion models, 2023.\n' +
      '* [79] Chuanxia Zheng, Long Tung Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation, 2022.\n' +
      '* [80] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>