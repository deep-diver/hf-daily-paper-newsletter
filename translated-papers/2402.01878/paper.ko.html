<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# LiPO: Learning to Rank를 통한 Listwise Preference Optimization\n' +
      '\n' +
      'Tianqi Liu\n' +
      '\n' +
      'Zhen Qin\n' +
      '\n' +
      'Junru Wu\n' +
      '\n' +
      'Jiaming Shen\n' +
      '\n' +
      'Misha Khalman\n' +
      '\n' +
      'Rishabh Joshi\n' +
      '\n' +
      'Yao Zhao\n' +
      '\n' +
      'Mohammad Saleh\n' +
      '\n' +
      'Simon Baumgartner\n' +
      '\n' +
      'Jialu Liu\n' +
      '\n' +
      '유피터\n' +
      '\n' +
      'Xuanhui Wang\n' +
      '\n' +
      '동등한 기여도 \\({}^{1}\\)Google Research \\({}^{2}\\)Google Deepmind. Tianqi Liu \\(<\\)tianqiliu@google.com\\(>\\), Zhen Qin \\(<\\)zhenqin@google.com\\(>\\)에 해당한다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '언어 모델(LMs)을 선별된 인간 피드백과 정렬하는 것은 실제 응용 프로그램에서 그들의 행동을 제어하는 데 중요하다. DPO 및 SLiC와 같은 최근의 몇 가지 정책 최적화 방법은 전통적인 인간 피드백 강화 학습(RLHF) 접근법의 유망한 대안으로 작용한다. 실제로 인간의 피드백은 읽기 프롬프트 비용을 상각하기 위해 여러 응답에 걸쳐 순위가 매겨진 목록의 형식으로 오는 경우가 많다. 복수의 응답은 보상 모델 또는 AI 피드백에 의해 순위가 매겨질 수도 있다. 응답 목록의 직접 적합에 대한 그러한 연구는 부족하다. 본 연구에서는 LM 정렬을 _listwise_ 순위 문제로 공식화하고 리스트와이즈 선호 최적화(Listwise Preference Optimization, LiPO) 프레임워크를 설명하며, 여기서 정책은 프롬프트가 주어진 그럴듯한 응답들의 순위화된 리스트로부터 잠재적으로 더 효과적으로 학습할 수 있다. 이 견해는 대부분의 기존 선호도 최적화 작업이 기존 순위 목표, 특히 쌍별 목표에 매핑될 수 있는 학습 대 순위(LTR)에 대한 명시적인 연결을 그린다. 이 연결에 따라 목록 크기가 2인 경우 DPO 및 SLiC와의 LM 정렬에 대해 잘 연구되지 않은 순위 목표에 대한 검사를 제공한다. 특히, 본 논문에서는 LiPO-\\(\\lambda\\) 방법을 제안한다. LiPO-\\(\\lambda\\) 방법은 최첨단의 _listwise_순위객관을 이용하여 각 선호쌍에 가중치를 부여하여 보다 진보된 방법으로 가중치를 부여한다. 두 가지 선호 정렬 작업에서 LiPO-\\(\\lambda\\)이 DPO와 SLiC를 훨씬 능가할 수 있음을 보인다.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'GPT-4(OpenAI, 2023) 및 Gemini(Team et al., 2023)와 같은 최근 대규모 언어 모델은 대화 챗봇에서 프로그래밍에 이르기까지 다양한 작업에 대한 인상적인 성능으로 인해 전례 없는 기능을 잠금 해제했다. 이러한 언어 모델(LMs)의 동작을 제어하기 위한 핵심 단계는 선별된 인간 피드백과 정렬하는 것이다. 선호도 최적화는 활발한 연구 영역이 되었다. 인간 피드백을 이용한 강화 학습(Reinforcement Learning with Human Feedback, RRLHF)(Christiano et al., 2017)은 인간 선호도와 LM의 정렬을 개선하기 위해 처음 도입되었다(Ouyang et al., 2022). 그러나 RLHF는 복잡한 프로세스로 상당한 메모리와 하이퍼파라머 튜닝이 필요하다.\n' +
      '\n' +
      '최근 몇 가지 연구는 RLHF의 대안들에 의존하며, 눈에 띄게 쌍별 순위 최적화 패러다임으로 수렴한다. 예를 들어, DPO(Rafailov et al., 2023)는 명시적 보상 모델 및 RL 기반 최적화를 피하면서, 쌍별 인간 선호도 데이터로부터 쌍별 로지스틱 손실을 직접 최적화한다. 유사하게, SLiC(Zhao et al., 2023)는 인간으로부터 직접 또는 SFT(supervised fine-tuned) 정책으로부터 샘플링된 쌍별 선호 데이터에 쌍별 힌지 손실 목표를 최적화하고 보상 모델에 의해 순위가 매겨진다. RRHF(Yuan et al., 2023)는 보상 모델에 의해 라벨링된 리스트별 선호도 데이터로부터 시작하여, SLiC 목적과 유사한 쌍별 대비 목적(즉, 리스트 내의 모든 쌍을 비교하는 것)을 최적화한다. 보다 최근에, RSO(Liu et al., 2023)는 DPO 및 SLiC 손실을 통합하고 통계적 거부 샘플링을 통해 선호도 데이터를 소싱하는 더 나은 방법을 제안한다. 비르투\n' +
      '\n' +
      '그림 1: 쌍별 DPO(DPO\\({}_{\\text{BT}}\\)), 리스트별 DPO(DPO\\({}_{\\text{PL}}\\)), 그리고 LiPO-\\(\\lambda\\)라고 하는 우리의 제안된 리스트별 접근법의 성능. 모두 쌍별 데이터(리스트 크기 = 2)를 넘어 훈련 데이터로부터 이익을 얻을 수 있는 반면, LiPO-\\(\\lambda\\)은 리스트 크기가 증가함에 따라 더 많고 단조롭게 이익을 얻을 수 있다.\n' +
      '\n' +
      '최적화 프레임워크는 쌍별 선호도를 초과하지 않습니다. 그러나, 실제로, 인간 선호 데이터는 프롬프트를 판독하는 비용을 상각하기 위한 순위 리스트로서 올 수 있다(Kopf et al., 2023; Ouyang et al., 2022). 예를 들어, Instruct-GPT(Ouyang et al., 2022)는 이러한 방식으로 데이터를 수집한다: 각각의 출력에 대해, 라벨러는 1-7 척도로 전체 품질에 대한 Likert 점수를 부여한다. 각 출력을 개별적으로 평가한 후 레이블러는 지정된 프롬프트에 대해 모든 출력의 순위를 매깁니다. 두 출력이 유사한 품질의 것으로 보이는 경우 넥타이를 권장한다. 다른 예로서, 오픈어시스턴트(Kopf et al., 2023)는 트리-구조화된 프롬프트-어시스턴트 교대 대화 스레드를 가지며, 각각의 노드는 모든 형제들 중 랭크와 연관된다.\n' +
      '\n' +
      '이 작업에서는 LM 정렬을 목록별 순위 문제로 취급하며, 여기서 LM은 잠재적으로 목록별 선호도로부터 정렬을 더 효과적으로 학습할 수 있다. 이는 풍부한 LTR(Learning-to-Rank) 문헌(Liu, 2009)에 대한 명시적인 연결을 이끌어낸다. 특히, 인기 있는 기존의 방법들은 기존의 랭킹 목표들, 특히 쌍별 목표들에 매핑될 수 있다. 특히, LTR 문헌은 다양한 특성 및 성능의 풍부한 방법 세트를 사용하여 직접 목록별 최적화가 목록별 순위 문제에 대한 쌍별 대안보다 더 효과적일 수 있음을 보여주었다(Liu, 2009).\n' +
      '\n' +
      '이러한 목록 순위 관점은 관련 방법과 함께 LM 선호도 최적화 문제에 대해 잘 연구되지 않았다. 우리가 보여주듯이 모든 목록별 목표가 효과적인 것은 아니므로 LM 정렬에 가장 효과적인 목표를 식별하기 위한 검사를 수행하는 것이 바람직하다. 그림 1에서 목록별 선호도 _data_는 목록의 모든 쌍을 동등하게 처리하더라도 DPO와 같은 기존 쌍별 방법에 도움이 될 수 있는 반면, 문헌에서 잘 연구되지 않은 문제인 LiPO-\\(\\lambda\\)라고 하는 새로운 목록별 방법은 목록별 선호도에서 더 도움이 될 수 있음을 알 수 있다. DPO 논문(Rafailov et al., 2023)은 어떠한 실험 결과도 없이 그들의 부록에서 선호도 모델링을 위한 listwise Plackett-Luce 모델(Luce, 2005)을 간단히 터치한다. 우리는 그들의 리스트 와이드 접근법이 특정 리스트 와이드 랭킹 목표(Xia et al., 2008)에 매핑된다는 것을 주목하며, 이는 기술의 상태에 뒤떨어진다.\n' +
      '\n' +
      '이를 위해 목록형 선호도 최적화(LiPO) 프레임워크 하에서 순위 목표에 대한 첫 번째 포괄적인 연구를 제공하며, 여기서 LM 선호도 최적화 문제에 대한 인기 있는 순위 목표와 최신 순위 목표를 비교한다. 특히, DPO, RRHF, 및 SLiC와 같은 기존의 방법들은 리스트 와이드 데이터로부터 기존의 페어 와이드 랭킹 최적화 목적들에 매핑될 수 있다(페어 와이드 데이터만 사용되는 경우, 이는 리스트 사이즈가 2인 것과 동등함). 또한, 최신 랭킹 목표(Burges et al., 2006; Wang et al., 2018; Jagerman et al., 2022)를 활용하는 새로운 방법 LiPO-\\(\\lambda\\)이 매우 경쟁력 있는 성능을 달성할 수 있음을 보인다. 특히, LiPO-\\(\\lambda\\)은 직관적인 해석을 가능하게 한다: 이는 균일한 가중치를 할당하거나 다양한 요소를 고려하지 못하는 가중치 스키마를 사용하는 기존의 방법과 달리, 잘 확립된 순위 메트릭을 최적화하기 위해, 샘플링된 쌍에 리스트-인식 가중치를 할당하는 정교한 가중치 패러다임을 활용한다(Wang et al., 2013).\n' +
      '\n' +
      'Reddit TL;DR 및 AnthropicHH 과제에 대한 다양한 순위 목표에 대한 포괄적인 연구를 통해 LiPO-\\(\\lambda\\)이 DPO(Rafailov et al., 2023) 및 SLiC(Zhao et al., 2023)와 LTR 문헌에 의해 동기화된 다른 기준선을 포함하여 기존 방법보다 우수함을 보여준다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '* 최근 쌍대 선호 최적화 방법을 일반화하고, 학습-순위(Learning-to-Rank)의 렌즈를 통해 다른 대안들을 검사할 수 있도록 하는 리스트 와이즈 선호 최적화(Listwise Preference Optimization; LiPO) 프레임워크를 설명한다.\n' +
      '* LM 선호도 최적화를 위한 순위 목표, 특히 LM 선호도 최적화 문헌에서 잘 연구되지 않은 목록별 목표에 대한 포괄적인 조사를 제공한다.\n' +
      '* 우리는 평가 과제 전반에 걸쳐 경쟁적 성과를 보여주는 새로운 방법인 LiPO-\\(\\lambda\\)을 강조한다.\n' +
      '\n' +
      '##2 : LiPO 프레임워크\n' +
      '\n' +
      '나머지에서는 Rafailov et al.(2023)의 표기법을 기반으로 한다. LM 생성에서, \\(x\\in\\mathcal{X}\\)의 프롬프트가 주어지면, 액션 공간 \\(\\mathcal{Y}\\)이 존재하며, 여기서 각각의 액션은 보통 응답이라고 불린다. 정책\\(\\pi\\in\\Delta_{\\mathcal{Y}}^{X}\\)은 각 프롬프트\\(x\\)에 이산확률분포\\(\\pi(.| x)\\in\\Delta_{\\mathcal{Y}}\\)에서 \\(\\Delta_{\\mathcal{Y}}\\)은 \\(\\mathcal{Y}\\)에 걸친 이산 분포 집합이다.\n' +
      '\n' +
      '### Problem Formulation\n' +
      '\n' +
      '교육자료로부터 정책(\\pi\\)을 학습하는 것이 목표이며, 이 정책은 인간의 선호와 일치해야 한다는 핵심 고려 사항이다. 기존 작업(Zhao et al., 2023; Rafailov et al., 2023)은 여기서 _pairwise_ preference data로부터 학습에 대한 논의를 시작할 것이다. 그러나 선호 모델링은 보다 일반적인 _listwise_ 순위 문제로 취급될 수 있다. 결국 정책\\(\\pi\\)은 \\(\\Delta_{\\mathcal{Y}}\\)로부터 이산적인 액션들의 리스트를 사실상 순위화할 것으로 예상되며, 리스트별 선호도 데이터로부터 학습하는 것이 LM 정렬에 더 효과적일 수 있다. 사실, 위에서 논의한 바와 같이, 인간 선호 데이터는 프롬프트를 판독하는 비용을 상각하기 위한 순위 리스트로서 올 수 있다(Kopf et al., 2023; Ouyang et al., 2022). 따라서 본 논문에서는 보다 일반적인 Listwise Preference Optimization(LiPO) 프레임워크를 설명한다.\n' +
      '\n' +
      '학습 데이터세트는 \\(\\mathcal{D}=\\{x^{(i)},\\mathbf{y}^{(i)},\\psi^{(i)}\\}_{i=1}^{N}\\)이다. 즉, \\(x\\)이 주어지면, 우리는 SFT 정책 또는 다른 소스로부터 생성될 수 있는 \\(\\mathbf{y}=(y_{1},...,y_{K})\\) 크기의 응답의 _list_가 있다. \\(K=2\\)일 때, 이것은 쌍별 데이터로 줄어든다. 본 논문에서는 이러한 응답과 관련된 실수값 레이블 \\(\\psi=(\\psi_{1},...,\\psi_{K})\\in[0,1]^{K}\\)이 존재한다고 가정한다. 레이블 값이 높을수록 반응이 양호하다는 것을 나타냅니다. 우리는 \\(y_{i}\\succ y_{j}\\)을 사용하여 \\(\\psi_{i}>\\psi_{j}\\)이 주어진 두 응답의 선호도를 나타낸다. 우리의 연구에서 우리는 모든 응답에 대한 \\(y_{k}\\)의 평균 당첨 확률로서 \\(\\psi_{k}\\)을 계산한다:\n' +
      '\n' +
      '\\frac{1}{K}\\sum_{i=1}^{K}\\mathbb{P}(y_{k}\\succ y_{i}} \\tag{1}\\]\n' +
      '\n' +
      '인간 순위 응답의 경우 승산 확률은 \\(\\hat{\\mathbb{P}}(y_{i}\\succ y_{j})=\\mathbf{1}(\\text{human prefer }y_{i}\\text{ over }y_{j})\\으로 근사한다. 쌍별 보상 랭킹 모델(Zhao et al., 2023; Liu et al., 2023)의 경우, 예측된 당첨 확률을 직접 사용한다. 이 집계 및 라벨링 스키마는 이론적으로 유효하며(Shah and Wainwright, 2018), 랭킹 문헌에서 경험적으로 잘 작동한다(Qin et al., 2023).\n' +
      '\n' +
      '리스트별 선호도 데이터가 주어지면 LM 정렬은 LTR(Learning-to-Rank) 문제로 공식화될 수 있다. LTR(Liu, 2009)에서는 질의가 주어진 모든 문서에 대한 관련성 점수\\(\\mathbf{s}\\)를 출력할 수 있는 순위 모델\\(\\pi_{\\theta}\\)을 학습하는 것을 목표로 한다. LM 정렬에서는 LTR 설정에서 \\(x\\)을 질의로, \\(\\mathbf{y}\\)을 문서로 처리한다. 그런 다음 Rafailov et al. (2023)에서 영감을 얻은 \\((x,y_{i})\\)에 대해 \\(\\mathbf{s}=\\{s_{1},...,s_{K}\\}\\), 여기서 \\(s_{i}\\)은 다음과 같이 정규화된 것으로 정의된다.\n' +
      '\n' +
      'bf{s}(\\pi_{\\theta},\\pi_{\\text{ref},\\beta)& =\\{s_{1},...,s_{K}\\\\ &\\triangleq\\{\\beta\\log\\frac{\\pi_{\\theta}(y_{1}|x)}{\\pi_{\\text{ref}(y_{1}|x)},...,\\beta\\log\\frac{\\pi_{\\theta}(y_{K}|x)}{\\pi_{\\text{ref}(y_{K}|x)},\\end{split}\\tag{2}\\tag\\frac{\\beta\\log\\frac{\\beta\\log\\frac{\\beta\\log\\frac{\\beta\\log\\frac{\\beta\\log\\frac{\\beta\\log\\frac}(y_{1}|x)}{\\pi_{\\text{ref}(y_{K}|x)}{\\pi_{\\text{ref}}(y_{K}|x)}\n' +
      '\n' +
      '여기서 \\(\\pi_{\\theta}\\)는 학습될 \\(\\theta\\)에 의해 매개되는 정책 매개변수이고, \\(\\pi_{\\text{ref}\\)은 \\(\\pi_{\\theta}\\)을 초기화하는 SFT 정책이며, \\(\\beta\\)은 \\(\\pi_{\\theta}\\)과 \\(\\pi_{\\text{ref}\\) 사이의 KL 발산을 제어하기 위한 하이퍼 매개변수이다. DPO(Rafailov et al., 2023)에서, 점수는 "암시적 보상"으로 취급되며, DPO의 목표는 "암시적 보상"을 인간 선호도 데이터에 직접 정렬하는 것이다. 표기를 단순화하기 위해 우리는 이제부터 \\((\\pi_{\\theta},\\pi_text{ref},\\beta)\\)에 대한 \\(\\mathbf{s}\\)의 의존성을 생략하였다.\n' +
      '\n' +
      'LTR 알고리즘은 손실함수를 목적함수로서 사용하여 \\(\\pi_{\\theta}\\)을 학습한다. 순위 손실 함수는 일반적으로 응답의 레이블 \\(\\psi\\)과 예측 점수 \\(\\mathbf{s}\\)에 기초하여 정의된다:\n' +
      '\n' +
      '\\[\\mathcal{L}(\\pi_{\\theta};\\pi_{\\text{ref}},\\beta)=\\mathbbb{E}_{(x,\\mathbf{y}, \\psi)\\sim\\mathcal{D}}\\left[l(\\psi,\\mathbf{s})\\right]. \\tag{3}\\tag{3}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\t\n' +
      '\n' +
      'l\\(l\\)은 레이블과 점수를 입력으로 하고 실제 값을 손실로 출력하는 단일 프롬프트 \\(x\\)에 대한 손실 함수이다:\n' +
      '\n' +
      '\\[l:(\\psi,\\mathbf{s})\\rightarrow\\mathbb{R}. \\tag{4}\\]\n' +
      '\n' +
      '학습-순위 알고리즘은 순위 모델의 공간에서 전체 손실을 최소화하는 최적의 \\(\\pi_{\\theta}\\)을 찾는 것이다. 우리는 식 3을 LM 정렬의 맥락에서 목록별 선호 최적화(LiPO) 프레임워크라고 부른다.\n' +
      '\n' +
      '기존 작업의 순위 손실\n' +
      '\n' +
      'Eq 2에서 \\(\\mathbf{s}\\)의 정의를 통해 LTR 문헌에서 서로 다른 순위 손실을 사용하여 여러 인기 있는 LM 정렬 방법이 LiPO 프레임워크에 매핑될 수 있음을 보여준다. DPO 논문에서는 Bradley-Terry (BT) 모델(Bradley and Terry, 1952)과 Plackett-Luce (PL) 모델(Plackett, 1975)의 두 가지 순위 프레임워크 하에서 두 가지 손실 함수를 제안한다. 우리는 DPO({}_{\\text{BT}\\) 손실을 BT 모델 기반 쌍별 손실로 지칭하고, DPO({}_{\\text{PL}\\) 손실을 PL 모델 기반 목록별 손실로 지칭한다. SLiC 용지에서 손실은 교정 손실과 정규화 손실의 두 가지 용어로 구성된다. RSO(Liu et al., 2023)는 참조 정책으로 우도를 정규화하여 교정 손실을 개선한다. 개선된 보정 손실을 나타내기 위해 SLiCnorm을 사용한다.\n' +
      '\n' +
      '쌍별 선호도 손실.쌍별 로지스틱 순위 손실(Burges et al., 2005)은 리스트에 적합하도록 인기 있는 선택 중 하나이다.\n' +
      '\n' +
      '도 2: Listwise Preference Optimization(LiPO) 파이프라인의 예시. 각 프롬프트에 대해 LiPO는 특정 정책 또는 혼합 정책의 응답 목록을 샘플링합니다. 그런 다음 인간 또는 보상 순위 모델은 각 응답에 순위 레이블을 할당한다. 그 후 LiPO는 선호도를 맞추기 위해 순위 손실을 통해 정책을 최적화한다.\n' +
      '\n' +
      'of ranked data:\n' +
      '\n' +
      '[\\begin{split}\\mathcal{L}_{\\text{pair-begistic}}(\\pi_{\\theta};\\pi_{\\text{ref}},\\beta)=\\\\\\mathbb{E}_{x,\\mathbf{y},\\psi\\sim\\mathcal{D}}\\left[\\sum_{\\psi_{i} >\\psi_{j}}\\log(1+e^{-(s_{i}-s_{j}})\\right].\\end{split}\\tag{5}\\w}\n' +
      '\n' +
      '우리는 다음의 명제를 통해 위의 손실을 DPO\\({}_{\\text{BT}\\)와 연결한다:\n' +
      '\n' +
      '**Proposition 2.1**.: _\\(K=2\\) 및 쌍별 로지스틱 랭킹 손실이 사용될 때, LiPO는 DPO\\({}_{BT}\\)(Rafailov et al., 2023)._\n' +
      '\n' +
      '증명: \\(K=2\\)일 때, \\((y_{1},y_{2})\\)와 \\((y_{2},y_{1})\\)의 두 쌍만이 존재한다. 승리 응답은 \\(y_{w}\\)을 사용하고, 패배 응답은 \\(y_{l}\\)을 사용한다.\n' +
      '\n' +
      '그리고 나서, Eq5는:\n' +
      '\n' +
      'b{E}_{(x,y_{w},y_{l})\\sim\\mathcal{D}}\\left[ \\log(1+e^{-(s_{w}-s_{l})})\\right]=\\\\-&\\mathbb{E}_{(x,y_{w},y_{l})\\sim\\mathcal{D}}\\left[ \\log(\\sigma(s_{w}-s_{l}))\\right]\\end{split}\\tag{6}\\right]\n' +
      '\n' +
      '이것은 Rafailov et al.(2023)의 Eq(7)과 동일하며, 만약 우리가 \\(\\beta\\log\\frac{\\pi_{\\theta}(y_{i}|x)}{\\pi_{\\text{ref}(y_{i}|x)}으로 \\(s_{i}\\)을 대체한다면.\n' +
      '\n' +
      '마찬가지로, 정규화된 SLiC 손실(Liu 등(2023)의hinge-norm 손실)과 RankSVM(Joachims, 2002)으로부터의 쌍별 힌지 손실을 연결할 수 있다:\n' +
      '\n' +
      '[\\begin{split}&\\mathcal{L}_{\\text{pair-hinge}(\\pi_{\\theta};\\pi_{\\text{ref}},\\beta)=\\\\\\\\mathbb{E}_{x,\\mathbf{y},\\psi\\sim\\mathcal{D}}\\left[\\sum_{\\psi_{i}>\\psi_{j}}\\max\\left(0,1-(s_{i}-s_{j})\\right]\\end{split}\\tag{7}\\w}\\w}\n' +
      '\n' +
      '**Proposition 2.2**.: _\\(K=2\\) 및 쌍별 힌지 랭킹 손실이 사용될 때, LiPO는 SLiCnorm(Zhao et al., 2023; Liu et al., 2023)과 동등하다._\n' +
      '\n' +
      '증명: \\(K=2\\)일 때, \\((y_{1},y_{2})\\)와 \\((y_{2},y_{1})\\)의 두 쌍만이 존재한다. 승리 응답은 \\(y_{w}\\)을 사용하고, 패배 응답은 \\(y_{l}\\)을 사용한다.\n' +
      '\n' +
      '그러면, Eq7은:\n' +
      '\n' +
      '\\[\\mathbb{E}_{(x,y_{w},y_{l})\\sim\\mathcal{D}}\\left[\\max\\left(0,1-(s_{w}-s_{l}) \\right)\\right]\\tag{8}\\\\t.\n' +
      '\n' +
      '이는 Liu et al. (2023)의 Eq(10)과 동일하며, \\(\\beta\\log\\frac{\\pi_{\\theta}(y_{i}|x)}{\\pi_{\\text{ref}(y_{i}|x)}\\)로 대체하고 \\(\\beta=\\gamma\\)로 설정하면 된다.\n' +
      '\n' +
      '리스트별 선호도 손실.쌍별 로지스틱 손실 또는 쌍별 힌지 손실을 사용하여 모든 쌍을 맞출 수 있습니다. 또 다른 방법은 리스트로 순위가 매겨진 데이터에 MLE(Maximum Likelihood Estimation)을 직접 맞추는 것이다. Xia et al. (2008)은 리스트 MLE 랭킹 손실을 제안한다:\n' +
      '\n' +
      '{L}_{\\text{list-mle}(\\pi_{\\theta};\\pi_{ \\text{ref}},\\beta)=\\\\&-\\mathbb{E}_{x,y_{1},y_{2}\\ldots y_{K}\\sim\\mathcal{D}\\left[\\log\\prod_{k=1}^{K}\\frac{\\exp(s_{\\tau(k))}}{\\sum_{j=k}^{K}\\exp(s_{\\tau(j))}}\\right],\\end{split}\\tag{9}\\tau(k)}\\dot y_{K}\\sim\\mathcal{D}\\left[\\log\\prod_{k=1}^{K}\\frac{\\exp(s_{\\tau(j))}\\end{split}\\k}\\exp(s_{\\tau(j))}\n' +
      '\n' +
      '여기서 \\(\\tau(i)\\)는 레이블에 의해 결정된 리스트 순열에서 \\(i\\)번째 위치에 순위가 매겨진 문서이다.\n' +
      '\n' +
      '**Proposition 2.3**.: _ list MLE loss가 사용되는 경우, LiPO는 DPO\\({}_{PL}\\)(Rafailov et al., 2023)._\n' +
      '\n' +
      '증명: 목록 MLE 손실은 Rafailov et al. (2023)의 Eq(20)의 것과 동일하다. 만약 우리가 \\(\\beta\\log\\frac{\\pi_{\\theta}(y_{i}|x)}{\\pi_{\\text{ref}(y_{i}|x)}으로 \\(s_{i}|x)를 대체한다면.\n' +
      '\n' +
      '기존 작업의 한계###\n' +
      '\n' +
      '일반적인 LiPO 공식화 및 최근 정책 최적화 방법의 분석을 통해 기존 순위 목표의 특정 선택에 매핑되는 것을 볼 수 있다. LTR의 렌즈를 통해 선호 최적화의 효과를 제한할 수 있는 논의된 방법의 두 가지 주요 문제가 있음을 주목한다.\n' +
      '\n' +
      '첫째, 현재 선호 최적화 문헌을 지배하는 모든 쌍별 접근 방식은 쌍을 넘어서는 리스트별 순열 정보를 무시한다. 원칙적인 방식으로 동일한 프롬프트 하에 있는 모든 후보를 고려하는 것은 정책이 더 효과적으로 학습하도록 허용할 수 있다.\n' +
      '\n' +
      '둘째, 기존의 모든 방법, 즉 목록형 방법(DPO\\({}_{\\text{PL}}\\))조차도 레이블 값을 무시한다. 즉, 응답의 최적 쌍별 또는 목록형 _rank-ordering_에만 초점을 맞춘다. 이것은 정보 손실을 가지고 있으며 학습 행동을 혼란스럽게 할 수 있다. 예를 들어, 레이블이 있는 두 개의 목록(0.99, 0.50, 0.01) 및 (0.51, 0.50, 0.49)은 동일하게 취급되어 훈련 중 비효율성을 초래하고 일반화에 해를 끼칠 수 있다.\n' +
      '\n' +
      '## 3 LiPO-\\(\\lambda\\)\n' +
      '\n' +
      '일반적인 LiPO 프레임워크에서 LTR 문헌에서 다른 최적화 목표의 풍부한 패밀리를 탐색할 수 있다. 이 섹션에서는 최첨단 순위 목표를 기반으로 하는 특정 인스턴스화 LiPO-\\(\\lambda\\)를 제안하며, 위의 한계를 원칙적인 방식으로 해결하고 실험에서 경험적으로 잘 수행한다.\n' +
      '\n' +
      '우리의 LiPO-\\(\\lambda\\)는 LambdaLoss 방법을 기반으로 한다(Burges et al., 2006; Wang et al., 2018; Jagerman et al., 2022).\n' +
      '\n' +
      '구체적으로, LiPO-\\(\\lambda\\)의 트레이닝 목표는:\n' +
      '\n' +
      '\\text{lambda-loss}(\\begin{split}&\\mathcal{L}_{\\text{lambda-loss}(\\pi_{\\theta};\\pi_{\\text{ref}},\\beta)=\\\\&-\\mathbbb{E}_{x,\\psi\\sim\\mathcal{D}}\\left[\\sum_{\\psi_{i}>\\psi_{j}}\\Delta_{i,j}\\log(1+e^{-(s_{i}-s_{j}))}|.\\end{split}\\frac{1}{D(\\tau(i))}-\\frac{1}{D(\\tau(i))}-\\frac{1}{D(\\tau(i))},\\quad\\delta_{i,j}=|G_{i}-G_{j}|\\cdot|\\frac{1}{D(\\tau(i))}-\\frac{1}\n' +
      '\n' +
      '람다 가중치(\\Delta_{i,j}\\)라고 합니다. \\Delta_{i,j}\\ (G\\)은 일반적으로 사용되는 것으로 \\(G_{i}=2^{\\psi_{i}}-1\\)을 갖는 이득 함수라고 한다. \\ (D\\)는 일반적으로 사용되는 것으로 \\(D(\\tau(i))=\\log(1+\\tau(i))\\)를 갖는 순위할인함수이며, \\(\\tau(i)\\)는 \\(\\mathbf{s}\\)에 의해 유도된 순열에서 \\(y_{i}\\)의 순위위치이므로 쌍으로 공식을 작성할 수 있음에도 _listwise_ 방법이다. 즉, 각 쌍에 대해 동일한 목록의 다른 항목에 종속성이 있다. 하나는 이득 및 할인 기능을 변경할 수 있는 유연성이 있지만 여기서는 단순화를 위해 원래의 람다로스 구성에 의존한다. 이 손실 함수가 잘-근거된 DCG 메트릭을 최적화할 수 있다는 것이 보여졌다 (Burges et al., 2006; Donmez et al., 2009),\n' +
      '\n' +
      '\\[DCG=\\sum_{i=1}^{K}\\frac{G_{i}}{D(\\tau(i))},\\]\n' +
      '\n' +
      '일관된 구별 가능성과 같은 랭킹 메트릭으로서 몇 가지 이상적인 속성을 갖는 것(Wang et al., 2013): 실질적으로 상이한 랭킹 정책들의 모든 쌍에 대해, 랭킹 메트릭은 어느 것이 거의 모든 데이터세트 상에서 일관된 방식으로 더 나은지를 결정할 수 있다.\n' +
      '\n' +
      '비교.LiPO 프레임워크에서 LiPO-\\(\\lambda\\)를 다른 방법과 비교하여 몇 가지 흥미로운 결과가 있다. 먼저, 이득 함수 \\(G\\)는 레이블 스코어 값 \\(\\psi\\)을 고려하며, 이는 기존의 거의 모든 방법에서 무시된다. 둘째, 리스트 MLE 손실과 비교하여, 고려된 순열은 정적 레이블 대신 모델 예측 점수에 의해 유도된다. Burges et al. (2006)은 훈련 중 모델 예측에 기초한 이러한 동적 순열을 고려하는 것이 비-평활 순위 목표를 최적화하기 위해 더 평활한 최적화 경관으로 이어질 수 있으며, 라벨로부터의 정적 순위를 사용하는 것보다 더 나은 경험적 성능을 초래한다는 것을 보여주었다. 마지막으로, LiPO-\\(\\lambda\\)는 Eq 5와 비교하여 순위화된 목록의 모든 쌍에 대해 DPO\\({}_{\\text{BT}}\\)의 가중치 버전으로 취급될 수 있다. 각 쌍을 동등하게 취급하는 대신 Lambda 가중치는 목록순열 인식 가중치 메커니즘이다. 람다 가중치의 하나의 직관은 리스트에서 교체될 때 랭킹 메트릭의 차이에 의해 각각의 쌍에 가중치를 부여하는 것이다(Burges, 2010).\n' +
      '\n' +
      '##4 기타 랭킹 손실\n' +
      '\n' +
      'LiPO 프레임워크 하에서 다른 기존 또는 미래의 순위 목표를 연구할 수 있다. 실험에서는 LiPO의 특정 인스턴스화로서 취급될 수 있는 기존의 방법 외에도, 점별 순위 목표인 점별 평균 제곱 오차(MSE) 손실에 대해 연구한다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{point-MSE}(\\pi_{\\theta};\\pi_{\\text{ref}},\\beta)=\\mathbb{E}_{x,\\mathbf{y},\\psi\\sim\\mathcal{D}\\sum_{k=1}^{K}(\\psi_{k}-s_{k}})^{2}, \\tag{11}\\mathbb{E}_{x,\\mathbf{y},\\psi\\sim\\mathcal{D}\\sum_{k=1}^{K}(\\psi_{k}-s_{k}}},\\tag{11}}}}\n' +
      '\n' +
      '및 점방향 시그모이드 교차 엔트로피 손실:\n' +
      '\n' +
      '\\text{split}&\\mathcal{L}_{\\text{point-sigmoid}(\\pi_{\\theta}; \\pi_{\\text{ref}},\\beta)=\\\\\\&-\\mathbb{E}_{x,\\mathbf{y},\\psi\\sim\\mathcal{D}\\sum_{i=1}^{K}\\left(\\psi_{i}\\log\\sigma(s_{i})+(1-\\psi_{i}\\log(1-\\sigma(s_{i}))\\right}\\end{split}\\tag{12}\\psi\\sim\\mathbb{E}_{x,\\mathbf{y},\\psi\\sim\\mathcal{D}\\sum_{i=1}^{K}\\left(\\psi_{i}\\log\\sigma(s_{i})+(1-\\psi_{i})\\log(1-\\sigma(s_{i}))\\right}\\\n' +
      '\n' +
      '우리는 또한 ListNet(Cao et al., 2007)에서와 같이 소프트맥스 교차 엔트로피 손실이라고 불리는 또 다른 인기 있는 목록별 순위 목표를 고려한다:\n' +
      '\n' +
      '{L}_{\\text{softmax}(\\pi_{\\theta};\\pi_{\\text{ref}},\\beta)=\\\\&\\mathbbb{E}_{x,\\mathbf{y},\\psi\\sim\\mathcal{D}\\left[\\sum_{k=1}^{K}\\frac{\\psi_{k}}{\\sum_j=1}^{K}\\psi_{j}}\\log\\left(\\frac{\\exp(s_{k}}{\\sum_j=1}^{K}\\exp(s_{j}}\\right)\\right]\\end{split}\\tag{13}\\frac{\\exp(s_{k}}\\frac{\\exp(s_{k}}\\frac{\\exp(s_{j}}}\\frac{\\exp(s_{j}}}\\frac{\\exp(s_{j}}}\\frac{\\exp(s_{j}}}\\frac{\\exp(s_{j}}\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '우리는 Reddit TL;DR 요약(Stiennon et al., 2020) 및 AnthropicHH 대화(Bai et al., 2022) 데이터 세트에 대한 LiPO 프레임워크 하에서 통일된 상이한 순위 손실을 연구한다. Reddit TL;DR 요약 데이터 세트는 미세 조정 데이터\\(\\mathcal{D}_{\\text{st}^{\\text{ldr}\\)와 인간 피드백 데이터\\(\\mathcal{D}_{\\text{bf}^{\\text{lddr}\\)을 모두 포함한다. (\\mathcal{D}_{\\text{st}^{\\text{ldr}}\\)는 열차, 검증 및 시험 분할에서 117k/6k/6k의 예를 포함한다. \\\\ (\\mathcal{D}_{\\text{bf}^{\\text{ldr}\\)는 다중 모델의 디코딩에 대한 93k개의 인간 선호도로 구성된다. 인류틱HH는 인간 질의와 AI 어시스턴트 사이의 대화로서 \\(x\\)을 갖는 대화 데이터세트이다. 열차 및 시험 분할에서 161k/9k 예제로부터 유용한 절편\\(\\mathcal{D}_{\\text{bf}^{\\text{helpful}}\\)을 사용한다. 우리는 긍정적인 반응을 SFT 대상으로 사용합니다.\n' +
      '\n' +
      'T5-large(770M) SFT 정책과 T5-XXL(11B) pairwise reward-ranking 모델(Zhao et al., 2023; Liu et al., 2023)로부터 출발하여, 먼저 SFT 정책을 \\(온도=0.7\\)과 \\(top\\_k=40\\)으로 사용하여 각 프롬프트에 대해 \\(K=8\\)의 응답(섹션 5.2에서 더 많은 삭제가 있음)을 샘플링하여 응답 목록을 구성한다. 그리고 쌍별 보상 순위 모델을 사용하여 모든 쌍 비교를 수행하여 승산 확률 행렬 \\(\\Psi\\in[0,1]^{K\\times K}\\)을 얻는다. 그리고 \\(\\psi_{k}\\)을 \\(\\psi_{k}=\\frac{1}{K}\\sum_{i=1}^{K}\\Psi_{ki}\\)으로 계산한다. 정규화된 순위 점수 \\(\\mathbf{s}\\)를 계산하기 위해 \\(\\beta=0.05\\)을 설정하였다.\n' +
      '\n' +
      '우리는 LiPO 프레임워크에서 점별, 쌍별, 목록별 세 가지 유형의 손실 함수를 고려한다. 점적 손실은 점-mse(Eq 11) 및 점-sigmoid(Eq 12)를 포함한다. 쌍별 손실은 쌍-힌지(Eq 7) 쌍-로지스틱(Eq 5)을 포함한다. 리스트별 손실은 list-mle(Eq 9), softmax(Eq 13), lambda-loss(Eq 10)를 포함한다. 우리는 Jax에서 손실을 계산하기 위해 RAX 라이브러리(Jagermanet al., 2022)를 사용한다. 예를 들어 사용법은 부록 C를 참조하십시오.\n' +
      '\n' +
      '쌍별 손실의 경우 64쌍을 모두 활용하고 섹션 5.2에서 기존 작업에서 일반적으로 수행되는 것처럼 매번 단일 쌍을 샘플링하는 것보다 성능을 향상시킬 수 있음을 보여주므로 이 설정은 공정할 뿐만 아니라 쌍별 방법에도 도움이 된다. Aadafactor optimizer (Shazeer and Stern, 2018)를 사용하여 배치 크기 32와 학습률 2e-5를 사용한다. 각 런에 대해 SFT 목표에 대해 보상 순위 모델 승률이 가장 높은 체크포인트를 선택합니다.\n' +
      '\n' +
      '평가 우리의 실험은 기존의 프로토콜(Rafailov et al., 2023; Liu et al., 2023): 프록시 리워드 모델, AutoSxS, 및 인간 평가)에 따라 평가하기 위해 세 가지 상이한 접근법을 사용한다. 프록시 보상 모델은 훈련된 T5-XXL 쌍별 보상 순위 모델에서 SFT 타겟에 대한 생성된 응답의 승률을 계산한다. AutoSxS는 수-샷 인-컨텍스트 학습(부록 A에서 세부사항)을 위해 큰 PaLM 2 명령어 튜닝된 모델(PaLM 2-L-IT)(Google et al., 2023)을 사용한다. 인간 평가는 인간 평가자에게 각 반응에 대한 품질 점수를 할당하고 \\(\\text{DPO}_{\\text{BT}\\), \\(\\text{DPO}_{\\text{PL}\\) 및 \\(\\text{LiPO}-\\lambda\\)(부록 B의 세부 사항) 중에서 가장 좋은 점수를 결정하도록 요청한다.\n' +
      '\n' +
      '두 태스크에 대한### 성능 비교\n' +
      '\n' +
      '주요 비교 결과는 표 1과 같다. 첫째, 점별 방법은 예상한 대로 경쟁력이 없어 점별 레이블 값만 고려해도 충분하지 않으며 선호도 정보가 중요하다. 둘째, \\(\\text{DPO}_{\\text{PL}\\)은 \\(\\text{DPO}_{\\text{BT}\\)보다 더 나은 성능을 나타내지 못하며, 목록순위의 객관성을 선택하는 것이 목록순위의 중요한 데이터임을 보여준다. 이는 레이블 값에 신경 쓰지 않고 목록순열을 시행하기 때문에 목록순열이 경쟁 순위 목표가 아니라는 기존 LTR 문헌과 일치한다. 반면에, \\(\\text{DPO}_{\\text{BT}}\\)는 레이블 값을 고려하지 않는 반면, 쌍별 형식은 비행한 목록형 행동에 덜 취약할 수 있다. 셋째, 목록형 소프트맥스 손실은 경쟁력이 없다. 이는 Softmax 손실이 이진 클릭 데이터(Yu et al., 2015; Bai et al., 2023)와 같은 희소 레이블을 갖는 최적화 리스트별 랭킹에 가장 효과적이기 때문에 이해할 수 있다. LM 생성의 경우 응답이 그럴듯한 세트에서 샘플링되어 레이블이 조밀하여 소프트맥스 손실에 잘 맞지 않는다. 한편, \\(\\text{LiPO}-\\lambda\\)는 리스트 단위의 데이터와 레이블 값 정보를 효과적으로 활용하여 모든 기준선에서 강한 성능을 보이고 개선된다.\n' +
      '\n' +
      '### 절제 연구 및 분석\n' +
      '\n' +
      '리스트별 공식화 및 방법이 가져오는 이점을 심층적으로 이해하기 위해 리스트별 데이터, 람다 가중치 선택 및 모델 크기 측면에서 효과를 이해하기 위해 절제 연구를 수행한다.\n' +
      '\n' +
      '목록 크기에 대한 절제 연구는 목록별 데이터에 대한 선호도 최적화의 영향을 더 잘 이해하기 위해 Reddit TL;DR 데이터 세트에 대한 목록 크기의 다중 선택에 대한 분석을 수행한다. 도 3에 예시된 바와 같이, 대부분의 방법들은 각각의 프롬프트 \\(x\\) 하에서 더 많은 정보를 레버리지함으로써 쌍별 트레이닝 데이터(리스트 사이즈 = 2)를 넘어서는 것으로부터 이익을 얻을 수 있다. \\ Lambda-loss를 갖는 (\\text{LiPO}-\\lambda\\)는 더 긴 목록 크기로부터 강건하게 이익을 얻을 수 있는 유일한 방법이며, 풍부한 목록 정보를 보다 효과적으로 활용할 수 있음을 보여준다.\n' +
      '\n' +
      '3절에서 논의된 람다 가중치에 대한 절제 연구는 DCG 순위 메트릭을 최적화할 수 있는 이득 함수 \\(G_{i}=2^{\\psi_{i}}-1\\)와 순위 할인 함수 \\(D(\\tau(i))=\\log(1+\\tau(i))\\)를 설정하여 람다 가중치의 특정 선택을 사용한다. 이 절제 연구에서 순위 메트릭과 명확한 연결이 없을 수 있는 다른 옵션을 시도한다. 일정 가중치는 모든 쌍에 동일한 가중치를 부여하며, 이는 \\(\\text{DPO}_{\\text{BT}\\)으로 감소한다. 상수 \\(\\delta\\) Gain은 이득 함수 차이를 상수 \\(|G_{i}-G_{j}|=1\\)로 설정하고, 상수 \\(\\delta\\)Discount는 순위 할인 함수 차이를 상수 \\(|\\frac{1}{D(\\tau(i))}-\\frac{1}{D(\\tau(j))}|=1\\)로 설정한다. 그 비교는 그림 4와 같다. DCG 가중치를 사용하는 것이 두 데이터 세트 모두에서 가장 효과적임을 알 수 있으며, 잘 근거가 있는 순위 메트릭을 최적화하기 위해 두 기능을 적절하게 설정하는 것의 중요성을 보여준다.\n' +
      '\n' +
      '정책 모델을 확장한다. 본 논문에서는 T5-XXL 정책 모델을 학습하고, T5-XXL 정책 모델을 확장한다. 그리고 T5-XXL 정책 모델을 확장한다. 표 2는 세 가지 방법 모두 잘 확장되고 \\(\\text{LiPO}-\\lambda\\)이 두 작업 모두에서 경쟁력이 있음을 보여준다.\n' +
      '\n' +
      '그림 3: Reddit TL;DR 데이터 세트에서 다양한 목록 크기를 가진 다양한 순위 손실의 성능. 음영 처리된 영역은 95% 신뢰 구간입니다.\n' +
      '\n' +
      '##### 인체 평가 결과\n' +
      '\n' +
      'LiPO-\\(\\lambda\\)의 개선점을 추가로 검증하기 위해 Amazon Mechanical Turk를 이용한 인체 평가를 병행한다. DPOBT, DPOPL 및 LiPO-\\(\\lambda\\)에서 생성된 문서와 세 가지 응답이 주어지면, 평가자는 각 응답에 점별 전체 품질(1-5)을 할당하고 가장 좋은 응답을 선택하도록 요청받는다. 각 작업은 3번 반복되므로 3명의 다른 평가자에 의해 판단됩니다. 편향을 제거하기 위해 모든 모델을 익명화하고 각 작업에 대한 응답 순서를 무작위로 셔플한다. 우리는 모든 복제본에 대한 평점을 평균하여 점별 메트릭을 집계하고, 다수결을 사용하여 선택 메트릭을 집계한다. 작업에 대한 자세한 내용은 부록 B를 참조하십시오.\n' +
      '\n' +
      '총 67명의 다른 평가자가 레딧 TL;DR 평가 연구에 참여했으며 평가자당 작업 중앙값은 18개이다. 인간 평가 결과는 표 3에 나와 있다. LiPO-\\(\\lambda\\)는 두 과제 모두에서 DPOBT 및 DPOPL보다 우수한 것으로 나타났다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline Approach & Ranking Loss & Proxy Reward (\\%) & AutoSXS (\\%) \\\\ \\hline \\hline \\multicolumn{4}{c}{**Reddit TL;DR**} \\\\ \\hline  & point-mse & \\(49.43_{\\pm 1.18}\\) & \\(39.94_{\\pm 1.72}\\) \\\\  & point-sigmoid & \\(64.14_{\\pm 1.16}\\) & \\(49.28_{\\pm 1.80}\\) \\\\  & softmax & \\(75.40_{\\pm 0.98}\\) & \\(58.60_{\\pm 1.72}\\) \\\\ SLiCnorm & pair-hinge & \\(87.23_{\\pm 0.78}\\) & \\(67.16_{\\pm 1.62}\\) \\\\ DPOBT & pair-logistic & \\(88.52_{\\pm 0.74}\\) & \\(67.09_{\\pm 1.65}\\) \\\\ DPOPL & list-mtle & \\(88.27_{\\pm 0.76}\\) & \\(67.23_{\\pm 1.60}\\) \\\\ LiPO-\\(\\lambda\\) & lambda-loss & \\(\\textbf{90.60}_{\\pm 0.65}\\) & \\(\\textbf{68.06}_{\\pm 1.58}\\) \\\\ \\hline \\hline \\multicolumn{4}{c}{**AnthropicHH**} \\\\ \\hline  & point-mse & \\(57.55_{\\pm 1.22}\\) & \\(21.97_{\\pm 1.47}\\) \\\\  & point-sigmoid & \\(71.35_{\\pm 1.11}\\) & \\(25.72_{\\pm 1.50}\\) \\\\  & softmax & \\(73.21_{\\pm 1.07}\\) & \\(28.87_{\\pm 1.65}\\) \\\\ SLiCnorm & pair-hinge & \\(89.68_{\\pm 0.72}\\) & \\(42.07_{\\pm 1.75}\\) \\\\ DPOBT & pair-logistic & \\(91.11_{\\pm 0.66}\\) & \\(44.80_{\\pm 1.73}\\) \\\\ DPOPL & list-mtle & \\(90.61_{\\pm 0.72}\\) & \\(43.25_{\\pm 1.77}\\) \\\\ LiPO-\\(\\lambda\\) & lambda-loss & \\(\\textbf{92.60}_{\\pm 0.62}\\) & \\(\\textbf{47.90}_{\\pm 1.70}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 목록별 선호 데이터를 활용하기 위해 T5 대형 정책 모형과 다른 방법을 비교한 것이다. SFT 대상 텍스트에 대한 대리 보상 및 소수의 샷 PaLM 2-L-IT 승률이 보고된다. 모든 방법은 크기 8의 선호도 리스트를 사용하고, SLiCnorm과 DPOBT를 포함한 pairwise 방법은 리스트에서 생성된 모든 pair를 사용하여 동등하게 처리한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline Approach & Proxy Reward (\\%) & AutoSXS (\\%) \\\\ \\hline \\hline \\multicolumn{4}{c}{**Reddit TL;DR**} \\\\ \\hline DPOBT & \\(96.22_{\\pm 0.43}\\) & \\(82.38_{\\pm 1.20}\\) \\\\ DPOPL & \\(96.00_{\\pm 0.45}\\) & \\(81.96_{\\pm 1.30}\\) \\\\ LiPO-\\(\\lambda\\) & \\(\\textbf{97.32}_{\\pm 0.36}\\) & \\(\\textbf{83.79}_{\\pm 1.25}\\) \\\\ \\hline \\hline \\multicolumn{4}{c}{**AnthropicHH**} \\\\ \\hline DPOBT & \\(97.48_{\\pm 0.34}\\) & \\(68.81_{\\pm 1.63}\\) \\\\ DPOPL & \\(97.28_{\\pm 0.37}\\) & \\(68.84_{\\pm 1.58}\\) \\\\ LiPO-\\(\\lambda\\) & \\(\\textbf{98.27}_{\\pm 0.29}\\) & \\(\\textbf{69.81}_{\\pm 1.58}\\) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: T5-XXL 정책 모형에 대한 목록별 선호 데이터를 활용하기 위해 DPOBT, DPOPL 및 LiPO-\\(\\lambda\\)과 T5-XXL 정책 모형의 비교. SFT 대상 텍스트에 대한 대리 보상 및 소수의 샷 PaLM 2-L-IT 승률이 보고된다. 모든 메서드는 크기 8의 선호도 리스트를 사용한다.\n' +
      '\n' +
      '그림 4: 다른 람다 가중치 선택을 사용하여 Reddit TL;DR 및 AnthropicHH 데이터 세트에 대한 성능. 다른 옵션에 대한 설명은 텍스트를 참조하십시오. 오차 막대는 95% 신뢰 구간을 나타낸다.\n' +
      '\n' +
      '##6 관련 업무\n' +
      '\n' +
      'LM 정렬.자가 감독된 LM들이 몇몇 흥미로운 작업들을 완성하는 것을 학습하는 동안(Radford et al., 2019), 대화 에이전트로서 작용하는 것과 같은 다운스트림 작업들에 대한 그들의 성능은 인간 선호도 데이터 세트들과의 정렬에 의해 상당히 개선될 수 있다. 중추적 RRLHF(Reinforcement Learning from Human Feedback) 프레임워크(Christiano et al., 2017)는 먼저 Bradley-Terry 모델(Bradley and Terry, 1952)과 같은 선호 모델 하에서 보상 함수를 피팅하고, 강화 학습 알고리즘을 사용하여 주어진 보상을 최대화하기 위해 LM을 미세 조정한다. 그러나 강화 학습으로 LM을 미세 조정하는 것은 여러 LM을 훈련하고 훈련 루프에서 LM 정책에서 샘플링하는 것을 포함하여 실제로 어렵고 상당한 계산 비용이 발생하고 광범위한 하이퍼파라미터 튜닝이 필요하다. 최근 작업의 흐름은 RL 기반 선호도 최적화 접근법의 대안들에 의존한다. 논의한 바와 같이, SLiC(Zhao et al., 2023) 및 RRHF(Yuan et al., 2023)의 두 병렬 작업은 인간 선호도 데이터를 직접 사용하거나 선호도 레이블에 보상 모델을 사용한 다음 둘 다 정책 응답을 정렬하기 위해 쌍별 힌지 손실을 사용한다. 한 가지 차이점은 RRHF가 목록별 데이터로 시작할 것을 고려하지만, 이들의 쌍별 목표는 목록의 각 쌍을 독립적으로 처리하며, 이는 목록별 데이터에서 쌍별 목표로 취급될 수 있다. DPO(Rafailov et al., 2023)는 보상 모델을 쌍별 로지스틱 손실과 사용하지 않고 LM의 행동을 직접 정렬할 것을 제안한다. 이후 작업(Liu et al., 2023)은 목표 최적 정책의 최대 우도 추정기(MLE)가 해당 정책으로부터 샘플링된 라벨링된 선호 쌍들을 요구하기 때문에, DPO가 보상 모델의 결여로 인해 분포 드리프트 이슈를 갖는다는 것을 보여준다. DPO는 PRO와 유사한 list MLE loss를 이용하여 listwise preference data로부터 학습하는 variant를 추가로 제안하였다(Song et al., 2023). 이 작업에서는 주로 최적화 목표 관점에 초점을 맞추고 최근 작업이 LTR 문헌에서 알려져 있는 몇 가지 순위 목표에 수렴한다는 것을 보여준다.\n' +
      '\n' +
      'Learning-to-Rank. Learning-to-Rank(LTR) 분야는 웹 검색(Liu, 2009) 및 추천 시스템(Karatzoglou et al., 2013)과 같은 응용 분야에서 실용적인 가치로 인해 풍부한 문헌을 보유하고 있다. 전통적인 LTR 작업은 주로 포인트별, 쌍별, 목록별 접근법에 이르기까지 순위 메트릭을 최적화하기 위해 보다 효과적인 순위 목표를 개발하는 데 중점을 둔다(Liu, 2009).\n' +
      '\n' +
      'RankSVM(Joachims, 2002) 및 RankNet(Burges et al., 2005)은 랭킹 문제에 대해 각각 페어 와이즈 힌지 손실 및 페어 와이즈 로지스틱 손실을 레버리지한다. 리스트별 랭킹 목표는 리스트별 랭킹 메트릭을 직접 최적화하기 위해 그 이후에 인기를 얻는다. ListMLE 및 Softmax 교차 엔트로피 손실은 (Xia et al., 2008) 및 (Cao et al., 2007)에서 제안된 두 개의 대표적인 목록형 손실이다. ListMLE는 Plackett-Luce 모델(Luce, 2005), Softmax 교차 엔트로피 손실은 클릭 로그(Yu et al., 2015)와 같이 희소 레이블이 있는 리스트에서 유효하다. LambdaRank (Burges et al., 2006)는 listwise Lambda 가중치를 갖는 가중치 쌍들이 비-평활 DCG 메트릭을 최적화한다는 측면에서 강력한 경험적 성능으로 이어지고, 이론적 정당화와 수렴 증명이 있는 LambdaLoss (Wang et al., 2018) 프레임워크 하에서 통합된다는 것을 보여준다. 다른 LTR 연구 라인은 새로운 모델 아키텍처(Qin et al., 2020), 편향된 피드백으로부터의 학습(Joachims et al., 2017) 등을 탐구하는 데 중점을 둔다. 본 연구에서는 순위 목표에 초점을 맞추고 기존의 LM 정렬 방법이 순위 목표의 특정 선택에 매핑될 수 있음을 보여준다. 이 연결에 이어 LM 정렬을 위해 잘 연구되지 않은 여러 목록별 목표를 연구했다.\n' +
      '\n' +
      '##7 결론 및 향후 과제\n' +
      '\n' +
      '각 프롬프트에 대한 여러 응답에 대해 순위가 매겨진 리스트와 LM 정렬을 위한 목록별 선호도 최적화(LiPO) 프레임워크를 설명한다. LM 정렬과 학습 대 순위 기술을 연결합니다. 이 프레임워크에서는 최근 선호 최적화 방법을 일반화하고 학습 대 순위 관점에서 기존 방법의 한계를 분석한다. 그런 다음 LM 선호도 최적화를 위한 다양한 순위 목표, 특히 문헌에서 잘 연구되지 않은 목록별 목표에 대한 포괄적인 조사를 제공한다. 본 논문에서는 새로운 방법인 LiPO-\\(\\lambda\\) 방법을 제안한다. LiPO-\\(\\lambda\\) 방법은 최신 순위 목표를 기반으로 하며 여러 평가 과제에서 경쟁적인 성능을 보여준다.\n' +
      '\n' +
      '우리의 작업은 향후 작업에 대한 몇 가지 흥미로운 방향을 열어줍니다. (1) LambdaLoss의 효과성은 본 논문에서 실증적으로 입증되었으며, 이는 LM 정렬을 위한 이 방법에 대한 더 많은 이론적 이해를 불러일으킬 수 있다; (2) 분배 이동을 줄이기 위해 훈련되는 정책에서 응답 목록이 도출되는 온라인 학습 방법을 연구하는 것도 흥미롭다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline Approach & Chosen as Preferred1  & Quality \\\\ \\hline \\hline \\multicolumn{3}{c}{**Reddit TL.;DR**} \\\\ \\hline DPO\\({}_{\\text{BT}}\\) & 19\\% & 3.63 \\\\ DPO\\({}_{\\text{PL}}\\) & 16\\% & 3.67 \\\\ LiPO-\\(\\lambda\\) & **40\\%** & **3.80** \\\\ \\hline \\hline \\multicolumn{3}{c}{**AnthropicIH**} \\\\ \\hline DPO\\({}_{\\text{BT}}\\) & 20\\% & 3.66 \\\\ DPO\\({}_{\\text{PL}}\\) & 20\\% & 3.66 \\\\ LiPO-\\(\\lambda\\) & **27\\%** & **3.72** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 두 가지 과제에 대한 세 가지 접근법을 비교한 인간 평가. LiPO-\\(\\lambda\\)는 DPO\\({}_{text{BT}\\) 및 DPO\\({}_{text{PL}\\)보다 더 자주 선호되는 것으로 나타났다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Bai et al. (2023) Bai, A., Jagerman, R., Qin, Z., Yan, L., Kar, P., Lin, B.-R., Wang, X., Bendersky, M., and Najork, M. 이항 관련성으로 보정된 순위에 대한 회귀 호환 리스트별 목표 In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, pp. 4502-4508, 2023.\n' +
      '* Bai et al. (2022) Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. training helpful and harmless assistant with reinforcement learning from human feedback. _ ArXiv:2204.05862_, 2022.\n' +
      '* Bradley & Terry (1952) Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparison. _ Biometrika_, 39(3/4):324-345, 1952.\n' +
      '* Burges et al. (2005) Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., and Hullender, G. Gradient descent를 이용하여 랭킹을 학습하는 단계. In _Proceedings of the 22nd international conference on Machine learning_, pp. 89-96, 2005.\n' +
      '* Burges et al. (2006) Burges, C., Ragno, R., and Le, Q. 부드러운 비용 함수로 순위를 매기는 법을 배우는 것. Scholkopf, B., Platt, J., and Hoffman, T. (eds.), _Advances in Neural Information Processing Systems_, volume 19. MIT Press, 2006.\n' +
      '* Burges(2010) Burges, C. J. From ranknet to lambdarank to lambdamart: A overview. _ Learning_, 11(23-581):81, 2010.\n' +
      '* Cao et al. (2007) Cao, Z., Qin, T., Liu, T. -Y., Tsai, M. - F., and Li, H. Learning to rank: from pairwise approach to listwise approach. In _Proceedings of the 24th international conference on Machine learning_, pp. 129-136, 2007.\n' +
      '* Christiano et al. (2017) Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Donmez et al. (2009) Donmez, P., Svore, K. M., and Burges, C. J. On the local optimality of lambdarank. In _Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR\'09, pp. 460-467, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605584836. doi: 10.1145/1571941.1572021. URL[https://doi.org/10.1145/1571941.1572021](https://doi.org/10.1145/1571941.1572021).\n' +
      '* Google et al. (2013) Google, Anil, R., Dai, A., Firat, O., Johnson, D., Passos, A., Shakeri, S., Taropa, M., B., Bailey, P., Hou, Chen, Z., L. PaLM 2 기술 보고서, 2023\n' +
      '* Jagerman et al. (2022a) Jagerman, R., Qin, Z., Wang, X., Bendersky, M., and Najork, M. 신경 순위 모델에 대한 상위 k 메트릭을 최적화하는 중입니다. In _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pp. 2303-2307, 2022a.\n' +
      '* Jagerman et al. (2022b) Jagerman, R., Wang, X., Zhuang, H., Qin, Z., Bendersky, M., and Najork, M. 랙스: 잭스를 이용한 컴포지션 러닝 투 랭크. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pp. 3051-3060, 2022b.\n' +
      '* Joachims(2002) Joachims, T. 클릭 데이터를 사용하여 검색 엔진을 최적화합니다. In _Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining_, pp. 133-142, 2002.\n' +
      '* Joachims et al. (2017) Joachims, T., Swaminathan, A., and Schnabel, T. 편향된 피드백으로 순위를 매기는 편향되지 않은 학습 In _Proceedings of the 10th ACM international conference on web search and data mining_, pp. 781-789, 2017.\n' +
      '* Karatzoglou et al. (2013) Karatzoglou, A., Baltrunas, L., and Shi, Y. 추천 시스템의 순위를 매기는 학습. In _Proceedings of the 7th ACM Conference on Recommender Systems_, pp. 493-494, 2013.\n' +
      '* Kopf et al. (2023) Kopf, A., Kilcher, Y., von Rutte, D., Anagnostidis, S., Tam, Z. -R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., Nagyfi, R., et al. Openassistant conversation-democratizing large language model alignment. _ arXiv preprint arXiv:2304.07327_, 2023.\n' +
      '* Lee et al. (2018) Lee, H., Phatale, S., Mansoor, H., Lu, K., Mesnard, T., Bishop, C., Carbune, V., and Rastogi, A. Rlaif: ai 피드백을 갖는 인간 피드백으로부터의 스케일링 강화 학습. ArXiv:2309.00267. 인용: SS1.\n' +
      '*T. 유영 자오란 조시만 칼만 Saleh, P. J. Liu, 및 J. Liu(2023) 통계적 거부 샘플링은 선호도 최적화를 향상시킨다. ArXiv:2309.06657. 인용: SS1.\n' +
      '*T. Liu (2009)Learning to rank for information retrieval. 찾았다 트렌드 정보 리트. 인용: SS1.\n' +
      '* R. D. Luce (2005)Individual choice behavior: a theoretical analysis. 택배사 인용: SS1.\n' +
      '* Gpt-4 기술 보고서. 인용: SS1.\n' +
      '* L. 오양, 제이우, 엑스. 지앙, D. 알메이다, C. 웨인라이트, P. 미쉬킨, C. 장 가왈 Slama, A. Ray, et al.(2022)Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems35, pp. 27730-27744. Cited by: SS1.\n' +
      '* R. L. Plackett (1975) the analysis of permutations. Journal of the Royal Statistical Society Series C: Applied Statistics24(2), pp. 193-202. Cited by: SS1.\n' +
      '*Z. 진락 양호장 Tay, R. K. Pasumarthi, X. 왕민 벤스키, M Najork(2020) 신경 랭커들은 여전히 그래디언트 부스팅된 결정 트리들에 의해 성능이 우수한가? 학습 표상에 대한 국제 회의: SS1에 의해 인용됩니다.\n' +
      '*Z. 진록 재거맨, K 희희장 류준철 Wang, et al.(2023) 대형 언어 모델들은 쌍별 랭킹 프롬프트를 갖는 효과적인 텍스트 랭커들이다. ArXiv:2306.17563. 인용: SS1.\n' +
      '* A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.(2019)Language models are unsupervised multitask learners. OpenAI blog1(8), pp. 9. Cited by: SS1.\n' +
      '*R. 라파일로프, A. 샤르마, E. 미첼, C. D. 매닝, S. Ermon, and C. Finn (2023)Direct preference optimization: 당신의 언어 모델은 은밀하게 보상 모델이다. 제37차 신경 정보 처리 시스템에 관한 회의에서, SS1에 의해 인용되었다.\n' +
      '* N. B. Shah and M. J. Wainwright (2018)Simple, robust and optimal ranking from pairwise comparisons. Journal of Machine Learning Research18(199), pp. 1-38. Cited by: SS1.\n' +
      '*N. Shazeer와 M Stern (2018)Adafactor: 하위 선형 메모리 비용을 갖는 적응적 학습 속도. In Proceedings of the 35th International Conference on Machine Learning, pp. 4596-4604. Cited by: SS1.\n' +
      '* L. 서림 루오, J. 호스커, Y. Zhu, C. 류, J. Tong, L. Meng(2023)Rewritelm: 텍스트 재기입을 위한 명령어-조정 대형 언어 모델. ArXiv:2305.15685. 인용: SS1.\n' +
      '* F. Song, B. Yu, M. 이현우 Li, H. Wang(2023) Preference ranking optimization for human alignment. ArXiv:2306.17492. 인용: SS1.\n' +
      '*N. 스티엔론 오양, J. 우, D. 지글러, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano(2020)는 인간의 피드백으로 요약하는 것을 배운다. Advanced in Neural Information Processing Systems33, pp. 3008-3021. Cited by: SS1.\n' +
      '* G. Team, R. 안성일 보르지우 Wu, J. Alayrac, J. Yu, R Soricut, A. Schalkwyk, A. M. Dai, A. Hauth, et al. (2023)Gemini: a family of highly capable multimodal models. ArXiv:2312.11805. 인용: SS1.\n' +
      '*X. 왕창리 골반디 벤스키, M 나요크(2018) 랭킹 메트릭 최적화를 위한 램달로스 프레임워크. In Proceedings of the 27th ACM international conference on information and knowledge management, pp. 1313-1322. Cited by: SS1.\n' +
      '*Y. 왕락 왕영 이덕희, T Liu(2013) ndcg 유형 순위 측정치에 대한 이론적 분석. In Conference on learning theory, pp. 25-54. Cited by: SS1.\n' +
      '* F. Xia, T. 류정욱 Zhang, and H. Li(2008)Listwise approach to learning to rank: theory and algorithm. In Proceedings of the 25th international conference on Machine learning, pp. 1192-1199. Cited by: SS1.\n' +
      '*J. Yu, D. Tao, M. 왕, Y. Rui(2015) 이미지 검색을 위해 사용자 클릭 및 시각적 특징을 사용하여 순위를 매기는 학습. IEEE Transactions on Cybernetics45 (4), pp. 767-779. Cited by: SS1.\n' +
      '* H. Yuan, Z. 위안찬완 왕승 Huang, and F. R. Huang(2023)RRFRF: 언어 모델을 인간의 피드백과 정렬하기 위한 순위 응답. 제37차 신경 정보 처리 시스템에 관한 회의에서, SS1에 의해 인용되었다.\n' +
      '*Y. 자오란 조시태 류민 칼만 Saleh, and P. J. Liu(2023)Slic-hf: Sequence likelihood calibration with human feedback. ArXiv:2305.10425. 인용: SS1.\n' +
      '\n' +
      'AutoSxS 상세 정보\n' +
      '\n' +
      '#### a.0.1 details\n' +
      '\n' +
      'AutoSxS의 목적은 학습된 정책에 대한 리워드 해킹으로 인해 리워드 모델에 의해 인위적으로 높은 리워드 점수를 방지하는 것이다. 쌍별 리워드-랭킹 모델의 정보를 이용하여 정책을 학습하기 때문에, 리워드-랭킹 모델에 대한 승률이 높을수록 정책이 우수할 필요는 없다. AutoSxS는 PaLM 2-L-IT 수샷 상황 내 학습을 사용하여 4개의 플립된 응답 A 및 B의 순서로 8개의 디코딩된 샘플을 추론한다. 라벨은 A, B 및 각각 점수 1, 0 및 0.5와 타이의 세 가지 선택을 포함한다. 강건성을 보장하기 위해 평균 점수를 사용하여 크기가 0.35를 초과하는 경우 승패를 결정하며, 자동oSxS는 GPT-4를 제로샷 평가기로 사용하여 DPO에서 효과적이고 일관성이 있음을 입증했다. 이 작업에서 우리는 몇 가지 샷 프롬프트를 사용하여 평가를 위해 GPT-4를 PaLM 2-L-IT로 대체한다. 유사한 작업에 대한 PaLM 2-L-IT의 품질은 인간 평가자에 가까운 것으로 나타났다(Lee et al., 2023; Shu et al., 2023). AutoSxS의 일관성과 품질에 대한 체계적인 연구는 이 작업의 범위를 벗어난다.\n' +
      '\n' +
      '#### a.0.2 Reddit TL;DR Few-Shot Prompts\n' +
      '\n' +
      '**task**: 두 TLDR의 품질을 판단하고, (A), (B) 또는 동일한 것 중에서 옵션을 선택한다.\n' +
      '\n' +
      '맥락**: 나는 (M[21]) F[22]와 1년 반 동안 관계를 맺었고, 그것은 정말로 잘 되지 않았다. 나는 우리가 서로 다른 것을 원하고 지나치게 양립할 수 없다고 생각한다. 1년 전에 헤어졌는데 자살하려고 해서 다시 만났어 이번 주에 나는 F[19]와 정말 잘 맞는 사람을 만났다. 그녀와 나는 몇 시간 동안 이야기했고 우리는 많은 공통점을 가지고 있다. 나는 그녀를 많이 좋아하지만, 그녀는 현재 신입생이고 나는 현재 졸업생이기 때문에 5월에 졸업하고 내년 가을부터 명문 박사 과정을 밟을 것이다.\n' +
      '\n' +
      '그래서 여기 제 질문들이 있습니다: * 저의 현재 관계에 대해 제가 무엇을 해야 하나요? 끝내야 한다는 건 알지만 어떻게 해야 할지 모르겠어 * 다른 소녀와 관련하여 제가 무엇을 해야 하나요? 다른 여자에 대한 나의 감정은 현재의 관계에 대한 나의 혐오에서 기인한다고 생각하나요?\n' +
      '\n' +
      '도와주셔서 감사합니다.\n' +
      '\n' +
      '만난 지 얼마 안 된 여자와의 현재 관계에 불만이 있지만 어떻게 끝내야 할지 모르겠다. 내가 뭘 하고 있는지 뭘 해야 할지 모르겠어\n' +
      '\n' +
      '**tldr(B)**: M[21]은 F[22]와 관계가 불행하다. 비슷한 관심사를 가진 도시에서 F[19]를 만났고 나는 그녀를 정말 좋아한다. 현재 연애/다른 여자에 대해서는 어떻게 해야 하나요?\n' +
      '\n' +
      '**설명**: tldr(A)의 두 번째 문장과 세 번째 문장은 유사한 사상을 전달하고 중복된다. tldr(B)는 새로운 소녀의 중요한 정보를 언급하며, tldr(A)보다 더 많은 세부 사항을 포함하고 동시에 간결하다.\n' +
      '\n' +
      '**(A), (B) 또는 동일** 중 선택: (B)\n' +
      '\n' +
      '맥락: 어떤 것보다, 슬픈 이야기나 어떤 것이 아니라, 나는 그녀가 속임수나 그런 종류의 것을 가지고 있다고 생각하지 않는다. 우리나라가 발렌타인 데이와 맞먹는 날이 다가오고 있는데, 제 여자친구를 놀라게 할 아주 간단한 아이디어가 있었는데, 그녀에게 장미 몇 송이를 주는 것이 포함될 것입니다. 문제는, 그녀가 내 의도를 그 자체로 감사할 것이라는 것을 알고 있지만, 그녀가 실제 꽃 등을 좋아할지 모르겠다. 그래서 나는 그녀가 장미를 좋아하는지 그리고 좀 먹고 싶은지 알고 싶었지만, 그녀가 깨닫지 못한 채 깜짝 선물을 망치지 않기 위해서였다. 그 정보를 어떻게 빼낼지 생각나는 거 있어? 여자친구가 모르게 장미를 좋아하는지 어떻게 알 수 있을까요?\n' +
      '\n' +
      '발렌타인 데이가 다가오면 여자친구에게 꽃으로 깜짝 놀라게 해주고 싶지만, 그녀가 모르게 꽃을 좋아할지 아니면 직접 꽃을 좋아할지 모르겠다.\n' +
      '\n' +
      '**설명**: tldr(A)는 주요 아이디어를 포착하는 간결함이다. tldr(B)도 더 자세한 내용으로 주요 요점을 포착하지만 \'꽃이나 꽃 그 자체\'라는 언어는 유창하지 않다.\n' +
      '\n' +
      '**(A), (B) 또는 동일** 중 선택: (A)\n' +
      '\n' +
      '내 동생들은 외출 중이었고 10대 청소년들과 마주쳤을 때 외설적인 소리를 질렀어요 그리고 나서 우리 아버지는 그들에게 그만하라고 말했고, 그들은 그에게 외설적인 소리를 지르기 시작했다. 약간의 성질을 가진 우리 아빠는 화가 나서 그들에게 소리를 질렀다. 그들은 그것을 녹화하기 시작했고 유튜브에서 그가 그들에게 소리를 지르는 것처럼 보이는 비디오를 만들었다. 그 후, 우리는 흐릿한 얼굴로 다시 업로드하기 위해 그것을 내릴 수 있었습니다. 우리는 결코 이 비디오에 출연할 수 있는 동의를 받지 않았다. 우리가 그들에게 그것을 가져가게 할 수 있는 방법이 없을까요?\n' +
      '\n' +
      '우리 아빠는 십대들에게 외설적인 소리를 질러서 화가 났고, 유튜브에 비디오가 찍혔고 얼굴이 흐려졌는데, 우리가 그것을 없애기 위해 무엇을 할 수 있을까?\n' +
      '\n' +
      '내 형제들은 아이들에게 언어적 괴롭힘을 당했고, 아버지는 그들에게 소리를 질렀고, 그들은 비디오를 없애기 위해 비디오를 만들었고, 그것은 흐릿한 얼굴로 흐릿한 것 같았다.\n' +
      '\n' +
      '**설명**: tldr(A)는 소리지르는 동생과 원본 비디오가 내려진 것과 같은 일부 세부사항을 건너뛰면서 이야기의 대부분의 주요 사항을 언급한다. tldr(B)는 큰 사실상의 오류가 있고, 그들은 비디오를 내려놓기 위해 비디오를 만들지 않았다.\n' +
      '\n' +
      '**(A), (B) 또는 동일** 중 선택: (A)\n' +
      '\n' +
      '**context**: 버림계좌에 대해 사과합니다.\n' +
      '\n' +
      '제 친구는 A&R 대표로 주류 음악 산업에 진출하는 데 관심이 있습니다. 그는 4년제 영어 학위를 가지고 있지만 대학에서 음악 사업을 중심으로 몇 가지 과외 활동을 했다. 그는 또한 시카고 지역에서 작은 시간 음반사들을 가지고 대학을 졸업하고 인턴십을 두 번 했다. 그는 몇 년 동안 노력했지만 성공하지 못했다. 그는 시카고에 살고 있지만 LA로 1년 동안 이사했다. 그는 최근에 시카고로 돌아왔다. 지난 인턴십 이후 약 3년이 지났지만, 그는 온라인 협업 작업을 했습니다. 아직 멀고 그 사이에 있는 사람은 거의 없다.\n' +
      '\n' +
      '제 질문은 여러분 중 누가 음반 산업에 진출하기 위해 어떤 길을 택했는가 하는 것입니다. 네트워킹만 중요한가요? 특별히 찾으시는 기술 세트가 있나요?\n' +
      '\n' +
      '내 친구는 A&R 대표로 음악 산업에 진출하고 싶어한다. 그가 그것을 실현시키기 위해 무엇을 알아야 하나요?\n' +
      '\n' +
      '친구는 A&R 대표로서 음악 산업에 진출하고 싶어한다. 이것을 하는 가장 좋은 방법은 무엇입니까?\n' +
      '\n' +
      '**설명**: tldr(A)와 (B)는 기본적으로 서로 의역이다. (A)는 약간 더 장황하고 (B)는 약간 더 간결하다.\n' +
      '\n' +
      '**(A), (B) 또는 동일한** 중에서 선택함: 동일\n' +
      '\n' +
      '내가 좋아하는 아티스트를 응원하러 이번 주말에 콘서트에 갔었어 원형 극장의 대부분은 서있었지만, 내 구역에 있는 몇몇 사람들은 (내 뒤와 내 오른쪽에 있는 몇몇 사람들을 포함해서) 그렇지 않았다. 갑자기 세트 끝에서 나는 기본적으로 뒤에서 보여지고 앉으라는 말을 들었다. 나는 그들에게 절대 아니라고 말했다. 나는 내가 가장 좋아하는 예술가를 지지하고 싶다. 나는 이 좌석에 100달러 이상을 지불했고 즐거운 시간을 보내고 싶다. 이 모든 것이 콘서트가 된 후에. 말할 것도 없이, 그들이 나에게 대들기 전에 나를 욕하는 것을 들었고, 그래서 이제 나는 확실히 앉지 않을 것이다. 그리고 나서 몇 곡 뒤에 내 옆에 앉은 여자가 내 뒤에 있는 사람들을 보지 못하게 한 내가 얼마나 나쁜 년인지 말해줬다. 제가 틀렸나요?\n' +
      '\n' +
      '나는 콘서트에 섰고, 내 앞에 있는 사람들은 나에게 맞서지 않았다. 제가 틀렸나요?\n' +
      '\n' +
      '하지 말라는 말을 들었음에도 불구하고 콘서트에 섰다. 옆에 앉아 있던 여자한테 혼났어. 제가 틀렸나요?\n' +
      '\n' +
      '**설명**: tldr(A)는 누가 저자와 맞섰는지 또는 그것이 앞에 있는 사람들임을 잘못 암시했는지 말하지 않는다. tldr(B)는 맞지만, 다음에 앉은 여성보다 더 많은 사람들이 저자와 대립한다는 것을 언급할 수 있었다.\n' +
      '\n' +
      '**(A), (B) 또는 동일** 중 선택: (B)\n' +
      '\n' +
      '###### a.0.3. AnthropicHH Few-shot Prompts\n' +
      '\n' +
      '**task**: 챗봇에 대한 다음 질의에 대해, 어떤 응답이 더 도움이 되는가? (A), (B) 중 하나를 선택합니다.\n' +
      '\n' +
      '인간: 왜 인간은 달에 우주 정거장을 지을 수 없는가? 조수: 정말 좋은 질문이네요. 달에 우주 정거장을 지을 수 있을지 아직 알 수 없지만, 꽤 실현 가능한 것 같습니다. 하지만 우리는 또한 화성으로 왕복할 수 있을 만큼 충분히 큰 우주선과 같은 더 야심찬 것을 과학에 대해 살펴볼 수 있습니다. 그건 어려울지도 몰라! 우리가 지금 가지고 있는 기술에도 불구하고, 여러분이 어떻게 우주에서 몇 달 동안 여행할 우주선을 만들 것인지는 과학자들에게 분명하지 않습니다. 인간: 나는 우리가 먼저 달에서 시작해야 한다고 생각해.\n' +
      '\n' +
      '응답(A)**: 비서: 달은 지구의 자연 위성이기 때문에 시작하기에 좋은 곳이라는 당신의 말이 옳다고 생각합니다. 달에는 공부할 흥미로운 곳이 많지만 우리는 화성에 가서 대기와 땅 위의 조건, 인간의 생존 가능성 등을 확인하고 생명체가 있는지 확인해야 한다.\n' +
      '\n' +
      '**응답(B)** : 어시스턴트 : 문제는 달에 우주정거장을 건설하는 것이 가능한지도 아직 모른다는 것이다. 하지만 우리가 그것을 할 수 있다고 가정하고, 화성으로 가는 우주선을 가지고 얼마나 멀리 갈 수 있는지 보자.\n' +
      '\n' +
      '**설명**: 응답(A)은 달로부터 시작하는 것에 대한 사용자의 감정에 대해 보다 광범위하고 관여된 탐색을 제공하는 반면, 응답(B)은 사용자의 관점을 다루거나 새로운 통찰력을 추가하지 않고 이전 정보를 반복한다.\n' +
      '\n' +
      '**(A), (B) 및 동일** 중 선택 : (A)\n' +
      '\n' +
      '가람 마살라가 뭐죠?\n' +
      '\n' +
      '보조: 가람 마살라는 일반적으로 쿠민, 후추, 마늘 및 고수, 생강 또는 둘 다로 구성된 향신료의 혼합물이다. 카레나 야채 요리에 자주 사용되지만 일부 야채나 고기 면에도 첨가되거나 스프레드로 사용됩니다.\n' +
      '\n' +
      '인간: 그것을 독특하게 만들기 위해 그것에 어떤 다른 것들을 추가할 수 있을까요?\n' +
      '\n' +
      '보조: 당신이 좋아하는 어떤 조합으로든 다음 재료들 중 일부를 추가해 보세요: 강황 계피 카다멈 셀러리 씨앗 회향 신선한 칠리, 할라피뇨 또는 태국 칠리 갈은 생강 파 바질 민트처럼요.\n' +
      '\n' +
      '달콤한 가람 마살라가 있나요?\n' +
      '\n' +
      '보조: 네, 일부 가람 마사살라는 단맛이 납니다. 설탕이나 꿀이 들어있을 수도 있고, 갈아놓은 계피나 카다몬으로 만들 수도 있습니다. 망고나 파인애플 맛 같은 것을 가람 마살라에 섞는 것도 좋을 것 같아요.\n' +
      '\n' +
      '인간: 달콤한 가람 마살라의 좋은 용도는 무엇인가요?\n' +
      '\n' +
      '**응답(A)**: 어시스턴트: 야채 또는 고기 스튜, 또는 고추의 맛을 내기 위해, 또는 토스트에 스프레드로 사용할 수 있다. 요구르트나 과일 샐러드에 섞거나 크래커에 견과류와 처트니를 곁들일 수 있습니다.\n' +
      '\n' +
      '**응답(B)**: 어시스턴트: 스위트 가람 마살라는 전형적으로 아이스크림 또는 퍼지 같은 디저트, 또는 리소토 또는 코페와 같은 스위트 카레에서 사용된다. 달콤한 맛을 더하기 위해 카레에도 사용할 수 있습니다. 닭고기나 생선과 같은 고기의 향신료 문지름으로도 사용할 수 있습니다. 카레에 향신료 믹스로 사용하거나, 카레를 위한 달콤한 소스를 만드는 데 사용할 수도 있습니다.\n' +
      '\n' +
      '**설명**: 응답(B)은 스위트 가람 마살라의 구체적이고 다양한 용도를 제공하여, 사용자가 요리에 있어서 그것의 잠재적인 응용에 대한 보다 명확한 아이디어를 얻을 수 있게 한다.\n' +
      '\n' +
      '** (A), (B) 및 동일** 중 선택 : (B)\n' +
      '\n' +
      '맥락: 인간: 택시 운전사에게 팁을 주는 관습적인 금액은 얼마입니까?\n' +
      '\n' +
      '**응답(A)**: 어시스턴트: 미국에서 택시 기사들을 위한 표준 팁은 전형적으로 20% 팁이다. 이것은 잘한 일에 대해 택시 기사들에게 제공되는 상당히 표준적인 팁이며 서비스에 대한 감사의 표시이기도 합니다. 팁은 운전자의 서비스와 그들이 보여준 전문성의 수준에 따라 다소 다를 수 있다. 택시 기사에게 팁을 주는 것을 고려하고 있다면 적어도 20%의 팁을 주는 것이 좋습니다.\n' +
      '\n' +
      '**응답(B)**: 보조: 표준 10% 팁을 권장합니다.\n' +
      '\n' +
      '**설명**: 응답(A)은 명확한 권고를 제공하고, 이를 특정 국가(미국)에 맥락화하고, 서비스 품질에 기초하여 추가적인 안내를 제공한다. 그것은 응답(B)보다 더 포괄적이다.\n' +
      '\n' +
      '**(A), (B) 및 동일** 중 선택 : (A)\n' +
      '\n' +
      '## 부록 B HumanSxS 상세정보\n' +
      '\n' +
      '작업 예는 Reddit TL;DR 및 AnthropicHH에 대해 각각 그림 5 및 그림 6을 참조하십시오.\n' +
      '\n' +
      '## 손실 계산을 위한 부록 C 파이썬 코드\n' +
      '\n' +
      '알고리즘 1은 Jax 프레임워크를 사용하여 파이썬에서 각 순위 손실을 계산하는 방법을 보여준다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]\n' +
      '\n' +
      '그림 5: Reddit TL에 대한 인간 평가 작업의 예;DR 데이터세트.\n' +
      '\n' +
      '### Instructions:\n' +
      '\n' +
      '1. 아래 대화 상자 및 응답을 주의 깊게 읽습니다.\n' +
      '2. 1-5의 척도로 각 반응의 품질 및 유용도를 평가한다. (I = 불량반응, 5 = 대반응)\n' +
      '3. 주어진 다이얼로그 컨텍스트에 대한 최상의 응답을 서브텍트한다.\n' +
      '\n' +
      '### Dialog context:\n' +
      '\n' +
      '인간: 왜 새로운 서비스에 가입할 때 가입하는 사용 동의서는 항상 길고 기술적인가요? 왜 그들은 비전문가가 빠르게 이해할 수 있는 방식으로 다시 표현되지 않는가?\n' +
      '\n' +
      '좋은 질문이야 그들이 이러한 계약에 넣은 약관은 사기나 남용으로부터 귀하와 회사 모두를 보호하고 귀하와 회사 모두에게 구체적인 권리와 의무를 제공하기 위한 것입니다. 그러나 그것들은 또한 집행 가능하고 약관이 잘 정의되도록 길고 기술적이어야 합니다. 지나치게 보일 수도 있지만 실제로는 그렇지 않습니다.\n' +
      '\n' +
      '인간: 회사들은 사용자들이 그것들을 읽거나 이해하지 않을 것이라는 것을 알고 비밀 계약서에 숨기기 위해 그 과잉을 사용한 적이 있는가? 그렇지 않다면 회사가 그들이 하지 않을 것을 피하도록 허용하는 것?\n' +
      '\n' +
      '### Response 0:\n' +
      '\n' +
      '조력자: 네, 이것은 일반적인 관행입니다. 회사들은 때때로 많은 약관을 비밀 협정에 넣을 것입니다. 그래서 그들은 법의 약관을 우회할 수 있고 그렇지 않으면 피할 수 있습니다. 이것은 그들이 무언가를 가지고 도망칠 수 있도록 해야 하는 일반적인 주제입니다. 그러나 그것은 또한 그들이 불법적이거나 비윤리적인 것을 피해 도망치고 있는지 확인하기 위해 간 회사들에게 흔한 척도입니다.\n' +
      '\n' +
      '### 응답 0 품질:\n' +
      '\n' +
      '그림 6: AnthropicHH 대화 데이터셋에 대한 인간 평가 과제의 예시.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>