<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding\n' +
      '\n' +
      'Pengkun Liu\n' +
      '\n' +
      '1Academy for Engineering and Technology, Fudan University, Shanghai, China 12Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China 2\n' +
      '\n' +
      'Yikai Wang\n' +
      '\n' +
      '2Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China 2\n' +
      '\n' +
      'Fuchun Sun\n' +
      '\n' +
      '2Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China 2\n' +
      '\n' +
      'Jiafang Li\n' +
      '\n' +
      '2Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China 2\n' +
      '\n' +
      'Hang Xiao\n' +
      '\n' +
      '1Academy for Engineering and Technology, Fudan University, Shanghai, China 12Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China 2\n' +
      '\n' +
      'Hongxiang Xue\n' +
      '\n' +
      '1Academy for Engineering and Technology, Fudan University, Shanghai, China 12Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China 2\n' +
      '\n' +
      'Xinzhou Wang\n' +
      '\n' +
      '3Tongji University, Shanghai, China 32Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China 2\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Encouraged by the growing availability of pre-trained 2D diffusion models, image-to-3D generation by leveraging Score Distillation Sampling (SDS) is making remarkable progress. Most existing methods combine novel-view lifting from 2D diffusion models which usually take the reference image as a condition while applying hard L2 image supervision at the reference view. Yet heavily adhering to the image is prone\n' +
      '\n' +
      'Figure 1: Isotropic3D is a novel framework to generate multiview-consistent and high-quality 3D content from **a single CLIP embedding** of the reference image. Our method is proficient in generating multi-view images that maintain mutual consistency, as well as producing a 3D model characterized by symmetrical and neat content, regular geometry, rich colored texture, and less distortion, all while preserving similarity.\n' +
      '\n' +
      'to corrupting the inductive knowledge of the 2D diffusion model leading to flat or distorted 3D generation frequently. In this work, we reexamine image-to-3D in a novel perspective and present Isotropic3D, an image-to-3D generation pipeline that takes only an image CLIP embedding as input. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuth angle by solely resting on the SDS loss. The core of our framework lies in a two-stage diffusion model fine-tuning. Firstly, we fine-tune a text-to-3D diffusion model by substituting its text encoder with an image encoder, by which the model preliminarily acquires image-to-image capabilities. Secondly, we perform fine-tuning using our Explicit Multi-view Attention (EMA) which combines noisy multi-view images with the noise-free reference image as an explicit condition. CLIP embedding is sent to the diffusion model throughout the whole process while reference images are discarded once after fine-tuning. As a result, with a single image CLIP embedding, Isotropic3D is capable of generating multi-view mutually consistent images and also a 3D model with more symmetrical and neat content, well-proportioned geometry, rich colored texture, and less distortion compared with existing image-to-3D methods while still preserving the similarity to the reference image to a large extent. The project page is available at [https://isotropic3d.github.io/](https://isotropic3d.github.io/). The code and models are available at [https://github.com/pkunliu/Isotropic3D](https://github.com/pkunliu/Isotropic3D).\n' +
      '\n' +
      'Keywords:Image-to-3D CLIP Embedding Multi-view Attention\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Generating novel 3D contents that resemble a single reference image plays a crucial role in 3D computer vision, widely applicable to animation production, game development, and virtual reality [8, 10, 15, 24, 26, 44]. Thanks to the rapid growth of diffusion models in denoising high-quality images, there emerges a novel 3D generation pipeline that further synthesizes 3D objects by optimizing any 2D image views based on Score Distillation Sampling (SDS), as initially designed by DreamFusion [30] and widely adopted in many follow-up works [3, 19, 21, 22, 23, 38, 40, 45, 46, 47].\n' +
      '\n' +
      'Specifically for the image-to-3D task, it is natural to apply SDS optimization on novel azimuth angles with additional hard L2 supervision so that the rendered image at the reference view complies with the reference image. Furthermore, it should be noted that these methods [21, 22, 31, 40] mostly concatenate the reference image latent to the input noisy latent directly. In this way, they make the synthesis view resemble the input view as much as possible. However, empirical results indicate that such a kind of pipeline usually leads to three issues: i) 3D distortion or flattening. The conditional diffusion model will be limited in its generation capability. The way of forced supervision deviates from the original intention of generation, causing the model to compromise on conditional images and leading to flat or distorted 3D generation frequently. ii) Multi-face problem. Due to the self-occlusion and invisible area, the network needs to rely on illusions to generate novel views. Generating other views that closely resemble the input view is a common challenge. iii) Multi-view inconsistency. The generated 3D content cannot remain consistent across different viewpoints. These methods can only ensure that the reference image is as consistent as possible with the generated novel views, but tend to be weak at constraining the strong consistency between the multiple generated views.\n' +
      '\n' +
      'To better address these issues, recent works [5, 11, 17, 23, 38, 41, 42, 45, 57] strive to generate multi-view images from a single image using 2D diffusion models. A text-to-3D generation method MVDream [38] proposes a multi-view diffusion model that can generate consistent images. It turns out that the consistency between generated views and the quality of novel views largely determines the geometry and texture of the 3D content generated.\n' +
      '\n' +
      'In contrast to existing SDS-based image-to-3D generation methods, we introduce **Isotropic3D** in this work, an image-to-3D generation pipeline that takes only an image CLIP embedding as input. It allows the optimization to be isotropic w.r.t. the azimuth angle since the SDS loss is uniformly applied without being corrupted by the additional L2 supervision loss. We provide a systematic comparison with typical image-to-3D methods in Table 1, where ours is unique regarding both the input style and loss. The key idea of Isotropic3D is to leverage the power of the 2D diffusion model itself without compromising on the input reference image by adding hard supervision during the 3D generation stage. Concretely, to preliminarily enable the diffusion to have the capability of image-conditioning, we first fine-tune a text-to-3D diffusion model with a substituted image encoder. We then propose a technique dubbed Explicit Multi-view Attention (EMA) which further fine-tunes the diffusion model with the combination of noisy multi-view images and the **noise-free reference image** as an explicit condition. CLIP embedding is sent to the diffusion model throughout the whole process while reference images are discarded once after fine-tuning.\n' +
      '\n' +
      'Naively, an image CLIP embedding preserves semantic meanings but lacks geometry structures and textural details. However, thanks to our designed techniques in Isotropic3D, as shown in Fig. 6, We demonstrate that even with a simple CLIP, our framework can still generate high-quality 3D models with rich color and well-proportioned geometry. We observe that our method is robust to\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l} \\hline \\hline Method & Prompt & 3D model & Input style & \\(L_{2}\\) loss & SDS \\\\ \\hline Realfusion [25] & Image & NeRF & CLIP + Image & ✓ & ✓ \\\\ Zero123 [21] & Image & SJC & CLIP + Image & ✓ & ✓ \\\\ Makelt3D [40] & Image + Text & NeRF & CLIP + Image & ✓ & ✓ \\\\ Magic123 [31] & Image + Text & NeRF & CLIP + Image & ✓ & ✓ \\\\ Syncdreamer [22] & Image / Text & NeRF / NeuS & CLIP + Image & ✓ & \\(\\times\\) \\\\ Wonder3D [23] & Image & NeuS & CLIP + Image & ✓ & \\(\\times\\) \\\\\n' +
      '**Our Isotropic3D** & Image & NeRF & **CLIP** & \\(\\times\\) & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Overview of related works in image-to-3D generation.** Distinguishing from previous works (especially SDS-based image-to-3D methods), our **Isotropic3D** only takes an image CLIP embedding as input and gets rid of the \\(L_{2}\\) supervision loss.\n' +
      '\n' +
      'the object pose of the reference image. Besides, there is still a large degree of consistency retained with the reference image.\n' +
      '\n' +
      'To summarize the contribution of our paper as follows:\n' +
      '\n' +
      '* We propose a novel image-to-3D pipeline called Isotropic3D that takes only an image CLIP embedding as input. Isotropic3D aims to give full play to 2D diffusion model priors without requiring the target view to be utterly consistent with the input view.\n' +
      '* We introduce a view-conditioned multi-view diffusion model that integrates Explicit Multi-view Attention (EMA), aimed at enhancing view generation through fine-tuning. EMA combines noisy multi-view images with the noise-free reference image as an explicit condition. Such a design allows the reference image to be discarded from the whole network during the SDS-based 3D generation process.\n' +
      '* Experiments demonstrate that with a single CLIP embedding, Isotropic3D can generate promising 3D assets while still showing similarity to the reference image.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Our work focuses on 3D generation from a single image. In this section, we review the literature on 3D generative models and optimize-based 3D generation, which has achieved remarkable performance by utilizing the capability of diffusion architecture and pre-trained models.\n' +
      '\n' +
      '### 3D Generative Models\n' +
      '\n' +
      'Generative models such as variational autoencoders (VAEs) [18], generative adversarial networks (GANs) [10], and diffusion models (DMs) [14] have achieved remarkable success in the field of 2D generation. Recently, research [1, 2, 7, 12, 13, 9, 1, 28] has extended its application to 3D generation. AutoSDF [27] applied VQ-VAE [43] to project high-dimensional continuous 3D shapes into low-dimensional latent space and combined it with a transformer to complete the conditional generation task. By integrating 3D scenes into GANs, the new model [29, 39, 50, 52, 54, 55] exhibits improved capability in generating images of higher quality and controllability.\n' +
      '\n' +
      'Building upon the 2D diffusion model, 3D-aware methods like [51] have reformulated the task of 3D perceptual image generation. They approach it by generating a multi-view 2D image set, followed by developing a sequential unconditional-conditional process for multi-view image generation. Dream-Fields [16] combined neural rendering with image and text representations to synthesize diverse 3D objects from natural language prompts independently. The model can generate the geometry and color of a variety of objects without 3D supervision. Based on the DreamFields [16], DreamFusion [30] used the Imagen text-to-image diffusion model [35] to replace the CLIP model [32], which enhanced the quality of 3D content derived from natural language and demonstrated the feasibility of generating a 3D model from a single 2D image.\n' +
      '\n' +
      '### Optimize-based 3D Generation\n' +
      '\n' +
      'Dreamfusion [30] proposed Score Distillation Sampling (SDS) to address 3D data limitations, which has driven the recent development of 2D lifting methods [21, 33, 34, 36, 40, 53].\n' +
      '\n' +
      'Zero123 [21] proposed a single-view 3D generation framework, that leveraged geometric prior knowledge learned from natural images using large-scale diffusion models to generate novel views. The generative model, when coupled with NeRF [48], is capable of effectively modeling 3D scenes from single-view images. MakeIt3D [40] designed a universal 3D generation framework that utilized diffusion priors as 3D perception supervision in a coarse-to-fine manner to create high-fidelity 3D content from a single image. Although achieving high-quality and high-fidelity target generation without suffering from the limitations of 3D data, these models occurred inconsistent multi-view generation. To cope with the problem, some methods [20, 22, 23, 49, 56] try to add conditional constraints to supervise the image consistency in the process of applying the 2D diffusion model to generate multi-view images. Wonder3D [23] enhanced information exchange among different views through the introduction of cross-domain attention, which is proficient in generating multi-view images that preserve both semantic and geometric coherence. MVDream [38] integrates 2D image generation with 3D data consistency, guiding 3D generation through a multi-view prior. This approach not only preserves the generalization capability of 2D generation but also enhances the performance of 3D tasks. As a concurrent effort, Imagedream [45] necessitates highly-matched image-text correspondence. Nevertheless, well-designed text prompts also struggle to accurately describe image information. It also introduces a new set of MLPs inserted in the MVDiffusion side, which increases the difficulty of model training. In contrast, Isotropic3D only requires a single image as input to the model, eliminating the need for text prompts. Additionally, we employ the pre-trained CLIP model directly as the image encoder and keep it frozen throughout the training process.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      'We propose Isotropic3D, as shown in Fig. 2, which is an image-to-3D generation pipeline that takes only an image CLIP embedding as input and allows the optimization to be isotropic w.r.t. the azimuth angle by solely resting on the SDS loss. Isotropic3D is composed of two parts: i) **View-conditioned muti-view diffusion model**. A framework with Explicit Multi-view Attention (EMA) is used to generate diverse but high-quality consistent multi-view images. ii) **Neural Radiance Field (NeRF)**. A 3D network yields high-quality 3D content optimized by rendered images via Score Distillation Sampling (SDS).\n' +
      '\n' +
      '### Motivation\n' +
      '\n' +
      'In order to align the reference image and target images, Zero123 adopts two strategies: one concatenates the latent target view encoded by VAE [18] withthe input view latent on the channel, and the other takes the CLIP embedding of the reference image as conditional information.\n' +
      '\n' +
      'Some recent works improve on this basis, consistent123 [49] and Zero123plus [37] apply to share self-attention mechanism which appends a self-attention key and value matrix from a noisy input view image to the corresponding attention layer. The same level of Gaussian noise as the target view is added to the input view image and then denoising via the UNet network together with the noisy target view. However, we found that existing methods combine novel-view lifting from 2D diffusion models which usually take the reference image as a condition while applying hard L2 image supervision at the reference view.\n' +
      '\n' +
      'Unlike the previous 3D generation with complex strong constraints, our goal is to generate more regular geometry, naturally colored textures, and less distortion with only an image CLIP embedding as input. At the same time, 3D content still preserves the similarity to the reference image to a large extent. Therefore, we present Isotropic3D, an image-to-3D generation pipeline that takes only an image CLIP embedding as input. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuth angle by solely resting on the SDS loss.\n' +
      '\n' +
      '### View-Conditioned Multi-view Diffusion\n' +
      '\n' +
      '**Architecture.** Given a reference image \\(y\\in\\mathbb{R}^{1\\times H\\times W\\times C}\\) as model input view, our method is to generate multi-view images \\(x\\in\\mathbb{R}^{N\\times H\\times W\\times C}\\) from \\(N\\) different\n' +
      '\n' +
      'Figure 2: The pipeline of Isotropic3D. Neural Radiance Field (NeRF) utilizes volume rendering to extract four orthogonal views, which are subsequently augmented with random Gaussian noise. These views, along with noise-free reference images, are then transferred to a multi-view diffusion model for predicting added noise. Note that, we **set the timestep \\(t\\) to zero** at the corresponding position of noise-free reference images. The framework that generates consistent multi-view images from only a single CLIP embedding can be aligned with the input view while retaining the consistency of the output target view. Finally, NeRF yields high-quality 3D content optimized by rendered images via Score Distillation Sampling (SDS). \\(\\mathcal{L}_{\\mathcal{SDS}}\\) can refer to Eq. (7).\n' +
      '\n' +
      'viewpoints aligned with input view and keep consistent to each other. The VAE encoder is denoted as \\(\\mathcal{E}\\). The latent vector of reference image can be written as \\(z^{v}=\\mathcal{E}(y)\\). The camera parameters of different viewpoints is \\(\\pi=\\{\\pi_{1},\\pi_{2},...,\\pi_{N}\\}\\). We denote joint probability distribution as \\(p(x,y)=p_{\\theta}(x|y)p_{\\theta}(y)\\). In multi-view diffusion, this distribution can be written as\n' +
      '\n' +
      '\\[p(x^{(1:N)},y):=p_{\\theta}(x^{(1:N)}|y). \\tag{1}\\]\n' +
      '\n' +
      'Therefore, the reverse process of the view-conditioned multi-view diffusion model can be extended. We can formulate this process as\n' +
      '\n' +
      '\\[p_{\\theta}(\\mathbf{x}_{0:T}^{1:N},c)=p(x_{T}^{1:N},c)\\prod_{t=1}^{T}p_{\\theta}(\\bm {x}_{t-1}^{1:N}\\mid\\mathbf{x}_{t}^{1:N},c), \\tag{2}\\]\n' +
      '\n' +
      'where \\(p(x_{T}^{1:N},c)\\) represents Gaussian noises, while \\(p_{\\theta}(\\mathbf{x}_{t-1}^{1:N}\\mid\\mathbf{x}_{t}^{1:N},c)\\) denotes a Gaussian distribution. Here \\(t\\) is the time step, and \\(c\\) encompasses condition information, comprising the reference image \\(y\\) and camera parameters \\(\\pi\\).\n' +
      '\n' +
      'Figure 3: View-Conditioned Multi-view Diffusion pipeline. Our training process is divided into two stages. In the first stage (Stage1), we fine-tune a text-to-3D diffusion model by substituting its text encoder with an image encoder, by which the model preliminarily acquires image-to-image capabilities. Stage1-a and Stage1-b are the single-view diffusion branch and the multi-view diffusion branch for the first stage respectively. In the second stage (Stage2), we perform fine-tuning multi-view diffusion model integrated Explicit Multi-view Attention (EMA). EMA combines noisy multi-view images with the noise-free reference image as an explicit condition. Stage2-a and Stage2-b are diffusion branches for the second stage. During inference, we only need to send the **CLIP embedding** of the reference image and camera pose to generate consistent high-quality images from multiple perspectives.\n' +
      '\n' +
      'To inherit the performance of MVDream [38], the view-conditioned muti-view diffusion model is designed with two branches: single-view generation and multi-view generation, as shown in Fig. 3. Single-view generation branch receives input from a pair of random perspectives. The purpose of this is to preserve the model\'s ability to generate arbitrary views. Multi-view generation branch takes one of the random perspectives as input view, but the outputs are from four perspectives. Through such supervised training, we lay the foundation for ensuring that the model can generate arbitrary perspectives while ensuring consistency between generated views. We will introduce the data preparation in Sec. 4.1.\n' +
      '\n' +
      '**Explicit multi-view attention (EMA).** Achieving high-quality and consistent target views is fundamental to generating regular geometry and detailed texture. To this end, we design a new attention mechanism called Explicit Multi-view Attention (EMA), as shown in Fig. 4.\n' +
      '\n' +
      'In contrast to Zero123 [21], MVDream [38] and Wonder3D [23], our Explicit Multi-view Attention concatenates the noise-free reference image feature with the noisy image latent/latents as the network input. At the same time, the corresponding timesteps \\(t^{v}\\) and Gaussian noise \\(\\epsilon^{v}\\) of the noise-free reference image are set to \\(\\mathbf{0}\\). The noisy latent vector \\(z_{t}\\) can be written as\n' +
      '\n' +
      '\\[\\mathbf{z}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{z}+\\sqrt{1-\\bar{\\alpha}_{t}}\\mathbf{ \\epsilon}, \\tag{3}\\]\n' +
      '\n' +
      'and thus the noise-free latent vector \\(z_{t}^{v}\\) is denoted as\n' +
      '\n' +
      '\\[\\mathbf{z}_{t}^{v}=\\mathbf{z}^{v}\\quad s.t.\\sqrt{\\bar{\\alpha}_{t}}=1,t=t^{v}=0, \\epsilon=\\epsilon^{v}=\\mathbf{0}, \\tag{4}\\]\n' +
      '\n' +
      'where \\(\\bar{\\alpha}_{t}\\) is variance schedule [14], \\(\\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\). The purpose is that our target view can clearly capture the characteristic details of the input view during the self-attention process of the model.\n' +
      '\n' +
      '**Optimazation.** The core of our Isotropic3D lies in this two-stage view-conditioned multi-view diffusion model fine-tuning. The first stage aims to trans\n' +
      '\n' +
      'Figure 4: Illustration of the Explicit Multi-view Attention (EMA). “View-Input” is a feature map of the noise-free reference image. “View 1” and “View 1 \\(\\sim\\) 4” are feature maps of noisy rendered views. “Alternative” means a 30% chance of using single-view diffusion (Stage2-a) and a 70% chance of training with the multi-view diffusion branch (Stage2-b).\n' +
      '\n' +
      'form the model from text-to-image to image-to-image. We fine-tune a text-to-3D diffusion model by substituting its text encoder with an image encoder, by which the model preliminarily acquires image-to-image capabilities. Following the above discussion, the optimization objective for the first stage can be denoted as\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathcal{MV}}=\\mathbb{E}_{z,t,\\pi,\\epsilon}\\left\\|\\epsilon_{\\theta }(z_{t},t,\\pi)-\\epsilon\\right\\|_{2}^{2}, \\tag{5}\\]\n' +
      '\n' +
      'where \\(\\epsilon_{\\theta}\\) signifies the multi-view diffusion process targeted at denoising the noisy latent variable \\(z_{t}\\). The variable \\(t\\) indicates the timestep, and the parameter \\(\\pi\\) pertains to the camera parameters.\n' +
      '\n' +
      'In the second stage, we perform fine-tuning using Explicit Multi-view Attention (EMA), which integrates noisy multi-view images with the noise-free reference image as an explicit condition. To prevent the model from interfering with the consistent relationship of target views, we opt for the prediction noise associated with the target views rather than the prediction noise linked to the reference image. It allows the model only to learn the consistency of the target views and ignores the input view. This strategy enables the model to focus solely on learning the consistency of the target view while disregarding the reference view. The optimization objective for this process can be expressed as\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathcal{E}-\\mathcal{MV}}=\\mathbb{E}_{z^{v},z,t_{v},t,\\pi_{v}, \\pi,\\epsilon}\\left\\|\\epsilon_{\\theta}((z_{t}^{v}\\oplus z_{t}),(t_{v}\\oplus t),(\\pi_{v}\\oplus\\pi))-\\epsilon\\right\\|_{2}^{2}, \\tag{6}\\]\n' +
      '\n' +
      'where noise-free latent \\(z^{v}\\) is derived from the reference image, which is encoded by a Variational Autoencoder (VAE). The variable \\(t_{v}\\) indicates the timestep set to \\(0\\). The parameter \\(\\pi_{v}\\) specifies the camera parameters when both elevation and azimuth are set to \\(0\\). We performed explicit multi-view attention on both single-view generation and multi-view generation branches.\n' +
      '\n' +
      '### NeRF Optimization Stage\n' +
      '\n' +
      'Given a Nerual Radiance Fields \\(\\mathcal{G}\\), we can randomly sample a camera pose parameter and render a corresponding view \\(x\\). The rendered view can be denoted as \\(x=\\mathcal{G}(\\theta)\\). Dreamfusion [30] proposes to use a 2D diffusion model prior to optimizing the NeRF via score distillation sampling (SDS) loss. With the help of an image-to-image 2D diffusion model, a target view is generated when the loss function is minimized, and then the parameter \\(\\theta\\) is optimized so that \\(x\\) looks like a sample of the frozen diffusion model. The SDS loss is formulated as\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathcal{SDS}}=\\mathbb{E}_{z,t,c,\\epsilon}\\left\\|\\epsilon- \\epsilon_{\\phi}(z_{t},t,c)\\right\\|_{2}^{2}, \\tag{7}\\]\n' +
      '\n' +
      'where \\(z\\) is the latent rendered by NeRF with added noise, \\(\\epsilon\\) refer as the Gaussian noise, \\(c\\) is composed of camera parameters \\(\\pi\\) and the reference image \\(y\\). For NeRF optimization, we solely utilize SDS and orientation loss [30] which encourage normal vectors of the density field facing toward the camera when they are visible. The orientation loss [30] is written as\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{orient}}\\ =\\sum_{i}\\text{stop\\_grad}\\left(w_{i}\\right)\\max \\left(0,\\mathbf{n}_{i}\\cdot\\mathbf{v}\\right)^{2}, \\tag{8}\\]where \\(w_{i}\\) is rendering weights, and the direction of the ray is denoted as \\(\\mathbf{v}\\). For regularizing geometry, we choose point lighting and soft shading. We empirically set the guidance scale to 10 which is the same as during multi-view diffusion training. We define our total loss function as\n' +
      '\n' +
      '\\[\\mathcal{L}=\\lambda_{e}\\mathcal{L}_{\\mathcal{SDS}}+\\lambda_{o}\\mathcal{L}_{ \\text{orient}}\\;, \\tag{9}\\]\n' +
      '\n' +
      'where \\(\\lambda_{e}\\) and \\(\\lambda_{o}\\) are loss weights.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'We provide implementation details in Sec. 4.1 and evaluate novel view synthesis with baselines in Sec. 4.2. Furthermore, we compare the ability of 3D generation with image-to-3D methods based on SDS in Sec. 4.3. To assess EMA module and the advantages of Isotropic3D with a single embedding as input, we conduct an ablation study in Sec. 4.4.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '**Datasets preparation.** The Objaverse dataset [4] is a large-scale dataset comprising over 800k annotated 3D objects. We fine-tune our model using this extensive 3D dataset. Following the rendering settings of Syncedreamer [22], all images are resized to \\(256\\times 256\\), with the background reset to white. The camera distance is set to 1.5, and the lighting is randomized HDRI sourced from Blender. We render both a random view set and a fixed perspective set. Each object is rendered with 16 views for both the random and fixed view sets. In the random view set, the elevation range of images is [\\(-10^{\\circ}\\), \\(40^{\\circ}\\)], while the azimuths remain constant. For the fixed view set, the azimuths of target views are evenly spaced within the range [\\(0^{\\circ}\\), \\(360^{\\circ}\\)], with a fixed elevation of \\(30^{\\circ}\\). Additionally, we utilize the Google Scanned Objects (GSO) dataset [6] and randomly collected images to evaluate the performance of our method.\n' +
      '\n' +
      'Figure 5: Qualitative comparison of synthesizing novel views with baseline models [21, 22] on GSO [6] and randomly collected images.\n' +
      '\n' +
      '**Training procedure.** The multi-view generation framework comprises two main branches: single-view diffusion and multi-view diffusion. During tuning, We have a 30% chance of using single-view diffusion and a 70% chance of training with the multi-view diffusion branch. The whole tuning process is divided into two stages. In the first stage, we train an image-to-image model from the text-to-image model called MVDream [38] and keep the same settings of optimizer and \\(\\epsilon\\)-prediction. The training with a batch size of 768 takes about 7 days. In the second stage, we incorporate the explicit attention mechanism to multi-view diffusion model and fine-tune full UNet. The batch size is set to 128 and the training time takes about 1 day. All training is done on 8 Nvidia A800 GPUs. After tuning, Isotropic3D demonstrates the capability to generate multi-view images with only a single CLIP embedding that exhibit mutual consistency and a 3D model characterized by more well-proportioned geometry and colored texture. The 3D generation typically takes around 1 hour on a single GPU.\n' +
      '\n' +
      '**Baselines.** We reproduce and compare the diffusion-based baseline methods including Zero123 [21], MakeIt3D [40], Magic123 [31], Syncedreamer [22]. Zero123 [21] can generate novel-view images of an object from a single-view image. In addition, the model can also be combined with NeRF to perform 3D reconstruction of objects. MakeIt3D [40] leverage prior knowledge from a well-trained 2D diffusion model to act as 3D-aware supervision for high-quality 3D creation. Magic123 [31] adopts a two-stage optimization framework to gen\n' +
      '\n' +
      'Figure 6: Qualitative comparisons of 3D Generation with baseline models. We conducted verification on GSO [6] and randomly collected images. **Isotropic3D** is capable of generating more regular geometry, detailed texture, and less flat compared with Zero123 and Magic123. A video of this result is available at [https://isotropic3d.github.io/](https://isotropic3d.github.io/).\n' +
      '\n' +
      'erate high-quality 3D content by combining 2D prior and 3D prior. Although Zero123 [21] can generate high-quality novel images, there are still difficulties in maintaining consistency in multi-view images. Therefore, SyncDreamer [22] is proposed that generates consistent images from multiple views by utilizing a 3D-aware feature attention mechanism.\n' +
      '\n' +
      '### Novel View Synthesis\n' +
      '\n' +
      'Two factors affect the quality of 3D content generation: one is view consistency, and the other is the quality of new view generation. We compare the synthesis quality of novel views with the baseline models. The qualitative results are shown in Fig. 5. We can find that the images generated by zero123 [21] maintain consistency with the reference images, but there is a lack of consistency between the generated views. Syncreamer [22] designed the volume attention module to enhance the consistency between views, but its generated results appeared to be pathological views when far away from the reference image and were inconsistent with other generated views. Compared with above methods, our model can ensure high-quality novel views and is aligned with the semantics of input views.\n' +
      '\n' +
      '### 3D Generation\n' +
      '\n' +
      'We evaluate the geometry quality generated by different methods. The qualitative comparison results are shown in Fig. 6. For each instance, we only optimize NeRF once via SDS loss, and the 3D contents shown in Fig. 6 are NeRF renderings. For a fair comparison, we perform the first stage of Zero123 and Magic123. For Zero123 [21] and Magic123 [31], their normal is rougher and the geometry is smoother. In contrast, our method performs well in generating 3D models. We do not require the generated content to be aligned completely with the input view, only semantic consistency with the reference image. We can find that our 3D assets maintain high-quality and detailed geometry in texture. Isotropic3D is capable of generating regular geometry, colored texture, and less distortion from a single CLIP embedding compared with existing image-to-3D methods.\n' +
      '\n' +
      'Figure 7: Ablation studies on Explicit Multi-view Attention.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**Explicit multi-view attention.** To verify the effectiveness of our Explicit Multi-view Attention (EMA), we compared the method using multi-view attention proposed by MVDream [38], which is also used in Wonder3D [23]. The qualitative results are shown in Fig. 7. We can find that after the second stage of fine-tuning, the lion\'s leg posture in the first row is more similar to the reference image. At the same time, the texture details of the shoes in the second row are more similar to the reference image. Using explicit multi-view attention can improve the similarity between the target views and the input view without changing the consistency of the target views.\n' +
      '\n' +
      '**Comparison results with other methods on different settings.** As shown in Fig. 8, we compare Isotropic3D with Zero123 [21], MakeIt3D [40], Magic123 [31] and Syncedreamer [22] under different settings:\n' +
      '\n' +
      '* **Using full setting.** All settings are set according to the original parameters of the model. Here we use threestudio library for Zero123 and Magic123. MakeIt3D and Syncedreamer use official implementation.\n' +
      '* **Removing channel-concatenate reference image.** Zero123, Magic123 and Syncedreamer concatenate the reference image with the noisy image in the channel dimension. MakeIt3D does not use channel-concatenate reference image. In order to ensure that the input dimension of the network remains unchanged, we replace the position corresponding to the reference image with a zero-like matrix of equal dimensions.\n' +
      '\n' +
      'Figure 8: Qualitative comparisons in different settings. _CCR_ is denote as **c**hannel-concatenate **r**eference image. _NOTHING_ means that it does not generate anything. A video of this result is available at [https://isotropic3d.github.io/](https://isotropic3d.github.io/).\n' +
      '\n' +
      '* **Removing \\(L_{2}\\) loss supervision.** Zero123, MakeIt3D, Magic123 and Syncedramer use reference image for L2 supervision. We reset all loss weights related to the reference image to zero, including RGB loss, depth loss, and mask loss.\n' +
      '* **Removing channel-concatenate reference image and \\(L_{2}\\) loss supervision together.** Removing channel-concatenate reference image and \\(L_{2}\\) loss supervision together means **generating 3D content with a single CLIP embedding.** Note that MakeIt3D does not use channel-concatenate reference image, we only remove \\(L_{2}\\) loss supervision.\n' +
      '\n' +
      'In Fig. 8, existing image-to-3D methods rely so much on the reference image that they are almost impossible to generate a complete 3D object. When we remove the channel-concatenate reference image, the texture of the 3D model generated by Zero123, Magic123 and Syncedreamer will be reduced. MakeIt3D does not generate properly in most cases. After removing \\(L_{2}\\) loss supervision, MakeIt3D and Syncedreamer can not generate anything at all. When removing channel-concatenate reference image and \\(L_{2}\\) loss supervision together, it means that only using **a single CLIP embedding** to generate 3D models. Only Zero123 and Magic123 can generate low-quality objects without regular geometry and clear texture. MakeIt3D and Syncedreamer can not generate anything completely in our test cases. In comparison, our method can generate multi-view mutually consistent images and high-quality 3D models with only an image CLIP embedding as input.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'In this paper, we propose Isotropic3D, a new image-to-3D pipeline to generate high-quality geometry and texture only from an image CLIP embedding. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuth angle by solely resting on the SDS loss. To achieve this feat, we fine-tune a multi-view diffusion model in two stages, which aims to utilize the semantic information of the reference image but does not require it to be completely consistent with the reference image, thereby preventing the diffusion model from compromising the reference view. Firstly, we perform fine-tuning a text-to-image diffusion model to an image-to-image model by substituting its text encoder with an image encoder. Subsequently, we fine-tune the model with explicit multi-view attention mechanism (EMA) which combines noisy multi-view images with the noise-free reference image as an explicit condition. CLIP embedding is sent to diffusion model throughout the whole process while reference images are discarded once after fine-tuning. Extensive experimental results demonstrate that with a single image CLIP embedding, Isotropic3D is capable of generating multi-view mutually consistent images and a 3D model with more well-proportioned geometry, colored texture, and less distortion compared with existing image-to-3D methods while still preserving the similarity to the reference image as much as possible.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Anciukevicius, T., Xu, Z., Fisher, M., Henderson, P., Bilen, H., Mitra, N.J., Guerrero, P.: Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation. In: CVPR (2023)\n' +
      '* [2] Baillif, B., Cole, J., McCabe, P., Bender, A.: Deep generative models for 3d molecular structure. Current Opinion in Structural Biology (2023)\n' +
      '* [3] Chen, Z., Wang, F., Wang, Y., Liu, H.: Text-to-3d using gaussian splatting. In: CVPR (2024)\n' +
      '* [4] Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3d objects. In: CVPR (2023)\n' +
      '* [5] Deng, C., Jiang, C., Qi, C.R., Yan, X., Zhou, Y., Guibas, L., Anguelov, D., et al.: Nerdi: Single-view nerf synthesis with language-guided diffusion as general image priors. In: CVPR (2023)\n' +
      '* [6] Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K., McHugh, T.B., Vanhoucke, V.: Google scanned objects: A high-quality dataset of 3d scanned household items. In: ICRA (2022)\n' +
      '* [7] Dundar, A., Gao, J., Tao, A., Catanzaro, B.: Fine detailed texture learning for 3d meshes with generative models. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023)\n' +
      '* [8] Eswaran, M., Bahubalendruni, M.R.: Challenges and opportunities on ar/vr technologies for manufacturing systems in the context of industry 4.0: A state of the art review. Journal of Manufacturing Systems (2022)\n' +
      '* [9] Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Gojcic, Z., Fidler, S.: Get3d: A generative model of high quality 3d textured shapes learned from images. NeurIPS (2022)\n' +
      '* [10] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. NeurIPS (2014)\n' +
      '* [11] Gu, J., Trevithick, A., Lin, K.E., Susskind, J.M., Theobalt, C., Liu, L., Ramamoorthi, R.: Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion. In: ICML (2023)\n' +
      '* [12] Henderson, P., Ferrari, V.: Learning single-image 3d reconstruction by generative modelling of shape, pose and shading. International Journal of Computer Vision (2020)\n' +
      '* [13] Henderson, P., Tsiminaki, V., Lampert, C.H.: Leveraging 2d data to learn textured 3d mesh generation. In: CVPR (2020)\n' +
      '* [14] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS (2020)\n' +
      '* [15] Hsiang, E.L., Yang, Z., Yang, Q., Lai, P.C., Lin, C.L., Wu, S.T.: Ar/vr light engines: Perspectives and challenges. Advances in Optics and Photonics (2022)\n' +
      '* [16] Jain, A., Mildenhall, B., Barron, J.T., Abbeel, P., Poole, B.: Zero-shot text-guided object generation with dream fields. In: CVPR (2022)\n' +
      '* [17] Kim, G., Chun, S.Y.: Datid-3d: Diversity-preserved domain adaptation using text-to-image diffusion for 3d generative model. In: CVPR (2023)\n' +
      '* [18] Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013)\n' +
      '* [19] Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation. In: CVPR (2023)* [20] Lin, Y., Han, H., Gong, C., Xu, Z., Zhang, Y., Li, X.: Consistent123: One image to highly consistent 3d asset using case-aware diffusion priors. arXiv preprint arXiv:2309.17261 (2023)\n' +
      '* [21] Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. In: CVPR (2023)\n' +
      '* [22] Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: SyncDreamer: Generating multiview-consistent images from a single-view image. arXiv preprint arXiv:2309.03453 (2023)\n' +
      '* [23] Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H., Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008 (2023)\n' +
      '* [24] Luo, S., Hu, W.: Diffusion probabilistic models for 3d point cloud generation. In: CVPR (2021)\n' +
      '* [25] Melas-Kyriazi, L., Laina, I., Rupprecht, C., Vedaldi, A.: Realfusion: 360deg reconstruction of any object from a single image. In: CVPR (2023)\n' +
      '* [26] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM (2021)\n' +
      '* [27] Mittal, P., Cheng, Y.C., Singh, M., Tulsiani, S.: Autosdf: Shape priors for 3d completion, reconstruction and generation. In: CVPR (2022)\n' +
      '* [28] Muller, N., Siddiqui, Y., Porzi, L., Bulo, S.R., Kontschieder, P., Niessner, M.: Diffrf: Rendering-guided 3d radiance field diffusion. In: CVPR (2023)\n' +
      '* [29] Niemeyer, M., Geiger, A.: Giraffe: Representing scenes as compositional generative neural feature fields. In: CVPR (2021)\n' +
      '* [30] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022)\n' +
      '* [31] Qian, G., Mai, J., Hamdi, A., Ren, J., Siarohin, A., Li, B., Lee, H.Y., Skorokhodov, I., Wonka, P., Tulyakov, S., et al.: Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843 (2023)\n' +
      '* [32] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021)\n' +
      '* [33] Raj, A., Kaza, S., Poole, B., Niemeyer, M., Ruiz, N., Mildenhall, B., Zada, S., Aherman, K., Rubinstein, M., Barron, J., et al.: Dreambooth3d: Subject-driven text-to-3d generation. arXiv preprint arXiv:2303.13508 (2023)\n' +
      '* [34] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aherman, K.: Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In: CVPR (2023)\n' +
      '* [35] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS (2022)\n' +
      '* [36] Shen, Q., Yang, X., Wang, X.: Anything-3d: Towards single-view anything reconstruction in the wild. arXiv preprint arXiv:2304.10261 (2023)\n' +
      '* [37] Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.: Zero123++: a single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110 (2023)\n' +
      '* [38] Shi, Y., Wang, P., Ye, J., Long, M., Li, K., Yang, X.: Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512 (2023)\n' +
      '* [* [39] Tan, F., Fanello, S., Meka, A., Orts-Escolano, S., Tang, D., Pandey, R., Taylor, J., Tan, P., Zhang, Y.: Volux-gan: A generative model for 3d face synthesis with hdri relighting. In: ACM SIGGRAPH (2022)\n' +
      '* [40] Tang, J., Wang, T., Zhang, B., Zhang, T., Yi, R., Ma, L., Chen, D.: Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior. arXiv preprint arXiv:2303.14184 (2023)\n' +
      '* [41] Tang, S., Zhang, F., Chen, J., Wang, P., Furukawa, Y.: MVdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion. arXiv preprint arXiv:2307.01097 (2023)\n' +
      '* [42] Tseng, H.Y., Li, Q., Kim, C., Alsisan, S., Huang, J.B., Kopf, J.: Consistent view synthesis with pose-guided diffusion models. In: CVPR (2023)\n' +
      '* [43] Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning. NeurIPS (2017)\n' +
      '* [44] Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689 (2021)\n' +
      '* [45] Wang, P., Shi, Y.: Imagedream: Image-prompt multi-view diffusion for 3d generation. arXiv preprint arXiv:2312.02201 (2023)\n' +
      '* [46] Wang, X., Wang, Y., Ye, J., Wang, Z., Sun, F., Liu, P., Wang, L., Sun, K., Wang, X., He, B.: Animatabledreamer: Text-guided non-rigid 3d model generation and reconstruction with canonical score distillation. arXiv preprint arXiv:2312.03795 (2023)\n' +
      '* [47] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In: NeurIPS (2023)\n' +
      '* [48] Wang, Z., Wu, S., Xie, W., Chen, M., Prisacariu, V.A.: Nerf-: Neural radiance fields without known camera parameters. arXiv preprint arXiv:2102.07064 (2021)\n' +
      '* [49] Weng, H., Yang, T., Wang, J., Li, Y., Zhang, T., Chen, C., Zhang, L.: Consistent123: Improve consistency for one image to 3d object synthesis. arXiv preprint arXiv:2310.08092 (2023)\n' +
      '* [50] Wu, J., Zhang, C., Xue, T., Freeman, B., Tenenbaum, J.: Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. NeurIPS (2016)\n' +
      '* [51] Xiang, J., Yang, J., Huang, B., Tong, X.: 3d-aware image generation using 2d diffusion models. arXiv preprint arXiv:2303.17905 (2023)\n' +
      '* [52] Xie, J., Ouyang, H., Piao, J., Lei, C., Chen, Q.: High-fidelity 3d gan inversion by pseudo-multi-view optimization. In: CVPR (2023)\n' +
      '* [53] Xu, D., Jiang, Y., Wang, P., Fan, Z., Wang, Y., Wang, Z.: Neurallift-360: Lifting an in-the-wild 2d photo to a 3d object with 360deg views. In: CVPR (2023)\n' +
      '* [54] Xu, Y., Chai, M., Shi, Z., Peng, S., Skorokhodov, I., Siarohin, A., Yang, C., Shen, Y., Lee, H.Y., Zhou, B., et al.: Discoscene: Spatially disentangled generative radiance fields for controllable 3d-aware scene synthesis. In: CVPR (2023)\n' +
      '* [55] Xue, Y., Li, Y., Singh, K.K., Lee, Y.J.: Giraffe hd: A high-resolution 3d-aware generative model. In: CVPR (2022)\n' +
      '* [56] Ye, J., Wang, P., Li, K., Shi, Y., Wang, H.: Consistent-1-to-3: Consistent image to 3d view synthesis via geometry-aware diffusion models. arXiv preprint arXiv:2310.03020 (2023)\n' +
      '* [57] Zhou, Z., Tulsiani, S.: Sparsefusion: Distilling view-conditioned diffusion for 3d reconstruction. In: CVPR (2023)\n' +
      '* [\n' +
      '\n' +
      '## Appendix 0.A Camera Model\n' +
      '\n' +
      'We define \\(\\theta\\), \\(\\varphi\\) and \\(d\\) to represent elevation angle, azimuth angle and camera distance respectively. We systematically sample camera viewpoints within the ranges \\(\\theta\\in[-10^{\\circ},40^{\\circ}]\\), \\(\\varphi\\in[0^{\\circ},360^{\\circ}]\\), and fix \\(d\\) at 1.5. In practice, we convert these angles into radians. The positions of the camera for reference viewpoints and target views are denoted as \\((\\theta_{r},\\varphi_{r},d_{r})\\) and \\((\\theta_{n},\\varphi_{n},d_{n})\\), respectively, with \\(d=d_{r}=d_{n}=1.5\\) and \\(n\\in\\{1,2,...,N\\}\\) representing \\(N\\) distinct target views. The relative transformation between the camera positions is captured by the expression \\(((\\theta_{n}-\\theta_{r},\\varphi_{n}-\\varphi_{r},d))\\). For the single-view diffusion branch, the target view can be any viewpoint. In contrast, during the multi-view diffusion procedure, the target views are confined to four distinct orthogonal viewpoints, each characterized by \\(\\theta=30^{\\circ}\\) and varying \\(\\varphi\\in[0^{\\circ},360^{\\circ}]\\).\n' +
      '\n' +
      '## Appendix 0.B Training Details\n' +
      '\n' +
      '**View-Conditioned Multi-view Diffusion.** We take MVDream [38] as the base model for the first stage of fine-tuning. In our optimization process, the AdamW optimizer is employed with a constant learning rate of \\(10^{-4}\\) and a warm-up phase of 10,000 steps, complemented by the parameters \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.999\\), and a weight decay of \\(10^{-2}\\). We adopt gradient accumulation over 4 steps, resulting in a total batch size of 768. This configuration allowed us to fine-tune the entirety of the UNet model across 50,000 iterations on eight A800 GPUs, spanning a total duration of approximately 7 days. For the second stage, we continue utilizing the AdamW optimizer; however, we transition to a linear learning rate schedule that peaks at \\(5\\times 10^{-5}\\). No gradient accumulation is used. We fine-tune the full UNet with a total batch size of 128 for 5k steps, which takes about 5 hours. Other settings are consistent with the first stage.\n' +
      '\n' +
      '**Isotropic3D.** Our 3D contents are optimized with 10,000 steps on a single A800 GPU. We adopt the AdamW optimizer at a learning rate of 0.01. For regularizing geometry, we choose point lighting and soft shading. We empirically set the guidance scale to 10 which is the same as during multi-view diffusion training. For SDS, we adjust the maximum and minimum time steps from 0.98 to 0.5 and 0.02 respectively in the 8,000 steps. The coefficient of SDS loss is set as \\(\\lambda_{e}=1\\) and the weight of orient loss can be written as\n' +
      '\n' +
      '\\[\\lambda_{o}=\\begin{cases}0.2x,&x\\leq 5,000,\\\\ 1000,&5000<x\\leq 10,000,\\end{cases} \\tag{10}\\]\n' +
      '\n' +
      'where \\(x\\) is the global step. Other settings not explicitly mentioned adhere to the specifications outlined in MVDream.\n' +
      '\n' +
      'Discuss Removing the Reference Image During Inference\n' +
      '\n' +
      'During training, the rationale behind concatenating the noise-free reference image and the noisy target images is to ensure consistency between the noisy targets and the reference image at the feature level. However, we aim to prevent the reference image from influencing the generation of multi-view images. Therefore, although the noise-free reference image is inputted into the network during training, we deliberately exclude it when calculating the loss with ground truth. It means that the reference image only gives explicit feature-level information to other target views in the self-attention layer without affecting the optimization of the model. Consequently, during the inference process, leveraging a single image CLIP embedding, our model is adept at generating multi-view images that are mutually consistent while still retaining a substantial degree of similarity to the reference image.\n' +
      '\n' +
      '## Appendix 0.D Limitations and Discussion\n' +
      '\n' +
      'Although Isotropic3D has better geometry and texture than existing image-to-3D models and exhibits isotropy, the resolution of the rendered 3D contents is not high. We analyze that this may be because our training data only has a resolution of 256. After our extensive experimental results, our model does not perform well on faces, which may require further fine-tuning in downstream tasks.\n' +
      '\n' +
      '## Appendix 0.E Additional Results\n' +
      '\n' +
      '**Addtional results for qualitative comparisons**. In Fig. 9, we show more additional comparison results between Isotropic3D and other baselines. The video on the project website shows a more intuitive effect.\n' +
      '\n' +
      '**More results.** In order to verify the stability of the 3D generation, Fig. 10 shows that two groups of results are from two runs with different seeds. For more detailed and vivid results, we show more examples of 3D contents generated by Isotropic3D in Fig. 11 to Fig. 17. Please visit our website at [https://isotropic3d.github.io/](https://isotropic3d.github.io/).\n' +
      '\n' +
      'Figure 10: Additional Results generated by **Isotrpic3D**. Two groups of results are from two runs.\n' +
      '\n' +
      'Figure 9: Additional Results for qualitative comparisons of Isotropic3D with baseline models. A video of this result is available on our project website.\n' +
      '\n' +
      'Figure 11: Example 3D contents generated by **Isotrpic3D**.\n' +
      '\n' +
      'Figure 12: Example 3D contents generated by **Isotrpic3D**.\n' +
      '\n' +
      'Figure 14: Example 3D contents generated by **Isotrpic3D**.\n' +
      '\n' +
      'Figure 13: Example 3D contents generated by **Isotrpic3D**.\n' +
      '\n' +
      'Figure 16: Example 3D contents generated by **Isotrpic3D**.\n' +
      '\n' +
      'Figure 15: Example 3D contents generated by **Isotrpic3D**.\n' +
      '\n' +
      'Figure 17: Example 3D contents generated by **Isotrpic3D**.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>