<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 등방성 3D: 단일 CLIP 임베딩 기반의 이미지-투-3D 생성\n' +
      '\n' +
      'Pengkun Liu\n' +
      '\n' +
      '1공과대학, 푸단대학교, 상하이, 중국 12베이징 국가정보과학기술연구센터(BNRist), 국가지능기술시스템 핵심연구실, 중국 칭화대학교, 베이징, 중국 2\n' +
      '\n' +
      'Yikai Wang\n' +
      '\n' +
      '2베이징 국가정보과학기술연구센터(BNRist), 국가지능기술시스템 핵심연구실, 중국 베이징 칭화대학교 컴퓨터과학과 2\n' +
      '\n' +
      'Fuchun Sun\n' +
      '\n' +
      '2베이징 국가정보과학기술연구센터(BNRist), 국가지능기술시스템 핵심연구실, 중국 베이징 칭화대학교 컴퓨터과학과 2\n' +
      '\n' +
      'Jiafang Li\n' +
      '\n' +
      '2베이징 국가정보과학기술연구센터(BNRist), 국가지능기술시스템 핵심연구실, 중국 베이징 칭화대학교 컴퓨터과학과 2\n' +
      '\n' +
      'Hang Xiao\n' +
      '\n' +
      '1공과대학, 푸단대학교, 상하이, 중국 12베이징 국가정보과학기술연구센터(BNRist), 국가지능기술시스템 핵심연구실, 중국 칭화대학교, 베이징, 중국 2\n' +
      '\n' +
      'Hongxiang Xue\n' +
      '\n' +
      '1공과대학, 푸단대학교, 상하이, 중국 12베이징 국가정보과학기술연구센터(BNRist), 국가지능기술시스템 핵심연구실, 중국 칭화대학교, 베이징, 중국 2\n' +
      '\n' +
      'Xinzhou Wang\n' +
      '\n' +
      '3통지대학교, 상하이, 중국 32베이징 국가정보과학기술연구센터(BNRist), 국가지능기술시스템 핵심연구실, 중국 칭화대학교, 베이징, 중국 2 컴퓨터과학기술학과\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '사전 훈련된 2D 확산 모델의 가용성이 증가함에 따라 점수 증류 샘플링(SDS)을 활용하여 이미지 대 3D 생성이 눈에 띄게 발전하고 있다. 기존의 대부분의 방법들은 2D 확산 모델로부터 새로운 뷰 리프팅을 결합하는데, 이는 일반적으로 기준 뷰에서 하드 L2 영상 감독을 적용하면서 기준 영상을 조건으로 한다. 그러나 이미지에 크게 집착하는 경향이 있습니다.\n' +
      '\n' +
      '도 1: 등방성 3D는 참조 이미지의 **a 단일 CLIP 임베딩**로부터 다시점-일관성 및 고품질 3D 콘텐츠를 생성하기 위한 새로운 프레임워크이다. 제안하는 방법은 상호 일관성을 유지하는 다시점 영상을 생성하고, 대칭적이고 깔끔한 콘텐츠, 규칙적인 기하학, 풍부한 컬러 텍스처, 왜곡이 적은 3D 모델을 생성하면서도 유사성을 보존하는 데 능숙하다.\n' +
      '\n' +
      '평탄하거나 왜곡된 3D 생성으로 이어지는 2D 확산 모델의 귀납적 지식을 자주 손상시킨다. 본 연구에서는 이미지-to-3D를 새로운 관점에서 재검토하고 이미지 CLIP 임베딩만을 입력으로 하는 이미지-to-3D 생성 파이프라인인 등방성 3D를 제시한다. 등방성 3D를 사용하면 SDS 손실에 대해서만 방위각을 등방성 wrt로 최적화할 수 있다. 이 프레임워크의 핵심은 2단계 확산 모델 미세 조정에 있다. 먼저, 텍스트 인코더를 이미지 인코더로 대체하여 텍스트-투-3D 확산 모델을 미세 조정한다. 이 모델은 이미지-투-이미지 능력을 미리 획득한다. 둘째, 노이지 다시점 영상과 무노이즈 기준 영상을 명시적 조건으로 결합한 명시적 다시점 주의력(Explicit Multi-view Attention, EMA)을 이용하여 미세 조정을 수행한다. CLIP 임베딩은 전체 프로세스에서 확산 모델로 전송되고 참조 이미지는 미세 조정 후 한 번 폐기된다. 그 결과, 단일 이미지 CLIP 임베딩으로 등방성 3D는 참조 이미지와의 유사성을 상당 부분 보존하면서도, 기존의 이미지-대-3D 방식에 비해 대칭적이고 깔끔한 내용, 잘 비례된 기하학, 풍부한 컬러 텍스처, 왜곡이 적은 다시점 상호 일치 이미지를 생성할 수 있다. 프로젝트 페이지는 [https://isotropic3d.github.io/](https://isotropic3d.github.io/]에서 사용할 수 있습니다. 코드 및 모델은 [https://github.com/pkunliu/Isotropic3D](https://github.com/pkunliu/Isotropic3D)에서 사용할 수 있다.\n' +
      '\n' +
      '키워드: Multi-view Attention을 내장한 Image-to-3D CLIP\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '단일 참조 이미지와 유사한 새로운 3D 콘텐츠를 생성하는 것은 3D 컴퓨터 비전에서 중요한 역할을 하며, 애니메이션 제작, 게임 개발 및 가상 현실에 널리 적용 가능하다[8, 10, 15, 24, 26, 44]. DreamFusion[30]에 의해 처음 설계되었고 많은 후속 작업[3, 19, 21, 22, 23, 38, 40, 45, 46, 47]에서 널리 채택된 바와 같이, 고품질의 이미지를 잡음제거하는 확산 모델의 급속한 성장 덕분에, 스코어 증류 샘플링(SDS)에 기초한 임의의 2D 이미지 뷰를 최적화하여 3D 객체를 추가로 합성하는 새로운 3D 생성 파이프라인이 등장한다.\n' +
      '\n' +
      '구체적으로, 이미지-대-3D 태스크에 대해, 참조 뷰에서 렌더링된 이미지가 참조 이미지를 준수하도록 추가적인 하드 L2 감독과 함께 새로운 방위각에 SDS 최적화를 적용하는 것은 자연스럽다. 또한, 이들 방법 [21, 22, 31, 40]은 대부분 입력 잡음 잠재에 잠재된 참조 이미지를 직접 연결한다는 점에 유의해야 한다. 이러한 방식으로, 그들은 합성 뷰를 가능한 한 입력 뷰와 유사하게 만든다. 그러나, 이러한 파이프라인은 일반적으로 3차원 왜곡이나 평탄화의 세 가지 문제를 야기한다. 조건부 확산 모형은 그 생성 능력이 제한적일 것이다. 강제 감독 방식은 생성의 원래 의도에서 벗어나 모델이 조건부 이미지에 타협하게 되고 평평하거나 왜곡된 3D 생성으로 이어지는 경우가 많다. ii) 다중 얼굴 문제. 자기 폐색과 보이지 않는 영역으로 인해 네트워크는 새로운 뷰를 생성하기 위해 환상에 의존할 필요가 있다. 입력 뷰와 매우 유사한 다른 뷰를 생성하는 것은 일반적인 도전이다. iii) 다시점 불일치. 생성된 3D 콘텐츠는 상이한 시점들에 걸쳐 일관성을 유지할 수 없다. 이러한 방법들은 참조 이미지가 생성된 신규 뷰들과 가능한 한 일치함을 보장할 수 있을 뿐, 다수의 생성된 뷰들 사이의 강한 일관성을 제약하는 데 약한 경향이 있다.\n' +
      '\n' +
      '이러한 문제를 더 잘 해결하기 위해 최근 작업[5, 11, 17, 23, 38, 41, 42, 45, 57]은 2D 확산 모델을 사용하여 단일 이미지에서 다중 뷰 이미지를 생성하기 위해 노력한다. 텍스트-투-3D 생성 방법 MVDream[38]은 일관된 이미지를 생성할 수 있는 다시점 확산 모델을 제안한다. 생성된 뷰 간의 일관성과 새로운 뷰의 품질은 생성된 3D 콘텐츠의 지오메트리 및 텍스처를 크게 결정하는 것으로 나타났다.\n' +
      '\n' +
      '기존의 SDS 기반 이미지 대 3D 생성 방법과 달리 이미지 CLIP 임베딩만을 입력으로 하는 이미지 대 3D 생성 파이프라인인 **등방성 3D**를 이 작업에 도입한다. 추가적인 L2 감독 손실에 의해 손상되지 않고 SDS 손실이 균일하게 적용되기 때문에 최적화가 등방성 w.r.t. 방위각이 되도록 한다. 우리는 입력 스타일과 손실 모두에 대해 고유한 표 1의 일반적인 이미지 대 3D 방법과 체계적인 비교를 제공한다. 등방성 3D의 핵심 아이디어는 3D 생성 단계에서 하드 감독을 추가하여 입력 참조 영상에 손상을 주지 않고 2D 확산 모델 자체의 힘을 활용하는 것이다. 구체적으로, 먼저 영상 부호화기를 대체하여 텍스트-3D 확산 모델을 미세 조정한다. 그런 다음, 우리는 명시적 조건으로 잡음 다중 시점 영상과 ** 잡음이 없는 참조 영상**의 조합으로 확산 모델을 더 미세 조정하는 명시적 다중 시점 주의(Explicit Multi-view Attention, EMA)라고 불리는 기술을 제안한다. CLIP 임베딩은 전체 프로세스에서 확산 모델로 전송되고 참조 이미지는 미세 조정 후 한 번 폐기된다.\n' +
      '\n' +
      '순수하게, 이미지 CLIP 임베딩은 의미론적 의미를 보존하지만 기하학적 구조 및 텍스처 세부사항이 부족하다. 그러나 그림 1과 같이 등방성 3D에서 설계된 기술 덕분에. 6, 우리는 간단한 CLIP를 사용하더라도, 우리의 프레임워크가 풍부한 색상과 잘 비례된 기하학을 가진 고품질의 3D 모델을 여전히 생성할 수 있음을 보여준다. 우리는 우리의 방법이 견고하다는 것을 관찰한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l} \\hline \\hline Method & Prompt & 3D model & Input style & \\(L_{2}\\) loss & SDS \\\\ \\hline Realfusion [25] & Image & NeRF & CLIP + Image & ✓ & ✓ \\\\ Zero123 [21] & Image & SJC & CLIP + Image & ✓ & ✓ \\\\ Makelt3D [40] & Image + Text & NeRF & CLIP + Image & ✓ & ✓ \\\\ Magic123 [31] & Image + Text & NeRF & CLIP + Image & ✓ & ✓ \\\\ Syncdreamer [22] & Image / Text & NeRF / NeuS & CLIP + Image & ✓ & \\(\\times\\) \\\\ Wonder3D [23] & Image & NeuS & CLIP + Image & ✓ & \\(\\times\\) \\\\\n' +
      '**Our Isotropic3D** & Image & NeRF & **CLIP** & \\(\\times\\) & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **이미지-to-3D 생성에서의 관련 작품 개요.** 이전 작품(특히 SDS 기반 이미지-to-3D 방법)과 구별되는, 우리의 **등방성 3D**은 이미지 CLIP 임베딩을 입력으로만 취하고 \\(L_{2}\\)의 감독 손실을 제거한다.\n' +
      '\n' +
      '참조 이미지의 객체 포즈. 게다가, 참조 이미지와 유지되는 많은 정도의 일관성이 여전히 존재한다.\n' +
      '\n' +
      '본 논문의 기여도를 요약하면 다음과 같다.\n' +
      '\n' +
      '* 이미지 CLIP 임베딩만을 입력으로 하는 Isotropic3D라는 새로운 이미지-to-3D 파이프라인을 제안한다. 등방성 3D는 목표 뷰가 입력 뷰와 완전히 일치하도록 요구하지 않고 2D 확산 모델 사전에 완전한 플레이를 제공하는 것을 목표로 한다.\n' +
      '* Fine-tuning을 통한 뷰 생성 향상을 목표로 하는 명시적 다시점 주의력(Explicit Multi-view Attention, EMA)을 통합한 뷰 조건 다시점 확산 모델을 소개한다. EMA는 잡음이 있는 다시점 영상과 잡음이 없는 기준 영상을 명시적인 조건으로 결합한다. 이러한 설계는 SDS 기반 3D 생성 프로세스 동안 기준 이미지가 전체 네트워크로부터 폐기될 수 있게 한다.\n' +
      '* 실험은 단일 CLIP 임베딩으로 등방성 3D가 참조 이미지와 여전히 유사성을 나타내면서 유망한 3D 자산을 생성할 수 있음을 입증한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '우리의 작업은 단일 이미지에서 3D 생성에 중점을 둡니다. 본 절에서는 3D 생성 모델에 대한 문헌을 검토하고 확산 아키텍처 및 사전 훈련된 모델의 능력을 활용하여 놀라운 성능을 달성한 기반 3D 생성을 최적화한다.\n' +
      '\n' +
      '### 3D 생성 모델\n' +
      '\n' +
      'VAE(variational autoencoders) [18],GAN(generative adversarial networks) [10], DM(diffusion model) [14]와 같은 생성 모델은 2D 생성 분야에서 괄목할 만한 성공을 거두었다. 최근 연구[1, 2, 7, 12, 13, 9, 1, 28]는 3D 세대로 그 적용 범위를 확장하였다. AutoSDF[27]은 VQ-VAE[43]을 적용하여 고차원 연속 3차원 형상을 저차원 잠재 공간으로 투영하고 변압기와 결합하여 조건 생성 작업을 완료하였다. 3D 장면들을 GAN들에 통합함으로써, 새로운 모델 [29, 39, 50, 52, 54, 55]은 더 높은 품질 및 제어가능성의 이미지들을 생성하는데 향상된 능력을 나타낸다.\n' +
      '\n' +
      '2D 확산 모델을 기반으로 [51]과 같은 3D 인식 방법은 3D 지각 이미지 생성의 작업을 재구성했다. 그들은 멀티뷰 2D 이미지 세트를 생성하고, 이어서 멀티뷰 이미지 생성을 위한 순차적 무조건-조건 프로세스를 개발함으로써 그것에 접근한다. Dream-Fields[16]는 신경망 렌더링과 이미지 및 텍스트 표현을 결합하여 자연 언어 프롬프트로부터 다양한 3D 객체를 독립적으로 합성한다. 모델은 3D 감독 없이 다양한 객체의 기하학 및 색상을 생성할 수 있다. DreamFields[16]을 기반으로 DreamFusion[30]은 Imagen text-to-image 확산 모델[35]을 사용하여 CLIP 모델[32]을 대체하였으며, 이는 자연 언어에서 파생된 3D 콘텐츠의 품질을 향상시키고 단일 2D 이미지로부터 3D 모델을 생성하는 가능성을 보여주었다.\n' +
      '\n' +
      '### 최적화 기반 3D 생성\n' +
      '\n' +
      '드림퓨전[30]은 최근 2D 리프팅 방법[21, 33, 34, 36, 40, 53]의 개발을 주도한 3D 데이터 한계를 해결하기 위해 스코어 증류 샘플링(SDS)을 제안했다.\n' +
      '\n' +
      'Zero123[21]은 새로운 뷰를 생성하기 위해 대규모 확산 모델을 사용하여 자연 이미지에서 학습된 기하학적 사전 지식을 활용하는 단일 뷰 3D 생성 프레임워크를 제안했다. 생성 모델은 NeRF[48]와 결합하면 단일 뷰 이미지에서 3D 장면을 효과적으로 모델링할 수 있다. MakeIt3D[40]은 단일 영상으로부터 고충실도의 3D 콘텐츠를 생성하기 위해 확산 전과를 거친-미세 방식으로 3D 인식 감독으로 활용하는 범용 3D 생성 프레임워크를 설계하였다. 3D 데이터의 한계를 겪지 않고 고품질 및 고충실도 목표 생성을 달성했지만 이러한 모델은 일관되지 않은 다시점 생성이 발생했다. 이러한 문제에 대처하기 위해 일부 방법[20, 22, 23, 49, 56]에서는 2D 확산 모델을 적용하여 다시점 영상을 생성하는 과정에서 영상의 일관성을 감독하기 위한 조건부 제약 조건을 추가하고자 한다. Wonder3D[23]은 교차 도메인 주의의 도입을 통해 서로 다른 뷰들 간의 정보 교환을 향상시켰으며, 이는 의미적 일관성과 기하학적 일관성을 모두 보존하는 멀티뷰 이미지를 생성하는 데 능숙하다. MVDream[38]은 2D 이미지 생성을 3D 데이터 일관성과 통합하여 멀티뷰를 통해 3D 생성을 사전에 안내한다. 이 접근법은 2D 생성의 일반화 능력을 보존할 뿐만 아니라 3D 태스크의 성능을 향상시킨다. 동시 노력으로서, 이미드림[45]은 정합성이 높은 이미지-텍스트 대응을 필요로 한다. 그럼에도 불구하고, 잘 설계된 텍스트는 또한 이미지 정보를 정확하게 기술하기 위해 고군분투한다. 또한 MVD 확산 쪽에 삽입된 새로운 MLP 세트를 도입하여 모델 훈련의 난이도를 높인다. 대조적으로, 등방성 3D는 모델에 대한 입력으로서 단일 이미지만을 필요로 하여 텍스트 프롬프트가 필요하지 않다. 또한, 사전 학습된 CLIP 모델을 영상 인코더로 직접 사용하여 학습 과정 내내 동결 상태를 유지한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '그림 1과 같이 등방성 3D를 제안한다. 2는 이미지 CLIP 임베딩만을 입력으로 하고 SDS 손실에만 쉬어서 방위각을 등방성 wrt로 최적화할 수 있는 이미지-3D 생성 파이프라인이다. 등방성 3D는 i) **View-conditioned muti-view diffusion model**의 두 부분으로 구성된다. EMA(Explicit Multi-view Attention) 프레임워크는 다양하고 고품질의 일관성 있는 다시점 영상을 생성하기 위해 사용된다. ii) **Neural Radiance Field(NeRF)**. 3D 네트워크는 스코어 증류 샘플링(SDS)을 통해 렌더링된 이미지에 의해 최적화된 고품질의 3D 콘텐츠를 산출한다.\n' +
      '\n' +
      '### Motivation\n' +
      '\n' +
      'Zero123은 기준 이미지와 대상 이미지를 정렬하기 위해 VAE[18]로 인코딩된 잠재 대상 뷰를 채널에 잠재된 입력 뷰와 연결하고, 다른 하나는 기준 이미지의 CLIP 임베딩을 조건부 정보로 취하는 두 가지 전략을 채택한다.\n' +
      '\n' +
      '최근 몇 가지 연구는 이를 기반으로 개선되는데, 일관성 있는 123[49]와 Zero123plus[37]은 노이즈 입력 뷰 영상으로부터 해당 어텐션 레이어에 자기 어텐션 키와 값 행렬을 추가하는 자기 어텐션 메커니즘을 공유하기 위해 적용된다. 타겟 뷰와 동일한 레벨의 가우시안 노이즈가 입력 뷰 이미지에 추가된 다음, 노이지 타겟 뷰와 함께 UNet 네트워크를 통해 잡음제거된다. 그러나 기존의 방법은 참조 뷰에서 하드 L2 영상 감독을 적용하면서 참조 영상을 조건으로 하는 2D 확산 모델로부터 새로운 뷰 리프팅을 결합한다는 것을 발견하였다.\n' +
      '\n' +
      '복잡한 제약 조건을 가진 기존의 3D 생성과는 달리, 우리의 목표는 이미지 CLIP 임베딩만을 입력으로 하여 보다 규칙적인 기하학, 자연스런 컬러 텍스처, 그리고 더 적은 왜곡을 생성하는 것이다. 동시에, 3D 콘텐츠는 여전히 기준 이미지와의 유사성을 상당 부분 보존한다. 따라서 본 논문에서는 이미지 CLIP 임베딩만을 입력으로 하는 이미지-투-3D 생성 파이프라인인 Isotropic3D를 제시한다. 등방성 3D를 사용하면 SDS 손실에 대해서만 방위각을 등방성 wrt로 최적화할 수 있다.\n' +
      '\n' +
      '### View-Conditioned Multiview Diffusion\n' +
      '\n' +
      '*아키텍처.** 참조영상\\(y\\in\\mathbb{R}^{1\\times H\\times W\\times C}\\)을 모델 입력 뷰로 주어지면, 본 방법은 \\(N\\)의 서로 다른 다시점 영상\\(x\\in\\mathbb{R}^{N\\times H\\times C}\\)을 생성하는 것이다.\n' +
      '\n' +
      '도 2: 등방성 3D의 파이프라인. Neural Radiance Field (NeRF)는 볼륨 렌더링을 이용하여 4개의 직교 뷰들을 추출하고, 이 뷰들은 연속적으로 랜덤 가우시안 잡음으로 증강된다. 이러한 뷰들은 잡음이 없는 참조 이미지들과 함께 추가된 잡음을 예측하기 위한 멀티뷰 확산 모델로 전달된다. 노이즈가 없는 참조 이미지의 해당 위치에서 타임스텝 \\(t\\)을 0**로 설정합니다. 단일 CLIP 임베딩만으로 일관된 다시점 영상을 생성하는 프레임워크는 출력 대상 뷰의 일관성을 유지하면서 입력 뷰와 정렬될 수 있다. 마지막으로, NeRF는 스코어 증류 샘플링(SDS)을 통해 렌더링된 이미지에 의해 최적화된 고품질의 3D 콘텐츠를 산출한다. \\ (\\mathcal{L}_{\\mathcal{SDS}}\\)는 Eq를 참조할 수 있다. (7).\n' +
      '\n' +
      '시점은 입력 뷰와 정렬되고 서로 일관성을 유지합니다. VAE 인코더는 \\(\\mathcal{E}\\)로 표시된다. 참조영상의 잠재벡터는 \\(z^{v}=\\mathcal{E}(y)\\)로 표기될 수 있다. 각 시점의 카메라 파라미터는 \\(\\pi=\\{\\pi_{1},\\pi_{2},...,\\pi_{N}\\})이다. 우리는 결합 확률 분포를 \\(p(x,y)=p_{\\theta}(x|y)p_{\\theta}(y)\\로 나타낸다. 다시점 확산에서, 이 분포는 다음과 같이 기록될 수 있다.\n' +
      '\n' +
      '\\[p(x^{(1:N)},y):=p_{\\theta}(x^{(1:N)}|y). \\tag{1}\\]\n' +
      '\n' +
      '따라서, 시점 조절된 다시점 확산 모델의 역과정을 확장할 수 있다. 이 과정을 다음과 같이 공식화할 수 있습니다.\n' +
      '\n' +
      '\\[p_{\\theta}(\\mathbf{x}_{0:T}^{1:N},c)=p(x_{T}^{1:N},c)=p(x_{T}^{t=1}^{T}p_{\\theta}(\\bm{x}_{t-1}^{1:N}\\mid\\mathbff{x}_{t}^{1:N},c), \\tag{2}\\t}\n' +
      '\n' +
      '여기서 \\(p(x_{T}^{1:N},c)\\)는 가우시안 잡음을 나타내고, \\(p_{\\theta}(\\mathbf{x}_{t-1}^{1:N}\\mid\\mathbff{x}_{t}^{1:N},c)\\)는 가우시안 분포를 나타낸다. 여기서 \\(t\\)는 시간 단계이며, \\(c\\)는 기준 이미지 \\(y\\)와 카메라 매개변수 \\(\\pi\\)를 포함하는 조건 정보를 포함한다.\n' +
      '\n' +
      '도 3: View-Conditioned Multi-view Diffusion 파이프라인. 우리의 훈련 과정은 두 단계로 나뉩니다. 첫 번째 단계(Stage1)에서는 텍스트 인코더를 이미지 인코더로 대체하여 텍스트-대-3D 확산 모델을 미세 조정하며, 이 모델은 이미지-대-이미지 능력을 미리 획득한다. Stage1-a 및 Stage1-b는 각각 1단에 대한 단일 시점 확산 분기 및 멀티 시점 확산 분기이다. 두 번째 단계(Stage2)에서는 미세 조정 멀티뷰 확산 모델 통합 명시적 멀티뷰 주의(EMA)를 수행한다. EMA는 잡음이 있는 다시점 영상과 잡음이 없는 기준 영상을 명시적인 조건으로 결합한다. 단계 2-a 및 단계 2-b는 두 번째 단계에 대한 확산 분기이다. 추론하는 동안 참조 이미지와 카메라 포즈의 **CLIP 임베딩**만 보내면 여러 관점에서 일관된 고품질 이미지를 생성할 수 있다.\n' +
      '\n' +
      'MVDream[38]의 성능을 계승하기 위해, 뷰-조건 뮤티-뷰 확산 모델은 그림 3과 같이 단일-뷰 생성과 다중-뷰 생성의 두 가지 분기로 설계된다. 단일-뷰 생성 분기는 한 쌍의 무작위 관점에서 입력을 받는다. 이 목적은 임의 뷰를 생성할 수 있는 모델의 기능을 보존하는 것입니다. 멀티뷰 생성 브랜치는 랜덤한 관점 중 하나를 입력 뷰로 취하지만 출력은 네 가지 관점에서 나온다. 이러한 감독 훈련을 통해, 우리는 모델이 생성된 뷰 간의 일관성을 보장하면서 임의의 관점을 생성할 수 있도록 하는 기반을 마련한다. 우리는 4.1절에 자료 준비를 소개할 것이다.\n' +
      '\n' +
      '**명시적 다중 뷰 주의(EMA).** 고품질 및 일관된 목표 뷰를 달성하는 것은 규칙적인 기하학 및 상세한 텍스처를 생성하는 데 기본이다. 이를 위해 그림 4와 같이 명시적 다시점 주의(Explicit Multi-view Attention, EMA)라는 새로운 주의 메커니즘을 설계한다.\n' +
      '\n' +
      'Zero123 [21], MVDream [38] 및 Wonder3D [23]과 달리, 명시적 멀티뷰 어텐션은 잡음이 없는 참조 영상 특징을 잡음 영상 잠재/잠재와 네트워크 입력으로 연결한다. 이와 동시에 잡음이 없는 기준영상의 대응시간\\(t^{v}\\)과 가우시안 잡음\\(\\epsilon^{v}\\)을 \\(\\mathbf{0}\\)으로 설정한다. 노이지 잠재 벡터 \\(z_{t}\\)는 다음과 같이 쓸 수 있다.\n' +
      '\n' +
      '\\[\\mathbf{z}_{t}=\\sqrt{\\bar{\\alpha}_{t}\\mathbf{z}+\\sqrt{1-\\bar{\\alpha}_{t}\\mathbf{\\epsilon}, \\tag{3}\\\n' +
      '\n' +
      '따라서 잡음이 없는 잠재 벡터 \\(z_{t}^{v}\\)는 다음과 같이 표시된다.\n' +
      '\n' +
      '\\mathbf{z}_{t}^{v}=\\mathbf{z}^{v}\\quad s.t.\\sqrt{\\bar{\\alpha}_{t}=1,t=t^{v}=0, \\epsilon=\\epsilon^{v}=\\mathbf{0}, \\tag{4}\\.\n' +
      '\n' +
      '여기서 \\(\\bar{\\alpha}_{t}\\)는 분산 스케줄[14], \\(\\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\이다. 목적은 우리의 목표 뷰가 모델의 자기 주의 과정에서 입력 뷰의 특징적인 세부 사항을 명확하게 캡처할 수 있다는 것이다.\n' +
      '\n' +
      '**Optimazation.** 등방성 3D의 핵심은 이 2단계 뷰 조건 다중 뷰 확산 모델 미세 조정에 있다. 첫 번째 단계는 트랜스를 목표로 합니다.\n' +
      '\n' +
      '도 4: EMA(Explicit Multi-view Attention)의 일러스트레이션. "View-Input"은 노이즈가 없는 기준 이미지의 특징 맵이다. “View 1” 및 “View 1 \\(\\sim\\)4”는 잡음이 있는 렌더링된 뷰들의 특징 맵들이다. "대안"은 단일 시점 확산(Stage2-a)을 사용할 확률 30%와 다중 시점 확산 분기(Stage2-b)로 훈련할 확률 70%를 의미한다.\n' +
      '\n' +
      '텍스트에서 이미지까지 모델을 형성합니다. 본 논문에서는 텍스트 인코더를 이미지 인코더로 대체하여 텍스트-투-3D 확산 모델을 미세 조정한다. 이 모델은 이미지-투-이미지 기능을 미리 획득한다. 이상의 논의에 따라, 첫 번째 단계에 대한 최적화 목표는 다음과 같이 표시될 수 있다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathcal{MV}=\\mathbb{E}_{z,t,\\pi,\\epsilon}\\left\\|\\epsilon_{\\theta}(z_{t},t,\\pi)-\\epsilon\\right\\|_{2}^{2}, \\tag{5}\\}\n' +
      '\n' +
      '여기서 \\(\\epsilon_{\\theta}\\)는 잡음 잠재 변수 \\(z_{t}\\)의 잡음 제거를 목표로 하는 멀티뷰 확산 프로세스를 의미한다. 변수 \\(t\\)는 타임스테프를 나타내며, 매개변수 \\(\\pi\\)는 카메라 매개변수와 관련이 있다.\n' +
      '\n' +
      '두 번째 단계에서는 노이지 다시점 영상과 잡음이 없는 기준 영상을 명시적 조건으로 통합하는 명시적 다시점 주의력(Explicit Multi-view Attention, EMA)을 이용하여 미세 조정을 수행한다. 모델이 타겟 뷰들의 일관된 관계를 방해하는 것을 방지하기 위해, 참조 이미지에 링크된 예측 잡음보다는 타겟 뷰들과 연관된 예측 잡음을 선택한다. 모델이 대상 뷰의 일관성만 학습할 수 있게 하고 입력 뷰는 무시한다. 이 전략을 통해 모델은 참조 뷰를 무시하면서 목표 뷰의 일관성을 학습하는 데에만 집중할 수 있다. 이 과정에 대한 최적화 목표는 다음과 같이 나타낼 수 있다.\n' +
      '\n' +
      '\\mathcal{L}_{\\mathcal{E}-\\mathcal{MV}=\\mathbb{E}_{z^{v},z,t_{v},t,\\pi_{v}, \\pi,\\epsilon}\\left\\|\\epsilon_{\\theta}((z_{t}^{v}\\oplus z_{t}),(t_{v}\\oplus t),(\\pi_{v}\\oplus\\pi))-\\epsilon\\right\\|_{2}^{2}, \\tag{6}\\epsilon\\left\\|\\epsilon_{\\theta}((z_{t}^{v}\\oplus z_{t}),(t_{v}\\oplus t),(\\pi_{v}\\oplus\\pi))-\\epsilon\\right\\|_{2}^{2}, \\tag{6}\\epsilon\\left\\|\\epsilon_{\\theta}((z_{t}^{v}\\oplus\n' +
      '\n' +
      '여기서 잡음이 없는 잠재 \\(z^{v}\\)는 VAE(Variational Autoencoder)에 의해 인코딩되는 참조 이미지로부터 유도된다. 변수 \\(t_{v}\\)는 \\(0\\)으로 설정된 타임스텝을 나타낸다. 파라미터 \\(\\pi_{v}\\)는 고도 및 방위각을 모두 \\(0\\)으로 설정할 때 카메라 파라미터를 지정한다. 단일 뷰 생성과 다중 뷰 생성 분기 모두에 대해 명시적인 다중 뷰 주의를 수행했다.\n' +
      '\n' +
      '#### : NeRF 최적화 단계\n' +
      '\n' +
      'Nerual Radiance Fields\\(\\mathcal{G}\\)이 주어지면, 우리는 카메라 포즈 파라미터를 랜덤하게 샘플링하고 대응하는 뷰\\(x\\)를 렌더링할 수 있다. 렌더링된 뷰는 \\(x=\\mathcal{G}(\\theta)\\)로 표시될 수 있다. 드림퓨전[30]은 스코어 증류 샘플링(SDS) 손실을 통해 NeRF를 최적화하기 전에 2D 확산 모델을 사용할 것을 제안한다. 이미지 대 이미지 2D 확산 모델의 도움으로 손실 함수를 최소화하면 목표 뷰를 생성한 다음 매개변수 \\(\\theta\\)를 최적화하여 \\(x\\)이 동결 확산 모델의 샘플처럼 보이게 한다. SDS 손실은 다음과 같이 공식화된다.\n' +
      '\n' +
      '\\mathcal{L}_{\\mathcal{SDS}=\\mathbb{E}_{z,t,c,\\epsilon}\\left\\|\\epsilon-\\epsilon_{\\phi}(z_{t},t,c)\\right\\|_{2}^{2}, \\tag{7}\\}\n' +
      '\n' +
      '여기서 \\(z\\)은 잡음이 첨가된 NeRF에 의해 잠재된 것이고, \\(\\epsilon\\)은 가우시안 잡음을 지칭하며, \\(c\\)은 카메라 파라미터 \\(\\pi\\) 및 참조 이미지 \\(y\\)으로 구성된다. NeRF 최적화를 위해 SDS 및 방향 손실[30]만을 사용하여 카메라가 보일 때 카메라를 향하는 밀도 필드의 정규 벡터를 유도한다. 배향 손실 [30]은 다음과 같이 기록된다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{text{orient}}\\ =\\sum_{i}\\text{stop\\_grad}\\left(w_{i}\\right)\\max\\left(0,\\mathbf{n}_{i}\\cdot\\mathbf{v}\\right)^{2}, \\tag{8}\\] 여기서 \\(w_{i}\\)는 렌더링 가중치이고, 광선의 방향은 \\(\\mathbf{v}\\)으로 표시된다. 지오메트리를 정규화하기 위해 점 조명과 부드러운 음영 처리를 선택합니다. 다시점 확산 훈련 시와 동일한 유도 척도를 10으로 경험적으로 설정하였다. 우리는 총 손실 함수를 다음과 같이 정의합니다.\n' +
      '\n' +
      '\\[\\mathcal{L}=\\lambda_{e}\\mathcal{L}_{\\mathcal{SDS}+\\lambda_{o}\\mathcal{L}_{\\text{orient}\\;, \\tag{9}\\]\n' +
      '\n' +
      '여기서 \\(\\lambda_{e}\\) 및 \\(\\lambda_{o}\\)는 손실 가중치이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '본 논문에서는 Sec. 4.1에서 구현 세부사항을 제시하고, Sec. 4.2에서 베이스라인으로 새로운 뷰 합성을 평가한다. 또한, Sec. 4.3에서 SDS를 기반으로 한 이미지-투-3D 방법과 3D 생성의 성능을 비교한다. EMA 모듈과 단일 임베딩을 입력으로 하는 등방성 3D의 장점을 평가하기 위해 Sec. 4.4에서 삭마 연구를 수행한다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '**Datasets preparation.** Objaverse 데이터셋 [4]는 800k 이상의 주석이 달린 3D 객체를 포함하는 대규모 데이터셋이다. 우리는 이 광범위한 3D 데이터 세트를 사용하여 모델을 미세 조정한다. Syncedreamer[22]의 렌더링 설정에 따라 모든 이미지는 \\(256\\times 256\\)으로 크기가 조정되고 배경은 흰색으로 재설정된다. 카메라 거리는 1.5로 설정되고 조명은 블렌더에서 조달한 무작위 HDRI이다. 우리는 임의의 뷰 집합과 고정된 원근법 집합을 모두 렌더링한다. 각 객체는 랜덤 뷰 및 고정 뷰 세트 모두에 대해 16개의 뷰로 렌더링된다. 랜덤 뷰 집합에서 영상의 고도 범위는 [\\(-10^{\\circ}\\), \\(40^{\\circ}\\])인 반면 방위각은 일정하게 유지된다. 고정 뷰 집합의 경우, 목표 뷰의 방위각은 [\\(0^{\\circ}\\), \\(360^{\\circ}\\]) 범위 내에서 고르게 이격되어 있으며, 고도는 \\(30^{\\circ}\\)이다. 또한 Google Scanned Objects (GSO) 데이터셋 [6]과 무작위로 수집한 이미지를 활용하여 본 논문에서 제안한 방법의 성능을 평가한다.\n' +
      '\n' +
      '그림 5: GSO [6]에 대한 기본 모델 [21, 22] 및 무작위로 수집된 이미지와 새로운 뷰를 합성하는 정성적 비교.\n' +
      '\n' +
      '**트레이닝 절차.** 멀티뷰 생성 프레임워크는 단일 뷰 확산 및 멀티뷰 확산의 두 가지 주요 분기로 구성된다. 튜닝하는 동안 단일 시점 확산을 사용할 확률은 30%이고 다중 시점 확산 분기로 훈련할 확률은 70%이다. 전체 튜닝 과정은 두 단계로 나뉜다. 첫 번째 단계에서는 MVDream[38]이라는 텍스트-이미지 모델로부터 이미지-이미지 모델을 학습하고, 최적화기와 \\(\\epsilon\\)-예측의 설정을 동일하게 유지한다. 배치 크기가 768인 훈련은 약 7일이 소요됩니다. 두 번째 단계에서는 다중 뷰 확산 모델에 명시적 주의 메커니즘을 통합하고 전체 UNet을 미세 조정한다. 배치 크기는 128로 설정되며, 훈련 시간은 약 1일이 소요된다. 모든 훈련은 8개의 Nvidia A800 GPU에서 수행된다. 튜닝 후, 등방성 3D는 상호 일관성을 나타내는 단일 CLIP 임베딩과 더 잘 비례하는 기하학 및 컬러 텍스처를 특징으로 하는 3D 모델만으로 멀티뷰 이미지를 생성할 수 있는 능력을 보여준다. 3D 생성은 일반적으로 단일 GPU에서 약 1시간이 소요된다.\n' +
      '\n' +
      '**기준.** Zero123[21], MakeIt3D[40], Magic123[31], Syncedreamer[22]를 포함한 확산 기반 기준 방법들을 재현하고 비교한다. 제로123[21]은 단일 시점 이미지로부터 객체의 신규 시점 이미지를 생성할 수 있다. 또한, 모델은 또한 물체의 3D 복원을 수행하기 위해 NeRF와 결합될 수 있다. MakeIt3D[40]은 잘 훈련된 2D 확산 모델의 사전 지식을 활용하여 고품질 3D 생성을 위한 3D 인식 감독 역할을 한다. Magic123[31]은 2단계 최적화 프레임워크를 도입한다.\n' +
      '\n' +
      '그림 6: 기준 모델과 3D 생성의 질적 비교. 우리는 GSO [6]에 대한 검증을 수행하고 무작위로 이미지를 수집했다. 등방성 3D**는 Zero123과 Magic123에 비해 보다 규칙적인 기하학, 세밀한 질감, 그리고 덜 평탄한 이미지를 생성할 수 있다. 이 결과에 대한 비디오는 [https://isotropic3d.github.io/](https://isotropic3d.github.io/]에서 볼 수 있다.\n' +
      '\n' +
      '2D 사전과 3D 사전의 결합으로 고품질의 3D 콘텐츠를 지웁니다. Zero123[21]은 고품질의 새로운 이미지를 생성할 수 있지만, 다시점 이미지의 일관성을 유지하는데 여전히 어려움이 있다. 따라서, 3D 인식 특징 주의 메커니즘을 활용하여 다수의 뷰로부터 일관된 이미지를 생성하는 SyncDreamer[22]를 제안한다.\n' +
      '\n' +
      '새로운 시각 합성\n' +
      '\n' +
      '3D 콘텐츠 생성의 품질에는 두 가지 요인이 영향을 미치며, 하나는 뷰 일관성이고 다른 하나는 새로운 뷰 생성의 품질이다. 우리는 새로운 뷰의 합성 품질을 기준 모델과 비교한다. 정성적 결과는 그림 5와 같다. 0123[21]에 의해 생성된 이미지는 참조 이미지와 일관성을 유지하지만 생성된 뷰 간에는 일관성이 부족하다는 것을 알 수 있다. Syncreamer[22]는 뷰 간의 일관성을 높이기 위해 볼륨 어텐션 모듈을 설계하였으나, 생성된 결과는 참조 이미지에서 멀리 떨어져 있을 때 병리학적 뷰로 나타났고 다른 생성된 뷰와 일치하지 않는 것으로 나타났다. 위의 방법과 비교할 때, 본 모델은 고품질 신규 뷰를 보장할 수 있으며 입력 뷰의 의미와 일치합니다.\n' +
      '\n' +
      '### 3D Generation\n' +
      '\n' +
      '다양한 방법으로 생성된 지오메트리 품질을 평가한다. 정성적 비교 결과는 그림 6에 나와 있으며 각 경우에 대해 SDS 손실을 통해 NeRF를 한 번만 최적화하고 그림 3D 내용은 그림 6에 나와 있다. 도 6은 NeRF 렌더링이다. 공정한 비교를 위해 Zero123과 Magic123의 첫 번째 단계를 수행하였고, Zero123[21]과 Magic123[31]의 경우, 그 정상은 더 거칠고 기하학은 더 매끄럽다. 이와는 대조적으로, 본 방법은 3D 모델을 생성하는데 있어서 좋은 성능을 보인다. 우리는 생성된 콘텐츠가 입력 뷰와 완전히 정렬될 것을 요구하지 않고, 참조 이미지와의 의미적 일관성만을 요구한다. 3D 자산은 질감에서 고품질 및 상세한 기하학을 유지한다는 것을 알 수 있습니다. 등방성 3D는 기존의 이미지-대-3D 방법에 비해 단일 CLIP 임베딩에서 규칙적인 기하학, 컬러 텍스처 및 더 적은 왜곡을 생성할 수 있다.\n' +
      '\n' +
      '그림 7: 명시적 다시점 주의력에 대한 절제 연구.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**명시적 멀티뷰 어텐션.** 우리의 명시적 멀티뷰 어텐션(EMA)의 유효성을 검증하기 위해, 원더3D[23]에도 사용되는 MVDream[38]에서 제안한 멀티뷰 어텐션을 이용한 방법을 비교하였다. 정성적 결과는 그림 7과 같다. 미세 조정 2단계 이후 첫 번째 행의 사자 다리 자세가 기준 이미지와 더 유사함을 알 수 있다. 동시에, 두 번째 행의 신발의 텍스처 디테일은 기준 이미지와 더 유사하다. 명시적 멀티뷰 어텐션을 사용하면 타겟 뷰들의 일관성을 변경하지 않고 타겟 뷰들과 입력 뷰 사이의 유사성을 개선할 수 있다.\n' +
      '\n' +
      '**다른 설정에 대한 다른 방법과의 비교.** 그림과 같이. 도 8을 참조하면, 서로 다른 설정 하에서 등방성3D와 Zero123[21], MakeIt3D[40], Magic123[31] 및 Syncedreamer[22]를 비교한다:\n' +
      '\n' +
      '**전체 설정을 사용합니다.** 모든 설정은 모델의 원래 매개변수에 따라 설정됩니다. 본 논문에서는 Zero123과 Magic123을 위한 threestudio 라이브러리를 사용하였으며, MakeIt3D와 Syncedreamer는 정식 구현을 사용하였다.\n' +
      '***channel-concatenate reference image.** Zero123, Magic123 및 Syncedreamer는 channel dimension에서 noisy image와 reference image를 연결한다. MakeIt3D는 채널 연결 참조 이미지를 사용하지 않는다. 네트워크의 입력 차원이 변하지 않는 것을 보장하기 위해, 우리는 참조 이미지에 대응하는 위치를 동일한 차원의 제로-유사 매트릭스로 대체한다.\n' +
      '\n' +
      '도 8: 상이한 설정에서의 정성적 비교. _ CCR_는 **c**hannel-concatenate **r**eference image로 나타낸다. _ NOTHING_는 그것이 어떤 것도 생성하지 않는다는 것을 의미한다. 이 결과에 대한 비디오는 [https://isotropic3d.github.io/](https://isotropic3d.github.io/]에서 사용할 수 있다.\n' +
      '\n' +
      '***Removing \\(L_{2}\\) loss supervision.** Zero123, MakeIt3D, Magic123 및 Syncedramer는 L2 supervision을 위한 참조 영상을 사용한다. 우리는 RGB 손실, 깊이 손실 및 마스크 손실을 포함하여 참조 이미지와 관련된 모든 손실 가중치를 0으로 재설정한다.\n' +
      '***channel-concatenate reference image와 \\(L_{2}\\) loss supervision을 함께 제거한다.** channel-concatenate reference image와 \\(L_{2}\\) loss supervision을 함께 제거한다는 것은 하나의 CLIP embedding으로 **3D content를 생성한다는 것을 의미한다.** MakeIt3D는 channel-concatenate reference image를 사용하지 않고 \\(L_{2}\\) loss supervision만을 제거한다.\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 8에 도시된 바와 같이, 기존의 이미지-투-3D 방법들은 참조 이미지에 너무 많이 의존하여 완전한 3D 객체를 생성하는 것은 거의 불가능하다. 채널 연결 참조 영상을 제거하면 Zero123, Magic123, Syncedreamer에서 생성된 3차원 모델의 텍스쳐가 줄어들게 된다. MakeIt3D는 대부분의 경우 제대로 생성되지 않는다. L_{2}\\의 손실 감독을 제거한 후 MakeIt3D와 Syncedreamer는 전혀 생성할 수 없다. 채널 연결 참조 영상과 \\(L_{2}\\) 손실 감리를 함께 제거할 때, 3D 모델을 생성하기 위해 **a 단일 CLIP 임베딩**만을 사용하는 것을 의미한다. Zero123과 Magic123만이 규칙적인 기하학적 구조와 명확한 질감 없이 저화질 객체를 생성할 수 있다. MakeIt3D와 Syncedreamer는 테스트 케이스에서 완전히 어떤 것도 생성할 수 없습니다. 이에 비해 제안하는 방법은 영상 CLIP 임베딩만을 입력으로 하여 다시점 상호 일관된 영상과 고품질의 3D 모델을 생성할 수 있다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 이미지 CLIP 임베딩만으로 고품질의 기하학과 텍스쳐를 생성하기 위한 새로운 이미지-대-3D 파이프라인인 등방성 3D를 제안한다. 등방성 3D를 사용하면 SDS 손실에 대해서만 방위각을 등방성 wrt로 최적화할 수 있다. 이를 위해 다중 뷰 확산 모델을 두 단계로 미세 조정하는데, 이는 참조 이미지의 의미 정보를 활용하는 것을 목표로 하지만 참조 이미지와 완전히 일치할 필요는 없으므로 확산 모델이 참조 뷰를 손상시키는 것을 방지한다. 먼저, 텍스트 인코더를 이미지 인코더로 대체하여 텍스트-이미지 확산 모델을 이미지-이미지 모델로 미세 조정한다. 다음으로, 잡음이 많은 다시점 영상과 잡음이 없는 참조 영상을 명시적 조건으로 결합하는 명시적 다시점 주의 메커니즘(EMA)으로 모델을 미세 조정한다. CLIP 임베딩은 전체 프로세스에서 확산 모델로 전송되고 참조 이미지는 미세 조정 후 한 번 폐기된다. 광범위한 실험 결과는 단일 이미지 CLIP 임베딩으로 등방성 3D가 기존 이미지-대-3D 방법에 비해 더 잘 비례하는 기하학, 컬러 텍스처, 왜곡이 적은 다시점 상호 일치 이미지를 생성할 수 있으며, 참조 이미지와의 유사성을 최대한 보존할 수 있음을 보여준다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Anciukevicius, T., Xu, Z., Fisher, M., Henderson, P., Bilen, H., Mitra, N.J., Guerrero, P.: Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation. In: CVPR (2023)\n' +
      '* [2] Baillif, B., Cole, J., McCabe, P., Bender, A.: Deep generative models for 3d molecular structure. Current Opinion in Structural Biology (2023)\n' +
      '* [3] Chen, Z., Wang, F., Wang, Y., Liu, H.: Text-to-3d using gaussian splatting. In: CVPR (2024)\n' +
      '* [4] Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3d objects. In: CVPR (2023)\n' +
      '* [5] Deng, C., Jiang, C., Qi, C.R., Yan, X., Zhou, Y., Guibas, L., Anguelov, D., et al.: Nerdi: Single-view nerf synthesis with language-guided diffusion as general image priors. In: CVPR (2023)\n' +
      '* [6] Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K., McHugh, T.B., Vanhoucke, V.: Google scanned objects: A high-quality dataset of 3d scanned household items. In: ICRA (2022)\n' +
      '* [7] Dundar, A., Gao, J., Tao, A., Catanzaro, B.: Fine detailed texture learning for 3d meshes with generative models. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023)\n' +
      '* [8] Eswaran, M., Bahubalendruni, M.R.: Challenges and opportunities on ar/vr technologies for manufacturing systems in the context of industry 4.0: A state of the art review. Journal of Manufacturing Systems (2022)\n' +
      '* [9] Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Gojcic, Z., Fidler, S.: Get3d: A generative model of high quality 3d textured shapes learned from images. NeurIPS (2022)\n' +
      '* [10] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. NeurIPS (2014)\n' +
      '* [11] Gu, J., Trevithick, A., Lin, K.E., Susskind, J.M., Theobalt, C., Liu, L., Ramamoorthi, R.: Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion. In: ICML (2023)\n' +
      '* [12] Henderson, P., Ferrari, V.: Learning single-image 3d reconstruction by generative modelling of shape, pose and shading. International Journal of Computer Vision (2020)\n' +
      '* [13] Henderson, P., Tsiminaki, V., Lampert, C.H.: Leveraging 2d data to learn textured 3d mesh generation. In: CVPR (2020)\n' +
      '* [14] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS (2020)\n' +
      '* [15] Hsiang, E.L., Yang, Z., Yang, Q., Lai, P.C., Lin, C.L., Wu, S.T.: Ar/vr light engines: Perspectives and challenges. Advances in Optics and Photonics (2022)\n' +
      '* [16] Jain, A., Mildenhall, B., Barron, J.T., Abbeel, P., Poole, B.: Zero-shot text-guided object generation with dream fields. In: CVPR (2022)\n' +
      '* [17] Kim, G., Chun, S.Y.: Datid-3d: Diversity-preserved domain adaptation using text-to-image diffusion for 3d generative model. In: CVPR (2023)\n' +
      '* [18] Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013)\n' +
      '* [19] Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation. In: CVPR (2023)* [20] Lin, Y., Han, H., Gong, C., Xu, Z., Zhang, Y., Li, X.: Consistent123: One image to highly consistent 3d asset using case-aware diffusion priors. arXiv preprint arXiv:2309.17261 (2023)\n' +
      '* [21] Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. In: CVPR (2023)\n' +
      '* [22] Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: SyncDreamer: Generating multiview-consistent images from a single-view image. arXiv preprint arXiv:2309.03453 (2023)\n' +
      '* [23] Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H., Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008 (2023)\n' +
      '* [24] Luo, S., Hu, W.: Diffusion probabilistic models for 3d point cloud generation. In: CVPR (2021)\n' +
      '* [25] Melas-Kyriazi, L., Laina, I., Rupprecht, C., Vedaldi, A.: Realfusion: 360deg reconstruction of any object from a single image. In: CVPR (2023)\n' +
      '* [26] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM (2021)\n' +
      '* [27] Mittal, P., Cheng, Y.C., Singh, M., Tulsiani, S.: Autosdf: Shape priors for 3d completion, reconstruction and generation. In: CVPR (2022)\n' +
      '* [28] Muller, N., Siddiqui, Y., Porzi, L., Bulo, S.R., Kontschieder, P., Niessner, M.: Diffrf: Rendering-guided 3d radiance field diffusion. In: CVPR (2023)\n' +
      '* [29] Niemeyer, M., Geiger, A.: Giraffe: Representing scenes as compositional generative neural feature fields. In: CVPR (2021)\n' +
      '* [30] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022)\n' +
      '* [31] Qian, G., Mai, J., Hamdi, A., Ren, J., Siarohin, A., Li, B., Lee, H.Y., Skorokhodov, I., Wonka, P., Tulyakov, S., et al.: Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843 (2023)\n' +
      '* [32] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021)\n' +
      '* [33] Raj, A., Kaza, S., Poole, B., Niemeyer, M., Ruiz, N., Mildenhall, B., Zada, S., Aherman, K., Rubinstein, M., Barron, J., et al.: Dreambooth3d: Subject-driven text-to-3d generation. arXiv preprint arXiv:2303.13508 (2023)\n' +
      '* [34] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aherman, K.: Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In: CVPR (2023)\n' +
      '* [35] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS (2022)\n' +
      '* [36] Shen, Q., Yang, X., Wang, X.: Anything-3d: Towards single-view anything reconstruction in the wild. arXiv preprint arXiv:2304.10261 (2023)\n' +
      '* [37] Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.: Zero123++: a single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110 (2023)\n' +
      '* [38] Shi, Y., Wang, P., Ye, J., Long, M., Li, K., Yang, X.: Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512 (2023)\n' +
      '*[*[39] Tan, F., Fanello, S., Meka, A., Orts-Escolano, S., Tang, D., Pandey, R., Taylor, J., Tan, P., Zhang, Y.:Volux-gan: hdri relighting과 함께 3d 얼굴 합성을 위한 생성 모델. In: ACM SIGGRAPH(2022)\n' +
      '* [40] Tang, J., Wang, T., Zhang, B., Zhang, T., Yi, R., Ma, L., Chen, D.: Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior. arXiv preprint arXiv:2303.14184 (2023)\n' +
      '* [41] Tang, S., Zhang, F., Chen, J., Wang, P., Furukawa, Y.: MVdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion. arXiv preprint arXiv:2307.01097 (2023)\n' +
      '* [42] Tseng, H.Y., Li, Q., Kim, C., Alsisan, S., Huang, J.B., Kopf, J.: Consistent view synthesis with pose-guided diffusion models. In: CVPR (2023)\n' +
      '* [43] Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning. NeurIPS (2017)\n' +
      '* [44] Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689 (2021)\n' +
      '* [45] Wang, P., Shi, Y.: Imagedream: Image-prompt multi-view diffusion for 3d generation. arXiv preprint arXiv:2312.02201 (2023)\n' +
      '* [46] Wang, X., Wang, Y., Ye, J., Wang, Z., Sun, F., Liu, P., Wang, L., Sun, K., Wang, X., He, B.: Animatabledreamer: Text-guided non-rigid 3d model generation and reconstruction with canonical score distillation. arXiv preprint arXiv:2312.03795 (2023)\n' +
      '* [47] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In: NeurIPS (2023)\n' +
      '* [48] Wang, Z., Wu, S., Xie, W., Chen, M., Prisacariu, V.A.: Nerf-: Neural radiance fields without known camera parameters. arXiv preprint arXiv:2102.07064 (2021)\n' +
      '* [49] Weng, H., Yang, T., Wang, J., Li, Y., Zhang, T., Chen, C., Zhang, L.: Consistent123: Improve consistency for one image to 3d object synthesis. arXiv preprint arXiv:2310.08092 (2023)\n' +
      '* [50] Wu, J., Zhang, C., Xue, T., Freeman, B., Tenenbaum, J.: Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. NeurIPS (2016)\n' +
      '* [51] Xiang, J., Yang, J., Huang, B., Tong, X.: 3d-aware image generation using 2d diffusion models. arXiv preprint arXiv:2303.17905 (2023)\n' +
      '* [52] Xie, J., Ouyang, H., Piao, J., Lei, C., Chen, Q.: High-fidelity 3d gan inversion by pseudo-multi-view optimization. In: CVPR (2023)\n' +
      '* [53] Xu, D., Jiang, Y., Wang, P., Fan, Z., Wang, Y., Wang, Z.: Neurallift-360: Lifting an in-the-wild 2d photo to a 3d object with 360deg views. In: CVPR (2023)\n' +
      '* [54] Xu, Y., Chai, M., Shi, Z., Peng, S., Skorokhodov, I., Siarohin, A., Yang, C., Shen, Y., Lee, H.Y., Zhou, B., et al.: Discoscene: Spatially disentangled generative radiance fields for controllable 3d-aware scene synthesis. In: CVPR (2023)\n' +
      '* [55] Xue, Y., Li, Y., Singh, K.K., Lee, Y.J.: Giraffe hd: A high-resolution 3d-aware generative model. In: CVPR (2022)\n' +
      '* [56] Ye, J., Wang, P., Li, K., Shi, Y., Wang, H.: Consistent-1-to-3: Consistent image to 3d view synthesis via geometry-aware diffusion models. arXiv preprint arXiv:2310.03020 (2023)\n' +
      '* [57] Zhou, Z., Tulsiani, S.: Sparsefusion: Distilling view-conditioned diffusion for 3d reconstruction. In: CVPR (2023)\n' +
      '* [\n' +
      '\n' +
      '## 부록 0. 카메라 모델\n' +
      '\n' +
      '우리는 각각 고도각, 방위각 및 카메라 거리를 나타내기 위해 \\(\\theta\\), \\(\\varphi\\) 및 \\(d\\)을 정의한다. 우리는 1.5에서 \\(\\theta\\in[-10^{\\circ},40^{\\circ}]\\), \\(\\varphi\\in[0^{\\circ},360^{\\circ}]\\) 및 \\(d\\)의 범위 내에서 카메라 시점을 체계적으로 샘플링하고, 실제로 이러한 각도를 라디안으로 변환한다. 참조 시점과 대상 뷰에 대한 카메라의 위치는 각각 \\((\\theta_{r},\\varphi_{r},d_{r})\\)와 \\((\\theta_{n},\\varphi_{n},d_{n})\\)으로 표시되며, \\(d=d_{r}=d_{n}=1.5\\)와 \\(n\\in\\{1,2,...,N\\}\\)은 별개의 대상 뷰를 나타낸다. 카메라 위치 사이의 상대적인 변환은 식 \\((\\theta_{n}-\\theta_{r},\\varphi_{n}-\\varphi_{r},d))에 의해 캡처된다. 단일 뷰 확산 브랜치에 대해, 타겟 뷰는 임의의 뷰일 수 있다. 이와는 대조적으로, 다중 뷰 확산 과정 동안, 타겟 뷰들은 각각 \\(\\theta=30^{\\circ}\\) 및 \\(\\varphi\\in[0^{\\circ},360^{\\circ}]\\)으로 특징지어지는 4개의 별개의 직교 뷰들로 제한된다.\n' +
      '\n' +
      '## 부록 0.B 훈련 세부사항\n' +
      '\n' +
      '**View-Conditioned Multi-view Diffusion.** MVDream[38]을 미세조정의 1단계 기본 모델로 삼는다. 최적화 과정에서 AdamW 최적화기는 매개변수 \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.999\\) 및 \\(10^{-2}\\)에 의해 보완된 10,000 단계의 예열 단계와 일정한 학습률을 갖는 최적화기를 사용하였다. 4단계 이상의 그라디언트 누적(gradient accumulation)을 적용하여 총 배치 크기는 768이며, 이 구성을 통해 8개의 A800 GPU에서 50,000번의 반복에 걸쳐 UNet 모델 전체를 미세 조정할 수 있으며, 총 기간은 약 7일이다. 두 번째 단계에서는 AdamW 최적화기를 계속 사용한다. 그러나, 우리는 \\(5\\times 10^{-5}\\)에서 정점을 이루는 선형 학습률 스케줄로 전환한다. 그라디언트 축적이 사용되지 않습니다. 전체 UNet을 총 배치 크기가 128인 5k 단계로 미세 조정하며, 약 5시간이 소요됩니다. 다른 설정은 첫 번째 단계와 일치합니다.\n' +
      '\n' +
      '**등방성 3D.** 단일 A800 GPU에서 10,000단계로 당사의 3D 콘텐츠를 최적화하였습니다. 학습률 0.01에서 AdamW 최적화기를 채택하였으며 기하학을 정규화하기 위해 점 조명과 부드러운 음영을 선택하였다. 다시점 확산 훈련 시와 동일한 유도 척도를 10으로 경험적으로 설정하였다. SDS의 경우 8,000단계에서 최대 및 최소 시간 단계를 각각 0.98에서 0.5 및 0.02로 조정한다. SDS 손실의 계수는 \\(\\lambda_{e}=1\\)로 설정되고 배향 손실의 가중치는 다음과 같이 기입될 수 있다.\n' +
      '\n' +
      '\\[\\lambda_{o}=\\begin{cases}0.2x,&x\\leq 5,000,\\\\1000,&5000<x\\leq 10,000,\\end{cases}\\tag{10}\\]\n' +
      '\n' +
      '여기서 \\(x\\)는 글로벌 단계이다. 명시적으로 언급되지 않은 다른 설정들은 MVDream에 요약된 사양들을 준수한다.\n' +
      '\n' +
      '추론 중 참조 이미지 제거 논의\n' +
      '\n' +
      '트레이닝 동안, 잡음이 없는 기준 이미지와 잡음이 있는 타겟 이미지들을 연결하는 이면의 근거는 특징 레벨에서 잡음이 있는 타겟들과 기준 이미지 사이의 일관성을 보장하기 위한 것이다. 그러나 참조 영상이 다시점 영상의 생성에 영향을 미치지 않도록 하는 것을 목표로 한다. 따라서, 훈련 중에 잡음이 없는 참조 영상이 네트워크에 입력되지만, 지면 진리로 손실을 계산할 때 의도적으로 제외한다. 이는 참조 이미지가 모델의 최적화에 영향을 미치지 않으면서 자기 주목 계층 내의 다른 타겟 뷰들에 명시적인 특징-레벨 정보만을 제공한다는 것을 의미한다. 결과적으로, 추론 과정에서 단일 이미지 CLIP 임베딩을 활용하는 동안, 본 모델은 참조 이미지와 상당한 유사성을 유지하면서 상호 일관성이 있는 다시점 이미지를 생성하는 데 능숙하다.\n' +
      '\n' +
      '## 부록 0.D 제한 및 토론\n' +
      '\n' +
      '등방성 3D는 기존의 이미지-투-3D 모델보다 더 나은 기하학 및 텍스처를 가지며 등방성을 나타내지만, 렌더링된 3D 콘텐츠의 해상도는 높지 않다. 이는 학습 데이터가 256의 해상도만을 가지고 있기 때문인 것으로 분석되며, 광범위한 실험 결과 얼굴에서 성능이 좋지 않아 다운스트림 작업에서 더 많은 미세 조정이 필요할 수 있다.\n' +
      '\n' +
      '## 부록 0.E 추가 결과\n' +
      '\n' +
      '**정성적 비교를 위한 추가 결과** 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 9에서, 우리는 등방성 3D와 다른 기준선 사이의 더 많은 추가 비교 결과를 보여준다. 프로젝트 웹사이트의 영상은 보다 직관적인 효과를 보여줍니다.\n' +
      '\n' +
      '** 더 많은 결과.** 3D 세대의 안정성을 검증하기 위해, 도 10은 두 그룹의 결과가 서로 다른 종자를 가진 두 번의 실행에서 나온 것임을 보여준다. 보다 상세하고 생생한 결과를 위해 그림 1에서 등방성 3D로 생성된 3D 콘텐츠의 더 많은 예를 보여준다. 도 11 내지 도 11을 참조하면 다음과 같다. 17. [https://isotropic3d.github.io/](https://isotropic3d.github.io/]에서 저희 웹사이트를 방문해 주세요.\n' +
      '\n' +
      '도 10: **Isotrpic3D**에 의해 생성된 추가 결과. 두 개의 결과 그룹이 두 번의 실행에서 가져온 것입니다.\n' +
      '\n' +
      '그림 9: 기준 모델과 등방성 3D의 정성적 비교에 대한 추가 결과. 이 결과에 대한 비디오는 프로젝트 웹사이트에서 볼 수 있습니다.\n' +
      '\n' +
      '도 11: **Isotrpic3D**에 의해 생성된 실시예 3D 컨텐츠.\n' +
      '\n' +
      '도 12: **Isotrpic3D**에 의해 생성된 실시예 3D 컨텐츠.\n' +
      '\n' +
      '도 14: **Isotrpic3D**에 의해 생성된 실시예 3D 컨텐츠.\n' +
      '\n' +
      '도 13: **Isotrpic3D**에 의해 생성된 실시예 3D 컨텐츠.\n' +
      '\n' +
      '도 16: **Isotrpic3D**에 의해 생성된 실시예 3D 컨텐츠.\n' +
      '\n' +
      '도 15: **Isotrpic3D**에 의해 생성된 실시예 3D 컨텐츠.\n' +
      '\n' +
      '도 17: **Isotrpic3D**에 의해 생성된 실시예 3D 컨텐츠.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>