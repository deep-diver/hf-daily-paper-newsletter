<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Infinite-ID: Identity-preserved Personalization via ID-semantics Decoupling Paradigm\n' +
      '\n' +
      'Yi Wu\n' +
      '\n' +
      'First two authors contributed equally to this work.1University of Science and Technology of China, China1\n' +
      '\n' +
      'Ziqiang Li\n' +
      '\n' +
      'Corresponding Author.1University of Science and Technology of China, China1\n' +
      '\n' +
      'Heliang Zheng\n' +
      '\n' +
      '1University of Science and Technology of China, China1\n' +
      '\n' +
      'Chaoyue Wang\n' +
      '\n' +
      '2The University of Sydney, Australia2\n' +
      '\n' +
      'Bin Li\n' +
      '\n' +
      'Corresponding Author.1University of Science and Technology of China, China1\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Drawing on recent advancements in diffusion models for text-to-image generation, identity-preserved personalization has made significant progress in accurately capturing specific identities with just a single reference image. However, existing methods primarily integrate reference images within the text embedding space, leading to a complex entanglement of image and text information, which poses challenges for preserving both identity fidelity and semantic consistency. To tackle this challenge, we propose **Infinite-ID**, an ID-semantics decoupling paradigm for identity-preserved personalization. Specifically, we introduce identity-enhanced training, incorporating an additional image cross-attention module to capture sufficient ID information while _deactivating the original text cross-attention module_ of the diffusion model. This ensures that the image stream faithfully represents the identity provided by the reference image while mitigating interference from textual input. Additionally, we introduce a feature interaction mechanism that combines a mixed attention module with an AdaIN-mean operation to seamlessly merge the two streams. This mechanism not only enhances the fidelity of identity and semantic consistency but also enables convenient control over the styles of the generated images. Extensive experimental results on both raw photo generation and style image generation demonstrate the superior performance of our proposed method.\n' +
      '\n' +
      'Keywords:Personalized Text-to-image Generation, Stable Diffusion, Identity-preserved Personalization\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Human photo synthesis [17, 34] has experienced notable advancements, particularly with the introduction of large text-to-image diffusion models such as Stable Diffusion (SD) [24], Imagen [27], and DALL-E 3 [3]. Benefit from personalized text-to-image generation, recent researches focus on _Identity-preserved personalization_. This specialized area aims to produce highly customized photos thatfaithfully reflect a specific identity in novel scenes, actions, and styles, drawing inspiration from one or more reference images. This task has garnered considerable attention, leading to the development of numerous applications, including personalized AI portraits and virtual try-on scenarios. In the context of Identity-preserved personalization, the emphasis is placed on maintaining the invariance of human facial identity (ID), requiring a heightened level of detail and fidelity compared to more general styles or objects.\n' +
      '\n' +
      'Recent tuning-free methods exhibit promise for large-scale deployment, yet they face a notable challenge in balancing the trade-off between the fidelity of identity representation (ID fidelity) and the consistency of semantic understanding conveyed by the text prompt. This challenge arises due to the inherent entanglement of image and text information. Typically, tuning-free methods extract ID information from reference images and integrate it into the semantic information in two distinct ways. The first type, exemplified by PhotoMaker [16], incorporates text information with ID details in the text embedding space of the text encoder. While this merging approach aids in achieving semantic consistency, it compresses image features into the text embedding space, thereby weakening the ID information of the image and compromising identity fidelity. The second type, demonstrated by IP-Adapter [36], directly injects ID information into the U-Net of the diffusion model through an additional trainable cross-attention module. Although this approach aims to enhance the strength of ID information for improved fidelity, it tends to favor the image branch during training, consequently weakening the text branch and compromising semantic consistency. In summary, existing methods entangle image and text information, resulting in a significant trade-off between ID fidelity and semantic consistency (as illustrated in Fig. 6).\n' +
      '\n' +
      'To address the entanglement between image and text information, we propose **Infinite-ID**, an innovative approach to personalized text-to-image gen\n' +
      '\n' +
      'Figure 1: With just a single reference image, our Infinite-ID framework excels in synthesizing high-quality images while maintaining superior identity fidelity and text semantic consistency in various styles.\n' +
      '\n' +
      ' eration. Our method tackles the trade-off between maintaining high fidelity of identity and ensuring semantic consistency of the text prompt by implementing the ID-semantics decoupling paradigm. Specifically, we adopt identity-enhanced training that introduces an additional image cross-attention module to capture sufficient ID information and deactivate the original text cross-attention module to avoid text interference during training stage. Accordingly, our method can faithfully capture identity information from reference image, significantly improving the ID fidelity. Additionally, we employ a novel feature interaction mechanism that leverages a mixed attention module and an AdaIN-mean operation to effectively merge text information and identity information. Notably, our feature interaction mechanism not only preserves both identity and semantic details effectively but also enables convenient control over the styles of the generated images (as depicted in Fig. 1). Our contributions are summarized as:\n' +
      '\n' +
      '* We propose a novel ID-semantics decoupling paradigm to resolve the entanglement between image and text information, acquiring a remarkable balance between ID fidelity and semantic consistency in _Identity-preserved personalization_.\n' +
      '* We propose a novel feature interaction mechanism incorporating a mixed attention module and an AdaIN-mean operation to effectively merge ID information and text information and also conveniently control the styles of the generated image in diffusion models.\n' +
      '* Experimental results demonstrate the excellent performance of our proposed method as compared with current state-of-the-art methods on both raw photo generation and style image generation.\n' +
      '\n' +
      '## 2 Related Works\n' +
      '\n' +
      '### Text-to-image Diffusion Models\n' +
      '\n' +
      'Text-to-image diffusion models, such as those explored in [38, 38, 24, 27, 12, 40], have garnered significant attention due to their impressive image generation capabilities. Current research endeavors aim to further enhance these models along multiple fronts, including the utilization of high-quality and large-scale datasets [29, 30], refinements to foundational architectures [3, 24, 27], and advancements in controllability [11, 40, 26]. Present iterations of text-to-image diffusion models typically follow a two-step process: first, encoding the text prompt using pre-trained text encoders such as CLIP [21] or T5 [23], and then utilizing the resulting text embedding as a condition for generating corresponding images through the diffusion process. Notably, the widely adopted Stable Diffusion model [24] distinguishes itself by executing the diffusion process in latent space instead of the original pixel space, leading to significant reductions in computation and time costs. An important extension to this framework is the Stable Diffusion XL (SDXL) [20], which enhances performance by scaling up the U-Net architecture and introducing an additional text encoder. Thus, our proposed method builds upon the SDXL. However, our method can also be extended to other text-to-image diffusion models.\n' +
      '\n' +
      '### Identity-preserved personalization\n' +
      '\n' +
      'Identity-preserved personalization aims to generate highly customized photos that accurately reflect a specific identity across various scenes, actions, and styles, drawing inspiration from one or more reference images. Initially, _tuning-based methods_, exemplified by DreamBooth [26] and Textual Inversion [7], employ images of the same identity (ID) to fine-tune the model. While these methods yield results with high fidelity in preserving facial identity (ID), a significant drawback emerges: the customization of each ID necessitates a time investment of 10-30 minutes [14], consuming substantial computing resources and time. This limitation poses a significant obstacle to large-scale deployment in commercial applications. Consequently, recent advancements in _tuning-free methods_[5, 6, 8, 15, 16, 18, 32, 35, 36] have been introduced to streamline the generation process. These methods specifically leverage the construction of a vast amount of domain-specific data and the training of an encoder or hyper-network to represent input ID images as embeddings or LoRA weights within the model. Post-training, users need only input an image of the ID for customization, enabling personalized generation within seconds during the inference phase. These tuning-free methods typically contain two distinct manners.\n' +
      '\n' +
      'On one hand, methods [1, 16, 19, 35] incorporate text information alongside identity details within the text embedding space of the text encoder. For example, PhotoMaker [16] extracts identity embeddings from single or multiple reference images and merges them with corresponding class embeddings (e.g., "man" and "woman") in the text embedding space of the text encoder. While this stacking operation aids in achieving semantic consistency, it compresses image features into the text embedding space, leading to compromised identity fidelity. On the other hand, some studies [5, 32, 36] directly integrate identity information into the U-Net of the diffusion model. IP-Adapter [36] distinguishes itself by incorporating a additional cross-attention layer for each existing cross-attention layer within the original UNet model. This approach merges identity information with semantic details directly within the U-net but leads to distortion of the semantic space of the U-Net model. Consequently, this compromises the semantic consistency of the text prompt.\n' +
      '\n' +
      'In summary, existing methods entangle the identity and text information, leading to a significant trade-off between ID fidelity and semantic consistency. To mitigate these limitations, we propose an identity-enhanced training to capture ID and text information separately. Moreover, we design an effective feature interaction mechanism leveraging a mixed attention module and an AdaIN-mean operation to preserve both identity and semantic details while also enabling convenient control over the styles of generated images.\n' +
      '\n' +
      '### Attention Control in Diffusion model\n' +
      '\n' +
      'Previous studies have investigated various attention control techniques within diffusion models. Hertz _et al._[9] employed a shared attention mechanism, concatenating and applying an AdaIN module on the key and value between reference and synthesis images within the self-attention layer to ensure style-consistentimage generation using a reference style. Cao _et al._[4] utilized a mutual self-attention approach to achieve consistent image generation and non-rigid image editing, wherein the key and value of the synthesis image were replaced with those of the reference image within the self-attention layers of the diffusion model. Similarly, Shi _et al._[31] proposed a method termed reference attention, enabling consistent multi-view generation of target objects by concatenating the key and value features between the condition signal and the synthesis image in the self-attention layers. Wang _et al._[35] and Avrahami _et al._[2] exploited attention maps within the cross-attention layers to guide the optimization process towards disentangling learned concepts in personalized generation tasks.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '**Stable Diffusion XL.** Our method builds upon Stable Diffusion XL [20], comprising three core components: a Variational AutoEncoder (VAE) denoted as \\(\\xi(\\cdot)\\), a conditional U-Net [25] represented by \\(\\epsilon_{\\theta}(\\cdot)\\), and two pre-trained text encoders [22] denoted as \\(\\Theta_{1}(\\cdot)\\) and \\(\\Theta_{2}(\\cdot)\\). Specifically, given a training image \\(x_{0}\\) and its corresponding text prompt \\(T\\), the VAE encoder \\(\\xi\\) transforms \\(x_{0}\\) from its original space \\(R^{H\\times W\\times 3}\\) to a compressed latent representation \\(z_{0}=\\xi(x_{0})\\), where\n' +
      '\n' +
      'Figure 2: **Framework of ID-semantics Decoupling Paradigm**. In the training phase, we adopt Face embeddings extractor to extract rich identity information and identity-enhanced training for faithfully representing the identity provided by the reference image while mitigating interference from textual input. In the inference stage, a mixed attention module is introduced to replace the original self-attention mechanism within the denoising U-Net model, facilitating the fusion of both identity and text information.\n' +
      '\n' +
      '\\(z_{0}\\in R^{h\\times w\\times c}\\) and \\(c\\) denotes the latent dimension. Subsequently, the diffusion process operates within this compressed latent space to conserve computational resources and memory. Once the two text encoders process the text prompt \\(T\\) into a text embedding \\(c=\\text{Concat}(\\Theta_{1}(T),\\Theta_{2}(T))\\), the conditional U-Net \\(\\epsilon_{\\theta}\\) predicts the noise \\(\\epsilon\\) based on the current timestep \\(t\\), the \\(t-\\)th latent representation \\(z_{t}\\), and the text embedding \\(c\\). The training objective is formulated as follows:\n' +
      '\n' +
      '\\[L_{\\text{diffusion}}=E_{z_{t},t,c,\\epsilon\\in N(0,I)}[||\\epsilon-\\epsilon_{ \\theta}(z_{t},t,c)||_{2}^{2}]. \\tag{1}\\]\n' +
      '\n' +
      'Attention mechanism in diffusion models.The fundamental unit of the stable diffusion model comprises a resblock, a self-attention layer, and a cross-attention layer. The attention mechanism is represented as follows:\n' +
      '\n' +
      '\\[\\text{Attn}(Q,K,V)=\\text{Softmax}(\\frac{QK^{T}}{\\sqrt{d}})V, \\tag{2}\\]\n' +
      '\n' +
      'where \\(Q\\) denotes the query feature projected from the spatial features generated by the preceding resblock, \\(K\\) and \\(V\\) represent the key and value features projected from the same spatial features as the query feature (in self-attention) or the text embedding extracted from the text prompt (in cross-attention).\n' +
      '\n' +
      '### Methodology\n' +
      '\n' +
      'Overview.In this section, we introduce our ID-semantics decoupling paradigm, as illustrated in Fig. 2, which effectively addresses the severe trade-off between high-fidelity identity and semantic consistency within identity-preserved personalization. Subsequently, we present our mixed attention mechanism, depicted in Fig. 3, designed to seamlessly integrate ID information and semantic information within the diffusion model during the inference stage. Additionally, we utilize an adaptive mean normalization (AdaIN-mean) operation to precisely align the style of the synthesized image with the desired style prompts.\n' +
      '\n' +
      'ID-semantics Decoupling Paradigm.To faithfully capture high-fidelity identity, we implement a novel identity-enhanced strategy during the training stage, as depicted in Fig. 2. Diverging from conventional methods [5, 16, 36] that utilize text-image pairs for training, we opt to exclude the text prompt input and deactivate cross-attention modules for text embeddings within the U-Net model. Instead, we establish a training pair consisting of an ID image, where the face is aligned to extract identity information, and denoising image (\\(x_{t}\\) in Fig. 2). Both the denoising image used for training and the ID image belong to the same individual, but they vary in factors such as viewpoints and facial expressions. This approach fosters a more comprehensive learning process [16]. We adopt Face embeddings extractor to accurately capture and leverage identity information from the input ID image. Additionally, to seamlessly integrate identity information into the denoising U-Net model, we introduce an extra trainable cross-attention mechanism (\\(K^{\\prime}_{id}\\) and \\(V^{\\prime}_{id}\\) in Fig. 2) for image embeddings.\n' +
      '\n' +
      'Throughout the training phase, we exclusively optimize the parameters associated with the face mapper, CLIP mapper, and the image cross-attentionmodule, while keeping the parameters of the pre-trained diffusion model fixed. The optimization loss closely resembles the original diffusion loss formulation (as delineated in Eq. 1), with the sole distinction being the shift from a text condition to an identity condition as the conditional input.\n' +
      '\n' +
      '\\[L_{\\text{diffusion}}=E_{z_{t},t,c_{id},e\\in N(0,I)}[||\\epsilon-\\epsilon_{\\theta }(z_{t},t,c_{id})||_{2}^{2}], \\tag{3}\\]\n' +
      '\n' +
      'where the \\(c_{id}\\) is the identity embeddings of the input ID image.\n' +
      '\n' +
      'During the inference phase, we align the desired face within the ID image to afford identity information. Following the approach adopted during the training phase, we utilize a face recognition backbone and a CLIP image encoder to extract identity features from the aligned face image. Leveraging the trained face mapper, clip mapper, and image cross-attention mechanisms, these identity features are seamlessly integrated into the denoising U-Net model. Subsequently, we compute the key and value features for self-attention in the original stable diffusion model, considering only the text prompt input. These text key and value features are instrumental in the mixed attention process (illustrated in Fig. 3), facilitating the fusion of text and identity information. Moreover, to further augment the text information during the denoising process, we incorporate the original text cross-attention mechanism, integrating the resulting text hidden states with the output image hidden states obtained from the image cross-attention module.\n' +
      '\n' +
      '**Face Embeddings Extractor.** We adopt a multifaceted approach by incorporating pre-trained models to extract facial features. Firstly, following the method\n' +
      '\n' +
      'Figure 3: **Mixed attention mechanism**. On the left side, we employ mixed attention to fuse identity and text information. This involves concatenating their respective key and value features and subsequently applying mixed attention, where identity features are updated based on the concatenated key and value features. On the right side, for style merging, we introduce an additional AdaIN-mean operation (as depicted in Eq. 8) to the concatenated key and value features.\n' +
      '\n' +
      'ologies of prior research, we utilize a pre-trained CLIP image encoder as one of our facial feature extractors. Specifically, we leverage the local embeddings, comprising the last hidden states obtained from the CLIP image encoder, forming a sequence of embeddings with a length of N (N=257 in our implementation). Subsequently, we employ a CLIP mapper to project these image embeddings (with a dimensionality of 1664) to the same dimension as the text features in the pre-trained diffusion model. As elucidated in [36], the features extracted by the CLIP image encoder are instrumental in capturing the structural information pertinent to the identity face within identity-preserved personalization tasks. Additionally, we leverage the backbone of a face recognition model as another facial feature extractor. As highlighted in [36], features extracted by the face recognition backbone are adept at capturing the characteristics associated with human facial features within identity-preserved personalization tasks. More specifically, we utilize the global image embedding derived from the extracted features and subsequently employ a face mapper to align the dimensionality (512 dimensions) of the extracted global image embedding with the dimensionality of the text features in the pre-trained diffusion model.\n' +
      '\n' +
      'In summary, the identity embeddings \\(c_{id}\\) corresponding to the ID image \\(x\\) can be expressed as:\n' +
      '\n' +
      '\\[c_{id}=\\text{Concat}\\bigg{(}\\text{M}_{\\text{clip}}\\big{(}\\text{E}_{\\text{clip} }(\\text{FA}(x))\\big{)},\\text{M}_{\\text{face}}\\big{(}\\text{E}_{\\text{face}}( \\text{FA}(x))\\big{)}\\bigg{)}, \\tag{4}\\]\n' +
      '\n' +
      'where \\(\\text{Concat}(\\cdot,\\cdot)\\), \\(\\text{M}_{\\text{clip}}(\\cdot)\\), \\(\\text{E}_{\\text{clip}}(\\cdot)\\), \\(\\text{M}_{\\text{face}}(\\cdot)\\), \\(\\text{E}_{\\text{face}}(\\cdot)\\), and \\(\\text{FA}(\\cdot)\\) denote the concatenation function, CLIP mapper, CLIP image encoder, face mapper, face recognition backbone, and face alignment module [39], respectively.\n' +
      '\n' +
      '**Mixed Attention Mechanism.** As explored in previous studies [4, 13, 33], the features in the self attention layers play a crucial role in consistency image generation (across-frame in text-to-video works), which indicates these features provide a refined and detailed semantics information. In our study, we extract features from the self-attention layers of the original text-to-image diffusion model to capture rich semantic information, represented as \\(K_{t}\\) and \\(V_{t}\\). We enhance the self-attention mechanism by incorporating it into a mixed attention framework, depicted in Fig. 3 (a). This fusion enables the integration of semantic features (\\(K_{t}\\) and \\(V_{t}\\)) with the identity-based features (\\(K_{id}\\) and \\(V_{id}\\)), thereby encapsulating identity information. Through this integration, the mixed attention mechanism seamlessly merges semantic details into the generated features across different resolutions. The formulation of the mixed attention mechanism is as follows:\n' +
      '\n' +
      '\\[\\begin{split}&\\text{Attn}_{\\text{mix}}(Q,K,V)\\triangleq\\text{ Attn}(Q,\\hat{K},\\hat{V})\\\\ \\text{w.r.t}&\\hat{K}=\\text{Concat}(K_{id},K_{t}), \\quad\\hat{V}=\\text{Concat}(V_{id},V_{t}),\\\\ & K_{id}=W_{k}^{id}Z_{id},\\quad K_{t}=W_{k}^{t}Z_{t},\\\\ & V_{id}=W_{v}^{t}Z_{id},\\quad V_{t}=W_{v}^{t}Z_{t},\\end{split} \\tag{5}\\]where \\(Z_{id}\\) and \\(Z_{t}\\) are the corresponding spatial features of generated features and semantic features, respectively. The parameters \\(W_{k}^{id}\\), \\(W_{k}^{t}\\), \\(W_{v}^{id}\\), and \\(W_{v}^{t}\\) correspond to the weights of the corresponding fully connected layers.\n' +
      '\n' +
      '**Cross-attention Merging.** To further refine semantic control, we incorporate text features into the identity feature within the cross-attention layers using the following formulation:\n' +
      '\n' +
      '\\[\\text{Attn}_{\\text{cross}}(Q,K,V)\\triangleq\\text{Attn}(Q,K_{id}^{\\prime},V_{ id}^{\\prime})+\\text{Attn}(Q,K_{t}^{\\prime},V_{t}^{\\prime}), \\tag{6}\\]\n' +
      '\n' +
      'where \\(K_{id}^{\\prime}=\\hat{W}_{k}^{id}c_{id}\\), \\(V_{id}^{\\prime}=\\hat{W}_{v}^{id}c_{id}\\), \\(K_{t}^{\\prime}=\\hat{W}_{k}^{t}c_{t}\\), and \\(V_{t}^{\\prime}=\\hat{W}_{v}^{t}c_{t}\\). \\(c_{id}\\) and \\(c_{t}\\) are the identity embedding and text embedding, respectively. \\(\\hat{W}_{k}^{id}\\), \\(\\hat{W}_{v}^{id}\\), \\(\\hat{W}_{k}^{t}\\), and \\(\\hat{W}_{v}^{t}\\) correspond to the weights of the trainable fully connected layers within cross-attention module.\n' +
      '\n' +
      '**Style Information Merging.** Inspired by [9], we propose an adaptive mean normalization (AdaIN-mean) operation to further align the style of the synthesis image with the style prompts. Concretely, we align the key and value features projected from identity features in both mixed attention and cross-attention with the key and value features projected from text features, formulated as follows:\n' +
      '\n' +
      '\\[\\begin{split} K_{id}&=\\text{AdaIN-m}(K_{id},K_{t}), \\quad V_{id}=\\text{AdaIN-m}(V_{id},V_{t}),\\quad\\text{For Mixed Attention}\\\\ K_{id}^{\\prime}&=\\text{AdaIN-m}(K_{id}^{\\prime},K_{t} ^{\\prime}),\\quad V_{id}^{\\prime}=\\text{AdaIN-m}(V_{id}^{\\prime},V_{t}^{\\prime}),\\quad\\text{For Cross Attention}\\end{split} \\tag{7}\\]\n' +
      '\n' +
      'where the AdaIN-mean operation (AdaIN-m(\\(\\cdot\\))) is defined as:\n' +
      '\n' +
      '\\[\\text{AdaIN-m}(x,y)=x-\\mu(x)+\\mu(y), \\tag{8}\\]\n' +
      '\n' +
      'where \\(\\mu(x)\\in R^{d_{k}}\\) is the mean of key and value features across different pixels. The mixed attention with AdaIN-mean has been illustrated in Fig. 3 (b).\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'After outlining the experimental setup in Sec. 4.1, we conduct a comparative analysis of raw photo generation and style image generation in Sec. 4.2. Ablation studies, highlighting the significance of various components, are presented in Sec. 4.3. Additionally, experiments involving multiple input ID images are detailed in A.3 of the supplementary materials. For further insights, Sec. A.4 and Sec. A.5 of the supplementary materials provide additional qualitative results for raw photo generation and style photo generation, respectively.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '**Implementation Details.** Our experiments leverage a pre-trained Stable Diffusion XL (SDXL) model [20]. For image encoding, we utilize the OpenCLIP ViT-H/14 [37] and the backbone of ArcFace [10]. The SDXL model consists of 70 cross-attention layers, to each of which we append an additional image cross-attention module. Training is conducted on 16 A100 GPUs for 1 million steps,with a batch size of 4 per GPU. We employ the AdamW optimizer with a fixed learning rate of 1e-4 and weight decay set to 0.01. During inference, we employ the DDIM Sampler with 30 steps and guidance scale is set to 5.0. Training data are sourced from multiple datasets, including the LAION-2B dataset [30], the LAION-Face dataset [41], and images collected from the internet. We curate a dataset where each individual is represented by multiple photographs.\n' +
      '\n' +
      '**Evaluation.** We assess the efficacy of our approach in preserving both identity fidelity and semantic consistency. Specifically, we measure identity fidelity, utilizing metrics such as \\(M_{\\text{FaceNet}}\\) (measured by FaceNet [28]) and CLIP-I [7]. Identity fidelity is evaluated based on the similarity of detected faces between the reference image and generated images. Semantic consistency is quantified using CLIP text-image consistency (CLIP-T [21]), which compares the text prompt with the corresponding generated images. More definition of metrics are detailed in Sec. A.1 of the supplementary materials.\n' +
      '\n' +
      '### Comparison to Previous Methods\n' +
      '\n' +
      '**Raw Photo Generation.** We benchmark our Infinite-ID against established identity-preserving personalization approaches. The qualitative outcomes are il\n' +
      '\n' +
      'Figure 4: **Qualitative comparison on raw photo generation. The results demonstrate that our Infinite-ID consistently maintains identity fidelity and achieves high-quality semantic consistency with just a single input image.**\n' +
      '\n' +
      'lustrated in Fig. 4, while quantitative results are provided in Table 1 and visually represented in Fig. 6. Notably, all methods are tuning-free, necessitating no test-time adjustments. FastComposer [35] exhibits challenges in maintaining identity fidelity, often presenting undesired artifacts in synthesized images. While IP-Adapter [36] and IP-Adapter-Face [36] demonstrates relatively fewer artifacts, its semantic consistency fall short. This phenomenon arises from the direct fusion of identity information with semantic details within the U-net model, leading to a compromise in semantic consistency. In contrast, PhotoMaker [16] exhibits commendable semantic consistency but falls short in preserving identity fidelity. Leveraging our ID-semantics decoupling paradigm, our method excels in preserving identity fidelity. Furthermore, our mixed attention mechanism effectively integrate the semantic information into the denoising process, positioning our method favorably against existing techniques.\n' +
      '\n' +
      '**Style Image Generation.** We demonstrate the results of the stylization results in Fig. 5, which compare our method with state-of-the-art tuning-free identity-preserving personalization methods. The style of the synthesis images include anime style, comic book style, and line art style. According to the stylization results, the IP-Adapter [36] and IP-Adapter-Face fails to depict the desired style in the prompt and the style of generation results always obey the tone in the\n' +
      '\n' +
      'Figure 5: **Qualitative comparison on style image generation.** The results demonstrate that our method maintains strong identity fidelity, high-quality semantic consistency, and precise stylization using only a single reference image.\n' +
      '\n' +
      'reference image. The training pipeline of the IP-Adapter and IP-Adapter-Face entangles the text embedding and image embedding which leads to the distortion of the text embeddings space. FastComposer [35] also fails in stylization generation and shows undesired artifacts. PhotoMaker [16] achieves a better semantic consistency and stylization, but the identity fidelity is still unsatisfactory. In contrast, our method achieves high identity fidelity, appealing semantic consistency, and precise stylization meantime.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'In this section, we begin by conducting ablation studies to assess the influence of our identity-enhanced training and mixed attention mechanism. Furthermore, we conduct style image generation to evaluate the effectiveness of our AdaIN-mean operation in regulating the style of the generated images. More ablation studies are demonstrated in Sec. A.2 of the supplementary materials.\n' +
      '\n' +
      '**Ablation of identity-enhanced training.** In Fig. 7 and Table 1, we compare our method with "Ours (w/o identity-enhanced training)", which is implemented using an identity-semantics entangled training strategy. This strategy utilizes text-image pairs during training and activates cross-attention modules for text embeddings within the original U-Net model. It is noteworthy that both methods\n' +
      '\n' +
      'Figure 6: **Visualization of the quantitative comparison.** Identity fidelity, represented by the average of CLIP-I and \\(M_{\\text{FaceNet}}\\) scores, both normalized through the z-score algorithm, indicates how accurately the generated image preserves the identity. Meanwhile, semantic consistency, measured by the CLIP-T score, assesses the coherence between the generated image and the provided text prompt. Higher scores indicate better identity fidelity and semantic consistency. The compared methods including IP-Adapter, IP-Adapter-Face [36], FastComposer [35], PhotoMaker [16], and ablation versions of our method including w/o identity-enhanced training (Ours-1), w/o mixed attention (Ours-2) and mixed attention \\(\\Rightarrow\\) mutual attention (Ours-3).\n' +
      '\n' +
      'share the same inference processing. The qualitative comparison demonstrates that our identity-enhanced training notably enhances identity fidelity.\n' +
      '\n' +
      '**Ablation of mixed attention mechanism.** To assess the effectiveness of our proposed mixed attention (M-A) mechanism, we compare our method with "Ours (w/o M-A)" and "Ours (M-A \\(\\Rightarrow\\) MU-A)" in Fig. 7 and Table 1. Mutual self-attention (MU-A) [4] converts the existing self-attention into \'cross-attention\' for consistent image editing, where the crossing operation happens in\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline  & CLIP-T \\(\\uparrow\\) & CLIP-I \\(\\uparrow\\) & \\(M_{\\text{FaceNet}}\\uparrow\\) \\\\ \\hline FastComposer [35] & 0.292 & 0.887 & 0.556 \\\\ \\hline IP-Adapter [36] & 0.274 & 0.905 & 0.474 \\\\ \\hline IP-Adapter-Face [36] & 0.313 & **0.919** & 0.513 \\\\ \\hline PhotoMaker [16] & **0.343** & 0.814 & 0.502 \\\\ \\hline Ours & 0.340 & 0.913 & **0.689** \\\\ Ours (w/o identity-enhanced training) & 0.329 & 0.891 & 0.593 \\\\ \\hline Ours (w/o Mixed Attention) & 0.331 & 0.905 & 0.700 \\\\ \\hline Ours (Mixed Attention \\(\\Rightarrow\\) Mutual Attention) & 0.316 & 0.808 & 0.398 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Quantitative comparison.** The evaluation metrics encompass CLIP-T, CLIP-I, and \\(M_{\\text{FaceNet}}\\). Our approach outperforms other methods in terms of identity fidelity while simultaneously achieving satisfactory semantic consistency. The best result is shown in **bold**, and the second best is underlined. Additionally, the quantitative comparison of ablation studies have been shown in the gray part.\n' +
      '\n' +
      'Figure 7: **Ablation study of our identity-enhanced training and mixed attention (M-A).** It is evident that identity-enhanced training significantly improves the identity fidelity, and mixed-attention mechanism enhances semantic consistency compared to mutual attention (MU-A) approach [4].\n' +
      '\n' +
      'the self-attentions of two related diffusion processes. The results show that our mixed attention mechanism demonstrates superior ability to improve semantic consistency while maintaining identity fidelity.\n' +
      '\n' +
      '**Ablation of AdaIN-mean operation.** To assess the effectiveness of our proposed AdaIN-mean operation, we compare our Infinite-ID model with variations, namely Ours (w/o AdaIN-mean) and Ours (AdaIN-mean \\(\\Rightarrow\\) AdaIN), as depicted in Fig. 8. The results reveal that: i) Ours (w/o AdaIN-mean) exhibits superior ID fidelity compared to Infinite-ID but fails to achieve style consistency with the text prompt; ii) Both our AdaIN-mean and AdaIN modules successfully achieve style consistency with the text prompt, yet AdaIN-mean maintains better ID fidelity than AdaIN. In conclusion, our proposed AdaIN-mean operation facilitates precise stylization while concurrently preserving ID fidelity.\n' +
      '\n' +
      '## 5 Conclusion and limitations.\n' +
      '\n' +
      'In this paper, we introduce Infinite-ID, an innovative identity-preserved personalization method designed to meet the requirement of identity (ID) fidelity and semantic consistency of the text prompt, all achievable with just one reference image and completed within seconds. Infinite-ID comprises three key components: the identity-enhanced training, mixed attention mechanism, and adaptive mean normalization (AdaIN-mean). Through extensive experimentation, our results illustrate that Infinite-ID outperforms baseline methods, delivering strong ID fidelity, superior generation quality, and precise semantic consistency in both raw photo generation and style image generation tasks. However, it\'s important to note that our method lacks multi-object personalization capability. Moreover, artifacts may occur when the human face occupies only a small portion of the entire image, attributed to limitations inherent in the original diffusion model.\n' +
      '\n' +
      'Figure 8: **Ablation study of our AdaIN-mean operation.** The results show that AdaIN-mean play a crucial role in style image generation. Compare to AdaIN [9] module, our AdaIN-mean helps to achieve a higher identity fidelity.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Achlioptas, P., Benetatos, A., Fostiropoulos, I., Skourtis, D.: Stellar: Systematic evaluation of human-centric personalized text-to-image methods. arXiv preprint arXiv:2312.06116 (2023)\n' +
      '* [2] Avrahami, O., Aherman, K., Fried, O., Cohen-Or, D., Lischinski, D.: Break-a-scene: Extracting multiple concepts from a single image. arXiv preprint arXiv:2305.16311 (2023)\n' +
      '* [3] Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al.: Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf **2**, 3 (2023)\n' +
      '* [4] Cao, M., Wang, X., Qi, Z., Shan, Y., Qie, X., Zheng, Y.: Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465 (2023)\n' +
      '* [5] Chen, L., Zhao, M., Liu, Y., Ding, M., Song, Y., Wang, S., Wang, X., Yang, H., Liu, J., Du, K., et al.: Photoverse: Tuning-free image customization with text-to-image diffusion models. arXiv preprint arXiv:2309.05793 (2023)\n' +
      '* [6] Chen, X., Huang, L., Liu, Y., Shen, Y., Zhao, D., Zhao, H.: Anydoor: Zero-shot object-level image customization. arXiv preprint arXiv:2307.09481 (2023)\n' +
      '* [7] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.: An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022)\n' +
      '* [8] Gal, R., Arar, M., Atzmon, Y., Bermano, A.H., Chechik, G., Cohen-Or, D.: Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG) **42**(4), 1-13 (2023)\n' +
      '* [9] Hertz, A., Voynov, A., Fruchter, S., Cohen-Or, D.: Style aligned image generation via shared attention. arXiv preprint arXiv:2312.02133 (2023)\n' +
      '* [10] Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance normalization. In: 2017 IEEE International Conference on Computer Vision (ICCV) (Oct 2017). [https://doi.org/10.1109/iccv.2017.167](https://doi.org/10.1109/iccv.2017.167), [http://dx.doi.org/10.1109/iccv.2017.167](http://dx.doi.org/10.1109/iccv.2017.167)\n' +
      '* [11] Inoue, N., Kikuchi, K., Simo-Serra, E., Otani, M., Yamaguchi, K.: Layoutdm: Discrete diffusion model for controllable layout generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10167-10176 (2023)\n' +
      '* [12] Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani, M.: Imagic: Text-based real image editing with diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6007-6017 (2023)\n' +
      '* [13] Khachatryan, L., Movsisyan, A., Tadevosyan, V., Henschel, R., Wang, Z., Navasardyan, S., Shi, H.: Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439 (2023)\n' +
      '* [14] Kumari, N., Zhang, B., Zhang, R., Shechtman, E., Zhu, J.Y.: Multi-concept customization of text-to-image diffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1931-1941 (2023)\n' +
      '* [15] Li, D., Li, J., Hoi, S.C.: Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. arXiv preprint arXiv:2305.14720 (2023)\n' +
      '* [16] Li, Z., Cao, M., Wang, X., Qi, Z., Cheng, M.M., Shan, Y.: Photomaker: Customizing realistic human photos via stacked id embedding. arXiv preprint arXiv:2312.04461 (2023)* [17] Li, Z., Wang, C., Zheng, H., Zhang, J., Li, B.: Fakeclr: Exploring contrastive learning for solving latent discontinuity in data-efficient gans. In: European Conference on Computer Vision. pp. 598-615. Springer (2022)\n' +
      '* [18] Ma, J., Liang, J., Chen, C., Lu, H.: Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. arXiv preprint arXiv:2307.11410 (2023)\n' +
      '* [19] Peng, X., Zhu, J., Jiang, B., Tai, Y., Luo, D., Zhang, J., Lin, W., Jin, T., Wang, C., Ji, R.: Portraitbooth: A versatile portrait model for fast identity-preserved personalization. arXiv preprint arXiv:2312.06354 (2023)\n' +
      '* [20] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023)\n' +
      '* [21] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* arXiv,Cornell University\n' +
      '- arXiv (Feb 2021)\n' +
      '* [23] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research **21**(1), 5485-5551 (2020)\n' +
      '* [24] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [25] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. pp. 234-241. Springer (2015)\n' +
      '* [26] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22500-22510 (2023)\n' +
      '* [27] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems **35**, 36479-36494 (2022)\n' +
      '* [28] Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face recognition and clustering. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 815-823 (2015)\n' +
      '* [29] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.: Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems **35**, 25278-25294 (2022)\n' +
      '* [30] Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., Komatsuzaki, A.: Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114 (2021)\n' +
      '* [31] Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.: Zero123++: a single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110 (2023)* [32] Wei, Y., Zhang, Y., Ji, Z., Bai, J., Zhang, L., Zuo, W.: Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848 (2023)\n' +
      '* [33] Wu, J.Z., Ge, Y., Wang, X., Lei, S.W., Gu, Y., Shi, Y., Hsu, W., Shan, Y., Qie, X., Shou, M.Z.: Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7623-7633 (2023)\n' +
      '* [34] Wu, Y., Li, Z., Wang, C., Zheng, H., Zhao, S., Li, B., Tao, D.: Domain remodulation for few-shot generative domain adaptation. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [35] Xiao, G., Yin, T., Freeman, W.T., Durand, F., Han, S.: Fastcomposer: Tuning-free multi-subject image generation with localized attention. arXiv preprint arXiv:2305.10431 (2023)\n' +
      '* [36] Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023)\n' +
      '* [37] Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P., Riquelme, C., Lucic, M., Djolonga, J., Pinto, A.S., Neumann, M., Dosovitskiy, A., et al.: A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867 (2019)\n' +
      '* [38] Zhang, C., Zhang, C., Zhang, M., Kweon, I.S.: Text-to-image diffusion model in generative ai: A survey. arXiv preprint arXiv:2303.07909 (2023)\n' +
      '* [39] Zhang, K., Zhang, Z., Li, Z., Qiao, Y.: Joint face detection and alignment using multitask cascaded convolutional networks. IEEE signal processing letters **23**(10), 1499-1503 (2016)\n' +
      '* [40] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3836-3847 (2023)\n' +
      '* [41] Zheng, Y., Yang, H., Zhang, T., Bao, J., Chen, D., Huang, Y., Yuan, L., Chen, D., Zeng, M., Wen, F.: General facial representation learning in a visual-linguistic manner. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18697-18709 (2022)Supplementary Materials\n' +
      '\n' +
      '### Evaluation metrics\n' +
      '\n' +
      '**Evaluation of face similarity.** To assess face similarity, we utilize the face alignment module \\(\\text{FA}(\\cdot)\\), the face recognition backbone \\(\\text{E}_{\\text{face}}(\\cdot)\\), and the CLIP image encoder \\(\\text{E}_{\\text{clip}}(\\cdot)\\) to compute the metrics \\(M_{\\text{FaceNet}}\\) and CLIP-I. Specifically, for each generated image \\(\\text{I}_{\\text{gen}}\\) and its corresponding identity image \\(\\text{I}_{\\text{id}}\\), we first employ the \\(\\text{FA}(\\cdot)\\) module to detect the face. Subsequently, we calculate the pairwise identity similarity using \\(\\text{E}_{\\text{face}}(\\cdot)\\) and \\(\\text{E}_{\\text{clip}}(\\cdot)\\), respectively:\n' +
      '\n' +
      '\\[\\begin{split} M_{\\text{FaceNet}}=cos(\\text{E}_{\\text{face}}( \\text{FA}(\\text{I}_{\\text{gen}})),\\text{E}_{\\text{face}}(\\text{FA}(\\text{I}_{ \\text{id}}))),\\\\ \\text{CLIP-I}=cos(\\text{E}_{\\text{clip}}(\\text{FA}(\\text{I}_{ \\text{gen}})),\\text{E}_{\\text{clip}}(\\text{FA}(\\text{I}_{\\text{id}}))),\\end{split} \\tag{9}\\]\n' +
      '\n' +
      'where \\(cos(\\cdot,\\cdot)\\) is the cosine similarity function. Furthermore, in order to illustrate the identity fidelity depicted in Figure 6 of the main paper, we integrate both \\(M_{\\text{FaceNet}}\\) and CLIP-I by utilizing z-score normalization:\n' +
      '\n' +
      '\\[mean(\\text{z-score}(M_{\\text{FaceNet}}),\\text{z-score}(\\text{CLIP-I})), \\tag{10}\\]\n' +
      '\n' +
      'where \\(\\text{z-score}(x)=(x-\\mu)/\\sigma\\), \\(\\mu\\) and \\(\\sigma\\) are the average and standard deviation of the \\(x\\), respectively.\n' +
      '\n' +
      '**Definition of semantic consistency.** We adopt the CLIP-T metric to assess semantic consistency. Specifically, for a generated image \\(I_{\\text{gen}}\\) paired with its corresponding prompt \\(P\\), we compute the CLIP-T metric utilizing both the CLIP image encoder \\(E_{\\text{clip}}\\) and the CLIP text encoder \\(E_{\\text{text}}\\):\n' +
      '\n' +
      '\\[\\text{CLIP-T}=cos(E_{\\text{clip}}(I_{gen}),E_{\\text{text}}(P)), \\tag{11}\\]\n' +
      '\n' +
      'where the \\(cos(\\cdot,\\cdot)\\) is the cosine similarity function.\n' +
      '\n' +
      '### More Results on Ablation Study\n' +
      '\n' +
      '**Ablation Study of Cross-attention Merge.** We conduct ablation experiments on the cross-attention merge to evaluate its effectiveness. As depicted in Fig. 9 and Table 2, the incorporation of cross-attention merge demonstrates improvement in semantic consistency.\n' +
      '\n' +
      '**Ablation Study of Input ID Images\' Resolution.** We perform an ablation study on the resolution of input ID images to assess the robustness of our method. Specifically, we utilize images with varying resolutions while maintaining the same text prompt for personalization. As illustrated in Fig. 10, the identity fidelity exhibits only a marginal decrease with decreasing image resolution, while semantic consistency remains stable across all resolutions. In conclusion, our method demonstrates robustness to changes in input image resolution.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline  & CLIP-T\\(\\uparrow\\) & CLIP-I\\(\\uparrow\\) & \\(M_{\\text{FaceNet}}\\uparrow\\) \\\\ \\hline Ours w/o Cross-attention merge & 0.335 & 0.910 & 0.681 \\\\ \\hline Ours & **0.340** & **0.913** & **0.689** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Quantitative ablation of cross-attention merge.** The metrics includes CLIP-T (higher is better) measuring the semantic consistency, CLIP-I (higher is better) and \\(M_{\\text{FaceNet}}\\) (higher is better) which are both reflect the identity fidelity. The best result is shown in **bold**.\n' +
      '\n' +
      'Figure 10: **Ablation study of input ID images resolution**. The identity fidelity slightly drops along with the lower image resolution and the semantic consistency is stable for all the resolution. Our method is robust to the resolution of input ID image.\n' +
      '\n' +
      'Figure 9: **Quantitative ablation of cross-attention merge.** It is obvious that the cross-attention merge helps to improve the semantic consistency.\n' +
      '\n' +
      '### Identity mixing\n' +
      '\n' +
      'Upon receiving multiple images from distinct individuals, we stack all the identity embeddings to merge corresponding identities, as depicted in Fig. 11. The generated image can well retain the characteristics of different IDs, which releases possibilities for more applications. Additionally, by adjusting the interpolation of the identity embeddings, we can regulate the similarity between the generated identity and different input identities, as demonstrated in Fig. 12.\n' +
      '\n' +
      '### More Qualitative Results of Raw Photo Generation\n' +
      '\n' +
      'Fig. 13 demonstrates the ability of our method to extract identity information from artworks while preserving identity for personalization purposes. Additionally, Fig. 14 illustrates the capability of our method to alter attributes of the extracted identities for raw photo generation. Additional visual samples for raw photo generation are provided in Fig. 15 and Fig. 16, showcasing identities of ordinary individuals sampled from the FFHQ dataset, spanning diverse races, skin tones, and genders.\n' +
      '\n' +
      'Figure 11: **Identity mixing. When receiving multiple input ID images from different individuals, our method can mix these identities by stacking all the identity embeddings.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:21]\n' +
      '\n' +
      'Figure 14: Applications on attribute change.\n' +
      '\n' +
      'Figure 13: Applications on artworks to raw photo.\n' +
      '\n' +
      'Figure 15: **Raw photo generation.** These identities are ordinary people sampled from FFHQ dataset, including various races, skin colors, male and female.\n' +
      '\n' +
      'Figure 16: **Raw photo generation.** These identities are ordinary people sampled from FFHQ dataset, including various races, skin colors, male and female.\n' +
      '\n' +
      'Figure 17: **More visual examples for stylization.** These identities are ordinary people sampled from FFHQ dataset, including various races, skin colors, male and female.\n' +
      '\n' +
      'Figure 18: **More visual examples for stylization.** These identities are ordinary people sampled from FFHQ dataset, including various races, skin colors, male and female.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>