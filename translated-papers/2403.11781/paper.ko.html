<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Infinite-ID: ID-semantics Decoupling Paradigm을 통한 신원 보존 개인화\n' +
      '\n' +
      'Yi Wu\n' +
      '\n' +
      '처음 두 명의 저자는 이 작업에 동등하게 기여했다.1중국과학기술대학교, 중국1\n' +
      '\n' +
      'Ziqiang Li\n' +
      '\n' +
      '교신저.1중국과학기술대학교\n' +
      '\n' +
      'Heliang Zheng\n' +
      '\n' +
      '1중국과학기술대학교\n' +
      '\n' +
      'Chaoyue Wang\n' +
      '\n' +
      '2호주 시드니대학교\n' +
      '\n' +
      'Bin Li\n' +
      '\n' +
      '교신저.1중국과학기술대학교\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '텍스트-이미지 생성을 위한 확산 모델의 최근 발전에 따라, 아이덴티티 보존 개인화는 단일 참조 이미지만으로 특정 아이덴티티를 정확하게 캡처하는 데 상당한 진전을 이루었다. 그러나, 기존의 방법들은 주로 텍스트 임베딩 공간 내에 참조 이미지들을 통합함으로써, 이미지와 텍스트 정보의 복잡한 얽힘으로 이어지며, 이는 아이덴티티 충실도와 의미적 일관성을 모두 보존하기 위한 과제를 제기한다. 이 문제를 해결하기 위해 우리는 신원 보존 개인화를 위한 ID-의미 디커플링 패러다임인 **무한-ID**를 제안한다. 구체적으로, 확산 모델의 원본 텍스트 교차 주의 모듈_을 비활성화하면서 충분한 ID 정보를 캡처하기 위해 추가 이미지 교차 주의 모듈을 통합하는 신원 강화 교육을 소개한다. 이것은 이미지 스트림이 텍스트 입력으로부터의 간섭을 완화하면서 기준 이미지에 의해 제공된 아이덴티티를 충실하게 나타내는 것을 보장한다. 또한 AdaIN-mean 연산과 혼합 어텐션 모듈을 결합하여 두 스트림을 끊김없이 병합하는 특징 상호 작용 메커니즘을 소개한다. 이 메커니즘은 동일성 및 의미 일관성의 충실도를 향상시킬 뿐만 아니라 생성된 이미지의 스타일에 대한 편리한 제어를 가능하게 한다. 원시 사진 생성과 스타일 이미지 생성 모두에 대한 광범위한 실험 결과는 제안된 방법의 우수한 성능을 보여준다.\n' +
      '\n' +
      '키워드: 개인화된 텍스트-이미지 생성, 안정적인 확산, 아이덴티티 보존 개인화\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '인간 사진 합성[17, 34]은 특히 SD(Stable Diffusion) [24], Imagen [27], DALL-E 3 [3]과 같은 큰 텍스트 대 이미지 확산 모델의 도입으로 주목할 만한 발전을 경험했다. 개인화된 텍스트-이미지 생성의 이점, 최근 연구는 _Identity-preserved personalization_에 초점을 맞추고 있다. 이 전문 영역은 참신한 장면, 행동, 스타일에 특정 정체성을 충실하게 반영하는 고도로 맞춤화된 사진을 제작하여 하나 이상의 참조 이미지에서 영감을 끌어내는 것을 목표로 한다. 이 작업은 개인화된 AI 초상화 및 가상 시도 시나리오를 포함한 수많은 응용 프로그램의 개발로 인해 상당한 관심을 받았다. 아이덴티티-보존된 개인화의 맥락에서, 인간 얼굴 아이덴티티(ID)의 불변성을 유지하는 것에 강조가 배치되고, 보다 일반적인 스타일들 또는 객체들에 비해 더 높은 레벨의 디테일과 충실도를 요구한다.\n' +
      '\n' +
      '최근의 튜닝 프리 방법은 대규모 배포에 대한 가능성을 보여주지만, 신원 표현의 충실도(ID 충실도)와 텍스트 프롬프트에 의해 전달되는 의미적 이해의 일관성 사이의 균형을 맞추는 데 주목할만한 도전에 직면해 있다. 이러한 도전은 이미지 및 텍스트 정보의 고유한 얽힘으로 인해 발생한다. 일반적으로, 튜닝 프리 방법은 참조 이미지로부터 ID 정보를 추출하고 두 가지 별개의 방식으로 의미 정보에 통합한다. 포토메이커[16]에 의해 예시되는 제1 유형은 텍스트 인코더의 텍스트 임베딩 공간에 ID 세부사항을 갖는 텍스트 정보를 통합한다. 이 병합 접근법은 의미 일관성을 달성하는 데 도움이 되지만, 이미지 특징을 텍스트 임베딩 공간으로 압축하여 이미지의 ID 정보를 약화시키고 아이덴티티 충실도를 손상시킨다. IP-Adapter[36]에 의해 입증된 두 번째 유형은 추가 훈련 가능한 교차 주의 모듈을 통해 확산 모델의 U-Net에 ID 정보를 직접 주입한다. 이 접근법은 향상된 충실도를 위해 ID 정보의 강도를 향상시키는 것을 목표로 하지만, 트레이닝 동안 이미지 분기를 선호하여 결과적으로 텍스트 분기를 약화시키고 의미 일관성을 손상시키는 경향이 있다. 요약하면, 기존의 방법들은 이미지와 텍스트 정보를 얽히게 되어, (도 6에 예시된 바와 같이) ID 충실도와 의미 일관성 사이의 상당한 트레이드-오프를 초래한다.\n' +
      '\n' +
      '이미지와 텍스트 정보 사이의 얽힘을 해결하기 위해, 우리는 개인화된 텍스트 대 이미지 진에 대한 혁신적인 접근법인 **Infinite-ID**를 제안한다.\n' +
      '\n' +
      '그림 1: 단일 참조 이미지만으로, 우리의 무한-ID 프레임워크는 다양한 스타일에서 우수한 아이덴티티 충실도와 텍스트 의미 일관성을 유지하면서 고품질 이미지를 합성하는 데 탁월하다.\n' +
      '\n' +
      ' 기권. 이 방법은 ID-의미론적 디커플링 패러다임을 구현함으로써 높은 동일성 유지와 텍스트 프롬프트의 의미적 일관성 보장 사이의 상충 관계를 해결한다. 구체적으로, 충분한 ID 정보를 캡처하기 위해 추가적인 이미지 교차 주의 모듈을 도입하고 훈련 단계에서 텍스트 간섭을 피하기 위해 원본 텍스트 교차 주의 모듈을 비활성화하는 신원 강화 훈련을 채택한다. 따라서 본 논문에서 제안하는 방법은 참조 영상으로부터 신원 정보를 충실하게 포착할 수 있어 ID 충실도를 크게 향상시킬 수 있다. 또한, 텍스트 정보와 신원 정보를 효과적으로 병합하기 위해 혼합 어텐션 모듈과 AdaIN-mean 연산을 활용하는 새로운 특징 상호 작용 메커니즘을 사용한다. 특히, 우리의 특징 상호작용 메커니즘은 아이덴티티 및 시맨틱 세부사항 모두를 효과적으로 보존할 뿐만 아니라 생성된 이미지의 스타일(도 1에 도시된 바와 같이)에 대한 편리한 제어를 가능하게 한다. 우리의 기여는 다음과 같이 요약된다.\n' +
      '\n' +
      '* 우리는 이미지와 텍스트 정보 사이의 얽힘을 해결하기 위한 새로운 ID-의미 디커플링 패러다임을 제안하며, _Identity-preserved personalization_에서 ID 충실도와 의미 일관성 사이의 현저한 균형을 획득한다.\n' +
      '* ID 정보와 텍스트 정보를 효과적으로 병합하고 확산 모델에서 생성된 이미지의 스타일을 편리하게 제어하기 위해 혼합 어텐션 모듈과 AdaIN-mean 연산을 통합한 새로운 특징 상호작용 메커니즘을 제안한다.\n' +
      '* 실험 결과는 원시 사진 생성 및 스타일 이미지 생성 모두에서 현재 최신 방법에 비해 제안된 방법의 우수한 성능을 보여준다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '### 텍스트-이미지 확산 모델\n' +
      '\n' +
      '[38, 38, 24, 27, 12, 40]에서 탐색된 것과 같은 텍스트 대 이미지 확산 모델은 인상적인 이미지 생성 능력으로 인해 상당한 관심을 받았다. 현재 연구는 고품질 및 대규모 데이터 세트의 활용[29, 30], 기반 아키텍처의 개선[3, 24, 27], 제어 가능성의 발전[11, 40, 26]을 포함하여 여러 전선을 따라 이러한 모델을 더욱 향상시키는 것을 목표로 하고 있다. 텍스트-이미지 확산 모델의 현재 반복은 전형적으로 2단계 프로세스를 따른다: 먼저, CLIP[21] 또는 T5[23]과 같은 미리 훈련된 텍스트 인코더를 사용하여 텍스트 프롬프트를 인코딩한 다음, 결과적인 텍스트 임베딩을 확산 프로세스를 통해 대응하는 이미지를 생성하기 위한 조건으로 활용한다. 특히, 널리 채택된 안정 확산 모델[24]은 원래 픽셀 공간이 아닌 잠재 공간에서 확산 프로세스를 실행함으로써 자신을 구별하여 계산 및 시간 비용의 상당한 감소를 초래한다. 이 프레임워크의 중요한 확장은 SDXL(Stable Diffusion XL) [20]으로, U-Net 아키텍처를 확장하고 추가 텍스트 인코더를 도입함으로써 성능을 향상시킨다. 따라서, 제안된 방법은 SDXL을 기반으로 한다. 그러나 이 방법은 다른 텍스트-이미지 확산 모델에도 확장될 수 있다.\n' +
      '\n' +
      '### Identity-preserved personalization\n' +
      '\n' +
      '아이덴티티-보존 개인화는 다양한 장면들, 액션들 및 스타일들에 걸쳐 특정 아이덴티티를 정확하게 반영하는 고도로 맞춤화된 사진들을 생성하고, 하나 이상의 참조 이미지들로부터 영감을 끌어내는 것을 목표로 한다. 처음에, DreamBooth[26] 및 Textual Inversion[7]로 예시된 _tuning-based methods_는 동일한 아이덴티티(ID)의 이미지들을 채용하여 모델을 미세 조정한다. 이러한 방법은 얼굴 신원(ID)을 보존하는 데 높은 충실도를 갖는 결과를 산출하지만, 중요한 단점이 나타난다: 각 ID의 사용자 지정은 10-30분[14]의 시간 투자를 필요로 하며, 상당한 컴퓨팅 자원과 시간을 소비한다. 이러한 제한은 상업적 응용에서 대규모 배치에 상당한 장애물을 제기한다. 결과적으로, 최근 _tuning-free 방법_[5, 6, 8, 15, 16, 18, 32, 35, 36]의 발전은 생성 프로세스를 간소화하기 위해 도입되었다. 이러한 방법들은 특히 방대한 양의 도메인-특정 데이터의 구성 및 인코더 또는 하이퍼-네트워크의 트레이닝을 활용하여 입력 ID 이미지를 모델 내의 임베딩 또는 LoRA 가중치로서 표현한다. 학습 후, 사용자는 커스터마이징을 위해 ID의 이미지만 입력하면 되므로 추론 단계에서 몇 초 이내에 개인화된 생성이 가능하다. 이러한 튜닝 프리 방법은 일반적으로 두 가지 별개의 방식을 포함한다.\n' +
      '\n' +
      '한편, 방법 [1, 16, 19, 35]는 텍스트 인코더의 텍스트 임베딩 공간 내에 신원 세부사항과 함께 텍스트 정보를 통합한다. 예를 들어, 포토메이커[16]는 단일 또는 다수의 참조 이미지로부터 아이덴티티 임베딩을 추출하고 이를 텍스트 인코더의 텍스트 임베딩 공간 내의 대응하는 클래스 임베딩(예를 들어, "남자" 및 "여자")과 병합한다. 이러한 스태킹 동작은 의미적 일관성을 달성하는 데 도움이 되지만, 이미지 특징을 텍스트 임베딩 공간으로 압축하여 손상된 아이덴티티 충실도를 유도한다. 한편, 일부 연구 [5, 32, 36]은 신원 정보를 확산 모델의 U-Net에 직접 통합한다. IP-어댑터 [36]은 원래의 UNet 모델 내에 존재하는 각각의 교차-어텐션 레이어에 대한 추가적인 교차-어텐션 레이어를 통합함으로써 자신을 구별한다. 이 접근법은 신원 정보를 U-net 내에서 직접 의미론적 세부사항과 병합하지만 U-Net 모델의 의미론적 공간의 왜곡을 초래한다. 결과적으로, 이것은 텍스트 프롬프트의 의미적 일관성을 손상시킨다.\n' +
      '\n' +
      '요약하면, 기존의 방법들은 아이덴티티와 텍스트 정보를 얽히게 되어, ID 충실도와 의미 일관성 사이의 상당한 트레이드오프로 이어진다. 이러한 한계를 완화하기 위해 ID와 텍스트 정보를 별도로 캡처하는 신원 강화 교육을 제안한다. 또한, 아이덴티티와 시맨틱 디테일을 동시에 보존하면서 생성된 이미지의 스타일을 편리하게 제어할 수 있도록 혼합 어텐션 모듈과 AdaIN-mean 연산을 활용하는 효과적인 특징 상호작용 메커니즘을 설계한다.\n' +
      '\n' +
      '확산모델에서### 주의집중제어\n' +
      '\n' +
      '이전 연구에서는 확산 모델 내에서 다양한 주의 제어 기법을 조사하였다. Hertz _et al._[9]는 공유 주의 메커니즘을 사용하여 참조 스타일을 사용하여 스타일 일관성 있는 이미지 생성을 보장하기 위해 자기 주의 계층 내에서 참조 이미지와 합성 이미지 사이의 키 및 값에 AdaIN 모듈을 연결하고 적용한다. Cao _et al._[4]는 일관된 이미지 생성과 비강경 이미지 편집을 달성하기 위해 상호 자기 주의 접근법을 활용했으며, 여기서 합성 이미지의 키와 값은 확산 모델의 자기 주의 계층 내에서 참조 이미지의 키와 값으로 대체되었다. 유사하게, Shi _et al._[31]은 참조 주의라고 불리는 방법을 제안하여, 자기 주의 계층에서 조건 신호와 합성 이미지 사이의 키 및 값 특징을 연결함으로써 목표 객체의 일관된 멀티뷰 생성을 가능하게 하였다. Wang _et al._[35] 및 Avrahami _et al._[2]는 개인화된 생성 작업에서 학습된 개념을 분리하도록 최적화 프로세스를 안내하기 위해 교차 주의 계층 내의 주의 지도를 이용했다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '본 논문에서 제안한 방법은 3개의 핵심 구성 요소, 즉 \\(\\xi(\\cdot)\\로 표기된 Variational AutoEncoder(VAE), \\(\\epsilon_{\\theta}(\\cdot)\\로 표기된 Conditional U-Net[25), 그리고 \\(\\theta_{1}(\\cdot)\\)와 \\(\\theta_{2}(\\cdot)\\로 표기된 2개의 사전 학습된 텍스트 인코더[22]를 포함하는 안정적인 Diffusion XL[20]을 기반으로 한다. 구체적으로, 학습 이미지\\(x_{0}\\)와 그에 대응하는 텍스트 프롬프트\\(T\\)이 주어지면, VAE 인코더\\(\\xi\\)는 원래의 공간\\(R^{H\\times W\\times 3}\\)에서 압축된 잠재 표현\\(z_{0}=\\xi(x_{0})\\)으로 \\(x_{0}\\)을 변환하며, 여기서\n' +
      '\n' +
      '그림 2: ** ID-의미론적 디커플링 패러다임**의 프레임워크. 학습 단계에서는 얼굴 임베딩 추출기를 사용하여 풍부한 신원 정보를 추출하고, 텍스트 입력으로부터 간섭을 완화하면서 참조 영상에 의해 제공되는 신원을 충실하게 표현하기 위한 신원 강화 학습을 수행한다. 추론 단계에서는 디노이징 U-Net 모델 내에서 원래의 자기 주의 메커니즘을 대체하기 위해 혼합 주의 모듈을 도입하여 아이덴티티와 텍스트 정보의 융합을 용이하게 한다.\n' +
      '\n' +
      '(z_{0}\\in R^{h\\times w\\times c}\\) 및 \\(c\\)은 잠재 차원을 나타낸다. 이어서, 확산 프로세스는 계산 자원 및 메모리를 보존하기 위해 이 압축된 잠재 공간 내에서 작동한다. 두 개의 텍스트 인코더가 텍스트 프롬프트\\(T\\)를 텍스트 임베딩\\(c=\\text{Concat}(\\Theta_{1}(T),\\Theta_{2}(T))\\으로 처리하면, 조건부 U-Net\\(\\epsilon_{\\theta}\\)은 현재 타임스텝\\(t\\), 잠재표현\\(z_{t}\\), 텍스트 임베딩\\(c\\)을 기반으로 잡음\\(\\epsilon\\)을 예측한다. 훈련 목표는 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\[L_{text{diffusion}}=E_{z_{t},t,c,\\epsilon\\in N(0,I)}[||\\epsilon-\\epsilon_{ \\theta}(z_{t},t,c)||_{2}^{2}]. \\tag{1}\\\n' +
      '\n' +
      '확산 모델에서의 주의 메커니즘.안정 확산 모델의 기본 단위는 재블록, 자기 주의층 및 교차 주의층으로 구성된다. 어텐션 메커니즘은 다음과 같이 표현된다:\n' +
      '\n' +
      '\\[\\text{Attn}(Q,K,V)=\\text{Softmax}(\\frac{QK^{T}}{\\sqrt{d}})V, \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(Q\\)은 선행 resblock에 의해 생성된 공간 특징들로부터 투영된 질의 특징을 나타내고, \\(K\\) 및 \\(V\\)은 질의 특징과 동일한 공간 특징들로부터 투영된 키 및 값 특징들 또는 텍스트 프롬프트로부터 추출된 텍스트 임베딩(cross-attention)을 나타낸다.\n' +
      '\n' +
      '### Methodology\n' +
      '\n' +
      '개요.이 섹션에서는 그림 1과 같이 ID-의미론적 디커플링 패러다임을 소개한다. 2는 정체성이 보존된 개인화 내에서 고충실도 정체성과 의미 일관성 사이의 심각한 상충 관계를 효과적으로 해결한다. 그 후, 그림 1에 표시된 혼합 주의 메커니즘을 제시한다. 3은 추론 단계에서 확산 모델 내에서 ID 정보와 의미 정보를 원활하게 통합하도록 설계되었다. 또한, 적응 평균 정규화(AdaIN-mean) 연산을 사용하여 합성된 이미지의 스타일을 원하는 스타일 프롬프트와 정확하게 정렬한다.\n' +
      '\n' +
      'ID-의미 디커플링 패러다임.고충실도 아이덴티티를 충실하게 포착하기 위해 그림 2와 같이 훈련 단계에서 새로운 아이덴티티 강화 전략을 구현한다. 텍스트-이미지 쌍을 훈련에 활용하는 기존 방법 [5, 16, 36]에서 벗어나 텍스트 프롬프트 입력을 배제하고 U-Net 모델 내에서 텍스트 임베딩을 위한 교차 주의 모듈을 비활성화하기로 선택한다. 대신, 우리는 신원 정보를 추출하기 위해 얼굴이 정렬된 ID 이미지와 노이즈 제거 이미지(그림 2의\\(x_{t}\\)로 구성된 훈련 쌍을 설정한다. 훈련에 사용되는 노이즈 제거 영상과 ID 영상은 모두 동일한 개인에 속하지만 시점, 표정 등의 요인이 다양하다. 이 접근법은 보다 포괄적인 학습 과정을 육성한다[16]. 입력된 ID 이미지에서 신원 정보를 정확하게 캡처하고 활용하기 위해 얼굴 임베딩 추출기를 채택한다. 또한, 디노이징 U-Net 모델에 신원 정보를 원활하게 통합하기 위해, 우리는 여분의 훈련 가능한 교차 주의 메커니즘(그림 2의\\(K^{\\prime}_{id}\\) 및 \\(V^{\\prime}_{id}\\)을 도입한다. 를 포함하는 이미지 임베딩 방법.\n' +
      '\n' +
      '훈련 단계에서는 얼굴 맵퍼, CLIP 맵퍼 및 이미지 교차 주의 모듈과 관련된 매개변수를 독점적으로 최적화하는 동시에 사전 훈련된 확산 모델의 매개변수를 고정한다. 최적화 손실은 원래 확산 손실 공식(식 1에 설명된 대로)과 매우 유사하다. 유일한 구분은 텍스트 조건으로부터 조건 입력으로서 동일성 조건으로의 이동이다.\n' +
      '\n' +
      '\\[L_{text{diffusion}}=E_{z_{t},t,c_{id},e\\in N(0,I}[||\\epsilon-\\epsilon_{\\theta}(z_{t},t,c_{id})||_{2}^{2}], \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(c_{id}\\)는 입력 ID 이미지의 아이덴티티 임베딩이다.\n' +
      '\n' +
      '추론 단계 동안, 우리는 신원 정보를 제공하기 위해 ID 이미지 내에 원하는 얼굴을 정렬한다. 훈련 단계에서 채택된 접근법에 따라, 우리는 정렬된 얼굴 이미지로부터 신원 특징을 추출하기 위해 얼굴 인식 백본과 CLIP 이미지 인코더를 활용한다. 학습된 얼굴 맵퍼, 클립 맵퍼 및 이미지 교차 주의 메커니즘을 활용하여 이러한 아이덴티티 특징은 노이즈 제거 U-Net 모델에 매끄럽게 통합된다. 그 후, 텍스트 프롬프트 입력만을 고려하여 원래의 안정 확산 모델에서 자기 주의를 위한 키 및 값 특징을 계산한다. 이러한 텍스트 키와 가치 특징들은 혼합 주의 과정(도 3에 도시됨)에서 도구적이다. 텍스트와 신원 정보의 융합을 용이하게 한다. 또한, 디노이징 과정에서 텍스트 정보를 더 증강하기 위해 원본 텍스트 교차 주의 메커니즘을 통합하여 결과 텍스트 숨김 상태를 이미지 교차 주의 모듈로부터 얻은 출력 이미지 숨김 상태와 통합한다.\n' +
      '\n' +
      '**얼굴 임베딩 추출기.** 얼굴 특징을 추출하기 위해 미리 훈련된 모델을 통합하여 다각적인 접근법을 채택한다. 첫째, 방법을 따르는 것\n' +
      '\n' +
      '그림 3: **혼합 주의 메커니즘**. 왼쪽에는 아이덴티티와 텍스트 정보를 융합하기 위해 혼합된 주의를 사용한다. 이것은 그들의 각각의 키 및 값 특징들을 연접하고 이어서 혼합된 주의를 적용하는 것을 포함하며, 여기서 신원 특징들은 연접된 키 및 값 특징들에 기초하여 업데이트된다. 오른쪽에는 스타일 병합을 위해 추가 AdaIN-mean 연산(식 8에 표시된 대로)을 도입한다. 연결된 키 및 값 피쳐에 연결됩니다.\n' +
      '\n' +
      '선행연구에서는 사전 학습된 CLIP 영상 인코더를 얼굴 특징 추출기 중 하나로 활용한다. 구체적으로, CLIP 이미지 인코더로부터 획득된 마지막 은닉 상태들로 구성된 로컬 임베딩들을 활용하여, N(구현시 N=257)의 길이를 갖는 임베딩들의 시퀀스를 형성한다. 그 후, CLIP 매퍼를 사용하여 이러한 이미지 임베딩을 사전 훈련된 확산 모델의 텍스트 특징과 동일한 차원으로 투영한다(차원 1664). [36]에서 설명된 바와 같이, CLIP 이미지 인코더에 의해 추출된 특징들은 신원-보존된 개인화 작업들 내에서 신원 얼굴에 관련된 구조적 정보를 캡처하는 데 중요한 역할을 한다. 또한 얼굴 인식 모델의 백본을 다른 얼굴 특징 추출기로 활용한다. [36]에서 강조된 바와 같이, 얼굴 인식 백본에 의해 추출된 특징들은 신원-보존된 개인화 작업들 내에서 인간 얼굴 특징들과 연관된 특징들을 캡처하는 데 능숙하다. 보다 구체적으로, 추출된 특징들로부터 유도된 전역 이미지 임베딩을 활용하고, 이어서 얼굴 맵퍼를 채용하여, 추출된 전역 이미지 임베딩의 차원(512 차원)을 미리 트레이닝된 확산 모델에서 텍스트 특징들의 차원성과 정렬한다.\n' +
      '\n' +
      '요약하면, ID 이미지 \\(x\\)에 대응하는 아이덴티티 임베딩 \\(c_{id}\\)은 다음과 같이 표현될 수 있다:\n' +
      '\n' +
      '\\text{M}_{\\text{clip}}\\bigg{(}\\text{M}_{\\text{clip}}\\big{(}\\text{E}_{\\text{clip}}(\\text{FA}(x))\\big{},\\text{M}_{\\text{face}}\\big{(}\\text{E}_{\\text{face}}( \\text{FA}(x))\\bigg{}, \\tag{4}\\big{}\n' +
      '\n' +
      '여기서 \\(\\text{Concat}(\\cdot,\\cdot)\\), \\(\\text{M}_{\\text{clip}}(\\cdot)\\), \\(\\text{E}_{\\text{clip}}(\\cdot)\\), \\(\\text{M}_{\\text{face}(\\cdot)\\), \\(\\text{E}_{\\text{face}(\\cdot)\\), \\(\\text{FA}(\\cdot)\\)는 각각 연결 함수, CLIP 매핑기, CLIP 이미지 인코더, 얼굴 매핑기, 얼굴 인식 백본 및 얼굴 정렬 모듈[39]을 나타낸다.\n' +
      '\n' +
      '**혼합 주의 메커니즘.** 이전 연구 [4, 13, 33]에서 탐색된 바와 같이, 자기 주의 계층의 특징은 일관성 이미지 생성(텍스트-비디오 작업에서 프레임 간)에 중요한 역할을 하며, 이는 이러한 특징이 세련되고 상세한 의미 정보를 제공한다는 것을 나타낸다. 본 연구에서는 원본 텍스트-이미지 확산 모델의 자기 주의 계층에서 특징을 추출하여 \\(K_{t}\\) 및 \\(V_{t}\\)으로 표현되는 풍부한 의미 정보를 캡처한다. 우리는 그림 1에 표시된 혼합 주의 프레임워크에 통합함으로써 자기 주의 메커니즘을 향상시킨다. 3(a). 이 융합은 의미적 특징(\\(K_{t}\\) 및 \\(V_{t}\\))과 신원 기반 특징(\\(K_{id}\\) 및 \\(V_{id}\\))의 통합을 가능하게 하여 신원 정보를 캡슐화한다. 이러한 통합을 통해, 혼합 주의 메커니즘은 서로 다른 해상도에 걸쳐 생성된 특징들에 의미론적 세부사항들을 매끄럽게 병합한다. 혼합 주의 메커니즘의 제형은 다음과 같다:\n' +
      '\n' +
      '\\text{attn}_{text{mix}(Q,K,V)\\triangleq\\text{Attn}(Q,\\hat{K},\\hat{K}=\\text{Concat}(K_{id},K_{t}),\\quad\\hat{V}=\\text{Concat}(V_{id},V_{t}),\\\\&K_{id}=W_{k}^{id}Z_{t},\\quad K_{t}=W_{v}^{t}Z_{t},\\end{split}\\tag{5}\\(Z_{id}) 및\\(Z_{t}\\)는 각각 생성된 특징 및 의미 특징의 대응하는 공간 특징이다. 매개변수 \\(W_{k}^{id}\\), \\(W_{k}^{t}\\), \\(W_{v}^{id}\\) 및 \\(W_{v}^{t}\\)는 완전히 연결된 층의 가중치에 해당한다.\n' +
      '\n' +
      '**교차-어텐션 병합.** 의미론적 제어를 더욱 정교화하기 위해, 우리는 다음의 공식을 사용하여 교차-어텐션 층들 내의 아이덴티티 피처에 텍스트 피처들을 통합한다:\n' +
      '\n' +
      '\\[\\text{Attn}_{\\text{cross}}(Q,K,V)\\triangleq\\text{Attn}(Q,K_{id}^{\\prime},V_{ id}^{\\prime})+\\text{Attn}(Q,K_{t}^{\\prime},V_{t}^{\\prime}), \\tag{6}\\text{Attn}(Q,K_{id}^{\\prime},V_{t}^{\\prime})\n' +
      '\n' +
      '여기서 \\(K_{id}^{\\prime}=\\hat{W}_{k}^{id}c_{id}\\), \\(V_{id}^{\\prime}=\\hat{W}_{v}c_{id}\\), \\(K_{t}^{\\prime}=\\hat{W}_{k}^{t}c_{t}\\), 및 \\(V_{t}^{\\prime}=\\hat{W}_{v}c_{t}\\). \\(V_{t}^{\\prime}=\\hat{W}_{v}c_{t}\\) (c_{id}\\) 및 \\(c_{t}\\)는 각각 아이덴티티 임베딩 및 텍스트 임베딩이다. \\ (\\hat{W}_{k}^{id}\\), \\(\\hat{W}_{v}^{id}\\), \\(\\hat{W}_{k}^{t}\\), \\(\\hat{W}_{v}^{t}\\)는 교차 주의 모듈 내에서 훈련 가능한 완전 연결 층의 가중치에 해당한다.\n' +
      '\n' +
      '**스타일 정보 병합.** [9]에서 영감을 받아 합성 이미지의 스타일을 스타일 프롬프트와 더 정렬하기 위한 적응적 평균 정규화(AdaIN-mean) 연산을 제안한다. 구체적으로, 우리는 혼합 주의 및 교차 주의 모두에서 아이덴티티 피처로부터 투영된 키 및 값 피처와 텍스트 피처로부터 투영된 키 및 값 피처를 정렬하고, 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\text{AdaIN-m}(K_{id},K_{t}),\\quad V_{id}=\\text{AdaIN-m}(V_{id},V_{t}),\\quad\\text{For Mixed Attention}\\\\K_{id}^{\\prime}&=\\text{AdaIN-m}(K_{id}^{\\prime},K_{t}^{\\prime}),\\quad V_{id}^{\\prime}=\\text{AdaIN-m}(V_{id}^{\\prime},V_{t}^{\\prime}),\\quad\\text{For Cross Attention}\\end{split}\\tag{7}\\text{For Mixed Attention}\\text{AdaIN-m}(K_{id}^{\\prime},K_{t}^{\\prime}),\\quad V_{id}^{\\prime}=\\text{AdaIN-m}(V_{id}^{\\prime},V_\n' +
      '\n' +
      '여기서, AdaIN-mean 연산(AdaIN-m(\\(\\cdot\\)))은 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[\\text{AdaIN-m}(x,y)=x-\\mu(x)+\\mu(y), \\tag{8}\\]\n' +
      '\n' +
      '여기서 \\(\\mu(x)\\in R^{d_{k}}\\)는 서로 다른 픽셀에 걸친 키 및 값 특징의 평균이다. AdaIN-평균과 혼합된 주의가 그림 1에 나와 있다. 3(b).\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '본 논문에서는 Sec. 4.1에서 실험 설정을 설명한 후, Sec. 4.2에서 원시 사진 생성과 스타일 이미지 생성에 대한 비교 분석을 수행하고, 다양한 구성 요소의 중요성을 강조하는 절제 연구를 Sec. 4.3에서 제시하고, 추가 자료의 A.3에서 다중 입력 ID 이미지를 포함하는 실험에 대해 자세히 설명한다. 추가적인 통찰력을 위해 보조 자료의 Sec. A.4와 Sec. A.5는 각각 원시 사진 생성과 스타일 사진 생성에 대한 추가 질적 결과를 제공한다.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '**구현 상세.** 우리의 실험은 미리 훈련된 SDXL(Stable Diffusion XL) 모델[20]을 활용한다. 이미지 인코딩을 위해 OpenCLIP ViT-H/14 [37]과 ArcFace [10]의 백본을 활용한다. SDXL 모델은 70개의 교차 주의 레이어로 구성되며, 각각에 추가 이미지 교차 주의 모듈을 추가한다. 100만 단계 동안 16개의 A100 GPU에 대한 교육이 수행되며 GPU당 배치 크기가 4이다. 학습률은 1e-4, 가중치 감쇠는 0.01로 고정된 AdamW 최적화기를 사용하였으며, 추론 시에는 30단계의 DDIM Sampler를 사용하였고, 학습 데이터는 LAION-2B 데이터세트[30], LAION-Face 데이터세트[41], 인터넷에서 수집한 이미지 등 여러 개의 데이터세트로부터 획득하였다. 우리는 각 개인이 여러 사진으로 표현되는 데이터 세트를 큐레이션한다.\n' +
      '\n' +
      '**평가.** 동일성 충실도와 의미적 일관성을 모두 보존하는 데 있어 접근법의 유효성을 평가한다. 구체적으로, 우리는 \\(M_{\\text{FaceNet}}\\)(FaceNet[28]) 및 CLIP-I[7])과 같은 메트릭을 사용하여 신원 충실도를 측정한다. 동일성 충실도는 기준 이미지와 생성된 이미지 사이의 검출된 얼굴의 유사도에 기초하여 평가된다. 의미적 일관성은 텍스트 프롬프트를 대응하는 생성된 이미지와 비교하는 CLIP 텍스트-이미지 일관성(CLIP-T[21])을 사용하여 정량화된다. 메트릭에 대한 더 많은 정의는 보충 자료의 Sec. A.1에 자세히 설명되어 있다.\n' +
      '\n' +
      '### 이전 방법의 비교\n' +
      '\n' +
      '**Raw Photo Generation.** 우리는 확립된 신원 보존 개인화 접근법에 대해 무한 ID를 벤치마킹한다. 질적 결과는 다음과 같다.\n' +
      '\n' +
      '그림 4: 원시 사진 생성에 대한 **정성적 비교. 그 결과, Infinite-ID는 단일 입력 영상만으로도 일관성 있게 동일성 충실도를 유지하고 높은 품질의 의미 일관성을 달성함을 알 수 있었다.\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 4, 정량적 결과가 표 1에 제공되고 그림 6에 시각적으로 표시된다. 특히, 모든 방법은 튜닝이 없으므로 테스트 시간 조정이 필요하지 않다. FastComposer[35]는 종종 합성 이미지들에서 원하지 않는 아티팩트들을 제시하면서, 아이덴티티 충실도를 유지하는 데 있어서 도전들을 나타낸다. IP-어댑터[36]와 IP-어댑터-페이스[36]는 상대적으로 더 적은 아티팩트를 보여주지만, 그것의 의미적 일관성은 부족하다. 이러한 현상은 U-net 모델 내에서 신원 정보와 의미론적 세부 사항의 직접적인 융합으로 인해 발생하며, 의미론적 일관성의 타협으로 이어진다. 대조적으로, 포토메이커 [16]은 칭찬할 만한 의미 일관성을 나타내지만 정체성 충실도를 보존하는 데 부족하다. 우리의 ID-의미론적 디커플링 패러다임을 활용하여, 우리의 방법은 신원 충실도를 보존하는 데 탁월하다. 또한, 제안된 혼합 주의 메커니즘은 의미 정보를 잡음 제거 과정에 효과적으로 통합함으로써 기존의 기법들에 비해 제안된 방법을 유리하게 포지셔닝한다.\n' +
      '\n' +
      '**스타일 이미지 생성.** 그림에서 스타일화 결과의 결과를 보여준다. 5는 우리의 방법을 최신 튜닝 프리 아이덴티티 보존 개인화 방법과 비교한다. 합성 이미지의 스타일에는 애니메이션 스타일, 만화책 스타일, 라인 아트 스타일이 있습니다. 스타일화 결과에 따르면 IP-어댑터[36] 및 IP-어댑터-페이스는 프롬프트에서 원하는 스타일을 묘사하지 못하며 생성 결과의 스타일은 항상 톤을 준수한다.\n' +
      '\n' +
      '그림 5: **스타일 이미지 생성에 대한 질적 비교.** 결과는 우리의 방법이 단일 참조 이미지만을 사용하여 강한 아이덴티티 충실도, 고품질 의미 일관성 및 정밀한 스타일화를 유지한다는 것을 보여준다.\n' +
      '\n' +
      '참조 이미지. IP-Adapter와 IP-Adapter-Face의 학습 파이프라인은 텍스트 임베딩과 이미지 임베딩을 얽히게 하여 텍스트 임베딩 공간의 왜곡을 초래한다. 패스트컴포저[35] 역시 스타일라이제이션 생성에 실패하여 원하지 않는 아티팩트를 보여준다. 포토메이커[16]는 더 나은 의미 일관성과 양식화를 달성하지만, 아이덴티티 충실도는 여전히 만족스럽지 못하다. 이와는 대조적으로, 우리의 방법은 높은 정체성 충실도, 매력적인 의미 일관성, 그리고 그 동안 정밀한 양식화를 달성한다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '이 섹션에서는 정체성 강화 훈련과 혼합 주의 메커니즘의 영향을 평가하기 위해 절제 연구를 수행하는 것으로 시작한다. 또한, 생성된 이미지의 스타일을 조절하는데 있어서 AdaIN-mean 연산의 효율성을 평가하기 위해 스타일 이미지 생성을 수행한다. 추가 절제 연구는 보충 재료의 Sec. A.2에서 입증된다.\n' +
      '\n' +
      '**신원 강화 훈련의 절제.** 도 7 및 표 1에서, 우리는 우리의 방법을 신원-의미 얽힘 훈련 전략을 사용하여 구현된 "Ours(w/o 신원-강화 훈련)"과 비교한다. 이 전략은 훈련 중에 텍스트-이미지 쌍을 활용하고 원래 U-Net 모델 내에서 텍스트 임베딩을 위한 교차 주의 모듈을 활성화한다. 두 가지 방법 모두 주목할 만하다.\n' +
      '\n' +
      '그림 6: 정량적 비교의 **시각화.** 아이덴티티 충실도는 z-score 알고리즘을 통해 정규화된 CLIP-I 및 \\(M_{\\text{FaceNet}}\\) 점수의 평균으로 표시되는 생성 이미지가 아이덴티티를 얼마나 정확하게 보존하는지 나타낸다. 한편 CLIP-T 점수로 측정한 의미 일관성은 생성된 이미지와 제공된 텍스트 프롬프트 간의 일관성을 평가한다. 점수가 높을수록 동일성 충실도와 의미적 일관성이 우수함을 나타낸다. 비교된 방법은 IP-Adapter, IP-Adapter-Face[36], FastComposer[35], PhotoMaker[16], 그리고 w/o identity-enhanced training(Ours-1), w/o mixed attention(Ours-2) 및 mixed attention\\(\\Rightarrow\\) mutual attention(Ours-3)을 포함한 본 방법의 절제 버전들을 포함한다.\n' +
      '\n' +
      '동일한 추론 처리를 공유한다. 질적 비교는 우리의 정체성 강화 훈련이 정체성 충실도를 현저하게 향상시킨다는 것을 보여준다.\n' +
      '\n' +
      '**혼합 주의 메커니즘의 절제.** 제안된 혼합 주의(M-A) 메커니즘의 효과를 평가하기 위해 그림의 "Ours(w/o M-A)" 및 "Ours(M-A\\(\\Rightarrow\\) MU-A)"와 방법을 비교한다. 도 7 및 표 1. 상호 셀프 어텐션(MU-A)[4]은 기존의 셀프 어텐션을 일관된 이미지 편집을 위한 \'크로스 어텐션\'으로 변환하는데, 여기서 크로싱 동작이 발생한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline  & CLIP-T \\(\\uparrow\\) & CLIP-I \\(\\uparrow\\) & \\(M_{\\text{FaceNet}}\\uparrow\\) \\\\ \\hline FastComposer [35] & 0.292 & 0.887 & 0.556 \\\\ \\hline IP-Adapter [36] & 0.274 & 0.905 & 0.474 \\\\ \\hline IP-Adapter-Face [36] & 0.313 & **0.919** & 0.513 \\\\ \\hline PhotoMaker [16] & **0.343** & 0.814 & 0.502 \\\\ \\hline Ours & 0.340 & 0.913 & **0.689** \\\\ Ours (w/o identity-enhanced training) & 0.329 & 0.891 & 0.593 \\\\ \\hline Ours (w/o Mixed Attention) & 0.331 & 0.905 & 0.700 \\\\ \\hline Ours (Mixed Attention \\(\\Rightarrow\\) Mutual Attention) & 0.316 & 0.808 & 0.398 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **정량적 비교.** 평가 메트릭은 CLIP-T, CLIP-I, \\(M_{\\text{FaceNet}}\\)을 포괄한다. 우리의 접근 방식은 만족스러운 의미 일관성을 달성하는 동시에 동일성 충실도 측면에서 다른 방법을 능가한다. 가장 좋은 결과는 **굵은**로 표시되며, 두 번째로 좋은 것은 밑줄이 그어져 있습니다. 또한, 절제 연구의 정량적 비교는 회색 부분에서 나타났다.\n' +
      '\n' +
      '도 7: **우리의 정체성 강화 훈련 및 혼합 주의력(M-A)의 절제 연구.** 정체성 강화 훈련은 정체성 충실도를 상당히 향상시키고, 혼합 주의력 메커니즘은 상호 주의력(MU-A) 접근법에 비해 의미론적 일관성을 향상시킴[4]이 명백하다.\n' +
      '\n' +
      '두 가지 관련 확산 과정의 자기 주의. 그 결과, 제안된 혼합 주의 메커니즘은 동일성 충실도를 유지하면서 의미 일관성을 향상시키는 우수한 능력을 보여준다.\n' +
      '\n' +
      '본 논문에서 제안한 AdaIN-mean 연산의 효율성을 평가하기 위해 그림 8과 같이 Infinite-ID 모델을 Ours(w/o AdaIN-mean)와 Ours(AdaIN-mean \\(\\Rightarrow\\) AdaIN)의 변화와 비교한다. 그 결과 i) Ours(w/o AdaIN-mean)는 Infinite-ID에 비해 우수한 ID 충실도를 나타내지만 텍스트 프롬프트와 스타일 일관성을 달성하지 못하며, ii) AdaIN-mean과 AdaIN 모듈 모두 성공적으로 텍스트 프롬프트와 스타일 일관성을 달성했지만 AdaIN-mean은 AdaIN보다 더 나은 ID 충실도를 유지한다. 결론적으로, 제안된 AdaIN-mean 연산은 ID 충실도를 동시에 유지하면서 정밀한 스타일화를 용이하게 한다.\n' +
      '\n' +
      '##5 결론 및 한계.\n' +
      '\n' +
      '본 논문에서는 텍스트 프롬프트의 아이덴티티(ID) 충실도 및 의미적 일관성의 요건을 충족하도록 설계된 혁신적인 아이덴티티 보존 개인화 기법인 Infinite-ID를 소개하며, 모두 하나의 참조 이미지만으로 달성 가능하고 몇 초 이내에 완료된다. 무한 ID는 신원 강화 훈련, 혼합 주의 메커니즘 및 적응 평균 정규화(AdaIN-mean)의 세 가지 핵심 구성 요소로 구성된다. 광범위한 실험을 통해 무한 ID가 기본 방법보다 우수한 ID 충실도, 우수한 생성 품질, 그리고 미가공 사진 생성과 스타일 이미지 생성 작업 모두에서 정확한 의미 일관성을 제공한다는 것을 보여준다. 그러나, 우리의 방법은 다중 객체 개인화 능력이 부족하다는 점에 유의하는 것이 중요하다. 더욱이, 인간 얼굴이 원래의 확산 모델에 내재된 제한에 기인하여 전체 이미지의 작은 부분만을 점유할 때 아티팩트가 발생할 수 있다.\n' +
      '\n' +
      '그림 8: **AdaIN-mean 동작에 대한 절제 연구.** 결과는 AdaIN-mean이 스타일 이미지 생성에 중요한 역할을 한다는 것을 보여준다. AdaIN[9] 모듈과 비교하여, 우리의 AdaIN-mean은 더 높은 아이덴티티 충실도를 달성하는 데 도움이 된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Achlioptas, P., Benetatos, A., Fostiropoulos, I., Skourtis, D.: Stellar: Systematic evaluation of human-centric personalized text-to-image methods. arXiv preprint arXiv:2312.06116 (2023)\n' +
      '* [2] Avrahami, O., Aherman, K., Fried, O., Cohen-Or, D., Lischinski, D.: Break-a-scene: Extracting multiple concepts from a single image. arXiv preprint arXiv:2305.16311 (2023)\n' +
      '* [3] Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al.: Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf **2**, 3 (2023)\n' +
      '* [4] Cao, M., Wang, X., Qi, Z., Shan, Y., Qie, X., Zheng, Y.: Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465 (2023)\n' +
      '* [5] Chen, L., Zhao, M., Liu, Y., Ding, M., Song, Y., Wang, S., Wang, X., Yang, H., Liu, J., Du, K., et al.: Photoverse: Tuning-free image customization with text-to-image diffusion models. arXiv preprint arXiv:2309.05793 (2023)\n' +
      '* [6] Chen, X., Huang, L., Liu, Y., Shen, Y., Zhao, D., Zhao, H.: Anydoor: Zero-shot object-level image customization. arXiv preprint arXiv:2307.09481 (2023)\n' +
      '* [7] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.: An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022)\n' +
      '* [8] Gal, R., Arar, M., Atzmon, Y., Bermano, A.H., Chechik, G., Cohen-Or, D.: Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG) **42**(4), 1-13 (2023)\n' +
      '* [9] Hertz, A., Voynov, A., Fruchter, S., Cohen-Or, D.: Style aligned image generation via shared attention. arXiv preprint arXiv:2312.02133 (2023)\n' +
      '* [10] Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance normalization. In: 2017 IEEE International Conference on Computer Vision (ICCV) (Oct 2017). [https://doi.org/10.1109/iccv.2017.167](https://doi.org/10.1109/iccv.2017.167), [http://dx.doi.org/10.1109/iccv.2017.167](http://dx.doi.org/10.1109/iccv.2017.167)\n' +
      '* [11] Inoue, N., Kikuchi, K., Simo-Serra, E., Otani, M., Yamaguchi, K.: Layoutdm: Discrete diffusion model for controllable layout generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10167-10176 (2023)\n' +
      '* [12] Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani, M.: Imagic: Text-based real image editing with diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6007-6017 (2023)\n' +
      '* [13] Khachatryan, L., Movsisyan, A., Tadevosyan, V., Henschel, R., Wang, Z., Navasardyan, S., Shi, H.: Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439 (2023)\n' +
      '* [14] Kumari, N., Zhang, B., Zhang, R., Shechtman, E., Zhu, J.Y.: Multi-concept customization of text-to-image diffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1931-1941 (2023)\n' +
      '* [15] Li, D., Li, J., Hoi, S.C.: Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. arXiv preprint arXiv:2305.14720 (2023)\n' +
      '* [16] Li, Z., Cao, M., Wang, X., Qi, Z., Cheng, M.M., Shan, Y.: Photomaker: Customizing realistic human photos via stacked id embedding. arXiv preprint arXiv:2312.04461 (2023)* [17] Li, Z., Wang, C., Zheng, H., Zhang, J., Li, B.: Fakeclr: Exploring contrastive learning for solving latent discontinuity in data-efficient gans. In: European Conference on Computer Vision. pp. 598-615. Springer (2022)\n' +
      '* [18] Ma, J., Liang, J., Chen, C., Lu, H.: Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. arXiv preprint arXiv:2307.11410 (2023)\n' +
      '* [19] Peng, X., Zhu, J., Jiang, B., Tai, Y., Luo, D., Zhang, J., Lin, W., Jin, T., Wang, C., Ji, R.: Portraitbooth: A versatile portrait model for fast identity-preserved personalization. arXiv preprint arXiv:2312.06354 (2023)\n' +
      '* [20] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023)\n' +
      '* [21] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* arXiv, 코넬대학교\n' +
      '- arXiv (Feb 2021)\n' +
      '* [23] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research **21**(1), 5485-5551 (2020)\n' +
      '* [24] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [25] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. pp. 234-241. Springer (2015)\n' +
      '* [26] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22500-22510 (2023)\n' +
      '* [27] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems **35**, 36479-36494 (2022)\n' +
      '* [28] Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face recognition and clustering. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 815-823 (2015)\n' +
      '* [29] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.: Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems **35**, 25278-25294 (2022)\n' +
      '* [30] Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., Komatsuzaki, A.: Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114 (2021)\n' +
      '* [31] Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.: Zero123++: a single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110 (2023)* [32] Wei, Y., Zhang, Y., Ji, Z., Bai, J., Zhang, L., Zuo, W.: Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848 (2023)\n' +
      '* [33] Wu, J.Z., Ge, Y., Wang, X., Lei, S.W., Gu, Y., Shi, Y., Hsu, W., Shan, Y., Qie, X., Shou, M.Z.: Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7623-7633 (2023)\n' +
      '* [34] Wu, Y., Li, Z., Wang, C., Zheng, H., Zhao, S., Li, B., Tao, D.: Domain remodulation for few-shot generative domain adaptation. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [35] Xiao, G., Yin, T., Freeman, W.T., Durand, F., Han, S.: Fastcomposer: Tuning-free multi-subject image generation with localized attention. arXiv preprint arXiv:2305.10431 (2023)\n' +
      '* [36] Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023)\n' +
      '* [37] Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P., Riquelme, C., Lucic, M., Djolonga, J., Pinto, A.S., Neumann, M., Dosovitskiy, A., et al.: A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867 (2019)\n' +
      '* [38] Zhang, C., Zhang, C., Zhang, M., Kweon, I.S.: Text-to-image diffusion model in generative ai: A survey. arXiv preprint arXiv:2303.07909 (2023)\n' +
      '* [39] Zhang, K., Zhang, Z., Li, Z., Qiao, Y.: Joint face detection and alignment using multitask cascaded convolutional networks. IEEE signal processing letters **23**(10), 1499-1503 (2016)\n' +
      '* [40] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3836-3847 (2023)\n' +
      '* [41] Zheng, Y., Yang, H., Zhang, T., Bao, J., Chen, D., Huang, Y., Yuan, L., Chen, D., Zeng, M., Wen, F.: General facial representation learning in a visual-linguistic manner. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18697-18709 (2022)Supplementary Materials\n' +
      '\n' +
      '### Evaluation metrics\n' +
      '\n' +
      '얼굴 유사도 평가.** 얼굴 유사도를 평가하기 위해 얼굴 정렬 모듈\\(\\text{FA}(\\cdot)\\), 얼굴 인식 백본\\(\\text{E}_{\\text{face}}(\\cdot)\\), CLIP 영상 인코더\\(\\text{E}_{\\text{clip}}(\\cdot)\\)을 사용하여 메트릭스\\(M_{\\text{FaceNet}}) 및 CLIP-I를 계산한다. 구체적으로, 생성된 이미지\\(\\text{I}_{\\text{gen}\\)와 그에 대응하는 아이덴티티 이미지\\(\\text{I}_{\\text{id}\\)에 대해, 먼저 얼굴 검출을 위해 \\(\\text{FA}(\\cdot)\\) 모듈을 사용한다. 이어서, 각각 \\(\\text{E}_{\\text{face}}(\\cdot)\\)와 \\(\\text{E}_{\\text{clip}}(\\cdot)\\)을 이용하여 쌍별 동일성 유사도를 계산한다.\n' +
      '\n' +
      '[\\begin{split}M_{\\text{face}}(\\text{FA}(\\text{I}_{\\text{gen}})),\\text{E}_{\\text{face}(\\text{FA}(\\text{I}_{id}})),\\text{CLIP-I}=cos(\\text{E}_{\\text{clip}(\\text{FA}(\\text{I}_{gen}})),\\text{E}_{\\text{clip}}(\\text{FA}(\\text{I}_{\\text{id}})),\\end{split}\\tag{9}}}\n' +
      '\n' +
      '여기서 \\(cos(\\cdot,\\cdot)\\)는 코사인 유사도 함수이다. 또한, 본 논문의 그림 6에 묘사된 아이덴티티 충실도를 설명하기 위해 z-score 정규화를 활용하여 \\(M_{\\text{FaceNet}}\\)과 CLIP-I를 통합한다:\n' +
      '\n' +
      '\\[mean(\\text{z-score}(M_{\\text{FaceNet}}),\\text{z-score}(\\text{CLIP-I})), \\tag{10}\\]\n' +
      '\n' +
      '여기서 \\(\\text{z-score}(x)=(x-\\mu)/\\sigma\\), \\(\\mu\\) 및 \\(\\sigma\\)은 각각 \\(x\\)의 평균 및 표준 편차이다.\n' +
      '\n' +
      '**의미 일관성의 정의.**의미 일관성을 평가하기 위해 CLIP-T 메트릭을 채택한다. 구체적으로, 해당 프롬프트와 쌍을 이루는 생성된 이미지\\(I_{\\text{gen}}\\)에 대해, CLIP 이미지 인코더\\(E_{\\text{clip}}\\) 및 CLIP 텍스트 인코더\\(E_{\\text{text}}\\)을 모두 사용하여 CLIP-T 메트릭을 계산한다:\n' +
      '\n' +
      '\\[\\text{CLIP-T}=cos(E_{\\text{clip}}(I_{gen}),E_{\\text{text}}(P)), \\tag{11}\\]\n' +
      '\n' +
      '여기서 \\(cos(\\cdot,\\cdot)\\)는 코사인 유사성 함수이다.\n' +
      '\n' +
      '### Ablation Study의 결과\n' +
      '\n' +
      '**교차 주의 병합의 절제 연구** 교차 주의 병합에 대한 절제 실험을 수행하여 그 효과를 평가한다. 도 1에 도시된 바와 같다. 도 9 및 표 2를 참조하면, 교차-어텐션 병합의 통합은 의미 일관성의 개선을 보여준다.\n' +
      '\n' +
      '**입력 ID 이미지의 해상도에 대한 절제 연구** 입력 ID 이미지의 해상도에 대한 절제 연구를 수행하여 본 방법의 견고성을 평가한다. 구체적으로, 우리는 개인화를 위해 동일한 텍스트 프롬프트를 유지하면서 다양한 해상도의 이미지를 활용한다. 도 1에 도시된 바와 같다. 도 10을 참조하면, 동일성 충실도는 이미지 해상도가 감소함에 따라 미미한 감소만을 나타내는 반면, 의미적 일관성은 모든 해상도에 걸쳐 안정적으로 유지된다. 결론적으로, 본 논문에서 제안하는 방법은 입력 영상의 해상도 변화에 강인함을 보인다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline  & CLIP-T\\(\\uparrow\\) & CLIP-I\\(\\uparrow\\) & \\(M_{\\text{FaceNet}}\\uparrow\\) \\\\ \\hline Ours w/o Cross-attention merge & 0.335 & 0.910 & 0.681 \\\\ \\hline Ours & **0.340** & **0.913** & **0.689** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **교차-주의 병합의 정량적 절제.** 메트릭은 의미적 일관성을 측정하는 CLIP-T(높을수록 좋다), 동일성 충실도를 모두 반영하는 CLIP-I(높을수록 좋다) 및 \\(M_{\\text{FaceNet}}\\)(높을수록 좋다)를 포함한다. 가장 좋은 결과는 **bold**로 표시됩니다.\n' +
      '\n' +
      '그림 10: **입력 ID 이미지의 해상도**에 대한 절제 연구. 동일성 충실도는 낮은 이미지 해상도와 함께 약간 떨어지고 의미적 일관성은 모든 해상도에 대해 안정적이다. 제안하는 방법은 입력 ID 영상의 해상도에 강인하다.\n' +
      '\n' +
      '도 9: **교차-어텐션 병합의 정량적 절제.**교차-어텐션 병합이 의미 일관성 향상에 도움이 되는 것은 자명하다.\n' +
      '\n' +
      '### Identity mixing\n' +
      '\n' +
      '별개의 개인으로부터 여러 이미지를 수신하면 그림 1에 표시된 대로 해당 ID를 병합하기 위해 모든 ID 임베딩을 적층한다. 11. 생성된 이미지는 상이한 ID들의 특성들을 잘 유지할 수 있고, 이는 더 많은 애플리케이션들에 대한 가능성들을 방출한다. 또한, 아이덴티티 임베딩의 보간을 조정함으로써 그림 1에서 입증된 바와 같이 생성된 아이덴티티와 다른 입력 아이덴티티 사이의 유사성을 조절할 수 있다. 12.\n' +
      '\n' +
      '### 원시 사진 생성의 보다 질적인 결과\n' +
      '\n' +
      '도. 도 13은 개인화 목적으로 아이덴티티를 보존하면서 미술품으로부터 아이덴티티 정보를 추출하는 우리의 방법의 능력을 보여준다. 추가적으로, 도 1을 참조하여 설명한다. 도 14는 원시 사진 생성을 위해 추출된 아이덴티티의 속성을 변경하는 우리의 방법의 능력을 도시한다. 원시 사진 생성을 위한 추가 시각적 샘플이 그림 1에 나와 있다. 도 15 및 도 1을 참조하여 설명한다. 도 16은 다양한 인종, 피부색 및 성별에 걸쳐 FFHQ 데이터 세트에서 샘플링된 일반 개인의 신원을 보여준다.\n' +
      '\n' +
      '도 11: ** 아이덴티티 혼합. 서로 다른 개인으로부터 여러 개의 입력 ID 이미지를 수신할 때, 우리의 방법은 모든 ID 임베딩을 적층하여 이러한 ID를 혼합할 수 있다.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:21]\n' +
      '\n' +
      '그림 14: 속성 변경에 대한 응용 프로그램.\n' +
      '\n' +
      '그림 13: 원시 사진에 대한 미술품에 대한 응용 프로그램.\n' +
      '\n' +
      '그림 15: **원시 사진 생성.** 이러한 신원은 다양한 인종, 피부색, 남성과 여성을 포함하여 FFHQ 데이터 세트에서 샘플링된 일반인이다.\n' +
      '\n' +
      '그림 16: **원시 사진 생성.** 이러한 ID는 다양한 인종, 피부색, 남성과 여성을 포함하여 FFHQ 데이터 세트에서 샘플링된 일반인이다.\n' +
      '\n' +
      '그림 17: **스타일화를 위한 더 많은 시각적 예.** 이러한 아이덴티티는 다양한 인종, 피부색, 남성과 여성을 포함하여 FFHQ 데이터 세트에서 샘플링된 일반인이다.\n' +
      '\n' +
      '그림 18: **스타일화를 위한 더 많은 시각적 예.** 이러한 아이덴티티는 다양한 인종, 피부색, 남성과 여성을 포함하여 FFHQ 데이터 세트에서 샘플링된 일반인이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>