<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '그림 1: 샌프란시스코에서 걷는 **휴머노이드.** 우리는 일주일 동안 샌프란시스코의 다양한 위치에 정책을 배포한다. 비디오는 프로젝트 페이지를 참조하십시오. 우리는 우리의 정책이 보도, 콘크리트, 아스팔트, 타일형 광장, 샌딩 도로를 포함한 다양한 표면을 걸을 수 있음을 보여줍니다. 우리는 우리의 정책이 전방위 속도 명령을 잘 따르고 샌프란시스코와 같은 어려운 도시 환경에서 배치를 가능하게 한다는 것을 발견했다.\n' +
      '\n' +
      ' 우리의 방법을 검증하기 위해 실제 휴머노이드 운동의 도전적인 작업에 적용한다. 우리는 민첩성 로보틱스가 개발한 풀사이즈 디지트 휴머노이드 로봇을 사용합니다. 우리는 먼저 시뮬레이션에서 센서 모터 궤적의 데이터 세트를 수집한다. 여기에는 강화 학습[33]으로 훈련된 신경망 정책으로부터의 완전한 궤적뿐만 아니라 (i) 모델 예측 제어에 기초한 민첩성 로봇 제어기, (ii) 인간의 모션 캡처 및 (iii) 인간의 유튜브 비디오의 세 가지 다른 소스로부터의 불완전한 궤적이 포함된다. 우리는 컴퓨터 비전 기술을 사용하여 인간 비디오를 재구성하고 역 운동학을 통해 모션 캡처와 유튜브 궤적을 모두 리타겟팅한다. 그런 다음 트랜스포머 모델을 학습하여 궤적을 자동으로 예측합니다. 테스트 시간에 우리는 동작을 자동으로 실행하고 감각 예측을 무시한다.\n' +
      '\n' +
      '우리는 우리의 정책이 현실 세계 제로샷에 배치되고 다른 표면에서 걸을 수 있음을 보여준다. 특히, 일주일 동안 샌프란시스코의 다양한 위치에 모델을 배치하십시오. 예제는 그림 1을 참조하고 동영상은 프로젝트 페이지를 참조하십시오. 우리의 접근법의 다양한 측면을 정량적으로 평가하기 위해 시뮬레이션에서 광범위한 연구를 수행한다. 오프라인 데이터만으로 훈련된 자기회귀 정책은 테스트된 환경에서 강화 학습[33]을 사용하는 최첨단 접근법과 비슷하다는 것을 발견했다. 우리는 또한 우리의 접근법이 불완전한 궤적으로부터 쉽게 이익을 얻을 수 있고 유리한 스케일링 속성을 가지고 있다는 것을 발견했다.\n' +
      '\n' +
      '이러한 결과는 센서모터 궤적의 대규모 모음을 생성 모델링하여 실제 로봇 제어 작업에 도전하는 학습을 위한 유망한 경로를 제안한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**Generative Modeling.** 데이터의 연구는 Shannon의 기초 작업[37]에서 최근 대형 언어 모델의 시대에 이르기까지 광범위했다. 지난 10년 동안 다양한 그러한 모델들이 등장했다. 이러한 주목할 만한 모델은, 픽셀을 생성하기 위한 GAN[12] 및 확산 모델[39, 16], 언어 토큰을 생성하기 위한 LSTM[17] 및 GPT[29]를 포함한다. 이러한 모델은 다른 양식에도 채택되었다[27, 11, 43]. 이 중 자기회귀 변압기 모델은 인상적인 스케일링 거동[19]과 상황 내 예제[3]로부터 학습할 수 있는 능력으로 인해 선두 주자가 되었다. 이 동작은 심지어 픽셀[6], 언어-픽셀[36] 및 언어-픽셀-오디오[21]와 같은 다른 모달리티까지 확장되는 것으로 나타난다. 우리는 실제 휴머노이드 운동의 맥락에서 자기회귀 생성 모델을 탐구한다.\n' +
      '\n' +
      '**로보틱스에서의 트랜스포머.** 자연어 처리[29, 8, 30, 3] 및 컴퓨터 비전[9, 13]에서 트랜스포머 모델[42]의 성공에 따라, 지난 몇 년 동안, 로보틱스에서의 트랜스포머 모델 사용에 대한 관심이 증가하였다. 우리는 변압기가 행동 복제에 효과적일 수 있다는 것을 보여주는 여러 작품을 보았다. 예를 들어, [38]은 언어로 다중 작업 트랜스포머 정책을 학습하고, [2]는 대규모 데이터로부터 언어 조건 조작 정책을 학습한다. [10] 구현된 데이터로 언어 모델을 훈련시킵니다. 또한 변압기 정책이 대규모 강화 학습에 효과적일 수 있음을 보았다[33]. [32]. 가면을 쓴 예측으로 감각 운동 표현을 배우다. [1] 목표 조건을 갖춘 기차는 시위에서 배운다. 마찬가지로, 우리는 로봇 공학에 트랜스포머 모델을 사용하는 목표를 공유하지만 실제 휴머노이드 운동을 위한 다양한 궤적의 자동 회귀 모델링에 중점을 둔다.\n' +
      '\n' +
      '**휴머노이드 운동** 로봇이 걸을 수 있는 능력을 마스터하는 것은 로봇 공학에서 오랜 도전이었습니다. 지난 수십 년 동안, 로봇 공학자들은 인간과 같은 운동 기술을 탐구하기 위해 다양한 휴머노이드 로봇[20, 15, 26, 40, 7]을 만들었습니다. 안정적인 운동 거동은 모델 기반 제어 접근법[34, 18]을 통해 달성되었으며, 최적화 기반 방법은 매우 역동적인 휴머노이드 운동을 더욱 가능하게 한다[22]. 이러한 전략으로 상당한 진전이 있었고 이를 학습과 결합[5]했지만, 학습 기반 접근법은 광범위한 환경을 개선하고 적응하는 능력으로 인해 주목을 받고 있다. 최근에 우리는 시뮬레이션에서 대규모 강화 학습으로 훈련된 순수 학습 기반 접근법이 실제 휴머노이드 운동을 가능하게 할 수 있음을 보았다[33]. 이전 작업과 마찬가지로, 우리의 모델은 인과변압기이다. 선행 연구와 달리 강화학습 대신 자기회귀 모델링을 수행한다.\n' +
      '\n' +
      '## 3 Approach\n' +
      '\n' +
      '이 절에서는 센서모터 궤적의 데이터세트 \\(\\mathcal{D}\\)를 가정하고 아래에 우리의 접근법을 설명한다.\n' +
      '\n' +
      '### Objective\n' +
      '\n' +
      '각 감각 운동 궤적은 감각 관찰과 행동의 순서이다. \\(\\mathcal{T}=(o_{1},a_{1},o_{2},a_{2},...,o_{T},a_{T})\\. 우리는 먼저 궤적을 K개의 토큰으로 토큰화하여 \\(t=(t_{1},t_{2},t_{3},...,t_{K})\\)을 얻는다. 우리의 목표는 밀도 함수 \\(p(t)\\)를 자동으로 모델링하기 위해 신경망을 훈련시키는 것이다:\n' +
      '\n' +
      '\\[p(t)=\\prod_{k=1}^{K}p(t_{k}|t_{k-1},...,t_{1}) \\tag{1}\\]\n' +
      '\n' +
      '우리는 궤적 데이터 세트에 대한 음의 로그 우도를 최소화하여 모델을 훈련한다.\n' +
      '\n' +
      '우리는 일정한 분산을 갖는 가우시안 분포를 가정하고 예측 토큰과 지상진리 토큰 사이의 평균 제곱 오차를 최소화하기 위해 신경망을 학습한다:\n' +
      '\n' +
      '\\[L=\\frac{1}{K}\\sum_{k=1}^{K}(\\widehat{t}_{k}-t_{k})^{2} \\tag{3}\\]\n' +
      '\n' +
      '원시 토큰 값을 회귀하는 대신 각 차원을 빈으로 양자화하거나 벡터 양자화를 수행할 수 있다. 그러나 우리는 회귀 접근법이 실제로 합리적으로 잘 작동한다는 것을 발견했고 단순함을 위해 선택했다.\n' +
      '\n' +
      '### Missing modalities\n' +
      '\n' +
      '지금까지 논의에서 우리는 각 궤적이 관찰과 행동의 연속이라고 가정했다. 다음으로, 우리는 우리의 프레임워크가 동작이 없는 인간 비디오에서 추출된 궤적과 같이 모달리티가 누락된 시퀀스로 일반화될 수 있는 방법을 보여준다. 우리가 행동\\(\\mathcal{T}=(o_{1},o_{2},...,o_{T})\\)이 없는 관측 궤적이 주어졌다고 가정하자. 우리의 핵심 통찰력은 행동이 가려진 상태에서 규칙적인 궤적처럼 행동 없이 궤적을 다룰 수 있다는 것이다. 즉, 마스크 토큰[M]을 삽입하여 \\(\\mathcal{T}=(o_{1},\\,[\\texttt{M}]\\,o_{2},\\,[\\texttt{M}]\\,...,o_{T},\\,[\\texttt{M}]\\,)\\을 얻을 수 있다. 이 궤적은 이제 우리의 정규 궤적과 동일한 형식을 가지므로 통일된 방식으로 처리될 수 있다. 우리는 입력의 마스킹된 부분에 해당하는 예측에 대한 손실을 무시한다. 이 원칙은 액션에 국한되지 않으며 다른 양식에도 적용된다는 점에 유의해야 한다.\n' +
      '\n' +
      '### Aligned prediction\n' +
      '\n' +
      '다음 토큰을 모달리티 진단 방식으로 예측하기보다는 모달리티 정렬 방식으로 예측한다. 즉, 각 입력 토큰에 대해 _same_ modality의 다음 토큰을 예측한다. 도표는 그림 3을 참조하십시오.\n' +
      '\n' +
      '### Joint training\n' +
      '\n' +
      '소음 수준 또는 양식 하위 집합 측면에서 다양한 궤적을 포함하는 컬렉션에 대한 교육을 위한 두 가지 옵션이 있습니다. 우리는 완전한 궤적과 불완전한 궤적을 포함하여 모든 데이터와 동시에 공동으로 훈련할 수 있다. 또는 먼저 시끄럽고 불완전한 궤적에 대해 사전 훈련을 할 수 있다. 이는 완전한 궤적에 대한 학습을 위한 좋은 초기화를 제공하는 것으로 볼 수 있다. 우리는 두 접근법 모두 우리의 환경에서 비교적 잘 작동하고 단순화를 위해 대부분의 실험에서 공동 훈련을 선택한다는 것을 발견했다.\n' +
      '\n' +
      '### Model architecture\n' +
      '\n' +
      '우리 모델은 바닐라 변압기[42]이다. 완전한 데이터 또는 불완전한 데이터의 궤적을 감안할 때 먼저 궤적을 토큰으로 토큰화한다. 우리는 각 촬영장비에 대해 별도의 선형 투영 레이어를 학습하지만 시간에 따라 공유한다. 시간 정보를 인코딩하기 위해 우리는 위치 임베딩을 사용한다. \\(o_{i}\\in\\mathcal{R}^{m}\\) 및 \\(a_{i}\\in\\mathcal{R}^{n}\\)을 가정하자.\n' +
      '\n' +
      '\\[t_{i} =\\texttt{concat}(o_{i},a_{i}), \\tag{4}\\] \\[h_{i}^{0} =Wt_{i}, \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(W\\in\\mathcal{R}^{d\\times(m+n)}\\)은 연속 관측 및 작용 양식을 \\(d\\)차원 임베딩 벡터에 투영하기 위한 선형 투영 층이다. 위 첨자 \\(0\\)은 \\(0\\)번째 레이어, _i.e._ 입력 레이어에서의 임베딩을 나타낸다. 액션을 사용할 수 없을 때, 우리는 마스크 토큰 [M]\\(\\in\\mathcal{R}^{n}\\)을 사용하여 \\(a_{i}\\)을 대체하고, [M]을 랜덤 벡터로 초기화하고 전체 모델과 종단간 학습을 한다. 모델은 임베딩 벡터(H_{0}=\\{h_{1}^{0},h_{2}^{0},...,h_{t}^{0}\\})의 시퀀스를 입력으로 한다.\n' +
      '\n' +
      '변압기 구조는 다중 헤드 자기 주의 모듈과 MLP 모듈로 구성된 \\(L\\) 레이어를 포함한다. 층 \\(l\\)의 출력을 \\(H_{l}\\)이라 가정하면,\n' +
      '\n' +
      '그림 2: **Humanoid locomotion as next token prediction. 우리는 신경망 정책, 모델 기반 컨트롤러, 인간 모션 캡처 및 인간의 유튜브 비디오와 같은 다양한 소스에서 궤적에 대한 데이터 세트를 수집한다. 그런 다음 이 데이터 세트를 사용하여 관찰 및 작업의 자동 회귀 모델링을 통해 변압기 정책을 훈련한다. 저희 변압기는 휴머노이드가 샌프란시스코 주변의 다양한 지형에서 제로 샷을 걸을 수 있도록 합니다. 비디오 결과는 프로젝트 페이지를 참조하십시오.**\n' +
      '\n' +
      '층 \\(l+1\\) 출력은 다음과 같이 계산된다:\n' +
      '\n' +
      '\\texttt{LayerNorm}(H_{l}} =\\texttt{LayerNorm}(H_{l}) \\tag{6}\\] \\tilde{H_{l}} =\\tilde{H_{l}}+MHSA(\\tilde{H_{l}})\\] (7) \\[H_{l+1} =\\tilde{H_{l}}+MLP(\\tilde{H_{l}}) \\tag{8}\\]\n' +
      '\n' +
      '여기서, 다중-헤드 자기-어텐션은 인과 마스킹을 가지며, 여기서 토큰은 오직 자신 및 과거 토큰에만 참석한다. 모든 레이어를 통해 토큰이 처리되면 선형 투영 레이어\\(\\widehat{W}\\in\\mathcal{R}^{(m+n)\\times d}\\)을 학습하여 예측된 상태 및 동작에 임베딩을 투영한다:\n' +
      '\n' +
      '\\widehat{t}_{i+1} =\\widehat{W}h_{i}^{L}\\tag{9}\\] \\[\\widehat{o}_{i+1} =(\\widehat{t}_{i+1})_{0:m}\\] (10) \\[\\widehat{a}_{i+1} =(\\widehat{t}_{i+1})_{m:(m+n)} \\tag{11}\\]\n' +
      '\n' +
      '그리고 (3)의 목적을 가지고 변압기를 훈련시킨다. 토큰이 마스킹된 경우에는 손실을 적용하지 않습니다. 우리는 그림 3과 같이 두 가지 유형의 데이터로 변압기를 훈련한다. 이를 통해 다양한 데이터 소스를 사용할 수 있으므로 데이터 측면에서 스케일링이 가능하다.\n' +
      '\n' +
      '### Model inference\n' +
      '\n' +
      '추론 시간에, 우리의 트랜스포머 모델은 항상 관찰-행동 쌍에 액세스할 수 있다. 이 설정에서는 각 관측-액션 쌍 토큰에 대해 변압기 모델을 자동으로 적용합니다. 과거 관찰 및 액션에 대한 조건화를 통해 다음 액션(또는 관찰-액션 쌍)을 예측하고 액션을 실행합니다. 그런 다음 로봇에서 관측치를 가져온 다음 예측된 관측치를 폐기합니다. 관찰된 관찰 및 예측된 액션을 다음 토큰 집합으로 사용하고 과거 쌍과 연결하여 다음 관찰-액션 쌍을 예측한다.\n' +
      '\n' +
      '## 4 Dataset\n' +
      '\n' +
      '우리의 접근 방식은 모델을 훈련하기 위해 궤적 데이터 세트를 구축하는 데 동기를 부여한다. 데이터 세트에는 (i) 신경망 정책, (ii) 모델 기반 컨트롤러, (iii) 인간 모션 캡처 및 (iv) 유튜브의 인간 비디오와 같은 다양한 소스로부터의 궤적이 포함된다. 서로 다른 데이터 소스의 그림이 그림 4에 나와 있습니다. 다음에는 각각을 차례로 설명합니다.\n' +
      '\n' +
      '### 신경회로망 궤적\n' +
      '\n' +
      '훈련 궤적의 첫 번째 원천으로 대규모 강화 학습으로 훈련된 신경망 정책을 사용한다(33). 특히, 이 정책은 아이작 짐(25)의 수천 개의 무작위 환경에서 수십억 개의 샘플로 훈련되었다. 이 정책을 민첩성 로보틱스의 시뮬레이터에서 실행하고 도메인 랜덤화 없이 평지에서 각각 10s의 10k 궤적을 수집한다. 각 궤적은 직선속도 전진 \\([0.0,1.0]\\) m/s, 직선속도 측면 \\([-0.5,0.5]\\) m/s, 선회각속도 \\([-0.5,0.5]\\) rad/s와 같이 클리핑된 정규분포로부터 샘플링된 속도지령에 따라 조절된다.\n' +
      '\n' +
      '데이터 생성 정책에 액세스할 수 있기 때문에 모델이 예측한 정확한 조치뿐만 아니라 완전한 관찰을 기록할 수 있습니다. 우리는 이 세트를 지상 진실 동작뿐만 아니라 완전한 관찰을 갖는 완전한 감각 운동 궤적의 소스로 사용한다.\n' +
      '\n' +
      '### Model-based trajectories\n' +
      '\n' +
      '두 번째 궤적 소스로는 민첩성 로보틱스가 개발한 모델 기반 컨트롤러를 사용한다. 디지트 휴머노이드 로봇에 배치되고 민첩성 로보틱스의 시뮬레이터에서도 사용할 수 있는 컨트롤러입니다. 우리는 각각 10s의 평평한 땅에서 걷는 10k 궤적의 두 세트를 수집한다. 두 경우 모두 속도 명령을 샘플링합니다.\n' +
      '\n' +
      '그림 3: 다양한 데이터 소스로 훈련하기 위한 **일반적인 프레임워크. 데이터 모델링을 통해 여러 훈련 모드로 변압기를 훈련할 수 있습니다. 관측-작용 쌍이 사용 가능한 경우 변압기를 훈련하여 다음 관측-작용 쌍을 예측합니다. MoCap 및 인터넷 데이터와 함께 사용 가능한 액션 데이터가 없을 때, 우리는 마스크 토큰으로 액션을 마스킹하여 다음 관찰을 예측하도록 변압기를 훈련시킬 뿐이다. 이 두 가지 훈련 모델은 모델이 두 가지 유형의 데이터를 모두 활용할 수 있도록 하며, 이를 통해 데이터 측면에서 훈련을 확장할 수 있다.**\n' +
      '\n' +
      '다음과 같다 : 선속도 전진 \\([-1.0,1.0]\\) m/s, 선속도 측면 \\([-1.0,1.0]\\) m/s, 선회각속도 \\([-1.0,1.0]\\) rad/s. 우리는 한 세트에 대해 기본 모델 기반 구성을 사용하고 다른 세트에 대해 바닥의 다리 길이, 걸음 간격 및 튕김을 무작위화한다.\n' +
      '\n' +
      '이 컨트롤러는 관절 토크를 출력하므로 관절 위치 액션 공간과 일치하지 않습니다. 작업 없이 관측치만 기록합니다. 이 데이터는 동일한 형태학에서 합리적으로 우수한 관찰을 받았지만 액션이 없는 궤적의 소스 역할을 한다.\n' +
      '\n' +
      '인간의 움직임 포착 궤적\n' +
      '\n' +
      '궤적의 다음 소스로 AMASS 저장소(24)를 통해 배포된 KIT 데이터 세트(28)에서 사람의 모션 캡처(MoCap) 기록을 사용한다. 이 데이터는 실험실 환경에서 광학 마커 기반 추적을 사용하여 기록되었다. 데이터 세트는 \\(\\sim\\)4k 궤적으로 구성된다. 우리는 \\(\\sim\\)1k 서기, 걷기 및 달리기 궤적의 하위 집합을 사용한다.\n' +
      '\n' +
      '그라운드 트루스 동작을 포함하지 않는 것 외에도 MoCap 궤적은 다른 형태학이라는 추가 도전과 함께 제공된다. 즉, MoCap 궤적은 3D에서 _human_ 키포인트 위치를 포착한다. 이러한 궤적들을 로봇 훈련에 활용하기 위해 역기구학 문제를 해결하여 해당 로봇의 자세를 찾는다.\n' +
      '\n' +
      '역기구학 최적화 문제를 공식화한다:\n' +
      '\n' +
      '\\sum_{t=1}{N}\\varphi^{\\text{traj}[\\min_{\\mathbf{q}[t]+\\varphi^{t] (12a) s.t. \\mathbf{q}[t+1]+\\frac{\\dot{\\mathbf{q}[t+1]+\\dot{12b}\\in\\mathcal{V}\\tag{12c}\\t.\n' +
      '\n' +
      '여기서 \\(\\mathbf{q}\\)는 일반화된 좌표에서의 로봇 상태이고, \\(N\\) 및 \\(dt\\)은 최적화 지평선 및 샘플링 시간이다. 최적화 변수는 \\(\\mathbf{q}\\), \\(\\dot{\\mathbf{q}\\)을 포함한다. 제약을 위해 (12b)는 자세의 오일러 적분(\\(\\mathbf{q}\\)이고, (12c)는 허용집합 \\(\\mathcal{Q}\\)과 \\(\\mathcal{V}\\)에 대한 \\(\\mathbf{q}\\)과 \\(\\dot{\\mathbf{q}\\)의 범위를 제한한다. 비용함수에서 \\(\\varphi^{\\text{traj}\\)는 사람의 궤적으로부터 키포인트 위치를 추적하며, \\(\\varphi^{\\text{reg}\\)는 관절의 속도 최소화와 평활도와 같은 규칙화 비용을 나타낸다.\n' +
      '\n' +
      '유튜브 비디오의 궤적\n' +
      '\n' +
      '다양한 활동을 하는 사람들의 인터넷 비디오는 잠재적으로 인간의 움직임을 학습하기 위한 데이터의 통 소스이다. 그러나, 원시 픽셀들은 사람의 상태 및 동작에 대한 정보를 갖지 않는다. 이를 복구하기 위해 먼저 컴퓨터 비전 추적 알고리즘 PHALP(35)를 실행하여 사람의 궤적을 3D로 추출한다. 이것은 인체 SMPL(23) 파라미터들의 3D 관절들의 추정치 및 세계 좌표들에서의 인체 관절들의 잡음 추정치를 제공한다. 우리는 위에서 설명한 역 운동학 최적화를 사용하여 인체 관절 위치를 사용하여 휴머노이드 로봇에 움직임을 재표적화한다. 일단 인터넷 비디오에서 휴머노이드 궤적으로 움직임을 재타겟팅하면, 적은 최적화 비용으로 궤적을 필터링한다. 이 데이터의 규모는 소음이 발생하는 비용과 함께 발생한다는 점에 유의하십시오.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '우리는 휴머노이드 운동의 도전적인 작업에 대한 접근법을 평가한다. 실제 하드웨어에 대한 실외 실험을 수행하고 시뮬레이션에서 체계적인 평가를 수행한다.\n' +
      '\n' +
      '### Experimental setup\n' +
      '\n' +
      '**로봇 플랫폼.** 디지트는 민첩성 로보틱스가 개발한 휴머노이드 로봇 플랫폼입니다. 신장 1.6m, 몸무게 45kg의 풀 사이즈 휴머노이드입니다. 20개의 자유도가 작동합니다. 고차원성과 4절 연결 구조로 인해 어려움이 있다.\n' +
      '\n' +
      '그림 4: **트레이닝 데이터세트. 우리의 모델을 훈련시키기 위해, 우리는 네 가지 다른 출처에서 오는 궤적의 데이터 세트를 구성한다. _ (i) 신경망 정책:_는 완전한 관찰 및 동작을 갖는 궤적을 제공한다. _ (ii) 모델 기반 제어기:_는 액션이 없는 궤적을 생성한다. _ (iii) 사람의 모션 캡처:_는 동작을 포함하지 않고 로봇 상으로 대략 리타겟팅된다. _ (iv) 인간:_ 노이즈 인간 포즈의 인터넷 비디오는 먼저 3D 재구성을 통해 재구성되고, 그 다음 로봇 상으로 대략 리타겟팅된다.**\n' +
      '\n' +
      '빠르게 최적화하여 우리와 같은 궤적 모음에서 효율적으로 학습할 수 있는 접근 방식을 학습하는 데 특히 흥미롭습니다.\n' +
      '\n' +
      '**Model.** 우리 모델은 \\(192\\) 차원의 숨겨진 크기를 가지고 있으며, 4개의 자기 주의 레이어와 MLP 레이어가 있다. 각 자기 주의에는 4개의 머리가 있습니다. 각 어텐션 레이어 이전에 LayerNorm을 사용하고, MLP 레이어 이후에 ReLU 활성화를 사용한다. 배치Norm 레이어를 사용하여 변압기 모델 이전의 입력을 처리합니다. 시간 \\(k\\)에서 토큰을 예측할 때 컨텍스트 길이를 적당한 크기로 유지하기 위해 지난 16단계 입력만 유지한다. 섹션 5.9에서 우리는 모델이 더 많은 매개변수와 더 긴 컨텍스트 길이를 확장하고 더 높은 성능을 달성할 수 있음을 보여준다.\n' +
      '\n' +
      '### Real-world deployment\n' +
      '\n' +
      '우리는 실제 세계에 정책을 배포한 결과를 보고하는 것으로 시작합니다. 특히, 우리는 일주일 동안 샌프란시스코의 다양한 위치에 로봇을 배치하는 것을 평가한다. 동영상에 대한 예제 및 프로젝트 페이지는 그림 1을 참조하십시오. 우리는 우리의 정책이 보도, 콘크리트, 아스팔트, 타일형 광장, 흙길을 포함한 다양한 표면을 걸을 수 있다는 것을 발견했습니다. 샌프란시스코와 같은 대도시 환경에 배치하는 것은 제한된 환경보다 훨씬 더 어렵다. 도시 환경은 훨씬 더 붐비고, 덜 인내하며, 용서하지 않는다. 이렇게 하면 오류 허용 오차가 낮아지고 일관되게 잘 작동하는 정책이 필요합니다.\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      '우리는 두 가지 메트릭인 _tracking error_와 _prediction error_를 사용하여 이동 정책을 평가한다. 추적 오류는 로봇이 특정 이동 명령을 얼마나 정확하게 따르는지 측정합니다. 예측 오차는 별도의 검증 데이터 세트 상에서 측정된 다음 토큰 예측 손실이다. 세부 사항이 포함된 두 가지 메트릭을 소개하고 두 가지 메트릭이 일관되게 이동 성능을 예측할 수 있음을 보여준다.\n' +
      '\n' +
      '**추적 오차.** 모든 실험에서 로봇은 시뮬레이션된 환경에서 휴식을 취하기 시작하여 \\([0.35,0.70]\\)m/s로 샘플링된 원하는 헤딩 속도, \\([-0.4,0.4]\\)rad/s로 샘플링된 각속도 및 0 측면 속도로 구성된 일정한 자연 보행 명령을 내린다. 우리는 모든 시간 단계에서 속도 명령 \\(\\mathbf{v}^{*}(t)\\)을 완전히 만족하는 이상적인 로봇 베이스 위치 궤적의 \\(\\mathbf{x}^{*}(t)\\)을 계산한다. 명령 추적의 정확도를 측정하기 위해 위치 추적 오차를 \\(\\frac{1}{T}\\sum_{t=0}^{T}\\|\\mathbf{x}(t)-\\mathbf{x}^{*}(t)\\|\\으로 정의한다. 평가에는 MuJoCo 시뮬레이터(41)를 사용하며, 모든 궤적은 10초 동안 지속된다.\n' +
      '\n' +
      '**예측 오류.** 모델은 다음 토큰 예측으로 학습되기 때문에, 우리는 학습 데이터로부터 분리되고 RL 정책으로부터 수집된 상태-행동 궤적을 포함하는 검증 데이터 세트에 대해 예측 오류를 테스트한다. 이는 대형 언어 모델에 대한 언어 모델링 평가(14)와 유사하다. 상태 및 동작 예측 오류를 모두 테스트하고 최종 오류 메트릭으로 함께 추가한다.\n' +
      '\n' +
      '### 최신 기술과의 비교\n' +
      '\n' +
      '**Trajectory Adherence.** 우리는 강화 학습(RL)(33)으로 훈련된 신경망:추적 네트워크 제어기와 우리의 정책을 비교한다. 그림 5는 이러한 최첨단 기준선에 대한 제어기의 궤적 준수의 시각적 비교를 보여준다. 원점에 있는 로봇을 시작으로 \\(\\{0.00,\\pm 0.05,\\pm 0.10,\\pm 0.20,\\pm 0.30,\\pm 0.40\\}\\) rad/s에서 선택된 11개의 다른 요 명령을 사용하여 로봇의 실제 궤적을 표시한다. 각 정책에 대해 로봇 기반이 추적한 원하는 실제 경로를 공동으로 표시합니다. 우리의 모델은 모든 회전 속도에서 RL 컨트롤러보다 우수한 추적을 나타내며 직선 보행에 거의 완벽한 추적을 가지고 있습니다.\n' +
      '\n' +
      '도 5: **최첨단과의 비교, 궤적 준수. 로봇은 원점에서 출발하여 \\(0.5\\)m/s의 고정된 헤딩 명령과 \\([-0.4,0.4]\\)rad/s의 다양한 요 명령을 사용하여 보행하도록 명령한다. 정책 및 강화 학습 훈련 정책(RL)에 대해 원하는(점선) 및 실제(실선) 궤적을 표시합니다.**\n' +
      '\n' +
      '그림 6: **추적 오류 비교. 우리는 최첨단 벤치마크(왼쪽)에 대한 정책의 추적 오류와 액션 라벨이 붙은 RL 궤적을 액션이 없는 궤적(오른쪽)으로 보완하여 생성된 개선을 측정한다.\n' +
      '\n' +
      '**정량적 평가.** 그림 6에서 왼쪽, 위의 비교를 RL 제어기 \\((N=245)\\)와 반복하며 섹션 5.3에서 언급한 모든 범위의 헤딩 및 요 속도. 명령된 각 요에 의해 비닝된 평균 위치 추적 오류를 표시한다. 두 모델 모두 낮은 요에서 추적 오류가 더 낮지만, 우리의 모델은 기준 RL 정책보다 일관되게 우수하다. 이 모델은 바로 이 정책에 의해 생성된 궤적에 대한 다음 토큰 예측에 대해 훈련되었기 때문에 흥미로운 결과이다.\n' +
      '\n' +
      '### 예측 오류와 성능 상관 관계\n' +
      '\n' +
      '서로 다른 훈련 레시피, 모델 아키텍처, 데이터 크기 및 유형으로 훈련된 14개의 모델을 수집하고 각각에 대한 테스트 추적 오류 및 예측 오류를 수집한다. 우리는 그림 7과 같이 모든 모델의 추적 및 예측 오차를 단일 산점도로 표시하며, 추적 및 예측 오차는 피어슨 계수 \\(r=0.87\\)과 높은 상관 관계가 있음을 알 수 있으며, 이는 검증 세트에서 예측 오차가 낮은 모델이 정확도가 높은 다른 명령을 따를 가능성이 있음을 의미한다. 이는 예측 오차가 예측 과제 수행임을 시사한다.\n' +
      '\n' +
      '### Gait quality\n' +
      '\n' +
      '휴머노이드 운동에서 로봇의 걸음걸이의 부드러움은 작동된 무릎 관절의 리듬 기능에 달려 있다. 이를 측정하는 한 가지 방법은 시간 경과에 따른 관절의 일반화된 위치와 속도의 매개변수 도표인 위상 초상화이다. 플롯의 패턴은 관절이 겪고 있는 움직임의 유형에 대한 정보를 드러낼 수 있다. 예를 들어, 순환 패턴은 반복적인 움직임을 나타낼 수 있는 반면, 불규칙한 패턴은 비틀림과 같은 복잡하거나 다양한 움직임을 나타낼 수 있다. 그림 8에서 우리는 로봇이 \\(0.5\\)m/s로 앞으로 걸어가도록 명령하고 왼쪽 무릎 관절의 관련 위상 초상을 플로팅한다. 우리의 정책은 더 적은 수차를 가지면서도 RL 정책의 전체적인 형태를 유지한다는 것을 주목하라. 이것은 우리 정책에서 볼 수 있는 보다 규칙적인 행동에 대한 우리의 질적 평가를 뒷받침한다.\n' +
      '\n' +
      '###보이지 않는 명령에 대한 일반화\n' +
      '\n' +
      '우리는 우리의 정책이 또한 행동 라벨이 붙은 훈련 데이터에 포함된 _not_와 같은 뒤로 걷는 것과 같은 새로운 기술을 추론한다는 것을 발견했다. 그림 9에서 알 수 있듯이, 우리의 컨트롤러에 헤딩 명령에 대해 음의 값으로 프롬프트함으로써, 우리는 로봇이 낙하 없이 0.5 m/s까지의 속도로 자연스럽게 후방 보행을 수행한다는 것을 발견한다.\n' +
      '\n' +
      '액션프리 데이터를 이용한###훈련\n' +
      '\n' +
      '우리 접근법의 이점 중 하나는 유튜브의 인간 동영상의 경우 행동과 같은 정보가 누락되는 등 다양한 출처의 궤적에 적용할 수 있다는 것이다. 그림 6에서 우리는 완전한 궤적과 불완전한 궤적 모두에 대한 관절 훈련과 완전한 궤적과만 훈련의 성능을 비교한다. 우리는 불완전한 궤적을 포함하는 것이 일관되게 더 나은 성능으로 이어진다는 것을 관찰한다. 이것은 다양한 궤적의 대규모 집합에 대한 접근 방식을 확장하기 위한 유망한 신호이다.\n' +
      '\n' +
      '그림 8: **Gait quality.** 우리는 \\(0.5\\) m/s의 헤딩 속도로 로봇을 명령하고 왼쪽 무릎 관절의 결과 위상 초상을 플롯한다. RL 정책에 비해, 우리의 정책은 더 적은 불규칙성과 더 부드럽고 순환적인 걸음걸이를 특징으로 한다.\n' +
      '\n' +
      '그림 7: **예측 오차는 성능과 상관관계가 있다.** 14개 모델에 대한 추적 오차 및 예측 오차를 표시한다. 예측 오차는 작업 추적 오차와 \\(r=0.87\\)의 선형 상관관계가 있으며, 이는 낮은 예측 손실이 더 정확한 명령 후속을 나타낼 가능성이 있음을 의미한다.\n' +
      '\n' +
      '그림 9: ** 보이지 않는 명령.** 우리의 정책은 훈련 중에 보이지 않는 테스트 시간에 역방향 명령을 따를 수 있다.\n' +
      '\n' +
      '### Scaling studies\n' +
      '\n' +
      '**트레이닝 데이터.** 그림 10에서 왼쪽, 우리는 트레이닝 데이터세트의 크기를 증가시켜 모델의 성능 스케일링을 연구한다. 우리는 더 많은 궤적에 대한 훈련이 더 큰 데이터 세트에 대한 훈련 시 성능 증가에 긍정적인 신호인 위치 추적 오류를 감소시킨다는 것을 발견했다.\n' +
      '\n' +
      '**Context length.** 우리는 그림 10 중간에서 16, 32, 48 단계 사이에서 변화시키면서 변압기 정책의 컨텍스트 윈도우에서 사용되는 토큰의 수를 증가시키는 효과를 연구한다. 더 큰 컨텍스트 창은 더 나은 정책을 생성하며, 이는 우리의 생성 정책이 규모에 따라 개선되는 컨텍스트 내 적응의 형태를 수행함을 시사한다.\n' +
      '\n' +
      '**모델 크기.** 임베딩 차원(144, 192, 384), 어텐션 헤드 수(3, 4, 12), 트랜스포머 블록 수(4, 4, 6)를 각각 달리하여 파라미터 수(1M, 2M, 8M)가 증가하는 모델을 비교한다. 추적 오류는 모델 크기에 따라 단조롭게 감소합니다.\n' +
      '\n' +
      '### Ablation studies\n' +
      '\n' +
      '**연접 _vs._ 트랜스포머의 입력을 위해 각 단계에서 관찰과 행동을 하나의 토큰으로 연결하거나 두 개의 개별 토큰에 임베딩할 수 있다.** 표 1에서 이 두 가지 선택을 비교한다. 토큰들을 분리하면 추적 오류가 더 낮은 반면, 연접은 예측 오류가 더 낮다는 것을 알 수 있다. 전반적으로 이 두 가지 토큰은 별도의 토큰을 사용하는 동안 비교적 잘 수행되며 입력 길이를 두 배로 늘리고 계산 오버헤드를 도입한다.\n' +
      '\n' +
      '**Modality-aligned _vs._ 비정렬 예측.** 관측 및 동작을 위한 별도의 토큰을 입력으로 사용할 경우, 예측과 입력 사이의 모달리티를 정렬하는 \\(o_{i}\\)에서 \\(\\widehat{o}_{i+1}\\)과 \\(a_{i}\\)에서 \\(\\widehat{a}_{i+1}\\)을 예측하거나, 정렬이 없는 \\(o_{i}\\)에서 \\(\\widehat{o}_{i+1}\\)과 \\(o_{i+1}\\)에서 \\(\\widehat{a}_{i+1}\\)을 예측할 수 있다. 표 1에서 모달리티 정렬이 정렬이 없는 것보다 분명히 더 나은 성능을 가지고 있음을 알 수 있다. 추론 중 \\(t\\)번째 단계에서 \\((t+1)\\)번째 단계의 행동을 예측할 때 정렬이 없기 때문에 먼저 \\(\\widehat{o}_{i+1}\\)을 예측하고 이 예측을 입력으로 사용하여 \\(\\widehat{a}_{i+1}\\)을 예측해야 하기 때문이다. 예측된 \\(\\widehat{o}_{i+1}\\)이 실제 \\(o_{i+1}\\)에 비해 정확하지 않을 경우, 훈련 중 \\(\\widehat{a}_{i+1}\\)을 예측하는데 사용된다.\n' +
      '\n' +
      '** 합동 훈련 _vs._ 단계적 훈련.** 액션이 있는 완전한 데이터와 액션이 없는 불완전한 데이터가 모두 주어지면 섹션 3에 설명된 대로 두 데이터에 대해 공동으로 훈련하거나 먼저 상태 예측만 있는 모든 데이터에 대해 모델을 사전 훈련한 다음 액션 예측이 있는 완전한 데이터에 대해 모델을 미세 조정할 수 있다. 우리는 표 1의 두 접근법을 비교하였는데, 이 두 접근법 사이에 유의미한 차이가 관찰되지 않았으며, 이는 상태 예측에 대한 사전 훈련과 행동 예측에 대한 미세 조정도 합리적인 운동 정책을 제공함을 나타낸다.\n' +
      '\n' +
      '**State-action prediction _vs._ 액션 전용 예측.** 우리는 액션만을 예측하는 것으로 훈련될 때와 상태 및 액션을 모두 예측하는 것으로 훈련될 때의 정책의 성능을 비교한다. 표 1의 결과는 상태-행동 예측이 궤적 추적에 대한 모델 성능을 향상시킨다는 것을 보여준다. 우리는 추가 학습 신호를 통해 모델이 운동 과제에 유익한 세계의 더 풍부한 표현을 학습할 수 있다고 가정한다.\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      '우리는 실제 휴머노이드 운동을 위한 자체 감독 접근법을 제시한다. 우리의 모델은 이전의 신경망 정책, 모델 기반 컨트롤러, 인간 모션 캡처 및 인간의 유튜브 비디오에서 나오는 감각 운동 궤적 모음에 대해 훈련된다. 우리는 우리의 모델이 실물 크기의 휴머노이드가 실제 제로샷을 걸을 수 있게 한다는 것을 보여준다. 이러한 결과는 대규모 궤적 집합의 생성 모델링을 통해 실제 로봇 제어 작업에 도전하는 학습을 위한 유망한 경로를 제안한다.\n' +
      '\n' +
      '그림 10: **Scaling Study. 우리는 우리의 접근 방식이 훈련 데이터 세트(왼쪽), 컨텍스트 길이(중간) 및 더 큰 모델(오른쪽)의 궤적 수로 확장된다는 것을 발견했다.**\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '이 작업은 DARPA Machine Common Sense 프로그램, ONR MURI 프로그램(N00014-21-1-2801), NVIDIA, 홍콩 물류 로봇 센터, AI 연구소, BAIR의 산업 제휴 프로그램이 부분적으로 지원되었다. 역기구학 시뮬레이션 실험을 도와준 사너 카키르와 비카스 우마디세티에게 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]K. Bousmalis, G. Vezzani, D. Rao, C. Devin, A. X. Lee, M. 바우자, T 다브초프 Zhou, A. Gupta, A. Raju, et al.(2023) Robocat: a self- improved foundation agent for robotic manipulation. ArXiv:2306.11706. 인용: SS1.\n' +
      '*[2]A. 브로한남 브라운, J. 카르바할, Y. 체보타르, J. 다비스, C. 핀, K. 고팔라크리쉬난 Hausman, A. Herzog, J. Hsu, et al.(2022) Rt-1: scale에서 실세계 제어를 위한 로봇 트랜스포머. 2212.06817. 인용: SS1.\n' +
      '*[3]T. 브라운, B. 만, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) 언어 모델은 소수의 학습자이다. NeurIPS에서 인용: SS1.\n' +
      '*[4]T. B. Brown, B. Mann, N. 라이더 Subbiah, J. A. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) 언어 모델은 소수의 학습자이다. 뉴립스 인용: SS1.\n' +
      '*[5]G. A. Castillo, B. Weng, W. Zhang, and A. Hereid (2021) Robust feedback motion policy design using reinforcement learning on the 3d digit bipedal robot. IROS에서 인용: SS1.\n' +
      '*[6]M. 천아래드포드 Child, J. Wu, H. Jun, D. Luan, and I. Sutskever (2020) Generative pretraining from pixels. ICML에서 인용됨: SS1.\n' +
      '*[7]M. 치뇨리, D. Kim, E. Stanger-Jones, S. Kim (2021) The mit humanoid robot: Design, Motion Planning, and control for acrobatic behavior. 휴머노이드에서 인용: SS1.\n' +
      '*[8]J. 데블린 장경 이경호 Toutanova (2019) Bert: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. NAACL-HCT에서 인용된 것은 SS1이다.\n' +
      '*[9]A. L. 도소비츠키 Beyer, A. Kolesnikov, D. Weissenborn, X. 자이태 Unterthiner, M 데하니 민더러, G. 헤이골드, S. Gelly, et al.(2020) 이미지는 16x16 단어들의 가치가 있다: 스케일에서 이미지 인식을 위한 트랜스포머들. ICLR에서 인용: SS1.\n' +
      '*[10]D. Driess F. Xia, M. 사자디, C. 린치, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. 부엉태 Yu, et al. (2023) Palm-e: embodied multimodal language model. ArXiv:2303.03378. 인용: SS1.\n' +
      '*[11]J. 엔겔, K. K. 아그라발, S. Chen, I. Gulrajani, C. Donahue, and A. Roberts(2019) Gansynth: 적대적 신경 오디오 합성. 1902.08710. 인용: SS1.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & Track Err. & Pred. Err. \\\\ \\hline Joint training & **0.310** & 0.88 \\\\ Staged training & 0.311 & - \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 모델링 및 훈련에서 서로 다른 설계 선택에 대한 **절제.** 각 절제에 대해 명령 세트에 대한 평균 추적 오류와 테스트 세트에 대한 다음 토큰 예측 오류를 비교한다. 공정한 비교를 위해 액션만 예측하는 모델에 대해 다음 토큰 예측 오류를 보고하지 않습니다.\n' +
      '\n' +
      '* [12] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In _NeurIPS_, 2014.\n' +
      '* [13] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. Masked autoencoders are scalable vision learners. _arXiv:2111.06377_, 2021.\n' +
      '* [14] Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.\n' +
      '* [15] Hirai, K., Hirose, M., Haikawa, Y., and Takenaka, T. The development of honda humanoid robot. In _ICRA_, 1998.\n' +
      '* [16] Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In _NeurIPS_, 2020.\n' +
      '* [17] Hochreiter, S. and Schmidhuber, J. Long short-term memory. _Neural computation_, 1997.\n' +
      '* [18] Kajita, S., Kanehiro, F., Kaneko, K., Yokoi, K., and Hirukawa, H. The 3d linear inverted pendulum mode: A simple modeling for a biped walking pattern generation. In _IROS_, 2001.\n' +
      '* [19] Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. _arXiv:2001.08361_, 2020.\n' +
      '* [20] Kato, I. Development of wabot 1. _Biomechanism_, 1973.\n' +
      '* [21] Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Hornung, R., Adam, H., Akbari, H., Alon, Y., Birodkar, V., et al. Videopoet: A large language model for zero-shot video generation. _arXiv:2312.14125_, 2023.\n' +
      '* [22] Kuindersma, S. Recent progress on atlas, the world\'s most dynamic humanoid robot, 2020. URL [https://youtu.be/EGABAx52GKI](https://youtu.be/EGABAx52GKI).\n' +
      '* [23] Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., and Black, M. J. Smpl: A skinned multi-person linear model. In _Seminal Graphics Papers: Pushing the Boundaries, Volume 2_, 2023.\n' +
      '* [24] Mahmood, N., Ghorbani, N., Troje, N. F., Pons-Moll, G., and Black, M. J. AMASS: Archive of motion capture as surface shapes. In _ICCV_, 2019.\n' +
      '* [25] Makoviychuk, V., Wawrzyniak, L., Guo, Y., Lu, M., Storey, K., Macklin, M., Hoeller, D., Rudin, N., Allshire, A., Handa, A., et al. Isaac gym: High performance gpu-based physics simulation for robot learning. In _NeurIPS_, 2021.\n' +
      '* [26] Nelson, G., Saunders, A., Neville, N., Swilling, B., Bondaryk, J., Billings, D., Lee, C., Playter, R., and Raibert, M. Petman: A humanoid robot for testing chemical protective clothing. _Journal of the Robotics Society of Japan_, 2012.\n' +
      '* [27] Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. Wavenet: A generative model for raw audio. _arXiv:1609.03499_, 2016.\n' +
      '* [28] Plappert, M., Mandery, C., and Asfour, T. The KIT motion-language dataset. _Big Data_, 2016.\n' +
      '* [29] Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pre-training. 2018.\n' +
      '* [30] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. 2019.\n' +
      '* [31] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* [32] Radosavovic, I., Shi, B., Fu, L., Goldberg, K., Darrell, T., and Malik, J. Robot learning with sensorimotor pre-training. In _CoRL_, 2023.\n' +
      '* [33] Radosavovic, I., Xiao, T., Zhang, B., Darrell, T., Malik, J., and Sreenath, K. Real-world humanoid locomotion with reinforcement learning. _arXiv:2303.03381_, 2023.\n' +
      '* [34] Raibert, M. H. _Legged robots that balance_. MIT press, 1986.\n' +
      '* [35] Rajasegaran, J., Pavlakos, G., Kanazawa, A., and Malik, J. Tracking people by predicting 3d appearance, location and pose. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 2740-2749, 2022.\n' +
      '* [36] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-to-image generation. In _ICML_, 2021.\n' +
      '* [37] Shannon, C. E. Prediction and entropy of printed english. _Bell system technical journal_, 1951.\n' +
      '* [38] Shridhar, M., Manuelli, L., and Fox, D. Perceiver-actor: A multi-task transformer for robotic manipulation. In _CoRL_, 2022.\n' +
      '* [39] Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, 2015.\n' +
      '\n' +
      '* [40] Stasse, O., Flayols, T., Budhiraja, R., Giraud-Esclasse, K., Carpentier, J., Mirabel, J., Del Prete, A., Soueres, P., Mansard, N., Lamiraux, F., et al. Talos: A new humanoid research platform targeted for industrial applications. In _Humanoids_, 2017.\n' +
      '* [41] Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In _IROS_, 2012.\n' +
      '* [42] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In _NeurIPS_, 2017.\n' +
      '* [43] Wu, J., Zhang, C., Xue, T., Freeman, B., and Tenenbaum, J. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. In _NeurIPS_, 2016.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>