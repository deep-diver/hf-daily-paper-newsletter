<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'Figure 1: **A humanoid that walks in San Francisco.** We deploy our policy to various locations in San Francisco over the course of one week. Please see our project page for videos. We show that our policy can walk over different surfaces including walkways, concrete, asphalt, tiled plazas, and sanded roads. We find that our policy follows omnidirectional velocity commands well and enables deployment in a challenging city environment like San Francisco.\n' +
      '\n' +
      ' To validate our method, we apply it to the challenging task of real-world humanoid locomotion. We use the full-sized Digit humanoid robot developed by Agility Robotics. We first collect a dataset of sensorimotor trajectories in simulation. These include complete trajectories from a neural network policy trained with reinforcement learning [33], as well as incomplete trajectories from three different sources: (i) Agility Robotics controller based on model predictive control, (ii) motion capture of humans, and (iii) YouTube videos of humans. We reconstruct human videos by using computer vision techniques and retarget both motion capture and YouTube trajectories via inverse kinematics. We then train a transformer model to autoregressively predict trajectories. At test time, we execute the actions autoregressively and ignore the sensory predictions.\n' +
      '\n' +
      'We demonstrate that our policy can be deployed in the real world zero-shot and walk on different surfaces. Specifically, deploy our model across a range of different locations in San Francisco over the course of one week. Please see Figure 1 for examples and our project page for videos. To quantitatively evaluate different aspects of our approach, we perform an extensive study in simulation. We find that our autoregressive policies trained from offline data alone are comparable to the state-of-the-art approaches that use reinforcement learning [33] in tested settings. We further find that our approach can readily benefit from incomplete trajectories and has favorable scaling properties.\n' +
      '\n' +
      'These findings suggest a promising path toward learning challenging real-world robot control tasks by generative modeling of large collections of sensorimotor trajectories.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '**Generative modeling.** The study of data has been extensive, ranging from Shannon\'s foundational work [37] to the recent era of large language models. Various such models emerged over the last decade. Notable such models includes, GAN [12] and Diffusion models [39, 16] for generating pixels, LSTM [17] and GPT [29] for generating language tokens. These models have been adopted for other modalities as well [27, 11, 43]. Among these, autoregressive transformer models became the front runner, due to the impressive scaling behaviours [19] and ability to learn from in-context examples [3]. This behavior is even shown to extend to other modalities such as pixels [6], language-pixels [36], and language-pixels-audio [21]. We explore autoregressive generative models in the context of real-world humanoid locomotion.\n' +
      '\n' +
      '**Transformers in robotics.** Following the success of transformer models [42] in natural language processing [29, 8, 30, 3] and computer vision [9, 13], over the last few years, there has been an increased interested in using transformer models in robotics. We have seen several works showing that transformers can be effective with behavior cloning. For example, [38] learns multi-task transformer policies with language, and [2] trains language-conditioned manipulation policies from large-scale data. [10] trains language models with embodied data. We have also seen that transformer policies can be effective for large-scale reinforcement learning [33]. [32] learns sensorimotor representations with masked prediction. [1] trains goal-conditioned policies are learned from demonstrations. Likewise, we share the goal of using transformer models for robotics but focus on autoregressive modeling of diverse trajectories for real-world humanoid locomotion.\n' +
      '\n' +
      '**Humanoid locomotion.** Mastering the ability for robots to walk has been a long-standing challenge in robotics. In the past several decades, roboticists have built a variety of humanoid robots [20, 15, 26, 40, 7] to explore human-like locomotion skills. Stable locomotion behaviors have been achieved through model-based control approaches [34, 18], and optimization-based methods further enable highly dynamic humanoid motions [22]. Although significant progress has been made with these strategies and combining them with learning [5], learning-based approaches are gaining attention for their ability to improve and adapt to a wide range of environments. Recently, we have seen that a purely learning based approach trained with large-scale reinforcement learning in simulation can enable real-world humanoid locomotion [33]. Like in prior work, our model is a causal transformer. Unlike prior work, we perform autoregressive modeling instead of reinforcement learning.\n' +
      '\n' +
      '## 3 Approach\n' +
      '\n' +
      'In this section, we assume that a dataset \\(\\mathcal{D}\\) of sensorimotor trajectories \\(\\mathcal{T}\\) is given and describe our approach below.\n' +
      '\n' +
      '### Objective\n' +
      '\n' +
      'Each sensorimotor trajectory is a sequence of sensory observations and actions: \\(\\mathcal{T}=(o_{1},a_{1},o_{2},a_{2},...,o_{T},a_{T})\\). We first tokenize the trajectory into K tokens to obtain \\(t=(t_{1},t_{2},t_{3},...,t_{K})\\). Our goal is to train a neural network to model the density function \\(p(t)\\) autoregressively:\n' +
      '\n' +
      '\\[p(t)=\\prod_{k=1}^{K}p(t_{k}|t_{k-1},...,t_{1}) \\tag{1}\\]\n' +
      '\n' +
      'We train our model by minimizing the negative log-likelihood over our trajectory dataset:\n' +
      '\n' +
      '\\[L=\\sum_{t\\in\\mathcal{D}}-\\log p(t) \\tag{2}\\]We assume a Gaussian distribution with constant variance and train a neural network to minimize the mean squared error between the predicted and the ground truth tokens:\n' +
      '\n' +
      '\\[L=\\frac{1}{K}\\sum_{k=1}^{K}(\\widehat{t}_{k}-t_{k})^{2} \\tag{3}\\]\n' +
      '\n' +
      'Instead of regressing the raw token values, we could quantizing each dimension into bins or perform vector quantization. However, we found the regression approach to work reasonably well in practice and opt for it for simplicity.\n' +
      '\n' +
      '### Missing modalities\n' +
      '\n' +
      'In the discussion so far we have assumed that each trajectory is a sequence of observations and actions. Next, we show how our framework can be generalized to sequences with missing modalities, like trajectories extracted from human videos that do not have actions. Suppose we are given a trajectory of observations without the actions \\(\\mathcal{T}=(o_{1},o_{2},...,o_{T})\\). Our key insight is that we can treat a trajectory without actions like a regular trajectory with actions masked. Namely, we can insert mask tokens [M] to obtain \\(\\mathcal{T}=(o_{1},\\,[\\texttt{M}]\\,,o_{2},\\,[\\texttt{M}]\\,,...,o_{T},\\,[ \\texttt{M}]\\,)\\). This trajectory now has the same format as our regular trajectories and thus can be processed in a unified way. We ignore the loss for the predictions that correspond to the masked part of inputs. Note that this principle is not limited to actions and applies to any other modality as well.\n' +
      '\n' +
      '### Aligned prediction\n' +
      '\n' +
      'Rather than predicting the next token in a modality-agnostic way, we make predictions in a modality-aligned way. Namely, for each input token we predict the next token of the _same_ modality. Please see Figure 3 for diagrams.\n' +
      '\n' +
      '### Joint training\n' +
      '\n' +
      'We have two options for training on collections that contain diverse trajectories in terms of noise levels or modality subsets. We can either train jointly with all data at once, including complete and incomplete trajectories. Alternatively, we can first pre-train on noisy and incomplete trajectories. This can be viewed as providing a good initialization for then training on complete trajectories. We find that both approaches work comparably in our setting and opt for joint training in the majority of the experiments for simplicity.\n' +
      '\n' +
      '### Model architecture\n' +
      '\n' +
      'Our model is a vanilla transformer [42]. Given the trajectories from either complete or incomplete data, we first tokenize the trajectories into tokens. We learn separate linear projection layers for each modality but shared across time. To encode the temporal information we use positional embeddings. Let\'s assume \\(o_{i}\\in\\mathcal{R}^{m}\\) and \\(a_{i}\\in\\mathcal{R}^{n}\\), then:\n' +
      '\n' +
      '\\[t_{i} =\\texttt{concat}(o_{i},a_{i}), \\tag{4}\\] \\[h_{i}^{0} =Wt_{i}, \\tag{5}\\]\n' +
      '\n' +
      'where \\(W\\in\\mathcal{R}^{d\\times(m+n)}\\) is a linear projection layer to project concatenated observation and action modalities to \\(d\\) dimensional embedding vector. The superscript \\(0\\) indicates the embedding at \\(0\\)-th layer, _i.e._, the input layer. When action is unavailable, we use a mask token [M]\\(\\in\\mathcal{R}^{n}\\) to replace \\(a_{i}\\), and [M] is initialized as a random vector and learned end-to-end with the whole model. The model takes the sequence of embedding vectors \\(H_{0}=\\{h_{1}^{0},h_{2}^{0},...,h_{t}^{0}\\}\\) as input.\n' +
      '\n' +
      'The transformer architecture contains \\(L\\) layers, each consisting of a multi-head self-attention module and an MLP module. Assume the output of the layer \\(l\\) is \\(H_{l}\\), then the\n' +
      '\n' +
      'Figure 2: **Humanoid locomotion as next token prediction. We collect a dataset on trajectories from various sources, such as from neural network policies, model-based controllers, human motion capture, and YouTube videos of humans. Then we use this dataset to train a transformer policy by autoregressive modeling of observations and actions. Our transformer allows a humanoid to walk zero-shot on various terrains around San Francisco. Please see our project page for video results.**\n' +
      '\n' +
      'layer \\(l+1\\) output is computed as follows:\n' +
      '\n' +
      '\\[\\tilde{H_{l}} =\\texttt{LayerNorm}(H_{l}) \\tag{6}\\] \\[\\tilde{H_{l}} =\\tilde{H_{l}}+MHSA(\\tilde{H_{l}})\\] (7) \\[H_{l+1} =\\tilde{H_{l}}+MLP(\\tilde{H_{l}}) \\tag{8}\\]\n' +
      '\n' +
      'Here, the multi-head self-attention has causal masking, where the token only attends to itself and the past tokens. Once the tokens are processed through all the layers, we project the embedding to predicted states and actions, by learning a linear projection layer \\(\\widehat{W}\\in\\mathcal{R}^{(m+n)\\times d}\\):\n' +
      '\n' +
      '\\[\\widehat{t}_{i+1} =\\widehat{W}h_{i}^{L} \\tag{9}\\] \\[\\widehat{o}_{i+1} =(\\widehat{t}_{i+1})_{0:m}\\] (10) \\[\\widehat{a}_{i+1} =(\\widehat{t}_{i+1})_{m:(m+n)} \\tag{11}\\]\n' +
      '\n' +
      'Then we train the transformer with the objective in (3). In the cases where the token is masked, we do not apply any losses. We train our transformer with both types of data, as shown in Figure 3. This allows us to use various sources of data, thus enabling scaling in terms of data.\n' +
      '\n' +
      '### Model inference\n' +
      '\n' +
      'At inference time, our transformer model will always have access to observation-action pairs. In this setting, we apply our transformer model autoregressively for each observation-action pair token. By conditioning on past observations and actions, we predict the next actions (or observation-action pair) and execute the action. Then we take the observations from the robot and discard the predicted observations. We use the observed observation and predicted action as the next set of tokens and concatenate them with past pairs to predict the next observation-action pair.\n' +
      '\n' +
      '## 4 Dataset\n' +
      '\n' +
      'Our approach motivates building a dataset of trajectories for training our model. Our dataset includes trajectories from different sources: (i) neural network policies, (ii) model-based controllers, (iii) human motion capture, and (iv) human videos from YouTube. An illustration of different data sources is shown in Figure 4. We describe each in turn next.\n' +
      '\n' +
      '### Neural network trajectories\n' +
      '\n' +
      'As the first source of training trajectories, we use a neural network policy trained with large-scale reinforcement learning (33). Specifically, this policy was trained with billions of samples from thousands of randomized environments in Isaac Gym (25). We run this policy in the Agility Robotics\' simulator and collect 10k trajectories of 10s each on flat ground, without domain randomization. Each trajectory is conditioned on a velocity command sampled from a clipped normal distribution as follows: linear velocity forward \\([0.0,1.0]\\) m/s, linear velocity sideways \\([-0.5,0.5]\\) m/s, and turning angular velocity \\([-0.5,0.5]\\) rad/s.\n' +
      '\n' +
      'Since we have access to the data generation policies, we are able to record complete observations as well as the exact actions that the model predicted. We use this set as our source of complete sensorimotor trajectories that have complete observations as well as ground truth actions.\n' +
      '\n' +
      '### Model-based trajectories\n' +
      '\n' +
      'As the second source of trajectories, we use the model-based controller developed by Agility Robotics. It is the controller that is deployed on the Digit humanoid robot and available in the Agility Robotics\' simulator as well. We collect two sets of 10k trajectories of walking on a flat ground of 10s each. In both cases, we sample the velocity commands\n' +
      '\n' +
      'Figure 3: **A general framework for training with different data sources. Our data modeling allows us to train our transformer with multiple modes of training. In the case of observation-action pairs being available, we train our transformer to predict the next pair of observation-action. When there is no action data available, with MoCap and internet data, we only train our transformer to predict the next observations by masking the actions with a mask token. These two models of training allow our model to utilize both types of data, and this enables us to scale our training in terms of data.**\n' +
      '\n' +
      'as follows: linear velocity forward \\([-1.0,1.0]\\) m/s, linear velocity sideways \\([-1.0,1.0]\\) m/s, and turning angular velocity \\([-1.0,1.0]\\) rad/s. We use the default model-based configurations for one set and randomize the leg length, step clearance, and bounciness of the floor for the other.\n' +
      '\n' +
      'As this controller outputs joint torques, which are not consistent with our joint position action space. We only record the observations without the actions. This data serves as a source of trajectories with reasonably good observations from the same morphology but without the actions.\n' +
      '\n' +
      '### Human motion capture trajectories\n' +
      '\n' +
      'As the next source of trajectories, we use the motion capture (MoCap) recordings of humans from the KIT dataset (28) distributed via the AMASS repository (24). This data was recorded using optical marker-based tracking in a laboratory setting. The dataset consists of \\(\\sim\\)4k trajectories. We use a subset of \\(\\sim\\)1k standing, walking, and running trajectories.\n' +
      '\n' +
      'In addition to not containing the ground truth actions, the MoCap trajectories come with an additional challenge: different morphology. Namely, MoCap trajectories capture _human_ keypoint positions in 3D. In order to use these trajectories for training a robot, we solve an inverse kinematics problem to find the corresponding robot poses.\n' +
      '\n' +
      'We formulate an inverse kinematics optimization problem:\n' +
      '\n' +
      '\\[\\min_{\\mathbf{q}[t],\\dot{\\mathbf{q}}[t]} \\sum_{t=1}^{N}\\varphi^{\\text{traj}}[t]+\\varphi^{\\text{reg}}[t]\\] (12a) s.t. \\[\\mathbf{q}[t+1]=\\mathbf{q}[t]+\\frac{\\dot{\\mathbf{q}}[t+1]+\\dot{ \\mathbf{q}}[t]}{2}dt, \\tag{12b}\\] \\[\\mathbf{q}\\in\\mathcal{Q},\\dot{\\mathbf{q}}\\in\\mathcal{V} \\tag{12c}\\]\n' +
      '\n' +
      'where \\(\\mathbf{q}\\) is the robot state in the generalized coordinates, and \\(N\\) and \\(dt\\) are the optimization horizon and sampling time. The optimization variables include \\(\\mathbf{q}\\), \\(\\dot{\\mathbf{q}}\\). For constraints, (12b) is the Euler integration of posture \\(\\mathbf{q}\\), (12c) constrains the range of \\(\\mathbf{q}\\) and \\(\\dot{\\mathbf{q}}\\) to their admissible sets \\(\\mathcal{Q}\\) and \\(\\mathcal{V}\\). In the cost function, \\(\\varphi^{\\text{traj}}\\) tracks keypoint locations from human trajectories, and \\(\\varphi^{\\text{reg}}\\) represents the regularization costs, such as joint velocity minimization and smoothness.\n' +
      '\n' +
      '### Trajectories from YouTube videos\n' +
      '\n' +
      'Internet videos of people doing various activities are potentially a vat source of data for learning human locomotion. However, the raw pixels have no information about the state and actions of the human. To recover this, we first we run a computer vision tracking algorithm PHALP (35) to extract human trajectories in 3D. This provides an estimate of the 3D joints of the human body SMPL (23) parameters and a noisy estimate of the human joints in the world coordinates. We use the human body joint positions to retarget the motion to the humanoid robot using the inverse kinematics optimization described above. Once we retarget the motion from the Internet videos to humanoid trajectories, we filter the trajectories with the low optimization cost. Note that the scale of this data comes with the cost of being noisy.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      'We evaluate our approach on the challenging task of humanoid locomotion. We perform outdoor experiments on real hardware and systematic evaluations in simulation.\n' +
      '\n' +
      '### Experimental setup\n' +
      '\n' +
      '**Robot platform.** Digit is a humanoid robot platform developed by Agility Robotics. It is a full-sized humanoid that is 1.6m tall and weighs 45 kilograms. It has 30 degrees of freedom of which 20 are actuated. Due to its high dimensionality and four-bar linkage structure, it is challenging\n' +
      '\n' +
      'Figure 4: **Training dataset. To train our model, we construct a dataset of trajectories coming from four different sources. _(i) neural network policy:_ provides trajectories with complete observations and actions. _(ii) model-based controller:_ produces trajectories without actions. _(iii) motion capture of humans:_ does not contain actions and is approximately retargeted onto the robot. _(iv) internet videos of humans:_ noisy human poses are first reconstructed via 3D reconstruction and then approximately retargeted onto the robot.**\n' +
      '\n' +
      'to optimize fast which makes it particularly interesting for learning approaches that can learn efficiently from trajectory collections like ours.\n' +
      '\n' +
      '**Model.** Our model has a hidden size of \\(192\\) dimensions, with 4 layers of self-attention layers and MLP layers. Each self-attention has 4 heads. We use LayerNorm before each attention layer and ReLU activation after the MLP layer. We use a BatchNorm layer to process the input before the transformer model. When predicting a token at time \\(k\\), to keep the context length at a reasonable size, we only keep the past 16 steps in input. In Section 5.9, we show the model is able to scale up to more parameters and longer context length and achieve higher performance.\n' +
      '\n' +
      '### Real-world deployment\n' +
      '\n' +
      'We begin by reporting the results of deploying our policy in the real world. Specifically, we evaluate deploying our robot at various locations in San Francisco over the course of one week. Please see Figure 1 for examples and project page for videos. We find that our policy is able to walk over a variety of surfaces including walkways, concrete, asphalt, tiled plazas, and dirt roads. Note that the deployment in a large city environment, like San Francisco, is considerably more challenging than in constrained environments. The city environment is much more crowded, less patient, and not forgiving. This makes the error tolerance low and requires a policy that works consistently well.\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      'We evaluate locomotion policies with two metrics: _tracking error_ and _prediction error_. Tracking error measures how accurately the robot follows a specific locomotion command. The prediction error is the next token prediction loss measured on a separate set of validation data. We introduce two metrics with details as follows and show that two metrics can consistently predict locomotion performance.\n' +
      '\n' +
      '**Tracking error.** In all experiments, the robot starts from rest in a simulated environment and is issued a constant natural walking command consisting of a desired heading velocity sampled in \\([0.35,0.70]\\) m/s, angular velocity sampled in \\([-0.4,0.4]\\) rad/s, and zero lateral velocity. We compute \\(\\mathbf{x}^{*}(t)\\), the ideal robot base position trajectory that fully satisfies the velocity command \\(\\mathbf{v}^{*}(t)\\) at all time steps. To measure the accuracy of command tracking, we define the position tracking error as \\(\\frac{1}{T}\\sum_{t=0}^{T}\\|\\mathbf{x}(t)-\\mathbf{x}^{*}(t)\\|\\). We use the MuJoCo simulator (41) for evaluations, and all trajectories last for a duration of 10 seconds.\n' +
      '\n' +
      '**Prediction error.** Since the model is trained with the next token prediction, we test the prediction error on a set of validation data that is separated from training data and contains state-action trajectories collected from the RL policy. This is similar to the language modeling evaluation for large language models (14). We test both state and action prediction errors and add them together as the final error metric.\n' +
      '\n' +
      '### Comparison to the state of the art\n' +
      '\n' +
      '**Trajectory Adherence.** We compare our policy to a neuralfig:tracking network controller trained with reinforcement learning (RL) (33). Figure 5 presents a visual comparison of the trajectory adherence of our controller against these state-of-the-art baselines. Starting with a robot at the origin, we plot the actual trajectory of the robot with eleven different yaw commands selected from \\(\\{0.00,\\pm 0.05,\\pm 0.10,\\pm 0.20,\\pm 0.30,\\pm 0.40\\}\\) rad/s. For each policy, we jointly plot the desired and actual path traced by the robot base. Our model exhibits superior tracking to the RL controller at all turning speeds, and has near-perfect tracking for straight-line walking.\n' +
      '\n' +
      'Figure 5: **Comparison to state of the art, trajectory adherence. The robot is commanded to walk starting from the origin with a fixed heading command of \\(0.5\\) m/s and varying yaw commands in \\([-0.4,0.4]\\) rad/s. We plot the desired (dotted) and actual (solid) trajectories for our policy and a reinforcement-learning trained policy (RL).**\n' +
      '\n' +
      'Figure 6: **Tracking error comparisons. We measure the tracking error of our policy against a state-of-the-art benchmark (left), as well as the improvement produced by complementing action-labeled RL trajectories with action-free trajectories (right).**\n' +
      '\n' +
      '**Quantitative Evaluation.** In Figure 6, left, we repeat the above comparison to the RL controller \\((N=245)\\), with the full range of heading and yaw velocities mentioned in Section 5.3. We plot the mean position tracking error, binned by the commanded angular yaw. While both models have lower tracking errors at lower yaw, ours consistently outperforms the baseline RL policy. This is an interesting result, since our model was trained on next token prediction on trajectories produced by this very policy.\n' +
      '\n' +
      '### Prediction error correlates with performance\n' +
      '\n' +
      'We collect 14 models trained with different training recipes, model architectures, data size and types, and test tracking error and prediction error for each one of them. We plot the tracking and prediction errors of all the models into a single scatter plot, as shown in Figure 7. We can see that tracking and prediction error are highly correlated with Pearson coefficient \\(r=0.87\\), which means models with lower prediction error on the validation set likely follow different commands with higher accuracy. This suggests that the prediction error is predictive task performance.\n' +
      '\n' +
      '### Gait quality\n' +
      '\n' +
      'In humanoid locomotion, the smoothness in the robot\'s gait is contingent on the rhythmic functioning of its actuated knee joints. One way to measure this is a phase portrait, which is a parametric plot of a joint\'s generalized position and velocity over time. Patterns in the plot can reveal information about the type of movement the joint is undergoing. For example, a cyclic pattern may indicate repetitive motion, while irregular patterns might suggest complex or varied movements, such as stumbling. In Figure 8, we command the robot to walk forward at \\(0.5\\) m/s, and plot the associated phase portrait of its left knee joint. Notice that our policy retains the overall shape of the RL policy while having fewer aberrations. This supports our qualitative assessment of the more regularized behavior seen on our policy.\n' +
      '\n' +
      '### Generalization to unseen commands\n' +
      '\n' +
      'We find that our policy also extrapolates new skills such as walking backward, which was _not_ included in the action-labeled training data. As Figure 9 illustrates, by prompting our controller with negative values for the heading command, we find that the robot naturally performs backward walking at speeds up to 0.5 m/s without falling.\n' +
      '\n' +
      '### Training with action-free data\n' +
      '\n' +
      'One of the benefits of our approach is that it can be applied to trajectories from diverse sources, including missing information like actions in the case of human videos from YouTube. In Figure 6, right, we compare the performance of training only with complete trajectories to joint training on both complete and incomplete trajectories. We observe that including incomplete trajectories consistently leads to better performance. This is a promising signal for scaling our approach to a large collection of diverse trajectories.\n' +
      '\n' +
      'Figure 8: **Gait quality.** We command the robot with a heading velocity of \\(0.5\\) m/s and plot the resulting phase portrait of the left knee joint. Compared to the RL policy, our policy features fewer irregularities and a smoother, cyclic gait.\n' +
      '\n' +
      'Figure 7: **Prediction error correlates with performance.** We plot the tracking error and prediction error for 14 models. The prediction error linearly correlates with task tracking error with \\(r=0.87\\), which means lower prediction loss likely indicates more accurate command following.\n' +
      '\n' +
      'Figure 9: **Unseen commands.** Our policy is able to follow backward commands at test time, unseen during training.\n' +
      '\n' +
      '### Scaling studies\n' +
      '\n' +
      '**Training data.** In Figure 10, left, we study the scaling of our model\'s performance by increasing the size of the training dataset. We find that training on more trajectories reduces position tracking error, which is a positive signal for increased performance when training on larger datasets.\n' +
      '\n' +
      '**Context length.** We study the effect of increasing the number of tokens used in the context window of the transformer policy, varying it between 16, 32, and 48 steps in Figure 10 middle. Larger context windows produce better policies, which suggests that our generative policy performs a form of in-context adaptation that improves with scale.\n' +
      '\n' +
      '**Model size.** We compare models with increasing number of parameters (1M, 2M, 8M) by varying the embedding dimension (144, 192, 384), number of attention heads (3, 4, 12), and number of transformer blocks (4, 4, 6) respectively. Tracking error monotonically decreases with model size.\n' +
      '\n' +
      '### Ablation studies\n' +
      '\n' +
      '**Concatenated _vs._ separate tokens.** For the input of transformer, we can either concatenate observation and action at each step into a single token, or embed them into two separate tokens. We compare these two choices in Table 1. We can see that concatenation has lower prediction error while separating tokens has lower tracking error. Overall these two perform comparably while using separate tokens doubles the input length and introduces computation overhead.\n' +
      '\n' +
      '**Modality-aligned _vs._ non-aligned prediction.** When we use separate tokens for observation and actions as input, we can either predict \\(\\widehat{o}_{i+1}\\) from \\(o_{i}\\) and \\(\\widehat{a}_{i+1}\\) from \\(a_{i}\\), which aligns modality between prediction and input, or we can predict \\(\\widehat{o}_{i+1}\\) from \\(a_{i}\\) and \\(\\widehat{a}_{i+1}\\) from \\(o_{i+1}\\), which does not have alignment. From Table 1, we can see that modality alignment has clearly better performance than no alignment. We suspect this is because, at \\(t\\)-th step during inference, when predicting action of \\((t+1)\\)-th step, since there is no alignment, we need to first predict \\(\\widehat{o}_{i+1}\\) and use this prediction as input to predict \\(\\widehat{a}_{i+1}\\). If the predicted \\(\\widehat{o}_{i+1}\\) is not accurate compared to real \\(o_{i+1}\\) (which is used to predict \\(\\widehat{a}_{i+1}\\) during training), there will be a discrepancy between test and training data which will cause error in action prediction.\n' +
      '\n' +
      '**Joint training _vs._ staged training.** Given both complete data with action and incomplete data without action, we can either jointly train on both data as described in Section 3, or we can first pre-train the model on all the data with state prediction only, then fine-tune the model on complete data with action prediction. We compare these two approaches in Table 1. We observe no significant difference between these two, which indicates that pre-training on state prediction then fine-tuning on action prediction also gives a reasonable locomotion policy.\n' +
      '\n' +
      '**State-action prediction _vs._ action-only prediction.** We compare the performance of our policy when trained with only predicting actions, versus when trained with predicting both states and actions. The results in Table 1 show that the state-action prediction improves model performance on trajectory tracking. We hypothesize that the additional learning signal enables the model to learn richer representations of the world that are beneficial for the locomotion task.\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      'We present a self-supervised approach for real-world humanoid locomotion. Our model is trained on a collection of sensorimotor trajectories, which come from prior neural network policies, model-based controllers, human motion capture, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in the real-world zero-shot. These findings suggest a promising path toward learning challenging real-world robot control tasks by generative modeling of large collections of trajectories.\n' +
      '\n' +
      'Figure 10: **Scaling studies. We find that our approach scales with the number of trajectories in the training dataset (left), context length (middle), and larger models (right).**\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'This work was supported in part by DARPA Machine Common Sense program, ONR MURI program (N00014-21-1-2801), NVIDIA, Hong Kong Centre for Logistics Robotics, The AI Institute, and BAIR\'s industrial alliance programs. We thank Saner Cakir and Vikas Ummadisetty for help with the inverse kinematics simulation experiments.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]K. Bousmalis, G. Vezzani, D. Rao, C. Devin, A. X. Lee, M. Bauza, T. Davchev, Y. Zhou, A. Gupta, A. Raju, et al. (2023) Robocat: a self-improving foundation agent for robotic manipulation. arXiv:2306.11706. Cited by: SS1.\n' +
      '* [2]A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, et al. (2022) Rt-1: robotics transformer for real-world control at scale. arXiv:2212.06817. Cited by: SS1.\n' +
      '* [3]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. In NeurIPS, Cited by: SS1.\n' +
      '* [4]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. A. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. NeurIPS. Cited by: SS1.\n' +
      '* [5]G. A. Castillo, B. Weng, W. Zhang, and A. Hereid (2021) Robust feedback motion policy design using reinforcement learning on a 3d digit bipedal robot. In IROS, Cited by: SS1.\n' +
      '* [6]M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever (2020) Generative pretraining from pixels. In ICML, Cited by: SS1.\n' +
      '* [7]M. Chignoli, D. Kim, E. Stanger-Jones, and S. Kim (2021) The mit humanoid robot: design, motion planning, and control for acrobatic behaviors. In Humanoids, Cited by: SS1.\n' +
      '* [8]J. Devlin, M. Chang, K. Lee, and K. Toutanova (2019) Bert: pre-training of deep bidirectional transformers for language understanding. In NAACL-HCT, Cited by: SS1.\n' +
      '* [9]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. (2020) An image is worth 16x16 words: transformers for image recognition at scale. In ICLR, Cited by: SS1.\n' +
      '* [10]D. Driess, F. Xia, M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. (2023) Palm-e: an embodied multimodal language model. arXiv:2303.03378. Cited by: SS1.\n' +
      '* [11]J. Engel, K. K. Agrawal, S. Chen, I. Gulrajani, C. Donahue, and A. Roberts (2019) Gansynth: adversarial neural audio synthesis. arXiv:1902.08710. Cited by: SS1.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & Track Err. & Pred. Err. \\\\ \\hline Joint training & **0.310** & 0.88 \\\\ Staged training & 0.311 & - \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Ablations on different design choices in modeling and training.** For each ablation we compare the average tracking error on a set of commands, as well as the next token prediction error on the test set. For a fair comparison, we do not report next token prediction error for models that only predict actions.\n' +
      '\n' +
      '* [12] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In _NeurIPS_, 2014.\n' +
      '* [13] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. Masked autoencoders are scalable vision learners. _arXiv:2111.06377_, 2021.\n' +
      '* [14] Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.\n' +
      '* [15] Hirai, K., Hirose, M., Haikawa, Y., and Takenaka, T. The development of honda humanoid robot. In _ICRA_, 1998.\n' +
      '* [16] Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In _NeurIPS_, 2020.\n' +
      '* [17] Hochreiter, S. and Schmidhuber, J. Long short-term memory. _Neural computation_, 1997.\n' +
      '* [18] Kajita, S., Kanehiro, F., Kaneko, K., Yokoi, K., and Hirukawa, H. The 3d linear inverted pendulum mode: A simple modeling for a biped walking pattern generation. In _IROS_, 2001.\n' +
      '* [19] Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. _arXiv:2001.08361_, 2020.\n' +
      '* [20] Kato, I. Development of wabot 1. _Biomechanism_, 1973.\n' +
      '* [21] Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Hornung, R., Adam, H., Akbari, H., Alon, Y., Birodkar, V., et al. Videopoet: A large language model for zero-shot video generation. _arXiv:2312.14125_, 2023.\n' +
      '* [22] Kuindersma, S. Recent progress on atlas, the world\'s most dynamic humanoid robot, 2020. URL [https://youtu.be/EGABAx52GKI](https://youtu.be/EGABAx52GKI).\n' +
      '* [23] Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., and Black, M. J. Smpl: A skinned multi-person linear model. In _Seminal Graphics Papers: Pushing the Boundaries, Volume 2_, 2023.\n' +
      '* [24] Mahmood, N., Ghorbani, N., Troje, N. F., Pons-Moll, G., and Black, M. J. AMASS: Archive of motion capture as surface shapes. In _ICCV_, 2019.\n' +
      '* [25] Makoviychuk, V., Wawrzyniak, L., Guo, Y., Lu, M., Storey, K., Macklin, M., Hoeller, D., Rudin, N., Allshire, A., Handa, A., et al. Isaac gym: High performance gpu-based physics simulation for robot learning. In _NeurIPS_, 2021.\n' +
      '* [26] Nelson, G., Saunders, A., Neville, N., Swilling, B., Bondaryk, J., Billings, D., Lee, C., Playter, R., and Raibert, M. Petman: A humanoid robot for testing chemical protective clothing. _Journal of the Robotics Society of Japan_, 2012.\n' +
      '* [27] Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. Wavenet: A generative model for raw audio. _arXiv:1609.03499_, 2016.\n' +
      '* [28] Plappert, M., Mandery, C., and Asfour, T. The KIT motion-language dataset. _Big Data_, 2016.\n' +
      '* [29] Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pre-training. 2018.\n' +
      '* [30] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. 2019.\n' +
      '* [31] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* [32] Radosavovic, I., Shi, B., Fu, L., Goldberg, K., Darrell, T., and Malik, J. Robot learning with sensorimotor pre-training. In _CoRL_, 2023.\n' +
      '* [33] Radosavovic, I., Xiao, T., Zhang, B., Darrell, T., Malik, J., and Sreenath, K. Real-world humanoid locomotion with reinforcement learning. _arXiv:2303.03381_, 2023.\n' +
      '* [34] Raibert, M. H. _Legged robots that balance_. MIT press, 1986.\n' +
      '* [35] Rajasegaran, J., Pavlakos, G., Kanazawa, A., and Malik, J. Tracking people by predicting 3d appearance, location and pose. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 2740-2749, 2022.\n' +
      '* [36] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-to-image generation. In _ICML_, 2021.\n' +
      '* [37] Shannon, C. E. Prediction and entropy of printed english. _Bell system technical journal_, 1951.\n' +
      '* [38] Shridhar, M., Manuelli, L., and Fox, D. Perceiver-actor: A multi-task transformer for robotic manipulation. In _CoRL_, 2022.\n' +
      '* [39] Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, 2015.\n' +
      '\n' +
      '* [40] Stasse, O., Flayols, T., Budhiraja, R., Giraud-Esclasse, K., Carpentier, J., Mirabel, J., Del Prete, A., Soueres, P., Mansard, N., Lamiraux, F., et al. Talos: A new humanoid research platform targeted for industrial applications. In _Humanoids_, 2017.\n' +
      '* [41] Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In _IROS_, 2012.\n' +
      '* [42] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In _NeurIPS_, 2017.\n' +
      '* [43] Wu, J., Zhang, C., Xue, T., Freeman, B., and Tenenbaum, J. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. In _NeurIPS_, 2016.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>