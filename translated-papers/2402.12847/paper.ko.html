<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 명령어-조정된 언어 모델은 더 나은 지식 학습자\n' +
      '\n' +
      ' Zhengbao Jiang\\({}^{2}\\) Zhiqing Sun\\({}^{2}\\) Weijia Shi\\({}^{1,3}\\) Pedro Rodriguez\\({}^{1}\\) Chunting Zhou\\({}^{1}\\)\n' +
      '\n' +
      'Graham Neubig\\({}^{2}\\) Xi Victoria Lin\\({}^{1}\\) Wen-tau Yih\\({}^{1}\\) Srinivasan Iyer\\({}^{1}\\)\n' +
      '\n' +
      '워싱턴대학교\n' +
      '\n' +
      '{zhengbaj,gneubig}@cs.cmu.edu {victorialin,scottyih,sviyer}@meta.com\n' +
      '\n' +
      '메타에서 인턴십을 하는 동안 대부분의 일이 이루어졌다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM) 기반 어시스턴트가 진화하는 정보 요구에 효과적으로 적응하기 위해서는, 새로운 데이터에 대한 지속적인 훈련을 통해 그들의 사실적 지식을 업데이트하는 것이 가능해야 한다. 이를 위한 표준 레시피는 새로운 문서에 대한 사전 교육을 계속한 후 질문-답변(QA) 쌍에 대한 지시 튜닝을 포함한다. 그러나 우리는 이 레시피로 훈련된 LLM이 문서의 복잡성을 최소화하더라도 질문에 답하는 데 어려움을 겪는다는 것을 발견했다. 우리는 QA 쌍이 일반적으로 간단한 반면 문서는 더 복잡하여 복잡한 방식으로 많은 사실적 진술을 함께 엮는다는 것을 발견했다. 따라서 우리는 복잡한 문서로부터 지식을 인코딩하는 과정이 질문을 통해 이 지식이 어떻게 액세스되는지 고려하도록 LLM을 문서에 대한 QA 쌍 _before_ 계속 사전 훈련에 노출시키는 것이 유익하다고 가정한다. 이를 바탕으로 문서에 대한 학습에 앞서 질문에 대한 학습을 진행하는 방법인 **pre-instruction-tuning(PIT)**을 제안한다. 이는 문서에 대한 교육 후 지식을 추출하는 방법을 학습하는 표준 명령어-튜닝과 대비된다. 광범위한 실험 및 절제 연구는 PIT가 새로운 문서로부터 지식을 흡수하는 LLM의 능력을 크게 향상시켜 표준 명령어 튜닝을 17.8% 능가한다는 것을 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델(LLM)은 대규모 사전 훈련을 통해 방대한 양의 사실적 지식을 매개변수에 저장하고 있으며, 이 지식은 "세계 최대 빙상이 어디에 위치하는가"와 같은 다양한 질문에 답하는 데 사용될 수 있다(Brown et al., 2020; OpenAI, 2023; Chowdhery et al., 2022; Zhang et al., 2022; Touvron et al., 2023, 2023; Gemini Team, 2023). 그러나 이러한 사실적 지식은 정적이며, 이는 세계가 진화함에 따라 구식화되거나 LLM이 전문 또는 개인 영역에서 사용될 때 불충분하다는 것을 입증할 수 있음을 의미한다.\n' +
      '\n' +
      'LLM들을 최신 상태로 유지하기 위해, 지식들을 파라미터들에 저장하기 위해 새로운 문서들에 대한 사전 트레이닝을 계속하는 것이 일반적이며, 이는 LLM들이 최신 정보를 필요로 하는 질의들에 효과적으로 응답할 수 있게 한다(Jang et al., 2022). 널리 알려진 견해는 파라미터에 저장된 사실적 지식이 프롬프트를 통해 도출될 수 있고(Brown et al., 2020; Petroni et al., 2019; Roberts et al., 2020), 명령어-튜닝(지도된 미세-튜닝 또는 정렬이라고도 함)이 이러한 도출을 더 효과적으로 만든다는 것이다(Sanh et al., 2022; Wei et al., 2022; Ouyang et al., 2022). 본 논문의 첫 부분(SS 4)에서는 Llama-2(Touvron et al., 2023)를 사용하여 광범위한 실험을 수행하여 다음과 같은 질문에 답한다. _다음 명령-tuning_가 있거나 없는 새로운 문서에 대한 사전 훈련을 계속함으로써 현대 LLMs에 저장된 지식을 어느 정도 증가시킬 수 있는가? 우리는 LLMs가 복잡성이 1로 최소화될 정도로 문서에 대해 반복적으로 훈련함에 따라 LLMs가 올바르게 답변하는 문서에 대한 질문 비율이 27.6%로 일관되게 증가한다는 것을 발견했다. 후속 명령어 조정은 30.3%로 더 개선되어 LLMs.1에서 더 많은 지식을 이끌어내는 데 이 널리 사용되는 관행이 유용하다는 것을 확인시켜준다. 그러나, 문서의 복잡성을 최소화하더라도 유도된 지식의 양은 여전히 제한적이며, 우리가 "복잡성 저주"라고 부르는 현상이다.2\n' +
      '\n' +
      '각주 1: 이 용량은 비교적 작은 LMs 또는 무작위로 초기화된 변압기를 사용하거나 철저한 훈련 또는 지시-튜닝의 부족으로 인해 이전 작업에 의해 과소평가될 수 있다(Wang et al., 2021; Hu et al., 2023; Zhu and Li, 2023).\n' +
      '\n' +
      '각주 2: Berglund et al.(2023)의 "역저주"에 의해 영감을 받았다.\n' +
      '\n' +
      '논문의 두 번째 부분(SS 5)에서는 LLM을 문서로부터 지식을 흡수하는 데 더 능숙하게 만들어 당혹스러운 저주를 완화시키는 방법을 연구한다. Zhu와 Li(2023)는 전기와 관련 질문의 혼합에서 무작위로 초기화된 변압기를 처음부터 훈련하는 것이 새로운 질문으로 강력한 일반화를 초래한다는 흥미로운 발견을 제시했다. 그러나 이러한 발견의 이유를 이해하고 새로운 문서로부터 지식을 흡수하기 위해 실제로 적용할 수 있는 방법을 탐구하는 것은 추가 조사가 필요하다. 우리는 질문-답변(QA) 쌍이 일반적으로 간단하고 쉽게 소화될 수 있는 반면, 문서는 더 복잡하고 어수선하며 종종 더 복잡한 방식으로 많은 사실적 진술을 함께 엮는 경향이 있다는 것을 발견했다. 따라서 우리는 복잡한 문서로부터 지식을 인코딩하는 과정이 질문_을 통해 이 지식이 어떻게 액세스되는지 고려하도록 문서에 대한 사전 교육을 계속하기 전에 LLM을 QA 데이터에 의도적으로 노출시키는 것이 유익하다고 가정한다. 이를 **사전 명령어 조정(PIT)**라고 하며 이 방법의 다양한 변형을 벤치마킹하기 위해 포괄적인 실험을 수행한다. 도 1에 도시된 바와 같다. 도 1에서, 우리의 가장 성능이 좋은 변형은 지식이 어떻게 액세스되는지 파악하기 위해 QA 쌍(예를 들어, "오펜하이머의 편집을 처리한 사람")에 대한 독점적인 훈련으로 시작한다. 이어서, 이들 QA 쌍들 및 관련 문서들(예를 들어, "오펜하이머의 편집을 처리한 사람" 및 "오펜하이머"에 관한 문서)의 조합에 대한 트레이닝이 뒤따른다. 이 단계에서 LLM은 정보 밀도가 높은 문서에서 지식을 흡수하는 능력을 향상시켜 이미 마스터한 QA 쌍을 구축한다. 지속적인 지식 획득을 연구하기 위해 위키2023이라는 데이터 세트를 구축하는데, 위키2023에 대한 포괄적인 실험은 PIT 이후 LLM이 새로운 문서(예: "바비"에 관한 문서)로부터 지식을 흡수하는 향상된 능력을 보인다는 것을 보여준다. 상세한 절제 연구는 이러한 능력이 주로 문서로부터 지식을 인코딩하기 위해 학습보다 지식에 접근하는 방법을 배우는 데 우선순위를 두는 것에서 비롯된다는 것을 보여준다. 전반적으로, PIT는 표준 명령어 조정 접근법(SS 5.1 및 SS 5.2)보다 크게 향상되어 LLama-2 7B(30.3%\\(\\sim\\) 48.1%)에서 17.8%, LLama-2 70B(46.4%\\(\\sim\\) 62.7%)에서 16.3%의 QA 정확도를 향상시켰다. 더욱이, PIT는 또한 _different_ 도메인의 문서로부터 지식을 흡수하는 능력을 향상시켜, 이 방법을 보다 강력한 일반화를 위한 더 다양한 문서 및 명령으로 확장할 가능성을 조명한다(SS 5.4).\n' +
      '\n' +
      '지속적인 지식 습득을 위한 데이터세트 구축\n' +
      '\n' +
      'LLM이 새로운 문서로부터 지식을 학습할 수 있는 능력을 평가하기 위해서는 원래의 사전 훈련 말뭉치와의 중첩을 최소화한 문서 말뭉치를 사용하는 것이 필수적이다. 이것은 LLM이 질문에 정확하게 답할 때, 우리는 이 능력을 원래의 사전 훈련 코퍼스에서 유사한 질문을 접하기보다는 새로운 문서로부터의 학습에 자신 있게 돌릴 수 있다는 것을 보장한다. 이 섹션에서는 위키피디아에서 이러한 말뭉치를 구축하기 위한 방법론을 설명한다.\n' +
      '\n' +
      '# 위키2023 문서 말뭉치\n' +
      '\n' +
      '다음 실험(SS 4 및 SS 5)에서는 Llama-2(7B 및 70B) [12]가 가장 성능이 좋은 LLMs 중 하나이기 때문에 사용한다. 우리는 "2023" 범주에 속하는 위키피디아 기사를 사용한다. 영화, 예술, 경제, 정치, 사건 등과 같은 다양한 영역의 주제를 포함하는 카테고리 3 이 사실 정보가 아닐 가능성\n' +
      '\n' +
      '그림 1: 평가 문항에 대한 정확도와 함께 지속적인 사전 훈련(첫 번째 행), 지속적인 사전 훈련 후 명령어-튜닝(두 번째 행), 지속적인 사전 훈련 전 사전 명령어-튜닝(마지막 행)의 그림이다. 오른쪽을 가리키는 밝은 파란색 삼각형은 훈련 단계를 나타냅니다.\n' +
      '\n' +
      ' 원본 트레이닝 코퍼스에 포함된 것은 탭에서의 낮은 QA 성능에 의해 지원된다. 1(7B/70B의 경우 9.5%/17.2%)4 교육 과정을 가속화하기 위해 각 기사의 첫 번째 섹션만 사용하며, 이는 철저한 요약과 많은 사실적 진술을 포함한다. 수집된 문서의 수 및 "오펜하이머"에 관한 예시 문서는 도에서 찾을 수 있다. 도 2 및 도 3. 우리는 이것을 위키2023 데이터세트라고 지칭한다.\n' +
      '\n' +
      '각주 4: 위키2023과 라마-2의 사전 훈련 말뭉치 사이의 사실적 중복을 완전히 피하기 어렵다는 점에 주목하는 것이 중요하다. 예를 들어, 2023년에 개봉된 영화는 2023년 이전에 정보를 입수할 수 있었을 수 있으며, 데이터 중복 검출은 본 연구의 초점을 벗어나는 적극적인 연구 방향이다.\n' +
      '\n' +
      '# 위키2023 질의응답 쌍\n' +
      '\n' +
      '지시튜닝이나 성능평가를 위한 QA 쌍을 수집하기 위해 공개 LLM을 사용하여 프롬프트 1에 따라 다양한 질문과 해당 기사를 컨텍스트로 하여 각 질문에 대한 답변을 생성했으며, 각 기사에 대해 평균 4.93개의 질문이 생성되었다. 도. 도 2 및 도 2를 참조하여 설명한다. 도 3은 각각 "오펜하이머"에 대한 상세한 통계 및 예시적인 QA 쌍을 도시한다.\n' +
      '\n' +
      '### Splits\n' +
      '\n' +
      '모든 영역 중 평가를 위해 필름 영역을 선택하고 테스트 분할(Wiki2023-필름-테스트)로 256개의 기사를 무작위로 선택한다. 우리는 테스트 스플릿(Wiki2023-film-test-doc)의 문서에 대해 LLM을 지속적으로 훈련하고, 해당 질문의 정확도(Wiki2023-film-test-QA)를 기반으로 성능을 평가한다. 나머지 1720개 기사 및 해당 QA 쌍(Wiki2023-필름-트레인)은 그림 2의 도메인 내 설정에 해당하는 다른 훈련 전략을 연구하는 데 사용될 것이다. 또한 그림 2의 도메인 교차 설정에 해당하는 도메인 간 다른 방법의 효과를 연구하기 위해 필름 도메인에 대한 평가 전에 다른 도메인에 대해 훈련한다.\n' +
      '\n' +
      '도 3: 위키2023으로부터의 "오펜하이머" 및 대응하는 QA 쌍들에 관한 예시적인 문서. 컴퓨팅 손실들에 사용되는 토큰들은 녹색으로 강조된다.\n' +
      '\n' +
      '그림 2: 위키2023 데이터세트. **상단-우측**: 문서 및 QA 쌍의 수; **상단-좌측**: 질문의 빈발 키워드; **하단**: 문서, 질문 및 답변의 토큰 카운트 분포.\n' +
      '\n' +
      'Experimental Settings\n' +
      '\n' +
      '### Objectives\n' +
      '\n' +
      '문서에 대한 학습 시, <bos> 토큰을 작성하고, 문서의 모든 토큰에 대해 평균하여 표준 다음-토큰 예측 손실을 계산한다: \\(L_{\\mathbf{d}}=-\\sum_{t}\\log P(\\mathbf{d}_{t}|\\mathbf{d}_{<t})/|\\mathbf{d}|\\.5 QA 쌍에 대한 학습 시, 질문의 접두사로 주어진 답변에서 토큰에 대해서만 평균 음의 로그-우도 손실을 계산한다: \\(L_{\\mathbf{a}=-\\sum_{t}\\log P(\\mathbf{a}_{t}|\\mathbf{q},\\mathbf{a}_{t})/|\\mathbf{a}||\\). 도. 도 3은 컴퓨팅 손실에 사용되는 토큰이 강조되는 QA 쌍과 함께 예시적인 문서를 나타낸다.\n' +
      '\n' +
      '각주 5: 우리는 전체 조문의 결론을 의미하지 않는 첫 번째 섹션만을 사용하기 때문에 문서 끝에 <eos> 토큰을 추가하지 않는다.\n' +
      '\n' +
      '### Hyperparameters\n' +
      '\n' +
      '본 논문에서는 AdamW Loshchilov와 Hutter (2019)를 이용하여 \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.95\\), 가중치 감쇠 0.1을 이용하여 학습률을 초기값의 10%까지 감쇠시켰다. 문서에 대한 사전 학습 시, 256개의 문서의 배치 크기와 3e-5의 초기 학습률을 이용하며, QA 쌍에 대한 명령어 튜닝 시, 256개의 QA 쌍의 동일한 배치 크기를 이용하지만, 계산 손실에 사용되는 단일 배치의 토큰 수가 더 적기 때문에 5e-6의 감소된 초기 학습률을 선택한다. 에폭의 수는 설정에 따라 달라지며 해당 섹션에 자세히 설명되어 있다.\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      '추론 시간에는 탐욕 디코딩을 사용하여 그림 3의 형식을 따라 주어진 질문에 대한 답변을 컨텍스트로 생성하고 원본 Llama-2를 평가하기 위해 5개의 QA 쌍을 컨텍스트 내 예제로 추가하여 QA 형식을 따르는지 확인한다. 대부분의 질문은 단순한 팩토이드 질문이고 대부분의 답변은 비교적 짧기 때문에, 모델의 출력이 정규화(예: 기사 및 구두점 제거) 후에 정확하게 금 답과 일치하는지 여부를 측정하는 주요 메트릭 Kwiatkowski et al. (2019)로 정확한 일치(EM)를 사용한다. 더 긴 응답을 평가하고 작은 어휘 차이를 수용하기 위해 모델의 출력에 금 답변이 나타나는지 평가하는 답변 리콜과 모델의 출력과 금 답변 사이의 가장 긴 공통 서브시퀀스를 측정하는 ROUGE-L을 보고한다.\n' +
      '\n' +
      '##4 얼마나 많은 지식이 LLM을 할 수 있는지\n' +
      '\n' +
      '지속적인 사전 교육을 통한 흡수\n' +
      '\n' +
      '그다음이 지시 조율인가요?\n' +
      '\n' +
      'LLM의 파라미터에 저장된 사실적 지식은 추가적인 훈련 없이 프롬프트를 통해 질문에 대한 답변에 액세스되고 적용될 수 있다. Brown et al.(2020); Petroni et al.(2019); Jiang et al.(2020); Roberts et al.(2020). 고품질 데이터 Sanh et al.(2022); Wei et al.(2022)에 대한 추가적인 명령어-튜닝(supervised fine-tuning)으로, 지식이 LLMs로부터 더 효과적으로 유도되는 것으로 보인다. 그러나 LLM이 질문에 정확하게 답할 때 사전 훈련 데이터의 다양성으로 인해 지식의 출처가 불분명하다. 예를 들어, "세계에서 가장 큰 빙상이 어디에 위치하는가"라는 질문에 답할 때, LLMs는 남극 빙상에 대해 본 문서에서 정보를 회상하고 일반화하여 응답을 도출하거나, 훈련 데이터에서 접한 유사한 질문의 답변을 반복하는 것일 뿐인가? 전자의 시나리오는 나중에 이끌어낼 수 있는 방식으로 문서를 이해하고 매개변수를 효과적으로 저장할 수 있는 능력을 내포하는 반면 후자는 단순한 암기에 불과하기 때문에 이러한 구분이 중요하다.\n' +
      '\n' +
      '여러 연구에서 이 문제를 연구했으며 주요 발견은 LMs가 Wang et al. (2021), Zhu 및 Li (2023)에 대해 훈련된 문서에 대한 질문에 답하기 위해 고군분투한다는 것이다. 그러나 이러한 실험은 주로 BART, T5, 또는 GPT-2 Wang et al. (2021); Jang et al. (2022); Hu et al. (2023), 랜덤하게 초기화된 트랜스포머 Zhu 및 Li (2023), 또는 명령어-튜닝 Ovadia et al. (2023)과 같은 비교적 작은 LM을 사용하여 수행되었다는 점에 유의하는 것이 중요하다. 이를 통해 우리는 새로운 문서로부터 지식을 흡수하고 표준적인 사전 훈련에 이어 명령어 조정 레시피_를 사용하여 새로운 문서에 대한 질문에 답하는 현대 LLM의 실제 한계가 무엇인지 궁금하다. 이 섹션에서는 위키2023 필름에서 Llama-2 7B 및 70B를 사용하여 광범위한 실험을 실행하여 한계를 테스트한다.\n' +
      '\n' +
      '바닐라 연속 사전교육 및 교육튜닝\n' +
      '\n' +
      '실험 설정은 두 가지 표준 설정으로 실험하고 관련 질문에 답하여 성능을 평가한다.\n' +
      '\n' +
      '* 계속된 사전 훈련: 지시-동조 없이 시험 문서 상에서 훈련(도 4) .6* 표준 명령-튜닝: 열차 QA 쌍에 대한 명령-튜닝 전 열차 및 테스트 문서 모두에서 열차\n' +
      '\n' +
      '우리는 단일 에폭에 대해 명령어 튜닝을 수행하는데, 이는 더 많은 에폭들이 일반적으로 성능 저하를 초래하기 때문이다. 문서에 대한 교육을 위해 우리는 효과적인 지식 습득을 가능하게 하고 적당한 크기의 코퍼스에 대해 저렴하게 유지하는 다중 에포크(7B/70B 모델의 경우 10/5)를 선택한다.\n' +
      '\n' +
      '실험 결과는 탭과 같다. 도 1을 참조하면, 원본 Llama-2 모델의 상대적으로 낮은 성능(7B/70B의 경우 9.5%/17.2%)은 테스트 문서에 대한 대부분의 지식이 원본 사전 훈련 코퍼스에 포함되지 않음을 나타낸다. 문서에 대한 사전 교육을 계속한 후, 수행도는 27.2%/41.7%로 증가하여 LLM이 어느 정도의 지식을 흡수할 수 있음을 나타낸다. 명령어 튜닝은 성능을 30.3%/46.4%로 더욱 증가시켜 이 표준 레시피의 효과를 확인시켜준다. 이 관찰은 Zhu 및 Li(2023)와 다르며, 이는 사전 훈련 후 지시 튜닝이 무작위로 초기화된 GPT-2 유사 변압기에서 효과가 없음을 보여준다. 이 차이는 아마도 라마-2가 원시 문서와 QA 데이터로 구성된 다양한 말뭉치에 대한 사전 교육을 통해 질문을 통해 매개변수에서 지식을 추출하는 데 어느 정도 숙련도를 개발했기 때문일 것이다. 또한 해당 문서가 Llama-2에게 문맥("탭 1의 오픈북 w/ doc")으로 직접 제공되는 성능을 보고한다. 닫힌 책과 열린 책 설정 사이의 상당한 차이는 LLM의 매개변수에서 지식을 검색하는 것이 여전히 어렵다는 것을 시사한다.\n' +
      '\n' +
      '### 훈련동역학의 분석: 복잡성과 일반화\n' +
      '\n' +
      '문서의 당혹감을 낮추면 관련 질문에 대한 답변으로 어떻게 일반화될 수 있는가? 우리는 에폭의 수를 변화시킨다. 도 5(a) 및 학습률(도. 5(b)) 문서에 대한 지속적인 사전 훈련 및 훈련 역학 연구를 위한 세 가지 메트릭 모니터링.7\n' +
      '\n' +
      '각주 7: 우리는 항상 학습률을 초기값의 10%로 붕괴시키기 때문에, 더 많은 에폭에 대한 트레이닝은 더 적은 에폭 후에 획득된 체크포인트로부터 트레이닝을 계속하는 것과 동일하지 않다.\n' +
      '\n' +
      '정확하게 일치하여 측정된 테스트 질문에 대한 ** 지식 획득** QA 정확성.\n' +
      '문서들의 복잡도** 문서들 내의 모든 토큰들에 대해 복잡도(PPL)를 계산한다.\n' +
      '***지식 보유** 자연 질문(NQ) 데이터 세트에 대한 QA 정확도를 평가하여 사전 훈련 동안 축적된 지식의 보유를 근사화한다. NQ는 2019년에 출시되었으며, 주로 당시 위키피디아 기사를 기반으로 한 질문을 포함한다.\n' +
      '\n' +
      '### Experiment results\n' +
      '\n' +
      '*에 도시된 바와 같다. 도 5의 (a)를 참조하면, QA 정확도는 Perplexity가 1에 접근함에 따라 일관되게 향상되며, 이는 _factual 지식 학습이 모든 토큰_에 대한 철저한 손실 최소화를 필요로 함을 나타낸다. 이것은 지나치게 최적화하면 과적합으로 이어지는 일반적인 기술을 배우는 것과 대조된다.\n' +
      '\n' +
      '그림 4: 본 논문에서 살펴본 다양한 실험 설정. 각 행은 고유한 이름과 번호를 가진 다른 실험 설정을 나타내며 오른쪽을 가리키는 밝은 파란색 삼각형으로 강조 표시된 각 수직 섹션은 훈련 단계를 나타낸다. 모델은 모든 설정에 걸쳐 테스트 QA에서 평가됩니다. 여러 데이터 세트가 점선 사각형 안에 동봉될 때마다 훈련 과정에서 함께 혼합된다.\n' +
      '\n' +
      '*에 도시된 바와 같다. 도 5(a) 및 도 6을 참조하면, 도 5의 (b)를 참조하면, LLM들이 문서들에 대한 당혹감을 최소화한 모든 경우들 중에서, 더 많은 에포크들 또는 더 큰 학습률로 훈련된 경우들은 전형적으로 우수한 QA 성능을 나타낸다. 우리는 더 적극적인 훈련이 문서의 기만적인 패턴에 덜 적합하고 질문에 응답할 때 더 나은 일반화로 이어진다고 가정한다.\n' +
      '\n' +
      '요약하면, 더 낮은 당혹감은 질문에 응답할 때 더 강력한 일반화로 이어지지만 이전에 습득한 지식을 잊어버리는 것을 희생한다.\n' +
      '\n' +
      '##5 문서지식 흡수에서 LLMs 향상\n' +
      '\n' +
      '표준 지도-조정을 통해 도출된 지식의 양은 여전히 제한적이며, 비록 문서의 당혹성을 최소화하더라도 우리가 "당혹 저주"라고 지칭하는 현상이다. 우리의 다음 질문은 당혹스러운 저주를 완화하기 위해 문서에서 지식을 흡수하는 LLM의 능력을 어떻게 향상시킬 수 있는가이다. 주요 과제는 지식이 원시 문서에서 제시되는 방식과 질문 응답을 통해 접근하는 방식 사이의 차이이다. 우리는 QA 쌍이 일반적으로 간단한 반면 문서는 더 복잡하고 어수선하여 더 복잡한 방식으로 많은 사실적 진술을 함께 엮는 경향이 있음을 발견했다. 을 이용하여 형성하는 것을 특징으로 하는 반도체 소자의 제조 방법. 도 3을 예로 들면, "오펜하이머 편집을 누가 담당했는가"라는 질문에 대한 답변은 "오펜하이머"를 명시적으로 언급하지 않은 "편집은 제니퍼 레임...에 의해 처리되었다"라는 글의 중간에 있는 문장에 포함되어 있다. 훈련 중에 LLM은 문맥을 이해하고 "편집"이 매개변수에서 이 지식을 효과적으로 인코딩하기 위해 "오펜하이머의 편집"을 의미한다고 추론해야 한다.\n' +
      '\n' +
      'Zhu와 Li(2023)는 합성 전기에서 무작위로 초기화된 GPT-2 유사 변압기를 처음부터 훈련하여 이 문제를 연구하고 개인에 대한 질문에 답하는 능력을 평가했다. 그들은 그 중 절반에 관련된 전기와 질문의 혼합에 대한 훈련을 발견했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c|c c c} \\hline \\hline  & \\multicolumn{2}{c|}{**Llama-2 7B**} & \\multicolumn{2}{c}{**Llama-2 70B**} \\\\\n' +
      '**Settings** & **EM Rec. R-L** & **EM Rec. R-L** & **EM Rec. R-L** \\\\ \\hline \\multicolumn{6}{l}{_closed- and open-book performance before training_} \\\\ closed-book & 9.5 & 10.0 & 21.2 & 17.2 & 18.1 & 31.4 \\\\ open-book w/ doc & 72.2 & 75.4 & 91.5 & 78.2 & 80.6 & 94.9 \\\\ \\hline \\multicolumn{6}{l}{_closed-book performance w/ standard methods_} \\\\ cont. pre-training \\(\\Uparrow\\) & 27.6 & 31.6 & 43.8 & 41.7 & 45.8 & 60.2 \\\\ +instruction-tuning \\(\\Uparrow\\) & 30.3 & 34.7 & 47.4 & 46.4 & 50.9 & 64.1 \\\\ mix all data \\(\\Uparrow\\) & 39.4 & 44.6 & 56.7 & 57.1 & 63.4 & 72.4 \\\\ \\hline \\multicolumn{6}{l}{_closed-book performance w/ pre-instruction-tuning (PIT)_} \\\\ PIT (QA only) \\(\\Uparrow\\) & 28.6 & 32.7 & 45.2 & 49.7 & 53.7 & 67.9 \\\\ PIT (QA \\(\\rightarrow\\) docs) \\(\\Uparrow\\) & 32.5 & 37.2 & 49.0 & 54.6 & 60.0 & 73.8 \\\\ PIT \\(\\Uparrow\\) & **45.4** & **51.2** & **63.2** & **62.7** & **68.6** & **78.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 표준 명령어-튜닝과 사전 명령어-튜닝 간의 QA 성능(%) 비교. 가장 좋은 결과는 굵은 글씨입니다. 레크. 는 답변 리콜의 줄임말이고, R-L은 ROUGE-L을 의미한다.\n' +
      '\n' +
      '도 5: 에폭의 수를 변화시킨다(도 5). 도 5(a) 및 학습률(도. 5(b)) 라마-2 7B의 훈련 역학을 연구하기 위한 지속적인 사전 훈련 동안. 왼쪽 축은 정확한 일치로 측정된 테스트 질문에 대한 QA 정확도이다. 오른쪽 축에는 문서에 있는 모든 토큰의 복잡성과 자연 질문 데이터 세트에 QA 정확도로 측정한 지식 보유 정확도의 두 가지 뚜렷한 색상으로 표시된 메트릭을 표시한다. 우리는 모든 문서 토큰의 복잡성이 1로 최소화되는 상황을 강조한다.\n' +
      '\n' +
      '전기는 그림 4의 설정 1과 유사한 나머지 전기의 절반에 대한 질문에 답할 때 강력한 일반화로 이어졌으며 대조적으로 전기와 QA 쌍에 대한 교육은 순차적으로 실패했다. 그러나, 성공의 주요 기여자는 데이터가 함께 혼합되었기 때문에 불확실하며, 새로운 문서에서 지식을 흡수하기 위해 이를 실제로 적용하는 방법이 불분명하다. QA 쌍과 문서 사이의 서로 다른 난이도에 대한 관찰과 Zhu와 Li의 발견(2023)에 영감을 받아, 우리는 복잡한 문서로부터 지식을 인코딩하는 프로세스가 이 지식이 액세스되는 방법을 고려하도록 계속 사전 훈련 전에 LLM을 지시 조정 데이터에 의도적으로 노출시키는 것이 유익하다고 가정한다.__ 이를 **pre-instruction-tuning (PIT)**라고 하며, 지속적인 학습(SS 5.1) 전에 PIT의 다양한 구현을 연구한 후, 성능에 대한 키 기여자(SS 5.2 및 SS 5.3)를 식별하는 상세한 삭제를 수행하고, 마지막으로 PIT가 도메인 간에 얼마나 잘 수행되는지 평가한다(SS 5.4). 우리는 SS 3.2에 요약된 하이퍼파라미터를 준수하고 달리 명시되지 않는 한 3 에폭에 대해 PIT를 수행한다.\n' +
      '\n' +
      'Pre-instruction-tuning의 변인\n' +
      '\n' +
      'pre-instruction-tuning w/QA only we start with exposure instruction-tuning data before continued pre-training on documents--training on the localically related QA pairs on training on test documents (도 4) 이는 계속된 사전 훈련 설정(도 4)과 직접 비교할 수 있다. 직관은 질문이 LLMs가 주요 유형의 정보를 인식하는 데 도움이 되어 LLMs가 문서에 직접 묶여 있지 않더라도 후속 문서에 대한 사전 교육 중에 중요한 정보에 집중할 수 있다는 것이다. 예를 들어, "오펜하이머의 편집을 누가 처리했는가"와 같은 질문에 대한 훈련은 LLM이 "바비"와 같은 새로운 문서에 대한 훈련을 할 때 시나리오 작가에게 관심을 기울이는 데 도움이 될 수 있다. 탭에 표시된 대로입니다. 1, 이 방법은 특히 7B/70B의 경우 27.6%/41.7%\\(\\rightarrow\\) 28.6%/49.7%의 큰 LLM에서 지속적인 사전 훈련보다 우수하다. 문서에 대한 트레이닝 후에 QA 데이터에 대해 트레이닝하는 절제("Tab. 2)에서 명령어-튜닝 w/o 트레인 닥") 문서의 인코딩 전에 준비 작업으로 질문에 대한 교육의 중요성을 확인하는 것은 비효율적이다.\n' +
      '\n' +
      'QA 및 문서에 대한 사전 지시-튜닝 순차적으로 QA 및 관련 문서에 대한 두 번째 구현 열차(도 4) LLM이 관련된 더 간단한 QA 쌍을 파악한 후 복잡한 문서에 대해 훈련되면 문서로부터 지식을 흡수하는 능력이 강화될 수 있다는 직관을 가지고 있다. 예를 들어, LLM이 이미 "제니퍼 레임"이 "오펜하이머의 편집을 처리한 사람"에 대한 답이라는 것을 알게 된 경우 문서학에 대한 훈련\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c} \\hline \\hline\n' +
      '**Setting names** & **Setting configurations** & **EM** & **Rec.** & **R-L** \\\\ \\hline \\multicolumn{5}{c}{_baselines_} \\\\ continued pre-training 1 & test doc & 27.6 & 31.6 & 43.8 \\\\ +instruction-tuning 2 & train doc + test doc \\(\\rightarrow\\) train QA & 30.3 & 34.7 & 47.4 \\\\ +instruction-tuning (w/o forget) 3 & train doc + test doc \\(\\rightarrow\\) train QA + test doc & 30.2 & 34.1 & 46.4 \\\\ +instruction-tuning (w/o train doc) & test doc \\(\\rightarrow\\) train QA & 27.1 & 30.7 & 42.3 \\\\ weighted continued pre-training & test doc (weighted) & 27.7 & 32.7 & 43.3 \\\\ adapted continued pre-training & train doc \\(\\rightarrow\\) test doc & 26.9 & 32.7 & 44.2 \\\\ mix all data 3 & train QA + train doc + test doc & 39.4 & 44.6 & 56.7 \\\\ \\hline \\multicolumn{5}{c}{_various pre-instruction-tuning (PIT) methods and ablation studies_} \\\\ train QA + train doc (3 epochs) \\(\\rightarrow\\) test doc & 45.4 & 51.2 & 63.2 \\\\ \\hline \\multicolumn{5}{c}{_ablation studies of the number of epochs_} \\\\ \\multicolumn{5}{c}{1 epoch} \\\\ \\multicolumn{5}{c}{5 epochs} \\\\ \\multicolumn{5}{c}{10 epochs} \\\\ PIT 2 & 10 epochs & 45.8 & 52.1 & 63.6 \\\\ \\multicolumn{5}{c}{_ablation studies of different learning mechanisms_} \\\\ QA before doc (grouped) & 38.2 & 43.2 & 56.3 \\\\ QA after doc (grouped) & 27.2 & 31.1 & 42.1 \\\\ QA before doc (interleaved) & 45.9 & 51.3 & 64.5 \\\\ QA after doc (interleaved) & 43.2 & 49.1 & 61.6 \\\\ PIT\\(-\\) & train QA + train doc \\(\\rightarrow\\) train QA \\(\\rightarrow\\) test doc & 44.4 & 51.3 & 63.4 \\\\ PIT++ 3 & train QA \\(\\rightarrow\\) train QA + train doc \\(\\rightarrow\\) test doc & **48.1** & **54.4** & **66.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: Llama-2 7B를 사용하여 향상된 성능의 주요 기여자를 식별하기 위한 다양한 사전 명령-튜닝 방법 및 절제 연구의 비교(%). 서로 다른 배경 색상은 서로 다른 사전 지시-튜닝 방법을 나타낸다. 가장 좋은 결과는 굵은 글씨입니다.\n' +
      '\n' +
      '편집 "편집은 제니퍼 레임에 의해 처리되었습니다"는 매개변수에 대한 지식 저장을 보다 효율적으로 정제할 수 있습니다. 탭에 표시된 대로입니다. 도 1을 참조하면, QA 쌍 및 문서에 대한 PIT는 QA 전용 변형을 순차적으로 능가한다(도 4). 및 표준 명령어-튜닝(도. 4) (7B/70B의 경우 30.3%/46.4% \\(\\rightarrow\\) 32.5%/54.6%로, 그 효과를 입증하였다.\n' +
      '\n' +
      'PIT의 효과를 사전-명령-튜닝하는 것은 연관된 QA 쌍들이 각각의 문서들을 인코딩하기 전에 이미 학습된다는 것을 보장하는 것에 의존한다. 그러나 우리는 문서(도 4의 훈련 문서)에 대한 훈련 후 이를 관찰했다. , 해당 질문에 대한 정확도(도 4의 열차 QA) 거의 완벽에서 30%로 떨어졌는데, 이는 심각한 망각을 나타낸다. 이를 해결하기 위해 관련 QA 쌍과 문서를 함께 학습한다(도 4). 탭에 표시된 대로입니다. 도 1에 도시된 바와 같이, 이는 모든 데이터를 함께 혼합하는 것을 포함하는 다른 모든 접근법들을 능가하는 성능을 상당히 개선한다(도 1). 4) 큰 마진(39.4%/57.1% \\(\\rightarrow\\) 45.5%/62.7% for 7B/70B). QA 쌍과 문서 모두에 대한 훈련은 망각을 방지하지만 학습 과정이 어떻게 작동하는지도 모호하게 한다. LLM이 문서로부터 지식을 인코딩하기 전에 QA 쌍을 파악하는지 또는 그 반대로 작동하는지는 불분명하다. 다음 섹션에서는 이를 검토하기 위해 훈련 중에 QA 쌍과 문서의 순서를 의도적으로 배열하여 개선된 버전의 PIT를 제안한다.\n' +
      '\n' +
      '### Pre-instruction-tuning++\n' +
      '\n' +
      '우리는 먼저 성능이 시대 수에 따라 어떻게 달라지는지 연구합니다. 탭에 표시된 대로입니다. 2는 1 에폭에 대한 훈련이 부족하고 3, 5, 10 에폭의 수행도 유사하다. 우리는 에폭의 수를 3으로 고정하고 그림 6과 같이 QA 쌍과 해당 문서의 순서를 배열한다. 인터리브 배열은 모든 데이터를 3회 반복하여 각 에폭에서 질문이 관련 문서에 선행하거나 후속하도록 한다. 한편, 그룹화된 배열은 각각의 예시의 3개의 출현을 함께 군집화함으로써, 반복된 질문들이 각각의 반복된 문서들 전 또는 후에 위치됨을 보장한다. 탭에 표시된 대로입니다. 도 2에서, 대응하는 문서들 이전에 QA 쌍들을 위치시키는 것은 그룹화된 배열 및 인터리빙된 배열 모두에서 더 나은 성능을 달성하며, 이는 PIT 동안, 학습 메커니즘이 더 복잡하고 정보 밀도가 높은 문서들로부터 정보를 흡수하기 위해 학습하기 전에 지식에 액세스하는 방법을 이해하는 것을 우선시한다는 것을 나타낸다.\n' +
      '\n' +
      '이를 기반으로 지식 접근의 패턴을 이해하기 위해 QA 쌍에서만 학습한 다음 질문과 문서로부터의 지식 인코딩을 통해 지식 접근을 정렬하기 위해 QA와 문서 데이터의 조합에 대한 학습으로 진행하는 사전 명령어-튜닝++라는 개선된 변형을 제안한다(도 4). 탭에 표시된 대로입니다. 도 2에서, PIT++는 PIT를 상당히 능가한다(도 4). 혼합 후 QA 데이터에 대해 훈련하는 동안(탭에서 PIT-2) 45.4%에서 48.1%까지). 추가 혜택을 제공하지 않습니다. 이는 지식이 접근하는 방식을 이해하는 것이 문서로부터 지식을 흡수하는 데 도움이 되므로 우선시되어야 한다는 우리의 가설을 강화한다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '표준지시조정은 표준지시조정의 단점으로는 QA 쌍에 대한 교육 후에 시험문서의 지식이 잊혀질 수 있다는 것이다(Ouyang et al., 2022). 표준 명령어-튜닝의 낮은 성능이 망각 때문이 아님을 보여주기 위해, 우리는 망각 방지를 위해 명령어-튜닝 동안 테스트 문서와 트레인 QA를 혼합하는 설정을 추가한다(도 4). 탭에 표시된 대로입니다. 2, 이것은 우리의 가설을 확인하는 데 도움이 되지 않는다.\n' +
      '\n' +
      '사전 명령-튜닝은 단순히 문서들로부터 현저성 토큰들을 가중시키는 것이 아니라, 현저성 정보에 초점을 맞추기 위해 문서들에 대해 사전 트레이닝할 때 토큰들을 가중시키는 Hu et al.(2023)에 의해 영감을 받은 삭제를 포함한다. 우리는 답변에 포함된 문서에서 토큰에 1.0의 가중치를 할당하고(예를 들어, "편집은 제니퍼 레임에 의해 처리되었다"라는 문장에서 "제니퍼 레임") 다른 토큰에 0.5의 더 낮은 가중치를 할당한다. 탭에 표시된 대로입니다. 2, 이 가중 연속 프리\n' +
      '\n' +
      '도 6: QA 쌍들과 대응하는 문서들 사이의 상이한 배열들. 타원들은 다른 예들을 나타낸다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      'LMs의 훈련동역학 분석\n' +
      '\n' +
      '많은 작품들이 다양한 관점에서 LM의 훈련 역학을 연구한다. Carlini et al. (2022)는 모델 크기 및 데이터 중복 빈도 전반에 걸친 암기를 정량화한다. Tirumala et al. (2022)은 더 큰 LMs들이 덜 과적합으로 트레이닝 데이터를 더 빨리 암기한다는 것을 발견한다. Xia et al.(2023)은 Perplexity가 다른 요인들보다 모델 행동을 더 잘 예측한다는 것을 보여준다. Dery et al.(2022)은 분류 작업과 RoBERTa 모델을 이용한 엔드-태스크 인식 사전 훈련을 연구한다. 우리의 작업은 특히 볼 수 있는 문서에서 정보를 회수하고 일반화하여 질문에 답하는 능력에 초점을 맞춘다는 점에서 다르다.\n' +
      '\n' +
      '### Retrieval-augmented Generation\n' +
      '\n' +
      'RAG(Retrieval-augmented generation)는 외부 소스로부터의 검색된 정보로 고정된 LLMs를 증강함으로써 LLMs에 새로운 지식을 통합하기 위해 널리 사용되는 접근법이다(Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Borgeaud et al., 2022; Wang et al., 2023; Alon et al., 2022; He et al., 2021; Sachan et al., 2021; Izacard et al., 2023; Lee et al., 2022; Jiang et al., 2022; Shi et al., 2023; Jiang et al., 2023; Asai et al., 2023; Nakano et al., 2021; Qin et al., 2023; Lin et al., 2023). RAG는 매개 변수에 저장된 지식에만 의존할 때 일반적으로 경험하는 환각을 줄이는 데 효과적이지만 검색 및 생성 과정은 여분의 지연과 복잡성을 추가한다. 대조적으로, 지식을 매개변수에 저장하고 저장된 지식을 클로즈북 방식으로 질문에 답하기 위해 활용하는 지속적인 사전 훈련은 추론 시간에 더 간단하고 더 빠르다. 이 기능을 향상시키는 것은 또한 LLM을 정보에 액세스하기 위한 신뢰할 수 있는 보조자로 사용하는 근본적인 단계를 나타내기 때문에 과학적으로 중요하다. 따라서 본 논문은 매개변수 접근 방식을 탐색하는 데 중점을 둔다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '우리는 나중에 사실적 지식을 이끌어내는 것을 목표로 새로운 문서에 대한 지속적인 훈련의 최선의 방법을 연구한다. 우리는 문서로부터 지식을 인코딩하기 전에 QA 쌍을 통해 지식이 어떻게 액세스되는지 학습하는 사전 명령어 튜닝을 제안한다. 광범위한 실험은 사전 명령어-튜닝 대 표준 명령어-튜닝의 우수성을 입증한다. 향후 방향에는 이 방법을 보다 강력한 일반화를 위한 광범위한 문서 및 지침으로 확장하는 것이 포함된다.\n' +
      '\n' +
      '## Limitations\n' +
      '\n' +
      '위키2023 데이터 세트는 지속적인 지식 획득을 연구하기 위한 비교적 깨끗한 테스트 베드를 제공한다. 그러나 그 범위는 커먼 크롤의 웹 페이지 또는 arXiv의 과학 문서와 같은 다른 소스에 대한 훈련된 모델의 적응성을 제한하는 위키피디아로 제한된다. 우리는 이 논문에서 QA 데이터에 대한 지도 조정을 통해 사실적 지식을 도출하는 데 중점을 둔다. 추론이나 이해와 같은 다른 기술을 향상시키기 위해 다양한 유형의 데이터로 사전 지도 조정의 효과는 향후 연구에서 탐구해야 할 것이다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '제위안 알렌주, Zexuan Zhong, Shuyan Zhou, Frank F. Xu, Qian Liu, Ruohong Zhang이 실험과 건설적인 피드백에 도움을 준 것에 대해 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Alon et al. (2022) Uri Alon, Frank F. Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. 2022. automaticaton-augmented retrieval을 이용한 신경상징 언어 모델링. In _International Conference on Machine Learning_.\n' +
      '* Asai et al. (2023) Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. 자기래그: 자기반성을 통한 검색, 생성, 비평 학습. _ CoRR_, abs/2310.11511.\n' +
      '* Berglund et al. (2023) Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balseni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. 역전 저주: "a는 b"에 훈련된 LLM들은 "b는 a"를 학습하지 못한다. _ CoRR_, abs/2309.12288.\n' +
      '* Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. 레이, 에리히 엘슨, 로랑 시프레 2022. 수조 개의 토큰에서 검색함으로써 언어 모델을 개선한다. In _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 2206-2240. PMLR.\n' +
      '* Brown et al. (2021) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henigman, Rewon Child, Aditya Ramesh, Daniel M. 지글러, 제프리 우, 클레멘스 윈터, 크리스토퍼 헤세, 마크 첸, 에릭 시글러, 마테우스 리트윈, 스콧 그레이, 벤자민 체스, 잭 클락, 크리스토퍼 버너, 샘 맥캔들시, 알렉 래드포드, 일리아 서츠키버, 다리오 아모데이. 2020. 언어 모델은 소수의 학습자이다. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_.\n' +
      '* Carlini et al. (2022) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022. 신경 언어 모델에 걸친 암기 정량화_ CoRR_, abs/2202.07646.\n' +
      '* August 4, Volume 1: Long Papers_, pages 1870-1879. Association for Computational Linguistics.\n' +
      '* Cheng et al. (2023) Daixuan Cheng, Shaohan Huang, and Furu Wei. 2023. 독해력을 통한 대용량 언어 모델 적응. _ CoRR_, abs/2309.09530.\n' +
      '* Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Liamin Zhang, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: 90%* chatgpt 품질의 gpt-4를 인상하는 오픈소스 챗봇.\n' +
      '* Chowdhery et al.(2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Joshua Maynez, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyun택 Lim, Barret Zoph, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanunalayan Sankaranarayana Pilali, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Zuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel. 2022. Palm: 경로를 갖는 스케일링 언어 모델링 _ CoRR_, abs/2204.02311.\n' +
      '* Dery et al.(2022) Lucio M. Dery, Paul Michel, Ameet Talwalkar 그리고 Graham Neubig. 2022. 사전 훈련을 해야 하나요? 대안으로서 최종 과제 인식 훈련에 대한 주장 10차 국제학술대회에서는 ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net.\n' +
      '*팀(2023) 쌍둥이자리팀. 2023. 쌍둥이자리: 매우 유능한 멀티모달 모델들의 패밀리.\n' +
      '* Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: 검색-증강 언어 모델 사전 학습. _ CoRR_, abs/2002.08909.\n' +
      '* 의료 대화 AI 모델 및 훈련 데이터의 오픈 소스 컬렉션. _ CoRR_, abs/2304.08247.\n' +
      '* He et al. (2021) Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. 2021. 효율적인 최근접 이웃 언어 모델. 자연어 처리에서의 경험적 방법에 대한 회의.\n' +
      '* Hu et al. (2023) Nathan Hu, Eric Mitchell, Christopher D. Manning, and Chelsea Finn. 2023. 언어 모델의 메타 학습 온라인 적응. _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023_, pages 4418-4432. Association for Computational Linguistics.\n' +
      '* Ivison et al. (2023) Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. 변화하는 기후에 있는 낙타: 튤루 2. _CoRR_, abs/2311.10702로 LM 적응 강화.\n' +
      '* Iyer et al. (2022) Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O\'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asil Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. 2022. OPT-IML: 일반화 렌즈를 통한 스케일링 언어 모델 명령어 메타 학습. _ CoRR_, abs/2212.12017.\n' +
      '* Izacard et al.(2023) Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: 검색 증강 언어 모델을 이용한 소수 샷 학습 _ J 마흐 배워 Res._ , 24:251:1-251:43.\n' +
      '* 장 등(2022) 조엘 장, 성현 예, 소희 양, 중보 신, 장훈 한, 경훈 김, 스탠리정규 최, 민준 서. 2022. 언어 모델의 지속적인 지식 학습을 향한다. 10차 국제학술대회에서는 ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net.\n' +
      '* Jiang et al. (2022) Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun Araki, Haibo Ding, Jamie Callan, and Graham Neubig. 2022. 주의로서의 검색: 단일 변압기 내에서 검색 및 판독의 종단간 학습. InProceedings of 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, 아랍에미리트 아부다비, December 7-11, 2022_, pages 2336-2349. Association for Computational Linguistics.\n' +
      '* Jiang et al. (2020) Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. 언어 모델이 무엇을 알고 있는지 어떻게 알 수 있는가? _ 트랜스 Assoc. 컴퓨터 Linguistics_, 8:423-438.\n' +
      '* Jiang et al. (2023) Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval Augmented Generation. _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023_, pages 7969-7992. Association for Computational Linguistics.\n' +
      '* 대형 언어 모델 정렬을 민주화하는 단계; _ ArXiv_, abs/2304.07327.\n' +
      '* Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov. 2019. Natural question: question answer research의 벤치마크. _ 트랜스 Assoc. 컴퓨터 Linguistics_, 7:452-466.\n' +
      '* Lee et al. (2022) Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Parangjape, Christopher D. Manning, and Kyung-Gu Woo. 2022. 오픈 도메인 질문 응답을 위한 하나의 모델만 있으면 된다. _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pages 3047-3060. Association for Computational Linguistics.\n' +
      '* Lewis et al. (2020a) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: 자연어 생성, 번역 및 이해를 위한 시퀀스-대-시퀀스 사전 트레이닝을 잡음 제거한다. _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, pages 7871-7880. Association for Computational Linguistics.\n' +
      '* Lewis et al. (2020b) Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. 2020b. 지식 집약적 NLP 작업에 대한 검색 증강 생성 In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_.\n' +
      '* Lin et al. (2023) Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. 2023. RA-DIT: 검색-증강 이중 명령어 튜닝 _ CoRR_, abs/2310.01352.\n' +
      '* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net.\n' +
      '* Mishra et al. (2022) Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. 자연어 크라우드소싱 명령어를 통한 교차 작업 일반화. _Proceedings of the 60th Annual Meeting of the Computational Linguistics Association (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 3470-3487. Association for Computational Linguistics.\n' +
      '* Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. 웹gt: 인간 피드백을 이용한 브라우저 보조 질문-답변_ CoRR_, abs/2112.09332.\n' +
      '* Nguyen et al. (2023) Tuan Dung Nguyen, Yuan-Sen Ting, Ioana Ciuca, Charlie O\'Neill, Ze-Chang Sun, Maja Jablonska, Sandor Kruk, Ernest Perkowski, Jack W. 밀러, 제이슨 리, 조쉬 픽, 카르테크 아이이어, 토마스 로잔스키, 프라나브 케타르팔, 샤라프 자만, 데이비드 브로드릭, 세르히오 J. 로드리게스 멘데스, 상 부이, 알리사 굿맨, 알베르토 아코마치, 질 P. 나이만, 제시 크래니, 케빈 샤윈스키, 유니버스TBD. 2023. 애스트롤라마: 천문학의 전문화된 기초 모델을 향하여. _ CoRR_, abs/2309.06126.\n' +
      '* OpenAI(2023) OpenAI. 2023. GPT-4 기술보고서 _ CoRR_, abs/2303.08774.\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. 인간의 피드백으로 지시를 따르도록 언어 모델을 훈련시키는 단계 _ CoRR_, abs/2203.02155.\n' +
      '* Ovadia et al. (2023) Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023년, 미세 조정? 회수? llms에서 지식 주입 비교. _ CoRR_, abs/2312.05934.\n' +
      '* Petroni et al. (2021) Fabio Petroni, Tim Rocktaschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. 2019. 지식 기반으로서의 언어 모델? _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_, pages 2463-2473. Association for Computational Linguistics.\n' +
      '* Qin et al. (2023) Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, 및 Jie Zhou. 2023. Webcpm: Interactive web search for chinese longform question answering. _ CoRR_, abs/2305.06849.\n' +
      '* Radford 등(2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. 언어 모델은 비감독 멀티태스크 학습자들 _ OpenAI Blog_, 1(8).\n' +
      '* Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. 직접 선호도 최적화: 당신의 언어 모델은 비밀리에 보상 모델이다. _ CoRR_, abs/2305.18290.\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. 통일된 텍스트-텍스트 변환기를 이용한 전이학습의 한계점 탐색. _ J 마흐 배워 Res._ , 21:140:1-140:67.\n' +
      '* Roberts et al. (2020) Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. 언어 모델의 매개변수에 얼마나 많은 지식을 포장할 수 있습니까? _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_, pages 5418-5426. Association for Computational Linguistics.\n' +
      '* Sachan et al. (2021) Devendra Singh Sachan, Siva Reddy, William L. 해밀턴, 크리스 다이어, 대니 요가타마 2021. 오픈 도메인 질의 응답을 위한 다중 문서 판독기 및 리트리버의 종단간 훈련. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 25968-25981.\n' +
      '* Sanh et al. (2023) Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Mannan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. 나약, 데바요티 다타, 조나단 장, 마이크 톈-지안 장, 한 왕, 마테오 마니카, 정셴, 정신 용, 개똥 팬디, 레이첼 바덴, 토마스 왕, 트리살라 니에르즈, 조스 로젠, 아베쉬 샤르마, 안드레아 산틸리, 티볼트 페브리, 제이슨 앨런 프라이스, 라이언 티한, 티한 르 스카오, 스텔라 바이더만, 레오 가오, 토마스 울프, 알렉산더 M. 러쉬 2022. 멀티 태스크 프롬프트 트레이닝은 제로 샷 태스크 일반화를 가능하게 한다. _ The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net.\n' +
      '* Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjun Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: 검색-증강 블랙박스 언어 모델들 _ CoRR_, abs/2301.12652.\n' +
      '* Sun et al. (2023a) Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David D. Cox, Yiming Yang, and Chuang Gan. 2023a. SALMON: Self-alignment with principle-following reward models. _ CoRR_, abs/2310.05910.\n' +
      '* Sun et al. (2023b) Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David D. Cox, Yiming Yang, and Chuang Gan. 2023b. 최소한의 인간 감독으로 처음부터 언어 모델의 원리 기반 자기 정렬 CoRR_, abs/2305.03047.\n' +
      '* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. 스탠포드 알파카: 명령어-추종 라마 모델. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca).\n' +
      '* Tian et al. (2023) Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, and Chelsea Finn. 2023. 사실성을 위한 언어 모델을 미세 조정한다. _ CoRR_, abs/2311.08401.\n' +
      '* 2022년 12월 9일_.\n' +
      '* Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. 개방적이고 효율적인 기초 언어 모델 CoRR_, abs/2302.13971.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Likel, Likas Blecher, Cristian Canton-Ferrer, Moya Bhosale, David Esibou, Jude Fernandes, Jeremy Fu, Wenyin Fu, Cynthia Gao, Vedanqui Goswami, Naman Goyal, Vedan Hartshorn, Saghar Hosseini, Rui Hou, Hakan Kardas, Vicktor Kerkez, Madian Lavril, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Lavril, Yixin Nie, E. Michael Smith, R. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. 2023b. 라마 2: 오픈 파운데이션 및 미세 조정 채팅 모델들_ CoRR_, abs/2307.09288.\n' +
      '* Wang et al. (2023a) Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, 이동, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, Anima Anandkumar, and Bryan Catanzaro. 2023a. 자아회귀 언어 모델을 검색으로 사전 훈련할까요? 종합적인 연구. _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023_, pages 7763-7786. Association for Computational Linguistics.\n' +
      '* Wang et al. (2021) Cunxiang Wang, Pai Liu, and Yue Zhang. 2021. 생성 사전 훈련된 언어 모델이 닫힌 책 q의 지식 베이스 역할을 할 수 있는가? _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021_, pages 3241-3251. Association for Computational Linguistics.\n' +
      '* Wang et al. (2023b) Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023b. 낙타는 얼마나 멀리 갈 수 있나요? 개방형 리소스에 대한 명령어 튜닝 상태를 탐색합니다. _ CoRR_, abs/2306.04751.\n' +
      '* Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Y. 자오, 켈빈 구, 아담스 웨이유, 브라이언 레스터, 난두, 앤드류 M. 다이, 콕 V 르 2022. Finetuned language models is zero-shot learners. 10차 국제학술대회에서는 ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net.\n' +
      '* Wu et al. (2023) Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023. Pmc-llama: 의학을 위한 오픈 소스 언어 모델을 구축하기 위해.\n' +
      '* Xia et al. (2023) Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Veselin Stoyanov. 2023. 언어 모델들의 궤적들을 스케일에 걸쳐 트레이닝한다. _Proceedings of the 61st Annual Meeting of the Computational Linguistics Association (Volume 1: Long Papers), ACL 2023, Toronto, July 9-14, 2023_, pages 13711-13738. Association for Computational Linguistics.\n' +
      '* Zhang et al. (2023) Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zang Zhang, Fangzhou Ai, Yiming Yang, and Hongxia Yang. 2023. 지식 마이닝 및 다이제스트를 통한 도메인별 챗봇 교육을 위한 자가 향상 접근법 _ CoRR_, abs/2311.10614.\n' +
      '* Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihalyolov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models. _ ArXiv_, abs/2205.01068.\n' +
      '* Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Bechen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, 및 Ji-Rong Wen. 2023. 대형 언어 모델에 대한 조사. _ CoRR_, abs/2303.18223.\n' +
      '* Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: less is more for alignment. _ CoRR_, abs/2305.11206.\n' +
      '* Zhu and Li(2023a) Zeyuan Allen Zhu and Yuanzhi Li. 2023a. 언어 모델의 물리학: Part 3.1, 지식 저장 및 추출 CoRR_, abs/2309.14316.\n' +
      '* Zhu and Li(2023b) Zeyuan Allen Zhu and Yuanzhi Li. 2023b. 언어 모델의 물리학: Part 3.2, 지식 조작 CoRR_, abs/2309.14402.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>