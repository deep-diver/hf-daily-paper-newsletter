<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 욜로브9: 배우고 싶은 것을 배우기\n' +
      '\n' +
      '프로그램 가능한 기울기 정보\n' +
      '\n' +
      'Chien-Yao Wang\\({}^{1,2}\\)\n' +
      '\n' +
      'I-Hau Yeh\\({}^{2}\\)\n' +
      '\n' +
      '홍유안 마크 랴오\\({}^{1,2,3}\\)\n' +
      '\n' +
      '({}^{1}\\)Institute of Information Science, Academia Sinica, Taiwan\n' +
      '\n' +
      '대만 타이베이공대\n' +
      '\n' +
      '대만청원기독교대학교 정보컴퓨터공학과\n' +
      '\n' +
      'kinyiu@iis.sinica.edu.tw, ihyeh@emc.com.tw, liao@iis.sinica.edu.tw.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '오늘날의 딥러닝 방법은 모델의 예측 결과가 Ground truth에 가장 근접할 수 있도록 가장 적절한 목적함수를 어떻게 설계할 것인가에 초점을 맞추고 있다. 한편, 예측을 위한 충분한 정보의 획득을 용이하게 할 수 있는 적절한 아키텍처가 설계되어야 한다. 기존의 방법들은 입력 데이터가 계층별 특징 추출과 공간 변환을 겪을 때, 많은 양의 정보가 손실된다는 사실을 무시한다. 본 논문은 심층 네트워크를 통해 데이터가 전송될 때 데이터 손실의 중요한 문제, 즉 정보 병목 현상과 가역적 기능에 대해 탐구할 것이다. 본 논문에서는 여러 가지 목적을 달성하기 위해 딥 네트워크가 요구하는 다양한 변화에 대응하기 위해 프로그래머블 그래디언트 정보(programmable gradient information, PGI) 개념을 제안하였다. PGI는 목적 함수를 계산하기 위해 타겟 태스크에 대한 완전한 입력 정보를 제공할 수 있고, 따라서 네트워크 가중치를 업데이트하기 위해 신뢰성 있는 구배 정보가 획득될 수 있다. 또한, 새로운 경량 네트워크 구조 - GELAN(Generalized Efficient Layer Aggregation Network)을 그래디언트 경로 계획에 기반하여 설계한다. GELAN의 아키텍처는 PGI가 경량 모델에서 우수한 결과를 얻었음을 확인시켜준다. 제안된 GELAN과 PGI를 MS COCO 데이터세트 기반 객체 검출에서 검증하였다. 결과는 GELAN이 깊이-와이즈 컨볼루션에 기초하여 개발된 최첨단 방법보다 더 나은 매개변수 활용을 달성하기 위해 기존의 컨볼루션 연산자만을 사용한다는 것을 보여준다. PGI는 경량부터 대형까지 다양한 모델에 사용할 수 있습니다. 완전한 정보를 얻기 위해 사용될 수 있어 스크래치로부터 트레인 모델이 대규모 데이터 세트를 사용하여 미리 훈련된 최신 모델보다 더 나은 결과를 얻을 수 있도록, 비교 결과는 그림 1에 나와 있다. 소스 코드는 [https://github.com/WongKinYiu/yolov9](https://github.com/WongKinYiu/yolov9)이다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '딥러닝 기반 모델은 컴퓨터 비전, 언어 처리, 음성 인식 등 다양한 분야에서 과거 인공 지능 시스템보다 훨씬 우수한 성능을 입증했다. 최근, 딥러닝 분야의 연구자들은 CNNs[21, 22, 42, 55, 71, 72], 트랜스포머[8, 9, 40, 69, 70, 41, 60, 70], 퍼시버[26, 26, 32, 52, 56, 81, 81], 및 맘바[17, 38, 80]와 같은 보다 강력한 시스템 아키텍처 및 학습 방법을 개발하는 방법에 주로 초점을 맞추고 있다. 또한, 일부 연구자들은 손실 함수[5, 45, 50, 77, 78, 46], 라벨 할당[10, 12, 33, 67, 79] 및 보조 감독[18, 24, 28, 29, 51, 54, 68, 76]과 같은 보다 일반적인 목적 함수를 개발하려고 노력했다. 위의 연구들은 모두 입력과 목표 태스크 간의 매핑을 정밀하게 찾기 위해 노력한다. 그러나, 대부분의 과거 접근법들은 입력 데이터가 피드포워드 프로세스 동안 무시할 수 없는 양의 정보 손실을 가질 수 있다는 것을 무시했다. 이러한 정보의 손실은 편향된 구배 흐름으로 이어질 수 있으며, 이는 후속적으로 모델을 업데이트하는 데 사용된다. 상기 문제들은 타겟들과 입력들 사이의 부정확한 연관들을 확립하는 딥 네트워크들을 초래하여, 트레이닝된 모델이 부정확한 예측들을 생성하게 할 수 있다.\n' +
      '\n' +
      '그림 1: MS COCO 데이터 세트에서 실시간 객체 검출기의 비교. GELAN과 PGI 기반 객체 검출 방법은 객체 검출 성능 측면에서 이전의 모든 트레인-from-스크래치 방법을 능가했다. 정확도 측면에서, 새로운 방법은 대규모 데이터세트로 미리 훈련된 RT DETR[43]보다 우수하고, 파라미터화 활용 측면에서 깊이별 컨볼루션 기반 설계 YOLO MS[7]보다 우수하다.\n' +
      '\n' +
      '딥 네트워크에서, 피드포워드 프로세스 동안 입력 데이터가 정보를 잃는 현상은 일반적으로 정보 병목 현상[59]으로 알려져 있으며, 그 개략도는 도 2에 도시된 바와 같다. 현재, 이러한 현상을 완화할 수 있는 주요 방법은 다음과 같다: (1) 가역적 아키텍처의 사용 [19, 3, 16]: 이 방법은 주로 반복된 입력 데이터를 사용하고 입력 데이터의 정보를 명시적인 방식으로 유지한다; (2) 마스킹된 모델링 [1, 6, 9, 27, 73, 71]: 주로 재구성 손실을 사용하고 추출된 특징을 최대화하고 입력 정보를 유지하기 위한 암시적 방법을 채택한다; 및 (3) 딥 슈퍼비전 개념[54, 51, 28, 68]의 도입: 중요한 정보가 더 깊은 계층으로 전달될 수 있도록 특징들로부터 타겟으로의 매핑을 미리 확립하기 위해 너무 많은 중요한 정보를 손실하지 않은 얕은 특징들을 사용한다. 그러나, 상기 방법들은 훈련 과정과 추론 과정에서 서로 다른 단점을 가지고 있다. 예를 들어, 가역 아키텍처는 반복적으로 공급되는 입력 데이터를 조합하기 위해 추가 계층을 필요로 하며, 이는 추론 비용을 상당히 증가시킬 것이다. 또한, 출력 레이어로의 입력 데이터 레이어는 너무 깊은 경로를 가질 수 없기 때문에, 이러한 제한은 트레이닝 프로세스 동안 고차 시맨틱 정보를 모델링하는 것을 어렵게 할 것이다. 마스크드 모델링의 경우 재구성 손실이 목표 손실과 충돌하는 경우가 있다. 또한, 대부분의 마스크 메커니즘은 데이터와의 잘못된 연관성도 생성한다. 심층 감독 메커니즘의 경우 오류 축적을 생성하고, 얕은 감독이 훈련 과정에서 정보를 잃는 경우 후속 계층은 필요한 정보를 검색할 수 없다. 위와 같은 현상은 어려운 과제와 작은 모형에서 더 크게 나타날 것이다.\n' +
      '\n' +
      '위에서 언급한 문제를 해결하기 위해 프로그래밍 가능한 그라디언트 정보(PGI)라는 새로운 개념을 제안한다. 이 개념은 보조 가역 분기를 통해 신뢰할 수 있는 기울기를 생성하여 딥 피쳐가 여전히 목표 작업을 실행하기 위한 핵심 특성을 유지할 수 있도록 하는 것이다. 보조 가역 분기의 설계는 다중 경로 특징을 통합하는 전통적인 심층 감독 프로세스에 의해 야기될 수 있는 의미론적 손실을 피할 수 있다. 즉, 서로 다른 의미 수준에서 그래디언트 정보 전파를 프로그래밍하여 최상의 훈련 결과를 달성한다. PGI의 가역 아키텍처는 보조 브랜치에 구축되어 추가적인 비용이 들지 않는다. PGI는 대상 태스크에 적합한 손실 함수를 자유롭게 선택할 수 있기 때문에 마스크 모델링에 의해 발생하는 문제점도 극복한다. 제안된 PGI 메커니즘은 다양한 크기의 심층 신경망에 적용될 수 있으며, 매우 심층 신경망에만 적합한 심층 감독 메커니즘보다 더 일반적이다.\n' +
      '\n' +
      '또한, 본 논문에서는 ELAN[65]을 기반으로 일반화된 ELAN(GELAN)을 설계하였으며, GELAN의 설계는 파라미터 수, 계산 복잡도, 정확도 및 추론 속도를 동시에 고려하였다. 이 설계는 사용자들이 상이한 추론 디바이스들에 대해 적절한 계산 블록들을 임의로 선택할 수 있게 한다. 제안된 PGI와 GELAN을 결합한 후, YOLOv9라고 불리는 새로운 세대의 YOLO 시리즈 객체 검출 시스템을 설계하고, MS COCO 데이터 세트를 사용하여 실험을 수행했으며, 실험 결과는 제안된 YOLOv9가 모든 비교에서 최고의 성능을 달성했음을 입증했다.\n' +
      '\n' +
      '본 논문의 기여도를 요약하면 다음과 같다.\n' +
      '\n' +
      '1. 가역 함수의 관점에서 기존의 심층 신경망 아키텍처를 이론적으로 분석하였고, 이러한 과정을 통해 기존에 설명하기 어려웠던 많은 현상들을 성공적으로 설명하였다. 또한 이러한 분석을 바탕으로 PGI와 보조 가역 분기를 설계하여 우수한 결과를 얻었다.\n' +
      '2. 우리가 설계한 PGI는 깊은 감독이 극히 깊은 신경망 아키텍처에만 사용될 수 있다는 문제를 해결하고, 따라서 새로운 경량 아키텍처가 일상생활에서 진정으로 적용될 수 있게 한다.\n' +
      '3. GELAN은 기존의 컨벌루션만을 사용하여 가장 진보된 기술을 기반으로 하는 깊이별 컨벌루션 설계보다 더 높은 파라미터 사용을 달성하면서도 가볍고 빠르며 정확하다는 큰 장점을 보인다.\n' +
      '4. 제안된 PGI와 GELAN을 결합하여 MS COCO 데이터셋에서 YOLOv9의 객체 검출 성능은 모든 측면에서 기존의 실시간 객체 검출기를 크게 능가한다.\n' +
      '\n' +
      '도 2: 상이한 네트워크 아키텍처에 대한 랜덤 초기 가중치 출력 특징 맵의 시각화 결과: (a) 입력 이미지, (b) PlainNet, (c) ResNet, (d) CSPNet, 및 (e) 제안된 GELAN. 그림에서 우리는 다른 아키텍처에서 손실을 계산하기 위해 목적 함수에 제공된 정보가 다양한 정도로 손실된다는 것을 알 수 있으며, 우리의 아키텍처는 목적 함수를 계산하기 위해 가장 완전한 정보를 유지하고 가장 신뢰할 수 있는 구배 정보를 제공할 수 있다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### 실시간 객체검출기\n' +
      '\n' +
      '현재의 주류 실시간 객체 검출기는 YOLO 시리즈[2, 7, 13, 14, 15, 25, 30, 31, 47, 48, 49, 53, 61, 62, 63, 74, 64, 65, 66, 67, 75]이며, 이들 모델의 대부분은 CSPNet[64] 또는 ELAN[65] 및 이들의 변형을 주 컴퓨팅 유닛으로 사용한다. 특징 통합의 관점에서, 개선된 PAN[37] 또는 FPN[35]이 툴로서 종종 사용되고, 이어서 개선된 YOLOv3 헤드[49] 또는 FCOS 헤드[57, 58]가 예측 헤드로서 사용된다. 최근에는 DETR[4]에 기반을 둔 RT DETR[43]과 같은 실시간 객체 검출기도 제안되고 있다. 그러나, DETR 시리즈 객체 검출기가 해당 도메인 사전 훈련된 모델 없이 새로운 도메인에 적용되기는 매우 어렵기 때문에, 현재 가장 널리 사용되는 실시간 객체 검출기는 여전히 YOLO 시리즈이다. 본 논문은 다양한 컴퓨터 비전 작업과 다양한 시나리오에서 효과적인 것으로 입증된 YOLOv7[63]을 제안 방법을 개발하기 위한 기반으로 선택한다. 제안된 PGI를 사용하여 GELAN을 사용하여 구조와 훈련 과정을 개선한다. 위의 새로운 접근 방식은 제안된 YOLOv9를 새로운 세대의 최상위 실시간 객체 검출기로 만든다.\n' +
      '\n' +
      '### Reversible Architectures\n' +
      '\n' +
      '가역 아키텍처의 연산 유닛[3, 16, 19]은 가역 변환의 특성을 유지해야 하므로, 연산 유닛의 각 계층의 출력 특징 맵이 완전한 원본 정보를 유지할 수 있음을 보장할 수 있다. 이전 RevCol[3]은 전통적인 가역 단위를 여러 단계로 일반화하고, 그렇게 함으로써 서로 다른 계층 단위로 표현되는 의미 수준을 확장할 수 있다. 다양한 신경망 아키텍처에 대한 문헌 검토를 통해 가역적 특성의 정도가 다양한 고성능 아키텍처가 많다는 것을 발견했다. 예를 들어, Res2Net 모듈[11]은 서로 다른 입력 파티션을 다음 파티션에 계층적으로 결합하고, 변환된 모든 파티션을 뒤로 넘기기 전에 연결한다. CBNet[34, 39]은 합성 백본을 통해 원본 입력 데이터를 재도입하여 완전한 원본 정보를 획득하고, 다양한 합성 방법을 통해 서로 다른 레벨의 다단계 가역 정보를 획득한다. 이러한 네트워크 아키텍처는 일반적으로 우수한 파라미터 활용도를 갖지만, 추가 합성 계층은 느린 추론 속도를 야기한다. DynamicDet[36]은 CBNet[34]과 고효율 실시간 객체 검출기 YOLOv7[63]을 결합하여 속도, 파라미터 수, 정확도 중 매우 우수한 트레이드 오프를 달성한다. 본 논문에서는 가역 브랜치를 설계하기 위한 기초로서 DynamicDet 구조를 소개한다. 또한, 가역 정보는 제안된 PGI에 추가로 도입된다. 제안된 새로운 아키텍처는 추론 과정에서 추가 연결이 필요하지 않으므로 속도, 매개변수 양 및 정확도의 이점을 충분히 유지할 수 있다.\n' +
      '\n' +
      '### Auxiliary Supervision\n' +
      '\n' +
      '심층 감독[54, 28, 68]은 가장 일반적인 보조 감독 방법으로 중간 계층에 추가적인 예측 계층을 삽입하여 훈련을 수행한다. 특히 변압기 기반 방법에 도입된 다층 디코더의 적용이 가장 일반적이다. 다른 일반적인 보조 감독 방법은 관련 메타 정보를 활용하여 중간 계층들에 의해 생성된 특징 맵들을 안내하고 타겟 태스크들에 의해 요구되는 속성들을 갖도록 하는 것이다[18, 20, 24, 29, 76]. 이러한 유형의 예는 객체 검출기의 정확도를 향상시키기 위해 분할 손실 또는 깊이 손실을 사용하는 것을 포함한다. 최근 문헌[53, 67, 82]에는 서로 다른 레이블 할당 방법을 사용하여 서로 다른 보조 감독 메커니즘을 생성하여 모델의 수렴 속도를 높이고 동시에 견고성을 향상시키는 보고서가 많이 있다. 그러나 보조 감독 메커니즘은 일반적으로 대형 모델에만 적용 가능하기 때문에 경량 모델에 적용할 경우 언더 파라미터화 현상이 발생하기 쉬워 성능이 저하된다. 제안된 PGI는 다단계 의미 정보를 재프로그래밍하는 방법을 고안하였으며, 이 설계로 경량 모델도 보조 감독 메커니즘의 이점을 얻을 수 있다.\n' +
      '\n' +
      '## 3 문제 진술\n' +
      '\n' +
      '일반적으로 사람들은 기울기 소실이나 기울기 포화 등의 요인으로 인해 심층 신경망 수렴 문제의 어려움을 고려하며, 이러한 현상은 전통적인 심층 신경망에 존재한다. 그러나, 현대의 심층 신경망은 이미 다양한 정규화 및 활성화 함수를 설계함으로써 상기 문제를 근본적으로 해결하였다. 그럼에도 불구하고 심층 신경망은 여전히 수렴 속도가 느리거나 수렴 결과가 좋지 않은 문제가 있다.\n' +
      '\n' +
      '본고에서는 위 쟁점의 성격을 더 탐구한다. 정보 병목 현상에 대한 심층 분석을 통해 이 문제의 근본 원인은 원래 매우 깊은 네트워크에서 오는 초기 기울기가 전송된 직후 목표를 달성하는 데 필요한 많은 정보를 잃었기 때문이라고 추론했다. 이러한 추론을 확인하기 위해 초기 가중치로 서로 다른 아키텍처의 심층 네트워크를 피드포워드한 후 그림 2에 시각화 및 예시하였다. 분명히 PlainNet은 심층 계층에서 객체 검출에 필요한 많은 중요한 정보를 잃었다. ResNet, CSPNet, GELAN이 보유할 수 있는 중요 정보의 비율은 훈련 후 얻을 수 있는 정확도와 긍정적인 관련이 있는 것이 사실이다. 우리는 위의 문제의 원인을 해결하기 위해 가역 네트워크 기반 방법을 추가로 설계한다. 이 절에서는 정보 병목 원리와 가역 기능에 대한 분석을 자세히 설명한다.\n' +
      '\n' +
      '### 정보 병목현상 원리\n' +
      '\n' +
      '정보 병목 원리에 따르면, 우리는 데이터 \\(X\\)가 식과 같이 변환을 수행할 때 정보 손실을 유발할 수 있음을 알고 있다. 하기의 도 1:\n' +
      '\n' +
      '\\[I(X,X)\\geq I(X,f_{\\theta}(X))\\geq I(X,g_{\\phi}(f_{\\theta}(X))), \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(I\\)은 상호 정보를 나타내고 \\(f\\) 및 \\(g\\)은 변환 함수이며 \\(\\theta\\) 및 \\(\\phi\\)은 각각 \\(f\\) 및 \\(g\\)의 매개변수이다.\n' +
      '\n' +
      '심층 신경망에서 \\(f_{\\theta}(\\cdot)\\)와 \\(g_{\\phi}(\\cdot)\\)는 각각 심층 신경망에서 두 개의 연속적인 레이어의 연산을 나타낸다. Eq. 도 1을 참조하면, 네트워크 계층의 수가 깊어질수록 원본 데이터가 손실될 가능성이 더 클 것으로 예측할 수 있다. 그러나, 심층 신경망의 파라미터는 주어진 타겟뿐만 아니라 네트워크의 출력에 기초하고, 손실 함수를 계산하여 새로운 그래디언트를 생성한 후 네트워크를 갱신한다. 상상할 수 있듯이, 더 깊은 신경망의 출력은 예측 대상에 대한 완전한 정보를 덜 보유할 수 있다. 이것은 네트워크 트레이닝 동안 불완전한 정보를 사용하는 것을 가능하게 하여, 신뢰할 수 없는 구배 및 열악한 수렴을 초래할 것이다.\n' +
      '\n' +
      '상기 문제를 해결하는 한 가지 방법은 모델의 크기를 직접 증가시키는 것이다. 모델을 구성하기 위해 많은 수의 매개변수를 사용할 때 데이터의 보다 완전한 변환을 수행할 수 있다. 상기 접근법은 데이터 피드포워드 프로세스 동안 정보가 손실되더라도, 타겟으로의 매핑을 수행하기 위해 충분한 정보를 보유할 기회가 여전히 존재한다. 위 현상은 대부분의 현대 모형에서 깊이보다 너비가 더 중요한 이유를 설명한다. 그러나, 위의 결론은 매우 깊은 신경망에서 신뢰할 수 없는 기울기의 문제를 근본적으로 해결할 수 없다. 아래에서는 가역 함수를 이용하여 문제를 해결하고 상대 분석을 수행하는 방법을 소개하겠다.\n' +
      '\n' +
      '### Reversible Functions\n' +
      '\n' +
      '함수 \\(r\\)에 역변환 함수 \\(v\\)이 있을 때 우리는 이 함수를 식 2와 같이 가역 함수라고 부른다.\n' +
      '\n' +
      '\\[X=v_{\\zeta}(r_{\\psi}(X)), \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(\\psi\\) 및 \\(\\zeta\\)은 각각 \\(r\\) 및 \\(v\\)의 매개변수이다. 데이터 \\(X\\)는 식 3과 같이 정보를 잃지 않고 가역 함수에 의해 변환된다.\n' +
      '\n' +
      '\\[I(X,X)=I(X,r_{\\psi}(X))=I(X,v_{\\zeta}(r_{\\psi}(X))). \\tag{3}\\]\n' +
      '\n' +
      '네트워크의 변환 함수가 가역 함수들로 구성될 때, 모델을 갱신하기 위해 보다 신뢰성 있는 구배를 얻을 수 있다. 오늘날의 인기 있는 딥러닝 방법들은 대부분 식 4와 같이 가역적 성질에 부합하는 아키텍처들이다.\n' +
      '\n' +
      '\\[X^{l+1}=X^{l}+f_{\\theta}^{l+1}(X^{l}), \\tag{4}\\]\n' +
      '\n' +
      '여기서 \\(l\\)은 PreAct ResNet의 \\(l\\)번째 층을 나타내고 \\(f\\)은 \\(l\\)번째 층의 변환 함수이다. PreAct ResNet[22]은 원본 데이터 \\(X\\)를 명시적인 방법으로 후속 레이어에 반복적으로 전달한다. 이러한 설계는 천 개 이상의 층들이 매우 잘 수렴하는 심층 신경망을 만들 수 있지만, 그것이 우리가 심층 신경망이 필요한 중요한 이유를 파괴한다. 즉, 어려운 문제에 대해서는 데이터를 타겟에 매핑하기 위한 간단한 매핑 함수를 직접 찾기가 어렵다. 이는 PreAct ResNet이 레이어 수가 적을 때 ResNet[21]보다 성능이 떨어지는 이유도 설명한다.\n' +
      '\n' +
      '또한 변압기 모델이 상당한 돌파구를 달성할 수 있도록 마스킹 모델링을 사용하려고 했다. 우리는 Eq와 같은 근사 방법을 사용한다. 도 5를 참조하면, \\(r\\)의 역변환 \\(v\\)을 구하여 변환된 특징들이 희소 특징을 이용하여 충분한 정보를 유지할 수 있도록 한다. Eq의 형태. 도 5는 다음과 같다:\n' +
      '\n' +
      '\\[X=v_{\\zeta}(r_{\\psi}(X)\\cdot M), \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(M\\)은 동적 이진 마스크이다. 위의 작업을 수행하기 위해 일반적으로 사용되는 다른 방법은 확산 모델 및 변량 오토인코더이며, 둘 다 역함수를 찾는 기능을 가지고 있다. 그러나 위의 접근법을 경량 모델에 적용하면 경량 모델이 많은 양의 원시 데이터에 매개변수화되기 때문에 결함이 있다. 이러한 이유로 데이터\\(X\\)을 타겟\\(Y\\)에 매핑하는 중요한 정보\\(I(Y,X)\\)도 동일한 문제에 직면하게 된다. 이 문제에 대해서는 정보 병목 현상의 개념을 이용하여 탐색한다[59]. 정보 병목 현상에 대한 공식은 다음과 같다:\n' +
      '\n' +
      '\\[I(X,X)\\geq I(Y,X)\\geq I(Y,f_{\\theta}(X))\\geq...\\geq I(Y,\\hat{Y}. \\tag{6}\\]\n' +
      '\n' +
      '일반적으로 \\(I(Y,X)\\)는 \\(I(X,X)\\)의 극히 작은 부분만을 차지할 것이다. 그러나, 그것은 목표 임무에 중요하다. 따라서 피드포워드 단계에서 손실되는 정보의 양이 크지 않더라도 \\(I(Y,X)\\)를 포괄하는 한 훈련 효과가 크게 영향을 받게 된다. 경량 모델 자체는 파라미터화되지 않은 상태이므로 피드포워드 단계에서 중요한 정보를 많이 잃어버리기 쉽다. 따라서 본 논문에서 제안하는 경량 모델의 목적은 \\(I(X,X)\\)에서 \\(I(Y,X)\\)을 정확하게 필터링하는 것이다. \\(X\\)의 정보를 완전히 보존하는 것은 달성하기 어렵다. 이상의 분석을 바탕으로 모델을 갱신하기 위해 신뢰할 수 있는 기울기를 생성할 수 있을 뿐만 아니라, 얕고 가벼운 신경망에 적합할 수 있는 새로운 심층 신경망 학습 방법을 제안하고자 한다.\n' +
      '\n' +
      '## 4 Methodology\n' +
      '\n' +
      '### 프로그래머블 그라데이션 정보\n' +
      '\n' +
      '앞서 언급한 문제점을 해결하기 위해 그림 3(d)와 같이 PGI(Programmable Gradient Information)라는 새로운 보조 감독 프레임워크를 제안한다. PGI는 주로 (1) 주 분기, (2) 보조 가역 분기, (3) 다단계 보조 정보의 세 가지 구성 요소를 포함한다. 그림 3(d)에서 PGI의 추론 과정은 주 분기만 사용하므로 추가 추론 비용이 필요하지 않음을 알 수 있다. 다른 두 가지 구성 요소는 딥러닝 방법에서 몇 가지 중요한 문제를 해결하거나 늦추는 데 사용된다. 이 중 보조 가역 브랜치는 신경망의 심화로 인한 문제를 다루기 위해 고안되었다. 네트워크 심화는 정보 병목 현상을 유발하여 손실 함수가 신뢰할 수 있는 기울기를 생성할 수 없게 됩니다. 다단계 보조 정보는 특히 다중 예측 분기의 아키텍처 및 경량 모델에 대해 깊은 감독으로 인한 오류 누적 문제를 처리하도록 설계되었다. 다음으로 이 두 가지 구성요소를 단계별로 소개하겠습니다.\n' +
      '\n' +
      '######4.1.1 보조가역지점\n' +
      '\n' +
      'PGI에서는 신뢰성 있는 기울기 생성과 네트워크 파라미터 갱신을 위한 보조 가역 분기를 제안한다. 데이터로부터 타겟으로 매핑하는 정보를 제공함으로써, 손실 함수는 안내를 제공할 수 있고 타겟과 덜 관련된 불완전한 피드포워드 피처로부터 잘못된 상관을 찾을 가능성을 피할 수 있다. 우리는 가역 아키텍처를 도입하여 완전한 정보의 유지를 제안하지만, 가역 아키텍처에 주 분기를 추가하는 것은 많은 추론 비용을 소모할 것이다. 그림 3(b)의 아키텍처를 분석한 결과 깊은 층에서 얕은 층까지 추가 연결이 추가되면 추론 시간이 20% 증가하는 것으로 나타났다. 우리가 입력 데이터를 네트워크(노란색 상자)의 고해상도 컴퓨팅 계층에 반복적으로 추가할 때 추론 시간은 심지어 시간의 두 배를 초과한다.\n' +
      '\n' +
      '우리의 목표는 신뢰할 수 있는 기울기를 얻기 위해 가역 아키텍처를 사용하는 것이기 때문에 추론 단계에서 "가역"만이 필요한 조건은 아니다. 이를 감안하여 가역적 분기를 심층 감독 분기의 확장으로 간주한 후 그림 3(d)와 같이 보조적 가역적 분기를 설계한다. 정보 병목 현상으로 인해 중요한 정보가 손실되었을 주요 분기 심층 특징은 보조 가역 분기로부터 신뢰할 수 있는 기울기 정보를 수신할 수 있다. 이러한 그래디언트 정보는 정확하고 중요한 정보를 추출하는 것을 돕기 위한 파라미터 학습을 구동시킬 것이며, 위의 액션들은 메인 브랜치가 타겟 태스크에 더 효과적인 특징들을 획득할 수 있게 할 수 있다. 더욱이, 가역 아키텍처는 복잡한 작업이 더 깊은 네트워크에서 변환을 요구하기 때문에 일반적인 네트워크보다 얕은 네트워크에서 더 나쁜 성능을 수행한다. 제안된 방법은 주 분기가 완전한 원본 정보를 유지하도록 강제하지 않고 보조 감독 메커니즘을 통해 유용한 기울기를 생성하여 업데이트한다. 이 설계의 장점은 제안된 방법이 더 얕은 네트워크에도 적용될 수 있다는 것이다.\n' +
      '\n' +
      '그림 3: PGI 및 관련 네트워크 아키텍처 및 방법. (a) Path Aggregation Network (PAN) [37], (b) Reversible Columns (RevCol) [3], (c) conventional deep supervision, (d) our proposed Programmable Gradient Information (PGI). PGI는 크게 (1) 메인 브랜치: 추론에 사용되는 아키텍처, (2) 보조 가역 브랜치: 역방향 전송을 위한 메인 브랜치를 공급하기 위한 신뢰성 있는 그라디언트 생성, (3) 멀티 레벨 보조 정보: 제어 메인 브랜치 학습 플래너블 멀티 레벨 시맨틱 정보의 세 가지 구성요소로 구성된다.\n' +
      '\n' +
      '마지막으로, 추론 단계 동안 보조 가역 분기가 제거될 수 있기 때문에, 원래 네트워크의 추론 능력들이 유지될 수 있다. 우리는 또한 보조 가역 분기의 역할을 수행하기 위해 PGI에서 임의의 가역 아키텍처를 선택할 수 있다.\n' +
      '\n' +
      '다단계 보조정보 4.1.2\n' +
      '\n' +
      '이 절에서는 다단계 보조 정보가 어떻게 작동하는지 논의할 것이다. 다중 예측 분기를 포함하는 심층 감독 아키텍처는 그림 3(c)와 같다. 객체 검출을 위해, 상이한 특징 피라미드들은 상이한 태스크들을 수행하기 위해 사용될 수 있으며, 예를 들어 이들은 상이한 크기의 객체들을 검출할 수 있다. 따라서, 깊은 감독 브랜치에 연결한 후, 얕은 피처들은 작은 오브젝트 검출에 필요한 피처들을 학습하도록 안내될 것이고, 이 때 시스템은 다른 사이즈의 오브젝트들의 포지션들을 배경으로 간주할 것이다. 그러나, 상기 증서는 깊은 피쳐 피라미드가 타겟 객체를 예측하는 데 필요한 많은 정보를 잃게 할 것이다. 이 문제와 관련하여 우리는 각 특징 피라미드가 모든 대상 객체에 대한 정보를 수신해야 후속 주 분기가 다양한 대상에 대한 예측을 학습하기 위해 완전한 정보를 보유할 수 있다고 믿는다.\n' +
      '\n' +
      '다단계 보조정보의 개념은 보조감독의 특징 피라미드 계층층과 본점 사이에 통합망을 삽입한 후 이를 이용하여 그림 3(d)와 같이 서로 다른 예측헤드로부터 반환되는 그라디언트를 결합하는 것이다. 그런 다음 다단계 보조 정보는 모든 대상 객체가 포함된 기울기 정보를 집계하고 이를 주 분기로 전달한 다음 매개변수를 업데이트한다. 이때 본점의 특징 피라미드 위계의 특징은 특정 객체의 정보에 의해 지배되지 않을 것이다. 결과적으로, 우리의 방법은 깊은 감독에서 깨진 정보 문제를 완화할 수 있다. 또한, 임의의 통합 네트워크는 멀티-레벨 보조 정보에 사용될 수 있다. 따라서, 우리는 서로 다른 크기의 네트워크 아키텍처의 학습을 안내하기 위해 필요한 의미 수준을 계획할 수 있다.\n' +
      '\n' +
      '### Generalized ELAN\n' +
      '\n' +
      '본 절에서는 제안된 새로운 네트워크 구조인 GELAN에 대해 설명한다. 경사 경로 계획으로 설계된 CSPNet[64]과 ELAN[65]의 두 신경망 구조를 결합하여 경량화, 추론 속도, 정확도를 고려한 일반화된 효율적인 계층 집합 네트워크(GELAN)를 설계하였다. 전체 아키텍처는 그림 4에 나와 있다. 원래 컨볼루션 레이어의 적층만을 사용하던 ELAN[65]의 성능을 모든 계산 블록을 사용할 수 있는 새로운 아키텍처로 일반화했다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '제안된 방법을 MS COCO 데이터셋을 이용하여 검증한다. 모든 실험 설정은 YOLOv7 AF[63]를 따르는 반면 데이터 세트는 MS COCO 2017 분할이다. 우리가 언급한 모든 모델은 트레인-드래치 전략을 사용하여 훈련되며 총 훈련 횟수는 500회이다. 학습률을 설정함에 있어 처음 세 개의 에폭에서 선형 웜업을 사용하고, 이후 에폭에서는 모델 척도에 따라 해당 붕괴 방식을 설정한다. 지난 15개의 에폭에 대해, 우리는 모자이크 데이터 증강을 끕니다. 자세한 설정은 부록을 참조하십시오.\n' +
      '\n' +
      '### Implimentation Details\n' +
      '\n' +
      '우리는 각각 YOLOv7[63]과 Dynamic YOLOv7[36]을 기반으로 일반 버전과 확장 버전의 YOLOv9를 구축했다. 네트워크 구조의 설계에서 계산 블록으로 CSPNet 블록[64]을 계획 RepConv[63]으로 사용하여 ELAN[65]을 GELAN으로 대체했다. 또한 다운샘플링 모듈을 단순화하고 앵커가 없는 예측 헤드를 최적화했다. PGI의 보조 손실 부분은 YOLOv7의 보조 헤드 설정을 완전히 따릅니다. 자세한 내용은 부록을 참조하십시오.\n' +
      '\n' +
      '도 4: GELAN의 아키텍처: (a) CSPNet[64], (b) ELAN[65], 및 (c) 제안된 GELAN. 우리는 CSPNet을 모방하고 ELAN을 모든 계산 블록을 지원할 수 있는 GELAN으로 확장한다.\n' +
      '\n' +
      '### 최신식과의 비교\n' +
      '\n' +
      '표 1은 제안된 YOLOv9와 다른 스크래치에서 실시간 객체 검출기의 비교를 나열한다. 전체적으로 기존 방법 중 가장 성능이 좋은 방법은 경량 모델의 경우 YOLO MS-S[7], 중형 모델의 경우 YOLO MS[7], 일반 모델의 경우 YOLOv7 AF[63], 대형 모델의 경우 YOLOv8-X[15]이다. 경량 및 중형 모델 YOLO MS[7]에 비해 YOLOv9는 약 10% 적은 매개변수 및 5\\(\\sim\\)15% 적은 계산량을 갖지만 여전히 AP에서 0.4\\(\\sim\\)0.6% 향상된 성능을 갖는다. YOLOv7 AF와 비교하여 YOLOv9-C는 42% 적은 매개변수 및 21% 적은 계산을 갖지만 동일한 AP(53%)를 달성한다. YOLOv8-X와 비교하여 YOLOv9-X는 파라미터가 15% 적고 계산이 25% 적으며 AP가 1.7% 크게 개선되었다. 위의 비교 결과는 제안된 YOLOv9가 기존 방법에 비해 모든 측면에서 크게 개선되었음을 보여준다.\n' +
      '\n' +
      '한편, 이미지넷 사전학습 모델도 비교에 포함시켰으며, 그 결과는 그림 5와 같으며, 각각 파라미터와 연산량을 기준으로 비교하였다. 파라미터의 수에 있어서, 가장 성능이 좋은 대형 모델은 RT DETR[43]이다. 그림 5를 통해 파라미터 활용도에서 깊이별 컨벌루션을 사용하는 YOLOv9가 깊이별 컨벌루션을 사용하는 YOLO MS보다 훨씬 우수함을 알 수 있다. 대형 모델의 매개변수 활용도 이미지넷 사전 훈련 모델을 사용한 RT DETR을 크게 능가한다. 더욱 좋은 점은 딥 모델에서 YOLOv9가 PGI 사용의 큰 장점을 보여준다는 점이다. 데이터를 표적에 매핑하는 데 필요한 정보를 정확하게 유지하고 추출함으로써 RT DETR-X와 같은 정확도를 유지하면서 매개변수의 64%만 필요로 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **\\#Param. (M)** & **FLOPs (G)** & **AP\\({}_{0.95}^{val}\\) (\\%)** & **AP\\({}_{50}^{val}\\) (\\%)** & **AP\\({}_{75}^{val}\\) (\\%)** & **AP\\({}_{S}^{val}\\) (\\%)** & **AP\\({}_{M}^{val}\\) (\\%)** & **AP\\({}_{L}^{val}\\) (\\%)** \\\\ \\hline\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      ' (Ours)** & 58.1 & 192.5 & 55.6 & 72.8 & 60.6 & 40.2 & 61.0 & 71.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 최첨단-첨단-실시간 객체 검출기의 비교.\n' +
      '\n' +
      '연산량은 가장 작은 모델부터 가장 큰 모델까지 현존하는 모델이 YOLO MS[7], PP YOLOE[74], RT DETR[43]이다. 그림 5에서 YOLOv9가 계산 복잡도 측면에서 트레인-from-스크래치 방법보다 훨씬 우수함을 알 수 있다. 또한, 깊이-와이즈 컨볼루션 및 이미지넷 기반 사전 훈련된 모델에 기반한 것과 비교하면 YOLOv9도 매우 경쟁력이 있다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '###### 5.4.1 일반화 ELAN\n' +
      '\n' +
      'GELAN의 경우 먼저 계산 블록에 대한 절제 연구를 수행한다. 실험을 수행하기 위해 Res 블록[21], Dark 블록[49], CSP 블록[64]을 각각 사용했다. 표 2는 ELAN에서 컨볼루션 계층들을 상이한 계산 블록들로 대체한 후, 시스템이 양호한 성능을 유지할 수 있음을 보여준다. 사용자는 실제로 계산 블록을 대체하여 각각의 추론 장치에 사용할 수 있다. 상이한 계산 블록 대체들 중에서, CSP 블록들은 특히 잘 수행된다. 그들은 파라미터와 계산량을 줄일 뿐만 아니라 AP를 0.7% 향상시킨다. 따라서 우리는 YOLOv9에서 GELAN의 구성요소 단위로 CSP-ELAN을 선택한다.\n' +
      '\n' +
      '다음으로 서로 다른 크기의 GELAN에 대해 ELAN 블록 깊이 및 CSP 블록 깊이 실험을 수행하고, 그 결과를 표 3에 표시하였다. ELAN의 깊이를 1에서 2로 증가시키면 정확도가 크게 향상됨을 알 수 있다. 그러나 깊이가 2 이상일 경우 ELAN 깊이 또는 CSP 깊이를 개선하더라도 매개변수 수, 계산량 및 정확도는 항상 선형 관계를 나타낸다. 이는 GELAN이 깊이에 민감하지 않다는 것을 의미한다. 즉, 사용자는 GELAN 내의 컴포넌트를 임의로 조합하여 네트워크 아키텍처를 설계할 수 있으며, 특별한 설계 없이 안정적인 성능을 가진 모델을 가질 수 있다. 표 3에서 YOLOv9-{S,M,C}의 경우 ELAN 깊이와 CSP 깊이의 페어링을 {{2, 3}, {2, 1}, {2, 1}}로 설정하였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Model** & **CB type** & **\\#Param.** & **FLOPs** & \\(\\mathbf{AP}_{50:95}^{\\circ d}\\) \\\\ \\hline\n' +
      '**GELAN-S** & Conv & 6.3M & 24.0G & 44.8\\%\n' +
      '**GELAN-S** & Res [21 & 5.5M & 21.1G & 44.3\\% \\\\\\\n' +
      'GELAN-S** & Dark [49] & 5.7M & 21.8G & 44.5\\%\n' +
      '**GELAN-S** & CSP [64] & 5.9M & 22.5G & 45.5\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '* 계산 블록 타입으로서 CB 타입 노드.\n' +
      '*\\({}^{2}\\)-S 노드들의 작은 크기 모델.\n' +
      '\n' +
      '\\end{table}\n' +
      '표 2: 다양한 계산 블록에 대한 절제 연구.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Model** & **D\\({}_{ELAN}\\)** & **D\\({}_{CSP}\\)** & **\\#Param.** & **FLOPs** & \\(\\mathbf{AP}_{50:95}^{\\circ d}\\) \\\\ \\hline\n' +
      'GELAN-S** & 2 & 1 & 5.9M & 22.5G & 45.5\\%\n' +
      '**GELAN-S** & 2 & 2 & 6.6M & 24.6G & 46.0\\%\n' +
      'GELAN-S** & 3 & 1 & 7.1M & 26.4G & 46.5\\%\n' +
      '**GELAN-S** & 2 & 3 & 7.2M & 26.7G & 46.7\\% \\\\ \\hline\n' +
      '**GELAN-M** & 2 & 1 & 20.1M & 76.8G & 51.1\\%\n' +
      '**GELAN-M** & 2 & 22.4M & 86.1G & 51.7\\%\n' +
      'GELAN-M** & 3 & 1 & 24.5M & 94.2G & 51.8\\%\n' +
      '**GELAN-M** & 2 & 3 & 24.8M & 95.5G & 52.3\\% \\\\ \\hline\n' +
      '**GELAN-C** & 1 & 1 & 19.0M & 77.8G & 50.7\\%\n' +
      'GELAN-C** & 2 & 1 & 25.5M & 102.8G & 52.5\\%\n' +
      '**GELAN-C** & 2 & 2 & 28.9M & 115.8G & 53.0\\%\n' +
      '**GELAN-C** & 3 & 1 & 32.0M & 127.9G & 53.2\\%\n' +
      '**GELAN-C** & 2 & 3 & 32.4M & 128.7G & 53.3\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '*\\({}^{1}\\)**D\\({}_{ELAN}\\)**와 **D\\({}_{CSP}\\)**는 각각 ELAN과 CSP의 깊이 노드이다.\n' +
      '*\\({}^{2}\\)-{S, M, C}는 소형, 중형, 소형 모델을 나타낸다.\n' +
      '\n' +
      '\\end{table}\n' +
      '표 3: ELAN 및 CSP 깊이에 대한 절제 연구.\n' +
      '\n' +
      '그림 5: 최첨단 실시간 객체 검출기의 비교. 비교에 참여하는 방법은 모두 이미지넷을 RT DETR[43], RTMDet[44], PP-YOLOE[74] 등을 포함하여 사전 훈련된 가중치로 사용한다. Train-from-scratch 방법을 사용하는 YOLOv9는 다른 방법의 성능을 분명히 능가한다.\n' +
      '\n' +
      '###### 5.4.2 프로그래머블 기울기 정보\n' +
      '\n' +
      'PGI 측면에서, 우리는 척추와 목에 각각 보조 가역 가지와 다단계 보조 정보에 대한 절제 연구를 수행했다. 다단계 가역 정보를 얻기 위해 DHLC[34] 연결을 사용하기 위해 보조 가역 분기 ICN을 설계했다. 다단계 보조 정보는 절제 연구에 FPN과 PAN을 사용하며 PFH의 역할은 전통적인 심층 감독과 동일하다. 모든 실험의 결과는 표 4에 나열되어 있다. 표 4에서 PFH는 심층 모델에서만 효과적인 반면 제안된 PGI는 다른 조합에서 정확도를 향상시킬 수 있음을 알 수 있다. 특히 ICN을 사용할 때 안정적이고 더 나은 결과를 얻을 수 있다. 또한 YOLOv7[63]에서 제안한 리드헤드 유도 과제를 PGI의 보조 감독에 적용하려고 노력하였고, 훨씬 더 나은 성능을 달성하였다.\n' +
      '\n' +
      '다양한 크기의 모델에 대해 PGI와 심층 감독의 개념을 추가로 구현하고 결과를 비교했으며, 이러한 결과는 표 5에 나와 있으며, 초기에 분석된 바와 같이 심층 감독의 도입은 얕은 모델에 대한 정확성의 손실을 초래할 것이다. 일반 모델은 심층 감독을 도입하는 것이 불안정한 성능을 야기할 것이고, 심층 감독의 설계 개념은 극히 심층적인 모델에서만 이득을 가져올 수 있다. 제안된 PGI는 정보 병목 현상 및 정보 파손과 같은 문제를 효과적으로 처리할 수 있으며, 다양한 크기의 모델의 정확도를 종합적으로 향상시킬 수 있다. PGI의 개념은 두 가지 가치 있는 기여를 가져온다. 첫 번째는 얕은 모델에 보조 감독 방법을 적용할 수 있도록 하는 것이고, 두 번째는 깊은 모델 훈련 과정이 더 신뢰할 수 있는 기울기를 얻을 수 있도록 하는 것이다. 이러한 기울기를 통해 심층 모델은 데이터와 대상 간의 올바른 상관 관계를 설정하기 위해 보다 정확한 정보를 사용할 수 있다.\n' +
      '\n' +
      '마지막으로 기준선 YOLOv7에서 YOLOv9-E로 구성 요소를 점진적으로 증가시킨 결과를 표에 보여준다. 우리가 제안한 GELAN과 PGI는 모델에 전면적인 개선을 가져왔다.\n' +
      '\n' +
      '### Visualization\n' +
      '\n' +
      '이 섹션에서는 정보 병목 현상 문제를 탐색하고 시각화합니다. 또한, 제안된 PGI가 데이터와 대상 간의 올바른 상관 관계를 찾기 위해 신뢰할 수 있는 기울기를 사용하는 방법을 시각화할 것이다. 그림 6에서 우리는 서로 다른 아키텍처에서 무작위 초기 가중치를 피드포워드로서 사용하여 얻은 특징 맵의 시각화 결과를 보여준다. 레이어의 수가 증가함에 따라 모든 아키텍처의 원래 정보가 점차 감소하는 것을 알 수 있다. 예를 들어, PlainNet의 50({}^{th}\\) 레이어에서는 객체의 위치를 보기 어렵고, 100({}^{th}\\) 레이어에서는 식별 가능한 모든 특징이 손실된다. ResNet은 50\\({}^{th}\\) 레이어에서 물체의 위치를 여전히 볼 수 있지만 경계 정보가 손실되었다. 깊이가 100\\({}^{th}\\) 층에 도달하면 전체 이미지가 흐릿해진다. CSPNet과 제안된 GELAN은 모두 매우 좋은 성능을 보이며, 200\\({}^{th}\\) 레이어까지 객체의 명확한 식별을 지원하는 특징을 유지할 수 있다. 비교 중 GELAN은 더 안정적인 결과와 더 명확한 경계 정보를 가지고 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline\n' +
      '모델** & **#Param.** & **FLOPs** & \\(\\mathbf{AP}_{50.95}^{val}\\) & \\(\\mathbf{AP}_{S}^{val}\\) & \\(\\mathbf{AP}_{M}^{val}\\) & \\(\\mathbf{AP}_{L}^{val}\\) \\\\\n' +
      'YOLOv7**[63] & 36.9 & 104.7 & 51.2\\% & 31.8\\% & 55.5\\% & 65.0\\%\n' +
      '**+ AF[63]** & 43.6 & 130.5 & 53.0\\% & 35.8\\% & 58.7\\% & 68.9\\%\n' +
      '**+ GELAN** & 41.7 & 127.9 & 53.2\\% & 36.2\\% & 58.5\\% & 69.9\\%\n' +
      '**+ DHLC [34]** & 58.1 & 192.5 & 55.0\\% & 38.0\\% & 60.6\\% & 70.9\\% \\\\\\%\n' +
      '**+ PGI** & 58.1 & 192.5 & 55.6\\% & 40.2\\% & 61.0\\% & 71.4\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      'ELAN과 CSP의 노드 깊이는 각각 \\({}^{1}\\)D\\({}_{ELAN}\\)과 \\(\\mathbf{D}_{CSP}\\)이다.\n' +
      '*\\({}^{2}\\)LHG는 YOLOv7[63]에 의해 제안된 리드 헤드 유도 훈련을 나타낸다.\n' +
      '\n' +
      '\\end{table}\n' +
      '표 6: GELAN 및 PGI에 대한 절제 연구.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline\n' +
      '모델** & \\(\\mathbf{G}_{backbone}\\) & \\(\\mathbf{G}_{neck}\\) & \\(\\mathbf{AP}_{50.95}^{val}\\) & \\(\\mathbf{AP}_{50}^{val}\\) & \\(\\mathbf{AP}_{M}^{val}\\) & \\(\\mathbf{AP}_{L}^{val}\\) \\\\(\\mathbf{AP}_{L}^{val}\\)\n' +
      '**GELAN-C** & \\(-\\\\) & \\(-\\) & 52.5\\% & 35.8\\% & 57.6\\% & **69.4\\%**\n' +
      '**GELAN-C** & PFH &\\(-\\) & 52.5\\% & 35.3\\% & 58.1\\% & 68.9\\% \\\\\\% & 58.1\\% & 68.9\\%\n' +
      '**GELAN-C** & FPN &\\(-\\) & 52.6\\% & 35.3\\% & 58.1\\% & 68.9\\% \\\\\\% & 68.9\\%\n' +
      '**GELAN-C** & \\(-\\\\) & ICN & 52.7\\% & 35.3\\% & 58.4\\% & 68.9\\% \\\\\\% &\n' +
      '**GELAN-C** & FPN & ICN & 52.8\\% & 35.8\\% & 58.2\\% & 69.1\\%\n' +
      '**GELAN-C** & ICN &\\(-\\) & **52.9\\%** & 35.2\\% & **58.7\\%** & 68.6\\% \\\\\\%\n' +
      '**GELAN-C** & LHG-ICN & \\(-\\) & **53.0\\%** & **36.3\\%** & 58.5\\% & 69.1\\% \\\\ \\hline\n' +
      '**GELAN-E** & \\(-\\\\) & 55.0\\% & 38.0\\% & 60.6\\% & 70.9\\% \\\\\\% & 70.9\\%\n' +
      '**GELAN-E** & PFH &\\(-\\) & 55.3\\% & 38.3\\% & 60.3\\% & 71.6\\% \\\\\\% & 71.6\\%\n' +
      '**GELAN-E** & FPN &\\(-\\) & **55.6\\%** & **40.2\\%** & 61.0\\% & 71.4\\% \\\\%\n' +
      '**GELAN-E** & PAN & \\(-\\) & 55.5\\% & 39.0\\% & **61.1\\%** & 71.5\\% \\\\%\n' +
      '**GELAN-E** & FPN & ICN & **55.6\\%** & 39.8\\% & 60.9\\% & **71.9\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      'ELAN과 CSP의 노드 깊이는 각각 \\({}^{1}\\)D\\({}_{ELAN}\\)과 \\(\\mathbf{D}_{CSP}\\)이다.\n' +
      '*\\({}^{2}\\)LHG는 YOLOv7[63]에 의해 제안된 리드 헤드 유도 훈련을 나타낸다.\n' +
      '\n' +
      '\\end{table}\n' +
      '표 4: 척추와 목의 PGI에 대한 절제 연구.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '모델** & \\(\\mathbf{AP}_{50.95}^{val}\\) & \\(\\mathbf{AP}_{50}^{val}\\) & \\(\\mathbf{AP}_{75}^{val}\\) & \\(\\mathbf{AP}_{75}^{val}\\) & \\(\\mathbf{AP}_{75}^{val}\\) & \\\\(\\mathbf{AP}_{75}^{val}\\) & \\\\\n' +
      '**GELAN-S** & 46.7\\% & 63.0\\% & **50.7\\%** &\\\\\n' +
      '**+ DS** & 46.5\\% & -0.2 & 62.9\\% & -0.1 & 50.5\\% & -0.2\\%\n' +
      '**+ PGI** & **46.8\\%** & +0.1 & **63.4\\%** & +0.4 & **50.7\\%** & = \\\\ \\hline\n' +
      '**GELAN-M** & 51.1\\% & 67.9\\% & 55.7\\% &\\\n' +
      '**+ DS** & 51.2\\% & +0.1 & **68.2\\%** & +0.3 & 55.7\\% & \\\\\n' +
      '**+ PGI** & **51.4\\%** & +0.3 & 68.1\\% & +0.2 & **56.1\\%** & +0.4 \\\\ \\hline\n' +
      '**GELAN-C** & 52.5\\% & & 69.5\\% & 57.3\\% &\\\\\n' +
      '**+ DS** & 52.5\\% & & 69.9\\% & +0.4 & 57.1\\% & -0.2\\\\%\n' +
      '**+ PGI** & **53.0\\%** & +0.5 & **70.3\\%** & +0.8 & **57.8\\%** & +0.5 \\\\ \\hline\n' +
      '**GELAN-E** & 55.0\\% & & 71.9\\% & 60.0\\% &\\\n' +
      '**+ DS** & 55.3\\% & +0.3 & 72.3\\% & +0.4 & 60.2\\% & +0.2\\%\n' +
      '**+ PGI** & **55.6\\%** & +0.6 & **72.8\\%** & +0.9 & **60.6\\%** & +0.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '* DS는 깊은 감독을 나타낸다.\n' +
      '\n' +
      '\\end{table}\n' +
      '표 5: PGI에 대한 절제 연구.\n' +
      '\n' +
      '그림 7은 PGI가 트레이닝 프로세스 동안 더 신뢰할 수 있는 그라디언트를 제공할 수 있는지 여부를 보여주기 위해 사용되며, 따라서 업데이트에 사용되는 파라미터들은 입력 데이터와 타겟 사이의 관계를 효과적으로 캡처할 수 있다. 그림 7은 PAN 바이어스 워밍업에서 GELAN과 YOLOv9(GELAN + PGI)의 특징 맵의 시각화 결과를 보여준다. 그림 7(b)와 (c)의 비교를 통해 PGI가 객체를 포함하는 영역을 정확하고 간결하게 포착한다는 것을 명확히 알 수 있다. PGI를 사용하지 않는 GELAN은 객체 경계를 탐지할 때 발산이 있음을 발견했으며 일부 배경 영역에서도 예상치 못한 응답을 생성했다. 이 실험은 PGI가 실제로 매개변수를 업데이트하고 주 분기의 피드포워드 단계가 더 중요한 기능을 유지할 수 있도록 더 나은 기울기를 제공할 수 있음을 확인한다.\n' +
      '\n' +
      '## 6 Conclusions\n' +
      '\n' +
      '본 논문에서는 PGI를 사용하여 정보 병목 현상과 심층 감독 메커니즘이 경량 신경망에 적합하지 않다는 문제를 해결하는 것을 제안한다. 고효율 경량 신경망인 GELAN을 설계했습니다. 객체 검출의 관점에서, GELAN은 상이한 계산 블록 및 깊이 설정에서 강하고 안정적인 성능을 갖는다. 그것은 실제로 다양한 추론 장치에 적합한 모델로 광범위하게 확장될 수 있다. 위의 두 가지 문제에 대해 PGI를 도입하면 경량 모델과 심층 모델 모두 정확도에서 상당한 개선을 달성할 수 있다. PGI와 GELAN을 접목해 설계한 욜로브9는 강력한 경쟁력을 보여줬다. 우수한 설계로 딥 모델은 YOLOv8에 비해 매개변수 수를 49%, 계산량을 43% 줄일 수 있지만 MS COCO 데이터 세트에서는 여전히 0.6%의 AP 개선이 있다.\n' +
      '\n' +
      '## 7 Acknowledgements\n' +
      '\n' +
      '저자들은 컴퓨팅 및 스토리지 리소스를 제공한 국립 고성능 컴퓨팅 센터(NCHC)에 감사를 표하고 싶습니다.\n' +
      '\n' +
      '도 6: 서로 다른 깊이에서 PlainNet, ResNet, CSPNet, GELAN의 무작위 초기 가중치에 의해 출력되는 특징 맵(시각화 결과) 100개의 레이어 이후, ResNet은 오브젝트 정보를 난독화하기에 충분한 피드포워드 출력을 생성하기 시작한다. 제안된 GELAN은 150\\({}^{th}\\) 층까지 매우 완전한 정보를 유지할 수 있으며, 200\\({}^{th}\\) 층까지 충분히 판별할 수 있다.\n' +
      '\n' +
      '도 7: 바이어스 워밍업의 한 에폭 이후의 GELAN 및 YOLOv9(GELAN + PGI)의 PAN 특징 맵(시각화 결과) GELAN은 원래 약간의 차이가 있었지만 PGI의 가역적인 가지를 추가한 후 대상 물체에 집중할 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT pre-training of image transformers. In _International Conference on Learning Representations (ICLR)_, 2022.\n' +
      '* [2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. YOLOv4: Optimal speed and accuracy of object detection. _arXiv preprint arXiv:2004.10934_, 2020.\n' +
      '* [3] Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiangwen Kong, Jun Li, and Xiangyu Zhang. Reversible column networks. In _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* [4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 213-229, 2020.\n' +
      '* [5] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, and Junri Zou. AP-loss for accurate one-stage object detection. _IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)_, 43(11):3782-3798, 2020.\n' +
      '* [6] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang, Wenrui Dai, Hongkai Xiong, and Qi Tian. SdAE: Self-distillated masked autoencoder. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 108-124, 2022.\n' +
      '* [7] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin Hou, and Ming-Ming Cheng. YOLO-MS: rethinking multi-scale representation learning for real-time object detection. _arXiv preprint arXiv:2308.05480_, 2023.\n' +
      '* [8] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, and Lu Yuan. DaVIT: Dual attention vision transformers. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 74-92, 2022.\n' +
      '* [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations (ICLR)_, 2021.\n' +
      '* [10] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott, and Weilin Huang. TOOD: Task-aligned one-stage object detection. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 3490-3499, 2021.\n' +
      '* [11] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip Torr. Res2Net: A new multi-scale backbone architecture. _IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)_, 43(2):652-662, 2019.\n' +
      '* [12] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian Sun. OTA: Optimal transport assignment for object detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 303-312, 2021.\n' +
      '* [13] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. YOLOX: Exceeding YOLO series in 2021. _arXiv preprint arXiv:2107.08430_, 2021.\n' +
      '* [14] Jocher Glenn. YOLOv5 release v7.0. [https://github.com/ultralytics/yolov5/releases/tag/v7.0](https://github.com/ultralytics/yolov5/releases/tag/v7.0), 2022.\n' +
      '* [15] Jocher Glenn. YOLOv8 release v8.1.0. [https://github.com/ultralytics/ultralytics/v8.1.0](https://github.com/ultralytics/ultralytics/v8.1.0), 2024.\n' +
      '* [16] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network: Backpropagation without storing activations. _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.\n' +
      '* [17] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_, 2023.\n' +
      '* [18] Chaoxu Guo, Bin Fan, Qian Zhang, Shiming Xiang, and Chunhong Pan. AugFPN: Improving multi-scale feature learning for object detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 12595-12604, 2020.\n' +
      '* [19] Qi Han, Yuxuan Cai, and Xiangyu Zhang. RevColV2: Exploring disentangled representations in masked image modeling. _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.\n' +
      '* [20] Zeeshan Hayder, Xuming He, and Mathieu Salzmann. Boundary-aware instance segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5696-5704, 2017.\n' +
      '* [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2016.\n' +
      '* [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 630-645. Springer, 2016.\n' +
      '* [23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4700-4708, 2017.\n' +
      '* [24] Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, and Winston H Hsu. MonoDTR: Monocular 3D object detection with depth-aware transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4012-4021, 2022.\n' +
      '* [25] Lin Huang, Weisheng Li, Linlin Shen, Haojie Fu, Xue Xiao, and Suihan Xiao. YOLOCS: Object detection based on dense channel compression for feature spatial solidification. _arXiv preprint arXiv:2305.04170_, 2023.\n' +
      '* [26] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In _International Conference on Machine Learning (ICML)_, pages 4651-4664, 2021.\n' +
      '* [27] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of NAACL-HLT_, volume 1, page 2, 2019.\n' +
      '**[28] Lee Chen-Yu, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu. 심층 감시망 _Artificial Intelligence and Statistics_에서, 페이지 562-570, 2015.\n' +
      '* [29] Alex Levinshtein, Alborz Rezazadeh Serenskkeh, and Konstantinos Derpanis. DATNet: Dense auxiliary tasks for object detection. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pages 1419-1427, 2020.\n' +
      '* [30] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangxiang Chu. YOLOv6 v3.0: A full-scale reloading. _arXiv preprint arXiv:2301.05586_, 2023.\n' +
      '* [31] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng, Weiqiang Nie, et al. YOLOv6: A single-stage object detection framework for industrial applications. _arXiv preprint arXiv:2209.02976_, 2022.\n' +
      '* [32] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2691-2700, 2023.\n' +
      '* [33] Shuai Li, Chenhang He, Ruihuang Li, and Lei Zhang. A dual weighting label assignment scheme for object detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9387-9396, 2022.\n' +
      '* [34] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang, Zhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. CBNet: A composite backbone network architecture for object detection. _IEEE Transactions on Image Processing (TIP)_, 2022.\n' +
      '* [35] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2117-2125, 2017.\n' +
      '* [36] Zhihao Lin, Yongtao Wang, Jinhe Zhang, and Xiaojie Chu. DynamicDet: A unified dynamic architecture for object detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6282-6291, 2023.\n' +
      '* [37] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8759-8768, 2018.\n' +
      '* [38] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vamaba: Visual state space model. _arXiv preprint arXiv:2401.10166_, 2024.\n' +
      '* [39] Yudong Liu, Yongtao Wang, Siwei Wang, TingTing Liang, Qijie Zhao, Zhi Tang, and Haibin Ling. CBNet: A novel composite backbone network architecture for object detection. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, pages 11653-11660, 2020.\n' +
      '* [40] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 10012-10022, 2021.\n' +
      '* [42] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A ConvNet for the 2020s. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11976-11986, 2022.\n' +
      '* [43] Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang, Jinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, and Yi Liu. DETRs beat YOLOs on real-time object detection. _arXiv preprint arXiv:2304.08069_, 2023.\n' +
      '* [44] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen. RTMDet: An empirical study of designing real-time object detectors. _arXiv preprint arXiv:2212.07784_, 2022.\n' +
      '* [45] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan Kalkan. A ranking-based, balanced loss function unifying classification and localisation in object detection. _Advances in Neural Information Processing Systems (NeurIPS)_, 33:15534-15545, 2020.\n' +
      '* [46] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan Kalkan. Rank & sort loss for object detection and instance segmentation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 3009-3018, 2021.\n' +
      '* [47] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 779-788, 2016.\n' +
      '* [48] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster, stronger. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7263-7271, 2017.\n' +
      '* [49] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental improvement. _arXiv preprint arXiv:1804.02767_, 2018.\n' +
      '* [50] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: A metric and a loss for bounding box regression. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 658-666, 2019.\n' +
      '* [51] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang, Yurong Chen, and Xiangyang Xue. Object detection from scratch with deep supervision. _IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)_, 42(2):398-412, 2019.\n' +
      '* [52] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for robotic manipulation.\n' +
      '\n' +
      '_Conference on Robot Learning (CoRL)_, pages 785-799, 2023.\n' +
      '* [53] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan, Changhu Wang, and Ping Luo. What makes for end-to-end object detection? In _International Conference on Machine Learning (ICML)_, pages 9934-9944, 2021.\n' +
      '* [54] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1-9, 2015.\n' +
      '* [55] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2818-2826, 2016.\n' +
      '* [56] Zineng Tang, Jaemin Cho, Jie Lei, and Mohit Bansal. Perceiver-VL: Efficient vision-and-language modeling with iterative latent attention. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pages 4410-4420, 2023.\n' +
      '* [57] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS: Fully convolutional one-stage object detection. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9627-9636, 2019.\n' +
      '* [58] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS: A simple and strong anchor-free object detector. _IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)_, 44(4):1922-1933, 2022.\n' +
      '* [59] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In _IEEE Information Theory Workshop (ITW)_, pages 1-5, 2015.\n' +
      '* [60] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. MaxVIT: Multi-axis vision transformer. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 459-479, 2022.\n' +
      '* [61] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Kai Han, and Yunhe Wang. Gold-YOLO: Efficient object detector via gather-and-distribute mechanism. _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.\n' +
      '* [62] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Scaled-YOLOv4: Scaling cross stage partial network. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 13029-13038, 2021.\n' +
      '* [63] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7464-7475, 2023.\n' +
      '* [64] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSPNet: A new backbone that can enhance learning capability of CNN. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPR)_, pages 390-391, 2020.\n' +
      '* [65] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh. Designing network design strategies through gradient path analysis. _Journal of Information Science and Engineering (JISE)_, 39(4):975-995, 2023.\n' +
      '* [66] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. You only learn one representation: Unified network for multiple tasks. _Journal of Information Science & Engineering (JISE)_, 39(3):691-709, 2023.\n' +
      '* [67] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. End-to-end object detection with fully convolutional network. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 15849-15858, 2021.\n' +
      '* [68] Liwei Wang, Chen-Yu Lee, Zhuowen Tu, and Svetlana Lazebnik. Training deeper convolutional networks with deep supervision. _arXiv preprint arXiv:1505.02496_, 2015.\n' +
      '* [69] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 568-578, 2021.\n' +
      '* [70] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. PVT v2: Improved baselines with pyramid vision transformer. _Computational Visual Media_, 8(3):415-424, 2022.\n' +
      '* [71] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. ConvNeXt v2: Co-designing and scaling convnets with masked autoencoders. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 16133-16142, 2023.\n' +
      '* [72] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1492-1500, 2017.\n' +
      '* [73] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. SimMIM: 마스킹된 이미지 모델링을 위한 간단한 프레임워크. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9653-9663, 2022.\n' +
      '* [74] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing Dang, Shengyu Wei, Yuning Du, et al. PP-YOLOE: An evolved version of YOLO. _arXiv preprint arXiv:2203.16250_, 2022.\n' +
      '* [75] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang, Yuan Zhang, and Xiuyu Sun. DAMO-YOLO: A report on real-time object detection design. _arXiv preprint arXiv:2211.15444_, 2022.\n' +
      '* [76] Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, Ziteng Cui, Yu Qiao, Hongsheng Li, and Peng Gao. MonoDETR: Depth-guided transformer for monocular 3D object detection. InProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9155-9166, 2023.\n' +
      '* [77] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren. Distance-IoU loss: Faster and better learning for bounding box regression. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, volume 34, pages 12993-13000, 2020.\n' +
      '* [78] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo Yin, Yuchao Dai, and Ruigang Yang. IoU loss for 2D/3D object detection. In _International Conference on 3D Vision (3DV)_, pages 85-94, 2019.\n' +
      '* [79] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong, Songtao Liu, Zeming Li, and Jian Sun. AutoAssign: Differentiable label assignment for dense object detection. _arXiv preprint arXiv:2007.03496_, 2020.\n' +
      '* [80] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. _arXiv preprint arXiv:2401.09417_, 2024.\n' +
      '* [81] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni-perceiver: Pre-training unified architecture for generic perception for zeroshot and few-shot tasks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 16804-16815, 2022.\n' +
      '* [82] Zhuofan Zong, Guanglu Song, and Yu Liu. DETRs with collaborative hybrid assignments training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6748-6758, 2023.\n' +
      '\n' +
      '## 부록 구현 세부사항\n' +
      '\n' +
      'YOLOv9의 훈련 매개변수는 표 1에 나와 있다. 우리는 SGD 최적화기를 사용하여 500개의 에폭스를 훈련시키는 YOLOv7 AF[63]의 설정을 완전히 따른다. 우리는 먼저 3시대에 대한 워밍업을 하고 워밍업 단계에서 편향만 업데이트한다. 다음으로 선형 감쇠 방식으로 초기 학습 속도 0.01에서 0.0001로 물러나고 데이터 증강 설정은 표 1의 하단 부분에 나열되며 마지막 15개의 에폭에서 모자이크 데이터 증강 동작을 종료한다.\n' +
      '\n' +
      'YOLOv9의 네트워크 토폴로지는 YOLOv7 AF[63]를 완전히 따르며, 즉 제안된 CSP-ELAN 블록으로 ELAN을 대체한다. 표 2에 열거된 바와 같이, CSP-ELAN의 깊이 파라미터는 각각 ELAN 깊이 및 CSP 깊이로 표시된다. CSP-ELAN 필터의 파라미터는 ELAN 출력 필터, CSP 출력 필터 및 CSP 내부 필터로 표시된다. 다운 샘플링 모듈 부분에서는 CSP-DOWN 모듈을 DOWN 모듈로 단순화한다. DOWN 모듈은 크기 2와 보폭 1의 풀링 레이어와 크기 3과 보폭 2의 Conv 레이어로 구성되며, 최종적으로 예측 레이어를 최적화하고 회귀 분기에서 상단, 좌측, 하단, 우측을 디커플드 분기로 교체하였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline\n' +
      '**hyper parameter** & **value** \\\\ \\hline epochs & 500 \\\\ optimizer & SGD \\\\ initial learning rate & 0.01 \\\\ finish learning rate & 0.0001 \\\\ learning rate decay & linear \\\\ momentum & 0.937 \\\\ weight decay & 0.0005 \\\\ warm-up epochs & 3 \\\\ warm-up momentum & 0.8 \\\\ warm-up bias learning rate & 0.1 \\\\ box loss gain & 7.5 \\\\ class loss gain & 0.5 \\\\ DFL loss gain & 1.5 \\\\ HSV saturation augmentation & 0.7 \\\\ HSV value augmentation & 0.4 \\\\ translation augmentation & 0.1 \\\\ scale augmentation & 0.9 \\\\ mosaic augmentation & 1.0 \\\\ MixUp augmentation & 0.15 \\\\ copy \\& paste augmentation & 0.3 \\\\ close mosaic epochs & 15 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: YOLOv9의 하이퍼 파라미터 설정.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline\n' +
      '**Index** & **Module** & **Route** & **Filters** & **Depth Size** & **Stride**\\\\\\\n' +
      '0 & Conv & \\(-\\) & 64 & \\(-\\) & 3 & 2\\\\\\\n' +
      '1 & Conv & 0 & 128 & \\(-\\) & 3 & 2\\\\\n' +
      '2 & CSP-ELAN & 1 & 256, 128, 64 & \\(2,1\\) & (-\\) & 1 \\\\\\\n' +
      '3 & DOWN & 2 & 256 & \\(-\\) & 3 & 2\\\\\n' +
      '4 & CSP-ELAN & 3 & 512, 256, 128 & \\(2,1\\) & (-\\) & 1\\\\\\\n' +
      '5 & DOWN & 4 & 512 & \\(-\\) & 3 & 2\\\\\n' +
      '6 & CSP-ELAN & 5 & 512, 512, 256 & \\(2,1\\) & (-\\) & 1\\\\\n' +
      '7 & DOWN & 6 & 512 & \\(-\\) & 3 & 2\\\\\n' +
      '8 & CSP-ELAN & 7 & 512, 512, 256 & \\(2,1\\) & (-\\) & 1\\\\\n' +
      '9 & SPP-ELAN & 8 & 512, 256, 256 & \\(3,1\\) & (-\\) & 1 \\\\\n' +
      '10 & Up & 9 & 512 & \\(-\\) & 2\\\\\n' +
      '11 & Concat & \\(10,6\\ & 1024 & \\(-\\) & \\(-\\) & 1\\\\\\\n' +
      '12 & CSP-ELAN & 11 & 512, 512, 256 & \\(2,1\\) & (-\\) & 1 \\\\\n' +
      '13 & Up & 12 & 512 & \\(-\\) & 2\\\\\\\n' +
      '14 & Concat & \\(13,4\\) & 1024 & \\(-\\) & 1\\\\\\\n' +
      '15 & CSP-ELAN & 14 & 256, 256, 128 & \\(2,1\\) & (-\\) & 1\\\\\\\n' +
      '16 & DOWN & 15 & 256 & \\(-\\) & 3 & 2\\\\\n' +
      '17 & Concat & \\(16,12\\ & 768 & \\(-\\) & 1\\\\\\\n' +
      '18 & CSP-ELAN & 17 & 512, 512, 256 & \\(2,1\\) & (-\\) & 1\\\\\n' +
      '19 & DOWN & 18 & 512 & \\(-\\) & 3 & 2\\\\\n' +
      '20 & Concat & \\(19,9\\) & 1024 & \\(-\\) & 1\\\\\\\n' +
      '21 & CSP-ELAN & 20 & 512, 512, 256 & \\(2,1\\) & (-\\) & 1\\\\\n' +
      '22 & Predict & 15, 18, 21 & \\(-\\) & \\(-\\) & \\(-\\) & \\(-\\) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: YOLOv9의 네트워크 구성.\n' +
      '\n' +
      '## 부록 B 추가 비교\n' +
      '\n' +
      '우리는 YOLOv9를 다양한 방법으로 훈련된 최첨단 실시간 객체 검출기와 비교한다. 그것은 주로 네 가지 다른 훈련 방법을 포함한다: (1) 스크래치에서 트레인: 우리는 텍스트에서 대부분의 비교를 완료했다. 여기에는 비교를 위한 DynamicDet[36]의 추가 데이터 목록만 있다; (2) ImageNet에 의해 사전 훈련된다: 이것은 지도된 사전 훈련과 자가 지도된 사전 훈련을 위해 ImageNet을 사용하는 두 가지 방법; (3) 지식 증류: 훈련이 완료된 후 추가적인 자가 증류를 수행하는 방법; 및 (4) 보다 복잡한 훈련 프로세스: ImageNet에 의해 사전 훈련된 단계, 지식 증류, DAMO-YOLO 및 심지어 추가적인 사전 훈련된 대형 객체 검출 데이터세트를 포함하는 단계들의 조합이다. 우리는 표 3의 결과를 보여준다. 이 표를 통해 제안된 YOLOv9가 다른 모든 방법보다 더 잘 수행되었음을 알 수 있다. 제안한 방법은 ImageNet과 Objects365를 이용하여 학습된 PPYOLOE+-X와 비교하여 파라미터 수를 54%, 연산량을 9% 감소시켰으며, AP를 0.4% 개선하였다.\n' +
      '\n' +
      '표 4는 파라미터 크기별로 정렬된 모든 모델의 성능을 보여준다. 제안된 YOLOv9는 크기가 다른 모든 모델에서 파레토 최적이다. 이 중 20M 이상의 매개변수를 가진 모델에서 파레토 최적에 대한 다른 방법을 찾지 못했다. 위의 실험 데이터는 우리의 YOLOv9가 우수한 매개변수 사용 효율성을 가지고 있음을 보여준다.\n' +
      '\n' +
      '<표 5>는 연산량별로 정렬된 모든 참여 모델의 성능이다. 제안된 YOLOv9는 다른 척도를 가진 모든 모델에서 파레토 최적이다. 60 GFLOP 이상의 모델 중 ELAN 기반 DAMO-YOLO와 DETR 기반 RT DETR만이 제안된 YOLOv9와 비교할 수 있으며, 위의 비교 결과는 YOLOv9가 계산 복잡도와 정확도의 트레이드오프에서 가장 뛰어난 성능을 가지고 있음을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l c c c c c c c c|} \\hline\n' +
      '**Model** & **\\#Param. (M)** & **FLOPs (G)** & \\(\\mathbf{AP}_{0:0.95}^{val}\\) (\\%)** & \\(\\mathbf{AP}_{50}^{val}\\) (\\%) & \\(\\mathbf{AP}_{75}^{val}\\) (\\%) & \\(\\mathbf{AP}_{S}^{val}\\) (\\%) & \\(\\mathbf{AP}_{M}^{val}\\) (\\%) & \\(\\mathbf{AP}_{L}^{val}\\) (\\%) \\\\ \\hline\n' +
      '**YOLOv6-N v3.0 [30] (D)** & 4.7 & 11.4 & **37.5** & 53.1 & &\n' +
      '**RTMDet-T[44] (I)** & 4.8 & 12.6 & **41.1** & 57.9 & – & –\n' +
      '**Gold YOLO-N [61] (D)** & 5.6 & 12.1 & 39.9 & 55.9 & – \\\\O-N [61] (D)** & -\n' +
      '**YOLOv9-S(S)** & 7.2 & 26.7 & **46.8** & 63.4 & 50.7 & 26.6 & 56.0 & 64.5\\\\\n' +
      '**PYOLO-E-S[74] (C)** & 7.9 & 14.4 & 43.7 & 60.6 & 47.9 & 23.2 & 46.4 & 56.9\\\\\n' +
      '**PYOLO-E-S[74] (I)** & 7.9 & 14.4 & 43.0 & 60.5 & 46.6 & 23.2 & 46.4 & 56.9\\\\\n' +
      '**DAMO YOLO-T[75] (D)** & 8.5 & 18.1 & 43.6 & 59.4 & 46.6 & 23.3 & 47.4 & 61.0 \\\\O\n' +
      '**RTMDet-S[44] (I)** & 9.0 & 25.6 & 44.6 & 61.9 & – \\\\S[44] & – –\n' +
      '**DAMO YOLO-O-S[75] (D)** & 16.3 & 37.8 & **47.7** & 63.5 & 51.1 & 26.9 & 51.7 & 64.9\\\\\n' +
      '**YOLO-C-S[30] (D)** & 18.5 & 45.3 & 45.0 & 61.8 & –\n' +
      '**RT DETR-R18 [43] (I)** & 20 & 60 & 46.5 & 63.8 & – \\\\ETR-R18 [43] (I)** & 20 & 60 & 46.5 & 63.8 & – –\n' +
      '**YOLO-v9-M(S)** & 20.1 & 76.8 & **51.4** & 68.1 & 56.1 & 33.6 & 57.0 & 68.0\\\\\n' +
      '**Gold YOLO-S[61] (C)** & 21.5 & 46.0 & 46.4 & 63.4 & – \\\\O-S[61] & –\n' +
      '**Gold YOLO-S[61] (D.)** & 21.5 & 46.0 & 46.1 & 63.3 & –\\\\O-S[61] (D.)** & 21.5 & 46.0 & 46.1 & 63.3 & –\n' +
      '**Gold YOLO-S[61] (D)** & 21.5 & 46.0 & 45.5 & 62.2 & – \\\\O-S[61] & –\n' +
      '**PYOLO-E-M[74] (C)** & 23.4 & 49.9 & 49.8 & 67.1 & 54.5 & 31.8 & 53.9 & 66.2\\\\\n' +
      '**PYYOLOE-M[74] (I)** & 23.4 & 49.9 & 49.0 & 66.5 & 53.0 & 28.6 & 52.9 & 63.8 \\\\\n' +
      '**RTMDet-M[44] (I)** & 24.7 & 78.6 & 49.4 & 66.8 & – \\\\\n' +
      '**YOLO-V5-C(S)** & 25.5 & 102.8 & **53.0** & 70.2 & 57.8 & 36.2 & 58.5 & 69.3\\\\\n' +
      '**DAMO YOLO-M[75] (D)** & 28.2 & 61.8 & 50.4 & 67.2 & 55.1 & 31.6 & 55.3 & 67.1\\\\\n' +
      '**RT DETR-R34 [3] (I)** & 31 & 92 & 48.9 & 66.8 & – & – \\\\\n' +
      '**RT DETR-L[43] (I)** & 32 & 110 & 53.0 & 71.6 & 57.3 & 34.6 & 57.3 & 71.2 \\\\\n' +
      '**YOLOv6-M v3.0 [30] (D)** & 34.9 & 85.8 & 50.0 & 66.9 & – \\\\\n' +
      '**YOLO-V5-E(S)** & 35.0 & 148.1 & **54.5** & 71.7 & 59.2 & 38.1 & 59.9 & 70.3\\\\\n' +
      '**RT DETR-R5M[43] (I)** & 36 & 100 & 51.3 & 69.6 & – \\\\ETR-R5M[43] (I)** & 36 & 100 & 51.3 & 69.6 & – –\n' +
      '**Gold YOLO-M [61] (C)** & 41.3 & 57.5 & 51.1 & 68.5 & – \\\\O-M [61] (C)** & -\n' +
      '**Gold YOLO-M [61] (D)** & 41.3 & 57.5 & 50.9 & 68.2 & –\\\\O-M [61] (D)** & –\n' +
      '**RT DETR-R50[43] (D)** & 42.1 & 36 & 53.1 & 71.3 & 57.7 & 34.8 & 58.0 & 70.0 \\\\\\\n' +
      '**DAMO YOLO-L[75] (D)** & 42.1 & 97.3 & 51.9 & 68.5 & 56.7 & 33.3 & 57.0 & 67.6 \\\\O\n' +
      '**YOLO-P-S(S)** & 44.8 & 187.0 & **55.1** & 72.3 & 60.7 & 38.7 & 60.6 & 71.4\\\\\n' +
      '**PYYOLOE+L[74] (C)** & 52.2 & 110.1 & 52.9 & 70.1 & 57.9 & 35.2 & 57.5 & 69.1\\\\\n' +
      '**PYYOLOE-L[74] (I)** & 52.2 & 110.1 & 51.4 & 68.9 & 55.6 & 31.4 & 55.3 & 66.1\\\\\n' +
      '**RTMDet-L[44] (I)** & 52.3 & 160.4 & 51.5 & 68.8 & – \\\\\n' +
      '**YOLO-R-S**[66] (C)** & 52.9 & 120.4 & 52.8 & 71.2 & 57.6 & –\\\\R-S**[66] (C)** & 52.9 & 120.4 & 52.8 & 71.2 & 57.6 & –\n' +
      '**YOLO-V9-E(S)** & 58.1 & 192.5 & **55.6** & 72.8 & 60.6 & 40.2 & 61.0 & 71.4\\\\\n' +
      '**YOLO-V6-L v3.0 [30] (D)** & 59.6 & 150.7 & 52.8 & 70.3 & – \\\\V6-L v3.0 [30] (D)** & 59.6 & 150.7 & 52.8 & 70.3 & – –\n' +
      '**RT DETR-X[43] (I)** & 67 & 234 & 54.8 & 73.1 & 59.4 & 35.7 & 59.6 & 72.9 \\\\\n' +
      '**Gold YOLO-L[61] (C)** & 75.1 & 151.7 & 53.3 & 70.9 & –\n' +
      '**Gold YOLO-L[61] (D.)** & 75.1 & 151.7 & 53.2 & 70.5 & – \\\\OLO-L[61] (D.)** & 75.1 & 151.7 & 53.2 & 70.5 & – & –\n' +
      '**Gold YOLO-L[61] (I)** & 75.1 & 151.7 & 52.3 & 69.6 & – \\\\OLO-L[61] (I)** & 75.1 & 151.7 & 52.3 & – –\n' +
      '**RT DETR-R101[43] (I)** & 76 & 259 & 54.3 & 72.7 & 58.6 & 36.0 & 58.8 & 72.1 \\\\\n' +
      '**RTMDet-S[44] (I)** & 94.9 & 283.4 & 52.8 & 70.4 & – \\\\\n' +
      '**YOLO-C-S-X[66] (C)** & 96.9 & 226.8 & 54.8 & 73.1 & 59.7 & – \\\\\n' +
      '**PYYOLOE+X[74] (C)** & 98.4 & 206.6 & 54.7 & 72.0 & 59.9 & 37.9 & 59.3 & 70.4\\\\\n' +
      '**PYYOLOE-X [74] (I)** & 98.4 & 206.6 & 52.3 & 69.5 & 56.8 & 35.1 & 57.0 & 68.6 \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '*(S), (I), (D), (C)는 각각 트레인-from-스크래치, ImageNet 사전 훈련, 지식 증류 및 복합 설정을 나타낸다.\n' +
      '\n' +
      '\\end{table}\n' +
      '표 4: 상이한 트레이닝 설정(파라미터 수로 정렬됨)을 갖는 최첨단 물체 검출기의 비교.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l c c c c c c c c|} \\hline\n' +
      '**Model** & **\\#Param. (M)** & **FLOPs (G)** & **AP\\({}^{\\text{o1}}_{50.95}\\) (\\%)** & **AP\\({}^{\\text{o1}}_{50}\\) (\\%)** & **AP\\({}^{\\text{o2}}_{75}\\) (\\%)** & **AP\\({}^{\\text{o1}}_{5}\\) (\\%)** & **AP\\({}^{\\text{o1}}_{M}\\) (\\%)** & **AP\\({}^{\\text{o2}}_{L}\\) (\\%)** \\\\ \\hline\n' +
      '**YOLOv6-N v3.0 [30] (D)** & 4.7 & 11.4 & **37.5** & 53.1 & – & –\n' +
      '**Gold YOLO-N**[61] (D)** & 5.6 & 12.1 & **39.9** & 55.9 & –\n' +
      '**RTMDet-T**[44] (I)** & 4.8 & 12.6 & **41.1** & 57.9 & – & –\n' +
      'PPYOLOE+S**[74] (C)** & 7.9 & 14.4 & **43.7** & 60.6 & 47.9 & 23.2 & 46.4 & 56.9\\\\\n' +
      'PPYOLOE-S**[74] (I)** & 7.9 & 14.4 & 43.0 & 60.5 & 46.6 & 23.2 & 46.4 & 56.9 \\\\\n' +
      '**DMMO YOLO-T**[75] (D)** & 8.5 & 18.1 & 43.6 & 59.4 & 46.6 & 23.3 & 47.4 & 61.0 \\\\O\n' +
      '**RTMDet-S**[44] (I)** & 9.0 & 25.6 & **44.6** & 61.9 & – & – \\\\\n' +
      '**YOLOv9-S**[75] (D)** & 7.2 & 26.7 & **46.8** & 63.4 & 50.7 & 26.6 & 56.0 & 64.5 \\\\\n' +
      '**DAMO YOLO-S**[75] (D)** & 16.3 & 37.8 & **47.7** & 63.5 & 51.1 & 26.9 & 51.7 & 64.9 \\\\\n' +
      '**YOLOE-S**[30] (D)** & 18.5 & 45.3 & 45.0 & 61.8 & –\n' +
      '**Gold YOLO-S**[61] (C)** & 21.5 & 46.0 & 46.4 & 63.4 & – \\\\\n' +
      '**Gold YOLO-S**[61] (D)** & 21.5 & 46.0 & 46.1 & 63.3 & –\\\\\n' +
      '**Gold YOLO-S**[61] (D)** & 21.5 & 46.0 & 45.5 & 62.2 & –\\\\\n' +
      '**PPYOLOE+M**[74] (C)** & 23.4 & 49.9 & **49.8** & 67.1 & 54.5 & 31.8 & 53.9 & 66.2 \\\\\n' +
      'PPYOLOE-M**[74] (I)** & 23.4 & 49.9 & 49.0 & 66.5 & 53.0 & 28.6 & 52.9 & 63.8 \\\\\n' +
      '**Gold YOLO-M**[61] (C)** & 41.3 & 57.5 & **51.1** & 68.5 & –\n' +
      '**Gold YOLO-M**[61] (D)** & 41.3 & 57.5 & 50.9 & 68.2 & –\\\\O-M**[61] (D)** & –\n' +
      '**RT DETR-R18**[43] (I)** & 20 & 60 & 46.5 & 63.8 & – \\\\\n' +
      '**DAMO YOLO-M**[75] (D)** & 28.2 & 61.8 & 50.4 & 67.2 & 55.1 & 31.6 & 55.3 & 67.1\\\\\n' +
      '**YOLOv9-M**(S) & 20.1 & 76.8 & **51.4** & 68.1 & 56.1 & 33.6 & 57.0 & 68.0\\\\ RTMDet-M**[44] (I)** & 24.7 & 78.6 & 49.4 & 66.8 & – & – \\\\ YOLO-M**[30] (D)** & 34.9 & 85.8 & 50.0 & 66.9 & – – & –\n' +
      '**RT DETR-R43**[**4**] (I)** & 31 & 92 & 48.9 & 66.8 & – \\\\\n' +
      '**DAMO YOLO-L**[75] (D)** & 42.1 & 97.3 & **51.9** & 68.5 & 56.7 & 33.3 & 57.0 & 67.6 \\\\O\n' +
      '**RT DETR-R50M**[43] (I)** & 36 & 100 & 51.3 & 69.6 & – \\\\\n' +
      '**YOLOv9-C**(S) & 25.5 & 102.8 & **53.0** & 70.2 & 57.8 & 36.2 & 58.5 & 69.3\\\\\n' +
      '**RT DETR-L**[43] (I)** & 32 & 110 & 53.0 & 71.6 & 57.3 & 34.6 & 57.3 & 71.2 \\\\\n' +
      '**PPYOLOE+L**[74] (C)** & 52.2 & 110.1 & 52.9 & 70.1 & 57.9 & 35.2 & 57.5 & 69.1 \\\\\n' +
      '**PPYOLOE-L**[74] (I)** & 52.2 & 110.1 & 51.4 & 68.9 & 55.6 & 31.4 & 55.3 & 66.1 \\\\\n' +
      '**YOLOR-CSP**[66] (C) & 52.9 & 120.4 & 52.8 & 71.2 & 57.6 & –\\\\\n' +
      '**RT DETR-R50**[43] (I)** & 42 & 136 & **53.1** & 71.3 & 57.7 & 34.8 & 58.0 & 70.0 \\\\\n' +
      '**YOLOv9-E**(S) & 35.0 & 148.1 & **54.5** & 71.7 & 59.2 & 38.1 & 59.9 & 70.3\\\\\n' +
      '**YOLOv1-C** **V3.0 [30] (D)** & 59.6 & 150.7 & 52.8 & 70.3 & – \\\\\n' +
      '**Gold YOLO-L**[61] (C)** & 75.1 & 151.7 & 53.3 & 70.9 & – \\\\OLO-L**[61] (C)** & 75.1 & 151.7 & – –\n' +
      '**Gold YOLO-L**[61] (D**) & 75.1 & 151.7 & 53.2 & 70.5 & – \\\\OLO-L**[61] (D**) & –\n' +
      '**Gold YOLO-L**[61] (I)** & 75.1 & 151.7 & 52.3 & 69.6 & –\n' +
      '**RTMDet-L**[44] (I)** & 52.3 & 160.4 & 51.5 & 68.8 & – \\\\\n' +
      '**Dy-YOLOv7**[36] (S) & – & 181.7 & 53.9 & 72.2 & 58.7 & 35.3 & 57.6 & 66.4 \\\\\n' +
      '**YOLOv9-E**(S) & 44.8 & 187.0 & **55.1** & 72.3 & 60.7 & 38.7 & 60.6 & 71.4\\\\\n' +
      '**YOLOv9-E**(S) & 58.1 & 192.5 & **55.6** & 72.8 & 60.6 & 40.2 & 61.0 & 71.4\\\\\n' +
      '**PYOLOE+X**[74] (C)** & 98.4 & 206.6 & 54.7 & 72.0 & 59.9 & 37.9 & 59.3 & 70.4\\\\\n' +
      '**PPYOLOE-X**[74] (I)** & 98.4 & 206.6 & 52.3 & 69.5 & 56.8 & 35.1 & 57.0 & 68.6 \\\\\n' +
      '**YOLOR-CSP-X**[66] (C)** & 96.9 & 226.8 & 54.8 & 73.1 & 59.7 & –\\\\\n' +
      '**RT DETR-X**[43] (I)** & 67 & 234 & 54.8 & 73.1 & 59.4 & 35.7 & 59.6 & 72.9 \\\\\\\n' +
      '**RT DETR-R10**[43] (I)** & 76 & 259 & 54.3 & 72.7 & 58.6 & 36.0 & 58.8 & 72.1 \\\\\n' +
      'RTMDet-X**[44] (I)** & 94.9 & 283.4 & 52.8 & 70.4 & – \\\\\n' +
      '**Dy-YOLOv7-X**[36] (S)** & – & 307.9 & 55.0 & 73.2 & 60.0 & 36.6 & 58.7 & 68.5 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 상이한 트레이닝 설정(연산량별로 분류됨)을 갖는 최첨단 물체 검출기의 비교.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>