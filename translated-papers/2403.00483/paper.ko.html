<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# _RealCustom_: 실시간 오픈 도메인을 위한 리얼 텍스트 워드 좁히기\n' +
      '\n' +
      'Text-to-Image Customization\n' +
      '\n' +
      '맹치황1, 전동마오1, 명콩류2, 건허2, 용동장1\n' +
      '\n' +
      '중국 1과학기술대학교; 2ByteDance Inc.\n' +
      '\n' +
      '{huangmq}@mail.ustc.edu.cn, {zdmao, zhyd73}@ustc.edu.cn, {liumingcong, heqian}@bytedance.com\n' +
      '\n' +
      '바이트댄스.젠동 마오에서 인턴십을 하는 동안 이루어진 작품들이 해당 작가이다.\n' +
      '\n' +
      '각주 1: 각주:\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '주어진 주제에 대해 텍스트 기반 이미지를 합성하는 것을 목표로 하는 텍스트 대 이미지 커스터마이징은 최근 콘텐츠 생성에 혁명을 일으켰다. 기존의 작품들은 의사 단어 패러다임을 따르는데, 즉 주어진 주제를 의사 단어로 표현한 후 주어진 텍스트로 구성한다. 그러나 의사 단어와 주어진 텍스트의 고유한 얽힌 영향 범위는 이중 최적 역설, 즉 주어진 주제의 유사성과 주어진 텍스트의 제어 가능성을 동시에 최적화할 수 없다. 우리는 **RealCustom**을 처음으로 주제 영향력을 관련 부분으로만 정확하게 제한하여 유사성에서 통제성을 분리하고, **real** 텍스트 단어를 일반적인 내포에서 특정 주제로 점진적으로 좁히고 관련성을 구별하기 위해 교차 주의를 사용하여 달성한다. 구체적으로, RealCustom은 새로운 "train-inference" decoupled framework를 도입한다: (1) 훈련 중에 RealCustom은 시각적 조건과 원래의 텍스트 조건 사이의 일반적인 정렬을 새로운 적응 스코어링 모듈에 의해 학습하여 영향량을 적응적으로 변조한다; (2) 추론 중에 주어진 피험자의 영향 범위와 영향량을 반복적으로 업데이트하여 실제 텍스트 단어의 생성을 점진적으로 좁히는 새로운 적응 마스크 안내 전략을 제안한다. 포괄적인 실험은 오픈 도메인에서 RealCustom의 우수한 실시간 커스터마이징 능력을 입증하여 주어진 주제의 전례 없는 유사성과 주어진 텍스트의 제어 가능성을 처음으로 달성한다. 프로젝트 페이지는 [https://corleone-huang.github.io/realcustom/](https://corleone-huang.github.io/realcustom/)이다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '사전 훈련된 대규모 텍스트-이미지 모델[6, 24, 25, 28](_i.e._, _text-to-image customization_)의 커스터마이징의 최근 상당한 발전은 콘텐츠 생성에 혁명을 일으켜 학계와 산업계 모두에서 빠르게 증가하는 연구 관심을 받았다. 이 작업은 AI 생성 콘텐츠(AIGC) 및 개인 이미지&비디오 생성(7)과 같은 실세계 애플리케이션의 기초가 되는 사용자(예를 들어, 사람의 가장 가까운 친구 또는 좋아하는 그림)에 의해 지정된 주제에 대한 상상력 텍스트 기반 장면을 생성할 수 있는 능력을 미리 훈련된 모델에 부여한다. 커스터마이즈의 주요 목표는 이중 측면이다: (1) 고품질 _similarity_, _i.e._, 생성된 이미지에서 대상 피사체는 _given subjects_를 밀접하게 반영해야 한다; (2) 고품질 _control\n' +
      '\n' +
      '그림 1: 기존 패러다임과 우리 패러다임의 비교. **(a)** 기존 패러다임은 _given subject_를 의사 단어(_e.g._, \\(S^{*}\\))로 표현하는데, 이는 _given text_와 동일한 전체 영향 범위를 얽히게 되어 _dual-optimum paradox_, _i.e._, _given subject_에 대한 유사성과 _given text_에 대한 제어성을 동시에 최적으로 달성할 수 없었다. **(b)** 우리는 _RealCustom_을 제안한다. _RealCustom_은, 처음으로, _given subjects_가 관련 부분에만 영향을 미치도록 정확하게 제한함으로써 유사성이 제어가능성으로부터 분리되고 나머지 부분은 _given text_에 의해 순수하게 제어된다. 이는 _given subjects_의 영향 범위 및 영향량을 반복적으로 업데이트함으로써 달성된다. **(c)** 정량적 비교는 우리의 패러다임이 기존 패러다임의 최첨단보다 우수한 유사성과 통제성을 모두 달성함을 보여준다. 유사성 및 제어성을 평가하기 위해 CLIP-이미지 점수(CLIP-I) 및 CLIP-텍스트 점수(CLIP-T)가 사용된다. 자세한 내용은 실험을 참조하십시오.\n' +
      '\n' +
      '_lability_, _i.e_, 나머지 주제 관련 부분은 _given text_의 제어를 일관되게 준수해야 한다.\n' +
      '\n' +
      '기존의 문헌들은 주어진 주제를 표현하기 위해 _pseudo-word_ paradigm, _i.e.., (1) 학습 의사단어(_e.g., \\(S^{*}\\)[10] 또는 희귀토큰[27])를 따른다; (2) 이러한 의사단어들을 맞춤형 생성을 위해 주어진 텍스트와 합성한다. 최근의 연구들은 보다 포괄적인 의사-워드들(1, 8, 22, 32, 38)을 학습하여 더 많은 주제 정보, _e.g_, 상이한 확산 타임스텝들(1, 38) 또는 계층들(32)에 대한 상이한 의사-워드들을 캡처하는 것에 초점을 맞추고 있다. 한편, 다른 사람들은 객체-데이터 세트들(17) 상에서 인코더(11, 18, 30, 34)를 트레이닝함으로써 의사-단어 학습의 속도를 높일 것을 제안한다. 동시에, 학습된 의사 단어들에 기초하여, 많은 작업들은 사전 훈련된 모델들[16, 18, 27, 34]을 더 미세하게 조정하거나 더 높은 유사성을 위해 추가 어댑터들[30]을 추가한다. 주어진 피험자의 더 많은 정보가 사전 훈련된 모델에 도입됨에 따라 과적합 위험이 증가하여 제어 가능성이 저하된다. 따라서 제어성을 유지하기 위해 다양한 정규화(_e.g_., \\(l_{1}\\) 패널티[10, 16, 34], 사전 보존 손실[27])가 사용되며, 이는 차례로 유사성을 희생시킨다. 본질적으로, 기존의 방법들은 _dual-optimum paradox_, _i.e_에 갇히게 되고, 유사성과 제어성은 동시에 최적일 수 없다.\n' +
      '\n' +
      '우리는 이 _dual-optimum paradox_의 근본적인 원인이 기존의 의사-워드 패러다임에 뿌리를 두고 있다고 주장하며, 여기서 주어진 주제를 생성하기 위한 유사성 성분(_i.e_, 의사-워드)은 주제 관련 없는 부분을 생성하기 위해 제어성 성분(_i.e_, 주어진 텍스트)과 본질적으로 _entangled_이다. 1(a) 구체적으로, 이러한 얽힘은 이 두 구성 요소의 동일한 전체 영향 범위에서 나타난다. _ 즉, 의사 단어와 주어진 텍스트는 모두 모든 생성 영역에 영향을 미친다. 이는 사전 학습된 텍스트-이미지 확산 모델에서 텍스트 상호 주의 집중을 통해 각 영역이 모든 단어 특징의 가중 합으로 업데이트되기 때문이다. 따라서, 유사도 성분의 영향을 증가시키는 것은 주제 관련 부분에서의 유사성을 동시에 강화시키고, 다른 관련 없는 부분에서의 주어진 텍스트의 영향을 약화시켜 제어가능성의 저하를 야기하고, _vice versa_이다. 더욱이, 의사 단어와 피험자 사이의 필요한 대응은 기존의 방법을 제한된 범주를 갖는 객체 데이터세트 [17] 상의 긴 테스트-시간 최적화 [10, 16, 27] 또는 훈련 [18, 34] 중 하나로 한정한다. 결과적으로, 기존의 패러다임은 실세계에서 실시간 오픈 도메인 시나리오에 대한 열악한 일반화 능력을 내재하고 있다.\n' +
      '\n' +
      '본 논문에서는 새로운 커스터마이제이션 패러다임인 _RealCustom_을 제안한다. _RealCustom_의 핵심 아이디어는, 도 2에 도시된 바와 같이, 주어진 텍스트들에 의해 순전히 제어되지 않는 다른 불경스러운 것들을 유지하면서, 주어진 주제들이 관련 부분들에만 영향을 미치도록 정밀하게 제한함으로써, 유사도 구성 요소를 제어가능성 구성 요소로부터 분리한다. _RealCustom_의 핵심 아이디어는, 주제들을 의사 단어들로 표현하는 대신에, 사전 훈련된 모델들의 교차 주의에서 상위 텍스트-이미지 정렬을 그들의 초기 일반 내포(_e.g_, 다양한 종류의 오 장난감들)에서 특정 주제들(_e.g_, 독특한 슬로스 장난감들)로 점진적으로 좁힐 수 있다는 것이다. 여기서, 사전 훈련된 모델들의 교차 주의에서 우수한 텍스트-이미지 정렬은 도 2에 도시된 바와 같이, 주제 관련성을 구별하기 위해 활용될 수 있다. 1(b) 구체적으로, 각 생성 단계에서, (1) 주어진 피험자의 영향 범위는 더 큰 관련성을 나타내는 더 높은 주의 점수와 함께 목표 실제 단어의 교차 주의에 의해 식별된다; (2) 이 영향 범위는 그 다음 현재 단계에서 주어진 피험자의 영향량, _i.e_, 이 범위에 주입될 피험자 정보의 양을 결정한다; (3) 이 영향량은, 차례로, 각 단계의 생성 결과가 이전의 출력에 기초함에 따라, 다음 단계에 대한 더 정확한 영향 범위를 형성한다. 이러한 반복적인 업데이트를 통해, 실제 단어의 생성 결과는 주어진 주제로 매끄럽고 정확하게 변환되는 반면, 다른 관련 없는 부분은 주어진 텍스트에 의해 완전히 제어된다.\n' +
      '\n' +
      '기술적으로, _RealCustom_은 혁신적인 "train-inference" decoupled framework: (1) 트레이닝 동안, _RealCustom_은 텍스트 및 현재 생성된 특징들에 기초하여 영향량을 변조하는 새로운 _adaptive scoring module_를 통해 대규모 텍스트-이미지 데이터세트 상에서 시각적 조건들과 미리 트레이닝된 모델들의 원래 텍스트 조건들 사이의 일반화된 정렬 능력들만을 학습한다. (2) 추론 동안, 실시간 맞춤화는 학습된 정렬 능력에 기초하여 실제 텍스트 단어를 점진적으로 좁히는 새로운 _적응 마스크 안내 전략_에 의해 달성된다. (주)특집\n' +
      '\n' +
      '그림 2: 제안된 새로운 패러다임 _RealCustom_의 사용자 정의 결과 생성. 개방 도메인에서 주어진 주제를 나타내는 _single_ 이미지가 주어지면(_any subjects_, portrait painting, favorite toys, _etc_.), _RealCustom_는 실시간으로 주어진 주제에 대해 주어진 텍스트를 일관되게 고수하는 사실적인 이미지를 생성할 수 있다(_with any test-time optimization steps_).\n' +
      '\n' +
      '이 두 가지 점수의 융합을 위해 먼저, (1)_adaptive scoring module_은 텍스트와 현재 생성된 특징과의 상관성 점수를 추정하며, 이 두 점수의 융합을 위해 타임스탬프 인식 스케쥴을 적용한다. (2)_adaptive mask guidance strategy_는 텍스트와 이미지 사이의 상호주의(cross-attention)를 다른 시각적 상호주의(visual cross-attention)로 확장함으로써 미리 학습된 확산 모델에 통합된다. (2)_adaptive mask guidance strategy_는 _text-to-image (T2I)_ branch (시각적 조건이 \\(\\mathbf{0}\\)로 설정된)와 _text\\(\\&\\)image-to-image (T2I)_ branch (시각적 조건이 주어진 주제로 설정된)로 구성된다. 먼저, T2I branch 내의 모든 레이어들의 대상 실제 단어에 대한 상호주의 맵은 하나의 레이어로 집합되며, 영향 범위로서 높은 주의 영역만을 선택한다. 둘째, T2I 분기에서는 영향 범위에 현재 생성된 특징을 곱하여 영향량을 생성하고 동시에 시각적 교차 주의의 출력을 곱하여 주제 관련 부분에 영향을 미치지 않도록 한다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '개념. 처음으로, 우리는 (1) _dual-optimum paradox_가 유사(_i.e_, 주어진 피험자를 나타내는 의사-단어)와 제어 가능성(_i.e_, 주어진 텍스트) 사이의 기존의 의사-단어 패러다임의 얽힌 영향 범위에 뿌리를 두고 있음을 지적한다; (2) _real_ 단어를 주어진 피험자로 점진적으로 좁힘으로써 엉킴을 달성하는 새로운 패러다임인 _RealCustom_를 제시하며, 여기서 주어진 피험자의 영향 범위는 실제 단어의 교차 주의에 기초하여 제한된다.\n' +
      '\n' +
      '기술 제안된 _RealCustom_은 새로운 "train-inference" decoupled framework를 도입한다: (1) 훈련 중에, 영향량을 변조하기 위해 _adaptive scoring module_에 의해 시각적 조건들과 원래의 텍스트 조건들 사이의 일반화된 정렬을 학습하는 것; (2) 추론 중에, 주어진 피험자의 영향 범위와 양을 반복적으로 업데이트함으로써 실제 단어를 좁히는 _adaptive mask guidance strategy_를 제안한다.\n' +
      '\n' +
      '의미 처음으로 그림 1과 같이 우수한 유사성 및 제어성 _simultaneously_를 달성했다. 1(c); (2) 실시간 오픈 도메인 커스터마이징 능력.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '### Text-to-Image Customization\n' +
      '\n' +
      '기존의 커스터마이징 방법은 주어진 주제를 _pseudo-words_로 표현한 후 커스터마이징을 위해 주어진 텍스트와 함께 구성하는 _pseudo-words_ paradigm, _i.e_를 따른다. 의사 단어와 주어진 주제 사이의 필요한 대응 관계 때문에 기존 작업은 제한된 범주를 가진 객체 데이터 세트에 대해 훈련된 번거로운 테스트 시간 최적화 기반 [1, 8, 9, 10, 16, 22, 27, 32] 또는 인코더 기반 [7, 11, 14, 18, 30, 34]에 국한된다. 예를 들어, 최적화 기반 스트림에서, DreamBooth[27]는 의사 단어로서 희토큰을 사용하고 더 나은 유사성을 위해 사전 훈련된 확산 모델 전체를 추가로 미세 조정한다. 사용자 정의 확산[16]은 대신 주요 매개 변수의 부분 집합을 찾아 최적화만 합니다. 이 스트림의 주요 단점은 각 새로운 주제에 대해 긴 최적화 시간을 필요로 한다는 것이다. 인코더 기반 스트림에 대해, 최근의 ELITE[34]는 유사성을 향상시키기 위해 로컬 매핑 네트워크를 사용하는 반면, BLIP-확산[18]은 더 나은 피사체 표현을 위해 멀티모달 인코더를 도입한다. 이러한 인코더 기반 작업은 보통 최적화 기반 작업보다 덜 유사성을 보이고 훈련에서 보이지 않는 범주에 대해 제대로 일반화되지 않는다. _ 요약하자면, 의사 단어와 주어진 텍스트의 얽힌 영향 범위는 현재 작업이 최적의 유사성과 제어성을 모두 달성하는 것을 자연스럽게 제한하며 실시간 오픈 도메인 커스터마이징을 방해한다.\n' +
      '\n' +
      '확산모델에서### 교차주의\n' +
      '\n' +
      '현대 대규모 텍스트 대 이미지 확산 모델 [2, 24, 25, 28, 6]에서의 텍스트 안내는 일반적으로 교차 주의 메커니즘을 사용하여 수행된다. 따라서, 많은 연구들은 생성된 이미지 또는 실제 이미지에 대한 텍스트 기반 편집[3, 12]을 위해 교차 주의 맵을 반전[31]을 통해 조작하고, _e.g_., Prompt-to-Prompt[12]는 생성된 이미지를 편집하기 위해 교차 주의 가중치를 재할당하는 것을 제안한다. 작업의 또 다른 분기는 의미 정렬을 개선하기 위해 추가적인 공간 제어[20, 21] 또는 후처리를 추가하여 교차 주의력을 개선하는 데 중점을 둔다[5, 19]. 한편, 다수의 작품[35, 36, 33]은 분할과 같은 판별 작업을 위해 확산 모델에 교차 주의를 사용하는 것을 제안한다. 그러나, 기존 문헌과 달리, _RealCustom_의 핵심 아이디어는 실제 텍스트 단어를 초기 일반적인 함축(_e.g_., 그의 교차-어텐션은 다양한 형태 및 세부사항을 갖는 임의의 장난감을 나타낼 수 있음)으로부터 고유한 주어진 주제(_e.g_., 그의 교차-어텐션은 고유한 장난감을 정확하게 나타낼 수 있음)로 점진적으로 좁히는 것으로, 완전히 미개척된다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '본 연구에서는 주어진 주제를 표현하는 _single_ 이미지만을 가지고 주어진 텍스트로부터 해당 주제에 대한 새로운 고품질 이미지를 생성하는 가장 일반적인 커스터마이징 시나리오에 초점을 맞춘다. 생성된 주체는 위치, 포즈, 스타일, _etc_가 다양할 수 있지만, 주어진 주체와 높은 _similarity_를 유지해야 한다. 나머지 부분은 주어진 텍스트에 일관되게 준수되어야 하며, 따라서 _제어성_를 보장한다.\n' +
      '\n' +
      '제안된 _RealCustom_은 도 3에 예시된 바와 같은 새로운 "열차-추론" 디커플드 패러다임을 도입한다. 구체적으로, 트레이닝 동안 _RealCustom_은 미리 트레이닝된 모델들의 시각적 조건들과 원래의 텍스트 조건들 사이의 일반적인 정렬을 학습한다. 추론 동안, 학습된 정렬 능력에 기초하여, _RealCustom_는 주어진 주제의 각 단계의 영향 범위 및 영향량을 반복적으로 업데이트함으로써 실제 텍스트 워드들(_e.g_, "toy")의 생성을 주어진 주제(_e.g_, 고유한 갈색 슬롯 장난감)로 점진적으로 좁힌다.\n' +
      '\n' +
      '먼저 Sec. 3.1의 예제를 간략히 소개하고, _RealCustom_의 훈련 및 추론 패러다임을 Sec. 3.2와 Sec. 3.3에서 구체적으로 설명한다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '우리의 패러다임은 두 가지 구성요소인 _i.e_, 오토인코더 및 조건부 UNet [26] 데노이저로 구성된 안정확산 [25]를 통해 구현된다. 먼저, Autoencoder의 encoder \\(\\mathbf{x}\\in\\mathbb{R}^{H\\times W\\times 3}\\)이 주어지면, \\(\\mathbf{z}=\\mathcal{E}(\\cdot)\\)=\\mathcal{E}(\\mathbf{x})\\in\\mathbb{R}^{h\\times w\\times c}\\)이 다운샘플링 인자이고, \\(f=\\frac{H_{0}}{h}=\\frac{W_{0}}{w}\\)이 잠재 채널 차원을 나타낸다. 해당 디코더 \\(\\mathcal{D}(\\cdot)\\)는 잠재 벡터를 다시 \\(\\mathcal{D}(\\mathcal{E}(\\mathbf{x}))\\approx\\mathbf{x}\\으로 영상에 매핑한다. 둘째, 조건부 디노이저\\(\\epsilon_{\\theta}(\\cdot)\\)를 학습하여 텍스트 조건\\(y\\)을 기반으로 잠재 벡터를 생성한다. 사전 학습된 CLIP 텍스트 인코더[23]\\(\\tau_{\\text{text}}(\\cdot)\\)는 텍스트 조건\\(y\\)을 텍스트 특징\\(\\mathbf{f_{ct}}=\\tau_{\\text{text}(y)\\)으로 인코딩하기 위해 사용된다. 그런 다음, 데노이저는 평균 제곱 손실로 훈련된다:\n' +
      '\n' +
      '\\mathbbb{E}_{\\mathbf{z}\\sim\\mathcal{E}(\\mathbf{x}),\\mathbf{f_{y},\\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{l}),t}\\left[\\|\\mathbf{\\epsilon}-\\epsilon_{\\theta}\\left(\\bm{z_{t},t,\\mathbf{f_{ct}\\right}\\|_{2}^{2}\\right], \\tag{1}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{l}),t}\\left[\\|\\mathbf{\\epsilon}-\\epsilon\\heta}\\left(\\bm{z_{t},t,\\mathbf{f_{ct}\\right],\\tag{1}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{\n' +
      '\n' +
      '여기서 \\(\\mathbf{\\epsilon}\\)는 스케일링되지 않은 잡음에 대해 나타내고 \\(t\\)는 타임스테프이다. \\(t\\) (\\mathbf{z_{t}}\\)는 \\(t\\)에 따라 잡음화된 잠재 벡터이다:\n' +
      '\n' +
      '\\[\\mathbf{z_{t}=\\sqrt{\\hat{\\alpha}_{t}\\mathbf{z_{0}}+\\sqrt{1-\\hat{\\alpha}_{t}\\mathbf{ \\epsilon}, \\tag{2}\\\n' +
      '\n' +
      '여기서 \\(\\hat{\\alpha}_{t}\\in[0,1]\\)은 추가된 잡음의 양을 조절하는 하이퍼-파라미터이다. 큰 \\(t\\)은 작은 \\(\\hat{\\alpha}_{t}\\)을 의미하고,\n' +
      '\n' +
      '도 3: 새로운 "train-inference" decoupled framework: (a) 트레이닝 동안, 시각적 및 원래의 텍스트 조건들 사이의 일반적인 정렬은 텍스트 및 현재 생성된 특징들에 기초하여 시각적 조건들을 정확하게 도출하는 제안된 _adaptive scoring module_에 의해 학습된다. (b) 추론 시, 시각적 조건이 \\(\\mathbf{0}\\)으로 설정된 텍스트-이미지(T2I) 브랜치, 시각적 조건이 주어진 주제로 설정된 텍스트-이미지(TI2I) 브랜치의 두 가지 분기로 구성된 제안된 적응 마스크 안내 전략_에 의해, 초기 일반적인 함축에서 주어진 주제(_e.g_, "toy")로 실제 단어(_e.g_, "toy")를 점진적으로 좁힌다. T2I 브랜치는 타겟 리얼 워드의 (_e.g_., "toy") 교차 어텐션을 집계하여 영향 범위를 계산하는 것을 목표로 하는 반면, TI2I 브랜치는 이 범위에 영향 수량을 주입하는 것을 목표로 한다.\n' +
      '\n' +
      '따라서 보다 잡음화된 잠재 벡터 \\(\\mathbf{z_{t}}\\)을 얻을 수 있다. 추론하는 동안, 랜덤 가우시안 잡음\\(\\mathbf{z_{T}}\\)을 \\(\\mathbf{z_{0}}\\)으로 반복적으로 잡음제거하고, \\(\\mathbf{x^{\\prime}}=\\mathcal{D}(\\mathbf{z_{0}})\\)을 통해 최종 생성된 영상을 얻는다.\n' +
      '\n' +
      '안정적 확산에서 텍스트 조건의 통합은 텍스트 상호 주의로서 구현된다:\n' +
      '\n' +
      '\\[\\text{Attention}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\text{Softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}{\\sqrt{d}})\\mathbf{V},\\tag{3}\\text{Softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^{top}{\\sqrt{d}})\n' +
      '\n' +
      '여기서 질의 \\(\\mathbf{Q}=\\mathbf{W_{Q}}\\cdot\\mathbf{f_{i}}\\), 키 \\(\\mathbf{K}=\\mathbf{W_{K}}\\cdot\\mathbf{f_{ct}}\\) 및 값 \\(\\mathbf{V}=\\mathbf{W_{V}}\\cdot\\mathbf{f_{ct}}\\). (\\mathbf{W_{Q}},\\mathbf{W_{K}},\\mathbf{W_{V}}\\)는 질의, 키 및 값 투영 레이어의 가중치 파라미터이다. \\mathbf{W_{Q}},\\mathbf{W_{K},\\mathbf{W_{V}}\\ (\\mathbf{f_{i}},\\mathbf{f_{ct}}\\)는 잠재 이미지 특징 및 텍스트 특징이고, \\(d\\)는 키 및 쿼리 특징의 채널 차원이다. 그런 다음 잠재 이미지 특징은 주의 블록 출력으로 업데이트된다.\n' +
      '\n' +
      '### Training Paradigm\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 3(a), 텍스트(y\\)와 이미지(x\\)는 먼저 미리 학습된 CLIP 텍스트/이미지 인코더[23]에 의해 텍스트 특징 \\(\\mathbf{f_{ct}}\\in\\mathbb{R}^{n_{t}\\times c_{t}\\)과 이미지 특징 \\(\\mathbf{f_{ci}\\in\\mathbb{R}^{n_{i}\\times c_{i}\\)으로 인코딩된다. 여기서 \\(n_{t},c_{t},n_{i},c_{i}\\)는 각각 텍스트 특징수/차원과 이미지 특징수/차원이다. 그 후, _adaptive scoring module은 텍스트 특징 \\(\\mathbff{f_{ct}\\), 현재 생성된 특징 \\(\\mathbff{z_{t}\\in\\mathbbb{R}^{h\\times w\\times c}\\), timestep \\(t\\)을 입력으로 하여 \\(\\mathbff{f_{ci}\\)에서 각 특징들에 대한 점수를 추정하며, 여기서 \\(\\hat{n}_{i}<n_{i}\\in\\mathbbb{R}^{h\\times c}\\)는 선택된 이미지 특징 번호이다. 다음으로 시각적 조건(\\mathbf{\\hat{f_{yi}}}\\)을 통합하기 위해 텍스트적 교차 주의를 다른 시각적 교차 주의와 확장한다. 구체적으로는, Eq. 도 3은 다음과 같이 재기입된다:\n' +
      '\n' +
      '\\text{Attention}(\\mathbf{Q},\\mathbf{K},\\mathbf{K_{i},\\mathbf{V_{i}},\\mathbf{V_{i}})=\\\\text{Softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}{\\sqrt{d}})\\mathbf{V}+\\text{Softmax}(\\frac{\\mathbf{Q}\\mathbf{K_{i}}^{\\top}{\\sqrt{d}})\\mathbf{V_{i},\\tag{4}\\mathbf{Q}\\mathbf{K}^{\\top}{\\sqrt{d}})\\mathbf{V_{i},\\tag{4}\\mathbf{Q}\\mathbf{Q}\\mathbf{K}^{\\top}{\\sqrt{d}}}\\text{Softmax}(\\frac{\\mathbf{\n' +
      '\n' +
      '여기서 새로운 키 \\(\\mathbf{K_{i}}=\\mathbf{W_{Ki}}\\cdot\\mathbf{\\hat{f_{ci}}}), 값 \\(\\mathbf{V_{i}}=\\mathbf{W_{Vi}}\\cdot\\mathbf{\\hat{f_{ci}}})가 추가된다. \\\\ (\\mathbf{W_{Ki}}\\) 및 \\(\\mathbf{W_{Vi}}\\)는 가중치 파라미터이다. 학습 중 각 주의 블록에서 적응 채점 모듈 및 투영 레이어 \\(\\mathbf{W_{Ki}}\\), \\(\\mathbf{W_{Vi}}\\)만이 학습 가능한 반면, 다른 사전 학습된 모델의 체중은 동결된 상태로 유지된다.\n' +
      '\n' +
      '**적응형 스코어링 모듈** 한편으로 확산 모델 자체의 생성은 본질적으로 잡음이 제거되고 세부 사항이 단계적으로 추가된 거친-미세 공정이다. 이 과정에서 다른 단계는 초기 글로벌 구조에서 후자의 로컬 텍스처까지 걸쳐 있는 다양한 정도의 주제 세부 사항[2]에 중점을 둔다. 이에 따라, 각 이미지 특징의 중요도 또한 동적으로 변화한다. 실제 텍스트 단어를 부드럽게 좁히기 위해 피사체의 이미지 조건도 동시에 적응하여 거친 입에서 미세한 입으로 안내해야 한다. 이를 위해서는 서로 다른 이미지 특징의 중요도 점수를 추정할 수 있는 능력을 _RealCustom_에 장착해야 한다. 반면에, 모든 이미지 특징을 시각적 조건으로 활용하는 것은 "열차-추론" 갭을 초래한다. 이는 주어진 피험자 및 추론 생성 결과가 피험자 부분에서만 유사성을 유지해야 하는 학습 단계와 달리 시각적 조건과 데노이저 \\(\\epsilon_{\\theta}\\)에 대한 입력과 동일한 이미지가 필요하기 때문이다. 따라서, 이러한 갭은 추론에서 유사성과 제어성을 모두 저하시킬 수 있다.\n' +
      '\n' +
      '위의 근거는 맞춤화를 위한 부드럽고 정확한 시각적 조건을 제공하는 _적응 스코어링 모듈_에 동기를 부여한다. 도 1에 도시된 바와 같다. 도 4를 참조하면, 가중치 풀링을 통해 텍스트\\(\\mathbff{f_{ct}\\in\\mathbbb{R}^{n_{t}\\times c_{t}\\) 및 현재 생성된 특징\\(\\mathbfff{z_{t}\\in\\mathbbb{R}^{h\\times w\\times c}=\\mathbbb{R}^{n_{x}\\times c}\\)은 먼저 텍스트 컨텍스트\\(\\mathbf{C}_{\\text{visual}\\) 및 시각적 컨텍스트\\(\\mathbf{C}_{\\text{visual}\\)으로 집합된다.\n' +
      '\n' +
      '\\text{textualual}=\\text{Softmax}(\\mathbf{f_{ct}}\\mathbf{W_{a}^{t}})\\in\\mathbb{R}^{n_{t}\\times 1}\\tag{5}\\text{\n' +
      '\n' +
      '\\text{visual}=\\text{Softmax}(\\mathbf{z_{t}}\\mathbf{W_{a}^{v}})\\in\\mathbb{R}^{n_{x}\\times 1}\\tag{6}\\text{\n' +
      '\n' +
      '\\mathbf{C}_{\\text{textual}}=\\mathbf{A}_{\\text{textual}}^{\\top}\\mathbf{f_{y}}\\in\\mathbbb{R}^{1\\times c_{t},\\mathbf{C}_{\\text{visual}}=\\mathbf{A}_{\\text{visual}}^{\\top}\\mathbf{z_{t}\\in\\mathbbb{R}^{1\\times c},\\tag{7}\\times\n' +
      '\n' +
      '여기서 \\(\\mathbf{W_{a}^{t}}\\in\\mathbb{R}^{c_{t}\\times 1},\\mathbff{W_{a}^{v}}\\in\\mathbb{R}^{c\\times 1}\\in\\mathbb{R}^{c\\times 1})은 가중치 파라미터이고, "Softmax"는 숫자 차원으로 동작된다. 그런 다음 이러한 컨텍스트를 공간적으로 복제하고 이미지 특징\\(\\mathbf{f_{ci}\\in\\mathbbb{R}^{n_{i}\\times c_{i}\\)과 연결하여 텍스트 점수\\(\\mathbff{S}_{textual}\\in\\mathbbb{R}^{n_{i}\\times 1}\\)과 시각적 점수\\(\\mathbff{S}_{visual}\\in\\mathbbb{R}^{n_{i}\\times 1}\\)을 각각 추정한다. 이 두 점수는 2-레이어 MLP로 구현된 2개의 경량 점수-넷에 의해 예측된다.\n' +
      '\n' +
      '텍스트 특징들이 대략 정확하고 생성된 특징들이 점진적으로 정제된다는 점을 고려하여, 이 두 스코어들을 융합하기 위해 타임스탬프-인식 스케줄이 제안된다:\n' +
      '\n' +
      '\\[\\mathbf{S}=(1-\\sqrt{\\hat{\\alpha}_{t})\\mathbf{S}_{\\text{textual}+\\sqrt{\\hat{\\alpha}_{t}\\mathbf{S}_{\\text{visual}, \\tag{8}\\\n' +
      '\n' +
      '여기서 \\(\\sqrt{\\hat{\\alpha}_{t}\\)는 생성된 피쳐에 추가된 노이즈의 양을 조절하는 사전 훈련된 확산 모델의 하이퍼파라미터이다. 그리고 융합된 점수에 소프트맥스 활성화를 적용한 결과, 각 영상특징의 상대적 중요도를 \\(\\mathbf{S}=\\text{Softmax}(\\mathbf{S})\\)로 강조할 수 있었다. 융합된 점수들은 점수-네트의 학습을 가능하게 하기 위해 이미지 특징들과 곱해진다:\n' +
      '\n' +
      '\\[\\mathbf{f_{ci}}=\\mathbf{f_{ci}}\\circ(1+\\mathbf{S}), \\tag{9}\\]\n' +
      '\n' +
      '여기서 \\(\\circ\\)은 원소별 곱을 나타낸다. 마지막으로 Top-K 비율(\\gamma_{\\text{num}\\in[0,1]\\)이 주어지면, 가장 높은 점수를 갖는 주요 특징들의 하위 집합이 출력으로 선택된다.\\(\\mathbf{\\hat{f_{yi}}\\in\\mathbb{R}^{\\hat{n}_{i}\\times c_{i}\\), 여기서 \\(\\hat{n}_{i}=\\gamma_{\\text{num}n_{i}\\). 성능 저하 없이 서로 다른 \\(\\gamma_{\\text{num}}\\)으로 유연한 추론을 가능하게 하기 위해, 우리는 훈련 동안 균일하게 랜덤 비율을 사용하는 것을 제안한다:\n' +
      '\n' +
      '\\[\\gamma_{\\text{num}}=\\text{uniform}[\\gamma_{\\text{num}}^{\\text{low},\\gamma_{\\text{num}}^{\\text{high}], \\tag{10}\\\n' +
      '\n' +
      '여기서 \\(\\gamma_{\\text{num}}^{\\text{low},\\gamma_{\\text{num}}^{\\text{high}\\)는 각각 \\(0.3,1.0\\)으로 설정된다.\n' +
      '\n' +
      '### Inference Paradigm\n' +
      '\n' +
      '_RealCustom_의 추론 패러다임은 그림과 같이 시각적 입력이 \\(\\mathbf{0}\\)으로 설정된 텍스트-이미지(T2I) 브랜치와 시각적 입력이 주어진 피사체로 설정된 텍스트(\\&\\) 이미지-이미지(TI2I) 브랜치의 두 가지로 구성된다. 3(b). 이 두 가지 브랜치는 제안된 적응 마스크 안내 전략_에 의해 연결된다. 구체적으로, 이전 단계의 출력 \\(\\mathbf{z_{t}}\\)이 주어졌을 때, T2I 분기에서는 순수 텍스트 조건부 잡음 제거 과정을 수행하여 출력 \\(\\mathbf{z_{t-1}^{T}}\\)을 얻는데, 여기서 타겟 리얼 워드의 모든 레이어 교차 어텐션 맵(_e.g._, "toy")을 추출하고 동일한 해상도(Stable Diffusion에서 가장 큰 맵 크기, _i.e._, \\(64\\times 64\\)로 리사이징한다. 집계된 주의맵은 \\(\\mathbf{M}\\in\\mathbb{R}^{64\\times 64}\\)으로 표시된다. 다음으로, Top-K 선정을 적용하는데, 목표 비율\\(\\gamma_{\\text{scope}}\\in[0,1]\\)이 주어지면 교차 주의 점수가 가장 높은 \\(\\gamma_{\\text{scope}}\\times 64\\times 64\\) 영역만 남아 있고 나머지는 \\(0\\)으로 설정된다. 선택된 교차 주의 맵\\(\\mathbf{\\bar{M}}\\)은 최대값으로 정규화된다:\n' +
      '\n' +
      '\\[\\mathbf{\\hat{M}}=\\frac{\\mathbf{\\bar{M}}}{\\text{max}(\\mathbf{\\bar{M}})}, \\tag{11}\\]\n' +
      '\n' +
      '여기서 \\(\\text{max}(\\cdot)\\)는 최대값을 나타낸다. 이러한 선별된 부분에서도 서로 다른 지역의 주체적 관련성 역시 다르다는 것이 이면의 근거이다.\n' +
      '\n' +
      'TI2I 분기에서는 영향 범위\\(\\mathbf{\\hat{M}\\)에 현재 생성된 특징\\(\\mathbf{z_{t}\\)을 먼저 곱하여 현재 생성 단계의 정확한 시각적 조건을 제공한다. 그 이유는 영향량 산정을 위해서는 대상 관련 부분만을 고려하여야 하기 때문이다. 둘째, 다른 주제 관련 부분에서 주어진 텍스트의 제어 가능성에 대한 부정적인 영향을 방지하기 위해 \\(\\mathbf{\\hat{M}\\)에 시각적 교차 주의 결과를 곱한다. 구체적으로는, Eq. 도 4는 다음과 같이 재기입된다:\n' +
      '\n' +
      '\\text{Attention}(\\mathbf{Q},\\mathbf{K},\\mathbf{K_{i},\\mathbf{V_{i}}),\\mathbf{V_{i})=\\\\text{Softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}{\\sqrt{d}})\\mathbf{V}+(\\text{Softmax}(\\frac{\\mathbf{Q}\\mathbf{K_{i}}^{\\top}{\\sqrt{d})\\mathbf{V_{i}},\\tag{12}\\mathbf{Q}\\mathbf{Q}\\mathbf{K}^{\\top}{\\sqrt{d}})\\mathbf{V_{i}},\\tag{12}\\mathbf{Q}\\mathbf{Q}\\mathbf{K}^{\\top}{\\sqrt{d}}}\\mathbf{V}\n' +
      '\n' +
      '각 크로스 어텐션 블록의 해상도와 \\(\\mathbf{\\hat{M}}\\)의 크기를 맞추기 위해 필요한 리사이즈 연산을 적용한다. TI2I 분기의 잡음 제거 출력은 \\(\\mathbf{z_{t-1}^{T}}\\)로 표시된다. 클래시퍼-프리 안내 [13]은 다음 단계의 잡음 제거된 잠재 특징 \\(\\mathbf{z_{t-1}}\\)을 생성하기 위해 확장된다:\n' +
      '\n' +
      '\\epsilon_{\\theta}(\\emptyset)+\\omega_{t}(\\mathbf{z_{t-1}^{T}}-\\epsilon_{\\theta}(\\emptyset))+\\omega_{i}(\\mathbf{z_{t-1}^{T}}-\\mathbf{z_{t-1}^{T}}), \\tag{13}\\text{t}}\n' +
      '\n' +
      '여기서 \\(\\epsilon_{\\theta}(\\emptyset)\\)는 무조건적으로 잡음 제거된 출력이다.\n' +
      '\n' +
      '현재 단계에 투입되는 주어진 주제의 원활하고 정확한 영향량으로 실제 단어의 생성은 초기 일반적인 함축에서 특정 주제로 점차 좁아질 것이며, 이는 다음 단계의 생성을 위한 보다 정확한 영향 범위를 형성할 것이다. 이러한 반복적 갱신과 생성을 통해 주어진 주제에 대한 유사성이 주어진 텍스트에 대한 제어 가능성과 단절되어 둘 다의 최적으로 이어지는 실시간 커스터마이징을 달성한다. 더 중요한 것은, _adaptive scoring module_ 뿐만 아니라 시각적 교차-어텐션 계층들 모두가 일반적인 텍스트-이미지 데이터세트들에 대해 트레이닝되기 때문에, 추론은 임의의 타겟 실수 단어들을 사용함으로써 임의의 카테고리들에 일반적으로 적용될 수 있어서, 우수한 오픈-도메인 커스터마이징 능력을 가능하게 한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setups\n' +
      '\n' +
      '**구현.**_RealCustom_는 Stable Diffusion 상에서 구현되고, 1e-5 학습률을 갖는 16w 반복에 대해 16개의 A100 GPU를 사용하여, 미적 점수에 기초하여 Laion-5B[29]의 필터링된 서브세트에 대해 트레이닝된다. 특별히 명시되지 않는 한, 50개의 샘플 단계가 있는 DDIM 샘플러 [31]을 샘플링에 사용하고 분류기가 없는 안내 \\(\\omega_{t},\\omega_{i}\\)는 7.5 및 12.5이다. Top-K 비율 \\(\\gamma_{\\text{num}=0.8\\), \\(\\gamma_{\\text{scope}=0.25\\).\n' +
      '\n' +
      '**평가.**_유사도._ 최첨단 분할 모델(_i.e._, SAM[15])을 이용하여 피사체를 분할한 후, 생성된 영상과 실제 영상에서 분할된 피사체의 평균 쌍별 코사인 유사도 CLIP ViT-B/32 또는 DINO 임베딩인 CLIP-I와 DINO[4] 점수로 평가한다. 제어성._ 우리는 프롬프트와 이미지 CLIP ViT-B/32 임베딩(CLIP-T) 사이의 코사인 유사성을 계산한다. 또한, 이미지넷[37]을 이용하여 제어성 및 심미성(품질)을 평가한다.\n' +
      '\n' +
      '**Prior SOTAs.** 최적화 기반(_i.e._, Textual Inversion[10], DreamBooth[27], CustomDiffusion[16])과 인코더 기반(ELITE[34], BLIP-Diffusion[18])의 기존 패러다임과 비교한다.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '**정량적 결과.** 탭에 표시된 대로 1, _RealCustom_는 모든 메트릭에서 기존 방법보다 우수한 성능을 보인다. (1) 제어성을 위해 CLIP-T와 ImageNet을 각각 8.1%와 223.5% 개선한다. 이미지넷의 상당한 개선은 우리의 패러다임이 훨씬 더 높은 품질의 사용자 정의를 생성한다는 것을 보여준다; (2) 유사성의 경우 CLIP-I와 DINO-I 모두에서 최첨단 성능을 달성한다.\n' +
      '\n' +
      '도 4: _adaptive scoring module_의 일러스트레이션. 텍스트 특징 및 현재 생성된 특징은 먼저 텍스트 및 시각적 컨텍스트로 집합되고, 이어서 텍스트 및 시각적 스코어를 예측하기 위해 이미지 특징과 공간적으로 연결된다. 그런 다음 이러한 점수는 현재 타임스테프를 기반으로 융합된다. 궁극적으로, 주요 특징들의 서브세트만이 융합된 스코어에 기초하여 선택된다.\n' +
      '\n' +
      '"CLIP-T 절 DINO"의 그림은 기존의 패러다임이 _dual-optimum paradox_에 갇혀 있는 반면 RealCustom은 이를 효과적으로 근절하고 있음을 검증한다.\n' +
      '\n' +
      '**정성적 결과.** 도에 도시된 바와 같이. 도 5에 도시된 바와 같이, _RealCustom_는 우수한 제로-샷 오픈-도메인 커스터마이징 능력(_e.g_, 첫 번째 행의 희귀 형상 장난감)을 입증하여, 기존 작업들에 비해 주어진 주제와 더 나은 유사성 및 주어진 텍스트와의 더 나은 제어성을 갖는 더 높은 품질의 커스터마이징 이미지들을 생성한다.\n' +
      '\n' +
      '### Ablations\n' +
      '\n' +
      '**_적응형 마스크 안내 전략의 효과_.** 우리는 먼저 도 6에서 제안된 적응형 마스크 안내 전략에 의해 실제 단어의 좁혀지는 과정을 시각화한다. 우리는 동일한 상태(첫 번째 단계에서 주어진 주제의 정보가 도입되지 않기 때문에 동일한 마스크)로부터 시작하여, _RealCustom_는 주어진 주제의 구조와 세부사항을 점진적으로 형성하여, 열린 도메인 제로-샷 커스터마이징을 유지하면서 달성한다는 것을 관찰할 수 있었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline \\multirow{2}{*}{Methods} & \\multicolumn{2}{c}{_controllability_} & \\multicolumn{2}{c}{_similarity_} & \\multicolumn{1}{c}{_efficiency_} \\\\ \\cline{2-5}  & CLIP-T \\(\\uparrow\\) & ImageReward \\(\\uparrow\\) & CLIP-I \\(\\uparrow\\) & DINO-I\\(\\uparrow\\) & test-time optimize steps \\\\ \\hline Textual Inversion [10] & 0.2546 & -0.9168 & 0.7603 & 0.5956 & 5000 \\\\ DreamBooth [27] & 0.2783 & 0.2393 & 0.8466 & 0.7851 & 800 \\\\ Custom Diffusion [16] & 0.2884 & 0.2558 & 0.8257 & 0.7093 & 500 \\\\ \\hline ELITE [34] & 0.2920 & 0.2690 & 0.8022 & 0.6489 & 0 (real-time) \\\\ BLIP-Diffusion [18] & 0.2967 & 0.2172 & 0.8145 & 0.6486 & 0 (real-time) \\\\ \\hline _RealCustom_**(ours)** & **0.3204** & **0.8703** & **0.8552** & **0.7865** & **0 (real-time)** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 기존 방법과의 정량적 비교. **Left**: 본 논문에서 제안한 _RealCustom_는 모든 메트릭에서 기존의 방법보다 우수한 제어성을 보였으며, CLIP-T와 ImageReward에서 각각 8.1%와 223.5%의 성능 향상을 보였다. ImageReward에 대한 상당한 개선은 또한 _RealCustom_이 훨씬 더 높은 품질(더 높은 심미적 점수)로 맞춤형 이미지를 생성할 수 있다는 것을 입증한다; (2) 유사성에 대해 CLIP-I와 DINO-I 모두에서 최첨단 성능을 달성한다. **Right**: 우리는 "CLIP-T 절 DINO"를 플롯하여, 기존의 방법들이 _dual-optimum paradox_에 갇히는 반면, _RealCustom_는 그것을 완전히 제거하고 고품질 유사성과 제어성을 모두 달성함을 보여준다. CLIP-T 절 CLIP-I에서도 같은 결론을 찾을 수 있다. 1(c).\n' +
      '\n' +
      '도 5: 기존 방법들과의 질적 비교. _ RealCustom_은 기존 작품들에 비해 주어진 주제와 더 잘 유사하고 주어진 텍스트와 더 나은 제어성을 갖는 훨씬 더 높은 품질의 커스터마이징 결과를 생성할 수 있다. 더욱이, _RealCustom_는 우수한 다양성(상이한 피사체 포즈, 위치, _etc_.) 및 생성 품질(_e.g_, 세 번째 행의 "가을 잎" 장면)을 보여준다.\n' +
      '\n' +
      '다른 주제-관련 부분들(_e.g_., 도시 배경)은 주어진 텍스트에 의해 완전히 제어된다.\n' +
      '\n' +
      '그런 다음 탭에서 Top-K 비율\\(\\gamma_{\\text{scope}}\\)을 제거한다. 2: (1) 적절한 범위(실험적으로, \\(\\gamma_{\\text{scope}}\\in[0.2,0.4]\\) 내에서, 결과는 상당히 강건하다; (2) 식에서 최대 정규화. 도 11은 선택된 부분들에서 상이한 영역들이 상이한 주제 관련성을 가지며 상이한 가중치들로 설정되어야 하기 때문에 높은 유사성 및 제어가능성의 단일성을 위해 중요하다. (3) 너무 작거나 너무 큰 영향 범위는 각각 유사성 또는 제어성을 저하시킬 것이다. 이러한 결론은 그림 7의 시각화에 의해 검증된다.\n' +
      '\n' +
      '**_adaptive scoring module_.**의 효과: 탭에 도시된 바와 같다. 3, (1) 먼저 모든 이미지 특징(ID-2)을 단순하게 사용하는 것과 비교하여 유사성과 제어성을 모두 저하시켜 조대-미세 확산 생성 과정과 함께 정확하고 부드러운 영향량 제공의 중요성을 증명하고, (2) 모듈 설계(ID-3,4,5,ID-5)에서 삭제하여 이미지 점수를 사용하는 것은 더 나쁜 성능만을 초래한다는 것을 발견한다. 그 이유는 초기에 생성 특징이 시끄럽기 때문에 부정확한 점수 예측이 발생하기 때문이다. 따라서 본 논문에서는 텍스트와 이미지 점수를 적응적으로 융합하여 최적의 성능을 얻을 수 있는 스텝스케줄러를 제안한다. (3) 마지막으로 ID-6과 7에서 영향량\\(\\gamma_{\\text{num}\\)의 선택을 제거한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 새로운 "train-inference" 프레임워크에서 주제 영향력(subject influence)을 특정 주제(subject)로 점차 좁혀지는 주제 영향력(subject influence)을 특정 주제(subject)로 정확하게 제한함으로써 주어진 주제들의 제어 가능성(controlability)으로부터 주어진 주제들의 유사성을 처음으로 해소하는 새로운 커스터마이징 패러다임 _RealCustom_을 제시한다. _adaptive scoring module_는 훈련 중에 영향량을 적응적으로 변조하도록 학습한다; (2) _adaptive mask guidance strategy_가 반복적으로 in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in-in\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline inference setting & CLIP-T \\(\\uparrow\\) & CLIP-I \\(\\uparrow\\) \\\\ \\hline \\(\\gamma_{\\text{scope}}=0.1\\) & 0.32 & 0.8085 \\\\ \\(\\gamma_{\\text{scope}}=0.2\\) & 0.3195 & 0.8431 \\\\ \\(\\gamma_{\\text{scope}}=0.25\\) & **0.3204** & **0.8552** \\\\ \\(\\gamma_{\\text{scope}}=0.25\\), binary & 0.294 & 0.8567 \\\\ \\(\\gamma_{\\text{scope}}=0.3\\) & 0.3129 & 0.8578 \\\\ \\(\\gamma_{\\text{scope}}=0.4\\) & 0.3023 & 0.8623 \\\\ \\(\\gamma_{\\text{scope}}=0.5\\) & 0.285 & 0.8654 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 추론 중 _RealCustom_에서 주어진 피험자의 영향 범위를 나타내는 상이한 \\(\\gamma_{\\text{scope}}\\)의 절제. "이진"은 Eq에서 max norm 대신 이진 마스크를 사용하는 것을 의미한다. 11.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline ID & settings & CLIP-T \\(\\uparrow\\) & CLIP-I \\(\\uparrow\\) \\\\ \\hline\n' +
      '1 & full model, \\(\\gamma_{\\text{num}}=0.8\\) & **0.3204** & **0.8552** \\\\\\\n' +
      '2 & _w/o_ adaptive scoring module & 0.3002 & 0.8221 \\\\ \\hline\n' +
      '3 & Textual score only, \\(\\gamma_{\\text{num}}=0.8\\) & 0.313 & 0.8335 \\\\\\\n' +
      '4 & visual score only, \\(\\gamma_{\\text{num}}=0.8\\) & 0.2898 & 0.802\\\\\\\n' +
      '5 & (textual+ visual) / 2, \\(\\gamma_{\\text{num}}=0.8\\) & 0.3156 & 0.8302 \\\\ \\hline\n' +
      '6 & full model, \\(\\gamma_{\\text{num}}=0.9\\) & 0.315 & 0.8541 \\\\\\\n' +
      '7 & full model, \\(\\gamma_{\\text{num}}=0.7\\) & 0.3202 & 0.8307 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 적응적 스코어링 모듈의 절제, 여기서 \\(\\gamma_{\\text{num}}\\)는 추론 동안 주어진 피험자의 영향량을 의미한다.\n' +
      '\n' +
      '그림 6: 실제 단어를 주어진 주제로 점차 좁히는 그림. **Upper**: _RealCustom_ 생성 결과(첫 번째 행)와 원본 텍스트 대 이미지 생성 결과(두 번째 행)는 동일한 시드로 미리 학습된 모델들에 의해 생성된다. 마스크는 실제 단어 "장난감"의 Top-25% 최고 주의 점수 영역에 의해 시각화된다. 우리는 동일한 상태(주어진 피험자의 정보가 처음에 도입되지 않았기 때문에 동일한 마스크)에서 시작하여 _RealCustom_이 제안된 _adaptive mask strategy_에 의해 주어진 피험자의 구조와 세부 사항을 점진적으로 형성하여 오픈 도메인 제로 샷 커스터마이징을 달성한다는 것을 관찰할 수 있었다. **하위**: 시각화 케이스가 더 많습니다.\n' +
      '\n' +
      '도 7: 상이한 영향 범위의 시각화.\n' +
      '\n' +
      '추론하는 동안 주어진 피험자의 유창성 범위와 양에 영향을 미친다. 광범위한 실험은 Real-Custom이 실시간 오픈 도메인 시나리오에서 고품질 유사성과 제어 가능성의 단일성을 달성한다는 것을 보여준다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. A neural space-time representation for text-to-image personalization. _arXiv preprint arXiv:2305.15391_, 2023.\n' +
      '* [2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. _arXiv preprint arXiv:2211.01324_, 2022.\n' +
      '* [3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. _arXiv preprint arXiv:2304.08465_, 2023.\n' +
      '* [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660, 2021.\n' +
      '* [5] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. _ACM Transactions on Graphics (TOG)_, 42(4):1-10, 2023.\n' +
      '* [6] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-augmented text-to-image generator. _arXiv preprint arXiv:2209.14491_, 2022.\n' +
      '* [7] Zhuowei Chen, Shancheng Fang, Wei Liu, Qian He, Mengqi Huang, Yongdong Zhang, and Zhendong Mao. Dreamidentity: Improved editability for efficient face-identity preserved image generation. _arXiv preprint arXiv:2307.00300_, 2023.\n' +
      '* [8] Giannis Daras and Alexandros G Dimakis. Multiresolution textual inversion. _arXiv preprint arXiv:2211.17115_, 2022.\n' +
      '* [9] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-to-image generation via contrastive prompt-tuning. _arXiv preprint arXiv:2211.11337_, 2022.\n' +
      '* [10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. _arXiv preprint arXiv:2208.01618_, 2022.\n' +
      '* [11] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. Designing an encoder for fast personalization of text-to-image models. _arXiv preprint arXiv:2302.12228_, 2023.\n' +
      '* [12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.\n' +
      '* [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [14] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-image diffusion models. _arXiv preprint arXiv:2304.02642_, 2023.\n' +
      '* [15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* [16] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1931-1941, 2023.\n' +
      '* [17] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _International Journal of Computer Vision_, 128(7):1956-1981, 2020.\n' +
      '* [18] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. _arXiv preprint arXiv:2305.14720_, 2023.\n' +
      '* [19] Yumeng Li, Margret Keuper, Dan Zhang, and Anna Khoreva. Divide & bind your attention for improved generative semantic nursing. _arXiv preprint arXiv:2307.10864_, 2023.\n' +
      '* [20] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22511-22521, 2023.\n' +
      '* [21] Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Guiding text-to-image diffusion model towards grounded generation. _arXiv preprint arXiv:2301.05221_, 2023.\n' +
      '* [22] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis with multiple subjects. _arXiv preprint arXiv:2305.19327_, 2023.\n' +
      '* [23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [24] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1 (2):3, 2022.\n' +
      '* [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '**[26] 올라프 론버거, 필립 피셔, 토마스 브록스. U-net: 생체 의학 영상 분할을 위한 컨볼루션 네트워크. _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.\n' +
      '* [27] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.\n' +
      '* [28] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* [29] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.\n' +
      '* [30] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-booth: Personalized text-to-image generation without test-time finetuning. _arXiv preprint arXiv:2304.03411_, 2023.\n' +
      '* [31] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* [32] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. \\(p+\\): Extended textual conditioning in text-to-image generation. _arXiv preprint arXiv:2303.09522_, 2023.\n' +
      '* [33] Jinglong Wang, Xiawei Li, Jing Zhang, Qingyuan Xu, Qin Zhou, Qian Yu, Lu Sheng, and Dong Xu. Diffusion model is secretly a training-free open vocabulary semantic segmenter. _arXiv preprint arXiv:2309.02773_, 2023.\n' +
      '* [34] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. _arXiv preprint arXiv:2302.13848_, 2023.\n' +
      '* [35] Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chunhua Shen. Diffumask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models. _arXiv preprint arXiv:2303.11681_, 2023.\n' +
      '* [36] Changming Xiao, Qi Yang, Feng Zhou, and Changshui Zhang. From text to mask: Localizing entities using the attention of text-to-image diffusion models. _arXiv preprint arXiv:2309.04109_, 2023.\n' +
      '* [37] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagenetward: Learning and evaluating human preferences for text-to-image generation. _arXiv preprint arXiv:2304.05977_, 2023.\n' +
      '* [38] Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver Deussen, and Changsheng Xu. Prospect: Expanded conditioning for the personalization of attribute-aware image generation. _arXiv preprint arXiv:2305.16225_, 2023.\n' +
      '\n' +
      '## 6 Supplementary\n' +
      '\n' +
      '더 많은 질적 비교\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 8을 참조하면, **real-time** 커스터마이징 시나리오에서 제안된 _RealCustom_과 이전 _pseudo-word_ 패러다임의 최신 최신 방법 간의 보다 정성적 비교를 제공한다. 기존의 최신 기술과 비교할 때, 우리는 (1) 주어진 피험자와 더 나은 유사성** 및 주어진 텍스트와 더 나은 제어 가능성**을 동시에**(7^{\\text{th}}\\) 행에서 _RealCustom_에 의해 생성된 장난감이 정확하게 만리장성에 부착되지 않는 반면, 기존 작업은 주어진 텍스트에 부착되지 않는다는 결론을 도출할 수 있었다. 한편, _RealCustom_에 의해 생성된 장난감은 기존의 작업들이 그것들을 보존하지 못하는 동안 주어진 것의 모든 세부사항들을 정확히 모방한다. (2) **더 나은 이미지 품질**, _i.e._, 더 나은 미적 점수로, _e.g._, 두 번째 행의 눈 장면, 세 번째 행의 흙길 장면, _etc_. 본 논문의 ImageReward [37]에 대한 연구 결과는 ImageReward [37]에 대한 제어 가능성과 화질 평가 모두에서 유의미한 개선(223.5% 개선)을 고수하고 있다. (3) open domain**에서 **b better generalization, _i.e._, _any given subjects_에 대해, _RealCustom_는 개와 같은 공통 주제(_e.g._, \\(5^{th},6^{th}\\) 행)와 고유 백팩(_i.e._, \\(1^{st}\\) 행)과 같은 희귀 주제를 포함하여 실시간으로 주어진 주제에 대해 주어진 텍스트를 일관되게 고수하는 사실적인 이미지를 생성할 수 있는 반면, 기존의 최첨단 기술은 첫 번째 행의 배낭, 마지막 행의 특수 장난감, _etc_와 같은 희귀 주제에 대해 제대로 작동하지 않는다. 그 이유는 처음으로 제안된 _RealCustom_이 실제 텍스트 단어를 초기 일반적인 함축에서 고유한 주제로 점진적으로 좁혀 주어진 주제와 학습된 의사 단어 사이의 필요한 대응 관계를 완전히 제거하고 따라서 더 이상 제한된 범주를 가진 객체 데이터 세트에 대한 훈련에만 국한되지 않는다는 것이다.\n' +
      '\n' +
      '### More Visualization\n' +
      '\n' +
      '우리는 그림에서 제안된 _RealCustom_의 실제 단어의 좁혀지는 과정에 대한 보다 포괄적인 시각화를 제공한다. 도 9 및 도 9를 참조하여 설명한다. 10. 여기서, 우리는 동일한 주어진 텍스트 "사막의 장난감"과 4개의 다른 주어진 주제를 가진 4개의 커스터마이징 케이스를 제공한다. 좁을 때 사용되는 진짜 텍스트 단어는 "장난감"이다. 마스크는 실제 텍스트 단어 "장난감"의 Top-25% 최고 주의 점수 영역에 의해 시각화된다. 총 50개의 DDIM 샘플링 단계에서 모든 마스크를 시각화한다. 우리는 "장난감"의 마스크가 점차 부드럽고 정확하게 주어진 특정 주제로 좁혀지는 것을 관찰할 수 있었다. 한편, 이러한 주제 관련 부분(이들 경우에 실제 텍스트 단어 "장난감"의 상위 25% 최고 주의 점수 영역)에서도, 이들의 관련성은 또한 도에서 _예를 들어, 다르다. 도 9에 도시된 바와 같이, 제1 피험자의 눈과 같은 더 중요한 부분들은 더 높은 가중치(마스크에서 더 밝음)를 부여받는다. 도 10에 도시된 바와 같이, 제2 피험자의 눈과 같이 더 중요한 부분들은 더 높은 가중치를 부여받는다.\n' +
      '\n' +
      '서로 다른 실제 단어의 영향\n' +
      '\n' +
      '다른 실제 텍스트 단어를 사용하는 사용자 정의 결과가 그림 1에 나와 있다. 11. 커스터마이징을 위해 좁혀진 실제 텍스트 단어는 빨간색으로 강조 표시된다. 우리는 다음과 같은 결론을 도출할 수 있었다. (1) 제안된 _RealCustom_의 사용자 정의 결과는 **꽤 강력한**, _i.e._이다. 주어진 주제를 표현하기 위해 얼마나 거친 텍스트 단어를 사용하더라도, 사용자 정의 결과에서 생성된 주제는 항상 주어진 주제와 거의 동일하다. 예를 들어, 상위 세 행에서, 우리가 주어진 주제를 맞춤화하기 위해 "corgi", "dog" 또는 "animal"을 사용할 때, 결과는 모두 주어진 주제에 일관되게 고수된다. 이 현상은 또한 제안된 새로운 패러다임 _RealCustom_의 일반화 및 견고성을 검증한다. (2) 주어진 주제를 나타내기 위해 _completely different word_를 사용하는 경우, _e.g._, 코기를 나타내기 위해 "parroft"를 사용하는 경우, 제안된 _RealCustom_는 새로운 응용 프로그램인 _i.e._, **novel concept creation**를 위한 문을 연다. 즉, _RealCustom_은 이 두 개념을 결합하여 새로운 개념인 _e.g._를 생성하려고 할 것이며, 아래 세 행과 같이 주어진 갈색 코기의 모양과 문자와 함께 파로프트를 생성한다. 이 응용 프로그램은 영화나 게임에서 새로운 캐릭터인 _etc_를 디자인하는 데 매우 유용할 것이다.\n' +
      '\n' +
      '그림 8: **real-time** 커스터마이징 시나리오에서 제안된 _RealCustom_과 이전 _pseudo-word_ 패러다임의 최신 최신 방법 간의 질적 비교. 우리는 (1) 현존하는 최신 기술과 비교하여, _RealCustom_은 주어진 주제와 훨씬 더 유사한 ***을 보이고, 또한 주어진 텍스트와 함께 **보다 더 나은 제어성을 보인다는 결론을 내릴 수 있었다. (7^{\\text{th}}\\) 행에서, _RealCustom_에 의해 생성된 장난감은 만리장성에서 정확하게 생성되지만, 현존하는 작품들은 주어진 텍스트에 충실하지 못한다. 한편, _RealCustom_에 의해 생성된 장난감은 기존의 작업들이 그것들을 보존하지 못하는 동안 주어진 것의 모든 세부사항들을 정확히 모방한다. (2) _RealCustom_는 훨씬 **b better quality**, _i.e._, 더 나은 미적 점수, _e.e._, 두 번째 행의 눈 장면, 세 번째 행의 흙길 장면, _etc._ 이 결과는 ImageNet이 제어성과 화질을 모두 평가하기 때문에 본 논문의 ImageNetward [37]에 대한 유의미한 개선(223.5% 개선)을 고수한다. (3) _RealCustom_는 공개 도메인**, _i.e._에서 더 나은 일반화를 보여주며, _RealCustom_는 개와 같은 공통 주제(_e.g._, \\(5^{th},6^{th}\\) 행)와 독특한 배낭과 같은 희귀 주제(_i.e._, \\(1^{st}\\) 행)를 포함하여 실시간으로 주어진 주제에 대해 주어진 텍스트에 일관되게 고수하는 사실적인 이미지를 생성할 수 있는 반면, 기존의 최첨단 제품은 첫 번째 행의 배낭과 같은 희귀 주제, 마지막 행의 특수 장난감, _etc._ 및 ^{st}\\) 행과 같은 희귀 주제에서 제대로 작동하지 않는다.\n' +
      '\n' +
      '그림 9: 실제 단어를 주어진 주제로 점차 좁히는 그림. 여기에서 우리는 동일한 주어진 텍스트 "사막의 장난감"과 두 가지 다른 주어진 주제를 가진 두 가지 커스터마이징 사례를 제공한다. 좁을 때 사용되는 실제 텍스트 단어는 "장난감"입니다. 마스크는 실제 텍스트 단어 "장난감"의 Top-25% 최고 주의 점수 영역에 의해 시각화된다. 우리는 왼쪽에 표시된 총 50개의 DDIM 샘플링 단계에서 모든 마스크를 시각화한다. 우리는 "장난감"의 마스크가 점차 부드럽고 정확하게 주어진 특정 주제로 좁혀지는 것을 관찰할 수 있었다. 한편, 이러한 주제 관련 부분들(이러한 경우들에서 실제 텍스트 단어 "장난감"의 상위 25% 최고 주의 점수 영역)에서도, 이들의 관련성은 또한 상이하며, _예를 들어, 첫 번째 주제의 눈과 같은 더 중요한 부분들은 더 높은 가중치를 부여받는다(마스크에서 더 밝음).\n' +
      '\n' +
      '그림 10: 실제 단어를 주어진 주제로 점차 좁히는 그림. 여기에서 우리는 동일한 주어진 텍스트 "사막의 장난감"과 두 가지 다른 주어진 주제를 가진 두 가지 커스터마이징 사례를 제공한다. 좁을 때 사용되는 실제 텍스트 단어는 "장난감"입니다. 마스크는 실제 텍스트 단어 "장난감"의 Top-25% 최고 주의 점수 영역에 의해 시각화된다. 우리는 왼쪽에 표시된 총 50개의 DDIM 샘플링 단계에서 모든 마스크를 시각화한다. 우리는 "장난감"의 마스크가 점차 부드럽고 정확하게 주어진 특정 주제로 좁혀지는 것을 관찰할 수 있었다. 한편, 이러한 주제 관련 부분들(이러한 경우들에서 실제 텍스트 단어 "장난감"의 상위 25% 최고 주의 점수 영역)에서도, 이들의 관련성은 또한 상이하며, _예를 들어, 제2 주제의 눈과 같은 더 중요한 부분들은 더 높은 가중치를 부여받는다(마스크에서 더 밝음).\n' +
      '\n' +
      '도 11: 커스터마이징은 상이한 실제 텍스트 단어들을 사용하는 결과를 초래한다. 사용자 정의를 위해 좁혀진 실제 텍스트 단어는 빨간색으로 강조 표시됩니다. 우리는 다음과 같은 결론을 도출할 수 있었다. (1) 제안된 _RealCustom_의 사용자 정의 결과는 **꽤 강력한**, _i.e._이다. 주어진 주제를 표현하기 위해 얼마나 거친 텍스트 단어를 사용하더라도, 사용자 정의 결과에서 생성된 주제는 항상 주어진 주제와 거의 동일하다. 예를 들어, 상위 세 행에서, 우리가 주어진 주제를 맞춤화하기 위해 "corgi", "dog" 또는 "animal"을 사용할 때, 결과들은 모두 주어진 주제를 일관되게 고수한다. 이 현상은 또한 제안된 새로운 패러다임 _RealCustom_의 일반화 및 견고성을 검증한다. (2) 주어진 주제를 나타내기 위해 _completely different word_를 사용하는 경우, _e.g._, 코기를 나타내기 위해 "parrot"를 사용하는 경우, 제안된 _RealCustom_는 새로운 응용 프로그램인 _i.e._, **novel concept creation**를 위한 문을 연다. 즉, _RealCustom_은 이 두 개념을 결합하여 새로운 개념인 _e.g._를 생성하려고 할 것이며, 아래 세 행과 같이 주어진 갈색 코기의 모양과 문자로 앵무새를 생성한다. 이 응용 프로그램은 영화나 게임에서 새로운 캐릭터를 디자인하는 데 매우 유용할 것이다._etc._\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>