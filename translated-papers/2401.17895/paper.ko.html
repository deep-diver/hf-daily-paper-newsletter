<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#ReplaceAnything3D: 합성 신경 복사 필드를 사용한 텍스트 유도 3D 장면 편집\n' +
      '\n' +
      'Edward Bartrum \\({}^{1,2}\\)\n' +
      '\n' +
      '메타리얼리티 연구소에서 인턴을 하는 동안 수행한 작업\n' +
      '\n' +
      '프로젝트 페이지: [https://replaceanything3d.github.io](https://replaceanything3d.github.io)\n' +
      '\n' +
      'Thu Nguyen-Phuoc \\({}^{3}\\)\n' +
      '\n' +
      'Chris Xie\\({}^{3}\\)\n' +
      '\n' +
      '정진리\\({}^{3}\\)\n' +
      '\n' +
      '누메르 칸\\({}^{3}\\)\n' +
      '\n' +
      'Armen Avetisyan \\({}^{3}\\)\n' +
      '\n' +
      '더글라스 란만\\({}^{3}\\)\n' +
      '\n' +
      'Lei Xiao \\({}^{3}\\)\n' +
      '\n' +
      '({}^{1}\\) University College London \\({}^{2}\\) Alan Turing Institute \\({}^{3}\\) Reality Lab Research, Meta\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '_우리는 장면 내에서 특정 객체의 교체가 가능한 새로운 텍스트 유도 3D 장면 편집 방법인 ReplaceAnything3D 모델(RAM3D)을 소개한다. 장면의 다시점 영상, 대체할 객체를 묘사하는 텍스트 프롬프트, 새로운 객체를 묘사하는 텍스트 프롬프트가 주어지면, 지우고 바꾸기 접근법은 여러 시점에 걸쳐 3D 일관성을 유지하면서 장면 내의 객체를 새로 생성된 콘텐츠로 효과적으로 바꿀 수 있다. 악마\n' +
      '\n' +
      '그림 1: 우리의 방법은 다양한 사실적인 3D 장면에 대한 신속한 구동 객체 교체를 가능하게 한다.\n' +
      '\n' +
      'ReplaceAnything3D를 다양한 사실적 3D 장면에 적용하여 ReplaceAnything3D의 범용성을 입증하며, 전체 무결성에 영향을 미치지 않으면서 나머지 장면과 잘 통합된 수정된 전경 객체의 결과를 보여준다._\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '새로운 소셜 미디어 플랫폼과 디스플레이 장치의 폭발적인 증가는 고품질 3D 콘텐츠에 대한 수요의 급증을 촉발했다. 몰입형 게임 및 영화에서부터 최첨단 가상현실(VR) 및 혼합현실(MR) 애플리케이션에 이르기까지, 3D 콘텐츠를 생성하고 편집하기 위한 효율적인 도구의 필요성이 증가하고 있다. 3D 재구성 및 생성에 상당한 진전이 있었지만, 3D 편집은 아직 덜 연구된 영역으로 남아 있다. 본 논문에서는 사용자의 자연어 프롬프트만으로 장면 내의 현재 객체들을 새로운 콘텐츠로 대체함으로써 3차원 장면 조작에 초점을 맞춘다. VR 헤드셋을 착용하고 거실을 리모델링하려고 한다고 상상해 보세요. 매끄러운 새 디자인으로 현재의 소파를 교체하거나, 무성한 녹지를 추가하거나, 잡동사니를 제거하여 더 넓은 느낌을 연출할 수 있습니다.\n' +
      '\n' +
      '이 프로젝트에서는 장면 편집을 위한 텍스트 유도 소거 및 대체 방법인 ReplaceAnything3D 모델(RAM3D)을 소개한다. RAM3D는 정적 장면의 다시점 영상을 입력으로 하고, 어떤 객체를 지울지, 무엇을 대체해야 하는지를 텍스트 프롬프트로 지정한다. 지울 객체를 탐지하고 세그먼트화하기 위해 텍스트 프롬프트와 함께 LangSAM[24]을 사용합니다. 2) 객체를 지우기 위해, 제거된 객체에 의해 가려진 배경 영역을 채우기 위한 텍스트 유도 3D 인페인팅 기법을 제안한다. 3) 다음으로, 유사한 텍스트 유도 3D 인페인팅 기술이 입력 텍스트 설명과 일치하는 새로운 객체(들)를 생성하기 위해 사용된다. 중요하게는, 이것은 객체의 질량이 최소가 되도록 수행된다. 4) 마지막으로, 새로 생성된 객체를 학습 뷰에서 인페인팅된 배경에 매끄럽게 합성하여 편집된 3D 장면의 일관된 다시점 영상을 획득한다. 그런 다음 새로운 뷰 합성을 위해 편집된 장면의 3D 표현을 획득하기 위해 NeRF[26]가 이러한 새로운 멀티뷰 이미지들에 대해 트레이닝될 수 있다. 이 합성 구조는 편집된 장면에서 배경과 전경 모두의 시각적 품질을 크게 향상시킨다는 것을 보여준다.\n' +
      '\n' +
      '2D 이미지에 비해 3D 장면에서 객체를 대체하는 것은 다시점 일관성의 요구로 인해 훨씬 더 어렵다. 마스킹과 비도색을 위한 2D 방법을 순진하게 적용하면 각 비도색 시점의 시각적 불일치로 인해 일관성 없는 결과가 발생한다. 이 문제를 해결하기 위해 우리는 대규모 이미지 확산 모델, 특히 텍스트 유도 이미지 인페인팅 모델에 대한 사전 지식을 학습된 3D 장면 표현과 결합하는 것을 제안한다. 새로운 다시점 일관 3차원 객체를 생성하기 위해 텍스트-3차원 증류 방식인 Hifa[57]을 3차원 인페인팅 프레임워크에 적용한다. 순수 텍스트 대 3D 접근법에 비해 ReplaceAnything3D는 입력 텍스트 프롬프트를 따를 뿐만 아니라 나머지 장면의 모습과 호환되는 새로운 콘텐츠를 생성할 필요가 있다. Pre-trained text-guided image Inpainting 모델과 합성 장면 구조를 결합함으로써 ReplaceAnything3D는 원 장면의 나머지 부분과 매끄럽게 혼합된 새로운 객체들로 일관성 있는 편집된 3D 장면들을 생성할 수 있다.\n' +
      '\n' +
      '요약하자면, 우리의 기여는 다음과 같습니다.\n' +
      '\n' +
      '* 우리는 높은 해상도로 장면 내의 특정 객체들의 교체를 가능하게 하는 3D 장면 편집에 대한 Erase-and-Replace 접근법을 소개한다.\n' +
      '* 객체 교체뿐만 아니라 제거 및 다중 객체 추가가 가능한 다단계 접근 방식을 제안한다.\n' +
      '* 우리는 ReplaceAnything3D가 순방향 및 360\\({}^{\\circ}\\) 장면을 포함한 여러 장면 유형에서 3D 일관된 결과를 생성할 수 있음을 보여준다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '텍스트 유도 이미지 편집을 위한 확산 모델 광범위한 텍스트 이미지 데이터 세트에 대해 훈련된 확산 모델은 텍스트 프롬프트로부터 복잡한 의미를 캡처하는 능력을 보여주며 놀라운 결과를 보여주었다[38, 40, 42]. 결과적으로, 이러한 모델들은 다양한 텍스트 유도 이미지 편집 작업들 [6, 11, 18, 30, 31]에 대해 강한 사전들을 제공한다. 특히, 텍스트-유도 이미지 인페인팅[1, 2]을 위한 방법들은 마스킹된 영역들을 이미지의 나머지 부분과 매끄럽게 섞이는 새로운 콘텐츠로 대체함으로써 로컬 이미지 편집을 가능하게 하여 객체 제거, 교체, 추가를 가능하게 한다. 이러한 방법은 3D 장면에 대한 접근법에 대한 직접적인 2D 대응이며, 각 뷰는 이미지 인페인팅 작업으로 취급될 수 있다. 그러나, 3D 장면들은 기본 3D 표현들로 인한 멀티-뷰 일관성 및 메모리 제약들에 대한 요구와 같은 추가적인 도전들을 제시한다. 이 작업에서 ReplaceAnything3D는 미리 훈련된 이미지 인페인팅 모델과 구성 3D 표현을 결합하여 이러한 문제를 해결한다.\n' +
      '\n' +
      '신경 복사 필드 편집 최근 NeRF의 발전은 시각적 품질[3, 5, 17], 훈련 및 추론 속도[9, 14, 32], 잡음 또는 희소 입력에 대한 견고성[19, 33, 53, 55]에서 상당한 개선을 가져왔다. 그러나 NeRF를 편집하는 것은 여전히 어려운 영역으로 남아 있다. 대부분의 기존 작업은 객체의 모양이나 형상을 편집하는 데 중점을 둡니다[13, 22, 45, 48, 56]. 장면 레벨 편집을 위해, 최근의 작업들은 주로 전방-대향 장면들에 대한 객체 제거 작업들을 다룬다[27, 29, 52]. Instruct-NeRF2NeRF[10]은 외관 편집 및 객체 추가 모두에 대한 포괄적인 접근법을 제공한다. 그러나, 전체 장면을 수정하는 반면, Blended-Nerf[45] 및 DreamEditor[58]는 로컬화된 객체 편집을 허용하지만 객체 제거를 지원하지 않는다. 우리와 가장 근접한 작업은 Mirzaei et al. [27]에 의한 것이며, 이는 사용자로부터 하나의 단일 이미지 참조를 이용하여 객체를 제거하고 대체할 수 있다. 그러나, 이 방법은 단지 하나의 인페인팅된 이미지에 의존하기 때문에, 상이한 뷰들에 걸쳐 큰 폐색을 갖는 영역들을 처리할 수 없고, 따라서 전방-대향 장면들에만 적용된다.\n' +
      '\n' +
      'ReplaceAnything3D는 장면 콘텐츠의 기존 형상이나 모양을 수정하는 대신 지역화된 장면 편집을 위해 삭제 및 대체 접근법을 채택한다는 점에 유의하는 것이 중요하다. 이를 통해 ReplaceAnything3D는 동일한 프레임워크 내에서 국부적인 객체 제거, 교체 및 추가를 전체적으로 제공하는 첫 번째 방법이다.\n' +
      '\n' +
      '텍스트 대 3D 합성은 텍스트 대 이미지 확산 모델의 놀라운 성공으로 인해 점점 더 많은 관심을 받고 있다. 이 분야의 대부분의 작업은 미리 훈련된 텍스트-이미지 모델을 3D 모델로 증류하는 데 중점을 두고 있으며, 주요 작업인 드림퓨전[34]과 스코어 자코비안 체인(SJC)[49]를 시작한다. 후속 연구에서는 합성물[20, 25, 51, 57]과 디엔탱글 기하학 및 외관[7]의 품질을 높이기 위한 다양한 방법을 탐색하였다. 사전 훈련된 텍스트 대 이미지 모델에만 의존하는 대신, 최근 연구는 텍스트 또는 단일 이미지로부터 3D 합성의 품질을 향상시키기 위해 Objaverse[8]와 같은 대규모 3D 데이터 세트를 활용했다[21, 36].\n' +
      '\n' +
      '본 연구에서는 텍스트 프롬프트와 주변 장면 정보를 입력으로 통합하여 텍스트에서 3D 합성으로 넘어간다. 이 접근법은 3D 객체의 외관을 장면의 나머지 부분과 조화롭게 혼합하고 오클루전 및 그림자와 같은 객체-객체 상호작용을 정확하게 모델링하는 것과 같은 추가적인 복잡성을 도입한다. 텍스트 대 3D 증류 접근법인 HiFA[57]를 미리 훈련된 텍스트 대 이미지 인페인팅 모델과 결합함으로써, ReplaceAnything3D는 합성된 3D 객체를 매끄럽게 통합하는 보다 현실적이고 일관된 3D 장면을 만드는 것을 목표로 한다.\n' +
      '\n' +
      '## 3 Preliminary\n' +
      '\n' +
      'NeRFNeural Radiance Fields(NeRFs) [26]은 3D 장면 재구성 및 렌더링을 위한 작고 강력한 암시적 표현이다. 특히, NeRF는 입력이 3D 위치\\(\\mathbf{x}\\)와 2D 시청방향\\(\\mathbf{d}\\)이고 출력이 방출색\\(\\mathbf{c}=(r,g,b)\\)과 부피밀도\\(\\sigma\\)인 연속 5D 함수이다. 이 함수는 다층 퍼셉트론(MLP: \\(F_{\\theta}:(\\mathbf{x},\\mathbf{d})\\mapsto(\\mathbf{c},\\sigma)\\)로 근사되며, 이는 이미지 복원 손실을 이용하여 학습된다. 화소를 렌더링하기 위해 MLP로부터 \\(t{=}0\\)에서 \\(D\\)까지 샘플링된 카메라 광선을 따라 여러 점의 색상과 밀도를 조회한다. 이 값들은 볼륨 렌더링을 사용하여 최종 픽셀 컬러를 계산하기 위해 누적된다:\n' +
      '\n' +
      '\\[\\mathbf{C}=\\int_{0}^{D}\\mathcal{T}(t)\\cdot\\sigma(t)\\cdot\\mathbf{c}(t)\\dt\\tag{1}\\]\n' +
      '\n' +
      '트레이닝 동안, 다양한 시점들로부터 샘플링된 광선들의 랜덤 배치가 재구성된 객체들의 3D 위치들이 잘 구속되는 것을 보장하기 위해 사용된다. 최적화된 장면 MLP로부터 새로운 시점을 렌더링하기 위해, 신규 이미지 내의 모든 픽셀들에 대응하는 광선들의 세트가 샘플링되고 결과적인 컬러 값들이 2D 프레임으로 배열된다.\n' +
      '\n' +
      '본 논문에서는 다중 해상도 해시 인코딩으로 인해 보다 효율적이고 빠른 버전의 NeRF인 Instant-NGP[32]를 사용한다. 이를 통해 더 높은 해상도로 이미지를 처리할 수 있고 향상된 이미지 품질을 위해 렌더링 광선을 따라 더 많은 수의 샘플을 쿼리할 수 있다.\n' +
      '\n' +
      '텍스트-이미지 확산 모델 드림퓨전[34]은 3D 신경 복사 필드(NeRF)의 매개변수를 최적화하기 위해 2D 사전 훈련된 텍스트-이미지 확산 모델의 기울기를 계산하기 위해 점수 증류 샘플링이라고 하는 기술을 제안한다. 최근 HiFA[57]은 LDM(Latent Diffusion Model)에 대해 명시적으로 계산할 수 있는 대체 손실 공식을 제안한다. 암시적 3차원 장면의 매개변수는 \\(\\theta_{scene}\\), 텍스트 프롬프트는 \\(y\\), 인코더 \\(E\\) 및 디코더 \\(D\\), \\(\\theta_{scene}\\)을 사용하여 미리 학습된 LDM 모델인 \\(\\epsilon_{\\phi}(\\mathbf{z_{t}},t,y)\\)을 사용하여 최적화될 수 있다.\n' +
      '\n' +
      '\\phi,\\mathbf{z},\\mathbf{x})=\\mathbbb{E}_{t,\\epsilon}w(t)\\left[\\|\\mathbf{z}-\\mathbf{\\hat{z}\\|^{2}+\\lambda_{RGB}\\|\\mathbf{x}-\\mathbf{\\hat{x}\\|^{2}\\right]\\tag{2}\\t.\n' +
      '\n' +
      '여기서, \\(\\mathbf{z}=E(\\mathbf{x})\\)는 학습 데이터세트로부터 카메라 관점에서 \\(\\theta_{scene}\\)의 렌더링된 이미지 \\(\\mathbf{x}\\)를 인코딩하여 잠재 벡터를 구하고, \\(\\mathbf{\\hat{z}\\)는 데노이저 \\(\\epsilon_{\\phi}\\)에 의한 잠재 벡터 \\(\\mathbf{z}\\)의 추정치이고, \\(\\mathbf{\\hat{x}=D(\\mathbf{\\hat{z})\\)는 LDM의 디코더 \\(D\\)를 통해 얻은 복원 이미지이다. 간결함을 위해 타임스텝 \\(t\\)에서 \\(w(t)\\)에 관련된 계수를 통합한다.\n' +
      '\n' +
      '여기서 우리는 생성된 객체가 텍스트 프롬프트에 의해서만 조건화되는 텍스트-3D 합성 태스크에서 벗어나게 된다. 대신, 우리는 합성된 객체에 대한 추가 입력으로서 장면 뷰들의 집합을 고려한다. 이를 위해, 우리는 HiFA를 이미지 내에서 원활한 인페인팅 영역을 생성하기 위해 미세 조정된 최첨단 텍스트-이미지 _inpainting_ LDM과 함께 활용한다. 이 LDM\\(\\epsilon_{\\psi}(\\mathbf{z_{t}},t,y,\\mathbf{m})\\)은 텍스트 프롬프트\\(y\\)뿐만 아니라, 채워질 영역을 나타내는 이진 마스크\\(\\mathbf{m}\\)을 필요로 한다.\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      '학습 데이터셋은 사용자가 대체하고자 하는 대상을 설명하는 \\(n\\) 이미지 \\(I_{i}\\), 해당 카메라 시점 \\(\\mathbf{v}_{i}\\) 및 텍스트 프롬프트 \\(y_{text{erase}\\)의 집합으로 구성된다. 이 텍스트 프롬프트를 이용하여 모든 이미지 및 카메라 시점에 해당하는 마스크 \\(\\mathbf{m}_{i}\\)를 얻을 수 있다. 또한 기존 객체를 대체할 새로운 객체를 설명하는 텍스트 프롬프트(y_{\\text{replace}\\)가 있다. 우리의 목표는 데이터세트의 모든 이미지에서 마스킹된 객체를 텍스트 프롬프트\\(y_{\\text{replace}}\\)와 일치하도록 다중 뷰 일관성 있게 수정하는 것이다. 그런 다음 새로운 관점에서 편집된 장면의 렌더링을 얻기 위해 수정된 이미지를 사용하여 NeRF와 같은 장면 재구성 모델을 훈련할 수 있다.\n' +
      '\n' +
      '그림 2는 지우기 및 교체 프레임워크의 전체 파이프라인을 보여준다. 다른 방법[10, 58]과 같이 대상 텍스트 설명과 일치하는 기존 객체의 지오메트리 및 모양을 수정하는 대신 지우기 및 대체 접근법을 채택한다. 먼저, **Erase** 단계에서는 마스킹된 객체를 완전히 제거하고 배경의 가려진 영역을 인페인팅한다. 둘째, **Replace** 단계의 경우, 새로운 객체를 생성하고, 새로운 객체가 나머지 배경과 혼합되도록 인페인팅된 배경 장면에 합성한다. 마지막으로, 원본 장면으로부터 편집된 영상과 카메라 포즈를 이용하여 새로운 학습 세트를 생성하고, 새로운 시점 합성을 위해 수정된 장면에 대한 새로운 NeRF를 학습한다.\n' +
      '\n' +
      '텍스트 유도 장면 편집을 가능하게 하기 위해, 우리는 HiFA[57]를 사용하여 장면 내의 새로운 3D 객체들을 생성하기 위해 미리 훈련된 텍스트-이미지 인페인팅 잠재 확산 모델(LDM)을 증류한다. NeRF와 같은 암시적 3D 장면 표현의 메모리 제약을 해결하기 위해, 우리는 전체 장면 대신에 편집 동작에 의해 영향을 받는 장면의 국부화된 부분만을 모델링하는 버블-NeRF 표현(도 3 및 4 참조)을 제안한다.\n' +
      '\n' +
      '### Erase stage\n' +
      '\n' +
      '이레이즈 단계에서는 장면으로부터 \\(y_{erase}\\)으로 기술된 객체를 제거하고, 폐색된 배경 영역을 다시점 일관성 있게 인페인팅하는 것을 목표로 한다. 이를 위해, 내재된 배경 장면을 암시적으로 나타내는 RAM3D 파라미터 \\(\\theta_{bg}\\)를 최적화한다. 지우기 단계는 제거하려는 원하는 객체에 대해 한 번만 수행하면 되며, 그 후 교체 단계(섹션 4.3)를 사용하여 결과 섹션에서 설명한 대로 객체를 생성하거나 장면에 새 객체를 추가할 수도 있습니다. 전처리 단계로 텍스트 프롬프트 \\(y_{erase}\\)가 있는 LangSAM [24]를 사용하여 데이터 세트에서 각 이미지에 대한 마스크 \\(\\mathbf{m}_{i}\\)을 얻는다. 그런 다음 각 \\(\\mathbf{m}_{i}\\)을 확장하여 원래 입력 마스크 주위에 _halo_ 영역 \\(\\mathbf{h}_{i}\\)을 얻는다(도 3 참조).\n' +
      '\n' +
      '각 학습 단계에서 랜덤한 \\(i\\in\\{1..n\\}\\)에 대해 이미지 \\(I_{i}\\), 카메라 \\(\\mathbf{v}_{i}\\), 마스크 \\(\\mathbf{m}_{i}\\) 및 할로 영역 \\(\\mathbf{h}_{i}\\)을 샘플링하여 RAM3D에 입력으로 제공하여 학습 손실을 계산(도 2의 왼쪽)한다(이후 명확성을 위해 첨자 i를 삭제한다). RAM3D 볼륨은 \\(\\mathbf{m}\\) 및 \\(\\mathbf{h}\\) (Bubble-NeRF 영역)에서 가시 픽셀을 통과하는 카메라 시점 \\(\\mathbf{v}\\)에서 방출되는 광선 위에 암시적 3D 표현 \\(\\theta_{bg}\\)을 렌더링한다. 버블-NeRF의 외부에 있는 나머지 픽셀의 RGB 값은 \\(I\\)에서 샘플링된다(도 3 참조). 이러한 렌더링 및 샘플링된 픽셀 rgb 값은 2D 배열로 배열되고 주어진 뷰에 대한 RAM3D의 인페인팅 결과인 \\(\\mathbf{x}^{bg}\\)를 형성한다. HiFA 공식(섹션 3 참조)에 이어 냉동 LDM의 \\(E\\)을 사용하여 \\(\\mathbf{x}^{bg}\\)을 인코딩하여 \\(\\mathbf{z}^{bg}\\)을 얻고, 노이즈를 추가하고 \\(\\epsilon_{\\psi}\\)으로 잡음을 추가하고 \\(\\hat{\\mathbf{z}^{bg}\\)을 얻고 \\(D\\)으로 디코딩하여 \\(\\hat{\\mathbf{x}^{bg}\\)을 얻는다. 우리는 이 단계에서 새로운 내용을 페인트하는 것을 목표로 하지 않기 때문에 \\(I\\), \\(\\mathbf{m}\\) 및 빈 프롬프트로 \\(\\epsilon_{\\psi}\\)을 컨디셔닝한다.\n' +
      '\n' +
      '우리는 이제 이 입력들을 이용하여 \\(\\mathcal{L}_{HiFA}\\)을 계산한다(식 2 참조). 다음으로 \\(\\mathcal{L}_{recon}\\)과 \\(\\mathcal{L}_{vgg}\\)을 \\(\\mathbf{h}\\)에 계산(그림 3 참조)하여 증류된 신경장 \\(\\theta_{bg}\\)을 배경의 정확한 재구성으로 유도한다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{recon}=MSE(\\mathbf{x}^{bg}\\odot\\mathbf{h},I\\odot\\mathbf{h}) \\tag{3}\\]\n' +
      '\n' +
      '\\[\\mathcal{L}_{vgg}=MSE(vgg_{16}(\\mathbf{x}^{bg}\\odot\\mathbf{h}),vgg_{16}(I\\odot\\mathbf{h}))\\tag{4}\\.\n' +
      '\n' +
      '이 단계는 RAM3D가 (도 12에 도시된 바와 같이) 배경을 올바르게 인페인팅하도록 보장하는 데 중요하다. [47]과 같은 공식을 채택하여, 우리는 미리 훈련된 깊이 추정기로부터 기하학을 이용하여 깊이 규칙화 \\(\\mathcal{L}_{depth}\\)을 계산한다. 요약하면, 지우기 단계 동안의 총 손실은:\n' +
      '\n' +
      '\\mathcal{L}_{Erase}=\\mathcal{L}_{HiFA}+\\lambda_{recon}\\mathcal{L}_{recon}+\\lambda_{vgg}\\mathcal{L}_{vgg}+\\lambda_{depth}\\mathcal{L}_{depth}\\tag{5}\\mathcal{L}_{depth}\n' +
      '\n' +
      '### Replace stage\n' +
      '\n' +
      '두 번째 단계에서는 색칠된 장면에 \\(y_{replace}\\)으로 기술된 새로운 객체를 추가하는 것을 목표로 한다. 이를 위해 전경신경장\\(\\theta_{fg}\\)을 최적화하여 \\(\\mathbf{x}^{fg}\\)을 렌더링하고, 이를 \\(\\mathbf{x}^{bg}\\)과 합성하여 \\(\\mathbf{x}\\)을 형성한다. \\(\\theta_{bg}\\)과 달리\n' +
      '\n' +
      '도 3: 마스킹된 영역(파란색)은 LDM에 대한 컨디셔닝 신호로서, 인페인팅될 영역을 나타낸다. Halo 영역\\(\\mathbf{h}\\) (green)은 Erase 단계에서 RAM3D에 의해 부피적으로 렌더링된다. 이들 2개의 영역의 합집합은 _Bubble-NeRF_ 영역이고, 나머지 픽셀은 입력 이미지(적색)로부터 샘플링된다.\n' +
      '\n' +
      '도 2: RAM3D **Erase** 및 **Replace** 단계의 개요.\n' +
      '\n' +
      'Erase stage, \\(\\theta_{fg}\\)는 배경 장면의 재구성을 추구하지 않고, 대신에 \\(\\mathbf{m}\\)의 내부에 위치한 LDM-인페인팅된 콘텐츠만을 추구한다. 따라서 Replace 단계에서 RAM3D는 \\(\\mathbf{h}\\)이 교차하는 후광선을 고려하지 않고, \\(\\mathbf{m}\\)이 교차하는 광선만을 고려한다 (그림 4). 이 렌더링된 픽셀들은 마스킹된 영역에 2D 어레이로 배열되어 전경 이미지\\(\\mathbf{x}^{fg}\\), 마스킹되지 않은 픽셀들은 RGB 값이 0이 할당된다. 누적된 밀도는 유사하게 전경 알파 맵(A\\), 마스킹되지 않은 픽셀들은 알파 값이 0이 할당된다. 이제 알파 블렌딩을 사용하여 전경\\(\\mathbff{x}^{\\mathbf{bg}\\)과 배경\\(\\mathbff{x}^{\\mathbf{bg}\\)을 합성한다.\n' +
      '\n' +
      '\\[\\mathbf{x}=A\\odot\\mathbf{x}^{\\mathbf{fg}}+(1-A)\\odot\\mathbf{x}^{\\mathbf{bg}} \\tag{6}\\]\n' +
      '\n' +
      '합성된 결과 \\(\\mathbf{x}\\)를 이용하여 기존과 같이 \\(\\mathcal{L}_{HiFA}\\)을 계산하지만, 인페인팅을 위한 새로운 객체를 지정하는 prompt \\(y_{replace}\\)으로 현재 조건 \\(\\epsilon_{\\psi}\\)을 계산한다. 우리는 더 이상 다른 손실을 요구하지 않기 때문에 \\(\\lambda_{recon},\\lambda_{regg},\\lambda_{depth}\\)를 0으로 설정했다.\n' +
      '\n' +
      '지우기 단계는 이미 우리에게 좋은 배경을 제공하기 때문에, 이 단계에서는 전경 객체만을 표현하면 된다. 전경/배경 분할을 유도하기 위해 k차 훈련 단계마다 랜덤 샘플링된 RGB 강도로 상수 값 RGB 텐서로 \\(\\mathbf{x}^{\\mathbf{bg}}\\)을 대체한다. 이것은 새로운 물체에 대한 밀도만을 포함하도록 \\(\\theta_{fg}\\)의 증류를 안내한다; 배경 위에 나타나는 가짜 부유물을 피하기 위한 임계 증강이다(도 11 참조).\n' +
      '\n' +
      '최종 NeRF 훈련\n' +
      '\n' +
      '인페인팅된 배경과 객체가 생성되면, 새로 생성된 객체와 모든 학습 시점에 대해 인페인팅된 배경 영역을 합성하여 새로운 다시점 데이터셋을 생성할 수 있다. 그런 다음 선택한 변형과 프레임워크를 사용하여 새로운 NeRF를 훈련하여 새로운 뷰 합성에 사용할 수 있는 편집된 장면의 3D 표현을 생성한다.\n' +
      '\n' +
      '## 5 Results\n' +
      '\n' +
      '본 논문에서는 앞으로 향하는 장면부터 360\\({}^{\\circ}\\) 장면까지 복잡도가 다양한 실제 3D 장면에 대한 실험을 수행하였다. 전방 장면에 대해 SPIn-NeRF 데이터세트 [29]와 NeRF [26]의 양치류 장면에서 조각상과 레드넷 장면에 대한 결과를 보여준다. 360\\({}^{\\circ}\\) 장면에 대하여, 우리는 Mip-NeRF 360\\({}^{\\circ}\\)[4]의 정원 장면에서 결과를 보여준다. 각 데이터 세트에서, 다양한 종류의 편집된 3D 장면들을 생성하기 위해 다양한 종류의 \\(y_{\\text{replace}}\\)으로 RAM3D를 학습한다. 보다 질적인 결과는 프로젝트 페이지를 참고하시기 바랍니다.\n' +
      '\n' +
      '트레이닝 세부사항 각각은 512와 동일한 최단 이미지 측면 길이(높이)를 갖도록 다운샘플링되어, LDM 인페인터에 제공되는 정사각형 작물은 입력 이미지의 전체 높이를 포함한다. 양치류 장면은 2의 다운샘플 계수를 갖는 데이터세트 이미지 내에서 512개의 작은 이미지 크롭을 샘플링하는 예외로서, 입력 이미지의 해상도와 크롭에 대한 세부 사항과 기타 구현 세부 사항이 부록 B.4 및 B.6에 포함된다.\n' +
      '\n' +
      '### Qualitative Comparisons\n' +
      '\n' +
      '도 5, 도 6 및 도 7은 Instruct-NeRF2NeRF[10], Blended-NeRF[2] 및 Mirzaei et al. [28]에 의한 작업에 대한 정성적 비교를 각각 나타낸다.\n' +
      '\n' +
      '도 5에 도시된 바와 같이, Instruct-NeRF2NeRF는 새로운 객체가 원래의 객체와 상당히 다른 경우(예를 들어, 도 5 초 및 세 번째 열의 파인애플 또는 체스 피스로 중심 피스를 교체함)에서 고전한다. 더 중요한 것은, Instruct-NeRF2NeRF는 편집이 로컬이어야 하는 경우에도 장면의 글로벌 구조를 상당히 변화시킨다(예를 들어, 센터피스만을 파인애플로 대체한다). 마지막으로, 본 방법은 Instruct-NeRF2NeRF가 할 수 없는 반면, 장면으로부터 객체를 완전히 제거할 수 있다는 점에 유의한다(도 5 첫 번째 열).\n' +
      '\n' +
      '그림 6은 Blended-NeRF와의 질적 비교를 보여준다. 우리의 방법은 나머지 장면과 훨씬 더 잘 어우러지는 훨씬 더 사실적이고 상세한 객체를 생성한다. 한편, Blended-NeRF는 주변 장면을 고려하지 않고 완전히 새로운 대상을 합성하는 것에만 초점을 맞추고 있다. 따라서 합성된 물체는 나머지 장면에서 포화되고 기이해 보인다. 또한, CLIP[37]과 NeRF의 메모리 제약으로 인해 Blended-NeRF는 우리보다 4시간 작은 영상에서만 동작한다(2016\\(\\times\\)1512 vs. 504\\(\\times\\)378.\n' +
      '\n' +
      'Mirzaei et al. [28]은 그들의 코드를 공개적으로 공유하지 않았기 때문에, 우리는 그림 7에서 그들의 논문에서 채택한 이미지를 보고한다. 우리의 방법은 전경 객체와 배경 객체 사이의 그림자와 같은 더 복잡한 조명 효과를 처리하면서 유사한 객체 교체 결과를 달성한다.\n' +
      '\n' +
      '그림 4: 교체 단계: RAM3D는 마스킹된 픽셀(파란색으로 표시됨)을 부피로 렌더링하여 \\(\\mathbf{x}^{fg}\\)을 제공한다. 그 결과를 \\(\\mathbf{x}^{bg}\\)과 합성하여 합성영상 \\(\\mathbf{x}\\)을 형성한다.\n' +
      '\n' +
      '### Quantitative Results\n' +
      '\n' +
      '3D 장면 편집은 매우 주관적인 작업이다. 따라서 우리는 주로 다양한 유형의 질적 결과와 비교를 보여주고 독자들에게 더 많은 결과를 위해 프로젝트 페이지를 참조할 것을 권장한다. 그러나 본 논문에서는 Table 1과 같이 Instruct-NeRF2NeRF를 따르고, 2가지 보조 메트릭인 CLIP Text-Image Direction Similarity와 CLIP Direction consistency를 보고하고, 다양한 프롬프트를 위해 두 데이터세트 정원과 양치류에서 객체 대체 작업에 대해 Instruct-NeRF2NeRF와 Blended-NeRF를 정량적으로 비교한다.\n' +
      '\n' +
      '표 1은 우리의 방법이 CLIP 텍스트-이미지 방향 유사성에 대해 가장 높은 점수를 달성함을 보여준다. 흥미롭게도 Blended-NeRF는 생성된 객체와 대상 텍스트 프롬프트가 있는 이미지의 CLIP 임베딩 간의 유사성을 직접 최적화하지만 여전히 우리의 방법보다 낮은 점수를 달성한다. 시간적 일관성 손실을 측정하는 방향 일관성 점수의 경우, Instruct-NeRF2NeRF가 완전히 실패하는 편집 프롬프트에 대해 우리의 방법보다 높은 점수를 받는 것을 관찰한다(도 5 참조). 예를 들어, 정원 데이터세트 내의 편집 "파인애플"에 대해, Instruct-NeRF2NeRF는 파인애플의 세부사항을 생성하는데 실패할 뿐만 아니라 배경에서 고주파 세부사항을 제거하여, 흐릿한 배경을 생성한다. 우리는 이것이 편집이 성공적이지 않은 경우에도 일관성 점수를 증가시킨다고 가정한다. 따라서 더 자세한 내용은 독자에게 프로젝트 비디오의 비교를 참조합니다.\n' +
      '\n' +
      '물체교체를 넘어\n' +
      '\n' +
      'Object를 제거하여 새로운 콘텐츠로 장면을 수정한다. ReplaceAnything3D는 새로운 전경 객체를 장면에 추가하기 전에 객체 제거 및 배경 인페인팅을 수행한다. 물체 제거는 우리 작업의 초점이 아니지만 여기에서 다른 NeRF-인페인팅 방법, 특히 SPin-NeRF와 질적 비교를 보여준다.\n' +
      '\n' +
      '도 5: Instruct-NeRF2NeRF와의 비교.\n' +
      '\n' +
      '그림 6: 객체 교체를 위한 Blended-NeRF와의 질적 비교. 우리의 방법은 더 높은 품질로 결과를 생성하고 더 사실적인 조명과 디테일을 캡처합니다.\n' +
      '\n' +
      '그림 7: 객체 교체에 대한 [28]에 의한 참조 유도 인페인팅과의 질적 비교.\n' +
      '\n' +
      '[29] and work by Mirzaei et al. [27], to show the effectiveness of our Erase stage. 이 두 방법 모두 그림 8과 같이 정방향 장면에서만 작동한다는 점에 유의한다. 한편 Instruct-NeRF2NeRF와 같은 360\\({}^{\\circ}\\) 장면과 함께 작동하는 다른 장면 편집 기술은 그림 5와 같이 객체 제거가 불가능하다.\n' +
      '\n' +
      '본 논문에서 제안하는 방법은 장면 내의 객체를 제거하고 대체하는 것 외에도 사용자의 입력 마스크를 기반으로 새로운 객체를 추가할 수 있다. 그림 9는 사실적인 조명과 그림자를 가진 완전히 새로운 객체들이 생성되어 현재의 3D 장면에 합성될 수 있음을 보여준다. 특히, 그림 9 하단 행과 같이, 우리의 방법은 사실적인 장면 외관과 멀티 뷰 일관성을 유지하면서 동일한 장면에 둘 이상의 객체를 추가할 수 있다.\n' +
      '\n' +
      '개인화된 콘텐츠를 이용한 장면 편집\n' +
      '\n' +
      '텍스트 프롬프트 외에도 RAM3D는 사용자가 자신의 자산을 3D 장면에 대체하거나 추가할 수 있게 한다. 이는 먼저 미리 훈련된 인페인팅 확산 모델을 드림부스를 이용하여 대상 객체의 다중 이미지로 미세 조정함으로써 달성된다[41]. 그 후, 결과적인 미세 조정 모델은 RAM3D에 통합되어 3D 장면들에서 객체 교체를 가능하게 한다. 도 10에 도시된 바와 같이, 미세 조정 후에, RAM3D는 타겟 오브젝트를 새로운 3D 장면들에 효과적으로 대체하거나 추가할 수 있다.\n' +
      '\n' +
      '### Ablation studies\n' +
      '\n' +
      '우리는 우리의 방법과 훈련 전략의 효과를 입증하기 위해 일련의 절제 연구를 수행한다. 도 1에 도시된 바와 같이,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c|} \\hline \\hline Prompts & CLIP Text-Image Direction Similarity \\(\\uparrow\\) & \\multicolumn{2}{c|}{CLIP Direction Consistency \\(\\uparrow\\)} \\\\ \\hline  & Ours & Instruct-NeRF2NeRF & Ours & Instruct-NeRF2NeRF \\\\ \\hline Pineapple & \\(\\mathbf{0.2041}\\) & \\(0.0661\\) & \\(0.9590\\) & \\(\\mathbf{0.9660}\\) \\\\ Chess & \\(\\mathbf{0.1200}\\) & \\(0.0061\\) & \\(0.9457\\) & \\(\\mathbf{0.9705}\\) \\\\ \\hline  & Ours & BlendedNerf & Ours & BlendedNerf \\\\ \\hline Mushroom & \\(\\mathbf{0.0928}\\) & \\(0.0535\\) & \\(\\mathbf{0.9781}\\) & \\(0.9748\\) \\\\ Strawberry & \\(\\mathbf{0.3165}\\) & \\(0.2224\\) & \\(\\mathbf{0.9808}\\) & \\(0.9698\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 우리는 (위) 정원, (중간) 얼굴, (아래) 양치식물 등 다양한 데이터 세트에 대한 CLIP 기반 메트릭을 계산한다.\n' +
      '\n' +
      '그림 8: 객체 제거와 배경 인페인팅 작업에 대한 질적 비교. 물체 제거는 ReplaceAnything3D의 주요 초점은 아니지만, 우리의 방법은 최첨단 방법으로 경쟁적인 결과를 얻을 수 있다.\n' +
      '\n' +
      '그림 9: 사용자 정의 마스크가 주어지면, Anyplace3D는 나머지 장면과 조화를 이루는 완전히 새로운 객체를 추가할 수 있다. RAM3D는 구성 구조로 인해 사실적인 외관, 조명 및 멀티 뷰 일관성(하단 행)을 유지하면서 3D 장면에 여러 객체를 추가할 수 있다.\n' +
      '\n' +
      '11, 우리는 우리의 구성 전경/배경 구조와 배경 증강 훈련 전략의 이점을 보여준다. 구체적으로, 배경과 새로운 객체(결합 \\(\\theta_{bg}\\) 및 \\(\\theta_{fg}\\)을 모델링하기 위해 모놀리식 NeRF를 사용하여 RAM3D 버전을 트레이닝한다. 즉, 이 모델은 별도의 지우기 및 교체 단계가 아닌 하나의 단일 단계에서 장면을 편집하도록 훈련된다. 그림 11-a의 코기 머리 뒤의 흐릿한 울타리에서 분명히 알 수 있듯이 이 경우 더 낮은 품질의 배경 재구성을 관찰한다.\n' +
      '\n' +
      '또한 전경 객체를 배경과 분리하는 데 무작위 배경 증강을 사용하는 이점을 입증한다(섹션 4.3 참조). 이러한 증강이 없으면, 모델은 전경 및 배경 알파 맵을 정확하게 분리할 수 없어, 비디오 상에서 볼 때 특히 눈에 띄는 흐릿한 배경 및 플로터를 생성한다(도 11-b). 대조적으로, 배경 증강으로 훈련된 우리의 전체 합성 모델은 전경 및 배경을 성공적으로 분리하여 전체 장면에 대해 날카로운 결과를 생성한다(도 11-c).\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 12에서는 지우기 훈련 단계에서 헤일로 지역 감독의 중요성을 보여준다. 이 모델이 없으면, 우리의 모델은 중요한 주변 공간 정보가 부족하기 때문에 배경 장면을 성공적으로 생성할 수 없다.\n' +
      '\n' +
      '### Discussion\n' +
      '\n' +
      '이 방법은 장면 편집을 위한 지우기 및 바꾸기 접근법으로 인해 원래 객체에서 중요한 구조적 정보를 제거할 수 있다. 따라서 본 논문에서 제안하는 방법은 객체의 외형이나 기하학(예를 들어 청동상을 금으로 바꾸는 것)과 같은 속성만을 수정하는 편집 작업에는 적합하지 않다. 또한 ReplaceAnything3D는 텍스트-이미지 모델 증류 기술을 기반으로 하므로 야누스 다중 얼굴 문제와 같은 이러한 방법과 유사한 아티팩트를 겪는다.\n' +
      '\n' +
      '본 논문에서는 NeRF나 Instant-NGP와 같은 암시적 3차원 장면 표현을 사용한다. 향후 연구를 위해, 본 방법은 DreamGaussian [46]과 유사한 3D Gaussian splats [15]와 같은 다른 표현으로도 확장될 수 있다. 흥미로운 향후 탐색 방향은 장면 편집을 위한 보다 세밀한 제어를 가능하게 하는 디엔탠글링 기하학 및 외형을 포함하고, 프롬프트-디바이어싱 방법[12] 또는 멀티뷰 데이터세트에서 미리 훈련된 모델[35, 43]을 채택함으로써 다중-얼굴 문제를 해결하고, 로레인 등[23]과 유사하게 객체 교체 프로세스를 가속화하기 위한 상각 모델을 개발하는 것을 포함한다.\n' +
      '\n' +
      '도 11: 프롬프트 _"A corgi"_를 위한 조각상 장면에서, 3개의 RAM3D 변형들에 대한 절제 결과들. RGB 샘플은 왼쪽 상단 모서리에 축적된 NeRF 밀도(알파 맵)로 표시된다. 버블 렌더링 영역은 점선으로 표시된다. a) 전경과 배경을 모두 포함하는 단일 장면 표현입니다. b) 임의의 배경 증강이 없는 구성 장면 모델. c) 우리의 전체 모델.\n' +
      '\n' +
      '도 12: 소거 스테이지에 대한 동상 장면에서 훈련된 2개의 RAM3D 변이체에 대한 절제 결과. a) 인페인팅 마스크를 둘러싼 후광 영역에 대한 감독 없이 훈련한다. 훈련 목표가 모호하고 버블-NeRF 모델이 흐릿한 구름으로 무너진다. b) Bubble-NeRF를 둘러싸고 있는 halo 영역에 대한 halo 손실(\\(\\mathcal{L}_{recon}\\)과 \\(\\mathcal{L}_{ugg}\\)을 추가하면 폐색 물체 근처를 통과하는 광선에서 관찰된 바와 같이 실제 배경 쪽으로 \\(\\theta_{bg}\\)의 증류를 유도한다. RAM3D는 이제 배경 장면을 정확하게 칠할 수 있다.\n' +
      '\n' +
      '그림 10: 사용자는 미세 조정된 RAM3D를 사용하여 자신의 자산을 교체하거나 추가함으로써 3D 장면을 개인화할 수 있다. 이를 위해 먼저 대상 객체(왼쪽)의 5개의 이미지로 인페인팅 확산 모델을 미세 조정한 다음 RAM3D와 결합하여 객체 교체 및 추가를 사용자 정의 콘텐츠로 수행한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 장면 내에서 특정 객체를 대체할 수 있는 텍스트 기반의 3차원 장면 편집 방법인 ReplaceAnything3D를 제안한다. 지오메트리나 모양과 같은 기존 객체 특성을 수정하는 다른 방법과 달리 지우기 및 바꾸기 접근법은 상당히 다른 내용으로 객체를 효과적으로 대체할 수 있다. 또한, 이 방법은 사실적인 외관과 다시점 일관성을 유지하면서 새로운 객체를 제거하거나 추가할 수 있다. 본 논문에서는 ReplaceAnything3D(ReplaceAnything3D)의 효용성을 전방향 및 360^{\\circ}\\(360^{\\circ}\\) 장면을 포함한 다양한 실감형 3D 장면에서 입증한다. 우리의 접근 방식은 원활한 객체 교체를 가능하게 하여 VR/MR, 게임 및 영화 제작에서 향후 응용 프로그램을 위한 강력한 도구가 됩니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended Diffusion for Text-driven Editing of Natural Images. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 18187-18197, New Orleans, LA, USA, 2022. IEEE.\n' +
      '* [2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended Latent Diffusion, 2023. arXiv:2206.02779 [cs].\n' +
      '* [3] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 5855-5864, 2021.\n' +
      '* [4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. _CVPR_, 2022.\n' +
      '* [5] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. _ICCV_, 2023.\n' +
      '* [6] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. InstructPix2Pix: Learning to Follow Image Editing Instructions, 2023. arXiv:2211.09800 [cs].\n' +
      '* [7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Anirudinda Kembhavi, and Ali Farhadi. Objaresse: A universe of annotated 3d objects. _arXiv preprint arXiv:2212.08051_, 2022.\n' +
      '* [9] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. _arXiv preprint arXiv:2103.10380_, 2021.\n' +
      '* [10] Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions, 2023. arXiv:2303.12789 [cs].\n' +
      '* [11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-Prompt Image Editing with Cross Attention Control, 2022. arXiv:2208.01626 [cs].\n' +
      '* [12] Susung Hong, Donghoon Ahn, and Seungryong Kim. Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation, 2023. arXiv:2303.15413 [cs].\n' +
      '* [13] Clement Jambon, Bernhard Kerbl, Georgios Kopanas, Stavros Diolatzis, Thomas Leimkuhler, and George" Drettakis. Nerfshop: Interactive editing of neural radiance fields". _Proceedings of the ACM on Computer Graphics and Interactive Techniques_, 6(1), 2023.\n' +
      '* [14] Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy J. Mitra. Relu fields: The little non-linearity that could. _Transactions on Graphics (Proceedings of SIGGRAPH)_, 41(4):13:1-13:8, 2022.\n' +
      '* [15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM Transactions on Graphics_, 42(4), 2023.\n' +
      '* [16] Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations (ICLR)_, 2015.\n' +
      '* [17] Quewei Li, Feichao Li, Jie Guo, and Yanwen Guo. Uhdnerf: Ultra-high-definition neural radiance fields. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 23097-23108, 2023.\n' +
      '* [18] Tianle Li, Max Ku, Cong Wei, and Wenhu Chen. Dreamedit: Subject-driven image editing. _Transactions on Machine Learning Research_, 2023.\n' +
      '* [19] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In _IEEE International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [20] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023.\n' +
      '* [22] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, and Bryan Russell. Editing conditional radiance fields. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [23] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized text-to-3d object synthesis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 17946-17956, 2023.\n' +
      '* [24] Luca Medeiros. Language segment anything. GitHub repository, 2021.\n' +
      '*[25]*[25] Gal Metzer, Elad Richardson, or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures, 2022. arXiv:2211.07600 [cs].\n' +
      '* [26] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _ECCV_, 2020.\n' +
      '* [27] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A. Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G. Derpanis, and Igor Gilitschenski. Reference-guided controllable inpainting of neural radiance fields. In _ICCV_, 2023.\n' +
      '* [28] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A. Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G. Derpanis, and Igor Gilitschenski. Reference-guided Controllable Inpainting of Neural Radiance Fields, 2023. arXiv:2304.09677 [cs].\n' +
      '* [29] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Konstantinos G. Derpanis, Jonathan Kelly, Marcus A. Brubaker, Igor Gilitschenski, and Alex Levinshtein. SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural Radiance Fields, 2023. arXiv:2211.12254 [cs].\n' +
      '* [30] Ron Mokady, Amir Hertz, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. Null-text Inversion for Editing Real Images using Guided Diffusion Models, 2022. arXiv:2211.09794 [cs].\n' +
      '* [31] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models, 2023. arXiv:2307.02421 [cs].\n' +
      '* [32] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Trans. Graph._, 41(4):102:1-102:15, 2022.\n' +
      '* [33] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [34] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D Diffusion, 2022. arXiv:2209.14988 [cs, stat].\n' +
      '* [35] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard Ghanem. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. _arXiv preprint arXiv:2306.17843_, 2023.\n' +
      '* [36] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard Ghanem. Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors, 2023. arXiv:2306.17843 [cs].\n' +
      '* [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _ICML_, pages 8748-8763, 2021.\n' +
      '* [38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _ArXiv_, abs/2204.06125, 2022.\n' +
      '* [39] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 12179-12188, 2021.\n' +
      '* [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, 2022.\n' +
      '* [41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aherman. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation, 2023. arXiv:2208.12242 [cs].\n' +
      '* [42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* [43] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. MVDream: Multi-view Diffusion for 3D Generation, 2023. arXiv:2308.16512 [cs].\n' +
      '* [44] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In _International Conference on Learning Representations_, 2015.\n' +
      '* [45] Hyeonseop Song, Seokhun Choi, Hoseok Do, Chul Lee, and Taehyeong Kim. Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields, 2023. arXiv:2308.11974 [cs].\n' +
      '* [46] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. _arXiv preprint arXiv:2309.16653_, 2023.\n' +
      '* [47] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior, 2023. arXiv:2303.14184 [cs].\n' +
      '* [48] Can Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Clip-nerf: Text-and-image driven manipulation of neural radiance fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3835-3844, 2022.\n' +
      '* [49] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 12619-12629, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      'Fern 장면에서 실험을 통해, 우리는 장면의 비어 있는 영역을 덮고 데이터 세트 이미지에 걸쳐 일관된 위치에 사용자 주석이 지정된 마스크를 생성한다.\n' +
      '\n' +
      '### 데노이저 입력을 크롭하는 단계\n' +
      '\n' +
      'LDM 디노이징 U-net은 512(\\times\\)512 크기의 입력 영상을 취하지만, RAM3D 모델 출력은 입력 장면과 동일한 해상도를 가지며, 이는 비정방형이 될 수 있다. 크기 호환성을 보장하기 위해 RAM3D 출력을 512(\\times\\)512로 자르고 크기를 조정한 후 데노이저에 전달해야 한다(그림 2). 불상 및 정원 장면의 경우 모든 이미지를 높이 512로 조정하고 항상 전체 객체 마스크 영역을 포함하는 512\\(\\times\\)512의 중심 크롭을 취한다. 레드넷 장면의 경우 객체 마스크는 이미지의 왼쪽에 위치하므로 크롭을 위해 가장 왼쪽 512 픽셀을 선택한다.\n' +
      '\n' +
      '양치류 장면에 대해, 입력 이미지들은 작은 사용자 제공 마스크들로 주석이 달린다. 우리는 이전 접근법이 LDM의 디노이저에 너무 작은 마스크 영역을 제공한다는 것을 발견했다. 이 경우, 2016\\(\\times\\)1512의 해상도로 2배 다운샘플링된 원본 데이터셋을 이용하여 RAM3D를 학습하고, 객체 마스크 주변의 직사각형 크롭을 선택한다. 마스크 영역을 덮고 있는 가장 단단한 직사각형 작물을 계산한 다음 중심부를 그대로 유지하면서 작물 영역 높이와 너비를 두 배로 늘린다. 마지막으로 작물 영역의 높이와 너비를 최대치로 늘립니다.\n' +
      '\n' +
      '그림 13: 우리의 방법 RAM3D(마지막 행)와 순진한 2D 기준선 방법(세 번째 행) 사이의 정성적 비교. 이는 각각의 입력 영상이 독립적으로 처리되기 때문에 서로 매우 다양하기 때문이다(두 번째 행).\n' +
      '\n' +
      '높이 및 너비를 사용하여 인페인팅 마스크 영역을 포함하는 사각 작물을 얻을 수 있습니다. 이 작물을 RAM3D 출력에 적용한 후 512\\(\\times\\)512로 보간한 후 이전과 같이 진행한다.\n' +
      '\n' +
      '### Loss functions\n' +
      '\n' +
      '지우기 훈련 단계에서는 인페인팅 마스크(그림 12)에 가까운 픽셀을 통해 재구성 손실 구배를 역전파하여 배경 장면을 성공적으로 재구성할 필요가 있다. 따라서 Halo 영역(B.3, Fig. 3) 내부의 픽셀을 추가로 렌더링하고, 입력 영상의 해당 영역과 함께 복원 손실\\(\\mathcal{L}_{recon}\\)과 지각 손실\\(\\mathcal{L}_{vgg}\\)을 계산한다. 입력 영상에서 마스킹된 영상 콘텐츠는 Halo 영역 내에 속하지 않으며, 따라서 \\(\\mathcal{L}_{recon}\\) 및 \\(\\mathcal{L}_{vgg}\\)은 장면 배경에 대한 감독만을 제공한다. 재구성 손실을 위해 입력 영상과 RAM3D의 RGB 출력 사이의 평균 제곱 오차를 사용한다. 지각적 손실을 위해 사전 훈련된 VGG-16 네트워크의 계층 8에서 계산된 특징 간의 평균 제곱 오차를 사용한다[44]. 두 경우 모두 인페인팅 마스크의 외관에 손실이 계산되고 헤일로 영역을 통해 역전파된다. Replace 훈련 단계에서 Zhu와 Zhuang[57]에 이어 렌더링된 출력\\(\\mathbf{x}\\)과 LDM이 제거된 출력\\(\\hat{\\mathbf{x}\\) 사이에 \\(\\mathcal{L}_{BGT_{+}}\\) 손실을 적용하고, LDM 이미지에 대한 NeRF-장면 가중치를 업데이트하기 위한 기울기를 얻는다(그림 2의 HiFA 손실, eqn 11[57] 참조). 이 위상 동안 다른 손실 함수가 적용되지 않으므로 손실 구배는 인페인팅 마스크의 내부의 픽셀에만 백프로포그팅된다. 메모리 및 속도 효율을 위해, RAM3D는 이 단계에서 인페인팅 마스크 내부에 놓인 픽셀들을 렌더링하고(도 4), 그렇지 않으면 대응하는 입력 이미지로부터 RGB 값들을 직접 샘플링한다.\n' +
      '\n' +
      '마지막으로 Tang et al. [47]에 이어, 우리는 NeRF-렌더링된 깊이 맵과 LDM-디노이징된 RGB 출력에서 계산된 단안 깊이 추정치 사이의 음의 피어슨 상관 계수를 사용하여 깊이 정규화를 적용한다. 깊이 추정치는 기성 모형[39]을 이용하여 구한다. 이 손실은 렌더링된 모든 픽셀, 즉 그림 3에 표시된 인페인팅 마스크와 헤일로 영역의 결합을 통해 역전 촉진된다. 우리는 교체 단계에서 이 정규화를 적용하지 않는다. 요약하면, Replace 단계에 대한 전체 손실 함수는:\n' +
      '\n' +
      '\\mathcal{L}_{total}=\\mathcal{L}_{BGT_{+}}+\\lambda_{depth}\\mathcal{L}_{depth}+\\lambda_{recon}\\mathcal{L}_{recon}+\\lambda_{vgg}\\mathcal{L}_{vgg}\\tag{7}\\text{\n' +
      '\n' +
      '(\\lambda_{recon}=3,\\lambda_{vgg}=0.03,\\lambda_{depth}=3).\n' +
      '\n' +
      '우리는 1e-3의 학습률을 갖는 Adam optimiser[16]을 사용하였으며, Instant-NGP 해시 인코딩 파라미터에 대해 10으로 확장하였다.\n' +
      '\n' +
      '기타 훈련 세부사항\n' +
      '\n' +
      '[34, 57]에 이어, 우리는 분류기 없는 유도(CFG)가 LDM 디노이저로부터 증류 샘플링을 위한 효과적인 기울기를 얻는 데 중요하다는 것을 발견했다. 우리는 교체 단계에서 30, 지우기 단계에서 7.5의 CFG 척도를 사용한다. 또한, HiFA 잡음 수준 스케줄은 \\(t\\_min\\) = 0.2, \\(t\\_max\\) = 0.98이고, 확률적 하이퍼파라미터 \\(\\eta\\) = 0을 사용한다. \\(\\mathcal{L}_{BGT_{+}}\\) 손실의 정의에서, 우리는 HiFA를 따르고 0.1의 \\(\\lambda_{rgb}\\) 값을 선택한다. RAM3D 복사 함수는 128개의 coarse-to-fine 샘플링 전략을 사용하여 렌더링한다. 교체 훈련 단계에서는 3차 훈련 단계마다 합성 배경 이미지와 무작위로 선택된 일반 RGB 이미지를 교체한다. 도 11에 도시된 바와 같이, 이 단계는 전경과 배경의 깨끗한 분리를 달성하는 데 중요하다.\n' +
      '\n' +
      '우리는 단일 32GB V100 GPU에서 약 12시간이 소요되는 지우기 및 교체 훈련 단계에서 20,000개의 훈련 단계를 위해 RAM3D를 훈련한다. 리플레이스 스테이지 트레이닝의 출력은 가시 영역 상의 입력 장면 이미지들과 매칭되고, 뷰들에 걸쳐 일관되는 마스킹된 영역의 내부 상의 인페인팅된 콘텐츠를 포함하는 멀티뷰 이미지들의 세트이다. 새로운 뷰를 얻기 위해 RAM3D 편집 이미지와 원본 장면 카메라를 학습 데이터 세트로 사용하여 표준 새로운 뷰 합성 방법을 학습한다. LLFF 장면(statue, fern, red-net scene)에는 nerf-pytorch[54], 정원 장면에는 Gaussian Splatting[15]를 사용하였다.\n' +
      '\n' +
      '그림 14: 객체 추가에 대한 DreamEditor [18]와의 질적 비교. 그림은 원래 드림 에디터 용지에서 각색되었습니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>