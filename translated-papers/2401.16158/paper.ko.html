<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 모바일-에이전트: 시지각성을 갖는 자율적 멀티모달 모바일 디바이스 에이전트\n' +
      '\n' +
      ' 중양왕({}^{1}\\) 해양수({}^{2}\\) 지아보예({}^{2}\\) 명양({}^{2}\\)\n' +
      '\n' +
      'Weizhou Shen\\({}^{2}\\) Ji Zhang\\({}^{2}\\) Fei Huang\\({}^{2}\\) Jitao Sang\\({}^{1}\\)\n' +
      '\n' +
      '{준양왕, jtsang}@bjtu.edu.cn, {shuofeng.xhy, ym119608}@alibaba-inc.com\n' +
      '\n' +
      '베이징자오동대학교 알리바바그룹\n' +
      '\n' +
      '알리바바 그룹.교신저자 인턴쉽에서 수행한 작업\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'MLLM(Multimodal Large Language Models) 기반의 모바일 디바이스 에이전트가 대중적인 응용으로 자리잡고 있다. 본 논문에서는 자율 멀티모달 모바일 디바이스 에이전트인 Mobile-Agent를 소개한다. 모바일-에이전트는 먼저 시각적 인식 도구를 활용하여 앱의 프론트엔드 인터페이스 내에서 시각적 요소와 텍스트 요소를 정확하게 식별하고 위치를 파악한다. 지각된 시각 맥락을 바탕으로 복잡한 연산 작업을 자율적으로 계획 및 분해하고, 연산을 통해 모바일 앱을 단계별로 탐색한다. 모바일 에이전트는 앱 또는 모바일 시스템 메타데이터의 XML 파일에 의존하는 기존 솔루션과 달리 비전 중심 방식으로 다양한 모바일 운영 환경에 걸쳐 더 큰 적응성을 허용하여 시스템별 커스터마이징의 필요성을 제거한다. 모바일 에이전트의 성능을 평가하기 위해 모바일 디바이스 동작을 평가하기 위한 벤치마크인 Mobile-Eval을 도입하였다. Mobile-Eval을 기반으로 모바일 에이전트에 대한 종합적인 평가를 수행하였다. 실험 결과는 모바일 에이전트가 뛰어난 정확도와 완료율을 달성했음을 나타낸다. 멀티 앱 작동과 같은 까다로운 지시에도 불구하고 모바일 에이전트는 여전히 요구 사항을 완료할 수 있다. 코드 및 모델은 [https://github.com/X-PLUG/MobileAgent](https://github.com/X-PLUG/MobileAgent)에서 오픈 소스될 것이다.\n' +
      '\n' +
      '도 1: 모바일-에이전트는 모바일 디바이스를 동작시키기 위한 자율 에이전트이다. 사용자 지시에 따라 모바일 에이전트는 요구 사항을 완료하기 위해 일련의 작업을 계획할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'LLM 기반 에이전트 Li et al. (2023); Liu et al. (2023); Shen et al. (2023); Wu et al. (2023); Yang et al. (2023); Shen et al. (2024); Yang et al. (2023); Hong et al. (2023); Yang et al. (2023); Yang et al. (2023); Yang et al. Multimodal Large Language Models (MLLM) Liu et al. (2023); Zhu et al. (2023); Ye et al. (2023); Dai et al. (2023); Liu et al. (2023); Chen et al. (2023); Ye et al. (2023); Ye et al. (2023); Bai et al. (2023); Lin et al. (2023); Lin et al. (2023); Lin et al. (2023)은 빠르게 발전하고 현저하게 시각적 이해 능력을 나타내므로, MLLM 기반 에이전트의 실현이 실현가능해지고, 또한 다양한 혁신적인 애플리케이션에 대한 가능성을 촉발시킨다.\n' +
      '\n' +
      '최근 모바일 디바이스 에이전트는 MLLM 기반 에이전트의 새롭고 대중적인 응용으로 부상하고 있다. 에이전트는 스크린 및 사용자 지시에 기초하여 모바일 디바이스를 동작시킬 필요가 있다. 이는 에이전트가 시각적 지각 능력과 의미적 이해 능력을 모두 보유할 것을 요구한다. 그러나, 최신 GPT-4V를 포함한 기존의 MLLM은 여전히 효과적인 에이전트 역할을 할 수 있는 충분한 시각적 인식 능력이 부족하다. Zheng et al.(2024)은 GPT-4V가 효과적인 동작들을 생성할 수 있지만, 스크린 상에서 이들 동작들의 위치들을 정확하게 위치시키기 위해 고군분투하고 있음을 지적한다. 이러한 제한은 고급 MLLM을 통해서만 모바일 장치에서 작동하는 능력을 방해한다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 기존 연구에서는 사용자 인터페이스 레이아웃 파일을 활용하여 GPT-4V의 로컬화를 지원하려고 시도했다. Yang et al.(2023)은 안드로이드 어플리케이션 XML 파일에 접근하여 화면상의 행동 가능한 위치를 추출하였다. Zheng et al.(2024)은 로컬리제이션을 돕기 위해 웹 애플리케이션으로부터의 HTML 코드를 사용하였다. 이러한 메서드는 기본 파일의 접근성에 의존합니다. 그러나, 많은 시나리오들에서, 이러한 파일들에 액세스하기 위한 권한들은 이용가능하지 않을 수 있어, 이러한 방법들을 비효율적으로 만든다.\n' +
      '\n' +
      '기존의 로컬라이제이션 기법에서 기본 파일에 대한 의존성을 제거하기 위해 본 논문에서는 시지각 기능을 가진 자율 모바일 디바이스 에이전트인 Mobile-Agent를 제안한다. 모바일 에이전트는 시각적 인식 모듈을 통해 모바일 장치에서 스크린샷만 사용하여 작업을 정확하게 찾을 수 있다. 시지각 모듈은 로컬화된 스크린 영역들의 콘텐츠를 기술하고 스크린 내의 텍스트를 식별하는 것을 각각 담당하는 검출 및 OCR 모델들로 구성된다. 세심하게 조작된 프롬프트를 통해 에이전트와 도구 간의 효과적인 상호 작용을 촉진하여 모바일 장치 작업의 자동화를 가능하게 합니다. GPT-4V의 강력한 컨텍스트 기능을 활용하여 모바일 에이전트는 스크린샷, 사용자 명령 및 작업 이력을 기반으로 작업을 전체적으로 계획할 수 있는 자체 계획 기능을 달성한다. 에이전트의 잘못된 조작과 불완전한 지시를 식별하는 능력을 향상시키기 위해 자기 성찰 방법을 소개한다. 프롬프트에 따라 에이전트는 유효하지 않고 잘못된 작업을 계속 반영하며, 명령이 완료되면 에이전트는 중지할 수 있습니다. 모바일 에이전트의 역량을 종합적으로 평가하기 위해 현재 주류 모바일 앱을 중심으로 벤치마크인 모바일-Eval을 도입했다. 모바일 평가에는 다양한 난이도에 대한 지침이 포함되어 있습니다. Mobile-Eval을 기반으로 Mobile-Agent를 분석하고, 그 안에 있는 사례들을 소개하고 분석하였다. 실험 결과는 Mobile-Agent가 뛰어난 명령어 완성률과 연산 정확도를 나타냄을 보여준다. 여러 앱을 작동하는 것과 같은 어려운 명령에서도 모바일 에이전트는 작업을 성공적으로 완료할 수 있습니다.\n' +
      '\n' +
      '요약된 기여도는 다음과 같다:\n' +
      '\n' +
      '* 자율 이동 장치 에이전트인 Mobile-Agent를 제안한다. 모바일 에이전트는 동작 로컬화를 위해 시각적 인식 도구를 활용한다. 각 단계를 스스로 계획하고 자기 성찰을 완성할 수 있다. 모바일 에이전트는 시스템 코드 없이 장치 스크린샷에만 의존하며, 이는 순전히 비전 기반 솔루션이다.\n' +
      '* 모바일 디바이스 에이전트를 평가하기 위해 설계된 벤치마크인 Mobile-Eval을 소개한다. 이 벤치마크는 일반적으로 사용되는 10개의 앱과 세 가지 난이도가 다양한 기능 지침으로 구성됩니다.\n' +
      '* Mobile-Eval을 기반으로 Mobile-Agent에 대한 종합적인 분석을 진행하였다. 우리는 그것의 능력을 분석하기 위해 전형적인 선택된 사례들을 제시했다.\n' +
      '\n' +
      '## 2 Mobile-Agent\n' +
      '\n' +
      '이 섹션에서는 모바일 에이전트 프레임워크를 소개합니다. 프레임워크는 최첨단 MLLM GPT-4V, 텍스트 로컬리제이션을 위한 텍스트 검출 모듈, 아이콘 로컬리제이션을 위한 아이콘 검출 모듈로 구성된다. 우리는 먼저 GPT-4V에서 생성된 명령어를 모바일 장치의 특정 위치에 위치시키기 위해 시각적 도구를 사용하는 방법을 설명할 것이다. 이어서, 모바일-에이전트의 워크플로우에 대해 설명한다.\n' +
      '\n' +
      '### Visual Perception\n' +
      '\n' +
      '**GPT-4V Lacks Localization Capability.** GPT-4V가 명령어 및 스크린샷에 대한 올바른 동작을 제공할 수 있는 반면, 기존의 작업 Zheng 등(2024)은 GPT-4V가 동작 스테이케이션 장소가 있는 위치를 효과적으로 출력할 수 없음을 나타낸다. 따라서 GPT-4V가 모바일 기기 화면에 출력될 수 있도록 동작 로컬화를 지원할 수 있는 외부 도구가 필요하다.\n' +
      '\n' +
      '**Text Localization.** 에이전트가 스크린 상의 특정 텍스트를 탭해야 할 때, 우리는 OCR 툴을 사용하여 스크린 상의 대응하는 텍스트의 위치를 검출한다. 우리는 세 가지 시나리오에 대해 논의할 것이다.\n' +
      '\n' +
      '* OCR 검출 결과들이 특정된 텍스트를 포함하지 않을 때, 에이전트는 탭핑하기 위해 텍스트를 재선택하거나 대안적인 동작을 선택하도록 지시될 것이다. 이러한 상황은 GPT-4V가 소수의 환각을 가질 수 있는 복잡한 시나리오에서 종종 발생한다.\n' +
      '* OCR 검출 결과가 지정된 텍스트의 하나의 인스턴스만을 가질 때, 우리는 직접 그 텍스트 박스의 중심 좌표를 클릭하는 연산을 생성한다.\n' +
      '* OCR 검출 결과가 지정된 텍스트의 여러 인스턴스를 포함할 때, 우리는 결과의 수를 평가한다. 인스턴스가 많은 경우, 그것은 현재 스크린 상에 너무 많은 유사한 콘텐츠가 있음을 나타내며, 에이전트가 선택을 하는 것을 어렵게 한다. 이러한 경우, 에이전트는 탭핑을 위해 텍스트를 재선택하도록 요청된다. 인스턴스가 적으면 이 영역을 자르고 탐지 상자를 그립니다. 그런 다음 이러한 영역을 사용하여 에이전트가 클릭할 영역을 선택하도록 한다. 크롭할 때 텍스트 탐지 상자를 일정 범위 밖으로 확장한 다음 이 크롭된 이미지 위에 탐지 상자를 그립니다. 이는 더 많은 정보를 보존하고 에이전트의 의사 결정 과정을 용이하게 하기 위해 수행된다. 이 과정은 그림 2의 왼쪽 상단에 나와 있다.\n' +
      '\n' +
      '그림 2: Mobile-Agent의 프레임워크.\n' +
      '\n' +
      '**아이콘 Localization.** 에이전트가 아이콘을 클릭할 필요가 있을 때, 아이콘 검출 툴과 CLIP Radford et al.(2021)을 사용하여 그 위치를 찾는다. 구체적으로, 먼저 에이전트에 색상과 모양을 포함하여 클릭할 아이콘의 속성을 제공하도록 요청한다. 이어서, 우리는 스크린샷 상의 모든 아이콘들을 식별하기 위해 프롬프트 "아이콘"과 함께 접지 DINO Liu 등(2023f)을 사용한다. 마지막으로 CLIP를 사용하여 검출된 모든 아이콘 간의 유사도와 클릭 영역에 대한 설명을 계산하여 클릭에 대한 유사도가 가장 높은 영역을 선택한다. 이 과정은 그림 2의 오른쪽 상단에 나와 있다.\n' +
      '\n' +
      '### Instruction Execution\n' +
      '\n' +
      '**Operation.** 에이전트에 의해 출력된 액션들을 스크린 상의 동작들로 더 잘 변환하기 위해, 모바일-에이전트에 대한 8개의 동작들을 정의한다:\n' +
      '\n' +
      '*Open App(_App_): 바탕화면 페이지에서 특정 App을 오픈한다.\n' +
      '* 텍스트(_Text_) 클릭: 텍스트 "_Text_"가 위치한 화면의 영역을 클릭한다.\n' +
      '* 아이콘(_Icon_, _Position_) 클릭: "_Position_"에서 "_Icon_"로 기술된 영역을 클릭한다. "_Icon_"는 탭핑 위치의 색상, 아이콘 형상 등과 같은 속성을 포함하는 설명을 제공한다. "_Position_"는 에러의 가능성을 최소화하기 위해, 하나 또는 두 개의 옵션으로 상부, 하부, 좌측, 우측 또는 중앙으로부터 선택될 필요가 있다.\n' +
      '* 타입(_Text_): "_Text_"를 현재 입력란에 입력한다.\n' +
      '* 페이지 업 앤 다운: 현재 페이지의 업 앤 다운 스크롤에 사용된다.\n' +
      '* 뒤로: 마지막 페이지로 돌아갑니다.\n' +
      '* 종료: 현재 페이지에서 바로 바탕 화면으로 돌아갑니다.\n' +
      '* Stop : 지시가 완료되면 전체 과정을 종료한다.\n' +
      '\n' +
      '**Self-Planning.** Mobile-Agent는 동작의 각 단계를 반복적으로 완료한다. 반복이 시작되기 전에, 사용자는 명령을 입력할 필요가 있다. 명령어를 기반으로 전체 프로세스에 대한 시스템 프롬프트를 생성합니다. 각 반복이 시작될 때 현재 모바일 화면의 스크린샷을 캡처하여 에이전트에게 제공한다. 에이전트는 시스템 프롬프트, 동작 이력 및 현재 화면 캡쳐를 관찰함으로써 동작의 다음 단계를 출력한다. 에이전트의 출력이 프로세스를 종료하는 경우 반복이 중지되고, 그렇지 않으면 새로운 반복이 계속됩니다. 모바일 에이전트는 작업 이력을 활용하여 현재 작업 진행 상황을 인지하고 시스템 프롬프트를 기반으로 현재 스크린샷에서 작업을 생성하여 반복적인 자기 계획 프로세스를 수행한다. 이러한 과정은 도 2의 하단에 도시되어 있다.\n' +
      '\n' +
      '**Self-Reflection.** 반복 중에 에이전트가 오류를 일으켜 명령을 완료할 수 없게 될 수 있다. 수업의 성공률을 높이기 위해 자기 성찰 방법을 도입하였다. 이 방법은 두 가지 상황에서 적용됩니다. 첫 번째는 에이전트가 올바르지 않거나 올바르지 않은 작업을 생성하여 프로세스가 고착되는 경우입니다. 에이전트가 특정 작업 후 스크린샷이 변경되지 않았거나 스크린샷이 잘못된 페이지를 표시하면 에이전트에 대체 작업을 시도하거나 현재 작업의 매개변수를 수정하도록 지시할 것이다. 두 번째는 에이전트가 복잡한 명령의 특정 요구 사항을 간과할 수 있는 경우이다. 에이전트가 자체 계획을 통해 모든 작업을 완료한 후 에이전트에게 작업, 이력, 현재 스크린샷 및 사용자 명령을 분석하여 명령이 완료되었는지 여부를 결정하도록 지시할 것이다. 그렇지 않은 경우 에이전트는 자체 계획을 통해 작업을 계속 생성해야 합니다. 이러한 과정은 도 2의 하단에 도시되어 있다.\n' +
      '\n' +
      '** 프롬프트 형식.** 위에서 설명한 기능을 더 잘 구현하기 위해 ReAct에서 사용하는 프롬프트 형식에서 영감을 얻었습니다. 우리는 에이전트가 관찰, 사고, 행동의 세 가지 구성 요소를 출력하도록 요구한다. 관찰은 현재 스크린샷의 에이전트와 작업 이력에 의한 설명이다. 이렇게 하면 에이전트가 스크린샷의 업데이트를 확인하고 이력 기록을 기반으로 오류를 신속하게 식별할 수 있습니다. 사상은 관찰과 지시로부터 생성된 다음 단계의 작동에 대한 에이전트의 고려를 나타낸다. 그 요원은 생각에서 다가오는 작전을 묘사할 필요가 있다. 행동은 에이전트가 사고를 기반으로 8개의 연산과 매개변수 중 하나를 선택해야 한다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '본 절에서는 모바일 에이전트에 대한 종합적인 평가를 실시하고자 한다. 우리는 편리한 조작 호출 인터페이스로 인해 안드로이드 운영체제를 사용한다. 향후 작업에서 다른 운영 체제를 탐색할 것입니다. 우리의 실험은 크게 정량적 실험과 정성적 실험의 두 부분으로 나뉜다. 정량적 실험에서, 제안된 모바일-Eval 벤치마크에서 모바일-에이전트를 평가할 것이다. 정성적 실험에서 우리는 구체적인 사례를 분석할 것이다.\n' +
      '\n' +
      '### Setup\n' +
      '\n' +
      '**Mobile-Eval.** Mobile-Agent의 기능을 종합적으로 평가하기 위해 현재 주류 앱 기반의 벤치마크인 Mobile-Eval을 소개한다. 모바일-Eval은 모바일 기기에서 일반적으로 사용되는 총 10개의 앱으로 구성되어 있다. 에이전트의 다중 애플리케이션 사용 능력을 평가하기 위해 두 개의 앱을 동시에 사용해야 하는 지침도 도입했다. 각 앱에 대해 세 가지 지침을 설계했습니다. 첫 번째 인스트럭션은 비교적 간단하여 기본적인 App 동작의 완료만을 요구한다. 두 번째 지침은 첫 번째 지침에 몇 가지 추가 요구 사항을 추가하여 더 어렵다. 제3 명령은 추상적인 사용자 명령을 포함하며, 여기서 사용자는 어떤 앱을 사용할지 또는 어떤 동작을 수행할지를 명시적으로 지정하지 않고 에이전트가 자신의 판단을 하도록 둔다. 표 1에서는 모바일-Eval에 사용된 앱과 지침을 제시한다.\n' +
      '\n' +
      '**Metrics.** 다양한 관점에서 모바일-에이전트의 성능을 평가하기 위해 4가지 메트릭을 설계했습니다:\n' +
      '\n' +
      '*Success(Su): Mobile-Agent가 명령어를 완성하면 성공한 것으로 간주한다.\n' +
      '* 프로세스 스코어(PS): 이 메트릭은 명령어들의 실행에서 각 단계의 정확도를 측정한다. 구체적으로, 그것은 정확한 단계의 수를 총 단계의 수로 나눈 것과 같다. 에이전트가 궁극적으로 일부 지침에서 성공하지 못할 수 있지만, 각각의 올바른 단계는 계획 점수에 기여한다.\n' +
      '* 상대 효율(RE). 우리는 각 명령을 수동으로 수행하고 사람이 취한 단계의 수를 기록했다. 우리는 인간의 작동을 최적의 해결책으로 간주한다. 모바일 에이전트가 모바일 디바이스를 더 효율적으로 사용할 수 있는지 여부를 입증하기 위해 모바일 에이전트가 취한 단계의 수와 인간이 취한 단계를 비교할 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} \\hline \\hline\n' +
      '**Application** & **Instruction** \\\\ \\hline \\multirow{2}{*}{Alibaba.com} & Help me find caps in Alibaba.com. If the “Add to cart” is available in the item information page, please add the item to my cart. \\\\  & Help me find caps in Alibaba.com. If the “Add to cart” is available in the item information page, please add the item to my cart. \\\\  & I want to buy a cap. Te weard things are cheap on Alibaba.com. Maybe you can find it for me. \\\\ \\hline \\multirow{4}{*}{Amazon Music} & Search singer Jay Chou in Amazon Music. \\\\ Amazon Music & Search a music about “agent” in Amazon Music and play it. \\\\  & I want to listen music to relax. Find an App to help me. \\\\ \\hline \\multirow{2}{*}{Chrome} & Search result for today’s Lakes game. \\\\  & Search the information about Taylor Swift. \\\\  & I want to know the result for today’s Lakes game. Find an App to help me. \\\\ \\hline \\multirow{2}{*}{Gmail} & Send an empty email to to (address). \\\\  & Send an email to [address] to tell my new work. \\\\  & I want to let my friend know my new work, and his address is [address]. Find an App to help me. \\\\ \\hline \\multirow{2}{*}{Google Maps} & Navigate to Hangzhou West Lake. \\\\  & Navigate to a nearby gas station. \\\\  & I want to go to Hangzhou West Lake, but I don’t know the way. Find an App to help me. \\\\ \\hline \\multirow{2}{*}{Google Play} & Download WhatsApp in Play Store. \\\\  & Download Instagram in Play Store. \\\\  & I want WhatsApp on my phone. Find an App to help me. \\\\ \\hline \\multirow{3}{*}{Notes} & Create a new note in Notes. \\\\  & Create a new note in Notes and write “Hello, this is a note”, then save it. \\\\  & I suddenly have something to record, so help me find an App and write down the following content: meeting at 3pm. \\\\ \\hline \\multirow{2}{*}{Settings} & Turn on the dark mode. \\\\  & Turn on the airplane mode. \\\\  & I want to see the real time internet speed at the battery level, please turn on this setting for me. \\\\ \\hline \\multirow{3}{*}{TikTok} & Swipe a video about pet cat in TiTok and click a “like” for this video. \\\\  & Swipe a video about pet cat in TiTok and comment “Ohhhh, so cute cart”. \\\\  & Swipe videos in TiTok. Click “like” for 3 pet video cat. \\\\ \\hline \\multirow{3}{*}{YouTube} & Search for videos about Stephen Curry on YouTube. \\\\  & Search for videos about Stephen Curry on YouTube and open “Comments” to comment “Oh, chef, your basketball spirit has always inspired me”. \\\\  & I need you to help me show my love for Stephen Curry on YouTube. \\\\ \\hline \\multirow{3}{*}{Multi-App} & Open the calendar and look at today’s date, then go to Notes and create a new note to write “Today is [today’s data]”. \\\\  & Check the temperature in the next 5 days, and then create a new note in Notes and write a temperature analysis. \\\\  & Search the result for today’s Lakes game, and then create a note in Notes to write a sport news for this result. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: Mobile-Eval에 사용된 애플리케이션 및 명령어.\n' +
      '\n' +
      '* 완성률(CR). 모바일 에이전트가 수행할 수 있는 인간 동작 단계의 수를 계산하고, 이를 인간이 수행한 총 단계의 수로 나누어 주어진 명령에 대한 모바일 에이전트의 완료율을 증명한다. 명령어가 완료되면, 이 메트릭은 1과 동일할 것이다.\n' +
      '\n' +
      '### Quantitative Results\n' +
      '\n' +
      '실험 결과를 표 2에 제시하였으며, 첫째, 3가지 명령어에서 모바일 에이전트는 각각 91%, 82% 및 82%의 완료율을 달성하였다. 일부 명령이 성공적으로 실행되지 않았음에도 불구하고 세 가지 유형의 명령 모두에 대한 완료율은 90%를 초과했다. 다음으로, PS 메트릭을 통해 모바일 에이전트가 세 가지 명령어에 걸쳐 올바른 연산을 생성할 확률이 약 80%를 달성함을 관찰할 수 있다. 마지막으로, RE 메트릭은 모바일 에이전트가 인간 최적 동작에 도달하는 80%의 능력을 달성할 수 있음을 나타낸다. 이상의 결과는 모바일 디바이스 어시스턴트로서 모바일-에이전트의 효과를 총체적으로 나타낸다.\n' +
      '\n' +
      '일부 명령어들에 대한 PS 값들이 1에 도달하지 않아, 모바일-에이전트가 일부 유효하지 않거나 부정확한 동작들을 만들 수 있음을 나타내는 것에 주목할 필요가 있다. 그러나 이러한 경우 대부분의 지침이 궁극적으로 완료되었다. 이는 모바일 에이전트가 자기 성찰적 능력이 우수하다는 것을 시사한다. 유효하지 않거나 잘못된 작업이 있는 경우에도 스크린샷을 기반으로 반영하여 궁극적으로 오류를 수정할 수 있습니다. 이것은 인간과 마찬가지로 모든 작업이 정확하다는 것을 보장할 수 없으며 에이전트가 오류를 수정할 수 있는 능력이 있어야 하기 때문에 모바일 장치 에이전트에게 중요하다.\n' +
      '\n' +
      '### Case Study\n' +
      '\n' +
      '그림 3에서 우리는 모바일 에이전트가 사용자 명령을 이해하고 작업을 자율적으로 계획하는 능력을 보여준다. 이 명령어에는 특정 연산이 포함되어 있지 않을 수 있지만, Mobile-Agent는 사용자의 요구사항을 성공적으로 이해하고 구체적인 실행 가능한 연산으로 변환하였다. 그 후, 에이전트는 일련의 계획 단계를 통해 지침을 수행했다.\n' +
      '\n' +
      '그림 4에서는 유효하지 않거나 잘못된 지침에 직면했을 때 모바일 에이전트의 반영 능력을 보여준다. 이 경우 모바일 에이전트는 처음에 잘못된 작업을 사용하여 스크린샷에 변화가 없습니다. 반성 후 Mobile-Agent는 오류를 수정하고 작업을 계속했으며 궁극적으로 지시를 완료했다. 그림 5에서 우리는 또 다른 경우를 보여준다. 두 번의 연속적인 유효하지 않거나 부정확한 작업에 직면한 Mobile-Agent는 전체 프로세스의 원활한 실행을 보장하기 위해 작업을 신속하게 수정할 수 있었다.\n' +
      '\n' +
      '그림 6, 7에서는 여러 앱에 걸친 작업을 포함하는 시나리오에서 모바일 에이전트의 기능을 보여주었다. 이를 위해서는 에이전트가 두 앱 간의 정보 전달을 용이하게 하기 위해 일정 수준의 메모리 용량을 보유해야 한다. 사례에서 알 수 있듯이 모바일 에이전트는 첫 번째 열린 앱에서 두 번째 앱으로 정보를 정확하게 전달하고 재가공된 콘텐츠를 생성할 수 있다.\n' +
      '\n' +
      '그림 8에서 우리는 모바일 에이전트의 다국어 능력을 보여준다. GPT-4V는 현재 중국어를 다루는 데 한계가 있을 수 있지만 강력한 시각적 인식을 통해 간단한 중국 시나리오를 효과적으로 처리할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c c c|c c c} \\hline \\hline \\multirow{2}{*}{**App**} & \\multicolumn{4}{c}{Instruction 1} & \\multicolumn{4}{c}{Instruction 2} & \\multicolumn{4}{c}{Instruction 3} \\\\ \\cline{2-13}  & SU & PS & RE & CR & SU & PS & RE & CR & SU & PS & RE & CR \\\\ \\hline Alibaba.com & ✓ & 0.75 & 4 / 3 & 100\\% & ✗ & 0.39 & 13 / 8 & 62.5\\% & ✓ & 0.9 & 10 / 9 & 100\\% \\\\ Amazon Music & ✗ & 0.44 & 9 / 5 & 80.0\\% & ✓ & 0.75 & 8 / 6 & 100\\% & ✗ & 0.50 & 12 / 3 & 66.7\\% \\\\ Chrome & ✓ & 1.00 & 4 / 4 & 100\\% & ✓ & 0.8 & 5 / 4 & 100\\% & ✓ & 0.43 & 8 / 5 & 100\\% \\\\ Gmail & ✓ & 1.00 & 4 / 4 & 100\\% & ✗ & 0.56 & 9 / 8 & 37.5\\% & ✗ & 0.56 & 9 / 8 & 37.5\\% \\\\ Google Maps & ✓ & 1.00 & 5 / 5 & 100\\% & ✓ & 1.00 & 6 / 6 & 100\\% & ✓ & 1.00 & 6 / 6 & 100\\% \\\\ Google Play & ✓ & 1.00 & 3 / 3 & 100\\% & ✓ & 0.50 & 10 / 4 & 100\\% & ✓ & 1.00 & 3 / 3 & 100\\% \\\\ Notes & ✓ & 0.57 & 7 / 4 & 100\\% & ✓ & 0.67 & 6 / 4 & 100\\% & ✓ & 1.00 & 5 / 5 & 100\\% \\\\ Settings & ✓ & 1.00 & 4 / 4 & 100\\% & ✓ & 1.00 & 4 / 4 & 100\\% & ✓ & 1.00 & 5 / 5 & 100\\% \\\\ TikTok & ✓ & 1.00 & 4 / 4 & 100\\% & ✓ & 1.00 & 10 / 100\\% & ✓ & 1.00 & 7 / 7 & 100\\% \\\\ YouTube & ✓ & 1.00 & 4 / 4 & 100\\% & ✓ & 1.00 & 9 / 9 & 100\\% & ✓ & 1.00 & 7 / 7 & 100\\% \\\\ Multi-App & ✓ & 1.00 & 6 / 6 & 100\\% & ✓ & 1.00 & 6 / 6 & 100\\% & ✓ & 1.00 & 10 / 10 & 100\\% \\\\ \\hline Avg. & 0.91 & 0.89 & 4.9 / 4.2 & 98.2\\% & 0.82 & 0.77 & 7.9 / 6.3 & 90.9\\% & 0.82 & 0.84 & 7.5 / 6.2 & 91.3\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 모바일-Eval에 대한 모바일-에이전트의 전체 평가 결과, 여기서 RE의 두 값은 각각 모바일-에이전트와 인간이 취한 단계의 수를 나타낸다.\n' +
      '\n' +
      '그림 9에서는 모바일 에이전트가 포커 게임을 할 수 있는 능력을 보여준다. 모바일 에이전트는 게임의 규칙을 설명한 후 지정된 규칙에 따라 작업을 실행할 수 있다.\n' +
      '\n' +
      '그림 10, 11, 12, 13, 14에서 우리는 Mobile-Eval에서 Mobile-Agent의 강력한 기능을 보여준다. 이러한 앱들의 사용자 인터페이스들 및 동작들의 변화들, 그리고 도전적인 명령들의 존재에도 불구하고, 모바일-에이전트는 주어진 명령들을 성공적으로 완료한다.\n' +
      '\n' +
      '그림 4: 무효 연산 사용 후 자기 성찰 및 오류 보정 사례.\n' +
      '\n' +
      '그림 3: 수업 이해 및 실행 계획 사례.\n' +
      '\n' +
      '##4 관련 업무\n' +
      '\n' +
      '### LLM-based Agent\n' +
      '\n' +
      '대규모 언어 모델(Large Language Models; LLM)의 급속한 발전으로, 이들 모델을 기반으로 구축된 에이전트는 Li 등(2023); Liu 등(2023, 2023, 2023); Shen 등(2023); Wu 등(2023); Yang 등(2023); Shen 등(2024); Yang 등(2023); Hong 등(2023); Yang 등(2023); Yang 등(2023); Yang 등(2023). 핵심 역할을 하는 이 에이전트는 사용자 명령을 능숙하게 해석하고 다양한 도구 배열을 배치하여 실행할 수 있습니다.\n' +
      '\n' +
      '그림 5: 유효하지 않은 조작과 부정확한 조작을 사용한 후 자기반성과 오류정정을 한 경우로서, 여기서 조작 “클릭 텍스트(댓글 추가)”는 잘못된 페이지로 이어지고 조작 “클릭 텍스트(Post)”는 잘못된 조작이다. 잘못된 작업과 잘못된 작업은 빨간색 글꼴로 강조 표시됩니다.\n' +
      '\n' +
      '그림 6: 게임 결과를 검색하기 위해 여러 앱을 운영하는 사례.\n' +
      '\n' +
      '복잡한 작업. 다양한 도구의 광범위한 통합은 LLM을 순수한 텍스트 처리의 한계에서 해방시킨다. 현재 LLM 기반 에이전트는 이미지 및 비디오 편집, 이미지 생성, 시각적 질문 응답, 지능형 예측 등과 같은 작업에서 능력을 보여주며 다양한 영역에서 번창하고 있다. 이는 LLM이 AI 애플리케이션의 환경에 미치는 변형적 영향을 강조한다.\n' +
      '\n' +
      '### 이동단말기용 에이전트\n' +
      '\n' +
      '단말 장치를 작동시키기 위한 에이전트의 적용은 핫스팟이 되고 있다. AppAgent Yang et al.(2023d)는 GPT-4V 기반의 모바일 앱 어시스턴트이다. 그들은 안드로이드 시스템에서 XML 파일을 호출하여 반투명한 태그로 앱의 UI의 조작 가능한 영역에 라벨을 붙인다. 에이전트는 자기 탐색의 세 가지 방법을 통해 운영 능력을 획득하고,\n' +
      '\n' +
      '그림 8: 중국 시스템 및 앱 운영 사례.\n' +
      '\n' +
      '그림 7: 온도 분석을 작성하기 위해 여러 앱을 작동시킨 사례.\n' +
      '\n' +
      '사용자 비디오 데모를 관찰하고 사용자 문서를 활용합니다. 일정 정도의 탐색 후에, 에이전트는 동작가능한 영역들에 대한 충분한 이해를 얻음으로써, 명령들에 기초하여 올바른 동작들을 실행할 수 있게 한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 통합 시각 인식 프레임워크를 통해 광범위한 모바일 애플리케이션을 효율적으로 운용할 수 있는 자율 멀티모달 에이전트인 Mobile-Agent를 소개한다. 모바일 에이전트는 시각적 및 텍스트 요소를 앱의 인터페이스 내에서 정확하게 식별하고 찾기 위해 시각적 인식 도구를 사용한다. 지각된 시각적 맥락을 활용하여 복잡한 업무를 자율적으로 기획하고 분해하며, 모바일 앱을 통해 단계적으로 탐색한다. XML 모바일 시스템 메타데이터에 의존하는 기존 솔루션과 달리, 모바일 에이전트는 비전 중심 방식으로 다양한 모바일 운영 환경에 걸쳐 향상된 적응성을 제공하여 시스템별 커스터마이징의 필요성을 제거한다. 실험을 통해 다양한 차원에 걸쳐 모바일 에이전트의 효과와 효율성을 입증한다. 이것은 언어 진단 방식으로 모바일 애플리케이션과 상호 작용할 수 있는 다재다능하고 적응 가능한 솔루션으로서의 잠재력을 보여준다.\n' +
      '\n' +
      '그림 10: Alibaba.com의 도매 캡 사례.\n' +
      '\n' +
      '그림 9: 게임을 하는 경우.\n' +
      '\n' +
      '그림 11: 유튜브에서 동영상을 검색하고 이 동영상을 댓글을 달았던 사례.\n' +
      '\n' +
      '도 12: 구글 플레이에서 특정 앱을 다운로드한 사례.\n' +
      '\n' +
      '도 13: 네비게이션을 위한 지도 앱을 이용한 사례.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Li et al. (2023) Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, et al. Modelscope-agent: open-source large language models로 사용자화 가능한 에이전트 시스템을 구축한다. _ arXiv preprint arXiv:2309.00986_, 2023.\n' +
      '* Liu et al. (2023) Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai, et al. Controlllm: Augment language models with tools by search on graph. _ arXiv preprint arXiv:2310.17796_, 2023a.\n' +
      '* Liu et al. (2023) Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Zeqiang Lai, Yang Yang, Qingyun Li, et al. Interngpt: 언어 이상의 채팅과 상호 작용하여 비전 중심 과제를 해결한다. _ arXiv preprint arXiv:2305.05662_, 3, 2023b.\n' +
      '* Liu et al. (2023c) Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. _ arXiv preprint arXiv:2311.05437_, 2023c.\n' +
      '* Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggt: 껴안는 얼굴에서 채팅 및 그 친구들과 함께 ai 작업을 해결하는 것. _ arXiv preprint arXiv:2303.17580_, 2023.\n' +
      '* Wu et al. (2023) Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 시각적 채팅: 시각적 기초 모델로 대화, 그리기 및 편집. _ arXiv preprint arXiv:2303.04671_, 2023.\n' +
      '* Yang et al. (2023) Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: 자가 명령어를 통해 툴을 사용하도록 큰 언어 모델을 가르치는 것 _ arXiv preprint arXiv:2305.18752_, 2023a.\n' +
      '* Shen et al. (2024) Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. 작은 lms는 약한 도구 학습자들입니다. 멀티-llm 에이전트입니다. _ arXiv preprint arXiv:2401.07324_, 2024.\n' +
      '* Yang et al. (2023) Zhen규안 Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: multimodal reasoning and action에 대한 신속한 채팅; _ arXiv preprint arXiv:2303.11381_, 2023b.\n' +
      '* Hong et al. (2023) Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. _ arXiv preprint arXiv:2308.00352_, 2023.\n' +
      '* Yang et al. (2023) Hui Yang, Sifu Yue, and Yunzhong He. 온라인 의사 결정을 위한 오토-gpt: 벤치마크 및 추가 의견. _ arXiv preprint arXiv:2306.02224_, 2023c.\n' +
      '* Liu et al. (2023d) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 시각적 지시 조율 arXiv preprint arXiv:2304.08485_, 2023d.\n' +
      '\n' +
      '그림 14: 아마존뮤직을 이용하여 특정 콘텐츠로 음악을 검색하여 재생하는 사례.\n' +
      '\n' +
      '* Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: 고급 대형 언어 모델로 비전 언어 이해력 향상. _ arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '* Ye et al. (2023a) Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. _ arXiv preprint arXiv:2304.14178_, 2023a.\n' +
      '* Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 인스트럭션 블립: 명령어 튜닝이 있는 범용 비전 언어 모델에 대해. _ arXiv preprint arXiv:2305.06500_, 2023.\n' +
      '* Liu et al. (2023) Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 시각적 지시 조정을 통해 개선된 기준선입니다. _ arXiv preprint arXiv:2310.03744_, 2023e.\n' +
      '* Chen et al. (2023) Jun Chen, Deyao Ziaoqian Shen Xiang Li, Zechun Liu Pengchuan Zhang, Raghuraman Krishnamoorthi Vikas Chandra Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: 비젼-언어 멀티태스크 학습을 위한 통합 인터페이스로서의 대용량 언어 모델. _ arXiv preprint arXiv:2310.09478_, 2023.\n' +
      '* Ye et al. (2023b) Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, 및 Jingren Zhou. mplug-owl2: 모달리티 협업으로 멀티모달 대형 언어 모델을 혁명화. _ arXiv preprint arXiv:2311.04257_, 2023b.\n' +
      '* Bai et al. (2023) Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: 다재다능한 능력을 가진 프론티어 대형 비전 언어 모델. _ arXiv preprint arXiv:2308.12966_, 2023.\n' +
      '* Lin et al. (2023) Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. 시각 언어 모델을 위한 사전 훈련 중 arXiv preprint arXiv:2312.07533_, 2023.\n' +
      '* Zheng et al. (2024) Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision)는 접지된 경우 일반 웹 에이전트이다. _ arXiv preprint arXiv:2401.01614_, 2024.\n' +
      '* Yang et al. (2023) Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: 스마트폰 사용자로서의 멀티모달 에이전트 _ arXiv preprint arXiv:2312.13771_, 2023d.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* Liu et al. (2023) Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. _ arXiv preprint arXiv:2303.05499_, 2023f.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>