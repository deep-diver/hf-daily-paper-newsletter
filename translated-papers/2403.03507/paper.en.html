<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection\n' +
      '\n' +
      'Jiawei Zhao\n' +
      '\n' +
      'Zhenyu Zhang\n' +
      '\n' +
      'Beidi Chen\n' +
      '\n' +
      'Zhangyang Wang\n' +
      '\n' +
      'Anima Anandkumar\n' +
      '\n' +
      'Yuandong Tian\n' +
      '\n' +
      'Equal advising 1California Institute of Technology 2Meta AI\n' +
      '\n' +
      'Footnote 1: The calculation is based on LLaMA architecture, BF16 numerical format, and maximum sequence length of 2048.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (**GaLore**), a training strategy that allows _full-parameter_ learning but is more _memory-efficient_ than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.\n' +
      '\n' +
      'Machine Learning, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large Language Models (LLMs) have shown impressive performance across multiple disciplines, including conversational AI and language translation. However, pre-training and fine-tuning LLMs require not only a huge amount of computation but is also memory intensive. The memory requirements include not only billions of trainable parameters, but also their gradients and optimizer states (e.g., gradient momentum and variance in Adam) that can be larger than parameter storage themselves (Raffel et al., 2023; Touvron et al., 2023; Chowdhery et al., 2022). For example, pre-training a LLaMA 7B model from scratch with a single batch size requires at least 58 GB memory (14GB for trainable parameters, 42GB for Adam optimizer states and weight gradients, and 2GB for activations1). This makes the training not feasible on consumer-level GPUs such as NVIDIA RTX 4090 with 24GB memory.\n' +
      '\n' +
      'Footnote 1: The calculation is based on LLaMA architecture, BF16 numerical format, and maximum sequence length of 2048.\n' +
      '\n' +
      'In addition to engineering and system efforts, such as gradient checkpointing (Chen et al., 2016), memory offloading (Rajbhandari et al., 2020), etc., to achieve faster and more efficient distributed training, researchers also seek to develop various optimization techniques to reduce the memory usage during pre-training and fine-tuning.\n' +
      '\n' +
      'Figure 1: Memory consumption of pre-training a LLaMA 7B model with a token batch size of 256 on a single device, without activation checkpointing and memory offloading. Details refer to Section 5.5.\n' +
      '\n' +
      'Parameter-efficient fine-tuning (PEFT) techniques allow for the efficient adaptation of pre-trained language models (PLMs) to different downstream applications without the need to fine-tune all of the model\'s parameters (Ding et al., 2022). Among them, the popular Low-Rank Adaptation (LoRA Hu et al. (2021)) _reparameterizes_ weight matrix \\(W\\in\\mathbb{R}^{m\\times n}\\) into \\(W=W_{0}+BA\\), where \\(W_{0}\\) is a frozen full-rank matrix and \\(B\\in\\mathbb{R}^{m\\times r}\\), \\(A\\in\\mathbb{R}^{r\\times n}\\) are additive low-rank adaptors to be learned. Since the rank \\(r\\ll\\min(m,n)\\), \\(A\\) and \\(B\\) contain fewer number of trainable parameters and thus smaller optimizer states. LoRA has been used extensively to reduce memory usage for fine-tuning in which \\(W_{0}\\) is the frozen pre-trained weight. Its variant ReLoRA is also used in pre-training, by periodically updating \\(W_{0}\\) using previously learned low-rank adaptors (Lialin et al., 2023).\n' +
      '\n' +
      'However, many recent works demonstrate the limitation of such a low-rank reparameterization. For fine-tuning, LoRA is not shown to reach a comparable performance as full-rank fine-tuning (Xia et al., 2024). For pre-training from scratch, it is shown to require a full-rank model training as a warmup (Lialin et al., 2023), before optimizing in the low-rank subspace. There are two possible reasons: (1) the optimal weight matrices may not be low-rank, and (2) the reparameterization changes the gradient training dynamics.\n' +
      '\n' +
      '**Our approach:** To address the above challenge, we propose Gradient Low-rank Projection (**GaLore**), a training strategy that allows _full-parameter_ learning but is more _memory-efficient_ than common low-rank adaptation methods, such as LoRA. Our key idea is to leverage the slow-changing low-rank structure of the _gradient_\\(G\\in\\mathbb{R}^{m\\times n}\\) of the weight matrix \\(W\\), rather than trying to approximate the weight matrix itself as low rank.\n' +
      '\n' +
      'We first show theoretically that the gradient matrix \\(G\\) becomes low-rank during training. Then, we propose GaLore that computes two projection matrices \\(P\\in\\mathbb{R}^{m\\times r}\\) and \\(Q\\in\\mathbb{R}^{n\\times r}\\) to project the gradient matrix \\(G\\) into a low-rank form \\(P^{\\top}GQ\\). In this case, the memory cost of optimizer states, which rely on component-wise gradient statistics, can be substantially reduced. Occasional updates of \\(P\\) and \\(Q\\) (e.g., every 200 iterations) incur minimal amortized additional computational cost. GaLore is more memory-efficient than LoRA as shown in Table 1. In practice, this yields up to 30% memory reduction compared to LoRA during pre-training.\n' +
      '\n' +
      'We demonstrate that GaLore works well in both LLM pre-training and fine-tuning. When pre-training LLaMA 7B on C4 dataset, 8-bit GaLore, combined with 8-bit optimizers and layer-wise weight updates techniques, achieves comparable performance to its full-rank counterpart, with less than 10% memory cost of optimizer states.\n' +
      '\n' +
      'Notably, for pre-training, GaLore keeps low memory throughout the entire training, without requiring full-rank training warmup like ReLoRA. Thanks to GaLore\'s memory efficiency, for the first time it is possible to train LLaMA 7B from scratch on a single GPU with 24GB memory (e.g., on NVIDIA RTX 4090), without any costly memory offloading techniques (Fig. 1).\n' +
      '\n' +
      'GaLore is also used to fine-tune pre-trained LLMs on GLUE benchmarks with comparable or better results than existing low-rank methods. When fine-tuning RoBERTaBase on GLUE tasks with a rank of 4, GaLore achieves an average score of 85.89, outperforming LoRA, which achieves a score of 85.61.\n' +
      '\n' +
      'As a gradient projection method, GaLore is independent of the choice of optimizers and can be easily plugged into existing ones with only two lines of code, as shown in Algorithm 1. Our experiment (Fig. 3) shows that it works for popular optimizers such as AdamW, 8-bit Adam, and Adafactor. In addition, its performance is insensitive to very few hyper-parameters it introduces. We also provide theoretical justification on the low-rankness of gradient update, as well as the convergence analysis of GaLore.\n' +
      '\n' +
      '## 2 Related Works\n' +
      '\n' +
      'Low-Rank AdaptationHu et al. (2021) proposed Low-Rank Adaptation (LoRA) to fine-tune pre-trained models with low-rank adaptors. This method reduces the memory footprint by maintaining a low-rank weight adaptor for each layer. There are a few variants of LoRA proposed to enhance its performance (Renduchintala et al., 2023; Sheng et al., 2023; Xia et al., 2024), supporting multi-task learning (Wang et al., 2023), and further reducing the memory footprint (Dettmers et al., 2023). Lialin et al. (2023) proposed ReLoRA, a variant of LoRA designed for pre-training, but requires a full-rank training warmup to achieve comparable performance as the standard baseline.\n' +
      '\n' +
      'Subspace LearningRecent studies have demonstrated that the learning primarily occurs within a significantly low-dimensional parameter subspace (Larsen et al., 2022; Gur-Ari et al., 2018). These findings promote a special type of learning called _subspace learning_, where the model weights are optimized within a low-rank subspace. This notion has been widely used in different domains of machine learning, including meta-learning and continual learning (Lee and Choi, 2018; Chaudhry et al., 2020).\n' +
      '\n' +
      'Projected Gradient DescentGaLore is closely related to the traditional topic of projected gradient descent (PGD) (Chen and Wainwright, 2015; Chen et al., 2019). A key difference is that, GaLore considers the specific gradient form that naturally appears in training multi-layer neural net works (e.g., it is a matrix with specific structures), proving many of its properties (e.g., Lemma 3.1, Theorem 3.2, and Theorem 3.6). In contrast, traditional PGD mostly treats the objective as a general blackbox nonlinear function, and study the gradients in the vector space only.\n' +
      '\n' +
      'Memory-Efficient OptimizationThere have been some works trying to reduce the memory cost of gradient statistics for adaptive optimization algorithms (Shazeer & Stern; Anil et al.; Dettmers et al., 2021). Adafactor (Shazeer & Stern) achieves sub-linear memory cost by factorizing the second-order statistics by a row-column outer product. GaLore shares similarities with Adafactor in terms of utilizing low-rank factorization to reduce memory cost, but GaLore focuses on the low-rank structure of the gradients, while Adafactor focuses on the low-rank structure of the second-order statistics. GaLore can reduce the memory cost for both first-order and second-order statistics, and can be combined with Adafactor to achieve further memory reduction. Quantization is also widely used to reduce the memory cost of optimizer states (Dettmers et al., 2021; Li et al., 2023). Furthermore, Lv et al. (2023) proposed fused gradient computation to reduce the memory cost of storing weight gradients during training.\n' +
      '\n' +
      'In contrast to the previous memory-efficient optimization methods, GaLore operates independently as the optimizers directly receive the low-rank gradients without knowing their full-rank counterparts.\n' +
      '\n' +
      '## 3 GaLore: Gradient Low-Rank Projection\n' +
      '\n' +
      '### Background\n' +
      '\n' +
      'Regular full-rank trainingAt time step \\(t\\), \\(G_{t}=-\\nabla_{W}\\varphi_{t}(W_{t})\\in\\mathbb{R}^{m\\times n}\\) is the backpropagated (negative) gradient matrix. Then the regular pre-training weight update can be written down as follows (\\(\\eta\\) is the learning rate):\n' +
      '\n' +
      '\\[W_{T}=W_{0}+\\eta\\sum_{t=0}^{T-1}\\tilde{G}_{t}=W_{0}+\\eta\\sum_{t=0}^{T-1}\\rho _{t}(G_{t}) \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\tilde{G}_{t}\\) is the final processed gradient to be added to the weight matrix and \\(\\rho_{t}\\) is an entry-wise stateful gradient regularizer (e.g., Adam). The state of \\(\\rho_{t}\\) can be memory-intensive. For example, for Adam, we need \\(M,V\\in\\mathbb{R}^{m\\times n}\\) to regularize the gradient \\(G_{t}\\) into \\(\\tilde{G}_{t}\\):\n' +
      '\n' +
      '\\[M_{t} =\\beta_{1}M_{t-1}+(1-\\beta_{1})G_{t} \\tag{2}\\] \\[V_{t} =\\beta_{2}V_{t-1}+(1-\\beta_{2})G_{t}^{2}\\] (3) \\[\\tilde{G}_{t} =M_{t}/\\sqrt{V_{t}+\\epsilon} \\tag{4}\\]\n' +
      '\n' +
      'Here \\(G_{t}^{2}\\) and \\(M_{t}/\\sqrt{V_{t}+\\epsilon}\\) means element-wise multiplication and division. \\(\\eta\\) is the learning rate. Together with \\(W\\in\\mathbb{R}^{m\\times n}\\), this takes \\(3mn\\) memory.\n' +
      '\n' +
      'Low-rank updates.For a linear layer \\(W\\in\\mathbb{R}^{m\\times n}\\), LoRA and its variants utilize the low-rank structure of the update matrix by introducing a low-rank adaptor \\(AB\\):\n' +
      '\n' +
      '\\[W_{T}=W_{0}+B_{T}A_{T}, \\tag{5}\\]\n' +
      '\n' +
      'where \\(B\\in\\mathbb{R}^{m\\times r}\\) and \\(A\\in\\mathbb{R}^{r\\times n}\\), and \\(r\\ll\\min(m,n)\\). \\(A\\) and \\(B\\) are the learnable low-rank adaptors and \\(W_{0}\\) is a fixed weight matrix (e.g., pre-trained weight).\n' +
      '\n' +
      '### Low-Rank Property of Weight Gradient\n' +
      '\n' +
      'While low-rank updates are proposed to reduce memory usage, it remains an open question whether the weight matrix should be parameterized as low-rank. In many situations, this may not be true. For example, in linear regression \\(\\mathbf{y}=W\\mathbf{x}\\), if the optimal \\(W^{*}\\) is high-rank, then imposing a low-rank assumption on \\(W\\) never leads to the optimal solution, regardless of what optimizers are used.\n' +
      '\n' +
      'Surprisingly, while the weight matrices are not necessarily low-rank, the gradient indeed becomes low-rank during the training for certain gradient forms and associated network architectures:\n' +
      '\n' +
      '**Lemma 3.1** (Gradient becomes low-rank during training).: _Let \\(m\\leq n\\) without loss of generality. The gradient update:_\n' +
      '\n' +
      '\\[G_{t}=A-BW_{t}C,\\quad W_{t}=W_{t-1}+\\eta G_{t-1} \\tag{6}\\]\n' +
      '\n' +
      '_with constant \\(A\\) and PSD matrices \\(B\\) and \\(C\\) and randomly initialized \\(W_{0}\\) leads to low-rank gradient with high probability:_\n' +
      '\n' +
      '\\[\\text{stable-rank}(G_{t})\\leq 1+\\sum_{i=2}^{m}O\\left(\\frac{1-\\eta\\lambda_{i} \\nu_{1}}{1-\\eta\\lambda_{1}\\nu_{1}}\\right)^{2t} \\tag{7}\\]\n' +
      '\n' +
      '_Here \\(\\nu_{1}=\\lambda_{\\min}(C)\\) is the smallest eigenvalues of \\(C\\) and \\(\\lambda_{1}\\leq\\ldots\\leq\\lambda_{n}\\) are eigenvalues of \\(B\\). Furthermore, if \\(\\lambda_{2}>\\lambda_{1}\\) and \\(\\nu_{1}>0\\), then \\(G_{t}\\) converges to rank-\\(1\\) exponentially._\n' +
      '\n' +
      'Note that in Lemma 3.1, we assume a parametric form (Eqn. 6) of the gradient. This is not a limiting assumption. It not only holds for simple linear network with objective \\(\\varphi(W)=\\|\\mathbf{y}-W\\mathbf{x}\\|_{2}^{2}\\), but also hold in more general nonlinear networks known as "reversible networks" (Tian et al., 2020), including deep ReLU networks:\n' +
      '\n' +
      '**Theorem 3.2** (Gradient Form of reversible models).: _In a chained reversible neural network \\(\\mathcal{N}(\\mathbf{x}):=\\mathcal{N}_{L}(\\mathcal{N}_{L-1}(\\ldots\\mathcal{N}_{1} (\\mathbf{x})))\\) with \\(\\ell_{2}\\)-objective \\(\\varphi:=\\frac{1}{2}\\|\\mathbf{y}-\\mathcal{N}(\\mathbf{x})\\|_{2}^{2}\\), the weight matrix \\(W_{l}\\) at layer \\(l\\) has gradient \\(G_{l}\\) of the following form for batchsize 1:_\n' +
      '\n' +
      '\\[G_{l}=\\underbrace{J_{l}^{\\top}\\mathbf{y}\\mathbf{f}_{l-1}^{\\top}}_{\\mathcal{A}}- \\underbrace{J_{l}^{\\top}J_{l}}_{\\mathcal{B}}W_{l}\\underbrace{\\mathbf{f}_{l-1}\\mathbf{ f}_{l-1}^{\\top}}_{\\mathcal{C}} \\tag{8}\\]\n' +
      '\n' +
      '_where \\(J_{l}:=\\operatorname{Jacobian}(\\mathcal{N}_{L})\\ldots\\operatorname{Jacobian}( \\mathcal{N}_{l+1})\\) and \\(\\mathbf{f}_{l}:=\\mathcal{N}_{l}(\\mathcal{N}_{l-1}\\ldots\\mathcal{N}_{1}(\\mathbf{x}))\\)._Note that for softmax objective with small logits, we can also prove a similar structure of backpropagated gradient, and thus Theorem 3.2 can also apply.\n' +
      '\n' +
      '**Lemma 3.3** (Gradient structure of softmax loss).: _For \\(K\\)-way logsoftmax loss \\(\\varphi(\\mathbf{y};\\mathbf{f}):=-\\log\\left(\\frac{\\exp(\\mathbf{y}^{\\top}\\mathbf{f})}{\\mathbf{1}^{ \\top}\\exp(\\mathbf{f})}\\right)\\), let \\(\\hat{\\mathbf{f}}=P_{\\mathbf{1}}^{\\perp}\\mathbf{f}\\) be the zero-mean version of network output \\(\\mathbf{f}\\), where \\(P_{\\mathbf{1}}^{\\perp}:=I-\\frac{1}{K}\\mathbf{1}\\mathbf{1}^{\\top}\\), then we have:_\n' +
      '\n' +
      '\\[-\\mathrm{d}\\varphi=\\mathbf{y}^{\\top}\\mathrm{d}\\hat{\\mathbf{f}}-\\gamma\\mathbf{f}^{\\top} \\mathrm{d}\\hat{\\mathbf{f}}/K+O(\\hat{\\mathbf{f}}^{2}/K)\\mathrm{d}\\hat{\\mathbf{f}} \\tag{9}\\]\n' +
      '\n' +
      '_where \\(\\gamma(\\mathbf{y},\\mathbf{f})\\approx 1\\) and \\(\\mathbf{y}\\) is a data label with \\(\\mathbf{y}^{\\top}\\mathbf{1}=1\\)._\n' +
      '\n' +
      'With this lemma, it is clear that for a reversible network \\(\\mathbf{f}:=\\mathcal{N}(\\mathbf{x})=J_{l}(\\mathbf{x})W_{l}\\mathbf{f}_{l-1}(\\mathbf{x})\\), the gradient \\(G_{l}\\) of \\(W_{l}\\) has the following form:\n' +
      '\n' +
      '\\[G_{l}=\\underbrace{J_{l}P_{\\mathbf{1}}^{\\perp}\\mathbf{y}\\mathbf{f}_{l-1}}_{\\mathcal{A}}- \\underbrace{\\gamma J_{l}^{\\top}P_{\\mathbf{1}}^{\\perp}J_{l}}_{B}W_{l}\\underbrace{ \\mathbf{f}_{l-1}\\mathbf{f}_{l-1}^{\\top}/K}_{C} \\tag{10}\\]\n' +
      '\n' +
      'which is consistent with the form \\(G_{l}=A-BW_{l}C\\). For a detailed introduction to reversibility, please check the Appendix A.2.\n' +
      '\n' +
      '### Gradient Low-rank Projection (GaLore)\n' +
      '\n' +
      'Since the gradient \\(G\\) may have a low-rank structure, if we can keep the gradient statistics of a small "core" of gradient \\(G\\) in optimizer states, rather than \\(G\\) itself, then the memory consumption can be reduced substantially. This leads to our proposed GaLore strategy:\n' +
      '\n' +
      '**Definition 3.4** (Gradient Low-rank Projection (**GaLore**)).: Gradient low-rank projection (**GaLore**) denotes the following gradient update rules (\\(\\eta\\) is the learning rate):\n' +
      '\n' +
      '\\[W_{T}=W_{0}+\\eta\\sum_{t=0}^{T-1}\\tilde{G}_{t},\\qquad\\tilde{G}_{t}=P_{t}\\rho_{ t}(P_{t}^{\\top}G_{t}Q_{t})Q_{t}^{\\top}, \\tag{11}\\]\n' +
      '\n' +
      'where \\(P_{t}\\in\\mathbb{R}^{m\\times r}\\) and \\(Q_{t}\\in\\mathbb{R}^{n\\times r}\\) are projection matrices.\n' +
      '\n' +
      'Different from LoRA, GaLore _explicitly utilizes the low-rank updates_ instead of introducing additional low-rank adaptors and hence does not alter the training dynamics.\n' +
      '\n' +
      'In the following, we show that GaLore converges under a similar (but more general) form of gradient update rule (Eqn. 6). This form corresponds to Eqn. 8 but with a larger batch size.\n' +
      '\n' +
      '**Definition 3.5** (\\(L\\)-continuity).: A function \\(\\mathbf{h}(W)\\) has (Lipschitz) \\(L\\)-continuity, if for any \\(W_{1}\\) and \\(W_{2}\\), \\(\\|\\mathbf{h}(W_{1})-\\mathbf{h}(W_{2})\\|_{F}\\leq L\\|W_{1}-W_{2}\\|_{F}\\).\n' +
      '\n' +
      '**Theorem 3.6** (Convergence of GaLore with fixed projections).: _Suppose the gradient has the following form (Eqn. 8 with batchsize \\(>1\\)):_\n' +
      '\n' +
      '\\[G=\\sum_{i}A_{i}-\\sum_{i}B_{i}WC_{i} \\tag{12}\\]\n' +
      '\n' +
      '_where \\(B_{i}\\) and \\(C_{i}\\) are PSD matrices, \\(A_{i}\\), \\(B_{i}\\) and \\(C_{i}\\) have \\(L_{A}\\), \\(L_{B}\\) and \\(L_{C}\\) continuity with respect to \\(W\\) and \\(\\|W_{t}\\|\\leq D\\). Let \\(R_{t}:=P_{t}^{\\top}G_{t}Q_{t}\\), \\(\\hat{B}_{it}:=P_{t}^{\\top}B_{i}(W_{t})P_{t}\\), \\(\\hat{C}_{it}:=Q_{t}^{\\top}C_{i}(W_{t})Q_{t}\\) and \\(\\kappa_{t}:=\\frac{1}{N}\\sum_{i}\\lambda_{\\min}(\\hat{B}_{it})\\lambda_{\\min}(\\hat {C}_{it})\\). If we choose constant \\(P_{t}=P\\) and \\(Q_{t}=Q\\), then GaLore with \\(\\rho_{t}\\equiv 1\\) satisfies:_\n' +
      '\n' +
      '\\[\\|R_{t}\\|_{F}\\leq\\left[1-\\eta(\\kappa_{t-1}-L_{A}-L_{B}L_{C}D^{2})\\right]\\|R_{t -1}\\|_{F} \\tag{13}\\]\n' +
      '\n' +
      '_As a result, if \\(\\min_{t}\\kappa_{t}>L_{A}+L_{B}L_{C}D^{2}\\), \\(R_{t}\\to 0\\) and thus GaLore converges with fixed \\(P_{t}\\) and \\(Q_{t}\\)._\n' +
      '\n' +
      '**Setting \\(P\\) and \\(Q\\)**. The theorem tells that \\(P\\) and \\(Q\\) should project into the subspaces corresponding to the first few largest eigenvectors of \\(\\hat{B}_{it}\\) and \\(\\hat{C}_{it}\\) for faster convergence (large \\(\\kappa_{t}\\)). While all eigenvalues of the positive semidefinite (PSD) matrix \\(B\\) and \\(C\\) are non-negative, some of them can be very small and hinder convergence (i.e., it takes a long time for \\(G_{t}\\) to become \\(0\\)). With the projection \\(P\\) and \\(Q\\), \\(P^{\\top}B_{it}P\\) and \\(Q^{\\top}C_{it}Q\\) only contain the largest eigen subspaces of \\(B\\) and \\(C\\), improving the convergence of \\(R_{t}\\) and at the same time, reduces the memory usage.\n' +
      '\n' +
      'While it is tricky to obtain the eigenstructure of \\(\\hat{B}_{it}\\) and \\(\\hat{C}_{it}\\) (they are parts of Jacobian), one way is to instead use the spectrum of \\(G_{t}\\) via Singular Value Decomposition (SVD):\n' +
      '\n' +
      '\\[G_{t} =USV^{\\top}\\approx\\sum_{i=1}^{r}s_{i}u_{i}v_{i}^{\\top} \\tag{14}\\] \\[P_{t} =[u_{1},u_{2},...,u_{r}],\\quad Q_{t}=[v_{1},v_{2},...,v_{r}] \\tag{15}\\]\n' +
      '\n' +
      '**Difference between GaLore and LoRA.** While both GaLore and LoRA have "low-rank" in their names, they follow very different training trajectories. For example, when \\(r=\\min(m,n)\\), GaLore with \\(\\rho_{t}\\equiv 1\\) follows the exact training trajectory of the original model, as \\(\\tilde{G}_{t}=P_{t}P_{t}^{\\top}G_{t}Q_{t}Q_{t}^{\\top}=G_{t}\\). On the other hand, when \\(BA\\) reaches full rank (i.e., \\(B\\in\\mathbb{R}^{m\\times m}\\) and \\(A\\in\\mathbb{R}^{m\\times n}\\)), optimizing \\(B\\) and \\(A\\) simultaneously follows very different training trajectory from the original model.\n' +
      '\n' +
      '## 4 GaLore for Memory-Efficient Training\n' +
      '\n' +
      'For a complex optimization problem such as LLM pretraining, it may be difficult to capture the entire gradient trajectory with a single low-rank subspace. One reason is that the principal subspaces of \\(B_{t}\\) and \\(C_{t}\\) (and thus \\(G_{t}\\)) may change over time. In fact, if we keep the same projection \\(P\\) and \\(Q\\), then the learned weights will only grow along these subspaces, which is not longer full-parameter training. Fortunately, for this, GaLore can switch subspacesduring training and learn full-rank weights without increasing the memory footprint.\n' +
      '\n' +
      '### Composition of Low-Rank Subspaces\n' +
      '\n' +
      'We allow GaLore to switch across low-rank subspaces:\n' +
      '\n' +
      '\\[W_{t}=W_{0}+\\Delta W_{T_{1}}+\\Delta W_{T_{2}}+\\ldots+\\Delta W_{T_{n}}, \\tag{16}\\]\n' +
      '\n' +
      'where \\(t\\in\\left[\\sum_{i=1}^{n-1}T_{i},\\sum_{i=1}^{n}T_{i}\\right]\\) and \\(\\Delta W_{T_{1}}=\\eta\\sum_{t=0}^{T_{i}-1}\\tilde{G_{t}}\\) is the summation of all \\(T_{i}\\) updates within the \\(i\\)-th subspace. When switching to \\(i\\)-th subspace at step \\(t=T_{i}\\), we re-initialize the projector \\(P_{t}\\) and \\(Q_{t}\\) by performing SVD on the current gradient \\(G_{t}\\) by Equation 14. We illustrate how the trajectory of \\(\\tilde{G_{t}}\\) traverses through multiple low-rank subspaces in Fig. 2. In the experiment section, we show that allowing multiple low-rank subspaces is the key to achieving the successful pre-training of LLMs.\n' +
      '\n' +
      'Following the above procedure, the switching frequency \\(T\\) becomes a hyperparameter. The ablation study (Fig. 5) shows a sweet spot exists. A very frequent subspace change increases the overhead (since new \\(P_{t}\\) and \\(Q_{t}\\) need to be computed) and breaks the condition of constant projection in Theorem 3.6. In practice, it may also impact the fidelity of the optimizer states, which accumulate over multiple training steps. On the other hand, a less frequent change may make the algorithm stuck into a region that is no longer important to optimize (convergence proof in Theorem 3.6 only means good progress in the designated subspace, but does not mean good overall performance). While optimal \\(T\\) depends on the total training iterations and task complexity, we find that a value between \\(T=50\\) to \\(T=1000\\) makes no significant difference. Thus, the total computational overhead induced by SVD is negligible (\\(<10\\%\\)) compared to other memory-efficient training techniques such as memory offloading (Rajbhandari et al., 2020).\n' +
      '\n' +
      '### Memory-Efficient Optimization\n' +
      '\n' +
      '**Reducing memory footprint of gradient statistics**. GaLore significantly reduces the memory cost of optimizer that heavily rely on component-wise gradient statistics, such as Adam (Kingma & Ba, 2014). When \\(\\rho_{t}\\equiv\\mathrm{Adam}\\), by projecting \\(G_{t}\\) into its low-rank form \\(R_{t}\\), Adam\'s gradient regularizer \\(\\rho_{t}(R_{t})\\) only needs to track low-rank gradient statistics. where \\(M_{t}\\) and \\(V_{t}\\) are the first-order and second-order momentum, respectively. GaLore computes the low-rank normalized gradient \\(N_{t}\\) as follows:\n' +
      '\n' +
      '\\[N_{t}=\\rho_{t}(R_{t})=M_{t}/(\\sqrt{V_{t}}+\\epsilon). \\tag{17}\\]\n' +
      '\n' +
      'GaLore can also apply to other optimizers (e.g., Adafactor) that have similar update rules and require a large amount of memory to store gradient statistics.\n' +
      '\n' +
      '**Reducing memory usage of projection matrices.** To achieve the best memory-performance trade-off, we only use one project matrix \\(P\\) or \\(Q\\), projecting the gradient \\(G\\) into \\(P^{\\top}G\\) if \\(m\\leq n\\) and \\(GQ\\) otherwise. We present the algorithm applying GaLore to Adam in Algorithm 2.\n' +
      '\n' +
      '```\n' +
      'Input: A layer weight matrix \\(W\\in\\mathbb{R}^{m\\times n}\\) with \\(m\\leq n\\). Step size \\(\\eta\\), scale factor \\(\\alpha\\), decay rates \\(\\beta_{1},\\beta_{2}\\), rank \\(r\\), subspace change frequency \\(T\\). Initialize first-order moment \\(M_{0}\\in\\mathbb{R}^{n\\times r}\\gets 0\\) Initialize second-order moment \\(V_{0}\\in\\mathbb{R}^{n\\times r}\\gets 0\\) Initialize step \\(t\\gets 0\\) repeat \\(G_{t}\\in\\mathbb{R}^{m\\times n}\\leftarrow-\\nabla_{W}\\varphi_{t}(W_{t})\\) if\\(t\\bmod T=0\\)then \\(U,S,V\\leftarrow\\text{SVD}(G_{t})\\) \\(P_{t}\\gets U[\\cdot,\\cdot r]\\) {Initialize left projector as \\(m\\leq n\\)} else \\(P_{t}\\gets P_{t-1}\\) {Reuse the previous projector} endif \\(R_{t}\\gets P_{t}^{\\top}G_{t}\\) {Project gradient into compact space}\n' +
      '```\n' +
      '\n' +
      '**Algorithm 2**Adam with GaLore\n' +
      '\n' +
      'With this setting, GaLore requires less memory than LoRA during training. As GaLore can always merge \\(\\Delta W_{t}\\) to \\(W_{0}\\) during weight updates, it does not need to store a separate low-rank factorization \\(BA\\). In total, GaLore requires \\((mn+mr+2nr)\\) memory, while LoRA requires\n' +
      '\n' +
      'Figure 2: Learning through low-rank subspaces \\(\\Delta W_{T_{1}}\\) and \\(\\Delta W_{T_{2}}\\) using GaLore. For \\(t_{1}\\in[0,T_{1}-1]\\), \\(W\\) are updated by projected gradients \\(\\tilde{G}_{t_{1}}\\) in a subspace determined by fixed \\(P_{t_{1}}\\) and \\(Q_{t_{1}}\\). After \\(T_{1}\\) steps, the subspace is changed by re-computing \\(P_{t_{2}}\\) and \\(Q_{t_{2}}\\) for \\(t_{2}\\in[T_{1},T_{2}-1]\\), and the process repeats until convergence.\n' +
      '\n' +
      '\\((mn+3mr+3nr)\\) memory. A comparison between GaLore and LoRA is shown in Table 1.\n' +
      '\n' +
      'As Theorem 3.6 does not require the projection matrix to be carefully calibrated, we can further reduce the memory cost of projection matrices by quantization and efficient parameterization, which we leave for future work.\n' +
      '\n' +
      '### Combining with Existing Techniques\n' +
      '\n' +
      'GaLore is compatible with existing memory-efficient optimization techniques. In our work, we mainly consider applying GaLore with 8-bit optimizers (Dettmers et al., 2021) and per-layer weight updates (Lv et al., 2023).\n' +
      '\n' +
      '8-bit optimizers.Dettmers et al. (2022) proposed 8-bit Adam optimizer that maintains 32-bit optimizer performance at a fraction of the original memory footprint. We apply GaLore directly to the existing implementation of 8-bit Adam.\n' +
      '\n' +
      'Per-layer weight updates.In practice, the optimizer typically performs a single weight update for all layers after backpropagation. This is done by storing the entire weight gradients in memory. To further reduce the memory footprint during training, we adopt per-layer weight updates to GaLore, which performs the weight updates during backpropagation (Lv et al., 2023).\n' +
      '\n' +
      '### Hyperparameters of GaLore\n' +
      '\n' +
      'In addition to Adam\'s original hyperparameters, GaLore only introduces very few additional hyperparameters: the rank \\(r\\) which is also present in LoRA, the subspace change frequency \\(T\\) (see Sec. 4.1), and the scale factor \\(\\alpha\\).\n' +
      '\n' +
      'Scale factor \\(\\alpha\\) controls the strength of the low-rank update, which is similar to the scale factor \\(\\alpha/r\\) appended to the low-rank adaptor in Hu et al. (2021). We note that the \\(\\alpha\\) does not depend on the rank \\(r\\) in our case. This is because, when \\(r\\) is small during pre-training, \\(\\alpha/r\\) significantly affects the convergence rate, unlike fine-tuning.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      'We evaluate GaLore on both pre-training and fine-tuning of LLMs. All experiments are conducted on NVIDIA A100 GPUs2.\n' +
      '\n' +
      'Footnote 2: The implementation of GaLore is available here\n' +
      '\n' +
      'Pre-training on C4.To evaluate its performance, we apply GaLore to train LLaMA-based large language models on the C4 dataset. C4 dataset is a colossal, cleaned version of Common Crawl\'s web crawl corpus, which is mainly intended to pre-train language models and word representations (Raffel et al., 2023). To best simulate the practical pre-training scenario, we train without data repetition over a sufficiently large amount of data, across a range of model sizes up to 7 Billion parameters.\n' +
      '\n' +
      'Architecture and hyperparameters.We follow the experiment setup from Lialin et al. (2023), which adopts a LLaMA-based3 architecture with RMSNorm and SwiGLU activations (Touvron et al., 2023; Zhang and Sennrich, 2019; Shazeer, 2020). For each model size, we use the same set of hyperparameters across methods, except the learning rate. We run all experiments with BF16 format to reduce memory usage, and we tune the learning rate for each method under the same amount of computational budget and report the best performance. The details of our task setups and hyperparameters are provided in the appendix.\n' +
      '\n' +
      'Footnote 3: LLaMA materials in our paper are subject to LLaMA community license.\n' +
      '\n' +
      'Fine-tuning on GLUE tasks.GLUE is a benchmark for evaluating the performance of NLP models on a variety of tasks, including sentiment analysis, question answering, and textual entailment (Wang et al., 2019). We use GLUE tasks to benchmark GaLore against LoRA for memory-efficient fine-tuning.\n' +
      '\n' +
      '### Comparison with low-rank methods\n' +
      '\n' +
      'We first compare GaLore with existing low-rank methods using Adam optimizer across a range of model sizes.\n' +
      '\n' +
      'Full-RankOur baseline method that applies Adam optimizer with full-rank weights and optimizer states.\n' +
      '\n' +
      'Low-RankWe also evaluate a traditional low-rank approach that represents the weights by learnable low-rank factorization: \\(W=BA\\)(Kamalakara et al., 2022).\n' +
      '\n' +
      'LoRAHu et al. (2021) proposed LoRA to fine-tune pre-trained models with low-rank adaptors: \\(W=W_{0}+BA\\), where \\(W_{0}\\) is fixed initial weights and \\(BA\\) is a learnable low-rank adaptor. In the case of pre-training, \\(W_{0}\\) is the\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & GaLore & LoRA \\\\ \\hline Weights & \\(mn\\) & \\(mn+mr+nr\\) \\\\ Optim States & \\(mr+2nr\\) & \\(2mr+2nr\\) \\\\ \\hline Multi-Subspace & ✓ & ✗ \\\\ Pre-Training & ✓ & ✗ \\\\ Fine-Tuning & ✓ & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Comparison between GaLore and LoRA. Assume \\(W\\in\\mathbb{R}^{m\\times n}\\) (\\(m\\leq n\\)), rank \\(r\\).\n' +
      '\n' +
      'full-rank initialization matrix. We set LoRA alpha to 32 and LoRA dropout to 0.05 as their default settings.\n' +
      '\n' +
      'ReLoRALialin et al. (2023) is a variant of LoRA designed for pre-training, which periodically merges \\(BA\\) into \\(W\\), and initializes new \\(BA\\) with a reset on optimizer states and learning rate. ReLoRA requires careful tuning of merging frequency, learning rate reset, and optimizer states reset. We evaluate ReLoRA without a full-rank training warmup for a fair comparison.\n' +
      '\n' +
      'For GaLore, we set subspace frequency \\(T\\) to 200 and scale factor \\(\\alpha\\) to 0.25 across all model sizes in Table 2. For each model size, we pick the same rank \\(r\\) for all low-rank methods, and we apply them to all multi-head attention layers and feed-forward layers in the models. We train all models using Adam optimizer with the default hyperparameters (e.g., \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.999\\), \\(\\epsilon=10^{-8}\\)). We also estimate the memory usage based on BF16 format, including the memory for weight parameters and optimizer states. As shown in Table 2, GaLore outperforms other low-rank methods and achieves comparable performance to full-rank training. We note that for 1B model size, GaLore even outperforms full-rank baseline when \\(r=1024\\) instead of \\(r=512\\). Compared to LoRA and ReLoRA, GaLore requires less memory for storing model parameters and optimizer states. A detailed training setting of each model and our memory estimation for each method are provided in the appendix.\n' +
      '\n' +
      '### GaLore with Memory-Efficient Optimizers\n' +
      '\n' +
      'We demonstrate that GaLore can be applied to various learning algorithms, especially memory-efficient optimizers, to further reduce the memory footprint. We apply GaLore to AdamW, 8-bit Adam, and Adafactor optimizers (Loshchilov and Hutter, 2019; Dettmers et al., 2022; Shazeer and Stern). We consider Adafactor with first-order statistics to avoid performance degradation.\n' +
      '\n' +
      'We evaluate them on LLaMA 1B architecture with 10K training steps, and we tune the learning rate for each setting and report the best performance. As shown in Fig. 3, applying GaLore does not significantly affect their convergence. By using GaLore with a rank of 512, the memory footprint is reduced by up to 62.5%, on top of the memory savings from using 8-bit Adam or Adafactor optimizer. Since 8-bit Adam requires less memory than others, we denote 8-bit GaLore as GaLore with 8-bit Adam, and use it as the default method for the following experiments on 7B model pre-training and memory measurement.\n' +
      '\n' +
      '### Scaling up to LLaMA 7B Architecture\n' +
      '\n' +
      'Scaling ability to 7B models is a key factor for demonstrating if GaLore is effective for practical LLM pre-training scenarios. We evaluate GaLore on an LLaMA 7B architecture with an embedding size of 4096 and total layers of 32. We train the model for 150K steps with 19.7B tokens, using 8-node training in parallel with a total of 64 A100 GPUs. Due to computational constraints, we only compare 8-bit GaLore (\\(r=1024\\)) with 8-bit Adam with a single trial without tuning the hyperparameters. As shown in Table 3, after 150K steps, 8-bit GaLore achieves a perplexity of 14.65, which is comparable to 8-bit Adam with a perplexity of 14.61.\n' +
      '\n' +
      '### Memory-Efficient Fine-Tuning\n' +
      '\n' +
      'GaLore not only achieves memory-efficient pre-training but also can be used for memory-efficient fine-tuning. We\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r} \\hline \\hline  & **60M** & **130M** & **350M** & **1B** \\\\ \\hline Full-Rank & 34.06 (0.36G) & 25.08 (0.76G) & 18.80 (2.06G) & 15.56 (7.80G) \\\\ \\hline\n' +
      '**GaLore** & **34.88** (0.24G) & **25.36** (0.52G) & **18.95** (1.22G) & **15.64** (4.38G) \\\\ Low-Rank & 78.18 (0.26G) & 45.51 (0.54G) & 37.41 (1.08G) & 142.53 (3.57G) \\\\ LoRA & 34.99 (0.36G) & 33.92 (0.80G) & 25.58 (1.76G) & 19.21 (6.17G) \\\\ ReLoRA & 37.04 (0.36G) & 29.37 (0.80G) & 29.08 (1.76G) & 18.33 (6.17G) \\\\ \\hline \\(r/d_{model}\\) & 128 / 256 & 256 / 768 & 256 / 1024 & 512 / 2048 \\\\ Training Tokens & 1.1B & 2.2B & 6.4B & 13.1B \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Comparison with low-rank algorithms on pre-training various sizes of LLaMA models on C4 dataset. Validation perplexity is reported, along with a memory estimate of the total of parameters and optimizer states based on BF16 format. The actual memory footprint of GaLore is reported in Fig. 1 and Fig. 4.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r|r r r r} \\hline \\hline  & **Mem** & **40K** & **80K** & **120K** & **150K** \\\\ \\hline\n' +
      '**8-bit GaLore** & 18G & 17.94 & 15.39 & 14.95 & 14.65 \\\\\n' +
      '8-bit Adam & 26G & 18.09 & 15.47 & 14.83 & 14.61 \\\\ \\hline Tokens (B) & & 5.2 & 10.5 & 15.7 & 19.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Pre-training LLaMA 7B on C4 dataset for 150K steps. Validation perplexity and memory estimate are reported.\n' +
      '\n' +
      'fine-tune pre-trained RoBERTa models on GLUE tasks using GaLore and compare its performance with a full fine-tuning baseline and LoRA. We use hyperparameters from Hu et al. (2021) for LoRA and tune the learning rate and scale factor for GaLore. As shown in Table 4, GaLore achieves better performance than LoRA on most tasks with less memory footprint. This demonstrates that GaLore can serve as a full-stack memory-efficient training strategy for both LLM pre-training and fine-tuning.\n' +
      '\n' +
      '### Measurement of Memory and Throughput\n' +
      '\n' +
      'While Table 2 gives the theoretical benefit of GaLore compared to other methods in terms of memory usage, we also measure the actual memory footprint of training LLaMA models by various methods, with a token batch size of 256. The training is conducted on a single device setup without activation checkpointing, memory offloading, and optimizer states partitioning (Rajbhandari et al., 2020).\n' +
      '\n' +
      '**Training 7B models on consumer GPUs with 24G memory.** As shown in Fig. 4, 8-bit GaLore requires significantly less memory than BF16 baseline and 8-bit Adam, and only requires 22.0G memory to pre-train LLaMA 7B with a small per-GPU token batch size (up to 500 tokens). This memory footprint is within 24GB VRAM capacity of a single GPU such as NVIDIA RTX 4090. In addition, when activation checkpointing is enabled, per-GPU token batch size can be increased up to 4096. While the batch size is small per GPU, it can be scaled up with data parallelism, which requires much lower bandwidth for inter-GPU communication, compared to model parallelism. Therefore, it is possible that GaLore can be used for elastic training (Lin et al.) 7B models on consumer GPUs such as RTX 4090s.\n' +
      '\n' +
      'Specifically, we present the memory breakdown in Fig. 1. It shows that 8-bit GaLore reduces 37.92G (63.3%) and 24.5G (52.3%) total memory compared to BF16 Adam baseline and 8-bit Adam, respectively. Compared to 8-bit Adam, 8-bit GaLore mainly reduces the memory in two parts: (1) low-rank gradient projection reduces 9.6G (65.5%) memory of storing optimizer states, and (2) using per-layer weight updates reduces 13.5G memory of storing weight gradients.\n' +
      '\n' +
      '**Throughput overhead of GaLore.** We also measure the throughput of the pre-training LLaMA 1B model with 8-bit GaLore and other methods, where the results can be found in the appendix. Particularly, the current implementation of 8-bit GaLore achieves 1019.63 tokens/second, which induces 17% overhead compared to 8-bit Adam implementation. Disabling per-layer weight updates for GaLore achieves 1109.38 tokens/second, improving the throughput by 8.8%. We note that our results do not require offloading strategies or checkpointing, which can significantly impact training throughput. We leave optimizing the efficiency of GaLore implementation for future work.\n' +
      '\n' +
      '## 6 Ablation Study\n' +
      '\n' +
      '### How many subspaces are needed during pre-training?\n' +
      '\n' +
      'We observe that both too frequent and too slow changes of subspaces hurt the convergence, as shown in Fig. 5(left).\n' +
      '\n' +
      'Figure 4: Memory usage for different methods at various model sizes, evaluated with a token batch size of 256. 8-bit GaLore (retaining grad) disables per-layer weight updates but stores weight gradients during training.\n' +
      '\n' +
      'Figure 3: Applying GaLore to different optimizers for pre-training LLaMA 1B on C4 dataset for 10K steps. Validation perplexity over training steps is reported. We apply GaLore to each optimizer with the rank of 512 and 1024, where the 1B model dimension is 2048.\n' +
      '\n' +
      'The reason has been discussed in Sec. 4.1 and is more prevalent for small \\(r\\), since in such case, the subspace switching should happen at the right time to avoid wasting optimization steps in the wrong subspace, while for large \\(r\\) the gradient updates cover more subspaces, providing more cushion.\n' +
      '\n' +
      '### How does the rank of subspace affect the convergence?\n' +
      '\n' +
      'Within a certain range of rank values, decreasing the rank only slightly affects the convergence rate, causing a slow-down that is close to linear. As shown in Fig. 5(right), training with a rank of 128 using 80K steps achieves a lower loss than training with a rank of 512 using 20K steps. This shows that GaLore can be used to trade-off between memory and computational cost. In a memory-constrained scenario, reducing the rank allows us to stay within the memory budget while training for more steps to preserve the performance.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'We propose GaLore, a memory-efficient pre-training and fine-tuning strategy for large language models. GaLore significantly reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for large-scale LLM pre-training and fine-tuning.\n' +
      '\n' +
      'We identify several open problems for GaLore, which include (1) applying GaLore on training of other types of models such as vision transformers and diffusion models, (2) further improving memory efficiency by employing low-memory projection matrices, through quantization or special parameterization, and (3) exploring the possibility of elastic data distributed training on low-bandwidth consumer-grade hardware.\n' +
      '\n' +
      'We hope that our work will inspire future research on memory-efficient LLM training strategies from the perspective of low-rank gradient projection. We believe that GaLore will be a valuable tool for the community to train large language models with consumer-grade hardware and limited resources.\n' +
      '\n' +
      '## 8 Impact Statement\n' +
      '\n' +
      'This paper aims to improve the memory efficiency of training large language models (LLMs) in order to reduce the environmental impact of LLM pre-training and fine-tuning. By enabling the training of larger models on hardware with lower memory, our approach helps to minimize energy consumption and carbon footprint associated with training LLMs.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Anil et al. (2020) Anil, R., Gupta, V., Koren, T., and Singer, Y. Memory Efficient Adaptive Optimization.\n' +
      '* Chaudhry et al. (2020) Chaudhry, A., Khan, N., Dokania, P., and Torr, P. Continual Learning in Low-rank Orthogonal Subspaces. In _Advances in Neural Information Processing Systems_, volume 33, pp. 9900-9911. Curran Associates, Inc., 2020.\n' +
      '* Chen et al. (2019) Chen, H., Raskutti, G., and Yuan, M. Non-Convex Projected Gradient Descent for Generalized Low-Rank Tensor Regression. _Journal of Machine Learning Research_, 20(5):1-37, 2019. ISSN 1533-7928.\n' +
      '* Chen et al. (2016) Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training Deep Nets with Sublinear Memory Cost, April 2016.\n' +
      '* Chen & Wainwright (2018) Chen, Y. and Wainwright, M. J. Fast low-rank estimation\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c c c|c} \\hline \\hline  & **Memory** & **CoLA** & **STS-B** & **MRPC** & **RTE** & **SST2** & **MNLI** & **QNLI** & **QQP** & **Avg** \\\\ \\hline Full Fine-Tuning & 747M & 62.24 & 90.92 & 91.30 & 79.42 & 94.57 & 87.18 & 92.33 & 92.28 & 86.28 \\\\ \\hline\n' +
      '**GaLore (rank=4)** & 253M & 60.35 & **90.73** & **92.25** & **79.42** & **94.04** & **87.00** & **92.24** & 91.06 & **85.89** \\\\ LoRA (rank=4) & 257M & **61.38** & 90.57 & 91.07 & 78.70 & 92.89 & 86.82 & 92.18 & **91.29** & 85.61 \\\\ \\hline\n' +
      '**GaLore (rank=8)** & 257M & 60.06 & **90.82** & **92.01** & **79.78** & **94.38** & **87.17** & 92.20 & 91.11 & **85.94** \\\\ LoRA (rank=8) & 264M & **61.83** & 90.80 & 91.90 & 79.06 & 93.46 & 86.94 & **92.25** & **91.22** & 85.93 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Evaluating GaLore for memory-efficient fine-tuning on GLUE benchmark using pre-trained RoBERTa-Base. We report the average score of all tasks.\n' +
      '\n' +
      'Figure 5: Ablation study of GaLore on 130M models. **Left:** varying subspace update frequency \\(T\\). **Right:** varying subspace rank and training iterations.\n' +
      '\n' +
      'by projected gradient descent: General statistical and algorithmic guarantees, September 2015.\n' +
      '* Chowdhery et al. (2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. PaLM: Scaling Language Modeling with Pathways, October 2022.\n' +
      '* Dettmers et al. (2021) Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L. 8-bit Optimizers via Block-wise Quantization. _arXiv:2110.02861 [cs]_, October 2021.\n' +
      '* Dettmers et al. (2022) Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L. 8-bit Optimizers via Block-wise Quantization, June 2022.\n' +
      '* Dettmers et al. (2023) Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. QLoRA: Efficient Finetuning of Quantized LLMs, May 2023.\n' +
      '* Ding et al. (2022) Ding, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y., Hu, S., Chen, Y., Chan, C.-M., Chen, W., Yi, J., Zhao, W., Wang, X., Liu, Z., Zheng, H.-T., Chen, J., Liu, Y., Tang, J., Li, J., and Sun, M. Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pretrained Language Models, March 2022.\n' +
      '* Gur-Ari et al. (2018) Gur-Ari, G., Roberts, D. A., and Dyer, E. Gradient Descent Happens in a Tiny Subspace, December 2018.\n' +
      '* Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-Rank Adaptation of Large Language Models, October 2021.\n' +
      '* Kamalakara et al. (2022) Kamalakara, S. R., Locatelli, A., Venkitesh, B., Ba, J., Gal, Y., and Gomez, A. N. Exploring Low Rank Training of Deep Neural Networks, September 2022.\n' +
      '* Kingma & Ba (2014) Kingma, D. P. and Ba, J. Adam: A Method for Stochastic Optimization. _arXiv:1412.6980 [cs]_, December 2014.\n' +
      '* Larsen et al. (2022) Larsen, B. W., Fort, S., Becker, N., and Ganguli, S. How many degrees of freedom do we need to train deep networks: A loss landscape perspective, February 2022.\n' +
      '* Lee & Choi (2018) Lee, Y. and Choi, S. Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace, June 2018.\n' +
      '* Li et al. (2023) Li, B., Chen, J., and Zhu, J. Memory Efficient Optimizers with 4-bit States. [https://arxiv.org/abs/2309.01507v3](https://arxiv.org/abs/2309.01507v3), September 2023.\n' +
      '* Lialin et al. (2023) Lialin, V., Shivagunde, N., Muckatira, S., and Rumshisky, A. ReLoRA: High-Rank Training Through Low-Rank Updates, December 2023.\n' +
      '* Lin et al. (2019) Lin, H., Zhang, H., Ma, Y., He, T., Zhang, Z., Zha, S., and Li, M. Dynamic Mini-batch SGD for Elastic Distributed Training: Learning in the Limbo of Resources. URL [http://arxiv.org/abs/1904.12043](http://arxiv.org/abs/1904.12043).\n' +
      '* Loshchilov & Hutter (2019) Loshchilov, I. and Hutter, F. Decoupled Weight Decay Regularization, January 2019.\n' +
      '* Lv et al. (2023) Lv, K., Yang, Y., Liu, T., Gao, Q., Guo, Q., and Qiu, X. Full Parameter Fine-tuning for Large Language Models with Limited Resources, June 2023.\n' +
      '* Raffel et al. (2023) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, September 2023.\n' +
      '* Rajbhandari et al. (2020) Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models, May 2020.\n' +
      '* Renduchintala et al. (2023) Renduchintala, A., Konuk, T., and Kuchaiev, O. Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying, November 2023.\n' +
      '* Shazeer (2020) Shazeer, N. GLU Variants Improve Transformer, February 2020.\n' +
      '* Shazeer & Stern (2020) Shazeer, N. and Stern, M. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost.\n' +
      '* Sheng et al. (2023) Sheng, Y., Cao, S., Li, D., Hooper, C., Lee, N., Yang, S., Chou, C., Zhu, B., Zheng, L., Keutzer, K., Gonzalez, J. E., and Stoica, I. S-LoRA: Serving Thousands of Concurrent LoRA Adapters, November 2023.\n' +
      '* Tian et al. (2020) Tian, Y., Yu, L., Chen, X., and Ganguli, S. Understanding self-supervised learning with dual deep networks. _arXiv preprint arXiv:2010.00578_, 2020.\n' +
      '* Tian et al. (2024) Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. _ICLR_, 2024.\n' +
      '* Touvron et al. (2020) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J.,Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Scheletn, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023.\n' +
      '* Wang et al. (2019) Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding, February 2019.\n' +
      '* Wang et al. (2023) Wang, Y., Lin, Y., Zeng, X., and Zhang, G. MultiLoRA: Democratizing LoRA for Better Multi-Task Learning, November 2023.\n' +
      '* Xia et al. (2024) Xia, W., Qin, C., and Hazan, E. Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning, January 2024.\n' +
      '* Zhang & Sennrich (2019) Zhang, B. and Sennrich, R. Root Mean Square Layer Normalization, October 2019.\n' +
      '\n' +
      '## Appendix A Proofs\n' +
      '\n' +
      '### Gradient becomes low-rank\n' +
      '\n' +
      '**Lemma A.1** (Gradient becomes low-rank during training).: _Let \\(m\\leq n\\) without loss of generality. The gradient update:_\n' +
      '\n' +
      '\\[G_{t}=A-BW_{t}C,\\quad W_{t}=W_{t-1}+\\eta G_{t-1} \\tag{6}\\]\n' +
      '\n' +
      '_with constant \\(A\\) and PSD matrices \\(B\\) and \\(C\\) and randomly initialized \\(W_{0}\\) leads to low-rank gradient with high probability:_\n' +
      '\n' +
      '\\[\\text{stable-rank}(G_{t})\\leq 1+\\sum_{i=2}^{m}O\\left(\\frac{1-\\eta\\lambda_{i} \\nu_{1}}{1-\\eta\\lambda_{1}\\nu_{1}}\\right)^{2t} \\tag{7}\\]\n' +
      '\n' +
      '_Here \\(\\nu_{1}=\\lambda_{\\min}(C)\\) is the smallest eigenvalues of \\(C\\) and \\(\\lambda_{1}\\leq\\ldots\\leq\\lambda_{n}\\) are eigenvalues of \\(B\\). Furthermore, if \\(\\lambda_{2}>\\lambda_{1}\\) and \\(\\nu_{1}>0\\), then \\(G_{t}\\) converges to rank-\\(1\\) exponentially._\n' +
      '\n' +
      'Proof.: We have\n' +
      '\n' +
      '\\[G_{t}=A-BW_{t}C=A-B(W_{t-1}+\\eta G_{t-1})C=G_{t-1}-\\eta BG_{t-1}C \\tag{18}\\]\n' +
      '\n' +
      'Let \\(B=UD_{B}U^{\\top}\\) and \\(C=VD_{C}V^{\\top}\\) be the eigen decomposition of \\(B\\) and \\(C\\). \\(D_{B}=\\operatorname{diag}(\\lambda_{1},\\ldots,\\lambda_{m})\\) and \\(D_{C}=\\operatorname{diag}(\\nu_{1},\\ldots,\\nu_{n})\\) are their eigenvalues sorted in ascending orders (i.e., \\(\\lambda_{1}\\leq\\ldots\\leq\\lambda_{m}\\) and \\(\\nu_{1}\\leq\\ldots\\leq\\nu_{n}\\)). Define \\(H_{t}:=U^{\\top}G_{t}V\\). It is clear that \\(\\operatorname{rank}(H_{t})=\\operatorname{rank}(G_{t})\\) and we have:\n' +
      '\n' +
      '\\[H_{t}:=U^{\\top}G_{t}V=H_{t-1}-\\eta D_{B}H_{t-1}D_{C} \\tag{19}\\]\n' +
      '\n' +
      'Suppose \\(h_{t,ij}\\) is the \\(ij\\) component of \\(H_{t}\\), then from the equation above we have:\n' +
      '\n' +
      '\\[h_{t,ij}=h_{t-1,ij}-\\eta\\lambda_{i}\\nu_{j}h_{t-1,ij}=(1-\\eta\\lambda_{i}\\nu_{j}) h_{t-1,ij}=(1-\\eta\\lambda_{i}\\nu_{j})^{t}h_{0,ij} \\tag{20}\\]\n' +
      '\n' +
      'Then for first few rows \\(i\\) and columns \\(j\\) that correspond to large eigenvalues, \\(h_{t,ij}\\to 0\\) quickly and \\(\\operatorname{rank}(H_{t})\\) becomes small.\n' +
      '\n' +
      'To make it more precise, consider the stable rank:\n' +
      '\n' +
      '\\[\\text{stable-rank}(G_{t})=\\text{stable-rank}(H_{t})=\\frac{\\|H_{t}\\|_{F}^{2}}{ \\|H_{t}\\|_{2}^{2}} \\tag{21}\\]\n' +
      '\n' +
      'Then we have:\n' +
      '\n' +
      '\\[\\|H_{t}\\|_{F}^{2}=\\sum_{i=1}^{m}\\sum_{j=1}^{n}(1-\\eta\\lambda_{i}\\nu_{j})^{2t}h _{0,ij}^{2} \\tag{22}\\]\n' +
      '\n' +
      'and\n' +
      '\n' +
      '\\[\\|H_{t}\\|_{2}^{2}\\geq\\sum_{j=1}^{n}H_{t,1j}^{2}=\\sum_{j=1}^{n}(1-\\eta\\lambda_ {1}\\nu_{j})^{2t}h_{0,1j}^{2} \\tag{23}\\]\n' +
      '\n' +
      'With high probability, \\(h_{0,1j}^{2}\\geq\\epsilon_{0}^{2}\\), since \\(|h_{1i}^{2}|\\leq c_{0}\\) is bounded, we have:\n' +
      '\n' +
      '\\[\\text{stable-rank}(G_{t})\\leq 1+\\frac{c_{0}^{2}}{\\epsilon_{0}^{2}}\\sum_{i=2}^{m} \\frac{\\sum_{j=1}^{n}(1-\\eta\\lambda_{i}\\nu_{j})^{2t}}{\\sum_{j=1}^{n}(1-\\eta \\lambda_{1}\\nu_{j})^{2t}} \\tag{24}\\]\n' +
      '\n' +
      'Using Mediant inequality, \\(\\frac{a}{b}\\leq\\frac{a+c}{b+d}\\leq\\frac{c}{d}\\) for \\(a,b,c,d>0\\), therefore, we know that for \\(i\\)-th row (\\(i\\geq 2\\)), since \\(\\lambda_{i}\\geq\\lambda_{1}\\):\n' +
      '\n' +
      '\\[\\frac{\\sum_{j=1}^{n}(1-\\eta\\lambda_{i}\\nu_{j})^{2t}}{\\sum_{j=1}^{n}(1-\\eta \\lambda_{1}\\nu_{j})^{2t}}\\leq\\max_{j}\\left(\\frac{1-\\eta\\lambda_{i}\\nu_{j}}{1- \\eta\\lambda_{1}\\nu_{j}}\\right)^{2t}=\\left(\\frac{1-\\eta\\lambda_{i}\\nu_{1}}{1- \\eta\\lambda_{1}\\nu_{1}}\\right)^{2t}\\leq 1 \\tag{25}\\]\n' +
      '\n' +
      'and the conclusion follows.\n' +
      '\n' +
      '### Reversibility\n' +
      '\n' +
      '**Definition A.2** (Reversiblity (Tian et al., 2020)).: A network \\(\\mathcal{N}\\) that maps input \\(\\mathbf{x}\\) to output \\(\\mathbf{y}=\\mathcal{N}(\\mathbf{x})\\) is _reversible_, if there exists \\(K(\\mathbf{x};W)\\) so that \\(\\mathbf{y}=K(\\mathbf{x};W)\\mathbf{x}\\), and the backpropagated gradient \\(\\mathbf{g}_{\\mathbf{x}}\\) satisfies \\(\\mathbf{g}_{\\mathbf{x}}=K^{\\top}(\\mathbf{x};W)\\mathbf{g}_{\\mathbf{y}}\\), where \\(\\mathbf{g}_{\\mathbf{y}}\\) is the backpropagated gradient at the output \\(\\mathbf{y}\\). Here \\(K(\\mathbf{x};W)\\) depends on the input \\(\\mathbf{x}\\) and weight \\(W\\) in the network \\(\\mathcal{N}\\).\n' +
      '\n' +
      'Note that many layers are reversible, including linear layer (without bias), reversible activations (e.g., ReLU, leaky ReLU, polynomials, etc). Furthermore, they can be combined to construct more complicated architectures:\n' +
      '\n' +
      '**Property 1**.: _If \\(\\mathcal{N}_{1}\\) and \\(\\mathcal{N}_{2}\\) are reversible networks, then (**Parallel**) \\(\\mathbf{y}=\\alpha_{1}\\mathcal{N}_{1}(\\mathbf{x})+\\alpha_{2}\\mathcal{N}_{2}(\\mathbf{x})\\) is reversible for constants \\(\\alpha_{1}\\) and \\(\\alpha_{2}\\), and (**Composition**) \\(\\mathbf{y}=\\mathcal{N}_{2}(\\mathcal{N}_{1}(\\mathbf{x}))\\) is reversible._\n' +
      '\n' +
      'From this property, it is clear that ResNet architecture \\(\\mathbf{x}+\\mathcal{N}(\\mathbf{x})\\) is reversible, if \\(\\mathcal{N}\\) contains bias-free linear layers and reversible activations, which is often the case in practice. For a detailed analysis, please check Appendix A in (Tian et al., 2020). For architectures like self-attention, one possibility is to leverage JoMA (Tian et al., 2024) to analyze, and we leave for future work.\n' +
      '\n' +
      'The gradient of chained reversible networks has the following structure:\n' +
      '\n' +
      '**Theorem 3.2** (Gradient Form of reversible models).: _In a chained reversible neural network \\(\\mathcal{N}(\\mathbf{x}):=\\mathcal{N}_{L}(\\mathcal{N}_{L-1}(\\dots\\mathcal{N}_{1}( \\mathbf{x})))\\) with \\(\\ell_{2}\\)-objective \\(\\varphi:=\\frac{1}{2}\\|\\mathbf{y}-\\mathcal{N}(\\mathbf{x})\\|_{2}^{2}\\), the weight matrix \\(W_{l}\\) at layer \\(l\\) has gradient \\(G_{l}\\) of the following form for batchsize 1:_\n' +
      '\n' +
      '\\[G_{l}=\\underbrace{J_{l}^{\\top}\\mathbf{y}\\mathbf{f}_{l-1}^{\\top}}_{\\Lambda}- \\underbrace{J_{l}^{\\top}J_{l}}_{\\hat{\\mathbf{g}}}W_{l}\\underbrace{\\mathbf{f}_{l-1}\\bm {f}_{l-1}^{\\top}}_{C} \\tag{8}\\]\n' +
      '\n' +
      '_where \\(J_{l}:=\\operatorname{Jacobian}(\\mathcal{N}_{L})\\dots\\operatorname{Jacobian}( \\mathcal{N}_{l+1})\\) and \\(\\mathbf{f}_{l}:=\\mathcal{N}_{l}(\\mathcal{N}_{l-1}\\dots\\mathcal{N}_{1}(\\mathbf{x}))\\)._\n' +
      '\n' +
      'Proof.: Note that for layered reversible network, we have\n' +
      '\n' +
      '\\[\\mathcal{N}(\\mathbf{x})=\\mathcal{N}_{L}(\\mathcal{N}_{L-1}(...\\mathcal{N}_{1}(\\mathbf{ x})))=K_{L}(\\mathbf{x})K_{L-1}(\\mathbf{x})\\dots K_{1}(\\mathbf{x})\\mathbf{x} \\tag{26}\\]\n' +
      '\n' +
      'Let \\(\\mathbf{f}_{l}:=\\mathcal{N}_{l}(\\mathcal{N}_{l-1}(\\dots\\mathcal{N}_{1}(\\mathbf{x})))\\) and \\(J_{l}:=K_{L}(\\mathbf{x})\\dots K_{l+1}(\\mathbf{x})\\), and for linear layer \\(l\\), we can write \\(\\mathcal{N}(\\mathbf{x})=J_{l}W_{l}\\mathbf{f}_{l-1}\\). Therefore, for the linear layer \\(l\\) with weight matrix \\(W_{l}\\), we have:\n' +
      '\n' +
      '\\[\\mathrm{d}\\varphi =(\\mathbf{y}-\\mathcal{N}(\\mathbf{x}))^{\\top}\\mathcal{\\mathrm{d}}\\mathcal{ N}(\\mathbf{x}) \\tag{27}\\] \\[=(\\mathbf{y}-\\mathcal{N}(\\mathbf{x}))^{\\top}K_{L}(\\mathbf{x})\\dots K_{l+1}( \\mathbf{x})\\mathrm{d}W_{l}\\mathbf{f}_{l-1}\\ \\ +\\ \\ \\text{terms not related to}\\ \\mathrm{d}W_{l}\\] (28) \\[=(\\mathbf{y}-J_{l}W_{l}\\mathbf{f}_{l-1})^{\\top}J_{l}\\mathrm{d}W_{l}\\mathbf{ f}_{l-1}\\] (29) \\[=\\mathrm{tr}(\\mathrm{d}W_{l}^{\\top}J_{l}^{\\top}(\\mathbf{y}-J_{l}W_{l }\\mathbf{f}_{l-1})\\mathbf{f}_{l-1}^{\\top}) \\tag{30}\\]\n' +
      '\n' +
      'This gives the gradient of \\(W_{l}\\):\n' +
      '\n' +
      '\\[G_{l}=J_{l}^{\\top}\\mathbf{y}\\mathbf{f}_{l-1}^{\\top}-J_{l}^{\\top}J_{l}W_{l}\\mathbf{f}_{l-1} \\mathbf{f}_{l-1}^{\\top} \\tag{31}\\]\n' +
      '\n' +
      '**Lemma A.3** (Gradient structure of softmax loss).: _For \\(K\\)-way logsoftmax loss \\(\\varphi(\\mathbf{y};\\mathbf{f}):=-\\log\\left(\\frac{\\exp(\\mathbf{y}^{\\top}\\mathbf{f})}{\\mathbf{1}^{ \\top}\\exp(\\mathbf{f})}\\right)\\), let \\(\\hat{\\mathbf{f}}=P_{\\mathbf{1}}^{\\perp}\\mathbf{f}\\) be the zero-mean version of network output \\(\\mathbf{f}\\), where \\(P_{\\mathbf{1}}^{\\perp}:=I-\\frac{1}{K}\\mathbf{1}\\mathbf{1}^{\\top}\\), then we have:_\n' +
      '\n' +
      '\\[-\\mathrm{d}\\varphi=\\mathbf{y}^{\\top}\\mathrm{d}\\hat{\\mathbf{f}}-\\gamma\\hat{\\mathbf{f}}^{ \\top}\\mathrm{d}\\hat{\\mathbf{f}}/K+O(\\hat{\\mathbf{f}}^{2}/K)\\mathrm{d}\\hat{\\mathbf{f}} \\tag{9}\\]\n' +
      '\n' +
      '_where \\(\\gamma(\\mathbf{y},\\mathbf{f})\\approx 1\\) and \\(\\mathbf{y}\\) is a data label with \\(\\mathbf{y}^{\\top}\\mathbf{1}=1\\)._\n' +
      '\n' +
      'Proof.: Let \\(\\hat{\\mathbf{f}}:=P_{\\mathbf{1}}^{\\perp}\\mathbf{f}\\) be the zero-mean version of network output \\(\\mathbf{f}\\). Then we have \\(\\mathbf{1}^{\\top}\\hat{\\mathbf{f}}=0\\) and \\(\\mathbf{f}=\\hat{\\mathbf{f}}+c\\mathbf{1}\\). Therefore, we have:\n' +
      '\n' +
      '\\[-\\varphi=\\log\\left(\\frac{\\exp(c)\\exp(\\mathbf{y}^{\\top}\\hat{\\mathbf{f}})}{\\exp(c)\\mathbf{1}^{ \\top}\\exp(\\hat{\\mathbf{f}})}\\right)=\\mathbf{y}^{\\top}\\hat{\\mathbf{f}}-\\log(\\mathbf{1}^{\\top} \\exp(\\hat{\\mathbf{f}})) \\tag{32}\\]Using the Taylor expansion \\(\\exp(x)=1+x+\\frac{x^{2}}{2}+o(x^{2})\\), we have:\n' +
      '\n' +
      '\\[\\mathbf{1}^{\\top}\\exp(\\hat{\\mathbf{f}}) =\\mathbf{1}^{\\top}(\\mathbf{1}+\\hat{\\mathbf{f}}+\\frac{1}{2}\\hat{\\mathbf{f}} ^{2})+o(\\hat{\\mathbf{f}}^{2})=K(1+\\hat{\\mathbf{f}}^{\\top}\\hat{\\mathbf{f}}/2K+o(\\hat{\\mathbf{f}}^ {2}/K)) \\tag{33}\\]\n' +
      '\n' +
      'So\n' +
      '\n' +
      '\\[-\\varphi =\\mathbf{y}^{\\top}\\hat{\\mathbf{f}}-\\log(1+\\hat{\\mathbf{f}}^{\\top}\\hat{\\mathbf{f}}/ 2K+o(\\hat{\\mathbf{f}}^{2}/K))-\\log K \\tag{34}\\]\n' +
      '\n' +
      'Therefore\n' +
      '\n' +
      '\\[-\\mathrm{d}\\varphi =\\mathbf{y}^{\\top}\\mathrm{d}\\hat{\\mathbf{f}}-\\frac{\\gamma}{K}\\hat{\\mathbf{f} }^{\\top}\\mathrm{d}\\hat{\\mathbf{f}}+O\\left(\\frac{\\hat{\\mathbf{f}}^{2}}{K}\\right) \\mathrm{d}\\hat{\\mathbf{f}} \\tag{35}\\]\n' +
      '\n' +
      'where \\(\\gamma:=(1+\\hat{\\mathbf{f}}^{\\top}\\hat{\\mathbf{f}}/2K+o(\\hat{\\mathbf{f}}^{2}/K))^{-1}\\approx 1\\). \n' +
      '\n' +
      '### Convergence of GaLore\n' +
      '\n' +
      '**Theorem 3.6** (Convergence of GaLore with fixed projections).: _Suppose the gradient has the following form (Eqn. 8 with batchsize \\(>1\\)):_\n' +
      '\n' +
      '\\[G=\\sum_{i}A_{i}-\\sum_{i}B_{i}WC_{i} \\tag{12}\\]\n' +
      '\n' +
      '_where \\(B_{i}\\) and \\(C_{i}\\) are PSD matrices, \\(A_{i}\\), \\(B_{i}\\) and \\(C_{i}\\) have \\(L_{A}\\), \\(L_{B}\\) and \\(L_{C}\\) continuity with respect to \\(W\\) and \\(\\|W_{t}\\|\\leq D\\). Let \\(R_{t}:=P_{t}^{\\top}G_{t}Q_{t}\\), \\(\\hat{B}_{it}:=P_{t}^{\\top}B_{i}(W_{t})P_{t}\\), \\(\\hat{C}_{it}:=Q_{t}^{\\top}C_{i}(W_{t})Q_{t}\\) and \\(\\kappa_{t}:=\\frac{1}{N}\\sum_{i}\\lambda_{\\min}(\\hat{B}_{it})\\lambda_{\\min}(\\hat{ C}_{it})\\). If we choose constant \\(P_{t}=P\\) and \\(Q_{t}=Q\\), then GaLore with \\(\\rho_{t}\\equiv 1\\) satisfies:_\n' +
      '\n' +
      '\\[\\|R_{t}\\|_{F} \\leq\\left[1-\\eta(\\kappa_{t-1}-L_{A}-L_{B}L_{C}D^{2})\\right]\\|R_{t -1}\\|_{F} \\tag{13}\\]\n' +
      '\n' +
      '_As a result, if \\(\\min_{t}\\kappa_{t}>L_{A}+L_{B}L_{C}D^{2}\\), \\(R_{t}\\to 0\\) and thus GaLore converges with fixed \\(P_{t}\\) and \\(Q_{t}\\)._\n' +
      '\n' +
      'Proof.: Using \\(\\mathrm{vec}(AXB)=(B^{\\top}\\otimes A)\\mathrm{vec}(X)\\) where \\(\\otimes\\) is the Kronecker product, the gradient assumption can be written as the following:\n' +
      '\n' +
      '\\[g_{t}=a_{t}-S_{t}w_{t} \\tag{36}\\]\n' +
      '\n' +
      'where \\(g_{t}:=\\mathrm{vec}(G_{t})\\in\\mathbb{R}^{mn}\\), \\(w_{t}:=\\mathrm{vec}(W_{t})\\in\\mathbb{R}^{mn}\\) be the vectorized versions of \\(G_{t}\\) and \\(W_{t}\\), \\(a_{t}:=\\frac{1}{N}\\sum_{i}\\mathrm{vec}(A_{it})\\) and \\(S_{t}=\\frac{1}{N}\\sum_{i}C_{it}\\otimes B_{it}\\) are \\(mn\\)-by-\\(mn\\) PSD matrix.\n' +
      '\n' +
      'Using the same notation, it is clear to show that:\n' +
      '\n' +
      '\\[(Q\\otimes P)^{\\top}g_{t} =(Q^{\\top}\\otimes P^{\\top})\\mathrm{vec}(G_{t})=\\mathrm{vec}(P^{ \\top}G_{t}Q)=\\mathrm{vec}(R_{t})=:r_{t} \\tag{37}\\] \\[\\tilde{g}_{t} :=\\mathrm{vec}(\\tilde{G}_{t})=\\mathrm{vec}(PP^{\\top}G_{t}QQ^{\\top })=(Q\\otimes P)\\mathrm{vec}(R_{t})=(Q\\otimes P)r_{t} \\tag{38}\\]\n' +
      '\n' +
      'Then we derive the recursive update rule for \\(g_{t}\\):\n' +
      '\n' +
      '\\[g_{t} =a_{t}-S_{t}w_{t} \\tag{39}\\] \\[=(a_{t}-a_{t-1})+(S_{t-1}-S_{t})w_{t}+a_{t-1}-S_{t-1}w_{t}\\] (40) \\[=e_{t}+a_{t-1}-S_{t-1}(w_{t-1}+\\eta\\tilde{g}_{t-1})\\] (41) \\[=e_{t}+g_{t-1}-\\eta S_{t-1}\\tilde{g}_{t-1} \\tag{42}\\]\n' +
      '\n' +
      'where \\(e_{t}:=(a_{t}-a_{t-1})+(S_{t-1}-S_{t})w_{t}\\). Left multiplying by \\((Q\\otimes P)^{\\top}\\), we have:\n' +
      '\n' +
      '\\[r_{t}=(Q\\otimes P)^{\\top}e_{t}+r_{t-1}-\\eta(Q\\otimes P)^{\\top}S_{t-1}(Q \\otimes P)r_{t-1} \\tag{43}\\]Let\n' +
      '\n' +
      '\\[\\hat{S}_{t}:=(Q\\otimes P)^{\\top}S_{t}(Q\\otimes P)=\\frac{1}{N}\\sum_{i}(Q\\otimes P )^{\\top}(C_{it}\\otimes B_{it})(Q\\otimes P)=\\frac{1}{N}\\sum_{i}(Q^{\\top}C_{it}Q) \\otimes(P^{\\top}B_{it}P) \\tag{44}\\]\n' +
      '\n' +
      'Then we have:\n' +
      '\n' +
      '\\[r_{t}=(I-\\eta\\hat{S}_{t-1})r_{t-1}+(Q\\otimes P)^{\\top}e_{t} \\tag{45}\\]\n' +
      '\n' +
      'Now we bound the norm. Note that since \\(P\\) and \\(Q\\) are projection matrices with \\(P^{\\top}P=I\\) and \\(Q^{\\top}Q=I\\), we have:\n' +
      '\n' +
      '\\[\\|(Q\\otimes P)^{\\top}e_{t}\\|_{2}=\\|\\mathrm{vec}(P^{\\top}E_{t}Q)\\|_{2}=\\|P^{ \\top}E_{t}Q\\|_{F}\\leq\\|E_{t}\\|_{F} \\tag{46}\\]\n' +
      '\n' +
      'where \\(E_{t}:=\\frac{1}{N}\\sum_{i}(A_{it}-A_{i,t-1})+\\frac{1}{N}\\sum_{i}(B_{i,t-1}W_{t }C_{i,t-1}-B_{it}W_{t}C_{it})\\). So we only need to bound \\(\\|E_{t}\\|_{F}\\). Note that:\n' +
      '\n' +
      '\\[\\|A_{t}-A_{t-1}\\|_{F} \\leq L_{A}\\|W_{t}-W_{t-1}\\|_{F}=\\eta L_{A}\\|\\tilde{G}_{t-1}\\|_{F} \\leq\\eta L_{A}\\|R_{t-1}\\|_{F} \\tag{47}\\] \\[\\|(B_{t}-B_{t-1})W_{t}C_{t-1}\\|_{F} \\leq L_{B}\\|W_{t}-W_{t-1}\\|_{F}\\|W_{t}\\|_{F}\\|C_{t-1}\\|_{F}=\\eta L _{B}L_{C}D^{2}\\|R_{t-1}\\|_{F}\\] (48) \\[\\|B_{t}W_{t}(C_{t-1}-C_{t})\\|_{F} \\leq L_{C}\\|B_{t}\\|_{F}\\|W_{t}\\|_{F}\\|W_{t-1}-W_{t}\\|_{F}=\\eta L _{B}L_{C}D^{2}\\|R_{t-1}\\|_{F} \\tag{49}\\]\n' +
      '\n' +
      'Now we estimate the minimal eigenvalue of \\(\\hat{S}_{t-1}\\). Let \\(\\underline{\\lambda}_{it}:=\\lambda_{\\min}(P^{\\top}B_{it}P)\\) and \\(\\underline{\\nu}_{it}:=\\lambda_{\\min}(Q^{\\top}C_{it}Q)\\), then \\(\\lambda_{\\min}((P^{\\top}B_{it}P)\\otimes(Q^{\\top}C_{it}Q))=\\underline{\\lambda} _{it}\\underline{\\nu}_{it}\\) and for any unit vector \\(\\mathbf{\\nu}\\):\n' +
      '\n' +
      '\\[\\mathbf{\\upsilon}^{\\top}\\hat{S}_{t}\\mathbf{v}=\\frac{1}{N}\\sum_{i}\\mathbf{\\upsilon}^{\\top} \\left[\\left(P^{\\top}B_{it}P\\right)\\otimes(Q^{\\top}C_{it}Q)\\right]\\mathbf{v}\\geq \\frac{1}{N}\\sum_{i}\\underline{\\lambda}_{it}\\underline{\\nu}_{it} \\tag{50}\\]\n' +
      '\n' +
      'And thus \\(\\lambda_{\\min}(\\hat{S}_{t})\\geq\\frac{1}{N}\\sum_{i}\\underline{\\lambda}_{it} \\underline{\\nu}_{it}\\). Therefore, \\(\\lambda_{\\max}(I-\\eta\\hat{S}_{t-1})\\leq 1-\\frac{\\eta}{N}\\sum_{i} \\underline{\\lambda}_{i,t-1}\\underline{\\nu}_{i,t-1}\\). Therefore, let \\(\\kappa_{t}:=\\frac{1}{N}\\sum_{i}\\underline{\\lambda}_{it}\\underline{\\nu}_{it}\\) and using the fact that \\(\\|r_{t}\\|_{2}=\\|R_{t}\\|_{F}\\), we have:\n' +
      '\n' +
      '\\[\\|R_{t}\\|_{F}\\leq\\left[1-\\eta(\\kappa_{t-1}-L_{A}-2L_{B}L_{C}D^{2})\\right]\\|R_ {t-1}\\|_{F} \\tag{51}\\]\n' +
      '\n' +
      'and the conclusion follows. \n' +
      '\n' +
      '## Appendix B Details of Pre-Training Experiment\n' +
      '\n' +
      '### Architecture and Hyperparameters\n' +
      '\n' +
      'We introduce details of the LLaMA architecture and hyperparameters used for pre-training. Table 5 shows the most hyperparameters of LLaMA models across model sizes. We use a max sequence length of 256 for all models, with a batch size of 131K tokens. For all experiments, we adopt learning rate warmup for the first 10% of the training steps, and use cosine annealing for the learning rate schedule, decaying to 10% of the initial learning rate.\n' +
      '\n' +
      'For all methods on each size of models (from 60M to 1B), we tune their favorite learning rate from a set of \\(\\{0.01,0.005,0.001,0.0005,0.0001\\}\\), and the best learning rate is chosen based on the validation perplexity. We find GaLore is insensitive to hyperparameters and tends to be stable with the same learning rate across different model sizes. For all models, GaLore use the same hyperparameters, including the learning rate of \\(0.01\\), scale factor \\(\\alpha\\) of \\(0.25\\), and the subspace change frequency of \\(T\\) of \\(200\\). We note that since \\(\\alpha\\) can be viewed as a fractional learning rate, most of the modules (e.g., multi-head attention and feed-forward layers) in LLaMA models have the actual learning rate of \\(0.0025\\). This is, still, a relatively large stable learning rate compared to the full-rank baseline, which usually uses a learning rate \\(\\leq 0.001\\) to avoid spikes in the training loss.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline Params & Hidden & Intermediate & Heads & Layers & Steps & Data amount \\\\ \\hline\n' +
      '60M & 512 & 1376 & 8 & 8 & 10K & \\(1.3\\,\\mathrm{B}\\) \\\\\n' +
      '130M & 768 & 2048 & 12 & 12 & 20K & \\(2.6\\,\\mathrm{B}\\) \\\\\n' +
      '350M & 1024 & 2736 & 16 & 24 & 60K & \\(7.8\\,\\mathrm{B}\\) \\\\ \\(1\\,\\mathrm{B}\\) & 2048 & 5461 & 24 & 32 & 100K & \\(13.1\\,\\mathrm{B}\\) \\\\ \\(7\\,\\mathrm{B}\\) & 4096 & 11008 & 32 & 32 & 150K & \\(19.7\\,\\mathrm{B}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Hyperparameters of LLaMA models for evaluation. Data amount are specified in tokens.\n' +
      '\n' +
      '### Memory Estimates\n' +
      '\n' +
      'As the GPU memory usage for a specific component is hard to measure directly, we estimate the memory usage of the weight parameters and optimizer states for each method on different model sizes. The estimation is based on the number of original parameters and the number of low-rank parameters, trained by BF16 format. For example, for a 60M model, LoRA (\\(r=128\\)) requires \\(42.7\\)M parameters on low-rank adaptors and \\(60M\\) parameters on the original weights, resulting in a memory cost of \\(0.20\\)G for weight parameters and \\(0.17\\)G for optimizer states. Table 6 shows the memory estimates for weight parameters and optimizer states for different methods on different model sizes, as a compliment to the total memory reported in the main text.\n' +
      '\n' +
      '## Appendix C Details of Fine-Tuning Experiment\n' +
      '\n' +
      'We fine-tune the pre-trained RoBERTa-Base model on the GLUE benchmark using the model provided by the Hugging Face1. We trained the model for 30 epochs with a batch size of 16 for all tasks except for CoLA, which uses a batch size of 32. We tune the learning rate and scale factor for GaLore. Table 7 shows the hyperparameters used for fine-tuning RoBERTa-Base for GaLore.\n' +
      '\n' +
      'Footnote 1: [https://huggingface.co/transformers/model_doc/roberta.html](https://huggingface.co/transformers/model_doc/roberta.html)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 6: Memory estimates for weight parameters and optimizer states.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 7: Hyperparameters of fine-tuning RoBERTa base for GaLore.\n' +
      '\n' +
      '## Appendix D Additional Memory Measurements\n' +
      '\n' +
      'We empirically measure the memory usage of different methods for pre-training LLaMA 1B model on C4 dataset with a token batch size of 256, as shown in Table 8.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c c} \\hline \\hline Model Size & Layer Wise & Methods & Token Batch Size & Memory Cost & \\multicolumn{2}{c}{Throughput} \\\\  & & & & & \\#Tokens / s & \\#Samples / s \\\\ \\hline \\multirow{4}{*}{1B} & \\multirow{4}{*}{✗} & AdamW & 256 & 13.60 & 1256.98 & 6.33 \\\\  & & Adafactor & 256 & 13.15 & 581.02 & 2.92 \\\\  & & Adam8bit & 256 & 9.54 & 1569.89 & 7.90 \\\\  & & 8-bit GaLore & 256 & 7.95 & 1109.38 & 5.59 \\\\ \\hline \\multirow{4}{*}{1B} & \\multirow{4}{*}{✓} & AdamW & 256 & 9.63 & 1354.37 & 6.81 \\\\  & & Adafactor & 256 & 10.32 & 613.90 & 3.09 \\\\ \\cline{1-1}  & & Adam8bit & 256 & 6.93 & 1205.31 & 6.07 \\\\ \\cline{1-1}  & & 8-bit GaLore & 256 & 5.63 & 1019.63 & 5.13 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Measuring memory and throughput on LLaMA 1B model.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>