<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# _StableDrag_: 점 기반 영상 편집을 위한 안정적인 Dragging\n' +
      '\n' +
      'Yutao Cui\n' +
      '\n' +
      '난징대학교 소프트웨어 신기술 1국가핵심연구소\n' +
      '\n' +
      '2Tencent Inc.\n' +
      '\n' +
      '[https://stabledrag.github.io/12](https://stabledrag.github.io/12)\n' +
      '\n' +
      'Xiaotong Zhao\n' +
      '\n' +
      '2Tencent Inc.\n' +
      '\n' +
      '[https://stabledrag.github.io/2](https://stabledrag.github.io/2)\n' +
      '\n' +
      'Guozhen Zhang\n' +
      '\n' +
      '난징대학교 소프트웨어 신기술 1국가핵심연구소\n' +
      '\n' +
      '2Tencent Inc.\n' +
      '\n' +
      '[https://stabledrag.github.io/1](https://stabledrag.github.io/1)\n' +
      '\n' +
      'Shengming Cao\n' +
      '\n' +
      '2Tencent Inc.\n' +
      '\n' +
      '[https://stabledrag.github.io/1](https://stabledrag.github.io/1)\n' +
      '\n' +
      'Kai Ma\n' +
      '\n' +
      '2Tencent Inc.\n' +
      '\n' +
      '[https://stabledrag.github.io/1](https://stabledrag.github.io/1)\n' +
      '\n' +
      'Limin Wang\n' +
      '\n' +
      '교신저자, 전자메일: 난징대학교 소프트웨어공학을 위한 lmwang@nju.edu.cn1State Key Laboratory\n' +
      '\n' +
      '2Tencent Inc.\n' +
      '\n' +
      '[https://stabledrag.github.io/1](https://stabledrag.github.io/1)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '점 기반 이미지 편집은 DragGAN의 등장 이후 괄목할 만한 주목을 받고 있다. 최근에 DragDiffusion은 이 드래그 기법을 확산 모델에 적용하여 생성 품질을 더욱 향상시킨다. 이러한 큰 성공에도 불구하고, 이러한 드래깅 스킴은 부정확한 포인트 추적 및 불완전한 모션 감독이라는 두 가지 주요 단점을 나타내며, 이는 만족스럽지 못한 드래깅 결과를 초래할 수 있다. 이러한 문제를 해결하기 위해, 우리는 모션 수퍼비전을 위한 판별 포인트 추적 방법과 신뢰도 기반 잠재 향상 전략을 설계함으로써 안정적이고 정밀한 드래그 기반 편집 프레임워크인 _StableDrag_를 구축한다. 전자는 업데이트된 핸들 포인트를 정확하게 찾을 수 있게 하여 장거리 조작의 안정성을 높이는 반면 후자는 모든 조작 단계에서 가능한 한 고품질로 최적화된 잠재성을 보장하는 역할을 한다. 이러한 독특한 디자인 덕분에 보다 안정적인 드래그 성능을 얻을 수 있는 StableDrag-GAN과 StableDrag-Diff의 두 가지 이미지 편집 모델을 DragBench에 대한 광범위한 정성적 실험과 정량적 평가를 통해 인스턴스화한다.\n' +
      '\n' +
      '키워드: 안정적인 드래깅 이미지 편집 Drscriminative Tracking Confident Motion supervision\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '생성 모델[19, 25, 27, 30, 36, 40]을 사용한 제어 가능한 이미지 편집은 지난 몇 년 동안 놀라운 성과를 달성했으며, 이는 생성 결과를 추가 개선 목적으로 사용자 정의할 수 있다. 최근 선구적인 DragGAN[43]은 인터랙티브 포인트 기반 조작, 즉 사용자 입력 핸들 포인트에 기반한 시맨틱 객체를 해당 타겟 포인트로 향하게 하여 정확한 이미지 편집을 크게 추진하고 있다. DragGAN은 새로운 드래깅 기술을 공식화하고, 주로 모션 감독 및 포인트 추적을 포함하며, 전자는 핸들 포인트 주변의 로컬 패치를 단계적으로 타겟 포인트 쪽으로 감독하는 반면, 후자는 각 단계에서 업데이트된 핸들 포인트를 위치시키는 역할을 한다.\n' +
      '\n' +
      '드래그GAN의 큰 성공에도 불구하고, 그 편집 능력은 여전히 생성적 적대 네트워크의 고유한 모델 용량과 일반성에 의해 제한된다. 따라서 최근 작품[41, 50]은 고품질 드래그 스타일 이미지 편집을 위해 확산 모델[13, 16, 17, 20, 23, 28, 31, 42, 46, 48, 49]에 의존한다. 대표적인 연구인 DragDiffusion[50]은 드래깅 기법을 확산 모델에 적용하기 위해 탐색한다. 즉, 먼저 LoRA를 미세 조정한 다음, 단일 확산 단계에서 잠재성을 최적화하고, 마지막으로 MasaCtrl[7]을 기반으로 최적화된 잠재성을 제거한다. 확산 잠재 최적화의 핵심 구성 요소는 모션 감독 및 포인트 추적을 반복적으로 수행하는 DragGAN의 관습을 직접 따른다. 우리는 현재 드래그 방식이 여전히 다음과 같은 문제를 겪고 있다고 분석한다.\n' +
      '\n' +
      'i) _Inaccurate point tracking._ 이들 방법들은 특징 차이를 유사성 측정으로서 레버리지하여 업데이트된 핸들 포인트들을 추적하는데, 이는 산만자들로부터 올바른 것(즉, 주변들)을 정확하게 위치시키기에 불충분하다.\n' +
      '\n' +
      '그림 1: **DragGAN/DragDiffusion [50]과 제안된 StableDrag.** StableDrag-GAN 및 StableDrag-Diff 간의 비교는 각각 GAN 및 Diffusion 모델을 기반으로 구성된 제안된 방법이다. 이미지 입력(GAN/확산 모델에 의한 합성 이미지, 또는 실제 이미지)이 주어지면, 사용자는 핸들 포인트(적색 포인트) 및 타겟 포인트(파란색 포인트)를 할당하여 핸들 포인트의 의미적 위치를 구동하여 대응하는 타겟 포인트에 도달할 수 있다. 모나리자 초상화의 예와 마지막 행의 예는 실제 이미지 입력이고, 나머지는 StyleGAN2 또는 Stable Diffusion-V1.5[47] 모델의 합성이다. 본 논문에서 제안한 방법은 DragGAN과 DragDiffusion에 비해 더 정밀한 포인트 레벨 조작을 달성하고 더 높은 품질의 편집 이미지를 생성함을 보인다.\n' +
      '\n' +
      '유사한 내용으로 오해의 소지가 있는 점 특히 확산 모델에서는 잡음 주입이 많은 중간 확산 과정에서 특징들이 샘플링되기 때문에, 업데이트된 점들은 국부적인 주변 환경과 구별하기가 점점 더 어려워진다. 이것은 그림 1의 모나리자 초상화와 꽃병의 예에서 알 수 있듯이 불만족스러운 끌림 결과를 초래할 수 있다. 동작 감독 프로세스 동안, 잠재물은 특정 단계에서 적절하게 최적화되지 않을 수 있고, 결과적으로 조작 품질의 저하를 초래할 수 있다(도 1의 코끼리와 여성의 예를 참조). 뿐만 아니라 점 추적 드리프트도 있습니다. 확산 모델에서 잠재 잠재량은 GAN의 [50]보다 더 안정적이고 조작하기 어려우며, 특히 특정 이미지에서 LoRA를 미세 조정할 때 문제가 악화될 수 있다.\n' +
      '\n' +
      '앞서 언급한 문제를 고려하여 보다 안정적인 드래깅 프레임워크를 설계하기 위한 두 가지 기본 원칙이 있다고 주장한다. 첫째, 부정확한 점을 찾고 많은 지연 시간을 증가시키는 것을 피하기 위해, 견고하면서도 효율적인 점 추적_ 방법이 요구되며, 따라서 점 기반 드래그가 정확해질 수 있게 한다. 둘째, 모든 조작 과정에서 편집 내용을 최대한 고품질로 유지하고 생성 모델의 강력한 복원력을 완전히 발휘할 수 있도록 각 최적화 단계에서 모션 감독이 완료되도록 보장해야 한다. 또한, 완전한 모션 감독은 주어진 핸들 포인트들의 콘텐츠와 업데이트된 포인트들 사이의 유사성을 강화시켜, 추적 에러들의 누적을 방지할 수 있다.\n' +
      '\n' +
      '위와 같은 분석을 바탕으로 DragGAN과 DragDiffusion에서 점추적과 모션감독의 드래깅 기법을 재구성하고, **StableDrag**로 만들어진 점기반 영상 편집을 위한 보다 안정적인 드래깅 프레임워크를 제시한다. 구체적으로, 시각적 객체 추적[4, 10, 12]의 성공에 영감을 받아, 우리는 판별적 학습 손실로부터 컨볼루션 필터 형태의 간단하면서도 강력한 포인트 추적 모델을 도출하고자 한다. 이 모델은 산만점들의 추적 신뢰도 점수를 억제할 수 있을 뿐만 아니라 핸들 점들의 추적 신뢰도 점수를 향상시킬 수 있다. 조작 단계 초기에는 맞춤형 유사도 학습 기능의 감독하에 추적 모델 가중치를 업데이트한다. 추적 모델이 준비되면, 강인하고 정밀한 점 추적을 위해 원래의 특징 차이 방법과 함께 이를 사용한다. 특히, 이 접근법은 초기 조작 단계에서 단순 추적 모델(즉, 단일 컨볼루션 필터)을 최적화하기만 하면 되기 때문에 추론 지연을 거의 증가시키지 않는다. 또한 각 단계에서 모션 감리를 충분히 완료할 수 있도록 신뢰 기반 잠재 향상 전략을 설계한다. 구체적으로, 현재 조작 프로세스의 품질을 평가하기 위해 핸들 포인트의 추적 신뢰 점수를 활용한다. 일반적으로 우리는 DragDiffusion과 같은 모션 감독 방식을 사용합니다. 그럼에도 불구하고 품질 점수가 허용 가능한 임계값 아래로 떨어질 때 템플릿 특징(즉, 주어진 시작 핸들 포인트의 초기 특징)을 사용하여 신뢰 점수가 만족할 때까지 현재 핸들 포인트의 콘텐츠를 감독한다. 드래그 기법에 대한 독특한 설계 덕분에, GAN과 Diffusion 모델에 각각 구축된 StableDrag-GAN과 StableDrag-Diff의 두 가지 이미지 편집 모델을 인스턴스화하여 보다 안정적이고 정밀한 드래그 성능을 얻는다. 우리의 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '* 우리는 모델이 업데이트된 핸들 포인트를 산만자 포인트와 정확하게 구별할 수 있도록 하여 드래그의 안정성을 촉진하는 판별 포인트 추적 방법을 제안한다.\n' +
      '* 각 조작 단계에서 최적화 품질을 향상시킬 수 있는 모션 수퍼비전을 위한 신뢰도 기반 잠재 향상 전략을 고안한다.\n' +
      '* 이러한 설계 하에서 GAN 및 Stable Diffusion을 포함한 다양한 생성 모델을 기반으로 점 기반 이미지 편집 프레임워크인 **StableDrag**를 구축한다. 다양한 예제에 대한 광범위한 정성적 실험과 DragBench [50]에 대한 정량적 평가를 통해 StableDrag-GAN과 StableDrag-Diff의 효과를 입증한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### Image Editing\n' +
      '\n' +
      '이미지 편집은 광범위한 응용 프로그램으로 화제입니다. GAN(Generative Adversarial Networks)은 이미지 생성[18, 26] 분야에서 상당한 발전을 이루어 GAN 프레임워크를 기반으로 수많은 이전 이미지 편집 기술[2, 14, 32, 43, 53, 45]을 구축했다. 그럼에도 불구하고 GAN의 모델 용량은 다소 제한적이며 실제 이미지를 GAN 잠재 공간[1, 9, 37]으로 효과적으로 변환해야 하는 어려움은 이러한 접근법의 실용성이 불가피하게 제한되었다. 최근 대규모 텍스트-이미지 확산 모델들은 매우 사실적인 생성 결과[13, 16, 17, 20, 23, 28, 31, 42, 46, 48, 49]를 만들어냈고, 이는 수많은 확산 기반 이미지 편집 방법[3, 6, 7, 15, 22, 29, 35, 38, 39, 41, 44, 51]을 낳았다. 이러한 기술은 주로 이미지와 관련된 프롬프트를 조정하여 이미지를 편집하기 위해 노력한다. 그럼에도 불구하고 많은 편집 노력이 텍스트를 통해 전달하기 어려운 것으로 판명됨에 따라 신속한 기반 전략은 이미지의 높은 수준의 의미 또는 스타일을 자주 수정하여 정확한 픽셀 수준의 공간 조작을 달성할 수 있는 능력이 부족하다.\n' +
      '\n' +
      '세립 편집을 용이하게 하기 위해, [14, 43, 52]와 같은 점 기반 수정을 실행하기 위한 다수의 연구가 제안되었다. 특히 DragGAN은 두 가지 간단한 구성 요소를 통해 놀라운 드래그 기반 조작을 보여주었다: 잠재 코드의 최적화는 핸들 포인트를 원하는 목적지 포인트로 이동시키고 업데이트된 핸들 포인트를 찾기 위한 포인트 추적 메커니즘이다. 그러나 GAN의 제한된 용량으로 인해 일반성이 제한된다. 드래그디퓨전[50] 및 드래곤디퓨전[41]은 드래깅 방식을 확산 모델로 확장하여 우수한 생성 능력을 활용합니다. FreeDrag[36]은 점 추적 없는 패러다임을 도입하여 DragGAN을 개선하자는 제안을 하였다. 본 논문에서는 안정적인 점 기반 영상 편집이 가능하도록 자신감 있는 모션 감독 모듈과 판별점 추적 모듈을 재형성하는 새로운 드래깅 기법을 제안한다.\n' +
      '\n' +
      '### Visual Tracking\n' +
      '\n' +
      '제안된 판별점 추적은 시각적 추적 연구에서 영감을 얻으므로 이러한 방법에 대한 간략한 개요를 제공한다. 우리는 작품을 세 가지 범주로 나눈다. 먼저, 상관 필터 기반 추적기[12, 5, 21]는 추적을 위한 온라인 표적 종속 판별 모델을 학습하였다. [5, 21] 취업자 온라인 상관 필터를 통해 대상과 배경을 구분하고 빠른 주행 속도로 좋은 성능을 얻을 수 있어 지금까지는 매우 실용적이다. 둘째, 샴 기반의 트래커[34, 4]는 단순성과 효율성으로 인해 많은 관심을 끌고 있다. 이러한 방법들은 Siamese 네트워크와 상관 연산을 결합하여 타겟과 검색 사이의 외관 유사성 및 상관 관계를 모델링한다. SiamFC[4]는 높은 추적 속도로 템플릿과 탐색 영역 사이의 유사성을 측정하기 위해 Siamese 네트워크를 사용했다. SiamRPN++[33]은 깊이-방향 교차 상관으로 교차 상관을 개선함으로써 성능과 효율성을 모두 높일 수 있다. 마지막으로, 최근 일부 추적기 [8, 10, 11]는 표적과 탐색 영역 사이의 유사성을 포착하기 위해 변압기 기반 통합 모듈을 도입했다. 이러한 발견에서 영감을 얻은 우리는 차별적 학습을 통해 강력한 포인트 추적 모델을 고안한다. 이러한 작업과 달리, 우리는 GAN 또는 확산 모델의 중간 특징 위에 추적 모델을 구축하여 식별성을 활용하고 추적 모델을 최적화하기만 하면 효과적이지만 효율적이다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### 점 기반 드래그에 대한 예비\n' +
      '\n' +
      '먼저, 연구의 기본인 GAN 및 확산 모델 뒤에 있는 점 기반 드래그 프레임워크에 대한 최근 문헌을 간략하게 검토한다.\n' +
      '\n' +
      'DragGAN.GAN 모델 [24, 27]에 의해 생성된 이미지를 사용자 입력 핸들 포인트 \\(\\{p_{i}=(x_{pi},y_{pi}), i=1,2,...,n\\}\\) 및 목표 포인트 \\(\\{t_{i}=(x_{ti},y_{ti}), i=1,2,...,n\\}\\), DragGAN은 모든 핸들 포인트 \\(p_{i}\\)에서 콘텐츠를 해당 목표 포인트 \\(t_{i}\\)으로 이동시키는 것을 목표로 한다. 이러한 의미에서 주요 관심사는 높은 이미지 충실도를 유지하면서 포인트 레벨 편집을 정확하게 제어하는 방법에 있다. 이 목표를 달성하기 위해 DragGAN은 반복적인 모션 감독과 포인트 추적을 포함하는 새로운 패러다임을 조정한다. 중간 특징이 매우 차별적이라는 제너레이터의 특성을 고려하여 단순한 온라인 모션 감독 손실을 활용하여 잠재 코드를 최적화한다. \\(p_{i}\\) 주변의 국부 영역을 \\(\\Theta(p_{i})\\), 즉 \\(p_{i}\\)까지의 거리가 반지름 \\(r_{i}\\)보다 작은 픽셀로 표현할 때, 손실은 다음과 같이 정의될 수 있다.\n' +
      '\n' +
      '\\theta(p_{i}))-\\mathbf{F}(\\theta(p_{i}+d_{i})\\|_{1}+\\eta\\|(\\mathbf{F}-\\mathbf{F}^{0})\\cdot(1-\\mathbf{M})\\|_{1}, \\tag{1}\\tag\n' +
      '\n' +
      '여기서, \\(\\mathbf{F}\\)은 현재 최적화 단계에서의 중간 특징에 대해 나타내고, \\(\\mathbf{F}^{0}\\)은 초기 단계에서의 특징이고, \\(n\\)은 핸들 포인트의 수이고, \\(d_{i}=\\frac{t_{i}-p_{i}}{\\|t_{i}-p_{i}\\|_{2}}\\)은 편차 벡터이고 \\(\\mathbf{M}\\)은 변화하는 영역을 제어하기 위해 미리 정의된 마스크이다. 특히, \\(\\mathbf{F}(\\Theta(p_{i}))\\)가 분리되기 때문에, 전류 \\(p_{i}\\)의 함량은 작은 스텝에 의해 \\(t_{i}\\)으로 동기화될 것이다. 그러나 최적화라는 내재적 불확정성 때문에 \\(p_{i}\\)에 접근하기 위해서는 \\(p_{i}+d_{i}\\)을 보장하기 어렵다. 결과적으로, 그들은 \\(pi\\)의 업데이트된 상태를 결정하기 위해 점 추적으로서 간단한 특징 차이 방법을 사용한다. 위의 최적화 과정은 핸들 포인트들 \\(p_{i}\\)이 각각의 타겟 포인트들 \\(t_{i}\\)에 수렴할 때까지 반복된다.\n' +
      '\n' +
      'DragDiffusion.DragDiffusion[50]은 점 기반 편집 프레임워크를 Stable Diffusion(SD-V1.5[47])과 같은 확산 모델로 확장하여 높은 안정성과 우수한 발전 품질의 강력한 힘을 발휘한다. 이 편집 방법은 세 가지 하위 프로세스, 즉 실제 이미지 상의 LoRA를 미세조정하고, 특정 확산 단계 상의 잠재성을 최적화하고, 편집된 이미지를 생성하기 위해 업데이트된 잠재성을 디노이징하는 것을 포함한다. 구체적으로, 이들은 잠재된 것을 조작하기 위해 단일 중간 확산 단계 상에서 반복된 모션 감독 및 포인트 추적의 동일한 드래깅 공식을 채택한다. 또한, 전체 조작 과정을 통해 이미지 동일성을 보존하기 위해 LoRA Finetuing 전략을 사용한다. 마지막으로, 자기 주의 제어 메커니즘인 MasaCtrl[7]을 사용하여 원본 이미지와 편집 이미지 간의 일관성을 강화한다.\n' +
      '\n' +
      '그림 2: 중간 단일 단계 최적화를 위한 드래그 계획의 **그림.** 여기에 설명된 드래그 파이프라인의 핵심은 GAN을 기반으로 하는 반면 확산 모델을 기반으로 하는 것은 동일하게 유지된다. \'Discriminative PT.\'는 판별점 추적 모듈을 나타내고, \'Confident MS.\'는 자신감 있는 동작 감독 과정을 나타낸다. \\ (P_{i}\\)는 \\(i^{th}\\) 단계 최적화에서 현재 핸들 포인트를 의미한다. 특히, 컨볼루션 필터 형태의 추적 모델은 첫 번째 최적화 단계에서만 학습되며 후속 단계에서만 사용될 수 있다. 첫 번째 단계에서 학습 과정에 대한 자세한 내용은 그림 3에 설명되어 있으며 잠재 코드\\(w\\)는 모든 단계에서 역방향 업데이트를 통해 최적화되어야 한다.\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 1을 참조하면, DragGAN 및 DragDiffusion은 부정확한 포인트 추적 및 불완전한 모션 감독으로 인해 편집 이미지가 악화될 수 있다. 따라서 본 연구에서는 보다 안정적이고 정밀한 영상 조작을 위해 현재 드래그 기법에 주목하였다. 개발된 드래깅 파이프라인이 그림 1에 나와 있다. 판별 포인트 추적 모듈 및 자신감 있는 모션 감독 모듈을 포함하는, 도 2. 구체적으로, 학습된 판별 추적 모델에서 산출된 추적 점수와 원래의 특징 차이를 통합하여 항력 정밀도뿐만 아니라 점 추적 정확도를 높이는 새로운 점 추적 접근법을 설계한다. 추적 점수를 기반으로 충분한 모션 감독을 달성하기 위해 신뢰 기반 잠재 향상 전략을 탐색한다. 우리는 또한 DragGAN이 짧은 런타임 내에 큰 변형과 창의적인 콘텐츠(예: 입을 닫은 사자를 포효 상태로 변형)를 마스터한다는 것을 관찰한다. 드래그 확산은 우수한 품질과 높은 충실도 편집 결과를 생성하는 데 탁월합니다. 드래그 모델이 다양한 시나리오를 수용할 수 있도록 하기 위해 설계된 드래깅 스킴을 사용하여 DragGAN과 DragDiffusion에 대한 StableDrag를 구축한다. 본 절에서는 제안된 드래깅 방법에 대해 자세히 소개한다.\n' +
      '\n' +
      '### 판별점 추적\n' +
      '\n' +
      '포인트 추적은 업데이트된 핸들 포인트 \\(p_{i}\\)을 식별하는데 중추적인 역할을 하며, 잘못된 포인트를 드래그하는 것을 우회하고 만족스럽지 못한 편집 결과를 생성한다. DragGAN과 DragDiffusion에서 널리 사용되는 접근 방법은 초기 특징 템플릿인 \\(p_{0}\\)에 대해 최소한의 특징 차이로 위치를 식별하여 가장 가까운 이웃 검색을 수행하는 것이 간단하다. 그러나, 이는 복잡한 장면에서 핸들 포인트를 유사한 것과 구별하는 데 중요한 배경 외관 정보를 완전히 무시한다. 특히, 확산 모델에서는 상당한 잡음을 포함하는 중간 확산 단계에서 감독 특징이 추출되기 때문에 업데이트된 지점을 식별하는 것이 점진적으로 어려워진다. 예를 들어, 그림의 모나리자 초상화의 경우에 나타난 바와 같다. 1, 코의 핸들 포인트는 인접한 포인트와 유사한 외관을 가지며, 이는 DragDiffusion에서 오해의 소지가 있는 위치를 야기한다. 따라서 본 연구에서는 보다 차별적이면서도 간단한 점 추적을 수행하기 위한 대안적인 방법을 모색한다.\n' +
      '\n' +
      '주어진 핸들 포인트를 산만자와 구별하는 것은 학습 가능한 판별 추적 모델을 사용하여 해결될 수 있다. 우리의 설계에서 점 추적 모델은 컨볼루션 레이어의 가중치를 구성하여 점 분류 점수를 출력으로 제공한다. 구체적으로, 우리는 함수 \\(g(\\mathbf{F}(\\Theta_{2}),z_{i})\\)를 학습하는데, 여기서 \\(g\\)은 컨볼루션 함수를 나타내고, \\(\\Theta_{2}\\)은 현재 핸들 포인트 \\(pi\\) 주변의 국부적 패치이며, \\(z_{i}\\)은 학습된 추적 모델로서, 추적 모델 \\(z_{i}\\)이 특정 위치의 콘텐츠와 일치하여 업데이트된 핸들 포인트 \\(p_{i}\\)으로 식별되면 높은 점수를 반환하고 그렇지 않으면 낮은 점수를 반환한다. 특히, 추적 모델 \\(z_{i}\\)은 잠재 최적화 전에 학습되며 모든 조작 단계에 걸쳐 변하지 않는다. 이러한 의미에서, 이러한 접근법은 편집 런타임을 거의 증가시키지 않는다. 마지막으로, 추적 모델에 의해 계산된 분류 점수를 원래의 특징 차이 점수와 병합하여 판별 및 정확한 점 위치를 모두 달성한다. 판별점 추적의 세부 절차는 그림 2에 나와 있다.\n' +
      '\n' +
      '형식적으로, 국소 패치 \\(\\theta_{2}(p_{i},r_{2})=\\{(x,y)\\\\big{|}\\|x-x_{pi}<r_{2}|,|y-y_{pi}<r_{2}|\\}\\)가 주어지면, 추적된 점 \\(p_{i}\\)은 다음과 같이 업데이트된다.\n' +
      '\n' +
      '\\theta_{2})=\\lambda*e^{-\\|\\mathbf{F}(\\theta_{2})-f_{i}\\|_{1}+(1-\\lambda)*g(\\mathbf{F}(\\theta_{2}),z_{i}),\\\\&p_{i}:=\\operatorname*{arg\\,max}_{q_{i}\\in\\theta_{2}(p_{i},r_{2})),\\\\&s_{i}=\\max_{q_{i}\\in\\theta_{2}(p_{i},r_{2}}S(\\theta_{2}(p_{i},r_{2})),\\end{split}\\tag{2}\\cplit}\n' +
      '\n' +
      '여기서, \\(S(\\Theta_{2})\\)는 국부 패치 \\(\\Theta_{2}\\)의 추적 신뢰 점수 맵에 대해 나타내고, \\(\\lambda\\)은 가중 인자이고, \\(f_{i}=\\mathbf{F}^{0}(p_{i}^{0})\\)는 단계-0에서 초기 핸들 포인트 \\(p_{i}^{0}\\)의 원래 특징이고, \\(s_{i}\\)은 현재 단계에서의 최대 추적 신뢰 점수로서, 모션 감독을 안내하는데 사용된다. (S(\\Theta_{2})\\)의 관점에서 전자는 템플릿과 탐색 영역의 특징 차이를 측정한다. 대부분의 경우에 정확한 포인트 로컬리제이션을 제공할 수 있지만, 산만점들에 의해 오도될 수 있다. 따라서 두 번째 항은 \\(z_{i}\\)에 대한 초기 최적화 과정에서 주변 점의 점수를 억제하는 판별 학습으로 추적 견고성을 향상시키는 역할을 한다. 평범한 특징 차이 방법과 달리, 이 추적 모델은 배경 정보를 활용하고 중간 특징의 구별 특성을 활용할 수 있으므로 원래 접근법에 귀중한 향상을 제공한다.\n' +
      '\n' +
      '추적 모델 \\(\\mathbf{z_{i}}\\)을 위한 학습. \\(1\\times C\\times 1\\times 1\\times) 크기의 컨볼루션 필터인 점 추적 모델 \\(z_{i}\\)의 학습을 조작 과정 전에 수행한다. 학습 과정의 개요는 그림 3에 나와 있다. 우리는 \\(f_{i}\\)을 사용하여 \\(z_{i}\\)을 초기화하고 다음 손실의 감독 하에 가중치를 업데이트한다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{track}=\\|g(\\mathbf{F_{0}}(\\theta_{2}(p_{i},r_{2})),z_{i})-y_{i} \\|^{2}. \\tag{3}\\\\tag{3}}\n' +
      '\n' +
      '그림 3: 우리의 포인트 추적 모델의 **학습 과정. 조작 과정 이전에만 수행됩니다. 로컬 패치의 초기 피쳐가 제거되어 추적 모델만 최적화되어야 함을 나타냅니다. 추적 모델 가중치는 템플릿 특징 \\(f_{i}\\)으로 초기화된다.**\n' +
      '\n' +
      '여기서, \\(\\mathbf{F_{0}}\\)는 단계-0에서의 초기 특징을 나타내고, \\(y_{i}\\)는 일반적으로 \\(p_{i}\\)에 중심을 둔 가우시안 함수로 설정된 각 위치에서의 원하는 신뢰 점수인 지상-진실 라벨에 대해 나타낸다. 학습 과정에서 기울기는 \\(\\mathbf{F_{0}}(\\Theta_{2}(p_{i},r_{2}))을 통해 역전파되지 않는다. 즉, 추적 모델 \\(z_{i}\\)만 최적화하면 빠른 수렴이 가능하다. 최적화를 통해 핸들 포인트를 강조함과 동시에 배경 포인트의 신뢰 점수를 억제한다. 그런 다음 후속 조작 단계에서 추적 모델 \\(z_{i}\\)은 효율성을 위해 변경되지 않고 유지된다.\n' +
      '\n' +
      '자신감 넘치는 운동 감독\n' +
      '\n' +
      '동작 감독은 포인트가 의도된 목적지로 이동하도록 점진적으로 장려하는 핵심이다. DragGAN은 목표 달성을 위해 식 (1)에서 온라인 손실을 사용하지만 장거리 드래그에서 만족스럽지 못한 결과를 초래할 수 있다. 대안적으로, 우리는 각 단계에서 고품질 및 포괄적인 감독을 보장할 뿐만 아니라 업데이트된 상태_에 대한 신규 콘텐츠 생성을 수용하기 위한 적절한 수정을 허용한다는 신조에 기초하여 자신감 있는 모션 감독 컴포넌트를 고안한다. 예를 들어, Fig.에서 치마를 입은 여성의 경우를 예로 들 수 있다. 도 1은 시각적 일관성을 유지하는 데 있어 완전한 감독의 중요성을 보여준다.\n' +
      '\n' +
      '위의 목표를 달성하기 위해 그림 2와 같은 신뢰 기반 잠재 향상 전략을 제안한다. 먼저 현재 감독 자신감을 나타내기 위해 추적 점수의 최대값, 즉 \\(s_{i}\\)을 도입하고 강화 전략을 위한 임계값을 생성하기 위해 단계-1에서 신뢰 점수 \\(s_{1}\\)을 도입한다. 일반적으로 식 (1)과 같은 원래 모션 감독은 현재 상태가 충분히 자신 있는 상태를 식별할 때 사용된다. 현재 신뢰 점수가 미리 정의된 임계값 아래로 떨어지면 감독을 위해 초기 템플릿에 의존한다. 구체적 강화 감독은 다음과 같이 정의된다.\n' +
      '\n' +
      '\\mathcal{L}_{2}=&\\sum_{i=0}^{n}\\|\\mathbf{F}^{0}(\\Theta(p_{i}^{0}))-\\mathbf{F}(\\Theta(p_{i}+d_{i})\\|_{1}\\\\\\&+\\eta\\|(\\mathbf{F}-\\mathbf{F}^{0})\\cdot(1-\\mathbf{M})\\|_{1}, \\end{split}\\tag{4}\\mathbf{F}^{0}(\\theta(p_{i}+d_{i})\\mathbf{F}^{0})\\cdot(1-\\mathbf{M})\\|_{1}, \\end{split}\\tag{4}\\mathbf{F}^{0}(\\theta(p_{i}+d_{i})\\mathbf{F}\n' +
      '\n' +
      '여기서 \\(\\mathbf{F}^{0}(\\Theta(p_{i}^{0}))\\)는 기울기 역 전파가 없는 고정된 템플릿으로, 초기 상태를 모방하기 위해 업데이트된 포인트의 콘텐츠를 강제할 수 있다. 더욱이, 이러한 잠재 강화 감독의 사용 여부에 대한 선택은 다음 지침에 따라 결정된다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{motion}=\\left\\{\\begin{aligned} &\\mathcal{L}_{1},\\\\s_{i}>\\tau*s_{1},\\\\&\\mathcal{L}_{2},\\,s_{i}<=\\tau*s_{1},\\end{aligned}\\right. \\tag{5}\\right.\n' +
      '\n' +
      '여기서 \\(\\tau\\)는 강화 강도를 제어하기 위한 임계 레이트이다. 이러한 방식으로, 핸들 포인트들의 현재 콘텐츠가 원래의 템플릿으로부터 상당히 벗어나는 것을 방지할 수 있고, 따라서, 자신감 있는 모션 감독을 달성할 수 있다. 한편, 신뢰도 점수가 임계값을 초과할 경우, 높은 편집성을 유지하기 위해 동적 모션 감독 \\(\\mathcal{L}_{1}\\)에 의존한다.\n' +
      '\n' +
      '논의.자신감 있는 모션 감독에 대한 통찰력을 더 잘 설명하기 위해, 우리는 점 추적 모듈을 자유롭게 설정하기 위해 적응형 템플릿과 선형 검색을 사용하는 FreeDrag[36]에서 제안된 방법과 비교한다. 첫째, FreeDrag에서 미리 설정된 선형 탐색은 잠재 최적화의 유연성에 제약을 가할 수 있고, 이에 따라 드래깅의 난이도를 크게 증가시킬 수 있다. 도 1의 좌측 상단 예시에 도시된 바와 같다. 4, FreeDrag의 핸들 포인트는 미리 정의된 경로를 따라 자주 진동하고 320단계의 최적화가 필요하다. 그러나, 본 방법은 핸들 포인트들이 단지 46개의 단계들에서 선형이 아닌 _a 더 최적의 path_를 따라 목적지를 향해 이동할 수 있게 한다. 또한, 프리드래그(FreeDrag)는 업데이트 전략이 사용되더라도 감독을 위한 템플릿 기능에 주로 의존하기 때문에 그림 4의 왼쪽 하단 예에서 알 수 있듯이 창의적이고 배포되지 않은 콘텐츠를 생성하는 데 어려움을 겪는다. 대조적으로, 우리의 StableDrag-GAN은 장거리 드래그 경로를 주어 만족스러운 크리에이티브 콘텐츠를 생성할 수 있어 더 나은 편집성을 보여준다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '우리는 PyTorch를 기반으로 StableDrag-GAN과 StableDrag-Diff를 포함한 접근 방식을 구현한다. 잠재코드 \\(w_{i}\\)를 최적화하는 과정에서 학습률이 StableDrag-Diff는 0.01, StableDrag-GAN은 0.001인 Adam Optimizer를 사용하여 기본 설정을 따른다. 대부분의 경우 \\(\\lambda\\)과 \\(\\tau\\)의 하이퍼파라미터는 각각 0.3과 0.4로 설정된다. 다른 매개변수 및 모델 설정의 경우 DragGAN 및 DragDiffusion의 기본 설정을 따릅니다. 실험은 NVIDIA V100 GPU에서 수행된다.\n' +
      '\n' +
      '그림 4: ** FreeDrag[36]와 우리의 StableDrag.** 간의 비교 왼쪽 상단에 있는 예를 들어 각 최적화 단계의 핸들 포인트를 시각화하여 FreeDrag와 우리의 StableDrag-GAN의 최적화 경로의 차이를 보여준다. 왼쪽 하단에 있는 예는 새로운 콘텐츠를 만드는 데 있어 우리의 방법의 힘을 보여주는 것입니다. 그리고 다른 것들은 스테이블 드래그가 더 정확한 드래깅 결과를 생성할 수 있다는 것을 보여주는 것이다.\n' +
      '\n' +
      '### Qualitative Comparison\n' +
      '\n' +
      '도. 도 5는 공정한 비교를 위해 DragGAN과 StableDrag-GAN, DragDiffusion과 StableDrag-Diff, FreeDrag-Diff와 StableDrag-Diff의 정성적인 결과를 보여준다. 방법의 일반성을 평가하기 위해 GAN 기반 모델에 대해 입력 이미지는 StyleGAN2[27]에서 생성된다. 확산 기반 모델의 경우 실제 영상을 입력하고 DDIM 역산을 사용하여 재구성한다. 우리의 방법은 핸들 포인트를 산봉우리, 사자 턱, 사슴의 이마, 작은 램프 등 목표 포인트로 보다 정밀하게 이동시킬 수 있음을 알 수 있다. 또한, 스테이블 드래그는 가방, 안경, 말 및 테라 코타 워리어스 조각품의 외관을 유지하는 것과 같은 고품질 및 더 높은 충실도 편집 결과를 생성할 수 있습니다. 또한 Diffusion 모델을 기반으로 StableDrag-Diff와 FreeDrag[36]을 비교한다. 우리는 우리의 디프가 더 정확한 결과를 생성하고 초기 이미지의 세부 사항을 유지한다는 것을 알 수 있다. 이는 보다 안정적인 드래깅 성능을 얻을 수 있는 제안된 판별 포인트 추적 및 자신감 있는 모션 감독의 유효성을 입증한다.\n' +
      '\n' +
      '### Quantitative Results\n' +
      '\n' +
      '우리는 미리 정의된 드래그 포인트와 마스크를 가진 205개의 샘플로 구성된 DragBench[50]에서 우리의 방법을 정량적으로 평가한다. 드래그벤치에서\n' +
      '\n' +
      '그림 5: **DragGAN[43]/DragDiffusion[50]/FreeDrag[36]과 우리의 StableDrag의 비교. DragGAN에서와 같이, 사용자는 선택적으로 플렉서블 영역(밝은 영역)의 마스크를 그릴 수 있고, 이미지의 나머지를 고정 상태로 유지할 수 있다. Terra Cotta Warriors Sculpture_와 _the Panda_의 예에서 초록색 대시 박스는 그 차이점을 상세하게 보여주기 위한 것이다. Zoom in.**에서 가장 잘 볼 수 있는 것은 적절한 StyleGAN2 모델과 호환되지 않는 많은 예이므로 DragDiffusion 및 our-Diff에 대한 실험만 수행합니다. 우리는 안정적인 드래그-디프를 드래그 확산과 비교하고 공정한 비교를 위해 동일한 LoRA 가중치와 공통 하이퍼-파라미터를 사용한다. 표 1에 나타난 바와 같이, 최적화 단계의 세 가지 다른 설정 하에서, StableDrag-Diff는 DragDiffusion을 일관되게 능가하며, 특히 60단계 최적화에서 평균 거리 점수의 3.22 및 이미지 충실도 점수의 0.017만큼 기준선을 능가한다. 이는 StableDrag가 제안한 자신감 있는 모션 감독과 판별 포인트 추적을 통해 편집 정확도와 콘텐츠 일관성에서 유망한 결과를 얻을 수 있음을 나타낸다.\n' +
      '\n' +
      '### Exploration Study\n' +
      '\n' +
      '제안된 방법의 유효성을 검증하고 철저한 분석을 제공하기 위해 GAN과 확산 모델을 기반으로 한 정성적 시각화와 확산 모델을 기반으로 한 DragBench에 대한 정량적 평가를 통해 상세한 절제 연구를 수행한다.\n' +
      '\n' +
      '자신감 있는 동작 감독.여기서는 자신감 있는 동작 감독 구성 요소의 효과를 연구합니다. 먼저, StableDrag-GAN 기반의 말 편집 실험을 수행한다. 도 1에서 확인할 수 있다. 도 6에 도시된 바와 같이, 신뢰도 점수가 점차 감소함에 따라, 자신감 있는 모션 감독 없이 StableDrag\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Optimization Steps & 60 & 80 & 100 \\\\ Metric & MD/IF & MD/IF & MD/IF \\\\ \\hline DragDiffusion & 39.58/0.876 & 37.98/0.868 & 38.86/0.863 \\\\ StableDrag-Diff & **36.36/0.893** & **36.98/0.884** & **35.92/0.869** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 드래그벤치에 대한 정량적 비교. \'MD\'는 Mean Distance \\(\\downarrow\\)를 의미하고, \'IF\'는 Image Fidelity (1-LIPIPS) \\(\\uparrow\\)를 의미한다.\n' +
      '\n' +
      '모듈은 낮은 품질의 편집 이미지를 생성합니다. 이는 각 단계에서 자신감 있는 수퍼비전을 수행하는 것의 중요성을 나타내며, 추적 점수가 모션 수퍼비전의 품질을 반영할 수 있음을 보여준다. 표 2에서 보는 바와 같이 DragGAN에서 자신감 있는 모션 수퍼비전을 독창적인 수퍼비전 방법으로 대체할 경우 이미지 충실도가 0.018 감소하여 위의 결론을 더욱 입증한다.\n' +
      '\n' +
      '식별점 추적. 도 7 및 표 2에서, 우리는 판별 추적 모델이 없는 스테이블 드래그와 스테이블 드래그를 평가한다. 우리는 판별 추적 모델이 없는 스테이블드래그(StableDrag)가 배경 산만점(background distractor point)에 의해 오해의 소지가 있어 부정확한 결과를 초래할 수 있음을 알 수 있다. 특히, 판별 추적 모델이 없는 StableDrag-Diff는 Mean Distance의 2.27만큼 StableDrag-Diff를 증가시킨다. 그 결과, 제안된 판별 추적 모델은 드래그 모델이 보다 정확한 점 기반 이미지 편집을 달성하는 데 도움이 된다는 것을 도출할 수 있다.\n' +
      '\n' +
      '추적 모듈의 실용성, 제안된 점 추적기는 공식화와 구현 모두에서 간결하다. 표에서 보는 바와 같이. 도 3을 참조하면, 트래커의 트레이닝 과정(약 1초)은 드래그 과정보다 훨씬 적은 시간이 소요된다. 각 감독 단계 이전의 포인트 추적은 컨볼루션 연산만 수행되어야 하므로 매우 빠르게 실행된다. 점 추적 프로세스 동안, 우리는 글로벌 영역에서 두 개의 완전히 유사한 객체(예를 들어, 두 개의 거의 동일한 개)를 식별하는 것을 피하기 위해 로컬 검색 전략을 사용한다는 점에 주목할 필요가 있다. 또한, 기준선에 약 60행 코드만 추가되기 때문에 코어 코드 구현은 간단하고 다른 관련 방법에 쉽게 적응할 수 있다. 그리고 우리는 코드를 공개할 것이다.\n' +
      '\n' +
      '\\(\\tau\\) 및 \\(\\lambda\\)에 대한 민감도 분석. 제안된 방법의 견고성을 더 잘 이해하기 위해 표 4와 같이 \\(\\tau\\) 및 \\(\\lambda\\)에 대한 민감도 분석을 수행했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Metric & Mean Distance \\(\\downarrow\\) & Image Fidelity \\(\\uparrow\\) \\\\ \\hline DragDiffusion & 39.58 & 0.876 \\\\ StableDrag-Diff W/O DPT. & 38.63 & **0.895** \\\\ StableDrag-Diff W/O CMS. & 37.87 & 0.875 \\\\ StableDrag-Diff & **36.36** & 0.893 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 우리의 차별적 포인트 추적 및 자신감 있는 모션 수퍼비전의 효과. ‘DPT’는 식별점 추적을 의미하고, ‘CMS’는 자신감 있는 모션 수퍼비전을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c|c c} \\hline \\hline Model & Tracker iters. & Drag steps & Tracker time(s) & Drag time(s) \\\\ \\hline StableDrag-Diff & 1000 & 60 & 1.17 & 29.06 \\\\ StableDrag-Diff & 1000 & 80 & 1.08 & 38.80 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 트레이닝 트래커 및 드래그 프로세스의 시간 소모에 대한 분석. 평가는 안정 드래그-디프에 대해 수행된다.\n' +
      '\n' +
      '그리고 표 5. 결과를 통해, i) 안정적인 드래그를 위해 자신감 있는 모션 감독이 중요하고 적절한 임계값이 중요하다는 것을 알 수 있으며, ii) 제안된 추적기를 원래의 특징 차이와 병합함으로써 최적의 드래깅 성능을 얻을 수 있다.\n' +
      '\n' +
      '### \\(z_{i}\\)에 대한 학습과정의 시각화\n' +
      '\n' +
      '식별 추적 모델에 대한 보다 포괄적인 이해를 제공하기 위해 그림 1에 나와 있다. 도 8을 참조하면, 학습 과정에서 추적 모델의 예측 결과를 시각화한다. 트레이닝 반복이 증가함에 따라, 배경 포인트(즉, 중심으로부터 떨어진 포인트)가 점진적으로 억제되어, 더 강건하고 차별적인 포인트 추적 모델이 생성되는 것을 알 수 있으며, 이는 드래깅 모델이 더 정확한 편집 결과를 생성하는 것을 도울 수 있다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리는 모션 수퍼비전을 위한 판별 포인트 추적 방법과 신뢰도 기반 잠재 향상 전략을 설계하여 스테이블드래그(StableDrag)로 만들어진 안정적인 드래그 기반 편집 프레임워크를 구축했다. 제안된 포인트 추적 방법을 이용하면 갱신된 핸들 포인트를 정확하게 찾을 수 있어 원거리 조작의 안정성을 높일 수 있다. 후자는 모든 조작 단계에서 가능한 한 높은 품질의 최적화된 잠재성을 보장할 수 있다. 독특한 디자인 덕분에 우리는 일반성을 입증하기 위해 StableDrag-GAN과 StableDrag-Diff를 포함한 두 가지 유형의 모델을 인스턴스화했다. 다양한 예제에 대한 광범위한 정성적 및 정량적 실험을 통해 안정 항력은 안정적이고 정확한 항력 성능을 달성했다. 우리는 우리의 발견과 분석이 정확한 이미지 편집의 개발을 촉진할 수 있을 것으로 기대한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\(\\tau\\) & 0.0 & 0.2 & 0.4 & 0.6 & 0.8 & 1.0 \\\\ \\hline MD/IF & 42.1/0.868 & 41.6/0.874 & **39.8**/0.891 & 43.3/0.913 & 47.4/0.939 & 51.2/**0.955** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: \\(\\tau\\)에 대한 민감도 분석이며, 여기서 \\(\\lambda\\)은 0.0으로 고정된다.\n' +
      '\n' +
      '그림 8: 추적 모델 \\(z_{i}\\)에 대한 학습 과정의 시각화.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\(\\lambda\\) & 0.0 & 0.2 & 0.4 & 0.6 & 0.8 & 1.0 \\\\ \\hline MD/IF & 42.1/0.868 & 41.6/0.869 & 41.6/0.87 & **37.9**/**0.875** & 40.7/0.874 & 39.0/0.875 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: \\(\\lambda\\)에 대한 민감도 분석이며, 여기서 \\(\\tau\\)은 0.0으로 고정된다.\n' +
      '\n' +
      '## Appendix\n' +
      '\n' +
      '우리는 각각 DragGAN[43]과 DragDiff[50]에 기반을 둔 StableDragGAN과 StableDrag-Diff를 포함하여 StableDrag에 대한 더 많은 시각화 결과를 제공한다. 이는 Fig.에서 확인할 수 있다. 도 9에 도시된 바와 같이, 본 방법은 대부분의 시나리오에서 정확하고 안정적인 드래깅 성능을 생성할 수 있다. 또한, [https://stabledrag.github.io/](https://stabledrag.github.io/)에서 StableDrag와 FreeDrag[36] 드래깅 과정을 비교하여 보다 많은 시각화 결과를 제공한다. 승인 시 코드가 릴리스됩니다.\n' +
      '\n' +
      '도 9: 우리의 StableDrag-GAN 및 StableDrag-Diff의 더 많은 결과.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Abdal, R., Qin, Y., Wonka, P.: Image2stylegan: How to embed images into the stylegan latent space? In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 4432-4441 (2019)\n' +
      '* [2] Abdal, R., Zhu, P., Mitra, N.J., Wonka, P.: Styleflow: Attribute-conditioned exploration of stylegan-generated images using conditional continuous normalizing flows. ACM Transactions on Graphics (ToG) **40**(3), 1-21 (2021)\n' +
      '* [3] Bar-Tal, O., Ofri-Amar, D., Fridman, R., Kasten, Y., Dekel, T.: Text2live: Text-driven layered image and video editing. In: European Conference on Computer Vision. pp. 707-723. Springer (2022)\n' +
      '* [4] Bertinetto, L., Valmadre, J., Henriques, J.F., Vedaldi, A., Torr, P.H.S.: Fully-convolutional siamese networks for object tracking. In: ECCV Workshops (2016)\n' +
      '* [5] Bolme, D.S., Beveridge, J.R., Draper, B.A., Lui, Y.M.: Visual object tracking using adaptive correlation filters. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR (2010)\n' +
      '* [6] Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image editing instructions. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18392-18402 (2023)\n' +
      '* [7] Cao, M., Wang, X., Qi, Z., Shan, Y., Qie, X., Zheng, Y.: Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465 (2023)\n' +
      '* [8] Chen, X., Yan, B., Zhu, J., Wang, D., Yang, X., Lu, H.: Transformer tracking. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR (2021)\n' +
      '* [9] Creswell, A., Bharath, A.A.: Inverting the generator of a generative adversarial network. IEEE transactions on neural networks and learning systems **30**(7), 1967-1974 (2018)\n' +
      '* [10] Cui, Y., Jiang, C., Wang, L., Wu, G.: Mixformer: End-to-end tracking with iterative mixed attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2022)\n' +
      '* [11] Cui, Y., Song, T., Wu, G., Wang, L.: Mixformerv2: Efficient fully transformer tracking. In: Advances in Neural Information Processing Systems (2023)\n' +
      '* [12] Danelljan, M., Bhat, G., Khan, F.S., Felsberg, M.: ATOM: accurate tracking by overlap maximization. In: CVPR (2019)\n' +
      '* [13] Dhariwal, P., Nichol, A.: Diffusion models beat GANs on image synthesis. In: NeurIPS (2021)\n' +
      '* [14] Endo, Y.: User-controllable latent transformer for stylegan image layout editing. arXiv preprint arXiv:2208.12408 (2022)\n' +
      '* [15] Epstein, D., Jabri, A., Poole, B., Efros, A.A., Holynski, A.: Diffusion self-guidance for controllable image generation. arXiv preprint arXiv:2306.00986 (2023)\n' +
      '* [16] Esser, P., Rombach, R., Blattmann, A., Ommer, B.: ImageBART: Bidirectional context with multinomial diffusion for autoregressive image synthesis. In: NeurIPS (2021)\n' +
      '* [17] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.: An image is worth one word: Personalizing text-to-image generation using textual inversion. In: ICLR (2023)\n' +
      '* [18] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., Weinberger, K. (eds.) Advancesin Neural Information Processing Systems. vol. 27. Curran Associates, Inc. (2014), [https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)\n' +
      '* [19] Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.C., Bengio, Y.: Generative adversarial nets. In: NeurIPS (2014)\n' +
      '* [20] Gu, S., Chen, D., Bao, J., Wen, F., Zhang, B., Chen, D., Yuan, L., Guo, B.: Vector quantized diffusion model for text-to-image synthesis. In: CVPR (2022)\n' +
      '* [21] Henriques, J.F., Caseiro, R., Martins, P., Batista, J.: High-speed tracking with kernelized correlation filters. IEEE Trans. Pattern Anal. Mach. Intell. **37**(3), 583-596 (2015)\n' +
      '* [22] Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or, D.: Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626 (2022)\n' +
      '* [23] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS (2020)\n' +
      '* [24] Karras, T., Aittala, M., Laine, S., Harkonen, E., Hellsten, J., Lehtinen, J., Aila, T.: Alias-free generative adversarial networks. In: NeurIPS (2021)\n' +
      '* [25] Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial networks. In: CVPR (2019)\n' +
      '* [26] Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial networks. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4401-4410 (2019)\n' +
      '* [27] Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of StyleGAN. In: CVPR (2020)\n' +
      '* [28] Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani, M.: Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210.09276 (2022)\n' +
      '* [29] Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani, M.: Imagic: Text-based real image editing with diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6007-6017 (2023)\n' +
      '* [30] Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013)\n' +
      '* [31] Kumari, N., Zhang, B., Zhang, R., Shechtman, E., Zhu, J.Y.: Multi-concept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488 (2022)\n' +
      '* [32] Leimkuhler, T., Drettakis, G.: Freestylegan: Free-view editable portrait rendering with the camera manifold. arXiv preprint arXiv:2109.09378 (2021)\n' +
      '* [33] Li, B., Wu, W., Wang, Q., Zhang, F., Xing, J., Yan, J.: Siamrpn++: Evolution of siamese visual tracking with very deep networks. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR (2019)\n' +
      '* [34] Li, B., Yan, J., Wu, W., Zhu, Z., Hu, X.: High performance visual tracking with siamese region proposal network. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR (2018)\n' +
      '* [35] Liew, J.H., Yan, H., Zhou, D., Feng, J.: Magicmix: Semantic mixing with diffusion models. arXiv preprint arXiv:2210.16056 (2022)\n' +
      '* [36] Ling, P., Chen, L., Zhang, P., Chen, H., Jin, Y.: Freedrag: Point tracking is not you need for interactive point-based image editing. arXiv preprint arXiv:2307.04684 (2023)\n' +
      '* [37] Lipton, Z.C., Tripathi, S.: Precise recovery of latent vectors from generative adversarial networks. arXiv preprint arXiv:1702.04782 (2017)* [38] Mao, J., Wang, X., Aizawa, K.: Guided image synthesis via initial image editing in diffusion model. arXiv preprint arXiv:2305.03382 (2023)\n' +
      '* [39] Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.Y., Ermon, S.: Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073 (2021)\n' +
      '* [40] Mirza, M., Osindero, S.: Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784 (2014)\n' +
      '* [41] Mou, C., Wang, X., Song, J., Shan, Y., Zhang, J.: Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421 (2023)\n' +
      '* [42] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., Chen, M.: GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021)\n' +
      '* [43] Pan, X., Tewari, A., Leimkuhler, T., Liu, L., Meka, A., Theobalt, C.: Drag your GAN: Interactive point-based manipulation on the generative image manifold. arXiv preprint arXiv:2305.10973 (2023)\n' +
      '* [44] Parmar, G., Singh, K.K., Zhang, R., Li, Y., Lu, J., Zhu, J.Y.: Zero-shot image-to-image translation. arXiv preprint arXiv:2302.03027 (2023)\n' +
      '* [45] Patashnik, O., Wu, Z., Shechtman, E., Cohen-Or, D., Lischinski, D.: Styleclip: Text-driven manipulation of stylegan imagery. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2085-2094 (2021)\n' +
      '* [46] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125 (2022)\n' +
      '* [47] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10684-10695 (2022)\n' +
      '* [48] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR (2022)\n' +
      '* [49] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S.K.S., Ayan, B.K., Mahdavi, S.S., Lopes, R.G., et al.: Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487 (2022)\n' +
      '* [50] Shi, Y., Xue, C., Pan, J., Zhang, W., Tan, V.Y., Bai, S.: Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. arXiv preprint arXiv:2306.14435 (2023)\n' +
      '* [51] Tumanyan, N., Geyer, M., Bagon, S., Dekel, T.: Plug-and-play diffusion features for text-driven image-to-image translation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1921-1930 (2023)\n' +
      '* [52] Wang, S.Y., Bau, D., Zhu, J.Y.: Rewriting geometric rules of a gan. ACM Transactions on Graphics (TOG) **41**(4), 1-16 (2022)\n' +
      '* [53] Zhu, J.Y., Krahenbuhl, P., Shechtman, E., Efros, A.A.: Generative visual manipulation on the natural image manifold. In: Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14. pp. 597-613. Springer (2016)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>