<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#다시점 이미지 편집을 위한 집중 특징 통합\n' +
      '\n' +
      'Or Patashnik\\({}^{1}\\) Rinon Gal\\({}^{1,2}\\) Daniel Cohen-Or\\({}^{1}\\) Jun-Yan Zhu\\({}^{3}\\) Fernando De la Torre\\({}^{3}\\)\n' +
      '\n' +
      '텔아비브대학교 ({}^{1}\\)NVIDIA \\({}^{3}\\) 카네기멜론대학교\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 텍스트 대 이미지 모델은 텍스트 프롬프트 또는 공간 컨트롤을 사용하여 광범위한 이미지 편집 기술을 가능하게 합니다. 그러나, 이러한 편집 방법들을 단일 장면을 묘사하는 멀티뷰 이미지들에 적용하는 것은 3D-불일치 결과를 초래한다. 본 연구에서는 공간 제어 기반의 기하학적 조작에 초점을 맞추고 다양한 뷰에 걸쳐 편집 프로세스를 통합하는 방법을 소개한다. 우리는 두 가지 통찰력을 기반으로 구축한다. (1) 생성 프로세스 전반에 걸쳐 일관된 특징을 유지하는 것은 멀티뷰 편집의 일관성을 달성하는 데 도움이 되며, (2) 자기 주의 계층에서의 쿼리는 이미지 구조에 상당한 영향을 미친다. 따라서 본 논문에서는 질의의 일관성을 강화하여 편집된 영상의 기하학적 일관성을 향상시키는 방법을 제안한다. 이를 위해, 우리는 편집된 이미지의 내부 쿼리 특징에 대해 훈련된 신경 복사 필드인 QNeRF를 소개한다. 일단 훈련되면, QNeRF는 3D-일관성 쿼리들을 렌더링할 수 있고, 그 쿼리들은 생성 동안 셀프-어텐션 레이어들에 부드럽게 다시 주입되어 멀티-뷰 일관성을 크게 향상시킨다. 우리는 확산 타임스텝에 걸쳐 질의를 더 잘 통합하는 점진적이고 반복적인 방법을 통해 프로세스를 정제한다. 제안한 방법을 기존의 다양한 기법들과 비교하였으며, 입력 장면에 대해 더 나은 다시점 일관성과 더 높은 충실도를 얻을 수 있음을 보였다. 이러한 이점을 통해 우리는 표적 기하학과 더 잘 정렬된 시각적 아티팩트가 적은 NeRF를 훈련할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대규모 텍스트 대 이미지 모델의 출현은 이미지 편집 기술의 급속한 발전으로 이어졌다. 일반적으로, 이러한 기법들은 미리 훈련된 텍스트-이미지 간 확산 모델에서 발견되는 풍부한 시각적 및 의미적 사전(semantic prior)을 레버리지함으로써 _single_ 이미지를 수정하는데 사용된다. 그러나, 공유된 장면을 묘사하는 이미지들의 _sets_를 고려할 때, 그러한 방법들의 순진한 애플리케이션들은 세트에 걸쳐 일관성 없는 편집들로 이어진다(도 3 참조).\n' +
      '\n' +
      '이미지 세트가 여러 방향에서 관찰된 단일 객체를 묘사하는 멀티뷰 편집의 영역에서, 최근의 작업 라인은 편집을 보다 3D-일관성 있는 세트[20]로 통합하기 위한 수단으로서 3D 표현[39]의 내재적 일관성을 활용하는 것을 제안한다. 실제로, 기존의 방법들은 수행된 편집들이\n' +
      '\n' +
      '도 2: 느슨한 깊이 맵[4]으로 부트의 멀티뷰 이미지들을 편집하는 단계. 우리는 세트에서 세 가지 이미지의 샘플을 보여준다.\n' +
      '\n' +
      '그림 1: 객체 중심 멀티뷰 이미지 세트(중심)가 주어지면, 우리는 신체 골격 변경과 같은 3D 기하학적 제어를 사용하여 모든 이미지를 동시에 편집한다(좌우). 다양한 뷰에 걸쳐 일관성을 촉진하기 위해 이미지 확산 모델을 활용하고 쿼리 특징 공간 신경 복사 필드인 QNeRF를 도입하여 생성 프로세스 동안 주의 특징을 점진적으로 통합한다.\n' +
      '\n' +
      '기본적인 3D 표현이 일관되지 않은 변화에 대해 성공적으로 평균화할 수 있을 만큼 충분히 작습니다. 더 단순한 질감이나 외관의 변화를 잘 유지하지만 더 복잡한 기하학적 변화를 다룰 때 실패하는 가정. 이처럼 이러한 방법은 사람의 초상화 양식을 그림으로 바꾸는 데 사용될 수 있지만, 손을 들게 하기 위해 고군분투한다.\n' +
      '\n' +
      '본 연구에서는 그림 1과 그림 2와 같이 조음 및 형태 변화에 초점을 맞춘 일관된 다시점 영상 편집을 위한 접근 방법을 제시한다. 거친 공간 컨트롤(_예:_, 신체 골격 또는 느슨한 깊이 맵[4])을 입력으로 하도록 훈련된 ControlNet[59]을 사용하여 이에 정렬된 영상을 합성한다. 이러한 러프 컨트롤에서 이미지의 생성을 컨디셔닝하는 것은 편집된 이미지의 거친 형상에 대한 사전 이해를 모델에 제공한다. 그러나 이 거친 지오메트리 신호에만 의존하는 것은 편집된 이미지 간의 높은 일관성에 미치지 못한다.\n' +
      '\n' +
      '우리의 핵심 아이디어는 멀티뷰 편집 이미지를 생성하는 동안 컨트롤넷의 기능이 일관되도록 장려하는 것이다. 최근 작품[19]에서 볼 수 있듯이, 내부 특징의 일관성을 높이는 것은 비디오 생성에서 편집된 프레임의 일관성을 향상시키는 데 도움이 될 수 있다. 특히, 확산 모델 내에서 자기 주의 계층의 질의가 출력 영상의 구조에 큰 영향을 미친다는 것을 알 수 있다. 따라서 본 논문에서는 QNeRF라는 질의 특징 공간에서 신경 필드를 학습하여 생성된 모든 이미지의 질의를 3D 일관성 있는 공유 표현으로 통합하는 방법을 제안한다. 그런 다음 QNeRF에서 렌더링된 질의를 사용하여 편집된 이미지의 생성을 안내하여 편집된 다시점 이미지의 일관성을 높인다. QNeRF를 트레이닝하고 렌더링된 쿼리를 사용하는 프로세스는 디노이징 프로세스 동안 인터리빙된 프로그래시브 프로세스이다. 도 4에 예시된 바와 같이, 업데이트된 QNeRF는 추출된 쿼리들로 트레이닝되고, 렌더링된 쿼리들은 확산 네트워크 내에서 생성된 이미지들의 특징들을 안내한다.\n' +
      '\n' +
      '우리는 우리의 접근법이 광범위한 관절 및 모양 기반 수정을 가능하게 하면서, 대안적인 일관성 보존 접근법보다 더 큰 시각적 품질을 달성한다는 것을 입증한다. 이러한 결과는 정성적인 평가뿐만 아니라 자동화된 메트릭 및 사용자 선호도 점수를 통해 검증된다. 마지막으로, 결과에 대해 훈련된 NeRF의 기본 기하학이 표적 대조군과 더 나은 정렬을 나타냄을 보여준다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '확산 모델을 사용한 이미지 편집.대규모 확산 모델의 발전은 이미지 편집 기술을 크게 향상시켰다[6, 7, 25, 27, 38]. 구체적으로, 잡음 제거 프로세스 동안 확산 모델 내의 내부 표현들의 조작은 고품질 및 의미론적으로 의미있는 편집들을 가능하게 하는 것으로 나타났다[16, 18, 19, 21, 42, 43, 53]. 특히 최근 일부 연구는 자기 주의 계층에 초점을 맞추고 자기 주의 내에서 쿼리, 키 및 값의 역할을 활용하여 다양한 편집을 얻는다[1, 8, 22]. 위의 연구들은 하나의 이미지를 편집하는 것에 초점을 맞추고 있지만, 우리는 일관된 멀티뷰 이미지 편집을 달성하기 위해 셀프 어텐션 컴포넌트의 기능을 기반으로 한다.\n' +
      '\n' +
      '그림 4: 확산 모델로 다시점 편집 영상을 동시에 생성한다. 영상을 통합하기 위해, 잡음 제거 과정을 통해 (1) 네트워크로부터 자기 관심 질의를 추출하고, (2) 추출된 질의에 대해 NeRF(termed QNeRF)를 학습하여 통합 질의를 렌더링하고, (3) 렌더링된 질의를 각 뷰에 대해 네트워크로 부드럽게 다시 주입한다. 우리는 잡음 제거 과정 내내 이러한 단계들을 반복한다.\n' +
      '\n' +
      '그림 3: 첫 번째 행과 세 번째 행은 서로 다른 시점에서 촬영된 이미지를 보여준다. 컨트롤넷[59] 및 MasaCtrl[8]을 사용하여 이들을 개별적으로 편집하면 불일치가 발생한다. 램프(위) 또는 벽(아래)에서 발까지의 거리에 주의하십시오. 이미지는 공유된 3D 모델(골격, 상자)에서 투영된 2D 컨트롤을 사용하여 편집되었다. 가장 왼쪽 열은 뷰 1에 해당하는 컨트롤을 표시합니다.\n' +
      '\n' +
      ' 다시점 데이터 합성 및 편집.대규모 이미지 확산 모델에 내장된 풍부한 선행은 새로운 3D 모델 편집 및 합성 패러다임의 길을 열었다. 이 접근법에서, 상이한 카메라 뷰들에 대응하는 이미지들은 편집되거나 생성된 다음, 이들을 통합하는 3D 모델을 구성하는 데 사용된다. Instruct-NeRF2NeRF[20]에서 저자는 일부 초기 NeRF를 훈련하는 데 사용되는 데이터 세트를 반복적으로 업데이트할 것을 제안한다. 각각의 업데이트 반복에서, 데이터세트로부터의 이미지는 Instruct-Pix2Pix로 편집되고, 이어서 NeRF는 상이한 뷰들을 통합하기 위해 그에 따라 업데이트된다. 이 데이터 세트 업데이트 접근법은 후속 작업[30, 31, 48, 50]에서 널리 채택되었다. 텍스처링 작업에서, 최근의 접근법은 다수의 뷰들로부터 주어진 3D 메시를 렌더링하고, 텍스트-유도 모델을 사용하여 이러한 뷰들에 대한 UV 맵들을 생성한 다음, 이들을 3D 모델에 적용하는 것이다[9, 44]. 이 프로세스는 중복되는 뷰로 반복적으로 수행되어 결국 일관된 텍스처링으로 이어진다. 유사하게, Kapon et al. [26]은 최근 일관성을 보장하기 위해 3D 표현을 채용하면서, 2D 뷰들을 생성하기 위해 인간 확산 모델을 사용하였다. 주어진 데이터세트를 편집하는 대신에, 동시 작업[56]은 3D-일관된 가짜 뷰들을 생성함으로써 몇몇 이미지들로부터 장면을 재구성한다.\n' +
      '\n' +
      '우리의 작업은 또한 여러 뷰에 걸쳐 2D 편집을 통합합니다. 그러나 기본 형상도 변경합니다. 또한, 픽셀 공간에서 작업하기보다는 주의 기능에 대한 NeRF를 훈련하여 뷰를 통합한다. 따라서 우리는 실제 이미지 다양체와 더 정렬되고 픽셀 공간 평균화 아티팩트에 덜 취약하다.\n' +
      '\n' +
      '이미지 공간에서 편집을 수행하기보다는 다른 작업 라인이 3D 표현에서 직접 작동한다. 특히, 암시적 3D 표현[28, 39]의 급속한 향상과 함께, 3D 편집 방법들이 등장하였다[17, 34, 36, 47, 58, 60]. 최근 연구는 텍스트 기반 이미지 편집의 발전을 사용하여 텍스트[2, 20, 47, 54, 55]로 암시적 3D 표현을 편집한다. 그러나 이러한 작업은 프롬프트에 의해 안내되므로 제어 기반 안내를 쉽게 처리할 수 없다. 실제로 일반적으로 모양과 작은 구조 수정에 중점을 두는 반면 큰 기하학적 변화를 목표로 합니다.\n' +
      '\n' +
      '마지막으로, 일부 작품들은 NeRF로부터 메쉬를 추출하여 고전적인 기법으로 편집한 후, 그에 따라 NeRF를 변형함으로써 형상 편집을 수행한다[3, 57, 58]. 그러나, 이러한 기술들은 전문가 지식을 필요로 하며, 단순한 제어 기반 방법들만큼 광범위하게 적용가능하지 않을 수 있다.\n' +
      '\n' +
      'Feature NeRFs. 이전 연구들은 NeRFs가 RGB 이미지뿐만 아니라 의미적 잠재 특징을 나타낼 수 있음을 보여주었다. 일부 작업은 [29, 33, 52] 시맨틱 2D 피쳐를 NeRF로 증류하여 시맨틱 3D 정보를 얻을 수 있게 한다. 다른 작업 [11, 41]은 NeRF로부터 렌더링된 특징들이 일관된 멀티-뷰 이미지 생성을 위해 채용될 수 있음을 보여준다. 본 연구에서는 확산 모델의 주의 특징을 NeRF로 증류하고, 디노이징 과정에서 렌더링된 특징을 이용하여 다시점 편집을 수행한다.\n' +
      '\n' +
      '## 3 Preliminaries\n' +
      '\n' +
      '확산모델에서### 자기주목\n' +
      '\n' +
      '최근의 확산 모델들은 전형적으로 교차-관심, 자기-관심, 및 컨볼루션 계층들로 구성된 UNet[46]으로 구현된다. 기존 연구에서는 주의층에 초점을 맞추어 이러한 구성 요소의 역할을 연구했다. 우리의 작업에서 우리는 _self-attention_ layer의 질의, 키 및 값에 초점을 맞춘다. 구체적으로, 셀프-어텐션 계층들에서의 각각의 쿼리는 그것에 대응하는 픽셀의 의미론적 의미를 결정하는 것으로 나타났다[1, 8]. 따라서, 질의들은 생성된 이미지의 구조와 연관된다. 더욱이, 셀프-어텐션 계층들의 키들 및 값들은 이미지의 외관을 결정하고, 다른 이미지의 잡음제거 프로세스에서 하나의 이미지의 키들 및 값들을 사용함으로써, 외관은 전송[1]된다. 특히, MasaCtrl[8]에서는 비강체 편집이 이미지에 적용된다. 원본 이미지의 외관을 보존하기 위해, 이들은 생성된 이미지에 원본 이미지로부터 자기-관심 층들의 키들 및 값들을 주입한다. 본 논문에서 제안하는 방법은 원본 장면의 모습을 보존하기 위해 이 기법을 사용한다.\n' +
      '\n' +
      '### 신경 복사 필드\n' +
      '\n' +
      'NeRF(Neural Radiance Field)는 네트워크에 의해 파라미터화된 암시적 3D 표현이다. 공간적 위치\\(\\mathbf{x}\\)와 시청방향\\(\\mathbf{d}\\)이 주어지면, 네트워크는 밀도\\(\\sigma(\\mathbf{x})\\)와 그 위치의 RGB 값\\(c(\\mathbf{x},\\mathbf{d})\\)을 출력한다. 이어서, 이들은 고전적인 볼륨 렌더링 기법들을 사용하여, 원하는 뷰잉 방향으로부터 이미지를 렌더링하는데 사용될 수 있다[37, 39]. 구체적으로, 카메라 광선\\(\\mathbf{r}(t)=\\mathbf{o}+t\\mathbf{d}\\)이 주어지면, 예상 색상\\(C(\\mathbf{r})\\)은 다음과 같이 주어진다.\n' +
      '\n' +
      '\\[C(\\mathbf{r})=\\int_{t_{n}}^{t_{f}}T(t)\\sigma(\\mathbf{r}(t))c(\\mathbf{r}(t), \\mathbf{d})dt, \\tag{1}\\t)\n' +
      '\n' +
      'where\n' +
      '\n' +
      '\\[T(t)=\\exp\\left(-\\int_{t_{n}}^{t}\\sigma(\\mathbf{r}(s))ds\\right). \\tag{2}\\]\n' +
      '\n' +
      '본 연구에서는 RGB 값이 아닌 잠재표상에 대한 NeRF를 학습한다. 이전 작업 [29, 33]에 이어서, 우리는 잠재 표현들을 렌더링하기 위해 동일한 체적 렌더링 접근법을 사용한다.\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      '제안된 방법은 여러 시점에서 동일한 장면을 묘사하는 포즈 영상(\\(\\{x^{v}\\}_{v=1}^{n}\\)과 각 시점에서 주 객체의 목표 기하를 느슨하게 지정하는 2차원 공간 제어(\\{c^{v}\\}_{v=1}^{n}\\)의 집합에 대해 동작한다(도 1, 2). 이러한 컨트롤은 골격이나 상자와 같이 조작하기 쉬운 저차원 3D 모델의 투영이다. 우리는 부록 A에서 편집된 컨트롤을 얻는 과정에 대해 자세히 설명한다.\n' +
      '\n' +
      '이러한 제어가 주어지면, 모든 입력 영상을 동시에 편집하여 출력 영상을 생성한다. 이러한 출력 이미지는 제공된 컨트롤과 정렬되도록 피사체의 형상이 변경된 상태에서 입력 이미지와 동일한 장면을 묘사해야 한다. 이미지를 편집하기 위해 사전 훈련된 안정 확산 모델[45]과 MasaCrtrl[8] 접근법을 활용한다. 그곳에서, 이미지들은 먼저 DDIM으로 반전된다[49]. 그런 다음, 주어진 컨트롤에 따라 다시 합성되며, 원본 이미지에서 자기 주의 레이어의 키와 값을 편집된 이미지에 주입하여 원본 장면의 모양을 보존한다. 그러나, 이 접근법은 각각의 이미지를 격리 상태로 고려하므로, 대응하는 편집 출력들은 시점들 간에 불일치한다. 이 장애물을 극복하기 위한 우리의 핵심 아이디어는 주의 특징 공간에서 멀티뷰 일관성을 개선하여 편집을 통합하는 것이다. 특히, 우리는 불일치가 물체 모양에 크게 있다는 것을 알아챘다. 따라서 본 논문에서는 서로 다른 뷰 간의 자기 주목 질의를 통합하여 모양을 정렬하는 방법을 제안한다. 우리는 QNeRF라고 불리는 잡음 제거 과정에서 질의에 대한 NeRF를 훈련함으로써 그렇게 한다.\n' +
      '\n' +
      '통합 과정에 대한 개념적 개요는 그림 4에 설명되어 있으며, 각 병렬 네트워크는 단일 뷰를 디노이징하는 병렬 네트워크를 묘사한다. 질의 특징은 네트워크에서 추출되고 QNeRF를 훈련하는 데 사용되며, QNeRF는 이를 3D 일관성 표현으로 통합한다. 그런 다음 통합 쿼리 기능을 부드럽게 주입(섹션 4.2)하여 노이즈 제거 네트워크에 다시 주입하여 편집된 이미지의 다중 뷰 일관성을 개선합니다.\n' +
      '\n' +
      '실제로 우리는 잡음 제거 과정을 간격을 두고 수행한다. 각 구간에서 통합 단계를 피쳐가 진화할 수 있는 단계로 인터리빙합니다. 단일 구간의 모든 통합 단계는 동일한 훈련된 QNeRF를 사용합니다. 추가적으로, 우리는 원래의 장면 외관을 보존하기 위해 모든 단계에서 자기 주목 키 및 값을 주입한다[8]. QNeRF, 피쳐 주입 접근 방식 및 간격 구조에 대한 자세한 내용은 아래에 나와 있습니다.\n' +
      '\n' +
      '### QNeRF\n' +
      '\n' +
      '우리의 접근법의 핵심은 잡음 제거 과정에서 확산 모델에서 추출된 쿼리 특징에 대해 훈련된 QNeRF - NeRF[39]이다. QNeRF의 고유한 3D 일관성은 쿼리의 통합을 유도한다. 구체적으로, 각 구간의 마지막 단계(섹션 4.3)에서, 우리는 모든 UNet 디코더 계층을 따라 확산 모델로부터 해상도\\(\\in(16,32,64)\\)을 갖는 자기 주의 질의를 추출한다. 이들은 주어진 잡음 제거 타임스텝에서 잡음 제거된 이미지당 총 9개의 질의 세트를 산출한다. 이것은 우리가 QNeRF를 훈련하는 훈련 세트를 구성한다.\n' +
      '\n' +
      'QNeRF 자체는 깊이-네팩토[14, 40, 51]이며 사용 사례에 더 잘 맞도록 일련의 적응이 있다. 먼저, 각 입력 좌표에 대한 RGB 값을 생성하기 보다는, \\(9\\) 추출된 쿼리 레이어에 해당하는 \\(9\\) 쿼리 값을 출력한다. 우리는 기본 네팩토 네트워크에 9개의 헤드를 추가하여 각 헤드가 특정 자기 주의 계층의 쿼리를 출력하도록 최적화된다. 따라서, 각 헤드의 출력의 차원은 각 레이어의 쿼리에서 채널 수로 설정된다. 기본 네팩토 네트워크는 원래 네팩토 아키텍처에서와 같이 각 지점의 밀도를 예측한다. 서로 다른 질의들 사이의 밀도를 공유함으로써, 우리는 교차 계층 정보를 더 잘 공유하고 형상을 안정화시킬 수 있다. 또한, 우리는 관찰 방향에 대한 QNeRF의 의존성을 생략했다. 이 선택은 쿼리가 뷰 방향에 의존하지 않는 지오메트리를 나타낸다는 사실을 구현한다. QNeRF의 전체 아키텍처는 그림 5에 나와 있다. 이를 훈련하기 위해 우리는 q-loss를 사용한다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{q}=\\sum_{\\mathbf{r}\\in\\mathcal{R}}\\sum_{l}\\|\\hat{\\mathbf{Q}}_{l}(r)-q^{r}(l)\\|, \\tag{3}\\|\n' +
      '\n' +
      '여기서 \\(\\mathcal{R}\\)은 샘플링된 광선이고, \\(q^{r}(l)\\)은 ray\\(r\\) 및 층 \\(l\\)에 대응하는 추출된 쿼리이다. \\(q^{r}(l)\\) (\\hat{\\mathbf{Q}}_{l}(r)\\)는 수학식 1과 같이 정의되며, 여기서 RGB 값 \\(c\\)을 자기 주의 쿼리로 대체한다. 또한, Deng 등이 제안한 depth-loss\\(\\mathcal{L}_{\\text{depth}\\)를 사용하고, QNeRF를 훈련하기 위한 최종 손실은 \\(\\mathcal{L}=\\mathcal{L}_{q}+\\mathcal{L}_{\\text{depth}\\)으로 작성된다.\n' +
      '\n' +
      '훈련되면 QNeRF를 사용하여 통합 쿼리를 렌더링하여 잡음 제거 프로세스를 안내할 수 있다. 우리는 표준 볼륨 렌더링 기법[37, 39]을 사용하여 그렇게 한다. 마지막으로, 구간들 사이에 기하학이 크게 변화하지 않을 것으로 예상하기 때문에, 우리는 이전 구간에서 훈련된 QNeRF로부터 각각의 QNeRF를 초기화한다.\n' +
      '\n' +
      '### Query Guidance\n' +
      '\n' +
      'QNeRF를 사용하여, 우리는 잡음 제거 프로세스를 안내하기 위해 그것에 의해 생성된 통합 쿼리를 사용하기를 원한다. 주어진 카메라 시점을 갖는 특정 프레임의 편집 과정을 고려한다. QNeRF를 사용하는 직접적인 방법은 이러한 특정 관점에 대한 쿼리를 렌더링하고 노이즈 제거 단계에서 UNet이 자연적으로 생성한 쿼리를 대체하는 것이다. 그러나 초기 실험에서 이러한 직접적인 교체가 편집된 프레임에서 시각적 아티팩트로 이어질 수 있음을 관찰했다. 대신에\n' +
      '\n' +
      '도 5: QNeRF의 아키텍처. 9개의 헤드가 베이스 네트워크에 부착되어, 확산 모델의 9개의 자기 주목 레이어에 대응하는 쿼리를 생성한다. 헤드들의 각각의 그룹은 특정 해상도의 자기-관심 계층에 대응하고, 화살표 위에 표시된 숫자는 그 그룹 내의 채널들의 수를 나타낸다(1280, 640, 320).\n' +
      '\n' +
      '이전 작품[12, 15, 16, 42]에서 영감을 얻은 "소프트 유도" 메커니즘을 제안합니다. 각 질의 유도 잡음 제거 단계에서 먼저 UNet을 통해 단일 순방향 통과를 수행하고 자연적으로 생성된 모든 질의를 추출한다. 그런 다음, 생성된 질의와 QNeRF로부터 렌더링된 질의 사이의 거리를 최소화하는 것을 목표로 입력 래턴트 자체에 대해 단일 최적화 단계를 수행한다. 형식적으로, 질의 안내는 다음과 같이 정의된다:\n' +
      '\n' +
      'z_{t}^{v}\\gets z_{t}^{v}-\\alpha\\nabla_{z_{t}^{v}}\\sum_{l}\\|q^{v}(l)-q_{\\text{ren}}^{v}(l)\\|^{2}, \\tag{4}\\}\n' +
      '\n' +
      '여기서 \\(z_{t}^{v}\\)는 timestep \\(t\\)에서 시점 \\(v\\)에 대응하는 잡음 잠재 코드이고, \\(q^{v}(l), q_{text{ren}}^{v}(l)\\)은 각각 층 \\(l\\)의 생성 및 렌더링된 자기-관심 쿼리이다. 이 갱신 단계 후에 DDIM[49] 잡음 제거 단계를 적용하여 \\(z_{t-1}^{v}\\)를 구한다.\n' +
      '\n' +
      '### 다시점 영상 잡음 제거 간격\n' +
      '\n' +
      '앞서 언급한 바와 같이 모든 잡음 제거 단계에 대해 QNeRF를 훈련하기보다는 간격 기반 접근법을 사용한다. 각 구간의 구조는 두 개의 관측치에 의해 동기화된다. 한편, 표준 디노이징 프로세스에서, 인접한 디노이징 타임스템들의 내부 UNet 피처들은 그들의 계산이 종종 스킵되고 재사용될 수 있는 정도로 유사한 [35]인 것이 관찰되었다. 따라서, 동일한 질의 특징을 갖는 여러 개의 인접 타임스테프를 가이드하는 것은 결과의 품질을 저하시키지 않을 것으로 기대한다. 다른 한편으로, 확장된 수의 타임스테프들에 걸쳐 동일한 질의들을 계속 재사용한다면, 확산 특징들에서 발생하는 점진적인 변화의 여지를 남기지 않는다. 극단적인 경우, 모든 타임스테프들에 대해 동일한 쿼리-안내를 사용하는 것은 전체 확산 경로에 걸쳐 동일한 쿼리들이 사용되는 것으로 이어질 것이다. 이상적으로, 우리의 메커니즘은 쿼리가 잡음 제거 프로세스를 따라 자유롭게 진화할 수 있도록 해야 한다.\n' +
      '\n' +
      '따라서 영상 생성을 여러 겹침 구간으로 분할하는 인터리브 과정을 제안한다. 확산 타임스텝 \\(T_{i}\\)에서 시작하여 \\(2\\tau\\) 단계에 걸쳐 있는 하나의 간격(그림 6에 도시됨)을 고려하십시오. 우리는 이전 간격의 끝에서 얻은 QNeRF를 사용하여 \\(\\tau\\) QNeRF 유도 단계를 수행하여 간격을 시작한다. 이 시점에서 얻어진 잡음(\\(T_{i}-\\tau\\))을 미래용으로 저장한다. 이 단계들에 이어서, 질의 안내 없이 바닐라 MasaCtrl 편집을 수행하는 다른 단계들(\\tau\\)을 수행한다. 따라서 이러한 단계를 통해 쿼리는 보다 진보된 단계와 일치하여 자유롭게 진화할 수 있다. 이러한 유도되지 않은 단계가 끝나면 업데이트된 쿼리 특징을 추출하고 이를 사용하여 새로운 QNeRF를 최적화한다. 마지막으로, 갱신된 QNeRF를 이용하여 \\(T_{i}-\\tau\\)에 저장된 잠복기를 회수하고 이 시점부터 새로운 간격을 시작한다.\n' +
      '\n' +
      '첫 번째 간격의 특별한 경우, 우리는 QNeRF를 가지고 있지 않다. 따라서 본 논문에서는 \\(2\\tau\\) 단계(_i.e., \\(T-2\\tau\\)까지)에 대한 비유도 편집을 수행하고, 이 단계에서 추출된 특징들로부터 QNeRF를 학습하고, \\(T-2\\tau\\)에서 다음 간격을 시작한다.\n' +
      '\n' +
      '### Progressive Consolidation\n' +
      '\n' +
      '디노이징 프로세스를 따라 쿼리의 통합은 생성의 중간 단계 동안 QNeRF를 즉석에서 훈련함으로써 점진적으로 수행된다. 구체적으로, 각각의 간격에서 QNeRF를 트레이닝하기 위해 사용되는 쿼리들은, 이전으로부터 렌더링된 통합 쿼리들에 의해 영향을 받는다.\n' +
      '\n' +
      '그림 6: 각 다중 뷰 잡음 제거 간격에서 우리는 질의 유도 단계를 가지고, 이어서 안내 없이 단계를 가진다. 질의 유도 단계에서는 잠재 코드에 의해 생성된 자기 주의 질의와 QNeRF로부터 렌더링된 질의 사이의 근접성을 목표로 잡음 잠재 코드를 변경한다. 구간 마지막 단계에서 생성된 질의를 추출하여 다음 구간에 대한 안내를 제공하는 QNeRF를 학습하는데 사용한다. 조회 안내는 다양한 뷰에 걸쳐 형상을 통합합니다. 또한, 원본 이미지에서 자기 주의 계층의 키와 값을 주입하여 외관을 보존한다.\n' +
      '\n' +
      '구간 QNeRF입니다. 이 접근 방식을 각 간격 내의 스케줄링과 결합함으로써, 우리는 주입되는 모든 쿼리가 여전히 이전 간격의 끝에 형성된 통합 쿼리의 결과임을 보장하면서 쿼리를 자유롭게 개발할 수 있는 공간을 모델에 제공한다. 결정적으로, 이것은 질의들이 다시 통합되기 전에 너무 멀리 멀어지는 것을 방지한다. 이러한 진행 과정의 효과를 파악하기 위해 그림 7과 같이 지퍼 메커니즘과 유사한 점을 고려한다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '다음으로, 우리는 우리의 방법을 질적으로 그리고 정량적으로 평가하고, 다른 기준 방법과 비교한다. 우리는 각각 \\(200\\)-\\(500\\)의 다시점 영상 집합에 대한 결과를 보여준다. 이 중 \\(3\\) 집합은 비교를 위해 사용된다. 추가 데이터 세트 세부 정보는 부록 B를 참조하십시오.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '우리는 우리 방법의 주요 구성 요소의 중요성에 대한 연구로 시작한다. 우리는 (i) MasaCtrl[8]로 이미지를 독립적으로 편집하고, (ii) 소프트 인젝션 메커니즘 대신 렌더링된 쿼리를 직접 주입하고, (iii) 비-프로그레시브 통합 프로세스를 사용한다. 먼저, MasaCtrl을 사용하여 모든 이미지를 독립적으로 편집하고 서로 다른 타임스테프를 따라 자기 주의 쿼리를 캐시한다. 그런 다음 이러한 쿼리에 대해 QNeRF를 훈련하고 마지막으로 부드러운 Q 주입으로 편집을 다시 생성한다. 정성적 결과는 그림 8에 제시되어 있다. 알 수 있듯이, 독립적인 이미지 편집(두 번째 행)은 일관되지 않은 결과로 이어진다. 예를 들어, 피험자의 발은 목재 데크(제3 기둥과 비교하여 제2 기둥)에 대해 상이한 위치에 위치한다. 또한 다리는 모양(가장 오른쪽 기둥)이 다르고 데크의 높이는 이미지마다 다르다. 렌더링된 쿼리(세 번째 행)를 직접 주입하면 이미지가 일관되지만 원래 이미지와 너무 많이 분기되어 다리가 절단되고 데크의 높이가 증가한다. 비진행 접근법(4번째 행)으로 QNeRF를 훈련하면 4번째 열의 다리가 누락되는 것과 같은 더 많은 아티팩트가 발생할 수 있다. 마지막 행에서, 다리들은 일관되게 위치되고, 그들의 형상은 상이한 이미지들 사이에서 변화하지 않는다. 또한 데크의 모양은 원본 이미지와 동일하게 유지됩니다.\n' +
      '\n' +
      '### Comparisons\n' +
      '\n' +
      '다음으로, 다중 이미지 편집 문제를 해결하는 기존 기술과 우리의 방법을 비교한다. 구체적으로, 다중 영상 일관성 문제에 대해 서로 다른 접근 방식을 취하는 세 가지 방법과 비교한다. 먼저, 데이터에 대한 NeRF를 먼저 훈련하는 데이터셋 업데이트 기법인 InstructNeRF2NeRF(IN2N) [20]과 비교한 다음, 기존 데이터셋 사진을 대체하기 위해 새로운 이미지를 반복적으로 렌더링하고, 편집하고, NeRF를 편집과 더 잘 정렬하도록 훈련하여 수정한다. 둘째, SDS(Collaborative Score distillation sampling) 과정을 수행하는 CSD[31]과 비교하여, 이미지의 큰 부분 집합에 걸쳐 SDS 구배를 더 잘 정렬한다. 우리는 저자를 따라 이미지 편집 단계 자체가 CSD 기반 멀티뷰 편집 단계로 대체되는 IN2N과 CSD를 통합한다. 마지막으로 크로스 프레임 일관성을 개선한 최근의 텍스트 기반 동영상 편집 방법인 TokenFlow[19]와 비교한다.\n' +
      '\n' +
      '그림 8: 절제 연구 결과. 우리의 전체 방법은 원래의 장면을 정확하게 보존하면서 보다 일관된 이미지를 생성한다.\n' +
      '\n' +
      '그림 7: 우리의 점진적인 노이즈 제거 과정은 지퍼 메커니즘과 유사하다. 지퍼의 각 단계는 모든 선행 부품의 폐쇄에 의존하며, 서로 더 가까워졌다는 사실로부터 이익을 얻는다. 점선 빨간색 곡선(왼쪽)은 두 뷰의 확산 과정을 따라 생성된 쿼리를 보여준다. 지퍼는 빨간색 점을 통합하는 QNeRF를 나타내며, 더 가까운 궤적을 따라 있는 주황색 점(왼쪽)을 형성하도록 투영한다. 몇 단계 후에 주황색 및 녹색 곡선(오른쪽)으로 이 프로세스를 반복하여 생성된 쿼리를 점진적으로 통합한다.\n' +
      '\n' +
      '흐름 기반 접근 방식을 통해 여기에서 초기 이미지 세트를 단일 비디오로 연결한 다음 편집합니다.\n' +
      '\n' +
      '위의 방법들은 원래 편집을 위한 인터페이스로서 텍스트에 의존한다. 이를 ControlNet[59]과 통합하여 공간 제어로 대상 편집을 지정할 수 있다. 특히 IN2N과 CSD에서는 InstructPix2Pix 편집기를 MasaCtrl[8]로 교체한다. 우리는 각각 IN2N-CN 및 CSD-CN에 의한 이러한 버전의 방법을 나타낸다. 토큰플로우의 경우 ControlNet 버전을 사용합니다. 특히, TokenFlow는 원래 비디오에서 패치 유사성에 따라 흐름을 구축하는데, 이는 우리의 경우 변경된 기하학과 일치하지 않는다. 우리는 흐름이 일관되지 않은 프레임별 MasaCtrl 이미지(편집되지 않은 이미지 대신)에 접지될 때 성능이 크게 향상되므로 모델의 이러한 변화와 비교된다.\n' +
      '\n' +
      '마지막으로 CSD와 IN2N의 출력은 NeRF이다. 따라서, TokenFlow와 우리의 방법과의 공정한 비교를 위해, 우리는 TokenFlow의 출력과 우리의 방법에 대한 NeRF를 간단히 트레이닝한다. 출력에서 NeRF를 훈련하면 NeRF의 깊이를 관찰하여 서로 다른 방법 간의 기하학을 비교할 수 있다.\n' +
      '\n' +
      '모든 방법의 렌더링된 이미지는 그림 9에 제시되어 있다. IN2N 및 CSD 모두에서, 모델은 부분 데이터세트 업데이트 - _i.e._에 의존하며, 타겟 편집이 점진적으로 수행될 수 있고, 부분 편집된 뷰들에 대한 평균화가 일관된 NeRF로 이어질 것이라는 암묵적인 가정이 존재한다. 이 가정은 외관 편집에 대해 성립하지만 관절과 같은 모양 변경에는 성립하지 않는다. 예를 들어, 데이터세트의 일부에서 사람의 팔을 움직이는 것은 결국 부분 팔이 소스 및 타겟 위치 둘 다에 위치하는 NeRF로 이어진다. 이제, 피험자는 \\(4\\)개의 팔을 가지고 있으며, 베이스라인 편집 방법은 복구하는데 어려움을 겪는다. 실제로 이러한 유령 다리 인공물은 두 가지 방법 모두에서 볼 수 있다. 예를 들어, 그림 9의 마지막 행에서 원래 암은 CSD의 RGB 및 깊이 이미지 모두에서 볼 수 있다. IN2N의 경우 원래 암은 깊이 이미지에서 볼 수 있으며 새 오른쪽 암은 노이즈 지오메트리를 가지고 있다. CSD와 MasaCtrl을 결합하면 모든 데이터 세트 업데이트 단계에 대해 수천 개의 DDIM[49] 역전을 실행해야 하기 때문에 계산 시간이 크게 증가한다. 한 가지 극단적인 경우(악어 장난감 장면, \\(491\\) 이미지를 포함하는 경우) 이 방법은 H100 GPU에서 일주일 동안 훈련한 후에도 NeRF를 변경할 수 있는 충분한 이미지를 업데이트하지 못했다. 자동화된 평가에서 이 인스턴스를 생략합니다. 토큰플로우는 NeRF를 통해 a-프리오리 평균화 없이 모든 이미지를 편집한다. 따라서 원하는 모양과 정렬하고 유령 같은 다리 인공물을 피하는 것이 좋다. 그러나 기하학적 편집은 여전히 흐름 일관성 가정을 위반하여 시각적 아티팩트로 이어진다. 또한, 특징 주입 접근법은 MasaCtrl의 충실성을 원래 프레임과 일치시키기 위해 고군분투한다.\n' +
      '\n' +
      '우리의 접근법은 기존 NeRF에 대한 점진적인 데이터 세트 업데이트로 발생하는 아티팩트를 성공적으로 극복하는 동시에 원래 프레임과 높은 유사성을 유지한다.\n' +
      '\n' +
      '이 방법의 정성적 결과는 그림 10, 11에 나와 있으며, 편집된 이미지에 대해 학습된 NeRF를 보여주는 보충 비디오를 제공한다.\n' +
      '\n' +
      '다음으로, 우리는 우리의 방법을 정량적으로 평가한다. 우리의 평가는 "statue", "person" 및 "alligator-toy" 장면(도 8, 9에 도시됨)에 걸쳐 수행되었다. 우리는 편집된 결과와 원본 이미지 사이의 커널 인셉션 거리(Kernel Inception Distance, KID) [5]로 측정한 결과, (1) 출력 이미지 품질 측정 방법, (5) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, (3) 출력 이미지 품질 측정 방법, ( (2) 사용자 연구를 통해 측정된 최종 3D 표현의 품질.\n' +
      '\n' +
      '영상 품질을 평가하기 위해 각 장면 및 편집에 대해 각 방법의 출력과 원본 장면 이미지 간의 KID를 계산한다. 우리는 모든 편집에 걸쳐 평균 점수를 보고한다. KID는 FID(Frechet Inception Distance) [23]과 관련이 있지만, 작은 (50,000\\(50,000\\) 이미지보다 적은) 데이터 세트로 더 잘 나타내도록 설계되었다. 완전성을 위해 FID도 보고합니다. 결과는 표 1에 나와 있으며, 모든 시나리오에서 제안된 방법은 향상된 시각적 충실도를 달성하여 기준선과 비교할 때 더 적은 시각적 아티팩트로 원본 장면에 더 충실함을 나타낸다.\n' +
      '\n' +
      '3D 표현의 품질을 평가하기 위해 각 방법에 대해 훈련된 NeRF에서 추출한 깊이 맵의 비디오를 기술 사용자에게 보여주는 사용자 연구를 수행했다. 그런 다음 사용자에게 목표 포즈와의 정렬 및 품질에 따라 비디오의 순위를 매겨 달라고 요청했습니다. 경쟁 방법의 깊이 맵은 일반적으로 NeRF에 의해 생성된 구멍과 구름을 포함하여 뷰 전체에 걸쳐 일관되지 않은 형상을 우회하므로 사용자 간에 더 낮은 점수를 받을 것으로 예상한다. 우리는 \\(20\\)의 고유 사용자들로부터 \\(120\\)의 응답을 수집했다. 표 1에서 우리는 모든 장면과 편집에 대해 평균을 낸 각 방법에 대한 평균 순위 및 윈레이트를 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Metric & IN2N-CN & CSD-CN & TokenFlow & Ours \\\\ \\hline KID (\\(\\downarrow\\)) & 0.280 & 0.090 & 0.440 & **0.072** \\\\ FID (\\(\\downarrow\\)) & 201 & 87 & 295 & **73** \\\\ \\hline User Study Rank (\\(\\downarrow\\)) & 2.26 & 2.12 & 3.70 & **1.90** \\\\ User Study Win-rate (\\(\\uparrow\\)) & 14.16\\% & 34.16\\% & 0.83\\% & **50.83\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 정량적 평가 메트릭. 이 방법은 충실도 및 사용자 선호도 측면에서 기준선보다 우수하다.\n' +
      '\n' +
      '우리의 방법은 대부분의 경우 기준선보다 선호되었으며, 이는 이미지에서 추출된 3D 표현이 원하는 편집과 더 높은 시각적 품질에 더 잘 정렬되었음을 나타낸다.\n' +
      '\n' +
      '##6 결론, 한계 및 향후 작업\n' +
      '\n' +
      '멀티뷰 편집 결과를 통합하기 위한 기법을 제시하였다. 우리는 편집 과정 전반에 걸쳐 이미지의 주의 특징을 점진적으로 통합하기 위한 수단으로 QNeRF를 소개한다. 제안된 방법은 일반적이며, 이미지 레이아웃이 수정되는 다양한 확산 기반 편집 기술에 적용할 수 있다. 여기에서 우리는 관절과 거친 경계 상자의 두 가지 유형의 컨트롤로 접근 방식을 시연했다. 이러한 조건은 의도적으로 관대하여 제어의 용이성을 제공한다.\n' +
      '\n' +
      '우리의 작업은 텍스트 대 이미지 모델의 생성력을 기반으로 한다. 그러나, 그것은 또한 그들의 공통된 약점을 계승한다. 예를 들어, 모델은 사람의 손을 생성하기 위해 고군분투한다. 유사하게, 그 모델은 여전히 미세한 세부 사항들을 환각할 수 있다. 우리 작업에서는 모양을 통합하는 데 중점을 두었지만 매우 상세한 객체에서는 공유된 기본 모양에도 불구하고 이러한 미세한 세부 사항이 일관되지 않는다. 기하학적 조작에 의해 가려지지 않는 상세한 배경 영역에서 유사한 환각이 관찰될 수 있다. 이러한 불일치는 편집된 다중 뷰 이미지에서 NeRF를 훈련할 때 흐릿한 영역으로 이어질 수 있다. 예제는 보충을 참조하십시오.\n' +
      '\n' +
      '우리의 작업에서 우리는 블랙박스 최적화기로 QNeRF를 최적화한다. 이는 강력한 통계 기법을 적용하여 걸러내는 것이 더 유리할 수 있는 경우에도 이상치에 대해 "평균"이 될 수 있다. 또한, 우리는 가우시안 스플랫[28]과 같은 다른 3차원 표현의 활용을 포함하여 기능을 통합하기 위한 대체 수단을 탐구하는 것을 구상한다.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      '우리는 그들의 초기 피드백과 유익한 토론에 대해 가우라프 파마르, 맥스웰 존스, 가이 테벳, 캥글 뎅, 또는 페렐에게 감사한다. 우리는 또한 우리의 원고를 교정하고 그들의 유용한 제안에 대해 Kfir Aherman, Yuval Alaluf, Ruihan Gao, Songwei Ge, Oren Katzir, Sean Liu, Sigal Raab, 그리고 Guy Tevet에게 감사한다. 이 프로젝트는 패커드 펠로우십, 소니 코퍼레이션 및 시스코 리서치에 의해 부분적으로 지원된다.\n' +
      '\n' +
      '그림 9: 우리의 접근법과 기준선 방법의 질적 비교. IN2N-CN 및 CSD-CN과 같은 "데이터셋 업데이트"에 의존하는 기술은 기하학을 변경하기 위해 고군분투한다. 이것은 IN2N-CN을 사용할 때 동상의 오른팔의 시끄러운 깊이와 CSD-CN이 있는 동상의 유령 같은 오른팔에서 볼 수 있다. 토큰플로우는 원본 이미지의 외관을 보존하기 위해 고군분투하고, 잡음이 많은 기하학을 생성하는 경향이 있어 편집된 프레임 간의 일관성이 부족함을 시사한다. 제안하는 방법은 기하학을 일관성 있게 변화시키면서 원본 이미지의 모양을 보존한다.\n' +
      '\n' +
      '그림 10: 우리 방법의 질적 결과. 여기에서 우리는 골격으로 이미지를 편집하고 각 예제에 대해 세 가지 다른 뷰의 샘플을 보여준다.\n' +
      '\n' +
      '그림 11: 우리 방법의 질적 결과. 여기에서 우리는 느슨한 깊이 맵[4]으로 이미지를 편집하고 각 예제에 대해 세 가지 다른 뷰의 샘플을 보여준다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, and Daniel Cohen-Or. Cross-image attention for zero-shot appearance transfer, 2023.\n' +
      '* [2] Chong Bao, Yinda Zhang, Banghang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In _The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR)_, 2023.\n' +
      '* [3] Bao and Yang, Zeng Junyi, Bao Hujun, Zhang Yinda, Cui Zhaopeng, and Zhang Guofeng. Neumesh: Learning disentangled neural mesh-based implicit field for geometry and texture editing. In _European Conference on Computer Vision (ECCV)_, 2022.\n' +
      '* [4] Shariq Farooq Bhat, Niloy J. Mitra, and Peter Wonka. Loosecontrol: Lifting contronlnet for generalized depth conditioning, 2023.\n' +
      '* [5] Mikolaj Binkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. In _International Conference on Learning Representations_, 2018.\n' +
      '* [6] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting. Sega: Instructing text-to-image models using semantic guidance, 2023.\n' +
      '* [7] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. _arXiv preprint arXiv:2211.09800_, 2022.\n' +
      '* [8] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing, 2023.\n' +
      '* [9] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, and KangXue Yin. Texfusion: Synthesizing 3d textures with text-guided image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [10] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. Openpose: Realtime multi-person 2d pose estimation using part affinity fields. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2019.\n' +
      '* [11] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3D generative adversarial networks. In _arXiv_, 2021.\n' +
      '* [12] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models, 2023.\n' +
      '* 3D 모델링 및 렌더링 패키지_. 블렌더 재단 스티칭 블렌더 재단 암스테르담, 2018년\n' +
      '* [14] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised NeRF: Fewer views and faster training for free. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [15] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.\n' +
      '* [16] Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. 2023.\n' +
      '* [17] Yutao Feng, Yintong Shang, Xuan Li, Tianjia Shao, Chenfanfu Jiang, and Yin Yang. Pie-nerf: Physics-based interactive elastodynamics with nerf, 2023.\n' +
      '* [18] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin Huang. Expressive text-to-image generation with rich text. In _IEEE International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [19] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. _arXiv preprint arxiv:2307.10373_, 2023.\n' +
      '* [20] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2023.\n' +
      '* [21] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. 2022.\n' +
      '* [22] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. 2023.\n' +
      '* [23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022.\n' +
      '* [25] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations, 2023.\n' +
      '* [26] Roy Kapon, Guy Tevet, Daniel Cohen-Or, and Amit H. Bermano. Mas: Multi-view ancestral sampling for 3d motion generation using 2d diffusion, 2023.\n' +
      '* [27] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In _Conference on Computer Vision and Pattern Recognition 2023_, 2023.\n' +
      '* [28] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM Transactions on Graphics_, 42(4), 2023.\n' +
      '* [29] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In _International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '\n' +
      '* [30] Umar Khalid, Hasan Iqbal, Nazmul Karim, Jing Hua, and Chen Chen. Latenteditor: Text driven local editing of 3d scenes, 2023.\n' +
      '* [31] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, and Jinwoo Shin. Collaborative score distillation for consistent visual synthesis, 2023.\n' +
      '* [32] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. _arXiv:2304.02643_, 2023.\n' +
      '* [33] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. In _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* [34] Juil Koo, Chanho Park, and Minhyuk Sung. Posterior distillation sampling. _arXiv preprint arXiv:2311.13831_, 2023.\n' +
      '* [35] Senmao Li, Taihang Hu, Fahad Shahbaz Khan, Linxuan Li, Shiqi Yang, Yaxing Wang, Ming-Ming Cheng, and Jian Yang. Faster diffusion: Rethinking the role of unet encoder in diffusion models, 2023.\n' +
      '* [36] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, and Bryan Russell. Editing conditional radiance fields. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [37] Nelson Max. Optical models for direct volume rendering. _IEEE Transactions on Visualization and Computer Graphics_, 1(2):99-108, 1995.\n' +
      '* [38] Chenlin Meng, Yutong He, Yang Song, Jiqjun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2022.\n' +
      '* [39] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _ECCV_, 2020.\n' +
      '* [40] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Trans. Graph._, 41(4):102:1-102:15, 2022.\n' +
      '* [41] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2021.\n' +
      '* [42] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In _Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Proceedings_. ACM, 2023.\n' +
      '* [43] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-Or. Localizing object-level shape variations with text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [44] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In _Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Proceedings_. ACM, 2023.\n' +
      '* [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021.\n' +
      '* [46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.\n' +
      '* [47] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar Averbuch-Elor. Vox-e: Text-guided voxel editing of 3d objects, 2023.\n' +
      '* [48] Ka Chun Shum, Jaeyeon Kim, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit Yeung. Language-driven object fusion into neural radiance fields with pose-conditioned dataset updates, 2023.\n' +
      '* [49] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2021.\n' +
      '* [50] Liangchen Song, Liangliang Cao, Jiatao Gu, Yifan Jiang, Junsong Yuan, and Hao Tang. Efficient-nerf2nerf: Streamlining text-driven 3d editing with multiview correspondence-enhanced diffusion models. _arXiv preprint arXiv:2312.08563_, 2023.\n' +
      '* [51] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahahi, Abhik Ahuja, David Mcallister, Justin Kerr, and Angjoo Kanazawa. Nerfsudio: A modular framework for neural radiance field development. In _Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Proceedings_. ACM, 2023.\n' +
      '* [52] Vadim Tschernezki, Iro Laina, Diane Larlus, and Andrea Vedaldi. Neural Feature Fusion Fields: 3D distillation of self-supervised 2D image representations. In _Proceedings of the International Conference on 3D Vision (3DV)_, 2022.\n' +
      '* [53] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1921-1930, 2023.\n' +
      '* [54] Can Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Clip-nerf: Text-and-image driven manipulation of neural radiance fields. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 2022.\n' +
      '* [55] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Nerf-art: Text-driven neural radiance fields stylization. _arXiv preprint arXiv:2212.08070_, 2022.\n' +
      '* [56] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul P. Srinivasan, Dor Verbin, Jonathan T. Barron, Ben Poole, and Aleksander Holynski. Reconfusion: 3d reconstruction with diffusion priors. _arXiv_, 2023.\n' +
      '\n' +
      '* [57] Tianhan Xu and Tatsuya Harada. Deforming radiance fields with cages. In _ECCV_, 2022.\n' +
      '* [58] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma, Rongfei Jia, and Lin Gao. Nerf-editing: geometry editing of neural radiance fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18353-18364, 2022.\n' +
      '* [59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023.\n' +
      '* [60] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and Guanbin Li. Dreameditor: Text-driven 3d scene editing with neural fields. _arXiv preprint arXiv:2306.13455_, 2023.\n' +
      '\n' +
      '## 부록 구현 세부사항\n' +
      '\n' +
      '한 장면의 다시점 영상 집합이 주어졌을 때, 우리는 OpenPose[10]를 사용하여 2D 포즈를 추출하는 것으로 시작한다. 그런 다음 키포인트를 삼각 측량하여 편집할 수 있는 장면에서 피사체를 묘사하는 3D 골격(_e.g_., Blender[13])을 얻는다. 원 영상의 카메라 시점을 이용하여 편집된 3차원 뼈대를 투영하여 편집된 2차원 뼈대를 얻는다.\n' +
      '\n' +
      '논문에 나타난 예시들에 대해, 우리는 장면당 2분이 소요된 블렌더 [13]에서 박스들 및 평면들을 사용하여 장면을 수동으로 모델링하였다. 이 프로세스는 LooseControl[4]의 데이터 처리 파이프라인으로 자동화될 수 있다. 모델링된 장면을 편집한 후, 원본 이미지의 카메라 시점으로 블렌더[13]를 사용하여 깊이를 렌더링했다.\n' +
      '\n' +
      '원본 이미지 보존은 앞서 언급한 바와 같이, MasaCtrl[8]에서와 같이 자기 주의 계층의 키와 값을 주입하여 원본 이미지의 외관을 보존한다. 구체적으로, MasaCtrl에 이어서, 해상도\\(32\\) 및 \\(64\\)의 디코더 계층에서, 제4 디노이징 단계로부터 이 주입을 수행한다. 이 주입 외에도 입력 영상에 대해 약간의 LoRA 미세 조정[24]을 수행한다. 모델은 300단계 동안 배치 크기가 2인 미세 조정됩니다. 공정성을 위해 모든 기준선에 대해 동일한 미세 조정 모델을 사용합니다.\n' +
      '\n' +
      '깊이 감독 QNeRF는 이미지에서 주요 피사체만 편집하기 때문에 이 객체 주변의 깊이는 원래 이미지와 동일하게 유지되어야 한다. 우리는 질의의 깊이가 대응하는 이미지와 동일하다고 가정한다. 따라서 본 논문에서는 다시점 영상의 원본 집합에 대한 NeRF를 학습하고 그 깊이를 렌더링한다. 그런 다음, 편집 전후에 객체의 결합을 거친 마스크를 사용하여 이러한 깊이 이미지를 마스킹한다[32]. DS-NeRF[14]에 소개된 깊이 손실항을 사용한다.\n' +
      '\n' +
      'Hyperparameters는 Stable Diffusion v1.5[45]를 사용하고 50단계의 잡음제거를 수행한다. 각 구간의 끝에서 QNeRF는 깊이 손실 계수 1을 가지고 10,000 단계로 학습되며, 질의 안내를 위해 본 논문에서는 수학식 4에 \\(\\alpha=60\\)을 설정하였다.\n' +
      '\n' +
      '## 부록 B 데이터세트 세부사항\n' +
      '\n' +
      '인물 데이터 세트는 Instruct-NeRF2NeRF에서 가져온 것이다. 게다가, 다른 모든 데이터 세트는 간단한 전화 카메라를 사용하여 우리가 수집했다. 아래는 각 데이터 세트의 크기입니다.\n' +
      '\n' +
      '## 부록 C 실험\n' +
      '\n' +
      '### Comparisons\n' +
      '\n' +
      '완성도를 위해 텍스트를 편집 인터페이스로 사용하여 바닐라 Instruct-NeRF2NeRF 방법과 비교한다. 우리는 NeRF가 다른 프롬프트와 하이퍼파라미터를 시도하더라도 원래의 것으로 남아 있음을 관찰한다. 즉각적인 "그의 손을 들어"에 대한 결과는 그림 12에 나와 있다.\n' +
      '\n' +
      '### 데이터세트 크기 분석\n' +
      '\n' +
      '우리의 통합은 QNeRF 훈련의 성공에 달려 있습니다. 따라서 이 방법은 NeRF를 훈련하기 위해 필요한 크기와 동일한 데이터 세트 크기를 요구한다. NeRF는 일반적으로 이미지 수의 증가에 따라 개선되며, 따라서 우리의 방법에도 동일하게 적용된다. 주목해야 할 것은, 멀티뷰 세트 크기가 NeRF를 트레이닝하기에 충분하다면, NeRF를 트레이닝하고 새로운 뷰를 렌더링함으로써 우리의 입력 세트의 크기를 증가시킬 수 있다는 것이다.\n' +
      '\n' +
      '완전성을 위해 입력 세트 크기가 우리의 접근법에 미치는 영향을 분석한다. 구체적으로, 50, 100, 150, 200, 250, 305 크기의 세트로 구성된 램프 장면에 대한 방법을 적용하고, 이러한 세트를 생성하기 위해 원본 세트에서 무작위로 이미지를 샘플링한다. 결과는 그림 13에 나와 있다. 결과에서 볼 수 있듯이 50개의 이미지 집합으로도 각 이미지를 격리하여 편집하는 MasaCtrl[8]에 비해 훨씬 더 일관된 결과를 얻을 수 있다. 우리의 결과는 데이터 세트의 크기를 증가시킬 때 개선되며 200개의 이미지가 포함된 세트와 매우 일치한다.\n' +
      '\n' +
      '## 부록 D 한계\n' +
      '\n' +
      '논문에서 논의한 바와 같이, 우리의 방법은 몇 가지 한계를 가지고 있다. 우리는 그것들을 그림 14에 보여준다.\n' +
      '\n' +
      '첫째, 사람의 두 편집된 이미지에서 볼 수 있듯이, 그의 손에는 손가락이 없고, 손바닥은 시각적 품질이 낮다.\n' +
      '\n' +
      '악어 장난감은 그의 다른 신체 부위(예를 들어, 팔과 얼굴)의 질감으로 볼 수 있듯이 매우 상세합니다. 우리의 방법은 장난감의 발에 있는 손가락 수에서 볼 수 있듯이 이 모든 미세한 세부 사항을 일관되게 만들기 위해 고군분투한다. 원래 이미지에서 장난감은 오른발에 네 개의 손가락을 가지고 있습니다.\n' +
      '\n' +
      '도 12: "그의 손을 들어"라는 프롬프트에 대한 Instruct-NeRF2NeRF의 결과.\n' +
      '\n' +
      '이 방법은 뷰 2의 편집된 이미지에서 4개의 손가락을 생성하는 반면 뷰 1에서는 5개의 손가락을 생성하는데, 이는 생성된 이미지 자체에서 해상도가 낮은 질의의 일관성을 보장하기 때문에 해상도 문제가 될 수 있다. 또한, 장난감 뒤의 데크가 원본 이미지와 편집 이미지 간에 다르게 보이는 것을 확인할 수 있다.\n' +
      '\n' +
      '## 부록 E 실행시간\n' +
      '\n' +
      '본 논문에서 제안한 방법의 실행시간은 멀티뷰 집합의 이미지 수에 선형적이다. 300개의 이미지 데이터 세트에 대한 전체 파이프라인의 일반적인 실행 시간은 NVIDIA RTX A5000 GPU에서 약 8.5시간이다. 여기에는 이미지를 반전하는 시간, 멀티뷰 세트의 노이즈 제거 및 프로세스에 따른 QNeRF의 트레이닝이 포함된다.\n' +
      '\n' +
      '이에 비해 IN2N-CN [20, 59]는 동일한 데이터셋과 GPU에서 대략 5.5시간이 소요된다. MasaCtrl[8]은 각 데이터 세트 업데이트 반복에서 DDIM 반전[49]을 필요로 하기 때문에 IN2N-CN은 바닐라 IN2N보다 느리다.\n' +
      '\n' +
      'CSD-CN[31]은 동일한 데이터 세트를 갖는 NVIDIA H100 GPU에서 약 3일이 걸린다.\n' +
      '\n' +
      '그림 14: 우리 방법의 한계를 입증한다.\n' +
      '\n' +
      '그림 13: 데이터 세트의 다양한 수의 이미지에 대한 접근 방식을 평가한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>