<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# _Gen4Gen_: Generative Multi-Concept Composition를 위한 Generative Data Pipeline\n' +
      '\n' +
      'Chun-Hsiao Yeh\\({}^{1}\\)\n' +
      '\n' +
      '등등 기여도\\({}^{\\dagger}\\) 교신 저자\n' +
      '\n' +
      'Ta-Ying Cheng\\({}^{2}\\)\n' +
      '\n' +
      '등등 기여도\\({}^{\\dagger}\\) 교신 저자\n' +
      '\n' +
      'He-Yen Hsieh\\({}^{3}\\)\n' +
      '\n' +
      'Chuan-En Lin\\({}^{4}\\)\n' +
      '\n' +
      'Yi Ma\\({}^{1,5}\\)\n' +
      '\n' +
      'Andrew Markham\\({}^{2}\\)\n' +
      '\n' +
      'Niki Trigoni\\({}^{2}\\)\n' +
      '\n' +
      'H.T. Kung\\({}^{3}\\)\n' +
      '\n' +
      'Yubei Chen\\({}^{6\\dagger}\\)\n' +
      '\n' +
      '\\({}^{1}\\)UC Berkeley\n' +
      '\n' +
      '옥스퍼드대학교\n' +
      '\n' +
      'Harvard University\n' +
      '\n' +
      'CMU\n' +
      '\n' +
      'HKU\n' +
      '\n' +
      'UC Davis\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근의 텍스트-이미지 확산 모델은 훈련을 위한 몇 가지 예만으로 새롭고 개인화된 개념(예를 들어, 자신의 애완동물 또는 특정 아이템)을 포함하는 이미지를 학습하고 합성할 수 있다. 본 논문은 텍스트-이미지 확산 모델을 개인화하는 이 영역 내에서 두 가지 상호 연결된 문제를 다룬다. 첫째, 현재 개인화 기술은 여러 개념으로 안정적으로 확장되지 못하며, 이는 사전 학습 데이터 세트(예: LAION)에서 복잡한 장면과 간단한 텍스트 설명 간의 불일치로 인한 것이라고 가정한다.\n' +
      '\n' +
      '_둘째, 다수의 개인화된 개념을 포함하는 이미지가 주어진 경우, 개인화된 개념의 유사성 정도뿐만 아니라 모든 개념이 이미지에 존재하는지 여부 및 이미지가 전체 텍스트 설명을 정확하게 반영하는지 여부에 대한 성능을 평가하는 총체적인 메트릭이 부족하다. 이러한 문제를 해결하기 위해 우리는 개인화된 개념을 텍스트 설명과 함께 복잡한 구성으로 결합하기 위해 생성 모델을 활용하는 반자동 데이터 세트 생성 파이프라인인 Gen4Gen을 소개한다. 이를 이용하여 멀티컨셉 개인화 작업을 벤치마킹할 수 있는 마이캠버스라는 데이터셋을 생성한다. 또한, 다중 개념, 개인화된 텍스트-이미지 확산 방법의 성능을 더 잘 정량화하기 위해 두 가지 점수(CP-CLIP 및 TI-CLIP)로 구성된 포괄적인 메트릭을 설계한다. 우리는 미래 연구자들이 MyCamvas에서 평가할 수 있는 경험적 자극 전략과 함께 Custom Diffusion 위에 구축된 간단한 기준선을 제공한다. 우리는 데이터 품질 개선 및 촉진 전략을 통해 모델 아키텍처 또는 훈련 알고리즘에 대한 수정 없이 다중 개념 개인화된 이미지 생성 품질을 크게 증가시킬 수 있음을 보여준다. 우리는 강력한 기반 모델을 체인화하는 것이 컴퓨터 비전 커뮤니티에서 다양한 도전적인 작업을 목표로 고품질 데이터 세트를 생성하는 유망한 방향이 될 수 있음을 보여준다. 프로젝트는 [https://Danielchyeh.github.io/Gen4Gen/._](https://Danielchyeh.github.io/Gen4Gen/._)에서 사용할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '실사 초상화에서 판타지 생물의 그림에 이르기까지 지난 한 해 동안 텍스트에서 이미지로의 확산 모델[4, 15, 26, 29, 33, 34, 36]의 능력이 현저하게 급증했다. 일부 최근의 노력은 이러한 생성 모델의 "개인화"에 초점을 맞추었으며, 여기서 사전 훈련된 텍스트-이미지 확산 모델은 이러한 개인 개념을 통합하는 새로운 장면(예를 들어, 도 1에 도시된 바와 같이 타임스퀘어의 야경에서 그들의 애완동물)을 생성하기 위해 사용자 제공 컨셉 이미지(예를 들어, 그들의 애완동물 또는 최근에 구입한 집 식물)의 최소 세트로 증강된다. 이 분야[1, 17, 19, 24, 35]에서 주목할 만한 작품은 생성 과정에 대한 사용자의 통제력을 높여 다양한 맞춤형 애플리케이션을 끌어내는 중요한 이정표이다.\n' +
      '\n' +
      '그러나, 여러 개념에 대한 개인화를 동시에 수행하고 주어진 텍스트 설명을 정확하게 따르도록 이미지 생성을 제어하는 것은 어려울 수 있다. 더욱이, [19]는 일반적인 경우에도, 안정된 확산[34]이 그들의 잠재 공간이 유사한 경우(예를 들어, 개 및 고양이) 동일한 이미지에 다수의 개념들을 디엔탱글링하지 못하고 제시하지 못함을 지적한다. 이 문제는 종종 후속적으로 미세 조정된 개인화 모델에 상속된다. 우리는 이 행동이 사전 훈련 데이터 세트(예: LAION[37])에서 텍스트 이미지 쌍 간의 불일치의 결과라고 추측한다. LAION의 많은 이미지는 종종 단일 객체 중심이며, 수반되는 캡션은 개별 개념에 대한 상세한 설명을 제공하기보다는 장면의 광범위한 개요를 제공한다. 텍스트와 복잡한 이미지 구성 사이의 대응 부족은 특히 개념들이 의미적으로 유사할 때 다수의 개념들을 생성하는 데 어려움을 제기한다.\n' +
      '\n' +
      '더 나은 데이터 품질이 더 나은 다중 개념 개인화로 이어질 것이라는 가설을 검증하기 위해, 우리는 다중 개념 중심 이미지와 텍스트 설명으로 개인화를 위한 개념 증명 데이터 세트를 구성하여 이 문제를 해결하면서 이전 모델 기반 기술과 달리 다른 경로를 가기로 결정했다. 이를 위해, 최근 발전된 기초 모델을 활용하며, 다양한 개인화된 개념의 구성을 위해 반자동 생성 데이터 파이프라인을 도입한다; 따라서 우리는 접근 방식인 _Gen4Gen_라고 부른다. 이 데이터셋 생성 파이프라인은 이미지 전경 추출[30], 대용량 언어 모델(LLM)[20], 이미지 인페인팅[29], 멀티모달 대용량 언어 모델(MLLM)[22]의 최근 발전을 활용하여 사용자 제공 사진의 세트를 조밀하게 대응하는 텍스트 설명이 있는 사실적이고 개인화된 다중 개념 이미지로 재구성한다. 또한, 보다 나은 이미지-텍스트 정렬을 위해 훈련 시간 동안 캡션 품질을 더욱 향상시키기 위해 프롬프트 엔지니어링의 영역으로 뛰어들었다. 10k 이상의 이미지를 생성하고 필터링하여 최종 벤치마킹 데이터셋 _MyCamvas_를 생성한다.\n' +
      '\n' +
      '더 나은 벤치마크를 만드는 과정에서 대부분의 벤치마크 [3, 17, 18, 19, 35]가 일반화의 보다 일반적인 사례에 초점을 맞추거나 사용자 조사에 의존하는 비교의 양이 많은 최대 3개의 개인화된 개념만 평가한다는 점을 감안할 때 모든 개인화 미세 조정 방법에 적용할 수 있는 합리적인 평가 메트릭의 중요성을 깨닫는다. 따라서, [3, 10, 13, 18, 28, 36]의 분류에서 영감을 얻고, 컴포지션-개인화-CLIP 점수(CP-CLIP)와 텍스트-이미지 정렬 CLIP 점수(TI-CLIP)를 제시한다. 두 점수는 구성 및 개인화 정확성과 다양한 시나리오에 일반화하는 능력을 모두 포함하는 단순하면서도 총체적인 메트릭으로 작용한다.\n' +
      '\n' +
      '본 논문에서는 기존의 방법[19, 35]과 개선된 데이터세트인 _MyCamvas_와 프롬프팅 전략을 사용하여 개인화된 개념의 정체성을 유지하면서 다양한 배경을 가진 사실적인 다중 개념 이미지를 생성하는 데 상당한 개선을 얻을 수 있음을 보여준다. 개선들은 매우 복잡한 구성들, 도전적인 안내(상대 포지션들), 및 다수의 의미적으로 유사한 개념들(예를 들어, 동일한 이미지 내의 두 개의 개의 아이덴티티들) 하에서 훨씬 더 명백하다. 반자동 데이터 생성 접근법을 사용하여 데이터 세트 품질을 개선함으로써 얻은 유망한 결과는 가까운 장래에 대규모 고품질 데이터 세트를 생성하기 위해 AI 기반 모델을 체인화하는 기회의 동기이다.\n' +
      '\n' +
      '전반적으로, 우리의 논문은 세 가지 중요한 발견에 기여한다:\n' +
      '\n' +
      '**(i)** **AI 기반 모델을 통합하는 것이 중요하다:** 반자동 데이터 세트 생성 파이프라인인 _Gen4Gen_는 고품질 데이터 세트를 생성하기 위해 캐스케이드 AI 기반 모델을 사용할 가능성을 소개하고 광범위한 작업에 도움이 될 가능성을 보유한다. **(ii) 데이터세트 품질 문제:** 우리의 개념 증명 _MyCanvas_ 데이터세트는 단순히 잘 정렬된 이미지와 텍스트 기술 쌍을 구성하는 것이 다중 개념 개인화의 작업을 상당히 향상시킬 것이라는 반영이다. **(iii) 다중 개념 개인화를 위한 벤치마크가 요구된다:** 우리의 전체론적 평가 벤치마크는 다중 개념 개인화의 작업에서 개인화 정확도, 구성 정확성, 및 텍스트-이미지 정렬을 고려한다. 우리는 메트릭, CP-CLIP 및 TI-CLIP 점수와 함께 우리의 _MyCanvas_ 데이터 세트가 이러한 목적을 해결하기 위한 더 나은 척도로 사용될 수 있기를 바란다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '**개인화된 텍스트-이미지 생성.** 미리 훈련된 텍스트-이미지 확산 모델 및 특정 개념을 나타내는 매우 적은 사용자 제공 이미지가 주어지면, 개인화의 목표는 모델을 미세 조정하고 개념에 매핑되는 특별한 식별자를 찾는 것이다. 그런 다음 식별자는 특정 개념을 포함하는 새로운 장면을 생성하는 데 사용된다. 텍스트 역전[14]과 드림부스[35]는 이 작업을 해결하는 첫 번째 몇 명입니다. 전자는 토큰 임베딩을 학습하여 모델을 변경하지 않고 매핑을 생성하는 반면, 후자는 일반화 능력을 보장하면서 전체 모델을 미세 조정한다. 사물[1, 7, 8, 9, 40, 41]의 충실도와 정체성 보존에 초점을 맞추고 다개념 개인화[2, 16, 17, 19, 23, 42]로 확장한 많은 작품들이 곧 이어졌다. 구체적으로, 이러한 방법들은 정규화된 미세조정(regularized finetuning)을 사용하여 데이터 희소성을 개선하는데 초점을 맞추었다. 예를 들어, Custom Diffusion[19]은 K층과 V층의 교차-어텐션 계층만을 미세조정하여 잠재적으로 과적합을 줄이고, SVDiff[17]은 가중치의 특이값을 최적화하며, Cones2[23]은 잔여 임베딩을 학습하여 일반적인 개념을 개인화된 개념으로 전환한다. 한편, 우리는 데이터 중심 접근법으로 동일한 문제를 목표로 하며, 데이터 세트를 단순히 개선하는 것이 다중 개념 개인화 측면에서 상당한 성능 도약으로 이어질 수 있음을 보여준다.\n' +
      '\n' +
      '**Text-to-Image Datasets and Benchmarks.** 확산 모델의 성공 이면의 주요 발화점은 방대한 양의 데이터[4, 29, 34, 36]에 있다. 텍스트 인코더 및 후속적으로 확산 모델 자체는 종종 수십억 [31, 32, 37] 규모의 데이터 세트에 구축된다. 불가피하게, 데이터세트 내의 많은 데이터는 특히 텍스트와 이미지 사이의 정렬 측면에서 품질이 좋지 않으며, 여기서 복잡한 장면은 단지 몇 개의 단어로 기술될 수 있다[37]. DALLE-3[4], RECAP[38] 등의 동시 작업에서도 이와 유사한 현상이 관찰되었다. 본 연구의 목적은 동일한 장면에서 여러 개념을 포함하는 개념 증명 개인화 데이터 집합과 잘 정렬된 텍스트 설명이 데이터의 양이 적은 경우에도 미세 조정 과정을 개선할 수 있음을 보여주는 것이다. 이러한 생성 모델에 대한 또 다른 개방형 과제는 전체론적으로 평가하는 방법이다. 최근 DrawBench[36], T2I-CompBench[18], HRS[3]와 같은 연구는 이러한 텍스트 대 이미지 확산 모델을 평가하는 데 있어 보다 총체적인 접근 방식을 제공했다. 다개념 개인화의 구체적인 과제를 평가할 때 첫 번째 총체적인 벤치마크를 제안하기 위해 그들로부터 영감을 얻는다.\n' +
      '\n' +
      '##3 _Gen4Gen_: 데이터 기반 다중 개념 개인화 기법\n' +
      '\n' +
      '여러 개념(예: 개 및 집 식물)을 캡처하는 사용자 제공 사진 세트가 주어지면, 다중 개념 개인화의 목표는 다양한 배경 및 구성을 가진 다중으로 구성된 새로운 이미지를 합성할 수 있도록 각 개념의 정체성을 학습하는 것이다. 이전 접근법 [19]에서 볼 수 있듯이, 문제 난이도는 이미지에 주입하려는 개인화된 개념의 수에 따라 크게 증가한다.\n' +
      '\n' +
      '기존 연구들[16, 17, 19, 23, 40, 42]은 학습 전략을 최적화하는 데 초점을 맞추고 있지만, 본 논문에서는 학습 전반에 걸쳐 데이터 품질을 향상시키는 것이 다중 개념 개인화된 이미지의 생성 품질을 향상시킨다는 것을 보여준다.\n' +
      '\n' +
      '이 절에서는 다개념 개인화의 발전에 대한 우리의 주요 기여에 대해 논의할 것이다.\n' +
      '\n' +
      '### 데이터세트 디자인 원칙\n' +
      '\n' +
      'LAION 데이터셋의 가장 심미적인 부분집합(_LAION-2B-en improved Aesthetics_)으로부터 [37]은 단순한 설명과 비교하여 이미지의 복잡성 사이의 불일치를 명확하게 볼 수 있다. 데이터세트는 주로 웹 검색되기 때문에 불일치가 발생할 수 있다. 예를 들어, 이미지에 대한 부정확하고 풍부한 텍스트 설명뿐만 아니라 여러 객체를 포함하는 이미지에 대한 낮은 해상도가 존재할 수 있다(자세한 내용은 부록 참조).\n' +
      '\n' +
      '따라서 우리는 이러한 불일치에 대한 영감을 도출하고 세 가지 핵심 설계 원칙을 제공한다. i)_** 상세한 텍스트 설명 및 이미지 페어링**: 텍스트는 그 대응하는 이미지와 잘 정렬되어야 하며, 전경 및 배경 객체 모두에 대한 정보를 제공한다. _ ii)_**합리적인 객체 레이아웃 및 배경 생성**: 인공 Cut-Mixes처럼 보이는 이미지를 피하고 LAION 데이터셋의 기존 정보를 활용하려면, 실제 상황에서 객체를 캡처할 수 있을 때 하나의 이미지에서만 객체가 공존하도록 해야 하며, 이미지에서의 위치가 말이 되도록 해야 한다. _ iii)_**고해상도**: 이것은 우리의 데이터세트가 고품질, 다중-개념 개인화된 이미지들을 생성하는 우리의 최종 목표에 부합하도록 보장할 것이다.\n' +
      '\n' +
      '### _Gen4Gen_ Pipeline\n' +
      '\n' +
      '도 2는 우리의 _Gen4Gen_ 생성 파이프라인을 예시한다. 그것은 1) 객체 연관 및 전경 분할, 2) LLM 유도 객체 구성, 3) 배경 재도색 및 이미지 재도색의 세 가지 주요 단계로 구성된다. 전체 데이터 생성 프로세스가 완전히 자동화되는 것이 최적이기는 하지만, 현재의 최신 모델[20, 29, 30]은 여전히 모든 단계에서 아티팩트를 포함한다. 우리의 주요 목표는 복잡한 이미지 생성에서 현재 모델의 기능을 이해하는 데 총체적인 벤치마크를 제공하는 것이므로 데이터 세트 준비에는 중간 및 최종 청소를 위한 인간 루프가 포함된다.\n' +
      '\n' +
      '**1) 객체 연관 및 전경 분할.** 우리의 데이터 세트는 \\(k\\) 객체 \\(O=\\{o_{i}\\}_{i=1}^{k}\\)로 시작되며, 여기서 각각의 객체 \\(o_{i}\\)은 \\(n\\) 이미지 \\(X_{o_{i}}=\\{x_{j}\\}_{j=1}^{n}\\)으로 표현된다. 이러한 세트는 드림부스, 커스텀 확산 및 온라인 저작권 없는 소스의 데이터 세트에서 얻는다. 우리는 먼저 객체 조합의 부분집합 \\(O^{\\prime}=\\{o_{a},o_{b},...\\}을 찾는다. ,O^{\\prime}\\in O\\은 그림 2와 같이 개, 고양이, 그리고 가정과 같은 자연스러운 장면에서 공존할 가능성이 직관적으로 있다.\n' +
      '\n' +
      '그런 다음, 각 소스 이미지 집합에서 O^{\\prime}\\(O^{\\prime}\\) 내의 객체를 나타내는 하나의 이미지를 가져와 이미지 집합을 형성한다. (X^{\\prime}=\\{x_{a}\\in X_{o_{a},x_{b}\\in X_{o_{b},...\\}\\) 그리고 각 이미지에 대한 전경을 획득하기 위해 DIS[30]을 적용한다. 이 영상을 \\(\\mathcal{D}(X^{\\prime})\\)이라 하고, 해당 마스크를 \\(\\mathcal{M}(\\mathcal{D}(X^{\\prime}))이라 한다. DIS는 카테고리 없는 현저성 객체 검출기이므로, 우리가 사용하고 있는 객체 세트에 불가지론적이라는 점에 유의한다. 흥미롭게도, 종종 공존하는 이러한 객체들 중 상당수는 잠재 공간의 유사성으로 인해 Custom Diffusion에 실패하고 심지어 안정적인 확산에 실패한 객체이기도 하다. 이것은 편리하게 데이터 세트를 더 어렵게 만들고 따라서 이 작업에 대한 더 나은 벤치마크 역할을 한다.\n' +
      '\n' +
      '**2) LLM-유도 객체 조합.** LLM의 제로-샷 능력[5, 12, 27, 39]을 이용하고 이들 객체 세트가 주어진 바운딩 박스의 가능한 세트를 조회한다[20]. 구체적으로, COCO 데이터셋 [21] (부록에 제공된 템플릿)에서 객체 바운딩 박스들이 주어진 바운딩 박스 포인트들을 제공하는 태스크를 설명하는 ChatGPT에 매우 적은 샘플들을 보여주고, ChatGPT에게 주어진 바운딩 박스들의 세트(O^{\\prime}\\)를 제공하도록 요청한다. 그런 다음 경계 상자 위치에 따라 개별 이미지를 \\(\\mathcal{D}(X^{\\prime})\\) 안에 배치하고 주어진 크기로 전경 이미지를 채색할 준비를 한다. 이 합성 전경 영상과 그에 대응하는 마스크를 \\(I_{fg}\\)과 \\(\\mathcal{M}(I_{fg})\\)이라 한다. 또한 동일한 LLM 모델(예를 들어, 정원에서의 ___는 개, 고양이, 가정에서 유효한 프롬프트)에 대한 유효성을 검증함으로써 가능한 장면들\\(I_{fg}\\)을 설명하는 프롬프트 집합을 얻는다.\n' +
      '\n' +
      '상술한 방법은 때때로 스케일링 문제로 이어질 것이며, 여기서 일부 객체는 비현실적이다.\n' +
      '\n' +
      '그림 2: **MyCamas_ Dataset 생성을 위한 _Gen4Gen_ 파이프라인의 개요.** 여러 개념을 나타내는 소스 이미지가 주어지면 이미지 전경 추출, LLMs, MLLMs 및 인페인팅에서 최근 진보를 활용하여 사실적이고 개인화된 이미지와 쌍을 이루는 텍스트 설명을 구성한다. 우리의 _Gen4Gen_ 파이프라인에는 세 단계가 있다. 먼저, 범주-진단적 현저성 객체 검출기를 적용하여 전경이 주어진 객체를 구도(O^{\\prime}\\) 내에서 분할한다. 두 번째 (2) 우리는 가능한 경계 상자 구성을 제공하기 위해 LLM에 문의한다. 이것은 합성 전경 이미지\\(I_{fg}\\)와 그에 대응하는 마스크\\(\\mathcal{M}(I_{fg})\\을 형성한다. 또한, LLM은 \\(O^{\\prime}\\)에 대한 잠재적인 장면들을 기술하는 일련의 배경 프롬프트들을 제공할 것을 요청한다. 세 번째(3)는 확산 인페인팅 모델을 사용하여 인터넷에서 조달한 배경 이미지\\(I_{bg}\\)에 임베딩하여 최종 이미지\\(I_{O^{\\prime}\\)을 재도색한다. 정렬을 유지하면서 다양한 텍스트 설명을 늘리기 위해 MLLM(LLaVA)을 질의하여 모든 조합의 하위 집합에 \\(I_{O^{\\prime}}\\)을 설명하는 상세한 캡션을 제공한다.\n' +
      '\n' +
      '다른 것보다 더 크다(예를 들어, 양이 차보다 크다). 이 문제를 완화하기 위해 GPT-4[27]를 통한 논리적 향상을 활용하고 각 경계 상자에 대한 스케일이 현실적이 되도록 요청한다. 구체적으로, 우리는 다음과 같이 GPT-4에 프롬프트한다: [객체 이름 목록이 주어지면, 당신의 작업은 가장 큰 객체에 대한 비율이 1.0으로 설정된 실제 용어로 이러한 객체에 대해 합리적인 축척 비율을 생성하는 것이다]. 이러한 축척은 이후 실제 비율을 적절하게 반영하기 위해 생성 레이아웃을 조정하는 데 사용됩니다.\n' +
      '\n' +
      '**3) Background Repainting and Image Recaption.** \\(\\mathcal{D}(X^{\\prime})\\)와 \\(\\mathcal{M}(\\mathcal{D}(X^{\\prime}))\\)으로부터 배경을 생성하는 가장 간단한 방법은 배경을 인페인팅하기 위해 최첨단 텍스트-이미지 확산 모델을 적용하는 것이다. 그러나, 우리는 모델이 매우 모호한 텍스트 프롬프트 이전에 후속하여 합리적인 배경(_즉, 객체가 절단되고 붙여진 것처럼 보이지 않는 경우)을 생성하도록 강제하는 것이 종종 예측 불가능한 결과를 초래한다는 것을 깨닫는다. 문제 설정을 단순화하기 위해 프롬프트를 반영한 고해상도 이미지를 사용한 다음 그로부터 "repainting"을 사용하면 생성 품질이 크게 향상된다는 것을 깨달았다(세부 정성적 삭제는 부록에 있다). 따라서, 텍스트-이미지 확산 인페인팅 모델(Stable-Diffusion-XL[29])이 주어지면, 저작권이 없는 소스2로부터 시작 배경 이미지(I_{bg}\\)를 프롬프트(p\\in P\\)로 찾고, 그림(I_{fg}\\)을 목표로 하고, 최종 이미지(I_{O^{\\prime}}=f(I_{fg},\\mathcal{M}(I_{fg}),I_{bg})를 얻는다. 재도장 단계에서는 평활화된 소프트 마스크를 사용하는 것이 이진 하드 마스크에 비해 전경 객체의 배경으로의 통합을 향상시키는 것을 관찰하며, 따라서 \\(5\\times 5\\) 윈도우로 \\(\\mathcal{M}(I_{fg})\\)에 대해 평균 평활화를 수행한다. (I_{O^{\\prime}\\)에 대응하는 프롬프트는 이제 \\(p\\)와 결합된 \\(O^{\\prime}\\) 내의 모든 객체를 나열하는 프롬프트이다.\n' +
      '\n' +
      '각주 2: [https://unsplash.com/](https://unsplash.com/)\n' +
      '\n' +
      '전체적인 벤치마크 데이터 세트를 구성하기 위해 노력함에 따라 섹션 3.1에 설명된 지침을 준수하면서 텍스트 설명의 다양성을 풍부하게 하고 확장된 길이의 경우에도 텍스트가 이미지를 밀접하게 따르도록 한다. 따라서, Multimodal Large Language Models (MLLMs) [11, 22, 27, 44]의 최근 업적에 비추어, 우리는 특정 명령어를 사용하여 자동 캡셔닝을 위해 최종 이미지 중 일부를 LLaVA-1.5 [22]에 공급합니다: _"이 이미지에서 보이는 것을 자세히 설명합니다. 단어 수는 30"_로 제한됩니다. 우리는 최대 77개의 토큰을 허용하는 CLIP의 컨텍스트 제약 조건[31]을 수용하기 위해 단어 제한을 제한한다. 본 논문의 리캡션은 _MyCamvas_ 데이터셋 내에서 10개의 객체 조합(O^{\\prime}\\)에 적용됨을 강조한다.\n' +
      '\n' +
      '구도당 이미지 및 텍스트 설명 집합을 얻기 위해 1)에서 3) 단계를 반복하며(O^{\\prime}\\), 이는 최종 _MyCamvas_ 데이터 세트에 포함된다. 예들은 도 3에 도시되어 있다.\n' +
      '\n' +
      '### Dataset Statistics\n' +
      '\n' +
      '_MyCamvas_ dataset의 경우, 150개의 객체(단일 이미지를 가진 일부 및 다중 이미지를 가진 다른 일부)를 수집하고, 41개의 가능한 구도(즉, 구도를 집합으로 지칭함) 및 10K 이상의 이미지를 생성한 다음, 재도색 결과(부록에서 더 자세한 내용) 측면에서 최상의 품질의 2684개의 이미지로 수동으로 필터링한다.\n' +
      '\n' +
      '그림 4는 우리의 _MyCamvas_의 통계를 보여준다. **a)**는 캡션당 단어 수의 구성을 나타낸다(섹션 3.4에 열거된 학습 및 추가 프롬프트 전략을 목표로 하는 희귀 토큰을 제외). 우리의 평균 단어 길이는 17.7이고, 모든 단어가 특정하고 이미지에 맞춤화된 20을 넘어 연장되는 길이의 약 30%이다. 한편, **b)* *는 사용자 정의 콘셉트101 및 드림부스 데이터 세트를 모두 능가하는 데이터 세트에 제시된 광범위한 객체를 나타낸다.\n' +
      '\n' +
      '워드 클라우드는 또한 **c)** 훈련 및 **d)** 추론 동안 사용한 다양한 객체 및 배경 프롬프트를 제시한다. 드림부스 및 커스텀 확산과 같은 이전 벤치마크와 비교하여, 우리의 데이터 세트는 다중 개념 조합으로 더 다양한 객체를 커버하므로 개인화의 작업을 측정하기 위한 더 포괄적인 데이터 세트이다.\n' +
      '\n' +
      '### 훈련시간 텍스트 프롬프트 개선\n' +
      '\n' +
      '데이터 세트 내의 이미지와 잘 정렬된 프롬프트를 설계하는 것 외에도 훈련 중에 최상의 프롬프트 설계가 무엇인지 탐색하는 데 한 걸음 더 나아가야 한다. 우리는 다음과 같은 몇 가지 경험적 결과와 직관을 공유한다.\n' +
      '\n' +
      '**글로벌 컴포지션 토큰.** 드림부스와 같은 이전 예술은 그들이 새로운 토큰을 매우 어렵고 도전적인 개념(예를 들어, 모네 아트처럼 추상적인 스타일)에 매핑하는 것을 배울 수 있다는 것을 보여주었다. 우리는 이 개념을 복잡한 구성에 적응시킨다. 각 객체에 대한 개별 토큰과 함께 글로벌 토큰을 도입함으로써, 우리의 모델은 상세한 장면 배열을 기술하는 능력을 향상시켜 보다 사실적이고 일관된 이미지 생성으로 이어진다.\n' +
      '\n' +
      '**훈련 중에 개념 토큰 프롬프트를 반복합니다.** 여러 개념을 포함하는 복잡한 구성이 종종 하나 또는 두 개의 개념이 누락될 수 있는 많은 경우에 주목합니다[6, 43]. 이것은 모델이 때때로 매우 긴 프롬프트가 주어진 세부 사항을 잊어버리기 때문일 수 있다. 따라서 우리는 훈련 중에 개념 토큰 프롬프트를 반복하는 전략을 사용한다. 이는 모델이 생성된 이미지들에서 각각의 특정된 컨셉의 존재를 보장하도록 장려하여, 전체적인 객체 지속성 및 완전성을 향상시킨다.\n' +
      '\n' +
      '** 배경 프롬프트를 통합.** 토큰 피쳐 공간에서 객체 아이덴티티로 배경들이 부주의하게 학습되는 이슈를 관찰한다. 배경 및 개념 구성을 풀기 위한 노력으로, 우리는 객체 아이덴티티만을 학습하는 개념 토큰을 장려하기 위해 학습 프롬프트에서 배경을 명시해야 한다.\n' +
      '\n' +
      '#개인화 구성 미터법\n' +
      '\n' +
      '개체 수를 증가시켜 개인화 도전의 난이도를 높임으로써, 중요한 세부 사항을 생성하는 방법을 학습하지 않는 모델과 새로운 배경을 생성하는 능력을 잃는 과적합 사이의 고유한 균형을 실현한다. 이 트레이드오프 문제는 1) _MyCanvas_와 같은 복잡한 데이터 세트가 평가에 사용되지 않았고 2) 훈련 세트에 완전히 오버핏되어 고품질 결과를 얻을 수 있기 때문에 이전에 사용된 벤치마크에 반영되지 않는다.\n' +
      '\n' +
      '이를 극복하기 위해 [3, 18]에서 영감을 도출하고 두 가지 메트릭을 제안한다. 첫 번째 메트릭인 컴포지션-개인화-CLIP 점수(CP-CLIP)는 컴포지션 및 개인화의 정확도를 평가한다. 두 번째 메트릭인 텍스트 이미지 정렬 CLIP 점수(TI-CLIP)는 다양한 텍스트 배경에 걸쳐 모델의 일반화 품질을 평가하여 잠재적인 과적합의 지표 역할을 한다.\n' +
      '\n' +
      '**장면 구성 및 개인화 정확성** 일반적인 개념의 구성에 주로 초점을 맞춘 기존의 벤치마크 및 메트릭과 달리 [3, 18]의 메트릭은 두 가지 핵심 질문을 다룬다: 1)_텍스트에서 언급된 각각의 개인화된 개념이 이미지 생성 중에 반영되었는가? (**구성 정확도**)_ 및 2) _생성된 개인화된 개념이 소스 대응물과 유사하게 보이나요? (**fidelity**)_\n' +
      '\n' +
      '전체 평가 프레임워크를 자동화하기 위해 개방형 어휘 객체 탐지를 위한 최신 모델인 OWL-ViT[25]로 시작한다. 개방형 어휘의 선택은 _MyCanvas_ 데이터세트 내의 임의의 객체를 캡처할 수 있게 한다. 구체적으로, \\(O^{\\prime}\\) 내의 모든 객체를 포함하는 것을 목표로 하는 생성된 이미지\\(I_{gen}\\)이 주어지면, 예측된 바운딩 박스들에 의해 특정된 임의의 크롭된 이미지 세트를 획득한다:\n' +
      '\n' +
      '\\[B_{pred}=\\{b_{pred_{1}},b_{pred_{2}},...\\}=\\text{OWL}(I_{gen},l_{O^{\\prime}}), \\tag{1}\\]\n' +
      '\n' +
      '도 4:_MyCanvas Dataset Statistics._ a) 파이 차트는 _MyCanvas_에 있는 이미지들의 대략 30%가 20개의 단어들의 길이에 걸쳐 텍스트 설명들과 쌍을 이루는 것을 묘사한다. b) 사용된 객체들의 다양성을 보여주기 위해 이미지들 내에서 사용된 카테고리들의 워드 클라우드. c) 및 d) 훈련 및 추론 중에 사용되는 빈번한 설명의 워드 클라우드는 비교의 공정성을 보장하기 위해 매우 다르다.\n' +
      '\n' +
      '도 3: **우리의 _MyCanvas_ Dataset.** 우리의 반자동 생성 데이터세트는 정확한 텍스트 설명(단축 및 상세)과 함께 고해상도, 사실적인 이미지를 갖는 복잡한 구도에서 다수의 개인화된 객체를 포함한다.\n' +
      '\n' +
      '여기서 \\(l_{O^{\\prime}\\)은 OWL-ViT의 대상 어휘로 사용하는 \\(O^{\\prime}\\) 내의 개별 레이블이다.\n' +
      '\n' +
      '모든 크롭된 이미지\\(b_{pred_{i}}\\in B_{pred}\\)에 대해 우리는 식에서 얻었다. 도 1을 참조하면, 이미지 집합에 대한 평균 클립 점수\\(S_{i,j}\\)를 다음과 같이 계산한다.\n' +
      '\n' +
      '\\[S_{i,j}=\\frac{\\sum_{x\\in X_{o_{j}}}C(b_{pred_{i}},x}{|X_{o_{j}}|}, \\tag{2}\\}}\n' +
      '\n' +
      '여기서 \\(C(\\cdot)\\)는 두 정규화된 이미지 특징 사이의 내적을 계산한다. \\(b_{pred_{i}}}\\)에 대한 최종 개인화 클립 점수는 다음:\n' +
      '\n' +
      '\\[S_{i}=\\max(\\{S_{i,j}\\}_{o_{j}\\in O^{\\prime}}). \\tag{3}\\\n' +
      '\n' +
      '만약 동일한 \\(o_{j}\\)에 해당하는 바운딩 박스가 두 개 이상일 경우, 우리는 \\(B_{pred}\\)에서 가장 높은 점수를 가진 것을 제외하고 모두 제거하므로, 텍스트에서 프롬프트된 개인화된 객체들이 생성된 이미지에 얼마나 많이 반영되는지 크기 \\(|B_{pred}|\\)가 적절하게 반영된다.\n' +
      '\n' +
      '마지막으로, 우리는 이미지당 전반적인 CP-CLIP 점수를 얻는다:\n' +
      '\n' +
      '\\[\\text{CP-CLIP}_{pred}=\\frac{\\sum_{b_{pred_{i}}\\in B_{pred}}S_{i}}{|O^{\\prime}|}. \\tag{4}\\}\n' +
      '\n' +
      '분모는 바운딩 박스의 수가 아닌 \\(O^{\\prime}\\) 내의 오브젝트의 수이며, 이는 특정 개인화된 오브젝트가 이미지 \\(I_{gen}\\)에 반영되지 않을 때 벌점으로 작용한다는 점에 유의한다. 더 많은 제한이 있을 때 처벌하지 않습니다.\n' +
      '\n' +
      '도 5: **다중 개념 구성에 대한 정성적 결과.** 구성 난이도의 오름차순으로 4개의 결과 세트를 제시한다(보다 개인화된 개념). 사용자 정의 확산과 같은 훈련 방법이 주어지면, 우리의 _MyCamvas_는 잠재 공간(예: 고양이 및 사자, 트랙터1 및 트랙터2)에서 유사한 디엔탱글링 객체 아이덴티티의 급격한 개선을 가져와 각 객체의 구별성을 보존한다. 우리의 프롬프트 전략을 추가하면 이미지 생성 동안 캡션을 정렬하는 데 훨씬 더 많은 개선(즉, 모든 객체가 적절하게 반영됨)을 얻을 수 있다. 더 많은 결과가 부록에 제시되어 있다.\n' +
      '\n' +
      '생성 모델은 텍스트 안내를 따르는 한 요청보다 더 많은 객체를 자유롭게 생성할 수 있어야 하기 때문에 의도보다 상자를 생성하는 것이다.\n' +
      '\n' +
      'Text-Image Alignment.** 과적합의 양을 정량적으로 측정하기 위해 TI-CLIP를 \\(I_{gen}\\)과 \\(I_{gen}\\)을 생성하는 데 사용된 prompt \\(p_{gen}\\) 사이의 CLIP 점수로 계산한다. TI-CLIP의 공식은 CP-CLIP와 매우 유사하지만(_i.e._, TI-CLIP는 전체 이미지의 바운딩 박스 및 개인화의 대상이 텍스트인 개인화 클립 스코어의 특별한 경우라고 생각할 수 있지만), 이는 모델의 일반화 품질의 직교 개념을 평가하고 있으므로 별도의 메트릭으로서 측정되어야 한다.\n' +
      '\n' +
      '높은 레벨 뷰에서 TI-CLIP는 생성된 이미지 전체를 가지고 배경 프롬프트를 측정한다; 개인화하는 동안 배경이 개선되었다고 믿을 이유가 없으므로 TI-CLIP의 유지 보수는 CP-CLIP 점수를 높일 때 목표로 하는 것이어야 한다. 이것은 모델이 훈련 세트 배경에 과도하게 맞지 않는다는 것을 보여준다.\n' +
      '\n' +
      '**Score Interpretability.** 실제로 CP-CLIP에서 좋은 점수는 0.5 정도이며, TI-CLIP는 증가하지 않고 유지되는 점수여야 함을 깨닫는다. 우리는 부록에서 점수 해석 가능성의 세부 사항에 대해 파고든다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### 기준 및 구현 세부사항.\n' +
      '\n' +
      '먼저 Custom Diffusion 구현을 시작하며, 1) 개별 소스 컨셉 이미지를 이용한 Custom Diffusion, 2) _MyCamvas_로 구성된 Custom Diffusion, 3) _MyCamvas_로 구성된 Custom Diffusion을 기반으로 하는 prompting 전략의 정량적, 정성적 성능을 측정한다. 우리는 재현 가능한 코드 베이스와 이전 방법과 광범위한 비교로 인해 사용자 정의 확산을 선택한다. 각 구성에 대해 위에서 언급한 각 방법(부록의 학습 세부 사항)에 대한 모델을 학습한다.\n' +
      '\n' +
      '평가를 위해 모든 구성에 대해 최상의 체크포인트를 사용합니다. 우리는 훈련 중에 사용되는 것과 뚜렷하게 다른 구성당 프롬프트를 선택한다. 이를 통해 배경 설명이 모든 데이터에 대해 보이지 않기 때문에 각 모델의 일반화 능력을 더 잘 분석할 수 있다. OWL-ViT 경계 상자 추출과 CP-CLIP 및 TI-CLIP의 두 CLIP 기반 점수를 계산하기 위한 백본으로 ViT-B-32를 사용한다.\n' +
      '\n' +
      '### Quantitative Analysis\n' +
      '\n' +
      '표 1은 객체의 수로 구성된 모든 구성에 대한 결과를 보여준다. 41개의 텍스트 프롬프트를 사용하고 각 구성에 대해 프롬프트당 6개의 샘플을 사용하여 총 246개의 생성된 이미지를 생성했다. 사용자 정의 확산은 원본 소스 이미지를 사용하여 학습할 때, 구성된 _MyCamvas_ 데이터 세트를 사용하는 것과 비교하여 50%의 성능 감소를 보이는 것이 분명하다. 사용자 정의 확장에 프롬프트 전략을 적용하면 CP-CLIP 점수가 더욱 증폭됩니다. 특히, 배경 일반화를 나타내는 TI-CLIP 점수는 모든 방법에 걸쳐 일관성을 유지하여 구성 정확도의 관찰된 증가가 과적합의 결과가 아님을 확인한다.\n' +
      '\n' +
      '### Qualitative Comparisons\n' +
      '\n' +
      '그림 5에서 우리는 추론 중 도전적이고 상세한 프롬프트에 대한 질적 결과에 특히 초점을 맞춘다. 이 프롬프트는 훈련 장면과 구별되는 새로운 장면에서 개념을 생성하는 모델의 능력을 시험하기 위해 신중하게 설계되었으며, 알려진 다른 객체(예: 카누 위의 고양이, 플로에 위의 사자)와 개념을 구성하고, 개념의 상대적 위치(예: 배경_에서 나란히_)를 설명한다. 1) 원래 소스 이미지와 사용자 정의 확산, 2) 소스에서 데이터 세트를 구성한 사용자 정의 확산, 3) 세 가지 설정에서 정성적 결과를 비교한다. MyCamvas_와 함께 Custom Diffusion 위에 구축된 우리의 프롬프트 전략. 그림 5에 묘사된 바와 같이, 매우 도전적인 배경 기술에도 불구하고, 우리의 구성 전략은 종종 안정적인 확산에서 실패 사례인 잠재 공간(예: 사자 및 고양이, 2개의 트랙터)에서 유사한 물체를 성공적으로 분리한다[34]. 더욱이, 구도의 난이도가 증가함에 따라(즉, 각 행을 하강시키는 것은 구도의 객체들의 수를 증가시킴), 우리의 프롬프트 방법은 생성 프로세스 동안 어떠한 개념도 남겨지지 않도록 보장한다. 특히, 우리는 _MyCamvas_ dataset을 사용함으로써, 기존의 개인화 모델들(예를 들어, Custom Diffusion)의 생성 품질이 상당히 향상될 수 있음을 입증한다(다른 방법들에 대한 결과들이 부록에서 발견될 수 있음).\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**Evaluation of _MyCamvas_ Generation Quality.** We developed filtering tool(description in Appendix) to evaluate the quality of 800 images generated by our _Gen4Gen_ pipeline. We evaluate each image based: _1) inclusion of personalized concepts, 2) their appropriate placement, and 3) 시각적 아티팩트의 제외,_1에서 5까지 순위를 매깁니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & \\(c-3\\) Objects & & 4 Objects & & 5 Objects \\\\  & **CP-CLIP** & **TI-CLIP** & **CP-CLIP** & **TI-CLIP** & **CP-CLIP** & **TI-CLIP** \\\\ \\hline CP - Source Images & 0.26 & 0.16 & 0.21 & 0.13 & 0.23 & 0.17 \\\\ CP -45\\(\\mu\\)m\\(\\alpha\\) & 0.41 & 0.17 & 0.47 & 0.17 & 0.50 & 0.15 \\\\ Ours + _MyCamvas_ & **0.51** & 0.17 & **0.55** & 0.16 & **0.57** & 0.14 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ** 개인화 효과에 대한 비교. 본 논문에서 제안한 메트릭(CP-CLIP, TI-CLIP)을 사용하여 세 가지 다른 설정에서 정량적 성능을 측정한다. CD는 사용자 정의 확산을 나타내고 프롬프트는 CD 훈련 파이프라인에 구축된 프롬프트 전략을 나타낸다. CP-CLIP의 베스트 스코어는 **볼드**로 강조 표시됩니다. TI-CLIP는 프롬프트를 여전히 반영하는지 여부를 나타내는 지표로 작용하므로 모든 벤치마크_.** 이후에 점수 분포를 분석하기 위해 이러한 순위를 집계한다. 4/5 등급의 이미지만 _MyCanvas_ 데이터 세트에 추가되었다. 표 2의 결과는 고품질 이미지를 생성하는 것이 관련된 4개 미만의 개념으로 더 실현 가능하다는 것을 나타낸다.\n' +
      '\n' +
      '**트레이닝 데이터 크기 v.s. Concepts.** 우리는 다양한 수의 이미지(1 내지 100)로 트레이닝하는, 도 6에 예시된 분석을 제공한다. 3\\(\\leq 3\\) 개념에 대한 구도를 훈련할 때 매우 적은 이미지로 충분하지만, 4개 이상의 개념이 있을 때 10~50개의 이미지 사이에서 훈련이 안정화된다. 이것은 우리의 데이터 세트 크기가 안정적인 성능을 얻기에 충분하다는 것을 보여준다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '결론적으로, 우리는 다중 개념 개인화를 위한 벤치마크로 잘 정렬된 이미지와 텍스트 설명이 있는 데이터 세트인 _MyCanvas_를 제시한다. 우리는 데이터 품질이 복잡한 구성에 대해 훨씬 더 나은 이미지 생성으로 이어질 수 있음을 보여주기 위해 일부 교육 신속한 수정 및 전체론적 메트릭과 함께 데이터 세트에 대한 광범위한 연구를 제시한다. 우리는 우리의 기여가 개인화된 텍스트 대 이미지 생성 및 자동화된 데이터 세트 생성의 가능성에 대한 선견지명이 되기를 바란다.\n' +
      '\n' +
      '**제한.** 도 7에 묘사된 바와 같이, 우리의 현재 데이터 생성 파이프라인은 특히 도전적인 시나리오에서 여전히 결함을 포함한다. 이러한 어려움은 객체 위치에 대한 비실용적인 지침을 제공하는 LLM과 객체에 아티팩트를 도입하는 확산에서 비롯된다. 현재로서는 이러한 문제를 해결하기 위해 반자동 심사 프로세스에 의존합니다. 향후 작업은 필터링 프로세스를 자동화하고 데이터 세트 품질을 평가하는 데 집중할 수 있다. 또한, 새로운 MLLM들은 풍부한 멀티모달 이해도를 가지고 있다[27, 44], 우리는 더 나은 바운딩 박스 생성을 위한 추가적인 시각적 안내들을 포함할 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Yuval Alaluf, Elad Richardson, Gal Metzler, and Daniel Cohen-Or. A neural space-time representation for text-to-image personalization. _arXiv preprint arXiv:2305.15391_, 2023.\n' +
      '* [2] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from a single image. _arXiv preprint arXiv:2305.16311_, 2023.\n' +
      '* [3] Eslam Mohamed Bakr, Pengzhan Sun, Xiaogian Shen, Faizan Farooq Khan, Li Erran Li, and Mohamed Elhoseiny. Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 20041-20053, 2023.\n' +
      '* [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Lia, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. [https://cdn.openai.com/papers/dall-e-3.pdf](https://cdn.openai.com/papers/dall-e-3.pdf), 2023.\n' +
      '* [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [6] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. _ACM Transactions on Graphics (TOG)_, 42(4):1-10, 2023.\n' +
      '* [7] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disenbooth: Identity-preserving disentangled tuning for subject-driven text-to-image generation. _arXiv preprint arXiv:2305.03374_, 2023.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline  & **Rank: 1** & **Rank: 2** & **Rank: 3** & **Rank: 4** & **Rank: 5** & **Total Images** \\\\ \\hline \\(\\Leftarrow\\)**3 **Concepts** & **9**(34\\(\\%\\)) & **43**(63\\(\\%\\)) & **72**(27\\(\\%\\)) & **84**(31\\(\\%\\)) & **56**(212\\(\\%\\)) & **264** \\\\ \\hline\n' +
      '**4 **Concepts** & **16**(6\\(\\%\\)) & **53**(39\\(\\%\\)) & **122**(42\\(\\%\\)) & **54**(20\\(\\%\\)) & *32**(12\\(\\%\\)) & *207**\n' +
      '**5 **Concepts** & **19**(7\\(\\%\\)) & **43**(234\\(\\%\\)) & **127**(27\\(\\%\\)) & **42**(25\\(\\%\\)) & **18**(67\\(\\%\\)) & **209** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **_MyCanvas_의 품질 평가 (랭크: 1 내지 5).** 우리의 평가 기준은: _1) 개인화된 개념의 포함, 2) 그들의 배치의 정확성, 및 3) 시각적 아티팩트의 부재._ 이미지는 이러한 요인에 따라 1에서 5까지 순위가 매겨진다.\n' +
      '\n' +
      '그림 6: **Dataset Size에 기반한 훈련 성능 \\(\\leq 3\\) 개념을 가진 구도에 대한 훈련은 효능을 위해 더 적은 수의 이미지를 필요로 하는 반면, 안정적인 성능은 4개 이상의 개념에 대해 10~50개의 이미지로만 달성된다.\n' +
      '\n' +
      '도 7: **Failure Dataset Creation Case.** 우리의 데이터 생성 파이프라인은 두 가지 주요 이유로 인해 어려운 구성 시나리오에서 실패한다: 1) LLM이 비현실적인 오브젝트 위치를 제안하면, 오브젝트의 아이덴티티가 확산 인페인팅 프로세스(예를 들어, 컵이 펜던트 램프로 변형됨), 2) 확산 인페인팅은 때때로 오브젝트에 아티팩트를 도입한다.\n' +
      '\n' +
      '* [8] Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing Liu, Kang Du, et al. Photoverse: Tuning-free image customization with text-to-image diffusion models. _arXiv preprint arXiv:2309.05793_, 2023.\n' +
      '* [9] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William W Cohen. Subject-driven text-to-image generation via apprenticeship learning. _arXiv preprint arXiv:2304.00186_, 2023.\n' +
      '* [10] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3043-3054, 2023.\n' +
      '* [11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructublip: Towards general-purpose vision-language models with instruction tuning, 2023.\n' +
      '* [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* [13] Tan M Dinh, Rang Nguyen, and Binh-Son Hua. Tise: Bag of metrics for text-to-image synthesis evaluation. In _European Conference on Computer Vision_, pages 594-609. Springer, 2022.\n' +
      '* [14] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. _arXiv preprint arXiv:2208.01618_, 2022.\n' +
      '* [15] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis, 2022.\n' +
      '* [16] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. _arXiv preprint arXiv:2305.18292_, 2023.\n' +
      '* [17] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 7323-7334, 2023.\n' +
      '* [18] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. _arXiv preprint arXiv:2307.06350_, 2023.\n' +
      '* [19] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1931-1941, 2023.\n' +
      '* [20] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. _arXiv preprint arXiv:2305.13655_, 2023.\n' +
      '* [21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.\n' +
      '* [22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '* [23] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis with multiple subjects. _arXiv preprint arXiv:2305.19327_, 2023.\n' +
      '* [24] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. _arXiv preprint arXiv:2307.11410_, 2023.\n' +
      '* [25] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Arxivindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection. In _European Conference on Computer Vision_, pages 728-755. Springer, 2022.\n' +
      '* [26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.\n' +
      '* [27] OpenAI. Gpt-4 technical report, 2023.\n' +
      '* [28] Vitali Petsiuk, Alexander E Siemenn, Saisamrit Surbehera, Zad Chin, Keith Tyser, Gregory Hunter, Arvind Raghavan, Yann Hicke, Bryan A Plummer, Ori Kerret, et al. Human evaluation of text-to-image models on a multi-task benchmark. _arXiv preprint arXiv:2211.12112_, 2022.\n' +
      '* [29] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [30] Xuebin Qin, Hang Dai, Xiaobin Hu, Deng-Ping Fan, Ling Shao, and Luc Van Gool. Highly accurate dichotomous image segmentation. In _ECCV_, 2022.\n' +
      '* [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.\n' +
      '* [33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022.\n' +
      '* [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '* [35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.\n' +
      '* [36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* [37] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.\n' +
      '* [38] Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Levitathan. A picture is worth a thousand words: Principled reexponiting improves image generation. _arXiv preprint arXiv:2310.16656_, 2023.\n' +
      '* [39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [40] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 15943-15953, 2023.\n' +
      '* [41] Zijie Wu, Chaohui Yu, Zhen Zhu, Fan Wang, and Xiang Bai. Singleinsert: Inserting new concepts from a single image into text-to-image models for flexible editing. _arXiv preprint arXiv:2310.08094_, 2023.\n' +
      '* [42] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multi-subject image generation with localized attention. _arXiv preprint arXiv:2305.10431_, 2023.\n' +
      '* [43] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. _arXiv preprint arXiv:2206.10789_, 2(3):5, 2022.\n' +
      '* [44] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '\n' +
      '## Appendix\n' +
      '\n' +
      '이 보충에서 우리는 먼저 LAION 데이터 세트에 대한 분석에 대한 추가 세부 정보를 제공할 것이다(섹션 6). 섹션 7에서는 _MyCamvas_ 데이터 세트를 수집하는 과정에 대한 추가 세부 정보를 제공합니다. 섹션 8은 드림부스가 사용하는 _MyCamvas_ 데이터 세트에 대한 추가 실험을 제공한다. 섹션 9에서는 메트릭에 대한 자세한 내용을 설명합니다. 섹션 10은 접근법의 추가 구현 세부 정보를 제공한다.\n' +
      '\n' +
      '## 6 라이온 반사\n' +
      '\n' +
      '**분석.** 그림 8은 LAION[37]에서 가장 심미적인 네 가지 이미지의 텍스트와 이미지 쌍을 묘사하고 있다. 특히, 그것들은 LAION-aesthetic dataset에서 가장 높은 미적 점수를 가지고 있다. 정성적 결과는 심미성이 높은 이미지는 이미지의 복잡성과 높은 상관관계가 있음을 보여준다.\n' +
      '\n' +
      '이 데이터 세트 내에 높은 오정렬이 존재한다는 점에 주목하는 것이 중요하다. 이러한 오류에는 설명 부족(예: 내부 항목에 대한 구체적인 설명이 없는 "베이커리 스위츠" 또는 이미지에서 램프나 자동차를 설명하지 않은 "이브닝 뉴욕"), 불필요한 설명(예: "ON SALE", "1:12" 및 "HOL770205)이 포함되며, 이는 모두 모델을 미세 조정할 때 다중 개념 이미지 생성의 품질을 방해할 수 있다.\n' +
      '\n' +
      '개인화 작업을 목표로 하기 때문에 현재 데이터 품질은 작지만 향후 작업은 기초 생성 모델을 더 잘 개선하기 위해 품질뿐만 아니라 데이터 양을 개선하는 데 집중할 수 있을 것으로 예상한다.\n' +
      '\n' +
      '##7_MyCamvas_Dataset\n' +
      '\n' +
      '__MyCamvas_예.___MyCamvas_예. 그림 9에서 우리는 _MyCamvas_에서 41개의 구도 내에서 더 많은 예를 보여준다. 이러한 구성은 정원에서 하늘과 바다에 이르기까지 뚜렷한 배경에 대해 설정된 다양한 수의 개념을 포함한다. 각 구성은 정확하고 적절한 텍스트 설명으로 간결하거나 상세한 형태로 이미지의 포괄적인 묘사를 보장한다.\n' +
      '\n' +
      '**Background Repainting.** 도 10에서, 1) 초기 노이즈 배경을 사실적인 이미지 배경과 비교하고, 2) 전경 객체에 하드 또는 소프트 마스크를 사용하는 것의 영향을 평가하는 두 가지 주요 측면에 대한 삭제를 수행한다. 확산 인페인팅 모델 \\(f(.)\\)[29]에 대해, 우리는 개념을 지정하는 마스크 \\(\\mathcal{M}(I_{fg})\\과 함께 구성된 개념 \\(O^{\\prime}\\)을 갖는 전경 이미지 \\(I_{fg}\\)을 제공하는 반면, 모델 \\(f(.)\\)은 주어진 프롬프트 \\(p\\)에 기초하여 배경을 인페인팅할 수 있다.\n' +
      '\n' +
      '실험에서는 검은 배경이 포함된 전경 영상\\(I_{fg}\\)을 사용하였다. 처음에는 배경에 노이즈를 추가하고 확산 인페인팅 모델에 입력하여 제공된 배경 프롬프트(예: 정원)를 기반으로 배경을 생성함으로써 이를 향상시키려고 시도한다. 그러나, 그림 10-(a)는 확산 모델이 초기 노이즈로부터 직접 스크래치로부터 배경을 생성하도록 허용하는 것이 일관되게 비현실적인 배경 및 아티팩트를 초래한다는 것을 드러낸다.\n' +
      '\n' +
      '또한 확산 인페인팅 모델의 입력으로 \\(\\mathcal{M}(I_{fg})\\)을 사용하면 사물과 배경 사이의 가시적인 경계선이 나타나며, 이는 복사 붙여넣기 유사 아티팩트와 유사하다(그림 10-(b)). 이를 완화하기 위해, 재도장 단계에서는 평활화된 소프트 마스크를 사용하여 전경 객체를 배경에 통합시키는 것을 관찰한다. 따라서 5 x 5 윈도우로 \\(\\mathcal{M}(I_{fg})\\)에 대한 평균 평활화를 수행한다. 이 섹션에서는 이러한 절제 전략을 탐색하고 전경 강화 기술에 대한 개선을 제공한다.\n' +
      '\n' +
      '**이미지 품질 및 정렬에 대한 평가 인터페이스.** 도 12에 예시된 바와 같이, 이미지 품질 및 이미지-텍스트 정렬을 모두 평가하기 위해 맞춤화된 인터페이스를 제작했다. 사용자는 이미지를 1에서 5까지 순위를 매겨 선호도를 표현하는 간단한 선택권을 가지고 있다. 화질에 대한 평가 기준은 1) 이미지에 지정된 모든 개념의 존재를 보장하는 것, 2) 개념의 합리적인 위치를 확인하는 것, 3) 이미지 또는 개념 객체에 아티팩트가 없는지 확인하는 것 등 세 가지 주요 고려 사항을 포함한다.\n' +
      '\n' +
      '**LLM 유도 배경 프롬프트에 대한 템플릿**\n' +
      '\n' +
      '1. 먼저 LLM(ChatGPT[27])을 이용하여 [20]에서 제공하는 템플릿을 따라 배경 프롬프트를 생성한다.\n' +
      '\n' +
      '도 8: ** Most Aesthetic LAION 텍스트-이미지 쌍의 예.** 이러한 예는 설명이 불충분한 경우(예를 들어, 특정 아이템 설명이 결여된 "베이커리 스위츠", 또는 램프 또는 자동차에 대한 세부사항을 생략한 "이브닝 뉴욕") 또는 불필요한 요소(예를 들어, "ON SALE", "1:12", 및 "HOL770205")를 포함하는 경우를 포함하는 LAION 데이터세트 내의 텍스트-이미지 오정렬을 나타낸다[37]. 이러한 불일치는 고품질 다중 개념 이미지 생성을 위한 모델 미세 조정 동안 문제를 제기한다.\n' +
      '\n' +
      '2. 작업 설명 후 출력 형식을 명확히 하기 위해 몇 가지 선별된 예를 제공한다. 푸른 트럭 왼쪽에 주차한 녹색 차 사진 빨간색 풍선과 하늘에 새가 있는 사진 정원, 숲, 잔디밭, 숲, 잔디밭에 사과가 그려진 나무 탁자 그림 거실에 사과가 그려진 그림 거실에 사과가 그려진 나무 탁자 그림 거실에 사과가 그려진 나무 탁자 그림 ** 캡션:** 새로 꾸며진 집에 있는 검정색 냉장고. **Scene:** 주방, 방, 사무실 **Background:** 주방, 방, 사무실\n' +
      '3. 우리는 구성(예: 자동차, 고양이, 개, 집)을 프롬프트하고 LLM으로부터 세 가지 가능한 배경을 얻는다. *Retrieved Background: **Text prompt: **자동차 한 대, 고양이 한 대, 개 한 대, 집 한 대 ** background prompt: *길, 교외 지역, 시골 지역\n' +
      '\n' +
      '**LLM 유도 객체 구성 레이아웃에 대한 템플릿**\n' +
      '\n' +
      '1. 대상 작업 및 원하는 레이아웃 형식을 정의한다. **Few-Shot Template:*****당신은 지능형 경계 상자 생성기입니다. 사진, 이미지 또는 그림에 대한 캡션을 제공합니다. 당신의 작업은 장면을 설명하는 배경 프롬프트와 함께 캡션에서 언급된 객체들에 대한 경계 박스들을 생성하는 것이다. 이미지들은 크기 512x512이고, 바운딩 박스들은 이미지 경계들을 중첩하거나 넘어서는 안 된다. 각 바운딩 박스는 (객체명, [상단-좌측 x 좌표, 상단-좌측 y 좌표, 박스 폭, 박스 높이])의 포맷이어야 하며, 정확히 하나의 객체를 포함한다. 가능하면 상자를 더 크게 만드세요. 경계 상자에 이미 제공된 객체를 배경 프롬프트에 넣지 마십시오. 필요하다면, 당신은 합리적인 추측을 할 수 있다. 캡션이 영어가 아닐 수도 있지만 객체 설명 및 배경 프롬프트를 영어로 생성합니다. 백그라운드 프롬프트에 존재하지 않거나 제외된 개체를 포함하지 마십시오. 원하는 형식은 아래 예제를 참조하십시오. 대화 상자도 대상이라는 점에 유의하시기 바랍니다. 가능성 있는 추측의 대상이 어디에 있을지 몰라 왼쪽 상단 x 좌표 + 상자 너비는 512보다 높지 않아야 합니다. 왼쪽 상단 y 좌표 + 상자 높이는 512보다 높지 않아야 합니다. 경계 상자가 서로 겹치지 않도록 생성되었습니다.\n' +
      '2. LLMin이 컨텍스트 및 포맷 요구 사항을 이해하는 데 도움이 되는 관련 예를 제공한다.\n' +
      '\n' +
      '**Few-Shot Queries:**\n' +
      '\n' +
      '테이블에 노트북이 있는 사람 몇 명\n' +
      '\n' +
      '**Objects:**[["keyboard", [141, 309, 197, 117]]\n' +
      '\n' +
      '**캡션:** 전체 코스 저녁 식사는 음료와 디저트를 포함한 큰 접시에 제공됩니다.\n' +
      '\n' +
      '**Objects:**[["cup", [0, 69, 92, 165]], ["cup", [18, 10, 96, 137]], ["fork", [165, 326, 61, 168]], ["carrot", [95, 308, 84, 71]], ["cake", [244, 41, 91, 93]]\n' +
      '\n' +
      '**Caption :** 다색 식탁보에 제공되는 흰색 접시\n' +
      '\n' +
      '**Objects:**[["포크", [462, 120, 36, 248]], ["칼", [0, 162, 74, 212]], ["오렌지", [287, 129, 92, 100]], ["브로콜리", [199, 163, 186, 170]]\n' +
      '\n' +
      '...\n' +
      '\n' +
      '테이블 위의 바나나 옆에 플라스틱 용기 두 개\n' +
      '\n' +
      '**Objects:**[["사과", [391, 207, 134, 69]], ["사과", [376, 264, 167, 187]], ["피자", [124, 79, 202, 118]], ["피자", [81, 128, 213, 120]], ["보울", [366, 189, 202, 270]], ["샌드위치", [86, 124, 215, 128]]\n' +
      '3. 마지막으로 레이아웃 생성을 위한 텍스트 프롬프트를 입력한다. LLM은 주어진 쿼리에 응답하여 레이아웃을 생성한다.\n' +
      '\n' +
      '**Text Prompt:**\n' +
      '\n' +
      '문자 메시지: 자동차 한 대, 고양이 한 대, 개 한 대, 길거리에 있는 집 한 대\n' +
      '\n' +
      '**Returned Objects:**([(\'car\', [0, 960, 836, 1408]], (\'cat\', [1364, 1476, 1856, 1864])), (\'dog\', [280, 1460, 880, 2048]), (\'house\', [960, 772, 2048, 2016])\n' +
      '\n' +
      '## 8 Experiments\n' +
      '\n' +
      '이 섹션에서는 절제 및 방법 분석을 포함하여 더 많은 실험을 보여준다.\n' +
      '\n' +
      'DreamBooth_[35]**.** 도 11에서, 우리는 DreamBooth와의 정성적 비교를 제시한다. Custom Diffusion [19]에서 관찰된 현상과 유사하게, 우리의 _MyCanvas_ 데이터 세트는 DreamBooth와 결합될 때 상당한 개선을 보여준다. 고양이와 사자를 구분하는 등 잠재된 공간에서 유사성을 공유하는 디엔탱글링 객체 정체성에 탁월하다. 우리의 프롬프트 전략의 통합은 이미지 생성 동안 캡션을 정렬하는 데 추가적인 개선을 제공하여 모든 객체의 정확한 표현을 보장한다. 이것은 5개의 객체를 포함하는 구성과 같은 보다 도전적인 시나리오에서 특히 분명하다. 다양한 개인화된 텍스트-이미지 확산 모델에 대한 우리의 _MyCanvas_의 적응성과 생성 품질의 향상을 가져올 수 있는 능력을 보여준다.\n' +
      '\n' +
      '## 9 개인화 구성 메트릭스\n' +
      '\n' +
      '이 섹션에서는 메트릭에 대한 자세한 내용을 보여줍니다. 특히, 우리는 CP-CLIP 및 TI-CLIP를 해석하는 방법에 관한 몇 가지 간단한 직관을 제공한다.\n' +
      '\n' +
      '**Score Interpretability.** 우리가 도출한 두 점수 모두 \\(0\\leq\\) CP-CLIP, TI-CLIP \\(\\leq 1\\)의 이론적 한계를 가지고 있지만 최대치에 도달하는 것은 현실적으로 불가능하다는 점에 유의하는 것이 중요하다. CP-CLIP의 경우 평균 CLIP 점수를 계산한다. 우리가 생성하는 것이 소스 이미지의 인스턴스 중 하나와 정확히 동일한 경우에도 다른 이미지와 달라서 평균이 1보다 작을 수 있으며, 서로 다른 각도에서 동일한 객체의 두 이미지가 약 0.6에서 0.7의 CP-CLIP 점수를 생성한다는 것을 경험적으로 알 수 있다.\n' +
      '\n' +
      '반면에 TI-CLIP에 최대 1을 적용하는 것은 두 가지 양식에서 완전히 동일한 기능을 강제한다. 또한, TI-CLIP는 전체 이미지를 취하지만 배경 텍스트 프롬프트와 비교할 뿐이며, 전경이 더 많은 공간을 점유하고 있을 때, 이 점수는 또한 감소할 가능성이 있다. 따라서 더 나은 개인화 모델은 CP-CLIP가 증가하지만 TI-CLIP**에서 **유지보수가 증가하여 과적합하지 않음을 보여야 한다.\n' +
      '\n' +
      '## 10 구현 세부사항\n' +
      '\n' +
      '우리의 훈련 설정은 기초 기반으로서 사용자 정의 확산[19]에 의해 확립된 구성을 고수한다. 그러나 우리는 다중 개념 실험을 수행할 때 거의 수정하지 않는다. 모든 실험은 Stable Diffusion 1.5 [34]를 사용하여 수행되었으며, 배치 크기는 8(단일 GPU의 경우 배치 크기는 2, 일반적으로 4개의 GPU를 활용함)이고 학습률은 \\(10^{-5}\\)이다. 특히, 2~3개의 개념을 포함하는 구도에 대해 2000단계의 훈련은 구성성과 개념 충실도에 관한 만족스러운 이미지 생성 품질을 제공한다는 것을 관찰한다. 구성이 4~5개의 개념을 포함하는 경우 실험에서 교육을 3000단계로 확장한다. 정규화 집합은 각 개념 범주에 대해 70개의 이미지를 사용한다.\n' +
      '\n' +
      '도 10: **Ablations of _MyCamvas_Data Creation Pipeline.** in (c) In, we enhance image data quality by incorporating realistic background images and employing soft mask for foreground objects. 서브피그레이션(a)은 인페인팅을 확산하기 위한 배경으로 노이즈를 직접 활용하는 것이 비현실적이고 비색채적인 배경의 생성으로 이어진다는 것을 예시한다. 한편, (b)는 복사 붙여넣기 같은 아티팩트의 출현을 피하면서 객체와 배경 사이의 가시적인 경계선의 생성을 방지하기 위해 전경 객체에 소프트 마스크를 사용하는 것의 중요성을 보여준다.\n' +
      '\n' +
      '도 9: **MyCamvas_.**에서의 다양한 구성 예제들 _MyCamvas_로부터, 별개의 배경들에 대해 설정된 3 내지 5개의 개념들을 포함하는 다양한 구성들을 특징으로 하는 예들을 도시한다. 이 컬렉션은 정확한 텍스트 설명과 일치합니다.\n' +
      '\n' +
      '그림 11: **우리의 _MyCamvas_와 통합된 DreamBooth에 대한 정성적 결과. 이 그림은 우리의 _MyCamvas_를 DreamBooth에 통합함으로써 달성된 향상에 대한 정성적인 평가를 보여준다. 맞춤형 확산에서 관찰된 개선 사항과 마찬가지로 당사의 _MyCamvas_는 상당한 발전을 보여줍니다.\n' +
      '\n' +
      '도 12: **A Evaluation Interface for _MyCamvas_Data Filtering. 우리는 화질과 정렬을 평가하기 위한 평가 인터페이스를 시연한다. 사용자는 1) 개인화된 개념의 존재, 2) 합리적인 위치 설정 및 3) 아티팩트의 부재라는 요소를 고려하여 이미지를 1에서 5까지 순위를 매길 수 있다.**\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>