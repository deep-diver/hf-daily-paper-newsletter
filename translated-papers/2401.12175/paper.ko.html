<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '대형 레이건 모듈로 3D 휴먼디지털입니다.\n' +
      '\n' +
      '진천 풍미({}^{1\\dagger}\\), 진유산 류\\({}^{2}\\), 하오 탄\\({}^{2}\\), 자한Xu\\({}^{2}\\), 양주성({}^{2}\\), 양주성({}^{2}\\)\n' +
      '\n' +
      '세레나 예웅-레비\\({}^{1}\\), 지메이 양\\({}^{2}\\)\n' +
      '\n' +
      '도비 리서치({}^{1}\\,\\)는 스트란포드 대학교, \\({}^{2}\\).\n' +
      '\n' +
      '>{}^{2}\\){}^{2}\\.{zzyliu,\\({}^{2}\\){진기리루, 모자란,zhaxu,yaz적이다, indyang}@adobe.com찜}@스탄포드.@스탄포드.@스탄포드.\n' +
      '\n' +
      '아도베 리서치에서 인턴으로 일했습니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문에서는 단일 이미지로부터 인간 신경방사선전방(NeRF)을 예측하도록 설계된 단일 단계 사료-포워드 대형 재건 모델인 인간-LRM을 소개한다. 우리의 접근법은 3D 스캔 및 다중 뷰 캡처를 포함하는 광범위한 데이터 세트를 사용하여 훈련에서 놀라운 적응력을 보여준다. 또한, 특히 교합과 함께 양식 내 시나리오에 대한 모델의 적용 가능성을 높이기 위해 조건부 삼평면 확산 모델을 통해 다중뷰 재구성을 단일뷰로 왜곡시키는 새로운 전략을 제안한다. 이 생성 확장은 단일 뷰에서 관찰할 때 인체 형상의 고유한 변화를 다루고, 폐색된 이미지에서 전체 신체 인체를 재구성할 수 있게 한다. 광범위한 실험을 통해 인간-LRM이 여러 벤치마크에서 상당한 마진으로 이전 방법을 능가한다는 것을 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '단일 영상에서 3D 인간 모델을 변환하는 것은 컴퓨터 비전에서 실용적인 응용 프로그램의 배열로 중요한 연구 주제이다. 이러한 응용 프로그램은 AR/VR, 자산 창출, 이전 등과 같은 영역을 포함한다. 이 도전적 과제를 해결하기 위해 다양한 기술이 개발되었으며, 각각의 장점은 물론 한계가 있다. 인간 메쉬 회수법(HMR) [10, 20, 46] 회귀 방법은 세부 사항을 완화하지 않는 SMPL(스킨드 멀티-개인 라인바) 인체 메쉬 모델[23]의 포즈 및 모양 매개변수를 제공한다. 이것은 현실적이고 상세한 인간 표현을 필요로 하는 응용 분야에서 효용성을 제한한다. 반대로 암묵적인 체적 재구성 방법[31, 32]은 픽셀 정렬 기능으로 미세 제조 의복 세부 정보를 캡처하지만 다양한 포즈에 걸쳐 일반화되지 않는다. 최근의 하이브리드 접근법[40, 41, 49]은 예측된 SMPL 바디 메쉬를 컨디셔닝으로 사용하여 모수 및 암묵적 재구성 방법의 이점을 결합하여 완전한 응고 재구성을 안내한다. 그러나 이러한 SMPL 조건 방법은 불가피한 한계에 직면하며, SMPL 예측 오차는 후속 전체 재구성 단계로 전파되어 재구성된 메쉬와 입력 이미지 사이의 오정렬이 발생한다. 이러한 오류는 종종 회복할 수 없으며 사후 최적화[40, 41, 49]에 의해 완전히 고정될 수 없다. 마지막으로 이러한 암묵적 재구성 방법은 교육을 위한 고품질 3D 스캔의 부족으로 인해 방해를 받는다.\n' +
      '\n' +
      '한편, 네오RF[25]를 인간의 질감뿐만 아니라 기하학을 배우기 위한 표현으로 사용하는 다양한 작품이 있었지만, 이러한 작업은 일반적으로 새로운 관찰에 일반화되지 않는 시간 소비인 단일 이미지[1, 16]에만 미세 조정 작업을 수행한다. 최근에는 대형재건축 모델(LRM)[14]과 같은 피드포워드 NeRF 예측 모델이 제안되어 일반성이 높고 임의의 이미지 입력으로부터 NeRF뿐만 아니라 고품질 3D 재구성을 생성한다. 그러나 사전 훈련된 제네릭 LRM을 인간에게 직접 적용하면 미세 조정에도 최적이 아닌 결과를 얻을 수 있다(그림 2). 주로 재구성된 표면은 충분한 세부 사항을 보존하지 않고 거친 경향이 있다.\n' +
      '\n' +
      '이 연구에서 우리는 단일 이미지로부터 인간의 기하학 및 외관을 예측하는 단일 단계 사료-포워드 모델인 인간-LRM을 제시한다. 뉴럴 라디에이터 필드를 3D 표현으로 레버리지하면, 우리는 멀티 뷰 인간 데이터 세트를 포함하도록 훈련을 스케일링할 수 있다. 결과적으로, 우리는 제한 3D 스캔 감독에 의존하는 이전 방법[31, 32, 40, 41]에 비해 개선된 일반화를 달성할 수 있다. SHERF[15]와 달리 예측된 SMPL 메쉬를 사용하여 이미지 특징을 표준 공간으로 변환시키는 기존 일반화된 인간 NeRF 예측 모델과 달리, 인간-LRM은 완전히 템플릿이 없는 것으로, SMPL 조건 방법이 부적절한 복잡한 상황에서 효과적인 일반화가 가능하다. LRM과 대조적으로, 인간-LRM은 SDF 값을 예측하고 고전적인 NeRF[25] 대신 VolSDF[42]로 만들어 최종 재구성에 대한 표면 충실도가 향상된다. 우리는 또한 정상 및 깊이 지도를 통해 인간의 기하학을 감독한다. 이러한 개선은 더 높은 품질 표면 재구성을 가능하게 하는 데 효과적이라는 것을 증명한다. 마지막으로 야생의 일반적인 폐색 시나리오를 해결하기 위해 조건부 3평면의 확산[13, 34]을 통해 다관 재구성을 단일관으로 왜곡시키는 새로운 훈련 패러다임을 제안한다. 이것은 인간-LRM을 생성 능력과 동일하여 부분 관찰(그림 1의 마지막 컬럼)에서 전체 신체 인간을 출력한다. 우리의 기여금은 아래에 요약되어 있습니다.\n' +
      '\n' +
      '* 우리는 표면 충실도가 향상된 인간에게 특화된 LRM인 인간-LRM을 소개한다. 다중 뷰 RGB 데이터와 3D 스캔을 모두 사용하여 광범위한 데이터 세트(10K 이상의 형태)에서 훈련된 우리 모델은 더 넓은 스펙트럼의 시나리오 및 응용 분야에 걸쳐 훨씬 향상된 일반화 가능성 및 엑셀로우에 속한다.\n' +
      '**가 길들여진 시나리오에 대해 인간-LRM의 적용 가능성을 높이기 위해 조건부 확산 모델을 통해 다중뷰 인간-LRM을 단일뷰로 증류한다. 생성 인간-LRM은 다른 견해와 교합 모두에서 비산체 부분을 더 잘 처리할 수 있다.\n' +
      '* 광범위한 실험을 통해 우리는 인간-LRM이 포괄적인 평가 세트에 대해 이전 방법을 상당히 능가한다는 것을 보여준다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '**-모수 재건*** 다수의 3D 인체 재구성 작품[10, 20, 22, 46]은 SMPL[23]과 같은 메쉬 기반 모수 신체 모델에 구축된다. 입력 이미지를 감안할 때, 인간 매쉬 회복(HMR)이라고 하는 이러한 방법은 신경망을 사용하여 SMPL 형태를 예측하고 타겟 인체 메쉬가 구성된 파라미터들을 포즈한다. 이 SMPL 조건 접근법은 네트워크 출력 복잡성을 크게 감소시키고 또한 다른 생존할 수 있는 메쉬 래스터화 [20, 38]를 통해 2D 포즈 추정치를 사용하여 약하게 전처리된 훈련에 적응할 수 있다. SMPL은 고정된 토폴로지의 부드러운 메쉬로 최소한으로 덮인 인체 모델을 모델링함에 따라 이러한 방법은 상세한 기하학과 질감을 재구성하는 것을 방지한다. 그럼에도 불구하고, 예측된 SMPL 메쉬는 기본 체형을 포착하고 그 포즈 구조를 묘사함에 따라 완전히 응고된 재구성에 매우 좋은 프록시이다. HMR의 약속은 추적 작업을 동기화하여 3D 오프셋[2, 24, 27, 50]을 예측하거나 베이스 바디 메쉬 위에 다른 레이어 기하학을 구축하여 응고된 인간 형상[5, 19]을 수용할 수 있도록 한다. 그러나 이 "바디+오프셋" 전략은 광범위한 의류 유형을 나타내는 유연성이 부족하다.\n' +
      '\n' +
      '생체 기능*** 모방 재구성은 인간 모델링을 위한 토폴로지 진단 표현을 제공하며 인간 재구성을 모델링하기 위한 토폴로지 진단 표현을 제공한다.\n' +
      '\n' +
      '그림 2: LRM [14]의 한계: 비-장 제네릭 LRM(왼쪽), 거친 기하학 심지어 인간(오른쪽)에 대한 LRM의 깊이 모호성이다.\n' +
      '\n' +
      '씹어. PiFU [31]는 픽셀 정렬된 이미지 특징을 사용하여 미리 정의된 그리드에서 샘플링된 3D 지점에서 3D 점유 값과 색상을 예측한다. 이를 기반으로 PIFuHD[32]는 프론트백 정상 지도를 추가 입력으로 하여 기하학적, 질감 디테일을 예측하기 위해 고해상도 모듈을 개발한다. 깨끗한 배경에 맞서 서 있는 인간과 같은 단순한 입력에 대한 표현적 재구성 결과를 생성하지만, 그러한 모델은 복잡한 시나리오에 잘 일반화할 수 없으며 제한된 모델 용량 및 전체론적 표현 부족으로 인해 도전 포즈와 빛에 부러지고 지저분한 모양을 생성하는 경우가 많다.\n' +
      '\n' +
      '*** 하이브리드 재구성*** 새로운 유형의 접근 방식은 완전한 수정 암묵적 재구성 방법의 일반화 가능성을 개선하기 위해 모수 신체 모델(예: SMPL[23])을 강조한다. 주어진 이미지와 추정된 SMPL 메쉬에서 시작하여 시우 등[40]은 국부적으로 절단된 특징으로부터 형태를 회귀하여 포즈에 일반화한다. 왕 등은 GAN 기반 생성 성분으로 ICON을 확장한다[36] 시우 등[41]은 느슨한 의류의 세부 사항을 보존하기 위해 가변적인 정상 통합과 형상 완성을 강조한다. SMPL의 통합은 큰 포즈에 대한 일반화 가능성을 향상시키지만 이러한 방법은 또한 SMPL 예측의 정확도에 의해 제약된다. 추정된 SMPL 매개변수의 모든 오류는 후속 메쉬 재구성 단계에 캐스케이딩 효과를 갖는다.\n' +
      '\n' +
      '** 인간 NeRF** 신경방사성 필드(NeRF) [25]는 3D 재구성에서 중추적인 이정표를 나타낸다. NeRF는 2D 관찰에서만 물체의 3D 표현 학습을 촉진한다. 인간 NeRF를 재구성하는 데 중점을 둔 몇 가지 주목할 만한 작품이 존재하지만, 이러한 노력은 종종 단일 비디오[37] 또는 이미지[16, 39] 미세 조정 설정을 수십분에서 몇 시간 범위의 상당한 계산 시간 비용으로 중심으로 한다. 대조적으로, 우리의 초점은 모델이 단일 이미지로부터 인간 NeRF를 예측하는 데 필요한 시간을 일반적으로 몇 초 만에 근본적으로 감소시키는 사료 포워드 패러다임에 있다. 최근 몇 가지 작품[11, 21]은 또한 SMPL을 기하학적 사전으로 사용하고 희소 관찰에서 특징을 집계하는 일반성에 대한 사료 포워드 패러다임을 사용하지만 여러 견해를 필요로 한다. 더 가까운 관련 작업[15]은 단일 영상에서 피드 포워드 인간 NeRF 예측을 고려한다. 그럼에도 불구하고 그들의 방법은 모델 표현력을 제한하는 근거 진실 SMPL 바디 메쉬에 회신한다. 우리의 방법은 완전히 템플릿이 없고 실제 응용 프로그램의 광범위한 스펙트럼을 개방하여 NeRF 기반 인간 재구성이 다양한 시나리오에 더 접근 가능하고 실용적이다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '인간-LRM의 개요는 그림 3에 나와 있으며, 이 방법은 변압기 기반 삼면 디코더와 삼면 NeRF의 두 가지 주요 빌딩 블록으로 구성된 LRM[14] 위에 구축된다. 3.1절에서는 모델 백본으로 삼평면 예측을 간략히 소개하고 3.2절에서 개선된 삼평면 NeRF를 도입하여 인간의 표면 재건 품질을 높였다. 건축물에 대한 자세한 내용은 독자를 [14]라고 합니다. 마지막으로, 새로운 뷰를 완료하고 폐색을 해결하기 위해 설계된 조건부 확산에 기초한 모델의 생성 확장을 소개합니다(섹션 3.3).\n' +
      '\n' +
      '기저발.\n' +
      '\n' +
      'RGB 이미지를 입력으로 하는 경우, LRM은 먼저 미리 학습된 비전 변압기(ViT), DINO [7]를 적용하여 패치별 특징 토큰(\\{\\mathbf{h}_{i}}_{i =1}^{n}\\{n}\\in\\mathbb{R}^{768}\\)에 영상을 인코딩하며, 여기서 I\\(i\\)는 패치의 총 수, \\(i\\)는 잠재 차원이다.\n' +
      '\n' +
      '그런 다음 변압기 모듈을 사용하여 이미지 토큰을 3D 삼면 [8]으로 디코딩한다. 구체적으로, 디코더 업데이트들은 PerceiverIO[17]의 설계와 유사하게 카메라 변조 및 이미지 토큰과의 교차 의사를 통해 최종 삼면 특징들에 대한 학습 가능한 토큰들을 업데이트한다. 보다 구체적으로, 각 변압기층은 교차 의도, 자기 의도 및 다층 퍼셉트론(MLP) 서브 레이어를 포함하며, 여기서 각 서브 레이어에 대한 입력 토큰은 카메라 피처 \\(\\mathbf{c}\\)에 의해 변조(26])된다. 교차 표시층은 3평면에서 이미지 토큰에 걸쳐 있으며, 이는 이미지 정보를 삼평면으로 연결하는 데 도움이 될 수 있다. 그런 다음, 자기 의도 계층은 공간적으로 구조화된 삼평면의 엔트리들에 걸쳐 모달 내 관계를 추가로 모델링한다.\n' +
      '\n' +
      '트리플레인[8]은 효율적인 3D 표현으로 사용된다. 삼각형 \\(\\mathbf{T}\\)는 3개의 축 정렬 특징 평면 \\(\\mathbf{T}_{\\text{XY}}}}), \\(\\mathbf{T}_{\\text{YZ}}}) 및 \\(\\mathbf{T}_{\\text{XZ}_{\\)를 포함한다. 우리의 구현에서 각 평면은 치수 \\(h_{T}\\times w_{T}\\times d_{T}\\i d_{T}\\)이고 \\(h_{T}\\)는 공간 해상도이고 \\(d_{T}\\)는 특징 채널의 수이다. NRF 오브젝트 바운딩 박스 \\([-1,1]^{3}\\)의 3D 포인트에 대해 각각 투사하고 해당 포인트 특징을 질의할 수 있으며 \\(\\mathbf{T}_{xy}\\), \\(\\mathbf{T}_{yz}\\), \\(\\mathbf{T}_{yz}\\), 이중간 보간을 통해 a\\(\\mathbf{T}_{yz}_{yz}_{g}_{xz}_{g}_{g}_{g}_{T}_{g}_{g}_{g}_{g}_{T}_{g}_{g}_{g}_{g}_{g}_{g}_{g}_{g}_{g}_{g}_{g}_{g}_{xz}\\)를 렌더링을 통해 배열(\\)를 렌더링하기 위해 렌더링을 통해 렌더링(If}_{g}_{g}_{g}_{xz}\\)를 통해 렌더링(\n' +
      '\n' +
      '간단히 말해서 입력 이미지 \\(\\math{T}_{math{I})\\(\\math{T} <\\math{I}}{math{I}}}{math{t}}}},\\{math{T}}}}}>\\{math{t}}}.\n' +
      '\n' +
      '### Triplane NeRF\n' +
      '\n' +
      '전통적인 신경 부피 렌더링 방법(LRM[14]) 모델 기하학에 사용되는 것)은 일반화된 밀도 함수를 통해 이루어진다. 이 기하학의 추출은 밀도 함수의 무작위 레벨 세트를 사용하여 달성되며, 이는 종종 시끄럽고 충실도가 낮은 재건을 초래한다. 따라서 재구성의 충실도를 향상시키기 위해 밀도 대신 설계된 거리 기능(SDF)을 예측한다. 구체적으로, 우리는 2개의 MLP(i.e. "SDF MLP" 및 "RGB MLP"를 사용하여 삼평면의 표현 \\(T\\)에서 쿼디드된 점에서의 SDF 및 RGB를 예측한다. SDF MLP는 점 특징 및 출력 SDF 및 잠재 벡터 \\(\\mathbf{h}_{p}\\)를 취한다. RGB MLP는 샘플링된 지점(\\hat{\\mathbf{n}}_{p}\\)에서 점 특징, 중간 벡터 및 정규(유한 차이를 사용하여 예측된 SDF에서 계산됨) 및 출력 RGB 값을 취한다. \\(텍스트{RGB}=\\text{RLP}_{\\ath{T}_{\\math{T}},\\math{T}},\\math{T}}},\\math{T}}},\\math{T}}},\\math{T}}},\\math{f{T}},\\math{T}_{f{T}}},\\math{f{T}}_{f{f{T}}}}_{f{f{f{T}}}}}_{f{f{f{T}}}}}}{f{f{f{T}}}}}{f{f{f{T}}}}}}{f{f{f{T}}}}}}}{f{f{T}}}}/{f{f{T}}}}}}},\\math{T}}}},\\math{T}}}},\\math{T}}}}},\\math{T}}}} (\\mathbf{v}^ug3}\\) 방향,\\(\\mathbf{v}\\)에 의해 정의된 ai(\\mathbf{v}\\)의 경우, a\\mathbf{v}}|1\\)는 수치적분(\\mathbf{v}}TP(t) 방향으로 계산된다.\n' +
      '\n' +
      '>자}(1-\\text_{i}) 지분{i}(1-\\textty_{i}\\\\{i})\n' +
      '\n' +
      '\\(\\sigma_{i}\\)가 [42]를 사용하여 SDF에서 변환된 밀도이고 \\(\\delta_{i}\\)는 샘플 간의 거리이다. 일반적으로 샘플링된 지점에서 예측된 규범보다 통합되는 동일한 공식을 사용하여 렌더링될 수 있다.\n' +
      '\n' +
      '교육 목적*** 우리 학습 데이터** 저희 학습 데이터에는 사람당 여러 가지 조회수와 각각의 카메라 파라미터가 포함되어 있습니다. 각 인간에게 무작위로 몇 가지 측면을 선택하고 [37]과 유사한 각 관점에서 무작위 \\(\\hat{\\\\mathbf{x}\\in\\mathbb{R}^{h\\i 3}\\) 패치를 렌더링한다. 패치에 대한 지상 진리 RGB 값은 \\(\\mathbf{x}\\in\\mathbb{R}^{h\\tcer 3}\\)이다. {\\mathbb{d}\\mathbb{R}^{h\\times w\\i}}\\s{mathb{h\\times w}\\) 및 \\(\\mathbf{d}\\in\\mathb}\\s}\\s)의 깊이 및 정규(\\mathbf{d}\\mathb}\\mathb}\\mathb}\\mathb}^mathb}^mathb}\\mathbf{h\\mathb}^mathb}^mathb}\\mathb}\\mathb}\\mathb}\\mathb}\\mathb}\\mathb}\\mathb}\\mathb}\\mathb}\\mathb}\\mathb}\\mathb}\\mathb}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\ 감독 깊이와 정상 지도는 지상진실 렌더링 또는 장외 예측일 수 있다. 단일뷰 재구성 방법의 훈련 목적은 \\(V\\) 렌더링된 뷰로부터의 손실에 대해 계산되며, 입력 뷰는 물론 \\(V-1) 측면 뷰로 계산된다. 전반적으로, 훈련 목표는 \\(\\mathcal{L}\\), \\(\\mathcal{L}\\)를 최소화하는 것이다.\n' +
      '\n' +
      '}} (\\math{d})\\math{v}(\\math{{d})\\math{v}.\n' +
      '\n' +
      '가입자\\(v\\)는 해당 변수가 \\(v_{th}\\) 감독관임을 의미한다. (\\mathcal{L}_{\\text{MSE}}\\)는 정규화된 픽셀별 L2 손실, \\(\\mathcal{L}_{\\text{LPIPS}}}})는 지각적 이미지 패치 유사성[47]이고 \\(\\mathcal{L}_{\\text{DSI}}}})은 스케일 불변 깊이 손실[4]이다. (\\mathcal{L}_{\\text{Eikonal}}\\)는 광선을 따라 샘플링된 지점의 SDF 값을 사용하여 계산된 Eikonal 정규화 [12]이다. (\\lambda_{\\text{pips}}\\), \\(\\lambda_{n}\\), \\(\\lambda_{d}\\), \\(\\lambda_{eik}\\)는 중량 계수이다.\n' +
      '\n' +
      '조건부 확산 모델.\n' +
      '\n' +
      '위에서 언급한 단일 가치 결정론적 모델은 1) 비실증 부분에 대한 재건 붕괴와 2) 교합 처리 불능이라는 두 가지 한계를 가지고 있다. 이 절에서는 조건부 확산이 있는 인간-LRM의 생성 확장을 제안한다. 이 모델의 개요는 그림 3의 우측에 나와 있으며, 구체적으로 먼저 다중뷰 재구성 모델을 훈련시킨다. 단일 뷰 모델과 달리 다중 뷰 모델은 ViT 인코더 내에 카메라 컨디셔닝을 통합한다. 멀티뷰 모델의 삼면 디코더는 카메라 컨디셔닝을 하지 않는 것을 제외하고는 단일뷰 모델과 동일한 구조를 유지한다. 충분한 견해 수로 학습된 삼면 \\(\\mathbf{T}^{mv}\\)를 인간의 거의 완전한 표현으로 개념화할 수 있다. 선택했습니다.\n' +
      '\n' +
      '그림 3: **Left**: 단일 뷰 인간-LRM의 오버뷰입니다. 단일 이미지를 감안할 때 ViT[7]를 사용하여 이미지를 인코딩하고 변환기를 사용하여 삼면 표현[8]을 해독한 다음 새로운 관점에서 RGB, 정상 및 깊이의 체적 렌더링을 위한 SDF 및 RGB MLP를 사용한다. ** 우측**: 생성 인간-LRM을 참조하세요. 우리는 먼저 공유 NeRF 디코더로 멀티뷰와 싱글뷰 인간-LRM을 훈련시킨 다음 단일뷰 트라이플레인들을 컨디셔닝으로 사용하여 학습된 트라이플레인들을 멀티뷰에서 변성시키는 확산 모델을 훈련시킨다. 확산 모델 훈련 동안 단일 뷰 인코더는 실제 교반을 시뮬레이션하기 위해 추가적인 이진 마스크를 취한다.\n' +
      '\n' +
      '4뷰 모델을 훈련하세요. 이 결정은 네 가지 견해를 활용하는 것이 모델의 능력을 과도하게 증가시키지 않으면서 인간의 주제에 대한 확정적이고 포괄적인 묘사를 제공하는 경향이 있다는 관찰에 기초한다. 그런 다음 다중 뷰 인코더의 가중치를 동결하고 단일 뷰와 다중 뷰 삼평면의 특징 사이의 추가 l2 손실로 단일 뷰 인코더를 훈련시킨다. 이 l2 손실은 0.1에서 10까지 시작하는 가열냉각 중량을 가지고 있으며 단일 뷰 및 다중 뷰 모델 모두 tanh 층을 사용하여 3평면의 특징을 \\([-1,1]\\)로 클램핑한다. 실세계 교반을 시뮬레이션하고 환각하는 확산 모델을 안내하기 위해 단일뷰 입력 영상에 무작위 마스크를 적용하고, 추가 마스크 채널을 통해 바이너리 마스크를 단일뷰 인코더에 통과시킨다.\n' +
      '\n' +
      '} (\\math{v})\\math{v} (\\math{d})\\math{d}.\n' +
      '\n' +
      'i\\(\\mathcal{M}_{1}\\in[0,1]^{H,W}\\)가 단일뷰 입력 영상에 대한 이진 마스크이고, \\(m\\)는 멀티뷰 모델에 대한 입력 뷰의 수이다.\n' +
      '\n' +
      '조건부 모델을 훈련시키기 위해 먼저 단일뷰 모델과 다중뷰 모델에서 예측된 트라이플라인을 평평하게 하여 \\(\\tilde{\\mathbf{T}}^{{sv}\\) 및 \\(\\tilde{\\mathbf{T}}^{mv}}\\)를 생성했다. 그런 다음 우리는 가우시안 잡음의 \\(t\\) 단계를 멀티뷰 트라이플레인 \\((CStilde{\\mathbf{T}}^{mv}}^{t}\\)에 추가하고 조건부 확산 모델을 훈련시켜 \\(\\tilde{\\mathbf{T}}^{mv}\\)를 복원한다. 단일 뷰 삼평면은 컨디셔닝으로 사용되며, 무지도 멀티 뷰 삼평면과 연결되어 확산 모델[13]에 대한 입력을 형성한다. 확산 훈련의 목적은 확산 훈련의 목표이다.\n' +
      '\n' +
      '} <\\mathbf{T>[1]} 〈^mv}-U_{\\\\ta}], (\\tilde{\\mathbf{T})^{t}.\n' +
      '\n' +
      '아이티(t\\)가 무작위로 샘플링된 타임스팟이고 \\(T=1000\\)는 최대 단계 수이다. \\(t\\)는 무작위로 샘플링된 타임스팟이고 \\(T=1000\\)는 최대 단계이다. (U_{\\theta}\\)는 단일 뷰 삼평면과 타임스팟에 대한 변성 다뷰 3평면의 컨디셔닝을 예측하는 UNet[30](중량 \\(\\theta\\)이다. 단일 뷰 인코더의 입력 채널을 변경했기 때문에 확산 훈련 동안 단일 뷰 인코더를 고용한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '*** 훈련 데이터** 우리의 완전한 훈련 세트는 1,426개의 고품질 스캔(THuman 2.0 [44]에서 500개[4] 및 Alloy++에서 926개)으로 구성되며 HuMMan [6] v1.0.0에서 약 8,000개의 제기된 다중 뷰 캡처와 HuMMan 모두 간단한 의류가 있는 성인을 포함한다. 따라서 일반화 능력을 추가로 평가하기 위해 우리는 인간 앨로이드[3]와 내부 포획에서 알로이++를 수집한다. 인간 알로이의 각 스캔에는 약 40K 폴리곤과 내부 캡처 100K 폴리곤이 있습니다. 이러한 스캔의 품질은 Render사람들은 [29] (100K 폴리곤)과 유사하다. 알로이++에는 더 어려운 옷, 포즈, 작은 아이들이 있는 사람이 포함되어 있습니다.\n' +
      '\n' +
      '*** 평가 세트** 우리는 최대 2.0의 인간 20명과 알로이++의 20명을 각각 고르게 이격된 18개의 관점에서 렌더링하여 평가한다. 또한 X-인간 [33]에서 평가 세트를 제작합니다. 우리는 서열당 무작위로 2개의 프레임을 샘플했으며, 이는 20명의 인간 피험자로부터 460개의 프레임을 생성하며 모두 뚜렷한 포즈가 있다. X-인간 테스트 세트는 훈련 중 이 데이터세트로부터 이미지를 본 모델이 없어 설정된 도메인 외 평가 역할을 한다.\n' +
      '\n' +
      '*** 데이터 전처리**. THuman 2.0 및 Alloy++의 각 스캔에 대해 원점으로 중심을 잡고 가장 긴 면이 길이 1.8을 가지고 있습니다. 우리는 기원을 가리키는 동일한 카메라로 무작위로 샘플링된 32개의 관점에서 각각의 인간 스캔을 렌더링한다. HuMMan v1.0의 경우 포즈당 10개의 카메라가 있습니다. 총 16K, 14K 및 80K 별개의 입력 이미지가 THuman 2.0, Alloy++ 및 HuMMan v1.0의 훈련 분할에서 각각 있다.\n' +
      '\n' +
      '해킹 시간**** 영상 인코더 및 3평면의 디코더는 입력 이미지(들)로부터 3평면의 표현을 얻기 위해 약 0.7초, 그리고 1.3초 정도 소요되어 트라이플레인으로부터 256 x 256 이미지를 렌더링한다. 단일 뷰 조건부 확산 모델의 경우 200단계를 사용한 DDIM 샘플링은 단일 A100 GPU에서 약 10초가 소요된다.\n' +
      '\n' +
      '### Geometry Comparisons\n' +
      '\n' +
      '우리는 기존의 단일 뷰 인간 재구성 방법 PiFU[31], PiFuHD[32], Pamir[49], ICON[40] 및 ECON[41]1과 비교되며, Pamir, ICON 및 ECON은 모델에 대한 입력으로 SMPL 매개변수를 필요로 하기 때문에 오프 시트 SMPL 예측 변수[10, 46]를 사용하여 생성합니다. 이 모든 기저부는 감독으로서 근거 진실 기하학을 필요로 하므로 일반화 가능성은 그러한 데이터 세트의 가용성에 의해 제한되는 반면, 우리의 방법은 더 접근하기 쉬운 멀티 뷰 캡처만으로 작용한다.\n' +
      '\n' +
      '발주 1:왕 등[36]은 또 다른 관련 작업이지만 코드 출시가 없어 비교할 수 없었고 저자도 모델 체크포인트가 손실됐다고 알려줬다.\n' +
      '\n' +
      '이전 작품들에 이어 참퍼 거리, 포인트 대 표면(P2S) 및 노말 컨시스트리티(NC)를 보고한다. 첫째, 우리는 그들의 공공 사전 조작 모델과 비교합니다. 기준 방법 중 일부는 상업적으로 이용 가능한 루더 사람들에게 훈련되기 때문에 유사한 척도를 가진 공개적으로 이용 가능한 데이터 세트인 THuman 2.0을 선택하여 공정한 비교를 보장합니다. 훈련 데이터의 영향을 제거하기 위해 동일한 데이터 세트에 대한 모든 접근법을 훈련합니다. 바젤린은 GT 기하학을 사용하여 점유를 직접 감독하지만 GT 기하학에서 정상 및 깊이 지도를 얻고 표면 예측을 안내하는 데 사용한다.\n' +
      '\n' +
      '3.1절에서 설명한 바와 같이 SMPL 이전(PiFU 및 PiFuHD)과 SMPL을 필요로 하는 작품(Pamir, ICON 및 PiFuHD)을 포함하여 우리의 방법으로 예측된 기하학은 "표 1"에서 정량적 결과를 보고한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '유전적 인간-LRM 평가.\n' +
      '\n' +
      '3.3절에서 설명한 대로 단일 뷰 삼면 기능을 컨디셔닝으로 사용하여 확산 모델을 훈련한다. 다중 뷰 모델의 증류 정보로 강화된 조건부 확산 모델은 단 하나의 관점에서 신뢰할 수 있는 포즈를 재구성할 수 있다. 이는 특히 인간의 일부가 가려지는 시나리오(예: 그림 5의 1열)에서 성능을 향상시킨다. 우리는 표 2a에서 THuman 2.0에 대한 정량적 성능을 보고한다. 기억 문제로 인해 트라이플레인 크기가 작은 확산 모델(128 x 128)을 학습시켜 단일뷰 모델의 성능이 약간 저하된다. 그러나 조건부 확산에 따라 256 x 256 트라이플레인으로 단일뷰 결정론적 모델보다 모델의 성능이 더욱 우수해짐을 보여준다.\n' +
      '\n' +
      '또한, 우리의 확산 모델은 재구성할 수 있습니다.\n' +
      '\n' +
      '그림 4: 이전 체적 재구성 방법과의 비교: PiFU[31], PiFUHD[32], Pamir[49], ICON[40], ECON[41], LRM[14]. 모든 모델은 THuman 2.0에서 훈련되며, 각 예는 4개의 뷰에서 기하학(메쉬 규범에 의한 색)을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c|c c c|c c}  & \\multicolumn{3}{c}{THuman 2.0} & \\multicolumn{3}{c}{Alloy++} & \\multicolumn{3}{c}{X-Human} \\\\ Model & Training Data & Chamfer \\(\\downarrow\\) & P2S \\(\\downarrow\\) & NC \\(\\downarrow\\) & Chamfer \\(\\downarrow\\) & P2S \\(\\downarrow\\) & NC \\(\\downarrow\\) & Chamfer \\(\\downarrow\\) & P2S \\(\\downarrow\\) & NC \\(\\downarrow\\) \\\\ \\hline Est. d.n. & THuman 2.0, HuMMan v1.0 & 2.63 & 2.38 & 0.134 & 3.35 & 3.08 & 0.147 & 2.45 & 2.28 & 0.103 \\\\ No d.n. & THuman 2.0, HuMMan v1.0 & 2.63 & 2.40 & 0.132 & 3.68 & 3.20 & 0.163 & 2.75 & 2.43 & 0.117 \\\\ Predict \\(\\sigma\\) & THuman 2.0, HuMMan v1.0 & 2.49 & 2.32 & 0.124 & 3.48 & 3.44 & 0.156 & 2.67 & 2.59 & 0.116 \\\\ Full Model & THuman 2.0, HuMMan v1.0 & **2.41** & **2.21** & **0.115** & **3.16** & **2.92** & **0.145** & **2.37** & **2.21** & **0.103** \\\\ \\hline Small training set & THuman 2.0 & 2.62 & 2.60 & 0.124 & 3.22 & 2.99 & 0.145 & 2.43 & 2.25 & 0.106 \\\\ Medium training set & THuman 2.0, HuMMan v1.0 & 2.41 & 2.21 & 0.115 & 3.16 & 2.92 & 0.145 & 2.37 & 2.21 & 0.103 \\\\ Large training set & THuman 2.0, Alloy++, HuMMan v1.0 & **2.23** & **2.03** & **0.114** & **2.35** & **2.12** & **0.116** & **2.29** & **2.15** & **0.099** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 단일 가치 결정론적 모델의 분류는 다음과 같다. 상단: 수심 및 정상 지도(d.n)를 사용하여 SDF를 관리 및 예측하는 효과가 있다. Bottom: 훈련 데이터의 척도의 효과.\n' +
      '\n' +
      '인간이 폐색된 경우에도 단일 뷰 이미지의 완전한 인간. 표 2에서 우리는 실제 폐색 시나리오를 시뮬레이션하기 위해 각 입력 영상에 마스크를 무작위로 적용한다. 전신 SMPL에 조절되는 ICON 및 ECON과 같은 SMPL 유도 작업에도 모든 기저부가 누락된 신체 부분을 재구성하지 못한다는 것을 알 수 있다. 반면에 생성 모델은 그림 5의 폐색 부분(예: \\(3_{rd}\\) 행)을 환각할 수 있다. 또한, 다른 무작위 씨앗으로 생성 모델은 신뢰할 수 있는 다양한 포즈 8을 재구성할 수 있다.\n' +
      '\n' +
      '5배제 및 미래 근무.\n' +
      '\n' +
      '이 작업에서 우리는 단일 영상에서 인간 NeRF를 재구성하기 위한 접근법을 도입했다. 이전 암묵적인 체적 인간 재구성 방법과 다른 접근 방식을 설정하는 것은 놀라운 확장성이므로 크고 다양한 멀티 뷰 RGB 데이터셋에 대한 학습에 매우 적응적이다. 이러한 적응력은 차례로 일반성을 크게 향상시켜 확립된 기준선을 능가할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c} Method & GT SMPL & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) \\\\ \\hline NHP [21] & ✓ & 18.99 & 0.84 & 0.18 \\\\ MPS-NERF [11] & ✓ & 17.44 & 0.82 & 0.19 \\\\ SHERF [15] & ✓ & 20.83 & 0.89 & 0.12 \\\\ \\hline SHERF [15] & \\(\\times\\) & 14.46 & 0.79 & 0.20 \\\\ Ours & \\(\\times\\) & 17.13 & 0.87 & 0.12 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: HuMMan v1.0 [6]에 대한 일반화된 인간 NeRF 방법과 비교. 탑 섹션: 추론 동안 GT SMPL 파라미터를 사용하는 피드-포워드 방법. 바닥 섹션: 추론 중에 GT SMPL을 사용하지 않는 방법.\n' +
      '\n' +
      '그림 5: 교합이 있는 이미지에 대한 정성적 예. 각 예에 대해 우리는 측면 뷰뿐만 아니라 입력 뷰를 보여준다.\n' +
      '\n' +
      '그림 8: 불완전한 이미지를 통해 생성 모델은 신뢰할 수 있는 다양한 포즈를 생성할 수 있다.\n' +
      '\n' +
      '그림 6: 노블 뷰는 HuMMan v1.0에 대한 결과를 렌더링한다.\n' +
      '\n' +
      '그림 7:블루. 추정 대 추정 사용 효과를 보여준다. 지상 진리는 정상 및 깊이와 감독뿐만 아니라 LRM[14]에서와 같이 간단한 MLP를 사용하여 SDF 대신 밀도를 예측한다.\n' +
      '\n' +
      '다양한 테스트 세트에 대한 모델입니다. 또한, 우리의 새로운 다중 뷰 특징 증류 접근법은 인체 포획의 고유한 변화를 처리하여 단일 뷰에서 그럴듯하고 완전한 인간 기하학 컨디셔닝을 생성한다.\n' +
      '\n' +
      '인간-LRM은 글로벌 기하학을 캡처하는 데 탁월하지만 여전히 더 미세한 얼굴 디테일을 보존하는 데 짧다. 향후 방향에는 트라이플레인 또는 잠재적인 정제 기술보다 더 강력한 표현을 사용하는 것이 포함된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang. Single-image 3d human digitization with shape-guided diffusion. In _SIGGRAPH Asia_, 2023.\n' +
      '* [2] Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian Theobalt, and Gerard Pons-Moll. Learning to reconstruct people in clothing from a single rgb camera. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1175-1186, 2019.\n' +
      '* [3] Human Alloy. Human alloy, 2023.\n' +
      '* [4] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. _arXiv preprint arXiv:2302.12288_, 2023.\n' +
      '* [5] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, and Gerard Pons-Moll. Multi-garment net: Learning to dress 3d people from images. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 5420-5430, 2019.\n' +
      '* [6] Zhongang Cai, Davuan Ren, Ailing Zeng, Zhengyu Lin, Tao Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang Pan, et al. Human: Multi-modal 4d human dataset for versatile sensing and modeling. In _European Conference on Computer Vision_, pages 557-577. Springer, 2022.\n' +
      '* [7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660, 2021.\n' +
      '* [8] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16123-16133, 2022.\n' +
      '* [9] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Obiayexre: A universe of annotated 3d objects. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13142-13153, 2023.\n' +
      '* [10] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, and Michael J Black. Collaborative regression of expressive bodies using moderation. In _2021 International Conference on 3D Vision (3DV)_, pages 792-804. IEEE, 2021.\n' +
      '* [11] Xiangjun Gao, Jiaolong Yang, Jongyoo Kim, Sida Peng, Zicheng Liu, and Xin Tong. Mps-nerf: Generalizable 3d human rendering from multiview images. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.\n' +
      '* [12] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization for learning shapes. _arXiv preprint arXiv:2002.10099_, 2020.\n' +
      '* [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [14] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. _arXiv preprint arXiv:2311.04400_, 2023.\n' +
      '* [15] Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei Yang, and Ziwei Liu. Sherf: Generalizable human nerf from a single image. _arXiv preprint arXiv:2303.12791_, 2023.\n' +
      '* [16] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Jiaxiang Tang, Deng Cai, and Justus Thies. Tech: Text-guided reconstruction of lifelike clothed humans. _arXiv preprint arXiv:2308.08545_, 2023.\n' +
      '* [17] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture for structured inputs & outputs. _arXiv preprint arXiv:2107.14795_, 2021.\n' +
      '* [18] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12753-12762, 2021.\n' +
      '* [19] Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang Liu, and Hujun Bao. Bcnet: Learning body and cloth shape from a single image. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XX 16_, pages 18-35. Springer, 2020.\n' +
      '* [20] Angjoo Kanazawa, Michael J Black, David W Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7122-7131, 2018.\n' +
      '* [21] Youngjoo Kwon, Dahun Kim, Duygu Ceylan, and Henry Fuchs. Neural human performer: Learning generalizable radiance fields for human performance rendering. _Advances in Neural Information Processing Systems_, 34:24741-24752, 2021.\n' +
      '* [22] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, and Youliang Yan. Cliff: Carrying location information in full frames into human pose and shape estimation. In _European Conference on Computer Vision_, pages 590-606. Springer, 2022.\n' +
      '* [23] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl: A skinned multi-person linear model. In _Seminal Graphics Papers: Pushing the Boundaries, Volume 2_, pages 851-866. 2023.\n' +
      '* [24] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael J Black. Learning to dress 3d people in generative clothing. In _Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6469-6478, 2020.\n' +
      '* [25] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.\n' +
      '* [26] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4195-4205, 2023.\n' +
      '* [27] Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J Black. Clothcap: Seamless 4d clothing capture and retargeting. _ACM Transactions on Graphics (ToG)_, 36(4):1-15, 2017.\n' +
      '* [28] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. _CoRR_, abs/2103.13413, 2021.\n' +
      '* [29] RenderPeople. Renderpeople, 2018.\n' +
      '* [30] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.\n' +
      '* [31] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 2304-2314, 2019.\n' +
      '* [32] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 84-93, 2020.\n' +
      '* [33] Kaiyue Shen, Chen Guo, Manuel Kaufmann, Juan Zarate, Julien Valentin, Jie Song, and Otmar Hilliges. X-avatar: Expressive human avatars. _Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [34] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* [35] Twindom. Twindom. 6\n' +
      '* [36] Junying Wang, Jae Shin Yoon, Tuanfeng Y Wang, Krishna Kumar Singh, and Ulrich Neumann. Complete 3d human reconstruction from a single incomplete image. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8748-8758, 2023.\n' +
      '* [37] Chung-Yi Weng, Brian Curless, Pratul P Srinivasan, Jonathan T Barron, and Ira Kemelmacher-Shlizerman. Humannerf: Free-viewpoint rendering of moving people from monocular video. In _Proceedings of the IEEE/CVF conference on computer vision and pattern Recognition_, pages 16210-16220, 2022.\n' +
      '* [38] Zhenzhen Weng, Kuan-Chieh Wang, Angjoo Kanazawa, and Serena Yeung. Domain adaptive 3d pose augmentation for in-the-wild human mesh recovery. In _2022 International Conference on 3D Vision (3DV)_, pages 261-270. IEEE, 2022.\n' +
      '* [39] Zhenzhen Weng, Zeyu Wang, and Serena Yeung. Zeroavatar: Zero-shot 3d avatar generation from a single image. _arXiv preprint arXiv:2305.16411_, 2023.\n' +
      '* [40] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J Black. Icon: Implicit clothed humans obtained from normals. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 13286-13296. IEEE, 2022.\n' +
      '* [41] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J Black. Econ: Explicit clothed humans optimized via normal integration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 512-523, 2023.\n' +
      '* [42] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. _Advances in Neural Information Processing Systems_, 34:4805-4815, 2021.\n' +
      '* [43] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard Szeliski, Jonathan T Barron, and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-time view synthesis. _arXiv preprint arXiv:2302.14859_, 2023.\n' +
      '* [44] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR2021)_, 2021.\n' +
      '* [45] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: A large-scale dataset of multi-view images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9150-9161, 2023.\n' +
      '* [46] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang, Yebin Liu, Limin Wang, and Zhenan Sun. Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11446-11456, 2021.\n' +
      '* [47] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 586-595, 2018.\n' +
      '* [48] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and Yebin Liu. Deephuman: 3d human reconstruction from a single image. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7739-7749, 2019.\n' +
      '* [49] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. Pamir: Parametric model-conditioned implicit representation for image-based human reconstruction. _IEEE transactions on pattern analysis and machine intelligence_, 44(6):3170-3184, 2021.\n' +
      '* [50] Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, and Ruigang Yang. Detailed human shape estimation from a single image by hierarchical mesh deformation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4491-4500, 2019.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:13]\n' +
      '\n' +
      '그림 11: HDNet[18]에 대한 정규 비교이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>