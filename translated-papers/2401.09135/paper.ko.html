<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '언어 모델링을 위한 다차원적인 로컬-SGD 훈련.\n' +
      '\n' +
      'Bo Liu\\({}^{*}\\)\n' +
      '\n' +
      '1\n' +
      '\n' +
      ' 라키타 헌카리아\\({}^{2}\\)\n' +
      '\n' +
      ' 아더 더빌라드\\({}^{2}\\)\n' +
      '\n' +
      ' 사티엔 케일\\({}^{3}\\)\n' +
      '\n' +
      ' 안드로이 A. 러슈\\({}^{2}\\)\n' +
      '\n' +
      ' 자준선성({}^{2}\\)\n' +
      '\n' +
      ' 아더 스즈마티({}^{2}\\)\n' +
      '\n' +
      'Marc\'Aurelio Ranzato\\({}^{2}\\)\n' +
      '\n' +
      '오스틴 텍사스대학 1명, 구글 딥민드 2명, 구글 딥민드 3명, 구글 연구소 3명이 구글 딥민드의 인턴으로 활동했다.\n' +
      '\n' +
      '부츠 1: 때때로 페데레이티드 평균(FedAvg)이라고도 하는 로컬-SGD 용어는 여기에서 사용자가 다른 작업자에게 데이터 할당에 대한 통제를 갖는 분산 최적화에서 뿌리를 강조하는 데 사용된다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '각 디바이스가 통신당 하나 이상의 SGD 업데이트를 수행하는 분산 최적화에 대한 접근 방식인 피지 평균화라고도 하는 국소 확률적 구배 하강(지방-SGD)도 있다. 이 작업은 언어 모델을 훈련하기 위한 _asynchronous_ 로컬-SGD에 대한 실증 연구를 제시하고, 즉 각 작업자는 SGD 단계를 마치자마자 글로벌 파라미터를 업데이트한다. 작업자 하드웨어 이질성, 모델 크기, 근로자 수, 최적화자가 학습 성과에 어떤 영향을 미칠 수 있는지 조사하여 포괄적인 조사를 실시한다. 우리는 순진한 구현과 함께 비동기적 로컬-SGD가 (글로벌) 모델 파라미터를 더 자주 업데이트했음에도 불구하고 동기적 대응물보다 수렴하기 위해 더 많은 반복을 취한다는 것을 발견했다. 작업자 구배가 핵심 도전으로 엉망일 때 전 세계 매개변수에 대한 운동량 가속도를 식별한다. 지연된 네스테로프 운동량 업데이트를 활용하여 작업자의 지역 훈련 단계를 연산 속도에 따라 조정하는 새로운 방법을 제안한다. C4 데이터세트 상의 최대 150M 파라미터로 평가된 이 접근법은 업데이트 단계당 엄격성 측면에서 동기 로컬-SGD의 성능과 일치하며 벽 시계 시간 측면에서 크게 능가한다.\n' +
      '\n' +
      '비동기식 훈련, 언어 모델링, 대규모 분산 학습 2024\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델(LLM)은 많은 응용 분야에 혁명을 일으켜 기계가 인간 언어와 상호 작용하는 방식을 변화시켰다. 이 혁명의 초석은 이러한 모델을 대규모 규모로 훈련하고 있다. 이러한 대규모 교육을 합리적인 시간으로 관리하기 위해서는 여러 장치에 걸쳐 계산을 배포할 필요가 있었다. 그러나 이 분산 학습에 대한 표준 접근법은 빠른 상호 연결과 함께 공동 위치된 장치를 사용한다.\n' +
      '\n' +
      '보다 강력한 대형 모델을 구축하기 위해 지리적으로 서로 멀리 떨어진 광범위한 범위의 계산 자원을 효과적으로 활용할 수 있기를 희망할 수 있다. 그러나 수많은 먼 기기를 사용하는 것은 통신 지연의 중요한 장애물에 직면한다. 디바이스들이 중앙 서버로 되돌리기 전에 컴퓨팅 구배에만 초점을 맞출 때, 통신 시간은 계산 시간을 과대평가하여 효율성의 병목 현상을 생성할 수 있다.\n' +
      '\n' +
      '국소학적 Gradient Descent(지방-SGD)은 통신 병목 현상을 감소시킬 수 있는 최적화 방법의 집합체이며, 이 방법은 파라미터 서버와 파라미터 업데이트를 동기화하기 전에 여러 지역 구배 단계를 수행하는 각 장치를 포함한다. 지방-SGD는 통신 빈도를 감소시켜 훈련 효율을 향상시키지만 _straggler__straggler__straggler_를 겪을 수 있다.\n' +
      '\n' +
      '그림 1: 아스네크의 일러스트레이션. v.s. 동기. 2명의 근로자(파란색과 빨간색으로)와 교육합니다. 싱크. 훈련은 가려움증 효과와 비인크에 시달립니다. 훈련은 빠른 작업자의 공회전 시간을 줄입니다.\n' +
      '\n' +
      '이종 기기로 인한 효과_ 예를 들어, 더 빠른 장치는 더 느린 장치를 따라잡기 위해 유휴하게 기다리고 있어 시스템의 전반적인 효율성을 약화시킨다. 더욱이, 모든 디바이스들은 파라미터 서버와의 높은 대역폭 연결을 필요로 하는 동시에 통신하도록 강제된다. 동기형 로컬-SGD는 작업자의 업데이트가 가능한 즉시 서버에서 모델을 업데이트할 수 있게 하여 계산 활용도를 높이고 통신 대역폭 요구사항을 최소화할 수 있어 보다 실행 가능한 솔루션(그림 1에서 하향 조정)을 제시한다.\n' +
      '\n' +
      '이 연구에서 우리는 로컬-SGD를 사용하여 비동기적으로 LLM을 훈련시키는 생존력을 탐구한다. 우리는 동기 로컬-SGD(Douillard et al, 2023; Ryabinin et al, 2021) 동안 작업자의 하위 집합에 대한 대체 단계를 시도하거나 작업자의 특정 하위 집합을 무작위로 낮추려는 이전 작업을 확장한다. 주요 콘텐츠는 세 부분으로 구성되어 있습니다.\n' +
      '\n' +
      '1. 프레임워크(섹션 3)는 비동기식 훈련 프레임워크에 대한 고수준의 디자인을 소개합니다. 우리는 각 작업자가 학습율이 무엇인지, 서버가 모델을 어떻게 비동기적으로 업데이트하는지, 몇 단계 동안, 어떤 데이터를 훈련시킬지를 결정하는지에 대해 논의한다.\n' +
      '\n' +
      '2. 최적화 챌린지(섹션 4)에서 비동기적 지방-SGD에 적합한 다양한 기존 최적화 전략에 대한 실증 연구를 수행한다. 여기에는 작업자 측 최적화(제너 최적화)와 서버 측 최적화(제터 최적화)가 모두 포함된다. 운동량을 효과적으로 활용하는 데 있어 핵심 과제를 발견합니다. 특히 적응 운동량 방법은 일반적으로 내부 및 외부 최적화의 융합을 가속화하는 반면, 비동기 로컬-SGD의 효능은 특히 동기 구현과 비교할 때 운동량 기술을 모두 사용할 때 상대적으로 감소한다.\n' +
      '\n' +
      '3개의 제안 솔루션(섹션 5)은 딜레이 네스테로프 모멘텀 업데이트(DN)와 다이나믹 로컬 업디테스(DyLU)의 단순하고 효과적인 두 가지 기술을 소개한다. 이러한 기술은 조합되고 훈련하는 란을 조합하고 평가할 때 이러한 기술을 사용한다.\n' +
      '\n' +
      '그림 2: 동기화를 이용한 언어모델 비교평가. 비누와 비네크. 20M 매개변수 모델에서 4명의 이질적인 작업자를 가진 지역-SGD 방법. 최첨단 동기입니다. 지역-SGD 방법인 디LoCo(Douillard et al, 2023)는 각각 아담W와 네스테로프 모멘텀을 근로자측 및 서버측 최적화기로 사용한다. 이 최적화기 조합은 아스네크의 가장 강력한 것으로 남아 있다. 지역-SGD 훈련(그림 5 참조)은 그러나 저성능 디LoCo는 상당히 낮다. 외부 최적화를 위한 딜레이 네스테로프(DN)(알고리즘 3)와 동적 로컬 업도트(DyLU)(섹션 5)를 통합함으로써, 우리는 동기화 간의 업데이트에 대한 엄격성 측면에서 성능 격차를 크게 가교한다. 비누와 비네크. 언어 모델링에 대한 훈련. 더욱이, 제안된 방법은 벽 시계 시간 대 비장 측면에서 디LoCo를 크게 능가한다.\n' +
      '\n' +
      ' 기저 모델은 비동기적인 로컬-SGD가 지역 업데이트의 총 수에 대한 경막성 측면에서 동기적 로컬-SGD에 접근하고 비동기적 로컬-SGD 대 비동기적 로컬-SGD를 더욱 향상시킬 수 있도록 한다. 그림 2에 자세히 설명된 바와 같이 경막 대 벽시계 측면에서 동기적 지방-SGD가 있다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '본 연구에서는 \\(k\\) 데이터 음영 전반에 걸쳐 공유 모델 매개변수 \\(\\theta\\)의 분산 최적화에 중점을 두고 \\(\\mathcal{D}=\\{\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{k}\\) 작업자를 사용하여\\(k\\) 작업자를 사용하여 1차 목표를 설명한다.\n' +
      '\n' +
      '부츠 2: 근로자 수(\\(k\\))는 데이터 샤드 수와 동일하지만, 우리의 방법은 데이터 샤드보다 작업자가 적을 때도 적용 가능하다.\n' +
      '\n' +
      '}.{j}{mathcal{D}}{mathb{D}}\\{{{}\\]\n' +
      '\n' +
      'Hi(\\ell(\\cdot;\\theta)\\가 손실 함수(예를 들어 언어 모델링에서 다음 토큰 예측에 대한 교차 엔트로피 손실)를 나타내고, \\(|\\cdot|\\)는 설정된 크기를 나타낸다.\n' +
      '\n' +
      '이 작업에서 로컬-SGD의 정의를 확장하여 원래 로컬-SGD 방법뿐만 아니라 고급 최적화 기술을 통합하는 변이체를 포함한다. 특히 언어 모델링에서 동기 로컬-SGD의 기준을 설정하는 디LoCo(Douillard et al., 2023)에 초점을 맞추고 있다. 알고리즘 1에서 Each 근로자 \\(i\\)는 데이터 샤드 \\(\\mathcal{D}_{i}_{i}\\)에 대한 _inner 최적화기_를 사용하여 로컬 업데이트를 수행하여 파라미터 변경(슈도- 업그레이팅) \\(슈도델타_{i}^{(t)}=\\theta_{i}^{(t)}^{(t)를 서버에 다시 전송하기 전에 데이터 샤드 \\(\\mathcal{D}_{i}_\\)에 대한 inner 최적화기_inner 최적기_\\)를 사용하여 로컬 업데이트를 수행한다. 그런 다음 서버는 응집된 외부 구배 \\(\\Delta^{{(t)}=\\frac{1}{k}\\sum_{i =1}^{k}\\delta_{i}^{{(t)}\\\\)를 계산하고 \\(\\Delta^{{(t)}\\\\)를 사용하여 _outer 최적화기__outer 최적화기를 적용하여 \\(\\Delta^{{(t)를 갱신한다. 디LoCo의 주요 통찰력은 각각 최고의 내부 및 외부 최적제로 AdamW 및 Nrierov 모텐톤의 최적 사용이다.\n' +
      '\n' +
      '## 3 Async. 현지-SGD 프레임워크\n' +
      '\n' +
      '이 섹션에서는 중심 서버가 모든 작업자를 제어하고 업데이트를 비동기적으로 응집시키는 비동기 로컬-SGD 파이프라인 설계를 설명한다.\n' +
      '\n' +
      '데이터 샤드 샘플링은 각 디바이스가 자신의 데이터에 부착된 피딩된 학습 설정과 달리 분산 최적화에서 사용자는 어떤 데이터가 어떤 작업자가 동적으로 어떤 작업자에게 할당되는지 선택할 수 있는 권리를 가진다. 작업자가 새로운 로컬 최적화 라운드를 시작할 준비가 되었을 때 다른 데이터 샤드(근로자는 이질적)에 대한 학습 진행의 균형을 맞추기 위해 "학습 진행"에 반비례하는 데이터를 샘플링한다. 구체적으로 \\(n_{i}\\)를 \\(\\mathcal{D}_{i}\\)에서 학습된 데이터 포인트의 수로 정의하는 다음, 샤드 \\(i_{\\text{ 샘플링}}\\)를 샘플링한다.\n' +
      '\n' +
      '}\\fegin{splid}}}\\\\ &\\text{split}}}\\\\ &\\text{i}\\\\ &\\text{i}\\frac{|}\\frac{|\\mathcal{D}_{i}}}{frac{mathcal{D}_{j}_{j}},0.\n' +
      '\n' +
      '즉, 우리는 데이터 샤드(즉, \\(\\frac{n_{i}}{\\sum_{j}}{\\sum_{j}}\\leq\\frac{mathcal{D}_{i}}{\\q\\frac{D}_{\\sum_{j}{\\mathcal{D}_{\\mathcal{D}_{\\mathcal{D}_{j}{\\mathcal{D}_{j}{\\mathcal{D}_{j}{\\mathcal{D}{j}{j}{\\mathcal{D}{j}{j}{\\mathcal{D}{j}{j}{j}{\\mathcal{D}_{j}{j}{j}{j}_{j}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}|}_{j}| 샤드가 과소 샘플링된 정도는 샘플링 속도를 결정한다. 그렇게 함으로써, 더 느린 경과를 가진 데이터 샤드가 훈련을 위해 샘플링될 가능성이 더 높기 때문에 화소에 걸친 학습 경과를 균형을 맞출 수 있다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '```\n' +
      '0: 경구 전치 모델 \\(\\ta^{(0)}\\)\n' +
      '0:\\(k\\) workers\n' +
      '0:Grace 기간 \\(\\tau_{\\text{grace}}\\)\n' +
      '0: 총 로컬 업데이트 \\(t_{\\text{max}}\\)\n' +
      '1:\\(t_{\\text{local}}=0\\)\n' +
      '2:\\(\\theta\\leftarrow\\theta^{(0)}\\)\n' +
      '3:\\(\\mathcal{W}\\) = [\\(k\\)]에서 \\(i\\)에 대한 [init_worker(k\\)][init_worker]]\n' +
      '4:\\(성실{W}_{\\text{{}}\\)\n' +
      '5:\\(\\text{train}(\\mathcal{W},\\ \\theta)\\)\n' +
      '6:\\(\\tau_{\\text{sync}=\\riangleright\\) \\(\\tau_{\\text{sync})의 시작.\n' +
      '7:while\\(t_{\\text{local}}<t_{\\text{max}}\\)do\n' +
      '8:\\(w\\)=\\(\\mathcal{W}),\\tau_{\\text{sync}}\n' +
      '종업원 9:9:\\(첼트리랑플러라이트\\)를 받아 완성근로자를 얻게 된다.\n' +
      '그런 다음 10:if\\(w\\)가 존재합니다.\n' +
      '서버와 업데이트를 동기화한다.\n' +
      '12:\\(\\tau_{\\text{sync}}=\\min(\\tau_{\\text{sync}},w.\\text{completed\\_time})\\)\n' +
      '13:\\(\\theta\\leftarrow\\text{sync}(\\theta,\\ w.\\text{update})\\)\n' +
      '14:\\(\\mathcal{W}_{\\text{completed}}.\\text{add}(w)\\)\n' +
      '15:\\(t_{\\text{ 로컬}}\\) += \\(w.\\text{ 로컬\\_updates}\\)).\n' +
      '16:else\n' +
      '완성된 근로자를 위한 일자리를 보유하고 있다.\n' +
      '18:\\(\\tau_{\\text{sync}}=\\infty\\)\n' +
      '19:\\(\\text{train}(\\mathcal{W}_{\\text{completed}},\\ \\theta)\\)\n' +
      '20:\\(성실{W}_{\\text{{}}\\) = ["\n' +
      '21:endif\n' +
      '22:endwhile\n' +
      '```\n' +
      '\n' +
      '**알고리즘 2** Async. 지역-SGD 태스크 세글링.\n' +
      '\n' +
      '4 Optimization 챌린지 4 Optimization 챌린지.\n' +
      '\n' +
      '최적화가 비동기적인 로컬-SGD의 언어 모델링 성능에 어떻게 영향을 미치는지 연구하기 위해 먼저 SGD+Nrierov, SGD+Adam, AdamW+SGD, AdamW+SGD 모멘텀, AdamW+Adam, AdamW+Nicidesov의 내부 및 외부 최적화기(A+B를 사용하여 내부 및 외부 최적기로 A 및 B를 각각 나타내는지)의 서로 다른 조합을 실험한다. 각각의 조합에 대한 하이퍼파라미터는 AdamW에 대해 InnerOpt로서 디폴트 값을 유지하기 위해 별도로 조정된다. 우리는 장치 속도가 그림 4에 표시된 \\(k=4\\) 작업자가 있다고 가정하며, 초기 모델 체크포인트가 분산 훈련 없이 아담으로 24,000단계를 전처리한 근로자 1인당 64,000단계(총 14만6,000단계)에 대해 20M 매개변수 언어 모델에 비동기적인 로컬-SGD 핀셋링을 적용한다. 지방-SGD 스타일 방법이 핀셋링에서 잘 작동하지만 스크래치(Lin et al, 2018)에서 덜 효율적이라는 것이 관찰되었지만 다른 사람들은 로컬-SGD가 스크래치(Douillard et al, 2023)에서 훈련에도 잘 작용한다는 것을 관찰했기 때문에 로컬-SGD로 핀셋링을 선택한다. 학습률 스케줄링 및 작업 스케줄링은 제3절에서 설명한 절차를 따른다. 우리는 기본으로 모든 실험에서 모든 작업자에 걸쳐 내부 단계(\\(=50\\)를 사용한다. 결과는 그림 5에 나와 있다.\n' +
      '\n' +
      '분석 결과, AdamW를 내부 최적자로 하여 노스테로프 운동량을 외부 최적기로 결합하면 디LoCo 방법과 같이 동기 훈련의 결과와 일치하는 최상의 결과가 도출된다는 것을 알 수 있다. 특히, AdamW를 외부 최적화기로 사용하는 것은 덜 효과적이다. 아담에서 유래한 애덤W가 정규화 효과를 도입하여 상쇄할 수 있기 때문일 것이다.\n' +
      '\n' +
      '그림 4: 각 장치에 대해 초당 스톤입니다.\n' +
      '\n' +
      '그림 5: 4명의 작업자와 함께 20M 언어 모델에서 비동기 로컬-SGD 교육을 위한 내부 및 외부 최적화기의 다양한 조합을 사용하는 성능이다.\n' +
      '\n' +
      '의사 등급은 실제 구배보다 더 크며 잠재적으로 수렴이 둔화되는 경향이 있다. 내부 최적화에 AdamW를 사용할 경우 SGD, SGD 모텐텀 및 네스테로프가 비슷한 성능을 보인다. 그러나 네스테로프는 학습 곡선을 안정화시킬 뿐만 아니라 최종 성능도 약간 향상시킵니다. 이는 업데이트 메커니즘(표기를 악용하고 \\(t\\)이 \\(t_{\\text{server}}\\)임을 나타내는 경우)에 기인할 수 있다.\n' +
      '\n' +
      '<타_{t <\\\\>} (}\\beta^{2} {t})\\{t} (1+\\beta)\\.\n' +
      '\n' +
      '아이티(\\epsilon\\)가 학습률인 경우 \\(m_{t}\\)는 운동량, \\(g_{t}\\)는 시간 \\(t\\), \\(\\beta\\in(0,1)의 구배이다. 네스테로프와 SGD 모멘텀의 주요 차이점은 네스테로프가 가중치를 조정하여 \\(\\beta\\)) 대신 운동량 성분(\\(\\(\\beta^{2}\\)을 감소시키고 \\(1\\) 대신 구배 성분(\\(1+\\beta\\)을 증가시키는 방법에 있다. 이것은 운동량이 지방-SGD에서 중요하면서도 복잡한 역할을 한다는 것을 시사한다.\n' +
      '\n' +
      '외측 최적기에 대한 운동량항의 모멘텀은 외부 최적기에 대한 모멘텀항의 영향에 더 깊이 정착하기 위해 동기 및 비동기 훈련 환경 모두에서 AdamW+SGD와 AdamW+Nrierov 사이의 비교 분석을 수행했다. 이러한 실험은 앞서 설명한 것과 동일한 조건에서 수행되었다. 결과는 그림 6에 보고되어 있다.\n' +
      '\n' +
      '그림은 비동기적인 로컬-SGD에서 운동량 용어가 없는 AdamW+SGD가 동기적 대응물보다 최종 엄격성과 학습 효율을 향상시킨다는 것을 분명히 보여준다. 그러나, 네스테로프 운동량을 아웃레이트에 통합하면 동기 로컬-SGD의 성능이 크게 향상되어 비동기 버전을 능가한다. 비동기 아담W+네스테로프는 테스트된 모든 내부 및 외부 최적화기 조합(그림 5에서 볼 수 있는 것)에 걸쳐 최고의 연주자로 남아 있다는 점은 주목할 만하다. 이 관찰은 운동량이 언어 모델링을 위한 비동기 로컬-SGD에 유익하지만 동기 설정에서 그 효과가 더 뚜렷하다는 것을 나타낸다.\n' +
      '\n' +
      '우리는 원인 원인인가? 동질적인 장치와 비동기적인 디LoCo 알고리즘을 더 적용한다. 그렇게 함으로써, 우리는 서버 모델을 업데이트하기 위해 오래된 외부 구배를 사용하고 있다는 것을 지칭하는 로컬-SGD에서 스티칭된 구배 문제를 최대적으로 감소시킨다. 특히, 이는 우리가 \\(k\\) 근로자를 가지고 있다면, 모두 계산된 외부 구배를 동시에 서버에 되돌릴 것임을 의미한다. 따라서 유일한 인기는 개별 업데이트를 함께 집계하지 않고 순차적으로 적용하여 한 번 적용하고 있다는 사실에서 비롯된다. 결과는 그림 7에 요약되어 있다.\n' +
      '\n' +
      '관찰 그림 7은 작업자 간의 동질성에도 불구하고 비동기 디LoCo는 동기 상대방에 크게 뒤처지는 주목할 만한 발견을 보여준다. 이는 동시 업데이트를 순차적으로 적용하는 _부착성 스테이탈리티_가 상당한 성능 저하로 이어진다는 것을 시사한다. 이러한 효과를 설명하기 위해 \\(k=4\\) 작업자가 동일한 외부 구배(g\\(g\\)를 제공하는 시나리오를 고려해보자. 표준 네스테로프 운동량 업데이트는 식 (4)에 설명되어 있다. 의사학년의 순차적 적용에서.\n' +
      '\n' +
      'r\\_{t+}(}4+\\beta^{2} <\\beta^{2} <\\beta^{2} <\\beta^{2} <\\beta^{2}> <\\beta^{2} <\\beta{2}> <\\beta{2} <\\beta{2>} <\\beta^{2>} <\\beta^{2>} <\\beta{2>} <\\beta^{2 <\\beta{2>} <\\beta^{2>} <\\beta^{2>} <\\beta{2>} <\\beta^{2 <\\beta{2>} <\\beta^{2>} <\\beta{2>} <\\beta{2 <\\beta{2>} <\\beta{2 <\\beta{2>} <\\beta{2 <\\beta{2>} <\\beta{2>} <\\beta{2>} <\\beta{2>} <\\beta{2>}<\n' +
      '\n' +
      '이를 통해 순차적 적용이 보다 빠르게 붕괴되는 운동량 항을 초래하지만 매개변수 \\(\\theta\\)의 실제 변화를 증폭시킨다는 것을 관찰하였다. 결과적으로, 더 높은 \\(\\beta\\)는 더 최근의 모멘텀을 유지하지만 파라미터의 더 큰 변화로 이어질 수 있으며 그 반대의 경우도 마찬가지이다. 중요한 것은 이러한 불균형이 단순히 학습률을 줄여 시정할 수 없다는 것이다.\n' +
      '\n' +
      '우리는 문헌의 여러 동기적 기저부와 비동기적 설정에 대한 순진한 응용: **1)*** Finetune 1 근로자(4xbatch): 이것은 동기 SGD와 동일하고 더 큰 배치 크기를 가진 단일 작업자를 고용하는 것을 포함한다. 이 동기적 지방-SGD 방법은 AdamW와 네스테로프의 ***3)**D 방법을 결합한 것이다. * Async. 디LoCo: 디LoCo의 비동기 버전입니다.\n' +
      '\n' +
      '기존의 Fixes는 관찰된 문제를 해결하기 위해 비동기 로컬-SGD 문헌에서 잠재적인 고정을 조사했다. 다음 방법은 **1)** Async로 간주되었다. DiLoCo + 폴리(Xie et al., 2019): Async를 계산합니다. 디LoCo는 \\(g\\lelearrow(1+\\text{staleness})^{-0.5}g\\)를 사용하여 유사 업그레이드를 하향 가중하여 **2)**Async. DiLoCo + 폴리-Thres: 10. **3)*^{-0.5}g\\ 이상으로 스테이플리티로 업데이트를 폐기하는 임계값을 제공한다. * Async. 디LoCo + 델레이 화합물(정 등 2017): 생산 지연 보상(지연 화합물)은 진정한 의사 등급에 근사한다. 1차 테일러 근사치(g(\\theta_{t}})\\ a\\(\\theta_{t})\\ 기울기 g(\\theta_{t}})를 사용하여\\(\\theta_{t})\\ 기울기 g(\\theta_{t})\\ a\\<\\<\\)\\ a\\(\\theta_{t})\\<\\<\\<\\)\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\<\\ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff (\\nabla_{\\mathcal{B}},\\nabla_{\\mathcal{B}})\\o타_{\\prime}}(\\theta_{\\prime}})\\ododot g(\\theta_{\\prime})\\\\odot g(\\nabla_{\\prime})\\odot g(\\theta_{\\prime})\\odot g(\\theta_{\\prime})\\odot g(\\nathime})\\ododot g(\\ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff * Async. 버퍼: 네스테로프 업데이트를 적용하기 전에 첫 번째 외부 패션에서 모든 구배를 측정하고 평균하며, 애덤W+네스테로프를 사용하여 원래 페데버프 알고리즘(Nguyen et al, 2022)의 변화이다. 결과는 그림 8에 나와 있다.\n' +
      '\n' +
      '의사 등급에 대한 관찰 다항식 할인은 한계 이점을 보여준다. 보유 및 지연 보상 기술은 많은 개선을 제공하지 않습니다. 다시 말하지만, 지연 보상이 잘 작동되지 않는다는 사실은 비동기 SGD와 비동기 로컬-SGD의 차이를 지적한다. Async. 버퍼 방법은 융합에 탁월하지만 훈련 초기에 불안정성을 나타낸다. 즉, 방법의 _none_은 동기 디LoCo 기준선들의 성능과 일치한다.\n' +
      '\n' +
      '그림 8: 상이한 비동기 로컬-SGD 접근법의 비교이다. 폴리, 폴리 티즈, 델레이 컴파운드는 비인크를 거의 향상시키지 않습니다. 지역-SGD 공연. 아닌크. 버퍼는 동기 간의 간격을 상당히 닫습니다. 비누와 비네크. 훈련, 훈련 초기 단계에서 불안정성을 도입하면서 훈련한다.\n' +
      '\n' +
      '5개는 솔루션입니다.\n' +
      '\n' +
      '4절에서 요약된 최적화 과제를 해결하기 위해 우리는 두 가지 전략을 개발했다.\n' +
      '\n' +
      '특히, 누에스테로프 업데이트를 지연시킨다. 버퍼 방법은 유망한 성능(그림 8과 같이)을 보여주었다. 또한, 우리의 분석에서는 AdamW+SGD, sans 외부 운동량, 동기적 방법(그림 5)과의 비동기적 훈련이 동기적 방법을 능가한다는 것을 보여주었다. 이러한 통찰력을 기반으로 알고리즘 2에서 동기() 기능을 나타내는 _지연된 네스테로프_(DN) 전략을 제안하며, 이 접근법은 간헐적으로-매우 \\(N\\) 서버 업데이트를 사용하는 것을 포함한다. 네스테로프 업데이트 사이에서 우리는 완충액 \\(\\Delta\\)에서 유사 업그레이트를 집계하고 기울기 하강(또는 구배 하강 및 오래된 운동량의 작은 부분)을 사용하여 모델 파라미터를 업데이트한다. 구배와 운동량 기반 하강성을 균형을 맞추기 위해 매개변수 \\(c\\in[0,1/N]\\)를 소개한다. Hf(0\\)의 \\(c\\) 값은 네스테로프 업데이트 간에 순수한 구배 하강도를 나타내는 반면, \\(1\\)와 동일한 \\(c\\)는 \\(N\\) 업데이트보다 운동량항을 고르게 분배한다. 이 알고리즘의 구체적인 내용은 알고리즘 3에 자세히 설명되어 있으며, Async와 같다. N\\(N\\) 기간에 한 번만 모델 파라미터를 업데이트하는 버퍼(Nguyen et al, 2022)는 구배를 사용하여 계속 업데이트하고, 오래된 운동량의 일부를 통합하고, 모든 \\(N\\) 서버 업데이트에서 한 번 운동량을 업데이트한다.\n' +
      '\n' +
      '```\n' +
      '0: 인티티 모델 매개변수 \\(\\ta_{0}\\)\n' +
      '0: 엄마 부패 \\ (0,1)\n' +
      '0: 엄마 활성화 \\ (c\\in[0,1/N]\\) \\ (c=0\\)에 기본이다.\n' +
      '0: Buffer 크기 \\(m_{0}=0\\) \\(m_{0\\) \\(\\triangleright\\) 운동량 \\(\\Delta=0\\) \\(\\triangleright\\) 응집 구배를 완료하지 않고 유사 등급 \\(g_{t}\\) \\(\\triangleright\\) \\(\\ a\\) \\(m_{0\\) \\(\\,\\) \\(\\,\\) \\(\\,\\) \\(\\,\\) \\(\\-Ex(\\ -0\\) \\(\\,\\) \\(g_{t}\\) \\(\\) \\(g_{t}\\) \\(\\) \\(\\) \\(g_{tincangleright\\) \\(\\) \\(\\) \\(\\,\\ \\(g_{tobacteriumleright\\) \\(\\,\\) \\(\\,\\,\\,\\) \\(g_{triang 다른 \\(m_{t+/\\)\\(m_{t+}{t/\\)\\(mm_{t+}<\\-d\\) <\\<<\\<<\\<<\\<<\\<<<<<<<<<<<<<<<<<<<<<<<>>>> < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <>>>>>> < <\\>>> < <\\>>> < <\\>> < <\\>>> < <\\>> < <\\>> < <\\>> < <\\>>> < <\\>> < <\\>>> < <\\>> < <\\>> < <\\>> < <\\>>> < <\\>> < <\\>>> < < <\\>>>\n' +
      '```\n' +
      '\n' +
      '**알고리즘 3** 딜레이드 네스테로프 업데이트입니다.\n' +
      '\n' +
      '다이나믹 로컬 업데이트 델레이 네스테로프 접근법은 의사 업그레이트를 완충하고 전략적으로 모멘텀 업데이트를 지연시켜 아웃에오폼의 모멘텀 챌린지를 다룬다. 대안적 관점은 동기적 훈련을 모든 근로자의 의사 등급들이 동기화되는 해결책으로 간주한다. 그러나 장치 역량의 다양성은 종종 각 작업자가 동일한 수의 지역 훈련 단계를 실행하면 동시 의사 업그레이드를 방해한다. 생존 가능한 작업은 각 장치의 처리 속도를 기반으로 로컬 트레이닝 단계(예를 들어, \\(w\\)를 맞춤화하는 것을 포함한다. 특히 근로자 \\(초당 단계)의 훈련 속도(초당 단계)로서 \\(v(w)\\를 의미하는데, 근로자의 원하는 훈련 단계를 그대로 계산할 수 있다.\n' +
      '\n' +
      '텍스트{ 단계}(w)}{\\frac{v(w)}{\\frac{v({\\frac{v:{w^{{\\prime}}, \\tag{6}\\]\\]\\[w.\n' +
      '\n' +
      'H\\(H\\)는 가장 빠른 작업자 실행의 수를 나타내고 \\(차층 x\\rfloor\\)는 \\(x\\)보다 크지 않은 가장 큰 정수(DyLU)를 나타낸다. 이러한 조정은 더 느린 작업자가 더 적은 단계를 실행하여 다른 작업자에 걸쳐 완료 시간을 정렬할 수 있게 한다. 이 설정에서 모델 동기화를 위한 유예 기간을 통합하는 것은 스테이플 구배의 영향을 더욱 감소시켜 전반적인 성능을 향상시킵니다.\n' +
      '\n' +
      '3: 여기, 우리는 장치 속도가 선험적으로 알려져 있다고 암묵적으로 가정한다. 만약 이것이 그렇지 않다면 경험적 관측에 기초하여 장치 속도를 추정하는 것은 간단하다.\n' +
      '\n' +
      '6개의 최소 도예.\n' +
      '\n' +
      '향후 연구의 편의성과 새로운 아이디어의 신속한 프로토타이핑의 편의를 위해 관찰된 최적화 도전을 비동기적인 로컬-SGD(그림 9.4)에서 복제하는 최소한의 장난감 예를 제시하며 가우시안 데이터의 혼합물의 혼합물에 대한 분류를 수행하는 것이 과제이다.\n' +
      '\n' +
      '부츠 4: [https://github.com/google-deepmind/asyncdiloco] (https://github.com/google-deepmind/asyncdiloco)에서 콜랩을 확인해 주세요.\n' +
      '\n' +
      '그림 9와 그림 6을 비교한 관찰 결과는 장난감 예시가 동일한 최적화 도전을 나타낸다는 것을 관찰하였다.\n' +
      '\n' +
      '## 7 Experiments\n' +
      '\n' +
      '이 섹션 세부 실험은 두 가지 제안된 방법인 델레이 네스테로프(DN) 및 다이나믹 로컬 업도트(DyLU)의 효능을 평가하기 위해 수행되었다. 또한, 절제 연구는 근로자 수와 모델 크기가 다양하기 때문에 이러한 방법의 효과를 탐구한다.\n' +
      '\n' +
      '델레이드 네스테로프(DN) 및 DyLU(DyLU)를 평가한 그림 2는 그림 8과 동일한 설정을 사용하여 단일 작업자 핀셋링 및 디LoCo와 같은 기저부에 대해 비동기 로컬-SGD와 DN 및 DyLU를 비교한다.\n' +
      '\n' +
      '결과는 DyLU와 결합된 DN이 갱신보다 동기 디LoCo의 성능을 능가하여 퍼플렉시를 크게 감소시킨다는 것을 보여준다. 또한 DN+DyLU는 더 느린 작업자의 지연을 피하기 위해 시간 효율성 측면에서 더 이상 성능을 보인다.\n' +
      '\n' +
      '작업자 Heterogeneity의 차별화 수준을 평가하면 그림 10(각도 곡선) 및 표 1(최종 엄격도)과 같이 다양한 정도의 작업자 장치 이질성에서 제안된 DN+DyLU 방법과 바닐라 비동기 디LoCo 요금이 어떻게 되는지 살펴본다.\n' +
      '\n' +
      '관찰DN+DyLU는 균질한 장치에도 불구하고 모든 이질성 수준에서 일관되게 탁월하며 바닐라 비동기 디LoCo 투쟁은 문제가 부분적으로 유사 조사자의 순차적 적용에 있음을 시사한다. 이는 지연 모멘텀 업데이트의 중요성을 강조한다. 또한, 특정 디바이스 그룹화에서 성능에서의 주기적인 진동이 관찰되어 원래 비동기 알고리즘의 견고성 부족을 더욱 강조한다.\n' +
      '\n' +
      '5: Async가 있음을 알립니다. DN+DyLU는 이질성이 없을 때 디LoCo보다 약간 더 잘 수행하는데, 이는 두 가지 방법이 동일하게 감소하고 훈련 곡선이 거의 완벽하게 일치하기 때문에 수치 오류 때문이다.\n' +
      '\n' +
      '근로자의 차별적인 무기력으로 재구성하면 20M 모형을 이용하여 근로자 수(4, 8, 16)를 변화시키면서 DN+DyLU를 적용하는데, 그 결과는 그림 11(각도 곡선)과 표 2(최종 각성)에 요약되어 있다.\n' +
      '\n' +
      '근로자가 증가함에 따라 지역-SGD 훈련의 편익은 감소한다. 특히 근로자 16명(16x 배치 크기)과 함께 1인 근로자 고용(16x 배치 크기)이 가장 우수한 실적을 보이고 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Level of heterogeneity & no & slight & moderate & very \\\\ \\hline Pretrained (24K) & 61.64 & 61.64 & 61.64 & 61.64 \\\\ Finetune (4\\% batch size) & 42.47 & 42.47 & 42.47 & 42.47 \\\\ DiLoCo (Doillard et al., 2023) & 41.35 & 41.35 & 41.35 & 41.35 \\\\ Async. DiLoCo & 44.27 & 44.38 & 44.29 & 44.27 \\\\ Async. DN + DyLU (ours) & **41.27** & **41.09** & **41.13** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 1>은 근로자 이질성 수준 (**top-left***, ***top-right****, **bottom-left******, ***slight**********, ***ion********************와 같다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Number of workers \\(k\\) & 4 & 8 & 16 \\\\ \\hline Pretrained (24K) & 61.64 & 61.64 & 61.64 \\\\ Finetune (\\(k\\times\\) batch size) & 42.47 & 41.28 & **40.60** \\\\ DiLoCo (Douillard et al., 2023) & 41.35 & 41.23 & 41.25 \\\\ \\hline Async. DiLoCo & 44.27 & 44.23 & 44.23 \\\\ Async. DN + DyLU (ours) & **41.13** & **41.02** & 40.98 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 2>는 근로자 수를 가리는 것이다.\n' +
      '\n' +
      '그림 9: 장난감 예에 대한 최적화 챌린지를 복제하세요. **Left**: 데이터 세트는 가우스의 혼합물의 혼합물로 구성된다. ** 우측**: 아sync. 지방-SGD는 동기보다 더 나쁨/베터를 수행한다. AdamW+Nrierov/AdamW+SGD를 사용할 때 국소-SGD를 사용한다.\n' +
      '\n' +
      '피규어. 그러나 DN+DyLU는 성능에서 동기 디LoCo와 밀접하게 일치하여 이질적인 환경에서 디LoCo 대안으로서의 잠재력을 보여준다.\n' +
      '\n' +
      '마지막으로 차별화 모델을 사용한 추출은 DN+DyLU를 다양한 크기의 모델(20M, 60M, 150M)에 적용하며, 결과는 그림 12(각도 곡선) 및 표 3(최종 퍼플로우)에 요약되어 있다.\n' +
      '\n' +
      '동기적 및 비동기적 로컬-SGD 방법을 모두 관찰하는 것은 배치 크기가 증가된 단일 작업자를 고용하는 접근법을 능가한다. 특히, 이러한 이점은 융합의 후기 단계에서 더욱 두드러지게 되며, 이는 지역-SGD의 우수한 일반화 능력(Gu et al., 2023)을 강조하는 이전 연구의 결과와 일치한다. 또한 제안된 DN+DyLU 방법은 다양한 모델 크기에 걸쳐 일관된 효능을 보여준다. 동기식 디LoCo와 비동기식 디LoCo 사이의 성능 차이는 모델 크기가 증가함에 따라 감소하지 않는다는 점에 유의하는 것이 중요하다.\n' +
      '\n' +
      '다른\\(c\\)를 사용한 증폭은 Async에 \\(c\\in\\{0,0.1\\}\\)를 적용한다. 다양한 \\(k\\)(4, 8, 16), 모델 크기(20M, 60M, 150M)를 가진 DN+DyLU는 4명의 "매우" 이질적인 작업자를 가지고 있다. 이는 이질성 수준이 작을 때 서로 다른 \\(c\\)를 사용하면 더 작은 차이(예: 이질성이 없을 때, 임의의 \\(c\\)가 동일한 알고리즘을 초래하기 때문이다. 결과는 표 4에 요약되어 있다.\n' +
      '\n' +
      '관찰학적으로 \\(c=0\\)와 \\(c=0.1\\) 사이에 유의미한 차이가 없음을 관찰하여 중간 단계에서 약간의 운동량을 추가하는 것이 너무 큰 도움이 되지 않음을 나타낸다. 그 결과, 우리는 2개의 연속 네스테로프 업데이트 간의 SGD 업데이트를 수행하는 것에 해당하는 알고리즘 3의 디폴트 값으로 \\(c=0\\)를 설정하였다. \\(c\\)의 값을 설정하는 것은 전체 알고리즘에 어떤 오버헤드를 도입하지 않는다는 점에 유의한다.\n' +
      '\n' +
      '8은 특정 일꾼입니다.\n' +
      '\n' +
      '이 섹션은 특히 비동기 환경에서 응용 프로그램에 초점을 맞춘 피빙된 학습과 지역-SGD 스타일의 분산 최적화에 대한 문헌의 간결한 개요를 제공한다.\n' +
      '\n' +
      '지역-SGD 및 분산 최적화 로컬-SGD는 통신 빈도를 줄이기 위해 설계된 특정 분산 최적화 기법(Bijral et al., 2016; Coppola, 2015; McDonald et al., 2010; Stich, 2018; Zhang et al., 2016; Zinkevich et al., 2010)이다. 지방-SGD의 핵심 원리는 각 작업자가 글로벌 동기화에 참여하기 전에 여러 지역 훈련 반복을 실행할 수 있도록 하는 것이다. 이 기술은 이후 연방 학습 설정에 적용되어 통신 오버헤드를 줄이는 것을 목표로 하는 FedAvg 방법(McMahan et al, 2017)의 개발로 이어졌다. 지역-SGD와 달리 피드백된 학습은 또한 사용자 프라이버시 문제를 해결하고 일반적으로 이질적인 장치를 포함한다. 통신 오버헤드를 더욱 최소화하기 위해 FedOpt는 SGD 운동량과 Adam(Reddi et al., 2020)과 같은 적응 최적화 방법을 통합한다. 그러나 고객/근로자의 이질성이 증가할수록 융합률이 저하되는 경우가 많다. SCAFFOLD(카리메디 et al., 2020) 및 MIME(카리메디 et al., 2021)와 같은 방법이 이질적인 환경에 대한 이러한 최적화 방법을 적응시키기 위해 도입되었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Number of workers \\(k\\) & 4 & 8 & 16 \\\\ \\hline Async. DN + DyLU (\\(c=0\\)) & **41.13** & 41.02 & **40.98** \\\\ Async. DN + DyLU (\\(c=0.1\\)) & 41.16 & **40.93** & 41.04 \\\\ \\hline Model size & 20M & 60M & 150M \\\\ \\hline Async. DN + DyLU (\\(c=0\\)) & **41.13** & **24.53** & **17.26** \\\\ Async. DN + DyLU (\\(c=0.1\\)) & 41.16 & 24.69 & 17.27 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 알고리즘 3에서 \\(c\\in\\{0,0.1\\}\\)를 변경하는 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model size & 20M & 60M & 150M \\\\ \\hline Pretrained (24K) & 61.64 & 30.19 & 22.80 \\\\ Finetune (4x batch size) & 42.47 & 24.80 & 17.47 \\\\ DiLoCo (Douillard et al., 2023) & 41.35 & 24.55 & **17.23** \\\\ \\hline Async. DiLoCo & 44.27 & 25.64 & 18.08 \\\\ Async. DN + DyLU (ours) & **41.13** & **24.53** & 17.26 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: | Varying 모형 크기를 나타낸다.\n' +
      '\n' +
      '학습 효율이 가장 느린 작업자에 의해 병목화되는 동기 분산 최적화(Dean et al, 2012, Diskin et al, 2021, Koh et al, 2006, Lian et al, 2015, 2018, Recht et al, 2011)에서 관찰된 "스트레글러 효과"를 완화하기 위해 동기 훈련 비동기 교육이 개발되었다. 비동기 최적화에서 중요한 과제는 단단한 구배 문제이며, 이는 최근에 업데이트된 모델에 오래된 기울기가 적용될 때 발생한다. 지연 보상(정 등 2017)이 있는 근사 동기 SGD로 이 문제를 해결한다.\n' +
      '\n' +
      '그림 11: 근로자 수를 변경하는 것이다.\n' +
      '\n' +
      '그림 10: 기기의 이질성을 가리는 것이다.\n' +
      '\n' +
      '오래된 구배를 사용하여 진정한 구배를 만들어 보세요. 비동기식 방법은 또한 연방 학습 맥락(Xie et al., 2019)에서 탐구되었다. 도전에도 불구하고 비동기 훈련은 전 세계적으로 이질적인 장치를 사용하여 언어 모델링(디스킨 등, 2021)에 대한 성공을 입증했다.\n' +
      '\n' +
      '언어 모델링을 위한 로컬-SGD(또는 FedAvg)의 개념은 언어 모델링 영역에서 이전에 적용되었다. 예를 들어, 교차 장치 공급 학습은 사전 및 미세 조정 언어 모델(Borzunov et al., 2022; Diskin et al., 2021; Hilmkl et al., 2021; 프레스러, 2020; Ro et al, 2022; Ryabinin et al., 2021)에 활용되었다. 보다 최근에, 디LoCo는 더 큰 언어 모델을 포함하도록 로컬-SGD 방법론을 확장했으며, 특히 인너캣 + 아웃커롭 페어링으로 아담W + 네스테로프 모멘텀의 사용을 제안한다. 비동기 환경에서 페드버프(Nguyen et al, 2022) 알고리즘은 고객의 의사 조사자를 버퍼링하여 충분한 수의 유사 조사자를 축적한 후에야 서버 모델을 업데이트한다. 타임리FL(장 et al., 2023)은 느린 디바이스가 모델의 일부만 훈련하도록 함으로써 비동기화를 줄이는 것을 목표로 한다.\n' +
      '\n' +
      '## 9 Limitations\n' +
      '\n' +
      '이 연구는 포괄적인 반면 몇 가지 한계를 가지고 있다. 먼저, OuterOpt에서 운동량 업데이트와 관련된 중요한 최적화 문제를 식별하지만 이 문제의 정확한 원인은 여전히 불분명하다. 강력한 이론적 백싱으로 이 문제를 이해하는 것은 향후 연구를 위한 흥미로운 방법을 제시한다. 둘째, 우리의 경험적 관찰은 지역-SGD 방법의 이점이 작업자가 증가함에 따라 감소한다는 것을 시사하며, 이는 근본적인 이유가 아직 이해되지 않은 현상이다. 이 문제는 현재 비동기 로컬-SGD의 확장성을 방해한다. 마지막으로 제안된 방법 DN+DyLU는 향상된 경험적 성능을 보여주지만 추가 조사가 가능한 측면인 형식적 이론적 융합 보증이 부족하다.\n' +
      '\n' +
      '## 10 Conclusion\n' +
      '\n' +
      '이 연구는 언어 모델링에서 비동기적 지방-SGD에 대한 철저한 검사를 제시한다.\n' +
      '\n' +
      '그림 12: 모델 크기를 변경합니다.\n' +
      '\n' +
      '우리의 중심 발견은 외부 최적화 루프의 운동량이 중요하지만 순진한 구현 시 동기 시나리오에 비해 비동기 시나리오에서 덜 효과적일 수 있다는 것이다. 이러한 격차를 해소하기 위해 지속적인 확률적 유사 방사선 업데이트와 결합된 완충된 유사 조사자를 사용한 산발적인 운동량 업데이트를 중심으로 새로운 접근법을 소개한다. 또한, 우리의 연구는 각 작업자의 계산 속도에 대한 지역 훈련 단계를 테일링하는 것이 간단할 뿐만 아니라 성능을 향상시키는 효율적인 전략임을 보여준다.\n' +
      '\n' +
      '하지만 해야 할 일이 많습니다. 표준("지역" 구배 하강 설정과 반대되는 것)에서 체중 업데이트의 수 측면에서 가능한 한 빨리 손실 감소 측면에서 최적의 배치 크기는 보통 "가능한 한 큰 것"이 아니다. 우리의 견해에서도 마찬가지로 동기적인 로컬-SGD보다 지역 업데이트당 더 나은 결과를 제공하는 비동기적인 로컬-SGD 방법에 대한 희망이 있다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '아담 피쉬의 소중한 피드백에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* S Bijral et al. (2016) 아비엔 S Bijral, Anand D Sarwate 및 Nathan Srebro. 분산 확률 최적화의 데이터 의존성에 대한 __ 분산 확률 최적화의 데이터 의존성에 대한 것이다. arXiv 프리프린트 arXiv:1603.04379_ 2016.\n' +
      '* 보즈노프(2022) 알렉산더 보즈노프, 다미트리 바반코프, 팀 디트머스, 맥스 리야바닌, 예네스 벨카다, 아르테미 차마첸코, 파벨 삼긴, 콜린 라펠 등이 있다. Petals: 거대 모델의 협업 추론 및 미세 조정: 큰 모델의 협력 추론 및 미세 조정. arXiv 프리프린트 라이브러리_, 2022.\n' +
      '* 코폴라(2015) 그레고리 프란치스코 쿠폴라. 자연어 처리를 위한 구조화된 예측 변수의 분산 대마진 교육을 위한 보정 파라미터 혼합이다. 2015년.\n' +
      '* 디안 등은 (2012) 제프리 데안, 그레그 코라도, 라자트 몽라, 카이 크헨, 마티누 데빈, 마크 마오, 마크리우리오 라나토, 앤드루 시니어, 폴 터커, 케양 등의 대규모 척도를 심층 네트워크를 배포했다. 신경 정보 처리 시스템_, 2012년 25.\n' +
      '마이클 디킨, 알렉시 북하바로프, 맥스 리아비닌, 루실 사야비에, 퀘스틴 로케스트, 퀀틴 시미틴, 드미트리 포코프, 드미트리 피르킨, 맥심 카슈노프, 알렉산더 보즈노프, 알베르트 빌라노바 델 모랄, 데니스 모비스, 일리아 고베레브, 야키 제나이트, 토마스 볼프, 제니 페크히메코, 데르노바 델 모랄레스, 데르바노바 델 모랄레스, 야키 페크헤르, 다트르노바 델 모랄레스, 데르나노바 델 모랄레스, 데나노바 델 모랄레스, 이바노바 델 모랄레스, 일리아 고베르, 야키 고베르, 야키 고베르, 야키 고베르, 야신 페르나이트, 토마스 볼프, 야키 페크헤르, 토마스 볼프, 게나디, 토마스 볼프, 게나디 펠크헤르, 게나디 페크헤르, 게나디 페크헤르, 게나디 개방형 협업에서 딥러닝을 분산시켰는데 __ 개방 협업에서 딥러닝을 분배하였다. 2021년 신경 정보 처리 시스템(NeurIPS)_, 2021a에서 발전한다.\n' +
      '* 디킨(2021b) 마이클 디킨, 알렉시 북히타로프, 맥스 로야바로프, 맥스 리아비닌, 루실 사울린, 안토니 시미틴, 드미트리 포코프, 드미트리 비 피르킨, 맥심 카시린, 알렉산더 보즈노프, 알버트 빌라노바 델 모랄 등 오픈 협업에서 딥 러닝을 확장했다. 신경 정보 처리 시스템_, 2021b 34:7879-7897의 발전입니다.\n' +
      '*두빌라드 등은 알(2023) 아르투르 두빌라드, 키수안 펑, 안드레이 아 러쿠, 라치타 치카리아, 야니 돈체프, 아디구나 쿠코로, 마크의 아우렐리오 라나주토, 아르투르 사슬라마, 지아준 심 등이다. Diloco: 언어 모델의 저통신 훈련을 확장했다. __ arXiv 프리프린트 arXiv:2311.08105_, 2023.\n' +
      '* 구 등은 (2023) 신란구, 카리펑 루유, 롱보 황, 산지프 아로라 등이 있다. 왜(및 언제) 지역 sgd가 sgd보다 더 잘 일반화하는가? arXiv 프리프린트 arXiv:2303.01215_, 2023.\n' +
      '* 힐킬로 등(2021) 아그린 힐 킬로, 세바스티안 칼, 마테오 바비에리, 레온 리네 수펠트, 에드빈 리스트 제크, 올로프 모그렌 등이 있다. 대형 언어 모델의 미세 조정에 대한 피드백 피드백 학습을 제공합니다. 정보화시스템_페이지에 대한 자연어의 적용에 관한 _국제회의에서 2021년 15-23쪽 스프링거.\n' +
      '(2021) 요르단 호프만, 엘비안 보르츠카야, 아르비제나 루터카우, 엘비즈 데 루터카스, 엘비즈 데 라스 카사, 리사 안네 헨드크, 요하네스 웰블, 원조 클라크, 톰 헤니간, 에릭 니란디, 카티 밀리칸, 조지 밴 데 데 데리체, 보단 다노, 아우렐리아 다노, 에렌 시노니, 에리히 엘센, 잭 Woff만, 엘비안 헨더만, 쿠르드 밀란, 코르드 밀란, 코르드 밀란, 코르드 밀란, 코르네 밀란, 코르네 밀란, 코르네 밀란, 코르네 밀란, 코르네 밀란, 코르네 밀란, 시런 다노, 시린지, 시노의 다노, 에렌 시노, 에렌 시노, 에레엘란, 에렌 시노만, 잭 Woff만, 잭 Woff만, 잭 Woff만, 잭 Woff만, 알(2021). 래, 오리올 비넬, 로랑 시프르.\n' +
      '\n' +
      '훈련은 계산하는 최적의 대형 언어 모델 __ 최적이 아닌 대형 언어 모델. 신경정보처리시스템(NeurIPS)_ 2022년 개발.\n' +
      '* Karimireddy et al. [2020] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International conference on machine learning_, pages 5132-5143. PMLR, 2020.\n' +
      '* Karimireddy et al. [2021] Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian U Stich, and Ananda Theertha Suresh. Breaking the centralized barrier for cross-device federated learning. _Advances in Neural Information Processing Systems_, 34:28663-28676, 2021.\n' +
      '* Koh et al. [2006] Byung-Il Koh, Alan D George, Raphael T Haftka, and Benjamin J Fregly. Parallel asynchronous particle swarm optimization. _International journal for numerical methods in engineering_, 67(4):578-595, 2006.\n' +
      '* Lian et al. [2015] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. _Advances in neural information processing systems_, 28, 2015.\n' +
      '* Lian et al. [2018] Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic gradient descent. In _International Conference on Machine Learning_, pages 3043-3052. PMLR, 2018.\n' +
      '* Lin et al. [2018] Tao Lin, Sebastian U Stich, Kumar Khitij Patel, and Martin Jaggi. Don\'t use large mini-batches, use local sgd. _arXiv preprint arXiv:1808.07217_, 2018.\n' +
      '* Lin et al. [2020] Tao Lin, Sebastian U. Stich, Kumar Khitij Patel, and Martin Jaggi. Don\'t use large mini-batches, use local sgd. _Proceedings of the International Conference on Learning Representations (ICLR)_, 2020.\n' +
      '* McDonald et al. [2010] Ryan McDonald, Keith Hall, and Gideon Mann. Distributed training strategies for the structured perceptron. In _Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics_, pages 456-464, 2010.\n' +
      '* McMahan et al. [2017] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.\n' +
      '* Nguyen et al. [2022] John Nguyen, Khitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Mike Rabbat, Mani Malek, and Dzmitry Huba. Federated learning with buffered asynchronous aggregation. In _International Conference on Artificial Intelligence and Statistics_, pages 3581-3607. PMLR, 2022.\n' +
      '* 프레스러[2020] 전당포 프레스러. 2020년도 URL[https://barm.com/swarm-트레이닝-v01a.pdf](https://battlen.com/swarm-트레이닝-v01a.pdf) 훈련.\n' +
      '* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 2020.\n' +
      '* Recht et al. [2011] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. _Advances in neural information processing systems_, 24, 2011.\n' +
      '* Reddi et al. [2020] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. _arXiv preprint arXiv:2003.00295_, 2020.\n' +
      '* Ro et al. [2022] Jae Hun Ro, Theresa Breiner, Lara McConnaughey, Mingqing Chen, Ananda Theertha Suresh, Shankar Kumar, and Rajiv Mathews. Scaling language model size in cross-device federated learning. _arXiv preprint arXiv:2204.09715_, 2022.\n' +
      '* Ryabinin et al. [2021] Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, and Gennady Pekhimenko. Moshpit sgd: Communication-efficient decentralized training on heterogeneous unreliable devices. _Advances in Neural Information Processing Systems_, 34:18195-18211, 2021.\n' +
      '\n' +
      '* 스타치(2018) 세바스티안 우 스티치. 로컬 sgd는 빠르게 수렴하고 거의 통신을 하지 않는다. __ 국소 sgd가 빠르게 수렴하여 거의 통신을 수행하지 않는다. arXiv 프리프린트 arXiv:1805.09767_ 2018.\n' +
      '* Xie et al. (2019) Cong Xie, 산미 기예조, Indranil Gupta. 비동기식 피회 최적화 __비동기식 최적화. arXiv 프리프린트 arXiv:1903.03934_ 2019.\n' +
      '* 장 등은 (2016) 지안 장, 크리스토퍼 데 사, 이오만니스 미틀리아가스, 크리스토퍼 레 등이 있다. 평행 시그드: 평균은 언제 도움이 됩니까? arXiv 프리프린트 arXiv:1606.07365_ 2016.\n' +
      '* 장 등은 (2023) 토오 장, 레이 가오, 선우 이, 미 장, 살만 아베레메메이지. 타임플: 적응 부분 훈련을 통해 Heterogeneity-인식 비동기식 피겨 학습을 수행했다. 컴퓨터 비전 및 패턴 인식_ 페이지 5063-5072, 2023에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '*정 등은(2017) 샤신정, 키멍, 타이펑왕, 위첸, 응이유, 지히밍마, 티야리무 등이 있다. 지연 보상과 함께 비동기 확률적 기울기가 하강한다. 기계학습_국제회의에서는 2017년 4120-4129쪽. PMLR.\n' +
      '*Zinkevich et al. (2010) 마르틴 Zinkevich, 마르커스 웨머, 리홍 리, 알렉스 스놀라. 평행화된 확률적 구배 하강 __평행화된 확률적 구배 하강. __평행화된 확률적 기울기 하강. 신경 정보 처리 시스템_, 23, 2010의 발전이다.\n' +
      '\n' +
      '## Supplementary Materials\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '네트워크 아키텍처는 20M, 60M, 150M 모델의 건축적 차이를 표 6에 나타냈다. 모두 차치닐라 계열(Hoffmann et al, 2022)을 기반으로 한 변압기 디코더 전용입니다.\n' +
      '\n' +
      '우리는 공통 크로슬(Raffel et al., 2020)에서 파생된 데이터세트인 C4 데이터셋에 대한 언어 모델링 작업을 고려한다. 총 단계 수는 모든 모델에 대해 88,000단계로 설정되며, 사전 훈련은 어떤 피드백된 학습 방법 없이 24,000단계, 즉 _포스트 로컬-SGD_(Lin et al., 2020)와 유사하다.\n' +
      '\n' +
      '표 5에서 우리는 이 연구를 위해 고려된 최적화 하이퍼모수들을 윤곽으로 설명한다.\n' +
      '\n' +
      'Douillard et al.(2023)에 이어 인너 옵티미이저 국가는 모든 실험에서 근로자 B가 데이터 샤드 작업자 A를 훈련 종료할 때 AdamW의 최적 상태를 재설정했다. 즉, 각 지역 근로자 측 훈련은 새로운 최적화를 가진 독립적인 훈련 과정이며, 3절에서 설명한 대로 학습률만을 조정한다.\n' +
      '\n' +
      '### Aync. Aocode\n' +
      '\n' +
      '이 절에서는 알고리즘 2에서 기차(O)와 get_worker(O) 기능에 대한 가블록을 제공한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Hyperparameter & 20M & 60M & 150M \\\\ \\hline Number of layers & 6 & 3 & 12 \\\\ Hidden dim & 256 & 896 & 896 \\\\ Number of heads & 4 & 16 & 16 \\\\ K/V size & 64 & 64 & 64 \\\\ Vocab size & & 32,000 & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '세 가지 평가된 크기에 대한 표 6: ** 모델 구성***이다. 모두 변압기 아키텍처인 키칠라 스타일(Hoffmann et al, 2022)을 기반으로 합니다.\n' +
      '\n' +
      '```\n' +
      '0: 이용 가능한 노동자 \\(\\mathcal{W}\\)\n' +
      '0: 전류 서버 모델 \\(\\ta\\)\n' +
      '1:for\\(w\\in\\mathcal{W}\\)do\n' +
      '2: 삼플 샤드 \\(\\mathcal{D}^{\\prime}\\) \\(Eq. 2)에 대해.\n' +
      '3:\\(w\\) 로컬_updates = DyLU(\\(\\mathcal{D}^{\\prime}\\)) (Eq. 6)이다.\n' +
      '4: 데미드 lr 일정(Eq. 3)\n' +
      '5:\\ (w\\) a\\ = 트레이스_워크러 (\\ (w\\), \\ (\\mathcal{D}^{\\prime}\\), \\ (\\theta\\)\n' +
      '6:endfor\n' +
      '```\n' +
      '\n' +
      '알고리즘 2에서**알고리즘 4** 열차 ()\n' +
      '\n' +
      '```\n' +
      '0: 근로자 \\(\\mathcal{W}\\)\n' +
      '0: 그레이스 기간 \\(\\tau_{\\text{grace}}\\)\n' +
      '0: 유예 기간 시작(\\tau_{\\text{sync}\\)\n' +
      '그런 다음 모든 일꾼이 하지 않을 때(성심증{W}\\)\n' +
      '2:return null\n' +
      '3:else\n' +
      '4:\\(w\\) = \\(\\mathcal{W}\\)에서 가장 빨리 작업자를 완료했다.\n' +
      '복잡한_시간 - \\(\\tau_{\\text{sync}\\leq\\tau_{\\text{sync}\\leq\\tau_{\\text{grace}}}}\\)then.5:if\\(w\\)\n' +
      '6:return\\(w\\)\n' +
      '7:else\n' +
      '8:return null\n' +
      '9:endif\n' +
      '10:endif\n' +
      '```\n' +
      '\n' +
      '알고리즘 2에서**알고리즘 5** get_worker()가 된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>