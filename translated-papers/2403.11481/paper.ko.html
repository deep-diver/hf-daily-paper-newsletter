<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# _VideoAgent_: 비디오 이해를 위한 메모리 증강 멀티모달 에이전트\n' +
      '\n' +
      'Yue Fan\n' +
      '\n' +
      '베이징 종합인공지능연구소 1국가핵심연구실\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Xiaojian Ma\n' +
      '\n' +
      '인용문은 기여도 1.1\n' +
      '\n' +
      'Rujie Wu\n' +
      '\n' +
      '베이징 종합인공지능연구소 1국가핵심연구실\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Yuntao Du\n' +
      '\n' +
      '베이징 종합인공지능연구소 1국가핵심연구실\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Jiaqi Li\n' +
      '\n' +
      '베이징 종합인공지능연구소 1국가핵심연구실\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Zhi Gao\n' +
      '\n' +
      '베이징 종합인공지능연구소 1국가핵심연구실\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Qing Li\n' +
      '\n' +
      '베이징 종합인공지능연구소 1국가핵심연구실\n' +
      '\n' +
      '1\n' +
      '\n' +
      '각주 1: 이메일: {maxiaojian,liqing}@bigai.ai\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '제안된 멀티모달 에이전트 _VideoAgent_:1)은 비디오의 일반적인 시간 이벤트 설명과 객체 중심 추적 상태를 모두 저장하기 위해 구조화된 메모리를 구성한다. 입력된 태스크 쿼리가 주어지면, LLMs. __의 제로 샷 툴 사용 능력을 활용하여 태스크를 대화식으로 해결하기 위해 다른 시각적 기초 모델과 함께 비디오 세그먼트 로컬화 및 객체 메모리 쿼리를 포함하는 툴을 사용한다. VideoAgent_는 여러 롱-호라이즌 비디오 이해 벤치마크에서 인상적인 성능을 보여주며, NExT-QA에서 평균 6.6%, 기준선 초과 에고스키마에서 평균 26.0% 증가하여 오픈 소스 모델과 제미니 1.5 프로를 포함한 개인 대응 간의 격차를 좁힌다.\n' +
      '\n' +
      '키워드: 비디오 이해 LLMs 툴-사용 멀티모달 에이전트\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '동영상에 대한 이해와 자유로운 형태의 질의에 대한 답변(질문 답변, 내용 검색, _etc._) 컴퓨터 비전과 인공지능의 주요 과제[1, 12, 22, 23, 26, 31, 44]가 남아있다. 특히 최근 많은 발전이 종단간 사전 훈련된 대형 변압기 모델에 의해 이루어졌으며, 특히 강력한 대형 언어 모델(LLM)[4, 11, 22, 31], _i.e._멀티모달 LLM에 의해 개발되었다. 그러나 풍부한 이벤트와 복잡한 시공간 종속성을 가진 긴 형식의 비디오를 처리할 수 있는 능력에 대한 우려가 증가하고 있다[7, 9, 16, 24, 33]. 특히, 긴 비디오를 처리할 때 계산, 특히 메모리 비용이 크게 증가하고 심지어 엄청나게 비싸질 수 있다[26, 32]. 또한, 자기 주의 메커니즘은 때때로 장거리 관계를 포착하는 데 어려움을 겪을 수 있다[25]. 이러한 문제는 정교한 기초 모델을 비디오 이해에 적용하는 데 추가 발전을 방해했다.\n' +
      '\n' +
      '최근, LLMs[3, 20]의 도구 사용 능력 덕분에, _멀티모달 에이전트_[6, 12, 23, 34]와 같은 새로운 종류의 멀티모달 이해 접근법이 빠르게 개발되었다. 핵심 아이디어는 여러 **tool** 기초 모델(객체 탐지, 시각적 질문 응답, _etc._)을 호출하여 LLM이 멀티모달 작업을 해결하도록 유도하는 것이다. 인터랙티브. 이러한 방법은 대부분 훈련 없이 도구 세트로 유연하기 때문에 큰 잠재력을 가지고 있다. 그러나 특히 긴 형식의 비디오에서 비디오 이해로 확장하는 것은 **사소하지 않은**입니다. 도구로서 단순히 비디오 기초 모델을 추가하는 것은 여전히 계산 비용과 주의 제한 문제를 겪을 수 있다[11, 22]. 다른 연구에서는 더 나은 도구[30, 37, 14]를 사용하여 보다 정교한 프롬프트 전략을 탐구했지만 일반적으로 복잡한 파이프라인으로 이어지며 이러한 방법의 성능은 비디오 특정 에이전트 설계의 부족으로 인해 엔드 투 엔드 대응물과 일치하지 않을 수 있다.\n' +
      '\n' +
      '본 논문에서는 비디오 이해 작업을 위한 간단하면서도 효과적인 LLM 기반의 멀티모달 도구 사용 에이전트 _VideoAgent_를 소개한다. 우리의 **key insight**는 비디오를 구조화된 통합 메모리로 표현하여 LLM의 강력한 시공간 추론 및 도구 사용을 촉진하고 엔드 투 엔드 모델을 매칭/수행하는 것이다. 우리의 메모리 디자인은 최소이지만 충분하다는 원칙에 의해 **동기 부여된다: 우리는 객체에 대한 전체 이벤트 컨텍스트 설명 및 시간적으로 일관된 세부 정보가 비디오에 대한 가장 빈번한 쿼리를 포함할 수 있음을 발견했다. 그 결과, 1) 전체 비디오에서 슬라이스된 각 짧은(2초) 비디오 세그먼트의 텍스트 설명을 저장하는 _temporal memory_와, 2) 비디오에서 객체와 사람의 발생을 추적하여 저장하는 _object memory_의 두 가지 메모리 구성 요소를 설계한다. 조회에 응답하려면,\n' +
      '\n' +
      '도 1: 비디오 QA 상의 _VideoAgent_와 종단간 비디오-언어 모델들 간의 비교. 비디오에 대한 구조화된 표현으로서 통합된 메모리가 없다면, 엔드 투 엔드 모델은 특히 객체 및 긴 비디오에 대해 질문될 때 기본적인 시공간 세부 사항을 캡처하는 데 어려움을 겪을 수 있다. _VideoAgent_는 큐레이션된 툴 세트를 활용하여 _temporal memory_(미도시) 및 _object memory_에 대한 정교한 질의를 수행하고, 정답으로 응답할 수 있다.\n' +
      '\n' +
      'LLM은 이를 여러 하위 작업으로 분해하고 도구 모델을 호출합니다. 통합 메모리는 두 질의 시간 단계 사이의 모든 이벤트 설명을 반환하는 _\\(\\mathscr{P}\\) 캡션 검색_; 주어진 텍스트 질의의 짧은 비디오 세그먼트를 시간 메모리 내의 이벤트 설명과 비교하여 검색하는 _\\(\\mathscr{L}\\) 세그먼트 로컬라이제이션_; 검색된 비디오 세그먼트가 주어진 질문에 답하는 _\\(\\mathscr{P}\\) 시각적 질문 응답_; SQL 질의를 사용하여 객체 메모리로부터 정교한 객체 상태 검색을 가능하게 하는 _\\(\\mathscr{P}\\) 객체 메모리 질의_로 구성된다. 마지막으로, LLM은 대화형 도구 사용의 응답을 집계하고 입력 질의에 대한 답변을 생성할 것이다.\n' +
      '\n' +
      '본 논문에서는 Ego4D NLQ[5]를 이용한 자유 형식 질의 현지화, NExT-QA[35]를 이용한 일반 동영상 질의 응답, EgoSchema[15]를 이용한 자기 중심적 질의 응답을 포함한 여러 동영상 이해 태스크에 대한 _VideoAgent_의 광범위한 평가를 수행한다. 우리는 표준 종단간 비디오 언어 기반 모델과 다른 멀티모달 에이전트와 _VideoAgent_를 비교한다. 결과는 기준선보다 NExT-QA에서 평균 6.6%, EgoSchema에서 26.0% 증가하는 _VideoAgent_의 장점을 보여준다. 우리의 추가 조사는 통합된 기억과 도구 선택이 수행하는 역할을 조사했다.\n' +
      '\n' +
      '요약하자면, 우리의 기여는 다음과 같다:\n' +
      '\n' +
      '* 세그먼트-레벨 디스크립션들을 저장하는 _temporal memory_와 비디오에 나타나는 오브젝트들의 상태를 추적하는 _object memory_를 포함하는 긴-폼 비디오들에 대한 구조화된 표현들을 구축하기 위한 통합 메모리 메커니즘을 제안한다.\n' +
      '* 통합 메모리를 기반으로 영상 이해를 위한 LLM 구동 멀티모달 에이전트인 _VideoAgent_를 설계한다. 입력된 태스크 질의를 분해하고, 최종 응답에 도달할 때까지 메모리에서 정보를 검색하기 위해 도구를 대화식으로 호출한다.\n' +
      '* 엔드 투 엔드 비디오 언어 모델과 멀티모달 에이전트 베이스라인 모두에 대해 다중 비디오 이해 벤치마크에서 _VideoAgent_의 철저한 평가를 수행하여 _VideoAgent_의 유효성을 입증한다. 추가적인 질적 및 절제 분석은 우리가 내린 중요한 디자인 선택을 추가로 확인시켜준다.\n' +
      '\n' +
      '## 2 VideoAgent\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      '본 논문에서는 부록 0.D.5에서 제안된 _VideoAgent_를 설명한다. 이는 입력된 비디오를 통합 표현으로 변환하는 것으로 시작된다: _temporal memory_ (Sec. 2.2)와 _object memory_ (Sec. 2.3). 모든 들어오는 작업에 대해, 그것은 메모리 및 원시 비디오 세그먼트로부터 정보를 수집하기 위한 툴들을 대화식으로 호출하고, 궁극적으로 응답을 생성한다(Sec. 2.4). 메모리 구축과 과제 해결(추론) 절차는 각각 알고리즘 1과 알고리즘 2에 요약되어 있다.\n' +
      '\n' +
      '### 시간기억 \\(\\mathcal{M}_{T}\\)\n' +
      '\n' +
      '시간 메모리는 비디오들의 전반적인 이벤트 컨텍스트 설명들 및 특징들을 저장하도록 설계된다. 비디오에서 슬라이스된 \\(n\\) 비디오 세그먼트 \\([v_{1},\\dots,v_{n}]\\)이 주어지면, 비디오 세그먼트 캡션 \\(s_{\\text{caption}}\\), 비디오 세그먼트 특징 \\(e_{\\text{video}}\\) 및 캡션 텍스트 임베딩 \\(e_{\\text{caption}}\\):\n' +
      '\n' +
      '**비디오 세그먼트 캡션.** 우리는 각 비디오 세그먼트에 대한 캡션을 생성하기 위해 LaViLa[45]라는 사전 훈련된 비디오 캡션 모델을 사용한다. 구체적으로, 짧은 자막 문장을 생성하기 위해서는 2초 세그먼트부터 4프레임이 소요된다. 전형적인 LaViLa 캡션은 "#C C 절단기로 나무를 자른다" 및 "#O 남자 Y는 왼손으로 길에서 유모차를 밀어낸다"일 수 있으며, 여기서 "#C" 및 "#O"는 캡션 문장이 카메라 착용자에 관한 것인지 또는 카메라 착용자 이외의 사람에 관한 것인지를 나타내기 위해 사용되며, 따라서 자기 중심적 및 일반 비디오 모두에서 효과적인 LaViLa 캡션을 만든다.\n' +
      '\n' +
      '**비디오 세그먼트 특징 및 캡션 특징.** 비디오 세그먼트 특징을 얻기 위해, 비디오 세그먼트를 인코딩하기 위해 ViCLIP[28]의 비디오 인코더를 채택한다. 우리는 ViCLIP에 대한 입력으로 각 비디오 세그먼트에서 10개의 프레임을 균일하게 샘플링하고 세그먼트의 생성된 특징을 저장한다. 캡션 기능은\n' +
      '\n' +
      '도 2: _VideoAgent_의 개요. 좌측: 먼저 입력 영상을 시간 메모리와 객체 메모리라는 구조화된 표현으로 변환한다; 우측: _VideoAgent_ 내의 LLM은 대화식으로 도구(\\(\\mathscr{E}\\)\\(\\mathscr{E}\\)\\(\\mathscr{E}\\)\\(\\mathscr{E}\\)\\(\\mathscr{E}\\)을 호출하여 주어진 작업을 해결하도록 유도한다. 제안된 도구들은 주로 메모리(예:_e.g._\\(\\mathscr{E}\\)와 시간메모리의 자막부분과 상호작용하는 반면에 객체메모리는 찾는다.\n' +
      '\n' +
      'LaViLa에서 얻은 비디오 세그먼트 캡션의 임베딩을 계산하기 위해 OpenAI에서 제공하는 텍스트 임베딩-3-large4이다.\n' +
      '\n' +
      '각주 4: [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)\n' +
      '\n' +
      '### Object Memory \\(\\mathcal{M}_{O}\\)\n' +
      '\n' +
      '시간 메모리에 저장된 일반적인 비디오 이벤트 컨텍스트 외에도, 시간적으로 일관된 세부사항들: _e.g. 사람, 객체 및 주변 환경의 존재, _etc._ 직관은 비디오에 대한 대부분의 질의는 객체(사람) 관련이므로 객체(사람)의 발생을 추적하여 _object memory_에 저장한다. 구체적으로, 객체 메모리는 객체 시각적 특징을 고유한 객체 식별자와 연결하는 특징 테이블과 비디오 전반에 걸쳐 객체(사람) 발생 정보를 저장하는 SQL 데이터베이스를 구성한다. 구성에 대한 자세한 내용은 아래에서 확인할 수 있다.\n' +
      '\n' +
      '**추적 및 재식별.** 우리의 객체 메모리 구축 파이프라인의 핵심은 비디오 전체에 걸쳐 모든 객체를 추적하고 있으며, 객체 중복을 제거하기 위해 이전에 나타난 객체를 재식별(re-ID)하는 것이다. 객체 발견 및 추적 부분을 위해 다중 객체 추적기 ByteTrack[43]와 함께 객체 탐지 모델 RT-DETR[13]을 파이프라인한다. 이 조합은 각각의 비디오 프레임에서 추적된 객체들의 추적 ID들, 카테고리들 및 바운딩 박스들을 생성한다. 재-ID 부분의 핵심 아이디어는 먼저 발견되고 추적된 모든 객체의 특징을 계산한 다음 특징 유사성에 따라 그룹화하는 것이다. 이를 통해 각 객체에 대한 고유한 객체 ID를 얻을 수 있습니다. 보다 구체적으로, 객체가 나타나는 10개의 랜덤 샘플링된 프레임으로부터 크롭된 객체 이미지에 객체 특징이 생성되며; 또한 최근 연구 [27]을 따라 객체의 앙상블을 사용한다.\n' +
      '\n' +
      '그림 3: 객체 추적 및 재-ID의 시각화. 동영상으로부터 6개의 프레임이 순서대로 표시된다. 컵(연두색 상자)과 우유병(분홍색 상자)은 다른 자세에서 성공적으로 재식별된다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '```\n' +
      '입력: video \\(V\\), video captioning model video_cap(\\(\\cdot\\)), video embedding model video_emb(\\(\\cdot\\)), text embedding model text_emb(\\(\\cdot\\)), video object tracker with re-identification object_track_reid(\\(\\cdot\\)) 출력: temporal memory \\(\\mathcal{M}_{T}\\), object memory \\(\\mathcal{M}_{O}\\)\n' +
      '1 Initize \\(\\mathcal{M}_{T}=\\varnothing\\), \\(\\mathcal{M}_{O}=\\varnothing\\);\n' +
      '2 비디오를 \\(n\\)개의 짧은 세그먼트 \\(V=[v_{1},v_{2},...v_{n}]\\)(각 세그먼트는 대략 2초에 걸쳐 있음);\n' +
      '3for\\(v_{i}\\ in \\([v_{1},v_{2},...v_{n}]\\)do\n' +
      '4\\(s_{\\text{caption}}\\leftarrow\\texttt{video\\_cap}(v_{i})\\);\n' +
      '5\\(e_{\\text{video}}\\leftarrow\\texttt{video\\_emb}(v_{i})\\);\n' +
      '6\\(e_{\\text{text}}\\leftarrow\\texttt{text\\_emb}(s_{\\text{caption}})\\);\n' +
      '7\\(\\mathcal{M}_{T}=\\mathcal{M}_{T}+(s_{\\text{caption}},e_{\\text{video}},e_{ \\text{text}})\\)\n' +
      '8results \\(\\leftarrow\\texttt{object\\_track\\_reid}(V)\\);\n' +
      '9for\\(S\\) in results do\n' +
      '10\\(s_{\\text{id}},s_{\\text{category},\\{I_{1},\\cdots I_{k}\\}\\gets S\\)//객체 ID, 객체 카테고리, 객체 \\(o_{i}\\)가 나타나는 모든 \\(k\\) 비디오 세그먼트(\\(I\\)는 비디오 세그먼트 인덱스를 나타냄);\n' +
      '11\\(e_{text{object}}^{(i)}\\leftarrow\\frac{1}{k}\\sum_{j=1}^{k}e_{o_{j}}^{\\text{ CLIP}}\\)//Sec. 2.3 참조;\n' +
      '12\\(\\mathcal{M}_{O}=\\mathcal{M}_{O}+(s_{\\text{id},s_{\\text{category},e_{\\text{ object}}^{(i)},\\{I_{1},\\cdots I_{t}\\});\n' +
      '13 return \\(\\mathcal{M}_{T}\\), \\(\\mathcal{M}_{O}\\);\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1**_VideoAgent_의 메모리 구성\n' +
      '\n' +
      '######4.2.2\\(\\mathscr{F}\\) Visual question answering\n' +
      '\n' +
      '목적은 짧은 비디오 세그먼트에 대한 주어진 질문(s_{\\text{question}}\\)에 시간적으로 답하는 것이다. 즉, 시간 메모리나 객체 메모리에서 캡션에 의해 가려지지 않는 추가 정보를 수집할 수 있다. 구체적으로, Visual_question_answering(\\(\\cdot\\))을 호출할 때 Video-LLaVA[11]를 실행한다.\n' +
      '\n' +
      'Object memory querying.** object memory \\(\\mathscr{F}\\)** object memory \\(\\mathcal{M}_{O}\\)에서 비디오에 등장하는 객체들에 대한 정교한 정보 검색을 수행하는 것을 목표로 한다. 구체적으로, 도구 객체_memory_querying(\\(\\cdot\\))을 텍스트 질의(s_{\\text{query}}\\)로 호출할 때(\\(e.g.\\) "냉장고에서 빨강 컵을 몇 개 꺼냈는가?"), 질의(\\(e.g.\\) "빨강 컵")에서 관련 객체 설명을 먼저 추출하고, 다음으로 설명문의 텍스트 특징과 \\(\\mathcal{M}_{O}\\)의 특징 테이블에서 객체 특징을 비교하여 해당 객체 ID를 획득하고, 마지막으로 LLM은 \\(s_{\\text{query}}\\)과 검색된 객체 ID를 기반으로 SQL 코드를 작성하여 데이터베이스에 질의하고 필요한 정보(e.g.\\)를 획득한다. LLM에 의해 추가로 처리된 후, \\(s_{\\text{query}}\\)에 대한 응답이 반환될 것이다.\n' +
      '\n' +
      '_VideoAgent_의 추론 절차는 다소 간단하다. 입력 질의로 초기화된 이력 버퍼 \\(q\\)를 시작으로 _VideoAgent_는 어떤 도구를 사용할지를 결정하고, 생성된 인수로 툴을 호출하고, 그 결과를 버퍼에 추가한 후 중지하거나 최대 단계 수에 도달할 때까지 반복한다. 마지막으로, 이력 버퍼 내의 콘텐츠에 기초하여 응답이 이루어질 것이다. 우리는 그림 4에서 이 절차의 예를 제공한다. _VideoAgent_는 GPT-4를 주요 LLM으로 하는 LangChain5를 사용하여 구현된다.\n' +
      '\n' +
      '각주 5: [https://www.langchain.com/](https://www.langchain.com/)\n' +
      '\n' +
      '##3 능력 및 분석\n' +
      '\n' +
      '본 논문에서는 EgoSchema(Sec. 3.1), Ego4D Natural Language Query(Sec. 3.2), NExT-QA(Sec. 3.3)를 포함한 다양한 롱폼 비디오 이해 벤치마크에 대해 _VideoAgent_를 평가하며, 성능을 최신 엔드 투 엔드 비디오 언어 모델 및 멀티모달 에이전트와 비교한다.\n' +
      '\n' +
      '### EgoSchema\n' +
      '\n' +
      '**개요.** 이고스키마 [15]는 긴 형식의 비디오에 대한 복잡한 질문에 초점을 맞춘 데이터셋이다. 3분짜리 비디오와 질문이 주어졌을 때, 모델은 5가지 가능한 옵션 중에서 진정한 답을 선택해야 하며 평가 메트릭은 모든 질문에 대한 정확성 \\(acc\\)이다. EgoSchema의 질문은 일반적으로 "방에서의 일반적인 활동과 다른 캐릭터와 그들의 행동이 이 환경에 어떻게 기여하는지 묘사"와 같은 비디오 수준 추론을 포함하며, 기존의 비디오 이해 모델에 큰 도전을 제기한다. 전체 EgoSchema 테스트 세트는 약 5000개의 질문을 포함하고, EgoSchema의 공식 서브세트는 500개의 질문을 포함한다. 비용을 고려하여, _VideoAgent_는 최신 방법 SeViLA[39], Video-LLaVA[11], mPLUG-Owl[38] 및 ViperGPT[23]와 비교하여 500-질문 서브셋 상에서 테스트된다. EgoSchema의 전체 세트에서 FrozenBiLM[36], VIOLET[2], mPLUG-Owl[38] 및 InternVideo[29]의 성능은 기술 보고서 6에 제공된 Gimini 1.5 Pro의 성능과 함께 벤치마크[15]에 의해 보고된다.\n' +
      '\n' +
      '각주 6: [https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c} \\hline \\hline \\multicolumn{6}{c}{**EgoSchema (full set)**} \\\\ \\hline\n' +
      '**Method** & FrozenBiLM & VIOLET & mPLUG-Owl & InternVideo & Gemini 1.5 Pro \\\\ \\hline \\(Acc\\) & 26.9 & 19.9 & 30.2 & 32.0 & **63.2** \\\\ \\hline \\multicolumn{6}{c}{**EgoSchema (subset, 500 questions)**} \\\\ \\hline\n' +
      '**Method** & SeViLA & Video-LLaVA & mPLUG-Owl & ViperGPT & _VideoAgent_ \\\\ \\hline \\(Acc\\) & 25.8 & 36.8 & 33.8 & 15.8 & **62.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: EgoSchema 데이터셋에 대한 결과. 맨 위 행: 전체 EgoSchema 테스트 세트의 결과; 맨 아래 행: 평가 비용으로 인한 EgoSchema 500 서브세트의 결과.\n' +
      '\n' +
      '도 4: _VideoAgent_ 추론의 일례. 질문이 주어지면 _VideoAgent_는 답에 도달할 때까지 여러 개의 도구 사용 단계를 실행한다. 각 단계의 노란색, 빨간색 및 파란색 블록은 생각의 사슬, 취해야 할 조치 및 도구 사용의 결과를 나타낸다.\n' +
      '\n' +
      '**주요 결과** 탭 도 1은 EgoSchema에 대한 다양한 모델의 성능을 나타낸다. _ VideoAgent_는 SeViLA 및 Video-LLaVA와 같은 다른 최첨단 비디오 이해 모델을 거의 30%로 크게 능가하여 500개 질문에서 62.8의 정확도를 달성했으며 전체 세트에서 제미니 1.5 Pro의 성능에 근접했다. Egoschema에 대한 _VideoAgent_의 강력한 성능은 제안된 메모리 기반 멀티모달 도구 사용 에이전트가 종단간 VLM 및 에이전트 대응보다 긴 형식의 비디오에서 복잡한 비디오 작업을 더 잘 해결할 수 있음을 증명한다. 이에 대한 우리의 통찰력은 아래 두 가지 점에서 찾을 수 있다.\n' +
      '\n' +
      '**통합된 기억은 더 강한 추론을 용이하게 한다.** 이고스키마의 질문은 긴 비디오에 대한 근본적인 추론 측면에서 다소 복잡하다. 따라서 강한 시공간 추론이 필수적이다. 비디오 언어 모델(Video-LLaVA, _etc._)과 같은 표준 접근 방식 또는 대응 멀티모달 에이전트(ViperGPT)는 비디오에 대한 구조화된 표현으로서 통일된 메모리가 부족하다는 공통점이 있다. 이러한 표현이 없으면 추론은 암시적이거나 사용 가능한 도구(ViperGPT에서와 같이)에 의해 상당히 제한되어야 하며, 이는 우리보다 더 나쁜 성능을 초래한다.\n' +
      '\n' +
      '** 유연한 도구 사용을 통한 전체적 비디오 이해** "c의 행동은 비디오 전체에서 어떻게 진화했으며 작업과의 참여 단계는 식별할 수 있습니까?"와 같은 일반적인 질문을 감안할 때 질문에서 설명 텍스트를 도출하고 비디오 접지에 사용하는 것은 어렵다. 그러나, \\(\\mathscr{E}\\) segment_localization과는 별개로, _VideoAgent_는 또한 \\(\\mathscr{E}\\) caption_retrieval을 사용하여 비디오의 주요 컨텍스트를 포착하고 어떤 세그먼트가 중요한지를 결정할 수 있으므로, 이 장애물을 해결할 수 있다.\n' +
      '\n' +
      '에고4D 자연어 질의\n' +
      '\n' +
      '**개요.** Ego4D Natal Language Query[5]의 태스크는 질의에 가장 잘 응답할 수 있는 비디오(평균 9분)에서 시간적 윈도우(평균 9초)를 찾는 것이다. 모델은 \\(k\\) 후보 윈도우를 검색할 수 있도록 하고, 각 후보 윈도우와 지상진실 윈도우 사이의 교차점 오버 유니언(\\(IoU\\))을 계산한다. 메트릭 \\(Rk@t\\)은 \\(k\\) 후보 중 적어도 하나가 \\(t\\)보다 큰 \\(IoU\\)을 달성하는 예측의 비율을 평가하는 데 사용된다. 감독된 방법 중 2D-TAN[42]과 VSLNet[41]은 벤치마크에서 제공하는 두 가지 기준선이며, GroundNLQ[8]은 Ego4D NLQ 챌린지 2023에서 1위를 차지했다. _VideoAgent_는 1) ViCLIP 시각적 특징만을 사용하여 \\(\\mathscr{E}\\) segment_localization tool의 다른 변형으로 제로샷을 평가한다; 2) LaViLa 캡션 또는 Ego4D Ground-truth narrations에 기초한 텍스트 특징; 3) 텍스트 특징과 시각적 특징(LaViLa+ViCLIP, Ego4D+ViCLIP)의 조합이다. 검색된 세그먼트는 접지-진실 윈도우로 \\(IoU\\)을 계산하기 위해 양 끝에 3초로 패딩된다. LaViLa+ViCLIP와 Ego4D+ViCLIP의 경우, 학습 집합에서 격자 탐색에 의해 비디오 텍스트와 텍스트 텍스트 유사도의 앙상블 가중치가 각각 \\(18:11\\)과 \\(7:8\\)으로 나타났다.\n' +
      '\n' +
      '**주요 결과** 탭 2는 Ego4D NLQ의 검증 세트에 대한 결과를 제시한다. 텍스트적 특징과 시각적 특징의 조합은 더 나은 비디오 접지에서 결과를 가져온다고 추론할 수 있다. Supervised GroundNLQ와 성능 차이가 있지만, _VideoAgent_는 2D-TAN과 VSLNet을 능가하며, 단순한 구조와 Zero-shot 특성을 고려하여 좋은 성능을 보인다.\n' +
      '\n' +
      '** 캡션 기능 대. 시각적 특징.** 탭에서 ViCLIP, LaViLa 및 Ego4D 간의 비교. 도 2를 참조하면, 비디오-쿼리 유사도를 이용하는 것보다 자막-쿼리 유사도를 비디오 접지에 이용하는 것이 더 효과적임을 유추할 수 있다. 더 높은 품질의 캡션(LaViLa\\(\\rightarrow\\)Ego4D)은 또한 캡션 기반 비디오 접지 성능을 향상시킬 것이다.\n' +
      '\n' +
      '**유사성 기반 대. LLM 기반 현지화.** 탭. 도 3은 _VideoAgent_와 LifeLongMemory[30]의 비교를 제시한다. 쿼리가 주어지면, LifeLongMemory는 LLM(GPT-4)을 사용하여 비디오 세그먼트들의 캡션들을 소화 및 정제하고, LLM에 의해 선택된 캡션들에 기초하여 후보 윈도우들의 리스트를 쿼리에 출력하며, _i.e._ 세그먼트 로컬리제이션은 LLM에 의해 완전히 행해진다. 라이프롱 메모리는 LLM 생성 후보들 중 적어도 하나가 \\(0.3\\)보다 큰 \\(IoU\\)을 달성하는 예측의 비율을 계산하기 위해 표준 \\(R1@0.3\\) 및 \\(R5@0.3\\) 대신 맞춤형 \\(R@0.3\\) 메트릭을 채택한다. Tab에서 유추할 수 있다. 동일한 캡션 타입(Ego4D 또는 LaViLa)이 주어진 3과,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c} \\hline \\hline \\multicolumn{5}{c}{**EGO4D NLQ Val.**} \\\\ \\hline\n' +
      '**Method** & \\(R1@0.3\\) & \\(R1@0.5\\) & \\(R5@0.3\\) & \\(R5@0.5\\) \\\\ \\hline \\multicolumn{5}{c}{**Supervised**} \\\\ \\hline\n' +
      '2D-TAN & 5.04 & 2.02 & 12.89 & 5.88 \\\\ VSLNet & 5.45 & 3.12 & 10.74 & 6.63 \\\\ GroundNLQ & **27.20** & **18.91** & **54.42** & **39.98** \\\\ \\hline\n' +
      '**Zero-Shot (_VideoAgent_ with \\(\\mathscr{E}\\) segment\\_localization variants)** & \\(\\mathscr{E}\\) segment\\_localization variants) & \\\\ \\hline ViCLIP & 8.40 & 3.97 & 17.36 & 8.50 \\\\ LaViLa & 10.07 & 4.19 & 22.53 & 10.58 \\\\ Ego4D & 16.41 & 6.96 & 31.96 & 15.01 \\\\ LaViLa+ViCLIP & 11.13 & 4.76 & 25.31 & 12.08 \\\\ Ego4D+ViCLIP & **17.39** & **7.47** & **33.05** & **15.73** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: Ego4D NLQ 검증 세트 상의 상이한 툴 구현 변형들을 갖는 감독된 베이스라인들과 _VideoAgent_ 간의 비교.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c} \\hline \\hline\n' +
      '**Method** & \\(R1@0.3\\) & \\(R5@0.3\\) & \\(R@0.3\\) \\\\ \\hline LifeLongMemory(Ego4D) & * & * & 15.99 \\\\ LifeLongMemory(LaViLa) & * & * & 9.74 \\\\ _VideoAgent_ (Ego4D) & **16.41** & **31.96** & - \\\\ _VideoAgent_ (LaViLa) & 10.07 & 22.53 & - \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: Ego4D NLQ 상의 _VideoAgent_와 LifeLongMemory[30]의 두 제로샷 접근법 간의 비교. *LifelongMemory는 맞춤형 메트릭(_R@0.3_)으로 결과만 보고하지만 표준 메트릭 _R1@0.3_ 및 _R5@0.3_ 상의 숫자는 보고되지 않았지만 _R@0.3_ 상의 숫자보다 작거나 같아야 한다.\n' +
      '\n' +
      '질의에 대해 오직 1개의 후보만이 허용되는 _VideoAgent_ on \\(R1@0.3\\)의 성능은 이미 LifeLongMemory on \\(R@0.3\\)의 성능을 능가했다. 질의에 대한 5개의 후보들을 제공함으로써, _VideoAgent_의 성능은 LifeLongMemory를 2배 이상 초과할 것이다. 이는 LLM 기반 방법보다 유사성 기반 세그먼트 국소화가 더 효과적임을 나타낸다.\n' +
      '\n' +
      '### NExT-QA\n' +
      '\n' +
      '**개요.** NExT-QA[35]는 시간적, 인과적 및 서술적 질문을 포함하는 비디오 질문 답변 벤치마크이다. 각 질문에 대해 모델은 5가지 옵션 중 올바른 옵션을 선택해야 합니다. 각 유형의 질문에 대해 정확성 \\(acc\\)을 계산한다. 유효성 검사 세트에는 총 약 5000개의 질문이 포함되어 있습니다. 비용적인 이유로, 우리는 각 유형에 대해 200개의 질문을 무작위로 샘플링하고 총 600개의 질문의 하위 집합을 얻어 _VideoAgent_의 성능을 테스트했다. 이 서브세트 상에서 _VideoAgent_와 직접 비교되는 방법들은 ViperGPT[23], mPLUG-Owl[38], Video-LLaVA[11] 및 SeViLA[39]를 포함한다. 전체 유효성 검사 세트에 대한 세 가지 대표적인 방법 InternVideo[29], SeViLA[39] 및 TCR[9]의 결과도 제공된다.\n' +
      '\n' +
      '**주요 결과** 탭 도 4는 NExT-QA에 대한 주요 결과를 나타낸다. 전체적으로, _VideoAgent_는 모든 비교 방법 중에서 가장 강한 성능을 달성한다. 특히, 강한 시간적 이해와 추론을 필요로 하는 어려운 인과적 질문에 대해, _VideoAgent_는 NExT-QA에서 최신 모델 중 하나인 SeViLA보다 거의 10% 더 우수하다. 또한, \\(\\not\\) video_question_answering tool에 의해 사용되는 _VideoAgent_와 Video-LLaVA의 비교를 통해, 본 논문에서 제안한 _VideoAgent_가 멀티모달 도구 사용 에이전트의 일부로서 이러한 비디오 언어 모델이 단독으로 사용되는 것보다 더 잘 동작함을 알 수 있다.\n' +
      '\n' +
      '**절제 연구를 위한 설정.** 600-질문 서브세트에서 각 질문 유형에 대해 50개의 질문을 추출하여 총 150개의 질문의 서브세트를 생성하여 _VideoAgent_에서 상이한 컴포넌트의 기여도를 절제로서 평가한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c} \\hline \\hline  & \\multicolumn{4}{c}{**NExT-QA**} \\\\ \\hline\n' +
      '**Method** & **Temporal** & **Causal** & **Descriptive** & **Average** \\\\ \\hline \\multicolumn{4}{c}{**Val. Set**} \\\\ \\hline InternVideo & 43.4 & 48.0 & 65.1 & 49.1 \\\\ SeViLA(zero-shot) & **61.3** & **61.5** & **75.6** & 63.6 \\\\ TCR(pre-training) & - & - & - & **66.1** \\\\ \\hline \\multicolumn{4}{c}{**Val. Subset (600)**} \\\\ \\hline ViperGPT & 17.2 & 19.0 & 26.7 & 21.0 \\\\ mPLUG-Owl & 36.0 & 41.0 & 52.5 & 43.2 \\\\ Video-LLaVA & 42.0 & 53.5 & 65.0 & 53.5 \\\\ SeViLA(zero-shot) & 56.0 & 66.5 & 70.0 & 64.2 \\\\ _VideoAgent_ & **60.0** & **76.0** & **76.5** & **70.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: NExT-QA에 대한 결과. 우리는 평가 비용으로 인해 원래 전체 집합과 부분 집합(600개 질문) 모두에 대한 기준선을 기준으로 비교한다.\n' +
      '\n' +
      '공부. 탭 도 5는 tab에서 \'VQA\', \'Grounding\', \'Captions\', \'Database\'로 표기된 \\(\\not\\)\\(\\mathtt{visual\\_question\\_answering}\\), \\(\\not\\)\\(\\mathtt{segment\\_localization}\\), \\(\\not\\)\\(\\mathtt{caption\\_retrieval}\\), \\(\\not\\)\\(\\mathtt{object\\_memory\\_querying}\\) 중에서 각각 고유한 도구 세트를 장착한 _VideoAgent_의 6개 삭제의 성능을 보여준다.\n' +
      '\n' +
      '** 캡션 검색의 필요성** \\(\\not\\)\\(\\mathtt{caption\\_retrieval}\\) 툴은 비디오의 주요 문맥에 대한 기본적인 정보를 제공하기 때문에 _VideoAgent_의 기초를 제공한다. 6형의 _VideoAgent_는 \\(\\not\\)\\(\\mathtt{caption\\_retrieval}\\)만으로 평균 40.7의 결과를 얻었으며, 이는 600 질의 서브셋에서 종단간 비디오 언어 모델 mPLUG-Owl의 성능 43.2와 비교된다.\n' +
      '\n' +
      '**객체 메모리는 모든 질문 유형을 부스팅한다.** 유형 2와 3 사이의 비교는 신뢰할 수 있는 객체 메모리가 비디오 세그먼트에 걸쳐 중요한 시간적으로 일관된 객체 정보를 제공하고, 객체 관련 시간적 국소화를 용이하게 하며, 비디오에 대한 에이전트의 이해를 향상시키기 때문에 시간적 및 인과적 질문에 실질적으로 도움이 될 수 있음을 나타낸다. 유형 4와 유형 5 사이의 성능 격차는 객체 re-ID 알고리즘으로 기술 질문(대부분 양)에 대한 성능이 크게 향상되어 객체 re-ID의 유효성을 검증할 것임을 시사한다.\n' +
      '\n' +
      'VQA와 Segment localization이 가장 많은 보너스를 제공한다. 3과 6을 비교해보면, \\(\\not\\)\\(\\mathtt{visual\\_question\\_answering}\\)과 \\(\\not\\)\\(\\not\\)\\(\\mathtt{segment\\_localization}\\)을 동시에 추가하는 것이 객체 메모리(유형 4와 6에서 추론)를 추가하는 것에 비해 캡션 전용 _VideoAgent_를 평균 22% 증가시키는 것을 알 수 있다. 또한 비디오-LLaVA에서 GPT-4V로 \\(\\not\\)\\(\\mathtt{visual\\_question\\_answering}\\) (유형 1과 2)로 전환함으로써 성능이 3.4% 향상되어 강력한 VQA 모델에 의해 식별된 정확한 시각적 세부 정보가 더 나은 질의 응답 성능에 도움이 될 것임을 나타낸다.\n' +
      '\n' +
      '##4 관련 업무\n' +
      '\n' +
      '동영상 이해를 위한### 멀티모달 LLM\n' +
      '\n' +
      'LLM이 자연어를 처리하고 이해하는 뛰어난 능력을 입증했기 때문에[17, 4], 최근 몇 가지 작업이 이를 확장하기 위해 탐구했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c|c c c} \\hline \\hline\n' +
      '**Type** & **VQA** & **Grounding** & **Captions** & **Database** & **Tem.** & **Cau.** & **Des.** & **Avg.** \\\\ \\hline\n' +
      '1 & GPT-4V & ✓ & ✓ & w/re-ID & 64.0 & 78.0 & 82.0 & 74.7\\\\\n' +
      '2 & Video-LLaVA & ✓ & ✓ & w/re-ID & 60.0 & 74.0 & 80.0 & 71.3\\\\\n' +
      '3 & Video-LLaVA & ✓ & ✓ & ✗ & 46.0 & 64.0 & 78.0 & 62.7\\\\\n' +
      '4 & ✗ & ✗ & ✓ & w/re/ID & 48.0 & 52.0 & 68.0 & 56.0 \\\\\n' +
      '5 & ✗ & ✓ & w/o re-ID & 46.0 & 46.0 & 54.0 & 48.7\\\\\n' +
      '6 & ✗ & ✗ & ✓ & ✗ & 34.0 & 46.0 & 42.0 & 40.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: NExT-QA 서브세트에 대한 _VideoAgent_의 상이한 컴포넌트들의 효과. ✓ 및 ✗는 툴의 포함 여부를 나타낸다. w/re-ID는 re-ID로 구성된 객체 메모리를 사용하는 반면, w/o re-ID는 중복된 객체를 포함할 수 있는 객체 메모리를 사용한다.\n' +
      '\n' +
      '멀티모달 설정으로, 특히 이미지 및 비디오[11, 10, 1, 26, 44, 21]에 대해. LaViLa[45]는 비디오-텍스트 대조 표현 사전 훈련을 위한 자동 비디오 서술자로서 거대하고 다양한 텍스트 세트를 생성한다. 비디오-LLaMA[40]은 시각적 장면의 시간적 변화를 포착하고 더 나은 교차 모드 훈련을 위해 시청각 신호를 통합함으로써 비디오 이해를 가능하게 한다. 우리가 Sec에서 논의했던 것처럼. 1, 이러한 멀티모달 기반 모델 중 다수는 긴 형식의 비디오 이해에 어려움을 겪을 수 있다. 이를 개선하기 위해 LSTP[31]은 공간 및 시간 샘플러 모듈을 사용하여 광학 흐름 기반 시간 특징 및 정렬된 공간 관계를 비디오에서 추출하여 긴 형식의 비디오 이해도를 달성한다; Gemini[26]은 멀티모달 모델을 수만 개의 TPU와 대규모 개인 비디오 텍스트 데이터 세트를 사용하여 더 긴 비디오로 확장한다. 이러한 end-to-end 모델에 의한 빠른 진행에도 불구하고, 긴 형식의 비디오에 대한 엄청난 계산 비용과 변압기의 고유한 한계는 이러한 end-to-end 학습된 멀티모달 기반 모델을 비디오 이해에 적용하는 데 여전히 중요하다.\n' +
      '\n' +
      '영상 이해를 위한 멀티모달 도구 사용 에이전트\n' +
      '\n' +
      '또 다른 연구 라인은 값비싼 교육 없이 멀티모달 작업을 해결하기 위해 일련의 **도구**로 LLM을 강화하는 데 중점을 둔다. 특히, 이러한 **멀티모달 에이전트** 내의 LLM은 원래 작업을 해결하기 위한 단계별 계획을 생성하도록 프롬프트되고, 대화식으로 여러 멀티모달 기반 모델("도구"), _e.g_를 호출한다. captioning, VQA, _etc_. VisProg[6] 파일럿들은 GPT-3 플래너에 많은 시각적 도구 모음을 장착하여 복잡한 현실 세계의 시각적 추론 문제를 해결한다. 이러한 에이전트를 비디오 이해에 적용하려면 많은 도구 모델이 비디오에 대한 일반화를 보장하지 않기 때문에 신중한 설계가 필요하다. LifeLongMemory[30]은 자연어 비디오 내레이션을 사용하여 텍스트 기반 에피소드 메모리를 생성하고 LLM이 다운스트림 작업에 필요한 정보를 추론하고 검색하도록 유도한다. 도라에몽GPT[37]는 몬테카를로 트리 검색(MCTS)을 사용하여 비디오 이해 작업을 해결하기 위해 도구와 구조화된 메모리를 모두 호출하는 정교한 프롬프트 전략을 소개한다. 이러한 멀티모달 에이전트는 큰 잠재력을 가지고 있지만 현재까지는 복잡한 파이프라인 및 비디오 특정 설계의 부족으로 인해 공통 벤치마크에서 엔드 투 엔드 기반 모델 대응물에 대한 온-파 성능을 달성하는 데 어려움을 겪고 있다.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      '우리는 비디오 이해를 위한 새로운 통합 메모리 메커니즘으로 여러 기초 모델(대형 언어 모델과 비전 언어 모델)을 조화시키는 멀티모달 도구 사용 에이전트인 _VideoAgent_를 제시했다. End-to-end 비디오 언어 모델 및 툴-use 에이전트 대응들에 비해, _VideoAgent_는 미니멀리스트 툴-use 파이프라인을 채택하고 값비싼 트레이닝을 요구하지 않는 반면, EgoSchema, Ego4D NLQ, 및 NExT-QA를 포함하는 도전적인 롱폼 비디오 이해 벤치마크들에 대해 비교가능하거나 더 나은 경험 결과들을 제공한다.\n' +
      '\n' +
      '가능한 미래 방향은 로봇 공학, 제조 및 증강 현실의 실제 응용 프로그램에 대한 더 많은 탐색을 포함한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems (NeurIPS) (2022)\n' +
      '* [2] Fu, T.J., Li, L., Gan, Z., Lin, K., Wang, W.Y., Wang, L., Liu, Z.: An empirical study of end-to-end video-language transformers with masked visual modeling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22898-22909 (2023)\n' +
      '* [3] Gao, Z., Du, Y., Zhang, X., Ma, X., Han, W., Zhu, S.C., Li, Q.: Clova: A closed-loop visual assistant with tool usage and update. arXiv preprint arXiv:2312.10908 (2023)\n' +
      '* [4] Gong, R., Huang, Q., Ma, X., Vo, H., Durante, Z., Noda, Y., Zheng, Z., Zhu, S.C., Terzopoulos, D., Fei-Fei, L., et al.: Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971 (2023)\n' +
      '* [5] Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., et al.: Ego4d: Around the world in 3,000 hours of egocentric video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18995-19012 (2022)\n' +
      '* [6] Gupta, T., Kembhavi, A.: Visual programming: Compositional visual reasoning without training. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14953-14962 (2023)\n' +
      '* [7] Han, T., Xie, W., Zisserman, A.: Temporal alignment networks for long-term video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2906-2916 (2022)\n' +
      '* [8] Hou, Z., Ji, L., Gao, D., Zhong, W., Yan, K., Li, C., Chan, W.K., Ngo, C.W., Duan, N., Shou, M.Z.: Groundnlq@ ego4d natural language queries challenge 2023. arXiv preprint arXiv:2306.15255 (2023)\n' +
      '* [9] Korbar, B., Xian, Y., Tonioni, A., Zisserman, A., Tombari, F.: Text-conditioned resampler for long form video understanding. arXiv preprint arXiv:2312.11897 (2023)\n' +
      '* [10] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)\n' +
      '* [11] Lin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., Yuan, L.: Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122 (2023)\n' +
      '* [12] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint arXiv:2304.08485 (2023)\n' +
      '* [13] Lv, W., Xu, S., Zhao, Y., Wang, G., Wei, J., Cui, C., Du, Y., Dang, Q., Liu, Y.: Detrs beat yolos on real-time object detection. arXiv preprint arXiv:2304.08069 (2023)\n' +
      '* [14] Maaz, M., Rasheed, H., Khan, S., Khan, F.S.: Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424 (2023)* [15] Mangalam, K., Akshulakov, R., Malik, J.: Egoschema: A diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [16] Miech, A., Zhukov, D., Alayrac, J.B., Tapaswi, M., Laptev, I., Sivic, J.: Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 2630-2640 (2019)\n' +
      '* [17] OpenAI: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)\n' +
      '* [18] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023)\n' +
      '* [19] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning (ICML) (2021)\n' +
      '* [20] Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., Scialom, T.: Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [21] Shafiullah, N.M.M., Paxton, C., Pinto, L., Chintala, S., Szlam, A.: Clipfields: Weakly supervised semantic fields for robotic memory. arXiv preprint arXiv:2210.05663 (2022)\n' +
      '* [22] Song, E., Chai, W., Wang, G., Zhang, Y., Zhou, H., Wu, F., Guo, X., Ye, T., Lu, Y., Hwang, J.N., et al.: Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449 (2023)\n' +
      '* [23] Suris, D., Menon, S., Vondrick, C.: Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128 (2023)\n' +
      '* [24] Tapaswi, M., Zhu, Y., Stiefelhagen, R., Torralba, A., Urtasun, R., Fidler, S.: Movieqa: Understanding stories in movies through question-answering. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4631-4640 (2016)\n' +
      '* [25] Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., Metzler, D.: Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006 (2020)\n' +
      '* [26] Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023)\n' +
      '* [27] Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., Xie, S.: Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209 (2024)\n' +
      '* [28] Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Li, X., Chen, G., Chen, X., Wang, Y., et al.: Internvid: A large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942 (2023)\n' +
      '* [29] Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu, Y., Wang, Z., et al.: Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191 (2022)\n' +
      '* [30] Wang, Y., Yang, Y., Ren, M.: Lifelongmemory: Leveraging llms for answering queries in egocentric videos. arXiv preprint arXiv:2312.05269 (2023)\n' +
      '* [31] Wang, Y., Wang, Y., Wu, P., Liang, J., Zhao, D., Zheng, Z.: Lstp: Language-guided spatial-temporal prompt learning for long-form video-text understanding. arXiv preprint arXiv:2402.16050 (2024)* [32] Wiles, O., Carreira, J., Barr, I., Zisserman, A., Malinowski, M.: Compressed vision for efficient video understanding. In: Proceedings of the Asian Conference on Computer Vision. pp. 4581-4597 (2022)\n' +
      '* [33] Wu, C.Y., Krahenbuhl, P.: Towards long-form video understanding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1884-1894 (2021)\n' +
      '* [34] Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., Duan, N.: Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671 (2023)\n' +
      '* [35] Xiao, J., Shang, X., Yao, A., Chua, T.S.: Next-qa: Next phase of question-answering to explaining temporal actions. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9777-9786 (2021)\n' +
      '* [36] Yang, A., Miech, A., Sivic, J., Laptev, I., Schmid, C.: Zero-shot video question answering via frozen bidirectional language models. Advances in Neural Information Processing Systems **35**, 124-141 (2022)\n' +
      '* [37] Yang, Z., Chen, G., Li, X., Wang, W., Yang, Y.: Doraemongpt: Toward understanding dynamic scenes with large language models. arXiv preprint arXiv:2401.08392 (2024)\n' +
      '* [38] Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al.: mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178 (2023)\n' +
      '* [39] Yu, S., Cho, J., Yadav, P., Bansal, M.: Self-chained image-language model for video localization and question answering. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [40] Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858 (2023)\n' +
      '* [41] Zhang, H., Sun, A., Jing, W., Zhou, J.T.: Span-based localizing network for natural language video localization. arXiv preprint arXiv:2004.13931 (2020)\n' +
      '* [42] Zhang, S., Peng, H., Fu, J., Luo, J.: Learning 2d temporal adjacent networks for moment localization with natural language. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 12870-12877 (2020)\n' +
      '* [43] Zhang, Y., Sun, P., Jiang, Y., Yu, D., Weng, F., Yuan, Z., Luo, P., Liu, W., Wang, X.: Bytetrack: Multi-object tracking by associating every detection box. In: European Conference on Computer Vision. pp. 1-21. Springer (2022)\n' +
      '* [44] Zhao, H., Cai, Z., Si, S., Ma, X., An, K., Chen, L., Liu, Z., Wang, S., Han, W., Chang, B.: Mmicl: Empowering vision-language model with multi-modal in-context learning. arXiv preprint arXiv:2309.07915 (2023)\n' +
      '* [45] Zhao, Y., Misra, I., Krahenbuhl, P., Girdhar, R.: Learning video representations from large language models. In: CVPR (2023)In this Appendix, we will first detail the implementation of object re-ID method. Then, the tasks included in _VideoAgent_ and the corresponding models will be listed, followed by the experiment settings of _VideoAgent_ and all the comparative methods. Finally, cases of the inference of _VideoAgent_ will be illustrated.\n' +
      '\n' +
      '## 부록 0.객체 재식별\n' +
      '\n' +
      '추적 결과에 기초하여, 객체 재식별(re-ID)은 상이한 주기(다양한 추적 ID)에서 동일한 객체의 발생을 병합하는 것을 목표로 한다. 다음 알고리즘은 객체 re-ID의 절차를 보여준다. 그것은 추적 ID들의 세트를 수신하고, Re-ID 그룹들의 세트를 출력하는데, 여기서 각각의 Re-ID 그룹은 데이터베이스 내의 고유한 객체 ID를 나타내는 동일한 객체에 속하는 여러 추적 ID들을 포함한다.\n' +
      '\n' +
      '```\n' +
      '입력: 비디오\\(V\\), 트래킹 ID\\(\\{t_{1},t_{2},...,t_{n}\\}\\) 출력: RE-ID 그룹 \\(G=\\{U_{1},U_{2},...,U_{m}\\}\\)의 리스트\n' +
      '1 검사하고자 하는 추적 ID \\(T=\\{t_{1},t_{2},...,t_{n}\\}\\)를 초기화하며;\n' +
      '2 re-ID 그룹들의 집합을 초기화한다 \\(G=\\{\\}\\);\n' +
      '3forframe \\(f\\) in \\(V\\)do\n' +
      '4for\\(t_{i}\\)는 \\(f\\)과 \\(t_{i}\\in T\\)도에 나타난다.\n' +
      '5forRe-ID group \\(U\\) in Gdo\n' +
      '6if\\(\\forall t_{j}\\in U,\\text{share-no-frame}(t_{i},t_{j})\\) 및 \\(\\forall t_{j}\\in U,\\text{sim}(t_{i},t_{j})>0.5\\) 및 \\(\\exists t_{j}\\in U,\\text{sim}(t_{i},t_{j})>0.62\\then\n' +
      '7은 \\(T\\)에서 \\(t_{i}\\)을 제거하는 단계;\n' +
      '8 \\(t_{i}\\)을 \\(U\\)에 추가하는 단계;\n' +
      '9 break;\n' +
      '10\n' +
      '11if\\(t_{i}\\in T\\)then\n' +
      '12는 \\(T\\)에서 \\(t_{i}\\)을 제거하는 단계;\n' +
      '13은 새로운 그룹 \\(U=\\{t_{i}\\}\\});\n' +
      '도 14는 \\(G\\)에 \\(U\\)을 첨가하는 단계;\n' +
      '15\n' +
      '16\n' +
      '17\n' +
      '18 출력\\(G\\);\n' +
      '```\n' +
      '\n' +
      '**알고리즘 3**그룹화에 의한 객체 재식별\n' +
      '\n' +
      '각 비디오 프레임에 대해, 알고리즘은 검사되지 않은 프레임 내의 모든 추적 ID를 확인하고 기존의 임의의 Re-ID 그룹에 할당하려고 시도한다. 추적 ID\\(t_{i}\\)는 Re-ID 그룹 \\(U\\)에 병합되기 위해서는 세 가지 조건을 만족해야 한다. 1) 동일한 객체가 각 프레임마다 하나의 경계 박스만을 가지고 있기 때문에 \\(U\\) 내의 임의의 추적 ID와 공존해서는 안 된다; 2) 모든 \\(U\\) 내의 모든 \\(t_{j}\\(t_{i},t_{j}>0.5\\)을 가져야 한다; 여기서 sim은 CLIP와 DINOv2 특징 유사도를 가리키며; 3) 그룹 \\(U\\) 내의 적어도 하나의 추적 ID\\(t_{j}\\(t_{i},o_{j}>0.62\\)을 만족한다. 만약\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      '## 부록 0.C 실험 설정\n' +
      '\n' +
      '### _VideoAgent_의 설정\n' +
      '\n' +
      '_VideoAgent_\n' +
      '\n' +
      'LLM(GPT-4)의 도구 사용 능력은 LangChain7을 사용하여 촉진된다. LLM은 비디오 질문 응답 태스크에 대해 다음 텍스트에 의해 프롬프트된다.\n' +
      '\n' +
      '각주 7: [https://www.langchain.com/](https://www.langchain.com/)\n' +
      '\n' +
      '동영상과 관련된 객관식 질문에 답변해야 합니다. 이 질문에는 0, 1, 2, 3, 4로 표시된 5개의 선택이 있다. 비디오는 각각 0에서 시작하여 시간 순서대로 증가하는 정수 ID를 갖는 2초 세그먼트로 분할된다. 각 세그먼트에는 이벤트를 나타내는 캡션이 있습니다. 각 세그먼트에 나타나는 객체를 기록하는 객체 메모리가 있다. 오브젝트 메모리는 다른 에이전트에 의해 유지된다. 다음 도구에 액세스할 수 있습니다.\n' +
      '\n' +
      '\\(\\{\\)**tools\\(\\}\\)**\n' +
      '\n' +
      'ATTENTION:\n' +
      '\n' +
      '1. 프리픽스 \'#C\'를 갖는 세그먼트 캡션들은 카메라 착용자를 지칭하는 반면, 프리픽스 \'#O\'를 갖는 캡션들은 카메라 착용자 이외의 누군가를 지칭한다.\n' +
      '\n' +
      '2. \'visual_question_answering\'과 \'object_memory_querying\'을 모두 이용하여 객체 또는 사람과 관련된 질문에 답할 수 있다.\n' +
      '\n' +
      '3. \'visual_question_answering\'은 환각을 가질 수 있다. \'visual_question_answering\'의 답보다는 설명에 더 신경을 써야 한다.\n' +
      '\n' +
      '4. 툴에 대한 입력은 토큰\' 뿐만 아니라 다른 툴의 이름을 포함하지 않아야 한다.\n' +
      '\n' +
      '5. 선택지를 검증하여 객관식 질문에 보다 쉽게 답할 수 있다.\n' +
      '\n' +
      '6. 정보가 너무 모호하여 정확한 답변을 제공할 수 없다면 최선의 추측을 한다.\n' +
      '\n' +
      '다음 형식을 사용합니다.\n' +
      '\n' +
      '질문: 답변해야 하는 입력 질문\n' +
      '\n' +
      '생각: 당신은 항상 무엇을 해야 할지 생각해야 합니다.\n' +
      '\n' +
      '액션: 취할 액션은 [{tool_names}] 중 하나이어야 한다.\n' +
      '\n' +
      '액션 입력: 액션에 대한 입력\n' +
      '\n' +
      '관찰: 행동의 결과... (이 사고/행동/행동 입력/관찰은 N회 반복할 수 있음)\n' +
      '\n' +
      '생각: 이제 최종 답변을 알 수 있습니다: 원래 입력 질문에 대한 올바른 선택 레이블(0, 1, 2, 3, 4)\n' +
      '\n' +
      'Begin!\n' +
      '\n' +
      '질문: {**input**} Thought: {**agent_scratchpad**}\n' +
      '\n' +
      '상기 프롬프트 포맷에서, **툴들**은 툴 이름들의 세트 및 이들의 기능적 설명을 지칭하며, 다음을 포함한다:\n' +
      '\n' +
      'caption_retrieval: 입력 튜플(start_segment, end_segment)이 주어지면, 두 세그먼트 ID들 사이의 모든 캡션들, 기껏해야 15개의 캡션들을 얻는다. end_segment<start_segment+15.\n' +
      '\n' +
      'segment_localization: 단일 스트링 디스크립션이 주어지면, 이 툴은 캡션-설명 유사도가 가장 높은 상위-5 후보 세그먼트 및 세그먼트의 총 수를 반환한다.\n' +
      '\n' +
      'visual_question_answering: 입력 튜플(question, segment_id)이 주어지면, 이 툴은 segment_id-1로부터 segment_id+1로 시작하는 비디오 세그먼트에 초점을 맞출 것이다. 비디오 세그먼트에 대한 설명 및 세그먼트에 기초한 질문의 답변을 반환한다.\n' +
      '\n' +
      'object_memory_querying: \'비디오에 어떤 객체가 있는가?\', \'비디오에 몇 명이 있는가?\'와 같은 객체 관련 질문이 주어지면, 이 도구는 객체 메모리에 기초하여 답을 줄 것이다. 이 도구는 완전히 정확하지 않다.\n' +
      '\n' +
      '입력은 객관식 질문 입력을 의미하며, 질문과 5개의 옵션을 포함한다. **agent_scratchpad**는 에이전트의 중간 단계를 저장하는 LangChain에 의해 유지되는 리스트이다.\n' +
      '\n' +
      'Object Memory QueryingThe object_memory_querying 툴은 SQL 쓰기에 특화된 다른 LLM 에이전트(GPT-4)에 의해 달성되며, 다음과 같은 툴이 장착된다:\n' +
      '\n' +
      '* database_querying(\\(program\\): SQL \\(program\\)을 실행하여 객체 메모리 데이터베이스로부터 결과를 반환한다.\n' +
      '* open_vocabulary_retrieval(\\(description\\): 객체 \\(description\\)을 만족하는 가능한 객체 ID를 반환한다.\n' +
      '\n' +
      '중앙 에이전트에 의해 제기된 객체 관련 질의가 주어지면, 메모리 에이전트는 개방형 어휘 검색에 의해 관련 객체 ID들을 획득하고, 질의를 SQL 프로그램으로 번역하고, SQL 프로그램을 실행함으로써 객체 메모리에 있는 데이터베이스로부터 결과를 가져오고, 자연 언어 답변을 중앙 에이전트에 반환할 것이다.\n' +
      '\n' +
      'NExT-QA 및 EgoSchema에 대한 _VideoAgent_의 실험 설정, _VideoAgent_의 성능을 테스트하기 위해 위의 프롬프트를 사용한다. Ego4D NLQ의 경우, LaViLa+VICLIP에 대한 비디오-텍스트 및 텍스트-텍스트 유사도의 앙상블 비율은 18:11이고, Ego4D+ViCLIP의 앙상블 비율은 7:8이다. 앙상블 비율은 Ego4D NLQ의 훈련 세트에서 _R1@0.3_, _R1@0.5_, _R5@0.3_ 및 _R5@0.5_에서 최대 전체 성능에 따라 그리드 검색을 통해 찾는다.\n' +
      '\n' +
      '비교방법의###설정\n' +
      '\n' +
      '실험에서 우리는 다음과 같은 방법의 성능을 자체 테스트한다. 다양한 비교 방법에 대한 실험 설정은 다음과 같이 자세히 설명되어 있다.\n' +
      '\n' +
      '* SeViLA: 그들의 코드에 제공된 디폴트 설정들이 평가를 위해 채택된다. 비디오 프레임 번호는 32로 설정되고, 키 프레임 번호는 4로 설정된다.\n' +
      '* Video-LLaVA: 그들의 코드에 제공된 디폴트 설정이 평가를 위해 채택된다. 입력 프레임 번호는 8로 설정된다.\n' +
      '* mPLUG-Owl: EgoSchema dataset paper [15]의 평가 절차를 따르며, 이는 \'주어진 질문 <질문 텍스트>에 의해 mPLUG-Owl이 답변 <답변 텍스트> 정답? 비디오 프레임과 함께 출력 텍스트에서 토큰 \'예\'의 소프트맥스 점수가 가장 높은 옵션을 mPLUG-Owl의 답변으로 볼 것이다. 입력 프레임 번호는 EgoSchema 데이터셋 용지에 제공된 최상의 mPLUG-Owl 설정에 따라 5로 설정된다.\n' +
      '* ViperGPT: GPT-3.5를 코드 생성기로 채택한다. 4개의 프레임은 비디오로부터 균일하게 샘플링되고 생성된 코드는 질문에 응답하기 위한 정보를 수집하기 위해 4개의 프레임 상에서 실행된다.\n' +
      '\n' +
      '## 부록 0.D 사례연구\n' +
      '\n' +
      '이 섹션에서는 NExT-QA 및 EgoSchema 모두에 대한 _VideoAgent_의 성공적인 사례들이 예시된다. 최종 답변을 향한 각 단계에서 LLM은 먼저 취해야 할 행동에 대한 이유를 설명한 다음 행동(도구)과 그 입력을 출력한다. 도구가 실행되고 텍스트 결과를 LLM으로 반환합니다. 이 절차는 LLM이 최종 답을 얻을 때까지 반복될 것이다.\n' +
      '\n' +
      '### Case 1\n' +
      '\n' +
      '이 경우, LLM은 질문에 답하기 위해 도구 segment_localization, caption_retrieval 및 visual_question_answering을 사용한다. 붉은색 옷을 입은 남자가 화면의 작은 영역만을 차지하기 때문에 Video-LLaVA는 남자를 찾지 못하고 "남자는 다음에 무엇을 하는가"라는 질문에 환각으로 답을 내놓는다. 그러나 LLM은 모든 정보를 종합하여 정답을 만들어 낼 것이다. 비디오는 여기서 8을 찾을 수 있다.\n' +
      '\n' +
      '각주 8: [https://youtu.be/StCWCmCWJKw?si=-IKmlA20_2SqaI_W](https://youtu.be/StCWCmCWJKw?si=-IKmlA20_2SqaI_W)\n' +
      '\n' +
      '**질문**: "빨간 옷을 입은 남자는 출발점에서 비행기와의 거리를 보고 무엇을 하나요?"\n' +
      '\n' +
      '0: "그의 머리를 흔든다"\n' +
      '\n' +
      '1: "happy"\n' +
      '\n' +
      '2 : "점 전진"\n' +
      '\n' +
      '3 : "카운트 다운"\n' +
      '\n' +
      '4 : "이동"\n' +
      '\n' +
      ' 에이전트 실행자 체인에 들어가는 중입니다.\n' +
      '\n' +
      ' 빨간 옷을 입은 남자가 비행기에서 멀리 떨어진 곳을 보고 있는 부분을 찾아야 해요. 그런 다음 비주얼_쿼스_답변 도구를 사용하여 그가 다음에 무엇을 하는지 알 수 있습니다.\n' +
      '\n' +
      '**Action**: segment_localization\n' +
      '\n' +
      '**Action Input** : "붉은 옷을 입은 남자가 비행기에서 거리를 보고 있다"\n' +
      '\n' +
      '**결과**: 0에서 43까지의 총 44개의 세그먼트가 있다. 22: \'#CC는 지상에서 드론을 응시한다\', 15: \'#CC는 지역을 돌아본다\', 41: \'#CC는 지역을 돌아본다\', 39: \'#OA man x는 위에서 드론을 조정한다\', 40: \'#CC는 설명과 가장 유사성이 높은 세그먼트에서 드론을 응시한다\'는 세그먼트 22이지만 평면은 언급하지 않는다. 그 다음으로 높은 것은 세그먼트 15이지만, 그것은 또한 평면을 언급하지 않는다. 세그먼트 39와 40은 질문에서 언급한 "비행기"일 수 있는 드론을 언급한다. 더 많은 컨텍스트를 얻기 위해 이 세그먼트 주변의 캡션을 확인할 것입니다.\n' +
      '\n' +
      '***액션** : 캡션 검색\n' +
      '\n' +
      '**Action Input** : (37, 42)\n' +
      '\n' +
      '**결과**: {37: \'#C C look around the parking\', 38: \'#C C look around the\', 39: \'#O A man x adjust a drone on the\', 40: \'#C C stares the drone on the\', 41: \'#C C look around the area\', 42: \'#C C look around the\'\n' +
      '\n' +
      '세그먼트 37에서 42까지의 캡션들은 한 남자가 드론을 조정한 다음 그것을 응시하고 있는 것을 보여준다. 이것은 그 문제에서 언급한 "비행기에서 거리를 바라보며 빨간 옷을 입은 남자"일 수 있다. 나는 그가 다음에 무엇을 하는지 알아보기 위해 visual_question_answering 도구를 사용할 것이다.\n' +
      '\n' +
      '**Action**: visual_question_answering\n' +
      '\n' +
      '**Action Input**: ("남자는 다음에 무엇을 하나요?", 40).\n' +
      '\n' +
      '**결과**: 설명: 비디오는 활주로에 주차되어 있는 대형 비행기에 관한 것이다. 그것은 평면의 정면을 클로즈업하는 것을 포함하여 다른 각도에서 평면을 보여준다. 정답: 비디오 속의 남자는 비행기로 걸어가 문을 엽니다.\n' +
      '\n' +
      '영상 속 남자는 비행기를 향해 걸어가 문을 연다. 이 작업은 선택사항에 나열되지 않습니다. 가장 가까운 선택은 "이동"이며, 이는 남성이 원래 위치에서 비행기를 향해 이동하는 것으로 해석될 수 있다.\n' +
      '\n' +
      '**최종 정답**: 4\n' +
      '\n' +
      'Finished chain.\n' +
      '\n' +
      '### Case 2\n' +
      '\n' +
      '이 경우, 질문에 답하기 위해 segment_localization 및 visual_question_answering이 사용된다. Video-LLaVA에 의해 제공된 일부 세부사항들은 부정확하지만, LLM은 비디오의 캡션들 및 Video-LLaVA로부터의 결과에 기초하여 정답을 선택할 수 있다. 비디오는 여기서 9를 찾을 수 있다.\n' +
      '\n' +
      '각주 9: [https://youtu.be/vzlPCFqdtQQ?si=X9vATb1ClBVMsoMM](https://youtu.be/vzlPCFqdtQQ?si=X9vATb1ClBVMsoMM)\n' +
      '\n' +
      '\'소년은 어떻게 방망이를 땅에 고정시킬까?\'\n' +
      '\n' +
      '0: "그의 몸에 기대어 기울어짐"\n' +
      '\n' +
      '1 : "쿠션에 앉기"\n' +
      '\n' +
      '2 : "돌 위에 놓기"\n' +
      '\n' +
      '3 : "스윙의자가 보호된다"\n' +
      '\n' +
      '4 : "신발 착용"\n' +
      '\n' +
      ' 에이전트 실행자 체인에 들어가는 중입니다.\n' +
      '\n' +
      ' 문제는 영상에서 소년이 수행하는 구체적인 행위에 대해 묻는 것이다. 나는 소년이 박쥐와 상호작용하는 세그먼트를 찾은 다음 시각_질문_답변 도구를 사용하여 박쥐를 안정적으로 유지하는 방법을 이해해야 한다. 먼저, 나는 세그먼트_국소화 도구를 사용하여 소년과 박쥐가 함께 언급된 세그먼트를 찾을 것이다.\n' +
      '\n' +
      '**Action**: segment_localization\n' +
      '\n' +
      '**액션입력**: "소년배트"\n' +
      '\n' +
      '**결과**: 0에서 11까지의 총 12개의 세그먼트가 있다. {7: \'#O A man Y가 야구 방망이를 모두 들고 있음\', 6: \'#O B가 오른손에 갈퀴로 공을 치고 있음\', 9: \'#O man Y가 머리를 따뜻하게 들고 있음\', 10: \'#O A man Z가 양손으로 머리에 헬멧을 쓰고 있음\', 2: \'#O Aman X가 필드 주위를 걷고 있음\'} 세그먼트_국소화 도구는 몇 개의 세그먼트를 반환했지만, 세그먼트 7만이 야구 방망이를 들고 있는 남자(아마도 소년)를 언급함에 따라 질문과 관련이 있는 것으로 보인다. 이 세그먼트의 visual_question_answering 도구를 사용하여 소년이 배트를 안정적으로 유지하는 방법에 대해 자세히 알아보겠습니다. **Action**: visual_question_answering **Action Input**: ("How how the boy keep the bat?", 7) **Results**: Description: 비디오는 공원에서 자신의 양궁 기술을 연습하고 있는 남자에 관한 것이다. 그는 활과 화살을 들고 표적을 향해 쏘는 모습이 보인다. 정답: 소년은 방망이를 손에 단단히 잡고 일정하게 유지함으로써 방망이를 안정적으로 유지시킨다. 그는 또한 박쥐를 안정적으로 유지하기 위해 몸의 움직임을 사용합니다. Visual_question_answering 도구는 소년이 배트를 손에 단단히 잡고 몸의 움직임을 이용하여 안정적으로 유지한다는 답변을 제공하였다. 이는 객관식 질문의 첫 번째 옵션에 해당한다. **최종 정답** : 0\n' +
      '\n' +
      'Finished chain.\n' +
      '\n' +
      '### Case 3\n' +
      '\n' +
      '이 경우, visual_question_answering 툴은 영상에서 여성의 행동을 \'남자와 가위바위보\'로 착각하지만, 여성의 왼손에 있는 결혼반지를 식별하기 때문에 성공적인 질의응답에 가장 큰 기여를 한다. 비디오는 여기에서 10을 찾을 수 있습니다.\n' +
      '\n' +
      '각주 10: [https://youtu.be/EFMdUrDELqw?si=6iXTKJCW0x96a7Ap](https://youtu.be/EFMdUrDELqw?si=6iXTKJCW0x96a7Ap)\n' +
      '\n' +
      '\'처음엔 검은 안경을 쓴 여자가 결혼했다는 걸 어떻게 알죠?\'\n' +
      '\n' +
      '0 : "웨딩드레스 착용"\n' +
      '\n' +
      '1 : "꽃꽂이"\n' +
      '\n' +
      '2 : "결혼반지 착용"\n' +
      '\n' +
      '3: "조부모와 손자"\n' +
      '\n' +
      '4: "신랑과 함께 걷기"\n' +
      '\n' +
      '에이전트 실행자 체인에 들어가는 중입니다.\n' +
      '\n' +
      '영상이 시작될 때 검은 안경을 쓴 여자가 나오는 부분을 찾아야 해요. 그런 다음 visual_question_answering 도구를 사용하여 주어진 선택에 따라 그녀가 결혼했는지 결정할 수 있습니다.\n' +
      '\n' +
      '**Action**: segment_localization\n' +
      '\n' +
      '**액션 입력**: "시작에 검은 안경을 낀 여성"\n' +
      '\n' +
      '**결과**: 전체 44개의 세그먼트가 0에서 43 사이이다. {30: \'#O 여성 A는 왼손으로 얼굴에 안경을 만진다.} 11: \'#O 여성 X는 오른손으로 안경을 만진다.} 14: \'#O 사람 X는 안경을 조정한다.\' 16: \'#O 여성 X는 텔레비전을 쳐다본다.\' 39: \'#C C stares at a woman\'\n' +
      '\n' +
      '검은색 안경을 쓴 여성이 첫 번째 부분(0)에 등장한다. 이제 시각_질문_답변 도구를 사용하여 주어진 선택에 따라 그녀가 결혼했는지 여부를 결정할 것이다.\n' +
      '\n' +
      '**Action**: visual_question_answering\n' +
      '\n' +
      '**Action Input**: ("여성이 결혼했다는 것을 어떻게 아나요", 0).\n' +
      '\n' +
      '**결과**: 설명: 비디오는 한 남자와 여자가 가위바위보 게임을 하는 것을 보여준다. 여자가 이기고 남자는 화를 낸다. 정답: 그 비디오는 그 여성의 결혼 상태에 대한 어떠한 정보도 제공하지 않는다. 그러나 왼손에 결혼반지를 끼고 있는 것을 관찰할 수 있다.\n' +
      '\n' +
      '여자가 결혼반지를 끼고 있어 결혼했음을 알 수 있다. 이것은 선택 2와 일치한다.\n' +
      '\n' +
      '**최종 답변** : 2\n' +
      '\n' +
      'Finished chain.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:29]\n' +
      '\n' +
      '### Case 5\n' +
      '\n' +
      '이는 EgoSchema에서 전형적인 질문으로, 모델의 강력한 텍스트적 이해와 추론 능력이 있어야 올바르게 대답할 수 있다. 자막을 검색함으로써, LLM은 다양한 세그먼트에 있는 성분들의 정보를 수집할 수 있고, 각 성분들의 기능을 조사하기 위해 해당 세그먼트에 visual_question_answering\' 툴을 사용한다. 비디오는 여기서 12를 찾을 수 있습니다.\n' +
      '\n' +
      '각주 12: [https://youtu.be/sOLU2K7Ukwo?si=RNEIfZjGcmWeM-1t](https://youtu.be/sOLU2K7Ukwo?si=RNEIfZjGcmWeM-1t)\n' +
      '\n' +
      '**질문**: \'비디오에서 재료의 선택과 중요도를 분석하라. 그들이 최종 요리에 어떻게 기여하겠는가?\'\n' +
      '\n' +
      '0: \'티슈 페이퍼는 요리의 중요한 성분입니다. 그것은 모든 유출물을 청소하는 데 사용됩니다.\'\n' +
      '\n' +
      '1: "요리의 중요한 성분인 캔커버는 다진 쇠고기의 신선도를 보호하여 효과적으로 유지하는 역할을 한다."\n' +
      '\n' +
      '2: \'접시의 중요한 재료인 왁스 페이퍼는 잠재적인 누출을 방지하기 위해 캔을 안전하게 감싸기 위해 효과적으로 사용된다.\n' +
      '\n' +
      '3: \'다진 소고기, 파, 마요네즈는 모두 요리에 중요한 재료이다. 다진 소고기는 단백질을 제공하고, 파는 비타민과 미네랄을 제공하며, 마요네즈는 풍미를 제공한다.\'\n' +
      '\n' +
      '4: "투명 유리 그릇은 요리 준비에 중요한 중요한 재료 역할을 합니다." 다양한 재료를 잘 섞는 데 효율적으로 사용됩니다.\n' +
      '\n' +
      '에이전트 실행자 체인에 들어가는 중입니다.\n' +
      '\n' +
      '마지막 요리에 들어가는 재료들의 중요성에 대한 질문입니다. 비디오에서 어떤 재료가 사용되고 최종 요리에 어떻게 기여하는지 알아야 합니다. 나는 비디오에서 이벤트의 개요를 얻기 위해 caption_retrieval 도구를 사용하는 것으로 시작할 것이다.\n' +
      '\n' +
      '**Action**: caption_retrieval\n' +
      '\n' +
      '**Action Input** : (0, 14)\n' +
      '\n' +
      '**결과: {0: \'#C C 스쿱스 소스를 오른손에 숟가락이 있는 용기로부터\', 1: \'#C C 스쿱스 소스를 오른손에 숟가락이 있는 소스 용기로부터 토마토 페이스트를 붓고\', 2: \'#C C 스쿱스 소스를 오른손에 숟가락이 있는 스크램블 에그의 접시에 붓고\', 3: \'#C C 스쿱스 소스를 오른손에 숟가락이 있는 샐러드 상에 숟가락이 있는 접시 상에 붓고\', 4: \'#C C 스쿱스 소스를 오른손에 숟가락이 있는 테이블 상에 붓고\', 5: \'#C C 스쿱스 소스를 오른손에 숟가락이 있는 테이블 상에 붓고\', 6: \'#C C 스쿱스 소스를 오른손에 숟가락이 있는 테이블 상에 붓고\', 7: \'#C C 스쿱스 소스를 왼손에 숟가락이 있는 테이블 상에 붓고\', 8: \'#C C 스쿱스 소스를 왼손에 숟가락이 있는 테이블 상에 붓고\', 9: \'#C C 스쿱스 소스를 왼손에 숟가락이 있는 테이블 상에 붓고\', 9\n' +
      '\n' +
      '캡션은 요리에 소스, 토마토 페이스트, 마카로니 및 치즈, 향신료, 스크램블 에그, 케첩, 샐러드, 토마토 소스, 우유 및 티슈 페이퍼가 포함됨을 나타낸다. 티슈 페이퍼는 성분으로 사용하기보다는 세척용으로 사용하는 것 같습니다. 이 재료들이 어떻게 사용되는지 좀 더 알아봐야겠어요. 시각_질문_답변 도구를 사용하여 이러한 성분의 사용에 대한 자세한 내용을 얻을 것입니다.\n' +
      '\n' +
      '**Action**: visual_question_answering\n' +
      '\n' +
      '**Action Input**: (\'요리에서 소스의 역할은 무엇인가?\', 0)\n' +
      '\n' +
      '**결과**: 설명: 비디오는 주방에서 식사를 준비하는 사람에 관한 것이다. 그들은 음식을 자르고 그릇에 섞는 것을 볼 수 있습니다. 정답: 국수 위에 소스를 붓고 다른 재료들과 섞어서 요리에 풍미와 식감을 더합니다.\n' +
      '\n' +
      '소스는 요리에 풍미와 질감을 더하기 위해 사용됩니다. 이제 비주얼_쿼스_답변 도구를 다시 사용하여 다른 성분의 역할에 대해 자세히 알아보겠습니다.\n' +
      '\n' +
      '**Action**: visual_question_answering\n' +
      '\n' +
      '**Action Input**: (\'접시에서 향신료와 스크램블 에그의 역할은 무엇인가?\', 3)\n' +
      '\n' +
      '**결과**: 설명: 비디오는 넓고 조명이 잘 켜진 방에서 식사를 준비하고 먹는 사람에 관한 것이다. 정답: 향신료와 스크램블데그가 요리에 첨가되어 풍미와 식감을 높입니다. 향신료는 풍미와 향이 폭발하는 반면 스크램블 에그는 다른 성분을 보완하는 크림 같고 부드러운 질감을 제공합니다.\n' +
      '\n' +
      '향신료와 스크램블 에그는 요리의 풍미와 질감을 향상시키기 위해 사용됩니다. 이제 visual_question_answering 도구를 다시 사용하여 남은 재료의 역할에 대해 자세히 알아보겠습니다.\n' +
      '\n' +
      '**Action**: visual_question_answering\n' +
      '\n' +
      '**Action Input**: (\'요리에서 토마토 소스와 우유의 역할은?\', 6)\n' +
      '\n' +
      '**결과**: 설명: 비디오는 넓고 조명이 잘 켜진 주방에서 식사를 준비하고 먹는 사람에 관한 것이다. 그 사람은 테이블 위에서 음식을 자르고 그릇에 섞은 다음 음식을 먹는 모습이 보입니다. 정답: 토마토 소스와 우유를 아이스크림 위에 붓고, 요리에 달콤하고 톡 쏘는 맛을 더합니다.\n' +
      '\n' +
      '토마토 소스와 우유는 요리에 달콤하고 톡 쏘는 맛을 더하기 위해 사용됩니다. 그러나 그 대답은 캡션에서 언급되지 않았던 아이스크림에 대한 환각으로 보인다. 나머지 성분의 역할에 대해 더 알아보기 위해 visual_question_answering 도구를 다시 사용하겠습니다.\n' +
      '\n' +
      '**Action**: visual_question_answering\n' +
      '\n' +
      '**Action Input**: (\'요리에서 토마토 페이스트와 마카로니와 치즈의 역할은?\', 1)\n' +
      '\n' +
      '**결과**: 설명: 비디오는 넓고 조명이 잘 켜진 방에서 식사를 준비하고 먹는 사람에 관한 것이다. 정답: 토마토 페이스트와 마카로니, 치즈가 요리에 첨가되어 맛과 식감을 높여줍니다. 토마토 페이스트는 풍부하고 고소한 맛을 더하며, 마카로니와 치즈는 크리미하고 편안한 요소를 제공합니다.\n' +
      '\n' +
      '토마토 페이스트와 마카로니, 치즈는 요리의 풍미와 질감을 높이는 데 사용됩니다. 토마토 페이스트는 풍부하고 고소한 맛을 더하며, 마카로니와 치즈는 크리미하고 편안한 요소를 제공합니다.\n' +
      '\n' +
      '이제 시각_질문_답변 도구를 다시 사용하여 티슈 페이퍼의 역할에 대해 자세히 알아보겠습니다.\n' +
      '\n' +
      '**Action**: visual_question_answering\n' +
      '\n' +
      '**Action Input**: (\'접시에서 티슈 페이퍼의 역할은?\', 13)\n' +
      '\n' +
      '**결과**: 설명: 비디오는 화장실을 청소하는 사람에 관한 것이다. 정답: 휴지 종이는 접시를 청소하는 데 사용됩니다.\n' +
      '\n' +
      '티슈 페이퍼는 접시의 재료가 아닌 세척에 사용됩니다. 이제 제가 수집한 정보와 질문에 제공된 선택을 비교할 수 있습니다.\n' +
      '\n' +
      '**최종 답변**: 3\n' +
      '\n' +
      'Finished chain.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>