<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '3D 컨트롤이 있는 무빙플을 선택하세요.\n' +
      '\n' +
      ' 알렉시 간델스만(알렉세이 A) 에페로스 자센드라 말리쿠산 라자세가르란 李시 간델스만 李필드라 말리키 리 李이이다.\n' +
      '\n' +
      'UC Berkeley\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문에서는 주어진 목표 3D 모션 시퀀스에 대해 단일 이미지로부터 사람들을 애니메이션하기 위한 확산 모델 기반 프레임워크를 제시한다. 우리의 접근법은) 인체와 의류의 보이지 않는 부분에 대한 학습 사제, b) 새로운 몸을 적절한 의류와 질감으로 포즈하는 두 가지 핵심 구성 요소를 가지고 있다. 첫 번째 부분에서는 단일 이미지가 주어진 사람의 미필 부분을 복조하기 위해 인필링 확산 모델을 배웁니다. 이 모델을 텍스처 맵 공간에서 훈련시켜 포즈와 관점에 불변하기 때문에 더 많은 샘플 효율이 있습니다. 둘째, 우리는 3D 인간 포즈에 의해 제어되는 확산 기반 렌더링 파이프라인을 개발한다. 이것은 의복, 머리카락, 그럴듯한 비산 지역을 포함한 사람의 새로운 포즈들의 현실적인 렌더링들을 생성한다. 이 단절된 접근법은 우리의 방법이 3D 포즈에서 목표 모션에 충실한 이미지들의 시퀀스를 생성하고 시각적 유사성 측면에서 입력 이미지에 생성할 수 있게 한다. 그 외에도 3D 제어로 다양한 합성 카메라 궤적이 사람을 렌더링할 수 있습니다. 우리의 실험은 우리의 방법이 이전 방법과 비교하여 장기간 움직임을 생성하는 데 탄력적이며 다양한 도전적이고 복잡한 포즈를 생성한다는 것을 보여준다. 자세한 내용은 저희 웹사이트를 확인해 주세요: 3DHM.기투비.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '사람의 무작위 사진을 감안할 때, 우리는 그 사람을 다른 사람의 행동을 모방할 수 있다고 정확하게 추정할 수 있습니까? 이 문제에는 시간이 지남에 따라 인간이 어떻게 변화하는지에 대한 깊은 이해와 인간의 모습과 의류에 대한 배우기들이 필요하다. 예를 들어, 그림 1에서 ** 배우***는 걷기, 달리기 등 간단한 행동부터 전투, 춤 등 보다 복잡한 행위까지 다양한 행동을 할 수 있다. **Imitator**의 경우 외관 및 의류에 대한 시각적인 학습을 통해 서로 다른 포즈와 관점에서 그것들을 분리해야 한다. 이 문제를 해결하기 위해 단일 영상에서 텍스처 맵을 완료한 다음 3D 인간을 렌더링하여 행위자의 행동을 모방하는 2단계 프레임워크인 **3DHM*****(그림 2 참조)를 제안한다.\n' +
      '\n' +
      '우리는 시간의 흐름에 따라 재구성하여 추적함으로써 행위자의 움직임 신호를 추출하기 위해 최첨단 3D 인간 포즈 회수 모델 4DHumans[9, 19]를 사용한다. 일단 3D에 모션 신호가 있으면, 메서의 순서로서, 모방 작업의 중간 렌더링을 얻기 위해 단순히 모방기의 텍스처 맵으로 다시 텍스트화할 수 있다고 생각할 것이다. 그러나, 이것은 이미이터의 완전한 텍스처 맵을 필요로 한다. 이미이터의 단일 뷰 이미지만 주어졌을 때, 우리는 몸의 일부, 아마도 앞면 또는 뒷면만을 보지만 양쪽은 절대 보지 않는다. 단일 뷰 이미지로부터 이미이터의 완전한 텍스처 맵을 얻기 위해, 우리는 텍스쳐 맵의 미필 영역을 캡처하기 위한 확산 모델을 배우게 된다. 이것은 본질적으로 인간의 의류와 외모에 대해 사전 학습을 한다. 예를 들어, 블루 셔츠를 착용한 사람의 프론트뷰 이미지는 보통 뒷면에 같은 색상을 가질 것이다. 이 완벽한 텍스처 맵으로 이제 배우의 행동을 하는 모방자의 중간 렌더링을 얻을 수 있습니다. 중간 렌더링 수단은 SMPL[17] 메쉬 상단의 텍스처 맵을 포장하여 모방기의 바디-라이트 렌더링을 얻는 것을 의미한다.\n' +
      '\n' +
      '그러나 SMPL[17] 메쉬 렌더링은 몸통이며 스커트나 다양한 헤어스타일과 같이 옷에 변형되는 것을 포착하지 않는다. 이를 해결하기 위해 3D 포즈로 움직임을 제어함으로써 메쉬 렌더링에서 보다 현실적인 이미지로 매핑하는 두 번째 모델을 배우게 된다. 우리는 그러한 간단한 프레임워크가 특히 긴 비디오 세대에 대해 현실적이고 충실한 인간 비디오를 성공적으로 합성할 수 있음을 알아본다. 3D 컨트롤이 보다 미세화하고 정확한 모션 흐름을 제공하고 모방기의 시각적 유사성을 충실히 포착한다는 것을 보여준다.\n' +
      '\n' +
      '배우(4, 14, 28)의 동작을 재작성하는 작업이 많았지만, 각각 많은 양의 데이터, 감독 제어 신호 또는 교육 데이터의 세심한 주의가 필요하다. 예를 들어, 메이크-a-비디오 [24]는 인간 비디오에 대해 괜찮은 결과를 생성할 수 있으며, 종종 불완전하거나 비시퀀스적인 비디오를 생성하고 인간의 충실한 재구성에 실패한다. 일부 작품[8]은 오픈팟[6]을 중간 감독으로 사용한다. 그러나 개방은 주로 인간의 해부학적 핵심점을 포함하고 있으며 신체 형태, 깊이 또는 기타 관련 인체 정보를 나타내는 데 사용할 수 없다. DensePose[10]은 이미지와 신체 표면 사이의 매우 정확한 조밀한 대응성을 회복하여 촘촘한 인간의 포즈 추정을 제공하는 것을 목표로 한다. 다만, 원래 입력으로부터 질감 정보를 반영할 수 없다. 이 작업 라인에 비해, 우리는 모션의 정확한 조밀한 3D 흐름을 제공함으로써 모션을 제어하기 위해 3D 모델을 완전히 사용하고 텍스처 맵 표현은 몇 천개의 샘플에서 이전에 외관을 쉽게 배울 수 있도록 한다.\n' +
      '\n' +
      '2번으로 작업했습니다.\n' +
      '\n' +
      '** 제어 가능한 인간 세대** 인간 세대는 쉬운 일이 아닙니다. 이미지 번역 [16]과 달리 다른 인간을 생성하는 것은 모델이 인체의 3D 구조를 이해하도록 요구한다. 임의의 텍스트 프롬프트 또는 포즈 조건[5, 15]을 감안할 때, 기존 생성 모델이 종종 불합리한 인간 이미지 또는 비디오를 생성한다는 것을 종종 알게 된다. 확산-HPC[31]는 인간 포즈 교정을 가진 확산 모델을 제안하고, 생성 과정에서 인체 구조 사제들을 주입하는 것이 생성된 이미지의 품질을 향상시킬 수 있음을 발견했다. 제어넷[34]은 오픈팟[6]과 같은 추가 입력 조건을 지원하기 위해 미리 훈련된 대형 확산 모델을 제어하기 위해 신경망 아키텍처에 설계된다. GestureDiffuCLIP[3]는 신경망을 설계하여 공적분 제스처를 생성한다. 그러나 이러한 기술은 필요한 인간의 모습과 의류를 보장할 수 없는 인간 애니메이션에 맞춰지지 않는다.\n' +
      '\n' +
      '이동하는 사람을 합성하는**** 움직이는 사람을 합성하는 것은 매우 어려운 일이다. 예를 들어, 메이크-a-비디오[24] 또는 이젠 비디오[23]는 주어진 지시에 따라 비디오를 합성할 수 있다. 그러나 생성된 영상은 인간의 속성을 정확하게 포착할 수 없으며 생성된 인간의 이상한 구성을 유발할 수 있다. 선행 방법[8, 29]은 포즈 대 픽셀 매핑을 직접 학습한다. 그러나 이러한 디자인은 한 사람에게만 훈련되어 사용될 수 있습니다. SMPLitex[7]과 같은 최근 작품들은 단일 영상에서 무기후 인물에 이르기까지 인간의 질감 추정을 고려한다. 그러나 예측된 질감 지도를 통해 렌더링된 사람과 실제 인간 사이에는 시각적 격차가 있다. 많은 작품이 디램프스[14] 및 디스코[28]와 같은 확산 모델을 기반으로 픽셀을 직접 예측하기 시작한다. 드림포스는 DensePose[10]에 의해 제어되며, 인체의 포즈 시퀀스에 기초하여 인간과 직물 운동을 모두 포함하는 비디오를 합성하는 것을 목표로 한다. 디스코는 오픈메이트[6]에 의해 직접 제어되며, 2D 포즈 정보를 기반으로 인간의 생리를 목표로 한다. 그러나, 트레이닝 정규화를 위한 출력 픽셀들을 정렬하는 접근 방식은 종종 이러한 모델이 특정 트레이닝 데이터에 지나치게 전문화되도록 이끈다. 더욱이, 이 방법론은 데이터 분포가 훈련 데이터 세트와 밀접하게 일치하는 소수의 사람들에게 잘 수행되기 때문에 모델의 일반화 능력을 제한한다.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '3개는 사람을 움직이는 것을 선택합니다.\n' +
      '\n' +
      '이 섹션에서는 모션 시퀀스를 모방하기 위한 2단계 접근법에 대해 논의한다. 우리의 3DHM 프레임워크는 최신 예측 모델 4DHumans[9, 19]에서 정확한 3D 포즈 예측의 이점을 수용하며, 이는 인간 움직임을 정확하게 추적하고 배우 비디오의 3D 인간 포즈들을 추출할 수 있다. 우리가 모방하고 싶은 배우의 주어진 비디오에 대해 3D 재구성 기반 추적 알고리즘을 사용하여 배우의 3D 메쉬 서열을 추출한다. 인포팅 및 렌더링 부분의 경우, 우리는 다양한 생성 비전 작업에 비해 높은 경쟁 결과를 달성하는 가장 최근의 확산 모델 중 하나인 사전 훈련된 스테이블 디확산 [22] 모델에 의존한다.\n' +
      '\n' +
      '우리의 접근 방식 3DHM은 2개의 핵심 부분으로 구성되어 있으며, 파지-1로 텍스처 맵 인포팅 및 Stage-2로 인간 렌더링을 위한 라이딩 디퓨전(Rendering Diffusion)은 프레임워크에 대한 고수준의 개요를 보여준다. 스테이지-1에서 먼저 주어진 단일 뷰 이미지에 대해 이미지 상에 메서를 렌더링하고 각 가시 메시 삼각형에 픽셀을 할당하여 텍스쳐 맵의 대략적인 추정치를 추출하여 다시 렌더링하면 입력 이미지와 유사한 이미지를 생성할 것이다. 이 예측된 텍스처 맵은 입력 이미지의 가시적인 부분만을 갖는다. 스테이지-1 디퓨전 인포팅 모델은 이 부분 텍스처 맵을 취하고, 언세넨 영역을 포함하는 완전한 텍스처 맵을 생성한다. 이 완성된 텍스처 맵을 감안할 때, 우리는 SMPL[17] 메서의 중간 렌더링을 생성하고 Stage-2 모델을 사용하여 의류가 있는 보다 현실적인 이미지에 바디-라이트 렌더링들을 투사한다. 스테이지-2 디퓨전 모델의 경우, 우리는 배우의 행동을 복사하기 위해 모방자에게 3D 제어를 적용한다.\n' +
      '\n' +
      '실화 맵, 문헌 맵 도화.\n' +
      '\n' +
      '스테이지-1 모델의 목표는 이미이터의 비선 영역을 도식화하여 그럴듯한 완전한 텍스처 맵을 생성하는 것이다. 우리는 먼저 4DHumans[9]에 이어 가시 삼각형의 입력 이미지 및 샘플 색상 별로 3D 메쉬를 렌더링하여 부분적으로 보이는 텍스처 맵을 추출한다.\n' +
      '\n' +
      '** 입력*** 우리는 먼저 픽셀 대 표면 대응성을 추론하기 위한 공통 접근법을 사용하여 단일 RGB 영상에서 3D 메쉬를 텍스쳐링하기 위한 불완전한 UV 텍스처맵[7, 32]을 구축한다. 또한 가시성 마스크를 계산하여 3D에서 어떤 픽셀이 보이는지 그리고 어떤 픽셀이 보이지 않는지 나타낸다.\n' +
      '\n' +
      '*** 목표** 이 모델링의 목적은 완전한 텍스처 맵을 생성하는 것이기 때문에 비디오 데이터를 사용하여 유사 완전 텍스처 맵을 생성한다. 4DHumans는 시간이 지남에 따라 사람들을 추적할 수 있기 때문에 가시 영역의 이동 평균으로 내부 텍스처 맵 표현을 지속적으로 업데이트한다. 그러나 더 날카로운 이미지를 생성하기 위해 생성 작업에 대해 중간 필터링이 이동 평균보다 더 적합하다는 것을 발견했다. 이 기술은 모든 영상에 적용될 수 있지만 이 단계에서 우리는 2,205개의 인간 비디오에 의존한다. 각 인간 비디오에 대해 우리는 먼저 각 프레임에서 부분적인 텍스처 맵을 추출한다. 각 비디오에는 360도의 사람의 뷰가 포함되어 있기 때문에 전체 비디오에서 유사 완전 텍스처 맵을 계산하고 이를 페이지 1에 대한 목표 출력으로 설정했는데, 자세한 내용은 비디오의 텍스처 맵의 전체 가시 부분을 취한다.\n' +
      '\n' +
      '** 모델*** W핀셋은 이미지 완료 작업에 큰 성능을 보이는 스테이블 디퓨전 인포팅 모델 [21]에서 직접 볼 수 있다. 부분 텍스처 맵과 대응하는 가시성 마스크를 입력하고 회수된 사람의 예측 맵을 얻는다. 우리는 텍스트 인코더 브랜치와 잠금합니다.\n' +
      '\n' +
      '그림 2: ** 3DHM:**의 개요는 모델 파이프라인의 개요를 보여준다. 이미이터의 이미지와 배우로부터의 3D 포즈 시퀀스의 서열을 감안할 때, 우리는 먼저 이미이터의 완전한 전체 텍스처 맵을 생성하는데, 이는 배우로부터 추출된 3D 포즈 시퀀스에 적용되어 이미이터의 텍스처 캡핑된 중간 렌더링들을 생성할 수 있다. 그런 다음 이러한 중간 렌더링을 Stage-2 모델에 통과시켜 SMPL 메쉬 렌더링을 실제 이미지의 보다 현실적인 렌더링에 투영한다. 적색 상자는 입력을 나타내고, 노란색 상자는 단계 1의 중간 예측을 나타내며, 파란색 상자는 단계 2의 최종 출력을 나타내며, ** 가변** 지속 기간과 **any 수***3D 포즈로 이동 사람 애니메이션을 생성하려면 완전한 텍스처 맵을 획득하기 위해 단계 1 **once***를 실행할 필요가 있다.\n' +
      '\n' +
      '갈로는 고정형 스테이블 디퓨전 모델의 입력 텍스트로서 \'유익한 인간\'을 먹인다. 트레이닝된 모델을 인포팅 디퓨전이라고 합니다. 모델 아키텍처에 대해 그림 3을 참조하세요.\n' +
      '\n' +
      '### Human Rendering\n' +
      '\n' +
      '단계 2에서 우리는 행위자의 행동을 수행하는 인간 모방자의 현실적인 렌더링을 얻는 것을 목표로 한다. 중간 렌더링(스테이지-1에서 나온 포즈와 텍스처 맵으로 연결됨)은 다양한 인간의 움직임을 반영할 수 있지만, 이러한 SMPL 메쉬 렌더링은 몸빛이며 의류, 헤어 스타일, 체형으로 현실적인 렌더링을 나타낼 수 없다. 예를 들어, 소녀가 드레스를 입고 춤을 추는 장면을 입력하면 중간 렌더링은 "춤"이 가능할 수 있지만 SMPL 메쉬 렌더링으로 치마를 미비한 것은 불가능하다. 이 모델을 훈련시키기 위해 완전히 자명한 패션에서 우리는 배우가 모방자라고 가정하며, 좋은 배우가 모두 좋은 모방자가 되어야 한다. 이렇게 하면 모든 비디오를 찍을 수 있고 4DHumans[9]에서 포즈 시퀀스를 얻고 단일 프레임을 취하여 Stage-1에서 완전한 텍스처 맵을 얻은 다음 3D 포즈에 텍스처 맵을 렌더링하여 중간 렌더링을 얻을 수 있다. 이제 중간 렌더링과 실제 RGB 이미지의 쌍을 이루는 데이터가 있습니다. 이를 이용하여 많은 양의 페어링된 데이터를 수집하고 컨디셔닝으로 Stage-2 확산 모델을 훈련시킨다.\n' +
      '\n' +
      '** 입력:** 우리는 먼저 생성된 텍스처 맵(매우 완전한)을 스테이지 1에서 배우 3D 바디 메쉬 시퀀스에 적용하여 배우의 액션들을 수행하는 모방기의 중간 렌더링에 적용한다. 이때 중간 렌더링은 3D 메쉬(바디라이트 의류)에 맞는 의류만 반영할 수 있지만 스커트, 겨울 재킷 또는 모자의 퍼프업 영역 등 SMPL 바디 외부의 질감을 반영하지 못한다. 완전 의복 식감을 가진 인간을 얻기 위해 얻은 중간 렌더링과 사람의 원래 이미지를 엔딩 디퓨전(Rendering Diffusion)에 입력하여 인간이 사실적인 모습으로 새로운 포즈로 렌더링한다.\n' +
      '\n' +
      '** 목표:**는 행위자가 모방자라고 가정하여 데이터를 수집했기 때문에 중간 렌더링과 실제 RGB 이미지의 쌍을 이루는 데이터를 가지고 있다. 이를 통해 직접적인 3D 감독을 필요로 하지 않고 많은 데이터에 이 모델을 훈련시킬 수 있습니다.\n' +
      '\n' +
      '** 모델*** 제어넷과 유사하게 3D 조건을 처리하기 위해 Stable Diffusion [20] 모델의 인코더 가중치를 제어 가능한 분기("변형 가능한 복사본)"로 직접 복제한다. 사전 훈련된 스테이블 디퓨전 및 입력 시끄러운 래치(\\(64\\시 64\\)를 동결합니다. 한편, 우리는 고정 VAE 인코더에 입력된 시간 \\(t\\)에서 3D 인간을 매핑한 텍스쳐와 원래 인간 사진을 입력하고 컨디셔닝 래치로서 3D 인간 래치(\\(64\\t 64\\)) 및 외관 래치(\\(64\\t 64\\))를 매핑한 텍스쳐를 얻는다. 우리는 이 두 개의 컨디셔닝 래치들을 레전드 디확산 제어 가능한 지점으로 먹인다. 이 지점의 핵심 설계 원리는 인간 입력으로부터 질감을 학습하고 변성 과정을 통해 훈련 중에 매핑된 3D 인간의 질감에 적용하는 것이다. 목표는 생성된 (텍스트 매핑된) 3D 인간으로부터 생생한 질감을 가진 실제 인간을 스테이지 1로부터 렌더링하는 것이다. 우리는 출력이 잠재되어 확산 단계 절차 및 고정된 VAE 디코더를 통해 픽셀 공간으로 처리한다. 스테이지 1과 마찬가지로 텍스트 인코더 가지를 잠그고 항상 고정된 스테이블 디퓨전 모델의 입력 텍스트로서 \'실제 인간이 작용하는 행위\'를 먹인다. 트레이닝된 모델을 레전드 디퓨전이라고 합니다. 렌딩 디퓨전에서 프레임별 출력 프레임을 예측합니다. 우리는 보여 준다.\n' +
      '\n' +
      '그림 4: **Stage-2의 3DHM:*** 이 그림은 우리의 Stage-1 접근법의 추론을 보여준다. 배우의 포즈와 이미이터의 실제 RGB 이미지를 가진 모방자의 중간 렌더링을 감안할 때, 우리의 모델은 행위자의 포즈에 대한 모방자의 현실적인 렌더링들을 합성할 수 있다.\n' +
      '\n' +
      '그림 3: **Stage-1 3DHM:**의 첫 번째 단계에서 이미이터의 단일 뷰 이미지를 감안할 때, 우리는 먼저 부분 텍스쳐 맵과 그에 상응하는 가시성 맵을 추출하기 위해 4차원 [9] 스타일 샘플링 접근법을 적용한다. 이 두 개의 입력이 그림 내 확산 모델에 전달되어 그럴듯한 완전한 텍스처 맵을 생성한다. 이 예에서 이미이터의 **백 뷰**만 볼 수 있지만 모델은 의류와 일치하는 그럴듯한 전면 영역을 복각할 수 있었다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '***Dataset*** 2K2K[11], THuman2.0[33] 및인민-Snapshot[2] 데이터셋에서 2,524개의 3D 인간 영상을 수집합니다. 2K2K는 2K 해상도 이미지에서 재구성된 3D 인간 모델을 가진 대규모 인간 데이터세트이다. T 인간 2.0은 밀도가 높은 DLSR 장비에 의해 캡처된 500개의 고품질 인간 스캔을 포함한다. 사람-스냅샷은 24개의 서열을 캡처하는 더 작은 인간 데이터세트이다. 3D 인간 데이터 세트를 영상으로 변환하고 4DHumans를 사용하여 인간 비디오에서 3D 포즈 추출합니다. 검증 및 테스트를 위해 훈련 및 기타 비디오에 2,205개의 비디오를 사용합니다. 의류의 데이터세트 분포에 대한 자세한 내용은 부록을 참조하세요.\n' +
      '\n' +
      '*** 평가 측정** 우리는 이미지 기반 및 비디오 기반 메트릭으로 우리 방법의 생성된 프레임의 품질을 평가한다. 이미지 기반 평가를 위해 발전 품질을 평가하기 위해 DisCO[28]의 평가 프로토콜을 따른다. 우리는 평균 PSNR[13], SSIM[30], FID[12], LPIPS[35], L1을 보고하고 비디오 기반 평가를 위해 FVD[26]를 사용한다. 3D 포즈 정확도를 평가하기 위해 MPVPE 및 PA-MVPVE를 사용한다. MPVPE[18], 또는 Mean Per-Vertex 페이즈 에로어는 3D 인간 포즈 추정에서 중요한 메트릭으로, 모델 전반에 걸쳐 예측된 3D 정점과 실제 3D 정점 사이의 평균 거리를 정량화한다. 이 측정은 3D 재구성 및 포즈 추정의 정확도를 평가하기 위해 필수적이며 MPVPE가 낮을수록 더 높은 정밀도를 나타낸다. 이를 보완하기 위해 PA-MPVPE 또는 프로테우스 정렬 메안 퍼-베어텍스 위치 유도는 이 평가에 또 다른 차원을 추가한다. 평균 오차를 계산하기 전에 방향, 규모 및 위치의 차이를 중화시키는 프로튜스 분석을 사용하여 예측 및 근거 진실 데이터를 정렬하는 것을 포함한다. 이 정렬을 통해 PA-MPVPE가 예측의 구조적 정확도에 초점을 맞출 수 있어 절대 공간 좌표와 무관하게 모델에서 정점들의 상대적 위치를 평가하기 위한 귀중한 메트릭이 된다.\n' +
      '\n' +
      '모든 데이터 세트를 학습하기 위해 일정한 학습률을 5e-05로 설정하고 모든 데이터 세트를 트레이닝하기 위해 스페이지-1 및 스트레이지-2의 확산 모델[27]을 사용하는데, 파일 1 인핑징 디퓨전 모델[21]에 대해 우리는 이 단계에서 VAE가 동결되기 때문에 총 859M 훈련 가능 매개변수와 206M 전체 변형 가능 매개변수가 있는 스톤블 디퓨전 인핑팅 인포팅 입력 모델[21]에 대한 선행 학습률을 설정했다. 우리는 50epoch에 대한 런딩 디퓨전 훈련을 하고 훈련 데이터세트 수프에서 모델을 실행하는 데 약 2주가 걸린다. 스테이지 2렌드링 디퓨전에는 제어 가능한 가지를 훈련하고 냉동 스테이블 디퓨전 백본을 훈련합니다. 이 경우 훈련 가능한 파라미터의 총 수는 876M이고, 변형 불가능한 파라미터의 총 수는 1.1B이다. 30epoch에 대한 엔딩 디퓨젼을 훈련하고 배치 크기가 4인 8 NVIDIA A100 GPU를 기반으로 트레이닝 데이터셋에 모델을 실행하는데 약 2주가 소요되며 추론은 모방기의 전체 텍스쳐 맵을 재구성하기 위해 Stage-1만 실행하면 되며, 다른 모든 새로운 포즈와 관점에 사용된다. 우리는 각 프레임에 대해 독립적으로 Stage-2 추론을 실행하지만, 이미이터의 초기 RGB 프레임은 모든 프레임에 대해 조건화되어 있기 때문에 Stage-2 모델은 일시적으로 일관된 샘플을 생성할 수 있다.\n' +
      '\n' +
      '### Quantitative Results\n' +
      '\n' +
      '*** 기본** 우리는 우리의 접근법을 과거 및 최첨단 방법, 드림포즈[14], 디스코[28] 및 컨트롤넷[34](포즈 정확도 비교)1과 비교하고 공정 비교를 위한 모든 접근법에 대해 추론 단계를 50으로 설정했다.\n' +
      '\n' +
      '2세대 품질***를 비교한 3DMM을 \\(256\\tco 256\\) 해상도에서 50개의 인간 비디오로 구성된 \\(2\\mathrm{K2K}\\) 테스트 데이터셋의 다른 방법과 비교했다. 인간 비디오 각각에 대해, 우리는 각 비인물의 다른 관점을 나타내는 30개의 프레임을 취한다. 각도는 \\(0^{\\ 회로}\\)에서 \\(360^{\\ 회로}\\)까지이며, 각 모델의 예측 및 일반화 능력을 더 잘 평가하기 위해 \\(12^{\\ 회로}\\)마다 하나의 프레임을 취한다. 디스코의 경우, 우리는 그들의 설정을 엄격하게 따르고 추론을 위해 OpenPose를 추출한다. 드림포스의 경우 추론을 위해 디센스포스를 추출한다. 우리는 결과를 평가하고 각 비디오의 모든 프레임에 대한 평균 점수를 계산한다. 우리는 공정한 비교를 위해 모든 접근법에 대한 배경을 검정색으로 설정했다. 우리는 동일한 50개의 비디오 전체의 평균 점수를 보고하고 표 1의 비교를 보여준다. 3DMM이 다른 메트릭의 모든 기저부를 능가한다는 것을 관찰한다.\n' +
      '\n' +
      '1: 우리는 저자가 제공하는 오픈 소스 공식 코드 및 모델을 사용하여 이러한 기저부를 구현한다. 제어넷 및 산출물 추출에는 확산기[27]와 디스코용 디센스포즈 추출을 위한 디텍트론2를 사용한다. 찬 등[8]은 특정인을 애니메이션하는 데만 일할 수 있기 때문에 본 논문에서는 그와 비교하지 않는다.\n' +
      '\n' +
      '비디오 수준 세대 품질***를 비교하여 3DMM의 시간적 일관성을 검증하기 위해 이미지 수준 평가에서와 동일한 테스트 세트 및 기준 구현에 따른 결과도 보고한다. 이미지 수준 비교와 달리, 우리는 연속 16 프레임마다 연결하여 도전적인 관점에 대해 각 비인물의 샘플을 형성한다. \\(150^{\\circ}\\)에서\\(195^{\\circ}\\)까지의 각도 범위(195^{\\circ}\\)로 하나의 프레임을 취한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c} \\hline \\hline Method & PSNR\\(\\uparrow\\) & SSIM \\(\\uparrow\\) & FID \\(\\downarrow\\) & LPIPS \\(\\downarrow\\) & L1 \\(\\downarrow\\) \\\\ \\hline \\hline DreamPose & 35.06 & 0.80 & 245.19 & 0.18 & 2.12e-04 \\\\ DisCO & 35.38 & 0.81 & 164.34 & 0.15 & 1.44e-04 \\\\ \\hline \\hline Ours & **36.18** & **0.86** & **154.75** & **0.12** & **9.88e-05** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ** 프레임별 생성 품질에 대한 정량적 비교 :**. 포즈 조건 생성 작업에 대한 이전 작업과 방법을 비교하고 샘플의 생성 품질을 측정한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '그림 6. 드림포즈, DisCO, 3DHM에서 다양한 관점의 질적 결과를 나타내면 모두 스테이블 디퓨젼의 사전 학습된 가중치로 U-Net 모델을 초기화한다. 우리는 3DHM이 제한된 3D 인간에게만 훈련되지만 실제 인간에 대한 일반화를 잘 할 수 있음을 알아차린다. 꿈포스는 더 나은 결과를 얻기 위해 UNet의 주제별 핀셋을 필요로 하기 때문에 무작위 인간 사진에서 직접 잘 일반화할 수 없다. 디스코는 인간에게 더 나은 일반화 가능성을 위해 여러 공공 데이터 세트를 사전 훈련하는 효과적인 인간 속성으로 훈련되었지만 여전히 목표 포즈 없이 사람들을 합성하지 못한다. 3DHM은 경직된 3D 제어를 추가하여 포즈에 외관을 더 잘 연관시키고 체형을 보존하기 때문이라고 가정한다. 오픈포즈 또는 디센스포스와 훈련하는 것은 질감과 포즈 사이의 매핑을 보장할 수 없으므로 모델이 일반화하기 어렵다.\n' +
      '\n' +
      '그림 5: 동일한 포즈의 다른 관점에 대한 정성적 결과, 텍스트 입력으로부터 랜덤 비디오 및 모션으로부터의 움직임이다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '3DHM은 독립적으로 인간 모션 비디오의 프레임을 생성함에 따라 시간의 일관성 보장이 없다. 예컨대, 의류등은 연속 프레임 간에 변할 수 있다. 한 가지 가능한 해결책은 여러 프레임을 동시에 예측하기 위해 모델을 훈련시키는 것이다. 또 다른 가능한 해결책은 확률적 컨디셔닝[1]을 통해 이전에 생성된 프레임들에 대한 생성 과정을 조건화하는 것이다. 또한 3DHM은 2K인 데이터셋으로 훈련되기 때문에 추론(예: 옷에 고유한 로고) 동안 모든 세부 질감을 완전히 재구성할 수 있는 것은 아니다. 우리는 이것을 더 많은 인간 데이터로 훈련함으로써 완화할 수 있다고 가정한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 하나의 랜덤 사진과 타겟 인간 포즈를 기반으로 움직이는 사람들을 합성할 수 있는 2단계 확산 모델 기반 프레임워크인 3DHM을 제안한다. 접근법의 주목할만한 측면은 인간 운동을 생성하기 위해 최첨단 3D 포즈 추정 모델을 사용한다는 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c} \\hline Method & MPVPE \\(\\downarrow\\) & PA-MPVPE \\(\\downarrow\\) \\\\ \\hline \\hline Default & 41.08 & 31.86 \\\\ \\hline w/o Texture map & 92.94 & 59.18 \\\\ w/o Appearance Latents & 41.99 & 32.82 \\\\ adding SMPL parameters & **39.16** & **29.67** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6:렌드링 디퓨젼에 대한 구조 연구는 표 6이다. 다양한 설정에서 포즈 정확도를 비교합니다.\n' +
      '\n' +
      '그림 6: 무작위 실제 인간 사진(한국 여배우)에서 다른 2D 제어 접근법과 정성적 비교를 하였다. 다양한 3D 포즈 또는 다른 관점에서 동일한 3D 포즈를 적용합니다. 2D 포즈가 폴딩 모션과 인체의 디테일을 포착할 수 없다는 것을 알 수 있었다. 우리는 우리의 접근 방식 3DHM이 **3D 제어**와 이 격차를 해소할 수 있음을 알아차릴 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c} \\hline Method & FID-VID\\(\\downarrow\\) & FVD \\(\\downarrow\\) \\\\ \\hline \\hline Default & **55.40** & **422.38** \\\\ \\hline w/o Texture map & 113.97 & 632.67 \\\\ w/o Appearance Latents & 93.21 & 715.51 \\\\ adding SMPL parameters & 72.35 & 579.90 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5:렌드링 디퓨젼에 대한 구조 연구는 표 5이다. 다양한 설정에서 비디오 레벨 생성 품질을 비교합니다. SMPL 매개변수를 추가하면 프레임별 설정에서 더 나은 성능을 달성하지만 기본 설정보다 더 나쁜 시간적 일관성을 초래할 수 있다는 것을 알아차린다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]S. 아리아카바리아, F. 사다트 세일레, M. 살즈만, L. 페테르슨과 S. 다양한 인간 움직임 예측을 위한 확률적 컨디셔닝 방식(2020)을 사용할 수 있습니다. 컴퓨터 비전 및 패턴 인식 관련 IEEE/CVF 회의 개최에서 pp. 5223-5232: SS1에 의해 발표되었다.\n' +
      '* [2]T. 알디넥, M. 마그너, W. Xu, C. Theobalt, G. Pons-Moll(2018) 비디오 기반 3d 사람 모델 재구성입니다. 컴퓨터 비전 및 패턴 인식 관련 IEEE 회의 개최에서 pp. 8387-8397:SS1이 발표했다.\n' +
      '* [3]T. 아오, Z. 장, L. Liu(2023) GestGrowthiff-풀링: 클립 래치들이 있는 제스처 확산 모델이다. arXiv 프리프린트 arXiv:2303.14613: SS1에 의해 계산된다.\n' +
      '*[4]C. 브레플러, M. 코벨, M. 슬란디(2023) 비디오 재작성: 오디오로 시각적 스피치를 구동합니다. 세미날 그래픽 팩스: 바운더리를 잡은 권 2, pp 715-722.: SS1에 의해 계산됩니다.\n' +
      '*[5]T. 브룩스와 A. A. Efros (2022) 격막 포즈 호환 장면입니다. 컴퓨터 비전 관련 유럽 회의에서: SS1이 제안했습니다.\n' +
      '* [6]Z. 카오, T. 시몬, S. 아이, Y. 시크(2017) 실시간 다인 2d 포즈 추정은 파트 친화도 필드를 활용한 것이다. 컴퓨터 비전 및 패턴 인식에 관한 IEEE 회의의 진행에서 pp 7291-7299:SS1에 의해 체결되었다.\n' +
      '*[7]D. 카사스와 M. C. 트리니다드(2023) Smplitex: 단일 이미지에서 3d 인간 텍스처 추정을 위한 생성 모델 및 데이터셋이다. arXiv 프리프린트 arXiv:2309.01855: SS1에 의해 계산된다.\n' +
      '* [8]C. 차, S. 고노사, T. 주, A. Efros (2019) 에브리신 춤은 지금. 컴퓨터 비전에 대한 IEEE/CVF 국제 회의의 개최에서 pp. 5933-5942: SS1이 발표했다.\n' +
      '* [9]S. 고엘, 가벨라코스, 자라세가르안, 아칸카자와, 조 말릭(2023) 허먼스는 4D로 변압기로 인간을 재구성하고 추적한다. ICCV에서: SS1에 의해 계산된다.\n' +
      '*[10]R. Alp 굴러, N. 노노바와 I. 코키노스(2018) 디센포스의 조밀한 인간 포즈 추정. 컴퓨터 비전 및 패턴 인식에 대한 IEEE 회의의 진행에서 pp 7297-7306:SS1에 의해 발표되었다.\n' +
      '*[11]S. 한, M. 박, 박, 윤 J H. 윤, 강 J. Y. 박씨와 H. 전(2023)고충성 3d 인간 디지털화가 단일 2k 해상도 영상에서 이루어졌다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서: SS1이 수립했다.\n' +
      '*[12]M. 허셀, H. 람사우어, T. 유니터타이너, B. 네슬러, S. 2개의 시간 규모의 업데이트 규칙에 의해 훈련된 호크로리터(2017) 간스는 로컬 나시 평형으로 수렴한다. 신경 정보 처리 시스템(30)의 발전은: SS1에 의해 계산된다.\n' +
      '*[13]A. Hore 및 D. Ziou(2010) 이미지 품질 메트릭: psnr vs. 심아. 2010년 제20차 패턴 인식 국제 회의에서 pp. 2366-2369: SS1로 지정되었다.\n' +
      '* [14]J. 카라스, 아시네토박터 성렌스키, T. 왕, 그리고 I. Kemelm 스승-Shlizerman(2023) Dreampose: 안정적인 확산을 통한 패션 이미지 대 비디오 합성입니다. arXiv 프리프린트 arXiv:2304.06025: SS1에 의해 계산된다.\n' +
      '*[15]S. 칼랄, T. 브룩스, 아키켄, Jiken, J 우, J. 양, J. 루, A. Efros, K. K. Singh (2023) 사람들이 자신의 자리에 있는 사람, 즉 어포던스 인식 인간 삽입을 장면에 삽입한다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE 회의의 진행에서: SS1이 수립했다.\n' +
      '*[16]B. 리, Y. 의, T. 린, S. Belongie(2022) Sitta: 데이터 증강을 위한 단일 이미지 텍스처 번역입니다. 컴퓨터 비전 관련 유럽 회의에서 pp는 3-20: SS1이 발표했다.\n' +
      '*[17]M. 로퍼, N. Mahmood, J. 로마로, G. Pons-Moll 및 M. J. 블랙(2023) Smpl: 피부 다인 선형 모델이다. 세미날 그래픽 팩스: 바운더리를 잡은 권 2, pp 851-866.: SS1에 의해 계산됩니다.\n' +
      '*[18]G. 문, 최, K. M. 이모(2022)는 전신 3d 인체 메쉬 추정에 대한 추정이 가능하다. 컴퓨터 비전 및 패턴 인식 관련 IEEE/CVF 회의 개최에서 pp. 2308-2317: SS1에 의해 발표되었다.\n' +
      '* [19]J. 라자세가난, 가블라코스, 아칸타자와, 말릭(2022)은 3d 외관, 위치 및 포즈를 예측하여 사람을 추적한다. 컴퓨터 비전 및 패턴 인식 관련 IEEE/CVF 회의 개최에서 pp. 2740-2749:SS1에 의해 발표되었다.\n' +
      '*[20]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2021) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '* [21]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR) CVF 콘퍼런스에서 pp 10674-10695: SS1로 지정되었다.\n' +
      '* [22]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[23]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '* [24]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '* [25]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[26]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[27]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[28]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '* [29]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[30]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[31]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[32]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[33]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[34]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[35]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[36]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[37]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[38]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[39]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[40]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[41]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '*[42]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 대한 IEEE/CVF 회의의 진행에서 pp 10684-10695: SS1이 발표했다.\n' +
      '* [43]R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer(2022) 고해상도 이미지 합성은 잠재 확산 모델을 가지고 있다. *[23] 채토완 사하라리아, 윌리엄 찬, 소라바 시세나, 라라 리, 자 휘앙, 에밀릴 리 디멘톤, 카마르 가헤미포, 라파엘 고몬티조 로프, 버쿠 카가골 아얀, 팀 살림산 등의 제작에서 언어 이해가 깊은 사진 텍스트 대 이미지 확산 모델. 신경정보처리시스템_, 2022년 35:36479-36494의 효과.\n' +
      '* [24] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. _arXiv preprint arXiv:2209.14792_, 2022.\n' +
      '* [25] Guy Tvet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* [26] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. _arXiv preprint arXiv:1812.01717_, 2018.\n' +
      '* [27] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. [https://github.com/huggingface/diffusers](https://github.com/huggingface/diffusers), 2022.\n' +
      '* [28] Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for referring human dance generation in real world. _arXiv preprint arXiv:2307.00040_, 2023.\n' +
      '* [29] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-video synthesis. _arXiv preprint arXiv:1808.06601_, 2018.\n' +
      '* [30] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE transactions on image processing_, 13(4):600-612, 2004.\n' +
      '* [31] Zhenzhen Weng, Laura Bravo-Sanchez, and Serena Yeung. Diffusion-hpc: Generating synthetic images with realistic humans. _arXiv preprint arXiv:2303.09541_, 2023.\n' +
      '* [32] Xiangyu Xu and Chen Change Loy. 3d human texture estimation from a single image with transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 13849-13858, 2021.\n' +
      '* [33] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR2021)_, 2021.\n' +
      '* [34] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. _arXiv preprint arXiv:2302.05543_, 2023.\n' +
      '* [35] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 586-595, 2018.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>