<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#CLoVe: Encoding Compositional Language in\n' +
      '\n' +
      '비교적 시각언어 모델\n' +
      '\n' +
      'Santiago Castro\\({}^{1}\\) Amir Ziai\\({}^{2}\\) Avneesh Saluja\\({}^{2}\\) Zhuoning Yuan\\({}^{2}\\) Rada Mihalcea\\({}^{2}\\)\n' +
      '\n' +
      '미시간 대학 - 앤 아버, \\({}^{2}\\)넷플릭스\n' +
      '\n' +
      'sacastro@umich.edu\n' +
      '\n' +
      '넷플릭스에서 인턴으로 일했어요\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근 몇 년 동안 비전과 언어 작업의 성능이 크게 증가했습니다. CLIP와 같은 기본 비전 언어 모델(VLM)은 여러 설정에서 활용되었으며 여러 작업에서 놀라운 성능을 보여주었다. 이러한 모델은 객체 중심 인식에 탁월하지만 어순에 불변하는 것처럼 보이는 텍스트 표현을 학습하여 알려진 개념을 새로운 방식으로 구성하지 못한다. 그러나 GPT-4V와 같은 대규모 단일 스트림 모델을 포함한 VLM이 구성을 성공적으로 식별한다는 증거는 없다. 본 논문에서는 표준 객체 인식 및 검색 벤치마크에 대한 성능을 유지하거나 개선하면서 구성 언어 인코딩에 대한 기존 모델의 성능을 10% 이상 향상시키는 프레임워크를 소개한다. 우리의 코드와 사전 훈련된 모델은 [https://github.com/netflix/clove](https://github.com/netflix/clove]에서 공개적으로 사용할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '지난 몇 년 동안 비전 및 언어 태스크들의 성능에서 상당한 증가가 있었다(Radford et al., 2021; Jia et al., 2021; Rombach et al., 2022; Alayrac et al., 2022; Laurencon et al., 2023). CLIP(Radford et al., 2021)와 같은 VLMs(Vision-Language Models)는 기초 모델로서 직간접적으로 다중 설정에서 활용되었으며, 여러 작업(Bommasani et al., 2021; Ramesh et al., 2021, 2022; Rombach et al., 2022; Castro and Caba, 2022; Li et al., 2023)에서 현저한 성능을 보여주었다.\n' +
      '\n' +
      '이러한 모델은 객체 중심 인식에 탁월하지만 어순에 불변하는 것처럼 보이는 텍스트 표현을 학습한다(Thrush et al., 2022; Yuksekgonul et al., 2023; Castro et al., 2023), 새로운 방식으로 알려진 개념을 구성하지 못한다(Ma et al., 2023; Hsieh et al., 2023). 예를 들어, 그림 1과 같이 CLIP는 ImageNet 작업에서 최고 성능을 갖지만 구성성 벤치마크에서는 뒤처진다.\n' +
      '\n' +
      '언어 구성성은 이미지에서 보다 복잡한 개념을 인식하거나 텍스트 대 이미지 모델을 특정 제약 조건을 가진 새로운 장면을 성공적으로 생성하는데 필수적이다(Hafri et al., 2023). 예를 들어, "여자는 남자에게 함성을 지른다"를 묘사한 이미지에서, 누가 누구에게 소리를 지르고 있는지 정확하게 장면을 이해하기 위해 인식하는 것이 필수적이다.\n' +
      '\n' +
      '그러나 GPT와 같은 대규모 단일 스트림 모델을 포함한 VLM이 있다는 증거는 없다.\n' +
      '\n' +
      '그림 1: 제안된 프레임워크 CLoVe는 사전 훈련된 CLIP 유사 모델의 구성 성능(슈가크레페의 7가지 세립 작업의 평균으로 측정됨)을 크게 향상시키면서 다른 다운스트림 작업(이미지넷으로 측정됨)에 대한 성능을 보존한다. 더 많은 벤치마크와의 비교는 표 1 및 표 2에 제시된다. 기준: REPLACE(Hsieh et al., 2023) 및 NegCLIP(Yuksekgonul et al., 2023).\n' +
      '\n' +
      '4V(OpenAI, 2023), 조성물을 성공적으로 식별한다. 이러한 주장은 테스트 구성성이 계속 오픈 챌린지(Thrush et al., 2022; Yuksekgonul et al., 2023; Ma et al., 2023; Hsieh et al., 2023).1\n' +
      '\n' +
      '각주 1: 자세한 내용은 섹션 2를 참조하십시오.\n' +
      '\n' +
      '이러한 한계를 해결하기 위해 이전 연구에서는 NegCLIP(Yuksekgonul et al., 2023) 및 REPLACE(Hsieh et al., 2023)와 같은 사전 훈련된 VLM의 구성 능력을 증가시키는 기술을 도입했다. 그러나 이러한 방법은 ImageNet(Deng et al., 2009), EuroSAT(Helber et al., 2019, 2018) 및 CIFAR100(Krizhevsky, 2009)에 의해 측정된 보다 일반적인 객체 중심 인식에 대한 성능을 희생하는 상당한 비용이 발생한다. 예를 들어, 도 1에 도시된 바와 같이, NegCLIP는 슈가크레페(Hsieh et al., 2023) 구성성 벤치마크를 다루는 능력이 72.9%에서 82.5%로 증가(사전 훈련된 모델에 비해)한 반면, 동시에 ImageNet(Deng et al., 2009)에서의 성능은 63.4%에서 55.8%로 떨어졌다. 유사하게, Hsieh et al. (2023)은 SugarCrepe 상에서 84.7%의 높은 스코어에 도달하기 위해 REPLACE를 적용했지만, 그 ImageNet 정확도 상에서 52.9%로 상당한 하락의 대가를 치렀다.\n' +
      '\n' +
      '본 논문에서는 그림 1과 같이 기존 투타워 모델이 보다 표준적인 벤치마크에서 성능을 유지하면서 구성 언어를 인코딩하는 능력을 크게 향상시킬 수 있는 프레임워크를 소개한다. 구체적으로, 우리의 기여는 다음과 같다. 먼저, 우리는 **데이터 큐레이션**이 모델이 구성 지식을 처리하는 방법에 상당한 영향을 미칠 수 있음을 보여준다. 둘째, **하드 네거티브**와 함께 교육이 추가 개선을 가져올 수 있음을 확인합니다. 셋째, 기존 작업에서 모델 성능을 보존하기 위해 **모델 패치**를 사용할 수 있음을 실험적으로 보여준다. 마지막으로 이러한 아이디어를 CLoVe라는 새로운 프레임워크에 결합하고 대조적으로 미리 훈련된 VLM**보다 구성성을 크게 향상시킬 수 있음을 보여준다. 사례 연구로, 우리는 우리의 프레임워크가 다른 과제에 대한 성과를 유지하면서 CLIP의 구성 능력을 효과적으로 향상시킬 수 있는 방법을 보여준다. 출판 시, 우리는 다른 사람들이 CLIP 유사 모델 가중치를 훨씬 더 나은 언어 구성 능력을 가진 버전에 대체하기 위해 사용할 수 있는 체크포인트를 제공할 것이다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      'Benchmarking Compositionality. 언어 구성에 대한 모델 성능을 측정하기 위한 몇 가지 프레임워크가 제안되었다. Shekhar et al. (2017)은 정확한 캡션으로부터 하나의 단어를 변경함으로써 생성된 호일 이미지 캡션의 벤치마크를 크래프트하였다. 모델들은 이미지-캡션 쌍이 다른 작업들 중에서 서로 대응하는지를 식별해야 한다. Winoground(Thrush et al., 2022)는 각각 2개의 이미지와 2개의 캡션으로 구성된 400개의 예의 고품질 데이터 세트를 조심스럽게 구축했다. 이 두 자막은 정확한 단어를 포함하지만 여러 전략 중 하나(예: 주제와 대상을 바꾸는 것)에 따라 다른 순서로 포함한다. 모델이 이 테스트를 통과하려면 각 이미지가 올바른 캡션과 일치해야 합니다. 모델은 요소가 반복되지만 다르게 구성되기 때문에 단순히 이미지에서 개념을 인식하는 능력에 의존할 수 없다.\n' +
      '\n' +
      'Diwan et al. (2022)은 Winoground 벤치마크를 통과하는 것은 상식 추론과 작은 물체의 위치와 같은 많은 다른 것들과 함께 작곡 기술을 성공적으로 필요로 한다는 것을 발견했다. 육섹고눌 외(2023)는 위노그라운드가 너무 작아서 통계적으로 유의미한 결론을 도출할 수 없다고 주장하며 단일 이미지, 올바른 캡션, 자동으로 생성된 여러 개의 잘못된 캡션을 가진 예제로 구성된 ARO라는 벤치마크를 구축했다. CREPE(Ma et al., 2023)는 체계성과 생산성 측면에서 구성성을 측정하기 위해 벤치마크를 제작했다. 그것은 다른 현상들 중에서 보이는 화합물과 보이지 않는 화합물을 모두 고려한다. SugarCrepe(Hsieh et al., 2023)는 큰 반면 비문법적 및 비센스적 네거티브 캡션을 피하는 최근의 벤치마크이다. 그들은 이미지를 보지 않고는 텍스트 캡션의 확률을 계산하여 쉽게 해결할 수 없음을 보여주었다. VALSE(Parcalabescu et al., 2022), RareAct(Miech et al., 2020), VLChecklist(Zhao et al., 2022), Cola(Ray et al., 2023), SVO-Probes(Hendricks and Nematzadeh, 2021), 및 CLEVR(Johnson et al., 2017)과 같은 다른 현상들뿐만 아니라 구성성을 고려하는 다른 벤치마크들이 또한 생성되었다.\n' +
      '\n' +
      '구성성을 향상시키는 방법들. 몇몇 작업들은 VLM들이 구성들을 성공적으로 인식할 수 없다는 것을 보여주었다 (Shekhar et al., 2017; Miech et al., 2020; Parcalabescu et al., 2022; Thrush et al., 2022; Hendricks and Nematzadeh, 2021; Yuksekgonul et al., 2023; Castro et al., 2023; Ma et al., 2023). 이러한 이유로, CLIP(Radford et al., 2021)가 개념을 구성하는 방식을 개선하기 위해 NegCLIP(Yuk sekgonul et al., 2023)가 제안되었다. 학습 배치에서 자막을 취하여 딱딱한 부정적인 텍스트를 추가하고 정확한 단어가 있지만 순서가 다른 문장을 자동으로 생성하는 것으로 구성된다. 이 접근법은 모델이 임의의 순서(배치 내의 다른 네거티브 캡션들뿐만 아니라)의 정확한 단어들에 비해 정확한 순서로 이미지와 캡션을 구별하게 한다. Hsieh et al. (2023)은 NegCLIP 및 CREPE (Ma et al., 2023)를 기반으로 하고 랜덤 네거티브들을 생성하는 세 가지 방식들: REPLACE, SWAP, 및 NEGATE를 제안한다. 이 모든 방법은 문장의 장면 그래프 표현에서 시작하여 그 위에 작동한다. 전체적인 결과가 가장 좋았던 REPLACE는 단일 원자 대체를 수행한다. SWAP는 장면 그래프 내에서 두 개의 원자를 교환한다. 마지막으로, NEGATE는 부정어(즉, _no_ 또는 _not_)를 도입한다. 우리는 NegCLIP(Yuksekgonul et al., 2023)와 REPLACE(Hsieh et al., 2023)를 기반으로 합성 생성 캡션을 사용하여 확장하고 모델 패치(Ilharco et al., 2022)를 적용하여 치명적인 망각을 방지할 것을 제안한다. 우리가 아는 한, 우리는 다른 다운스트림 작업에서 제로 샷 성능을 유지하면서 대조적으로 훈련된 모델의 구성 기술을 크게 향상시키는 첫 번째 접근법을 소개한다.\n' +
      '\n' +
      'Cap 및 CapPa(Tschannen et al., 2023)는 VLM을 훈련시키기 위해 (CLIP에서와 같이) 대비 학습 대신에 캡셔닝을 채용하는 두 개의 최근에 소개된 모델이다. Tschannen et al. (2023)은 ARO (Yuksekgonul et al., 2023) 및 SugarCrepe (Hsieh et al., 2023)에 의해 측정된 바와 같이 조성성에 대해 우수한 성능을 나타냄을 보여주었다. 이러한 모델은 캡셔닝에 의존하므로 이미지가 주어진 텍스트의 확률을 계산하는 데 의존하기 때문에 검색 및 분류에 비효율적이다. ARO의 경우, 그들은 이미지를 보지 않고도 높은 성능을 달성할 수 있다는 것을 보여주었다(그들은 그것을 "블라인드 디코더"라고 부른다). 슈가크레페의 경우 저자는 이 특정 기준선을 계산하지 않았다. 따라서 이러한 모델이 구성을 성공적으로 처리하는 정도를 추론할 수 없다. 우리의 접근 방식은 검색 및 분류에 효율적인 대조적인 2-타워 모델 위에 구축되고, 모든 텍스트가 동등하게 가능성(이미지 캡션에서와 달리)이 있는 것과 같이 일반적으로 중요하지 않은 텍스트의 확률을 계산하는 것에 의존하지 않기 때문에 그들과 다르다.\n' +
      '\n' +
      '##3 CLoVe: 대조 VLM의 구성성 증대를 위한 프레임워크\n' +
      '\n' +
      '이전 모델에서 관찰된 구성성 한계를 해결하기 위해 대조 VLM 개발의 세 가지 주요 측면인 데이터 큐레이션, 대조 학습 및 모델 조정을 해결하는 전략을 제안한다. 기존의 사전 훈련된 대비형 VLM의 장점을 활용하고 언어 구성 기술로 이를 향상시키는 프레임워크인 CLoVe를 소개한다. 도 2는 개요를 도시한다.\n' +
      '\n' +
      'CLoVe는 하기 단계를 포함하며, 하기에서 더 상세히 제시된다:\n' +
      '\n' +
      '**3.1 합성 캡션.**: 합성 데이터 생성은 학습 데이터를 확대하는데 효과적으로 사용될 수 있다. 우리는 합성 캡션이 있는 큰 데이터 세트를 사용한다.\n' +
      '**3.2 하드 네거티브**: 대조 VLM은 네거티브 트레이닝 데이터의 가용성에 의존한다. 우리는 무작위로 생성된 하드 텍스트 네거티브를 추가한다.\n' +
      '\n' +
      '그림 2: 우리의 CLoVe 프레임워크는 세 단계로 구성된다. 먼저, 큰 이미지 데이터세트에 대한 합성 캡션을 획득한다. 둘째, 사전 훈련된 대조적 VLM에 대해 하드 네거티브 텍스트와 함께 미세 조정한다. 셋째, 원래 모델을 미세 조정된 모델로 패치합니다.\n' +
      '\n' +
      '데이터 세트 및 구성 기능이 향상된 미세 조정 모델을 훈련합니다.\n' +
      '3.3 모델 패칭.모델 패칭을 통해 미리 학습된 모델과 미세 조정된 모델을 결합한다. 패칭을 통해 미세 조정 모델로 얻은 구성성을 유지하면서 이전에 지원된 작업에서 미리 훈련된 모델 성능을 복구할 수 있다.\n' +
      '\n' +
      '### Synthetic Captions\n' +
      '\n' +
      '합성 캡션은 학습 데이터 세트 크기와 캡션의 품질 사이에 큰 혼성화를 제공한다. LAION-COCO(Schuhmann et al., 2022)는 BLIP ViT-L/14(Li et al., 2022)로 캡션된 LAION-5B(Schuhmann et al., 2022)의 20억 크기의 영어 서브세트의 이미지를 가진 6억 개의 데이터세트인 LAION-COCO(Schuhmann et al., 2022)를 활용하여 COCO에서 미세 조정되고 OpenAI 사전 훈련된 CLIP(Radford et al., 2021; ViT-L/14 및 RN50x64)의 두 버전으로 필터링되었다. 비록 캡션이 스타일(일반적으로 COCO 캡션의 스타일을 따름)이 제한적이지만, LAION-COCO 저자는 합성적으로 생성된 캡션이 인간이 쓴 캡션과 유사한 품질을 갖는다는 것을 발견했다. 우리는 이러한 캡션이 원래 데이터 세트(LAION)의 캡션보다 시각적 정보를 설명하는 데 더 중점을 두고 있다고 믿는다. 교육 데이터 세트의 삭제는 섹션 4.3을 참조하십시오.\n' +
      '\n' +
      '### Hard Negatives\n' +
      '\n' +
      '텍스트 하드 네거티브는 모델이 캡션에서 어떻게 사용되는지에 따라 이미지와 관련이 있는지 식별할 필요가 있기 때문에 각 단어의 의미를 더 잘 학습하도록 모델을 강제할 수 있다. 육섹고눌 외(2023)는 이미지 캡션 단어들을 재배열하여 일괄적으로 각 예제에 대한 하드 네거티브 텍스트를 생성하는 CLIP의 트레이닝 절차의 확장인 NegCLIP를 제안하였다. 이렇게 생성된 네거티브들은 학습 목표의 네거티브 테스트 세트들 내에 포함된다. Hsieh et al. (2023)은 REPLACE라는 대안을 제안했고, 그러한 네거티브들이 신중하게 선택된 단일-워드 대체들로부터 생성된다면, 모델이 더 나은 구성성 기술을 달성할 수 있음을 보여주었다. 이러한 대체들은 먼저 문장을 장면 그래프로 파싱한 후, WordNet(Fellbaum, 2010)2를 이용하여 반의어 또는 공저음으로부터 대체 단어를 선택함으로써 얻어진 엔티티들, 관계들 또는 속성들 중 하나에 대해 수행된다. 이러한 방법들은 고품질 캡션들에 의존한다. 그렇지 않으면 생성된 음은 시각적으로 인식할 수 없거나 대부분 비문법적이거나 무의미한 변화를 갖게 되며 모델의 다운스트림 성능에 심각한 영향을 미칠 것이다. 카드 소지자의 이미지를 동반하는 LAION의 다음 예를 들어보자: _"5x Orange Ball Wedding Party PLACE CARD HOLDER Table Name Memo Paper Note Clip."_REPLACE를 적용하면 문장을 올바르게 구문 분석할 수 있다고 가정하면 "table"이라는 단어가 "bed"로 대체될 수 있다. 그러나, 이는 표가 시각적으로 인식될 수 없는 포함된 캡션이 추가 컨텍스트 정보이기 때문에 부정적이지는 않을 것이다. 이러한 변화는 모델의 훈련 과정에 더 많은 잡음을 도입할 것이다.\n' +
      '\n' +
      '각주 2: 더 정확하게는, 이 방법은 그랜드-코-하이퍼마임을 공유하는 단어들을 찾는 것을 제안한다.\n' +
      '\n' +
      '이러한 이유로 본 연구에서는 COCO 캡션(Lin et al., 2014; Chen et al., 2015) 데이터셋을 사용하였다. COCO는 이를 설명하는 고품질 인간 주석 캡션과 함께 이미지로 구성된다. 그럼에도 불구하고, 600,000개의 이미지-텍스트 쌍으로 COCO는 일반적으로 사용되는 이미지-텍스트 트레이닝 데이터세트보다 최소 3배 작다. 이 문제는 학습을 제한하고 모델을 과도하게 적합시킵니다. 또한, COCO는 제한된 수의 객체 및 동작을 제시한다. ImageNet-1k의 1000개의 오브젝트 클래스 중 700개는 COCO에 존재하지 않는다(Venugopalan et al., 2017). 우리는 이러한 하드 네거티브 기술을 LAION-COCO(Schuhmann et al., 2022)(이전 하위 섹션에 소개됨)와 같은 합성 캡션 데이터 세트와 결합하는 것을 제안한다.\n' +
      '\n' +
      '### Model Patching\n' +
      '\n' +
      '모델 패칭(Ilharco et al., 2022)은 목표 태스크에 대한 성능을 유지하면서 이전에 지원된 태스크에 대한 성능을 미세 조정된 모델이 복구하도록 한다. NegCLIP(Yuksekgonul et al., 2023) 및 REPLACE(Hsieh et al., 2023)는 언어 구성 기술을 상당히 향상시키기 위해 모델을 미세 조정한다. 그러나 그 대가로 그들은 이미지넷 성능으로 측정한 일반적인 객체 인식에 대한 성능을 희생한다. 이러한 이유로 우리는 이러한 방법 중 하나를 적용하고 후속적으로 모델 패칭을 사용하는 것을 제안한다. 이 절차는 사전 훈련된 모델과 미세 조정된 모델 간의 가중치 공간 평균을 수행하는 것으로 구성된다. 구체적으로, 미리 학습된 모델 가중치\\(w_{i}^{PT}\\)와 미세 조정된 모델 가중치\\(w_{i}^{FT}\\)에 대해 가중치 평균을 계산하여 새로운 모델 가중치\\(w_{i}\\):\n' +
      '\n' +
      '\\[w_{i}=(1-\\alpha)w_{i}^{PT}+\\alpha w_{i}^{FT}\\tag{1}\\\\alpha\n' +
      '\n' +
      '섹션 4.3에서 이 접근법이 객체 인식 성능을 유지하면서 구성 특성을 얻는 데 도움이 됨을 보여준다.\n' +
      '\n' +
      '##4 CLIP 사례연구\n' +
      '\n' +
      '본 프레임워크의 효율성을 입증하기 위해 가장 널리 사용되는 대조 VLM 중 하나인 CLIP Radford et al.(2021)에 적용한다. 이전 연구에서 기존 표준 작업에서 구성 능력과 모델 성능 간의 상충 관계를 강조했다는 점을 감안할 때, 우리는 객체 인식 및 이미지 대 텍스트 및 텍스트 대 이미지 검색을 위한 표준 벤치마크뿐만 아니라 도전적인 구성 벤치마크에 대해 평가를 수행한다. CLoVe 프레임워크의 세 가지 주요 구성 요소가 수행하는 역할에 대한 통찰력을 얻기 위해 세 가지 절제 연구를 수행하여 (1) 합성 캡션의 역할을 결정하고 (2) 훈련 중 하드 네거티브 텍스트를 사용하는 것이 구도의 인식 성능을 향상시키는지 평가하고 (3) 하드 네거티브 텍스트로 훈련 후 원래 모델을 패치하는 것의 중요성을 테스트한다. 달리 명시되지 않는 한 모든 평가는 제로샷이며, 이는 벤치마크별 훈련 분할에 대해 도메인 내 미세 조정을 수행하지 않는다는 것을 의미한다.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '사전 훈련된 모델. 처음부터 시작하기보다는 기존 대비형 VLM의 구성 능력을 향상시키는 것을 목표로 한다. 이 작업은 CLIP 대조 언어-이미지 사전 훈련; Radford et al.(2021)을 사용하며, 사전 훈련 방법은 비전 또는 언어를 포함하는 분류 및 검색 작업에 대해 인상적인 제로 샷 성능을 보여준다. 그것은 대규모 약지도 데이터 세트를 활용하여 공동 공간에서 이미지와 텍스트 표현을 학습하는 것을 포함한다. 이러한 데이터 세트에는 다양한 대응 정도를 갖는 이미지-텍스트 쌍이 포함된다. 각 이미지에 대해, 모델은 InfoNCE objective Oord et al.(2018)을 채용함으로써, 이 텍스트 및 \\(N-1\\) 다른 텍스트(음성 샘플)의 랜덤 샘플을 포함하는 세트로부터 대응하는 포지티브 텍스트를 학습해야 한다. 유사하게, 모델은 어떤 이미지가 주어진 텍스트에 대응하는지를 식별해야 한다. CLIP는 미니 배치 경사 하강으로 훈련되며, 여기서 이 목표는 \\(N\\) 크기의 배치에서 각 쌍에 적용되고 음수는 일반적으로 배치의 나머지에서 조달된다.\n' +
      '\n' +
      '구현 세부사항.달리 언급되지 않는 한, 구현 세부사항은 다음과 같다. We write our code on Python 3.10 using PyTorch Paszke et al. (2019) v2.1, starting from open_clip\'s Ilharco et al. (2021); Cherti et al. (2023) codebase. 실험은 AdamW optimizer Loshchilov와 Hutter(2019)를 이용하여 수행하였으며, 1e-6까지의 2000단계에 대한 선형 학습률 워밍업 후 코사인 스케줄 Loshchilov와 Hutter(2017)로 감쇠하였다. 우리는 0.1의 가중치 감쇠를 사용한다. 우리의 초기 사전 훈련된 모델은 OpenAI Radford et al.(2021)의 ViT-B-32이다. 최대 10\\(10\\,000\\) 샘플의 파편에서 무작위로 샘플링하여 10억 개의 예를 통해 모델을 훈련하며, 각 샘플의 최종 크기는 다운로드 시점의 이미지 가용성에 따라 달라진다. LAION-400M Schuhmann et al.(2021)의 80%, LAION-COCO Schuhmann et al.(2022)의 80%, COYO-700M Byeon et al.(2022)의 60%의 영상을 성공적으로 다운로드하였다. 텍스트 캡션은 영어로 되어 있습니다. 우리는 4일 반 동안 8x A100 Nvidia GPU와 96개의 CPU 코어(AWS에서 p4d.24xlarge)를 가진 하나의 노드를 사용한다. 배치 크기는 GPU당 256입니다.\n' +
      '\n' +
      '학습률 선택은 학습 속도가 너무 느리거나 훈련 손실이 증가하는지 확인하기 위해 여러 예비 실험을 기반으로 했다. 학습 단계와 샘플은 방법이 학습하고 수렴할 수 있는 충분한 시간을 제공하기 위해 선택되었다. 총 배치 크기와 계산 예산의 선택은 가용도 계산과 CLIP 유사 방법이 큰 배치 크기가 필요하다는 점을 고려하여 결정되었다. 보고된 모든 실험은 계산 비용이 많이 들기 때문에 단일 실행을 기반으로 한다.\n' +
      '\n' +
      '우리는 주로 이 부분에 대한 코드를 사용할 수 없기 때문에 다음과 같은 변경 및 결정으로 REPLACE Hsieh et al.(2023)을 다시 구현했다. 생성된 음수들을 필터링하기 위해 BERT Devlin et al.(2019)을 사용하는 것을 건너뛰고, 대신 상황화된 모델로 확률 계산의 1차 근사인 새로운 단어들의 빈도에 기초하여 단어들을 교체하는 것을 진행한다. 교체의 경우 저자가 전치사를 언급하지 않고 제공된 데이터에서 교체된 것을 발견했기 때문에 전치사를 교체했다. 대체어의 경우, 나머지 문장(예: 동사에 대한 사람, 명사에 대한 숫자)을 접합하고 대체어와 유사한 케이싱을 사용하여 문장을 존중하려고 한다. 우리는 spaCy Honnibal et al. (2020) v3.7.2 (the model en_core_web_sm)와 pyinflect v0.5.1을 사용했다. 우리는 다른 Scene Graph Parsing 구현, SceneGraphParser v0.1.0을 사용했다. 우리는 WordNet Fellbaum (2010), leverageaging NLTK Bird et al. (2009) v3.8.1에서 그들의 lemmas의 공통점을 살펴봄으로써 단어를 잠재적인 동의어로 대체하는 것을 피했다. 우리는 원저자들이 보고한 것과 동일한 숫자를 재현할 수 있었다. 우리는 누구나 쉽게 결과를 재현하고 구축할 수 있도록 코드를 공개적으로 사용할 수 있도록 할 것입니다.\n' +
      '\n' +
      '우리는 섹션 4.3의 절제를 기반으로 모델 패칭에 대한 \\(\\alpha=0.6\\)을 설정했다.\n' +
      '\n' +
      'CLOVe를 사용하여 CLIP에 구성을 가져오는###\n' +
      '\n' +
      '우리는 그림 1과 같이 Clove 프레임워크로 향상된 CLIP 모델을 여러 기준선과 비교한다: CLIP+Clove는 사전 훈련된 CLIP 모델과 비교할 때 도전적인 구성성 벤치마크인 SugarCrepe Hsieh et al.(2023)에서 평균 10%의 절대 개선을 가져오며, 모두 이미지넷 성능을 1% 이내로 유지한다. 또한, 모델 패칭 단계를 적용하지 않았을 때, 본 모델이 다른 모델보다 구성성에 대해 더 우수한 성능을 보임을 보인다.\n' +
      '\n' +
      '표 1에서는 ARO Yuksekouni et al. (2023), SugarCrepe Hsieh et al. (2023) (3개의 거친 조립 작업에 걸쳐), SVOProbes Hendricks and Nematzadeh (2021)의 세 가지 구성성 벤치마크에서 향상된 CLIP+Clove 모델의 비교를 보여준다. SugarCrepe의 경우 매크로 평균을 사용하여 Tschannen et al.(2023)과 같이 거친 작업 결과를 계산하고 원래 논문과 달리 작업 샘플 크기에 중요성을 부여하는 대신 글로벌 현상을 측정하는 데 관심이 있기 때문이다. 각 세립 작업에 대한 슈가크레페에 대한 수행은 부록 A를 참조하십시오.\n' +
      '\n' +
      '모델 구성성을 높이는 방법을 고안할 때 이전 작업의 주요 관심사는 다른 작업에 대한 성능 손실이었기 때문에 객체 인식 및 이미지 대 텍스트 및 텍스트 대 이미지 검색 작업에 대한 CLIP+Clove 모델 성능을 평가한다.\n' +
      '\n' +
      '표 2에서, 우리는 이미지넷 Deng et al. (2009), 스탠포드 카스 크라우스 et al. (2013), CIFAR10 Krizhevsky (2009), CIFAR100 Krizhevsky (2009), MNIST LeCun et al. (1994), EuroSAT Helber et al. (2019, 2018), Oxford Flowers 102 Nilsback and Zisserman (2008), Describable Textures (DTD) Cimpoi et al. (2014), UCF101 Soomro et al. (2012), 및 HMDB51 Kuehne et al. (2011). Radford et al.(2021)에 이어서, 우리는 클래스당 평균을 사용하는 Oxford Flowers 102를 제외하고 상위-1 정확도 메트릭을 사용한다.\n' +
      '\n' +
      '표 3에서 제로샷 텍스트에 대한 결과를 제시한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c|c c c|c c c|c}  & \\multicolumn{3}{c|}{ARO} & \\multicolumn{3}{c|}{SugarCrepe} & \\multicolumn{3}{c|}{SVO-Probes} & \\multicolumn{1}{c}{} \\\\  & Attr. & Rel. & C-Ord. & F-Ord. & Repl. & Swap & Add. & Subj. & Verbs & Obj. & avg. \\\\ \\hline pre-trained & 63.5 & 59.8 & 47.7 & 59.9 & 80.1 & 62.3 & 72.8 & 84.0 & 79.3 & 87.8 & 69.7 \\\\ \\hline NegCLIP & 70.5 & **80.1** & 87.0 & 90.1 & 85.1 & 75.3 & 85.9 & 90.9 & 84.7 & 92.3 & 84.2 \\\\ REPLACE & **71.2** & 72.9 & 80.1 & 86.7 & 88.2 & 74.8 & 89.5 & **92.0** & 84.6 & 93.0 & 83.3 \\\\ \\hline CLIP+CLove w/o patching & 69.0 & 77.4 & **91.7** & **93.6** & **88.6** & **76.1** & **90.5** & 88.2 & 83.7 & 91.6 & **85.0** \\\\ CLIP+CLove (\\(\\alpha=.6\\)) & 69.7 & 72.7 & 86.6 & 92.1 & 87.0 & 74.6 & 85.8 & 90.5 & **86.4** & **93.3** & 83.9 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 제로샷 조성 평가 결과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c c c c c|c}  & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline pre-trained & **63.4** & **59.7** & 89.8 & 64.2 & **48.9** & 50.5 & **66.6** & **44.4** & 69.3 & 44.3 & 60.1 \\\\ \\hline NegCLIP & 55.8 & 45.6 & 85.9 & 60.9 & 45.3 & 32.9 & 55.9 & 39.0 & 65.6 & 42.7 & 53.0 \\\\ REPLACE & 52.9 & 42.7 & 84.6 & 60.2 & 36.6 & 34.3 & 51.9 & 34.5 & 62.2 & 40.9 & 50.1 \\\\ \\hline CLIP+CLove w/o patching & 53.1 & 48.7 & 88.5 & 62.0 & 40.4 & 46.9 & 43.2 & 36.3 & 62.3 & 41.0 & 52.2 \\\\ CLIP+CLove (\\(\\alpha=.6\\)) & 62.8 & 56.8 & **91.4** & **68.1** & **48.7** & **57.4** & 61.1 & 41.2 & **70.4** & **46.0** & **60.4** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: Zero-shot 분류 결과.\n' +
      '\n' +
      '이미지 간 및 이미지 간 텍스트 검색 작업 사용된 데이터셋은: Conceptual Captions Sharma et al. (2018)(CC3M), Distinct Describable Moments Anne Hendricks et al. (2017)(DiDeMo), MSR-VTT Xu et al. (2016), and YouCook2 Zhou et al. (2018)(YC2). 결과는 Radford 등에 의해 사용된 동일한 메트릭인 Recall@5를 측정함으로써 제시된다(2021). 분류와 달리, 우리의 접근법은 평균 4% 이상 개선된다(절대). 이러한 개선은 검색 캡션이 클래스 레이블보다 더 길고 복잡하기 때문에 모델의 풍부한 텍스트 표현을 감상할 수 있다고 추측한다. 우리는 또한 분류 작업에서 클래스당 여러 프롬프트를 사용하면 다른 모델의 텍스트 표현 노이즈를 평균한다고 믿는다(이에 대한 분석을 위해 부록 B를 참조). 전반적으로, 우리는 REPLACE와 동등한 성능을 가진 텍스트 투 이미지의 DiDeMo를 제외하고 CLIP의 CLoVe 프레임워크를 사용하여 모든 작업 및 메트릭에서 더 나은 성능을 얻는다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '합성 캡션의 중요성.우리는 구성 성능을 모델링하기 위해 훈련 데이터 세트 품질이 필수적이라고 가정한다. 예를 들어, 대조 VLM을 훈련시키기 위해 일반적으로 사용되는 데이터세트인 LAION Schuhmann et al.(2021)에서, 임의의 이미지에 묘사된 시각적 개념들에 쉽게 매핑될 수 없는 과도한 정보를 제시하는 예를 찾을 수 있는데, 예컨대: _"플래티넘 댄스 아카데미 티셔츠. 주문은 9월 26일 금요일까지 배치되어야 한다. 대략 2주 이하의 배송."__2주 이하의 배송.\n' +
      '\n' +
      'COCO Lin et al. (2014); Chen et al. (2015)와 같은 고품질 주석을 갖는 데이터 세트가 사용될 수 있지만, 그러한 데이터 세트는 전형적으로 작다(백만 샘플 미만). 3.1절에서 설명한 대로 고품질 데이터와 대규모 데이터 세트를 합성 캡션을 사용하여 하이브리드 접근법을 얻을 수 있다. 우리는 이 데이터 세트를 LAION-400M 또는 COCO와 직접 비교하는 것과 데이터 세트를 결합하는 두 가지 방법, 즉 a) 연결 및 b) 동일한 확률로 샘플링하는 데 관심이 있다.3 LAION과 COCO를 결합하는 이러한 전략이 LAION-COCO 데이터 세트와 완전히 다르다는 점에 유의한다. 또한 LAION-400M과 유사하게 구성된 대규모 데이터 세트인 COYO-700M 변 등(2022)을 고려한다.\n' +
      '\n' +
      '각주 3: 노트 레온-400M은 COCO보다 약 700배 더 크다.\n' +
      '\n' +
      '표 4는 네거티브를 사용하지 않고 다른 데이터 세트에 대해 사전 훈련된 CLIP 모델을 미세 조정하는 성능을 비교한다. 이 표와 후속 표에서 최상의 결과는 **굵은**에 있으며 밑줄은 최상의 1% 이내의 결과를 나타낸다. LAION-COCO Schuhmann et al.(2022)은 ARO에 큰 마진을 가지면서 전반적으로 가장 좋은 결과를 제시하고 있다. 이 벤치마크에 대해 사전 훈련된 모델을 크게 능가하는 유일한 제시된 데이터 세트이다. SugarCrepe 벤치마크의 경우, 우리는 tha\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c} \\hline \\hline Fine-tuning dataset & Attr. & Rel. & C-Ord. & F-Ord. \\\\ \\hline \\hline \\multicolumn{5}{c}{_Without hard negative texts_} \\\\ \\hline COYO & 63.6 & 55.4 & 34.8 & 43.4 \\\\ LAION (L) & 64.9 & 64.0 & 40.2 & 47.0 \\\\ COCO (C) & 62.5 & 61.6 & **73.8** & 39.8 \\\\ concat. L \\& C & **65.9** & 59.0 & 43.7 & 50.3 \\\\ sample unf. L \\& C & 64.6 & 55.7 & 59.8 & 29.7 \\\\ LAION-COCO & 65.4 & **66.0** & 70.5 & **76.9** \\\\ \\hline \\hline \\multicolumn{5}{c}{_With hard negative texts_} \\\\ \\hline COYO & 69.5 & 75.6 & 71.7 & 79.7 \\\\ LAION (L) & 67.9 & 72.6 & 78.3 & 85.4 \\\\ COCO (C) & **70.2** & 67.6 & 90.9 & 74.5 \\\\ concat. L \\& C & 70.1 & 76.2 & 83.4 & 88.6 \\\\ sample unf. L \\& C & 69.9 & 71.6 & 82.7 & 60.8 \\\\ LAION-COCO & 69.0 & **77.4** & **91.7** & **93.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 하드 네거티브 텍스트가 있거나 없는 다른 데이터 세트로 CLIP를 미세 조정하는 제로 샷 성능.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c c|c} \\hline \\hline  & \\multicolumn{4}{c|}{Text-to-Image/Video} & \\multicolumn{4}{c|}{Image/Video-to-Text} & \\multirow{4}{*}{avg.} \\\\ \\cline{2-2} \\cline{5-10}  & & & & & & & & \\\\ \\cline{1-1} pre-trained & 52.3 & 48.4 & 54.9 & 13.8 & 51.0 & 40.7 & 50.8 & 11.3 & 40.4 \\\\ \\hline NegCLIP & 50.3 & 48.8 & 56.9 & 13.9 & 47.9 & 41.9 & 48.2 & 09.8 & 39.7 \\\\ REPLACE & 49.6 & **50.2** & 56.2 & 13.6 & 44.8 & 40.8 & 47.9 & 09.7 & 39.1 \\\\ \\hline CLIP+CLoVe w/o patching & 47.3 & 35.0 & 53.1 & 11.4 & 43.4 & 37.8 & 42.7 & 08.0 & 34.8 \\\\ CLIP+CLoVe (\\(\\alpha=.6\\)) & **58.7** & 49.9 & **60.5** & **15.7** & **57.5** & **47.5** & **54.5** & **12.4** & **44.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 제로샷 검색 결과.\n' +
      '\n' +
      '사전 훈련된 모델에 대한 개선 흥미롭게도 Betker et al.(2023)도 텍스트 대 이미지 생성 모델에 도움이 되는 합성 캡션을 발견했다. 합성 캡션은 이러한 모델이 입력 텍스트와 더 잘 일치하는 이미지를 생성하는 데 도움이 된다는 것을 보여줍니다.\n' +
      '\n' +
      'Hard Negatives.Yuksekgonul et al. (2023); Hsieh et al. (2023)은 훈련 과정의 일부로 무작위로 생성된 텍스트 네거티브를 사용하는 것이 사전 훈련된 모델의 언어 구성 능력을 상당히 향상시킬 수 있음을 보여주었다. REPLACE(Hsieh et al., 2023)를 적용하여 LAION-COCO 데이터셋(Schuhmann et al., 2022)과 함께 랜덤하게 생성된 하드 네거티브 텍스트를 획득하고, 이를 네거티브가 없는 미세 조정과 비교한다. 표 5의 결과를 제시한다. 이 설정에서, 우리는 ARO 벤치마크(Yuksekgonul et al., 2023)에 의해 측정된 바와 같이, 네거티브들을 사용하는 것이 그것들을 사용하지 않는 것보다 성능을 향상시킨다는 것을 관찰할 수 있다(그 작업들은, 우리가 그들에게 보여주는 순서: VG-Attribution, VG-Relation, COCO-Order, 및 Flickr30k-Order).\n' +
      '\n' +
      'Model Patching의 중요성.Yuksekgonul et al.(2023); Hsieh et al.(2023)은 ImageNet(Deng et al., 2009)과 같은 보다 표준적인 객체 중심 벤치마크에서 모델의 성능을 상당히 해침으로써 CLIP의 구성성을 향상시키는 기존의 방법들이다.\n' +
      '\n' +
      '그림 3은 구성 벤치마크와 객체 중심 벤치마크 모두에 대해 이 값을 변경하는 효과를 나타낸다. \\(\\alpha\\)이 약 0.4-0.7일 때, 모델은 두 가지 모두에 대해 좋은 성능을 보인다.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      '본 논문에서는 CLoVe(Contrastive VLM)를 기존의 방법과 달리 다른 태스크에 대한 성능을 유지하면서 미리 훈련된 대비형 VLM의 구성성을 상당히 향상시키기 위한 프레임워크를 도입하였다. 우리의 접근법은 품질과 양 사이의 우수한 트레이드오프를 제공할 수 있기 때문에 합성 캡션 이미지를 활용하여 미세 조정 대조 VLM과 하드 네거티브 텍스트를 결합한다. 그 후, 그것은 원래 모델을 미세 조정된 모델로 패치하여 다른 작업에 대한 성능을 유지하면서 두 가지 세계 중 최고의 구성 기술을 전달합니다.\n' +
      '\n' +
      '우리는 CLoVe가 구성 관련 및 비구성 관련 모두에서 여러 벤치마크에서 CLIP 유사 모델의 성능을 향상시킨다는 것을 실험적으로 보여주었다. 우리는 프레임워크의 다양한 구성 요소를 제거하고 데이터 품질, 교육에서 하드 네거티브 사용 및 모델 패치라는 중요성을 보여주었다.\n' +
      '\n' +
      '우리의 코드와 사전 훈련된 모델은 [https://github.com/netflix/clove](https://github.com/netflix/clove]에서 공개적으로 사용할 수 있다. 우리의 코드는 CLIP와 같은 가중치를 우리가 제공하는 가중치로 쉽게 대체할 수 있게 하여 언어 구성 성능을 상당히 높일 것이다.\n' +
      '\n' +
      '## Limitations\n' +
      '\n' +
      '우리의 작업은 다음과 같은 방식으로 제한된다.\n' +
      '\n' +
      '우리의 접근은 구성성 문제를 완전히 해결하지는 못한다. 구성성 벤치마크에 대한 성과는 여전히 인간이 보고한 성과와 격차를 나타낸다.\n' +
      '\n' +
      '도 3: 모델 패칭을 객체 중심 벤치마크(ImageNet, Deng et al., 2009; x-axis) 및 조성 벤치마크(ARO, Yuksekgonul et al., 2023; the four y-axes represent its four tasks) 모두에 적용하는 효과, 평균에서 가중치의 값을 변화시킬 때, \\(\\alpha\\) \\(\\alpha\\) 값은 0(사전 훈련된 모델)에서 0.05 단위로 1(미세 조정 모델)까지 다양하며 선은 이러한 점을 연결한다. 우리는 이미지넷에서 0-shot 성능이 좋고 \\(\\alpha\\)이 0.4-0.7일 때 합성성이 좋은 모델을 얻을 수 있다. 4개의 y축은 미리 훈련된 모델 포인트와 미세 조정된 모델 포인트가 일치하도록 조정되어 선이 서로 어떻게 달라지는지에 초점을 맞추었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c}  & Attr. & Rel. & C-Ord. & F-Ord. \\\\ \\hline pre-trained & 63.5 & 59.8 & 47.7 & 59.9 \\\\ \\hline fine-tuned & 65.4 & 66.0 & 70.5 & 76.9 \\\\ + negatives & 69.0 & **77.4** & **91.7** & **93.6** \\\\ \\hline + negatives* & **69.4** & 75.4 & 77.5 & 86.1 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 구도 인식에 대한 제로 샷 성능을 향상시키기 위해 네거티브를 사용하는 것의 중요성. *마지막 행은 배치 크기의 절반을 사용한 결과를 보여주는데, 네거티브를 사용하는 것이 배치 크기를 효과적으로 두 배로 증가시킨다는 점을 감안할 때 전체 장치 메모리가 동일한 경우에도 이득이 있다.\n' +
      '\n' +
      '사용된 벤치마크 각각과 관련된 문서입니다.\n' +
      '\n' +
      '합성 캡션을 사용하면 원하지 않는 소음이 발생할 수 있습니다. 이미지 캡셔너들은 때때로 부정확한 개념들 또는 그러한 객체들에 대한 부정확한 설명들을 도입하면서 환각을 볼 수 있다. 장면에 말이 4마리 있는 경우와 같이 양에는 특히 해당되지만 합성 캡션에는 3개가 언급된다. 향후 연구는 합성 자막 품질을 향상시키기 위한 방법에 초점을 맞출 수 있다.\n' +
      '\n' +
      '우리는 패치된 모델의 성능이 다양한 인구 통계에 미치는 영향을 연구하지 않았다. 모델이 패치된 후 일부 작업 수행(구성 여부)에서 일부 인구 통계가 잘못 표시되는 경우일 수 있습니다. 사용자는 이러한 측면에 주의해야 합니다.\n' +
      '\n' +
      '본 연구에서는 분류 및 검색의 효율성 때문에 2-타워 모델에 초점을 맞춘다. 우리는 향후 작업을 위해 단일 타워 모델에 대한 연구를 남겨둔다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '우리는 파블로 델가도와 넷플릭스의 교육 플랫폼 팀이 넷플릭스의 전산 자원을 사용하는 데 도움을 준 것에 감사한다. 여러 가지 통찰력 있는 토론에 대해 무하마드 칼리파, 오아나 이그나트, 앤드류 리, 미시간 대학의 언어 및 정보 기술 그룹에 감사드립니다. 이 자료는 부분적으로 자동차 연구 센터("ARC")에서 지원하는 작업에 기초한다. 이 자료에 표현된 의견, 결과, 결론 또는 권장 사항은 저자의 의견이며 ARC 또는 기타 관련 기관의 견해를 반드시 반영하는 것은 아니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]J. 베커고 징태 브룩스 J. Wang, L. 이림 오양, 장정, 이영 곽우 마나스라, P. 다리왈, C. 추영 Jiao, and A. Ramesh (2023) improved image generation with better captionions. 외부 링크: 2303.03032 인용: SS1.\n' +
      '*[2]S. Bird, E. Klein, and E. Loper (2009) natural language processing with python: analyzing text with the natural language toolkit. 참고: 오라일리 미디어 외부 링크: ISBN 978-3-319-3495-3 인용: SS1.\n' +
      '*[3]R. Bommasani, D. A. Hudson, E. Adeli, R. 알트만 아로라 Von Arx, M. S. Bernstein, J. Bohg, A. Bosseltut, E. Brunskill, E. Brynjolfsson, S. Buch D. Card, R. Castellon, N. S. Chatterji, A. S. Chhen, K. A. Creel, J. Davis, D. Demszky, C. Donahue, M. 더움보야, E. 뒤머스, S. Ermon, J. Etchemendy, K. L. Ethayarajh 페이페이, C. 핀, T 게일 L. E. 길레스피, K. 고네덕 굿맨 그로스만 구하태 Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. 허재황 카드S Jain D. Jurafsky, P. Kalluri, S. 카람체티, G. 킬링, F. 카니, O. Khattab, P. W. Koh, M. S. Krass, R. 크리슈나 쿠디티푸디, A. 쿠마르, F. 라드학, M. 이태호 이준호 리사, L. 이태환 Ma, A. Malik, C. D. Manning, S. P. Mirchandani, E. Mitchell, Z. 무니과 Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. Carlos Niebles, H. Nilfrooshan, J. F. Nyarko, G. Ogut, L. Orr, I. P. Piatti, J. Sung Park, C. Piech, E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. H. Roohani, C. Ruiz, J. Ryan, C. R\'e, D. Sadigh, S. 사카와, K. 산타남 A.시경 파라수람 스리니바산, A. 탐킨, R. 타오리, A. W. Thomas, F. Tramer, R. E. Wang, W. 왕병우 우영 우상민 야수나가, J. You, M. A. 자하리아, M. 장태 장진 장영 장창영 장룡 Zheng, C. Zhou, P. Liang(2021)은 기초 모델의 기회와 위험에 대해 언급하였다. 아르시브 외부 링크: 2103.03032 인용: SS1.\n' +
      '*[3]M. 변병현 이원 백승훈 Kim(2022) COYO-700M: 이미지-텍스트 쌍 데이터세트. 참고: [https://github.com/kakaobrain/coyo-dataset](https://github.com/kakaobrain/coyo-dataset) 인용: SS1.\n' +
      '*[4]S. Castro and F. Caba (2022) FitClip: 제로 샷 비디오 이해 작업을 위해 대규모 사전 훈련된 이미지 텍스트 모델을 정제한다. 제33회 영국 머신비전 콘퍼런스 2022에서는 BMVC 2022, 영국 런던, 2022년 11월 21∼24일 BMVA Press, 산티아고 카스트로, 오아나 이그나트, 라다 미할체아 등이 개최된다. 2023. 비전 언어 모델에 대한 확장 가능한 성능 분석. _Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)_, pages 284-294, Canada, Toronto. 컴퓨터 언어학과의 연관성\n' +
      '* Chen et al. (2015) Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. 2015. Microsoft COCO Captions: Data Collection and Evaluation Server. _ arXiv preprint arXiv:1504.00325_.\n' +
      '* Cherti et al. (2023) Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. 2023. 대조적 언어-이미지 학습을 위한 재현 가능한 스케일링 법칙. IEEE/CVF Conference on Computer Vision and Pattern Recognition_의 _Proceedings, pages 2818-2829.\n' +
      '* Cimpoi et al.(2014) M. 심포이 마지일옥키노스 모하메드와 A. 베달디 2014. 야생의 질감을 묘사하고 있습니다. In _Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_.\n' +
      '* Costa-Luis et al. (2023) Casper da Costa-Luis, Stephen Karl Larque, Kyle Altendorf, Hadrien Mary, Richardsheirdan, Mikhail Korobov, Noam Raphael, Ivan Ivanov, Marcel Bargull, Nishant Rodrigues, Guangshuo Chen, Antony Lee, Charles Newey, Crazpy Python, JC, Martin Zugnoni, Matthew D. Pagel, majstevens77, Mikhail Dektyarev, Alex Rothberg, Alexander Flavin, Daniel Pantleit, Fabian Dill, FichteFoll, Gregor Sturm, HeoHugo van Kemenade, Jack McCracken, MapleCCC, and Max Nordlund. 2023. tqdm: 파이썬과 CLI를 위한 빠르고 확장 가능한 진행률 바.\n' +
      '* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, 및 Li Fei-Fei. 2009. ImageNet: 대규모 계층 이미지 데이터베이스. 2009년 IEEE Conference on Computer Vision and Pattern Recognition_, pages 248-255.\n' +
      '* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. [Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics Association for Human Language Technologies, Volume 1(Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota. 컴퓨터 언어학과의 연관성\n' +
      '* Diwan et al. (2022) Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath, and Kyle Mahowald. 2022년 윈오그라운드가 왜 어렵죠? 비언어적 구성의 실패를 조사하는 것. _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 2236-2250, Abu Dhabi, United Arab Emirates. 컴퓨터 언어학과의 연관성\n' +
      '* 펠바움(2010) 크리스티안 펠바움. 2010. _Theory and Applications of Ontology: Computer Applications_, chapter WordNet. 스프링거 네덜란드, 도르드레흐트\n' +
      '* Hafri et al. (2023) Alon Hafri, E. J. Green, and Chaz Firestone. 2023. 시지각에서의 합성 Behavioral and Brain Sciences_, 46:e277\n' +
      '* Harris et al. (2020) Charles R Harris, K Jarrod Millman, Stefan J Van Der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, et al. 2020. Array programming with NumPy. _ Nature_, 585(7825):357-362.\n' +
      '* Helber et al. (2018) Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. 2018. Introduction Euroat: A novel dataset and deep learning benchmark for land use and land cover classification. _IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium_, pages 204-207. IEEE.\n' +
      '* Helber et al. (2019) Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. 2019. Eurosat: 토지이용 및 토지피복 분류를 위한 신규 데이터셋 및 딥러닝 벤치마크. _ IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing_.\n' +
      '* Hendricks and Nematzadeh (2021) Lisa Anne Hendricks and Aida Nematzadeh. 2021. 동사 이해를 위한 Probing 이미지-언어 트랜스포머. _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pages 3635-3644, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Honnibal et al. (2020) Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Python에서의 산업용 강도 자연어 처리.\n' +
      '* Hsieh et al. (2023) Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. 2023. SugarCrepe: vision-language compositionality를 위한 해킹 가능한 벤치마크 수정. 제37차 신경정보처리시스템 컨퍼런스에서 데이터세트 및 벤치마크 Track_\n' +
      '*Hunter (2007) John D Hunter. 2007. Matplotlib: A 2D 그래픽 환경. _ Computing in science & engineering_, 9(03):90-95.\n' +
      '* Ilharco et al. (2022) Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. 2022. 가중치를 보간하여 개방형 어휘 모델을 패치하는 단계. In _Advances in Neural Information Processing Systems_, volume 35, pages 29262-29277. Curran Associates, Inc.\n' +
      '* Ilharco et al. (2021) Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. 2021. Openclip. 이 소프트웨어를 사용하신다면 아래와 같이 인용해 주시기 바랍니다.\n' +
      '* Jia et al. (2021) Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. 시끄러운 텍스트 감독으로 시각 및 시각 언어 표현 학습을 확대한다. 제38회 기계 학습 국제 회의_Proceedings of the 38th International Conference on Machine Learning_, vol 139 of _Proceedings of Machine Learning Research_, pages 4904-4916. PMLR.\n' +
      '* Johnson et al. (2017) Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. 2017. CLEVR: 작곡 언어 및 초등 시각적 추론을 위한 진단 데이터세트. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_.\n' +
      '* 재현 가능한 전산 워크플로우를 위한 출판 형식. The _Positioning and Power in Academic Publishing: Player, Agents and Agendas_, pages 87-90, Netherlands. IOS 프레스입니다\n' +
      '* Krause et al. (2013) Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 2013. 3d object representation for fine-grained categorization. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops_.\n' +
      '* Krizhevsky (2009) Alex Krizhevsky. 2009. 작은 이미지에서 여러 겹의 피쳐를 학습합니다. 토론토 대학 기술 보고서\n' +
      '* Kuehne et al. (2011) H. Kuehne, H. Jhuang, E. Garrote, T. 포지오와 T 세레 2011. HMDB: 사람 동작 인식을 위한 대형 비디오 데이터베이스. 2011년 컴퓨터 비전 국제 회의에서, 페이지 2556-2563.\n' +
      '* Laurencon et al. (2023) Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kisla, Matthieu Cord, and Victor Sanh. 2023. OBELICS: 인터리빙된 이미지-텍스트 문서들의 오픈 웹-스케일 필터링된 데이터세트. 제37차 신경정보처리시스템 컨퍼런스에서 데이터세트 및 벤치마크 Track_\n' +
      '* LeCun et al. (1994) Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. 1994. 손글씨 숫자들의 MNIST 데이터베이스.\n' +
      '* Lhoest et al. (2021) Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhabalani, Bhavitya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clement Delangue, Theo Matsu-siere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Francois Lagunas, Alexander Rush, Thomas Wolf. 2021. 데이터세트: 자연어 처리를 위한 커뮤니티 라이브러리. [Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 175-184, Online and Punta Cana, Dominican Republic. 컴퓨터 언어학과의 연관성\n' +
      '* Li et al. (2023) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. BLIP-2: 동결 이미지 인코더 및 대형 언어 모델로 부트스트래핑 언어-이미지 사전 트레이닝. Machine Learning에 관한 40번째 국제 회의에서, 페이지 12888-12900. PMLR.\n' +
      '* ECCV 2014_, pages 740-755, Cham. 스프링거 국제 출판사\n' +
      '* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. SGDR: 따듯한 재시작과 함께 확률적 기울기 하강. _International Conference on Learning Representations_.\n' +
      '* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. _International Conference on Learning Representations_.\n' +
      '* Ma et al. (2023) Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. 2023. 크레이프: 시각 언어 기반 모델이 구성적으로 추론할 수 있습니까? _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10910-10921.\n' +
      '* 유지보수자 및 기여자(2016) 토치비전 유지보수자 및 기여자. 2016. TorchVision: PyTorch\'s computer vision library. [https://github.com/pytorch/vision] (https://github.com/pytorch/vision).\n' +
      '* Miech et al. (2020) Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Andrew Zisserman. 2020. RareAct: 비정상적 상호작용의 비디오 데이터세트. _ arXiv preprint arXiv:2008.01018_.\n' +
      '* Nilsback and Zisserman (2008) 마리아-엘레나 Nilsback and Andrew Zisserman. 2008. Automated flower classification over a large number class. 2008년 제6차 인도 컴퓨터 비전 회의, 그래픽 & 이미지 처리_, 페이지 722-729.\n' +
      '* van den Oord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. 대조적 예측 코딩을 이용한 표현 학습. _ arXiv preprint arXiv:1807.03748_.\n' +
      '* OpenAI(2023) OpenAI. 2023. GPT-4V(ision) 시스템 카드. 기술 보고서, 오픈AI\n' +
      '* Parcalabescu et al. (2022) Letitia Parcalabescu, Michele Cafagna, Lalitta Muradjan, Anette Frank, Iacer Calixto, and Albert Gatt. 2022. VALSE: 언어 현상을 중심으로 한 시각 및 언어 모델에 대한 태스크 독립적인 벤치마크. _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8253-8280, Dublin, Ireland. 컴퓨터 언어학과의 연관성\n' +
      '* Parcalabescu et al. (2017)Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Kimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: Imperative Style, High-Performance Deep Learning Library. In _Advances in Neural Information Processing Systems 32_, pages 8024-8035. Curran Associates, Inc.\n' +
      '* Perez and Granger (2007) Fernando Perez and Brian E. Granger. 2007. IPython: interactive scientific computing을 위한 시스템. _ Computing in Science and Engineering_, 9(3):21-29.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. 자연어 감독으로부터 전이 가능한 시각적 모델을 학습하는 단계. 제38회 머신러닝 국제회의_Proceedings of the 38th International Conference on Machine Learning_, vol 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR.\n' +
      '* Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. 클립 래턴트를 갖는 계층적 텍스트-조건부 이미지 생성_ arXiv preprint arXiv:2204.06125_.\n' +
      '* Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. 제38회 머신러닝 국제회의_Proceedings of the 38th International Conference on Machine Learning_, vol 139 of _Proceedings of Machine Learning Research_, pages 8821-8831. PMLR.\n' +
      '* Ray et al. (2023) Arijit Ray, Filip Radenovic, Abhimanyu Dubey, Bryan A. Plummer, Ranjay Krishna, and Kate Saenko. 2023. Cola: 합성 텍스트-이미지 검색을 위한 벤치마크. 제37차 신경정보처리시스템 컨퍼런스에서 데이터세트 및 벤치마크 Track_\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. 잠재 확산 모델을 이용한 고해상도 영상 합성. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695.\n' +
      '* Schuhmann et al. (2022a) Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022a. LAION-5B: 차세대 이미지-텍스트 모델을 훈련시키기 위한 개방형 대규모 데이터세트. IMT2000 3GPP - 제36차 신경정보처리시스템 회의 : 데이터세트 및 벤치마크 Track_\n' +
      '* Schuhmann et al. (2022b) Christoph Schuhmann, Andreas Kopf, Theo Coombes, Richard Vencu, Benjamin Trom, and Romain Beaumont. 2022b. LAIONCO: LAION2B-EN의 600M 합성 캡션.\n' +
      '* Schuhmann et al. (2021) Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021. LAION-400M: CLIP 필터링된 4억 개의 이미지-텍스트 쌍의 오픈 데이터셋 _ arXiv preprint arXiv:2111.02114_.\n' +
      '* Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2556-2565, Melbourne, Australia. 컴퓨터 언어학과의 연관성\n' +
      '* Shekhar et al. (2017) Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurelie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella Bernardi. 2017. FOIL it!\' 이미지와 언어 캡션의 불일치를 찾습니다. _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 255-265, Vancouver, Canada. 컴퓨터 언어학과의 연관성\n' +
      '* Soomro et al. (2012) Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012. UCF101: 야생의 비디오로부터 101개의 인간 행동 클래스의 데이터세트. _ CRCV-TR-12-01_.\n' +
      '* Speer(2019) Robyn Speer. 2019. ftfy. 제노도 버전 5.5\n' +
      '* 명령줄 전동공구. _; login: The USENIX Magazine_, 36(1):42-47.\n' +
      '* (2023) The Pandas 개발팀. 2023년 판다-데브/판다: 판다\n' +
      '* Thrush et al. (2022) Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. 2022. Winoground: Probing vision and language models for visio-linguistic compositionality. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5238-5248.\n' +
      '* Tschannen et al. (2023) Michael Tschannen, Manoj Kumar, Andreas Peter Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer. 2023. 이미지 캡셔너는 확장 가능한 비전 학습자도 있습니다. IMT-2000 3GPP-신경정보처리시스템에 관한 제37차 회의\n' +
      '* Venugopalan et al. (2017) Subhashini Venugopalan, Lisa Anne Hendricks, Marcus Rohrbach, Raymond Mooney, Trevor Darrell, Kate Saenko. 2017. 다양한 객체들로 이미지를 캡션하는 단계. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_.\n' +
      '\n' +
      'Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, et al. 2020. SciPy 1.0: 파이썬에서 과학 컴퓨팅을 위한 기본 알고리즘 _ Nature methods_, 17(3):261-272.\n' +
      '* Waskom(2021) Michael L. 와스콤 2021. 해저면 : 통계자료 시각화_ Journal of Open Source Software_, 6(60):3021.\n' +
      '* Wightman(2019) Ross Wightman. 2019. PyTorch 이미지 모델. [https://github.com/rwightman/pytorch-image-models] (https://github.com/rwightman/pytorch-image-models).\n' +
      '* Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. 트랜스포머: 최첨단 자연어 처리. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online. 컴퓨터 언어학과의 연관성\n' +
      '*Xu et al.(2016) Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. MSRVTT: 브리징 비디오 및 언어를 위한 대형 비디오 기술 데이터세트. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_.\n' +
      '* 복잡한 응용 프로그램을 우아하게 구성하기 위한 프레임워크. 기섭\n' +
      '* Young et al. (2014) Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. 이미지 설명부터 시각적 표현까지: 이벤트 설명에 대한 의미론적 추론을 위한 새로운 유사성 메트릭 _ The Association for Computational Linguistics_, 2:67-78.\n' +
      '* Yuksekgonul et al. (2023) Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. 2023. 시각 언어 모델은 언제, 왜 가발처럼 행동하는지, 그리고 그것에 대해 어떻게 해야 하나요? _The Eleventh International Conference on Learning Representations_.\n' +
      '* Zhao et al. (2022) Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, 및 Jianwei Yin. 2022. VL-CheckList: 객체, 속성 및 관계를 갖는 사전 훈련된 비전 언어 모델을 평가하는 단계. _ arXiv preprint arXiv:2207.00221_.\n' +
      '* Zhou et al. (2018) Luowei Zhou, Chenliang Xu, and Jason Corso. 2018. Towards a automatic learning of procedures from web instructional video. _ AAAI Conference on Artificial Intelligence_, 32(1).\n' +
      '\n' +
      '## 부록 A 슈가 크립트 퍼포먼스\n' +
      '\n' +
      '표 6에서는 슈가크레페의 세립형 과제 결과를 보여준다.\n' +
      '\n' +
      '## 프롬프트가 없는 부록 B 분류\n' +
      '\n' +
      'CLIP 유사 모델은 본 논문에서와 같이 일반적으로 OpenAI의 CLIP(Radford et al., 2021)에 의해 원래 테스트된 모델에 의존하는 분류를 위한 여러 프롬프트로 평가된다. 예를 들어, ImageNet의 경우, "{클래스 이름}의 사진", "{클래스 이름}의 탭"과 같이 사용되는 80개의 프롬프트(템플릿)가 있다. 이러한 프롬프트들이 사용되는 이유는 텍스트 표현들이 보통 시끄럽고, 만족스러운 평균 클래스 표현이 이들 모든 텍스트들에 대한 임베딩들로부터 얻어질 수 있기 때문이다. 이러한 프롬프트는 클래스와 데이터 집합의 특성에 맞게 신중하게 작성되었습니다. 표 7에서는 클래스 이름만을 입력으로 하여 어떠한 프롬프트도 사용하지 않고 분류 결과를 보여준다. 패칭 없이, 우리의 방법은 완전히 형성된 문장("허스키"와 같은 클래스 이름과는 대조적으로)을 보도록 조정된 경우에도 표 2의 결과에 대해 약간의 감소(2.5%)를 제시한다. 패칭을 적용할 경우, 사전 학습된 모델보다 벤치마크 10개 중 7개에서 성능이 덜 떨어지며, 2와 동등하다.\n' +
      '\n' +
      '## 플리커 및 COCO 검색 태스크에서의 부록 C 성능\n' +
      '\n' +
      'Flickr30k(Young et al., 2014)와 COCO Captions(Chen et al., 2015)에서 CLIP 유사 모델(Radford et al., 2021)로 보고되어 검색 성능을 평가한다. 우리는 이러한 결과를 주요 검색 결과와 함께 포함하지 않는다. 왜냐하면 우리는 그것들이 거의 샷에 가깝거나 제로 샷(도메인 내)이 아니라고 믿기 때문이다. NegCLIP와 REPLACE는 COCO의 훈련 세트에서 미세 조정되었다. 이 방법은 LAION-COCO에 대해 훈련되며, 캡션은 COCO와 유사한 형식을 따르며, 동시에 COCO 이미지는 Flickr에서 나온다. 결과를 표 8에 제시한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>