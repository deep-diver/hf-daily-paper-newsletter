<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'VR Facial Animation의 경우 Potorealistic Avatar의 등록# Fast 등록은 VR Facial Animation의 경우 Potorealistic Avats에 등록되었다.\n' +
      '\n' +
      ' 샤냐 파멜트({}^{1,2}\\)\n' +
      '\n' +
      'Shaojie Bai\\({}^{2}\\)\n' +
      '\n' +
      'Te-Li Wang\\({}^{2}\\)\n' +
      '\n' +
      'Jason Saragih\\({}^{2}\\)\n' +
      '\n' +
      'Shih-En Wei\\({}^{2}\\)\n' +
      '\n' +
      'Met Reality Labs University({}^{2}\\,{}^{2}\\)\n' +
      '\n' +
      '[https://chaitanya100100.github.io/FastRegistration](https://chaitanya100100.github.io/FastRegistration)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '우리는 먼저 이 작업에 대해 인간 특이적 아바타(HMC) 이미지를 변환하기 어려운 1차 모델들 사이의 도메인 갭이 다른 부분들에 대해 매우 까다롭다는 것을 보여주지만, 이 작업에는 사람별 아바타(VR)의 성능을 정확하게 체감할 수 있는 가상적 기반 이미지(VR)의 대응 능력, 즉 VR 헤드셋을 착용하는 데 있어서 하나의 지능적 아바타-이미지 기반 이미지들의 성능을 정확하게 체감할 수 있는 일련의 기능(VR)에 대한 인식 인식(Virtual-imeing)에 대한 인식 인식(AVC)의 성능을 충족할 수 있다는 것을 의미한다. 표현과 머리 포즈. 이 두 모듈은 면대지-진실 예시를 보여주면 이미지 스타일 전달이 쉬워지고, 더 나은 도메인-갭 제거가 등록에 도움이 되기 때문에 서로를 보강한다. 저희 시스템은 양질의 결과를 효율적으로 생산하여 맞춤형 라벨을 생성하기 위해 비용이 많이 드는 오프라인 등록의 필요성을 방해합니다. 우리는 상품 헤드셋에 대한 광범위한 실험을 통해 접근법의 정확성과 효율성을 검증하여 오프라인 등록뿐만 아니라 직접 회귀 방법에 비해 상당한 개선을 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '광학적 아바타 생성은 최근 몇 년 동안 많은 진전을 보이고 있다. 신경 표현 및 역 렌더링(19, 20, 27, 28], 상호 작용 애플리케이션(30])에 대한 실시간 렌더링을 지원하면서 전화 스캔[5]와 같은 제한된 캡처에서도 개인의 매우 정확한 표현을 생성할 수 있다. 이러한 아바타에 대한 새로운 사용 사례는 가상 현실(VR)에서 사회적 상호 작용을 가능하게 하기 위한 것이다. 이 애플리케이션은 VR 헤드셋에 의해 일반적으로 환경으로부터 사용자의 얼굴이 폐색되는 특정 문제를 제시한다. 따라서 헤드셋 장착 카메라에 의존하여 사용자의 아바타를 구동합니다. 정확한 결과가 입증되었지만, 이들은 아바타가 헤드셋 장착 카메라와 관련된 대응 사항이 추가 정교한 캡처 장비[30] 사용에 의존하는 사람별 경우로 제한되었다. 보다 일반적인 경우 높은 정확한 추적은 여전히 열린 문제로 남아 있다.\n' +
      '\n' +
      '이 연구에서 우리는 제네릭 표정 등록이 아바타 대 이미지 대응성을 제공하기 위해 추가 캡처 장치에 의존하지 않고 비선 정체성에 정확하고 효율적일 수 있음을 보여준다. 이를 위해 먼저 헤드셋 장착 카메라와 사용자의 아바타의 양식이 일치하는 경우 발현 추정과 머리 포즈를 반복적으로 반박하는 새로운 변압기 기반 네트워크를 사용하여 정확한 결과가 가능하다는 것을 보여준다. 이 발견을 기반으로 카메라의 도메인에서 아바타의 영역으로 교차 동일성 도메인 전달 기능을 배울 것을 제안한다. 여기에서 핵심 과제는 헤드셋 장착 카메라에 의해 제시된 얼굴의 도전적인 관점 때문에 도메인 전달에 필요한 높은 정확도에 있으며, 몇 개의 화소 오류라도 추정된 아바타의 표현에 상당한 영향을 미칠 수 있다. 우리의 방법의 핵심 설계는 반복적 표현과 머리 포즈 추정 및 도메인 전달이 서로를 강화한다는 것이다. 한편으로, 도메인 전달 결과의 더 높은 품질은 반복 정제를 더 쉽게 만든다. 유사한 보강은 정제 단계에서 추정된 것을 포함하여 표현과 머리 포즈가 유연하게 구성될 수 있는 여러 아바타 렌더링으로 도메인 전달 기능을 조건화하는 설계로부터 다른 방향으로 유지된다. 정제된 추정이 근거 진리에 가까울 때, 도메인 전달 네트워크는 입력된 HMC 이미지 및 컨디셔닝 이미지를 사용하여 국부적으로 쉽게 이유할 수 있다.\n' +
      '\n' +
      '접근법의 효능을 입증하기 위해 우리는 208개의 동일성의 데이터셋에 대한 실험을 수행하며, 각각 멀티뷰 캡처 시스템[19] 및 수정된 퀘스트프로 헤드셋[22]에서 캡처되었으며, 후자는 구동 카메라와 아바타 사이의 근거 진실 대응을 제공하는 데 사용되었다. 직접 회귀 방법에 비해 반복 구조는 비실증 정체성의 새로운 출현 변화에 대해 유의하게 향상된 견고성을 보여준다.\n' +
      '\n' +
      '요약하면, 이 작업의 기여는 포함된다.\n' +
      '\n' +
      '* A 시연은 뉴럴 렌더링 모델에 대한 반복적인 변압기 기반 아키텍처와 카카바타르 도메인과 일치하는 상태에서 정확하고 효율적인 일반 얼굴 등록이 달성 가능하다는 것을 입증한다.\n' +
      '* A는 광학적 아바타 렌더링에서 탄력적으로 조건화되는 도메인 전달 네트워크를 일반화한다.\n' +
      '* 회귀 방법을 능가하고 전처리 없이 사람별 수준의 정확도에 접근하는 상품 헤드셋 카메라로부터의 첫 번째 일반 표현 추정 시스템입니다.\n' +
      '\n' +
      '나머지 논문들은 다음과 같이 구성되어 있다. 다음 섹션에서는 문헌 검토를 제시한다. 그런 다음 SS3에서 일반적인 표정 추정을 위한 방법을 설명한다. SS4에서 광범위한 실험을 통해 접근법의 효능을 보여준다. 우리는 향후 작업에 대한 논의로 SS5에서 결론을 내린다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      'VR 페이스 트래킹.\n' +
      '\n' +
      '얼굴 추적은 오랜 연구 문제이지만 헤드 마운트 카메라(HMC)에서 VR 사용자의 얼굴 추적은 독특한 도전을 제기한다. 어려움은 주로 헤드셋으로 인한 카메라 배치 및 폐색 제한에서 비롯된다. 센서 이미지는 얼굴 부분의 비스듬하고 부분적으로 겹치는 뷰만을 제공한다. 이전 작업은 이러한 어려움을 우회하는 다양한 방법을 탐구했다. [16]에서는 돌출된 마운트에 카메라를 부착하여 하부 얼굴의 전두경을 획득했지만 비과학적인 하드웨어 디자인으로 구성하였다. [29]에서, 외부인 제3인 뷰 카메라는 사용자의 머리 포즈 범위를 제한한다. 이 두 작품은 모두 RGBD 센서에 의존하여 기하학 전용 모델로 하면을 직접 등록한다. 하드웨어 요구 사항을 줄이기 위해 [23]은 하부 면에 단일 RGB 센서를 사용하고 블렌다페 계수의 직접 회귀를 수행했다. 트레이닝 데이터 세트는 미리 정의된 표현 세트를 수행하는 피험자와 관련 아티스트 생성 혼합 계수들을 갖는 문장들로 구성된다. 혼합 표지된 애니메이션 제한된 애니메이션 충실도와 피험자의 공연 간의 불일치가 제한된다.\n' +
      '\n' +
      '광학적 아바타[19]가 있는 소비자 헤드셋(오쿨러스 리프트)의 VR 얼굴 추적 시스템은 [30]에 처음 제시되었다. 그들은 (1) _ 훈련-추적-헤드셋_의 개념이라는 두 가지 소설을 도입했는데, 여기서 전자에는 후자의 카메라가 초설정되어 있다. TSA_에 의존하여 개인별 트레이닝을 통해 [30]와 보조 방식의 헤드셋을 사용하여 조작하는 것을 통해 [27]에서 개별적으로 트레이닝한 후, 더 나은 위치된 카메라로부터의 보조 뷰가 폐기될 수 있고, 이에 대한 회귀 모델을 사용하여 HMC의 입력에서 완벽하게 등록할 수 있음을 입증했으며, 이러한 작업은 또한 RMC의 입력에서 식별되지 않은 데이터 제작에 대해 잘 학습할 수 있다는 것을 입증한다. 이러한 효율적으로 생성된 이미지 라벨 쌍은 나중에 미인딩 얼굴 추적기를 조정하고 애니메이션을 보다 정밀하게 만드는 데 사용될 수 있다.\n' +
      '\n' +
      '임팩트트랜스.\n' +
      '\n' +
      '이미지 스타일 전달의 목표는 입력의 콘텐츠로부터 의미 및 구조적 콘텐츠를 유지하면서, 정보 컨디셔닝에 의해 제공되는 타겟 스타일 도메인에서 이미지를 렌더링하는 것이다. 컨볼루션 신경망은 콘텐츠 및 스타일 정보를 인코딩하기 위해 [11] 활용이 시작되었다. Pix2 픽셀[13]은 쌍을 이루는 근거 진리의 가용성을 가정하여 고주파 선명도를 장려하기 위해 \\(L_{1}\\) 이미지 손실과 함께 조건부 GAN을 학습한다. 쌍방향 이미지 획득의 어려움을 완화하기 위해 사이클-합리적 개념을 도입하였으나, 각 모델은 특정 쌍의 도메인에 대해서만 학습되어 입력과 출력 사이의 의미적 이동에 시달린다. 스타GAN[7]은 개념을 미리 정의된 도메인의 고정된 세트로 확장한다. 보다 지속적인 제어를 위해 많은 탐구 텍스트 컨디셔닝[2] 또는 이미지 컨디셔닝[1, 6, 8, 18, 31]을 검색했다. 이러한 설정은 일반적으로 최적 출력이 독특하지 않을 수 있는 입력 공간과 출력 공간 사이의 정보 불균형을 갖는다. 이 작업에서 잠재 공간 제어 얼굴 아바타[5]를 고려할 때, 지상-진술 생성 방법[27]과 함께 우리의 이미지 스타일 전달 문제는 단순히 직접 감독될 수 있으며, 불균형 정보 문제를 해결하기 위해 아바타에서 렌더링된 컨디셔닝 이미지와 함께 가능하다.\n' +
      '\n' +
      '학습 기반 활동적 페이스 등록.\n' +
      '\n' +
      '고정밀 얼굴 추적에 대한 일반적인 접근법은 점점 더 등록된 기하학에서 추출된 이미지 특징을 사용하는 회귀의 캐스케이드를 포함한다. 이 접근법을 사용하는 첫 번째 방법 중 하나는 SIFT 특징[33]을 사용하여 확장된 간단한 선형 모델 원시 이미지 픽셀[26]을 사용했다. 이후의 방법은 이진 트리[4, 14]와 같은 더 강력한 회귀기를 사용하고 3D 형상 표현을 제형에 통합했다. 효율성은 이진 특징과 선형 모델[25]에 의해 달성될 수 있다.\n' +
      '\n' +
      '이러한 얼굴 추적 방법은 기하학의 현재 추정치를 사용하여 이미지에서 관련 특징을 추출하지만 일반적인 검출 및 등록을 위해 유사한 캐스케이드 아키텍처도 탐색되었다. 이러한 작품에서 기하학의 현재 추정치를 사용하여 _extracting_ 피처 대신 입력 데이터는 기하학의 현재 추정치의 _renderings_로 증강되며, 이는 현대 컨볼루션 딥러닝 아키텍처를 활용하는 데 있어 회귀자의 백본을 단순화한다. 예를 들어, 카사데 포즈 회귀 [9]는 원래 입력과 일치하는 신체 키 포인트들의 현재 추정치를 중심으로 2D 가우스들을 끌어들이며 일종의 연성 주의 맵으로 작용한다. 3D 히트맵 예측에 [3]의 유사한 설계를 사용하였다. Xia et al. [32]는 랜드마크 쿼리와의 정렬에 직면하기 위해 비전 변압기[10]를 적용했다. 이 연구에서 우리는 멀티뷰 이미지로부터 머리 포즈와 표현의 정확한 수정을 예측하기 위해 랜드마크의 지침이 필요하지 않은 변압기 기반 네트워크를 보여준다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '\\(\\mathbf{H}=\\{H_{c}\\}_{c}\\ C}\\)에 제시된 아바타 모델을 다중 뷰(H_{c}\\in\\mathbb{R}^{h\\i)에 등록하는 것을 목표로 하며, 각 카메라 뷰 \\(H_{c}\\in\\mathbb{R}\\in\\mathbb{c}\\)는 [5]에서 제시한 다중 뷰(H_{c}\\in\\mathbb}\\in\\mathbb}\\in\\in\\in\\-view HMC 이미지)에서 제시되는 다중 뷰(H_{c}\\in\\mathbb}\\in\\in\\in\\mathbb}\\in\\in\\in\\in\\in\\in\\in\\mathbbbb}{c}{c}{c}{c}{c}\\in\\in\\in\\in\\in\\in\\in\\in\\in\\in\\in\\in C}{c}{c}{c}{c}{c}\\in\\in\\in 상면과 하면의 각 측면 사이의 비오버링 뷰의 패치워크를 포함한다. 일부 예는 그림 2에 나와 있으며, 카메라 각도와 헤드셋 기부 변형에 도전하기 때문에 기계 학습 모델(예: 그림 7 참조)에 의해 미묘한 표정들이 정확하게 인식되기 어렵다.\n' +
      '\n' +
      '그림 2: ** HMC 이미지** 및 오프라인 등록 방법[27]에서 아바타에 렌더링된 해당 지상 진리 표현의 예(그림 2: **)는 더 나은 전두 뷰(녹색에서 고광광)로 증강 카메라를 활용하는 것이다. 이 작품에서 우리는 소비자 헤드셋에 카메라를 사용하여 얼굴을 효율적으로 등록하는 것을 목표로 하며, 이는 비스듬한 뷰(빨간색으로 고라이트)만 있다. 이러한 관점에서 미묘한 표현(예: 입술 움직임)에 대한 정보는 종종 매우 적은 픽셀을 커버하거나 보이지 않는다.\n' +
      '\n' +
      '우리는 설정 [5]에서 아바타의 디코더 모델을 \\(\\mathcal{D}\\)로 나타낸다. \\(i^{I}^{th}\\) 주제, \\(i^{\\bf{I}^{i}\\)\\(R=\\math{R}^{i}\\)에 의해 지정된 관점에서 이 피험자의 아바타를 렌더링할 수 있다. 특정 머리 장착 카메라(R_{c},\\mathbf{v},\\mathbf{v}_{c}^{i}}^{i})의 관점, 즉, 우리가 \\(H_{c}\\mathb}{v} <\\mathb}{c}<\\mathb}:\\mathb}}}. [5]에 이어 특정 동일성 \\(i\\)에 대한 신원 정보 \\(\\mathbf{I}^{i}\\)가 디코더 신경망에 대한 다중 규모 미해결 바이어스 맵으로 제공된다. 이 논문에서 우리는 \\(\\mathbf{I}^{i}\\)를 광학 단계 또는 전화 스캐닝1에서 훈련 및 테스트 정체성에 사용할 수 있으며 모든 헤드 마운트 카메라의 보정이 알려져 있다고 가정한다. 우리는 [27]의 방법을 사용하여 지상 진리 HMC 이미지 대(\\(\\mathbf{z}\\), 값비싼 최적화 프로세스와 증강 추가 카메라 세트인 \\(C^{\\prime}\\)에 의존하는\\(\\mathbf{v}\\)에 대응한다. 그림 2의 녹색 상자에서 예가 강조되는 이 연구의 목표는 아비타 모델(즉, 등록)을 활용하는 새로운 정체성에 대해 동일한 최적 \\(\\mathbf{z}\\) 및 \\(\\mathbf{v}\\)를 추정하는 반면, 그림 2의 적색 상자에서 강조된 원래 카메라 세트 \\(C\\)만을 사용하는 것이다.\n' +
      '\n' +
      '주절 1: 이 작품에서 우리는 아바타 세대와 아바타 세대에 대한 비신원을 구별한다. HMC 운전을 위한 정체성이었습니다. 우리는 항상 이전 작업에서 방법을 통해 새로운 정체성에 대한 아바타를 구할 수 있다고 가정하며, 그 정체성의 비세엔 HMC 이미지에 대한 표현 추정 방법의 성능을 평가한다.\n' +
      '\n' +
      '역할화 사건.\n' +
      '\n' +
      '정확한 VR 얼굴 등록은 각 헤드 마운트 카메라 \\(c\\)에 대해 \\(H_{c}\\)와 \\(R_{c}\\) 사이의 정확한 정렬을 수반한다. 그러나 여기에서 중요한 도전은 엄청난 도메인 갭이며, \\(\\mathbf{H}=\\{H_{c}}_{c\\}_{c\\ C}\\)는 근거리 조명과 강한 그림자가 있는 모노크롬 적외선 이미지이고, \\(\\mathbf{R}=\\{R_{c}}_{c}}\\)는 가시 스펙트럼에서 균일하게 채색된 이미지에서 내장된 아바타를 렌더링한다. 정체성 특이적 환경에서 이러한 격차를 해소하기 위해 스타일 전달 네트워크를 활용했다. 일반적 다정체성 사건의 문제를 단순화하기 위해 먼저, 도메인 차이가 없을 때 어떤 성과가 가능한지 질문을 한다. 이를 연구하기 위해 \\(\\mathbf{R}_{gt})를 \\(\\mathbf{H}=\\mathcal{D}=\\mathcal{D}(\\mathbf{z}_{gt},\\mathbf{v}_{gt})로 대체하며, 이는 [27]에서 값비싼 방법으로 얻은 \\(\\mathbf{D}) 발현을 정확히 유지하는 3D 아바타 렌더링 공간으로 대체한다. (\\mathbf{z}_{gt},\\mathbf{v}_{gt})\\에서 \\(\\mathbf{v}_{gt})\\를 추출하려면 모바일NetV3[12]와 같은 회귀 CNN을 구축하는 것이 순진한 방법이다. 대안적으로, \\(\\mathcal{D}\\)가 다를 수 있고 입력이 동일한 도메인에 있는 것을 감안할 때, 다른 간단한 접근법은 픽셀별 이미지 손실을 사용하여 \\((\\mathbf{z},\\mathbf{v})에 적합하도록\\(\\mathbf{R}_{gt}\\)를 최적화하는 것이다. 표 1에서 알 수 있듯이 회귀 모형은 매우 가볍지만 잘 일반화하지 못하는 반면, 이 오프라인 방법(불완전)은 수렴하는 데 매우 긴 비용으로 낮은 오차를 생성한다. 입력 도메인 차이(즉, \\(\\mathbf{R}_{gt}\\)에 대한 접근(\\mathbf{H}\\)가 아닌 \\(\\mathbf{R}_{gt}\\)를 단순화했음에도 불구하고, 고유한 비스듬한 시야각, 헤드셋 기부 변동 및 동일성 일반화의 필요성 때문에 여전히 등록이 어렵다.\n' +
      '\n' +
      '대조적으로, 우리는 \\(\\mathcal{F}_{0}.{0}(\\cdot|\\mathcal{D})\\로 표시되는 a\\(\\mathcal{D}\\)) 정보가 좋은 균형을 달성하며(1)의 사료-포식(비식별에 필요한 최적화 없음)을 피드백으로 사용하여 입력 \\(H_{c}\\)과 비교하고 오정렬을 최소화한다고 주장한다. 나중에 전에 전에 우리는 나중에 전에 전에 전에 우리는 그 전에 전에 전에 전에 우리는 그 전에 전에.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\hline \\hline  & Aug. & Frontal & Rot. & Trans. & \\\\  & Cams & Image \\(L_{1}\\) & Err. & Err. & Speed \\\\  & & & (deg.) & (mm) & \\\\ \\hline Offline [27] & ✗ & \\(0.784\\) & \\(0.594\\) & \\(0.257\\) & \\(\\sim\\)1 day \\\\ Regression [12] & ✗ & \\(2.920\\) & \\(-\\) & \\(-\\) & 7ms \\\\ Regression [12] & ✓ & \\(2.902\\) & \\(-\\) & \\(-\\) & 7ms \\\\ Our \\(\\mathcal{F}_{0}(\\mathbf{R}_{gt}|\\mathcal{D})\\) & ✗ & \\(1.652\\) & \\(0.660\\) & \\(0.618\\) & 0.4sec \\\\ Our \\(\\mathcal{F}_{0}(\\mathbf{R}_{gt}|\\mathcal{D})\\) & ✓ & \\(1.462\\) & \\(0.636\\) & \\(0.598\\) & 0.4sec \\\\ \\hline Our \\(\\mathcal{F}_{0}(\\mathbf{H}|\\mathcal{D})\\) & ✗ & \\(2.851\\) & \\(1.249\\) & \\(1.068\\) & 0.4sec \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ** 단순화 설정의 등록 정확도:** 테스트 세트의 모든 프레임에 걸쳐 오차가 평균화된다. 증강 카메라는 \\(C\\) 대신 카메라 세트 \\(C^{\\prime}\\)를 사용하는 것을 의미한다. 프론탈 이미지 \\(L_{1}\\)는 표현 예측 오차를 설명하는 반면, 회전 및 번역 오류는 머리 예측 오차를 설명한다.\n' +
      '\n' +
      '그림 3: ** 방법의 과잉 뷰.* 우리는 문제를 아바타-조건 이미지-이미지 스타일 전달 모듈 \\(\\mathcal{S}\\) 및 반복적 정제 모듈 \\(\\mathcal{F}\\)로 해독한다. 모듈 \\(\\mathcal{F}_{0}\\)은 HMC 입력 \\(\\mathbf{H}\\)에서 직접 추정함으로써 두 모듈을 모두 초기화한다.\n' +
      '\n' +
      'SS 3.3에서 \\(\\mathcal{F}_{0}\\)를 요약하면 표 1에서 이 단순화된 설정 하에서 앞서 언급한 방법의 결과를 보고한다.\n' +
      '\n' +
      '구체적으로 \\(\\mathcal{F}_{0}\\)가 오프라인 등록[27]에 접근하는 성능을 달성할 수 있음을 보여준다. 대조적으로, 순진한 직접적인 회귀는 증강된 카메라 세트에서도 실질적으로 더 악화된다. 이는 목표 정체성 아바타(우리의 경우 \\(\\mathcal{D}\\)에 대한 정보를 가진 컨디셔닝 페이스 등록 학습의 중요성을 강조한다. 그러나 중요한 것은 \\(\\mathbf{R}_{gt}\\)를 \\(\\mathbf{H}\\)로 대체함으로써 실제 문제로 되돌아갈 때, \\(\\mathcal{F}_{0}\\)의 성능도 크게 분해된다. 이 관찰은 입력 도메인 갭 차이로 인한 도전을 보여주며, 다음을 설명하는 바와 같이 등록에서 스타일 전달 문제를 해결하도록 동기를 부여한다.\n' +
      '\n' +
      '### Overall Design\n' +
      '\n' +
      'SS3.1의 관찰에 비추어, 우리는 반복적 정제 모듈, \\(\\mathcal{F}\\) 및 스타일 전달 모듈, \\(\\mathcal{S}\\)의 두 모듈의 학습으로 문제를 해독할 것을 제안한다. I\\(\\mathcal{F}\\)의 목표는 주어진 프레임의 추정 발현 \\(\\mathbf{z}\\) 및 머리 위치 \\(\\mathbf{v}\\)에 대한 반복적인 업데이트를 생성하는 것이다. 그러나 표 1에서 볼 수 있듯이 아바타 모델 \\(\\mathcal{D}\\)에 대한 컨디셔닝만으로는 충분하지 않으며, 이러한 \\(\\mathcal{F}\\)의 좋은 성능은 \\(\\mathbf{H}\\)와 \\(\\mathbf{R}_{gt}\\) 사이의 간격을 닫는 데 비판적으로 의존한다. 따라서 모듈 \\(\\mathcal{F}\\)은 이 단색 영역 격차를 폐쇄하기 위한 스타일 전달 모듈 \\(\\mathcal{S}\\)에 의존해야 한다. 구체적으로, 원시 HMC 이미지 \\(\\mathbf{H}\\) 외에도 \\(\\mathcal{S}\\)에 의해 생성된 도메인 전달 버전(\\hat{\\mathbf{R}\\)을 \\(\\mathcal{F}\\)에 입력으로 사료한다. 그리고 표 1은 \\(\\math{S}{d\\)를 사용하여 다양한 컨디셔닝(\\math{S}}\\)의 품질을 개선하며(\\math{S}}\\) 모델을 사용하여 다양한 컨디셔닝(\\math{D}\\)을 선택하고(\\math{D}\\) 유형을 사용하는 것을 알 수 있다.\n' +
      '\n' +
      '그림 4: ** 측정 정제 모듈 \\(\\mathcal{F}\\)****. 각 뷰(c\\in C\\)에 대해 공유 CNN은 스타일과 함께 현재 렌더링 \\(R_{t,c}\\)와 입력 이미지 \\(H_{c}\\) 사이의 정렬 정보를 특징 그리드로 인코딩한다(\\hat{R}_{c}\\). 학습 가능한 그리드 위치 인코딩 및 카메라 뷰 임베딩을 첨가한 후, 그리드 특징은 현재 추정치 \\((\\mathbf{z}_{t},\\mathbf{v}_{t})와 연결되어 토큰의 시퀀스로 평탄화된다. 이들 토큰은 학습 가능한 디코더 쿼리를 갖는 변압기 모듈에 의해 처리되어 추정에 잔여 업데이트를 출력한다.\n' +
      '\n' +
      '그림 5: **Style 이송 모듈 \\(\\mathcal{S}\\)****. (\\mathbf{z}_{0},\\mathbf{v}_{0})\\의 추정치를 감안할 때, 컨디셔닝 이미지는 동일한 추정치와 \\(M\\(M\\) 다른 핵심 표현으로부터 생성되고 채널과 연결되며 U-Net 인코더에 의해 인코딩된다. 입력 HMC 이미지는 별도의 U-Net 인코더에 의해 인코딩된다. 슬라이딩 윈도우 기반 주의력[24] 모듈은 입력 피처 및 컨디셔닝 기능을 융합하여 그들 간의 오정렬을 보상하는 데 사용된다. 이러한 융합된 특징들은 스타일-전송된 이미지를 출력하기 위해 U-Net 디코더 내의 스킵 연결으로서 제공된다.\n' +
      '\n' +
      '따라서 바람직한 상호 보강이 형성되며, 더 나은\\(\\mathcal{S}\\) 수행, \\(\\mathcal{F}\\)의 오류가 얼굴 등록에 있으며, 차례로 \\(\\mathcal{F}\\)가 더 잘 수행되고, 더 가까운 컨디셔닝 이미지가 근거 진리에 작용하여 \\(\\mathcal{S}\\)에 대한 문제를 단순화시킬 것이다. 이 강화 과정에 대한 초기화 \\((\\mathbf{z}_{0},\\mathbf{v}_{0})=\\mathcal{F}_{0}(\\mathbf{H})\\)는 단일색 입력 \\(\\mathbf{H}\\)에서 직접 작동하는 모든 모델에 의해 제공될 수 있다. 그림. 3는 시스템의 전반적인 디자인을 보여줍니다. 다음과 같이 각 모듈의 설계를 설명하겠습니다.\n' +
      '\n' +
      '전환기 기반 Iterative Refinement Network\n' +
      '\n' +
      '반복적 정제 모듈 \\(\\mathcal{F}\\)의 역할은 입력 및 현재 렌더링으로부터 업데이트된 파라미터 \\((\\mathbf{z}_{t+1},\\mathbf{v}_{t+1})를 예측하는 것이다.\n' +
      '\n' +
      'cal{F}}\\math{mathbf{f{f}}\\mathbf{t}(\\mathbf{t}_{t}),\\mathbf{t}(\\mathbf{t}_{t})\n' +
      '\n' +
      '\\(t\\in[1,T]\\)가 단계 수이고 \\(\\hat{\\mathbf{R}－\\mathcal{S}=\\mathcal{S}(\\mathbf{H})\\)가 형태 전달 결과(그림 4 참조)이다. (\\mathcal{F}\\) 입력 \\(\\mathbf{H}\\)과 현재 렌더링 \\(\\mathcal{D}(\\mathcal{D}_{t}_{t},\\mathbf{v}_{t}) 간의 오정렬에 대한 더 쉬운 이유를 나타낼 수 있다. 그림에서. 4, 우리는 \\(\\mathcal{F}\\)의 하이브리드 변환기 [10] 기반 구조를 보여준다. SS4.2에서 이 하이브리드 변환기 구조가 정체성 전반에 걸쳐 일반화를 달성하기 위한 중요한 설계 선택임을 보여줄 것이다. 변압기 레이어는 모델 크기 폭발 및 정보 병목 현상을 피하면서 여러 카메라 뷰에서 피라미드를 융합하는 데 도움이 됩니다. 모델의 출력은 \\((\\Delta\\mathbf{z}_{t},\\Delta\\mathbf{v}_{t})\\로 처리되고 \\((\\mathbf{v}_{t}_{t},\\mathbf{v}_{t})\\)에 추가되어 다음 반복에 대한 새로운 추정치를 생성한다. 그림. 6은 단계에 걸쳐 \\(\\mathbf{R}_{t}\\)의 진행을 보여준다. 이 반복적 정제 모듈은 최소화를 위해 훈련된다.\n' +
      '\n' +
      '}}\\tag{lathcal{F}}\\tag{2}\\\\\\\\.\n' +
      '\n' +
      'where\n' +
      '\n' +
      '}}\\math{d}\\math{d}\\math{v}\\math{{i}.\n' +
      '\n' +
      '여기서, \\(\\mathbf{v}_{\\text{front}}\\)는 렌더링된 아바타의 미리 정의된 전두관(예를 들어 그림 2 참조)이다. I\\(\\mathcal{L}_{\\text{hmc}}\\)는 예측된 HMC 이미지 간의 정렬을 장려하지만, \\(\\mathcal{L}_{\\text{hmc}}\\)은 HMC 영상에서 비스듬한 시야 각도의 전투 효과를 방지하기 위해 얼굴 전체에 걸쳐 심지어 재구성을 촉진한다.\n' +
      '\n' +
      '\\(\\mathcal{F}_{0}\\)는\\(\\{\\mathbf{H}}_{0},\\mathbf{z}_{0},\\mathbf{v}_{0}\\)를 제공하는 목적으로 HMC 이미지 \\(\\mathbf{H}\\)에서 작동하는 임의의 모듈일 수 있지만, 일관성은 \\(\\mathbf{H}_{0},\\mathbf{H}},\\)에서 작동하는 임의의 모듈일 수 있지만,\\(\\mathbf{H}_{0}:\\)에서 작동하는 임의의 모듈로,\\(\\mathbf{H}}_{0}/\\)에서 작동하는 임의의 모듈로,\\(\\mathbf{H} 및\\)에서 작동하는 임의의 모듈로,\\(\\mathbf{H}}_{0}},\\mathbf{v}_{0}.\\)에서 작동하는 임의의 모듈로,\\는 단순히 \\(\\)와 동일하지만,\\(\\mathbf{v}_{0} (1)은 \\(\\hat{\\mathbf{R}}\\)를 입력으로 하지 않는 것을 제외하고이다.\n' +
      '\n' +
      '영상 전송.\n' +
      '\n' +
      '양식의 전달 모듈인 \\(\\mathcal{S}\\)의 목표는 원시 IR 입력 이미지 \\(\\mathbf{H}\\)를 아바타 렌더링 \\(\\mathbf{R}}\\)과 유사한 \\(\\hat{\\mathbf{R}}\\)로 직접 변환하는 것이다. 우리의 설정은 우리의 스타일 전달 이미지가 IR 도메인에서 크게 누락되는 피부 톤, 프랙클 등을 포함한 동일성 특이적 세부 정보를 회수해야 한다는 점에서 문헌의 방법과 다르며, 그러나 조명 차이와 정체에 걸친 비스듬한 뷰 각도는 입력의 약간의 변화가 표현의 더 큰 변화로 매핑될 수 있음을 의미한다. 이러한 이슈들은 매우 상세한 컨디셔닝 없이 스타일 전송 문제를 불구하게 만든다.\n' +
      '\n' +
      '이를 위해 \\(\\mathcal{F}_{0}\\)에 의해 주어진 사전 등록 추정을 활용하는 새로운 스타일 전달 아키텍처를 설계한다. 구체적으로, 우리는 단일크롬 이미지 \\(\\mathbf{H}\\)에서 직접 훈련된 \\(\\mathcal{F}_{0}\\)를 사용하여 현재 프레임에 대해 \\((\\mathbf{z}_{0},\\mathbf{v}_{0}) 추정치를 얻을 수 있다. 또한 \\(mathbf{z}_{k_{1}}),\\mathbf{z}_{k_{k_{M}})를 선택하여 참조 표현의 범위를 포괄하며, 예를 들어 입 개방, 스쿼팅 눈, 폐쇄 눈 등(Appendix에서 이러한 컨디셔닝 참조 표현의 예들)을 스타일 전이하는 데 크게 도움이 된다는 것을 보여준다. 현재의 프레임 HMC 이미지 \\(\\mathbf{H}\\)를 감안할 때, 우리는 기본적으로 현재 프레임 HMC 이미지 \\(\\mathbf{H}\\)를 계산합니다.\n' +
      '\n' +
      '\\[\\hat{\\mathbf{R}}=\\mathcal{S}}(\\mathbf{H})(\\mathbf{H}_{0},\\mathbf{z}_{0},\\mathbf{z}_{k_{1}},\\mathbf{v}_{v}_{0}),\\mathbf{v}_{0}} <\\math{S}} <\\math{S}} <\\math{S}} <\\math{H} <{H} <{H} <{H} <{H}.\n' +
      '\n' +
      '(\\mathbf{z}_{0},\\mathbf{v}_{0})\\(\\mathcal{F}_{0}\\)에서 제공하는 \\(\\mathbf{v}_{0})\\의 더 나은 추정으로 이러한 컨디셔닝 이미지는 근거 진리에 더 가까워지므로 \\(\\mathcal{S}\\)의 스타일 전달 학습 과제를 단순화한다. 그림. 5는 \\(\\mathcal{S}\\)의 UNet 기반 구조를 보여준다. U-Net 디코더는 입력 이미지 특징을 결합된 피처로부터의 스킵 연결과 RGB 이미지 \\(\\hat{\\mathbf{R}\\)로 디코딩한다. 이 스타일 전송 모듈은 단순한 이미지 \\(L_{1}\\) 손실로 훈련된다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathcal{S}}=\\lVert\\hat{\\mathbf{R}}-\\mathbf{R}_{gt}\\rVert_{1}. \\tag{4}\\]\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '우리는 총 208개의 동일성(32\\(M\\) 프레임의 데이터 세트)에 대한 실험을 수행하며, 각각 라이트 스테이지 [19]와 증강 카메라 뷰가 있는 수정된 퀘스트-프로 헤드셋[22]에서 캡처된다. 아바타는 [5]의 방법을 이용하여 통일된 잠재 표현 공간을 갖는 모든 정체성에 대해 생성된다. 우리는 고품질 라벨을 생성하기 위해 [27]의 광범위한 오프라이너 등록 파이프라인을 사용한다. 우리는 검증 세트로 26개의 동일성을 유지했다. 우리는 훈련 중 \\(T=3\\) 정제 반복을 사용하고 \\(M=4\\) 핵심 표현은 \\(192\\t 192\\) 해상도에서 작동하는 스타일 전달을 위한 컨디셔닝 이미지를 제공하는 데 사용된다. 모델 아키텍처 및 훈련에 대한 자세한 내용은 부록을 참조하세요.\n' +
      '\n' +
      '바젤린과의 비교.\n' +
      '\n' +
      '논의한 바와 같이 일반 얼굴 등록에 대한 비교 방법은 (1) [27]에서 동일한 **오프라인 등록** 방식이지만 카메라 세트 \\(C\\)만을 사용하는 두 가지 명백한 유형이 있다. 그것의 성능은 컴퓨팅 시간이 제한되지 않는 경우 카메라 각도로부터 도전을 고정시킨다. 여기에서 훈련은 동일한 동일성의 프레임들에만 걸쳐 있기 때문에, 그것이 다른 정체성의 이미지로부터 레버리지할 수 있는 사전 지식이 적다. (2) ** 직접 회귀**: 동일한 지상 진리 라벨 세트를 사용하여 모바일NetV3 [12]를 훈련하여 HMC 이미지를 표현 코드 \\(\\mathbf{z}\\)에 직접 재조정한다. 이 방법은 \\(\\mathcal{D}\\)의 사용이 금지된 실시간 시스템에서 사용할 수 있는 온라인 모델을 나타낸다. 표 2는 비교를 요약하였다. 오프라인 방법은 좋은 평균 전방 영상 손실을 달성합니다. 높은 정밀도로 그림 7과 같이 관찰이 좋지 않은 낮은 턱과 내부 입에서 일반적인 실패 모드를 가지고 있으며, 비교해보면 우리의 방법은 교차 동일성 데이터세트로부터 학습을 레버리지하여 보다 균일하게 분포된 에러와 더 나은 머리 포즈 추정을 생성할 수 있다. 사료 포워드 디자인으로 인해 저희 방식도 훨씬 빠릅니다. 반면, 직접 회귀 방법은 예상대로 평균적으로 현저히 악화된다. 또한 입력으로서 또는 증강 카메라를 사용하여 완화 조건(예: \\(\\mathbf{R}_{gt}\\)을 제공하고 흥미로운 방법은 개선되지 않는 반면, 우리의 방법은 이러한 조건을 크게 활용할 수 있다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '이 절에서는 \\(\\mathcal{F}\\)와 \\(\\mathcal{S}\\)의 설계를 남용한다. 보다 자세한 분석과 실험을 위해 부록을 참조하세요.\n' +
      '\n' +
      '우리는 \\(\\mathcal{F}\\)의 변압기 층을 제거하고(\\{\\mathbf{F}}_{c}\\}_{c\\ C}\\) 기능을 3차적으로 융합한 다음 MLP 네트워크를 생성하는 간단한 기준선을 설계한다. 우리는 이 기준선을 서로 일치하는 형태(\\mathcal{F}_{0}(\\mathbf{R}_{gt})\\와 유사한)의 단순화된 경우에 훈련시킨다. ((\\mathbf{z},\\mathbf{v})\\의 반복적 정제를 배우지 못하고 \\(3.65\\)의 전두 영상 \\(L_{1}\\)과 \\(5.09\\)의 회전 오차 및 \\(5.84\\)mm의 번역 오류를 달성한다. 이러한 오류는 표 1과 같이 \\(\\mathcal{F}_{0}\\) 모델보다 상당히 높으며, 이는 변압기가 멀티뷰 특징으로부터 정보를 더 잘 융합할 수 있음을 보여준다.\n' +
      '\n' +
      'Style 트랜스퍼 모듈 \\(\\mathcal{S}\\)에서 그림 1에 나와 있다. 8, 우리는 스타일 전달 모듈 \\(\\mathcal{S}\\)의 결과와 기저부를 비교한다. (텍스트{StyTr}^{2}\\)[8]은 대형 데이터셋으로 비전 변압기[10]의 힘을 받아들이는 최근의 스타일 전달 방법 중 하나이다. 비교를 위해 우리는 \\(\\mathbf{R}_{gt}\\)로서 \\(\\mathbf{R}_{gt}\\)를 먹인다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c} \\hline \\hline  & Aug. & \\multicolumn{3}{c}{Frontal} & Rot. & Trans. & \\\\  & \\multicolumn{1}{c}{c} & Image & Err. & Err. & \\multicolumn{1}{c}{Speed} \\\\  & & \\multicolumn{1}{c}{\\(L_{1}\\)} & (deg.) & (mm) & \\\\ \\hline Offline [27] & ✗ & \\(\\mathbf{H}\\) & \\(1.713\\) & \\(2.400\\) & \\(2.512\\) & \\(\\sim\\)1 day \\\\ Regression [12] & ✗ & \\(\\mathbf{H}\\) & \\(2.956\\) & \\(-\\) & \\(-\\) & 7ms \\\\ Regression [12] & ✗ & \\(\\mathbf{R}_{gt}\\) & \\(2.920\\) & \\(-\\) & \\(-\\) & 7ms \\\\ Regression [12] & ✓ & \\(\\mathbf{H}\\) & \\(2.967\\) & \\(-\\) & \\(-\\) & 7ms \\\\ Regression [12] & ✓ & \\(\\mathbf{R}_{gt}\\) & \\(2.902\\) & \\(-\\) & \\(-\\) & 7ms \\\\ \\hline\n' +
      '**(\\(\\(\\mathcal{F}\\)+\\(\\mathcal{S}\\)) & ✗ & \\(\\mathbf{H:\\) & \\(0.947\\) & \\(0.886\\) & \\(0.886\\) & \\(0.886\\) 및 \\(0.886\\) 및 \\(0.886\\) 및 \\(0.886\\) 및 \\(0.886\\) 및 \\(0.886\\) & \\(0.886\\) & \\(0.886\\) \\(0.886\\) \\(0.886\\) \\(0.886\\) \\(0.4 \\) \\(0.886\\) \\(0.886\\) \\(0.4 \\) \\(0.886\\) \\(0.4 \\) \\(0.886\\) \\(0.886\\)\n' +
      '**Ours** (\\(\\mathcal{F}\\)+\\(\\mathcal{S}\\)) & ✓ & \\(\\mathbf{H}\\) & \\(2.399\\) & \\(0.917\\) & \\(0.845\\) & 0.4s \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **는 직접 회귀 및 오프라인 방법과 비교되며, 오차**는 테스트 세트의 모든 프레임의 평균이다. 증강 뷰는 \\(C\\) 대신 카메라 세트 \\(C^{\\prime}\\)를 사용하는 것을 의미한다.\n' +
      '\n' +
      '그림 6: ** 반복 정제 억제:**는 검증 동일성에 대한\\(\\mathcal{F})에서 중간 결과 \\(\\mathcal{D}(\\mathbf{z}_{t},\\mathbf{v}_{t})를 보여준다.\n' +
      '\n' +
      '콘텐츠 이미지로서 스타일 이미지 및 \\(\\mathbf{H}\\)를 사용한다. 입력의 내용이 잘 보존되지만 \\(텍스트{StyTr}^{2}\\)는 도메인 격차를 해소하지 못한다. (\\mathbf{z}_{k_{M}}),\\mathbf{v}}}}(\\math{v}{v}}}}) 평균 관점에서 반복되는 피드백의 이점을 보여주기 위해,\\mathbf{v}}}}}(\\math{v}{v}}}} <\\math{v}{v}}}}} <\\math{v} <{v}}}}}}}}}})의 반복 피드백의 이점을 보여주기 위해,\\math{v}}.{v}}}}}}}}}}}}와 함께 렌더링된 a\\math{v} <{v}}}}} <\\math{v} <{v}{v}}}}}}}}}} <\\math{v}:\\math{v}}} <{v}}}}}}}} <\\math{v}} <{v}}}} <{v}}} <\\math{v}}} <{v}}}}}}}} <\\math StyTr\\({}^{2}\\)[8]보다 더 나은 스타일 전달을 생성하지만, 눈과 코 근처의 프리클, 치아, 연조직 변형 등 고주파 디테일을 매끄럽게 해줍니다. 이러한 고주파 디테일은 미묘한 표현을 애니메이션화하는 데 중요합니다. 우리의 스타일 전달 모델 \\(\\mathcal{S}\\)는 \\(\\mathcal{F}_{0}\\)에서 제공하는 추정치를 활용하여 그러한 세부 사항을 유지할 수 있다.\n' +
      '\n' +
      '5축제와 미래일.\n' +
      '\n' +
      '본 논문에서는 소비자 가상현실(VR) 헤드셋 카메라의 단색 이미지에 광학적 3D 아바타를 등록하기 위한 경량 일반 방법을 제시한다. 아바타의 렌더링과 헤드셋 이미지 사이의 도메인 갭을 닫는 것이 높은 등록 품질을 달성하는 핵심임을 보여준다. 이를 통해 문제를 두 개의 모듈로 분해하고 스타일 전달과 반복적 정제하며, 한 명이 다른 모듈을 강화하는 시스템을 제시한다. 실제 캡처 데이터에 대한 광범위한 실험은 우리 시스템이 직접 회귀 방법보다 우수한 등록 품질을 달성하고 온라인 사용을 감당할 수 있음을 보여준다. 우리는 이것이 플라이에서 고품질의 이미지 라벨 쌍을 효율적으로 생성하기 위한 실행 가능한 경로를 제공하여 향후 온 디바이스 계산기로 실시간 표정 인코더를 조정할 수 있다고 믿는다.\n' +
      '\n' +
      '그림 8: ** 스타일 전달 결과에 대한 증폭** \\(\\mathcal{F}_{0}\\)에 의한 추정 없이 일반적인 스타일 전달 방법과 기준 방법과 결과를 비교한다.\n' +
      '\n' +
      '그림 7: ** 정성적 결과:**는 HMC 관점에서 **(b,c,d,e)*** 프론탈 렌더링(오차 맵 포함) 및 **(f,g)*** 오차 맵을 평가하여 서로 다른 방법을 비교한다. 더 많은 예를 들어 부록을 참조하세요.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Jie An, Siyu Huang, Yibing Song, Dejing Dou, Wei Liu, and Jiebo Luo. Artflow: Unbiased image style transfer via reversible neural flows. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 862-871, 2021.\n' +
      '* [2] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 18392-18402, 2023.\n' +
      '* [3] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks). In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, 2017.\n' +
      '* [4] Chen Cao, Qiming Hou, and Kun Zhou. Displaced dynamic expression regression for real-time facial tracking and animation. _ACM Trans. Graph._, 33(4), 2014.\n' +
      '* [5] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz, Michael Zollhoefer, Shun-Suke Saito, Stephen Lombardi, Shih-En Wei, Danielle Belko, Shoou-I Yu, Yaser Sheikh, and Jason Saragih. Authentic volumetric avatars from a phone scan. _ACM Trans. Graph._, 41(4), 2022.\n' +
      '* [6] Haibo Chen, Zhizhong Wang, Huiming Zhang, Zhiwen Zuo, Ailin Li, Wei Xing, Dongming Lu, et al. Artistic style transfer with internal-external learning and contrastive learning. _Advances in Neural Information Processing Systems_, 34:26561-26573, 2021.\n' +
      '* [7] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2018.\n' +
      '* [8] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and Changsheng Xu. Styt\\({}^{2}\\): Image style transfer with transformers. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [9] Piotr Dollar, Peter Welinder, and Pietro Perona. Cascaded pose regression. In _2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pages 1078-1085, 2010.\n' +
      '* [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. _ArXiv_, abs/2010.11929, 2020.\n' +
      '* [11] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.\n' +
      '* [12] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1314-1324, 2019.\n' +
      '* [13] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. _CVPR_, 2017.\n' +
      '* [14] Vahid Kazemi and Josephine Sullivan. One millisecond face alignment with an ensemble of regression trees. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2014.\n' +
      '* [15] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V 16_, pages 491-507. Springer, 2020.\n' +
      '* [16] Hao Li, Laura Trutoiu, Kyle Olszewski, Lingyu Wei, Tristan Trutna, Pei-Lun Hsieh, Aaron Nicholls, and Chongyang Ma. Facial performance sensing head-mounted display. _ACM Transactions on Graphics (TOG)_, 34(4):47:1-47:9, 2015.\n' +
      '* [17] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. _arXiv preprint arXiv:1908.03265_, 2019.\n' +
      '* [18] Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Meiling Wang, Xin Li, Zhengxing Sun, Qian Li, and Errui Ding. Adaattn: Revisit attention mechanism in arbitrary neural style transfer. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6649-6658, 2021.\n' +
      '* [19] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser Sheikh. Deep appearance models for face rendering. _ACM Trans. Graph._, 37(4):68:1-68:13, 2018.\n' +
      '* [20] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. _arXiv preprint arXiv:1906.07751_, 2019.\n' +
      '* [21] Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mixture of volumetric primitives for efficient neural rendering. _ACM Trans. Graph._, 40(4), 2021.\n' +
      '* [22] Meta Inc. Meta Quest Pro: Premium Mixed Reality. [https://www.meta.com/ie/quest/quest-pro/](https://www.meta.com/ie/quest/quest-pro/), 2023.\n' +
      '* [23] Kyle Olszewski, Joseph J. Lim, Shunsuke Saito, and Hao Li. High-fidelity facial and speech animation for vr hmds. _ACM Transactions on Graphics (TOG)_, 35(6):1-14, 2016.\n' +
      '***[24] 니키 파마르, 프라지트 라마차란, 아슈 비스완이, 이와완 비넬, 안셀름 레브스카야, 조온톤 슈렌스 등이다. 시력모델에 있어서 독립자아적 자기의도. 2019.\n' +
      '* [25] Shaoqing Ren, Xudong Cao, Yichen Wei, and Jian Sun. Face alignment at 3000 fps via regressing local binary features. In _2014 IEEE Conference on Computer Vision and Pattern Recognition_, pages 1685-1692, 2014.\n' +
      '* 권 02_, 미국 1196-1195 페이지 2006. IEEE 컴퓨터 학회.\n' +
      '* [27] Gabriel Schwartz, Shih-En Wei, Te-Li Wang, Stephen Lombardi, Tomas Simon, Jason Saragih, and Yaser Sheikh. The eyes have it: An integrated eye and face model for photorealistic facial animation. _ACM Trans. Graph._, 39(4), 2020.\n' +
      '* [28] Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, I. Pasechnik, Dmitry Ulyanov, Alexander Vakhitov, and Victor S. Lempitsky. Textured neural avatars. _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2382-2392, 2019.\n' +
      '* [29] Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias Niessner. Facevr: Real-time gaze-aware facial reenactment in virtual reality. _ACM Transactions on Graphics (TOG)_, 37(2):25:1-25:15, 2018.\n' +
      '* [30] Shih-En Wei, Jason Saragih, Tomas Simon, Adam W. Harley, Stephen Lombardi, Michal Perdoch, Alexander Hypes, Dawei Wang, Herman Badino, and Yaser Sheikh. Vr facial animation via multiview image translation. _ACM Trans. Graph._, 38(4), 2019.\n' +
      '* [31] Xiaolei Wu, Zhihao Hu, Lu Sheng, and Dong Xu. Styleformer: Real-time arbitrary style transfer via parametric style composition. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 14618-14627, 2021.\n' +
      '* [32] Jiahao Xia, Weiwei Qu, Wenjian Huang, Jianguo Zhang, Xi Wang, and Min Xu. Sparse local patch transformer for robust face alignment and landmarks inherent relation learning. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4042-4051, 2022.\n' +
      '* [33] Xuehan Xiong and Fernando De la Torre. Supervised descent method and its applications to face alignment. _2013 IEEE Conference on Computer Vision and Pattern Recognition_, pages 532-539, 2013.\n' +
      '* [34] Yi Zhou, Connelly Barnes, Lu Jingwan, Yang Jimei, and Li Hao. On the continuity of rotation representations in neural networks. In _The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [35] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _IEEE International Conference on Computer Vision (ICCV)_, 2017.\n' +
      '\n' +
      '6번.\n' +
      '\n' +
      '우리는 그림에서 테스트 정체성에 대해 더 질적 결과를 보여준다. 13과 그림. 회귀 방법과 오프라인 방법을 비교한 14개이다. 첨부된 비디오에서 더 많은 결과를 찾을 수 있습니다. 전반적으로 회귀 방법은 표현 측면에서 오류가 더 커서 미묘한 입 모양과 눈에 보이는 치아/통량을 포착하지 못하는 경우가 많다. 반면, 표현코드와 머리 포즈를 천천히 최적화할 수 있는 오프라인 방법은 전체적으로 가장 낮은 표현오차로 이어진다. 그러나 HMC 이미지(예: 그림 13의 1,3열 및 그림 14의 1,3,4,5,8열)에서 키 얼굴 영역이 잘 관찰되지 않을 때. 우리의 방법은 종종 더 나은 표현을 추정한다. 우리의 방법은 머리 포즈 추정에서도 우수합니다. 예를 들어, 그림 3,5의 행에서. 13, 우리의 방법은 전두 오차(발현)가 약간 높은 반면, 오프라인 방법은 HMC 관점(컬럼(f)과 (g)에서 더 높은 이미지 오차로 표시되는 머리 포즈 에러가 더 높다. 이는 모델이 보다 쉽게 과적합할 수 있는 사람별 훈련 체제[27]에서 등록 오류를 보상하는 스타일 전달 모듈로 인해 발생하는 경우가 많다. 대조적으로, 우리의 스타일 전달 모듈은 다양한 정체성 세트에 걸쳐 훈련되며 쉽게 과적합되지 않아 더 나은 유지 안면 구조가 결과적으로 더 정확한 머리 포즈로 이어진다. 그림. 12는 일반적으로 흔하지 않은 표현, HMC 카메라에서 폐색된 입 영역 및 극단적인 머리 포즈로 인한 우리의 방법의 일부 실패 사례를 보여준다.\n' +
      '\n' +
      '디자인 촉매 7개.\n' +
      '\n' +
      '이 섹션은 Iterative Refinement 모듈 \\(\\mathcal{F}\\) 및 Style Transfer 모듈 \\(\\mathcal{S}\\)의 아키텍처에 대한 자세한 설명을 제공한다. 또한 주요 디자인 선택을 검증하기 위해 절제 실험을 수행합니다.\n' +
      '\n' +
      '반복적 정제 모듈.\n' +
      '\n' +
      '반복적 정제 모듈 \\(\\mathcal{F}\\)은 \\(\\ason\\)28M 훈련 가능한 파라미터를 가지고 있다. CNN은 각 카메라 뷰에 대한 크기 \\(128\\t 128\\)의 입력 이미지로 취하는 ResNetV2-50 [15]을 기반으로 하며,\\(512\\t 4\\t 4\\) 그리드 특징을 출력한다. (\\mathbf}_{t},\\mathbf{v}_{t},\\mathbf{v}_{t})\\의 학습 가능한 패치 임베딩 및 뷰 임베딩, 그리고 \\(|C|\\times 4\\t Prevention 4\\t) 피처 토큰의 시퀀스를 추가한 후, 갱신 \\(\\Delta\\mathbf{v}_{t},\\Delta\\mathbf{v}_{t}) 업데이트를 출력하는 ViT 기반 변압기 모듈[10]에 의해 변환(|C|\\i)특성 토큰은 ViT 기반 변압기 4\\T 기반 변압기 모듈[10]에 의해 처리된다. 변압기 모듈은 512-다이임 토큰에서 동작하는 6개의 인코더 층과 4개의 디코더 층으로 구성되어 있다. (\\mathcal{F}_{0}\\)은 스타일 전달 이미지 \\(\\hat{\\mathbf{R}}\\)를 입력으로 하는 것을 제외하고는 \\(\\mathcal{F}\\)와 동일한 구조를 따른다.\n' +
      '\n' +
      '디자인에 대한 핵심은 모든 카메라 뷰의 기능 그리드에 변압기를 적용하는 것입니다. 우리는 이 디자인을 \\(\\mathcal{F}_{0}(\\hat{\\mathbf{R}}_{gt})에 대한 성능과 다음 설정(표 3 참조)을 비교하여 검증한다.\n' +
      '\n' +
      '* **w/o 변압기***, 여기서 우리는 변압기를 MLP로 대체한다. 이 경우, 4개의 카메라 조회들 모두에서 \\(512개 점수 4\\t시간 4\\) 그리드 특징은 단순히 업데이트 \\((\\Delta\\mathbf{z}_{t},\\Delta\\mathbf{v}_{t})를 출력하는 MLP에 의해 연결 및 처리된다. 이 사소한 연결은 훈련 가능한 매개변수의 수를 2배 증가시키는 결과를 초래한다.\n' +
      '* **w/o 그리드 특징***, 여기서 우리는 평균 풀 그리드가 각 카메라 뷰에 대해 단일 \\(512\\)-dim 특징을 얻고 동일한 변압기 설계를 사용하여 \\(|C|\\) 토큰을 처리한다.\n' +
      '* **w/o 변압기 & w/o 그리드 특징***는 MLP를 사용하여 모든 카메라 뷰에서 풀링된 특징의 연결을 처리한다.\n' +
      '\n' +
      '우리는 변압기를 사용한 처리 그리드 특징이 사소한 연결과 함께 MLP를 사용하는 것에 비해 더 적은 매개변수가 필요하면서도 우수한 일반화를 초래한다는 것을 관찰했다. 풀링 그리드 기능은 또한 우리의 모델보다 훨씬 더 나쁜 성능을 발휘합니다. 이것은 입력 픽셀의 사소한 변화라도 표현의 더 큰 변화를 초래할 수 있기 때문에 헤드셋 카메라의 비스듬한 시야각에서 특히 해롭습니다. 그리드 토큰에서 동작하는 변환기는 미세화된 정보를 효과적으로 보존하고 미묘한 표현 내역을 추출할 수 있다.\n' +
      '\n' +
      '스타일 이송 모듈.\n' +
      '\n' +
      '스타일 전달 모듈, \\(\\mathcal{S}\\)는 \\(\\Ex\\)25M 트레이너블 파라미터를 갖고 있으며(\\sim\\)25M 트레이너블 파라미터를 갖고 있으며, 이미지 해상도로 동작한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline  & Aug. & Frontal & Rot. & Trans. \\\\  & Cams & Image \\(L_{1}\\) & Err. & Err. \\\\  & & & (deg.) & (mm) \\\\ \\hline Our \\(\\mathcal{F}_{0}(\\hat{\\mathbf{R}}_{gt}|\\mathcal{D})\\) & ✗ & \\(1.652\\) & \\(0.660\\) & \\(0.618\\) \\\\ w/o transformer & ✗ & 2.533 & 2.335 & 2.023 \\\\ w/o grid features & ✗ & 2.786 & 2.818 & 3.081 \\\\ w/o transformer \\& & & 3.645 & 5.090 & 5.839 \\\\ w/o grid features & & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 3> \\(\\mathcal{F}\\)**\\(192\\)의 설계에 대한 **Ablation. 입력 인코더와 컨디셔닝 인코더는 물론 디코더 모두 UNet 아키텍처를 따른다. UNet의 각 레이어에서 학습 가능한 뷰 임베딩을 통합하여 모든 카메라 뷰에 대해 단일 스타일 전송 네트워크를 훈련합니다. 컨디셔닝 이미지는 아바타 모델인 \\(\\mathcal{D}\\)를 사용하여 생성되기 때문에 렌더링된 이미지와 함께 컨디셔닝 인코더에도 입력된 가이드 메쉬[21]의 전경 마스크 및 투영된 UV 이미지에도 액세스할 수 있다.\n' +
      '\n' +
      '그림. 9는 우리 실험에 사용된 4가지 핵심 컨디셔닝 표현 \\((\\mathbf{z}_{k_{1}}),\\mathbf{z}_{k_{4}})\\를 보여준다. 이러한 표현은 스타일 전송 컨디셔닝에서 정보 결핍을 보상하기 위해 발현 공간의 극단을 덮는 것으로 선택되었으며 추정치 \\(\\mathbf{z}_{0}\\)는 최적이 아니다. 슬라이딩 윈도 인식(SWA) [24]는 입력 가지 각각의 격자 특징이 컨디셔닝 지점의 정렬된 특징 주위에 \\(5\\t 5\\) 이웃에 교차 작용하는 변압기의 교차 의도 계층을 기반으로 한다. SWA는 추정치 \\(\\mathbf{v}_{0}\\)가 최적이면 미등록을 보상한다.\n' +
      '\n' +
      '다음의 기저부와 비교하여 디자인을 검증합니다.\n' +
      '\n' +
      '* **w/o SWA***, 여기서 우리는 각 층에서 컨디셔닝 지점의 특징과 입력 지점의 특징을 단순히 연결한다.\n' +
      '* **w/o 키 컨디셔닝 표현**. 현재 추정치 \\((\\mathbf{z}_{0},\\mathbf{v}_{0})\\에 해당하는 컨디셔닝만 사용된다.\n' +
      '* **w/o \\(\\mathcal{F}_{0}\\)***, 여기서 컨디셔닝은 카메라당 평균 관점, \\(\\mathbf{v}_{\\text{mean}}\\)를 사용하여 렌더링되는 4가지 핵심 표현으로만 구성된다.\n' +
      '\n' +
      '표 4는 테스트 세트에서 평가된 바와 같이 접지진 이미지의 전경 픽셀과 예측된 스타일 전달 이미지 사이의 \\(L_{1}\\) 오차를 보여준다. I\\(\\mathcal{F}_{0}\\)가 없는 스타일 전달의 더 큰 오류는 그라운드 진실 \\((\\mathbf{z}_{gt}_{gt},\\mathbf{v}_{gt})\\에 더 가까운 컨디셔닝을 제공하여 더 나은 스타일 전달을 달성할 수 있다는 동기를 검증한다. SWA 또는 핵심 컨디셔닝 표현을 통합하지 않을 때, 모델은 추정치 \\(\\mathbf{v}_{0}\\) 및 \\(\\mathbf{z}_{0}\\)가 각각 최적이면 제대로 수행되지 않아 오류가 더 높다. 우리는 그림에서 테스트 정체성에 대한 더 많은 스타일 전달 결과를 보여준다. 11.\n' +
      '\n' +
      '8개의 HMC D 브로커가 있습니다.\n' +
      '\n' +
      '이 작업에서 우리는 [30]에서 _트레이닝 헤드셋_ 및 _트랙 헤드셋_의 개념을 따르고 있으며, 여기서 전자는 후자의 카메라 수퍼셋을 가지고 있다(그림 10 참조). 이 작품에서는 추적 헤드셋으로 보다 최근 진행된 VR 소비자 헤드셋 퀘스트프로[22]를 사용하고, 훈련 헤드셋으로 확장된 구조에 추가 카메라로 증강한다. 그림과 같이. 10(a) 훈련 헤드셋에는 10개의 카메라가 있습니다. 우리는 이들 모두를 사용하여 [27]의 방법으로 근거 진실을 확립한다. 추적 헤드셋의 추적 헤드셋 세트 \\(C\\) 상의 추적 헤드셋 세트 \\(C\\)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} \\hline \\hline  & Image \\(L_{1}\\) Error \\\\ \\hline Our \\(\\mathcal{S}\\) & \\(2.55\\) \\\\ w/o SWA & \\(2.82\\) \\\\ w/o key cond. expressions & \\(2.75\\) \\\\ w/o \\(\\mathcal{F}_{0}\\) & \\(2.99\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 4> <표 4> <표 4> <표 4> <표 1> < \\의 설계에 대한 ** 아세틸화>** \\(\\mathcal{S}\\)**＋>와 같다.\n' +
      '\n' +
      '그림 10: **HMC 상세:** 우리는 이 작업에 근거 진실 확립을 위해 헤드셋을 훈련하는 모든 카메라를 사용한다. 본고에서 사용하는\\ 세트 \\(C\\)와 \\(C^{\\prime}\\)는 주석을 달았다.\n' +
      '\n' +
      '그림 9: \\(\\mathcal{S}\\)에 대한 ** 컨디셔닝 익스프레스:**4 컨디셔닝 표현((\\mathbf{z}_{k_{1}},\\mathbf{z}_{k_{4}}))은 세 가지 다른 동일성을 나타낸다.\n' +
      '\n' +
      '본 논문의 비교에 사용되는 치주 세트 \\(C^{\\prime}\\)도 그림 1에 주석을 달았다. 10. 노트는 제한된 관찰 및 극한 조명으로 인해 추적 헤드셋의 사이클로페란 카메라를 카메라 세트(C\\)에서 제외한다. 우리는 또한 입 영역에 초점을 맞추고 훈련 헤드셋의 다른 2개의 눈 카메라와 비교하지 않았다. 모든 카메라는 72fps에서 동기화되고 캡처됩니다.\n' +
      '\n' +
      '9개의 훈련 조회수.\n' +
      '\n' +
      '우리의 모델은 \\(\\mathcal{F}_{0}\\)를 먼저 훈련한 다음 미리 훈련된 \\(\\mathcal{S}\\)를 입력으로 하는 단계에서 훈련된다. (\\mathbf{z}_{0},\\mathbf{v}_{0}_{0})\\(\\mathcal{F}_{0}\\)에서 제공한 추정치 \\(\\mathcal{F}_{0}\\)의 오차 분포는 \\(\\mathcal{F}_{0}\\)에서 \\(\\mathcal{S}_{0}<\\)에 내재된 일반화 갭(\\mathcal{S}_{0}_{0})에서 \\(\\mathcal{S}:\\)에서 \\(\\mathcal{S})에서 \\(\\mathcal{S}_{0})에서 \\(\\mathcal{S}_{0})에 내재된 일반화 갭(\\mathcal{S}_{0}.\\)에 내재된 일반화 갭으로 인해 훈련과 테스트 사이에 달라질 것이다.{S}_{0}_{0}<\\)에 내재된 일반화 갭으로 인해 훈련과 테스트 사이에 다를 것이다. 이러한 불일치를 해결하기 위해 무작위 가우시안 노이즈를 \\(\\mathcal{S}\\) 훈련 시 추정에 도입한다. 유사하게, 우리는 무작위 가우시안 노이즈를 추가한다. \\(\\mathcal{F}\\)를 훈련할 때 \\(\\mathcal{S}\\)의 예측을 나타낸다. (\\mathcal{F}\\)은 \\(T=3\\) 정제 반복을 위해 훈련된다. 각 반복의 구배 교육을 안정화하기 위해 사전 반복으로 역프로파지가 되지 않으며((\\mathbf{z}_{t+1},\\mathbf{v}_{t+1}) 예측을 분리했으며 다음 반복에 대한 입력으로 통과했다.\n' +
      '\n' +
      'I\\(\\mathcal{F}\\)와 \\(\\mathcal{F}_{0}\\)는 모두 RAdam 최적화기[17]를 사용하여 미니부치 크기 4로 200K 단계에 대해 훈련된다. 체중 붕괴는 \\(10^{-4}\\)로 설정되며, 초기 학습율은 \\(3\\t10^{-4}\\)로 설정된다. 그런 다음 이 학습 속도는 코사인 스케줄러를 사용하여 점차 \\(3\\t 10^{-6}\\)로 붕괴된다. (\\mathcal{S}\\)는 체중 붕괴가 \\(3\\t10^{-4}\\)로 설정된 것을 제외하고는 유사하게 훈련된다. 점 \\(\\mathbf{v}\\)의 회전 성분은 네트워크로 전가하기 전에 6D-회전 표현[34]으로 변환된다. 손실 가중치 \\(\\lambda_{\\text{hmc}}\\)와 \\(\\lambda_{\\text{front}}\\)는 모두 1로 설정된다.\n' +
      '\n' +
      '그림 11: ** St스타일 트랜스퍼에 대한 보다 확실한 결과:** 우리는 결과를 \\(\\mathcal{F}_{0}\\)에 의한 추정 없이 일반 스타일 전달 방법과 기준 방법과 비교한다.\n' +
      '\n' +
      '그림 12: ** 방법의 실패 사례:**는 HMC 관점에서 **(b,c,d,e)*** 프론탈 렌더링(오차 맵 포함) 및 **(f,g)*** 오차 맵을 평가하여 서로 다른 방법을 비교한다.\n' +
      '\n' +
      '그림 13: ** 더 많은 양적 결과(1/2):**는 HMC 관점에서 **(b,c,d,e)*** 프론탈 렌더링(오차 맵 포함) 및 **(f,g)*** 오차 맵을 평가하여 서로 다른 방법을 비교한다.\n' +
      '\n' +
      '그림 14: ** 더 많은 양적 결과(2/2):**는 HMC 관점에서 **(b,c,d,e)** 프론탈 렌더링(오차 맵 포함) 및 **(f,g)*** 오차 맵을 평가하여 서로 다른 방법을 비교한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>