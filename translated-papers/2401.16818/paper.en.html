<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# H2O-Danube-1.8B Technical Report\n' +
      '\n' +
      'Philipp Singer Pascal Pfeiffer Yauhen Babakhin Maximilian Jeblick\n' +
      '\n' +
      'Nischay Dhankhar Gabor Fodor Sri Satish Ambati\n' +
      '\n' +
      'H2O.ai\n' +
      '\n' +
      '{firstname.lastname, sri}@h2o.ai\n' +
      '\n' +
      'The first two authors contributed equally.\n' +
      '\n' +
      '## 1 Abstract\n' +
      '\n' +
      'We present _H2O-Danube-1.8B_, a \\(1.8B\\) language model trained on \\(1T\\) tokens following the core principles of LLama 2 and Mistral. We leverage and refine various techniques for pre-training large language models. Although our model is trained on significantly fewer total tokens compared to reference models of similar size, it exhibits highly competitive metrics across a multitude of benchmarks. We additionally release a chat model trained with supervised fine-tuning followed by direct preference optimization. We make _H2O-Danube-1.8B_ openly available under Apache 2.0 license further democratizing LLMs to a wider audience economically.\n' +
      '\n' +
      '**Base model:**[https://huggingface.co/h2oai/h2o-danube-1.8b-base](https://huggingface.co/h2oai/h2o-danube-1.8b-base)\n' +
      '\n' +
      '**Chat model:**[https://huggingface.co/h2oai/h2o-danube-1.8b-chat](https://huggingface.co/h2oai/h2o-danube-1.8b-chat)\n' +
      '\n' +
      '## 2 Introduction\n' +
      '\n' +
      'Research over the past few years has significantly enhanced language models\' capabilities, making them pivotal in tasks like text and code generation, question answering, translation, summarization, and more [42]. Most state-of-the-art large language models (LLMs) leverage decoder attention architectures [41] popularized by the series of GPT models [7; 34; 35] exemplifying the benefits of pre-training such models on extensive text corpora.\n' +
      '\n' +
      'Scaling laws for LLMs suggest that performance scales by factors such as model and dataset size as well as computational resources for training [26]. This understanding has led to the development of a plethora of models, ranging in size to optimize performance given certain data and compute constraints. Among others, notable representatives are: Falcon [33], Llama 2 [40], Owen [3], Mistral [23], or Mistral [24].\n' +
      '\n' +
      'Despite the trend towards larger models, smaller LLMs have taking an important place in today\'s landscape allowing for efficient inference on consumer hardware and edge devices. While larger models often times excel across various generic tasks [3; 23; 40], fine-tuning smaller models for specific tasks can enable competitive performance with benefits of model size and inference speed [16], a concept also proven by the success of BERT and its derivatives [13; 18]. In this work, we want to extend previous research in this area [3; 5; 39; 47; 48] and present _H2O-Danube-1.8B_, a 1.8B large language model with open weights released under Apache 2.0.\n' +
      '\n' +
      '_H2O-Danube-1.8B_ is a decoder LLM architecture adopting core principles from Llama 2 [40] and Mistral [23]. The model is trained on \\(1T\\) tokens from a combination of, but not limited to, web documents, encyclopedia and public knowledge databases, excluding coding data. Despite being trained on significantly fewer total tokens and the exclusion of coding data, compared to recent models released in this parameter range [3; 39; 47], it demonstrates to be highly competitive across various benchmarks. Alongside the base model, we release a chat variant _H2O-Danube-1.8B-Chat_, enhanced with supervised fine-tuning on instruction data and preference data optimization (DPO).\n' +
      '\n' +
      '## 3 Model architecture\n' +
      '\n' +
      'We adjust the Llama 2 architecture [40] for a total of around 1.8b parameters with a hidden size of \\(2,560\\), an intermediate size of \\(6,912\\), and a total of \\(24\\) hidden layers. We use the original Llama 2 tokenizer with a vocabulary size of \\(32,000\\) and train our model up to a context length of \\(16,384\\) (see Section 4). In the following, we elaborate more details about the architecture of _H2O-Danube-1.8B_.\n' +
      '\n' +
      '**Sliding window.** We adopt the sliding window approach for local attention popularized by Mistral [23] as implemented in FlashAttention-2 [12]. For training, we use a fixed sliding window of \\(4,096\\).\n' +
      '\n' +
      '**Rotary Positional Embedding.** To model dependencies of elements at different positions of a sequence, we use the Rotary Positional Embedding (RoPE) technique as introduced by Su et al. [38] and successfully being applied in multiple popular foundation models [23; 40].\n' +
      '\n' +
      '**Grouped-query attention.** For reducing the memory bandwidth overhead, we utilize grouped-query attention [1], setting our architecture to use \\(32\\) attention heads and \\(8\\) key-value heads.\n' +
      '\n' +
      '**Further details.** We rely on root mean square layer normalization (RMSNorm) [46] separately for pre- and post-normalization to stabilize training as commonly used in modern LLMs [40]. We do not use bias within linear layers nor tie word embeddings.\n' +
      '\n' +
      '## 4 Training\n' +
      '\n' +
      'We train on a single node consisting of 8xH100 GPUs. With Distributed Data Parallel (DDP), each GPU holds a full copy of the model. For finding good settings for our training routine and hyper-parameters, we conducted initial experiments on smaller subsets of the data and model sizes up to \\(500M\\) parameters.\n' +
      '\n' +
      'Figure 1: **Training logs.** Training (top left) and validation (top right) cross-entropy loss, learning rate schedule (bottom left) and sequence length (bottom right). X-axis shows the number of tokens that have been trained up to the step.\n' +
      '\n' +
      'Among other findings, these initial experiments showed, that for higher token throughput and compute efficiency, we can iteratively increase the sequence length during the training using a constant sliding window of 4,096 (see Section 3). Out of the \\(1T\\) tokens in total, we train subsequently on\n' +
      '\n' +
      '* \\(700B\\) tokens with a sequence length of \\(2,048\\),\n' +
      '* \\(100B\\) tokens with a sequence length of \\(4,096\\),\n' +
      '* \\(100B\\) tokens with a sequence length of \\(8,192\\),\n' +
      '* \\(100B\\) tokens with a sequence length of \\(16,384\\).\n' +
      '\n' +
      'We employ recent advances in 8-bit floating-point (FP8) calculations on the Hopper architecture to further speed up the training. For this, we cast all linear layers in the Grouped-Query Attention and in the Multi-Layer Perceptron to FP8. The lm_head layer remains in bfloat16 precision to ensure training stability.\n' +
      '\n' +
      'We use AdamW optimizer [28] with \\(\\beta_{1}=0.9\\) and \\(\\beta_{2}=0.95\\) as well as a cosine learning rate scheduler. We warm up the learning rate for \\(\\sim\\)\\(2.36B\\) tokens to a maximum of \\(2e-4\\) and then decay it to a minimum of \\(1e-5\\). Our total batch size across GPUs is \\(\\sim\\)\\(1.18M\\) tokens, weight decay is \\(1.e-1\\) and gradient clipping threshold is set to \\(1.0\\). With these settings, we achieved an average throughput of \\(292.7k\\) tokens/s on the single node during training.\n' +
      '\n' +
      '## 5 Results\n' +
      '\n' +
      'We evaluate _H2O-Danube-1.8B_ on a wide range of benchmarks and compare it with other existing open-source language models which have a similar number of parameters. Such models include TinyLlama with \\(1.1B\\) parameters [47], Falcon with \\(1.3B\\) parameters [33], OPT with \\(1.3B\\) and \\(2.7B\\) parameters [48], Cerebras-GPT with \\(1.3B\\) and \\(2.7B\\) parameters [14], Pythia-deduped with \\(1.4B\\) and \\(2.8B\\) parameters [5], Qwen with \\(1.8B\\) parameters [3], and most recent Stable LM 2 with \\(1.6B\\) parameters [39]. The majority of these models have Apache 2.0 license, however, Stable LM 2 and Qwen require additional conditions for commercial use and are marked with an asterisk in Table 1.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Size** & **Tokens** & **ARC-e** & **ARC-c** & **Bool** & **HS** & **OB** & **PIQA** & **Triv** & **Wino** \\\\  & & & acc\\_n & acc\\_n & acc & acc\\_n & acc\\_n & acc\\_n & em & acc \\\\ \\hline TinyLlama & 1.1B & 3.00T & 55.35 & 30.12 & 57.80 & 59.18 & 36.00 & 73.18 & 28.78 & 58.88 \\\\ \\hline Falcon & 1.3B & 0.35T & 57.32 & 32.00 & 62.84 & 61.52 & 35.20 & 74.65 & 27.27 & 60.77 \\\\ \\hline OPT & 1.3B & 0.18T & 50.93 & 29.52 & 57.71 & 53.73 & 33.40 & 72.52 & 15.67 & 59.59 \\\\  & 2.7B & 0.18T & 54.34 & 31.31 & 60.31 & 60.61 & 35.20 & 74.81 & 22.38 & 60.85 \\\\ \\hline Cerebras & 1.3B & 0.03T & 45.88 & 25.26 & 59.36 & 38.37 & 29.20 & 66.76 & 05.49 & 52.01 \\\\  & 2.7B & 0.05T & 52.53 & 27.30 & 59.20 & 48.84 & 32.00 & 70.89 & 12.41 & 55.96 \\\\ \\hline Pythia & 1.4B & 0.30T & 56.57 & 29.86 & 56.76 & 54.40 & 33.20 & 72.36 & 18.46 & 56.20 \\\\  & 2.8B & 0.30T & 58.96 & 32.68 & 64.19 & 59.45 & 35.60 & 73.78 & 24.39 & 58.17 \\\\ \\hline Qwen* & 1.8B & 2.20T & 58.25 & 34.98 & 67.13 & 58.82 & 33.40 & 72.85 & 23.92 & 58.96 \\\\ \\hline Stable LM 2* & 1.6B & 4.00T & **67.63** & **39.08** & **75.60** & **68.78** & **40.00** & 76.39 & 33.84 & **63.30** \\\\ \\hline H2O-Danube & 1.8B & 1.00T & 62.29 & 35.84 & 65.81 & 68.20 & 37.60 & **76.93** & **38.99** & 61.96 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Commonsense reasoning, world knowledge and reading comprehension benchmarks.**_H2O-Danube-1.8B_ exhibits consistently good results across all the benchmarks compared to other models of a similar size. It shows better performance than Qwen on all the benchmarks except for BooIQ, being of the same size but trained on 2.2 times fewer tokens. Stable LM 2 slightly outperforms _H2O-Danube-1.8B_ on the majority of the benchmarks, but was trained on four times the number of tokens. Moreover, neither Qwen nor Stable LM 2 models have the Apache 2.0 license requiring additional conditions for commercial use.\n' +
      '\n' +
      'To evaluate the models, we use the Language Model Evaluation Harness framework [17]. Specifically, we use the version of the framework that is utilized in Open LLM Leaderboard [4]. We report model capabilities across a wide variety of benchmark domains: commonsense reasoning, world knowledge, reading comprehension and an aggregated Open LLM Leaderboard benchmark.\n' +
      '\n' +
      '**Commonsense Reasoning.** In Table 1 we present six standard common sense reasoning benchmarks in the 0-shot setting: ARC easy and challenge [9], HellaSwag [45], OpenBookQA [29], PIQA [6], Winogrande [37].\n' +
      '\n' +
      '**World Knowledge.** We evaluate 5-shot performance on TriviaQA [25] which represents a closed-book question answering benchmark. Results are presented in Table 1.\n' +
      '\n' +
      '**Reading Comprehension.** We report 0-shot performance on BoolQ [8]. Results are presented in Table 1.\n' +
      '\n' +
      '**Open LLM Leaderboard.** It evaluates models on six key benchmarks which test a variety of reasoning and general knowledge across a wide range of fields in 0-shot and few-shot settings. It consists of the following benchmarks: ARC challenge (25-shot) [9], HellaSwag (10-shot) [45], MMLU (5-shot) [19], TruthfulQA (0-shot) [27], Winogrande (5-shot) [37], GSM8k (5-shot) [10]. Results are presented in Table 2\n' +
      '\n' +
      'For each model in Table 1 we report its number of parameters and the total number of tokens it was trained on. _H2O-Danube-1.8B_ achieves good results across all the commonsense reasoning, world knowledge and reading comprehension benchmarks compared to other models of a similar size. The closest competitors are Owen and Stable LM 2 models. _H2O-Danube-1.8B_ shows better performance than Owen on all the benchmarks except for BoolQ. Owen model has the same 1.8B parameters but was trained on 2.2 times more tokens - 2.2T. At the same time, _H2O-Danube-1.8B_ is slightly worse than Stable LM 2 on the majority of the benchmarks, while Stable LM 2 was trained on four times more tokens - 2T tokens for 2 epochs. Also, it is important to note that neither Owen nor Stable LM 2 models have the Apache 2.0 license requiring additional conditions for commercial use.\n' +
      '\n' +
      'Similarly, _H2O-Danube-1.8B_, Qwen and Stable LM 2 are the strongest models on Open LLM Leaderboard (see Table 2). All of them have comparable results on the majority of the benchmarks except for MMLU and GSM8k. One of the potential explanations for such a behavior might be a specifically tailored data that was used for training of Qwen and Stable LM 2 models. Such data can be utilized to improve some particular benchmarks, for example, Qwen used gsm8k-ScRel dataset [44] for better math reasoning.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Size** & **ARC** & **HS** & **MMLU** & **TQA** & **Wino** & **GSM** & **Average** & **Average** \\\\  & & 25-shot & 10-shot & 5-shot & 0-shot & 5-shot & 5-shot & & no GSM \\\\ \\hline TinyLlama & 1.1B & 33.87 & 60.31 & 26.04 & 37.32 & 59.51 & 01.44 & 36.42 & 43.41 \\\\ \\hline Falcon & 1.3B & 35.07 & 63.56 & 25.28 & 35.96 & 62.04 & 00.53 & 37.07 & 44.38 \\\\ \\hline \\multirow{2}{*}{OPT} & 1.3B & 29.52 & 54.53 & 24.96 & 38.71 & 59.75 & 00.15 & 34.60 & 41.49 \\\\  & 2.7B & 33.96 & 61.43 & 25.43 & 37.43 & 61.96 & 00.23 & 36.74 & 44.04 \\\\ \\hline \\multirow{2}{*}{Cerebras} & 1.3B & 26.28 & 38.54 & 26.59 & 42.70 & 53.43 & 00.23 & 31.30 & 37.51 \\\\  & 2.7B & 29.10 & 49.29 & 25.17 & 41.37 & 54.14 & 00.45 & 33.25 & 39.81 \\\\ \\hline \\multirow{2}{*}{Pythia} & 1.4B & 32.68 & 54.96 & 25.56 & 38.66 & 57.30 & 00.83 & 35.00 & 41.83 \\\\  & 2.8B & 36.26 & 60.66 & 26.78 & 35.56 & 60.22 & 00.83 & 36.72 & 43.90 \\\\ \\hline Qwen* & 1.8B & 37.71 & 58.87 & **46.37** & **39.41** & 61.72 & **24.41** & 44.75 & 48.82 \\\\ \\hline Stable LM 2* & 1.6B & **43.52** & **70.45** & 39.08 & 36.81 & **65.82** & 17.36 & **45.51** & **51.14** \\\\ \\hline H2O-Danube & 1.8B & 39.68 & 69.75 & 25.97 & 33.63 & 64.17 & 02.05 & 39.21 & 46.64 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Open LLM Leaderboard.** For each model in the table we report all the individual benchmarks, the average score and the average score without GSM8k benchmark. _H2O-Danube-1.8B_ shows the results similar to Qwen and Stable LM 2 models on the majority of the benchmarks apart from GSM8k and MMLU. It can be explained by the data used for model training, for example, Qwen used gsm8k-ScRel dataset [44] for the better math reasoning.\n' +
      '\n' +
      '## 6 Chat Fine-Tuning\n' +
      '\n' +
      'One of the most common use cases for LLMs evolves around instructing and chatting. We thus also provide a chat fine-tuned version _H2O-Danube-1.8B-Chat_ released under Apache 2.0. We utilize _H2O LLM Studio2_, an Apache 2.0 open-source framework and no-code GUI for fine-tuning LLMs.\n' +
      '\n' +
      'Footnote 2: [https://github.com/h2oai/h2o-llmstudio](https://github.com/h2oai/h2o-llmstudio)\n' +
      '\n' +
      '### Supervised fine-tuning\n' +
      '\n' +
      'As first step, we tune the base model using supervised fine-tuning (SFT) on input/output conversational pairs. In detail, we combine the following datasets totalling \\(157k\\) samples: OpenOrca [31] following the steps outlined in Orca [30], MetaMathQA [43], UltraChat200k [15; 21], and Oasst2 [32].\n' +
      '\n' +
      'We train all layers of the model for a single epoch using a learning rate of \\(1e-5\\), a batch size of \\(8\\), and using cosine learning rate scheduling with a short warmup. We use the full pre-trained context length of \\(16,384\\), mask the prompt loss, and use a custom prompt format. Hyperparameters were optimized iterating over multiple experiments.\n' +
      '\n' +
      '### Dpo\n' +
      '\n' +
      'We follow supervised fine-tuning, by direct preference optimization (DPO) [36] using a combination of the following datasets totalling around \\(17k\\) samples: UltraFeedback Binarized [11], Orca DPO Pairs [22] and Distilabel Math Preference DPO [2]. The DPO model is trained using LoRA [20] with \\(r=4\\), \\(alpha=\\)16 for one epoch using a batch size of \\(2\\) with a learning rate of \\(1e-5\\) using cosine learning rate decay, and \\(beta=0.2\\) for DPO loss.\n' +
      '\n' +
      'Afterwards, we run a final DPO fine-tune using Oasst2 [32] dataset building preference pairs from ranks where the chosen answer is the lowest rank, and the rejected answer is the highest one, limited to only English conversations totalling around \\(5k\\) samples. The training run uses similar hyperparameters as the previous one, just a lower learning rate of \\(3e-6\\).\n' +
      '\n' +
      '### Mt-bench chat benchmark.\n' +
      '\n' +
      'This table shows both turn 1 and turn 2 evaluations for mt-bench excluding the coding category. Results highlight the excellent performance of _H2O-Danube-1.8B-Chat_, particularly for single turn conversations where it exhibits the highest Mt-bench scores for multiple categories and the average.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multicolumn{2}{c}{TinyLlama-1.1B-Chat} & Qwen-1.8B-Chat & Stablelm-2-Zephyr-1.6B & H2O-Danube-1.8B-Chat \\\\ \\hline \\multicolumn{5}{c}{Turn 1} \\\\ \\hline Extraction & 2.50 & 4.70 & **5.20** & 3.40 \\\\ Humanities & 8.15 & 6.60 & **9.20** & 8.90 \\\\ Math & 1.20 & 2.40 & 3.50 & **3.80** \\\\ Reasoning & 3.10 & 3.50 & 3.50 & **4.30** \\\\ Roleplay & 5.60 & 6.70 & 6.80 & **7.05** \\\\ STEM & 6.60 & 6.50 & 7.35 & **8.10** \\\\ Writing & 8.65 & 9.20 & **9.35** & **9.35** \\\\ \\hline Average & 5.11 & 5.66 & **6.41** & **6.41** \\\\ \\hline \\multicolumn{5}{c}{Turn 2} \\\\ \\hline Extraction & 1.20 & 2.40 & **4.80** & 3.20 \\\\ Humanities & 8.10 & 7.30 & **9.70** & 8.90 \\\\ Math & 1.40 & **1.60** & **1.60** & 1.50 \\\\ Reasoning & 2.30 & **3.90** & 3.20 & 2.70 \\\\ Roleplay & 5.60 & 6.70 & **6.95** & 6.90 \\\\ STEM & 4.60 & 5.80 & **6.80** & 6.10 \\\\ Writing & 2.70 & 4.80 & **7.50** & 3.10 \\\\ \\hline Average & 3.70 & 4.64 & **5.79** & 4.63 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **Mt-bench chat benchmark.** This table shows both turn 1 and turn 2 evaluations for mt-bench excluding the coding category. Results highlight the excellent performance of _H2O-Danube-1.8B-Chat_, particularly for single turn conversations where it exhibits the highest Mt-bench scores for multiple categories and the average.\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      'Evaluating chat and instruct fine-tuned LLMs remains a critical challenge and can most reliably be conducted by large scale human assessment. In order to give an initial evaluation of our chat model, we resort to _MT-Bench_, a collection of multi-turn questions across different categories followed by judgement by GPT4 [49]. We keep all categories apart from coding which is out of scope for _H2O-Danube-1.8B_. Each model is run with \\(repetition\\_penalty=1.1\\) and \\(temperature=0.0\\) to reduce randomness and a more fair comparison between models.\n' +
      '\n' +
      'We compare our results to popular chat models below \\(2B\\) parameters and highlight them in Table 3. We can see that _H2O-Danube-1.8B-Chat_ is exhibiting strong results across categories, particularly for natural language tasks as focused on in this work. For single turn conversations, _H2O-Danube-1.8B-Chat_ is the best model for five out of seven categories, and on average on-par with Stablem 2. For turn 2, we can see that it is comparable to Qwen 2, while Stablem 2 outperforms other models.\n' +
      '\n' +
      'We make the intermediate sft version3 as well as the final DPO model weights4 available online. We plan on exploring further improvements for the chat version in the future, working on SFT as well as improved DPO. Particularly, we aim at enhancing preference data with multi turn conversations. We also hope for the open source community to further fine-tune our models for various use cases.\n' +
      '\n' +
      'Footnote 3: [https://huggingface.co/h2oai/h2o-danube-1.8b-sft](https://huggingface.co/h2oai/h2o-danube-1.8b-sft)\n' +
      '\n' +
      'Footnote 4: [https://huggingface.co/h2oai/h2o-danube-1.8b-chat](https://huggingface.co/h2oai/h2o-danube-1.8b-chat)\n' +
      '\n' +
      'Additionally, we also evaluate chat models on commonsense reasoning, world knowledge, reading comprehension and aggregated Open LLM Leaderboard benchmarks. Similarly as for base models, we report 0-shot benchmark results of the chat versions of _H2O-Danube-1.8B_, TinyLlama, Qwen and Stable LM 2 in Table 4, and Open LLM Leaderboard results are available in Table 5. We show that _H2O-Danube-1.8B-Chat_ and Stablem-2-Zephyr perform better than Qwen-Chat and TinyLlama-Chat models on the majority of the benchmarks while being on par between each other. Only exceptions are, again, MMLU and GSM8k benchmarks. As we mentioned in Section 5, one of the potential explanations for the worse _H2O-Danube-1.8B_ performance might be a specifically tailored data that was used for training of Qwen and Stable LM 2 base models to optimize those benchmarks.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Size** & **ARC-e** & **ARC-c** & **Bool** & **HS** & **OB** & **PIQA** & **Triv** & **Wino** \\\\  & & acc\\_n & acc\\_n & acc & acc\\_n & acc\\_n & acc\\_n & acc\\_n & em & acc \\\\ \\hline TinyLlama-1.1B-Chat & 1.1B & 54.34 & 33.36 & 60.83 & 60.39 & 37.20 & 74.59 & 29.04 & 59.91 \\\\ \\hline Qwen-1.8B-Chat & 1.8B & 49.28 & 32.94 & 67.74 & 54.73 & 34.60 & 71.82 & 19.04 & 59.43 \\\\ \\hline Stablem-2-Zephyr-1.6B & 1.6B & 60.35 & **39.25** & **80.18** & **68.85** & **39.60** & 74.92 & 31.46 & 64.48 \\\\ \\hline H2O-Danube-1.8B-Chat & 1.8B & **67.51** & **39.25** & 77.89 & 67.60 & 39.20 & **76.71** & **36.29** & **65.35** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: **Commonsense reasoning, world knowledge and reading comprehension benchmarks for chat models.**_H2O-Danube-1.8B-Chat_ outperforms TinyLlama-Chat and Qwen-Chat models, and is on-par with Stablem-2-Zephyr on all 0-shot benchmarks. As we mentioned in Section 5, one of the potential explanations for the worse _H2O-Danube-1.8B_ performance might be a specifically tailored data that was used for training of Qwen and Stable LM 2 base models to optimize those benchmarks.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Size** & **ARC** & **HS** & **MMLU** & **TQA** & **Wino** & **GSM** & **Average** \\\\  & & 25-shot & 10-shot & 5-shot & 0-shot & 5-shot & 5-shot & \\\\ \\hline TinyLlama-1.1B-Chat & 1.1B & 36.01 & 61.05 & 25.04 & 37.86 & 60.77 & 02.35 & 37.18 \\\\ \\hline Qwen-1.8B-Chat & 1.8B & 36.95 & 54.34 & **44.55** & 43.70 & 58.88 & 19.26 & 42.94 \\\\ \\hline Stablem-2-Zephyr-1.6B & 1.6B & **43.69** & **69.34** & 41.85 & **45.21** & 64.09 & **35.18** & **49.89** \\\\ \\hline H2O-Danube-1.8B-Chat & 1.8B & 41.47 & 68.02 & 33.49 & 40.82 & **64.40** & 15.54 & 43.96 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: **Open LLM Leaderboard for chat models.**_H2O-Danube-1.8B-Chat_ shows better results than TinyLlama-Chat, and similar results to Qwen-Chat and Stablelm-2-Zephyr models on the majority of benchmarks apart from GSM8k and MMLU, as also already imminent from results on base models discussed in Table 2.\n' +
      '\n' +
      '## 7 Conclusions\n' +
      '\n' +
      'We introduce _H2O-Danube-1.8B_, a new open-source pre-trained foundation model with \\(1.8B\\) parameters, that was trained on \\(1T\\) tokens from diverse sources. The Apache 2.0 license allows for commercial use and for further fine-tuning by the community. We also release a SFT + DPO fine-tuned chat version, exhibiting state-of-the art results in commonsense reasoning, world knowledge and reading comprehension benchmarks. We show that _H2O-Danube-1.8B_ outperforms other models of a similar size on many of the benchmarks. _H2O-Danube-1.8B_ is our first contribution to the growing ecosystem of permissive open-source foundation models and we strive to continue publishing high quality foundation models and chat fine-tunes in the near future. Notably, small models can be used on consumer hardware further democratizing LLMs to a wider audience economically.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebron, and S. Sanghai (2023) Gqa: training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245. Cited by: SS1.\n' +
      '* [2]A. Beeching, C. Fourrier, N. Habib, S. Han, N. Lambert, N. Rajani, O. Sanseviero, L. Tunstall, and T. Wolf (2023) Open llm leaderboard. Note: [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) External Links: Link Cited by: SS1.\n' +
      '* [3]S. Biderman, H. Schoelkopf, Q. A. Anthony, H. Bradley, K. O\'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Pasrashth, E. Raff, et al. (2023) Pythia: a suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397-2430. Cited by: SS1.\n' +
      '* [4]Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. (2020) Piqa: reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, pp. 7432-7439. Cited by: SS1.\n' +
      '* [5]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.\n' +
      '* [6]C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova (2019) Boolq: exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044. Cited by: SS1.\n' +
      '* [7]P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabbharwal, C. Schoenick, and O. Tafjord (2018) Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Cited by: SS1.\n' +
      '* [8]K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. (2021) Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Cited by: SS1.\n' +
      '* [9]G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun (2023) Ultrafeedback: boosting language models with high-quality feedback. Cited by: SS1.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* [15] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. _arXiv preprint arXiv:2305.14233_, 2023.\n' +
      '* [16] Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. _arXiv preprint arXiv:2301.12726_, 2023.\n' +
      '* [17] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021.\n' +
      '* [18] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. _arXiv preprint arXiv:2006.03654_, 2020.\n' +
      '* [19] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.\n' +
      '* [20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [21] HuggingFaceH4.ultrachat_200k, 2023. Last accessed on 2024-01-15. [https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k).\n' +
      '* [22] Intel. Orca dpo pairs, 2023. Last accessed on 2024-01-15. [https://huggingface.co/datasets/Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs).\n' +
      '* [23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* [24] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral of experts, 2024.\n' +
      '* [25] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. _arXiv preprint arXiv:1705.03551_, 2017.\n' +
      '* [26] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* [27] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2022.\n' +
      '* [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* [29] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. _arXiv preprint arXiv:1809.02789_, 2018.\n' +
      '* [30] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. _arXiv preprint arXiv:2306.02707_, 2023.\n' +
      '* [31] Open-Orca. Openorca, 2023. Last accessed on 2024-01-15. [https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca).\n' +
      '* [32] OpenAssistant. oaast2, 2023. Last accessed on 2024-01-15. [https://huggingface.co/datasets/OpenAssistant/oaast2](https://huggingface.co/datasets/OpenAssistant/oaast2).\n' +
      '* [33] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. _arXiv preprint arXiv:2306.01116_, 2023.\n' +
      '\n' +
      '* [34] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.\n' +
      '* [35] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n' +
      '* [36] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _arXiv preprint arXiv:2305.18290_, 2023.\n' +
      '* [37] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106, 2021.\n' +
      '* [38] Jianlin Su, Muratdha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.\n' +
      '* [39] Stability AI Language Team. Introducing stable lm 2 1.6b. Last accessed on 2024-01-22. [https://stability.ai/news/introducing-stable-lm-2](https://stability.ai/news/introducing-stable-lm-2).\n' +
      '* [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [42] Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. _arXiv preprint arXiv:2303.10420_, 2023.\n' +
      '* [43] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. _arXiv preprint arXiv:2309.12284_, 2023.\n' +
      '* [44] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models, 2023.\n' +
      '* [45] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? _arXiv preprint arXiv:1905.07830_, 2019.\n' +
      '* [46] Biao Zhang and Rico Sennrich. Root mean square layer normalization. _Advances in Neural Information Processing Systems_, 32, 2019.\n' +
      '* [47] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. _arXiv preprint arXiv:2401.02385_, 2024.\n' +
      '* [48] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.\n' +
      '* [49] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _arXiv preprint arXiv:2306.05685_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>