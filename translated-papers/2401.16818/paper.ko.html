<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# H2O-다뉴브-1.8B 기술보고서\n' +
      '\n' +
      '필리핀 가수 파스칼 파이퍼 야언 바바킨 막시밀리안 제블릭\n' +
      '\n' +
      '이스체이 다한카르 가보 포드 스리 사티 암바티\n' +
      '\n' +
      'H2O.ai\n' +
      '\n' +
      '{firstname.lastname, sri}@h2o.ai\n' +
      '\n' +
      '처음 두 저자는 동등하게 기여했다.\n' +
      '\n' +
      '## 1 Abstract\n' +
      '\n' +
      '우리는 LLama 2와 Mistral의 핵심 원리에 따라 \\(1T\\) 토큰으로 훈련된 \\(1.8B\\) 언어 모델인 _H2O-Danube-1.8B_를 제시한다. 우리는 대규모 언어 모델을 사전 훈련하기 위해 다양한 기술을 활용하고 개선합니다. 우리의 모델은 유사한 크기의 참조 모델에 비해 훨씬 적은 총 토큰에 대해 훈련되지만, 많은 벤치마크에서 매우 경쟁력 있는 메트릭을 나타낸다. 감독 미세 조정에 이어 직접 선호도 최적화로 훈련된 채팅 모델을 추가로 출시합니다. 우리는 아파치 2.0 라이선스에서 LLM을 경제적으로 더 많은 청중에게 민주화하는 _H2O-다뉴브-1.8B_를 공개적으로 사용할 수 있도록 한다.\n' +
      '\n' +
      '**Base model:**[https://huggingface.co/h2oai/h2o-danube-1.8b-base](https://huggingface.co/h2oai/h2o-danube-1.8b-base)\n' +
      '\n' +
      '**Chat model:**[https://huggingface.co/h2oai/h2o-danube-1.8b-chat](https://huggingface.co/h2oai/h2o-danube-1.8b-chat)\n' +
      '\n' +
      '## 2 Introduction\n' +
      '\n' +
      '지난 몇 년 동안의 연구는 언어 모델의 능력을 상당히 향상시켜 텍스트 및 코드 생성, 질문 응답, 번역, 요약 등과 같은 작업에서 중추적인 역할을 했다[42]. 대부분의 최신 대형 언어 모델(LLM)은 일련의 GPT 모델[7; 34; 35]에 의해 대중화된 디코더 주의 아키텍처[41]를 레버리지하여 광범위한 텍스트 코퍼스에서 이러한 모델을 사전 훈련하는 것의 이점을 예시한다.\n' +
      '\n' +
      'LLM에 대한 스케일링 법칙은 훈련용 전산자원뿐만 아니라 모델 및 데이터셋 크기와 같은 요인별 성능척도를 제시한다[26]. 이러한 이해는 특정 데이터와 계산 제약 조건이 주어진 성능을 최적화하기 위해 크기에 따라 다양한 모델의 개발로 이어졌다. 그 중에서도 주목할 만한 대표자는 팔콘[33], 라마2[40], 오웬[3], 미스트랄[23], 또는 미스트랄[24]이다.\n' +
      '\n' +
      '더 큰 모델에 대한 추세에도 불구하고, 더 작은 LLM은 오늘날의 환경에서 소비자 하드웨어 및 에지 장치에 대한 효율적인 추론을 가능하게 하는 중요한 위치를 차지하고 있다. 더 큰 모델은 종종 다양한 일반 작업[3; 23; 40]에 걸쳐 탁월하지만, 특정 작업에 대해 더 작은 모델을 미세 조정하는 것은 모델 크기와 추론 속도의 이점으로 경쟁적 성능을 가능하게 할 수 있으며[16], BERT 및 그 파생물의 성공으로 증명된 개념[13; 18]도 가능하다. 본 연구에서는 이 영역[3; 5; 39; 47; 48]에 대한 선행 연구를 확장하여 Apache 2.0 하에서 공개된 개방형 가중치를 갖는 1.8B 대형 언어 모델인 _H2O-Danube-1.8B_를 제시하고자 한다.\n' +
      '\n' +
      '_H2O-다뉴브-1.8B_는 Llama 2[40] 및 Mistral[23]로부터의 핵심 원리들을 채택하는 디코더 LLM 아키텍처이다. 모델은 코딩 데이터를 제외한 웹 문서, 백과사전 및 공공 지식 데이터베이스의 조합으로부터 \\(1T\\) 토큰에 대해 훈련된다. 이 매개변수 범위[3; 39; 47]에서 출시된 최근 모델에 비해 훨씬 적은 총 토큰과 코딩 데이터의 배제에 대해 훈련되었음에도 불구하고 다양한 벤치마크에서 높은 경쟁력을 보여준다. 기본 모델과 함께 명령어 데이터 및 선호 데이터 최적화(DPO)에 대한 감독 미세 조정으로 강화된 채팅 변형 _H2O-다뉴브-1.8B-Chat_를 출시한다.\n' +
      '\n' +
      '## 3 모델 아키텍처\n' +
      '\n' +
      'Llama 2 구조 [40]은 은닉층의 크기가 \\(2,560\\), 중간 크기의 \\(6,912\\), 은닉층의 크기가 \\(24\\)인 약 1.8b 파라미터에 대해 조정한다. 어휘크기가 \\(32,000\\)인 원래의 Llama 2 토큰화기를 사용하고, 문맥길이 \\(16,384\\)까지 모델을 학습시킨다(4절 참조). 아래에서는 _H2O-다뉴브-1.8B_의 아키텍처에 대해 자세히 설명한다.\n' +
      '\n' +
      '**슬라이딩 윈도우.** 플래시 어텐션-2[12]에서 구현된 바와 같이 미스트랄[23]에 의해 대중화된 로컬 어텐션을 위한 슬라이딩 윈도우 접근법을 채택한다. 훈련에는 \\(4,096\\)의 고정 슬라이딩 윈도우를 사용한다.\n' +
      '\n' +
      '**Rotary Positional Embedding.** Sequence의 서로 다른 위치에 있는 요소의 종속성을 모델링하기 위해 Su et al. [38]에 의해 소개된 로터리 Positional Embedding(RoPE) 기법을 사용하고 다수의 인기 있는 기초 모델[23; 40]에 성공적으로 적용되었다.\n' +
      '\n' +
      '**Grouped-query attention.** 메모리 대역폭 오버헤드를 줄이기 위해 Grouped-query attention [1]을 사용하며, \\(32\\) attention head와 \\(8\\) key-value head를 사용하도록 아키텍처를 설정한다.\n' +
      '\n' +
      '**더 자세한 사항.** 우리는 현대 LLMs[40]에서 일반적으로 사용되는 바와 같이 트레이닝을 안정화시키기 위해 사전 및 사후 정규화를 위해 RMSNorm(Root Mean Square Layer Normalization) [46]에 별도로 의존한다. 우리는 선형 레이어 내에서 바이어스를 사용하거나 단어 임베딩을 묶지 않는다.\n' +
      '\n' +
      '## 4 Training\n' +
      '\n' +
      '우리는 8xH100 GPU로 구성된 단일 노드에서 훈련한다. 분산 데이터 병렬(DDP)을 사용하면 각 GPU가 모델의 전체 사본을 보유합니다. 학습 루틴 및 하이퍼 파라미터에 대한 좋은 설정을 찾기 위해 데이터의 더 작은 부분 집합과 최대 500M\\의 모델 크기에 대한 초기 실험을 수행했다.\n' +
      '\n' +
      '도 1: **트레이닝 로그.**트레이닝(좌상단) 및 유효성 검사(우단) 교차 엔트로피 손실, 학습률 스케쥴(좌하단) 및 시퀀스 길이(우하단) X축은 단계까지 트레이닝된 토큰의 개수를 나타낸다.\n' +
      '\n' +
      '다른 연구 결과들 중에서, 이러한 초기 실험들은 더 높은 토큰 처리량과 계산 효율을 위해, 4,096의 일정한 슬라이딩 윈도우를 사용하여 트레이닝 동안 시퀀스 길이를 반복적으로 증가시킬 수 있다는 것을 보여주었다(섹션 3 참조). 총 \\(1T\\) 토큰 중에서 우리는 후속적으로 훈련한다.\n' +
      '\n' +
      '*\\(700B\\) 토큰들의 시퀀스 길이가 \\(2,048\\)이고,\n' +
      '*\\(100B\\) 토큰들의 시퀀스 길이가 \\(4,096\\)이고,\n' +
      '*\\(100B\\) 토큰들의 시퀀스 길이가 \\(8,192\\)이고,\n' +
      '*\\(100B\\) 토큰의 시퀀스 길이는 \\(16,384\\)이다.\n' +
      '\n' +
      '우리는 최근 8비트 부동 소수점(FP8) 계산의 발전을 호퍼 아키텍처에 적용하여 훈련을 더욱 빠르게 한다. 이를 위해 Grouped-Query Attention과 Multi-Layer Perceptron에서 FP8로 모든 선형 레이어를 캐스팅하였으며, lm_head 레이어는 bfloat16 정밀도로 유지되어 훈련 안정성을 보장한다.\n' +
      '\n' +
      '본 논문에서는 코사인 학습률 스케줄러와 함께 \\(\\beta_{1}=0.9\\)와 \\(\\beta_{2}=0.95\\)의 AdamW 최적화기[28]를 사용한다. 우리는 \\(\\sim\\)\\(2.36B\\) 토큰에 대한 학습률을 최대 \\(2e-4\\)으로 예열한 후 최소 \\(1e-5\\)으로 감쇠시킨다. GPU에 걸친 총 배치 크기는 \\(\\sim\\)\\(1.18M\\) 토큰이고, 중량 감쇠는 \\(1.e-1\\)이며, 기울기 클리핑 임계값은 \\(1.0\\)으로 설정된다. 이러한 설정으로 훈련 동안 단일 노드에서 \\(292.7k\\) 토큰/s의 평균 처리량을 달성했다.\n' +
      '\n' +
      '## 5 Results\n' +
      '\n' +
      '우리는 다양한 벤치마크에서 _H2O-Danube-1.8B_를 평가하고 유사한 수의 매개 변수를 가진 기존의 다른 오픈 소스 언어 모델과 비교한다. 이러한 모델에는 \\(1.1B\\) 매개변수를 갖는 TinyLlama[47], \\(1.3B\\) 매개변수를 갖는 팔콘[33], \\(1.3B\\) 및 \\(2.7B\\) 매개변수를 갖는 OPT[48], \\(1.3B\\) 및 \\(2.7B\\) 매개변수를 갖는 Cerebras-GPT[14], \\(1.4B\\) 및 \\(2.8B\\) 매개변수를 갖는 Pythia-deduped[5], \\(1.8B\\) 매개변수를 갖는 Qwen[3], 그리고 \\(1.6B\\) 매개변수를 갖는 가장 최근의 Stable LM 2[39]가 포함된다. 이러한 모델의 대부분은 아파치 2.0 라이선스를 가지고 있지만 안정적인 LM 2와 Qwen은 상업적 사용을 위한 추가 조건이 필요하며 표 1에 별표로 표시되어 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Size** & **Tokens** & **ARC-e** & **ARC-c** & **Bool** & **HS** & **OB** & **PIQA** & **Triv** & **Wino** \\\\  & & & acc\\_n & acc\\_n & acc & acc\\_n & acc\\_n & acc\\_n & em & acc \\\\ \\hline TinyLlama & 1.1B & 3.00T & 55.35 & 30.12 & 57.80 & 59.18 & 36.00 & 73.18 & 28.78 & 58.88 \\\\ \\hline Falcon & 1.3B & 0.35T & 57.32 & 32.00 & 62.84 & 61.52 & 35.20 & 74.65 & 27.27 & 60.77 \\\\ \\hline OPT & 1.3B & 0.18T & 50.93 & 29.52 & 57.71 & 53.73 & 33.40 & 72.52 & 15.67 & 59.59 \\\\  & 2.7B & 0.18T & 54.34 & 31.31 & 60.31 & 60.61 & 35.20 & 74.81 & 22.38 & 60.85 \\\\ \\hline Cerebras & 1.3B & 0.03T & 45.88 & 25.26 & 59.36 & 38.37 & 29.20 & 66.76 & 05.49 & 52.01 \\\\  & 2.7B & 0.05T & 52.53 & 27.30 & 59.20 & 48.84 & 32.00 & 70.89 & 12.41 & 55.96 \\\\ \\hline Pythia & 1.4B & 0.30T & 56.57 & 29.86 & 56.76 & 54.40 & 33.20 & 72.36 & 18.46 & 56.20 \\\\  & 2.8B & 0.30T & 58.96 & 32.68 & 64.19 & 59.45 & 35.60 & 73.78 & 24.39 & 58.17 \\\\ \\hline Qwen* & 1.8B & 2.20T & 58.25 & 34.98 & 67.13 & 58.82 & 33.40 & 72.85 & 23.92 & 58.96 \\\\ \\hline Stable LM 2* & 1.6B & 4.00T & **67.63** & **39.08** & **75.60** & **68.78** & **40.00** & 76.39 & 33.84 & **63.30** \\\\ \\hline H2O-Danube & 1.8B & 1.00T & 62.29 & 35.84 & 65.81 & 68.20 & 37.60 & **76.93** & **38.99** & 61.96 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **상식 추론, 세계 지식 및 읽기 이해 벤치마크.**_H2O-다뉴브-1.8B_는 유사한 크기의 다른 모델에 비해 모든 벤치마크에서 일관되게 좋은 결과를 나타낸다. BooIQ를 제외한 모든 벤치마크에서 Qwen보다 우수한 성능을 보이며, 크기는 동일하지만 2.2배 적은 토큰으로 훈련되었다. 안정적인 LM 2는 대부분의 벤치마크에서 _H2O-다뉴브-1.8B_보다 약간 뛰어나지만 토큰 수의 4배에 대해 훈련되었다. 또한 Qwen이나 Stable LM 2 모델 모두 상업적 사용을 위한 추가 조건이 필요한 아파치 2.0 라이선스를 가지고 있지 않다.\n' +
      '\n' +
      '모델을 평가하기 위해 언어 모델 평가 하니스 프레임워크를 사용한다[17]. 구체적으로 Open LLM Leaderboard[4]에서 사용하는 프레임워크의 버전을 사용한다. 우리는 상식 추론, 세계 지식, 읽기 이해 및 집계된 개방형 LLM 리더보드 벤치마크와 같은 다양한 벤치마크 도메인에 걸쳐 모델 기능을 보고한다.\n' +
      '\n' +
      '**상식 추론.** 표 1에서 우리는 0-샷 설정에서 6개의 표준 상식 추론 벤치마크를 제시한다: ARC 쉽고 도전[9], HellaSwag[45], OpenBookQA[29], PIQA[6], Winogrande[37].\n' +
      '\n' +
      '**World Knowledge.** 비공개 북 질문 응답 벤치마크를 나타내는 TriviaQA[25]에서 5-shot 성능을 평가한다. 결과는 표 1에 나와 있다.\n' +
      '\n' +
      '**Read Comprehension.** BoolQ[8]에 대한 0-shot 성능을 보고한다. 결과는 표 1에 나와 있다.\n' +
      '\n' +
      '**Open LLM Leaderboard.**0-shot 및 few-shot 설정에서 광범위한 분야에 걸쳐 다양한 추론 및 일반 지식을 테스트하는 6개의 핵심 벤치마크에 대한 모델을 평가한다. ARC 챌린지(25-shot)[9], HellaSwag(10-shot)[45], MMLU(5-shot)[19], TruthfulQA(0-shot)[27], Winogrande(5-shot)[37], GSM8k(5-shot)[10]으로 구성된다. 결과는 표 2에 나와 있다\n' +
      '\n' +
      '표 1의 각 모델에 대해 매개변수 수와 훈련된 토큰의 총 수를 보고한다. _ H2O-Danube-1.8B_는 유사한 크기의 다른 모델에 비해 모든 상식 추론, 세계 지식 및 읽기 이해 벤치마크에서 좋은 결과를 달성한다. 가장 가까운 경쟁자는 Owen과 Stable LM 2 모델입니다. _ BoolQ를 제외한 모든 벤치마크에서 H2O-Danube-1.8B_가 Owen보다 우수한 성능을 보인다. 오웬 모델은 동일한 1.8B 파라미터를 갖지만 2.2배 더 많은 토큰 - 2.2T로 훈련되었다. 동시에, _H2O-다뉴브-1.8B_는 대부분의 벤치마크에서 Stable LM 2보다 약간 더 나쁘지만, Stable LM 2는 2 에폭 동안 4배 더 많은 토큰 - 2T 토큰에 대해 훈련되었다. 또한 오웬과 스테이블 LM 2 모델 모두 상업적 사용을 위한 추가 조건이 필요한 아파치 2.0 라이선스를 가지고 있지 않다는 점에 유의하는 것이 중요하다.\n' +
      '\n' +
      '마찬가지로, _H2O-Danube-1.8B_, Qwen 및 Stable LM 2는 Open LLM Leaderboard에서 가장 강력한 모델이다(표 2 참조). MMLU와 GSM8k를 제외한 대부분의 벤치마크에서 모두 비슷한 결과를 보인다. 이러한 행동에 대한 잠재적인 설명 중 하나는 Qwen 및 Stable LM 2 모델의 훈련에 사용된 특별히 맞춤화된 데이터일 수 있다. 이러한 데이터는 일부 특정 벤치마크를 개선하는데 활용될 수 있는데, 예를 들어 Qwen은 더 나은 수학 추론을 위해 gsm8k-ScRel 데이터세트[44]를 사용하였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Size** & **ARC** & **HS** & **MMLU** & **TQA** & **Wino** & **GSM** & **Average** & **Average** \\\\  & & 25-shot & 10-shot & 5-shot & 0-shot & 5-shot & 5-shot & & no GSM \\\\ \\hline TinyLlama & 1.1B & 33.87 & 60.31 & 26.04 & 37.32 & 59.51 & 01.44 & 36.42 & 43.41 \\\\ \\hline Falcon & 1.3B & 35.07 & 63.56 & 25.28 & 35.96 & 62.04 & 00.53 & 37.07 & 44.38 \\\\ \\hline \\multirow{2}{*}{OPT} & 1.3B & 29.52 & 54.53 & 24.96 & 38.71 & 59.75 & 00.15 & 34.60 & 41.49 \\\\  & 2.7B & 33.96 & 61.43 & 25.43 & 37.43 & 61.96 & 00.23 & 36.74 & 44.04 \\\\ \\hline \\multirow{2}{*}{Cerebras} & 1.3B & 26.28 & 38.54 & 26.59 & 42.70 & 53.43 & 00.23 & 31.30 & 37.51 \\\\  & 2.7B & 29.10 & 49.29 & 25.17 & 41.37 & 54.14 & 00.45 & 33.25 & 39.81 \\\\ \\hline \\multirow{2}{*}{Pythia} & 1.4B & 32.68 & 54.96 & 25.56 & 38.66 & 57.30 & 00.83 & 35.00 & 41.83 \\\\  & 2.8B & 36.26 & 60.66 & 26.78 & 35.56 & 60.22 & 00.83 & 36.72 & 43.90 \\\\ \\hline Qwen* & 1.8B & 37.71 & 58.87 & **46.37** & **39.41** & 61.72 & **24.41** & 44.75 & 48.82 \\\\ \\hline Stable LM 2* & 1.6B & **43.52** & **70.45** & 39.08 & 36.81 & **65.82** & 17.36 & **45.51** & **51.14** \\\\ \\hline H2O-Danube & 1.8B & 39.68 & 69.75 & 25.97 & 33.63 & 64.17 & 02.05 & 39.21 & 46.64 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **Open LLM Leaderboard.** 표의 각 모델에 대해 GSM8k 벤치마크가 없는 개별 벤치마크, 평균점수 및 평균점수를 모두 보고한다. _ H2O-Danube-1.8B_는 GSM8k 및 MMLU를 제외한 대부분의 벤치마크에서 Qwen 및 Stable LM 2 모델과 유사한 결과를 보여준다. 그것은 모델 훈련에 사용된 데이터로 설명될 수 있으며, 예를 들어 Qwen은 더 나은 수학 추론을 위해 gsm8k-ScRel 데이터세트[44]를 사용했다.\n' +
      '\n' +
      '## 6 채팅 미세조정\n' +
      '\n' +
      'LLM의 가장 일반적인 사용 사례 중 하나는 지시 및 채팅을 중심으로 진화한다. 또한, Apache 2.0 하에서 공개된 채팅 Fine-tuned version _H2O-Danube-1.8B-Chat_를 제공하며, Fine-tuning LLM을 위해 Apache 2.0 오픈소스 프레임워크인 _H2O LLM Studio2_와 no-code GUI를 활용한다.\n' +
      '\n' +
      '각주 2: [https://github.com/h2oai/h2o-llmstudio](https://github.com/h2oai/h2o-llmstudio)\n' +
      '\n' +
      '### Supervised fine-tuning\n' +
      '\n' +
      '첫 번째 단계로 입력/출력 대화 쌍에 대해 SFT(supervised fine-tuning)를 사용하여 기본 모델을 튜닝한다. 세부적으로, 우리는 Orca[30], MetaMathQA[43], UltraChat200k[15; 21] 및 Oasst2[32]에 요약된 단계에 따라 OpenOrca[31] 샘플 총합 \\(157k\\) 데이터 세트를 결합한다.\n' +
      '\n' +
      '우리는 학습률 \\(1e-5\\), 배치 크기 \\(8\\) 및 짧은 워밍업으로 코사인 학습률 스케줄링을 사용하여 단일 에폭에 대한 모델의 모든 레이어를 훈련한다. 사전 훈련된 전체 컨텍스트 길이\\(16,384\\)을 사용하고, 프롬프트 손실을 마스킹하며, 사용자 지정 프롬프트 형식을 사용한다. 여러 실험에 걸쳐 하이퍼파라미터를 반복해서 최적화했다.\n' +
      '\n' +
      '### Dpo\n' +
      '\n' +
      '본 논문에서는 UltraFeedback Binarized [11], Orca DPO Pairs [22], Distilabel Math Preference DPO [2]. DPO 모델은 LoRA[20]와 \\(r=4\\), \\(알파=\\)16을 사용하여 한 에폭에 대해 코사인 학습 속도 감쇠를 사용하여 \\(1e-5\\), DPO 손실에 대해 \\(베타=0.2\\)의 배치 크기를 사용하여 학습된다.\n' +
      '\n' +
      '그 후, 선택된 답변이 가장 낮은 순위이고, 거부된 답변이 가장 높은 순위인 순위로부터 Oasst2 [32] 데이터 세트 구축 선호도 쌍을 사용하여 최종 DPO 미세 조정을 실행하며, 이는 단지 \\(5k\\) 샘플 주변의 영어 대화에만 국한된다. 트레이닝 런은 이전 것과 유사한 하이퍼파라미터를 사용하며, 단지 \\(3e-6\\)의 낮은 학습률이다.\n' +
      '\n' +
      '### Mt-bench 채팅 벤치마크.\n' +
      '\n' +
      '이 표는 코딩 범주를 제외한 mt-벤치에 대한 턴 1 및 턴 2 평가를 모두 보여준다. 결과는 _H2O-다뉴브-1.8B-Chat_의 우수한 성능을 강조하며, 특히 단일 턴 대화에서 여러 범주와 평균에 대해 가장 높은 Mt-벤치 점수를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multicolumn{2}{c}{TinyLlama-1.1B-Chat} & Qwen-1.8B-Chat & Stablelm-2-Zephyr-1.6B & H2O-Danube-1.8B-Chat \\\\ \\hline \\multicolumn{5}{c}{Turn 1} \\\\ \\hline Extraction & 2.50 & 4.70 & **5.20** & 3.40 \\\\ Humanities & 8.15 & 6.60 & **9.20** & 8.90 \\\\ Math & 1.20 & 2.40 & 3.50 & **3.80** \\\\ Reasoning & 3.10 & 3.50 & 3.50 & **4.30** \\\\ Roleplay & 5.60 & 6.70 & 6.80 & **7.05** \\\\ STEM & 6.60 & 6.50 & 7.35 & **8.10** \\\\ Writing & 8.65 & 9.20 & **9.35** & **9.35** \\\\ \\hline Average & 5.11 & 5.66 & **6.41** & **6.41** \\\\ \\hline \\multicolumn{5}{c}{Turn 2} \\\\ \\hline Extraction & 1.20 & 2.40 & **4.80** & 3.20 \\\\ Humanities & 8.10 & 7.30 & **9.70** & 8.90 \\\\ Math & 1.40 & **1.60** & **1.60** & 1.50 \\\\ Reasoning & 2.30 & **3.90** & 3.20 & 2.70 \\\\ Roleplay & 5.60 & 6.70 & **6.95** & 6.90 \\\\ STEM & 4.60 & 5.80 & **6.80** & 6.10 \\\\ Writing & 2.70 & 4.80 & **7.50** & 3.10 \\\\ \\hline Average & 3.70 & 4.64 & **5.79** & 4.63 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: **Mt-벤치 채팅 벤치마크.** 이 표는 코딩 카테고리를 제외한 mt-벤치에 대한 턴 1 및 턴 2 평가 모두를 나타낸다. 결과는 _H2O-다뉴브-1.8B-Chat_의 우수한 성능을 강조하며, 특히 단일 턴 대화에서 여러 범주와 평균에 대해 가장 높은 Mt-벤치 점수를 나타낸다.\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      '채팅을 평가하고 미세 조정된 LLM을 지시하는 것은 여전히 중요한 과제로 남아 있으며 대규모 인간 평가에 의해 가장 안정적으로 수행될 수 있다. 채팅 모델의 초기 평가를 제공하기 위해, 우리는 다른 카테고리들에 걸친 멀티턴 질문들의 집합인 _MT-Bench_에 의지하고 GPT4의 판단에 따른다[49]. 우리는 모든 범주를 _H2O-다뉴브-1.8B_의 범위를 벗어난 코딩과 구분한다. 각 모델은 \\(repetition\\_penalty=1.1\\) 및 \\(temperature=0.0\\)으로 실행되어 랜덤성을 줄이고 모델 간의 보다 공정한 비교가 가능하다.\n' +
      '\n' +
      '본 연구의 결과를 \\(2B\\) 이하의 인기 채팅 모델과 비교하여 표 3에서 강조하고 있다. _H2O-Danube-1.8B-Chat_는 특히 이 작업에 초점을 맞춘 자연 언어 작업에 대해 범주 전반에 걸쳐 강력한 결과를 나타내고 있음을 알 수 있다. 단일 턴 대화의 경우, _H2O-다뉴브-1.8B-Chat_는 7개 카테고리 중 5개 카테고리에 대해 가장 좋은 모델이며, Stablem 2와 평균적으로 온-파이다. 턴 2의 경우 Qwen 2에 필적하는 반면 Stablem 2는 다른 모델보다 우수함을 알 수 있다.\n' +
      '\n' +
      '중간 sft 버전3과 최종 DPO 모델 가중치4를 온라인으로 사용할 수 있도록 한다. 향후 SFT와 개선된 DPO를 작업하면서 채팅 버전에 대한 추가 개선 사항을 모색할 계획입니다. 특히, 다중 턴 대화로 선호도 데이터를 향상시키는 것을 목표로 한다. 또한 오픈 소스 커뮤니티가 다양한 사용 사례에 대해 모델을 더욱 미세 조정하기를 바랍니다.\n' +
      '\n' +
      '각주 3: [https://huggingface.co/h2oai/h2o-danube-1.8b-sft](https://huggingface.co/h2oai/h2o-danube-1.8b-sft)\n' +
      '\n' +
      '각주 4: [https://huggingface.co/h2oai/h2o-danube-1.8b-chat](https://huggingface.co/h2oai/h2o-danube-1.8b-chat)\n' +
      '\n' +
      '또한, 상식 추론, 세계 지식, 읽기 이해 및 집계된 Open LLM 리더보드 벤치마크에 대한 채팅 모델을 평가한다. 기본 모델과 유사하게, 우리는 표 4의 _H2O-다뉴브-1.8B_, TinyLlama, Qwen 및 Stable LM 2의 채팅 버전의 0샷 벤치마크 결과를 보고하고, 표 5의 Open LLM 리더보드 결과를 보여준다. 우리는 _H2O-다뉴브-1.8B-Chat_ 및 Stablem-2-Zephyr이 대부분의 벤치마크에서 서로 동등하면서 Qwen-Chat 및 TinyLlama-Chat 모델보다 더 나은 성능을 보여준다. 예외만 다시 MMLU 및 GSM8k 벤치마크입니다. 섹션 5에서 언급한 바와 같이, 더 나쁜 _H2O-다뉴브-1.8B_ 성능에 대한 잠재적인 설명 중 하나는 이러한 벤치마크를 최적화하기 위해 Qwen 및 Stable LM 2 기본 모델의 훈련에 사용된 특별히 맞춤화된 데이터일 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Size** & **ARC-e** & **ARC-c** & **Bool** & **HS** & **OB** & **PIQA** & **Triv** & **Wino** \\\\  & & acc\\_n & acc\\_n & acc & acc\\_n & acc\\_n & acc\\_n & acc\\_n & em & acc \\\\ \\hline TinyLlama-1.1B-Chat & 1.1B & 54.34 & 33.36 & 60.83 & 60.39 & 37.20 & 74.59 & 29.04 & 59.91 \\\\ \\hline Qwen-1.8B-Chat & 1.8B & 49.28 & 32.94 & 67.74 & 54.73 & 34.60 & 71.82 & 19.04 & 59.43 \\\\ \\hline Stablem-2-Zephyr-1.6B & 1.6B & 60.35 & **39.25** & **80.18** & **68.85** & **39.60** & 74.92 & 31.46 & 64.48 \\\\ \\hline H2O-Danube-1.8B-Chat & 1.8B & **67.51** & **39.25** & 77.89 & 67.60 & 39.20 & **76.71** & **36.29** & **65.35** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **상식 추론, 채팅 모델에 대한 세계 지식 및 읽기 이해 벤치마크.**_H2O-다뉴브-1.8B-Chat_는 TinyLlama-Chat 및 Qwen-Chat 모델을 능가하며 모든 0샷 벤치마크에서 Stablem-2-Zephyr과 동등하다. 섹션 5에서 언급한 바와 같이, 더 나쁜 _H2O-다뉴브-1.8B_ 성능에 대한 잠재적인 설명 중 하나는 이러한 벤치마크를 최적화하기 위해 Qwen 및 Stable LM 2 기본 모델의 훈련에 사용된 특별히 맞춤화된 데이터일 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Size** & **ARC** & **HS** & **MMLU** & **TQA** & **Wino** & **GSM** & **Average** \\\\  & & 25-shot & 10-shot & 5-shot & 0-shot & 5-shot & 5-shot & \\\\ \\hline TinyLlama-1.1B-Chat & 1.1B & 36.01 & 61.05 & 25.04 & 37.86 & 60.77 & 02.35 & 37.18 \\\\ \\hline Qwen-1.8B-Chat & 1.8B & 36.95 & 54.34 & **44.55** & 43.70 & 58.88 & 19.26 & 42.94 \\\\ \\hline Stablem-2-Zephyr-1.6B & 1.6B & **43.69** & **69.34** & 41.85 & **45.21** & 64.09 & **35.18** & **49.89** \\\\ \\hline H2O-Danube-1.8B-Chat & 1.8B & 41.47 & 68.02 & 33.49 & 40.82 & **64.40** & 15.54 & 43.96 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 채팅 모델에 대한 **Open LLM Leaderboard.**_H2O-다뉴브-1.8B-Chat_는 TinyLlama-Chat보다 더 나은 결과를 보여주며, 표 2에서 논의된 기본 모델에 대한 결과에서도 이미 임박한 GSM8k 및 MMLU를 제외한 대부분의 벤치마크에서 Qwen-Chat 및 Stablelm-2-Zephyr 모델과 유사한 결과를 보여준다.\n' +
      '\n' +
      '## 7 Conclusions\n' +
      '\n' +
      '본 논문에서는 다양한 소스의 토큰에 대해 학습한 새로운 오픈소스 사전학습 기반 모델인 _H2O-Danube-1.8B_를 소개한다. 아파치 2.0 라이선스는 상업적인 사용과 커뮤니티의 추가 미세 조정이 가능합니다. 우리는 또한 상식 추론, 세계 지식 및 읽기 이해 벤치마크에서 최첨단 결과를 보여주는 SFT + DPO 미세 조정 채팅 버전을 출시한다. 우리는 _H2O-Danube-1.8B_가 많은 벤치마크에서 비슷한 크기의 다른 모델보다 우수하다는 것을 보여준다. _ H2O-다뉴브-1.8B_는 허용 오픈 소스 기반 모델의 생태계 성장에 대한 첫 번째 기여이며 가까운 장래에 고품질 기반 모델을 계속 출판하고 미세 조정을 채팅하기 위해 노력한다. 특히, 소규모 모델은 LLM을 경제적으로 더 많은 청중에게 더욱 민주화하는 소비자 하드웨어에서 사용할 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]J. 에인슬리, J. 리-토프, M. 데종영 젬리안스키, F. 르브론, S. Sanghai(2023) Gqa: 다중 헤드 검문소에서 일반화된 다중 쿼리 변압기 모델을 훈련한다. ArXiv:2305.13245. 인용: SS1.\n' +
      '*[2]A. 비칭, C. 포에리어, N. 하빕 한남 N. 램버트 오자니 산세비에로 툰스톨, T Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) External Links: Link Cited by: SS1.\n' +
      '*[3]S. Biderman, H. Schoelkopf, Q. A. Anthony, H. Bradley, K. 오브라이언, E. 한라한, M. A. 칸, S. Purohit, U. S. Pasrashth, E. Raff, et al.(2023) Pythia: a suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397-2430. Cited by: SS1.\n' +
      '*[4]Y. 비스크 젤러스, J. 가오, Y. Choi, et al. (2020) Piqa: 자연 언어에서의 물리적 상식에 대한 추론. In Proceedings of the AAAI conference on artificial intelligence, pp. 7432-7439. Cited by: SS1.\n' +
      '*[5]T. 브라운, B. 만, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) 언어 모델은 소수의 학습자이다. neural information processing systems33, pp. 1877-1901. Cited by: SS1.\n' +
      '*[6]C. 클라크 이명 장태 Kwiatkowski 콜린스, K Toutanova (2019) Boolq: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구합니다. ArXiv:1905.10044. 인용: SS1.\n' +
      '*[7]P. 클라크, 나 카우히, 오 에치오니, T. Hot, A. Sabbharwal, C. Schoenick, O. 타피오르(2018) 질의응답을 해결했다고 생각하십니까? try arc, the ai2 reasoning challenge. ArXiv:1803.05457. 인용: SS1.\n' +
      '*[8]K. 코비, V 고사라주 바이에른 M. 천현준 카이저 플래퍼트, J 트워렉, J 힐튼, R. Nakano, et al. (2021) Training verifiers to solve math word problems. ArXiv:2110.14168. 인용: SS1.\n' +
      '*[9]G. 최림 위안남 딩기야오 주영 Ni, G. Xi, Z. 류모 Sun(2023) Ultrafeedback: 고품질 피드백으로 언어 모델을 부스팅합니다. 인용: SS1.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* [15] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. _arXiv preprint arXiv:2305.14233_, 2023.\n' +
      '* [16] Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. _arXiv preprint arXiv:2301.12726_, 2023.\n' +
      '* [17] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021.\n' +
      '* [18] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. _arXiv preprint arXiv:2006.03654_, 2020.\n' +
      '* [19] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.\n' +
      '* [20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [21] HuggingFaceH4.ultrachat_200k, 2023. Last accessed on 2024-01-15. [https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k).\n' +
      '* [22] Intel. Orca dpo pairs, 2023. Last accessed on 2024-01-15. [https://huggingface.co/datasets/Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs).\n' +
      '* [23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* [24] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral of experts, 2024.\n' +
      '* [25] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. _arXiv preprint arXiv:1705.03551_, 2017.\n' +
      '* [26] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* [27] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2022.\n' +
      '* [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* [29] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. _arXiv preprint arXiv:1809.02789_, 2018.\n' +
      '* [30] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. _arXiv preprint arXiv:2306.02707_, 2023.\n' +
      '* [31] Open-Orca. Openorca, 2023. Last accessed on 2024-01-15. [https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca).\n' +
      '* [32] OpenAssistant. oaast2, 2023. Last accessed on 2024-01-15. [https://huggingface.co/datasets/OpenAssistant/oaast2](https://huggingface.co/datasets/OpenAssistant/oaast2).\n' +
      '* [33] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. _arXiv preprint arXiv:2306.01116_, 2023.\n' +
      '\n' +
      '* [34] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.\n' +
      '* [35] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n' +
      '* [36] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _arXiv preprint arXiv:2305.18290_, 2023.\n' +
      '* [37] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106, 2021.\n' +
      '* [38] Jianlin Su, Muratdha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.\n' +
      '* [39] Stability AI Language Team. Introducing stable lm 2 1.6b. Last accessed on 2024-01-22. [https://stability.ai/news/introducing-stable-lm-2](https://stability.ai/news/introducing-stable-lm-2).\n' +
      '* [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [42] Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. _arXiv preprint arXiv:2303.10420_, 2023.\n' +
      '* [43] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. _arXiv preprint arXiv:2309.12284_, 2023.\n' +
      '* [44] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models, 2023.\n' +
      '* [45] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? _arXiv preprint arXiv:1905.07830_, 2019.\n' +
      '* [46] Biao Zhang and Rico Sennrich. Root mean square layer normalization. _Advances in Neural Information Processing Systems_, 32, 2019.\n' +
      '* [47] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. _arXiv preprint arXiv:2401.02385_, 2024.\n' +
      '* [48] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.\n' +
      '* [49] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _arXiv preprint arXiv:2306.05685_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>