<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'perform like an expert adult, as argued in (Yue et al., 2023; Hendrycks et al., 2020; Zhong et al., 2023; Zhang et al., 2023). Additionally, with benchmarks a void, the development of bilingual LMMs has no sense of direction. We fill the gap by proposing CMMMU, a new comprehensive Chinese benchmark designed to evaluate LMMs on massive multi-discipline tasks, guiding the development of bilingual LMMs towards a path toward expert-level artificial intelligence.\n' +
      '\n' +
      'CMMMU, including 12k manually collected Chinese multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering, is one of the most comprehensive benchmarks for evaluating LMMs\' complex reasoning and perception abilities.\n' +
      '\n' +
      'Each question in CMMMU is further annotated with detailed subfields and image types to investigate which types of questions are difficult for LMMs.\n' +
      '\n' +
      'We provide a comprehensive error analysis of 300 samples, which GPT-4V(vision) answers incorrectly, evenly distributed among 30 subjects, and covering most cases leading the most advanced LMMs to astray. By evaluating top-performing LMMs, _e.g._, Qwen-VL-Plus and GPT-4V, on CMMMU, we argue that there is still a long way to go towards an expert-level bilingual LMM. Even the most advanced closed-source LMMs, GPT-4V and Qwen-VL-Plus, only achieve accuracies of 42% and 36%, respectively, indicating significant room for improvement. We further reveal that the gap between LMMs released by the open-source community and the most powerful closed-source LMMs in a Chinese context is much smaller than in English, as demonstrated in MMMU. For example, the most powerful open-source LMM, _i.e._, Qwen-VL-Chat, achieves an accuracy of 28%, with a 14% gap compared to GPT-4V, while the gap in English is 21%. In light of the insights obtained while developing CMMMU and benchmarking existing open-source LMMs, we observe that only Yi-VL-6B1, Yi-VL-34B2, and Qwen-VL-Chat perform notably better compared to a random choice setting and are close to GPT-4V, while other open-source LMMs perform similarly to the random choice setting on CMMMU. Surprisingly, Yi-VL-34B even narrows the gap between open-source LMMs and GPT-4V on CMMMU to 7%.\n' +
      '\n' +
      'Footnote 1: [https://huggingface.co/01-ai/Yi-VL-6B](https://huggingface.co/01-ai/Yi-VL-6B)\n' +
      '\n' +
      'Footnote 2: [https://huggingface.co/01-ai/Yi-VL-34B](https://huggingface.co/01-ai/Yi-VL-34B)\n' +
      '\n' +
      'We believe CMMMU can benefit the ongoing LMM research and development efforts, and promote the democratization of LMMs.\n' +
      '\n' +
      'Our contributions are summarized as follows:\n' +
      '\n' +
      '* We introduce CMMMU, the first Chinese Massive Multi-discipline Multimodal Understanding benchmark.\n' +
      '* We reveal that existing LMMs, even including GPT-4V, perform poorly on complex reasoning and understanding in a Chinese context.\n' +
      '* We analyze the disparity between open-source bilingual LMMs and closed-source LMMs in a Chinese context and point out that it is notably smaller compared to an English context.\n' +
      '\n' +
      'Figure 1: Disciplines of CMMMU.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Multimodal Benchmark\n' +
      '\n' +
      'Traditionally, multimodal benchmarks are task-oriented, thus not designed to evaluate LMMs. Benchmarking these multimodal models relies on a set of tasks of aligning and leveraging the representations from different modalities, such as visual question answering (VQA) (Antol et al., 2015), image captioning (Vinyals et al., 2014), and information retrieval (Wei et al., 2023; Wu et al., 2024). The success of building such multimodal tasks and benchmarks heavily relies on large-scale annotated datasets like MSCOCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2015). Some work also evaluates the cross-modal alignment ability with VQA data derived from general knowledge bases (Marino et al., 2019; Schwenk et al., 2022).\n' +
      '\n' +
      'A recent line of research attempts to design benchmarks tailored to evaluating LMMs. For example, we can examine the models by requiring them to perceive and learn the complicated knowledge from the given data distribution, _e.g._, in the scientific domain (Lu et al., 2022; Wu et al., 2024). To construct benchmarks compatible with generative LMMs, MME (Fu et al., 2023) uses yes-no problems, and MMBench (Liu et al., 2023) is based on the multi-choice format. Some recent studies propose examining whether models can perceive and interpret information produced in more challenging scenarios like math reasoning (Lu et al., 2023), website interaction Deng et al. (2023), or comprehensive college-level knowledge reasoning (Yue et al., 2023). Though promising progress in this field of multimodal benchmarking has been made, a dominant ratio of the dataset is in English, which makes it an urgent gap to build a comprehensive and challenging benchmark in other frequently used languages like Chinese.\n' +
      '\n' +
      '### Bilingual Large Multimodal Models\n' +
      '\n' +
      'Different from the development trace of the benchmarks, many of the existing multimodal models support both English and Chinese due to the integrated bilingual large language models. Although such a statement is established in different models, the cases may vary on nuanced features. While multimodal models aim to go beyond the textual data by adapting the language models with cross-modality alignment methods, some language models pre-trained with Chinese-English bilingual corpus are selected as the component for text modeling (Hu et al., 2023; Bai et al., 2023; Ding et al., 2021; Du et al., 2022; LinkSoul-AI, 2023). Although some interesting insights are explored, only a few of the models are evaluated on Chinese multimodal tasks. For instance, it is revealed by Hu et al. (2023) that multimodal models trained only with English instruction tuning data work well in Chinese even in the zero-shot setting. Another set of models selects the language models adapted to Chinese with efficient tuning (Cui et al., 2023). Given the proper alignment architecture designs and training data selection, these models still show strong performances on bilingual multimodal tasks (Ye et al., 2023; Sun et al., 2023; Chen et al., 2023; Wang et al., 2023; Hong et al., 2023; LinkSoul-AI, 2023). Moreover, even though the closed-source GPT-4 (Achiam et al., 2023) does not provide architecture-relevant details, it is a worth mentioning baseline for being evaluated on Chinese multimodal benchmarks, given it achieves visual understanding tasks in English close to human-level.\n' +
      '\n' +
      'Regardless of the choice of language models and the training data, many multimodal models show the capability for Chinese tasks at a certain level in practical use. In this work, we aim to quantitatively measure the ability boundaries of the models with comprehensive and challenging Chinese multimodal tasks, as most of them have been only assessed with English tasks.\n' +
      '\n' +
      '## 3 The CMMMU Benchmark\n' +
      '\n' +
      'We introduce the Chinese Massive Multi-discipline Multimodal Understanding (_CMMMU_) benchmark, a manually curated benchmark covering college-level knowledge to evaluate LMMs\' expert-level multimodal understanding capability across a broad scope of tasks. CMMMU is the first multimodal question-answering benchmark in a Chinese context and one of the few existing multimodal benchmarks investigating LMMs\' complex understanding and reasoning capacities.\n' +
      '\n' +
      '### Data Curation Process\n' +
      '\n' +
      '**Data Collection:** We carefully design a three-stage data collection procedure. In **Stage 1**, annotator organizers (mainly the authors) collect sources satisfying license requirements in\n' +
      '\n' +
      'Figure 3: The proportion of 6 disciplines and 30 subjects in the CMMMU. The multimodal samples in 30 subjects uniformly cover the relevant expert-level domain knowledge.\n' +
      '\n' +
      'Figure 2: CMMMU examples sampled from each discipline. The pictures include music scores, tables, chemical structures, curves, circuit diagrams and other types of pictures, and the difficulty of the questions requires expert-level knowledge to understand and reason.\n' +
      '\n' +
      'the format of website links or book titles. The annotator organizers are well instructed to adhere to copyright and license regulations, avoiding data from sites prohibiting copying and redistribution. We collect at least 20 annotation sources, _i.e._, websites or books, for each subject in each discipline. In the **Stage 2**, annotator organizers forward the annotation sources to the crowdsourcing annotators for further annotation. All annotators are under-graduate students or have higher degrees to ensure they can verify the annotated questions and related explanations. During the annotation process, we ask the annotators to strictly follow several key principles to filter out unqualified questions with images:\n' +
      '\n' +
      '* Questions that can be answered without the images should be filtered out.\n' +
      '* Questions that use the same image should be filtered out as much as possible.\n' +
      '* Questions not requiring expert knowledge to answer should be filtered out as much as possible.\n' +
      '* The number of questions that are about the same specific knowledge point and have similar question angles should not exceed 10.\n' +
      '\n' +
      'We also ask annotators to follow the data annotation protocol in the Appendix.G of (Yue et al., 2023). In **Stage 3**, annotator organizers additionally supplement questions to subjects that lack questions, _e.g._, Arts, Diagnostics, and Economics, to balance the datasets.\n' +
      '\n' +
      '**Data Quality Control:** To further improve the data quality of CMMMU, we follow a strict data quality control protocol. **First**, each question is manually verified by at least one of the paper\'s authors. We carefully filter out questions with answers that are too hard to extract from the responses generated by LMMs. During the process, we also carefully filter out all the questions that are not up to college-level examinations. **Second**, given the concern of data contamination, we filter out all the questions that can be correctly solved by GPT-4, Qwen-7B, Deepseek-7B, and Yi-7B simultaneously without the assistance of OCR.\n' +
      '\n' +
      '### Comparison with Existing Multimodal Benchmarks\n' +
      '\n' +
      'From the input image type, the common image formats in the benchmark can be roughly divided into three simple categories, namely (1) visual input, such as VQA, GQA, and VisWiz. (2) optical characters, such as TextVQA. (3) visual input + optical characters, such as OKVQA, SEED, MMBench, MM-Vet. In addition, there are 5 types of image formats in the ScienceQA benchmark. CMMMU benchmark has 39 types, involving charts, tables, diagrams, chemical structures, photos, paintings, geometric shapes, musical scores, and medical images.\n' +
      '\n' +
      'In terms of question types, most of the common benchmarks are (1) Open QA, such as VQA, GQA, VisWiz, OKVQA. (2) Multiple choice questions, such as TextVQA, SEED, MMBench,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Dataset & Size & Images & Format & Source & Answer \\\\ \\hline VQA (Agrawal et al., 2015) & \\(>\\) 1M & V & I+T & Annotated & Open \\\\ GQA (Hutson and Manning, 2019) & \\(>\\) 1M & V & I+T & Synthesized & Open \\\\ Vi2Wiz (Gurari et al., 2018) & 32K & V & I+T & Annotated & Open \\\\ TextVQA (Ganz et al., 2023) & 45K & OC & I+T & Annotated & MC \\\\ OKVQA (Marino et al., 2019) & 14K & V+OC & I+T & Annotated & Open \\\\ SEED (Li et al., 2023) & 19K & V+OC & I+T & Annotated & MC \\\\ MMBench (Liu et al., 2023) & 3K & V+OC & I+T & Repurposed & MC \\\\ MM-Vet (Yu et al., 2023) & 0.2K & V+OC & I+T & Repurposed & MC \\\\ ScienceQA (Lu et al., 2022) & 6K & 5 Types & I+T & Textbooks & MC \\\\ MathVista (Lu et al., 2023) & 6K & V+OC & I+T & Synthesized & MC/Open \\\\ \\hline MMMU (Yue et al., 2023) & 11.5K & 30 Types & Interleaved & \\begin{tabular}{c} Textbooks \\\\ Internet \\\\ Annotated \\\\ \\end{tabular} & Open \\\\ \\hline CMMMU & 12K & 39 Types & Interleaved & \n' +
      '\\begin{tabular}{c} Textbooks \\\\ Internet \\\\ Annotated \\\\ \\end{tabular} & Open \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Comparison with Other Multimodal BenchmarksMM-Vet, ScienceQA. CMMMU not only contains open-ended questions and multiple choice questions, but also adds judgment questions to enrich the question types.\n' +
      '\n' +
      'In terms of knowledge depth, previous benchmarks typically require common sense or simple physical or temporal reasoning. In contrast, our proposed CMMMU benchmark requires thoughtful reasoning with university-level subject knowledge.\n' +
      '\n' +
      'In addition, we noticed that other benchmarks have recently been used to evaluate AGI, for example Mind2Web is used to develop and evaluate generalist agents for the web that can follow language instructions to complete complex tasks on any website. Unlike evaluating agents, our CMMMU aims to evaluate the capabilities of LMMs expert AGL. The MathVista Benchmark is designed to evaluate mathematical reasoning ability under visual background, while our CMMMU not only includes the evaluation of mathematical reasoning ability, but also includes the evaluation of expert knowledge in 30 sub-fields such as chemical structure and circuit diagram. MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. As a partner of MMMU, our CMMMU Benchmark extends the evaluation expert AGI to the bilingual domain, Contributing to our understanding of expert AGI progress\n' +
      '\n' +
      '### Statistics of CMMMU\n' +
      '\n' +
      'This paper introduces a large-scale Chinese Multidisciplinary Multimodal Understanding and Reasoning (CMMMU) benchmark, a carefully designed new benchmark to assess the expertise-level multimodal understanding capabilities of base models across a wide range of tasks in Chinese. It covers 6 disciplines, including arts, business, health and medicine, science, humanities and social sciences, technology, and engineering, spanning over 30 subjects. CMMMU consists of 12K questions, divided into few-shot development set, validation set, and test set. The few-shot development set comprises 5 questions for each topic, the validation set aids in hyperparameter selection with 900 questions, and the test set includes 11K questions.\n' +
      '\n' +
      'The pictures include 39 types such as pathological diagrams, musical scores, circuit diagrams, and chemical structure diagrams. We categorized the data as Easy (30%), Medium(58%), and Hard(12%) by logical difficulty rather than intellectual difficulty. According to the question type, there are 7738 multiple choice questions, 2998 fill-in-the-blank questions, and 1276 judgment questions. Of these examples, 11,760 are images in the question, 2169 are images in the option, and 597 are images with multiple images. Statistical analysis finds\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Statistics & Number \\\\ \\hline Total Questions & 12012 \\\\ Total Disciplines/Subjects/Subfields & 6/30/4165 \\\\ Image Types & 39 \\\\ \\hline Dev:Validation:Test & 112:900:11000 \\\\ Difficulties (Easy: Medium: Hard) & 30\\%:58\\%:12\\% \\\\ \\hline Multiple-choice Questions & 7738 (64.41\\%) \\\\ Fill in the blank Questions & 2998 (24.95\\%) \\\\ True or false Questions & 1276 (10.62\\%) \\\\ \\hline Questions with an Explanation & 247 (2.05\\%) \\\\ Image in the Question & 11760 (84.42\\%) \\\\ Image in Options & 2169 (15.57\\%) \\\\ Example with Multiple Images & 597 (4.97\\%) \\\\ \\hline Average question length & 51.12 \\\\ Average option length & 8.76 \\\\ Average explanation length & 78.29 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: statistics of CMMMU\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '* CogAgent (Hong et al., 2023) is a 180 billion-parameter Vision-Language Model designed for GUI comprehension and navigation.\n' +
      '* Qwen-VL (Bai et al., 2023b) uses Qwen-7B as the initialization of the LLM, and Openclip ViT-bigG as the initialization of the visual encoder. And connects them with a randomly initialized cross-attention layer. We choose QWen-VL-Chat and QWen-VL-plus in Qwen-VL series as our baseline models.\n' +
      '* InternVL (Chen et al., 2023) scales up the Vision Transformer (ViT) to 6B parameters and aligns it with LLM. There are multimodal models with varying sizes of language models within the InternVL series, including InternVL-Chat-vit-6B-Vicuna-7B, InternVL-Chat-vit-6B-Vicuna-13B, and InternVL-Chat-vit-6B-Llama2-13B.\n' +
      '* GPT-4V 3 is a closed-source large multimodal model from OpenAI that accepts image and text inputs and emits text outputs, demonstrating human-level performance on a variety of professional and academic benchmarks. Footnote 3: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)\n' +
      '* Yi-VL-6B and Yi-VL-3B are our multimodal models, providing image understanding capabilities to large language models. In these models, Vit is the Openclip 224, and the language model is either Yi-6B-Chat or Yi-34B-Chat.\n' +
      '\n' +
      '**Text-only LLMs.** We evaluate the performance of LLMs (_e.g._,GPT4, Owen-7B (Bai et al., 2023a), Deepseek-7B (DeepSeek-Al et al., 2024), Yi-6B5) when dealing with plain text, and Baichuan-7B on multimodal data. In addition, to verify whether external image tools can enhance the performance of LLMs on multimodal data, we deploy OCR by Mathipix 6 processing images to convert certain image information into textual forms.\n' +
      '\n' +
      'Footnote 4: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)\n' +
      '\n' +
      'Footnote 5: [https://huggingface.co/01-ai/Yi-6B-Chat](https://huggingface.co/01-ai/Yi-6B-Chat)\n' +
      '\n' +
      'Footnote 6: [https://mathpix.com/](https://mathpix.com/)\n' +
      '\n' +
      '**Evaluation.** We build a systematic and rule-based evaluation pipeline. Robust regular expressions are built to extract answers from the model responses. Specifically, for multiple-choice questions, we directly use options as keywords to extract model responses, and take the one with the highest number of options in the model response as the answer. If there is no valid answer in the model\'s response, random selection is performed for multiple-choice questions. For the judgment and open-ended question answering questions, we utilize specific rules to extract some segments where the answer may occur, and then detect whether the answer occurs in them. We add random selection and frequent selection as baselines: the former randomly selects an option, while the latter selects the most frequent option for each specific topic in the validation set based on its frequency of occurrence in that topic. Finally, we adopt micro-average accuracy as the evaluation metric.\n' +
      '\n' +
      'The prompts we use and their corresponding question types are as follows:\n' +
      '\n' +
      '**Multiple-choice questions:** (Please answer the following multiple-choice questions and select the correct options. These questions may include both single-choice and multiple-choice formats. If the provided information is not sufficient to determine a definite answer, please choose the option that is most likely correct based on the available data and your judgment.)7\n' +
      '\n' +
      'Footnote 7: The English version is not part of the input to the models.\n' +
      '\n' +
      '**True/False questions:** (Please answer the following true/false questions and determine the correctness of the statements based on the question descriptions and the provided information. If the information is incomplete or insufficient for an absolute judgment, please use your logical reasoning and available information to make the most likely judgment.)\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:9]\n' +
      '\n' +
      'of correct responses and 141 examples of incorrect responses are detailed in the Appendix, and the characteristics of each error type are described next.\n' +
      '\n' +
      '**Perceptual Errors (26%):** Perceptual errors are one of the primary reasons for the generation of erroneous examples by GPT-4V. On one hand, when the model fails to comprehend arrows and symbols in the image, misinterprets the sequence from top to bottom and left to right, it introduces deviations in the basic perception of the image, leading to incorrect responses. On the other hand, when the model encounters ambiguity in domain-specific knowledge, hidden meanings, or unclear formulas, it tends to exhibit perceptual errors specific to that domain. In such cases, GPT-4V tends to rely more on answering based on textual information (_i.e.,_ the question and options), prioritizing textual information over visual input, causing a bias in understanding multimodal data.\n' +
      '\n' +
      '**Resoning Errors (26%):** Reasoning Error is another major factor contributing to the generation of erroneous examples by GPT-4V. On the one hand, reasoning errors arise when the model receives incorrect information, often stemming from the perceptual errors mentioned earlier, such as in the illustration of Figure 5, where the model fails to perceive the hidden meaning of symbols, leading to erroneous inferences and outputs. On the other hand, even\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline  & **Validation** & **Test** & **Art \\& **B** & **Business** & **Science** & **Health \\&**Human. \\&** **Tech \\&** \\\\  & **Overall** & **Overall** & **Design** & & **Medicine** & **Social Sci.** & **Eng.** \\\\  & (900) & (11,000) & (1,091) & (1,538) & (2,494) & (1,086) & (1,038) & (2,974) \\\\ \\hline Random Choice & 21.6 & 21.6 & 32.9 & 9.1 & 18.8 & 23.8 & 23.9 \\\\ Frequent Choice & 24.1 & 26.0 & 36.2 & 11.8 & 23.9 & 30.2 & 28.5 & 27.7 \\\\ \\hline \\multicolumn{8}{c}{**Large Multimodal Models (MMs: Text + image as Input)**} \\\\ \\hline mPLUG-Owl2 & 20.8 & 22.2 & 30.4 & 13.3 & 19.6 & 25.2 & 24.7 & 23.4 \\\\ VisCPM & 25.2 & 22.7 & 37.7 & 11.3 & 19.1 & 26.1 & 24.0 & 23.7 \\\\ Chinese-LIAVA & 25.5 & 23.4 & 34.4 & 11.7 & 21.6 & 25.5 & 26.3 & 24.7 \\\\ Ennn2-Chat & 23.8 & 24.5 & 35.3 & 11.7 & 22.1 & 25.5 & 28.0 & 27.1 \\\\ CogAgent-Chat & 24.6 & 23.6 & 33.8 & 14.1 & 20.6 & 26.3 & 24.8 & 25.3 \\\\ Owen-VL-Chat & 30.7 & 31.3 & 52.6 & 18.5 & 26.9 & 33.4 & 34.1 & 31.4 \\\\ InternVL-Chat-ViT-6B-Vicuna-7B & 26.4 & 26.7 & 39.7 & 13.8 & 23.0 & 31.7 & 26.5 & 28.5 \\\\ InternalVL-Chat-ViT-6B-Vicuna-13B & 27.4 & 26.1 & 38.5 & 13.9 & 22.1 & 30.2 & 29.8 & 27.5 \\\\ Yi-VL-6B-Vicuna-13B & 35.8 & 35.0 & 58.0 & 19.9 & 32.8 & 39.3 & 40.6 & 32.1 \\\\ Yi-VL-34B & 36.2 & 36.5 & 62.9 & 19.1 & 31.5 & 42.1 & 42.5 & 34.5 \\\\ \\hline Owen-VL-Plus & 39.5 & 36.8 & 61.5 & 23.2 & 32.8 & 40.5 & 43.4 & 33.3 \\\\ GPT-4V & **42.5** & **43.7** & 61.0 & **36.3** & **40.9** & **46.8** & **44.2** & **41.5** \\\\ \\hline \\multicolumn{8}{c}{**Large Language Models (LIMs: Only Text as Input)**} \\\\ \\hline DeepSeek-7B & 22.3 & 21.9 & 41.3 & 11.2 & 18.3 & 23.5 & 24.7 & 21.3 \\\\ Batchman-7B & 26.0 & 24.3 & 42.7 & 12.6 & 19.6 & 28.0 & 27.8 & 23.9 \\\\ Queue-7B & 24.7 & 25.1 & 43.8 & 12.6 & 20.7 & 30.5 & 26.9 & 24.5 \\\\ Yi-6B & 25.6 & 24.2 & 26.3 & 15.0 & 23.4 & 29.1 & 27.0 & 24.7 \\\\ \\hline DeepSeek-7B + OCR & 25.2 & 23.2 & 41.2 & 13.2 & 19.4 & 26.1 & 26.5 & 21.8 \\\\ Batchman-7B + OCR & 25.3 & 24.7 & 40.2 & 15.2 & 21.0 & 27.9 & 30.7 & 22.8 \\\\ Queen-7B + OCR & 27.0 & 26.1 & 44.6 & 14.3 & 22.1 & 29.3 & 29.8 & 25.4 \\\\ Yi-6B + OCR & 28.4 & 26.8 & 33.4 & 16.9 & 24.8 & 32.3 & 33.2 & 25.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Overall results of open-source and closed-source models on the CMMMU validation and test set. **bold results** in LMMs indicate the best results for all models, and the **blue results** indicate the best results among the open-source models.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Models & Multiple-choice & Fill in the blank & True or false & Overall \\\\  & (7076) & (2753) & (1171) & (11000) \\\\ \\hline mPLUG-Owl2 & 22.9 & 7.0 & 53.8 & 22.2 \\\\ VisCPM & 24.5 & 5.4 & 52.8 & 22.7 \\\\ Chinese-LIaVA & 25.6 & 5.4 & 52.7 & 23.4 \\\\ Emu2 & 28.4 & 2.9 & 51.4 & 24.5 \\\\ CogAgent & 25.9 & 5.9 & 51.9 & 23.6 \\\\ InternVL-Chat-ViT-6B-Vicuna-7B & 28.5 & 7.3 & 61.6 & 26.7 \\\\ Yi-VL-6B & 40.8 & 11.7 & 54.9 & 35.0 \\\\ Yi-VL-34B & 42.5 & 10.4 & 61.6 & 36.5 \\\\ \\hline Owen-VL-Plus & 42.9 & 15.7 & 49.4 & 36.8 \\\\ GPT-4V & **46.4** & **27.4** & **66.0** & **43.7** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Result decomposition across question type.\n' +
      '\n' +
      'if the model correctly perceives the meaning conveyed by the image and text, errors in the reasoning process can occur when solving problems that require complex logical and mathematical reasoning. Typically, such errors result from the model\'s weaker logical and mathematical reasoning capabilities.\n' +
      '\n' +
      '**Lack of Knowledge (22%):** The lack of expertise is also one of the reasons why GPT-4V generates erroneous responses. The example in Figure 6 shows GPT-4V producing incorrect answers due to the lack of corresponding physics knowledge. Since CMMMU is the benchmark for evaluating expert AGI of LMMs, expert-level knowledge in different disciplines and subfields is required. So, injecting expert-level knowledge into LMMs is also one of the directions that can be worked towards AGI.\n' +
      '\n' +
      '**Rejection (12%):** The phenomenon of the model refusing to answer, resulting in incorrect responses, is also a common occurrence. Through analysis, we have identified several reasons for the model\'s refusal to answer: _(i)_ The model fails to perceive information from the image, and the textual information in the question is insufficient, causing the model to wait for more information.\n' +
      '\n' +
      '_(ii)_ Questions involving religious matters or personal real-life information lead the model to refrain from answering, adhering to human values. _(iii)_ When questions involve gender and subjective matters, the model avoids providing accurate responses.\n' +
      '\n' +
      '**Other Errors:** The remaining errors are text comprehension errors (7%), annotation errors (2%), and answer extraction errors (5%). These errors are attributed to various factors such as complex instruction following ability, complex text logic understanding, limitations in response generation, errors in data annotation, and problems encountered in answer matching extraction.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Models & Easy & Medium & Hard & Overall \\\\  & (3369) & (6328) & (1303) & (11000) \\\\ \\hline mPLUG-Owl2 & 25.5 & 20.8 & 20.7 & 22.2 \\\\ VisCPM & 26.8 & 21.1 & 20.1 & 22.7 \\\\ Chinese-LLaVA & 25.5 & 26.3 & 24.7 & 23.4 \\\\ Emu2 & 28.0 & 22.4 & 25.1 & 24.5 \\\\ CogAgent & 27.7 & 21.7 & 22.7 & 23.6 \\\\ InterVL-Chat-ViT-6B-Vicuna-7B & 30.3 & 25.6 & 22.6 & 26.7 \\\\ Yi-VL-6B & 43.3 & 31.6 & 30.3 & 35.0 \\\\ Yi-VL-34B & 45.6 & 32.6 & 31.9 & 36.5 \\\\ \\hline Qwen-VL-Plus & 46.7 & 32.9 & 29.9 & 36.8 \\\\ GPT-4V & **51.5** & **40.7** & **38.3** & **43.7** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Result decomposition across question difficulty levels. **bold results** in LMMs indicate the best results for all models, and the blue results indicate the best results among the open-source models.\n' +
      '\n' +
      'Figure 4: GPT-4V error response distribution.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'CMMMU benchmark represents a significant stride in developing Advanced General Intelligence (AGI). The CMMMMU\'s design is tailored to rigorously evaluating the latest Large Multimodal Models (LMMs), and testing elementary perceptual skills, intricate logical reasoning, and profound expertise in specific domains. We reveal the disparity between the reasoning capacity of the most advanced bilingual LMMs in a Chinese context and an English context by comparing LMMs\' performance on CMMMU and MMMU. Such an exhaustive assessment is pivotal for delineating the trajectory towards achieving AGI that parallels the proficiency of seasoned professionals in various fields.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Agrawal et al. (2015) Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, and Devi Parikh. Vqa: Visual question answering. _arXiv preprint arXiv: 1505.00468_, 2015.\n' +
      '* Antol et al. (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In _Proceedings of the IEEE international conference on computer vision_, pp. 2425-2433, 2015.\n' +
      '* Bai et al. (2023a) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023a.\n' +
      '* Bai et al. (2023b) Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. _arXiv preprint arXiv:2308.12966_, 2023b.\n' +
      '* Chen et al. (2023) Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. _arXiv preprint arXiv:2312.14238_, 2023.\n' +
      '* Cui et al. (2023) Yiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese Ilama and alpaca. _arXiv preprint arXiv:2304.08177_, 2023. URL [https://arxiv.org/abs/2304.08177](https://arxiv.org/abs/2304.08177).\n' +
      '* DeepSeek-Al et al. (2021) DeepSeek-Al, ; Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wentfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuil Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism. _arXiv preprint arXiv: 2401.02954_, 2024.\n' +
      '* Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. _arXiv preprint arXiv:2306.06070_, 2023.\n' +
      '* Deng et al. (2021)* Ding et al. (2021) Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. _Advances in Neural Information Processing Systems_, 34:19822-19835, 2021.\n' +
      '* Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GIm: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 320-335, 2022.\n' +
      '* Fu et al. (2023) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023.\n' +
      '* Ganz et al. (2023) Roy Ganz, Oren Nuriel, Aviad Aberdam, Yair Kittenplon, Shai Mazor, and Ron Litman. Towards models that can see and read. _IEEE International Conference on Computer Vision_, 2023. doi: 10.1109/ICCV51070.2023.01985.\n' +
      '* Gurari et al. (2018) Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 3608-3617, 2018.\n' +
      '* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.\n' +
      '* Hong et al. (2023) Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2023.\n' +
      '* Hu et al. (2023) Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, and Maosong Sun. Large multilingual models pivot zero-shot multimodal learning across languages. 2023.\n' +
      '* Hudson and Manning (2019) Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 6700-6709, 2019.\n' +
      '* Li et al. (2023) Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. _arXiv preprint arXiv:2307.16125_, 2023.\n' +
      '* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pp. 740-755. Springer, 2014.\n' +
      '* Liu et al. (2023) LinkSoul-AI. Chinese llava. [https://github.com/LinkSoul-AI/Chinese-LLaVA](https://github.com/LinkSoul-AI/Chinese-LLaVA), 2023.\n' +
      '* Liu et al. (2023) Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281_, 2023.\n' +
      '* Lu et al. (2022) Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. _Advances in Neural Information Processing Systems_, 35:2507-2521, 2022.\n' +
      '* Lu et al. (2023) Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. _arXiv preprint arXiv:2310.02255_, 2023.\n' +
      '* Lu et al. (2022)* Marino et al. (2019) Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* Plummer et al. (2015) Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In _Proceedings of the IEEE international conference on computer vision_, pp. 2641-2649, 2015.\n' +
      '* Schwenk et al. (2022) Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In _European Conference on Computer Vision_, pp. 146-162. Springer, 2022.\n' +
      '* Sun et al. (2023) Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multi-modal models are in-context learners. 2023.\n' +
      '* Vinyals et al. (2014) Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. corr abs/1411.4555 (2014). _arXiv preprint arXiv:1411.4555_, 2014.\n' +
      '* Wang et al. (2023) Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2023.\n' +
      '* Wei et al. (2023) Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. Uniir: Training and benchmarking universal multimodal information retrievers. _arXiv preprint arXiv:2311.17136_, 2023.\n' +
      '* Wu et al. (2024) Siwei Wu, Yizhi LI, Kang Zhu, Ge Zhang, Yiming Liang, Kaijing Ma, Chenghao Xiao, Haoran Zhang, Bohao Yang, Wenhu Chen, Wenhao Huang, Noura Al Moubayed, Jie Fu, and Chenghua Lin. SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval, January 2024. URL [https://doi.org/10.5281/zenodo.10521030](https://doi.org/10.5281/zenodo.10521030).\n' +
      '* Ye et al. (2023) Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. _arXiv preprint arXiv:2311.04257_, 2023.\n' +
      '* Yu et al. (2023) Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. _arXiv preprint arXiv: 2308.02490_, 2023.\n' +
      '* Yue et al. (2023) Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. _arXiv preprint arXiv:2311.16502_, 2023.\n' +
      '* Zhang et al. (2023) Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on gaokao benchmark. _arXiv preprint arXiv:2305.12474_, 2023.\n' +
      '* Zhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. _arXiv preprint arXiv:2304.06364_, 2023.\n' +
      '\n' +
      '## Appendix A Appendix\n' +
      '\n' +
      'The appendix is our sample analysis of GPT-4V, including an analysis of 141 error examples and 65 correct examples.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n' +
      '**Ground Truth: (B) **Front-**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '**Error Category: Reasoning Error Error Reason:** GPT-4V accurately analyzed the image: a person lying on their back for leg exercises and inferred the possible correct answers in both (A) and (D). However, it overlooked that the patient was admitted due to lumbar disc herniation, and the exercise was intended to prevent (D) nerve root adhesion after lumbar disc herniation surgery, leading to an reasoning error.\n' +
      '\n' +
      '**Ground Truth:** (D)\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:19]\n' +
      '\n' +
      '**Error Category:** **Answer Extraction Error**\n' +
      '\n' +
      '**Error Reason:** GPT-4V correctly interpreted the image and made an accurate judgment. However, in its subsequent analysis, the inclusion of keywords such as "EL" led to errors in the Extract & Match process, resulting in the extraction of answers (A) and (C).\n' +
      '\n' +
      '**Ground Truth:** **(A)****\n' +
      '\n' +
      '**Option:**\n' +
      '\n' +
      '(A) 40cm\\(\\sim\\)50cm\\(\\#\\)30cm\n' +
      '\n' +
      '(B) 50cm\\(\\sim\\)60cm\\(\\#\\)40cm\n' +
      '\n' +
      '(C) 60cm\\(\\sim\\)70cm\\(\\#\\)50cm\n' +
      '\n' +
      '(D) 70cm\\(\\sim\\)80cm\\(\\#\\)60cm\n' +
      '\n' +
      '**Error Category: Lack of Knowledge**\n' +
      '\n' +
      '**Error Reason:** GPT-4V correctly interpreted the image and identified both 1 and 2, but due to a lack of practical knowledge, it could not determine the height of 1 and 2.\n' +
      '\n' +
      '**Ground Truth: (C) 60cm\\(\\sim\\)70cm\\(\\#\\)50cm**\n' +
      '\n' +
      '**Error Category: Perceptual Errorack of Knowledge Error Reason:** GPT-4V failed to accurately understand or analyze the information in the image, such as the direction of the electric current, the winding of the coil, and the related magnetic field effects. This error occurred because GPT-4V cannot directly parse details in images, such as circuit diagrams or symbols, and therefore could not accurately determine the magnetic pole changes in the soft iron pieces A, B, and C after the electrical key is closed. In this case, GPT-4V did not provide a definitive answer but explained based on general principles of electromagnetism, indicating its inability to process specific visual information in the image.\n' +
      '\n' +
      '**Ground Truth:** (A) **H3NNX (D) **Z4V **H3NX**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:23]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:24]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:25]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:26]\n' +
      '\n' +
      '**Error Category: Answer Extraction Error Error Reason:** The answer extracted by the code is not the correct answer for GPT-4V. The question asked to identify the type of connection shown in the image, and GPT-4V incorrectly chose (D) Keyed Connection, while the actual connection shown in the image is a Keyless Connection. This indicates that GPT-4V made an error in extracting the correct answer from the text and image, selecting an inappropriate answer.\n' +
      '\n' +
      '**Ground Truth: (B) Keyed\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:28]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:29]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:30]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:31]\n' +
      '\n' +
      '**Error Category: Reasoning Error Error Reason:** GPT-4V correctly understood the question and the image, deducing that the image is a blood smear under the microscope. However, it excluded the option based on blue cells (smear cells), typically not being a component of blood. In usual circumstances, smear cells refer to damaged white blood cells during the process of making a blood smear.\n' +
      '\n' +
      '**Ground Truth: (A)T\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:33]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:34]\n' +
      '\n' +
      '**Error Category:Lack of Knowledge**\n' +
      '\n' +
      '**Error Reason:\\(\\mathrm{GPT}\\)-4V overlooked or lacked some knowledge. In fact, lymphocytes do indeed possess the characteristics mentioned by GPT-4V. However, the cell in the image not only has blue cytoplasm but also has a very irregular shape, characteristic of reactive lymphocytes. Similarly, reactive lymphocytes also possess the aforementioned characteristics.**\n' +
      '\n' +
      '**Ground Truth: (D)\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:36]\n' +
      '\n' +
      '**Error Category: Lack of Knowledge**\n' +
      '\n' +
      '**Error Reason:**The text given in the question is actually the reverse complementary sequence of the image DNF, so it\'s correct that the former is generated by the latter through DNA polymerase. GPT-4V lacks knowledge about reverse complementary sequences, leading to an incorrect judgment.\n' +
      '\n' +
      '**Ground Truth: (LE.)**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:38]\n' +
      '\n' +
      '**Error Category: Reasoning Error Error Reason:**Transcription indeed proceeds from right to left, therefore it occurs along the 3\' to 5\' direction of the template (negative) strand. The strand mentioned above (negative strand) is the transcription template. GPT-4V overextended in its deduction, leading to an incorrect conclusion.\n' +
      '\n' +
      '**Ground Truth:**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:40]\n' +
      '\n' +
      '**Error Category:Perceptual Error**\n' +
      '\n' +
      '**Error Reason:**The molecule in option C contains an amino (-NH-) group and an ether (-O-) structure. These two functional groups might be confused with components of ethyl carbamate. While these two parts contain nitrogen and oxygen atoms, respectively, they do not form an ethyl carbamate structure. Moreover, the compound in option C includes an ether oxygen (-O-) situated between a phenyl ring and an ethoxy group, which led to the incorrect selection by GPT-4\n' +
      '\n' +
      '**Ground Truth: (D):**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:42]\n' +
      '\n' +
      '**Figure 1:** The image clearly shows a turbulence nozzle, but GPT-4V might not have recognized it. Due to insufficient ability to read images or weaker recognition capabilities, it failed to correctly obtain all the true information from the image, which further led to the wrong selection.\n' +
      '\n' +
      '**Error Category:Perceptual Error Error Reason:**Obviously, GPT-4V incorrectly counted the number of hydroxyl groups in the figure. In reality, option C has the same number of hydroxyl groups as D, and there are two methoxy groups on the same ring as the hydroxyl groups, which would greatly enhance the acidity of the compound. Therefore, this is a case of misreading the diagram.\n' +
      '\n' +
      '**Ground Truth: (C) \\(\\langle\\)C\\(\\rangle\\) \\(\\langle\\)H\\(\\rangle\\) 3**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:45]\n' +
      '\n' +
      '**Error Category:Perceptual Error**\n' +
      '\n' +
      '**Error Reason:\\(\\mathrm{GPT}\\)-\\(4\\mathrm{V}\\) observed that the system contains at least two containers for storing hot water, and the process is a forced circulation indirect heating mode. However, in the picture, there is one water tank and one water jug, while GPT-4 incorrectly identified both as water jug, thus leading to an erroneous selection.**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:47]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:48]\n' +
      '\n' +
      '**Error Category:Lack of Knowledge Error Reason:**The device in the picture is a model of a coal mining machine. GPT-4V did not recognize it, indicating that it may lack this kind of knowledge. Therefore, this error is categorized as a Lack of Knowledge.\n' +
      '\n' +
      '**Ground Truth:** (A)\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:50]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:51]\n' +
      '\n' +
      '**Error Category:Reasoning Error**\n' +
      '\n' +
      '**Error Reason:**GPT-4V possesses knowledge in this area and has already answered that "heavy water reactors can use unenriched natural uranium as fuel". However, in the final answer, it responded with (A) heavy water, which is a Reasoning Error.\n' +
      '\n' +
      '**Error Category:Textual Understanding**\n' +
      '\n' +
      '**Error Reason:GPT-4V correctly understood the image and identified the key point that \'the manufacturing link is located in the middle valley position\', but it failed to correctly differentiate between \'lowest\' and\'relatively low\' in the question options. It mistook the globally lowest position for being relatively low, which led to GPT-4V answering this question incorrectly.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:54]\n' +
      '\n' +
      '**Error Category:Reasoning Error Error**\n' +
      '\n' +
      '**Error Reason:When player A chooses "Down" and player B chooses "Right," GPT-4V considers this a Nash equilibrium. However, in this situation, if player B changes their choice to "Left," they will gain a higher payoff. Therefore, the combination of "Down, Right" is not a Nash equilibrium. GPT-4V made a Reasoning Error in this case.**\n' +
      '\n' +
      '**Ground Truth: (C) \\(\\overline{\\mathbf{\\mathsf{F}}}\\), \\(\\mathbf{\\dot{\\mathbf{\\mathsf{Z}}}}\\)**\n' +
      '\n' +
      '**Error Category:Reasoning Error Error Reason:**GPT-4V has already listed the best responses for each firm when the other chooses different strategies, and knows that a Nash equilibrium occurs when no party has the incentive to unilaterally change their strategy. Up to this point, there are no issues. However, the error occurred when reasoning which situation belongs to "no party having the incentive to unilaterally change their strategy." When both firms 1 and 2 reduce prices, firm 2\'s change in strategy actually leads to a greater profit. Therefore, the Nash equilibrium does not occur in option A but rather in option C.\n' +
      '\n' +
      '**Error Category:Reject to Answer Error Reason:In economics, firms produce in the second stage, which in the diagram refers to L=5 to L=8. Therefore, the correct answer is D. The main reason GPT-4V got this wrong is due to a Lack of Knowledge.**\n' +
      '\n' +
      '**Ground Truth: (D)5<L<8\n' +
      '\n' +
      '**Error Category:Lack of Knowledge Error Reason:GPT-4V made some deductions in the early stage, but the final conclusion failed to yield the correct answer. This is due to GPT-4V\'s lack of specific problem-solving knowledge. Therefore, the reason for GPT-4V\'s error here is \'Lack of Knowledge\'.**\n' +
      '\n' +
      '**Ground Truth: (C)70**\n' +
      '\n' +
      '**Error Category: Lack of Knowledge Error Reason:** The response provided by GPT-4V is inaccurate. In the field of finance, the line formed by the relationship between the required rate of return (Ri) for asset i and its beta (\\(\\beta\\)) is typically referred to as the Security Market Line (SML), not the Capital Market Line. GPT-4V\'s error stems from a lack of knowledge in the relevant domain.\n' +
      '\n' +
      '**Ground Truth:**\n' +
      '\n' +
      '**Error Category: Reasoning Error Error Reason:**GT-4V correctly presented the variance formula for an investment portfolio during the response: \\(\\sigma_{p}^{2}=w_{A}^{2}\\cdot\\sigma_{A}^{2}+w_{B}^{2}\\cdot\\sigma_{B}^{2}+w_{C}^{ 2}\\cdot\\sigma_{C}^{2}+2\\cdot w_{A}\\cdot w_{B}\\cdot Cov(A,B)+2\\cdot w_{A}\\cdot w_{C} \\cdot Cov(A,C)+2\\cdot w_{B}\\cdot w_{C}\\cdot Cov(B,C)\\)\n' +
      '\n' +
      'Furthermore, it successfully utilized this formula to compute the portfolio\'s variance.\n' +
      '\n' +
      'However, during subsequent reasoning, GPT-4V did not accurately calculate the standard deviation. Instead, it opted for an answer closest to the variance, resulting in a reasoning error.\n' +
      '\n' +
      '**Ground Truth: (C) 6.2**\n' +
      '\n' +
      '**Error Category: Textual Understanding Error Reason:**GPT-4V incorrectly interpreted the question prompt. The prompt explicitly stated that stocks A and B are two completely negatively correlated stocks, providing information about their correlation coefficient. However, GPT-4V overlooked this crucial information, leading to other errors in subsequent inference calculations and resulting in multiple answers, including AB.\n' +
      '\n' +
      '**Ground Truth: (A) 0.0225**\n' +
      '\n' +
      '**Error Category: Reject to Answer Error Reason:**GPT-4V believes that the image does not directly display the data on "financing from the securities market," making it impossible to directly calculate the specific proportion of financing from the securities market in the non-financial corporate sector. In reality, the "net borrowing from the securities market" in the image is the key information needed for an answer. GPT-4V considers that the lack of specific data prevents it from providing a response.\n' +
      '\n' +
      '**Ground Truth: 0.48**\n' +
      '\n' +
      '**Error Category: Annotation Error Error Reason:**In this case, GPT-4V accurately grasped the meaning of the question and provided a rigorous formulation and calculation. The computed results were completely correct. However, due to an issue with annotating the standard answer, the correct response should be D 0.16, leading to an Annotation Error.\n' +
      '\n' +
      '**Ground Truth: (C) 0.15**\n' +
      '\n' +
      '**Error Category: Reasoning Error Error Reason:**In this case, GPT-4V accurately calculated the situations for projects A and C. However, when calculating project B, it made an incorrect inference. With a cash flow of 3000 in the third year, which is less than 5000, project B does not meet the requirement for recovering the initial investment. The correct answer should be A and C.\n' +
      '\n' +
      '**Ground Truth: (B) A\\(\\overline{\\mu}\\)C**\n' +
      '\n' +
      '**Error Category: Perceptual Error**\n' +
      '\n' +
      '**Error Reason:**GPT-4V provides the formula for calculating the total cost; however, it incorrectly interprets the information in the chart. Taking Product A as an example: it mistakenly considers the base period output as the unit cost and attributes the output of Product B as its own, resulting in a misalignment of calculation data and consequently leading to errors in the computation.\n' +
      '\n' +
      '**Ground Truth: (D) 2.51; 820**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:66]\n' +
      '\n' +
      '**Error Category: Lack of Knowledge Error Reason:**GPT-4V mistakenly provided an incorrect formula for calculating the Earnings Before Interest and Taxes (EBIT) to Total Assets ratio. The correct formula should be Earnings Before Interest and Taxes (EBIT) divided by the average total assets employed. Additionally, GPT-4V exhibited deviation in processing image data, inaccurately reading the values of Earnings Before Interest and Taxes (EBIT) and total assets\n' +
      '\n' +
      '**Ground Truth: 22.86**\n' +
      '\n' +
      '**Error Category: Lack of Knowledge**\n' +
      '\n' +
      '**Error Reason:GPT-4V provided an incorrect calculation method for residual income, stating that it is equal to profit minus (capital cost rate \\(\\times\\) capital investment). In the case of Center A, with a profit of 285,000, an average operating asset of 1,350,000, and a capital cost rate of 18%, the correct result should be 285,000 - (1,350,000 \\(\\times\\) 18%) = 42,000.**\n' +
      '\n' +
      '**Ground Truth: 4.27**\n' +
      '\n' +
      '**Error Category: Textual Understanding**\n' +
      '\n' +
      '**Error Reason:**GPT-4V misunderstood the meaning of the task. During the reasoning process, it overlooked the prerequisite that the processing should first occur on the first device before moving to the second device. This oversight led to errors in the reasoning process\n' +
      '\n' +
      '**Ground Truth: (D) D-A-C-B**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:70]\n' +
      '\n' +
      '**Error Category: Lack of Knowledge Error Reason:**CPT-4V lacks relevant knowledge about the \'one-to-one comparison method,\' leading to incorrect processing during comparative ranking. It mistakenly interprets higher comparisons as lower ones, resulting in erroneous inferences\n' +
      '\n' +
      '**Ground Truth: (D) XXX**\n' +
      '\n' +
      '**Error Category: Perceptual Error**\n' +
      '\n' +
      '**Error Reason:**GPT-4V misunderstood the meaning of the data in the image. The data in the second row should represent annual maintenance costs, and the data in the third row corresponds to the respective probabilities. Each column represents a different supplier, and it does not involve changes between different years. However, GPT mistakenly interpreted the data in different columns as changes over different years\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:73]\n' +
      '\n' +
      '**Ground Truth:[B] 0.994343; 8**\n' +
      '\n' +
      '**Error Category: Lack of Knowledge Error Reason:**GPT-4V provides a detailed explanation of the specific steps to establish a simple linear regression model, and there are no errors in the information. However, GPT-4V cannot directly compute or establish a simple linear regression model as it requires specialized statistical software or tools for the calculations.\n' +
      '\n' +
      '**Ground Truth: (B) 466.54~485.06**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:76]\n' +
      '\n' +
      '**Error Category: Lack of Knowledge Error Reason:GPT-4V lacks relevant knowledge in calculating the marginal cost of capital and mistakenly interprets it as "finding the cost of the last capital in each scenario, i.e., the highest percentage of capital cost." The correct approach should involve calculating the proportion of each fundraising amount relative to the total fundraising amount and using these proportions as weights to compute the marginal cost of capital**\n' +
      '\n' +
      '**Error Category: Lack of Knowledge Error Reason:**GPT-4V lacks relevant knowledge in calculating marginal costs and mistakenly interprets variable costs from the chart as marginal costs. However, the true meaning of marginal cost should be the change in total cost for a one-unit change in production. Therefore, when producing 4 units, the marginal cost would be 19-10=9, at which point the marginal cost equals the market equilibrium price.\n' +
      '\n' +
      '**Ground Truth: (B) 41 400**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:79]\n' +
      '\n' +
      '**Error Category: Reasoning Error Reason:** The GPT-4V model exhibited a reasoning error in analyzing the question. In this problem, the key is to understand how interference fringes change when a slab of glass, surrounded by air, is displaced vertically under monochromatic parallel light with perpendicular incidence. The model\'s response incorrectly suggested that as the upper glass slab is moved upwards, the thickness of the wedge near the edge increases, leading to an increase in optical path difference, causing the interference fringes to shift away from the edge. This is an incorrect inference.\n' +
      '\n' +
      '**Ground Truth:** (C) **\n' +
      '**Error Category: Reasoning Error**\n' +
      '\n' +
      '**Error Reason:** The GPT-4V model made a reasoning error when analyzing this physics problem. The problem requires calculating the resultant moment of the given force system about point I. The model incorrectly calculated the moment of force F about point I by adding the length of OA to the length of AB for the lever arm.\n' +
      '\n' +
      '**Ground Truth: (D) M1=500N cm ()**\n' +
      '\n' +
      '**Error Category: Reasoning Error Error Reason:**GPT-4V correctly pointed out that the moment diagram can be used to determine the shear force diagram and that the maximum shear typically occurs near the maximum or minimum points of the moment diagram. However, there was a deviation in the model\'s reasoning when analyzing specific changes in the moment variation rate.\n' +
      '\n' +
      '**Ground Truth: (D) 2F**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:83]\n' +
      '\n' +
      '**Error Category: Reasoning Error Reason:** GPT-4V demonstrated a certain level of domain knowledge when reasoning about possible options, but it failed to accurately apply this knowledge to specific questions. The model correctly described the general characteristics of several structural systems but couldn\'t effectively judge specific options based on image information. This disconnect between understanding and application may be due to the model lacking sufficient contextual information or because its knowledge base lacks extensive expertise in this area.\n' +
      '\n' +
      '**Ground Truth:** (B)\n' +
      '\n' +
      '**Error Category: Perceptual Error, Reject to Answer Error Reason:** First, regarding the perception error, GPT-4V actually has image recognition capabilities, but in this response, it incorrectly stated that it cannot see or understand image content. This indicates that the model did not correctly use its image processing abilities to parse the content of the image in the question, resulting in an incomplete understanding of the question. Secondly, regarding the refusal to answer, after confirming that it cannot understand the image content, the model chose the path of not providing a specific answer. While exercising caution in situations where it cannot fully comprehend a question is reasonable, in such cases, the model should have made an attempt to use the information in its knowledge base to provide an answer that closely aligns with the question\'s request, or at least provide a reasonable guess based on textual descriptions.\n' +
      '\n' +
      '**Ground Truth:** (A)\n' +
      '\n' +
      '**Error Category: Reject to Answer Error Reason:** While GPT-4V does possess the capability of image recognition, in this specific context, it did not fully leverage this ability to analyze the image content and provide an answer. The model chose to avoid directly answering the question and instead emphasized the need for more background information. This might be due to the model\'s strategy of being overly cautious when dealing with image-based multiple-choice questions, thus avoiding making direct judgments about the image content.\n' +
      '\n' +
      '**Ground Truth:** (B)\n' +
      '\n' +
      '**Error Category: Reject to Answer Error Reason:**GPT-4V chose not to answer this question. The model indicated that it cannot directly provide the specific geometric composition types of a mechanical system because this typically requires a deep understanding of the kinematics and dynamics of the mechanical system, as well as an analysis of the constraints and degrees of freedom of the structures depicted in the illustration. However, this response approach demonstrates the limitations of the model when dealing with image-based questions.\n' +
      '\n' +
      '**Ground Truth:** (D)\n' +
      '\n' +
      '**Error Category: Perceptual Error Error Reason:** In its response, GPT-4V mentioned that it couldn\'t clearly identify the number of nodes, members, and constraints due to the low image resolution. This indicates that the model attempted to answer the question based on visual information, but due to the limitations of its visual recognition capabilities, it was unable to accurately interpret the content in the image. This is a typical perception error, where the model\'s interpretation of visual information is either inaccurate or insufficient.\n' +
      '\n' +
      '**Ground Truth:-2**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:89]\n' +
      '\n' +
      '**Error Category: Lack of Knowledge Error Reason:** GPT-4V demonstrates a lack of knowledge in certain music theories. On a piano keyboard, each black key is indeed an ascending or descending tone of two white keys, but they have specific tone names. In this case, the model failed to accurately provide the correct tone name for the black key, but instead gave general descriptions, which demonstrates a lack of knowledge in music theory. The model should have enough knowledge to accurately identify and name each key on the piano keyboard.\n' +
      '\n' +
      '**Ground Truth: (A) [R]**\n' +
      '\n' +
      '**Error Category: Perceptual Error**\n' +
      '\n' +
      '**Error Reason:** GPT-4V was unable to correctly recognize and interpret the information in the images to answer the questions. This suggests a perceptual limitation in the model\'s ability to process image input. Although the model was able to recognize some basic elements in the image (e.g., actors\' costumes and make-up styles), it failed to accurately associate these elements with specific types of theatre. This may be due to the fact that the model lacks a sufficiently in-depth understanding of the visual features and stylistic details of various types of Chinese theater.\n' +
      '\n' +
      '**Error Category: Lack of Knowledge Error Reason:** In this response, the image content does provide enough information to determine the source of the melodic elements of the song Singing Faces, and the model\'s failure to recognize and utilize this information demonstrates its lack of domain-specific knowledge.\n' +
      '\n' +
      '**Ground Truth:** (A)\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:93]\n' +
      '\n' +
      '**Error Category: Perceptual Error Error**\n' +
      '\n' +
      '**Error Reason:** GPT-4 attempts to guess possible answers by characterizing the type of line drawing provided by each option. However, this method is very limited in its accuracy and validity in the absence of direct observation and analysis of the picture. Without access to the specifics of the pictures, the GPT-4 is unable to effectively analyze or provide accurate answers. In addition, GPT-4 made it clear in its response that it could not "see" the actual lines.\n' +
      '\n' +
      '**Error Category: Perceptual Error**\n' +
      '\n' +
      '**Error Reason:** GPT-4V incorrectly interpreted the angle of the calligraphy brush when processing the image. In this case, the correct answer needs to be based on the angle of the tip of the calligraphy brush relative to the surface of the paper. In the correct case, the tip of the brush should be perpendicular to the surface of the paper, i.e., a "center-front" stroke, but the model incorrectly recognizes that the tip of the brush is not perpendicular, and thus incorrectly chooses a "side-front" stroke as the answer.\n' +
      '\n' +
      '**Groupd Truth:** (B)\n' +
      '\n' +
      '**Error Category: Lack of Knowledge Error Reason:** GPT-4V is unable to recognize the content of the image directly, and is unable to determine what kind of artwork it is. Moreover, in the answer, the model mentions that "Huang Chyuan was a painter in the Southern Song Dynasty", which indicates that it relies on the historical information in its training data. However, the model\'s training data may not contain accurate information about the specific era in which Huang Chyuan\'s Rare Bird Drawings were created, or this information may not be correctly learned and recalled by the model. This causes the model to exhibit knowledge deficiencies in its responses.\n' +
      '\n' +
      '**Ground Truth: (D) \\(\\mathbf{\\mathbb{R}}\\)**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:97]\n' +
      '\n' +
      '**Error Category: Lack of Knowledge**\n' +
      '\n' +
      '**Error Reason:** GPT-4V did not accurately identify or understand these specialized painting terms. The model\'s choice of (A) Folded Band Chafing suggests that it has a limited understanding of Chinese painting terminology and techniques and is unable to accurately differentiate between different chafing techniques. Additionally, the model\'s responses appear to be based on general descriptions of painting styles rather than a precise understanding of specific terms.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:99]\n' +
      '\n' +
      '**Error Category: Reasoning Error Error Reason:** The error made by GPT-4V may stem from a misunderstanding of the function of the small square of the color bar in the Gradient Editor. In graphic editing software, the design and functionality of the gradient editor may vary from software to software. The model may infer answers based on general knowledge from its training data or the functionality of a particular software without accurately grasping the details of a particular situation in the question.\n' +
      '\n' +
      '**Ground Truth: (C) [XMM] * [DE]**\n' +
      '\n' +
      '**Error Category: Lack of Knowledge Error Reason:** The source of the error is the lack of expertise in GPT-4V, i.e., the lack of proper recognition and understanding of the specific text style in the image. Although the text in the image shows an arc-like visual effect, the correct answer should be "(D) Fan". This suggests a knowledge limitation in the model\'s ability to understand and differentiate between different text morphing styles. The model relies on the information in its training data when answering such questions. If there is insufficient information about specific text styles (e.g., fan, downward arc, etc.) in the training data, the model may not be able to accurately recognize and understand the characteristics of these styles and application scenarios.\n' +
      '\n' +
      '**Ground Truth: (D)\n' +
      '\n' +
      '**Error Category: Reject to Answer**\n' +
      '\n' +
      '**Error Reason:** In this response, the GPT-4V rejects to answer the question. This choice may be due to a misunderstanding of its image processing capabilities. In fact, GPT-4V has some image analysis capabilities, and is able to perform basic visual content analysis on supplied images, such as identifying objects, colors, layouts, etc. In this question, the user asked about the image processing capabilities of GPT-4V. In this question, the user is asking about the impact of image editing techniques ("faux stamp tools"), which is something that GPT-4V could theoretically answer by analyzing the visual characteristics of the image. In addition, the model may not have properly understood the context of the question and mistakenly assumed that it was a request involving the direct modification or generation of image content, a feature that GPT-4V does not currently have. In reality, however, the question is only asking to analyze the image content, not to modify the image.\n' +
      '\n' +
      '**Ground Truth:** (B)\n' +
      '\n' +
      '**Error Category: Perceptual Error Error Reason:** GPT-4V explicitly shows that it is unable to view images and therefore cannot directly analyze their content in order to answer questions. This demonstrates the model\'s limitations in processing image-related information, even if it possesses some degree of image processing capability. The model attempts to compensate for the lack of perception of image content by providing a general explanation of each option, but this approach cannot accurately answer questions specific to image content. While the model\'s explanations are technically correct, they are not directly applicable to a specific image, as this requires direct analysis of the image content.\n' +
      '\n' +
      '**Ground Truth:** (A)\n' +
      '\n' +
      '**Error Category: Reject to Answer Error Reason:** GPT-4V chooses to refuse to answer directly rather than attempting to reason logically or provide historical knowledge based on the options given in the question, reflecting a rejecting attitude. In this case, even though it is not possible to analyze the pictures directly, the model can still use its extensive knowledge base to provide contextual information about the options, thus assisting the user in reasoning about the possible correct answers. For example, describing the historical sites or eras mentioned in each option, or providing some of the common features and historical context of these sites.\n' +
      '\n' +
      '**Ground Truth:** (D)\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:105]\n' +
      '\n' +
      '**Error Category: Reasoning Error Error Reason:** While explaining the image, GPT-4V correctly referenced general principles from nucleation theory but made an error in applying this principle for inference. In the question, the image depicted the relationship between the nucleation radius \'r\' and the change in free energy \'\\(\\Delta\\)G,\' and it included three curves representing different temperatures. In theory, the height of the free energy change curve is inversely proportional to temperature, meaning that a lower curve represents a higher temperature.\n' +
      '\n' +
      'However, when analyzing this image, GPT-4V incorrectly assumed that the curve with the highest peak represented the lowest temperature, neglecting a crucial piece of information mentioned in the question: "G1>G2>G3, T1>T2>T3 when r is the same." This implies that, for the same nucleation radius, a larger change in free energy \'\\(\\Delta\\)_G_\' corresponds to a lower temperature.\n' +
      '\n' +
      '**Ground Truth: (C) T3**\n' +
      '\n' +
      '**Error Category: Reject to Answer Error Reason:** The GPT-4V model chose to not answer the question directly (Reject to Answer) and instead provided descriptive information about different types of square lattices. This is because the model lacks the ability to extract and understand crucial information from the image to determine which type of lattice is being displayed. The model adopted a cautious strategy of providing relevant information rather than a direct answer to avoid giving an incorrect response.\n' +
      '\n' +
      '**Ground Truth:** (C)\n' +
      '\n' +
      '**Error Category: Reasoning Error Error Reason:** The GPT-4V model mentioned "additional half-planes inserted into the crystal," which is a typical characteristic of an edge dislocation. However, based on the question description and the correct answer, the image should be depicting a screw dislocation. This indicates that the model deviated in its logical reasoning about the content of the image, possibly due to incorrectly associating the image features with an edge dislocation.\n' +
      '\n' +
      '**Ground Truth:** (B)\n' +
      '\n' +
      '**Error Category: Perceptual Error Error**\n' +
      '\n' +
      '**Error Reason:** According to the question description, the issue pertains to the interpretation of a phase diagram obtained through a thermal analysis method. However, GPT-4V\'s response demonstrates its inability to accurately identify and interpret the crucial information in the image. The model incorrectly interpreted the features in the image as possibly indicative of eutectic or eutectoid reactions, whereas the correct answer is "eutectic."\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:110]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:111]\n' +
      '\n' +
      '**Error Category: Reasoning Error Error Reason:** GPT-4V, in answering the question, only qualitatively estimated the direct labor unit cost and used this as a basis to conclude that labor costs increased. However, in reality, the increase in labor expenses for a company is mainly due to the rise in hourly wage rates, and there has been no decline in the company\'s production efficiency. Improved efficiency has actually reduced the labor cost per unit product by 2 yuan. Here, the model did not engage in quantitative reasoning, and therefore, it constitutes a Reasoning Error.\n' +
      '\n' +
      '**Ground Truth: (C) \\(\\mathcal{L}\\).**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:113]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:114]\n' +
      '\n' +
      '**Error Category: Lack of Knowledge Error Reason:** GPT-4V correctly understood the image and inferred that the answer lies within the approximate range of (A), (B), and (C) based on the image. However, due to a lack of domain-specific knowledge, it couldn\'t provide a specific answer.\n' +
      '\n' +
      '**Ground Truth: (A) #4.1**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:116]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:117]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:118]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:119]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:120]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:121]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:122]\n' +
      '\n' +
      '**Error Category: Reasoning Error Error Reason:** GPT-4V correctly understood the requirements of the question and the configuration of the circuit but made a mistake in the reasoning process. It erroneously assumed that the conducting diode was the one connected to the higher potential \\(U_{B}\\), ignoring that the direction of current through a diode is determined by the potential difference, not just the absolute value of potential. In this circuit, since both \\(U_{A}\\) and \\(U_{B}\\) are positive and diodes only conduct when forward-biased, the diode connected to the lower potential \\(U_{A}\\) will conduct, causing the diode connected to \\(U_{B}\\) to be cut-off. Therefore, the potential \\(u_{F}\\) should be equal to the potential of \\(U_{A}\\), which is \\(1\\)V. Hence, the correct answer is (A) \\(1\\)V.\n' +
      '\n' +
      '**Ground Truth: (A) 1V**\n' +
      '\n' +
      '**Error Category: Reasoning Error Error Reason:** The solution correctly identifies the need to use Kirchhoff\'s Current Law (KCL) to find the current \\(I_{1}\\) but makes a reasoning error in the application of the law to node C. The given currents entering and leaving the node were not correctly accounted for, leading to an incorrect calculation of \\(I_{1}\\) The correct approach would involve writing and solving the KCL equation considering all currents entering and leaving the node, not just the 10A and 5A currents.\n' +
      '\n' +
      '**Ground Truth: 9**\n' +
      '\n' +
      '**Error Category: Reasoning Error Error Reason:** The error occurred because GPT-4V correctly understood the components and structure of the circuit from the image but failed to apply the correct reasoning process to determine the voltage at point B. It should have used the principles of electrical circuits, such as Ohm\'s law and Kirchhoff\'s voltage law, to establish equations based on the given values of resistances and voltages. By solving these equations, the voltage at point B could have been determined. Instead, GPT-4V claimed it could not determine the voltage at point B without additional information, which was a reasoning error since the necessary information was present in the circuit diagram.\n' +
      '\n' +
      '**Ground Truth: -14.286**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:126]\n' +
      '\n' +
      '**Groupal Truth: (C) 2,4**\n' +
      '\n' +
      '**Error Category: Lack of Knowledge Error Reason:** (A) Computing fact(n) requires performing n+1 recursions, because in addition to the original call and each call for decrementing n, there is also a base case call when n reaches 0.\n' +
      '\n' +
      '**Ground Truth:** **(B) fact (7) =5040**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:129]\n' +
      '\n' +
      '**Error Category: Lack of Knowledge**\n' +
      '\n' +
      '**Error Reason: Multiple cropping: The practice of growing two or more crops on the same land in one year.**\n' +
      '\n' +
      '**In the picture, two crops are grown on dry land at the same time, so the correct answer is A.**\n' +
      '\n' +
      '**Ground Truth: (A)\n' +
      '\n' +
      '**Error Category: Perceptual Error Error Reason:** Modern agriculture pays more attention to production efficiency and technology application, aiming to improve food production and quality, while tourism agriculture focuses more on providing agricultural experience and education, adding economic value to agriculture. There are obviously many tourists in the picture, so choose D instead of C.\n' +
      '\n' +
      '**Ground Truth:** (D)\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:132]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:133]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:134]\n' +
      '\n' +
      '**Error Category: Lack of Knowledge, Reject to Answer Error Reason:** GPT-4V refuses to answer the question due to a lack of professional paleontological knowledge and understanding of specific fossil studies.\n' +
      '\n' +
      '**Ground Truth:**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:136]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:137]\n' +
      '\n' +
      '**GPT-4V:** GPT-4V has knowledge about the process of bacteriophage infection in bacteria, understanding that the sequence of infection comprises the bacteriophage attaching itself to the bacterial cell wall, injecting genetic material, synthesizing bacteriophage components within the cell, assembling mature bacteriophages, and cell lysis to release the phages. However, it fails to accurately identify images depicting each stage, leading to incorrect matches between the images and their corresponding names.\n' +
      '\n' +
      '**Ground Truth: (A) BDAEC**\n' +
      '\n' +
      '**Ground Truth: (B) 2**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:140]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:141]\n' +
      '\n' +
      '**GPT-4V:**: **B.B.**\n' +
      '\n' +
      '**Error Category:**: **GPT-4V successfully understands the images and text and correctly recalls the knowledge that a higher water level corresponds to a larger flow rate in a flood situation. However, the model directly select the answer without fixing the variables. Actually, the model should fix one variable and compare the values of the other variable.**\n' +
      '\n' +
      '**Ground Truth: C**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:143]\n' +
      '\n' +
      '**Error Category: Perceptual Error, Lack of Knowledge Error Reason:** The GPT-4V lacks specific knowledge of the concept of "absolute height", failing to comprehend that it refers to the height relative to sea level. Furthermore, GPT-4V misinterprets the dotted line in the image indicating sea level and the point labeled "\\(\\not\\subseteq\\)" and incorrectly calculates the height difference between point "\\(\\not\\subseteq\\)" and "\\(\\not\\subseteq\\)".\n' +
      '\n' +
      '**Ground Truth: 2500**\n' +
      '\n' +
      '**Error Category: Textual Understanding**\n' +
      '\n' +
      '**Error Reason:** The question inquired about the precipitation characteristics of areas M and N, however, GPT-4V\'s response pertained to the rainfall patterns of area P, which consequently resulted in an incorrect answer.\n' +
      '\n' +
      '**Ground Truth: (A)\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:146]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:147]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:148]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:149]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:150]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:151]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:152]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:153]\n' +
      '\n' +
      '**Error Category: Lack of Knowledge Error Reason:**The conclusion in the options can be deduced and demonstrated from the physical processes expressed in the image, but the model did not answer using specific knowledge.\n' +
      '\n' +
      '**Ground Truth: (D)\n' +
      '\n' +
      '**Error Category: Perceptual Error Error Reason:** The model did not fully understand the image, especially failing to recognize the numerical information in the image. Therefore, it produced an incorrect answer.\n' +
      '\n' +
      '**Ground Truth: 2.60**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:156]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:157]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:158]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:159]\n' +
      '\n' +
      '**Figure 1:** The model correctly understood the knowledge and requirements of the problem, but it made mistakes in reasoning.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:161]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:162]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:163]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:164]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:166]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:169]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:170]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:176]\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '\\begin{tabular}{|l|l|l|l|l|l|l|} \\hline \\(r\\) (h) & 1.0 & 2.0 & 3.0 & 4.0 & 5.0 & 6.0 \\\\ \\hline \\(C\\) (\\(\\mu\\)g/mL ) & 8.40 & 5.94 & 4.20 & 2.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:180]\n' +
      '\n' +
      '**Ground Truth: (\\(\\mathbf{\\mathbb{X}}\\))**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:185]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:188]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:191]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:194]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:195]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:196]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:197]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:199]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:200]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:201]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:202]\n' +
      '\n' +
      '**Ground Truth: (B)**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:204]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:205]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:206]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:207]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:209]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:211]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:213]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:214]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:216]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:218]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:219]\n' +
      '\n' +
      '**G\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:222]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:223]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:225]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:226]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:227]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:228]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:230]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:233]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:235]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:236]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:237]\n' +
      '\n' +
      'Figure 5: the case of Perceptual Error.\n' +
      '\n' +
      '## Appendix A Appendix\n' +
      '\n' +
      'Figure 6: the case of Lack of Knowledge.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>