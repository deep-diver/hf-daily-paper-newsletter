<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '기존의 방법들은 주로 NeRF(Neural Radiance Field)를 기반으로 한다[37]. 일반적으로 다층 퍼셉트론(MLP)을 사용하여 점 위치에서 방사선 밀도까지의 매핑을 학습한 다음 광선을 따라 볼륨 렌더링을 통해 투영을 생성한다. 이 광선 추적 기법은 많은 3D 포인트들을 샘플링한 후 매 광선마다 계산해야 하기 때문에 훈련 및 추론 과정이 느려지기 때문에 시간이 많이 걸린다. 최근 가장 효율적인 NeRF 기반 방법[60]조차도 여전히 훈련에 1시간 이상이 필요하고 그림 1과 같이 2fps의 느린 추론 속도로 차선책 결과를 산출한다. 이는 환자와 의사의 대기 시간을 증가시켜 진단 효율이 낮다.\n' +
      '\n' +
      '최근 3차원 가우시안 스플래팅(3DGS, 3DGaussian splatting) [26]은 RGB 영역에서 NeRF 기반 알고리즘보다 훨씬 빠른 추론 속도를 즐기면서 유망한 재구성 품질을 보여주었으며, 이는 우리가 이러한 기술적 경로를 따르도록 동기를 부여한다. 그러나 엑스레이와 자연광 이미징의 근본적인 차이로 인해 원래 3DGS를 엑스레이 NVS에 직접 적용하면 두 가지 문제가 발생할 수 있다. **첫째, RGB 3DGS에서 구면 고조파(SH)는 3D 포인트의 X선 방사 강도를 모델링하는데 적합하지 않다. 구체적으로, 자연광 이미징은 표면으로부터의 반사에 의존한다. 3D 포인트의 색상은 이방성이고 뷰에 의존한다. 이러한 성질에 기초하여, 원래의 가우시안 포인트 클라우드 모델은 조명 분포에 적합하도록 SH를 사용한다. 이에 반해, X선은 대상체를 투과하여 감쇠함으로써 화상을 형성한다. 특정 X-선이 주어지면, 3D 점의 방사선 강도는 그것의 방사선 강도에 의존하며 뷰 방향에 독립적이며, 이는 점 방사선 강도가 등방성을 의미한다는 것을 의미한다. 둘째로, 원래의 포인트 클라우드 개시 알고리즘인 SfM(structure-from-motion) [46] 또한 X-선 이미징에 적합하지 않다. RGB 이미지에 비해 X-ray 이미지는 회색조이며 대비가 낮다. 추가적으로, X-선의 투과 이미징 특성 때문에 물체의 상이한 층들이 투영의 동일한 위치 상에서 중첩될 수 있다. 이 두 가지 문제는 SfM에서 특징 검출 및 매칭의 정확도를 저하시킨다. 한편, SfM을 실행하는 것은 시간이 많이 소요되어 가우시안 포인트 클라우드의 학습 과정이 길어진다.\n' +
      '\n' +
      '그림 2: 원래 3DGS[26](위)와 우리의 X-가우시안(아래)의 포인트 클라우드 시각화. 우리는 서로 다른 훈련 반복에서 가우시안 점 구름의 위치와 불투명도를 시각화한다. 또한 발의 볼륨을 기준으로 시각화합니다. 볼륨이 점 구름의 지상 진실이 아니라는 점에 유의하십시오. 우리의 X-Gaussian은 3DGS보다 상세한 구조를 더 잘 나타낼 수 있으며, 더 빠르고 더 나은 수렴을 보여준다.\n' +
      '\n' +
      ' 이러한 문제점을 해결하기 위해 본 논문에서는 X-ray NVS를 위한 새로운 3DGS 기반 방법인 X-Gaussian 방법을 제안한다. 우리의 X-Gaussian은 두 가지 핵심 기술을 구성한다. 먼저, 우리는 X-선 영상의 등방성 특성에 영감을 받아 복사 가우시안 포인트 클라우드 모델을 재설계한다. 본 논문에서는 원래 3DGS의 SH 함수를 대체할 방사선 세기 반응 함수(RIRF)를 제시한다. SH와 달리, 우리의 RIRF는 뷰 방향의 영향을 배제한다. 이를 위해, 3차원 점의 방사 세기에 적합하도록 고유 점 특징을 나타내는 학습 가능한 벡터와 기저 가중치 집합 사이의 내적을 채택한다. 이 점 구름 모델을 기반으로 새로운 투영을 렌더링하기 위한 CUDA 구현과 함께 미분 복사 래스터화(DRR)를 추가로 개발한다. 둘째, 카메라 캘리브레이션 파라미터와 가우시안 포인트 클라우드를 위한 ACUI(Angle-pose Cuboid Uniform Initialization) 전략을 사용자화한다. 제안된 ACUI는 먼저 X-선 스캐너의 파라미터를 이용하여 내부 행렬과 외부 행렬을 계산한다. 그런 다음 스캔된 객체를 완전히 둘러쌀 수 있는 직육면체를 설정합니다. 이 큐보이드 내에서 우리는 가우시안 점 구름의 중심 위치를 초기화하기 위해 간격을 두고 3D 점을 균일하게 샘플링한다. SfM 알고리즘을 실행하지 않아도 ACUI는 훈련 시간을 크게 줄입니다. 제안된 두 가지 기술을 갖춘 X-Gaussian은 그림과 같이 최첨단(SOTA) 알고리즘보다 더 빠른 수렴, 더 나은 성능 및 더 짧은 실행 시간을 즐긴다. 1과 2. 놀랍게도, X-Gaussian은 **73\\(\\times\\)** 추론 속도와 **7\\(\\times\\)** 훈련 속도를 즐기면서 **6.5 dB**의 SOTA 방법을 능가한다.\n' +
      '\n' +
      '이 작업의 주요 기여는 다음과 같이 요약할 수 있다.\n' +
      '\n' +
      '* 우리는 X-선 신규 뷰 합성을 위한 새로운 3D 가우시안 스플래팅 기반 프레임워크인 X-가우시안(X-Gaussian)을 제안한다. 우리가 아는 한, 이것은 X-선 신경 렌더링에서 가우시안 스플래팅의 가능성을 탐구하려는 첫 번째 시도이다.\n' +
      '* X-선 영상의 등방성 특성을 기반으로 미분 가능한 복사 래스터화를 갖는 복사 가우시안 점군 모델을 설계한다.\n' +
      '* 우리는 원형 원뿔 빔 X-선 스캐닝에서 가우시안 포인트 클라우드와 카메라 보정을 위한 각도-포즈 큐보이드 균일 초기화 전략을 제시한다.\n' +
      '* 우리의 X-가우시안 방법은 훨씬 빠른 속도로 SOTA NeRF 기반 방법보다 상당히 우수하다. 또한 실험을 통해 제안된 방법이 희소 시점 CT 재구성의 성능을 향상시킬 수 있음을 보여 실제적 가치를 보여준다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '신경 복사 필드\n' +
      '\n' +
      'NeRF[37]은 3D 점 및 뷰 방향의 위치를 고려할 때 색상 및 볼륨 밀도의 암시적 신경 장면 표현을 학습한다. 그것은 NVS에서 큰 성공을 거두었고 품질[3, 48, 5, 23, 4]과 속도[10, 12, 22, 31, 38, 41, 54]를 향상시키기 위해 후속 논문의 폭발에 영감을 주었다. 예를 들어, Instant-NGP[38]은 빠른 트레이닝 및 추론을 위해 작은 MLP를 허용하기 위해 인코더로서 해시 테이블들을 채택한다. 이후 일부 작업은 NeRF의 적용 영역을 자연광에서 X선[15, 58, 60, 9]으로 확장한다. 예를 들어 NAF[60]은 Instant-NGP의 설정을 따라 3D 위치에서 감쇠까지의 암시적 매핑을 학습한다. 그러나, 레이 트레이싱 및 볼륨 렌더링 기법은 시간이 많이 소요되어 NeRF 기반 X-ray NVS 알고리즘의 학습 및 추론 속도를 제한한다.\n' +
      '\n' +
      '### Gaussian Splatting\n' +
      '\n' +
      '3DGS[26]은 수백만 개의 3D 가우시안 포인트 클라우드를 사용하는 장면들을 나타낸다. 이 접근법은 고도로 병렬화된 래스터화 워크플로우와 결합된 명시적 표현을 사용함으로써 NeRF 기반 알고리즘과 근본적으로 다르다. 이러한 기능은 보다 효율적인 계산 및 렌더링 프로세스를 가능하게 합니다. 따라서 3DGS는 3D Generation[21, 29, 32, 47, 55], Dynamic Scene Modeling[34, 50, 53], SLAM[25, 36, 52, 57], Inverse Rendering[24, 33, 51], _etc._ 그러나, 3DGS의 대부분의 응용은 RGB 색상을 갖는 자연스러운 장면에 초점을 맞추고 있다. X선 이미징에서 3DGS의 잠재력은 여전히 조사되지 않은 상태로 남아 있다. 우리의 목표는 이 연구 공백을 채우는 것입니다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      'X-가우시안 파이프라인은 그림 3에 나와 있으며, 먼저 X-선 스캐너의 매개변수로부터 고유 행렬과 외부 행렬을 계산하기 위해 ACUI(Angle-pose Cuboid Uniform Initialization)를 설계한다. 3(a). 그런 다음 ACUI는 그림 1에서 복사 가우스 점 구름의 중심 위치를 초기화하기 위해 스캔된 물체를 완전히 둘러쌀 수 있는 직육면체 내의 3D 점을 균일하게 샘플링한다. 3(b). 뷰 방향이 주어지면 3D 포인트 클라우드는 그림과 같이 렌더링된 이미지를 도출하기 위해 미분 복사 래스터화(DRR)를 거친다. 3(c). 본 절에서는 먼저 복사 가우시안 포인트 클라우드 모델과 DRR 처리를 소개하고 ACUI 전략을 소개한다.\n' +
      '\n' +
      '### 방사형 가우시안 포인트 클라우드 모델\n' +
      '\n' +
      '물체는 다음과 같이 기본 가우시안 점 구름 집합 \\(\\mathcal{G}\\)으로 표현될 수 있다.\n' +
      '\n' +
      '\\[\\mathcal{G}=\\{G_{i}(\\mathbf{\\mu}_{i},\\mathbf{\\Sigma}_{i},\\alpha_{i})\\|\\i=1,2,\\ldots,N_{p}\\, \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(G_{i}\\)는 \\(i\\)번째 가우시안 포인트 클라우드를 의미한다. 중심 위치, 공분산 및 불투명도는 \\(\\mathbf{\\mu}_{i}\\in\\mathbb{R}^{3}\\), \\(\\mathbf{\\Sigma}_{i}\\in\\mathbb{R}^{3\\times 3}\\, \\(\\alpha_{i}\\in\\mathbb{R}\\)으로 정의된다. (\\mathbf{\\Sigma}_{i}\\)를 나타낸다.\n' +
      '\n' +
      '그림 3: 우리 방법의 파이프라인. (a) Angle-pose Cuboid Uniform Initialization (ACUI) 전략은 X-ray scanner의 parameter를 이용하여 내적 행렬과 외적 행렬을 계산하고, 3D Gaussians의 중심점을 샘플링한다. (b) 우리의 복사 가우시안 포인트 클라우드 모델은 3D 포인트들의 복사 강도를 예측하도록 학습한다. (c) 우리의 가우시안 모델을 기반으로 GPU-friendly Differentiable Radiative Rasterization (DRR)을 개발한다.\n' +
      '\n' +
      '로테이션 행렬 \\(\\mathbf{R}_{i}\\in\\mathbbb{R}^{3}\\)과 스케일링 행렬 \\(\\mathbf{S}_{i}\\in\\mathbbb{R}^{3}\\(\\mathbf{\\Sigma}_{i}=\\mathbff{R}_{i}\\mathbff{S}_{i}^{\\top}\\mathbf{R}_{i}^{\\top}\\)을 이용하여, (\\boldsymbol{\\mu}_{i}\\), \\(\\mathbf{\\Sigma}_{i}\\), \\(\\alpha_{i}\\), \\(\\mathbf{R}_{i}\\) 및 \\(\\mathbf{S}_{i}\\)은 학습 가능한 매개변수이다. 이러한 기본 속성 외에도 각 가우시안 포인트 클라우드는 다른 이미징 시나리오, _예: 자연광 이미징 및 X선 이미징에 맞게 추가 학습 가능한 매개변수를 사용한다.\n' +
      '\n' +
      '우리는 먼저 자연광 이미징에서 원래의 RGB 가우시안 포인트 클라우드 모델[26]을 검토한다. 도 1에 도시된 바와 같다. 도 4의 (a)에서, 3D 포인트의 색상은 구형 고조파(SH)로 표현된다. 점 색상은 이방성이며 뷰 방향에 따라 변경됩니다. 각 가우시안 포인트 클라우드는 SH 계수(\\(\\mathbf{k}=\\{k_{l}^{m}|0\\leq l\\leq L, -l\\leq m\\leq l\\in\\mathbb{R}^{(L+1)^{2}}\\times 3}\\)를 예측하도록 학습하며, 여기서 각 \\(k_{l}^{m}\\in\\mathbb{R}^{3}\\)는 RGB 성분에 해당하는 3개의 계수 집합이다. (L\\)은 SH의 정도이다. 그리고 시점방향에서의 점색 \\(\\mathbf{c}\\in\\mathbb{R}^{3}\\)을 \\(\\mathbf{d}=(\\theta,\\phi)\\)으로 유도한다.\n' +
      '\n' +
      '\\[\\mathbf{c}(\\mathbf{d},\\mathbf{k})=\\sum_{l=0}^{L}\\sum_{m=-l}^{l}k_{l}^{m}\\ Y_{l}^{m}(\\theta,\\phi), \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(Y_{l}^{m}:\\mathbb{S}^{2}\\rightarrow\\mathbb{R}\\)는 구면의 점을 실수에 매핑하는 SH 함수이다. 자세한 제형은 보충서를 참고하시기 바랍니다.\n' +
      '\n' +
      '3DGS [26]은 자연광 이미징에서 빠른 추론 속도와 좋은 성능을 달성하지만, RGB 가우시안 포인트 클라우드 모델은 자연광 이미징과 X선 이미징의 근본적인 차이로 인해 X선 시나리오에 적합하지 않다. 자연광 이미징은 물체의 표면에서 반사되는 것에 의존한다. SH에 의해 모델링된 이방성 색상은 뷰-의존적이다. _ 예를 들어, 도 1에 도시된 바와 같다. 도 4의 (a)를 참조하면, 포인트 컬러는 좌측 시점에서는 청색이고 우측 시점에서는 녹색이다. 대조적으로, X-선 이미징은 대상체를 관통할 때의 감쇠에 기초한다. 감쇠 정도는 등방성 방사 강도 특성에 따라 달라진다. 따라서, 3D 포인트의 방사선 강도는 뷰-독립적이다.\n' +
      '\n' +
      '위의 분석에 비추어, 우리는 복사 가우시안 포인트 클라우드 모델을 재설계한다. 각 지점에 대한 색상 정보를 맞추기 위해 SH를 사용하는 원래의 3DGS와 달리, 본 모델은 3D 지점의 방사선 강도를 예측하기 위해 방사선 강도 반응 함수(RIRF)를 도입한다. 도 1에 도시된 바와 같다. 도 4의 (b)를 참조하면, 각 가우시안 포인트 클라우드는 특징 벡터 \\(\\mathbf{f}\\in\\mathbb{R}^{N_{f}}\\)을 학습하여 고유의 복사 특성을 나타낸다. 이어서, 복사 세기\\(\\mathbf{i}\\in\\mathbb{R}\\)의 복사 세기\\(\\mathbf{i}\\in\\mathbb{R}\\)\n' +
      '\n' +
      '그림 4: 원본 3DGS의 가우시안 점군 모델과 우리의 X-가우시안 모델 간의 비교. (a) 원래의 RGB 가우시안 점군 모델은 구면 고조파(SH)를 사용하여 비등방성 자연광 분포와 시점 의존 색상을 시뮬레이션한다. (b) 우리의 복사 가우시안 점군 모델은 등방성 X-선 투과도와 시점 독립 방사선 강도에 적합하도록 점 특징의 가중 합을 사용한다.\n' +
      '\n' +
      '임의의 뷰 방향에서의 3D 가우시안 중심점은 RIRF에 의해 모델링된다.\n' +
      '\n' +
      '\\[\\mathbf{i}(\\mathbf{f})=\\text{RIRF}(\\mathbf{f})=\\text{Sigmoid}(\\mathbf{\\lambda}\\odot\\mathbf{f}), \\tag{3}\\\n' +
      '\n' +
      '여기서 Sigmoid 함수는 방사 강도를 활성화하고 정규화한다. \\ (\\mathbf{\\lambda\\in\\mathbb{R}^{N_{f}\\)는 \\(\\mathbf{f}\\)의 각 성분의 중요도를 조절하는 상수 가중치 집합이다. 그런 다음 복사 가우시안 점 구름의 집합을 다음과 같이 공식화한다.\n' +
      '\n' +
      '\\[\\mathcal{G}_{x}=\\{G_{i}(\\mathbf{\\mu}_{i},\\mathbf{\\Sigma}_{i},\\alpha_{i},\\mathbf{f}_{i}}\\mid i=1,2,\\ldots,N_{p}, \\tag{4}\\)\n' +
      '\n' +
      '여기서 \\(\\mathbf{f}_{i}\\in\\mathbb{R}^{N_{f}}\\)는 \\(i\\)번째 가우시안 포인트 클라우드의 특징 벡터를 나타낸다. 참고로 Eq. (3) X-ray 영상의 등방성 특성과 일치하는 view 방향 \\(\\mathbf{d}=(\\theta,\\phi)\\)의 영향을 배제한다. 한편, Eq. (3)은 SH 함수의 복잡한 계산으로부터 자유롭다. 따라서, X-Gaussian의 순방향 및 역방향 과정은 원래의 3DGS보다 훨씬 빠르다.\n' +
      '\n' +
      '### 미분 복사 래스터화\n' +
      '\n' +
      '복사 가우스 포인트 클라우드를 기반으로 그림과 같이 미분 가능한 복사 래스터화(DRR)를 개발한다. 3(c). 먼저 전체 DRR 처리(F_{\\text{DRR}}\\)를 요약하고 그 세부 사항을 설명한다. DRR은 다음과 같이 표현된다.\n' +
      '\n' +
      '[\\mathbf{I}=F_{\\text{DRR}(\\mathbf{M}_{ext},\\mathbf{M}_{int},\\{G_{i}(\\mathbf{\\mu}_{i},\\mathbf{\\Sigma}_{i},\\alpha_{i},\\mathbf{f}_{i},\\mathbf{f}_{i})\\mid i=1,2,\\ldots,N_{p}\\}), \\tag{5}\\}\n' +
      '\n' +
      '여기서 \\(\\mathbf{I}\\in\\mathbb{R}^{H\\times W}\\)은 렌더링된 이미지를 나타내고, \\(\\mathbf{M}_{ext}\\in\\mathbb{R}^{4\\times 4}\\)은 외재적 행렬을 나타내며, \\(\\mathbf{M}_{int}\\in\\mathbb{R}^{4\\times 3}\\)은 내재적 행렬을 나타낸다. 이어서, \\(F_{\\text{DRR}}\\)의 세부사항을 소개한다. 우선, 3차원 점 위치에서의 \\(i\\)번째 가우시안 분포의 가능성 값 \\(\\mathbf{x}\\in\\mathbb{R}^{3}\\)은 다음과 같이 공식화된다.\n' +
      '\n' +
      '[P(\\mathbf{x}|\\mathbf{\\mu}_{i},\\mathbf{\\Sigma}_{i})=\\exp\\bigl{(}-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_{i})^{\\top}\\mathbf{\\Sigma}_{i}^{-1}(\\mathbf{x}-\\mathbf{\\mu}_{i})\\bigr{}. \\tag{6}\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_{i})^{\\top}\\mathbf{\\Sigma}_{i}^{-1}(\\mathbf{x}-\\mathbf{\\mu}_{i})\\bigr{}}.\n' +
      '\n' +
      '그런 다음 3D 가우시안들을 2D 검출기 평면에 투영하여 렌더링한다. \\ (\\mathbf{\\mu}_{i}\\)는 먼저 세계 좌표계에서 카메라 좌표계로 전달된 후 이미지 좌표계로 투영된다.\n' +
      '\n' +
      'bmatrix}\\mathbf{M}_{ext}\\widetilde{\\mu}}_{i}=\\mathbf{M}_{ext}\\\\left[\\begin{matrix}\\mathbf{t}_{i}\\\\end{bmatrix}, \\tag{7}\\mathbf{t}\\widetilde{i}=\\mathbf{M}_{int}\\widetilde{\\mu}\\begin{bmatrix}\\mathbf{m}_{i}=\\mathbf{m}_{int}\\widetilde{\\mu}\\begin{bmatrix}\\widetilde{\\mu}\\begin{bmatrix}\\mathbf{i}=\\mathbf{m}_{i}\\widetilde{\\mu}\\begin{bmatrix}\\widetilde{\\mu}\\begin{bmatrix}\\mathbf{m}\\widetilde{\\mu}\\\n' +
      '\n' +
      '여기서 \\(\\mathbf{t}_{i}=(t_{x},t_{y},t_{z})\\in\\mathbb{R}^{3}\\)의 카메라 좌표는 \\(\\mathbf{\\mu}_{i}\\)이고 \\(\\mathbf{u}_{i}\\in\\mathbb{R}^{2}\\)의 영상 좌표는 \\(\\mathbf{\\mu}_{i}\\)이다. \\(\\mathbf{\\mu}},t_{y},t_{z})\\in\\mathbb{R}^{3}\\). (\\widetilde{\\mathbf{u}}_{i}\\), \\(\\widetilde{\\mathbf{t}_{i}\\) 및 \\(\\widetilde{\\mathbf{\\mu}}_{i}\\)는 각각 \\(\\mathbf{u}_{i}\\), \\(\\mathbf{t}_{i}\\) 및 \\(\\mathbf{\\mu}}\\)의 균질 좌표이다. 이어서, 카메라 좌표계에서 3차원 공분산 행렬\\(\\mathbf{\\Sigma}_{i}\\)을 대응 연산자\\(\\mathbf{\\Sigma}_{i}^{{}^{\\prime}\\in\\mathbbb{R}^{3\\times 3}\\)로 변환한다.\n' +
      '\n' +
      '\\mathbf{J}_{i}\\mathbf{W}_{i}\\mathbf{W}_{i}^{\\top}\\mathbf{J}_{i}^{\\top}, \\tag{8}\\mathbf{W}_{i}\\mathbf{W}_{i}\\mathbf{J}_{i}^{\\top}\n' +
      '\n' +
      '여기서 \\(\\mathbf{J}_{i}\\in\\mathbb{R}^{3\\times 3}\\)는 사영 변환의 아핀 근사치의 자코비안이다. \\\\ (\\mathbf{W}_{i}\\in\\mathbb{R}^{3\\times 3}\\)는 시청 변환이다. 우리는 그것을 도출한다.\n' +
      '\n' +
      '\\begin{bmatrix}\\frac{L_{i}}{t_{z}}&-\\frac{L_{z}}\\\\0&\\frac{L_{z}}&-\\frac{L_{z}}{t_{z}}{t_{z}}\\\\0&0&0\\cos\\phi&0\\end{bmatrix},\\tag{9}\\mathbf{W}_{i}=\\begin{bmatrix}-\\sin\\phi&0\\cos\\phi&-1\\cos\\phi&-\n' +
      '\n' +
      '여기서 \\(L_{SD}\\)는 X-선 소스와 검출기 사이의 거리를 나타낸다. \\ (\\phi\\)는 소스의 방위각을 의미한다. [26, 30, 61]에 이어서, \\(\\mathbf{\\Sigma}_{i}^{}^{prime}\\in\\mathbbb{R}^{2\\times 2}\\)의 세 번째 행과 열을 건너뛰어 2D 공분산 행렬 \\(\\mathbf{\\Sigma}_{i}^{prime}\\)을 얻는다. 그런 다음 2D 투영은 겹치지 않는 타일로 분할된다. 3D 가우시안(\\(\\mathbf{\\mu}_{i}\\),\\(\\mathbf{\\Sigma}_{i}\\))은 그림의 왼쪽 이미지와 같이 2D 투영(\\(\\mathbf{u}_{i}\\),\\(\\mathbf{\\Sigma}_{i}^{prime\\prime}\\))에 따라 다른 타일에 할당된다. 3(c). 이 3D 가우시안들은 2D 검출기까지의 거리에 의해 분류된다. 그런 다음 픽셀\\(p\\)에서 강도\\(\\mathbf{I}(p)\\in\\mathbb{R}\\)는 해당 타일에서 픽셀과 겹치는 \\(\\mathcal{N}\\) 순서점을 다음과 같이 블렌딩하여 얻는다.\n' +
      '\n' +
      '\\mathbf{I}(p)=\\sum_{j\\in\\mathcal{N}\\mathbf{i}_{j}\\sigma_{j}\\prod_{k=1}^{j-1}(1-\\sigma_{k}),\\\\\\sigma_{j}=\\alpha_{j}P(\\mathbf{x}_{j}|\\mathbf{\\mu}_{j},\\mathbf{\\Sigma}_{j}), \\tag{10}\\tag{\n' +
      '\n' +
      '여기서 \\(\\mathbf{x}_{j}\\)는 3D 공간에서 픽셀 \\(p\\)에 X-선 착지의 \\(j\\)번째 교차점 3D 점과 가우시안 점 구름이다. \\(p\\) (\\mathbf{i}_{j}\\)는 \\(\\mathbf{x}_{j}\\)의 복사 강도이다.\n' +
      '\n' +
      '결국, 훈련 목적 \\(\\mathcal{L}\\)은 렌더링된 영상과 지상진실 투영 영상 사이의 \\(\\mathcal{L}_{1}\\) 손실과 SSIM 손실의 가중합이다.\n' +
      '\n' +
      '\\[\\mathcal{L}=(1-\\gamma)\\mathcal{L}_{1}+\\gamma\\mathcal{L}_{\\text{SSIM}}, \\tag{11}\\]\n' +
      '\n' +
      '여기서 \\(\\gamma\\)는 두 손실항의 중요도와 균형을 이루는 하이퍼파라미터이다. Eq를 최소화함으로써. (11) 식에서 3D 가우시안, _i.e., \\(\\mathbf{\\mu}_{i},\\mathbf{\\Sigma}_{i},\\alpha_{i},\\text{and}\\\\mathbf{f}_{i}\\의 속성을 최적화할 수 있다. (5) \\ (N_{p}\\)는 적응 제어에 의해 조정된다[26]. 최적화 과정은 그림 1에 나와 있다. 2와 비디오 파일을 보충한다.\n' +
      '\n' +
      '3DGS [26]의 RGB 래스터화와 비교하여, DRR은 순방향 및 역방향 프로세스에서 뷰 방향과 관련된 복잡한 계산을 회피함으로써, 더 저렴한 트레이닝 비용과 더 빠른 추론 속도를 향유한다.\n' +
      '\n' +
      '### 각도-포즈 큐보이드 균일 초기화\n' +
      '\n' +
      '훈련 초기에 우리는 식에서 파라미터를 초기화할 필요가 있다. (5) 래스터화. 구체적으로 \\(\\mathbf{\\Sigma}_{i}\\), \\(\\alpha_{i}\\), \\(\\mathbf{f}_{i}\\)을 랜덤하게 초기화한다. 자연광 영상에서 원래의 3DGS[26]는 초기 \\(\\mathbf{\\mu}_{i}\\), \\(N_{p}\\), \\(\\mathbf{M}_{ext}\\) 및 \\(\\mathbf{M}_{int}\\)을 계산하기 위해 SfM[46] 알고리즘을 채택한다. SfM은 멀티뷰 이미지에서 피쳐를 감지하고 일치시킵니다. 두 가지 이유로 X선 촬영에는 적합하지 않습니다. 첫째, X-ray 영상은 회색조 및 저대비이다. 두 번째로, 물체의 서로 다른 레이어들은 투영의 동일한 위치들 상에서 중첩될 수 있다. 이 두 가지 문제는 SfM에서 특징 검출 및 매칭의 정확도를 저하시킨다. 또한, SfM 알고리즘을 실행하려면 일반적으로 시간이 오래 걸리기 때문에 훈련 과정이 길어진다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해, 우리는 스캐너가 원뿔 모양의 X-선 빔을 방출하고 동일한 각도 간격으로 투영을 캡처하는 원형 원뿔 빔 X-선 스캐닝 시나리오에 대한 ACUI(Angle-pose Cuboid Uniform Initialization) 전략을 맞춤화한다. 도 1에 도시된 바와 같다. 도 3의 (a)를 참조하면, ACUI는 X-선 스캐너의 파라미터를 이용하여 외인성 행렬\\(\\mathbf{M}_{ext}\\)과 내인성 행렬\\(\\mathbf{M}_{int}\\)을 계산한다.\n' +
      '\n' +
      '[\\mathbf{M}_{ext}=\\begin{bmatrix}-\\sin\\phi&\\cos\\phi&0&0\\\\0&-1&0\\cos\\phi&0&L_{SO}\\\\0&0&0&1\\end{bmatrix},\\\\\\\\mathbf{M}_{int}=\\begin{bmatrix}L_{SD}&0&W/2&0\\\\0&L_{SD}&H/2&0\\\\end{bmatrix}, \\tag{12}\\\n' +
      '\n' +
      '여기서 \\(L_{SO}\\)는 X-선 소스와 스캔된 물체 사이의 거리를 나타낸다. X선 소스의 고도각은 0으로 설정되고 변경되지 않습니다. ACUI의 다음 단계는 3D 가우시안들의 중심 위치를 초기화하는 것이다. 초기에는 주사 대상물의 정밀한 형상이 주어지지 않지만, 주사 공간을 근사화할 수 있다. 우리는 물체를 완전히 둘러쌀 수 있는 크기\\(S_{1}\\times S_{2}\\times S_{3}\\)(mm)의 큐보이드를 설정하였다. 이 직육면체의 중심은 물체의 중심이자 세계 좌표계의 원점이기도 하다. 우리는 이 큐보이드를 크기\\(M_{1}\\times M_{2}\\times M_{3}\\)의 격자로 나눈다. 그런 다음 격자 내의 점들을 간격 \\(d\\in\\mathbb{R}\\)으로 균일하게 샘플링한다.\n' +
      '\n' +
      '\\mmathcal{P}=\\Big{\\{}\\big{(}\\frac{n_{1}S_{1}d}{M_{1}},\\frac{n_{2}S_{2}d}{M_{2}},\\frac{n_{3}S_{3}d}{M_{3}}\\big{}\\|\\-[\\frac{M_{i}}{2d}]-1\\leqn_{i}\\leq[\\frac{M_{i}}{2d}]+1,\\i=1,2,3\\Big{\\},\\tag{13}\\big{{n_{3}S_{3}d}{M_{3}}}\\big{}\\frac{n_{i}}{1}\\lq[\\frac{M_{i}}}{1},\\i=1,2,3\\Big{\\}},\\tag{13}}}}\\frac{n_{i}\n' +
      '\n' +
      '여기서 \\(n_{i}\\in\\mathbb{Z}\\). 그리고 \\(\\mathcal{P}\\)의 크기와 원소를 이용하여 \\(N_{p}\\)과 \\(\\boldsymbol{\\mu}_{i}\\)을 초기화한다. SfM을 실행하는 것을 피하는 ACUI는 X-가우시안(X-Gaussian)이 더 빠른 트레이닝 속도를 즐길 수 있게 한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Settings\n' +
      '\n' +
      '**Dataset.** NAF[60]에 이어, 우리는 우리의 방법을 평가하기 위해 인간 장기 CT의 공개 데이터세트, _i.e._, LIDC-IDRI[2] 및 개방형 과학 시각화 데이터세트[28]를 채택한다. 테스트 장면에는 가슴, 발, 머리, 복부, 췌장이 포함됩니다. 0\\(0\\sim 180^{\\circ}\\)의 범위에서 3%의 잡음으로 100개의 투영을 포착하기 위해 오픈소스 단층촬영 툴박스 TIGRE[6]을 채택하였다. NVS 작업에서 50개의 돌기는 훈련에 사용되고 나머지 50개의 돌기는 테스트에 사용된다. CT 볼륨은 희소 뷰 CT 재구성 작업에서 테스트에 사용된다.\n' +
      '\n' +
      '**구현 상세.** 우리의 X-Gaussian은 PyTorch[40] 및 CUDA[18]에 의해 구현된다. 모델은 Adam optimizer [27] (\\(\\beta_{1}\\) = 0.9, \\(\\beta_{2}\\) = 0.999, \\(\\epsilon\\) = 1\\(\\times 10^{-15}\\))으로 2\\(\\times 10^{4}\\) 반복 학습한다. 점군 위치에 대한 학습률은 초기 1.9\\(\\times 10^{-4}\\)로 설정되었고, 지수함수적으로 1.9\\(\\times 10^{-6}\\)으로 감소되었다. 점 특징, 불투명도, 스케일링 및 회전에 대한 학습률은\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Method & Infer Speed & Train Time & Chest & Foot & Head & Abdomen & Pancreas & Average \\\\ \\hline InTomo [58] & 0.62 fps & 125 min & 28.948 & 39.482 & 34.832 & 27.641 & 20.031 & 30.187 \\\\  & & 0.9915 & 0.9979 & 0.9977 & 0.9646 & 0.8537 & 0.9611 \\\\ \\hline NeRF [37] & 0.14 fps & 313 min & 36.157 & 41.053 & 29.760 & 24.620 & 19.853 & 30.289 \\\\  & & 0.9988 & 0.9989 & 0.9991 & 0.9559 & 0.8560 & 0.9617 \\\\ \\hline TensoRF [10] & 0.77 fps & 178 min & 23.609 & 37.728 & 34.429 & 27.382 & 29.235 & 30.477 \\\\  & & 0.9402 & 0.9929 & 0.9879 & 0.8730 & 0.8031 & 0.9194 \\\\ \\hline NeAT [42] & 1.78 fps & 69 min & 40.765 & 38.236 & 27.738 & 26.741 & 37.526 & 34.201 \\\\  & & 0.9990 & 0.9963 & 0.9295 & 0.8563 & 0.9017 & 0.9366 \\\\ \\hline NAF [60] & 2.01 fps & 63 min & 42.366 & 38.353 & 30.174 & 37.590 & 36.228 & 36.942 \\\\  & & 0.9993 & 0.9913 & 0.9531 & 0.9855 & 0.8844 & 0.9627 \\\\ \\hline X-Gaussian & **148 fps** & **9 min** & **43.887** & **42.153** & **41.579** & **45.762** & **43.640** & **43.404** \\\\  & & **0.9998** & **0.9997** & **0.9997** & **0.9999** & **0.9976** & **0.9993** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 신규 뷰 합성 과제에 대한 정량적 결과. RTX 8000 GPU에서 평가된 모든 장면의 평균 추론 속도와 학습 시간이 보고된다. 각 장면의 결과의 셀에는 PSNR(상단)과 SSIM(하단)이 나열되어 있다.\n' +
      '\n' +
      '2\\(\\times\\)10\\({}^{-3}\\), 8\\(\\times\\)10\\({}^{-3}\\), 5\\(\\times\\)10\\({}^{-3}\\, 1\\(\\times\\)10\\({}^{-3}\\)으로 설정한다. \\times\\)10\\({}^{-3}\\). (\\gamma\\) in Eq. (11)은 0.2로 설정하였으며, 성능 평가를 위해 PSNR(Peak Signal-to-Noise Ratio)과 SSIM(Structural similarity index measure)[49]를 채택하였다. 추론 속도를 측정하기 위해 초당 프레임(fps)이 사용된다. 실험은 RTX 8000 GPU에서 수행됩니다.\n' +
      '\n' +
      '새로운 시각 합성\n' +
      '\n' +
      '** 정량적 결과** 탭 도 1은 NVS 태스크에서 X-가우시안과 InTomo[58], NeRF[37], TensoRF[10], NeAT[42], NAF[60]을 포함한 5개의 SOTA NeRF 기반 알고리즘 간의 정량적 비교를 보여준다.\n' +
      '\n' +
      '우리는 모든 장면에서 서로 다른 방법의 평균 추론 속도와 훈련 시간을 보고한다. 각 장면의 결과의 셀에는 PSNR(셀 내 상위 엔트리)과 SSIM(셀 내 하위 엔트리)이 나열되어 있다. 우리의 X-Gaussian은 성능면에서 큰 마진으로 SOTA 방법을 능가할 뿐만 아니라 훨씬 더 빠른 추론 속도와 더 저렴한 훈련 비용을 즐긴다는 것을 알 수 있다. 보다 구체적으로, 최근 가장 우수한 X-ray NeRF 기반 방법 NAF와 비교하여, 본 논문에서 제안한 X-Gaussian은 평균 6.5 dB의 성능 향상을 보였으며, 추론 속도는 73\\(\\times\\)으로 15% 미만의 훈련 시간을 필요로 한다. SOTA RGB NeRF 기반 TensoRF와 비교했을 때, 192\\(\\times\\)의 추론 속도와 20\\(\\times\\)의 학습 속도를 즐기면서 X-Gaussian은 12.93 dB 더 높다.\n' +
      '\n' +
      '우리 방법의 우수성을 직관적으로 입증하기 위해 그림 1에서 서로 다른 알고리즘의 PSNR-분-fps 비교를 플로팅한다. 세로축은 PSNR(dB)의 성능을 나타낸다. 가로축은 fps의 추론 속도를 나타낸다. 원의 반지름은 분 단위의 훈련 시간을 의미한다. X-가우시안(X-Gaussian)이 훈련 시간이 가장 짧은 우상단 코너를 완전히 차지함을 알 수 있어 모델 효율에서 극도의 장점을 보인다.\n' +
      '\n' +
      '**정성적 결과.** 도 5 및 도 6은 췌장, 흉부, 발 및 머리의 장면에 대한 NVS의 질적 비교를 나타낸다. 이전 NeRF 기반 알고리즘이 선명하게 렌더링하지 못하는 줌인 패치에서 관찰할 수 있다.\n' +
      '\n' +
      '그림 5: 췌장(위)과 흉부(아래)의 장면에 대한 새로운 시각 합성의 질적 결과. 우리의 X-가우시안 결과는 더 명확하다. 더 나은 보기를 보려면 확대하십시오.\n' +
      '\n' +
      '새로운 경치 그들은 원하지 않는 인공물을 도입하거나 발의 발가락 뼈와 같은 흐릿한 질감을 생성한다. 이와는 대조적으로, 본 방법은 보다 세밀한 세부 사항과 보다 명확한 구조적 내용을 렌더링함으로써 시각적으로 사실적인 이미지를 생성한다.\n' +
      '\n' +
      '### 희소시점 CT 재구성\n' +
      '\n' +
      '본 논문에서 제안한 방법을 SOTA NeRF 기반의 희소 시점 CT 재구성 알고리즘과 비교한다. 가우시안 포인트 클라우드는 CT 볼륨의 방사선을 직접 추론할 수 없기 때문에, 분석 방법(FDK[17])과 두 가지 반복 방법(SART[1] 및 ASD-POCS[45])을 포함한 세 가지 학습 없는 CT 재구성 방법에 대한 새로운 뷰 투영을 생성하기 위해 다양한 NeRF 기반 방법과 X-가우시안 방법을 평가한다. 구체적으로, 이 세 가지 방법은 서로 다른 NVS 알고리즘에 의해 렌더링된 5개의 원본 투영과 95개의 신규 뷰 투영으로부터 CT를 재구성한다. 정량적 결과는 탭 2에 나열되어 있다. 5개의 원본 뷰(+ 없음)만을 사용하는 경우, FDK, SART 및 ASD-POCS는 PSNR에서 각각 7.41, 17.24 및 17.03 dB를 달성한다. CT 볼륨을 재구성하지 못합니다. FDK, SART 및 ASD-POCS에 대한 새로운 X-선 투영을 생성하기 위해 X-Gaussin을 사용할 때 PSNR에서 15.19, 13.01 및 13.53dB의 가장 큰 개선을 얻었다. 이러한 개선은 NAF를 사용하는 개선보다 1.32, 2.41 및 2.65dB 더 높다.\n' +
      '\n' +
      '무화과 도 7과 도 8은 발, 가슴, 복부, 머리 장면에 대한 희소 시점 CT 재구성의 질적 결과를 보여준다. 렌더링된 새로운 뷰 투영을 사용하지 않고, SART 및 ASD-POCS는 CT 슬라이스를 재구성하는데 실패한다. 새로운 뷰를 생성하기 위해 SOTA X-ray NeRF 기반 방법을 사용할 때 SART 및 ASD-POCS는 구조적 내용이 흐릿한 과도하게 매끄러운 CT 슬라이스를 생성한다. 반대로, SART 및 ASD-POCS를 보조하기 위해 X-가우시안(X-Gaussian)을 사용할 때, 그들은 흉부 혈관과 같은 더 높은 주파수 질감과 세밀한 구조적 세부 사항으로 훨씬 더 선명한 CT 슬라이스를 재구성할 수 있다(도 7). (도 8). 이러한 결과는 희소 뷰 CT 재구성 작업에 대한 우리의 방법의 잠재적 실용적 가치를 분명히 보여준다.\n' +
      '\n' +
      '그림 6: 발(위)과 머리(아래)에서 새로운 뷰 합성의 정성적 결과. 우리의 방법은 더 세밀한 세부 사항을 재구성한다. 더 나은 보기를 보려면 확대하십시오.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**Break-down Ablation.** 우리는 먼저 더 높은 성능과 더 빠른 속도에 대한 각각의 제안된 기술의 효과를 연구하기 위해 break-down Ablation 실험을 수행한다. 우리는 원래 3DGS[26]을 기준 모델로 채택하고 RGB 채널을 순진하게 평균하여 방사선 강도의 값을 나타낸다. 결과는 탭에 나열되어 있습니다. 2(a) 기준선 모델은 성능에서 37.21 dB PSNR을 산출한다. 평균 훈련 시간과 추론 속도는 각각 31분 38초와 64fps이다. 탭에서 관찰할 수 있습니다. 2(a) : **(i)** ACUI를 사용하여 초기화를 위한 시간이 많이 소요되는 SfM[46] 알고리즘을 대체할 경우, 훈련 시간은 34% 크게 단축되는 반면 성능은 1.66dB의 개선 효과를 얻을 수 있다. 이 증거는 ACUI 전략이 원래 3DGS에서 사용된 SfM[46] 알고리즘보다 훨씬 빠르고 3D 가우시안 및 후속 렌더링에 대한 보다 정확한 카메라 보정 매개변수를 계산할 수 있음을 시사한다. *(ii)** 다음, 제안된 미분 방사 래스터화(DRR)를 탑재한 복사 가우스 점군 모델을 적용하여 원래의 RGB 가우스 점군 모델과 그 RGB 래스터화를 대체한다. Sec. 3.1에서 분석하고 그림 1에서 비교했다. 도 4에 도시된 바와 같이, 이방성 구형 고조파(SH)는 관통에 기초한 X선 이미징이 등방성이기 때문에 X선 이미징에 적합하지 않다. 대조적으로, 우리의 복사 가우시안 포인트 클라우드 모델은 더 잘 맞을 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c c c c c c c|c c} \\hline \\hline Method & \\multicolumn{3}{c}{+ None} & \\multicolumn{3}{c}{+ InTamo [58]} & \\multicolumn{3}{c}{+ NeRF [37]} & \\multicolumn{3}{c}{+ TensoRF [10]} & \\multicolumn{3}{c}{+ NeAT [42]} & \\multicolumn{3}{c}{+ NAF [64]} & \\multicolumn{3}{c}{+ X-Gaussian} \\\\ Metric & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\\\ \\hline FDK & 7.41 & 0.093 & 20.31 & 0.498 & 20.57 & 0.502 & 20.61 & 0.501 & 20.94 & 0.511 & 21.28 & 0.523 & **22.60** & **0.584** \\\\ \\(\\Delta\\) FDK & 0 & 0 & 12.90 & 0.405 & 13.16 & 0.409 & 13.20 & 0.408 & 13.52 & 0.418 & 13.87 & 0.430 & **15.19** & **0.491** \\\\ \\hline SART & 17.24 & 0.528 & 26.28 & 0.859 & 26.78 & 0.853 & 27.06 & 0.867 & 27.31 & 0.869 & 27.84 & 0.879 & **30.25** & **0.907** \\\\ \\(\\Delta\\) SART & 0 & 0 & 9.04 & 0.331 & 9.54 & 0.325 & 9.82 & 0.339 & 10.07 & 0.341 & 10.60 & 0.351 & **13.01** & **0.379** \\\\ \\hline ASD-POCS & 17.03 & 0.525 & 25.44 & 0.847 & 26.58 & 0.857 & 26.93 & 0.868 & 26.95 & 0.865 & 27.91 & 0.880 & **30.56** & **0.926** \\\\ \\(\\Delta\\) ASD-POCS & 0 & 0 & 8.41 & 0.322 & 9.55 & 0.332 & 9.90 & 0.343 & 9.92 & 0.340 & 10.88 & 0.355 & **13.53** & **0.401** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 희소 뷰 CT 재구성에 대한 결과. NeRF 기반 방법과 X-Gaussian을 사용하여 FDK[17], SART[1], ASD-POCS[45]에 대한 새로운 뷰를 생성한다.\n' +
      '\n' +
      '도 7: 발과 가슴의 장면에 희소 뷰 CT 재구성의 시각적 결과. NeRF 기반 방법과 X-Gaussian은 SART [1]에 대한 새로운 뷰를 생성하기 위해 사용된다.\n' +
      '\n' +
      '3D 공간에서의 뷰 독립적인 방사선 세기. 따라서 PSNR에서 4.53 dB의 성능 향상을 보였다. 또한, 래스터화의 순방향 및 역방향 프로세스에서 뷰 방향과 관련된 계산을 제거하는 것은 트레이닝 및 추론 속도를 더욱 가속화할 수 있다. 따라서 학습시간은 54.10% 감소하였고, 추론속도는 2.1\\(\\times\\) 빨라졌다.\n' +
      '\n' +
      '**포인트 포지션의 초기화.** 랜덤, 구형, FDK[17] 및 큐빅 초기화를 포함하는 가우시안 포인트 클라우드의 중심 위치에 대한 상이한 초기화 전략을 비교한다. 구체적으로, 랜덤 초기화란 스캔된 영역 내의 점들을 3차원 공간에서 랜덤하게 샘플링하는 것을 의미한다. 구형 초기화는 스캔된 객체를 완전히 둘러쌀 수 있는 구 내의 점 위치를 균일하게 샘플링한다. FDK[17] 초기화는 FDK 알고리즘을 채택하여 주어진 투영을 3D 점 위치로 역투영한다. 큐보이드 초기화는 ACUI입니다. 서로 다른 전략 간의 공정한 비교를 위해 계산된 내재 행렬과 외재 행렬을 동일하게 유지한다는 점에 유의하십시오. 결과는 탭에 보고됩니다. 2(b). FDK 초기화는 ACUI보다 0.066 dB 정도 성능이 향상되지만, 훈련 시간은 2.59\\(\\times\\) 더 길고 추론 속도는 ACUI보다 55 fps 더 느리다. FDK에서의 역투영은 시간이 많이 걸리고 중복점들을 초기화시키기 때문이다. 랜덤 및 구형 초기화 전략은 3차 초기화보다 낮은 PSNR과 느린 속도를 산출한다. 더 나은 트레이드 오프를 달성하기 위해, 우리는 좋은 성능, 가장 저렴한 트레이닝 비용 및 가장 빠른 추론 속도를 즐기는 큐빅 초기화 _i.e._, ACUI를 채택한다.\n' +
      '\n' +
      '**파라미터 분석** 특징 개수\\(N_{f}\\) 및 샘플링 간격\\(d\\)에 대한 파라미터 분석을 실시한다. 결과는 탭에 나와 있습니다. 2(c) 및 탭. 2(d).\n' +
      '\n' +
      '탭에서 2(c), **(i)** \\(N_{f}\\)을 증가시키면 성능은 점차 향상되지만 개선의 크기는 감소한다. 특히, \\(N_{f}=32\\)는 PSNR에서 43.42 dB의 최상의 결과를 달성한다. \\(N_{f}=32\\) (N_{f}=16\\)\\(N_{f}=32\\)의 온-파 결과를 얻었으며, 0.013 dB만 더 낮았다. **(ii)** 훈련 시간과 추론 속도가 단조롭게 변하지 않는다는 것을 알 수 있다. 이는 다양한 특징 차원을 갖는 가우시안 포인트 클라우드가 표현 능력이 다르고,\n' +
      '\n' +
      '도 8: 복부 및 머리에 희소 뷰 CT 재구성의 시각적 결과. NeRF-기반 방법들 및 X-가우시안들은 ASD-POCS에 대한 새로운 뷰들을 생성하기 위해 사용된다[45].\n' +
      '\n' +
      '계산 복잡성. 훈련 후 최종 3D 가우시안들의 수도 상당히 다양하다. (N_{f}=16\\)일 때, 트레이닝 시간은 국부 최소값에 도달하고 추론 속도는 국부 최대값에 도달한다. 따라서 우리는 성능과 속도 사이의 보다 최적의 균형에 도달하기 위해 결국 \\(N_{f}=16\\)을 채택한다.\n' +
      '\n' +
      '탭에서 2(d), 가장 좋은 성능, 가장 저렴한 메모리 비용 및 가장 빠른 추론 속도는 \\(d=8\\)에서 달성된다. (d=8\\)에서 훈련시간(\\(538\\) s)은 \\(d=16\\)에서 가장 짧은 훈련시간(\\(534\\) s)과 거의 같다. 따라서, \\(d\\)을 \\(8\\)으로 설정하였다.\n' +
      '\n' +
      '**융합 분석.** 그림의 X-가우시안과 원본 3DGS[26] 사이의 수렴을 비교하기 위해 두 가지 시각적 분석을 수행한다. 도 2, 및 도 9의 X-가우시안 및 SOTA X-선 NeRF 기반 방법 NAF[60] 사이에서.\n' +
      '\n' +
      '구체적으로, 가우시안 포인트 클라우드 모델과 래스터화를 비교하는 데 초점을 맞추기 위해 원본 3DGS에 대해 동일한 ACUI 전략을 채택한다. 공정성을 위해 3DGS와 X-Gaussian을 동일한 설정으로 발 장면에서 훈련하고 그림 2의 훈련 과정의 \\(100\\)-\\(th\\), \\(1000\\)-\\(th\\), \\(5000\\)-\\(th\\), \\(10000\\)-\\(th\\), \\(20000\\)-\\(th\\) 반복에서 가우시안 포인트 클라우드의 위치와 불투명도를 시각화하고 발의 CT 볼륨을 기준으로 시각화한다. CT 볼륨은 포인트 클라우드의 지상 진실이 아닙니다. RGB 래스터화를 갖는 원래의 3DGS는 천천히 수렴하고 더 시끄러운 포인트 클라우드를 겪는다는 것을 알 수 있다. 또한, \\(20000\\)-\\(th\\) 반복에서 3DGS의 최종 훈련된 모델은 발의 3D 구조를 표현하는데 불필요한 여분의 가우시안들을 더 포함하고 있어 모델의 추론 속도를 감소시킨다. 이와는 대조적으로, 제안된 DRR이 장착된 X-Gaussian은 더 빠르고 더 나은 수렴을 보여준다. 특히, \\(1000\\)-\\(th\\) 반복일 때, 우리의 복사 가우시안 점 구름은 기본적으로 발의 기본 모양을 형성했다. 또한, \\(20000\\)-\\(th\\) 반복에서 최종 훈련된 X-Gaussian은 원래의 3DGS보다 3D 기하구조와 정확한 구조적 내용을 더 잘 표현할 수 있다.\n' +
      '\n' +
      '부가적으로, 도 1에 도시된 바와 같다. 도 9를 참조하면, 흉부 장면에서 훈련 과정의 \\(20\\)s, \\(60\\)s, \\(180\\)s에서 NAF와 X-Gaussian의 동일한 각도 \\(\\phi\\)로 렌더링된 새로운 사영을 시각화한다. NAF는 훈련 후 처음 3분 이내에 배경 영역에 심각한 잡음이 있는 흐릿한 이미지를 생성한다. 대조적으로, 우리의 X-가우시안(X-Gaussian)은 훈련 첫 분에 흉부 배경이 더 깨끗한 갈비뼈와 혈관과 같은 더 명확한 구조적 세부 사항을 재구성할 수 있다.\n' +
      '\n' +
      '두 가지 시각적 분석은 X-Gaussian의 수렴 장점을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 3: 절제 연구. PSNR, SSIM, 훈련 시간 및 추론 속도가 보고된다.\n' +
      '\n' +
      '공분산 분석.그림의 학습 뷰 수에 따라 3D 가우시안 포인트 클라우드의 모양이 어떻게 변하는지를 연구한다. 10. 트레이닝 뷰의 수가 증가함에 따라, 가우시안 포인트 클라우드의 크기를 제어하는 3D 가우시안들의 평균 공분산이 감소한다. 이는 3D 가우시안 포인트 클라우드가 거친 것에서 미세한 것으로 점차 변화하여 복부의 작은 종양과 같은 미세한 구조를 나타낼 수 있음을 나타낸다.\n' +
      '\n' +
      '## 5 Limitation\n' +
      '\n' +
      'NeRF 기반 방법에 비해 X-Gaussian은 컴퓨터 그래픽과 3D 비전에 더 많은 기초적인 배경 지식을 필요로 하기 때문에 더 복잡하고 추적하기 어렵다. X-Gaussian에 대한 많은 기술적 세부 사항은 Pytorch 대신 CUDA에 의해 구현된다. C++에 기반한 CUDA는 파이썬에 의존하는 파이토치보다 디버깅하기가 더 어렵고 해석하기 어렵다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 X-ray 새로운 뷰 합성을 위한 첫 번째 3DGS 기반 프레임워크인 X-Gaussian을 제안한다. 우리의 X-가우시안에는 두 가지 기술적인 요소가 있다. 먼저, X-선 영상의 등방성 특성을 고려하기 위해 복사 가우시안 점군 모델을 재설계한다. 이 모델은 복사 강도를 맞출 때 뷰 방향의 영향을 배제합니다. 이 모델을 기반으로 RGB 래스터화보다 더 빠른 속도로 투영을 렌더링하는 GPU 친화적인 미분 가능한 복사 래스터화 CUDA 커널을 추가로 개발한다. 둘째, RGB 3DGS와 같이 시간이 많이 소요되는 SfM 알고리즘을 실행할 필요가 없는 초기화 전략인 ACUI를 사용자화한다. 대신, ACUI는 X-선 스캐너의 파라미터를 명시적으로 사용하여 외부 행렬과 내부 행렬을 계산한 다음 스캔된 물체를 둘러싸는 직육면체 내에서 3D 가우시안들에 대한 중심점을 균일하게 샘플링한다. 종합적인 실험을 통해 X-Gaussian이 SOTA 방법보다 6.5 dB 이상 우수한 성능을 보임을 알 수 있었고, 더 짧은 훈련 시간(15%)과 더 빠른 추론 속도(73\\(\\times\\))를 얻었다. 희소 시점 CT 재구성에 대한 응용은 또한 우리의 방법의 잠재적인 실용적인 가치를 보여준다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Andersen, A.H., Kak, A.C.: Simultaneous algebraic reconstruction technique (sart): a superior implementation of the art algorithm. Ultrasonic imaging (1984) 10, 11\n' +
      '* [2] Armato III, S.G., McLennan, G., Bidaut, L., McNitt-Gray, M.F., Meyer, C.R., Reeves, A.P., Zhao, B., Aberle, D.R., Henschke, C.I., Hoffman, E.A., et al.: The lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on ct scans. Medical physics (2011) 8\n' +
      '* [3] Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srinivasan, P.P.: Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In: ICCV (2021) 3\n' +
      '* [4] Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In: CVPR (2022) 3\n' +
      '* [5] Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Zip-nerf: Anti-aliased grid-based neural radiance fields. In: ICCV (2023) 3\n' +
      '* [6] Biguri, A., Dosanjh, M., Hancock, S., Soleimani, M.: Tigre: a matlab-gpu toolbox for cbct image reconstruction. Biomedical Physics & Engineering Express (2016) 8\n' +
      '* [7] Boone, J., Shah, N., Nelson, T.: A comprehensive analysis of coefficients for pendant-geometry cone-beam breast computed tomography. Medical physics (2004) 1\n' +
      '* [8] Boone, J.M., Nelson, T.R., Lindfors, K.K., Seibert, J.A.: Dedicated breast ct: radiation dose and image quality evaluation. Radiology (2001) 1\n' +
      '* [9] Cai, Y., Wang, J., Yuille, A., Zhou, Z., Wang, A.: Structure-aware sparse-view x-ray 3d reconstruction. In: CVPR (2023) 3\n' +
      '* [10] Chen, A., Xu, Z., Geiger, A., Yu, J., Su, H.: Tensorf: Tensorial radiance fields. In: ECCV (2022) 3, 8, 9, 11\n' +
      '* [11] Chen, B., Ning, R.: Cone-beam volume ct breast imaging: Feasibility study. Medical physics (2002) 1\n' +
      '* [12] Chen, Z., Funkhouser, T., Hedman, P., Tagliasacchi, A.: Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In: CVPR (2023) 3\n' +
      '* [13] Cormack, A.M.: Representation of a function by its line integrals, with some radiological applications. Journal of applied physics (1963) 1\n' +
      '* [14] Cormack, A.M.: Representation of a function by its line integrals, with some radiological applications. ii. Journal of Applied Physics (1964) 1\n' +
      '* [15] Corona-Figueroa, A., Frawley, J., Bond-Taylor, S., Bethapudi, S., Shum, H.P., Willcocks, C.G.: Mednerf: Medical neural radiance fields for reconstructing 3d-aware ct-projections from a single x-ray. In: International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC) (2022) 3\n' +
      '* [16] Elbakri, I.A., Fessler, J.A.: Segmentation-free statistical image reconstruction for polyenergetic x-ray computed tomography with experimental validation. Physics in Medicine & Biology (2003) 1\n' +
      '* [17] Feldkamp, L.A., Davis, L.C., Kress, J.W.: Practical cone-beam algorithm. Josa a (1984) 10, 11, 12, 13\n' +
      '* [18] Guide, D.: Cuda c programming guide. NVIDIA (2013) 8\n' +
      '* [19] Hounsfield, G.N.: Computerized transverse axial scanning (tomography): Part 1. description of system. The British journal of radiology (1973) 1\n' +
      '* [20] Hounsfield, G.N.: Computed medical imaging. Science (1980) 1* [21] Hu, S., Liu, Z.: Gauhuman: Articulated gaussian splatting from monocular human videos. arXiv preprint arXiv: (2023)\n' +
      '* [22] Hu, T., Liu, S., Chen, Y., Shen, T., Jia, J.: Efficientnerf efficient neural radiance fields. In: CVPR (2023)\n' +
      '* [23] Hu, W., Wang, Y., Ma, L., Yang, B., Gao, L., Liu, X., Ma, Y.: Tri-miprf: Tri-mip representation for efficient anti-aliasing neural radiance fields. In: ICCV (2023)\n' +
      '* [24] Jiang, Y., Tu, J., Liu, Y., Gao, X., Long, X., Wang, W., Ma, Y.: Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces. arXiv preprint arXiv:2311.17977 (2023)\n' +
      '* [25] Keetha, N., Karhade, J., Jatavallabhula, K.M., Yang, G., Scherer, S., Ramanan, D., Luiten, J.: Splatam: Splat, track & map 3d gaussians for dense rgb-d slam. arXiv preprint arXiv:2312.02126 (2023)\n' +
      '* [26] Kerbl, B., Kopanas, G., Leimkuhler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (2023)\n' +
      '* [27] Kingma, D.P., Ba, J.L.: Adam: A method for stochastic optimization. In: ICLR (2015)\n' +
      '* [28] Klacansky, P.: Scientific visualization datasets (2022), [https://klacansky.com/open-scivis-datasets/](https://klacansky.com/open-scivis-datasets/)\n' +
      '* [29] Kocabas, M., Chang, J.H.R., Gabriel, J., Tuzel, O., Ranjan, A.: Hugs: Human gaussian splats. arXiv preprint arXiv:2311.17910 (2023)\n' +
      '* [30] Kopanas, G., Philip, J., Leimkuhler, T., Drettakis, G.: Point-based neural rendering with per-view optimization. In: Computer Graphics Forum (2021)\n' +
      '* [31] Li, R., Gao, H., Tancik, M., Kanazawa, A.: Nerfacc: Efficient sampling accelerates nerfs. In: ICCV (2023)\n' +
      '* [32] Liang, Y., Yang, X., Lin, J., Li, H., Xu, X., Chen, Y.: Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. arXiv preprint arXiv:2311.11284 (2023)\n' +
      '* [33] Liang, Z., Zhang, Q., Feng, Y., Shan, Y., Jia, K.: Gs-ir: 3d gaussian splatting for inverse rendering. arXiv preprint arXiv:2311.16473 (2023)\n' +
      '* [34] Luiten, J., Kopanas, G., Leibe, B., Ramanan, D.: Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713 (2023)\n' +
      '* [35] Manglos, S.H., Gagne, G.M., Krol, A., Thomas, F.D., Narayanaswamy, R.: Transmission maximum-likelihood reconstruction with ordered subsets for cone beam ct. Physics in Medicine & Biology (1995)\n' +
      '* [36] Matsuki, H., Murai, R., Kelly, P.H., Davison, A.J.: Gaussian splatting slam. arXiv preprint arXiv:2312.06741 (2023)\n' +
      '* [37] Mildenhall, B., Srinivasan, P., Tancik, M., Barron, J., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)\n' +
      '* [38] Muller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives with a multiresolution hash encoding. ACM ToG (2022)\n' +
      '* [39] Pan, J., Zhou, T., Han, Y., Jiang, M., et al.: Variable weighted ordered subset image reconstruction algorithm. International Journal of Biomedical Imaging (2006)\n' +
      '* [40] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance deep learning library. In: NeurIPS (2019)\n' +
      '* [41] Reiser, C., Szeliski, R., Verbin, D., Srinivasan, P., Mildenhall, B., Geiger, A., Barron, J., Hedman, P.: Merf: Memory-efficient radiance fields for real-time view synthesis in unbounded scenes. TOG (2023)42] Ruckert, D., Wang, Y., Li, R., Idoughi, R., Heidrich, W.: Neat: Neural adaptive tomography. TOG (2022)\n' +
      '* [43] Sauer, K., Bouman, C.: A local update strategy for iterative reconstruction from projections. TIP (1993)\n' +
      '* [44] Scarfe, W.C., Farman, A.G., Sukovic, P., et al.: Clinical applications of cone-beam computed tomography in dental practice. Journal-Canadian Dental Association (2006)\n' +
      '* [45] Sidky, E.Y., Pan, X.: Image reconstruction in circular cone-beam computed tomography by constrained, total-variation minimization. Physics in Medicine & Biology (2008)\n' +
      '* [46] Snavely, N., Seitz, S.M., Szeliski, R.: Photo tourism: exploring photo collections in 3d. In: SIGGRAPH (2006)\n' +
      '* [47] Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)\n' +
      '* [48] Verbin, D., Hedman, P., Mildenhall, B., Zickler, T., Barron, J.T., Srinivasan, P.P.: Ref-nerf: Structured view-dependent appearance for neural radiance fields. In: CVPR (2022)\n' +
      '* [49] Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncell, E.P.: Image quality assessment: from error visibility to structural similarity. TIP (2004)\n' +
      '* [50] Wu, G., Yi, T., Fang, J., Xie, L., Zhang, X., Wei, W., Liu, W., Tian, Q., Wang, X.: 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528 (2023)\n' +
      '* [51] Xie, T., Zong, Z., Qiu, Y., Li, X., Feng, Y., Yang, Y., Jiang, C.: Physgaussian: Physics-integrated 3d gaussians for generative dynamics. arXiv preprint arXiv:2311.12198 (2023)\n' +
      '* [52] Yan, C., Qu, D., Wang, D., Xu, D., Wang, Z., Zhao, B., Li, X.: Gs-slam: Dense visual slam with 3d gaussian splatting. arXiv preprint arXiv:2311.11700 (2023)\n' +
      '* [53] Yang, Z., Yang, H., Pan, Z., Zhu, X., Zhang, L.: Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642 (2023)\n' +
      '* [54] Yariv, L., Hedman, P., Reiser, C., Verbin, D., Srinivasan, P.P., Szeliski, R., Barron, J.T., Mildenhall, B.: Bakedsdf: Meshing neural sdfs for real-time view synthesis. In: SIGGRAPH (2023)\n' +
      '* [55] Yi, T., Fang, J., Wu, G., Xie, L., Zhang, X., Liu, W., Tian, Q., Wang, X.: Gausi-sandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. arXiv preprint arXiv:2310.08529 (2023)\n' +
      '* [56] Yu, L., Zou, Y., Sidky, E.Y., Pelizzari, C.A., Munro, P., Pan, X.: Region of interest reconstruction from truncated data in circular cone-beam ct. TMI (2006)\n' +
      '* [57] Yugay, V., Li, Y., Gevers, T., Oswald, M.R.: Gaussian-slam: Photo-realistic dense slam with gaussian splatting. arXiv preprint arXiv:2312.10070 (2023)\n' +
      '* [58] Zang, G., Idoughi, R., Li, R., Wonka, P., Heidrich, W.: Intratomo: self-supervised learning-based tomography via sinogram synthesis and prediction. In: CVPR (2021)\n' +
      '* [59] Zbijewski, W., Defrise, M., Viergever, M.A., Beekman, F.J.: Statistical reconstruction for x-ray ct systems with non-continuous detectors. Physics in Medicine & Biology (2006)\n' +
      '* [60] Zha, R., Zhang, Y., Li, H.: Naf: neural attenuation fields for sparse-view cbct reconstruction. In: MICCAI (2022)\n' +
      '* [61] Zwicker, M., Pfister, H., Van Baar, J., Gross, M.: Ewa volume splatting. In: Proceedings Visualization, 2001. VIS\'01. IEEE (2001)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>