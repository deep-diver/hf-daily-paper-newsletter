<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# TinyLLaVA: A Framework of Small-scale Large Multimodal Models\n' +
      '\n' +
      'Baichuan Zhou\\({}^{1}\\)  Ying Hu\\({}^{2}\\)  Xi Weng\\({}^{1}\\)  Junlong Jia\\({}^{1}\\)  Jie Luo\\({}^{1}\\)  Xien Liu\\({}^{2}\\)  Ji Wu\\({}^{2}\\)  Lei Huang\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)SKLCCSE, Institute of Artificial Intelligence, Beihang University, Beijing, China\n' +
      '\n' +
      '\\({}^{2}\\)Department of Electronic Engineering, Tsinghua University, China\n' +
      '\n' +
      'Technical ReportCorresponding author: Lei Huang (_huanglei4@buaa.edu.cn_)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large Multimodal Models (LMMs). We empirically study the effects of different vision encoders, connection modules, language models, training data and training recipes. Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs. Under our framework, we train a family of small-scale LMMs. Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as baselines for future research in terms of data scaling, training setups and model selections. Our model weights and codes will be made public1.\n' +
      '\n' +
      'Footnote 1: available at [https://github.com/DLCV-BUAA/TinyLLaVABench](https://github.com/DLCV-BUAA/TinyLLaVABench).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'The AI community has witnessed remarkable capabilities of Large Language Models (LLMs). With the scaling laws [19, 26] serving as guidelines and emergent abilities [51] being studied, recent years have featured a trend towards scaling up model sizes, with the largest dense language models over 500 billion parameters [9, 46]. Inspired by LLMs, Large Multimodal Models (LMMs) [3, 38, 49, 62] stand on the shoulders of giants - aligning visual perception with LLMs to acquire multimodal perceptions [21], so that they can directly inherit powerful capabilities from LLMs. This synergy has led to various LMMs released with a massive amount of parameters, like Flamingo with 80B parameters [2], and PaLM-E with 562B parameters [14].\n' +
      '\n' +
      'Despite the fact that scaling up model sizes can significantly enhance performance across various tasks, training such large models requires expensive computational resources and their large sizes may lead to unaffordable training/inference budget, which restricts research access to only well-funded industries and organizations. From a practical perspective, another line of work that focuses on small-scale models has gained attention because of affordable cost and efficient training and inference, opening up opportunities for resource-limited academic community.\n' +
      '\n' +
      'In this context, the LLM community starts to release versions of relatively smaller scales, such as 7-B versions [48, 53] and tiny versions under 3B parameters [41, 59, 25], without performance degradation compared to their previous larger counterparts. Following the trend of LLMs, large multimodal models have experienced a similar transformation of model shrinking down to small scales by leveraging relatively smaller LLMs, such as OpenFlamingo [3] and LLaVA series [37, 38], ranging from 3B to 15B. More recent efforts on LMMs have explored various ways for efficient training and deploying in terms of using tiny LLMs [57, 63], applying sparse MoE [35], freezing or lora tuning backbones [49, 54].\n' +
      '\n' +
      'While large multimodal models with small-scale LLMs make it available for more researchers, current attempts [57, 63, 35] take only a glimpse at the wide landscape of design choices of each architecture component, training recipes, the\n' +
      '\n' +
      'Figure 1: TinyLLaVA-3.1B vs. LLaVA-1.5-7B.\n' +
      '\n' +
      'scales of training data, and more. The variability of design options and diversity of techniques in this burgeoning field lead to the complexity in designing LMMs and difficulty in understanding the space of existing methods. In this work, we investigate the wide landscape of large multimodal models under the setting of leveraging small-scale LLMs, which allows us to provide a thorough empirical analysis of different approaches, thereby assisting the researchers and practitioners to navigate in this space. As a result, we present TinyLLaVA, a framework that consists of a vision encoder, small-scale LLM decoder, and intermediate connector, as well as training pipelines.\n' +
      '\n' +
      'Based on this framework, we investigate the effects of different vision encoders, connection modules, language models, training data and training recipes. Our empirical experiments show that with better training recipes and quality of data, smaller LMMs can achieve on-par performance with larger counterparts, setting new baselines for the research field. We finally present a family of small-scale LMMs, encompassing three language models: Phi-2 [33], StableLM-2 [47], and TinyLlama [59], and two vision encoders: CLIP [44], and SigLIP [58]. Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 [37] and Qwen-VL [4].\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Large Multimodal ModelsWith the development of powerful Large Language Models (LLMs) [5, 9, 48] and vision models [44, 58], Large Multimodal Models (LMMs) have seen great improvements [1]. Early works [8, 49, 50] pioneered introducing autoregressive LLMs to vision-language learning. The following research focused on effectively exploiting LLMs by viewing visual signals as conditional information [2, 15, 31]. In particular, Flamingo [2] consider inserting adapters in LLMs and utilizing a perceiver-like [24] architecture to extract visual features and demonstrated impressive performance on vision-language few-shot learning. BLIP models [31, 32] introduce data filtering to improve performance on vision language tasks such as VQA [17] and image captioning [36]. While these models exhibited great vision-language abilities, they only possessed limited zero-shot abilities as they were not trained to follow instructions.\n' +
      '\n' +
      'To better align LMMs with human preferences, recent works, such as LLaVA [38] and InstructBLIP [12], follow [11, 43] and fine-tune LMMs with visual instruction tuning data [30, 62], which greatly enhance LMM\'s zero-shot capabilities. Following this line of work, several techniques are raised to further improve the performances by discussing the possibilities of unlocking vision encoders during training [7, 34], curating high-quality visual instruction tuning datasets [7, 37, 60], and scaling up image resolutions [4, 6, 34].\n' +
      '\n' +
      'Small-scale LMMsDeploying LMMs is expensive as they require high computation overhead. The computation bottleneck is usually introduced by LLMs as they tend to scale to billions of parameters [48, 52]. However, recent small-scale LLMs such as Phi-2 [33], TinyLlama [59] and StableLM-2 [47] have reached impressive performances while maintaining reasonable compute budgets. Following these efforts, a variety of works [10, 35, 57, 63] explored ways to train and deploy small-scale LMMs. In particular, TinyGPT-V [57] fine-tuned the projection layers following MiniGPT-4 [62] and replaced the LLM [61] with Phi [33]; LLaVA-Phi [33] followed LLaVA-1.5\'s procedure and replaced the LLM [52] with Phi-2 [33]; MoE-LLaVA [35] introduced Mixture-of-Experts [23] to the LLaVA architecture and reached competitive performance with LLaVA-1.5 using less activated parameters.\n' +
      '\n' +
      'Distinct to these works [10, 35, 57, 63] that focus on building and training specific small-scale LMMs, our work aims to provide a unified analysis on how model selections, training recipes, and data contribute to model performance for small-scale LMMs. We noted that concurrent work [27] also provides a unified analysis of visually-conditioned language models, but they focus on standard LMMs while we focus on small-scale LMMs. The investigation on small-scale LMMs shows different behaviors than the standard ones, based on our experiments.\n' +
      '\n' +
      '## 3 TinyLLaVA Framework\n' +
      '\n' +
      'In this section, we describe the details of the TinyLLaVA framework that focuses on exploiting small-scale LLMs for large multimodal models. Our TinyLLaVA framework follows the design of LLaVA [38] but generalizes from it for better investigating the variants of the model architecture and training recipe in a unified perspective.\n' +
      '\n' +
      'Figure 2: TinyLLaVA Framework.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      'The architecture of TinyLLaVA (Figure 2) consists of a small-scale LLM \\(F_{\\theta}\\), a vision encoder \\(V_{\\varphi}\\), and a connector \\(P_{\\phi}\\), where \\(\\theta\\), \\(\\varphi\\) and \\(\\phi\\) are the (learnable) parameters respectively. This architecture can model various multimodal understanding tasks that take as input a pair of image and text sequence and output a text sequence.\n' +
      '\n' +
      'Small-scale LLM.The small-scale LLM \\(F_{\\theta}\\) takes as input a sequence of vectors \\(\\{\\mathbf{h}_{i}\\}_{i=0}^{N-1}\\) with length of \\(N\\) in the \\(d\\) dimensional (text) embedding space, and output the corresponding next-predictions \\(\\{\\mathbf{h}_{i}\\}_{i=1}^{N}\\). A tokenizer \\(\\&\\) embedding module is usually bound to the small-scale LLM, mapping the text input sequences \\(\\{\\mathbf{y}_{i}\\}_{i=0}^{N-1}\\) to the embedding space and similarly from the embedding space to the text output sequences \\(\\{\\mathbf{y}_{i}\\}_{i=1}^{N}\\).\n' +
      '\n' +
      'Vision Encoder.The vision encoder \\(V_{\\varphi}\\) take as input an image \\(\\mathbf{X}\\) and output a sequence of (visual) patch features \\(\\mathbf{V}=\\{\\mathbf{v}_{j}\\in\\mathbb{R}^{d_{x}}\\}_{i=j}^{M}\\), where \\(\\mathbf{V}=V_{\\varphi}(\\mathbf{X})\\). The vision encoder can be Vision Transformers [13][44][58] that directly output a sequence of patch features or CNNs that output a grid features followed by a reshape operation to obtain patch features.\n' +
      '\n' +
      'Connector.The connector \\(P_{\\phi}\\) maps the visual patch sequences \\(\\{\\mathbf{v}_{j}\\}_{j=1}^{M}\\) to the text embedding space \\(\\{\\mathbf{h}_{j}\\}_{j=1}^{M}\\), where \\(\\mathbf{h}_{j}=P_{\\phi}(\\mathbf{v}_{j}),j=1,...,M\\). Note that the connector \\(P_{\\phi}\\) is designed for effectively leveraging the capability of both the pre-trained LLM and vision encoder.\n' +
      '\n' +
      '### Training Pipeline\n' +
      '\n' +
      'The data for training TinyLLaVA consists of image-text pairs \\((\\mathbf{X},\\mathbf{Y})\\). Furthermore, the text sequence \\(\\mathbf{Y}\\) is structured as a form of multi-turn conversation \\(\\mathbf{Y}=(\\mathbf{Y}_{q}^{1},\\mathbf{Y}_{a}^{1},...,\\mathbf{Y}_{q}^{T}, \\mathbf{Y}_{a}^{T})\\), where \\(T\\) is the total number of turns, \\(\\mathbf{Y}_{q}^{t}\\) is the human instruction and \\(\\mathbf{Y}_{a}^{t}\\) is the corresponding assistant\'s response2. The training of TinyLLaVa is divided into two stages, pre-training and supervised fine-tuning.\n' +
      '\n' +
      'Pre-training for Feature Alignment.In this stage, we aim to better align the vision and text information in the embedding space. We thus use the image-caption style data format \\((\\mathbf{X},\\mathbf{Y}_{a})\\) that can be derived from the original multi-turn conversation, where \\(\\mathbf{X}\\) is the image and \\(\\mathbf{Y}_{a}\\) is a response (description of the image). Given the target response \\(Y_{a}=\\{\\mathbf{y}_{i}\\}_{i=1}^{N_{a}}\\) with length of \\(N_{a}\\), we compute the probability of generating \\(Y_{a}\\) conditioned by the image as:\n' +
      '\n' +
      'Footnote 2: We omit the system-massage for better readability, since they can be merged into the instruction as conditional input for predicting response.\n' +
      '\n' +
      '\\[p(\\mathbf{Y}_{a}|\\mathbf{X})=\\prod_{i=1}^{N_{a}}F_{\\theta}(\\mathbf{y}_{i}|P_{ \\phi}\\circ V_{\\varphi}(\\mathbf{X})), \\tag{1}\\]\n' +
      '\n' +
      'and maximize its log-likelyhood autoregressively as training objective:\n' +
      '\n' +
      '\\[\\max_{\\phi,\\theta^{{}^{\\prime}},\\varphi^{{}^{\\prime}}}\\ \\ \\sum_{i=1}^{N_{a}}\\log F_{\\theta}( \\mathbf{y}_{i}|P_{\\phi}\\circ V_{\\varphi}(\\mathbf{X})), \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\theta^{{}^{\\prime}}\\) and \\(\\varphi^{{}^{\\prime}}\\) are the subset of \\(\\theta\\) and \\(\\varphi\\), respectively. Note that our framework allows to adjust partially learnable parameters of the LLM and vision encoder during the pre-training stages, considering that only training the connector may not well align the vision and text information when using small-scale LLM.\n' +
      '\n' +
      'Supervised Fine-tuning.We use the image-text pair \\((\\mathbf{X},\\mathbf{Y})\\) in the original form of multi-turn conversation. Let \\(\\mathbb{A}\\) denotes the set of all the tokens that belong to the assistant responses, _i.e._, \\(\\mathbb{A}=\\{\\mathbf{y}|\\mathbf{y}\\in\\mathbf{Y}_{a}^{t},\\ for\\ any\\ t=1,...,T\\}\\). We maximize the log-likelihood of assistant\'s responses autoregressively as training objective during supervised fine-tuning:\n' +
      '\n' +
      '\\[\\max_{\\phi,\\theta^{{}^{\\prime}},\\varphi^{{}^{\\prime}}}\\ \\ \\sum_{i=1}^{N}\\mathbb{I}( \\mathbf{y}_{i}\\in\\mathbb{A})\\log F_{\\theta}(\\mathbf{y}_{i}|P_{\\phi}\\circ V_{ \\varphi}(\\mathbf{X})), \\tag{3}\\]\n' +
      '\n' +
      'where \\(N\\) is the length of text sequence \\(\\mathbf{Y}\\), and \\(\\mathbb{I}(\\mathbf{y}_{i}\\in\\mathbb{A})\\) equals to 1 if \\(\\mathbf{y}_{i}\\in\\mathbb{A}\\), 0 otherwise. We also allow the adjustment of partially learnable parameters of the LLM and vision encoder during the supervised fine-tuning stage.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c} \\hline \\hline Type & Name & Abb. & HF path & Size \\\\ \\hline \\multirow{3}{*}{Small-scale LLM} & TinyLlama & TL & TinyLlama/TinyLlama-1.1B-Chat-v1.0 & 1.1B \\\\  & StableLM-2 & SLM & stabilityai/stablelm-2-zephyr-1\\_6b & 1.6B \\\\  & Phi-2 & Phi & microsoft/phi-2 & 2.7B \\\\ \\hline \\hline \\multirow{2}{*}{Vison encoder} & CLIP & C & openai/clip-vit-large-patch14-336 & 0.3B \\\\  & SigLIP & Sig & google/siglip-so400m-patch14-384 & 0.4B \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Small-scale LLMs and vision encoders used for TinyLLaVA framework in current experiments. “Abb.” refers to the abbreviated model name, which is used in the naming convention for TinyLLaVA models. ”HF path” denotes the pathway to the pre-trained weights of the relevant models we are using on HuggingFace.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'In this section, we conduct comprehensive experiments to investigate how the model architectures, datasets, and training recipes affect small-scale Large Multimodal Models (LMMs) performances based on our TinyLLaVA frameworks.\n' +
      '\n' +
      '### Experimental Settings\n' +
      '\n' +
      '#### 4.1.1 Model Architectures\n' +
      '\n' +
      'We select several representative small-scale LLMs, vision encoders, and connectors to instantiate the models following our TinyLLaVA framework.\n' +
      '\n' +
      'Small-scale LLMs.Table 1 illustrates our LLM selections. We select three representative small-scale LLMs: TinyLlama (1.1B) [59], StableLM-2-1.6B(1.6B) [47] and Phi-2(2.7B) [33]. We find these selections cover a comprehensive parameter range of current small-scale LLMs.\n' +
      '\n' +
      'Vision Encoders.We select CLIP-Large [44] as our vision encoder. Through our preliminary experiments, we found that SigLIP [58] combined with small-scale LLMs yield better performance than CLIP, thus incorporating it into our framework. We use the official checkpoints from HuggingFace for both vision encoders and small-scale LLMs to initialize our models as shown in Table 1.\n' +
      '\n' +
      'Connector.Following LLaVA-1.5 [37], we apply a two-layer Multi-Layer Perceptron (MLP) with GELU activation [18] as the connector between the vision encoders and small-scale LLMs. We also tried to employ resamplers as our connectors, which were implemented similarly to [30].\n' +
      '\n' +
      '#### 4.1.2 Training Data and Recipes\n' +
      '\n' +
      'Training Data.We select two different training datasets, proposed in LLaVA-1.5 [37] and ShareGPT4V [7], to study how data quality affects LMM\'s performance. We outline their differences in Table 2.\n' +
      '\n' +
      'LLaVA-1.5-PT consists of 558k captions, as described in [37]. LLaVA-1.5-SFT contains a total of 665k visual instruction tuning conversations, which is a combination of academic-oriented visual question answering (VQA) [17, 22, 45, 28] samples, instruction tuning data from LLaVA-Instruct [38] and ShareGPT [20].\n' +
      '\n' +
      'ShareGPT4V-PT [7] includes 1246k captions generated by the Share-Captioner [7]. ShareGPT4V-SFT dataset is similar to LLaVA-1.5-SFT [37], with the exception that the 23K detailed description data in LLaVA-1.5-SFT being replaced with detailed captions randomly sampled from the 100K ShareGPT4V data [7].\n' +
      '\n' +
      'Training Recipes.We explore two existing training recipes from [37][7] and study their effects on our model\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline Dataset & Stage & Source & \\#Sample \\\\ \\hline \\multirow{2}{*}{LLaVA-1.5} & PT & LLaVA-1.5-558k & 558k \\\\  & ST & LLaVA-1.5-mix-665k & 665k \\\\ \\hline \\hline \\multirow{2}{*}{ShareGPT4V} & PT & ShareGPT4V-PT-1246k & 1246k \\\\  & ST & ShareGPT4V-mix-665k & 665k \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Datasets used for training TinyLLaVA. ”PT” and ”SFT” refer to two stages of training: pre-training and supervised fine-tuning, respectively.\n' +
      '\n' +
      'Figure 3: The primary differences between two recipes. In the base recipe, we keep parameters of both the vision encoder and small-scale LLM frozen and solely updating the connector. In the share recipe, we freeze the first 12 layeres of the vision encoder and update the rest of the model. Additionally, we initialize connector from the base’s pretrained counter part.\n' +
      '\n' +
      'variants. Their primary distinction is summarized in Figure 3.\n' +
      '\n' +
      'The first recipe is adopted from LLaVA-1.5 [37] and named **base**, which serves as our baseline recipe. During pre-training, we only update the connector \\(P_{\\phi}\\) and keep the rest of the model frozen, and tune the model for one epoch with a learning rate of 1e-3 and a batch size of 256. In the supervised fine-tuning stage, we keep the vision encoder \\(V_{\\varphi}\\) frozen and update both the connector \\(P_{\\phi}\\) and the small-scale LLM \\(F_{\\theta}\\), and tune the model for one epoch with a learning rate of 2e-5 and a batch size of 128.\n' +
      '\n' +
      'We establish our second training recipe **share**, following ShareGPT4V [7]. During pre-training of the share recipe, we initialize the connector from the base\'s pretrained counterpart. Additionally, we keep the first 12 layers of the vision encoder \\(V_{\\varphi}\\) frozen and update the rest of the model for one epoch, with a learning rate of 2e-5 and a batch size of 256. The setup of supervised fine-tuning is the same as the base recipe.\n' +
      '\n' +
      '#### 4.1.3 Evaluation Benchmark\n' +
      '\n' +
      'We evaluate our model variants on four image question-answering benchmarks: VQA-v2 [17], GQA [22], ScienceQA-IMG [40], and TextVQA [45], and five comprehensive benchmark: POPE [55], MM-Vet [56], LLaVA-W (LLaVA-Bench-in-the-Wild) [38], MME [16] and MM-Bench [39]. We provide a brief overview of the key aspects of each benchmark focuses on when assessing model capabilities (See Appendix A).\n' +
      '\n' +
      'Figure 4: Ablation of small-scale LLM backbones. Under the base recipe, We train six variants with three small-scale LLMs and two vision encoders mentioned in Table 1 on LLaVA-1.5 dataset. The titles of the subplots indicate the corresponding vision encoders.\n' +
      '\n' +
      'Figure 5: Ablation of vision encoders. These results are inherited from Figure 4. The titles of the subplots indicate the corresponding small-scale LLMs.\n' +
      '\n' +
      '### Experimental Results\n' +
      '\n' +
      '#### 4.2.1 Investigating the Effects of Model Architectures\n' +
      '\n' +
      'Ablation of Small-scale LLMs.We conduct ablations on small-scale LLMs backbones. The results are presented in Figure 4. We can observe that model variants using Phi-2 [33] perform exceptionally well across various configurations and benchmark evaluations, which could be attributed to the larger parameters of Phi-2. Notably, the Phi-2 variants significantly outperform the other variants on SQA-I [40], which may be attributed to its intensive training on text-book data. While the TinyLlama [59] variants are our smallest model and exhibit slightly lower overall performances, they show better POPE accuracy compared to the StableLM-2 [47] variants. Our ablations confirm that larger language models improve performance under the base settings.\n' +
      '\n' +
      'Ablation of Vision Encoders.Following the experimental findings presented in Figure 4, we showcase them in Figure 5. It is noteworthy that model variants with SigLIP [58] exhibit substantial enhancements in model performances compared to those with CLIP [44], which is particularly evident in TextVQA [45] and LLaVA-W [38] benchmarks. It is essential to note that the SigLIP variants we employed have higher input resolutions (384 vs. 336) and more visual tokens (729 vs. 576) compared to CLIP. These factors may contribute to SigLIP containing more beneficial visual information to perform fine-grained image understanding.\n' +
      '\n' +
      'Preliminary Exploration of Connectors.We offer preliminary insights into connectors by comparison between MLP and resampler. Our observations shown in Figure 6 reveal that, using resampler as the connector results in a degradation of performance under a similar parameter setting compared with MLP, which is consistent with previous research findings [37]. We anticipate further exploration of diverse connectors in future studies.\n' +
      '\n' +
      'Summary.In this part, we observe that model variants with larger LLMs can achieve better overall performance. Besides, Applying SigLIP [58] (with a higher input resolution and more visual tokens) as the vision encoder can improve performance significantly compared to CLIP [44].\n' +
      '\n' +
      '#### 4.2.2 Investigating Data Mixtures and Training Recipes\n' +
      '\n' +
      'Ablation of Data Mixtures.We also conduct ablation experiments under the base recipe to showcase the impact of different data mixtures. Our results in Figure 7 indicate that, when pretrained on the more extensive and more diverse ShareGPT4V [7] dataset under the base recipe, model variants with TinyLlama [59] as the small-scale LLM demonstrate an overall improvement in evaluation performance compared to the LLaVA-1.5 dataset [37]. However, notable degradation is observed in POPE [55]. In contrast, the performance of model variants with StableLM-2 and Phi-2 experienced a comprehensive improvement. We speculate that this may be due to the insufficient parameters of TinyLlama [59], which prevents it from adequately fitting to a larger amount of data and results in partial knowledge degradation and more hallucinations.\n' +
      '\n' +
      'Ablation of Training Recipes.Furthermore, we explore the impact of different training recipes. The results are shown in Figure 8. We observe that when models pre-trained on the larger and more diverse ShareGPT4V dataset [7], the share recipe can significantly improve performance for all variants. Note that we partially fine-tune the vision encoder in the share recipe. This observation suggests that fine-tuning the vision encoder can improve the performance when using small-scale LLMs, which is contrary to the result in [27] that fine-tuning the vision encoder dramatically degrades performance when using standard LLMs. We conjecture that whether fine-tuning the vision encoders can improve performance depends on the size of the accompanied LLMs and the size of the training data, which is an interesting direction for further work.\n' +
      '\n' +
      'DiscussionAn intriguing observation is that when employing the share recipe, model variants with StableLM-2 and Phi-2 exhibit a significant decline in performance on POPE (indicating more hallucinations) while experiencing improvements on other benchmarks. Compared to the base recipe, we note that the share recipe significantly increases the number of trainable parameters during the pre-training stage,\n' +
      '\n' +
      'Figure 6: Preliminary exploration of connectors in TinyLLaVA. Utilizing CLIP as the vision encoder and TinyLlama as the small-scale LLM, we train TinyLLAVAs with the two different connectors on the LLaVA-1.5 dataset, respectively. The results indicate that MLP outperforms Resampler in overall performance.\n' +
      '\n' +
      'which may be a key factor contributing to these observed phenomena. From the above phenomena, we conjecture that model variants with smaller LLMs may require more trainable parameters to fit larger datasets well in the pre-training stage. Therefore, having more trainable parameters enables model variants with TinyLlama to achieve better results on ShareGPT4V. However, using more trainable parameters during pre-training may not be entirely benign for larger models. For instance, while model variants with StableLM-2 and Phi-2 generally exhibit improved performance, worse performance in handling hallucinations is also introduced.\n' +
      '\n' +
      '**Summary.** In this part, we observe that training model variants on larger and more diverse data enables them to achieve overall better performance. Besides, model variants with smaller LLMs may require more trainable parameters to decrease hallucinations, while for variants with larger LLMs, using more trainable parameters leads to more hallucinations.\n' +
      '\n' +
      '#### 4.2.3 Overall Comparison among Different LMMs.\n' +
      '\n' +
      '**Comparison among TinyLLaVA Variants.** Here, we thoroughly compare various variants of TinyLLaVA models (See more details in Table A1 of Appendix). For reference, we name the variants under three design axes: training recipe, vision encoder, and language model, and format their names as TinyLLaVA-{recipe name}-{vision encoder}-{language model}. For instance, TinyLLaVA-base-C-TL is interpreted as trained under the base recipe, with CLIP and TinyLlama as backbones. We find that smaller TinyLLaVA variants can achieve results comparable to their larger counterparts when using appropriate combinations of data and training recipes, as shown in Figure 9.\n' +
      '\n' +
      '**Comparison with other LMMs.** Finally, we compare our TinyLLaVA variants to the state-of-the-art LMMs as shown in Table 3. Our TinyLLaVA-share-Sig-Phi with 3.1B parameters achieves comprehensive superiority over LLaVA-1.5 [37] with 7B parameters. It is worth noting that TinyLLaVA-share-Sig-Phi achieves comparable results to MoE-LLaVA [35] on VQAv2 [17] with fewer parameters and outperforms it in terms of POPE accuracy [55]. These findings highlight the promising potential of thorough explorations into the design space of LMMs.\n' +
      '\n' +
      '**Summary.** In this part, we observe that smaller model vari\n' +
      '\n' +
      'Figure 8: Ablation of training recipes. We set CLIP as the vision encoder and train our model variants under the two training recipes. The titles of the subplots indicate the corresponding to the LLM backbones.\n' +
      '\n' +
      'Figure 7: Ablation of training datasets. We fix the vision encoder to CLIP and train our model variants with two datasets under the base recipe. The titles of the subplots indicate the corresponding to the LLM backbones.\n' +
      '\n' +
      'ants can achieve results comparable to their larger counterparts when using appropriate combinations of data and training recipes. Meanwhile, our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'We propose the TinyLLaVA framework, which provides a unified perspective in designing and analyzing the small-scale LMMs. In our experiments, while under the same settings larger models perform better than smaller ones, we prove that with a better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger ones. Using results from our ablations, we train our best model, TinyLLaVA-3.1B, which achieves better overall performance against existing 7B models. Our findings suggest that the design space of LMMs are vastly under-explored. We hope our findings can serve as baselines for future research in terms of data scaling, training setups, and model selections.\n' +
      '\n' +
      '**Acknowledgement** This work was partially supported by the National Key Research and Development Plan of China under Grant 2022ZD0116310, National Natural Science Foundation of China (Grant No. 62106012), the Fundamental Research Funds for the Central Universities.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c c c c|c c c c} \\hline \\hline Method & LLM & Size & Res. & \\multicolumn{3}{c|}{Image Question Answering} & \\multicolumn{3}{c}{Benchmark Toolkit} \\\\  & & & & VQA\\({}^{v2}\\) & GQA & SQA\\({}^{I}\\) & VQA\\({}^{T}\\) & MM-Vet & POPE & LLaVA-W & MME & MMB \\\\ \\hline I-9B [29] & L-7B & 9B & 224 & 50.9 & 38.4 & - & 25.9 & - & - & - & - & 48.2 \\\\ InstructBLIP [12] & V-7B & 8.2B & 224 & - & 49.2 & 60.5 & 50.1 & 26.2 & - & 60.9 & - & 36 \\\\ LLaVA-1.5 [37] & V-7B & 7B & 336 & 78.5\\({}^{*}\\) & 62.0\\({}^{*}\\) & 66.8 & 58.2 & 30.5 & 85.9 & 63.4 & **1510.7** & 64.3 \\\\ Qwen-VL [4] & Q-7B & 7B & 448 & 78.8\\({}^{*}\\) & 59.3\\({}^{*}\\) & 67.1 & **63.8** & - & - & - & - & 38.2 \\\\ MoE-LLaVA [35] & Phi2-2.7B & 3.9B & 336 & 77.6\\({}^{*}\\) & 61.4\\({}^{*}\\) & 68.5 & 51.4 & 34.3 & 86.3 & 94.1 & - & 65.5 \\\\ MoE-LLaVA [35] & Phi2-2.7B & 3.9B & 384 & **79.9\\({}^{*}\\)** & **62.6\\({}^{*}\\)** & **70.3** & 57.0 & **35.9** & 85.7 & **97.3** & - & 68.0 \\\\ LLaVA-Phi [63] & Phi2-2.7B & 3.0B & 336 & 71.4\\({}^{*}\\) & - & 68.4 & 48.6 & 28.9 & 85.0 & - & 1335.1 & 59.8 \\\\ MobileVLM [10] & ML-2.7B & 3.0B & 336 & - & 59.0\\({}^{*}\\) & 61.0 & 47.5 & - & 84.9 & - & 1288.9 & 59.6 \\\\ \\hline TinyLLaVA-share-C-Phi & Phi2-2.7B & 3.0B & 336 & 77.7\\({}^{*}\\) & 61.0\\({}^{*}\\) & 70.1 & 53.5 & 31.7 & 86.3 & 67.1 & 1437.3 & **68.3** \\\\ TinyLLaVA-share-Sig-Phi & Phi2-2.7B & 3.1B & 384 & **79.9\\({}^{*}\\)** & 62.0\\({}^{*}\\) & 69.1 & 59.1 & 32.0 & **86.4** & 75.8 & 1464.9 & 66.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Comparison with SOTA LMMs on image understanding benchmarks. “L”, “V”, “Q”, and “ML” respectively represent LlaMA, Vicuna, Qwen, and MobileVLM. Other abbreviations can be found in Table 1. \\(*\\) donates the training images of the datasets observed during training. The best and second best results are indicated by **boldface** and underline, respectively.\n' +
      '\n' +
      'Figure 9: Instances where TinyLLaVAs with smaller parameters outperform their counterpart with larger parameters.\n' +
      '\n' +
      '* [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _Advances in Neural Information Processing Systems_, 35:23716-23736, 2022.\n' +
      '* [3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. Technical report, 2023.\n' +
      '* [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.\n' +
      '* [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [6] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. _arXiv preprint arXiv:2310.09478_, 2023.\n' +
      '* [7] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.\n' +
      '* [8] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In _International Conference on Machine Learning_, pages 1931-1942. PMLR, 2021.\n' +
      '* [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(240):1-113, 2023.\n' +
      '* [10] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices. _arXiv preprint arXiv:2312.16886_, 2023.\n' +
      '* [11] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.\n' +
      '* [12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructublip: Towards general-purpose vision-language models with instruction tuning, 2023.\n' +
      '* [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. _ICLR_, 2021.\n' +
      '* [14] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. _arXiv preprint arXiv:2303.03378_, 2023.\n' +
      '* [15] Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and Anette Frank. Magma-multimodal augmentation of generative models through adapter-based finetuning. _arXiv preprint arXiv:2112.05253_, 2021.\n' +
      '* [16] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023.\n' +
      '* [17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017.\n' +
      '* [18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.\n' +
      '* [19] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '* [20][https://sharegpt.com/](https://sharegpt.com/). Sharegpt. 2023.\n' +
      '* [21] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. _Advances in Neural Information Processing Systems_, 36, 2023.\n' +
      '* [22] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6700-6709, 2019.\n' +
      '* [23] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. _Neural computation_, 3(1):79-87, 1991.\n' +
      '* [24] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In _International conference on machine learning_, pages 4651-4664. PMLR, 2021.\n' +
      '* [25] Dakota Mahan Carlos Riquelme Ruiz Jonathan Tow, Marco Bellagente. Stablelm: Stability ai language models. Technical report, 2023.\n' +
      '* [26] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* [27] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. _arXiv preprint arXiv:2402.07865_, 2024.\n' +
      '\n' +
      '* [28] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _International journal of computer vision_, 123:32-73, 2017.\n' +
      '* [29] Hugo Laureonon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddarth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. _Advances in Neural Information Processing Systems_, 36, 2023.\n' +
      '* [30] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. _arXiv preprint arXiv:2305.03726_, 2023.\n' +
      '* [31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [32] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, pages 12888-12900. PMLR, 2022.\n' +
      '* [33] Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. _arXiv preprint arXiv:2309.05463_, 2023.\n' +
      '* [34] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. _arXiv preprint arXiv:2311.06607_, 2023.\n' +
      '* [35] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. Moe-Ilava: Mixture of experts for large vision-language models. _arXiv preprint arXiv:2401.15947_, 2024.\n' +
      '* [36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, 2014.\n' +
      '* [37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.\n' +
      '* [38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2023.\n' +
      '* [39] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mnpbench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281_, 2023.\n' +
      '* [40] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In _The 36th Conference on Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [41] Senior Researcher Sebastien Bubeck Mojan Javaheripi. Ph-2: The surprising power of small language models. Technical report, 2023.\n' +
      '* [42] OpenAI. Chatgpt: Openai\'s gpt-based conversational agent. [https://openai.com/chatgpt](https://openai.com/chatgpt), 2022.\n' +
      '* [43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [45] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards yqa models that can read. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2019.\n' +
      '* [46] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. _arXiv preprint arXiv:2201.11990_, 2022.\n' +
      '* [47] Stability AI Language Team. Stable lm 2 1.6b.\n' +
      '* [48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [49] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. _Advances in Neural Information Processing Systems_, 34:200-212, 2021.\n' +
      '* [50] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. _arXiv preprint arXiv:2108.10904_, 2021.\n' +
      '* [51] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _Transactions on Machine Learning Research_, 2022.\n' +
      '* [52] Zi Lin Ying Sheng Zhanghao Wu Hao Zhang Lianmin Zheng Siyuan Zhuang Yonghao Zhuang Joseph E. Gonzalez Ion Stoica Wei-Lin Chiang, Zhuohan Li and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90Technical report, 2023.\n' +
      '* [53] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. Technical report, 2023.\n' +
      '* [54] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. _arXiv preprint arXiv:2304.14178_, 2023.\n' +
      '* [55] Kun Zhou Jinpeng Wang Wayne Xin Zhao Yifan Li, Yifan Du and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In _The 2023 Conference on Empirical Methods in Natural Language Processing_, 2023.\n' +
      '* [56] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. _arXiv preprint arXiv:2308.02490_, 2023.\n' +
      '* [57] Zhengqing Yuan, Zhaoxu Li, and Lichao Sun. Tinygpt-v: Efficient multimodal large language model via small backbones. _arXiv preprint arXiv:2312.16862_, 2023.\n' +
      '* [58] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 11975-11986, October 2023.\n' +
      '* [59] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. _arXiv preprint arXiv:2401.02385_, 2024.\n' +
      '* [60] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up visual instruction tuning. _arXiv preprint arXiv:2307.04087_, 2023.\n' +
      '* [61] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n' +
      '* [62] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '* [63] Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, and Jian Tang. Llava-phi: Efficient multi-modal assistant with small language model. _arXiv preprint arXiv:2401.02330_, 2024.\n' +
      '\n' +
      'A Brief Overviews of Evaluation Benchmark.\n' +
      '\n' +
      'Here, we provide a brief overview of the key aspects each benchmark focuses on when assessing model capabilities.\n' +
      '\n' +
      '\\(\\bullet\\) VQAv2 [17] contains image-question-answer tuples with images collected from the COCO dataset [36]. The test set of VQAv2 evaluates models\' capabilities in terms of visual recognition, visual grounding, spatial reasoning as well as language understanding.\n' +
      '\n' +
      '\\(\\bullet\\) GQA [22] collected its data according to the scene graph structure provided by the Visual Genome [28] dataset. The test set of GQA extensively evaluates models\' capabilities in terms of visual and compositional reasoning.\n' +
      '\n' +
      '\\(\\bullet\\) TextVQA [45] is an image question answering dataset that contains images with texts. The test set of TextVQA requires models to not only recognize textual information in the given images but also to reason over them.\n' +
      '\n' +
      '\\(\\bullet\\) ScienceQA-IMG [40] is a subset of the ScienceQA [40] benchmark that contains images. The benchmark contains scientific questions and answers collected from lectures and textbooks. During the evaluation, the model is prompted with questions, choices, and relevant contexts, and is asked to predict the correct answers. This benchmark mainly evaluates models\' capabilities in reasoning with respect to scientific knowledge.\n' +
      '\n' +
      '\\(\\bullet\\) POPE [55] benchmark is designed to evaluate the hallucination issues in LMMs. Its test samples incorporate positive and negative objects (non-existent objects), which require the model to not only recognize positive samples accurately but also correctly identify negative samples (measuring hallucination). It effectively assesses the model\'s ability to handle hallucinations.\n' +
      '\n' +
      '\\(\\bullet\\) MM-Vet [56] is a comprehensive benchmark that evaluates LMMs on complicated multimodal tasks. MM-Vet uses GPT-4 [1] to evaluate the outputs generated by LMMs. Its test set evaluates LMMs on six dimensions: visual recognition, spatial reason- ing, common knowledge deduction, language generation, visual math reasoning, and OCR recognition.\n' +
      '\n' +
      '\\(\\bullet\\) LLaVA-W benchmark includes 24 images and 60 questions, which are collected to evaluate LMMs\' capabilities in challenging tasks and generalizability in novel domains [38].\n' +
      '\n' +
      '\\(\\bullet\\) MME is a LMM evaluation benchmark that measures both perception and cognition abilities on a total of 14 subtasks [16]. This benchmark is automatically evaluated by GPT-4 [1].\n' +
      '\n' +
      '\\(\\bullet\\) MMBench is a LMM evaluation benchmark that comprehensively assess models\' capabilities across 20 dimensions [39]. This benchmark is automatically evaluated by ChatGPT [42].\n' +
      '\n' +
      '## Appendix B TinyLLaVA Variants.\n' +
      '\n' +
      'We show all TinyLLaVA variants in Table A1. The results suggest that enhancing overall performance is attainable through the application of larger models, diverse datasets, and meticulously crafted training recipes.\n' +
      '\n' +
      'Here, we provide some examples generated by our TinyLLaVA-3.1B.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>