<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#TinyLLaVA: 소규모 대규모 멀티모달 모델의 프레임워크\n' +
      '\n' +
      'Baichuan Zhou\\({}^{1}\\) Ying Hu\\({}^{2}\\) Xi Weng\\({}^{1}\\) Junlong Jia\\({}^{1}\\) Jie Luo\\({}^{1}\\) Xien Liu\\({}^{2}\\) Ji Wu\\({}^{2}\\) Lei Huang\\({}^{1}\\)\n' +
      '\n' +
      'SKLCCSE, 인공지능연구소, 베이항대학교, 베이징, 중국 베이징\n' +
      '\n' +
      '중국 칭화대학교 전자공학과\n' +
      '\n' +
      'Technical ReportCorresponding author: Lei Huang (_huanglei4@buaa.edu.cn_)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문에서는 LMM(Small-scale Large Multimodal Models)의 설계 및 분석에 있어 통일된 관점을 제공하는 TinyLLaVA 프레임워크를 제시한다. 다양한 비전 인코더, 연결 모듈, 언어 모델, 훈련 데이터 및 훈련 레시피에 대한 효과를 경험적으로 연구합니다. 우리의 광범위한 실험은 더 나은 훈련 레시피와 결합된 더 나은 데이터 품질, 더 작은 LMM이 더 큰 LMM에 비해 동등 수준의 성능을 일관되게 달성할 수 있음을 보여주었다. 우리의 프레임워크 하에서, 우리는 소규모 LMM 가족을 훈련시킨다. 우리의 최고의 모델인 TinyLLaVA-3.1B는 LLaVA-1.5 및 Qwen-VL과 같은 기존 7B 모델에 비해 더 나은 전체 성능을 달성한다. 우리는 우리의 연구 결과가 데이터 스케일링, 훈련 설정 및 모델 선택 측면에서 향후 연구의 기준이 되기를 바란다. 모델 가중치 및 코드는 공개됩니다.\n' +
      '\n' +
      '각주 1: [https://github.com/DLCV-BUAA/TinyLLaVABench](https://github.com/DLCV-BUAA/TinyLLaVABench)에서 이용 가능.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'AI 커뮤니티는 대규모 언어 모델(LLM)의 놀라운 능력을 목격했다. 스케일링 법칙[19, 26]이 지침과 비상 능력[51]으로 연구되고 있는 가운데, 최근 몇 년 동안 5,000억 매개 변수[9, 46] 이상의 가장 큰 밀집 언어 모델을 사용하여 모델 크기를 확장하는 경향이 있다. LLM에서 영감을 얻은 대형 멀티모달 모델(LMM)[3, 38, 49, 62]은 거인의 어깨에 서서 시각을 LLM과 정렬하여 멀티모달 인식을 획득[21]하므로 LLM에서 강력한 기능을 직접 계승할 수 있다. 이러한 시너지 효과로 인해 80B 매개변수가 있는 플라밍고[2], 562B 매개변수가 있는 PaLM-E[14]와 같이 방대한 양의 매개변수로 다양한 LMM이 출시되었다.\n' +
      '\n' +
      '모델 크기를 확장하는 것이 다양한 작업에 걸쳐 성능을 크게 향상시킬 수 있다는 사실에도 불구하고, 이러한 대규모 모델을 훈련하는 것은 값비싼 계산 리소스를 필요로 하며, 그 큰 크기는 감당할 수 없는 훈련/추론 예산으로 이어질 수 있으며, 이는 잘 자금을 지원하는 산업 및 조직에만 대한 연구 액세스를 제한한다. 실용적인 관점에서 소규모 모델에 초점을 맞춘 또 다른 작업 라인은 저렴한 비용과 효율적인 훈련 및 추론으로 인해 주목을 받아 자원이 제한된 학계의 기회를 열었다.\n' +
      '\n' +
      '이러한 맥락에서, LLM 커뮤니티는 이전의 더 큰 대응물에 비해 성능 저하 없이 7-B 버전 [48, 53] 및 3B 매개변수 [41, 59, 25] 아래의 작은 버전과 같은 상대적으로 더 작은 규모의 버전을 릴리스하기 시작한다. LLM의 추세에 따라 대형 멀티모달 모델은 OpenFlamingo[3] 및 LLaVA 시리즈[37, 38]와 같이 3B에서 15B 범위의 비교적 작은 LLM을 활용하여 모델이 소규모로 축소되는 유사한 변환을 경험했다. LMM에 대한 보다 최근의 노력은 작은 LLMs[57, 63], 희박한 MoE[35], 동결 또는 로라 튜닝 백본[49, 54]을 적용하는 측면에서 효율적인 훈련 및 배치를 위한 다양한 방법을 탐구했다.\n' +
      '\n' +
      '작은 규모의 LLM을 가진 대형 멀티모달 모델이 더 많은 연구자가 사용할 수 있도록 하는 반면, 현재 시도 [57, 63, 35]는 각 아키텍처 구성요소의 광범위한 디자인 선택, 교육 레시피, 광범위한 경관을 엿볼 수 있다.\n' +
      '\n' +
      '도 1: TinyLLaVA-3.1B vs. LLaVA-1.5-7B.\n' +
      '\n' +
      '학습 데이터 등의 규모입니다. 이 급증하는 분야에서 설계 옵션의 가변성과 기술의 다양성은 LMM을 설계하는 복잡성과 기존 방법의 공간을 이해하는 데 어려움을 초래한다. 이 연구에서는 소규모 LLM을 활용하는 환경에서 대규모 멀티모달 모델의 넓은 경관을 조사하여 다양한 접근법에 대한 철저한 경험적 분석을 제공하여 연구자와 실무자가 이 공간에서 탐색할 수 있도록 지원한다. 그 결과, 훈련 파이프라인뿐만 아니라 비전 인코더, 소규모 LLM 디코더, 중간 커넥터로 구성된 프레임워크인 TinyLLaVA를 제시한다.\n' +
      '\n' +
      '이 프레임워크를 기반으로 다양한 비전 인코더, 연결 모듈, 언어 모델, 훈련 데이터 및 훈련 레시피가 미치는 영향을 조사한다. 우리의 경험적 실험은 더 나은 훈련 레시피와 데이터의 품질로 더 작은 LMM이 더 큰 상대와 동등한 성능을 달성하여 연구 분야의 새로운 기준을 설정할 수 있음을 보여준다. 마지막으로 Phi-2[33], StableLM-2[47], TinyLlama[59]의 세 가지 언어 모델과 CLIP[44], SigLIP[58]의 두 가지 비전 인코더를 포함하는 소규모 LMM 패밀리를 제시한다. 우리의 가장 좋은 모델인 TinyLLaVA-3.1B는 LLaVA-1.5[37] 및 Qwen-VL[4]와 같은 기존의 7B 모델에 비해 더 나은 전체 성능을 달성한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '대규모 멀티모달 모델은 강력한 대규모 언어 모델(LLM)[5, 9, 48]과 비전 모델[44, 58]이 개발됨에 따라 대규모 멀티모달 모델(LMM)이 크게 개선되었다[1]. 초기 작품[8, 49, 50]은 비전 언어 학습에 자기회귀 LLM을 도입하는 것을 개척했다. 다음 연구는 시각적 신호를 조건부 정보로 보고 LLM을 효과적으로 활용하는 데 중점을 두었다[2, 15, 31]. 특히 플라밍고[2]는 LLM에 어댑터를 삽입하고 지각자와 같은 [24] 아키텍처를 활용하여 시각적 특징을 추출하는 것을 고려하고 비전 언어 수 샷 학습에서 인상적인 성능을 보여주었다. BLIP 모델[31, 32]은 VQA[17] 및 이미지 캡션[36]과 같은 비전 언어 작업에 대한 성능을 향상시키기 위해 데이터 필터링을 도입한다. 이 모델들은 훌륭한 시각 언어 능력을 보였지만 지시를 따르도록 훈련되지 않았기 때문에 제한된 제로 샷 능력만 가지고 있었다.\n' +
      '\n' +
      'LMM을 인간의 선호도에 더 잘 정렬하기 위해 LLaVA[38] 및 InstructBLIP[12]와 같은 최근 작업은 [11, 43]을 따르고 시각적 명령어 튜닝 데이터[30, 62]로 LMM을 미세 조정하며, 이는 LMM의 제로 샷 기능을 크게 향상시킨다. 이 작업 라인에 따라 훈련[7, 34] 동안 비전 인코더를 잠금 해제하고 고품질 시각적 지시 튜닝 데이터 세트[7, 37, 60]를 큐레이션하고 이미지 해상도를 확장[4, 6, 34]할 수 있는 가능성을 논의함으로써 성능을 더욱 향상시키기 위한 몇 가지 기술이 제기된다.\n' +
      '\n' +
      'LMM을 배포하는 소규모 LMM은 높은 계산 오버헤드가 필요하기 때문에 비용이 많이 든다. 계산 병목 현상은 수십억 개의 매개변수로 확장되는 경향이 있기 때문에 LLM에 의해 일반적으로 도입된다[48, 52]. 그러나 최근 Phi-2[33], TinyLlama[59], StableLM-2[47]와 같은 소규모 LLM은 합리적인 계산 예산을 유지하면서 인상적인 성능에 도달했다. 이러한 노력에 따라 다양한 작업[10, 35, 57, 63]이 소규모 LMM을 훈련하고 배치하는 방법을 탐구했다. 특히, TinyGPT-V[57]는 MiniGPT-4[62]에 이어 프로젝션 레이어를 미세 조정하고 LLM[61]을 Phi[33]으로 대체했으며, LLaVA-Phi[33]은 LLaVA-1.5의 절차를 따르고 LLM[52]을 Phi-2[33]으로 대체했으며, MoE-LLaVA[35]는 Mixture-of-Experts[23]을 LLaVA 아키텍처에 도입하고 덜 활성화된 파라미터를 사용하여 LLaVA-1.5로 경쟁적 성능에 도달했다.\n' +
      '\n' +
      '특정 소규모 LMM을 구축하고 훈련하는 데 중점을 둔 이러한 작업[10, 35, 57, 63]과 달리, 우리의 작업은 모델 선택, 훈련 레시피 및 데이터가 소규모 LMM의 모델 성능에 어떻게 기여하는지 통합 분석을 제공하는 것을 목표로 한다. 우리는 동시 작업[27]도 시각 조건 언어 모델의 통합 분석을 제공하지만 표준 LMM에 초점을 맞추고 소규모 LMM에 초점을 맞춘다는 점에 주목했다. 소규모 LMM에 대한 조사는 실험을 기반으로 표준과 다른 행동을 보여준다.\n' +
      '\n' +
      '## 3 TinyLLaVA 프레임워크\n' +
      '\n' +
      '이 섹션에서는 대규모 다중 모드 모델에 대해 소규모 LLM을 활용하는 데 중점을 둔 TinyLLaVA 프레임워크의 세부 사항을 설명한다. 우리의 TinyLLaVA 프레임워크는 LLaVA[38]의 설계를 따르지만 통합된 관점에서 모델 아키텍처 및 훈련 레시피의 변형을 더 잘 조사하기 위해 일반화한다.\n' +
      '\n' +
      '도 2: TinyLLaVA 프레임워크.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      'TinyLLaVA(그림 2)의 구조는 소형 LLM\\(F_{\\theta}\\), 비전 인코더\\(V_{\\varphi}\\) 및 커넥터\\(P_{\\phi}\\으로 구성되며, 여기서 \\(\\theta\\), \\(\\varphi\\) 및 \\(\\phi\\)은 각각 (학습 가능한) 매개변수이다. 이 아키텍처는 이미지와 텍스트 시퀀스의 쌍을 입력하고 텍스트 시퀀스를 출력하는 다양한 멀티모달 이해 작업을 모델링할 수 있다.\n' +
      '\n' +
      'Small-scale LLM.Small-scale LLM(F_{\\theta\\)은 \\(d\\)차원(text) 임베딩 공간에서 길이가 \\(N\\)인 벡터(\\{\\mathbf{h}_{i}\\}_{i=0}^{N-1}\\)의 시퀀스를 입력으로 하고, 그에 상응하는 다음 예측치 \\(\\{\\mathbf{h}_{i}\\}_{i=1}^{N}\\)을 출력한다. 토너마이저(\\&\\) 임베딩 모듈은 일반적으로 작은 크기의 LLM에 바인딩되며, 텍스트 입력 시퀀스\\(\\{\\mathbf{y}_{i}\\}_{i=0}^{N-1}\\)을 임베딩 공간에 매핑하고, 임베딩 공간으로부터 텍스트 출력 시퀀스\\(\\{\\mathbf{y}_{i}\\}_{i=1}^{N}\\)에 매핑한다.\n' +
      '\n' +
      '비전 인코더(V_{\\varphi}\\)는 영상(\\mathbf{X}\\)을 입력으로 하고, (시각적) 패치 특징(\\(\\mathbf{V}=\\{\\mathbf{v}_{j}\\in\\mathbbb{R}^{d_{x}}\\}_{i}=j}^{M}\\)의 시퀀스를 출력하며, 여기서 \\(\\mathbf{V}=V_{\\varphi}(\\mathbf{X})\\. 비전 인코더는 패치 피처들의 시퀀스를 직접 출력하는 비전 트랜스포머[13][44][58] 또는 그리드 피처들을 출력하고 이어서 패치 피처들을 획득하기 위한 재성형 동작이 뒤따르는 CNN들일 수 있다.\n' +
      '\n' +
      '커넥터(P_{\\phi}\\)는 시각적 패치 시퀀스(\\(\\{\\mathbf{v}_{j}\\}_{j=1}^{M}\\)를 텍스트 임베딩 공간(\\{\\mathbf{h}_{j}\\}_{j=1}^{M}\\)에 매핑한다. 여기서, \\(\\mathbf{h}_{j}=P_{\\phi}(\\mathbf{v}_{j}), j=1,...,M\\). 커넥터\\(P_{\\phi}\\)는 미리 훈련된 LLM과 비전 인코더 모두의 능력을 효과적으로 활용하기 위해 설계되었다.\n' +
      '\n' +
      '### Training Pipeline\n' +
      '\n' +
      'TinyLLaVA 트레이닝을 위한 데이터는 이미지-텍스트 쌍\\((\\mathbf{X},\\mathbf{Y})\\)으로 구성된다. 또한, 텍스트 시퀀스 \\(\\mathbf{Y}=(\\mathbf{Y}_{q}^{1},\\mathbf{Y}_{a}^{1},...,\\mathbf{Y}_{q}}, \\mathbf{Y}_{a}^{T}, \\mathbf{Y}_{a}^{T})는 다중 턴 대화의 형태로 구성되며, 여기서 \\(T\\)는 총 턴수, \\(\\mathbf{Y}_{q}^{t}\\)는 인간의 지시이고 \\(\\mathbf{Y}_{a}^{t}\\)는 해당 보조자의 응답 2단계로 구분된다.\n' +
      '\n' +
      '피쳐 정렬을 위한 사전 훈련.이 단계에서는 임베딩 공간에서 비전과 텍스트 정보를 더 잘 정렬하는 것을 목표로 한다. 따라서 본 논문에서는 기존의 다중턴 대화에서 얻을 수 있는 이미지-캡션 스타일 데이터 형식 \\((\\mathbf{X},\\mathbf{Y}_{a})\\)을 이용하는데, 여기서 \\(\\mathbf{X}\\)은 이미지이고 \\(\\mathbf{Y}_{a}\\)은 이미지의 응답(설명)이다. 길이가 \\(N_{a}\\)인 목표 응답 \\(Y_{a}=\\{\\mathbf{y}_{i}\\}_{i=1}^{N_{a}\\)이 주어지면, 우리는 이미지에 의해 조절된 \\(Y_{a}\\)의 생성 확률을 다음과 같이 계산한다.\n' +
      '\n' +
      '각주 2: 응답 예측을 위한 조건부 입력으로서 명령어에 병합될 수 있기 때문에 더 나은 가독성을 위해 시스템-마사지를 생략한다.\n' +
      '\n' +
      '\\[p(\\mathbf{Y}_{a}|\\mathbf{X})=\\prod_{i=1}^{N_{a}}F_{\\theta}(\\mathbf{y}_{i}|P_{ \\phi}\\circ V_{\\varphi}(\\mathbf{X})), \\tag{1}\\\n' +
      '\n' +
      '훈련 목표로서 로그 유사성을 최대화합니다:\n' +
      '\n' +
      '\\[\\max_{\\phi,\\theta^{{}^{\\prime},\\varphi^{{}^{\\prime}}\\sum_{i=1}^{N_{a}\\log F_{\\theta}(\\mathbf{y}_{i}|P_{\\phi}\\circ V_{\\varphi}(\\mathbf{X})),\\tag{2}\\tag{i}\n' +
      '\n' +
      '여기서 \\(\\theta^{}^{\\prime}\\)와 \\(\\varphi^{}^{\\prime}\\)은 각각 \\(\\theta\\)과 \\(\\varphi\\)의 부분집합이다. 우리의 프레임워크는 소규모 LLM을 사용할 때 커넥터만 잘 정렬할 수 없다는 점을 고려하여 사전 훈련 단계 동안 LLM 및 비전 인코더의 부분적으로 학습 가능한 매개변수를 조정할 수 있다.\n' +
      '\n' +
      'Supervised Fine-tuning.We use the image-text pair \\((\\mathbf{X},\\mathbf{Y})\\) in original form of multi-turn conversation. [\\(\\mathbb{A}\\)는 보조 응답에 속하는 모든 토큰의 집합인 _i.e._, \\(\\mathbb{A}=\\{\\mathbf{y}|\\mathbf{y}\\in\\mathbf{Y}_{a}^{t},\\ for\\ any\\t=1,...,T\\}\\을 나타낸다. 감독된 미세 조정 동안 훈련 목표로 보조자의 응답 로그 우도를 자동으로 최대화한다:\n' +
      '\n' +
      '\\mathbbb{I}(\\mathbf{y}_{i}\\in\\mathbbb{A})\\log F_{\\theta}(\\max_{\\phi,\\theta^{}^{\\prime},\\varphi^{i}|P_{\\phi}\\circ V_{\\varphi}(\\mathbf{X})), \\tag{3}\\\n' +
      '\n' +
      '여기서 \\(N\\)은 텍스트 시퀀스의 길이 \\(\\mathbf{Y}\\), \\(\\mathbb{I}(\\mathbf{y}_{i}\\in\\mathbb{A})\\)는 \\(\\mathbf{y}_{i}\\in\\mathbb{A}\\), 0이면 1과 같다. 또한 감독 미세 조정 단계에서 LLM 및 비전 인코더의 부분적으로 학습 가능한 매개변수를 조정할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c} \\hline \\hline Type & Name & Abb. & HF path & Size \\\\ \\hline \\multirow{3}{*}{Small-scale LLM} & TinyLlama & TL & TinyLlama/TinyLlama-1.1B-Chat-v1.0 & 1.1B \\\\  & StableLM-2 & SLM & stabilityai/stablelm-2-zephyr-1\\_6b & 1.6B \\\\  & Phi-2 & Phi & microsoft/phi-2 & 2.7B \\\\ \\hline \\hline \\multirow{2}{*}{Vison encoder} & CLIP & C & openai/clip-vit-large-patch14-336 & 0.3B \\\\  & SigLIP & Sig & google/siglip-so400m-patch14-384 & 0.4B \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 현재 실험에서 TinyLLaVA 프레임워크에 사용되는 소규모 LLM 및 비전 인코더. “ab.”는 축약된 모델명을 의미하며, 이는 TinyLLaVA 모델에 대한 명명 규약에 사용된다. “HF 경로” 이것은 우리가 허깅페이스에서 사용하고 있는 관련 모델의 미리 훈련된 가중치로의 경로를 나타낸다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '이 섹션에서는 TinyLLaVA 프레임워크를 기반으로 모델 아키텍처, 데이터 세트 및 훈련 레시피가 소규모 대규모 다중 모드 모델(LMM) 성능에 어떤 영향을 미치는지 조사하기 위한 포괄적인 실험을 수행한다.\n' +
      '\n' +
      '### Experimental Settings\n' +
      '\n' +
      '######4.1.1 모델 아키텍처\n' +
      '\n' +
      '우리는 TinyLLaVA 프레임워크에 따라 모델을 인스턴스화하기 위해 몇 가지 대표적인 소규모 LLM, 비전 인코더 및 커넥터를 선택한다.\n' +
      '\n' +
      '소규모 LLM 표 1은 LLM 선택을 보여준다. 우리는 TinyLlama (1.1B) [59], StableLM-2-1.6B (1.6B) [47], Phi-2 (2.7B) [33]의 세 가지 소규모 LLM을 선택한다. 이러한 선택은 현재 소규모 LLM의 포괄적인 매개변수 범위를 포함한다는 것을 발견했다.\n' +
      '\n' +
      '비전 인코더.우리는 비전 인코더로 CLIP-Large[44]를 선택합니다. 예비 실험을 통해 소규모 LLM과 결합된 SigLIP[58]이 CLIP보다 더 나은 성능을 제공하여 프레임워크에 통합한다는 것을 발견했다. 우리는 표 1과 같이 모델을 초기화하기 위해 비전 인코더와 소규모 LLM 모두에 대해 허깅페이스의 공식 체크포인트를 사용한다.\n' +
      '\n' +
      '커넥터.LLaVA-1.5 [37]에 이어 비전 인코더와 소규모 LLMs 사이의 커넥터로 GELU 활성화 [18]를 갖는 2층 다층 퍼셉트론(MLP)을 적용한다. 또한 [30]과 유사하게 구현된 리샘플러를 커넥터로 사용하려고 했습니다.\n' +
      '\n' +
      '######4.1.2 훈련 데이터 및 레시피\n' +
      '\n' +
      '트레이닝 데이터.우리는 LLaVA-1.5[37]와 ShareGPT4V[7]에서 제안된 두 가지 다른 트레이닝 데이터 세트를 선택하여 데이터 품질이 LMM의 성능에 어떤 영향을 미치는지 연구한다. 우리는 표 2에서 그들의 차이점을 요약한다.\n' +
      '\n' +
      'LLaVA-1.5-PT는 [37]에 설명된 바와 같이 558k 캡션으로 구성된다. LLaVA-1.5-SFT는 총 665k개의 시각적 명령어 튜닝 대화를 포함하며, 이는 학술 지향적 시각적 질문 응답(VQA)[17, 22, 45, 28] 샘플, LLaVA-Instruct[38] 및 ShareGPT[20]로부터의 명령어 튜닝 데이터의 조합이다.\n' +
      '\n' +
      'ShareGPT4V-PT[7]은 Share-Captioner[7]에 의해 생성된 1246k 캡션을 포함한다. ShareGPT4V-SFT 데이터셋은 LLaVA-1.5-SFT의 23K 상세 설명 데이터가 100K ShareGPT4V 데이터에서 무작위로 샘플링된 상세 캡션으로 대체되는 것을 제외하고는 LLaVA-1.5-SFT[37]와 유사하다.\n' +
      '\n' +
      '교육 레시피.[37][7]의 두 가지 기존 교육 레시피를 탐색하고 모델에 미치는 영향에 대해 연구합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline Dataset & Stage & Source & \\#Sample \\\\ \\hline \\multirow{2}{*}{LLaVA-1.5} & PT & LLaVA-1.5-558k & 558k \\\\  & ST & LLaVA-1.5-mix-665k & 665k \\\\ \\hline \\hline \\multirow{2}{*}{ShareGPT4V} & PT & ShareGPT4V-PT-1246k & 1246k \\\\  & ST & ShareGPT4V-mix-665k & 665k \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: TinyLLaVA 훈련에 사용되는 데이터셋" PT"와 "SFT"는 각각 사전 훈련과 감독 미세 조정이라는 두 단계의 훈련을 의미한다.\n' +
      '\n' +
      '그림 3: 두 조리법 사이의 주요 차이점. 기본 레시피에서는 비전 인코더와 소규모 LLM의 파라미터를 모두 동결하고 커넥터를 업데이트만 유지한다. 공유 레시피에서는 비전 인코더의 처음 12개 레이어를 동결하고 나머지 모델을 업데이트합니다. 또한 베이스의 미리 훈련된 카운터 부분에서 커넥터를 초기화합니다.\n' +
      '\n' +
      '변이체. 이들의 주요 구분은 그림 3에 요약되어 있다.\n' +
      '\n' +
      '첫 번째 레시피는 LLaVA-1.5 [37]에서 채택되었으며 기준 레시피 역할을 하는 **베이스**로 명명되었다. 사전 학습 시 커넥터\\(P_{\\phi}\\)만 업데이트하고 나머지 모델은 냉동 상태로 유지하고 학습률 1e-3, 배치 크기 256으로 한 에폭에 대한 모델을 튜닝한다. 지도 미세 조정 단계에서는 비전 인코더\\(V_{\\varphi}\\)을 냉동 상태로 유지하고 커넥터\\(P_{\\phi}\\)과 소규모 LLM\\(F_{\\theta}\\)을 모두 업데이트하며 학습률 2e-5, 배치 크기 128로 한 에폭에 대한 모델을 튜닝한다.\n' +
      '\n' +
      '우리는 ShareGPT4V[7]에 이어 두 번째 훈련 레시피 **share**를 설정한다. 공유 레시피를 사전 훈련하는 동안, 우리는 베이스의 사전 훈련된 상대방으로부터 커넥터를 초기화한다. 또한, 비전 인코더의 첫 번째 12개 레이어(V_{\\varphi}\\)를 동결한 상태에서 학습률 2e-5, 배치 크기 256으로 나머지 모델을 업데이트하며, 지도 미세 조정 설정은 기본 레시피와 동일하다.\n' +
      '\n' +
      '1.3 평가 벤치마크\n' +
      '\n' +
      'VQA-v2[17], GQA[22], ScienceQA-IMG[40], TextVQA[45]의 4가지 이미지 질문 응답 벤치마크와 POPE[55], MM-Vet[56], LLaVA-W[LLaVA-Bench-in-the-Wild][38], MME[16], MM-Bench[39]의 5가지 포괄적인 벤치마크에서 모델 변형을 평가한다. 우리는 모델 기능을 평가할 때 각 벤치마크의 주요 측면에 대한 간략한 개요를 제공한다(부록 A를 참조).\n' +
      '\n' +
      '그림 4: 소규모 LLM 백본의 절제. 기본 레시피에 따라 LLaVA-1.5 데이터 세트에서 표 1에 언급된 3개의 소규모 LLM 및 2개의 비전 인코더로 6개의 변형을 훈련한다. 서브플롯의 제목은 대응하는 비전 인코더를 나타낸다.\n' +
      '\n' +
      '그림 5: 비전 인코더의 제거. 이러한 결과는 그림 4에서 상속되며 서브플롯의 제목은 해당 소규모 LLM을 나타낸다.\n' +
      '\n' +
      '### Experimental Results\n' +
      '\n' +
      '모형건축의 효과에 관한 연구\n' +
      '\n' +
      '소규모 LLMs의 절제. 소규모 LLMs 백본에 대한 절제를 수행한다. 그 결과는 그림 4에 제시되어 있다. Phi-2[33]을 사용한 모델 변형은 다양한 구성 및 벤치마크 평가 전반에 걸쳐 예외적으로 잘 수행됨을 관찰할 수 있으며, 이는 Phi-2의 더 큰 매개변수에 기인할 수 있다. 특히, Phi-2 변형은 SQA-I[40]의 다른 변형보다 상당히 우수하며, 이는 교과서 데이터에 대한 집중적인 훈련에 기인할 수 있다. TinyLlama[59] 변이체는 가장 작은 모델이며 전체 성능이 약간 낮지만 StableLM-2[47] 변이체에 비해 POPE 정확도가 더 우수하다. 우리의 삭제는 더 큰 언어 모델이 기본 설정에서 성능을 향상시킨다는 것을 확인한다.\n' +
      '\n' +
      '비전 인코더의 절제.그림 4에 제시된 실험 결과에 따라 그림 5에 보여준다. SigLIP[58]을 사용한 모델 변형은 텍스트VQA[45] 및 LLaVA-W[38] 벤치마크에서 특히 분명한 CLIP[44]를 사용한 모델 변형에 비해 모델 성능이 상당히 향상된다는 점은 주목할 만하다. 우리가 사용한 SigLIP 변이체가 더 높은 입력 해상도(384 대 336)를 가지고 있다는 점에 주목하는 것이 필수적이다. 를 더 포함하는 시각적 토큰들(729 vs. 576) CLIP와 비교됩니다. 이러한 요인들은 세밀한 이미지 이해를 수행하기 위해 더 유익한 시각적 정보를 포함하는 SigLIP에 기여할 수 있다.\n' +
      '\n' +
      '커넥터의 예비 탐색 MLP와 리샘플러를 비교하여 커넥터에 대한 예비 통찰력을 제공합니다. 그림 6에 표시된 우리의 관찰은 리샘플러를 커넥터로 사용하면 MLP와 비교하여 유사한 매개변수 설정에서 성능이 저하된다는 것을 보여주며, 이는 이전 연구 결과와 일치한다[37]. 향후 연구에서 다양한 커넥터에 대한 추가 탐사를 기대한다.\n' +
      '\n' +
      '요약.이 부분에서 우리는 더 큰 LLM을 가진 모델 변형이 더 나은 전체 성능을 달성할 수 있음을 관찰한다. 또한, 비전 인코더로서 SigLIP[58]을 적용하는 것(더 높은 입력 해상도와 더 많은 시각적 토큰을 갖는 것)은 CLIP[44]에 비해 성능을 상당히 향상시킬 수 있다.\n' +
      '\n' +
      '###### 4.2.2 탐색적 데이터 혼합 및 교육 레시피\n' +
      '\n' +
      '데이터 혼합물의 절제.우리는 또한 다양한 데이터 혼합물의 영향을 보여주기 위해 기본 레시피에 따라 절제 실험을 수행한다. 그림 7의 결과는 기본 레시피에 따라 더 광범위하고 더 다양한 ShareGPT4V [7] 데이터 세트에서 사전 훈련되었을 때 소규모 LLM으로 TinyLlama [59]를 사용한 모델 변형이 LLaVA-1.5 데이터 세트 [37]에 비해 평가 성능의 전반적인 개선을 입증함을 나타낸다. 그러나 POPE[55]에서는 현저한 저하가 관찰된다. 대조적으로, StableLM-2 및 Phi-2를 사용한 모델 변형의 성능은 포괄적인 개선을 경험했다. 우리는 이것이 더 많은 양의 데이터에 적절하게 맞지 않고 부분적인 지식 저하와 더 많은 환각을 초래하는 TinyLlama[59]의 불충분한 매개변수 때문일 수 있다고 추측한다.\n' +
      '\n' +
      '교육 레시피의 절제.또한 다양한 교육 레시피의 영향을 탐구한다. 결과는 그림 8에 나와 있다. 우리는 모델이 더 크고 더 다양한 ShareGPT4V 데이터셋에서 사전 훈련될 때 [7] 공유 레시피가 모든 변형에 대해 성능을 크게 향상시킬 수 있음을 관찰한다. 공유 레시피에서 비전 인코더를 부분적으로 미세 조정합니다. 이러한 관찰은 비전 인코더를 미세 조정하는 것이 소규모 LLM을 사용할 때 성능을 향상시킬 수 있음을 시사하며, 이는 표준 LLM을 사용할 때 비전 인코더를 미세 조정하는 것이 성능을 극적으로 저하시킨다는 [27]의 결과와 반대이다. 비전 인코더의 미세 조정이 성능을 향상시킬 수 있는지 여부는 동반된 LLM의 크기와 훈련 데이터의 크기에 따라 달라지며, 이는 추가 작업에 흥미로운 방향이라고 추측한다.\n' +
      '\n' +
      '토론 흥미로운 관찰은 공유 레시피를 사용할 때 StableLM-2 및 Phi-2를 사용한 모델 변형이 다른 벤치마크에서 개선을 경험하는 동안 POPE(더 많은 환각을 나타내는)에서 상당한 성능 감소를 보인다는 것이다. 베이스 레시피와 비교하여, 우리는 공유 레시피가 사전 트레이닝 단계 동안 트레이닝가능한 파라미터의 수를 상당히 증가시킨다는 것에 주목하고,\n' +
      '\n' +
      '그림 6: TinyLLaVA의 커넥터에 대한 예비 탐색. CLIP를 비전 인코더로 사용하고 TinyLlama를 소규모 LLM으로 사용하여 LLaVA-1.5 데이터 세트에 각각 두 개의 서로 다른 커넥터로 TinyLLAVA를 훈련한다. 결과는 MLP가 전체 성능에서 Resampler보다 우수하다는 것을 나타낸다.\n' +
      '\n' +
      '이는 이러한 관찰된 현상에 기여하는 핵심 요인일 수 있다. 위의 현상으로부터 우리는 더 작은 LLM을 가진 모델 변형들이 더 큰 데이터 세트를 사전 훈련 단계에서 잘 맞추기 위해 더 많은 훈련 가능한 파라미터를 필요로 할 수 있다고 추측한다. 따라서 더 많은 훈련 가능한 매개변수를 갖는 것은 TinyLlama를 가진 모델 변형이 ShareGPT4V에서 더 나은 결과를 얻을 수 있게 한다. 그러나 사전 훈련 중에 더 많은 훈련 가능한 매개변수를 사용하는 것이 더 큰 모델에 대해 완전히 양성인 것은 아닐 수 있다. 예를 들어, StableLM-2 및 Phi-2를 갖는 모델 변이체는 일반적으로 개선된 성능을 나타내지만, 환각을 처리하는 데 있어 더 나쁜 성능도 도입된다.\n' +
      '\n' +
      '**요약.** 이 부분에서 더 크고 다양한 데이터에 대한 학습 모델 변형이 전반적으로 더 나은 성능을 달성할 수 있음을 관찰한다. 게다가, 더 작은 LLM을 갖는 모델 변형들은 환각을 감소시키기 위해 더 많은 훈련가능한 파라미터들을 필요로 할 수 있는 반면, 더 큰 LLM을 갖는 변형들의 경우, 더 많은 훈련가능한 파라미터들을 사용하는 것은 더 많은 환각을 유도한다.\n' +
      '\n' +
      '####4.2.3 LMM간 비교\n' +
      '\n' +
      '**TinyLLaVA 변종 간의 비교.** 여기서, 우리는 TinyLLaVA 모델의 다양한 변종을 철저히 비교한다(부록의 표 A1에서 더 자세한 내용을 참조). 참고로, 학습 레시피, 비전 인코더, 언어 모델의 세 가지 설계 축에 따라 변형을 명명하고, 그 이름을 TinyLLaVA-{recipe name}-{vision encoder}-{language model}로 포맷한다. 예를 들어, TinyLLaVA-base-C-TL은 CLIP 및 TinyLlama를 백본으로 사용하여 기본 레시피에 따라 훈련된 것으로 해석된다. 우리는 그림 9와 같이 더 작은 TinyLLaVA 변이체가 데이터와 훈련 레시피의 적절한 조합을 사용할 때 더 큰 변이체와 유사한 결과를 얻을 수 있음을 발견했다.\n' +
      '\n' +
      '**다른 LMM과의 비교.** 마지막으로, 표 3에 나타낸 바와 같이 우리의 TinyLLaVA 변이체를 최신 LMM과 비교한다. 3.1B 파라미터를 갖는 우리의 TinyLLaVA-공유-Sig-Phi는 7B 파라미터를 갖는 LLaVA-1.5[37]보다 포괄적인 우월성을 달성한다. TinyLLaVA-share-Sig-Phi는 더 적은 파라미터로 VQAv2[17]에서 MoE-LLaVA[35]와 유사한 결과를 달성하고 POPE 정확도[55] 측면에서 이를 능가한다는 점에 주목할 필요가 있다. 이러한 발견은 LMM의 설계 공간에 대한 철저한 탐사의 유망한 잠재력을 강조한다.\n' +
      '\n' +
      '**요약** 이 부분에서 우리는 더 작은 모델 vari를 관찰한다\n' +
      '\n' +
      '도 8: 트레이닝 레시피들의 절제. 우리는 CLIP를 비전 인코더로 설정하고 두 가지 훈련 레시피에 따라 모델 변형을 훈련한다. 하위 플롯의 제목은 LLM 백본에 해당하는 것을 나타냅니다.\n' +
      '\n' +
      '도 7: 트레이닝 데이터세트들의 절제. 우리는 비전 인코더를 CLIP에 고정하고 기본 레시피에 따라 두 개의 데이터 세트로 모델 변형을 훈련한다. 하위 플롯의 제목은 LLM 백본에 해당하는 것을 나타냅니다.\n' +
      '\n' +
      '데이터 및 훈련 레시피의 적절한 조합을 사용할 때 더 큰 대응물에 필적하는 결과를 얻을 수 있다. 한편, 우리의 가장 좋은 모델인 TinyLLaVA-3.1B는 LLaVA-1.5 및 Qwen-VL과 같은 기존의 7B 모델에 비해 더 나은 전체 성능을 달성한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 소규모 LMM을 설계하고 분석하는 데 있어 통일된 관점을 제공하는 TinyLLaVA 프레임워크를 제안한다. 실험에서 동일한 설정에서 더 큰 모델이 더 작은 모델보다 더 나은 성능을 발휘하지만 더 나은 훈련 레시피와 결합된 더 나은 데이터 품질로 더 작은 LMM이 더 큰 모델에 비해 일관되게 동등 수준의 성능을 달성할 수 있음을 입증한다. 본 논문에서 제안하는 TinyLLaVA-3.1B 모델은 기존의 7B 모델보다 우수한 성능을 보인다. 본 연구 결과는 LMM의 설계 공간이 매우 미개척되어 있음을 시사한다. 우리의 연구 결과가 데이터 스케일링, 훈련 설정 및 모델 선택 측면에서 향후 연구의 기준이 될 수 있기를 바란다.\n' +
      '\n' +
      '**인정** 이 작업은 중앙대학 기초연구기금인 중국 국립자연과학재단(제62106012호) 2022ZD0116310호 보조금 아래 중국의 국가핵심연구개발계획에 의해 부분적으로 지원되었다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c c c c|c c c c} \\hline \\hline Method & LLM & Size & Res. & \\multicolumn{3}{c|}{Image Question Answering} & \\multicolumn{3}{c}{Benchmark Toolkit} \\\\  & & & & VQA\\({}^{v2}\\) & GQA & SQA\\({}^{I}\\) & VQA\\({}^{T}\\) & MM-Vet & POPE & LLaVA-W & MME & MMB \\\\ \\hline I-9B [29] & L-7B & 9B & 224 & 50.9 & 38.4 & - & 25.9 & - & - & - & - & 48.2 \\\\ InstructBLIP [12] & V-7B & 8.2B & 224 & - & 49.2 & 60.5 & 50.1 & 26.2 & - & 60.9 & - & 36 \\\\ LLaVA-1.5 [37] & V-7B & 7B & 336 & 78.5\\({}^{*}\\) & 62.0\\({}^{*}\\) & 66.8 & 58.2 & 30.5 & 85.9 & 63.4 & **1510.7** & 64.3 \\\\ Qwen-VL [4] & Q-7B & 7B & 448 & 78.8\\({}^{*}\\) & 59.3\\({}^{*}\\) & 67.1 & **63.8** & - & - & - & - & 38.2 \\\\ MoE-LLaVA [35] & Phi2-2.7B & 3.9B & 336 & 77.6\\({}^{*}\\) & 61.4\\({}^{*}\\) & 68.5 & 51.4 & 34.3 & 86.3 & 94.1 & - & 65.5 \\\\ MoE-LLaVA [35] & Phi2-2.7B & 3.9B & 384 & **79.9\\({}^{*}\\)** & **62.6\\({}^{*}\\)** & **70.3** & 57.0 & **35.9** & 85.7 & **97.3** & - & 68.0 \\\\ LLaVA-Phi [63] & Phi2-2.7B & 3.0B & 336 & 71.4\\({}^{*}\\) & - & 68.4 & 48.6 & 28.9 & 85.0 & - & 1335.1 & 59.8 \\\\ MobileVLM [10] & ML-2.7B & 3.0B & 336 & - & 59.0\\({}^{*}\\) & 61.0 & 47.5 & - & 84.9 & - & 1288.9 & 59.6 \\\\ \\hline TinyLLaVA-share-C-Phi & Phi2-2.7B & 3.0B & 336 & 77.7\\({}^{*}\\) & 61.0\\({}^{*}\\) & 70.1 & 53.5 & 31.7 & 86.3 & 67.1 & 1437.3 & **68.3** \\\\ TinyLLaVA-share-Sig-Phi & Phi2-2.7B & 3.1B & 384 & **79.9\\({}^{*}\\)** & 62.0\\({}^{*}\\) & 69.1 & 59.1 & 32.0 & **86.4** & 75.8 & 1464.9 & 66.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 이미지 이해 벤치마크에 대한 SOTA LMM과의 비교. "L", "V", "Q" 및 "ML"은 각각 LlaMA, Vicuna, Qwen 및 MobileVLM을 나타낸다. 다른 약어는 표 1에서 찾을 수 있다. \\(*\\)은 훈련 중에 관찰된 데이터 세트의 훈련 이미지를 기부한다. 가장 좋은 결과와 두 번째로 좋은 결과는 각각 **볼드체** 및 밑줄로 표시된다.\n' +
      '\n' +
      '그림 9: 더 작은 매개 변수를 가진 TinyLLaVAs가 더 큰 매개 변수를 가진 대응 변수를 능가하는 인스턴스.\n' +
      '\n' +
      '* [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _Advances in Neural Information Processing Systems_, 35:23716-23736, 2022.\n' +
      '* [3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. Technical report, 2023.\n' +
      '* [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.\n' +
      '* [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [6] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. _arXiv preprint arXiv:2310.09478_, 2023.\n' +
      '* [7] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.\n' +
      '* [8] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In _International Conference on Machine Learning_, pages 1931-1942. PMLR, 2021.\n' +
      '* [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(240):1-113, 2023.\n' +
      '* [10] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices. _arXiv preprint arXiv:2312.16886_, 2023.\n' +
      '* [11] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.\n' +
      '* [12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructublip: Towards general-purpose vision-language models with instruction tuning, 2023.\n' +
      '* [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. _ICLR_, 2021.\n' +
      '* [14] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. _arXiv preprint arXiv:2303.03378_, 2023.\n' +
      '* [15] Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and Anette Frank. Magma-multimodal augmentation of generative models through adapter-based finetuning. _arXiv preprint arXiv:2112.05253_, 2021.\n' +
      '* [16] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023.\n' +
      '* [17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017.\n' +
      '* [18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.\n' +
      '* [19] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '*[20][https://sharegpt.com/](https://sharegpt.com/) 샤렉트 2023년\n' +
      '* [21] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. _Advances in Neural Information Processing Systems_, 36, 2023.\n' +
      '* [22] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6700-6709, 2019.\n' +
      '* [23] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. _Neural computation_, 3(1):79-87, 1991.\n' +
      '* [24] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In _International conference on machine learning_, pages 4651-4664. PMLR, 2021.\n' +
      '* [25] Dakota Mahan Carlos Riquelme Ruiz Jonathan Tow, Marco Bellagente. Stablelm: Stability ai language models. Technical report, 2023.\n' +
      '* [26] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* [27] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. _arXiv preprint arXiv:2402.07865_, 2024.\n' +
      '\n' +
      '* [28] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _International journal of computer vision_, 123:32-73, 2017.\n' +
      '* [29] Hugo Laureonon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddarth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. _Advances in Neural Information Processing Systems_, 36, 2023.\n' +
      '* [30] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. _arXiv preprint arXiv:2305.03726_, 2023.\n' +
      '* [31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [32] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, pages 12888-12900. PMLR, 2022.\n' +
      '* [33] Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. _arXiv preprint arXiv:2309.05463_, 2023.\n' +
      '* [34] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. _arXiv preprint arXiv:2311.06607_, 2023.\n' +
      '* [35] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. Moe-Ilava: Mixture of experts for large vision-language models. _arXiv preprint arXiv:2401.15947_, 2024.\n' +
      '* [36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, 2014.\n' +
      '* [37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.\n' +
      '* [38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2023.\n' +
      '* [39] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mnpbench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281_, 2023.\n' +
      '* [40] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In _The 36th Conference on Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [41] Senior Researcher Sebastien Bubeck Mojan Javaheripi. Ph-2: The surprising power of small language models. Technical report, 2023.\n' +
      '* [42] OpenAI. Chatgpt: Openai\'s gpt-based conversational agent. [https://openai.com/chatgpt](https://openai.com/chatgpt), 2022.\n' +
      '* [43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [45] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards yqa models that can read. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2019.\n' +
      '* [46] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. _arXiv preprint arXiv:2201.11990_, 2022.\n' +
      '* [47] Stability AI Language Team. Stable lm 2 1.6b.\n' +
      '* [48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [49] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. _Advances in Neural Information Processing Systems_, 34:200-212, 2021.\n' +
      '* [50] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. _arXiv preprint arXiv:2108.10904_, 2021.\n' +
      '* [51] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _Transactions on Machine Learning Research_, 2022.\n' +
      '* [52] Zi Lin Ying Sheng Zhanghao Wu Hao Zhang Lianmin Zheng Siyuan Zhuang Yonghao Zhuang Joseph E. Gonzalez Ion Stoica Wei-Lin Chiang, Zhuohan Li and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90Technical report, 2023.\n' +
      '* [53] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. Technical report, 2023.\n' +
      '* [54] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. _arXiv preprint arXiv:2304.14178_, 2023.\n' +
      '* [55] Kun Zhou Jinpeng Wang Wayne Xin Zhao Yifan Li, Yifan Du and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In _The 2023 Conference on Empirical Methods in Natural Language Processing_, 2023.\n' +
      '* [56] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. _arXiv preprint arXiv:2308.02490_, 2023.\n' +
      '* [57] Zhengqing Yuan, Zhaoxu Li, and Lichao Sun. Tinygpt-v: Efficient multimodal large language model via small backbones. _arXiv preprint arXiv:2312.16862_, 2023.\n' +
      '* [58] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 11975-11986, October 2023.\n' +
      '* [59] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. _arXiv preprint arXiv:2401.02385_, 2024.\n' +
      '* [60] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up visual instruction tuning. _arXiv preprint arXiv:2307.04087_, 2023.\n' +
      '* [61] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n' +
      '* [62] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '* [63] Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, and Jian Tang. Llava-phi: Efficient multi-modal assistant with small language model. _arXiv preprint arXiv:2401.02330_, 2024.\n' +
      '\n' +
      '평가 벤치마크 개요\n' +
      '\n' +
      '여기서는 각 벤치마크가 모델 기능을 평가할 때 중점을 두는 주요 측면에 대한 간략한 개요를 제공합니다.\n' +
      '\n' +
      '(\\bullet\\) VQAv2 [17]은 COCO 데이터 세트 [36]에서 수집된 이미지와 함께 이미지-질문-답변 튜플을 포함한다. VQAv2의 테스트 세트는 언어 이해뿐만 아니라 시각적 인식, 시각적 접지, 공간 추론 측면에서 모델의 능력을 평가한다.\n' +
      '\n' +
      'GQA[22]는 Visual Genome[28] 데이터셋에서 제공하는 장면 그래프 구조에 따라 데이터를 수집하였다. GQA의 테스트 세트는 시각적 및 구성 추론 측면에서 모델의 능력을 광범위하게 평가한다.\n' +
      '\n' +
      'TextVQA[45]는 텍스트가 있는 이미지를 포함하는 이미지 질의 응답 데이터 세트이다. TextVQA의 테스트 집합은 주어진 이미지에서 텍스트 정보를 인식할 뿐만 아니라 추론할 수 있는 모델을 요구한다.\n' +
      '\n' +
      '(\\(\\bullet\\) ScienceQA-IMG[40]은 이미지를 포함하는 ScienceQA[40] 벤치마크의 하위 집합이다. 벤치마크에는 강의와 교과서에서 수집한 과학적 질문과 답변이 포함되어 있다. 평가하는 동안 모델은 질문, 선택 및 관련 컨텍스트로 프롬프트되고 정답을 예측하도록 요청된다. 이 벤치마크는 주로 과학적 지식과 관련하여 추론에 대한 모델의 능력을 평가한다.\n' +
      '\n' +
      'LMM의 환각 문제를 평가하기 위해 고안된 POPE[55] 벤치마크이다. 테스트 샘플은 양성 및 음성 객체(존재하지 않는 객체)를 통합하므로 모델이 양성 샘플을 정확하게 인식할 뿐만 아니라 음성 샘플을 올바르게 식별해야 한다(환각 측정). 그것은 환각을 다루는 모델의 능력을 효과적으로 평가한다.\n' +
      '\n' +
      'MM-Vet[56]은 복잡한 멀티모달 태스크에 대한 LMM을 평가하는 포괄적인 벤치마크이다. MM-Vet는 GPT-4 [1]을 사용하여 LMM에 의해 생성된 출력을 평가한다. 테스트 세트는 시각적 인식, 공간적 이유-잉, 공통 지식 추론, 언어 생성, 시각적 수학 추론 및 OCR 인식의 6가지 차원으로 LMM을 평가한다.\n' +
      '\n' +
      '(\\(\\bullet\\) LLaVA-W 벤치마크에는 24개의 이미지와 60개의 질문이 포함되어 있으며, 이는 새로운 도메인에서 도전적인 작업에 대한 LMM의 능력과 일반화 가능성을 평가하기 위해 수집된다[38].\n' +
      '\n' +
      '(\\(\\bullet\\) MME는 총 14개의 하위 과제에서 인지 능력과 인지 능력을 모두 측정하는 LMM 평가 벤치마크이다[16]. 이 벤치마크는 GPT-4 [1]에 의해 자동으로 평가된다.\n' +
      '\n' +
      '(\\(\\bullet\\) MMBench는 20개 차원에 걸쳐 모델의 기능을 종합적으로 평가하는 LMM 평가 벤치마크이다[39]. 이 벤치마크는 ChatGPT[42]에 의해 자동으로 평가된다.\n' +
      '\n' +
      '## 부록 B TinyLLaVA 변종.\n' +
      '\n' +
      '표 A1에서 모든 TinyLLaVA 변형을 보여주는데, 결과는 더 큰 모델, 다양한 데이터 세트 및 세심하게 조작된 훈련 레시피의 적용을 통해 전반적인 성능을 향상시킬 수 있음을 시사한다.\n' +
      '\n' +
      '여기에서 우리는 TinyLLaVA-3.1B에 의해 생성된 몇 가지 예를 제공한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>