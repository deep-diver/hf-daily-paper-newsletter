<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# API-Blend: A Comprehensive Corpora for Training and Benchmarking API LLMs\n' +
      '\n' +
      ' Kinjal Basu\\({}^{\\star}\\), Ibrahim Abdelaziz\\({}^{\\star}\\), Subhajit Chaudhury, Soham Dan,\n' +
      '\n' +
      '**Maxwell Crouse, Asim Munawar, Sadhana Kumararel, Vinod Muthusamy,**\n' +
      '\n' +
      '**Pavan Kapanipathi, and Luis A. Lastras**\n' +
      '\n' +
      'IBM Research\n' +
      '\n' +
      '{kinjal.basu, ibrahim.abdelaziz1, subhajit, soham.dan, maxwell.crouse, asim}@ibm.com\n' +
      '\n' +
      'sadhana.kumaravell@ibm.com, {vmuthus, kapanipa, lastrasl}@us.ibm.com\n' +
      '\n' +
      'These authors contributed equally to this work\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-Blend, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrate the utility of the API-Blend dataset for both training and benchmarking purposes1.\n' +
      '\n' +
      'Footnote 1: API-Blend data and evaluation code will be released soon.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large Language Models (LLMs) have shown remarkable abilities across a variety of Natural Language Understanding (NLU) tasks [15], e.g., text generation [1, 2], summarization [16, 2], and mathematical reasoning [14]. There has been strong recent interest in enabling LLMs to call APIs or external tools (such as calculators, calendars, or web searches [1, 15, 16]) to accomplish high level tasks like booking a hotel, reserving a table, and automating a job requisition tasks. These higher-level tasks are generally conversational and complex. However, in order to perform such complex tasks, LLMs should be able to perform simpler tasks with APIs such as (a) APIs detection: Based on a user query, correctly choose which API to call, (b) Slot filling2: Given the API, extract either the slots/parameters from the user utterances or request from the user more information to fill the required parameters of the detected API, and (c) Sequencing: Given an utterance specifying a task, write the sequence of APIs that needs to be called to accomplish the task.\n' +
      '\n' +
      'Footnote 2: In this paper, _slot_ and _input parameters_ are used interchangeably.\n' +
      '\n' +
      'Data for the above-mentioned API tasks, both for training and benchmarking LLMs has been scarce. Addressing this issue, in the recent past,\n' +
      '\n' +
      'Figure 1: _Top:_ an example from the API Bank-Level1 dataset (OOD) that showcases LLMaMA-2-7b fine-tuned with API-Blend generates the correct API and parameter, whereas the other models hallucinate. _Bottom:_ performance comparison among three models of similar sizes; two recent tool-augmented models (Lynx and ToolLLaMA-2-7B) and a LLaMA-2-7B model trained with API-Blend (API-Blend-LLaMA-2-7B), which significantly outperforms the other two models.\n' +
      '\n' +
      'most approaches have relied on synthetically generated data for training API-augmented LLMs. For instance, ToolLLM Qin et al. (2023) produces multi-sequence REST-API data sourced from GPT-4 (Achiam et al., 2023), while datasets like Gorilla Patil et al. (2023) utilize synthetic, single-sequence API data, specifically Deep Learning libraries\' APIs, generated from language models.\n' +
      '\n' +
      'Although generating synthetic data from LLMs offers a cost-effective means of obtaining substantial training data, they suffer from several drawbacks. First, data generation methods are plagued by critical issues such as bias inherent in the sampling model, and a lack of diversity in the training dataset. Previous work has shown synthetically generated data suffers from lack of diversity Gupta et al. (2023), and diverse data can improve out-of-domain (OOD) generalization Yu et al. (2022). Consequently, models trained on such data can overfit on in-distribution APIs and struggle to generalize to OOD APIs that were not encountered during training, restricting the broad applicability of LLMs for tool usage. In addition, datasets have primarily included API detection (single and multiple) and Slot filling in different settings, whereas Sequencing, a prominent task to perform higher-level human tasks using APIs has rarely been the focus in existing works. Lastly, datasets in domains such as digital assistants and semantic parsing that are related to API-tasks and are human-annotated have gone unnoticed in literature due to the emergence of synthetic data generation techniques;\n' +
      '\n' +
      'In light of the above issues, we have developed an API-focused training dataset that leverages a hybrid approach for data generation. This is built upon five human-annotated datasets with LLM-assisted generation comprising of over 150k, 17k, and 11k train, development, and test instances. The transformation primarily focuses on sequencing, including API detection and slot-filling due to the sparsity and importance of sequencing data for training models. Furthermore, these datasets are collected from different domains such as semantic parsing, dialog, and digital assistants resulting in a higher diversity of API data. We show that models trained on this diverse dataset yield significantly better OOD generalization performance compared to other state-of-the-art methods, with an example shown in Figure 1. As an evaluation/benchmark dataset, we include five different existing benchmarks for OOD evaluation. In conclusion, we release API-Blend, a comprehensive API training, and benchmarking dataset, comprising of 10 datasets (5 for training, and 5 for OOD testing), see Figure 2.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Tool-Usage by LLMs\n' +
      '\n' +
      'Many recent works Komeili et al. (2022); Thoppilan et al. (2022); Gao et al. (2023); Schick et al. (2023) have explored how to address the susceptibility of current LLMs to certain errors (e.g., arithmetic Patel et al. (2021)) through the use of external tools. Such tools can be called by an LLM to provide itself with access to up-to-date information Komeili et al. (2022); Schick et al. (2023), perform mathematical operations He-Yueya et al. (2023), and even execute formal programs Gao et al. (2023).\n' +
      '\n' +
      'Early approaches to general-purpose training of LLM tool-use leveraged large amounts of human-annotated data Komeili et al. (2022); Thoppilan et al. (2022). The difficulty in scaling these approaches was addressed by later works, which utilized self-supervision Schick et al. (2023); Parisi et al. (2022) and few-shot prompting Yao et al. (2022). The prompting framework of Yao et al. (2022) has become widely used when augmenting LLMs with tools, with many follow-up works exploring how to improve its cost-effectiveness Xu et al. (2023), performance Shinn et al. (2023); Yang et al. (2023), and data generation quality Qin et al. (2023); Tang et al. (2023).\n' +
      '\n' +
      'The utility of tool-calling itself has been explored with many standard benchmarks for question answering Saikh et al. (2022); Yang et al. (2018), mathematical reasoning Cobbe et al. (2021), machine translation Scarton et al. (2019); Lewis et al. (2020), and planning Shridhar et al. (2020). While useful to serve as a comparison against task-specific, supervised methods, it is unclear to what extent these datasets actually require the usage of tools. As observed by Zhuang et al. (2023), such benchmarks do not adequately distinguish be\n' +
      '\n' +
      'Figure 2: API-Blend Datasets: 10 datasets, 6 curated as part of this paper and 4 are off-the-shelf datasets used for out-of-domain testing.\n' +
      '\n' +
      ' tween problems that can be solved using only an LLM\'s internal knowledge and those that can only be solved through tool calls.\n' +
      '\n' +
      '### API Datasets\n' +
      '\n' +
      'The first self-supervised approaches to constructing tool-use datasets (Schick et al., 2023; Parisi et al., 2022) focused on a small set of general-purpose tools. Soon after, tool-use was quickly expanded to general API function calling (Qin et al., 2023; Tang et al., 2023; Patil et al., 2023), where the volume and diversity of APIs and scenarios were instead emphasized. While all of the aforementioned datasets highlight the number of APIs involved in their respective corpora, they each vary in terms of how those API calls are utilized. For instance, some datasets curate scenarios involving only a single API call (Tang et al., 2023; Patil et al., 2023; Xu et al., 2023b) while others involve multiple calls (Qin et al., 2023; Hao et al., 2023). In addition, some require actual calls to a real API to solve their problems (Qin et al., 2023; Li et al., 2023; Xu et al., 2023b), which contrasts with other works that simulate API calls with a prompted LLM (Tang et al., 2023; Patil et al., 2023).\n' +
      '\n' +
      'A limitation of the above-listed self-supervised corpora lies in the evaluation of API-use scenarios. Some approaches evaluate based on hallucination rate (Patil et al., 2023) while others rely on a separate LLM to assess the quality of an example (Tang et al., 2023; Qin et al., 2023). Recent works have focused on this issue, with Farn and Shin (2023) relying on smaller sets of manually collected ground truth annotations and Huang et al. (2023) performing manual inspection of generated data.\n' +
      '\n' +
      '## 3 API-Blend Dataset Curation\n' +
      '\n' +
      'We focus on the setting where the input is a single natural language utterance and the output is a sequence of API calls with their parameter names and values. API-Blend consists of datasets created via the following three approaches: (1) Language Model Assisted approach where prompts are used based on existing API outputs, (2) a grammar rule-based approach to convert existing semantic parsing and personal assistant notations into API data, and (3) off-the-shelf datasets. Table 1 depicts the statistics of each dataset and the details of the approach/dataset is below.\n' +
      '\n' +
      '### Language Model Assisted Generation\n' +
      '\n' +
      '**SeqSGD**: We created SeqSGD, a dataset based on Schema-Guided Dialogue (SGD) (Rastogi et al., 2020) dataset tailored towards API sequence evaluation. SGD contains about 20k annotated conversations between a human and a virtual assistant. These dialogues consist of engagements with various services and APIs across 20 domains, including banks, events, media, calendar, travel, and weather. To convert this dataset, for each conversation, we prompted a pretrained FLAN-T5-XXL3 model to convert each API into a request in natural language. We then append the corresponding text of each API to generate the summarized utterance. Figure 3 shows an example. We did not summarize the conversation itself, because it also contains API execution results, and using this as input to a summarization model resulted in many noisy details. To make sure the generated text captures all APIs and parameter values, we post-process the dataset to remove any examples where the utterance does not correctly reflect the ground truth APIs. As a result of this process, we generated 6.8K train, 1.1K validation, and 1.6K test examples having an average API count of 2.44 and an average parameters count per API of 3.5.\n' +
      '\n' +
      'Footnote 3: [https://huggingface.co/google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl)\n' +
      '\n' +
      '**SeqMultiWoz**: MultiWoz (Ye et al., 2021) is another multi-domain task-oriented dialogue dataset. Following the same process of curating SeqSGD from the SGD dataset, we created SeqMultiWoz, another API dataset based on MultiWoz. The resulting dataset includes about 6.8k train, 485 validation, and 1k test samples with an average API\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c} \\hline \\hline \\multirow{2}{*}{**Datasets**} & \\multirow{2}{*}{**Train**} & \\multirow{2}{*}{**Dev**} & \\multirow{2}{*}{**Test**} & \\multicolumn{2}{c}{**Avg. No.**} \\\\  & & & & **Len** & **Params** \\\\ \\hline \\multicolumn{1}{l|}{**SeqATIS**} & 11,670 & 694 & 774 & 2.13 & 4.85 \\\\ \\multicolumn{1}{l|}{**SeqSGD**} & 6,782 & 1,092 & 1,567 & 2.44 & 3.5 \\\\ \\multicolumn{1}{l|}{**SeqSNIPS**} & 39,750 & 2,198 & 2,199 & 1.96 & 5.06 \\\\ \\multicolumn{1}{l|}{**SeqMultiWoz**} & 6,816 & 485 & 983 & 2.36 & 3.67 \\\\ \\multicolumn{1}{l|}{**SeqTopV2**} & 94,458 & 13,477 & 6,095 & 1.2 & 1.98 \\\\ \\multicolumn{1}{l|}{**Total \\(\\times\\) 15/4/76**} & **17,946** & **15,618** & & \\\\ \\hline \\multicolumn{1}{l|}{**TopLLM-G1**} & - & - & 197 & 2.28 & - \\\\ \\multicolumn{1}{l|}{**TopLLM-G2**} & - & - & 197 & 2.55 & - \\\\ \\multicolumn{1}{l|}{**TopLLM-G3**} & - & - & 97 & 2.91 & - \\\\ \\multicolumn{1}{l|}{**API Bank-1**} & - & - & 386 & 1.65 & 2.25 \\\\ \\multicolumn{1}{l|}{**API Bank-2**} & - & - & 71 & 1.34 & 2.44 \\\\ \\multicolumn{1}{l|}{**TopLLM-H8**} & - & - & 100 & 7.01 & 0.86 \\\\ \\multicolumn{1}{l|}{**TopLendR-B**} & - & - & 120 & 9.45 & 0.89 \\\\ \\multicolumn{1}{l|}{**SeqTotalQA**} & - & - & 358 & 2.42 & 1.45 \\\\ \\multicolumn{1}{l|}{**TopLAlpaa**} & - & - & 211 & 1.38 & 2.01 \\\\ \\multicolumn{1}{l|}{**Total**} & - & - & **173** & - & - \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: API-Blend Datasets Statistics: datasets colored in red are used for training and in-domain testing, while the green ones are used for OOD testing only count of 2.36 and an average parameters count per API of 3.67.\n' +
      '\n' +
      '### Grammar-Based Generation\n' +
      '\n' +
      '**SeqATIS and SeqSNIPS**: ATIS [10] is a collection of human-annotated queries about flights. Queries ask for information such as flight numbers, airports, dates, and other relevant details. The dataset also provides a range of semantic labels, including intent and slot values. Intents are the overall goals of the queries, such as "flight query" or "airfare query" while slot values are the specific pieces of information that are being requested, such as "departure city" or "arrival time". SNIPS [14] is another dataset focused on voice assistants. It consists of human-annotated queries that cover various domains such as weather, music, and calendar.\n' +
      '\n' +
      'MixATIS and MixSNIPS are multi-intent datasets [15] built based on ATIS and SNIPS, respectively. It was created by collecting sentences from the ATIS/SNIPS dataset and connecting them with conjunctions, such as "and". The resulting data had sentences with 1-3 intents at a probability of 30%, 50%, and 20%, respectively.\n' +
      '\n' +
      'One issue with using these two datasets for API calling is that they do not indicate which parameters should be associated with which API. For example, as Figure 4 shows, the original MixSNIPS dataset only evaluates model\'s ability to detect the two gold intents and which segments of the text are the target slots. To convert MixATIS and MixSNIPS datasets to a sequence of API calls, we divided utterances back to their original single intent utterances to get the corresponding parameters for each API. We then parsed its IOB (Inside/Outside/Beginning) parameter notations to generate the list of API parameter names and values. Now, we merge the utterances back along with the APIs and their parameters to get the sequence of API calls. In this way, we have curated SeqATIS and SeqSNIPS from MixATIS and MixSNIPS, respectively. In SeqATIS, we have around 11.5k train, 700 validation, and 800 test examples having an average API sequence length of 2.13 and an average parameter count per API of 4.85. Whereas, SeqSNIPS consists of around 40k train, 2.2k validation, and 2.2k test samples with an average API\n' +
      '\n' +
      'Figure 4: Example of how SeqSNIPS is created. Using a natural language utterance from MixSNIPS and the flat list of slots, we convert it into a sequence of API calls, each with a dictionary of parameter names and values.\n' +
      '\n' +
      'Figure 3: Example of the creation process of seqSGD. Starting from the list of APIs, we use few-shot prompting to generate the summarized single utterance.\n' +
      '\n' +
      'sequence length of 1.96 and an average parameters count per API of 5.06.\n' +
      '\n' +
      '**SeqToolQA**: ToolQA Zhuang et al. (2023) is an open-source dataset designed for tool-augmented LLMs evaluation. The dataset tries to address the issue with current tool-based evaluation methods which do not distinguish between questions that can be answered using LLMs\' internal knowledge and those that require external information through tool use. To do so, the datasets come with 8 independant datasets (e.g. Kaggle Flights, Coffee, Yelp, Airbnb, etc.) and a set of questions that can be answered only by querying those datasets, hence ruling out the internal knowledge of LLMs. The dataset is provided as a set of template-based questions with their final answers.\n' +
      '\n' +
      'However, ToolQA does not provide the intermediate set of steps (tools) that need to be executed to generate the answer to the given questions. The listing below shows an example:\n' +
      '\n' +
      '```\n' +
      '{"idid":"easy-flight-0003","question":"WhatwasthedeparturetimeoftheQL891flightfromSEAtoLAXon2022-01-227","answer":"11:54" }\n' +
      '```\n' +
      '\n' +
      'Too address this issue, we propose SeqToolQA where we used the provided implementation of ToolQA on how each template question is answered and transformed into a corresponding set of APIs. We abstracted 17 different APIs covering 6 domains and created a total of 358 examples for testing purposes. We show an example below:\n' +
      '\n' +
      '```\n' +
      '{"id":"easy-flight-0000","question":"WhatwasthedeparturetimeoftheUAS480flightfromORDtoHSVon2022-07-067","apis":"Load08DB[DSName=flights]","Filter08[Origin=ORD,Dest=HSV,FlightDate=2822-07-06,Flight_Number_Marketing_Airline=5480,IATA_Code_Marketing_Airline=UA]","GetValue[ValueName=DepTime]","answer":"18:11" }\n' +
      '```\n' +
      '\n' +
      '**SeqTopV2**: Topv2 Chen et al. (2020) is a multi-domain task-oriented semantic parsing dataset comprising examples from eight domains; alarm, event, messaging, music, navigation, reminder, timer, and weather. The total dataset consists of 180k samples, randomly divided into training, development, and test sets for each domain. We followed a straightforward approach to convert this dataset into APIs using its intents "IN:" and slot "SL:" notations. Note that this dataset genuinely has a sequence of APIs that has to be followed. In the example "Remind me to email Joy about the details with the new client tonight", we transform it into two APIs; "SEND_MESSAGE" and "CREATE_REMINDER" where "CREATE_REMINDER" has prerequisite for "SEND_MESSAGE".\n' +
      '\n' +
      'The original dataset had a lot of "UNSUPPORTED" notations for examples where there is no matching intent. We excluded these cases from our API dataset. Along with them, we also removed the samples that had duplicate utterances, ambiguous intents, and analogous slot names inside the same intent. We call the resulting dataset SeqTopV2, and it has 94K, 13.5K, and 6K for training, development, and testing splits, respectively.\n' +
      '\n' +
      '### Off-the-Shelf Usage\n' +
      '\n' +
      '**ToolBench**: This is a subset of ToolBench Xu et al. (2023) focused on two domains, HomeSearch and Booking. We did not do any transformation to these datasets and rather used it "as-is", since they are already in API form.\n' +
      '\n' +
      '**ToolLLM** Qin et al. (2023) proposes an instruction-tuning tools dataset and a tool-augmented LLM model based on LLaMA-2-7B. The dataset is created synthetically based on Chat-GPT and a collection of 16K APIs from RapidAPI. The dataset include three subsets; G1,G2, G3 which refer to single-tool, intra-category multi-tool and intra-collection multi-tool data, respectively. We used those three subsets above as-is for out-of-distribution model evaluation.\n' +
      '\n' +
      '**API Bank** Li et al. (2023): is a benchmark designed for evaluating tool-augmented LLMs. It includes 314 tool-use dialogues with 753 API calls to assess the existing LLMs\' capabilities in planning, retrieving, and calling APIs. It also comes with a larger training dataset of 1.8K dialogues and a model trained on it (Lynx) initialized from Alpaca. In this paper, we use the the test sets of API Bank for out-of-distribution evaluation. Since this is a dialogue with tools being called in between, we divided each dialogue into multiple test examples where each example include a conversation and a sequence of APIs needed thus far.\n' +
      '\n' +
      '**ToolAlpaca**Tang et al. (2023): ToolAlpaca is a training and evaluation benchmark that is automatically curated through a multi-agent simulation environment using ChatGPT and GPT-3.5. The corpus contains 3,938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Although it comes with a training set,in this paper, we have only used the test set for out-of-distribution evaluation.\n' +
      '\n' +
      '## 4 Experiments and Results\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      'In our experiments, we have used 9 open sourced models as baselines: (1) LLaMA-2-70B (Touvron et al., 2023), (2) Falcon-180B (Almazrouei et al., 2023), (3) LLAMA-2-7B (Touvron et al., 2023), (4) FLAN-T5-XXL (Chung et al., 2022), (5) Falcon-40B (Almazrouei et al., 2023), (6) StarCoder-15B (Li et al., 2023c), (7) MPT-30B (Team et al., 2023), (8) ToolLLaMA-2-7B (Qin et al., 2023), and (9) Lynx-7B (Li et al., 2023b). We tested these models in three settings; (1) few shot testing: we evaluated LLAMA-2-70B, Falcon-180B, and ToolLLaMA-2-7B in a 3 shot mode; (2) Instruction fine-tuning on target dataset: we consider this setting for FLAN-T5-XXL, StarCoder-15B, Falcon-40B, and MPT-30B; and (3) Instruction fine-tuning on combined datasets: we evaluated this setting for all models in (2) along with LLaMA-2-7B to evaluate whether we can get a single model trained on the plurality of all datasets and still perform well on each individual test set. For the OOD experiments, we have used all the fine-tuned models from (3) in conjunction with the ToolLLaMA-2-7B and Lynx-7B which are already fine-tuned with the ToolLLM and APIBench data, respectively.\n' +
      '\n' +
      '### Instruction Tuning\n' +
      '\n' +
      'In all experiments, we have used the same instructions for training and testing. We show below the instruction template. Only when evaluating non-fine-tuned models or for the OOD experiments, we also provide 3 ICL examples via the part "Here are some examples: ICL_EXAMPLES" in the instruction) and remove it otherwise.\n' +
      '\n' +
      '```\n' +
      '##InstructionTemplatewithICLExamples### GiventheAPIsandSlotsbelow,sequencethemintheorderinwhichtheyhavetobecalledtoanswerfollowingquery.PossibleAPIs:(INTER_LIST) PossibleSlots:(SL0_LIST) Herearesomeexamples:(ICL_EXAMPLES) Query:(QUERY) Answer:\n' +
      '```\n' +
      '\n' +
      '### Settings and Parameters:\n' +
      '\n' +
      'We used QLoRA (Dettmers et al., 2023) to fine-tune all our models. While fine-tuning the models on targeted datasets, we made sure that the model saw 100k samples in the training process. In combined data training, we fine-tuned the models for 2 epochs over the cumulated datasets. In both cases, the batch size was \\(1\\) with gradient accumulation steps of \\(8\\) and a learning rate of \\(5e^{-5}\\).\n' +
      '\n' +
      '### Metrics\n' +
      '\n' +
      'To perform a fine-grained evaluation of the generated responses, we use two kinds of evaluation - standard information retrieval metrics (precision, recall, and f1 scores) and Longest Common Subsequence (LCS). We report F1 APIs and F1 slots/Parameters to compute the F1 scores by comparing the predicted APIs with the gold ones and the predicted parameters of each API with its gold counterparts. To also measure the model\'s ability to follow the sequence of API calls as dictated by the given natural language query, we also report LCS by comparing the gold sequence of API names and the sequence predicted by the model.\n' +
      '\n' +
      '### In-Distribution Evaluation Results\n' +
      '\n' +
      '**No Fine-tuning evaluation**: The first experiment we did was to check how the state-of-the-art open LLMs perform in such a setting. In particular, we evaluated LLaMA-2-70B and Falcon-180B using 3-shot prompting. We also considered ToolLLaMA-2-7B (Qin et al., 2023); a LLAMA-2-7B based model trained on API datasets generated using ChatGPT based on specifications from RapidAPIs. Table 2 shows the evaluation results on five in-distribution datasets: SeqATIS, SeqSNIPS, SeqSGD, SeqMultiWoz, and SeqTopV2. On all datasets, all three non-fine-tuned models seem to get some of the APIs correctly but fail to get the parameters to call such APIs.\n' +
      '\n' +
      '**Fine-tuning on One Dataset**: In this experiment, we fine-tune the baselines discussed above on each dataset and test it on the corresponding test split. We have evaluated four models here: FLAN-T5-XXL, StarCoder-15B, Falcon-40B, and MPT-30B. Ideally, this setting should give the best performance since the training data is focused towards one dataset and its APIs. As shown in Table 2, all models achieved very good performance (\\(>\\) 90%) detecting the right APIs with the performance of all models reaching 100% API-F1 scores for SeqMultiWoz dataset. We hypothesize that this high performance is because the number of APIs in most datasets is not very large (e.g., 12 APIs for SeqMultiWoz). Detecting the correct set of parameter names and values is a more challenging problem for most models with performance being the lowest for the SeqSGD dataset. The weighted average \n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      'ToolLAMA-2-7B and Lynx-7B are from Tool-LLM and API-Bank respectively, still they are performing poorly. In their papers, they used different metrics, e.g., pass rate to determine how many times the model reached an answer, and win rate which uses ChatGPT as a judge. In both the Tool-Bench datasets, Falcon-40B is outperforming the others. On SeqToolQA, even though the models have scored some points in API detection, however, all the models performed poorly on detecting the parameter names and values, which leads to a low Parameter-F1 score. This is because the parameter values in SeqToolQA contain codes, SQL, math functions, etc., which models have not seen in any training data, and these special slot values are not trivial to generate by seeing only a few ICL examples.\n' +
      '\n' +
      '### Qualitative Studies\n' +
      '\n' +
      'We also performed extensive studies on the outputs generated by the models in our experiments. In this section, we are going to discuss our findings on the failed cases along with some common mistakes demonstrated by the models. We found, in most of the cases, that parameters names and values are the obvious reason for not doing well on slot detection in both in and out-of-distribution test sets. We provide samples for each case for better understanding. We would like to mention that most of the provided examples contain "gold_output" and "predicted_output" and we have shown only the error part (sub-string) from the original outputs for brevity.\n' +
      '\n' +
      '#### 4.7.1 Unnormalized Slot-Values\n' +
      '\n' +
      'In an ideal scenario, the parameter values should be extracted exactly from the query by the models while generating the texts. However, sometimes, the model extracts the sub-part of it or represents it in a different form after extracting it. In a human evaluation, we would consider the generated text matches the gold, although while doing it programmatically it\'s showing a mismatch and we have not found a good way to normalize the parameter values. The following examples capture some of the unnormalized parameter value mismatches. In the first example, the month and the day in the predicted output are repeated. The predicted output on the second one contains only the city name, whereas the gold contains the city and the state. In the final example, even if the intent and slot values are correct, they have used different parameter formats to represent it. We plan to investigate further these issues, but we keep it for future work.\n' +
      '\n' +
      '```\n' +
      '1"""SeqSGD"""\n' +
      '2("gold":"March3rd,thisSunday","predicted":"March3rd,the3rd"\n' +
      '3("gold":"NYC,NewYork","predicted":"NYC")"""Toolbench#\n' +
      '4"""Toolbench#\n' +
      '5("gold":"Date(3,9,2023)","predicted":"Date(year=2023,month=3,day=9)")\n' +
      '```\n' +
      '\n' +
      '#### 4.7.2 Semantically similar slot-names in API Specification\n' +
      '\n' +
      'In our instructions, we provide the possible list of APIs and parameters to be used by the model while answering the queries. We extract these APIs and parameters from the original dataset\'s API specifications, if any. However, we found in some cases the parameter names are semantically very similar across the datasets. Here are some examples from the SeqSGD dataset: (1) _leaving_date_ and _departure_date_; (2) _leaving_time_ and _departure_time_; (3) _origin_, _from_city_, and _from_location_; and (4) _destination_, _to_city_, and _to_location_. Now, it often happens that the parameter values are correct in the generated text but the parameter names do not exactly match with the gold, even if they are very close. Following are some examples of such cases.\n' +
      '\n' +
      '```\n' +
      '1"""SeqSGD"""\n' +
      '2("gold":"destination_airport=ATL","predicted":"destination=ATL")\n' +
      '3("gold":"show_type=imax","predicted":"theater_name=imax")\n' +
      '4"""SeatXIS##\n' +
      '5("gold":"cuisine=souvlaki","predicted":"served_dish=souvlaki")\n' +
      '```\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'This paper presents API-Blend, a large corpora for training and evaluation of tool-augmented LLMs, curated from real-world datasets of different domains such as dialog, semantic parsing, digital assistants, etc. API-Blend consists of 10 datasets (5 in-distribution and 5 out-of-distribution) comprising over 190k instances (including train, development, and test). We have demonstrated that the models fine-tuned with API-Blend generalize better than the other tool-augmented LLMs on OOD experiments. Our findings not only substantiate the importance of API-Blend in training and benchmarking tool-augmented LLMs but also highlight the necessity of generalizability to improve the API usage capability of LLMs.\n' +
      '\n' +
      '## 6 Limitations and Risks\n' +
      '\n' +
      'A limitation of our benchmark, API-Blend, is that it does not deal with environment interactions for an API agent. In future work, it will be interesting to explore this setting of an embodied agent, where the API calls effect changes in the grounded environment. Further, our benchmark is focused on English API commands, and in the future, it will be interesting to develop a multilingual API-Blend. We do not perceive any risks associated with our work.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_.\n' +
      '* Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The Falcon series of open language models. _arXiv preprint arXiv:2311.16867_.\n' +
      '* Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901.\n' +
      '* Chen et al. (2020) Xilun Chen, Asish Ghoshal, Yashar Mehdad, Luke Zettlemoyer, and Sonal Gupta. 2020. Low-resource domain adaptation for compositional task-oriented semantic parsing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_. Association for Computational Linguistics.\n' +
      '* Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models.\n' +
      '* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_.\n' +
      '* Coucke et al. (2018) Alice Coucke, Alaa Saade, Adrien Ball, Theodore Bluche, Alexandre Caulier, David Leroy, Clement Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, et al. 2018. Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces. _arXiv preprint arXiv:1805.10190_.\n' +
      '* Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. _arXiv preprint arXiv:2305.14314_.\n' +
      '* Farn and Shin (2023) Nicholas Farn and Richard Shin. 2023. Tooltalk: Evaluating tool-usage in a conversational setting. _arXiv preprint arXiv:2311.10775_.\n' +
      '* Gao et al. (2023) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language models. In _International Conference on Machine Learning_, pages 10764-10799. PMLR.\n' +
      '* Gupta et al. (2023) Himanshu Gupta, Kevin Scaria, Ujjwala Anantheswaran, Shreyas Verma, Mihir Parmar, Saurabh Arjun Sawant, Swaroop Mishra, and Chitta Baral. 2023. Targen: Targeted data generation with large language models. _arXiv preprint arXiv:2310.17876_.\n' +
      '* Hao et al. (2023) Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2023. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. _arXiv preprint arXiv:2305.11554_.\n' +
      '* He-Yueya et al. (2023) Joy He-Yueya, Gabriel Poesia, Rose E Wang, and Noah D Goodman. 2023. Solving math word problems by combining language models with symbolic solvers. _arXiv preprint arXiv:2304.09102_.\n' +
      '* Hemphill et al. (1990) Charles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990. The ATIS spoken language systems pilot corpus. In _Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27,1990_.\n' +
      '* Huang et al. (2023) Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, et al. 2023. Metatool benchmark for large language models: Deciding whether to use tools and which to use. _arXiv preprint arXiv:2310.03128_.\n' +
      '* Imani et al. (2023) Shima Imani, Liang Du, and Harsh Shrivastava. 2023. Mathprompter: Mathematical reasoning using large language models. _arXiv preprint arXiv:2303.05398_.\n' +
      '* Komeili et al. (2022) Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2022. Internet-augmented dialogue generation. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8460-8478.\n' +
      '* Krizhevsky et al. (2012)Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2020. Mlqa: Evaluating cross-lingual extractive question answering. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 7315-7330.\n' +
      '* Li et al. (2023a) Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023a. Apibank: A benchmark for tool-augmented llms. _arXiv preprint arXiv:2304.08244_.\n' +
      '* Li et al. (2023b) Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023b. Apibank: A benchmark for tool-augmented llms.\n' +
      '* Li et al. (2023c) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023c. Starcode: may the source be with you! _arXiv preprint arXiv:2305.06161_.\n' +
      '* Min et al. (2023) Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023. Recent advances in natural language processing via large pre-trained language models: A survey. _ACM Computing Surveys_, 56(2):1-40.\n' +
      '* Parisi et al. (2022) Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm: Tool augmented language models. _arXiv preprint arXiv:2205.12255_.\n' +
      '* Patel et al. (2021) Arkil Patel, Satwik Bhattacharya, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2080-2094, Online. Association for Computational Linguistics.\n' +
      '* Patil et al. (2023) Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2023. Gorilla: Large language model connected with massive apis. _arXiv preprint arXiv:2305.15334_.\n' +
      '* Qin et al. (2020) Libo Qin, Xiao Xu, Wanxiang Che, and Ting Liu. 2020. Agif: An adaptive graph-interactive framework for joint multiple intent detection and slot filling. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 1807-1816.\n' +
      '* Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. ToollIM: Facilitating large language models to master 16000+ real-world apis. _arXiv preprint arXiv:2307.16789_.\n' +
      '* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9.\n' +
      '* Rastogi et al. (2020) Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 8689-8696.\n' +
      '* Saikh et al. (2022) Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. 2022. Scienceqa: A novel resource for question answering on scholarly articles. _International Journal on Digital Libraries_, 23(3):289-301.\n' +
      '* Scarton et al. (2019) Scarton Scarton, Mikel L Forcada, Miquel Espla-Gomis, and Lucia Specia. 2019. Estimating post-editing effort: a study on human judgements, task-based and reference-based metrics of mt quality. In _Proceedings of the 16th International Conference on Spoken Language Translation_.\n' +
      '* Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. _ArXiv_, abs/2302.04761.\n' +
      '* Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. In _Thirty-seventh Conference on Neural Information Processing Systems_.\n' +
      '* Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. In _International Conference on Learning Representations_.\n' +
      '* Tang et al. (2023a) Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. 2023a. Toolapaca: Generalized tool learning for language models with 3000 simulated cases.\n' +
      '* Tang et al. (2023b) Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. 2023b. Toolapaca: Generalized tool learning for language models with 3000 simulated cases.\n' +
      '* Team et al. (2023) MN Team et al. 2023. Introducing mpt-7b: a new standard for open-source, commercially usable llms.\n' +
      '* Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models, 2023. _URL https://arxiv. org/abs/2307.09288_.\n' +
      '* Xu et al. (2023a) Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu. 2023a. Rewoo: Decoupling reasoning from observations for efficient augmented language models. _arXiv preprint arXiv:2305.18323_.\n' +
      '\n' +
      'Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. 2023b. On the tool manipulation capability of open-source large language models. _arXiv preprint arXiv:2305.16504_.\n' +
      '* Yang et al. (2023) Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-react: Prompting chatgpt for multimodal reasoning and action. _arXiv preprint arXiv:2303.11381_.\n' +
      '* Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2369-2380.\n' +
      '* Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In _The Eleventh International Conference on Learning Representations_.\n' +
      '* Ye et al. (2021) Fanghua Ye, Jarana Manotumruksa, and Emine Yilmaz. 2021. Multiwoz 2.4: A multi-domain task-oriented dialogue dataset with essential annotation corrections to improve state tracking evaluation. _arXiv preprint arXiv:2104.00773_.\n' +
      '* Yu et al. (2022) Yu Yu, Shahram Khadivi, and Jia Xu. 2022. Can data diversity enhance learning generalization? In _Proceedings of the 29th international conference on computational linguistics_, pages 4933-4945.\n' +
      '* Zhang et al. (2020) Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In _International Conference on Machine Learning_, pages 11328-11339. PMLR.\n' +
      '* Zhuang et al. (2023) Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2023. Toolqa: A dataset for llm question answering with external tools. _arXiv preprint arXiv:2306.13304_.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>