<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# API-Blend: 훈련 및 벤치마킹 API LLMs를 위한 포괄적 말뭉치\n' +
      '\n' +
      ' 킨잘 바수({}^{\\star}\\), 이브라힘 압델라지즈({}^{\\star}\\), 수하지트 차우두리, 소함단.\n' +
      '\n' +
      '맥스웰 크라우스, 사다나 쿠마라렐, 비노드 무쓰사미\n' +
      '\n' +
      '파반 카파니파티와 루이스 A 라스트라스\n' +
      '\n' +
      'IBM Research\n' +
      '\n' +
      '{kinjal.basu, ibrahim.abdelaziz1, subhajit, soham.dan, maxwell.crouse, asim}@ibm.com\n' +
      '\n' +
      'sadhana.kumaravell@ibm.com, {vmuthus, kapanipa, lastrasl}@us.ibm.com\n' +
      '\n' +
      '이 작가들은 이 작업에 동등하게 기여했다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대용량 언어 모델(LLM)은 도구를 효과적으로 사용하고 외부 응용 프로그램 인터페이스(API)는 작업을 계획하고 완료할 필요가 증가하고 있다. 이와 같이, 툴/API로의 호출을 수반하는 충분한 양의 열차 및 테스트 데이터를 획득할 수 있는 방법에 엄청난 관심이 있다. 이 문제를 해결하기 위한 주요 전략으로 두 가지 연구 라인이 등장했다. 첫째는 합성 데이터 생성 기술에 초점을 맞춘 반면, 둘째는 API/도구 기반 작업으로 변환될 수 있는 작업 인접 데이터 세트를 큐레이팅하는 것을 포함했다. 본 논문에서는 기존의 데이터 세트를 식별하고, 큐레이션하고, 변환하는 작업에 초점을 맞추고, 도구 증강 LLM의 훈련 및 체계적인 테스트를 위한 대규모 말뭉치인 API-Blend를 소개한다. 데이터세트들은 API/도구 검출, 슬롯 충전, 및 검출된 API들의 시퀀싱과 같은 API-태스크들을 수반하는 실제-세계 시나리오들을 모방한다. 우리는 훈련 및 벤치마킹 목적1 모두에 대한 API-Blend 데이터 세트의 유용성을 보여준다.\n' +
      '\n' +
      '각주 1: API-Blend 데이터 및 평가 코드가 곧 공개될 예정입니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대규모 언어 모델(LLM)은 텍스트 생성[1, 2], 요약[16, 2] 및 수학적 추론[14]과 같은 다양한 자연 언어 이해(NLU) 작업[15]에서 놀라운 능력을 보여주었다. 최근 LLM이 호텔 예약, 테이블 예약, 작업 요청 작업 자동화와 같은 고급 작업을 수행하기 위해 API 또는 외부 도구(계산기, 달력 또는 웹 검색[1, 15, 16])를 호출할 수 있도록 하는 데 큰 관심이 있다. 이러한 상위 과제는 일반적으로 대화적이고 복잡하다. 그러나, 이러한 복잡한 태스크들을 수행하기 위해, LLMs들은 (a) API 검출: 사용자 질의에 기초하여, 호출할 API를 정확하게 선택할 수 있어야 하고, (b) Slot filling2: API가 주어지면, 사용자 발화들로부터 슬롯들/파라미터들 중 하나를 추출하거나, 검출된 API의 필요한 파라미터들을 채우기 위해 사용자로부터의 더 많은 정보를 요청하고, (c) Sequencing: 태스크를 지정하는 발언이 주어지면, 태스크를 달성하기 위해 호출될 필요가 있는 API들의 시퀀스를 기록해야 한다.\n' +
      '\n' +
      '각주 2: 본 논문에서는 _slot_와 _input parameter_를 혼용하여 사용한다.\n' +
      '\n' +
      'LLM을 훈련하고 벤치마킹하기 위한 위에서 언급한 API 작업에 대한 데이터는 부족했다. 최근 과거에 이 문제를 해결하는 것은\n' +
      '\n' +
      '도 1:_Top:_ API-Blend로 미세 조정된 LLMaMA-2-7b를 보여주는 API 뱅크-레벨1 데이터세트(OOD)로부터의 일례는 올바른 API 및 파라미터를 생성하는 반면, 다른 모델들은 환각을 나타낸다. _ Bottom:_ 유사한 크기의 세 가지 모델들; 두 개의 최근 툴-증강 모델들(Lynx 및 ToolLLaMA-2-7B) 및 API-Blend(API-Blend-LLaMA-2-7B)로 훈련된 LLaMA-2-7B 모델 사이의 성능 비교는 다른 두 모델들보다 상당히 우수하다.\n' +
      '\n' +
      '대부분의 접근법은 API-증강 LLM을 훈련하기 위해 합성적으로 생성된 데이터에 의존해 왔다. 예를 들어, ToolLLM Qin et al.(2023)은 GPT-4(Achiam et al., 2023)로부터 소싱된 다중 시퀀스 REST-API 데이터를 생성하는 반면, Gorilla Patil et al.(2023)과 같은 데이터 세트는 언어 모델로부터 생성된 합성, 단일 시퀀스 API 데이터, 구체적으로 Deep Learning 라이브러리의 API를 이용한다.\n' +
      '\n' +
      'LLM에서 합성 데이터를 생성하는 것은 상당한 학습 데이터를 얻는 비용 효율적인 수단을 제공하지만 몇 가지 단점을 겪는다. 첫째, 데이터 생성 방법은 샘플링 모델에 내재된 편향과 훈련 데이터 세트의 다양성 부족과 같은 중요한 문제로 인해 어려움을 겪는다. 이전 연구에서는 합성적으로 생성된 데이터가 Gupta 등(2023)의 다양성 부족으로 어려움을 겪고 있으며, 다양한 데이터가 OOD(Out-of-domain) 일반화 Yu 등(2022)을 개선할 수 있음을 보여주었다. 결과적으로, 이러한 데이터에 대해 훈련된 모델은 배포 내 API에 과도하게 적합할 수 있으며 훈련 중에 발생하지 않은 OOD API로 일반화하기 위해 어려움을 겪을 수 있으므로 도구 사용에 대한 LLM의 광범위한 적용 가능성을 제한한다. 또한, 데이터 세트에는 주로 API 탐지(단일 및 다중)와 슬롯 채우기가 다른 설정에서 포함되었지만, API를 사용하여 더 높은 수준의 인간 작업을 수행하는 두드러진 작업인 시퀀싱은 기존 작업에서 거의 초점이 되지 않았다. 마지막으로, API-작업과 관련되고 인간 주석이 부여된 디지털 어시스턴트 및 시맨틱 파싱과 같은 도메인의 데이터 세트는 합성 데이터 생성 기술의 출현으로 인해 문헌에서 주목받지 못했다.\n' +
      '\n' +
      '위의 문제에 비추어, 우리는 데이터 생성을 위해 하이브리드 접근법을 활용하는 API 중심의 훈련 데이터 세트를 개발했다. 이것은 150k, 17k 및 11k 트레인, 개발 및 테스트 인스턴스로 구성된 LLM 보조 세대가 있는 5개의 인간 주석 데이터 세트를 기반으로 한다. 변환은 주로 훈련 모델에 대한 시퀀싱 데이터의 희소성과 중요성으로 인해 API 탐지 및 슬롯 채우기를 포함한 시퀀싱에 중점을 둔다. 또한, 이러한 데이터 세트는 시맨틱 파싱, 다이얼로그 및 디지털 어시스턴트와 같은 상이한 도메인으로부터 수집되어 API 데이터의 더 높은 다양성을 초래한다. 이 다양한 데이터 세트에 대해 훈련된 모델은 그림 1과 같은 예를 통해 다른 최신 방법에 비해 훨씬 더 나은 OOD 일반화 성능을 산출함을 보여준다. 평가/벤치마크 데이터 세트로서 OOD 평가를 위한 5가지 다른 기존 벤치마크를 포함한다. 결론적으로, 우리는 포괄적인 API 훈련인 API-Blend와 10개의 데이터세트(훈련용 5개, OOD 테스트용 5개)로 구성된 벤치마킹 데이터세트를 릴리스한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### Tool-Usage by LLMs\n' +
      '\n' +
      '최근 많은 연구들 Komeili et al. (2022); Thoppilan et al. (2022); Gao et al. (2023); Schick et al. (2023)은 외부 툴들의 사용을 통해 특정 에러들(예를 들어, 산술 Patel et al. (2021))에 대한 현재의 LLM들의 민감성을 해결하는 방법을 탐색하였다. 이러한 툴들은 LLM에 의해 호출되어 최신의 정보 Komeili et al.(2022); Schick et al.(2023), 수학적 연산 He-Yueya et al.(2023), 심지어 공식 프로그램 Gao et al.(2023)에 대한 액세스를 스스로 제공할 수 있다.\n' +
      '\n' +
      'LLM 툴-사용의 범용 트레이닝에 대한 초기 접근법들은 대량의 인간-주석이 달린 데이터 Komeili et al.(2022); Thoppilan et al.(2022)을 이용한다. 이러한 접근법들의 스케일링의 어려움은 자기 감독 Schick et al. (2023); Parisi et al. (2022) 및 few-shot prompting Yao et al. (2022)을 이용한 후기 작업들에 의해 해결되었다. Yao et al. (2022)의 프롬프트 프레임워크는 LLM을 도구로 증강할 때 널리 사용되고 있으며, 많은 후속 작업들이 비용 효율성 Xu et al. (2023), 성능 Shinn et al. (2023), Yang et al. (2023), 데이터 생성 품질 Qin et al. (2023); Tang et al. (2023).\n' +
      '\n' +
      'Saikh et al. (2022); Yang et al. (2018), mathematical reasoning Cobbe et al. (2021), machine translation Scarton et al. (2019); Lewis et al. (2020), planning Shridhar et al. (2020). 작업별 감독 방법과 비교하는 데 유용하지만 이러한 데이터 세트가 실제로 도구의 사용을 어느 정도 필요로 하는지는 불분명하다. Zhuang et al.(2023)에 의해 관찰된 바와 같이, 그러한 벤치마크들은 적절하게 구별되지 않는다.\n' +
      '\n' +
      '그림 2: API-블렌드 데이터세트: 10개의 데이터세트, 본 논문의 일부로 큐레이션된 6개 및 4개는 도메인 외 테스트에 사용되는 기성 데이터세트이다.\n' +
      '\n' +
      ' LLM의 내부 지식만을 사용하여 해결할 수 있는 열세 가지 문제와 도구 호출을 통해서만 해결할 수 있는 문제.\n' +
      '\n' +
      '### API Datasets\n' +
      '\n' +
      '도구 사용 데이터 세트를 구성하기 위한 첫 번째 자체 감독 접근법(Schick et al., 2023; Parisi et al., 2022)은 작은 범용 도구 세트에 초점을 맞췄다. 곧이어 도구 사용이 일반 API 기능 호출(Qin et al., 2023; Tang et al., 2023; Patil et al., 2023)로 빠르게 확장되었으며, 여기서 API 및 시나리오의 볼륨 및 다양성이 대신 강조되었다. 앞서 언급한 모든 데이터 세트는 각각의 코퍼스에 관련된 API의 수를 강조하지만 API 호출이 사용되는 방식에 따라 각각 다르다. 예를 들어, 일부 데이터 세트는 단일 API 호출(Tang et al., 2023; Patil et al., 2023; Xu et al., 2023b)만을 포함하는 시나리오를 큐레이트하는 반면, 다른 데이터 세트는 다중 호출(Qin et al., 2023; Hao et al., 2023)을 포함한다. 또한, 일부는 그들의 문제를 해결하기 위해 실제 API에 대한 실제 호출을 필요로 한다(Qin et al., 2023; Li et al., 2023; Xu et al., 2023b). 이는 프롬프트된 LLM으로 API 호출을 시뮬레이션하는 다른 작업과 대조된다(Tang et al., 2023; Patil et al., 2023).\n' +
      '\n' +
      '위에 나열된 자체 감독 코퍼스의 한계는 API 사용 시나리오 평가에 있다. 일부 접근법들은 환각률에 기초하여 평가하는 반면(Patil et al., 2023), 다른 접근법들은 하나의 예의 품질을 평가하기 위해 별도의 LLM에 의존한다(Tang et al., 2023; Qin et al., 2023). 최근 연구는 Farn과 Shin(2023)이 수동으로 수집된 더 작은 세트의 Ground truth 주석에 의존하고 Huang et al.(2023)이 생성된 데이터의 수동 검사를 수행하는 등 이 문제에 초점을 맞추고 있다.\n' +
      '\n' +
      '##3 API-Blend Dataset 큐레이션\n' +
      '\n' +
      '우리는 입력이 단일 자연어 발화이고 출력이 매개변수 이름과 값을 가진 API 호출의 시퀀스인 설정에 초점을 맞춘다. API-Blend는 (1) 기존의 API 출력을 기반으로 프롬프트가 사용되는 언어 모델 보조 접근법, (2) 기존의 시맨틱 파싱 및 개인 비서 표기법을 API 데이터로 변환하는 문법 규칙 기반 접근법, (3) 기성 데이터세트로 구성된다. 표 1은 각 데이터 세트의 통계를 나타내며 접근/데이터 세트의 세부 정보는 다음과 같다.\n' +
      '\n' +
      '### 언어 모델 보조 생성\n' +
      '\n' +
      '**SeqSGD**: 우리는 API 시퀀스 평가에 맞춘 Schema-Guided Dialogue (SGD) (Rastogi et al., 2020) 데이터세트를 기반으로 한 데이터세트인 SeqSGD를 만들었다. SGD는 인간과 가상 어시스턴트 사이의 약 20k 주석이 달린 대화를 포함한다. 이러한 대화는 은행, 이벤트, 미디어, 캘린더, 여행 및 날씨를 포함한 20개 도메인에 걸친 다양한 서비스 및 API와의 참여로 구성된다. 이 데이터 세트를 변환하기 위해 각 대화에 대해 미리 훈련된 FLAN-T5-XXL3 모델이 각 API를 자연 언어로 요청으로 변환하도록 프롬프트했다. 그런 다음 각 API의 해당 텍스트를 추가하여 요약된 발화를 생성한다. 도 3은 예를 도시한 것이다. 또한 API 실행 결과를 포함하고 있기 때문에 대화 자체를 요약하지 않았으며, 이를 요약 모델에 입력으로 사용하면 많은 잡음이 있는 세부 사항이 발생했다. 생성된 텍스트가 모든 API와 파라미터 값을 캡처하는지 확인하기 위해, 발화가 그라운드 트루스 API를 올바르게 반영하지 않는 예를 제거하기 위해 데이터 세트를 후처리한다. 이 과정을 통해 평균 API 수가 2.44이고 API당 평균 파라미터 수가 3.5인 6.8K train, 1.1K validation, 1.6K test 예제를 생성하였다.\n' +
      '\n' +
      '각주 3: [https://huggingface.co/google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl)\n' +
      '\n' +
      '**SeqMultiWoz**: MultiWoz(Ye et al., 2021)는 또 다른 멀티 도메인 태스크 지향 대화 데이터세트이다. SGD 데이터 세트에서 SeqSGD를 큐레이션하는 동일한 프로세스에 따라 멀티워즈를 기반으로 하는 또 다른 API 데이터 세트인 SeqMultiWoz를 생성했다. 결과 데이터 세트는 평균 API를 갖는 약 6.8k 트레인, 485 검증 및 1k 테스트 샘플을 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c} \\hline \\hline \\multirow{2}{*}{**Datasets**} & \\multirow{2}{*}{**Train**} & \\multirow{2}{*}{**Dev**} & \\multirow{2}{*}{**Test**} & \\multicolumn{2}{c}{**Avg. No.**} \\\\  & & & & **Len** & **Params** \\\\ \\hline \\multicolumn{1}{l|}{**SeqATIS**} & 11,670 & 694 & 774 & 2.13 & 4.85 \\\\ \\multicolumn{1}{l|}{**SeqSGD**} & 6,782 & 1,092 & 1,567 & 2.44 & 3.5 \\\\ \\multicolumn{1}{l|}{**SeqSNIPS**} & 39,750 & 2,198 & 2,199 & 1.96 & 5.06 \\\\ \\multicolumn{1}{l|}{**SeqMultiWoz**} & 6,816 & 485 & 983 & 2.36 & 3.67 \\\\ \\multicolumn{1}{l|}{**SeqTopV2**} & 94,458 & 13,477 & 6,095 & 1.2 & 1.98 \\\\ \\multicolumn{1}{l|}{**Total \\(\\times\\) 15/4/76**} & **17,946** & **15,618** & & \\\\ \\hline \\multicolumn{1}{l|}{**TopLLM-G1**} & - & - & 197 & 2.28 & - \\\\ \\multicolumn{1}{l|}{**TopLLM-G2**} & - & - & 197 & 2.55 & - \\\\ \\multicolumn{1}{l|}{**TopLLM-G3**} & - & - & 97 & 2.91 & - \\\\ \\multicolumn{1}{l|}{**API Bank-1**} & - & - & 386 & 1.65 & 2.25 \\\\ \\multicolumn{1}{l|}{**API Bank-2**} & - & - & 71 & 1.34 & 2.44 \\\\ \\multicolumn{1}{l|}{**TopLLM-H8**} & - & - & 100 & 7.01 & 0.86 \\\\ \\multicolumn{1}{l|}{**TopLendR-B**} & - & - & 120 & 9.45 & 0.89 \\\\ \\multicolumn{1}{l|}{**SeqTotalQA**} & - & - & 358 & 2.42 & 1.45 \\\\ \\multicolumn{1}{l|}{**TopLAlpaa**} & - & - & 211 & 1.38 & 2.01 \\\\ \\multicolumn{1}{l|}{**Total**} & - & - & **173** & - & - \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: API-블렌드 데이터세트 통계: 빨간색으로 채색된 데이터세트는 훈련 및 도메인 내 테스트에 사용되는 반면, 녹색 데이터세트는 OOD 테스트에 2.36의 카운트만 사용되며 API당 평균 매개변수 카운트는 3.67이다.\n' +
      '\n' +
      '### Grammar-Based Generation\n' +
      '\n' +
      '**SeqATIS 및 SeqSNIPS**: ATIS [10]은 항공편에 대한 인간 주석 질의의 집합이다. 쿼리는 항공편 번호, 공항, 날짜 및 기타 관련 세부 정보와 같은 정보를 요청합니다. 데이터세트는 또한 의도 및 슬롯 값을 포함하는 시맨틱 라벨의 범위를 제공한다. 의도는 "비행 쿼리" 또는 "항공 쿼리"와 같은 쿼리의 전체 목표이고 슬롯 값은 "출발 도시" 또는 "도착 시간"과 같이 요청되고 있는 특정 정보 조각이다. SNIPS [14]는 음성 어시스턴트에 초점을 맞춘 또 다른 데이터세트이다. 날씨, 음악, 달력 등 다양한 영역을 아우르는 인간 주석이 달린 질의로 구성된다.\n' +
      '\n' +
      'MixATIS와 MixSNIPS는 각각 ATIS와 SNIPS를 기반으로 구축된 다중 의도 데이터세트[15]이다. ATIS/SNIPS 데이터셋에서 문장을 수집하여 "및"과 같은 접속사와 연결하여 생성하였다. 결과 데이터는 각각 30%, 50% 및 20%의 확률로 1-3 인텐트의 문장을 가졌다.\n' +
      '\n' +
      'API 호출을 위해 이 두 데이터 세트를 사용하는 한 가지 문제는 어떤 매개변수가 어떤 API와 연관되어야 하는지 표시하지 않는다는 것이다. 예를 들어, 그림 4에서 볼 수 있듯이 원래 MixSNIPS 데이터 세트는 두 개의 금 의도를 탐지하는 모델의 능력과 텍스트의 어떤 세그먼트가 대상 슬롯인지만 평가한다. MixATIS와 MixSNIPS 데이터세트를 API 호출의 시퀀스로 변환하기 위해, 각 API에 해당하는 파라미터를 얻기 위해 발화를 원래 단일 의도 발화로 다시 나눴다. 그런 다음 API 매개변수 이름과 값 목록을 생성하기 위해 IOB(내부/외부/시작) 매개변수 표기를 구문 분석했다. 이제 API 호출의 시퀀스를 얻기 위해 API 및 매개 변수와 함께 발화를 병합한다. 이러한 방식으로, 우리는 MixATIS와 MixSNIPS로부터 각각 SeqATIS와 SeqSNIPS를 큐레이션했다. SeqATIS에서는 평균 API 시퀀스 길이가 2.13이고 API당 평균 파라미터 카운트가 4.85인 약 11.5k 트레인, 700 검증 및 800 테스트 예가 있다. 반면 SeqSNIPS는 평균 API가 4.85인 약 40k 트레인, 2.2k 검증 및 2.2k 테스트 샘플로 구성된다.\n' +
      '\n' +
      '도 4: SeqSNIPS가 생성되는 방법의 예. MixSNIPS의 자연어 발화와 슬롯의 평면 목록을 사용하여 각각 매개변수 이름과 값의 사전이 있는 API 호출의 시퀀스로 변환한다.\n' +
      '\n' +
      '도 3: seqSGD의 생성 과정의 예시. API 목록에서 시작하여 요약된 단일 발화를 생성하기 위해 몇 개의 샷 프롬프트를 사용한다.\n' +
      '\n' +
      '시퀀스 길이가 1.96이고 API당 평균 매개변수가 5.06입니다.\n' +
      '\n' +
      '**SeqToolQA**: ToolQA Zhuang et al. (2023)은 툴-증강 LLMs 평가를 위해 설계된 오픈-소스 데이터세트이다. 데이터 세트는 LLM의 내부 지식을 사용하여 답변할 수 있는 질문과 도구 사용을 통해 외부 정보가 필요한 질문을 구별하지 않는 현재 도구 기반 평가 방법으로 문제를 해결하려고 한다. 이를 위해 8개의 독립 데이터셋(예: Kaggle Flights, Coffee, Yelp, Airbnb 등)과 해당 데이터셋에 대한 질의만으로 답변할 수 있는 일련의 질문으로 인해 LLM의 내부 지식을 배제한다. 데이터세트는 최종 답변과 함께 템플릿 기반 질문의 집합으로 제공된다.\n' +
      '\n' +
      '그러나, ToolQA는 주어진 질문들에 대한 답변을 생성하기 위해 실행될 필요가 있는 단계들(도구들)의 중간 세트를 제공하지 않는다. 아래 목록은 예를 보여준다:\n' +
      '\n' +
      '```\n' +
      '{"idid":"easy-flight-0003","question":"WhatwasthedeparturetimeoftheQL891flightfromSEAtoLAXon2022-01-227","answer":"11:54" }\n' +
      '```\n' +
      '\n' +
      '이 문제를 해결하기 위해, 우리는 각 템플릿 질문이 어떻게 답변되고 해당 API 집합으로 변환되는지에 대한 도구QA의 제공된 구현을 사용하는 SeqToolQA를 제안한다. 6개의 도메인을 포함하는 17개의 서로 다른 API를 추상화하고 테스트 목적을 위해 총 358개의 예를 생성했다. 우리는 아래의 예를 보여준다:\n' +
      '\n' +
      '```\n' +
      '{"id":"easy-flight-0000","question":"WhatwasthedeparturetimeoftheUAS480flightfromORDtoHSVon2022-07-067","apis":"Load08DB[DSName=flights]","Filter08[Origin=ORD,Dest=HSV,FlightDate=2822-07-06,Flight_Number_Marketing_Airline=5480,IATA_Code_Marketing_Airline=UA]","GetValue[ValueName=DepTime]","answer":"18:11" }\n' +
      '```\n' +
      '\n' +
      '**SeqTopV2**: Topv2 Chen et al. (2020)은 알람, 이벤트, 메시징, 음악, 네비게이션, 리마인더, 타이머, 및 날씨의 8개의 도메인으로부터의 예를 포함하는 다중 도메인 태스크 지향 시맨틱 파싱 데이터세트이다. 총 데이터 세트는 180k개의 샘플로 구성되며, 각 도메인에 대한 훈련, 개발 및 테스트 세트로 무작위로 나뉜다. 우리는 의도 "IN:" 및 슬롯 "SL:" 표시를 사용하여 이 데이터 세트를 API로 변환하기 위한 간단한 접근법을 따랐다. 이 데이터 세트에는 실제로 따라야 하는 API 시퀀스가 있습니다. 예제 "Joy에게 오늘 밤 새로운 클라이언트와의 세부 사항에 대해 이메일을 보내도록 상기시켜 주세요"에서, 우리는 그것을 "SEND_MESSAGE" 및 "CREATE_REMINDER"의 두 개의 API로 변환하는데, 여기서 "CREATE_REMINDER"는 "SEND_MESSAGE"에 대한 전제 조건이 있다.\n' +
      '\n' +
      '원래 데이터 세트에는 일치하는 의도가 없는 예에 대한 "UNSUPPORTED" 표기가 많이 있었다. API 데이터 세트에서 이러한 경우를 제외했습니다. 이들과 함께 동일한 의도 내에서 중복된 발화, 모호한 의도 및 유사한 슬롯 이름이 있는 샘플도 제거했다. 우리는 결과 데이터세트 SeqTopV2라고 부르며, 훈련, 개발 및 테스트 분할을 위해 각각 94K, 13.5K 및 6K를 가지고 있다.\n' +
      '\n' +
      '### Off-the-Shelf Usage\n' +
      '\n' +
      '**ToolBench**: 이것은 HomeSearch and Booking이라는 두 개의 도메인에 초점을 맞춘 ToolBench Xu et al.(2023)의 서브세트이다. 우리는 이러한 데이터 세트에 대한 변환을 수행하지 않았으며 이미 API 형태로 있기 때문에 "있는 그대로"를 사용했다.\n' +
      '\n' +
      '**ToolLLM** Qin et al. (2023)은 LLaMA-2-7B에 기반한 명령어-튜닝 툴 데이터셋 및 툴-증강 LLM 모델을 제안한다. 데이터 세트는 Chat-GPT와 RapidAPI의 16K API 모음을 기반으로 합성적으로 생성된다. 데이터세트에는 단일 도구, 범주 내 다중 도구 및 수집 내 다중 도구 데이터를 각각 나타내는 G1,G2,G3의 세 가지 하위 집합이 포함된다. 우리는 유통 외 모델 평가를 위해 위의 세 가지 하위 집합을 사용했다.\n' +
      '\n' +
      '**API Bank** Li 등(2023): 툴-증강 LLMs를 평가하기 위해 설계된 벤치마크이다. 여기에는 API를 계획, 검색 및 호출하는 데 있어 기존 LLM의 기능을 평가하기 위해 753개의 API 호출이 있는 314개의 도구 사용 대화 상자가 포함된다. 또한 1.8K 대화의 더 큰 학습 데이터 세트와 알파카에서 초기화된 그것에 대해 훈련된 모델(Lynx)이 함께 제공됩니다. 본 논문에서는 유통 외 평가를 위해 API 뱅크의 테스트 세트를 사용한다. 이것은 중간에 호출되는 도구가 있는 대화이므로 각 대화는 여러 테스트 예제로 나누었으며 각 예제에는 지금까지 필요한 대화와 API 시퀀스가 포함된다.\n' +
      '\n' +
      '**ToolAlpaca**Tang et al. (2023): ToolAlpaca는 ChatGPT 및 GPT-3.5를 사용하는 다중 에이전트 시뮬레이션 환경을 통해 자동으로 큐레이션되는 훈련 및 평가 벤치마크이다. 코퍼스는 50개의 별개의 카테고리에 걸쳐 있는 400개 이상의 실제 도구 API로부터 3,938개의 도구 사용 인스턴스를 포함한다. 학습 세트가 함께 제공되지만, 본 논문에서는 유통 외 평가에 테스트 세트만 사용했다.\n' +
      '\n' +
      '##4 실험 및 결과\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '우리의 실험에서, 우리는 (1) LLaMA-2-70B (Touvron et al., 2023), (2) Falcon-180B (Almazrouei et al., 2023), (3) LLAMA-2-7B (Touvron et al., 2023), (4) FLAN-T5-XXL (Chung et al., 2022), (5) Falcon-40B (Almazrouei et al., 2023), (6) StarCoder-15B (Li et al., 2023c), (7) MPT-30B (Team et al., 2023), (8) ToolLLaMA-2-7B (Qin et al., 2023), 및 (9) Lynx-7B (Li et al., 2023b). 본 연구에서는 LLAMA-2-70B, Falcon-180B, ToolLLaMA-2-7B를 3 shot mode로 평가하였고, LLAMA-2-70B, Falcon-180B, ToolLLaMA-2-7B를 3 shot mode로 평가하였으며, FLAN-T5-XXL, StarCoder-15B, Falcon-40B, MPT-30B에 대한 명령어 미세조정을 고려하였다. OD 실험을 위해, 우리는 이미 ToolLLM 및 APIBench 데이터로 미세 조정된 ToolLLaMA-2-7B 및 Lynx-7B와 함께 (3)의 모든 미세 조정된 모델을 사용했다.\n' +
      '\n' +
      '### Instruction Tuning\n' +
      '\n' +
      '모든 실험에서 우리는 훈련과 테스트를 위해 동일한 지침을 사용했다. 우리는 지시 템플릿 아래에 보여준다. 비 미세 조정 모델을 평가하거나 OOD 실험을 위한 경우에만 지침에서 "여기 몇 가지 예: ICL_EXAMPLES" 부분을 통해 3개의 ICL 예제를 제공하고 그렇지 않으면 제거한다.\n' +
      '\n' +
      '```\n' +
      '###GiventheAPIsandSlotsbelow,sequencethemintheorderinwhichtheyhavetobecalledtoanswerfollowingquery.PossibleAPIs:(INTER_LIST) PossibleSlots:(SL0_LIST) Herearesomeexamples:(ICL_EXAMPLES) Query:(QUERY) Answer:\n' +
      '```\n' +
      '\n' +
      '### 설정 및 파라미터:\n' +
      '\n' +
      '우리는 모든 모델을 미세 조정하기 위해 QLoRA(Dettmers et al., 2023)를 사용했다. 대상 데이터 세트에서 모델을 미세 조정하는 동안 모델이 훈련 과정에서 100k 샘플을 보는지 확인했다. 결합된 데이터 훈련에서, 우리는 누적 데이터 세트에 대해 2개의 에폭에 대한 모델을 미세 조정했다. 두 경우 모두 배치크기는 \\(1\\)의 경사축적단계와 \\(8\\)의 학습률을 보였다 (5e^{-5}\\).\n' +
      '\n' +
      '### Metrics\n' +
      '\n' +
      '생성된 응답에 대한 세밀한 평가를 수행하기 위해 표준 정보 검색 메트릭(정확도, 재현율 및 f1 점수)과 가장 긴 공통 서브시퀀스(LCS)의 두 가지 평가를 사용한다. 우리는 예측된 API와 골드 API를 비교하고 각 API의 예측된 파라미터를 골드 대응물과 비교하여 F1 점수를 계산하기 위해 F1 API와 F1 슬롯/파라미터를 보고한다. 또한 주어진 자연어 질의에 의해 지시된 API 호출의 시퀀스를 따르는 모델의 능력을 측정하기 위해 API 이름의 골드 시퀀스와 모델에 의해 예측된 시퀀스를 비교하여 LCS를 보고한다.\n' +
      '\n' +
      '### 유통중 평가결과\n' +
      '\n' +
      '**No Fine-tuning evaluation**: 첫 번째 실험은 이러한 설정에서 최첨단 개방형 LLMs가 어떻게 수행되는지 확인하는 것이었다. 특히 3-shot prompting을 이용하여 LLaMA-2-70B와 Falcon-180B를 평가하였다. 또한 ToolLLaMA-2-7B(Qin et al., 2023); RapidAPIs로부터의 사양에 기초하여 ChatGPT를 사용하여 생성된 API 데이터세트에 대해 트레이닝된 LLAMA-2-7B 기반 모델을 고려하였다. 표 2는 SeqATIS, SeqSNIPS, SeqSGD, SeqMultiWoz, SeqTopV2의 5가지 분포 내 데이터 세트에 대한 평가 결과를 보여준다. 모든 데이터 세트에 대해 세 가지 비 미세 조정 모델 모두 일부 API를 올바르게 얻었지만 이러한 API를 호출하기 위한 매개변수를 얻지 못하는 것으로 판단된다.\n' +
      '\n' +
      '**One Dataset**에서 미세 조정: 이 실험에서는 위에서 논의한 기준선을 각 데이터 세트에서 미세 조정하고 해당 테스트 분할에서 테스트한다. 우리는 FLAN-T5-XXL, StarCoder-15B, Falcon-40B 및 MPT-30B의 4가지 모델을 평가했다. 이상적으로, 이 설정은 트레이닝 데이터가 하나의 데이터세트 및 그것의 API들에 집중되기 때문에 최상의 성능을 제공해야 한다. 표 2에 나타난 바와 같이, 모든 모델은 SeqMultiWoz 데이터 세트에 대해 모든 모델의 성능이 100% API-F1 점수에 도달하여 올바른 API를 검출하는 매우 좋은 성능(\\(>\\) 90%)을 달성했다. 우리는 이 높은 성능이 대부분의 데이터 세트에서 API의 수가 그리 많지 않기 때문이라고 가정한다(예: SeqMultiWoz의 경우 12개의 API). 파라미터 이름 및 값의 올바른 세트를 검출하는 것은 SeqSGD 데이터 세트에 대해 성능이 가장 낮은 대부분의 모델에서 더 어려운 문제이다. 상기 가중 평균은\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      'ToolLAMA-2-7B와 Lynx-7B는 각각 Tool-LLM과 API-Bank에서 나왔지만 여전히 성능이 좋지 않다. 그들의 논문에서, 그들은 모델이 몇 번 대답에 도달했는지 결정하기 위해 통과율과 ChatGPT를 판사로 사용하는 승소율과 같은 다양한 측정 기준을 사용했다. 두 도구 벤치 데이터 세트 모두에서 팔콘-40B가 다른 데이터 세트보다 성능이 뛰어납니다. SeqToolQA에서는 모델이 API 검출에서 일부 점수를 매겼음에도 불구하고 모든 모델이 매개변수 이름과 값을 검출하는 데 제대로 수행되지 않아 매개변수-F1 점수가 낮다. 이는 SeqToolQA 내의 파라미터 값들은 어떤 학습 데이터에서도 보지 못한 코드, SQL, 수학 함수 등을 포함하고 있으며, 이러한 특수한 슬롯 값들은 소수의 ICL 예만을 보고 생성하는 것이 사소하지 않기 때문이다.\n' +
      '\n' +
      '### Qualitative Studies\n' +
      '\n' +
      '또한 실험에서 모델에 의해 생성된 출력에 대한 광범위한 연구를 수행했다. 이 섹션에서는 실패한 사례에 대한 발견과 모델에 의해 입증된 몇 가지 일반적인 오류에 대해 논의할 것이다. 우리는 대부분의 경우 파라미터 이름과 값이 유통 중 및 유통 외 테스트 세트 모두에서 슬롯 탐지를 잘하지 못하는 명백한 이유임을 발견했다. 더 나은 이해를 위해 각 사례에 대한 샘플을 제공합니다. 우리는 제공된 예들의 대부분이 "gold_output" 및 "predicted_output"을 포함하고, 간결함을 위해 원래의 출력들로부터의 에러 부분(서브 스트링)만을 도시했다는 것을 언급하고 싶다.\n' +
      '\n' +
      '######4.7.1 비정규화된 슬롯 값\n' +
      '\n' +
      '이상적인 시나리오에서, 파라미터 값들은 텍스트들을 생성하는 동안 모델들에 의해 쿼리에서 정확하게 추출되어야 한다. 그러나 때로는 모델이 그 하위 부분을 추출하거나 추출한 후 다른 형태로 표현하기도 한다. 인간 평가에서는 생성된 텍스트가 금과 일치한다고 간주할 수 있지만 프로그램적으로 수행하는 동안 불일치를 나타내며 매개변수 값을 정규화하는 좋은 방법을 찾지 못했다. 다음 예에서는 정규화되지 않은 매개변수 값 불일치의 일부를 캡처합니다. 첫 번째 예제에서는 예측 출력의 월과 요일을 반복한다. 두 번째의 예측 산출물은 도시 이름만 포함하는 반면 금은 도시와 국가를 포함한다. 최종 예에서, 의도 및 슬롯 값들이 정확하더라도, 그들은 그것을 나타내기 위해 상이한 파라미터 포맷들을 사용하였다. 우리는 이러한 문제를 더 조사할 계획이지만 향후 작업을 위해 유지합니다.\n' +
      '\n' +
      '```\n' +
      '1"""SeqSGD"""\n' +
      '2("gold":"March3rd,thisSunday","predicted":"March3rd,the3rd"\n' +
      '3("gold":"NYC,NewYork","predicted":"NYC")"""Toolbench#\n' +
      '4"""Toolbench#\n' +
      '5("gold":"Date(3,9,2023)","predicted":"Date(year=2023,month=3,day=9)")\n' +
      '```\n' +
      '\n' +
      'API 명세에서 의미적으로 유사한 슬롯 이름 4.7.2\n' +
      '\n' +
      '우리의 지침에서는 쿼리에 응답하는 동안 모델이 사용할 API 및 매개변수의 가능한 목록을 제공한다. 원래 데이터 세트의 API 사양에서 이러한 API와 매개변수를 추출합니다. 그러나 일부 경우에 매개변수 이름이 데이터 세트에서 의미적으로 매우 유사하다는 것을 발견했다. 다음은 SeqSGD 데이터 세트의 몇 가지 예이다: (1) _leaving_date_ 및 _departure_date_; (2) _leaving_time_ 및 _departure_time_; (3) _origin_, _from_city_ 및 _from_location_; 및 (4) _destination_, _to_city_ 및 _to_location_. 이제 생성된 텍스트에서 매개 변수 값이 정확하지만 매개 변수 이름이 금과 정확히 일치하지 않는 경우가 종종 발생한다. 다음은 이러한 사례의 몇 가지 예입니다.\n' +
      '\n' +
      '```\n' +
      '1"""SeqSGD"""\n' +
      '2("gold":"destination_airport=ATL","predicted":"destination=ATL")\n' +
      '3("gold":"show_type=imax","predicted":"theater_name=imax")\n' +
      '4"""SeatXIS##\n' +
      '5("gold":"cuisine=souvlaki","predicted":"served_dish=souvlaki")\n' +
      '```\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문은 대화, 시맨틱 파싱, 디지털 어시스턴트 등과 같은 서로 다른 도메인의 실제 데이터 세트로부터 큐레이션된 툴-증강 LLM의 학습 및 평가를 위한 대규모 코퍼스인 API-Blend를 제시한다. API-Blend는 190k 이상의 인스턴스(열차, 개발 및 테스트 포함)를 포함하는 10개의 데이터 세트(5개의 내부 배포 및 5개의 외부 배포)로 구성된다. 우리는 OOD 실험에서 API-Blend로 미세 조정된 모델이 다른 도구 증강 LLM보다 더 잘 일반화됨을 입증했다. 본 연구 결과는 학습 및 벤치마킹 도구 활용 LLM에서 API-Blend의 중요성을 입증할 뿐만 아니라 LLM의 API 사용 능력을 향상시키기 위한 일반화 가능성의 필요성을 강조한다.\n' +
      '\n' +
      '##6 제한 및 위험\n' +
      '\n' +
      '우리의 벤치마크인 API-Blend의 한계는 API 에이전트에 대한 환경 상호 작용을 다루지 않는다는 것이다. 향후 작업에서는 API 호출 효과가 접지된 환경에서 변경되는 구체화된 에이전트의 이러한 설정을 탐구하는 것이 흥미로울 것이다. 또한, 우리의 벤치마크는 영어 API 명령에 초점을 맞추고 있으며, 앞으로는 다국어 API-Blend를 개발하는 것이 흥미로울 것이다. 우리는 우리의 일과 관련된 어떤 위험도 인식하지 않는다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 기술보고서; _ arXiv preprint arXiv:2303.08774_.\n' +
      '* Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. Falcon series of open language models. _ arXiv preprint arXiv:2311.16867_.\n' +
      '*Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer _ arXiv preprint arXiv:2004.05150_.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 소수의 학습자를 의미한다. _ 신경 정보 처리 시스템들_, 33:1877-1901의 진보들.\n' +
      '* Chen et al. (2020) Xilun Chen, Asish Ghoshal, Yashar Mehdad, Luke Zettlemoyer, and Sonal Gupta. 2020. compositional task-oriented semantic parsing을 위한 Low-resource domain adaptation. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_. 컴퓨터 언어학과의 연관성\n' +
      '* Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. 르와 제이슨 웨이 2022. Scaling instruction-finetuned language models.\n' +
      '* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. 수학 단어 문제를 해결하기 위한 훈련 검증자 __ arXiv preprint arXiv:2110.14168_.\n' +
      '* Coucke et al. (2018) Alice Coucke, Alaa Saade, Adrien Ball, Theodore Bluche, Alexandre Caulier, David Leroy, Clement Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, et al. 2018. Snips 음성 플랫폼: private-by-design 음성 인터페이스를 위한 임베디드 음성 언어 이해 시스템. _ arXiv preprint arXiv:1805.10190_.\n' +
      '* Dettmers et al.(2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Quantized llms의 효율적인 finetuning. _ arXiv preprint arXiv:2305.14314_.\n' +
      '* Farn and Shin (2023) Nicholas Farn and Richard Shin. 2023. 툴토크: 대화 설정에서 툴-사용을 평가하는 단계. _ arXiv preprint arXiv:2311.10775_.\n' +
      '* Gao et al. (2023) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language models. _International Conference on Machine Learning_, pages 10764-10799. PMLR.\n' +
      '* Gupta et al. (2023) Himanshu Gupta, Kevin Scaria, Ujjwala Anantheswaran, Shreyas Verma, Mihir Parmar, Saurabh Arjun Sawant, Swaroop Mishra, and Chitta Baral. 2023. 타젠: 대규모 언어 모델을 가진 타겟 데이터 생성 _ arXiv preprint arXiv:2310.17876_.\n' +
      '* Hao et al. (2023) Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2023. 툴켄프: 툴 임베딩을 통해 방대한 툴로 냉동 언어 모델을 증강한다. _ arXiv preprint arXiv:2305.11554_.\n' +
      '* He-Yueya et al. (2023) Joy He-Yueya, Gabriel Poesia, Rose E Wang, and Noah D Goodman. 2023. 언어 모델을 기호 풀이기와 결합하여 수학 단어 문제를 풀이하는 단계. _ arXiv preprint arXiv:2304.09102_.\n' +
      '* Hemphill et al. (1990) Charles T. 헴필, 존 J. 고드프리, 그리고 조지 R. 도딩턴 1990. ATIS 음성 언어 시스템 파일럿 코퍼스. _Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990_.\n' +
      '* Huang et al. (2023) Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, et al. 2023. Metatool 벤치마크 for large language models: Metatool 벤치마크는 도구 사용 여부 및 어느 것을 사용할 것인지 결정한다. _ arXiv preprint arXiv:2310.03128_.\n' +
      '*Imani et al. (2023) Shima Imani, Liang Du, and Harsh Shrivastava. 2023. Mathprompter : 대형 언어 모델을 이용한 수학적 추론_ arXiv preprint arXiv:2303.05398_.\n' +
      '* Komeili et al. (2022) Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2022. 인터넷 증강 대화 생성. _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8460-8478.\n' +
      '* Krizhevsky et al. (2012) Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2020. Mlqa: 교차 언어 추출형 질문 답변 평가. _Proceedings of the 58th Annual Meeting for Computational Linguistics_, pages 7315-7330.\n' +
      '* Li 등(2023a) Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023a. Apibank: tool-augmented llms의 벤치마크. _ arXiv preprint arXiv:2304.08244_.\n' +
      '* Li 등(2023b) Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023b. 아피뱅크: 툴-증강 llms의 벤치마크.\n' +
      '* Li et al. (2023c) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023c. 스타코드: 출처가 당신과 함께 있기를! _ arXiv preprint arXiv:2305.06161_.\n' +
      '* Min et al. (2023) Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023. 대규모 사전 훈련된 언어 모델을 통한 자연어 처리의 최근 진보: 설문조사. _ ACM Computing Surveys_, 56(2):1-40.\n' +
      '* Parisi et al. (2022) Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm : 도구 증강 언어 모델 _ arXiv preprint arXiv:2205.12255_.\n' +
      '* Patel et al. (2021) Arkil Patel, Satwik Bhattacharya, and Navin Goyal. 2021. NLP 모델이 정말 간단한 수학 단어 문제를 해결할 수 있나요? _Proceedings of the 2021 of the North American chapter of the Computational Linguistics Association: Human Language Technologies_, pages 2080-2094, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Patil et al. (2023) Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2023. 고릴라: massive apis와 연결된 대용량 언어 모델 _ arXiv preprint arXiv:2305.15334_.\n' +
      '* Qin et al. (2020) Libo Qin, Xiao Xu, Wanxiang Che, and Ting Liu. 2020. Agif: 공동 다중 의도 검출 및 슬롯 채우기를 위한 적응적 그래프-대화형 프레임워크. _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 1807-1816.\n' +
      '* Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. ToollIM: large language models to master 16000+ actual-world apis. _ arXiv preprint arXiv:2307.16789_.\n' +
      '* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비감독 멀티태스크 학습자들이다. _ OpenAI blog_, 1(8):9.\n' +
      '* Rastogi et al. (2020) Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. 확장 가능한 다중 도메인 대화 에이전트를 향해: 스키마 유도 대화 데이터세트. _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 8689-8696.\n' +
      '* Saikh et al. (2022) Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. 2022. Scienceqa: 학술 논문에 대한 질의응답을 위한 신규 자원. _ International Journal on Digital Libraries_, 23(3):289-301.\n' +
      '* Scarton et al. (2019) Scarton Scarton, Mikel L Forcada, Miquel Espla-Gomis, and Lucia Specia. 2019. 사후 편집 노력 추정: 인간 판단, 과제 기반 및 참조 기반 mt 품질 메트릭에 대한 연구. In _Proceedings of the 16th International Conference on Spoken Language Translation_.\n' +
      '* Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. 툴포머: 언어 모델들은 스스로 툴을 사용하는 것을 가르칠 수 있다. _ ArXiv_, abs/2302.04761.\n' +
      '* Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. 2023. 반사: 언어 강화 학습을 갖는 언어 에이전트. IMT-2000 3GPP-신경정보처리시스템에 관한 제37차 회의\n' +
      '* Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: 대화형 학습을 위한 텍스트 및 구체화된 환경의 정렬. _International Conference on Learning Representations_.\n' +
      '* Tang et al. (2023a) Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. 2023a. 툴라파카: 3000개의 시뮬레이션 사례가 있는 언어 모델에 대한 일반화된 도구 학습.\n' +
      '* Tang et al. (2023b) Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. 2023b. 툴라파카: 3000개의 시뮬레이션 사례가 있는 언어 모델에 대한 일반화된 도구 학습.\n' +
      '* Team et al. (2023) MN Team et al. 2023. Introducing mpt-7b: a new standard for open-source, commercially usable llms.\n' +
      '* Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. _ arXiv preprint arXiv:2201.08239_.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models, 2023. _URL https://arxiv. org/abs/2307.09288_.\n' +
      '* Xu et al. (2023a) Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu. 2023a. Rewoo: 효율적인 증강 언어 모델을 위한 관찰로부터의 분리 추론__ arXiv preprint arXiv:2305.18323_.\n' +
      '\n' +
      '키안통수, 펑루홍, 보리, 창란후, 진규천, 지안장 등이 있다. 2023b. 오픈소스 대형 언어 모델의 도구 조작 능력에 대해. _ arXiv preprint arXiv:2305.16504_.\n' +
      '* Yang et al. (2023) Zhen규안 Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-react: Prompting chatgpt for multimodal reasoning and action. _ arXiv preprint arXiv:2303.11381_.\n' +
      '* Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: 다양하고 설명 가능한 멀티홉 질문 응답을 위한 데이터셋. _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2369-2380.\n' +
      '* Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2022. React: 추론 동기화 및 언어 모델에서의 연기. _The Eleventh International Conference on Learning Representations_.\n' +
      '* Ye et al. (2021) Fanghua Ye, Jarana Manotumruksa, and Emine Yilmaz. 2021. Multiwoz 2.4: 상태 추적 평가를 향상시키기 위한 필수 주석 수정을 갖는 다중 도메인 태스크 지향 대화 데이터세트. _ arXiv preprint arXiv:2104.00773_.\n' +
      '* Yu et al. (2022) Yu Yu, Shahram Khadivi, and Jia Xu. 2022. 데이터 다양성이 학습 일반화를 향상시킬 수 있는가? _Proceedings of the 29th international conference on computational linguistics_, pages 4933-4945.\n' +
      '* Zhang et al. (2020) Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: 추상적 요약을 위한 추출된 갭-문장으로 사전 훈련. _International Conference on Machine Learning_, pages 11328-11339. PMLR.\n' +
      '* Zhuang et al. (2023) Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2023. Toolqa: 외부 도구를 이용한 llm 질의 응답을 위한 데이터셋. _ arXiv preprint arXiv:2306.13304_.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>