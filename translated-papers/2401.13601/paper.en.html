<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MM-LLMs: Recent Advances in MultiModal Large Language Models\n' +
      '\n' +
      'Duzhen Zhang\\({}^{1}\\), Yahan Yu\\({}^{2}\\), Chenxing Li\\({}^{1}\\), Jiahua Dong\\({}^{3}\\), Dan Su\\({}^{1}\\),\n' +
      '\n' +
      'Chenhui Chu\\({}^{2}\\)  and Dong Yu\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Tencent AI Lab\n' +
      '\n' +
      '\\({}^{2}\\)Kyoto University\n' +
      '\n' +
      '\\({}^{3}\\)Mohamed Bin Zayed University of Artificial Intelligence\n' +
      '\n' +
      'scoutzhang@tencent.com, yahan@nlp.ist.i.kyoto-u.ac.jp\n' +
      '\n' +
      'Equal contributions.Corresponding authors.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of \\(26\\) existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website1 for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.\n' +
      '\n' +
      'Footnote 1: [https://mm-llms.github.io](https://mm-llms.github.io)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'MultiModal (MM) pre-training research has witnessed significant advancements in recent years, consistently pushing the performance boundaries across a spectrum of downstream tasks (Li et al., 2020; Akbari et al., 2021; Fang et al., 2021; Yan et al., 2021; Li et al., 2021; Radford et al., 2021; Li et al., 2022; Zellers et al., 2022; Zeng et al., 2022; Yang et al., 2022; Wang et al., 2022, 2022). However, as the scale of models and datasets continues to expand, traditional MM models incur substantial computational costs, particularly when trained from scratch. Recognizing that MM research operates at the intersection of various modalities, a logical approach is to capitalize on readily available pre-trained unimodal foundation models, with a special emphasis on powerful Large Language Models (LLMs) (OpenAI, 2022). This strategy aims to mitigate computational expenses and enhance the efficacy of MM pre-training, leading to the emergence of a novel field: MM-LLMs.\n' +
      '\n' +
      'MM-LLMs harness LLMs as the cognitive powerhouse to empower various MM tasks. LLMs contribute desirable properties like robust language generation, zero-shot transfer capabilities, and In-Context Learning (ICL). Concurrently, foundation models in other modalities provide high-quality representations. Considering foundation models from different modalities are individually pre-trained, the core challenge facing MM-LLMs is how to effectively connect the LLM with models in other modalities to enable collaborative inference. The predominant focus within this field has been on refining alignment between modalities and aligning with human intent via a MM Pre-Training (PT) + MM Instruction-Tuning (IT) pipeline.\n' +
      '\n' +
      'With the debut of GPT-4(Vision) (OpenAI, 2023) and Gemini (Team et al., 2023), showcasing impressive MM understanding and generation capabilities, a research f\n' +
      '\n' +
      'Figure 1: The timeline of MM-LLMs.\n' +
      '\n' +
      'LLMs has been sparked. Initial research primarily focuses on MM content comprehension and text generation like (Open)Flamingo (Alayrac et al., 2022; Awadalla et al., 2023), BLIP-2 (Li et al., 2023c), Kosmos-1 (Huang et al., 2023c), LLaVA/LLaVA-1.5 (Liu et al., 2023e,d), MiniGPT-4 (Zhu et al., 2023a), MultiModal-GPT (Gong et al., 2023), VideoChat (Li et al., 2023d), Video-LLaMA (Zhang et al., 2023e), IDEFICS (IDEFICS, 2023), Fuyu-8B (Bavishi et al., 2023), and Qwen-Audio (Chu et al., 2023b). In pursuit of MM-LLMs capable of both MM input and output (Aiello et al., 2023), some studies additionally explore the generation of specific modalities, such as Kosmos-2 (Peng et al., 2023) and MiniGPT-5 (Zheng et al., 2023b) introducing image generation, and SpeechGPT (Zhang et al., 2023a) introducing speech generation. Recent research endeavors have focused on mimicking human-like any-to-any modality conversion, shedding light on the path to artificial general intelligence. Some efforts aim to amalgamate LLMs with external tools to reach an approaching \'any-to-any\' MM comprehension and generation, such as Visual-ChatGPT (Wu et al., 2023a), ViperGPT (Suris et al., 2023), MM-REACT (Yang et al., 2023), HuggingGPT (Shen et al., 2023), and AudioGPT (Huang et al., 2023b). Conversely, to mitigate propagated errors in the cascade system, initiatives like NExT-GPT (Wu et al., 2023d) and CoDi-2 (Tang et al., 2023b) have developed end-to-end MM-LLMs of arbitrary modalities. The timeline of MM-LLMs is depicted in Figure 1.\n' +
      '\n' +
      'In this paper, we present a comprehensive survey aimed at facilitating further research of MM-LLMs. To provide readers with a holistic understanding of MM-LLMs, we initially delineate general design formulations from model architecture (Section 2) and training pipeline (Section 3). We break down the general model architecture into five components: Modality Encoder (Section 2.1), Input Projector (Section 2.2), LLM Backbone (Section 2.3), Output Projector (Section 2.4), and Modality Generator (Section 2.5). The training pipeline elucidates how to enhance a pre-trained text-only LLM to support MM input or output, primarily consisting of two stages: MM PT (Section 3.1) and MM IT (Section 3.2). In this section, we also provide a summary of mainstream datasets for MM PT and MM IT. Next, we engage in discussions of \\(26\\) State-of-the-Art (SOTA) MM-LLMs, each characterized by specific formulations, and summarize their development trends in Section 4. In Section 5, we comprehensively review the performance of major MM-LLMs on mainstream benchmarks and distill key training recipes to enhance the efficacy of MM-LLMs. In Section 6, we offer promising directions for MM-LLMs research. Moreover, we have established a website ([https://mm-llms.github.io](https://mm-llms.github.io)) to track the latest progress of MM-LLMs and facilitate crowd-sourcing updates. Finally, we summarize the entire paper in Section 7 and discuss related surveys on MM-LLMs in Appendix A. We aspire for our survey to aid researchers in gaining a deeper understanding of this field and to inspire the design of more effective MM-LLMs.\n' +
      '\n' +
      '## 2 Model Architecture\n' +
      '\n' +
      'In this section, we provide a detailed overview of the five components comprising the general model architecture, along with the implementation choices for each component, as illustrated in Figure 2. MM-LLMs that emphasize MM understanding only include the first three components. During training, Modality Encoder, LLM Backbone, and Modality Generator are generally maintained in a frozen state. The primary optimization emphasis is on Input and Output Projectors. Given that Projectors are lightweight components, the proportion of trainable parameters in MM-LLMs is notably small compared to the total parameter count (typically around \\(2\\)%). The overall parameter count is contingent on the scale of the core LLM utilized in the MM-LLMs. As a result, MM-LLMs can be efficiently trained to empower various MM tasks.\n' +
      '\n' +
      '### Modality Encoder\n' +
      '\n' +
      'The Modality Encoder (ME) is tasked with encoding inputs from diverse modalities \\(I_{X}\\) to obtain corresponding features \\(\\mathbf{F}_{X}\\), formulated as follows:\n' +
      '\n' +
      '\\[\\mathbf{F}_{X}=\\text{ME}_{X}(I_{X}). \\tag{1}\\]\n' +
      '\n' +
      'Various pre-trained encoder options \\(\\text{ME}_{X}\\) exist for handling different modalities, where \\(X\\) can be image, video, audio, 3D, or etc. Next, we will offer a concise introduction organized by modality.\n' +
      '\n' +
      'Visual ModalityFor images, there are generally four optional encoders: **NFNet-F6**(Brock et al., 2021), **ViT**(Dosovitskiy et al., 2020), **CLIP ViT**(Radford et al., 2021), and **Eva-CLIP ViT**(Fang et al., 2023). **NFNet-F6** is a normalizer-free ResNet (He et al., 2016), showcasing an adaptive gradient clipping technique that allows training on extensively augmented datasets while achieving the Transformer Vaswani et al. (2017) to images by first dividing the image into patches. It then undergoes linear projection to flatten the patches, followed by encoding via multiple Transformer blocks. **CLIP ViT** builds connections between text and images, comprising a ViT and a text encoder. Utilizing a vast amount of text-image pairs, it optimizes ViT by contrastive learning, treating paired text and images as positive samples and others as negative ones. Its **Eva** version stabilizes the training and optimization process of the massive CLIP, offering new directions in expanding and accelerating the expensive training of MM base models. For videos, they can be uniformly sampled to \\(5\\) frames, undergoing the same pre-processing as images.\n' +
      '\n' +
      'Audio Modalityis typically encoded by **C-Former**Chen et al. (2023), **HuBERT**Hsu et al. (2021), **BEATs**Chen et al. (2023), and **Whisper**Radford et al. (2023). **C-Former** employs the CIF alignment mechanism Dong and Xu (2020); Zhang et al. (2022) for sequence transduction and a Transformer to extract audio features. **HuBERT** is a self-supervised speech representation learning framework based on BERT Kenton and Toutanova (2019), achieved by the masked prediction of discrete hidden units. **BEATs** is an iterative audio pre-training framework designed to learn Bidirectional Encoder representations from Audio Transformers.\n' +
      '\n' +
      '3D Point Cloud Modalityis typically encoded by **ULIP-2**Salesforce (2022); Xu et al. (2023, 2023) with a PointBERT Yu et al. (2022) backbone.\n' +
      '\n' +
      'Moreover, to handle numerous heterogeneous modal encoders, some MM-LLMs, particularly any-to-any ones, use **ImageBind**Girdhar et al. (2023), a unified encoder covering six modalities, including image, video, text, audio, heat map, etc.\n' +
      '\n' +
      '### Input Projector\n' +
      '\n' +
      'The Input Projector \\(\\mathbf{\\Theta}_{X\\to T}\\) is tasked with aligning the encoded features of other modalities \\(\\mathbf{F}_{X}\\) with the text feature space \\(T\\). The aligned features as prompts \\(\\mathbf{P}_{X}\\) are then fed into the LLM Backbone alongside the textual features \\(\\mathbf{F}_{T}\\). Given \\(X\\)-text dataset \\(\\{I_{X},t\\}\\), the goal is to minimize the \\(X\\)-conditioned text generation loss \\(\\mathcal{L}_{\\text{txt-gen}}\\):\n' +
      '\n' +
      '\\[\\operatorname*{arg\\,min}_{\\mathbf{\\Theta}_{X\\to T}}\\mathcal{L}_{\\text{txt-gen}}( \\text{LLM}(\\mathbf{P}_{X},\\mathbf{F}_{T}),t), \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\mathbf{P}_{X}=\\mathbf{\\Theta}_{X\\to T}(\\mathbf{F}_{X})\\).\n' +
      '\n' +
      'The Input Projector can be achieved directly by a **Linear Projector** or Multi-Layer Perceptron (**MLP**), _i.e._, several linear projectors interleaved with non-linear activation functions. There are also more complex implementations like **Cross-attention**, **Q-Former**Li et al. (2023), or **P-Former**Jian et al. (2023). **Cross-attention** uses a set of trainable vectors as queries and the encoded features \\(\\mathbf{F}_{X}\\) as keys to compress the feature sequence to a fixed length. The compressed representation is then fed directly into the LLM Bai et al. (2023) or further used for X-text cross-attention fusion Alayrac et al. (2022). **Q-Former** extracts relevant features from \\(\\mathbf{F}_{X}\\), and the selected features are then used as prompts \\(\\mathbf{P}_{X}\\). Meanwhile, **P-Former** generates\'reference prompts\', imposing an alignment constraint on the prompts produced by Q-Former. However, both Q- and P-Former require a **separate PT** process for initialization.\n' +
      '\n' +
      '### LLM Backbone\n' +
      '\n' +
      'Taking LLMs Zhao et al. (2023); Naveed et al. (2023); Luo et al. (2023) as the core agents, MM-LLMs can inherit some notable properties like zero-shot generalization, few-shot ICL, Chain-of-Thought (CoT), and instruction following. The\n' +
      '\n' +
      'Figure 2: The general model architecture of MM-LLMs and the implementation choices for each component.\n' +
      '\n' +
      'LLM Backbone processes representations from various modalities, engaging in semantic understanding, reasoning, and decision-making regarding the inputs. It produces (1) direct textual outputs \\(t\\), and (2) signal tokens \\(\\mathbf{S}_{X}\\) from other modalities (if any). These signal tokens act as instructions to guide the generator on whether to produce MM contents and, if affirmative, specifying the content to produce:\n' +
      '\n' +
      '\\[t,\\,\\mathbf{S}_{X}=\\text{LLM}(\\mathbf{P}_{X},\\mathbf{F}_{T}), \\tag{3}\\]\n' +
      '\n' +
      'where the aligned representations of other modalities \\(\\mathbf{P}_{X}\\) can be considered as soft Prompt-tuning for the LLM Backbone. Moreover, some research works have introduced Parameter-Efficient Fine-Tuning (PEFT) methods, such as Prefix-tuning Li and Liang (2021), Adapter Houlsby et al. (2019), and LoRA Hu et al. (2021). In these cases, the number of additional trainable parameters is exceptionally minimal, even less than 0.1% of the total LLM parameter count. We provide an introduction to mainstream PEFT methods in Appendix B.\n' +
      '\n' +
      'The commonly used LLMs in MM-LLMs include **Flan-T5**Chung et al. (2022), **ChatGLM**Zeng et al. (2022), **UL2**Tay et al. (2022), **Qwen**Bai et al. (2023), **Chinchilla**Hoffmann et al. (2022), **OPT**Zhang et al. (2022), **PaLM**Chowdhery et al. (2023), **LLaMA**Touvron et al. (2023), **LLaMA-2**Touvron et al. (2023), and **Vicuna**Chiang et al. (2023). We provide a brief introduction to these models in Appendix C.\n' +
      '\n' +
      '### Output Projector\n' +
      '\n' +
      'The Output Projector \\(\\mathbf{\\Theta}_{T\\to X}\\) maps the signal token representations \\(\\mathbf{S}_{X}\\) from the LLM Backbone into features \\(\\mathbf{H}_{X}\\) understandable to the following Modality Generator \\(\\text{MG}_{X}\\). Given the \\(X\\)-text dataset \\(\\{I_{X},t\\}\\), \\(t\\) is first fed into LLM to generate the corresponding \\(\\mathbf{S}_{X}\\), then mapped into \\(\\mathbf{H}_{X}\\). To facilitate alignment of the mapped features \\(\\mathbf{H}_{X}\\), the goal is to minimize the distance between \\(\\mathbf{H}_{X}\\) and the conditional text representations of \\(\\text{MG}_{X}\\):\n' +
      '\n' +
      '\\[\\operatorname*{arg\\,min}_{\\mathbf{\\Theta}_{T\\to X}}\\mathcal{L}_{\\text{mse}}(\\mathbf{ H}_{X},\\tau_{X}(t)). \\tag{4}\\]\n' +
      '\n' +
      'The optimization only relies on captioning texts, without utilizing any audio or visual resources \\(X\\), where \\(\\mathbf{H}_{X}=\\mathbf{\\Theta}_{T\\to X}(\\mathbf{S}_{X})\\) and \\(\\tau_{X}\\) is the textual condition encoder in \\(\\text{MG}_{X}\\). The Output Projector is implemented by a **Tiny Transformer** or **MLP**.\n' +
      '\n' +
      '### Modality Generator\n' +
      '\n' +
      'The Modality Generator \\(\\text{MG}_{X}\\) is tasked with producing outputs in distinct modalities. Commonly, existing works use off-the-shelf Latent Diffusion Models (LDMs) Zhao et al. (2022), _i.e._, **Stable Diffusion**Rombach et al. (2022) for image synthesis, **Zeroscope**Cerspense (2023) for video synthesis, and **AudioLDM-2**Liu et al. (2023, 2023) for audio synthesis. The features \\(\\mathbf{H}_{X}\\) mapped by the Output Projector serve as conditional inputs in the denoising process to generate MM content. During training, the ground truth content is first transformed into a latent feature \\(z_{0}\\) by the pre-trained VAE Kingma and Welling (2013). Then, noise \\(\\epsilon\\) is added to \\(z_{0}\\) to obtain the noisy latent feature \\(z_{t}\\). A pre-trained Unet Ronneberger et al. (2015)\\(\\epsilon_{X}\\) is used to compute the conditional LDM loss \\(\\mathcal{L}_{\\text{X-gen}}\\) as follows:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{X-gen}}:=\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,1),t}|| \\epsilon-\\epsilon_{X}(z_{t},t,\\mathbf{H}_{X})||_{2}^{2}, \\tag{5}\\]\n' +
      '\n' +
      'optimize parameters \\(\\mathbf{\\Theta}_{X\\to T}\\) and \\(\\mathbf{\\Theta}_{T\\to X}\\) by minimizing \\(\\mathcal{L}_{\\text{X-gen}}\\).\n' +
      '\n' +
      '## 3 Training Pipeline\n' +
      '\n' +
      'MM-LLMs\' training pipeline can be delineated into two principal stages: MM PT and MM IT.\n' +
      '\n' +
      '### Mm Pt\n' +
      '\n' +
      'During the PT stage, typically leveraging the X-Text datasets, Input and Output Projectors are trained to achieve alignment among various modalities by optimizing predefined objectives (PEFT is sometimes applied to the LLM Backbone). For MM understanding models, optimization focuses solely on Equation (2), while for MM generation models, optimization involves Equations (2), (4), and (5). In the latter case, Equation (2) also includes the ground-truth signal token sequence.\n' +
      '\n' +
      'The X-Text datasets encompass Image-Text, Video-Text, and Audio-Text, with Image-Text having two types: Image-Text pairs ("<im1><tx1>") and interleaved Image-Text corpus ("<txt1><im1><txt2><txt3><im2><txt4>"). The detailed statistics for these X-Text datasets are presented in Table 3 of Appendix F.\n' +
      '\n' +
      '### Mm It\n' +
      '\n' +
      'MM IT is a methodology that entails the fine-tuning of pre-trained MM-LLMs using a set of instruction-formatted datasets Wei et al. (2021). Through this tuning process, MM-LLMs can generalize to unseen tasks by adhering to new instructions, thereby enhancing zero-shot performance. This straightforward yet impactful concept has catalyzed the success of subsequent endeavors in the field of NLP,exemplified by works such as InstructGPT [14], OPT-IML [15], and InstructBLIP [16].\n' +
      '\n' +
      'MM IT comprises Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), aiming to align with human intents or preferences and enhance the interaction capabilities of MM-LLMs. SFT converts part of the PT stage data into an instruction-aware format. Using visual Question-Answer (QA) as an example, various templates may be employed like **(1) <Image>{Question}** A short answer to the question is; **(2) <Image>**Examine the image and respond to the following question with a brief answer: **{Question}**. **Answer**::, and so on. Next, it fine-tunes the pre-trained MM-LLMs using the same optimization objectives. The SFT dataset can be structured as either single-turn QA or multi-turn dialogues.\n' +
      '\n' +
      'After SFT, RLHF involves further fine-tuning of the model, relying on feedback regarding the MM-LLMs\' responses (_e.g._, Natural Language Feedback (NLF) labeled manually or automatically) [20]. This process employs a reinforcement learning algorithm to effectively integrate the non-differentiable NLF. The model is trained to generate corresponding responses conditioned on the NLF [13, 1]. The statistics for SFT and RLHF datasets are presented in Table 4 of Appendix F.\n' +
      '\n' +
      'The datasets used by existing MM-LLMs in the MM PT and MM IT stages are diverse, but they are all **subsets** of the datasets in Tables 3 and 4.\n' +
      '\n' +
      '## 4 SOTA MM-LLMs\n' +
      '\n' +
      'Based on the previously defined design formulations, we conduct a comprehensive comparison of the architectures and training dataset scales for \\(26\\) SOTA MM-LLMs, as illustrated in Table 1. Subsequently, we will provide a concise introduction to the core contributions of these models and summarize their developmental trends.\n' +
      '\n' +
      '**(1) Flamingo**[1] represents a series of Visual Language (VL) Models designed for processing interleaved visual data and text, generating free-form text as the output. **(2) BLIP-2**[11] introduces a more resource-efficient framework, comprising the lightweight Q-Former to bridge modality gaps and the utilization of frozen LLMs. Leveraging LLMs, BLIP-2 can be guided for zero-shot image-to-text generation using natural language prompts. **(3) LLaVA**[11] pioneers the transfer of IT techniques to the MM domain. Addressing data scarcity, LLaVA introduces a novel open-source MM instruction-following dataset created using ChatGPT/GPT-4, alongside the MM instruction-following benchmark, LLaVA-Bench. **(4) MiniGPT-4**[16] proposes a streamlined approach where training only one linear layer aligns the pre-trained vision encoder with the LLM. This efficient method enables the replication of the exhibited capabilities of GPT-4. **(5) mPLUG-Owl**[12] presents a novel modularized training framework for MM-LLMs, incorporating the visual context. To assess different models\' performance in MM tasks, the framework includes an instructional evaluation dataset called OwlEval. **(6) X-LLM**[13] is expanded to various modalities, including audio, and demonstrates strong scalability. Leveraging the language transferability of the Q-Former, X-LLM is successfully applied in the context of Sino-Tibetan Chinese. **(7) VideoChat**[11] pioneers an efficient chat-centric MM-LLM for video understanding dialogue, setting standards for future research in this domain and offering protocols for both academia and industry. **(8) InstructBLIP**[16] is trained based on the pre-trained BLIP-2 model, updating only the Q-Former during MM IT. By introducing instruction-aware visual feature extraction and corresponding instructions, the model enables the extraction of flexible and diverse features. **(9) PandaGPT**[14] is a pioneering general-purpose model with the capability to comprehend and act upon instructions across \\(6\\) different modalities: text, image/video, audio, thermal, depth, and inertial measurement units. **(10) PaLIP-X**[13] is trained using mixed VL objectives and unimodal objectives, including prefix completion and masked-token completion. This approach proves effective for both downstream task results and achieving the Pareto frontier in the fine-tuning setting. **(11) Video-LLaMA**[16] introduces a multi-branch cross-modal PT framework, enabling LLMs to simultaneously process the vision and audio content of a given video while engaging in conversations with humans. This framework aligns vision with language as well as audio with language. **(12) Video-ChatGPT**[16] is a model specifically designed for video conversations, capable of generating discussions about videos by integrating spatiotemporal revision representations. **(13) Shikra**[13] is a \n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      'More Challenging BenchmarksExisting benchmarks might not adequately challenge the capabilities of MM-LLMs, given that many datasets have previously appeared to varying degrees in the PT or IT sets. This implies that the models may have learned these tasks during training. Moreover, current benchmarks predominantly concentrate on the VL sub-field. Thus, it is crucial for the development of MM-LLMs to construct a more challenging, larger-scale benchmark that includes more modalities and uses a unified evaluation standard. Concurrently, benchmarks can be tailored to assess the MM-LLMs\' proficiency in practical applications. For instance, the introduction of GOATBench Lin et al. (2024) aims to evaluate various MM-LLMs\' capacity to discern and respond to nuanced aspects of social abuse presented in memes.\n' +
      '\n' +
      'Mobile/Lightweight DeploymentTo deploy MM-LLMs on resource-constrained platforms and achieve optimal performance meanwhile, such as low-power mobile and IoT devices, lightweight implementations are of paramount importance. A notable advancement in this realm is MobileVLM Chu et al. (2023). This approach strategically downscales LLaMA, allowing for seamless off-the-shelf deployment. MobileVLM further introduces a Lightweight Downsample Projector, consisting of fewer than \\(20\\) million parameters, contributing to improved computational speed. Nevertheless, this avenue necessitates additional exploration for further advancements in development.\n' +
      '\n' +
      'Embodied IntelligenceThe embodied intelligence aims to replicate human-like perception and interaction with the surroundings by effectively understanding the environment, recognizing pertinent objects, assessing their spatial relationships, and devising a comprehensive task plan Firoozi et al. (2023). Embodied AI tasks, such as embodied planning, embodied visual question answering, and embodied control, equips robots to autonomously implement extended plans by leveraging real-time observations. Some typical work in this area is PaLM-E Driess et al. (2023) and EmbodiedGPT Mu et al. (2023). PaLM-E introduces a multi-embodiment agent through the training of a MM-LLM. Beyond functioning solely as an embodied decision maker, PaLM-E also demonstrates proficiency in handling general VL tasks. EmbodiedGPT introduces an economically efficient method characterized through a CoT approach, enhancing the capability of embodied agents to engage with the real world and establishing a closed loop that connects high-level planning with low-level control. While MM-LLM-based Embodied Intelligence has made advancements in integrating with robots, further exploration is needed to enhance the autonomy of robots.\n' +
      '\n' +
      'Continual ITIn practical applications, MM-LLMs are expected to adapt to new MM tasks for supporting additional functionalities. Nevertheless, current MM-LLMs remain static and are unable to adjust to continuously emerging requirements. Therefore, an approach is needed to make the model flexible enough to efficiently and continually leverage emerging data, while avoiding the substantial cost of retraining MM-LLMs. This aligns with the principles of continual learning, where models are designed to incrementaly learn new tasks similar to human learning. Continual IT aims to continuously fine-tune MM-LLMs for new MM tasks while maintaining superior performance on tasks learned during the original MM IT stage. It introduces two primary challenges: (1) catastrophic forgetting, where models forget previous knowledge when learning new tasks Robins (1995); McCloskey and Cohen (1989); Goodfellow et al. (2013); Zhang et al. (2023); Zheng et al. (2023), and (2) negative forward transfer, indicating that the performance of unseen tasks is declined when learning new ones Zheng et al. (2024); Dong et al. (2023, 2023). Recently, He et al. established a benchmark to facilitate the development of continual IT for MM-LLMs. Despite these advancements, there is still a significant opportunity and room for improvement in developing better methods to address the challenges of catastrophic forgetting and negative forward transfer.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'In this paper, we have presented a comprehensive survey of MM-LLMs with a focus on recent advancements. Initially, we categorize the model architecture into five components, providing a detailed overview of general design formulations and training pipelines. Subsequently, we introduce various SOTA MM-LLMs, each distinguished by its specific formulations. Our survey also sheds light on their capabilities across diverse MM benchmarks and envisions future developments in this rapidly evolving field. We hope this survey can provide insights for researchers, contributing to the ongoing advancements in the MM-LLMs domain.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'In this paper, we embark on a comprehensive exploration of the current MM-LLMs landscape, presenting a synthesis from diverse perspectives enriched by our insights. Acknowledging the dynamic nature of this field, it is plausible that certain aspects may have eluded our scrutiny, and recent advances might not be entirely encapsulated. To tackle this inherent challenge, we\'ve established a dedicated website for real-time tracking, using crowdsourcing to capture the latest advancements. Our goal is for this platform to evolve into a continuous source of contributions propelling ongoing development in the field. Given the constraints of page limits, we are unable to delve into all technical details and have provided concise overviews of the core contributions of mainstream MM-LLMs. Looking ahead, we commit to vigilant monitoring and continual enhancement of relevant details on our website, incorporating fresh insights as they emerge.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Asai, S. Min, Z. Zhong, and D. Chen (2023)Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pp. 41-46. Cited by: SS1.\n' +
      '* E. Aiello, L. Yu, Y. Nie, A. Aghajanyan, and B. Oguz (2023)Jointly training large autoregressive multimodal models. arXiv preprint arXiv:2309.15564. Cited by: SS1.\n' +
      '* H. Akbari, L. Yuan, R. Qian, W. Chuang, S. Chang, Y. Cui, and B. Gong (2021)Vatt: transformers for multimodal self-supervised learning from raw video, audio and text. Advances in Neural Information Processing Systems34, pp. 24206-24221. Cited by: SS1.\n' +
      '* A. Feyza Akyurek, E. Akyurek, A. Madaan, A. Kalyan, P. Clark, D. Wijaya, and N. Tandon (2023)RL4F: generating natural language feedback with reinforcement learning for repairing model outputs. arXiv preprint arXiv:2305.08844. Cited by: SS1.\n' +
      '* J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. (2022)Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems35, pp. 23716-23736. Cited by: SS1.\n' +
      '* A. Asadi, S. Min, Z. Zhong, and D. Chen (2023)Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pp. 41-46. Cited by: SS1.\n' +
      '* A. Asadi, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Gadre, S. Sagawa, et al. (2023)Openflamingo: an open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390. Cited by: SS1.\n' +
      '* J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. (2023)Qwen technical report. arXiv preprint arXiv:2309.16609. Cited by: SS1.\n' +
      '* J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou (2023)Qwen-VL: a frontier large vision-language model with versatile abilities. CoRRabs/2308.12966. Cited by: SS1.\n' +
      '* M. Bain, A. Nagrani, G. Varol, and A. Zisserman (2021)Frozen in time: a joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1728-1738. Cited by: SS1.\n' +
      '* R. Bavishi, E. Elsen, C. Hawthorne, M. Nye, A. Odena, A. Somani, and S. Tasirlar (2023)Introducing our multimodal models. Cited by: SS1.\n' +
      '* A. Biten, R. Litman, Y. Xie, S. Appalaraju, and R. Manmatha (2022)LATT: layout-aware transformer for scene-text vqa. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16548-16558. Cited by: SS1.\n' +
      '* A. Brock, S. De, S. L. Smith, and K. Simonyan (2021)High-performance large-scale image recognition without normalization. In International Conference on Machine Learning, pp. 1059-1071. Cited by: SS1.\n' +
      '* T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020)Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.\n' +
      '* M. Byeon, B. Park, H. Kim, S. Lee, W. Baek, and S. Kim (2022)Coyo-700m: image-text pair dataset. Cited by: SS1.\n' +
      '* F. C. Heilbron, V. Escorcia, B. Ghanem, and J. C. Niebles (2015)Activitynet: a large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pp. 961-970. Cited by: SS1.\n' +
      '* C. C. (2023)Zeroscope: diffusion-based text-to-video synthesis. Cited by: SS1.\n' +
      '\n' +
      '* Chen et al. (2023a) Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xi-Yi Chen, Jing Shi, Shuang Xu, and Bo Xu. 2023a. Vlp: A survey on vision-language pre-training. _Machine Intelligence Research_, 20(1):38-56.\n' +
      '* Chen et al. (2023b) Feilong Chen, Minglu Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. 2023b. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. _arXiv preprint arXiv:2305.04160_.\n' +
      '* Chen et al. (2023c) Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023c. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. _arXiv preprint arXiv:2310.09478_.\n' +
      '* Chen et al. (2023d) Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. 2023d. Shikra: Unleashing Multimodal LLM\'s Referential Dialogue Magic. _arXiv preprint arXiv:2306.15195_.\n' +
      '* Chen et al. (2023e) Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023e. ShareGPT4V: Improving Large Multi-Modal Models with Better Captions. _arXiv preprint arXiv:2311.12793_.\n' +
      '* Chen et al. (2023f) Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, Wanxiang Che, Xiangzhan Yu, and Furu Wei. 2023f. BEATs: Audio Pre-Training with Acoustic Tokenizers. In _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, pages 5178-5193.\n' +
      '* Chen et al. (2022a) Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. 2022a. Adaptformer: Adapting vision transformers for scalable visual recognition. _Advances in Neural Information Processing Systems_, 35:16664-16678.\n' +
      '* Chen et al. (2023g) Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. 2023g. PaLI-X: On Scaling up a Multilingual Vision and Language Model. _arXiv preprint arXiv:2305.18565_.\n' +
      '* Chen et al. (2022b) Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. 2022b. Pali: A jointly-scaled multilingual language-image model. _arXiv preprint arXiv:2209.06794_.\n' +
      '* Chen et al. (2015) Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishn Vedantam, Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. _arXiv preprint arXiv:1504.00325_.\n' +
      '* Chen et al. (2023h) Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. 2023h. Dress: Instructing large vision-language models to align and interact with humans via natural language feedback. _arXiv preprint arXiv:2311.10081_.\n' +
      '* Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* Chat-GPT Quality.\n' +
      '* Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(240):1-113.\n' +
      '* Chu et al. (2023a) Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. 2023a. Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices. _arXiv preprint arXiv:2312.16886_.\n' +
      '* Chu et al. (2023b) Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. 2023b. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. _arXiv preprint arXiv:2311.07919_.\n' +
      '* Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_.\n' +
      '* Cui et al. (2024) Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, et al. 2024. A survey on multimodal large language models for autonomous driving. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 958-979.\n' +
      '* Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. 2023. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. In _Thirty-seventh Conference on Neural Information Processing Systems_.\n' +
      '* Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized l1ms. _arXiv preprint arXiv:2305.14314_.\n' +
      '* Dong et al. (2023a) Jiahua Dong, Wenqi Liang, Yang Cong, and Gan Sun. 2023a. Heterogeneous forgetting compensation for class-incremental learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11742-11751.\n' +
      '* Dong et al. (2023b) Jiahua Dong, Duzhen Zhang, Yang Cong, Wei Cong, Henghui Ding, and Dengxin Dai. 2023b. Federated Incremental Semantic Segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3934-3943.\n' +
      '\n' +
      'Linhao Dong and Bo Xu. 2020. Cif: Continuous integrate-and-fire for end-to-end speech recognition. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 6079-6083. IEEE.\n' +
      '* Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In _International Conference on Learning Representations_.\n' +
      '* Driess et al. (2023) Danny Driess, Fei Xia, Mehdi SM Sajiadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023. Palm-e: An embodied multimodal language model. _arXiv preprint arXiv:2303.03378_.\n' +
      '* Du et al. (2022a) Yifan Du, Zikang Liu, Junyi Li, and Wayne Xin Zhao. 2022a. A Survey of Vision-Language Pre-Trained Models. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022_, pages 5436-5443.\n' +
      '* Du et al. (2022b) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022b. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335.\n' +
      '* Fang et al. (2021) Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen. 2021. Clip2video: Mastering video-text retrieval via image clip. _arXiv preprint arXiv:2106.11097_.\n' +
      '* Fang et al. (2023) Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. 2023. Eva: Exploring the limits of masked visual representation learning at scale. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19358-19369.\n' +
      '* Feng et al. (2023) Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang Li, and Can Huang. 2023. DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding. _arXiv preprint arXiv:2311.11810_.\n' +
      '* Firoozi et al. (2023) Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, et al. 2023. Foundation Models in Robotics: Applications, Challenges, and the Future. _arXiv preprint arXiv:2312.07843_.\n' +
      '* Fu et al. (2023) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. 2023. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_.\n' +
      '* Fu et al. (2022) Chin-Lun Fu, Zih-Ching Chen, Yun-Ru Lee, and Hung-Yi Lee. 2022. AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks. In _Findings of the Association for Computational Linguistics: NAACL 2022_, pages 2608-2621.\n' +
      '* Gadre et al. (2023) Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. 2023. Datacomp: In search of the next generation of multimodal datasets. _arXiv preprint arXiv:2304.14108_.\n' +
      '* Girdhar et al. (2023) Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. Imagebind: One embedding space to bind them all. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15180-15190.\n' +
      '* Gong et al. (2023) Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. 2023. Multimodal-gpt: A vision and language model for dialogue with humans. _arXiv preprint arXiv:2305.04790_.\n' +
      '* Goodfellow et al. (2013) Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. 2013. An empirical investigation of catastrophic forgetting in gradient-based neural networks. _arXiv preprint arXiv:1312.6211_.\n' +
      '* Goyal et al. (2017) Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6904-6913.\n' +
      '* Gu et al. (2022) Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et al. 2022. Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. _Advances in Neural Information Processing Systems_, 35:26418-26431.\n' +
      '* Gurari et al. (2018) Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. 2018. Vizwix grand challenge: Answering visual questions from blind people. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3608-3617.\n' +
      '* He et al. (2023) Jinghan He, Haiyun Guo, Ming Tang, and Jinqiao Wang. 2023. Continual instruction tuning for large multimodal models. _arXiv preprint arXiv:2311.16206_.\n' +
      '* He et al. (2021) Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021. Towards a Unified View of Parameter-Efficient Transfer Learning. In _International Conference on Learning Representations_.\n' +
      '\n' +
      'Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778.\n' +
      '* Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_.\n' +
      '* Honovich et al. (2022) Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. Unnatural instructions: Tuning language models with (almost) no human labor. _arXiv preprint arXiv:2212.09689_.\n' +
      '* Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In _International Conference on Machine Learning_, pages 2790-2799. PMLR.\n' +
      '* Hsu et al. (2021) Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 29:3451-3460.\n' +
      '* Hu et al. (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. LoRA: Low-Rank Adaptation of Large Language Models. In _International Conference on Learning Representations_.\n' +
      '* Huang et al. (2023a) Jiaxing Huang, Jingyi Zhang, Kai Jiang, Han Qiu, and Shijian Lu. 2023a. Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey. _arXiv preprint arXiv:2312.16602_.\n' +
      '* Huang et al. (2023b) Rongjie Huang, Mingze Li, Dongchao Yang, Jia-tong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. 2023b. Audioopt: Understanding and generating speech, music, sound, and talking head. _arXiv preprint arXiv:2304.12995_.\n' +
      '* Huang et al. (2023c) Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. 2023c. Language is not all you need: Aligning perception with language models. _arXiv preprint arXiv:2302.14045_.\n' +
      '* Hudson and Manning (2019) Drew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6700-6709.\n' +
      '* IDEFICS (2023) IDEFICS. 2023. Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model.\n' +
      '* Iyer et al. (2022) Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. _arXiv preprint arXiv:2212.12017_.\n' +
      '* Jia et al. (2021) Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In _International conference on machine learning_, pages 4904-4916. PMLR.\n' +
      '* Jian et al. (2023) Yiren Jian, Chongyang Gao, and Soroush Vosoughi. 2023. Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. In _Thirty-seventh Conference on Neural Information Processing Systems_.\n' +
      '* Kafle et al. (2018) Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. 2018. Dvqa: Understanding data visualizations via question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5648-5656.\n' +
      '* Mahabadi et al. (2021) Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. 2021. Compacter: Efficient low-rank hypercomplex adapter layers. _Advances in Neural Information Processing Systems_, 34:1022-1035.\n' +
      '* Kazemzadeh et al. (2014) Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. 2014. Referitgame: Referring to objects in photographs of natural scenes. In _Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)_, pages 787-798.\n' +
      '* Meng-Wei Chang Kenton and Toutanova (2019) Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _Proceedings of NAACL-HLT_, pages 4171-4186.\n' +
      '* Kiela et al. (2020) Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: Detecting hate speech in multimodal memes. _Advances in neural information processing systems_, 33:2611-2624.\n' +
      '* Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_.\n' +
      '* Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _International journal of computer vision_, 123:32-73.\n' +
      '* Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 3045-3059.\n' +
      '\n' +
      'Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. 2023a. Mimic-it: Multi-modal in-context instruction tuning. _arXiv preprint arXiv:2306.05425_.\n' +
      '* Li et al. (2023b) Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023b. Seed-bench: Benchmarking multimodal lms with generative comprehension. _arXiv preprint arXiv:2307.16125_.\n' +
      '* Li et al. (2023c) Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023c. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, pages 19730-19742.\n' +
      '* Li et al. (2022) Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, pages 12888-12900. PMLR.\n' +
      '* Li et al. (2021) Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language representation learning with momentum distillation. _Advances in neural information processing systems_, 34:9694-9705.\n' +
      '* Li et al. (2023d) KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023d. Videochat: Chat-centric video understanding. _arXiv preprint arXiv:2305.06355_.\n' +
      '* Li et al. (2023e) Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. 2023e. M\\({}^{3}\\)IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning. _arXiv preprint arXiv:2306.04387_.\n' +
      '* Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 4582-4597.\n' +
      '* Li et al. (2020) Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. 2020. Oscar: Object-semantics aligned pre-training for vision-language tasks. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXX 16_, pages 121-137. Springer.\n' +
      '* Li et al. (2023f) Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin Fu, Guosheng Lin, Chunhua Shen, Ling Chen, and Yunchao Wei. 2023f. Stabellava: Enhanced visual instruction tuning with synthesized image-dialogue data. _arXiv preprint arXiv:2308.10253_.\n' +
      '* Li et al. (2023g) Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023g. Evaluating object hallucination in large vision-language models. _arXiv preprint arXiv:2305.10355_.\n' +
      '* Li et al. (2023h) Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. 2023h. Monkey: Image Resolution and Text Label Are Important Things for Large Multimodal Models. _arXiv preprint arXiv:2311.06607_.\n' +
      '* Lin et al. (2024) Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang, and Jing Ma. 2024. GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. _arXiv preprint arXiv:2401.01523_.\n' +
      '* Lin et al. (2023) Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. 2023. VLA: On Pre-training for Visual Language Models. _arXiv preprint arXiv:2312.07533_.\n' +
      '* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer.\n' +
      '* Liu et al. (2023a) Fangyu Liu, Guy Emerson, and Nigel Collier. 2023a. Visual spatial reasoning. _Transactions of the Association for Computational Linguistics_, 11:635-651.\n' +
      '* Liu et al. (2023b) Haobe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo P. Mandic, Wenwu Wang, and Mark D. Plumbley. 2023b. AudioLDM: Text-to-Audio Generation with Latent Diffusion Models. In _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, pages 21450-21474.\n' +
      '* Liu et al. (2023c) Haobe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. 2023c. Audio-cDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining. _CoRR_, abs/2308.05734.\n' +
      '* Liu et al. (2023d) Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023d. Improved Baselines with Visual Instruction Tuning. In _NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following_.\n' +
      '* Liu et al. (2023e) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023e. Visual Instruction Tuning. In _Thirty-seventh Conference on Neural Information Processing Systems_.\n' +
      '* Liu et al. (2023f) Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2023f. Mmbench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281_.\n' +
      '* Long et al. (2022) Siqu Long, Feiqi Cao, Soyeon Caren Han, and Haiqin Yang. 2022. Vision-and-Language Pretrained Models: A Survey. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022_, pages 5530-5537.\n' +
      '\n' +
      'Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. _Advances in Neural Information Processing Systems_, 35:2507-2521.\n' +
      '* Lu et al. (2021) Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. 2021. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_.\n' +
      '* Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. _arXiv preprint arXiv:2306.08568_.\n' +
      '* Maaz et al. (2023) Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models. _arXiv preprint arXiv:2306.05424_.\n' +
      '* Mathew et al. (2021) Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021. Docvqa: A dataset for vqa on document images. In _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pages 2200-2209.\n' +
      '* McCloskey and Cohen (1989) Michael McCloskey and Neal J Cohen. 1989. Catastrophic interference in connectionist networks: The sequential learning problem. In _Psychology of learning and motivation_, volume 24, pages 109-165. Elsevier.\n' +
      '* Mei et al. (2023) Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley, Yuexian Zou, and Wenwu Wang. 2023. Wvacaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research. _arXiv preprint arXiv:2303.17395_.\n' +
      '* Mishra et al. (2019) Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. 2019. Ocr-vqa: Visual question answering by reading text in images. In _2019 international conference on document analysis and recognition (ICDAR)_, pages 947-952. IEEE.\n' +
      '* Mu et al. (2023) Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. 2023. Embodiedgpt: Vision-language pre-training via embodied chain of thought. In _Thirty-seventh Conference on Neural Information Processing Systems_.\n' +
      '* Naveed et al. (2023) Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Nick Barnes, and Ajmal Mian. 2023. A comprehensive overview of large language models. _arXiv preprint arXiv:2307.06435_.\n' +
      '* OpenAI (2022) OpenAI. 2022. OpenAI: Introducing ChatGPT.\n' +
      '* OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report.\n' +
      '* Ordonez et al. (2011) Vicente Ordonez, Girish Kulkarni, and Tamara Berg. 2011. Im2text: Describing images using 1 million captioned photographs. _Advances in neural information processing systems_, 24.\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744.\n' +
      '* Panagopoulou et al. (2023) Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese, Caiming Xiong, and Juan Carlos Niebles. 2023. X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning. _arXiv preprint arXiv:2311.18799_.\n' +
      '* Peng et al. (2023) Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023. Kosmos-2: Grounding Multimodal Large Language Models to the World. _arXiv preprint arXiv:2306.14824_.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR.\n' +
      '* Radford et al. (2023) Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust Speech Recognition via Large-Scale Weak Supervision. In _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, pages 28492-28518.\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551.\n' +
      '* Rebuffi et al. (2017) Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. _Advances in neural information processing systems_, 30.\n' +
      '* Robins (1995) Anthony Robins. 1995. Catastrophic forgetting, re-hearsal and pseudorehearsal. _Connection Science_, 7(2):123-146.\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695.\n' +
      '* Rombach et al. (2019)Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer.\n' +
      '* Ruan and Jin (2022) Ludan Ruan and Qin Jin. 2022. Survey: Transformer based video-language pre-training. _AI Open_, 3:1-13.\n' +
      '* Salesforce (2022) Salesforce. 2022. Ulip.\n' +
      '* Schuhmann et al. (2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294.\n' +
      '* Schuhmann et al. (2022b) Christoph Schuhmann, Andreas Kopf, Richard Vencu, Theo Coombes, and Romain Beaumont. 2022b. Laion coco: 600m synthetic captions from laion2b-en.\n' +
      '* Schuhmann et al. (2021) Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_.\n' +
      '* Schwenk et al. (2022) Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa: A benchmark for visual question answering using world knowledge. In _European Conference on Computer Vision_, pages 146-162. Springer.\n' +
      '* Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2556-2565.\n' +
      '* Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. _arXiv preprint arXiv:2303.17580_.\n' +
      '* Sidorov et al. (2020) Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. 2020. Textcaps: a dataset for image captioning with reading comprehension. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pages 742-758. Springer.\n' +
      '* Singh et al. (2019) Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards vqa models that can read. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8317-8326.\n' +
      '* Song et al. (2023) Shezheng Song, Xiaopeng Li, and Shasha Li. 2023. How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model. _arXiv preprint arXiv:2311.07594_.\n' +
      '* Su et al. (2023) Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023. Pandagpt: One model to instruction-follow them all. _arXiv preprint arXiv:2305.16355_.\n' +
      '* Sun et al. (2023) Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. 2023. Aligning large multimodal models with factually augmented rhlf. _arXiv preprint arXiv:2309.14525_.\n' +
      '* Suris et al. (2023) Didac Suris, Sachit Menon, and Carl Vondrick. 2023. Vipergrpt: Visual inference via python execution for reasoning. _arXiv preprint arXiv:2303.08128_.\n' +
      '* Tang et al. (2023) Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. 2023a. CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation. _arXiv preprint arXiv:2311.18775_.\n' +
      '* Tang et al. (2023b) Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. 2023b. Any-to-Any Generation via Composable Diffusion. In _Thirty-seventh Conference on Neural Information Processing Systems_.\n' +
      '* Tay et al. (2022) Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. 2022. Ul2: Unifying language learning paradigms. In _The Eleventh International Conference on Learning Representations_.\n' +
      '* Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_.\n' +
      '* Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_.\n' +
      '* Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. _Advances in neural information processing systems_, 30.\n' +
      '* Vaswani et al. (2017)Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022a. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In _International Conference on Machine Learning_, pages 23318-23340. PMLR.\n' +
      '* Wang et al. (2022) Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. 2023. Cogvlm: Visual expert for pretrained language models. _arXiv preprint arXiv:2311.03079_.\n' +
      '* Wang et al. (2022b) Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. 2022b. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. _arXiv preprint arXiv:2208.10442_.\n' +
      '* Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned Language Models are Zero-Shot Learners. In _International Conference on Learning Representations_.\n' +
      '* Wu et al. (2023a) Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023a. Visual chatpt: Talking, drawing and editing with visual foundation models. _arXiv preprint arXiv:2303.04671_.\n' +
      '* Wu et al. (2023b) Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. 2023b. Qbench: A benchmark for general-purpose foundation models on low-level vision. _arXiv preprint arXiv:2309.14181_.\n' +
      '* Wu et al. (2017) Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan, Rui Liang, Wenjia Wang, Shipei Zhou, Guosen Lin, Yanwei Fu, et al. 2017. Ai challenger: A large-scale dataset for going deeper in image understanding. _arXiv preprint arXiv:1711.06475_.\n' +
      '* Wu et al. (2023c) Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Philip S Yu. 2023c. Multimodal large language models: A survey. _arXiv preprint arXiv:2311.13165_.\n' +
      '* Wu et al. (2023d) Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023d. Next-gpt: Any-to-any multimodal llm. _arXiv preprint arXiv:2309.05519_.\n' +
      '* Xu et al. (2016) Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-vtt: A large video description dataset for bridging video and language. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5288-5296.\n' +
      '* Xu et al. (2023a) Rongtao Xu, Changwei Wang, Jiaxi Sun, Shibiao Xu, Weiliang Meng, and Xiaopeng Zhang. 2023a. Self Correspondence Distillation For End-to-End Weakly-Supervised Semantic Segmentation. In _Proceedings of the AAAI Conference on Artificial Intelligence_.\n' +
      '* Xu et al. (2023b) Rongtao Xu, Changwei Wang, Jiguang Zhang, Shibiao Xu, Weiliang Meng, and Xiaopeng Zhang. 2023b. Rssformer: Foreground saliency enhancement for remote sensing land-cover segmentation. _IEEE Transactions on Image Processing_, 32:1052-1064.\n' +
      '* Yan et al. (2021) Rui Yan, Mike Zheng Shou, Yixiao Ge, Alex Jinpeng Wang, Xudong Lin, Guanyu Cai, and Jinhui Tang. 2021. Video-text pre-training with learned regions. _arXiv preprint arXiv:2112.01194_.\n' +
      '* Yang et al. (2022) Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. 2022. Vision-language pre-training with triple contrastive learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15671-15680.\n' +
      '* Yang et al. (2023) Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-react: Prompting chatgpt for multimodal reasoning and action. _arXiv preprint arXiv:2303.11381_.\n' +
      '* Ye et al. (2023) Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality. _arXiv preprint arXiv:2304.14178_.\n' +
      '* Yin et al. (2023a) Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023a. A Survey on Multimodal Large Language Models. _arXiv preprint arXiv:2306.13549_.\n' +
      '* Yin et al. (2023b) Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, et al. 2023b. Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. _arXiv preprint arXiv:2306.06687_.\n' +
      '* Young et al. (2014) Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_, 2:67-78.\n' +
      '* Yu et al. (2016) Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. 2016. Modeling context in referring expressions. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14_, pages 69-85. Springer.\n' +
      '* Yu et al. (2023) Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. _arXiv preprint arXiv:2308.02490_.\n' +
      '* Yu et al. (2022) Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. 2022. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1041-1042.\n' +
      '\n' +
      'Conference on Computer Vision and Pattern Recognition_, pages 19313-19322.\n' +
      '* Zellers et al. (2022) Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. 2022. Merlot reserve: Neural script knowledge through vision and language and sound. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16375-16387.\n' +
      '* Zeng et al. (2022a) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022a. GLM-130B: An Open Bilingual Pre-trained Model. In _The Eleventh International Conference on Learning Representations_.\n' +
      '* Zeng et al. (2022b) Yan Zeng, Xinsong Zhang, and Hang Li. 2022b. Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. In _International Conference on Machine Learning_, pages 25994-26009. PMLR.\n' +
      '* Zhang et al. (2023a) Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023a. SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities. In _Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023_, pages 15757-15773.\n' +
      '* Zhang et al. (2023b) Duzhen Zhang, Wei Cong, Jiahua Dong, Yahan Yu, Xiuyi Chen, Yonggang Zhang, and Zhen Fang. 2023b. Continual Named Entity Recognition without Catastrophic Forgetting. In _The 2023 Conference on Empirical Methods in Natural Language Processing_.\n' +
      '* Zhang et al. (2023c) Duzhen Zhang, Hongliu Li, Wei Cong, Rongtao Xu, Jiahua Dong, and Xiuyi Chen. 2023c. Task relation distillation and prototypical pseudo label for incremental named entity recognition. In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, pages 3319-3329.\n' +
      '* Zhang et al. (2023d) Duzhen Zhang, Yahan Yu, Feilong Chen, and Xiuyi Chen. 2023d. Decomposing Logits Distillation for Incremental Named Entity Recognition. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 1919-1923.\n' +
      '* Zhang et al. (2022a) Duzhen Zhang, Tielin Zhang, Shuncheng Jia, Qingyu Wang, and Bo Xu. 2022a. Recent Advances and New Frontiers in Spiking Neural Networks. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022_, pages 5670-5677.\n' +
      '* System Demonstrations, Singapore, December 6-10, 2023_, pages 543-553.\n' +
      '* Zhang et al. (2020) Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. 2020. Side-tuning: a baseline for network adaptation via additive side networks. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16_, pages 698-714. Springer.\n' +
      '* Zhang et al. (2020) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022b. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_.\n' +
      '* Zhang et al. (2023f) Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. 2023f. Llavar: Enhanced visual instruction tuning for text-rich image understanding. _arXiv preprint arXiv:2306.17107_.\n' +
      '* Zhao et al. (2023a) Bo Zhao, Boya Wu, and Tiejun Huang. 2023a. Svit: Scaling up visual instruction tuning. _arXiv preprint arXiv:2307.04087_.\n' +
      '* Zhao et al. (2023b) Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong, Chunrui Han, et al. 2023b. Chatspot: Bootstrapping multimodal lms via precise referring instruction tuning. _arXiv preprint arXiv:2307.09474_.\n' +
      '* December 9, 2022_.\n' +
      '* Zhao et al. (2023c) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023c. A survey of large language models. _arXiv preprint arXiv:2303.18223_.\n' +
      '* Zhao et al. (2023d) Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. 2023d. Bubogpt: Enabling visual grounding in multi-modal lms. _arXiv preprint arXiv:2307.08581_.\n' +
      '* Zheng et al. (2024) Junhao Zheng, Qianli Ma, Zhen Liu, Binquan Wu, and Huawen Feng. 2024. Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer. _arXiv preprint arXiv:2401.09181_.\n' +
      '* Zheng et al. (2023a) Junhao Zheng, Shengjie Qiu, and Qianli Ma. 2023a. Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models. _arXiv preprint arXiv:2312.07887_.\n' +
      '* Zheng et al. (2023b) Kaizhi Zheng, Xuehai He, and Xin Eric Wang. 2023b. Minigpt-5: Interleaved vision-and-language generation via generative vokens. _arXiv preprint arXiv:2310.02239_.\n' +
      '* Zhu et al. (2023a) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023a. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_.\n' +
      '\n' +
      'Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. 2023b. Multimodal c4: An open, billion-scale corpus of images interleaved with text. _arXiv preprint arXiv:2304.06939_.\n' +
      '* Zhu et al. (2016) Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. 2016. Visual7w: Grounded question answering in images. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4995-5004.\n' +
      '\n' +
      '## Appendix A Related Surveys\n' +
      '\n' +
      'Prior to the emergence of LLMs, several surveys on traditional MM PT have been conducted (Ruan and Jin, 2022; Du et al., 2022; Long et al., 2022; Chen et al., 2023a). Most of these models entail a substantial computational cost during the PT phase, attributable to end-to-end training using large-scale models and datasets. As a consequence of not incorporating LLMs, these models suffer from deficiencies in instruction following, ICL, CoT, and interactive capabilities. Moreover, the training pipeline solely encompasses the PT phase without the inclusion of an IT stage.\n' +
      '\n' +
      'In recent times, several surveys have emerged on MM-LLMs. Yin et al. and Wu et al. exclusively delve into early VL understanding models. Huang et al. place a primary emphasis on visual IT, while Song et al. focus on modal alignment methods. Lastly, Cui et al. provide a comprehensive review of the applications of MM-LLMs within the realm of autonomous driving.\n' +
      '\n' +
      'Compared with their works, the main distinctions are outlined as follows:\n' +
      '\n' +
      '* We have comprehensively covered nearly all MM-LLMs over the past year, including not only understanding models but also generative models. Our coverage extends beyond VL modalities to encompass various modes such as audio and 3D;\n' +
      '* To offer readers a comprehensive understanding of MM-LLMs, we have introduced a general model architecture that incorporates any-to-any modality transformations, offering a detailed overview of the functional roles and implementation choices for each component;\n' +
      '* We have summarized the developmental trends of existing MM-LLMs and provided some training recipes that can enhance effectiveness;\n' +
      '* We have established an open-source website for MM-LLMs researchers, supporting crowdsourced updates and aiming to facilitate collaboration in the MM-LLMs field. We anticipate that this survey will illuminate future research in the MM-LLMs domain.\n' +
      '\n' +
      '## Appendix B Mainstream PEFT Methods\n' +
      '\n' +
      'PEFT entails maintaining the pre-trained LLM in a frozen state while adjusting a small number of ad ditional trainable parameters. In the following section, we revisit several representative PEFT methods, where \\(\\mathbf{x}\\) and \\(\\mathbf{h}\\) represent the input and output of the original module, and \\(\\mathbf{h}^{\\prime}\\) signifies the output of this module when attached with PEFT.\n' +
      '\n' +
      'Prefix-tuningLi and Liang (2021); Lester et al. (2021) involves the addition of learnable tokens to the keys and values of the attention module. This process is formulated as follows:\n' +
      '\n' +
      '\\[\\mathbf{h}^{\\prime}=\\operatorname{Attn}\\left(\\mathbf{x}\\mathbf{W}_{q},[ \\mathbf{P}_{k},\\mathbf{x}\\mathbf{W}_{k}],[\\mathbf{P}_{v},\\mathbf{x}\\mathbf{W} _{v}]\\right), \\tag{6}\\]\n' +
      '\n' +
      'with \\(\\mathbf{P}_{k},\\mathbf{P}_{v}\\in\\mathbb{R}^{l\\times d}\\) representing two sets of prefix tokens. \\([\\cdot,\\cdot]\\) denotes concatenation, and \\(\\operatorname{Attn}\\) is defined as:\n' +
      '\n' +
      '\\[\\operatorname{Attn}\\left(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\right):= \\operatorname{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d}}\\right) \\mathbf{V}.\\]\n' +
      '\n' +
      'AdapterHoulsby et al. (2019); He et al. (2021); Rebuffi et al. (2017); Zhang et al. (2020) is typically a residual block consisting of a down-projection matrix \\(\\mathbf{A}\\), a nonlinear activation function \\(\\sigma(\\cdot)\\), and an up-projection matrix \\(\\mathbf{B}\\). It can be inserted into any layer of the pre-trained LLM, formulated as follows:\n' +
      '\n' +
      '\\[\\mathbf{h}^{\\prime}=\\mathbf{h}+\\sigma(\\mathbf{x}\\mathbf{A})\\mathbf{B}. \\tag{7}\\]\n' +
      '\n' +
      'LoRAHu et al. (2021) is the most commonly used PEFT method. It assumes that the change in parameters occurs within a low-rank space. Given a pre-trained matrix \\(\\mathbf{W}\\in\\mathbb{R}^{c\\times d}\\), LoRA learns an incremental update \\(\\Delta\\mathbf{W}\\) and decomposes \\(\\Delta\\mathbf{W}\\) into a matrix multiplication between two low-rank matrices \\(\\mathbf{A}\\in\\mathbb{R}^{c\\times r}\\) and \\(\\mathbf{B}\\in\\mathbb{R}^{r\\times d}\\), where \\(r\\ll\\text{min}(c,d)\\). LoRA follows the forward process as outlined below:\n' +
      '\n' +
      '\\[\\mathbf{h}=\\mathbf{W}\\mathbf{x}+\\Delta\\mathbf{W}\\mathbf{x}=\\mathbf{W}\\mathbf{x}+\\mathbf{A}\\mathbf{B}\\mathbf{x}. \\tag{8}\\]\n' +
      '\n' +
      'QLoRADettmers et al. (2023) is a quantized LoRA. The underlying principle of QLoRA includes the quantization of pre-trained weights to \\(4\\) bits, followed by the execution of PEFT using LoRA.\n' +
      '\n' +
      'In addition to the aforementioned PEFT methods, there are several others, including AdaptBias Fu et al. (2022), CompacterKarimi Mahabadi et al. (2021), and AdapterFormerChen et al. (2022).\n' +
      '\n' +
      '## Appendix C Commonly Used LLMs\n' +
      '\n' +
      'The commonly used LLM Backbones in existing MM-LLMs research are as follows:\n' +
      '\n' +
      '* **Flan-T5**Chung et al. (2022) investigates IT for T5Raffel et al. (2020), an encoder-decoder architecture using unified text-to-text training for all natural language processing issues, exhibiting robust zero-shot and CoT capabilities.\n' +
      '* **ChatGLM2** is a Chinese-English bilingual dialogue model, optimized by an auto-regressive mask infilling objective. It is based on the GLMDu et al. (2022); Zeng et al. (2022) architecture, optimized for Chinese question answering and dialogues. Footnote 2: [https://github.com/THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)\n' +
      '* **UL2**Tay et al. (2022) is an encoder-decoder model trained utilizing a mixture of denoisers objectives, surpassing T5 on numerous benchmarks.\n' +
      '* **Qwen**Bai et al. (2023) is trained on large-scale and diverse datasets, with a primary focus on Chinese and English. It employs SFT and RLHF techniques for alignment, resulting in dialogue models like Qwen-Chat.\n' +
      '* **Chinchilla**Hoffmann et al. (2022) is a causal decoder, trained on extensive text data. It posits that model size should double for every doubling of training tokens.\n' +
      '* **OPT**Zhang et al. (2022) is a GPT-3Brown et al. (2020) clone, striving to release an open-source model that replicates the performance of GPT-3.\n' +
      '* **PaLM**Chowdhery et al. (2023) is a causal decoder structure with parallel attention and feed-forward layers, enabling training speeds up to \\(15\\) times faster. Notable changes contain RoPE embeddings, SwiGLU activation, multi-query attention, and etc.\n' +
      '* **LLaMA**Touvron et al. (2023) comprises decoder-only models with efficient causal attention.\n' +
      '* **LLaMA-2**Touvron et al. (2023) focuses on fine-tuning a superior and safer LLaMA-2-Chat model for conversation generation, incorporating 40% more training data with grouped-query attention and a larger context length.\n' +
      '\n' +
      '* **Vicuna**[14] is a model built on top of LLaMA, utilizing user dialogue data obtained from ShareGPT.com and trained by SFT.\n' +
      '\n' +
      '## Appendix D SOTA MM-LLMs (continued)\n' +
      '\n' +
      '**(20) LLaVA-1.5**[15] reports simple modifications to the LLaVA framework, including applying an MLP projection and introducing VQA data tailored for academic tasks, along with simple response formatting prompts. These adjustments result in enhanced capabilities for MM understanding.\n' +
      '\n' +
      '**(21) MiniGPT-v2**[13] is an MM-LLM designed as a unified interface for diverse VL multi-task learning. To create a single model proficient in handling multiple VL tasks, identifiers are incorporated for each task during both training and inference. This facilitates clear task distinction, ultimately enhancing learning efficiency.\n' +
      '\n' +
      '**(22) CogVLM**[12] is an open-source MM-LLM that bridges the gap between modalities via a trainable visual expert module within the attention and feedforward layers. This allows for a deep fusion of MM features without compromising performance on NLP downstream tasks.\n' +
      '\n' +
      '**(23) DRESS**[13] introduces a method using natural language feedback to enhance alignment with human preferences. DRESS extends the conditional reinforcement learning algorithm to integrate non-differentiable natural language feedback, training the model to generate appropriate responses based on feedback.\n' +
      '\n' +
      '**(24) X-InstructBLIP**[15] introduces a cross-modal framework with instruction-aware representations, scalable enough to empower LLMs to handle diverse tasks across multiple modalities, including image/video, audio, and 3D. Notably, it achieves this without the need for modality-specific PT.\n' +
      '\n' +
      '**(25) CoDi-2**[15] is a MM generation model excelling in modality-interleaved instruction following, in-context generation, and user-model interaction by multi-turn conversations. It enhances CoDi [15] to process intricate modality-interleaved inputs and instructions, generating latent features autoregressively.\n' +
      '\n' +
      '**(26) VILA**[15] outperforms in vision tasks and shows remarkable reasoning ability while maintaining text-only capabilities. It achieves this by harnessing the full capabilities of LLM learning, using the interleaved attributes of image-text pairs, and implementing meticulous text data re-blending.\n' +
      '\n' +
      '## Appendix E VL Benchmarks\n' +
      '\n' +
      'The \\(18\\) VL benchmarks presented in Table 2 include **OKVQA**[13], **Icon-VQA**[16], **VQA\\({}^{\\textbf{v2}}\\)**[12], **GQA**[17], **VizWiz**[13], **SQA\\({}^{\\textbf{I}}\\)**: ScienceQA-IMG [14], **VQA\\({}^{\\textbf{T}}\\)**: TextVQA [12], **POPE**[15], **MME\\({}^{\\textbf{P}}\\)**: MME Perception [11], **MME\\({}^{\\textbf{C}}\\)**: MME Cognition [11], **MMB\\({}^{\\textbf{CN}}\\)**: MMBenchmark [15], **MMBCN**: MMBench-Chinese [14], **SEED\\({}^{\\textbf{I}}\\)**: SEED-Bench (Image) [15], **LLaVA\\({}^{\\textbf{W}}\\)**: LLaVA-Bench (In-the-Wild) [15], **MM-Vet**[11], **QBench**[12], **HM**: HatefulMemes [16], and **VSR**[15].\n' +
      '\n' +
      '## Appendix F Training Dataset\n' +
      '\n' +
      'The statistics for MM PT and MM IT dataset are presented in Table 3 and Table 4, respectively.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Dataset Name** & **X Modality** & **\\#.X** & **\\#.T** & **\\#.X-T** \\\\ \\hline ALIGN (Jia et al., 2021) & Image & 1.8B & 1.8B & 1.8B \\\\ LTIP (Alayrac et al., 2022) & Image & 312M & 312M & 312M \\\\ MS-COCO (Lin et al., 2014) & Image & 124K & 620K & 620K \\\\ Visual Genome (Krishna et al., 2017) & Image & 108K & 4.5M & 4.5M \\\\ CC3M (Sharma et al., 2018) & Image & 3.3M & 3.3M & 3.3M \\\\ CC12M (Changpinyo et al., 2021) & Image & 12.4M & 12.4M \\\\ SBU (Ordonez et al., 2011) & Image & 1M & 1M \\\\ LAION-5B (Schuhmann et al., 2022) & Image & 5.9B & 5.9B \\\\ LAION-400M (Schuhmann et al., 2021) & Image & 400M & 400M \\\\ LAION-en (Schuhmann et al., 2022) & Image & 2.3B & 2.3B \\\\ LAION-zh (Schuhmann et al., 2022) & Image & 142M & 142M \\\\ LAION-COCO (Schuhmann et al., 2022b) & Image & 600M & 600M \\\\ Flickr30k (Young et al., 2014) & Image & 31K & 158K & 158K \\\\ AI Challenger Captions (Wu et al., 2017) & Image & 300K & 1.5M & 1.5M \\\\ COYO (Byeon et al., 2022) & Image & 747M & 747M & 747M \\\\ Wukong (Gu et al., 2022) & Image & 101M & 101M & 101M \\\\ COCO Caption (Chen et al., 2015) & Image & 164K & 1M & 1M \\\\ WebLI (Chen et al., 2022b) & Image & 10B & 12B & 12B \\\\ Episodic WebLI (Chen et al., 2023g) & Image & 400M & 400M & 400M \\\\ CC595k (Liu et al., 2023e) & Image & 595K & 595K & 595K \\\\ RefCOCO (Kazemzadeh et al., 2014) & Image & 20K & 142K & 142K \\\\ RefCOCO+ (Yu et al., 2016) & Image & 20K & 142K & 142K \\\\ Visual-7W (Zhu et al., 2016) & Image & 47.3K & 328K & 328K \\\\ OCR-VQA (Mishra et al., 2019) & Image & 207K & 1M & 1M \\\\ ST-VQA (Biten et al., 2022) & Image & 23K & 32K & 32K \\\\ DocVQA (Mathew et al., 2021) & Image & 12K & 50K & 50K \\\\ TextVQA (Singh et al., 2019) & Image & 28.4K & 45.3K & 45.3K \\\\ DataComp (Gadre et al., 2023) & Image & 1.4B & 1.4B & 1.4B \\\\ GQA (Hudson and Manning, 2019) & Image & 113K & 22M & 22M \\\\ VGGQA (Krishna et al., 2017) & Image & 108K & 1.7M & 1.7M \\\\ VQA\\({}^{\\rm{\\color[rgb]{0,0,0}2}2}\\)(Goyal et al., 2017) & Image & 265K & 1.4M & 1.4M \\\\ DVQA (Kafle et al., 2018) & Image & 300K & 3.5M & 3.5M \\\\ OK-VQA (Schwenk et al., 2022) & Image & 14K & 14K & 14K \\\\ A-OKVQA (Schwenk et al., 2022) & Image & 23.7K & 24.9K & 24.9K \\\\ Text Captions (Sidorov et al., 2020) & Image & 28K & 145K & 145K \\\\ M3W (Interleaved) (Alayrac et al., 2022) & Image & 185M & 182GB & 43.3M (Instances) \\\\ MMC4 (Interleaved) (Zhu et al., 2023b) & Image & 571M & 43B & 101.2M (Instances) \\\\ MSRVTT (Xu et al., 2016) & Video & 10K & 200K & 200K \\\\ WebVid (Bain et al., 2021) & Video & 10M & 10M & 10M \\\\ VTP (Alayrac et al., 2022) & Video & 27M & 27M & 27M \\\\ AISHELL-2 (Chen et al., 2023b) & Audio & – & – & 128K \\\\ AISHELL-2 (Chen et al., 2023b) & Audio & – & – & 1M \\\\ WaveCaps (Mei et al., 2023) & Audio & 403K & 403K & 403K \\\\ VSDial-CN (Chen et al., 2023b) & Image, Audio & 120K (Image), 1.2M(Audio) & 120K & 1.2M \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: The statistics for MM PT datasets. **#.X** represents the quantity of X, **#.T** represents the quantity of Text, and **#.X-T** represents the quantity of X-Text pairs, where X can be Image, Video, or Audio.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:22]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>