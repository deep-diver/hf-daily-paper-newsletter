<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '멀티압축 키트 모델# MM-LLM\n' +
      '\n' +
      'Duzhen Zhang\\({}^{1}\\), 요아화 동\\({}^{1}\\), 자아화 동\\({}^{1}\\), 지아픽 장\\({}^{1}\\), 야아픽 장\\({}^{1}\\), 야아픽 주얼스({}^{1}\\), 유\\({}^{1}\\), 유\\({}^{1}II,{{{1}\\), 유\\({{{{1}.\n' +
      '\n' +
      '멘희 추\\({}^{2}\\)와 동유\\({}^{1}\\)\n' +
      '\n' +
      '성숙한 AI랩({}^{{1}\\) 10%의 AI랩(\\,{}^{{1}\\)입니다.\n' +
      '\n' +
      '\\({}^{2}\\)Kyoto University\n' +
      '\n' +
      '인공 지능대학({}^{3}\\)의 명예를 안았다.\n' +
      '\n' +
      'scoutzhang@tencent.com, yahan@nlp.ist.i.kyoto-u.ac.jp\n' +
      '\n' +
      '동일한 기여도. 대응 저자들은.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '지난 1년 동안 다중 모델 대형 언어 모델(MM-LLM)은 비용 효율적인 훈련 전략을 통해 MM 입력 또는 출력을 지원하기 위해 시크릿 LLM을 증가시키면서 상당한 발전을 겪었다. 생성된 모델은 LLM의 고유한 추론 및 의사 결정 능력을 보존할 뿐만 아니라 다양한 범위의 MM 작업을 권한 부여한다. 본 논문에서는 MM-LLM에 대한 추가 연구를 용이하게 하기 위한 포괄적인 조사를 제공한다. 구체적으로, 우리는 먼저 모델 아키텍처 및 훈련 파이프라인을 위한 일반적인 설계 제형을 설명한다. 그 후, 우리는 각각 특정 제형을 특징으로 하는 \\(26\\) 기존 MM-LLM의 간략한 소개를 제공한다. 또한, 우리는 MM-LLM의 효능을 향상시키기 위해 주류 벤치마크에 대한 MM-LLM의 성능을 검토하고 주요 훈련 레시피를 요약한다. 마지막으로, 우리는 현장에서 최신 발전을 위해 실시간 추적 웹사이트1을 동시에 유지하면서 MM-LLM에 대한 유망한 방향을 탐색한다. 이 조사가 MM-LLM 도메인의 지속적인 발전에 기여하기를 바랍니다.\n' +
      '\n' +
      '부타주 1: [https://mm-llms.githubio] (https://mm-llms.githubio) (https://mm-llms.github)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '멀티모달(MM) 사전 훈련 연구는 최근 몇 년 동안 하류 과제 스펙트럼(Li et al, 2020; Akbari et al., 2021; Fang et al., 2021; 옌 et al., 2021; Li et al., al., 2021; Radford et al., al., 2022; Li et al., al., 2022; Z remodel et al., 2022; 양국이 등, 2022, 2022, 2022, 2022, 2022)에 걸쳐 일관되게 공연 경계를 추진하는 데 상당한 발전을 목격했다. 그러나 모델 및 데이터 세트의 규모가 계속 확장됨에 따라 전통적인 MM 모델은 특히 처음부터 훈련될 때 상당한 계산 비용이 발생한다. MM 연구가 다양한 양식의 교차점에서 작동한다는 것을 인식하는 논리적 접근법은 강력한 대형 언어 모델(LLM, 오픈AI, 2022)을 특별 강조하여 쉽게 구할 수 있는 미리 훈련된 비모달 기반 모델에 대해 자본화하는 것이다. 이 전략은 계산 비용을 완화하고 MM 사전 훈련의 효능을 향상시켜 새로운 분야인 MM-LLM의 출현을 목표로 한다.\n' +
      '\n' +
      'MM-LLM 하네스 LLM을 인지 강국으로 하여 다양한 MM 과제에 힘을 실어준다. LLM은 강력한 언어 생성, 제로샷 전달 능력, 인콘텍스트 학습(ICL)과 같은 바람직한 속성에 기여한다. 현재 다른 양식의 기반 모델은 고품질 표현을 제공합니다. 서로 다른 양식의 기반 모델을 개별적으로 사전 훈련하는 것을 고려할 때, MM-LLM이 직면한 핵심 과제는 LLM과 다른 양식의 모델을 효과적으로 연결하여 협력 추론을 가능하게 하는 방법이다. 이 분야 내의 주요 초점은 MM 사전 훈련(PT) + MM 명령-투닝(IT) 파이프라인을 통해 양식과 인간 의도 간의 정렬을 정제하는 데 있었다.\n' +
      '\n' +
      'GPT-4(Vision, 2023)와 게미니(팀 등 2023)가 데뷔하며 인상적인 MM 이해와 발전 능력을 선보이며 연구 f는 인상적인 MM 이해력과 생성 능력을 선보였다.\n' +
      '\n' +
      '그림 1: MM-LLM의 타임라인.\n' +
      '\n' +
      'LLM이 촉발되었습니다. LLaVA-4(Liu et al., 2023a), 미니GPT-4(Zhu et al., 2023a), 미니BPT-4(Lii et al., 2023), 비디오Chat(Li et al., 2023), 비디오Chat(Li et al., 2023), 비디오Chat(Li et ald), 비디오Chat(Li et al), 비디오Chat(Li et al), 비디오Chat(Li et al., 2023), 비디오Chat(Li et al., 2023), 비디오Chat(Li et al., 2023), 비디오Chat(Li et al., 2023), 비디오Chat(Li et al., 2023), 비디오Chat(Li et al., 2023), 비디오Chat(Li et al), 비디오Chat(Li et al., 2023), 비디오Chat(Li et al., 2023), 비디오Chat(Li et al., 2023), 비디오Chat(Li et al., 2023), 비디오Chat), 비디오Chat(Li et al., 2023), 비디오Chat(Li et ald), 비디오Chat(Li et al), 비디오Chat(Li et ald), 비디오Chat(Li et al., 2023), 비디오VLaMA(Li et al.61), 비디오 MM 투입과 산출물(Aiello et al., 2023)이 모두 가능한 MM-LLM을 추구함에 있어 일부 연구에서는 Kosmos-2(펑 et al., 2023), 미니GPT-5(정 et al., 2023b)와 같은 특정 양식의 생성을 추가로 탐구하며, 스피치GPT(Zhang et al., 2023a)는 음성 생성을 소개한다. 최근의 연구 노력들은 인간과 같은 모든 양식의 전환을 모방하여 인공 일반 지능으로의 경로에 빛을 흘리는 데 초점을 맞추었다. 일부 노력은 Visual-ChatGPT(Wu et al., 2023a), ViperGPT(ineis et al., 2023), MM-REACT(양 et al., 2023), HuggingGPT(Shen et al., 2023b), 오디오GPT(Huang et al., 2023b)와 같은 \'수많은\' MM 이해력과 세대에 다가갈 수 있는 외부 도구와의 LLM을 목표로 한다. 반대로 캐스케이드 시스템의 전파된 오류를 완화하기 위해 NExT-GPT(Wu et al., 2023d) 및 CoDi-2(Tang et al., 2023b)와 같은 이니셔티브는 임의의 양식의 말단에서 말단 MM-LLM을 개발했다. MM-LLM의 타임라인은 그림 1에 나와 있다.\n' +
      '\n' +
      '본 논문에서는 MM-LLM에 대한 추가 연구를 용이하게 하기 위한 포괄적인 조사를 제시한다. 독자에게 MM-LLM에 대한 전체적 이해를 제공하기 위해 처음에 모델 아키텍처(섹션 2) 및 훈련 파이프라인(섹션 3)에서 일반적인 설계 제형을 설명한다. 일반 모델 아키텍처를 모형 엔코더(섹션 2.1), 입력 프로젝트(섹션 2.2), LLM 백본(섹션 2.3), 출력 프로젝트(섹션 2.4), 모듈 유전체(섹션 2.5)의 다섯 가지 구성 요소로 분해한다. 훈련 파이프라인은 MM PT(섹션 3.1)와 MM IT(섹션 3.2)의 두 단계로 주로 구성된 MM 입력 또는 출력을 지원하기 위해 미리 학습된 텍스트 전용 LLM을 향상시키는 방법을 설명한다. 이 섹션에서는 MM PT 및 MM IT에 대한 주류 데이터 세트의 요약도 제공합니다. 다음으로, 각각의 특정 제형을 특징으로 하는 \\(26\\) 주미술(SOTA) MM-LLM에 대한 논의를 수행하고, 4절의 개발 동향을 요약하면, 5절에서는 MM-LLM의 효능을 향상시키기 위해 주류 벤치마크 및 증류 키 훈련 레시피에 대한 주요 MM-LLM의 성능을 종합적으로 검토한다. 6절에서 MM-LLM 연구를 위한 유망한 방향을 제공합니다. 더욱이, 우리는 MM-LLM의 최신 진행을 추적하고 군중 생성 업데이트를 용이하게 하기 위해 웹사이트([https://mm-llms.githubio](https://mm-llms.githubio)를 구축했다. 마지막으로 7절의 전체 논문을 요약하고 부록 A의 MM-LLM에 대한 관련 조사에 대해 논의하며, 우리는 연구자들에게 이 분야에 대한 더 깊은 이해도를 얻고 보다 효과적인 MM-LLM의 설계를 고취하기 위해 조사를 돕습니다.\n' +
      '\n' +
      '2개의 모델 아키텍처 제품입니다.\n' +
      '\n' +
      '이 섹션에서는 MM 이해를 강조하는 MM-LLM에는 첫 번째 세 가지 구성 요소만 포함하는 그림 2와 같이 각 구성 요소에 대한 구현 선택과 함께 일반 모델 아키텍처를 포함하는 5가지 구성요소에 대한 자세한 개요를 제공한다. 훈련하는 동안, 모델 엔코더, LLM 백본 및 모듈성 유전체기는 일반적으로 냉동 상태로 유지된다. 1차 최적화 강조점은 인풋 및 아웃풋 프로젝터에 관한 것이다. 프로젝트가 경량 성분이라는 점을 감안할 때, MM-LLM에서 훈련 가능한 파라미터의 비율은 총 매개변수 수(2\\)에 비해 현저히 작다. 전체 파라미터 수는 MM-LLM에서 사용되는 코어 LLM의 규모에서 우발된다. 결과적으로 MM-LLM은 다양한 MM 과제에 힘을 실어주기 위해 효율적으로 훈련될 수 있다.\n' +
      '\n' +
      '### Modality Encoder\n' +
      '\n' +
      '모니스 엔코더(ME)는 다양한 양식의 인코딩 입력(I_{X}\\)을 사용하여 다음과 같이 공식화된 해당 피처 \\(\\mathbf{F}_{X}\\)를 얻는 작업이다.\n' +
      '\n' +
      '\\[\\mathbf{F}_{X}=\\text{ME}_{X}(I_{X}). \\tag{1}\\]\n' +
      '\n' +
      '다양한 사전 훈련된 인코더 옵션 \\(텍스트{ME}_{X}\\)는 서로 다른 방식을 처리하기 위해 존재하며, 여기서 \\(X\\)는 이미지, 비디오, 오디오, 3D 등일 수 있다. 다음으로, 모달리티로 구성된 간결한 소개를 제공할 것입니다.\n' +
      '\n' +
      '이미지의 시각적 Modality은 일반적으로 **NFNet-F6**(BEck et al., 2021), **ViT**(Dosovitskiy et al., 2020), **CLIP ViT**(라드포드 et al., 2021), **Eva-CLIP ViT**(Fang et al., 2023) 등 네 가지 선택적인 인코더가 있다. ***NFNet-F6**는 정상화기가 없는 ResNet(He et al., 2016)으로, 이미지들을 먼저 패치들로 분할하여 이미지들에 대해 변환기 바스완이(2017)를 달성하면서 광범위하게 증강된 데이터셋들에 대한 트레이닝을 허용하는 적응형 구배 클램핑 기술을 보여준다. 그런 다음 패치를 평탄화하기 위해 선형 투영을 거친 다음 다중 트랜스포머 블록을 통해 인코딩한다. ***CLIP ViT***는 ViT와 텍스트 인코더를 포함하는 텍스트와 이미지 간의 연결을 구축한다. 광범위한 양의 텍스트 이미지 쌍을 사용하여 대조 학습, 짝을 이루는 텍스트 및 이미지를 양성 샘플 등으로 처리하여 ViT를 최적화한다. **Eva** 버전은 대규모 CLIP의 훈련 및 최적화 과정을 안정화시켜 MM 기반 모델의 고가의 교육을 확대하고 가속화하는 새로운 방향을 제공한다. 비디오의 경우, 이미지들과 동일한 전처리 과정을 거친 \\(5\\) 프레임으로 균일하게 샘플링될 수 있다.\n' +
      '\n' +
      'Audio Modalityis는 보통 **C-전*Chen et al.(2023), **HuBERT**Hsu et al.(2021), **BEATs**Chen et al.(2023), **Whisper***Radford et al.(2023)에 의해 암호화된다. **C-전***는 CIF 정렬 메커니즘 동과 Xu(2020)를 사용하고 있으며, 장 등은 서열 형질도입을 위해 al.(2022), 오디오 특징을 추출하는 트랜스포머를 사용한다. ***HuBERT***는 이산 은닉 단위의 마스킹 예측에 의해 달성된 BERT 켄톤과 투타노바(2019)를 기반으로 하는 자기 지도 음성 표현 학습 프레임워크이다. ***BEATs**는 오디오 트랜스포머로부터 방향 엔코더 표현을 학습하도록 설계된 반복 오디오 사전 훈련 프레임워크이다.\n' +
      '\n' +
      '3D포인트 클라우드 모달리티스는 일반적으로 **ULIP-2** 판매력(2022), Xu et al.(2023, 2023)에 의해 암호화되어 포인트BERT 유 등(2022) 백본을 가지고 있다.\n' +
      '\n' +
      '더욱이, 수많은 이질적인 모달 인코더, 일부 MM-LLM, 특히 모든 것을 처리하기 위해 **ImageBind**Girdhar et al.(2023)는 이미지, 비디오, 텍스트, 오디오, 히트 맵 등을 포함한 6가지 양식을 포함하는 통합 인코더이다.\n' +
      '\n' +
      '### Input Projector\n' +
      '\n' +
      '입력 프로젝트 \\(\\mathbf{\\Theta}_{X\\to T}\\)는 다른 모달리티 \\(\\mathbf{F}_{X}\\)의 인코딩된 특징을 텍스트 특징 공간 \\(T\\)과 정렬하는 작업이다. 프롬프트로서 정렬된 특징(\\mathbf{P}_{X}\\)은 텍스트 특징 \\(\\mathbf{F}_{T}\\)과 함께 LLM 백 뼈에 공급된다. \\(X\\)-텍스트 데이터셋 \\(\\{I_{X},t\\}\\)를 고려할 때, \\(X\\)-조건 텍스트 생성 손실 \\(I\\)-조건 텍스트 생성 손실(\\mathcal{L}_{\\text{txt-gen}}\\)을 최소화하는 것이 목표다.\n' +
      '\n' +
      '세타}_{X\\to T}\\mathcal{L}}(\\text{L}_{\\text{txt},\\mathbf{P}_{X},\\mathbf{F}_{T}),\\tag{2}.\n' +
      '\n' +
      'HH(\\mathbf{P}_{X}=\\mathbf{\\ta}_{X\\to T}(\\mathbf{F}_{X})\\이다.\n' +
      '\n' +
      '인풋 프로젝터는 ** 선형 프로젝터*** 또는 멀티 플레이어 퍼셉트론(**MLP**) _i._, 비선형 활성화 기능으로 인터리빙된 여러 선형 프로젝터에 의해 직접 달성될 수 있다. **Cross-attention******, **Q 전**Li et al.(2023), **P 전**Jian 등 보다 복잡한 구현도 있다. **C 교차 지향***는 훈련 가능한 벡터 세트를 쿼리로서 사용하고 인코딩된 피처 \\(\\mathbf{F}_{X}\\)를 키로 사용하여 특징 시퀀스를 고정된 길이로 압축한다. 그런 다음 압축된 표현은 LLM Bai et al.(2023)에 직접 공급되거나 X-텍스트 교차 의도 융합 Alayrac et al.(2022)에 추가로 사용된다. **Q 전***는 \\(\\mathbf{F}_{X}\\)에서 관련 특징을 추출하고 선택된 특징은 프롬프트 \\(\\mathbf{P}_{X}\\)로 사용된다. 한편, **P-전**는 Q-전자에 의해 생성된 프롬프트에 정렬 제약을 부과하기 위해 프롬프트를 생성한다. 그러나 Q- 및 P-전 모두 초기화를 위해 **분리 PT** 공정을 필요로 한다.\n' +
      '\n' +
      '### LLM Backbone\n' +
      '\n' +
      'LLM Zhao et al.(2023), Naveed et al.(2023), Naveed et al.(2023)를 핵심 요원으로서 MM-LLM은 0샷 일반화, 소수의 샷 ICL, Chain-Thought(CoT) 및 지시에 따라 몇 가지 주목할 만한 특성을 계승할 수 있다. 그.\n' +
      '\n' +
      '그림 2: MM-LLM의 일반적인 모델 아키텍처 및 각 구성 요소에 대한 구현 선택.\n' +
      '\n' +
      'LLM 백본 과정은 투입물에 대한 의미 이해, 추론 및 결정에 관여하며 다양한 양식의 표현을 나타낸다. (1) 직접 텍스트 출력 \\(t\\) 및 (2) 신호 토큰 \\(\\mathbf{S}_{X}\\)을 다른 양식(언제든)에서 생성한다. 이러한 신호 토큰은 MM 콘텐츠 제작 여부에 대한 발전기를 안내하고, 긍정적이면 생산 내용을 명시하는 지침으로 작용한다.\n' +
      '\n' +
      '\\[t,\\,\\mathbf{S}_{X}=\\text{LLM}(\\mathbf{P}_{X},\\mathbf{F}_{T}), \\tag{3}\\]\n' +
      '\n' +
      '다른 모달리티의 정렬된 표현(\\mathbf{P}_{X}\\)이 LLM 백뼈에 대한 부드러운 프롭토닝으로 간주될 수 있다. 더욱이 일부 연구 작업은 프레픽스-튜닝 리와 리앙(2021), 아드레터 호울스비(2019년), LoRA Hu et al(2021년) 등 파라직경-효율적인 파인-투닝(PEFT) 방법을 도입했다. 이러한 경우, 전체 LLM 파라미터 수의 0.1% 미만에도 불구하고, 추가로 훈련 가능한 파라미터의 수는 예외적으로 최소이다. 부록 B에서 주류 PEFT 방법에 대한 소개를 제공합니다.\n' +
      '\n' +
      '***T*Zoffmann et al.(2023), **OPT*Zoffmann et al. **OPT*Zoffn et al.(2022), **OPLM**Zat et al. **T*Zat et al.(2023), **L*L*Zouy et al. **T*Zoffn et al. **T*Zoffn(2022), **T*Zoff만 et al. **T*Zoffn(2022), **T*Zoffn(2022), **T*Zoffn(2022), **T*Zoffn(2022), **T*Zoffn(2022), **T*T*Zoffn(2022), **T*ZatGLM*Zat) 및 **T*Zoffn(2022), **T*T*Ztra. **T*Zun. **T*Zter. **T*Zat. **T*Zat. **T*Zat. **T*ZatGLM*Zat) 및 **T*Zat.*T* 부록 C에서 이러한 모델에 대한 간단한 소개를 제공합니다.\n' +
      '\n' +
      '### Output Projector\n' +
      '\n' +
      '입력 프로젝트(\\mathbf{\\-ta}_{T\\to X}\\)는 신호 토큰 표현 \\(\\mathbf{S}_{X}\\)을 LLM 백뼈의 특징(\\mathbf{S}_{X}\\)으로 매핑하여 다음 모형 유전자 \\(\\mathbf{H}_{X}_{X}\\)로 이해할 수 있다. HPA(X\\)-텍스트 데이터세트 \\(\\{I_{X},t\\})를 감안할 때, \\(t\\)는 먼저 LLM에 공급되어 해당 \\(\\mathbf{S}_{X}\\)를 생성한 다음, \\(\\mathbf{H}_{X}\\)에 매핑된다. 부착된 피처 \\(\\mathbf{H}_{X}\\)의 정렬을 용이하게 하기 위해 \\(\\mathbf{H}_{X}\\)와 \\(\\text{MG}_{X}\\)의 조건부 텍스트 표현(\\text{MG}_{X}\\) 사이의 거리를 최소화하는 것이 목표다.\n' +
      '\n' +
      '\\[재수술자명*{arg\\,{arg\\,min}_{\\mathbf{\\}_{T\\to X}}\\mathcal{L}_{\\text{mse}}(\\mathbf{ H}_{X}_{X},\\tau_{X})]\\ 태그{4}.\n' +
      '\n' +
      '최적화는 \\(\\mathbf{H}_{X}=\\mathbf{X}=\\mathbf{\\-X})(\\mathbf{\\to X}(\\mathbf{S}_{X})\\) 및 \\(\\tau_{X}_{X}\\)는 \\(\\text{MG}_{X}_{X}\\)의 텍스트 조건 인코더인 오디오 또는 시각적 자원 \\(X\\)를 사용하지 않고 자막 텍스트에만 의존한다. 출력 프로젝터는 **Tiny Transformer*** 또는 **MLP**에 의해 구현된다.\n' +
      '\n' +
      '### Modality Generator\n' +
      '\n' +
      '모달리티 유전체 \\(\\text{MG}_{X}\\)는 별개의 양식으로 출력을 생성하는 작업이다. 일반적으로 기존 작업은 이미지 합성을 위해 _i._i._i._, **Stable Diffusion**Rombach et al.(2022), 비디오 합성을 위해 **Zeroscope**Cerspense(2023), 오디오 합성을 위해 **AudioLDM-2**Liu et al.(2023,2023) 등을 사용한다. 출력 프로젝터에 의해 매핑된 피처 \\(\\mathbf{H}_{X}\\)는 데노징 과정에서 조건부 입력으로서 MM 함량을 생성하는 역할을 한다. 훈련 중 지반 진리 내용은 먼저 사전 훈련된 VAE 킹마 및 웰링(2013)에 의해 잠재 피처 \\(z_{0}\\)로 변환된다. 그런 다음 노이즈 \\(z_{0}\\)를 \\(z_{t}\\)에 추가하여 시끄러운 잠재 특징 \\(z_{t}\\)을 얻는다. (2015)\\(\\epsilon_{X}\\)는 다음과 같이 조건부 LDM 손실(\\mathcal{L}_{\\text{X}_{\\text{X-gen}}\\)을 계산하는 데 사용된다.\n' +
      '\n' +
      '{mathb{E}(0,1), γ}(z_{t,\\mathbf{H}_{{X})\n' +
      '\n' +
      '파라미터 \\(\\mathbf{\\}_{X\\to T}\\)와 \\(\\mathbf{\\to T}\\)를 최적화(\\mathcal{L}_{T\\to X}\\)하여\\(\\mathcal{L}_{\\text{X-gen}}\\)를 최소화한다.\n' +
      '\n' +
      '편선 3개 훈련.\n' +
      '\n' +
      'MM-LLM의 훈련 파이프라인은 MM PT와 MM IT의 두 가지 주요 단계로 묘사될 수 있다.\n' +
      '\n' +
      '### Mm Pt\n' +
      '\n' +
      'PT 단계 동안, 일반적으로 X-텍스트 데이터 세트를 레버리지하는 입력 및 출력 프로젝트들은 미리 정의된 목표를 최적화함으로써 다양한 양식들 간의 정렬을 달성하기 위해 훈련된다(PEFT가 LLM 백본에 적용되기도 한다). MM 이해 모델의 경우 최적화는 식 (2)에만 초점을 맞추는 반면 MM 생성 모델의 경우 최적화는 식 (2), 식 (4), (5)를 포함한다. 후자의 경우 식 (2)는 지상-진실 신호 토큰 시퀀스도 포함한다.\n' +
      '\n' +
      'X-텍스트 데이터 세트는 이미지-텍스트, 비디오-텍스트 및 오디오-텍스트를 포함하며 이미지-텍스트는 이미지-텍스트 쌍("<im1><tx1>")과 인터리빙된 이미지-텍스트 코퍼스(<<txt1><txt2><txt2><txt4>)의 두 가지 유형을 가진다. 이러한 X-텍스트 데이터 세트에 대한 자세한 통계는 부록 F의 표 3에 나와 있다.\n' +
      '\n' +
      '### Mm It\n' +
      '\n' +
      'MM IT는 명령어 형태의 데이터 세트 Wei et al(2021)를 사용하여 미리 학습된 MM-LLM을 미세 조정하는 것을 수반하는 방법론이다. 이러한 튜닝 과정을 통해 MM-LLM은 새로운 명령어를 준수하여 비일시적 과제로 일반화할 수 있어 제로샷 성능을 향상시킬 수 있다. 이 간단하지만 영향을 미치는 개념은 구조GPT[14], OPT-IML[15], 구조BLIP[16]과 같은 작품에 의해 구체화된 NLP 분야에서 후속 노력의 성공을 촉매했다.\n' +
      '\n' +
      'MM IT는 인간 의도 또는 선호도와 일치하고 MM-LLM의 상호 작용 능력을 향상시키는 것을 목표로 하는 인간 피드백(RLHF)으로부터의 슈퍼엔드 파인-튜닝(SFT) 및 강화 학습을 포함한다. SFT는 PT 스테이지 데이터의 일부를 명령어 인식 형식으로 변환한다. 시각 질문-응답기(QA)를 예로 들면 **(1) <이미지>{ 질문}** A 단답과 같이 다양한 템플릿을 사용할 수 있으며, **(2) <이미지>**Ex아민이며 간략한 답변으로 다음 질문에 응답한다. **정답***:: 등. 다음으로 동일한 최적화 목표를 사용하여 미리 학습된 MM-LLM을 미세 조정한다. SFT 데이터 세트는 단일턴 QA 또는 멀티턴 대화들로 구성될 수 있다.\n' +
      '\n' +
      'SFT 후, RLHF는 MM-LLM의 반응(_e.g._, 수동으로 또는 자동으로 표지된 천연 언어 피드백(NLF) [20]에 대한 피드백에 의존하여 모델의 추가 미세 조정을 포함한다. 이 과정은 미분할 수 없는 NLF를 효과적으로 통합하기 위해 강화 학습 알고리즘을 사용한다. 모델은 NLF[13, 1]에서 조건화된 대응하는 응답을 생성하기 위해 훈련된다. SFT 및 RLHF 데이터 세트에 대한 통계는 부록 F의 표 4에 나와 있다.\n' +
      '\n' +
      'MM PT 및 MM IT 단계에서 기존 MM-LLM이 사용하는 데이터 세트는 다양하지만 모두 표 3 및 4의 데이터 세트의 ** 서브세트***이다.\n' +
      '\n' +
      '4 SOTA MM-LLMM.\n' +
      '\n' +
      '이전에 정의된 설계 제형을 기반으로 표 1에 도시된 바와 같이 \\(26\\) SOTA MM-LLM에 대한 아키텍처 및 훈련 데이터세트 척도를 포괄적으로 비교하고 결과적으로 이러한 모델의 핵심 기여도에 대한 간결한 소개를 제공하고 발달 경향을 요약할 것이다.\n' +
      '\n' +
      '**(1) 플라밍고***[1]은 인터리빙된 시각적 데이터와 텍스트를 처리하기 위해 설계된 일련의 비주얼어(VL) 모델을 나타내며, 이는 자유형 텍스트를 출력으로 생성한다. ***(2) BLIP-2**[11]은 모달리티 갭을 가교하기 위해 이전 경량 Q-전류와 냉동 LLM의 활용을 포함하는 보다 자원 효율적인 프레임워크를 소개한다. 레버징 LLM, BLIP-2는 자연어 프롬프트를 사용하여 제로샷 이미지 대 텍스트 생성을 위해 안내될 수 있다. **(3) LLaVA**[11]은 IT 기술을 MM 영역으로 전달하는 것을 개척한다. 데이터 부족을 추가하면 LLaVA는 MM 명령어-폴딩 벤치마크인 LLaVA-Bench와 함께 ChatGPT/GPT-4를 사용하여 생성된 새로운 오픈 소스 MM 명령어-폴딩 데이터 세트를 소개한다. ***(4) 미니GPT-4**[16]는 하나의 선형 계층만이 미리 학습된 비전 인코더와 LLM을 정렬하는 유선화된 접근법을 제안한다. 이 효율적인 방법은 GPT-4. **(5) mPLUG-Owl**[12]의 표시된 능력의 복제를 가능하게 하여 시각적 맥락을 통합하는 MM-LLM에 대한 새로운 모듈화된 훈련 프레임워크를 제시한다. MM 태스크에서 서로 다른 모델의 성능을 평가하기 위해 프레임워크에는 OwlEval이라는 수업 평가 데이터셋이 포함된다. **(6) X-LLM**[13]는 오디오를 포함한 다양한 양식으로 확장되어 강한 확장성을 보여준다. Q-전, X-LLM의 언어 전달 가능성을 조작하는 것은 중-티베트 중국인의 맥락에서 성공적으로 적용된다. ***(7) 비디오챗****[11]는 화상 이해를 위한 효율적인 채팅 중심 MM-LLM을 개척하고 이 영역의 향후 연구에 대한 기준을 설정하고 학계 및 산업 모두에서 프로토콜을 제공한다. **(8) 구조BLIP**[16]는 사전 훈련된 BLIP-2 모델을 기반으로 훈련되어 MM IT 동안 Q 전만을 업데이트한다. 지시 인식 시각적 특징 추출 및 대응하는 명령어를 도입함으로써 모델은 유연하고 다양한 특징을 추출할 수 있게 한다. **(9) 판다GPT**[14]는 텍스트, 이미지/비디오, 오디오, 열, 깊이 및 관성 측정 단위의 다양한 양식에 걸쳐 지침을 이해하고 행동할 수 있는 능력을 가진 선구적인 범용 모델이다. **(10) PaLIP-X**[13]는 프리픽스 완료 및 마스킹 톤 완료를 포함한 혼합 VL 목표와 비모달 목표를 사용하여 훈련된다. 이 접근법은 하류 태스크 결과 모두에 효과적인 것으로 입증되고 미세 조정 환경에서 파레토 프런티어를 달성한다. ***(11) 비디오-LLaMA**[16]는 멀티브랜치 크로스 모달 PT 프레임워크를 도입하여 LLM이 인간과 대화를 하면서 주어진 동영상의 비전 및 오디오 콘텐츠를 동시에 처리할 수 있도록 한다. 이 프레임워크는 비전과 언어 및 오디오를 언어와 정렬합니다. ***(12) 영상-ChatGPT***[16]는 시공간 개정 표상을 통합하여 동영상에 대한 논의를 생성할 수 있는 영상 대화를 위해 특별히 설계된 모델이다. [13]은 시크라***[13]이고, 식라**[13]은****(13)***(13)***(13)이다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '더 많은 데이터 세트가 이전에 PT 또는 IT 세트에서 다양한 정도로 나타났다는 점을 감안할 때 기존 벤치마크에 대한 더 많은 결빙은 MM-LLM의 능력에 적절하게 도전하지 못할 수 있다. 이는 모델이 훈련 중에 이러한 작업을 학습했을 수 있음을 의미한다. 더욱이, 현재 벤치마크들은 주로 VL 서브 필드 상에 집중된다. 따라서 MM-LLM의 개발은 더 많은 양식을 포함하고 통일된 평가 표준을 사용하는 보다 도전적이고 더 큰 규모의 벤치마크를 구성하는 것이 중요하다. 현재 벤치마크는 실제 응용 분야에서 MM-LLM의 숙련도를 평가하기 위해 맞춤화될 수 있으며, 예를 들어 GOATBench Lin et al.(2024)의 도입은 메모에 제시된 사회적 남용의 미묘한 측면을 식별하고 대응하는 다양한 MM-LLM의 능력을 평가하는 것을 목표로 한다.\n' +
      '\n' +
      'MM-LLM을 자원 제한 플랫폼에 배치하고 최적의 성능을 달성하기 위해서는 저전력 모바일 및 IoT 장치와 같은 경량화 구현이 무엇보다 중요하다. 이 영역에서 주목할만한 발전은 모바일VLM 추 등(2023)이다. 이 접근법은 전략적으로 LLaMA를 다운스윙하여 원활한 오프 시크릿 배치를 허용한다. 모바일VLM은 \\(20\\) 백만 매개변수 이하로 구성된 라이트급 다운샘플 프로젝터를 추가로 도입하여 연산 속도 향상에 기여한다. 그럼에도 불구하고 이 방법은 개발의 추가 발전을 위해 추가 탐구가 필요하다.\n' +
      '\n' +
      '구조화된 지능은 환경을 효과적으로 이해하고, 관련 대상을 인식하고, 공간적 관계를 평가하고, 포괄적인 과제 계획인 피로지 등(2023)을 고안함으로써 주변과의 인간과 유사한 인식과 상호 작용을 복제하는 것을 목표로 한다. 체화된 계획, 체화된 시각 질문 응답 및 체화된 제어와 같은 구조적 AI 작업은 로봇이 실시간 관찰을 활용하여 확장된 계획을 자율적으로 구현하도록 동일시한다. 이 지역의 일부 전형적인 작업은 PaLM-E Driess et al.(2023) 및 엠보디드GPT Mu et al.(2023)이다. PaLM-E는 MM-LLM의 교육을 통해 다중 색전제를 소개한다. 체화된 결정자로서만 기능하는 것 외에도 PaLM-E는 일반적인 VL 작업을 처리하는 데 능력을 보여준다. 실시예GPT는 CoT 접근법을 통해 특성화된 경제적으로 효율적인 방법을 도입하여 체화된 에이전트가 현실 세계와 참여하는 능력을 높이고 고위급 계획을 저수준 통제와 연결하는 폐쇄 루프를 구축한다. MM-LLM 기반 구현 지능이 로봇과 통합하는 데 기여한 반면, 로봇의 자율성을 높이기 위해서는 추가 탐구가 필요하다.\n' +
      '\n' +
      '실제 응용 분야에서 MM-LLM은 추가 기능을 지원하기 위한 새로운 MM 작업에 적응할 것으로 예상된다. 그럼에도 불구하고, 현재 MM-LLM은 정적으로 남아 있으며 지속적으로 새로운 요구 사항을 조정할 수 없다. 따라서 MM-LLM을 재교육하는 상당한 비용을 회피하면서 모델을 효율적이고 지속적으로 새로운 데이터를 레버리지할 수 있을 만큼 유연하게 만들기 위한 접근이 필요하다. 이는 모델이 인간 학습과 유사한 새로운 과제를 증분하도록 설계된 지속적인 학습의 원칙과 일치한다. 지속적인 IT는 원래의 MM IT 단계에서 학습한 작업에 대해 우수한 성능을 유지하면서 새로운 MM 작업에 대해 지속적으로 미세 조정 MM-LLM을 목표로 한다. 신과제 로빈스(1995)와 맥클로스키, 코헨(1989)을 배울 때 이전의 지식을 잊고 있는 (1) 재난적 망각, 선노란 등 (2013), 장(2023), 정(2023) 및 (2) 음전향 전이, 동(2023, 2023)의 두 가지 주요 과제를 소개한다. 최근에 헤 등은 MM-LLM에 대한 지속적인 IT 개발을 촉진하기 위해 벤치마크를 확립했다. 이러한 진전에도 불구하고 재앙적 망각과 부정적 전이의 과제를 해결하기 위한 더 나은 방법을 개발할 수 있는 중요한 기회와 여지가 여전히 있다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '이 논문에서 최근 발전에 초점을 맞춘 MM-LLM에 대한 포괄적인 조사를 제시했다. 처음에 모델 아키텍처를 5개의 구성 요소로 분류하여 일반적인 설계 제형과 훈련 파이프라인에 대한 자세한 개요를 제공한다. 그 후, 우리는 각각 특정 제형으로 구별되는 다양한 SOTA MM-LLM을 소개한다. 우리의 조사는 또한 다양한 MM 벤치마크에 걸쳐 그들의 능력을 조명하고 이 빠르게 진화하는 분야에서 미래의 발전을 부러워한다. 이 조사가 연구자들에게 통찰력을 제공하여 MM-LLM 도메인의 지속적인 발전에 기여할 수 있기를 바랍니다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '이 논문에서 우리는 현재 MM-LLM 풍경에 대한 포괄적인 탐구에 착수하여 우리의 통찰력에 의해 풍부한 다양한 관점에서 합성을 제시한다. 이 분야의 동적 특성을 발견하면 특정 측면이 우리의 조사를 생략했을 수 있으며 최근의 발전은 완전히 캡슐화되지 않을 수 있다. 이 고유한 도전을 해결하기 위해 최신 발전을 포착하기 위해 인파소킹을 사용하여 실시간 추적을 위한 전용 웹사이트를 구축했습니다. 우리의 목표는 이 플랫폼이 현장에서 진행 중인 개발을 촉진하는 지속적인 기여원으로 진화하는 것이다. 페이지 한계의 제약을 감안할 때 모든 기술 세부 사항에 대해 설명할 수 없으며 주류 MM-LLM의 핵심 기여에 대한 간결한 개요를 제공했다. 우선, 우리는 그들의 출현과 같은 새로운 통찰력을 통합하면서 당사 웹사이트에서 관련 세부 정보를 경계하고 지속적으로 개선하려고 노력합니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. 아사이, S. 민, Z. 중, D. Chen(2023)은 귀납 기반 언어 모델 및 응용 프로그램을 검색한다. 종합통계학회 제61차 연차회의(금액 6: 튜토리얼 Abstracts)의 개최에서 pp. 41-46은: SS1에 의해 발표되었다.\n' +
      '* E. Aiello, L. 유, Y. Nie, A. Aghajanyan 및 B. Oguz(2023)J 공동 훈련 큰 자기회귀 복합 모델을 훈련한다. arXiv 프리프린트 arXiv:2309.15564: SS1에 의해 계산된다.\n' +
      '* H. 아스퍼리, L. 위안, R. 아니, W. 추앙, S. Y창. 쿠이, B. 공(2021)바트: 원시 영상, 오디오, 텍스트에서 멀티모달 자기지도 학습을 위한 변압기입니다. 신경 정보 처리 시스템34, pp 24206-24221의 발전은: SS1에 의해 계산된다.\n' +
      '* A. Feyza Akyurek, E. Akyurek, A.Madaan, A. 칼리아, P. 클라크, D. Wijaya 및 N. 탄돈(2023)RL4F: 모델 출력 수리를 위한 강화 학습으로 자연어 피드백을 생성한다. arXiv 프리프린트 arXiv:2305.08844: SS1에 의해 계산된다.\n' +
      '* J. 알레이크, J. 도야에, P. 루스, A. 미흐, I. 바, Y. 하손, K. enc, A. 엠쉬, K. 밀리칸, M. 리놀즈, et al.(2022)Flamingo: 소수의 샷 학습을 위한 시각적 언어 모델. 신경 정보 처리 시스템 35, pp 23716-23736의 발전은: SS1에 의해 계산된다.\n' +
      '* A. 아사디, S. 민, Z. 중, D. Chen(2023)은 귀납 기반 언어 모델 및 응용 프로그램을 검색한다. 종합통계학회 제61차 연차회의(금액 6: 튜토리얼 Abstracts)의 개최에서 pp. 41-46은: SS1에 의해 발표되었다.\n' +
      '* A. 아사디, I. 가오, J. 가드너, J. 헤셀, Y. 하나피, W. 주 K. 마마테, Y. 비트톤, S. 가드레, S. 사가와 등 (2023) 오픈플밍고: 대규모 자기회귀 비전-언어 모델을 훈련하기 위한 오픈 소스 프레임워크입니다. arXiv 프리프린트 arXiv:2308.01390: SS1에 의해 계산된다.\n' +
      '* J Bai, S. 바이, Y. 추, Z. 쿠이, K. 당, X. 펑, Y. 팬, W. 지, Y. 한, F. 황, et al.(2023)Qwen 기술 보고서. arXiv 프리프린트 arXiv:2309.16609: SS1에 의해 계산된다.\n' +
      '* J Bai, S. 바이, S. 양, S. 와, S. 탄, P. 왕, J. Lin, C. 저우 및 J. 저우(2023)큐웬-VL: 다재다능한 능력을 가진 프론티어 대형 비전 언어 모델이다. CoRRabs/2308.12966: SS1에 의해 계산됩니다.\n' +
      '*M. 베인, A. 나가란이, G. 바롤 및 A. 지스머만(2021)은 엔드 투 엔드 검색을 위한 공동 비디오 및 이미지 인코더이다. 컴퓨터 비전에서 IEEE/CVF 국제 회의의 개최에서 pp 1728-1738: SS1이 게시했다.\n' +
      '* R. 바비시, E. 엘센, C. 호세토렌, M. 노예, A. 오데나, A. 소미 및 S. 타스걸라(2023)는 복합 모델을 소개합니다. SS1: SS1로 받았습니다.\n' +
      '* A. 비트넨, R. 리트맨, Y. 시, S. 아팔라주, R. 맨마사(2022)LATT: 장면-텍스트 vqa에 대한 레이아웃 인식 변압기이다. 컴퓨터 비전 및 패턴 인식에 대한 IEEE/CVF 회의의 진행에서 pp 16548-16558: SS1에 의해 제안된다.\n' +
      '* A. 브룩, S. De, S. L. 스미스 및 K. 심니안(2021)이 정상화되지 않은 고성능의 대규모 이미지 인식을 가지고 있다. 기계학습 국제회의에서는 pp. 1059-1071: SS1로 작성되었다.\n' +
      '* T. 브라운, B. Mann, N. 리더, M. 하위피아, J D Kaplan, P. D. D. 카플란, P. 다라리왈, A. 네이만탄, P. Shyam, G. Sastry, A. Askell, et al.2020) 언어 모델은 소수의 샷 학습자들이다. 신경 정보 처리 시스템(33, pp 1877-1901)의 발전은: SS1에 의해 계산된다.\n' +
      '*M. 편, B 박, H.김, S. 이, W. 백과 S. 김(2022) 코요-700m: 이미지-텍스트 쌍 데이터셋. SS1: SS1로 받았습니다.\n' +
      '* F. C. Heilbron, V. Escorcia, B. Ghanem 및 J. C. Nieied(2015) 활성넷: 인간 활동 이해를 위한 대규모 비디오 벤치마크이다. 컴퓨터 비전 및 패턴 인식에 관한 ieee 회의의 진행에서 pp. 961-970: SS1에 의해 발표되었다.\n' +
      '* C. C.(2023) 제로스코프: 확산 기반 텍스트 대 비디오 합성. SS1: SS1로 받았습니다.\n' +
      '\n' +
      '* 첸 등은 (2023a) 페이 장, 두셴 장, 명룬 한, 시야 선, 징시, 슈앙 주, 보Xu 등이 있다. 2023a. 비언어 사전학습에 관한 조사 __Vlp: 비전언어 사전학습에 관한 조사. 기계 정보 연구_ 20(1):38-56입니다.\n' +
      '* 첸 등은 (2023b) 필롱 첸, 명루 한, 해즈히 자오, 칭양 장, 징시, 슈앙 주, 보 Xu 등이 있다. 2023b. X-llm: Bootstrapping은 멀티모달을 외국어로 취급하여 대규모 언어 모델을 고도화하였다. __ arXiv 프리프린트 arXiv:2305.04160_.\n' +
      '* 첸 등은 (2023c) 준첸, 데야오 주, 샤오치안 선, 시앙 리, 자쿤 류, 펭추안 장, 라구르만 크리샤와누키, 비카스 추드라, 윤양 시온그, 모하메드 엘호세니 등이 있다. 2023c. 시력-언어 다중 태스크 학습을 위한 통일된 인터페이스로서 __Minigpt-v2: 큰 언어 모델이다. arXiv 프리프린트 arXiv:2310.09478_.\n' +
      '* 첸 등은 (2023d) Keqin Chen, Zhao Zhang, Weili Zeng, 리봉 Zhang, Feng Zhu 및 Rui Zhao. 2023d. 시크라: 멀티모달 LLM의 리커플링 멀티모달 LLM의 리프레젠테이션 대화 매직. __. arXiv 프리프린트 arXiv:2306.15195_입니다.\n' +
      '* 첸 등은 린 첸, 지송 리, 샤오이동, 판장, 콩휘하, 지아키 왕, 펑자오, 다후린 등이다. 2023e. ShareGPT4V: Better Captions를 사용하여 대용량 멀티-모델들을 개선한다. arXiv 프리프린트 arXiv:2311.12793_.\n' +
      '* 첸 등은 (2023f) 산안 첸, 유우, 코니치 왕, 쇼지 류, 다니엘 토킨스, 주코 첸, 완시낭 체, 샤앙잔 유, 푸루 웨이를 들 수 있다. 2023f. BEAT: 어쿠스틱 토넨저와의 오디오 프리 트레이닝입니다. 기계학습 _국제회의에서는 ICML 2023, 23-29 7월 23일, 호놀룰루, 하와이, USA_, 5178-5193쪽이다.\n' +
      '* 첸 등은 샤우파 첸, 충젠게, 자한통, 장리루왕, 이빙송, 주왕, 핑루오 등이 있다. 2022a. 확장 가능한 시각적 인식을 위한 적응형: 적응형 비전 변압기. __확장형 시각적 인식을 위한 적응형 비전 변환기. 신경정보처리시스템_, 35:16664-16678의 효과.\n' +
      '* 첸 등은 2023년 (2023g) 시 첸, 조시립 도졸롱가, 피엇 파들레세키, 바실 무스타파, 소라비트 창피요, 지알린 우, 카를로스 리퀴멜메 루즈, 세바스티안 굿만, 샤오 왕, 이 테 등. PaLI-X: 멀티언어 비전 및 언어 모델. __ On Scaling은 멀티언어 비전 및 언어 모델입니다. arXiv 프리프린트 arXiv:2305.18565_.\n' +
      '* 첸 등은 2022b(2022b) 시 첸, 샤오 왕, 소라비트 창피요, AJ 피르지오반니, 피엇 파들레세키, 다니엘 살즈, 세바스티안 굿맨, 아담 게르스너, 바실 무스타파, 루카스 베이어 등. 팔리: A 공동 측정 다국어 언어 이미지 모델. __좌표 다언어 언어 이미지 모델. arXiv 프리프린트 arXiv:2209.06794_입니다.\n' +
      '* 첸 등은 (2015) 신레이 첸, 하오 포, 타성지 린, 라마크리슈린 베단텀, 소라브 구파타, 푸엇 덴, 코렌스 지트닉 등이 있다. 데이터 수집 및 평가 서버 __2015 마이크로소프트 코모 캡션: 데이터 수집 및 평가 서버. _2015. 마이크로소프트 코모 캡션: 데이터 수집 및 평가 서버. arXiv 프리프린트 arXiv:1504.00325_입니다.\n' +
      '* 첸 등은 양이첸, 카란식카, 마이클 코그스넬, 허지, 아야 디바키나. >2023. 스트레스: 대형 비전-언어 모델을 구성하여 자연어 피드백을 통해 인간과 정렬하고 상호 작용하도록 구성합니다. __ arXiv 프리프린트 arXiv:2311.10081_.\n' +
      '*치앙 등은 (2023) 위린치앙, 주한리, 지린, 빙선고, 장하오우, 하오장, 리안민정, 시유안주강, 용하오 주앙, 조셉 에르니제일즈, 아이온 스투카, 에릭 퍼시링 등이다. 2023년 비쿠나: 90%* Chat-GPT 품질로 GPT-4를 만족하는 An Open-Source Chatbot.\n' +
      '* 차우더리 등은 (2023) 아악크하 차우디, 샤란 나랑, 샤노브 데브린, 마르트엔 보스마, 가우라바 미슈라, 아담 로버츠, 폴 바햄, 형원정, 찰스 수턴, 세바스티안 게르만 등 2023. 팜. 기계학습연구_ 저널, 24(240):1-113.\n' +
      '*추(2023a) 샤앙샹추(2023a)와 리멍샤오 신양린, 슈앙쉬우, 양양, 예밍후, 페이위, 신유장, 보장, 샤오올린위 등 2023a. 이동식 모바일블름: 모바일 장치에 대한 빠르고 재현 가능하고 강력한 비전 언어 보조기 __모바일vlm: 모바일 디바이스에 대한 빠르고 재현 가능하고 강력한 비전 언어 보조기입니다. arXiv 프리프린트 arXiv:2312.16886_.\n' +
      '*추(2023b) 윤피 추, 진주, 샤오안 주, 기안 양, 장랑 장, 지히 옌, 창주, 진렌 주. 2023b. Qwen-오디오io: 통일된 대규모 오디오-언어 모델을 통한 보편적인 오디오 이해도를 높이는 _Qwen-오디오: 통일된 대규모 오디오-언어 모델을 통해 보편적인 오디오 이해도를 창출한다. arXiv 프리프린트 arXiv:2311.07919_입니다.\n' +
      '* 정씨(2022) 형원정, 르후, 샤인 롱프레, 바렛 조프, 이 테, 윌리엄 페투스, 윤수안 리, 쿠에히 왕, 모스타파 데헤야, 시데카 브라스틱 등 2022년 스칼링 명령어 모델. arXiv 프리프린트 arXiv:2210.11416_.\n' +
      '*의 등이(2024) 카나쿠이, 윤소정 마, 잔카오, 원키안 예, 양주, 카즈하오 리앙, 진타이 커먼, 주안위 루, 지청 양, 쿠에다 랴오 등에 대한 조사. 컴퓨터 비전_페이지 958-979의 적용에 관한 IEEE/CVF 동계 회의의 _검토에서.\n' +
      '*다이 등은 (2023) 원리앙다이, 준난리, 동수리, 앤서니 멍후트 티온그, 준치 자오, 위청 왕, 보양 리, 파스카일 포웅, 스티븐 C. 호이. 2023년 구성BLIP: 명령어 튀김이 있는 노트북 일반 목적 비전 언어 모델. Nural 정보 처리 시스템_에 관한 _35-seventh 콘퍼런스.\n' +
      '* 디트리머 등은 (2023) 팀 디트머, 아르티도로 파니니, 아리 홀츠만, 루크 제트렘거 등이 있다. 2023. 큐라: 정량화된 l1ms의 효율적인 미세 조정. __ 정량화된 l1ms. arXiv 프리프린트 arXiv:2305.14314_.\n' +
      '*동 등은 (2023a) 자아화 동, 원키 류, 양 콩, 간선 등이 있다. 2023a. 수업 증가 학습에 대한 보상을 총체적으로 잊어버린다. 컴퓨터 비전_에 대한 IEEE/CVF 국제 회의의 _검토에서, 페이지 11742-11751.\n' +
      '*동 등은 (2023b) 자아화동, 두천장, 양콩, 웨콩, 허의잉, 덩킨다이 등이 있다. 2023b. 연쇄된 인스테이션 아라비아 세그먼트입니다. 컴퓨터 비전 및 패턴 인식_ 페이지 3934-3943에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '\n' +
      '린하오 동과 보주. 2020. Cif: 엔드 투 엔드 음성 인식을 위한 연속 통합 및 화재이다. I_ICASSP 2020-2020 IEEE 음향, 스피치 및 신호 처리(ICASSP)_ 페이지 6079-6083. IEEE. IEEE.\n' +
      '‘2020’ 알렉시 도소비츠키, 루카스 비이어, 루카스 베이어, 알렉산더 케르니코프, 디르코아 위세노프, 샤오하아 지하이, 토마스 유니터너, 마토스타파 데헤가니, 마타시아 민더러, 게르그 히이폴드, 실베인 젤리 등에서 이미지 인식을 위한 트랜스포머 16x16코드다. M_국제 학습 발표 컨퍼런스_.\n' +
      '* 듀스 등(2023) 다니 데스, 페이샤, 메흐디 SM 사지디, 코피 린치, 아악크하 차와헤리, 브라이언 아이히터, 에아자안 와히드, 조나단 톰슨, 취안 부엉, 톈허 등 2023. arXiv 프리프린트 arXiv:2303.03378_입니다.\n' +
      '* 듀 등은 (2022a) 예판 두, 직강 류, 준이 리, 웨인 신 자오 등이 있다. 2022a. 비전 언어 사전 훈련 모델 설문 조사입니다. 인공 지능에 관한 더러운 제1차 국제 공동 회의인 IJCAI 2022, 빈, 오스트리아, 2022년 7월 23-29일, 5436-5443쪽이다.\n' +
      '* 듀 등은 정시아오 듀, 유지 Qian, 샤오 Liu, 명딩, 지중 Qiu, 지린 양, 지 당 등이 있다. 2022b. GLM: 자기회귀 블랭크 인플링을 통한 일반 언어 모델 재학습. 공동컴퓨팅학회 제60회 연례회의 _검토(1: 롱 파이어스)_, 320-335쪽.\n' +
      '* Fang et al.(2021) 한팡, 펭페이 시온그, 루의Xu, 유헨 등이 있다. 이미지 클립을 통한 마스터링 비디오-텍스트 검색: 이미지 클립을 통한 마스터링 비디오-텍스트 검색(___ip2video: 클립2video: 마스터링 비디오-텍스트 검색)이다. arXiv 프리프린트 arXiv:2106.11097_입니다.\n' +
      '* Fang et al.(2023) Yuxin Fang, Wen 왕, 빈휘 Xie, Quan 선, Ledell 우, Xing강 왕, Tie준 황, 신장 왕, Yue Cao. 2023. Eva: 스케일에서 마스킹된 시각적 표현 학습의 한계를 요약한다. 컴퓨터 비전 및 패턴 인식_페이지에 대한 IEEE/CVF 회의의 _검토에서 19358-19369페이지.\n' +
      '*펑 등은 하오펑(2023) 하오펑, 큐리우, 하오류, 위랑주, 허창리, 운황 등이 있다. 2023. DocPedia: Vers휘발성 문서 양해용 주파수 도메인에서 대형 멀티모달 모델의 전력을 삭제한다. __ arXiv 프리프린트 arXiv:2311.11810_.\n' +
      '* Firoozi et al. (2023) 로하 푸로지, 존아탄 토로지, 스티븐 톈, 안루다하 마즈마다르, 지랭카 선, 웨이유 리우, 유케 주, 슈란 송, 아슈 카푸어, 카롤 하우즈만 등 2023년 로보틱스의 재단 모델. arXiv 프리프린트 arXiv:2312.07843_입니다.\n' +
      '* P.(2023) 차오유푸, 페릭스안 첸, 윤항 선, 예렐리 진, 멍단 장, 잔린, 진루 양, 샤우 정, 커 리, 시잉 선 등은 2023. Mme: 복합 대형 언어 모델에 대한 포괄적인 평가 벤치마크이다. arXiv 프리프린트 arXiv:2306.13394_.\n' +
      '* Fu et al. (2022) Chin-Lun Fu, Zch-Ching Chen, 윤루 리, 헝가리-Yi 이. 2022. 아캡터Bias: NLP 플라스크의 어댑터용 직경 효율적인 토켄 의존적 제시 시프트: 컴퓨팅 언어학 협회의 _ 찾기에서 NAACL 2022_ 페이지는 2608-2621이다.\n' +
      '* 가드레 등 (2023) 사미르 야츠학 가드르, 가브리엘 일하르코, 알렉스 포, 조나단 하나스, 조르지오스 세미니스, 타오 응우옌, 라이언 마르텐, 미첼 위즈만, 다우바 고쉬, 지유 장 등 차세대 멀티모달 데이터 세트를 검색한다. arXiv 프리프린트 arXiv:2304.14108_입니다.\n' +
      '* 버다르 등은 (2023) 노니트 거다하, 알라엘딘 엘-나우비, 주랑 류, 마나티 싱, 칼리옌 바스드프 알왈라, 아르만드 자울린, 이산 미스라가 있다. 2023. 이미지빈드: 모든 것에 결합하기 위한 원 임베딩 공간. 컴퓨터 비전 및 패턴 인식_ 페이지 15180-15190에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '*공 등은 (2023) 도공, 청치유, 샤룽장, 유동왕, 무오정, Qian Zhao, 쿠이쿤 류, 원웨이 장, 핑 루오, 카이 첸 등이다. 인간과의 대화를 위한 비전 및 언어 모델 __멀티모달-gpt: 인간과의 대화를 위한 모델. __멀티모달-gpt: A 비전 및 언어 모델. arXiv 프리프린트 arXiv:2305.04790_입니다.\n' +
      '* 굿펠로우 등 (2013) 이안 조 굿펠라, 메흐디 미자, 다샤오, 아론포빌, 요슈아 벤지오. 2013. __ 구배 기반 신경망에서 재앙적 잊어버림에 대한 경험적 조사. arXiv 프리프린트 arXiv:1312.6211_.\n' +
      '* 고달 등은 (2017) 야시 고일, 테자스 킬, 더글러스 스머스-스테이, 다우루브 바트라, 데비 파라이크이다. 2017. vqa 문제에 v를 착용하고, 시각적 질문에 답하는 이미지 이해의 역할을 해결한다. 컴퓨터 비전 및 패턴 인식_페이지에 대한 IEEE 회의의 _수익에서 6904-6913페이지.\n' +
      '*구 등은 알(2022) 자오시구, 샤오준 멍, 구송 루, 루 하우, 니후 미니허, 샤오단 리앙, 레이웨이 야오, 루니휘 황, 웨이 장, 신장 등 2022년 우룽 등 1억 개의 대규모 중국 교차 학습 전 벤치마킹 벤치마크이다. 신경 정보 처리 시스템_, 35:26418-26431의 발전.\n' +
      '* 구라리 등은 (2018) 다나 구라리, 청리, 압게일 J 스트랑플, 안홍 구오, 치린, 크리스텐 그레이만, 지보 루오, 제프리 파 비삼 등이 있다. 2018년 비즈웨스 대도전: 시각장애인들의 시각적 질문을 해결하십시오. 컴퓨터 비전 및 패턴 인식_ 페이지 3608-3617에 대한 IEEE 회의의 _발표에서.\n' +
      '* He 등은 (2023) 징한고, 하이윤구오, 명당, 진치오 왕이다. 큰 다중 모드 모델에 대한 __.2023. 대규모 다중 모드 모델에 대한 현재 지시 조정. arXiv 프리프린트 arXiv:2311.16206_.\n' +
      '* He et al.(2021) Junxian He, 춘팅 저우, Xuezhe Ma, Taylor Berg-Kirkpatrick 및 Graham Neubig입니다. 2021. 파라직경 효율적인 전송 학습의 통합 뷰를 참조하세요. M_국제 학습 발표 컨퍼런스_.\n' +
      '\n' +
      '하이, 샤앙유 장, 샤오킹 르, 지안 선을 맞는다. 2016년 이미지 인식을 위한 딥 잔차 학습. 컴퓨터 비전 및 패턴 인식_ 페이지 770-778에 대한 IEEE 회의의 _발표에서.\n' +
      '* 호프만(2022) 조던 호프만, 세바스티안 보게우드, 아르투르 엠치, 엘레나 보차차, 트레보르 카야, 엘리자 루터포드, 디에고 데 라스 카사, 리사 안네 헨드릭스, 요하네스 웰블, 에단 클라크 등 2022년 교육을 통해 최적의 대형 언어 모델을 구성했습니다. arXiv 프리프린트 arXiv:2203.15556_입니다.\n' +
      '* 호노비치 등은 (2022) 오리 호노비치, 토마스 인솜, 오머 레비, 티모 슈윅 등이 있다. 자연적 지시: (최적의) 인간 노동력이 없는 튀는 언어 모델: (가장) 인간의 노동력이 없는 튀기는 언어 모델. arXiv 프리프린트 arXiv:2212.09689_.\n' +
      '* 호울스비(2019) 네일 하울스비, 안드레이 기루기우, 스탠리스시 지스트러즈키, 브루나 모리온, 퀘틴 데 라로실헤, 안드레아 게스문도, 모나 아리아얀, 실베인 가리 등이 있다. 2019년 NLP에 대한 직경 효율성 전달 학습. 기계학습_국제회의에서는 2790-2799페이지가 PMLR이다.\n' +
      '* Hsu 등은 (2021) Wei-Ning Hsu, Benjamin 볼테, Yao-Hung Hubert Tsai, 쿠살 라카시아, 루슬란 살락후트디노프, 압델라만 모하메드이다. 히든 유닛에 대한 마스킹 예측에 의한 자기 지도 음성 표현 학습 __Chubert: 숨겨진 유닛의 마스킹에 의한 자기 지도 음성 표현 학습. IEEE/ACM 거래는 오디오, 스피치 및 언어 처리_, 29:3451-3460에 관한 것이다.\n' +
      '* Hu et al.(2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, 셰안 왕, 루 왕, Weizhu Chen, et al. 2021. LoRA: 로크 로어 모델 적응. M_국제 학습 발표 컨퍼런스_.\n' +
      '*황 등은 (2023a) 자우싱황, 징이장, 카이장, 한귀, 샤이앙 루. 2023a. 일반 목적 멀티모달 모델에 튀기는 시각적 지시는 일반적인 목적 멀티모달 모델: 조사: 조사. __ 조사. arXiv 프리프린트 arXiv:2312.16602_.\n' +
      '*황 등(2023b) 봉지황, 명제 리, 동차오 양, 자아통 시, 잔카이 창, 조이예, 윤희우, 지칭 홍, 지아웨이 황, 징린 류, 등 등 2023b. Audioopt: 언어, 음악, 소리, 대화 머리. __의 이해 및 생성: 언어, 음악, 소리, 대화 머리. arXiv 프리프린트 arXiv:2304.12995_.\n' +
      '*황 등(2023c) 샤오한황, 리동, 원희왕, 야루하오, 사샤암신할, 마, 벅차오 Lv, 레이 코이, 오와이스 칸 모하메드, 키앙 류, 등 2023c. 언어가 필요한 것은 아닙니다. _ 언어 모델과의 정렬 인식: 언어 모델과의 업데이트 인식입니다. arXiv 프리프린트 arXiv:2302.14045_입니다.\n' +
      '* 허드슨, 매니닝(2019) 드류 아 허드슨, 크리스토퍼 디 매니닝. 2019년 Gqa: 현실 세계 시각 추론 및 구성 질문 대답을 위한 새로운 데이터셋입니다. 컴퓨터 비전 및 패턴 인식_ 페이지 6700-6709에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '* IDEFICS (2023) IDEFICS. 2023. IDEFICS를 도입: 미술 기간 언어 모델의 오픈 도입.\n' +
      '*아이이어 등은 알(2022) 스리바산 이이어, 시 빅토리아 린, 라마칸트 파수누루, 토도 미하예로프, 다니엘 시미고, 핑유, 커트샵, 톈루 왕, 청 류, 펀릿 싱 쿠우 등 2022년 일반화의 렌즈를 통한 스칼링 언어 모델 지도 메타 학습을 했다. arXiv 프리프린트 arXiv:2212.12017_.\n' +
      '*자아 등은 (2021) 차오자아, 예파이양, 예샤, 이딩첸, 자라나 파레크, 후우 파람, 퀀코 르, 윤호안 성, 줌리, 톰 듀레이가 있다. 2021. 시끄러운 텍스트 감독으로 시각적, 시력-언어 표현 학습을 마무리합니다. 기계학습_국제회의에서는 4904-4916쪽.\n' +
      '* 지안 등은 (2023) 예렌 지안, 충양 가오, 세시 보서기 등이 있다. 2023. 디커플링된 언어 프리트레이닝이 있는 부트스트래핑 비전-언어 학습. Nural 정보 처리 시스템_에 관한 _35-seventh 콘퍼런스.\n' +
      '* 카플 등은 (2018) 쿠슈날 카플, 브라이언 프라이스, 스콧 코헨, 크리스토퍼 가난 등이 있다. 2018 Dvqa: 질문 답변을 통한 이해 데이터 시각화를 참조하세요. 컴퓨터 비전 및 패턴 인식_ 페이지 5648-5656에 대한 IEEE 회의의 _발표에서.\n' +
      '* 마하바디 등은 (2021) 라비히 카라미 마하바디, 제임스 헨더슨, 세바스티안 루더 등이 있다. 효율적인 저 순위 하이퍼복합 어댑터 층인 ___.2021. 직업: 효율적인 저 순위 하이퍼복합 어댑터 계층: 효율적인 저 순위 하이퍼복합 어댑터 계층이다. 신경 정보 처리 시스템_, 34:1022-1035의 발전.\n' +
      '* 카제마자데 등 (2014) 사하라 카제메자데, 비센테 오도네즈, 마크 마텐, 타라 베르가 있다. 2014. 리커리트 게임: 자연 장면 사진의 물건에 대한 전달. 2014년 컨퍼런스 _검토는 자연어 처리(EMNLP)_, 페이지 787-798에서 실증 방법에 관한 것이다.\n' +
      '* 멍위 창 켄톤과 투타노바(2019) 제이콥 데블린 명리 창 켄톤, 이 크리스티나 투타노바 등이 있다. 2019 BERT: 언어 이해를 위한 딥 바이방향 트랜스퍼의 사전 훈련. NAACL-HLT_의 _수익에서 4171-4186 페이지는 있다.\n' +
      '* 키엘라 등은 (2020) 다우위 킬라, 하메드 피로즈, 아라빈드 모한, 베다누즈 고사미, 아만프레트 싱, 프라티크 레이세아, 다빈드 테스트게린 등이다. 2020. 운명 멤스 도전: 다모달 메모에서 혐오 연설을 감지한다. 신경 정보 처리 시스템_, 33:2611-2624에서의 기능.\n' +
      '* 킹마, 웰링(2013) 디에릭 P 킹마, 맥스 웰링. ____ Auto 인코딩 변량 만. _2013 오토 인코딩 변량 만. _2013. 오토 인코딩 변량 만. arXiv 프리프린트 arXiv:1312.6114_입니다.\n' +
      '* 크리샤나 등은 (2017) 란제이 케리샤나, 유케 주, 오리버 그로스, 저스틴 존슨, 켄지 하타, 조슈아 크라비츠, 스테파니 커엔, 야니스 칼란티디스, 리지아 리, 다비드 A 샤마 등 2017년 인파가 밀집된 이미지 주석을 사용하여 언어와 비전을 연결한다. 국제 컴퓨터 비전 저널_, 123:32-73.\n' +
      '* 리스터 등은 (2021) 브라이언 리스터, 라미 알 로푸, 노아 콘스탄트 등이다. 2021. 직경의 비효율적인 프로빗 튀기기를 위한 스코일 파워. 2021년 자연 언어 처리 방법 회의 _검토에서 3045-3059페이지.\n' +
      '\n' +
      '보 리, 원한장, 리앙유 첸, 진하오 왕, 파니 푸, 징강 양, 춘유안 리, 지웨이 류. 2023a. 미미치-it: 멀티-모달 인-텍스트 명령어 튜닝. __Mimic-it: 멀티-모달 인-텍스트 명령어 튜닝. arXiv 프리프린트 arXiv:2306.05425_입니다.\n' +
      '*리 등은 알(2023b) 보하오 리, 리의왕, 광지왕, 유잉게, 요시아오게, 빙샨 등이 있다. 2023b. 세드벤치: 벤클마크 멀티모달 라운드는 생성적 이해를 가진 __Seed-bench: 벤클마크 다중 모드 라인입니다. arXiv 프리프린트 arXiv:2307.16125_입니다.\n' +
      '* Li et al. (2023c) Junnan Li, 동수 Li, 실비오 사바레스 및 스티븐 C. 호이. 2023c. BLIP-2: 냉동 이미지 엔코더 및 대형 언어 모델과의 부트스트래핑 언어 이미지 사전 훈련. 기계학습 _국제회의에서는 ICML 2023, 23-29 7월 23일, 호놀룰루, 하와이, USA_, 19730-19742쪽이다.\n' +
      '* 리 등은 알(2022) 준난 리, 동수 리, 커밍 시온그, 스티븐 호이 등이 있다. 2022. Blip: 통일된 비전-언어 이해와 생성을 위한 Bootstrapping 언어-이미지를 사전 훈련한다. 기계학습_국제회의에서는 12888-12900페이지가 PMLR이다.\n' +
      '* 리는 (2021)준난 리, 라펠라사스 셀바라주, 아하일즈 고트마어, 샤피크 자티, 커밍 시온그, 스티븐 추 홍호이 등이 있다. 퓨즈 전 __2021. 퓨즈: 비전 및 언어 표현 학습과 운동량 증류를 통한 언어 표현 학습. 신경 정보 처리 시스템_, 34:9694-9705에서의 발전이다.\n' +
      '*리 등은 동(2023d) 쿤창리, 진안허, 이왕, Yizhuo 리, 원하이왕, 핑루오, 압리왕, 리인왕, 유키오 등이 있다. 2023d. 비디오차트: 차트 중심 비디오 이해. __ 비디오차트: 차트 중심 비디오 이해. arXiv 프리프린트 arXiv:2305.06355_.\n' +
      '* 리(2023) 리, 유웨이 유인, 시정 리, 리앙 첸, 페이 왕, 슈후이 르, 무카이 리, 야정 양, 징징 Xu, Xu 선 등 2023e. M\\({}^{3}\\)IT: 멀티-모델 다중 언어 명령 튀기기에 대한 대형-스카일 데이터베이스. arXiv 프리프린트 arXiv:2306.04387_.\n' +
      '* 리와 리앙(2021) 샤앙 리와 페시 리앙. 2021. 프레픽스-트닝: 세대용 연속 프로빗을 최적화합니다. 제59회 컴퓨터 통계협회 연차 회의 및 제11차 자연 언어 처리에 관한 국제 공동 회의(1: 롱 파이어)_, 4582-4597쪽)에서 말이다.\n' +
      '* 리 등은 (2020) 시우준 리, 시인, 천유안 리, 펑추안 장, 샤오위 후, 레이 장, 리주안 왕, 허동 후, 리동, 푸루위 등 2020. 오스카: 비전 언어 과제에 대한 사전 학습을 정렬했다. _컴퓨터 비전-ECCV 2020: 제16차 유럽 회의, 글래스고, 영국, 2020년 8월 23-28일, 제작, 파트 XXX 16_, 페이지 121-137. 스프링거.\n' +
      '* 리씨(2023f) 유다 리, 치장, 강유, 지빈왕, 빈푸, 구정린, 춘화선, 링첸, 야치오웨이 등이 있다. 2023f. 스타벨라바: 합성된 이미지 대화 데이터로 튜닝된 시각적 명령어 튜닝. __아벨라바: 합성 이미지 대화 데이터와 함께 실행된 시각적 명령어 튜닝이다. arXiv 프리프린트 arXiv:2308.10253_입니다.\n' +
      '* 리는 (2023g) 예판 리, 예판 두, 쿡 주, 진펑 왕, 웨인 신 자오, 지롱 원 등이 있다. 2023g. 대형 비전 언어 모델에서 오브젝트 홀링을 평가하는 __대시 언어 모델에서 객체 홀링을 평가한다. arXiv 프리프린트 arXiv:2305.10355_.\n' +
      '*리 등은 (2023) 장리, Biao 양, Biao Liu, Qiang Liu, Zhiyin Ma, Shuo Zhang, 진수 양, 야보 선, 유리앙 류, 샤앙 바이 등이 있다. >2023. 모노키: 이미지 해결 및 텍스트 라벨 영역 중요 사물인터넷은 대형 멀티모달 모달에 대해 중요하다. __ arXiv 프리프린트 arXiv:2311.06607_.\n' +
      '* Lin 등 (2024) 홍잔 린, 지양 루오, 보왕, 루히오 양, 징마. 2024. GOAT-Bench: Meme 기반 사회적 Abuse를 통해 대형 멀티모달 모달에 대한 안전 인텐츠. __Meme 기반 소셜 Abuse. arXiv 프리프린트 arXiv:2401.01523_.\n' +
      '* 린 등은 (2023) 지린, 홍수유인, 위핑, 야오루, 파블로 몰찬코프, 앤드루 도, 후지 마오, 얀 커츠, 모하마드 시이비, 송한 등이 있다. <2023. VLA: __비주얼언어모달의 사전훈련>. __비주얼언어모달의 사전훈련. arXiv 프리프린트 arXiv:2312.07533_.\n' +
      '* 린 등은 (2014) 타성이린, 마이클 마이어, 세르게 벨롱기, 제임스 하이이스, 피에트로 페로나, 데바 라만, 프리오르 딜, 코렌스 지트닉 등이 있다. 2014. 마이크로소프트 코모: 맥락의 공통 객체. "컴퓨터 비전-ECCV 2014: 제13차 유럽 회의, 스위스 취리히, 2014년 9월 6-12일, 제작, 파트 V 13_, 페이지 740-755. 스프링거.\n' +
      '* 류 등은 (2023a) 포앙유 류, 가이 에슨, 니겔 콜라이 등이 있다. 2023a. 시각 공간 추론 __ 시각적 공간 추론. __ 시각적 공간 추론. __ 시각 공간 추론. 컴퓨팅 언어학협회, 11:635-651의 거래.\n' +
      '* 류(2023b) 하비 류, 제화 첸, 이위안, 신하오 메이, 주보 류, 다닐로 P. 만디치, 원우 왕, 마크 디덤블리 등이 있다. 2023b. AudioLDM: 라텐트 디퓨전 모텔이 있는 텍스트에서 아우디오 세대입니다. 기계학습 _국제회의에서는 2023년 7월 23-29일 ICML 2023, 호놀룰루, 하와이, USA_, 21450-21474페이지가 있다.\n' +
      '* 류 등은 (2023c) 하비 류, Qiao 톈, 이위안, Xubo Liu, Xubo Liu, 신하오 메이, Qiqiang콩, 유빙왕, 원우왕, 유수왕, 유수왕, 마크 다이덤슬리 등이 있다. 2023c. 자기관리증을 가진 __Audio-cDM 2: 학습 홀리스틱 오디리오 세대는 자기관리 기술을 가지고 있다. CoRR_, 복근/2308.05734.\n' +
      '* 류 등은 (2023d) 해토피아 류, 춘유안 리, 유정 리, 용재 이씨 등이 있다. 2023d. 비주얼 지시가 튀는 바젤을 개선했습니다. _NeurIPS 2023 워크샵에서 조달 및 명령 Following_에 관한 작업입니다.\n' +
      '* 류 등은 (2023) 해오티안 류, 춘유안 리, 청양 우, 용재 이씨 등이 있다. 2023e. 시각적 보증이 튀겨요. Nural 정보 처리 시스템_에 관한 _35-seventh 콘퍼런스.\n' +
      '* 류(2023f) 유안 류, 하동 단안, 원한 장, 보 리, 송양 장, 왕보 자오, 야이크 원, 자이 왕, 콩희 하이, 지웨이 류, 등 2023f. Mmbench: 멀티 모달 모델이 만능 플레이어입니까? arXiv 프리프린트 arXiv:2307.06281_.\n' +
      '* 롱 et al. (2022) Siqu 롱, Feiqi Cao, 소연 카렌 한, Haiqin 양. 2022년 비전 및 언어 재배치 모델: 조사. 인공 지능에 관한 더러운 제1차 국제 공동 회의인 IJCAI 2022, 빈, 오스트리아, 2022년 7월 23-29일, 5530-5537쪽이다.\n' +
      '\n' +
      '판루, 세로필 미슈라, 탕린샤, 리앙큐, 카이위창, 송춘주, 오이빈드 타프조드, 피터 클락, 아슈윈 칼라이언. 2022. _1972. 과학 질문 답변에 대한 사고 사슬을 통한 멀티모달 추론: 과학 질문 답변에 대한 사고 사슬을 통한 멀티모달 추론을 설명해야 한다. 신경정보처리시스템_, 35:2507-2521의 효과.\n' +
      '* 루 등은 (2021) 판 루, 리앙큐, 지아치 첸, 토니샤, 요이저우 자오, 웨이 장, 주유, 샤오단 류, 송춘 주 등이 있다. 2021. Iconqa: 추상적 다이어그램 이해와 시각적 언어 추론을 위한 새로운 벤치마크이다. 신경정보처리시스템(Datasets) 및 벤치마크 트래크(Round 2)_에 관한 _31번째 회의에서.\n' +
      '* 루오 등은 (2023) 지양 루오, 칸 주, 푸 자오, 칭펑 선, 시우보 팽, 원서앙 후, 충양 도, 징마, 칭웨이 린, 다신 장이다. Evol-강사가 있는 황후 코드 대어 모델:2023. 위저드 코더:황후 코드 대어 모델. __ 및 Evol-강제가 있다. arXiv 프리프린트 arXiv:2306.08568_입니다.\n' +
      '* 마즈 등은 (2023) 무함마드 마자, 하노나 라셰드, 살만 칸, 파하드 샤바즈 칸 등이 있다. 비디오-ChatGPT: 토워드 디바이저는 대형 비전 및 언어 모델을 통해 비디오 이해를 설명했습니다. __영상-ChatGPT. arXiv 프리프린트 arXiv:2306.05424_.\n' +
      '* 마테와 (2021) 미네쉬 마테와, 디모스테니스 카라자스, CV 자와하르. 2021 Docvqa: 문서 영상에서 vqa에 대한 데이터셋. 컴퓨터 비전_의 적용에 대한 IEEE/CVF 겨울 회의의 _발표에서 2200-2209 페이지가 된다.\n' +
      '* 맥클로스키와 코헨(1989) 마이클 맥클로스키와 네알 J 코언. 1989년 연결주의 네트워크에서 영양적 간섭: 순차적 학습 문제. 학습과 동기_심리학에서 부피 24, 페이지 109-165. Elsevier.\n' +
      '* Mei 등은 (2023) 신하오 메이, 추통 멍, 하오허 류, 기우창 콩, 톰 고, 청치 자오, 마크 도플리, 유시아 자우, 원우 왕 등이 있다. 2023. Wvacaps: 오디오-언어 다중 모드 연구를 위한 대화식 보조 오디오 캡션 데이터셋. __ 챗봇 보조 오디오 캡션 데이터셋. arXiv 프리프린트 arXiv:2303.17395_입니다.\n' +
      '* 미샤라 등은 (2019) 안드 미샤라, 샤슈크 셰카하, 아제트 쿠마르 싱, 안리반 샤크라비 등이 있다. 2019 Ocr-vqa: 이미지에서 텍스트를 읽음으로써 답하는 시각적 질문이다. <2019년 문서 분석 및 인식 국제회의>(ICDAR)_, 페이지 947-952. IEEE.\n' +
      '* 무(2023) 요오무, 칭롱장, 멍강후, 원하이왕, 명유딩, 준진, 빈왕, 지펑다이, 유샤오, 핑루오 등이 있다. 2023. 엠보디케이트: 체화된 사고 사슬을 통한 비전 언어 사전 훈련. Nural 정보 처리 시스템_에 관한 _35-seventh 콘퍼런스.\n' +
      '* 나베드 등은 (2023) 허마자 나베드, 아사드 우야 칸, 시큐, 무함마드 사칭, 새드 안워, 무함마드 우스만, 닉 바네스, 아주말 미안 등이다. 큰 언어 모델에 대한 포괄적인 개요. __.2023. __ 큰 언어 모델에 대한 포괄적인 개요. arXiv 프리프린트 arXiv:2307.06435_입니다.\n' +
      '* 오픈AI(2022) 오픈AI입니다. 2022. OpenAI: ChatGPT를 생성합니다.\n' +
      '* 오픈AI(2023) 오픈AI입니다. 2023. GPT-4 기술 보고서.\n' +
      '* 오도네즈 등은 (2011) 비센테 오도네즈, 기리시 굴카리, 타라 버그 등이 있다. 2011. Im2텍스트: 100만 개의 자막 사진을 사용한 영상 설명: __. Im2텍스트이다. 신경 정보 처리 시스템_, 24의 기능.\n' +
      '*오양(2022) 롱오우양, 제프리우, 주장, 디아고 알미다, 카롤 웨인웨이드, 파멜라 미시킨, 총 장, 샌히니 아가왈, 카타리나 슬라, 알렉스 레이 등 2022년 언어 모델을 통해 인간의 피드백을 받아 지침을 따르도록 했다. 신경정보처리시스템_, 35:27730-27744의 효과.\n' +
      '* 파타고포울루 등은 (2023) 아르테미스 판가굴루, 르 샤에, 닝유, 준난리, 동크스우 리, 샤피크 자티, 라안 Xu, 실비오 사바레, 시임 시온그, 후안 카를로스 누에블 등이 있다. 2023. X-구조BLIP: X-모달 지시 인식 표현을 LLM 및 에르고텐트 크로스-모달 컨버팅에 정렬하기 위한 프레임워크. _. arXiv 프리프린트 arXiv:2311.18799_.\n' +
      '*펑 등은 (2023) 장랑펑, 원희왕, 리동, 야루 하오, 샤오한 황, 슈밍 마, 푸루 웨이를 포함한다. 코스모스-2: 페이딩 멀티모달 대형 언어 모델. __ 세계 대문 모델. arXiv 프리프린트 arXiv:2306.14824_.\n' +
      '* Radford et al.(2021) 알레크 라드포드, 종욱 김, 크리스 홀리스, 아디아 레즈, 가브리엘 고, 샌히니 아가왈, 기리시 사스트리, 아미다 아셀, 파멜라 미슈킨, 잭 클락 등 2021년 학습형 시각적 모델을 자연어 감독으로 전달합니다. 기계학습_국제회의에서는 8748-8763쪽 PMLR.\n' +
      '* 라드포드 등 (2023) 알레크 라드포드, 김종욱, 도Xu, 그레그 브룩만, 크리스틴 매리비, 아이라이아 세이츠케버. 2023년 대형 스코일 위크 감독을 통한 로부스 스피치 인식입니다. 기계학습 _국제회의에서 ICML 2023, 23-29 7월 23일, 호놀룰루, 하와이, USA_, 28492-28518쪽이다.\n' +
      '* 라펠 등은 (2020) 콜린 라펠, 노암 샤제르, 아담 로버츠, 캐서린 리, 샤이니 나랑, 마이클 마테나, 옌치 주, 웨이 리, 피터 J Liu 등이다. 2020년 통일된 텍스트-텍스트 변압기로 전이 학습의 한계를 탐색한다. __, 통합 텍스트-텍스트 변압기로 탐색한다. 기계학습연구_ 저널 21(1):5485-5551.\n' +
      '* 레버피 등은 (2017) 실베레-알비즈 레버피, 학안 빌렌 및 안드레아 베달디. 2017. __2017. 잔차 어댑터가 있는 여러 시각 영역을 학습했다. 신경 정보 처리 시스템_, 30의 정보를 제공한다.\n' +
      '* 로빈스(1995) 앤서니 로빈스. 1995. __위축성 망각, 재혼 및 가유충성 망각, 재혼 및 가유충성 망명.__위축성 망명. 연결 과학_, 7(2):123-146.\n' +
      '* 람바흐 등(2022) 로빈 라이바흐, 안드레아스 블라트만, 도미니크 로렌츠, 패트릭 에저, 비콘 오머 등이 있다. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성입니다. 컴퓨터 비전 및 패턴 인식_ 페이지 10684-10695에 대한 IEEE/CVF 회의의 _프로젝션에서.\n' +
      '* 롬바흐 등은 (2019) 오라프 로네버거, 필리필 피셔, 토마스 브로크스 등이다. 2015. U-net: 생물의학적 이미지 세분화를 위한 컨솔루션 네트워크. E_ 의료 이미지 컴퓨팅 및 컴퓨터 보조 개입-MICCAI 2015: 제18차 국제 회의, 뮌헨, 독일, 2015년 10월 5-9일, 제작, 파트 III 18_, 페이지 234-241. 스프링거.\n' +
      '*루안, 진(2022) 루단 루안, 진진 등이 있다. 트랜스포머 기반 비디오 언어 사전 훈련:트랜스포머 기반 비디오 언어 사전 훈련: __2022 조사:트랜스포머 기반 비디오 언어 사전 훈련. AI 오픈_, 3:1-13.\n' +
      '*영업력(2022) 판매력. 2022. 울립.\n' +
      '* 슈하만(2022) 크리스토프 슈하만(2022) 크리스틴 슈하만, 로메인 베어먼, 리처드 버몬, 케이드 고든, 로즈 웨이트만, 메흐디 커티, 테오 코퍼티, 아루시 카타, 클레이튼 풀리스, 미첼 위츠만 등 2022년 라온-5b는 차세대 이미지-텍스트 모델을 훈련하기 위한 대규모 데이터세트를 오픈한다. 신경정보처리시스템_, 35:25278-25294의 효과.\n' +
      '* 슈하만 등(2022b) 크리스토프 슈하만, 안드레아스 코프, 리처드 빈쿠, 테오 코브리스, 로메인 베아몬트 등이다. 2022b. 라온코코: laion2b-en의 600m 합성 캡션입니다.\n' +
      '* 슈하만 등은 (2021) 크리스토프 슈하만, 리처드 베르누, 로메인 베아룸트, 로베르트 카카마키크, 클레이튼 갈리스, 아루시 카타, 테오 코바스, 제니아 지트세프, 아란 코마쓰자키 등이다. V__ Laion-400m: 클립 필터링된 4억 이미지-텍스트 쌍의 오픈 데이터셋. __ Laion-400m: 클립 필터링된 4억 이미지-텍스트 쌍의 오픈 데이터셋. arXiv 프리프린트 arXiv:2111.02114_.\n' +
      '* 슈웬크 등은 (2022) 더스틴 슈웬크, 아틀레브 케델왈, 크리스토퍼 클라크, 케네스 마리노, 루 우즈베크 모타게이 등이 있다. 2022. A-okvqa: 세계 지식을 사용하여 답하는 시각적 질문에 대한 벤치마킹. 컴퓨터 비전_에 관한 _유럽 회의에서 146-162 페이지는 스프링거이다.\n' +
      '* 샤마 등은 (2018) Piyush Sharma, Nan Ding, Sebastian 굿만 및 Radu Soricut. 2018년 개념 캡션: 자동 이미지 캡싱을 위한 세척, 과공, 이미지 제트-텍스트 데이터셋. 제56회 컴퓨터 통계협회 연차 회의(1: 롱 파이어)_, 2556-2565쪽)의 _검토에서.\n' +
      '* 선 등은 (2023) 용리강 선, 미라오 송, 주탄, 동정 리, 바이밍 루, 유 만남 주앙 등이 있다. >2023. 하그징그롭: __ 우깅그롭: 챗봇과 친구들과의 ai 작업을 포옹에 해결한다. arXiv 프리프린트 arXiv:2303.17580_.\n' +
      '* 시도로프 등은 (2020) 올레키 시도로프, 루항 후, 마커스 노르바흐, 아만프로트 싱이다. 2020. Textcaps: 읽기 이해력으로 이미지 캡싱을 위한 데이터셋입니다. _컴퓨터 비전-ECCV 2020: 제16차 유럽 회의, 글라스고, 영국, 2020년 8월 23-28일, 제작, 파트 II 16_, 페이지 742-758. 스프링거.\n' +
      '* 싱 등은 (2019) 아만프레트 싱, 비브크 나타잔, 메트 샤, 유장, 신글리 첸, 디루 바트라, 데비 파라크, 마커스 노르바흐 등이다. 2019년 독서를 할 수 있는 타우드 브카 모델. 컴퓨터 비전 및 패턴 인식_ 페이지 8317-8326에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '* 송 등은 (2023) 셰정송, 샤오펑리, 샤샤하리 등이 있다. 다중모달 대언어모델 종합조사: 다중모달 대언어모델에 대한 포괄적인 조사: 모델 간 갑을 브리지하는 방법은2023. arXiv 프리프린트 arXiv:2311.07594_.\n' +
      '* 수 등은 (2023) 유산수, 톈란, 후예앙 리, 지알루 주, 옌왕, 덩카이 등이 있다. 한 모형은 __andagpt: _.2023. Pandagpt: 한 모델을 사용하여 모든 것을 지시-제거한다. arXiv 프리프린트 arXiv:2305.16355_.\n' +
      '*선(2023) 지칭선, 선가선, 선가오오, 하오타오, 천유안 류, 천유안 리, 요강간, 추앙간, 리앙야, 유시온고 왕, 예밍 양 등 2023년 사실적으로 증강한 대형 멀티모달 모델을 정렬한다. arXiv 프리프린트 arXiv:2309.14525_.\n' +
      '* 수리이스 등 (2023) 디다크 수리스, 사치트 메논, 카를 본드릭 등이다. 추론을 위한 피톤 실행을 통한 비주얼 추론: 추론에 대한 피톤 실행을 통한 시각적 추론. __2023. Vipergrpt: 비주얼 추론. arXiv 프리프린트 arXiv:2303.08128_입니다.\n' +
      '* 탕 등은 (2023) 지윈 당, 지이 양, 마흐무드 카다미, 양 류, 첸구랑 주, 모하트 반살 등이 있다. 2023a. CoDi-2: In-Context, 인터리빙 및 인터액티브 Any-to-모든 세대는 __n-Context, 인터리빙 및 인터액티브 Any-2이다. arXiv 프리프린트 arXiv:2311.18775_입니다.\n' +
      '* 탕 등은 지윈탕, 지이양, 선구앙주, 마이클생, 모하트반살 등이 있다. 2023b. 복합 가능한 디퓨전(Diffusion)을 통해 모든 세대를 능가합니다. Nural 정보 처리 시스템_에 관한 _35-seventh 콘퍼런스.\n' +
      '* 테이(2022) 이테이, 모스타파 데헤이니, 비네 큐 트란, 사비에라 가르시아, 제이슨 웨이, 쉬에히 왕, 형원정, 다라 바히, 탈슈머, 스티븐 정 등 2022. Ul2: 언어 학습 패러다임을 통일한다. _Eleventh 국제 학습 발표회의_.\n' +
      '*팀, 알(2023) 게미니팀, 로한안일, 세바스티안 보게우, 용희우, 장바스티테 알레이크, 자아휘유, 라두소굿, 요한 슐크, 안드루 마다이, 안자 하우스 등 2023. 게미니 가족. arXiv 프리프린트 arXiv:2312.11805_.\n' +
      '* Touvron et al.(2023a) Hugo Touvron, Tmonaryaut Lavril, Gautier Izacard, Xavier 마르티네, Marie-Anne Lachaux, 팀에서ot희 Lacroix, Baptiste Roziere, 남안 고살, Eaisal Azro, Faisal Azhar, et al. 라마: 오픈과 효율적인 기초 언어 모델 __Llama: 오픈과 효율적인 기초 언어 모델. arXiv 프리프린트 arXiv:2302.13971_.\n' +
      '* 투브론 등은 (2023b) 허고 타우브론, 루이 마틴, 케빈 스톤, 피터 알베르트, 암자드 알마헤이, 야스민 바브레이, 니콜레이 바시코프, 소우미아 바트라, 프라지왈 바하바바, 샤르티 보세일 등. 라마2: 오픈 파운데이션과 미세 조정 채팅 모델 __라마2: 오픈 파운데이션 및 미세 조정 채팅 모델. arXiv 프리프린트 arXiv:2307.09288_.\n' +
      '* 바스완이 등은 (2017) 아샤시 바소와이, 노암 샤제리, 니키 파마르, 작노 우즈코레이트, 리온 존스, 디단 노 고메즈, 루카즈 카이저, 일리아 폴로숙신 등이 있다. 2017년 __의도 모두 필요한 사항입니다. 신경 정보 처리 시스템_, 30의 정보를 제공한다.\n' +
      '* 바스완이 등은 (2017)펑왕, 안양, 루이메, 준양린, 슈이바이, 주상리, 지앙리, 지아니신마, 창주, 진렌주, 홍시아양 등이다. 2022a. 즉, 단순한 서열 간 학습 프레임워크를 통해 아키텍처, 과제 및 양식을 일원화하는 것이다. 기계 학습_ 국제 회의에서 23318-23340 페이지는 PMLR이다.\n' +
      '*왕(2022) 위한왕(2022) 위송 Lv, 진송 Lv, 원멍유, 원이홍, 지큐, 옌왕, 준희지, 주이양, 레이 자오, 시수안 송 등 2023. Cogvlm: 비주얼 전문가. arXiv 프리프린트 arXiv:2311.03079_입니다.\n' +
      '*왕(2022b) 원희왕, 항보바오, 리동, 조한보르크, 지랑펑, 기랑리루, 키티 아가왈, 오와이스 칸 모하메드, 사샤암 싱할, 수호지트솜 등 2022b. 외국어로서의 이미지, 즉 모든 비전 및 시력-언어 과제에 대한 비트의 척: 외국어로서의 이미지이다. _. arXiv 프리프린트 arXiv:2208.10442_.\n' +
      '*웨이 등은 (2021) 제이슨웨이, 마마르텐 보스마, 빈센트 자오, 켈빈 구우, 아담스 웨이유, 브라이언 리스터, 난두, 앤드류 M다이, 콰크 V 르 등이 있다. 2021. Finetuned 언어 모델은 제로샷 학습자입니다. M_국제 학습 발표 컨퍼런스_.\n' +
      '* 우 등은 (2023a) 첸페이 우, 선밍 진, 위즈언 키, 샤오동 왕, 제청 당, 난두안 등이다. 2023a. 시각적 채팅:톡킹, 드로잉, 비주얼 기반 모델이 있는 편집: __시각적 채팅: 시각적인 채팅: 시각화 파운데이션 모델. arXiv 프리프린트 arXiv:2303.04671_.\n' +
      '* 우(2023b) 하닝 우, 지청 장, 에르리 장, 차오펑 첸, 리앙 랴오, 안난 왕, 천이 리, 위시우 선, 광타오 자하이, 등 2023b. 저수준의 비전에 대한 범용 기반 모델의 벤치마크 __Qbench: 범용 기반 모델의 벤치마크. arXiv 프리프린트 arXiv:2309.14181_입니다.\n' +
      '* 우 등은 2017년 (2017) 자중우, 허정, 보 자오, 유신 리, 배밍 옌, 로이 리앙, 웨니아 왕, 시비아 저우, 구젠 린, 옌웨이 푸 등 영상 이해에 더 깊이 나서기 위한 대규모 데이터셋. arXiv 프리프린트 arXiv:1711.06475_입니다.\n' +
      '* 우 등은 (2023c) 지예앙 우, 원청 간, 제펑 첸, 시정완, 필립 스유. 2023c. 다중모달 대형 언어 모델: A 조사: __다모달 대형 언어 모델: A 설문. arXiv 프리프린트 arXiv:2311.13165_.\n' +
      '* 우 등은 (2023d) 선고치오 우, 하오 페이, 레강 Qu, 웨지, 토트-생 추아이다. 2023d. 다음pt: Any-to-any 다중 모드 llm. __많은 다중 모드 llm. arXiv 프리프린트 arXiv:2309.05519_입니다.\n' +
      '*Xu et al. (2016) Jun Xu, Tao Mei, Ting Yao 및 용루이. 2016. Msr-vtt: 비디오와 언어를 가교하기 위한 대형 비디오 기술 데이터셋입니다. 컴퓨터 비전 및 패턴 인식_페이지에 대한 IEEE 컨퍼런스의 _프로젝션에서 5288-5296페이지.\n' +
      '* Xu(2023a) Rongtao Xu, 창웨이 왕, 지시 선, 시비아오 Xu, 위리랑 멍, 샤오펑 장 등이 있다. 2023a. 셀프 대응 교차로 엔드-엔드 위콜리-초지도 다좌위 서열화이다. 인공 지능_에 관한 AAAI 회의의 _검토에서.\n' +
      '* Xu(2023b) Rongtao Xu, 창웨이 왕, 지구강 장, 시비아오 Xu, 위리랑 멍, 샤오펑 장 등이 있다. 2023b. Rssformer: 원격 감지 토지-커버 분할을 위한 포그라운드 염도 향상. __ 원격 감지 토지-커버 분할을 위한 포그라운드 개선. 이미지 처리_, 32:1052-1064에 대한 IEEE 거래.\n' +
      '* 옌 et al.(2021) 루이 옌, 마이크 정슈, 요시아오 거, 알렉스 진펑 왕, 주동 린, 구야누 캐이, 진희 당 등이 있다. 학습된 지역과의 영상-텍스트 사전 훈련. __ 비디오-텍스트 사전 훈련.2021. __ 학습된 지역과의 영상-텍스트 사전 훈련. arXiv 프리프린트 arXiv:2112.01194_.\n' +
      '* 양 등은 (2022)진유 양, 지알리 단안, 손 트란, 이누, 삼파스 차다, 리쿤 첸, 벨린다 짐, 트라이틀 칠보리, 준저우 황 등을 들 수 있다. 2022. 비전 언어는 삼중 대조 학습으로 사전 훈련한다. 컴퓨터 비전 및 패턴 인식_ 페이지 15671-15680에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '* 양 등은 (2023)선규안양, 린지리, 지안펑왕, 케빈린, 에한산 아자나브, 피살 아메드, 시정 류, 세리우, 마이클생, 리주안왕 등이 있다. 2023. Mm-react: 다중 모드 추론 및 작용에 대한 프로모팅 챗봇: 다중 모드 추론 및 작용에 대한 프로모팅 채팅. __-react. arXiv 프리프린트 arXiv:2303.11381_.\n' +
      '*예(2023) 칭하오예, 하양수, 구오하이수, 지바오예, 명옌, 이양주, 준양왕, 안웬후, 펑청시, 야야시 등 2023.mplng Shi, et al. arXiv 프리프린트 arXiv:2304.14178_입니다.\n' +
      '*유인(2023a) 꽹유인, 차오유푸, 시의자오, 케리, 시잉선, 통주, 엔홍첸 등이 있다. 2023a. 멀티모달 대어 모델 조사 __ 멀티모달 대어 모델 조사. arXiv 프리프린트 arXiv:2306.13549_입니다.\n' +
      '*유인(2023b) 지네페이 유인, 지온가 왕, 지안자오, 자운시, 딩닝리, 묵마이 리, 루 선가, 레이바이, 샤오희황, 지용왕 등. 삼: 언어 지원 다중 모달 명령 조정 데이터 세트, 프레임워크 및 벤치마크. __언어 지원 데이터 세트, 프레임워크 및 벤치마크입니다. arXiv 프리프린트 arXiv:2306.06687_.\n' +
      '*영 등은 (2014) 피터영, 앨리스라이, 미라 호도시, 줄리아 호켄마이어 등이 있다. 2014년 이미지 설명에서 시각적 데코트에 대한 이미지 설명: 이벤트 설명보다 의미론적 추론을 위한 새로운 유사성 메트릭: 시각적 데코팅을 위한 새로운 유사성 메트릭이다. 컴퓨팅 로직_, 2:67-78에 대한 협회의 거래.\n' +
      '*유 등은 (2016) 리청유, 패트릭 포로손, 샨양, 알렉산더 C 버그, 타라 라 베르가 있다. 2016년은 표현을 지칭하는 맥락을 모델링한다. "_컴퓨터 비전-ECCV 2016: 제14차 유럽 회의, 암스테르담, 네덜란드, 2016년 10월 11-14일, 제작, 파트 II 14_, 페이지 69-85. 스프링거.\n' +
      '*유 등은 알(2023) 위하오 유, 지니규안 양, 린지 리, 지안펑 왕, 케빈 린, 지청 류, 시치오 왕, 리주안 왕이다. 2023.Mm-vet: 통합 역량을 위한 대형 멀티모달 모델을 평가하는 __Mm-vet: 통합 역량을 위한 대형 멀티모달 모델을 평가한다. arXiv 프리프린트 arXiv:2308.02490_입니다.\n' +
      '*유 등은 알(2022) 루민유, 뤼루 당, 용밍 라오, 티준 황, 지주, 지웬 루 등이 있다. 2022. 포인트-버트: 마스크 포인트 모델링이 있는 사전 훈련 3d 포인트 클라우드 변압기입니다. 컴퓨터 비전 및 패턴 인식_ 페이지 1041-1042에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '\n' +
      '컴퓨터 비전 및 패턴 인식_ 페이지 19313-19322에 대한 세션.\n' +
      '*자셀러 등은 알(2022) 로완 자셀러, 지아센 루, 시밍 루, 영재 유, 옌펑 자오, 모하마드레자 세일히, 아디샤 쿠수파티, 잭 헤셀, 알리 파하디, 예진 최씨 등이다. 2022. 메로트 예비: 비전과 언어와 소리를 통한 신경 스크립트 지식. 컴퓨터 비전 및 패턴 인식_페이지에 대한 IEEE/CVF 회의의 _검토에서 16375-16387 페이지가 있다.\n' +
      '*젠 등은 알(2022a) 아오한생, 샤오류, 자정시아오두, 자한왕, 하누라이, 명딩, 주이양, 예파안시, 위니정, 샤오샤, 등 알 알. GLM-130B: An Open B언어 사전 학습 모델. _Eleventh 국제 학습 발표회의_.\n' +
      '*Zeng et al.(2022b) 옌멍, 신송장, 항리. 2022b. 다중 학습된 비전 언어 사전 훈련: 비주얼 컨셉트와 함께 텍스트를 정렬하세요. 기계학습_국제회의에서는 25994-26009쪽.\n' +
      '* 장 등은 동장(2023a) 동장, 심인 리, 신장 장, 준한, 펑유 왕, 야키안 주, 시펑 키우 등이 있다. 2023a. 스피치GPT: 인트라믹 크로스-모달 대화성을 가진 대형 언어 모델황후입니다. 컴퓨팅 언어학 협회의 _ 찾기에서 EMNLP 2023, 싱가포르, 12월 6-10, 2023_, 15757-15773 페이지가 있다.\n' +
      '*장 등은 (2023b) 두천장, 웨이 콩, 자아화동, 야한유, 시우이 천, 용강장, 선방 등이 있다. 2023b. 유병적 포스팅 없이 지속적인 남인격 인정. _ 2023년 자연 언어 처리_에서 실증적 방법에 관한 컨퍼런스이다.\n' +
      '* 장 등은 두천 장, 홍리루 리, 웨이 콩, 루타오 주, 자아화 동, 시우이 첸 등을 들 수 있다. 2023c. 과제 관계 증류 및 개체 인식이라는 증분성을 위한 원형 유사 라벨이다. 제32회 ACM 국제정보지식관리회의 _검토에서 3319-3329쪽.\n' +
      '* 장 등은 (2023d) 두천 장, 야한 유, 필롱 첸, 시우이 첸이다. 2023d. 위증적인 남인격 인정을 위한 로테이션 도동을 감소시킵니다. 제46차 국제 ACM SIGIR 컨퍼런스 프로그램 정보 리트리발_ 페이지 1919-1923.\n' +
      '*장 등은 두천장, 틸린장, 윤청주아, 청유왕, 보주 등이 있다. 2022a. 스피킹 신경네트웍스의 최근 어드밴스 및 뉴 프론티어즈입니다. 인공 지능에 관한 더러운 제1차 국제 공동 회의인 IJCAI 2022, 빈, 오스트리아, 2022년 7월 23-29일, 5670-5677쪽이다.\n' +
      '* 시스템 실증, 싱가포르, 12월 6-10일, 2023_페이지 543-553.\n' +
      '* 장 등은 (2020) 제프리 오장, 알렉산더 스락스, 아미르 자미르, 레오니다스 구이바스, 지텐드라 말릭 등이 있다. 2020. 사이드-튜닝: 부가적인 측면 네트워크를 통한 네트워크 적응을 위한 베이스라인이다. _컴퓨터 비전-ECCV 2020: 제16차 유럽 회의, 글래스고, 영국, 2020년 8월 23-28일, 제작, 파트 III 16_, 페이지 698-714. 스프링거.\n' +
      '*장(2020) 수산장, 스테판 롤러, 남안 고살, 미켈 아르테크스, 모야 첸, 샤오의 첸, 크리스토퍼 듀란, 몬아 디브, 시안 리, 시 빅토리아 린 등이 2022b. 최적: 오픈 사전 훈련된 변압기 언어 모델. __오픈 사전 훈련된 변압기 언어 모델. arXiv 프리프린트 arXiv:2205.01068_입니다.\n' +
      '*장 등은 옌즈허 장(2023f) 옌즈허 장, 루이 장, 지시아앙 구, 유판 주, 누디임 리포카, 디이 양, 통선 등이 있다. 2023f. 슬라비아: 텍스트가 풍부한 이미지 이해를 위해 튜닝된 시각적 명령어 튜닝. __ 텍스트가 풍부한 이미지 이해를 위한 강화 시각 명령어 튜닝이다. arXiv 프리프린트 arXiv:2306.17107_입니다.\n' +
      '* 자오 등은 알(2023a) 보 자오, 보나 우, 티준 황 등이 있다. 2023a. 비트: 시각적 지시 튜닝. _Svit: 시각 지시 튜닝 스칼링 업. arXiv 프리프린트 arXiv:2307.04087_입니다.\n' +
      '* 자오(2023b) 리앙 자오, 엔유, 정거, 진롱 양, 하란 위, 홍유 저우, 지안 지안 선, 유앙 펑, 루니 동, 춘루이 한 등 2023b. 정밀한 참고 명령어 튜닝을 통해 __Chatspot: Bootstrapping 멀티모달 루츠: Bootstrapping 멀티모달 라운드가 있습니다. arXiv 프리프린트 arXiv:2307.09474_.\n' +
      '* 2022. 12. 9.\n' +
      '* 자오 등은 2023c. 대형 언어 모델에 대한 설문 조사 __ 대형 언어 모델 조사. __ arXiv 프리프린트 arXiv:2303.18223_.\n' +
      '* 자오 등(2023d) 양자오, 지히린, 다칸주, 자룽황, 지시펑, 빙이강 등이 있다. 2023d. 부보그릿: __ 멀티 모달 lms에서 시각적 접지력을 구현한다. arXiv 프리프린트 arXiv:2307.08581_.\n' +
      '*정 등은 (2024) 준하오 정, 치안리 마, 줌 류, 빌란 우, 화웬 펑 등이 있다. 2024. 비접지 방지 목적 : Positive Forward 트랜스퍼로 튀는 멀티모달 컨템포러리 명령어 __를 넘어. arXiv 프리프린트 arXiv:2401.09181_.\n' +
      '* 정 등은 (2023a) 준하오 정, 선기귀, 게리 마. 2023a. 학습하거나 읽나요? 사전 훈련된 언어모달과 함께, __교육형 언어학습으로 수정형 어학학습이 이루어졌다. arXiv 프리프린트 arXiv:2312.07887_.\n' +
      '* 정 등은 (2023b) 카이저히 정, 주하이 하, 신장 에릭 왕 등이 있다. 2023b. 민글로트-5: 생성 보켄스를 통한 인터리빙된 비전 및 언어 생성. __ 인터리빙된 비전과 언어 생성. arXiv 프리프린트 arXiv:2310.02239_.\n' +
      '* 주 등은 알(2023a) 드야오 주, 준 첸, 샤오치안 선, 샤앙 리, 모하메드 엘호세니 등이 있다. 2023a. 미니톤-4: 첨단 대형 언어 모델을 사용한 비전-언어 이해 향상. __학력-4: 첨단 대형 언어 모델을 사용한 비전-언어 이해 arXiv 프리프린트 arXiv:2304.10592_.\n' +
      '\n' +
      '월롱쩌, 잭 헤셀, 아나스 아와달라, 사미 요츠학 가드레, 제시 도그, 알렉스 포, 영재 유, 루드비히 슈미트, 윌리엄 양왕, 예진 최 등이 있다. 2023b. 멀티모달 c4: 텍스트로 인터리빙된 이미지의 80억 규모의 코퍼스를 공개한다. arXiv 프리프린트 arXiv:2304.06939_입니다.\n' +
      '* 주 등은 (2016) 유케 주, 오리버 그로스, 마이클 베르나인, 리피-피. 2016. Visual7w: 영상에서 답하는 그라운드 질의. 컴퓨터 비전 및 패턴 인식_페이지에 대한 IEEE 컨퍼런스의 _수익에서, 페이지 4995-5004.\n' +
      '\n' +
      '확인 A.\n' +
      '\n' +
      'LLM이 출현하기 전에 전통적인 MM PT에 대한 여러 조사(루안 및 진, 2022, 두 등은 2022, 롱 et al, 2022, Chen et al., 2023a)가 수행되었다. 이러한 모델의 대부분은 PT 단계 동안 상당한 계산 비용을 수반하며, 이는 대규모 모델과 데이터 세트를 사용한 엔드 투 엔드 트레이닝에 기인한다. LLM을 통합하지 않은 결과로 이러한 모델은 ICL, CoT 및 상호 작용 능력에 따라 명령어의 결핍으로 고통받는다. 더욱이, 트레이닝 파이프라인은 IT 단계의 포함 없이 PT 단계만을 포함한다.\n' +
      '\n' +
      '최근 MM-LLM에 대한 여러 조사가 나타났다. 예인 등. 우 등은 초기 VL 이해 모델에 독점적으로 설명되어 있다. 황 등은 시각적 IT에 일차적으로 중점을 두는 반면 송 등은 모달 정렬 방법에 초점을 맞추고 있다. 마지막으로, 코이 등은 자율 주행 영역 내에서 MM-LLM의 적용에 대한 포괄적인 검토를 제공한다.\n' +
      '\n' +
      '이들의 작품과 비교하여 주요 차이를 살펴보면 다음과 같다.\n' +
      '\n' +
      '* 우리는 모델 이해뿐만 아니라 생성 모델까지 포함하여 지난 1년 동안 거의 모든 MM-LLM을 포괄적으로 다루었다. 당사의 적용 범위는 VL 방식을 넘어 오디오 및 3D와 같은 다양한 모드를 포괄합니다.\n' +
      '*는 독자들에게 MM-LLM에 대한 포괄적인 이해를 제공하기 위해 모든 양식의 변형을 통합하는 일반적인 모델 아키텍처를 도입하여 각 구성 요소에 대한 기능적 역할과 실행 선택에 대한 자세한 개요를 제공한다.\n' +
      '* 우리는 기존 MM-LLM의 발달 동향을 요약하고 효과를 높일 수 있는 훈련 레시피를 제공했습니다.\n' +
      '* 우리는 MM-LLM 연구자를 위한 오픈 소스 웹사이트를 구축하고 인파가 많은 업데이트를 지원하고 MM-LLM 분야의 협업을 촉진하기 위한 것을 목표로 했다. 우리는 이 조사가 MM-LLM 영역에서 향후 연구를 조명할 것으로 기대한다.\n' +
      '\n' +
      '부록 B 주스트림 PEF\n' +
      '\n' +
      'PEFT는 미리 훈련된 LLM을 냉동 상태로 유지하는 동시에 소수의 광고 이중 훈련 가능한 매개변수를 조정한다. 다음 섹션에서는 \\(\\mathbf{x}\\) 및 \\(\\mathbf{h}\\)가 원본 모듈의 입력 및 출력을 나타내는 몇 가지 대표적인 PEFT 방법을 재방문하고, \\(\\mathbf{h}^{\\ime}\\)는 PEFT와 부착할 때 이 모듈의 출력을 나타낸다.\n' +
      '\n' +
      '프리픽스-튜닝 리 및 리앙(2021); 레스터 등(2021)은 관심 모듈의 키 및 값에 학습 가능한 토큰을 추가하는 것을 포함한다. 이 과정은 다음과 같이 제조되었습니다.\n' +
      '\n' +
      'f{q}\\mathbf{x}\\mathbf{k}\\mathbf{f}}\\mathbf{v}\\mathbf{f}.\n' +
      '\n' +
      'I\\(\\mathbf{P}_{k},\\mathbf{P}_{v}_{v}\\in\\mathbb{R}^{l\\i d}\\)와 함께 두 세트의 프리픽스 토큰을 나타낸다. ([\\cdot,\\cdot]\\)은 연결된 것을 나타내고, \\(\\operator:{Attn}\\)는 연결된 것으로 정의된다.\n' +
      '\n' +
      '\\[\\ 수술자명{Attn}\\mathbf{Q},\\mathbf{K},\\mathbf{K},\\mathbf{V}\\right):= \\ 운영자명{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{Q}\\mathbf{K}\\mathbf{K}\\mathbf{K}\\mathbf{K}\\mathbf{K}\\math{f{K}\\math{f{V}\\math{f{V}\\math{f{V}\\math{f{V}\\math{f{f{V}\\math{f{V}\\math{f{K}\\mathbf{K}\\left,\\mathbf{V}\\left,\\mathbf{V}\\left)(\\mathbf{V}\\ather)}\\left(\\mathbf{V}\\left:\\left\n' +
      '\n' +
      'Hebuffi et al.(2017); He et al.(2021); Rebuffi et al.(2020)는 보통 하향 주입 매트릭스 \\(\\mathbf{A}\\), 비선형 활성화 함수 \\(\\sigma(\\cdot)\\ 및 상향 주입 매트릭스 \\(\\mathbf{B}\\)로 구성된 잔류 블록이다. 사전 훈련된 LLM의 어떤 층에도 삽입할 수 있으며, 다음과 같이 공식화된다.\n' +
      '\n' +
      '\\[\\mathbf{h}^{\\prime}=\\mathbf{h}+\\sigma(\\mathbf{x}\\mathbf{A})\\mathbf{B}. \\tag{7}\\]\n' +
      '\n' +
      'LoRAHu et al.(2021)은 가장 일반적으로 사용되는 PEFT 방법이다. 이는 파라미터의 변화가 낮은 순위 공간 내에서 발생한다고 가정한다. LoRA는 두 개의 저 순위 행렬 \\(\\mathbf{A}\\mathbf{A}\\mathbf{W}\\) 사이의 매트릭스 곱셈(\\mathbf{A}\\mathb{R}\\in\\mathb{R}\\)을 증분적으로 학습하며(\\mathbf{A}\\mathbf{A}\\mathbf{R}\\mathbf{A}\\bf{R}\\bf{R}\\bf{R}\\bf{R}\\bf{R}\\bf{R}\\bf{R}\\bf{R}\\mathbf{R}\\bf{R}\\bf{R}\\bf{R}\\bf{R}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\ LoRA는 아래와 같이 전진 과정을 따른다.\n' +
      '\n' +
      '\\[\\mathbf{h}=\\mathbf{W}\\mathbf{x}+\\Delta\\mathbf{W}\\mathbf{x}=\\mathbf{W}\\mathbf{x}+\\mathbf{A}\\mathbf{B}\\mathbf{x}. \\tag{8}\\]\n' +
      '\n' +
      'QLoRADettmers 등(2023)은 양자화된 LoRA이다. QLoRA의 기본 원리는 미리 학습된 가중치를 \\(4\\) 비트로 양자화한 후 LoRA를 이용한 PEFT의 실행을 포함한다.\n' +
      '\n' +
      '앞서 언급한 PEFT 방법 외에도 AdaptBias Fu et al.(2022), CompacterKarimi Mahabadi et al.(2021), Ad캡터 전 Chen et al.(2022) 등 여러 가지가 있다.\n' +
      '\n' +
      '사용 가능한 LLM\n' +
      '\n' +
      '기존 MM-LLM 연구에서 일반적으로 사용되는 LLM 백본은 다음과 같다.\n' +
      '\n' +
      '** **Flan-T5***ale et al.(2022)는 모든 자연어 처리 문제에 대해 단일화된 텍스트-텍스트 훈련을 사용하여 인코더-디코더 아키텍처인 T5Raffel et al.(2020)에 대한 IT를 조사하여 강력한 제로샷 및 CoT 능력을 나타낸다.\n' +
      '* **ChatGLM2***는 자동 억제 마스크 주입 목적에 의해 최적화된 중국 영어 이중 언어 대화 모델이다. 이는 GLMDu 등(2022)을 기반으로 하며, 윤 등은 중국의 질문에 답하고 대화하는 데 최적화된 건축물(2022)을 기반으로 한다. 유도 2: [https://github.com/THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM/ChatGLM-6B)\n' +
      '* **UL2**Tay et al.(2022)는 수많은 벤치마크에서 T5를 능가하는 데오이소머 목표의 혼합물을 사용하여 훈련된 인코더-디코더 모델이다.\n' +
      '****Qwen**Bai et al.(2023)은 중국어와 영어에 일차적인 초점을 두고 대규모 및 다양한 데이터 세트에 대해 훈련된다. 그것은 정렬을 위해 SFT 및 RLHF 기술을 사용하여 Qwen-Chat와 같은 대화 모델을 생성한다.\n' +
      '***chchilla**Hoffmann 등(2022)은 광범위한 텍스트 데이터에 대해 훈련된 인과적 디코더이다. 학습 토큰의 두 배마다 모델 크기가 두 배가 되어야 한다는 것이다.\n' +
      '* **OPT**Zhang et al.(2022)는 GPT-3Brown et al.(2020) 클론으로 GPT-3의 성능을 복제하는 오픈 소스 모델을 방출하기 위해 노력하고 있다.\n' +
      '** **PaLM**Chowdhery et al.(2023)는 병렬 주의력 및 피드포워드 레이어가 있는 인과 디코더 구조로 최대 \\(15\\)의 훈련 속도를 더 빠르게 할 수 있다. 보이지 않는 변경에는 RoPE 임베딩, SwiGLU 활성화, 멀티 쿼리 관심 등이 포함되어 있다.\n' +
      '* **LLaMA**Touvron et al.(2023)는 효율적인 인과적 주의를 갖는 디코더 전용 모델을 포함한다.\n' +
      '*****LLaMA-2*Touvron et al.(2023)는 대화 생성을 위한 우수하고 안전한 LLaMA-2-Chat 모델을 미세 조정하는데 초점을 맞추고 있으며, 그룹화된 사각형 관심과 더 큰 맥락 길이를 가진 40% 더 많은 훈련 데이터를 통합했다.\n' +
      '\n' +
      '* **Vicuna****[14]는 ShareGPT.com에서 획득되고 SFT에 의해 훈련된 사용자 대화 데이터를 사용하여 LLaMA 위에 구축된 모델이다.\n' +
      '\n' +
      '## 부록 D SOTA MM-LLM (연속)\n' +
      '\n' +
      '**(20) LLaVA-1.5**[15]는 간단한 응답 포맷 프롬프트와 함께 MLP 프로젝션을 적용하고 학업에 맞춘 VQA 데이터를 도입하는 등 LLaVA 프레임워크에 간단한 수정을 보고한다. 이러한 조정은 MM 이해를 위한 향상된 능력을 초래한다.\n' +
      '\n' +
      '***(21) 미니GPT-v2**[13]는 다양한 VL 멀티 태스크 학습을 위한 통일된 인터페이스로 설계된 MM-LLM이다. 다중 VL 태스크 처리에 능숙한 단일 모델을 생성하기 위해 트레이닝 및 추론 모두에서 각 태스크에 대해 식별자가 통합된다. 이는 명확한 과제 구분을 용이하게 하여 궁극적으로 학습 효율성을 제고한다.\n' +
      '\n' +
      '***(22) CogVLM**[12]는 관심층과 피드포워드 층 내에서 훈련 가능한 시각적 전문가 모듈을 통해 양식의 격차를 연결하는 오픈 소스 MM-LLM이다. 이것은 NLP 다운스트림 작업에 대한 성능을 손상시키지 않으면서 MM 특징의 깊은 융합을 허용한다.\n' +
      '\n' +
      '**(23) DRESS**[13]는 인간의 선호와의 정렬을 향상시키기 위해 자연어 피드백을 사용하는 방법을 소개한다. DRESS는 조건부 강화 학습 알고리즘을 확장하여 미분할 수 없는 자연어 피드백을 통합하고, 모델을 학습하여 피드백을 기반으로 적절한 응답을 생성한다.\n' +
      '\n' +
      '**(24) X-구조BLIP***[15]는 이미지/비디오, 오디오 및 3D를 포함한 여러 양식에 걸쳐 다양한 작업을 처리할 수 있을 만큼 충분히 확장 가능한 명령어 인식 표현과 교차 모달 프레임워크를 소개한다. 특히, 그것은 모달리티 특이적 PT의 필요 없이 이를 달성한다.\n' +
      '\n' +
      '**(25) CoDi-2**[15]는 멀티턴 대화에 따른 모달리티 인터리빙된 명령어, 인컨텍스트 생성 및 사용자 모델 상호 작용에 탁월한 MM 생성 모델이다. 그것은 복잡한 모달리티 인터리빙된 입력들 및 명령어들을 처리하기 위해 CoDi[15]를 향상시키며, 이는 자동적으로 잠재된 특징을 생성한다.\n' +
      '\n' +
      '***(26) VILA**[15]는 비전 과제에 있어 성능을 능가하고 텍스트 전용 능력을 유지하면서 놀라운 추론 능력을 보인다. LLM 학습의 완전한 능력을 활용하고, 이미지 텍스트 쌍의 인터리빙된 속성을 사용하고, 세심한 텍스트 데이터를 재블렌딩하여 이를 달성한다.\n' +
      '\n' +
      '도표 E VL 벤치마크.\n' +
      '\n' +
      '표 2에 제시된 **V****************************************************-M.\n' +
      '\n' +
      '다타세트 훈련.\n' +
      '\n' +
      'MM PT 및 MM IT 데이터 세트에 대한 통계는 각각 표 3 및 표 4에 나와 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Dataset Name** & **X Modality** & **\\#.X** & **\\#.T** & **\\#.X-T** \\\\ \\hline ALIGN (Jia et al., 2021) & Image & 1.8B & 1.8B & 1.8B \\\\ LTIP (Alayrac et al., 2022) & Image & 312M & 312M & 312M \\\\ MS-COCO (Lin et al., 2014) & Image & 124K & 620K & 620K \\\\ Visual Genome (Krishna et al., 2017) & Image & 108K & 4.5M & 4.5M \\\\ CC3M (Sharma et al., 2018) & Image & 3.3M & 3.3M & 3.3M \\\\ CC12M (Changpinyo et al., 2021) & Image & 12.4M & 12.4M \\\\ SBU (Ordonez et al., 2011) & Image & 1M & 1M \\\\ LAION-5B (Schuhmann et al., 2022) & Image & 5.9B & 5.9B \\\\ LAION-400M (Schuhmann et al., 2021) & Image & 400M & 400M \\\\ LAION-en (Schuhmann et al., 2022) & Image & 2.3B & 2.3B \\\\ LAION-zh (Schuhmann et al., 2022) & Image & 142M & 142M \\\\ LAION-COCO (Schuhmann et al., 2022b) & Image & 600M & 600M \\\\ Flickr30k (Young et al., 2014) & Image & 31K & 158K & 158K \\\\ AI Challenger Captions (Wu et al., 2017) & Image & 300K & 1.5M & 1.5M \\\\ COYO (Byeon et al., 2022) & Image & 747M & 747M & 747M \\\\ Wukong (Gu et al., 2022) & Image & 101M & 101M & 101M \\\\ COCO Caption (Chen et al., 2015) & Image & 164K & 1M & 1M \\\\ WebLI (Chen et al., 2022b) & Image & 10B & 12B & 12B \\\\ Episodic WebLI (Chen et al., 2023g) & Image & 400M & 400M & 400M \\\\ CC595k (Liu et al., 2023e) & Image & 595K & 595K & 595K \\\\ RefCOCO (Kazemzadeh et al., 2014) & Image & 20K & 142K & 142K \\\\ RefCOCO+ (Yu et al., 2016) & Image & 20K & 142K & 142K \\\\ Visual-7W (Zhu et al., 2016) & Image & 47.3K & 328K & 328K \\\\ OCR-VQA (Mishra et al., 2019) & Image & 207K & 1M & 1M \\\\ ST-VQA (Biten et al., 2022) & Image & 23K & 32K & 32K \\\\ DocVQA (Mathew et al., 2021) & Image & 12K & 50K & 50K \\\\ TextVQA (Singh et al., 2019) & Image & 28.4K & 45.3K & 45.3K \\\\ DataComp (Gadre et al., 2023) & Image & 1.4B & 1.4B & 1.4B \\\\ GQA (Hudson and Manning, 2019) & Image & 113K & 22M & 22M \\\\ VGGQA (Krishna et al., 2017) & Image & 108K & 1.7M & 1.7M \\\\ VQA\\({}^{\\rm{\\color[rgb]{0,0,0}2}2}\\)(Goyal et al., 2017) & Image & 265K & 1.4M & 1.4M \\\\ DVQA (Kafle et al., 2018) & Image & 300K & 3.5M & 3.5M \\\\ OK-VQA (Schwenk et al., 2022) & Image & 14K & 14K & 14K \\\\ A-OKVQA (Schwenk et al., 2022) & Image & 23.7K & 24.9K & 24.9K \\\\ Text Captions (Sidorov et al., 2020) & Image & 28K & 145K & 145K \\\\ M3W (Interleaved) (Alayrac et al., 2022) & Image & 185M & 182GB & 43.3M (Instances) \\\\ MMC4 (Interleaved) (Zhu et al., 2023b) & Image & 571M & 43B & 101.2M (Instances) \\\\ MSRVTT (Xu et al., 2016) & Video & 10K & 200K & 200K \\\\ WebVid (Bain et al., 2021) & Video & 10M & 10M & 10M \\\\ VTP (Alayrac et al., 2022) & Video & 27M & 27M & 27M \\\\ AISHELL-2 (Chen et al., 2023b) & Audio & – & – & 128K \\\\ AISHELL-2 (Chen et al., 2023b) & Audio & – & – & 1M \\\\ WaveCaps (Mei et al., 2023) & Audio & 403K & 403K & 403K \\\\ VSDial-CN (Chen et al., 2023b) & Image, Audio & 120K (Image), 1.2M(Audio) & 120K & 1.2M \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: MM PT 데이터셋에 대한 통계. ***#.X**는 X, **#.T**의 양을 나타내며 **#.T**는 Text의 양을 나타내며 **#.X-T**는 X가 이미지, 비디오 또는 오디오일 수 있는 X-Text 쌍의 양을 나타낸다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:22]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>