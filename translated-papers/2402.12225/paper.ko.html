<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 용량 및 확장성에서의 3차원 형상 생성을 위한 푸쉬 자동 회귀 모델\n' +
      '\n' +
      '슈엘린 치안\\({}^{*}\\), 유왕\\({}^{*}\\), 시미안 루오, Yinda Zhang, Ying Tai, Zhenyu Zhang, Chhengjie Wang Xiangyang Xue, Bo Zhao, Tiejun Huang, Yunsheng Wu, Yanwei Fu\n' +
      '\n' +
      '({}^{*}\\)는 동일한 기여도를 나타낸다. Xuelin Qian, Yu Wang 및 Yanwei Fu는 데이터 과학 학교, Fudan ISTBI--ZJNU 알고리즘 Centre for Brain-Intired Intelligence, Fudan University, China. Email: {xqian, yu,w13, yu, Weyli@fudan.edu.cn, Xuelin Qian and Yi Wang is equal contributions.Simian Luo is with the Artificial Intelligence in IIIS(Institute for Interdisciplinary Information Sciences), Ysinghua University. 이메일: hqanz2@mails.tsinghua.edu.cn.Yinda Zhang은 구글과 함께 있다. Email: Shangyinda@gmail.com. Ting Tai and Zhenyu Zhang은 중국 난징 대학교(쑤저우 캠퍼스)의 지능 과학 기술 학교와 함께 있습니다. Email: yingjian@nih.edu.cn, zhangjieie@fudan.com.Chengjie Wang and Yhusheng Wu is with Tencent Youtu llab, Shenzhen, China. Email: {jasoncywang, sinomxu}@tencent.com.Bo Zhao and Tiejun Huang is with BAAT(Beijing Academy of Artificial Intelligence), Email: zhqhoo@baat.ac.cn, dijhuang@pku.edu.cn.Xiangyang Xue with the Computer Science, and Shanghai Key Lab of Intelligent Information Processing, Fudan University. 이메일: yxuz@fudan.edu.cn.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '자기회귀모델은 격자공간에서 관절분포를 모델링하여 2차원 영상 생성에서 인상적인 결과를 얻었다. 본 논문에서는 자동 회귀 모델을 3D 도메인으로 확장하고, 용량과 확장성에서 동시에 자동 회귀 모델을 개선함으로써 보다 강력한 3D 형상 생성 능력을 추구한다. 첫째, 대규모 모델의 학습을 용이하게 하기 위해 공개적으로 사용 가능한 3D 데이터 세트의 앙상블을 활용한다. 그것은 메쉬, 포인트, 복셀, 렌더링된 이미지 및 텍스트 캡션의 여러 속성을 가진 대략 \\(900,000\\)개의 객체의 포괄적인 집합으로 구성된다. _Objavorse-Mix_라고 하는 이 다양한 _labeled_ 데이터 세트는 우리 모델이 광범위한 객체 변형으로부터 학습할 수 있게 한다. 데이터 처리를 위해 프로세스 복잡성으로 인해 거의 100TB의 스토리지를 활용하여 4주에 걸쳐 64코어 CPU와 8개의 A100 GPU가 있는 4대의 기계를 사용한다. 그리고 데이터 세트는 [https://huggingface.co/datasets/BAAI/Objavorse-MLK](https://huggingface.co/datasets/BAAI/Objavorse-MLK)에 있다. 그러나 3차원 자동 회귀를 직접 적용하는 것은 체적 격자에 대한 높은 계산 요구와 격자 치수에 따른 모호한 자동 회귀 순서의 중요한 문제에 직면하여 3차원 형상의 품질이 떨어진다. 이를 위해 우리는 용량 측면에서 새로운 프레임워크 _Argus3D_를 제시한다. 구체적으로, 본 연구에서는 체적 격자 대신 잠재 벡터에 기반한 이산 표현 학습을 도입함으로써 계산 비용을 줄일 뿐만 아니라 관절 분포를 보다 다루기 쉬운 순서로 학습함으로써 필수적인 기하학적 세부 사항을 보존한다. 따라서 조건부 생성의 용량은 포인트 클라우드, 카테고리, 이미지 및 텍스트와 같은 잠재 벡터에 다양한 컨디셔닝 입력을 단순히 연결함으로써 실현될 수 있다. 또한, 모델 아키텍처의 단순성 덕분에, 우리는 자연스럽게 인상적인 3.6억 매개 변수를 가진 더 큰 모델에 대한 접근 방식을 확장함으로써 다용도 3D 생성의 품질을 더욱 향상시킨다. 4세대 작업에 대한 광범위한 실험은 Argus3D가 여러 범주에 걸쳐 다양하고 충실한 모양을 합성하여 놀라운 성능을 달성할 수 있음을 보여준다. 데이터세트, 코드 및 모델은 프로젝트 웹사이트 [https://argus-3d.github.io](https://argus-3d.github.io)에서 사용할 수 있다.\n' +
      '\n' +
      '3차원 형상 생성, 자기회귀 모델, 다중모달 조건 생성, 이산 표현 학습.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '3D 형상 생성은 로봇 공학[1], 자율 주행[2, 3], 증강 현실[4] 및 가상 현실[5]에서 광범위한 응용으로 인해 학계와 산업계 모두에 대한 관심이 증가하고 있다. 사용자 전제 조건이 제공되는지 여부에 따라 모양 생성은 일반적으로 무조건 또는 조건으로 분류됩니다. 생성 모델이 효과적이기 위해서는 합성된 모양이 인간이나 주어진 조건의 보편적인 인식에 _diverse_와 _fa faithful_가 모두 있어야 한다. 이러한 자질은 형상 완성, 단일 뷰 재구성 등과 같은 다른 결정론적 작업의 기초가 된다. 이전의 접근법 [6, 7, 8]은 전형적으로 형상 재구성을 통해 잠재 특징들을 학습하기 위해 AutoEncoder(AE)를 채용한다. 그런 다음, 생성적 적대 네트워크(GAN)는 이러한 잠재 특징의 분포에 맞게 훈련되어 AE에서 학습된 잠재 코드를 샘플링하여 3D 모양을 생성할 수 있다. 이러한 접근 방식은 설득력 있는 결과를 산출하지만 여전히 열악한 용량 및 확장성 문제로 어려움을 겪고 있다.\n' +
      '\n' +
      '최근 대규모 언어 모델(LLM) [9, 10, 11]의 진행은 더 많은 매개변수가 복잡한 작업을 처리하고 고급 성능을 달성할 수 있음을 보여준다. 따라서 자연스러운 질문이 발생한다: 우리는 다재다능한 3D 형상 생성을 위한 비교 가능한 큰 모델을 배울 수 있는가? 직관적으로, 우리는 그것이 (1) 상응하는 라벨들을 갖는 매우 큰 규모의 3D 데이터세트 및 (2) 거대한 파라미터들을 갖는 상당히 확장가능한 3D 모델을 요구한다고 생각한다. 불행히도, 고전적인 3D 데이터세트 ShapeNet[12]은 약 51K 모양의 55개의 카테고리만을 포함하고, 비교적 작은 데이터 cale와 카테고리 간의 불균형한 3D 인스턴스 분포를 포함한다. 따라서 대형 3D 모델에 의해 쉽게 오버핏될 수 있습니다. 새롭게 수집된 매우 큰 규모의 데이터 세트인 Obiayverse[13]에는 800K 3D 모양의 대규모 데이터가 포함되어 있다. 이러한 데이터 세트는 데이터에서 나오는 3D 지식이 약간 시끄럽고 3D 주석이 부족하지만, Obiayverse는 이 논문에서 개선된 데이터 세트에 매우 중요한 데이터 소스를 제공한다.\n' +
      '\n' +
      '이 논문에서 우리의 첫 번째 기여는 _Obiayverse-Mix_라고 하는 대규모 3D 데이터 세트를 구축하는 것이다. 구체적으로 ModelNet40[14], ShapeNet[12], Obiayverse[13]_etc_와 같이 공개적으로 사용 가능한 3D 데이터 세트의 앙상블을 활용한다. 이를 통해 데이터를 효율적으로 확장할 수 있을 뿐만 아니라 설정 비용이 거의 들지 않습니다. 기존의 많은 3D 데이터 세트가 존재함에도 불구하고, 우리는 한 가지 중요한 단점은 이러한 3D 데이터 세트에서 카테고리의 품질과 다양한 3D 인스턴스 분포의 차이이다. 이를 위해 3D 데이터 세트의 품질을 향상시키기 위한 전처리 전략을 체계적으로 제시한다. 특히, 모든 샘플이 균일한 데이터 표준을 갖도록 모든 모양을 정규화하고 재스케일링하는 것으로 시작한다. 그 후, 3D 모양을 통일된 메쉬 또는 복셀 표현으로 변환하고 점유와 같은 여러 개의 3D 레이블을 생성하기 위해 기성 도구를 채택한다. 또한, 우리는 수밀 메쉬가 없는 모양을 자르고 그림 1과 같이 잡음이 있는 샘플(예: 이상한 모양, 어수선한 배경 또는 여러 개)을 걸러낸다. 0(b). 비판적으로, 우리는 각각 64코어 CPU와 8개의 A100 GPU를 갖춘 4대의 기계를 활용하고, 4주에 걸쳐 실행되며, 프로세스 복잡성으로 인해 거의 100TB의 스토리지를 소비한다. 결과적으로, Objayverse-Mix는 메쉬, 포인트, 복셀, 렌더링된 이미지 및 캡션의 다양한 속성을 가진 거의 \\(900,000\\)의 객체들을 인상적으로 수집한다. 예들이 도 1에 도시되어 있다. 0(c). 이 다양한 데이터 세트는 모델이 광범위한 객체 변형으로부터 학습할 수 있게 하여 생성된 3D 모양의 시각적 품질을 촉진한다.\n' +
      '\n' +
      '이러한 데이터셋을 이용하여 최근 하드웨어 로또 구조인 _i.e._, AR(Auto-Regressive) 모델을 이용하여 3D 모델을 추구한다. AR 모델은 2D 이미지[15, 16, 17] 및 3D 형상[18, 19] 생성에서 현저한 성능을 보였다. 연속적인 잠재 공간을 학습하는 대신, 이 모델들은 이산 표현 학습을 활용하여 각각의 2D/3D 입력을 그리드 기반 이산 코드로 인코딩한다. 그 후, 변압기 기반 네트워크를 사용하여 모든 코드의 분포를 공동으로 모델링하며, 이는 본질적으로 객체 이전의 기본을 반영하여 고품질 생성 및 다루기 쉬운 훈련을 용이하게 한다. 그러나 AR 모델을 3D에 직접 적용하는 것은 여전히 두 가지 한계를 겪고 있다. 첫째, 이산코드의 수가 기하급수적으로 증가함에 따라(제곱에서 세제곱으로) 변압기의 계산 부담이 급격히 증가하여 수렴이 어렵다. 둘째, 그리드 공간의 이산 코드는 고도로 결합된다. 단순하게 하향식 행-주 순서로 자동 회귀를 위해 그것들을 평평하게 하는 것은 모호하다. 이것은 불량한 품질 또는 심지어 생성된 형상의 붕괴로 이어질 수 있다(더 많은 논의를 위해 Sec. 4.1 참조).\n' +
      '\n' +
      '따라서 본 연구의 두 번째 기여는 3차원 형상 생성의 효율적인 학습을 향상시키기 위해 _capacity_ 측면에서 개선된 자동 회귀 모델을 제안하는 것이다. 우리의 핵심 아이디어는 3D 체적 공간과 대조적으로 1차원 공간에서 이산 표현 학습을 적용하는 것이다. 구체적으로, 먼저 3차원 형상으로부터 인코딩된 체적 그리드를 3축 정렬 직교 평면에 투영한다. 이 프로세스는 입력 지오메트리에 대한 필수 정보를 유지하면서 계산 비용을 세제곱에서 제곱으로 크게 줄입니다. 다음으로, 이산 표현 학습이 수행되는 컴팩트하고 다루기 쉬운 잠재 공간으로 세 개의 평면을 더 인코딩하기 위한 결합 네트워크를 제시한다. 우리의 디자인은 간단하고 효과적이며 두 가지 예측을 통해 앞서 언급한 한계를 간단히 해결한다. 따라서, 잠재공간으로부터 코드들의 결합 분포를 모델링하기 위해 바닐라 디코더 전용 변압기가 부착될 수 있다. 우리는 또한 포인트 클라우드, 카테고리, 이미지 및 텍스트와 같은 잠재 코드에 다양한 컨디셔닝 입력을 연결함으로써 무조건 생성과 조건부 생성 사이에서 자유롭게 전환할 수 있다. 그림 2는 상단 왼쪽 모서리에 주어진 조건이 있거나 없는 여러 범주에 걸쳐 다양하고 정확한 모양을 생성하기 위한 개선된 모델의 용량을 보여준다. 비판적으로, GPT-3[20]의 성공에 영감을 받아, 우리는 자동 회귀 아키텍처를 더욱 강화하기 위해 변압기를 자연스럽게 확장합니다. 레이어 수와 피쳐 치수를 증가시켜 구현합니다. 결과적으로, 우리의 노력은 현저한 3.6B 매개변수를 갖는 새로운 3D 모델을 확립하고, 이는 Sec. 5.7에서 검증된 바와 같이 다목적 3D 생성의 품질을 더욱 향상시킨다.\n' +
      '\n' +
      '전반적으로, 우리는 용량과 규모 모두에서 3D 형상 생성을 위한 자동 회귀 모델을 추진한다. 이를 위해 본 논문에서는 자동 회귀 모델을 3D 생성 분야에 적용하는 데 앞서 언급한 한계를 제거할 뿐만 아니라, 다중 모드 조건에서 다양한 3D 형상 생성을 지원하는 전례 없는 규모(3.6\\)의 매개변수로 모델을 확장하는 새로운 프레임워크인 _Argus3D_를 제안한다.\n' +
      '\n' +
      '** 기여도.** 주요 기여도를 요약하면 다음과 같다. (1) 3D 형상 생성을 위해 _Argus3D_라고 하는 새로운 자동 회귀 프레임워크를 제안한다. 우리의 Argus3D는 포인트 클라우드, 카테고리, 이미지 및 텍스트를 포함한 다양한 컨디셔닝 입력에 대해 무조건 생성과 조건부 생성 사이의 전환의 이점을 즐긴다. (2) 볼륨 그리드나 트리플 평면 대신 잠재 벡터에 대한 결합 분포를 구축하는 개선된 이산 표현 학습을 소개한다. 3차원 격자에 대한 높은 계산 요구와 자동 회귀에 대한 모호한 순서의 두 가지 한계를 해결하여 용량 측면에서 3차원 형상 생성을 위한 자동 회귀 모델을 개발한다. (3) 모델 파라미터와 데이터셋의 크기를 확장함으로써 확장성 측면에서 Argus3D의 생성 능력을 더욱 촉진한다. 우리가 아는 한, Argus3D는 가장 큰 3D 장군이다.\n' +
      '\n' +
      '도. 1: (a) 5개의 공개 3D 형상 데이터 세트를 결합하여 총 약 900,000개의 다양한 형상을 축적했다. (b) 불규칙 형상, 복잡한 장면, 비-수밀 메쉬 및 이산 형상들과 같은 일부 잡음 형상들을 수동으로 필터링한다. (c) 우리의 Objayverse-Mix 데이터셋은 메쉬, 포인트 클라우드, 점유율, 렌더링된 이미지 및 텍스트 캡션을 포함하여 멀티모달 속성을 보여준다.\n' +
      '\n' +
      ' 3.6억 개의 매개변수를 포함하고 있으며, 다양한 범주에서 약 90만 개의 3차원 형상에 대해 학습된다. 수집한 데이터 세트인 _Obiayerse-Mix_는 공개적으로 사용 가능한 일련의 3D 데이터 세트를 조립하고 전처리 후 여러 속성을 포함하여 더 많은 연구를 장려하고 3D 모양 생성 분야를 발전시키기를 바란다. (4) 우리의 Argus3D가 보다 충실하고 다양한 형상을 생성할 수 있음을 입증하기 위해 네 가지 작업에 대해 광범위한 실험을 수행하여 무조건 및 조건부 형상 생성에 대한 놀라운 성능을 달성했다.\n' +
      '\n' +
      '**확장.** 이 작품의 예비 버전이 ImAM[21]으로 출판되었다. 이 논문에서 우리는 우리의 Argus3D가 몇 가지 중요한 개선을 했다는 것을 강조한다. 보다 구체적으로, (1) 규모 확장을 통해 자기회귀모형의 생성능력을 강화한다. 본 논문에서는 3.6억 개의 파라미터를 갖는 대규모 3차원 형상 생성 모델을 구축하기 위해 변압기 디코더 층 수와 특징 치수를 체계적으로 설계 및 증가시켰다. (2) 대규모 모델의 학습을 용이하게 하기 위해 데이터 세트의 규모를 확장하고, 공개적으로 사용 가능한 여러 개의 3D 저장소를 조립하여 구축된 대략 \\(900,000\\)개의 객체를 갖는 대규모 3D 형상 데이터 세트를 제공한다. 3D 생성 연구를 촉진하기 위해 메쉬, 포인트 클라우드, 복셀 및 렌더링 이미지와 같은 풍부한 속성을 가진 신뢰할 수 있는 각 3D 모양을 제공하기 위해 데이터를 필터링하고 전처리한다. (3) 제안된 방법을 Chamfer 거리와 Earth mover의 거리를 포함한 더 많은 거리 메트릭으로 평가함으로써 개선된 Argus3D의 3차원 형상 생성의 우수성을 더욱 입증한다. (4) 우리는 제안된 Argus3D의 효과를 입증하기 위해 광범위한 절제 연구를 수행하여 용량 및 규모에 대한 개선 사항에 대한 심층 분석을 제공한다. (5) 다양한 3D 형상 생성 작업에서 다양한 카테고리에 걸쳐 보다 다양하고 충실한 형상을 합성하여 새로운 최첨단 성능을 달성한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**3D 생성 모델** 매우 도전적인 작업으로서, 대부분의 이전 노력은 복셀, 포인트 클라우드 및 암시적 표현을 사용하여 이루어진다: (1) 표준 복셀 그리드는 학습 기반 3D 작업에 대한 3D 컨볼루션에 의해 쉽게 처리될 수 있다[22, 23, 24, 25]. 그러나 복셀 표현은 3차 공간 복잡도로 인해 높은 해상도로 확장할 수 없으며, 일반적으로 \\(64^{3}\\)으로 제한된다. 옥트리 또는 다중-스케일 표현[26, 27, 28, 29]과 같은 효율적인 데이터 구조를 갖더라도, 이러한 표현들은 여전히 품질에서 한계에 직면한다. (2) 형상 표면들로부터 추출된 포인트 클라우드들은 대안적인 3D 표현[30, 31, 32, 33]을 제공한다. 복셀 표현과 달리 메모리 효율이 높고 해상도에 제한을 받지 않습니다. 그러나, 점 구름은 3차원 공간에서 위상 관계를 나타낼 수 없으며, 점 구름으로부터 형상 표면을 복구하는 것은 간단하다. (3) 최근, 암시적 표현들은 3D 형상들 [8, 34, 35, 36]을 표현하는 것의 단순성으로 주목받았다. 그들은 주어진 점의 부호 거리[8, 36] 또는 점유 레이블[34, 37]을 예측함으로써 작동하여 마칭 큐브[38] 방법을 사용하여 쉽게 표면 복구를 할 수 있다. 후속 작업 [6, 8, 34, 37, 39]는 전역 또는 국부 형상 전위로 암시적 함수 표현을 설계하는 데 중점을 둔다. 요약하면, 다양한 3D 표현은 3DGAN[25], PC-GAN[40], IM-GAN[6], GBIF[7]을 포함한 주목할 만한 예를 들어 다양한 모양 생성 모델로 이어졌다. 그러나 현재 대부분의 생성 방법은 태스크에 따라 다릅니다.\n' +
      '\n' +
      '도. 2: 다용도 3D 형상 생성을 학습하기 위한 개선된 자동 회귀 모델을 제안한다. 우리의 접근법은 무조건적인 방식(왼쪽으로 한 열)을 통해 여러 범주를 가진 다양하고 충실한 모양을 생성할 수 있거나 왼쪽 상단(오른쪽으로 세 열)에 주어진 다양한 컨디셔닝 입력을 통합하여 조건부 생성에 적합할 수 있다.\n' +
      '\n' +
      '그리고 서로 다른 생성 작업(_e.g._, 형상 완성)에 직접 적용되기 어렵다. 본질적으로, 그들은 모드 붕괴 및 트레이닝 불안정성과 같은 알려진 단점들로 고통받는 생성 단계를 위해 GAN들에 의존한다. 이와는 대조적으로, 우리는 다른 멀티모달 조건들로 쉽게 일반화되면서 고품질의 다양한 형상들을 합성할 수 있는 개선된 3D 형상 생성을 위한 자동 회귀 모델을 제안한다.\n' +
      '\n' +
      '**3D 자동 회귀 모델.** 자동 회귀 모델은 다루기 쉬운 확률 밀도를 갖는 확률적 생성 접근법이다. 확률 체인 규칙을 사용하여, 특히 고차원 데이터에 대해 주어진 샘플의 가능성은 조건부 확률의 일련의 곱으로 인수분해될 수 있다. 대조적으로, GAN은 그러한 다루기 쉬운 확률 밀도를 갖지 않는다. 최근 AR 모델은 2D 이미지 생성[15, 41, 42]에서 3D 작업[43, 44, 45]의 현저한 발전을 달성한다. 대부분의 3D AR 모델은 더 많은 점이나 얼굴을 가진 표현 학습의 어려움으로 인해 고품질 모양을 생성하는 데 어려움을 겪고 있다. 특히, 우리는 3D 작업에 AR 모델을 활용하는 것과 유사한 통찰력을 공유하는 두 개의 최근 위대한 작품[18, 19]을 주목한다. Yan et al. [19]는 3D 공간에서 비-빈 그리드들만을 양자화하기 위해 희소 표현을 도입하지만, 여전히 단조로운 행-주요 순서를 따른다. Mittal et al. [18]은 순서를 깨기 위한 비순차적 설계를 제시하지만, 모든 체적 그리드에 대해 수행한다. 그러나, 이들 모두는 상기 제한들 중 하나만을 다루고, 변압기의 구조 설계 및 훈련에 부담을 준다. 대조적으로, 우리의 Argus3D는 체적 격자 대신에 잠재 벡터에서 이산 표현 학습을 적용한다. 이러한 표현은 이산 코드의 더 짧은 길이, 자동 회귀로부터의 다루기 쉬운 순서, 빠른 수렴, 또한 필수적인 3D 정보를 보존하는 등 많은 이점을 제공한다. 또한 변압기의 단순성으로 인해 다양한 조건을 연결하여 무조건 생성에서 조건부 생성으로 자유롭게 전환할 수 있다.\n' +
      '\n' +
      '**3D 확산 모델.** 최근에 디노이징 확산 확률 모델(DDPM: Denoising Diffusion Probabilistic Models) [46, 47, 48, 49]은 안정성과 다양한 영상 생성 능력으로 상당한 주목을 받고 있다. 포인트 클라우드 기반 방법 또는 2D 리프트 3D[50, 51, 52, 53, 54, 55, 56, 57, 58, 59]를 통한 3D 생성에서도 상당한 진전이 있었다. DDPM은 종종 U-Net[60] 아키텍처를 기반으로 점진적인 잡음 제거 프로세스를 통해 가우시안 분포로부터 데이터 분포를 학습한다. 그러나, 이 접근법은 해상도를 제한하고 트레이닝 사이클을 연장하여, 3D 생성에서의 성능을 저해한다. 대조적으로, 변압기 기반 모델은 높은 수준의 확장성을 제공하며, 이는 모델 크기[61, 62, 63, 64]를 확대함으로써 다양한 다운스트림 작업에 걸쳐 성능을 향상시키는 것으로 나타났다. 본 논문에서는 이러한 확장성이 3차원 형상 생성의 영역에서도 유리하여 복잡한 형상의 보다 효과적이고 효율적인 모델링이 가능함을 제시한다.\n' +
      '\n' +
      '##3 Objaverse-Mix: 조립 3D 형상들로 데이터 셋을 스케일링하는 단계\n' +
      '\n' +
      '우리는 광범위한 훈련 데이터에 의해 지원되는 큰 모델의 효과가 설득력 있게 입증된 2D 비전 영역에서 이루어진 상당한 진전에 영감을 얻는다[65, 66, 67]. 그러나 대규모 3D 데이터 세트를 구축하는 것은 사소한 일이 아니다. 한편, 오늘날 일반적으로 사용되는 데이터 세트의 경우 ShapeNet[12] 또는 ModelNet40[14]은 수만 개의 3D 형상만 가지고 있으며, 그 스케일은 매우 큰 모델을 훈련하기에 너무 작아서 과적합의 위험에 직면해 있다. 한편, 3D 객체를 라벨링하는 과정은 상당히 높은 비용과 복잡성을 수반하므로 대규모 3D 데이터 세트를 구성하는 것과 관련된 고유한 과제 중 하나이다. 최근 Deitke _et al._[13]은 온라인 3D 마켓플레이스에서 수집한 800K 이상의 3D 모델을 가진 대규모 데이터셋 Objaverse를 제시한다. 오바버스는 규모 면에서 3D 리포지토리를 개선하지만 여전히 매우 시끄럽고 3D 주석이 없어 모델이 데이터에서 3D 지식을 직접 학습하는 것을 방지한다. 최근 Objaverse 데이터 세트의 확장은 Objaverse-XL[68]에서 정점에 달했으며 현재 1,000만 개의 모양을 특징으로 한다. 데이터 스케일의 순수한 볼륨은 매우 인상적인 반면, 우리는 그것이 여전히 매우 상세한 3D 주석과 멀티모달 속성을 가지고 있지 않다는 것을 알게 된다. 또한 GitHub 및 Thingiverse와 같은 다른 사이트의 일부 소싱 데이터가 여전히 어느 정도 시끄럽다는 것을 알 수 있습니다.\n' +
      '\n' +
      '본 논문에서는 Argus3D 모델 학습을 위한 충분한 데이터를 제공하는 대규모 3D 데이터셋을 구축하기 위한 간단하고 다른 방법을 찾는다. 구체적으로, 우리는 공개적으로 사용할 수 있는 5개의 3D 리포지토리의 앙상블을 활용하여 약 900K 3D 모양의 인상적인 컬렉션을 만듭니다. 이를 통해 데이터를 효율적으로 확장할 수 있을 뿐만 아니라 설정 비용이 거의 들지 않습니다. 따라서 우리는 데이터의 많은 부분이 Objaverse 데이터 세트에서 상속되고 청소된다는 것을 인정하기 때문에 데이터 세트 _Objaverse-Mix_의 이름을 지정한다. 그림 1과 같이 아래 각 3D 저장소에 대한 세부 정보를 제공한다. 0(a).\n' +
      '\n' +
      '**ModelNet40**[14]: 이 데이터 세트는 40개의 객체 카테고리에 걸쳐 12,300개 이상의 CAD 모델의 인상적인 컬렉션을 포함하는 귀중한 리소스로서 서 있다. 연구 목적을 위해 이 데이터 세트에서 9,843개의 모델을 전략적으로 선택하여 모델을 훈련하는 기반이 된다.\n' +
      '\n' +
      '**ShapeNet**[12]: 그 포괄성과 범위를 인정받은 ShapeNet은 광범위하고 다양한 3D 모델 모음을 제공한다. 55개의 공통 객체 범주를 포함하고 51,300개 이상의 고유한 3D 모델을 자랑하는 이 데이터 세트는 우리 모델의 학습 프로세스를 크게 향상시키는 풍부하고 다양한 표현을 제공한다. ShapeNet의 선택은 광범위한 객체 변화를 포착하는 우리 모델의 능력을 강화한다.\n' +
      '\n' +
      '**Pix3D**[69]: 대규모 특성상, Pix3D는 우리의 연구에 귀중한 자원으로 등장합니다. 10,069개의 이미지와 395개의 모양을 포함하는 이 데이터 세트는 상당한 변화를 나타내어 모델이 다양한 범위의 객체 모양과 특성에서 학습할 수 있다. Pix3D를 학습 데이터 세트에 통합함으로써 시각적으로 인상적이고 사실적인 3D 모양을 생성할 수 있는 기능을 모델에 장착한다.\n' +
      '\n' +
      '**3D-Future**[70]: 특화된 가구 데이터셋으로서, 3D-Future는 16,563개의 별개의 3D 인스턴스의 고유하고 상세한 컬렉션을 제공한다. 이 데이터 세트를 훈련 파이프라인에 포함시킴으로써 모델이 가구 디자인의 복잡성과 뉘앙스에 대한 통찰력을 얻을 수 있도록 합니다. 이를 통해 우리 모델은 가구 도메인에서 고품질 및 사실적인 3D 모양을 생성할 수 있습니다.\n' +
      '\n' +
      '**Objaverse**[13]: 최근 3D 형상 데이터 세트의 영역에 추가된 Objaverse는 그 광대함과 풍부함으로 우리를 사로잡는다. 스케치팹에서 조달한 이 엄청나게 큰 데이터 세트는 800,000개 이상의 3D 모델로 구성되어 광범위한 객체 범주 및 변형을 제공합니다. 이 광범위한 데이터 세트는 고품질 3D 모양을 학습하고 생성하기 위한 다양하고 대표적인 다양한 예를 모델에 제공한다.\n' +
      '\n' +
      '또한, 3D 객체의 품질을 보장하기 위해 일련의 전처리 전략을 체계적으로 활용한다. 우리는 먼저 모든 도형을 균일한 기준으로 정규화하고 재스케일링하기 위한 이전의 노력[24, 71]을 따른다. 그 다음, 확립된 기법[72]을 사용하여 각 3D 객체를 렌더링하고, 깊이 맵을 생성하고, 와 삼차원 메쉬를 생성하고, 이들 메쉬로부터 샘플 포인트를 생성한다. 이 단계는 수밀 메쉬가 없는 모양을 효과적으로 다듬는 데 도움이 됩니다. 다음으로, 잡음 샘플(_e.g._, 이상하거나 초자연적인 객체) 또는 어려운 예제(_e.g._, 잔디가 있는 3D 장면)를 수동으로 필터링하여 모델 수렴에 해로운 것으로 경험적으로 밝혀졌다. 멀티모달 생성의 학습을 용이하게 하기 위해, 우리는 또한 멀티뷰 이미지를 렌더링하고 텍스트 설명을 생성하기 위해 기성 도구[73, 74]를 활용한다. 각 3D 객체에 대해 512x512 해상도로 12개의 뷰 이미지를 렌더링하기 위해 블렌더를 사용한다. 랜덤 뷰를 렌더링을 위해 사용하는 [75]의 접근 방식과 달리 이미지를 렌더링하기 위해 고정된 관점을 선택했다. 여기에는 정면, 측면, 상면, 후면 및 저면 뷰와 45\\({}^{\\circ}\\) 및 135\\({}^{\\circ}\\)의 극각에서 3개의 등거리 점이 포함된다. 이러한 렌더링 방법은 낮은 품질의 이미지를 피하고 특히 정면 뷰 렌더링과 일관성을 보장하는 데 도움이 된다. 정면도는 일반적으로 형상의 외관에 대한 가장 많은 정보를 포함하기 때문에 특히 유용하다. 또한, 고정된 관점, 특히 정면 뷰를 사용하여 더 높은 품질의 텍스트 캡션을 생성할 수 있다. 무작위 뷰는 때때로 각 뷰에 제시된 다양한 정보로 인해 일관성 없는 캡션을 초래할 수 있다. 각 3D 모양에 대한 텍스트 캡션을 생성하기 위해 전면 뷰에서 렌더링된 이미지를 사용하여 BLIP2를 사용한다. 이 접근법은 우리의 텍스트 캡션이 일관성이 있고 3D 도형의 내용을 정확하게 반영한다는 것을 보장한다. 결과적으로 그림 1과 같이 포괄적인 혼합 데이터세트 Obojarese-Mix를 구성한다. 1은 메쉬들, 포인트 클라우드들 및 복셀들의 다수의 표현들, 및 점유, 렌더링된 이미지들 및 캡션들의 다양한 라벨들을 갖는 900K 객체들을 포함한다. 데이터 조립 및 전처리를 용이하게 하기 위해 각각 64코어 CPU와 8개의 A100 GPU를 특징으로 하는 4대의 기계를 4주 이상 활용합니다. 이 과정은 복잡한 특성으로 인해 거의 100TB의 저장이 필요하다.\n' +
      '\n' +
      '##4 Argus3D: 3차원 형상 생성을 위한 개선된 자동 회귀 모델\n' +
      '\n' +
      '그림 4는 4.1절에서 필요한 기호 정의와 필요한 예비로 제안된 3차원 형상 생성 프레임워크의 개략도를 나타내며, 용량 관점에서 2단계 훈련 절차로 구성된다. 먼저 입력 정보를 Sec. 4.2와 같이 학습된 이산 코드의 구성으로 표현하고, 트랜스포머 모델을 사용하여 Sec. 4.3의 상호 관계를 학습한다. 확장성의 관점에서 섹션 4.4는 개선된 표현을 활용하여 트랜스포머 계층과 데이터를 모두 확장함으로써 더 강력한 생성 능력을 제공한다.\n' +
      '\n' +
      '### _Preliminary_\n' +
      '\n' +
      '**모호함** 우리는 우리의 임무에 존재하는 모호성의 문제를 강조한다. 형식적으로는 일련의 조건부 확률 순으로 _\'모호성\'이 나타나 우도 학습의 난이도에 영향을 미쳐 관절 분포의 근사 오차로 이어진다._ 비판적으로, 자동 회귀 모델은 순차적인 출력을 필요로 하며, 모든 이전 코드에 조건화된 다음 코드를 자동 회귀적으로 예측한다. 이에 의해, 평탄화된 시퀀스의 순서는 조건부 확률들의 순서를 결정한다. 위치 임베딩[76]과 같은 몇몇 방법은 코드의 위치를 인식할 수 있지만, 조건 순서에 의해 야기되는 근사 오차를 제거할 수 없다. 특히, 이러한 \'모호성\' 현상도 [15]에서 논의되고 그림 1에 의해 시각화된다. [15]의 47에서 손실 곡선은 그림 1과 같다. 도 47은 다양한 주문에 걸친 가능성 학습을 위한 난이도 차이를 강조한다. 그림 3은 평탄화 순서가 자동 회귀 생성 방식에 어떤 영향을 미치는지 보여준다. 그리드 기반 표현에서는 축에 따른 평탄화 순서가 \\(y\\)-\\(x\\)-\\(z\\), \\(x\\)-\\(z\\)-\\(y\\) 또는 다른 조합인 경우 모호하다.\n' +
      '\n' +
      '*Discrete Representation.** 입력 포인트 클라우드 \\(P\\in\\mathbb{R}^{n\\times 3}\\)이 주어졌을 때, \\(n\\)은 점의 수를 의미하며, 인코더는 각 포인트 클라우드에 대한 특징을 추출하고 복셀화를 수행하여 정규 볼륨 그리드 \\(f^{v}\\in\\mathbb{R}^{r\\times r\\times c}\\)의 특징을 얻으며, 여기서 \\(r\\)은 복셀의 해상도를 나타내며 \\(c\\)은 특징 차원이다. 각 3차원 형상에 대한 이산표현을 학습하기 위해 코드북(\\mathbf{q}\\in\\mathbb{R}^{m\\times c}\\)을 도입하였으며, 이 코드북의 엔트리는 그리드에서 특정 형태의 국부 부분 형상을 기술하는 학습된 코드이다. 형식적으로 각 격자\\(\\left\\{f^{v}_{(h,l,w)}\\right\\}_{h,l,w=1}^{r}\\), 벡터 양자화\\(\\mathcal{Q}\\left(\\cdot\\right)\\)를 코드북에서 가장 가까운 엔트리로 치환하여 [15],\n' +
      '\n' +
      '\\mathbf{z}^{v}=\\mathcal{Q}\\left(f^{v}\\right):=\\arg\\min_{\\mathbf{e}_{i}\\in\\mathbf{q}}||f^{v}_{(h,l,w}-\\mathbf{e}_{i}|| \\tag{1}\\arg\\min_{\\mathbf{e}_{i}\\tag{1}\\mathbf{e}_{i}\\tag{1}\\\n' +
      '\n' +
      '여기서 \\(\\mathbf{e}_{i}\\in\\mathbf{q}\\)는 코드북의 \\(i\\)번째 엔트리를 나타낸다. 따라서 두 번째 단계에서 항목 간의 상관 관계를 학습하면 모양 생성을 위한 기본 사전 정보를 탐색할 수 있다. 그럼에도 불구하고 자동 회귀 생성[15, 19]은 두 가지 한계에 직면하여 순차적 출력을 필요로 한다. _ 먼저, \\(\\mathbf{z}^{v}\\)의 분해능은 합성된 형상들의 품질에 영향을 미친다. 너무 작은 \\(r\\)은 복잡하고 상세한 기하학을 표현할 수 있는 능력이 부족할 수 있다. 큰 값 \\(r\\)은 각 로컬 그리드에 대한 특정 코드를 학습할 수 있다. 이것은 필연적으로 \\(r\\)이 증가함에 따라 필요한 코드의 수가 폭발하기 때문에 계산 복잡도를 증가시킬 것이다. _ 둘째, \\(\\mathbf{z}^{v}\\)의 순서는 생성품질에 영향을 미친다. 각 그리드는 이웃과 고도로 연결되어 있습니다. 예를 들어 x-y-z 축을 따라 단순히 평탄화하는 것은 모호성의 문제를 야기하여 차선의 생성 품질로 이어질 수 있다.\n' +
      '\n' +
      '###_개선된 이산 표현 학습_\n' +
      '\n' +
      '첫 번째 한계를 해결하기 위한 한 가지 가능한 해결책은 [77]에서 영감을 얻은 체적 그리드 대신 공간 그리드에서 벡터 양자화를 적용하는 것이다. 구체적으로, 점군 특징 \\(f^{p}\\in\\mathbb{R}^{n\\times c}\\)을 구한 후, 먼저 점들을 3축 정렬 직교 평면에 투영한다. 동일한 평면 격자에 속하는 점들의 특징들은 합산을 통해 집합되어 3개의 평면(\\{f^{xy},f^{yz},f^{xz}\\}in\\mathbb{R}^{l\\times w\\times c}\\)에 대한 3개의 특징맵이 생성된다. 다음으로, 벡터 양자화는 세 개의 평면에 개별적으로 적용된다. 삼면 표현의 주요 이점은 효율적이고 컴팩트하다는 것이다. 이는 3차원 정보를 보존하면서 격자 수를 \\(\\mathcal{O}(r^{3})\\)에서 \\(\\mathcal{O}(r^{2})\\)으로 획기적으로 줄인다. 그러나 여전히 고통스럽습니다.\n' +
      '\n' +
      '도. 3: 그리드 기반 표현을 위한 자동 회귀 생성의 일러스트레이션. 여기서는 세 가지 다른 평탄화 순서를 예로 들어 보여준다. 색상으로 보는 것이 가장 좋다.\n' +
      '\n' +
      '순서상의 모호함으로부터. 더 나쁜 것은, 세 평면의 평탄화 순서와 각 평면의 엔트리 순서를 포함한다.\n' +
      '\n' +
      '이를 위해 우리는 세 평면의 특징에 대한 더 높은 잠재 공간을 학습하여 사영을 추가로 소개한다. 이것은 먼저 3개의 평면을 임의의 순서로 연결한 다음 결합 네트워크에 공급함으로써 간단히 달성된다. 마지막으로, 출력은 투영된 잠재 벡터로서 평탄화되고, 다음과 같이 공식화된다.\n' +
      '\n' +
      '\\tau\\left(\\mathcal{G}\\left([f^{xy};\\f^{yz};\\f^{xz}]\\,;\\theta\\right)\\right)\\in\\mathbb{R}^{m\\times d}\\tag{2}\\\\times\n' +
      '\n' +
      '여기서 \\([\\cdot;\\cdot]\\)는 연접연산을 의미하고; \\(\\mathcal{G}\\left(\\,\\cdot;\\theta\\right)\\)는 매개변수 \\(\\theta\\)을 갖는 일련의 컨볼루션 층이고; \\(\\tau\\left(\\cdot\\right)\\)는 행-주차를 갖는 평탄화 연산이고; \\(m\\) 및 \\(d\\)은 잠재 벡터의 길이와 특징 차원을 나타낸다. 이산표현학습을 잠재벡터에 적용하여 \\(\\mathbf{z}=\\mathcal{Q}\\left(f\\right)\\), \\(\\mathcal{Q}\\left(\\cdot\\right)\\) 벡터양자화로 각 3차원 형상을 표현할 수 있다.\n' +
      '\n' +
      '**Remark.** 모호한 순서의 문제를 해결하기 위해 2단계 구조 설계 및 훈련 전략에 의존하는 기존 작업 [18, 19]와 달리 2차 투영의 도움을 받아 1단계 공간 격자의 결합 관계를 학습하여 해결한다. 컨볼루션 레이어를 쌓음으로써 잠재 벡터에서 각 요소의 수용 필드를 증가시킨다. 추가적으로, 3개의 평면들 상의 각각의 공간 그리드의 특징들이 융합되고 고도로 인코딩되기 때문에, 각각의 요소는 3D 공간에서 명확한 위치 매핑을 갖지 않으며, 이는 자동-회귀를 위한 다루기 쉬운 순서를 초래한다. 보다 심도 있는 논의는 5.8절에서 찾아볼 수 있다.\n' +
      '\n' +
      '**훈련 목표** 재구성 손실로 매개 변수를 최적화합니다 코드북에서 엔트리들의 인덱스를 나타내는 이산 표현 \\(\\mathbf{z}\\)을 구한 후, 인덱스로 해당하는 코드들을 \\(\\mathbf{q}_{(\\mathbf{z})}\\)으로 표현한다. 이어서, 부호기의 대칭 구조를 갖는 복호기를 설계하여 세 평면의 특징으로 다시 \\(\\mathbf{q}_{(\\mathbf{z})}\\)을 복호한다. 주어진 샘플링 포인트 \\(x\\in\\mathbb{R}^{3}\\)이 주어지면, 세 개의 특징 평면에 각각 투영하고 쌍선형 보간을 수행하여 특징을 질의한다. 3개의 평면의 특징을 누적하고 이들의 점유 값을 예측하기 위해 암시적 함수에 입력한다. 마지막으로 예측값 \\(y_{o}\\)과 지상진실 \\(\\tilde{y_{o}}\\) 사이의 이진 교차 엔트로피 손실을 적용하고,\n' +
      '\n' +
      '\\\\[\\mathcal{L}_{\\mathrm{occ}}=-\\left(\\tilde{y_{o}}\\cdot\\log\\left(y_{o}\\right)+\\left(1-\\tilde{y_{o}}\\right)\\cdot\\log\\left(1-y_{o}\\right) \\tag{3}\\t.).\n' +
      '\n' +
      '코드북을 더 훈련시키기 위해, 우리는 벡터 양자화 전후의 특징들 사이의 거리를 끌어당기는 것을 권장한다. 따라서, 코드북 손실은 다음과 같이 도출되고,\n' +
      '\n' +
      '[\\mathcal{L}_{code}=\\beta||\\mathrm{sg}\\left[f\\right]-\\mathbf{q}_{(\\mathbf{z}}}||_{2}^{2}+||f-\\mathrm{sg}\\left[\\mathbf{q}_{(\\mathbf{z}}}\\right]||_{2}^{2} \\tag{4}\\tag{2}}}\n' +
      '\n' +
      '여기서 \\(\\mathrm{sg}\\left[\\cdot\\right]\\)은 정지-경사 연산[78]을 나타낸다. 그리고 기본적으로 \\(\\beta=0.4\\)을 설정한다. 요약하면, 첫 번째 단계의 전체 손실은 \\(\\mathcal{L}_{rec}=\\mathcal{L}_{occ}+\\mathcal{L}_{code}\\)이다.\n' +
      '\n' +
      '###_바닐라 트랜스포머를 이용한 학습 프리퍼스_\n' +
      '\n' +
      '컴팩트한 구성과 다루기 쉬운 이산 표현의 순서로 인해 두 번째 단계의 모델은 이산 코드 간의 상관 관계를 흡수적으로 학습하여 형상 구성의 이전을 효과적으로 탐색할 수 있다. 따라서 특별히 설계된 모듈 없이 바닐라 디코더 전용 변압기[15]를 채택한다.\n' +
      '\n' +
      '무조건 생성과 잠재 벡터 \\(\\mathbf{z}=\\left\\{\\mathbf{z}_{1},\\mathbf{z}_{2},\\cdots,\\mathbf{z}_{m}\\right\\})의 이산화된 인덱스를 주어 이산 인덱스 1로 특징을 검색하기 위해 학습 가능한 임베딩 레이어에 공급한 후, 다중 헤드 자기 주의 메커니즘을 갖는 변환기는 이전 인덱스들의 분포 \\(p\\left(\\mathbf{z}_{i}\\mid\\mathbf{z}_{<i}\\right)\\)을 학습하여 다음 가능한 인덱스를 예측한다. 이것은 완전한 표현의 공동 분포를 다음과 같이 제공한다.\n' +
      '\n' +
      '각주 1: 단순화를 위해 임베딩 후 \\(\\mathbf{z}\\)의 기호를 재사용한다\n' +
      '\n' +
      '\\\\[p\\left(\\mathbf{z}\\right)=\\prod_{i=1}^{m}p\\left(\\mathbf{z}_{i}\\mid\\mathbf{z}_{<i}\\right) \\tag{5}\\]\n' +
      '\n' +
      '조건부 생성_에 대해, 사용자들은 종종 추가적인 조건들을 제공함으로써 생성 프로세스를 제어하기를 기대한다. 복잡한 모듈이나 훈련 전략을 설계하는 대신, 우리는 단순히 조건 \\(\\mathbf{c}\\)을 \\(\\mathbf{z}\\)으로 전치하여 관절 분포를 학습한다. 따라서 수학식 5는 다음과 같이 확장된다.\n' +
      '\n' +
      '\\[p\\left(\\mathbf{z}\\right)=\\prod_{i=1}^{m}p\\left(\\mathbf{z}_{i}\\mid\\mathbf{c}, \\\\mathbf{z}_{<i}\\right)\\tag{6}\\]\n' +
      '\n' +
      '여기서 \\(\\mathbf{c}\\)는 주어진 조건들의 특징 벡터를 나타낸다. 우리 모델의 단순성은 어떤 형태의 조건도 배울 수 있는 유연성을 제공한다. 특히 포인트 클라우드와 같은 3D 조건에 대해 Sec. 4.2에서 이산 표현 학습을 사용하여 벡터로 변환한다. 클래스, 이미지 등의 1D/2D 조건에 대해서는,\n' +
      '\n' +
      '도. 4: 우리의 Argus3D에 대한 개요. 임의의 3차원 형상이 주어졌을 때, 우리는 먼저 인코딩된 체적 그리드를 3축 정렬 평면에 투영한 다음 결합 네트워크를 사용하여 잠재 벡터로 추가로 투영한다. 따라서, 벡터 양자화는 이산 표현을 위해 그것에 대해 수행된다. 이러한 조밀한 표현과 다루기 쉬운 순서를 사용하여 바닐라 변압기는 형상 분포를 자동 억제적으로 학습하기 위해 채택된다. 또한 포인트 클라우드, 카테고리, 이미지 등 다양한 조건을 연결하여 무조건 생성에서 조건부 생성으로 자유롭게 전환할 수 있다.\n' +
      '\n' +
      '우리는 미리 훈련된 모델을 채택하거나 특징을 추출하기 위해 레이어를 임베딩한다.\n' +
      '\n' +
      '**객관적** 2단계를 훈련하기 위해 우리는 식에 대한 음의 로그 우도를 최소화한다. 5 또는 6은 \\(\\mathcal{L}_{nll}=\\mathbb{E}_{x\\sim p(x)}\\left[-\\log p\\left(\\mathbf{z}\\right)\\right]\\), 여기서 \\(p\\left(x\\right)\\)는 실제 데이터의 분포이다.\n' +
      '\n' +
      '** 추론** 2단계로 훈련된 두 모델 모두 Eq를 사용합니다. 5 또는 6은 \\(\\mathbf{z}\\)의 모든 요소가 완료될 때까지 Top-\\(k\\) 샘플링 전략으로 다음 인덱스를 점진적으로 샘플링하여 모양 생성을 수행한다. 그런 다음 첫 번째 단계의 디코더에 \\(\\mathbf{q}_{(\\mathbf{z})}\\)을 공급하고 샘플링된 모든 3D 위치(_e.g._, \\(128^{3}\\))에 대한 점유 값의 쿼리 확률을 제공한다. 출력 형상은 마칭 큐브로 추출된다[38].\n' +
      '\n' +
      '###_More Parameters for Better Capacity_\n' +
      '\n' +
      '최근, 대형 모델은 GPT[20, 62], PaLM[79, 80], BLIP[74, 81] 및 EVA[82, 83]와 같은 자연 언어 처리 및 컴퓨터 비전 모두에서 현저한 발전을 보여주었다. 그들은 복잡한 다중 작업을 처리할 수 있는 능력을 가질 뿐만 아니라, 더 중요한 것은 다운스트림 작업에 필수적인 사전 훈련 역할을 한다는 것이다. 이러한 성공적인 작업을 통해 모델의 규모를 확대하여 3차원 형상 생성 능력을 동시에 향상시킬 수 있을 것으로 기대한다.\n' +
      '\n' +
      '특히, 두 번째 단계에서 변압기 층의 수와 특징 치수를 증가시키지만, 단순화를 위해 첫 번째 단계의 모델 매개변수를 유지한다. 여기서 바닐라 디코더 전용 변압기를 사용하기 때문에 스케일업 개발은 적은 노력으로 더 효과적이다. GPT-3 [20]의 프로토콜에 따라, Tab에서 증명된 바와 같이, 서로 다른 스케일인 _i.e., Argus3D-B(ase), Argus3D-L(arge) 및 Argus3D-H(uge)를 갖는 3가지 모델을 설계한다. I. 구체적으로, 우리의 Argus3D-H 모델은 32개의 트랜스포머 디코더 레이어를 가지며, 36억 개 이상의 파라미터를 갖는다. 3D 형상 생성 분야에서 가장 큰 모델 중 하나입니다. Sec. 5.7의 실험 결과는 스케일-업 모델이 3D 형상의 생성 품질을 더욱 향상시킬 수 있음을 보여준다. 모델을 확장한 이 중요한 성과는 3D 형상 생성의 경계를 전례 없는 발전으로 밀어붙이는 중추적인 이정표를 나타낸다. 다음 단락에서 달리 명시되지 않는 한 Argus3D를 기본 모델에 참조한다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '이 섹션은 Argus3D의 용량을 평가하는 것으로 시작한다. 공정한 비교를 위해, 우리는 Argus3D-B의 강력하고 유연한 능력을 입증하는 5세대 작업에 대한 광범위한 연구를 보여준다. (Sec. 5.2 \\(\\sim\\) 5.6). 다음으로, 우리는 다른 척도(Sec. 5.7)에서 Argus3D의 효능을 비교하고 논의한다. 마지막으로, 우리는 모듈의 효율성을 평가하고 실제 데이터와 제로샷 생성(Sec. 5.8)에 대한 일반화를 보여주기 위한 심층 연구를 제공한다. 모든 실험에서 필요한 경우 포아송 디스크 샘플링으로 출력 메쉬에서 포인트 클라우드를 샘플링하거나 출력 포인트에서 자동 인코더로 메쉬를 재구성하여 포아송 표면 재구성[84]보다 우수하다. 자세한 구현 내용은 부록을 참조하십시오.\n' +
      '\n' +
      '### _Evaluation Metrics_\n' +
      '\n' +
      '**Common metrics.** 제너레이터로서, 제안된 방법을 평가하는 핵심은 _fidelity_를 측정하는 것뿐만 아니라 합성된 형상들의 _diversity_에 초점을 맞추는 것이다. 따라서 본 논문에서는 Coverage (COV) [40], Minimum Matching Distance (MMD) [40], Edge Count Difference (ECD) [7] 및 1-Nearest Neighbor Accuracy (1-NNA) [85], Total Mutual Difference (TMD) [86], Unidirectional Hausdorff Distance (UHD) [86], Frechet Point Cloud Distance (FPD) [87] 및 Accuracy (Acc.) [88]을 포함하여 다양한 생성 태스크에 대해 8개의 메트릭을 채택한다. 특히, 우리는 [6]에서 제안한 COV, MMD 및 ECD에 대한 주요 유사성 거리 메트릭으로 LFD(Light Field Descriptor) [89]를 사용한다. FPD와 Acc 둘 다요 메트릭은 계산을 위해 분류기가 필요하므로 ShapeNet 데이터셋에서 13개의 카테고리로 PointNet2를 훈련하여 92%의 분류 정확도를 달성한다.\n' +
      '\n' +
      '각주 2: [https://github.com/jtpls/TreeGAN/blob/master/evaluation/pointnet.py](https://github.com/jtpls/TreeGAN/blob/master/evaluation/pointnet.py)\n' +
      '\n' +
      '형상 생성 작업을 위해 COV와 MMD는 각각 생성된 형상의 다양성과 충실도를 측정한다. 둘 다 약간의 단점[85]을 겪는다. FPD와 Acc요 생성된 형상들의 충실도를 각각 특징 공간 및 확률의 관점에서 측정한다. 반면, ECD와 1-NNA는 다양성과 품질 측면에서 합성된 형상 집합과 지상-진실 형상 집합의 분포 유사성을 측정한다. 따라서 ECD와 1-NNA는 형상 생성 성능을 정량화하기 위한 두 가지 더 신뢰할 수 있고 중요한 메트릭이다. 형상 완성 작업을 위해 TMD는 부분 입력 형상에 대해 생성된 형상의 다양성을 의미하며, 완성 충실도를 평가하기 위해 UHD를 제안한다. 두 메트릭은 완료 작업을 위해 특별히 설계된다[86].\n' +
      '\n' +
      '임계값에 대한___Coverage 위에서 논의한 바와 같이, 커버리지[40]는 생성된 형상 세트의 다양성을 측정한다. 그러나, 가장 가까운 생성 형상까지의 거리가 크더라도 여전히 커버된 것으로 간주되기 때문에 이상치를 처벌하지 않는다[85]. 거짓 양성 커버리지를 배제하기 위해 임계값(CovT)을 도입하여 이들 사이의 LFD[89]가 임계값\\(t\\)보다 작은 경우에만 세대와 지상 진실의 일치로 계산한다. 실제로, \\(t\\)는 형상의 스케일과 복잡성에 따라 다른 의미 범주에 걸쳐 달라질 수 있으며, MMD[40]을 임계값으로 경험적으로 사용한다. 본 논문에서는 \\(t\\)을 모든 경쟁사의 평균 MMD로 설정하였다.\n' +
      '\n' +
      '정확한 일치를 식별하는 개선된 COV의 효과를 평가하기 위해 임계값 필터링을 사용하기 전과 후에 가장 큰 MMD를 가진 매칭된 쌍을 시각화한다. 그림의 왼쪽에 표시된 것처럼. 도 6을 참조하면, 생성된 형상의 품질을 제약하는 임계값이 없는 경우, 이상치(_e.g._, 지저분한 형상)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Size & Dimensions & Layers & Heads & Params (M) \\\\ \\hline Base & 768 & 12 & 12 & 100 \\\\ Large & 2048 & 24 & 16 & 1239 \\\\ Huge & 3072 & 32 & 24 & 3670 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: Three designs of our Argus3D with different scales.\n' +
      '\n' +
      '도. 5: 모델을 확장한 그림입니다.\n' +
      '\n' +
      '모든 가능한 근거-진실 형태와 일치할 수 있는데, 이는 분명히 불합리하다. 반대로 필터링을 위해 임계값을 적용하면 그림의 오른쪽에 표시된 것과 같다. 도 6을 참조하면, 생성된 형상은 최대 형상 거리를 가지더라도 텍스처 또는 매칭된 지면-진실 형상과 일부 유사성이 일정하다. 그것은 세대의 다양성을 측정하는 우리의 개선의 타당성과 신뢰성을 강력하게 보여준다.\n' +
      '\n' +
      '###_무조건 형상 생성\n' +
      '\n' +
      '**Data.** 우리는 ShapeNet [90]을 이전 문헌 [6, 7, 43]에 이어 생성을 위한 주요 데이터 세트로 간주한다. 우리는 공정한 비교 가능성을 위해 [7]에서 동일한 훈련 분할 및 평가 설정을 사용한다. 자동차, 의자, 비행기, 소총, 테이블의 다섯 가지 범주가 시험에 사용된다. 지상진리로서 [27]에서 \\(256^{3}\\) 해상도의 복셀화된 모델로부터 메쉬를 추출한다.\n' +
      '\n' +
      '**기준.** 우리는 Argus3D를 GAN 기반 IM-GAN[6] 및 GBIF[7], 플로우 기반 PointFlow[85], 스코어 기반 ShapeGF[91] 및 확산 기반 PVD[54]를 포함한 5개의 최신 모델과 비교한다. 우리는 공식 구현과 동일한 데이터 분할에 대해 이러한 방법을 훈련한다.\n' +
      '\n' +
      '**Metrics and Settings.** 생성된 세트의 크기는 테스트 세트의 크기의 5배로 [6, 7]과 동일하다. CovT(Coverage with threshold), COV(Coverage) [40], MMD(Minimum Matching Distance) [40] 및 ECD(Edge Count Difference) [7]을 이용하여 합성된 형상들의 다양성, 충실도 및 전체적인 품질을 평가하였다. 또한 분포 유사도를 측정하기 위해 1-최근접 이웃 정확도(1-NNA) [85] (챔퍼 거리 포함)를 사용한다. 샘플링된 점의 수는 2048입니다.\n' +
      '\n' +
      '** 결과 분석.** 결과는 탭에 보고됩니다. II. 첫째, Argus3D는 ECD 및 1-NNA와 관련하여 최첨단 성능을 달성한다. 그것은 고품질 모양을 합성하는 것보다 우리 모델의 우수성을 크게 보여준다. 우리는 1-NNA 기준에서 차의 결과가 좋지 않다는 것을 알아챘다. 한 가지 가능한 이유는 Argus3D가 자동차 내부에서 타이어와 시트의 메쉬를 생성하려고 하기 때문에 CD에 그다지 친숙하지 않을 수 있다. 둘째, 우리의 모델은 모든 경쟁자와 비교하여 MMD 및 CovT 메트릭 모두에서 분명한 이점을 가지고 있으며, 이는 생성된 모양의 뛰어난 충실도와 다양성을 별도로 나타낸다. 셋째, GBIF는 COV에서 비교적 좋은 결과를 얻지만 CovT에서는 더 나쁜 결과를 얻으며, 이는 일치하는 샘플의 대부분이 COV에서 왔음을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline \\multirow{2}{*}{Metrics} & \\multirow{2}{*}{Methods} & \\multicolumn{5}{c}{Categories} & \\multirow{2}{*}{Avg} \\\\ \\cline{3-6}  & & & \\multicolumn{1}{c}{Plane} & \\multicolumn{1}{c}{Car} & \\multicolumn{1}{c}{Chair} & \\multicolumn{1}{c}{Rifle} & \\multicolumn{1}{c}{Table} \\\\ \\hline \\multirow{6}{*}{\\(\\text{ECD}\\downarrow\\)} & IM-GAN [6] & 923 & 3172 & 658 & 371 & 418 & 1108 \\\\  & GBIF [7] & 945 & 2388 & 354 & 195 & 411 & 858 \\\\  & PointFlow [85] & 2395 & 5318 & 426 & 2708 & 3559 & 2881 \\\\  & ShapeGF [91] & 1200 & 2547 & 443 & 672 & 114 & 915 \\\\  & PVD [54] & 6661 & 7404 & 1265 & 3443 & 745 & 3904 \\\\  & _Ours_ & **236** & **842** & **27** & **65** & **31** & **240** \\\\ \\hline \\multirow{6}{*}{\\(\\text{1-NNA}\\downarrow\\)} & IM-GAN [6] & 78.18 & 89.39 & 65.83 & 69.38 & 65.31 & 73.62 \\\\  & GBIF [7] & 80.22 & 87.19 & 63.95 & 66.98 & 60.96 & 71.86 \\\\  & PointFlow [85] & 73.61 & 74.75 & 70.18 & 64.77 & 74.81 & 71.62 \\\\  & ShapeGP [91] & 74.72 & 62.81 & 59.15 & 60.65 & 55.58 & 62.58 \\\\  & PVD [54] & 81.09 & **57.37** & 62.36 & 77.32 & 43.71 & 70.49 \\\\  & _Ours_ & **59.95** & 76.58 & **57.31** & **57.28** & **54.76** & **61.17** \\\\ \\hline \\multirow{6}{*}{COV \\(\\uparrow\\)} & IM-GAN [6] & 77.01 & 65.37 & 76.38 & 73.21 & 85.71 & 75.53 \\\\  & GBIF [7] & **80.96** & **78.85** & **80.95** & **77.00** & 85.13 & **80.57** \\\\  & PointFlow [85] & 65.64 & 69.57 & 57.49 & 48.52 & 71.95 & 61.71 \\\\  & ShapeGP [91] & 76.64 & 71.85 & 79.41 & 70.67 & **57.54** & 77.22 \\\\  & PVD [54] & 58.09 & 58.64 & 68.93 & 56.12 & 76.84 & 63.72 \\\\  & _Ours_ & 79.11 & 73.25 & 80.81 & 74.26 & 84.01 & 78.29 \\\\ \\hline \\multirow{6}{*}{\\(\\text{ConvT}\\uparrow\\)} & IM-GAN [6] & 41.03 & 50.63 & 45.68 & 51.68 & 46.50 & 47.10 \\\\  & GBIF [7] & 32.38 & 57.26 & 39.77 & 50.00 & 43.68 & 43.72 \\\\  & PointFlow [85] & 35.85 & 47.76 & 28.48 & 34.81 & 30.98 & 35.57 \\\\  & ShapeGP [91] & 40.17 & 53.63 & 43.69 & 51.05 & **48.50** & 47.41 \\\\  & PVD [54] & 12.11 & 43.36 & 38.82 & 33.33 & 43.68 & 34.26 \\\\  & _Ours_ & **45.12** & **56.64** & **49.82** & **55.27** & **43.68** & **30.98** \\\\ \\hline \\multirow{6}{*}{\\(\\text{MMD}\\downarrow\\)} & IM-GAN [6] & 3418 & 1290 & 2881 & 3691 & 2505 & 2757 \\\\  & GBIF [7] & 3754 & 1333 & 3015 & 3865 & 2584 & 2910 \\\\ \\cline{1-1}  & PointFlow [85] & 3675 & 1393 & 3322 & 4038 & 2936 & 3072 \\\\ \\cline{1-1}  & ShapeGP [91] & 3530 & 1307 & 2880 & 3762 & 2420 & 2780 \\\\ \\cline{1-1}  & PVD [54] & 4376 & 1432 & 3064 & 2747 & 2623 & 3154 \\\\ \\cline{1-1}  & _Ours_ & **3124** & **1213** & **2703** & **3628** & **2374** & **2608** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: Results of unconditional generation. Models are trained for each category. The best and second results are highlighted in **bold** and underlined.\n' +
      '\n' +
      '도. 6: _Coverage_의 메트릭을 계산할 때 임계값 쉬프팅을 사용하기 전과 후의 MMD가 가장 큰 매칭된 쌍을 보여준다. 각 쌍에 대해 왼쪽은 생성된 모양이고 오른쪽은 가장 가까운 지면-진리 모양이다. 가장 잘 본 것은 확대되었다.\n' +
      '\n' +
      '도. 7: 무조건적인 세대의 질적 결과.\n' +
      '\n' +
      '거짓 양성 쌍입니다. 반대로 Argus3D는 COV에서 두 번째로 우수한 성능을 얻지만 CovT에서 GBIF보다 약 6포인트 더 높아 생성된 모양이 고품질 및 더 적은 이상값의 이점을 누리고 있음을 보여준다. 마지막으로 그림 1의 여러 범주로 도형을 시각화한다. 24, 위에서 설명한 정량적 결과 및 결론을 추가로 뒷받침한다.\n' +
      '\n' +
      '### _Class-guide Generation_\n' +
      '\n' +
      '카테고리 레이블이 주어진 모양을 생성해야 하는 클래스 가이드 생성에 대한 Argus3D의 범용성을 평가한다. 기본적인 조건 생성 작업입니다. 우리는 Sec. 5.2에서와 동일한 데이터 세트 및 평가 메트릭을 사용한다.\n' +
      '\n' +
      '**기준.** 비슷한 동기부여로 최근 출판된 두 작품을 경쟁사로 선정합니다. 하나는 2단계 생성 모델 GBIF[7]이고, 다른 하나는 AR 방법 AutoSDF[18]이다. 우리는 단지 [7]의 마스크-조건을 클래스-조건으로 수정하고, Argus3D와 동일한 [18]의 트랜스포머에 클래스 토큰을 추가한다.\n' +
      '\n' +
      '**결과 분석** 탭에서 III, Argus3D는 5개 범주에서 두 경쟁사 모두를 상당한 차이로 능가하여 모든 메트릭에서 최첨단 결과를 달성한다. 특히, 1-NNA의 메트릭에서 이점을 얻음으로써 클래스-가이드 생성에서 Argus3D의 범용성을 강력하게 입증한다. 5가지 다른 범주의 정성적 결과가 그림 1에 추가로 나와 있다. 25. 관찰된 바와 같이, 본 방법의 생성 품질은 GBIF 및 AutoSDF보다 분명히 더 나은 반면, 유형과 모양에서 더 많은 다양성을 보존한다.\n' +
      '\n' +
      '### _Multi-modal Partial Point Completion_\n' +
      '\n' +
      '또한 부분 점 구름을 주어 조건부 생성에서 모델의 성능을 검증한다. 여기서는 부분 형상이 주어졌을 때 완성된 형상의 가능성이 많기 때문에 멀티모달 완성을 옹호한다. 그것은 주어진 부분 조건에 충실하면서 상상력을 사용하는 생성 모델의 본질이다.\n' +
      '\n' +
      '**Data.** ShapeNet 데이터셋을 테스팅에 사용합니다. 여기서 두 가지 설정을 고려하는데, (1) 원근 완성[92]: 시점을 무작위로 샘플링한 후 시점으로부터 가장 먼 지점(25\\%\\sim 75\\%\\)을 제거한다; (2) 하반 완성[18]: 도형의 상단 절반에서 모든 지점을 제거한다.\n' +
      '\n' +
      '**기준.** 생성적 적대 모델 cGAN[86], 확산 모델 PVD[54], AR 모델 ShapeFormer[19] 및 AutoSDF[18]를 포함하여 4개의 다중 모드 완성 모델이 기준으로서 선택된다.\n' +
      '\n' +
      '**Metrics and Settings.**, \\(100\\) randomly selected shapes of \\(100\\) categories, _i.e._, chair, sofa, table에 대한 \\(10\\) 샘플을 완성한다. [86]에 이어, 우리는 다양성을 측정하기 위해 총 상호 차이(TMD)를 사용한다. 완성된 도형의 충실도를 측정하기 위해 Chamfer Distance와 Unidirectional Hausdorff Distance(UHD)와의 최소 정합 거리[40](MMD)[87]를 채택하였다.\n' +
      '\n' +
      '**결과 분석** 먼저 탭에서 원근법 완성 결과를 보고합니다. IV Argus3D는 모든 기준선을 능가하고 최첨단 성능을 달성합니다. 중요한 것은, 우리는 형상 분포를 학습하기 위해 트랜스포머가 있는 AR 모델을 사용하는 모든 클래스 및 메트릭에서 ShapeFormer를 능가한다는 것이다. 반면에, 우리는 하위 절반 완료 설정에서 AutoSDF와 비교한다. 탭의 결과입니다. V는 Argus3D가 TMD 및 MMD 측면에서 AutoSDF보다 우수하다는 것을 보여준다. 유연성을 강하게 시사하고\n' +
      '\n' +
      '도. 8: 클래스-가이드 생성의 질적 결과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Metrics} & \\multirow{2}{*}{Methods} & \\multicolumn{5}{c}{Categories} & \\multirow{2}{*}{AVG} \\\\ \\cline{3-8}  & & Plane & Car & Chair & Rifle & Table \\\\ \\hline \\multirow{4}{*}{COV \\(\\uparrow\\)} & GBIF [7] & 68.72 & 69.64 & 75.94 & 68.98 & 81.72 & 73.00 \\\\  & AutoSDF [18] & 70.46 & 52.77 & 63.25 & 48.10 & 72.19 & 61.35 \\\\  & _Ours_ & **81.58** & **71.58** & **83.98** & **75.74** & **85.48** & **79.67** \\\\ \\hline \\multirow{4}{*}{CovT \\(\\uparrow\\)} & GBIF [7] & 24.10 & 38.63 & 32.69 & 35.44 & 37.80 & 33.73 \\\\  & AutoSDF [18] & 30.66 & 40.49 & 31.00 & 46.36 & 30.10 & 44.57 \\\\  & _Ours_ & **56.49** & **52.70** & **45.09** & **52.74** & **49.32** & **51.27** \\\\ \\hline \\multirow{4}{*}{MMD \\(\\downarrow\\)} & GBIF [7] & 4736 & 1479 & 3220 & 4246 & 2763 & 3289 \\\\  & AutoSDF [18] & 3706 & 1456 & 3249 & 4115 & 2744 & 3054 \\\\  & _Ours_ & **3195** & **1285** & **2871** & **3729** & **2430** & **2702** \\\\ \\hline \\multirow{4}{*}{ECD \\(\\downarrow\\)} & GBIF [7] & 1327 & 2752 & 1589 & 434 & 869 & 1394 \\\\  & AutoSDF [18] & 1619 & 4256 & 1038 & 1443 & 462 & 1764 \\\\  & _Ours_ & **571** & **1889** & **419** & **196** & **285** & **672** \\\\ \\hline \\multirow{4}{*}{1-NNA \\(\\downarrow\\)} & GBIF [7] & 91.47 & 92.43 & 75.61 & 83.12 & 70.19 & 82.56 \\\\  & AutoSDF [18] & 83.31 & 87.76 & 69.34 & 77.43 & 67.20 & 77.01 \\\\ \\cline{1-1}  & _Ours_ & **66.81** & **83.39** & **64.83** & **57.28** & **59.55** & **66.37** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE III: Results of class-guide generation. Models are trained on 13 categories of ShapeNet. Best results are highlighted in **bold**.\n' +
      '\n' +
      '도. 9: 부분 포인트 완성의 질적 결과.\n' +
      '\n' +
      '제안된 방법의 적대성. 정성적 결과는 그림 1과 같다. 도 26은 완성된 도형의 다양성과 충실도를 보여준다.\n' +
      '\n' +
      '### _Image-guide Generation_\n' +
      '\n' +
      '다음으로, Argus3D가 이미지 가이드 생성까지 쉽게 확장할 수 있는 유연성을 보여주며, 이는 더 어려운 작업이다. 유연성은 (1) 특징 연접의 가장 쉬운 방법으로 구현된다는 점에 있다; (2) 이미지의 조건 형태는 1-D 특징 벡터 또는 패치 토큰, 또는 2-D 특징 맵일 수 있는 다양한 형태이다. 단순화를 위해 미리 학습된 CLIP 모델(_i.e._, ViT-B/32)을 사용하여 이미지의 특징 벡터를 조건으로 추출한다. 모든 실험은 렌더링된 이미지를 사용하여 ShapeNet에서 수행된다.\n' +
      '\n' +
      '**기준.** CLIP-Forge[88]는 흐름 기반 모델로서, 이미지 및 형상의 쌍으로 훈련된다. 우리는 (1) 재구성 모델 대신 생성 모델이고 (2) 원래 CLIP 모델을 사용하여 이미지 특징을 추출하는 두 가지 이유로 기본 기준선으로 간주한다. 또한 3차원 형상 생성을 위한 자동 회귀 모델인 AutoSDF[18]와 비교한다. 입력 영상에서 위치별 조건을 추출하기 위해 공식 코드를 따르고, 비순차적 자기회귀 모델링을 유도하여 형태를 생성한다.\n' +
      '\n' +
      '**Metrics and Settings.** 우리는 13개 범주의 테스트 분할로 모델을 평가한다. 각 범주에 대해 50개의 단일 뷰 이미지를 무작위로 샘플링한 다음 평가를 위해 5개의 모양을 생성한다. 세대 작업으로 TMD를 채택하여 다양성을 측정한다. 또한 지상진실과 비교하여 충실도를 측정하기 위해 Chamfer Distance와 FPD(Frechet Point Cloud Distance) [87]과 함께 MMD를 사용한다.\n' +
      '\n' +
      '** 결과 분석.** 결과는 탭에 보고됩니다. VI. Argus3D는 충실도와 다양성 측면에서 모두 승리한다. 특히, TMD와 FPD 모두에서 큰 이점을 달성하여 이미지 가이드 생성에 적용된 Argus3D의 효과를 입증한다. 또한 그림 1의 질적 시각화에 의해 성공적으로 반향된다. 10. 표본은 다양하며 이미지에서 객체의 속성에 시각적으로 충실하게 나타난다. 가장 중요한 것은 우리의 접근 방식이 1D 또는 2D 형태로 입력을 수용하여 더 많은 유연성을 제공할 수 있다는 것이다. 이를 검증하기 위해 ViT32(또는 ResNet)를 이용하여 입력 영상에서 패치 임베딩(또는 이미지 특징)을 조건으로 추출하는 두 개의 베이스라인을 수행한다. 조건부 생성은 조건을 [SOS] 토큰에 단순히 프리펜딩함으로써 달성될 수 있다. 탭의 결과입니다. VI는 CLIP를 특징 추출기로 사용하는 모델과 경쟁력이 있음을 나타내며, 생성 방식 또는 조건부 형태로 우리의 Argus3D의 강력한 범용성을 추가로 시사한다.\n' +
      '\n' +
      '### _Text-guide Generation_\n' +
      '\n' +
      '이미지 가이드 생성에 대한 유망한 결과에 힘입어 Argus3D를 텍스트에서 모양 생성으로 전환한다. 동일한 사전 훈련된 CLIP 모델을 사용하여 텍스트 조건에 대한 단일 임베딩을 추출한다. 우리는 워드 시퀀스 임베딩을 사용하는 것이 아니라, 가장 단순한 형태의 조건에 대한 효과와 확장성을 보여주기 위해 모델에 단일 특징 벡터만을 사용하여 의도적으로 수행했다는 점에 유의한다. 우리는 나중에 텍스트 조건의 길이에 대해 논의할 것이다.\n' +
      '\n' +
      '**Data.** 중요한 쌍을 이루는 텍스트 모양 데이터 세트 중 하나는 Text2Shape[93]이며, 이는 ShapeNet, _i.e._, 의자 및 테이블의 두 개 객체에 대한 언어 설명을 제공한다. 따라서 텍스트 가이드 모양 생성을 수행하는 것을 주요 데이터 세트로 간주한다.\n' +
      '\n' +
      '**기준.** 우리는 우리의 모델을 세 가지 최첨단 텍스트 대 모양 생성 모델과 비교한다. 하나는 우리와 동일한 조건 추출기를 사용하여 지도 학습 설정에서 CLIP-Forge[88]이다. 하나는 ITG[94]로, BERT를 채택하여 텍스트를 시퀀스 임베딩으로 인코딩한다. 그리고 다른 하나는 AutoSDF[18]이고, 여기서 우리는 그것의 공식 코드를 사용하여 그것을 재훈련한다.\n' +
      '\n' +
      '**Metrics and Settings.** 모든 모델은 공식 코드로 Text2Shape 데이터셋의 열차 분할에 대해 학습된다. 본 연구는 텍스트 질의의 두 가지 유형, 즉 (1) Text2Shape의 테스트 분할, (2) 프롬프트, 의자와 테이블에 대한 속성을 포함하는 [88]에서 제공하는 맞춤형 짧은 문구를 사용하여 모델을 평가한다. 우리는 충실도를 측정하기 위해 Accuracy(Acc.)[88]을 추가적으로 사용한다. 정확도는 ShapeNet 상의 미리 훈련된 PointNet[32] 분류기에 의해 계산된다.\n' +
      '\n' +
      '**결과 분석** 탭에서 텍스트 가이드 생성의 정량적 결과를 보고한다. VII. Argus3D는 유망한 결과를 달성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline \\multicolumn{1}{c}{Method} & \\multicolumn{1}{c}{TMD (\\(\\times 10^{2}\\)) \\(\\uparrow\\)} & \\multicolumn{1}{c}{MMD (\\(\\times 10^{3}\\)) \\(\\downarrow\\)} & \\multicolumn{1}{c}{FPD \\(\\downarrow\\)} \\\\ \\hline AutoSDF [18] & 2.523 & **1.383** & 2.092 \\\\ Clip-Forge [88] & 2.858 & 1.926 & 8.094 \\\\ \\hline _Ours_ (ViT32) & 3.677 & 1.617 & 2.711 \\\\ _Ours_ (ResNet) & **4.738** & 1.662 & 3.894 \\\\ _Ours_ (CLIP) & 4.274 & 1.590 & **1.680** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VI: Quantitative results of image-guide generation.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l} \\hline \\hline \\multirow{2}{*}{Metrics} & \\multirow{2}{*}{Methods} & \\multicolumn{3}{c}{Categories} & \\multirow{2}{*}{AVG} \\\\ \\cline{3-4} \\cline{6-6}  & & Chair & Sofa & Table \\\\ \\hline \\multirow{3}{*}{\\begin{tabular}{l} TMD \\(\\uparrow\\) \\\\ (\\(\\times 10^{2}\\)) \\\\ \\end{tabular} } & cGAN [86] & 1.708 & 0.687 & **1.707** & 1.367 \\\\  & PVD [54] & 1.098 & 0.811 & 0.839 & 0.916 \\\\  & ShapeFormer [19] & 1.159 & 0.698 & 0.677 & 0.845 \\\\  & _Ours_ & **2.042** & **1.221** & 1.538 & **1.600** \\\\ \\hline \\multirow{3}{*}{\\begin{tabular}{l} UHD \\(\\downarrow\\) \\\\ (\\(\\times 10^{2}\\)) \\\\ \\end{tabular} } & cGAN [86] & 7.836 & 7.047 & 9.406 & 8.096 \\\\  & PVD [54] & 10.79 & 13.88 & 11.38 & 12.02 \\\\  & ShapeFormer [19] & 6.884 & 8.658 & 6.688 & 7.410 \\\\  & _Ours_ & **6.439** & **6.447** & **5.948** & **6.278** \\\\ \\hline \\multirow{3}{*}{\n' +
      '\\begin{tabular}{l} MMD \\(\\downarrow\\) \\\\ (\\(\\times 10^{3}\\)) \\\\ \\end{tabular} } & cGAN [86] & 1.665 & 1.813 & 1.596 & 1.691 \\\\  & PVD [54] & 2.352 & 2.041 & 2.174 & 2.189 \\\\ \\cline{1-1}  & ShapeFormer [19] & 1.055 & 1.100 & 1.066 & 1.074 \\\\ \\cline{1-1}  & _Ours_ & **0.961** & **0.819** & **0.828** & **0.869** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IV: Results of multi-modal partial point completion. The missing parts vary according to random viewpoints.\n' +
      '\n' +
      '도. 10: 이미지-가이드 형상 생성의 시각화.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l l l l l} \\hline \\hline \\multirow{2}{*}{Metrics} & \\multirow{2}{*}{Methods} & \\multicolumn{3}{c}{Categories} & \\multirow{2}{*}{AVG} \\\\ \\cline{3-4} \\cline{6-6}  & & Chair & Sofa & Table \\\\ \\hline \\multirow{3}{*}{\\begin{tabular}{l} TMD \\(\\uparrow\\) \\\\ (\\(\\times 10^{2}\\)) \\\\ \\end{tabular} } & AutoSDF [18] & 2.046 & 1.609 & 3.116 & 2.257 \\\\  & _Ours_ & **3.682** & **2.673** & **10.30** & **5.552** \\\\ \\hline \\multirow{3}{*}{\\begin{tabular}{l} UHD \\(\\downarrow\\) \\\\ (\\(\\times 10^{2}\\)) \\\\ \\end{tabular} } & AutoSDF [18] & **6.793** & 9.950 & **8.122** & 8.289 \\\\  & _Ours_ & 6.996 & **6.599** & 10.87 & **8.155** \\\\ \\hline \\multirow{3}{*}{\n' +
      '\\begin{tabular}{l} MMD \\(\\downarrow\\) \\\\ (\\(\\times 10^{3}\\)) \\\\ \\end{tabular} } & AutoSDF [18] & 1.501 & 1.154 & **2.600** & **1.751** \\\\  & _Ours_ & **1.477** & **1.127** & 2.906 & 1.837 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE V: Quantitative results of multi-modal partial point completion. The missing parts are always the top half.\n' +
      '\n' +
      'TMD, MMD 및 Acc는 서로 다른 조건 생성 태스크에 걸쳐 우수한 일반화 성능을 보여준다. 정성적 결과는 그림 1에 나와 있다. 11. 더 나아가, 이미지와 달리 텍스트는 자연스러운 형태의 시퀀스를 갖는다. 각 단어는 문맥과 밀접한 관련이 있습니다. 이를 통해 서열 임베딩을 조건으로 하는 텍스트 가이드 생성에 대한 Argus3D의 능력에 대해 논의한다. 구체적으로, 우리는 텍스트를 고정된 길이의 시퀀스 임베딩으로 인코딩하기 위해 BERT 및 CLIP 3 모델을 채택한다. 탭에서요 VII(a)는 시퀀스 임베딩을 사용하면 텍스트 가이드 생성 작업에 대한 Argus3D의 성능이 실제로 향상된다는 것을 발견했다. 더 다루기 쉬운 순서로 제안된 컴팩트한 이산 표현 덕분에 Argus3D는 다른 작업에 대해 다른 조건부 형태에 쉽게 적응할 수 있으므로 생성된 모양의 품질을 향상시킬 수 있다.\n' +
      '\n' +
      '각주 3: 이 경우 마지막 [END] 토큰 대신 모든 토큰의 특징을 텍스트 임베딩으로 출력한다.\n' +
      '\n' +
      '###_Generation with the larger scale_\n' +
      '\n' +
      'Sec. 4.4에서 논의한 바와 같이 컴팩트한 이산 표현과 바닐라 변압기 구조를 활용하여 더 큰 매개변수에 의해 부여된 더 강력한 발전 용량을 찾기 위해 Argus3D를 기본 모델 1억에서 최대 모델 36억으로 확장한다. 따라서 이 섹션에서는 다양한 규모에서 Argus3D의 효능과 대규모 3D 데이터 세트 Oboiverse-Mix의 영향을 조사하는 데 중점을 둔다. 본 논문에서는 태스크의 도전과 신뢰성을 고려하여 조건부 3차원 형상 생성 태스크인 클래스-가이드와 이미지-가이드에 대한 모델을 평가한다. 실험 설정 및 메트릭은 특별히 설명하지 않는 한 위의 해당 작업과 동일하다.\n' +
      '\n' +
      '**더 큰 모델의 이점.** 동일한 ShapeNet 데이터 세트를 사용하여 클래스 가이드 생성 작업을 위해 각각 기본, 큰 및 거대한 규모에서 세 개의 Argus3D 모델을 훈련한다. 세 가지 모델의 매개변수는 탭 1에 나열되어 있으며, 특히 모두 1단계 모델의 가중치를 공유한다. Sec. 5.3과 달리 효율성을 위한 성능 평가를 위해 거리 메트릭으로 Chamfer Distance(CD)와 Earth Mover\'s Distance(EMD)를 사용한다. 이러한 메트릭은 정확하고 다양한 모양을 생성하는 모델의 능력에 대한 다양한 관점을 제공한다.\n' +
      '\n' +
      '먼저 모델 매개변수가 생성 품질에 미치는 영향에 대해 논의한다. 그림 12는 세 가지 Argus3D 모델의 결과를 보여준다. 예상대로 모델 매개변수의 증가에 따라 4가지 메트릭의 성능이 상당히 향상된다. 또한, 우리는 우리의 모델을 최신 접근법과 비교한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table} TABLE VII: Quantitative results of text-guide generation.\n' +
      '\n' +
      '도. 11: 텍스트 질의로서 프롬프트를 사용하는, 텍스트-가이드 생성의 정성적 비교. 모델은 _i.e._, 의자 및 테이블의 두 가지 범주만 포함하는 Text2Shape 데이터 세트에 대해 학습된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table} TABLE VIII: Result comparisons of our large model on class-guide scales. Models are trained for class-guide generation on ShapeNet.\n' +
      '\n' +
      '도. 12: 다른 척도와 우리의 Argus3D의 정량적 비교. 모델은 ShapeNet에서 클래스 가이드 생성을 위해 훈련된다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '실제 응용 프로그램입니다. 일반성의 손실 없이 모든 실험은 Argus3D-Base 모델로 수행된다.\n' +
      '\n' +
      '**평탄화 순서의 영향**평탄화 순서가 형상 생성의 품질에 어떤 영향을 미치는지 먼저 조사한다. 본 연구에서는 제안된 coupling network_를 제거함으로써, 개선된 이산 표현(\'Vector\')이 자연스럽게 \'Tri-Plane\'으로 퇴화할 수 있기 때문에, Tri-Plane 표현을 예로 들었다. 앞서 Fig.에서 언급한 바와 같다. 도 3에서, 상이한 평탄화 차수들은 세 평면들의 상이한 자기-회귀 생성 차수들에 영향을 미칠 것이다. 정량적으로, Iter-A=p\\left(\\mathbf{z}^{xz}\\right)=p\\left(\\mathbf{z}^{xz}\\right)\\cdot p\\left(\\mathbff{z}^{yz}\\right)\\cdot p\\left(\\mathbff{z}^{yz}\\right)\\cdot p\\left(\\mathbff{z}^{xz}\\right)\\cdot p\\left(\\mathbff{z}^{z}^{xz}\\right)\\cdot p\\left(\\mathbff{z}^{z}^{xz}\\right)\\cdot p\\left(\\mathbff{z}^{z}^{xz}\\right)\\cdot p\\left(\\mathbff{z}^{z}^{xz}\\right)\\cdot p\\left(\\mathbff{z}^{z}^{xz}\\right)\\cdot p\\left(\n' +
      '\n' +
      '결과는 탭에 나와 있습니다. I와 Fig. 16. 관측된 바와 같이, 상이한 차수들은 성능에 상당한 영향을 주어 표준 편차의 큰 값을 초래한다. 예를 들어, Iter-A는 평면 카테고리 상에서 더 나은 결과를 달성한다(Iter-A: 73.67 _vs._ Iter-B: 83.37 on 1-NNA). 한편, Rifle의 경우 Iter-B(Iter-B:56.54 _vs._Iter-A:68.35 on 1-NNA)의 순서를 선호한다. 우리는 세 개의 평면에서 투영된 형상을 시각화하여 이 현상을 설명하고자 한다. 도 1에 도시된 바와 같다. 도 17을 참조하면, 평면 카테고리(First Column)의 경우, \\(xy\\)-평면으로의 투영은 제한된 형상 정보를 제공하는 반면, \\(xz\\)-평면은 항공기의 보다 대표적인 형상을 나타낸다. 우리는 더 많은 형상 정보를 포함하는 \\(xz\\)-평면이 먼저 생성된다면, 후속 평면들의 생성이 훨씬 더 쉬울 수 있다는 것에 동의한다. 결과적으로, Iter-A는 Iter-B보다 더 충실한 3D 형상을 생성하는 것이 유익하다. 대조적으로, 라이플과 의자는 \\(xy\\)-평면에서 더 많은 세부 사항을 나타내므로 Iter-B의 자기회귀 차수는 이 두 범주에 대해 더 나은 결과를 산출한다. 또한, 우리는 자동차가 비교적 단순한 형상, 예를 들어 직육면체 형태를 가지고 있어서, 상이한 평탄화 주문에 대한 생성 품질에 유사한 영향을 초래한다는 것을 알아챘다.\n' +
      '\n' +
      '**평탄화 순서의 효과.** 다음으로 효능 및 효율성 측면에서 개선된 이산 표현의 이점을 평가한다. 결합된 특징 맵을 행-주 또는 열-주 순서를 가진 벡터로 평평하게 하여 전체 모델의 두 가지 변형을 탐색한다. 탭에서 본 논문에서 제안하는 방법은 서로 다른 직렬화 순서에서도 유사한 성능을 보인다. 그림 16은 우리의 표준 편차가 \'Tri-Plane\'보다 현저히 낮다는 것을 보여주며, 생성 순서에 대한 개선된 표현의 견고성을 보여준다. 제안된 결합 네트워크는 보다 다루기 쉬운 순서를 도입함으로써 AR 학습을 용이하게 했다. 또한, 모든 범주에 대해 합성된 도형의 전반적인 품질은 모든 메트릭에 걸쳐 균형 있고 우수하여 우리 설계의 우수성을 나타낸다.\n' +
      '\n' +
      '또한, 낮은 연산 오버헤드의 장점도 조사한다. 잠재 벡터에 벡터 양자화를 적용하기 때문에 벡터(Vector)를 사용하고, 체적 격자(volumetric grid)에 벡터 양자화를 적용하는 베이스라인 방법을 \'Grid\'라 한다. 그림 18은 첫 번째 단계에서 IoU의 성능과 두 번째 단계에서 해당 메모리 비용을 비교한다. 우리는 체적 격자 표현으로 변압기를 훈련시킬 수 없기 때문에, 대안으로 비교를 위한 1단계 IoU 정확도를 보고한다. 를 포함하는 것을 특징으로 하는 반도체 소자의 제조 방법. 도 18을 참조하면, 두 가지 결론을 도출할 수 있다. **(1)** 특징 그리드(\'Grid\' 및 \'Vector\')의 해상도 \\(r\\)는 재구성된 형상의 품질에 상당한 영향을 미친다. 만약 \\(r\\)이 너무 작으면 복잡하고 세밀한 기하학(Grid Reso-32: \\(88.87\\)_vs_ Grid Reso-16: \\(81.12\\))을 표현할 수 있는 능력이 부족하다. 그러나, 큰 \\(r\\)은 \\(r\\)이 커짐에 따라 필요한 코드의 수가 폭발적으로 증가함에 따라 두 번째 단계에서 계산 복잡도가 증가할 수밖에 없다(Grid Reso-32: \\(\\geq 80\\)G _vs._ Grid Reso-16: \\(19.6\\)G. **(b)** 우리의 제안된 \'Vector\' 표현은 비교 가능한 재구성 결과(Vector Reso-32: \\(88.01\\)_vs._ Grid Reso-32: \\(88.87\\)), 그러나 또한 계산 오버헤드를 상당히 감소시킨다(Vector Reso-32: \\(3.8\\)G _vs._ Grid Reso-32: \\(\\geq 80\\)G.\n' +
      '\n' +
      '**이산 표현 학습에서의 설계 선택.** 이 논문의 핵심 기여로서, 우리는 먼저 개선된 이산 표현 학습의 효능에 대해 논의한다. 유사하게, \'Tri-Plane\'은 세 평면에 벡터 양자화를 적용하는 기준선을 지칭한다. 탭의 결과입니다. 그리드(Grid)는 1단계에서는 형상 재구성을 위한 IoU 성능이 향상되지만, 2단계에서는 긴 길이의 시퀀스(_e.g._, \\(32^{3}\\))로 인해 변압기를 훈련시키지 못하는 것으로 나타났다. 대조적으로, 우리의 모델은 비교 가능한 재구성 결과(#0 _vs._ #2)를 달성할 뿐만 아니라 뿐만 아니라,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline \\multirow{2}{*}{Metrics} & \\multirow{2}{*}{Methods} & \\multicolumn{5}{c}{Categories} \\\\ \\cline{3-6}  & & Plane & Rifle & Chair & Car \\\\ \\hline \\multirow{4}{*}{\\(\\text{ECD}\\downarrow\\)} & \\multirow{4}{*}{Tri-Plane} & Iter-A & 744 & 405 & 4966 & 3599 \\\\  & & Iter-B & 3501 & **36** & 1823 & 4735 \\\\  & & Iter-C & 3098 & **282** & 4749 & 3193 \\\\ \\cline{2-6}  & \\multirow{2}{*}{Vector (_ours_)} & Row-Major & 236 & 65 & **27** & **842** \\\\  & & Col-Major & **205** & 79 & 102 & 980 \\\\ \\hline \\multirow{4}{*}{1-NNA \\(\\downarrow\\)} & \\multirow{4}{*}{Tri-Plane} & Iter-A & 73.67 & 68.35 & 78.15 & 87.16 \\\\  & & Iter-B & 83.37 & **56.54** & 70.92 & 87.42 \\\\  & & Iter-C & 81.83 & 65.61 & 78.38 & 88.53 \\\\ \\cline{2-6}  & \\multirow{2}{*}{Vector (_ours_)} & Row-Major & **59.95** & 57.28 & **73.71** & **76.58** \\\\  & & Col-Major & 62.48 & 57.70 & 58.38 & 78.09 \\\\ \\hline \\multirow{4}{*}{\\(\\text{COV}\\uparrow\\)} & \\multirow{4}{*}{Tri-Plane} & Iter-A & **81.70** & 75.10 & 79.33 & 65.31 \\\\  & & Iter-B & 74.16 & 75.52 & **82.95** & 63.97 \\\\  & & Iter-C & 71.32 & **76.69** & 78.89 & 72.25 \\\\ \\cline{2-6}  & \\multirow{2}{*}{Vector (_ours_)} & Row-Major & 79.11 & 74.26 & 80.81 & **73.25** \\\\  & & Col-Major & 77.87 & 73.52 & 81.03 & 71.31 \\\\ \\hline \\multirow{4}{*}{\\(\\text{CoV}\\uparrow\\)} & \\multirow{4}{*}{Tri-Plane} & Iter-A & 43.51 & 41.65 & 23.10 & 50.30 \\\\  & & Iter-B & 26.57 & 49.78 & 35.50 & 49.43 \\\\  & & Iter-C & 30.28 & 41.35 & 24.94 & 51.23 \\\\ \\cline{2-6}  & \\multirow{2}{*}{Vector (_ours_)} & Row-Major & **45.12** & **55.27** & **49.82** & **56.64** \\\\  & & Col-Major & 44.87 & 53.58 & 48.93 & 56.63 \\\\ \\hline \\multirow{4}{*}{\\(\\text{MMD}\\downarrow\\)} & \\multirow{4}{*}{Tri-Plane} & Iter-A & 3237 & 3962 & 3392 & 1373 \\\\  & & Iter-B & 3860 & 3624 & 3119 & 1404 \\\\ \\cline{1-1}  & & Iter-C & 3631 & 3958 & 3430 & 1385 \\\\ \\cline{1-1} \\cline{2-6}  & \\multirow{2}{*}{Vector (_ours_)} & Row-Major & 3124 & 3628 & **2703** & **1213** \\\\ \\cline{1-1}  & & Col-Major & **3102** & **3623** & 2707 & 1214 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I0: The effect of flattening order on different discrete representations. Here, we take unconditional generation as an example and train one model per class. Please find statistic and qualitative analyses in Fig. 16 and 17, respectively.\n' +
      '\n' +
      '도. 15: DALL-E-2 생성 이미지를 기반으로 한 이미지 가이드 생성. Argus3D는 보이지 않는 이미지에서 광범위한 모양을 생성할 수 있다. 이러한 모양은 DALL-E 2의 텍스트 프롬프트를 활용하는 텍스처 모델에 의해 생성된 텍스처로 더욱 향상될 수 있으며, 다양한 텍스트 프롬프트의 사용은 새롭고 독특한 텍스처의 생성을 가능하게 한다.\n' +
      '\n' +
      '(#1 vs. #2). 후자는 \'순서의 모호성\'(Suppl 참조)으로 인해 열등한 결과를 보여준다. 이는 제안된 결합 네트워크의 유효성을 크게 입증하여 모델의 가소성과 유연성을 향상시킨다. 또한 탭에서 다양한 디자인 선택을 탐색합니다. IV 코드북의 특징 해상도 또는 엔트리 수를 점진적으로 증가시킴으로써, IoU에서 더 나은 성능을 달성한다. 이 관찰은 이산 표현의 능력이 이 두 요인에 의해 영향을 받기 때문에 [97]과 일치한다. 더 많은 수의 엔트리는 정확도를 더욱 향상시킬 것을 약속한다. 따라서 우리는 크고 거대한 모델에 대해 경험적으로 두 배입니다.\n' +
      '\n' +
      '**실세계에서의 이미지 가이드 생성.** 실세계 이미지에 대한 모델의 일반화 가능성을 추가로 조사한다. 우리는 Sec. 5.5에 설명된 대로 ShapeNet에서 훈련된 모델을 사용하고, 조건으로 인터넷에서 이미지를 다운로드한다. 도. 도 20은 상이한 카테고리들, 즉, _i.e._, 평면, 의자, 자동차 및 테이블에 대한 정성적 결과들을 도시한다. 우리의 모델은 이미지에서 객체의 주요 속성을 민감하게 포착하고 그에 충실한 도형을 생성한다. (왼쪽의 첫 번째 열을 참조) 한편, 우리의 합성 샘플은 두 개의 비행기 이미지에 대해 날개와 꼬리의 유형과 같은 이미지에 부분적으로 달라붙어 다양성의 이점을 누린다.\n' +
      '\n' +
      '그림 19는 또한 Pix3D 데이터셋( ShapeNet에서 훈련됨, 미세 조정 없이)에 대한 우리 모델의 결과를 보여준다. Argus3D는 다른 경쟁사들과 비교하여 실세계 이미지에서 물체의 모양과 매우 일치하는 고품질의 사실적인 모양을 생성할 수 있다. 그것은 우리의 방법의 강력한 일반화 능력을 크게 강조한다.\n' +
      '\n' +
      '**Zero-shot Text-to-shape Generation.** CLIP-Forge에서 영감을 받은 [88], 우리는 CLIP 모델을 활용하여 Zero-shot Text-to-shape generation을 달성한다. 훈련에서는 3D 모양의 렌더링된 이미지만 사용합니다. 추론에서 우리는 이미지 특징을 CLIP 모델에 의해 인코딩된 텍스트 특징으로 대체한다. 그림 21과 22는 모양 속성이 다른 프롬프트로 제어되는 제로 샷 생성 능력을 보여준다. 합성된 도형의 고품질은 우리의 강력한 범용성을 분명히 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c} \\hline \\multirow{2}{*}{Num.} & \\multirow{2}{*}{Type} & \\multirow{2}{*}{\\#Enturk} & \\multirow{2}{*}{Reso.} & Stage 1 & Stage 2 \\\\ \\cline{5-6}  & & & & IoU \\(\\uparrow\\) & 1-NNA / ECD \\(\\downarrow\\) \\\\ \\hline \\hline\n' +
      '0 & Grid & & 88.87 & \\(\\times\\ \\\\cline{2-6}\n' +
      '1 & Tin-Plane & 4096 & 32 & 87.81 & 73.67/743 \\\\cline{2-6}\n' +
      '2 & & 88.01 & **59.95/236**\\\\cline{2-6}\n' +
      '3 &\\멀티로우{2}{*}{Vector} & 4096 & 16 & 79.17 &\\멀티로우{2}{*}{-}\\\\\\cline{4-6}\n' +
      '4 & & 2048 & 32 & 86.59 &\\\\cline{2-6}\n' +
      '5 & & 1024 & 86.57 & & \\\\ \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XI: Ablation study of auto-encoder design choices. We report 1-NNA/ECD for plane category. ‘Reso.’ means the resolution of feature map for vector quantization. ‘\\(\\times\\)’: cannot report due to extreme memory cost. ‘-’: not report. _Notably, without the coupling network, our method naturally degenerates into ‘Tri-Plane’ representation._\n' +
      '\n' +
      '도. 16: 평탄화 순서의 효과에 대한 통계 분석. 우리는 평균과 표준 편차를 각각 히스토그램과 오차 막대로 보고한다. 색상으로 가장 잘 보고 확대했습니다.\n' +
      '\n' +
      '도. 17: 3개의 평면에서 투영된 형상의 시각화.\n' +
      '\n' +
      '도. 18: 1단계 IoU(%) 정확도와 2단계 메모리 비용(Mb)을 서로 다른 해상도와 이산 표현으로 비교한다. 메모리 비용은 배치 크기가 1로 계산됩니다.\n' +
      '\n' +
      '3D 형상 생성의 Argus3D는 실제 응용 분야에 큰 잠재력을 보여준다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '3차원 형상 생성을 위한 개선된 AR 모델을 소개한다. 인코딩된 입력 형상의 체적 그리드를 3개의 축-정렬 직교 특징 평면에 투영한 후 잠재 벡터로 결합함으로써 계산 비용을 줄이고 AR 학습에 대한 보다 다루기 쉬운 순서를 생성한다. 컴팩트하고 다루기 쉬운 표현으로 멀티모달 컨디셔닝 입력으로 무조건 생성과 조건부 생성 사이를 쉽게 전환할 수 있습니다. 광범위한 실험을 통해 본 논문에서 제안한 모델이 다중 생성 태스크에서 기존의 방법들보다 우수함을 보인다. 모델 매개변수와 데이터 세트 크기를 모두 확대하여 현재 사용 가능한 가장 큰 3D 모양 생성 모델인 Argus3D-Huge를 개발했다. 이와 함께, 우리는 멀티모달 데이터를 포함하는 포괄적인 컬렉션인 Obojaverse-Mix 데이터셋을 소개한다. 이러한 발전은 모델의 성능을 더욱 향상시켜 3D 형상 생성에서 강력한 기능을 보여줍니다.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      '해당 저자는 얀웨이 푸, 상양 슈, 욱다 장 등이다. 이 작업은 중국 박사 후 과학 재단(2022M710746), 중국 국가 핵심 R&D 프로그램(2021ZD0111102) 및 NSFC-62306046의 지원을 받으며, 옌웨이 푸는 푸단 대학의 데이터 과학 학교 및 MOE 프론티어스 뇌 과학 센터와 함께한다. Yanwei Fu는 또한 Fudan ISTBI--ZNU 알고리즘 Centre for Brain- inspired intelligence, Zhejiang Normal University, Jinhua, China와 함께 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] O. Mces, M. Tatarchenko, T. Brox, and W. Burgard, "Self-supervised 3d shape and viewpoint estimation from single images for robotics," in _2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_. IEEE, 2019, pp. 6083-6089.\n' +
      '* [2] J. Ye, Y. Chen, N. Wang, and X. Wang, "Online adaptation for implicit object tracking and shape reconstruction in the wild," _arXiv preprint arXiv:2111.12728_, 2021.\n' +
      '* [3] X. Qian, L. Wang, Y. Zhu, L. Zhang, Y. Fu, and X. Xue, "Im-pdet: Exploring implicit fields for 3d object detection," _arXiv preprint arXiv:2203.17240_, 2022.\n' +
      '* [4] Y. Sun, S. N. R. Kantareddy, R. Bhattacharyya, and S. E. Sarma, "X-vision: An augmented vision tool with real-time sensing ability in tagged environments," in _2018 1eee international conference on p4th technology & application (rfd-ta)_. IEEE, 2018, pp. 1-6.\n' +
      '* [5] J. D. Stets, Y. Sun, W. Corning, and S. W. Greenwald, "Visualization and labeling of point clouds in virtual reality," in _SIGGRAPH Asia 2017 Posters_, 2017, pp. 1-2.\n' +
      '* [6] Z. Chen and H. Zhang, "Learning implicit fields for generative shape modeling," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, pp. 5939-5948.\n' +
      '\n' +
      '도. 21: 제로 샷 텍스트 대 쉐이프 생성의 정성적 비교. CLIP-Forge의 결과는 그들의 논문에 보고되어 있다.\n' +
      '\n' +
      '도. 22: 제로-샷 텍스트-투-셰이프 생성의 더 많은 시각화.\n' +
      '\n' +
      'M. Ibing, I. Lim, L. Kobbelt, "3d shape generation with grid-based implicit functions," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021, pp. 13 559-13 568.\n' +
      '* [1] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove, "Deepsdf: Learning continuous signed distance functions for shape representation," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, pp. 165-174.\n' +
      '* [2] R. Thoppilan, D. De Freitas, J. Hall, N. Shazezeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du _et al._, "Lambda: Language models for dialog applications," _arXiv preprint arXiv:2201.08239_, 2022.\n' +
      '* [3] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever _et al._, "Improving language understanding by generative pre-training," 2018.\n' +
      '* [4] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever _et al._, "Language models are unsupervised multitask learners," _OpenAI blog_, vol. 1, no. 8, p. 9, 2019.\n' +
      '* [5] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su _et al._, "Shapenet: An information-rich 3d model repository," _arXiv preprint arXiv:1512.03102_, 2015.\n' +
      '* [6] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi, "ObiJaverse: A universe of annotated 3d objects," _arXiv preprint arXiv:2212.08051_, 2022.\n' +
      '* [7] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, "3d shapenets: A deep representation for volumetric shapes," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2015, pp. 1912-1920.\n' +
      '* [8] P. Esser, R. Rombach, and B. Ommer, "Taming transformers for high-resolution image synthesis," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021, pp. 12 873-12 883.\n' +
      '* [9] L. Zhao, Z. Zhang, T. Chen, D. Metaxas, and H. Zhang, "Improved transformer for high-resolution gans," _Advances in Neural Information Processing Systems_, vol. 34, pp. 18 367-18 380, 2021.\n' +
      '* [10] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman, "Maskgit: Masked generative image transformer," _arXiv preprint arXiv:2202.04200_, 2022.\n' +
      '* [11] P. Mittal, Y.-C. Cheng, M. Singh, and S. Tulsiani, "Autosdf: Shape priors for 3d competition, reconstruction and generation," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 306-315.\n' +
      '* [12] X. Yan, L. Lin, N. J. Mitra, D. Lischinski, D. Cohen-Or, and H. Huang, "Shapeformer: Transformer-based shape completion via sparse representation," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 6239-6249.\n' +
      '* [13] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell _et al._, "Language models are few-shot learners," _Advances in neural information processing systems_, vol. 33, pp. 1877-1901, 2020.\n' +
      '* [14] S. Luo, X. Qian, Y. Fu, Y. Zhang, Y. Tai, Z. Zhang, C. Wang, and X. Xue, "Learning versatile 3d shape generation with improved ard models," _arXiv preprint arXiv:2303.14700_, 2023.\n' +
      '* [15] D. Maturana and S. Scherer, "Voxnet: A 3d convolutional neural network for real-time object recognition," in _2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_. IEEE, 2015, pp. 922-928.\n' +
      '* [16] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, "3d shapenets: A deep representation for volumetric shapes," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2015, pp. 1912-1920.\n' +
      '* [17] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese, "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction," in _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14_. Springer, 2016, pp. 628-644.\n' +
      '* [18] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum, "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling," _Advances in neural information processing systems_, vol. 29, 2016.\n' +
      '* [19] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong, "O-cnn: Octree-based convolutional neural networks for 3d shape analysis," _ACM Transactions On Graphics (TOG)_, vol. 36, no. 4, pp. 1-11, 2017.\n' +
      '* [20] C. Hune, S. Tulsiani, and J. Malik, "Hierarchical surface prediction for 3d object reconstruction," in _2017 International Conference on 3D Vision (3DV)_. IEEE, 2017, pp. 412-420.\n' +
      '* [21] G. Riegler, A. O. Ulusoy, H. Bischof, and A. Geiger, "Octnetfusion: Learning depth fusion from data," in _2017 International Conference on 3D Vision (3DV)_. IEEE, 2017, pp. 57-66.\n' +
      '* [22] J. Xie, Z. Zheng, R. Gao, W. Wang, S.-C. Zhu, and Y. N. Wu, "Generative voxelnet: Learning energy-based models for 3d shape synthesis and analysis," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, vol. 44, no. 5, pp. 2468-2484, 2022.\n' +
      '* [23] H. Fan, H. Su, and L. J. Guibas, "A point set generation network for 3d object reconstruction from a single image," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2017, pp. 605-613.\n' +
      '* [24] W. Wu, Z. Qi, and L. Fuxin, "Pointconv: Deep convolutional networks on 3d point clouds," in _Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition_, 2019, pp. 9621-9630.\n' +
      '* [25] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, "Pointnet: Deep learning on point sets for 3d classification and segmentation," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2017, pp. 652-660.\n' +
      '* [26] J. Xie, Y. Xu, Z. Zheng, S.-C. Zhu, and Y. N. Wu, "Generative pointnet: Deep energy-based learning on unordered point sets for 3d generation," reconstruction and classification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2021, pp. 14 976-14 985.\n' +
      '* [27] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger, "Occupancy networks: Learning 3d reconstruction in function space," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, pp. 4460-4470.\n' +
      '* [28] M. Michalkewicz, J. K. Pontes, D. Jack, M. Baktashmotlagh, and A. Eriksson, "Implicit surface representations as layers in neural networks," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 4743-4752.\n' +
      '* [29] V. Sitzmann, E. Chan, R. Tucker, N. Snavely, and G. Wetzstein, "Metasdf: Meta-learning signed distance functions," _Advances in Neural Information Processing Systems_, vol. 33, pp. 10 136-10 147, 2020.\n' +
      '* [30] S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, and A. Geiger, "Convolutional occupancy networks," in _European Conference on Computer Vision_. Springer, 2020, pp. 523-540.\n' +
      '* [31] W. E. Lorensen and H. E. Cline, "Marching cubes: A high resolution 3d surface construction algorithm," _ACM siggraph computer graphics_, vol. 21, no. 4, pp. 163-1636, 1987.\n' +
      '* [32] C. Jiang, A. Sud, A. Makadia, J. Huang, M. Niessner, T. Funkhouser _et al._, "Local implicit grid representations for 3d scenes," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020, pp. 6001-6010.\n' +
      '* [33] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas, "Learning representations and generative models for 3d point clouds," in _International conference on machine learning_. PMLR, 2018, pp. 40-49.\n' +
      '* [34] A. Van den Oord, N. Kalchbrenner, L. Espbalt, O. Vinyals, A. Graves _et al._, "Conditional image generation with pixel clean decoders," _Advances in neural information processing systems_, vol. 29, 2016.\n' +
      '* [35] A. Razavi, A. Van den Oord, and O. Vinyals, "Generating diverse high-fidelity images with vq-vae-2," _Advances in neural information processing systems_, vol. 32, 2019.\n' +
      '* [36] Y. Sun, Y. Wang, Z. Liu, J. Siegel, and S. Sarma, "Pointgrow: Autoregressively learned point cloud generation with self-attention," in _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, 2020, pp. 61-70.\n' +
      '* [37] A.-C. Cheng, X. Li, S. Liu, M. Sun, and M.-H. Yang, "Autoregressive 3d shape generation via canonical mapping," _arXiv preprint arXiv:2204.01955_, 2022.\n' +
      '* [38] C. Nash, Y. Ganin, S. A. Eslami, and P. Battaglia, "Polygen: An autoregressive generative model of 3d meshes," in _International Conference on Machine Learning_. PMLR, 2020, pp. 7220-7229.\n' +
      '* [39] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-resolution image synthesis with latent diffusion models," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 10 684-10 695.\n' +
      '* [40] J. Ho, A. Jain, and P. Abbeel, "Denoising diffusion probabilistic models," 2020.\n' +
      '* [41] J. Song, C. Meng, and S. Ermon, "Denoising diffusion implicit models," _arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* [42] P. Dhariwal and A\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '## 부록 A 모델 아키텍처\n' +
      '\n' +
      '제안하는 프레임워크 Argus3D는 3차원 형상 생성을 위한 2단계 과정으로 구성된다. 첫 번째 단계는 자동 인코더 구조로 입력 3D 모양에 대한 이산 표현을 학습하는 것을 목표로 한다. 그리고 두 번째 단계는 이산 표현의 결합 분포를 학습하기 위해 변압기 구조를 도입한다. 아래에서 우리는 이 두 구조의 세부 사항에 대해 자세히 설명할 것이다.\n' +
      '\n' +
      '**Auto-encoder.** 도의 왼쪽에 도시된 바와 같이. 도 23에 도시된 바와 같이, 오토 인코더는 입력 포인트 클라우드를 \\(\\mathcal{P}\\in\\mathbb{R}^{n\\times 3}\\)으로 하고 \\(n\\)은 포인트 수를 의미하며, 예측된 3D 메쉬 \\(\\mathcal{M}\\)을 출력한다. 보다 구체적으로, 인코더는 포인트 클라우드를 로컬 풀링(local pooling)을 갖는 PointNet[28]에 공급함으로써, 치수(\\(\\mathbb{R}^{n\\times 32}\\)를 갖는 포인트 특징을 생성한다. 그런 다음, 3개의 축-정렬 직교 평면 상에 \\(256\\)의 해상도로 포인트를 투영한다. 동일한 공간 격자 셀에 속하는 점들의 특징은 평균 연산을 통해 집계되므로, 입력 점 구름은 체적 특징 대신 삼평면 특징으로 표현된다. 표현을 더 향상시키기 위해, 우리는 세 개의 특징 평면을 연결하고 세 개의 컨볼루션 레이어와 결합한다. 다음으로, 4개의 합성곱 레이어를 사용하여 특징 해상도를 3회 다운 샘플링할 뿐만 아니라 3차원 공간에서 각 공간 그리드의 위치 매핑을 고도로 인코딩하고 추상화한다. 따라서, 출력은 특징 벡터로서 직렬화될 다루기 쉬운 순서를 갖는다. 평탄화된 출력들에 대해 벡터 양자화를 수행하기 전에, 우리는 특징 차원을 \\(256\\)에서 \\(4\\)으로 스퀴징함으로써 저차원 코드북 룩업의 전략을 활용하기 위해 [97]을 따른다. 결과적으로, 임의의 3D 형상은 코드북에서 가장 가까운 엔트리들의 인덱스들인 컴팩트 양자화된 벡터로 표현될 수 있다.\n' +
      '\n' +
      '디코더는 2D U-Net 모듈 2개와 대칭 업샘플 블록 1개로 구성된다. 양자화된 벡터를 재구성하고 특징 차원을 \\(4\\)에서 \\(256\\)으로 압축 해제한 후, 2D U-Net 모듈을 적용하여 각 공간 격자 특징을 전역적 지식으로 보완한다. 이어서, 특징 해상도를 다시 \\(256\\)으로 업샘플링하기 위해 다운샘플 블록과 동일한 수의 2D 컨볼루션 레이어가 추가된다. 대칭 컨볼루션 레이어는 트리 플래너 피쳐로 추가 분리합니다. 각 평면에서 공간 그리드 간의 평활도를 향상시키기 위해 다른 공유 2D U-Net 모듈을 사용하여 3면 특징을 별도로 처리한다. 2D U-Net의 구조는 \\(3\\)[34]와 정렬되어 있다. 마지막으로, 각 질의 위치의 점유 확률을 예측하기 위해 암시적 함수로서 \\(5\\) 레이어를 갖는 완전 연결 잔차 블록의 스택을 구축한다.\n' +
      '\n' +
      '**트랜스포머.** 입력 3D 모양에 대해 다루기 쉬운 순서를 가진 컴팩트한 이산 표현으로 인해 이산 코드 간의 결합 분포를 학습하기 위해 특별히 설계된 모듈 없이 바닐라 디코더 전용 변압기를 채택한다. 변압기는 1개의 다중헤드 자기집중 계층과 1개의 피드포워드 네트워크를 갖는 \\(T\\) 디코더 계층으로 구성된다. 디코더층은 [15]와 동일한 구조를 가지며, 도의 우측에 도시되어 있다. 23. 구체적으로, 이산 벡터의 첫 번째 인덱스를 예측하기 위해 학습 가능한 시퀀스 시작 토큰([SOS] 토큰)을 사용하고, 이전 인덱스와 함께 다음 인덱스를 자동 회귀적으로 예측한다. 예를 들어, 첫 번째 \\(t\\) 인덱스들을 하나의 [SOS] 토큰과 함께 포함하는 입력이 주어지면, 우리는 먼저 임베딩 레이어를 사용하여 그것들을 특징으로 인코딩한다. 그런 다음 여러 변압층으로 공급합니다. 위치 임베딩은 또한 위치 정보를 제공하기 위해 추가된다. 마지막 변압기 층의 끝에서, 우리는 각 토큰의 _logit_를 예측하기 위해 두 개의 완전히 연결된 층을 사용한다. 그러나 다음 (_t_+1) 번째 인덱스에 대해 의미 있는 범주형 분포인 마지막 인덱스만 유지합니다.\n' +
      '\n' +
      '**구현 상세.** 표 XII는 자동 인코더 및 변압기 구조 모두에 대한 모든 파라미터 설정을 요약한다. 달리 명시되지 않는 한 모든 실험에 기본값으로 적용한다. 각 척도의 Argus3D에 대한 파라미터 설정은 Tab. I. 코드북의 엔트리 수 \\(m\\)는 Argus3D-Base, -Lager 및 -Huge에 대해 각각 4096, 8192 및 8192이다. 정보 유출을 방지하기 위해 모든 멀티 헤드 셀프 어텐션 계층에서 더 낮은 삼각형 마스크 매트릭스가 사용되며, 즉 현재 인덱스의 예측은 이전의 알려진 인덱스들과만 관련된다. 다양한 컨디셔닝 입력에 대해 가장 일반적인 인코딩 방법을 채택한다. 예를 들어, 학습 가능한 임베딩 레이어를 사용하여 각 카테고리의 특징 \\(\\in\\mathbb{R}^{1\\times d}\\)을 구한다. 부분 포인트 클라우드가 주어졌을 때, 제안된 자동 인코더는 이산 표현으로 인코딩하며, 이 이산 표현들은 다른 임베딩 계층에 공급되어 \\(d\\) 차원의 특징을 얻는다. 사전 학습된 CLIP 모델을 사용하여 이미지 또는 텍스트에 대한 특징(\\in\\mathbb{R}^{1\\times 512}\\)을 추출하고, 또한 하나의 완전 연결 레이어를 사용하여 차원을 \\(512\\)에서 \\(d\\)으로 증가시킨다. 인코딩된 모든 컨디셔닝 입력들은 생성을 안내하기 위해 연접을 통해 [SOS] 토큰으로 간단히 프리펜딩된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Layer Name & Notes & Input Size \\\\ \\hline \\hline\n' +
      '(32\\times 32\\times 256\\times 256\\times & k1s1p0 & \\(32\\times 32\\times 256\\times & k1s1p0 & \\(32\\times 32\\times 256\\times & k1s1p0 & \\(128\\times 256\\times & k2s2p0 & \\(128\\times 256\\times & k2s2p0 & \\(128\\times) ) )\n' +
      '2D U-Net & & \\(32\\times 32\\times 256\\) & k3s1p1 & \\(64\\times 64\\times 128\\times & k3s1p1 & \\(128\\times 128\\times 64\\) \\\\times & k1s1p0 & \\(256\\times 256\\times 96\\) \\\\times & k3s1p1 & \\(256\\times 256\\times 96\\) \\\\times & k3s1p1 & \\(256\\times 256\\times 96\\) \\\\times & k3s1p1 & \\(256\\times 256\\times 96\\) \\\\times & k3s1p1 & \\(256\\times 256\\times 96\\) \\\\times & k3s1p1 & \\(256\\times 256\\times 96\\) \\\\times & k3s1p1 & \\(256\\times 256\\times 96\\)\n' +
      '2D U-Net & & \\(256\\times 256\\times 3\\times 32\\) \\\\ \\hline \\hline\n' +
      '**Transformer** & Embedding Layer & & \\\\ Decoder Layers \\(\\times\\)\\(T\\) & & \\\\ Self-Attention & h & (K + 1 + L) \\(\\times d\\) \\\\ Feed-Forward & m4\\(d\\) & (K + 1 + L) \\(\\times d\\) \\\\ Head Layer & & \\\\ LinearLayer & & L \\(\\times d\\) \\\\ LinearLayer & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XII: The detailed architecture of our framework. ‘k’, ‘s’ and ‘p’ denote kernel size, stride and padding, respectively, in the convolution layer. ‘h’ means the number of heads in multi-head self-attention layer, and ‘\\(T\\)’ is the number of layers. The feature dimension \\(d\\) in the transformer varies for different scales. \\(m\\) stands for the dimension of the middle layer in the feed-forward network. ‘K’ and ‘L’ are the sequence length of conditioning inputs and discrete representation, and ‘1’ indicates the length of [SOS] token.\n' +
      '\n' +
      '## 부록 B 훈련 및 시험 절차\n' +
      '\n' +
      '**훈련:** 모든 모델은 최대 8개의 A100 GPU에서 훈련됩니다. 첫 번째 단계에서는 \\(n=30,000\\)을 입력으로 하는 조밀한 포인트 클라우드를 취하여 ShapeNet(또는 ObiayverseMix) 데이터 셋에서 자동 인코더를 학습하여 총 600k(또는 1300k) 반복을 수행한다. 학습률은 1e-4로 설정되며, 배치 크기는 \\(16\\)이다. 일단 훈련되면 모든 세대 작업에 공유됩니다. 두 번째 단계에서는 기본 모델은 1e-4, 크고 거대한 모델은 1e-5의 학습률을 채택한다. 학습률 감쇠는 Argus3D-H의 학습을 위해 수동으로 조정하며, 손실이 수렴에 가까워지면 각각 3e-6과 1e-6으로 설정한다. 매개 변수 수의 증가로 인해 서로 다른 스케일의 세 가지 모델에 대한 배치 크기는 각각 8, 3 및 1로 설정된다. 단일 GPU 카드에서 기본 모델의 경우 약 600k 반복, 8개의 GPU 카드에서 거대한 모델의 경우 3500k 반복이 소요된다.\n' +
      '\n' +
      '**테스트:** 추론 중에 먼저 잘 훈련된 변압기를 사용하여 컨디셔닝 입력이 있거나 없는 이산 인덱스 시퀀스를 예측한다. 각 지수에 대해 예측된 확률에 따라 다항 분포로 샘플링하며, 여기서 가장 높은 신뢰도를 가진 상위\\(k\\) 지수만 샘플링을 위해 유지된다. 우리는 시퀀스의 모든 요소가 완료될 때까지 다음 인덱스를 점진적으로 샘플링한다. 그런 다음, 예측된 인덱스 시퀀스를 디코더에 공급하여 삼평면 특징을 얻는다. 그 후, 3면 특징으로부터 해상도 \\(128^{3}\\)의 격자상의 각 점들의 특징을 보간하고, 해당 점유 확률을 질의하기 위해 암시적 함수를 채택한다. 마지막으로 3차원 형상의 등표면을 Marching Cubes[38]를 통해 \\(0.2\\)의 임계값으로 추출한다.\n' +
      '\n' +
      '** 추론 속도:** 기본 모델 Argus3D-B의 경우 단일 모양을 샘플링하는 데 걸리는 벽 시간은 대략 14초이며 3분 안에 32개의 모양을 병렬로 생성할 수도 있다. Argus3D-H의 경우, 더 많은 파라미터 및 레이어의 계산 증가로 인해 단일 형상을 샘플링하는 데 필요한 추론 속도는 약 50초이다.\n' +
      '\n' +
      '## 부록 C 시각화\n' +
      '\n' +
      '우리는 그림의 고해상도 시각화를 첨부합니다. 더 나은 보기를 위해 7, 8, 9 및 15입니다.\n' +
      '\n' +
      '도. 23: Argus3D의 구조로서, 첫 번째 단계(좌측)의 오토 인코더와 두 번째 단계(우측)의 트랜스포머로 구성된다.\n' +
      '\n' +
      '그림 24: 무조건 생성의 질적 결과.\n' +
      '\n' +
      '그림 25: 클래스 가이드 생성의 질적 결과.\n' +
      '\n' +
      '그림 27: DALL-E-2 생성 이미지를 기반으로 한 이미지 가이드 생성. Argus3D는 보이지 않는 이미지에서 광범위한 모양을 생성할 수 있다. 이러한 모양은 DALL-E 2의 텍스트 프롬프트를 활용하는 텍스처 모델에 의해 생성된 텍스처로 더욱 향상될 수 있으며, 다양한 텍스트 프롬프트의 사용은 새롭고 독특한 텍스처의 생성을 가능하게 한다.\n' +
      '\n' +
      '그림 26: 부분 포인트 완성의 질적 결과.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>