<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Pushing Auto-regressive Models for 3D Shape Generation at Capacity and Scalability\n' +
      '\n' +
      'Xuelin Qian\\({}^{*}\\), Yu Wang\\({}^{*}\\), Simian Luo, Yinda Zhang, Ying Tai, Zhenyu Zhang, Chengjie Wang Xiangyang Xue, Bo Zhao, Tiejun Huang, Yunsheng Wu, Yanwei Fu\n' +
      '\n' +
      '\\({}^{*}\\) indicates equal contributions.Xuelin Qian, Yu Wang and Yanwei Fu are with the School of Data Science, and Fudan ISTBI--ZJNU Algorithm Centre for Brain-inspired Intelligence, Fudan University, China. Email: {xqian, yu,w13, yu, weyli@fudan.edu.cn, Xuelin Qian and Yi Wang have equal contributions.Simian Luo is with the Artificial Intelligence at IIIS (Institute for Interdisciplinary Information Sciences), Ysinghua University, China. Email: hqanz2@mails.tsinghua.edu.cn.Yinda Zhang is with Google. Email: shangyinda@gmail.com.Ting Tai and Zhenyu Zhang are with the School of Intelligence Science and Technology, Nanjing University (Suzhou Campus), China. Email: yingjian@nih.edu.cn, zhangjieie@fudan.com.Chengjie Wang and Yhusheng Wu are with Tencent Youtu llab, Shenzhen, China. Email: {jasoncywang, sinomxu}@tencent.com.Bo Zhao and Tiejun Huang are with BAAT (Beijing Academy of Artificial Intelligence), Email: zhqhoo@baat.ac.cn, dijhuang@pku.edu.cn.Xiangyang Xue is with the School of Computer Science, and Shanghai Key Lab of Intelligent Information Processing, Fudan University, China. Email: yxuz@fudan.edu.cn.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Auto-regressive models have achieved impressive results in 2D image generation by modeling joint distributions in grid space. In this paper, we extend auto-regressive models to 3D domains, and seek a stronger ability of 3D shape generation by improving auto-regressive models at capacity and scalability simultaneously. Firstly, we leverage an ensemble of publicly available 3D datasets to facilitate the training of large-scale models. It consists of a comprehensive collection of approximately \\(900,000\\) objects, with multiple properties of meshes, points, voxels, rendered images, and text captions. This diverse _labeled_ dataset, termed _Objavorse-Mix_, empowers our model to learn from a wide range of object variations. For data processing, we employ four machines with 64-core CPUs and 8 A100 GPUs each over four weeks, utilizing nearly 100TB of storage due to process complexity. And the dataset is on [https://huggingface.co/datasets/BAAI/Objavorse-MLK](https://huggingface.co/datasets/BAAI/Objavorse-MLK). However, directly applying 3D auto-regression encounters critical challenges of high computational demands on volumetric grids and ambiguous auto-regressive order along grid dimensions, resulting in inferior quality of 3D shapes. To this end, we then present a novel framework _Argus3D_ in terms of capacity. Concretely, our approach introduces discrete representation learning based on a latent vector instead of volumetric grids, which not only reduces computational costs but also preserves essential geometric details by learning the joint distributions in a more tractable order. The capacity of conditional generation can thus be realized by simply concatenating various conditioning inputs to the latent vector, such as point clouds, categories, images, and texts. In addition, thanks to the simplicity of our model architecture, we naturally scale up our approach to a larger model with an impressive \\(3.6\\) billion parameters, further enhancing the quality of versatile 3D generation. Extensive experiments on four generation tasks demonstrate that Argus3D can synthesize diverse and faithful shapes across multiple categories, achieving remarkable performance. The dataset, codes and models are available on our project website [https://argus-3d.github.io](https://argus-3d.github.io).\n' +
      '\n' +
      '3D shape generation, autoregressive model, multi-modal conditional generation, discrete representation learning.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'The 3D shape generation has garnered increasing interest in both academia and industry due to its extensive applications in robotics [1], autonomous driving [2, 3], augmented reality [4] and virtual reality [5]. Based on whether user prerequisites are provided, shape generation is typically categorized as unconditional or conditional. For a generative model to be effective, it is crucial for the synthesized shapes to be both _diverse_ and _fa faithful_ to the universal cognition of humans or given conditions. These qualities serve as the foundation for other deterministic tasks, such as shape completion, single-view reconstruction, and more. Previous approaches [6, 7, 8] typically employ an AutoEncoder (AE) to learn latent features through shape reconstruction. Then, a Generative Adversarial Network (GAN) is trained to fit the distributions of these latent features, enabling the generation of 3D shapes by sampling the latent codes learned in the AE. While these approaches yield convincing results, they still struggle with the issues of poor capacity and scalability.\n' +
      '\n' +
      'Recently, the progress of Large Language Models (LLMs) [9, 10, 11] show that more parameters enable handling complex tasks and achieving advanced performance. A natural question thereby arises: can we learn a comparable large model for versatile 3D shape generation? Intuitively, we think it demands (1) a very large-scale 3D dataset ideally with corresponding labels, and (2) a quite scalable 3D model with huge parameters. Unfortunately, the classical 3D dataset ShapeNet [12] only contains 55 categories with about 51K shapes, with relatively small datas cale, and imbalanced 3D instance distribution among categories. Thus it may easily be overfitted by large 3D models. The newly collected very large-scale dataset, Obiayverse [13], contains massive data of 800K 3D shapes. While such a dataset is a bit noisy 3D knowledge from data, and lacks of 3D annotations, Obiayverse surely provides a very important data source for the improved dataset in this paper.\n' +
      '\n' +
      'In this paper, our first contribution is to build a large-scale 3D dataset, termed _Obiayverse-Mix_. Concretely, we harness an ensemble of publicly-available 3D datasets, such as ModelNet40 [14], ShapeNet [12], Obiayverse [13]_etc_. This not only allows us to efficiently scale the data, but also requires almost no set-up cost. Despite many existing 3D datasets are existed, we notice one significant drawback is the difference in quality and diverse 3D instance distribution of categories in these 3D datasets. To this end, we systematically present the pre-processing strategiesto improve the quality of our 3D dataset. Particularly, we start by normalizing and re-scaling all shapes, such that all samples have a uniform data standard. Subsequently, we adopt the off-the-shelf tools to convert 3D shapes to a unified mesh or voxel representation, and generate multiple 3D labels, such as occupancy. Additionally, we prune shapes that do not have watertight meshes, and filter out noisy samples (_e.g._, with weird shapes, cluttered backgrounds or multiple objects), as illustrated in Fig. 0(b). Critically, we utilize four machines, each with a 64-core CPU and 8 A100 GPUs, running over four weeks, and consuming nearly 100TB of storage due to process complexity. As a result, our Objayverse-Mix comprises an impressive collection of nearly \\(900,000\\) objects with various properties of meshes, points, voxels, rendered images and captions. Examples are shown in Fig. 0(c). This diverse dataset empowers our model to learn from a wide range of object variations, promoting the visual quality of the generated 3D shapes.\n' +
      '\n' +
      'With such a cleaned dataset, we turn to the recent hardware lottery structure, _i.e._, Auto-Regressive (AR) models, for pursuing the capable 3D models. AR models have shown remarkable performance in generating 2D images [15, 16, 17] and 3D shape [18, 19]. Instead of learning a continuous latent space, these models leverage discrete representation learning to encode each 2D/3D input into grid-based discrete codes. Subsequently, a transformer-based network is employed to jointly model the distribution of all codes, which essentially reflects the underlying prior of objects, facilitating high-quality generation and tractable training. However, directly applying AR models to 3D still suffers from two limitations. First, as the number of discrete codes increases exponentially (from squared to cubed), the computational burden of the transformer grows dramatically, making it difficult to be converged. Second, discrete codes in the grid space are highly coupled. It is ambiguous to simply flatten them for auto-regression, _e.g._, with a top-down row-major order. This may lead to poor quality or even collapse of generated shapes (see Sec. 4.1 for more discussion).\n' +
      '\n' +
      'Thus our second contribution is to propose an improved auto-regressive model in terms of _capacity_, to enhance the efficient learning of 3D shape generation. Our key idea is to apply discrete representation learning in a one-dimensional space, as opposed to a 3D volumetric space. Specifically, we first project volumetric grids encoded from 3D shapes onto three axis-aligned orthogonal planes. This process significantly reduces the computational costs from cubed to squared while maintaining the essential information about the input geometry. Next, we present a coupling network to further encode three planes into a compact and tractable latent space, on which discrete representation learning is performed. Our design is straightforward and effective, simply addressing the aforementioned limitations through two projections. Thus, a vanilla decoder-only transformer can be attached to model the joint distributions of codes from the latent spaces. We are further capable of switching freely between unconditional and conditional generation by concatenating various conditioning inputs to the latent codes, such as point clouds, categories, images and texts. Figure 2 illustrates the capacity of our improved model to generate diverse and accurate shapes across multiple categories, both with and without the given conditions on the top-left corner. Critically, inspired by the success of GPT-3 [20], we naturally scale up transformers to further strengthen our auto-regressive architecture. It is implemented by increasing the number of layers and feature dimensions. Consequently, our endeavor establishes a novel 3D model with remarkable 3.6B parameters, which further enhances the quality of versatile 3D generation as validated in Sec. 5.7.\n' +
      '\n' +
      'Overall, we push auto-regressive models for 3D shape generation at both capacity and scale. To this end, we propose _Argus3D_, a novel framework that not only eliminates the aforementioned limitations of applying auto-regressive models to the field of 3D generation, but also scales up the model to an unprecedented magnitude of \\(3.6\\) billion parameters, supporting versatile 3D shape generation under multi-modal conditions.\n' +
      '\n' +
      '**Contributions.** We summarize the key contributions as follows, (1) We propose a novel auto-regressive framework, termed _Argus3D_, for 3D shape generation. Our Argus3D enjoys the advantages of switching between unconditional and conditional generation for a variety of conditioning inputs, including point clouds, categories, images, and texts. (2) We introduce an improved discrete representation learning, which builds joint distributions for a latent vector instead of volumetric grids or triple planes. It develops auto-regressive models for 3D shape generation in terms of capacity, by tackling two limitations of high computational demands on 3D grids and ambiguous order for auto-regression. (3) We further promote the generation ability of our Argus3D in terms of scalability, by expanding the model parameters and dataset size. To the best of our knowledge, Argus3D is the largest 3D gener\n' +
      '\n' +
      'Fig. 1: (a) We have combined five public 3D shape datasets, amassing a total of approximately 900,000 diverse shapes. (b) We manually filter out some noisy shapes, such as irregular shapes, complex scenes, non-watertight meshes and discrete shapes. (c) Our Objayverse-Mix dataset includes meshes, point clouds, occupancies, rendered images, and text captions, showcasing its multi-modal properties.\n' +
      '\n' +
      ' ation model, which contains \\(3.6\\) billion parameters and is trained on approximately \\(900,000\\) 3D shapes across various categories. Our collected dataset, named _Obiayerse-Mix_, assembles a series of publicly-available 3D datasets and includes multiple properties after pre-processing, hoping to encourage more researches and advance the field of 3D shape generation. (4) Extensive experiments are conducted on four tasks to demonstrate that our Argus3D can generate more faithful and diverse shapes, achieving remarkable performance for unconditional and conditional shape generation.\n' +
      '\n' +
      '**Extensions.** A preliminary version of this work was published as ImAM [21]. In this paper, we highlight that our Argus3D has made several significant improvements over. More concretely, (1) We strengthen the generation ability of our auto-regressive model by expanding its scale. We systematically design and increase the number of transformer decoder layers and feature dimensions, in order to establish the large-scale 3D shape generation model with \\(3.6\\) billion parameters. (2) To facilitate the training of large models, we expand the scale of datasets and provide a large-scale 3D shape dataset with approximately \\(900,000\\) objects, which is built by assembling multiple publicly-available 3D repositories. We filter and pre-process the data to endow each reliable 3D shape with rich attributes, such as meshes, point clouds, voxels and rendered images, to promote the research of 3D generation. (3) We evaluate our proposed method with more distance metrics, including Chamfer distance and Earth mover\'s distance, further demonstrating the superiority of our improved Argus3D in 3D shape generation. (4) We conduct extensive ablation studies to demonstrate the effectiveness of our proposed Argus3D, providing an in-depth analysis of our improvements with respect to the capacity and scale. (5) We achieve new state-of-the-art performance on multiple 3D shape generation tasks, synthesizing more diverse and faithful shapes across multiple categories.\n' +
      '\n' +
      '## 2 Related work\n' +
      '\n' +
      '**3D Generative Models**. As an extremely challenging task, most previous efforts are made by using voxel, point clouds, and implicit representations: (1) Standard voxel grids can be easily processed by 3D convolution for learning-based 3D tasks [22, 23, 24, 25]. However, restricted by its cubic space complexity, voxel representation can not scale to a high resolution, usually limited to \\(64^{3}\\). Even with efficient data structures like octrees or multi-scale representations [26, 27, 28, 29], these representations still face limitations in quality. (2) Point clouds, extracted from shape surfaces, offer an alternative 3D representation [30, 31, 32, 33]. They are memory-efficient and not restricted by resolution, unlike voxel representations. However, point clouds cannot represent topological relations in 3D space, and recovering shape surfaces from point clouds is nontrivial. (3) Recently, implicit representations have gained attention for their simplicity in representing 3D shapes [8, 34, 35, 36]. They work by predicting the signed distance [8, 36] or occupancy label [34, 37] of a given point, allowing for easy surface recovery using Marching cubes [38] methods. Subsequent works [6, 8, 34, 37, 39] focus on designing implicit function representation with global or local shape priors. To sum up, various 3D representations have led to diverse shape generative models, with notable examples including 3DGAN [25], PC-GAN [40], IM-GAN [6], and GBIF [7]. However, most current generative methods are task-specific.\n' +
      '\n' +
      'Fig. 2: We propose an improved auto-regressive model to learn versatile 3D shape generation. Our approach can either generate diverse and faithful shapes with multiple categories via an unconditional way (one column to the left), or can be adapted for conditional generation by incorporating various conditioning inputs given on the left-top (three columns to the right).\n' +
      '\n' +
      'And it is difficult to be directly applied to different generative tasks (_e.g._, shape completion). Essentially, they rely on GANs for the generation step, suffering from known drawbacks such as mode collapse and training instability. In contrast, we propose an improved auto-regressive model for 3D shape generation that can synthesize high-quality and diverse shapes while being easily generalized to other multi-modal conditions.\n' +
      '\n' +
      '**3D Auto-regressive Models.** Auto-regressive models are probabilistic generative approaches that have tractable probability density. Using the probability chain rule, the likelihood of a given sample especially for high dimensional data, can be factorized into a series of products of conditional probability. In contrast, GANs do not have such a tractable probability density. Recently, AR models achieve remarkable progress in 2D image generation [15, 41, 42], to a less extent 3D tasks [43, 44, 45]. Most of 3D AR models are struggling with generating high-quality shapes due to the challenges of representation learning with more points or faces. Particularly, we notice two recent great works [18, 19] that share similar insights into utilizing AR models for 3D tasks. Yan et al. [19] introduces a sparse representation to only quantize non-empty grids in 3D space, but still follows a monotonic row-major order. Mittal et al. [18] presents a non-sequential design to break the orders, but performs on all volumetric grids. However, both of them address only one of the above limitations, and burden the structure design and training of transformer. In contrast, our Argus3D applies discrete representation learning in a latent vector instead of volumetric grids. Such a representation offers plenty of advantages, including shorter lengths of discrete codes, tractable orders from auto-regression, fast convergence, and also preserving essential 3D information. Moreover, benefiting from the simplicity of transformers, we can freely switch from unconditional generation to conditional generation by concatenating various conditions.\n' +
      '\n' +
      '**3D Diffusion Models.** Recently, Denoising Diffusion Probabilistic Models (DDPMs) [46, 47, 48, 49] have gained considerable attention for their stability and diverse image generation capabilities. Significant progress has also been made in 3D generation through point clouds based methods or 2D lift 3D [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]. DDPMs learn the data distribution from a Gaussian distribution through a gradual denoising process, often based on the U-Net [60] architecture. However, this approach limits the resolution and extends the training cycles, thus hindering its performance in 3D generation. In contrast, transformer-based models offer a high degree of scalability, which has been shown to enhance performance across various downstream tasks by scaling up the model size [61, 62, 63, 64]. In this paper, we posit that this scalability can also be advantageous in the domain of 3D shape generation, enabling more effective and efficient modeling of complex shapes.\n' +
      '\n' +
      '## 3 Objaverse-Mix: Scaling Up Dataset with Assembled 3D Shapes\n' +
      '\n' +
      'We draw inspiration from the significant progress made in the realm of 2D vision, where the efficacy of large models supported by extensive training data has been convincingly demonstrated [65, 66, 67]. However, it is nontrivial to build a large-scale 3D dataset. On one hand, for the datasets that are commonly used today, ShapeNet [12] or ModelNet40 [14] only has tens of thousands 3D shapes, whose scale is too small to train a very large model, facing the risk of overfitting. On the other hand, the process of labeling 3D objects involves considerably higher costs and complexities, making it one of the inherent challenges associated with constructing large-scale 3D datasets. Recently, Deitke _et al._[13] present a massive dataset Objaverse with more than 800K 3D models, which are collected from an online 3D marketplace. Though Objaverse improves 3D repositories in terms of scale, it is still very noisy and does not have 3D annotations, preventing the model from directly learning 3D knowledge from the data. The recent expansion of the Objaverse dataset has culminated in Objaverse-XL [68], now featuring 10 million shapes. The sheer volume of data scale is very impressive, while we notice that it still does not have very detailed 3D annotations and multi-modal properties. Additionally, we also notice that some sourcing data from other sites such as GitHub and Thingiverse are still noisy to some extent.\n' +
      '\n' +
      'In this paper, we find a simple and different way to build a large-scale 3D dataset, providing sufficient data for Argus3D model training. Concretely, we leverage an ensemble of five publicly-available 3D repositories, resulting in an impressive collection of approximately 900K 3D shapes. This not only allows us to efficiently scale the data, but also requires almost no set-up cost. We thus name the dataset _Objaverse-Mix_, as we acknowledge that a large portion of data is inherited and cleaned from Objaverse dataset. We provide the details of each 3D repository below, as shown in Fig. 0(a).\n' +
      '\n' +
      '**ModelNet40**[14]: This dataset stands as a valuable resource, containing an impressive collection of over 12,300 CAD models spanning 40 object categories. For our research objectives, we strategically select 9,843 models from this dataset to serve as the foundation for training our model.\n' +
      '\n' +
      '**ShapeNet**[12]: Recognized for its comprehensiveness and scope, ShapeNet provides an extensive and diverse collection of 3D models. Encompassing 55 common object categories and boasting over 51,300 unique 3D models, this dataset offers rich and varied representations that greatly enhance the training process of our model. The selection of ShapeNet bolsters the ability of our model to capture a wide range of object variations.\n' +
      '\n' +
      '**Pix3D**[69]: With its large-scale nature, Pix3D emerges as a valuable resource for our research. Comprising 10,069 images and 395 shapes, this dataset exhibits significant variations, allowing our model to learn from a diverse range of object appearances and characteristics. By incorporating Pix3D into our training dataset, we equip our model with the capability to generate visually impressive and realistic 3D shapes.\n' +
      '\n' +
      '**3D-Future**[70]: As a specialized furniture dataset, 3D-Future provides a unique and detailed collection of 16,563 distinct 3D instances. By including this dataset in our training pipeline, we ensure that our model gains insights into the intricacies and nuances of furniture designs. This enables our model to generate high-quality and realistic 3D shapes in the furniture domain.\n' +
      '\n' +
      '**Objaverse**[13]: A recent addition to the realm of 3D shape datasets, Objaverse captivates us with its vastness and richness. Sourced from Sketchfab, this immensely large dataset comprises over 800,000(and growing) 3D models, offering a wide array of object categories and variations. This extensive dataset provides our model with a wealth of diverse and representative examples for learning and generating high-quality 3D shapes.\n' +
      '\n' +
      'Furthermore, to ensure the quality of 3D objects, we systematically utilize a series of pre-processing strategies. We first follow previous efforts [24, 71] to normalize and re-scale all shapes to a uniform standard. Then, we employ the established techniques [72] to render each 3D object, create depth maps, generate wa tertight meshes, and sample points from these meshes. This step helps us effectively to prune shapes that do not have watertight meshes. Next, we manually filter out noisy samples (_e.g._, weird or supernatural objects) or difficult examples (_e.g._, 3D scenes with lawns), which are empirically found to be detrimental to model convergence. To facilitate the learning of multi-modal generation, we also utilize the off-the-shelf tools [73, 74] to render multi-view images and produce text descriptions. For each 3D object, we use Blender to render 12 views images in 512x512 resolution. Contrary to the approach employed by [75], which utilizes random views for rendering, we opted for fixed perspectives to render images. This includes the front, side, top, back, and bottom views, along with three equidistant points at polar angles of 45\\({}^{\\circ}\\) and 135\\({}^{\\circ}\\). This method of rendering helps avoid low-quality images and ensures consistency, particularly with the front-view renderings. The front view is particularly useful as it generally contains the most information about a shape\'s appearance. In addition, using fixed perspectives, especially the front view, allows us to generate higher-quality text captions. Random views can sometimes lead to inconsistent captions due to varying information presented in each view. To generate text captions for each 3D shape, we employ BLIP2, using the images rendered from the front view. This approach ensures that our text captions are coherent and accurately reflect the content of the 3D shapes. As a result, we construct a comprehensive mixed dataset Obojarese-Mix, as illustrated in Fig. 1, including 900K objects with multiple representations of meshes, point clouds and voxels, and various labels of occupancy, rendered images and captions. To facilitate data assembly and pre-processing, we leverage four machines, each featuring a 64-core CPU and 8 A100 GPUs, for more than four weeks. The process necessitates nearly 100TB of storage owing to its intricate nature.\n' +
      '\n' +
      '## 4 Argus3D: Improved Auto-regressive Models for 3D Shape Generation\n' +
      '\n' +
      'Figure 4 illustrates the schematic of our proposed framework for 3D shape generation with the necessary symbol definitions and necessary preliminaries in Sec. 4.1. From the perspective of capacity, it consists of a two-stage training procedure. We first represent the input as a composition of learned discrete codes as in Sec. 4.2; then utilize a transformer model to learn their interrelations in Sec. 4.3. From the perspective of scalability, Section 4.4 takes advantage of our improved representation, and thus scales up both transformer layers and data for stronger generation ability.\n' +
      '\n' +
      '### _Preliminary_\n' +
      '\n' +
      '**Ambiguity.** We highlight the problem of ambiguity existed in our task. Formally, _\'ambiguity\' appears in the order of a series of conditional probabilities, which affects the difficulty of likelihood learning, leading to approximation error of the joint distribution._ Critically, auto-regressive models require sequential outputs, auto-regressively predicting the next code conditioned on all previous ones. Thereby, the order of the flattened sequence determines the order of conditional probabilities. Although some methods such as position embedding [76] can be aware of positions of codes, it cannot eliminate approximation error caused by the condition order. Notably, such the \'ambiguity\' phenomenon is also discussed in [15] and visualized by Fig. 47 in [15], where the loss curves in Fig. 47 highlight the differences in difficulty for likelihood learning across various orders. Figure 3 illustrates how the flattening order affects the way of auto-regressive generation. For grid-based representation, it is ambiguous if the flattening order along axes is \\(y\\)-\\(x\\)-\\(z\\), \\(x\\)-\\(z\\)-\\(y\\) or other combinations.\n' +
      '\n' +
      '**Discrete Representation.** Given input point clouds \\(P\\in\\mathbb{R}^{n\\times 3}\\) where \\(n\\) means the number of points, an encoder is adopted to extract features for each point cloud and then perform voxelization to get features of regular volumetric grids \\(f^{v}\\in\\mathbb{R}^{r\\times r\\times r\\times c}\\), where \\(r\\) denote the resolution of voxels and \\(c\\) is the feature dimension. To learn discrete representation for each 3D shape, a codebook \\(\\mathbf{q}\\in\\mathbb{R}^{m\\times c}\\) is thus introduced whose entry is a learned code describing a particular type of local-part shape in a grid. Formally, for each grid \\(\\left\\{f^{v}_{(h,l,w)}\\right\\}_{h,l,w=1}^{r}\\), vector quantization \\(\\mathcal{Q}\\left(\\cdot\\right)\\) is performed by replacing it with the closest entry in codebooks [15],\n' +
      '\n' +
      '\\[\\mathbf{z}^{v}=\\mathcal{Q}\\left(f^{v}\\right):=\\arg\\min_{\\mathbf{e}_{i}\\in \\mathbf{q}}||f^{v}_{(h,l,w)}-\\mathbf{e}_{i}|| \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\mathbf{e}_{i}\\in\\mathbf{q}\\) represents the \\(i\\)-th entry in the codebook. Thus, learning the correlations between entries in the second stage can explore the underlying priors for shape generation. Nonetheless, auto-regressive generation [15, 19] requires sequential outputs, facing two limitations. _First, the resolution of \\(\\mathbf{z}^{v}\\) matters the quality of synthesized shapes_. Too small \\(r\\) may lack the capacity to represent intricate and detailed geometries. A large value of \\(r\\) can learn a specific code for each local grid. This will inevitably increase the computational complexity since the number of required codes explodes as \\(r\\) grows. _Second, the order of \\(\\mathbf{z}^{v}\\) affects the generation quality_. Each grid is highly coupled with neighbors. Simply flattening e.g. along x-y-z axes may cause the problem of ambiguity, leading to sub-optimal generation quality.\n' +
      '\n' +
      '### _Improved Discrete Representation Learning_\n' +
      '\n' +
      'One possible solution to solve the first limitation is applying vector quantization in spatial grids instead of volumetric grids inspired by [77]. Specifically, after obtaining point cloud features \\(f^{p}\\in\\mathbb{R}^{n\\times c}\\), we first project points onto three axis-aligned orthogonal planes. Features of points falling into the same plane grid are aggregated via summation, resulting in three feature maps for the three planes \\(\\{f^{xy},f^{yz},f^{xz}\\}\\in\\mathbb{R}^{l\\times w\\times c}\\). Next, the vector quantization is applied to the three planes separately. The primary advantage of tri-planar representation is efficient and compact. It dramatically reduces the number of grids from \\(\\mathcal{O}(r^{3})\\) to \\(\\mathcal{O}(r^{2})\\) while preserving essential 3D information. However, it still suffers\n' +
      '\n' +
      'Fig. 3: Illustration of auto-regressive generation for grid-based representation. Here, we show three different flattening orders as examples. Best viewed in color.\n' +
      '\n' +
      'from the order ambiguity. Even worse, it involves the flattening order of three planes and the order of entries of each plane.\n' +
      '\n' +
      'To this end, we further introduce a projection by learning a higher latent space for features of the three planes. This is simply achieved by first concatenating three planes with arbitrary order and then feeding them into a coupling network. Finally, the output is flattened as a projected latent vector, formulated as,\n' +
      '\n' +
      '\\[f=\\tau\\left(\\mathcal{G}\\left([f^{xy};\\ f^{yz};\\ f^{xz}]\\,;\\ \\theta\\right)\\right)\\in\\mathbb{R}^{m\\times d} \\tag{2}\\]\n' +
      '\n' +
      'where \\([\\cdot;\\cdot]\\) means the concatenation operation; \\(\\mathcal{G}\\left(\\,\\cdot;\\theta\\right)\\) is a series of convolution layers with parameters \\(\\theta\\); \\(\\tau\\left(\\cdot\\right)\\) is the flatten operation with row-major order; \\(m\\) and \\(d\\) indicate the length of the latent vector and feature dimension. By applying discrete representation learning in the latent vector, we can describe each 3D shape with \\(\\mathbf{z}=\\mathcal{Q}\\left(f\\right)\\), where \\(\\mathcal{Q}\\left(\\cdot\\right)\\) is vector quantization in Eq. 1.\n' +
      '\n' +
      '**Remark.** Different from existing works [18, 19] that rely on structure design and training strategies in the second stage to address the problem of ambiguous order, we tackle it by learning the coupling relationship of spatial grids in the first stage with the help of the second projection. By stacking convolution layers, we increase the receptive field of each element in the latent vector. Additionally, since the features of each spatial grid on the three planes are fused and highly encoded, each element does not have a definite position mapping in 3D space, which results in a tractable order for auto-regression. More in-depth discussions can be found in Sec. 5.8.\n' +
      '\n' +
      '**Training Objective.** We optimize parameters with reconstruction loss. After getting discrete representation \\(\\mathbf{z}\\) which represents indices of entries in the codebook, we retrieve the corresponding codes with indices, denoted as \\(\\mathbf{q}_{(\\mathbf{z})}\\). Subsequently, a decoder with symmetric structures of the encoder is designed to decode \\(\\mathbf{q}_{(\\mathbf{z})}\\) back to features of the three planes. Given sampling points \\(x\\in\\mathbb{R}^{3}\\), we query their features by projecting them onto each of the three feature planes and performing bilinear interpolation. Features from three planes are accumulated and fed into an implicit function to predict their occupancy values. Finally, we apply binary cross-entropy loss between the predicted values \\(y_{o}\\) and the ground-truth ones \\(\\tilde{y_{o}}\\),\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathrm{occ}}=-\\left(\\tilde{y_{o}}\\cdot\\log\\left(y_{o}\\right)+ \\left(1-\\tilde{y_{o}}\\right)\\cdot\\log\\left(1-y_{o}\\right)\\right) \\tag{3}\\]\n' +
      '\n' +
      'To further train the codebook, we encourage pulling the distance between features before and after the vector quantization. Thus, the codebook loss is derived as,\n' +
      '\n' +
      '\\[\\mathcal{L}_{code}=\\beta||\\mathrm{sg}\\left[f\\right]-\\mathbf{q}_{(\\mathbf{z})} ||_{2}^{2}+||f-\\mathrm{sg}\\left[\\mathbf{q}_{(\\mathbf{z})}\\right]||_{2}^{2} \\tag{4}\\]\n' +
      '\n' +
      'where \\(\\mathrm{sg}\\left[\\cdot\\right]\\) denotes the stop-gradient operation [78]. And we set \\(\\beta=0.4\\) by default. In sum, the overall loss for the first stage is \\(\\mathcal{L}_{rec}=\\mathcal{L}_{occ}+\\mathcal{L}_{code}\\).\n' +
      '\n' +
      '### _Learning Priors with Vanilla Transformers_\n' +
      '\n' +
      'Benefiting from a compact composition and tractable order of discrete representation, models in the second stage can absorbedly learn the correlation between discrete codes, effectively exploring priors of shape composition. We thus adopt a vanilla decoder-only transformer [15] without any specific-designed module.\n' +
      '\n' +
      '_For unconditional generation_ and given discretized indices of latent vector \\(\\mathbf{z}=\\left\\{\\mathbf{z}_{1},\\mathbf{z}_{2},\\cdots,\\mathbf{z}_{m}\\right\\}\\), we feed them into a learnable embedding layer to retrieve features with discrete indices 1. Then, the transformer with multi-head self-attention mechanism predicts the next possible index by learning the distribution of previous indices, \\(p\\left(\\mathbf{z}_{i}\\mid\\mathbf{z}_{<i}\\right)\\). This gives the joint distribution of full representation as,\n' +
      '\n' +
      'Footnote 1: We reuse the symbol of \\(\\mathbf{z}\\) after embedding for simplicity\n' +
      '\n' +
      '\\[p\\left(\\mathbf{z}\\right)=\\prod_{i=1}^{m}p\\left(\\mathbf{z}_{i}\\mid\\mathbf{z}_ {<i}\\right) \\tag{5}\\]\n' +
      '\n' +
      '_For conditional generation_, users often expect to control the generation process by providing additional conditions. Instead of designing complex modules or training strategies, we simply learn joint distribution given conditions \\(\\mathbf{c}\\) by prepending it to \\(\\mathbf{z}\\). Equation 5 is thus extended as,\n' +
      '\n' +
      '\\[p\\left(\\mathbf{z}\\right)=\\prod_{i=1}^{m}p\\left(\\mathbf{z}_{i}\\mid\\mathbf{c}, \\ \\mathbf{z}_{<i}\\right) \\tag{6}\\]\n' +
      '\n' +
      'where \\(\\mathbf{c}\\) denotes a feature vector of given conditions. The simplicity of our model gives the flexibility to learn conditions of any form. Specifically, for 3D conditions such as point clouds, we use our discrete representation learning in Sec. 4.2 to transform them into a vector. As for 1D/2D conditions such as classes and images,\n' +
      '\n' +
      'Fig. 4: Overview of our Argus3D. Given an arbitrary 3D shape, we first project encoded volumetric grids into the three axis-aligned planes, and then use a coupling network to further project them into a latent vector. Vector quantization is thus performed on it for discrete representation. Taking advantage of such a compact representation with tractable orders, vanilla transformers are adopted to auto-repressively learn shape distributions. Furthermore, we can freely switch from unconditional generation to conditional generation by concatenating various conditions, such as point clouds, categories and images.\n' +
      '\n' +
      'we either adopt pre-trained models or embedding layers to extract their features.\n' +
      '\n' +
      '**Objective.** To train second stage, we minimize negative log-likelihood of Eq. 5 or 6 as \\(\\mathcal{L}_{nll}=\\mathbb{E}_{x\\sim p(x)}\\left[-\\log p\\left(\\mathbf{z}\\right)\\right]\\), where \\(p\\left(x\\right)\\) is the distribution of real data.\n' +
      '\n' +
      '**Inference.** With both models trained on two stages, we use Eq. 5 or 6 to perform shape generation by progressively sampling the next index with top-\\(k\\) sampling strategy, until all elements in \\(\\mathbf{z}\\) are completed. Then, we feed \\(\\mathbf{q}_{(\\mathbf{z})}\\) into the decoder of the first stage, and query probabilities of occupancy values for all sampled 3D positions (_e.g._, \\(128^{3}\\)). The output shapes are extracted with Marching Cubes [38].\n' +
      '\n' +
      '### _More Parameters for Better Capacity_\n' +
      '\n' +
      'Recently, large models have shown remarkable advancements in both natural language processing and computer vision, such as GPT [20, 62], PaLM [79, 80], BLIP [74, 81] and EVA [82, 83]. They not only have the ability to handle complex multi-tasks, but more importantly, play a vital pre-training role for downstream tasks. Motivated by these successful works, we thus expect to enlarge the scale of our model, so as to simultaneously improve the capacity of 3D shape generation.\n' +
      '\n' +
      'Specifically, we increase the number of transformer layers and feature dimensions in the second stage, but retain the model parameters of the first stage for simplicity. Since we utilize a vanilla decoder-only transformer here, the scale-up development is more effective with less effort. Following the protocol of GPT-3 [20], we design three variations of our model with different scales, _i.e._, Argus3D-B(ase), Argus3D-L(arge) and Argus3D-H(uge), as demonstrated in Tab. I. Specifically, our Argus3D-H model has 32 transformer decoder layers, with more than 3.6 billion parameters. It is one of the largest models in the field of 3D shape generation. Experimental results in Sec. 5.7 demonstrate that our scale-up model can further enhance the generation quality of 3D shapes. This significant achievement in scaling up models marks a pivotal milestone, pushing the boundaries of 3D shape generation towards unprecedented advancements. In the following paragraphs, we refer Argus3D to the base model unless otherwise specified.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      'This section starts with evaluating the capacity of our Argus3D. For a fair comparison, we show extensive studies on five generation tasks, demonstrating the powerful and flexible ability of Argus3D-B. (in Sec. 5.2 \\(\\sim\\) 5.6). Next, we compare and discuss the efficacy of Argus3D at different scales (in Sec. 5.7). Lastly, we provide in-depth studies to evaluate the efficacy of our modules and show generalization to real-world data and zero-shot generation (in Sec. 5.8). For all experiments, if necessary, we sample point clouds from output meshes with Poisson Disk Sampling, or reconstruct meshes with our auto-encoder from output points, which is better than Poisson Surface Reconstruction [84]. Please refer to the Appendix for more implementation details.\n' +
      '\n' +
      '### _Evaluation Metrics_\n' +
      '\n' +
      '**Common metrics.** As a generator, the key to evaluating our proposed method is not only to measure the _fidelity_, but also to focus on the _diversity_ of the synthesized shapes. Therefore, we adopt eight metrics for different generation tasks, including Coverage (COV) [40], Minimum Matching Distance (MMD) [40], Edge Count Difference (ECD) [7] and 1-Nearest Neighbor Accuracy (1-NNA) [85], Total Mutual Difference (TMD) [86], Unidirectional Hausdorff Distance (UHD) [86], Frechet Point Cloud distance (FPD) [87] and Accuracy (Acc.) [88]. In particular, we use the Light Field Descriptor (LFD) [89] as our primary similarity distance metric for COV, MMD and ECD, as suggested by [6]. Since both FPD and Acc. metrics require a classifier to calculate, we thus train a PointNet2 with 13 categories on ShapeNet datasets, which achieves the classification accuracy of 92%.\n' +
      '\n' +
      'Footnote 2: [https://github.com/jtpls/TreeGAN/blob/master/evaluation/pointnet.py](https://github.com/jtpls/TreeGAN/blob/master/evaluation/pointnet.py)\n' +
      '\n' +
      'For shape generation task, COV and MMD measure the diversity and fidelity of the generated shapes, respectively. Both suffer from some drawbacks [85]. FPD and Acc. measure the fidelity of the generated shapes from the viewpoint of feature space and probability, respectively. On the contrary, ECD and 1-NNA measure the distribution similarity of a synthesized shape set and a ground-truth shape set in terms of both diversity and quality. Therefore, ECD and 1-NNA are two more reliable and important metrics to quantify the shape generation performance. For shape completion tasks, TMD is meant to the diversity of the generated shapes for a partial input shape, and UHD is proposed to evaluate the completion fidelity. Both metrics are specifically designed for the completion task [86].\n' +
      '\n' +
      '_Coverage with threshold._ As discussed above, Coverage [40] measures the diversity of a generated shape set. However, it does not penalize outliers since a ground truth shape is still considered as covered even if the distance to the closest generated shape is large [85]. To rule out the false positive coverage, we introduce Coverage with threshold (CovT), to count as a match between a generation and ground truth shape only if LFD [89] between them is smaller than a threshold \\(t\\). In practice, \\(t\\) could vary across different semantic categories based on the scale and complexity of the shape, and we empirically use MMD [40] as the threshold. In this paper, we set \\(t\\) as the mean MMD of all competitors.\n' +
      '\n' +
      'To evaluate the effectiveness of the improved COV in identifying correct matches, we visualize the matched pairs with the largest MMD before and after using threshold filtering. As shown on the left of Fig. 6, when there is no threshold constraining the quality of the generated shape, outliers (_e.g._, a messy shape)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Size & Dimensions & Layers & Heads & Params (M) \\\\ \\hline Base & 768 & 12 & 12 & 100 \\\\ Large & 2048 & 24 & 16 & 1239 \\\\ Huge & 3072 & 32 & 24 & 3670 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: Three designs of our Argus3D with different scales.\n' +
      '\n' +
      'Fig. 5: Illustration of scaling up our models.\n' +
      '\n' +
      'could match any possible ground-truth shape, which is clearly unreasonable. On the contrary, when the threshold is applied for filtering, as illustrated on the right of Fig. 6, the generated shape has certain similarity in texture or parts with the matched ground-truth shape, even if they have the maximum shape distance. It strongly demonstrates the validity and reliability of our improvement in measuring the diversity of generations.\n' +
      '\n' +
      '### _Unconditional Shape Generation_\n' +
      '\n' +
      '**Data.** We consider ShapeNet [90] as our main dataset for generation, following previous literature [6, 7, 43]. We use the same training split and evaluation setup from [7] for fair comparability. Five categories of car, chair, plane, rifle and table are used for testing. As ground truth, we extract mesh from voxelized models with \\(256^{3}\\) resolution in [27].\n' +
      '\n' +
      '**Baselines.** We compare Argus3D with five state-of-the-art models, including GAN-based IM-GAN [6] and GBIF [7], flow-based PointFlow [85], score-based ShapeGF [91] and diffusion-based PVD [54]. We train these methods on the same data split with the official implementation.\n' +
      '\n' +
      '**Metrics and Settings.** The size of the generated set is 5 times the size of the test set, the same as [6, 7]. Coverage with threshold (CovT), Coverage (COV) [40], Minimum Matching Distance (MMD) [40] and Edge Count Difference (ECD) [7] are adopted to evaluate the diversity, fidelity and overall quality of synthesized shapes. We also use 1-Nearest Neighbor Accuracy (1-NNA) [85] (with Chamfer Distance) to measure the distribution similarity. The number of sampled points is 2048.\n' +
      '\n' +
      '**Results Analysis.** Results are reported in Tab. II. First, Argus3D achieves state-of-the-art performance with regard to ECD and 1-NNA. It significantly demonstrates the superiority of our model over synthesizing high-quality shapes. We notice that the result of Car is not good on the metric of 1-NNA. One possible reason is that Argus3D tries to generate meshes of tires and seats inside cars, which may not be very friendly to CD. Second, our model has a clear advantage on both MMD and CovT metrics compared with all competitors, which separately indicates the outstanding fidelity and diversity of our generated shapes. Third, though GBIF achieves relatively good results on COV, it gets worse results on CovT, suggesting that most of the matched samples come from\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline \\multirow{2}{*}{Metrics} & \\multirow{2}{*}{Methods} & \\multicolumn{5}{c}{Categories} & \\multirow{2}{*}{Avg} \\\\ \\cline{3-6}  & & & \\multicolumn{1}{c}{Plane} & \\multicolumn{1}{c}{Car} & \\multicolumn{1}{c}{Chair} & \\multicolumn{1}{c}{Rifle} & \\multicolumn{1}{c}{Table} \\\\ \\hline \\multirow{6}{*}{\\(\\text{ECD}\\downarrow\\)} & IM-GAN [6] & 923 & 3172 & 658 & 371 & 418 & 1108 \\\\  & GBIF [7] & 945 & 2388 & 354 & 195 & 411 & 858 \\\\  & PointFlow [85] & 2395 & 5318 & 426 & 2708 & 3559 & 2881 \\\\  & ShapeGF [91] & 1200 & 2547 & 443 & 672 & 114 & 915 \\\\  & PVD [54] & 6661 & 7404 & 1265 & 3443 & 745 & 3904 \\\\  & _Ours_ & **236** & **842** & **27** & **65** & **31** & **240** \\\\ \\hline \\multirow{6}{*}{\\(\\text{1-NNA}\\downarrow\\)} & IM-GAN [6] & 78.18 & 89.39 & 65.83 & 69.38 & 65.31 & 73.62 \\\\  & GBIF [7] & 80.22 & 87.19 & 63.95 & 66.98 & 60.96 & 71.86 \\\\  & PointFlow [85] & 73.61 & 74.75 & 70.18 & 64.77 & 74.81 & 71.62 \\\\  & ShapeGP [91] & 74.72 & 62.81 & 59.15 & 60.65 & 55.58 & 62.58 \\\\  & PVD [54] & 81.09 & **57.37** & 62.36 & 77.32 & 43.71 & 70.49 \\\\  & _Ours_ & **59.95** & 76.58 & **57.31** & **57.28** & **54.76** & **61.17** \\\\ \\hline \\multirow{6}{*}{COV \\(\\uparrow\\)} & IM-GAN [6] & 77.01 & 65.37 & 76.38 & 73.21 & 85.71 & 75.53 \\\\  & GBIF [7] & **80.96** & **78.85** & **80.95** & **77.00** & 85.13 & **80.57** \\\\  & PointFlow [85] & 65.64 & 69.57 & 57.49 & 48.52 & 71.95 & 61.71 \\\\  & ShapeGP [91] & 76.64 & 71.85 & 79.41 & 70.67 & **57.54** & 77.22 \\\\  & PVD [54] & 58.09 & 58.64 & 68.93 & 56.12 & 76.84 & 63.72 \\\\  & _Ours_ & 79.11 & 73.25 & 80.81 & 74.26 & 84.01 & 78.29 \\\\ \\hline \\multirow{6}{*}{\\(\\text{ConvT}\\uparrow\\)} & IM-GAN [6] & 41.03 & 50.63 & 45.68 & 51.68 & 46.50 & 47.10 \\\\  & GBIF [7] & 32.38 & 57.26 & 39.77 & 50.00 & 43.68 & 43.72 \\\\  & PointFlow [85] & 35.85 & 47.76 & 28.48 & 34.81 & 30.98 & 35.57 \\\\  & ShapeGP [91] & 40.17 & 53.63 & 43.69 & 51.05 & **48.50** & 47.41 \\\\  & PVD [54] & 12.11 & 43.36 & 38.82 & 33.33 & 43.68 & 34.26 \\\\  & _Ours_ & **45.12** & **56.64** & **49.82** & **55.27** & **43.68** & **30.98** \\\\ \\hline \\multirow{6}{*}{\\(\\text{MMD}\\downarrow\\)} & IM-GAN [6] & 3418 & 1290 & 2881 & 3691 & 2505 & 2757 \\\\  & GBIF [7] & 3754 & 1333 & 3015 & 3865 & 2584 & 2910 \\\\ \\cline{1-1}  & PointFlow [85] & 3675 & 1393 & 3322 & 4038 & 2936 & 3072 \\\\ \\cline{1-1}  & ShapeGP [91] & 3530 & 1307 & 2880 & 3762 & 2420 & 2780 \\\\ \\cline{1-1}  & PVD [54] & 4376 & 1432 & 3064 & 2747 & 2623 & 3154 \\\\ \\cline{1-1}  & _Ours_ & **3124** & **1213** & **2703** & **3628** & **2374** & **2608** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: Results of unconditional generation. Models are trained for each category. The best and second results are highlighted in **bold** and underlined.\n' +
      '\n' +
      'Fig. 6: We show the matched pairs with the largest MMD before and after using threshold shifting when computing the metric of _Coverage_. For each pair, the left is the generated shape and the right is the closest ground-truth shape. Best viewed zoomed in.\n' +
      '\n' +
      'Fig. 7: Qualitative results of unconditional generation.\n' +
      '\n' +
      'false positive pairs. Argus3D, on the contrary, gets the second best performance on COV, but higher than GBIF on CovT by about 6 points, showing that our generated shapes enjoy the advantage of high-quality and fewer outliers. Lastly, we visualize shapes with multiple categories in Fig. 24, further supporting the quantitative results and conclusions described above.\n' +
      '\n' +
      '### _Class-guide Generation_\n' +
      '\n' +
      'We evaluate the versatility of Argus3D on class-guide generation, which requires generating shapes given a category label. It is a basic conditional generation task. We use the same dataset and evaluation metrics as in Sec. 5.2.\n' +
      '\n' +
      '**Baselines.** We choose two recently-published works as competitors due to similar motivation. One is a two-stage generative model GBIF [7], and the other is AR method AutoSDF [18]. We simply modify the mask-condition of [7] to class-condition, and additionally add the class token to transformers for [18] which is the same as Argus3D.\n' +
      '\n' +
      '**Results Analysis.** In Tab. III, Argus3D outperforms both competitors across 5 categories by a significant margin, achieving state-of-the-art results on all metrics. Particularly, it gets advantages on the metric of 1-NNA, strongly demonstrating the versatility of Argus3D on class-guide generation. Qualitative results of 5 different categories are further illustrated in Fig. 25. As observed, the generated quality of our method is clearly better than GBIF and AutoSDF, while preserving more diversity in types and shapes.\n' +
      '\n' +
      '### _Multi-modal Partial Point Completion_\n' +
      '\n' +
      'We further verify the ability of our model in conditional generation by giving partial point clouds. Here, we advocate the multi-modal completion, since there are many possibilities of the completed shape given the partial shape. It\'s the essence of generative models, where using your imagination while being faithful to the given partial conditions.\n' +
      '\n' +
      '**Data.** We use ShapeNet dataset for testing. Two settings are considered here, (1) perspective completion [92]: randomly sampling a viewpoint and then removing the \\(25\\%\\sim 75\\%\\) furthest points from the viewpoint; (2) bottom-half completion [18]: removing all points from the top half of shapes.\n' +
      '\n' +
      '**Baselines.** Four multi-modal completion models are chosen as baselines, including one generative adversarial model cGAN [86], one diffusion model PVD [54], two AR models ShapeFormer [19] and AutoSDF [18].\n' +
      '\n' +
      '**Metrics and Settings.** We complete \\(10\\) samples for \\(100\\) randomly selected shapes of three categories, _i.e._, chair, sofa and table. Following [86], we use Total Mutual Difference (TMD) to measure the diversity. Minimum Matching Distance [40] (MMD) with Chamfer Distance and Unidirectional Hausdorff Distance (UHD) [87] are adopted to measure the faithfulness of completed shapes.\n' +
      '\n' +
      '**Results Analysis.** We first report perspective completion results in Tab. IV. Argus3D beats all baselines and achieves state-of-the-art performance. Importantly, we outperform ShapeFormer on all classes and metrics, which also utilizes an AR model with transformers to learn shape distribution. On the other hand, we compare with AutoSDF in its bottom-half completion setting. Results from Tab. V illustrate that Argus3D outperforms AutoSDF in terms of TMD and MMD. It strongly suggests the flexibility and\n' +
      '\n' +
      'Fig. 8: Qualitative results of class-guide generation.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Metrics} & \\multirow{2}{*}{Methods} & \\multicolumn{5}{c}{Categories} & \\multirow{2}{*}{AVG} \\\\ \\cline{3-8}  & & Plane & Car & Chair & Rifle & Table \\\\ \\hline \\multirow{4}{*}{COV \\(\\uparrow\\)} & GBIF [7] & 68.72 & 69.64 & 75.94 & 68.98 & 81.72 & 73.00 \\\\  & AutoSDF [18] & 70.46 & 52.77 & 63.25 & 48.10 & 72.19 & 61.35 \\\\  & _Ours_ & **81.58** & **71.58** & **83.98** & **75.74** & **85.48** & **79.67** \\\\ \\hline \\multirow{4}{*}{CovT \\(\\uparrow\\)} & GBIF [7] & 24.10 & 38.63 & 32.69 & 35.44 & 37.80 & 33.73 \\\\  & AutoSDF [18] & 30.66 & 40.49 & 31.00 & 46.36 & 30.10 & 44.57 \\\\  & _Ours_ & **56.49** & **52.70** & **45.09** & **52.74** & **49.32** & **51.27** \\\\ \\hline \\multirow{4}{*}{MMD \\(\\downarrow\\)} & GBIF [7] & 4736 & 1479 & 3220 & 4246 & 2763 & 3289 \\\\  & AutoSDF [18] & 3706 & 1456 & 3249 & 4115 & 2744 & 3054 \\\\  & _Ours_ & **3195** & **1285** & **2871** & **3729** & **2430** & **2702** \\\\ \\hline \\multirow{4}{*}{ECD \\(\\downarrow\\)} & GBIF [7] & 1327 & 2752 & 1589 & 434 & 869 & 1394 \\\\  & AutoSDF [18] & 1619 & 4256 & 1038 & 1443 & 462 & 1764 \\\\  & _Ours_ & **571** & **1889** & **419** & **196** & **285** & **672** \\\\ \\hline \\multirow{4}{*}{1-NNA \\(\\downarrow\\)} & GBIF [7] & 91.47 & 92.43 & 75.61 & 83.12 & 70.19 & 82.56 \\\\  & AutoSDF [18] & 83.31 & 87.76 & 69.34 & 77.43 & 67.20 & 77.01 \\\\ \\cline{1-1}  & _Ours_ & **66.81** & **83.39** & **64.83** & **57.28** & **59.55** & **66.37** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE III: Results of class-guide generation. Models are trained on 13 categories of ShapeNet. Best results are highlighted in **bold**.\n' +
      '\n' +
      'Fig. 9: Qualitative results of partial point completion.\n' +
      '\n' +
      'versatility of our proposed method. Qualitative results in Fig. 26 show the diversity and fidelity of our completed shapes.\n' +
      '\n' +
      '### _Image-guide Generation_\n' +
      '\n' +
      'Next, we show the flexibility that Argus3D can easily extend to image-guide generation, which is a more challenging task. The flexibility lies in that (1) it is implemented by the easiest way of feature concatenation; (2) the condition form of images is various, which could be 1-D feature vectors or patch tokens, or 2-D feature maps. For simplicity, we use a pre-trained CLIP model (_i.e._, ViT-B/32) to extract feature vectors of images as conditions. All experiments are conducted on ShapeNet with rendered images.\n' +
      '\n' +
      '**Baselines.** CLIP-Forge [88] is a flow-based model, which is trained with pairs of images and shapes. We take it as our primary baseline for two reasons: (1) it is a generation model instead of a reconstruction model, and (2) it originally uses CLIP models to extract image features. We also compare with AutoSDF [18], which is an auto-regressive model for 3D shape generation. We follow its official codes to extract per-location conditionals from the input image and then guide the non-sequential auto-regressive modeling to generate shapes.\n' +
      '\n' +
      '**Metrics and Settings.** We evaluate models with the test split of 13 categories. For each category, we randomly sample 50 single-view images and then generate 5 shapes for evaluation. As a generation task, TMD is adopted to measure the diversity. We further use MMD with Chamfer Distance and Frechet Point Cloud distance (FPD) [87] to measure the fidelity compared with the ground truth.\n' +
      '\n' +
      '**Results Analysis.** Results are reported in Tab. VI. Argus3D wins out in terms of both fidelity and diversity. In particular, we achieve a great advantage on both TMD and FPD, demonstrating the effectiveness of Argus3D applied to image-guide generation. It is also successfully echoed by qualitative visualizations in Fig. 10. Our samples are diverse and appear visually faithful to attributes of the object in images. Most importantly, our approach can accept input in either 1D or 2D form, giving it more flexibility. To verify it, we conduct two baselines by using ViT32 (or ResNet) to extract patch embeddings (or image features) from the input image as conditions. The conditional generation can be achieved by simply prepending the condition to [SOS] token. Results in Tab. VI indicate that they are competitive with models using CLIP as the feature extractor, further suggesting the powerful versatility of our Argus3D in either generative ways or conditional forms.\n' +
      '\n' +
      '### _Text-guide Generation_\n' +
      '\n' +
      'Encouraged by promising results on image-guide generation, we also turn Argus3D to text-to-shape generation. The same pre-trained CLIP model is used to extract single embedding for text conditions. Note that we did it on purpose, not using word sequence embedding, but only using a single features vector in our model to show its efficacy and scalability to the simplest forms of conditions. We will discuss the length of text conditions later.\n' +
      '\n' +
      '**Data.** One of important paired text-shape datasets is Text2Shape [93], which provides language descriptions for two objects from ShapeNet, _i.e._, chair and table. Thereby, we consider it as our main dataset to perform text-guide shape generation.\n' +
      '\n' +
      '**Baselines.** We compare our model with three state-of-the-art text-to-shape generation models. One is CLIP-Forge [88] under the supervised learning setting, using the same condition extractor as ours. One is ITG [94], which adopts BERT to encode texts into sequence embeddings. And the other is AutoSDF [18], where we re-train it using its official codes.\n' +
      '\n' +
      '**Metrics and Settings.** All models are trained on the train split of Text2Shape dataset with their official codes. We evaluate our model with two types of text queries: (1) description: the test split of Text2Shape; (2) prompts: customized short phrases provided by [88] containing attributes for chair and table. We additionally use Accuracy (Acc.) [88] to measure the fidelity. The Accuracy is calculated by a pre-trained PointNet [32] classifier on ShapeNet.\n' +
      '\n' +
      '**Results Analysis.** We report quantitative results of text-guide generation in Tab. VII. Argus3D achieves promising results on\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline \\multicolumn{1}{c}{Method} & \\multicolumn{1}{c}{TMD (\\(\\times 10^{2}\\)) \\(\\uparrow\\)} & \\multicolumn{1}{c}{MMD (\\(\\times 10^{3}\\)) \\(\\downarrow\\)} & \\multicolumn{1}{c}{FPD \\(\\downarrow\\)} \\\\ \\hline AutoSDF [18] & 2.523 & **1.383** & 2.092 \\\\ Clip-Forge [88] & 2.858 & 1.926 & 8.094 \\\\ \\hline _Ours_ (ViT32) & 3.677 & 1.617 & 2.711 \\\\ _Ours_ (ResNet) & **4.738** & 1.662 & 3.894 \\\\ _Ours_ (CLIP) & 4.274 & 1.590 & **1.680** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VI: Quantitative results of image-guide generation.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l} \\hline \\hline \\multirow{2}{*}{Metrics} & \\multirow{2}{*}{Methods} & \\multicolumn{3}{c}{Categories} & \\multirow{2}{*}{AVG} \\\\ \\cline{3-4} \\cline{6-6}  & & Chair & Sofa & Table \\\\ \\hline \\multirow{3}{*}{\\begin{tabular}{l} TMD \\(\\uparrow\\) \\\\ (\\(\\times 10^{2}\\)) \\\\ \\end{tabular} } & cGAN [86] & 1.708 & 0.687 & **1.707** & 1.367 \\\\  & PVD [54] & 1.098 & 0.811 & 0.839 & 0.916 \\\\  & ShapeFormer [19] & 1.159 & 0.698 & 0.677 & 0.845 \\\\  & _Ours_ & **2.042** & **1.221** & 1.538 & **1.600** \\\\ \\hline \\multirow{3}{*}{\\begin{tabular}{l} UHD \\(\\downarrow\\) \\\\ (\\(\\times 10^{2}\\)) \\\\ \\end{tabular} } & cGAN [86] & 7.836 & 7.047 & 9.406 & 8.096 \\\\  & PVD [54] & 10.79 & 13.88 & 11.38 & 12.02 \\\\  & ShapeFormer [19] & 6.884 & 8.658 & 6.688 & 7.410 \\\\  & _Ours_ & **6.439** & **6.447** & **5.948** & **6.278** \\\\ \\hline \\multirow{3}{*}{\n' +
      '\\begin{tabular}{l} MMD \\(\\downarrow\\) \\\\ (\\(\\times 10^{3}\\)) \\\\ \\end{tabular} } & cGAN [86] & 1.665 & 1.813 & 1.596 & 1.691 \\\\  & PVD [54] & 2.352 & 2.041 & 2.174 & 2.189 \\\\ \\cline{1-1}  & ShapeFormer [19] & 1.055 & 1.100 & 1.066 & 1.074 \\\\ \\cline{1-1}  & _Ours_ & **0.961** & **0.819** & **0.828** & **0.869** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IV: Results of multi-modal partial point completion. The missing parts vary according to random viewpoints.\n' +
      '\n' +
      'Fig. 10: Visualizations of image-guide shape generation.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l l l l l} \\hline \\hline \\multirow{2}{*}{Metrics} & \\multirow{2}{*}{Methods} & \\multicolumn{3}{c}{Categories} & \\multirow{2}{*}{AVG} \\\\ \\cline{3-4} \\cline{6-6}  & & Chair & Sofa & Table \\\\ \\hline \\multirow{3}{*}{\\begin{tabular}{l} TMD \\(\\uparrow\\) \\\\ (\\(\\times 10^{2}\\)) \\\\ \\end{tabular} } & AutoSDF [18] & 2.046 & 1.609 & 3.116 & 2.257 \\\\  & _Ours_ & **3.682** & **2.673** & **10.30** & **5.552** \\\\ \\hline \\multirow{3}{*}{\\begin{tabular}{l} UHD \\(\\downarrow\\) \\\\ (\\(\\times 10^{2}\\)) \\\\ \\end{tabular} } & AutoSDF [18] & **6.793** & 9.950 & **8.122** & 8.289 \\\\  & _Ours_ & 6.996 & **6.599** & 10.87 & **8.155** \\\\ \\hline \\multirow{3}{*}{\n' +
      '\\begin{tabular}{l} MMD \\(\\downarrow\\) \\\\ (\\(\\times 10^{3}\\)) \\\\ \\end{tabular} } & AutoSDF [18] & 1.501 & 1.154 & **2.600** & **1.751** \\\\  & _Ours_ & **1.477** & **1.127** & 2.906 & 1.837 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE V: Quantitative results of multi-modal partial point completion. The missing parts are always the top half.\n' +
      '\n' +
      'TMD, MMD and Acc, showing good generalization performance across different conditional generation tasks. Qualitative results are illustrated in Fig. 11. Furthermore, different from images, texts have a natural form of sequence. Each word is closely related to its context. Thereby, we discuss the ability of Argus3D to text-guide generation conditioned on sequence embeddings. Concretely, we adopt BERT and CLIP 3 model to encode texts into a fixed length of sequence embeddings. From Tab. VII(a), we find that using sequence embeddings indeed boosts the performance of Argus3D for text-guide generation tasks. Thanks to our proposed compact discrete representation with more tractable orders, our Argus3D can be easily adapted to different conditional forms for different tasks, thus improving the quality of the generated shapes.\n' +
      '\n' +
      'Footnote 3: In this case, we output features of all tokens instead of the last [END] token as the text embedding.\n' +
      '\n' +
      '### _Generation with Larger Scale_\n' +
      '\n' +
      'As discussed in Sec. 4.4, taking advantage of our compact discrete representation and vanilla transformer structure, we scale up our Argus3D, from a base model of 100 million to a huge model of 3.6 billion, in search of stronger generation capacity endowed by larger parameters. In this section, we thus focus on investigating the efficacy of Argus3D at different scales, as well as the impact of our large-scale 3D dataset Oboiverse-Mix. We evaluate models on two conditional 3D shape generation tasks, _i.e._, class-guide and image-guide, taking into account the challenge and reliability of tasks. The experimental settings and metrics are the same as the corresponding task above, unless are specifically explained.\n' +
      '\n' +
      '**Benefit from Larger Models.** We use the same ShapeNet dataset to train three Argus3D models at the base, large, and huge scales respectively, for the task of class-guide generation. The parameters of three models are listed in Tab. 1. In particular, they all share weights of the first-stage model. Unlike Sec. 5.3, we employ Chamfer Distance (CD) and Earth Mover\'s Distance (EMD) as distance metrics to evaluate performance for efficiency. These metrics offer different perspectives on the model\'s ability to generate accurate and diverse shapes.\n' +
      '\n' +
      'We first discuss the effect of model parameters on the generation quality. Figure 12 demonstrates the results of our three Argus3D models. As expected, with the increase of model parameters, the performance of four metrics is substantially improved. In addition, we compare our model with state-of-the-art approaches\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table} TABLE VII: Quantitative results of text-guide generation.\n' +
      '\n' +
      'Fig. 11: Qualitative comparisons of text-guide generation, using prompts as text queries. Models are trained on Text2Shape dataset which only includes two categories, _i.e._, chair and table.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table} TABLE VIII: Result comparisons of our large model on class-guide scales. Models are trained for class-guide generation on ShapeNet.\n' +
      '\n' +
      'Fig. 12: Quantitative comparisons of our Argus3D with different scales. Models are trained for class-guide generation on ShapeNet.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      'real-world applications. Without loss of generality, all experiments are conducted with the Argus3D-Base model.\n' +
      '\n' +
      '**Effect of Flattening Orders.** We first investigate how the flattening order affects the quality of shape generation. This study takes tri-planar representation as an example (\'Tri-Plane\' for short), since _our improved discrete representation (\'Vector\') can naturally degenerate into \'Tri-Plane\' by removing the proposed coupling network_. As aforesaid in Fig. 3, different flattening orders will affect different auto-regressive generation orders of the three planes. Quantitatively, we consider three variants to learn joint distributions of tri-planar representation without loss of generality, Iter-A: \\(p\\left(\\mathbf{z}\\right)=p\\left(\\mathbf{z}^{xz}\\right)\\cdot p\\left(\\mathbf{z}^ {xy}|\\mathbf{z}^{xz}\\right)\\cdot p\\left(\\mathbf{z}^{yz}|\\mathbf{z}^{xz}, \\mathbf{z}^{xy}\\right)\\), Iter-B: \\(p\\left(\\mathbf{z}\\right)=p\\left(\\mathbf{z}^{xy}\\right)\\cdot p\\left(\\mathbf{z}^ {xz}|\\mathbf{z}^{xy}\\right)\\cdot p\\left(\\mathbf{z}^{yz}|\\mathbf{z}^{xy}, \\mathbf{z}^{xz}\\right)\\) and Iter-C: \\(p\\left(\\mathbf{z}\\right)=p\\left(\\mathbf{z}^{yz}\\right)\\cdot p\\left(\\mathbf{z}^ {xz}|\\mathbf{z}^{yz}\\right)\\cdot p\\left(\\mathbf{z}^{xy}|\\mathbf{z}^{yz}, \\mathbf{z}^{xz}\\right)\\).\n' +
      '\n' +
      'Results are presented in Tab. I and Fig. 16. As observed, different orders have a significant impact on performance, resulting in a large value of standard deviation. For instance, Iter-A achieves a better result on Plane category (Iter-A: 73.67 _vs._ Iter-B: 83.37 on 1-NNA), while for Rifle, it prefers the order of Iter-B (Iter-B: 56.54 _vs._ Iter-A: 68.35 on 1-NNA). We attempt to explain this phenomenon by visualizing projected shapes on three planes. As illustrated in Fig. 17, for Plane category (First Column), the projection onto the \\(xy\\)-plane provides limited shape information, while the \\(xz\\)-plane reveals more representative geometries of the aircraft. We agree that if the \\(xz\\)-plane that contains more shape information is generated first, the generation of subsequent planes may be much easier. Consequently, it is beneficial for Iter-A to generate more faithful 3D shapes than Iter-B. In contrast, Rifle and Chair exhibit more details on \\(xy\\)-plane, so the autoregressive order of Iter-B yields better results for these two categories. In addition, we notice that Car has a relatively simple shape, _e.g._, a cuboid, leading to similar impacts on the generation quality for different flattening orders.\n' +
      '\n' +
      '**Effect of Flattening Orders.** Next, We evaluate the advantage of our improved discrete representation in terms of efficacy and efficiency. We explore two variants of our full model by flattening coupled feature maps into vectors with row-major or column-major order. In Tab. I, our proposed method achieves similar well performance even with different serialization orders. Figure 16 shows that our standard deviation is significantly lower than \'Tri-Plane\', demonstrating the robustness of our improved representation to generation orders. The proposed coupling network has facilitated AR learning by introducing more tractable order. Additionally, the overall quality of synthesized shapes for all categories is balanced and excellent across all metrics, indicating the superiority of our design.\n' +
      '\n' +
      'Furthermore, we also investigate the advantage of low computation overhead. We use \'Vector\' to denote our design since we apply vector quantization to latent vectors, and \'Grid\' refers to the baseline method that applies vector quantization to volumetric grids. Figure 18 compares the performance of IoU at the first stage and the corresponding memory cost in the second stage. Since we cannot afford the training of transformers with volumetric grid representations, as an alternative, we report the first-stage IoU accuracy for comparisons. From Fig. 18, two conclusions can be drawn. **(1)** The resolution \\(r\\) of feature grids (\'Grid\' and \'Vector\') significantly affects the quality of reconstructed shapes. If \\(r\\) is too small, it lacks the capacity to represent intricate and detailed geometries (Grid Reso-32: \\(88.87\\)_vs_ Grid Reso-16: \\(81.12\\)). However, the large \\(r\\) will inevitably increase the computational complexity in the second stage, as the number of required codes explodes as \\(r\\) grows (Grid Reso-32: \\(\\geq 80\\)G _vs._ Grid Reso-16: \\(19.6\\)G). **(b)** Our proposed \'Vector\' representation not only achieves comparable reconstruction results (Vector Reso-32: \\(88.01\\)_vs._ Grid Reso-32: \\(88.87\\)), but also significantly reduces the computation overhead (Vector Reso-32: \\(3.8\\)G _vs._ Grid Reso-32: \\(\\geq 80\\)G).\n' +
      '\n' +
      '**Design Choices in Discrete Representation Learning.** As a key contribution of this paper, we first discuss the efficacy of our improved discrete representation learning. Similarly, \'Tri-Plane\' refers to baselines applying vector quantization to the three planes. Results in Tab. I show that \'Grid\' gets better IoU performance for shape reconstruction in the first stage, but fails to train transformers in the second stage due to the extreme long length of the sequence (_e.g._, \\(32^{3}\\)). In contrast, our model not only achieves comparable reconstruction results (#0 _vs._ #2), but also\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline \\multirow{2}{*}{Metrics} & \\multirow{2}{*}{Methods} & \\multicolumn{5}{c}{Categories} \\\\ \\cline{3-6}  & & Plane & Rifle & Chair & Car \\\\ \\hline \\multirow{4}{*}{\\(\\text{ECD}\\downarrow\\)} & \\multirow{4}{*}{Tri-Plane} & Iter-A & 744 & 405 & 4966 & 3599 \\\\  & & Iter-B & 3501 & **36** & 1823 & 4735 \\\\  & & Iter-C & 3098 & **282** & 4749 & 3193 \\\\ \\cline{2-6}  & \\multirow{2}{*}{Vector (_ours_)} & Row-Major & 236 & 65 & **27** & **842** \\\\  & & Col-Major & **205** & 79 & 102 & 980 \\\\ \\hline \\multirow{4}{*}{1-NNA \\(\\downarrow\\)} & \\multirow{4}{*}{Tri-Plane} & Iter-A & 73.67 & 68.35 & 78.15 & 87.16 \\\\  & & Iter-B & 83.37 & **56.54** & 70.92 & 87.42 \\\\  & & Iter-C & 81.83 & 65.61 & 78.38 & 88.53 \\\\ \\cline{2-6}  & \\multirow{2}{*}{Vector (_ours_)} & Row-Major & **59.95** & 57.28 & **73.71** & **76.58** \\\\  & & Col-Major & 62.48 & 57.70 & 58.38 & 78.09 \\\\ \\hline \\multirow{4}{*}{\\(\\text{COV}\\uparrow\\)} & \\multirow{4}{*}{Tri-Plane} & Iter-A & **81.70** & 75.10 & 79.33 & 65.31 \\\\  & & Iter-B & 74.16 & 75.52 & **82.95** & 63.97 \\\\  & & Iter-C & 71.32 & **76.69** & 78.89 & 72.25 \\\\ \\cline{2-6}  & \\multirow{2}{*}{Vector (_ours_)} & Row-Major & 79.11 & 74.26 & 80.81 & **73.25** \\\\  & & Col-Major & 77.87 & 73.52 & 81.03 & 71.31 \\\\ \\hline \\multirow{4}{*}{\\(\\text{CoV}\\uparrow\\)} & \\multirow{4}{*}{Tri-Plane} & Iter-A & 43.51 & 41.65 & 23.10 & 50.30 \\\\  & & Iter-B & 26.57 & 49.78 & 35.50 & 49.43 \\\\  & & Iter-C & 30.28 & 41.35 & 24.94 & 51.23 \\\\ \\cline{2-6}  & \\multirow{2}{*}{Vector (_ours_)} & Row-Major & **45.12** & **55.27** & **49.82** & **56.64** \\\\  & & Col-Major & 44.87 & 53.58 & 48.93 & 56.63 \\\\ \\hline \\multirow{4}{*}{\\(\\text{MMD}\\downarrow\\)} & \\multirow{4}{*}{Tri-Plane} & Iter-A & 3237 & 3962 & 3392 & 1373 \\\\  & & Iter-B & 3860 & 3624 & 3119 & 1404 \\\\ \\cline{1-1}  & & Iter-C & 3631 & 3958 & 3430 & 1385 \\\\ \\cline{1-1} \\cline{2-6}  & \\multirow{2}{*}{Vector (_ours_)} & Row-Major & 3124 & 3628 & **2703** & **1213** \\\\ \\cline{1-1}  & & Col-Major & **3102** & **3623** & 2707 & 1214 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I0: The effect of flattening order on different discrete representations. Here, we take unconditional generation as an example and train one model per class. Please find statistic and qualitative analyses in Fig. 16 and 17, respectively.\n' +
      '\n' +
      'Fig. 15: Image-guide generation based on DALL-E-2 generated images. Argus3D is capable of generating a wide range of shapes from unseen images. These shapes can be further enhanced with textures created by a texture model, which utilizes text prompts from DALL-E 2. Additionally, the use of various text prompts enables the generation of new and unique textures.\n' +
      '\n' +
      'outperforms \'Tri-Plane\' by a large margin on generation quality (#1 vs. #2). The latter shows inferior results due to \'ambiguity of order\' (see Suppl.) It significantly proves the efficacy of our proposed coupling network, improving the plasticity and flexibility of the model. We also explore different design choices in Tab. IV. By gradually increasing feature resolutions or the number of entries in the codebook, we achieve better performance on IoU. This observation is consistent with [97], as the capacity of the discrete representation is affected by these two factors. A larger number of entries promises to further improve accuracy. Thus, we empirically double it for our large and huge models.\n' +
      '\n' +
      '**Image-guide Generation in Real-world.** We further investigate the generalizability of our model on real-world images. We use the model trained on ShapeNet as described in Sec. 5.5, and download images from the internet as conditions. Fig. 20 shows the qualitative results for different categories, _i.e._, plane, chair, car and table. Our model sensitively captures major attributes of objects in the image and produces shapes faithful to them. (see the first column to the left). Meanwhile, our synthesized samples enjoy the advantage of diversity by partially sticking to the images, such as the types of wings and tail for two airplane images\n' +
      '\n' +
      'Figure 19 also shows the results of our model on Pix3D dataset (trained on ShapeNet, without any finetuning). Compared with other competitors, Argus3D is capable of generating high-quality and realistic shapes that highly match the shape of objects in real-world images. It significantly highlights the strong generalization ability of our method.\n' +
      '\n' +
      '**Zero-shot Text-to-shape Generation.** Inspired by CLIP-Forge [88], we utilize the CLIP model to achieve zero-shot text-to-shape generation. At training, we only use the rendered images of 3D shapes. At inference, we substitute image features with text features encoded by the CLIP model. Figure 21 and 22 show our ability of zero-shot generation, where shape attributes are controlled with different prompts. The high-quality of synthesized shapes clearly demonstrate the powerful versatility of our\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c} \\hline \\multirow{2}{*}{Num.} & \\multirow{2}{*}{Type} & \\multirow{2}{*}{\\#Enturk} & \\multirow{2}{*}{Reso.} & Stage 1 & Stage 2 \\\\ \\cline{5-6}  & & & & IoU \\(\\uparrow\\) & 1-NNA / ECD \\(\\downarrow\\) \\\\ \\hline \\hline\n' +
      '0 & Grid & & & 88.87 & \\(\\times\\) \\\\ \\cline{2-6}\n' +
      '1 & Tin-Plane & 4096 & 32 & 87.81 & 73.67 / 743 \\\\ \\cline{2-6}\n' +
      '2 & & & & 88.01 & **59.95 / 236** \\\\ \\cline{2-6}\n' +
      '3 & \\multirow{2}{*}{Vector} & 4096 & 16 & 79.17 & \\multirow{2}{*}{-} \\\\ \\cline{4-6}\n' +
      '4 & & 2048 & 32 & 86.59 & \\\\ \\cline{2-6}\n' +
      '5 & & 1024 & 86.57 & & \\\\ \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XI: Ablation study of auto-encoder design choices. We report 1-NNA/ECD for plane category. ‘Reso.’ means the resolution of feature map for vector quantization. ‘\\(\\times\\)’: cannot report due to extreme memory cost. ‘-’: not report. _Notably, without the coupling network, our method naturally degenerates into ‘Tri-Plane’ representation._\n' +
      '\n' +
      'Fig. 16: Statistic analysis of the effect of flattening order. We report mean and standard deviation as histograms and error bars, respectively. Best viewed in color and zoomed in.\n' +
      '\n' +
      'Fig. 17: Visualizations of projected shapes on three planes.\n' +
      '\n' +
      'Fig. 18: Comparisons of the first-stage IoU (%) accuracy and the second-stage memory cost (Mb) with different resolutions and discrete representations. The memory cost is calculated with a batch size of 1.\n' +
      '\n' +
      'Argus3D in 3D shape generation, showing great potential to real-world applications.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'We introduce an improved AR model for 3D shape generation. By projecting volumetric grids of encoded input shapes onto three axis-aligned orthogonal feature planes, which are then coupled into a latent vector, we reduce computational costs and create a more tractable order for AR learning. Our compact and tractable representations enable easy switching between unconditional and conditional generation with multi-modal conditioning inputs. Extensive experiments demonstrate that our model outperforms previous methods on multiple generation tasks. By scaling up both the model parameters and dataset size, we have developed Argus3D-Huge, the largest 3D shape generation model currently available. Alongside it, we introduce the Obojaverse-Mix dataset, a comprehensive collection containing multi-modal data. These advancements further enhance our model\'s performance, showcasing its robust capabilities in 3D shape generation.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      'Yanwei Fu, Xiangyang Xue, and Yinda Zhang are the corresponding authors. This work is supported by China Postdoctoral Science Foundation (2022M710746), National Key R&D Program of China (2021ZD0111102) and NSFC-62306046. Yanwei Fu is with the School of Data Science and MOE Frontiers Center for Brain Science, Fudan University. Yanwei Fu is also with Fudan ISTBI--ZNU Algorithm Centre for Brain-inspired Intelligence, Zhejiang Normal University, Jinhua, China.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] O. Mces, M. Tatarchenko, T. Brox, and W. Burgard, "Self-supervised 3d shape and viewpoint estimation from single images for robotics," in _2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_. IEEE, 2019, pp. 6083-6089.\n' +
      '* [2] J. Ye, Y. Chen, N. Wang, and X. Wang, "Online adaptation for implicit object tracking and shape reconstruction in the wild," _arXiv preprint arXiv:2111.12728_, 2021.\n' +
      '* [3] X. Qian, L. Wang, Y. Zhu, L. Zhang, Y. Fu, and X. Xue, "Im-pdet: Exploring implicit fields for 3d object detection," _arXiv preprint arXiv:2203.17240_, 2022.\n' +
      '* [4] Y. Sun, S. N. R. Kantareddy, R. Bhattacharyya, and S. E. Sarma, "X-vision: An augmented vision tool with real-time sensing ability in tagged environments," in _2018 1eee international conference on p4th technology & application (rfd-ta)_. IEEE, 2018, pp. 1-6.\n' +
      '* [5] J. D. Stets, Y. Sun, W. Corning, and S. W. Greenwald, "Visualization and labeling of point clouds in virtual reality," in _SIGGRAPH Asia 2017 Posters_, 2017, pp. 1-2.\n' +
      '* [6] Z. Chen and H. Zhang, "Learning implicit fields for generative shape modeling," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, pp. 5939-5948.\n' +
      '\n' +
      'Fig. 21: Qualitative comparisons of zero-shot text-to-shape generation. Results of CLIP-Forge are reported in their paper.\n' +
      '\n' +
      'Fig. 22: More visualizations of zero-shot text-to-shape generation.\n' +
      '\n' +
      'M. Ibing, I. Lim, and L. Kobbelt, "3d shape generation with grid-based implicit functions," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021, pp. 13 559-13 568.\n' +
      '* [1] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove, "Deepsdf: Learning continuous signed distance functions for shape representation," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, pp. 165-174.\n' +
      '* [2] R. Thoppilan, D. De Freitas, J. Hall, N. Shazezeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du _et al._, "Lambda: Language models for dialog applications," _arXiv preprint arXiv:2201.08239_, 2022.\n' +
      '* [3] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever _et al._, "Improving language understanding by generative pre-training," 2018.\n' +
      '* [4] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever _et al._, "Language models are unsupervised multitask learners," _OpenAI blog_, vol. 1, no. 8, p. 9, 2019.\n' +
      '* [5] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su _et al._, "Shapenet: An information-rich 3d model repository," _arXiv preprint arXiv:1512.03102_, 2015.\n' +
      '* [6] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi, "ObiJaverse: A universe of annotated 3d objects," _arXiv preprint arXiv:2212.08051_, 2022.\n' +
      '* [7] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, "3d shapenets: A deep representation for volumetric shapes," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2015, pp. 1912-1920.\n' +
      '* [8] P. Esser, R. Rombach, and B. Ommer, "Taming transformers for high-resolution image synthesis," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021, pp. 12 873-12 883.\n' +
      '* [9] L. Zhao, Z. Zhang, T. Chen, D. Metaxas, and H. Zhang, "Improved transformer for high-resolution gans," _Advances in Neural Information Processing Systems_, vol. 34, pp. 18 367-18 380, 2021.\n' +
      '* [10] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman, "Maskgit: Masked generative image transformer," _arXiv preprint arXiv:2202.04200_, 2022.\n' +
      '* [11] P. Mittal, Y.-C. Cheng, M. Singh, and S. Tulsiani, "Autosdf: Shape priors for 3d competition, reconstruction and generation," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 306-315.\n' +
      '* [12] X. Yan, L. Lin, N. J. Mitra, D. Lischinski, D. Cohen-Or, and H. Huang, "Shapeformer: Transformer-based shape completion via sparse representation," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 6239-6249.\n' +
      '* [13] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell _et al._, "Language models are few-shot learners," _Advances in neural information processing systems_, vol. 33, pp. 1877-1901, 2020.\n' +
      '* [14] S. Luo, X. Qian, Y. Fu, Y. Zhang, Y. Tai, Z. Zhang, C. Wang, and X. Xue, "Learning versatile 3d shape generation with improved ard models," _arXiv preprint arXiv:2303.14700_, 2023.\n' +
      '* [15] D. Maturana and S. Scherer, "Voxnet: A 3d convolutional neural network for real-time object recognition," in _2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_. IEEE, 2015, pp. 922-928.\n' +
      '* [16] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, "3d shapenets: A deep representation for volumetric shapes," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2015, pp. 1912-1920.\n' +
      '* [17] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese, "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction," in _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14_. Springer, 2016, pp. 628-644.\n' +
      '* [18] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum, "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling," _Advances in neural information processing systems_, vol. 29, 2016.\n' +
      '* [19] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong, "O-cnn: Octree-based convolutional neural networks for 3d shape analysis," _ACM Transactions On Graphics (TOG)_, vol. 36, no. 4, pp. 1-11, 2017.\n' +
      '* [20] C. Hune, S. Tulsiani, and J. Malik, "Hierarchical surface prediction for 3d object reconstruction," in _2017 International Conference on 3D Vision (3DV)_. IEEE, 2017, pp. 412-420.\n' +
      '* [21] G. Riegler, A. O. Ulusoy, H. Bischof, and A. Geiger, "Octnetfusion: Learning depth fusion from data," in _2017 International Conference on 3D Vision (3DV)_. IEEE, 2017, pp. 57-66.\n' +
      '* [22] J. Xie, Z. Zheng, R. Gao, W. Wang, S.-C. Zhu, and Y. N. Wu, "Generative voxelnet: Learning energy-based models for 3d shape synthesis and analysis," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, vol. 44, no. 5, pp. 2468-2484, 2022.\n' +
      '* [23] H. Fan, H. Su, and L. J. Guibas, "A point set generation network for 3d object reconstruction from a single image," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2017, pp. 605-613.\n' +
      '* [24] W. Wu, Z. Qi, and L. Fuxin, "Pointconv: Deep convolutional networks on 3d point clouds," in _Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition_, 2019, pp. 9621-9630.\n' +
      '* [25] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, "Pointnet: Deep learning on point sets for 3d classification and segmentation," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2017, pp. 652-660.\n' +
      '* [26] J. Xie, Y. Xu, Z. Zheng, S.-C. Zhu, and Y. N. Wu, "Generative pointnet: Deep energy-based learning on unordered point sets for 3d generation," reconstruction and classification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2021, pp. 14 976-14 985.\n' +
      '* [27] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger, "Occupancy networks: Learning 3d reconstruction in function space," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, pp. 4460-4470.\n' +
      '* [28] M. Michalkewicz, J. K. Pontes, D. Jack, M. Baktashmotlagh, and A. Eriksson, "Implicit surface representations as layers in neural networks," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 4743-4752.\n' +
      '* [29] V. Sitzmann, E. Chan, R. Tucker, N. Snavely, and G. Wetzstein, "Metasdf: Meta-learning signed distance functions," _Advances in Neural Information Processing Systems_, vol. 33, pp. 10 136-10 147, 2020.\n' +
      '* [30] S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, and A. Geiger, "Convolutional occupancy networks," in _European Conference on Computer Vision_. Springer, 2020, pp. 523-540.\n' +
      '* [31] W. E. Lorensen and H. E. Cline, "Marching cubes: A high resolution 3d surface construction algorithm," _ACM siggraph computer graphics_, vol. 21, no. 4, pp. 163-1636, 1987.\n' +
      '* [32] C. Jiang, A. Sud, A. Makadia, J. Huang, M. Niessner, T. Funkhouser _et al._, "Local implicit grid representations for 3d scenes," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020, pp. 6001-6010.\n' +
      '* [33] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas, "Learning representations and generative models for 3d point clouds," in _International conference on machine learning_. PMLR, 2018, pp. 40-49.\n' +
      '* [34] A. Van den Oord, N. Kalchbrenner, L. Espbalt, O. Vinyals, A. Graves _et al._, "Conditional image generation with pixel clean decoders," _Advances in neural information processing systems_, vol. 29, 2016.\n' +
      '* [35] A. Razavi, A. Van den Oord, and O. Vinyals, "Generating diverse high-fidelity images with vq-vae-2," _Advances in neural information processing systems_, vol. 32, 2019.\n' +
      '* [36] Y. Sun, Y. Wang, Z. Liu, J. Siegel, and S. Sarma, "Pointgrow: Autoregressively learned point cloud generation with self-attention," in _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, 2020, pp. 61-70.\n' +
      '* [37] A.-C. Cheng, X. Li, S. Liu, M. Sun, and M.-H. Yang, "Autoregressive 3d shape generation via canonical mapping," _arXiv preprint arXiv:2204.01955_, 2022.\n' +
      '* [38] C. Nash, Y. Ganin, S. A. Eslami, and P. Battaglia, "Polygen: An autoregressive generative model of 3d meshes," in _International Conference on Machine Learning_. PMLR, 2020, pp. 7220-7229.\n' +
      '* [39] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-resolution image synthesis with latent diffusion models," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 10 684-10 695.\n' +
      '* [40] J. Ho, A. Jain, and P. Abbeel, "Denoising diffusion probabilistic models," 2020.\n' +
      '* [41] J. Song, C. Meng, and S. Ermon, "Denoising diffusion implicit models," _arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* [42] P. Dhariwal and A \n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '## Appendix A Model Architectures\n' +
      '\n' +
      'Our proposed framework Argus3D consists of a two-step procedure for 3D shape generation. The first step is an auto-encoder structure, aiming to learn discrete representations for input 3D shapes. And the second step introduces a transformer structure to learn the joint distribution of discrete representations. Below we will elaborate on the details of these two structures.\n' +
      '\n' +
      '**Auto-encoder.** As shown in the left of Fig. 23, the auto-encoder takes as input point clouds \\(\\mathcal{P}\\in\\mathbb{R}^{n\\times 3}\\) with \\(n\\) means the number of points, and outputs the predicted 3D mesh \\(\\mathcal{M}\\). More concretely, the encoder starts by feeding point clouds into a PointNet [28] with local pooling, resulting in point features with dimensions \\(\\mathbb{R}^{n\\times 32}\\). Then, we project points on three axis-aligned orthogonal planes with the resolution of \\(256\\). Features of points falling into the same spatial grid cell are aggregated via mean-operation, so that input point clouds are represented as tri-planar features instead of volumetric features. To further improve the representation, we concatenate three feature planes and couple them with three convolution layers. Next, four stacked convolution layers are adopted to not only down-sample the feature resolution three times, but also highly encode and abstract the position mapping of each spatial grid in 3D space. Thus, the output has a tractable order to be serialized as a feature vector. Before we perform the vector quantization on the flattened outputs, we follow [97] to utilize the strategy of low dimensional codebook lookup, by squeezing the feature dimension from \\(256\\) to \\(4\\). Consequently, an arbitrary 3D shape can be represented with a compact quantized vector, whose elements are indices of those closest entries in the codebook.\n' +
      '\n' +
      'The decoder is composed of two 2D U-Net modules and one symmetric upsample block. After reshaping the quantized vector and unsqueezing its feature dimension from \\(4\\) to \\(256\\), we apply a 2D U-Net module to complement each spatial grid feature with global knowledge. Subsequently, the same number of 2D convolution layers as the downsample block is appended to upsample the feature resolution back to \\(256\\). Symmetric convolution layers further decouple it into tri-planer features. To further improve the smoothness between the spatial grids in each plane, we use the other shared 2D U-Net module to separately process tri-plane features. The structures of both 2D U-Net are in alignment with \\(3\\)[34]. Finally, we build a stack of fully-connected residual blocks with \\(5\\) layer, as an implicit function, to predict the occupancy probability of each query position.\n' +
      '\n' +
      '**Transformer.** Benefiting from the compact discrete representation with a tractable order for each input 3D shape, we adopt a vanilla decoder-only transformer without any specific-designed module to learn the joint distributions among discrete codes. Our transformer consists of \\(T\\) decoder layers, each of which has one multi-head self-attention layer and one feed-forward network. The decoder layer has the same structure as [15], and is illustrated in the right of Fig. 23. Specifically, we use a learnable start-of-sequence token ([SOS] token) to predict the first index of the discrete vector, and auto-regressively predict the next with the previous ones. For example, given an input containing the first \\(t\\) indices along with one [SOS] token, we first use an embedding layer to encode them as features. Then, we feed them into several transformer layers. The position embedding is also added to provide positional information. At the end of the last transformer layer, we use two fully-connected layers to predict the _logit_ of each token. However, we only keep the last one which is a meaningful categorical distribution for the next (_t_+1)-th index.\n' +
      '\n' +
      '**Implementation Details.** Table XII summarizes all parameter settings for both auto-encoder and transformer structures. We apply them as default to all experiments unless otherwise stated. The parameter settings for Argus3D of different scales are shown in Tab. I. The number of entries \\(m\\) in the codebook is 4096, 8192 and 8192 for Argus3D-Base, -Lager and -Huge, respectively. Lower triangular mask matrix is used in all multi-head self-attention layers to prevent information leakage, that is, the prediction of the current index is only related to the previous known indices. For various conditioning inputs, we adopt the most common way to encode them. For example, we use a learnable embedding layer to get the feature \\(\\in\\mathbb{R}^{1\\times d}\\) of each category. Given partial point clouds, our proposed auto-encoder encodes them into discrete representations, which are fed into another embedding layer to get features with \\(d\\) dimensions. We adopt pre-trained CLIP models to extract features \\(\\in\\mathbb{R}^{1\\times 512}\\) for images or texts, and further use one fully-connected layer to increase the dimension from \\(512\\) to \\(d\\). All of the encoded conditioning inputs are simply prepended to [SOS] token via concatenation to guide the generation.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Layer Name & Notes & Input Size \\\\ \\hline \\hline\n' +
      '**Auto-encoder** & & \\\\ PointNet & & \\(n\\times 3\\) \\\\ Coupler & & \\\\ ConvLayer & k3s1p1 & \\(256\\times 256\\times 3\\times 32\\) \\\\ ConvLayer & k3s1p1 & \\(256\\times 256\\times 96\\) \\\\ ConvLayer & k1s1p0 & \\(256\\times 256\\times 32\\) \\\\ Downsampler & & \\\\ ConvLayer & k2s2p0 & \\(256\\times 256\\times 32\\) \\\\ ConvLayer & k2s2p0 & \\(128\\times 128\\times 64\\) \\\\ ConvLayer & k2s2p0 & \\(64\\times 64\\times 128\\) \\\\ ConvLayer & k1s1p0 & \\(32\\times 32\\times 256\\) \\\\ Squeezer & k1s1p0 & \\(32\\times 32\\times 256\\) \\\\ Quantizer & & \\(32\\times 32\\times 4\\) \\\\ Ursuspaczer & k1s1p0 & \\(32\\times 32\\times 256\\) \\\\\n' +
      '2D U-Net & & \\(32\\times 32\\times 256\\) \\\\ Upsampler & & \\\\ DecomLayer & k3s1p1 & \\(32\\times 32\\times 256\\) \\\\ DecomLayer & k3s1p1 & \\(64\\times 64\\times 128\\) \\\\ DecomLayer & k3s1p1 & \\(128\\times 128\\times 64\\) \\\\ ConvLayer & k1s1p0 & \\(256\\times 256\\times 32\\) \\\\ Decoupler & & \\\\ ConvLayer & k3s1p1 & \\(256\\times 256\\times 32\\) \\\\ ConvLayer & k3s1p1 & \\(256\\times 256\\times 96\\) \\\\ ConvLayer & k1s1p0 & \\(256\\times 256\\times 96\\) \\\\\n' +
      '2D U-Net & & \\(256\\times 256\\times 3\\times 32\\) \\\\ \\hline \\hline\n' +
      '**Transformer** & Embedding Layer & & \\\\ Decoder Layers \\(\\times\\)\\(T\\) & & \\\\ Self-Attention & h & (K + 1 + L) \\(\\times d\\) \\\\ Feed-Forward & m4\\(d\\) & (K + 1 + L) \\(\\times d\\) \\\\ Head Layer & & \\\\ LinearLayer & & L \\(\\times d\\) \\\\ LinearLayer & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XII: The detailed architecture of our framework. ‘k’, ‘s’ and ‘p’ denote kernel size, stride and padding, respectively, in the convolution layer. ‘h’ means the number of heads in multi-head self-attention layer, and ‘\\(T\\)’ is the number of layers. The feature dimension \\(d\\) in the transformer varies for different scales. \\(m\\) stands for the dimension of the middle layer in the feed-forward network. ‘K’ and ‘L’ are the sequence length of conditioning inputs and discrete representation, and ‘1’ indicates the length of [SOS] token.\n' +
      '\n' +
      '## Appendix B Training and Testing Procedures\n' +
      '\n' +
      '**Training:** All models are trained on at most eight A100 GPUs. For the first stage, we take dense point clouds with \\(n=30,000\\) as input, and train the auto-encoder on ShapeNet (or ObiayverseMix) dataset for a total 600k (or 1300k) iterations. The learning rate is set as 1e-4, and the batch size is \\(16\\). Once trained, it is shared for all generation tasks. For the second stage, we adopt the learning rate of 1e-4 for the base model, and 1e-5 for the large and huge models. The learning rate decay is manually adjusted for the training of Argus3D-H, and set it to 3e-6 and 1e-6 respectively when the loss approaches convergence. Due to the increase in the number of parameters, the batch size for three models of different scales is set to 8, 3 and 1, respectively. It takes around 600k iterations for the base model on a single GPU card, and 3500k iterations for the huge model on eight GPU cards.\n' +
      '\n' +
      '**Testing:** During inference, we first use the well-trained transformer to predict discrete index sequences with or without conditioning inputs. For each index, we sample it with the multinomial distribution according to the predicted probability, where only the top-\\(k\\) indices with the highest confidence are kept for sampling. We progressively sample the next index, until all elements in the sequence are completed. Then, we feed the predicted index sequence into the decoder to get tri-planar features. Subsequently, we interpolate the feature of each point on a grid of resolution \\(128^{3}\\) from tri-planar features, and adopt the implicit function to query the corresponding occupancy probability. Finally, the iso-surface of 3D shapes is extracted with the threshold of \\(0.2\\) via Marching Cubes [38].\n' +
      '\n' +
      '**Inference Speed:** For our base model Argus3D-B, the wall time it takes to sample a single shape is roughly 14 seconds, and we can also generate 32 shapes in parallel in 3 minutes. For our Argus3D-H, the inference speed required to sample a single shape is about 50 seconds, due to the increased computation of more parameters and layers.\n' +
      '\n' +
      '## Appendix C Visualization\n' +
      '\n' +
      'We attach high-resolution visualizations of Figs. 7, 8, 9, and 15 for better views.\n' +
      '\n' +
      'Fig. 23: The architectures of Argus3D, consisting of an auto-encoder at the first stage (left) and a transformer at the second stage (right).\n' +
      '\n' +
      'Figure 24: Qualitative results of unconditional generation.\n' +
      '\n' +
      'Figure 25: Qualitative results of class-guide generation.\n' +
      '\n' +
      'Figure 27: Image-guide generation based on DALL-E-2 generated images. Argus3D is capable of generating a wide range of shapes from unseen images. These shapes can be further enhanced with textures created by a texture model, which utilizes text prompts from DALL-E 2. Additionally, the use of various text prompts enables the generation of new and unique textures.\n' +
      '\n' +
      'Figure 26: Qualitative results of partial point completion.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>