<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# SaulLM-7B: A pioneering Large Language Model for Law\n' +
      '\n' +
      'Pierre Colombo\\({}^{1,2,}\\) Telmo Pessoa Pires\\({}^{1,}\\) Malik Boudiaf\\({}^{1,}\\)1\n' +
      '\n' +
      '**Dominic Culver\\({}^{1,*}\\) Rui Melo\\({}^{1,*}\\) Caio Corro\\({}^{3}\\) Andre F. T. Martins\\({}^{4}\\) Fabrizio Esposito\\({}^{5}\\) Vera Lucia Raposo\\({}^{5}\\) Sofia Morgado\\({}^{1}\\) Michael Desa\\({}^{1}\\)**\n' +
      '\n' +
      '\\({}^{1}\\)Equall.ai, New York, Paris, Lisbon\n' +
      '\n' +
      '\\({}^{2}\\)MICS, CentraleSupelec, Universite Paris-Saclay\n' +
      '\n' +
      '\\({}^{3}\\)Sorbonne Universite, CNRS, ISIR, Paris\n' +
      '\n' +
      '\\({}^{4}\\)Instituto Superior Tecnico, Universidade de Lisboa\n' +
      '\n' +
      '\\({}^{5}\\) NOVA School of Law, Lisboa\n' +
      '\n' +
      'firstname@equall.ai\n' +
      '\n' +
      'Equal contribution.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistrat 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B\'s performance in legal tasks. SaulLM-7B is released under the MIT License.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'In the rapidly evolving landscape of artificial intelligence, the applications of large language models (LLMs) [1, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22] have witnessed large advancements across various domains, like _e.g._ translation [23], medical [20], and code generation [11, 12]. From natural language processing to machine translation, these models have exhibited exceptional capabilities in understanding and generating human-like text [21, 13, 15]. However, one field that has yet to experience the full benefit of this transformative technology is the legal domain [16, 17]. As legal professionals grapple with an ever-expanding volume of complex documents, there is a growing need for a dedicated LLM that can help navigate and interpret legal material [23, 24, 25].\n' +
      '\n' +
      'In this paper, we present a pioneering initiative to develop the first legal LLM publicly available. Legal text, characterized by its unique syntax and specialized vocabulary presents a distinct linguistic challenge [13, 14]. Our approach focuses on extensive pretraining [16, 20] using dedicated legal corpora from English-speaking jurisdictions such as the USA, Canada, the UK, and Europe [15, 17]. Leveraging the pretraining on a large and diverse legal dataset, both scraped by our team as well as from previous literature [14], our LLM, SaulLM-7B, aims not only to comprehend the complexities of legal documents but also to adapt to the evolving nature of legal discourse.\n' +
      '\n' +
      'By focusing on the needs of legal practitioners and harnessing the power of pretraining on dedicated legal corpora, our work represents an important step towards fulfilling the unique demands of the legal domain. We anticipate that introducing the first LLM for law will not only empower legal professionals but also catalyze further innovation at the intersection of artificial intelligence and the legal community - making a significant contribution to legal language understanding and application [18]. We summarize the contributions of this work as follows:\n' +
      '\n' +
      'Contribution 1: A family of legal LLMs.In this paper, we introduce the SaulLM-7B\'s family, a collection of Legal Language Models meticulously crafted to tackle the distinctive challenges encountered within the legal domain. We unveil SaulLM-7B, a 7-billion-parameter language model specifically tailored to legal text. With its specialized training regimen, SaulLM-7B demonstrates a superior understanding of the nuances in legal language compared to generic models. Furthermore, we release SaulLM-7B-Instruct, an instructiontuned variant, carefully engineered to outperform existing models such as Mistral or Llama on a variety of legal tasks1.\n' +
      '\n' +
      'Footnote 1: Model is available at [https://huggingface.co/Equall](https://huggingface.co/Equall).\n' +
      '\n' +
      'Contribution 2: An improved evaluation protocol for legal LLMs.Concurrently, we introduce LegalBench-Instruct, a supplemental iteration of LegalBench Guha et al. (2022, 2023)2, crafted to better gauge and refine the legal proficiency of language models, which we hope will contribute to future advancements into research in the legal domain. To further enrich the models\' capabilities in legal contexts, we also include the legal tasks of the popular MMLU benchmark Hendrycks et al. (2020) in our evaluation protocol, particularly focusing on international law, professional law3 and jurisprudence.\n' +
      '\n' +
      'Footnote 3: We use the term “professional law” here as defined in Hendrycks et al. (2020)\n' +
      '\n' +
      'Contribution 3: Model, Evaluation Code & Licensing.To foster widespread adoption and promote innovation, we release SaulLM-7B and SaulLM-7B-Instruct, as well as our evaluation code under the MIT License. This open licensing approach encourages collaborative development and adoption into a wide array of commercial and research endeavors within the legal domain and beyond.\n' +
      '\n' +
      '## 2 SaulLM-7B: Extending the legal capabilities of Language Models\n' +
      '\n' +
      'A wide range of open-source large language models is available for the backbone, spanning from \\(70\\) million parameter models like Pythia Biderman et al. (2023) to \\(180\\) billion parameter models like Falcon Almazrouei et al. (2023). In this work, we choose the Mistral 7B model, a \\(7\\) billion parameter open-source model that achieves high performance across benchmarks and tasks Jiang et al. (2023).\n' +
      '\n' +
      'Our methodology, shown in Figure 1 involves a two-step process that we describe below.\n' +
      '\n' +
      '### Enhancing Mistral\'s Legal Capabilities\n' +
      '\n' +
      'While generic models Touvron et al. (2023); Taylor et al. (2022); Zhang et al. (2022); Gu and Dao (2023); Almazrouei et al. (2023); Zhang et al. (2024); Faysse et al. (2024) gain some exposure to legal data during their training, it typically only represents a minor fraction of the overall data. A straightforward method to enhance performance for legal tasks is to perform additional training focusing on legal data. This approach, particularly focused on decoder models, has been successfully used in various fields such as medicine Chen et al. (2023); Ji et al. (2023), translation Xu et al. (2023); Wu et al. (2024), and coding Roziere et al. (2023). The key advantage of this approach is its scalability and independence from the specific characteristics of the training data. Other research on domain adaptation has attempted to specialize language models via pretext tasks. However, these efforts often rely on smaller-scale approaches Niklaus and Giofre (2023), are computationally expensive Vu et al. (2020); Lu et al. (2023), or lack scalability Cheng et al. (2023); Cui et al. (2023); Nishida et al. (2019).\n' +
      '\n' +
      'For these reasons, as well as the availability of large-scale legal corpora from the web, we chose to focus on _continued pretraining_. We meticulously curate a high-quality dataset sourced from diverse legal content repositories. After rigorous filtering Penedo et al. (2023) and deduplication Mou et al. (2023); Kocetkov et al. (2023), we end up with a corpus of \\(30\\) billion tokens, which serves as a robust foundation for continued pretraining.\n' +
      '\n' +
      '### Improving Legal Instruction Following\n' +
      '\n' +
      'To support user requests and conversational interaction, LLMs typically undergo instruction tuning, a critical process involving training on supervised conversational pairs. This step is essential for crafting a versatile model, adept at addressing user queries Wang et al. (2023); Wei et al. (2021); Chung et al. (2022); Faysse et al. (2023); Ding et al. (2023); Wang et al. (2023).\n' +
      '\n' +
      'For general-purpose language models, diversity and quality of instruction are crucial Cao et al. (2023); Zhou et al. (2023). However, in specialized domains it is crucial to incorporate task-specific and specialized prompts to enhance performance. Our instruction fine-tuning stage involves \\(2\\) key components: generic (ie, non-legal) and legal instructions. The former help enhance the model\'s understanding and following of commands, and includes data from diverse domains such as coding, mathematics, and general conversations. For the latter we employ an extensive collection of datasets tailored to the nuances of legal domains, covering legal question answering and summarization, among others. Through this metic lous fine-tuning on instructional data, our model, SaulLM-7B-Instruct, is able to grasp legal intricacies and excels in a wide range of associated tasks.\n' +
      '\n' +
      '**Remark**.: _It\'s worth noting that many common LLMs Tunstall et al. (2023) include an additional step of to align the model with human preference Rafailov et al. (2023); Munos et al. (2023); von Werra et al. (2020). In our case, early experiments did not show any meaningful improvement in performance and so we opted to not pursue this avenue for the present paper._\n' +
      '\n' +
      '## 3 Data\n' +
      '\n' +
      'In this section we describe our data collection and cleaning schemes.\n' +
      '\n' +
      '### Legal Pretraining Corpora\n' +
      '\n' +
      'Unlike fields such as science and medicine, the legal landscape varies significantly across countries and jurisdictions, reflecting differences not only in local laws but also in legal traditions, like common law versus civil law Henderson et al. (2022). Thus, we gathered legal texts from various jurisdictions, with a primary focus on the English language due to its widespread use in legal contexts worldwide. Our collection includes data from the U.S. Tuggener et al. (2020), Europe Chalkidis et al. (2019), and Australia Butler (2023), covering a diverse range of legal systems. Through this thorough curation process and aggressive cleaning (see Section 3.1.2), we end up with a corpus of 30 billion tokens, capturing the intricacies of legal language across regions.\n' +
      '\n' +
      '#### 3.1.1 Dataset Composition\n' +
      '\n' +
      'Legal SourcesWe combine both previously available datasets, such as the FreeLaw subset from The Pile Gao et al. (2020) and MultiLegal Pile Niklaus et al. (2023), as well as data scraped from publicly available sources on the Web. We list the different sources of data in Table 1.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Name & Tokens \\\\ \\hline FreeLaw4  & \\(15\\)B \\\\ EDGAR5  & \\(5\\)B \\\\ English MultiLegal Pile6  & \\(5\\)B \\\\ English EuroParl Koehn (2005) & \\(6\\)B \\\\ GovInfo7  & \\(11\\)B \\\\ Law Stack Exchange8  & \\(19\\)M \\\\ Commercial Open Australian Legal Corpus9  & \\(0.5\\)B \\\\ EU Legislation10  & \\(315\\)M \\\\ UK Legislation11  & \\(190\\)M \\\\ Court Transcripts12  & \\(350\\)M \\\\ UPSTO13  & \\(4.7\\)B \\\\ Total & \\(94\\)B \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Sources of Legal Pretraining Data.** These sources contain noise and heavily duplicated documents, which we filtered and deduplicated, resulting in a 30 billion tokens dataset.\n' +
      '\n' +
      'Figure 1: **Procedure for constructing SaulLM-7B.** We rely on legal datasets augmented with replay data, and instructions datasets. For fine-tuning we enrich our instruction finetuning dataset further with legal instructions.\n' +
      '\n' +
      'There is quite a lot of overlap between the different sources, and we run very aggressive cleaning and deduplication steps, described in Section 3.1.2.\n' +
      '\n' +
      'Replay SourcesTo reduce the risk of catastrophic forgetting McCloskey and Cohen (1989) during continued pretraining, we incorporate data from the prior training distribution, following prior literature Chen et al. (2023); Sun et al. (2020). However, since the training data for Mistral is undisclosed, we introduce commonly available "general" data from Wikipedia, StackExchange, and GitHub, comprising roughly \\(2\\%\\) of the final training mix. These datasets are sampled from SlimPajama Shen et al. (2023); Computer (2023); Soboleva et al. (2023).\n' +
      '\n' +
      'Instruction SourcesAdditionally, we found it beneficial to include conversational data during pretraining. This is inspired by recent advances in neural machine translation, which highlight that the robust capabilities of LLMs in translation are due to the existence of accidental parallel data in the training corpus Anil et al. (2023); Briakou et al. (2023). Specifically, this means that we include the Super Natural Instruction Wang et al. (2022) and FLAN collection Longpre et al. (2023) during pretraining.\n' +
      '\n' +
      '#### 3.1.2 Data Cleaning\n' +
      '\n' +
      'A significant fraction of the collected data is either in PDF files or is text extracted from PDFs14. This means that the text has some artifacts, including i) page numbers in the middle of sentences; ii) line numbers; iii) non-normalized unicode characters; iv) broken lines of text; v) repeated characters: new lines, dashes, etc; vi) other artifacts. We addressed these issues using a combination of rules and heuristics to filter the data.\n' +
      '\n' +
      'Footnote 14: We used Poppler for text extraction from PDF files.\n' +
      '\n' +
      'Text NormalizationWe normalize all unicode with the NFKC method, available through the unicodedata Python package.\n' +
      '\n' +
      'Rule filtersFollowing Elazar et al. (2023), we found the most common 10-grams in our dataset and used regular expressions to remove the undesired ones, which were mostly repeated characters. Concretely, \\(8\\) of the top \\(10\\) 10-grams in the original data were repeated characters, eg: "- - - - -"," or "* * * * * * * * * * * * *, and weird characters, ie encoding issues. Additionally, we removed repeated whitespace (spaces, new lines, and tabs), as well as any HTML tag that made it through our pipeline.\n' +
      '\n' +
      'Perplexity filteringWe trained a KenLM model Heafield (2011) on a small subset of carefully inspected legal data, and used it to filter any high perplexity paragraph. This removed non-English text as well as most of the "weird" unicode sequences present in the data. We show some of the most common \\(10\\)-grams in the filtered data on Table 2.\n' +
      '\n' +
      '#### 3.1.3 Data Deduplication\n' +
      '\n' +
      'Inspired by Kocetkov et al. (2023); Lee et al. (2021), we removed duplicates and near-duplicates from the training data using Mou et al. (2023), with default parameters, after which we were left with roughly \\(30\\)B tokens of high-quality text.\n' +
      '\n' +
      '### Instruction Finetuning Mixes\n' +
      '\n' +
      'Instruction fine-tuning is crucial for getting the best performance out of the pre-trained decoder models across different tasks. We use a mix of general and legal instructions to train the model to understand and follow instructions well, with a focus on legal expertise.\n' +
      '\n' +
      'General InstructionsWhen it comes to general instructions, we gather them from four primary sources:\n' +
      '\n' +
      '1. **SlimOrca** This subset of the FLAN collection comprises generic instructions, offering a focused resource for various tasks Mukherjee et al. (2023); Lian et al. (2023).\n' +
      '2. **Meta Math Question Answering Instructions** Designed for mathematical inquiry, this dataset15 presents a range of mathematical questions, facilitating research in math-based natural language processing Yu et al. (2023). Footnote 15: Accessible at meta-math/MetaMathQA\n' +
      '3. **General Conversations from UltraChat** Capturing diverse conversational contexts,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c} \\hline \\hline Common 10-grams \\\\ \\hline have been obvious to one of ordinary skill in the \\\\ before the effective filing date of the claimed invention \\\\ rejected under 35 U.S.C. 103 as being unpatentable over \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Most common 10-grams in the pretraining dataset.**this GPT-derived dataset contributes to enhancing natural language understanding and generation systems Ding et al. (2023).\n' +
      '4. **Code Instructions from Glaive Code Assistant v21** Training on code has been shown to increase the reasoning ability of models Ma et al. (2023)\n' +
      '\n' +
      'Footnote 16: Available at [https://huggingface.co/datasets/glaiveai/glaive-code-assistant-v2](https://huggingface.co/datasets/glaiveai/glaive-code-assistant-v2)\n' +
      '\n' +
      'We meticulously filter, deduplicate, and curate all this data, resulting in a refined dataset comprising \\(600\\)K instructions.\n' +
      '\n' +
      'Legal Instruction ConstructionWe synthetically generate comprehensive conversations addressing fundamental legal competencies across multiple legal document types Ding et al. (2023). We leverage a Mistral-7B-instruct to transform legal texts augmented with metadata into coherent conversations. The methodology involves initiating the conversation with \\(3\\) predefined turns: (1) the user articulates a request related to the legal document, (2) the assistant responds by rephrasing the metadata (e.g., document type, date, name of a judge), and (3) the user prompts the assistant to elaborate on its reasoning. Subsequently, we extend the conversation through a series of turns, where a user model progressively poses more specific questions to grasp the assistant\'s reasoning. Simultaneously, an assistant model provides in-depth insights. An illustrative example is presented in Figure 2. Notably, we ensure the exclusion of the test set from existing benchmarks.\n' +
      '\n' +
      '## 4 Evaluation of Legal Knowledge\n' +
      '\n' +
      'To evaluate the model\'s legal abilities, we use \\(3\\) benchmarks (i) we compare the perplexity of the backbones on \\(5\\) types of legal documents, (ii) we enhance LegalBench with LegalBench-Instruct for deeper evaluation, (iii) we rely on the legal section of MMLU for additional insights.\n' +
      '\n' +
      'Perplexity MeasurementTo evaluate the adaptability of the backbones to legal documents, we assess perplexity using benchmark datasets spanning four distinct legal domains: _contracts, judicial decisions, opinion text, and legislation_. We ensure that the datasets are up-to-date, and sourced after the collection cut-off date from LLM data. Specifically, contract data is sourced from EDGAR (first quarter of 2024), legal decisions from ICSID court decisions published after October 2023, legislation focuses on US bills submitted before the House or Senate after October 2023, and party submissions include Texas briefs submitted after October 2023.\n' +
      '\n' +
      'During our investigations, we found a significant limitation in the original prompts of LegalBench. The complex nature of these prompts, combined with the challenges encountered by open source LLMs in adhering to instructions - particularly in handling formatting - leads to a substantial drop in performance (as measured by accuracy). The generated sentences are often verbose and difficult to parse, rendering LegalBench in its current form too stringent and failing to accurately gauge improvement on the task.\n' +
      '\n' +
      'For example, in some of the tasks, performance is evaluated by the first word the model predicts, and this word is expected to be a _Yes/No_. This means that if the response is a bit verbose it will be counted as incorrect, even if a human would classify it as a correct answer. To remedy this shortcoming, we refine the prompts by 1) removing distracting few-shot examples and 2) concluding with a specific instruction for the model to generate tags (see Table 3).\n' +
      '\n' +
      'Massive Multitask Language Understanding (MMLU)The MMLU benchmark Hendrycks et al. (2020) has been widely employed to gauge\n' +
      '\n' +
      'Figure 2: **Turning dataset with metadata into a conversation.** Taking the example of Reddit post classification, we turn a labeled example {_My employer fired me because...Is it legal?_, _“employment”_}, we hard-code the first three turns of the conversation by simply reformulating the query and answer as a natural conversation. We then complete the conversation using a _user_ model(blue dashed), whose task is to continue generating relevant questions from the ongoing conversation, and an _assistant_ model that provides answers. Both _assistant_ and _user_ models are Mistral-7B-instruct.\n' +
      '\n' +
      'the advances in LLM performance. In our study, we center our analysis on the legal domain, with a specific focus on: _international law_, _professional law_, and _jurisprudence_. Those tasks respectively contain \\(120\\), \\(1500\\), and \\(110\\) examples.\n' +
      '\n' +
      '### Metrics\n' +
      '\n' +
      'We use the same metric as the original LegalBench Guha et al. (2023) paper: balanced accuracy. Balanced accuracy allows for handling better-imbalanced classification tasks, such as the ones presented in both benchmarks. We also use balanced accuracy for the legal tasks of MMLU. Unless otherwise noted, any score reported throughout this section refers to the balanced accuracy.\n' +
      '\n' +
      '## 5 Experimental Setting\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      'We compare the SaulLM-7B family to other state-of-the-art \\(7\\)B and \\(13\\)B open-source models. Concretely, we include the following instruction and DPO finetuned variants of Mistral-7B Jiang et al. (2023): Mistral-7B-Instruct-v0.1, Mistral-7B-Instruct-v0.2, as well as zephyr-7b-beta17. We also evaluate the Llama2 Touvron et al. (2023) family, more specifically Llama2-7b-Chatand Llama2-13b-Chat.\n' +
      '\n' +
      'Footnote 17: [https://huggingface.co/HuggingFaceH4/zephyr-7b-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'CodebaseOur codebase relies on open-source frameworks Shoeybi et al. (2019); Wolf et al. (2019); Lhoest et al. (2021) utilizing DeepSpeed (level 3) with Flash attention Dao et al. (2022); Dao (2023). It is built on PyTorch Paszke et al. (2019), and our models are available on the Huggingface hub.\n' +
      '\n' +
      'ComputeContinuous pretraining utilizes \\(256\\) MI250 AMD GPUs. For instruction fine-tuning, workload distribution occurs across 16 MI250. Evaluation procedures are seamlessly conducted on a single MI250.\n' +
      '\n' +
      '## 6 Results\n' +
      '\n' +
      'In this section, we discuss our main experimental findings and results.\n' +
      '\n' +
      '### LegalBench-Instruct\n' +
      '\n' +
      'Figures 3 and 4 summarize our results on LegalBench-Instruct. There are \\(3\\) main takeaways, which we discuss below.\n' +
      '\n' +
      'Figure 3: **Performance of base models on LegalBench-Instruct. Interestingly, although not instruction fine-tuned, SaulLM-7B is still able to achieve impressive improvements on the benchmark, compared to other base models, including SaulLM-7B’s initial checkpoint (Mistral-7B).**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline \\hline\n' +
      '**Original Prompt** \\\\ \\hline The Telemarketing Sales Rule is provided by 16 C.F.R. § 310.3(a)(2). \\\\ \\multicolumn{2}{l}{**Question:** Acme Toys is a telemarketer subject to the Telemarketing Sales Rule. Acme Toys told a customer that is fribeses cost $10 each, when in fact the fribes cost $12 each. The customer agreed to the sale and was charged $12. Is this a violation of the Telemarketing Sales Rule? \\\\ \\hline\n' +
      '**Answer:** Yes \\\\ \\hline\n' +
      '**Question:** Acme Toys is a telemarketer subject to the Telemarketing Sales Rule. Acme Toys told a customer that is fribeses cost $10 each, when in fact the fribeses did cost $10, but Acme Toys did not disclose that shipping would cost an additional $5. The customer agreed to the sale. Is this a violation of the Telemarketing Sales Sales Rule? \\\\ \\hline\n' +
      '**Answer:** No \\\\ \\hline\n' +
      '**Question:** Acme Industrial Products is a telemarketer subject to the Telemarketing Sales Rule. Acme Industrial Products told a customer that is would sell them 4 brooms for $10 and that shipping would be $5. Then, the customer agreed to the sale. Is this a violation of the Telemarketing Sales Sales Rule? \\\\ \\hline\n' +
      '**Answer:** No \\\\ \\hline\n' +
      '**Question:** \\{text\\}\n' +
      '**Question:** Acme Industrial Products is a telemarketer subject to the Telemarketing Sales Rule. Acme Industrial Products told a customer that it would sell them 4 brooms for $510 and that shipping would be $5. Then, the customer agreed to the sale. Is this a violation of the Telemarketing Sales Rule? \\\\ \\hline\n' +
      '**Answer:** No \\\\ \\hline\n' +
      '**Question:** \\{text\\}\n' +
      '**Question:** \\{text\\}\n' +
      '**Question:** \\{text\\}\n' +
      '**Question:** \\{text\\}\n' +
      '**Question:** \\{text\\}\n' +
      '**Question:** \\{text\\}\n' +
      '**Question:** \\{text\\}\n' +
      '**Question:** \\{text\\}{text\\}{text\\}{text\\I. Legal continued pretraining brings significant improvementsWe start by analyzing the impact of our proposed continued pretraining. As seen on Figure 3, SaulLM-7B is a strong standalone model. We speculate that its strong performance is largely due to the integration of instructions in the pre-training data, as mentioned in subsubsection 3.1.1. Nevertheless, we still note that even without a dedicated instruction fine-tuning stage, SaulLM-7B performs on par with Llama2-7B-chat (\\(0.38\\) v.s. \\(0.39\\)). More importantly, SaulLM-7B serves as a strong base model for building IFT models with strong legal capabilities. When combined with Generic instruction finetuning, as seen on Figure 4, it achieves a strong average of \\(0.59\\), i.e. \\(4\\) absolute points of improvement with respect to the best open-source instruct model Mistral-7B-Instruct-v0.1.\n' +
      '\n' +
      'II. Legal instruction finetuning further boosts the resultsAs seen on Figure 2, finetuning SaulLM-7B on both general and legal instructions (SaulLM-7B-Instruct) establishes a new state-of-the-art on the LegalBench-Instruct benchmark, with an average score of \\(0.61\\), i.e. an \\(11\\)% relative improvement compared to the best open-source instruct model (Figure 5. Finally, DPO-aligned models tend to underperform their instruction-tuned counterparts, which could be explained by the fact that generic alignment is not suited for out-of-distribution tasks, such as the ones present in LegalBench-Instruct. Although beyond the scope of the present work, an interesting research direction would be to explore how legal-specific DPO can help.\n' +
      '\n' +
      'III. There is still room for significant improvement.Next, we follow the original LegalBench\n' +
      '\n' +
      'Figure 4: **Influence of the base model.** Starting the instruction finetuning from our base model SaulLM-7B brings noticeable improvements compared to the Mistral-7B. Indeed, even with a generic IFT mix (without legal), SaulLM-7B (Gen.) outperforms its Mistral-Instruct counterpart significantly. Adding legal instructions to the IFT mix further boosts the results.\n' +
      '\n' +
      'Figure 5: **Comparison of instruct models on LegalBench-Instruct**. SaulLM-7B-Instruct establishes the state-of-the-art, outperforming the best Mistral-Instruct model by a significant 6 absolute points.\n' +
      '\n' +
      'Figure 6: **Instruct models on Legal-MMLU.** Echoing finding on LegalBench-Instruct, SaulLM-7B-Instruct displays superior performance on all three tasks of Legal-MMLU, with an average absolute improvement of 5 points with respect to Mistral-7B-Instruct-v0.1.\n' +
      '\n' +
      'taxonomy [14] to gain a more granular understanding of SaulLM-7B-Instruct\'s performance, by partitioning the tasks into \\(5\\) core legal abilities: Issue Spotting, Rule-Recall, Interpretation, Rhetoric Understanding, and Rule-Conclusion. Results show an interesting trend (Figure 7): SaulLM-7B-Instruct shows clear superior performance over the best non-legal competitor \\(\\mathtt{Mistral-7B-Instruct-v0.1}\\) on the four areas that require the most legal expertise, i.e. Issue, Rule, Interpretation and Understanding. On the other hand, it falls short of \\(\\mathtt{Mistral-7B-Instruct-v0.1}\\) on the Conclusion tasks, which interestingly require much more pure deductive reasoning than actual legal knowledge. We speculate that augmenting our pretraining and fine-tuning corpora with more deductive reasoning content, including but not limited to mathematics datasets could reduce the gap and fully unlock the potential of SaulLM-7B-Instruct.\n' +
      '\n' +
      '### Results on Legal-MMLU\n' +
      '\n' +
      'To confirm our observations on LegalBench-Instruct, we analyze the results on Legal-MMLU shown in Figure 6. Again, SaulLM-7B-Instruct exhibits consistent superiority over non-legal instruction-tuned models, with a gap between \\(3\\) and \\(4\\) absolute points to the best 7B open-source competitor across the three tasks, providing additional evidence that SaulM-7B-Instruct is as a strong foundation to build models tailored to legal workflows.\n' +
      '\n' +
      '### Perplexity Analysis\n' +
      '\n' +
      'To assess the adaptation of SaulLM-7B backbone to the legal domain, we present perplexity scores across four document types: contracts, legal decisions, legislation, and party submissions. Refer to Figure 8 for the results. Our model, SaulLM-7B, consistently outperforms \\(\\mathtt{Mistral-7B}\\) across all categories, exhibiting lower average perplexity scores with reduced variance. Interestingly, \\(\\mathtt{Llama2-7B}\\) demonstrates lower perplexity specifically in legislation documents, suggesting a potentially higher proportion of legislative text in the pertaining corpora compared to \\(\\mathtt{Mistral-7B}\\).\n' +
      '\n' +
      'Overall, compared to \\(\\mathtt{Mistral-7B}\\), our model shows a median perplexity reduction of 3 percent across legal corpora and 11 percent when compared to \\(\\mathtt{Llama2-7B}\\).\n' +
      '\n' +
      '## 7 Conclusion & Future Perspectives\n' +
      '\n' +
      'In this paper, we introduce SaulLM-7B, an open-source decoder model delivering state-of-the-art performance, compared to 7B models, within the legal domain. Our approach entails fine-tuning legal data alongside instruction fine-tuning on synthetic datasets. Additionally, we contribute by providing a cleaned version of LegalBench and introducing a new set of documents for perplexity measurement. We hope that our model, which is released under the MIT license, will contribute to the open-source ecosystem and the community.\n' +
      '\n' +
      'Figure 8: **Perplexity on legal documents for pretrained backbones.** SaulLM-7B-Instruct outperforms other pretrained backbones on most types of legal documents, but is outperformed by \\(\\mathtt{Llama2-7B}\\) on Legislation. SaulLM-7B-Instruct exhibits a median perplexity of \\(8.69\\), having a reduction of \\(5.5\\) percent compared to \\(\\mathtt{Mistral-7B}\\), \\(9.20\\), and \\(10.8\\) percent compared to \\(\\mathtt{Llama2-7B}\\), with a median perplexity of \\(9.74\\).\n' +
      '\n' +
      'Figure 7: **Per-task performance breakdown.** SaulLM-7B-Instruct largely outperforms generic Instruct models on tasks that most require legal-specific knowledge, but is outperformed by \\(\\mathtt{Mistral-Instruct}\\) on the conclusion tasks, which necessitates more deductive reasoning.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'We thank GENCI for generously granting us access to their cutting-edge computing resources. Our model, SaulLM-7B, has been trained on ADASTRA, with initial experimentation conducted on Jeanzay. The utilization of HPC resources was made possible through the Jeanzay grants 101838, 103256, and 103298, as well as the Adastra grants C1615122, CAD14770, and CAD15031.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_.\n' +
      '* Aletras et al. (2016) Nikolaos Aletras, Dimitrios Tsarapatsanis, Daniel Preotiuc-Pietro, and Vasileios Lampos. 2016. Predicting judicial decisions of the european court of human rights: A natural language processing perspective. _PeerJ computer science_, 2:e93.\n' +
      '* Almazrorouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. The falcon series of open language models.\n' +
      '* Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_.\n' +
      '* Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Biyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chenguang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report.\n' +
      '* Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\'Brien, Eric Hallahan, Mohammad Afah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In _International Conference on Machine Learning_, pages 2397-2430. PMLR.\n' +
      '* Briakou et al. (2023) Eleftheria Briakou, Colin Cherry, and George Foster. 2023. Searching for needles in a haystack: On the role of incidental bilingualism in palm\'s translation capability. _arXiv preprint arXiv:2305.10266_.\n' +
      '* Butler (2023) Umar Butler. 2023. Open australian legal corpus.\n' +
      '* Cao et al. (2023) Yihan Cao, Yanbin Kang, and Lichao Sun. 2023. Instruction mining: High-quality instruction data selection for large language models. _arXiv preprint arXiv:2307.06290_.\n' +
      '* Chalkidis et al. (2019) Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. 2019. Neural legal judgment prediction in english. _arXiv preprint arXiv:1906.02059_.\n' +
      '* Chalkidis et al. (2020) Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. 2020. Legal-bert: The muppets straight out of law school. _arXiv preprint arXiv:2010.02559_.\n' +
      '* Chen et al. (2023) Zeming Chen, Alejandro Hernandez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Kopf, Amrikeivan Mohtashami, et al. 2023. Meditron-70b: Scaling medical pretraining for large language models. _arXiv preprint arXiv:2311.16079_.\n' +
      '* Cheng et al. (2023) Daixuan Cheng, Shaohan Huang, and Furu Wei. 2023. Adapting large language models via reading comprehension. _arXiv preprint arXiv:2309.09530_.\n' +
      '* Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_.\n' +
      '* Computer (2023)Together Computer. 2023. Redpajama: an open dataset for training large language models.\n' +
      '* Cui et al. (2023) Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. 2023. Chatlaw: Open-source legal large language model with integrated external knowledge bases. _arXiv preprint arXiv:2306.16092_.\n' +
      '* Dao (2023) Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. _arXiv preprint arXiv:2307.08691_.\n' +
      '* Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. _Advances in Neural Information Processing Systems_, 35:16344-16359.\n' +
      '* Ding et al. (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. _arXiv preprint arXiv:2305.14233_.\n' +
      '* Elazar et al. (2023) Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, and Jesse Dodge. 2023. What\'s in my big data?Manuel Faysse, Patrick Fernandes, Nuno Guerreiro, Antonio Loison, Duarte Alves, Caio Corro, Nicolas Boizard, Joao Alves, Ricardo Rei, Pedro Martins, et al. 2024. Croissantllm: A truly bilingual french-english language model. _arXiv preprint arXiv:2402.00786_.\n' +
      '* Faysse et al. (2023) Manuel Faysse, Gautier Viaud, Celine Hudelot, and Pierre Colombo. 2023. Revisiting instruction fine-tuned model evaluation to guide industrial applications. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics.\n' +
      '* Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The pile: An 800gb dataset of diverse text for language modeling.\n' +
      '* Gu and Dao (2023) Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_.\n' +
      '* Guha et al. (2022) Neel Guha, Daniel E Ho, Julian Nyarko, and Christopher Re. 2022. Legalbench: Prototyping a collaborative benchmark for legal reasoning. _arXiv preprint arXiv:2209.06120_.\n' +
      '* Guha et al. (2023) Neel Guha, Julian Nyarko, Daniel E Ho, Christopher Re, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel N Rockmore, et al. 2023. Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models. _arXiv preprint arXiv:2308.11462_.\n' +
      '* Gururangan et al. (2020) Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. 2020. Don\'t stop pretraining: Adapt language models to domains and tasks. _arXiv preprint arXiv:2004.10964_.\n' +
      '* Gutierrez-Fandino et al. (2021) Asier Gutierrez-Fandino, Jordi Armengol-Estape, Aitor Gonzalez-Agirre, and Marta Villegas. 2021. Spanish legalese language model and corpora. _arXiv preprint arXiv:2110.12201_.\n' +
      '* Heafield (2011) Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In _Proceedings of the Sixth Workshop on Statistical Machine Translation_, pages 187-197, Edinburgh, Scotland. Association for Computational Linguistics.\n' +
      '* Henderson et al. (2022) Peter Henderson, Mark Krass, Lucia Zheng, Neel Guha, Christopher D Manning, Dan Jurafsky, and Daniel Ho. 2022. Pile of law: Learning responsible data filtering from the law and a 256gb open-source legal dataset. _Advances in Neural Information Processing Systems_, 35:29217-29234.\n' +
      '* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_.\n' +
      '* Islam et al. (2023) Niful Islam, Debopom Sutradhar, Humaira Noor, Jarin Tasnim Raya, Monowara Tabassum Maisha, and Dewan Md Farid. 2023. Distinguishing human generated text from chatgpt generated text using machine learning. _arXiv preprint arXiv:2306.01761_.\n' +
      '* Ji et al. (2023) Shaoxiong Ji, Tianlin Zhang, Kailai Yang, Sophia Ananiadou, Erik Cambria, and Jorg Tiedemann. 2023. Domain-specific continued pretraining of language models for capturing long context in mental health. _arXiv preprint arXiv:2304.10447_.\n' +
      '* Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. 2023. Mistral 7b.\n' +
      '* Jiang et al. (2024) Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. 2024. Mix-tral of experts.\n' +
      '* Katz et al. (2023) Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2023. Gpt-4 passes the bar exam. _Available at SSRN 4389233_.\n' +
      '* Koetkov et al. (2023) Denis Koetkov, Raymond Li, Loubna Ben allal, Jia LI, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro Von Werra, and Harm de Vries. 2023. The stack: 3 TB of permissively licensed source code. _Transactions on Machine Learning Research_.\n' +
      '* Koehn (2005) Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In _Proceedings of Machine Translation Summit X: Papers_, pages 79-86, Phuket, Thailand.\n' +
      '* Lee et al. (2021) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better. _arXiv preprint arXiv:2107.06499_.\n' +
      '* Lhoest et al. (2021) Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et al. 2021. Datasets: A community library for natural language processing. _arXiv preprint arXiv:2109.02846_.\n' +
      '* Li et al. (2020) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Koetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Joao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muthasham Obolkov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Scholkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. Starcoder: may the source be with you!\n' +
      '* Lian et al. (2023) Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium". 2023. Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification.\n' +
      '* Licari and Comande (2022) Daniele Licari and Giovanni Comande. 2022. Italian-legal-bert: A pre-trained transformer language model for italian law. In _CEUR Workshop Proceedings (Ed.), The Knowledge Management for Law Workshop (KM4LAW)_.\n' +
      '* Longpre et al. (2023) Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barrett Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. _arXiv preprint arXiv:2301.13688_.\n' +
      '* Lu et al. (2023) Keming Lu, Peter Potash, Xihui Lin, Yuwen Sun, Zihan Qian, Zheng Yuan, Tristan Naumann, Tianxi Cai, and Junwei Lu. 2023. Prompt discriminative language models for domain adaptation. In _Proceedings of the 5th Clinical Natural Language Processing Workshop_, pages 247-258.\n' +
      '* Ma et al. (2023) Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. 2023. At which training stage does code data help l1ms reasoning?\n' +
      '* Martin et al. (2024) Lauren Martin, Nick Whitehouse, Stephanie Yiu, Lizzie Catterson, and Rivindu Perera. 2024. Better call gpt, comparing large language models against lawyers. _arXiv preprint arXiv:2401.16212_.\n' +
      '* McCloskey and Cohen (1989) Michael McCloskey and Neal J. Cohen. 1989. Catastrophic interference in connectionist networks: The sequential learning problem. volume 24 of _Psychology of Learning and Motivation_, pages 109-165. Academic Press.\n' +
      '* Mitchell et al. (2023) Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. 2023. Detectgpt: Zero-shot machine-generated text detection using probability curvature. _arXiv preprint arXiv:2301.11305_.\n' +
      '* Mou et al. (2023) Chenghao Mou, Chris Ha, Kenneth Enevoldsen, and Peiyuan Liu. 2023. Chenghaomou/text-dedup: Reference snapshot.\n' +
      '* Mukherjee et al. (2023) Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4.\n' +
      '* Munos et al. (2023) Remi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. 2023. Nash learning from human feedback. _arXiv preprint arXiv:2312.00886_.\n' +
      '* Niklaus et al. (2021) Joel Niklaus, Ilias Chalkidis, and Matthias Sturmer. 2021. Swiss-judgment-prediction: A multilingual legal judgment prediction benchmark. _arXiv preprint arXiv:2110.00806_.\n' +
      '* Niklaus and Giofre (2022) Joel Niklaus and Daniele Giofre. 2022. Budget-longformer: Can we cheaply pretrain a sota legal language model from scratch? _arXiv preprint arXiv:2211.17135_.\n' +
      '* Niklaus and Giofre (2023) Joel Niklaus and Daniele Giofre. 2023. Can we pretrain a sota legal language model on a budget from scratch? Association for Computational Linguistics.\n' +
      '* Niklaus et al. (2023) Joel Niklaus, Veton Matoshi, Matthias Sturmer, Ilias Chalkidis, and Daniel E. Ho. 2023. Multilegalpile: A 689gb multilingual legal corpus.\n' +
      '* Nishida et al. (2019) Kosuke Nishida, Kyosuke Nishida, Itsumi Saito, Hisako Asano, and Junji Tomita. 2019. Unsupervised domain adaptation of language models for reading comprehension. _arXiv preprint arXiv:1911.10768_.\n' +
      '* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32.\n' +
      '* Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon l1m: Outperforming curated corpora with web data, and web data only. _arXiv preprint arXiv:2306.01116_.\n' +
      '* Prakken (2013) Henry Prakken. 2013. _Logical tools for modelling legal argument: a study of defeasible reasoning in law_, volume 32. Springer Science & Business Media.\n' +
      '* Radford et al. (2022) Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. Robust speech recognition via large-scale weak supervision.\n' +
      '* Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. _arXiv preprint arXiv:2305.18290_.\n' +
      '* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. 2023. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_.\n' +
      '* Savelka et al. (2023) Jaromir Savelka, Kevin D Ashley, Morgan A Gray, Hannes Westermann, and Huihui Xu. 2023. Explaining legal concepts with augmented large language models (gpt-4). _arXiv preprint arXiv:2306.09525_.\n' +
      '* De Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_.\n' +
      '* Shen et al. (2023) Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing. 2023. Slimpajama-dc: Understanding data combinations for llm training. _arXiv preprint arXiv:2309.10818_.\n' +
      '* Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. _arXiv preprint arXiv:1909.08053_.\n' +
      '* Soboleva et al. (2023) Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. 2023. Slimpajama: A 627b token cleaned and deduplicated version of redrojama.\n' +
      '* Sun et al. (2020) Jingyuan Sun, Shaonan Wang, Jiajun Zhang, and Chengqing Zong. 2020. Distill and replay for continual language learning. In _Proceedings of the 28th international conference on computational linguistics_, pages 3569-3579.\n' +
      '* Taylor et al. (2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. _arXiv preprint arXiv:2211.09085_.\n' +
      '* Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_.\n' +
      '* Touvron et al. (2020) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esibou, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaloyoy, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.\n' +
      '* Tuggener et al. (2020) Don Tuggener, Pius Von Daniken, Thomas Peetz, and Mark Cieliebak. 2020. Ledgar: A large-scale multi-label corpus for text classification of legal provisions in contracts. In _Proceedings of the Twelfth Language Resources and Evaluation Conference_, pages 1235-1241.\n' +
      '* Tunstall et al. (2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazzeen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, et al. 2023. Zephyr: Direct distillation of lm alignment. _arXiv preprint arXiv:2310.16944_.\n' +
      '* von Werra et al. (2020) Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. 2020. Trl: Transformer reinforcement learning. [https://github.com/huggingface/trl](https://github.com/huggingface/trl).\n' +
      '* Vu et al. (2020) Thuy-Trang Vu, Dinh Phung, and Gholamreza Haffari. 2020. Effective unsupervised domain adaptation with adversarially trained language models. _arXiv preprint arXiv:2010.01739_.\n' +
      '* Wang et al. (2023) Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavich Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. 2023a. How far can camels go? exploring the state of instruction tuning on open resources. _arXiv preprint arXiv:2306.04751_.\n' +
      '* Wang et al. (2023) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-instruct: Aligning language models with self-generated instructions.\n' +
      '* Wang et al. (2022) Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022. Super-natural instructions: Generalization via declarative instructions on 1600+ nlp tasks. _arXiv preprint arXiv:2204.07705_.\n' +
      '* Weber-Wulff et al. (2023) Debora Weber-Wulff, Alla Anohina-Naumeca, Sonja Bjelobaba, Tomas Foltynek, Jean Guerrero-Dib, Olumide Popoola, Petr Sigut, and Loma Waddington. 2023. Testing of detection tools for ai-generated text. _International Journal for Educational Integrity_, 19(1):26.\n' +
      '\n' +
      'Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_.\n' +
      '* Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. 2019. Huggingface\'s transformers: State-of-the-art natural language processing. _arXiv preprint arXiv:1910.03771_.\n' +
      '* Wu et al. (2024) Minghao Wu, Thuy-Trang Vu, Lizhen Qu, George Foster, and Gholamreza Haffari. 2024. Adapting large language models for document-level machine translation. _arXiv preprint arXiv:2401.06468_.\n' +
      '* Xiao et al. (2021) Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, and Maosong Sun. 2021. Lawformer: A pre-trained language model for chinese legal long documents. _AI Open_, 2:79-84.\n' +
      '* Xu et al. (2023) Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2023. A paradigm shift in machine translation: Boosting translation performance of large language models. _arXiv preprint arXiv:2309.11674_.\n' +
      '* Yao et al. (2021) Yunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong, and Furu Wei. 2021. Adapt-and-distill: Developing small, fast and effective pretrained language models for domains. _arXiv preprint arXiv:2106.13474_.\n' +
      '* Yu et al. (2023) Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. _arXiv preprint arXiv:2309.12284_.\n' +
      '* Zhang et al. (2024) Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. Tinyllama: An open-source small language model. _arXiv preprint arXiv:2401.02385_.\n' +
      '* Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_.\n' +
      '* Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Less is more for alignment. _arXiv preprint arXiv:2305.11206_.\n' +
      '* Zhou et al. (2021)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>