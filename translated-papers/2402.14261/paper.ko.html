<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '* **Bug-Fixing**(fix): LLM은 정적 분석 툴에 의해 제기된 경고 및 오류를 식별하고 수정하는 데 중요한 역할을 한다.\n' +
      '***자연어로부터의 코드 생성**(생성): LLM은 자연어 설명으로부터 코드 스니펫을 생성한다.\n' +
      '코드**(테스트)에 대한 **테스트 케이스 생성: LLM은 소프트웨어 품질 보증 관행을 향상시키는 것을 목표로 코드에 대한 테스트 케이스를 자동으로 생성하는 데 사용된다.\n' +
      '**워크스페이스 이해 및 질의 해결**(워크스페이스): LLM은 로컬 워크스페이스 내의 코드베이스의 이해에 따라 개발자 질의에 응답함으로써 개발자들이 현재 프로젝트를 이해하도록 돕는다.\n' +
      '\n' +
      '평가 하니스의 메트릭은 실제 개발 시나리오에서 LLM 유도 프로그래밍 상호 작용의 효과, 정확성 및 효율성을 평가하도록 설계되었다. 우리의 프레임워크는 모든 IDE가 우리의 메트릭을 사용하여 플러그인되고 평가될 수 있도록 한다. 따라서 우수한 LLM 통합 결과를 얻기 위해 IDE 매개변수 공간을 조정하는 시스템을 제공한다.\n' +
      '\n' +
      '이전 작업에서 코드 생성을 위한 평가 하네스를 제공했지만[10], IDE에서 여러 소프트웨어 엔지니어링 시나리오를 지원하는 새로운 대용량 언어 모델을 사용하여 보다 포괄적인 평가 하네스가 필요하다. LLM을 IDE와 통합할 때 튜닝하고 최적화할 수 있는 넓은 파라미터 공간이 있다: 모델에 대한 프롬프트는 어떻게 표현[38; 43]되는가? 어떤 순서로 정보가 주어져야 하는가[23]? 모델 응답은 원래 코드에 어떻게 구문 분석되고 다시 삽입됩니까? 쿼리[30; 33] 외에 모델에 어떤 컨텍스트가 제공되어야 하는가? 이러한 모든 요인 및 그 이상은 모델이 IDE 내에서 얼마나 잘 수행할 수 있는지에 대한 역할을 한다.\n' +
      '\n' +
      '이전 평가 하네스는 우리가 코파일럿 평가 하네스로 덮으려고 하는 이 넓은 공간에 틈을 남긴다. 예를 들어 HumanEval 데이터세트 [10]에서 모델은 박사학위로부터 기능을 생성하는 능력에 대해 평가된다. 이것은 우리의 방법 생성 평가 메트릭과 유사하다. 그러나 휴먼에벌에서는 테스트 사례가 간단하고 간단하며 알고리즘 코딩 인터뷰 스타일 질문이다. 우리의 테스트 사례에서 모델은 주어진 작업을 완료하기 위해 수십 가지 다른 방법 및 파일과 상호 작용하는 실제 세계 코드에서 함수를 생성해야 한다. 이러한 수준의 복잡성은 실제 환경에서 최신 LLM의 코드 생성 능력을 적절하게 평가하기 위해 필요하다.\n' +
      '\n' +
      '다른 작업은 LLMs 자체를 사용하여 출력[11; 15; 44; 45]을 평가한다. 이것이 효과적일 수 있지만 모델의 논리나 추론에 대한 보장은 없으며 LLM의 자연 확률성은 주어진 테스트 사례에 대해 "진정한" 점수를 계산하는 것을 어렵게 만든다(즉, 모델이 실행에서 실행으로 답변을 변경할 수 있음). 에러들 및 로직 갭들은 테스트 데이터로부터 평가 결과들로 전파될 수 있다.\n' +
      '\n' +
      '평가 프레임워크를 통해 모델 생성 코드에 대한 새로운 평가 표준을 제시한다. 우리의 평가 하네스는 실제 코드를 사용하여 광범위한 프로그래밍 시나리오에 걸쳐 수백 개의 테스트 케이스에 걸쳐 신속하고 매개변수가 성능에 미치는 영향을 자동으로 이해할 수 있습니다. 이 작업 반복에서 위에 요약된 5가지 메트릭 중 문서 생성 및 버그 고정의 두 가지 세부 결과에 대해 논의한다.\n' +
      '\n' +
      '전 세계적으로 1,500만 명의 프로그래머가 사용하는 IDE인 Visual Studio Code의 유효성을 평가하기 위해 평가 프레임워크를 적용한다. 우리의 평가는 OpenAI의 GPT-3.5 및 GPT-4와 같은 독점 모델부터 코드 라마와 같은 공개적으로 이용 가능한 대안까지 LLM 모델의 스펙트럼에 걸쳐 있다. 우리는 다양한 모델 세트가 LLM 유도 프로그래밍의 능력과 한계에 대한 전체론적 관점을 제공하기 위해 필수적이며, 광범위한 개발자 청중의 요구와 선호도를 충족한다.\n' +
      '\n' +
      '그림 3. 개발자는 모델에 피보나치 코드의 오류를 수정하도록 요청하고 모델은 수정(단어 "수율"을 정확하게 맞춤법)을 diff 형식으로 제시한다.\n' +
      '\n' +
      '그림 2. 개발자는 /doc을 사용하여 피보나치 번호를 생성하는 함수에 대한 문서를 생성합니다. LLM은 diff 형식으로 강조 표시된 이 기능에 대한 설명서를 생성합니다.\n' +
      '\n' +
      'Related Work\n' +
      '\n' +
      '아래에서는 소프트웨어 엔지니어링 작업에 대한 LLMs, LLMs 평가 및 LLMs 평가에 대한 관련 작업을 기반으로 하고 확장하는 방법을 설명합니다.\n' +
      '\n' +
      '### LLMs\n' +
      '\n' +
      'LLM(Large Language Models)(Liang et al., 2017; Liu et al., 2018; Liu et al., 2019)은 인간 언어를 이해하고 생성할 수 있는 방대한 파라미터 크기를 갖는 고급 언어 모델이다. GPT-3(Liu et al., 2019), InstructGPT(Peters et al., 2019), GPT-4(Peters et al., 2019)와 같이 잘 알려진 LLMs 중 다수는 Transformer 아키텍처(Xu et al., 2019)를 활용한다. 전통적인 기계 학습 모델과 비교하여 LLM은 많은 양의 데이터와 훈련을 위한 매우 높은 하드웨어 요구 사항을 필요로 한다. 그 대가로 LLM은 유사한 작업에 대한 응답 품질을 비교할 때 기존 기계 학습 모델보다 훨씬 더 높은 성능을 제공한다. 그러나 LLM의 결과는 기존 모델보다 해석할 수 없다.\n' +
      '\n' +
      'LLMs 연구진의 성공을 기반으로 LLMs를 확장하는 이점을 탐구하기 시작했다. 예를 들어, Gropher(Gropher, 2017)에는 280억 개의 파라미터가 있고, Megatron-turing NLG(Peters et al., 2019)에는 530억 개의 파라미터가 있으며, PaLM(Liu et al., 2019)에는 5,400억 개의 파라미터가 BIGbench 벤치마크(Liang et al., 2019)에서 평균 인간을 능가한다. 유사하게, 연구자들은 또한 특정 작업 및/또는 인간 피드백에 대한 미세 조정 LLM을 탐구했다(Peters et al., 2019).\n' +
      '\n' +
      '본 연구에서는 5가지 다른 소프트웨어 엔지니어링 시나리오에서 OpenAI의 GPT-3.5, GPT-4 및 CodeLlama의 세 가지 두드러진 LLM의 성능을 조사한다. 우리는 대규모 및 독점적 특성을 고려할 때 소프트웨어 엔지니어링 작업에 적용할 수 있는 범용 언어 모델의 대표자로 오픈AI의 LLM을 선택했다. 대조적으로, 코드 관련 애플리케이션을 위해 특별히 미세 조정된 오픈 소스, 더 작고 최적화된 모델의 예시로 CodeLlama를 포함했다.\n' +
      '\n' +
      '### Evaluating LLMs\n' +
      '\n' +
      '선행 연구는 자연어 과제에서의 수행, 추론, 강건성, 안전성 등을 포함한 다양한 각도에서 LLM의 효과를 평가하였다 (Chen et al., 2019). 예를 들어, 감성 분석의 경우, (Peters et al., 2019)와 (Peters et al., 2019)는 LLMs가 전통적인 감성 분석 모델보다 훨씬 더 나은 성능을 보이는 것으로 나타났다. 마찬가지로 (Peters et al., 2019)는 질문에 대한 답변, 텍스트 요약, 코드 생성, 추론 및 윤리적 문제 해결을 포함한 다양한 작업에 대한 ChatGPT의 성능을 평가했다.\n' +
      '\n' +
      'k-폴드 교차 검증이 일반적인 평가 프로세스였던 전통적인 기계 학습 모델과 달리 LLM은 종종 정적 데이터 세트를 사용하여 평가된다. LLM을 평가하기 위한 공통 데이터 세트는 GLUE(Zhu et al., 2019), SuperGLUE(Zhu et al., 2019), BIGBench(Liang et al., 2019), MMLU(Massive Multitask Language Understanding)(Liu et al., 2019), Ethics Benchmark(Liu et al., 2019) 등을 포함한다.\n' +
      '\n' +
      '본 논문에서는 기존 연구에서 일반적으로 사용되는 BLEU와 같은 기존의 언어 기반 메트릭에서 출발한다. 대신, 우리는 소프트웨어 엔지니어링과 고려 중인 작업에 특별히 맞춤화된 메트릭을 고안합니다.\n' +
      '\n' +
      '소프트웨어 엔지니어링 작업을 위한 LLM 평가\n' +
      '\n' +
      'LLM은 코드 생성, 코드 요약, 코드 완성, 코드 검색, 코드 문서화, 코드 검토, 버그 탐지 및 소프트웨어 테스트와 같은 다양한 소프트웨어 엔지니어링 작업에 널리 사용되었다. 그러나 SE 작업에 대한 LLM의 효과와 효율성을 평가하는 것은 많은 요인과 과제가 관련되어 있기 때문에 사소한 문제가 아니다. 이 하위 섹션에서는 SE 작업에 대한 LLM에 대해 다양한 평가 방법과 메트릭을 제안하거나 적용한 기존 작업 중 일부를 검토한다.\n' +
      '\n' +
      '가장 포괄적인 작품 중 하나는 LLM과 SE의 교차점에 대한 체계적인 문헌 검토를 제공하는 논문(Liu et al., 2019)으로 데이터 수집, 전처리, 응용, 최적화, 평가, 신속한 엔지니어링 등 다양한 측면을 다룬다. 이 논문은 또한 GPT-3, CodeBERT, GraphCodeBERT와 같은 SE 태스크에 사용된 서로 다른 LLM을 분류하고 비교하고, 그들의 장단점을 분석한다. 이 논문은 또한 SE에 대한 LLM의 현재 도전과 향후 방향을 식별한다.\n' +
      '\n' +
      'CodeXGLUE(Zhu et al., 2019)는 소프트웨어 엔지니어링 태스크에서 LLMs에 대한 종합 평가 플랫폼이다. CodeXGLUE는 코드 인텔리전스 시나리오를 다루는 14개의 태스크가 있는 벤치마크 데이터 세트를 포함하고 CodeBERT 및 CodeGPT와 같은 베이스라인 모델을 제공한다. SE용 LLM의 연구 개발을 촉진하여 다양한 프로그래밍 언어 및 작업에 대한 다양한 데이터 세트를 제공하는 것을 목표로 한다. 코드XGLUE의 평가 메트릭은 리더보드 및 온라인 플랫폼과 함께 자동 및 인간 기반 모두 모델 간의 공정한 비교를 용이하게 한다.\n' +
      '\n' +
      '코드 실행과 테스트 사례를 고려하여 LLM을 평가한 첫 번째 연구 중 하나는 HumanEval, 벤치마크 데이터셋 및 코드로 훈련된 LLM의 기능적 정확성을 측정하기 위한 과제를 소개하는 논문(Liu et al., 2019)이다. HumanEval은 파이썬에서 164개의 손으로 쓴 프로그래밍 문제로 구성되어 있으며, 각각은 함수 서명, 문서 문자열, 본문 및 여러 단위 테스트를 포함한다. 문제들은 언어 이해, 알고리즘, 단순 수학 등 다양한 주제를 다루고 있으며, 그 중 일부는 단순 소프트웨어 면접 문제와 견줄 만하다. HumanEval의 목표는 주어진 테스트 사례를 통과하는 문서 문자열에서 프로그램을 합성하는 LLM의 능력을 측정하는 것이다.\n' +
      '\n' +
      '우리 연구에서 우리는 문헌에 있는 이전 작업으로 만들어진 토대를 기반으로 기여도를 높이려고 한다. HumanEval과 마찬가지로 코드 실행 및 테스트 사례에 대한 고려 사항을 통합하지만 SE 작업의 폭과 평가 메트릭의 개선 측면에서 모두 확장한다. HumanEval과 대조적으로 우리의 평가는 크고 실제적인 코드베이스를 포함한다. 또한, IDE 상호 작용 내에서 LLM 유도 프로그래밍에 대한 포괄적인 평가 프레임워크를 개발하는 데 중점을 두고 있으며, 특히 다양한 프로그래밍 언어 및 시나리오에 걸친 실용성에 중점을 둔다.\n' +
      '\n' +
      '##3. LLM-유도 소프트웨어 프로그래밍 평가\n' +
      '\n' +
      'HumanEval(Hu et al., 2017) 외에도, BLEU(Krizhevsky et al., 2017) 또는 Code-BLEU(Zhu et al., 2017)와 같은 매치 기반 메트릭들이 소프트웨어 엔지니어링 태스크들에서 LLM의 성능을 벤치마킹하기 위해 일반적으로 채택된다. LLM이 더욱 유비쿼터스화되고 강력해짐에 따라, 많은 연구들은 LLM 모델들 자체를 사용하여 LLM 출력을 평가한다(Hu et al., 2017; Li et al., 2018; Li et al., 2019; Li et al., 2019). 그러나, 이전 연구는 코드 생성(Hu et al., 2017), 코드 변환(Zhu et al., 2018) 및 기타 작업에서 생성 모델의 성공을 더 잘 반영하는 기능적 정확성과 같은 대체 메트릭을 제안한다. 이 분야의 이전 작업을 기반으로 HumanEval 하네스를 확장하고 위에 나열된 5가지 소프트웨어 엔지니어링 작업에서 IDE 통합 모델 역량을 평가한다.\n' +
      '\n' +
      '### 코드(doc)로부터의 문서 생성\n' +
      '\n' +
      '이 작업에는 메서드에 대한 설명서가 생성됩니다. 도 2는 VS Code IDE에서의 일 예를 나타낸다. 이 경우 개발자는 LLM에 /doc를 사용하여 피보나치 함수에 대한 문서를 생성하도록 요청한다.\n' +
      '\n' +
      '#### 3.1.1. Metrics\n' +
      '\n' +
      '이 시나리오에서는 생성된 텍스트의 위치, 형식 및 커버리지가 정확하면 문서 문자열 생성이 성공적이라고 간주한다. 우리는 이 시나리오에 대한 다음 메트릭을 보고한다:\n' +
      '\n' +
      '* _Syntax Correctness_: 우리는 docstring이 그 추가와 함께 파일의 신택스를 방해하지 않는 방식으로 코드에 삽입되었음을 확인한다.\n' +
      '* _Format Correctness_: 문서화 코멘트가 주어진 언어에 대해 구문적으로 허용 가능한 방식으로 배치된 경우, 우리는 리턴 성명, 그들의 유형을 갖는 함수 인수, 함수 이름 및 함수 설명이 작성되었는지 여부를 문서화하는 정확성에 대해 추가로 확인한다.\n' +
      '\n' +
      '평가절차 3.1.2\n' +
      '\n' +
      '우리는 일련의 방법들로 시작한다. 각 방법에 대해 우리는 문맥으로 LLM에 방법의 서명과 본문을 제공한다. 그런 다음 LLM에 방법에 대한 문서를 생성하라는 요청을 요청하고 생성된 문서 문자열이 함수 내의 올바른 위치에 삽입된 입력 함수를 반환한다.\n' +
      '\n' +
      'LLM이 문서를 생성하고 생성된 docstring을 코드 파일에 삽입한 후, 생성된 docstring으로 파일의 구문 정확성과 docstring 자체의 정확성을 평가한다.\n' +
      '\n' +
      '### Bug-Fixing (fix)\n' +
      '\n' +
      '이 작업은 정적 분석 도구로 식별된 버그를 수정하기 위해 LLM을 사용하는 것을 포함하며, 결과적인 고정 코드는 원래 코드보다 전반적으로 더 적은 오류를 가질 것으로 예상한다. 다음 정적 분석기를 사용합니다.\n' +
      '\n' +
      '* javascript: eslint(Brandt et al., 2016);\n' +
      '*ts: eslint(Brandt et al., 2016), tsc(typescript compiler);\n' +
      '* python: pylint(Brandt et al., 2016), pyright(Brandt et al., 2016);\n' +
      '*java: spotbugs (Brandt et al., 2016);\n' +
      '*c#: roslyn(Brandt et al., 2016);\n' +
      '* cpp: clang(Brandt et al., 2016).\n' +
      '\n' +
      '원래의 오류는 고정되어 있지만 그 자리에 또 다른 오류가 도입되면 테스트 케이스는 실패하게 된다.\n' +
      '\n' +
      '도 4는 VS Code IDE에서의 일 예를 나타낸다. 프로그래머는 "수율"이라는 단어의 철자가 틀려서 오류가 있으며, 모델은 오류를 수정한다.\n' +
      '\n' +
      '#### 3.2.1. Metrics\n' +
      '\n' +
      '이 시나리오에서는 결과 코드가 구문적으로 올바르고 해당 정적 분석 경고 또는 오류가 사라지면 버그 수정이 성공적이라고 간주한다.\n' +
      '\n' +
      '* _Syntax Correctness_: 버그 픽스가 있는 코드 파일이 구문적으로 올바른 것으로 유지됨을 확인한다.\n' +
      '* _Fix Rate_: 우리는 기존의 정적 분석 경고 또는 코드의 오류가 다른 오류를 도입하지 않고 제안된 변경 사항에 의해 성공적으로 해결되었음을 확인한다.\n' +
      '\n' +
      '3.2.2. 평가절차\n' +
      '\n' +
      '정적 분석 도구에서 발견되는 버그 집합을 고려하여 파일 내용과 진단 정보를 LLM에 제공하여 픽스를 생성한다. 우리는 모델이 원래 오류를 수정했는지, 새로운 오류를 생성했는지, 수정 후 모델 수정 코드가 구문적으로 올바르게 유지되었는지 여부를 평가한다.\n' +
      '\n' +
      '### 자연어로부터의 코드 생성(생성)\n' +
      '\n' +
      '이 작업은 자연어 설명으로부터 코드 조각 생성을 포함한다. 도 1은 VS Code IDE에서 이러한 태스크의 예를 나타낸다. 이 경우, 개발자는 LLM에게 피보나치 시퀀스의 첫 번째 \\(n\\) 값을 생성하는 함수를 작성하도록 요청하고, 에디터는 생성된 함수를 diff view로 보여준다.\n' +
      '\n' +
      '#### 3.3.1. Metrics\n' +
      '\n' +
      '코드 생성의 이전 평가와 유사하게(Hu et al., 2017), 우리는 생성된 코드가 구문적으로 올바르고 생성된 코드를 커버하는 모든 테스트 케이스가 통과하면 생성된 코드 스니펫이 성공적인 것으로 간주한다. 따라서, 우리는 이 시나리오에 대한 다음 메트릭을 보고한다:\n' +
      '\n' +
      '* _Syntax Correctness_: 구문적으로 올바른 생성된 코드의 백분율을 계산하고 보고한다. 이 메트릭을 위해, 언어별 파서(예를 들어, 각 언어에 대한 트리-시터)를 사용하여 생성된 코드의 구문 정확성을 확인한다.\n' +
      '* _Test Pass Rate_: 통과 및 실패 테스트 횟수를 확인하고 통과 테스트 비율을 계산한다. 이 숫자를 계산하기 위해 사용자 프로젝트의 전체 테스트 세트를 실행하고 모델의 코드 주입 전에 통과된 테스트 실패 여부를 추적한다.\n' +
      '\n' +
      '###### 3.3.2. 평가절차\n' +
      '\n' +
      '테스트 케이스가 있는 저장소 세트부터 시작합니다. 각 리포지토리에서 1) 주어진 리포지토리의 테스트 스위트에서 테스트 케이스에 의해 커버되는 방법과 2) 문서 문자열이 있는 방법을 선택한다. 각 방법에 대해 우리는 LLM에 메소드의 서명과 문서 문자열이 주어진 메소드의 본문을 생성하도록 요청한다. 우리는 메서드 파일의 내용을 LLM에 컨텍스트로 제공하여 원래 메서드 본문을 "당신의 코드 여기"라는 주석 선으로 대체한다.\n' +
      '\n' +
      'LLM이 메소드 본문을 생성한 후 생성된 코드를 원래 메소드 본문 대신 다시 넣고 새로운 메소드 본문에 대해 리포지토리의 테스트 슈트를 실행하여 코드를 평가한다. 그런 다음 위에서 설명한 대로 구문 정확도와 테스트 통과율을 계산하고 보고한다.\n' +
      '\n' +
      '### 코드를 위한 테스트 케이스 생성(테스트)\n' +
      '\n' +
      '이 작업은 코드에 대한 테스트 케이스를 생성하기 위해 LLM을 사용하는 것을 포함한다. 개발자는 일반적으로 단위 테스트를 작성하는 데 있어 지름길입니다. 테스트 생성을 자동화하면 더 많은 개발자가 단위 테스트를 포함하도록 동기를 부여할 수 있다. 도 5는 VS Code IDE에서 테스트를 요청하는 개발자의 예를 나타낸다. 예시적인 경우에, 개발자는 LLM에 /테스트 채팅 시나리오 명령을 사용하여 피보나치 기능에 대한 테스트를 생성하도록 요청한다.\n' +
      '\n' +
      '#### 3.4.1. Metrics\n' +
      '\n' +
      '이 시나리오에서, 우리는 생성된 테스트가 구문적으로 올바르고 실행을 전달할 수 있는 경우 성공적인 것으로 간주한다. 이 평가를 위해, 이것은 우리가 시험이 쓰여진 코드가 옳다고 가정한다는 것을 의미한다.\n' +
      '\n' +
      '* _Syntax Correctness_: 구문적으로 정확한 생성된 테스트의 백분율을 계산한다. 언어별 구문 분석기를 사용하여 생성된 테스트의 구문 정확성을 확인한다.\n' +
      '* _Generated Test Pass Rate_: 생성된 Test의 Pass rate를 계산한다. 우리는 원래 방법이 정확하다고 가정하고 생성된 테스트를 초점 방법에 실행합니다.\n' +
      '\n' +
      '평가절차 3.4.2\n' +
      '\n' +
      '방법 세트가 주어지면 각 초점 방법에 대한 테스트를 생성하기 위해 LLM에 컨텍스트로 방법 서명, 문서 문자열 및 본문을 제공한다.\n' +
      '\n' +
      'LLM이 메소드에 대한 테스트를 생성하면 메소드를 포함하는 리포지토리에 테스트를 추가하고 테스트를 실행하려고 시도한다.\n' +
      '\n' +
      'Javascript 및 Typescript의 경우 Jest 또는 Mocha 라이브러리를 사용하여 테스트를 생성합니다. 리포지토리의 원본 테스트 스위트는 두 라이브러리로 작성될 필요는 없지만, 각 메서드의 원본 파일은 사소한 테스트 케이스(본질적으로 사실만을 주장함)가 파일에 추가될 때 오류 없이 통과할 수 있어야 합니다. 생성된 테스트를 평가할 때 임시로 초점 메서드의 파일에 추가하여 가져오기 오류를 완화하고 전체 파일을 실행합니다. 사소한 테스트 케이스(예: 항상 참이어야 하는 테스트)가 추가된 파일을 실행하면 거짓 또는 오류가 반환되는 경우 해당 파일에서 생성된 테스트 결과가 신뢰할 수 없다는 것을 알 수 있습니다.\n' +
      '\n' +
      '### 작업공간의 이해와 질의해결(작업공간)\n' +
      '\n' +
      '워크스페이스 태스크에서, 우리는 모델에 사용자의 자연어 질의를 제공하고, 사용자의 질문에 응답하는 데 도움이 될 수 있는 코드베이스의 관련 스니펫들을 식별하도록 요청한다. 이것은 사용자의 자연어 요청과 대량의 코드를 모두 이해할 수 있는 모델의 능력을 테스트한다.\n' +
      '\n' +
      '#### 3.5.1. Metrics\n' +
      '\n' +
      'LLM의 검색된 스니펫의 품질을 두 가지 방법으로 평가한다:\n' +
      '\n' +
      '* 평균 상호 순위(MRR): 모델의 검색된 스니펫의 순위가 매겨진 리스트가 주어지면, 우리는 \\(\\frac{1}{r}\\)을 계산하며, 여기서 \\(r\\)은 모델의 리스트에서 정확한 스니펫의 순위이다. 모델이 스니펫을 2위로 올리면\n' +
      '\n' +
      '그림 4. 개발자는 모델에 피보나치 코드의 오류를 수정하도록 요청하고 모델은 수정(단어 "수율"을 정확하게 맞춤법)을 diff 형식으로 제시한다.\n' +
      '\n' +
      '그 시험 사례에 대한 모델의 점수는 \\(\\frac{1}{2}\\)이라고 생각할 것이다. MRR은 모든 테스트 사례 점수의 평균이다.\n' +
      '* End to End Keyword Detection: 우리는 수동으로 생성된 사용자 질의 데이터세트 및 질의에 대한 정답과 연관된 키워드들로 시작한다. 검색된 스니펫의 모델 순위 목록을 추출하여 각 사용자 쿼리와 함께 모델에 전달한다. 그런 다음 쿼리 및 검색된 결과를 모두 고려하여 관련 키워드가 모델의 응답에 나타나는지 여부를 감지한다.\n' +
      '\n' +
      '3.5.2. 평가절차\n' +
      '\n' +
      '각 데이터포인트에 대해, 우리는 LLM에 사용자 질의와 주어진 질의와 연관된 코드베이스의 전체 컨텍스트를 제공한다. 우리는 LLM에 코드베이스에서 관련 코드 스니펫의 순위 목록을 검색할 것을 요청한다. 우리는 모델이 검색 중에 가장 관련성이 높은 코드 스니펫을 얼마나 잘 찾을 수 있는지를 점수화하는 메트릭인 MRR을 사용하여 모델의 검색된 결과의 품질을 직접 평가한다.\n' +
      '\n' +
      '또한, 질의와 스니펫을 컨텍스트로 제공하여 모델에게 원본 사용자 질의에 응답하도록 요청함으로써 검색된 모든 코드 스니펫의 품질을 평가한다. 우리는 주어진 질의와 관련된 키워드 세트에 대한 모델의 최종 응답을 검색하여 모델이 질문에 완전히 답하는 데 필요한 정보를 찾을 수 있는지 여부를 결정한다.\n' +
      '\n' +
      '이 메트릭을 사용하여 엔드 투 엔드 스케일에서 모델의 검색 능력을 평가하고 실제 질문에 답하는 데 도움이 되는 코드 스니펫을 찾는 모델의 기술을 결정한다.\n' +
      '\n' +
      '##4. 코파일럿 평가 하니스\n' +
      '\n' +
      '위에서 설명한 대로 평가 메트릭을 계산하기 위한 엔드 투 엔드 코파일럿 평가 하니스를 소개합니다. 먼저 각 평가에 필요한 자료 수집 내역을 공유합니다. 그런 다음 각 언어가 주어진 테스트 환경을 만드는 과정과 테스트 구축 및 실행의 필요성에 대해 설명한다. 마지막으로, 각 메트릭에 대한 평가 프로세스에 대한 추가 특정 구현 세부 정보를 제공한다.\n' +
      '\n' +
      '### Data Collection\n' +
      '\n' +
      '데이터 세트는 JavaScript, Typescript, Python, Java, C/C++ 및 C#의 6개 언어에 걸쳐 수백 개의 공용 GitHub 리포지토리의 방법으로 구성된다. 일부 평가에는 테스트 사례와 관련된 리포지토리에 대한 테스트를 구축하고 실행할 수 있는 기능이 필요합니다. 이 요구 사항을 충족하기 위해 임의의 저장소에서 다양한 빌드 및 테스트 전략을 시도하는 평가 하네스의 일부로 빌드 에이전트를 개발했다. 또한, 구축 및 테스트할 수 있는 리포지토리에서 정적 분석 도구를 실행할 수 있는 기능이 있습니다. 이 빌드 에이전트는 테스트 데이터 세트를 수집하고 평가를 수행하는 데 필수적이다.\n' +
      '\n' +
      '각 언어에 대해 코드를 만들 수 있고 테스트 스위트가 있는 기쓰브 공용 저장소에서 샘플을 추출합니다.\n' +
      '\n' +
      '그림 5. 개발자는 피보나치 숫자를 생성하는 함수에 대한 테스트를 생성하기 위해 /테스트를 사용한다. LLM은 테스트 파일에서 이 함수에 대한 test_fibonacci 함수를 생성한다.\n' +
      '\n' +
      '빌드 에이전트를 사용하여 실행할 수 있습니다. 빌드 에이전트는 Node 18+, Python 3.8+, Java JDK 1.8(requiring Maven),.NET 6.0, 7.0 및 8.0, 그리고 수동으로 큐레이션된 C++ 리포지토리 세트를 지원한다. 우리는 C++ 빌드 단계의 광범위한 가변성으로 인해 수동으로 C++ 리포지토리를 수집하는 데 의존했다. 우리는 1MB보다 작고 100MB보다 큰 리포지토리를 무시한다. 테스트를 만들고 실행하는 데 10분 이상 걸리는 리포지토리를 무시합니다. 마지막으로 방법이 포함되지 않은 리포지토리를 무시합니다.\n' +
      '\n' +
      '######4.1.1.1. Javascript and Typescript\n' +
      '\n' +
      'Javascript와 Typescript에서, 우리는 루트 디렉토리에 _package.json_ 파일을 포함하는 리포들을 서브-선택한다. _package.json_ 파일은 npm(Node Package Manager)과 일치하여 작동하여, 설치를 위한 종속성을 지정하고 테스트 스위트를 실행하는 것과 같은 repo 내의 다양한 작업을 처리한다. 우리는 자바스크립트 및 타이프스크립트 코드의 평가를 위해 npm에 의존하므로 인프라가 npm으로 관리되도록 구축된 저장소만 고려한다.\n' +
      '\n' +
      '#### 4.1.2. Java\n' +
      '\n' +
      '자바에서는 메이븐을 빌드 프로세스에 활용하는 리포지토리를 고려합니다. 또한 글쓰기 기준으로 JDK 1.8을 사용하는 프로젝트만 고려하고 있습니다.\n' +
      '\n' +
      '#### 4.1.3. Python\n' +
      '\n' +
      '파이썬에서는 가상 환경 내에서 모든 종속성을 성공적으로 설치할 수 있는 리포지토리만 고려합니다.\n' +
      '\n' +
      '#### 4.1.4. C/C++\n' +
      '\n' +
      'C/C++에서는 건물 프로젝트에 클랭을 활용합니다. C/C++ 리포지토리를 구축할 수 있는 방법이 매우 다양하기 때문에, 도커 이미지 내에서 구축 및 테스트할 수 있음을 검증한 수동 큐레이션 리포지토리 세트를 제시한다.\n' +
      '\n' +
      '테스트 케이스 수집\n' +
      '\n' +
      '각 언어에 적합한 리포지토리를 식별한 후, 리포지토리 내의 코드를 기반으로 각 평가 메트릭에 대한 테스트 케이스를 생성한다. 대부분의 평가는 기존 테스트에 적용되거나 정적 분석 도구의 경고를 포함하는 등 특정 조건을 충족하는 방법을 식별해야 한다. 평가 테스트 사례를 생성하는 기준은 메트릭마다 다르며, 아래 각 메트릭에 대해 설명한다.\n' +
      '\n' +
      '####4.2.1. 코드(doc)로부터의 문서 생성\n' +
      '\n' +
      '리포지토리에서 3개 라인보다 길고 미니화 또는 난독화의 결과가 아닌 방법을 식별하여 테스트 케이스를 생성한다. 우리는 방법을 제공하고 평가 중인 코딩 어시스턴트에게 방법에 대한 문서 문자열을 생성하도록 요청한다. 생성된 텍스트의 위치, 형식 및 커버리지가 정확하면 문서 문자열 생성이 성공적이라고 간주한다.\n' +
      '\n' +
      '####4.2.2. 버그 고침(고침)\n' +
      '\n' +
      '정적 분석 도구 경고와 지정된 리포지토리에 플래그가 지정된 오류를 기반으로 테스트 케이스를 만듭니다. 이러한 문제는 하나의 파일만으로는 해결하기 어렵기 때문에 가져오기 또는 구성과 관련이 없는 정적 분석 경고만 컨텍스트로 간주한다. 생성된 픽스가 구문적으로 올바르고 실행 시 정적 분석 경고의 수를 엄격하게 줄이면 성공적인 것으로 간주한다. 개발자가 완전한 수정으로 간주하지 않는 새로운 문제를 도입하면서 코딩 보조자가 원래 문제를 수정하는 것이 가능하기 때문에 원래 경고나 오류가 존재하기보다는 엄격한 감소를 고려해야 한다.\n' +
      '\n' +
      '####4.2.3. 자연어로부터의 코드 생성(생성)\n' +
      '\n' +
      '기존 통과 테스트에서 다루어지는 주어진 리포지토리에서 메소드를 식별하여 테스트 케이스를 생성한다. 테스트 케이스는 메서드 서명을 포함하여 전체 파일의 코딩 보조자에게 가시성을 제공합니다. 그 후, 코딩 어시스턴트는 방법 서명과 연관된 방법 본문을 생성하도록 요청된다. 우리는 생성된 코드가 구문적으로 올바르고 생성된 코드를 포함하는 모든 테스트 케이스가 통과하면 생성된 코드 조각이 성공적이라고 생각한다.\n' +
      '\n' +
      '####4.2.4. 코드로부터의 테스트 생성(테스트)\n' +
      '\n' +
      '우리는 주어진 리포지토리 내에서 메서드를 식별하여 테스트 케이스를 생성한다. 코딩 어시스턴트에게 주어진 방법에 대한 작업 테스트를 제공할 것을 요청합니다. 우리는 생성된 테스트가 주어진 방법을 호출하고 실행을 통과하면 성공적인 것으로 간주한다.\n' +
      '\n' +
      '####4.2.5. 작업영역 이해 및 질의해결(작업영역)\n' +
      '\n' +
      '우리는 특정 기능을 구축하는 관용적인 방법과 같은 프로젝트 작업 공간의 특정 측면에 대한 개발자의 질문을 수집했다. 작업영역 명령의 일부로 발생하는 컨텍스트 가져오기는 여러 개의 관련 코드 스니펫을 반환합니다. 위에서 설명한 대로 MRR을 사용하여 LLM의 검색된 스니펫의 품질을 평가한다.\n' +
      '\n' +
      '## 5. Experiments\n' +
      '\n' +
      'Copilot 평가 하네스 메트릭과 위에서 설명한 테스트 사례를 사용하여 700K 이상의 활성 사용자를 코드 어시스턴트로 사용하는 VSCode IDE에서 LLM 전원 채팅 확장을 사용하여 문서 생성 및 버그 수정 시나리오에서 코드라마뿐만 아니라 GPT-3.5 및 GPT-4의 두 가지 OpenAI 모델의 성공을 계산한다.\n' +
      '\n' +
      '우리의 실험은 다음과 같은 연구 질문에 답하는 것을 목표로 한다.\n' +
      '\n' +
      '**RQ1. 모델 비교**: 코딩 어시스턴트와 통합될 때 서로 다른 LLM이 어떻게 비교되는가?\n' +
      '**RQ2. 통합 개선**: 코파일럿 평가 하네스가 엔지니어들에게 코딩 어시스턴트에서 LLM의 통합을 개선할 수 있는 어떤 통찰력을 제공할 수 있는가?\n' +
      '**RQ3. Data Validity**: 우리의 평가 테스트 사례들이 LLM 파워 코딩 어시스턴트의 실제 사용과 어떻게 비교되는가? 우리 하네스의 테스트 사례는 실제 사용자가 LLM 전원 코딩 보조자와 어떻게 상호 작용하는지를 반영합니까?이 섹션에서는 이러한 연구 문제와 관련된 결과에 대해 논의한다.\n' +
      '\n' +
      '### RQ1. 모델 비교\n' +
      '\n' +
      '아래에서는 VSCode에서 목표 채팅 확장에 전원을 공급하는 데 사용될 때 세 가지 최신 LLM을 비교하는 학습에 대해 논의한다.\n' +
      '\n' +
      '#### 5.1.1. 코드(doc)로부터의 문서 생성\n' +
      '\n' +
      '표 1은 문서 스트링 생성에 대해 GPT-4가 일반적으로 GPT-3.5 및 코드 라마를 능가함을 보여준다. GPT-3.5와 GPT-4는 코드 라마가 약간 뒤처진 채 서로 성능이 매우 유사하다. 여기서 핵심 예외는 코드 라마가 GPT-4보다 약간 더 높은 수준에서 수행하는 파이썬과 코드 라마가 훨씬 더 나쁜 성능을 수행하는 C/C++이다. 한 가지 가능한 설명은 GPT-3.5와 GPT-4가 인터넷에서 많은 오픈 소스 코드를 포함하는 대규모 말뭉치에 대해 훈련되었다는 것이다. 따라서 GPT 모델의 성능은 다양한 코드 패턴을 보았기 때문에 부풀릴 수 있다. 비교적 작은 모델인 코드 라마는 주어진 코드 조각이 GPT 모델에 비해 잠재적으로 성능을 방해할 가능성이 훨씬 적다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c}  & \\multicolumn{3}{c}{**Ex**} \\\\ \\hline\n' +
      '**Language** & **Model** & **Syntax** & **Bugs** \\\\  & & **Correctness** & **Fixed** \\\\ \\hline \\multirow{3}{*}{Python} & GPT-4 & 96\\% & 74\\% \\\\  & GPT-3.5 & 93\\% & 68\\% \\\\  & CodeLlama & 88\\% & 39\\% \\\\ \\hline \\multirow{3}{*}{Javascript} & GPT-4 & 92\\% & 81\\% \\\\  & GPT-3.5 & 85\\% & 74\\% \\\\  & CodeLlama & 39\\% & 26\\% \\\\ \\hline \\multirow{3}{*}{Typescript} & GPT-4 & 83\\% & 75\\% \\\\  & GPT-3.5 & 74\\% & 75\\% \\\\  & CodeLlama & 70\\% & 30\\% \\\\ \\hline \\multirow{3}{*}{C\\#} & GPT-4 & 98\\% & 58\\% \\\\  & GPT-3.5 & 96\\% & 65\\% \\\\ \\cline{1-1}  & CodeLlama & 84\\% & 50\\% \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1. Doc 시나리오의 특정 성공 메트릭에 대해 Python, Javascript, Typescript, Java, C# 및 C/C++에 걸친 Doc 채팅 시나리오에 대한 LLMs 성능.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c}  & \\multicolumn{3}{c}{**Doc**} \\\\ \\hline\n' +
      '**Language** & **Model** & **Syntax** & **Format** \\\\  & & **Correctness** & **Correctness** \\\\ \\hline \\multirow{3}{*}{Python} & GPT-4 & 100\\% & 83\\% \\\\  & GPT-3.5 & 100\\% & 87\\% \\\\  & CodeLlama & 100\\% & 87\\% \\\\ \\hline \\multirow{3}{*}{Javascript} & GPT-4 & 83\\% & 100\\% \\\\  & GPT-3.5 & 83\\% & 100\\% \\\\  & CodeLlama & 79\\% & 55\\% \\\\ \\hline \\multirow{3}{*}{Typescript} & GPT-4 & 96\\% & 79\\% \\\\  & GPT-3.5 & 96\\% & 86\\% \\\\  & CodeLlama & 77\\% & 65\\% \\\\ \\hline \\multirow{3}{*}{Javascript} & GPT-4 & 100\\% & 93\\% \\\\  & GPT-3.5 & 100\\% & 80\\% \\\\  & CodeLlama & 100\\% & 64\\% \\\\ \\hline \\multirow{3}{*}{C\\#} & GPT-4 & 100\\% & 89\\% \\\\  & GPT-3.5 & 100\\% & 75\\% \\\\ \\cline{1-1}  & CodeLlama & 94\\% & 67\\% \\\\ \\hline \\multirow{3}{*}{C/C++} & GPT-4 & 92\\% & 94\\% \\\\ \\cline{1-1}  & GPT-3.5 & 92\\% & 77\\% \\\\ \\cline{1-1}  & CodeLlama & 90\\% & 38\\% \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2. 특정 Fix 성공 메트릭에 대한 파이썬, 자바스크립트, 타이프스크립트 및 C#에 걸친 Fix 채팅 시나리오에 대한 LLMs 성능.\n' +
      '\n' +
      '그림 6. 통과 수정 테스트 케이스에 대한 예제 프롬프트 및 응답입니다.\n' +
      '\n' +
      '####5.1.2. 버그-고정(fix)\n' +
      '\n' +
      '표 2는 버그 고정에 대한 결과를 보여준다: docstring 생성 평가와 유사하게 GPT-4는 GPT-3.5를 약간 능가하는 경향이 있으며 코드 라마는 더 뒤에 있다. 버그 고정의 경우 예외는 세 가지 모델 모두가 어려움을 겪는 것으로 보이는 C#인 것으로 보이며 GPT-3.5는 궁극적으로 GPT-4 및 코드 라마 모두를 능가한다. 그림 6과 그림 7은 GPT-4와 함께 통과 및 실패한 예를 보여준다. 그림 6에서 모델은 \\(if\\) 문을 추가하여 \\(self\\_writers\\)이 없는지를 확인하고 반복 가능한 것으로 사용하려고 시도하여 오류를 해결한다. 그림 7에서 모델은 마찬가지로 \\(Hero\\_age\\)이 없음인지에 대한 검사를 추가한다. 그러나, 에러를 야기하고 있는 동작 후에 체크를 추가하기 때문에, 에러가 계속 발생한다. 모델이 잠재적인 수정을 식별할 수 있었지만 잘못된 위치에 검사를 삽입하고 버그를 수정할 수 없었다.\n' +
      '\n' +
      'GPT-3.5 LLM과 GPT-4 LLM 간의 결과가 다른 일반적인 원인은 그림 8에서 볼 수 있듯이 모델이 "any" 유형의 오류를 해결하려고 할 때 발생한다.\n' +
      '\n' +
      '그림 8. GPT-3.5와 GPT-4가 모두 실패해야 하지만 GPT-3.5보다 버그를 고정하는 데 더 미묘한 접근을 시도하기 때문에 GPT-4만 실패하는 동일한 테스트 사례에 대한 예제 프롬프트 및 응답.\n' +
      '\n' +
      '도 7. 실패한 수정 채팅 시나리오에 대한 예제 프롬프트 및 응답. 여기에서 모델은 문제가 무엇인지 결정하고 문제를 해결하려고 시도합니다. 그러나 픽스가 정확하지 않고 동일한 구문 오류가 여전히 존재하는데, 이는 나이가 없는지 확인하기 전에 > 작업을 시도하기 때문이다.\n' +
      '\n' +
      'GPT-4 모델은 _res_라는 변수의 유형을 지정하려고 시도하며, 유형을 \\(Electron\\)으로 예측한다. (Message\\)\\(Box\\)\\(Return\\)\\(Value\\)을 이용하여 변수를 유형화하였다. 그러나 해당 유형은 코드에 대한 올바른 반환 유형이 아닙니다. 반면에 GPT-3.5는 변수를 유형 _any_로 캐스팅하여 더 복잡한 문제를 우회하지만 코드 냄새에 남긴다(변수를 유형 _any_로 캐스팅하는 것은 좋은 관행이 아니기 때문에). 이 경우 GPT-4의 시도된 수정이 더 미묘하고 고급임에도 불구하고 GPT-4는 실패하는 반면 GPT-3.5는 우리의 평가를 통과한다. GPT-3.5가 성공하고 GPT-4가 실패하는 경우를 자세히 살펴보면, GPT-4는 더 복잡한 접근법으로 실패하는 반면 GPT-3.5는 기술적으로 통과하지만 초보적이고 차선책인 이 현상을 자주 볼 수 있다.\n' +
      '\n' +
      '### RQ2. 통합 개선 사항\n' +
      '\n' +
      '아래에서는 평가 하네스를 사용하여 LLM을 IDE와 더 잘 통합하는 방법에 대한 통찰력을 배울 수 있는 방법에 대해 논의한다.\n' +
      '\n' +
      '#### 5.2.1. 코드(doc)로부터의 문서 생성\n' +
      '\n' +
      '표 1의 결과를 추가로 검사하면 문서 문자열 생성 평가가 실패하는 4가지 클래스의 오류가 나타난다.\n' +
      '\n' +
      '1. **Code Logic Changes:** 모델은 생성된 docstring과 함께 파일에 초점 함수를 재기입할 때 코드의 기본 논리를 변경한다.\n' +
      '2. **Syntax Changes:** 모델은 파일에 기록할 때 초점 코드의 구문을 변경한다. 여기에는 행 끝에 세미콜론을 추가하거나 기능 서명에 유형 데코레이터를 추가하는 등의 변경 사항이 포함됩니다.\n' +
      '3. **Incomplete Docstrings:** 모델은 정확한 함수에 대한 설명을 생성하지만, 반환되는 객체 및 함수의 모든 파라미터에 대해서는 설명하지 않는다.\n' +
      '4. **관련없는 닥스트링:** 모델은 우리가 문서화하도록 요청한 코드 블록에 관련되지 않은 닥스트링을 반환한다.\n' +
      '\n' +
      '자세히 검사하면 GPT-3.5 또는 GPT-4 중 하나만 통과하는 경우 GPT-4 모델이 GPT-3.5보다 코드를 더 깨끗하게 만드는 초점 코드를 변경할 가능성이 더 높다는 것을 알 수 있다. 예를 들어, 그림 9에서 GPT-4 모델은 함수의 입력에 유형 데코레이터를 추가하고 리턴 유형을 지정한다. GPT-4 모델의 docstring이 정확하지만(그리고 GPT-3.5 모델보다 더 상세하지만), GPT-4 모델은 초점 코드에 변화가 없을 것으로 예상하기 때문에 이 테스트 케이스에 실패한다. 그러나 이러한 오류는 더 관련된 개선에 대한 GPT-4 모델의 시도가 더 나쁜 성능을 나타내지 않으면서 점수를 감소시킬 수 있음을 보여준다.\n' +
      '\n' +
      '이 발견을 기반으로 모델에 초점 코드를 변경하지 않도록 구체적으로 알려주는 코딩 보조자의 문서 문자열 생성 프롬프트에 추가 명령을 삽입했다. 이는 C++의 5%에서 자바의 11%에 이르는 모든 언어에 대한 평가 결과의 상당한 개선을 가져왔다.\n' +
      '\n' +
      '우리는 또한 GPT-4가 특정 지침을 따르는 것이 더 낫다는 것을 안다. 이것은 그림 10의 예에 의해 강조되며, GPT-4는 _get_colour_at_ 함수 대신에 Vec 클래스에 대한 설명서를 생성한다. 처음에, 이것은\n' +
      '\n' +
      '도 9. 의사 평가를 위한 예시 프롬프트 및 응답. GPT-3.5가 통과되었고, GPT-4는 매개 변수와 반환 유형으로 원래 함수를 반환했기 때문에 실패했습니다. 이것은 모델이 초점 코드를 변경하지 않고 그대로 둘 것을 요구하기 때문에 우리의 평가에 실패한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '문서 생성을 위해 OpenAI의 ada 임베딩 모델 [3]을 사용하여 문서화된 코드 스니펫을 임베딩하고 데이터 세트의 스니펫과 마이크로소프트 개발자가 사용한 데이터를 비교한다. 유사하게, 버그 고정 원격 측정을 위해, 우리는 버그를 포함하는 코드 조각들을 내장한다. 우리는 PCA 차원 축소를 사용하여 데이터를 두 차원으로 표시한다. PCA 차원 감소는 점과 특이치 사이의 거리를 최대화하는 평면을 찾기 위해 최적화된다. 도 12 및 도 13은 이러한 비교의 결과를 나타낸다. 각 언어는 클러스터를 형성하고 실제 사용량과 데이터는 문서 생성과 버그 고정을 위해 각 언어 클러스터에 대해 유사한 공간 내에 존재한다는 것을 알 수 있다.\n' +
      '\n' +
      '우리는 데이터 세트의 테스트 케이스를 포인트의 실제 사용 포인트와 일치시키는 것을 목표로 하지 않는다. 오히려, 우리는\n' +
      '\n' +
      '그림 11. GPT-3.5의 예제 실패 프롬프트 및 응답. 모델은 변수 이름과 일치하는 유형(토큰)을 환각합니다.\n' +
      '\n' +
      '그림 12. 문서 생성 평가를 위한 데이터 세트를 언어에 걸친 실제 사용과 비교한다.\n' +
      '\n' +
      '그림 11. GPT-3.5의 예제 실패 프롬프트 및 응답. 모델은 변수 이름과 일치하는 유형(토큰)을 환각합니다.\n' +
      '\n' +
      '그림 13. Bug Fixing 평가를 위한 데이터 세트를 언어에 걸친 실제 사용과 비교한다.\n' +
      '\n' +
      '우리의 테스트 케이스가 실제 사용 공간의 이상치인지 여부를 나타냅니다. 그들이 특이치가 아니라면, 우리는 우리의 데이터 세트가 채팅 확장자의 실제 사용과 일치한다고 추론할 수 있다. 이 분석을 통해 문서 생성 및 버그 수정 평가를 위한 데이터 세트가 실제 사용량과 일치함을 알 수 있다.\n' +
      '\n' +
      '##6. 결론 및 향후 작업\n' +
      '\n' +
      '복잡한 엔지니어링 작업에서 개발자를 돕기 위해 LLM의 사용이 증가함에 따라 LLM 생성 코드에 대한 보다 강력한 평가가 필요하다. 특히 더 많은 회사와 제품이 LLM을 워크플로우에 통합하려고 할 때 기존 평가 메트릭은 기계 생성 코드의 품질과 정확성을 확인하기에 충분하지 않다. 본 논문에서는 코파일럿 평가 하니스(Copilot Evaluation harness)를 통해 이 문제에 대한 해결책을 제안한다. 코드 생성 문제 공간에 대한 5가지 주요 평가 메트릭인 메소드 생성, 테스트 생성, 문서 문자열 생성, 버그 수정 및 작업 공간 이해도를 정의한다. 이 다섯 가지 메트릭 각각에 대한 테스트 사례 및 평가 결과를 수집하는 데 필요한 방법론을 자세히 설명한다. 또한 무수한 프로그래밍 언어에 걸쳐 5가지 메트릭 중 2가지 메트릭에 대한 예비 결과를 제공합니다.\n' +
      '\n' +
      '평가 하네스를 만드는 우리의 목표는 LLM 생성 코드의 품질을 검증하는 것이다. 우리는 코드 생성 ML 공간에서 엄청난 발전을 보았지만 LLM을 코드 워크플로에 안정적이고 최적으로 통합하기 위해 얼마나 많은 감독 및 엔지니어링 노력이 필요한지 강조하려고 한다. 우리는 개발자에게 LLM의 코딩 워크플로우 통합을 최적화할 수 있는 포괄적인 평가 제품군을 제공하는 것을 목표로 합니다. 코파일럿 평가 하네스를 사용하면 프로그래머는 신속한 표현, 제공된 정보 순서의 변경, 모델에 제공된 컨텍스트의 변경 등과 같은 매개변수의 영향을 보다 체계적이고 강력하게 평가할 수 있다.\n' +
      '\n' +
      '또한, 코파일럿 평가 하네스는 보다 예산 친화적인 LLM 모델(예: CodeLLama)이 문서화와 같은 작업에서 만족스러운 성능을 나타낼 수 있음을 밝혀 비용 최적화에 사용할 수 있다. 이러한 통찰력은 개발자가 성능이 충분하다고 판단될 때 비용 효율적인 LLM에 작업을 할당하여 자원을 지능적으로 균형을 맞출 수 있도록 한다. 동시에 보다 복잡한 작업을 보다 강력한 LLM으로 전환하여 최적의 결과를 보장할 수 있습니다.\n' +
      '\n' +
      '우리는 이 논문을 진행 상황에 대한 살아있는 문서로 출판합니다. 이 프로젝트에 대한 향후 작업은 나머지 세 가지 평가 메트릭에 대한 결과를 보고하고 데이터 및 평가 코드를 공개 소싱하는 것을 포함한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*(1)Clang. [https://clang.llvm.org/] (https://clang.llvm.org/).\n' +
      '*(2)Eslint. [https://eslint.org] (https://eslint.org).\n' +
      '*(3)Openai ada. [https://openai.com/blog/new-and- improved-embedding-model] (https://openai.com/blog/new-and- improved-embedding-model).\n' +
      '*(4)Pylint. [https://pylint.pycqa.org/en/latest/user_guide/usage/run.html] (https://pylint.pycqa.org/en/latest/user_guide/usage/run.html).\n' +
      '*(5)저작권. [https://github.com/마이크로소프트/저작권] (https://github.com/microsoft/pyright).\n' +
      '*(6)Roslyn. [https://github.com/dotnet/roslyn-analyzers] (https://github.com/dotnet/roslyn-analyzers).\n' +
      '*(7)Spotbugs. [https://spotbugs.github.io/] (https://spotbugs.github.io/).\n' +
      '*(8)Chiang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zitu, K., Chen, H., Yi, X., Wang, C., Wang, Y., Ye, W., Zhang, Y., Chang, Y., Yu, P. S., Yang, Q., and Xie, X. 2023년, 대규모 언어 모델 평가에 대한 설문 조사.\n' +
      '*(9)Chien, B., Mustakin, N., Hoang, A., Fuad, S., and Wong, D. Vesuda: Llm based cuda extension for visual studio code. In _Proceedings of the SC 23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis_(New York, NY, USA, 2023), SC-W\'23, Association for Computing Machinery, pp. 11-17.\n' +
      '*(10)Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., et al. 코드 상에서 트레이닝된 대형 언어 모델들을 평가하는 단계를 포함한다.\n' +
      '*(11)Chen, Y., Wang, R., Jiang, H., Shi, S., and Xu, R. 참조 없는 텍스트 품질 평가를 위한 대규모 언어 모델의 사용 탐색: 2023년 실증 연구.\n' +
      '*(12)Chowdhiery, A., Narang, S., Devlin, J., Bosma, M., and Others. 팜: 2022년 경로가 있는 언어 모델링 스케일링\n' +
      '*(13)Devlin, J., Chang, M. - W., Lee, K., and Toutanova, K. Bert: 언어 이해를 위한 딥 양방향 변압기의 사전 훈련.\n' +
      '*(14)Floonth, L., and Chiriatti, M. Gpt-3: 그 성질, 범위, 한계 및 결과. _ Minds and Machines 30_(2020), 681-694.\n' +
      '*(15)Fu, J., Ng, S. - K., Jiang, Z., and Liu, P. Gptscore: 당신이 원하는 대로 평가하세요, 2023.\n' +
      '*(16)Gao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners, 2021.\n' +
      '*(17)Hendrycks, D., Burns, C., Basart, S., Caritch, A., Li, J., Song, D., and Steinhardt, J. Aligning ai with shared human values, 2023.\n' +
      '*(18)Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazrika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding, 2021.\n' +
      '*(19)Hou, X., Zhao, Y., Liu, Y., Yang, Z., Wang, K., Li, L., Luo, X., Lo, D., Grundy, J., and Wang, H. Large language models for software engineering: systematic literature review. _ arXiv preprint arXiv:2308.10620_(2023).\n' +
      '*(20)Komberink, S., Mikolov, T., Karafiat, M., and Burger, I. Recurrent neural network based language modeling in meeting recognition. In _Interspeech_ (2011), vol. 11, pp. 2877-2880.\n' +
      '*(21)Laskar, M. T. R., Bar, M. S., Rahman, M., Bhuiyan, M. A. H., Joty, S., and Huang, J. X. A systematic study and comprehensive evaluation of chatgpt on benchmark datasets, 2023.\n' +
      '*(22)Liang, P., Bommasani, R., Lee, T., Tsipras, D., and Others. 언어 모델의 전체론적 평가, 2023년\n' +
      '*(23)Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in middle: How language models use long context, 2023.\n' +
      '*(24)Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tuzano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K., Fu, S., and Liu, S. Codexglue: 코드 이해 및 생성을 위한 머신 러닝 벤치마크 데이터세트, 2021.\n' +
      '*(25)Nam, D., Macvean, A., Helliendoorn, V., Vasilescu, B., and Myers, B. In-ide generation-based information support with large language model, 2023.\n' +
      '*(26)OpenAI. Gpt-3 cmodels, 2023\n' +
      '*(27)OpenAI. Gpt-4 기술 보고서, 2023\n' +
      '*(28) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishra, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. training language models to follow instructions with human feedback. _ 신경 정보 처리 시스템(35_(2022), 27730-27744)에서의 발전.\n' +
      '\n' +
      '*(29)Papineni, K., Roukos, S., Ward, T., and Zhu, W. - J Bleu: 기계 번역의 자동 평가 방법. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics (2002)_, pp. 311-318.\n' +
      '*(30)Petrou, F., Lewis, P., Pirtus, A., Rocktaschel, T., Wu, Y., Miller, A. H., and Hedel, S. 컨텍스트가 언어 모델의 사실적 예측에 어떤 영향을 미치는지, 2020년이다.\n' +
      '*(31)Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., and Yang, D. Is chatgpt a general-purpose natural language processing task solver?, 2023.\n' +
      '*(32)Raz, J. W., Borgaud, S., Cai, T., Millican, K., and Others. 스케일링 언어 모델: Methods, analysis and insight from training gopher, 2022.\n' +
      '*(33)Ram, O., Levine, Y., Dalmiedgos, I., Muhlagar, D., Shaishua, A., Leyton-Brown, K., and Shoham, Y. 컨텍스트 검색-증강 언어 모델, 2023.\n' +
      '*(34)Ren, S., Goto, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M., Bianco, A., and Ma, S. Codebieu: 코드 합성의 자동 평가를 위한 방법. _ arXiv preprint arXiv:2009.10297_(2020).\n' +
      '*(35)Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adj, Y., Liu, J., Renze, T., Rapin, J., et al. Code flama: Open foundation models for code. _ arXiv preprint arXiv:2308.12950_(2023).\n' +
      '*(36)Roziere, B., Lactaux, M. - A., Chanussot, L., and Lample, G. Unsupervised translation of programming languages. [Proceedings of the 34th International Conference on Neural Information Processing Systems_(Red Hook, NY, USA, 2020), NIPS\'20, Curran Associates Inc.\n' +
      '*(37)Smith, S., Patrway, M., Norick, B., LeGresley, P., Rajphandari, S., Cassefi, J., Liu, Z., Prahmutoyt, S., Zervas, G., Korthinath, V., Zhang, E., Child, R., Aminabadi, R., T., Bernauer, J., Song, X., Shoguri, M., He, Y., Houston, M., Tiewar, S., and Catanzaro, B. deepspeed and megatron using training nlg 530b, largescale generative language model, 2022.\n' +
      '*(38)Shidian, A., Lo, R., Xu, F. F., Zhu, H., and Zhou, S. 계층적 프롬프트는 2023년 웹 네비게이션에서 대규모 언어 모델을 지원합니다.\n' +
      '*(39)Shivasatava, A., Pastogi, A., Rao, A., Shogi, A. A. M., et al. Beyond the imitation game: Quantifying and extrapating the capabilities of language models, 2023.\n' +
      '*(40)Vaswani, A., Shazeer, N., Parmar, N., Uszkorett, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention이면 된다. _ 신경 정보 처리 시스템들_30의 발전들(2017).\n' +
      '*(41)Wang, A., Pruksaactukun, Y., Mangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. _SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems_. Curran Associates Inc., Red Hook, NY, USA, 2019.\n' +
      '*(42)Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019.\n' +
      '*(43)Wel, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thought prompt to reasoning in large language models, 2023.\n' +
      '*(44)Zhang, X., Yu, B., Yu, H., Lv, Y., Liu, T., Huang, F., Xu, H., and Li, Y. 더 넓고 더 깊은 llm 네트워크는 더 공정한 llm 평가자, 2023이다.\n' +
      '*(45)Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoter, L., and Levy, O. 리마: 2023년, 정렬이 더 적습니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>