<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '* **Bug-Fixing** (fix): LLMs play a crucial role in identifying and rectifying warnings and errors raised by static analysis tools.\n' +
      '* **Code Generation from Natural Language** (generate): LLMs generate code snippets from natural language descriptions.\n' +
      '* **Test Case Generation for Code** (test): LLMs are employed to automatically generate test cases for code, aiming to enhance software quality assurance practices.\n' +
      '* **Workspace Understanding and Query Resolution** (workspace): LLMs help developers understand the current project by responding to developer queries according to a comprehension of the codebase in the local workspace.\n' +
      '\n' +
      'The metrics in our evaluation harness are designed to evaluate the effectiveness, accuracy, and efficiency of LLM-guided programming interactions across real-world development scenarios. Our framework allows for any IDE to be plugged in and evaluated using our metrics. As such, we provide a system for tuning the IDE parameter space to attain superior LLM-integration outcomes.\n' +
      '\n' +
      'While prior work has offered an evaluation harness for code generation [10], a more comprehensive evaluation harness is needed with the new Large Language Models supporting multiple software engineering scenarios in an IDE. There is a wide parameter space to tune and optimize when integrating an LLM with an IDE: how are the prompts to the model phrased [38; 43]? In what order should information be given [23]? How are model responses parsed and inserted back into original code? What context should be provided to the model, in addition to the query [30; 33]? All these factors and more play a role in how well a model can perform within the IDE.\n' +
      '\n' +
      'Previous evaluation harnesses leave gaps in this wide space that we seek to cover with the Copilot Evaluation harness. In the HumanEval dataset [10], for example, models are evaluated on their ability to generate functions from doctorings. This is similar to our method generation evaluation metric. However, in HumanEval, the test cases are simple, straightforward, algorithmic coding interview style questions. In our test cases, the model must generate functions from real world code, many of which interact with dozens of other methods and files in order to complete a given task. This level of complexity is necessary to properly evaluate the code generation capabilities of state-of-the-art LLMs in a real world setting.\n' +
      '\n' +
      'Other works use LLMs themselves to evaluate output [11; 15; 44; 45]. Although this may be effective, there is no guarantee of the model\'s logic or reasoning, and the natural stochasticity of LLMs makes it difficult to calculate a "true" score for any given test case (i.e. the model may change its answer from run to run). Errors and logic gaps may propagate from the test data into the evaluation results.\n' +
      '\n' +
      'With our evaluation framework, we present a new standard of evaluation for model-generated code. Our evaluation harness allows for automatic understanding of how prompt and parameter changes impact performance, across hundreds of test cases spanning a wide range of programming scenarios with real-world code. In this iteration of our work, we discuss detailed results from two of the five metrics outlined above: documentation generation and bug fixing.\n' +
      '\n' +
      'We apply our evaluation framework to evaluate the effectiveness of Visual Studio Code, an IDE used by 15 million programmers across the world. Our evaluation spans a spectrum of LLM models, ranging from proprietary models like OpenAI\'s GPT-3.5 and GPT-4 to openly available alternatives such as Code Llama. We believe that a diverse set of models is essential to provide a holistic perspective on the capabilities and limitations of LLM-guided programming, catering to the needs and preferences of a wide developer audience.\n' +
      '\n' +
      'Figure 3. A developer asks the model to fix an error in their fibonacci code, and the model presents the fix (spelling the word “yield” correctly) in diff format.\n' +
      '\n' +
      'Figure 2. A developer uses /doc to generate documentation for a function that generates Fibonacci numbers. The LLM generates the documentation for this function highlighted in diff format.\n' +
      '\n' +
      'Related Work\n' +
      '\n' +
      'Below, we explain how our work builds upon and extend the related work on LLMs, Evaluating LLMs, and Evaluating LLMs for software engineering tasks.\n' +
      '\n' +
      '### LLMs\n' +
      '\n' +
      'Large Language Models (LLMs) (Liang et al., 2017; Liu et al., 2018; Liu et al., 2019) are advanced language models with massive parameter sizes that can understand and generate human language. Many of the well-known LLMs such as GPT-3 (Liu et al., 2019), InstructGPT (Peters et al., 2019), and GPT-4 (Peters et al., 2019) leverage the Transformer architecture (Xu et al., 2019). In comparison with the traditional machine learning models, LLMs require a large amount of data and very high hardware requirements for training. In return LLMs offer much higher performance than traditional machine learning models when compared their response quality on similar tasks. However, the results from LLMs are less interpretable than the traditional models.\n' +
      '\n' +
      'Building on the success of the LLMs researchers have started to explore the advantages of scaling up LLMs. For example, Gropher (Gropher, 2017) has 280 billion parameters, Megatron-turing NLG (Peters et al., 2019) has 530 billion parameters and PaLM (Liu et al., 2019) has 540 billion parameters outperforming average humans on the BIGbench benchmark (Liang et al., 2019). Similarly, researchers also explored fine-tuning LLMs for specific tasks and/or with human feedback (Peters et al., 2019).\n' +
      '\n' +
      'In our study, we examine the performance of three prominent LLMs: OpenAI\'s GPT-3.5, GPT-4, and CodeLlama on five different software engineering scenarios. We have chosen OpenAI\'s LLMs as representatives of general-purpose language models applicable to Software Engineering tasks, given their large scale and proprietary nature. In contrast, we have included CodeLlama as an illustration of an open-source, smaller, and optimized model fine-tuned specifically for code-related applications.\n' +
      '\n' +
      '### Evaluating LLMs\n' +
      '\n' +
      'Previous work has evaluated the effectiveness of LLMs from various angles including performance in natural language tasks, reasoning, robustness, safety, etc (Chen et al., 2019). For example, when it comes to sentiment analysis, (Peters et al., 2019) and (Peters et al., 2019) showed that LLMs perform much better than traditional sentiment analysis models. Similarly, (Peters et al., 2019) evaluated ChatGPT\'s performance on a range of tasks including answering questions, text summarization, code generation, reasoning, and addressing ethical issues.\n' +
      '\n' +
      'Unlike traditional machine learning models where k-fold cross validation was a common evaluation process, LLMs are often evaluated using static data sets. Common dataset for evaluating LLMs include: GLUE (Zhu et al., 2019), SuperGLUE (Zhu et al., 2019), BIGBench (Liang et al., 2019), Massive Multitask Language Understanding (MMLU) (Liu et al., 2019), Ethics Benchmark (Liu et al., 2019), and others.\n' +
      '\n' +
      'In this paper, we depart from conventional language-based metrics, such as BLEU, commonly employed in previous studies. Instead, we devise metrics tailored specifically for Software Engineering and the tasks under consideration.\n' +
      '\n' +
      '### Evaluating LLMs for Software Engineering Tasks\n' +
      '\n' +
      'LLMs have been widely used in various software engineering tasks, such as code generation, code summarization, code completion, code search, code documentation, code review, bug detection, and software testing. However, evaluating the effectiveness and efficiency of LLMs for SE tasks is not a trivial problem, as there are many factors and challenges involved. In this subsection, we review some of the existing works that have proposed or applied different evaluation methods and metrics for LLMs for SE tasks.\n' +
      '\n' +
      'One of the most comprehensive works is the paper (Liu et al., 2019), which provides a systematic literature review on the intersection of LLMs and SE, covering various aspects such as data collection, preprocessing, application, optimization, evaluation, and prompt engineering. The paper also categorizes and compares different LLMs that have been used in SE tasks, such as GPT-3, CodeBERT, and GraphCodeBERT, and analyzes their strengths and weaknesses. The paper also identifies the current challenges and future directions for LLMs for SE.\n' +
      '\n' +
      'CodeXGLUE (Zhu et al., 2019), is a comprehensive evaluation platform for LLMs in Software Engineering tasks. CodeXGLUE includes a benchmark dataset with 14 tasks covering code intelligence scenarios and provides baseline models like CodeBERT and CodeGPT. It aims to stimulate research and development in LLMs for SE, offering a diverse dataset for various programming languages and tasks. CodeXGLUE\'s evaluation metrics, both automatic and human-based, along with a leaderboard and online platform, facilitate fair comparisons between models.\n' +
      '\n' +
      'One of the first works that evaluated LLMs for code considering code execution and test cases is the paper (Liu et al., 2019), which introduces HumanEval, a benchmark dataset and a challenge for measuring the functional correctness of LLMs trained on code. HumanEval consists of 164 hand-written programming problems in Python, each with a function signature, a docstring, a body, and several unit tests. The problems cover various topics, such as language comprehension, algorithms, and simple mathematics, and some of them are comparable to simple software interview questions. The goal of HumanEval is to measure the ability of LLMs to synthesize programs from docstrings that pass the given test cases.\n' +
      '\n' +
      'In our research, we build upon the foundation laid by prior works in the literature, seeking to enhance their contributions. Like HumanEval, we incorporate considerations of code execution and test cases, but we extend both in terms of the breadth of SE tasks addressed and the refinement of evaluation metrics. In contrast to HumanEval, our evaluation encompasses large and real-world codebases. Furthermore,our emphasis is on developing a comprehensive evaluation framework for LLM-guided programming within IDE interactions, with a particular focus on their practicality across diverse programming languages and scenarios.\n' +
      '\n' +
      '## 3. Evaluating LLM-Guided Software Programming\n' +
      '\n' +
      'In addition to HumanEval (Hu et al., 2017), match-based metrics such as BLEU (Krizhevsky et al., 2017) or Code-BLEU (Zhu et al., 2017) are commonly adopted to benchmark LLM\'s performance in software engineering tasks. As LLMs become more ubiquitous and powerful, many researches use LLM models themselves to evaluate LLM output (Hu et al., 2017; Li et al., 2018; Li et al., 2019; Li et al., 2019). However, previous work suggests alternative metrics such as functional correctness better reflect success of generative models in code generation (Hu et al., 2017), code translation(Zhu et al., 2018), and other tasks. Building upon previous work in this area, we expand the HumanEval harness and evaluate IDE-integrated model competency in the five software engineering tasks listed above.\n' +
      '\n' +
      '### Documentation Generation from Code (doc)\n' +
      '\n' +
      'This task involves generating documentation for a method. Figure 2 shows an example in VS Code IDE. In this case the developer asks the LLM to generate a documentation for a Fibonacci function using /doc.\n' +
      '\n' +
      '#### 3.1.1. Metrics\n' +
      '\n' +
      'In this scenario, we consider a docstring generation to be successful if the location, format, and coverage of the generated text is correct. We report the following metrics for this scenario:\n' +
      '\n' +
      '* _Syntax Correctness_: We check that the docstring has been inserted into the code in such a way that it does not disrupt the syntax of the file with its addition.\n' +
      '* _Format Correctness_: If the documentation comment is placed in a syntactically acceptable manner for the given language, we further check for the correctness of documenting the return statement, function arguments with their types, function name, and whether a function description was written.\n' +
      '\n' +
      '#### 3.1.2. Evaluation Procedure\n' +
      '\n' +
      'We begin with a set of methods. For each method, we provide the method\'s signature and body to the LLM as context. We then prompt the LLM with a request to generate documentation for the method, and return the input function with the generated docstring inserted in the correct location within the function.\n' +
      '\n' +
      'After the LLM generates the documentation and the generated docstring is inserted into the code file, we evaluate the syntax correctness of the file with the generated docstring, as well as the correctness of the docstring itself.\n' +
      '\n' +
      '### Bug-Fixing (fix)\n' +
      '\n' +
      'This task involves using LLMs to fix bugs identified by static analysis tools, with an expectation that the resulting fixed code will have fewer errors overall than the original code. We use the following static analyzers:\n' +
      '\n' +
      '* javascript: eslint (Brandt et al., 2016);\n' +
      '* ts: eslint (Brandt et al., 2016), tsc (typescript compiler);\n' +
      '* python: pylint (Brandt et al., 2016), pyright (Brandt et al., 2016);\n' +
      '* java: spotbugs (Brandt et al., 2016);\n' +
      '* c#: roslyn (Brandt et al., 2016);\n' +
      '* cpp: clang (Brandt et al., 2016).\n' +
      '\n' +
      'If the original error is fixed but another error is introduced in its place, the test case will fail.\n' +
      '\n' +
      'Figure 4 shows an example in the VS Code IDE. A programmer has an error because of a misspelling of the word "yield", and the model corrects the error.\n' +
      '\n' +
      '#### 3.2.1. Metrics\n' +
      '\n' +
      'In this scenario, we consider a bug fix to be successful if the resulting code is syntactically correct and the corresponding static analysis warning or error has disappeared.\n' +
      '\n' +
      '* _Syntax Correctness_: we confirm that the code file with the bug fix remains syntactically correct.\n' +
      '* _Fix Rate_: we check that an existing static analysis warning or error in the code has been successfully resolved by the suggested changes, without introducing any other errors.\n' +
      '\n' +
      '#### 3.2.2. Evaluation Procedure\n' +
      '\n' +
      'Given a set of bugs found by static analyzer tools, we provide the file contents and diagnostic information to the LLM to generate a fix. We assess whether the model fixed the original error, whether it created any new errors, and whether the model-modified code remained syntactically correct after the fix was inserted.\n' +
      '\n' +
      '### Code Generation from Natural Language (generate)\n' +
      '\n' +
      'This task involves generating a code snippet from a natural language description. Figure 1 shows an example of such a task in the VS Code IDE. In this case, the developer asks the LLM to write a function that produces the first \\(n\\) values of the Fibonacci sequence, and the editor shows the generated function in a diff view.\n' +
      '\n' +
      '#### 3.3.1. Metrics\n' +
      '\n' +
      'Similar to previous evaluations of code generations (Hu et al., 2017), we consider a generated code snippet to be successful if the generated code is syntactically correct and all test cases covering the generated code pass. Therefore, we report the following metrics for this scenario:\n' +
      '\n' +
      '* _Syntax Correctness_: We compute and report the percentage of generated code that is syntactically correct. For this metric, we check the syntax correctness of the generated code using a language-specific parser (e.g., tree-sitter for each language).\n' +
      '* _Test Pass Rate_: We check the number of passing and failing tests and compute the passing test ratio. To compute this number, we execute the entire test suite of the user project and track which tests fail that passed prior to the model\'s code injection.\n' +
      '\n' +
      '#### 3.3.2. Evaluation Procedure\n' +
      '\n' +
      'We begin with a set of repositories with test cases. From each repository, we select the methods that are: 1) covered by the test cases in the given repository\'s test suite, and 2) have a docstring. For each method, we ask an LLM to generate the body of the method given the method\'s signature and docstring. We provide the contents of method\'s file as context to the LLM, replacing the original method body with a commented line reading "Your Code Here."\n' +
      '\n' +
      'After the LLM generates the method body, we put the generated code back in place of the original method body and evaluate the code by running the repository\'s test suite against the new method body. We then compute and report the syntax correctness and test pass rate, as explained above.\n' +
      '\n' +
      '### Test Case Generation for Code (test)\n' +
      '\n' +
      'This task involves using LLMs to generate test cases for code. Developers usually shortcut when it comes to writing unit tests. Automating test generation can motivate more developers to include unit tests. Figure 5 shows an example of a developer requesting tests in the VS Code IDE. In the example case, the developer asks the LLM to generate a test for a Fibonacci function using the /test chat scenario command.\n' +
      '\n' +
      '#### 3.4.1. Metrics\n' +
      '\n' +
      'In this scenario, we consider a generated test to be successful if it is syntactically correct and can pass on execution. Note that, for this evaluation, this means we assume the code for which the test was written is correct.\n' +
      '\n' +
      '* _Syntax Correctness_: We compute the percentage of generated tests that are syntactically correct. We check the syntax correctness of the generated tests using a language-specific parser.\n' +
      '* _Generated Test Pass Rate_: We compute the pass rate of the generated test. We assume the original method is correct, and execute the generated test on its focal method.\n' +
      '\n' +
      '#### 3.4.2. Evaluation Procedure\n' +
      '\n' +
      'Given a set of methods, we provide the method signature, docstring, and body as context to the LLM to generate a test for each focal method.\n' +
      '\n' +
      'Once the LLM generates a test for the method, we add the test to the repository containing the method, and attempt to execute the test.\n' +
      '\n' +
      'For Javascript and Typescript, we generate tests using either the Jest or Mocha library. The original test suite of the repository does not need to be written with either library, but each method\'s original file must be able to pass without errors when a trivial test case (which essentially just asserts true) is appended to the file. When evaluating the generated tests, we temporarily append them to the focal method\'s file to mitigate import errors, and run the entire file. If running the file with a trivial test case appended (e.g. a test that should always be true) returns false or an error, we know the results from the generated test on that file are not reliable.\n' +
      '\n' +
      '### Workspace Understanding and Query Resolution (workspace)\n' +
      '\n' +
      'In the Workspace task, we give the model a user\'s natural language query, and ask it to identify relevant snippets of the codebase that may aid in answering the user\'s question. This tests a model\'s ability to comprehend both natural language requests from a user and large amounts of code.\n' +
      '\n' +
      '#### 3.5.1. Metrics\n' +
      '\n' +
      'We evaluate the quality of an LLM\'s retrieved snippets in two ways:\n' +
      '\n' +
      '* Mean Reciprocal Rank (MRR): Given a ranked list of the model\'s retrieved snippets, we calculate \\(\\frac{1}{r}\\), where \\(r\\) is the rank of the correct snippet in the model\'s list. So, if the model ranks the correct snippet second, we\n' +
      '\n' +
      'Figure 4. A developer asks the model to fix an error in their fibonacci code, and the model presents the fix (spelling the word ”yield” correctly) in diff format.\n' +
      '\n' +
      'would consider the model\'s score for that test case to be \\(\\frac{1}{2}\\). MRR is the mean of all the test case scores.\n' +
      '* End to End Keyword Detection: We begin with a manually created dataset of user queries and keywords associated with the correct answer to the query. We take the model\'s ranked list of retrieved snippets and pass it to the model along with each user query. Then, we detect whether or not the associated keyword appeared in the model\'s response, given both the query and the retrieved results.\n' +
      '\n' +
      '#### 3.5.2. Evaluation Procedure\n' +
      '\n' +
      'For each datapoint, we provide the LLM with a user query and the full context of the codebase associated with the given query. We ask the LLM to retrieve a ranked list of relevant code snippets from the codebase. We directly evaluate the quality of the model\'s retrieved results using MRR, a metric that scores how well models are able to find the most relevant code snippets during retrieval.\n' +
      '\n' +
      'We also evaluate the quality of all the retrieved code snippets by asking the model to answer the original user query, providing the query and the snippets as context. We search the model\'s final response for a set of keywords associated with the given query to determine whether or not the model was able to find the information it needed to fully answer the question.\n' +
      '\n' +
      'With this metric, we evaluate the model\'s retrieval abilities on an end to end scale, and determine a model\'s skill at finding code snippets that would actually help it answer the question at hand.\n' +
      '\n' +
      '## 4. Copilot Evaluation Harness\n' +
      '\n' +
      'We introduce the end to end Copilot Evaluation Harness for computing evaluation metrics as described above. First, we share the details of collecting the data required for each evaluation. Then, we explain the process of creating a test environment given each language and the need to build and run tests. Finally, we give additional specific implementation details about the evaluation process for each metric.\n' +
      '\n' +
      '### Data Collection\n' +
      '\n' +
      'Our dataset is made up of methods from hundreds of public GitHub repositories across 6 languages: JavaScript, Typescript, Python, Java, C/C++, and C#. Some of our evaluations require the ability to build and run tests for repositories associated with test cases. To meet this requirement, we have developed a build agent as part of our evaluation harness that attempts various build and test strategies on any arbitrary repository. In addition, we have the capability to run static analysis tools on the repositories that we can build and test. This build agent is essential in collecting the test datasets and performing evaluations.\n' +
      '\n' +
      'For each language, we sample from Github public repositories whose code we are able to build and whose test suites\n' +
      '\n' +
      'Figure 5. A developer uses /test to generate a test for a function that generates Fibonacci numbers. The LLM generates the test_fibonacci function for this function in a test file.\n' +
      '\n' +
      'we are able to run using our build agent. The build agent supports Node 18+, Python 3.8+, Java JDK 1.8 (requiring Maven),.NET 6.0, 7.0 and 8.0, and a manually curated set of C++ repositories. We resorted to manually gathering C++ repositories due to the wide variability of C++ build steps. We ignore repositories that are smaller than 1 MB and larger than 100 MB. We ignore repositories that take longer than 10 minutes to build and run tests. Lastly, we ignore repositories that do not contain any methods.\n' +
      '\n' +
      '#### 4.1.1. Javascript and Typescript\n' +
      '\n' +
      'In Javascript and Typescript, we sub-select on repos that contain a _package.json_ file at the root directory. The _package.json_ file works in concordance with npm (Node Package Manager) to handle various tasks within the repo, such as specifying dependencies for installation and running the test suite. We rely on npm for our evaluation of Javascript and Typescript code, so we only consider repos whose infrastructure is built to be managed with npm.\n' +
      '\n' +
      '#### 4.1.2. Java\n' +
      '\n' +
      'In Java, we consider repositories that leverage Maven for their build process. In addition, as of writing, we only consider projects that use JDK 1.8.\n' +
      '\n' +
      '#### 4.1.3. Python\n' +
      '\n' +
      'In Python, we only consider repositories for which we are able to successfully install all dependencies within a virtual environment.\n' +
      '\n' +
      '#### 4.1.4. C/C++\n' +
      '\n' +
      'In C/C++, we leverage clang for building projects. Because of the sheer variety of ways that C/C++ repositories can be built, we present a set of manually curated repositories which we have verified will build and test within a docker image.\n' +
      '\n' +
      '### Test Case Collection\n' +
      '\n' +
      'After identifying suitable repositories for each language, we generate test cases for each evaluation metric based on the code within the repositories. Most evaluations require identifying methods that meet certain conditions, such as being covered by existing tests or containing a warning from a static analysis tool. The criteria for generating evaluation test cases varies from metric to metric, and is explained for each metric below.\n' +
      '\n' +
      '#### 4.2.1. Documentation Generation from Code (doc)\n' +
      '\n' +
      'We create test cases by identifying methods in the repository that are longer than three lines and are not a result of minification or obfuscation. We provide the method and ask the coding assistant being evaluated to generate a docstring for the method. We consider a docstring generation to be successful if the location, format, and coverage of the generated text is correct.\n' +
      '\n' +
      '#### 4.2.2. Bug Fixing (fix)\n' +
      '\n' +
      'We create test cases based on static analysis tool warnings and errors flagged on a given repository. We only consider static analysis warnings that are not related to imports or configuration because such issues are difficult to fix with only a single file as context. We consider a generated fix to be successful if it is syntactically correct and strictly reduces the numbers of static analysis warnings on execution. We must consider a strict decrease rather than the presence of the original warning or error, because it is possible for the coding assistant to fix the original issue while introducing a new issue, which a developer would not look upon as a complete fix.\n' +
      '\n' +
      '#### 4.2.3. Code Generation from Natural Language (generate)\n' +
      '\n' +
      'We create test cases by identifying methods in a given repository that are covered by some existing passing test. The test case gives the coding assistant visibility of the entire file up to and including the method signature. The coding assistant is then asked to generate the method body associated with the method signature. We consider a generated code snippet to be successful if the generated code is syntactically correct and all test cases covering the generated code pass.\n' +
      '\n' +
      '#### 4.2.4. Test Generation from Code (test)\n' +
      '\n' +
      'We create test cases by identifying methods within a given repository. We ask the coding assistant to provide a working test for the given method. We consider the generated test to be successful if it invokes the given method and passes execution.\n' +
      '\n' +
      '#### 4.2.5. Workspace Understanding and Query Resolution (workspace)\n' +
      '\n' +
      'We collected questions from developers about certain aspects of their project workspace, such as the idiomatic way to build a certain feature. The context fetching that takes place as a part of the workspace command will return several related code snippets. We evaluate the quality of an LLM\'s retrieved snippets using MRR as explained above.\n' +
      '\n' +
      '## 5. Experiments\n' +
      '\n' +
      'Using Copilot evaluation harness metrics and the test cases detailed above, we compute the success of two OpenAI models: GPT-3.5 and GPT-4, as well as CodeLlama on the document generation and bug fixing scenarios using the an LLM powered chat extension in VSCode IDE with more than 700K active users as the code assistant.\n' +
      '\n' +
      'Our experimentation aims to answer the following research questions:\n' +
      '\n' +
      '* **RQ1. Model Comparison**: How do different LLM\'s compare to one another when integrated with a coding assistant?\n' +
      '* **RQ2. Integration Improvements**: What insights can the Copilot Evaluation harness provide engineers to improve the integration of LLM in a coding assistant?\n' +
      '* **RQ3. Data Validity**: How do our evaluation test cases compare with actual usage of a LLM powered coding assistant? Do the test cases in our harness reflect how real-world users interact with a LLM powered coding assistant?In this section, we discuss findings pertaining to these research questions.\n' +
      '\n' +
      '### RQ1. Model Comparison\n' +
      '\n' +
      'Below we discuss our learnings comparing three state of the art LLMs when used to power our target chat extension in VSCode.\n' +
      '\n' +
      '#### 5.1.1. Documentation Generation from Code (doc)\n' +
      '\n' +
      'Table 1 shows that, for docstring generation, GPT-4 generally outperforms GPT-3.5 and Code Llama. GPT-3.5 and GPT-4 are very similar in performance to one another, with Code Llama slightly behind. The key exceptions here are Python, where Code Llama performs at a slightly higher level than GPT-4, and C/C++, where Code Llama performs significantly worse. One possible explanation is that GPT-3.5 and GPT-4 were trained on a massive corpus, encompassing much of the open source code on the internet. As such, the GPT models\' performance could be inflated by the fact that it has seen many different code patterns. Code Llama, a comparatively small model, is much less likely to have seen a given code snippet, potentially hindering its performance in comparison to the GPT models.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c}  & \\multicolumn{3}{c}{**Ex**} \\\\ \\hline\n' +
      '**Language** & **Model** & **Syntax** & **Bugs** \\\\  & & **Correctness** & **Fixed** \\\\ \\hline \\multirow{3}{*}{Python} & GPT-4 & 96\\% & 74\\% \\\\  & GPT-3.5 & 93\\% & 68\\% \\\\  & CodeLlama & 88\\% & 39\\% \\\\ \\hline \\multirow{3}{*}{Javascript} & GPT-4 & 92\\% & 81\\% \\\\  & GPT-3.5 & 85\\% & 74\\% \\\\  & CodeLlama & 39\\% & 26\\% \\\\ \\hline \\multirow{3}{*}{Typescript} & GPT-4 & 83\\% & 75\\% \\\\  & GPT-3.5 & 74\\% & 75\\% \\\\  & CodeLlama & 70\\% & 30\\% \\\\ \\hline \\multirow{3}{*}{C\\#} & GPT-4 & 98\\% & 58\\% \\\\  & GPT-3.5 & 96\\% & 65\\% \\\\ \\cline{1-1}  & CodeLlama & 84\\% & 50\\% \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1. LLMs performance on the Doc chat scenario across Python, Javascript, Typescript, Java, C# and C/C++ for the specific success metrics of the Doc scenario.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c}  & \\multicolumn{3}{c}{**Doc**} \\\\ \\hline\n' +
      '**Language** & **Model** & **Syntax** & **Format** \\\\  & & **Correctness** & **Correctness** \\\\ \\hline \\multirow{3}{*}{Python} & GPT-4 & 100\\% & 83\\% \\\\  & GPT-3.5 & 100\\% & 87\\% \\\\  & CodeLlama & 100\\% & 87\\% \\\\ \\hline \\multirow{3}{*}{Javascript} & GPT-4 & 83\\% & 100\\% \\\\  & GPT-3.5 & 83\\% & 100\\% \\\\  & CodeLlama & 79\\% & 55\\% \\\\ \\hline \\multirow{3}{*}{Typescript} & GPT-4 & 96\\% & 79\\% \\\\  & GPT-3.5 & 96\\% & 86\\% \\\\  & CodeLlama & 77\\% & 65\\% \\\\ \\hline \\multirow{3}{*}{Javascript} & GPT-4 & 100\\% & 93\\% \\\\  & GPT-3.5 & 100\\% & 80\\% \\\\  & CodeLlama & 100\\% & 64\\% \\\\ \\hline \\multirow{3}{*}{C\\#} & GPT-4 & 100\\% & 89\\% \\\\  & GPT-3.5 & 100\\% & 75\\% \\\\ \\cline{1-1}  & CodeLlama & 94\\% & 67\\% \\\\ \\hline \\multirow{3}{*}{C/C++} & GPT-4 & 92\\% & 94\\% \\\\ \\cline{1-1}  & GPT-3.5 & 92\\% & 77\\% \\\\ \\cline{1-1}  & CodeLlama & 90\\% & 38\\% \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2. LLMs performance on the Fix chat scenario across Python, Javascript, Typescript and C# for the specific Fix success metrics.\n' +
      '\n' +
      'Figure 6. Example prompt and response for a passing fix test case.\n' +
      '\n' +
      '#### 5.1.2. Bug-Fixing (fix)\n' +
      '\n' +
      'Table 2 shows the results for bug fixing: similarly to the docstring generation evaluation, GPT-4 tends to slightly outperform GPT-3.5, with Code Llama further behind. For bug fixing, the exception seems to be C#, for which all three models seem to struggle, with GPT-3.5 ultimately outperforming both GPT-4 and Code Llama. Figures 6 and 7 show a passed and failed example with GPT-4. In Figure 6, the model adds an \\(if\\) statement to check whether \\(self\\_writers\\) is None before trying to use it as an iterable, which solves the error. In Figure 7, the model similarly adds a check for whether \\(Hero\\_age\\) is None. However, since it adds the check after the operation that is causing the error, the error continues to occur. Although the model was able to identify a potential fix, it inserted the check in the wrong location and was not able to fix the bug.\n' +
      '\n' +
      'A common cause of differing results between the GPT-3.5 and GPT-4 LLMs occurs when the models try to resolve the "has an \'any\' type" error, as can be seen in Figure 8. When the\n' +
      '\n' +
      'Figure 8. Example prompt and response for the same test case, where both GPT-3.5 and GPT-4 should fail, but only GPT-4 fails because it attempts a more nuanced approach to fixing the bug than GPT-3.5.\n' +
      '\n' +
      'Figure 7. Example prompt and response for the failed fix chat scenario. Here, the model determines what the problem might be and attempts to fix it. However, the fix is not correct, and the same syntax error still exists, because it tries to do the > operation before checking whether the age is None.\n' +
      '\n' +
      'GPT-4 model attempts to specify the type of a variable named _res_, it predicts the type as \\(Electron\\).\\(Message\\)\\(Box\\)\\(Return\\)\\(Value\\) and casts the variable to that type. That type, however, is not a valid return type for the code. GPT-3.5, on the other hand, casts the variable to type _any_, thereby circumventing more complex issues, but leaving in a code smell (since it is not good practice to cast variables to type _any_). In this case, GPT-3.5 passes our evaluation, while GPT-4 fails, even though GPT-4\'s attempted fix is more nuanced and advanced. Upon closer inspection of the cases where GPT-3.5 succeeds and GPT-4 fails, we see this phenomenon frequently: GPT-4 fails with a more complicated approach, while GPT-3.5 technically passes, but with a rudimentary and sub-optimal solution.\n' +
      '\n' +
      '### RQ2. Integration Improvements\n' +
      '\n' +
      'Below we discuss how our evaluation harness can be used to learn insights on how to better integrate LLMs with IDEs.\n' +
      '\n' +
      '#### 5.2.1. Documentation Generation from Code (doc)\n' +
      '\n' +
      'Further inspection of our results in table 1 reveals four classes of errors that cause the docstring generation evaluation to fail:\n' +
      '\n' +
      '1. **Code Logic Changes:** The model changes the fundamental logic of the code when it rewrites the focal function to the file along with the generated docstring.\n' +
      '2. **Syntax Changes:** The model changes the syntax of the focal code when writing to the file. This includes changes such as adding semicolons to the end of lines, or adding type decorators to the function signature.\n' +
      '3. **Incomplete Docstrings:** The model generates a description for the correct function, but does not describe the returned object and every parameter of the function.\n' +
      '4. **Irrelevant Docstrings:** The model returns a docstring that does not pertain to the code block we asked it to document.\n' +
      '\n' +
      'Upon closer inspection, in cases where only one of GPT-3.5 or GPT-4 passes, we notice that the GPT-4 model is more likely to make changes to the focal code that make the code cleaner than GPT-3.5. For example, in Figure 9, the GPT-4 model adds type decorators to the inputs of the function, and specifies the return type. Although the GPT-4 model\'s docstring is correct (and more detailed than that of the GPT-3.5 model), the GPT-4 model fails this test case, since we expect the model to make no changes to the focal code. However, such an error shows how the GPT-4 model\'s attempts at a more involved improvement may decrease its score without being indicative of worse performance.\n' +
      '\n' +
      'Based on this finding, we inserted an additional instruction in the coding assistant\'s docstring generation prompt that tells the model specifically not to change any of the focal code. This resulted in a significant improvement in the evaluation results for all the languages, ranging from 5% in C++ to 11% in Java.\n' +
      '\n' +
      'We also see that GPT-4 is better at following specific instructions. This is highlighted by the example in Figure 10, where GPT-4 generates documentation for the Vec class instead of the _get_colour_at_ function. At first, this appears\n' +
      '\n' +
      'Figure 9. Example prompt and response for doc evaluation. GPT-3.5 passed, and GPT-4 failed because it returned the original function with parameter and return types. This fails our evaluation because we require the model to leave the focal code unchanged.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      'For documentation generation, we use OpenAI\'s ada embedding model [3] to embed the documented code snippets and compare the snippets from our dataset with the one\'s gathered from Microsoft developers usage data. Similarly, for the bug fixing telemetry, we embed the code snippets that contain the bug. We use PCA dimensionality reduction to plot the data in two dimensions. PCA dimensionality reduction is optimized to find a plane that maximizes the distance between points and outliers. Figures 12 and 13 show the results of this comparison. We see that each language forms a cluster, and the real usage and our data exist within a similar space for each language cluster, for both documentation generation and bug fixing.\n' +
      '\n' +
      'We do not aim to match the test cases in our dataset to the real usage point for point. Rather, we are determining\n' +
      '\n' +
      'Figure 11. Example failing prompt and response from GPT-3.5. The model hallucinates a type (Token) that matches the name of the variable.\n' +
      '\n' +
      'Figure 12. Comparing our dataset for Documentation generation evaluation with real-world usage across languages.\n' +
      '\n' +
      'Figure 11. Example failing prompt and response from GPT-3.5. The model hallucinates a type (Token) that matches the name of the variable.\n' +
      '\n' +
      'Figure 13. Comparing our dataset for Bug Fixing evaluation with real-world usage across languages.\n' +
      '\n' +
      'whether or not our test cases are outliers in the space of the real usage. If they are not outliers, we can infer that our dataset is in line with the real-world usage of the chat extension. This analysis suggests that our dataset for both the documentation generation and bug fixing evaluation is in line with real world usage.\n' +
      '\n' +
      '## 6. Conclusion and Future Work\n' +
      '\n' +
      'With the growing use of LLMs to aid developers in complex engineering tasks comes the need for more robust evaluations of LLM-generated code. Especially as more companies and products seek to integrate LLMs into their workflows, existing evaluation metrics are not sufficient to confirm the quality and correctness of machine-generated code. In this paper, we propose a solution to this problem via the Copilot Evaluation harness. We define five key evaluation metrics for the code generation problem space: method generation, test generation, docstring generation, bug fixing and workspace understanding. We detail the methodology required to collect test cases and evaluation results for each of those five metrics. We also provide preliminary results for two of the five metrics across myriad programming languages.\n' +
      '\n' +
      'Our goal in creating the evaluation harness is to validate the quality of LLM-generated code. Although we have seen immense advancements in the code generation ML space, we seek to highlight how much oversight and engineering effort is required to reliably and optimally integrate LLMs into a code workflow. We aim to provide developers a comprehensive evaluation suite, with which they can optimize the integration of LLMs into their coding workflows. With the Copilot Evaluation harness, programmers can more systematically and robustly evaluate the impact of parameters such as prompt wordings, changes in the order of information provided, changes in the context provided to the model, and more.\n' +
      '\n' +
      'Moreover, the Copilot Evaluation harness can be used for cost optimizations by revealing that a more budget-friendly LLM model (e.g. CodeLLama) might exhibit satisfactory performance in tasks like documentation. This insight enables developers to intelligently balance resources by allocating tasks to the cost-effective LLM when its performance is deemed sufficient. Simultaneously, more complex tasks can be shifted to more powerful LLMs to ensure optimal outcomes.\n' +
      '\n' +
      'We publish this paper as a living documentation of our progress. Future work on this project involves reporting results for the remaining three evaluation metrics and open-sourcing our data and evaluation code.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)Clang. [https://clang.llvm.org/](https://clang.llvm.org/).\n' +
      '* (2)Eslint. [https://eslint.org](https://eslint.org).\n' +
      '* (3)Openai ada. [https://openai.com/blog/new-and-improved-embedding-model](https://openai.com/blog/new-and-improved-embedding-model).\n' +
      '* (4)Pylint. [https://pylint.pycqa.org/en/latest/user_guide/usage/run.html](https://pylint.pycqa.org/en/latest/user_guide/usage/run.html).\n' +
      '* (5)Pyright. [https://github.com/microsoft/pyright](https://github.com/microsoft/pyright).\n' +
      '* (6)Roslyn. [https://github.com/dotnet/roslyn-analyzers](https://github.com/dotnet/roslyn-analyzers).\n' +
      '* (7)Spotbugs. [https://spotbugs.github.io/](https://spotbugs.github.io/).\n' +
      '* (8)Chiang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zitu, K., Chen, H., Yi, X., Wang, C., Wang, Y., Ye, W., Zhang, Y., Chang, Y., Yu, P. S., Yang, Q., and Xie, X. A survey on evaluation of large language models, 2023.\n' +
      '* (9)Chien, B., Mustakin, N., Hoang, A., Fuad, S., and Wong, D. Vesuda: Llm based cuda extension for visual studio code. In _Proceedings of the SC 23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis_ (New York, NY, USA, 2023), SC-W \'23, Association for Computing Machinery, pp. 11-17.\n' +
      '* (10)Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., et al. Evaluating large language models trained on code.\n' +
      '* (11)Chen, Y., Wang, R., Jiang, H., Shi, S., and Xu, R. Exploring the use of large language models for reference-free text quality evaluation: An empirical study, 2023.\n' +
      '* (12)Chowdhiery, A., Narang, S., Devlin, J., Bosma, M., and Others. Palm: Scaling language modeling with pathways, 2022.\n' +
      '* (13)Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding.\n' +
      '* (14)Floonth, L., and Chiriatti, M. Gpt-3: Its nature, scope, limits, and consequences. _Minds and Machines 30_ (2020), 681-694.\n' +
      '* (15)Fu, J., Ng, S.-K., Jiang, Z., and Liu, P. Gptscore: Evaluate as you desire, 2023.\n' +
      '* (16)Gao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners, 2021.\n' +
      '* (17)Hendrycks, D., Burns, C., Basart, S., Caritch, A., Li, J., Song, D., and Steinhardt, J. Aligning ai with shared human values, 2023.\n' +
      '* (18)Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazrika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding, 2021.\n' +
      '* (19)Hou, X., Zhao, Y., Liu, Y., Yang, Z., Wang, K., Li, L., Luo, X., Lo, D., Grundy, J., and Wang, H. Large language models for software engineering: A systematic literature review. _arXiv preprint arXiv:2308.10620_ (2023).\n' +
      '* (20)Komberink, S., Mikolov, T., Karafiat, M., and Burger, I. Recurrent neural network based language modeling in meeting recognition. In _Interspeech_ (2011), vol. 11, pp. 2877-2880.\n' +
      '* (21)Laskar, M. T. R., Bar, M. S., Rahman, M., Bhuiyan, M. A. H., Joty, S., and Huang, J. X. A systematic study and comprehensive evaluation of chatgpt on benchmark datasets, 2023.\n' +
      '* (22)Liang, P., Bommasani, R., Lee, T., Tsipras, D., and Others. Holistic evaluation of language models, 2023.\n' +
      '* (23)Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts, 2023.\n' +
      '* (24)Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tuzano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K., Fu, S., and Liu, S. Codexglue: A machine learning benchmark dataset for code understanding and generation, 2021.\n' +
      '* (25)Nam, D., Macvean, A., Helliendoorn, V., Vasilescu, B., and Myers, B. In-ide generation-based information support with a large language model, 2023.\n' +
      '* (26)OpenAI. Gpt-3 cmodels, 2023.\n' +
      '* (27)OpenAI. Gpt-4 technical report, 2023.\n' +
      '* (28)Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishra, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems 35_ (2022), 27730-27744.\n' +
      '\n' +
      '* (29)Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics (2002)_, pp. 311-318.\n' +
      '* (30)Petrou, F., Lewis, P., Pirtus, A., Rocktaschel, T., Wu, Y., Miller, A. H., and Hedel, S. How context affects language models\' factual predictions, 2020.\n' +
      '* (31)Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., and Yang, D. Is chatgpt a general-purpose natural language processing task solver?, 2023.\n' +
      '* (32)Raz, J. W., Borgaud, S., Cai, T., Millican, K., and Others. Scaling language models: Methods, analysis & insights from training gopher, 2022.\n' +
      '* (33)Ram, O., Levine, Y., Dalmiedgos, I., Muhlagar, D., Shaishua, A., Leyton-Brown, K., and Shoham, Y. In-context retrieval-augmented language models, 2023.\n' +
      '* (34)Ren, S., Goto, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M., Bianco, A., and Ma, S. Codebieu: a method for automatic evaluation of code synthesis. _arXiv preprint arXiv:2009.10297_ (2020).\n' +
      '* (35)Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adj, Y., Liu, J., Renze, T., Rapin, J., et al. Code flama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_ (2023).\n' +
      '* (36)Roziere, B., Lactaux, M.-A., Chanussot, L., and Lample, G. Unsupervised translation of programming languages. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_ (Red Hook, NY, USA, 2020), NIPS\'20, Curran Associates Inc.\n' +
      '* (37)Smith, S., Patrway, M., Norick, B., LeGresley, P., Rajphandari, S., Cassefi, J., Liu, Z., Prahmutoyt, S., Zervas, G., Korthinath, V., Zhang, E., Child, R., Aminabadi, R., T., Bernauer, J., Song, X., Shoguri, M., He, Y., Houston, M., Tiewar, S., and Catanzaro, B. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model, 2022.\n' +
      '* (38)Shidian, A., Lo, R., Xu, F. F., Zhu, H., and Zhou, S. Hierarchical prompting assists large language model on web navigation, 2023.\n' +
      '* (39)Shivasatava, A., Pastogi, A., Rao, A., Shogi, A. A. M., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023.\n' +
      '* (40)Vaswani, A., Shazeer, N., Parmar, N., Uszkorett, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. _Advances in neural information processing systems_ 30 (2017).\n' +
      '* (41)Wang, A., Pruksaactukun, Y., Mangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. _SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems_. Curran Associates Inc., Red Hook, NY, USA, 2019.\n' +
      '* (42)Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019.\n' +
      '* (43)Wel, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models, 2023.\n' +
      '* (44)Zhang, X., Yu, B., Yu, H., Lv, Y., Liu, T., Huang, F., Xu, H., and Li, Y. Wider and deeper llm networks are fairer llm evaluators, 2023.\n' +
      '* (45)Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoter, L., and Levy, O. Lima: Less is more for alignment, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>