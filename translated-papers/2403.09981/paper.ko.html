<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Surface-Aligned Gaussian Splatting을 통한 제어 가능한 텍스트-3D 생성\n' +
      '\n' +
      'Zhiqi Li\n' +
      '\n' +
      '1 저장대학교 2 Westake University 3 Tongji University\n' +
      '\n' +
      'Yiming Chen\n' +
      '\n' +
      '2Westake University 3 Tongji University\n' +
      '\n' +
      'Lingzhe Zhao\n' +
      '\n' +
      '2Westake University 3 Tongji University\n' +
      '\n' +
      'Peidong Liu\n' +
      '\n' +
      '교신저자 프로젝트 페이지: [https://lizhiqi49.github.io/MVControl/](https://lizhiqi49.github.io/MVControl/)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '텍스트-투-3D 및 이미지-투-3D 생성 작업은 상당한 관심을 받고 있지만, 그 사이의 중요하지만 미개척된 분야 중 하나는 제어 가능한 텍스트-투-3D 생성이며, 이는 주로 이 작업에서 초점을 맞추고 있다. 이 작업을 해결하기 위해 **1)**에서는 에지, 깊이, 정규 및 스크리블 맵과 같은 추가 입력 조건을 통합하여 기존의 사전 훈련된 다시점 확산 모델을 향상시키도록 설계된 새로운 신경망 구조인 Multi-view ControlNet(MVControl)을 소개한다. 우리의 혁신은 입력 조건 이미지와 카메라 포즈에서 계산된 로컬 및 글로벌 임베딩을 모두 사용하여 베이스 확산 모델을 제어하는 컨디셔닝 모듈의 도입에 있다. 일단 훈련되면, MVControl은 3D 확산 안내를 제공할 수 있다.\n' +
      '\n' +
      '그림 1: 텍스트 프롬프트와 조건 이미지가 주어지면, 본 방법은 가우시안 바인딩 메쉬와 텍스처 메쉬의 높은 충실도와 효율적인 제어 가능한 텍스트 대 3D 생성을 달성할 수 있다.\n' +
      '\n' +
      '최적화 기반 3D 생성. 그리고, **2)**는 최근 대규모 재구성 모델과 점수 증류 알고리즘의 이점을 활용하는 효율적인 다단계 3D 생성 파이프라인을 제안한다. MVControl 아키텍처를 기반으로 최적화 프로세스를 지시하기 위해 고유한 하이브리드 확산 안내 방법을 사용한다. 효율성을 추구하기 위해, 우리는 일반적으로 사용되는 암시적 표현 대신 3D 가우시안들을 우리의 표현으로 채택한다. 우리는 또한 가우시안들을 메쉬 삼각형 면에 묶는 하이브리드 표현인 SuGaR의 사용을 개척했다. 이 접근법은 3D 가우시안 기하학의 열악한 기하학 문제를 완화하고 메쉬에 세립 기하학을 직접 조각할 수 있게 한다. 광범위한 실험을 통해 제안된 방법이 강력한 일반화를 달성하고 고품질 3D 콘텐츠를 제어할 수 있음을 입증한다.\n' +
      '\n' +
      '키워드: 제어 가능한 3D 생성 가우시안 스플래팅 SuGaR\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 2D 이미지 생성 분야에서 괄목할 만한 진전이 이루어졌으며, 이는 이후 3D 생성 작업에 대한 연구를 추진했다. 이러한 진전은 이미지 확산 모델 [31, 44] 및 미분 가능한 3D 표현 [48, 59, 24, 38]의 유리한 특성에 기인한다. 특히, 스코어 증류 최적화(SDS)[42]에 기초한 최근의 방법들은 미리 훈련된 대형 텍스트-이미지 생성 모델[31, 44, 50]로부터 3D 지식을 증류하려고 시도했으며, 이는 인상적인 결과[56, 57, 29, 37, 63, 29, 11]로 이어진다.\n' +
      '\n' +
      '여러 접근 방식은 여러 최적화 단계[11, 29]를 적용하고, 3D 표현으로 확산을 동시에 최적화하며[52, 63], 스코어 증류 알고리즘을 정제하고[68, 22], 파이프라인 세부 사항[4, 70, 20]을 개선하는 것과 같은 생성 품질을 향상시키는 것을 목표로 한다. 또 다른 초점은 미리 훈련된 확산 모델[31, 32, 34, 49, 50, 27]에 멀티뷰 지식을 통합함으로써 뷰 일관성 문제를 해결하는 것이다. 그러나 고품질 3D 자산을 달성하려면 종종 이러한 기술의 조합이 필요하며, 이는 시간이 많이 걸릴 수 있다. 이를 완화하기 위해 최근 작업은 3D 생성 네트워크가 자산을 신속하게 생산하도록 훈련하는 것을 목표로 한다[55, 21, 26, 40, 19, 61, 40]. 효율적이지만 이러한 방법은 훈련 데이터의 한계로 인해 종종 더 낮은 품질과 덜 복잡한 모양을 생성한다.\n' +
      '\n' +
      '많은 작품들이 텍스트 또는 이미지 대 3D 작업에 초점을 맞추고 있지만, 중요하지만 아직 탐구되지 않은 영역은 이 작품이 다루고자 하는 격차인 제어 가능한 텍스트 대 3D 생성에 있다. 본 논문에서는 이전 단락에서 언급한 두 연구 라인의 장점을 활용한 고효율 제어 가능한 3D 생성 파이프라인을 제안한다. Stable-Diffusion[44]의 필수 구성 요소인 2D ControlNet[69]의 업적에 동기 부여되어 멀티뷰 변종인 MVControl을 제안한다. 3D 생성에서 멀티뷰 기능의 중요한 역할을 고려할 때, MVControl은 2D ControlNet의 성공을 멀티뷰 영역으로 확장하기 위해 설계되었다. 우리는 새롭게 도입된 다시점 확산 네트워크인 MVDream[50]을 기본 모델로 채택한다. MVControl은 후속적으로 이 기본 모델과 협업하도록 제작되어 제어 가능한 텍스트 대 다중 뷰 이미지 생성을 용이하게 한다. [69]의 접근법과 유사하게 MV-Dream의 가중치를 동결하고 MVControl 컴포넌트 훈련에만 집중한다. 그러나 단일 이미지 생성을 위해 설계된 2D ControlNet의 컨디셔닝 메커니즘은 멀티뷰 시나리오로 쉽게 확장되지 않아 기본 모델과 상호작용하기 위해 제어 네트워크를 직접 적용함으로써 뷰 일관성을 달성하기 어렵다. 또한, MVDream은 절대 카메라 시스템에 대해 훈련되며, 응용 시나리오에서 상대 카메라 포즈에 대한 실제적인 필요성과 충돌한다. 이러한 문제를 해결하기 위해 간단하면서도 효과적인 컨디셔닝 전략을 소개합니다.\n' +
      '\n' +
      'MVControl을 학습한 후 이를 활용하여 제어 가능한 텍스트 대 3D 자산 생성을 위한 3D 전거를 설정할 수 있다. 본 논문에서는 NeRF[38] 기반 암시적 표현(implicit representation)의 활용에 크게 기인할 수 있는 SDS 기반 방법의 확장된 최적화 시간을 해결하기 위해 보다 효율적인 명시적 3차원 표현인 3차원 가우시안[24]을 사용하는 방법을 제안한다. 구체적으로, 텍스트 프롬프트와 조건 영상을 처리하기 위한 다단계 파이프라인(multi-stage pipeline)을 제안한다 : 1) 먼저 MVControl을 이용하여 4개의 다시점 영상을 생성하고, 이를 최근 소개된 가우시안 재구성 모델인 LGM[55]에 입력한다. 이 단계는 거친 3D 가우시안 세트를 산출한다. 2) 이어서, 거친 가우시안들은 하이브리드 확산 유도 접근법을 사용하여 최적화를 거치게 되며, 우리의 MVControl과 2D 확산 모델을 결합한다. 이 단계에서는 가우시안 기하학을 개선하기 위해 SuGaR[17] 정칙화 항을 소개한다. 3) 최적화된 가우시안들을 거친 가우시안 결합 메쉬로 변환시켜 텍스처와 기하학의 추가적인 미세화를 꾀한다. 마지막으로, 정제된 가우시안 결합 메쉬로부터 고품질의 텍스처 메쉬를 추출한다.\n' +
      '\n' +
      '요약하면, 우리의 주요 기여는 다음과 같다:\n' +
      '\n' +
      '* 우리는 제어 가능한 미세 입자 텍스트-다시점 이미지 생성을 위해 설계된 새로운 네트워크 아키텍처를 소개한다. 모델은 다양한 조건 유형(에지, 깊이, 정규 및 스크리블)에 걸쳐 평가되며, 그 일반화 능력을 입증하고;\n' +
      '* 대형 재구성 모델의 강점과 점수 증류를 결합한 다단적이지만 효율적인 3D 생성 파이프라인을 개발합니다. 이 파이프라인은 거친 가우시안에서 SuGaR까지 3D 자산을 최적화하여 메쉬로 절정에 달한다. 중요하게도, 우리는 3D 생성 영역에서 가우시안-메쉬 하이브리드 표현의 잠재력을 처음으로 탐구한다;\n' +
      '* 광범위한 실험 결과는 고충실도 다시점 영상과 3D 자산을 생성하는 우리의 방법의 능력을 보여준다. 이러한 출력은 입력 조건 이미지와 텍스트 프롬프트를 사용하여 정밀하게 제어될 수 있다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '다중 뷰 확산 모델.대형 확산 모델을 통한 텍스트 대 이미지 생성의 성공은 다중 뷰 이미지 생성의 발전에 영감을 준다. 일반적으로 채택되는 접근법은 추가 입력 이미지 및 타겟 포즈[31, 32, 34]에 의해 확산 모델을 컨디셔닝하는 것이다. 이러한 방법들과는 달리, Chan 등은 최근 단일 또는 다수의 입력 이미지로부터 3D 장면 표현을 학습한 다음, 타겟 신규 뷰 이미지 합성을 위한 확산 모델을 이용하는 것을 제안한다[10]. 단일 타겟 뷰 이미지를 생성하는 대신에, MVD 확산[58]은 하나의 피드-포워드 패스에서 멀티-뷰 일관성 이미지들을 생성하는 것을 제안한다. 그들은 더 나은 일반화 능력을 갖추기 위해 사전 훈련된 확산 모델을 기반으로 한다. MVDream[50]은 텍스트 프롬프트로부터 일관된 멀티뷰 이미지들을 생성하기 위한 방법을 도입한다. 그들은 3D 데이터 세트를 사용하여 미리 훈련된 확산 모델을 미세 조정함으로써 이를 달성한다. 그런 다음 훈련된 모델은 점수 증류 샘플링을 통해 3D 표현을 최적화하기 전에 3D로 활용된다. 유사 작품 이미지드림[60]은 텍스트 조건을 이미지로 대체한다. 이전 작업은 인상적인 신규/다시점 일관 이미지를 생성할 수 있지만, 텍스트 대 이미지 생성을 위해 ControlNet[69]이 달성한 것처럼 생성된 텍스트 대 다시점 이미지에 대한 세밀한 제어는 여전히 달성하기 어렵다. 따라서 본 논문에서는 확산 기반 다시점 영상 생성을 더욱 발전시키기 위해 다시점 제어넷(Multi-view ControlNet, MVControl)을 제안한다.\n' +
      '\n' +
      '**3D 생성 작업.**3D 모델을 생성하는 탐색은 전형적으로 두 가지 접근법으로 분류될 수 있다. 첫 번째는 먼저 DreamFusion[42]에서 제안한 SDS 기반 최적화 방법으로, 사전에 훈련된 대형 이미지 모델의 활용을 통해 3D 생성을 위한 지식을 추출하는 것을 목표로 한다. SDS 기반 방법은 확장된 3D 데이터 세트를 요구하지 않는 것으로부터 이익을 얻으므로 후속 작업 [11, 29, 52, 66, 63, 70, 56]에서 광범위하게 탐구되었다. 이러한 작업은 종종 더 정교한 점수 증류 손실 기능을 개발하고 최적화 전략을 정제하고/하거나 더 나은 3D 표현을 사용하여 생성된 객체의 품질을 더욱 향상시킴으로써 귀중한 통찰력을 제공한다. 텍스트 설명을 기반으로 높은 충실도의 3D 도형을 생성하는 데 이러한 방법에 의해 달성된 성공에도 불구하고, 일반적으로 텍스트 대 3D 생성 프로세스를 완료하는 데 시간이 필요하다. 반대로, 피드포워드 3D 네이티브 방법은 광범위한 3D 데이터 세트에 대한 트레이닝 후 몇 초 이내에 3D 자산을 생성할 수 있다[14]. 연구자들은 체적 표현[64, 6, 15, 28], 삼각형 메쉬[13, 16, 54, 67], 포인트 클라우드[1, 2], 암시적 신경 표현[19, 62, 12, 19, 26, 41, 47], 그리고 최근의 3D 가우시안[55]과 같은 향상된 결과를 얻기 위해 다양한 3D 표현을 탐구했다. 이러한 방법 중 일부는 입력 조건을 완전히 만족하는 3D 모델을 효율적으로 생성할 수 있다. 일부 방법은 입력 조건에 맞는 3D 모델을 효율적으로 생성하는 반면, 3D 생성 방법은 이미지 생성 모델링과 달리 제한된 3D 트레이닝 자산으로 인해 어려움을 겪는다. 이러한 희소성은 높은 충실도와 다양한 3D 객체를 생산하는 능력을 방해한다. 제안된 방법은 MVControl의 출력으로 조절된 피드포워드(feed-forward) 방법을 사용하여 거친 3D 객체를 생성한 후, 최종 표현을 위해 SDS 손실을 사용하여 정제하는 두 가지 접근법을 병합한다.\n' +
      '\n' +
      '**최적화 기반 메쉬 생성.** 메쉬 확산[33]과 같은 현재의 단일 단계 메쉬 생성 방법은 구조가 매우 복잡하여 고품질의 메쉬를 생산하는데 어려움을 겪고 있다. 기하학과 질감 모두에서 높은 등급의 메쉬를 달성하기 위해 연구자들은 종종 다단계 최적화 기반 방법[11, 29, 52]으로 돌아간다. 이러한 방법들은 일반적으로 처리하기 쉬운 비-메쉬 중간 표현들을 사용하고, 메쉬 재구성 방법들을 사용하여 메쉬들로 다시 변환하기 전에, 이는 긴 최적화 시간을 소모할 수 있다. DreamGaussian[56]은 훈련 시간을 효과적으로 줄이기 위해 보다 효율적인 표현인 3D Gaussian을 의미한다. 그러나, 수백만 개의 조직화되지 않은 작은 3D 가우시안으로부터 메쉬를 추출하는 것은 여전히 어려운 일이다. LGM[55]은 3D 가우시안(Gaussian)을 위한 새로운 메쉬 추출 방법을 제시하지만, 여전히 암시적 표현에 의존한다. 대조적으로, 우리는 SuGaR[17]에 의해 제안된 바와 같이 메쉬와 3D 가우시안 혼성인 완전 명시적 표현을 채택한다. 이 방법을 사용하면 합리적인 최적화 시간 내에 고품질 메쉬 생성을 달성할 수 있다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '먼저 3.1절에서 2D ControlNet[69], 스코어 증류 샘플링[42], Gaussian Splatting[24], SuGaR[17]과 같은 관련 방법들을 검토하고, 3.2절에서 Multi-view ControlNet을 학습하여 MVDream에 추가적인 공간 컨디셔닝을 도입하는 전략을 분석한다. 마지막으로 3.3절에서는 학습된 Multi-view ControlNet을 기반으로 Gaussian-binded mesh와 추가 텍스처 메쉬를 통해 제어 가능한 텍스트 대 3D 생성을 실현하기 위한 효율적인 3D 생성 파이프라인을 제안한다.\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      'ControlNet.ControlNet[69]은 텍스트 프롬프트들과 함께 추가적인 입력 조건들(예를 들어, 캐니 에지들, 스케치들, 깊이 맵들 등)을 수용하기 위해 사전 훈련된 대형 텍스트-이미지 확산 모델들을 용이하게 하여, 생성된 콘텐츠에 대한 정밀한 제어를 가능하게 한다. 추가 모듈로 SD의 인코더 블록과 미드 블록의 구조와 가중치를 백본 모델에 직접 복사한다. ControlNet 모듈의 인코더 블록 및 미드 블록의 각 레이어의 출력 피처 맵은 1x1 컨볼루션을 사용하여 백본 모델의 디코더 블록 및 미드 블록의 대응하는 대칭 레이어에 주입된다. 백본 모델의 생성 능력을 보존하고 원활한 훈련 개시를 보장하기 위해 1x1 컨볼루션을 0으로 초기화한다.\n' +
      '\n' +
      'Score Distillation Sampling.Score distillation sampling (SDS) [42, 29]는 텍스트-조건화된 3D 자산의 생성을 안내하기 전에 사전 훈련된 텍스트-이미지 확산 모델을 이용한다. 구체적으로, 사전 훈련된 확산 모델 \\(\\epsilon_{\\phi}\\)이 주어지면, SDS는 \\(\\theta\\)에 대한 손실 \\(\\mathcal{L}_{\\text{SDS}}\\)의 구배를 사용하여 미분 가능한 3D 표현(예: 신경 복사 필드)의 매개변수 \\(\\theta\\)을 최적화한다:\n' +
      '\n' +
      '[\\nabla_{\\theta}\\mathcal{L}_{\\text{SDS}(\\phi,\\mathbf{x})=\\mathbbb{E}_{t,\\epsilon}\\left[w(t)(\\hat{\\epsilon}_{\\phi}-\\epsilon)\\frac{\\partial z_{t}{\\partial\\theta}\\right], \\tag{1}\\t.\n' +
      '\n' +
      '여기서 \\(\\mathbf{x}=g(\\theta,c)\\)는 카메라 포즈 \\(c\\) 하에서 \\(g\\)에 의해 렌더링된 이미지이고, \\(w(t)\\)는 타임스탱 \\(t\\)에 의존하는 가중 함수이고, \\(z_{t}\\)은 \\(t\\)번째 타임스탱에 대응하는 \\(\\mathbf{x}\\)에 가우시안 잡음 \\(\\epsilon\\)을 더하여 얻어진 확산 모델에 입력되는 잡음 이미지이다. 주요 통찰력은 미리 훈련된 확산 모델의 분포를 고수하기 위해 학습 가능한 3D 표현의 렌더링된 이미지를 강제하는 것이다. 실제로, 모든 최적화 단계에서 타임스텝 \\(t\\)과 가우시안 잡음 \\(\\epsilon\\)의 값을 랜덤하게 샘플링한다.\n' +
      '\n' +
      'Gaussian Splatting과 SuGaR.Gaussian Splatting[24]는 3D Gaussians의 집합으로 장면을 표현하는데, 각 가우스 \\(g\\)은 중심 \\(\\mu_{g}\\in\\mathbb{R}^{3}\\)과 공분산 \\(\\Sigma_{g}\\in\\mathbb{R}^{3\\times 3}\\)으로 특징지어진다. 공분산\\(\\Sigma_{g}\\)은 스케일링 인자\\(s_{g}\\in\\mathbb{R}^{3}\\)와 회전 쿼터니언\\(q_{g}\\in\\mathbb{R}^{4}\\)에 의해 매개변수화된다. 또한 각 가우시안들은 스플래팅을 통한 렌더링을 위해 불투명도\\(\\alpha_{g}\\in\\mathbb{R}\\)와 색상 특징\\(c_{g}\\in\\mathbb{R}^{C}\\)을 유지한다. 일반적으로 색상 특징은 뷰 의존적 효과를 모델링하기 위해 구면 고조파를 사용하여 표시된다. 렌더링하는 동안 3차원 가우시안들은 2차원 가우시안으로서 2차원 이미지 평면에 투영되고, 컬러 값들은 이들 2차원 가우시안들의 전방-후방 깊이 순서로 알파 구성을 통해 계산된다. 바닐라 가우스 스플래팅 표현은 기하학적 모델링에서 잘 수행되지 않을 수 있지만, SuGaR[17]은 3D 가우시안들을 물체 표면과의 평탄도 및 정렬을 강제하기 위해 여러 규칙화 항들을 도입한다. 이것은 푸아송 재구성을 통해 가우시안으로부터 메쉬의 추출을 용이하게 한다[23]. 또한, SuGaR은 가우시안들을 메쉬 면에 결합함으로써 하이브리드 표현을 제공하여 역전파를 통한 텍스쳐와 기하학의 조인트 최적화를 가능하게 한다.\n' +
      '\n' +
      '### Multi-view ControlNet\n' +
      '\n' +
      '제어된 텍스트-투-이미지 생성 및 최근에 출시된 텍스트-투-멀티-뷰 이미지 확산 모델(예: MVDream)에서 ControlNet에 의해 영감을 받아 제어된 제어를 달성하기 위한 멀티-뷰 버전의 ControlNet(예: MVControl)을 설계하는 것을 목표로 한다.\n' +
      '\n' +
      '그림 2: 제안된 방법의 **개요.**(a) MVControl은 동결된 다시점 확산 모델과 훈련 가능한 MVControl로 구성된다. (b) 우리의 모델은 컨디셔닝 모듈을 통해 국부적 및 전역적으로 생성 프로세스를 제어하기 위해 모든 입력 조건을 처리한다. (c) MVControl이 훈련되면 SDS 최적화 절차를 통해 제어 가능한 텍스트 대 3D 콘텐츠 생성을 위해 하이브리드 확산을 제공하기 위해 활용할 수 있다.\n' +
      '\n' +
      '텍스트 대 다중 뷰 생성. 도 1에 도시된 바와 같다. 둘째, ControlNet과 유사한 아키텍처 스타일, 즉 잠김 사전 훈련된 MVDream과 훈련 가능한 제어 네트워크를 따른다. 주요 통찰은 학습된 MV-Dream의 사전 지식을 보존하는 반면, 제어 네트워크를 훈련하여 적은 양의 데이터로 귀납적 편향을 학습하는 것이다. 제어 네트워크는 컨디셔닝 모듈과 MVDream의 인코더 네트워크의 사본으로 구성된다. 우리의 주요 기여는 컨디셔닝 모듈에 있으며 다음과 같이 자세히 설명한다.\n' +
      '\n' +
      '컨디셔닝 모듈(도. 2b) 조건 영상\\(c\\), 4개의 카메라 행렬\\(\\mathcal{V}_{*}\\in\\mathbb{R}^{4\\times 4}\\) 및 타임스텝\\(t\\)을 입력으로 받아 4개의 로컬 제어 임베딩(e_{t,c,v_{*}}^{l}\\) 및 글로벌 제어 임베딩(e_{t,c,v_{*}}^{g}\\)을 출력한다. 로컬 임베딩은 입력 잡음 잠재 특성\\(\\mathcal{Z}_{t}\\in\\mathbb{R}^{4\\times C\\times H\\times W}\\)을 제어 네트워크에 입력으로 추가하고, 전역 임베딩\\(e_{t,c,v_{*}}^{g}\\)을 MVDream과 MVControl의 각 계층에 주입하여 전역 제어를 생성한다.\n' +
      '\n' +
      '조건 이미지 \\(c\\)(즉, 에지 맵, 깊이 맵 등)을 4개의 컨볼루션 레이어로 처리하여 특징 맵 \\(\\Psi\\)을 얻는다. MVDream의 절대 카메라 포즈 행렬 임베딩을 사용하는 대신, 임베딩을 컨디셔닝 모듈로 이동시킨다. 네트워크가 서로 다른 뷰들 사이의 공간적 관계를 더 잘 이해할 수 있도록, 조건 이미지에 대한 상대적인 카메라 포즈들은 카메라 행렬들\\(\\mathcal{V}_{*}\\)과 함께 사용된다. 실험 결과는 또한 설계의 유효성을 검증한다. 카메라 행렬 임베딩은 타임스텝 임베딩과 결합한 후, zero-initialized 모듈 \\(\\mathcal{M}_{1}\\)에 의해 특징맵 \\(\\Psi\\)과 동일한 차원을 갖도록 매핑된다. 이 두 부분의 합은 컨볼루션 레이어를 통해 국부 임베딩 \\(e_{t,c,v_{*}}^{l}\\)에 투영된다.\n' +
      '\n' +
      'MVDream은 절대 카메라 포즈로 사전 훈련되는 동안, 컨디셔닝 모듈은 상대 포즈를 입력으로 이용한다. 우리는 두 좌표 프레임의 불일치로 인해 네트워크가 거의 수렴하지 않는다는 것을 실험적으로 발견했다. 따라서 본 논문에서는 추가적인 네트워크 \\(\\mathcal{M}_{2}\\)을 이용하여 변환을 학습하고 전역 임베딩 \\(e_{t,c,v_{*}}^{g}\\)을 출력하여 MVDream의 원래 카메라 매트릭스 임베딩을 대체하고 MVDream과 MVControl 부분의 타임스텝 임베딩을 추가하여 의미적, 시점 의존적 특징을 전역적으로 주입한다.\n' +
      '\n' +
      '### 제어 가능한 3D 텍스처 메쉬 생성\n' +
      '\n' +
      '본 절에서는 효율적인 다단계 텍스처 메쉬 생성 파이프라인, 즉 조건 이미지와 그에 상응하는 기술 프롬프트가 주어지면, 먼저 훈련된 MVControl에 의해 생성된 4개의 다시점 이미지와 함께 LGM[55]을 사용하여 거친 3D 가우시안 집합을 생성한다. 그 후, 거친 가우시안들은 기하학을 향상시키고 거친 SuGaR 메쉬 추출을 용이하게 하는 것을 목표로 하는 여러 규칙화 용어들이 보충된 하이브리드 확산(hybrid diffusion)을 사용하여 미세화를 겪는다. 추출된 거친 SuGaR 메쉬의 질감과 기하학은 모두 높은 해상도에서 2D 확산 안내를 사용하여 정제되어 질감 메쉬의 달성에서 절정에 달한다. 전체 파이프라인이 그림 3에 나와 있다.\n' +
      '\n' +
      '**거친 가우시안 초기화.** LGM의 놀라운 성능[55] 덕분에, 우리의 MVControl 모델에 의해 생성된 이미지들은 LGM에 직접 입력되어 3D 가우시안 세트를 생성할 수 있다. 그러나 거친 가우시안들의 품질이 낮기 때문에 원래 논문에서 수행한 것처럼 메쉬로 직접 옮기는 것은 만족스러운 결과를 얻지 못한다. 대신, 우리는 최적화 단계를 더 적용하여 거친 가우시안들을 정제하고, 최적화의 시작점은 모든 거친 가우시안들의 특징들로 초기화되거나 그들의 위치들만으로 초기화된다.\n' +
      '\n' +
      '**Gaussian-to-SuGaR Optimization.** 이 단계에서, 우리는 거친 가우시안 \\(\\theta\\)의 최적화를 향상시키기 위해 2D 확산 모델의 하이브리드 확산 안내와 MVControl을 통합한다. MVControl은 4개의 정준 뷰\\(\\mathcal{V}_{*}\\)에 걸쳐 견고하고 일관된 기하학 안내를 제공하는 반면, 2D 확산 모델은 다른 랜덤 샘플링 뷰\\(\\mathcal{V}_{r}\\in\\mathbb{R}^{B\\times 4\\times 4}\\)에서 미세한 기하학 및 질감 조각에 기여한다. 여기서는 DeepFloyd-IF 기반 모델[3]이 거친 기하 구조를 정제하는 데 우수한 성능으로 활용된다. 텍스트 프롬프트\\(y\\)와 조건 이미지\\(h\\)이 주어지면, 하이브리드 SDS 구배\\(\\nabla_{\\theta}\\mathcal{L}_{SDS}^{hybrid}\\)는 다음과 같이 계산될 수 있다.\n' +
      '\n' +
      '[\\nabla_{\\theta}\\mathcal{L}_{SDS}^{hybrid}=\\lambda_{2D}\\nabla_{L}_{SDS}^{2D}(\\mathbf{x}_{r}=g(\\theta,\\mathcal{V}_{r});t,y)+\\lambda_{3D}\\nabla_{theta}\\mathcal{L}_{SDS}^{3D}(\\mathbf{x}_{*}=g(\\theta,\\mathcal{V}_{*});t,y,h), \\tag{2}\\tag{2}}\n' +
      '\n' +
      '여기서 \\(\\lambda_{1}\\) 및 \\(\\lambda_{2}\\)은 각각 2D 및 3D 이전의 강도이다. 가우스 최적화 단계에서 기하학 학습을 향상시키기 위해 깊이와 알파 값을 렌더링할 수 있는 가우시안 래스터화 엔진을 사용한다[5]. 구체적으로, 컬러 영상 외에도 장면의 깊이\\(\\hat{d}\\)와 알파\\(\\hat{m}\\)도 렌더링하고, 도함수를 취하여 표면 법선\\(\\hat{n}\\)을 추정한다.\n' +
      '\n' +
      '그림 3: **제안된 3D 생성 파이프라인.** 다단계 파이프라인은 LGM에 의해 생성된 거친 가우시안 세트로부터 시작하여 고품질 텍스처 메쉬를 효율적으로 생성할 수 있으며, 입력은 MVControl에 의해 생성된 다시점 이미지이다. 두 번째 단계에서는 가우시안 최적화를 위해 2D & 3D 하이브리드 확산을 사용한다. 마지막으로, 세 번째 단계에서, 우리는 SuGaR 표현을 정제하기 위해 VSD 손실을 계산한다.\n' +
      '\n' +
      'of \\(\\hat{d}\\) 결과적으로, 이들 성분에 대한 총 변동(TV) 정규화 항 [45]를 \\(\\mathcal{L}_{TV}^{d}\\) 및 \\(\\mathcal{L}_{TV}^{n}\\)으로 계산하고 하이브리드 SDS 손실에 통합한다. 또한 입력 조건이 기존 영상에서 변함없이 도출됨에 따라 중간 과정에서 전경 마스크 \\(m_{gt}\\)가 생성된다. 따라서 장면의 희소성을 보장하기 위해 마스크 손실\\(\\mathcal{L}_{mask}=\\text{MSE}(\\hat{m},mgt)\\)을 계산한다. 따라서, 가우시안 최적화를 위한 총 손실은 다음과 같이 표현된다:\n' +
      '\n' +
      '\\mathcal{L}_{GS}=\\mathcal{L}_{SDS}^{hybrid}+\\lambda_{1}\\mathcal{L}_{TV}^{d}+\\lambda_{2}\\mathcal{L}_{TV}^{n}+\\lambda_{3}\\mathcal{L}_{mask}, \\tag{3}\\mathcal{L}_{mask}\n' +
      '\n' +
      '여기서 \\(\\lambda_{k=1,2,3}\\)는 깊이 TV 손실, 정상 TV 손실 및 마스크 손실의 가중치이다. [11]의 접근법에 따라 SDS 기울기를 계산할 때 확산 모델에 입력으로 RGB 이미지 또는 정규 지도를 교대로 사용한다. 일정 수의 최적화 단계 \\(N_{1}\\) 후에 가우시안 분할과 가지치기를 중지한다. 그 후, 가우시안들이 평평해지고 물체 표면에 정렬되도록 하기 위해 새로운 손실 항으로 SuGaR 정규화 항 [17]을 \\(\\mathcal{L}_{GS}\\)에 도입한다. 이 과정은 추가 단계(N_{2}\\) 동안 계속되며, 그 후 불투명도가 임계값\\(\\bar{\\sigma}\\) 미만인 모든 점을 제거한다.\n' +
      '\n' +
      '**SuGaR Refinement.** [17]의 공식 파이프라인에 이어서, 최적화된 가우시안들을 거친 메쉬로 이송한다. 각 삼각형 면에 대해, 새로운 평평한 가우시안 세트가 묶여 있다. 이 새롭게 묶인 가우시안들의 색은 삼각형 꼭짓점의 색으로 초기화된다. 가우시안들의 위치는 미리 정의된 2차 중심 좌표로 초기화되며, 회전은 대응하는 삼각형 내에서 가우시안들을 구속하기 위해 2D 복소수로 정의된다. 원래 구현과 달리, 우리는 처음부터 최적화를 용이하게 하기 위해 많은 수, 특히 0.9로 가우시안들의 학습 가능한 불투명도를 초기화한다. 거친 메쉬의 기하학적 구조가 거의 고정되어 있기 때문에, 보다 높은 최적화 해상도를 얻기 위해 하이브리드 확산 안내를 Stable Diffusion[44]을 사용하여 계산된 2D 확산 안내만으로 대체한다. 또한, 질감 최적화에서 우수한 성능으로 Variational Score Distillation (VSD) [63]을 사용한다. 마찬가지로, 우리는 결합된 가우시안들을 통해 깊이\\(\\hat{d}^{\\prime}\\)와 알파\\(\\hat{m}^{\\prime}\\)을 렌더링한다. 반면, 메쉬 얼굴 정규식을 이용하여 정규지도(\\hat{n}^{\\prime}\\)를 직접 렌더링할 수 있다. 이러한 조건을 이용하여 이전 구간과 유사하게 TV 손실, \\(\\mathcal{L}_{TV}^{dd}\\) 및 \\(\\mathcal{L}_{TV}^{n}\\) 및 마스크 손실 \\(\\mathcal{L}_{mask}^{\\prime}\\)을 계산하였다. SuGaR 미세화에 대한 전체 손실은 다음과 같이 계산된다:\n' +
      '\n' +
      '수GaR}=\\mathcal{L}_{VSD}+\\lambda_{1}^{\\prime}\\mathcal{L}_{TV}^{ d}+\\lambda_{2}^{\\prime}\\mathcal{L}_{TV}^{n}+\\lambda_{3}^{\\prime}\\mathcal{L}_{mask}^{ \\prime}, \\tag{4}\\mathcal{L}_{VSD}+\\lambda_{1}^{\\prime}\\mathcal{L}_{TV}^{ d}+\\lambda_{2}^{\\prime}\\mathcal{L}_{TV}^{n}+\\lambda_{3}^{\\prime}\\mathcal{L}_{mask}^{ \\prime}, \\tag{4}\\mathcal{L}_{mask}^{\n' +
      '\n' +
      '여기서 \\(\\lambda_{k=1,2,3}^{\\prime}\\)는 서로 다른 손실항의 가중치를 각각 나타낸다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '####4.1.1 훈련 데이터.\n' +
      '\n' +
      'MVControl을 훈련하기 위해 공개적으로 사용할 수 있는 대규모 3D 데이터 세트인 Objavverse[14]에서 다중 뷰 렌더링을 사용한다. 처음에는 [53]의 레이블링 기준에 따라 CLIP 점수가 22 미만인 모든 샘플을 제거하여 데이터 세트를 전처리한다. 이 필터링은 약 400k의 나머지 샘플을 생성한다. 3D 자산의 이름과 태그를 활용하는 대신 [35]의 캡션을 보유 객체에 대한 텍스트 설명으로 사용한다. [50]의 접근 방식에 따라, 우리의 네트워크는 2D 및 3D 데이터 세트 모두에 대해 훈련된다. 구체적으로, 우리는 네트워크가 학습된 2D 이미지 이전을 유지하도록 훈련 동안 30% 확률로 LAION[46]의 AES v2 하위 집합에서 이미지를 무작위로 샘플링한다. 또한, 3D 지식을 학습하기 위해 70% 확률로 큐레이션된 3D/다시점 이미지 데이터 세트에서 샘플링한다. 다양한 유형의 조건 이미지를 준비하는 과정은 부록에 자세히 설명되어 있습니다.\n' +
      '\n' +
      'MVControl의 4.2.3 Training details.\n' +
      '\n' +
      '우리는 미리 훈련된 MVDream과 ControlNet의 가중치를 활용하여 네트워크를 초기화한다. 잠긴 네트워크와 훈련 가능한 네트워크 사이의 모든 연결은 0으로 초기화됩니다. MVControl은 256의 해상도로 훈련되며, 선택된 배치 크기는 2560 이미지이다. 이 모델은 8개의 Nvidia Tesla A100 GPU에 AdamW 최적화기 [25]를 사용하여 보수적인 학습률 \\(4e^{-5}\\)을 사용하여 50,000단계의 미세 조정을 거친다. [69]의 접근법과 유사하게, 우리는 분류기 없는 학습을 용이하게 하고 입력 조건 이미지에 대한 모델의 이해를 향상시키기 위해 훈련 중에 50% 확률로 텍스트 프롬프트를 빈 것으로 랜덤하게 드롭한다.\n' +
      '\n' +
      '####4.2.4 3D 세대.\n' +
      '\n' +
      '코어스 가우시안 최적화 단계는 총 3000 단계로 구성된다. 초기 1500 단계 동안 분할 및 프루닝으로 간단한 3D 가우시안 최적화를 수행한다. 1500 단계 후에, 우리는 가우스 분할 및 프루닝을 중단하고 대신에 장면을 정제하기 위해 SuGaR[17] 정규화 용어를 도입한다. 우리는 가장 가까운 이웃의 수를 16개로 설정하여 50단계마다 업데이트한다. 훈련의 마지막 단계에서, 우리는 불투명도가 \\(\\bar{\\sigma}=0.5\\) 이하인 모든 가우시안들을 가지치기한다. 2차원 및 3차원 확산 유도에는 각각 512와 256의 해상도로 \\(\\lambda_{2D}=0.1\\)과 \\(\\lambda_{3D}=0.01\\)을 사용한다. 거친 SuGaR 추출을 위해 2e5개의 정점을 대상으로 각 삼각형에 6개의 가우시안들을 결합한다. SuGaR 정제 단계에서, 우리는 VSD 손실로 5000 단계에 대해 장면을 최적화한다[63].\n' +
      '\n' +
      '그림 4: **Multi-view 이미지 생성 w/ 및 w/o의 MVControl.** MVDream 생성 결과를 에지 맵과 노말 맵이 각각 입력 조건으로 부착된 MVControl과 부착하지 않은 MVDream 생성 결과이다.\n' +
      '\n' +
      '### Qualitative Comparisons\n' +
      '\n' +
      '####4.2.1 다시점 영상 생성 방법.\n' +
      '\n' +
      'MVControl의 제어 가능성을 평가하기 위해 MVControl이 부착된 경우와 부착되지 않은 경우 모두 MVDream에 대한 실험을 수행한다. 첫 번째 경우 MVDream은 주어진 프롬프트에 따라 올바른 콘텐츠를 생성하지 못하여 옷이 없는 스탠딩 고양이를 생산하는데, 이는 프롬프트와 모순된다. 이와는 대조적으로, MVControl의 도움으로, 그것은 올바른 콘텐츠를 성공적으로 생성한다. 두 번째 사례에서도 MVControl이 MVDream의 생성을 효과적으로 제어하여 뷰 일관성이 높은 다시점 영상을 생성함을 보인다.\n' +
      '\n' +
      '####4.2.2 3차원 가우시안 기반 메쉬 생성\n' +
      '\n' +
      '3D 생성 파이프라인은 3D 가우시안으로부터 텍스처 메쉬를 생성하는 것을 목표로 하기 때문에, 본 방법을 최근 가우시안 기반 메쉬 생성 접근법인 DreamGaussian[56]과 비교한다.\n' +
      '\n' +
      '도 5: **기준 3D 생성 방법과의 비교.** 우리의 방법은 더 섬세한 텍스처를 산출하며, 비교된 방법보다 훨씬 더 우수한 메시를 생성한다.\n' +
      '\n' +
      'LGM[55]은 둘 다 RGB 이미지 상에서 컨디셔닝될 수 있다. 공정한 비교를 위해 2D ControlNet을 이용하여 2D RGB 영상을 생성한다. 도 1에 도시된 바와 같다. 도 5에 도시된 바와 같이, 드림가우시안(DreamGaussian)은 대부분의 예시들에 대한 기하학을 생성하기 위해 고군분투하고, 생성된 메쉬들에서 많은 부서지고 중공된 영역들을 생성한다. LGM은 DreamGaussian보다 성능이 우수하지만, 추출된 메쉬는 세부 정보가 부족하고 일부 경우 여전히 파손된 영역을 포함한다. 대조적으로, 본 방법은 RGB 조건이 없어도 보다 섬세한 질감의 미세 입자 메쉬를 생성한다. 공간 제한으로 인해 텍스트 프롬프트는 그림 1에 제공되지 않는다. 5, 부록에 포함시키겠습니다. 그림의 모든 이미지는 블렌더를 사용하여 렌더링됩니다.\n' +
      '\n' +
      '** 암시적 표현 기반 메쉬 생성.** 모델에 대한 종합적인 평가를 제공하기 위해, 우리는 암시적 표현을 기반으로 하는 3D 생성 작업으로 MVControl의 적용을 확장한다. 우리는 우리의 접근법을 최신 이미지-to-3D 방법, DreamCraft3D[52]와 비교한다. 간단한 coarse-to-fine 최적화 절차를 사용하여 이를 달성한다. 처음에 우리는 거친 NeuS[59] 모델을 생성한 다음 추가 지오메트리 최적화를 위해 거친 DMTet[48]으로 전달한다. 최종 단계는 지오메트리가 고정된 텍스처 미세화에 초점을 맞춘다. 하이브리드 확산 안내는 다양한 중량 항과 함께 세 단계 모두에서 활용된다. 도 1에 도시된 바와 같다. 도 6에 도시된 바와 같이, 본 방법은 RGB 신호가 없는 경우에도 드림크래프트3D에 필적하는 기하학 및 텍스처를 갖는 3D 자산을 생성할 수 있다. 또한, 동일한 프롬프트를 사용하여 기본 모델인 MVDream에 대한 실험을 수행했다. 결과는 프롬프트만으로는 MV컨트롤을 사용하지 않고 MVDream의 생성을 정확하게 제어할 수 없음을 보여준다.\n' +
      '\n' +
      '### Quantitative Comparisons\n' +
      '\n' +
      '이 섹션에서는 비교된 방법과 우리의 방법을 모두 평가하기 위해 CLIP 점수[39]를 채택한다. 작업에 참조 뷰의 프롬프트와 조건 이미지의 두 가지 조건이 포함된다는 점을 감안할 때 이미지 텍스트와 조건 이미지를 모두 계산합니다.\n' +
      '\n' +
      '그림 6: 암시적 표현 기반 3D 생성 방법과의 비교. 모델은 최신 이미지-to-3D 방법 드림크래프트3D[52] 및 기본 모델 MVDream[50]과 비교한다.\n' +
      '\n' +
      '이미지-이미지 유사성. 각 객체에 대해 36개의 주변 뷰를 균일하게 렌더링합니다. CLIP-T로 표시된 이미지-텍스트 유사도는 각 뷰와 주어진 프롬프트 사이의 유사성을 평균화하여 계산되며, 마찬가지로 CLIP-I로 표시된 이미지-이미지 유사도는 각 뷰와 참조 뷰 사이의 평균 유사성이다. 60개의 객체 집합에 대해 계산된 결과는 표 1에 보고되어 있으며, 이 방법을 사용할 때 각 객체에 사용된 조건 유형은 에지, 깊이, 정규 및 스크리블 맵에서 무작위로 샘플링된다. 또한, 동일한 조건 영상과 프롬프트를 갖는 2D ControlNet을 이용하여 DreamGaussian과 LGM에 대한 RGB 영상을 생성한다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'MVControl의####4.4.1 컨디셔닝 모듈.\n' +
      '\n' +
      '우리는 카메라 조건을 도입하기 위해 세 가지 다른 설정에서 모델의 훈련을 평가한다: 1) 절대(세계) 카메라 시스템(즉, Abs)을 활용한다. T) MVDream[50]과 같이 설계된 컨디셔닝 모듈(2D ControlNet과 동일한 설정 유지)을 사용하지 않고; 2) 컨디셔닝 모듈을 사용하지 않고 상대 카메라 시스템을 채택한다; 3) 완전한 컨디셔닝 모듈을 채택한다. 실험 결과는 그림 1에 나와 있다. 도 8을 참조하면, 완전한 컨디셔닝 모듈만이 컨디션 이미지에 의해 제공된 설명들을 준수하는 뷰-일관된 멀티-뷰 이미지들을 정확하게 생성할 수 있음을 입증한다.\n' +
      '\n' +
      '#### 4.4.2 하이브리드 확산 안내.\n' +
      '\n' +
      '가우시안 최적화 단계에서 사용되는 하이브리드 확산 안내에 대한 절제 연구를 수행한다. 도 1에 도시된 바와 같다. 8(상우측), 우리 MVControl에서 제공하는 \\(\\nabla_{\\theta}\\mathcal{L}_{SDS}^{3D}\\)을 제외한 경우, 생성된\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c} \\hline \\hline  & CLIP-T\\(\\uparrow\\) & CLIP-I\\(\\uparrow\\) \\\\ \\hline DreamGaussian [56] & 0.200 & 0.847 \\\\ LGM [55] & 0.228 & 0.872 \\\\ MVControl(Ours) & **0.245** & **0.897** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ** 정량적 비교.** CLIP 점수로 이미지-텍스트 및 이미지-이미지 유사성을 모두 계산한다.\n' +
      '\n' +
      '3D 가우시안들은 주어진 조건 에지 맵에 기술된 텍스처 세부사항들이 부족하다. 예를 들어, 토끼의 얼굴은 \\(\\nabla_{\\theta}\\mathcal{L}_{SDS}^{3D}\\) 없이 현저하게 흐릿하게 보인다.\n' +
      '\n' +
      '**렌더링된 정규 지도상의 손실.** 우리 방법의 정규 관련 손실은 단계 2에서 정규 지도를 입력으로 하는 SDS 손실과 정규 TV 정규화 항을 사용하여 교대로 계산된다. 2단계에서 모두 떨어뜨려 실험을 수행했으며 결과는 그림 1에 나와 있다. 8(좌측 하단). 전체 방법에 비해 3차원 가우시안들의 표면 법선은 정상과 관련된 손실 없이 열화된다.\n' +
      '\n' +
      '** 다단계 최적화.** 우리는 또한 그림 9와 같이 다양한 최적화 단계의 영향을 평가한다. 초기 단계 1에서 거친 가우시안들은 열악한 기하학적 일관성을 나타낸다. 그러나 가우시안 최적화 단계 이후에는 흐릿한 텍스처임에도 불구하고 뷰-일관성이 된다. 마지막으로, SuGaR 정제 단계에서 3D 모델의 질감은 세립화되고 고품질로 된다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '이 연구에서 우리는 제어 가능한 3D 생성의 중요하면서도 탐구되지 않은 분야를 탐구한다. 본 논문에서는 제어 가능한 텍스트-다시점 이미지 생성을 위한 새로운 네트워크 구조인 MVControl을 제안한다. 본 연구의 접근 방법은 제어 가능한 다시점 영상 생성을 가능하게 하기 위해 기본 영상 확산 모델과 상호 작용하는 훈련 가능한 제어 네트워크를 특징으로 한다. 일단 훈련되면, 훈련되면, 우리 네트워크는 다른 2D 확산 모델과 함께 하이브리드 SDS 구배를 사용하여 제어 가능한 텍스트 대 3D 생성을 위한 3D 확산 안내를 제공한다. 본 논문에서는 피드포워드(feed-forward)와 최적화(optimalization)를 이용한 효율적인 다단계 3D 생성 파이프라인을 제안한다. 우리의 선구적인 SuGaR(명시적 표현 혼합 메쉬와 3D 가우시안)의 사용은 이전의 가우시안 기반 메쉬 생성 접근법보다 우수하다. 실험 결과는 제어 가능한 고 충실도의 텍스트-다시점 이미지 및 텍스트-3D 자산을 생성하는 방법의 능력을 보여준다. 또한 다양한 조건에 걸친 테스트를 통해 본 방법의 일반화 기능을 보여줍니다. 우리는 우리 네트워크가 SDS 최적화를 통해 제어 가능한 3D 생성을 넘어 3D 비전 및 그래픽에서 더 광범위한 응용 프로그램을 가지고 있다고 믿는다.\n' +
      '\n' +
      '그림 9: 다단계 최적화에 대한 **절제 연구. 우리는 다양한 최적화 단계에서 절제 연구를 수행한다. 가우시안 최적화 및 SuGaR 정제 단계 모두 품질을 크게 향상시킨다. 또한 모든 단계에서 소비된 시간을 보고합니다.**\n' +
      '\n' +
      '## 부록 0. 소개\n' +
      '\n' +
      '이 보충 자료에서는 실험 설정 및 구현에 대한 추가 세부 정보를 제공합니다. 그 후, 다양한 형태의 조건 영상을 입력으로 하여 본 방법의 성능과 다양성을 보여주는 보다 질적인 결과를 제시한다.\n' +
      '\n' +
      '## 부록 0.B 구현 상세\n' +
      '\n' +
      '### Training Data\n' +
      '\n' +
      '**다시점 이미지 데이터세트.** 본 논문에서 설명한 바와 같이 Objavverse [14]의 객체 수를 약 400k로 필터링한다. 보유된 각 샘플에 대해 먼저 장면 경계 상자를 세계 원점을 중심으로 하는 단위 큐브로 정규화한다. 이어서, 카메라 거리 1.4~1.6, 시야각(FoV) 40~60도, 고도 0~30도, 시작 방위각 0~360도를 균일하게 선택하여 랜덤 카메라 설정을 샘플링한다. 랜덤 카메라 설정 하에서, 멀티뷰 이미지는 샘플링된 방위각으로부터 시작하여 동일한 고도에서 4개의 표준 뷰 하에서 256의 해상도로 렌더링된다. 트레이닝 동안, 이들 뷰들 중 하나가 조건 이미지에 대응하는 참조 뷰로서 선택된다. 우리는 각 대상에 대해 이 절차를 세 번 반복한다.\n' +
      '\n' +
      '**Canny Edges.** 우리는 Canny Edge 조건을 얻기 위해 렌더링된 모든 이미지에 랜덤 임계값을 갖는 Canny Edge 검출기[7]를 적용한다. 하위 임계치는 범위[50, 125]로부터 랜덤하게 선택되는 반면, 상위 임계치는 범위[175, 250]로부터 선택된다.\n' +
      '\n' +
      '**Depth Maps.** 미리 훈련된 깊이 추정기, Midas[43]를 사용하여 렌더링된 이미지의 깊이 맵을 추정한다.\n' +
      '\n' +
      '**Normal Maps.** 우리는 Midas가 예측한 깊이 값에 대해 Normal-from-distance를 계산하여 렌더링된 모든 이미지의 Normal Map 추정치를 계산한다.\n' +
      '\n' +
      '**User Scribble.** HED 경계 검출기 [65] 다음에 [69]에 설명된 것과 유사한 일련의 강력한 데이터 증강을 사용하여 렌더링된 이미지로부터 인간 낙서를 합성한다.\n' +
      '\n' +
      '### MVControl 훈련 세부사항\n' +
      '\n' +
      '기본 모델인 MVDream[50]은 Stable Diffusion v2.1[44]에서 미세 조정되지만, 일관성을 위해 Stable Diffusion v2.1에 적응된 공개적으로 사용 가능한 2D ControlNet 체크포인트1에서 멀티뷰 ControlNet 모델을 훈련한다. 모델은 8\\(\\times\\)A100 노드에서 학습되며, 각 GPU에는 160(40\\(\\times\\)4)개의 이미지가 있다. 2단계의 그래디언트 축적을 통해 총 2560개의 이미지 배치 크기를 달성한다. 우리는 1000단계의 워밍업으로 \\(4\\times 10^{-5}\\)의 일정한 학습률을 이용한다.\n' +
      '\n' +
      '### 3D Generation 구현 세부사항\n' +
      '\n' +
      '거친 가우시안 생성 단계에서는 MVControl이 MVDream에 부착된 상태에서 유도 척도가 9인 30단계 DDIM 샘플러[51]와 부정적인 프롬프트 "악성, 흐릿함, 픽셀화된 모호함, 부자연스러운 색상, 저조한 조명, 칙칙함, 불분명함, 크롭, 로우어, 저품질, 저품질, 아티팩트, 중복"을 사용하여 다시점 영상을 생성한다. 가우스 최적화 단계에서 가우시안들은 1500단계 이전에 300단계마다 치밀화 및 프루닝되며, 3D SDS(\\(\\nabla_{\\theta}\\mathcal{L}_{SDS}^{3D}\\))는 CFG 리스케일 트릭[30]을 사용하여 50의 유도 척도로 계산되고, \\(\\nabla_{\\theta}\\mathcal{L}_{SDS}^{2D}\\)는 20의 유도 척도로 계산되며, 이러한 점수증류항들은 7.5의 유도 척도로 계산된다. 우리의 구현은 트레스투디오 프로젝트를 기반으로 한다[18]. 상태 이미지 추출을 위한 모든 테스트 이미지는 civitai.com에서 다운로드됩니다.\n' +
      '\n' +
      '## 부록 0.C 추가 질적 결과\n' +
      '\n' +
      '### MVControl의 다양성\n' +
      '\n' +
      '2D ControlNet[69]과 유사하게, 우리의 MVControl은 동일한 조건 이미지와 프롬프트로 다양한 다시점 이미지를 생성할 수 있다. 결과 중 일부는 그림 1에 나와 있다. 10.\n' +
      '\n' +
      '### Textured Meshes\n' +
      '\n' +
      '또한 추가로 생성된 질감 메시를 제공합니다. 여러 개의 뷰 아래의 이미지들이 도에 도시되어 있다. 11 ~ 14. 비디오 및 대화형 메시 결과는 프로젝트 페이지를 참조하십시오.\n' +
      '\n' +
      '## 부록 0.3D 비교를 위한 텍스트 프롬프트\n' +
      '\n' +
      '여기에서 그림 1에서 누락된 텍스트 프롬프트를 제공한다. 아래와 같이 본 논문의 5:\n' +
      '\n' +
      '1. "A charming long brown coat dog, border collie, head of dog, upper body, dark brown fur on back,shelti,light brown fur on chest,ultra detailed, brown eye"\n' +
      '\n' +
      '2. "Wild Bear in a sheepskin coat and boots, open-armed, dancing, boots, patterned cotton clothes, cinematic, best quality."\n' +
      '\n' +
      '3. "두개골, 걸작, 브로콜리로 만든 인간 두개골"\n' +
      '\n' +
      '4. "흡연을 하고 있는 귀여운 펭귄은 스케이트보드, 사랑스러운 캐릭터, 매우 디테일이 있다."\n' +
      '\n' +
      '5. "명작, 배트맨, 초상화, 상체, 슈퍼히어로, 망토, 마스크"\n' +
      '\n' +
      '6. "Ral-chrome, fox, brown orange and white fur, sit, full body, 사랑스러운"\n' +
      '\n' +
      '7. "스파이더맨, 마스크, 검정 가죽 재킷, 펑크, 부조리, 만화책"\n' +
      '\n' +
      '8. "Marvel iron man, heavy armor suit, futuristic, very cool, slightly sideways, portrait, upper body" Figure 10: **Diverse generated multi-view images of MVControl.** Our MVControl can generate various multi-view images with same condition image and prompt"\n' +
      '\n' +
      '그림 11: 캐니 에지를 입력으로 하는 추가 질적 결과.\n' +
      '\n' +
      '그림 12: 정규 지도를 입력으로 하는 추가 정성적 결과.\n' +
      '\n' +
      '그림 13: 깊이 맵을 입력으로 하는 추가 정성적 결과.\n' +
      '\n' +
      '그림 14: 사용자 낙서를 입력으로 하는 추가 질적 결과.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Achlioptas, P., Diamanti, O., Mitliagkas, I., Guibas, L.: Learning representations and generative models for 3d point clouds. In: International conference on machine learning. pp. 40-49. PMLR (2018)\n' +
      '* [2] Albert Pumarola, Stefan Popov, F.M.N., Ferrari, V.: C-Flow: Conditional generative flow models for images and 3D point clouds. In: CVPR (2020)\n' +
      '* [3] Alex, S., Misha, K., Daria, B., Christoph, S., Ksenia, I., Nadiia, K.: Deepfloyd if: A modular cascaded diffusion model. [https://github.com/deep-floyd/IF/tree/developer](https://github.com/deep-floyd/IF/tree/developer) (2023)\n' +
      '* [4] Armandpour, M., Zheng, H., Sadeghian, A., Sadeghian, A., Zhou, M.: Re-imagine the negative prompt algorithm: Transform 2d diffusion into 3d, alleviate janus problem and beyond. arXiv preprint arXiv:2304.04968 (2023)\n' +
      '* [5] ashawkey: Differential gaussian rasterization. [https://github.com/ashawkey/diff-gaussian-rasterization](https://github.com/ashawkey/diff-gaussian-rasterization) (2023)\n' +
      '* [6] Brock, A., Lim, T., Ritchie, J.M., Weston, N.: Generative and discriminative voxel modeling with convolutional neural networks. arXiv preprint arXiv:1608.04236 (2016)\n' +
      '* [7] Canny, J.: A computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence (6), 679-698 (1986)\n' +
      '* [8] Cao, Z., Hong, F., Wu, T., Pan, L., Liu, Z.: Large-vocabulary 3d diffusion model with transformer. arXiv preprint arXiv:2309.07920 (2023)\n' +
      '* [9] Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., Mello, S.D., Gallo, O., Guibas, L., Tremblay, J., Khamis, S., Karras, T., Wetzstein, G.: Efficient Geometry Aware 3D Generative Adversarial Networks. In: CVPR (2022)\n' +
      '* [10] Chan, E.R., Nagano, K., Chan, M.A., Bergman, A.W., Park, J.J., Levy, A., Aittala, M., Mello, S.D., Karras, T., Wetzstein, G.: Generative novel view synthesis with 3D aware diffusion models. In: ICCV (2023)\n' +
      '* [11] Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873 (2023)\n' +
      '* [12] Chen, Z., Zhang, H.: Learning implicit fields for generative shape modeling. In: CVPR (2019)\n' +
      '* [13] Dario Pavllo, Jonas Kohler, T.H., Lucchi, A.: Learning generative models of textured 3D meshes from real-world images. In: ICCV (2021)\n' +
      '* [14] Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3d objects. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13142-13153 (2023)\n' +
      '* [15] Gadelha, M., Maji, S., Wang, R.: 3d shape induction from 2d views of multiple objects. In: 2017 International Conference on 3D Vision (3DV). pp. 402-411. IEEE (2017)\n' +
      '* [16] Gao, L., Yang, J., Wu, T., Yuan, Y., Fu, H., Lai, Y., Zhang, H.: SDM-Net: Deep generative network for structured deformable mesh. In: ACM TOG (2019)\n' +
      '* [17] Guedon, A., Lepetit, V.: Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering. arXiv preprint arXiv:2311.12775 (2023)\n' +
      '* [18] Guo, Y.C., Liu, Y.T., Shao, R., Laforte, C., Voleti, V., Luo, G., Chen, C.H., Zou, Z.X., Wang, C., Cao, Y.P., Zhang, S.H.: threestudio: A unified framework for 3d content generation. [https://github.com/threestudio-project/threestudio](https://github.com/threestudio-project/threestudio) (2023)* [19] Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400 (2023)\n' +
      '* [20] Huang, Y., Wang, J., Shi, Y., Qi, X., Zha, Z.J., Zhang, L.: Dreamtime: An improved optimization strategy for text-to-3d content creation. arXiv preprint arXiv:2306.12422 (2023)\n' +
      '* [21] Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463 (2023)\n' +
      '* [22] Katzir, O., Patashnik, O., Cohen-Or, D., Lischinski, D.: Noise-free score distillation. arXiv preprint arXiv:2310.17590 (2023)\n' +
      '* [23] Kazhdan, M., Bolitho, M., Hoppe, H.: Poisson surface reconstruction. In: Proceedings of the fourth Eurographics symposium on Geometry processing. vol. 7, p. 0 (2006)\n' +
      '* [24] Kerbl, B., Kopanas, G., Leimkuhler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (ToG) **42**(4), 1-14 (2023)\n' +
      '* [25] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)\n' +
      '* [26] Li, J., Tan, H., Zhang, K., Xu, Z., Luan, F., Xu, Y., Hong, Y., Sunkavalli, K., Shakhnarovich, G., Bi, S.: Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214 (2023)\n' +
      '* [27] Li, W., Chen, R., Chen, X., Tan, P.: Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. arXiv preprint arXiv:2310.02596 (2023)\n' +
      '* [28] Li, X., Dong, Y., Peers, P., Tong, X.: Synthesizing 3d shapes from silhouette image collections using multi-projection generative adversarial networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5535-5544 (2019)\n' +
      '* [29] Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 300-309 (2023)\n' +
      '* [30] Lin, S., Liu, B., Li, J., Yang, X.: Common diffusion noise schedules and sample steps are flawed. arXiv preprint arXiv:2305.08891 (2023)\n' +
      '* [31] Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9298-9309 (2023)\n' +
      '* [32] Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: Syncdreamer: Generating multiview-consistent images from a single-view image. arXiv preprint arXiv:2309.03453 (2023)\n' +
      '* [33] Liu, Z., Feng, Y., Black, M.J., Nowrouzezahrai, D., Paull, L., Liu, W.: Meshdiffusion: Score-based generative 3d mesh modeling. arXiv preprint arXiv:2303.08133 (2023)\n' +
      '* [34] Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H., Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008 (2023)\n' +
      '* [35] Luo, T., Rockwell, C., Lee, H., Johnson, J.: Scalable 3d captioning with pretrained models. arXiv preprint arXiv:2306.07279 (2023)\n' +
      '* [36] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy Networks: Learning 3D reconstruction in function space. In: CVPR (2019)\n' +
      '*[*[37] Metzer, G., Richardson, E., Patashnik, O., Giryes, R., Cohen-Or, D.: Latent-nerf for shape-guided generation of 3d shape and textureures. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12663-12673 (2023)\n' +
      '* [38] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM **65**(1), 99-106 (2021)\n' +
      '* [39] Mohammad Khalid, N., Xie, T., Belilovsky, E., Popa, T.: Clip-mesh: Generating textured meshes from text using pretrained image-text models. pp. 1-8 (2022)\n' +
      '* [40] Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751 (2022)\n' +
      '* [41] Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: DeepSDF: Learning continuous signed distance functions for shape representation. In: CVPR (2019)\n' +
      '* [42] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. In: ICLR (2023)\n' +
      '* [43] Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 12179-12188 (2021)\n' +
      '* [44] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [45] Rudin, L.I., Osher, S.: Total variation based image restoration with free local constraints. In: Proceedings of 1st international conference on image processing. vol. 1, pp. 31-35. IEEE (1994)\n' +
      '* [46] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.: Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems **35**, 25278-25294 (2022)\n' +
      '* [47] Schwarz, K., Sauer, A., Niemeyer, M., Liao, Y.,, Geiger, A.: VoxGRAF: Fast 3D-aware image synthesis with sparse voxel grids (2022)\n' +
      '* [48] Shen, T., Gao, J., Yin, K., Liu, M.Y., Fidler, S.: Deep marching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis. Advances in Neural Information Processing Systems **34**, 6087-6101 (2021)\n' +
      '* [49] Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.: Zero123++: a single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110 (2023)\n' +
      '* [50] Shi, Y., Wang, P., Ye, J., Long, M., Li, K., Yang, X.: Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512 (2023)\n' +
      '* [51] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020)\n' +
      '* [52] Sun, J., Zhang, B., Shao, R., Wang, L., Liu, W., Xie, Z., Liu, Y.: Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior. arXiv preprint arXiv:2310.16818 (2023)\n' +
      '* [53] Sun, Q., Li, Y., Liu, Z., Huang, X., Liu, F., Liu, X., Ouyang, W., Shao, J.: Unig3d: A unified 3d object generation dataset. arXiv preprint arXiv:2306.10730 (2023)\n' +
      '* [54] Tan, Q., Gao, L., Lai, Y., Xia, S.: Variational autoencoders for deforming 3D mesh models. In: CVPR (2018)\n' +
      '* [55] Zhang, Z., Liu, W., Liu, W., Xie, Z., Liu, Y.: Deep marching-based image synthesis with sparse voxel grids. arXiv preprint arXiv:2307.10681 (2023)* [55] Tang, J., Chen, Z., Chen, X., Wang, T., Zeng, G., Liu, Z.: Lgm: Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054 (2024)\n' +
      '* [56] Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)\n' +
      '* [57] Tang, J., Wang, T., Zhang, B., Zhang, T., Yi, R., Ma, L., Chen, D.: Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior. arXiv preprint arXiv:2303.14184 (2023)\n' +
      '* [58] Tang, S., Zhang, F., Chen, J., Wang, P., Furukawa, Y.: Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion (2023)\n' +
      '* [59] Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689 (2021)\n' +
      '* [60] Wang, P., Shi, Y.: Imagedream: Image-prompt multi-view diffusion for 3d generation. arXiv preprint arXiv:2312.02201 (2023)\n' +
      '* [61] Wang, P., Tan, H., Bi, S., Xu, Y., Luan, F., Sunkavalli, K., Wang, W., Xu, Z., Zhang, K.: Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024 (2023)\n' +
      '* [62] Wang, T., Zhang, B., Zhang, T., Gu, S., Bao, J., Baltrusaitis, T., Shen, J., Chen, D., Wen, F., Chen, Q., Guo, B.: Rodin: A generative model for sculpting 3D digital Avatars using diffusion. In: CVPR (2023)\n' +
      '* [63] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213 (2023)\n' +
      '* [64] Wu, J., Zhang, C., Xue, T., Freeman, B., Tenenbaum, J.: Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. Advances in neural information processing systems **29** (2016)\n' +
      '* [65] Xie, S., Tu, Z.: Holistically-nested edge detection. In: Proceedings of the IEEE international conference on computer vision. pp. 1395-1403 (2015)\n' +
      '* [66] Yi, T., Fang, J., Wu, G., Xie, L., Zhang, X., Liu, W., Tian, Q., Wang, X.: Gaussiandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. arXiv preprint arXiv:2310.08529 (2023)\n' +
      '* [67] Youwang, K., Ji-Yeon, K., Oh, T.H.: CLIP-Actor: text driven recommendation and stylization for animating human meshes. In: ECCV (2022)\n' +
      '* [68] Yu, X., Guo, Y.C., Li, Y., Liang, D., Zhang, S.H., Qi, X.: Text-to-3d with classifier score distillation. arXiv preprint arXiv:2310.19415 (2023)\n' +
      '* [69] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3836-3847 (2023)\n' +
      '* [70] Zhu, J., Zhuang, P., Koyejo, S.: Hifa: High-fidelity text-to-3d generation with advanced diffusion guidance. In: The Twelfth International Conference on Learning Representations (2023)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>