<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'OmniACT: 데스크톱과 웹을 위한 멀티모달 일반 자율 에이전트를 위한 데이터셋과 벤치마크\n' +
      '\n' +
      'Raghav Kapoor\\({}^{\\spadesuit}\\)\\({}\n' +
      '\n' +
      '마크. 가장 강력한 베이스라인인 GPT-4는 벤치마크에서 최고의 성능을 발휘하지만, 그 성능 수준은 여전히 작업을 완료할 수 있는 실행 스크립트를 생성하는 인간의 숙련도의 15%에 불과하여 기존 웹 에이전트에 대한 작업의 도전을 보여준다. 우리의 벤치마크는 컴퓨터 작업 자동화에 있어 언어 모델 에이전트의 진행 상황을 측정하고 평가할 수 있는 플랫폼을 제공하며 대규모 언어 모델을 연결하는 멀티모달 모델 구축과 컴퓨터 화면의 시각적 접지를 위한 향후 작업에 동기를 부여한다._\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '자연어 명령어를 기반으로 컴퓨터 작업을 수행하는 것은 인공 지능의 오랜 목표였다[45]. 연구 라인의 한 가지 구체적인 목적은 도미노의 "_"에서 피자를 주문하거나, 존에게 메시지를 작성한다._와 같이 인간이 컴퓨터 작업을 수행하는 데 도움을 줄 수 있는 일반 에이전트들을 개발하는 것이다._ 에이전트는 애플리케이션을 열고 작업을 수행할 수 있어야 한다. 개인용 컴퓨터에서 이러한 동작을 실행하는 것은 마우스 및 키보드와의 일련의 상호 작용을 포함한다. 예를 들어, 이메일을 쓰는 간단한 작업은 애플리케이션 아이콘 위에 호버링하는 것, 그것을 클릭하는 것, _\'새로운 이메일\'_ 버튼을 클릭하는 것, 이메일의 내용을 쓰는 것, 그리고 보내는 것을 클릭하는 것을 포함한다. 이메일을 성공적으로 전송하려면 각 단계에서 정확한 동작을 정확하게 예측하고 이를 정확하게 실행해야 하는데, 이는 오늘날 최고의 에이전트에 대해서도 예외적인 작업이다[13].\n' +
      '\n' +
      '컴퓨터 작업에 대한 일반 에이전트(generalist agent)는 자연어 명령어를 이해하고, 시각적 스크린샷을 처리하고, 의도된 작업을 달성하기 위해 수행될 정확한 동작 시퀀스를 생성해야 한다. 몇 가지 기존 접근법은 HTML 모델[9, 37, 57]에 기초한 빌딩 에이전트에 초점을 맞춘다. 그러나, 이 접근법은 몇 가지 도전과 제약을 도입한다. 이러한 에이전트는 웹 애플리케이션으로 제한되며 종종 복잡하거나 긴 컨텍스트 HTML 코드로 어려움을 겪는다. 기본 데스크톱 응용 프로그램과 상호 작용하거나 코드 편집기의 텍스트를 사용하여 전자 메일을 작성하는 것과 같이 여러 응용 프로그램에 걸쳐 있는 작업을 크게 변경하지 않고 수행할 수 없습니다. 또한 텍스트 전용 언어 모델에 의해 본질적으로 구동되는 HTML 기반 에이전트는 데스크탑의 오른쪽 상단 모서리에 있는 파란색 버튼을 식별하고 클릭하는 것과 같은 시각적 단서가 필요한 작업에서 일반적으로 성능이 떨어진다. 대조적으로, 인간은 드롭다운 메뉴, 타이핑 가능한 영역, 방향 변경 및 옵션과 같은 UI 요소를 한눈에 쉽게 이해할 수 있다.\n' +
      '\n' +
      '강력한 시각적 및 사용자 인터페이스(UI) 이해 능력을 가진 일반 자율 에이전트를 개발하는 목표를 향해, 우리는 다양한 운영 체제와 웹에 걸쳐 9.8K 쌍 이상의 이미지 및 명령어(그림 1)를 포함하는 새로운 작업 및 데이터 세트를 소개한다. 이 데이터세트에는 다양한 UI 화면의 스크린샷과 해당 자연어 지시가 포함된다. 이러한 명령어들의 목적은 _PyAutoGUI_ Python 라이브러리 [1]을 이용하여 실행가능한 명령어들을 생성하는 것이다. _ PyAutoGUI_는 마우스 및 키보드 동작의 자동화를 가능하게 하며, 이는 macOS, Windows 및 Linux에 걸쳐 다양한 네이티브 애플리케이션과의 상호 작용을 용이하게 하는데 도움을 준다. 이렇게 하면 다른 웹 도메인 및 기본 데스크톱 응용프로그램에 걸쳐 지정된 작업을 완료할 수 있습니다.\n' +
      '\n' +
      '이 데이터셋에서 LLaMA[43], Vicuna[7], Palmyra-X[43B][2], InstructPalmyra-30B[41], GPT 3.5, GPT-4[30] 등 여러 언어 모델 기반 에이전트 기준선을 평가한다. QLoRA[10]을 이용하여 Vicuna-13B 모델과 LLaMA-13B 모델을 미세조정하여 실험하였다. 또한 작업에 대한 멀티모달 기준선 LLaVa-v1.5-7B, LLaVa-v1.5-13B[43] 및 GPT-4-비전-프리뷰[50]를 벤치마킹한다. 우리의 연구 결과는 이러한 작업을 실행할 수 있는 멀티모달 모델의 필요성을 강조하고, 우리의 분석은 공간에서 유망한 미래 작업에 대한 통찰력을 제공한다. 우리의 주요 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '1. 인간 주석을 통해 수집된 9.8K 이상의 자연어 작업, UI 화면 및 해당 코드 스니펫으로 구성된 데스크탑 및 웹사이트 애플리케이션의 새로운 데이터 세트를 출시한다. 컴퓨터 작업에 맞는 맞춤형 성능 메트릭을 소개합니다.\n' +
      '2. OCR, 색상, 아이콘-템플릿 매칭의 신호를 이용하여 화면의 텍스트 표현을 생성하는 모듈인 DetACT를 제안한다.\n' +
      '3. 종합 벤치마크를 수행하고, 벤치마크에 대한 최신 LLMs 및 멀티모달 모델에 대한 분석을 수행한다. 우리의 결과는 오늘날 최고의 LLM 에이전트에게도 도전적인 과제이며 기존 모델은 인간의 성능에 훨씬 못 미친다는 것을 보여준다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### UI Understanding\n' +
      '\n' +
      '사용자 인터페이스(User Interface, UI)의 이해는 모바일 및 웹 사용자 인터페이스의 의미를 이해하는 데 초점을 맞춘 다양한 모델로 진화하면서 기계 학습 및 인간-컴퓨터 상호작용 커뮤니티에 대한 연구자들의 관심을 얻고 있다. UIBert[3], PixelBERT[15], ActionBert[14], VUT[23], Screen2Words[44], Widget-Captioning[22] 및 Pix2Act[36]은 이 분야에서 주목할 만한 모델이다. 이들은 이미지 및 뷰 계층을 이용하여 모바일 화면의 사용자-인터페이스 의미를 학습하기 위한 접근 방법을 제안한다. 이러한 모델은 성능 예측, 화면 분할 및 이해, 화면 캡션 생성과 같은 작업에서 효율성을 입증했다. Lexi[4] 및 Spotlight[20]은 뷰 계층 구조와 같은 메타데이터에 대한 의존도를 최소화하기 위해 비전 전용 입력을 사용하는 모델을 제안한다. Furata et al. [11]은 멀티모달 웹 네비게이션을 위한 미세조정의 사용을 보여준다. UI 이해를 위해 훈련된 대부분의 기계 학습 모델은 64,462개의 고유한 안드로이드 화면과 메타 데이터를 포함하는 리코 데이터세트[8]와 그 확장을 활용한다. 또한, Banerjee et al. [4]는 광범위한 응용 분야에 걸쳐 다양한 이미지-캡션 쌍으로 구성된 UICaptions 데이터셋을 공개했다. 픽셀헬프[21]는 또한 자연어 명령어를 해석하고 모바일 사용자 인터페이스 동작에 매핑할 수 있는 모델을 훈련시키기 위해 코퍼스를 출시했다.\n' +
      '\n' +
      '자율 컴퓨터 에이전트\n' +
      '\n' +
      '대규모 언어 모델(LLM)의 출현은 웹 페이지에서 작동하는 에이전트의 급속한 발전에 중추적이었다. ViperGPT[40] Chameleon[26], RCI Agent[17], VisProg[12], [28]과 같은 최근 연구는 자율 에이전트 개발에 있어 계획 또는 행동 예측을 위해 LLM을 사용한다. MiniWoB[37], WebShop[51], Macaw-LLM[27], ASHPrompting[38] Mind2Web[9] 및 WebArena[57]과 같은 벤치마크 데이터 세트도 웹 작업을 자동화하는 LLM 기반 에이전트의 능력을 측정하기 위해 제안되었다. 이러한 방법들은 주로 HTML 스크립트의 텍스트 기반 문서 객체 모델(DOM: Document Object Model) 상에서 동작하는 에이전트들을 포함한다. 이는 모델의 의사 결정 및 행동 수행 프로세스에 중요한 화면 컨텍스트에 대한 이해를 제한한다. 이러한 제한을 해결하기 위해, Rawles et al. [32]는 스크린들, 자연 언어 명령들, 및 대응하는 액션들을 포함하는 데이터세트인, Wild에서 Android를 출시하였다. 이후 [54]에서는 안드로이드 생태계에 국한된 와일드 데이터셋에서 안드로이드에 에이전트를 구축하도록 설계된 멀티모달 모델 AutoUI를 제안하였다.\n' +
      '\n' +
      '자율 에이전트에 대한 현재 벤치마크는 주로 웹 또는 안드로이드 환경에 초점을 맞추며 데스크톱 애플리케이션과 관련된 작업이나 웹 도메인 너머의 여러 애플리케이션에 걸쳐 있는 작업에 대한 문제를 제기한다. 사용자 인터페이스(UI) 요소를 추출하기 위한 기본 방법과 결합된 이 영역에 확립된 벤치마크 및 데이터 세트가 없다는 것은 현재 범위를 넘어 다양한 작업을 처리할 수 있는 보다 다재다능한 자율 에이전트를 개발하는 데 상당한 진전이 필요함을 강조한다. OmniACT가 유능한 자율 에이전트의 평가에서 도입하는 고유한 기능을 강조하기 위해 표 1에서 기존 벤치마크와 제안된 벤치마크인 OmniACT를 비교한다.\n' +
      '\n' +
      '## 3 OmniACT\n' +
      '\n' +
      '우리는 웹과 데스크톱 애플리케이션에서 자율 에이전트의 성능을 측정하는 새로운 데이터 세트와 벤치마크인 OmniACT를 소개한다. 텍스트 기반 추론[9, 16, 37, 51, 57]에 초점을 맞춘 이전의 벤치마크와 비교하여, 우리의 벤치마크는 대형 언어 모델 설계자와 UI 이해 비전 모델을 연결하는 멀티모달 에이전트를 측정하는 것을 목표로 한다. 옴니ACT는 모의 환경 아래 있지 않기 때문에 독립형 작업으로 수행할 수 있다.\n' +
      '\n' +
      '인간이 컴퓨터에서 실행할 수 있는 모든 동작은 _PyAutoGUI_[1] Python 프레임워크에서 인코딩될 수 있다. 이 프레임워크를 사용하면 사용자가 파이썬 코드를 실행하여 키보드 및 마우스 작업을 실행할 수 있습니다. 이러한 태스크들을 실행하기 위한 _PyAutoGUI_ 코드는 도 1의 세 번째 열에 도시되어 있다. 다른 컴퓨터 태스크들에 대해, _PyAutoGUI_ 라이브러리는 태스크를 실행하기 위해 사용될 수 있는 \'누르기\', \'쓰기\', \'스크롤\'과 같은 기능들을 제공한다. 데이터 세트는 성공적인 실행을 달성하는 자연어 작업, UI 스크린샷 및 그라운드 truth _PyAutoGUI_ 스크립트의 병렬 데이터로 구성된다.\n' +
      '\n' +
      '### Task Formulation\n' +
      '\n' +
      '스크린(S\\)과 태스크 기술(T\\)이 자연어로 정의된 컴퓨터의 입력 상태를 고려할 때, 태스크의 목표는 스크린샷(S\\in\\text{Linux, Windows, MacOS, Webpage}\\}\\) 내에서 태스크를 성공적으로 수행할 수 있는 일련의 동작(A\\)을 출력하는 것이다. 형식적으로 태스크는 전이 함수 \\(f:T\\times S\\to A\\)를 학습하는 것으로 정의할 수 있다. 데이터셋을 수집하는 동안, 우리는 모든 태스크 설명\\(T\\)이 실현 가능하고 현재 스크린샷\\(S\\)에서 달성될 수 있음을 보장한다. 모호성을 줄이고 더 나은 평가를 용이하게 하기 위해, 우리는 작업 설명이 상세하고 명확하다는 것을 보장한다. 작업들은 또한 시각적으로 접지될 수 있다(예를 들어, \'녹화를 시작하기 위해 빨간색 버튼을 클릭하세요\') 또는 자연 언어 기반(예를 들어, \'내 계정 버튼을 클릭하세요\')이다. 우리는 _PyAutoGUI_ 라이브러리의 기능들을 이용하여 액션 공간을 정의한다:\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline\n' +
      '**Datasets** & **Size** & **Env Type** & \\begin{tabular}{c} **Task** \\\\ **Heterogeneity** \\\\ \\end{tabular} & \\begin{tabular}{c} **Real-World** \\\\ **Portayal** \\\\ \\end{tabular} & \\begin{tabular}{c} **Executional** \\\\ **Correctness** \\\\ **Apps** \\\\ \\end{tabular} & \\begin{tabular}{c} **Supports** \\\\ **Desktop** \\\\ **Apps** \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} **Continuous Scale** \\\\ **Adaptive** \\\\ **Evaluation** \\\\ \\end{tabular} & **Task** \\\\ \\hline WebArena [57] & 812 & Web & Yes & Yes & No & No & Web Navigation \\\\ Mind2Web [9] & 2350 & Web & Yes & Yes & No & No & Web Navigation \\\\ WebShop [51] & 12000 Products & Web & No & No & Yes & No & Web Navigation \\\\ WebS [49] & 80 & Web & Yes & Yes & No & No & Web Navigation \\\\ WebSRC [6] & 2735 & Web & Yes & Yes & - & No & QA \\\\ \\hline MiniWoB+ [16] & 100 & Mobile & No & No & Yes & No & Web Navigation \\\\ PixelHelp [21] & 187 & Mobile & Yes & Yes & No & No & UI Grounding \\\\ MetaGUI [39] & 1125 & Mobile & Yes & Yes & Yes & No & No & Mobile Navigation \\\\ MoTIF [5] & 756 & Mobile & Yes & Yes & Yes & No & No & Mobile Navigation \\\\ ATIV [32] & 715142 & Mobile and Web & Yes & Yes & Yes & No & No & Mobile/Web \\\\ \\hline \\hline\n' +
      '**OmniACT** (Ours) & **9802** & **Desktop and Web** & Yes & Yes & Yes & Yes & Yes & **Code Generation** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: OmniACT와 다른 관련 벤치마크의 비교.\n' +
      '\n' +
      '(A\\in\\){\'click\', \'dragTo\',\'scroll\', \'write\',...}. 액션들의 전체 리스트는 표 2에 제공된다. 우리의 액션 공간은 두 개 또는 세 개의 상호작용 옵션들에 의존하는 다른 벤치마크들 [9, 37, 57]보다 훨씬 크다. \'oveTo\', \'click\', \'rightClick\', \'doubleClick\', \'dragTo\'와 같은 마우스 액션은 액션의 픽셀 위치를 나타내는 인수로 화면 좌표를 추가로 필요로 한다.\n' +
      '\n' +
      '도 1은 \\(\\mathsf{OmnIACT}\\): (1) 주식(MacOS), (2) 아파트닷컴(웹 페이지), (3) 날씨(MacOS) 내의 세 가지 애플리케이션에 대한 샘플 작업 및 대응하는 출력을 도시한다. 첫 번째 열은 입력 영상을 묘사하고, 두 번째 열은 현재 화면에서 실행될 자연어 작업을 보여준다. 이러한 작업을 실행하려면 사용자는 마우스와 키보드를 사용하여 일련의 작업을 정확하게 수행해야 한다. 예를 들어, 구글의 최근 한 달 간 주가 변동률을 확인하기 위해서는 마우스를 최근 한 달로 이동시키고, 왼쪽 클릭 버튼을 현재 한 달로 유지한 채 드래그해야 한다.\n' +
      '\n' +
      '### Dataset Preparation\n' +
      '\n' +
      '데이터 세트를 준비하기 위해 그림 2에 요약된 파이프라인 접근법을 따랐으며 먼저 다양한 응용 프로그램과 웹사이트를 선택했다. 각 애플리케이션 또는 웹사이트에 대해 주요 UI 요소 주위에 경계 상자를 만들고 기능에 따라 레이블을 지정했으며, 이는 인간 주석자가 정확한 _PyAutoGUI_ 스크립트를 작성하는 데 중요한 역할을 한다. 각 스크립트가 작성된 후 레이블을 다시 숫자 좌표로 변환하여 스크립트를 UI 요소의 위치에 정확하게 정렬할 수 있습니다. 마지막으로 각 스크립트의 실행 가능성과 구문 표준을 준수하는 데 중점을 두고 각 스크립트를 철저히 검토했다. 이를 통해 데이터 세트의 고품질과 기능이 보장되어 자율 에이전트를 훈련하고 평가하는 데 귀중한 자원이 되었다.\n' +
      '\n' +
      '######3.2.1 애플리케이션/웹 사이트 선택\n' +
      '\n' +
      '다양한 작업에 걸쳐 컴퓨터 에이전트의 일반화 능력을 테스트하기 위해 데스크톱 및 웹 애플리케이션 모두에서 여러 도메인에 걸쳐 작업을 수집한다. 전체 9802개의 데이터 포인트(표 3)를 수집하여 주석을 달았는데, HTML 기반 웹 페이지와 달리 문서 객체 모델(DOM) 계층을 포함하지 않는 데스크톱 애플리케이션에 대한 강조는 시각적 단서가 중요한 더 복잡한 멀티모달 챌린지를 제시한다. 우리는 가장 인기 있는 세 가지 운영 체제 내의 애플리케이션에서 작업을 수집합니다. 우리는 MacOS에서 22개의 네이티브 애플리케이션을 선택하고 리눅스와 윈도우에서 각각 8개를 선택한다. 우리는 모든 응용 프로그램에 대해 대략 3~4개의 화면에 주석을 달아요. 전체 응용프로그램 목록은 부록에 나와 있습니다.\n' +
      '\n' +
      '오늘날 많은 일반적인 컴퓨터 작업이 여전히 수행됨\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Type** & **Action** & **\\%** \\\\ \\hline \\multirow{8}{*}{**Mouse**} & Click & 63.73 \\\\  & Double Click & 0.58 \\\\  & Right Click & 0.77 \\\\  & Move/Hover & 1.85 \\\\  & Drag & 0.29 \\\\  & Scroll & 1.68 \\\\  & Horizontal Scroll & 0.17 \\\\ \\hline \\multirow{2}{*}{**Keyboard**} & Press & 16.28 \\\\  & Hotkey & 3.00 \\\\  & Write & 11.65 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: \\(\\mathsf{OmnIACT}\\)에 의해 지원되는 액션 타입 및 데이터세트 내의 액션들 각각에 대한 인스턴스들의 수.\n' +
      '\n' +
      '도 2: **Data Collection Pipeline.** (1) 다양성을 보장하기 위해 60개 이상의 애플리케이션 및 웹사이트를 선택하고, (2) 인간 주석이 달린 바운딩 박스를 통해 화면을 분할하고, (3) 기능에 기초하여 바운딩 박스를 라벨링하고, (4) 학생 자원자들에게 태스크를 작성하도록 요청하고, 스크린 이미지가 주어지고, (5) 실행 및 구문에 기초하여 스크립트를 좌표화하고 필터링하기 위해 텍스트 라벨을 역맵핑한다.\n' +
      '\n' +
      '우리는 또한 27개의 다른 웹 애플리케이션에서 3-4개의 스크린샷을 수집합니다. 업무의도의 다양성을 보장하기 위해 다음과 같은 6가지 범주 중 하나로 분류한다. (1) 쇼핑, (2) 엔터테인먼트, (3) 서비스, (4) 정부, (5) 여행, (6) 건강 [9]의 방법론에 영감을 받아 이러한 범주는 광범위한 사용자 의도와 기능을 포함하도록 선택되었다.\n' +
      '\n' +
      '###### 3.2.2 UI 화면 분할\n' +
      '\n' +
      '골드 표준 데이터를 수집하기 위해 먼저 화면에 존재하는 경계 상자를 식별하여 화면에 주석을 달고 분할한다. 우리는 경계 박스를 생성하기 위해 웹 및 데스크톱 애플리케이션을 위해 약간 다른 기술을 사용한다:\n' +
      '\n' +
      '1. **데스크톱 애플리케이션:** PyQt51을 기반으로 사용자 지정 주석 인터페이스를 구축하여 간단한 드래그 앤 클릭 메커니즘을 사용하여 스크린 이미지를 통해 수동으로 경계 상자를 생성한다. 이 사용자 지정 인터페이스는 프로세스를 촉진하고 데스크탑 이미지에 대한 매우 정확한 골드 라벨 데이터 포인트를 얻을 수 있습니다. 각주 1: [https://pypi.org/project/PyQt5/](https://pypi.org/project/PyQt5/)\n' +
      '2. **웹사이트:** 웹페이지의 경우 자바스크립트 코드를 작성하여 HTML 소스 코드에서 상호 작용 가능한 모든 영역(클릭, 호버, 타입 등)을 추출한다. 또한 화면에서 배너, 드롭다운, 제출 및 라디오 버튼을 추출합니다. 우리는 화면 내에서 볼 수 있고 상호 작용할 수 있는 요소만 유지하기 위해 요소를 필터링한다.\n' +
      '\n' +
      '######3.2.3 기능성 태깅\n' +
      '\n' +
      '각 경계 상자를 올바른 기능 설명에 매핑하기 위해 경계 상자가 있는 이미지가 주어지고 경계 상자 함수의 올바른 설명 또는 레이블을 작성해야 하는 아마존 MTurk 작업자를 활용합니다. 예를 들어, _search bar_가 있는 아마존 웹페이지의 이미지가 주어지면, 주석자는 이를 _"find-product-search-bar"_로 레이블링한다. 논리 설명들은 개별 바운딩 박스 좌표들을 식별할 필요 없이 구조화된 방식으로 태스크들을 생성하기 위해 사용된다.\n' +
      '\n' +
      '###### 3.2.4 과제 생성\n' +
      '\n' +
      '각 화면에 대한 우리의 접근법은 단일 화면의 범위 내에서 실행될 수 있는 작업을 생성하기 위해 모든 인간 주석 경계 상자와 레이블을 활용하는 것을 포함한다. 이러한 작업은 멀티모달 에이전트의 기능을 측정하기 위해 시각적으로 기초하도록 설계되었다. 우리는 평가 목적으로 경계 상자와 해당 레이블을 메타데이터로 공개할 계획이다.\n' +
      '\n' +
      '데이터세트 컴파일은 기본 파이썬 프로그래밍 기술을 가진 대학생들이 주석자 역할을 하며 _PyAutoGUI_에 대한 API 참조와 잠재적인 작업의 예에 액세스한다. 각 학생은 세 가지 대체 자연 언어 재구성을 동반한 여러 과제를 생성했다. 예를 들어, _"3+2?"_는 _"2와 3"_의 합을 계산하거나 _"2와 3"_의 합으로 재구성될 수 있다. 열차 테스트 누출을 피하기 위해 재구정된 작업을 동일한 데이터 세트 분할에 일관되게 배치했다. 주석 프로세스에 대한 자세한 내용은 부록에서 확인할 수 있습니다.\n' +
      '\n' +
      '###### 3.2.5 역 매핑 및 필터링\n' +
      '\n' +
      '고품질 데이터를 보장하기 위해 데이터 수집 파이프라인에 추가 단계를 통합합니다. 각 경계 상자의 텍스트 기반 레이블을 다시 숫자 좌표로 매핑하기 위해 스크립트를 구축한 다음 구문을 일치시키고 태스크가 화면에서 실행되는지 확인합니다. 이 필터를 사용하여 비작업 또는 구문적으로 잘못된 모든 데이터 포인트를 제거하고 마지막으로 작업 세트를 수동으로 검토한다.\n' +
      '\n' +
      '필터링 후 200개 이상의 데스크톱 및 웹 화면에 걸쳐 9802개의 인간 주석, 골드 라벨 데이터 포인트를 얻고(표 3), 트레인, 검증 및 테스트 세트로 7:1:2 비율로 분할한다. 수집된 모든 데이터는 멀티모달 에이전트에 대한 향후 작업을 장려하기 위해 공개적으로 공개될 것이다.\n' +
      '\n' +
      '##4 평가 메트릭스\n' +
      '\n' +
      '이 섹션에서는 OmniACT 데이터 세트에 대한 모델 성능을 벤치마킹하기 위한 다양한 평가 메트릭을 자세히 설명한다. UI 화면들은 BLEU[31], CodeBLEU[33], BERTScore[53] 및 CodeBERTScore[56]와 같은 대부분의 종래의 유사성 기반 메트릭들에서 팩터화되지 않은 공간 관련성과 같은 추가적인 제약들을 갖는다. 예를 들어, 유효한 클릭 동작은 일반적으로 단일 좌표로 구속되지 않지만 지정된 영역 내의 임의의 좌표일 수 있다. 유효하지 않은 좌표 예측의 경우, 유효 영역으로부터 더 멀리 떨어진 좌표를 예측하는 에이전트는 영역에 가까운 좌표를 예측한 에이전트에 비해 더 높은 페널티를 호출해야 한다. 우리는 UI 정보를 활용하는 것을 목표로 하는 시퀀스 점수(섹션 4.1)와 액션 점수(섹션 4.2)의 두 가지 새로운 메트릭을 제안한다.\n' +
      '\n' +
      '### Sequence Score\n' +
      '\n' +
      '시퀀스 스코어는 예측된 액션 시퀀스(예를 들어, \'클릭\', \'쓰기\', \'누름\')가 금 시퀀스와 정확하게 일치하는지 여부를 측정한다. 시퀀스에서 첫 번째 동작을 예측하는 것은 비교적 간단하고 나중의 동작이 더 어렵기 때문에, 우리는 시퀀스 스코어를 다음과 같이 정의한다:\n' +
      '\n' +
      '[SeqScore_{i}=\\begin{cases}\\beta_{1}+\\beta_{2}*(s-1)&\\text{if all actions match}\\\\0&\\text{otherwise}\\end{cases}\\text{if all actions match}\\\\0&\\text{otherwise}\\end{cases}\\begin{cases}\\beta_{1}+\\beta_{2}*(s-1)&\\text{if all actions match}\\\\0&\\text{otherwise}\\end{cases}\\begin{cases}\\beta_{2}*(s-1)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline  & **Domain** & **Train** & **Validation** & **Test** & **Total** \\\\ \\hline \\hline  & Mac OS & 3028 & 444 & 786 & 4258 \\\\ Desktop & Linux & 761 & 126 & 247 & 1134 \\\\  & Windows & 1573 & 216 & 458 & 2247 \\\\ Web & - & **1427** & **206** & **530** & **2163** \\\\ \\hline\n' +
      '**Total** & & 6789 & 992 & 2,021 & 9802 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 분할 및 플랫폼에 걸친 데이터 세트 분포.\n' +
      '\n' +
      '여기서 \\(s\\)은 작용 시퀀스 길이이고, \\(\\beta_{1}\\)은 0.1로 설정되고 \\(\\beta_{2}\\)은 1로 설정된다.\n' +
      '\n' +
      '### Action Score\n' +
      '\n' +
      '액션 점수는 올바른 액션 시퀀스를 포함하는 코드 스니펫이 작업을 얼마나 잘 수행할 수 있는지를 측정한다. 특히, 정확한 동작 시퀀스를 갖는 스크립트에 대해, 부정확한 동작에 대한 벌점을 도입한다. 상기 패널티들은 아래에 기술된다:\n' +
      '\n' +
      '1. **Click penalty (\\(M\\))**: 동작 \'click\', \'rightClick\', \'doubleClick\',\'moveTo\', 및 \'dragTo\'에 대해, 예측된 좌표가 UI 엘리먼트의 바운딩 박스 외부에 있는 코드 스니펫을 벌점한다. \\(i^{th}\\) 예제의 \\(j^{th}\\) 작용에 대한 클릭 페널티는 다음과 같이 정의된다. \\[M^{j}_{i}=\\alpha_{i}\\times\\begin{cases}\\dfrac{\\mu}L_{2}&\\text{if}SeqScore_{i}>0\\\\0&\\text{otherwise}\\end{cases}\\. 여기서 \\(L_{2}\\)은 예측된 좌표와 바운딩 박스 사이의 최소 유클리드 거리에 해당한다. (L_{2}\\)는 예측된 좌표가 목표 바운딩 박스 내에 있을 때 0이다. \\ (\\mu\\)는 경계 박스의 대각선의 길이로 동적으로 설정한 디리클레 평활 계수이다. 이는 바운딩 박스 외부의 포인트들에 대한 페널티가 바운딩 박스의 크기에 따라 변하는 것을 보장한다. 동일한 \\(L_{2}\\)을 갖는 두 개의 예측 포인트에 대해, 상자가 더 크면 메트릭은 더 큰 벌점을 부과한다. 이것은 더 큰 상자를 클릭할 확률이 더 높고 실수가 있을 경우 벌칙을 더 많이 받아야 한다는 직감으로 소리가 난다.\n' +
      '2. **키 패널티((\\(K\\))**: 동작 \'누름\' 및 \'핫키\'에 대해, 타겟 코드( \\(GK^{j}_{i}\\)로 표현됨) 내의 키 집합과 예측 코드( \\(PK^{j}_{i}\\)로 표현됨)가 동일한지 확인한다. 형식적으로 \\[K^{j}_{i}\\textbf{ = }\\alpha_{i}\\times\\begin{cases}1&\\text{if }GK^{j}_{i}=PK^{j}_{i}\\text{ 및 }SeqScore_{i}>0\\\\0&\\text{otherwise}\\end{cases}\\text{if }GK^{j}_{i}=PK^{j}_{i}\\text{ 및 }SeqScore_{i}>0\\\\0&\\text{otherwise}\\end{cases}\\text{\n' +
      '3. **쓰기 벌점((\\(W_{p}\\))**: 액션 타입 \'쓰기\'에 대해, 타이핑될 문장에 대한 출력을 벌점화한다. 구체적으로, BLEU 점수[31]를 사용하고, 계산: \\[W^{j}_{i}\\textbf{ = }\\alpha_{i}\\times\\begin{cases}1-BLEU(GS^{j}_{i},PS^{j}_{i})& \\text{if}SeqScore_{i}>1\\\\0&\\text{otherwise}\\end{cases}\\]을 사용한다. 여기서, \\(GS^{j}_{i}\\)는 실제 타이핑될 문장을 나타내고, \\(PS^{j}_{i}\\)는 예 \\(i\\)의 \\(j^{th}\\) 액션에서 모델에 의해 예측된 문장을 나타낸다.\n' +
      '\n' +
      '상기 수학식에서, (\\(\\alpha_{i}\\))는 가중 인자:\n' +
      '\n' +
      '\\[\\alpha_{i}=SeqScore_{i}/\\text{length of sequence }i\\]\n' +
      '\n' +
      '이것은 액션 스코어 \\(\\in[0,1]\\)을 보장한다. 평균 액션 스코어는 다음과 같이 계산된다:\n' +
      '\n' +
      '**Action Score =**\n' +
      '\n' +
      '\\[\\dfrac{\\sum_{i}max\\left(SeqScore_{i}-\\sum_{j}(M^{j}_{i}+K^{j}_{i}+W^{j}_{i}), 0\\right)}{\\sum_{i}SeqScore_{i}}\\]\n' +
      '\n' +
      '우리는 섹션 7의 모든 기준 모델에 대한 이러한 메트릭을 보고하고 논의한다.\n' +
      '\n' +
      '##5 DetACT: UI로부터의 액션을 검출하는 단계\n' +
      '\n' +
      'UI 화면을 이해하는 것은 멀티모달 컴퓨터 작업에 매우 중요합니다. 웹 기반 에이전트는 일반적으로 HTML DOM으로부터의 언어 전용 입력을 사용한다. 이는 많은 컴포넌트들이 HTML 코드로 쉽게 기술되지 않을 수 있기 때문에, 애플리케이션 UI의 전체 범위를 이해하는데 불충분하다. 이를 해결하기 위해, 우리는 DetACT를 제안하며, 이는 우리가\n' +
      '\n' +
      '그림 3: **DetACT Module.** 초기 이미지와 자연어 작업 설명이 주어지면 파이프라인 방식을 사용하여 OCR과 SAM을 화면에서 실행합니다. SAM으로부터의 출력들은 유용한 UI 엘리먼트들의 완전한 세트를 획득하기 위해 아이콘 및 컬러-매칭 모듈들에 의해 사용된다. 요소 목록은 주어진 작업과 관련된 요소만 선택하기 위해 LLM 기반 필터를 통과한다.\n' +
      '\n' +
      'UI 레이아웃의 이미지를 다운스트림 LLM에 대한 구조화된 코드 및 텍스트 출력으로 변환합니다. DetACT는 텍스트 모듈, 아이콘 모듈 및 컬러 모듈의 세 가지 별개의 모듈로 구성된 시스템이다.\n' +
      '\n' +
      '1. **Text Extraction:**EasyOCR model2를 사용하여 UI 화면을 파싱하고 모든 텍스트 기반 요소를 수집한다. 텍스트와 함께 이러한 각 요소의 위치도 기록합니다. 이것은 OCR 모듈을 사용하여 화면에서 발견된 텍스트 요소 목록과 함께 그림 3에 나와 있다. 우리는 Segment Anything Model (SAM) [18]을 사용하여 스크린샷 내에서 서로 다른 영역을 분할하고 분류한다. 출력에서 아이콘과 색상 검출을 위해 텍스트가 아닌 세그먼트를 필터링합니다. 각주 2: [https://github.com/JaidedAI/EasyOCR](https://github.com/JaidedAI/EasyOCR)\n' +
      '2. **아이콘 모듈:** 적절한 아이콘과의 매칭을 위해 1600개의 아이콘3의 팩을 템플릿으로 사용한다. 이들 아이콘들 각각은 그들의 적절한 기능으로 라벨링되고 필터링된 출력들 SAM[18]과 매칭된다. 두 이미지의 유사성을 위해 참조 아이콘과 분할된 관심 영역(ROI)의 크기를 동일한 크기로 조정하고 두 이미지를 그레이스케일로 변환한다. 이 후, SSIM(Structural Similarity Index)[48]을 사용하여 ROI의 가장 가까운 일치를 우리 집합 내의 아이콘들과 찾고, SSIM 임계치 0.95를 초과하는 것을 선택한다. 도 3에서 보는 바와 같이, 화면에 매칭되는 몇 개의 아이콘은 _Globe_ 아이콘, _Calendar_ 아이콘, _Person_ 아이콘, _Location_ 아이콘이며, 각각은 다른 사용 경우를 묘사한다. 각주 3: [https://icomoon.io/](https://icomoon.io/]\n' +
      '3. **컬러 모듈:** 마지막으로 관심 있는 모든 세그먼트를 적절한 색상의 버킷에 배치하기 위해 ROI에 걸쳐 RGB 픽셀 값을 평균화하고 그 값을 기반으로 다른 색상 범주로 버킷화한다. 우리는 각 색상의 범위에 대한 인간의 관점에 따라 색상을 다르게 분류한다. 모호함을 피하기 위해 노란색, 파란색, 녹색, 빨간색, 분홍색, 보라색, 흰색, 검은색, 주황색, 갈색, 회색 등 11가지 주요 색상을 고려한다. 색상과 함께 요소의 중심을 기록합니다.\n' +
      '\n' +
      '각 카테고리의 모든 요소가 좌표로 추출되면 GPT-4를 프롬프트하여 이러한 UI 요소를 필터링한다[30]. 선택한 요소가 작업에만 적합하도록 보장하며, 이를 위해 요소 목록과 함께 프롬프트에 작업 설명을 제공합니다. 프롬프트에 대한 자세한 내용은 용지의 부록 섹션에 나와 있습니다. 도 3에서 관찰하듯이, 익스피디아 애플리케이션으로부터의 이미지와 태스크(_"검은 위치 아이콘을 클릭하고 파리로서 목적지를 입력한다._)가 주어지면, LLM은 스크린으로부터 _"Go To"_, _"Location Icon"_, 및 _Black_ 컬러 엘리먼트들만을 유지하기 위해 엘리먼트들을 필터링한다. 이것은 LLM 또는 비전 언어 모델 백본에 입력으로 전달된다.\n' +
      '\n' +
      '## 6 Baselines\n' +
      '\n' +
      'OmniACT에서 기존 언어 모델 기반 에이전트의 성능을 평가하기 위해 언어 기반 및 멀티모달 기준선을 사용하여 실험을 수행한다. DetACT 모듈은 작업의 이미지 및 텍스트 설명을 취하고 색상, 아이콘 및 텍스트 기반 신호를 출력한다. 이것은 LLM 프롬프트 기반 기준선에 대한 프롬프트에 연결된다(도 4 참조). 모든 프롬프트는 역할 할당[55]으로 시작하고, 그 기능에 대한 텍스트 설명과 함께 _PyAutoGUI_ 기능 세트의 상세한 API 참조가 뒤따른다. 그런 다음 작업과 가장 밀접하게 일치하는 훈련 세트에서 5개의 컨텍스트 내 예제를 추가한다(참조 작업과 훈련 예제의 MiniLM[46] 임베딩의 코사인 유사성을 기반으로). 디택트 모듈에서 필터링한 UI 요소 목록을 프롬프트에 추가합니다. 마지막으로 작업 설명과 함께 규칙을 제공합니다. 멀티모달 기준선의 경우 이미지 픽셀을 비전 인코더로 전달하기도 합니다. 우리는 몇 가지 기준선의 결과를 보고한다:\n' +
      '\n' +
      '* [leftmargin=*,noitemsep,topsep=0pt]\n' +
      '* **Few-shot Generative LLM:** LLaMA-2[43], Vicuna-1.5[7], CodeLLaMA-34B[34], Palmyra[42] 및 GPT[30] 시리즈의 모델로 실험한다. 우리는 모형을 프롬프트하기 위해 그림 4와 같은 프롬프트 구조를 사용한다. LLaMA 및 CodeLLaMa의 경우 더 긴 프롬프트에서 성능이 좋지 않은 것을 관찰했기 때문에 DetACT 모듈에서 출력을 더 낮은 신뢰도로 제거하여 프롬프트 길이를 2000 토큰으로 줄인다. 다른 모델의 경우 최대 4000개의 토큰 크기로 프롬프트를 허용합니다.\n' +
      '**Finetuned Generative LLM:** We fine-tuned\n' +
      '\n' +
      '도 4: **기준 모델 아키텍처. 이미지 및 작업 설명은 DetACT 모듈로 보내지며, DetACT 모듈은 작업과 함께 프롬프트에 공급하는 것과 관련된 UI 요소의 필터링된 목록을 제공한다. 또한 액션 스크립트 생성에 사용되는 프롬프트 구조를 보인다. 이러한 구조는 자동화 스크립트를 생성하기 위해 LLM(멀티모달 LLM에 대한 이미지와 함께)을 통과한다.**\n' +
      '\n' +
      'LLaMA-13B 모델 및 Vicuna-13B는 QLoRa [10]을 사용하여 랭크 64 및 스케일링 팩터 16을 300 단계에 대해 사용하여 DetACT 모듈 및 명령어로부터 주어진 스크린 설명을 생성한다.\n' +
      '***Few-shot Generative Multimodal Models:** As\\(\\mathsf{OmnIACT}\\)는 주로 멀티모달이며, 대부분의 작업은 시각적으로 접지되어 있으며, 대규모 멀티모달 모델로 실험을 수행한다. 이 도메인[47, 52]에 대한 제한된 연구를 감안할 때 이 작업에 적합한 상당한 크기를 가진 사용 가능한 다중 모드 모델이 부족하다. 여기서는 화면 이미지뿐만 아니라 유사한 프롬프트를 제공하는 [24, 25]를 실험한다.\n' +
      '\n' +
      '우리는 표 4의 테스트 세트에 대한 모든 결과를 보고한다.\n' +
      '\n' +
      '##7 결과 및 분석\n' +
      '\n' +
      '표 4에 나타난 바와 같이, 우리는 Prompt 기반 LLMs, Fine-tuned LLMs 및 Prompt 기반 멀티모달 모델의 세 가지 다른 범주의 모델을 실험한다. GPT-4는 서열 점수에서 더 높은 점수를 매기고 좌표 예측 및 텍스트 입력에 대해 더 낮은 벌점을 유발하는 가장 성능이 좋은 접근법이다. 신속한 LLM의 경우 GPT-3.5-터보 및 GPT-4 모델이 LLaMA[43] 및 비쿠나[7] 모델을 포함한 다른 LLM 기준선을 능가한다. 코드 생성을 위해 훈련된 CodeLLaMA-34B[35]도 동작 시퀀스를 예측하는 데 있어 동일한 크기의 다른 모델보다 더 높은 성능을 달성함을 관찰한다.\n' +
      '\n' +
      '세밀하게 조정된 모델도 몇 안 되는 프롬프트 전용 모델보다 성능이 훨씬 우수합니다. 미세 조정은 LLaMA-13B의 시퀀스 점수(4.80 내지 8.92) 및 액션 점수(1.62 내지 2.14)뿐만 아니라 다른 메트릭을 실질적으로 향상시킨다. 그럼에도 불구하고, 우리는 프롬프트 기반 LLM과 미세 조정된 LLM이 특히 클릭 좌표에서 심각한 마우스 벌점에 직면한다는 것을 관찰했다. 텍스트 기반 신호에만 의존하기 때문입니다.\n' +
      '\n' +
      '이를 해결하기 위해 멀티모달 언어 모델을 실험한다(표 4). 전체 영상을 멀티모달 LLM에 입력으로 제공할 경우 화면 표현을 충분히 활용할 수 있기 때문에 좌표 예측이 크게 향상됨을 관찰한다. 또한 오픈소싱 모델 외에도 500개의 데이터 포인트(비용 오버헤드 및 오픈AI API 요청 한계로 인해)의 하위 집합에서 GPT-4-비전-프리뷰 API[50]를 실험한다. 표 5는 GPT-4 비전[50]이 시퀀스 점수 개선과 함께 Action Score에서 GPT-4보다 유의하게 우수한 것으로 나타났으며, 이는 GPT-4 비전-프리뷰 모델[50]의 향상된 시각적 이해 능력과 결합된 GPT-4의 강력한 추론 능력에 기인한다. 이러한 연구 결과는 장거리 계획 및 코드 생성을 위한 멀티모달 모델 구축에 대한 흥미로운 새로운 연구 방향을 제시한다.\n' +
      '\n' +
      '**\\(\\mathsf{OmnIACT}\\) 과제에 대한 인간의 수행:**\\(\\mathsf{OmnIACT}\\)은 시각적으로 복잡한 작업으로 구성되며, 다양한 종류의 컴퓨터 기술을 테스트한다. 인간이 얼마나 잘 수행하는지를 가늠하기 위해 인간 평가자로부터 평가 데이터를 수집한다. 우리는 10명의 인간 평가자 간에 테스트 세트를 균일하게 분할하고 스크린샷과 작업 지침을 제공했다. 우리는 주석자가 취한 조치를 기록하고 사전 정의된 메트릭에 대한 성능을 측정한다(표 4). 우리는 사용자들이 대부분의 작업을 처음 시도할 때 일반적으로 높은 수준의 숙련도를 보인다는 것을 발견한다. 그러나 사용자가 특정 작업을 성공적으로 완료하는 데 어려움을 겪는 경우가 있다. 이는 사용자가 태스크를 완전히 이해하지 못하거나, 제공된 스크린샷에 태스크를 접지하는 데 어려움이 있거나, UI에 대한 친숙도가 부족하기 때문이다.\n' +
      '\n' +
      '##8 결론 및 향후 과제\n' +
      '\n' +
      '자율적인 가상 에이전트는 일상적인 작업을 자동화할 수 있는 잠재력을 제공하여 제한된 기술 전문 지식을 가진 사용자에게 혜택을 줍니다. 이 과제를 해결하기 위해 우리는 9.8K 인간 표지 데이터 포인트의 고유한 데이터 세트인 \\(\\mathsf{OmnIACT}\\)을 소개한다. \\ (\\mathsf{OmnIACT}\\)는 웹 및 데스크톱 애플리케이션의 다양한 작업에 걸쳐 자율 에이전트를 벤치마킹한다. GPT-4와 같은LLM 기반 에이전트는 데이터 세트에서 11.6의 상당한 액션 점수를 달성한다. 그러나, \\(\\mathsf{OmnIACT}\\)는 현재 최첨단 언어 및 멀티모달 모델에 대한 도전을 제시한다. 컴퓨터 화면의 언어와 시각적 이해를 원활하게 통합하고 인간에게 전능한 지원을 제공하는 일반주의적 자율 에이전트의 다음 발전 물결을 주도할 태세를 갖춘 기반 멀티모달 모델에 대한 향후 연구의 방향을 제공한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Model** & **SS(\\(\\uparrow\\))** & \\(M_{p}\\) & \\(K_{p}\\) & \\(W_{p}\\) & **AS(\\(\\uparrow\\))** \\\\ \\hline \\multicolumn{7}{l}{**Prompt based LLMs**} \\\\ LLAM-7B [43] & 4.12 & 1.24 & 1.83 & 0.57 & 0.48 \\\\ Vicuna-7B [7] & 3.88 & 1.17 & 1.51 & 0.43 & 0.77 \\\\ LLaMA-13B [43] & 4.80 & 1.32 & 0.93 & 0.93 & 1.62 \\\\ Vicuna-13B [7] & 5.44 & 1.65 & 0.94 & 1.06 & 1.78 \\\\ Palmyra-Intarar-30B [41] & 7.51 & 5.68 & 0.12 & 0.40 & 1.31 \\\\ CodeLLaMA-34B [35] & 10.09 & 2.99 & 2.71 & 0.66 & 3.72 \\\\ Palmyra-X 43B [2] & 11.20 & 3.12 & 3.02 & 2.12 & 2.94 \\\\ GPT-3.5-turbo-0613 [29] & 22.85 & 8.13 & 4.51 & 2.31 & 7.89 \\\\ GPT-4[50] & **32.75** & 10.27 & 6.99 & 3.89 & **11.60** \\\\ \\hline \\multicolumn{7}{l}{**Fintuned LLMs**} \\\\ LLaMA-13B FT & **8.92** & 4.61 & 1.43 & 0.74 & 2.14 \\\\ Vicuna-13B FT & 8.78 & 4.12 & 1.31 & 0.63 & **2.72** \\\\ \\hline \\multicolumn{7}{l}{**Multimodal LLMs**} \\\\ LLaVA-v1.5-7B [24] & 13.23 & 4.73 & 1.24 & 1.44 & 5.82 \\\\ LLaVA-v1.5-13B [24] & **20.56** & 6.07 & 3.44 & 2.85 & **8.19** \\\\ \\hline Human Performance & 82.23 & 0.12 & 0.36 & 1.61 & 80.14 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 기준 성능. (A) Prompt-only LLMs, (B) Fine Tuned LLMs, (C) Prompt-only Multimodal Models. 표는 Sequence score(SS), click penalty(\\(M_{p}\\)), Key penalty(\\(K_{p}\\)), Write Penalty(\\(W_{p}\\)), Action Score(AS)를 나타낸다. (SS) 및 (AS)에 대한 최상의 결과가 강조 표시됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Model** & **Sequence Score (\\(\\uparrow\\))** & **Action Score (\\(\\uparrow\\))** \\\\ \\hline GPT-4 [50] & 36.42 & 12.77 \\\\ GPT-4V [50] & **39.43** & **20.76** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 500개의 샘플의 서브세트에 대한 GPT-4 및 GPT-4V의 결과.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Pyautogui: A cross-platform gui automation python module for human beings. [https://github.com/asweigart/pyautogui](https://github.com/asweigart/pyautogui), 2023.\n' +
      '* [2] Waseem AlShikh, Manhal Daaboul, Kirk Goddard, Brock Imel, Kiran Kamble, Parikshith Kulkarni, and Melisa Russak. Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning, 2023.\n' +
      '* [3] Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, and Blaise Aguera y Arcas. Uibert: Learning generic multimodal representations for ui understanding, 2021.\n' +
      '* [4] Pratyap Banerjee, Shweti Mahajan, Kushal Arora, Chitta Baral, and Oriana Riva. Lexi: Self-supervised learning of the ui language, 2023.\n' +
      '* [5] Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan A Plummer. Mobile app tasks with iterative feedback (motif): Addressing task feasibility in interactive visual environments.\n' +
      '* [6] Xingyu Chen, Zihan Zhao, Lu Chen, Jiabao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong, and Kai Yu. Websrc: A dataset for web-based structural reading comprehension. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 4173-4185, 2021.\n' +
      '* [7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatept quality, 2023.\n' +
      '* [8] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: A mobile app dataset for building data-driven design applications. In _Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology_, page 845-854, New York, NY, USA, 2017. Association for Computing Machinery.\n' +
      '* [9] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. _arXiv preprint arXiv:2306.06070_, 2023.\n' +
      '* [10] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023.\n' +
      '* [11] Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models. _arXiv preprint arXiv:2305.11854_, 2023.\n' +
      '* [12] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14953-14962, 2023.\n' +
      '* [13] Izzeddin Gur, Ulrich Rueckert, Aleksandra Faust, and Dilek Hakkani-Tur. Learning to navigate the web. In _International Conference on Learning Representations_, 2018.\n' +
      '* [14] Zecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu, Lijuan Liu, Nevan Wichers, Gabriel Schubiner, Ruby Lee, Jindong Chen, and Blaise Aguera y Arcas. Actionbert: Leveraging user actions for semantic understanding of user interfaces, 2021.\n' +
      '* [15] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers, 2020.\n' +
      '* [16] Peter C Humphreys, David Raposo, Tobias Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Adam Santoro, and Timothy Lillicrap. A data-driven approach for learning to control computers. In _International Conference on Machine Learning_, pages 9466-9482. PMLR, 2022.\n' +
      '* [17] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. _arXiv preprint arXiv:2303.17491_, 2023.\n' +
      '* [18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything, 2023.\n' +
      '* [19] Yann LeCun. A path towards autonomous machine intelligence version 0.9.\n' +
      '* [20] Gang Li and Yang Li. Spotlight: Mobile ui understanding using vision-language models with a focus, 2023.\n' +
      '* [21] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile ui action sequences, 2020.\n' +
      '* [22] Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements, 2020.\n' +
      '* [23] Yang Li, Gang Li, Xin Zhou, Mostafa Dehghani, and Alexey Gritsenko. Vut: Versatile ui transformer for multi-modal multi-task user interface modeling, 2021.\n' +
      '* [24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023.\n' +
      '* [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n' +
      '* [26] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models, 2023.\n' +
      '* [27] Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu. Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration. _arXiv preprint arXiv:2306.09093_, 2023.\n' +
      '* [28] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback.\n' +
      '* [29] OpenAI. Introducing chatgpt, 2023.\n' +
      '* [30] OpenAI. Gpt-4 technical report, 2023.\n' +
      '* [31] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318, 2002.\n' +
      '**[32] 크리스토퍼 롤스, 앨리스 리, 다니엘 로드리게스, 오리아나 리바, 티모시 릴릭랩. 야생에서 안드로이드: 안드로이드 장치 제어를 위한 대규모 데이터세트, 2023.\n' +
      '* [33] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code synthesis. _arXiv preprint arXiv:2009.10297_, 2020.\n' +
      '* [34] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code Ilama: Open foundation models for code, 2023.\n' +
      '* [35] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code Ilama: Open foundation models for code, 2023.\n' +
      '* [36] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow instructions via graphical user interfaces. _arXiv preprint arXiv:2306.00245_, 2023.\n' +
      '* [37] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In _International Conference on Machine Learning_, pages 3135-3144. PMLR, 2017.\n' +
      '* [38] Abishek Sridhar, Robert Lo, Frank F Xu, Hao Zhu, and Shuyan Zhou. Hierarchical prompting assists large language model on web navigation. _arXiv preprint arXiv:2305.14257_, 2023.\n' +
      '* [39] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. Meta-gui: Towards multi-modal conversational agents on mobile gui. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 6699-6712, 2022.\n' +
      '* [40] Didac Suris, Sachit Menon, and Carl Vondrick. Vibergpt: Visual inference via python execution for reasoning, 2023.\n' +
      '* [41] Writer Engineering team. InstructPalmyra-30b : Instruct tuned Palmyra-Large model. [https://dev.writer.com](https://dev.writer.com), 2023.\n' +
      '* [42] Writer Engineering team. Palmyra-base Parameter Autoregressive Language Model. [https://dev.writer.com](https://dev.writer.com), 2023.\n' +
      '* [43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.\n' +
      '* [44] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words: Automatic mobile ui summarization with multimodal learning. In _The 34th Annual ACM Symposium on User Interface Software and Technology_, pages 498-510, 2021.\n' +
      '* [45] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. _arXiv preprint arXiv:2308.11432_, 2023.\n' +
      '* [46] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. _Advances in Neural Information Processing Systems_, 33:5776-5788, 2020.\n' +
      '* [47] Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, and Wen Gao. Large-scale multi-modal pre-trained models: A comprehensive survey, 2023.\n' +
      '* [48] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE transactions on image processing_, 13(4):600-612, 2004.\n' +
      '* [49] Nancy Xu, Sam Mashing, Michael Du, Giovanni Campagna, Larry Heck, James Landay, and Monica Lam. Grounding open-domain instructions to automate web support tasks. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 1022-1032, 2021.\n' +
      '* [50] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmns: Preliminary explorations with gpt-4v (ision). _arXiv preprint arXiv:2309.17421_, 9, 2023.\n' +
      '* [51] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webbsp: Towards scalable real-world web interaction with grounded language agents. _Advances in Neural Information Processing Systems_, 35:20744-20757, 2022.\n' +
      '* [52] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models, 2023.\n' +
      '* [53] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert, 2020.\n' +
      '* [54] Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents, 2023.\n' +
      '* [55] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _arXiv preprint arXiv:2303.18223_, 2023.\n' +
      '* [56] Shuyan Zhou, Uri Alon, Sumit Agarwal, and Graham Neubig. Codebertscore: Evaluating code generation with pre-trained models of code. _arXiv preprint arXiv:2302.05527_, 2023.\n' +
      '* [57] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. _arXiv preprint arXiv:2307.13854_, 2023.\n' +
      '\n' +
      '본 논문에서는 데스크탑과 웹을 위한 보다 강력한 멀티모달 일반 자율 에이전트를 구축하는 데 도움이 되는 새로운 데이터세트 OrnniACT를 제시한다. 이와 함께, 컴퓨터 화면에서의 동작을 더 잘 평가할 수 있는 새로운 연속 척도 메트릭과 다중 LLM과 (비전 언어 모델) VLM에 통합한 DetACT 모듈을 제안하여 화면 이미지에서 유용한 특징을 추출하고 데이터 세트를 벤치마킹할 수 있다. 우리는 우리가 수행한 데이터 세트 및 실험에 대한 추가 통찰력을 제공하는 다음 항목을 제시한다.\n' +
      '\n' +
      '1. 애플리케이션 및 웹사이트의 목록\n' +
      '2. 추가 정량 결과\n' +
      '3. 주석 처리\n' +
      '4. 데이터세트 및 메타데이터 포맷\n' +
      '5. 모델 트레이닝을 위한 파라미터\n' +
      '6. 제한 및 브로드커 영향\n' +
      '7. 윤리성명\n' +
      '8. 작업 실행의 샘플\n' +
      '9. DetACT 및 기준선 및 샘플 응답에 대한 프롬프트\n' +
      '\n' +
      '## 부록 신청서 및 웹사이트 목록\n' +
      '\n' +
      '데이터 세트에 대해 선택된 애플리케이션 및 웹사이트의 전체 목록은 표 6에 나열되어 있으며, 데이터는 모든 도메인 및 운영 체제에 걸쳐 공평하게 표현되도록 보장한다. 우리는 각 운영 체제에 더 일반적으로 사용되는 데스크톱 애플리케이션을 선택합니다. 우리가 선택하는 일반적으로 사용되는 응용 프로그램은 절충 인구 통계를 대표한다. 또한 사용자가 모든 운영 체제에서 일반적으로 사용하는 몇 가지 타사 응용 프로그램을 선택합니다. 중복성을 피하고 작업의 다양성을 보장하기 위해 세 가지 운영 체제 모두에 대해 동일한 애플리케이션 세트를 선택하는 것을 삼간다. 웹사이트의 경우 사용자 의도에 따라 후보 집합을 선택합니다. 이는 (1) 쇼핑, (2) 엔터테인먼트, (3) 서비스, (4) 정부, (5) 여행, (6) 건강의 6개 영역으로 세분화된다.\n' +
      '\n' +
      '## 부록 B 추가토론\n' +
      '\n' +
      '###### 화면 영역 주의\n' +
      '\n' +
      '좌표 예측이 중력되는 위치를 확인하기 위해 데이터 세트에 대한 심층 분석을 수행한다. 우리는 참석하는 화면의 부분에 대한 흥미로운 통찰력을 찾습니다. 데스크톱 데이터에 대한 대부분의 예측은 화면의 중앙 또는 하단 모서리에 집중됩니다. 대부분의 웹 상호 작용은 화면의 하단 절반을 가리키고 있으며, 특히 모서리를 향합니다. 이것은 그림 5에 묘사되어 있다. 이것은 DetACT 모듈로부터 추출된 요소 외에 스크린 이미지에 크게 의존할 멀티모달 모델을 훈련시키기 위한 몇 가지 흥미로운 방향을 제시한다.\n' +
      '\n' +
      '그림 5의 화면 상단과 오른쪽에 있는 히스토그램에서 데이터 세트가 화면의 특정 영역에 크게 치우치지 않는다는 것을 알 수 있다. 이는 스크린의 특정 영역에만 참석하거나 스크린의 동일한 영역의 좌표를 예측할 가능성이 있는 모델이 데이터 세트에 대해 좋은 성능을 갖지 않을 것임을 의미한다.\n' +
      '\n' +
      'DetACT 모듈의### 영향\n' +
      '\n' +
      '이 실험 세트에서 우리는 DetACT 모듈을 사용하지 않고 결과를 제시하고 DetACT 출력이 있는 해당 모델과 비교한다. 우리는 표 7에서 GPT-4와 같은 모델도 언어 기반이기 때문에 DetACT에 의해 프롬프트되지 않을 때 0에 가깝게 수행한다는 것을 명확하게 관찰할 수 있다. 이는 모든 모델에 필수적인 단서인 텍스트 형태의 멀티모달 정보를 효과적으로 추출하는 DetACT 프레임워크와 같은 잘 갖춰진 화면 파싱 모듈의 중요성을 강조한다.\n' +
      '\n' +
      '### 시퀀스 길이 대 성능 비교\n' +
      '\n' +
      '우리는 동작들의 시퀀스를 시퀀스 길이의 함수로서 예측하는 성능을 분석한다. 구체적으로, 우리는\n' +
      '\n' +
      '도 5 : 바탕화면 및 웹 데이터용 출력 스크립트에서 화면 영역 주의사항\n' +
      '\n' +
      '대상 시퀀스가 길어지면 모델이 시퀀스를 정확하게 예측하는 것이 어려워지는지 확인하고 싶다. 그림 6은 테스트 세트에서 GPT-4에 대한 서열 길이의 함수로서 서열 예측의 정확도를 보여준다. 모델에 의해 예측된 동작들의 전체 시퀀스가 동작들의 골드 시퀀스와 동일할 때에만, 우리는 점수를 1로 부여하며, 표에서 정확도 백분율을 보고한다. 표 6에서 우리는 모델 성능이 더 짧은 시퀀스에 대해 더 우수하고 시퀀스 길이가 증가함에 따라 더 나빠진다는 것을 관찰한다.\n' +
      '\n' +
      '## 부록 C 주석 처리\n' +
      '\n' +
      '바운딩 박스 생성\n' +
      '\n' +
      '바운딩 박스를 생성하기 위해 데스크탑 및 웹 데이터에 대해 두 가지 다른 기술을 사용한다. 데스크톱 데이터의 경우 클릭 앤 드래그 메커니즘을 사용하여 경계 상자를 만드는 데 도움이 되는 맞춤형 PyQt5 도구를 사용합니다. 이렇게 하면 바운딩 박스 생성 프로세스가 향상되어 보다 직관적이고 사용자 친화적입니다. 또한 UI 요소의 대화형 영역을 포함할 수 있습니다. 예를 들어, 검색 바를 클릭할 때, 사용자는 텍스트를 입력할 수 있는 반면, 검색 바 아이콘(확대경 등)을 클릭하면 타이핑 영역이 활성화되지 않는다. 다음으로 웹 사이트의 경우 HTML의 DOM을 사용하고 JavaScript를 사용하여 경계 상자를 그린다. 이 두 프로세스 모두 높은 정확도를 보장하며 논문의 저자에 의해 수행된다. 각각에 대한 샘플은 그림 7에 나와 있으며 경계 상자가 설정된 후 저자는 작업을 교환하고 서로가 만든 상자를 검토한다.\n' +
      '\n' +
      '### Annotator Guidelines\n' +
      '\n' +
      '데스크톱 애플리케이션과 웹사이트 모두에 대한 경계 상자에 레이블을 지정하기 위해 MTurk를 활용합니다. 이것은 그림 8에 묘사되어 있다. MTurk 작업자들은 다른 박스들을 제거하는 스크린 상에 단일 바운딩 박스와 함께 존재하고, 이어서 스크린에 대한 맥락에서 UI 엘리먼트의 기능에 기초하여 박스를 라벨링하도록 요청된다. 각 상자에 대해 5단어 설명 또는 레이블을 작성하도록 요청됩니다. 모든 상자는 최종 필터링 동안 더 적절한 레이블을 유지하면서 두 배로 주석이 달린다. 모든 MTurk 근로자들은 20달러의 시간당 비율로 지불되는데, 이는 펜실베니아의 최저임금율을 훨씬 상회한다.\n' +
      '\n' +
      '다음으로 과제수립을 위해 파이썬과 코딩에 대한 기초 지식을 갖춘 학생 자원봉사자를 모집한다. 작업 공식화 프로세스를 시작하기 전에 30분 세션에서 모든 학생 자원 봉사자에게 _PyAutoGUI_ 사용에 대해 교육합니다. 그런 다음 모든 학생은 그 화면을 사용하여 만들 수 있는 작업의 샘플을 넘겨받고 몇 줄에 불과한 스크립트와 함께 더 많은 작업을 작성하도록 요청받는다. 모든 자원 봉사자들은 그 과제의 언어의 다양성을 보장하기 위해 언어적 변이와 함께 가능한 한 한 시간 안에 많은 과제를 생각해 내도록 요청받는다. 모든 자원봉사자들은 시간당 25달러의 임금률을 받으며, 이는 다시 주의 최저임금을 능가한다.\n' +
      '\n' +
      '스튜에 의해 생성된 작업을 수행할 스크립트를 구축합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l|l|l|l l l} \\hline \\hline \\multicolumn{4}{c}{**Desktop**} & \\multicolumn{3}{c|}{**Websites**} \\\\ \\hline \\hline  & \\multicolumn{2}{c}{**MacOS**} & \\multicolumn{2}{c|}{**Linux**} & \\multicolumn{2}{c|}{**Windows**} & & & \\\\ \\hline Mail & Music & App Store & Audible & Grammarly & AMC & Coursera & NPS \\\\ Maps & News & Calculator & Camera & Outlook & Apartments & FlightAware & NY Gov \\\\ Message & Photos & Calendar & Desktop & Spotify & Aramex & Google Careers & Samsung \\\\ Prime & Preview & Clock & Files & WeChat & Armani & HealthLine & Trip Advisor \\\\ Terminal & Reminder & Expedia & Settings & Calendar & Asics & Hertz & UIraul \\\\ Weather & Stocks & Finder & Todo & Files & AT\\&T & Indeed & Udemy \\\\ Zoom & System Preferences & iBooks & Text Editor & Settings & BestBuy & Instacart & United Airlines \\\\ Desktop & & & VLC & Windows Store & Booking & Mayo Clinic & UPS \\\\  & & & & Cmu.edu & Mint Mobile & Walmart \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 옴니ACT의 데스크톱 및 웹용 애플리케이션 목록\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Model & Sequence & Action \\\\  & Score & Score \\\\ \\hline GPT-4 (w/o DetACT) & 31.87 & 0.32 \\\\ GPT-4 + DetACT & 32.75 & 11.60 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: GPT-4 계열의 모델을 적용한 DetACT 모듈의 성능\n' +
      '\n' +
      '그림 6: 테스트 세트에 대한 GPT4의 시퀀스 예측 정확도. 시퀀스 길이가 증가함에 따라 성능이 감소합니다.\n' +
      '\n' +
      '작업이 실행 가능한지 확인하기 위해 화면을 찌그러뜨리고 실행합니다. 완료되면 저자는 고품질 작업만 유지하기 위해 최종 패스를 수행합니다.\n' +
      '\n' +
      '### 인간 피드백 및 평가\n' +
      '\n' +
      '인간 기준선을 평가하기 위해 학생 자원 봉사자를 다시 모집합니다. 이번에는 지원자에게 기준 출력 스크립트가 주어지지 않고 이미지와 작업 설명 쌍만 제공된다. 학생들은 몇 가지 샘플을 참조로 하여 자동화 스크립트를 작성하도록 요청받는다. 자원봉사자들은 자신의 경험과 직관을 활용하여 화면과 기능을 이해하고 이를 바탕으로 대본을 작성한다. 여기서 학생 근로자는 주 최저임금을 훨씬 초과하는 시간당 25달러의 비율로 보상받는다.\n' +
      '\n' +
      '## 부록 D 데이터세트 및 메타데이터 포맷\n' +
      '\n' +
      '데이터 세트는 각 분할에 대해 하나씩 세 개의 파일로 구성된다. 모든 파일에는 해당 특정 분할에 대한 작업 번호 목록이 있습니다. 모든 태스크 디렉토리는 (1) 화면의 스크린샷인 _image.png_ 파일과 (2) 태스크 디스크립션과 그라운드 트루스 출력 스크립트를 포함하는 _task.txt_ 파일로 구성된다. 또한 다른 파일인 (3) 메타데이터의 일부이며 상자의 좌표를 필요로 하는 평가를 실행하고 작업을 수행하거나 모델을 지원하는 데 사용되지 않는 _box.json_를 포함한다.\n' +
      '\n' +
      '## 모델 훈련을 위한 부록 E 파라미터\n' +
      '\n' +
      '표 8의 DetACT를 사용하여 DetACT 필터링 및 기준선에 대해 실행하는 모델에 대한 모델 사양을 등록한다.\n' +
      '\n' +
      '## 부록 F 브로드캐스팅 효과\n' +
      '\n' +
      '복잡하게 선별된 데이터 세트의 광범위한 영향은 다면적이다. 첫째, 데이터 세트는 기술적으로 문맹자에게 더 많은 권한을 부여할 수 있는 잠재력을 가지고 있습니다.\n' +
      '\n' +
      '그림 8: 모든 상자에 대한 라벨링 과정을 나타내는 MTurk 포털의 스크린샷.\n' +
      '\n' +
      '도 7: 데스크탑 애플리케이션 및 웹 사이트에 대한 바운딩 박스를 생성하기 위한 주석 프로세스.\n' +
      '\n' +
      '자동화되고 접근 가능한 방법으로 컴퓨터를 탐색하고 작동합니다.\n' +
      '\n' +
      '또한, 데이터 세트는 획기적인 UI 접지 연구의 길을 열어 운영 체제 수준에서 사용자 상호 작용에 대한 더 깊은 이해를 가능하게 한다. 이는 결국 OS 수준의 상호 작용을 포괄하는 AI의 범위를 확대함으로써 인공지능(AI) 분야를 발전시키는 데 기여한다. 데이터세트의 풍부한 콘텐츠는 대형 언어 모델(LLM)의 향상을 더욱 용이하게 하여, 기술 세트를 넓히고 다양한 사용자 입력을 이해하고 응답하는 능력을 향상시킨다.\n' +
      '\n' +
      '이 데이터 세트는 다양한 화면에서 원활하게 작동하고 장기적인 지평을 가진 작업을 실행할 수 있는 멀티모달 에이전트 개발의 기초가 된다. 애플리케이션의 이러한 다양성은 데이터 세트를 전통적인 한계를 초월하는 혁신적인 AI 기반 솔루션 생성을 위한 귀중한 자원으로 포지셔닝한다.\n' +
      '\n' +
      '추가적이고 영향력 있는 사용 사례는 컴퓨터 시스템을 운영하거나 인간-컴퓨터 상호 작용을 포함하는 일상적인 작업을 수행하는 데 어려움을 겪을 수 있는 장애인을 위한 보조 도구를 구축하는 것을 포함한다. 다양한 능력을 가진 개인의 요구를 충족시키기 위해 기술을 맞춤화함으로써 데이터 세트는 다양한 사용자의 접근성을 향상시키고 삶의 질을 향상시키는 보조 기술 개발에 기여한다.\n' +
      '\n' +
      '요약하면, 세심하게 선별된 데이터 세트의 광범위한 영향은 기술 접근성, UI 접지 연구, AI 기능 및 보조 도구의 발전을 견인할 수 있는 잠재력으로 나타나 궁극적으로 보다 포괄적이고 권한 있는 기술 환경을 육성한다.\n' +
      '\n' +
      '## 부록 G 윤리 성명\n' +
      '\n' +
      '이 작업은 귀중한 데이터 세트를 도입하지만 존재하는 몇 가지 한계를 인식한다. GPT-4와 같은 최첨단 모델은 환각에 대한 민감성과 특정 데이터 유형에 대한 편향을 나타내어 광범위한 적용 가능성을 방해할 수 있다. GPT-4V와 같은 폐쇄형 모델에 대한 의존은 실제 시스템의 높은 비용과 시간 제약으로 인해 통합 문제를 제기한다. 개인 정보가 없는 동등한 표현 및 데이터 수집을 위한 노력에도 불구하고 데이터 세트가 영어로 독점적이기 때문에 편향이 도입될 수 있으며 인간 큐어링된 콘텐츠는 시간적 편향을 가질 수 있다.\n' +
      '\n' +
      '## 부록 H 과제 수행 샘플\n' +
      '\n' +
      '우리는 선택된 작업에 대한 스크린샷과 대응하는 출력 스크립트를 포함하는 그림 9를 제시한다. 도 9에서, 태스크는 MacOS 네이티브 애플리케이션 _Books_로부터 취해진다. 마지막 줄을 녹색으로 강조 표시하고 다음 페이지로 스크롤하기 위해 작업 설명과 함께 책 장을 열었으며, 그 다음 작업에는 출력과 함께 시퀀스의 모든 동작을 나타내는 설명 스크린샷이 있다.\n' +
      '\n' +
      '## DetACT 및 기준선 및 샘플 응답에 대한 부록 I 프롬프트\n' +
      '\n' +
      '다음 페이지에서는 다음 작업에 사용하는 전체 길이 프롬프트를 제시합니다.\n' +
      '\n' +
      '*목록 1: DetACT 모듈에서의 UI 요소 필터링\n' +
      '* 리스트 2 : DetACT를 이용한 주행기준선 모델\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline  & **Model** & **Hyperparameters** \\\\ \\hline DetACT Filtering & GPT-4 & temperature = 0.1 \\\\ \\hline LLM Prompt Only Baselines & LLaMA-7B & max\\_new\\_tokens = 400, \\\\  & Vicums-7B & temperature = 0.8, repetition\\_penalty = 1.1 \\\\ \\hline LLaMA-13B & max\\_new\\_tokens = 400, \\\\  & Vicuma-13B & temperature = 0.8 \\\\ \\hline Palmyras-Instruct 30 B & temperature = 0.7 \\\\ \\hline Coded\\_LaMA-34B & max\\_new\\_tokens = 400, \\\\  & temperature = 0.7 \\\\ \\hline Palmyra-X 43B & temperature = 0.7 \\\\ \\hline GPT-3.5-urbo & temperature = 0.3 \\\\ \\hline LLM Fine-tuned baselines & LLaMA-13B FT & LoRA rank = 64, scaling factor = 16, \\\\  & Vicums-13B FT & LoRA dropout = 0.05, \\\\  & batch size = 4, learning rate = 5e-5 \\\\ \\hline Multimodal Model Prompt Only Baselines & LLaVA-v1.5-7B & temperature = 0.2, \\\\  & LLaVA-v1.5-13B & max\\_new\\_tokens = 512 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: DetACT 필터링 및 Baseline에 대한 모델 각각에 대해 사용되는 하이퍼파라미터.\n' +
      '\n' +
      '도 9: 모든 액션 시퀀스에 대한 중간 단계 및 대응하는 스크립트와 함께 태스크 데모.\n' +
      '\n' +
      'GivenbelowtheUIelementsextracted fromthescreen.Youhavetofiltertheelements forthisUIscreenthatarerelevantforcarryingoutthetaskgivenbelow. MakesuretofiltertheUIelementsthatmaybehelpfultocarryoutthetaskandmentionelementdescription and thecorrespondingcoordinatesforthetask. Formatforeveryelementwouldintheformof(ElementLabel,X-Coordinate,Y-Coordinate)\n' +
      '[IMPORTANT!!]Onlyremovetheelementsifyyouaresurethattheywillnothelpthetask.\n' +
      '[IMPORTANT!!]Followtheoutputstructurestrictlyasgiveninthebelowexampleandoutputnothingelseotherthantherequiredoutput.\n' +
      '\n' +
      'SampleTask: {SAMPLE_TASK}\n' +
      '\n' +
      'SampleUIElements: {LIST_OF_SAMPLE_UI_ELEMENTS}\n' +
      '\n' +
      'SampleFilteredUIElements: {LIST_OF_SAMPLE_FILTERED_UI_ELEMENTS}\n' +
      '\n' +
      'GivenTask: {TASK}\n' +
      '\n' +
      'GivenUIElements: {LIST_OF_UI_ELEMENTS}\n' +
      '\n' +
      '귀하는 귀하에게 주어진 작업에 대한 파이오토구이 스크립트를 생성해야 하는 훌륭한 로봇 프로세스 자동화 에이전트입니다. 각 튜플이 [UI 텍스트, X 좌표, Y 좌표] 형식인 튜플 목록으로 대화할 수 있는 UI 요소가 화면에 표시됩니다. 또한 생성해야 하는 스크립트의 형식을 도와줄 수 있는 예가 제공됩니다.\n' +
      '\n' +
      '[IMPORTANT:!] 예제에서 출력 스크립트의 형식을 고수합니다.\n' +
      '[중요도:!] API 문서에서 함수만 사용\n' +
      '[중요:!] 출력 형식을 엄격하게 따르십시오. 대본만 쓰고 나머지는 쓰지 마.\n' +
      '\n' +
      '여기 스크립트를 생성하기 위한 API 참조가 있다:\n' +
      '\n' +
      'def click(x=moveToX, y=moveToY):  """는 마우스를 위치로 이동시키고(moveToX, moveToY) 왼쪽 클릭을 한다. 예:  High Level Goal: 좌표에서 클릭(150, 230). 파이썬 스크립트:  import pyautogui  pyautogui.click(150, 230) ""\n' +
      '\n' +
      '```\n' +
      'pass def rightClick(x=moveToX, y=moveToY):  """는 마우스를 위치로 취하고(moveToX, moveToY) 오른쪽 클릭을 하는 예:  High Level Goal: 좌표에서 오른쪽 클릭(350, 680). 파이썬 스크립트:  import pyautogui  pyautogui.rightClick(350, 680)"\n' +
      '`` pass def doubleClick(x=moveToX, y=moveToY):  """는 마우스를 위치로 잡고(moveToX, moveToY) 오른쪽 클릭을 하는 예:  High Level Goal: 좌표에서 오른쪽 클릭(350, 680). 파이썬 스크립트:  import pyautogui  pyautogui.rightClick(350, 680)"\n' +
      '\n' +
      '```\n' +
      'pass def scroll(clicks=amount_to_scroll):  ""scrollst the window having mouse pointer by float value(amount_to_scroll)  예:  High Level Goal: Scroll screen by (30).  Python script:  import pyautogui  pyautogui.scroll(30)"\n' +
      '`` pass def hscroll(clicks=amount_to_scroll):  """scrollst the window having mouse pointer by float value(amount_to_scroll) 예:  High Level Goal: Scroll screen by horizontal by (30).\n' +
      '\n' +
      'Python script: import pyautogui  pyautogui.hscroll(30)  """ pass def dragTo(x=moveToX, y=moveToY, button=hold Button):  """=drags themouseto(moveToX, moveToY) with (hold Button) 홀드버튼은 \'왼쪽\', \'중간\' 또는 \'오른쪽\'일 수 있다. 예: 상위 목표: 마우스 왼쪽 클릭으로 화면을 현재 위치에서 (450, 600)으로 끕니다. Python script: import pyautogui  pyautogui.dragTo(450, 600, \'left\')  pass def pyautogui.moveTo(x=moveToX, y=moveToY)  """=takethemousepointeto(moveToX,moveToY) 예:  High Level Goal: hoverthemousepointeto(450, 600) Python script: import pyautogui  pyautogui.moveTo(450,600)  pass def write(str=stringType,interval=secs_between_keys):  ""=writesthestringwhereverkeycursorisatthefunctioncallingtimewith  (secs_between_keys)secondsbetweencharacters  Example:  High Level Goal: Write "Helloworld"with 0.1secondsrate  Python script: import pyautogui  pyautogui.write("Helloworld",0.1)" pass def press(str=string_to_type):  ""="=simulatepressingakeydownandthenreleasingitup.Samplekeyinclude \'enter\',\'shift\',arrowkey, \'fl" 예:  High Level Goal: Pressurekeynow  Python script:  import pyautogui  pyautogui.press("enter")  """ 통과 def hotkey(*args=list_of_hotkey):  """=KeyboardhotkeyslikeCtrl-SorCtrl-Shift-1canbedonebypassingalistofkey  namestohotkey().Multiplekeycanbepressedtogetherwithachotkey). 예:  High Level Goal: UseCtrlandVtopastefromclipboard  Python script:  import pyautogui  pyautogui.hotkey("ctrl","v")``\n' +
      '\n' +
      'forgeneratingthescriptandformattobollowed: 예1: {RETREIVED_EXAMPLE_1} 예2: {RETREIVED_EXAMPLE_3} 예4: {RETREIVED_EXAMPLE_4} 예5: {RETREIVED_ELEMENTS_FROM_DETACT} HerearetheIcon/ImageEentsfoundonthescreenalwiththeircoordinates: {TELOR_ELEMENTS_FROM_DETACT} HerearetheIcon/ImageEentsfoundonthescreenalwiththeircoordinates:\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>