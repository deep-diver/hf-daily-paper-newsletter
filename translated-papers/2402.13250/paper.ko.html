<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 비디오 리캡: 시간-장편 비디오의 재귀적 캡션화\n' +
      '\n' +
      'Md Mohaiminul Islam\\({}^{1}\\) Nagan Ho\\({}^{1}\\) Xitong Yang\\({}^{2}\\) Tushar Nagarajan\\({}^{2}\\)\n' +
      '\n' +
      'Lorenzo Torresani\\({}^{2}\\) Gedas Bertasius\\({}^{1}\\)\n' +
      '\n' +
      'Meta AI Chapel Hill \\({}^{1}\\) UNC Chapel Hill \\({}^{2}\\) Meta AI Chapel Hill \\({}^{1}\\)\n' +
      '\n' +
      '[https://sites.google.com/view/vidrecap](https://sites.google.com/view/vidrecap)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대부분의 비디오 캡션 모델은 몇 초의 짧은 비디오 클립을 처리하고 낮은 수준의 시각적 개념(예를 들어, 객체, 장면, 원자 액션)을 설명하는 텍스트를 출력하도록 설계된다. 그러나 대부분의 실제 비디오는 몇 분 또는 몇 시간 동안 지속되며 서로 다른 시간적 입도에 걸쳐 복잡한 계층적 구조를 가지고 있다. 본 논문에서는 비디오 캡셔닝 모델인 Video ReCap을 제안한다. Video ReCap은 매우 다른 길이(1초에서 2시간)의 비디오 입력을 처리하고 비디오 캡션을 여러 계층 수준에서 출력할 수 있는 재귀 비디오 캡션 모델이다. 재귀적 비디오 언어 아키텍처는 서로 다른 비디오 계층 간의 시너지를 이용하며, 한 시간 동안의 비디오를 효율적으로 처리할 수 있다. 교육과정 학습 훈련 기법을 활용하여 원자의 행동을 묘사하는 클립 수준 캡션부터 시작하여 세그먼트 수준 설명에 초점을 맞추고 1시간 동안의 비디오에 대한 요약을 생성하는 방식으로 비디오의 계층적 구조를 학습한다. 또한, 수동으로 수집된 8,267개의 장거리 비디오 요약으로 Ego4D를 증강하여 Ego4D-HCap 데이터 세트를 소개한다. 우리의 재귀적 모델은 다양한 계층 수준에서 캡션을 유연하게 생성할 수 있는 동시에 EgoSchema 상의 VideoQA와 같은 다른 복잡한 비디오 이해 작업에도 유용할 수 있다. 데이터, 코드 및 모델은 공개적으로 사용할 수 있습니다[1].\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '현실 세계의 많은 비디오들은 상이한 시간적 세분성들(즉, 원자 액션들, 중간 활동 단계들, 장기 목표들 등)에서 인간의 행동들에 걸쳐 있는 계층적 정보 구조를 나타낸다. 그러나, 대부분의 현대 비디오 캡션화 모델들은 계층적 비디오 구조를 무시하고, 전형적으로 5-15 초[4, 14, 22, 38, 39, 43, 46, 50, 51, 57, 63]로 제한된 짧은 비디오 입력들에 대해 구체적으로 맞춤화된다. 이러한 단거리 캡션 방법은 원자적 액션과 객체 및 장면과 같은 낮은 수준의 시각적 세부 사항을 캡처한다. 더욱이, 이러한 모델들은 종종 더 긴 비디오들에 적용될 때 엄청나게 자원 집약적이어서, 장기간(예를 들어, 수 시간)에 걸쳐 발생하는 인간 활동을 이해하기에 부적합하다[28, 46, 51, 63].\n' +
      '\n' +
      '본 논문에서는 긴 비디오 입력(예: 몇 분에서 몇 시간)이 주어졌을 때 여러 계층 수준에서 캡션을 생성해야 하는 계층적 비디오 캡션 작업을 조사한다. 심리학 [11, 19, 16]과 사회 인지 이론 [5]의 연구들은 인간 행동의 내재적 계층 구조를 보여주었으며, 가장 낮은 수준의 원자적 행동, 중간 단계의 중간 단계 및 위계의 가장 높은 수준의 전반적인 목표/의도들로 구성되었다. 이러한 선행 연구에서 영감을 얻은 우리는 또한 비디오 캡션 작업에 대해 세 가지 수준의 계층 구조를 가정한다. 가장 세분화된 수준에서 비디오 캡션은 객체, 장면 및 원자 액션과 같은 낮은 수준의 시각적 요소에 초점을 맞추어 몇 초의 개별 프레임 또는 짧은 비디오 클립을 설명한다. 계층 구조를 위로 이동함에 따라, 단기 캡션들은 더 넓은 활동들 내의 중간 단계들(예를 들어, 요리 레시피에서의 단일 단계) 또는 더 확장된 스토리라인 내의 짧은 세그먼트들 또는 시퀀스들(예를 들어, 영화 내의 몇 분 길이의 장면)과 같은 짧은 순간들을 넘어 연장되는 활동들에 걸쳐 있는 중간 길이의 비디오 세그먼트 설명들로 통합된다. 마지막으로, 계층 구조의 최상위 레벨은 비디오 내의 장기적인 인간 목표, 이벤트와 캐릭터 사이의 복잡한 관계, 비디오 이면의 가장 중요한 목적을 캡슐화하며, 이는 장거리 비디오 요약을 통해 캡처될 수 있다(도 1 참조).\n' +
      '\n' +
      '계층적 비디오 캡셔닝의 작업은 몇 가지 기술적 과제를 제기한다. 첫째, 몇 초에서 몇 시간까지 매우 다양한 입력 길이를 처리할 수 있는 모델이 필요하다. 이것은 최대 몇 분 동안 고정된 비디오 지속 시간을 위해 설계된 대부분의 기존 방법과 대조된다. 둘째, 장거리 비디오는 매우 중복되므로 모델이 중요하지 않은 시각적 신호를 폐기하면서 필수 정보만 집계해야 한다. 셋째, 긴 비디오에서 계층적 구조를 이해하고 별개의 계층 간의 시너지를 활용하는 것이 또 다른 중요한 과제이다.\n' +
      '\n' +
      '이러한 기술적 과제를 해결하기 위해 우리는 입력 시간이 최대 3배(몇 초에서 몇 시간)까지 다를 수 있는 극적으로 다른 길이의 비디오를 처리하고 여러 계층 수준에서 캡션을 생성할 수 있는 모델인 비디오 ReCap을 제안한다. 우리의 모델은 계층적 비디오 캡션 기능에 권한을 부여하는 세 가지 주요 속성을 포함한다. 첫째, Video ReCap은 재귀적 비디오 언어 구조를 채택하여 별개의 계층 계층에서 자막을 생성할 수 있다. 첫 번째 레벨에서, 모델은 일반적으로 몇 초 동안 지속되는 짧은 비디오 클립들로부터 추출된 특징들로부터 캡션들을 생성한다. 계층 구조를 위로 이동할 때, 모델은 현재 계층 구조에 대한 비디오 캡션을 생성하기 위해 이전 계층 수준에서 생성된 희소 샘플링된 비디오 피쳐 및 캡션을 입력으로 사용한다. 이러한 재귀적 설계는 상이한 비디오 계층들 사이의 시너지를 효과적으로 활용하고, 매우 긴 비디오 입력들(예를 들어, 최대 2시간)을 효율적으로 처리할 수 있게 한다. 또한, 우리의 모델이 현대 LLM의 강력한 추론 능력을 활용하는 것을 용이하게 한다. 둘째, 짧은 비디오 클립 캡션에 대한 교육을 시작하고 중간 길이의 세그먼트 설명 및 장거리 비디오 요약과 같은 상위 계층의 데이터를 점진적으로 통합하는 커리큘럼 학습 방식을 구현한다. 이러한 계층적 교육과정 학습 전략은 모델이 짧은 저수준 캡션에서 긴 고수준 비디오 요약까지 점진적으로 비디오의 계층적 구조를 학습할 수 있도록 한다. 셋째, 제한된 수동 주석이 달린 계층적 캡션 데이터의 문제를 완화하기 위해 LLM을 사용하여 서로 다른 시간 길이에 걸쳐 있는 의사 요약 데이터를 생성한 다음 이러한 의사 주석을 추가 데이터로 사용하여 모델을 훈련한다.\n' +
      '\n' +
      '비디오 ReCap을 평가하기 위해, 우리는 여러 계층 수준에서 수동으로 주석이 달린 캡션을 사용하여 최대 몇 시간 동안 지속되는 장거리 자기 중심 비디오를 포함하는 새로운 계층적 비디오 캡션 벤치마크인 Ego4D-HCap 데이터 세트를 소개한다. Ego4D-HCap 벤치마크를 구축하기 위해 최대 5분의 시간 스탬프 캡션 및 비디오 세그먼트 요약을 제공하는 가장 큰 공개적으로 이용 가능한 장거리 자기 중심 비디오 데이터 세트인 Ego4D[20]를 활용한다. 그런 다음 수동으로 주석이 달린 8,267개의 장거리 비디오 요약을 사용하여 Ego4D 비디오의 하위 집합을 추가하며 각 비디오는 최대 2시간에 걸쳐 있다. 결과적으로, Ego4D-HCap은 짧은 클립에 대한 캡션, 몇 분 비디오 세그먼트에 대한 중간 설명 및 긴 비디오 시퀀스에 대한 비디오 레벨 요약을 포함하는 긴 트리밍되지 않은 자기 중심적 비디오에 대한 3 레벨의 계층적 캡션을 갖는 풍부한 리소스가 된다.\n' +
      '\n' +
      '본 연구의 결과는 비디오 ReCap이 세 가지 시간적 계층에서 모두 강력한 이전 비디오 캡션 기준[29, 67]보다 큰 마진으로 우수함을 보여준다. 또한 비디오 ReCap이 EgoSchema[35]에 대한 긴 형식의 비디오 질문 응답과 같은 복잡한 비디오 이해 작업에 효과적으로 사용될 수 있음을 보여준다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '**비디오 캡션 방법.** 비디오 캡션에서 초기 작업은 템플릿 기반 접근법[25, 27, 43, 49, 61]을 사용하였다. 이어서, 이러한 방법들은 CNN-RNN 인코더-디코더 아키텍처들[8, 17, 37, 38, 47, 55, 56, 64]을 사용하여 구축된 딥 러닝 방법들로 대체되었다. 최근 트랜스포머[18, 53]의 도입으로 변압기 기반 비디오 캡션 방법[8, 22, 28, 38, 39, 46, 47, 51, 56, 63]이 과다하게 도입되었다. 비록 이러한 방법들이 짧은 클립 캡셔닝에서 큰 성공을 보여주었지만, 대부분은 몇 초의 짧은 비디오 클립 시퀀스에 한정되고, 따라서 시간 동안 다수의 시간적 계층들에 걸쳐 캡션을 생성할 수 없다.\n' +
      '\n' +
      '도 1: **계층적 비디오 캡션. 우리는 세 개의 시간적 세분도에서 장거리 비디오(예: 몇 시간 길이)에 대한 계층적 캡션을 생성하는 것을 목표로 한다. 먼저, 우리는 원자 인간의 행동에 초점을 맞춘 비디오의 몇 초마다 짧은 클립 캡션을 생성한다. 이후 동영상의 몇 분마다 중간 길이의 세그먼트 설명을 생성하여 더 긴 활동 내에서 중간 단계를 캡처하거나 확장된 스토리라인 내에서 비디오 세그먼트를 캡처한다. 마지막으로, 이 방법은 비디오에서 배우들의 전반적인 의도와 목표를 묘사하는 장거리 비디오에 대한 요약을 생성한다.**\n' +
      '\n' +
      'long videos.\n' +
      '\n' +
      '**비디오 캡션 데이터 세트.** 대부분의 기존 비디오 캡션 데이터 세트는 짧은 비디오 클립 입력(5-30초)을 포함한다[13, 42, 58, 60]. 1-5분[23, 26, 68]의 더 긴 비디오들을 갖는 몇몇 데이터세트들이 존재하지만, 이러한 데이터세트들의 캡션들은 여전히 단기적인 시각적 개념들(예를 들어, 원자적 액션들, 객체들의 존재 등)에 초점을 맞추고 있다. 대신에, 우리의 연구는 짧은 클립 캡션들로부터 장거리 비디오 요약들에 이르는 다수의 시간적 세분성 레벨들에 걸쳐 있는 계층적 비디오 캡셔닝을 위한 모델들 및 데이터세트들을 개발하는 것을 목표로 한다. 이를 위해 Ego4D-HCap 데이터 셋을 1시간짜리 비디오의 장거리 비디오 요약으로 증강하여 소개한다. 이는 짧은 클립 캡션, 중간 범위의 세그먼트 설명 및 장거리 비디오 요약으로 구성된 계층적 비디오 캡션 데이터세트로 이어진다.\n' +
      '\n' +
      '**Hierarchical Video Understanding.** 몇 가지 최근 데이터 세트는 절차적 비디오에 대한 계층적 활동 주석[7, 45, 48, 52, 69]을 포함한다. 그러나 이러한 데이터 세트는 각 계층의 활동 레이블에 대한 고정 분류법을 정의하고 절차적 활동 인식에 중점을 둔다. 대조적으로, 우리는 현실 세계 비디오(교육 비디오에만 국한되지 않음)에서 고유한 계층적 구조를 캡처하기 위해 여러 수준에 대한 자유 형식의 자연 언어 설명을 가정한다. 데이터 세트 외에도 여러 방법 [3, 30, 66]은 몇 분 길이의 비디오(예: 5분)에 대한 계층적 특징 임베딩을 학습한다. 대조적으로, 우리의 작업은 여러 시간 스케일에서 시간 길이의 비디오에 대한 자유 형식의 계층적 캡션을 생성하는 데 중점을 둔다.\n' +
      '\n' +
      '##3 기술적 접근\n' +
      '\n' +
      '### Problem Overview\n' +
      '\n' +
      '트리밍되지 않은 긴 비디오 입력이 주어지면, 우리는 비디오의 여러 계층 레벨에서 텍스트 캡션을 생성하는 것을 목표로 한다. 형식적으로, 입력으로서, 우리는 \\(I_{i}^{(t)}\\)으로 표시된 \\(T\\) RGB 프레임으로 구성된 장거리 비디오 시퀀스 \\(V_{i}=[I_{i}^{(t)}]_{t=1,\\dots,T}\\)을 고려한다. 본 논문의 목적은 3개의 계층적 레벨에서 캡션을 생성하는 것이다. 즉, \\(Y_{i}^{(\\ell)}=[y_{i,j}^{(\\ell)}]_{j=1,\\dots,|Y_{i}^{(\\ell)}}|}\\)은 \\(\\ell=1,2,3\\에 대해, 여기서 \\(y_{i,j}^{(\\ell)}\\)은 계층 레벨 \\(l\\)에 대한 캡션에서 \\(j^{th}\\) 단어를 묘사한다. 캡션의 각 계층은 단기 비디오 클립 캡션 \\(Y_{i}^{(1)}\\)으로 시작하여 순차적으로 생성되며, 비디오 전체에 걸쳐 몇 초 간격 내에 발생하는 세밀한 동작 및 객체를 설명한다(예를 들어, 사람이 그림 1에서 사과를 줍는다). 그 후, 모델은 중간 길이의 세그먼트 설명 \\(Y_{i}^{(2)}\\)을 출력하는데, 이는 비디오의 몇 분(예를 들어, 자동차를 운전하고 그것을 주차하는 사람)에 걸쳐 전개되는 중간 단계 또는 요약을 캡처한다. 마지막으로, 이 모델은 전체 비디오 입력에 대한 비디오 콘텐츠를 나타내는 장거리 비디오 요약 \\(Y_{i}^{(3)}\\)으로 생성을 완료한다.\n' +
      '\n' +
      '### 재귀적 비디오-언어 모델\n' +
      '\n' +
      '이제 비디오 인코더, 비디오 언어 정렬 및 재귀 텍스트 디코더의 세 가지 상위 구성 요소를 포함하는 비디오 ReCap 모델을 설명한다. 우리는 그림 2에서 우리의 접근법을 설명하고 아래에 각 구성요소를 설명한다.\n' +
      '\n' +
      '**비디오 인코더.** 먼저, 우리는 장거리 비디오로부터 특징들을 추출하기 위해 기성 비디오 인코더(예를 들어, TimeSformer[10])를 활용한다. 짧은 비디오 클립이 주어지면, 비디오 인코더는 조밀한 시공간 특징들을 출력한다. 전체 영상을 균일하게 분할하고 특징들의 시퀀스를 추출하는데, 여기서 \\(X_{i}=[x_{i,j}]_{j=1,\\dots,|C|}\\), \\(|C|\\)은 비디오 클립의 수, \\(x\\in\\mathbb{R}^{F\\times H\\times W\\times D}\\)은 특정 클립의 시공간적 특징, \\(F\\)은 프레임 수, \\(H\\)은 높이, \\(W\\)은 너비, \\(D\\)은 특징 차원이다. 모델이 낮은 수준의 시각적 단서(예: 객체 및 원자적 동작)를 식별할 수 있도록 짧은 클립 캡션에 조밀한 시공간 특징을 사용하고, 높은 수준의 캡션(예: 세그먼트 설명 및 비디오 요약)에 대해서는 전역 특징(예: CLS 특징)을 사용하여 계산 비용을 줄이고 긴 비디오 입력의 전역 특성을 캡처한다.\n' +
      '\n' +
      'Video-Language Alignment.** 다음으로, Video-Language(VL) Alignment 모듈을 이용하여 비디오의 특징, \\(X_{i}\\) 및 이전 계층구조에서 생성된 캡션 \\(Y_{i}^{(\\ell-1)}\\)을 입력으로 하고 고정된 임베딩 수 \\(Z_{i}=[z_{i,j}]_{j=1,\\dots,|Z|}\\)을 출력하며, 여기서 \\(z\\in\\mathbb{R}^{D_{i}}\\), \\(|Z|\\)은 임베딩 수, \\(D_{z}\\)은 은닉 차원을 나타낸다. 정렬 모듈의 목적은 비디오 및 텍스트 특징을 조인트 특징 공간에 매핑하여 후속 텍스트 디코더가 [29]와 같이 양쪽 특징을 공동으로 처리할 수 있도록 하는 것이다. 더욱이, 이 기법은 많은 수의 비디오 및 텍스트 특징(예를 들어, 수천 개)을 작은 임베딩 세트(예를 들어, 256 개)로 압축할 수 있게 하여 계산 비용을 극적으로 감소시킨다. 특히, 냉동 사전 훈련 언어 모델(예: DistilBERT[44])을 사용하여 LM의 각 트랜스포머 블록 내부에 훈련 가능한 교차 주의 레이어를 주입하여 비디오 피처 \\(X_{i}\\)로부터 고정된 수의 비디오 임베딩을 학습한다. 또한 학습 가능한 교차 주의 계층이 있는 유사한 냉동 LM을 사용하여 이전 계층 \\(Y_{i}^{(\\ell-1)}\\)에서 생성된 캡션으로부터 고정된 수의 텍스트 임베딩을 학습한다. 마지막으로, 자막을 생성하기 위해 후속 텍스트 디코더에서 사용하는 결합 임베딩(Z_{i}\\(Y_{i}^{(\\ell)}\\)을 얻기 위해 비디오와 텍스트 임베딩을 연결한다. 제1 계층 레벨(즉, 클립 캡션)은 텍스트 특징이 없고 단지 비디오 임베딩을 \\(Z_{i}\\)으로 사용한다는 점에 유의한다.\n' +
      '\n' +
      '**재귀적 텍스트 디코더.** 다중 계층 레벨에서 캡션을 생성하기 위한 재귀적 텍스트 디코더로서 사전 훈련된 언어 모델(예를 들어, GPT2[41])을 사용한다. 디코더는 비디오 언어 정렬 모듈에 의해 생성된 비디오 텍스트 임베딩(Z_{i}\\)을 가지고 계층 구조에 대한 캡션(Y_{i}^{\\ell}\\)을 생성한다. 입력의 하나로 이전 계층 레벨(Y_{i}^{\\ell-1}\\)에서 생성된 캡션(X_{i}\\)을 사용하는데, 이는 재귀적 캡션 생성 파이프라인을 가능하게 한다. 단기 자막 생성(즉, \\(Y_{i}^{1}\\))을 위해 텍스트 특징 세트는 빈 것으로 초기화된다(즉, 본 모델의 재귀의 기본 경우). 이전 작업 [2, 67]에 이어 텍스트 디코더의 각 변압기 층 내부에 훈련 가능한 교차 주의 블록을 삽입하고 나머지 층을 동결한다. 교차 주의 계층은 정렬 모듈의 비디오-텍스트 임베딩에 참석한다. 따라서 제안된 비디오 ReCap은 다음과 같은 훈련 목적을 이용하여 비디오에 조건화된 캡션 \\(Y^{(\\ell)}\\)과 하위 계층에서 생성된 캡션 \\(Y^{(\\ell-1)}\\)의 가능성을 모델링한다:\n' +
      '\n' +
      '\\[p(Y^{(\\ell)}|X)=\\prod_{k=1}^{K}p(y_{k}^{(\\ell)}|y_{<k}^{(\\ell)},X,Y^{(\\ell-1)}) \\tag{1}\\]\n' +
      '\n' +
      '여기서, \\(y_{k}^{(\\ell)}\\)는 자막의 언어토큰을 나타내고, \\(y_{<k}^{(\\ell)}\\)는 선행 토큰들의 집합이며, \\(Y^{(0)}=\\emptyset\\)을 나타낸다.\n' +
      '\n' +
      '계층적 교육과정 학습\n' +
      '\n' +
      '재귀적 비디오 언어 모델을 훈련하는 것은 여러 가지 이유로 어렵다. 먼저, 모델은 극적으로 상이한 입력 길이(즉, 수 초 내지 수 시간)의 비디오를 처리해야 한다. 둘째, 단기 클립 캡션이 비디오 세그먼트 설명 및 장거리 요약의 수를 크게 능가하는 상당한 데이터 불균형이 있다. 마지막으로, 서로 다른 계층 수준 간의 시너지 효과를 활용하는 것은 의미 있고 맥락적으로 관련된 캡션을 생성하는 데 중요하다. 이러한 도전을 극복하기 위해 인간의 행동 인식의 계층적 조직을 보여주는 심리학의 고전적 연구[5, 9, 11, 16]에서 동기를 도출한다. 인간이 중간 단계의 행동을 파악하기 전에 먼저 원자적 행동을 지각한 후 중간 단계의 활동으로부터 목표를 추론하는 것처럼 우리의 훈련 전략은 유사한 위계적 방식으로 전개된다. 특히, 우리의 훈련은 가장 낮은 계층 수준의 샘플, 즉 클립 캡션으로 시작한다. 그 후, 중간 길이의 세그먼트 설명 및 장거리 비디오 요약과 같은 더 높은 수준의 캡션을 사용하여 모델을 훈련한다. 이러한 전략적 진행을 통해 모델은 비디오에 내재된 복잡한 계층 구조를 점진적으로 이해하고 모든 계층 간의 시너지 효과를 극대화할 수 있다. 더욱이, 이 전략은 상이한 계층들에 걸쳐 고도로 불균형한 트레이닝 데이터를 효과적으로 처리한다. 그림 3은 제안된 교육과정 학습 전략에 대한 개요를 보여준다.\n' +
      '\n' +
      '언어 모델을 이용한### 추가 감독\n' +
      '\n' +
      '한 시간 동안의 비디오에 대한 캡션 주석을 수집하는 것은 시간이 많이 걸리고 비용이 많이 든다. 따라서, 또 다른 중요한 찰\n' +
      '\n' +
      '그림 2: ** Video ReCap 모델. (왼쪽) 먼저, 미리 훈련된 비디오 인코더(도면에 도시되지 않음)에 의해 추출된 조밀한 시공간 특징을 사용하여 비디오의 각 짧은 클립(예를 들어, 몇 초 길이)에 대한 캡션을 생성한다. (중간) 그리고 비디오 ReCap은 희박하게 샘플링된 특징들(예를 들어, CLS 특징들) 및 특정 세그먼트에 속하는 이전에 생성된 클립 캡션들을 사용하여 비디오의 수 분마다 세그먼트 디스크립션들을 생성한다. (오른쪽) 마지막으로, 비디오 ReCap은 전체 비디오에서 희박하게 샘플링된 CLS 특징과 이전에 생성된 세그먼트 설명을 활용하여 전체 비디오 요약을 생성한다. Video-Language(VL) Alignment 모듈은 비디오 및 텍스트 특징들을 조인트 공간에 매핑하여 후속 텍스트 디코더가 이들을 공동으로 처리할 수 있도록 한다. 참고: 노란색 상자는 오른쪽에서 왼쪽으로 줌인하는 세 개의 패널 각각에서 비디오의 첫 번째 세그먼트를 나타냅니다.**\n' +
      '\n' +
      '계층적 비디오 캡션과 관련된 lenge는 특히 중간 길이의 세그먼트 설명 및 장거리 비디오 요약에 대해 수동으로 주석이 달린 계층적 캡션 데이터의 희소성이다. 우리는 이 문제를 완화하기 위해 대규모 언어 모델(LLM)을 활용합니다. LLM은 다양한 길이의 텍스트 입력으로부터 정보를 효과적으로 통합할 수 있으며, 이는 비디오 모델이 여러 계층에 걸쳐 캡션을 생성하도록 안내하는 우리의 목표와 완벽하게 일치한다. 이러한 통찰력에 의해 우리는 LLM을 사용하여 중간 길이 및 장거리 비디오(즉, 마지막 두 계층)에 대한 많은 수의 의사 캡션 주석을 생성한다. 그 과정은 두 가지 주요 단계를 포함한다. 먼저, 수동으로 주석이 달린 계층적 캡션이 주어지면 LLM 교사를 미세 조정하여 다양한 시간 기간에 걸쳐 연결된 단기 클립 캡션에서 중간 길이 세그먼트 설명과 장거리 비디오 요약을 생성한다. 이후, 이러한 LLM 생성 의사 그라운드 트루스 캡션 데이터를 추가 학습 샘플로 사용하여 Video ReCap을 학습한다(도 4 참조). 실험을 통해 LLM에 의해 생성된 유사 지상진실 데이터는 수동으로 주석이 달린 데이터를 효과적으로 보완하고 모델의 캡션 기능을 크게 향상시킨다는 것을 알 수 있다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '비디오 인코더로 TimeSformer [10]을 사용하여 \\(224\\times 224\\) RGB 프레임의 입력 클립을 추출하는 특징을 추출한다. 우리는 GPT2[41]을 기본 텍스트 디코더로 사용하며, \\(768\\) 및 \\(12\\) 변환 블록의 숨겨진 차원이 있다. 우리는 학습률 \\(3^{-5}\\)과 중량감소 \\(0.01\\)의 Adam optimizer[24]를 사용한다. 우리의 훈련 파이프라인은 또한 코사인 스케줄링 전략을 활용했다[33]. 추가적인 실시 내역은 보충 자료를 참고하시기 바랍니다.\n' +
      '\n' +
      '## 4 Ego4D-HCap 데이터세트\n' +
      '\n' +
      '이제 우리는 소개된 Ego4D-HCap 데이터세트, 짧은 클립 수준 캡션, 중간 길이의 비디오 세그먼트 설명 및 장거리 비디오 수준 요약의 3계층 계층 구조로 구성된 계층적 비디오 캡션 데이터세트를 설명한다. Ego4D-HCap을 구성하기 위해 공개적으로 이용 가능한 가장 큰 자기 중심적 비디오 데이터 세트인 Ego4D[20]를 활용한다. Ego4D 비디오는 몇 가지 고유한 기능을 가지고 있어 계층적 비디오 캡션 작업에 이상적입니다. 첫째, Ego4D의 대부분의 비디오는 전통적인 비디오 캡션 데이터 세트보다 10배 더 긴(예: 몇 시간) 것이다. 둘째, 자기중심적 비디오는 일반적으로 다른 계층 수준에서 목표 지향 및 인간 활동을 포함한다. 셋째, Ego4D 동영상은 요리, 정원 가꾸기, 조립 등과 같은 다양한 시나리오에서 인간의 행동을 포착한다.\n' +
      '\n' +
      'Ego4D는 최대 5분에 걸쳐 있는 시간 스탬프 원자 캡션 및 비디오 세그먼트 설명과 함께 제공되는 반면, 더 긴 비디오 기간에 대한 비디오 레벨 요약이 부족하다. 이 문제를 해결하기 위해 각각 최대 2시간에 걸쳐 장거리 비디오 요약이 있는 8,267개의 Ego4D 비디오의 하위 집합에 주석을 달았다. 이 향상은 3단계의 캡션 계층을 제공하여 계층적 비디오 캡션 태스크에 대한 모델의 유효성을 검증하는 데 완벽한 자원이 된다. 표 1에서 우리는 소개된 Ego4D-HCap 하위 집합에 대한 자세한 요약을 제공한다.\n' +
      '\n' +
      '제안된 Ego4D-HCap 데이터 세트에는 가구 설정, 실외 환경, 직장, 여가 활동 등과 같은 다양한 컨텍스트에서 다양한 시나리오를 캡처하는 비디오가 포함되어 있으며 총 127개의 별개의 시나리오가 있다. 가장 일반적인 50개의 시나리오의 분포는 그림 5에 예시되어 있다. Ego4D-HCap 데이터세트에서 세 계층 레벨에 대한 캡션 길이의 분포는 그림 6에 예시되어 있다. 특히, 클립 캡션은 일반적으로 더 짧고 캡션당 평균 7.74개의 단어를 나타낸다. 이에 비해 세그먼트 설명은 중간 길이로 평균 15.79 단어를 표시하고 비디오 요약이 평균 25.59 단어로 가장 길다. 또한, 클립 캡션의 최대 길이는 43 단어이고, 세그먼트 설명 및 비디오 요약은 각각 73 단어 및 172 단어로 확장될 수 있음을 관찰한다. 당사의 보충 자료에는 데이터 세트 및 주석 수집 프로세스에 대한 자세한 내용이 포함되어 있습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Hierarchy Level & \\# Samples & Avg. Duration \\\\ \\hline Clip Caption & 5.27M & 0.96 sec \\\\ Segment Description & 17.5K & 2.87 min \\\\ Video Summary & 8.3K & 28.46 min \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **Ego4D-HCap 데이터세트의 요약.**\n' +
      '\n' +
      '그림 4: **Large Language Model Supervision.** 단기 지상 진실 캡션이 주어지면 LLM을 사용하여 중간 길이 세그먼트 설명 및 장거리 비디오 요약을 위한 의사 지상 진실 주석을 생성하여 훈련 데이터를 증강한다.\n' +
      '\n' +
      '그림 3: **계층적 교육과정 학습.** 짧은 저수준 자막에서 긴 고수준 동영상 요약을 시작으로 점차 동영상의 계층 구조를 학습한다.\n' +
      '\n' +
      '## 5 실험 설정\n' +
      '\n' +
      '### 계층적 비디오 캡션 베이스라인\n' +
      '\n' +
      '계층적 비디오 캡셔닝은 상대적으로 탐구되지 않은 작업이므로 우리의 작업을 비교할 수 있는 잘 정립된 기준이 없다. 따라서 우리는 이 작업을 위해 확장한 다음 비디오 언어 기준선을 소개한다.\n' +
      '\n' +
      '***Zero-Shot Baselines:**1. **BLIP2**[29]: 최첨단 이미지 캡션 모델을 활용하는 _short-term clip captioning_에 대한 제로-shot baseline.\n' +
      '**BLIP + GPT3.5**[12, 29]: _video segment description_ 및 _long-range video summaries_에 대한 제로-샷 텍스트 기반 베이스라인. BLIP2 생성 캡션이 주어지면 GPT3.5를 사용하여 비디오 세그먼트 설명 및 장거리 비디오 요약을 생성한다.\n' +
      '***LaViLa + GPT3.5**[12, 67]: 위와 유사하게, GPT3.5에 공급된 LaViLa 캡션을 사용하여 _video segment_ 및 _summary_ 생성을 위한 제로-샷 베이스라인.\n' +
      '***Finetuned Baselines:** 1. **LaViLa + GPT2**[41, 67]: LaViLa 생성 클립 캡션을 취하고 기본 LaViLa 모델을 동결 상태로 유지하면서 _segment description_ 및 _video summary_ 생성을 위한 텍스트 전용 GPT2 모델을 미세 조정하는 완전 미세 조정 텍스트 기반 기준선.\n' +
      '***LaViLa + FLAN-T5**[15, 67]: 위와 유사하게, _segment description_ 및 _video summary_ 생성을 위해 GPT2가 아닌 FLAN-T5를 사용하는 완전finetuned text-based baseline.\n' +
      '***LaViLa**[67]: 비디오 입력을 직접 사용하여 _short-term 캡션, 중간 길이 세그먼트 설명_ 및 _long-range 비디오 요약_를 생성하기 위해 비디오 기반 베이스라인, finetuned end-to-end. 이 기준선은 모델과 동일한 비디오 인코더, 텍스트 디코더 및 기타 실험 설정을 사용합니다.\n' +
      '\n' +
      '우리의 모델 변종들\n' +
      '\n' +
      '1. [leftmargin=*]\n' +
      '2. **Video ReCap.** 본 모델의 이러한 변형은 공유 비디오 인코더를 사용하지만 별개의 텍스트 디코더 및 비디오-언어 정렬 모듈을 사용하여 상이한 계층 레벨(즉, 상이한 계층에 걸친 가중치는 공유되지 않음)에서 캡션을 생성한다. 모델 증가로 인해\n' +
      '\n' +
      '그림 5: Ego4D-HCap 데이터셋에서 가장 일반적인 50개 시나리오의 **분포.**\n' +
      '\n' +
      '도 6: Ego4D-HCap 데이터세트의 3개의 계층적 캡션의 길이의 **분포.**계층별 특수 모듈을 갖는 능력, 이 변형은 전형적으로 최상의 성능을 생성한다.\n' +
      '2. **Video ReCap-U.** 모든 계층에 걸쳐 공유된 파라미터를 사용하는 통일된 변형. 이전 변형보다 훈련 가능한 매개 변수가 훨씬 적기 때문에 더 효율적이지만 특정 설정에서 약간 더 나쁜 성능을 발휘한다.\n' +
      '\n' +
      '##6 결과 및 분석\n' +
      '\n' +
      '### 계층적 비디오 캡션 결과\n' +
      '\n' +
      '표 2에서는 계층적 비디오 캡셔닝에 대한 주요 결과를 제시한다. 계층적 비디오 캡션 작업에 대한 모델을 평가하기 위해 CIDEr[54], ROUGE-L[31], METEOR[6]을 포함한 표준 캡션 메트릭을 사용한다. 이러한 결과를 바탕으로 몇 가지 흥미로운 경향을 관찰한다. 먼저, 제로샷 기준선(예를 들어, BLIP2[29], BLIP2 + GPT3.5[12], LaViLa + GPT3.5)이 완전히 세분화된 접근법(예를 들어, LaViLa[67], LaViLa + GPT2[41], LaViLa + FLAN-T5[15])보다 상당히 더 나쁜 성능을 나타냄에 주목하여, Ego4D-HCap 데이터 세트에서 도메인 내 학습의 중요성을 강조한다. 둘째, 훈련 가능한 매개변수(S86M vs 339M)를 사용함에도 불구하고, 텍스트 기반 베이스라인인 LaViLa + FLAN-T5 [15]는 비디오 세그먼트 기술에서 2.61% CIDEr, 비디오 요약 생성에서 9.94% CIDEr로 우리 모델에 미치지 못하는 것으로 나타났다. 이는 비디오 세그먼트 기술 및 장거리 비디오 요약 생성을 위해 텍스트만 사용하는 것이 아니라 계층적 비디오 및 텍스트 입력을 사용하는 이점을 나타낸다. 셋째, Ego4D[20]에 대한 클립 캡셔닝에서 가장 성능이 좋은 비디오 ReCap 변종은 강력한 LaViLa 기준선에서 상당히 향상되어 9.79% CIDEr의 성능을 능가하는 반면, 본 모델과 동일한 시각적 인코더, 텍스트 디코더 및 학습 데이터를 사용한다. 우리는 LaViLa가 변압기 리샘플러[2, 67]를 사용하는 반면, 우리의 모델은 언어 모델 기반 정렬 모듈(섹션 3.2 참조)을 사용하므로 이 특정 작업에 매우 효과적임을 발견했다.\n' +
      '\n' +
      '또한 LaViLa의 성능이 세그먼트 설명 및 비디오 요약 생성에 대해 크게 감소하여 장거리 비디오를 처리할 수 없음을 나타낸다. 대조적으로, Video ReCap은 이러한 더 긴 비디오 입력들에 대해 강한 성능을 유지하며, 세그먼트 기술에서 17.11% CIDEr, 비디오 요약 생성에서 21.52% CIDEr만큼 LaViLa를 능가한다. 또한 Video ReCap이 LaViLa(258M vs. 339M), 비디오 ReCap-U는 LaViLa보다 트레이닝 파라미터(113M)가 현저히 적지만, 실질적인 마진(+20.97% 및\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 2: **Ego4D-HCap 데이터 세트에 대한 주요 결과. 본 논문에서 제안한 비디오 ReCap 모델은 기존의 LaViLa[67]를 능가하는 3가지 계층에서 가장 우수한 성능을 보였으며, LLM에 의해 생성된 의사주석을 사용하면 성능이 크게 향상되었으며, 또한 단일화된 모델은 세그먼트 설명 및 비디오 요약 생성을 위해 CIDEr의 표준변수인**+24.50%보다 훨씬 적은 수의 훈련 가능한 파라미터를 가지고 경쟁적인 결과를 얻을 수 있었다. 이는 우리 모델의 성능 이득이 더 큰 용량의 모델이 아니라 재귀적 및 계층적 설계에서 비롯됨을 나타낸다. 또한 LLM 기반 감독(섹션 3.4 참조)을 통합하여 모델의 성능을 더욱 향상시킬 수 있음을 나타낸다. 마지막으로, 표 2의 마지막 두 행은 우리 모델의 두 변형 간의 트레이드 오프(trade-off), 즉 비디오 ReCap이 세 계층 중 두 계층에서 가장 높은 성능을 달성하는 반면, 통합된 변형인 비디오 ReCap-U는 훈련 가능한 매개 변수가 훨씬 적은 두 번째로 우수한 성능을 달성한다.\n' +
      '\n' +
      'EgoSchema에 대한 Long-Range VideoQA\n' +
      '\n' +
      '표 3에서, 우리는 최근에 도입된 장거리 비디오 질문 응답(VideoQA) EgoSchma 데이터 세트에 대한 계층적 비디오 모델의 유효성을 검증한다[35]. EgoSchema는 250시간의 실제 비디오에 걸쳐 5K 이상의 인간 큐어링된 객관식 질문-답변 쌍을 포함하고 있어 긴 비디오에 대한 계층적 추론이 필요하다. 우리는 EgoSchema에서 VideoQA를 수행하기 위해 간단한 2단계 접근법을 사용한다. 먼저, 긴 EgoSchema 비디오 입력이 주어지면, 우리는 이전처럼 계층적 비디오 캡션을 생성한다. 이후 생성된 계층적 비디오 캡션을 텍스트 전용 GPT3.5[12]에 입력으로 공급하고 제로 샷 방식으로 주어진 비디오에 대한 질문에 답하도록 프롬프트한다. 간단한 프레임워크는 단순함에도 불구하고 이 벤치마크에서 매우 잘 수행된다. 우리는 먼저 GPT3.5의 입력으로 단기 캡션만을 사용하는 방법의 변형과 비교하여 계층적 비디오 캡션을 사용하는 변형이 상당한 4.2% 성능 향상을 달성한다는 것을 관찰한다. 또한 계층적 비디오 캡션이 아닌 LaViLa 생성 단기 캡션을 GPT3.5의 입력으로 사용하는 유사한 기준선과 우리의 방법을 비교하고 우리의 접근법이 이 기준선보다 5.96% 더 우수함을 보여준다. 이것은 장거리 비디오QA에 대한 계층적 비디오 단서의 이점을 강조한다. 우리의 결과는 또한 우리의 방법이 이전의 최고의 모델인 InternVideo[59]를 18.13%의 큰 마진으로 능가하여 이 벤치마크에서 새로운 최신 기술을 설정함을 나타낸다. 그러나 우리는 InternVideo가 Ego4D에서 사전 훈련된 적이 없기 때문에 우리의 접근법과의 비교는 다소 불공평할 수 있다는 점에 주목한다. 따라서 비교에서 Ego4D, EgoVLP[32] 및 EgoVLPv2[40]에서 사전 훈련된 두 가지 최근 방법도 포함한다. 모든 평가에 대해 데이터 유출을 방지하기 위해 교육 세트에서 EgoSchema 벤치마크가 사용하는 모든 Ego4D 비디오를 제거했습니다. EgoVLP 및 EgoVLP2와 비교하여 우리의 접근법은 여전히 최상의 결과를 달성하여 이 두 기준선을 16%의 상당한 마진으로 능가하여 우리의 방법의 우수성을 나타낸다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '**재귀 아키텍처의 중요성.** 이 섹션에서 우리는 비디오 ReCap 모델의 재귀 아키텍처의 중요성을 분석한다. 이 변형의 경우, 재귀 입력(즉, 이전 계층 수준에서 생성된 캡션)을 버리고 성능을 재귀 모델과 비교한다. 결과를 표 4 에 나타낸다. 우리는 비디오 세그먼트 설명에 대해 비재귀적 변형의 성능이 1.57% CIDEr 감소하는 것을 관찰한다. 또한, 재귀적 모델 구조는 비재귀적 변형의 성능이 CIDEr 없이 2.42% 감소하는 장거리 비디오 요약 생성에 훨씬 더 중요하다. 이러한 실험은 비디오 ReCap의 재귀적 설계가 계층적 비디오 캡션 작업, 특히 장거리 비디오 입력에서 더 나은 성능을 유도한다는 것을 보여준다.\n' +
      '\n' +
      '**계층적 교육과정 학습의 의의** 다음으로, 계층적 교육과정 학습 체계의 의의를 조사한다. 이러한 교육과정 학습 방안의 중요도는 표 5와 같다. GPT2 사전 훈련된 초기화의 세그먼트 설명에 대해 모델을 직접 훈련하면 성능이 저하되는 것을 관찰한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c} \\hline \\multirow{2}{*}{Recursive} & \\multicolumn{2}{c|}{Segment Description} & \\multicolumn{3}{c}{Video Summary} \\\\ \\cline{2-7}  & C & R & M & C & R & M \\\\ \\hline ✗ & 40.17 & 38.65 & 17.59 & 25.64 & 29.61 & 13.57 \\\\ ✓ & **41.74** & **39.04** & **18.21** & **28.06** & **32.27** & **14.26** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **Recursive Inputs의 중요성.** 우리 모델의 비재귀적 변형은 세그먼트 설명 및 비디오 요약 생성(-1.57% 및 -2.42% CIDEr)에서 더 나쁜 성능을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\multirow{2}{*}{Model} & Input & Ego4D & QA \\\\  & Feature & Pretrain & Acc \\\\ \\hline Random & - & ✗ & 20.0 \\\\ GPT3.5 [12] & Question & ✗ & 19.57 \\\\ \\hline FrozenBiLM [62] & Video & ✗ & 26.9 \\\\ VIOLET [19] & Video & ✗ & 19.9 \\\\ mPLUG-Owl [65] & Video & ✗ & 31.1 \\\\ InternVideo [59] & Video & ✗ & 32.1 \\\\ \\hline EgoVLPv [32] & Video & ✓ & 34.86 \\\\ EgoVLPv2 [40] & Video & ✓ & 34.12 \\\\ \\hline LaViLa [67] + GPT3.5 [12] & Captions & ✓ & 44.27 \\\\ Video ReCap + GPT3.5 [12] & Captions & ✓ & 46.03 \\\\ Video ReCap + GPT3.5 [12] & Hier. Captions & ✓ & **50.23** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: **Long-Range VideoQA on EgoSchema**[35] 우리의 접근법은 18.13%의 실질적인 마진으로 이전의 최상의 방법인 InternVideo를 능가하는 최첨단 결과를 달성한다. 또한, 모델에 의해 생성된 계층적 캡션을 활용하면 모델의 짧은 클립 캡션 또는 LaViLa[67]에 의해 생성된 캡션을 사용하는 것과 비교하여 성능이 4.2% 및 5.96% 향상된다. 이것은 장거리 비디오 질문 답변을 위한 계층적 비디오 캡션의 효능을 입증한다.\n' +
      '\n' +
      '4.93% CIDEr의 상당한 마진 게다가, 교육과정 학습이 없는 비디오 요약 생성의 경우 성능 하락은 훨씬 더 재앙적(-19.44%)이다. 마지막으로, 단기 자막에서 시작하여 중간 길이의 세그먼트 설명으로 전환하고 마지막으로 장거리 비디오 요약으로 마무리하는 고급 자막을 점진적으로 통합하는 것이 유용함을 보여준다. 단기 캡션에서 장거리 비디오 요약 학습으로 진행되는 변형은 CIDEr 성능에서 3.22% 감소를 직접 나타낸다.\n' +
      '\n' +
      '**LLM 기반 수퍼비전의 중요성** 다음으로 중간 길이의 세그먼트 설명 및 장거리 비디오 요약에 대한 LLM 기반 수퍼비전의 중요성을 연구한다. 표 5(a)에서 우리는 의사 진리 데이터를 생성하기 위해 사용하는 서로 다른 LLM 교사(예: GPT2[41] 및 FLAN-T5[15])의 성능을 보여준다. 우리는 FLAN-T5-Large가 모든 메트릭에서 최고의 성능을 달성한다는 것을 관찰한다. 따라서 우리는 FLAN-T5-Large를 선생님으로 사용하여 세그먼트 설명 및 장거리 비디오 요약을 위한 의사 그라운드 진리 데이터를 생성한다. 구체적으로, 우리는 세그먼트 설명을 위해 100K 의사 주석을 생성하고 비디오 요약을 위해 15K를 생성한다. 우리는 이러한 의사 주석을 수동으로 주석이 달린 데이터와 결합하고 모델을 훈련한다. 표 5(b)는 LLM으로부터의 감독을 활용하는 것이 세그먼트 기술(+5.14% CIDEr 이득) 및 비디오 요약(+1.28% CIDEr 개선) 생성 성능 모두에서 실질적인 성능 부스트를 제공한다는 것을 보여준다.\n' +
      '\n' +
      '**입력 양식의 절제.** 섹션 3.2에 설명된 바와 같이, 본 모델은 세그먼트 설명 및 비디오 요약을 위해 비디오 특징과 텍스트 입력(이전 계층에서 생성됨)을 모두 활용한다. 클립 캡션에 대한 텍스트 입력은 재귀 비디오 모델의 기본 경우를 정의하므로 사용하지 않습니다. 우리는 GPU 메모리에 장거리 비디오를 맞추기 위해 비디오 특징을 희박하게 샘플링해야 하기 때문에 텍스트를 중간 표현으로 사용하는 것이 희박한 비디오 특징을 보완해야 한다고 가정한다. 표 7은 우리의 가설을 입증하고 비디오 및 텍스트 특징을 입력으로 사용하는 것이 우리 모델에 대한 최상의 성능을 산출한다는 것을 보여준다. 구체적으로, 세그먼트 기술 생성을 위해, 비디오 및 텍스트 입력을 조합하는 것은 CIDEr에서 비디오 전용에 대한 +1.57% 부스트 및 텍스트 전용 기준선에 대한 +1.64% 부스트를 생성한다. 더욱이, 장거리 비디오 요약 생성을 위해, 비디오 + 텍스트 입력은 비디오 전용 및 텍스트 전용 변형에 비해 +2.42% 및 +4.83% 이득을 제공한다.\n' +
      '\n' +
      'Ego4D-HCap의 정성적 결과\n' +
      '\n' +
      '그림 7에서 우리는 모델에 의해 생성된 계층적 캡션의 세 가지 인스턴스를 제시한다. 클립 캡션은 대부분 \'C가 탭을 닫는다\'(도 7(a)) 및 \'C가 트롤리를 밀는다\'(도 7(b))와 같은 원자적 액션들 및 객체들을 기술한다는 것이 명백하다. 대조적으로, 세그먼트 설명들은 더 긴 기간들에 걸쳐 있는 비디오 내의 중간 개념들, 즉 \'C는 부엌에 있었고, 세척된 도구들\'(도 7(a)), 및 \'C는 텐트를 배열하고 여성과 상호작용했다\'(도 7(c))에 초점을 맞춘다. 더욱이, 비디오 요약들은 비디오의 가장 중요한 콘텐츠 및 이벤트들을 캡슐화하는 것을 목표로 한다. 예를 들어, \'C는 슈퍼마켓에 갔다. C는 과일 채소를 주웠고, 다른 사람들과 교류했다. C bought gro\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c} \\hline \\multirow{2}{*}{\\begin{tabular}{c} Pseudo \\\\ Ann. \\\\ \\end{tabular} } & \\multicolumn{3}{c|}{Segment Description} & \\multicolumn{3}{c}{Video Summary} \\\\ \\cline{2-7}  & C & R & M & C & R & M \\\\ \\hline \\multirow{2}{*}{\n' +
      '\\begin{tabular}{c} KIT \\\\ \\end{tabular} } & 41.74 & 39.04 & 18.21 & 28.06 & 32.27 & 14.26 \\\\  & **46.88** & **39.73** & **18.55** & **29.34** & **32.64** & **14.45** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: **LLM Supervision의 중요성. Top:** 다양한 시간적 길이에 걸쳐 연결된 지상-진실 단기 캡션이 주어지면 FLAN-T5-Large는 세그먼트 설명 및 장거리 비디오 요약 주석을 위한 최고 품질의 의사 주석을 생성한다. 이 LLM 오라클을 사용하여 중간 길이의 세그먼트 설명에 대해 100K 의사 주석을 생성하고 장거리 비디오 요약에 대해 15K를 생성한다. **Bottom:** LLM 생성 주석과 훈련 중 수동 주석을 결합하면 세그먼트 설명을 위한 경우 5.14% CIDEr, 비디오 요약을 위한 경우 1.28% CIDEr의 성능 향상으로 이어진다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c} \\hline \\multirow{2}{*}{\n' +
      '\\begin{tabular}{c} Input \\\\ \\end{tabular} } & \\multicolumn{3}{c|}{Segment Description} & \\multicolumn{3}{c}{Video Summary} \\\\ \\cline{2-7}  & C & R & M & C & R & M \\\\ \\hline Video & 40.17 & 38.65 & 17.59 & 25.64 & 29.61 & 13.57 \\\\ Text & 40.10 & 38.02 & 17.41 & 23.23 & 29.17 & 13.31 \\\\ Video + Text & **41.74** & **39.04** & **18.21** & **28.06** & **32.27** & **14.26** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: **Video-Language Input Ablation.** 비디오 및 텍스트 특징 모두를 사용하는 것은 세그먼트 설명 및 비디오 요약 생성 모두에 대해 더 나은 성능을 이끈다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c} \\hline \\multirow{2}{*}{\\begin{tabular}{c} Training Scheme \\\\ \\end{tabular} } & \\multicolumn{3}{c|}{Segment Description} & \\multicolumn{3}{c}{Video Summary} \\\\ \\cline{2-7}  & C & R & M & C & R & M \\\\ \\hline \\multirow{2}{*}{\\begin{tabular}{c} Init \\(\\rightarrow\\) Segment \\\\ Caption \\(\\rightarrow\\) Segment \\\\ \\end{tabular} } & 36.81 & 38.70 & 17.17 & - & - & - \\\\ Caption \\(\\rightarrow\\) Segment & **41.74** & **39.04** & **18.21** & - & - & - \\\\ \\hline \\multirow{2}{*}{\n' +
      '\\begin{tabular}{c} Init \\(\\rightarrow\\) Video \\\\ Caption \\(\\rightarrow\\) Video \\\\ Caption \\(\\rightarrow\\) Segment \\(\\rightarrow\\) Video \\\\ \\end{tabular} } & - & - & - & 8.62 & 26.33 & 11.24 \\\\  & - & - & - & 24.84 & 30.74 & 13.25 \\\\ Caption \\(\\rightarrow\\) Segment \\(\\rightarrow\\) Video & - & - & - & **28.06** & **32.27** & **14.26** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: **계층적 교육과정 학습.** 제안된 교육과정 학습 방식을 사용하면 GPT2 사전 훈련 가중치(Init)로부터 모델을 훈련하는 것과 비교하여 세그먼트 설명에서 +4.93%, 장거리 비디오 요약 생성에서 +19.44%의 성능 부스트를 산출한다.\n' +
      '\n' +
      '출납원(도 7의 (b)).\n' +
      '\n' +
      '또한 클립 캡션 및 세그먼트 설명을 생성하는 것이 상대적으로 더 간단하지만 비디오 요약을 생성하는 것이 더 어렵다는 것을 알 수 있다. ♪ 나를 위해 ♪\n' +
      '\n' +
      '그림 7: **Ego4D-HCap.**에 대한 정성적 결과 일반적으로 클립 캡션은 원자적 행동과 객체를 묘사하며, 세그먼트 설명은 중간 개념에 초점을 맞추고 비디오 요약은 비디오의 전체 내용과 목표를 요약한다. 클립 캡션 및 세그먼트 설명을 생성하는 것은 종종 비교적 쉬운 작업이지만, 좋은 비디오 요약을 개발하는 것은 종종 어렵다. 우리의 모델은 비디오 요약(a) 및 (b)에서 잘 수행되지만 생성된 비디오 요약(c)은 더 향상될 수 있다.\n' +
      '\n' +
      '스턴스, 도 7(a) 및 도 7(b)의 생성된 비디오 요약은 양호한 품질이지만, 도 7(c)의 비디오 요약은 더욱 개선될 수 있었다. 도 7(c)의 비디오 요약은 비디오의 일부 중요한 이벤트를 캡처하지 못하고 반복되는 단어 및 구문을 포함한다. 이러한 과제는 장거리 비디오에서 콘텐츠를 요약하는 복잡성을 강조한다. 향후의 발전과 공개된 데이터의 사용이 이 까다로운 작업에 대한 보다 효과적인 방법과 모델의 개발에 기여할 것으로 기대한다.\n' +
      '\n' +
      '##7 결론 및 향후 작업\n' +
      '\n' +
      '본 논문에서는 비디오 리캡(Video ReCap)을 소개한다. 비디오 캡션은 간략한 클립 자막부터 광범위한 시간 요약까지 다양한 시간 입도에 걸쳐 있는 비디오에 대해 계층적 캡션을 생성할 수 있는 재귀적 비디오 캡션 모델이다. 인간 심리학에서 영감을 얻은 커리큘럼 학습 스킴과 LLM 기반 감독 전략의 통합은 계층적 비디오 캡션 문제를 해결하는 데 모델의 효능을 향상시킨다. 주된 초점을 넘어, 우리 모델의 계층적 캡션은 장거리 비디오 질문 답변에도 유리하다는 것을 증명합니다. 또한, 선별된 Ego4D-HCap 데이터 세트는 비디오 이해 연구의 진행 중인 진행을 촉매하기 위해 출시될 것이다. 몇 가지 유망한 미래 방향에는 실시간 캡션 생성, 대화형 비디오 이해 및 비디오 기반 대화가 포함된다.\n' +
      '\n' +
      '인정합니다. 펑청, 옌보 린, 세 장, 욱 양, 소미트리 채토파디야이에 도움이 되는 토론에 감사드립니다. 이 작업은 NC 주립 대학을 통한 소니 교수 혁신 상, 분석 과학 연구소, ONR 상 N00014-23-1-2356의 지원을 받았다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Video ReCap webpage: [https://sites.google.com/view/vidrecap](https://sites.google.com/view/vidrecap).\n' +
      '* [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _Advances in Neural Information Processing Systems_, 35:23716-23736, 2022.\n' +
      '* [3] Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, and Kristen Grauman. Hiervl: Learning hierarchical video-language embeddings. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 23066-23078, 2023.\n' +
      '* [4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. _arXiv preprint arXiv:1409.0473_, 2014.\n' +
      '* [5] Albert Bandura. Social cognitive theory: An agentic perspective. _Asian journal of social psychology_, 2(1):21-41, 1999.\n' +
      '* [6] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In _Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization_, pages 65-72, 2005.\n' +
      '* [7] Siddhant Bansal, Chetan Arora, and CV Jawahar. My view is the best view: Procedure learning from egocentric videos. In _European Conference on Computer Vision_, pages 657-675. Springer, 2022.\n' +
      '* [8] Lorenzo Baraldi, Costantino Grana, and Rita Cucchiara. Hierarchical boundary-aware neural encoder for video captioning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1657-1666, 2017.\n' +
      '* [9] R.G. Barker and H.F. Wright. _Midwest and Its Children: The Psychological Ecology of an American Town_. Row, Peterson, 1954.\n' +
      '* [10] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In _ICML_, page 4, 2021.\n' +
      '* [11] Matthew Botvinick and David C Plaut. Doing without schema hierarchies: a recurrent connectionist approach to normal and impaired routine sequential action. _Psychological review_, 111(2):395, 2004.\n' +
      '* [12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [13] David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In _Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies_, pages 190-200, 2011.\n' +
      '* [14] Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang, and Jing Liu. Valor: Vision-audio-language omni-perception pretraining model and dataset. _arXiv preprint arXiv:2304.08345_, 2023.\n' +
      '* [15] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.\n' +
      '* [16] Richard P Cooper and Tim Shallice. Hierarchical schemas and goals in the control of sequential behavior. 2006.\n' +
      '* [17] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2625-2634, 2015.\n' +
      '* [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* [19] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. An empirical study of end-to-end video-language transformers with masked visual modeling. _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 22898-22909, 2022.\n' +
      '* [20] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnai, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18995-19012, 2022.\n' +
      '* [21] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.\n' +
      '* [22] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Ziming Zhang, Bret Harsham, John R Hershey, Tim K Marks, and Kazuhiko Sumi. Attention-based multimodal fusion for video description. In _Proceedings of the IEEE international conference on computer vision_, pages 4193-4202, 2017.\n' +
      '* [23] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and Radu Soricut. Multimodal pretraining for dense video captioning. _arXiv preprint arXiv:2011.11760_, 2020.\n' +
      '* [24] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _CoRR_, abs/1412.6980, 2014.\n' +
      '* [25] Atsuhiro Kojima, Takeshi Tamura, and Kunio Fukunaga. Natural language description of human activities from video images based on concept hierarchy of actions. _International Journal of Computer Vision_, 50:171-184, 2002.\n' +
      '* [26] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In _Proceedings of the IEEE international conference on computer vision_, pages 706-715, 2017.\n' +
      '* [27] Weiyu Lan, Xirong Li, and Jianfeng Dong. Fluency-guided cross-lingual image captioning. In _Proceedings of the 25th ACM international conference on Multimedia_, pages 1549-1557, 2017.\n' +
      '* [28] Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara L Berg, and Mohit Bansal. Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning. _arXiv preprint arXiv:2005.05402_, 2020.\n' +
      '* [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [30] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hierarchical encoder for video+language omni-representation pre-training. In _Conference on Empirical Methods in Natural Language Processing_, 2020.\n' +
      '* [31] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pages 74-81, 2004.\n' +
      '* [32] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Z XU, Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentric video-language pretraining. _Advances in Neural Information Processing Systems_, 35:7575-7586, 2022.\n' +
      '* [33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2017.\n' +
      '* [34] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. Univl: A unified video and language pre-training model for multimodal understanding and generation. _arXiv preprint arXiv:2002.06353_, 2020.\n' +
      '* [35] Karttikeya Mangalam, Rajymbek Akshulakov, and Jitendra Malik. Egoschema: A diagnostic benchmark for very long-form video language understanding. _arXiv preprint arXiv:2308.09126_, 2023.\n' +
      '* [36] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. _arXiv preprint arXiv:2111.09734_, 2021.\n' +
      '* [37] Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, and Yueting Zhuang. Hierarchical recurrent neural encoder for video representation with application to captioning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1029-1038, 2016.\n' +
      '* [38] Yingwei Pan, Ting Yao, Houqiang Li, and Tao Mei. Video captioning with transferred semantic attributes. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6504-6512, 2017.\n' +
      '* [39] Wenjie Pei, Jiyuan Zhang, Xiangrong Wang, Lei Ke, Xiaoyong Shen, and Yu-Wing Tai. Memory-attended recurrent network for video captioning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8347-8356, 2019.\n' +
      '* [40] Shraman Pramanick, Yale Song, Sayan Nag, Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou, Rama Chellappa, and Pengchuan Zhang. Egovlpv2: Egocentric video-language pre-training with fusion in the backbone. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5285-5297, 2023.\n' +
      '* [41] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n' +
      '* [42] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville, and Bernt Schiele. Movie description. _International Journal of Computer Vision_, 123:94-120, 2017.\n' +
      '* [43] Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater, Manfred Pinkal, and Bernt Schiele. Translating video content to natural language descriptions. In _Proceedings of the IEEE international conference on computer vision_, pages 433-440, 2013.\n' +
      '* [44] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: Smaller, faster, cheaper and lighter. arxiv 2019. _arXiv preprint arXiv:1910.01108_, 2019.\n' +
      '* [45] Fadime Sener, Dibyadip Chatterjee, Daniel Shelepov, Kun He, Dipika Singhania, Robert Wang, and Angela Yao. Assembly101: A large-scale multi-view video dataset for understanding procedural activities. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 21096-21106, 2022.\n' +
      '\n' +
      '* [46] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid. End-to-end generative pretraining for multimodal video captioning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17959-17968, 2022.\n' +
      '* [47] Jingkuan Song, Yuyu Guo, Lianli Gao, Xuelong Li, Alan Hanjalic, and Heng Tao Shen. From deterministic to generative: Multimodal stochastic rnns for video captioning. _IEEE transactions on neural networks and learning systems_, 30(10):3047-3058, 2018.\n' +
      '* [48] Yale Song, Gene Byrne, Tushar Nagarajan, Huiyu Wang, Miguel Martin, and Lorenzo Torresani. Ego4d goal-step: Toward hierarchical understanding of procedural activities. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.\n' +
      '* [49] Chen Sun and Ram Nevatia. Semantic aware video transcription using random forest classifiers. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13_, pages 772-786. Springer, 2014.\n' +
      '* [50] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 7464-7473, 2019.\n' +
      '* [51] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. _Advances in neural information processing systems_, 27, 2014.\n' +
      '* [52] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: A large-scale dataset for comprehensive instructional video analysis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1207-1216, 2019.\n' +
      '* [53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [54] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4566-4575, 2015.\n' +
      '* [55] Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. Sequence to sequence-video to text. In _Proceedings of the IEEE international conference on computer vision_, pages 4534-4542, 2015.\n' +
      '* [56] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Reconstruction network for video captioning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7622-7631, 2018.\n' +
      '* [57] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, and Lu Yuan. Omnivl: One foundation model for image-language and video-language tasks. _Advances in neural information processing systems_, 35:5696-5710, 2022.\n' +
      '* [58] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4581-4591, 2019.\n' +
      '* [59] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Intervideo: General video foundation models via generative and discriminative learning. _arXiv preprint arXiv:2212.03191_, 2022.\n' +
      '* [60] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5288-5296, 2016.\n' +
      '* [61] Ran Xu, Caiming Xiong, Wei Chen, and Jason Corso. Jointly modeling deep video and compositional text to bridge vision and language in a unified framework. In _Proceedings of the AAAI conference on artificial intelligence_, 2015.\n' +
      '* [62] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. _ArXiv_, abs/2206.08155, 2022.\n' +
      '* [63] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of a visual language model for dense video captioning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10714-10726, 2023.\n' +
      '* [64] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, and Aaron Courville. Describing videos by exploiting temporal structure. In _Proceedings of the IEEE international conference on computer vision_, pages 4507-4515, 2015.\n' +
      '* [65] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Zhang, and Feiyan Huang. mplug-owl: Modularization empowers large language models with multimodality. _ArXiv_, abs/2304.14178, 2023.\n' +
      '* [66] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In _European Conference on Computer Vision_, 2018.\n' +
      '* [67] Yue Zhao, Ishan Misra, Philipp Krahenbuhl, and Rohit Girdhar. Learning video representations from large language models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6586-6597, 2023.\n' +
      '* [68] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2018.\n' +
      '* [69] Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Cross-task weakly supervised learning from instructional videos. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3537-3545, 2019.\n' +
      '\n' +
      '**비디오 리캡: 시간-장 비디오의 재귀적 캡션**\n' +
      '\n' +
      'Supplementary Material\n' +
      '\n' +
      '우리의 보충 자료에는 섹션 S1: 추가 구현 세부 사항, 섹션 S2: Ego4D-HCap 데이터 수집 프로세스, 섹션 S3: 추가 정량적 결과 및 섹션 S4: 정성적 결과가 포함된다.\n' +
      '\n' +
      '## S1 추가 구현 세부사항\n' +
      '\n' +
      '그림 S1은 제안된 비디오 ReCap 모델의 개략도를 보여준다.\n' +
      '\n' +
      '**비디오 인코더.** 비디오 인코더로서 TimeSformer 모델 [10]을 채용한다. 12개의 변압기 층으로 구성된 이 모델은 대조 목적을 사용하여 사전 훈련된다[67]. 인코더의 입력은 4개의 RGB 프레임 크기\\(224\\times 224\\)로 구성된다. 비디오를 처리하기 위해 4초 클립으로 분할하고 미리 훈련된 비디오 인코더를 사용하여 각 클립에 대한 특징을 추출한다. 클립 캡션의 경우, 조밀한 시공간 특징을 이용한다. 이를 통해 모델은 세밀한 세부 정보를 캡처할 수 있습니다. 그러나 우리는 세그먼트 설명 및 비디오 요약에 CLS 기능만 사용하여 효율적인 계산을 가능하게 한다.\n' +
      '\n' +
      '**Video-Language Alignment.** 사전 훈련된 언어 모델 DistilBERT[44]를 Video-Language(VL) Alignment 모듈로 활용한다. 6층 변압기 인코더 모델로서, 자기 주의 블록을 동결하고 각 층 내부에 훈련 가능한 교차 주의 모듈을 삽입한다. 비디오 인코더에 의해 출력된 비디오 피처와 이전 계층 구조에서 생성된 캡션을 입력으로 취한다. 클립 캡션에 대한 텍스트 입력이 없습니다. 세그먼트 설명을 위해 세그먼트의 4초마다 클립 캡션을 추출하고 비디오 요약을 위해 비디오의 3분마다 세그먼트 설명을 추출하여 해당 비디오 특징과 함께 VL 정렬 모듈에 전달한다.\n' +
      '\n' +
      '**텍스트 디코더.** 사전 훈련된 GPT2[41])를 텍스트 디코더로 활용합니다. 12층 변압기 모델이며, 각 변압기 층 내부에 게이트 교차 주의 블록을 삽입합니다. 교차 주의 모듈만 교육하고 나머지 모델은 동결합니다. 각각의 크로스-어텐션 블록은 크로스-어텐션 층 및 피드-포워드 층을 포함하고, 이어서 탠 게이팅[21]이 뒤따른다. tanh-gating은 초기값이 0으로 초기화되어 모델의 출력이 초기에 미리 학습된 LLM과 동일하다. 학습이 진행됨에 따라, 모델은 VL-정렬 모듈에 의해 출력된 비디오-텍스트 임베딩에 참석하도록 점진적으로 학습한다.\n' +
      '\n' +
      '**Video ReCap Model을 훈련.** Video ReCap Model에 대한 3단계 훈련 파이프라인을 따른다. 먼저, 비디오 특징만을 사용하는 클립 자막 데이터를 사용하여 128의 배치 크기를 사용하여 모델 5 에포크를 학습한다. 이후 1단계부터 학습된 모델을 사용하여 4초 간격으로 동영상 내 클립 캡션을 추출한다. 그런 다음, 두 번째 단계에서 비디오 특징과 텍스트 특징(클립 캡션)을 모두 입력으로 하는 세그먼트 설명 샘플을 사용하여 32의 배치 크기를 사용하여 10개의 에폭에 대한 모델을 학습한다. 마지막으로, 세 번째 단계에서는 두 번째 단계의 훈련된 모델을 사용하여 비디오의 3분마다 세그먼트 설명을 추출하고 비디오 요약 데이터를 사용하여 32의 배치 크기를 사용하여 10 에폭에 대한 모델을 추가로 훈련한다. 우리는 \\((\\beta_{1},\\beta_{2})=(0.9,0.999)\\)과 가중치 감쇠 0.01을 갖는 최적화기 [24]와 함께 AdamW 최적화기를 사용하고, \\(3^{-5}\\)의 학습률과 코사인 스케줄링 전략을 사용한다.\n' +
      '\n' +
      '**비디오 ReCap-U 모델을 훈련.** 세 가지 계층에 걸쳐 모든 매개변수를 공유하는 통합 모델을 훈련시키는 것은 더 어렵다. 우리는 몇 가지 추가 트릭과 함께 유사한 3단계 접근법을 사용한다. 특히, 1단계 학습은 Video ReCap 모델과 동일하다. 그러나, 두 번째 단계에서는 클립 캡션의 치명적인 망각을 방지하기 위해 클립 캡션과 세그먼트 설명 샘플을 모두 사용하여 비디오 ReCap-U 모델을 훈련한다. 한 가지 특별한 과제는 클립 캡션과 세그먼트 설명 데이터가 상당히 다르다는 것이다. 클립 캡션은 조밀한 시공간 특징을 사용하는 반면, 세그먼트 설명은 CLS 특징을 사용한다. 더욱이, 세그먼트 설명들은 비디오 및 텍스트 특징들을 입력으로 사용하는 반면, 클립 캡션들은 비디오 특징들만을 사용한다. 이 문제를 극복하기 위해 대체 배치 파이프라인을 사용하며, 여기서 훈련 중에 클립 캡션 및 세그먼트 설명 배치를 대안으로 샘플링한다. 세그먼트 디스크립션(\\(100K\\)에 비해 클립 캡션 데이터(\\(\\sim 4M\\))가 훨씬 많기 때문에 수동으로 주석을 달거나 LLM으로 생성된 의사 주석을 포함하여 무작위로 \\(100K\\) 클립 캡션을 샘플링하고 훈련 2단계 동안만 사용했다. 마지막으로, 유사한 대체 배치 접근법을 사용하여 세 계층 모두의 샘플을 사용하여 세 번째 단계에서 모델을 훈련한다. 우리는 비디오 요약에 대한 \\(\\sim 20K\\)(수동 주석 및 LLM 생성 의사 주석 포함) 샘플만 있기 때문에, 3단계 훈련 동안 무작위로 \\(20K\\) 클립 캡션 및 20K 세그먼트 설명을 샘플링하고 비디오 요약과 함께 사용했다. 이 전략은 모델의 치명적인 망각을 방지합니다. 계층 간에 모든 매개 변수를 공유하는 비디오 ReCap-U 모델의 학습을 허용합니다. 비디오 ReCap-U의 경우 비디오 ReCap에 대해 동일한 학습 속도, 배치 크기, 학습 에포크, 최적화기 및 스케줄러를 사용한다(이전 단락 참조).\n' +
      '\n' +
      '**Inference.** 추론 중에 우리는 해당 클립에서 4개의 프레임을 균일하게 샘플링하고 비디오 인코더를 사용하여 시공간 특징을 추출하여 클립 캡션을 생성하기 위한 입력으로 사용한다. 세그먼트 설명을 위해 우리는 세그먼트의 4초마다 CLS 특징과 클립 캡션을 추출하여 세그먼트 설명을 생성하기 위한 입력으로 사용한다. 마지막으로 비디오의 3분마다 세그먼트 설명을 추출하고 미리 추출된 CLS 특징과 함께 사용하여 비디오 요약을 생성한다. 세그먼트 설명들의 추론 동안 클립 경계들이 주어지지 않고, 비디오 요약들의 추론 동안 세그먼트 경계들이 주어지지 않는다는 것에 유의한다.\n' +
      '\n' +
      '코드, 데이터 및 사전 훈련된 모델을 출시할 것입니다.\n' +
      '\n' +
      '## S2 Ego4D-HCap 데이터 수집 프로세스\n' +
      '\n' +
      'Ego4D-HCap 데이터 세트는 2023년 4월부터 2023년 5월까지, 2023년 9월부터 10월까지 2개월 동안 수집되었으며, 참가자 소싱 회사인 CloudResearch1을 통해 91명의 전문 주석자를 모집했다. 모든 주석자는 미국에 기반을 두고 있으며 시간당 9달러의 비율로 보상되며 이는 국가 최저 임금을 상회한다.\n' +
      '\n' +
      '각주 1: [https://www.cloudresearch.com](https://www.cloudresearch.com)\n' +
      '\n' +
      '우리는 데이터 수집 인터페이스를 구축하기 위해 퀄트릭스와 구글 드라이브를 활용했다. 인터페이스는 프로젝트에 대한 소개, 비디오 요약 지침 및 좋은 요약 예제에서 시작되었습니다. 그런 다음 주석자에게 연결 ID를 요청하고 할당된 비디오 문서에 대한 링크를 제공했다. 각 문서에는 각 비디오의 이벤트를 요약하는 프롬프트 및 GIF와 함께 주석자가 요약할 10-25개의 비디오가 포함됩니다. 마지막 인터페이스에는 주석자가 각 비디오에 대한 텍스트 요약과 데이터 수집 인터페이스에 대한 주석자의 경험을 넣기 위한 텍스트 상자가 포함되어 있습니다. 주석이 달린 요약의 품질이 궁극적으로 더 좋아지도록 인터페이스에서 개선하기 위해 후자를 사용했다. 그림 S2는 우리의 데이터 수집 인터페이스를 보여준다.\n' +
      '\n' +
      '노토네이터의### 지침\n' +
      '\n' +
      '**개요.** 본 프로젝트에서는 긴 동영상을 자동으로 요약할 수 있는 모델을 개발하는 것을 목표로 하고 있습니다. 우리 모델은 3분마다 일어나는 일을 설명하는 각 비디오에 대한 텍스트 캡션을 생성합니다. 해당 캡션을 전체 비디오에 대한 요약으로 요약하려면 귀하의 도움이 필요합니다. 비디오의 총 길이는 10분에서 100분 사이일 수 있습니다.\n' +
      '\n' +
      '**Captions.**\n' +
      '\n' +
      '1. 각 비디오에 대한 캡션 리스트를 부여받는다.\n' +
      '2. 각 캡션은 3분마다 일어나는 일을 설명한다.\n' +
      '3. C는 제공된 캡션 내의 사람을 지칭한다.\n' +
      '4. 자막은 기계 학습 모델을 사용하여 생성되므로, 때때로 순서가 어긋나거나 부정확할 수 있다. 이 경우 요약에서 이치에 맞지 않는 이벤트나 세부 정보를 제외하거나 캡션 아래에 제공된 GIF를 참조할 수 있습니다.\n' +
      '5. 캡션은 또한 동일한 것을 지칭하기 위해 상이한 용어들을 사용할 수 있다. 기술 용어만 사용하는 경우 요약에 사용하십시오. 그렇지 않으면 일반 용어를 사용하는 것이 좋습니다.\n' +
      '\n' +
      '**GIFs.**\n' +
      '\n' +
      '1. 영상이 매우 길기 때문에 전체 영상을 제공하지 않습니다. 대신 각 비디오에 대한 GIF도 제공됩니다.\n' +
      '2. 비디오로부터 희박하게 샘플링된 프레임들에 의해 생성된 GIF들은 캡션들과 함께 비디오의 전체 내용을 더 잘 이해하도록 돕기 위한 것이다.\n' +
      '\n' +
      '**Summaries.**\n' +
      '\n' +
      '1. 요약은 한 단락의 길이가 되어야 한다. 5의 압축 계수를 유지하기 위해, 즉 5개의 캡션마다, 당신은 그것을 1문장으로 요약해야 한다. 그러나, 각각의 요약은 적어도 하나의 문장이어야 한다.\n' +
      '2. 요약은 동영상의 순서대로 이루어지는 설정, 문자, 이벤트 등을 다루어야 한다.\n' +
      '3. X, Y 또는 다른 글자를 사용하여 C 이외의 문자를 지칭하는 것을 피한다. 대신에, 여성과 남자를 사용한다. 다음 페이지의 좋은 요약 예제를 참조하십시오.\n' +
      '4. 요약문에는 인물의 성격이나 자질에 대한 해석이 없어야 한다.\n' +
      '5. 요약은 논리적으로 일관성 있고, 모호하지 않으며, 이해할 수 있어야 한다.\n' +
      '6. 요약은 문법적으로 정확해야 한다.\n' +
      '7. 행동의 반복은 근본적인 목적/패턴을 가져야 한다.\n' +
      '\n' +
      '### Quality Control\n' +
      '\n' +
      '주석의 품질을 제어하기 위해 공식 주석 작업으로 주석을 앞으로 이동하기 전에 미리 주석을 선택하고 수동으로 주석을 검토했다. 공식 주석 작업 전에 171명의 주석자에게 예비 주석 작업을 완료하도록 지불하고 바람직한 주석 품질을 제공한 이 풀 주석자로부터 선택했다. 고품질 주석자를 사전 선정하고 실제 주석 작업과 유사한 인터페이스를 숙지하여 저품질 주석을 얻을 수 있는 가능성을 최소화하였다.\n' +
      '\n' +
      '우리가 활용했던 또 다른 품질 관리 방법은 직접 주석을 검토하는 것이었다. 각 주석자에 대해 그들이 제공한 주석의 절반을 무작위로 샘플링했다. 우리는 섹션 S2.1에 설명된 기대를 따랐는지 여부를 기반으로 품질을 평가했으며 샘플링된 주석의 절반 미만이 품질이 낮으면 주석자 피드백을 제공하고 주석을 다시 작성하도록 요청할 것이다. 주석이 더 나은 품질이라면 초기 주석으로 대체할 것이다. 그렇지 않으면 두 버전을 모두 폐기하고 다른 주석자에게 할당합니다.\n' +
      '\n' +
      '### De-identification Process\n' +
      '\n' +
      '데이터세트와 작업의 특성상 데이터세트는 이미 비식별화되었습니다. 우리의 모든 비디오는 Ego4D에서 조달되기 때문에 민감한 객체 탐지, 거짓 양성 제거, 빠른 음성 수정 및 이미지 흐려짐[20]을 겪었다. 데이터 세트 수집 과정에서 수정되지 않았기 때문에 비디오는 식별되지 않은 상태로 남아 있다. 클라우드 리서치에 대한 주석자를 모집, 관리 및 대응했기 때문에 당사의 주석자도 익명화됩니다. 주석을 수정하기 위해 사용했던 ConnectID 외에도 주석자의 개인 정보를 수집하지 않았습니다.\n' +
      '\n' +
      '### 예제 비디오 요약.\n' +
      '\n' +
      '그림 S3은 Ego4D-HCap 데이터세트의 주석이 달린 비디오 요약의 예를 보여준다. 우리는 비디오 요약이 다양한 길이임을 관찰하고 다양한 시나리오, 장소 및 활동을 캡처한다. 일반적으로 각 비디오에는 여러 요약이 주석 처리되어 있습니다. 그러나 그림은 명확성과 간결성을 위해 비디오당 하나의 요약만 보여준다.\n' +
      '\n' +
      '## S3 추가 정량 결과\n' +
      '\n' +
      '**백본 디자인** 이 섹션에서는 비디오 언어 백본 디자인의 다양한 측면을 삭제합니다. 먼저, 선행 연구 [2, 67]에서 사용된 표준 트랜스포머 리샘플러가 아닌 언어 모델 기반 (LM) [44] 비디오 언어 정렬 모듈의 유효성을 검증한다. 표 S1은 LM 기반 정렬 모듈이 세 계층 모두에서 표준 트랜스포머 기반 리샘플러보다 훨씬 더 잘 수행됨을 보여준다. 둘째, 텍스트 디코더에 훈련 가능한 교차 주의 계층[2, 67]을 주입하여 비디오 특징을 통합한다. 대조적으로, 몇몇 선행 작업들 [29, 36]은 전체 텍스트 디코더를 동결시키면서 입력 계층에만 비디오 피처들을 주입한다. 표 S1은 텍스트 디코더에서 트레이닝가능한 교차-어텐션 계층들을 사용하는 것이 세 계층 레벨 모두에 걸쳐 입력 계층에서만 비디오 특징들을 사용하는 것보다 상당히 더 잘 수행함을 보여준다.\n' +
      '\n' +
      '## S4 EgoSchema에 대한 질적 결과\n' +
      '\n' +
      '그림 S4는 EgoSchema [35] 데이터 세트에 대한 장거리 비디오 질문 응답 실험의 정성적 결과를 보여준다. 섹션 6.2에 자세히 설명된 접근 방식은 비디오에 대한 비디오 ReCap 모델을 사용하여 계층적 캡션을 생성하는 것을 포함한다. 이어서, 이러한 캡션들은 질문들 및 답변 선택들과 함께 챗GPT에 프롬프트들로서 제시되어, 모델이 정답을 선택할 수 있게 한다. 그림 S4(a)와 그림 S4(b)에서 ChatGPT는 클립 캡션만으로 제공되는 경우 오답을 선택하는 경향이 있음을 알 수 있다. 그러나, 모델은 비디오 요약으로 보충될 때 두 시나리오 모두에서 일관되게 올바른 선택을 한다. 이것은 장거리 비디오 질문 응답 작업의 성능을 향상시키는 데 있어 생성된 계층적 캡션의 효율성을 강조한다. 그럼에도 불구하고, 특정 경우에, 그림 S4(c)에 묘사된 바와 같이, 우리의 접근법은 도전에 직면하고 정답을 식별하지 못한다.\n' +
      '\n' +
      '도 S3. **Ego4D-HCap 데이터세트의 주석이 달린 비디오 요약의 예.** 공간 제한 및 간결성으로 인해 우리는 비디오의 5분마다 하나의 프레임을 보여준다.\n' +
      '\n' +
      '도 S4: **Qualitative Results on EgoSchema.** 입력으로서 단거리 클립 캡션만을 사용하는 베이스라인 방법은 예 (a) 및 (b)에서 실패하며, 여기서 우리의 접근법은 계층적 캡션(즉, 클립 캡션 및 비디오 요약)을 활용하여 성공한다. 두 모델 모두 예제 (c)에서 실패합니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>