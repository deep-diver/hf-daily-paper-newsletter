<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'Factized Diffusion Distillation을 이용한 동영상 편집\n' +
      '\n' +
      'Uriel Singer\n' +
      '\n' +
      'Equal contribution.\n' +
      '\n' +
      'Amit Zohar\n' +
      '\n' +
      'Equal contribution.\n' +
      '\n' +
      'Yuval Kirstain\n' +
      '\n' +
      'Shelly Sheynin\n' +
      '\n' +
      'Adam Polyak\n' +
      '\n' +
      'Devi Parikh\n' +
      '\n' +
      'Yaniv Taigman\n' +
      '\n' +
      'Meta AI\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '우리는 감독 비디오 편집 데이터에 의존하지 않고 비디오 편집에서 새로운 최신 기술을 확립하는 모델인 에뮤 비디오 편집(EVE)을 소개한다. EVE를 개발하기 위해 이미지 편집 어댑터와 비디오 생성 어댑터를 별도로 훈련하고 둘 다 동일한 텍스트 대 이미지 모델에 부착한다. 그런 다음 어댑터를 비디오 편집에 맞게 정렬하기 위해 새로운 감독되지 않은 증류 절차인 Factorized Diffusion Distillation을 소개한다. 이 절차는 감독된 데이터 없이 하나 이상의 교사로부터 지식을 동시에 증류한다. 이 절차를 활용하여 EVE가 공동으로 지식을 증류하여 비디오를 편집하도록 교육하여 (i) 이미지 편집 어댑터에서 각 개별 프레임을 정확하게 편집하고 (ii) 비디오 생성 어댑터를 사용하여 편집된 프레임 간의 시간적 일관성을 보장한다. 마지막으로, 다른 기능을 잠금 해제하는 접근법의 잠재력을 입증하기 위해 어댑터의 추가 조합을 정렬한다.\n' +
      '\n' +
      '키워드: 비디오 편집 확산 어댑터 증류\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '온라인 콘텐츠로서 비디오의 사용이 증가함에 따라 텍스트 기반 비디오 편집 기능 개발에 대한 관심이 높아지고 있다. 그러나 감독 비디오 편집 데이터의 부족으로 인해 이러한 기능을 개발하는 것은 어려운 것으로 입증되었다. 이 문제를 해결하기 위해 연구 커뮤니티는 대부분 훈련이 없는 방법[10, 16, 17, 34, 40]에 중점을 두었다. 불행하게도, 지금까지 이러한 방법들은 성능 면에서, 그리고 그들이 제공하는 편집 능력의 범위 모두에서 제한적인 것으로 보인다(도 3).\n' +
      '\n' +
      '따라서, 우리는 임의의 감독 비디오 편집 데이터가 없는 _state-of-the-art_ 비디오 편집 모델을 트레이닝할 수 있는 새로운 접근법을 소개한다. 우리의 접근법의 주요 통찰력은 비디오 편집 모델에서 예상되는 것을 (i) 각각의 개별 프레임을 정확하게 편집하고 (ii) 편집된 프레임 간의 시간적 일관성을 보장하는 두 가지 별개의 기준으로 분리할 수 있다는 것이다.\n' +
      '\n' +
      '이 통찰력을 활용하면 두 단계를 따릅니다. 첫 번째 단계에서는 동일한 냉동 텍스트 대 이미지 모델, 즉 이미지 편집 어댑터 및 비디오 생성 어댑터 위에 두 개의 개별 어댑터를 교육한다. 그런 다음 두 어댑터를 동시에 적용하여 제한된 비디오 편집 기능을 사용할 수 있습니다. 두 번째 단계에서는 모델의 비디오 편집 기능을 획기적으로 개선한 새로운 비감독 정렬 방법인 Factorized Diffusion Distillation(FDD)을 소개한다. FDD는 학생 모델과 하나 이상의 교사 모델을 가정한다. 우리는 어댑터를 교사로 사용하고, 학생에게는 냉동 텍스트 대 이미지 모델 및 어댑터 위에 훈련 가능한 하위 순위 적응(LoRA) 가중치를 사용한다. 각 훈련 반복 FDD에서 학생을 사용하여 편집된 비디오를 생성한다. 그런 다음 생성된 비디오를 사용하여 점수 증류 샘플링[27] 및 적대적 손실[12]을 통해 모든 교사로부터 감독을 제공한다(도 2).\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:2]\n' +
      '\n' +
      '결과 모델인 EVE(Emu Video Edit)는 TGVE(Text Guided Video Editing) 벤치마크에 최신 결과를 설정한다[38]. 또한 TGVE에서 설정한 평가 프로토콜의 두 가지 측면을 개선한다. 먼저, 최근 ViCLIP[35] 모델을 활용하여 시간적으로 인지하는 추가적인 자동 메트릭을 도입한다. 둘째, TGVE 벤치마크를 확장하여 비디오에서 객체의 텍스처를 추가, 제거 및 변경하는 것과 같은 중요한 편집 작업을 용이하게 한다. 중요하게도, EVE는 또한 이러한 추가 편집 작업(탭. 1)을 수행할 때 최첨단 결과를 나타낸다.\n' +
      '\n' +
      '특히, 우리의 접근법은 이론적으로 임의의 확산 기반 어댑터 그룹에 적용될 수 있다. 이것이 실제로 성립하는지 확인하기 위해, 우리는 이미지 편집 어댑터를 다른 LoRA 어댑터와 정렬하여 개인화된 이미지 편집 모델을 개발하는 접근법을 활용한다(도 5).\n' +
      '\n' +
      '요약하면, 본 방법은 이미지 편집 어댑터와 비디오 생성 어댑터를 사용하고, 감독되지 않은 정렬 절차를 사용하여 비디오 편집을 수용할 수 있도록 정렬한다. 결과 모델인 EVE는 다양한 비디오 편집 기능을 제공하면서 최첨단 결과를 비디오 편집으로 보여줍니다. 또한, 추가적인 자동 메트릭을 제안함으로써 비디오 편집을 위한 평가 프로토콜을 확장하고, TGVE 벤치마크를 추가적인 중요한 편집 태스크로 보강한다. 마지막으로, 우리는 우리의 접근법이 다른 어댑터를 정렬하는 데 사용될 수 있음을 확인하고, 따라서 새로운 기능을 잠금 해제할 수 있는 잠재력을 보유한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '감독된 비디오 편집 데이터의 부족은 정확하고 다양한 비디오 편집 모델을 훈련하는 데 큰 과제를 제기한다. 이 문제를 해결하기 위한 일반적인 전략은 훈련이 없는 솔루션을 통한 것이다. 초기 작업은 확률적 차분 편집(SDEdit)의 사용을 제안했다[24]. 이 접근법은 입력 이미지에 노이즈를 추가한 다음 편집 이미지를 설명하는 캡션에서 모델을 컨디셔닝하는 동안 노이즈 제거를 통해 이미지 편집을 수행한다. 최근 루미에르[1]와 SORA[3] 등 여러 영상 기반 모델들이 SDEdit을 영상 편집에 활용하는 사례를 선보였다. 이 접근법은 입력 비디오의 일반적인 구조를 보존할 수 있지만, 입력 비디오에 노이즈를 추가하면 피사체 아이덴티티 및 텍스처와 같은 중요한 정보가 손실된다. 따라서 SDEdit은 이미지의 스타일을 변경하려고 할 때 잘 작동할 수 있지만 설계상 _precise_ 편집에 적합하지 않습니다.\n' +
      '\n' +
      '보다 지배적인 접근법은 교차-어텐션 상호작용들[4, 10, 16, 17, 19, 23, 34, 36, 40, 42]을 통해 키 프레임들로부터 입력 또는 생성된 비디오에 대한 정보를 주입하는 것이다. 또 다른 일반적인 전략은 깊이 맵 또는 광학 흐름과 같이 편집된 비디오에서 지속되어야 하는 특징을 추출하고, 모델을 트레이닝하여 원본 비디오를 사용하면서 잡음제거하는 것이다[8, 20, 39]. 그런 다음, 추론 시간 동안, 입력 비디오의 구조 또는 모션에 대한 충실성을 보장하기 위해 추출된 특징들을 사용하면서 편집 비디오를 예측할 수 있다. 이 전략의 주요 약점은 추출된 피쳐가 지속되어야 하는 정보(예: 손상되지 않은 채로 남아 있어야 하는 영역의 픽셀)가 부족하거나 변경되어야 하는 정보(예: 편집 작업이 비디오에 새로운 모션을 추가해야 하는 경우)를 보유할 수 있다는 것이다. 결과적으로, 편집된 비디오들은 여전히 입력 비디오에 대한 불성실 또는 편집 동작에 시달릴 수 있다.\n' +
      '\n' +
      '레이턴시의 비용으로 입력 비디오에 대한 충실성을 향상시키기 위해, 일부 작업은 [37, 41] 입력 캡션을 사용하여 입력 비디오를 반전시킨다. 그런 다음 반전된 노이즈와 출력된 비디오를 설명한 캡션을 사용하여 새로운 비디오를 생성한다. 또 다른 작품 [5]는 InstructPix2Pix[2]의 일반적인 전략을 동영상 편집에 적응시키며, 이를 통해 합성 데이터를 이용하여 동영상 편집 모델을 생성하고 학습시킬 수 있다. 이 접근법이 효과적인 것처럼 보이지만, 이미지 편집의 최근 작업 [32]는 프롬프트-투-프롬프트 [13]이 다양한 편집 작업에 대해 최적이 아닌 결과를 산출할 수 있음을 보여준다.\n' +
      '\n' +
      '이 논문에서 우리는 이전 작업에서 벗어났다. 대신 이미지 편집 교사와 비디오 생성 교사와 구별되는 비디오 편집 기능을 증류합니다. 적대적 확산 증류(ADD) [30] 손실과 유사하게, 우리의 접근법은 점수 증류 샘플링[27] 손실과 적대적 손실[12]을 결합하는 것을 포함한다. 그러나, 그것은 ADD와 상당히 다르다. 첫째, 본 논문에서 제안하는 방법은 감독되지 않으므로, 감독된 데이터셋을 활용하지 않고 감독에 사용되는 모든 데이터를 생성한다. 둘째, 증류를 사용하여 필요한 확산 단계의 수를 줄이기보다는 새로운 능력을 학습한다. 셋째, 증류 과정을 인수분해하여 이 새로운 능력을 학습하고 그 과정에서 둘 이상의 교사 모델을 활용한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '이 접근법의 핵심 통찰력은 비디오 편집은 (1) 이미지를 정확하게 편집하는 것과 (2) 생성된 프레임 간의 시간적 일관성을 보장하는 두 가지 주요 기능을 필요로 한다는 것이다. Sec. 3.1에서는 각 기능에 대한 전용 어댑터를 개발하는 방법을 자세히 설명합니다. 다음으로, 최종 아키텍처가 어댑터를 결합하여 비디오 편집을 가능하게 하는 방법에 대해 설명합니다. 마지막으로 Sec. 3.2에서는 어댑터를 정렬하는 방법인 FDD( Factorized Diffusion Distillation)를 소개한다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 2 우리는 모델의 아키텍처와 FDD에 대한 삽화를 제공합니다.\n' +
      '\n' +
      '### Architecture\n' +
      '\n' +
      '우리의 아키텍처는 이미지 편집 어댑터와 비디오 생성 어댑터를 동일한 텍스트 대 이미지 백본 위에 적층하는 것을 포함합니다. 잠재 확산 모델인 Emu[7]을 백본 모델로 사용하고, 그 가중치를 \\(\\theta\\)로 나타낸다. 다음으로 비디오 편집을 가능하게 하기 위해 다양한 구성 요소를 개발하고 결합하는 방법에 대해 설명합니다.\n' +
      '\n' +
      '그림 2: 모델 아키텍처 및 정렬 절차. 공유 텍스트 대 이미지 백본 위에 이미지 편집(파란색) 및 비디오 생성(주황색)을 위한 어댑터를 교육합니다. 그런 다음 두 어댑터를 공유 백본(녹색)에 함께 쌓고 두 어댑터를 정렬하여 학생 네트워크를 만듭니다. 학생들은 (i) 각 냉동 교사 어댑터로부터의 점수 증류(SDS로 표시됨), (ii) 각 교사(분홍색으로 표시됨)에 대한 적대적 손실을 사용하여 훈련된다. SDS는 학생이 소음으로부터 생성한 샘플에 대해 계산되며 판별자는 교사와 학생이 생성한 샘플을 구별하려고 시도한다.\n' +
      '\n' +
      '비디오 생성 어댑터는 고정된 Emu 모델 위에 훈련된 시간 계층으로 구성된 텍스트-투-비디오(T2V) 모델인 Emu Video[11]을 사용한다. 따라서 시간적 계층을 비디오 어댑터로 간주한다. 형식적으로, 텍스트-비디오 모델 출력은 \\(\\hat{x}_{\\rho}(x_{s},s,c_{out})\\)로 표시되며, 여기서 \\(\\rho=[\\theta,\\theta_{video}]\\)은 텍스트-이미지 및 비디오 어댑터 가중치이고, \\(x_{s}\\)은 잡음 비디오 샘플이고, \\(s\\)은 타임스텝이고, \\(c_{out}\\)은 출력 비디오 캡션이다.\n' +
      '\n' +
      '이미지 편집 어댑터를 생성하기 위해, Emu 편집을 학습하기 위해 개발된 학습 데이터셋에 매개변수\\(\\theta_{edit}\\)를 갖는 ControlNet[43] 어댑터를 학습시킨다[32]. ControlNet 훈련의 표준 실습을 따르고 텍스트 대 이미지 모델의 아래쪽 및 중간 블록의 사본으로 어댑터를 초기화한다. 학습하는 동안 출력 이미지 캡션에 텍스트 대 이미지 모델을 조건화하고, 컨트롤넷 이미지 편집 어댑터의 입력으로 입력 이미지와 편집 명령어를 사용한다. 따라서, 이미지 편집 모델의 출력은 \\(\\hat{x}_{\\psi}(x_{s},s,c_{out},c_{instruct},c_{img})\\으로 나타낼 수 있으며, 여기서 \\(\\psi=[\\theta,\\theta_{edit}]\\)은 텍스트 대 이미지 및 이미지 편집 어댑터 가중치이며, \\(x_{s}\\)은 노이즈 이미지 샘플, \\(s\\)은 타임스텝, \\(c_{out}\\)은 출력 이미지 캡션, \\(c_{instruct}\\)은 텍스트 편집 명령어, \\(c_{img}\\)은 입력 이미지 편집을 원하는 입력 이미지이다.\n' +
      '\n' +
      '비디오 편집을 가능하게 하기 위해 Adapters를 결합하여 텍스트-이미지 백본 1에 두 어댑터를 동시에 부착한다. 형식적으로는 입력 비디오 \\(c_{vid}\\), 편집 명령 \\(c_{instruct}\\), 출력 비디오 캡션 \\(c_{out}\\)을 사용하여 잡음이 많은 편집 비디오 \\(x_{s}\\)을 잡음제거하는 것을 목표로 한다.\n' +
      '\n' +
      '각주 1: 편집 어댑터가 비디오를 처리할 수 있도록 프레임을 배치로 독립적으로 적층합니다.\n' +
      '\n' +
      '특히, 이미지 편집 어댑터를 단독으로 부착할 때, 결과적인 기능은 각각의 프레임을 독립적으로 처리할 것이다. 따라서, 예측된 비디오 내의 각 프레임은 정확하고 입력 프레임 및 편집 지시에 충실해야 하지만, 편집된 나머지 프레임들에 대해서는 일관성이 결여되어야 한다. 유사하게, 비디오 생성 어댑터를 단독으로 부착할 때, 결과적인 기능은 출력 캡션에 충실하지만, 입력 비디오에 반드시 충실하지는 않은 시간적으로 일관된 비디오를 생성할 것이다.\n' +
      '\n' +
      '두 어댑터를 공유 텍스트-이미지 백본과 결합할 때, 결과 함수는 \\(\\hat{x}_{\\eta}(x_{s},s,c_{out},c_{instruct},c_{vid})\\), 여기서 \\(\\eta=[\\theta,\\theta_{edit},\\theta_{video}]\\이다. 이 공식은 입력에 대해 시간적으로 일관되고 충실한 비디오를 편집할 수 있게 해야 한다. 실제로 우리는 이러한 "플러그 앤 플레이" 접근법이 비디오 편집 기능을 가능하게 하지만 Sec. 4.3에서 보여주듯이 여전히 중요한 아티팩트를 포함한다는 것을 관찰한다.\n' +
      '\n' +
      '최종 아키텍처는 어댑터에 필요한 지식이 이미 존재하기 때문에 작은 정렬로도 충분할 것으로 예상한다. 따라서 본 논문에서는 텍스트-이미지 백본에 대해 저순위 적응(LoRA)[14] 가중치\\(\\theta_{align}\\)을 사용하고, 최종 아키텍처는 \\(\\phi=[\\theta,\\theta_{edit},\\theta_{video},\\theta_{align}]\\)이 된다. 필요한 모든 편집 및 시간 지식은 어댑터에 이미 존재하기 때문에 우리의 목표는 오로지 어댑터를 정렬하는 것입니다. 따라서 텍스트 대 이미지 백본을 통해 매개 변수만 학습하고 어댑터를 냉동 상태로 유지합니다. 우리는 다음 섹션에서 모델의 비디오 편집 품질을 향상시키기 위해 \\(\\theta_{align}\\)을 훈련하는 방법에 대해 설명한다.\n' +
      '\n' +
      '### 인수분해 확산 증류\n' +
      '\n' +
      '비디오 편집 데이터 없이 어댑터를 학습하고 정렬하기 위해 새로운 비감독 증류 절차인 Factorized Diffusion Distillation(FDD)을 제안한다. 이 절차에서 우리는 두 어댑터를 동결하고 공동으로 그들의 지식을 비디오 편집 학생으로 증류한다. 우리의 접근 방식은 감독 데이터를 가정할 수 없기 때문에 입력에 대한 데이터 세트만 수집한다. 데이터 세트의 각 데이터 포인트는 \\(y=(c_{out},c_{instruct},c_{vid})\\)으로 구성되며, 여기서 \\(c_{out}\\)은 출력 비디오 캡션이고, \\(c_{instruct}\\)은 편집 명령이며, \\(c_{vid}\\)은 입력 비디오이다. 우리는 Sec. 4에서 이 데이터 세트에 대한 추가 세부 정보를 제공한다.\n' +
      '\n' +
      'FDD의 각 반복에서, 우리는 먼저 학습자를 사용하여 \\(k\\) 확산 단계에 대한 데이터 포인트 \\(y\\)을 사용하여 편집된 비디오 \\(x^{\\prime}_{0}\\)을 생성한다(자세한 내용은 Sec. 3.3 참조). 중요하게도, 우리는 나중에 이러한 모든 확산 단계를 통해 손실을 역전파할 것이다. 그런 다음 각 교사를 사용하여 점수 증류 샘플링(SDS) [27] 손실을 적용한다. 구체적으로, 우리는 잡음(\\epsilon\\)과 시간 단계(t\\)를 샘플링하여 잡음(x^{\\prime}_{0}\\)을 x^{\\prime}_{t}\\으로 사용한다. 그리고 각 교사들에게 독립적으로 \\(x^{\\prime}_{t}\\)의 잡음을 예측하도록 한다. 교사(\\hat{\\epsilon}\\)의 경우 SDS 손실은 교사의 예측과 \\(\\epsilon\\)의 차이이다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{SDS}}(\\hat{x})=\\mathbb{E}_{x^{\\prime}_{0},t, \\epsilon}[c(t)sg(\\hat{\\epsilon}(x^{\\prime}_{t},t)-\\epsilon)x^{\\prime}_{0}],\\]\n' +
      '\n' +
      '여기서 \\(c(t)\\)는 가중 함수이고 \\(sg\\)는 교사들이 동결된 상태로 유지됨을 나타낸다. 이 메트릭은 학생 세대(x^{\\prime}_{0}\\), 샘플링된 타임스텝(t\\) 및 잡음(\\epsilon\\)에 걸쳐 평균된다. 편집 및 비디오 교사에 플러그를 꽂으면 손실이 발생합니다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{SDS-Edit}=\\mathcal{L}_{\\text{SDS}}(\\hat{x}_{\\psi}),\\mathcal{L}_{\\text{SDS-Video}=\\mathcal{L}_{\\text{SDS}}(\\hat{x}_{\\rho}),\\\n' +
      '\n' +
      '간결함을 위해 입력조건은 \\(\\hat{x}_{\\phi},\\hat{x}_{\\psi},\\hat{x}_{\\rho}에서 생략하였다. 각 교사는 충실하고 정확하게 편집하기 위한 이미지 편집 어댑터, 시간적 일관성을 위한 비디오 생성 어댑터 등 서로 다른 기준에 대한 피드백을 제공한다.\n' +
      '\n' +
      '증류 방법을 사용한 이전 작업[25]과 유사하게 흐릿한 결과를 관찰하므로 적대적 확산 증류(ADD) [31]과 유사한 교사 각각에 대해 추가 적대적 목표[12]를 사용한다. 특히, 우리는 두 명의 차별자를 훈련시킵니다. 첫 번째, \\(D_{e}\\)는 입력 프레임, 명령어 및 출력 프레임을 입력받아 이미지 편집 교사 또는 동영상 편집 학생이 편집을 수행하였는지 여부를 판단하려고 한다. 두 번째, \\(D_{v}\\)는 동영상과 자막을 입력받아 동영상 생성 교사 또는 동영상 편집 학생에 의해 동영상이 생성되었는지 여부를 판단하려고 한다. 우리는 추가로 ADD를 따르고 적대적 훈련을 위해 힌지 손실[21] 목표를 사용한다. 따라서 판별자는 다음과 같은 목적을 최소화한다.\n' +
      '\n' +
      '\\mathcal{L}_{\\text{D-Edit}=\\mathbb{E}_{x^{\\prime}_{\\psi}[\\max(0,1-D_{e}(x^{\\prime}_{\\psi}))]+\\mathbb{E}_{x^{\\prime}_{0}[\\max(0,1+D_{e}(x^{\\prime}_{0}))],\\\\mathcal{L}_{Video}=\\mathbb{E}_{x^{\\prime}_{0}[\\max(0,1-D_{v}(x^{}_{\\rho}))]+\\mathbb{E}_{x^{\\prime}_{0}[\\max(0,1-D_{v}(x^{}_{\\rho}))]\n' +
      '\n' +
      '학생은 다음 목표를 최소화합니다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{G-Edit}}=-\\mathbb{E}_{x^{\\prime}_{0}}[\\max(0,1+D_{e}(x^{\\prime}_{0}))],\\] \\[\\mathcal{L}_{\\text{G-Video}}=-\\mathbb{E}_{x^{\\prime}_{0}}[\\max(0,1+D_{v}(x^{\\prime}_{0}))],\\]\n' +
      '\n' +
      '여기서 \\(x^{\\prime}_{\\psi}\\)와 \\(x^{\\prime}_{\\phi}\\)은 DDIM 샘플링을 이용한 다중 순방향 확산 단계에 따라 이미지 편집 및 비디오 생성 교사들을 적용하여 랜덤 노이즈로부터 생성된 샘플이다.\n' +
      '\n' +
      '우리 학생 모델을 훈련시키기 위한 손실의 합은 다음과 같다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{G-FDD}}=\\alpha(\\mathcal{L}_{\\text{G-Edit}}+\\lambda\\mathcal{L}_{\\text{SDS-Edit}})+\\beta(\\mathcal{L}_{\\text{G-Video}}+\\lambda\\mathcal{L}_{\\text{SDS-Video}}),\\\n' +
      '\n' +
      '상기 판별기들은,\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{D-FDD}}=\\alpha\\mathcal{L}_{\\text{D-Edit}}+ \\beta\\mathcal{L}_{\\text{D-Video}}.\\]\n' +
      '\n' +
      '실제로 \\(\\alpha\\)와 \\(\\beta\\)을 \\(0.5\\)으로 설정하였다. [31]에 이어서 \\(\\lambda\\)을 \\(2.5\\)으로 설정하였다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '우리는 FDD 동안 타임스텝 샘플링과 판별기 아키텍처에 대한 아래 추가 구현 세부 정보를 제공한다.\n' +
      '\n' +
      '###### 3.3.2 K-Bin 확산 샘플링\n' +
      '\n' +
      '앞서 언급했듯이, 학생은 \\(k\\) 확산 단계를 사용하여 편집된 비디오를 생성하고 모든 단계를 통해 손실을 역전파한다. 훈련하는 동안 우리는 메모리에 맞는 최대 확산 단계의 수이므로 \\(k=3\\)을 설정한다. 특히, 훈련 시 동일한 \\(k\\) 타임스테프를 순진하게 사용하고 추론 시간 동안 더 큰 \\(k\\)을 설정하면 열차 테스트 불일치가 발생할 수 있다. 이러한 트레인 테스트 불일치를 피하기 위해 우리는 \\(T\\) 확산 단계를 각각 \\(T/k\\) 단계를 포함하는 \\(k\\) 크기의 빈으로 나눈다. 그런 다음 각 훈련 세대 반복 동안 해당 빈에서 무작위로 단계를 선택한다. 우리는 4.3절에서 이 전략을 폐지한다\n' +
      '\n' +
      '####3.3.3 구분구조\n' +
      '\n' +
      '우리 판별기의 기본 아키텍처는 [31]과 유사하다. 구체적으로, 우리는 DINO[26]을 냉동 피쳐 네트워크로 활용하고 훈련 가능한 헤드를 추가합니다. 입력영상(\\(D_{e}\\)에 대한 컨디셔닝을 추가하기 위해 텍스트 및 노이즈 영상 프로젝션 외에 영상 프로젝션을 사용하고, 조건들을 추가 어텐션 레이어와 결합한다. D_{v}\\(D_{v}\\)에 대한 비디오 컨디셔닝을 지원하기 위해 픽셀당 적용된 DINO의 투영된 특징 위에 단일 시간 주의 계층을 추가한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '우리는 우리의 접근 방식을 평가하고 분석하기 위해 일련의 실험을 수행한다. 우리는 지시된 비디오 편집 작업에 대한 우리의 방법을 평가하는 것으로 시작한다. 구체적으로, 비디오 편집 모델인 EVE(Emu Video Edit)와 TGVE(Text-Guided Video Editing) [38] 벤치마크의 여러 기준선을 벤치마킹한다. 또한 추가 편집 작업을 통해 TGVE를 확장하고 확장된 벤치마크에 대해 모델을 벤치마킹한다. 그런 다음 접근법에 도입한 다양한 설계 선택의 영향을 측정하기 위해 절제 연구를 수행한다. 정렬 단계에서 제시되지 않았지만 편집 어댑터의 지식의 일부인 작업에 대해 제로 샷 비디오 편집을 수행하는 EVE의 능력을 계속 검토한다. 마지막으로, 우리는 우리의 접근법이 어댑터의 다른 조합을 정렬하는 데 적용될 수 있는지 확인하기 위해 정성적 연구를 수행한다. 비디오 편집 예는 그림 1에 나와 있다. 1, 추가 샘플 및 정성적 비교가 추가 자료 및 웹 사이트에서 사용 가능.2\n' +
      '\n' +
      '각주 2: [https://fdd-video-edit.github.io/](https://fdd-video-edit.github.io/]\n' +
      '\n' +
      '#### 4.0.1 Metrics.\n' +
      '\n' +
      '텍스트 기반 비디오 편집의 목표는 입력 텍스트에 따라 비디오를 수정하면서, 변경되지 않아야 하는 비디오의 요소 또는 측면을 보존하는 것이다. 이를 위해 주관적 및 객관적 성공 측정 기준을 기반으로 평가한다. 우리는 평가를 위해 두 가지 범주의 객관적인 메트릭을 활용한다. 첫 번째 카테고리는 TGVE 경쟁에 의해 사용되는 메트릭들을 포함한다 : (i) CLIPFrame(Frame Consistency) [38] - 모든 비디오 프레임들에 걸쳐 CLIP 이미지 임베딩들 사이의 평균 코사인 유사도를 측정하고, (ii) PickScore [18] - 모든 비디오 프레임들에 걸쳐 평균화된 예측된 인간 선호도를 측정한다.\n' +
      '\n' +
      '두 메트릭의 고유한 한계 중 하나는 시간적 일관성에 대한 고려의 부족이다. 예를 들어, CLIPFrame은 이미지들 간의 유사성 점수에 걸쳐 간단한 평균을 적용한다. 결과적으로 CLIPFrame은 움직임이 제한적이거나 없는 정적 비디오를 선호한다. 비디오 편집 방법을 평가할 때 수정이 거의 또는 전혀 없는 비디오를 출력하면 이 메트릭에서 더 높은 점수를 얻을 수 있습니다.\n' +
      '\n' +
      '따라서 비디오 처리 시 시간적 정보를 고려한 비디오 CLIP[28] 모델인 ViCLIP[35]를 이용한 추가 메트릭을 소개한다. 구체적으로, (i) ViCLIP 텍스트-비디오 방향 유사도(ViCLIP\\({}_{dir}\\)와 CLIP\\({}_{dir}\\)[9]) - 캡션 변화와 비디오 변화 사이의 일치도 측정, (ii) ViCLIP 출력 유사도(ViCLIP\\({}_{out}\\)[28]) - 출력 캡션과 편집 이미지 유사성 측정 메트릭을 추가한다.\n' +
      '\n' +
      '주관적 평가를 위해 TGVE[38] 벤치마크를 따르고 인간 평가자에 의존한다. 입력 비디오, 출력 비디오를 설명하는 캡션, 편집된 비디오 두 개를 인간 평가기에 제시한다. 그 다음, 우리는 (i) 텍스트 정렬: 어떤 비디오가 캡션에 더 잘 매칭되는지, (ii) 구조: 어떤 비디오가 입력 비디오의 구조를 더 잘 보존하는지, 그리고 (iii) 품질: 심미적으로 어떤 비디오가 더 나은지 질문들에 응답하도록 평가자들에게 태스크한다. 또한, 세 문항 모두의 선호도 점수를 평균하여 전체적인 인간 평가 점수를 보고한다.\n' +
      '\n' +
      '비감독 데이터세트 4.1.2\n' +
      '\n' +
      '우리의 FDD 접근법은 학생과 교사를 위한 입력이 있는 데이터 세트를 필요로 한다. 비디오 편집의 경우, 각 데이터 포인트는 \\(y=(c_{out},c_{instruct},c_{vid})\\)을 포함하고, 여기서 \\(c_{out}\\)은 출력 비디오 캡션이고, \\(c_{instruct}\\)은 편집 명령이고, \\(c_{vid}\\)은 입력 비디오이다. 이 데이터 세트를 생성하기 위해 우리는 1600개의 비디오가 있는 에뮤 비디오의 고품질 데이터 세트의 비디오를 활용한다. 각 비디오에 대해 [32]와 작업 Llama-2 [33]을 따라 에뮤 편집에서 다음 작업 각각에 대해 하나씩 추가, 제거, 배경, 질감, 로컬, 스타일, 글로벌의 7가지 편집 명령을 생성합니다.\n' +
      '\n' +
      '정렬 중 보이지 않는 편집 작업 4.1.3\n' +
      '\n' +
      '정렬 단계를 통해 이미지 편집 어댑터가 원래 훈련된 작업의 하위 집합에 대해 학생 모델을 훈련한다. 예를 들어, 우리는 학생이 입력 비디오에서 객체를 분할하고, 포즈를 추출하고, 비디오를 스케치로 변환하거나, 입력 비디오로부터 깊이 맵을 도출하도록 훈련시키지 않는다. 그러나, 우리는 정렬 단계에 따라 이러한 과제에 대한 학생 수행의 상당한 향상을 관찰한다. 이것은 우리의 학생 모델이 이 지식의 하위 집합에만 노출되더라도 교사 모델의 전체 지식과 일치함을 시사한다. 그림 1에서 정성적 비교를 제공한다. 도 4는 위에서 언급된 작업들에 대한 정렬 단계의 기여도를 예시한다.\n' +
      '\n' +
      '######4.1.4 훈련 세부사항\n' +
      '\n' +
      '동일한 냉동 Emu 백본을 사용하여 두 어댑터를 훈련하고 훈련 중에 0-말단 SNR[22]을 시행한다. 우리는 워밍업 없이 배치 크기가 64이고 고정 학습률이 1e-5인 총 1500번의 반복에 대해 모델을 훈련한다. 처음 1000번의 반복에 대해 SDS 손실로만 훈련하고 이후 500번의 반복에서 적대적 손실을 추가한다. 본 논문에서는 512\\(\\times\\)512의 해상도로 8개의 프레임 비디오 클립을 학습하고, 이를 통해 DDIM(Denoising Diffusion Implicit Models) 알고리즘을 이용하여 예를 생성한다. [32] 다음 작업 라벨의 편집 어댑터와 [11] 다음 첫 번째 프레임의 비디오 어댑터를 조정합니다. 특히, 편집 어댑터를 사용하여 첫 번째 프레임을 편집합니다. 8 프레임 이상의 동영상을 생성하기 위해 [6]과 동일한 절차를 따르고 입력된 동영상 위에 슬라이딩 윈도우를 적용한다.\n' +
      '\n' +
      '비디오 편집 벤치마크\n' +
      '\n' +
      '현재 TGVE[38] 벤치마크는 텍스트 기반 비디오 편집 방법을 평가하기 위한 확립된 표준이다. 벤치마크에는 76개의 비디오가 포함되어 있으며 각각 4개의 편집 프롬프트가 있습니다. 모든 비디오는 32프레임 또는 128프레임으로 480\\(\\times\\)480의 해상도를 가지며, 벤치마크는 (i) 로컬 객체 수정, (ii) 스타일 변경, (iii) 배경 변경, (iv) 다중 편집 태스크 동시 실행의 네 가지 편집 태스크를 포함한다.\n' +
      '\n' +
      'TGVE의 좁은 범위의 편집 작업에 대한 집중으로 인해 우리는 (i) 객체 제거(제거), (ii) 객체 추가(추가) 및 (iii) 텍스처 변경(텍스처)의 세 가지 새로운 편집 작업을 추가하여 다양성을 높이기로 선택한다. TGVE의 각 비디오와 새로운 편집 작업마다 원하는 출력 비디오를 설명하는 편집 명령과 출력 캡션을 작성하는 작업을 크라우드 워커에게 할당한다. 이 섹션의 나머지 부분에서 TGVE 벤치마크에 대한 방법의 성능과 TGVE+라고 하는 그 확장을 보고한다. 보충 자료에서는 비디오 편집에 대한 지속적인 연구를 지원하기 위해 공개적으로 공개하는 벤치마크의 예를 제공한다.\n' +
      '\n' +
      '### Baseline Comparisons\n' +
      '\n' +
      '우리는 TGVE 벤치마크에서 선두적인 수행자인 InsV2V[6]에 대해 우리의 모델을 벤치마킹한다. 완성도를 위해 최신 모션 전달 작품 중 하나인 STDF(Space-Time Diffusion Features)[42], TGVE 콘테스트에서 베이스라인 역할을 했던 Tune-A-Video(TAV)[37], 보급형 확산 편집 베이스라인인 SDEdit[24], Fairy[36] 등과도 비교한다. SDEdit의 경우, 다중 잡음 수준을 비교하고 자동 측정 기준과 관련하여 가장 좋은 수준을 선택한 후 0.75의 잡음 수준을 사용한다. 모든 참가 방법을 TAV와 비교한 공식 TGVE 콘테스트와 달리 모델을 다른 기준선과 직접 비교한다.\n' +
      '\n' +
      '탭 도 1은 기준선에 대한 우리의 결과를 보여준다. 알 수 있듯이, 인간 평가자는 모든 기준선보다 상당한 차이로 EVE를 선호한다. 또한, 자동 메트릭을 고려할 때, EVE는 CLIPFrame을 제외한 모든 객관적 메트릭에 대한 최신 결과를 제시한다. 흥미롭게도 STDF와 Fairy는 CLIPFrame 메트릭에서 가장 높은 점수를 얻는 반면, 인간 평가자는 각각 72.4%와 71.7%의 시간에서 우리의 모델을 선호한다. 수치 결과 외에, Fig. 도 3은 EVE의 출력과 최상위 수행 기준선 사이의 시각적 비교를 제공한다.\n' +
      '\n' +
      '### Ablations\n' +
      '\n' +
      '우리는 탭에서 인간 등급에 대한 절제 연구를 제공한다. 2는 TGVE+ 벤치마크에 대한 우리의 다양한 기여의 효과를 평가한다. 우리는 정렬 절차 동안 공동으로 학습하기보다는 미리 훈련된 어댑터를 학생 모델에 추가하기로 한 결정을 수정하여 연구를 시작한다. 본 실험(Random Init)에서는 텍스트-이미지 인코더의 가중치로 ControlNet 편집 어댑터를 초기화하고, 시간 계층을 초기화한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l c c c|c c c c} \\hline \\hline Dataset & Method & PickScore\\(\\uparrow\\) & CLIPFrame\\(\\uparrow\\) & ViCLIP\\({}_{dir}\\uparrow\\) & ViCLIP\\({}_{out}\\uparrow\\) & Text & Struct. & Quality & Avg. \\\\ \\hline \\multirow{4}{*}{TGVE} & TAV [37] & 20.36 & 0.924 & 0.162 & 0.243 & 72.4 & 74.0 & 85.2 & 77.2 \\\\  & SDEdit [24] & 20.18 & 0.896 & 0.172 & 0.253 & 75.7 & 67.4 & 79.0 & 74.0 \\\\  & STDF [42] & 20.40 & **0.933** & 0.110 & 0.226 & 81.3 & 65.8 & 70.1 & 72.4 \\\\  & Fairy [36] & 19.80 & **0.933** & 0.164 & 0.208 & 77.3 & 62.8 & 75.0 & 71.7 \\\\  & InsV2V [6] & **20.76** & 0.911 & 0.208 & **0.262** & 79.5 & 55.9 & 65.1 & 59.6 \\\\  & EVE (Ours) & **20.76** & 0.922 & **0.221** & **0.262** & – & – & – & – \\\\ \\hline \\multirow{4}{*}{TGVE+} & TAV [37] & 20.47 & **0.933** & 0.131 & 0.242 & 72.2 & 74.0 & 77.2 & 74.5 \\\\  & SDEdit [24] & 20.35 & 0.899 & 0.144 & 0.246 & 74.5 & 68.5 & 77.9 & 73.6 \\\\ \\cline{1-1}  & STDF [42] & 20.60 & **0.933** & 0.093 & 0.227 & 78.6 & 70.2 & 72.6 & 73.8 \\\\ \\cline{1-1}  & Fairy [36] & 19.81 & **0.933** & 0.140 & 0.197 & 74.4 & 70.8 & 77.8 & 74.3 \\\\ \\cline{1-1}  & InsV2V [6] & 20.37 & 0.925 & 0.174 & 0.236 & 62.9 & 56.4 & 61.4 & 60.2 \\\\ \\cline{1-1}  & EVE (Ours) & **20.88** & 0.926 & **0.198** & **0.251** & – & – & – & – \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: TGVE 및 TGVE+ 벤치마크에 대한 비디오 편집 기준선과의 비교. 우리는 PickScore, CLIPFrame, ViCLIP 메트릭 및 인간 등급을 보고한다. 인간 평가는 EVE의 결과를 선호하는 평가자의 비율을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Method & Text Struct. & Quality Avg. \\\\ \\hline Random Init & 96.7 & 70.1 & 94.7 & 87.2 \\\\ w/o alignment & 77.6 & 91.4 & 89.8 & 86.3 \\\\ w/o SDS & 77.6 & 87.5 & 92.1 & 85.7 \\\\ w/o discriminators & 74.3 & 84.2 & 83.9 & 80.8 \\\\ w/o K-Bin Sampling & 57.6 & 49.7 & 51.6 & 53.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 우리의 다른 기여에 대한 절제 연구. 우리는 TGVE 벤치마크에서 비디오 편집 결과에 대한 인간 등급을 보고한다. 인간 평가는 EVE의 결과를 선호하는 평가자의 비율을 보여준다.\n' +
      '\n' +
      '신원 확인을 위해. 그런 다음 결과 모델 전체를 미세 조정합니다. 우리의 관찰은 이 특정 변형이 비디오 편집 작업에서 숙련도를 획득하는 데 실패했음을 나타내며, 이는 FDD가 처음부터 훈련을 시작하는 것보다 미리 훈련된 어댑터를 정렬하는 데 더 능숙함을 의미한다.\n' +
      '\n' +
      '우리는 (i) 정렬이 없는 (w/o 정렬), (ii) 적대적 손실만을 사용하고 SDS(w/o SDS)를 배제하는 (iii) SDS를 통합하지만 적대적 손실(w/o 구별자)을 배제하는 세 가지 어댑터를 결합하는 방법을 조사함으로써 정렬 절차 자체의 설계를 개선한다. 예상대로 정렬을 사용하지 않으면 구조 보존 및 품질 측면 모두에 대한 결과가 좋지 않다. 이는 서로 다른 작업에 대해 별도로 훈련된 어댑터를 결합할 때 FDD가 필수적임을 시사한다. EVE에서 각 항의 기여도, 즉 SDS와 적대적 손실을 평가할 때 SDS 항은 정렬 과정에 더 큰 영향을 미친다. 흥미롭게도 적대적 용어만 사용하면 어느 정도의 정렬을 달성하기에 충분하다. 그러나 두 용어의 사용은 성공적인 정렬을 위해 필수적이다.\n' +
      '\n' +
      '우리는 K-Bin 확산 샘플링 사용의 기여도를 검증함으로써 결론을 내린다. 이 절제를 위해 우리는 훈련 내내 무작위로 샘플링하지 않고 균일하게 \\(k\\) 단계를 샘플링한다.\n' +
      '\n' +
      '그림 3: 텍스트 가이드 비디오 편집(TGVE) [38] 벤치마크의 예를 사용하여 기본선과 모델의 비교 및 확장.\n' +
      '\n' +
      '양동이. 결과에서 알 수 있듯이 \\(k\\) 빈에서 샘플링 단계를 수행하는 프로세스는 FDD의 성능을 더욱 향상시킨다.\n' +
      '\n' +
      '### 어댑터의 추가 조합\n' +
      '\n' +
      '이 섹션에서는 다른 어댑터를 정렬하는 FDD의 능력을 탐구한다. 이를 위해 텍스트 대 이미지 백본에 4개의 다른 LoRA 어댑터를 훈련한다. 두 개는 피사체 중심 세대[29]이고 두 개는 스타일 중심 세대이다. 그런 다음 개별화되고 양식화된 이미지 편집 기능을 용이하게 하기 위해 각각의 이미지를 이미지 편집 어댑터와 정렬합니다.\n' +
      '\n' +
      '양식화된 편집을 위한 비지도 데이터 세트를 생성하기 위해 Emu Edit의 데이터 세트에서 1,000(입력 캡션, 명령어, 출력 캡션) 트리플렛을 활용한다. 개인화된 편집을 위해 1,000개의 입력 캡션과 작업 Llama-2를 사용하여 피사체를 추가하거나 이미지 내의 아이템을 피사체로 대체하기 위한 명령어를 생성한다. 특히, 훈련 중에 이미지를 사용하지 않고 LoRA 어댑터를 사용하여 입력 이미지를 생성한다. 각 LoRA 어댑터는 다른 정렬이 필요하지만 ReferenceNet[15]과 같은 주제 조절 어댑터를 사용하여 모든 주제 및 스타일에 대해 한 번에 단일 정렬을 수행할 수 있습니다.\n' +
      '\n' +
      '그림 5에서 이러한 조합에 적용할 때 방법의 정성적 예를 제시한다. 각 입력 이미지 및 지침에 대해 (i) 바닐라 에뮤 편집, (ii) 정렬 없이 두 어댑터를 부착하는 것, (iii) 정렬 후 샘플을 표시한다. 예상대로 에뮤 편집은 원하는 주제에 대한 인식이 부족하여 개인화된 편집이 불가능하다. 유사하게, 양식화된 편집의 경우, 입력의 스타일을 유지하는 데 어려움이 있다. "플러그 앤 플레이" 접근법을 사용할 때, 모델은 스타일 또는 주제 아이덴티티를 유지하지 못하거나,\n' +
      '\n' +
      '그림 4: 본 모델은 Emu Edit이 이미지에 대해 실행할 수 있는 작업에 대해 정렬 중에 명시적으로 훈련하지 않고 제로 샷 비디오 편집을 수행합니다.\n' +
      '\n' +
      '그것은 상당한 인공물로 불만족스러운 세대를 생산한다. 그러나 선형화 후에는 편집이 참조 스타일 및 주제와 더 일치하게 됩니다.\n' +
      '\n' +
      '## 5 Limitations\n' +
      '\n' +
      '우리는 우리의 접근법에서 두 가지 주요 한계를 식별한다. 근본적으로, 우리 모델의 성능은 다른 교사 모델의 역량에 의해 상한화된다. 예를 들어, 학생 모델의 편집 기능이 이미지 편집 어댑터 교사의 편집 기능을 초과하지 않을 것으로 예상합니다. 둘째, 사전 학습된 어댑터로 초기화된 학생을 정렬하는 방법을 제안한다. 우리가 Sec. 4.3에서 보여주듯이, 그것을 처음부터 훈련시키는 것은 저조한 성능을 초래한다 따라서 교사는 동결된 텍스트 대 이미지 백본에 대한 어댑터로 훈련되어야 한다. 향후 작업이 이러한 한계를 극복하여 보다 효율적인 훈련과 보다 나은 모델 수행이 가능하기를 바란다.\n' +
      '\n' +
      '## 6 Conclusions\n' +
      '\n' +
      '본 연구에서는 지도 동영상 편집 데이터 없이 동영상을 편집하는 방법을 학습하는 방법을 제안하였다. 제안하는 방법은 비디오 편집을 (1) 비디오 프레임의 정확한 편집과 (2) 편집된 프레임 간의 시간적 일관성을 보장하는 두 가지 별개의 기준으로 인수분해할 수 있다는 핵심 통찰을 기반으로 한다. 이러한 통찰력을 활용하여 비디오 편집 모델을 훈련하기 위한 2단계 접근법을 제안했다. 첫 번째 단계에서는 이미지 편집 어댑터와\n' +
      '\n' +
      '그림 5: FDD를 적용하여 편집 어댑터와 LoRA 기반 어댑터를 결합한다: (1) 반려견용 피사체 구동 어댑터, (2) 장난감 로봇용 피사체 구동 어댑터, (3) 라인 아트를 위한 스타일 구동 어댑터, (4) 스티커용 스타일 구동 어댑터.\n' +
      '\n' +
      '비디오 생성 어댑터 및 둘 다 냉동 텍스트 대 이미지 백본에 부착합니다. 그 후, Factorized Diffusion Distillation (FDD)을 사용하여 비디오 편집을 위해 어댑터를 정렬한다. 본 연구의 결과는 결과 모델인 에뮤 비디오 편집(Emu Video Edit, EVE)이 비디오 편집에서 새로운 최신 기술을 확립한다는 것을 보여준다. 마지막으로, 추가 어댑터를 정렬하여 다른 작업을 학습하는 접근법의 잠재력을 보여준다. 우리의 접근 방식을 다른 작업에 적용하고 일부 한계를 완화할 수 있는 기회는 여전히 많이 있으며 향후 연구 커뮤니티가 이 작업을 어떻게 활용하고 구축할지 기대됩니다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '우리는 그들의 기여(알파벳 순서)에 대해 다음과 같은 사람들에게 감사를 표한다: 앤드류 브라운, 비첸 우, 이산 미스라, 사케스 람바틀라, 샤오량 다이, 지젠 허.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Bar-Tal, O., Chefer, H., Tov, O., Herrmann, C., Paiss, R., Zada, S., Ephrat, A., Hur, J., Li, Y., Michaeli, T., Wang, O., Sun, D., Dekel, T., Mosseri, I.: Lumiere: A space-time diffusion model for video generation. ArXiv **abs/2401.12945** (2024), [https://api.semanticscholar.org/CorpusID:267095113](https://api.semanticscholar.org/CorpusID:267095113)\n' +
      '* [2] Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image editing instructions. In: CVPR (2023)\n' +
      '* [3] Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, E., Ng, C., Wang, R., Ramesh, A.: Video generation models as world simulators (2024), [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)\n' +
      '* [4] Ceylan, D., Huang, C.H.P., Mitra, N.J.: Pix2video: Video editing using image diffusion. 2023 IEEE/CVF International Conference on Computer Vision (ICCV) pp. 23149-23160 (2023), [https://api.semanticscholar.org/CorpusID:257663916](https://api.semanticscholar.org/CorpusID:257663916)\n' +
      '* [5] Cheng, J., Xiao, T., He, T.: Consistent video-to-video transfer using synthetic dataset. ArXiv **abs/2311.00213** (2023), [https://api.semanticscholar.org/CorpusID:264833165](https://api.semanticscholar.org/CorpusID:264833165)\n' +
      '* [6] Cheng, J., Xiao, T., He, T.: Consistent video-to-video transfer using synthetic dataset. In: The Twelfth International Conference on Learning Representations (2024), [https://openreview.net/forum?id=IoKRezZMxF](https://openreview.net/forum?id=IoKRezZMxF)\n' +
      '* [7] Dai, X., Hou, J., Ma, C.Y., Tsai, S., Wang, J., Wang, R., Zhang, P., Vandenhende, S., Wang, X., Dubey, A., Yu, M., Kadian, A., Radenovic, F., Mahajan, D., Li, K., Zhao, Y., Petrovic, V., Singh, M.K., Motwani, S., Wen, Y., Song, Y., Sumbaly, R., Ramanathan, V., He, Z., Vajda, P., Parikh, D.: Emu: Enhancing image generation models using photogenic needles in a haystack (2023)\n' +
      '* [8] Esser, P., Chiu, J., Atighehchian, P., Grasskog, J., Germanidis, A.: Structure and content-guided video synthesis with diffusion models. 2023 IEEE/CVF International Conference on Computer Vision (ICCV) pp. 7312-7322 (2023), [https://api.semanticscholar.org/CorpusID:256615582](https://api.semanticscholar.org/CorpusID:256615582)\n' +
      '* [9] Gal, R., Patashnik, O., Maron, H., Chechik, G., Cohen-Or, D.: Stylegan-nada: Clip-guided domain adaptation of image generators. arXiv preprint arXiv:2108.00946 (2021)\n' +
      '* [10] Geyer, M., Bar-Tal, O., Bagon, S., Dekel, T.: Tokenflow: Consistent diffusion features for consistent video editing. ArXiv **abs/2307.10373** (2023), [https://api.semanticscholar.org/CorpusID:259991741](https://api.semanticscholar.org/CorpusID:259991741)\n' +
      '* [11] Girdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S.S., Shah, A., Yin, X., Parikh, D., Misra, I.: Emu video: Factorizing text-to-video generation by explicit image conditioning (2023)\n' +
      '* [12] Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.C., Bengio, Y.: Generative adversarial networks. 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT) pp. 1-7 (2022), [https://api.semanticscholar.org/CorpusID:1033682](https://api.semanticscholar.org/CorpusID:1033682)\n' +
      '* [13] Hertz, A., Mokady, R., Tenenbaum, J., Aherman, K., Pritch, Y., Cohen-Or, D.: Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626 (2022)* [14] Hu, E.J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: LoRA: Low-rank adaptation of large language models. In: International Conference on Learning Representations (2022), [https://openreview.net/forum?id=n2eVKeeFYf9](https://openreview.net/forum?id=n2eVKeeFYf9)\n' +
      '* [15] Hu, L., Gao, X., Zhang, P., Sun, K., Zhang, B., Bo, L.: Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv preprint arXiv:2311.17117 (2023)\n' +
      '* [16] Kara, O., Kurtkaya, B., Yesiltepe, H., Rehg, J.M., Yanardag, P.: Rave: Randomized noise shuffling for fast and consistent video editing with diffusion models. arXiv preprint arXiv:2312.04524 (2023)\n' +
      '* [17] Khachatryan, L., Movsisyan, A., Tadevosyan, V., Henschel, R., Wang, Z., Navasardyan, S., Shi, H.: Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439 (2023)\n' +
      '* [18] Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna, J., Levy, O.: Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [19] Li, X., Ma, C., Yang, X., Yang, M.H.: Vidtome: Video token merging for zero-shot video editing. arXiv preprint arXiv:2312.10656 (2023)\n' +
      '* [20] Liang, F., Wu, B., Wang, J., Yu, L., Li, K., Zhao, Y., Misra, I., Huang, J.B., Zhang, P., Vajda, P., Marculescu, D.: Flowvid: Taming imperfect optical flows for consistent video-to-video synthesis. ArXiv **abs/2312.17681** (2023), [https://api.semanticscholar.org/CorpusID:266690780](https://api.semanticscholar.org/CorpusID:266690780)\n' +
      '* [21] Lim, J.H., Ye, J.C.: Geometric gan. arXiv preprint arXiv:1705.02894 (2017)\n' +
      '* [22] Lin, S., Liu, B., Li, J., Yang, X.: Common diffusion noise schedules and sample steps are flawed. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 5404-5411 (2024)\n' +
      '* [23] Ma, H., Mahdizadehghadam, S., Wu, B., Fan, Z., Gu, Y., Zhao, W., Shapira, L., Xie, X.: Maskint: Video editing via interpolative non-autoregressive masked transformers. arxiv preprint (2023)\n' +
      '* [24] Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.Y., Ermon, S.: Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073 (2021)\n' +
      '* [25] Meng, C., Rombach, R., Gao, R., Kingma, D., Ermon, S., Ho, J., Salimans, T.: On distillation of guided diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14297-14306 (2023)\n' +
      '* [26] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023)\n' +
      '* [27] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. In: The Eleventh International Conference on Learning Representations (2023), [https://openreview.net/forum?id=FjNys5c7VyY](https://openreview.net/forum?id=FjNys5c7VyY)\n' +
      '* [28] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [29] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22500-22510 (2023)\n' +
      '* [30] Sauer, A., Lorenz, D., Blattmann, A., Rombach, R.: Adversarial diffusion distillation. ArXiv **abs/2311.17042** (2023), [https://api.semanticscholar.org/CorpusID:265466173](https://api.semanticscholar.org/CorpusID:265466173)\n' +
      '* [31] Sauer, A., Lorenz, D., Blattmann, A., Rombach, R.: Adversarial diffusion distillation (2023)\n' +
      '* [32] Sheynin, S., Polyak, A., Singer, U., Kirstain, Y., Zohar, A., Ashual, O., Parikh, D., Taigman, Y.: Emu edit: Precise image editing via recognition and generation tasks (2023)\n' +
      '* [33] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)\n' +
      '* [34] Wang, W., Xie, k., Liu, Z., Chen, H., Cao, Y., Wang, X., Shen, C.: Zero-shot video editing using off-the-shelf image diffusion models. arXiv preprint arXiv:2303.17599 (2023)\n' +
      '* [35] Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Li, X., Chen, G., Chen, X., Wang, Y., et al.: Internvid: A large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942 (2023)* [36] Wu, B., Chuang, C.Y., Wang, X., Jia, Y., Krishnakumar, K., Xiao, T., Liang, F., Yu, L., Vajda, P.: Fairy: Fast parallelized instruction-guided video-to-video synthesis. ArXiv **abs/2312.13834** (2023), [https://api.semanticscholar.org/CorpusID:266435967](https://api.semanticscholar.org/CorpusID:266435967)\n' +
      '* [37] Wu, J.Z., Ge, Y., Wang, X., Lei, S.W., Gu, Y., Shi, Y., Hsu, W., Shan, Y., Qie, X., Shou, M.Z.: Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7623-7633 (2023)\n' +
      '* [38] Wu, J.Z., Li, X., Gao, D., Dong, Z., Bai, J., Singh, A., Xiang, X., Li, Y., Huang, Z., Sun, Y., et al.: Cvpr 2023 text guided video editing competition. arXiv preprint arXiv:2310.16003 (2023)\n' +
      '* [39] Yan, W., Brown, A., Abbeel, P., Girdhar, R., Azadi, S.: Motion-conditioned image animation for video editing. ArXiv **abs/2311.18827** (2023), [https://api.semanticscholar.org/CorpusID:265506378](https://api.semanticscholar.org/CorpusID:265506378)\n' +
      '* [40] Yang, S., Zhou, Y., Liu, Z.,, Loy, C.C.: Render a video: Zero-shot text-guided video-to-video translation. In: ACM SIGGRAPH Asia 2023 Conference Proceedings (2023)\n' +
      '* [41] Yang, S., Mou, C., Yu, J., Wang, Y., Meng, X., Zhang, J.: Neural video fields editing. arXiv preprint arXiv:2312.08882 (2023)\n' +
      '* [42] Yatim, D., Fridman, R., Tal, O.B., Kasten, Y., Dekel, T.: Space-time diffusion features for zero-shot text-driven motion transfer. arXiv preprint arXiv:2311.17009 (2023)\n' +
      '* [43] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 3836-3847 (October 2023)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>