<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '분석. 그러나 이러한 기술은 대규모 데이터 세트를 처리하고 고효율 스토리지 솔루션을 위해 노력할 때 상당한 장애물에 직면한다.\n' +
      '\n' +
      '암시적 신경 표현(INR) [49; 51]의 출현은 이미지 표현 기술의 중요한 패러다임 변화를 나타낸다. 전형적으로, INR들은 입력 좌표들로부터 대응하는 출력 값들로의 암시적 연속 매핑을 유도하기 위해 콤팩트 뉴럴 네트워크를 채용한다. 이것은 INR들이 더 큰 효율로 이미지 세부사항들을 캡처하고 유지할 수 있게 하며, 이는 이미지 압축[21; 22; 52; 25], 디블러링[45; 60; 62], 및 초해상도[14; 39; 44]를 포함한 다양한 애플리케이션들에 걸쳐 상당한 이점들을 제공한다. 그러나, 대부분의 최신 INR 방법들[23; 46; 48; 54; 49]은 고해상도 이미지들을 정확하게 나타내기 위해 큰 고차원 다층 퍼셉트론(MLP) 네트워크에 의존한다. 이러한 의존성은 트레이닝 시간의 연장, GPU 메모리 요구 사항의 증가 및 느린 디코딩 속도로 이어진다. 최근의 혁신[17; 40; 43; 53]은 훈련 및 추론을 가속화하기 위해 컴팩트 MLP와 결합된 다중 해상도 특징 그리드를 도입했지만, 자원이 제한될 때 충족하기 어려운 빠른 훈련 및 추론을 지원하기 위해 여전히 충분한 GPU 메모리를 필요로 한다. 결과적으로, 이러한 과제는 실제 시나리오에서 INR의 실제 배치를 실질적으로 방해한다.\n' +
      '\n' +
      '이러한 문제에 비추어, 본 연구는 효율적인 훈련, 친숙한 GPU 메모리 사용 및 빠른 디코딩을 가능하게 하는 고급 이미지 표현 기술을 개발하는 것을 목표로 한다. 이 목적을 달성하기 위해, 우리는 최근에 3D 장면 재구성을 위해 개발된 가우스 스플래팅(Gaussian Splatting, GS) [31]에 의존한다. 명시적 3D 가우시안 표현과 차별화 가능한 타일 기반 래스터화를 활용하여 3D GS는 경쟁적인 훈련 시간과 함께 높은 시각적 품질을 즐길 뿐만 아니라 실시간 렌더링 기능을 달성한다.\n' +
      '\n' +
      '그럼에도 불구하고 효율적인 단일 이미지 표현을 위해 3D GS를 직접 적용하는 것은 간단하지 않다. **First**, 기존의 3D GS 방법들 [11; 31]이 상이한 관점들의 이미지들을 렌더링하기 위해 다양한 카메라 변환 매트릭스들에 의존한다는 점을 고려하면, 단일 이미지에 대한 직접적인 적응은 단일 이미지에 대한 직접적인 적응을 고정시키고 있다.\n' +
      '\n' +
      '도 1: 코닥 및 DIV2K 데이터세트에서 각각 상이한 디코딩 시간을 갖는 이미지 표현(좌측) 및 압축(우측) 결과. 각 점의 반지름은 매개변수 크기(왼쪽) 또는 픽셀당 비트(오른쪽)를 나타냅니다. 제안하는 방법은 파라미터 크기나 bpp에 관계없이 가장 빠른 복호 속도를 가진다.\n' +
      '\n' +
      '단일 시야각에서 이미지를 렌더링하는 카메라 변환 행렬입니다. 불행하게도, 각각의 3D 가우시안들은 보통 59개의 학습 가능한 파라미터[31]를 포함하고, 수천 개의 3D 가우시안들이 단일 이미지를 표현하는데 요구된다. 이러한 순진한 접근 방식은 스토리지 및 통신 요구를 상당히 증가시킨다. 표 1로부터 추론될 수 있는 바와 같이, 수십 킬로바이트를 갖는 단일 이미지에 대한 저장 풋프린트는 수십 메가바이트로 증가할 수 있고, 이는 제한된 메모리를 갖는 저-엔드 디바이스들에서 렌더링을 어렵게 한다. 둘째, 3D GS의 래스터화 알고리즘 [31]은 \\(\\alpha\\)-블렌딩 근사화를 위해 설계되었으며, 카메라 파라미터로부터 유도된 깊이 정보를 기반으로 미리 분류된 가우시안들을 필요로 한다. 이는 상세한 카메라 파라미터가 종종 자연 개별 이미지에서 알려지지 않는 반면, 스크린샷 및 AI 생성 콘텐츠를 포함하는 비자연적 이미지는 카메라에 의해 캡처되지 않기 때문에 단일 이미지에 대한 도전을 제기한다. 정확한 깊이 정보가 없으면 가우시안 정렬이 손상되어 최종 피팅 성능이 저하될 수 있다. 더욱이, 현재 래스터화 과정은 누적된 불투명도가 주어진 임계값을 초과하면 나머지 가우시안들을 건너뛰고, 이는 가우시안 데이터의 과소 활용을 초래하여 고품질 렌더링을 위해 더 많은 가우시안들을 필요로 한다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해 본 논문에서는 2차원 가우시안 분할을 이용한 영상 표현과 압축, 즉 가우시안 영상의 새로운 패러다임을 제안한다. 먼저, 우리는 간결하고 표현적인 표현을 위해 3D 대신에 2D 가우시안들을 채택한다. 각각의 2D 가우시안들은 4개의 속성들(총 9개의 파라미터들): 위치, 이방성 공분산, 컬러 계수들 및 불투명도에 의해 정의된다. 이러한 수정은 등가 가우시안 점들을 갖는 3차원 가우시안들에 대한 \\(6.5\\times\\)의 압축을 초래하여 가우시안 표현의 저장 요구를 상당히 완화시킨다. 다음으로 깊이 기반 가우시안 정렬과 \\(\\alpha\\)-블렌딩을 누적 합산 과정으로 대체하는 독특한 래스터화 알고리즘을 제안한다. 이 새로운 접근법은 2D 가우시안들의 가중합으로부터 각 픽셀의 컬러를 직접 계산하는데, 이는 피팅 성능을 향상시키기 위해 현재 픽셀을 커버하는 모든 가우시안 포인트들의 정보를 충분히 활용할 뿐만 아니라 트레이닝 및 추론 속도를 가속화하기 위해 누적된 투명도의 지루한 계산을 회피한다. 더 중요한 것은, 이러한 합산 메커니즘을 통해 컬러 계수 및 불투명도를 가중된 컬러 계수들의 단일 집합으로 병합할 수 있고, 파라미터 카운트를 8로 감소시키고 압축비를 \\(7.375\\times\\)으로 더욱 개선할 수 있다는 것이다. 마지막으로, 2차원 가우시안 표현을 실제 영상 코덱으로 변환한다. 가우시안 속성 압축 태스크로서 프레임 이미지 압축은 속성 양자화 인식 미세 조정 및 인코딩이라는 2단계 압축 전략을 사용한다. 16비트 플로팅 양자화, 6비트 정수 양자화[10], 잔여 벡터 양자화(RVQ)[67]을 각각 위치, 공분산 파라미터, 가중 컬러 계수에 적용한 후 엔트로피 코딩을 수행하여 양자화된 값들 간의 통계적 상관관계를 제거함으로써 2차원 가우시안 분할 기반의 첫 번째 영상 코덱을 성공적으로 개발하였다. 개념의 예비 증명으로서, 부분 비트-백 코딩[47, 55]은 선택적으로 우리 코덱의 압축 성능을 더욱 향상시키기 위해 사용된다. 전반적으로, 우리의 기여는 3배이다:* 우리는 2D 가우시안 스플래팅에 의한 이미지 표현 및 압축의 새로운 패러다임을 제시한다. 컴팩트한 2D 가우시안 표현과 새로운 래스터화 방법을 사용하여 짧은 훈련 기간, 최소 GPU 메모리 오버헤드, 2000 FPS 렌더링 속도로 높은 표현 성능을 달성한다.\n' +
      '* 벡터 양자화를 이용한 저복잡도 신경 영상 코덱을 개발한다. 나아가, 비트 레이트를 감소시키기 위해 부분 비트-백 코딩 기술이 선택적으로 사용된다.\n' +
      '* 실험 결과는 기존의 INR 방법과 비교할 때, 우리의 접근법이 유사한 시각적 품질을 유지하면서 GPU 메모리 사용량을 줄이면서 현저한 훈련 및 추론 가속을 달성함을 보여준다. 효율적인 영상 코덱으로 사용될 때, 본 논문에서 제안하는 방법은 COIN[22]와 COIN++[21]과 경쟁적으로 압축 성능을 달성한다. 포괄적인 절제 및 분석은 제안된 각 구성 요소의 효과를 보여준다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '### 암시적 신경 표현\n' +
      '\n' +
      '최근, 암시적 신경 표현은 3D 장면 렌더링[7, 8, 41, 61], 이미지[43, 48, 17] 및 비디오[12, 13, 36, 68] 표현과 같은 광범위한 잠재적 응용 분야에서 점점 더 주목을 받고 있다. 기존의 영상 INR을 크게 두 가지로 분류하였다. (i) MLP 기반 INR [46, 48, 49, 54, 23]은 영상의 RGB 값을 학습하기 위해 MLP 네트워크의 입력으로 공간 좌표의 위치 인코딩을 취하는 반면, 모든 영상 정보를 인코딩하기 위해 신경망에만 의존하여 특히 고해상도 영상에 대한 비효율적인 학습 및 추론을 초래한다. (ii) 특징 그리드 기반 INR[43, 40, 17, 53]은 컴팩트한 MLP를 위한 사전 정보를 제공하기 위해 쿼드트리 및 해시 테이블과 같은 대규모 다중 해상도 그리드를 채택한다. 이는 MLP의 학습 난이도를 어느 정도 감소시키고 훈련 과정을 가속화시켜 INR을 보다 실용적으로 만든다. 불행히도, 그들은 여전히 대형 GPU 메모리를 소비하는데, 이는 보급형 장치에 수용하기 어렵다. 기존의 INR 방법을 따르는 대신, 우리는 스위프터 트레이닝, 더 빠른 렌더링 및 더 적은 GPU 리소스 소비를 즐길 수 있는 2D 가우시안 스플래팅 기반의 새로운 이미지 표현 패러다임을 제안하는 것을 목표로 한다.\n' +
      '\n' +
      '### Gaussian Splatting\n' +
      '\n' +
      '가우시안 스플래팅[31]은 최근 3D 뷰 합성에 대한 유망한 패러다임으로 엄청난 주목을 받고 있다. 명시적 3D 가우스 표현과 미분 가능한 타일 기반 래스터화를 통해 GS는 전례 없는 제어 및 편집성을 가져올 뿐만 아니라 3D 장면 재구성에서 고품질 및 실시간 렌더링을 용이하게 한다. 이러한 다양성은 SLAM(simultaneous localization and mapping)[29, 30, 63], 동적 장면 모델링[38, 58, 65], AI 생성 콘텐츠[15, 18, 70], 자율 주행[64, 69] 등 다양한 영역에서 새로운 길을 열었다. 3D 시나리오에서 큰 성공에도 불구하고 단일 이미지 표현에 GS를 적용하는 것은 아직 탐구되지 않았다. 본 연구는 2D 이미지 표현을 위한 GS의 적응을 개척하며, 고도로 병렬화된 워크플로우와 실시간 렌더링에서 GS의 장점을 활용하여 훈련 효율과 디코딩 속도 측면에서 INR 기반 방법을 능가한다.\n' +
      '\n' +
      '### Image Compression\n' +
      '\n' +
      'JPEG[56], JPEG2000[50] 및 BPG[9]와 같은 전통적인 이미지 압축 기술은 변환, 양자화 및 엔트로피 코딩 절차를 따라 상당한 압축 해제 속도로 양호한 압축 효율을 달성한다. 최근, Variational Auto-encoder(VAE)에 기반한 학습 기반 영상 압축 방법은 복잡한 비선형 변환[4, 19, 26, 37]과 진보된 엔트로피 모델[5, 6, 32, 42]을 통합하여 이 파이프라인을 재상상하였다. Rate-distortion (RD) 성능에서 전통적인 코덱을 능가하는 이러한 방법에도 불구하고, 그들의 매우 높은 계산 복잡도와 매우 느린 디코딩 속도는 그들의 실제적인 배치를 심각하게 제한한다. 기존 예술의 계산적 비효율성을 해결하기 위해, 일부 작품들은 INR 기반 압축 방법[21, 22, 25, 34, 35, 52]을 탐구했다. 그러나 이미지 해상도가 올라가면서 디코딩 속도가 급격히 느려져 실제 적용 가능성에 도전한다. 본 논문에서는 2차원 가우시안 스플래팅(Gaussian Splatting)을 이용하여 VAE와 INR 패러다임으로부터 분기하여 사상 초유의 복호 효율을 갖는 신경 영상 코덱을 위조한다. 이것은 신경 이미지 코덱에 중요한 이정표를 표시한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '도. 2는 가우시안 이미지의 전체 처리 파이프라인을 설명한다. 제안된 방법은 주로 2차원 공분산 행렬(\\mathbf{\\Sigma}\\)을 계산하는 과정인 이미지 평면에 2차원 가우시안들을 형성하는 것으로 시작된다. 그 후, 각 픽셀에 대한 값을 계산하기 위해 누적 혼합 메커니즘을 사용한다. 다음으로 3.1절에서 2D 가우시안 형성을 시작으로, 3D GS의 래스터화 과정을 적용하여 2D 영상 표현의 고유한 특성을 정렬하고 3.2절에서 파라미터가 적은 2D 가우시안 업그레이드를 하는 방법을 설명하고, 3.3절에서 가우시안 영상을 신경 영상 코덱으로 변환하기 위한 2단계 압축 파이프라인을 제시하고, 마지막으로 영상 표현 및 컴프레에 대한 훈련 과정을 기술한다.\n' +
      '\n' +
      '그림 2: 제안된 가우시안 이미지 프레임워크. 2D 가우시안들은 먼저 포맷된 후 래스터화되어 출력 이미지를 생성한다. 래스터라이저는 효율적인 2D 이미지 표현을 위해 제안된 누적 블렌딩을 사용한다.\n' +
      '\n' +
      '#####2D Gaussian Formation\n' +
      '\n' +
      '3D 가우시안 스플래팅에서, 각각의 3D 가우시안들은 뷰잉 및 프로젝션 변환을 통해 초기에 2D 평면으로 맵핑된다. 그런 다음 미분 가능한 래스터라이저를 사용하여 이러한 투영된 가우시안으로부터 현재 뷰 이미지를 렌더링한다. 우리의 응용 프로그램은 더 이상 3D 장면을 지향하지 않고 2D 이미지 표현을 지향하기 때문에 프로젝트 변환, 구형 고조파 등과 같은 3D GS에서 많은 팽창된 연산 및 중복 매개변수를 폐기한다.\n' +
      '\n' +
      '제안하는 프레임워크에서 영상 표현 단위는 2차원 가우시안이다. 기본적인 2차원 가우시안은 위치\\(\\mathbf{\\mu}\\in\\mathbb{R}^{2}\\), 2차원 공분산 행렬\\(\\mathbf{\\Sigma}\\in\\mathbb{R}^{2\\times 2}\\), 색계수\\(\\mathbf{c}\\in\\mathbb{R}^{3}\\) 및 불투명도\\(o\\in\\mathbb{R}\\)으로 설명된다. 가우시안 분포의 공분산 행렬\\(\\mathbf{\\Sigma}\\)은 양의 반정수를 필요로 한다는 점에 유의한다. 전형적으로, 그러한 유효한 매트릭스들을 생성하기 위해 경사 하강을 사용하여 학습가능한 파라미터들을 제약하는 것은 어렵다. 학습 중에 잘못된 행렬을 생성하는 것을 방지하기 위해 공분산 행렬의 인수분해된 형태를 최적화하도록 선택한다. 여기서는 원래의 공분산 행렬의 모든 정보를 포괄하는 두 가지 분해 방법을 제시한다. 한 가지 직관적인 분해는 촐레스키 인수분해[28]이며, 이는 아래 삼각형 행렬\\(\\mathbf{L}\\in\\mathbb{R}^{2\\times 2}\\)과 공액 전치 \\(\\mathbf{L}^{T}\\)의 곱으로 분해된다.\n' +
      '\n' +
      '\\[\\mathbf{\\Sigma}=\\mathbf{L}\\mathbf{L}^{T}. \\tag{1}\\]\n' +
      '\n' +
      '글쓰기를 위해, 우리는 Choleksy 벡터 \\(\\mathbf{l}=\\{l_{1},l_{2},l_{3}\\}\\)을 사용하여 행렬 \\(\\mathbf{L}\\)의 하부 삼각 요소를 표현한다. 59개의 학습 가능한 파라미터를 갖는 3D 가우시안과 비교할 때, 우리의 2D 가우시안에는 9개의 파라미터만 필요하므로 더 가볍고 이미지 표현에 적합하다.\n' +
      '\n' +
      '다른 분해는 공분산 행렬을 회전 행렬\\(\\mathbf{R}\\in\\mathbb{R}^{2\\times 2}\\)과 스케일링 행렬\\(\\mathbf{S}\\in\\mathbb{R}^{2\\times 2}\\)으로 인수분해하기 위해 3D GS[31]을 따른다:\n' +
      '\n' +
      '\\[\\mathbf{\\Sigma}=(\\mathbf{R}\\mathbf{S})(\\mathbf{R}\\mathbf{S})^{T}, \\tag{2}\\]\n' +
      '\n' +
      '여기서 회전 행렬 \\(\\mathbf{R}\\) 및 스케일링 행렬 \\(\\mathbf{S}\\)은 다음과 같이 표현된다.\n' +
      '\n' +
      '\\begin{bmatrix}\\cos(\\theta)&-\\sin(\\theta)\\\\sin(\\theta)&\\cos(\\theta)\\end{bmatrix},\\quad\\mathbf{S}=\\begin{bmatrix}s_{1}&0\\\\0&s_{2}\\end{bmatrix}. \\tag{3}\\heta\n' +
      '\n' +
      '여기서, \\(\\theta\\)는 회전각을 나타낸다. \\ (s_{1}\\)과 \\(s_{2}\\)은 서로 다른 고유벡터 방향의 스케일링 인자이다. 공분산 행렬의 분해는 고유하지 않지만 이미지를 나타내는 동등한 능력을 가지고 있다. 그러나, 상이한 분해 형태들의 압축에 대한 견고성은 일관성이 없으며, 이는 부록에서 상세히 설명된다. 따라서 서로 다른 영상 작업에 직면할 때 공분산 행렬의 분해 형태를 신중하게 선택할 필요가 있다.\n' +
      '\n' +
      '### 누적 혼합 기반 래스터화\n' +
      '\n' +
      '래스터화 단계 동안, 3D GS는 먼저 투영된 깊이 정보에 기초하여 Gaussians \\(\\mathcal{N}\\)의 정렬된 리스트를 형성한다. 그 다음 \\(\\알파\\)-블렌딩은 토렌더 픽셀 \\(i\\):\n' +
      '\n' +
      '\\sum_{n\\in\\mathcal{N}\\mathbff{c}_{n}\\cdot\\alpha_{n}\\cdot T_{n},\\quad T_{n}=\\prod_{m=1}^{n-1}(1-\\alpha_{m}), \\tag{4}\\\n' +
      '\n' +
      '여기서 \\(T_{n}\\)은 누적된 투명도를 나타낸다. \\(\\alpha_{n}\\)은 투영된 2D 공분산\\(\\mathbf{\\Sigma}\\)과 불투명도\\(o_{n}\\)으로 계산된다:\n' +
      '\n' +
      '\\[\\alpha_{n}=o_{n}\\cdot\\exp(-\\sigma_{n}),\\quad\\sigma_{n}=\\frac{1}{2}\\mathbf{n}^{T}\\mathbf{\\Sigma}^{-1}\\mathbf{d}_{n}, \\tag{5}\\tag{n}\n' +
      '\n' +
      '여기서 \\(\\mathbf{d}\\in\\mathbb{R}^{2}\\)는 픽셀 중심과 투영된 2D 가우시안 중심 사이의 변위이다.\n' +
      '\n' +
      '깊이 정보의 획득은 뷰잉 변환을 수반하기 때문에 카메라의 내재적 파라미터와 외재적 파라미터를 미리 알아야 한다. 그러나, 스크린샷 및 AI 생성 콘텐츠와 같은 비자연적 이미지는 카메라에 의해 캡처되지 않는 반면, 자연적 개별 이미지는 상세한 카메라 파라미터에 액세스하기 어렵다. 이 경우, 3D GS의 깊이 큐 없이 \\(\\alpha\\)-블렌딩을 유지하는 것은 임의의 블렌딩 시퀀스를 초래하여 렌더링 품질을 저하시킨다. 또한, 3차원 GS는 투영된 2차원 공분산을 계산하는 데 있어 수치적 불안정성의 문제를 해결하기 위해 99% 신뢰구간을 갖는 가우시안만을 유지하지만, 이는 화소\\(i\\)을 커버하는 가우시안 중 일부만을 화소\\(i\\)의 렌더링에 기여하게 하여 피팅 성능이 떨어지게 한다.\n' +
      '\n' +
      '이러한 한계를 극복하기 위해, 우리는 2D 가우시안 표현의 잠재력을 발휘하기 위한 누적 합산 메커니즘을 제안한다. 영상을 렌더링할 때 시점의 영향이 없기 때문에 각 요소에서 관측하는 광선들이 결정되어 모든 \\(\\alpha\\) 값이 결정된다. 따라서, 우리는 수학식 4의 \\(T_{n}\\) 부분을 \\(o_{n}\\) 항으로 병합하고, 가중치 합에 \\(\\알파\\)-블렌딩하는 계산을 단순화한다:\n' +
      '\n' +
      '\\mathbf{C}_{i}=\\sum_{n\\in\\mathcal{N}\\mathbf{c}_{n}\\cdot\\alpha_{n}=\\sum_{n\\in\\mathcal{N}\\mathbf{c}_{n}\\cdot o_{n}\\cdot\\exp(-\\sigma_{n}). \\tag{6}\\m\n' +
      '\n' +
      '이는 가우시안 시퀀스 순서의 필요성을 제거하여 래스터화에서 정렬을 제거할 수 있다.\n' +
      '\n' +
      '이 새로운 래스터화 알고리즘은 여러 가지 이점을 가져온다. 먼저, 누적 혼합 과정은 가우시안 점들의 순서에 둔감하다. 이 성질은 가우스 점들의 랜덤 순서가 렌더링에 미치는 영향을 피할 수 있게 하여 가우스 점들의 임의의 순서에 대한 견고성을 달성한다. 둘째, 수학식 4와 비교할 때, 우리의 렌더링은 누적된 투명도 \\(T_{n}\\)의 지루한 순차적 계산을 건너뛰어 학습 효율과 렌더링 속도를 향상시킨다. 셋째, 색 계수 \\(\\mathbf{c}_{n}\\)와 불투명도 \\(o_{n}\\)는 학습 가능한 파라미터이므로, 이들을 병합하여 수학식 6을 더욱 단순화할 수 있다:\n' +
      '\n' +
      '\\[\\mathbf{C}_{i}=\\sum_{n\\in\\mathcal{N}\\mathbf{c}_{n}^{\\prime}\\cdot\\exp(-\\sigma_{n}), \\tag{7}\\] 여기서 가중된 색상 계수 \\(\\mathbf{c}_{n}^{\\prime}\\in\\mathbb{R}^{3}\\)는 더 이상 \\([0,1]\\)의 범위에서 제한되지 않는다. 이러한 방식으로 섹션 3.1에서 4개의 속성이 필요한 기본 2D 가우시안 대신 업그레이드된 2D 가우시안에는 총 8개의 매개변수가 있는 3개의 속성(즉, 위치, 공분산 및 가중 색상 계수)만 설명되어 있다. 이는 등가 가우시안 점 하에서 3차원 가우시안과 비교할 때 압축률이 \\(7.375\\times\\)으로 더욱 향상된다.\n' +
      '\n' +
      '### Compression Pipeline\n' +
      '\n' +
      '영상을 과적합한 후 가우시안 영상을 이용한 영상 압축을 위한 압축 파이프라인을 제안한다. 도 1에 도시된 바와 같다. 도 3을 참조하면, 표준 압축 파이프라인은 이미지 오버피팅, 속성 양자화 인식 미세 조정, 속성 인코딩의 세 단계로 구성된다. 최상의 압축 성능을 달성하기 위해, 부분 비트-백 코딩[47, 55]은 선택적인 전략이다. 여기서는 촐레스키 인수분해에 기반한 가우시안 이미지를 예로 들어 압축 과정을 설명한다.\n' +
      '\n' +
      '**Attribute Quantization-aware Fine-tuning.** 2D Gaussian point 집합이 영상에 적합할 경우, 다양한 속성에 대해 뚜렷한 양자화 전략을 적용한다. 가우시안 위치는 양자화에 민감하기 때문에, 재구성 충실도를 보존하기 위해 위치 파라미터에 대해 16 비트 플로트 정밀도를 채택한다. \\(n\\)번째 가우시안에서의 Choleksy 벡터 \\(\\mathbf{l}_{n}\\)에 대해, 우리는 \\(b\\)비트 비대칭 양자화 기법 [10]을 통합하며, 여기서 스케일링 팩터 \\(\\gamma_{i}\\)과 오프셋 팩터 \\(\\beta_{i}\\)은 모두 미세 조정 동안 학습된다:\n' +
      '\n' +
      '\\frac{l}_{i}^{i}^{n}=\\left\\lfloor\\text{clamp}\\left(\\frac{l}_{i}^{n}-\\beta_{i}}{ \\gamma_{i}},0,2^{b}-1\\right}\\right\\rfloor,\\quad\\bar{l}_{i}^{n}=\\hat{l}_{i}^{n}\\times\\gamma_{i}+\\beta_{i}, \\tag{8}\\gamma_{i}}\\gamma_{i}},\\quad\\bar{l}_{i}^{n}=\\hat{l}_{i}^{n}\\times\\gamma_{i}+\\beta_{i},\\tag{8}\\gamma_{i}}\\gamma_{i}\\gamma_{i}\\gamma_{i}\\gam\n' +
      '\n' +
      '여기서 \\(i\\in\\{0,1,2\\}\\). 메타데이터 오버헤드를 줄이기 위해 모든 가우시안에서 동일한 스케일링 및 오프셋 인자를 공유한다. 미세 조정 후 공분산 파라미터는 \\(b\\)비트 정밀도로 인코딩되고, 재스케일링에 필요한 스케일링 및 오프셋 값은 32비트 플로트 정밀도로 저장된다.\n' +
      '\n' +
      '가중된 컬러 계수들에 대해, 코드북은 벡터 양자화(VQ)를 통한 대표 컬러 속성 인코딩을 가능하게 한다[24]. 순수하게 벡터 양자화를 적용하면 렌더링 품질이 떨어지지만 잔차 벡터를 사용한다.\n' +
      '\n' +
      '그림 3: 제안된 가우시안 이미지의 압축 파이프라인. 영상을 과적합한 후, 2단계 압축 절차를 적용하여 초고속 영상 코덱을 구축할 수 있다.\n' +
      '\n' +
      '성능 저하를 완화하기 위해 코드북 크기\\(B\\)로 VQ의 \\(M\\) 단계를 캐스케이드하는 양자화(RVQ) [67]:\n' +
      '\n' +
      'm^{c}^{m}\\mathcal{C}^{k}[i^{k}],\\quad m\\in\\{1,\\cdots,M\\,\\\\ & i_{n}^{m}=\\operatorname*{arg\\,min}_{m}\\left\\mathcal{C}^{m}[k] -(\\mathbf{c}_{n}^{\\prime}-\\hat{\\mathbf{c}_{n}^{\\prime0}=0,\\end{split}\\tag{9}\\m}}\\operatorname*{arg\\,min}_m}\\mathcal{C}^{m}[k] -(\\mathbf{c}_{n}^{\\prime}-\\hat{\\mathbf{c}_{n}^{\\prime0}=0,\\end{split}\\tag{9}\\m}\\operatorname*{arg\\,min}_m}\\mathcal{c}^{n}^{m}\n' +
      '\n' +
      '여기서 \\(\\hat{\\mathbf{c}}_{n}^{\\prime m}\\)은 \\(m\\) 양자화 단계 이후의 출력 컬러 벡터를 나타내고, \\(\\mathcal{C}^{m}\\in\\mathbb{R}^{B}3}\\)은 \\(m\\) 단계의 코드북을 나타내며, \\(i^{m}\\in\\{0,\\cdots,B-1\\}^{N}\\)은 \\(m\\) 단계의 코드북 인덱스이고, \\(\\mathcal{C}[i]\\in\\mathbb{R}^{3}\\)은 \\(\\mathcal{C}\\)의 인덱스에서의 벡터를 나타낸다. 코드북을 학습하기 위해 다음과 같이 몰입손실 \\(\\mathcal{L}_{c}\\)을 적용한다.\n' +
      '\n' +
      '[\\mathcal{L}_{c}=\\frac{1}{N\\times B}\\sum_{k=1}^{M}\\sum_{n=1}^{N}\\left\\|\\text{sg}[c_{n}^{\\prime}-\\hat{c}_{n}^{\\prime k-1}]-\\mathcal{C}^{k}[i_{n}^{k}]\\right\\|_{2}^{2}, \\tag{10}\\\\times\n' +
      '\n' +
      '여기서 \\(N\\)은 가우시안들의 수이고 \\(\\text{sg}[\\cdot]\\)은 정지-경사 연산이다.\n' +
      '\n' +
      '**엔트로피 인코딩.** 양자화 인식 미세 조정 후 엔트로피 코딩을 사용하여 양자화된 가우시안 파라미터의 크기를 더욱 줄인다. 문자 주파수의 통계적 상관관계를 이용하여 양자화된 공분산 파라미터와 색 코드북 인덱스를 각각 부호화하기 위해 비대칭 숫자 시스템(ANS, asymmetric number systems)[20]을 채택하였다.\n' +
      '\n' +
      '**부분 비트-백 코딩.** 2D 가우시안 파라미터들을 인코딩하기 위해 임의의 자동-회귀 컨텍스트[42]를 채택하지 않았기 때문에, 2D 가우시안 포인트들의 임의의 순열은 에지가 없는 등분산 그래프로서 볼 수 있다. 따라서, 우리는 비트 레이트를 절약하기 위해 [33]에 의해 설명된 등분산 그래프에 대해 비트-백 코딩 [55]를 채택할 수 있다. 보다 구체적으로, [33]은 \\(N\\) 원소를 갖는 순서화되지 않은 집합이 \\(N!\\)을 가짐을 보여준다. equivariant, and bit-back coding can save a bitrate\n' +
      '\n' +
      '\\[\\log N! -\\log N, \\tag{11}\\]\n' +
      '\n' +
      '순서가 지정되지 않은 요소를 직접 저장하는 것과 비교됩니다.\n' +
      '\n' +
      '그러나, 바닐라 비트-백 코딩은 \\(\\log N!\\)의 초기 비트 [55]를 필요로 한다. 즉, 단일 이미지가 아닌 데이터셋에서만 작동할 수 있습니다. 이 과제를 해결하기 위해, [47]은 이미지 데이터를 세그먼트화하는 부분 비트-백 코딩 전략을 도입하고, 초기 비트 할당으로서 이미지의 일부에 바닐라 엔트로피 코딩을 적용하고, 나머지는 비트-백 코딩을 통해 인코딩한다.\n' +
      '\n' +
      '우리의 경우, 우리는 [47]의 아이디어를 재사용한다. 구체적으로, 바닐라 엔트로피 코딩에 의해 초기 \\(K\\) 가우시안들을, 비트-백 코딩에 의해 후속 \\(N-K\\) 가우시안들을 부호화한다. 이 분할 접근법은 초기 비트\\(\\log(N-K)!\\)를 초과하는 초기 \\(K\\) 가우시안 비트율에 따라 단일 이미지 압축에 적용할 수 있다. \\(R_{k}\\)는 \\(k\\)번째 가우시안에서의 비트레이트를 나타내며, 최종 비트레이트 절약은 다음과 같이 공식화될 수 있다.\n' +
      '\n' +
      '\\[\\log(N-K^{*})!-\\log(N-K^{*}), \\tag{12}\\] \\[\\text{where }K^{*}=\\inf K,\\text{s.t.}\\sum_{k=1}^{K}R_{k}-\\log(N-K^{*})!\\text{s.t.}\\sum_{k=1}^{K}R_{k}-\\log(N-K^{*}) geq 0. \\tag{13}\\]의 이론적인 효과에도 불구하고, 비트-백 코딩은 느린 처리 지연으로 인해 초고속 코덱을 개발하는 목적과 일치하지 않을 수 있다[33]. 결과적으로, 우리는 이 부분을 1000 FPS로 달성할 수 있는 코덱의 최종 결과 대신 코덱이 달성할 수 있는 최상의 율-왜곡 성능에 대한 사전 개념 증명으로 남겨둔다.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      '영상표현을 위해 원본영상과 복원영상 사이의 왜곡을 최소화하는 것이 목적이다. 이를 위해 L2 손실 함수를 사용하여 가우시안 파라미터를 최적화한다. 이전의 GS 방법 [31]은 3D 장면을 최적화할 때 가우시안들을 분할하고 복제하기 위해 적응 밀도 제어를 도입한다는 점에 주목할 필요가 있다. 3D 공간에는 빈 공간이 많이 존재하기 때문에 이러한 공간을 채우는 것을 피하는 것을 고려할 필요가 있다. 대조적으로, 2D 이미지 공간에는 소위 빈 영역이 없다. 따라서 적응 밀도 제어를 폐기하여 2차원 영상 표현의 최적화 과정을 크게 단순화한다.\n' +
      '\n' +
      '이미지 압축 작업은 전체 손실\\(\\mathcal{L}\\)이 재구성 손실\\(\\mathcal{L}_{rec}\\)과 몰입 손실\\(\\mathcal{L}_{c}\\)으로 구성된다.\n' +
      '\n' +
      '\\[\\mathcal{L}=\\mathcal{L}_{rec}+\\lambda\\mathcal{L}_{c}, \\tag{14}\\]\n' +
      '\n' +
      '여기서 \\(\\lambda\\)는 각 손실 성분의 무게의 균형을 이루는 하이퍼-파라미터 역할을 한다. 색상 코드북은 K-means 알고리즘을 사용하여 초기화되어 후속 최적화를 위한 강력한 시작점을 제공한다. 미세 조정 중에 지수 이동 평균 모드를 채택하여 코드북을 업데이트한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 1: PSNR, MS-SSIM, 트레이닝 시간, 렌더링 속도, GPU 메모리 사용량 및 파라미터 크기에서 다양한 베이스라인과의 정량적 비교.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '**Dataset.** 이미지 표현 및 압축에 대한 우리의 평가는 두 개의 인기 있는 데이터 세트에 대해 수행된다. 해상도 768\\(\\times\\)512의 24개의 영상으로 구성된 Kodak 데이터셋 [1]과 2\\(\\times\\) bicubic downscaling의 DIV2K validation set [2]를 사용하여 408\\(\\times\\)1020에서 1020\\(\\times\\)1020까지 다양한 크기의 100개의 영상을 얻었다.\n' +
      '\n' +
      '**평가 메트릭.** 이미지 품질을 평가하기 위해 PSNR과 MS-SSIM [57]이라는 두 가지 존경받는 메트릭을 사용하여 재구성된 이미지와 원본 간의 왜곡을 측정한다. 이미지 압축을 위한 비트레이트는 픽셀당 비트(bpp)로 정량화된다.\n' +
      '\n' +
      '**구현 상세.** sgplat[66] 위에 개발된 우리의 가우시안 이미지는 축적된 블렌딩을 기반으로 래스터화를 위한 맞춤형 CUDA 커널을 통합한다. 달리 명시되지 않는 한 우리는 촐레스키 인수분해를 사용하여 2D 가우시안들의 공분산을 나타낸다. 가우시안 매개변수는 초기 학습률 \\(1e^{-3}\\)에서 시작하여 20000 단계마다 절반으로 줄어든 아단 최적화기 [59]를 사용하여 50000 단계에 걸쳐 최적화된다. 속성 양자화 인식 미세 조정 동안 공분산 매개변수의 양자화 정밀도 \\(b\\)는 RVQ와 함께 6비트로 설정된다.\n' +
      '\n' +
      '그림 4: PSNR 및 MS-SSIM에서 코닥 및 DIV2K 데이터 세트에 대한 접근법과 다른 기준선의 비율 왜곡 곡선이다. BB는 부분 비트-백 코딩을 나타낸다. 바운드는 우리의 코덱의 이론적인 비율을 나타낸다.\n' +
      '\n' +
      '색 벡터의 코드북 크기 \\(B\\)와 양자화 단계 \\(M\\)는 각각 8과 2로 고정되어 있다. K-means 알고리즘의 반복은 5로 설정되었으며 실험은 NVIDIA V100 GPU와 PyTorch를 사용하여 수행되었으며 추가 자료에서 사용할 수 있는 추가 세부 정보가 있다.\n' +
      '\n' +
      '**Benchmarks.** 이미지 표현 비교를 위해, GaussianImage는 SIREN[49], WIRE[48], I-NGP[43], NeuRBF[17]과 같은 경쟁 INR 방법에 대해 벤치마킹된다. 이미지 압축의 경우, 베이스라인들은 전통적인 코덱들(JPEG[56], JPEG2000[50]), VAE 기반 코덱들(Balle17[5], Balle18[6]), INR 기반 코덱들(COIN[22], COIN++[21])에 걸쳐 있다. 우리는 I-NGP를 위해 NeuRBF의 오픈 소스 PyTorch 구현[16]을 활용한다. 이러한 INR 방법들은 가우시안 이미지와 일관된 훈련 단계를 유지한다. 기준선에 대한 자세한 구현 노트는 부록에 나와 있습니다.\n' +
      '\n' +
      '### Image Representation\n' +
      '\n' +
      '도. 도 1(왼쪽) 및 표 1은 동일한 훈련 단계 하에서 코닥 및 DIV2K 데이터 세트에 대한 다양한 방법의 표현 성능을 나타낸다. MLP 기반 INR 방법(SIREN[49], WIRE[48])은 이미지에 적합하기 위해 더 적은 수의 파라미터를 사용하지만, 엄청난 훈련 시간과 초저 렌더링 속도로 인해 어려움을 겪는다. 최근의 특징 격자 기반 INR 방법들(I-NGP[43], NeuRBF[17])은 훈련 및 추론을 가속화하지만, GS 기반 방법들에 비해 실질적으로 더 많은 GPU 메모리를 요구한다. 원래의 3D GS는 3D 가우시안(Gaussian)을 표현 단위로 사용하기 때문에 학습을 줄이고 추론 속도를 제한하는 거대 파라미터 카운트의 도전에 직면한다. 표현 단위로 2D 가우시안(Gaussian)을 선택함으로써 학습 기간, 렌더링 속도 및 GPU 메모리 사용에서 현저한 이점을 확보하면서 저장된 매개변수의 수를 실질적으로 감소시키면서도 유사한 피팅 품질을 유지한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline Methods & Bpp\\(\\uparrow\\) & PSNR\\(\\uparrow\\) & MS-SSIM\\(\\uparrow\\) & Encoding FPS\\(\\uparrow\\) & Decoding FPS\\(\\uparrow\\) \\\\ \\hline JPEG [56] & 0.3197 & 25.2920 & 0.9020 & 608.61 & 614.68 \\\\ JPEG2000 [50] & 0.2394 & 27.2792 & 0.9305 & 3.46 & 4.32 \\\\ Balle17 [5] & 0.2271 & 27.7168 & 0.9508 & 21.23 & 18.83 \\\\ Balle18 [6] & 0.2533 & 28.7548 & 0.9584 & 16.53 & 15.87 \\\\ COIN [22] & 0.3419 & 25.8012 & 0.8905 & 8.68e-6 & 166.31 \\\\ Ours & 0.3164 & 25.6631 & 0.9154 & 4.11e-3 & 942.50 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: DIV2K 데이터세트 상의 전통적인 및 학습 기반 이미지 코덱의 계산 복잡도.\n' +
      '\n' +
      '그림 5: 코닥의 JPEG 및 COIN에 대한 우리의 방법의 주관적인 비교.\n' +
      '\n' +
      '### Image Compression\n' +
      '\n' +
      '**코딩 성능.** 도 4는 Kodak 및 DIV2K 데이터 세트에 대한 다양한 코덱의 RD 곡선을 나타낸다. 특히, PSNR에서 COIN[22] 및 COIN++[21]과 유사한 압축 성능을 보인다. 부분 비트-백 코딩의 도움으로, 우리의 코덱은 COIN 및 COIN++를 능가할 수 있다. 또한 MS-SSIM으로 측정했을 때, 우리의 방법은 큰 마진만큼 COIN을 능가한다. 도. 도 5는 본 논문의 방법인 JPEG[56]과 COIN을 정성적으로 비교하여, 본 논문의 방법이 보다 효과적으로 이미지 디테일을 복원하고 더 낮은 비트를 소비함으로써 우수한 복원 품질을 제공한다는 것을 보여준다.\n' +
      '\n' +
      '** 계산 복잡도.** 표 2는 NVIDIA V100 GPU에서 작동하는 학습 기반 코덱과 단일 스레드 모드에서 3.50GHz의 기본 주파수에서 인텔 코어(TM) i9-10920X 프로세서에서 작동하는 전통적인 코덱으로 DIV2K 데이터 세트의 여러 이미지 코덱의 계산 복잡도를 보고한다. 인상적이게도, 우리 코덱의 디코딩 속도는 JPEG와 같은 전통적인 코덱을 능가하는 942 FPS에 도달하면서 낮은 비트레이트에서도 향상된 압축 성능을 제공한다. 이것은 신경 이미지 코덱 분야에서 상당한 발전을 확립한다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**다른 성분의 효과.** GaussianImage에서 주요 성분의 기여도를 강조하기 위해 표 3에 자세히 설명된 대로 포괄적인 절제 연구 세트를 수행합니다. 초기에는 L1 손실과 SSIM 손실의 조합을 사용하는 원래 3D GS[31] 방법이 L2 손실을 사용하도록 조정되었습니다. 이 수정은 성능에 적은 비용으로 훈련 시간을 절반으로 줄입니다. 그리고 3.1절에서 3차원 가우시안(Gaussian)을 기본 2차원 가우시안(Basic 2D Gaussian)으로 대체하여 피팅 성능을 향상시키고 훈련 시간을 \\(\\frac{1}{3}\\) 감소시킬 뿐만 아니라 추론 속도를 배가시키고 파라미터 수를 \\(6.5\\times\\) 감소시킨다. 알파 블렌딩을 누적 블렌딩으로 단순화하여 랜덤 2D 효과를 제거합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline Methods & PSNR\\(\\uparrow\\) & MS-SSIM\\(\\uparrow\\) & Training Time(s)\\(\\downarrow\\) & FPS\\(\\uparrow\\) & Params(K)\\(\\downarrow\\) \\\\ \\hline\n' +
      '3D GS (w/L1+SSIM) & 37.75 & 0.9961 & 285.26 & 1067 & 1770 \\\\\n' +
      '3D GS (w/ L2) & 37.41 & 0.9947 & 197.90 & 1190 & 1770 \\\\ Ours (w/ L2+w/o AR+w/o M) & 37.89 & 0.9961 & 104.76 & 2340 & 270 \\\\ Ours (w/ L2+w/ AR+w/o M) & 38.69 & 0.9963 & 98.54 & 2555 & 270 \\\\ Ours(w/ L2+w/ AR+w/ M) & 38.57 & 0.9961 & 91.06 & 2565 & 240 \\\\ \\hline Ours (w/ L1) & 36.46 & 0.9937 & 92.68 & 2438 & 240 \\\\ Ours (w/ SSIM) & 35.65 & 0.9952 & 183.20 & 2515 & 240 \\\\ Ours (w/ L1+SSIM) & 36.57 & 0.9945 & 188.22 & 2576 & 240 \\\\ Ours (w/ L2+SSIM) & 34.73 & 0.9932 & 189.17 & 2481 & 240 \\\\ Ours (w/ L2) & **38.57** & **0.9961** & 91.06 & 2565 & 240 \\\\ \\hline Ours-RS & 38.83 & 0.9964 & 98.55 & 2321 & 240 \\\\ Ours-Cholesky & 38.57 & 0.9961 & 91.06 & 2565 & 240 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 50000개의 훈련 단계에 걸쳐 30000개의 가우시안 포인트를 갖는 코닥 데이터세트에 대한 이미지 표현의 절제 연구. AR은 누적 블렌딩 기반 래스터화를 의미하고, M은 컬러 계수 \\(\\boldsymbol{c}\\)와 불투명도 \\(o\\)을 병합하는 것을 의미한다. RS는 공분산 행렬을 회전 행렬과 스케일링 행렬로 분해하는 것을 의미한다. 각 하위 클래스의 마지막 행은 기본 솔루션을 나타냅니다.\n' +
      '\n' +
      '가우시안 오더링과 누적 투명도(T\\)에 대한 복잡한 계산을 우회함으로써 주목할 만한 훈련 및 추론 속도 이득과 함께 PSNR이 0.8dB 크게 향상되었다. 이는 제안된 누적 혼합 접근법의 효율성을 강조한다. 또한, 색상 벡터\\(\\mathbf{c}\\)와 불투명도\\(o\\)을 병합하여 개선된 2D 가우시안 구조를 형성함으로써, PSNR이 0.1dB 감소하면서 파라미터 수가 10% 감소하는 것을 확인하였다.\n' +
      '\n' +
      '**Loss function.** L2, L1 및 SSIM 손실의 다양한 조합을 표 3에 제시된 결과와 함께 평가한다. 이러한 결과는 L2 손실이 우리의 접근법에 최적으로 적합하여 신속한 훈련을 용이하게 하면서 이미지 재구성 품질을 크게 향상시킨다는 것을 확인한다.\n' +
      '\n' +
      '**공분산 행렬의 요인화된 형태.** 섹션 3.1에 요약된 바와 같이 분해를 통해 공분산 행렬의 요인화된 형태를 최적화한다. 표 3에 자세히 설명된 결과는 분해의 고유한 비특이성에도 불구하고 다양한 요인화된 형태가 이미지를 나타내는 데 유사한 능력을 가지고 있음을 보여준다. 부록은 다양한 인수분해 형태의 압축 견고성에 대한 추가 분석을 제공한다.\n' +
      '\n' +
      '** 양자화 전략.** 표 4는 상이한 양자화 방식이 이미지 압축에 미치는 영향을 조사한다. RVQ 코드북에 대한 수퍼비전이 없으면 코드북 벡터와 원벡터의 편차가 크게 발생하여 복원품질에 악영향을 미친다. 또한, 색상 매개변수(V2)에 대한 6비트 정수 양자화에 유리한 RVQ를 제거하면 기본 솔루션과 비교할 때 비트레이트 소비가 6.5% 증가했다. 이 결과는 서로 다른 가우시안에서의 색 벡터가 유사성을 공유하여 RVQ에 더 적합함을 시사한다. 더 높은 비트 양자화(V3)의 사용에 대한 추가 탐색은 압축 효율의 열화를 드러낸다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 2차원 가우시안 스플래팅(Gaussian Splatting)을 이용한 이미지 표현의 혁신적인 패러다임인 가우시안 이미지를 소개한다. 이 접근법은 일반적으로 사용되는 암시적 신경망에서 크게 분기하여 이미지의 이산적이고 명시적인 표현을 제공한다. 3차원 가우시안 스플래팅과 비교할 때, 2차원 가우시안 커널을 사용하면 이미지 표현에 두 가지 주목할만한 이점이 있다. 먼저, 계산 집약적인 알파 블렌딩을 효율적이고 순열 불변 누적 합산 블렌딩으로 단순화한다. 둘째, 각 가우시안에서 필요한 매개변수의 수가 59개에서 8개로 급격히 감소하여 com의 상당한 감소를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c} \\hline \\hline Variants & BD-PSNR (dB) \\(\\uparrow\\) & BD-rate (\\%) \\(\\downarrow\\) & BD-MS-SSIM \\(\\uparrow\\) & BD-rate (\\%) \\(\\downarrow\\) \\\\ \\hline Ours & 0 & 0 & 0 & 0 \\\\ (V1) w/o \\(\\mathcal{L}_{c}\\)+w/ RVQ + 6bit & -3.061 & 294.82 & -0.0820 & 304.21 \\\\ (V2) w/o \\(\\mathcal{L}_{c}\\)+w/o RVQ + 6bit & -0.176 & 6.50 & -0.0035 & 8.49 \\\\ (V3) w/o \\(\\mathcal{L}_{c}\\)+w/o RVQ + 8bit & -0.248 & 10.16 & -0.0081 & 16.80 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 코닥 데이터세트에 대한 양자화 스킴의 절제 연구. 첫 번째 행은 우리의 최종 솔루션을 나타내며 앵커로 설정된다.\n' +
      '\n' +
      '복잡성. 결과적으로, 가우시안 이미지는 이미지 코딩을 위한 매우 효율적이고 컴팩트한 기술로 등장한다. 실험 결과는 이러한 명시적 표현 전략이 훈련 및 추론 효율성을 실질적으로 향상시킨다는 것을 확인시켜준다. 또한, 암시적 신경 표현을 사용하는 방법에 비해 매개 변수에 벡터 양자화를 적용한 후 경쟁적인 율-왜곡 성능을 제공한다. 이러한 연구 결과는 비-엔드-투-엔드 이미지 압축 및 표현 전략에서 추가 탐색을 위한 유망한 방법을 제안한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Kodak lossless true color image suite. [https://r0k.us/graphics/kodak/](https://r0k.us/graphics/kodak/) (1999)\n' +
      '* [2] Agustsson, E., Timofte, R.: Ntire 2017 challenge on single image super-resolution: Dataset and study. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops (July 2017)\n' +
      '* [3] Ahmed, N., Natarajan, T., Rao, K.R.: Discrete cosine transform. IEEE transactions on Computers **100**(1), 90-93 (1974)\n' +
      '* [4] Balle, J., Laparra, V., Simoncelli, E.P.: Density modeling of images using a generalized normalization transformation. arXiv preprint arXiv:1511.06281 (2015)\n' +
      '* [5] Balle, J., Laparra, V., Simoncelli, E.P.: End-to-end optimized image compression. In: International Conference on Learning Representations (2017)\n' +
      '* [6] Balle, J., Minnen, D., Singh, S., Hwang, S.J., Johnston, N.: Variational image compression with a scale hyperprior. In: International Conference on Learning Representations (2018)\n' +
      '* [7] Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srinivasan, P.P.: Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5855-5864 (2021)\n' +
      '* [8] Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5470-5479 (2022)\n' +
      '* [9] Bellard, F.: Bpg image format. [https://bellard.org/bpg/](https://bellard.org/bpg/) (2014)\n' +
      '* [10] Bhalgat, Y., Lee, J., Nagel, M., Blankevoort, T., Kwak, N.: Lsq+: Improving low-bit quantization through learnable offsets and better initialization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 696-697 (2020)\n' +
      '* [11] Chen, G., Wang, W.: A survey on 3d gaussian splatting. arXiv preprint arXiv:2401.03890 (2024)\n' +
      '* [12] Chen, H., Gwilliam, M., Lim, S.N., Shrivastava, A.: Hnerv: A hybrid neural representation for videos. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10270-10279 (2023)\n' +
      '* [13] Chen, H., He, B., Wang, H., Ren, Y., Lim, S.N., Shrivastava, A.: Nerv: Neural representations for videos. Advances in Neural Information Processing Systems **34**, 21557-21568 (2021)\n' +
      '* [14] Chen, Y., Liu, S., Wang, X.: Learning continuous image representation with local implicit image function. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8628-8638 (2021)* [15] Chen, Y., Chen, Z., Zhang, C., Wang, F., Yang, X., Wang, Y., Cai, Z., Yang, L., Liu, H., Lin, G.: Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2024) 5\n' +
      '* [16] Chen, Z., Li, Z., Song, L., Chen, L., Yu, J., Yuan, J., Xu, Y.: [https://github.com/oppo-us-research/NeuRBF](https://github.com/oppo-us-research/NeuRBF) 12\n' +
      '* [17] Chen, Z., Li, Z., Song, L., Chen, L., Yu, J., Yuan, J., Xu, Y.: Neurbf: A neural fields representation with adaptive radial basis functions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4182-4194 (2023) 2, 4, 10, 12\n' +
      '* [18] Chen, Z., Wang, F., Liu, H.: Text-to-3d using gaussian splatting. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2024) 5\n' +
      '* [19] Cheng, Z., Sun, H., Takeuchi, M., Katto, J.: Learned image compression with discretized gaussian mixture likelihoods and attention modules. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7939-7948 (2020)\n' +
      '* [20] Duda, J.: Asymmetric numeral systems. arXiv preprint arXiv:0902.0271 (2009) 9\n' +
      '* [21] Dupont, E., Loya, H., Alizadeh, M., Golinski, A., Teh, Y., Doucet, A.: Coin++: neural compression across modalities. Transactions on Machine Learning Research **2022**(11) (2022) 2, 4, 5, 12\n' +
      '* [22] Dupont, E., Golinski, A., Alizadeh, M., Teh, Y.W., Doucet, A.: Coin: Compression with implicit neural representations. In: Neural Compression: From Information Theory to Applications-Workshop@ ICLR 2021 (2021) 2, 4, 5, 12, 13\n' +
      '* [23] Fathony, R., Sahu, A.K., Willmott, D., Kolter, J.Z.: Multiplicative filter networks. In: International Conference on Learning Representations (2020) 2, 4\n' +
      '* [24] Gray, R.: Vector quantization. IEEE Assp Magazine **1**(2), 4-29 (1984) 8\n' +
      '* [25] Guo, Z., Flamich, G., He, J., Chen, Z., Hernandez-Lobato, J.M.: Compression with bayesian implicit neural representations. Advances in Neural Information Processing Systems **36** (2024) 2, 5\n' +
      '* [26] He, D., Yang, Z., Peng, W., Ma, R., Qin, H., Wang, Y.: Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5718-5727 (2022) 5\n' +
      '* [27] Heil, C.E., Walnut, D.F.: Continuous and discrete wavelet transforms. SIAM review **31**(4), 628-666 (1989) 1\n' +
      '* [28] Higham, N.J.: Cholesky factorization. Wiley interdisciplinary reviews: computational statistics **1**(2), 251-254 (2009) 6\n' +
      '* [29] Huang, H., Li, L., Cheng, H., Yeung, S.K.: Photo-slam: Real-time simultaneous localization and photorealistic mapping for monocular, stereo, and rgb-d cameras. arXiv preprint arXiv:2311.16728 (2023) 4\n' +
      '* [30] Keetha, N., Karhade, J., Jatavallabhula, K.M., Yang, G., Scherer, S., Ramanan, D., Luiten, J.: Splatam: Splat, track & map 3d gaussians for dense rgb-d slam. arXiv preprint arXiv:2312.02126 (2023) 4\n' +
      '* [31] Kerbl, B., Kopanas, G., Leimkuhler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics **42**(4) (2023) 2, 3, 4, 6, 10, 13\n' +
      '* [32] Koyuncu, A.B., Gao, H., Boev, A., Gaikov, G., Alshina, E., Steinbach, E.: Contextformer: A transformer with spatio-channel attention for context modeling in learned image compression. In: European Conference on Computer Vision. pp. 447-463. Springer (2022)33] Kunze, J., Severo, D., Zani, G., van de Meent, J.W., Townsend, J.: Entropy coding of unordered data structures. In: The Twelfth International Conference on Learning Representations (2024), [https://openreview.net/forum?id=afQuNt3Ruh](https://openreview.net/forum?id=afQuNt3Ruh)[34]\n' +
      '* [35] Ladune, T., Philippe, P., Henry, F., Clare, G., Leguay, T.: Cool-chic: Coordinate-based low complexity hierarchical image codec. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13515-13522 (2023)\n' +
      '* [36] Leguay, T., Ladune, T., Philippe, P., Clare, G., Henry, F., Deforges, O.: Low-complexity overfitted neural image codec. In: 2023 IEEE 25th International Workshop on Multimedia Signal Processing (MMSP). pp. 1-6. IEEE (2023)\n' +
      '* [37] Li, Z., Wang, M., Pi, H., Xu, K., Mei, J., Liu, Y.: E-nerv: Expedite neural video representation with disentangled spatial-temporal context. In: European Conference on Computer Vision. pp. 267-284. Springer (2022)\n' +
      '* [38] Liu, J., Sun, H., Katto, J.: Learned image compression with mixed transformer-cnn architectures. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14388-14397 (2023)\n' +
      '* [39] Luiten, J., Kopanas, G., Leibe, B., Ramanan, D.: Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713 (2023)\n' +
      '* [40] Ma, C., Yu, P., Lu, J., Zhou, J.: Recovering realistic details for magnification-arbitrary image super-resolution. IEEE Transactions on Image Processing **31**, 3669-3683 (2022)\n' +
      '* [41] Martel, J.N., Lindell, D.B., Lin, C.Z., Chan, E.R., Monteiro, M., Wetzstein, G.: Acorn: adaptive coordinate networks for neural scene representation. ACM Transactions on Graphics (TOG) **40**(4), 1-13 (2021)\n' +
      '* [42] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: European Conference on Computer Vision. pp. 405-421. Springer (2020)\n' +
      '* [43] Minnen, D., Balle, J., Toderici, G.D.: Joint autoregressive and hierarchical priors for learned image compression. In: Advances in neural information processing systems (2018)\n' +
      '* [44] Muller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG) **41**(4), 1-15 (2022)\n' +
      '* [45] Nguyen, Q.H., Beksi, W.J.: Single image super-resolution via a dual interactive implicit neural network. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 4936-4945 (2023)\n' +
      '* [46] Quan, Y., Yao, X., Ji, H.: Single image defocus deblurring via implicit neural inverse kernels. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 12600-12610 (2023)\n' +
      '* [47] Ramasinghe, S., Lucey, S.: Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps. In: European Conference on Computer Vision. pp. 142-158. Springer (2022)\n' +
      '* [48] Ryder, T., Zhang, C., Kang, N., Zhang, S.: Split hierarchical variational compression. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 386-395 (2022)\n' +
      '* [49] Saragadam, V., LeJeune, D., Tan, J., Balakrishnan, G., Veeraraghavan, A., Baraniuk, R.G.: Wire: Wavelet implicit neural representations. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18507-18516 (2023)\n' +
      '* [50] Sitzmann, V., Martel, J., Bergman, A., Lindell, D., Wetzstein, G.: Implicit neural representations with periodic activation functions. Advances in neural information processing systems **33**, 7462-7473 (2020)* [50] Skodras, A., Christopoulos, C., Ebrahimi, T.: The jpeg 2000 still image compression standard. IEEE Signal processing magazine **18**(5), 36-58 (2001)\n' +
      '* [51] Stanley, K.O.: Compositional pattern producing networks: A novel abstraction of development. Genetic programming and evolvable machines **8**, 131-162 (2007)\n' +
      '* [52] Strumpler, Y., Postels, J., Yang, R., Gool, L.V., Tombari, F.: Implicit neural representations for image compression. In: European Conference on Computer Vision. pp. 74-91. Springer (2022)\n' +
      '* [53] Takikawa, T., Litalien, J., Yin, K., Kreis, K., Loop, C., Nowrouzezahrai, D., Jacobson, A., McGuire, M., Fidler, S.: Neural geometric level of detail: Real-time rendering with implicit 3d shapes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11358-11367 (2021)\n' +
      '* [54] Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J., Ng, R.: Fourier features let networks learn high frequency functions in low dimensional domains. Advances in Neural Information Processing Systems **33**, 7537-7547 (2020)\n' +
      '* [55] Townsend, J., Bird, T., Barber, D.: Practical lossless compression with latent variables using bits back coding. arXiv preprint arXiv:1901.04866 (2019)\n' +
      '* [56] Wallace, G.K.: The jpeg still picture compression standard. Communications of the ACM **34**(4), 30-44 (1991)\n' +
      '* [57] Wang, Z., Simoncelli, E.P., Bovik, A.C.: Multiscale structural similarity for image quality assessment. In: The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003. vol. 2, pp. 1398-1402. Ieee (2003)\n' +
      '* [58] Wu, G., Yi, T., Fang, J., Xie, L., Zhang, X., Wei, W., Liu, W., Tian, Q., Wang, X.: 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528 (2023)\n' +
      '* [59] Xie, X., Zhou, P., Li, H., Lin, Z., Shuicheng, Y.: Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models. In: Has it Trained Yet? NeurIPS 2022 Workshop (2022)\n' +
      '* [60] Xu, D., Wang, P., Jiang, Y., Fan, Z., Wang, Z.: Signal processing for implicit neural representations. Advances in Neural Information Processing Systems **35**, 13404-13418 (2022)\n' +
      '* [61] Xu, Q., Xu, Z., Philip, J., Bi, S., Shu, Z., Sunkavalli, K., Neumann, U.: Point-nerf: Point-based neural radiance fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5438-5448 (2022)\n' +
      '* [62] Xu, W., Jiao, J.: Revisiting implicit neural representations in low-level vision. In: International Conference on Learning Representations Workshop (2023)\n' +
      '* [63] Yan, C., Qu, D., Wang, D., Xu, D., Wang, Z., Zhao, B., Li, X.: Gs-slam: Dense visual slam with 3d gaussian splatting. arXiv preprint arXiv:2311.11700 (2023)\n' +
      '* [64] Yan, Y., Lin, H., Zhou, C., Wang, W., Sun, H., Zhan, K., Lang, X., Zhou, X., Peng, S.: Street gaussians for modeling dynamic urban scenes. arXiv preprint arXiv:2401.01339 (2024)\n' +
      '* [65] Yang, Z., Yang, H., Pan, Z., Zhu, X., Zhang, L.: Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642 (2023)\n' +
      '* [66] Ye, V., Turkulainen, M., the Nerfstudio team: sgplat, [https://github.com/nerfstudio-project/gsplat](https://github.com/nerfstudio-project/gsplat)\n' +
      '* [67] Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., Tagliasacchi, M.: Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing **30**, 495-507 (2021)* [68] Zhang, X., Yang, R., He, D., Ge, X., Xu, T., Wang, Y., Qin, H., Zhang, J.: Boosting neural representations for videos with a conditional decoder. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2024)\n' +
      '* [69] Zhou, X., Lin, Z., Shan, X., Wang, Y., Sun, D., Yang, M.H.: Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes. arXiv preprint arXiv:2312.07920 (2023)\n' +
      '* [70] Zielonka, W., Bagautdinov, T., Saito, S., Zollhofer, M., Thies, J., Romero, J.: Drivable 3d gaussian avatars. arXiv preprint arXiv:2311.08581 (2023)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>