<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# OWSM v3.1: E-Branchformer 기반의 더 좋고 빠른 개방형 속삭임 스타일 음성 모델\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근 연구는 투명성과 개방 과학을 촉진하기 위해 완전 개방 기반 모델을 옹호했다. 초기 단계로 Open Whisper-style Speech Model(OWSM)은 공개된 데이터와 오픈소스 툴킷을 이용하여 OpenAI의 Whisper를 재현하였다. 기존 OWSM v1~v3 모델은 위스퍼 재현을 목표로 여전히 트랜스포머를 기반으로 하고 있어 다른 최첨단 음성 인코더에 비해 성능이 떨어질 수 있다. 본 연구에서는 별도의 훈련 데이터 없이 OWSM의 성능과 효율성을 향상시키는 것을 목표로 한다. 우리는 E-Branchformer 기반 OWSM v3.1 모델을 100M 및 1B의 두 가지 척도로 제시한다. 1B 모델은 공개적으로 이용 가능한 가장 큰 E-Branchformer 기반 음성 모델이다. 대다수의 평가 벤치마크에서 기존 OWSM v3를 능가하는 동시에 최대 25% 더 빠른 추론 속도를 보여줍니다. 데이터 준비 스크립트, 사전 훈련 모델 및 훈련 로그.1\n' +
      '\n' +
      '각주 1: [https://www.wavlab.org/activities/2024/owsm/](https://www.wavlab.org/activities/2024/owsm/)\n' +
      '\n' +
      '**인덱스 용어**: 음성 기초 모델, 음성 인식, 음성 번역, 분기구\n' +
      '\n' +
      'Yifan Peng\\({}^{1}\\), Jinchuan Tian\\({}^{1}\\), William Chen\\({}^{1}\\), Siddhant Arora\\({}^{1}\\), Brian Yan\\({}^{1}\\), Yui Sudo\\({}^{2}\\), Muhammad Shakeel\\({}^{2}\\), Choi\\({}^{1}\\), Jiatong Shi\\({}^{1}\\), Xuankai Chang\\({}^{1}\\), Juankai Watanabe\\({}^{1}\\), Shinji Watanabe\\({}^{1}\\) Carnegie Mellon University, USA \\({}^{2}\\) Honda Research Institute Japan, Japan yifanpen@andrew.cmu.edu. swatanab@andrew.cmu.edu.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대규모 언어 기반 모델은 최근 몇 년 동안 인기를 얻었습니다. 언어 및 작업에 걸친 지식 공유뿐만 아니라 모델 및 데이터 크기의 스케일 업으로 인해, 이러한 대규모 다국어 멀티태스크 모델들은 다양한 음성 처리 벤치마크들 [1, 2, 3]에서 최첨단(SOTA) 성능을 달성한다. OpenAI의 속삭임[1]은 39M에서 1.5B까지의 5개의 음계로 사전 훈련된 모델을 출시하는 가장 널리 사용되는 음성 기초 모델 중 하나이다. 강한 성능에도 불구하고, 훈련 데이터 소스 및 모델 학습 역학과 같은 완전한 개발 파이프라인을 사용할 수 없다. 이러한 중요한 훈련 세부 사항에 대한 접근의 부재는 데이터 유출, 공정성 및 편향과 같은 문제로 이어질 수 있으며, 이는 대규모 언어 모델(LLM) 시대에 점점 더 주목을 받고 있다. 투명성과 개방형 과학을 촉진하기 위해 최근 연구에서는 LLMs[4, 5, 6], 자체 감독 음성 모델[7, 8], 속삭임 스타일 음성 모델[9]의 오픈 소스 복제를 옹호했다.\n' +
      '\n' +
      'Open Whisper-style Speech Model(OWSM) [9]는 공개 데이터 세트와 오픈 소스 툴킷 ESPnet[10]의 다양한 조합을 사용하여 Whisper-style 트레이닝을 재현한다. 다국어 자동 음성 인식(ASR), 임의의 음성 번역(ST), 언어 식별(LID) 및 발화 수준 정렬을 지원합니다. 또한 모든 스크립트, 사전 훈련된 모델 가중치 및 훈련 로그를 공개적으로 릴리스합니다. 완전 개방 음성 기반 모델을 향한 초기 단계로서, [9], OWSM v1, v2 및 v3에서 제시된 세 가지 버전은 여전히 OpenAI의 속삭임을 가능한 한 일치시키는 것을 목표로 하는 표준 트랜스포머 [11] 아키텍처를 사용한다. 그러나 Conformer [12] 및 Branchformer [13]과 같은 다른 SOTA 인코더에 비해 최적이 아닌 성능으로 이어질 수 있다.\n' +
      '\n' +
      '이 작업에서는 추가 학습 데이터** 없이 이전 OWSM v3 모델 **의 성능과 효율성을 향상시키는 것을 목표로 한다. 구체적으로, 음성 모델링 능력을 향상시키기 위해 인코더로서 E-Branchformer[14]를 사용한다. 우리는 각각 100M 및 1B 매개변수가 있는 "OWSM v3.1 염기" 및 "OWSM v3.1 배지"라는 두 가지 척도에서 새로운 OWSM 모델을 제시한다. 대형 E-Branchformer 모델의 학습을 안정화하고 가속화하기 위해, 우리는 새로운 부분적 선형 학습 속도 스케줄을 제안하고 훈련 중에 FlashAttention[15]를 채택한다. 광범위한 벤치마크에 대한 결과는 **OWSM v3.1이 9개의 영어 ASR 중 8개, 11개의 다국어 ASR 중 10개, 19개의 ST 중 13개, 4개의 SLUE-PERB[16] 테스트 세트 중 3개에서 이전 OWSM v3**보다 우수함을 보여준다. 또한, 우리의 **OWSM v3.1은 얕은 디코더 덕분에 CoVSt-2 ST 벤치마크에서 추론하는 동안 16%에서 25% 더 빠르다. **OWSM v3.1 베이스와 미디어도 각각 위스퍼 베이스와 미디어보다 추론 속도가 빠르다.** 동일한 오픈 사이언스 정신을 추구하여 [9] 최신 데이터 준비 스크립트, 사전 훈련된 모델 및 훈련 로그를 공개한다.\n' +
      '\n' +
      '##2 OWSM v3.1\n' +
      '\n' +
      '### Network architecture\n' +
      '\n' +
      '속삭임[1] 및 OWSM v3[9]는 둘 다 표준 트랜스포머 인코더-디코더 아키텍처[11]를 이용한다. 그러나, 컨포머 [12] 및 브랜치포머 [13, 14]와 같은 스피치 인코더에서의 최근의 발전은 다양한 스피치 처리 태스크 [17, 18]에서의 성능을 크게 향상시켰다. 따라서 대규모 음성 기반 모델에서 이러한 고급 아키텍처를 탐구하는 것은 자연스럽고 유망합니다. 본 논문에서는 엔코더로서 E-Branchformer[14]를 사용하였으며, 1B 파라미터 스케일에서 그 효용성을 입증하였다. 우리가 아는 한, 이것은 공개적으로 이용 가능한 가장 큰 E-브랜치포머 기반 음성 모델이다.\n' +
      '\n' +
      'E-Branchformer는 개선된 Branchformer[13]이며 다양한 작업에 걸쳐 SOTA 성능을 보여주었다[18]. E-Branchformer는 병렬 브랜치를 사용하여 음성 특징 시퀀스에서 로컬 및 글로벌 컨텍스트 정보를 캡처하고 컨볼루션과 병합한다. 속삭임 방식의 훈련에서는 입력 오디오의 길이가 30s로 고정되어 있기 때문에 Conformer의 상대 위치 임베딩 대신 정현파 절대 위치 임베딩을 채택한다.\n' +
      '\n' +
      '표 1은 네트워크 아키텍처를 요약한 것이다. 우리의 OWSM v3.1은 인코더가 E-브랜치포머로 전환되는 것을 제외하고는 대부분 OWSM v3의 설계를 따른다. 또한 은닉 크기와 레이어 수를 수정하여 모델 크기를 조정합니다. 스케일링 거동을 조사하기 위해 서로 다른 모델 크기의 두 가지 변형, 즉 100M 매개변수가 있는 OWSM v3.1 베이스 및 1B 매개변수가 있는 OWSM v3.1 매체를 조사한다. 동일한 규모(베이스 또는 매체)에서 OWSM v3 및 위스퍼보다 약간 크음에도 불구하고, 우리의 새로운 OWSM v3.1 모델은 주로 더 작은 디코더로 인해 섹션 3.3 및 표 4에 보고된 바와 같이 더 빠른 추론 속도를 갖는다. 또한 OWSM은 모델에 별도의 선형 투영 레이어를 추가하는 훈련 중 조인트 CTC 손실을 사용한다는 점에 주목한다(베이스의 경우 19M 매개변수, 매체의 경우 51M 매개변수). 이 CTC 계층은 일반적으로 추론 시간에 사용되지 않으므로 폐기될 수 있으며, 이는 기본 및 중간 모델에 대해 각각 전체 모델 크기를 19% 및 5% 감소시킨다.\n' +
      '\n' +
      '### Data preparation\n' +
      '\n' +
      '본 논문의 OWSM v3.1 모델의 학습 데이터 포맷은 OWSM v3[9]의 학습 데이터 포맷과 동일하며, 이는 다수의 음성 처리 태스크를 스퀀스-투-시퀀스 프레임워크로 통합한다. 이 멀티태스크 데이터 포맷은 음성 번역을 임의의 언어 방향들로 확장함으로써 원래의 위스퍼[1]로부터 적응된다. 자세한 내용은 독자들을 OWSM[9]으로 안내합니다.\n' +
      '\n' +
      '성능 비교를 위해 OWSM v3.1을 훈련할 때 추가 데이터를 사용하지 않고 ESPnet.2에서 공개적으로 사용할 수 있는 데이터 준비 스크립트를 재사용한다. 또한 텍스트 스크립트를 보다 일관성 있게 만들기 위해 다음과 같은 전처리 작업을 수행한다.\n' +
      '\n' +
      '각주 2: [https://github.com/espnet/respnet/tree/master/egs2/owsm_v3/szt1](https://github.com/espnet/respnet/tree/master/egs2/owsm_v3/szt1)\n' +
      '\n' +
      '* 우리는 WSJ를 훈련 데이터에서 제외한다; 구두점이 단어로서 명시적으로 발화되고 주석이 달린 다른 말하기 및 주석 스타일을 갖는다.\n' +
      '* 우리는 모든 상위 사례 전사체를 하위 사례로 제공하는 몇 가지 데이터 세트를 정규화한다. 다른 데이터는 변경되지 않은 상태로 유지됩니다.\n' +
      '* 우리는 두 언어 코드 "cmn"과 "zho"를 하나의 코드 "zho"로 병합한다.\n' +
      '\n' +
      '표 1은 우리의 훈련 데이터의 전체 통계를 보여준다. 그것은 OWSM v3[9]에서 사용된 것과 거의 동일하다. 전술한 전처리로 인해 극히 적은 양의 데이터만이 수정된다. 여기 우리의 OWSM v3.1 모델을 훈련시키는데 사용되는 전체 데이터 세트들이 있다: AISHELL-1[19], CoVoST2[20], GigaSpeech[21], LibriSpeech[22], MuST-C[23], SPGIS-speech[24], TEDLIUM3[25], GigaST[26], 다국어 LiriSpeech(MLS)[27], WentSpeech[28], AIDATATANG[29], AMI[30], Babel[31], Common Voice[32], Fisher(Switchboard)[33], Fisher Callhome Spanish[34], FLEURS[35], Google18n3, KsponSpeech[36], MagicData[37], Reazon-Speech[38], Russian Open STT[39], VCTK[41], VoxPopuli[42].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline \\multirow{2}{*}{Test set} & \\multicolumn{3}{c}{Whisper [1]} & \\multicolumn{3}{c}{OWSM v3 [9]} & \\multicolumn{3}{c}{OWSM v3.1 (ours)} \\\\ \\cline{2-7}  & base & small & medium & large & large-v3 & medium & base & medium \\\\ \\hline\n' +
      '**Network architecture** & & & & & & & \\\\ Encoder & & Transformer & & Transformer & Transformer & E-Branchformer \\\\ Decoder & & & Transformer & Transformer & Transformer & Transformer & Transformer \\\\ Parameters & 74M & 244M & 769M & 1.55B & 1.55B & 889M & 101M & 1.02B \\\\ Layers & 6 & 12 & 24 & 32 & 32 & 24 & 6 & 18 \\\\ Hidden size & 512 & 768 & 1024 & 1280 & 1280 & 1024 & 384 & 1024 \\\\ Attention heads & 8 & 12 & 16 & 20 & 20 & 16 & 6 & 16 \\\\ Time shift & 20ms & 20ms & 20ms & 20ms & 20ms & 40ms & 40ms & 40ms \\\\ \\hline\n' +
      '**Training data** & & & & & & & \\\\ Unlabelled hours & & n/a & & 4M & n/a & n/a \\\\ Labelled hours & & 680K & & 1M & 180K & 180K \\\\ - English ASR & & 438K & unknown & 73K & 73K & 73K \\\\ - Multilingual ASR & & 117K & unknown & 67K & 67K \\\\ - Translation & & 125K & unknown & 40K & 40K \\\\ Languages & & 99 & & 100 & 151 & 151 \\\\ BPE vocabulary & & 52K & & 52K & 50K & 50K \\\\ \\hline\n' +
      '**Hyperparameters** & & & & & & \\\\ Batch size & & 256 & unknown & 256 & 256 \\\\ Total updates & & 1M & 2 epochs & 970K\\({}^{\\dagger}\\) & 675K \\\\ Warmup updates & & 2K & unknown & 10K & 60K \\\\ Learning rate & 1e-3 & 5e-4 & 2.5e-4 & 1.75e-4 & unknown & 2.5e-4 & 1e-3 & 2e-4 \\\\ Optimizer & & AdamW & & unknown & AdamW & AdamW \\\\ CTC weight & & n/a & & n/a & 0.3 & 0.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 네트워크 아키텍처, 훈련 데이터 및 하이퍼파라미터 비교. \\ ({}^{\\dagger}\\): OWSM v3이 OWSM v2[9]로 초기화되기 때문에, 총 업데이트 수.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline \\multirow{2}{*}{Test set} & \\multicolumn{3}{c}{Whisper} & \\multicolumn{3}{c}{OWSM v3} & \\multicolumn{3}{c}{OWSM v3.1 (ours)} \\\\ \\cline{2-5}  & base & small & medium & base & medium \\\\ \\hline Common Voice en & 25.2 & 15.7 & **11.9** & 14.5 & 21.5 & 12.6 \\\\ FLEURS en & 12.4 & 9.6 & **6.4** & 10.9 & 14.8 & 9.0 \\\\ LibriSpeech test-clean & 5.1 & 3.3 & 2.8 & 2.7 & 3.6 & **2.4** \\\\ LibriSpeech test-other & 12.0 & 7.7 & 6.5 & 6.0 & 9.1 & **5.0** \\\\ MLS en & 13.4 & 9.1 & 10.2 & 7.4 & 12.0 & **7.1** \\\\ Switchboard eval200 & 25.7 & 22.2 & 19.4 & 17.2 & 22.9 & **16.3** \\\\ TEDLIUM test & 6.3 & **4.6** & 5.1 & 4.8 & 7.8 & 5.1 \\\\ VoxPopuli en & 10.2 & 8.5 & **7.6** & 9.2 & 12.0 & 8.4 \\\\ WSJ eval92 & 5.0 & 4.3 & **2.9** & 13.4 & 5.3 & 3.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: Greedy search를 이용한 영어 ASR의 WER %((\\(\\downarrow\\)). 속삭임 모델은 영어 오디오의 438K 시간에 대해 훈련되는 반면, OWSM v3 및 v3.1은 73K 시간에 대해 훈련된다. 대담한 텍스트가 가장 좋은 결과입니다. 밑줄 친 텍스트는 우리의 OWSM v3.1이 OWSM v3보다 우수하다는 것을 의미한다.\n' +
      '\n' +
      '### Training configuration\n' +
      '\n' +
      '이전의 OWSM 모델 [9]에 이어, 우리의 OWSM v3.1은 PyTorch [43]과 함께 ESPnet2 [10]에서 구현된다. 자세한 훈련 하이퍼파라미터는 표 1에 제시되어 있으며, 분산 데이터 병렬 및 혼합 정밀 훈련을 사용한다. 또한 플래시 어텐션[15]을 활용하여 훈련 효율성을 향상시킵니다. 플래시 어텐션으로 GPU당 배치 크기를 2배(샘플 2~4개)로 늘릴 수 있어 훈련 비용을 크게 줄일 수 있다. OWSM v3.1 베이스 및 매체는 각각 16 및 64 NVIDIA A100 GPU(40GB)로 훈련된다. 두 모델 모두 180K 시간의 데이터 중 약 3개의 전체 패스에 대해 학습된다. 훈련은 기본 및 중간에서 각각 약 6일과 16일이 소요된다. v2에서 미세 조정된 이전 OWSM v3와 달리, 우리의 새로운 OWSM v3.1 모델은 총 업데이트가 적은 처음부터 훈련된다.\n' +
      '\n' +
      '선행연구 [18, 14]에서는 E-Branchformer가 Conformer보다 더 안정적으로 수렴하는 것으로 나타났다. 그러나 1B E-Branchformer 기반 모델을 대규모 다국어, 멀티태스크, 긴 형태의 데이터로 훈련하는 것은 매우 어렵다.4 이러한 대규모 실험은 훈련 초기에 매우 적은 학습률을 필요로 하며, 그렇지 않으면 손실이 빠르게 감소하는 것을 멈출 것이다. 일반적으로 사용되는 선형 웜업 학습률 스케줄을 채택하면, 우리는 최대 학습률을 크게 줄이거나 웜업 단계를 증가시켜야 하며, 둘 다 예비 탐색에 따라 성능이 떨어진다. 이 문제를 완화하기 위해, 우리는 워밍업 초기에 천천히 그리고 나중에 워밍업 중에 빠르게 학습률을 증가시키는 부분 선형 워밍업 학습률 스케줄을 제안한다. 구체적으로, 학습률은 첫 번째 30K 단계에서 매우 작은 값(예를 들어, 5e-5)으로 선형적으로 증가하다가 두 번째 30K 단계에서 원하는 피크 학습률로 선형적으로 증가한다. 웜업 후 바닐라 버전과 같은 방식으로 기하급수적으로 감소한다. 제안된 부분 선형 스케줄은 OWSM v3.1의 성공적인 훈련을 가능하게 한다.\n' +
      '\n' +
      '각주 4: 우리의 경험에 따르면, 이것은 주로 30대의 긴 형태의 데이터 형식 때문이다. 심지어 매우 작은 모델들은 아키텍처(인코더-디코더 또는 CTC 전용)에 관계없이 수렴하는 데 어려움을 겪는다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '이 섹션에서는 동일한 설정에서 광범위한 작업 및 벤치마크에서 OWSM v3.1 모델을 이전 v3 버전과 비교한다. 우리는 OWSM v3.1이 대부분의 벤치마크에서 OWSM v3보다 우수하다는 것을 보여준다. 참고용으로 위스퍼 모델의 결과도 포함되지만, 전혀 다른 훈련 데이터로 인해 비교가 불공정할 수 있다는 점에 유의한다.\n' +
      '\n' +
      '영국어 음성인식\n' +
      '\n' +
      '표 2는 ESPnet에서 작성된 표준 벤치마크에 대한 영어 ASR 결과를 보여준다. 우리는 [9]를 따라 탐욕 검색으로 추론을 실행하고 채점 전에 속삭임 텍스트 정규화기를 적용한다. 동일한 규모(기본 또는 중간)에서 모델을 비교함으로써 다음과 같은 관찰 결과를 얻었다. (1) 우리의 OWSM v3.1 모델은 중간 규모에서 9개의 테스트 세트 중 8개에서 이전 OWSM v3 모델보다 우수하다. 특히 Common Voice, FLEURS, LibriSpeech, Switchboard, VoxPopuli 및 WSI.5에서 개선 효과가 크게 나타나 E-Branchformer 인코더의 유효성을 검증한다. (2) 중간 규모에서 OWSM v3.1은 LibriSpeech 및 Switchboard에서 Whisper를 능가한다. 기본 스케일에서 OWSM v3.1은 공통 음성뿐만 아니라 이러한 세트에서 더 낮은 WER을 달성한다. 이는 OWSM 모델이 위스퍼(73K 대 438K 시간)에 비해 훨씬 적은 데이터에 대해 훈련되었지만 경쟁적 성능을 보여준다.\n' +
      '\n' +
      '각주 5: [9]에서 논의된 바와 같이 WSJ 훈련 데이터는 OWSM v3에 의해 사용되지만 그 전사체는 완전히 상위이다. 이 모델은 그것을 또 다른 낮은 자원 언어로 취급할 수 있으며, 이로 인해 결과가 좋지 않습니다. v3.1에서 우리는 훈련 중에 WSJ를 배제하고 훨씬 더 낮은 WER을 달성한다.\n' +
      '\n' +
      '다국어 음성인식\n' +
      '\n' +
      '표 3은 다국어 ASR 결과를 제시한다. 다시, 우리는 동일한 설정에서 다른 모델을 비교하기 위해 [9]를 따른다. WER 또는 CER을 계산하기 전에 탐욕 디코딩을 사용하고 위스퍼 텍스트 정규화기를 적용한다. 우리는 OWSM v3.1 중간 모델이 다양한 언어에 걸쳐 11개의 테스트 세트 중 10개에서 OWSM v3보다 일반적으로 큰 마진만큼 성능이 우수함을 관찰한다. 구체적으로, 평균 에러율은 18.8%에서 15.2%로 감소된다. 이는 다양한 언어에서 OWSM v3.1의 우수한 성능을 보여준다.\n' +
      '\n' +
      '스위스퍼에 비해 OWSM v3.1은 제한된 훈련 데이터로 인해 여전히 많은 유럽 언어에서 뒤쳐진다. 대조적으로, 충분한 양의 데이터(예를 들어, 중국어 및 일본어)가 있는 경우, OWSM v3.1은 강력한 성능을 달성하고 위스퍼보다 우수하다. 이를 통해 학습 데이터 양의 중요성을 알 수 있다. 향후 OWSM을 더욱 개선하기 위해 YODAS[44]와 같은 다른 공공 소스의 데이터를 더 포함할 계획이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Test set} & \\multirow{2}{*}{Language} & \\multirow{2}{*}{Metric} & \\multicolumn{4}{c}{Whisper} & \\multicolumn{4}{c}{OWSM v3} & \\multicolumn{4}{c}{OWSM v3.1 (ours)} \\\\ \\cline{3-10}  & & & data & base & small & medium & data & medium & data & base & medium \\\\ \\hline \\multirow{8}{*}{Multilingual LibriSpeech} & Spanish & & 11.1K & 14.5 & 9.1 & **6.1** & 2.0K & 11.7 & 2.0K & 18.5 & 9.0 \\\\  & French & & 9.8K & 25.2 & 13.6 & **9.7** & 2.5K & 14.1 & 2.5K & 24.2 & 12.1 \\\\  & German & & 13.3K & 19.9 & 11.5 & **8.1** & 3.7K & 11.9 & 3.7K & 18.7 & 10.8 \\\\  & Dutch & Dutch & 2.1K & 30.9 & 18.2 & **12.2** & 1.7K & 17.7 & 1.7K & 28.6 & 18.1 \\\\  & Italian & 2.6K & 32.9 & 21.3 & **15.6** & 0.7K & 24.5 & 0.7K & 33.7 & 20.2 \\\\  & Portuguese & & 8.6K & 23.5 & 13.8 & **8.9** & 0.3K & 28.2 & 0.3K & 44.9 & 21.6 \\\\  & Polish & & 4.3K & 25.2 & 12.5 & **6.8** & 0.3K & 37.0 & 0.3K & 49.7 & 25.2 \\\\ \\hline \\multirow{2}{*}{AISHELL-1} & Chinese & & 23.4K & 39.1 & 25.1 & 15.7 & 16.0K & 7.1 & 16.3K & 12.2 & **6.4** \\\\  & KsponSpeech eval-clean & & 27.0 & 24.0 & 17.6 & 20.5 & 23.8 & **16.7** \\\\  & Korean & CER & 8.0K & 22.9 & 15.4 & **12.8** & 1.0K & 22.6 & 1.0K & 26.1 & 18.9 \\\\  & ReazonSpeech & Japanese & & 7.1K & 54.1 & 32.5 & 25.3 & 18.9K & 11.3 & 18.9K & 11.2 & **7.9** \\\\ \\hline Average WER/CER (\\(\\downarrow\\)) & & & 28.7 & 17.9 & **12.6** & & 18.8 & & 26.5 & 15.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: Greedy search를 이용한 다국어 ASR의 WER/CER %((\\(\\downarrow\\)). 훈련 데이터 크기(시간)도 표시됩니다. **대담한** 텍스트가 가장 좋은 결과입니다. 밑줄 친 텍스트는 우리의 OWSM v3.1이 OWSM v3.1보다 우수하다는 것을 의미한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '### 긴 형태의 음성인식\n' +
      '\n' +
      '표 5는 TEDLIUM2 테스트 세트에 대한 긴 형태의 영어 ASR 결과를 나타낸다. [1, 9]와 유사하게 OWSM은 전체 오디오 레코딩을 입력으로 취하고 녹취록을 청크로 생성한다. 각각의 청크는 30s의 고정된 길이를 가지며, 청크는 예측된 타임스탬프에 기초하여 점진적으로 시프트된다. 제안된 OWSM v3.1 미디어는 OWSM v3의 9.2와 비교하여 5.7의 WER을 달성하며, 이는 OWSM v3.1의 롱폼 오디오에 대한 강인성을 보여준다; 예측된 타임스탬프 또한 더 정확할 수 있다. OWSM v3.1은 여전히 베이스 및 중간 스케일 모두에서 속삭임에 뒤떨어지는데, 이는 (1) 우리의 트레이닝 데이터가 위스퍼의 트레이닝 데이터의 단지 1/4 정도이고, (2) OWSM에 의해 사용되는 많은 공공 데이터 세트는 세그먼트화되지 않은 긴-형태 버전을 제공하지 않고 트레이닝을 위해 세그먼트화된 짧은 오디오를 사용해야 하기 때문에 트레이닝과 디코딩 사이의 불일치를 초래한다. 앞으로 이 문제를 완화하기 위해 더 긴 형태의 학습 데이터를 추가할 계획입니다.\n' +
      '\n' +
      '### Language identification\n' +
      '\n' +
      '표 6은 ESPnet에서 작성된 FLEURS 테스트 세트에 대한 언어 식별의 정확도를 보고한다. 기존의 OWSM v3에 비해 OWSM v3.1의 성능이 저하되는 것을 알 수 있지만, 본 논문에서 제안한 모델은 다중언어 FLEURS와 Common Voice 데이터를 학습용으로 사용하기 때문에 여전히 Whisper 매체보다 훨씬 우수하다. 우리는 또한 우리의 OWSM v3.1이 위스퍼에 비해 확장으로 더 많은 이점을 얻는다는 것을 발견했다. 구체적으로, 기본에서 중간까지 OWSM v3.1의 정확도는 거의 두 배(41.9%에서 75.6%)인 반면 위스퍼의 정확도는 약간만 증가(47.6%에서 54.8%)했다. 가능한 이유는 (1) 우리의 OWSM은 더 많은 언어와 언어 쌍을 지원하므로 더 작은 모델이 학습하기 더 어렵다; (2) 현재의 OWSM v3.1 기본 모델은 더 긴 훈련 실행 또는 더 최적화된 하이퍼파라미터에 의해 개선될 수 있는 훈련 데이터를 여전히 과소 적합시킬 수 있다. 우리는 "기본" 및 "중간" 외에 "작은"과 같은 다른 크기의 모델을 훈련하여 이러한 스케일링 동작을 추가로 조사할 것이다.\n' +
      '\n' +
      '음성 언어 이해\n' +
      '\n' +
      '속삭임과 OWSM은 SLU 작업을 직접 수행할 수 없습니다. 인코더의 성능을 조사하기 위해, 최근에 제안된 SLUE-PERB 벤치마크를 사용하여 예비 실험을 수행한다[16]. 구체적으로, 미리 트레이닝된 음성 인코더는 동결되고, 랜덤하게 초기화된 얕은 디코더는 태스크-특정 SLU 트레이닝 데이터에 대해 트레이닝된다. 그런 다음 해당 SLU 테스트 데이터에 대해 모델을 평가한다. 이러한 평가 절차는 널리 사용되는 SUPERB 벤치마크[45]와 유사하다. 중형 위스퍼, OWSM v3 및 OWSM v3.1 모델을 네 가지 SLU 작업, 즉 감정 분석(SA), 명명된 개체 인식(NER), 명명된 개체 위치 확인(NEL) 및 대화 행위 분류(DAC)에 대해 평가한다. 표 7에 도시된 바와 같이, 우리의 OWSM v3.1은 우리의 E-브랜치포머 인코더의 강한 용량을 검증하는 NER, NEL 및 DAC에서 큰 마진으로 다른 것보다 우수하다. OWSM은 인코더 표현을 텍스트 정보와 더 잘 정렬하는 조인트 CTC 훈련을 사용하기 때문에 두 OWSM 모델 모두 NEL에 대해 위스퍼보다 훨씬 더 높은 프레임 레벨 F1 점수를 달성한다는 점에 유의한다.\n' +
      '\n' +
      '## 4 미래 방향\n' +
      '\n' +
      '본 논문에서는 E-Branchformer가 별도의 학습 데이터를 사용하지 않고도 대용량 다국어 멀티태스크 음성 기반 모델의 전체 성능과 효율성을 크게 향상시킴을 보인다. 이러한 연구 결과는 향후 몇 가지 방향을 모색하도록 영감을 준다.\n' +
      '\n' +
      '* OWSM v3.1은 다양한 라이선스를 가진 공공 데이터에 대해 훈련되며, 그 중 일부는 더 엄격한 제약을 갖는다. 이제 우리는 무료 라이선스가 있는 데이터의 하위 집합을 사용하여 다른 모델을 훈련하고 있습니다. 가까운 시일 내에 이 모델과 훈련 데이터를 공개할 예정입니다.\n' +
      '* 데이터는 음성 기초 모델을 개발할 때 고려해야 할 가장 중요한 요소 중 하나이다. OWSM v3.1은 제한된 훈련 데이터가 있을 때 여전히 속삭임에 뒤쳐진다. 우리는 그 성능을 더욱 향상시키기 위해 YODAS[44]와 같은 더 많은 공공 데이터를 포함해야 한다.\n' +
      '* 보다 강력하고 효율적인 음성 인코더 아키텍처를 개발하는 것은 빅 데이터 및 빅 모델 시대에도 여전히 음성 커뮤니티에 매우 중요하다.\n' +
      '* OWSM과 같은 사전 훈련된 모델을 사용하는 것은 음성 언어 이해[46, 47]와 같은 다른 다운스트림 작업에 매우 유망하며, 이는 특히 지속적인 학습의 맥락에서 탐구될 수 있다.\n' +
      '* 현재의 음성 기초 모델들은 대개 매우 크고 느리다. 증류[48], 가지치기[49], 공동 증류 및 가지치기[50] 또는 보다 유연한 입력 종속 동적 아키텍처[51]와 같은 효율성을 개선하기 위해 다양한 압축 알고리즘이 필요하다.\n' +
      '* 음성 모델들은 범용 멀티모달 기반 모델들을 향한 유망한 길인 텍스트 LLM들과 통합될 수 있다[52, 53].\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 E-Branchformer 기반의 Open Whisper-style Speech Model인 OWSM v3.1을 제안한다. 동일한 양의 데이터에 대해 훈련되었지만, 우리의 OWSM v3.1은 대다수의 평가 조건에서 이전의 OWSM v3보다 더 나은 결과를 달성하는 동시에 최대 25% 더 빠른 추론 속도를 보여준다. 우리는 투명성을 촉진하고 대규모 언어 기반 모델의 개발을 촉진하기 위해 모든 스크립트, 사전 훈련된 모델 가중치 및 훈련 로그를 공개적으로 공개한다.\n' +
      '\n' +
      '## 6 Acknowledgements\n' +
      '\n' +
      '우리의 컴퓨팅 자원은 국립 과학 재단의 보조금 #2138259, #2138286, #2138307, #2137603 및 #2138296에서 지원하는 ACCESS 할당 CIS210014를 통해 PSC 브리지2 및 NCSA 델타에서 제공한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. Mcleavey, and I. Sutskever, "Robust speech recognition via large-scale weak supervision," in _Proc. ICML_, 2023.\n' +
      '* [2] Y. Zhang, W. Han, J. Qin, Y. Wang, A. Bapna, Z. Chen, N. Chen, B. Li, V. Axelrod, G. Wang _et al._, "Google usm: Scaling automatic speech recognition beyond 100 languages," _arXiv preprint arXiv:2303.01037_, 2023.\n' +
      '* [3] L. Barrault, Y.-A. Chung, M. C. Meglioli, D. Dale, N. Dong, M. Duppenthaler, P.-A. Duquenne, B. Ellis, H. Elsahar, J. Haeheim _et al._, "Seamless: Multilingual expressive and streaming speech translation," _arXiv preprint arXiv:2312.05187_, 2023.\n' +
      '* [4] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin _et al._, "Opt: Open pre-trained transformer language models," _arXiv:2205.01068_, 2022.\n' +
      '* [5] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar_et al._, "Llama: Open and efficient foundation language models," _arXiv:2302.13971_, 2023.\n' +
      '* [2] Z. Liu, A. Qiao, W. Neiswanger, H. Wang, B. Tan, T. Tao, J. Li, Y. Wang, S. Sun, O. Pangarkar _et al._, "Llm360: Towards fully transparent open-source lms," _arXiv preprint arXiv:2312.06550_, 2023.\n' +
      '* [3] W. Chen, X. Chang, Y. Peng, Z. Ni, S. Maiti, and S. Watanabe, "Reducing Barriers to Self-Supervised Learning: HuBERT Pretraining with Academic Compute," in _Proc. Interspeech_, 2023.\n' +
      '* [4] W. Chen, J. Shi, B. Yan, D. Berrebbi, W. Zhang, Y. Peng, X. Chang, S. Maiti, and S. Watanabe, "Joint prediction and denoising for large-scale multilingual self-supervised learning," in _Proc. ASRU_, 2023.\n' +
      '* [5] Y. Peng, J. Tian, B. Yan, D. Berrebbi, X. Chang, X. Li, J. Shi, S. Arora, W. Chen, R. Sharma, W. Zhang, Y. Sudo, M. Shakael, J. wcon Jung, S. Maiti, and S. Watanabe, "Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data," in _Proc. ASRU_, 2023.\n' +
      '* [6] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Rendchuintala, and T. Ochiai, "ESPnet: End-to-End Speech Processing Toolkit," in _Proc. Interspeech_, 2018.\n' +
      '* [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," in _Proc. NeurIPS_, 2017.\n' +
      '* [8] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, "Conformer: Convolution-augmented Transformer for Speech Recognition," in _Proc. Interspeech_, 2020.\n' +
      '* [9] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, "Branchformer: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding," in _Proc. ICML_, 2022.\n' +
      '* [10] K. Kim, F. Wu, Y. Peng, J. Pan, P. Sridhar, K. J. Han, and S. Watanabe, "E-branchformer: Branchformer with enhanced merging for speech recognition," in _Proc. SLT_, 2023.\n' +
      '* [11] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Re, "Flashattention: Fast and memory-efficient exact attention with io-awareness," in _Proc. NeurIPS_, 2022.\n' +
      '* [12] S. Arora, R. Sharma, A. Pasad, H. Dhamyal, W. Chen, S. Shon, H.-Y. Lee, K. Livescu, and S. Watanabe, "SLUE-PERB: A Spoken Language Understanding Performance Benchmark and Toolkit," in _ASRU SPARKS Workshop_, 2023.\n' +
      '* [13] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y. Higuchi, H. Inaguma, N. Kamo, C. Li, D. Garcia-Romero, J. Shi _et al._, "Recent developments on espnet toolkit boosted by conformer," in _Proc. ICASSP_, 2021.\n' +
      '* [14] Y. Peng, K. Kim, F. Wu, B. Yan, S. Arora, W. Chen, J. Tang, S. Shon, P. Sridhar, and S. Watanabe, "A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks," in _Proc. Interspeech_, 2023.\n' +
      '* [15] H. Bu _et al._, "AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline," in _Proc. O-COCOSDA_, 2017.\n' +
      '* [16] C. Wang _et al._, "CoVoST 2 and Massively Multilingual Speech Translation," in _Interspeech_, 2021.\n' +
      '* [17] G. Chen _et al._, "GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio," in _Proc. Interspeech_, 2021.\n' +
      '* [18] V. Panayotov _et al._, "Librispeech: An ASR corpus based on public domain audio books," in _ICASSP_, 2015.\n' +
      '* [19] R. Cattoni _et al._, "Must-c: A multilingual corpus for end-to-end speech translation," _Computer speech & language_, vol. 66, p. 101155, 2021.\n' +
      '* [20] P. K. O\'Neill _et al._, "Spsigspeech: 5.000 hours of transcribed financial audio for fully formatted end-to-end speech recognition," _arXiv:2104.02014_, 2021.\n' +
      '* [21] F. Hernandez _et al._, "Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation," in _Speech & Computer_, 2018, pp. 198-208.\n' +
      '* [22] R. Ye _et al._, "Gigast: A 10,000-hour pseudo speech translation corpus," _arXiv:2204.03939_, 2022.\n' +
      '* [23] V. Pratap _et al._, "Mls: A large-scale multilingual dataset for speech research," _arXiv:2012.03411_, 2020.\n' +
      '* [24] B. Zhang _et al._, "Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition," in _Proc. ICASSP_, 2022.\n' +
      '* [25] "aidatatang_200zh, a free Chinese Mandarin speech corpus by Beijing DataTang Technology Co., Ltd."\n' +
      '* [26] J. Carletta, "Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus," _Lang. Res. Eval._, vol. 41, pp. 181-190, 2007.\n' +
      '* [27] "The babel program: [https://www.iarpa.gov/index.php/research-babel](https://www.iarpa.gov/index.php/research-babel)."\n' +
      '* [28] R. Ardila _et al._, "Common voice: A massively-multilingual speech corpus," _arXiv:1912.06670_, 2019.\n' +
      '* [29] J. Godfrey _et al._, "SWITCHBOARD: telephone speech corpus for research and development," in _Proc. ICASSP_, 1992.\n' +
      '* [30] M. Post _et al._, "Improved speech-to-text translation with the fisher and calmloem Spanish-English speech translation corpus," in _Proc. IWSLT_, 2013.\n' +
      '* [31] A. Conneau _et al._, "FLEURS: Few-Shot Learning Evaluation of Universal Representations of Speech," in _Proc. SLT_, 2022.\n' +
      '* [32] J.-U. Bang _et al._, "Ksponspeech: Korean spontaneous speech corpus for automatic speech recognition," _Applied Sciences_, vol. 10, no. 19, p. 6936, 2020.\n' +
      '* [33] Z. Yang _et al._, "Open source magicdata-ramc: A rich annotated mandarin conversational (ramc) speech dataset," _arXiv:2203.16844_, 2022.\n' +
      '* [34] Y. Yin, D. Mori _et al._, "ReazonSpeech: A Free and Massive Corpus for Japanese ASR," 2023.\n' +
      '* [35] A. Slizhikova _et al._, "Russian Open Speech To Text (STT/ASR) Dataset," 2020. [Online]. Available: [https://github.com/snakers4/open_att](https://github.com/snakers4/open_att)\n' +
      '* [36] J. Yamagishi _et al._, "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit," 2019.\n' +
      '* [37] "VoxForge: [http://www.voxforge.org/](http://www.voxforge.org/)."\n' +
      '* [38] C. Wang _et al._, "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation," in _Proc. ACL_, 2021.\n' +
      '* [39] A. Paszke _et al._, "Pytorch: An imperative style, high-performance deep learning library," in _Proc. NeurIPS_, 2019.\n' +
      '* [40] X. Li, S. Takamichi, T. Saeki, W. Chen, S. Shiota, and S. Watanabe, "Yodas: Youtube-oriented dataset for audio and speech," in _Proc. ASRU_, 2023.\n' +
      '* [41] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K.tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, "SUPERB: Speech Processing Universal PERformance Benchmark," in _Proc. Interspeech_, 2021.\n' +
      '* [42] Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, S. Dalmia, X. Chang, and S. Watanabe, "A Study on the Integration of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding," in _Proc. SLT_, 2022.\n' +
      '* [43] S. Arora, H. Futami, J.-w. Jung, Y. Peng, R. Sharma, Y. Kashiwagi, E. Tsuno, and S. Watanabe, "Universal: Universal spoken language understanding for diverse classification and sequence generation tasks with a single network," _arXiv preprint arXiv:2310.02973_, 2023.\n' +
      '* [44] H.-J. Chang, S.-w. Yang, and H.-y. Lee, "Distillhubert: Speech representation learning by layer-wise distillation of hidden-unit bert," in _Proc. ICASSP_, 2022.\n' +
      '\n' +
      '* [49] Y. Peng, K. Kim, F. Wu, P. Sridhar, and S. Watanabe, "Structured pruning of self-supervised pre-trained models for speech recognition and understanding," in _Proc. ICASSP_, 2023.\n' +
      '* [50] Y. Peng, Y. Sudo, S. Muhammad, and S. Watanabe, "DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models," in _Proc. Interspeech_, 2023.\n' +
      '* [51] Y. Peng, J. Lee, and S. Watanabe, "I3d: Transformer architectures with input-dependent dynamic depth for speech recognition," in _Proc. ICASSP_, 2023.\n' +
      '* [52] M. Wang, W. Han, I. Shafran, Z. Wu, C.-C. Chiu, Y. Cao, N. Chen, Y. Zhang, H. Soltau, P. K. Rubenstein, L. Zilka, D. Yu, G. Pundak, N. Siddhartha, J. Schalkwyk, and Y. Wu, "Slm: Bridge the thin gap between speech and text foundation models," in _Proc. ASRU_, 2023.\n' +
      '* [53] C. Tang, W. Yu, G. Sun, X. Chen, T. Tan, W. Li, L. Lu, Z. Ma, and C. Zhang, "Salmom: Towards generic hearing abilities for large language models," _arXiv preprint arXiv:2310.13289_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>