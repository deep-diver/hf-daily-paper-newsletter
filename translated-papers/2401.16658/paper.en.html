<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Recent studies have advocated for fully open foundation models to promote transparency and open science. As an initial step, the Open Whisper-style Speech Model (OWSM) reproduced OpenAI\'s Whisper using publicly available data and open-source toolkits. With the aim of reproducing Whisper, the previous OWSM v1 through v3 models were still based on Transformer, which might lead to inferior performance compared to other state-of-the-art speech encoders. In this work, we aim to improve the performance and efficiency of OWSM without extra training data. We present E-Branchformer based OWSM v3.1 models at two scales, i.e., 100M and 1B. The 1B model is the largest E-Branchformer based speech model that has been made publicly available. It outperforms the previous OWSM v3 in a vast majority of evaluation benchmarks, while demonstrating up to 25% faster inference speed. We publicly release the data preparation scripts, pre-trained models and training logs.1\n' +
      '\n' +
      'Footnote 1: [https://www.wavlab.org/activities/2024/owsm/](https://www.wavlab.org/activities/2024/owsm/)\n' +
      '\n' +
      '**Index Terms**: speech foundation models, speech recognition, speech translation, branchformer\n' +
      '\n' +
      'Yifan Peng\\({}^{1}\\), Jinchuan Tian\\({}^{1}\\), William Chen\\({}^{1}\\), Siddhant Arora\\({}^{1}\\), Brian Yan\\({}^{1}\\), Yui Sudo\\({}^{2}\\), Muhammad Shakeel\\({}^{2}\\), Kwanghee Choi\\({}^{1}\\), Jiatong Shi\\({}^{1}\\), Xuankai Chang\\({}^{1}\\), Jee-weon Jung\\({}^{1}\\), Shinji Watanabe\\({}^{1}\\)\\({}^{1}\\)Carnegie Mellon University, USA \\({}^{2}\\)Honda Research Institute Japan, Japan yifanpen@andrew.cmu.edu, swatanab@andrew.cmu.edu\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large-scale speech foundation models have gained popularity in recent years. Owing to the scaling up of model and data sizes as well as the knowledge sharing across languages and tasks, these massively multilingual multtask models achieve state-of-the-art (SOTA) performance in various speech processing benchmarks [1, 2, 3]. OpenAI\'s Whisper [1] is one of the most widely used speech foundation models, which releases pre-trained models at five scales ranging from 39M to 1.5B. Despite its strong performance, the full development pipeline such as the training data sources and model learning dynamics is unavailable. The absence of access to these important training details can lead to issues like data leakage, fairness and bias, which have attracted an increasing attention in the era of large language models (LLMs). To promote transparency and open science, recent studies have advocated for open-source reproduction of LLMs [4, 5, 6], self-supervised speech models [7, 8], and Whisper-style speech models [9].\n' +
      '\n' +
      'The Open Whisper-style Speech Model (OWSM) [9] reproduces Whisper-style training using a diverse combination of publicly available datasets and the open-source toolkit ESPnet [10]. It supports multilingual automatic speech recognition (ASR), any-to-any speech translation (ST), language identification (LID) and utterance-level alignment. It also publicly releases all scripts, pre-trained model weights and training logs. As an initial step towards fully open speech foundation models, the three versions presented in [9], OWSM v1, v2 and v3, still utilize the standard Transformer [11] architecture, which aims to match OpenAI\'s Whisper as much as possible. Yet, it can lead to sub-optimal performance compared to other SOTA encoders like Conformer [12] and Branchformer [13].\n' +
      '\n' +
      'In this work, we aim to improve the performance and efficiency of the previous OWSM v3 model **without additional training data**. Specifically, we use E-Branchformer [14] as the encoder to enhance the speech modeling capability. We present new OWSM models at two scales named "OWSM v3.1 base" and "OWSM v3.1 medium", with 100M and 1B parameters, respectively. To stabilize and accelerate the training of large E-Branchformer models, we propose a novel piecewise linear learning rate schedule and adopt FlashAttention [15] during training. Results on extensive benchmarks show that **OWSM v3.1 outperforms the previous OWSM v3** in 8 out of 9 English ASR, 10 out of 11 multilingual ASR, 13 out of 19 ST, and 3 out of 4 SLUE-PERB [16] test sets. Moreover, our **OWSM v3.1 is 16% to 25% faster during inference** on the CoVSt-2 ST benchmark, thanks to the shallower decoder. **OWSM v3.1 base and medium also have faster inference speeds than Whisper base and medium, respectively.** Pursuing the same open science spirit [9], we publicly release the latest data preparation scripts, pre-trained models and training logs.\n' +
      '\n' +
      '## 2 OWSM v3.1\n' +
      '\n' +
      '### Network architecture\n' +
      '\n' +
      'Whisper [1] and OWSM v3 [9] both utilize the standard Transformer encoder-decoder architecture [11]. However, recent advancements in speech encoders such as Conformer [12] and Branchformer [13, 14] have greatly improved the performance in various speech processing tasks [17, 18]. It is thus natural and promising to explore such advanced architectures in large-scale speech foundation models. In this work, we employ E-Branchformer [14] as the encoder and demonstrate its effectiveness at a scale of 1B parameters. To the best of our knowledge, this is the largest E-Branchformer based speech model that has been made publicly available.\n' +
      '\n' +
      'E-Branchformer is an enhanced Branchformer [13] and has shown SOTA performance across a wide variety of tasks [18]. E-Branchformer utilizes parallel branches to capture local and global contextual information in a speech feature sequence and merges them with convolutions. In Whisper-style training, the input audio has a fixed length of 30s, so we simply adopt the sinusoidal absolute positional embedding instead of the relative positional embedding of Conformer.\n' +
      '\n' +
      'Table 1 summarizes the network architectures. Our OWSM v3.1 mostly follows the design of OWSM v3, except that the encoder is switched to E-Branchformer. We also modify the hidden size and number of layers to adjust the model size. Weprovide two variants of different model sizes to investigate the scaling behavior, i.e., OWSM v3.1 base with 100M parameters and OWSM v3.1 medium with 1B parameters. Despite being slightly larger than OWSM v3 and Whisper at the same scale (base or medium), our new OWSM v3.1 models have faster inference speeds as reported in Section 3.3 and Table 4, mainly due to the smaller decoder. We also note that OWSM employs a joint CTC loss during training which adds a separate linear projection layer to the model (19M parameters in base and 51M parameters in medium). This CTC layer is usually not used at inference time and can thus be discarded, which reduces the overall model size by 19% and 5% for base and medium models, respectively.\n' +
      '\n' +
      '### Data preparation\n' +
      '\n' +
      'The training data format of our OWSM v3.1 models is identical to that of OWSM v3 [9], which unifies multiple speech processing tasks into a squence-to-sequence framework. This multitask data format is adapted from the original Whisper [1] by extending speech translation to any language directions. We guide the readers to OWSM [9] for full details.\n' +
      '\n' +
      'To fairly compare the performance, we do not use extra data when training OWSM v3.1. We reuse the data preparation scripts that are publicly available in ESPnet.2 Additionally, we perform the following preprocessing operations to make the text transcripts more consistent:\n' +
      '\n' +
      'Footnote 2: [https://github.com/espnet/respnet/tree/master/egs2/owsm_v3/szt1](https://github.com/espnet/respnet/tree/master/egs2/owsm_v3/szt1)\n' +
      '\n' +
      '* We exclude WSJ from the training data; it has a different speaking and annotation style, where the punctuation is explicitly uttered and annotated as a word.\n' +
      '* We normalize a few datasets which provide all upper case transcripts into lower case. Other data remains unchanged.\n' +
      '* We merge two language codes "cmn" and "zho" into a single code "zho".\n' +
      '\n' +
      'Table 1 shows the overall statistics of our training data. It is almost the same as that used in OWSM v3 [9]. Only a very small amount of data is modified due to the above-mentioned preprocessing. Here is a full list of datasets used to train our OWSM v3.1 models: AISHELL-1 [19], CoVoST2 [20], GigaSpeech [21], LibriSpeech [22], MuST-C [23], SPGIS- speech [24], TEDLIUM3 [25], GigaST [26], Multilingual LiriSpeech (MLS) [27], WentSpeech [28], AIDATATANG [29], AMI [30], Babel [31], Common Voice [32], Fisher (Switchboard) [33], Fisher Callhome Spanish [34], FLEURS [35], Google18n3, KsponSpeech [36], MagicData [37], Reazon-Speech [38], Russian Open STT [39], VCTK [40], VoxForge [41], and VoxPopuli [42].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline \\multirow{2}{*}{Test set} & \\multicolumn{3}{c}{Whisper [1]} & \\multicolumn{3}{c}{OWSM v3 [9]} & \\multicolumn{3}{c}{OWSM v3.1 (ours)} \\\\ \\cline{2-7}  & base & small & medium & large & large-v3 & medium & base & medium \\\\ \\hline\n' +
      '**Network architecture** & & & & & & & \\\\ Encoder & & Transformer & & Transformer & Transformer & E-Branchformer \\\\ Decoder & & & Transformer & Transformer & Transformer & Transformer & Transformer \\\\ Parameters & 74M & 244M & 769M & 1.55B & 1.55B & 889M & 101M & 1.02B \\\\ Layers & 6 & 12 & 24 & 32 & 32 & 24 & 6 & 18 \\\\ Hidden size & 512 & 768 & 1024 & 1280 & 1280 & 1024 & 384 & 1024 \\\\ Attention heads & 8 & 12 & 16 & 20 & 20 & 16 & 6 & 16 \\\\ Time shift & 20ms & 20ms & 20ms & 20ms & 20ms & 40ms & 40ms & 40ms \\\\ \\hline\n' +
      '**Training data** & & & & & & & \\\\ Unlabelled hours & & n/a & & 4M & n/a & n/a \\\\ Labelled hours & & 680K & & 1M & 180K & 180K \\\\ - English ASR & & 438K & unknown & 73K & 73K & 73K \\\\ - Multilingual ASR & & 117K & unknown & 67K & 67K \\\\ - Translation & & 125K & unknown & 40K & 40K \\\\ Languages & & 99 & & 100 & 151 & 151 \\\\ BPE vocabulary & & 52K & & 52K & 50K & 50K \\\\ \\hline\n' +
      '**Hyperparameters** & & & & & & \\\\ Batch size & & 256 & unknown & 256 & 256 \\\\ Total updates & & 1M & 2 epochs & 970K\\({}^{\\dagger}\\) & 675K \\\\ Warmup updates & & 2K & unknown & 10K & 60K \\\\ Learning rate & 1e-3 & 5e-4 & 2.5e-4 & 1.75e-4 & unknown & 2.5e-4 & 1e-3 & 2e-4 \\\\ Optimizer & & AdamW & & unknown & AdamW & AdamW \\\\ CTC weight & & n/a & & n/a & 0.3 & 0.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Comparison on network architecture, training data and hyperparameters. \\({}^{\\dagger}\\): the total number of updates, since OWSM v3 is initialized with OWSM v2 [9].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline \\multirow{2}{*}{Test set} & \\multicolumn{3}{c}{Whisper} & \\multicolumn{3}{c}{OWSM v3} & \\multicolumn{3}{c}{OWSM v3.1 (ours)} \\\\ \\cline{2-5}  & base & small & medium & base & medium \\\\ \\hline Common Voice en & 25.2 & 15.7 & **11.9** & 14.5 & 21.5 & 12.6 \\\\ FLEURS en & 12.4 & 9.6 & **6.4** & 10.9 & 14.8 & 9.0 \\\\ LibriSpeech test-clean & 5.1 & 3.3 & 2.8 & 2.7 & 3.6 & **2.4** \\\\ LibriSpeech test-other & 12.0 & 7.7 & 6.5 & 6.0 & 9.1 & **5.0** \\\\ MLS en & 13.4 & 9.1 & 10.2 & 7.4 & 12.0 & **7.1** \\\\ Switchboard eval200 & 25.7 & 22.2 & 19.4 & 17.2 & 22.9 & **16.3** \\\\ TEDLIUM test & 6.3 & **4.6** & 5.1 & 4.8 & 7.8 & 5.1 \\\\ VoxPopuli en & 10.2 & 8.5 & **7.6** & 9.2 & 12.0 & 8.4 \\\\ WSJ eval92 & 5.0 & 4.3 & **2.9** & 13.4 & 5.3 & 3.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: WER % (\\(\\downarrow\\)) of English ASR using greedy search. Whisper models are trained on 438K hours of English audio, whereas OWSM v3 and v3.1 are trained on 73K hours. Bold text is the best result. Underlined text means our OWSM v3.1 outperforms OWSM v3.\n' +
      '\n' +
      '### Training configuration\n' +
      '\n' +
      'Following the previous OWSM models [9], our OWSM v3.1 is implemented in ESPnet2 [10] with PyTorch [43]. The detailed training hyperparameters are presented in Table 1. We employ distributed data parallel and mixed precision training. We also leverage FlashAttention [15] to improve the training efficiency. With FlashAttention, the batch size per GPU can be doubled (2 to 4 samples), which greatly reduces the training cost. OWSM v3.1 base and medium are trained with 16 and 64 NVIDIA A100 GPUs (40GB), respectively. Both models are trained for approximately 3 entire passes of the 180K hours of data. The training takes approximately 6 and 16 days for base and medium, respectively. Different from the previous OWSM v3 which is fine-tuned from v2, our new OWSM v3.1 models are trained from scratch, which has fewer total updates.\n' +
      '\n' +
      'Prior studies [18, 14] find that E-Branchformer converges more stably than Conformer. However, we find it very difficult to train 1B E-Branchformer based models on massively multilingual, multitask, and long-form data.4 This large-scale experiment requires a very small learning rate at the beginning of training, otherwise the loss will quickly stop decreasing. If we adopt the commonly used linear warmup learning rate schedule, we have to greatly reduce the peak learning rate or increase the warmup steps, both leading to inferior performance according to our preliminary explorations. To alleviate this issue, we propose a piecewise linear warmup learning rate schedule which increases the learning rate slowly at the beginning and quickly later during warmup. Specifically, the learning rate is linearly increased to a very small value (e.g., 5e-5) in the first 30K steps and then linearly increased to the desired peak learning rate in the second 30K steps. After warmup, it is decreased exponentially in the same way as the vanilla version. The proposed piecewise linear schedule enables successful training of our OWSM v3.1.\n' +
      '\n' +
      'Footnote 4: Based on our experience, this is mainly due to the 30s long-form data format. Even very small models have a hard time converging regardless of the architecture (encoder-decoder or CTC only).\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      'In this section, we compare our OWSM v3.1 models against the previous v3 version in a wide range of tasks and benchmarks under the same setup. We show that OWSM v3.1 outperforms OWSM v3 in most of these benchmarks. We also include the results of Whisper models for reference, but we note that the comparison might be unfair due to completely different training data.\n' +
      '\n' +
      '### English speech recognition\n' +
      '\n' +
      'Table 2 shows English ASR results on standard benchmarks prepared in ESPnet. We follow [9] to run inference with greedy search and apply the Whisper text normalizer before scoring. By comparing models at the same scale (base or medium), we have the following observations: (1) Our OWSM v3.1 model outperforms the previous OWSM v3 model in 8 out of 9 test sets at the medium scale. The improvement is especially large in Common Voice, FLEURS, LibriSpeech, Switchboard, VoxPopuli and WSI.5 This verifies the effectiveness of our E-Branchformer encoder. (2) At the medium scale, OWSM v3.1 even outperforms Whisper in LibriSpeech and Switchboard. At the base scale, OWSM v3.1 also achieves lower WERs in those sets as well as Common Voice. This demonstrates the competitive performance of OWSM models although they are trained on far less data compared to Whisper (73K vs 438K hours).\n' +
      '\n' +
      'Footnote 5: As discussed in [9], the WSJ training data is used by OWSM v3, but its transcripts are fully uppercased. The model might treat it as another low-resource language, which leads to poor results. In v3.1, we exclude WSJ during training and achieve a significantly lower WER.\n' +
      '\n' +
      '### Multilingual speech recognition\n' +
      '\n' +
      'Table 3 presents multilingual ASR results. Again, we follow [9] to compare different models in the same setup. We employ greedy decoding and apply the Whisper text normalizer before calculating WER or CER. We observe that our OWSM v3.1 medium model outperforms OWSM v3 in 10 out of 11 test sets across various languages usually by a large margin. Specifically, the average error rate is reduced from 18.8% to 15.2%. This demonstrates the superior performance of OWSM v3.1 in diverse languages.\n' +
      '\n' +
      'Compared to Whisper, OWSM v3.1 still falls behind in many European languages due to limited training data. In contrast, when there is a sufficient amount of data (e.g., Chinese and Japanese), OWSM v3.1 achieves strong performance and outperforms Whisper. This reveals the importance of training data quantity. In the future, we plan to include more data from other public sources like YODAS [44] to further improve OWSM.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Test set} & \\multirow{2}{*}{Language} & \\multirow{2}{*}{Metric} & \\multicolumn{4}{c}{Whisper} & \\multicolumn{4}{c}{OWSM v3} & \\multicolumn{4}{c}{OWSM v3.1 (ours)} \\\\ \\cline{3-10}  & & & data & base & small & medium & data & medium & data & base & medium \\\\ \\hline \\multirow{8}{*}{Multilingual LibriSpeech} & Spanish & & 11.1K & 14.5 & 9.1 & **6.1** & 2.0K & 11.7 & 2.0K & 18.5 & 9.0 \\\\  & French & & 9.8K & 25.2 & 13.6 & **9.7** & 2.5K & 14.1 & 2.5K & 24.2 & 12.1 \\\\  & German & & 13.3K & 19.9 & 11.5 & **8.1** & 3.7K & 11.9 & 3.7K & 18.7 & 10.8 \\\\  & Dutch & Dutch & 2.1K & 30.9 & 18.2 & **12.2** & 1.7K & 17.7 & 1.7K & 28.6 & 18.1 \\\\  & Italian & 2.6K & 32.9 & 21.3 & **15.6** & 0.7K & 24.5 & 0.7K & 33.7 & 20.2 \\\\  & Portuguese & & 8.6K & 23.5 & 13.8 & **8.9** & 0.3K & 28.2 & 0.3K & 44.9 & 21.6 \\\\  & Polish & & 4.3K & 25.2 & 12.5 & **6.8** & 0.3K & 37.0 & 0.3K & 49.7 & 25.2 \\\\ \\hline \\multirow{2}{*}{AISHELL-1} & Chinese & & 23.4K & 39.1 & 25.1 & 15.7 & 16.0K & 7.1 & 16.3K & 12.2 & **6.4** \\\\  & KsponSpeech eval-clean & & 27.0 & 24.0 & 17.6 & 20.5 & 23.8 & **16.7** \\\\  & Korean & CER & 8.0K & 22.9 & 15.4 & **12.8** & 1.0K & 22.6 & 1.0K & 26.1 & 18.9 \\\\  & ReazonSpeech & Japanese & & 7.1K & 54.1 & 32.5 & 25.3 & 18.9K & 11.3 & 18.9K & 11.2 & **7.9** \\\\ \\hline Average WER/CER (\\(\\downarrow\\)) & & & 28.7 & 17.9 & **12.6** & & 18.8 & & 26.5 & 15.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: WER/CER % (\\(\\downarrow\\)) of multilingual ASR using greedy search. Training data sizes (in hours) are also shown. **Bold** text is the best result. Underlined text means our OWSM v3.1 outperforms OWSM v3.1\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '### Long-form speech recognition\n' +
      '\n' +
      'Table 5 presents the long-form English ASR results on the TEDLIUM2 test set. Similar to [1, 9], OWSM takes an entire audio recording as input and generates transcripts in chunks. Each chunk has a fixed length of 30s and the chunk is gradually shifted based on the predicted timestamp. Our OWSM v3.1 medium achieves a WER of 5.7, compared to 9.2 of OWSM v3. This demonstrates the robustness of OWSM v3.1 against long-form audio; the predicted timestamp might be more accurate as well. OWSM v3.1 still falls behind whisper at both base and medium scales, likely because (1) our training data is only around one quarter of Whisper\'s training data, and (2) many public datasets used by OWSM do not provide unsegmented long-form versions and we have to use the segmented short audio for training, which leads to a mismatch between training and decoding. In the future, we plan to add more long-form training data to mitigate this issue.\n' +
      '\n' +
      '### Language identification\n' +
      '\n' +
      'Table 6 reports the accuracy of language identification on the FLEURS test set prepared in ESPnet. We notice a degradation in OWSM v3.1 compared to the previous OWSM v3, but OWSM v3.1 medium is still much better than Whisper medium because our model uses the massively multilingual FLEURS and Common Voice data for training. We also find that our OWSM v3.1 benefits more from scaling up compared to Whisper. Specifically, from base to medium, the accuracy of OWSM v3.1 is almost doubled (41.9% to 75.6%), while the accuracy of Whisper is only slightly increased (47.6% to 54.8%). The possible reasons are: (1) our OWSM supports more languages and language pairs, which is more challenging for smaller models to learn; (2) the current OWSM v3.1 base model might still underfit the training data, which can be improved by longer training runs or more optimized hyperparameters. We will further investigate this scaling behavior by training models of other sizes like "small" in addition to "base" and "medium".\n' +
      '\n' +
      '### Spoken language understanding\n' +
      '\n' +
      'Whisper and OWSM cannot perform SLU tasks directly. In order to investigate the capability of the encoder, we conduct preliminary experiments using the recently proposed SLUE-PERB benchmark [16]. Specifically, the pre-trained speech encoder is frozen and a randomly initialized shallow decoder is trained on task-specific SLU training data. The model is then evaluated on the corresponding SLU test data. This evaluation procedure is similar to the widely used SUPERB benchmark [45]. We evaluate the medium-sized Whisper, OWSM v3 and OWSM v3.1 models on four SLU tasks, i.e., sentiment analysis (SA), named entity recognition (NER), named entity localization (NEL) and dialog act classification (DAC). As shown in Table 7, our OWSM v3.1 outperforms the others by a large margin in NER, NEL and DAC, which verifies the strong capacity of our E-Branchformer encoder. Note that both OWSM models achieve significantly higher frame-level F1 scores than Whisper for NEL, because OWSM uses joint CTC training which better aligns the encoder representations with the textual information.\n' +
      '\n' +
      '## 4 Future directions\n' +
      '\n' +
      'In this work, we demonstrate that E-Branchformer greatly improves the overall performance and efficiency of massively multilingual multitask speech foundation models without the use of any additional training data. These findings inspire us to explore several future directions.\n' +
      '\n' +
      '* OWSM v3.1 is trained on public data with various licenses, some of which have more strict constraints. Now we are training another model using a subset of data with free licenses. We will publicly release this model and training data in a near future.\n' +
      '* Data is one of the most important factors to consider when developing speech foundation models. OWSM v3.1 still falls behind Whisper when there is limited training data. We need to include more public data such as YODAS [44] to further improve its performance.\n' +
      '* Developing more powerful and efficient speech encoder architectures is still very important to the speech community even in the era of big data and big models.\n' +
      '* Using pre-trained models like OWSM is very promising for other downstream tasks like spoken language understanding [46, 47], which can be explored more especially in the context of continual learning.\n' +
      '* Current speech foundation models are usually very large and slow. We need various compression algorithms to improve efficiency, like distillation [48], pruning [49], joint distillation and pruning [50], or more flexible input-dependent dynamic architectures [51].\n' +
      '* Speech models can be integrated with textual LLMs, a promising avenue towards general-purpose multimodal foundation models [52, 53].\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'In this work, we present OWSM v3.1, a better and faster Open Whisper-style Speech Model based on E-Branchformer. Although trained on the same amount of data, our OWSM v3.1 achieves better results than the previous OWSM v3 in a vast majority of evaluation conditions, while showing up to 25% faster inference speed. We publicly release all scripts, pre-trained model weights and training logs to promote transparency and facilitate the development of large speech foundation models.\n' +
      '\n' +
      '## 6 Acknowledgements\n' +
      '\n' +
      'Our computing resources are provided by PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. Mcleavey, and I. Sutskever, "Robust speech recognition via large-scale weak supervision," in _Proc. ICML_, 2023.\n' +
      '* [2] Y. Zhang, W. Han, J. Qin, Y. Wang, A. Bapna, Z. Chen, N. Chen, B. Li, V. Axelrod, G. Wang _et al._, "Google usm: Scaling automatic speech recognition beyond 100 languages," _arXiv preprint arXiv:2303.01037_, 2023.\n' +
      '* [3] L. Barrault, Y.-A. Chung, M. C. Meglioli, D. Dale, N. Dong, M. Duppenthaler, P.-A. Duquenne, B. Ellis, H. Elsahar, J. Haeheim _et al._, "Seamless: Multilingual expressive and streaming speech translation," _arXiv preprint arXiv:2312.05187_, 2023.\n' +
      '* [4] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin _et al._, "Opt: Open pre-trained transformer language models," _arXiv:2205.01068_, 2022.\n' +
      '* [5] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar_et al._, "Llama: Open and efficient foundation language models," _arXiv:2302.13971_, 2023.\n' +
      '* [2] Z. Liu, A. Qiao, W. Neiswanger, H. Wang, B. Tan, T. Tao, J. Li, Y. Wang, S. Sun, O. Pangarkar _et al._, "Llm360: Towards fully transparent open-source lms," _arXiv preprint arXiv:2312.06550_, 2023.\n' +
      '* [3] W. Chen, X. Chang, Y. Peng, Z. Ni, S. Maiti, and S. Watanabe, "Reducing Barriers to Self-Supervised Learning: HuBERT Pretraining with Academic Compute," in _Proc. Interspeech_, 2023.\n' +
      '* [4] W. Chen, J. Shi, B. Yan, D. Berrebbi, W. Zhang, Y. Peng, X. Chang, S. Maiti, and S. Watanabe, "Joint prediction and denoising for large-scale multilingual self-supervised learning," in _Proc. ASRU_, 2023.\n' +
      '* [5] Y. Peng, J. Tian, B. Yan, D. Berrebbi, X. Chang, X. Li, J. Shi, S. Arora, W. Chen, R. Sharma, W. Zhang, Y. Sudo, M. Shakael, J. wcon Jung, S. Maiti, and S. Watanabe, "Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data," in _Proc. ASRU_, 2023.\n' +
      '* [6] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Rendchuintala, and T. Ochiai, "ESPnet: End-to-End Speech Processing Toolkit," in _Proc. Interspeech_, 2018.\n' +
      '* [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," in _Proc. NeurIPS_, 2017.\n' +
      '* [8] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, "Conformer: Convolution-augmented Transformer for Speech Recognition," in _Proc. Interspeech_, 2020.\n' +
      '* [9] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, "Branchformer: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding," in _Proc. ICML_, 2022.\n' +
      '* [10] K. Kim, F. Wu, Y. Peng, J. Pan, P. Sridhar, K. J. Han, and S. Watanabe, "E-branchformer: Branchformer with enhanced merging for speech recognition," in _Proc. SLT_, 2023.\n' +
      '* [11] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Re, "Flashattention: Fast and memory-efficient exact attention with io-awareness," in _Proc. NeurIPS_, 2022.\n' +
      '* [12] S. Arora, R. Sharma, A. Pasad, H. Dhamyal, W. Chen, S. Shon, H.-Y. Lee, K. Livescu, and S. Watanabe, "SLUE-PERB: A Spoken Language Understanding Performance Benchmark and Toolkit," in _ASRU SPARKS Workshop_, 2023.\n' +
      '* [13] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y. Higuchi, H. Inaguma, N. Kamo, C. Li, D. Garcia-Romero, J. Shi _et al._, "Recent developments on espnet toolkit boosted by conformer," in _Proc. ICASSP_, 2021.\n' +
      '* [14] Y. Peng, K. Kim, F. Wu, B. Yan, S. Arora, W. Chen, J. Tang, S. Shon, P. Sridhar, and S. Watanabe, "A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks," in _Proc. Interspeech_, 2023.\n' +
      '* [15] H. Bu _et al._, "AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline," in _Proc. O-COCOSDA_, 2017.\n' +
      '* [16] C. Wang _et al._, "CoVoST 2 and Massively Multilingual Speech Translation," in _Interspeech_, 2021.\n' +
      '* [17] G. Chen _et al._, "GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio," in _Proc. Interspeech_, 2021.\n' +
      '* [18] V. Panayotov _et al._, "Librispeech: An ASR corpus based on public domain audio books," in _ICASSP_, 2015.\n' +
      '* [19] R. Cattoni _et al._, "Must-c: A multilingual corpus for end-to-end speech translation," _Computer speech & language_, vol. 66, p. 101155, 2021.\n' +
      '* [20] P. K. O\'Neill _et al._, "Spsigspeech: 5.000 hours of transcribed financial audio for fully formatted end-to-end speech recognition," _arXiv:2104.02014_, 2021.\n' +
      '* [21] F. Hernandez _et al._, "Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation," in _Speech & Computer_, 2018, pp. 198-208.\n' +
      '* [22] R. Ye _et al._, "Gigast: A 10,000-hour pseudo speech translation corpus," _arXiv:2204.03939_, 2022.\n' +
      '* [23] V. Pratap _et al._, "Mls: A large-scale multilingual dataset for speech research," _arXiv:2012.03411_, 2020.\n' +
      '* [24] B. Zhang _et al._, "Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition," in _Proc. ICASSP_, 2022.\n' +
      '* [25] "aidatatang_200zh, a free Chinese Mandarin speech corpus by Beijing DataTang Technology Co., Ltd."\n' +
      '* [26] J. Carletta, "Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus," _Lang. Res. Eval._, vol. 41, pp. 181-190, 2007.\n' +
      '* [27] "The babel program: [https://www.iarpa.gov/index.php/research-babel](https://www.iarpa.gov/index.php/research-babel)."\n' +
      '* [28] R. Ardila _et al._, "Common voice: A massively-multilingual speech corpus," _arXiv:1912.06670_, 2019.\n' +
      '* [29] J. Godfrey _et al._, "SWITCHBOARD: telephone speech corpus for research and development," in _Proc. ICASSP_, 1992.\n' +
      '* [30] M. Post _et al._, "Improved speech-to-text translation with the fisher and calmloem Spanish-English speech translation corpus," in _Proc. IWSLT_, 2013.\n' +
      '* [31] A. Conneau _et al._, "FLEURS: Few-Shot Learning Evaluation of Universal Representations of Speech," in _Proc. SLT_, 2022.\n' +
      '* [32] J.-U. Bang _et al._, "Ksponspeech: Korean spontaneous speech corpus for automatic speech recognition," _Applied Sciences_, vol. 10, no. 19, p. 6936, 2020.\n' +
      '* [33] Z. Yang _et al._, "Open source magicdata-ramc: A rich annotated mandarin conversational (ramc) speech dataset," _arXiv:2203.16844_, 2022.\n' +
      '* [34] Y. Yin, D. Mori _et al._, "ReazonSpeech: A Free and Massive Corpus for Japanese ASR," 2023.\n' +
      '* [35] A. Slizhikova _et al._, "Russian Open Speech To Text (STT/ASR) Dataset," 2020. [Online]. Available: [https://github.com/snakers4/open_att](https://github.com/snakers4/open_att)\n' +
      '* [36] J. Yamagishi _et al._, "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit," 2019.\n' +
      '* [37] "VoxForge: [http://www.voxforge.org/](http://www.voxforge.org/)."\n' +
      '* [38] C. Wang _et al._, "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation," in _Proc. ACL_, 2021.\n' +
      '* [39] A. Paszke _et al._, "Pytorch: An imperative style, high-performance deep learning library," in _Proc. NeurIPS_, 2019.\n' +
      '* [40] X. Li, S. Takamichi, T. Saeki, W. Chen, S. Shiota, and S. Watanabe, "Yodas: Youtube-oriented dataset for audio and speech," in _Proc. ASRU_, 2023.\n' +
      '* [41] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K.tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, "SUPERB: Speech Processing Universal PERformance Benchmark," in _Proc. Interspeech_, 2021.\n' +
      '* [42] Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, S. Dalmia, X. Chang, and S. Watanabe, "A Study on the Integration of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding," in _Proc. SLT_, 2022.\n' +
      '* [43] S. Arora, H. Futami, J.-w. Jung, Y. Peng, R. Sharma, Y. Kashiwagi, E. Tsuno, and S. Watanabe, "Universal: Universal spoken language understanding for diverse classification and sequence generation tasks with a single network," _arXiv preprint arXiv:2310.02973_, 2023.\n' +
      '* [44] H.-J. Chang, S.-w. Yang, and H.-y. Lee, "Distillhubert: Speech representation learning by layer-wise distillation of hidden-unit bert," in _Proc. ICASSP_, 2022.\n' +
      '\n' +
      '* [49] Y. Peng, K. Kim, F. Wu, P. Sridhar, and S. Watanabe, "Structured pruning of self-supervised pre-trained models for speech recognition and understanding," in _Proc. ICASSP_, 2023.\n' +
      '* [50] Y. Peng, Y. Sudo, S. Muhammad, and S. Watanabe, "DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models," in _Proc. Interspeech_, 2023.\n' +
      '* [51] Y. Peng, J. Lee, and S. Watanabe, "I3d: Transformer architectures with input-dependent dynamic depth for speech recognition," in _Proc. ICASSP_, 2023.\n' +
      '* [52] M. Wang, W. Han, I. Shafran, Z. Wu, C.-C. Chiu, Y. Cao, N. Chen, Y. Zhang, H. Soltau, P. K. Rubenstein, L. Zilka, D. Yu, G. Pundak, N. Siddhartha, J. Schalkwyk, and Y. Wu, "Slm: Bridge the thin gap between speech and text foundation models," in _Proc. ASRU_, 2023.\n' +
      '* [53] C. Tang, W. Yu, G. Sun, X. Chen, T. Tan, W. Li, L. Lu, Z. Ma, and C. Zhang, "Salmom: Towards generic hearing abilities for large language models," _arXiv preprint arXiv:2310.13289_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>