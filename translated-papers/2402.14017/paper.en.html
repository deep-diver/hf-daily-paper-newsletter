<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# D-Flow: Differentiating through Flows for Controlled Generation\n' +
      '\n' +
      'Heli Ben-Hamu\n' +
      '\n' +
      'Omri Puny\n' +
      '\n' +
      'Itai Gat\n' +
      '\n' +
      'Brian Karrer\n' +
      '\n' +
      'Uriel Singer\n' +
      '\n' +
      'Yaron Lipman\n' +
      '\n' +
      'Work done while interning at Meta. \\({}^{1}\\)Meta \\({}^{2}\\)Weizmann Institute of Science. Correspondence to: Heli Ben-Hamu \\(<\\)heli.benhamu@weizmann.ac.il\\(>\\).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce _D-Flow_, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all.\n' +
      '\n' +
      'Machine Learning, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Controlled generation from generative priors is of great interest in many domains. Various problems such as conditional generation, inverse problems, sample editing etc., can all be framed as a controlled generation problem. In this work we focus on controlled generation from diffusion/flow generative models (Song and Ermon, 2019; Ho et al., 2020; Lipman et al., 2023) as they are the current state-of-the-art generative approaches across different data modalities.\n' +
      '\n' +
      'There are three main approaches for controlled generation from diffusion/flow models: (i) conditional training, where the model receives the condition as an additional input during training (Song et al., 2020; Dhariwal and Nichol, 2021; Ho and Salimans, 2022), although performing very well this approach requires task specific training of a generative model which in cases may be prohibitive; (ii) training-free approaches that modify the generation process of a pre-trained model, adding additional guidance (Bar-Tal et al., 2023; Yu et al., 2023). The guidance is usually built upon strong assumptions on the generation process that can lead to errors in the generation and mostly limit the method to observations that are linear in the target (Kawar et al., 2022; Chung et al., 2022; Song et al., 2023; Pokle et al., 2023); lastly, (iii) adopt a variational perspective, framing the controlled generation as an optimization problem (Graikos et al., 2023; Mardani et al., 2023; Wallace et al., 2023; Samuel et al., 2023), requiring only a differentiable cost to enforce the control. This paper belongs to this third class.\n' +
      '\n' +
      'The goal of this paper is to introduce a framework for adding controlled generation to a pre-trained Diffusion or Flow-Matching (FM) model based on _differentiation through the ODE sampling process_. Our key observation is that for Diffusion/FM models trained with standard Gaussian probability paths, differentiating an arbitrary loss \\(\\mathcal{L}(x)\\) through the generation process of \\(x\\) with respect to the initial point, \\(x_{0}\\), projects the gradient \\(\\nabla_{x}\\mathcal{L}\\) onto the "data manifold", i.e., onto major data directions at \\(x\\), implicitly injecting a valuable prior. Based on this observation we advocate a simple general algorithm that minimizes an arbitrary cost function \\(\\mathcal{L}(x)\\), representing the desired control, as a function of the source noise point \\(x_{0}\\) used to generate \\(x\\). That is,\n' +
      '\n' +
      '\\[\\min_{x_{0}}\\ \\mathcal{L}(x). \\tag{1}\\]\n' +
      '\n' +
      'Differentiating through a generator of a GAN or a normal\n' +
      '\n' +
      'Figure 1: Free-form inpainting with a latent T2I FM model (Ground truth image is taken from the MS-COCO validation set), conditionally generated molecule and audio inpainting using D-Flow.\n' +
      '\n' +
      'izing flow was proven generally useful for controlled generation (Bora et al., 2017; Asim et al., 2020; Whang et al., 2021). Recently, (Wallace et al., 2023; Samuel et al., 2023b) have been suggesting to differentiate through a discrete diffusion solver for the particular tasks of incorporating classifier guidance and generating rare concepts. In this paper we generalize this idea in two ways: (i) we consider general flow models trained with Gaussian probability paths, including Diffusion and Flow-Matching models; and (ii) we demonstrate, both theoretically and practically, that the inductive bias injected by differentiating through the flow is applicable to a much wider class of problems modeled by general cost functions.\n' +
      '\n' +
      'We experiment with our method on a variety of settings and applications: Inverse problems on images using conditional ImageNet and text-2-image (T2I) generative priors, conditional molecule generation with QM9 unconditional generative priors, and audio inpainting and super-resolution with unconditional generative prior. In all application we were able to achieve state of the art performance without carefully tuning the algorithm across domains and applications. One drawback of our method is the relative long time for generation (usually \\(5-15\\) minutes on ImageNet-128 on an NVidia V100 GPU) compared to some baselines, however the method\'s simplicity and its superior results can justify its usage and adaptation in many use cases. Furthermore, we believe there is great room for speed improvement.\n' +
      '\n' +
      'To summarize, our contributions are:\n' +
      '\n' +
      '* We formulate the controlled generation problem as a simple source point optimization problem using general flow generative models.\n' +
      '* We show that source point optimization of flows trained with Gaussian probability paths inject an implicit bias exhibiting a data-manifold projection behaviour to the cost function\'s gradient.\n' +
      '* We empirically show the generality and the effectiveness of the proposed approach for different domains.\n' +
      '\n' +
      '## 2 Preliminaries\n' +
      '\n' +
      '**Flow models.** Generative flow models, including Continuous Normalizing Flows (CNFs) (Chen et al., 2018; Lipman et al., 2023) and (deterministic sampling of) Diffusion Models (Song et al., 2020) generate samples \\(x(1)\\in\\mathbb{R}^{d}\\) by first sampling from some source (noise) distribution \\(x(0)\\sim p_{0}(x_{0})\\) and then solving an Ordinary Differential Equation (ODE)\n' +
      '\n' +
      '\\[\\dot{x}(t)=u_{t}(x(t)), \\tag{2}\\]\n' +
      '\n' +
      'from time \\(t=0\\) to time \\(t=1\\)1 using a predetermined velocity field \\(u:[0,1]\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}\\). We denote by \\(p_{1}\\) the distribution and density function of \\(x(1)\\) given \\(x(0)\\sim p_{0}(x(0))\\).\n' +
      '\n' +
      'Footnote 1: In this paper we use the convention of \\(t=0\\) corresponds to noise, and \\(t=1\\) to data.\n' +
      '\n' +
      '## 3 Controlled generation via source point optimization\n' +
      '\n' +
      'Given a pre-trained (frozen) flow model, \\(u_{t}(x)\\), represented by a neural network and some cost function \\(\\mathcal{L}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{+}\\), our goal is to find likely samples \\(x\\) that provide low cost \\(\\mathcal{L}(x)\\) and are likely under the flow model\'s distribution \\(p_{1}\\). We advocate a general framework formulating this problem as the following optimization problem\n' +
      '\n' +
      '\\[\\min_{x_{0}}\\quad\\mathcal{L}(x(1)) \\tag{3}\\]\n' +
      '\n' +
      'where in general \\(\\mathcal{L}\\) can also incorporate multiple costs including potentially a regularization term that can depend on \\(x_{0}\\) and \\(u\\),\n' +
      '\n' +
      '\\[\\tilde{\\mathcal{L}}(x)=\\mathcal{L}(x)+\\mathcal{R}(x_{0},u). \\tag{4}\\]\n' +
      '\n' +
      'In this formulation, the sample \\(x(1)\\) is constrained to be a solution of the ODE 2 with initial boundary condition \\(x(0)=x_{0}\\), where \\(x_{0}\\) is the (only) optimized quantity, \\(\\mathcal{L}\\) is the desired cost function. Optimizing equation 3 is done by computing the gradients of the loss w.r.t. the optimized variable \\(x_{0}\\) as listed in Algorithm 1. We call this method _D-Flow_. To better understand the generality of this framework we next consider several instantiations of equation 3.\n' +
      '\n' +
      '```\n' +
      '0: cost \\(\\mathcal{L}\\), pre-trained flow model \\(u_{t}(x)\\)  Initialize \\(x_{0}^{(0)}=x_{0}\\) for\\(i=1,\\dots,N\\)do \\(x^{(i)}(1)\\leftarrow\\texttt{solve}(x_{0}^{(i)},u_{t})\\) \\(x_{0}^{(i+1)}\\leftarrow\\texttt{optimize\\_step}(x_{0}^{(i)},\\nabla_{x_{0}} \\mathcal{L}(x^{(i)}(1)))\\) endfor return\\(x^{N}(1)\\)\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1** D-Flow.\n' +
      '\n' +
      '### Cost functions\n' +
      '\n' +
      '**Reversed sampling.** First, consider the simple case where \\(\\mathcal{L}(x)=\\left\\|x-y\\right\\|^{2}\\). In this case, the solution of 3 will be the \\(x_{0}\\) that its ODE trajectory reaches \\(y\\) at \\(t=1\\), i.e., \\(x(1)=y\\). Note that since (under some mild assumptions on \\(u_{t}(x)\\)) equation 2 defines a diffeomorphism \\(\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}\\) for an arbitrary \\(y\\in\\mathbb{R}^{d}\\) there exists a unique solution \\(x_{0}\\in\\mathbb{R}^{d}\\) to equation 3.\n' +
      '\n' +
      'Inverse problems.In this case we have access to some known corruption function \\(H:\\mathbb{R}^{d}\\to\\mathbb{R}^{n}\\) and a corrupted sample\n' +
      '\n' +
      '\\[y=H(x_{*})+\\epsilon, \\tag{5}\\]\n' +
      '\n' +
      'where \\(\\epsilon\\sim\\mathcal{N}(\\epsilon)\\) is an optional additive noise, and the cost function is usually\n' +
      '\n' +
      '\\[\\mathcal{L}(x)=\\|H(x)-y\\|^{2} \\tag{6}\\]\n' +
      '\n' +
      'where the norm can be some arbitrary \\(L_{p}\\) norm or even a general loss \\(\\ell(H(x),y)\\) comparing \\(H(x)\\) and \\(y\\). Specific choices of the corruption function \\(H\\) can lead to common applications: _Image inpainting_ corresponds to choosing the corruption function \\(H\\) to sub-sample known \\(n<d\\) pixels out of \\(d\\) total pixels; _Image deblurring_ corresponds to taking \\(H:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}\\) to be a blurring function, e.g., a convolution with a blurring kernel; _Super-resolution_ corresponds to \\(H:\\mathbb{R}^{d}\\to\\mathbb{R}^{d/k}\\) lowering the dimension by a factor of \\(k\\).\n' +
      '\n' +
      'Conditional sampling.Another important application is to guide the sampling process to satisfy some conditioning \\(y\\). In this case we can take \\(\\mathcal{L}(x)\\) to encourage a classifier or some energy function to reach a particular class or energy \\(y\\). For example, if \\(\\mathcal{F}:\\mathbb{R}^{d}\\to\\mathbb{R}\\) is some function and we would like to generate a sample from a certain level set \\(c\\in\\mathbb{R}\\) we can use the loss\n' +
      '\n' +
      '\\[\\mathcal{L}(x)=\\left(\\mathcal{F}(x)-c\\right)^{2}. \\tag{7}\\]\n' +
      '\n' +
      '### Initialization\n' +
      '\n' +
      'The initialization of \\(x_{0}\\) can have a great impact on the convergence of the optimization of equation 3. A natural choice will be to initialize \\(x_{0}\\) with a sample from the source distribution \\(p_{0}(x_{0})\\). We find that for cases when an observed signal \\(y\\) provides a lot of information about the desired \\(x\\) one can improve the convergence speed of the optimization. For example, in linear inverse problems on images, where the observed \\(y\\) has a strong prior on the structure of the image, it is beneficial to initialize \\(x_{0}\\) with a blend of a sample from the source distribution and the backward solution of the ODE from \\(t=1\\) to \\(t=0\\) of \\(y\\):\n' +
      '\n' +
      '\\[x_{0}=\\sqrt{\\alpha}\\cdot y(0)+\\sqrt{1-\\alpha}\\cdot z \\tag{8}\\]\n' +
      '\n' +
      'where \\(z\\sim p_{0}(x_{0})\\) and \\(y(0)=y+\\int_{1}^{0}u(t,y(t))dt\\).\n' +
      '\n' +
      '### Regularizations\n' +
      '\n' +
      'The formulation in equation 3 allows including different regularizations \\(\\mathcal{R}\\) (equation 4) discussed next. Maybe the most intriguing of these regularizations, and the main point of this paper, is the _implicit regularization_, i.e., corresponding to \\(\\mathcal{R}\\equiv 0\\), discussed last in what follows.\n' +
      '\n' +
      'Regularizing the target \\(x(1)\\).Maybe the most natural is incorporating the negative log likelihood (NLL) of the sample \\(x(1)\\), i.e., \\(\\mathcal{R}=-\\log p_{1}(x(1))\\) in equation 4. This prior can be incorporated by augmenting \\(x(t)\\in\\mathbb{R}^{d}\\) with an extra coordinate \\(z\\in\\mathbb{R}\\) and formulate equation 3 as\n' +
      '\n' +
      '\\[\\min_{x_{0}} \\mathcal{L}(x(1))-z(1)\\] (9a) s.t. \\[\\dot{x}(t)=u_{t}(x(t)), x(0)=x_{0} \\tag{9b}\\] \\[\\dot{z}(t)=-\\mathrm{div}\\,u_{t}(x(t)), z(0)=\\log p_{0}(x_{0}) \\tag{9c}\\]\n' +
      '\n' +
      'Indeed, solving the ODE system defined by equations 9b and 9c for times \\(t\\in[0,1]\\) provides \\(z(1)=\\log p_{1}(x(1))\\), see (Chen et al., 2018). However, aside from the extra complexity introduced by the divergence term in the ODE in equation 9c (see e.g., (Grathwohl et al., 2018) for ways to deal with this type of ODE) it is not clear whether likelihood is a good prior in deep generative models in high dimensions (Nalisnick et al., 2019); In Figure 3 we compare bits-per-dimension (BPD) of a test image of ImageNet-128 and a version of this image with a middle square masked with zeros providing a more likely image according to our ImageNet trained flow model.\n' +
      '\n' +
      'Regularizing the source \\(x(0)=x_{0}\\).Another option is to regularize the source point \\(x(0)=x_{0}\\). The first choice would again be to incorporate the NLL of the noise sample, i.e., \\(\\mathcal{R}=-\\log p_{0}(x_{0})\\), which for standard noise \\(p_{0}(x_{0})=\\mathcal{N}(x_{0}|0,I)\\) would reduce to \\(\\mathcal{R}=c+\\frac{1}{2}\\left\\|x_{0}\\right\\|^{2}\\), where \\(c\\) is a constant independent of \\(x_{0}\\). This however, would attract \\(x_{0}\\) towards the most likely all zero mean but far from most of the probability mass at norm \\(\\sqrt{d}\\).\n' +
      '\n' +
      'Figure 3: BPD of two images in an ImageNet-128 model.\n' +
      '\n' +
      'Figure 2: Intermediate \\(x(1)\\) during optimization. Given a distorted image and randomly initialized \\(x_{0}\\) defining the initial \\(x(1)\\), our optimization travels close to the natural image manifold passing through in-distribution images on its way to the GT sample from the face-blurred ImageNet-128 validation set.\n' +
      '\n' +
      'Following (Samuel et al., 2023a) we instead prefer to make sure \\(x_{0}\\) stays in the area where most mass of \\(p_{0}\\) is concentrated and therefore use the \\(\\chi^{d}\\) distribution, which is defined as the probability distribution \\(p(r)\\) of the random variable \\(r=\\left\\|x_{0}\\right\\|\\) where \\(x_{0}\\sim\\mathcal{N}(x_{0}|0,I)\\) is again the standard normal distribution. The NLL of \\(r\\) in this case is\n' +
      '\n' +
      '\\[\\mathcal{R}=-\\log p(r)=c+(d-1)\\log\\left\\|x_{0}\\right\\|-\\frac{\\left\\|x_{0} \\right\\|^{2}}{2}, \\tag{10}\\]\n' +
      '\n' +
      'where \\(c\\) is a constant independent of \\(x_{0}\\).\n' +
      '\n' +
      'Implicit regularization.Maybe the most interesting and potentially useful regularization in our formulation (equation 3) comes from the choice of optimizing the cost \\(\\mathcal{L}(x(1))\\) as a function of the source point \\(x(0)=x_{0}\\). For standard diffusion/flow models that are trained to zero loss:\n' +
      '\n' +
      'Optimizing the cost \\(\\mathcal{L}(x(1))\\) with respect to \\(x_{0}\\) follows the data distribution \\(p_{1}(x_{1})\\) by projecting the gradient \\(\\nabla_{x(1)}\\mathcal{L}(x(1))\\) with the local data covariance matrix.\n' +
      '\n' +
      'This is intuitively illustrated in Figure 4: while moving in direction of the gradient \\(\\nabla_{x(1)}\\mathcal{L}(x(1))\\) generally moves away from the data distribution (in pink), differentiating w.r.t. \\(x(0)\\) projects this gradient onto high variance data directions and consequently staying close to the data distribution. To exemplify this phenomena we show in Figure 2 optimization steps \\(x^{(0)}(1),x^{(2)}(1),x^{(4)}(1),\\ldots\\) of a loss \\(\\mathcal{L}(x)=\\left\\|H(x)-H(\\bar{x})\\right\\|^{2}\\), where \\(H\\) is a linear matrix that subsamples a (random) subset of the image\'s pixels consisting of \\(90\\%\\) of the total number of pixels, and \\(\\bar{x}\\) is a target image (different from the initial \\(x^{(0)}(1)\\)). The sampling process here is using an ImageNet trained flow model with the class condition \'bulbul\'. As can be seen in this sequence of images, the intermediate steps of the optimization stay close to the distribution and pass through different sub-species of the bulb bird. In the next section we provide a precise mathematical statement supporting this claim but for now let us provide some intuitive explanation.\n' +
      '\n' +
      '### Practical implementation\n' +
      '\n' +
      'The practical implementation of Algorithm 1 requires three algorithmic choices. First, one needs to decide how to initialize \\(x_{0}\\). In all experiment we either initialize \\(x_{0}\\) as a sample from the source distribution, i.e., normal Gaussian, or we use a variance preserving blend of a normal Gaussian with the backward solution from \\(t=1\\) to \\(t=0\\) of the observed signal when possible. Second, we need to choose the solver used to parameterize \\(x(1)\\). To this end we utilize the torchdiffeq package (Chen, 2018), providing a wide class of differentiable ODE solvers. Backpropagating through the solver can be expensive in memory and we therefore use gradient checkpointing to reduce memory consumption at the cost of runtime. In most of our experiments we use the midpoint method with 6 function evaluations. Lastly, we need to choose the optimizer for the gradient step. Since the optimization we perform is not stochastic we choose to use the LBFGS algorithm with line search in all experiments. The runtime of the optimization depends on the problem but typically ranges from \\(5-15\\) minutes per sample. For large text-2-image and text-2-audio models run times are higher and can reach \\(30-40\\) minutes.\n' +
      '\n' +
      '## 4 Theory\n' +
      '\n' +
      'In this section we provide the theoretical support to the implicit regularization claim made in the previous section. First, we revisit the family of Affine Gaussian Probability Paths (AGPP) taking noise to data that are used to supervise diffusion/flow models. When diffusion/flow models reach zero loss they reproduce these probability paths and we will therefore use them to analyze the implicit bias. Second, we use the method of adjoint dynamics to provide an explicit formula for the gradient \\(\\nabla_{x_{0}}\\mathcal{L}(x(1))\\) under the AGPP assumption, and consequently derive the asymptotic change (velocity vector) in \\(x(1)\\). Lastly, we interpret this velocity vector of \\(x(1)\\) to demonstrate why it is pointing in the direction of the data distribution \\(p_{1}(x)\\).\n' +
      '\n' +
      'Affine Gaussian Probability Paths.Diffusion and recent flow based models use Affine Gaussian Probability Path (AGPP) to supervise their training. In particular, denoting \\(p_{0}=\\mathcal{N}(0,\\sigma_{0}^{2}I)\\) the Gaussian noise (source) distribution and \\(p_{1}\\) data (target) distribution, an AGPP is defined by\n' +
      '\n' +
      '\\[p_{t}(x)=\\int p_{t}(x|x_{1})p_{1}(x_{1})dx_{1}, \\tag{11}\\]\n' +
      '\n' +
      'where \\(p_{t}(x|x_{1})=\\mathcal{N}(x|\\alpha_{t}x_{1},\\sigma_{t}^{2}I)\\) is a Gaussian kernel and \\(\\alpha_{t},\\sigma_{t}:[0,1]\\rightarrow[0,1]\\) are called the _scheduler_, satisfying \\(\\alpha_{0}=0\\), \\(\\sigma_{1}\\approx 0\\), and \\(\\alpha_{1}=1=\\sigma_{0}\\), consequently guaranteeing that \\(p_{t}\\) interpolates (exactly or approximately) the source and target distributions at times \\(t=0\\) and \\(t=1\\), respectively. The velocity field that generates this probability path and coincide with the velocity field trained by diffusion/flow models at zero loss is (Lipman et al., 2023; Shaul et al., 2023)\n' +
      '\n' +
      '\\[u_{t}(x)=\\int\\left[a_{t}x+b_{t}x_{1}\\right]p_{t}(x_{1}|x)dx_{1} \\tag{12}\\]\n' +
      '\n' +
      'where using Bayes\' Theorem\n' +
      '\n' +
      '\\[p_{t}(x_{1}|x)=\\frac{p_{t}(x|x_{1})p_{1}(x_{1})}{p_{t}(x)}, \\tag{13}\\]\n' +
      '\n' +
      'Figure 4: Implicit bias in differentiating through the solver.\n' +
      '\n' +
      '\\[a_{t}=\\frac{\\dot{\\sigma}_{t}}{\\sigma_{t}},\\quad b_{t}=\\dot{\\alpha}_{t}-\\alpha_{t} \\frac{\\dot{\\sigma}_{t}}{\\sigma_{t}}. \\tag{14}\\]\n' +
      '\n' +
      'Differentiating through the solver.When diffusion/flow models are optimized to a zero loss they perfectly reproduce the AGPP velocity field, i.e., equation 12 (Lipman et al., 2023). For this velocity field we find a formula for the cost\'s gradient with respect to \\(x_{0}\\):\n' +
      '\n' +
      '**Theorem 4.1**.: _For AGPP velocity field \\(u_{t}\\) (see equation 12) and \\(x(t)\\) defined via equation 2 the differential of \\(x(1)\\) as a function of \\(x_{0}\\) is_\n' +
      '\n' +
      '\\[D_{x_{0}}x(1)=\\sigma_{1}\\exp\\left[\\int_{0}^{1}\\gamma_{t}\\mathrm{Var}(x_{1}|x( t))dt\\right], \\tag{15}\\]\n' +
      '\n' +
      '_where \\(\\gamma_{t}=\\frac{1}{2}\\frac{d}{dt}\\mathrm{snr}(t)\\) and we define \\(\\mathrm{snr}(t)=\\frac{\\alpha_{t}^{2}}{\\sigma_{t}^{2}}\\)._\n' +
      '\n' +
      'The proof is given in Appendix A. In the exact case where \\(\\sigma_{1}=0\\) we also have \\(\\int_{0}^{1}\\gamma_{t}dt=\\infty\\), nevertheless we show in Appendix A that \\(D_{x_{0}}x(1)\\) is well defined also in this case. Now, the matrix \\(D_{x_{0}}x(1)\\in\\mathbb{R}^{d\\times d}\\) is symmetric positive definite and the matrix-vector product \\(D_{x_{0}}x(1)v\\) corresponds to iterative applications of the matrix \\(\\int_{0}^{1}\\gamma_{t}\\mathrm{Var}(x_{1}|x(t))dt\\) to \\(v\\). While a closed form expression to this integral is unknown, it is a weighted sum of the covariance matrices,\n' +
      '\n' +
      '\\[\\mathrm{Var}(x_{1}|x(t))=\\mathbb{E}_{p_{t}(x_{1}|x(t))}\\left[x_{1}-\\hat{x}_{1 }\\right]\\left[x_{1}-\\hat{x}_{1}\\right]^{T}, \\tag{16}\\]\n' +
      '\n' +
      'where \\(\\hat{x}_{1}=\\mathbb{E}_{p_{t}(x_{1}|x(t))}x_{1}\\) is the _denoiser_(Karras et al., 2022). The vector-matrix multiplication with \\(\\mathrm{Var}(x_{1}|x(t))v\\) projects \\(v\\) on the major axes of the distribution of the data conditioned on \\(x(t)\\), i.e., \\(x_{1}|x(t)\\). As we will soon see, \\(D_{x_{0}}x(1)\\) is key to understanding the implicit bias claim.\n' +
      '\n' +
      'The dynamics of \\(x(1)\\).Consider an optimization step updating the optimized variable \\(x_{0}\\) with a gradient step, i.e., \\(x_{0}^{\\star}=x_{0}-\\tau\\nabla_{x_{0}}\\mathcal{L}(x(1))\\), where the gradient \\(\\nabla_{x_{0}}\\mathcal{L}(x(1))\\) can be now computed with the chain rule and equation 18,\n' +
      '\n' +
      '\\[\\nabla_{x_{0}}\\mathcal{L}(x(1))=D_{x_{0}}x(1)\\nabla_{x(1)}\\mathcal{L}(x(1)), \\tag{17}\\]\n' +
      '\n' +
      'and we used that \\(D_{x_{0}}x(1)\\) is a symmetric matrix. We can now ask: _How the sample \\(x(1)\\) is changing infinitesimally under this gradient step?_Denote by \\(\\Phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}\\) the map taking initial conditions \\(x_{0}\\) to solutions of equation 2 at \\(t=1\\), i.e., \\(\\Phi(x_{0})=x(1)\\). The _variation_ of \\(x(1)\\) is\n' +
      '\n' +
      '\\[\\delta x(1) =\\frac{d}{d\\tau}\\Big{|}_{\\tau=0}\\Phi\\left(x_{0}-\\tau\\nabla_{x_{0} }\\mathcal{L}(x(1))\\right)\\] \\[=-\\left[D_{x_{0}}x(1)\\right]^{2}\\nabla_{x(1)}\\mathcal{L}(x(1)),\\]\n' +
      '\n' +
      'where the first equality is the definition of variation and the second equality is using chain rule, equation 18, and equation 17. Indeed, the dynamics of \\(x(1)\\) follows the projection of the gradient \\(\\nabla_{x(1)}\\mathcal{L}(x(1))\\) with the operator \\(D_{x_{0}}x(1)\\) that iteratively applies projection by the covariance matrix \\(\\mathrm{Var}(x_{1}|x(t))\\) at different times \\(t\\).\n' +
      '\n' +
      '## 5 Related Work\n' +
      '\n' +
      'Inverse Problems.A new line of works alter the diffusion generation process for training-free solutions of inverse problems. Most works can be viewed as building guidance strategies to the generation process of diffusion models. (Kawar et al., 2022) takes a variational approach deriving a solver for linear inverse problems. Similarly, (Chung et al., 2022; Wang et al., 2022) modify the generation process by enforcing consistency with the observations either via cost functions or projections (Choi et al., 2021; Wang et al., 2022; Lugmayr et al., 2022). Other approaches guide the sampling process with derivatives through the diffusion model at each denoising step (Ho et al., 2022; Chung et al., 2023; Song et al., 2023; Pokle et al., 2023). A recent work by (Rout et al., 2023) extends the ideas for latent diffusion models by chained applications of encoder-decoder. Similar to our approach (Mardani et al., 2023) performs optimization of a reconstruction loss with score matching regularization.\n' +
      '\n' +
      'Conditional sampling.Conditional sampling from diffusion models can be achieved by training an additional noise-aware condition predictor model (Song et al., 2020) or by incorporating the condition into the training process (Dhariwal and Nichol, 2021; Ho and Salimans, 2022). These approaches however require task specific training. Plug-and-play approaches, on the other hand, utilize a pre-trained unconditional generative model as a prior. (Graikos et al., 2023) perform constrained generation via optimization of a reconstruction term regularized by the diffusion loss. (Liu et al., 2023) seeks for optimal control optimizing through the generation process to learn guiding controls. Our method formulates a similar optimization problem like earlier works on generative normalizing flow models (Whang et al., 2021; Chavez, 2022). We note that using gradients through the solver for the case of discrete diffusion models was first used by (Wallace et al., 2023) for classifier guidance and by (Samuel et al., 2023) to generate rare samples.\n' +
      '\n' +
      '## 6 Experiments\n' +
      '\n' +
      'We test D-Flow on the tasks: linear inverse problems on images, inverse problems with latent flow models and conditional molecule generation. For all the inverse problems experiments, where the observed signal provides structural information, we use a blend initialization to our algorithm speeding up convergence and often improving performance. Furthermore, in most experiments we find that there is no need in adding an explicit regularizing term in the optimization. The only cases where we found regularization helpful was in the noisy case linear inverse problems and molecule generation. Additional details are in Appendix B.\n' +
      '\n' +
      '### Linear inverse problems on images\n' +
      '\n' +
      'We validate our method on standard linear inverse problems with a known degradation model on images. The tasks we consider are center-crop inpainting, super-resolution and Gaussian deblurring both in the noisless and noisy case. In all cases we stop the optimization at a task dependent target PSNR. For the noisy case we choose the target PSNR to be the PSNR corresponding to the known added noise.\n' +
      '\n' +
      'Tasks.We follow the same settings as in (Pokle et al., 2023): (i) For center-crop inpainting, we use a \\(40\\times 40\\) centered mask; (ii) for super-resolution we use bicubic interpolation to downsample the images by \\(\\times 2\\); and lastly (iii) for Gaussian deblur we apply a Gaussian blur kernel of size \\(61\\times 61\\) with intensity \\(1\\). For each task we report results for the noiseless and noisy (Gaussian noise of \\(\\sigma_{y}=0.05\\), see equation 5) cases. Further implementation details can be found in the Appendix B.1.\n' +
      '\n' +
      'Metrics.Following the evaluation protocol of prior works (Chung et al., 2022; Kawar et al., 2022) we report Frechet Inception Distance (FID) (Heusel et al., 2018), Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018), peak signal-to-noise ratio (PSNR), and structural similarity index (SSIM).\n' +
      '\n' +
      'Datasets and baselines.We use the face-blurred ImageNet-128 dataset and report our results on the \\(10k\\) split of the face-blurred ImageNet dataset used by (Pokle et al., 2023). We compare our method to three recent state of the art methods: IIGDM (Song et al., 2023), OT-ODE (Pokle et al., 2023) and RED-Diff (Mardani et al., 2023). We use the implementation of (Pokle et al., 2023) for all the baselines. All methods, including ours, are evaluated with the same Cond-OT flow-matching class conditioned model trained on the face-blurred ImageNet-128 unless the results we produced were inferior to the ones reported in (Pokle et al., 2023). In that case, we use the reported numbers from (Pokle et al., 2023).\n' +
      '\n' +
      'Results.As shown in Table 1, our method shows strong performance across all tasks, Figure 5 shows samples for each type of distortion. For inpainting and super-resolution our method improves upon state of the art in most metrics. We believe that our method\'s ability to reach images with higher fidelity to the ground truth is attributed to the source point optimization, which, differently from guided sampling approaches such as (Song et al., 2023; Pokle et al., 2023), iteratively correct the sampling trajectory to better match the observed signal. We further note that compared to RED-Diff, which is also an optimization approach, our method does not struggle in the noisy case and achieves SOTA performance. We show more samples in Figures 7,8.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c} \\hline \\hline  & \\multicolumn{3}{c}{**Inpainting-Center**} & \\multicolumn{3}{c}{**Super-Resolution X2**} & \\multicolumn{3}{c}{**Gaussian deblur**} \\\\ \\cline{2-13} Method & FID \\(\\downarrow\\) & LPIPS \\(\\downarrow\\) & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & FID \\(\\downarrow\\) & LPIPS \\(\\downarrow\\) & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & FID \\(\\downarrow\\) & LPIPS \\(\\downarrow\\) & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) \\\\ \\hline \\(\\sigma_{y}=0\\) & 5.73 & 0.096 & 36.89 & 0.908 & 6.01 & 0.104 & 34.31 & 0.911 & 4.27 & 0.066 & 37.61 & 0.961 \\\\ OT-ODE (Pokle et al., 2023) & 5.65 & 0.094 & 37.00 & 0.893 & 4.28 & 0.097 & 33.38 & 0.903 & 2.04 & 0.048 & 37.44 & 0.959 \\\\ RED-Diff (Mardani et al., 2023) & 5.40 & **0.068** & 38.91 & **0.928** & 3.05 & 0.091 & 33.74 & 0.900 & **1.62** & 0.055 & 35.18 & 0.937 \\\\ Ours & **4.14** & 0.072 & 37.67 & 0.922 & **2.50** & **0.069** & **34.88** & **0.924** & 2.37 & **0.015** & **39.47** & **0.976** \\\\ \\hline \\(\\sigma_{y}=0.05\\) & 7.99 & 0.122 & 34.57 & 0.867 & 4.38 & 0.148 & 32.07 & 0.831 & 30.30 & 0.328 & 29.96 & 0.606 \\\\ OT-ODE (Pokle et al., 2023) & 6.25 & 0.119 & **35.01** & 0.882 & 4.61 & 0.149 & **32.59** & **0.862** & **4.94** & 0.175 & 31.94 & **0.821** \\\\ RED-Diff (Mardani et al., 2023) & 14.63 & 0.171 & 32.42 & 0.820 & 10.54 & 0.182 & 31.82 & 0.852 & 21.43 & 0.229 & 31.41 & 0.807 \\\\ Ours & **4.76** & **0.102** & 34.609 & **0.890** & **4.26** & **0.146** & 32.35 & 0.858 & 5.35 & **0.167** & **31.99** & 0.820 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Quantitative evaluation of linear inverse problems on face-blurred ImageNet-128.\n' +
      '\n' +
      'Figure 5: Qualitative comparison for linear inverse problems on ImageNet-128. GT samples from ImageNet-128 validation.\n' +
      '\n' +
      '### Inverse problems with latent flow models\n' +
      '\n' +
      '#### 6.2.1 Image inpainting\n' +
      '\n' +
      'We demonstrate the capability of our approach for non-linear inverse problems by applying it to the task of free-form inpainting using a latent T2I FM model.\n' +
      '\n' +
      '**Metrics.** To quantitatively assess our results we report standard metrics used in T2I generation: PSNR, FID (Heusel et al., 2018), and Clip score (Ramesh et al., 2022).\n' +
      '\n' +
      '**Datasets and baselines.** The T2I model we use was trained on a proprietary dataset of \\(330m\\) image-text pairs. It was trained on the latent space of an autoencoder as in (Rombach et al., 2022). The architecture is based on GLIDE (Nichol et al., 2022) and uses a T5 text encoder (Raffel et al., 2023). We evaluate on a subset of \\(1k\\) samples from the validation set of the COCO dataset (Lin et al., 2015). We compare our method to RED-Diff (Mardani et al., 2023) as it is also not limited to linear inverse problems like the other baselines we used in the previous section. We tested different hyper-parameters for RED-Diff and report results with the best.\n' +
      '\n' +
      '**Results.** Table 3 reports metrics for the baseline and our method. The metrics indicate that while RED-Diff better matches the unmasked areas, achieving superior performance for structural metrics (PSNR, SSIM) our method produces more semantically plausible image completion winning in perceptual metrics. We do observe that RED-Diff often produces artifacts for this task. Results are visualized in Figure 9.\n' +
      '\n' +
      '#### 6.2.2 Audio inpainting and super-resolution\n' +
      '\n' +
      'We evaluate our method on the tasks of music inpainting and super-resolution, utilizing a latent flow-matching music generation model. For this, we used a trained Cond-OT flow-matching text conditioned model with a transformer architecture of 325m parameters that operates on top of End-Codec representation (Defossez et al., 2022). The model\'s performance aligns with the current state-of-the-art scores in text-conditional music generation, achieving a Frechet Audio Distance (FAD) score of \\(3.13\\)(Kilgour et al., 2018) on MusicCaps and FAD of \\(0.72\\) on in-domain data. The model is trained to generate ten-seconds samples. In the following, we evaluate the performance of inpainting and super-resolution using our method and RED-Diff as baseline, we report FAD and PSNR metrics.\n' +
      '\n' +
      '**Datasets and baselines.**\n' +
      '\n' +
      'For evaluation, we use the MusicCaps benchmark, which comprises of \\(5.5\\)K pairs of music and a textual description and an internal (in-domain) evaluation set of \\(202\\) samples, similar to (Copet et al., 2023; Ziv et al., 2024). Similar to prior work, we compute FAD metric using VGGish. We compare our method to RED-Diff (Mardani et al., 2023).\n' +
      '\n' +
      '**Results.** Table 2 studies our method in inpainting and super resolution tasks. This experiment demosntrates the ability of our method to work in non-linear setup, where the flow model is trained over a neural representation and the cost function is evaluated on the post-decoded signal (neural representation after decoding). In the inpainting task, we center crop the signal by \\(10\\)% and \\(20\\)%, i.e., for a ten-seconds signal, we mask out two and four seconds respectively. In the super-resolution task we upscale a signal by factors of two, four, and eight,i.e., from \\(4\\)kHz, \\(8\\)kHz, \\(16\\)kHz to \\(32\\)kHz respectively. Overall, our method improves upon the baseline. Specifically, in all experiments, our method obtain the lowest FAD metric. In the inpainting task our method obtains a slightly lower PSNR from the baseline. Audio samples are attached in a supplementray material. Additional implementation details appear in Appendix B.2.2.\n' +
      '\n' +
      '### Conditional molecule generation on QM9\n' +
      '\n' +
      'In this experiment we illustrate the application of our method for controllable molecule generation, which is of practical significance in the fields of material and drug design. The properties targeted for conditional generation (\\(c\\) in equation 7) include polarizability \\(\\alpha\\), orbital energies \\(\\varepsilon_{HOMO},\\varepsilon_{LUMO}\\) and their gap \\(\\Delta\\varepsilon\\), Diople moment \\(\\mu\\), and heat capacity \\(C_{v}\\). To assess the properties of the molecules generated, we used a property classifier (\\(\\mathcal{F}\\) in equation 7) for each property. Those classifiers were trained following the methodology outlined in (Hoogeboom et al., 2022). Further details are in Appendix B.3.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{4}{c}{**Inpainting (10\\%)**} & \\multicolumn{2}{c}{**Inpainting (20\\%)**} & \\multicolumn{2}{c}{**Super-Resolution X2**} & \\multicolumn{2}{c}{**Super-Resolution X4**} & \\multicolumn{2}{c}{**Super-Resolution X8**} \\\\  & FAD \\(\\downarrow\\) & PSNR \\(\\uparrow\\) & FAD \\(\\downarrow\\) & PSNR \\(\\uparrow\\) & FAD \\(\\downarrow\\) & PSNR \\(\\uparrow\\) & FAD \\(\\downarrow\\) & PSNR \\(\\uparrow\\) & FAD \\(\\downarrow\\) & PSNR \\(\\uparrow\\) \\\\ \\hline In-domain & \\multicolumn{4}{c}{} \\\\ RED-Diff (Mardani et al., 2023) & 0.75 & **31.19** & 0.78 & **29.99** & 0.93 & 35.27 & 1.63 & 33.51 & 1.73 & 29.12 \\\\ Ours & 0.22 & 31.02 & 0.49 & 29.57 & 0.22 & 44.51 & 0.50 & **42.64** & 1.01 & 36.50 \\\\ \\hline MusicCaps & \\multicolumn{4}{c}{} \\\\ RED-Diff (Mardani et al., 2023) & 3.59 & **32.81** & 3.72 & 30.39 & 3.07 & 37.13 & 3.51 & 34.99 & 3.97 & 30.49 \\\\ Ours & 1.19 & 31.78 & **1.31** & **31.08** & 1.25 & 38.93 & 1.42 & 35.83 & 2.09 & 32.20 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Quantitative evaluation of music generation with latent flow models.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline \\multicolumn{5}{c}{**Inpainting-Free-Form**} \\\\ \\hline Method & FID \\(\\downarrow\\) & LPIPS \\(\\downarrow\\) & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & Clip score \\(\\uparrow\\) \\\\ \\hline RED-Diff (Mardani et al., 2023) & 23.31 & 0.327 & 32.88 & 0.813 & 0.882 \\\\ Ours & 16.92 & **0.327** & 32.34 & 0.759 & **0.922** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Quantitative evaluation of free-form inpainting on MS-COCO with T2I latent model.\n' +
      '\n' +
      '**Metrics.** To assess conditional generation, we calculate the Mean Absolute Error (MAE) between the predicted property value of the generated molecule by the property classifier and the target property value. Additionally, we appraise the quality of the generated molecules by evaluating atom stability (the percentage of atoms with correct valency), molecule stability (the percentage of molecules where all atoms are stable), validity (as defined in RDKit (Landrum, 2016)), and the uniqueness of the generated molecules.\n' +
      '\n' +
      '**Dataset and baselines.** The generative models used for this experiment are trained using the QM9 dataset (Ramakrishnan et al., 2014), a commonly used molecular dataset containing small molecules with up to \\(29\\) atoms. The model we use as prior in these experiments is the unconditional diffusion model for molecule generation, E(3) Equivariant Diffusion Model (EDM) (Hoogeboom et al., 2022). EDM was trained as a noise prediction model (also known as \\(\\epsilon\\)-prediction) with polynomial schedule. To perform deterministic sampling for our optimization, we transform the noise prediction to velocity field prediction (see Equation 52) and use 50 steps midpoint ODE sampler. We compare our method to several state of the art _conditional models_: conditional EDM, Equivariant Flow-Matching (EQuiFM) (Song et al., 2023b), and Geometric Latent Diffusion Model (GoLDM)(Xu et al., 2023) an equivariant flow-matching model. Additionally, we report the test MAE of each property classifier (denoted as QM9\\({}^{*}\\) in Table 4), which serves as an empirical lower bound. It is important to note that for each specific property of conditional generation, the baseline methods utilized a distinct conditional model, each individually trained for generating that particular property while we used a single unconditional model. According to the conditional training protocol from (Satorras et al., 2022), the property classifier is trained over half of the QM9 train set (\\(50\\)K) while the conditional training is done with the remaining half. For our unconditional model, which operates without any conditional context, we utilized a pretrained model that was trained on the entire QM9 dataset.\n' +
      '\n' +
      '**Results.** Table 4 demonstrates that our approach significantly outperforms all other baseline methods in the quality of conditional molecule generation. This superior performance is attributed to our direct optimization of the conditional generation. Table 5 presents the stability and validity metrics for our method. In comparison with conditional EDM, which achieves an average molecular stability of \\(82.1\\%\\) across different properties, our method reveals a disparity in the quality of the generated molecules. This quality reduction is likely a consequence of altering the sampling path from stochastic to deterministic, as well as the absence of structural inductive bias in the optimization process. We present additional evidence for the first claim in Appendix B.3. Figure 6 visualize the controlled generation for different polarizability \\(\\alpha\\) values; all molecules in the figure are valid and stable with a classifier error lower than \\(1\\).\n' +
      '\n' +
      '## 7 Discussion, limitations and future work\n' +
      '\n' +
      'We have presented a simple and general framework for controlled generation from pre-trained diffusion/flow models and demonstrated its efficacy on a wide range of problems from various domains and data types ranging from images, and audio to molecules. The main limitation of our approach is in its relatively long runtimes (see Section 3.4, and Appendix B) which stems from the need to back-propagate through multiple compositions of the velocity field (equivalently, the diffusion model). Our theoretical analysis and empirical evidence show however that computing gradients through the ODE solution have a desirable implicit bias, producing state of the art results on common conditional generation tasks. Consequently, an interesting future direction is to utilize the implicit bias but with potentially cheaper computational overhead, and draw connections to other biases used in other controlled generation paradigms.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline Property & \\(\\alpha\\) & \\(\\Delta\\varepsilon\\) & \\(\\varepsilon_{HOMO}\\) & \\(\\varepsilon_{LUMO}\\) & \\(\\mu\\) & \\(C_{v}\\) \\\\ Units & Bohr\\({}^{2}\\) & meV & meV & meV & D & \\(\\frac{\\text{col}}{\\text{mol}^{2}}\\) K \\\\ \\hline QM9\\({}^{*}\\) & 0.10 & 64 & 39 & 36 & 0.043 & 0.040 \\\\ \\hline EDM & 2.76 & 655 & 356 & 584 & 1.111 & 1.101 \\\\ EQuiFM & 2.41 & 591 & 337 & 530 & 1.106 & 1.033 \\\\ GeoLDM & 2.37 & 587 & 340 & 522 & 1.108 & 1.025 \\\\ \\hline Ours & **1.38** & **340** & **179** & **330** & **0.299** & **0.784** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Quantitative evaluation of conditional molecule generation. Values reported in the table are MAE (over \\(10\\)K samples) for molecule property predictions (lower is better).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline Property & \\(\\alpha\\) & \\(\\Delta\\varepsilon\\) & \\(\\varepsilon_{HOMO}\\) & \\(\\varepsilon_{LUMO}\\) & \\(\\mu\\) & \\(C_{v}\\) \\\\ \\hline Molecule Stability (\\%) & 52.6 & 54.9 & 55.2 & 54.4 & 57.3 & 55.3 \\\\ Atom Stability (\\%) & 94.7 & 95.0 & 95.1 & 95.0 & 95.3 & 94.8 \\\\ Validity (\\%) & 78.7 & 79.5 & 80.6 & 81.0 & 82.0 & 80.0 \\\\ Validity \\& Uniqueness (\\%) & 77.3 & 77.9 & 80.0 & 79.8 & 80.6 & 78.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Stability and validity evaluation of D-flow on conditional molecule generation (10K samples).\n' +
      '\n' +
      'Figure 6: Qualitative visualization of controlled generated molecules for various polarizability (\\(\\alpha\\)) levels.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'OP is supported by a grant from Israel CHE Program for Data Science Research Centers and the Minerva Stiftung.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Anderson et al. (2019) Anderson, B., Hy, T.-S., and Kondor, R. Cormorant: Covariant molecular neural networks, 2019.\n' +
      '* Asim et al. (2020) Asim, M., Daniels, M., Leong, O., Ahmed, A., and Hand, P. Invertible generative models for inverse problems: mitigating representation error and dataset bias, 2020.\n' +
      '* Bar-Tal et al. (2023) Bar-Tal, O., Yariv, L., Lipman, Y., and Dekel, T. Multi-diffusion: Fusing diffusion paths for controlled image generation. _arXiv preprint arXiv:2302.08113_, 2023.\n' +
      '* Bora et al. (2017) Bora, A., Jalal, A., Price, E., and Dimakis, A. G. Compressed sensing using generative models, 2017.\n' +
      '* Chen et al. (2018) Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. Neural ordinary differential equations. _arXiv preprint arXiv:1806.07366_, 2018.\n' +
      '* Chen & torchdiffeq (2018) Chen, R. T. Q. torchdiffeq, 2018. URL [https://github.com/rtqichen/torchdiffeq](https://github.com/rtqichen/torchdiffeq).\n' +
      '* Choi et al. (2021) Choi, J., Kim, S., Jeong, Y., Gwon, Y., and Yoon, S. Ilvr: Conditioning method for denoising diffusion probabilistic models, 2021.\n' +
      '* Chung et al. (2022) Chung, H., Sim, B., Ryu, D., and Ye, J. C. Improving diffusion models for inverse problems using manifold constraints, 2022.\n' +
      '* Chung et al. (2023) Chung, H., Kim, J., Mccann, M. T., Klasky, M. L., and Ye, J. C. Diffusion posterior sampling for general noisy inverse problems, 2023.\n' +
      '* Chavez (2022) Chavez, J. A. Generative flows as a general purpose solution for inverse problems, 2022.\n' +
      '* Copet et al. (2023) Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y., and Defossez, A. Simple and controllable music generation. In _NeurIPS_, 2023.\n' +
      '* measuring reproducibility in pytorch. _Journal of Open Source Software_, 7(70): 4101, 2022. doi: 10.21105/joss.04101. URL [https://doi.org/10.21105/joss.04101](https://doi.org/10.21105/joss.04101).\n' +
      '* Dhariwal & Nichol (2021) Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. _arXiv preprint arXiv:2105.05233_, 2021.\n' +
      '* Defossez et al. (2022) Defossez, A., Copet, J., Synnaeve, G., and Adi, Y. High fidelity neural audio compression. _arXiv preprint arXiv:2210.13438_, 2022.\n' +
      '* Evans (2005) Evans, L. C. An introduction to mathematical optimal control theory. _Lecture Notes, University of California, Department of Mathematics, Berkeley_, 3:15-40, 2005.\n' +
      '* Graikos et al. (2023) Graikos, A., Malkin, N., Jojic, N., and Samaras, D. Diffusion models as plug-and-play priors, 2023.\n' +
      '* Grathwohl et al. (2018) Grathwohl, W., Chen, R. T. Q., Bettencourt, J., Sutskever, I., and Duvenaud, D. Ffjord: Free-form continuous dynamics for scalable reversible generative models, 2018.\n' +
      '* Heusel et al. (2018) Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium, 2018.\n' +
      '* Ho & Salimans (2022) Ho, J. and Salimans, T. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022a.\n' +
      '* Ho & Salimans (2022) Ho, J. and Salimans, T. Classifier-free diffusion guidance, 2022b.\n' +
      '* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. _arXiv preprint arXiv:2006.11239_, 2020.\n' +
      '* Ho et al. (2022) Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models, 2022.\n' +
      '* Hoogeboom et al. (2022) Hoogeboom, E., Satorras, V. G., Vignac, C., and Welling, M. Equivariant diffusion for molecule generation in 3d, 2022.\n' +
      '* Karras et al. (2022) Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35: 26565-26577, 2022.\n' +
      '* Kawar et al. (2022) Kawar, B., Elad, M., Ermon, S., and Song, J. Denoising diffusion restoration models. In _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* Kilgour et al. (2018) Kilgour, K., Zuluaga, M., Roblek, D., and Sharifi, M. Frechet audio distance: A metric for evaluating music enhancement algorithms. _arXiv preprint arXiv:1812.08466_, 2018.\n' +
      '* Landrum (2016) Landrum, G. Rdkit: Open-source cheminformatics software. 2016. URL [https://github.com/rdkit/rdkit/releases/tag/Release_2016_09_4](https://github.com/rdkit/rdkit/releases/tag/Release_2016_09_4).\n' +
      '* Lin et al. (2015) Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and Dollar, P. Microsoft coco: Common objects in context, 2015.\n' +
      '* Lipman et al. (2023) Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling, 2023.\n' +
      '\n' +
      '* Liu et al. (2023) Liu, X., Wu, L., Zhang, S., Gong, C., Ping, W., and Liu, Q. Flowgrad: Controlling the output of generative odes with gradients. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 24335-24344, June 2023.\n' +
      '* Lugmayr et al. (2022) Lugmayr, A., Danelljan, M., Romero, A., Yu, F., Timofte, R., and Gool, L. V. Repaint: Inpainting using denoising diffusion probabilistic models, 2022.\n' +
      '* Mardani et al. (2023) Mardani, M., Song, J., Kautz, J., and Vahdat, A. A variational perspective on solving inverse problems with diffusion models, 2023.\n' +
      '* Nalisnick et al. (2019) Nalisnick, E., Matsukawa, A., Teh, Y. W., Gorur, D., and Lakshminarayanan, B. Do deep generative models know what they don\'t know?, 2019.\n' +
      '* Nichol et al. (2022) Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing with text-guided diffusion models, 2022.\n' +
      '* Pokle et al. (2023) Pokle, A., Muckley, M. J., Chen, R. T. Q., and Karrer, B. Training-free linear image inversion via flows, 2023.\n' +
      '* Raffel et al. (2023) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023.\n' +
      '* Ramakrishnan et al. (2014) Ramakrishnan, R., Dral, P., Rupp, M., and von Lilienfeld, A. Quantum chemistry structures and properties of 134 kilo molecules. _Scientific Data_, 1, 08 2014. doi: 10.1038/sdata.2014.22.\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents, 2022.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models, 2022.\n' +
      '* Rout et al. (2023) Rout, L., Raoof, N., Daras, G., Caramanis, C., Dimakis, A. G., and Shakkottai, S. Solving linear inverse problems provably via posterior sampling with latent diffusion models, 2023.\n' +
      '* Samuel et al. (2023) Samuel, D., Ben-Ari, R., Darshan, N., Maron, H., and Chechik, G. Norm-guided latent space exploration for text-to-image generation, 2023a.\n' +
      '* Samuel et al. (2023) Samuel, D., Ben-Ari, R., Raviv, S., Darshan, N., and Chechik, G. Generating images of rare concepts using pre-trained diffusion models, 2023b.\n' +
      '* Satorras et al. (2022) Satorras, V. G., Hoogeboom, E., Fuchs, F. B., Posner, I., and Welling, M. E(n) equivariant normalizing flows, 2022.\n' +
      '* Shaul et al. (2023) Shaul, N., Chen, R. T., Nickel, M., Le, M., and Lipman, Y. On kinetic optimal probability paths for generative models. In _International Conference on Machine Learning_, pp. 30883-30907. PMLR, 2023.\n' +
      '* Song et al. (2023a) Song, J., Vahdat, A., Mardani, M., and Kautz, J. Pseudoinverse-guided diffusion models for inverse problems. In _International Conference on Learning Representations_, 2023a. URL [https://openreview.net/forum?id=9_gsMABMRKQ](https://openreview.net/forum?id=9_gsMABMRKQ).\n' +
      '* Song & Ermon (2019) Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. _arXiv preprint arXiv:1907.05600_, 2019.\n' +
      '* Song et al. (2020) Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '* Song et al. (2023b) Song, Y., Gong, J., Xu, M., Cao, Z., Lan, Y., Ermon, S., Zhou, H., and Ma, W.-Y. Equivariant flow matching with hybrid probability transport, 2023b.\n' +
      '* Wallace et al. (2023) Wallace, B., Gokul, A., Ermon, S., and Naik, N. End-to-end diffusion latent optimization improves classifier guidance, 2023.\n' +
      '* Wang et al. (2022) Wang, Y., Yu, J., and Zhang, J. Zero-shot image restoration using denoising diffusion null-space model, 2022.\n' +
      '* Whang et al. (2021) Whang, J., Lei, Q., and Dimakis, A. G. Solving inverse problems with a flow-based noise model, 2021.\n' +
      '* Xu et al. (2023) Xu, M., Powers, A., Dror, R., Ermon, S., and Leskovec, J. Geometric latent diffusion models for 3d molecule generation, 2023.\n' +
      '* Yu et al. (2023) Yu, J., Wang, Y., Zhao, C., Ghanem, B., and Zhang, J. Freedom: Training-free energy-guided conditional diffusion model, 2023.\n' +
      '* Zhang et al. (2018) Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as a perceptual metric, 2018.\n' +
      '* Ziv et al. (2024) Ziv, A., Gat, I., Lan, G. L., Remez, T., Kreuk, F., Defossez, A., Copet, J., Synnaeve, G., and Adi, Y. Masked audio generation using a single non-autoregressive transformer. 2024.\n' +
      '\n' +
      '## Appendix A Proofs and theorems\n' +
      '\n' +
      '### Proof of Theorem 4.1\n' +
      '\n' +
      'We restate Theorem 4.1 here:\n' +
      '\n' +
      '**Theorem A.1**.: _For AGPP velocity field \\(u_{t}\\) (see equation 12) and \\(x(t)\\) defined via equation 2 the differential of \\(x(1)\\) as a function of \\(x_{0}\\) is_\n' +
      '\n' +
      '\\[D_{x_{0}}x(1)=\\sigma_{1}\\exp\\left[\\int_{0}^{1}\\gamma_{t}\\mathrm{Var}(x_{1}|x(t) )dt\\right], \\tag{18}\\]\n' +
      '\n' +
      '_where \\(\\gamma_{t}=\\frac{1}{2}\\frac{d}{dt}\\mathrm{snr}(t)\\) and we define \\(\\mathrm{snr}(t)=\\frac{\\alpha_{t}^{2}}{\\sigma_{t}^{2}}\\)._\n' +
      '\n' +
      'Proof.: To compute the differential of \\(x(1)\\) w.r.t the initial point \\(x_{0}\\) we utilize adjoint dynamics.\n' +
      '\n' +
      'Let us define the adjoint \\(p(t)=D_{x(t)}x(1)\\). The dynamics of \\(p(t)\\) are defined by the following ODE (Evans, 2005):\n' +
      '\n' +
      '\\[\\dot{p}(t) =-D_{x}u_{t}(x(t))^{T}p(t) \\tag{19}\\] \\[p(1) =D_{x(1)}x(1)=I. \\tag{20}\\]\n' +
      '\n' +
      'To compute \\(D_{x_{0}}x(1)\\) we solve 19 from time \\(t=1\\) back to time \\(t=0\\). Then,\n' +
      '\n' +
      '\\[p(0)=D_{x_{0}}x(1). \\tag{21}\\]\n' +
      '\n' +
      'The adjoint ODE 19 is a linear ODE and together with the initial condition equation 20 its solution is given by:\n' +
      '\n' +
      '\\[p(t)=\\exp\\left[\\int_{t}^{1}D_{x}u_{s}(x(s))ds\\right] \\tag{22}\\]\n' +
      '\n' +
      'At \\(t=0\\), we get:\n' +
      '\n' +
      '\\[D_{x_{0}}x(1)=\\exp\\left[\\int_{0}^{1}D_{x}u_{s}(x(s))ds\\right] \\tag{23}\\]\n' +
      '\n' +
      'We will now use the properties of AGPPs to further analyze the differential of the velocity field, \\(D_{x}u_{t}(x(t))\\), inside the integral.\n' +
      '\n' +
      'We recall a general affine Gaussian path is defined by\n' +
      '\n' +
      '\\[p_{t}(x|x_{1}) =\\mathcal{N}(x|\\alpha_{t}x_{1},\\sigma_{t}^{2}I),\\] conditional probability path (24) \\[p_{t}(x) =\\int p_{t}(x|x_{1})q(x_{1})dx_{1},\\] marginal probability path (25)\n' +
      '\n' +
      'where \\((\\alpha_{t},\\sigma_{t})\\) define the scheduler and \\(q\\) is the dataset probability density. The velocity fields defining these paths are (Lipman et al., 2023)\n' +
      '\n' +
      '\\[u_{t}(x|x_{1}) =a_{t}x+b_{t}x_{1},\\qquad a_{t}=\\frac{\\dot{\\sigma}_{t}}{\\sigma_{ t}},\\;\\;b_{t}=\\dot{\\alpha}_{t}-\\alpha_{t}\\frac{\\dot{\\sigma}_{t}}{\\sigma_{t}} \\text{ conditional velocity field} \\tag{26}\\] \\[u_{t}(x) =\\int u_{t}(x|x_{1})p_{t}(x_{1}|x)dx_{1},\\qquad p_{t}(x_{1}|x)= \\frac{p_{t}(x|x_{1})q(x_{1})}{p_{t}(x)} \\text{ marginal velocity field} \\tag{27}\\]\n' +
      '\n' +
      'plugging 26 into 27 we get:\n' +
      '\n' +
      '\\[u_{t}(x)=a_{t}x+b_{t}\\hat{x}_{1} \\tag{28}\\]\n' +
      '\n' +
      'where \\(\\hat{x}_{1}=\\mathbb{E}_{p_{t}(x_{1}|x)}x_{1}=\\hat{x}_{1}(x,t)\\), also commonly called the _denoiser_ and we abuse notation here since we evaluate the denoiser at fixed \\(x\\) and \\(t\\) is our analysis.\n' +
      '\n' +
      'The differential of the velocity field is then:\n' +
      '\n' +
      '\\[D_{x}u_{t}(x)=a_{t}I+b_{t}D_{x}\\hat{x}_{1} \\tag{29}\\]\n' +
      '\n' +
      'We are now left with finding \\(D_{x}\\hat{x}_{1}\\):\n' +
      '\n' +
      '\\[D_{x}\\hat{x}_{1}=D_{x}\\int x_{1}p_{t}(x_{1}|x)dx_{1}=\\int x_{1}\\nabla_{x}p_{t}(x _{1}|x)dx_{1} \\tag{30}\\]\n' +
      '\n' +
      'First, since \\(p_{t}(x|x_{1})\\) is a Gaussian:\n' +
      '\n' +
      '\\[\\nabla_{x}p_{t}(x|x_{1})=\\frac{\\alpha_{t}x_{1}-x}{\\sigma_{t}^{2}}p_{t}(x|x_{1}) \\tag{31}\\]\n' +
      '\n' +
      'and plugging into 25, we have:\n' +
      '\n' +
      '\\[\\nabla_{x}p_{t}(x)=\\int\\frac{\\alpha_{t}x_{1}-x}{\\sigma_{t}^{2}}p_{t}(x|x_{1})q (x_{1})dx_{1} \\tag{32}\\]\n' +
      '\n' +
      'using 27, we get:\n' +
      '\n' +
      '\\[\\nabla_{x}p_{t}(x_{1}|x)=p_{t}(x_{1}|x)\\frac{\\alpha_{t}}{\\sigma_{t}^{2}}\\left( x_{1}-\\hat{x}_{1}\\right) \\tag{33}\\]\n' +
      '\n' +
      'Finally, 30 takes the form:\n' +
      '\n' +
      '\\[D_{x}\\hat{x}_{1}=\\int\\frac{\\alpha_{t}}{\\sigma_{t}^{2}}p_{t}(x_{1}|x)x_{1}(x_ {1}-\\hat{x}_{1})^{T}dx_{1}=\\int\\frac{\\alpha_{t}}{\\sigma_{t}^{2}}p_{t}(x_{1}|x) (x_{1}-\\hat{x}_{1})(x_{1}-\\hat{x}_{1})^{T}dx_{1}=\\frac{\\alpha_{t}}{\\sigma_{t} ^{2}}\\mathrm{Var}(x_{1}|x) \\tag{34}\\]\n' +
      '\n' +
      'where in the second equality we subtracted the integral \\(\\frac{\\alpha_{t}}{\\sigma_{t}^{2}}\\hat{x}_{1}\\int p_{t}(x_{1}|x)(x_{1}-\\hat{x} _{1})^{T}dx_{1}=0\\).\n' +
      '\n' +
      'Inserting back into 23 we get:\n' +
      '\n' +
      '\\[D_{x_{0}}x(1)=\\exp\\left[\\int_{0}^{1}a_{t}I+b_{t}\\frac{\\alpha_{t}}{\\sigma_{t}^ {2}}\\mathrm{Var}(x_{1}|x(t))dt\\right]. \\tag{35}\\]\n' +
      '\n' +
      'In case \\(a_{t}\\) is integrable we get\n' +
      '\n' +
      '\\[D_{x_{0}}x(1)=\\sigma_{1}\\exp\\left[\\int_{0}^{1}b_{t}\\frac{\\alpha_{t}}{\\sigma_{ t}^{2}}\\mathrm{Var}(x_{1}|x(t))dt\\right], \\tag{36}\\]\n' +
      '\n' +
      'where \\(\\exp\\left[\\int_{0}^{1}a_{t}dt\\right]=\\sigma_{1}\\), and\n' +
      '\n' +
      '\\[\\gamma_{t}=b_{t}\\frac{\\alpha_{t}}{\\sigma_{t}^{2}}=\\left(\\dot{\\alpha}_{t}- \\alpha_{t}\\frac{\\dot{\\sigma}_{t}}{\\sigma_{t}}\\right)\\frac{\\alpha_{t}}{\\sigma_ {t}^{2}}=\\frac{\\dot{\\alpha}_{t}\\alpha_{t}\\sigma_{t}^{2}-\\alpha_{t}^{2}\\dot{ \\sigma}_{t}\\sigma_{t}}{\\sigma_{t}^{4}}=\\frac{1}{2}\\frac{d}{dt}\\left(\\frac{ \\alpha_{t}^{2}}{\\sigma_{t}^{2}}\\right)=\\frac{1}{2}\\frac{d}{dt}\\mathrm{snr}(t) \\tag{37}\\]\n' +
      '\n' +
      'concluding the proof. \n' +
      '\n' +
      'Next we show that the integral in equation 18 is defined also for \\(\\sigma_{1}=0\\).\n' +
      '\n' +
      '**Lemma A.2**.: _For a Lipschitz function \\(f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\) we have that \\(\\int\\mathcal{N}(x|y,\\sigma^{2}I)f(x)dx=f(y)+\\mathcal{O}(\\sigma)\\)._\n' +
      '\n' +
      'Proof.: \\[\\left|\\int\\mathcal{N}(x|y,\\sigma^{2}I)f(x)dx-f(y)\\right| \\leq\\int\\mathcal{N}(x|y,\\sigma^{2}I))\\left|f(x)-f(y)\\right|dx\\] \\[=\\int\\mathcal{N}(z|0,I)\\left|f(\\sigma z+y)-f(y)\\right|dz\\] \\[\\leq K\\sigma\\int\\mathcal{N}(z|0,I)\\left|z\\right|dz\\] \\[=\\mathcal{O}(\\sigma),\\]where in the first equality we performed a change of variable \\(z=\\frac{x-y}{\\sigma}\\), and in the second inequality we used the fact that \\(f\\) is Lipschitz with constant \\(K>0\\). \n' +
      '\n' +
      'Using this Lemma we prove (under the assumption that \\(p_{1}(x)\\) and its derivatives is Lipschitz):\n' +
      '\n' +
      '**Proposition A.3**.: _The denoiser asymptotics at \\(t\\to 1\\) is_\n' +
      '\n' +
      '\\[\\hat{x}_{1}=\\frac{x}{\\alpha_{t}}+\\mathcal{O}(\\sigma_{t}) \\tag{38}\\]\n' +
      '\n' +
      'Proof.: First we note that we assume \\(\\sigma_{t}\\to 0\\) and \\(\\alpha_{t}\\to 1\\) as \\(t\\to 1\\),\n' +
      '\n' +
      '\\[\\mathcal{N}(x|\\alpha_{t}x_{1},\\sigma_{t}^{2}I)=c_{t}\\mathcal{N} \\left(x_{1}\\bigg{|}\\frac{x}{\\alpha_{t}},\\left(\\frac{\\sigma_{t}}{\\alpha_{t}} \\right)^{2}I\\right), \\tag{39}\\]\n' +
      '\n' +
      'where \\(c_{t}\\) is some normalization constant such that \\(c_{1}=1\\). Now,\n' +
      '\n' +
      '\\[p_{t}(x) =\\int\\mathcal{N}(x|\\alpha_{t}x_{1},\\sigma_{t}^{2}I)p_{1}(x_{1})dx _{1} \\tag{40}\\] \\[=c_{t}\\int\\mathcal{N}\\left(x_{1}\\bigg{|}\\frac{x}{\\alpha_{t}}, \\left(\\frac{\\sigma_{t}}{\\alpha_{t}}\\right)^{2}I\\right)p_{1}(x_{1})dx_{1}\\] (41) \\[=c_{t}p_{1}\\left(\\frac{x}{\\alpha_{t}}\\right)+\\mathcal{O}(\\sigma _{t}), \\tag{42}\\]\n' +
      '\n' +
      'where in second equality we used equation 39 and the last equality Lemma A.2.\n' +
      '\n' +
      '\\[\\hat{x}_{1} =\\int x_{1}p_{t}(x_{1}|x)dx_{1} \\tag{43}\\] \\[=\\int x_{1}\\frac{\\mathcal{N}(x|\\alpha_{t}x_{1},\\sigma_{t}^{2}I)p _{1}(x_{1})}{p_{t}(x)}dx_{1}\\] (44) \\[=\\int x_{1}\\frac{c_{t}\\mathcal{N}\\left(x_{1}\\bigg{|}\\frac{x}{ \\alpha_{t}},\\left(\\frac{\\sigma_{t}}{\\alpha_{t}}\\right)^{2}I\\right)p_{1}(x_{1}) }{p_{t}(x)}dx_{1}\\] (45) \\[=\\frac{c_{t}\\frac{x}{\\alpha_{t}}p_{1}\\left(\\frac{x}{\\alpha_{t}} \\right)+\\mathcal{O}(\\sigma_{t})}{c_{t}p_{1}\\left(\\frac{x}{\\alpha_{t}}\\right)+ \\mathcal{O}(\\sigma_{t})}\\] (46) \\[=\\frac{x}{\\alpha_{t}}+\\mathcal{O}(\\sigma_{t}), \\tag{47}\\]\n' +
      '\n' +
      'where in the second equality we used the definition of \\(p_{t}(x_{1}|x)\\), in the third equality we used equation 39, and in the fourth equality we used Lemma A.2. \n' +
      '\n' +
      'Now we can show that \\(D_{x}u_{t}(x(t))\\) is bounded as \\(t\\to 1\\)\n' +
      '\n' +
      '\\[D_{x}u_{t}(x(t)) =a_{t}I+b_{t}D_{x}\\hat{x}_{1} \\tag{48}\\] \\[=a_{t}I+b_{t}\\left(\\frac{1}{a_{t}}I+\\mathcal{O}(\\sigma_{t})\\right)\\] (49) \\[=\\frac{\\dot{\\alpha}_{t}}{\\alpha_{t}}I+\\mathcal{O}(1), \\tag{50}\\]\n' +
      '\n' +
      'where in the first equality we used equation 29, in the second Proposition A.3 (and the fact that the derivatives of \\(p_{1}\\) are Lipschitz for the derivation of the asymptotic rule), and in the last equality equation 26. Furthermore \\(D_{x}u_{t}(x(t))\\) is bounded as \\(t\\to 0\\) as both \\(a_{0},b_{0}\\) are well defined. This means that \\(D_{x}u_{t}(x(t))\\) is integrable over \\([0,1]\\).\n' +
      '\n' +
      '### On flow-matching, denoisers and noise prediction\n' +
      '\n' +
      'Consider a general affine conditional probability path defined by the following transport map:\n' +
      '\n' +
      '\\[x_{t}=\\sigma_{t}x_{0}+\\alpha_{t}x_{1}\\]\n' +
      '\n' +
      'where \\(x_{0}\\sim p_{0}\\) and \\(x_{1}\\sim p_{1}\\).\n' +
      '\n' +
      'For different choices of \\(\\sigma_{t},\\alpha_{t}\\) we can parametrize known diffusion and flow-matching paths. The corresponding conditional vector field on \\(x_{1}\\) is:\n' +
      '\n' +
      '\\[u_{t}(x|x_{1})=\\frac{\\dot{\\sigma}_{t}}{\\sigma_{t}}(x-\\alpha_{t}x_{1})+\\dot{ \\alpha}_{t}x_{1}=\\frac{\\dot{\\sigma}_{t}}{\\sigma_{t}}x-\\left(\\frac{\\dot{\\sigma }_{t}\\alpha_{t}}{\\sigma_{t}}-\\dot{\\alpha}_{t}\\right)x_{1}\\]\n' +
      '\n' +
      'and the conditional vector field on \\(x_{0}\\) is:\n' +
      '\n' +
      '\\[u_{t}(x|x_{0})=\\dot{\\sigma}_{t}x_{0}+\\frac{\\dot{\\alpha}_{t}}{\\alpha_{t}}(x- \\sigma_{t}x_{0})=\\frac{\\dot{\\alpha}_{t}}{\\alpha_{t}}x-\\left(\\frac{\\dot{\\alpha }_{t}\\sigma_{t}}{\\alpha_{t}}-\\dot{\\sigma}_{t}\\right)x_{0}\\]\n' +
      '\n' +
      'where \\(\\dot{f}=\\frac{d}{dt}f\\).\n' +
      '\n' +
      'Consider the marginal velocity field:\n' +
      '\n' +
      '\\[u_{t}(x)=\\int u_{t}(x|x_{1})p_{t}(x_{1}|x)dx_{1}=\\int u_{t}(x|x_{0})p_{t}(x_{0 }|x)dx_{0}\\]\n' +
      '\n' +
      'One can express it in terms of the optimal _denoiser_ function, \\(\\hat{x}_{1}^{*}(x,t)\\):\n' +
      '\n' +
      '\\[u_{t}(x)=\\frac{\\dot{\\sigma}_{t}}{\\sigma_{t}}\\int xp_{t}(x_{1}|x)dx_{1}-\\left( \\frac{\\dot{\\sigma}_{t}\\alpha_{t}}{\\sigma_{t}}-\\dot{\\alpha}_{t}\\right)\\int x_{ 1}p_{t}(x_{1}|x)dx_{1}=\\frac{\\dot{\\sigma}_{t}}{\\sigma_{t}}x-\\left(\\frac{\\dot{ \\sigma}_{t}\\alpha_{t}}{\\sigma_{t}}-\\dot{\\alpha}_{t}\\right)\\hat{x}_{1}^{*}(x,t)\\]\n' +
      '\n' +
      'For Cond-OT:\n' +
      '\n' +
      '\\[u_{t}(x)=\\frac{\\hat{x}_{1}^{*}(x,t)-x}{1-t} \\tag{51}\\]\n' +
      '\n' +
      'Or, in terms of the optimal noise predictor, \\(\\epsilon^{*}(x,t)\\), like in DDPM:\n' +
      '\n' +
      '\\[u_{t}(x)=\\frac{\\dot{\\alpha}_{t}}{\\alpha_{t}}\\int xp_{t}(x_{0}|x)dx_{0}-\\left( \\frac{\\dot{\\alpha}_{t}\\sigma_{t}}{\\alpha_{t}}-\\dot{\\sigma}_{t}\\right)\\int x_{ 0}p_{t}(x_{0}|x)dx_{0}=\\frac{\\dot{\\alpha}_{t}}{\\alpha_{t}}x-\\left(\\frac{\\dot {\\alpha}_{t}\\sigma_{t}}{\\alpha_{t}}-\\dot{\\sigma}_{t}\\right)\\epsilon^{*}(x,t)\\]\n' +
      '\n' +
      'and for Cond-OT:\n' +
      '\n' +
      '\\[u_{t}(x)=\\frac{x-\\epsilon^{*}(x,t)}{t} \\tag{52}\\]\n' +
      '\n' +
      '## Appendix B Implementation details\n' +
      '\n' +
      '### Linear inverse problems on images\n' +
      '\n' +
      '#### Optimization Details.\n' +
      '\n' +
      'For all experiments in this section we used the LBFGS optimizer with 20 inner iterations for each optimization step with line search. Stopping criterion was set by a target PSNR value, varying for different tasks. The losses, regularizations, initializations and stopping criterions of our algorithm for the linear inverse problems are listed in Table 6. In the Table \\(\\chi^{d}\\) regularization corresponds to equation 10 and \\(\\lambda\\) denotes the coefficients used.\n' +
      '\n' +
      '**Runtimes.** For noiseless tasks: inpainting center crop took on avarage \\(10\\) minutes per image, super resolution took \\(12.5\\) minutes per image and Gaussian deblurring took \\(15.5\\) minutes per image. For the noisy tasks: inpainting center crop took on avarage \\(4\\) minutes per image, super resolution took \\(2.5\\) minute per image and Gaussian deblurring took \\(3.5\\) minutes per image. Experiments ran on 32GB NVIDIA V100 GPU.\n' +
      '\n' +
      'Metrics are computed using the open source TorchMetrics library (Detlefsen et al., 2022).\n' +
      '\n' +
      '**RED-Diff Baseline.** To use the RED-Diff baseline with a FM cond-OT trained model we transform the velocity field to epsilon prediction according to 52. We searched for working parameters and reported results that outperformed the results that were produced by (Pokle et al., 2023) with an epsilon prediction model, otherwise we kept the number from (Pokle et al., 2023).\n' +
      '\n' +
      '### Inpainting with latent flow models\n' +
      '\n' +
      '#### b.2.1 Image inpainting\n' +
      '\n' +
      '**Optimization details.** In this experiment we used the LBFGS optimizer with 20 inner iterations for each optimization step with line search. Stopping criterion was set by a runtime limit of \\(30\\) minutes, but optimization usually convergences before. The solver used was midpoint with 6 function evaluations and the loss was negative PSNR without regularization. We initialized the algorithm with a backward blend with \\(\\alpha=0.25\\). To facilitate the backpropagation through a large T2I model we use gradient checkpointing.\n' +
      '\n' +
      'The validation set of the COCO dataset, used for evaluation, was downloaded from [http://images.cocodataset.org/zips/val2017.zip](http://images.cocodataset.org/zips/val2017.zip).\n' +
      '\n' +
      '**RED-Diff Baseline.** To adapt RED-Diff to a latent space diffusion model, let us recall the loss used in RED-Diff:\n' +
      '\n' +
      '\\[\\ell(\\mu)=\\|y-f(\\mu)\\|^{2}+\\lambda_{t}(\\texttt{s}\\texttt{q}\\left[\\epsilon(x( t),t)-\\epsilon\\right])^{T}\\mu \\tag{53}\\]\n' +
      '\n' +
      'where \\(f\\) can be any differentiable function. In latent diffusion/flow model for inverse problems we can model \\(f\\) as \\(f=H(\\texttt{decode}(\\mu))\\), where decode applies the decoder of the autoencoder used in the latent diffusion/flow model and \\(H\\) is the corruption operator. We use \\(\\text{lr}=0.25,\\lambda=0.25\\).\n' +
      '\n' +
      '#### b.2.2 Audio inpainting\n' +
      '\n' +
      '**Optimization details.** We follow the same setup described in B.2.1. Differently, we use \\(10\\) inner iterations and stop after \\(100\\) global iterations. We initialize the algorithm with a backward blend with \\(\\alpha=0.1\\).\n' +
      '\n' +
      '**RED-Diff Baseline.** We follow the same adaptation described above in B.2.1. We use \\(\\text{lr}=0.05,\\lambda=0.5\\).\n' +
      '\n' +
      '#### b.3 Conditional molecule generation on QM9\n' +
      '\n' +
      '**Optimization details.** In this section, we describe how Algorithm 1 was practically applied in the QM9 experiment. We initialized \\(x_{0}\\in\\mathbb{R}^{n\\times 9}\\) for the experiment, where \\(n\\) represents the molecule\'s atom count and \\(9\\) the number of attributes per atom, using a standard Gaussian distribution. To enhance optimization process stability, we ensured \\(x_{0}\\) had a feature-wise mean of zero and a standard deviation of one by normalizing it after every optimization phase. We employed the midpoint method for the ode solver, with a total of \\(100\\) function evaluations. The optimization technique utilized was LBFGS, configured with \\(5\\) optimization steps and a limit of \\(5\\) inner iterations for each step. The learning rate was set to \\(1\\). On average, generating a single molecule took approximately \\(2.5\\) minutes using a single NVIDIA Quadro RTX8000 GPU.\n' +
      '\n' +
      '**Noise prediction vs velocity field prediction.** In this section, we examine the impact of changing the stochastic sampler\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{**Inpainting-Center**} & \\multicolumn{2}{c}{**Super-Resolution X2**} & \\multicolumn{2}{c}{**Gaussian Deblur**} \\\\ \\cline{2-5}  & \\(\\sigma_{y}=0\\) & \\(\\sigma_{y}=0.05\\) & \\(\\sigma_{y}=0.05\\) & \\(\\sigma_{y}=0.05\\) & \\(\\sigma_{y}=0.05\\) \\\\ \\hline Loss & \\(-\\mathrm{PSNR}(Hx,y)\\) & \\(-\\mathrm{PSNR}(Hx,y)\\) & \\(-\\mathrm{PSNR}(H^{\\dagger}Hx,H^{\\dagger}y)\\) & \\(-\\mathrm{PSNR}(Hx,y)\\) \\\\ \\cline{3-5} Regularization & None & \\(\\chi^{d}\\), with \\(\\lambda=0.01\\) & None & \\(\\chi^{d}\\), with \\(\\lambda=0.01\\) & None & \\(\\chi^{d}\\), with \\(\\lambda=0.01\\) \\\\ Initialization & \\(0.1\\) blend & \\(0.1\\) blend & \\(0.1\\) blend & \\(0.1\\) blend & \\(0.1\\) blend \\\\ Target PSNR & 45 & 32 & 55 & 32 & 55 & 32 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Algorithmic choices for the ImageNet-128 linear inverse problems tasks.\n' +
      '\n' +
      'of the \\(\\epsilon\\)-prediction unconditional diffusion model to a velocity field (VF) deterministic sampler. Specifically, Our primary assertion is that this change in the sampler is responsible for the observed decline in the quality of the molecules we generate. Table 7 illustrates a comparison of the quality of molecules generated using the same diffusion model but with different samplers. The unconditional diffusion model from (Hoogeboom et al., 2022) was trained with a \\(1000\\) steps discrete diffusion. As mentioned in the previous part we sampled with D-flow using \\(100\\) function evaluations. The table reveals that the principal reason for the reduced quality is the alteration of the sampler, rather than the reduced step count or the optimization process itself. Future research will focus on assessing our approach with models trained with FM.\n' +
      '\n' +
      '**QM9.** The QM9 dataset (Ramakrishnan et al., 2014), a widely recognized collection, encompasses molecular characteristics and atomic positions for \\(130\\)K small molecules, each containing no more than \\(9\\) heavy atoms (up to \\(29\\) atoms when including hydrogens). The train/validation/test partitions used are according to (Anderson et al., 2019) and consists of \\(100\\)K/\\(18\\)K/\\(13\\) samples per partition. We provide additional details regarding the properties used in the experiment:\n' +
      '\n' +
      '* Tendency of a molecule to acquire an electric dipole moment when subjected to anexternal electric field.\n' +
      '* Highest occupied molecular energy.\n' +
      '* Lowest unoccupied molecular energy.\n' +
      '* The difference between HOMO and LUMO.\n' +
      '* Dipole moment.\n' +
      '* Heat capacity at \\(298.15\\)K.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Sample Method & \\multicolumn{2}{c}{NFE Molecule Stability Atom Stability} & \\multicolumn{2}{c}{Validity Validity \\& Uniqueness} \\\\  & (\\#) & (\\%) & (\\%) & (\\%) & (\\%) \\\\ \\hline EDM (\\(\\epsilon\\)-prediction, stochastic) & 1000 & 81.73 & 98.40 & 91.50 & 90.32 \\\\ EDM (\\(\\epsilon\\)-prediction, stochastic) & 100 & 78.22 & 98.00 & 90.26 & 89.00 \\\\ \\hline EDM (VF-prediction, deterministic) & 1000 & 65.90 & 97.03 & 83.70 & 83.00 \\\\ EDM (VF-prediction, deterministic) & 100 & 66.24 & 96.61 & 84.39 & 83.20 \\\\ \\hline EDM (VF-prediction, deterministic) + optimization & 100 & 54. 83 & 95.00 & 80.3 & 79.03 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Comparison of generated molecules quality using different sampling paths.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      'Figure 8: Qualitative comparison for linear inverse problems on ImageNet-128 for the noisy case. GT samples come from the face-blurred ImageNet-128 validation set.\n' +
      '\n' +
      'Figure 9: Qualitative comparison for free-form inpainting on the MS-COCO dataset using a T2I latent FM model. GT samples come from the MS-COCO validation set.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>