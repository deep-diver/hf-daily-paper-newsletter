<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Infini-gram: Unbounded \\(n\\)-gram 언어 모델을 Trillion Tokens로 스케일링\n' +
      '\n' +
      '지정 류({}^{\\heartsuit}\\) **세원민({}^{\\heartsuit}\\)**\n' +
      '\n' +
      '루크 젯틀모이어 **최예진\n' +
      '\n' +
      '**Hannaneh Hajishirzi\\({}^{\\heartsuit}\\)**\n' +
      '\n' +
      'Paul G. Allen Computer Science and Engineering, University of Washington\n' +
      '\n' +
      'Allen Institute for Artificial Intelligence\n' +
      '\n' +
      'liujc@cs.washington.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '이 시대에 \\(n\\)-그램 언어 모델은 여전히 신경 대규모 언어 모델(LLM)과 관련이 있습니까? 우리의 대답은 _yes_이며, 우리는 텍스트 분석과 신경 LLMs 개선 모두에서 그들의 값을 보여준다. 그러나 이것은 두 가지 측면에서 \\(n\\)-그램 모델을 현대화해야 한다. 먼저, 우리는 신경 LLM과 동일한 데이터 규모에서 훈련한다 - **1.4조 토큰**. 이것은 지금까지 만들어진 것 중 가장 큰 \\(n\\)-그램 모델이다. 둘째, 기존의 \\(n\\)-그램 모델은 성능을 저해하는 작은 \\(n\\)을 사용한다. 대신에 백오프가 있는 새로운 \\(\\infty\\)-그램 LM을 도입함으로써 \\(n\\)이 임의로 클 수 있도록 한다. 본 논문에서는 \\(n\\)-그램 카운트 테이블을 미리 계산하는 대신, \\(\\infty\\)-그램(뿐만 아니라 \\(n\\)-그램을 임의의 \\(n\\)) 확률로 계산할 수 있는 접미사 배열로 구동되는 **인피니-그램**이라는 이름의 엔진을 개발한다. 본 논문에서 제안한 \\(\\infty\\)-그램 프레임워크와 인피니-그램 엔진은 인간이 작성한 텍스트와 기계가 생성한 텍스트에 대해 새롭고 흥미로운 분석을 수행할 수 있게 한다. \\(\\infty\\)-그램 LM은 다음토큰 예측에 상당히 높은 정확도를 가지며(47%), 언어 모델링의 복잡성을 크게 줄일 수 있도록 신경 LLM을 보완할 수 있다. 기계 생성 텍스트를 분석할 때, 신경 LLM 사전 훈련의 결함 및 트랜스포머의 위치 임베딩을 나타내는 접미사 길이에 대한 기계-\\(\\infty\\)-그램 일치 수준의 불규칙성을 관찰한다. 우리는 대규모 텍스트 말뭉치에서 검색된 글자체 정보를 가장 잘 사용하는 방법에 대한 더 많은 연구를 가능하게 하기 위해 인피니그램 엔진을 오픈 소스한다.\n' +
      '\n' +
      '그림 1: 1.4조 톤의 말뭉치를 학습시켰을 때, 5-그램의 LM은 4의 짧은 고정된 컨텍스트 길이를 사용하기 때문에 주어진 프롬프트의 다음 토큰을 정확하게 예측할 수 없다. 우리의 \\(\\infty\\)-그램 LM은 말뭉치에서 0이 아닌 카운트를 갖는 프롬프트의 가장 긴 접미사를 기반으로 적응적으로 \\(n\\)을 선택하고, 이 경우 더 큰 \\(n\\)을 사용하여 올바른 예측을 산출한다. [\\(\\infty\\)-그램 LM에서의 계수 및 분포 추정은 인피니-그램 엔진에 의해 구동된다.\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      '조토큰 코퍼스에서 사전 훈련될 때, 신경망 대형 언어 모델들(LLMs)은 획기적인 성능을 달성한다(Touvron et al., 2023; Geng and Liu, 2023). 그러나 이러한 데이터 척도가 다른 언어 모델링 접근법에 어떤 도움이 될지는 아직 알지 못한다. 특히, 이러한 거대한 말뭉치로부터 추정된다면 고전적인 \\(n\\)-그램 언어 모델(LMs)은 얼마나 잘 수행되나요? 즉, 신경망 LLMs의 시대에 여전히 관련이 있는 _are\\(n\\)-gram LMs이다._\n' +
      '\n' +
      '우리의 대답은 _yes_이다. 우리가 보여주듯이 \\(n\\)-그램 LMs는 텍스트 분석과 신경 LMs 개선에 모두 유용하다. 그러나 우리는 먼저 훈련 데이터 크기와 \\(n\\)의 값의 두 가지 측면에서 전통적인 \\(n\\)-그램 LM을 조정해야 한다. 더 넓은 데이터 커버리지를 달성하기 위해, 우리는 \\(n\\)-그램 LMs에 대한 트레이닝 데이터를 **1.4조 토큰**으로 확장하는데, 이는 가장 큰 오픈 소스 텍스트 코퍼스(Together, 2023; Soldaini et al., 2023)의 크기와 경쟁한다. 우리가 아는 한, 이것은 지금까지 구축된 가장 큰 \\(n\\)그램 LM이다. 역사적으로, \\(n\\)-그램 인덱스는 작은 \\(n\\)의 인덱스(예: \\(n\\leq 5\\); Franz and Brants (2006))에만 구축되어 왔는데, 이는 나이브 \\(n\\)-그램 카운트 테이블의 크기가 \\(n\\)에 대해 거의 기하급수적으로 증가하기 때문이다. 우리는 대신에 \\(n\\)의 값을 증가시키는데 상당한 값이 있다는 것을 발견했다. 그림 1에서 알 수 있듯이 작은 \\(n\\)(\\(n=5\\))을 가진 \\(n\\)-그램 LM은 긴 프롬프트에서 풍부한 컨텍스트를 버리기 때문에 다음 토큰을 잘 예측하지 못하는 경우가 많다. 반면, 더 큰 \\(n\\)(이 경우 \\(n=16\\))을 사용할 수 있다면 예측은 훨씬 더 정확해질 수 있다. 따라서, 우리는 _unbounded_\\(n\\)을 갖는 \\(n\\)-그램 LM, 즉 \\(\\infty\\)**-그램 LM**을 개발한다. 우리는 더 긴 \\(n\\)-그램이 제로 카운트를 가질 때 더 작은 \\(n\\)에 의존하는 _backoff_(Jurafsky and Martin, 2000)의 변형을 사용한다. \\(\\infty\\)-gram 추정치의 _sparsity_로 인해, 이후의 몇몇 실험들(예: SS5)에서, 우리는 복잡도가 계산될 수 있는 혼합물 LM을 산출하기 위해 \\(\\infty\\)-gram LM과 신경 LMs 사이를 보간할 것이다.\n' +
      '\n' +
      '이 거대한 \\(\\infty\\)그램 LM을 제공하기 위해 낮은 지연 시간, 자원 효율적인 엔진을 개발합니다. 본 논문에서는 임의적으로 큰 \\(n\\)과 이와 같은 극단적인 데이터 스케일에 적용할 수 없는 명시적인 \\(n\\)-그램 카운트 테이블을 구축하는 대신, 추론 시간에 빠른 \\(n\\)-그램 카운팅을 지원하는 데이터 구조인 _suffix array_를 갖는 \\(\\infty\\)-그램 LM을 구현하여 저장 공간과 계산 모두에서 효율적이다. 접미사 배열 구현은 토큰당 7바이트의 스토리지(원시 데이터에 비해 3.5배 오버헤드)가 소요되며, 1.4조 토큰의 학습 데이터에서는 10 TiB의 디스크 스토리지를 사용하여 3일 이내에 단일 80코어 CPU 노드로 구축할 수 있다. 평균 추론 레이턴시는 \\(n\\)**-그램**을 카운트하기 위해 20 밀리초 미만이고, 그 발생의 모든 위치를 찾는 것( \\(n\\)이 얼마나 크거나 \\(n\\)-그램이 얼마나 자주 나타나는지에 관계없이) 및 \\(n\\)-그램/\\(\\infty\\)-그램 확률 추정 및 디코딩에 대해 토큰**당 **200 밀리초이다(SS3.4 및 표 1에서 보다). 모든 인덱스는 추론 시간에 온 디스크로 유지됩니다. 우리는 이 \\(\\infty\\)그램 엔진을 **infini-gram**이라고 부른다.\n' +
      '\n' +
      '[\\(\\infty\\)-gram(SS4)을 이용한 분석은 인간이 작성한 텍스트와 기계가 생성한 텍스트에 대한 새로운 통찰력을 제공한다. 우리는 인간이 작성한 문서의 접두사가 주어졌을 때 다음 토큰을 예측할 때 \\(\\infty\\)-gram이 상당히 높은 정확도(47%)를 가지며, 유효 \\(n\\)이 큰 토큰에서 이 정확도가 더 높다는 것을 발견했다. 대조적으로, 종래의 \\(n\\)-그램(작은 \\(n\\))은 다음 토큰을 예측하기에 충분히 긴 컨텍스트를 캡처하는데 불충분하다(29% 정확도). \\(\\infty\\)-gram 예측의 정확성은 신경 LMs에 의한 예측과 완벽하게 일치하며, 이는 \\(\\infty\\)-gram이 신경 LMs를 보완하고 개선할 수 있으며 결합 시 더 나은 성능에 도달할 수 있음을 의미한다. 사실, 우리의 실험들(SS5)은 \\(\\infty\\)-그램과 신경 LMs에 의해 만들어진 추정치들 사이에 휴리스틱하게 보간하는 것이 신경 LM이 70B만큼 큰 경우에도 신경 LMs 단독에 비해 언어 모델링 복잡성을 (최대 73%까지) 크게 감소시킬 수 있음을 보여준다. 본 논문에서는 \\(\\infty\\)-gram의 일치도를 분석할 때, 신경망 LM의 핵 샘플링(Holtzman et al., 2019)은 탐욕 디코딩 및 온도 샘플링과 같은 다른 디코딩 방법 중에서 인간이 작성한 텍스트와 가장 유사한 일치도를 갖는 기계 생성 텍스트를 생성한다. 탐욕 디코딩의 경우, 신경 LM 사전 훈련의 결함 및 트랜스포머의 위치 임베딩을 나타내는 접미사 길이에 대한 일치도의 상당한 변동을 관찰한다.\n' +
      '\n' +
      '우리는 일부 데이터 세트에 미리 구축된 인피니그램 인덱스, \\(n\\)-그램/\\(\\infty\\)-그램 추론을 수행하기 위한 코드, 새로운 코퍼스에 대한 인덱스 구축을 위한 코드를 오픈 소스1에 제공한다. 또한 \\(n\\)-gram/\\(\\infty\\)-gram 질의를 위한 공개 API 서비스를 진행할 계획이다. 우리는 이러한 자원이 대규모 텍스트 코퍼스에 대한 보다 통찰력 있는 분석과 이해를 가능하게 하고 데이터 기반 언어 모델링을 위한 새로운 길을 열 수 있기를 바란다.\n' +
      '\n' +
      '##2\\(\\infty\\)-그램 LM: 무제한 \\(n\\)으로 \\(n\\)-그램 LMs를 확장하는 것\n' +
      '\n' +
      '배경: \\(n\\)-gram LM. \\(n\\)-gram LM은 \\(n\\)-gram의 발생 횟수를 계산하는 고전적이고 통계적인 언어 모델이다. 가장 간단한 형태에서, 문맥이 주어졌을 때 토큰 \\(w_{i-(n-1):i-1}\\)의 확률은 다음과 같이 추정된다.\n' +
      '\n' +
      '[P_{n}(w_{i}|w_{i-(n-1):i-1})=\\frac{\\text{cnt}(w_{i-(n-1):i-1}w_{i}\\mid\\mathcal{D}}{\\text{cnt}(w_{i-(n-1):i-1}\\mid\\mathcal{D}}}\\tag{1}\\text{cnt}(w_{i-(n-1):i-1}\\mid\\mathcal{D}}}\n' +
      '\n' +
      '여기서 \\(\\text{cnt}(\\mathbf{w}\\mid\\mathcal{D})\\)는 학습 데이터 \\(\\mathcal{D}\\)에서 \\(n\\)-gram \\(\\mathbf{w}\\)이 나타나는 횟수이며, \\(n\\)은 미리 정의된 하이퍼파라미터이다. (n=1)일 때, 우리는 \\(w_{i-(n-1):i-1}\\)을 빈 문자열 \\(\\varepsilon\\)로 정의하며, 그 수는 \\(|\\mathcal{D}|\\)과 같다. 그러나, 이 \\(n\\)-그램 LM의 순진한 버전은 희소성 문제에 부딪힐 수 있다: 수학식 1의 분자는 0이고, 무한한 복잡성을 초래할 수 있다. 이 문제를 해결하기 위한 한 가지 일반적인 기술은 **backoff**(Jurafsky & Martin, 2000): 인스턴스 단위로 분자가 0일 때 우리는 1\\(n\\)만큼 감소하며, 분자가 양수가 될 때까지 반복해서 이것을 할 수 있다. 백오프 전략의 한 가지 주의사항은 **유효**\\(n\\)이 \\(w_{i}\\)에 의존하기 때문에 \\(P_{n}(*|w_{i-(n-1):i-1})\\에 대한 유효한 분포를 산출하지 못한다는 것이다. 그러므로, 분포의 정규화를 위해 추가적인 확률 할인이 요구된다(예를 들어, Katz backoff (Katz, 1987)).\n' +
      '\n' +
      '역사적으로 학습 데이터의 \\(n\\)-gram 카운트 테이블을 구축하여 \\(n\\)-gram LMs를 구현하였다. 이 표는 훈련 데이터에 나타나는 모든 고유한 \\(n\\)-그램을 저장하며, 각각은 그 카운트와 연관된다. 이러한 \\(n\\)-그램 카운트 테이블은 거대하고 \\(n\\)에 대해 거의 기하급수적으로 증가한다. 예를 들어, 우리는 1.4조 톤 코퍼스에 대한 5그램 카운트 테이블이 28개의 디스크 공간을 소비할 것으로 추정한다. 결과적으로, 이전의 \\(n\\)-그램 LMs는 매우 작은 \\(n\\), 가장 일반적으로 \\(n=5\\)(Franz & Brants, 2006; Aiden & Michel, 2011)으로 제한된다. 그림 1에서 설명하고 SS4에서 추가로 정량화할 것처럼 작은 \\(n\\)의 문제는 더 풍부한 컨텍스트를 폐기하여 이러한 \\(n\\)-그램 LMs가 미래 토큰을 제대로 예측하지 못한다는 것이다.\n' +
      '\n' +
      '\\(\\infty\\)g LM.\\(\\infty\\)g LM은 \\(n\\)g LM을 일반화한 것으로 개념적으로 \\(n=\\infty\\)에서 물러나기 시작한다. 우리는 백오프의 변형을 사용한다: 우리는 식 1의 분모가 0일 때만 백오프한다. 이것은 분모가 양수가 되는 순간, 분자가 여전히 0일 수도 있다는 것을 의미한다. 우리는 \\(\\infty\\)그램 LM 자체의 복잡성을 평가할 것이기 때문에 이것은 괜찮다. 인스턴스 측면에서 유효 \\(n\\)은 훈련 데이터에 나타나는 프롬프트의 **가장 긴 접미사**의 길이에 1을 더한 것과 같다.\n' +
      '\n' +
      '이 논문의 나머지 부분에 대해서는 "\\(\\infty\\)-gram"을 사용하여 \\(\\infty\\)-gram LM. \\\\ (\\infty\\)-gram은 형식적으로 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[P_{\\infty}(w_{i}|w_{1:i-1})=\\frac{\\text{cnt}(w_{i-(n-1):i-1}w_{i}\\mid\\mathcal{ D})}{\\text{cnt}(w_{i-(n-1):i-1}\\mid\\mathcal{D})}\\]\n' +
      '\n' +
      '여기서 \\(w_{1:i-1}\\)는 문서에서 \\(w_{i}\\) 앞에 있는 모든 토큰이고, 그리고\n' +
      '\n' +
      '\\[n=\\text{max}\\{n^{\\prime}\\in[1,i]\\mid\\text{cnt}(w_{i-(n^{\\prime}-1):i-1}\\mid \\mathcal{D})>0\\}.\\]\n' +
      '\n' +
      '카츠 백오프와 달리 \\(P_{\\infty}(*|w_{1:i-1})\\)는 구성에 따른 유효한 분포이며 할인이 필요하지 않다. 이는 유효 \\(n\\)은 \\(w_{1:i-1}\\)에만 의존하며 \\(w_{i}\\)에는 의존하지 않기 때문이다. \\(\\sum_{w_{i}\\in\\mathcal{V}}\\text{cnt}(w_{i-(n-1):i-1}w_{i}\\mid\\mathcal{D})= \\text{cnt}(w_{i-(n-1):i-1}\\mid\\mathcal{D})\\).\n' +
      '\n' +
      '또한, 이 \\(\\infty\\)-gram 추정치의 **sparsity**를 정의한다: 추정치는 \\(w_{i}|w_{i-(n-1):i-1})=1\\(w_{i}\\in\\mathcal{V}\\) 중 하나에 대해 희소 iff\\(P(w_{i}|w_{i-(n-1):i-1})=1\\)이고, 어휘의 다른 모든 토큰에 대해 0이다. 직관적으로, 이것은 훈련 데이터에 따라, 이러한 컨텍스트가 주어진 오직 하나의 가능한 다음 토큰이 있다는 것을 의미한다. SS4에서 보여주듯이 희소 추정치는 비희소 추정치보다 지상 진실 토큰을 더 잘 예측한다.\n' +
      '\n' +
      '\\(\\infty\\)-gram 추정치에서 신경 LMs.Sparsity를 보간하는 것은 그 평가에 문제를 야기한다: 지상진리 토큰에 할당된 제로 확률은 무한한 당혹감을 줄 것이다. 우리는 \\(\\infty\\)그램의 당혹감을 분리해서 계산하려고 시도하지 않는다. 대신, 신경망 LMs로 보간하고 신경망 LMs 단독(SS5)에 비해 복잡도 개선을 보인다. 상기 결합 모델은 형식적으로\n' +
      '\n' +
      '\\[P(y\\mid x)=\\lambda P_{\\infty}(y\\mid x)+(1-\\lambda)P_{\\text{neural}}(y\\mid x),\\\\\n' +
      '\n' +
      '여기서 \\(\\lambda\\in[0,1]\\)은 하이퍼파라미터이다.\n' +
      '\n' +
      '##3 Infini-gram: \\(n\\)-gram/\\(\\infty\\)-gram 질의를 위한 수행 엔진\n' +
      '\n' +
      '그들의 유용성을 극대화하기 위해, 우리는 현대적이고 조 단위의 토큰 텍스트 코퍼스에 \\(\\infty\\)-그램을 교육하고자 한다. 그러나, 이러한 방대한 학습 데이터에 대해 \\(|\\mathcal{D}|^{2})\\) 엔트리를 포함하기 때문에, 무한한 \\(n\\)그램 카운트 테이블을 구축하는 것은 현실적으로 불가능하다. 이 섹션에서는 \\(n\\)-그램/\\(\\infty\\)-그램 질의를 효율적으로 처리하는 인피니그램 엔진을 설명한다. 인피니-그램은 **서픽스 어레이**(SS3.1)라는 데이터 구조에 의해 전력을 공급받는다. 우리는 이 접미사 배열 인덱스(SS3.2)를 구축하는 방법과 이에 대한 \\(n\\)-그램/\\(\\infty\\)-그램 추론을 수행하는 방법을 보여줄 것이다. SS3.4에서 우리는 인피니그램으로 지원되는 6가지 유형의 쿼리를 나열하고 대기 시간을 벤치마킹한다.\n' +
      '\n' +
      '### Suffix Array\n' +
      '\n' +
      '학습데이터에서 \\(n\\)-그램과 \\(\\infty\\)-그램 LMs의 본질은 주어진 \\(n\\)-그램을 계산하는 것이다. 이러한 이유로 우리는 원래 주어진 "니들" 문자열(length \\(L\\))이 거대한 "헤이스택" 문자열(length \\(N\\))의 부분 문자열로 나타나는 횟수를 효율적으로 계산하기 위해 설계된 접미사 배열 데이터 구조를 활용한다. 서픽스 배열이 건초 더미 문자열에 대해 구축될 때, 주어진 바늘 문자열을 세는 것은 시간 복잡도 \\(O(L+\\log N)\\)를 가질 것이다.\n' +
      '\n' +
      '접미사 배열은 배열의 모든 접미사(또는 문자 배열인 문자열)의 어휘적 순서를 나타냅니다. 길이\\(N\\)의 배열에 대해 접미사 배열은 \\(N\\) 고유한 정수를 포함하며, 여기서 \\(i\\)번째 요소는 모든 접미사 중 \\(i\\)번째로 순위가 매겨진 접미사의 시작 위치이다. 도 2(왼쪽)는 예시적인 스트링, aabaca에 대한 접미사 어레이를 도시한다.\n' +
      '\n' +
      '우리는 토큰화된 학습 데이터의 바이트 배열에 접미사 배열을 구축한다(그림 2, 오른쪽). 문서는 4바이트의 문서 ID로 접두사를 붙이고 \\(\\backslash x\\)ff\\(\\backslash x\\)ff 토큰으로 분리한다. 바이트 배열에서, 각각의 연속적인 두 바이트는 토큰 ID를 나타낸다( \\(|\\mathcal{V}|<2^{16}=65536\\) 학습 데이터가 \\(N\\)개의 토큰을 가지고 있다는 점을 고려할 때, 바이트 배열의 크기는 \\(2N\\) 바이트이다. 접미사 배열에는 \\(N\\) 요소가 포함되어 있으며, 각 요소는 바이트 배열의 토큰을 저장하여 바이트 배열에 표시한다.\n' +
      '\n' +
      '그림 2: **왼쪽: 장난감 문자열에 대한 접미사 배열. 오른쪽: 훈련 데이터에 \\(N=4\\) 토큰이 있는 인피니그램 구현에서 접미사 배열의 예시.**\n' +
      '\n' +
      'byte offset. 접미사 배열의 모든 요소는 짝수(유효한 토큰을 가리키도록)이며 바이트 배열의 모든 토큰 위치는 접미사 배열에 정확히 한 번 나타납니다. 각 포인터는 \\(\\lceil\\log_{2}(2N)/8\\rceil\\) 바이트로 저장될 수 있다. 2B 내지 500B 토큰(우리가 다루는 범위, 샤딩 후(SS3.2))을 갖는 코퍼스의 경우, 이것은 포인터당 5 바이트이고, 따라서 접미사 어레이의 크기는 \\(5N\\) 바이트이다. 따라서 토큰화된 데이터와 접미사 배열의 결합된 크기(즉, **인피니-그램 인덱스**)는 \\(7N\\) 바이트이다.\n' +
      '\n' +
      '접미사 배열 구축\n' +
      '\n' +
      '서픽스 어레이는 바이트 어레이의 길이에 대하여 선형 시간으로 구축될 수 있다(Karkkainen et al., 2006). 본 논문에서는 Lee et al.(2022)의 접미사 배열 구현에 적응하여 효율성을 위해 최적화하였다. 그런 다음 360B-토큰 파일(Gao et al., 2020) 및 1.4T-토큰 RedPajama(Together, 2023)에 대한 접미사 배열을 구축했다. 80개의 CPU와 512G RAM을 가진 단일 노드에 RedPajama를 위한 접미사 배열을 만드는 데 56시간이 걸렸다.\n' +
      '\n' +
      '샤딩.접미사 어레이를 구축하는 것은 바이트 어레이에 대한 무거운 랜덤 액세스를 필요로 하며, 따라서 전체 바이트 어레이는 빌딩 시간이 합리적이도록 RAM에 보관되어야 한다. 그러나 바이트 배열이 너무 커서 RAM에 들어갈 수 없습니다. 우리는 바이트 배열을 여러 조각으로 나누고 각 조각에 대해 접미사 배열을 만든다. 샤딩은 추가 추론 지연을 유발하며, 이는 아래에서 논의하고 완화한다(SS3.3).\n' +
      '\n' +
      '### 접미사 배열의 추론\n' +
      '\n' +
      '\\(n\\)-gram counting. \\(n\\)-gram 확률을 계산하는 것은 토큰 문자열의 발생 횟수, 즉 \\(\\mathrm{cnt}(x_{1}...x_{n})\\을 계산하는 것을 포함한다. 접미사 배열은 학습 데이터의 모든 접미사들의 어휘적 순서를 나타내므로, \\(x_{1}...x_{n}\\)으로 시작하는 문자열의 발생 위치는 접미사 배열에서 연속되는 단일 세그먼트에 있다. 따라서 우리는 첫 번째와 마지막 발생 위치만 찾으면 되며 카운트는 이러한 위치 간의 차이일 것이다. 첫 번째 및 마지막 발생 위치를 모두 찾을 수 있습니다.\n' +
      '\n' +
      '그림 3: 훈련 데이터에 대한 \\(n\\)-그램/\\(\\infty\\)-그램 쿼리는 연관된 _suffix array_에 의해 지원된다. 트레이닝 데이터와 접미사 배열은 모두 일반 파일로 온 디스크에 저장됩니다. 흰색 스트립의 내용은 파일 데이터이고, 스트립 위의 주소는 바이트 간격띄우기입니다. 특정 \\(n\\)-gram에 대한 질의는 접미사 배열의 연속적인 세그먼트를 반환하며, 각 요소는 \\(n\\)-gram이 나타나는 훈련 데이터로의 포인터이다. 예를 들어, 조 단위 토큰 훈련 데이터에서는 인공지능, A Modern이 42번 나타나고, 모든 경우에 다음 토큰은 Approach이다.\n' +
      '\n' +
      '2진 탐색은 시간 복잡도 \\(O(n\\cdot\\log N)\\)와 \\(O(\\log N)\\) 랜덤 배열에 접근한다. 두 개의 이진 검색을 병렬화하여 대기 시간을 대략 2배 줄일 수 있습니다. 쿼리 길이\\(n\\)의 영향은 무시할 수 있는데, 이는 컴퓨터가 일반적으로 (일반적으로) 4096바이트의 페이지에서 메모리를 가져오고 문자열 비교가 페이지 가져오기보다 훨씬 빠르기 때문이다. 따라서, 이하에서 시간 복잡도를 분석할 때 랜덤 배열 접속 횟수를 참조한다.\n' +
      '\n' +
      '발생 위치와 문서를 찾습니다.\\ 접미사 배열을 이용한 (n\\)-그램 카운팅은 부산물을 가지고 있다. 또한 학습 데이터에서 \\(n\\)-그램이 나타나는 모든 위치를 무료로 알 수 있다. 이 위치 정보는 우리가 계수하는 동안 얻은 접미사 배열 세그먼트에 암묵적으로 포함되어 있으며 \\(n\\)-그램이 나타나는 원본 문서를 검색하기 위해 이 세그먼트 내의 각 포인터를 훈련 데이터로 다시 추적하고 문서 분리기에 부딪힐 때까지 양 방향으로 확장하기만 하면 된다.\n' +
      '\n' +
      '음영의 영향.접미사 배열이 샤드 바이트 배열에 구축되면 각 개별 샤드에 대해 카운팅을 수행하고 모든 샤드에 대해 카운트를 누적할 수 있다. 지연시간은 샤드의 수에 비례하며, 시간복잡도는 \\(O(S\\cdot\\log N)\\)이 된다. 다른 샤드들의 프로세싱은 병렬화될 수 있어서, 시간 복잡도를 다시 \\(O(\\log N)\\)로 감소시킨다.\n' +
      '\n' +
      '이전 검색 결과를 다시 사용하여 \\(n\\)-그램 계산 속도를 높인다. 접미사 배열에서 \\(x_{1}...x_{n}\\)에 대한 세그먼트는 \\(x_{1}...x_{n-1}\\에 대한 세그먼트의 하위 세그먼트여야 한다. 따라서 \\(n\\)-gram 확률 \\(P_{n}(x_{n}\\mid x_{1}...x_{n-1})\\)을 계산할 때 먼저 \\(x_{1}...x_{n-1}\\)을 계산하고 \\(x_{1}...x_{n}\\)을 계산할 때 \\(x_{1}...x_{n}\\)의 세그먼트 내에서 첫 번째와 마지막 발생 위치만 검색하면 최대 2배까지 지연 시간을 줄일 수 있다.\n' +
      '\n' +
      '온 디스크 검색.바이트 배열과 접미사 배열이 너무 커서 RAM에 맞지 않을 수 있으므로 실제로는 디스크에 보관하고 메모리 매핑 파일로 읽습니다. 그러나 이진 검색이 바이트 배열 및 접미사 배열에 대한 임의 액세스를 요구하기 때문에 이것은 상당한 대기 시간을 생성한다. 이를 완화하기 위해 가까운 장래에 읽을 어레이 오프셋을 시스템에 알려주는 메모리 **pre-fetching** 방법을 구현했다. 사전-인출은 평균 레이턴시를 대략 5배 감소시킨다.\n' +
      '\n' +
      '\\(\\infty\\)-gram 계산 속도를 높인다.\\(\\infty\\)-gram 확률을 계산하기 위해서는 각 접미사 \\(x_{i-n+1}...x_{i}\\)의 발생을 최대 \\(n\\)까지 세어 접미사들이 여전히 충분한 출현 요건을 만족하도록 해야 한다(우리는 이 최대 \\(n\\)을 \\(L\\)으로 나타낸다). 이것은 \\(O(L)\\) 카운팅 연산을 의미하며, 각 \\(\\infty\\)-그램 연산에 대한 시간 복잡도는 \\(O(L\\cdot\\log N)\\)이다. 그러나, 간단한 이진 리프팅+이진 검색 알고리즘은 계산 횟수를 \\(O(\\log L)\\으로 줄일 수 있으며, 따라서 각 \\(\\infty\\)-그램 계산의 시간 복잡도는 \\(O(\\log L\\cdot\\log N)\\)이 된다.\n' +
      '\n' +
      '조밀한 \\(\\infty\\)-그램 계산 속도를 높이며, 평가 시 테스트 문서에 있는 각 토큰의 \\(\\infty\\)-그램 확률을 계산해야 한다. 우리는 한 토큰에 대한 유효 \\(n\\)이 이전 토큰에 대한 유효 \\(n\\)보다 최대 하나의 토큰이 더 길다는 것을 관찰함으로써 계산을 절약할 수 있다. 이는 각 토큰을 평가하기 위한 상각 시간 복잡도를 \\(O(\\log N)\\)으로 감소시킨다.\n' +
      '\n' +
      '### 지원 쿼리 유형 및 대기 시간 벤치마킹\n' +
      '\n' +
      'Infini-gram은 다음과 같은 유형의 \\(n\\)-gram/\\(\\infty\\)-gram 질의를 지원한다:\n' +
      '\n' +
      '1. \\(n\\)-그램을 카운트하는 단계(Count);\n' +
      '2. \\(n\\)-gram LM(with given \\(n\\), no backoff)(NgramProb);\n' +
      '3. \\(n\\)-그램 LM(NgramDist)으로부터 전체 다음-토큰 분포를 계산하는 단계;\n' +
      '4. \\(\\infty\\)-gram LM(InfinigramProb)으로부터 토큰 확률을 계산하는 단계;\n' +
      '5. \\(\\infty\\)-그램 LM(InfinigramDist)으로부터 전체 다음-토큰 분포를 계산하는 단계;\n' +
      '6. AND\'s 및/또는 OR\'s (예를 들어, (자연어 처리 OR 인공 지능) AND (딥 러닝 OR 머신 러닝))와 연결된, \\(n\\)-gram, 또는 \\(n\\)-gram 용어의 CNF 논리 표현을 포함하는 랜덤 문서를 리턴하는 단계(GetDocument).\n' +
      '\n' +
      '이러한 쿼리 유형을 구현하는 알고리즘은 알고리즘 1 및 2(부록)를 참조하십시오. 당사의 온라인 데모는 위의 쿼리 유형을 완전히 지원합니다.\n' +
      '\n' +
      '본 논문에서는 다양한 종류의 \\(n\\)-그램과 \\(\\infty\\)-그램 질의에서 인피니-그램의 지연시간을 벤치마킹하고, 표 1의 결과를 보여준다. 추론 과정에서 학습 데이터와 접미사 배열은 SSD에 저장된다. 각 질의 유형에 대해, 벤치마킹은 파일 검증 데이터에서 무작위로 그리고 독립적으로 샘플링된 1,000개의 토큰(연속 토큰에서 \\(\\infty\\)-그램 LM으로부터 토큰 확률을 계산하는 작업 제외)에 대해 수행되며, 여기서 10개의 문서를 샘플링하고 각 문서에서 1000개의 연속 토큰을 처리했다.\n' +
      '\n' +
      '모든 유형의 쿼리는 조토큰 학습 데이터에서 2초 이하의 대기 시간을 보여줍니다. RedPajama를 사용하여 \\(\\infty\\)-그램으로부터 토큰 확률을 계산하는 것은 단지 135 밀리초밖에 걸리지 않는다. 또한, 구현은 임의로 큰 \\(n\\)을 갖는 \\(n\\)-그램의 발생을 계수하는 것을 지원하며, 대략 20 밀리초(n=1000\\)에서 일정한 지연시간을 갖는다. 디코딩은 전체 다음-토큰 분포를 계산해야 하므로, \\(n\\)그램 LMs로 토큰당 39밀리초, \\(\\infty\\)그램으로 토큰당 180밀리초로 약간 느려진다.\n' +
      '\n' +
      '##4 \\(\\infty\\)-gram을 이용한 인간 글과 기계 생성 텍스트 분석\n' +
      '\n' +
      '이 절에서는 주로 \\(\\infty\\)-그램과 실제 텍스트 사이의 토큰적 일치를 중심으로 \\(\\infty\\)-그램의 관점에서 인간 작성 텍스트와 기계 생성 텍스트에 대한 몇 가지 분석을 제시한다. 요약하면, 우리는 다음과 같은 것을 발견했다.\n' +
      '\n' +
      '1. \\(\\infty\\)-그램은 사람이 작성한 문서의 접두사가 주어진 다음 토큰을 예측할 때 상당히 높은 정확도(47%)를 가지며, 프롬프트의 더 긴 접미사가 사용될 수 있을 때(즉, 유효 \\(n\\)가 더 클 때) 이러한 정확도가 더 높다;\n' +
      '2. 기존의 \\(n\\)-그램(\\(n\\leq 5\\))은 다음 토큰을 결정하기에 충분한 긴 컨텍스트를 캡처하는데 불충분한 반면, 우리의 \\(\\infty\\)-그램 방법은 인간 작성 및 기계 생성 텍스트에 대한 높은 예측력;\n' +
      '3. \\(\\infty\\)-gram은 인간이 작성한 텍스트를 예측할 때 신경 LM을 보완하고 개선할 수 있는 상당한 잠재력을 가지고 있다(우리는 SS5에서 추가로 조사한다).\n' +
      '4. 접미사 길이에 대한 일치도를 그릴 때, 핵 샘플링이 있는 신경 LMs에 의해 생성된 텍스트는 탐욕 디코딩 및 온도 샘플링과 같은 다른 디코딩 방법 중에서 인간이 작성한 텍스트와 가장 유사하다. 탐욕 디코딩의 경우, 일치도는 상당한 변동을 겪으며, 이는 신경 LM 사전 훈련의 결함 및 트랜스포머의 위치 임베딩에 기인할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline \\multicolumn{2}{c}{**Reference Data \\((\\rightarrow)\\)**} & **Pile-train** & **RPJ** & **Time Complexity** \\\\  & \\(N=0.36T\\) & \\(N=1.4T\\) & (measured by number \\\\\n' +
      '**Query Type \\((\\downarrow)\\)** & \\(S=2\\) & \\(S=8\\) & of random disk accesses) \\\\ \\hline \\hline \\multicolumn{2}{c}{1. Counting an \\(n\\)-gram} & \\multicolumn{1}{c}{\\(O(\\log N)\\)} \\\\... (\\(n=1\\)) & 7 ms & 9 ms & \\\\... (\\(n=2\\)) & 13 ms & 20 ms & \\\\... (\\(n=5\\)) & 14 ms & 19 ms & \\\\... (\\(n=10\\)) & 13 ms & 18 ms & \\\\... (\\(n=100\\)) & 13 ms & 19 ms & \\\\... (\\(n=1000\\)) & 14 ms & 19 ms & \\\\ \\hline \\multicolumn{2}{c}{2. Computing a token probability from \\(n\\)-gram LM (\\(n=5\\))} & 19 ms & 30 ms & \\(O(\\log N)\\) \\\\ \\multicolumn{2}{c}{3. Computing full next-token distribution from \\(n\\)-gram LM (\\(n=5\\))} & 31 ms & 39 ms & \\(O(V\\cdot\\log N)\\) \\\\ \\multicolumn{2}{c}{4. Computing a token probability from \\(\\infty\\)-gram LM} & 90 ms & 135 ms & \\(O(\\log L\\cdot\\log N)\\) \\\\ \\multicolumn{2}{c}{... on consecutive tokens} & 12 ms & 20 ms & \\(O(\\log N)\\) \\\\ \\multicolumn{2}{c}{5. Computing full next-token distribution from \\(\\infty\\)-gram LM} & 88 ms & 180 ms & \\(O((\\log L+V)\\cdot\\log N)\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 상이한 유형의 질의에 대한 인피니-그램의 추론-시간 지연. 쿼리당 평균 대기 시간이 보고됩니다. C++로 작성된 추론 엔진(병렬화된 샤드 처리)으로 벤치마크되고 단일 80코어 CPU 노드에서 실행됩니다. 시간 복잡도에 대한 주석: 참조 데이터의 토큰 수; 접미사 배열에 대한 샤드 수; 질의 문서의 토큰 수; 어휘 크기.\n' +
      '\n' +
      '이 섹션의 분석을 위해 오염 제거 후 3,600억 토큰(LLaMA tokenizer 기준)으로 구성된 \\(\\infty\\)-gram의 훈련 데이터로 Pile의 훈련 세트(Gao et al., 2020)를 사용한다.\n' +
      '\n' +
      '제염.시험 유출을 방지하기 위해 훈련 데이터가 평가 데이터에 대해 제염되는 것이 중요하다. 그루네벨트(Groeneveld, 2023)의 방법을 이용하여 평가자료와 중복이 너무 많으면 문서를 필터링하는 방법으로 파일의 검증 및 시험자료(아래와 SS5에서도 사용)에 대하여 파일의 훈련자료에 대한 오염제거를 실행하였다. 자세한 내용은 SSA.1을 참조하십시오.\n' +
      '\n' +
      '오염 제거는 자명하지 않으며, 예를 들어 동일한 문장이 있는 경우 오염인지 또는 실제 테스트 시간 시나리오에서 자연적으로 발생하는 인용문인지 등 그 정의가 달라질 수 있다. 표준 오염 제거 모범 사례를 따르고 이후 섹션에서 자세한 분석을 수행한다.\n' +
      '\n' +
      '### Human-written Text\n' +
      '\n' +
      '설치.인간이 작성한 텍스트의 샘플로 파일의 유효성 검사 데이터를 사용합니다. 이 분석을 위해 파일의 유효성 검사 세트의 각 도메인에서 50개의 문서를 샘플링하고 각 문서를 최대 1024개의 토큰으로 절단했다(따라서 도메인당 총 토큰 수는 약 50k). 모든 도메인의 결과를 집계합니다.\n' +
      '\n' +
      '우리는 \\(\\infty\\)-gram의 추정치와 실제 사람이 쓴 텍스트 사이의 토큰-wise _agreement_를 측정한다. 전체 다음-토큰 분포(또는 그것의 argmax)를 \\(\\infty\\)-그램에서 계산하는 것은 비교적 느리기 때문에 실제 다음-토큰의 \\(\\infty\\)-그램 확률을 계산하고, 이 확률이 0.5.2보다 높으면 정확한 것으로 간주한다. 모든 토큰은 유효 \\(n\\), 즉 훈련 데이터에서 0이 아닌 카운트가 있는 프롬프트의 가장 긴 접미사 길이를 더한 값으로 더 분류한다. 각 범주에 대해 이러한 토큰의 수(회색 막대)와 일치 수준(녹색 점)을 시각화한다.\n' +
      '\n' +
      '각주 2: 이것은 간격이 작지만 argmax 정확도의 하한이다.\n' +
      '\n' +
      '시각화는 그림 4(중간 도표)에 나와 있다. 전체적으로 \\(\\infty\\)-gram은 토큰의 47%에서 인간이 작성한 텍스트와 일치한다. 우리는 유효 \\(n\\geq 16\\)의 증가에 따라 \\(\\infty\\)-gram이 더 정확해지는 것을 알 수 있다: 유효 \\(n\\geq 16\\)의 일치도가 75%보다 높을 때. 추가 분석(부록 그림 14)을 통해 학습 데이터에서 가장 긴 접미사의 개수가 일치도에 큰 영향을 미치지 않음을 알 수 있다.\n' +
      '\n' +
      '그림 4의 왼쪽 그림에서 우리는 동일한 훈련 데이터를 사용하여 5-그램 LM에 대해 동일한 분석을 보여주며 \\(\\infty\\)-그램보다 훨씬 낮은 일치를 보인다. 이전 문헌(Franz and Brants, 2006; Aiden and Michel, 2011)에서 광범위하게 사용된 5-그램은 다음 토큰을 올바르게 예측하기에 충분한 컨텍스트를 포착하지 못하는데, 평가 데이터의 90% 이상의 토큰은 최소 5의 유효 \\(n\\)을 가지며, \\(\\infty\\)-그램 분석은 유효 \\(n\\)의 중앙값은 7 토큰이고 평균은 9.1 토큰임을 보여준다.\n' +
      '\n' +
      '그림 4의 오른쪽 그림에서 우리는 전체 토큰의 50% 이상을 차지하는 희소 \\(\\infty\\)-그램 추정치를 가진 토큰에 대해서만 동일한 분석을 보여준다. 전체 일치도(75%)가 훨씬 높고, 유효 \\(n\\geq 14\\)일 때 일치도가 80%보다 높다. 이는,\n' +
      '\n' +
      '그림 4: 인간 생성 텍스트와 \\(n\\)-그램/\\(\\infty\\)-그램 사이의 토큰별 일치. 좌측:**\\(n\\)-그램; **중간:**\\(\\infty\\)-그램; **오른쪽:**\\(\\infty\\)-그램, 희소 추정치만 있는 토큰에서.\n' +
      '\n' +
      '다음 토큰이 훈련 데이터에 따라 고유할 때, 그 고유 토큰은 인간이 작성한 텍스트에서 실제 토큰일 가능성이 매우 높다.\n' +
      '\n' +
      '그림 5에서는 LLaMA-2 모델(Touvron et al., 2023b)에 의해 주어진 확률 분포를 인간 텍스트에서 실제 토큰에 나타내고, 각 확률 범위에서 토큰에 대한 인간-\\(\\infty\\)-그램 일치를 나타낸다. (할당된 확률이 높을수록 LLaMA-2는 실제 토큰과 더 높은 일치를 갖는다.) 우리는 신경망 LM과 \\(\\infty\\)-그램 사이의 실제 텍스트와의 일치에 대해 긍정적이지만 불완전한 상관 관계를 관찰한다. 특히, 신경망 LM 성능이 매우 나쁜 경우(히스토그램의 왼쪽)에 \\(\\infty\\)-그램은 여전히 20% 이상의 사소한 일치를 제공하며, 희소 \\(\\infty\\)-그램 추정치가 있는 토큰만 고려한다면 이는 50%만큼 높다. 이는 인간이 작성한 텍스트를 예측할 때 \\(\\infty\\)그램으로 신경 LM의 성능을 보완하고 개선할 수 있는 엄청난 잠재력을 의미하며, SS5에서 추가로 조사한다.\n' +
      '\n' +
      '### Machine-generated Text\n' +
      '\n' +
      '설정.인간이 작성한 텍스트를 사용한 분석과 유사하게 파일의 각 도메인에서 50개의 문서를 샘플링했다. 우리는 각 문서의 처음 50개의 토큰을 사용하여 연속을 생성하기 위해 신경 LM을 프롬프트한다. 제너레이션은 문서의 원래 길이까지 계속되거나 [EOS] 토큰이 생성될 때 계속됩니다. 우리는 탐욕 디코딩, 온도 샘플링, 핵 샘플링의 세 가지 디코딩 방법을 실험한다(Holtzman et al., 2019). 신경 LM들은 GPT-Neo(1.3b, 2.7b), GPT-J(6b), 및 LLaMA-2(7b, 13b, 70b)이다.\n' +
      '\n' +
      '디코딩 방법의 영향.도 6의 상단 행은 동일한 뉴럴 LM - LLaMA-2-70b에 대한 세 개의 디코딩 방법을 도시한다. 일반적으로 확률성이 증가하면 유효\\(n\\)이 작은 쪽으로 이동하고 일치 수준도 감소한다. 핵 샘플링(\\(p=0.8\\)과 함께)은 인간이 쓴 텍스트(그림 4, 중간 도표)와 효과적인 \\(n\\)의 분포가 가장 유사하며, 이는 아마도 핵 샘플링이 텍스트 생성에서 일반적으로 선호되는 이유일 것이다. 이와 비교하여, 탐욕 디코딩은 인간-필적 텍스트보다 훨씬 더 높은 유효 \\(n\\)을 갖는다.\n' +
      '\n' +
      '그림 5: 인간 작성 텍스트 토큰에 대한 신경 LMs에 의해 할당된 확률의 분포 및 이러한 토큰과의 \\(\\infty\\)-그램의 일치. ** 상위:** 모든 토큰에서 **하위:** 희소\\(\\infty\\)-그램 추정치가 있는 토큰에서 ** **takeaway:**\\(\\infty\\)-그램과 신경 LM은 서로 다른 토큰에서 실제 인간 텍스트를 예측하므로 \\(\\infty\\)-그램 추정치, 특히 희소 \\(\\infty\\)-그램 추정치는 신경 LM을 보완하는 데 사용할 수 있다.\n' +
      '\n' +
      '탐욕스러운 디코딩은 다양성 부족뿐만 아니라 훈련 데이터의 과도한 암기로 이어질 수 있다는 암시이다.\n' +
      '\n' +
      '한 가지 매우 흥미로운 현상은 효과적인 \\(n\\)이 증가함에 따라 그리디 디코딩에서 일치 수준이 크게 변동한다는 것이다(그러나 핵이나 온도 샘플링은 그렇지 않으며 일치 수준은 거의 단조롭게 증가한다). 이러한 변동은 더 작은 모델(LLaMA-2-13b/7b, GPT-J-6b, GPT-Neo-2.7b/1.3b/125m)에 대해 훨씬 더 빠르며, LLaMA-2-7b에 대해 변동은 심지어 주기적이다(유효 \\(n=20,24,28,32\\)에서 빠르게 감소함; 이것은 통계적으로 유의하며, 2-비례 z-검정은 p-값 \\(<10^{-99}\\)을 제공한다. 우리는 이것이 이러한 트랜스포머 기반 모델을 사전 훈련할 때 위치 임베딩의 적용으로 인해 발생할 수 있다고 의심하며 커뮤니티의 추가 조사를 환영한다.\n' +
      '\n' +
      '모델 크기의 영향.그림 6의 마지막 두 행은 그리디 디코딩 하에서 다른 크기의 신경 LM에 대해 동일한 분석을 보여준다. 일반적으로 모델 크기가 증가하면 유효\\(n\\)이 큰 쪽으로 약간 이동하고 일치 수준도 증가한다. 이것은 더 큰 모델들이 트레이닝 데이터로부터 더 많이 암기하고, 또한 글자바텀을 복사하는 경향이 더 높다는 것을 나타낸다. GPT-Neo/J 모델의 일치도는 LLaMA-2 모델보다 높은데, 이는 아마도 GPT-Neo/J가 우리의 \\(\\infty\\)그램 훈련 데이터(즉, 파일의 훈련 세트)와 동일한 데이터에 대해 훈련되기 때문일 것이다. 전반적으로, 이러한 신경망 LM에 의해 생성된 텍스트는 인간 텍스트로서 \\(\\infty\\)-그램과 유사한 일치 수준을 갖는다.\n' +
      '\n' +
      '##5 \\(\\infty\\)-gram을 이용한 신경망 LMs 개선\n' +
      '\n' +
      'SS4의 결과는 더 나은 언어 모델을 생성하기 위해 (interpolate; SS2) 신경 LMs와 \\(\\infty\\)-gram을 결합하도록 동기를 부여한다. 이 절에서는 결합 모델의 강한 실험 결과를 보여준다. SS4에서 우리는 \\(\\infty\\)그램 추정치가 _sparse_일 때 인간이 쓴 텍스트와 더 높은 일치를 갖는다는 것을 발견했다. 따라서, 우리는 \\(\\lambda_{1}\\)의 두 개의 개별 보간 하이퍼파라미터를 사용한다.\n' +
      '\n' +
      '그림 6: 기계 생성 텍스트와 \\(\\infty\\)-그램 간의 토큰별 일치. 모든 토큰이 고려됩니다.\n' +
      '\n' +
      'sparse와 \\(\\lambda_{2}\\)는 비-sparse \\(\\infty\\)-gram 추정치이다. 이러한 하이퍼파라미터는 결합 모델의 복잡성을 최소화하기 위해 검증 세트에서 조정된다.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      'Evaluation.We compute perplexity on the Pile validation and test data (Gao et al., 2020). 우리는 데이터의 각 문서를 최대 시퀀스 길이가 1,024이고 슬라이딩 윈도우가 512인 배치로 분할하며, 이는 사전 언어 모델링 문헌에서 표준인 설정이다(Baevski and Auli, 2019; Khandelwal et al., 2020).\n' +
      '\n' +
      'Metric.우리는 평가 데이터에 대한 각 모델의 복잡성을 측정하고 모델 간의 복잡성의 상대적 개선을 측정한다. 모델\\(M_{o}\\)에 대한 모델\\(M\\)의 상대적 개선은 다음과 같이 계산된다.\n' +
      '\n' +
      '\\[\\Delta=(1-\\frac{\\text{PPL}(M)-1}{\\text{PPL}(M_{o})-1}\\times 100\\%\\tag{2}\\]\n' +
      '\n' +
      '이것은 완벽한 언어 모델링을 향해 폐쇄된 복잡성 갭의 백분율이다(즉, \\(\\text{PPL}=1\\)).\n' +
      '\n' +
      '참조 데이터.혼동을 줄이기 위해 본 절에서는 "참조 데이터"를 사용하여 \\(\\infty\\)-그램의 학습 데이터를 참조할 것이다. 이전 분석(SS4)에서 사용한 Pile의 훈련 세트(Gao et al., 2020) 외에도 RedPajama(Together, 2023)를 참조 데이터로 간주한다. 파일과 레드파자마는 각각 3,600억 토큰과 1조 4,000억 토큰으로 최대 1조 8,000억 토큰(LLaMA tokenizer 기준)을 합산하고 있다. 우리는 나중에 참조 데이터의 다양한 크기와 도메인에 대한 삭제를 수행한다.\n' +
      '\n' +
      '신경 LMs. 우리는 \\(\\infty\\)-그램으로 보간하기 위해 기준선과 모델로서 크고 경쟁적인 신경 LMs의 범위를 사용한다. 총 14개의 모델이 고려됩니다.\n' +
      '\n' +
      '크기가 117M, 345M, 및 774M 내지 1.6B인 가장 초기의 자기회귀 언어 모델들 중 하나인, **GPT-2**(Radford et al., 2019). 그들의 학습 데이터는 공개되지는 않았지만 다양한 웹 텍스트 집합이다.\n' +
      '125M, 1.3B, 2.7B에서 6.7B까지 크기가 다양한 파일 상에서 학습된 언어 모델인 **GPT-Neo**(Gao et al., 2020) 및 **GPT-J**(Wang and Komatsuzaki, 2021)를 포함한다.\n' +
      'LLaMA(Touvron et al., 2023a)의 후속 버전인 **LLaMA-2**(Touvron et al., 2023b)는 2조 토큰에 대해 트레이닝되고 7B, 13B, 및 70B의 크기를 갖는다. LLaMA-2는 논문 작성 시 가중치를 사용할 수 있는 가장 경쟁력 있는 언어 모델 중 하나이다. LLaMA-2의 훈련 데이터는 알려지지 않았지만, 선행 버전은 RedPajama(함께, 2023)에 의해 복제된 커먼 크롤, 위키피디아 및 코드의 큰 코퍼스에 대해 훈련된다.\n' +
      '* **SILO**(Min et al., 2023a), 1.3B 언어 모델은 허가된 데이터에만 대해 트레이닝된다. 원본 논문은 허가된 데이터에 대한 훈련은 코드 및 정부 텍스트와 같은 매우 특정한 도메인에 편향되기 때문에 _extreme domain generalization_의 도전으로 이어진다는 것을 보여주었다. 우리는 서로 다른 허용 수준에 대해 훈련된 PD, PDSW 및 PDSWBY의 세 가지 다른 변형을 사용하여 다양한 수준의 도메인 일반화 챌린지를 유도한다.\n' +
      '\n' +
      'GPT-2, GPT-Neo 및 GPT-J의 복잡성은 서로 비슷하지만, LLaMA-2 및 SILO의 복잡성은 서로 유사하지 않으며, 이는 서로 다른 토큰화기를 기반으로 하기 때문이다. 우리는 각 유형의 토큰화기에 대해 하나씩 인피니그램 인덱스의 세 가지 버전을 구축했다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '실험 결과는 표 2와 표 3에 나와 있다. \\(\\infty\\)-gram으로 보간하면 신경 LMs의 복잡성이 크게 개선되고 일관되게 개선된다. 동일한 모델 시리즈 내에서 신경 LM 크기가 증가함에 따라 개선 경향의 양이 감소하는 반면 가장 큰 모델은 여전히 우리의 방법에서 많은 이점을 얻을 수 있다(예: 파일 단독으로 LLaMA-2(\\(70\\)B)를 12% 향상시킨다).\n' +
      '\n' +
      '그러나 이러한 추세는 다른 일련의 LM에 걸쳐 유지되지 않는다. 예를 들어 \\(\\infty\\)-gram은 GPT-2(1.6B)를 34% 개선할 수 있지만, 더 작은 모델인 GPT-Neo(1.3B)만 16% 개선할 수 있다. 이는 GPT-Neo/J 모델이 파일에서 훈련되는 반면 GPT-2 모델은 그렇지 않다는 사실에 기인할 수 있다. \\ (\\infty\\)-그램은 데이터 다양성의 중요성을 강조하는 사전 훈련 데이터 분포와 참조 데이터 분포가 다르거나 보완될 때 가장 잘 작동한다. 한편, \\(\\infty\\)-gram은 이미 참조 데이터에 미리 훈련된 신경 LMs를 개선한다는 사실은 \\(\\infty\\)-gram을 사용하는 장점이 일관적이라는 것을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r r r r} \\hline \\hline \\multirow{2}{*}{**Neural LM**} & \\multirow{2}{*}{**(Size)**} & \\multirow{2}{*}{**Ref Data**} & \\multicolumn{3}{c}{Validation} & \\multicolumn{3}{c}{Test} \\\\ \\cline{3-8}  & & & **Neural** & + \\(\\infty\\)-gram & **Neural** & **+ \\(\\infty\\)-gram** \\\\ \\hline GPT-2 & 117M & Pile & 22.82 & **13.71** & (42\\%) & 22.86 & **13.58** & (42\\%) \\\\ GPT-2 & 345M & Pile & 16.45 & **11.22** & (34\\%) & 16.69 & **11.18** & (35\\%) \\\\ GPT-2 & 774M & Pile & 15.35 & **10.39** & (35\\%) & 15.40 & **10.33** & (35\\%) \\\\ GPT-2 & 1.6B & Pile & 14.42 & **9.93** & (33\\%) & 14.61 & **9.93** & (34\\%) \\\\ \\hline GPT-Neo & 125M & Pile & 13.50 & **10.76** & (22\\%) & 14.08 & **10.79** & (25\\%) \\\\ GPT-Neo & 1.3B & Pile & 8.29 & **7.31** & (13\\%) & 8.61 & **7.36** & (16\\%) \\\\ GPT-Neo & 2.7B & Pile & 7.46 & **6.69** & (12\\%) & 7.77 & **6.76** & (15\\%) \\\\ GPT-j & 6.7B & Pile & 6.25 & **5.75** & (10\\%) & 6.51 & **5.85** & (12\\%) \\\\ \\hline LLMA-2 & 7B & Pile & 5.69 & **5.05** & (14\\%) & 5.83 & **5.06** & (16\\%) \\\\ LLMA-2 & 13B & Pile & 5.30 & **4.75** & (13\\%) & 5.43 & **4.76** & (15\\%) \\\\ LLMA-2 & 70B & Pile & 4.59 & **4.21** & (11\\%) & 4.65 & **4.20** & (12\\%) \\\\ \\hline \\hline LLMA-2 & 7B & Pile + RPJ & 5.69 & **4.66** & (22\\%) & 5.83 & **4.66** & (24\\%) \\\\ LLMA-2 & 13B & Pile + RPJ & 5.30 & **4.41** & (21\\%) & 5.43 & **4.42** & (23\\%) \\\\ LLMA-2 & 70B & Pile + RPJ & 4.59 & **3.96** & (18\\%) & **4.65** & **3.95** & (19\\%) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 파일의 검증 및 테스트 데이터 세트에 대한 복잡도(낮을수록 우수함) 괄호 안의 숫자는 수학식 2에 의해 계산된 개선의 백분율이다. 처음 8개의 행은 서로 토큰타이저를 공유하고 마지막 6개의 행은 서로 토큰타이저를 공유한다. 파일은 파일-트레인(3,600억 토큰)이고, RPJ는 레드파자마(1조 4,000억 토큰)이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r r r r r} \\hline \\hline \\multirow{2}{*}{**Neural LM**} & \\multicolumn{3}{c}{Validation} & \\multicolumn{3}{c}{Test} \\\\ \\cline{2-9}  & **Neural** & + \\(\\infty\\)-gram & + \\(\\mathrm{NN\\text{-}LM^{+}}\\) + \\(\\mathrm{RIC\\text{-}LM^{+}}\\) & **Neural** & **+ \\(\\infty\\)-gram** & + \\(\\mathrm{NN\\text{-}LM^{+}}\\) + \\(\\mathrm{RIC\\text{-}LM^{+}}\\) \\\\ \\hline \\multicolumn{9}{l}{_Eval data: Wikipedia_} \\\\ Silo PD & 26.60 & **15.30** (4\\%) & 20.62 & 27.91 & 28.42 & **14.44** (5\\%) & – & – \\\\ Silo PDSW & 18.93 & **12.36** (2\\%) & 14.10 & 18.90 & 20.02 & **11.84** (4\\%) & 14.5 & 19.4 \\\\ Silo PDSWD & 10.66 & **8.77** (1\\%) & 10.14 & 10.87 & 10.76 & **8.41** (2\\%) & – & – \\\\ Pythia & 9.00 & & 8.50 & 8.84 & 9.1 & – & – \\\\ \\hline \\multicolumn{9}{l}{_Eval data: Enron Emails_} \\\\ Silo PD & 19.56 & **6.31** (0\\%) & 8.56 & 15.45 & 15.71 & **4.85** (7\\%) & – & – \\\\ Silo PDSW & 14.66 & **5.58** (0\\%) & 6.70 & 10.80 & 11.23 & **4.35** (6\\%) & 5.9 & 9.9 \\\\ Silo PDSWD & 14.67 & **5.61** (4\\%) & 7.24 & 10.91 & 11.52 & **4.44** (4\\%) & – & – \\\\ Pythia & 7.577 & – & 4.99 & 6.16 & 6.9 & – & – \\\\ \\hline \\multicolumn{9}{l}{_Eval data: NIH ExPorters_} \\\\ Silo PD & 27.46 & **16.26** (4\\%) & 19.27 & 25.51 & 27.94 & **16.00** (4\\%) & – & – \\\\ Silo PDSW & 19.35 & **12.70** (3\\%) & 14.95 & 18.35 & 19.12 & **12.39** (0\\%) & 15.0 & 18.5 \\\\ Silo PDSWD & 15.01 & **10.62** (0\\%) & 12.33 & 14.29 & 14.81 & **10.33** (2\\%) & – & – \\\\ Pythia & **11.20** & – & 11.20 & 10.83 & 11.1 & – & – & – \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 파일의 위키피디아, 엔론 이메일 및 NIH ExPorter의 검증 및 테스트 데이터 세트에 대한 복잡성(아래가 더 우수함) 모든 신경망 모델은 1.3B 모델이며, 기준 데이터는 항상 파일이다. \\ (\\blacksquare\\)은 in-domain을, \\(\\blacksquare\\)은 out-of-domain을, \\(\\blacksquare\\)은 out-of-domain을, \\(\\blacksquare\\)은 out-domain을, \\(\\blacksquare\\)은 out-domain을, \\(\\blacksquare\\)은 out-domain을, \\(\\blacksquare\\)은 out-domain을, \\(\\blacksquare\\)은 in-domain에 관련 데이터를 가지고 있다. \\\\ (\\dagger\\): Min et al. (2023)로부터 검색된 결과들은 우리의 360억 토큰에 비해 45-백만에서 120억 토큰으로 훨씬 더 작은 참조 데이터를 사용한다.\n' +
      '\n' +
      '[\\(\\infty\\)-그램 참조 데이터 선택에서 파일과 RedPajama의 조합은 파일 단독보다 LLaMA-2 모델에서 더 큰 개선을 제공한다. LLaMA-2(13B)와 \\(\\infty\\)-gram을 Pile + RPJ로 보간하면 LLaMA-2(70B)보다 성능이 우수하며, \\(\\infty\\)-gram으로 보간하면 LLaMA-2(70B)의 복잡도가 4.0 이하로 증가한다.\n' +
      '\n' +
      '신경 LM이 SILO(허용-허가된 데이터에만 트레이닝되고 따라서 트레이닝 데이터가 적은)일 때, SILO가 더 제한적인 데이터(즉, \\(\\text{PD}>\\text{PDSW}>\\text{PDSWBY}\\text{PDSWBY}\\)에 트레이닝될 때 \\(\\infty\\)-그램 성분을 추가하는 것이 더 도움이 된다. 참조 데이터의 기여 문서(들)로부터 \\(\\infty\\)-gram의 사용은 정확하게 추적될 수 있는데, 이는 SILO의 철학과 일맥상통한다: 언어 모델링에 소스 데이터를 사용할 때 소스 데이터를 크레딧할 수 있게 한다. SILO에서 사용하는 기존의 검색-증강 방법, 즉 \\(k\\)NN-LM과 RIC-LM에 비해 \\(\\infty\\)gram이 더 많은 복잡도를 개선한다. 따라서, \\(\\infty\\)-gram은 SILO의 검색-증강 방법으로써 더 나은 대안이 될 수 있다.\n' +
      '\n' +
      '### Ablations\n' +
      '\n' +
      '참조 데이터 크기의 영향.그림 7은 결합 모델의 성능을 참조 데이터 크기로 보고한다. 점진적으로 더 작은 참조 데이터를 생성하기 위해 전체 참조 데이터를 2배(최대 256배)씩 반복적으로 서브샘플링하여 9개의 크기를 생성했다. 참조 데이터의 크기가 커질수록 \\(\\infty\\)-gram이 개선되는 것을 볼 수 있으며, 참조 데이터가 너무 작을 때 \\(\\infty\\)-gram이 도움이 되지 않는 NIH ExPorter 도메인을 제외하고 관계는 대략 로그 선형이다.\n' +
      '\n' +
      '참조 데이터의 도메인의 효과.그림 7은 \\(\\infty\\)-그램이 전체 참조 데이터 또는 도메인 내 참조 데이터를 사용하는 결합 모델의 성능을 비교한다. 도메인 내 참조 데이터만 사용하는 것은 전체 참조 데이터를 사용하는 것만큼 대략 강력하며, 이는 우리가 목격한 거의 모든 개선이 도메인 내 데이터(오염이 제거된 곳) 덕분임을 의미한다. 이것은 특히 테스트 도메인이 알려지지 않았거나 도메인 내 참조 데이터를 사용할 수 없을 때 전체 참조 데이터를 사용하는 것이 해를 끼치지 않는다는 것을 의미하지만 도메인 내 참조 데이터를 갖는 것이 가장 도움이 된다.\n' +
      '\n' +
      'Time-Shifted Data 평가###\n' +
      '\n' +
      '더 나아가 \\(\\infty\\)-gram의 효과를 보여주고 불충분한 오염 제거로 인해 성능 향상이 발생할 수 있다는 의심을 제거하기 위해, 우리는 \\(\\infty\\)-gram 참조 데이터의 컷오프 시간 후에 생성된 문서인 **시간 이동 데이터에 대해 평가한다. 우리는 파일과 레드파자마의 컷오프 시간 이후인 2023년 4월과 8월에 생성된 새로운 위키피디아 기사를 사용한다.\n' +
      '\n' +
      '표 4는 결합 모델뿐만 아니라 신경 LM의 복잡성을 보고한다. 5개월 중 4개월에 걸친 문헌에서 \\(\\infty\\)-gram으로 보간하면 신경 LM이 복잡해지는 것을 개선할 수 있다. 우리는 랜덤 포레스트를 적용하여 인스턴스 단위의 보간 하이퍼파라미터를 결정함으로써 이러한 개선이 더욱 강화될 수 있다는 것을 발견한다.\n' +
      '\n' +
      '그림 7: LLaMA-2 모델(7B, 13B, 70B)을 신경 LMs로 사용하고 파일을 참조 데이터로 사용하여 \\(\\infty\\)-그램의 데이터스토어를 스케일링하는 영향. : neural LM only (baseline) \\ (\\bullet\\): \\(\\infty\\)-gram은 전체 파일을 사용하고; \\(\\infty\\): \\(\\infty\\)-gram은 파일의 도메인 내 부분만을 사용한다. **이익은 데이터 저장소의 규모에 따라 지속적으로 증가합니다.**\n' +
      '\n' +
      '랜덤 포레스트는 기준 데이터에서 각 접미사의 빈도뿐만 아니라 접미사 길이(유효 \\(n\\)까지 1)이다. 랜덤 포레스트를 적용하였을 때, 복잡도 개선은 \\(3\\%-20\\%\\)의 범위이다.\n' +
      '\n' +
      '### 텍스트 생성에 관한 노트\n' +
      '\n' +
      '본 논문에서는 신경망 LMs로 \\(\\infty\\)-gram을 보간하여 복잡도를 크게 개선할 수 있는 반면, 본 논문의 예비 실험을 통해 이러한 방법이 개방형 텍스트 생성 작업에 도움이 되지 않을 수 있고, 심지어 해로울 수 있음을 보인다. 세대 동안 \\(\\infty\\)-그램은 이상한 실수(예: 매우 무관한 토큰 검색)를 일으켜 모델이 완전히 실패할 수 있다. 따라서 이 결합된 모델은 신경 LM을 대체할 준비가 되어 있다. 텍스트 생성(예: \\(\\infty\\)-gram과 신경 LMs 사이의 적응적 라우팅)에 \\(\\infty\\)-gram이 가장 잘 기여하도록 추가 조사가 필요하다.\n' +
      '\n' +
      '##6 토론과 폭넓은 시사점\n' +
      '\n' +
      'SS4와 SS5에서 우리는 인피니그램 엔진의 매우 예비적인 사용 사례를 보여주었다. 그러나 우리는 인피니-그램이 포함되지만 이에 국한되지 않는 훨씬 더 광범위한 조사 및 응용을 가능하게 할 수 있다고 믿는다.\n' +
      '\n' +
      '사전 학습 언어 모델에 사용되는 텍스트 말뭉치의 이해.텍스트 말뭉치는 엄청나게 커졌고, 우리는 그 내용에 대한 이해도가 상대적으로 제한적이다(Elazar et al., 2023). Infini-gram은 \\(n\\)-gram lookup (Count query)을 이용하여 말뭉치에 있는 것과 _not_인 것을 빠르게 알아낼 수 있는 유용한 도구가 될 수 있다.\n' +
      '\n' +
      '데이터 큐레이션.데이터 엔지니어들은 종종 독성, 혐오 발언, 개인 식별 정보(PII)와 같은 인터넷에서 긁어낸 말뭉치에서 문제가 되는 콘텐츠를 제거하기를 원한다. 인피니그램의 GetDOCument 질의(모든 문서를 반환하기 위해 쉽게 수정할 수 있음)를 사용하여, \\(n\\)-그램 항(또는 다중 \\(n\\)-그램 항을 갖는 CNF 표현식)을 포함하는 모든 문서를 검색하고 말뭉치에서 제거할 수 있다. 제거는 반복적으로 수행될 수 있다: 인피니그램 인덱스는 가산/감산이므로, 제거된 집합을 인덱싱하여 원 인덱스와 제거 인덱스의 차이를 취함으로써 라운드 1 제거 후 말뭉치의 인덱스를 얻을 수 있다.\n' +
      '\n' +
      '귀인.신경 LM을 사용하여 예측을 할 때, 사람들은 어떤 학습 데이터가 모델의 결정에 가장 영향을 미쳤는지 알고 싶을 수 있다. 핵심문구를 사용한 \\(n\\)-gram lookup을 이용하여 신경망 LMs의 학습 데이터에서 관련 문서들을 역추적할 수 있다.\n' +
      '\n' +
      '데이터 오염, 암기, 표절 검출.테스트 세트 오염은 언어 모델 평가의 주요 이슈가 되고 있다. \\ (n\\)-그램 룩업을 통해 평가 쿼리가 신경 LM의 학습 데이터에 몰래 들어갔는지 확인할 수 있다. 또한 기계 생성 텍스트에서 암기, 또는 인간이 작성한 텍스트에서 표절을 탐지할 수 있는 가능성을 열어준다.\n' +
      '\n' +
      '저작권 침해 완화.최근, 생성 AI는 틀림없이 저작권이 있는 자료를 생성하기 위한 수많은 소송에 직면하고 있다. 인피니-그램은 완화될 수 있을 수도 있다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Eval Data** & \\multicolumn{3}{c}{simple interpolation} & \\multicolumn{1}{c}{w/ Random Forest} \\\\ (Wikipedia) & **Neural** & \\(\\infty\\)**-gram** & **Neural** & \\(\\infty\\)**-gram** \\\\ \\hline April 2023 & 5.64 & **5.48** & (3\\%) & 5.86 & **4.89** & (20\\%) \\\\ May 2023 & 5.43 & **5.27** & (4\\%) & 6.01 & **5.70** & (6\\%) \\\\ June 2023 & 5.49 & **5.21** & (6\\%) & **4.87** & (7\\%) \\\\ July 2023 & 4.93 & 4.93 & (0\\%) & 4.91 & **4.78** & (3\\%) \\\\ August 2023 & 4.64 & **4.46** & (5\\%) & 4.81 & **4.50** & (8\\%) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 타임 시프트된 데이터에 대한 평가. 평가 데이터는 파일과 레드파자마가 모두 생성된 후인 2023년 4월부터 새로 추가된 위키피디아 기사로부터 수집된다. 신경 모델은 LLaMA-2(13B)이고, \\(\\infty\\)-그램 참조 데이터는 파일 + RPJ이다.\n' +
      '\n' +
      '저작권 침해는 학습 데이터에 나타나는 긴 \\(n\\)-그램을 생성하려고 할 때 신경 LM을 대체(아직 그럴듯한) 생성 경로로 전환함으로써, 특히 주로 저작권이 있는 소스의 문서에 나타나는 경우 그렇다.\n' +
      '\n' +
      '사실적 지식에서 환각을 줄이기.모수적 모델만이 비사실적 진술을 생성하기 쉬우며, 이는 환각 문제로 널리 알려져 있다. 인피니그램은 훈련 데이터에서 글자바텀을 읽음으로써 환각을 완화하는데 잠재적으로 사용될 수 있다. 우리는 \\(\\infty\\)-gram이 LAMA(Petroni et al., 2019)와 같은 사실적 프로빙 벤치마크에서 LLaMA-2-70B를 크게 능가할 수 있다는 증거를 발견했다.\n' +
      '\n' +
      'Non-parametric speculative decoding.Speculative decoding (Chen et al., 2023)은 fast 디코더와 slow 디코더를 채용하여 텍스트 생성을 빠르게 하는데, 여기서 fast 디코더는 autoregressive token 생성을 하는 더 작은 모델이고 slow 디코더는 다수의 토큰들의 forward pass를 병렬화하여 fast 디코더의 제안들을 확인한다. 인피니-그램의 낮은 레이턴시를 고려할 때, 우리는 He et al.(2023)과 유사하게 \\(\\infty\\)-그램을 빠른 디코더로 잠재적으로 사용할 수 있다.\n' +
      '\n' +
      '신경 모델에서 노트 암기를 오프로딩하는 것은 완전 모수 언어 모델은 엄청난 양의 사실 지식을 매개변수에 내재화해야 한다. 이러한 로타 암기는, 예를 들어, Ground truth로부터 \\(\\infty\\)-gram의 잔차를 맞추기 위해 신경망 LMs를 훈련시킴으로써, 신경망 모델로부터 비모수 모듈로 잠재적으로 오프로딩될 수 있다(Li et al., 2022).\n' +
      '\n' +
      '우리는 인피니그램으로 제공되는 오픈 소스 도구를 활용하여 커뮤니티가 앞서 언급한 방향을 향해 협력적으로 구축하는 것을 환영합니다.\n' +
      '\n' +
      '##7 관련 업무\n' +
      '\n' +
      '\\(n\\)-gram 언어 모델.\\\\(n\\)-gram 언어 모델.\\ (n\\)-gram은 자연어 처리를 시작한 이래로 가장 고전적인 언어 모델링 방법 중 하나이다(Jurafsky and Martin, 2000). 사람들은 훈련 데이터를 확장함으로써 \\(n\\)그램 LM의 한계를 밀어붙이고 있으며, 현재까지 가장 큰 \\(n\\)그램 테이블(Franz and Brants, 2006)은 최소 40번 나타나는 5그램에 대해 1조 개의 토큰을 인덱싱한다.\n' +
      '\n' +
      '현재 \\(n\\)-그램 LMs가 신경 LMs에 의해 크게 능가하는 반면, \\(n\\)-그램과 \\(n\\)-그램 LMs를 재방문하는 최근 연구가 있다. Khandelwal 등(2020)은 \\(n\\)-gram 모델로 신경망 LMs를 보간하지만 성능을 향상시키지 못한다. 대조적으로, Li 등(2022)은 \\(n\\)-그램 모델이 작은 신경 LM만큼 경쟁적이라는 것을 발견하고, \\(n\\)-그램 모델에 상보적이 되도록 신경 모델을 훈련시키고 추론 시간에 둘 모두를 사용하는 것이 신경 전용 LM보다 우수하다는 것을 발견한다. 그러나, 둘 다 제한된 기준 데이터(101M 토큰)를 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Method** & **\\# tokens (\\(\\uparrow\\))** & **\\# entries (\\(\\uparrow\\))** & **Storage usage (\\(\\downarrow\\))** & **max \\(n\\)** \\\\ \\hline _Vector-based index_ & & & & \\\\ Retro (Borgeaud et al., 2022) & 1.8 T & \\(2.8\\times 10^{10}\\) & 432 TB (16k bytes / entry) & – \\\\ Atlas (Lazcard et al., 2022) & 27 B & \\(4\\times 10^{8}\\) & 200 GB (8 bytes / entry) & – \\\\ KNN-LM (Khandelwal et al., 2020) & 3 B & \\(3\\times 10^{9}\\) & 200 GB (64 bytes / entry) & – \\\\ NPM (Min et al., 2023b) & 1 B & \\(1\\times 10^{9}\\) & 1.4 TB (\\(\\sim\\) 2k bytes / entry) & – \\\\ \\hline \\multicolumn{4}{l}{\\(n\\)_gram-based index_} \\\\ Google’s (Franz and Brants, 2006) & 1 T & \\(3.8\\times 10^{9}\\) & 24 GB & 5 \\\\ Google Books Ngram (Aiden and Michel, 2011) & 500 B & unreported & unreported & 5 \\\\ Stehouwer and van Zaanen (2010) & 90 M & unreported & unreported & \\(\\infty\\) \\\\ Kennington et al. (2012) & 3 M & \\(5\\times 10^{12}\\) & 330 MB (110 bytes / token) & \\(\\infty\\) \\\\ Shareghi et al. (2015) & 9 B & \\(8\\times 10^{18}\\) & 63 GB (7 bytes / token) & \\(\\infty\\) \\\\ \\hline infini-gram (ours) & 1.8 T & \\(1.6\\times 10^{24}\\) & 12 TB (7 bytes / token) & \\(\\infty\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 다른 비모수 언어 모델링 방법과의 비교. **# tokens**: 추론-시간 기준 데이터의 토큰 수. **# 엔트리**: 인덱스 내의 표현(카운트)의 수. **max \\(n\\)**: 고려되는 컨텍스트 토큰의 최대 수. 인피니그램의 경우 파일트레인과 레드파자마의 조합을 참조 데이터로 간주한다.\n' +
      '\n' +
      '그리고 작은 신경 LMs(117-250M 파라미터)와 비교한다. 일부 선행 연구는 \\(n\\)그램 훈련 데이터(Allamanis & Sutton, 2013)를 확장하는 데 있어 가치를 발견했다.\n' +
      '\n' +
      '우리의 \\(\\infty\\)-그램 LM은 \\(n\\)의 고정된 값을 사용하지 않는다. 우리는 학습 데이터를 1조 토큰 이상으로 크게 확장하고 신경망 모델로 보간하여 최대 70B 매개변수로 구성된 최첨단 신경망 모델을 크게 능가한다.\n' +
      '\n' +
      '기존 연구에서는 학습 데이터의 제한된 축척으로 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 아닌 한도가 Stehouwer & van Zaanen (2010)은 \\(\\infty\\)-gram에 접미사 배열을 사용할 것을 제안하지만, 그 공식화는 적절한 확률 분포를 산출하지 못하고 결과적으로 언어 모델을 생성한다. 케닝톤 외(2012)는 동일한 목적으로 접미사 트리를 사용할 것을 제안하지만, 접미사 트리의 저장 오버헤드가 매우 높아 스케일링을 방해하며, 이는 매우 복잡한 압축 기술로 완화될 수 있다(Shareghi 외, 2015). 앞서 언급한 3편의 논문 중 세 번째 논문만이 일반적인 언어 모델링 과제에 대한 평가를 하고 있으며, 복잡도 수치가 너무 높아 실용적으로 사용하기에는 무리가 있다. 우리는 표 5의 이 논문들과 \\(\\infty\\)-그램 지수의 척도를 비교한다; 우리의 훈련 데이터는 이전 작업에서 사용된 가장 큰 것보다 200배 더 크다.\n' +
      '\n' +
      '텍스트 인덱싱을 위한 다른 데이터 구조.접미사 배열 및 접미사 트리 외에도 다른 데이터 구조가 서로 다른 절충점을 만족시키기 위해 텍스트 코퍼스를 인덱싱하는 데 사용되었다. ROOTS 검색 도구(Piktus et al., 2023)는 ROOTS 코퍼스에 BM25 인덱스를 구축하고, \\(n\\)-그램의 정확한 일치와 퍼지 일치를 통해 문서 검색을 지원한다. 데이터 포트레이츠(Marone & Durme, 2023)는 Bloom Filter 기반의 경량 인덱스를 제안하며, 파일과 스택에 대한 확률적 멤버십 추론(50자의 \\(n\\)-그램의 정확한 일치, 여기서 \\(n\\approx 8\\))을 위해 맞춤화된다. ElasticSearch는 Lucene 인덱스를 기반으로 하는 독점적인 검색 엔진으로 Dodge et al.(2021)이 C4에서 문서를 검색하는 데, Elazar et al.(2023)이 \\(n\\)-그램을 카운트하고 480B 토큰까지 다양한 코퍼스에서 가장 빈번한 \\(n\\)-그램을 나열하는 데 사용되어 왔다.\n' +
      '\n' +
      '비모수적 언어 모델.비모수적 LM은 추론 시간에 주어진 데이터에 따라 복잡도가 증가하거나 업데이트될 수 있기 때문에 복잡도가 선험적으로 제한되지 않는 LM을 의미한다. 선행 작업은 크게 두 가지로 나뉘는데, 각 토큰을 하나의 벡터로 표현하고 비모수적 예측 함수를 사용하는 _token_ retrieval approach(Khandelwal et al., 2020; Zhong et al., 2022; Lan et al., 2023; Min et al., 2023; Shi et al., 2023)와 텍스트의 각 청크를 벡터로 표현하고 신경망 언어 모델에 가장 가까운 청크를 통합하는 _chunk_ retrieval approach(Guu et al., 2020; Izacard et al., 2022; Borgeaud et al., 2022)이다. 기준 데이터를 비모수 LMs로 스케일링하는 것은 모든 단위(토큰 또는 청크)에 대한 벡터를 저장해야 하기 때문에 매우 고가이다. 우리가 아는 한, 가장 큰 참조 데이터를 가진 선행 작업은 7B-파라미터 LM과 1.8조 토큰으로 구성된 참조 데이터를 사용하는 Retro(Borgeaud et al., 2022)이다. 그것은 432TB의 디스크 공간을 소비하는 것으로 추정되는 280억 개 이상의 벡터를 저장하고 검색한다.3(표 5의 상세한 비교)\n' +
      '\n' +
      '각주 3: 이것은 부분적으로 Retro가 \\(k\\)NN 검색에서 어떠한 근사치도 사용하지 않기 때문이다. Retro가 Khandelwal 등(2020)과 같이 근사 검색을 사용하더라도 여전히 10TB를 사용할 것이다. 또한 수백억 벡터 이상의 빠른 \\(k\\)NN 검색을 쉽게 지원할 수 있는 오픈소싱 소프트웨어가 없다.\n' +
      '\n' +
      '우리의 \\(\\infty\\)-그램 LM은 비모수 LM의 한 예이며, 단순성으로 적당한 자원(SS3.1)으로 참조 데이터를 상당히 확장할 수 있다. 우리가 아는 한, 우리의 \\(\\infty\\)-그램 LM은 참조 데이터 크기의 크기(파일-트레인과 레드파자마의 결합을 계산할 때 1.8조 토큰)와 기본 신경 LM의 크기(70B) 모두에서 가장 크다.\n' +
      '\n' +
      '##8 결론 및 향후 과제\n' +
      '\n' +
      '본 논문에서는 고전적인 \\(n\\)-그램 언어 모델을 수조 토큰으로 확장하고 무한한 \\(n\\)으로 확장하여 현대화했다. 이러한 극한 설정 하에서 효율적인 훈련과 추론을 수행하는 인피니그램 엔진을 제시하였다. 또한, 인피니-그램 엔진으로 구동되는 \\(\\infty\\)-그램 언어 모델을 제안하였으며, 인간 작성 및 기계 생성 텍스트에 대한 새로운 통찰력을 제공하고 기존 신경 언어 모델을 개선할 수 있음을 보여주었다. 인피니그램으로 구동되는 보다 통찰력 있는 분석과 창의적인 사용을 기대합니다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'Zexuan Zhong, Mike Lewis, Yanai Elazar, Will Merrill, Tim Dettmers, Ximing Lu, Alisa Liu, Weijia Shi, Xiaochuang Han, H2lab의 멤버들, Ziqi Ma에 대한 귀중한 피드백에 감사드립니다. 이 작업은 NIWC 태평양(N66001-19-2-4031), NSF IIS-2044660, NSF DMS-2134012 및 ONR N00014-18-1-2826을 통한 DARPA MCS 프로그램에 의해 부분적으로 자금 지원을 받았다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* 182, 2011. URL[https://api.semanticscholar.org/CorpusID:40104730](https://api.semanticscholar.org/CorpusID:40104730)\n' +
      '* Allamanis and Sutton (2013) Miltiadis Allamanis and Charles Sutton. 언어 모델링을 사용하여 소스 코드 저장소를 대규모로 채굴합니다. _ 2013 10th Working Conference on Mining Software Repositories (MSR)_, pp. 207-216, 2013. URL[https://api.semanticscholar.org/CorpusID:1857729](https://api.semanticscholar.org/CorpusID:1857729)\n' +
      '* Baevski and Auli (2019) Alexei Baevski and Michael Auli. 신경 언어 모델링을 위한 적응 입력 표현. In _Proceedings of the International Conference on Learning Representations_, 2019.\n' +
      '* Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 수백조의 토큰으로부터 회수함으로써 언어 모델을 개선한다. In _Proceedings of the International Conference of Machine Learning_, 2022.\n' +
      '* Chen et al. (2023) Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, L. 시프레와 존 M. 점퍼 추측 샘플링으로 대용량 언어 모델 디코딩을 가속화합니다. _ ArXiv_, abs/2302.01318, 2023. URL[https://api.semanticscholar.org/CorpusID:256503945](https://api.semanticscholar.org/CorpusID:256503945).\n' +
      '* Dodge et al. (2021) Jesse Dodge, Ana Marasovic, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 대형 웹텍스트 말뭉치 문서화: 대규모 크롤링 말뭉치에 대한 사례 연구 In _Conference on Empirical Methods in Natural Language Processing_, 2021. URL[https://api.semanticscholar.org/CorpusID:237568724](https://api.semanticscholar.org/CorpusID:237568724).\n' +
      '* Elazar et al. (2023) Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasa Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, and Jesse Dodge. 내 빅 데이터에 뭐가 있지? ArXiv_, abs/2310.20707, 2023. URL[https://api.semanticscholar.org/CorpusID:264803575](https://api.semanticscholar.org/CorpusID:264803575).\n' +
      '* Franz and Brants (2006) Alex Franz and Thorsten Brants. 모든 n-gram은 당신 거예요 Google Machine Translation Team_, 20, 2006. URL[https://blog.research.google/2006/08/all-our-n-gram-are-belong-to-you.html](https://blog.research.google/2006/08/all-our-n-gram-are-belong-to-you.html)\n' +
      '* Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: The Pile: 언어 모델링을 위한 다양한 텍스트의 800GB 데이터셋; _ ArXiv:2101.00027_, 2020.\n' +
      '* Geng and Liu(2023) 신양 Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL[https://github.com/openlm-research/open_llama](https://github.com/openlm-research/open_llama).\n' +
      '* Groeneveld(2023) Dirk Groeneveld. 크고 친근한 필터[https://github.com/allenai/bff] (https://github.com/allenai/bff), 2023.\n' +
      '* Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 증강 언어 모델 사전 교육을 검색합니다. In _Proceedings of the International Conference of Machine Learning_, 2020.\n' +
      '* He et al. (2023) Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. 휴식: 검색 기반 추측 디코딩. 2023. URL[https://api.semanticscholar.org/CorpusID:265157884](https://api.semanticscholar.org/CorpusID:265157884)\n' +
      '\n' +
      '* Holtzman et al. (2019) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 신경 텍스트 변성의 특이한 경우. _ ArXiv_, abs/1904.09751, 2019. URL[https://api.semanticscholar.org/CorpusID:127986954](https://api.semanticscholar.org/CorpusID:127986954).\n' +
      '* Izacard et al. (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 검색 증강 언어 모델을 이용한 소수 샷 학습 ArXiv:2208.03299_, 2022.\n' +
      '* 자연어 처리, 계산 언어학 및 음성 인식에 대한 소개. In _Prentice Hall series in artificial intelligence_, 2000. URL[https://api.semanticscholar.org/CorpusID:60691216](https://api.semanticscholar.org/CorpusID:60691216).\n' +
      '* Karkkainen et al. (2006) Juha Karkkainen, Peter Sanders, and Stefan Burkhardt. 선형 작업 접미사 배열 구성입니다. _ J ACM_, 53:918-936, 2006. URL[https://api.semanticscholar.org/CorpusID:12825385](https://api.semanticscholar.org/CorpusID:12825385).\n' +
      '* Katz(1987) Slava M. 카츠 음성 인식기의 언어 모델 컴포넌트에 대한 희소 데이터로부터 확률들의 추정. _ IEEE Trans. Acoust. 음성 신호 처리._ , 35:400-401, 1987. URL[https://api.semanticscholar.org/CorpusID:6555412](https://api.semanticscholar.org/CorpusID:6555412)\n' +
      '* Kennington et al. (2012) Casey Redd Kennington, Martin Kay, and Annemariie Friedrich. 언어 모델로 서픽스 트리입니다. In _International Conference on Language Resources and Evaluation_, 2012. URL[https://api.semanticscholar.org/CorpusID:12071964](https://api.semanticscholar.org/CorpusID:12071964).\n' +
      '* Khandelwal et al. (2020) Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 암기를 통한 일반화: 최근접 이웃 언어 모델. In _Proceedings of the International Conference on Learning Representations_, 2020.\n' +
      '* Lan et al. (2023) Tian Lan, Deng Cai, Yan Wang, Heyan Huang, and Xian-Ling Mao. 복사만 하면 됩니다. In _Proceedings of the International Conference on Learning Representations_, 2023.\n' +
      '* Lee et al. (2022) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 학습 데이터를 복제하면 언어 모델이 더 좋아집니다. _Proceedings of the Association for Computational Linguistics_, 2022.\n' +
      '* Li et al. (2022) Huayang Li, Deng Cai, Jin Xu, and Taro Watanabe. n-gram 언어 모델을 이용한 신경망 텍스트 생성의 잔여 학습. _Findings of the Association for Computational Linguistics: EMNLP 2022_, 2022. URL[https://aclanthology.org/2022.findings-emnlp.109](https://aclanthology.org/2022.findings-emnlp.109)이다.\n' +
      '* Marone and Van Durme (2023) Marc Marone and Benjamin Van Durme. 데이터 초상화: 기초 모델 학습 데이터를 기록하는 단계; _ ArXiv_, abs/2303.03919, 2023. URL[https://api.semanticscholar.org/CorpusID:257378087](https://api.semanticscholar.org/CorpusID:257378087).\n' +
      '* Min et al. (2023) Sewon Min, 수친 Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah Smith, and Luke Zettlemoyer. SILO 언어 모델: 비모수 데이터 저장소에서 법적 위험을 분리합니다. _ arXiv preprint arXiv:2308.04430_, 2023a. URL[https://arxiv.org/abs/2308.04430](https://arxiv.org/abs/2308.04430).\n' +
      '* Min et al. (2023) Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 비모수 마스킹 언어 모델링입니다. ACL_의 _Findings, 2023b.\n' +
      '* Petroni et al. (2019) Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. 지식 베이스로서의 언어 모델? _ ArXiv_, abs/1909.01066, 2019. URL[https://api.semanticscholar.org/CorpusID:202539551](https://api.semanticscholar.org/CorpusID:202539551)\n' +
      '* Piktus et al. (2023) Aleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Laurenccon, Gerard Dupont, Alexandra Sasha Luccioni, Yacine Jernite, and Anna Rogers. 루트 검색 도구: llms에 대한 데이터 투명도입니다. _Annual Meeting of the Association for Computational Linguistics_, 2023. URL[https://api.semanticscholar.org/CorpusID:257219882](https://api.semanticscholar.org/CorpusID:257219882).\n' +
      '* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever 등 언어 모델들은 비감독 멀티태스크 학습자들이다. _ OpenAI blog_, 1(8):9, 2019.\n' +
      '* Radford et al. (2019)Ehsan Shareghi, Matthias Petri, Gholamreza Haffari, and Trevor Cohn. 컴팩트하고 효율적이며 무제한적인 용량: 압축된 접미사 트리를 사용한 언어 모델링 In _Conference on Empirical Methods in Natural Language Processing_, 2015. URL[https://api.semanticscholar.org/CorpusID:225428](https://api.semanticscholar.org/CorpusID:225428).\n' +
      '* Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjun Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 검색-증강 블랙박스 언어 모델 arXiv preprint arXiv:2301.12652_, 2023.\n' +
      '* Soldaini et al. (2021) Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Khyathi Chandu, Jennifer Dumas, Li Lucy, Xinxi Lyu, Ian Magnusson, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilsa Ravichander, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Pete Walsh, Hannaneh Hajishirzi, Noah A. Smith, Luke Zettlemoyer, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: 언어 모델 사전 훈련 연구를 위한 3조 토큰의 오픈 코퍼스. 기술 보고서, Alen Institute for AI, 2023. Release under ImpACT License as Medium Risk artifact, [https://github.com/allenai/dolma](https://github.com/allenai/dolma).\n' +
      '* Stehouwer and van Zaanen (2010) Herman Stehouwer and Menno van Zaanen. 언어 모델로 접미사 배열을 사용하는 방법: n-그램을 스케일링합니다. 2010. URL[https://api.semanticscholar.org/CorpusID:18379946](https://api.semanticscholar.org/CorpusID:18379946).\n' +
      '* 함께(2023) 함께. RedPajama: A open source recipe to reproduce LLaMA training dataset, 2023. URL[https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).\n' +
      '* Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023b.\n' +
      '* Wang and Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. GPT-J-6B: 60억 파라미터 자기회귀 언어 모델 [https://github.com/kingoflolz/mesh-transformer-jax] (https://github.com/kingoflolz/mesh-transformer-jax), 2021년 5월.\n' +
      '* Zhong et al. (2022) Zexuan Zhong, Tao Lei, and Danqi Chen. 기억력 증강을 통한 언어 모델 훈련 In _Proceedings of Empirical Methods in Natural Language Processing_, 2022.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:21]\n' +
      '\n' +
      '```\n' +
      '\\(p:p:p]<(sa,m,P)\\) if\\(ds[p:p]<(p:p+B]\\) elseif\\(fetch}(b,l,r,r)\\) if\\(hint==NULL\\)then \\(l,l,r,depth+1)\\(p\\leftarrow\\lfloor(b,l,r,depth+1)\\) if\\(len(promptlds)==0)then return\\(0,N)\\(promptBuf\\leftarrow\\textsc{ReadPrt}(sa,m,P)\\) if\\(m==-1)then \\(l,l,r,depth+1)\\)\n' +
      '```\n' +
      '\n' +
      '알고리즘 1**\\(n\\)-그램/\\(\\infty\\)-그램 쿼리\n' +
      '```\n' +
      'procedureNgramDist(\\(promptIds,hint=NULL,startTokenId=NULL,endTokenId=NULL,endTokenId=NULL}(sa,end-1,P)\\) if\\(startTokenId==NULL\\)then \\(hintTokenId==endTokenId{ReadTokenId}(ds,p+B)\\) if\\(startTokenId==endTokenId\\(p\\leftarrow\\textsc{ReadTokenId}(ds,p+B)\\) if\\(startTokenId==endTokenId\\(p\\leftarrow\\textsc{ReadTokenId}(ds,p+B)\\) if\\(p+B<2N\\)do \\(p\\leftarrow\\textsc{Encode}(promptIds)\\) 이 경우 전체 세그먼트는 tokenId\\(freqByTokenId\\(treqByTokenId\\(tleq\\)\\(ttriangleright\n' +
      '```\n' +
      '\n' +
      '**알고리즘 2**\\(n\\)-그램/\\(\\infty\\)-그램 쿼리(계속됨)\n' +
      '\n' +
      '```\n' +
      'procedureNgramDist(\\(promptIds,hint=NULL,startTokenId=NULL\\)) if\\(hint=NULL\\then \\(promptIds)\\) \\(promptBuf\\leftarrow\\textsc{Encode}(promptIds)\\) if\\(read\\)\\(tokenId\\leftarrow\\textsc{ReadTokenId}(ds,p+B)\\)then \\(tokenId\\leftarrow\\textsc{ReadTokenId}(sa,rank,P)\\) if\\(p+B<2N\\)then \\(tokenId\\leftarrow\\textsc{ReadTokenId}(p+B<2N\\)then \\(tokenId\\leftarrow\\textsc{ReadTokenId}(sa,rank,P)\\) returnNgramDist(\\(promptIds,p+B)\\) if\\(p+B<2N\\)then \\(tokenId\\leftarrow\\textsc{Read\n' +
      '```\n' +
      '\n' +
      '**알고리즘 3**\\(n\\)-그램/\\(\\infty\\)-그램 쿼리(계속됨)\n' +
      '\n' +
      '`` procedureNgramDist(\\(promptIds,hint=NULL,startTokenId=NULL\\)) if\\(hint==NULL\\)then \\(hint\\leftarrow\\textsc{Fnn}(promptIds)\\) \\(promptBuf\\leftarrow\\textsc{Encode}(promptIds)\\) \\(promptIds\\leftarrow\\textsc{ReadPtr}(sa,rank,P)\\) \\(promptBuf\\leftarrow\\textsc{ReadPtr}(sa,rank,P)\\) \\(promptBuf\\leftarrow\\textsc{ReadPtr}(sa,rank,P)\\)\n' +
      '\n' +
      '##2 n-gram에서 마지막 토큰의 확률을 계산\n' +
      '\n' +
      '이는 이전 토큰들(즉, 1-i) 그램() 상에서 컨디셔닝된 마지막 토큰의 n-그램 확률을 계산한다.\n' +
      '\n' +
      '도 8: 질의 유형 1에 대한 예제: \\(n\\)-그램을 카운트한다(Count).\n' +
      '\n' +
      '도 10: 질의 유형 3에 대한 예제: \\(n\\)-그램 LM(NgramDist)으로부터 전체 다음-토큰 분포를 계산하는 것. 공간 제한으로 인해 상위 10개 토큰만 표시됩니다.\n' +
      '\n' +
      '도 9: 질의 유형 2에 대한 예제: \\(n\\)-그램 LM(주어진 \\(n\\), 백오프 없음)(NgramProb)으로부터 토큰 확률을 계산하는 것.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:25]\n' +
      '\n' +
      '그림 14: 참조 데이터에서 "유효한 \\(n\\)"과 해당 최장 접미사의 빈도로 분해된 인간 생성 텍스트와 \\(\\infty\\)-그램 사이의 토큰별 일치. 각 막대의 높이는 토큰 카운트를 나타내고, 색상은 일치를 나타낸다(적색은 0.0, 녹색은 1.0).\n' +
      '\n' +
      '도 13: 질의 유형 6에 대한 예제: AND 및/또는 OR과 연결된 \\(n\\)-그램, 또는 \\(n\\)-그램 용어의 CNF 논리 표현을 포함하는 하나의 문서를 반환함.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>