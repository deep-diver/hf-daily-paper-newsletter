<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MOSAIC : 모듈러 시스템\n' +
      '\n' +
      '보조 및 대화형 요리를 위해\n' +
      '\n' +
      ' Huaxiaoyue Wang\\({}^{*}\\), Kushal Kedia\\({}^{*}\\), Juntao Ren\\({}^{*}\\), Rahma Abdullah, Atikh Bhardwaj, Angela Chao.\n' +
      '\n' +
      '켈리 이첸, 나다니엘 친, 프리스위시 댄, 샤이니 판, 곤잘로 곤잘레스-푸마리에가, 아디티야 콤펠라\n' +
      '\n' +
      '막시무스 아드리안 페이스, 야쉬 샤르마, 샹관 선, 네하 선카라, 산지반 초우두리\n' +
      '\n' +
      '의 대응: 유키wang@cs.cornell.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '가정용 로봇이 일상 사용자와의 요리 등 복잡한 협업 작업을 수행할 수 있는 모듈형 아키텍처인 MOSAIC를 소개합니다. MOSAIC는 인간과 긴밀하게 협력하고, 자연 언어를 사용하여 사용자와 상호 작용하고, 여러 로봇을 조정하고, 일상적인 사물의 열린 어휘를 관리한다. 그 핵심에서 MOSAIC는 모듈성을 사용한다: 언어 및 이미지 인식과 같은 일반적인 작업에 대해 미리 훈련된 여러 대규모 모델을 활용하는 동시에 작업별 제어를 위해 설계된 간소화된 모듈을 사용한다. 우리는 두 로봇이 인간 사용자와 협력하여 \\(6\\) 조리법을 조합하여 요리하는 \\(60\\)의 엔드 투 엔드 시험에서 MOSAIC를 광범위하게 평가한다. 또한, 개별 모듈들을 대상으로 시운동 피킹(visuomotor picking) 에피소드(180\\)와 인체 움직임 예측(60\\) 에피소드, 태스크 플래너의 온라인 사용자 평가(46\\)를 통해 실험을 진행하였다. 우리는 MOSAIC가 인간과 효율적으로 협력할 수 있음을 보여준다.\n' +
      '\n' +
      '도. 1: 주방에서 **MOSAIC 조리.**(상단) MOSAIC는 자연어를 통해 사용자와 상호 작용하며 테이블탑 조작기(R1)와 모바일 조작기(R2)를 제어하여 사용자와 야채 스프를 준비한다. (하단) 우리는 인간 사용자 및 일상 객체와 상호작용하는 다양한 로봇 기술을 포함하는 여러 레시피에 대해 MOSAIC를 평가한다.\n' +
      '\n' +
      '전체 시스템을 실제 인간 사용자와 종단적으로 실행함으로써, 서브 태스크 완료율 \\(91.6\\%\\)으로 \\(68.3\\%(41/60)\\)의 다른 조리법의 공동 요리 시험을 완료한다. 마지막으로 현재 시스템의 한계와 이 영역에서 흥미로운 개방형 문제에 대해 논의한다. 프로젝트 웹사이트는 [https://portal-cornell.github.io/MOSAIC/](https://portal-cornell.github.io/MOSAIC/)에 있다.\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      '조정된 기술의 메들리가 필요한 가정 환경에서 협력 작업은 로봇에 중요한 과제를 제기한다. 이러한 작업은 로봇이 인간 사용자와 자연스러운 상호 작용을 하고 다양한 기술 세트를 학습하고 협업 방식으로 수행할 수 있는 능력을 필요로 한다. 이 도메인 [1, 2, 3, 4]의 이전 시스템은 인상적인 기능을 보여주었다. 그러나 일반적으로 두 가지 제한 사항 중 하나가 있는데, 이는 고립 상태에서 작동하고 인간과의 의미 있는 협업이 부족하거나 고도로 스크립팅된 방식으로 인간과 상호 작용하므로 좁은 세트의 미리 정의된 작업만 완료할 수 있다. 본 논문에서는 인간과 유동적으로 협업하고 광범위한 작업을 수행하는 시스템을 설계하여 이러한 두 가지 한계를 모두 극복하고자 한다.\n' +
      '\n' +
      '이를 위해 자연어를 통해 사용자와 상호작용하고, 일상 객체를 조작해야 하는 다양한 스킬을 수행하며, 인간과 원활하게 협업하는 3가지 핵심 데시데라타를 식별한다. 인간 사용자가 두 대의 로봇과 협력하여 식사를 준비하는 그림 1의 시나리오를 고려하십시오. 사용자는 새로운 레시피를 결정하기 위해 자연 언어를 통해 시스템과 쉽게 상호 작용할 수 있어야 한다. 로봇은 차례로 다양한 재료를 가져오고 함께 요리하는 등 레시피를 만드는 데 필요한 기술을 수행해야 한다. 마지막으로 로봇은 물건을 건네주는 등 인간과 유동적으로 협업해야 한다.\n' +
      '\n' +
      '야생에서 원활하게 기능하는 협력 에이전트를 구축하는 데 있어 중요한 과제 중 하나는 광범위한 가능한 입력 세트에 걸쳐 안전하게 작동할 수 있도록 하는 것이다. 단일 종단 간 모델은 대량의 데이터를 사용할 수 있는 언어 이해와 같은 작업에 잘 작동하지만, 이러한 접근법은 데이터가 덜 사용 가능하고 극도의 정밀도가 중요한 로봇 제어에는 어렵다. 우리의 핵심 통찰력은 아키텍처를 모듈화함으로써 언어 및 이미지 인식과 같은 광범위한 일반화가 필요한 프레임워크의 일부를 작업별 제어가 필요한 부분으로부터 분할할 수 있다는 것이다. 이러한 작업의 분할은 전문화를 통해 강력한 전체 성능을 달성할 수 있음을 의미한다: 우리는 크고 비구조화된 입력 공간으로부터 유용한 정보를 추출하기 위해 큰 _pre-trained model_를 사용할 수 있고 안전하고 정확한 결정을 내리기 위해 _task-specific models_를 사용할 수 있다.\n' +
      '\n' +
      '이 모듈식 접근 방식을 MOSAIC(**M**odular **S**ystem for **A**ssistive and **I**nteractive **C**ooking): 다수의 대규모 사전 훈련된 모델을 통합하는 가정용 로봇용 모듈식 아키텍쳐에 적용한다. 특히 대화식 작업 계획에는 대형 언어 모델(LLM), 시각 운동 기술에는 비전 언어 모델(VLM), 협업에는 사람의 의도를 예측하는 모션 예측 모델을 사용한다. 우리가 아는 한, 이것은 여러 가정용 로봇이 인간 사용자와 협력하여 요리와 같은 복잡하고 긴 수평선 작업을 해결할 수 있는 방식으로 여러 대규모 모델을 통합하는 첫 번째 시스템이다.\n' +
      '\n' +
      '모듈성의 원리는 (예를 들어, 자율 주행에서) 강력한 실세계 로봇 시스템을 개발하는 데 중심이었지만, 그러한 시스템은 종종 세심하게 조작된 컴포넌트에 의존한다. 우리는 인간과 유동적으로 협력하는 적응적이고 확장 가능한 시스템을 만들기 위한 몇 가지 핵심 혁신을 소개한다. 우리의 기여는 네 개의 그룹으로 구성될 수 있다:\n' +
      '\n' +
      '1. **Interactive Task Planner.**_behavior tree_ 내에 LLM(Large Language Models)을 내장하는 아키텍처를 제안한다. 선행 작업 [1, 5, 6, 7, 8]은 작업 계획에 LLM을 직접 사용하려고 시도한다. 그러나 LLM은 종종 실수를 하고 통제하기 어렵다. 이에 따라 트리 내의 노드로서 동작 공간과 추론 과정을 분할함으로써 LLM에서 요구하는 추론의 복잡성과 전체 오류율을 감소시킨다.\n' +
      '2. **Visuomotor Skills.** 객체 식별을 위해 미리 훈련된 비전 언어 모델과 액션 선택을 위한 시뮬레이션에서 RL을 통해 학습된 정책을 사용하는 경량 아키텍처를 제안한다. 객체 및 다양한 환경의 개방형 어휘 세트에 걸쳐 작업을 완료하는 선행 작업[9, 10, 11, 12]과 달리, 본 방법은 _any_ 온라인 시연이나 대규모 네트워크 훈련을 필요로 하지 않는다.\n' +
      '3. **Human Motion Forecasting.** 로봇이 조작 작업에서 인간과 원활하게 협업할 수 있는 Human Motion 예측 방법을 개발한다. 정적 실체로 인간을 모델링하는 선행 연구[13, 14]와는 달리 대규모의 인간 움직임 데이터[15]를 활용하여 예측 모델을 학습한다. 우리의 접근법은 다운스트림 계획 성능을 최적화하는 인간 움직임 예측치를 생성하는 데 초점을 맞추고, 우리의 로봇이 인간과 매우 가까운 곳에서 안전하고 읽기 쉬운 행동을 계획할 수 있게 한다.\n' +
      '4. **종합 평가.** 우리는 두 로봇이 복잡한 긴 수평 레시피를 요리하기 위해 인간 사용자와 협력하는 \\(60\\) 엔드 투 엔드 시도를 수행한다. 또한, 개별 모듈들을 대상으로 시운동 피킹(visuomotor picking) 에피소드(180\\)와 인체 움직임 예측(60\\) 에피소드, 태스크 플래너의 온라인 사용자 평가(46\\)를 통해 실험을 진행하였다. 우리는 실제 인간 사용자와 엔드 투 엔드 시스템을 실행하여 평균 하위 작업 완료율 91.6%로 6가지 다른 조리법의 68.3%(41/60) 공동 요리 시도를 완료했다.\n' +
      '\n' +
      '##2 문제 진술\n' +
      '\n' +
      '우리는 두 대의 로봇, 즉 하나의 모바일 매니퓰레이터와 하나의 탁상용 매니퓰레이터로 주방에서 공동으로 요리하는 인간의 작업에 초점을 맞춘다. 사용자는 요리할 레시피를 결정하기 위해 자연 언어 대화를 통해 시스템과 상호작용한다. 일단 레시피가 결정되면, 시스템은 그 다음 두 로봇과 인간에 하위 작업(예를 들어, 아이템을 가져오거나 버리는 것, 붓는 것 등)을 할당한다. 시스템은 인간으로부터의 피드백과 로봇의 상태에 기초하여 서브태스크를 재설계한다. 로봇이 동일한 탁상 위에서 보조하고 있기 때문에, 일부 하위 작업은 인간과 밀접하게 협력하는 것(예를 들어, 인간이 냄비에 재료를 추가하는 동안 물건을 건네거나 교반하는 것)을 포함한다.\n' +
      '\n' +
      '우리는 우리의 작업에서 간단한 가정 세트를 만든다: 1 종자 레시피 세트에 대한 접근\n' +
      '\n' +
      '레시피에는 시간 종속성이 있는 하위 작업 세트가 포함되어 있습니다. 우리는 초기 레시피 세트로 시스템을 시딩하지만, 사용자는 즉석에서 수정(예를 들어, 재료 추가)할 자유가 있다.\n' +
      '\n' +
      '_2) 맵에 대한 액세스:_ 우리는 시스템이 주방을 미리 매핑했다고 가정하므로 재료 및 도구가 어디에 저장되고 다른 위치로 이동하는 방법을 알고 있다.\n' +
      '\n' +
      '_3) Full Observability:_ 우리는 물체가 서로 옆에 있을 수 있지만 탐지 및 파악을 위해 가려지지 않는다고 가정한다. 우리는 또한 추적과 예측을 위해 인간의 상체가 카메라에 보인다고 가정한다.\n' +
      '\n' +
      '_4) 스킬 API:_ 특정 입력 파라미터(예를 들어, pick_up("salt"), stir())로 호출될 수 있는 로봇 스킬의 라이브러리에 대한 액세스를 가정한다.\n' +
      '\n' +
      '## III Approach\n' +
      '\n' +
      '우리는 협력 요리 작업을 해결하기 위해 다수의 대규모 사전 훈련 모델을 결합한 모듈식 아키텍처인 **A**ssistive 및 **I**nteractive **C**ooking을 위한 MOSAIC, **M**odular **S** 시스템을 제시한다. 도. 도 2는 MOSAIC의 개요를 도시한다. _1) 대화형 태스크 플래너(III-A):_자연어를 통해 실제 사용자와 상호작용하여 조리 과정에서 다양한 태스크 세트를 계획하고 하위 태스크를 조정하는 모듈 _1)의 세 가지 주요 구성 요소로 구성된다. 2) Visuomotor Skill(III-B):_ 로봇 스킬을 다양한 주방 객체 및 환경 세트로 일반화하는 모듈 _ 3) 인체 움직임 예측(III-C):_ 인체 움직임을 예측하기 위해 움직임 예측 모델을 활용하는 모듈로서 로봇이 인간과 안전하고 유동적으로 협업할 수 있도록 보장한다.\n' +
      '\n' +
      '###_Interactive Task Planner_\n' +
      '\n' +
      '태스크 플래너의 목표는 자연어를 사용하여 인간 사용자와 지속적으로 상호작용하고, 상이한 로봇 또는 사용자에게 서브태스크를 위임하고, 진행 상황을 모니터링하는 것이다. 구체적으로, 태스크 플래너는 태스크를 결정하기 위해 사용자와 상호작용한다(예를 들어, "채소 스프 준비"). 이는 태스크 \\(\\mathcal{T}\\)을 방향성 비순환 그래프(DAG)로 나타내며, 이는 서로 다른 서브 태스크들 사이의 시간적 종속성을 모델링하고 할당될 수 있는 이용가능한 서브 태스크들을 결정한다. 작업 계획자는 또한 각 로봇에 대한 하위 작업의 큐를 할당하고 유지합니다. 서브태스크(예를 들어, "페치 솔트")를 실행하기 위해, 태스크 플래너는 go_to("팬트리"), 픽("피크") 등과 같은 일련의 API 호출들을 발행하는 코드 스니펫을 생성한다. 구현 내용은 부록 B를 참조하시기 바랍니다.\n' +
      '\n' +
      '더 형식적으로 태스크 플래너는 현재 높은 수준의 관찰(o_{t}^{\\mathrm{high}}\\in\\mathcal{O}^{\\mathrm{high}}\\)을 입력으로 하는 높은 수준의 정책(\\pi^{\\mathrm{high}}\\)으로 채팅 이력, 현재 레시피, 현재 로봇 큐, 사용 가능한 하위 작업 등이 포함되어 있다. 그것은 set_recipe(name), assign(agent, subtask), say(msg)와 같은 하나 이상의 높은 수준의 행동들을 예측한다.\n' +
      '\n' +
      '최근 많은 접근법[1, 5, 6, 7, 8]이 작업 계획에 LLM을 직접 사용하지만 두 가지 주요 문제를 관찰한다. 첫째, 심지어\n' +
      '\n' +
      '도. 3: **행동 노드 스파이펫. 행동 트리에서 행동 노드의 두 개의 프롬프트 스니펫입니다. 상단 상자는 실행하고자 하는 행동 집합을 예측하는 노드를 나타낸다. 하단 상자에는 어떤 자식 노드\\(n^{\\prime}\\)로 이동할지 예측하는 노드가 표시됩니다.**\n' +
      '\n' +
      '도. 2: **MOSAIC 시스템 개요. _Interactive Task Planner_ 모듈은 자연어를 통해 사용자와 통신하여 레시피를 결정한다. 그에 따라 각 로봇에 하위 작업을 할당합니다. _휴먼 모션 예측_은 휴먼의 2D 포스트를 추출 및 3D 좌표로 변환하며, 이는 미래의 휴먼 모션을 예측하는 데 사용된다. 동시에, VLM은 이미지 및 언어를 입력으로 취하고 관심 객체 주위에 3D 파지 포즈를 생성한다. 결합하면, 세 가지 모두는 최종 로봇 액션을 생성하기 위해 _Visuomotor Skill_ 모듈의 실행 정책에 의해 취해진다.**s with chain-of-thought prompt [16], action space가 크고 추론 과정이 복잡하기 때문에 LLMs는 관찰을 잘못 해석하거나 잘못된 액션을 선택하는 등의 실수를 한다. 더 중요한 것은, LLMs들이 사용자와의 확인 없이 서브 태스크들을 할당하는 것과 같이 개발자가 지정하는 안전 제약들을 위반하는 경향이 있다. 둘째, 개발자는 하나의 단일 프롬프트에서 규칙과 제약 조건을 지정하는 것 외에는 LLM의 동작에 대한 제어가 거의 없으므로 디버깅과 축척에 어려움이 있다.\n' +
      '\n' +
      '두 가지 문제를 극복하기 위해, 우리는 행동 트리(BT) 내에 LLM을 내장하는 아키텍처를 제안한다[17]. 각 동작은 액션 공간 및 추론 프로세스를 분할함으로써 LLM의 복잡성 및 잠재적 오류율을 감소시킨다. 또한, BT의 모듈식 특성으로 인해 여러 행동으로 쉽게 확장할 수 있다.\n' +
      '\n' +
      '** 행동 트리 내에 LLM들을 임베딩.** 행동 트리는 개별 행동 노드들, 또는 단순히 행동들의 계층적 구조이다. 각 노드\\(n\\)는 현재 관측치\\(o_{t}^{\\mathrm{high}\\)을 보고 실행할 일련의 동작\\(a_{t}^{\\mathrm{high}\\)과 실행할 자식 노드\\(n^{\\prime}\\)을 선택한다. 우리의 아키텍처에서 각 노드는 특정 프롬프트와 선택할 미리 정의된 결정 세트를 가진 LLM에 대한 호출이다.\n' +
      '\n' +
      '예를 들어, 도 1을 참조하여 설명한다. 도 3은 상이한 행동들에 대한 프롬프트들의 스니펫들을 도시한다. 지침은 목표, 관찰의 어떤 부분에 초점을 맞출지, 준수해야 할 제약 조건 및 상황 내 예제를 설명합니다. 서브태스크 할당은 로봇에 서브태스크를 직접 할당하고 사용자에게 말하는 리프 노드이다. 한편, 조리법을 만들 때 무엇을 해야 할까요? 행동은 다른 행동을 호출하는 상위 노드(예: 할당 확인, 하위 작업 할당)입니다.\n' +
      '\n' +
      '각 노드의 LLM은 더 작은 추론 문제에 초점을 맞추기 위해 전문화되어 있기 때문에 각 행동은 경험적으로 더 신뢰할 수 있고 디버깅하기 쉽다. 도. 도 4는 제안된 접근법과 단일 모놀리식 프롬프트를 사용하는 기준선 사이의 비교를 도시한다. 제안된 접근 방식은 설계자가 지정하는 제약 조건(예를 들어, 할당하기 전에 사용자와 하위 작업을 확인해야 하는 요구 사항)을 보다 일관되게 존중한다. 이러한 제약은 안전하고 예측 가능한 행동을 보장하는 데 중요하다. 한편, 단일 프롬프트가 있는 LLM은 명령의 모든 측면을 따르려고 하는 데 있어 그러한 제약을 위반하기 쉽다.\n' +
      '\n' +
      '마지막으로 새로운 동작을 추가하는 것은 해당 동작에 대한 프롬프트를 만들고 다른 동작이 호출할 수 있는 옵션으로 추가하는 것만큼 간단합니다. 코드를 변경할 필요가 없습니다.\n' +
      '\n' +
      '### _Visuomotor Skills_\n' +
      '\n' +
      '시각 운동 기술의 목표는 작업 계획자가 할당한 하위 작업을 실행하는 것이다. 스킬은 현재 관측치\\(o_{t}\\in\\mathcal{O}\\)을 입력으로 하는 낮은 수준의 정책\\(\\pi\\)이다.\n' +
      '\n' +
      '도. 4: **트리 구조 태스크 플래너 대 단일 프롬프트 LLM.** 우리는 제약 조건을 위반하는 경향이 있는 하나의 LLM 프롬프트를 사용하는 것과 우리의 접근법을 비교한다. 관찰을 감안할 때, 하나의 단일 프롬프트가 있는 LLM은 로봇에 하위 작업을 직접 할당하며, 이는 작업을 할당하기 전에 인간과 확인해야 한다는 제약을 위반한다. 한편, 본 논문에서 제안하는 접근 방식은 행동 트리에서 행동 공간과 추론 과정을 구획하기 때문에 올바른 추론 경로를 따르고 사용자와의 서브 태스크 제안을 정확하게 확인할 수 있다.\n' +
      '\n' +
      '현재 이미지 및 로봇 상태, 그리고 태스크 플래너의 언어 지시 \\(\\ell_{t}\\in\\mathcal{L}\\) 그것은 그리퍼 명령으로 구성된 낮은 레벨의 동작\\(a_{t}\\in\\mathcal{A}\\)을 예측한다.\n' +
      '\n' +
      '시각 운동 기술을 훈련시키기 위한 일반적인 접근법은 엔드 투 엔드 트레이닝[10, 11, 12, 20, 9, 21, 22]을 통해 일련의 작업에서 인간 시연을 모방하는 것이다. 그러나, 이러한 접근법을 사용하는 최첨단 방법들은 일반적으로 (1) 상태들의 양호한 커버리지 및 (2) 이들 상태들로부터 전문가 액션 라벨들을 요구한다. 여기에는 로봇이 오류를 범한 후 복구하는 방법을 보여주는 데이터가 포함됩니다. 종합하면, 이것은 최대 수백 시간의 전문가 시연을 필요로 하는 알고리즘으로 이어지며, 이는 수집하기가 불가능하다.\n' +
      '\n' +
      '대신, 우리는 엔드 투 엔드 아키텍처를 객체 식별 및 액션 실행 모듈로 분할한다. 객체 식별은 많은 객체들로 일반화할 수 있는 미리 훈련된 VLM에 오프로드하고, 강화학습을 이용한 시뮬레이션에서 순수하게 정책을 탐색하여 액션 실행을 해결한다. 그렇게 함으로써 추가 데이터를 수집할 필요 없이 두 가지 문제를 모두 해결했습니다. 그림 5는 아래에서 논의하는 아키텍처를 보여준다.\n' +
      '\n' +
      '**사전 훈련된 모델을 통한 객체 탐지**입력 이미지와 언어 조건이 주어지면, 우리는 둘 다 사전 훈련된 OwlViT[18] 모델을 통과하여 바운딩 박스 세트를 제공한다. 로봇-특정 시점(이러한 대형 VLM의 훈련 데이터에서 덜 일반적일 수 있음)을 처리하기 위해, 우리는 Non-Maximum Suppression을 사용하여 박스를 필터링하고 CLIP 유사성 점수가 가장 높은 경계-박스 좌표를 취한다[23]. 우리는 OwlViT 및 CLIP 모델 모두의 무게를 동결한다.\n' +
      '\n' +
      '**포인트-클라우드 세그멘테이션을 통한 파지-포즈 생성.** 파이프라인의 다음 단계에서, 우리는 FastSAM[19]을 사용하여 바운딩 박스 내의 객체의 보다 정확한 세그멘테이션을 획득하고 깊이 카메라의 포인트 클라우드를 통해 세그멘테이션된 픽셀을 역투영한다. 우리는 이 사영의 질량 중심이 되는 자세를 취한다.\n' +
      '\n' +
      '**모델 기반 강화 학습을 통한 행동 예측.** 최종 행동을 예측하기 위해 시뮬레이터 및 보상 함수를 설계하여 세계의 일부 특권 정보, 이 경우 3D 파지 포즈를 입력으로 하는 일반적인 RL 에이전트를 훈련하고 설정된 제약을 위반하지 않고 해당 위치에 도달하기 위한 행동을 출력한다. 간단한 선택() 정책의 경우, 이 제약 조건은 대상 객체에 도달할 때 식료품 저장실에 부딪히거나 주변 객체를 넘어뜨리지 않는 것입니다. 이 액션 모듈은 온 정책 시연 없이 시뮬레이션에서 완전히 훈련되며 추론 중에 직접 적용되어 타임스테프별 액션을 예측합니다. 추가 구현 세부 사항은 부록 C에서 찾을 수 있다. 우리는 현재 픽()을 위한 시뮬레이션에서 RL 트레이닝을 사용하고, 엔지니어링하기 비교적 간단하기 때문에 이동() 및 장소()를 위한 엔지니어링된 정책을 사용한다.\n' +
      '\n' +
      '###_Human Motion Forecasting_\n' +
      '\n' +
      '인간과의 안전하고 효과적인 협응은 인간의 움직임을 예측하고 그에 따라 로봇 계획을 적응시켜야 한다. 로봇이 인간과 매우 가까운 곳에서 일하는 협력 요리에는 정확한 예측이 중요하다. 예를 들어, 도 6에서 인간 파트너와 함께 냄비를 젓는 로봇을 관찰한다. 인간이 냄비에 야채를 넣기 위해 안으로 이동할 때, 로봇은 그 움직임을 예상하고 팔을 뒤로 젖혀 인간을 위한 길을 만들어야 한다. _ 그러나 주방과 같은 동적 환경에서 인간의 움직임을 정확하게 예측하는 것은 어렵다._ 인간은 주방에서 다양한 물건을 조작하거나 역 사이를 이동하는 등 광범위한 동작을 수행할 수 있다. 많은 양의 훈련 데이터와 긴 컨텍스트 창을 가지고도, 현재의 최첨단 모델은 항상 인간의 움직임을 정확하게 예측하기 위해 고군분투한다. 대신, 우리의 목표는 로봇과의 상호작용 동안 예측된 인간의 움직임의 영향을 충분히 포착하는 예측을 생성하는 예측 모델을 구축하는 것이다.\n' +
      '\n' +
      '**대규모 데이터에 대한 사전 훈련.** 우리는 먼저 대규모 인간 활동 데이터에 대한 모델을 사전 훈련하여 입력으로서 관절 위치의 이력이 주어진 인간 동작의 원활한 예측을 생성한다. 우리는 300명 이상의 피험자와 40시간의 모션 캡처 데이터를 포함하는 인간 활동의 대규모 데이터 세트인 AMASS[15]를 사용한다. 예측자는 인간의 짧은 운동 이력을 입력으로 사용하고 예측은 인간의 움직임을 최적화한다.\n' +
      '\n' +
      '도. 5: **Visuomotor Skills를 위한 아키텍처 개요.** 로봇이 롤아웃되는 실제 환경을 모방한 시뮬레이터를 설계한다. RL 에이전트는 환경 제약들을 강제하는 보상 함수 하에서 목표 포지션이 주어진 동작들을 예측하도록 훈련된다. 테스트 시간에, 상기 비전모터 모듈은 이미지 및 자연어를 입력으로 하고, OwlViT를 이용하여 관심 객체 주변의 바운딩 박스를 리턴한다[18]. 이 바운딩 박스는 FastSAM[19]에 전달되며, 이는 이미지를 세그먼트화하고 그것을 포인트 클라우드에 백-프로젝션하여 3D 목표 포즈를 생성한다. 이 3D 목표 포즈는 최종 액션을 생성하는 훈련된 RL 에이전트로 전달된다.\n' +
      '\n' +
      'MLE 손실을 통한 미래의 인간 움직임 가능성 AMASS에 대한 훈련 후에, 예측자는 동적으로 일관되고 합리적인 인간의 움직임 예측을 생성할 수 있다. 그러나 모션 예측은 멀티 에이전트 주방 설정에 직접 적용할 수 없다. 첫째, AMASS에서의 활동은 일상적인 주방 활동을 대표하지 않는 점프, 걷기, 춤과 같은 일반적인 인간의 움직임으로 구성된다. 둘째, 데이터 세트는 단일 인간 모션으로 구성되어 추가 에이전트를 고려하는 예측자의 능력을 제한한다.\n' +
      '\n' +
      '**상호작용 데이터를 미세 조정.** 예보관의 동작 예측이 로봇이 동작을 계획하는 데 도움이 되는지 확인하기 위해 주방 환경에서 인간과 인간의 상호작용만으로 구성된 데이터 세트인 CoMaD(Collaborative Manipulation Dataset) [24]를 활용한다. CoMaD의 에피소드는 특정 주방 활동에 초점을 맞추고 인간이 서로 밀접하게 접촉하는 짧은 전환 창을 포함한다. 이러한 창에서 인간의 움직임을 정확하게 예측하는 것은 로봇의 계획 성능을 최적화하는데 매우 중요하다. CoMaD의 각 에피소드에 대해 이러한 전환 창을 식별하고 _전환 데이터세트_를 구성한다. 우리는 모션 예측자를 훈련시키기 위해 _전이 데이터세트_와 전체 CoMaD 데이터세트에서 동등하게 데이터를 샘플링한다. 이 접근법은 인간이 로봇에 접근하고 상호작용할 가능성이 있는 경우 상호작용의 임계 기간을 업샘플링함으로써 예측자가 작업 효율성을 최대화하는 데 도움이 된다. 따라서 전체 에피소드에 걸쳐 미래 인간 움직임의 가능성 추정치를 단순히 최대화하는 대신, 우리의 예측가는 로봇의 의사 결정에 대한 예측의 영향을 설명한다.\n' +
      '\n' +
      '** 추론 시간: 실시간, 비전 기반 예측 및 계획.** 7개의 상체 관절(어깨, 팔꿈치, 손목, 목)의 3D 위치를 이용하여 인간의 포즈를 표현한다. 인간의 몸통을 겨냥한 단일 RGB-D 카메라가 그들의 상반신 포즈를 감지하는데 사용된다. 이어서, 2D 포즈 검출기인 MediaPipe[25]를 사용하여 RGB 이미지 상에서 인간 관절 위치들이 식별된다. 그런 다음 이러한 위치는 이미지 깊이 맵을 사용하여 3D 세계 좌표로 역투영된다. 마지막으로, 인간의 자세는 로봇 계획에 사용되는 실시간 움직임 예측을 생성하는 데 사용된다. 그러나, RGB-D 카메라의 깊이 맵을 사용하여 획득된 3D 좌표는 스테레오 카메라로부터의 부정확성으로 인해 종종 잡음이 있다. 고충실도 모션 캡처 시스템의 데이터에 대해 트레이닝된 모션 예측 모델이 분포 외 잡음 입력에 대한 정확한 예측을 하지 못하기 때문에, 이것은 문제를 제기한다. 학습 시간에 랜덤 가우시안 잡음을 모델의 입력에 주입함으로써, 예측자는 잠재적으로 잡음이 있는 입력을 잡음제거하고 평활한 예측을 생성하도록 학습하도록 강제한다.\n' +
      '\n' +
      '도. 6: **인간 움직임 예측 개요. (상단 왼쪽) 사전 교육. 예측가는 인간 포즈의 짧은 이력이 주어진 미래의 동작을 예측하기 위해 인간 활동의 큰 데이터 세트에 대해 훈련된다[15]. (상위-우측) 상호작용-인식 미세 조정. 그런 다음 예측자는 주방 활동의 데이터 세트인 CoMaD[24]에서 미세 조정된다. 훈련의 모든 단계에서, 랜덤 노이즈는 모델의 입력에 주입되어 모델이 추론 동안 카메라 입력으로부터의 노이즈에 강건하게 된다. (하단) 실시간, 비전 기반 예측 및 계획. RGB-D 장면 이미지가 주어지면, 포즈 검출기는 카메라의 깊이 맵을 사용하여 3D 좌표로 변환되는 인간의 2D 포즈를 추출한다. 움직임 예측자는 로봇이 행동을 계획하는 데 사용하는 미래의 인간의 움직임을 예측합니다.**\n' +
      '\n' +
      '## IV Experiments\n' +
      '\n' +
      '본 논문에서는 MOSAIC를 60\\(60\\)의 종단간 실험을 통해 두 로봇이 인간 사용자와 협력하여 6\\(6\\)의 레시피를 조합하여 요리하는 방법을 평가한다. 또한 온라인 사용자 연구 실행, 보이지 않는 객체 및 배경에 대한 테스트, 여러 실제 사용자에 대한 테스트를 포함하는 개별 모듈을 테스트하기 위한 실험을 수행한다. 모든 실험에서 이동 매니퓰레이터는 6-DoF 스트레치 로봇 RE1[26], 탁상 매니퓰레이터는 7-DoF 프랑카 에미카 리서치 3[27]이다. 주방에는 작업 공간을 인식하고 사람의 움직임을 포착할 수 있는 두 개의 오버헤드 RGB-D 카메라도 있습니다. 사용자가 태스크 플래너와 상호 작용할 수 있도록 구글의 스피치 투 텍스트 API[28]를 사용하여 사용자의 구두 지침을 전사하고 텍스트 투 스피치 API를 사용하여 태스크 플래너의 응답을 발성한다.\n' +
      '\n' +
      '### _End-to-end Trials_\n' +
      '\n' +
      '두 대의 로봇과 사용자가 공동으로 \\(6\\) 조리법을 만들어 총 60\\(60\\)의 엔드 투 엔드 실험을 진행한다. 그림 7은 서로 다른 레시피(작업), 서로 다른 하위 작업 및 로봇 기술이 포함된 테이블을 보여준다. 각각의 레시피는 로봇 스킬의 상이한 조합 및 사용자와의 상이한 유형의 상호작용을 수반한다. 예를 들어, 사용자는 모호한 지시를 제공하고, 로봇의 서브태스크를 인터럽트하며, 레시피에 없는 새로운 서브태스크를 추가한다. 각 시행에 대해 두 가지 측정 기준을 계산합니다. 시행 성공 여부와 하위 작업 완료율입니다.\n' +
      '\n' +
      '전반적으로 MOSAIC는 평균 하위 작업 완료율 91.6%로 6가지 다른 조리법의 41/60(68.3%) 공동 요리 시험을 완료한다. 우리는 두 가지 구체적인 질문을 분석한다.\n' +
      '\n' +
      '** MOSAIC는 더 긴 수평선 작업으로 어떻게 규모를 조정합니까?** 우리는 \\(6\\) 기술을 포함하는 "토스샐러드"에서 \\(14\\) 기술을 포함하는 "닭 수프"까지 다양한 레시피를 테스트합니다. MOSAIC의 성공률은 예상했던 대로 증가하는 지평선과 함께 떨어지지만 기하급수적으로 떨어지지 않고 \\(50\\%\\) 이상을 유지한다. 주요한 이유는 MOSAIC의 각 모듈이 들어오는 입력의 오류에 강인하도록 훈련되기 때문이다(예를 들어, 태스크 플래너는 로봇에 의해 이루어진 지연을 처리하고, 비전 운동 스킬 픽()은 go_to()로부터의 오류를 처리하고, 예측은 포즈 추정으로부터의 오류를 처리함).\n' +
      '\n' +
      '**모듈성이 장애를 특정 모듈로 국지화하는 데 도움이 됩니까?** 각 모듈에는 하위 모듈이 있으며 각각은 명확한 입출력 계약을 가지고 있으므로 오류를 국지화하는 것은 쉽게 자동화됩니다. 우리는 이것을 사용하여 실패를 그림 7에 표시된 다음 \\(5\\) 범주로 클러스터링한다.\n' +
      '\n' +
      '(A)_[Visuomotor Skill] 오브젝트를 픽업하지 못했습니다:_ 때때로, VLM은 오브젝트 프롬프트가 주어지면 부정확한 오브젝트를 선택하고, 이는 섹션 IV-B에서 추가로 분석한다. 다른 경우, 예측된 목표는 높은 오차를 가지고, 그리퍼가 물체를 놓치게 한다.\n' +
      '\n' +
      '(B)_[Visuomotor Skill] 개체를 성공적으로 배치하지 못했습니다:_go_to() 스킬에서 오류는 로봇을 테이블에서 너무 멀리 떨어져서 개체를 성공적으로 배치합니다. 때로는 로봇이 물체를 잘못된 높이에서 방출하여 넘어지게 합니다.\n' +
      '\n' +
      '(C)_[Visuomotor Skill] 스킬 중에 물체를 떨어뜨렸다:_ The\n' +
      '\n' +
      '도. 7: **End-to-End 결과. 각 레시피가 10번의 시도를 통해 테스트되는 6가지 레시피에 대한 정책 결과입니다. 각 레시피에는 서로 다른 로봇 기술을 포함하는 다양한 하위 작업이 포함되어 있습니다. 오류 없이 완료되는 시행 횟수와 개별 하위 과제 완료율을 보고한다. 또한 실패 사례를 분류합니다. MOSAIC는 평균 서브태스크 완료율 91.6%로 41/60 태스크를 완료할 수 있다.**stir() 및 pour() 스킬은 그립이 불충분하여 오브젝트를 떨어뜨릴 수 있다.\n' +
      '\n' +
      '**(D)**: _[대화형 태스크 플래너] 서브태스크를 인터럽트하지 못함:_ 사용자가 로봇에 현재 서브태스크를 중지하도록 요청할 때, 스피치-텍스트 모듈은 때때로 사용자의 짧은 명령을 올바르게 전사하지 못한다. 불분명한 전사는 작업 계획자가 로봇을 즉시 중단시키는 대신 사용자에게 설명을 요청하게 한다.\n' +
      '\n' +
      '**(E)**: _[대화형 태스크 플래너] 잘못된 서브태스크를 할당함:_ 태스크 플래너는 사용자의 명령을 잘못 이해하고 완료된 서브태스크를 로봇에 재할당한다.\n' +
      '\n' +
      '**(F)**: _[인간 움직임 예측] 포즈 추적 실패:_ 사람의 포즈가 카메라의 시야 밖으로 이동하여 움직임을 예측하는 동안 추적 오류가 발생한다.\n' +
      '\n' +
      '### _Visuomotor Skills_\n' +
      '\n' +
      '엔드 투 엔드 시험은 비전 운동 기술 전체의 성능을 보여주지만 이제 내부 모듈을 독립적으로 분석한다. 비전 언어 모듈과 정책 모듈을 구분하여 살펴보고, 다음과 같은 질문에 질적으로 양적으로 답하는 것을 목표로 한다.\n' +
      '\n' +
      '1. **off-the-shelf VLM이 비전 백본으로서 얼마나 잘 수행되나요?** 픽() 정책을 완료하기 위한 OvIViT 전용 비전 모듈에 대해 off-the-shelf OvIViT[18]와 CLIP 포스트 프로세서[23]를 비교한다. 우리는 영상에 존재하는 물체의 밀도와 유형을 모두 변화시켜 한계와 고장 사례를 테스트한다.\n' +
      '2. **순전히 시뮬레이션에서 RL 에이전트를 훈련시키는 것이 바람직한 정책을 생성하는가?** 간단한 역기구학 및 행동 복제에 대해 정책 모듈을 평가한다. 4개의 다른 초기 위치에 걸쳐 환경 제약을 위반하지 않고 픽()을 완료할 수 있는 각 정책의 능력을 테스트한다.\n' +
      '\n' +
      '**비전-언어 모듈.** 비전-언어 모듈이 식료품 저장실의 객체를 안정적으로 식별하는 데 필요한 구성 요소를 먼저 정량적으로 평가한다. OvIViT는 Open-vocabulary_detector_로 훈련되고 이미지 내에서 객체 제안들에 대한 손실을 계산하기 때문에[18], 우리는 순수하게 대조적으로 훈련된 CLIP[23]보다 언어 조건 분류에 덜 능숙하다고 의심한다. 이를 위해 OvIViT가 직접 최종 경계 상자를 예측하는 기준선과 CLIP를 사용하여 \\(10\\) 상자를 예측하고 가장 높은 상자를 선택하는 접근법을 비교한다.\n' +
      '\n' +
      '표 I에서 점점 더 군집화되는 3개의 팬트리에 걸쳐 이 2개의 백본을 사용하는 픽() 정책 성공률을 제시한다. 우리는 식료품 저장실의 개체 수가 증가함에 따라 OvIViT의 성공률이 크게 감소한다는 것을 발견했다. 특히, OvIViT는 올바른 객체 위에 \\(a\\) 바운딩 박스를 배치하는 것이 덜 고통스럽지만, 오히려 confi에서 더 많은 어려움을 겪는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c} \\hline \\hline  & Easy & Realistic & Recovery & Obstacle \\\\ \\hline RL in Simulation & 10/10 & 10/10 & 10/10 & 4/10 \\\\ IK & 10/10 & 3/10 & 0/10 & 2/10 \\\\ BC & 6/10 & 4/10 & 0/10 & 3/10 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: **On-policy Evaluations of Policy Module. We see the RL agent trained in simulation successfully reaches the goal without hitting the pantry, despite an adversarial reset placements such as the arm extended into a lower pantry level. However, success rate deteriorates as object placements violate the initial assumptions made about the simulator used to train the agent.**\n' +
      '\n' +
      '도. 8: **비전 백본 예제 실패 사례. 우리는 VLM 모듈이 실패하는 세 가지 체제, 즉 많은 객체를 가진 고밀도 이미지, 관심 객체와 대비가 낮은 배경 및 신속한 감도를 보여준다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c} \\hline \\hline  & Single & Medium & High \\\\  & Object & Density & Density \\\\ \\hline OvIViT + CLIP (Ours) & 10/10 & 10/10 & 6/10 \\\\ OvIViT only & 10/10 & 3/10 & 0/10 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: **On-policy Evaluations of Visuomotor Architecture. The architecture is tested on its ability to pick up the language-specified object when (1) a single object is in the pantry, (2) 2-6 objects are in the pantry, (3) 7-15 objects in the pantry. We observe that using CLIP post-processing is critical in allowing for stable and correct bounding box predictions as the number of objects in an image increases.**that bounding box as the correct object of interest. Instead, CLIP takes all bounding box proposals in isolation to predict the one most aligned with the language embedding, leading to more stable predictions and, thus, a more reliable grasp-pose for our policy module.\n' +
      '\n' +
      '위의 내용은 신뢰할 수 있는 목표 예측에 대한 CLIP 후처리의 중요성을 보여주지만 그림 8은 우리의 비전 모듈의 세 가지 실패 체제를 충족한다. 첫째, 너무 많은 물체(특히 다양한 양념 병과 같은 유사하게 생긴 것)의 존재는 CLIP가 점수를 매기기 위한 최적이 아닌 바운딩 박스 제안의 세트로 이어진다. 더 나아가, 조명 및/또는 색상이 객체 윤곽을 배경으로 블렌딩할 때, 객체의 부분들만이 바운딩 박스에 포함될 수 있고, 따라서 CLIP 점수가 더 낮아질 수 있다. 마지막으로, 보다 구체적인 프롬프트가 더 나은 바운딩 박스 제안들을 생성한다는 것을 발견한다.\n' +
      '\n' +
      '**Policy Module.** 엔드 이펙터가 객체 a) _Easy_: 그리퍼가 객체에 근접할 때, b) _Realistic_: 그리퍼가 후퇴할 때, c) _Recovery_: 그리퍼가 객체로부터 멀리 확장된 위치에 있을 때, d) _Obstacle_: 객체가 부분적으로 가려질 때, 서로 다른 시나리오에 대해 정책 모듈을 평가한다. 우리는 간단한 역기구학(IK) 기준선과 행동 복제(BC) 정책과 비교한다. 추가 세부 사항은 부록 C에서 찾을 수 있다. 모든 경우에 로봇이 식료품 저장실에 부딪히거나 물체가 넘어지면 작업이 실패한 것으로 간주한다. 우리는 우리의 결과를 표 II에 제시한다. IK는 일반적으로 낮은 위치로 재설정할 때 식료품 저장실을 피하지 못한다. BC는 베이스가 이동하거나 그 트레이닝 분포 밖의 상태로 리셋될 때 복구하지 못한다. 본 논문에서 제안하는 RL 에이전트는 시뮬레이션에 의해 학습되기 때문에 제약 조건 인식 및 우수한 커버리지를 갖는 강력한 정책을 제공한다. 그럼에도 불구하고, 시뮬레이션에서의 훈련은 모델 및 특정된 보상 기능을 필요로 하며, 따라서 시뮬레이터에 의해 캡처되지 않은 시나리오 또는 보상이 제공될 때 실패하기 쉽다. 이러한 경우 BC가 충분한 시연과 함께 제공되는 경우 보다 표현력 있는 정책에 기여한다고 가정한다.\n' +
      '\n' +
      '###_Human Motion Forecasting_\n' +
      '\n' +
      '이제 실제 인간과 협업하는 7자유도 프랑카 로봇 팔의 온 정책 평가를 통해 인간 움직임 예측 모듈을 분석한다. 우리는 인간 사용자의 예측에 의존하는 두 가지 특정 기술, 교반 및 핸드오버()를 평가하고 다음 질문에 답하는 것을 목표로 한다:\n' +
      '\n' +
      '1. **예측이 단순히 다운스트림 성능을 위해 현재 포즈를 사용하는 데 얼마나 도움이 됩니까?** 인간 사용자와 함께 로봇의 교반 및 핸드오버 기술 온-정책을 실행하고 작업별 메트릭을 비교한다.\n' +
      '2. **우리의 방법은 시각 기반 센서로부터의 잡음이 많은 인간 검출에 강건한가?** 우리는 우리의 예측기를 모델 입력을 잡음 제거하지 않는 기준 접근법과 비교하여 다양한 정도의 정밀도를 요구하는 협력 조작 작업에 대한 우리의 방법을 평가한다.\n' +
      '\n' +
      '각 스킬에 대해 특정 메트릭을 계산하여 온 정책 성능을 측정합니다. 우리는 (1) 현재 인간 포즈가 전체 계획 지평선에 걸쳐 자신의 포즈가 될 것이라고 가정하는 _Current_와 (2) 잡음 제거 없이 예측을 예측하는 _Forecast (Base)_의 두 가지 기준선과 비교한다. 각 기준선 및 스킬 조합은 총 60\\(60\\)의 평가에 대해 10\\(10\\)회 평가된다.\n' +
      '\n' +
      '**Reactive Stirring.** 다음 설정에서 교반 기술을 평가한다: 로봇이 화분을 저으면서 인간이 야채를 썰고 주기적으로 화분에 항목을 추가한다. 인간이 냄비 위로 손을 움직일 때만 상호작용이 일어난다. 로봇이 야채를 집은 후 화분에 팔이 닿는 것을 감지하는 시간(ms)과 상호작용 시 로봇과 사람 손 사이의 최소 거리인 안전 거리(cm)를 측정한다. 우리는 또한 숫자를 측정한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} Task \\(\\rightarrow\\) & \\multicolumn{3}{c}{Reactive Stirring} & \\multicolumn{2}{c}{Robot to Human Handovers} \\\\ Model \\(\\downarrow\\) & Safety Dist. (cm) \\(\\uparrow\\) & Time to React (ms) \\(\\downarrow\\) & Collisions \\(\\downarrow\\) & Time to Goal. (s) \\(\\downarrow\\) & Path Length (cm) \\(\\downarrow\\) \\\\ \\hline Current & 13.5 (\\(\\pm\\)0.2) & 135.4 (\\(\\pm\\)10.4) & 2/10 & 1.54 (\\(\\pm\\)0.1) & 31.5 (\\(\\pm\\)1.2) \\\\ Forecast (Base) & 19.9 (\\(\\pm\\)0.2) & 64.9 (\\(\\pm\\)9.8) & 0/10 & 1.67 (\\(\\pm\\)0.2) & 32.7 (\\(\\pm\\)3.0) \\\\ Forecast (Ours) & 23.1(\\(\\pm\\)0.2) & 48.9 (\\(\\pm\\)5.0) & 0/10 & 1.15 (\\(\\pm\\)0.1) & 22.4 (\\(\\pm\\)0.2) \\\\ \\end{tabular}\n' +
      '\\end{table} TABLE III: **Task-Specific Performance Metrics.** We evaluate the robot’s interactions with the human user on 2 collaborative manipulation tasks. Integrating forecasts into the robot’s skills improves safety and fluidity across all metrics. We observe that solely relying on the current human pose during Reactive Stirring is risky and causes human-robot collisions. Robot-Handover tasks are completed more efficiently using forecasted human positions.\n' +
      '\n' +
      '도. 9: **On-Policy Reactive Stirring. (왼쪽) 전류: 사람의 현재 자세를 사용하면 로봇의 반응이 지연되고 사람의 손이 냄비에 들어가면 충돌이 발생한다. (오른쪽) 예측된 사람의 위치를 사용하여 예측하면 충돌의 더 부드러운 상호 작용과 더 빠른 반응 시간이 발생하여 충돌을 피할 수 있습니다. 즉, 사람의 손이 로봇 팔에 너무 가까울 경우(임계값 이내)입니다. (이러한 경우 로봇의 긴급 정지가 트리거된다.) 우리는 로봇이 인간의 현재 포즈만을 사용하면서 매우 늦게 반응하는 것을 관찰하며, 이는 표 III과 같이 두 가지 경우에 충돌로 이어진다. 이에 비해 예측치를 사용하면 모든 측정 지표에서 크게 향상되고 더 중요한 것은 충돌을 피할 수 있다는 것이다. 마지막으로, 잡음 제거 없이 예측들을 트레이닝하는 것은 현재의 인간 포즈를 사용하는 것보다 태스크 메트릭들의 개선을 보여주지만, 그 성능(각 메트릭들의 분산에 의해 측정됨)에서 더 큰 가변성이 있다.\n' +
      '\n' +
      '**로봇 to Human Handover.** 이 작업에서 사용자는 로봇에게 객체를 픽업하고 핸드오버할 것을 요청한다. 우리는 로봇이 요청한 물체를 픽업한 후 로봇의 핸드오버(handover) 기술의 속도와 효율성을 평가한다. 속도를 측정하기 위해 로봇이 물체를 인간에게 가져오고 핸드오버를 완료하는 데 걸리는 시간인 평균 목표 시간(ms)을 측정한다. 로봇의 움직임의 효율성은 사람의 손을 향해 이동하면서 로봇의 엔드 이펙터가 추적하는 거리를 측정하는 경로 길이(cm)를 통해 측정된다. 핸드오버를 위한 _Current_ 인간 포즈를 사용하여, 로봇은 단순히 현재의 인간 손목 위치를 따른다. 표 III은 로봇이 우리의 예측자가 예측한 핸드오버 위치를 사용하는 것에 비해 이 접근법을 사용하여 작업을 완료하는 데 상당히 느리다는 것을 보여준다. 인간의 예측을 사용하여 로봇은 최종 핸드오버 위치를 향해 직접 이동하여 현재 포즈를 따르는 것보다 훨씬 짧은 궤적으로 스킬을 마무리한다. 이 기술을 위해, 우리는 잡음이 제거되지 않은 예측을 사용하는 것이 계획 메트릭을 향상시키지 않는다는 점에 주목한다. 비전 기반 시스템에 의해 검출된 3D 인간 포즈는 종종 시끄럽고, 예측된 예측은 이러한 경우 불규칙할 수 있으며, 이는 로봇 팔에 의한 육포 움직임으로 이어진다. 이는 근접 협력 중 안전을 보장하기 위해 노이즈 제거 모듈의 중요성을 보여줍니다.\n' +
      '\n' +
      '###_Interactive Task Planner_\n' +
      '\n' +
      '태스크 플래너의 강건성을 테스트하기 위해 (1) 시스템이 증가하는 복잡한 상호 작용을 처리하는 방법을 테스트하기 위한 통합 실험과 (2) 시스템이 실제 사용자와 상호 작용하는 방법을 분석하기 위한 온라인 사용자 연구를 수행한다. 구체적으로, 행동 트리(_Tree_, 도 4에 도시된) 내에 LLM들을 내장하는 태스크 플래너를 하나의 LLM 프롬프트(_One-Prompt_)를 사용하는 것과 비교한다. 이 기준선에 대해, 그것의 단일 LLM 프롬프트는 태스크 플래너가 따라야 하는 제약들, 각 상황에서 어떤 동작들을 선택할지에 대한 설명들, 및 상황 내 예시들을 갖는다. 두 가지 실험을 통해 다음과 같은 질문에 답하고자 한다.\n' +
      '\n' +
      '1. **작업 계획자가 사용자와의 복잡한 상호작용을 얼마나 강력하게 처리할 수 있는가?** 두 실험에서, 사용자는 작업 계획자의 정상적인 작업 흐름을 방해하도록 요청받는다. 통합 검정에는 접근법이 각 유형의 비명목 상호 작용에 적절하게 대응할 수 있는지 여부를 조사하기 위한 검정 사례가 포함되어 있다.\n' +
      '2. **행동 트리 구조를 갖는 LLM이 태스크 플래너가 제약 조건을 준수하도록 강제하는 데 어떻게 도움이 되는가?** 상호작용 복잡도가 증가함에 따라 그리고 실제 사용자와 상호작용할 때 각 접근법이 갖는 제약 조건 위반의 수를 분석한다.\n' +
      '\n' +
      '**통합 테스트.** 태스크 플래너를 체계적으로 테스트하기 위해 사용자 상호 작용의 유형이 주어지면 태스크 플래너가 수행해야 하는 작업에 대한 단위 테스트를 설계합니다. 사용자가 명확한 지시를 내리고 태스크 플래너의 제안에 동의하는 명목상의 상호 작용 외에도 4가지 비명목상의 상호 작용 모드를 식별한다. 예를 들어, 사용자가 레시피를 만들고 있을 때, 그들은 태스크 플래너의 서브태스크 할당 제안들을 수정하거나 로봇들에게 레시피 외부에서 서브태스크들을 수행하도록 요청할 수 있다. 테스트 중 자연스러운 상호 작용을 생성하기 위해 제공하는 일상적인 사용자를 모방한 LLM 프롬프트를 만듭니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Difficulty & Approach & Avg. Non-nominal Pass Rate (\\%) \\\\ \\hline \\multirow{2}{*}{Easy} & One-Prompt & 100 \\(\\pm\\) 0.00 \\\\  & Tree & 90.0 \\(\\pm\\) 5.35 \\\\ \\multirow{2}{*}{Hard} & One-Prompt & 60.0 \\(\\pm\\) 9.04 \\\\  & Tree & 94.0 \\(\\pm\\) 2.30 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IV: **Task Planner Success Rate in Integration Tests. We present the average percentage of non-nominal interaction that gets successfully handled by the task planner. _Tree_ can more robustly handle complex interactions compared to _One-Prompt_.**\n' +
      '\n' +
      '도. 10: **통합 테스트에서 작업 계획자 제약 조건 위반 각 접근법은 다양한 수의 비명목 상호 작용으로 처음부터 끝까지 5개의 무작위 고유 레시피에 대해 평가된다. 각 접근 방식은 레시피당 3번 실행됩니다. 우리는 각 난이도에 대해 모든 런에 걸친 제약 위반의 총 수를 제시한다. _ Tree_는 모든 난이도 레벨에 대해 _One-Prompt_와 비교하여 가장 낮은 총 제약 위반 수를 갖는다.**우리가 프로그램적으로 설정하는 상호작용 모드에 기초하여 상이한 명령들을 포함한다.\n' +
      '\n' +
      '우리는 점점 더 복잡한 상호작용으로 다음과 같은 범주를 만든다: 하나의 무작위 비명목 상호작용으로 "쉬운"과 6으로 "어려운" 각 난이도에 대해 5가지 레시피에 대한 접근 방식을 테스트하고 각 레시피에 대해 동일한 비명목 상호 작용 세트로 전체 요리 프로세스를 3회 실행합니다. 이 실험은 접근 방식당 30회의 실행과 실행당 평균 34회의 채팅을 수행합니다.\n' +
      '\n' +
      '단위 테스트 통과의 평균 백분율을 측정하고 태스크 플래너가 프롬프트에 지정된 제약 조건을 위반한 횟수를 분석합니다. 제약 조건 위반은 3개의 범주로 그룹화됩니다.\n' +
      '\n' +
      '1. [label=()]\n' +
      '2. 권한 없는 행위: 태스크 플래너는 사용자의 권한 없이 하위 태스크를 할당하거나 제거한다.\n' +
      '3. 거짓말: 그것은 로봇이 그들의 능력 이상으로 하위 작업을 할 수 있다고 주장하거나, 무언가를 할 것을 주장하지만 그렇지 않다고 주장한다.\n' +
      '4. 사용자 무시: 사용자의 지시에 응답하지 않는다. 표 IV는 전반적으로 _One-Prompt_에 비해 _Tree_가 통과된 평균 단위 테스트의 비율이 더 높다는 것을 보여준다. 한편, 도 10은 _Tree_가 일관되게 더 적은 제약들을 위반한다는 것을 강조한다. 이러한 결과는 작업 공간을 구획화하고 각 LLM의 추론 문제를 단순화하는 것이 작업 계획자가 제약 조건을 존중하는 데 상당한 도움이 된다는 것을 시사한다.\n' +
      '\n' +
      '두 접근법 모두 먼저 사용자의 허락을 받지 않고 행동해서는 안 된다는 제약과 고군분투하고 있지만, 각 접근법이 이를 어떻게 위반하는지는 다르다. 요리가 막 시작된 경우에도 _One-Prompt_는 사용자와 확인하지 않고 로봇에 직접 하위 작업을 할당하는 경향이 있다. 한편, _Tree_는 사용자가 자신의 서브 작업을 완료했다고 말하는 것에 응답하여 후속 서브 작업을 로봇에 직접 할당할 때 이러한 제약을 위반하는 경향이 있다.\n' +
      '\n' +
      '**사용자 스터디.** 태스크 플래너가 실제 사용자와 상호 작용할 때 여전히 제약 조건을 준수하는지 확인하기 위해 온라인 사용자 스터디를 수행하여 사용자가 웹 기반 채팅을 통해 태스크 플래너와 대화한다. 사용자는 제안된 접근법(_Tree_) 또는 베이스라인(_One-Prompt_)을 랜덤하게 할당받는다. 그런 다음, 사용자는 완료할 레시피를 랜덤하게 할당받고, 각각의 레시피에 대해, 접근법당 적어도 하나의 응답이 존재한다. 작업 계획자가 제약 조건을 위반했는지 평가하기 위해 설문조사를 완료하기 전에 각 사용자가 조리 과정에서 3가지 별개의 비명목 상호 작용 모드를 완료해야 한다. 우리는 총 46개의 응답을 받았는데, 26개의 응답은 태스크 플래너의 능력에 익숙하지 않은 실험실 구성원으로부터, 20개는 Prolific[29]이라는 크라우드소싱 웹사이트에서 외부 사용자로부터 받았다. 도 11은 실제 사용자와 상호작용할 때, _Tree_가 _One-Prompt_에 비해 더 적은 제약들을 일관되게 위반함을 보여준다.\n' +
      '\n' +
      '각 접근법에 23명의 참가자가 참여한 경우, _One-Prompt_(\\(2.26\\pm 0.42\\))에 비해 _Tree_(\\(1.22\\pm 0.25\\))에서 제약 조건 "권한 없는 행위" 위반이 감소했으며 통계적으로 유의성은 \\(p_{.041}\\)이다. _Tree_(\\(1.22\\pm 0.25\\)) Tree_는 또한 사용자(\\(0.56\\pm 0.24\\) vs \\(1.39\\pm 0.31\\))에게 덜 자주 놓이며, 통계적 유의성은 \\(p_{.040}\\)이다. (p<0.836}\\)를 무시한 횟수는 유의한 영향을 미치지 않았으나, 두 접근방식 모두 위반이 거의 없는 것으로 나타났다 (\\(0.30\\pm 0.15\\) vs \\(0.34\\pm 0.15\\). 보고된 모든 오류는 표준 오류입니다.\n' +
      '\n' +
      '사용자는 경험에 대한 피드백을 남겨야 하며, 이는 이러한 발견에 대한 더 많은 통찰력을 제공한다. 일반적으로, _Tree_로 할당된 사용자들은 _One-Prompt_보다 더 나은 경험을 갖는다. _One-Prompt_로 배정된 한 사용자는 "나는 분명히 불복종 수준으로 내 자신을 압도하는 것을 볼 수 있었다."라고 논평했고, _Tree_로 배정된 다른 사용자는 "그것은 예상대로 작동했고, 빠르고 간결한 대답, 순응하며, 어떤 실수도 하지 않았다."라고 논평했다. _Tree_로 할당된 다른 사용자는 "한 번에 많은 작업을 할당하려고 하면서 때때로 약간 고압적이라고 느꼈습니다"라고 논평했는데, 이는 트리가 사용자의 허가 없이 하위 작업을 할당하지는 않지만 이러한 행동이 일부 사용자에게 성가실 수 있음을 시사한다.\n' +
      '\n' +
      '## V 관련 작품들\n' +
      '\n' +
      '**홈 로봇.** 최근 연구 노력은 로봇에게 가정과 같은 환경에 충분히 적응할 수 있는 일반주의적 능력을 제공하려고 시도했다[30, 1, 2, 3, 4]. 그러나, 이들 작업들 중 많은 것 [3, 4]은 명시적인 작업 계획을 필요로 하지 않는 간단한 미리 정의된 작업을 완료하는 것, 예를 들어, 단일 작업을 선택하는 것으로 제한된다.\n' +
      '\n' +
      '도. 11: **작업 계획자 제약 위반 실제 사용자 상호 작용.** 총 46개(내부 26개, 외부 20개)의 응답을 받는다. 각 사용자는 상호 작용하고 무작위 레시피를 만들기 위해 _One-Prompt_ 또는 _Tree_ 중 하나를 할당받는다. 우리는 변형당 제약 위반의 총 수를 제시한다. _ Tree_는 이러한 실제-사용자 상호작용에서 _One-Prompt_에 비해 적은 제약 위반을 나타낸다.\n' +
      '\n' +
      '항목. Liu et al. [30]은 우리의 작업과 유사한 개방 어휘 탐색 및 조작에 접근하지만, 여전히 초기화 후 세계의 정적 표현을 가정함으로써 동적 환경의 도전을 회피한다. 한편, 일부 작업은 협력 작업에 대한 다중 팔/다중 로봇 계획을 고려한다[31, 32, 8, 33]. 예를 들어, Mandi et al. [8]은 자신의 멀티-로봇 프레임워크를 인간-로봇 설정으로 확장하려고 시도하지만, 로봇이 자신의 작업을 진행할 수 있기 전에 인간이 특정 작업을 완료하도록 강제함으로써 인간-로봇 협업을 상당히 제한한다. 본 논문에서는 다수의 로봇이 인간과 유동적으로 협업하여 광범위한 작업을 수행할 수 있는 시스템을 설계함으로써 이러한 한계를 극복하고자 한다.\n' +
      '\n' +
      '**작업 계획.** 작업 기획자는 높은 레벨의 작업, 예를 들어 레시피를 요리하는 것을 입력으로 하고, 그 목표를 달성하기 위한 계획, 예를 들어 서브-작업들의 시퀀스를 생성한다. 전통적인 접근 방식은 이를 검색 문제로 프레임화하고 이를 해결하기 위해 상징적 설계자를 호출한다[34, 35, 36, 37]. 그러나 일상적인 작업에 이러한 방법을 사용하는 것은 검색 공간을 미리 정의해야 하고 작업을 대화식으로 전달할 수 있는 자연어 인터페이스가 부족하기 때문에 어렵다. 최근 작업은 이러한 두 가지 한계를 모두 극복하기 위해 작업 계획을 위해 LLM을 활용한다. 단일 로봇 설정에서 명확하게 정의된 언어 목표가 주어지면 최근 작업은 계획대로 액션 목록을 생성하는 것[38, 39, 40, 41], 로봇 액션 API를 호출하는 코드를 합성하는 것[42, 43, 5, 44] 또는 고전적인 플래너가 해결할 수 있는 문제로 변환하는 것[6, 45]으로 분류될 수 있다. 그러나 이러한 시스템 중 어느 것도 인간과 상호 작용하고 인간과 로봇 모두에 대한 작업을 조정하지 않는다. Li et al. [7]은 우리의 접근법에 가장 가까운 작업 계획 프레임워크를 가지고 있으며, 여기서 LLM은 각 단계에 대한 로봇 코드를 합성하기 전에 단계별 계획을 생성하기 위해 특정 자연어 목표를 취한다. 그러나 두 대의 로봇과 사용자가 참여하는 다중 에이전트 작업 계획 문제를 해결하기 때문에 작업 계획자는 단순히 단계 목록을 출력할 수 없다. 적절한 에이전트에 하위 작업을 적절하게 할당하려면 사용자와 지속적으로 통신해야 합니다.\n' +
      '\n' +
      '**Visuomotor Skills.** 몇 가지 최근 연구는 로봇 공학에 사전 훈련된 비전 언어 모델(VLMs)의 적용을 연구한다[46, 47, 48, 49, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]. 최근 작업의 한 패밀리[48, 49, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]는 사전 훈련된 VLM을 엔드 투 엔드 방식으로 통합하며, 예를 들어, 액션 예측을 돕기 위해 관심 영역을 분할한다[49, 12]. 접근 방식[47, 40, 1, 50]의 두 번째 맛은 VLM을 활용하여 환경에서 어포던스 및 제약을 인식하고 언어[47] 또는 코드[50]를 통해 대응하는 실행 명령을 제공한다. 우리의 모델은 환경 인식과 행동 실행의 훈련 목표를 구별하는 이 측면에서 유사하다. 이것은 우리가 좋은 커버리지를 제공하기 위해 인간 또는 로봇 시연의 대규모 데이터 세트를 필요로 하지 않고[48, 49, 10, 11, 12, 12], 대규모 로봇 학습 데이터 세트 간의 실시예 불일치를 걱정해야 하는 것을 효과적으로 해방시킨다[51, 52]. 그러나, 선행 연구[44, 47, 50, 53]와 달리, 우리는 VLM에 의해 어포던스가 제공되고 시뮬레이터에 제약이 내재된 시뮬레이션에서 강화 학습을 사용하여 행동 정책을 훈련한다.\n' +
      '\n' +
      '**인간 움직임 예측**인간과 근접한 협력적 조작 작업은 인간의 움직임을 예측할 것을 요구한다. 인간의 움직임은 복잡하고 매우 가변적이기 때문에 이것은 어려운 문제이다. 일반적인 접근법은 사람을 정적으로 간주하여 움직임 예측[13, 14]의 문제를 피하는 것이다 대신에, 최근의 연구는 과거의 관절 위치들의 짧은 이력을 기반으로 미래의 인간 동작을 예측하기 위해 신경망 및 지도 학습의 사용을 향해 이동하고 있다[54, 55, 56]. 인간 움직임의 대규모 오픈 소스 데이터 세트의 공개[58, 15]는 인간 자세 예측을 위한 대규모 RNN 및 GNN 기반 신경망 모델을 훈련하는 것을 가능하게 했다[59, 60]. 결과적으로, 이러한 데이터 세트는 협동 조작 작업에 초점을 맞추어 로봇 모션 계획에 통합되었다[61, 24]. 이 작업에 가장 가까운 ManiCast[61]은 비용 인식 인간 예측을 학습하는 프레임워크를 제안했다. 그러나, 이러한 접근법은 부피가 큰 모션 카메라 설정에 의존하여, 사용자가 마커와 함께 모션 캡처 슈트를 착용할 것을 요구한다. 본 연구에서는 단일 RGB-D 카메라를 이용하여 인간의 움직임을 실시간으로 예측 및 계획하는 시스템을 구현하였다.\n' +
      '\n' +
      '## VI 토론과 한계\n' +
      '\n' +
      '본 논문에서는 인간 사용자와 인터랙티브하게 다양한 레시피를 조리할 수 있도록 두 로봇을 제어할 수 있는 모듈형 시스템을 제안한다. 대규모 사전 훈련된 모델의 앙상블을 활용하여 본 시스템은 사용자와 통신하고 의도를 예측하며 일련의 시각 운동 기술을 완료한다. 설계 결정을 검증하기 위해, 우리는 여러 명의 인간 사용자를 대상으로 실제 세계에서 광범위한 실험을 수행한다. 모듈식 접근법을 채택하여 구성 요소에 대한 자세한 평가를 개별적으로 수행한다. 또한, 이러한 모듈러 평가 과정은 잠재적인 고장 모드를 밝히는 데 중요한 역할을 했다. 이 섹션에서는 향후 다루고자 하는 이 작업의 주요 제한 사항에 대해 논의한다.\n' +
      '\n' +
      '**1. 태스크 플래너의 접지.** 태스크 플래너는 대화를 통해서만 사용자의 상태를 관찰할 수 있다. 작업 공간 상태를 추적하고 사용자의 상위 의도를 예측하기 위해 작업 계획자를 사전 훈련된 모델로 접지하는 것은 향후 작업에 대한 흥미로운 방향이다.\n' +
      '\n' +
      '**2. 보다 복잡한 기술과 환경으로 확장.** 우리는 로봇이 선택, 배치 및 이동과 같은 여러 기술을 수행하는 것을 보여줍니다. 절단, 롤링 및 확산과 같은 보다 복잡한 작업으로의 확장은 덜 하찮다. 우리의 시스템은 다양한 요리법에 대해 광범위하게 테스트되지만 우리의 실험은 하나의 주방 환경으로 제한된다. 향후 작업은 광범위한 주방 환경에 직면하여 우리 시스템의 일반화 능력의 한계를 측정하려고 시도할 것이다.\n' +
      '\n' +
      '**3. 상호작용 및 피드백으로부터 학습.** 시스템의 능력은 일상적인 사용자의 가정에 배치된 후 정적 상태를 유지한다. 미래 연구의 흥미로운 영역은 실시간 인간 피드백과 상호 작용으로부터 지속적으로 배우는 것이다. 우리 시스템의 자연스러운 다음 단계는 사용자의 선호도에 적응하기 위해 사용자와 협력한 이력을 사용하는 것이다.\n' +
      '\n' +
      '**4. 오류 검출 및 복구.** 인간이 자연 언어를 사용하여 상위 레벨 태스크 플래너에게 시스템의 오류를 기술할 수 있지만, 폐쇄 루프 방식으로 이러한 오류를 자율적으로 검출하는 것은 성공률을 높이고 사용자 경험을 향상시킬 것이다.\n' +
      '\n' +
      '**5. 냉동 사전 훈련된 모델.** 우리 시스템의 여러 구성 요소는 데이터 효율성과 성능을 향상시키기 위해 사전 훈련된 여러 모델(예: 비전 운동 기술 비전 백본)을 활용합니다. 그럼에도 불구하고 미세조정이 없는 경우 여전히 일반적으로 각 모델의 타고난 능력으로 제한된다. 예를 들어, 이러한 모델의 교육 분포 외부에 있는 객체가 주방에 존재할 수 있다. 이상적인 시스템은 이러한 새로운 시나리오에 우아하게 적응할 것이다.\n' +
      '\n' +
      '우리는 다중 에이전트 시스템에 일반화할 수 있는 기술을 신속하게 갖추기 위해 대규모 사전 훈련된 모델을 활용하는 모듈식 프레임워크 세트를 설계한다. 이러한 특성으로 인해 MOSAIC는 복잡한 가정 환경에서 협력적인 인간-로봇 시스템과 이 시스템의 능력을 더욱 개선하고 확장하는 향후 작업에 바람직한 기반이 된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]M. 안아한 브라운영 체보타르, 오 코테스, B. 데이비드, C. 핀, C. 푸, K. 고팔라크리쉬난 Hausman, et al. (2022) Do as i can, not as i say: grounding language in robotic affordances. ArXiv:2204.01691. 인용: SS1.\n' +
      '*[2]M. 바즈라차랴, J. 보더스, R. 정동민헬믹 Kaul, D. Kruse, J. Leichty, J. Ma, C. Matl, F. Michel, C. Papazov, J. Petersen, K. 생카, M 티저슬란드(2023) 야생에서 모바일 조작을 시연: 메트릭 기반 접근 방식. In Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023, pp. 10.15607/RSS.2023. XIX.055. External Links: Link Cited by: SS1.\n' +
      '*[3]N. Mhamood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black (2019-10월 2019) AMASS: Motion capture의 아카이브. In International Conference on Computer Vision, pp. 5442-5451. Cited by: SS1.\n' +
      '*[4]S. 예나만드라, A. 라마찬드란, K. 야다브 왕모 한나태 게르벳, T 양병욱 Jain, A. William Clegg, J. Turner, et al.(2023) Homerobot: open-vocabulary mobile manipulation. ArXiv:2306.11565. 인용: SS1.\n' +
      '*[5]J. 양원 황, F. 샤, P. 쉬, K Hausman, B. Ichter, P. Florence, and A. Zeng (2023) Code as policy: language model programs for embodied control. 외부 링크: 인용된 링크: SS1입니다.\n' +
      '*[6]B. 유영 장욱 장규 류승 Zhang, J. Biswas, P. Stone(2023) L1m+p: 최적의 계획 능력으로 대형 언어 모델에 권한을 부여합니다. 외부 링크: 인용된 링크: SS1입니다.\n' +
      '*[7]B. Li, P. Wu, P. Abbeel, and J. Malik (2023) Interactive task planning with language models. 외부 링크: 인용된 링크: SS1입니다.\n' +
      '*[8]B. Liu, P. Wu, P. Abbeel, and J. Malik (2023) Interactive task planning with language models. 외부 링크: 인용된 링크: SS1입니다.\n' +
      '*[9]B. 유영 장욱 장규 류승 Zhang, J. Biswas, P. Stone(2023) L1m+p: 최적의 계획 능력으로 대형 언어 모델에 권한을 부여합니다. 외부 링크: 인용된 링크: SS1입니다.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      ' S. 펑영 두종 Xu, E. Cousineau, B. Burchfiel, diffusion. _ arXiv preprint arXiv:2303.04137_, 2023.\n' +
      '* Shridhar et al. [2022] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In _Conference on Robot Learning_, pages 894-906. PMLR, 2022.\n' +
      '* Nair et al. [2022] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot manipulation. _arXiv preprint arXiv:2203.12601_, 2022.\n' +
      '* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* Kedia et al. [2023] K. Kedia, Atiksh Bhardwaj, Prithwish Dan, and Sanjiban Choudhury. Interact: Transformer models for human intent prediction conditioned on robot actions. _ArXiv_, abs/2311.12943, 2023.\n' +
      '* Bazarevsky et al. [2020] Valentin Bazarevsky, Ivan Grishchenko, Karthik Raveendran, Tyler Lixuan Zhu, Fan Zhang, and Matthias Grundmann. Blazepose: On-device real-time body pose tracking. _ArXiv_, abs/2006.10204, 2020.\n' +
      '* Kemp et al. [2022] Charles C. Kemp, Aaron Edsinger, Henry M. Clever, and Blaine Matulevich. The design of stretch: A compact, lightweight mobile manipulator for indoor human environments, 2022.\n' +
      '* Franske[2022] Franka research 3, 2022. URL[https://franka.de/documents](https://franka.de/documents).\n' +
      '*URL[https://cloud.google.com/speech-to-text/](https://cloud.google.com/speech-to-text/)\n' +
      '* Prolific[2014] Prolific, 2014. URL[https://www.prolific.com](https://www.prolific.com).\n' +
      '* Liu et al. [2024] Peiqi Liu, Yaswanth Orru, Chris Paxton, Nur Muhammad Mahi Shafullah, and Lerrel Pinto. Ok-robot: What really matters in integrating open-knowledge models for robotics. _arXiv preprint arXiv:2401.12202_, 2024.\n' +
      '* Dogar et al. [2019] Mehmet Dogar, Andrew Spielberg, Stuart Baker, and Daniela Rus. Multi-robot grasp planning for sequential assembly operations. _Autonomous Robots_, 43:649-664, 2019.\n' +
      '* Ha et al. [2020] Huy Ha, Jingxi Xu, and Shuran Song. Learning a decentralized multi-arm motion planner. _arXiv preprint arXiv:2011.02608_, 2020.\n' +
      '* 티카와 바징카[2023] 아르팀 티카와 나임 바징카. 공동 작업 공간을 공유하는 협동 로봇의 예측 제어. _ IEEE Transactions on Control Systems Technology_, 2023.\n' +
      '* Brewka et al. [2011] Gerhard Brewka, Thomas Eiter, and Miroslaw Truszczynski. Answer set programming at a glance. _Commun. ACM_, 54(12):92-103, dec 2011. ISSN 0001-0782. doi: 10.1145/2043174.2043195. URL [https://doi.org/10.1145/2043174.2043195](https://doi.org/10.1145/2043174.2043195).\n' +
      '* Jiang et al. [2018] Yuqian Jiang, Shiqi Zhang, Piyush Khandelwal, and Peter Stone. An empirical comparison of pddl-based and asp-based task planners. _CoRR_, abs/1804.08229, 2018. URL [http://arxiv.org/abs/1804.08229](http://arxiv.org/abs/1804.08229).\n' +
      '* Lifschitz[2002] Vladimir Lifschitz. 정답 집합 프로그래밍 및 계획 생성. _ 인공지능_, 138(1):39-54, 2002. ISSN 0004-3702. doi:[https://doi.org/10.1016/S0004-3702](https://doi.org/10.1016/S0004-3702)(02)00186-8. URL[https://www.sciencedirect.com/science/article/pii/S0004370202001868](https://www.sciencedirect.com/science/article/pii/S0004370202001868) 지식 표현과 논리 프로그래밍\n' +
      '* Fox and Long[2011] Maria Fox and Derek Long. PDDL2.1: 시간적 계획 도메인을 표현하기 위한 PDDL로의 확장. _ CoRR_, abs/1106.4561, 2011. URL[http://arxiv.org/abs/1106.4561](http://arxiv.org/abs/1106.4561).\n' +
      '* Ahn et al. [2022] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022.\n' +
      '* Huang et al. [2022] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Petc Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models, 2022.\n' +
      '* Huang et al. [2022] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents, 2022.\n' +
      '* Lin et al. [2023] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2motion: from natural language instructions to feasible plans. _Autonomous Robots_, 47(8):1345-1365, November 2023. ISSN 1573-7527. doi: 10.1007/s10514-023-10131-7. URL [http://dx.doi.org/10.1007/s10514-023-10131-7](http://dx.doi.org/10.1007/s10514-023-10131-7).\n' +
      '* Singh et al. [2022] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progr prompt: Generating situated robot task plans using large language models, 2022.\n' +
      '* Wang et al. [2023] Huaxiaoyue Wang, Gonzalo Gonzalez-Pumariega, Yash Sharma, and Sanjiban Choudhury. Demo2code: From summarizing demonstrations to synthesizing code via extended chain-of-thought, 2023.\n' +
      '* Wu et al. [2022] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: personalized robot assistance with large language models. _Autonomous Robots_, 47(8):1087-1102, November 2023. ISSN 1573-7527. doi: 10.1007/s10514-023-10139-z. URL [http://dx.doi.org/10.1007/s10514-023-10139-z](http://dx.doi.org/10.1007/s10514-023-10139-z).\n' +
      '* Mavrogiannis et al. [2021] A. Mavrogiannis, Christoforos Mavrogiannis, and Yiannis Aloimonos. Cook2ltl: Translating cooking recipes to ltl formulae using large language models. _ArXiv_, abs/2310.00163, 2023.\n' +
      '* [46] Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharcco, Ludwig Schmidt, and Shuran Song. Clip on wheels: Zero-shot object navigation as object localization and exploration. _arXiv preprint arXiv:2203.10421_, 3(4):7, 2022.\n' +
      '* [47] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. _arXiv preprint arXiv:2303.03378_, 2023.\n' +
      '* [48] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. _arXiv_, 2022.\n' +
      '* [49] Siddharth Karamcheti, Suraj Nair, Annie S Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy Liang. Language-driven representation learning for robotics. _arXiv preprint arXiv:2302.12766_, 2023.\n' +
      '* [50] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. _arXiv preprint arXiv:2307.05973_, 2023.\n' +
      '* [51] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x-embodiment: Robotic learning datasets and rt-x models. _arXiv preprint arXiv:2310.08864_, 2023.\n' +
      '* [52] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning. _arXiv preprint arXiv:1910.11215_, 2019.\n' +
      '* [53] Priya Sundaresan, Suneel Belkhale, Dorsa Sadigh, and Jeannette Bohg. Kite: Keypoint-conditioned policies for semantic manipulation. _arXiv preprint arXiv:2306.16605_, 2023.\n' +
      '* [54] Hejing Ling, Guoliang Liu, Liujuan Zhu, Bin Huang, Fei Lu, Hao Wu, Guohui Tian, and Ze Ji. Motion planning combines human motion prediction for human-robot cooperation. In _2022 12th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)_, pages 672-677. IEEE, 2022.\n' +
      '* [55] Vaibhav Unhelkar, Przemyslaw A. Lasota, Quirin Tyroller, Rares-Darius Buhai, Laurie Marceau, Barbara Deml, and Julie A. Shah. Human-aware robotic assistant for collaborative assembly: Integrating human motion prediction with planning in time. _IEEE Robotics and Automation Letters_, 3:2394-2401, 2018.\n' +
      '* [56] Jim Mainprice, Rafi Hayne, and Dmitry Berenson. Predicting human reaching motion in collaborative tasks using inverse optimal control and iterative re-planning. _2015 IEEE International Conference on Robotics and Automation (ICRA)_, pages 885-892, 2015.\n' +
      '* [57] Vignesh Prasad, Dorothea Koert, Ruth Maria Stock-Homburg, Jan Peters, and Georgia Chalvatzaki. Mild: Multimodal interactive latent dynamics for learning human-robot interaction. _2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids)_, pages 472-479, 2022.\n' +
      '* [58] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. _IEEE transactions on pattern analysis and machine intelligence_, 36(7):1325-1339, 2013.\n' +
      '* [59] Wei Mao, Miaomiao Liu, and Mathieu Salzmann. History repeats itself: Human motion prediction via motion attention. In _European Conference on Computer Vision_, 2020.\n' +
      '* [60] Theodoros Sofianos, Alessio Sampieri, Luca Franco, and Fabio Galasso. Space-time-separable graph convolutional network for pose forecasting. _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 11189-11198, 2021.\n' +
      '* [61] K. Kedia, Prithwish Dan, Atiksh Bhardwaj, and Sanjiban Choudhury. Manicast: Collaborative manipulation with cost-aware human forecasting. _ArXiv_, abs/2310.13258, 2023.\n' +
      '* [62] Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. _arXiv preprint arXiv:2306.09683_, 2023.\n' +
      '* [63] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.\n' +
      '* [64] Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dormann. Stable baselines3, 2019.\n' +
      '\n' +
      '## 부록, 부록\n' +
      '\n' +
      'A절에서 자세히 설명한 바와 같이 시스템의 구성을 간략하게 설명하고, B절, C절 및 D절에 걸쳐 다루어진 시스템의 각 구성요소에 대한 추가 논의를 자세히 설명한다. 마지막으로 E절에서 자세히 설명된 실험 설정 및 보완 결과에 대한 세부 사항을 포함하는 사용자 연구에 대한 심층 분석을 제시한다.\n' +
      '\n' +
      '### _System Setup_\n' +
      '\n' +
      '**주방 장면과 로봇 배치**주방 장면은 모든 요리 활동이 이루어지는 중앙의 메인 주방 테이블로 구성되어 있습니다. 다양한 조건과 주방 스테이플이 들어 있는 식료품 저장실이 테이블 근처에 있습니다. 최종 요리를 서빙할 수 있는 중앙 테이블 옆에 보조 테이블도 있습니다. 우리의 로봇 시스템은 두 개의 로봇(R1과 R2)을 포함한다.\n' +
      '\n' +
      '* R1(_Franka Emika Research 3_[27])은 현장 중앙의 주방 테이블 한쪽 끝에 배치된 테이블 탑 7의 조작기이다.\n' +
      '* R2(_헬로 로봇 스트레치 RE1_[26])는 사용자가 요구하는 바와 같이, 조건 및 주방 용품을 인출 및 치울 수 있는, 주방 영역 주위를 탐색할 수 있는 모바일 조작기이다.\n' +
      '\n' +
      '**카메라 배치.** 테이블탑 조작기(R1)에 대해, 인식 스택은 중앙 주방 테이블 위에 배치된 2개의 인텔 릴센스 D435i RGB-D 카메라를 포함한다. 두 카메라 모두 테이블의 반대쪽 끝과 인간 사용자뿐만 아니라 탁상의 전체를 캡처하도록 각도로 배치된다. 두 카메라 관점을 통합하는 것은 어수선한 주방 환경 내에서 사물과 인간 포즈의 가시성을 향상시켜 폐색 문제를 효과적으로 완화한다. 모바일 매니퓰레이터(R2)는 물체 인식을 위해 탑재된 Intel Realsense D435i RGB-D 헤드 카메라를 사용한다.\n' +
      '\n' +
      '**Computational Details.** 로봇의 온보드 컴퓨팅 기능 외에도, 당사의 설정에는 다양한 시스템 모듈을 실행하는 데 전념하는 5대의 개인용 컴퓨터(PC)가 포함되어 있습니다. 이러한 PC는 통신을 위해 로봇 운영 체제(ROS)를 활용하여 동일한 네트워크에 연결되어 있다. 실시간 신경망 추론을 요구하는 태스크는 온보드 GPU(NVIDIA GeForce RTX 3060)를 사용한다. 각 PC의 역할 및 구성에 대한 상세한 정보가 제공된다:\n' +
      '\n' +
      '***C1:** 블루투스 마이크와 스피커로 연결된 이 PC는 사용자와 통신하기 위한 _Speech-Text_ 시스템과 GPT-4 API 호출을 활용하는 _Interactive Task Planner_를 실행한다.\n' +
      '***C2:** R2의 인지(객체 검출) 및 제어(RL 에이전트)와 관련된 신경망 모델을 실행하는 데 사용된다. 이 PC는 또한 C1과 통신하여 하위 작업을 R2에 할당한다.\n' +
      '***C3:** 이 PC는 객체 검출 및 포즈 추정을 위한 러닝 신경망 모델을 포함하는 R1에 대한 인식 스택을 형성한다.\n' +
      '***C4:** 이 PC는 C3에 의해 계산된 포즈 추정치를 사용하여 인간 예측 모델을 실행한다. 이 PC는 또한 예측된 객체 포즈 및 인간 예측에 기초하여 R1에 대한 모션 계획을 계산한다. 또한, C1과 통신하여 R2에 서브 태스크를 할당한다.\n' +
      '***C5:** 이 PC에는 실시간 커널이 설치되어 로봇 제조업체에서 권장하는 대로 1kHz 주파수로 R2에 관절 명령을 전송한다.\n' +
      '\n' +
      '###_Interactive Task Planner Details__\n' +
      '\n' +
      '인터랙티브 태스크 플래너는 레시피의 표현과 그 종속성, 레시피를 결정하고 다른 사람에게 서브태스크를 할당하는 메커니즘, 주어진 서브태스크에 사용할 수 있는 기술을 로봇과 소통하는 매체의 세 가지 주요 구성 요소로 구성된다. 우리는 직접 비순환 그래프(DAG), 행동 트리 및 LLM 생성 코드(ROS 액션 서비스를 통해 통신됨)를 사용하여 이를 구현한다.\n' +
      '\n' +
      '**Recipe DAG.** 우리는 레시피를 노드가 서브태스크를 나타내고 가장자리가 종속성을 나타내는 DAG로 나타낸다. 우리는 또한 어떤 하위 작업을 다음에 사용할 수 있는지 알기 위해 각 노드에 대해 완료된 상태를 유지한다. 사용 가능한 하위 작업을 결정하기 위해 수행 상태가 설정된 루트 노드에서 시작하여 첫 번째 하위 작업으로 나가는 모서리가 있다. 그런 다음, 수행된 상태가 설정되지 않은 노드에 도달할 때까지 각 아웃고잉 에지를 따라 세트에 추가한다. 이 과정을 통해 노드를 찾지 못하면 레시피가 완성된다. 시저 샐러드에 대한 DAG의 예는 도 12를 참조한다.\n' +
      '\n' +
      'DAG는, _sequential:_ \'do A before B\', _AND_ 종속성: \'do A and B before C\'와 같은 종속성을 나타낼 수 있게 한다; 현재 _OR_ 종속성을 허용하지 않는다(do A or B before C). 그러나 이러한 제한으로 여전히 32가지 고유한 레시피를 만들 수 있습니다.\n' +
      '\n' +
      '**Behavior Tree.** 우리는 행동 트리를 사용하여 레시피를 결정한 다음 우리가 기대하는 행동 주위에 트리를 설계하여 다른 사람들에게 서브태스크를 할당한다. 그것은 세계의 관찰을 입력으로 하고 높은 수준의 액션에 대한 인수를 출력한다.\n' +
      '\n' +
      '관찰은 다음과 같이 구성됩니다.\n' +
      '\n' +
      '1. 레시피명\n' +
      '2. 사용 가능한 서브 태스크\n' +
      '3. 각 로봇의 서브 태스크 큐, 현재 서브 태스크 및 현재 상태(Idle, Running 또는 Interrupted)\n' +
      '4. 사용자의 서브태스크 큐\n' +
      '5. 완료된 서브태스크 큐\n' +
      '6. 사용자의 현재 입력\n' +
      '7. 채팅 히스토리\n' +
      '\n' +
      '레시피가 아직 결정되지 않은 경우 레시피 이름은 비워둘 수 있습니다. 사용 가능한 하위 작업은 DAG로 채워집니다. 로봇 및 사용자 하위 작업은 모두 행동 트리의 상위 액션에 의해 채워지며, 로봇은 로봇이 하위 작업을 완료함에 따라 ROS 액션 서버를 통해 업데이트된 현재 하위 작업 및 상태 필드를 추가로 갖는다. 서브 태스크가 완료되면, 완료된 서브 태스크 큐가 업데이트된다. 마지막으로, 사용자의 현재 발화 입력은 저장되고 나중에 태스크 플래너의 메시지와 함께 채팅 히스토리에 추가된다.\n' +
      '\n' +
      '상기 상위 레벨 액션들은,\n' +
      '\n' +
      '* say(msg)\n' +
      '* set_recipe(name)\n' +
      '* assign(agent, subtask)* mark_complete(subtask)\n' +
      '* interrupt(agent)\n' +
      '* no_op()\n' +
      '\n' +
      'say(...)는 태스크 플래너가 메시지와 함께 사용자에게 통신할 수 있게 한다. 할당(...)은 에이전트(로봇 또는 인간)에게 하위 작업 목록을 할당합니다. mark_complete(...)는 완료된 대로 하위 작업의 목록을 설정합니다. 중단(...)은 로봇이 현재 하위 작업을 수행하는 것을 중지합니다. no_op()는 아무것도 하지 않는다.\n' +
      '\n' +
      '트리는 (1) 다음 노드가 실행할 결정을 출력하는 LLM 또는 (2) 취할 상위 레벨 액션에 대한 인수를 각각 쿼리하는 다양한 노드로 구성된다. 각 노드는 LLM을 쿼리할 때 사용되는 프롬프트와 연관됩니다. 노드의 쿼리 응답이 잘못된 경우(예: 잘못된 JSON) 또는 잘못된 경우(예: 잘못된 결정 또는 인수) 노드가 다시 실행됩니다. 각 노드는 입력으로서 관찰만을 필요로 하므로, 각 노드를 동시에 실행하여 LLM 질의를 병렬화하고 결정된 사항을 기반으로 루트로부터 리프까지의 경로를 그릴 수 있다.\n' +
      '\n' +
      '트리는 관측치가 과거 관측치와 다를 때마다 높은 수준의 조치를 취하는 주기를 실행합니다. 이를 통해 사용자는 태스크 플래너의 질문에 응답할 수 있는 시간을 갖게 된다. 사용자 입력을 수신하고 사용자에게 응답하기 위해, 우리는 각각 스피치 투 텍스트 및 텍스트 투 스피치 모듈을 사용한다. 트리는 스크립트가 종료될 때까지 무기한 실행됩니다.\n' +
      '\n' +
      '**코드 생성** 태스크 플래너가 로봇에 하위 작업을 할당할 때마다 로봇이 수행할 수 있는 하위 수준의 스킬의 시퀀스로 변환해야 합니다. 우리는 로봇이 실행하는 코드를 생성하기 위해 LLM을 사용하여 이것을 한다.\n' +
      '\n' +
      '작업 계획자가 로봇에 하위 작업을 할당할 때 먼저 로봇의 하위 작업 대기열에 추가됩니다. 로봇 전용 스레드는 큐에 하위 작업이 있는지 확인하고 현재 하위 작업에 추가하기 위해 팝하고 상태를 실행으로 설정합니다. 코드 생성을 위한 별도의 프롬프트는 제공된 서브태스크에 대한 코드를 생성하기 위해 LLM에 질의하기 위해 사용된다. 생성된 코드의 일 예는,\n' +
      '\n' +
      '**from** robot_utils **import**<robot_api>\n' +
      '\n' +
      '**from** env_utils **import** <env_constants>\n' +
      '\n' +
      'pick_up_item(LADLE)\n' +
      '\n' +
      'place_item_at(POT)\n' +
      '\n' +
      'stir()\n' +
      '\n' +
      '여기서 <robot_api>는 pick_up_item(...)과 같은 모든 하위 레벨 로봇 기술을 포함하고, <env_constants>는 환경 내의 객체들에 대한 열거들을 포함한다. 로봇 스킬을 실행하는 코드의 각 라인은 상기 스킬을 실행하기 위해 ROS 액션을 로봇에 전송한다. 로봇이 현재 스킬 실행을 마치면, 태스크 플래너에 완료되었음을 전달하고, 이는 다시 다른 스킬을 전송할 수 있다. 이는 전체 하위 작업이 완료될 때까지 계속되며, 이 경우 로봇의 현재 하위 작업이 지워지고 로봇의 상태가 유휴 상태로 설정됩니다. 로봇이 중단되면 현재 하위 작업도 삭제되지만 상태는 중단으로 설정됩니다.\n' +
      '\n' +
      '###_Visuomotor Skills Details\n' +
      '\n' +
      '**Skill Library** 태스크 플래너는 객체 위치들 및 타겟 위치들에 의해 파라미터화된 함수 호출들로서 표현되는 다수의 로봇 스킬들에 대한 액세스를 갖는다. 각 스킬에 대해, 태스크 플래너에 의해 제공된 텍스트 프롬프트가 주어진 개방 어휘 객체 검출 모델인 OWL-ViT(다음 섹션에서 더 자세한 내용)를 사용하여 객체의 위치를 추정한다. 내비게이션을 위해 주방 장면이 실행 간의 구성을 변경하지 않는다고 가정하여 매핑된 위치를 실제 좌표에 저장합니다. 우리는 이 논문에서 두 로봇이 수행하는 낮은 수준의 기술 세트 아래에 열거한다:\n' +
      '\n' +
      '1. 픽(<obj>): 두 로봇 모두 동일한 객체 검출 모듈을 공유하여 픽(<obj>) 작업을 완료하여 경계 박스를 획득하고 관심 객체를 중심으로 3D 파지 포즈를 취한다. R1(프랑카 암)은 역 운동학 기반 관절 임피던스 제어기를 사용하여 파지 포즈로 직접 이동한다. R2(스트레치 로봇)는 어수선한 식료품 저장실에서 물건을 줍는 작업을 수행합니다. 팬트리 및 주변 사물에 부딪히는 것을 방지하기 위해 로봇은 시뮬레이션을 통해 학습된 강화 학습 정책을 사용하여 액션을 실행합니다.\n' +
      '2. 이동(<loc>): 이 기술은 미리 획득된 주방의 맵 및 스트레치 RE1의 내부 로컬화 메커니즘을 사용하여 주방의 주변 지정된 위치로 탐색한다.\n' +
      '\n' +
      '도. 12: **Recipe DAG 예. 이 DAG는 시저 샐러드를 만드는 데 관련된 하위 작업과 종속성을 나타낸다. 레시피 제작 초기에는 \'상추 준비\', \'후추 가져오기\', \'농장 소스 가져오기\'가 사용 가능한 하위 작업이다. 하위 작업 중 하나가 수행된 것처럼 표시되면 다음과 같은 하위 작업이 사용 가능하게 된다(예: \'Get pepper\' 완료로 인해 \'Pour pepper in bowl\'이 사용 가능하게 된다).**3. 장소(<loc>): 장소(<loc>): 장소(<loc>) 스킬은 목표 위치에 의해 매개변수화되고 사전 코딩된 모션 프리미티브로 완료된다.\n' +
      '4. 교반(<obj1, obj2>) 로봇의 손(<obj1>), 예를 들어 액션이 일어나는 국자 및 목표 기구(<obj2>), 예를 들어 냄비 내의 툴에 의해 파라미터화된 R2(프랑카 암)에 대한 이 모션 프리미티브를 정의한다. 또한, 이 기술은 로봇의 교반 반경으로 사람의 움직임에 반응한다. 인간의 움직임 예측이 로봇의 작업 공간으로 도달하면 로봇은 교반을 멈추고 인간이 이동할 공간을 만든다.\n' +
      '5. 붓기(<obj1, obj2>) 교반() 기능과 유사하게, 이 기술은 R2가 소금 통과 같은 물체(<obj1>)를 그릇과 같은 목표 용기(<obj2>)에 붓도록 한다. 이 프로세스는 관련된 객체들의 추정된 위치들에 기초한 모션 프리미티브들의 활용을 수반한다. 구체적으로, 그릇에 소금을 붓는 시나리오에서, R2는 일련의 액션들을 실행한다: 그것은 먼저 테이블로부터 소금 캔을 회수하고, 계산된 경사각으로 그릇 위에 위치시킨 다음, 소금을 분배하기 위해 캔을 흔든다. 붓기 작업이 완료된 후 R2는 소금통을 테이블 위의 원래 위치로 반환한다.\n' +
      '6. 핸드오버(<obj>) R2(Franka arm)는 예측된 손목 위치를 향해 엔드 이펙터를 직접 이동시킴으로써 빠르고 효율적으로 핸드오버를 완료한다. 로봇의 엔드 이펙터가 사람의 손목 위치의 임계값 내에 있으면 로봇이 멈추고 물체를 로봇의 손 안으로 방출합니다. 마지막으로 로봇 암이 원래 위치로 다시 재설정됩니다.\n' +
      '\n' +
      '**객체 위치화** 객체 위치화 파이프라인은 먼저 관심 객체의 입력 RGB 이미지 및 텍스트 프롬프트로서 취하며, 이는 객체의 가능한 위치를 나타내는 \\(k\\) 바운딩 박스 제안들을 생성하는 OWLViT [62] 객체 검출 모델을 통과한다. 이 \\(k\\) 바운딩 박스들은 중첩 박스들을 제거하기 위해 비-최대 억제를 사용하여 필터링된다. 카메라의 각도 및 다른 잡음으로 인해, 우리는 상단 OWLViT 바운딩 박스가 원하는 객체와 확실하게 일치하지 않는다는 것을 발견한다. 따라서, 이러한 제안들은 크롭된 바운딩 박스들 및 텍스트 프롬프트의 이미지들 각각을 미리 트레이닝된 CLIP[23] 모델에 공급함으로써 정제되어, 각각의 크롭된 이미지가 텍스트 프롬프트1과 얼마나 정렬되었는지를 측정하는 CLIP 스코어를 생성한다. 다음으로, 이미지, 가장 높은 CLIP 스코어를 갖는 바운딩 박스 및 텍스트 프롬프트는 바운딩 박스 내에 위치된 객체를 세그먼트화하기 위해 미리 트레이닝된 FastSAM[19] 모델에 공급된다. 깊이 카메라에 의해 주어진 포인트 클라우드는 세그먼테이션 마스크 내부의 모든 포인트들을 3D 공간으로 투영하는데 사용된다. 객체의 모든 3D 포인트들은 최종적인 단일 3D 포인트를 획득하기 위해 평균화된다. 그런 다음 이 3D 포인트는 로봇이 어떻게 오브젝트로 이동해야 하는지에 대한 액션들을 생성하기 위해 실행 모듈에 공급된다.\n' +
      '\n' +
      '각주 1: \\(k\\)이 너무 낮게 설정되면, 세트는 CLIP에 의해 사용될 관심 객체 주위에 바운딩 박스를 포함하지 않을 수 있다. \\(k\\)이 너무 높게 설정되면 바운딩 박스의 세트가 너무 시끄러워서 정확도가 낮아질 수 있다. 모든 실험에 대해 \\(k=10\\)을 설정하였다.\n' +
      '\n' +
      '**RL a Differentiable IK Solver** RL 에이전트는 목표 예측을 취하고 그 목표에 안전하게 도달하기 위해 일련의 액션을 실행할 필요가 있다. 특히 선택하려면 품목이 들어 있는 식료품 저장실을 고려하십시오. 바람직한 궤적은 그리퍼가 접근함에 따라 팬트리 보드를 타격하고 이웃하는 물체를 타격하며 물체를 밀어내는 것을 피할 수 있다. 에이전트를 안내하기 위해 주어진 목표점에 대해 목표의 측면, 후면 및 바닥에 3차원 벽 세트를 구축하는 시뮬레이터를 생성한다. 잘못된 작업은 벽과 충돌하거나 로봇 관절 상태를 위반하는 작업입니다. 에피소드는 로봇이 도달할 수 있는 일정 거리 내의 시작 위치와 목표 위치를 샘플링함으로써 시작된다. 관측 공간은 목표 위치와 현재 위치 사이의 \\(L_{1}\\) 규범이다. 그런 다음 다음 비용 함수를 사용하여 시연 데이터에서 원격 조작 명령과 동일한 액션 공간을 갖는 Raffin et al. [64]의 구현을 사용하여 프록시멀 정책 최적화 [63] 에이전트를 훈련한다.\n' +
      '\n' +
      '\\[\\exp(-\\|O_{c}-O_{g}\\|_{2})-1 \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(O_{c}\\) 및 \\(O_{g}\\)은 각각 전류 및 원하는 엔드-이펙터 좌표를 나타내고 \\(\\|\\cdot\\|_{2}\\)은 유클리드 거리이다. 에이전트의 주요 고장 사례는 관찰 공간이 절리 상태를 포함하지 않기 때문에 벽을 피하려고 시도하면서 절리 제약을 위반하는 것이다.\n' +
      '\n' +
      '**베바오랄 복제 기준** 우리의 BC 정책은 256개의 뉴런이 있는 두 개의 피드포워드 레이어로 구성되며 로봇 기본 위치, 로봇 시작 포즈 및 식료품 저장실의 목표 위치에 변화가 있는 485개의 시연 궤적에 대해 훈련된다. 각 타임스텝에서 모델은 현재 엔드 이펙터 위치와 최종 위치(RL 에이전트와 동일)의 차이를 입력으로 하고 로짓의 10차원 벡터를 출력하며, 여기서 각 차원은 로봇의 10개의 관절 중 하나를 움직이는 것에 대응한다. 모델은 클래스 불균형을 설명하기 위해 가중 교차 엔트로피 손실 함수를 사용하여 트레이닝된다. 정책에서 최종 조치는 출력 벡터에서 범주형으로 샘플링하여 얻는다.\n' +
      '\n' +
      '###_Human Motion Forecasting_\n' +
      '\n' +
      '**모델 아키텍처.** 우리는 인간의 관절 위치를 서로 다른 타임스텝에서 노드로 인코딩하는 인간-모션 예측기를 위해 STS-GCN(Space-Time Separable Graph Convolutional Network) [60] 모델 아키텍처를 그래프에 사용한다. 모든 노드들 사이에 완전히 연결된 그래프를 단순히 구성하는 대신에, 모델은 시간적 및 공간적 차원들에 걸쳐 리던던트 에지들 없이 희소 네트워크를 구성한다. 에지는 연속적인 타임스텝을 통해 동일한 인간 관절 사이 및 동일한 타임스텝에서 모든 관절 사이에서만 연결된다.\n' +
      '\n' +
      '**Experimental Setup.** 실시간 추론을 위해 인간의 움직임 예측 모델을 사용하기 위해 인간의 몸통을 가리키는 RGB-D 카메라(Intel RealSense D435)를 사용한다. 인간의 자세는 7개의 상체 관절(어깨, 팔꿈치, 손목, 목)의 3D 위치로 표현된다. 입력된 RGB 영상에서 MediaPipe[25]를 이용하여 2차원 인간 관절 위치를 추적하고 깊이 맵을 이용하여 3차원 세계 좌표에 역 투영한다. 본 논문에서 제안한 방법은 고충실도 모션 캡처 데이터에 대해 학습된 모션 예측 모델에 대해 분포 외인 깊이 맵 투영으로부터 잡음이 발생하는 입력을 처리할 수 밖에 없다. 먼저 CoMaD[24]에 대한 예측 성능을 비교하여 동적 주방 환경에서 인간의 움직임을 예측하기에 적합한 모델을 선택한다. CoMaD는 모션 캡처 슈트를 통해 수집되며 에피소드당 평균 30초(총 데이터의 4시간 이상)의 길이로 3개의 다른 주방 작업에 걸쳐 270개의 인간-인간 상호작용 에피소드를 포함한다. 그런 다음, 열차-테스트 분포 불일치를 극복하기 위해 열차 시간에 다양한 수준의 랜덤 가우시안 노이즈를 모션 캡처 데이터에 주입하는 실험을 수행하고 단일 카메라 설정으로 추적된 인간 모션 데이터 세트에 대한 결과를 보고한다.\n' +
      '\n' +
      '**예측 메트릭.** 모든 예측 타임스텝에 대한 평균 변위 오차(Average Displacement Error, ADE)와 0.4초의 포즈 이력이 주어진 미래 1초 예측의 최종 변위 오차(Final Displacement Error, FDE)를 모두 측정하여 예측자에 의해 이루어진 오차를 정량화한다. 특히 모든 조인트와 리스트에 대한 메트릭을 보고합니다. 이는 우리가 실행하는 조작 작업에서 가장 관련성이 높은 조인트이기 때문입니다. 우리는 또한 접두사 \'T-\'(예: T-All Joints ADE, T-Wrists ADE)로 표시된 인간이 서로 밀접하게 접촉하는 짧은 전환 윈도우 동안 인간 모션의 CoMaD _transition dataset_에 대한 예측 메트릭을 보고한다. 테이블 설정 작업 중에 인간은 항상 매우 가까이 있습니다.\n' +
      '\n' +
      '**CoMaD 예측 결과.** 우리의 두 기준선은 (1) AMASS 데이터에 대해서만 훈련된 베이스와 (2) CoMaD 데이터에 대해서만 훈련된 스크래치이다. 두 가지 모델에 대한 결과를 보고한다. (3) FineTuned, AMASS 데이터 사전 훈련 및 CoMaD 데이터 미세 조정, (4) FineTuned-T, AMASS 데이터 사전 훈련 및 CoMaD 미세 조정은 _transition dataset_에서 업샘플링한다. 각 모델은 보류된 CoMaD 테스트 에피소드 세트에서 테스트된다. FineTuned-T는 반응 교반 및 핸드오버 작업에 대해 모든 메트릭에서 다른 모든 모델보다 훨씬 우수합니다. 표 설정 작업에서 파인튜닝은 파인튜닝-T에 비해 약간만 낮은 오류를 생성했으며 둘 다 기준선을 능가했다.\n' +
      '\n' +
      '우리는 인간이 밀접하게 접촉하는 CoMaD 전이 데이터를 업샘플링하면 주방 활동에 대한 보다 정확한 움직임 예측이 가능하다는 것을 발견했다. 베이스는 단일 인간의 AMASS[15] 데이터에 대해서만 훈련되고 상호 작용 데이터가 없기 때문에 매우 역동적인 조작 작업에서 정확한 예측을 생성하기 위해 고군분투한다. 스크래치는 훨씬 작은 CoMaD에서 일반적인 인간의 운동 역학을 배우는 데 어려움을 겪는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c|c} \\hline \\hline  & Metrics (mm) \\(\\downarrow\\) & Base & Scratch & FineTuned & FineTuned-T \\\\ \\hline \\multirow{5}{*}{\\begin{tabular}{c} Nothing \\\\ \\end{tabular} } & All Joints ADE & 60.3 (\\(\\pm\\) 0.6) & 40.0 (\\(\\pm\\) 0.3) & 32.1 (\\(\\pm\\) 0.2) & 29.9 (\\(\\pm\\) 0.2) \\\\  & All Joints FDE & 91.5 (\\(\\pm\\) 0.9) & 60.3 (\\(\\pm\\) 0.5) & 54.0 (\\(\\pm\\) 0.5) & 51.7 (\\(\\pm\\) 0.4) \\\\  & Wrists ADE & 83.7 (\\(\\pm\\) 0.6) & 58.0 (\\(\\pm\\) 0.4) & 47.9 (\\(\\pm\\) 0.3) & 44.9 (\\(\\pm\\) 0.3) \\\\  & Wrists FDE & 128.0 (\\(\\pm\\) 1.0) & 87.2 (\\(\\pm\\) 0.7) & 80.7 (\\(\\pm\\) 0.6) & 76.6 (\\(\\pm\\) 0.6) \\\\  & T-All Joints ADE & 58.0 (\\(\\pm\\) 0.4) & 38.7 (\\(\\pm\\) 0.2) & 31.1 (\\(\\pm\\) 0.1) & 28.8 (\\(\\pm\\) 0.1) \\\\  & T-All Joints FDE & 87.7 (\\(\\pm\\) 0.6) & 58.0 (\\(\\pm\\) 0.3) & 52.0 (\\(\\pm\\) 0.3) & 49.6 (\\(\\pm\\) 0.3) \\\\  & T-Wrists ADE & 81.8 (\\(\\pm\\) 0.4) & 56.8 (\\(\\pm\\) 0.2) & 46.8 (\\(\\pm\\) 0.2) & 43.8 (\\(\\pm\\) 0.2) \\\\  & T-Wrists FDE & 124.6 (\\(\\pm\\) 0.7) & 84.9 (\\(\\pm\\) 0.5) & 78.7 (\\(\\pm\\) 0.4) & 74.4 (\\(\\pm\\) 0.4) \\\\ \\hline \\multirow{5}{*}{\\begin{tabular}{c} Nothing \\\\ \\end{tabular} } & All Joints ADE & 56.3 (\\(\\pm\\) 0.3) & 40.4 (\\(\\pm\\) 0.2) & 32.9 (\\(\\pm\\) 0.1) & 31.4 (\\(\\pm\\) 0.1) \\\\  & All Joints FDE & 88.0 (\\(\\pm\\) 0.5) & 62.8 (\\(\\pm\\) 0.4) & 56.2 (\\(\\pm\\) 0.3) & 55.0 (\\(\\pm\\) 0.3) \\\\  & Wrists ADE & 88.5 (\\(\\pm\\) 0.4) & 64.2 (\\(\\pm\\) 0.3) & 51.8 (\\(\\pm\\) 0.3) & 50.0 (\\(\\pm\\) 0.2) \\\\  & Wrists FDE & 139.4 (\\(\\pm\\) 0.8) & 100.3 (\\(\\pm\\) 0.6) & 89.2 (\\(\\pm\\) 0.6) & 87.4 (\\(\\pm\\) 0.6) \\\\  & T-All Joints ADE & 54.0 (\\(\\pm\\) 0.2) & 38.9 (\\(\\pm\\) 0.1) & 31.7 (\\(\\pm\\) 0.1) & 30.2 (\\(\\pm\\) 0.1) \\\\  & T-All Joints FDE & 83.8 (\\(\\pm\\) 0.4) & 59.6 (\\(\\pm\\) 0.3) & 53.5 (\\(\\pm\\) 0.3) & 52.4 (\\(\\pm\\) 0.3) \\\\  & T-Wrists ADE & 85.2 (\\(\\pm\\) 0.3) & 61.9 (\\(\\pm\\) 0.3) & 50.1 (\\(\\pm\\) 0.2) & 48.3 (\\(\\pm\\) 0.2) \\\\  & T-Wrists FDE & 133.0 (\\(\\pm\\) 0.6) & 95.4 (\\(\\pm\\) 0.5) & 85.2 (\\(\\pm\\) 0.4) & 83.4 (\\(\\pm\\) 0.4) \\\\ \\hline \\multirow{5}{*}{\n' +
      '\\begin{tabular}{c} Nothing \\\\ \\end{tabular} } & All Joints ADE & 107.0 (\\(\\pm\\) 1.1) & 72.0 (\\(\\pm\\) 0.5) & 59.0 (\\(\\pm\\) 0.4) & 59.1 (\\(\\pm\\) 0.4) \\\\  & All Joints FDE & 181.0 (\\(\\pm\\) 1.9) & 118.1 (\\(\\pm\\) 0.9) & 108.0 (\\(\\pm\\) 0.8) & 108.8 (\\(\\pm\\) 0.8) \\\\ \\cline{1-1}  & Wrists ADE & 127.1 (\\(\\pm\\) 1.0) & 93.4 (\\(\\pm\\) 0.6) & 80.4 (\\(\\pm\\) 0.5) & 81.7 (\\(\\pm\\) 0.5) \\\\ \\cline{1-1}  & Wrists FDE & 224.7 (\\(\\pm\\) 2.0) & 152.6 (\\(\\pm\\) 1.1) & 143.1 (\\(\\pm\\) 1.0) & 145.8 (\\(\\pm\\) 1.0) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: **CoMaD 예측 메트릭.** 베이스, 스크래치, 파인튜닝 및 파인튜닝-T의 상이한 예측 모델에 대한 핸드오버, 반응성 교반 및 표 설정 작업에 대한 평균 변위 오차(ADE) 및 최종 변위 오차(FDE)를 보고한다. \'T-\'가 접두된 메트릭은 인간이 밀접하게 접촉하는 데이터인 _전이 데이터세트_로부터의 측정을 나타낸다. Finetuned-T는 반응 교반 및 핸드오버에서 가장 낮은 오류를 생성하고 표 설정에서는 매우 약간 더 높은 오류를 생성한다.\n' +
      '\n' +
      '데이터 세트는 AMASS에 비해 더 높은 오류로 반영된다. 궁극적으로, 우리는 대규모 인간 활동 데이터에 대한 사전 훈련 예측 모델과 인간-인간 상호작용 데이터에 대한 미세 조정이 근접 주방 조작 작업에서 최상의 성능을 산출한다는 것을 발견했다. 이 방법은 나머지 실험을 위해 Finetune-T를 사용한다.\n' +
      '\n' +
      '**Vision-Based Forecasting Results.** 우리는 RGB-D 기반 3D 포즈 추적 시스템에서 예측을 할 때 모션 예측 모델이 직면한 열차-테스트 분포 불일치(고충실도 모션 캡처 데이터에 대해 훈련되고 RGB-D 카메라에 의해 추정된 인간 포즈에 대해 테스트됨)를 해결하기 위해 시도하며, 이는 열차 시간에 랜덤 가우시안 노이즈를 모션 캡처 입력에 주입하여 모델을 강제로 입력을 제거하고 부드러운 예측을 생성하게 한다. 형식적으로, 인간의 자세이력(\\(K\\) timesteps\\(\\phi\\in\\mathbb{R}^{K\\times J\\times 3}\\)이 주어졌을 때, 가우시안 잡음(N\\in\\mathbb{R}^{K\\times J\\times 3}sim\\mathcal{N}(0,\\sigma^{2}I)\\)을 추가하여 (\\phi_{\\sigma}=\\phi+N\\) (\\(\\sigma\\)는 자세이력에 주입된 "노이즈 레벨"을 나타낸다. (\\xi_{H}\\in\\mathbb{R}^{T\\times J\\times 3}\\)이 다음 \\(T\\) 타임스테프에서 인간의 자세를 나타내도록 하자. 전통적인 방법으로 \\(P(\\xi_{H}|\\phi)\\)에 대한 모델을 학습하는 대신, 우리는 \\(P(\\xi_{H}|\\phi_{\\sigma})\\)에 대한 모델을 학습한다. 표 VI는 \\(\\sigma\\in\\{0,0.001,0.01,0.1\\}\\)으로 훈련된 모델에 대한 반응 교반 및 핸드오버 태스크에 대한 비전 기반 예측 메트릭을 보여준다. 단일 카메라 기반의 3차원 자세 이력으로부터 사람의 움직임을 예측할 때, 하이퍼파라미터 \\(\\sigma=0.01\\)으로 학습된 모델은 모든 메트릭(ADE와 FDE)에 걸쳐 가장 정확한 예측을 생성하여 전체 시스템에 통합하기에 가장 적합하다는 것을 알 수 있었다.\n' +
      '\n' +
      '### _ask Planner User Study_\n' +
      '\n' +
      '**실험 설정** 사용자 연구를 수행하기 위해 태스크 플래너와 채팅할 수 있는 웹 기반 애플리케이션을 구축한다. 애플리케이션은 주방의 환경을 가상으로 시뮬레이션하기 위한 것으로, 참가자들은 1) 플래너와의 채팅 이력, 2) 완전한 레시피, 3) 각 에이전트의 현재 태스크 큐, 4) 이용 가능한 태스크 및 5) 완료된 태스크를 본다(도 13 참조). 애플리케이션은 사용자가 태스크 플래너와 한번 상호작용하고, 미리 결정된 레시피를 준비한 다음, 그들의 경험에 기초하여 설문 질문에 답변할 수 있게 한다. 인터페이스를 사용하는 방법, 각 로봇의 기능은 무엇인지, 위반 사항을 모니터링하기 위한 안전 제약 사항은 무엇인지에 대한 지침과 예를 제공한다.\n' +
      '\n' +
      '우리는 내부 연구의 참가자들에게 할당하기 위해 "아보카도 토스트", "수나이드", "밀크셰이크", "비리아니", "라멘", "볶은 국수" 및 "파스타"의 7가지 레시피를 선택했으며, 레시피 DAG에 거의 동일한 수의 노드를 가진 디저트, 국수 및 앙트레의 혼합물을 무작위로 선택했다. 각 참가자는 각 플래너(_One-Prompt_ 및 _Tree_)와 함께 동일한 레시피를 두 번 준비했지만 두 상호 작용에서 플래너가 다르다는 것을 인식하지 못했다.\n' +
      '\n' +
      '또한 외부 연구를 수행하기 위해 "망고 찹쌀", "계란탕", "파스타 샐러드" 및 위에서 6개의 10가지 조리법을 선택했다. 우리는 다시 비슷한 길이의 다양한 다른 요리법을 추가했다. 내부 연구에 따르면 모든 지역과 문화의 참가자가 이 요리에 익숙하지 않을 수 있으며 레시피의 친숙성은 상호 작용에 집중하는 데 도움이 되기 때문에 "비리아니"를 특히 제외했다.\n' +
      '\n' +
      '따라서 \\(n=46\\) 상호작용 중 내부 참가자 13명 중 26명이 내부 스터디로 설정되었고, 외부 참가자 20명이 내부 스터디로 설정되었다. 내부 연구에서 두 설계자가 제시되는 순서를 무작위화하고 모든 참가자가 인터페이스에 대한 친숙성을 개발하기 위해 선택한 레시피를 사용하여 "파일럿" 연구에 참여했다. 우리는 레시피의 크기에 따라 상호작용의 큰 분산으로 인해 "파일럿" 연구의 결과를 포함하지 않는다.\n' +
      '\n' +
      '**사용자에게 지침.** 내부 및 외부 모든 사용자에게 계획자와 자연스럽게 상호 작용하고 적어도 3개의 비명목 상호 작용(제약 위반을 가져오기 위해)을 요청했습니다.\n' +
      '\n' +
      '1. 그들은 자신이 만들고 싶은 레시피를 직접 명명해서는 안 되며, 대신 조수가 제안하도록 유도해야 한다.\n' +
      '2. 그들은 "나는 교반을 다룰 것이다"와 같은 태스크의 할당에 적어도 하나의 개입을 해야 한다.\n' +
      '3. 그들은 적어도 하나의 태스크 중 일부가 아니었던 태스크를 추가해야 한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c|c c}  & Metrics (mm) \\(\\downarrow\\) & Noise \\({}_{0}\\) & Noise \\({}_{0.001}\\) & NoISE \\({}_{0.01}\\) & NoISE \\({}_{0.1}\\) \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} **Vision-Based Forecasting Metrics** \\\\ \\end{tabular} } & All Joints ADE & 75.1 (\\(\\pm\\) 1.2) & 70.8 (\\(\\pm\\) 1.2) & 64.8 (\\(\\pm\\) 0.9) & 136.2 (\\(\\pm\\) 0.9) \\\\  & All Joints FDE & 107.3 (\\(\\pm\\) 1.8) & 103.5 (\\(\\pm\\) 1.7) & 94.0 (\\(\\pm\\) 1.3) & 155.4 (\\(\\pm\\) 1.2) \\\\  & Wrists ADE & 97.6 (\\(\\pm\\) 1.8) & 90.4 (\\(\\pm\\) 1.8) & 81.8 (\\(\\pm\\) 1.5) & 116.0 (\\(\\pm\\) 1.3) \\\\  & Wrists FDE & 128.1 (\\(\\pm\\) 2.5) & 124.5 (\\(\\pm\\) 2.5) & 120.7 (\\(\\pm\\) 2.1) & 140.3 (\\(\\pm\\) 2.1) \\\\ \\hline \\multirow{4}{*}{\n' +
      '\\begin{tabular}{c} **Vision-Based Forecasting Metrics** \\\\ \\end{tabular} } & All Joints ADE & 66.1 (\\(\\pm\\) 1.0) & 59.9 (\\(\\pm\\) 1.0) & 55.2 (\\(\\pm\\) 0.8) & 151.1 (\\(\\pm\\) 0.5) \\\\  & All Joints FDE & 95.9 (\\(\\pm\\) 1.4) & 90.6 (\\(\\pm\\) 1.4) & 83.2 (\\(\\pm\\) 1.2) & 175.6 (\\(\\pm\\) 0.8) \\\\  & Wrists ADE & 97.5 (\\(\\pm\\) 2.0) & 88.0 (\\(\\pm\\) 1.9) & 80.1 (\\(\\pm\\) 1.7) & 136.0 (\\(\\pm\\) 1.0) \\\\  & Wrists FDE & 137.8 (\\(\\pm\\) 2.8) & 131.0 (\\(\\pm\\) 2.8) & 126.8 (\\(\\pm\\) 2.7) & 176.8 (\\(\\pm\\) 1.6) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VI: **Vision-based Forecasting Metrics. We report Average Displacement Error (ADE) and Final Displacement Error (FDE) for both Handover and Reactive Stirring tasks at various levels of Gaussian noise injection into training inputs ranging from 0 to 0.1. At noise level 0.01, the error is the lowest across all tasks and metrics.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:21]\n' +
      '\n' +
      '* _권한 없이 행위_: 태스크 플래너는 사용자의 권한 없이 서브태스크를 할당/제거한다.\n' +
      '* _Lying_: 태스크 플래너는 무언가를 한다고 주장하지만 그것을 하지 않는다.\n' +
      '*_Ignore User_: 사용자의 지시에 응답하지 않는다.\n' +
      '\n' +
      '표 VII에는 이러한 제약 조건 각각에 대한 위반 사례가 나열되어 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c|c c|c c}  & & \\multicolumn{2}{c}{Act Without Permission} & \\multicolumn{2}{c}{Lying} & \\multicolumn{2}{c}{Ignore User} \\\\ Study & Approach & M \\(\\pm\\) SE & t, p, df & M \\(\\pm\\) SE & t, p, df & M \\(\\pm\\) SE & t, p, df \\\\ \\hline \\multirow{2}{*}{**Combined Study (n = 46)**} & **One-Prompt Tree** & \\(2.26\\pm 0.42\\) & \\(-2.1,\\textbf{\\emph{{0}}4},36.5\\) & \\(1.39\\pm 0.31\\) & \\(-2.11,\\textbf{\\emph{{0}}4},41.76\\) & \\(0.35\\pm 0.15\\) & \\(-0.21,.83,43.9\\) \\\\  & **Tree** & \\(1.22\\pm 0.26\\) & \\(-2.1,\\textbf{\\emph{{0}}4},36.5\\) & \\(1.39\\pm 0.31\\) & \\(-2.11,\\textbf{\\emph{{0}}4},41.76\\) & \\(0.35\\pm 0.15\\) & \\(-0.21,.83,43.9\\) \\\\ \\hline \\multirow{2}{*}{Internal Study (n = 26)} & One-Prompt Tree & \\(2.15\\pm 0.42\\) & \\(-1.07,.29,24\\) & \\(1.23\\pm 0.32\\) & \\(-2.51,.02,24\\) & \\(0.23\\pm 0.12\\) & \\multirow{2}{*}{\\(1.13,.27,24\\)} \\\\  & Tree & \\(1.53\\pm 0.38\\) & & \\(0.3\\pm 0.17\\) & \\(-2.51,.02,24\\) & & \\(0.23\\pm 0.12\\) & \\multirow{2}{*}{\\(1.13,.27,24\\)} \\\\ \\hline \\multirow{2}{*}{External Study (n = 20)} & One-Prompt Tree & \\(2.4\\pm 0.83\\) & \\(-1.8,.08,24\\) & \\(1.6\\pm 0.58\\) & \\(0.90\\pm 0.50\\) & \\(-0.91,.37,18\\) & \\(0.50\\pm 0.30\\) & \\(-1.63,.12,18\\) \\\\  & Tree & 0.8 \\(\\pm\\) 0.29 & & \\(0.90\\pm 0.50\\) & \\(-0.91,.37,18\\) & \\(0.90\\pm 0.00\\) & \\(-1.63,.12,18\\) \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 VIII: 트리 태스크 플래너를 사용하여 _Act Without Permission_ 및 _Lying_(n = 46)에서 상당한 감소를 나타내는 사용자 스터디(들)의 결과. M: _Mean_, SE: _Standard Error_, t: _t-value_, p: _p-value_, df: _degrees of freedom_\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c|c c|c c}  & & \\multicolumn{2}{c}{Act Without Permission} & \\multicolumn{2}{c}{Lying} & \\multicolumn{2}{c}{Ignore User} \\\\ Study & Approach & M \\(\\pm\\) SE & t, p, df & M \\(\\pm\\) SE & t, p, df & M \\(\\pm\\) SE & t, p, df \\\\ \\hline \\multirow{2}{*}{**Combined Study (n = 46)**} & **One-Prompt Tree** & \\(2.26\\pm 0.42\\) & \\(-2.1,\\textbf{\\emph{{0}}4},36.5\\) & \\(1.39\\pm 0.31\\) & \\(-2.11,\\textbf{\\emph{{0}}4},41.76\\) & \\(0.35\\pm 0.15\\) & \\(-0.21,.83,43.9\\) \\\\  & **Tree** & \\(1.22\\pm 0.26\\) & \\(-2.1,\\textbf{\\emph{{0}}4},36.5\\) & \\(1.39\\pm 0.31\\) & \\(\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>