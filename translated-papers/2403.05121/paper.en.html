<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '\\(1024\\times 1024\\)[17, 5, 3]. The direct modeling on high resolution images aggravates the inference costs since every denoising step is performed on the high resolution space. To address such an issue, Luo _et al_. [14] and Sauer _et al_. [23] propose to distill diffusion models to significantly reduce the number of sampling steps. However, the generation quality tends to degrade noticeably during diffusion distillation, unless a GAN loss is introduced, which otherwise complicates the distillation and could lead to instability of training.\n' +
      '\n' +
      'In this work, we propose CogView3, a novel text-to-image generation system that employs relay diffusion [27]. Relay diffusion is a new cascaded diffusion framework, decomposing the process of generating high-resolution images into multiple stages. It first generates low-resolution images and subsequently performs relaying super-resolution generation. Unlike previous cascaded diffusion frameworks that condition every step of the super-resolution stage on low-resolution generations [9, 19, 21], relaying super-resolution adds Gaussian noise to the low-resolution generations and starts diffusion from these noised images. This enables the super-resolution stage of relay diffusion to rectify unsatisfactory artifacts produced by the previous diffusion stage. In CogView3, we apply relay diffusion in the latent image space rather than at pixel level as the original version, by utilizing a simplified linear blurring schedule and a correspondingly formulated sampler. By the iterative implementation of the super-resolution stage, CogView3 is able to generate images with extremely high resolutions such as \\(2048\\times 2048\\).\n' +
      '\n' +
      'Given that the cost of lower-resolution inference is quadratically smaller than that of higher-resolution, CogView3 can produce competitive generation results at significantly reduced inference costs by properly allocating sampling steps between the base and super-resolution stages. Our results of human evaluation show that CogView3 outperforms SDXL [17] with a win rate of 77.0%. Moreover,\n' +
      '\n' +
      'Figure 1: Showcases of CogView3 generation of resolution \\(2048\\times 2048\\)**(top)** and \\(1024\\times 1024\\)**(bottom)**. All prompts are sampled from Partiprompts [31].\n' +
      '\n' +
      ' through progressive distillation of diffusion models, CogView3 is able to produce comparable results while utilizing only 1/10 of the time required for the inference of SDXL. Our contributions can be summarized as follows:\n' +
      '\n' +
      '* We propose CogView3, the first text-to-image system in the framework of relay diffusion. CogView3 is able to generate high quality images with extremely high resolutions such as \\(2048\\times 2048\\).\n' +
      '* Based on the relaying framework, CogView3 is able to produce competitive results at a significantly reduced time cost. CogView3 achieves a win rate of 77.0% over SDXL with about 1/2 of the time during inference.\n' +
      '* We further explore the progressive distillation of CogView3, which is significantly facilitated by the relaying design. The distilled variant of CogView3 delivers comparable generation results while utilizing only 1/10 of the time required by SDXL.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '### Text-to-Image Diffusion Models\n' +
      '\n' +
      'Diffusion models, as defined by Ho _et al_. [8], establish a forward diffusion process that gradually adds Gaussian noise to corrupt real data \\(\\mathbf{x}_{0}\\) as follows:\n' +
      '\n' +
      '\\[q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t -1},\\beta_{t}\\mathbf{I}),\\ t\\in\\{1,...,T\\}, \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\beta_{t}\\) defines a noise schedule in control of diffusion progression. Conversely, the backward process generates images from pure Gaussian noise by step-by-step denoising, adhering a Markov chain.\n' +
      '\n' +
      'A neural network is trained at each time step to predict denoised results based on the current noised images. For text-to-image diffusion models, an additional text encoder encodes the text input, which is subsequently fed into the cross attention modules of the main network. The training process is implemented by optimizing the variational lower bound of the backward process, which is written as\n' +
      '\n' +
      '\\[\\mathbb{E}_{\\mathbf{x}_{0}\\sim p_{data}}\\mathbb{E}_{\\mathbf{\\epsilon}\\sim\\mathcal{N}( \\mathbf{0},\\mathbf{I}),t}\\|\\mathcal{D}(\\mathbf{x}_{0}+\\sigma_{t}\\mathbf{\\epsilon},t,c) -\\mathbf{x}_{0}\\|^{2}, \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\sigma_{t}\\) denotes the noise scale controlled by the noise schedule. \\(c\\) denotes input conditions including the text embeddings.\n' +
      '\n' +
      'Recent works [3, 17] consistently apply diffusion models to the latent space, resulting in a substantial saving of both training and inference costs. They first use a pretrained autoencoder to compress the image \\(\\mathbf{x}\\) into a latent representation \\(\\mathbf{z}\\) with lower dimension, which is approximately recoverable by its decoder. The diffusion model learns to generate latent representations of images.\n' +
      '\n' +
      '### Relay Diffusion Models\n' +
      '\n' +
      'Cascaded diffusion [9, 21] refers to a multi-stage diffusion generation framework. It first generates low-resolution images using standard diffusion and subsequently performs super-resolution. The super-resolution stage of the original cascaded diffusion conditions on low-resolution samples \\(\\mathbf{x}^{L}\\) at every diffusion step, by channel-wise concatenation of \\(\\mathbf{x}^{L}\\) with noised diffusion states. Such conditioning necessitates augmentation techniques to bridge the gap in low-resolution input between real images and base stage generations.\n' +
      '\n' +
      'As a new variant of cascaded diffusion, the super-resolution stage of relay diffusion [27] instead starts diffusion from low-resolution images \\(\\mathbf{x}^{L}\\) corrupted by Gaussian noise \\(\\sigma_{T_{r}}\\mathbf{\\epsilon}\\), where \\(T_{r}\\) denotes the starting point of the blurring schedule in the super-resolution stage. The forward process is formulated as:\n' +
      '\n' +
      '\\[q(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x}_{t}|F(\\mathbf{x}_{0},t),\\sigma_{t}{}^{ 2}\\mathbf{I}),\\quad t\\in\\{0,...,T\\}, \\tag{3}\\]\n' +
      '\n' +
      'where \\(F(\\cdot)\\) is a pre-defined transition along time \\(t\\) from high-resolution images \\(\\mathbf{x}=\\mathbf{x}_{0}\\) to the upsampled low-resolution images \\(\\mathbf{x}^{L}\\). The endpoint of \\(F\\) is set as \\(F(\\mathbf{x}_{0},T_{r})=\\mathbf{x}^{L}\\) to ensure a seamless transition. Conversely, the backward process of relaying super-resolution is a combination of denoising and deblurring.\n' +
      '\n' +
      'This design allows relay diffusion to circumvent the need for intricate augmentation techniques on lower-resolution conditions \\(\\mathbf{x}^{L}\\), as \\(\\mathbf{x}^{L}\\) is only inputted at the initial sampling step of super-resolution stage and is already corrupted by Gaussian noise \\(\\sigma_{T_{r}}\\mathbf{\\epsilon}\\). It also enables the super-resolution stage of relay diffusion to possibly rectify some unsatisfactory artifacts produced by the previous diffusion stage.\n' +
      '\n' +
      '### Diffusion Distillation\n' +
      '\n' +
      'Knowledge distillation [7] is a training process aiming to transfer a larger teacher model to the smaller student model. In the context of diffusion models, distillation has been explored as a means to reduce sampling steps thus saving computation costs of inference, while preventing significant degradation of the generation performance [22, 23, 14, 26].\n' +
      '\n' +
      'As one of the prominent paradigms in diffusion distillation, progressive distillation [22] trains the student model to match every two steps of the teacher model with a single step in each training stage. This process is repeated, progressively halving the sampling steps. On the other hand, consistency models [26, 14] propose a fine-tuning approach for existing diffusion models to project every diffusion step to the latest one to ensure step-wise consistency, which also reduces sampling steps of the model. While previous diffusion distillation methods mostly compromise on the quality of generation, adversial diffusion distillation [23] mitigates this by incorporating an additional GAN loss in the distillation. However, this makes the process of distillation more challenging due to the instability of GAN training.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Text Preprocessing\n' +
      '\n' +
      '#### 3.1.1 Image Recaption\n' +
      '\n' +
      'Following DALL-E-3 [3], we develop an automatic pipeline to re-caption images from the training dataset. While DALL-E-3 derives instruction-tuning data of the re-caption model from human labelers, we extract tripletsof <image, old_cap, new_cap> by automatically prompting GPT-4V [1], as shown in Figure 2. Generally, we prompt GPT-4V to propose several questions about the content of the uploaded image. The first question is forced to be about a brief description. Finally, we instruct the model to combine the answers together with the original caption to build a new caption.\n' +
      '\n' +
      'We collect approximately 70,000 reception triplets with this paradigm and finetune CogVLM-17B [28] by these examples to obtain a reception model. We finetune the model by a moderate degree, with batch size 256 and 1,500 steps to prevent model from severe overfitting. Eventually the model is utilized to re-caption the whole training dataset. The re-caption results provide comprehensive, graceful and detailed descriptions of images, in contrast to the original short and less relevant captions from the dataset. The prefix statement we use to prompt GPT-4V and the template we use in fine-tuning the reception model are both provided in Appendix 0.B.\n' +
      '\n' +
      '#### 3.2.2 Prompt Expansion\n' +
      '\n' +
      'On account that CogView3 is trained on datasets with comprehensive re-captions while users of text-to-image generation systems may tend to provide brief prompts lacking descriptive information, this introduces an explicit misalignment between model training and inference [3]. Therefore, we also explore to expand user prompts before sampling with the diffusion models. We prompt language models to expand user prompts into comprehensive descriptions, while encouraging the model generation to preserve the original intention from users. With human evaluation, we find results of the expanded prompts to achieve higher preference. We provide the template and showcases of our prompt expansion in Appendix 0.B.\n' +
      '\n' +
      'Figure 2: An example of re-caption data collection from GPT-4V.\n' +
      '\n' +
      '### Model Formulation\n' +
      '\n' +
      '#### 3.2.1 Model Framework\n' +
      '\n' +
      'The backbone of CogView3 is a 3-billion parameter text-to-image diffusion model with a 3-stage UNet architecture. The model operates in the latent image space, which is \\(8\\times\\) compressed from the pixel space by a variational KL-regularized autoencoder. We employ the pretrained T5-XXL [18] encoder as the text encoder to improve model\'s capacity for text understanding and instruction following, which is frozen during the training of the diffusion model. To ensure alignment between training and inference, user prompts are first rewritten by language models as mentioned in the previous section. We set the input token length for the text encoder as 225 to facilitate the implementation of the expanded prompts.\n' +
      '\n' +
      'As shown in Figure 3(left), CogView3 is implemented as a 2-stage relay diffusion. The base stage of CogView3 is a diffusion model that generates images at a resolution of \\(512\\times 512\\). The second stage model performs \\(2\\times\\) super-resolution, generating \\(1024\\times 1024\\) images from \\(512\\times 512\\) inputs. It is noteworthy that the super-resolution stage can be directly transferred to higher resolutions and iteratively applied, enabling the final outputs to reach higher resolutions such as \\(2048\\times 2048\\), as cases illustrated from the top line of Figure 1.\n' +
      '\n' +
      '#### 3.2.2 Training Pipeline\n' +
      '\n' +
      'We use Laion-2B [24] as our basic source of the training dataset, after removing images with politically-sensitive, pornographic or violent contents to ensure appropriateness and quality of the training data. The filtering\n' +
      '\n' +
      'Figure 3: **(left)** The pipeline of CogView3. User prompts are rewritten by a text-expansion language model. The base stage model generates \\(512\\times 512\\) images, and the second stage subsequently performs relaying super-resolution. **(right)** Formulation of relaying super-resolution in the latent space.\n' +
      '\n' +
      'process is executed by a pre-defined list of sub-strings to block a group of source links associated with unwanted images. In correspondence with Betker \\({\\it{et\\ al.}}\\)[3], we replace 95% of the original data captions with the newly-produced captions.\n' +
      '\n' +
      'Similar to the training approach used in SDXL [17], we train Cogview3 progressively to develop multiple stages of models. This greatly reduced the overall training cost. Owing to such a training setting, the different stages of CogView3 share a same model architecture.\n' +
      '\n' +
      'The base stage of CogView3 is trained on the image resolution of \\(256\\times 256\\) for 600,000 steps with batch size 2048 and continued to be trained on \\(512\\times 512\\) for 200,000 steps with batch size 2048. We finetune the pretrained \\(512\\times 512\\) model on a highly aesthetic internal dataset for 10,000 steps with batch size 1024, to achieve the released version of the base stage model. To train the super-resolution stage of CogView3, we train on the basis of the pretrained \\(512\\times 512\\) model on \\(1024\\times 1024\\) resolution for 100,000 steps with batch size 1024, followed by a 20,000 steps of finetuning with the loss objective of relaying super-resolution to achieve the final version.\n' +
      '\n' +
      '### Relaying Super-resolution\n' +
      '\n' +
      '#### 3.3.1 Latent Relay Diffusion\n' +
      '\n' +
      'The second stage of CogView3 performs super-resolution by relaying, starting diffusion from the results of base stage generation. While the original relay diffusion handles the task of image generation in the pixel level [27], we implement relay diffusion in the latent space and utilize a simple linear transformation instead of the original patch-wise blurring. The formulation of latent relay diffusion is illustrated by Figure 3(right). Given an image \\(\\mathbf{x}_{0}\\) and its low-resolution version \\(\\mathbf{x}^{L}=\\text{Downsample}(\\mathbf{x}_{0})\\), they are first transformed into latent space by the autoencoder as \\(\\mathbf{z}_{0}=\\mathcal{E}(\\mathbf{x}_{0}),\\ \\mathbf{z}^{L}=\\mathcal{E}(\\mathbf{x}^{L})\\). Then the linear blurring transformation is defined as:\n' +
      '\n' +
      '\\[\\mathbf{z}_{0}^{t}=\\mathcal{F}(\\mathbf{z}_{0},t)=\\frac{T_{r}-t}{T_{r}}\\mathbf{z}_{0}+\\frac{ t}{T_{r}}\\mathbf{z}^{L}, \\tag{4}\\]\n' +
      '\n' +
      'where \\(T_{r}\\) denotes the starting point set for relaying super-resolution and \\(\\mathbf{z}_{0}^{T_{r}}\\) matches exactly with \\(\\mathbf{z}^{L}\\). The forward process of the latent relay diffusion is then written as:\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{t}|\\mathbf{z}_{0})=\\mathcal{N}(\\mathbf{z}_{t}|\\mathbf{z}_{0}^{t},\\sigma_{t}^{2} \\mathbf{I}),\\ t\\in\\{1,...,T_{r}\\}. \\tag{5}\\]\n' +
      '\n' +
      'The training objective is accordingly formulated as:\n' +
      '\n' +
      '\\[\\mathbb{E}_{\\mathbf{x}_{0}\\sim p_{data}}\\mathbb{E}_{\\mathbf{\\epsilon}\\sim\\mathcal{N}( \\mathbf{0},\\mathbf{I}),t\\in\\{0,...,T_{r}\\}}\\|\\mathcal{D}(\\mathbf{z}_{0}^{t}+\\sigma _{t}\\mathbf{\\epsilon},t,c_{text})-\\mathbf{z}_{0}\\|^{2}, \\tag{6}\\]\n' +
      '\n' +
      'where \\(\\mathcal{D}\\) denotes the UNet denoiser function and \\(c_{text}\\) denotes the input text condition.\n' +
      '\n' +
      '#### 3.3.2 Sampler Formulation\n' +
      '\n' +
      'Next we introduce the sampler designed for the relaying super-resolution. Given samples \\(X^{L}\\) generated in the base stage, we bilinearly upsample \\(X^{L}\\) into \\(\\mathbf{x}^{L}\\). The starting point of relay diffusion is defined as \\(\\mathbf{z}_{0}^{T_{r}}+\\sigma_{T_{r}}\\mathbf{\\epsilon}\\), where \\(\\mathbf{\\epsilon}\\) denotes a unit isotropic Gaussian noise and \\(\\mathbf{z}_{0}^{T_{r}}=\\mathcal{E}(\\mathbf{x}^{L})\\) is the latent representation of the bilinearly-upsampled base-stage generation. Corresponding to the forward process of relaying super-resolution formulated in Equation 5, the backward process is defined in the DDIM [25] paradigm:\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{t-1}|\\mathbf{z}_{t},\\mathbf{z}_{0})=\\mathcal{N}(\\mathbf{z}_{t-1}|a_{t}\\mathbf{z}_{t}+ b_{t}\\mathbf{z}_{0}+c_{t}\\mathbf{z}_{0}^{t},\\delta_{t}^{2}\\mathbf{I}), \\tag{7}\\]\n' +
      '\n' +
      'where \\(a_{t}=\\sqrt{\\sigma_{t-1}^{2}-\\delta_{t}^{2}}/\\sigma_{t}\\), \\(b_{t}=1/t\\), \\(c_{t}=(t-1)/t-a_{t}\\), \\(\\mathbf{z}_{0}^{t}\\) is defined in Equation 4 and \\(\\delta_{t}\\) represents the random degree of the sampler. In practice, we simply set \\(\\delta_{t}\\) as 0 to be an ODE sampler. The procedure is shown in Algorithm 1. A detailed proof of the consistency between the sampler and the formulation of latent relay diffusion is shown in Appendix 0.A.\n' +
      '\n' +
      '```\n' +
      'Given \\(\\mathbf{x}^{L},\\mathbf{z}_{0}^{T_{r}}=\\mathcal{E}(\\mathbf{x}^{L})\\) \\(\\mathbf{z}_{T_{r}}=\\mathbf{z}_{0}^{T_{r}}+\\sigma_{T_{r}}\\mathbf{\\epsilon}\\)\\(\\triangleright\\) transform into the latent space and add noise for relaying for\\(t\\in\\{T_{r},\\dots,1\\}\\)do\\(\\tilde{\\mathbf{z}}_{0}=\\mathcal{D}(\\mathbf{z}_{t},t,c_{text})\\)\\(\\triangleright\\) predict \\(\\mathbf{z}_{0}\\) \\(\\mathbf{z}_{0}^{t-1}=\\mathbf{z}_{0}^{t}+(\\tilde{\\mathbf{z}}_{0}-\\mathbf{z}_{0}^{t})/t\\)\\(\\triangleright\\) linear blurring transition \\(a_{t}=\\sigma_{t-1}/\\sigma_{t},b_{t}=1/t,c_{t}=(t-1)/t-\\mathbf{a}_{t}\\)\\(\\triangleright\\) coefficient of each item \\(\\mathbf{z}_{t-1}=a_{t}\\mathbf{z}_{t}+b_{t}\\tilde{\\mathbf{z}}_{0}+c_{t}\\mathbf{z}_{0}^{t}\\)\\(\\triangleright\\) single sampling step endfor\\(\\mathbf{x}_{0}=\\text{Decode}(\\mathbf{z}_{0})\\)\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1** latent relay sampler\n' +
      '\n' +
      '### Distillation of Relay Diffusion\n' +
      '\n' +
      'We combine the method of progressive distillation [15] and the framework of relay diffusion to achieve the distilled version of CogView3. While the base stage of CogView3 performs standard diffusion, the distillation procedure follows the original implementation.\n' +
      '\n' +
      'For the super-resolution stage, we merge the blurring schedule into the diffusion distillation training, progressively halving sampling steps by matching two steps from the latent relaying sampler of the teacher model with one step of the student model. The teacher steps are formulated as:\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{z}_{t-1}&=a_{t}\\mathbf{z}_{t}+b_{t} \\tilde{\\mathbf{z}}_{0}(\\mathbf{z}_{t},t)_{teacher}+c_{t}\\mathbf{z}_{0}^{t},\\\\ \\mathbf{z}_{t-2}&=a_{t-1}\\mathbf{z}_{t-1}+b_{t-1}\\tilde{\\mathbf{ z}}_{0}(\\mathbf{z}_{t-1},t-1)_{teacher}+c_{t-1}\\mathbf{z}_{0}^{t-1},\\end{split} \\tag{8}\\]\n' +
      '\n' +
      'where \\((a_{k},b_{k},c_{k}),\\ k\\in\\{0,...,T_{r}\\}\\) refers to the item coefficients defined in Algorithm 1. One step of the student model is defined as:\n' +
      '\n' +
      '\\[\\mathbf{\\hat{z}}_{t-2}=\\frac{\\sigma_{t-2}}{\\sigma_{t}}\\mathbf{z}_{t}+\\frac{\\tilde{\\bm {z}}_{0}(\\mathbf{z}_{t},t)_{student}}{t}+(\\frac{t-2}{t}-\\frac{\\sigma_{t-2}}{ \\sigma_{t}})\\mathbf{z}_{0}^{t}. \\tag{9}\\]\n' +
      '\n' +
      'The training objective is defined as the mean square error between \\(\\mathbf{\\hat{z}}_{t-2}\\) and \\(\\mathbf{z}_{t-2}\\). Following Meng [15], we incorporate the property of the classifier-free guidance (CFG) [10] strength \\(w\\) into the diffusion model in the meantime of distillation by adding learnable projection embeddings of \\(w\\) into timestep embeddings. Instead of using an independent stage for the adaptation, we implement the incorporation at the first round of the distillation and directly condition on \\(w\\) at subsequent rounds.\n' +
      '\n' +
      'The inference costs of the low-resolution base stage are quadratically lower than the high-resolution counterparts, while it ought to be called from a complete diffusion schedule. On the other hand, the super-resolution stage starts diffusion at an intermediate point of the diffusion schedule. This greatly eases the task and reduces the potential error that could be made by diffusion distillation. Therefore, we are able to distribute final sampling steps for relaying distillation as 8 steps for the base stage and 2 steps for the super-resolution stage, or even reduce to 4 steps and 1 step respectively, which achieves both greatly-reduced inference costs and mostly-retained generation quality.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setting\n' +
      '\n' +
      'We implement a comprehensive evaluation process to demonstrate the performance of CogView3. With an overall diffusion schedule of 1000 time steps, we set the starting point of the relaying super-resolution at 500, a decision informed by a brief ablation study detailed in Section 4.4. To generate images for comparison, we sample 50 steps by the base stage of CogView3 and 10 steps by the super-resolution stage, both utilizing a classifier-free guidance [10] of 7.5, unless specified otherwise. The comparison is all conducted at the image resolution of \\(1024\\times 1024\\).\n' +
      '\n' +
      '#### 4.1.1 Dataset\n' +
      '\n' +
      'We choose a combination of image-text pair datasets and collections of prompts for comparative analysis. Among these, MS-COCO [13] is a widely applied dataset for evaluating the quality of text-to-image generation. We randomly pick a subset of 5000 image-text pairs from MS-COCO, named as COCO-5k. We also incorporate DrawBench [21] and PartiPrompts [31], two well-known sets of prompts for text-to-image evaluation. DrawBench comprises 200 challenging prompts that assess both the quality of generated samples and the alignment between images and text. In contrast, PartiPrompts contains 1632 text prompts and provides a comprehensive evaluation critique.\n' +
      '\n' +
      '#### 4.1.2 Baselines\n' +
      '\n' +
      'In our evaluation, we employ state-of-the-art open-source text-to-image models, specifically SDXL [17] and Stable Cascade [16] as our baselines. SDXL is a single-stage latent diffusion model capable of generating images at and near a resolution of \\(1024\\times 1024\\). On the other hand, Stable Cascade implements a cascaded pipeline, generating \\(16\\times 24\\times 24\\) priors at first and subsequently conditioning on the priors to produce images at a resolution of \\(1024\\times 1024\\). We sample SDXL for 50 steps and Stable Cascade for 20 and 10 steps respectively for its two stages. In all instances, we adhere to their recommended configurations of the classifier-free guidance.\n' +
      '\n' +
      '#### 4.1.3 Evaluation Metrics\n' +
      '\n' +
      'We use Aesthetic Score (Aes) [24] to evaluate the image quality of generated samples. We also adopt Human Preference Score v2 (HPS v2) [29] and ImageReward [30] to evaluate text-image alignment and human preference. Aes is obtained by an aesthetic score predictor trained from LAION datasets, neglecting alignment of prompts and images. HPS v2 and ImageReward are both used to predict human preference for images, including evaluation of text-image alignment, human aesthetic, etc. Besides machine evaluation, we also conduct human evaluation to further assess the performance of models, covering image quality and semantic accuracy.\n' +
      '\n' +
      '### Results of Machine Evaluation\n' +
      '\n' +
      'Table 1 shows results of machine metrics on DrawBench and Partiprompts. While CogView3 has the lowest inference cost, it outperforms SDXL and Stable Cascade in most of the comparisons except for a slight setback to Stable Cascade on the ImageReward of PartiPrompts. Similar results are observed from comparisons on COCO-5k, as shown in Table 2. The distilled version of CogView3 takes an extremely low inference time of 1.47s but still achieves a comparable performance. The results of the distilled variant of CogView3 significantly outperform the previous distillation paradigm of latent consistency model [14] on SDXL, as illustrated in the table.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c|c c c|c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multirow{2}{*}{Steps} & \\multirow{2}{*}{Time Cost} & \\multicolumn{4}{c|}{**DrawBench**} & \\multicolumn{4}{c}{**PartiPrompts**} \\\\ \\cline{5-8}  & & & & Aes\\(\\uparrow\\) & HPS v2\\(\\uparrow\\) & ImageReward\\(\\uparrow\\) & Aes\\(\\uparrow\\) & HPS v2\\(\\uparrow\\) & ImageReward\\(\\uparrow\\) \\\\ \\hline SDXL [17] & 50 & 19.67s & 5.54 & 0.288 & 0.676 & 5.78 & 0.287 & 0.915 \\\\ StableCascade [16] & 20+10 & 10.83s & 5.88 & 0.285 & 0.677 & 5.93 & 0.285 & **1.029** \\\\\n' +
      '**CogView3** & 50+10 & **10.33s** & **5.97** & **0.290** & **0.847** & **6.15** & **0.290** & 1.025 \\\\ \\hline LCM-SDXL [14] & 4 & 2.06s & 5.45 & 0.279 & 0.394 & 5.59 & 0.280 & 0.689 \\\\\n' +
      '**CogView3-distill** & 4+1 & **1.47s** & 5.87 & 0.288 & 0.731 & 6.12 & 0.287 & 0.968 \\\\\n' +
      '**CogView3-distill** & 8+2 & 1.96s & 5.90 & 0.285 & 0.655 & 6.13 & 0.288 & 0.963 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Results of machine metrics on DrawBench and PartiPrompts. All samples are generated on \\(1024\\times 1024\\). The time cost is measured with a batch size of 4.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multicolumn{2}{l}{**COCO-5k**} \\\\ \\hline Model & Steps & Time Cost & FID\\(\\downarrow\\) & Aes\\(\\uparrow\\) & HPS v2\\(\\uparrow\\) & ImageReward\\(\\uparrow\\) \\\\ \\hline SDXL [17] & 50 & 19.67s & **26.29** & 5.63 & 0.291 & 0.820 \\\\ StableCascade [16] & 20+10 & 10.83s & 36.59 & 5.89 & 0.283 & 0.734 \\\\\n' +
      '**CogView3** & 50+10 & **10.33s** & 31.63 & **6.01** & **0.294** & **0.967** \\\\ \\hline LCM-SDXL [14] & 4 & 2.06s & 27.16 & 5.39 & 0.281 & 0.566 \\\\\n' +
      '**CogView3-distill** & 4+1 & **1.47s** & 34.03 & 5.99 & 0.292 & 0.920 \\\\\n' +
      '**CogView3-distill** & 8+2 & 1.96s & 35.53 & 6.00 & 0.293 & 0.921 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Results of machine metrics on COCO-5k. All samples are generated on \\(1024\\times 1024\\). The time cost is measured with a batch size of 4.\n' +
      '\n' +
      'The comparison results demonstrate the performance of CogView3 for generating images of improved quality and fidelity with a remarkably reduced cost. The distillation of CogView3 succeeds in preserving most of the generation quality while reduces the sampling time to an extreme extent. We largely attribute the aforementioned comparison results to the relaying property of CogView3. In the following section, we will further demonstrate the performance of CogView3 with human evaluation.\n' +
      '\n' +
      '### Results of Human Evaluation\n' +
      '\n' +
      'We conduct human evaluation for CogView3 by having annotators perform pairwise comparisons. The human annotators are asked to provide results of win, lose or tie based on the prompt alignment and aesthetic quality of the generation. We use DrawBench [21] as the evaluation benchmark. For the generation of CogView3, we first expand the prompts from DrawBench to detailed descriptions\n' +
      '\n' +
      'Figure 4: Results of human evaluation on DrawBench generation. **(left)** Comparison results about prompt alignment, **(right)** comparison results about aesthetic quality. “(expanded)” indicates that prompts used for generation is text-expanded.\n' +
      '\n' +
      'Figure 5: Results of human evaluation on Drawbench generation for distilled models. **(left)** Comparison results about prompt alignment, **(right)** comparison results about aesthetic quality. “(expanded)” indicates that prompts used for generation is text-expanded. We sample 8+2 steps for CogView3-distill and 4 steps for LCM-SDXL.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      'Alignment Improvement with Text Expansion\n' +
      '\n' +
      'While prompt expansion hardly brings an improvement for the generation of SDXL and Stable Cascade, we highlight its significance for the performance of CogView3. Figure 7 shows the results of comparison with and without prompt expansion, explicitly demonstrating that prompt expansion significantly enhances the ability of prompt instruction following for CogView3. Figure 8 shows qualitative comparison between before and after the prompt expansion. The expanded prompts provide more comprehensive and in-distribution descriptions for model generation, largely improving the accuracy of instruction following for CogView3. Similar improvement is not observed on the generation of SDXL. The probable reason may be that SDXL is trained on original captions and only has an input window of 77 tokens, which leads to frequent truncation of the expanded prompts. This corroborates the statement in Section 3.1 that prompt expansion helps bridge the gap between model inference and training with re-captioned data.\n' +
      '\n' +
      'Figure 8: Comparison of the effect of prompt expansion for CogView3 and SDXL.\n' +
      '\n' +
      'Figure 7: Human evaluation results of CogView3 before and after prompt expansion on DrawBench.\n' +
      '\n' +
      '#### 4.2.3 Methods of Iterative Super-Resolution\n' +
      '\n' +
      'Although straightforward implementation of the super-resolution stage model on higher image resolutions achieves desired outputs, this introduces excessive requirements of the CUDA memory, which is unbearable on the resolution of \\(4096\\times 4096\\). Tiled diffusion [2][11] is a series of inference methods for diffusion models tackling such an issue. It separates an inference step of large images into overlapped smaller blocks and mix them together to obtain the overall prediction of the step. As shown in Figure 9, comparable results can be achieved with tiled inference. This enables CogView3 to generate images with higher resolution by a limited CUDA memory usage. It is also possible to generate \\(4096\\times 4096\\) images with tiled methods, which we leave for future work.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'In this work, we propose CogView3, the first text-to-image generation system in the framework of relay diffusion. CogView3 achieves preferred generation quality with greatly reduced inference costs, largely attributed to the relaying pipeline. By iteratively implementing the super-resolution stage of CogView3, we are able to achieve high quality images of extremely high resolution as \\(2048\\times 2048\\).\n' +
      '\n' +
      'Meanwhile, with the incorporation of data re-captioning and prompt expansion into the model pipeline, CogView3 achieves better performance in prompt understanding and instruction following compared to current state-of-the-art open-source text-to-image diffusion models.\n' +
      '\n' +
      'We also explore the distillation of CogView3 and demonstrate its simplicity and capability attributed to the framework of relay diffusion. Utilizing the progressive distillation paradigm, the distilled variant of CogView3 reduces the inference time drastically while still preserves a comparable performance.\n' +
      '\n' +
      'Figure 9: Comparison of direct higher super-resolution and tiled diffusion on \\(2048\\times 2048\\). We choose Mixture of Diffusers [11] in view of its superior quality of integration. Original prompts are utilized for the inference of all blocks.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]A. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. (2023) Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Cited by: SS1.\n' +
      '* [2]O. Bar-Tal, L. Yariv, Y. Lipman, and T. Dekel (2023) Multidiffusion: fusing diffusion paths for controlled image generation. Cited by: SS1.\n' +
      '* [3]J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. (2023) Improving image generation with better captions. Computer Science. External Links: Link Cited by: SS1.\n' +
      '* [4]C. Bishop and N. M. Nasrabadi (2006) Pattern recognition and machine learning. Vol. 4, Springer. Cited by: SS1.\n' +
      '* [5]X. Dai, J. Hou, C. Y. Ma, S. Tsai, J. Wang, R. Wang, P. Zhang, S. Vandenhende, X. Wang, A. Dubey, et al. (2023) EMU: enhancing image generation models using photogenic needles in a haystack. arXiv preprint arXiv:2309.15807. Cited by: SS1.\n' +
      '* [6]M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou, Z. Shao, H. Yang, et al. (2021) Cogview: mastering text-to-image generation via transformers. Advances in Neural Information Processing Systems34, pp. 19822-19835. Cited by: SS1.\n' +
      '* [7]G. Hinton, O. Vinyals, and J. Dean (2015) Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531. Cited by: SS1.\n' +
      '* [8]J. Ho, A. Jain, and P. Abbeel (2020) Denoising diffusion probabilistic models. Advances in neural information processing systems33, pp. 6840-6851. Cited by: SS1.\n' +
      '* [9]J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans (2022) Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning Research23 (1), pp. 2249-2281. Cited by: SS1.\n' +
      '* [10]J. Ho and T. Salimans (2022) Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598. Cited by: SS1.\n' +
      '* [11]A. B. Jimenez (2023) Mixture of diffusers for scene composition and high resolution image generation. arXiv preprint arXiv:2302.02412. Cited by: SS1.\n' +
      '* [12]M. Kang, J. Y. Zhu, R. Zhang, J. Park, E. Shechtman, S. Paris, and T. Park (2023) Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10124-10134. Cited by: SS1.\n' +
      '* [13]T. Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick (2014) Microsoft coco: common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740-755. Cited by: SS1.\n' +
      '* [14]S. Luo, Y. Tan, L. Huang, J. Li, and H. Zhao (2023) Latent consistency models: synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378. Cited by: SS1.\n' +
      '* [15]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [16]P. Pernias, D. Rampas, M. L. Richter, C. J. Pal, and M. Aubreville (2023) Wuerstchen: an efficient architecture for large-scale text-to-image diffusion models. Cited by: SS1.\n' +
      '* [17]D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Muller, J. Penna, and R. Rombach (2023) Sdxl: improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952. Cited by: SS1.\n' +
      '* [18]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS1.\n' +
      '* [19]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [20]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [21]P. Pernias, D. Rampas, M. L. Richter, C. J. Pal, and M. Aubreville (2023) Wuerstchen: an efficient architecture for large-scale text-to-image diffusion models. Cited by: SS1.\n' +
      '* [22]D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Muller, J. Penna, and R. Rombach (2023) Sdxl: improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952. Cited by: SS1.\n' +
      '* [23]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [24]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [25]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [26]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [27]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [28]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [29]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [30]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [31]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [32]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [33]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [34]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [35]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [36]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [37]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [38]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [39]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [40]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [41]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '* [42]C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans (2023) On* [19] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 **1**(2), 3 (2022)\n' +
      '* [20] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: Zero-shot text-to-image generation. In: International Conference on Machine Learning. pp. 8821-8831. PMLR (2021)\n' +
      '* [21] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems **35**, 36479-36494 (2022)\n' +
      '* [22] Salimans, T., Ho, J.: Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512 (2022)\n' +
      '* [23] Sauer, A., Lorenz, D., Blattmann, A., Rombach, R.: Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042 (2023)\n' +
      '* [24] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.: Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems **35**, 25278-25294 (2022)\n' +
      '* [25] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020)\n' +
      '* [26] Song, Y., Dhariwal, P., Chen, M., Sutskever, I.: Consistency models (2023)\n' +
      '* [27] Teng, J., Zheng, W., Ding, M., Hong, W., Wangni, J., Yang, Z., Tang, J.: Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350 (2023)\n' +
      '* [28] Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 (2023)\n' +
      '* [29] Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., Li, H.: Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341 (2023)\n' +
      '* [30] Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., Dong, Y.: Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [31] Yu, J., Xu, Y., Koh, J.Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B.K., et al.: Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789 **2**(3), 5 (2022)\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:18]\n' +
      '\n' +
      'Figure 11: Interface showcases of the human evaluation. The original prompts is translated to Chinese, the mother language of our human annotators, for evaluation.\n' +
      '\n' +
      'Figure 10: Examples of the recaption model results.\n' +
      '\n' +
      '## Appendix 0.D Additional Qualitative Comparisons\n' +
      '\n' +
      '### Qualitative model Comparisons\n' +
      '\n' +
      'Figure 12: Qualitative comparisons of CogView3 with SDXL, Stable Cascade and DALL-E 3. All prompts are sampled from Partiprompts.\n' +
      '\n' +
      '### Qualitative comparisons Between Distilled Models\n' +
      '\n' +
      'Figure 13: Qualitative comparisons of CogView3-distill with LCM-SDXL, recent model of diffusion distillation capable of generating \\(1024\\times 1024\\) samples. The first column shows samples from the original version of CogView3.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>