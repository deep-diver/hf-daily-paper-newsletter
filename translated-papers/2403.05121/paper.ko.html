<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '(1024\\times 1024\\)[17, 5, 3]. 고해상도 영상에 대한 직접 모델링은 모든 잡음 제거 단계가 고해상도 공간에서 수행되기 때문에 추론 비용을 가중시킨다. 이러한 문제를 해결하기 위해 Luo _et al_. [14] 및 Sauer _et al_. [23] 샘플링 단계의 수를 크게 줄이기 위해 확산 모델을 증류하는 것을 제안한다. 그러나 GAN 손실이 도입되지 않는 한 확산 증류 동안 생성 품질이 눈에 띄게 저하되는 경향이 있으며, 그렇지 않으면 증류가 복잡해지고 훈련의 불안정성으로 이어질 수 있다.\n' +
      '\n' +
      '본 논문에서는 릴레이 확산을 이용한 새로운 텍스트-이미지 생성 시스템인 CogView3를 제안한다[27]. 릴레이 확산은 고해상도 이미지를 생성하는 과정을 다단계로 분해하는 새로운 캐스케이드 확산 프레임워크이다. 먼저 저해상도 영상을 생성하고 이후 슈퍼 레졸루션 생성을 중계한다. 초해상도 단계의 모든 단계를 저해상도 세대[9, 19, 21]에서 조정하는 이전의 캐스케이드 확산 프레임워크와 달리, 초해상도를 중계하는 것은 저해상도 세대에 가우시안 잡음을 추가하고 이러한 잡음 영상으로부터 확산을 시작한다. 이는 릴레이 확산의 초해상도 단계가 이전 확산 단계에서 생성된 불만족스러운 아티팩트를 정정할 수 있게 한다. CogView3에서는 단순화된 선형 블러링 스케줄과 이에 대응하여 공식화된 샘플러를 활용하여 원본 버전으로 픽셀 수준이 아닌 잠재 이미지 공간에서 릴레이 확산을 적용한다. 초해상도 단계를 반복적으로 구현함으로써 CogView3는 \\(2048\\times 2048\\)과 같은 매우 높은 해상도의 영상을 생성할 수 있다.\n' +
      '\n' +
      '낮은 해상도의 추론 비용이 높은 해상도의 추론 비용보다 2차적으로 작다는 점을 감안할 때, CogView3는 기본 단계와 초해상도 단계 사이에 샘플링 단계를 적절하게 할당함으로써 상당히 감소된 추론 비용으로 경쟁력 있는 생성 결과를 생성할 수 있다. 우리의 인간 평가 결과는 CogView3가 77.0%의 승률로 SDXL[17]보다 우수함을 보여준다. 또한,\n' +
      '\n' +
      '그림 1: 해상도 \\(2048\\times 2048\\)**(위)** 및 \\(1024\\times 1024\\)**(아래)**의 CogView3 생성의 쇼케이스. 모든 프롬프트는 Partiprompt[31]에서 샘플링됩니다.\n' +
      '\n' +
      ' 확산 모델의 점진적인 증류를 통해 CogView3는 SDXL 추론에 필요한 시간의 1/10만 활용하면서 유사한 결과를 생성할 수 있다. 우리의 기여는 다음과 같이 요약할 수 있다:\n' +
      '\n' +
      '* 중계 확산의 틀에서 첫 번째 텍스트 대 이미지 시스템인 CogView3를 제안한다. CogView3는 \\(2048\\times 2048\\)과 같은 매우 높은 해상도로 고품질의 이미지를 생성할 수 있다.\n' +
      '* 중계 프레임워크를 기반으로 CogView3는 상당히 감소된 시간 비용으로 경쟁력 있는 결과를 낼 수 있다. CogView3는 추론 시 약 1/2의 시간으로 SDXL에 대해 77.0%의 승률을 달성한다.\n' +
      '* 중계 설계에 의해 상당히 촉진되는 CogView3의 점진적인 증류를 추가로 탐구한다. CogView3의 증류된 변종은 SDXL이 필요로 하는 시간의 1/10만 활용하면서 유사한 생성 결과를 제공한다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '### 텍스트-이미지 확산 모델\n' +
      '\n' +
      'Ho _et al_. [8]에 의해 정의된 바와 같은 확산 모델들. , 다음과 같이 손상된 실제 데이터 \\(\\mathbf{x}_{0}\\)에 가우스 잡음을 점진적으로 추가하는 순방향 확산 프로세스를 확립한다:\n' +
      '\n' +
      '[q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t -1},\\beta_{t}\\mathbf{I}),\\t\\in\\{1,...,T\\}, \\tag{1}\\mathbf{n}(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})\n' +
      '\n' +
      '여기서 \\(\\beta_{t}\\)는 확산 진행의 제어에서 잡음 스케줄을 정의한다. 반대로, 백워드 프로세스는 마르코프 체인을 접착하는 단계적 디노이징에 의해 순수한 가우시안 잡음으로부터 이미지들을 생성한다.\n' +
      '\n' +
      '뉴럴 네트워크는 현재 잡음화된 이미지들에 기초하여 잡음 제거된 결과들을 예측하기 위해 매 시간 단계마다 트레이닝된다. 텍스트 대 이미지 확산 모델의 경우, 추가 텍스트 인코더가 텍스트 입력을 인코딩하고, 이는 후속적으로 메인 네트워크의 크로스 어텐션 모듈에 공급된다. 상기 트레이닝 프로세스는 백워드 프로세스의 변량 하한을 최적화함으로써 구현되며, 이는 다음과 같이 쓰여진다.\n' +
      '\n' +
      '\\mathbbb{E}_{\\mathbf{x}_{0}\\sim p_{data}\\mathbbb{E}_{\\mathbf{\\epsilon}\\sim\\mathcal{N}( \\mathbf{0},\\mathbf{I}),t}\\|\\mathcal{D}(\\mathbf{x}_{0}+\\sigma_{t}\\mathbf{\\epsilon},t,c)-\\mathbf{x}_{0}\\|^{2}, \\tag{2}\\mathbbbb{E}_{\\mathbf{0}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}),t}\\|\\mathcal{D}(\\mathbf{x}_{0}+\\sigma_{t}\\mathbf{\\epsilon},t,c)-\\mathbf{x}_{0}\\|^{2},\\tag{2}\\mathbf{x}_\n' +
      '\n' +
      '여기서 \\(\\sigma_{t}\\)는 잡음 스케줄에 의해 제어되는 잡음 스케일을 나타낸다. \\ (c\\)는 텍스트 임베딩들을 포함하는 입력 조건들을 나타낸다.\n' +
      '\n' +
      '최근 연구 [3, 17]은 잠재 공간에 확산 모델을 일관되게 적용하여 훈련 비용과 추론 비용을 크게 절감한다. 그들은 먼저 미리 훈련된 오토인코더를 사용하여 이미지\\(\\mathbf{x}\\)를 낮은 차원으로 잠재 표현\\(\\mathbf{z}\\)으로 압축하는데, 이는 디코더에 의해 대략적으로 복구될 수 있다. 확산 모델은 이미지들의 잠재 표현들을 생성하도록 학습한다.\n' +
      '\n' +
      '### 릴레이 확산 모델\n' +
      '\n' +
      '캐스케이드 확산[9, 21]은 다단계 확산 생성 프레임워크를 의미한다. 그것은 먼저 표준 확산을 사용하여 저해상도 이미지를 생성하고 후속적으로 초해상도를 수행한다. 확산단계마다 저분해능 샘플 \\(\\mathbf{x}^{L}\\)에 대한 원래의 캐스케이드 확산 조건의 초분해능 단계는 잡음 확산 상태와 \\(\\mathbf{x}^{L}\\)의 채널별 연접에 의해 이루어진다. 이러한 컨디셔닝은 실제 이미지와 베이스 스테이지 세대 사이의 저해상도 입력의 갭을 메우기 위한 증강 기술을 필요로 한다.\n' +
      '\n' +
      '캐스케이드 확산의 새로운 변형으로서 릴레이 확산의 초해상도 단계[27]는 대신 가우시안 잡음에 의해 손상된 저해상도 이미지\\(\\sigma_{T_{r}\\mathbf{\\epsilon}\\)로부터 확산을 시작하며, 여기서 \\(T_{r}\\)은 초해상도 단계에서 블러링 스케줄의 시작점을 나타낸다. 전진 공정은 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\mathccal{N}(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x}_{t}|F(\\mathbf{x}_{0},t),\\sigma_{t}{}^{2}\\mathbf{I}),\\quad t\\in\\{0,...,T\\}, \\tag{3}\\t.\n' +
      '\n' +
      '여기서 \\(\\cdot)\\)은 고해상도 이미지 \\(\\mathbf{x}=\\mathbf{x}_{0}\\)에서 업샘플링된 저해상도 이미지 \\(\\mathbf{x}^{L}\\)으로 시간 \\(t\\)에 따라 미리 정의된 전이이다. \\(F\\)의 끝점은 끊김 없는 전이를 보장하기 위해 \\(F(\\mathbf{x}_{0},T_{r})=\\mathbf{x}^{L}\\)로 설정된다. 반대로, 슈퍼-레졸루션을 중계하는 백워드 프로세스는 잡음 제거와 디블러링의 조합이다.\n' +
      '\n' +
      '이 설계는 초해상도 단계의 초기 샘플링 단계에서만 입력되고 가우시안 잡음(\\sigma_{T_{r}\\mathbf{\\epsilon}\\sigma_{T_{r}\\mathbf{\\epsilon}\\sigma_{r}\\mathbf{\\epsilon}\\sigma_{r}\\mathbf{\\epsilon}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\sigma_{r}\\s 또한 릴레이 확산의 초해상도 단계를 통해 이전 확산 단계에서 생성된 일부 불만족스러운 아티팩트를 보정할 수 있다.\n' +
      '\n' +
      '### Diffusion Distillation\n' +
      '\n' +
      '지식 증류[7]은 더 큰 교사 모델을 더 작은 학생 모델로 옮기는 것을 목표로 하는 훈련 과정이다. 확산 모델의 맥락에서 증류는 샘플링 단계를 줄여 추론 계산 비용을 절약하면서 생성 성능의 상당한 저하를 방지하기 위한 수단으로 탐구되었다[22, 23, 14, 26].\n' +
      '\n' +
      '확산증류의 두드러진 패러다임 중 하나로서, 점진증류[22]는 학생 모델을 각 훈련 단계에서 단일 단계로 교사 모델의 두 단계마다 일치하도록 훈련시킨다. 이 과정을 반복하여 샘플링 단계를 점진적으로 절반으로 줄인다. 한편, 일관성 모델[26, 14]은 모든 확산 단계를 최신 단계로 투영하여 단계별 일관성을 보장하는 기존 확산 모델에 대한 미세 조정 접근법을 제안하며, 이는 또한 모델의 샘플링 단계를 감소시킨다. 이전의 확산 증류 방법은 대부분 생성 품질에 대해 타협하지만, 적대적 확산 증류[23]은 증류에 추가 GAN 손실을 통합함으로써 이를 완화한다. 그러나 이것은 GAN 훈련의 불안정성으로 인해 증류 과정을 더 어렵게 만든다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Text Preprocessing\n' +
      '\n' +
      '###### 3.1.1 이미지 재적응\n' +
      '\n' +
      'DALL-E-3 [3]에 이어서, 학습 데이터 세트로부터 이미지들을 재캡션하기 위한 자동 파이프라인을 개발한다. DALL-E-3은 인간 라벨러로부터 재-캡션 모델의 명령어-튜닝 데이터를 도출하는 동안, 그림 2와 같이 GPT-4V[1]를 자동으로 프롬프트함으로써 트리플tsof <이미지, old_cap, new_cap>을 추출한다. 일반적으로, GPT-4V가 업로드된 이미지의 내용에 대한 몇 가지 질문을 제안하도록 프롬프트한다. 첫 번째 질문은 간단한 설명에 대한 것일 수 밖에 없다. 마지막으로, 우리는 새로운 캡션을 구축하기 위해 원래 캡션과 함께 답변을 결합하도록 모델에 지시한다.\n' +
      '\n' +
      '우리는 이러한 패러다임을 가진 약 70,000개의 수신 트리플렛을 수집하고 이러한 예제에 의해 CogVLM-17B[28]를 세분화하여 수신 모델을 얻는다. 모델이 심하게 과적합되는 것을 방지하기 위해 배치 크기 256 및 1,500 단계로 모델을 중간 정도 미세 조정한다. 결국 이 모델은 전체 학습 데이터 세트를 재선택하는 데 사용된다. 재캡션 결과는 데이터 세트의 원래 짧고 관련성이 낮은 캡션과 대조적으로 이미지에 대한 포괄적이고 우아하며 상세한 설명을 제공한다. GPT-4V를 프롬프트하기 위해 사용하는 접두사 문과 수신 모델을 미세 조정할 때 사용하는 템플릿은 모두 부록 0.B에 제공된다.\n' +
      '\n' +
      '######3.2.2 프롬프트 확장\n' +
      '\n' +
      'CogView3는 포괄적인 재캡션을 갖는 데이터 세트에 대해 트레이닝되는 반면 텍스트-이미지 생성 시스템의 사용자는 기술 정보가 결여된 간략한 프롬프트를 제공하는 경향이 있을 수 있기 때문에, 이는 모델 트레이닝과 추론 사이의 명백한 오정렬을 도입한다[3]. 따라서 확산 모델로 샘플링하기 전에 사용자 프롬프트를 확장하기 위해 탐색한다. 우리는 사용자 프롬프트를 포괄적인 설명으로 확장하도록 언어 모델을 프롬프트하는 동시에 모델 생성을 장려하여 사용자의 원래 의도를 보존한다. 인간의 평가를 통해, 우리는 더 높은 선호도를 얻기 위해 확장된 프롬프트의 결과를 찾는다. 부록 0.B에서 신속한 확장의 템플릿과 쇼케이스를 제공합니다.\n' +
      '\n' +
      '도 2: GPT-4V로부터의 재-캡션 데이터 수집의 일례.\n' +
      '\n' +
      '### Model Formulation\n' +
      '\n' +
      '###### 3.2.1 모델 프레임워크\n' +
      '\n' +
      'CogView3의 백본은 3단계 UNet 아키텍처를 가진 30억 매개 변수 텍스트 대 이미지 확산 모델이다. 이 모델은 변분 KL-정규 오토인코더에 의해 픽셀 공간으로부터 \\(8\\times\\) 압축된 잠상 공간에서 동작한다. 확산 모델의 학습 중에 동결된 텍스트 이해 및 명령어 후속을 위한 모델의 용량을 향상시키기 위해 사전 훈련된 T5-XXL[18] 인코더를 텍스트 인코더로 사용한다. 훈련과 추론 사이의 정렬을 보장하기 위해, 사용자 프롬프트는 이전 섹션에서 언급된 바와 같이 언어 모델에 의해 먼저 재작성된다. 확장된 프롬프트의 구현을 용이하게 하기 위해 텍스트 인코더에 대한 입력 토큰 길이를 225로 설정했다.\n' +
      '\n' +
      '그림 3(왼쪽)과 같이 CogView3는 2단 릴레이 확산으로 구현된다. CogView3의 기본 단계는 \\(512\\times 512\\)의 해상도로 영상을 생성하는 확산 모델이다. 두 번째 단계 모델은 \\(512\\times 512\\) 입력으로부터 \\(1024\\times 1024\\)의 영상을 생성하여 \\(2\\times\\)의 초해상도를 수행한다. 초해상도 스테이지는 그림 1의 상단 라인에서 예시된 사례와 같이 더 높은 해상도로 직접 전달되고 반복적으로 적용되어 최종 출력이 \\(2048\\times 2048\\)과 같은 더 높은 해상도에 도달할 수 있다는 점은 주목할 만하다.\n' +
      '\n' +
      '###### 3.2.2 훈련 파이프라인\n' +
      '\n' +
      '우리는 훈련 데이터의 적절성과 품질을 보장하기 위해 정치적으로 민감한, 음란하거나 폭력적인 콘텐츠가 있는 이미지를 제거한 후 훈련 데이터 세트의 기본 소스로 Laion-2B[24]를 사용한다. 상기 필터링하는 단계는\n' +
      '\n' +
      '도 3: **(왼쪽)** CogView3의 파이프라인. 사용자 프롬프트는 텍스트-확장 언어 모델에 의해 재작성된다. 기본 단계 모델은 \\(512\\times 512\\) 이미지를 생성하고, 두 번째 단계는 후속적으로 초해상도를 중계한다. **(오른쪽)** 잠재공간에서 초해상도를 중계하는 형식.\n' +
      '\n' +
      '프로세스는 원하지 않는 이미지와 연관된 소스 링크 그룹을 차단하기 위해 미리 정의된 하위 문자열 목록에 의해 실행된다. 베커({\\it{et\\ al.}}\\)[3]에 대응시켜 원본 데이터 캡션의 95%를 새로 생성된 캡션으로 대체한다.\n' +
      '\n' +
      'SDXL[17]에서 사용된 훈련 접근법과 유사하게, 우리는 여러 단계의 모델을 개발하기 위해 Cogview3를 점진적으로 훈련한다. 이로 인해 전체 교육 비용이 크게 절감되었다. 이러한 훈련 설정으로 인해, CogView3의 상이한 스테이지들은 동일한 모델 아키텍처를 공유한다.\n' +
      '\n' +
      'CogView3의 기본 단계는 배치 크기 2048에서 600,000 단계에 대한 이미지 해상도(256\\times 256\\)를 학습하고 배치 크기 2048에서 200,000 단계에 대한 이미지 해상도(512\\times 512\\)를 계속 학습한다. 본 논문에서는 배치 크기 1024에서 10,000 단계에 대한 심미성이 높은 내부 데이터셋에서 미리 학습된 (512\\times 512\\) 모델을 세분화하여 기본 단계 모델의 릴리즈 버전을 달성한다. CogView3의 초해상도 단계를 훈련하기 위해, 배치 크기 1024에 대해 100,000 단계의 해상도에 대해 미리 훈련된 \\(512\\times 512\\) 모델을 기반으로 훈련하고, 최종 버전을 달성하기 위해 초해상도를 중계하는 손실 목적을 가지고 20,000 단계의 미세조정을 수행한다.\n' +
      '\n' +
      '### Relaying Super-resolution\n' +
      '\n' +
      '1 잠재 릴레이 확산\n' +
      '\n' +
      'CogView3의 두 번째 단계는 베이스 스테이지 생성 결과로부터 확산을 시작하는 릴레이로 초해상화를 수행한다. 원래의 릴레이 확산은 픽셀 수준에서 이미지 생성의 작업을 처리하지만[27], 잠재 공간에서 릴레이 확산을 구현하고 원래의 패치 방향 블러링 대신 간단한 선형 변환을 활용한다. 잠재 릴레이 확산의 공식화는 그림 3(오른쪽)에 나와 있다. 영상\\(\\mathbf{x}_{0}\\)과 저해상도 버전\\(\\mathbf{x}^{L}=\\text{Downsample}(\\mathbf{x}_{0})\\)이 주어지면, 이들은 먼저 오토인코더에 의해 \\(\\mathbf{z}_{0}=\\mathcal{E}(\\mathbf{x}_{0}),\\\\mathbf{z}^{L}=\\mathcal{E}(\\mathbf{x}^{L})로 변환된다. 그런 다음 선형 블러링 변환은 다음과 같이 정의된다.\n' +
      '\n' +
      '\\mathbf{z}_{0}^{t}=\\mathcal{F}(\\mathbf{z}_{0},t)=\\frac{T_{r}-t}{T_{r}}\\mathbf{z}_{0}+\\frac{t}{T_{r}}\\mathbf{z}^{L}, \\tag{4}\\frac{T_{r}}\n' +
      '\n' +
      '여기서 \\(T_{r}\\)은 초해상도를 중계하기 위한 시작점 집합을 나타내며 \\(\\mathbf{z}_{0}^{T_{r}\\)는 \\(\\mathbf{z}^{L}\\)과 정확히 일치한다. 그 후 잠재 릴레이 확산의 순방향 프로세스는 다음과 같이 기록된다:\n' +
      '\n' +
      '\\mathbf{z}_{t}|\\mathbf{z}_{0})=\\mathcal{N}(\\mathbf{z}_{t}|\\mathbf{z}_{0}^{t},\\sigma_{t}^{2} \\mathbf{I}),\\t\\in\\{1,...,T_{r}\\}. \\tag{5}\\}\n' +
      '\n' +
      '상기 훈련 목표는 이에 따라 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\mathbbbb{E}_{\\mathbf{x}_{0}\\sim p_{data}\\mathbbb{E}_{\\mathbf{\\epsilon}\\sim\\mathcal{N}( \\mathbf{0},\\mathbf{I}),t\\in\\{0,...,T_{r}\\|\\mathcal{D}(\\mathbf{z}_{0}^{t}+\\sigma _{t}\\mathbf{z}_{0}\\epsilon},t,c_{text})-\\mathbf{z}_{0}\\|^{2}, \\tag{6}\\t}\n' +
      '\n' +
      '여기서 \\(\\mathcal{D}\\)는 UNet denoiser 함수를 나타내고 \\(c_{text}\\)는 입력 텍스트 조건을 나타낸다.\n' +
      '\n' +
      '###### 3.3.2 샘플러 제형\n' +
      '\n' +
      '다음으로 중계 초해상도를 위해 설계된 샘플러를 소개한다. 기본 단계에서 생성된 샘플\\(X^{L}\\)이 주어지면, 우리는 이선형 업샘플\\(X^{L}\\)을 \\(\\mathbf{x^{L}\\)으로 만든다. 릴레이 확산의 시작점은 \\(\\mathbf{z}_{0}^{T_{r}+\\sigma_{T_{r}\\mathbf{\\epsilon}\\)으로 정의되며, 여기서 \\(\\mathbf{\\epsilon}\\)은 단위 등방성 가우시안 잡음을 나타내고 \\(\\mathbf{z}_{0}^{T_{r}=\\mathcal{E}(\\mathbf{x}^{L})\\)는 쌍선형 업샘플링된 기저단 생성의 잠재 표현이다. 수학식 5에서 공식화된 슈퍼-레졸루션을 중계하는 순방향 프로세스에 대응하여, 역방향 프로세스는 DDIM [25] 패러다임에서 정의된다:\n' +
      '\n' +
      '\\mathbf{z}_{t-1}|\\mathbf{z}_{t},\\mathbf{z}_{z}_{t},\\mathbf{z}_{0})=\\mathcal{N}(\\mathbf{z}_{t-1}|a_{t}\\mathbf{z}_{t}+b_{t}\\mathbf{z}_{0}+c_{t}\\mathbf{z}_{0}^{t},\\delta_{t}^{2}\\mathbf{I}), \\tag{7}\\mathbf{n}(\\mathbf{z}_{t-1}|a_{t}\\mathbf{z}_{t}+b_{t}\\mathbf{z}_{0}+c_{t}\\mathbf{z}_{0}^{t},\\delta_{t}^{2}\\mathbf{I},\\tag{7}\n' +
      '\n' +
      '여기서 \\(a_{t}=\\sqrt{\\sigma_{t-1}^{2}-\\delta_{t}^{2}/\\sigma_{t}\\), \\(b_{t}=1/t\\), \\(c_{t}=(t-1)/t-a_{t}\\), \\(\\mathbf{z}_{0}^{t}\\)는 수학식 4에 정의되고 \\(\\delta_{t}\\)는 샘플러의 랜덤도를 나타낸다. 실제로 우리는 ODE 샘플러로 \\(\\delta_{t}\\)을 0으로 간단히 설정했다. 그 절차는 알고리즘 1에 나와 있으며, 샘플러와 잠재 릴레이 확산의 공식 사이의 일관성에 대한 자세한 증명은 부록 0.A에 나와 있다.\n' +
      '\n' +
      '```\n' +
      '각 항목들의 \\(\\mathbf{z}_{t-1}=a_{t}_{t}+(\\t-mathbf{z}_{t})\\(\\triangleright\\) 예측 \\(\\mathbf{z}_{z}_{t}+(\\mathbf{z}_{t})\\(\\mathbf{z}_{z}_{t}+(\\mathbf{z}_{t})\\(\\triangleright\\)\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 잠재 릴레이 샘플러\n' +
      '\n' +
      '### 릴레이 확산 증류\n' +
      '\n' +
      'CogView3의 증류 버전을 얻기 위해 점진적 증류 방법[15]과 릴레이 확산의 프레임워크를 결합하며, CogView3의 기본 단계는 표준 확산을 수행하지만 증류 절차는 원래 구현을 따른다.\n' +
      '\n' +
      '초해상도 단계에서는 블러링 스케줄을 확산 증류 훈련에 병합하여 교사 모델의 잠재 릴레이 샘플러에서 두 단계를 학생 모델의 한 단계와 일치시켜 샘플링 단계를 점진적으로 절반으로 줄인다. 상기 교사 단계들은 다음과 같이 공식화된다:\n' +
      '\n' +
      'mmathbf{z}_{t-1}&=a_{t}\\mathbf{z}_{t}+b_{t}\\tilde{\\mathbf{z}_{0}(\\mathbf{z}_{t},t}_{teacher}+c_{t}\\mathbf{z}_{0}^{t},\\\\mathbf{z}_{t-2}&=a_{t-1}\\mathbf{z}_{t-1}+b_{t-1}\\tilde{0}(\\mathbf{z}_{t-1},t-1}\\mathbf{z}_{0}^{t-1},\\end{split}\\tag{8}\\mathbf{z}+c_{t-2}\\mathbf{z}_{t-1}\\mathbf{z}_{0}{t-1},\\end{split}\\tag{8}\\mathbf{\n' +
      '\n' +
      '여기서 \\((a_{k},b_{k},c_{k}),\\k\\in\\{0,...,T_{r}\\})는 알고리즘 1에서 정의된 항목 계수를 의미한다. 학생 모델의 한 단계는 다음과 같이 정의된다:\n' +
      '\n' +
      '[\\mathbf{\\hat{z}_{t-2}=\\frac{\\sigma_{t-2}{\\sigma_{t}}\\frac{\\sigma_{z}_{t}+\\frac{\\tilde{\\bm{z}_{0}(\\mathbf{z}_{t},t)_{student}{t}+(\\frac{t-2}{t}-\\frac{\\sigma_{t-2}}{\\sigma_{t}}\\mathbff{z}_{0}^{t}.\\tag{9}\\t}\n' +
      '\n' +
      '훈련 목표는 \\(\\mathbf{\\hat{z}_{t-2}\\)와 \\(\\mathbf{z}_{t-2}\\) 사이의 평균 제곱 오차로 정의된다. Meng[15]에 이어서, 우리는 증류 동안 타임스텝 임베딩에 학습 가능한 프로젝션 임베딩을 추가하여 분류기 없는 안내(CFG) [10] 강도\\(w\\)의 특성을 확산 모델에 통합한다. 적응을 위해 독립적인 단계를 사용하는 대신 증류의 첫 번째 라운드에서 통합을 구현하고 후속 라운드에서 \\(w\\)에 직접 조건을 설정한다.\n' +
      '\n' +
      '저해상도 기반 단계의 추론 비용은 고해상도 기반 단계보다 2차적으로 낮지만 완전한 확산 일정에서 호출해야 한다. 한편, 초해상도 스테이지는 확산 스케줄의 중간 지점에서 확산을 시작한다. 이는 작업을 크게 완화하고 확산 증류에 의해 이루어질 수 있는 잠재적인 오류를 감소시킨다. 따라서 증류를 중계하기 위한 최종 샘플링 단계를 기본 단계는 8단계, 초해상도 단계는 2단계로 분배하거나, 심지어는 각각 4단계 및 1단계로 줄일 수 있어 추론 비용이 크게 절감되고 대부분 생성 품질이 유지된다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setting\n' +
      '\n' +
      'CogView3의 성능을 입증하기 위한 종합적인 평가 프로세스를 구현하였으며, 전체 확산 일정 1000단계로 중계 초해상도의 시작점을 500으로 설정하였으며, 4.4절에 자세히 설명된 간략한 절제 연구에서 알려준 결정이다. 비교를 위한 이미지를 생성하기 위해 CogView3의 기본 단계별로 50단계, 초해상 단계별로 10단계를 샘플링하고, 달리 명시되지 않는 한 분류기 없는 지침 [10]을 7.5로 사용한다. 그 비교는 모두 \\(1024\\times 1024\\)의 이미지 해상도에서 수행된다.\n' +
      '\n' +
      '#### 4.1.1 Dataset\n' +
      '\n' +
      '비교 분석을 위해 이미지-텍스트 쌍 데이터 세트와 프롬프트 모음의 조합을 선택한다. 이 중 MS-COCO[13]은 텍스트 대 이미지 생성의 품질을 평가하기 위해 널리 적용되는 데이터셋이다. 우리는 COCO-5k로 명명된 MS-COCO에서 5000개의 이미지 텍스트 쌍의 하위 집합을 무작위로 선택한다. 또한 텍스트 대 이미지 평가를 위해 잘 알려진 두 가지 프롬프트 세트인 DrawBench [21] 및 PartiPrompts [31]을 통합한다. 드로벤치는 생성된 샘플의 품질과 이미지와 텍스트 간의 정렬을 모두 평가하는 200개의 도전 프롬프트로 구성된다. 대조적으로, PartiPrompts는 1632개의 텍스트 프롬프트를 포함하고 포괄적인 평가 비평을 제공한다.\n' +
      '\n' +
      '#### 4.1.2 Baselines\n' +
      '\n' +
      '평가에서는 최첨단 오픈 소스 텍스트 대 이미지 모델, 특히 SDXL [17] 및 스테이블 캐스케이드 [16]을 기준으로 사용한다. SDXL은 1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×10 한편, 스테이블 캐스케이드(Stable Cascade)는 캐스케이드 파이프라인(Cascaded pipeline)을 구현하며, 처음에는 16\\times 24\\times 24\\times의 전개를 생성하고, 이후 전개를 컨디셔닝하여 1024\\times 1024\\times의 해상도로 영상을 생성한다. 우리는 두 단계에 대해 각각 50단계에 대해 SDXL을 샘플링하고 20단계 및 10단계에 대해 스테이블 캐스케이드를 샘플링한다. 모든 경우에 분류기 없는 지침의 권장 구성을 준수합니다.\n' +
      '\n' +
      '######4.1.3 평가 메트릭스\n' +
      '\n' +
      '생성된 샘플의 이미지 품질을 평가하기 위해 Aesthetic Score(Aes) [24]를 사용한다. 또한 텍스트-이미지 정렬 및 인간 선호도를 평가하기 위해 인간 선호도 점수 v2(HPS v2) [29] 및 이미지 리워드 [30]을 채택한다. Aes는 LAION 데이터 세트에서 훈련된 미적 점수 예측기에 의해 얻어지며 프롬프트와 이미지의 정렬을 무시한다. HPS v2와 ImageReward는 모두 텍스트-이미지 정렬의 평가, 인간의 심미성 등을 포함하여 이미지에 대한 인간의 선호도를 예측하는 데 사용된다. 또한 기계 평가 외에도 이미지 품질과 의미론적 정확도를 포함하는 모델의 성능을 추가로 평가하기 위해 인간 평가를 수행한다.\n' +
      '\n' +
      '### 기계평가 결과\n' +
      '\n' +
      '표 1은 DrawBench 및 Partiprompt에 대한 기계 메트릭의 결과를 보여준다. CogView3는 추론 비용이 가장 낮지만, PartiPrompts의 Image Reward에서 Stable Cascade로 약간의 차질을 제외하고는 대부분의 비교에서 SDXL과 Stable Cascade를 능가한다. 표 2와 같이 COCO-5k에 대한 비교에서도 유사한 결과가 관찰된다. CogView3의 증류 버전은 1.47s의 매우 낮은 추론 시간이 소요되지만 여전히 유사한 성능을 달성한다. CogView3의 증류된 변형의 결과는 표에 예시된 바와 같이 SDXL에서 잠재 일관성 모델 [14]의 이전 증류 패러다임을 상당히 능가한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c|c c c|c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multirow{2}{*}{Steps} & \\multirow{2}{*}{Time Cost} & \\multicolumn{4}{c|}{**DrawBench**} & \\multicolumn{4}{c}{**PartiPrompts**} \\\\ \\cline{5-8}  & & & & Aes\\(\\uparrow\\) & HPS v2\\(\\uparrow\\) & ImageReward\\(\\uparrow\\) & Aes\\(\\uparrow\\) & HPS v2\\(\\uparrow\\) & ImageReward\\(\\uparrow\\) \\\\ \\hline SDXL [17] & 50 & 19.67s & 5.54 & 0.288 & 0.676 & 5.78 & 0.287 & 0.915 \\\\ StableCascade [16] & 20+10 & 10.83s & 5.88 & 0.285 & 0.677 & 5.93 & 0.285 & **1.029** \\\\\n' +
      '**CogView3** & 50+10 & **10.33s** & **5.97** & **0.290** & **0.847** & **6.15** & **0.290** & 1.025 \\\\ \\hline LCM-SDXL [14] & 4 & 2.06s & 5.45 & 0.279 & 0.394 & 5.59 & 0.280 & 0.689 \\\\\n' +
      '**CogView3-distill** & 4+1 & **1.47s** & 5.87 & 0.288 & 0.731 & 6.12 & 0.287 & 0.968\\\\\n' +
      '**CogView3-distill** & 8+2 & 1.96s & 5.90 & 0.285 & 0.655 & 6.13 & 0.288 & 0.963 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: DrawBench 및 PartiPrompts에 대한 기계 메트릭의 결과. 모든 시료는 \\(1024\\times 1024\\)에서 생성된다. 시간 비용은 배치 크기 4로 측정됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multicolumn{2}{l}{**COCO-5k**} \\\\ \\hline Model & Steps & Time Cost & FID\\(\\downarrow\\) & Aes\\(\\uparrow\\) & HPS v2\\(\\uparrow\\) & ImageReward\\(\\uparrow\\) \\\\ \\hline SDXL [17] & 50 & 19.67s & **26.29** & 5.63 & 0.291 & 0.820 \\\\ StableCascade [16] & 20+10 & 10.83s & 36.59 & 5.89 & 0.283 & 0.734 \\\\\n' +
      '**CogView3** & 50+10 & **10.33s** & 31.63 & **6.01** & **0.294** & **0.967** \\\\ \\hline LCM-SDXL [14] & 4 & 2.06s & 27.16 & 5.39 & 0.281 & 0.566 \\\\\n' +
      '**CogView3-distill** & 4+1 & **1.47s** & 34.03 & 5.99 & 0.292 & 0.920\\\\\n' +
      '**CogView3-distill** & 8+2 & 1.96s & 35.53 & 6.00 & 0.293 & 0.921 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: COCO-5k에 대한 기계 메트릭의 결과. 모든 시료는 \\(1024\\times 1024\\)에서 생성된다. 시간 비용은 배치 크기 4로 측정됩니다.\n' +
      '\n' +
      '비교 결과는 CogView3의 성능이 현저하게 감소된 비용으로 향상된 품질 및 충실도의 이미지를 생성하는 것을 보여준다. CogView3의 증류는 대부분의 생성 품질을 보존하는 데 성공하면서 샘플링 시간을 극단적으로 줄인다. 우리는 앞서 언급한 비교 결과를 CogView3의 중계 특성에 크게 기인하며, 다음 섹션에서는 인간 평가와 함께 CogView3의 성능을 추가로 입증할 것이다.\n' +
      '\n' +
      '### 인체 평가 결과\n' +
      '\n' +
      '주석이 쌍별 비교를 수행하도록 하여 CogView3에 대한 인간 평가를 수행한다. 인간 주석자는 세대의 신속한 정렬 및 미적 품질에 기초하여 승패 또는 동점의 결과를 제공하도록 요청받는다. 우리는 평가 벤치마크로 DrawBench[21]를 사용한다. CogView3 생성을 위해 먼저 DrawBench에서 자세한 설명까지 프롬프트를 확장한다.\n' +
      '\n' +
      '도 4: DrawBench 생성에 대한 인간 평가 결과. **(좌)** 신속 정렬에 대한 비교 결과, **(우)** 심미적 품질에 대한 비교 결과. “(확장)”는 생성에 사용된 프롬프트가 텍스트 확장임을 나타냅니다.\n' +
      '\n' +
      '그림 5: 증류 모델에 대한 Drawbench 생성에 대한 인간 평가 결과. **(좌)** 신속 정렬에 대한 비교 결과, **(우)** 심미적 품질에 대한 비교 결과. “(확장)”는 생성에 사용된 프롬프트가 텍스트 확장임을 나타냅니다. CogView3-distill의 경우 8+2단계, LCM-SDXL의 경우 4단계를 샘플링한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '텍스트 확장을 통한 정렬 개선\n' +
      '\n' +
      '신속한 확장은 SDXL과 Stable Cascade의 생성에 거의 도움이 되지 않지만, CogView3의 성능에 대한 중요성을 강조한다. 그림 7은 신속한 확장이 있는 경우와 없는 경우의 비교 결과를 보여주며, 신속한 확장이 CogView3에 대한 즉각적인 지시의 능력을 상당히 향상시킨다는 것을 분명히 보여준다. 그림 8은 신속한 확장이 있기 전과 후의 질적인 비교를 보여준다. 확장된 프롬프트는 모델 생성에 대한 보다 포괄적이고 배포 내 설명을 제공하여 CogView3에 대한 지시의 정확도를 크게 향상시켰으며 SDXL 생성에서도 유사한 개선이 관찰되지 않았다. 가능한 이유는 SDXL이 원래 캡션들에 대해 트레이닝되고 77개의 토큰들의 입력 윈도우만을 가지며, 이는 확장된 프롬프트들의 빈번한 절단을 초래하기 때문일 수 있다. 이것은 신속한 확장이 재옵션 데이터로 모델 추론과 훈련 사이의 격차를 해소하는 데 도움이 된다는 섹션 3.1의 진술을 확증한다.\n' +
      '\n' +
      '그림 8: CogView3와 SDXL에 대한 신속한 확장의 효과 비교.\n' +
      '\n' +
      '도 7: DrawBench 상의 신속한 확장 전후의 CogView3의 인간 평가 결과.\n' +
      '\n' +
      '반복적인 초해상화 4.2.3 방법\n' +
      '\n' +
      '더 높은 영상 해상도에 대한 초해상도 스테이지 모델의 간단한 구현은 원하는 출력을 달성하지만, 이는 CUDA 메모리의 과도한 요구 사항을 도입하며, 이는 \\(4096\\times 4096\\)의 해상도에서 견딜 수 없다. 타일 확산[2][11]은 이러한 문제를 해결하는 확산 모델에 대한 일련의 추론 방법이다. 이는 큰 영상들의 추론 단계를 중첩된 작은 블록들로 분리하고 이들을 함께 혼합하여 전체 단계의 예측을 얻는다. 그림 9와 같이 타일드 추론으로 필적할 만한 결과를 얻을 수 있다. 이를 통해 CogView3는 제한된 CUDA 메모리 사용으로 더 높은 해상도의 이미지를 생성할 수 있다. 또한 향후 작업을 위해 남겨둔 타일 방식으로 \\(4096\\times 4096\\) 이미지를 생성할 수 있다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 릴레이 확산 프레임워크에서 첫 번째 텍스트-이미지 생성 시스템인 CogView3를 제안한다. CogView3는 중계 파이프라인에 크게 기인하여 추론 비용이 크게 감소하면서 선호되는 생성 품질을 달성한다. CogView3의 초해상도 단계를 반복적으로 구현함으로써, \\(2048\\times 2048\\)의 매우 높은 해상도의 고품질 이미지를 얻을 수 있다.\n' +
      '\n' +
      '한편, 데이터 재캡셔닝 및 모델 파이프라인으로의 신속한 확장에 의해, CogView3는 현재의 최첨단 오픈 소스 텍스트-이미지 확산 모델에 비해 신속한 이해 및 후속 명령에서 더 나은 성능을 달성한다.\n' +
      '\n' +
      '또한 CogView3의 증류를 탐색하고 릴레이 확산 프레임워크에 기인한 단순성과 능력을 보여준다. 진행형 증류 패러다임을 활용하여 CogView3의 증류된 변종은 추론 시간을 대폭 단축하면서도 유사한 성능을 유지한다.\n' +
      '\n' +
      '그림 9: \\(2048\\times 2048\\)에서 직접 더 높은 초분해능과 타일드 확산의 비교. 우리는 통합의 우수한 품질을 고려하여 Diffusers의 혼합[11]을 선택한다. 오리지널 프롬프트는 모든 블록의 추론을 위해 사용된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]A. 아치암 S. 애들러 L. 애거월 Ahmad, I. Akkaya, F. Aleman, D. Almeida, J. Altenschmidt, S. 알트만 Anadkat, et al.(2023) Gpt-4 기술 보고서. ArXiv:2303.08774. 인용: SS1.\n' +
      '*[2]O. L. 바탈 야리브 립맨과 T Dekel(2023) Multidiffusion: 제어된 이미지 생성을 위한 확산 경로 융합. 인용: SS1.\n' +
      '*[3]J. 베커고 징태 브룩스 J. Wang, L. 이림 오양, 장정, 이영 Guo, et al.(2023) 더 나은 캡션을 갖는 이미지 생성 개선. 컴퓨터 과학 외부 링크: 인용된 링크: SS1입니다.\n' +
      '*[4]C. Bishop and N. M. Nasrabadi (2006) Pattern Recognition and Machine Learning. 스프링어 4권 인용: SS1.\n' +
      '*[5]X. 대정호 차재왕 왕필장 X 반덴헨데 Wang, A. Dubey, et al.(2023) EMU: enhanced image generation models using photogenic needle in a haystack. ArXiv:2309.15807. 인용: SS1.\n' +
      '*[6]M. 딩, 지 양원 홍원 정창주 주주 Shao, H. Yang, et al.(2021) Cogview: 트랜스포머를 통해 텍스트-투-이미지 생성을 마스터링함. Advances in Neural Information Processing Systems34, pp. 19822-19835. Cited by: SS1.\n' +
      '*[7]G. 힌튼 오 Vinyals, and J. Dean (2015) the knowledge Distilling the neural network. ArXiv:1503.02531. 인용: SS1.\n' +
      '*[8]J. Ho, A. Jain, and P. Abbeel (2020) Denoising diffusion probability model. neural information processing systems33, pp. 6840-6851. Cited by: SS1.\n' +
      '*[9]J. 호철사하리아 장동준 노루지, T. Salimans (2022) Cascaded diffusion model for high fidelity image generation. The Journal of Machine Learning Research23 (1), pp. 2249-2281. Cited by: SS1.\n' +
      '*[10]J. 호랑 T 살리만(2022) 분류기 없는 확산 안내. ArXiv:2207.12598. 인용: SS1.\n' +
      '*[11]A. B. 히메네스(2023) Mixture of diffusers for scene composition and high resolution image generation. ArXiv:2302.02412. 인용: SS1.\n' +
      '*[12]M. 강재영 장재만 파리, T Park(2023) 텍스트-이미지 합성을 위해 간스들을 스케일링한다. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10124-10134. Cited by: SS1.\n' +
      '*[13]T. Y. Lin, M. 마이어 Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick (2014) Microsoft coco: context in common objects. Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740-755. Cited by: SS1.\n' +
      '*[14]S. 류영 탄락 Huang, J. Li, and H. Zhao (2023) Latent consistency model: 몇 단계 추론을 통해 고해상도 영상을 합성한다. ArXiv:2310.04378. 인용: SS1.\n' +
      '*[15]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[16]P. 페르니아스, D. 램파스, M. L. 리히터, C. J. Pal, M. Aubreville (2023) Wuerstchen: 대규모 텍스트-이미지 확산 모델들을 위한 효율적인 아키텍처. 인용: SS1.\n' +
      '*[17]D. 포델, 지 영어, K. 레이시, A. 블랫만, T. Dockhorn, J. Muller, J. Penna, R. Rombach(2023) Sdxl: 고해상도 이미지 합성을 위한 잠재 확산 모델 개선. ArXiv:2307.01952. 인용: SS1.\n' +
      '*[18]C. 라펠 Shazeer, A. Roberts, K. 이승환 나랑만 마테나 주원 Li, P. J. Liu(2020) 통합 텍스트-텍스트 변환기를 이용한 전이학습의 한계를 탐색한다. The Journal of Machine Learning Research21(1), pp. 5485-5551. Cited by: SS1.\n' +
      '*[19]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[20]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[21]P. 페르니아스, D. 램파스, M. L. 리히터, C. J. Pal, M. Aubreville (2023) Wuerstchen: 대규모 텍스트-이미지 확산 모델들을 위한 효율적인 아키텍처. 인용: SS1.\n' +
      '*[22]D. 포델, 지 영어, K. 레이시, A. 블랫만, T. Dockhorn, J. Muller, J. Penna, R. Rombach(2023) Sdxl: 고해상도 이미지 합성을 위한 잠재 확산 모델 개선. ArXiv:2307.01952. 인용: SS1.\n' +
      '*[23]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[24]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[25]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[26]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[27]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[28]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[29]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[30]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[31]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[32]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[33]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[34]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[35]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[36]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[37]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[38]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[39]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[40]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[41]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans (2023) On distillation of guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297-14306. Cited by: SS1.\n' +
      '*[42]C. 맹룡 롬바흐 가오대국마 Ermon, J. Ho, and T. Salimans(2023) On*[19] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M. : 클립 래턴트를 갖는 계층적 텍스트-조건부 이미지 생성. arXiv preprint arXiv:2204.06125 **1**(2), 3(2022)\n' +
      '* [20] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: Zero-shot text-to-image generation. In: International Conference on Machine Learning. pp. 8821-8831. PMLR (2021)\n' +
      '* [21] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems **35**, 36479-36494 (2022)\n' +
      '* [22] Salimans, T., Ho, J.: Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512 (2022)\n' +
      '* [23] Sauer, A., Lorenz, D., Blattmann, A., Rombach, R.: Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042 (2023)\n' +
      '* [24] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.: Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems **35**, 25278-25294 (2022)\n' +
      '* [25] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020)\n' +
      '* [26] Song, Y., Dhariwal, P., Chen, M., Sutskever, I.: Consistency models (2023)\n' +
      '* [27] Teng, J., Zheng, W., Ding, M., Hong, W., Wangni, J., Yang, Z., Tang, J.: Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350 (2023)\n' +
      '* [28] Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 (2023)\n' +
      '* [29] Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., Li, H.: Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341 (2023)\n' +
      '* [30] Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., Dong, Y.: Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [31] Yu, J., Xu, Y., Koh, J.Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B.K., et al.: Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789 **2**(3), 5 (2022)\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:18]\n' +
      '\n' +
      '도 11: 인간 평가의 인터페이스 쇼케이스. 원래의 프롬프트는 평가를 위해 인간 주석자의 모국어인 중국어로 번역됩니다.\n' +
      '\n' +
      '그림 10: 재채택 모델 결과의 예.\n' +
      '\n' +
      '## 부록 0.D 추가 질적 비교\n' +
      '\n' +
      '### 정성적 모델 비교\n' +
      '\n' +
      '그림 12: CogView3와 SDXL, Stable Cascade 및 DALL-E 3의 정성적 비교. 모든 프롬프트는 Partiprompt에서 샘플링된다.\n' +
      '\n' +
      'Distilled Model간의 질적 비교\n' +
      '\n' +
      '그림 13: CogView3-distill과 LCM-SDXL의 정성적 비교, \\(1024\\times 1024\\) 샘플을 생성할 수 있는 최근 확산 증류 모델. 첫 번째 열은 CogView3의 원래 버전의 샘플을 보여줍니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>