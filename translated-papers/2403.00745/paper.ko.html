<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_EMPTY:1]\n' +
      '\n' +
      '최상위 기여 노드들의 작은 세트에 대해 관리하므로, 검증은 모든 노드들에 대해 반복하는 것보다 훨씬 저렴하다.\n' +
      '\n' +
      'Our contributions:\n' +
      '* 우리는 위음성을 생성하는 두 종류의 고장 모드를 찾는 AtP의 성능을 조사한다. 확장성을 유지하면서 이러한 고장 모드를 해결하기 위해 두 가지 변경 사항이 있는 AtP\\({}^{*}\\)이라고 하는 AtP의 변형을 제안한다.\n' +
      '* 질의들 및 키들을 패치할 때, 어텐션 소프트맥스를 재계산하고 그 이후부터 그래디언트 기반 근사치를 사용하는 것은, 그래디언트들이 포화 어텐션에 대한 불량한 근사치이기 때문이다.\n' +
      '* 역방향 패스에 드롭아웃을 사용하여 부서지기 쉬운 위음성을 수정하며, 여기서 유의한 긍정 및 부정 효과가 상쇄된다.\n' +
      '* 우리는 무차별 힘 활성화 패칭을 능가하는 AtP의 기준선으로 활성화 패칭을 근사화하는 몇 가지 대체 방법을 소개한다.\n' +
      '* 우리는 AtP와 이러한 대안들에 대한 첫 번째 체계적인 연구를 제시하고 AtP가 다른 모든 조사된 방법들보다 상당히 우수하다는 것을 보여주며, AtP\\({}^{*}\\)는 추가적인 상당한 개선을 제공한다.\n' +
      '* AtP\\({}^{*}\\)의 잔류 오차를 추정하고 나머지 위음성의 크기를 통계적으로 결합하기 위해 AtP를 사용하여 높은 충격 노드를 필터링한 다음 나머지 부분 집합을 패치하는 진단 방법을 제공한다. 좋은 진단은 실무자가 철저한 검증 비용 없이 관련 도메인에서 AtP가 신뢰할 수 있는지 여부를 여전히 측정할 수 있음을 의미한다.\n' +
      '\n' +
      '마지막으로, 우리는 5.4절에서 실제로 인과적 귀속을 성공적으로 수행하는 방법과 귀속 방법이 유용하고 어떤 상황에서 유용할 수 있는지에 대한 몇 가지 지침을 제공한다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '### Problem Statement\n' +
      '\n' +
      '우리의 목표는 개별 모델 구성 요소에 의한 모델 행동에 대한 기여도를 식별하는 것이다. 우리는 먼저 모델 구성 요소를 공식화한 다음 모델 행동을 공식화하고 마지막으로 기여 문제를 인과 언어로 진술한다. 디코더 전용 트랜스포머 언어 모델(Radford et al., 2018; Vaswani et al., 2017)의 관점에서 형식론을 설명하고, 해당 클래스의 모델에 대한 모든 실험을 수행하지만, 형식론은 다른 모델 클래스에도 쉽게 적용할 수 있다.\n' +
      '\n' +
      '모델 구성 요소.우리는 일련의 토큰에 대한 로짓들을 출력하기 위해 프롬프트(토큰 시퀀스)를 매핑하는 모델\\(\\mathcal{M}:X\\rightarrow\\mathbb{R}^{V}\\)을 주어, 시퀀스의 다음 토큰을 예측하는 것을 목표로 한다. 우리는 모델 \\(\\mathcal{M}\\)을 계산 그래프 \\(N,E)\\)로 볼 것이다. 여기서 노드 집합 \\(N\\)은 모델 성분의 집합이고, \\(n_{1},n_{2})\\(n_{1}\\)의 출력이 \\(n_{2}\\)의 계산에 직접 입력이면 방향 에지 \\(e=(n_{1},n_{2})\\이 존재한다. 우리는 \\(\\mathcal{M}(x)\\)을 계산할 때 \\(n\\)의 _activation_(중간 계산 결과)를 나타내기 위해 \\(n(x)\\)을 사용할 것이다.\n' +
      '\n' +
      '\\(N\\)의 선택은 귀속이 얼마나 세밀하게 결정될지를 결정한다. 예를 들어, 변압기 모델의 경우 각 층이 단일 노드로 간주되는 상대적으로 거친 속성(coarse-grained attribute)을 가질 수 있다. 이 논문에서 우리는 계산 비용이 더 많이 드는 더 세밀한 귀인을 주로 고려할 것이다(자세한 내용은 섹션 4 참조). 섹션 5에서 이 문제를 재검토한다.\n' +
      '\n' +
      '도 1 | 샘플 프롬프트 쌍들 상에서, 상이한 방법들을 사용하여 피티아-12B에서 가장 인과적으로 중요한 노드들을 찾는 비용들(표 1 참조). 음영은 기하학적 표준 편차를 나타냅니다. 비용은 전진 패스에서 측정되므로 각 점의 y 좌표는 상위 \\(x\\) 노드를 찾는 데 필요한 전진 패스의 수를 제공한다. 각 노드는 검증되어야 하므로 \\(y\\geq x\\)이므로 모든 선이 대각선 위에 있고 검증 순서에 대한 오라클이 대각선을 생성한다는 점에 유의한다. 자세한 설명은 4.3절을 참조하십시오.\n' +
      '\n' +
      '그림 2 | 샘플 프롬프트 쌍에 대한 모델 간 방법의 상대적 비용입니다. 비용은 오라클을 갖는 것과 관련이 있으며, 이는 진정한 기여 크기의 감소 순서로 노드를 검증할 것이다. 비용은 역순위 가중 기하 평균을 사용하여 집계됩니다. 이는 그림 1의 각 곡선에 대한 대각선 위의 면적에 해당하며 점선(오라클) 선 아래의 면적에 상대적인 것을 의미한다. 이 메트릭에 대한 자세한 내용은 섹션 4.2를 참조하십시오. GradDrop(AtP+QKfix와 AtP+)의 차이는 눈에 띄는 초기 비용과 함께 제공되므로 그림 1과 같이 위음성을 피하는 데 여전히 도움이 되면서도 이 비교에서 더 나빠 보인다.\n' +
      '\n' +
      '그림 1: 샘플 프롬프트 쌍에서 다른 방법을 사용하여 피티아-12B에서 가장 인과적으로 중요한 노드를 찾는 비용(표 1 참조). 음영은 기하학적 표준 편차를 나타냅니다. 비용은 순방향 패스에서 측정되며, 따라서 각 점의 y 좌표는 상위 \\(x\\) 노드를 찾는 데 필요한 순방향 패스의 수를 제공한다. 각 노드는 검증되어야 하므로 \\(y\\geq x\\)이므로 모든 선이 대각선 위에 있고 검증 순서에 대한 오라클이 대각선을 생성한다는 점에 유의한다. 자세한 설명은 4.3절을 참조하십시오.\n' +
      '\n' +
      '모델 행동. 과거 작업(Chan et al., 2022; Geiger et al., 2022; Wang et al., 2022) 이후, 우리는 입력 쌍에 대한 분포\\(\\mathcal{D}\\)을 가정하는데, 여기서 \\(x^{\\text{clean}},x^{\\text{noise}}\\)는 그 행동이 발생하는 프롬프트이고, \\(x^{\\text{noise}\\)는 우리가 1과 개입하기 위한 잡음의 소스로서 사용하는 기준 프롬프트이다. 또한, 관심의 행동을 정량화하는 메트릭2\\(\\mathcal{L}:\\mathbb{R}^{\\nu}\\rightarrow\\mathbb{R}\\(\\mathcal{L}:\\mathbb{R}\\)이 주어진다.\n' +
      '\n' +
      '각주 1: 이것은 제로-절제 또는 평균 절제와 같이 실제로 실현되지 않는 활성화 값을 사용하는 개입을 배제한다. 활성화 값의 분포를 통한 대체 제형도 가능하다.\n' +
      '\n' +
      '각주 2: 언어 모델의 공통 메트릭은 다음 토큰 예측 손실, 올바른 다음 토큰과 잘못된 다음 토큰 사이의 로그 프로브의 차이, 올바른 다음 토큰의 확률 등이다.\n' +
      '\n' +
      '구성요소의 기여도. 위에서 언급한 작업과 유사하게 우리는 모델의 행동에 대한 노드의 기여도\\(c(n)\\)를 클린 프롬프트에 대한 해당 노드를 참조 프롬프트에 대한 값으로 대체함으로써 예상되는 반사실적 절대 3의 영향으로 정의한다.\n' +
      '\n' +
      '각주 3: 영향의 징후는 흥미로울 수 있지만, 이 작업에서 우리는 인과적 중요성의 척도로 크기에 초점을 맞출 것이다.\n' +
      '\n' +
      'do-calculus notation (Pearl, 2000)을 사용하여 이것은 \\(c(n):=|\\mathcal{I}(n)|\\)로 표현될 수 있으며, 여기서\n' +
      '\n' +
      '=\\mathbb{E}_{(x^{\\text{clean}},x^{\\text{noise}})\\sim\\mathcal{D}}\\left[\\mathcal{I}(n;x^{\\text{clean}},x^{\\text{noise}})\\right], \\tag{1}\\tag}\n' +
      '\n' +
      '여기서 우리는 \\(x^{\\text{clean},x^{\\text{noise}})에 대한 개입 효과 \\(\\mathcal{I}\\)을 다음과 같이 정의한다.\n' +
      '\n' +
      '\\mathcal{I}(n;x^{\\text{clean}},x^{\\text{noise}}):=\\mathcal{L}(\\mathcal{M}(x^{\\text{clean}}\\mid\\text{do}(n\\gets n(x^{\\text{noise})))-\\mathcal{L}(\\mathcal{M}(x^{\\text{clean}))}\\tag{2}\\tag{2}}\n' +
      '\n' +
      '분포 전체에 걸쳐 효과를 평균화할 필요가 컴퓨팅 비용\\(c(n)\\에 잠재적으로 큰 곱셈 요소를 추가하여 이 작업에 동기를 부여한다는 점에 유의하십시오.\n' +
      '\n' +
      '우리는 또한 노드 집합 \\(\\eta=\\{n_{i}\\}\\})에 개입할 수 있다. 이를 위해 참조 프롬프트의 값으로 \\(\\eta\\)에 있는 모든 노드의 값을 덮어씁니다. 표기를 이용하여, 우리는 \\(\\mathcal{M}(x)\\)을 계산할 때 \\(\\eta\\)에 있는 노드들의 활성화 집합으로 \\(\\eta(x)\\)을 쓴다.\n' +
      '\n' +
      '\\mathcal{I}(\\eta;x^{\\text{clean}},x^{\\text{noise}}):=\\mathcal{L}(\\mathcal{M}(x^{\\text{clean}}\\mid\\text{do}(\\eta\\leftarrow\\eta(x^{\\text{noise})))-\\mathcal{L}(\\mathcal{M}(x^{\\text{clean})) \\tag{3}\\tag{3}})\n' +
      '\n' +
      '우리는 기여도를 참조 프롬프트에 대한 노드를 노이즈 제거 또는 녹인이라고도 하는 깨끗한 프롬프트에 대한 값으로 대체하는 예상 효과로 정의하는 것도 유효하다는 점에 주목한다. 우리는 Chan et al.(2022)을 따른다; Wang et al.(2022)은 잡음제거를 사용하지만, 잡음제거는 또한 문헌에서 널리 사용된다 (Lieberum et al., 2023; Meng et al., 2023). 이 선택이 섹션 5.2에서 AtP에 어떻게 영향을 미치는지 간략하게 고려한다.\n' +
      '\n' +
      '### Attribution Patching\n' +
      '\n' +
      '최첨단 모델들에서, 모든 \\(n\\)에 대한 \\(c(n)\\) 계산은 수십억 개 이상의 노드가 존재할 수 있기 때문에 엄청나게 비쌀 수 있다. 또한 이 값을 정확하게 계산하기 위해서는 모든 프롬프트 쌍에 대한 평가가 필요하며, 따라서 각 \\(n\\) 척도에 대한 식 (1)의 런타임 비용은 \\(\\mathcal{D}\\)의 지원 크기를 갖는다.\n' +
      '\n' +
      '따라서 우리는 식 (1)의 빠른 근사치로 돌아간다. Figurnov et al. (2016); Molchanov et al. (2017); Nanda (2022); we can make a first-order Taylor expansion to \\(\\mathcal{I}(n;x^{\\text{clean},x^{\\text{noise}})} around \\(n(x^{\\text{clean})}\\(hat{I}_{\\text{AtP}}(n;x^{\\text{clean}},x^{\\text{noise}}):=(n(x^{\\text{noise}})-n(x^{\\text{clean}}))^{\\intercal}\\frac{\\partial\\mathcal{L}(M(x^{\\text{clean}})}\n' +
      '\n' +
      '그런 다음 Syed et al.(2023)과 유사하게, 우리는 이것을 외부보다는 식 (1)의 기대치 내부의 절대값을 취하여 분포에 적용하는데, 이는 양의 효과와 음의 효과를 갖는 프롬프트 쌍에 걸친 추정치가 상당히 작은 추정치로 잘못 이어질 가능성을 감소시킨다. (우리는 부록 B.2의 참효과 분포에서 취소 행동의 양을 간단히 탐색한다.) 그 결과, 우리는 추정치를 얻는다.\n' +
      '\n' +
      '=\\mathbbb{E}_{x^{\\text{AtP}}(n):=\\mathbb{E}_{x^{\\text{clean},x^{\\text{noise}}}\\left[\\left|\\hat{I}_{\\text{AtP}}(n;x^{\\text{clean},x^{\\text{noise}}}\\right|\\right|. \\tag{5}\\tag{5}}}\n' +
      '\n' +
      '이러한 절차를 _Attribution Patching_(Nanda, 2022) 또는 _AtP_라고도 한다. AtP는 주어진 프롬프트 쌍 상의 _all 노드_에 대한 추정 점수를 계산하기 위해 2개의 순방향 패스와 1개의 역방향 패스를 필요로 하므로, 무차별 힘 활성화 패치보다 매우 상당한 속도 향상을 제공한다.\n' +
      '\n' +
      '## 3 Methods\n' +
      '\n' +
      '이제 AtP의 일부 고장 모드를 설명하고 이를 해결하여 개선된 방법 AtP*를 생성한다. 그런 다음 AtP(*)의 성능을 컨텍스트에 넣기 위해 \\(c(n)\\)을 추정하는 몇 가지 대체 방법에 대해 논의한다. 마지막으로 우리는 AtP*가 중요한 위음성을 놓쳤을 수 있는지 여부를 통계적으로 테스트하기 위한 진단을 제공하기 위해 섹션 3.3에 설명된 이러한 대안 방법 중 하나인 서브샘플링과 AtP*를 결합하는 방법에 대해 논의한다.\n' +
      '\n' +
      '### AtP improvements\n' +
      '\n' +
      '우리는 AtP를 사용할 때 발생하는 위음성의 두 가지 공통 부류를 식별한다.\n' +
      '\n' +
      '첫 번째 고장 모드는 \\(x^{\\text{clean}}\\)에 대한 사전 활성화가 활성화 함수의 평평한 영역(예: 포화 주의 가중치를 생성함)에 있을 때 발생하지만, \\(x^{\\text{noise}}\\)에 대한 사전 활성화는 그 영역에 있지 않다. 식 (4)에서 명백한 바와 같이, AtP는 식 (1)에서 지상 진실에 대한 선형 근사를 사용하므로, 비선형 함수가 국소 구배에 의해 잘못 근사되면 AtP는 정확하지 않게 된다 - 그림 3과 프롬프트 쌍 사이에서 관찰되는 주의의 최대 차이를 색으로 나타내는 그림 4를 참조하라. 이는 이러한 고장 모드가 실제로 발생함을 시사한다.\n' +
      '\n' +
      '또한, 직접효과와 간접효과 사이의 상쇄에 의해 관련이 없는 고장모드가 발생한다. 즉, 전체 효과(일부 프롬프트 쌍에 대한 총 효과)가 직접효과와 간접효과의 합(Pearl, 2001)\\(\\mathcal{I}(n)=\\mathcal{I}^{\\text{direct}}(n)+\\mathcal{I}^{\\text{ indirect}(n)\\)이면, 이는 상쇄에 가깝고, GELU와 소프트맥스 등의 비선형성으로 인해 \\(\\left|\\hat{I}_{\\text{AtP}^{\\text{direct}}(n)+\\hat{I}_{I}_{\\text{간접}(n)\\right|\\)의 작은 승산근사오차가 \\(\\left|\\mathcal{I}(n)+\\hat{I}_{\\text{AP}^{\\text{간접}(n)\\right|\\)보다 작은 크기만큼 발생할 수 있다.\n' +
      '\n' +
      'Attention saturation에 의한 false negative\n' +
      '\n' +
      'AtP는 각 활성화에서의 기울기가 해당 활성화에서의 개입과 관련된 기능의 진정한 행동을 반영하는 것에 의존한다. 그러나, 일부 경우들에서, 노드는 그 효과가 기울기에 의해 적절하게 예측되지 않을 수 있는 비선형성으로 즉시 피딩할 수 있다; 예를 들어, 어텐션 키 및 질의 노드들은 어텐션 소프트맥스 비선형성으로 피딩한다. 이를 보여주기 위해 그림 4(왼쪽)의 AtP에 의해 할당된 순위에 대한 각 노드의 효과의 실제 순위를 표시한다. 플롯은 특히 키와 쿼리들 사이에서 두드러진 위음성이 많이 있음을 보여준다(점선 아래).\n' +
      '\n' +
      '쿼리 및 키에 대한 정상적인 활성화 패칭은 쿼리 또는 키를 변경한 다음 나머지 모델을 다시 실행하여 다른 모든 것을 동일하게 유지하는 것을 포함한다. AtP는 모델의 재실행이 아닌 나머지 전체 모델에 대한 선형 근사를 취한다. 나머지 모델의 첫 번째 단계, 즉 주의력 소프트맥스를 명시적으로 재계산한 다음 나머지 모델에 선형 근사치를 취하는 것을 제안한다. 형식적으로, 어텐션 키와 질의 노드들의 경우, 그 노드들에 대한 그래디언트를 직접 사용하는 대신에, 우리는 그 키 또는 질의에 의해 야기되는 어텐션 가중치의 차이를 어텐션 가중치 자체에 대한 그래디언트와 곱한다. 이를 위해서는 각 키 및 쿼리 패치에서 주의 가중치의 변화를 찾아야 하지만, 두 개의 트랜스포머 전진 패스보다 계산량이 적은 (모든 키 및 쿼리에 대해) 효율적으로 수행할 수 있다. 이 보정은 AtP의 성능을 유지하면서 포화 주의 문제를 방지한다.\n' +
      '\n' +
      '질의에 대한 질의는 모델을 \\(x^{\\text{noise}}\\)에서 실행하고 잡음 질의를 캐싱함으로써 조정된 효과를 쉽게 계산할 수 있다. 그런 다음 모델을 \\(x^{\\text{clean}}\\)에서 실행하고 주의 키와 가중치를 캐시한다. 마지막으로, 앞쪽(x^{\\text{clean}}\\) 패스의 모든 키와 앞쪽(x^{\\text{noise}}\\) 패스의 질의를 결합하여 어텐션 가중치를 계산한다. 이것은 변압기 전방 통과의 교란되지 않은 주의 계산만큼 비용이 든다. 각 질의 노드(n\\(n\\)에 대해, 생성된 가중치 벡터는 클린 포워드 패스의 가중치(\\(\\operatorname{attn}(n)_{\\text{patch}})와 대조적으로 \\(\\operatorname{attn}(n)(x^{\\text{clean}})\\)으로 지칭된다. \\(n\\)에 대한 개선된 귀인 추정치는 다음과 같다.\n' +
      '\n' +
      '도 3 | 주의 확률에 대한 선형 근사치는 종점들 중 하나 또는 둘 모두가 소프트맥스의 포화 영역에 있는 경우에 특히 불량한 근사치이다. 단일 키만을 변화시킬 때, 소프트맥스는 그 키와 쿼리의 내적의 시그모이드가 된다는 점에 유의한다.\n' +
      '\n' +
      '{I}^{Q}_{\\text{APfix}(n;x^{text{clean},x^{noise}) :=\\sum_{k}\\hat{I}_{\\text{AP}}(\\text{attn}(n)_{k};x^{text{clean},x^{noise}) \\tag{6}\\[=(\\text{attn}(n)_{\\text{patch}-\\text{attn}(n)(x^{text{clean})) ^{\\intercal}\\frac{L}(\\mathcal}(n)}\\text{m}(x^{text{clean}})}{{m}(n)}\\text{m}(n)}\\text{m}(n)}\\text{m}(n)}\\text{m}(n)}\\text{m}(n)}\\text{m}(n)}\\text{m}(n)}\\text{m}(n)}\\text{m}(n)}\n' +
      '\n' +
      '키들에 대한 키들은 먼저 간단하지만 비효율적인 방법을 설명한다. 또한 잡음키를 캐슁하는 \\(x^{\\text{noise}}\\)에서 모델을 다시 실행하였다. 또한, 깨끗한 질의와 주의확률을 캐싱하여 x(x^{\\text{clean}}\\)에서 실행하였다. 하나의 어텐션 헤드에 대한 키 노드를 \\(n_{1}^{k},\\dots,n_{T}^{k}\\)으로 하고, \\(\\text{query}(n_{t}^{k})=\\{n_{1}^{q},\\dots,n_{T}^{q}\\})을 노드 \\(n_{t}^{k}\\)과 동일한 헤드에 대한 질의 노드 집합으로 한다. 그런 다음 정의합니다.\n' +
      '\n' +
      '\\text{attn}^{t}_{\\text{patch}(n^{q}):=\\text{attn}(n^{q})(x^{text{clean}}\\mid\\text{do}(n_{t}^{k}(x^{noise}))) \\tag{8}\\\\delta_{t}\\,\\text{attn}(n^{q}):=\\text{attn}^{t}_{\\text{patch}(n^{q})-\\text{attn}(n^{q})(x^{text{clean}}) \\tag{9}\\\\gets n_{t}^{k}(x^{noise}}))\n' +
      '\n' +
      '그리고 \\(n_{t}^{k}\\)에 대한 개선된 속성 추정치는 다음과 같다.\n' +
      '\n' +
      '[\\hat{I}^{K}_{\\text{APfix}(n_t}^{k};x^{text{clean},x^{noise}}):=\\sum_{n^{\\prime}\\text{query}(n_{t}^{k}}\\Delta_{t},\\text{attn}(n^{q})^{\\intercal}\\frac{\\partial\\mathcal{L}(\\mathcal{M}(x^{q}}}\\Big{|}_{\\text{attn}(n^{q})=\\text{attn}(n^{q}}),(x^{ \\text{clean}}}\\tag{10}\\text{t}\\text{t},\\text{attn}(n^{q}}}}\\text{m}(n^{q}}}}\\text{m}(n^{q}}}}\\text{t}\\text{t},\\text{attn}(n^{q}}\n' +
      '\n' +
      '그러나, 우리가 방금 설명한 절차는 모든 \\(T\\) 키에 대해 식 (9)를 순전히 계산하기 위해 O(\\(T^{3}\\)) 플롭을 필요로 하기 때문에 실행하는데 비용이 많이 든다. 부록 A.2.1에서 우리는 순방향 통과 주의 계산 자체(O(\\(T^{2}\\)) 플롭보다 더 많은 계산이 필요하지 않은 더 효율적인 변형을 설명한다. 식 (6)은 또한 순방향 패스보다 계산하는 것이 더 저렴하기 때문에, 완전한 QK 픽스는 2개 미만의 트랜스포머 순방향 패스를 필요로 한다(후자는 또한 MLP 계산을 포함하기 때문이다).\n' +
      '\n' +
      '어텐션 노드의 경우 그림 4(중간)에서 쿼리와 키 픽스를 적용한 효과를 보여준다. 우리는 Q/K 효과의 전파가 거짓 음수 비율을 줄이는 데 큰 영향을 미친다는 것을 관찰한다.\n' +
      '\n' +
      '######3.1.2 취소에 의한 위음성\n' +
      '\n' +
      '이러한 형태의 해제는 간접 효과로부터의 역전파 구배가 직접 효과로부터의 구배와 결합될 때 발생한다. 이 문제를 줄이기 위해 속성 패치 내에서 역전파를 수정하는 방법을 제안한다. 만약 우리가 간접효과에 기여하는 하류층에서 인위적으로 구배를 0으로 만들면, 상쇄가 방해를 받는다. (이것은 또한 층의 출력에서 깨끗한 활성화에서의 패칭과 동등하다.) 따라서 우리는 층을 가로질러 이것을 반복적으로, 스위핑할 것을 제안한다. 그래디언트-제로가 되는 층을 통해 효과가 라우팅되지 않는 임의의 노드는 그 추정치에 영향을 받지 않을 것이다.\n' +
      '\n' +
      '우리는 이 방법을 _GradDrop_라고 부른다. 모델의 모든 계층 \\(\\ell\\in\\{1,\\dots,L\\}\\)에 대해 GradDrop은 모든 노드에 대한 AtP 추정치를 계산하며, 여기서 \\(\\ell\\)의 잔차 기여도에 대한 기울기는 이전 계층으로의 전파를 포함하여 0으로 설정된다. 이렇게 하면 드롭된 각 계층에 대해 모든 노드에 대해 다른 추정치가 제공됩니다. 적하층 \\(\\ell\\,\\ell^{\\text{out}}\\gets n_{{\\text{out}}(x^{\\text{out}}))일 때, so-modified gradient \\(\\frac{\\partial\\,\\ell^{\\text{out}}=\\frac{\\partial\\,\\ell}{\\partial n}(\\mathcal{M}(x^{\\text{clean}}\\mid\\text{do}(n^{\\text{out}}\\gets n_{\\text{out}}(x^{\\text{clean}))이라고 한다. AtP 공식에서 \\(\\frac{\\partial\\,\\ell^{\\partial n}\\) 대신 \\(\\frac{\\partial\\,\\ell^{\\partial n}\\)을 사용하여 \\(\\hat{I}_{\\text{AP+GD}_{\\ell}(n)\\)의 추정치를 생성한다. 그 다음, 추정치들은 그들의 절대값들을 평균하여 집계되고, 그리고 나서 직접-효과 경로의 기여도를 변경하는 것을 피하기 위해 \\(\\frac{L}{L-1}\\)만큼 스케일링된다(그렇지 않으면 노드가 있는 층을 드롭할 때 제로아웃된다).\n' +
      '\n' +
      'n):=\\mathbb{E}_{x^{\\text{AP+GD}}(\\hat{\\varepsilon}_{\\text{clean}},x^{\\text{ noise}}\\left[\\frac{1}{L-1}\\sum_{\\ell=1}^{L}\\left|\\hat{I}_{\\text{AP+GD}_{\\ell}(n;x^{\\text{clean}},x^{\\text{noise}}}\\right|\\right]\\tag{11}\\trac{1}{L-1}\\sum_{\\ell=1}\\left|\\hat{I}_{\\text{AP+GD}_{\\ell}(n;x^{\\text{clean}},x^{text{noise}}}\\right|\\right]\\tag{11}\\trac{1}{L-1}\\sum_{\\ell=1}\\left|\\hat{I}_{\\text{AP+GD}_{\\ell}}(n;x^{\n' +
      '\n' +
      '계산시 필요한 순방향 패스는 \\(\\hat{I}_{\\text{AP+GD}_{\\ell}}(n;x^{\\text{clean}},x^{\\text{noise}})\\)에 의존하지 않으므로, GradDrop에 필요한 추가 계산은 깨끗한 순방향 패스에서 동일한 중간 활성으로부터 \\(L\\) 역방향 패스이다. 또한 QK 픽스의 경우도 마찬가지이다. 수정된 속성\\(\\hat{I}_{\\text{APfix}\\)은 주의력 기울기가 있는 내적이므로, \\(\\hat{I}_{\\text{APFix+GD}_{\\ell}(n)\\)에 대해 다시 계산해야 하는 것은 수정된 기울기\\(\\frac{\\partial\\mathcal{L}^{\\ell}{\\partial\\text{attn}(n)\\)이다. 따라서 식 (11)을 계산하면 AtP에 대한 비용 상위에 \\(L\\)이 4만큼 거꾸로 전달된다.\n' +
      '\n' +
      '각주 4: 이것은 중간 결과를 재사용함으로써 \\((L+1)/2\\)로 감소될 수 있다.\n' +
      '\n' +
      '그림 4(오른쪽)의 주의 노드와 그림 5의 MLP 노드에 GradDrop을 적용한 결과를 보여준다. 그림 5에서는 AtP+GradDrop 순위에 대한 진정한 효과 크기 순위를 보여주면서 GradDrop을 적용하여 급격히 개선된 노드를 강조한다. 우리는 부록 A.2.2에서 GradDrop의 이익에 대한 몇 가지 주장과 직관을 제공한다.\n' +
      '\n' +
      '직접효과비(Direct Effect Ratio)는 관측된 위음성이 취소에 의한 것이라는 몇 가지 증거를 제공하기 위해 직접효과\\(c^{\\text{direct}}(n)\\)와 총효과\\(c(n)\\) 사이의 비율을 계산한다. 직접 효과 비율이 높을수록 더 많은 상쇄를 나타낸다. 그림 5에서 GradDrop에 의해 수정된 가장 유의미한 위음성은 5.35, 12.2 및 0(직접 효과가 없음)의 높은 직접 효과 비율을 갖는 반면, 모든 노드의 중간 직접 효과 비율은 0(모든 노드를 계수하는 경우) 또는 0.77(직접 효과가 있는 노드를 계수하는 경우에만)임을 관찰한다. 직접 효과 비율은 노드에만 적용된다는 점에 유의하십시오.\n' +
      '\n' +
      '그림 4: CITY-PP의 피티아-12B에서 \\(\\hat{\\varepsilon}_{\\text{AP}(n)\\)의 순위에 대한 \\(|\\) c(n)\\의 순위. AtP에 대한 두 개선 모두 위음수(아래 오른쪽 삼각형 영역)의 수를 줄이며, 이 경우 대부분의 개선은 QK 수정에서 비롯된다. 색채는 \\(x^{\\text{clean}}\\)을 비교하고 주어진 질의 또는 키를 패치할 때 주의 확률의 최대 절대 차이를 나타낸다. 많은 위음성은 주의 확률에서 가장 큰 차이를 보이는 키와 질의로 그림 3과 같이 주의 포화도에 의한 것임을 알 수 있다. 출력 노드와 값 노드는 주의 확률에 기여하지 않아 회색으로 채색된다.\n' +
      '\n' +
      '사실상, 모든 단절된 노드들은 정의에 의해 0의 직접적인 효과를 갖기 때문에, 비-최종 토큰 위치들에서의 MLP 노드들에 대한 것이 아니라 출력에 대한 직접적인 연결을 갖는다.\n' +
      '\n' +
      '### Diagnostics\n' +
      '\n' +
      '섹션 3.1에서 제안한 개선 사항에도 불구하고 AtP*가 위음성을 생성하지 않는다는 보장은 없다. 따라서, AtP*에 의해 놓칠 수 있는 노드, 즉 일부 \\(K\\) AtP* 추정치에 있지 않은 노드의 효과 크기에 대한 상위 신뢰 한계를 얻는 것이 바람직하다. 상단(K) 노드를 \\(\\mathrm{Top}_{AtP*}^{K}\\)으로 하자. 우리는 그러한 경계를 얻기 위해 부분 집합 샘플링을 사용할 수 있다.\n' +
      '\n' +
      '알고리즘 1과 섹션 3.3에 설명된 바와 같이, 부분 집합 샘플링 알고리즘은 각 노드에 대한 요약 통계량(\\tilde{i}_{\\pm}^{n}\\), \\(s_{\\pm}^{n}\\) 및 \\(\\mathrm}^{n}\\)을 반환한다. (n\\): 노드에서 조건부 부분 집합의 평균 효과 크기(\\tilde{i}_{\\pm}^{n}\\), 표본 표준 편차(s_{\\pm}^{n}\\), 표본 크기(\\mathrm}^{n}\\). 이 때, 귀무가설 5\\(H_{0}^{n}\\)는 \\(left|\\mathcal{I}\\left(n\\right)\\right|\\geq\\theta\\), 대체 가설 \\(H_{1}^{n}\\)은 \\(left|\\mathcal{I}\\tilde{i}-\\tilde{i}-\\tilde{i}-\\tilde{i}-\\tilde{i}-\\tilde{welch}}^{n}}(T>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t>t> t^{n}))\\).\n' +
      '\n' +
      '각주 5: 이것은 \\(H_{0}\\)의 파격적인 형태입니다 – 일반적으로 귀무가설은 효과가 미미하다고 말할 것입니다. 그러나 통계적 가설 검증의 틀은 데이터가 귀무 가설을 기각하도록 하는지 여부를 결정하는 데 기초하며, 이 경우 우리가 기각하고자 하는 가설은 상당한 위음의 부재보다는 존재이다.\n' +
      '\n' +
      '각주 6: 이것은 모집단이 대략 편향되지 않고 정규 분포를 따르고 편향되지 않는 것에 의존한다. 이것은 검사에서는 사실인 경향이 있으며, 추가성 가정(섹션 3.3 참조)이 단일 프롬프트 쌍에 대해 예측하는 것입니다. 그러나 비모수 부트스트랩 테스트는 추가 계산을 위해 더 신뢰할 수 있습니다.\n' +
      '\n' +
      '(N\\setminus\\mathrm{Top}_{AtP*}^{K}\\)의 모든 노드에서 결합된 결론을 얻기 위해, N\\cap\\mathrm{Top}_{AtP*}^{K}\\)에 대한 가설 \\(H_{0}=\\bigvee_{n\\in N\\cap\\mathrm{Top}_{AtP*}^{K}\\), H_{0}^{n\\\\(H_{0}^{n}\\)의 모든 노드에서 임의의 노드가 참효과를 갖는다는 가설 \\(\\left|\\mathcal{I}\\left(n\\right)\\right>\\theta\\)을 생각해 보자. 이 역시 복합 귀무가설이므로 \\(\\max_{n}p^{n}\\)은 해당 \\(p\\)-값이다. 그런 다음…과 함께 상한 신뢰 구간을 찾습니다.\n' +
      '\n' +
      '그림 5: NeuronNodes가 있는 CITY-PP 분포에서 Pythia-12B를 사용하여 GradDrop 유무에 관계없이 AtP 추정치의 참 순위 및 순위. GradDrop은 Default AtP(주황색 교차)에 비해 가장 큰 뉴런 위음수(빨간색 원)에 상당한 개선을 제공한다.\n' +
      '\n' +
      '특정 신뢰 수준 \\(1-p\\), 우리는 이 절차를 뒤집어서 최소한 그 정도의 신뢰 수준을 가지고 있는 가장 낮은 \\(\\theta\\)을 찾는다. 우리는 알고리즘 1의 표본 크기 \\(m\\)의 다양한 설정에 대해 이것을 반복한다. 정확한 알고리즘은 부록 A.3에 설명되어 있다.\n' +
      '\n' +
      '그림 6에서는 주어진 \\(m\\)(오른쪽 서브플롯)을 사용하여 알고리즘 1을 실행한 신뢰 수준 90%, 99%, 99.9%에서 신뢰 상한과 \\(\\theta\\)(왼쪽 서브플롯)보다 큰 진정한 기여 \\(c(n)\\)를 갖는 노드의 수를 보고한다.\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '반복적인 가장 간단한 방법은 각 노드의 참효과 \\(c(n)\\)를 어떤 알려지지 않은 무작위 순서로 찾기 위해 Activation Patching을 직접 수행하는 것이다. 이것은 반드시 비효율적이다.\n' +
      '\n' +
      '그러나 분포로 스케일링하는 경우, 검증되지 않은 각 노드에 대해 (i)의 위상을 번갈아 가면서, 패치할 측정되지 않은 프롬프트 쌍을 선택하고, (ii) 검증되지 않은 노드를 평균 관찰된 패치 효과 크기로 순위를 매기고, 상위 \\(|N|/|\\mathcal{D}|\\) 노드를 선택하여 검증함으로써, 이를 개선할 수 있다. 이것은 두 작업에 대한 계산적 지출의 균형을 맞추고, 적어도 많은 프롬프트 쌍에 큰 효과가 나타나는 한 더 빨리 큰 노드를 찾을 수 있게 한다.\n' +
      '\n' +
      '나머지 기준선 방법들은 노드들의 집합에 개입할 때 측정된 효과\\(\\mathcal{I}(\\eta;x^{\\text{clean}},x^{\\text{noise})\\)가 \\(\\sum_{n\\in\\eta}\\mathcal{I}(n;x^{\\text{clean},x^{\\text{noise}})와 거의 같다는 근사 _node additivity 가정에 의존한다.\n' +
      '\n' +
      '근사적인 노드 가산성 가정 하에서, 우리는 \\(c(n)\\)의 대략적인 편향되지 않은 추정량을 구성할 수 있다. 우리는 각 노드를 독립적으로 포함할 집합 \\(\\eta_{k}\\)을 선택한다.\n' +
      '\n' +
      '그림 6: 위음성의 효과 크기에 대한 상위 신뢰 한계(즉, AtP\\({}^{*}\\)에 따라 상위 1024개 노드에 없는 노드)는 3개의 신뢰 수준에서 샘플링 예산을 변경한다. 왼쪽에서는 AtP\\({}^{*}\\)에 의해 가장 높은 순위에 있는 노드들의 참효과를 빨간색으로 보여준다. 우리는 또한 주황색으로 나머지 노드의 다양한 순위에서의 실제 효과 크기를 보여준다.\n' +
      '\n' +
      '일부 확률\\(p\\)과 추가적으로 샘플 프롬프트 쌍\\(x_{k}^{\\text{clean},x_{k}^{\\text{noise}\\sim\\mathcal{D}\\)을 가질 수 있다. 임의의 노드\\(n\\)과 노드\\(\\eta_{k}\\subset N\\)에 대하여, \\(\\eta^{+}(n)\\)을 포함하는 모든 노드들의 집합으로 하고, \\(\\eta^{-}(n)\\)을 포함하지 않는 노드들의 집합으로 하고, 이 노드 집합들을 \\(\\eta_{k}^{+}(n)\\)과 \\(\\eta_{k}^{-}(n)\\)으로 하고, 대응하는 프롬프트 쌍을 \\(x_{k}^{text{clean}^{+}(n),x_{k}^{clean}^{-}(n),x_{k}^{noise}^{-}(n)\\)과 \\(x_{k}^{clean}^{-}(n)\\)으로 한다. 상기 서브샘플링(또는 서브세트 샘플링) 추정기는 다음에\n' +
      '\n' +
      'cal{I}(\\frac{1}{|\\eta^{+}(n);x_{k}^{text{clean}^{+}(n);x_{k}^{text{noise}^{-}(n))}\\mathcal{I}(\\hat{\\mathcal{I}}(n))|\\tag{13}(n)\n' +
      '\n' +
      '추정량\\(\\hat{\\mathcal{I}}_{\\text{SS}}(n)\\)는 상호작용 효과가 없는 경우 편향되지 않으며, 간단한 상호작용 모델(증명은 섹션A.1.1 참조)에서 \\(p\\)에 비례하는 작은 편향을 갖는다.\n' +
      '\n' +
      '실제로 i.i.d. Bernoulli\\({}^{|N|}(p)\\)에서 모든 노드에 대해 이진 마스크를 샘플링하여 모든 추정치\\(\\hat{\\mathcal{E}}_{\\text{SS}(n)\\)을 계산한다. 각 이진 마스크는 노드 집합\\(\\eta\\)으로 식별할 수 있다. 알고리즘1에서는 모든 노드 \\(n\\in N\\)에 대해 수학식 13과 관련된 요약 통계량을 효율적으로 계산하는 방법을 설명한다. 평균(\\tilde{i}^{\\pm}\\)은 \\(\\hat{\\mathcal{E}}_text{SS}(n)\\)을 계산하기에 충분하고, 다른 요약 통계는 위음수(cf. Section3.2)의 크기를 경계짓는 데 관여한다. (주, \\(\\text{count}_{n}^{\\pm}\\)은 \\(|\\eta^{\\pm}(n)|\\)에 대한 대체 표기일 뿐이다)\n' +
      '\n' +
      '```\n' +
      '0:\\(p\\in(0,1)\\), 모델\\(\\mathcal{M}\\), 메트릭\\(\\mathcal{L}\\), 프롬프트 쌍 분포\\(\\mathcal{D}\\), num 샘플\\(m\\)\n' +
      '1:count\\({}^{\\pm}\\), runSum\\({}^{\\pm}\\), runSquaredSum\\({}^{\\pm}\\gets 0^{|N|}\\)\\(\\triangleright\\) 초기 카운트 및 실행 합을 0 벡터로\n' +
      '2:for\\(i\\gets 1\\ to \\(m\\)do\n' +
      '3:\\(x^{\\text{clean}},x^{\\text{noise}}\\sim\\mathcal{D}\\)\n' +
      '4:\\(\\text{mask}^{+}\\leftarrow\\text{Bernoulli}^{|N|}(p)\\)\\(\\triangleright\\) 패치용 샘플 바이너리 마스크\n' +
      '5:\\(\\text{mask}^{-}\\gets 1-\\text{mask}^{+}\\)\n' +
      '6:\\(i\\gets I(\\{n\\in N:\\text{mask}_{n}^{+}=1\\};\\times x^{\\text{clean}},x^{ \\text{noise}})\\)\\(\\triangleright\\)\\(\\eta^{+}=\\{n\\in N:\\text{mask}_{n}^{+}=1\\}\\)\n' +
      '7:count\\({}^{\\pm}\\leftarrow\\text{count}^{\\pm}+\\text{mask}^{\\pm}\\)\n' +
      '8:runSum\\({}^{\\pm}\\leftarrow\\text{runSum}^{\\pm}+i\\cdot\\text{mask}^{\\pm}\\)\n' +
      '9:runSquaredSum\\({}^{\\pm}\\leftarrow\\text{runSquaredSum}^{\\pm}+i^{2}\\cdot\\text{mask}^{\\pm}\\)\n' +
      '10:\\(\\tilde{i}^{\\pm}\\leftarrow\\text{runSum}^{\\pm}/\\text{count}^{\\pm}\\)\n' +
      '11:\\(\\tilde{s}^{\\pm}\\leftarrow\\sqrt{(\\text{runSquaredSum}^{\\pm}-(\\tilde{i}^{\\pm})^ {2})/(\\text{count}^{\\pm}-1)}\\)\n' +
      '12:return\\(\\text{count}^{\\pm}\\), \\(\\tilde{i}^{\\pm}\\), \\(s^{\\pm}\\)\\(\\triangleright\\) 진단이 필요하지 않으면 \\(\\tilde{i}^{\\pm}\\)으로 충분하다.\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 서브샘플링\n' +
      '\n' +
      '블록과 계층 구조는 각 블록들을 독립적으로 샘플링하는 대신, 노드들을 일정한 크기의 고정된 "블록들"로 그룹화하고, 각 블록들을 패치하여 집합된 기여도\\(c(\\eta)\\)를 찾을 수 있다. 그리고, 높은 기여도 블록부터 시작하여 노드들을 순회할 수 있다.\n' +
      '\n' +
      '블록 크기 측면에서 절충이 있다: 큰 블록을 사용하는 것은 높은 기여 블록을 횡단하는 데 필요한 계산량을 증가시키지만, 작은 블록을 사용하는 것은 모든 블록을 횡단하는 것을 완료하는 데 필요한 계산량을 증가시킨다. 우리는 고정 블록 크기 설정을 _Blocks_라고 한다. 이 트레이드오프를 처리하는 또 다른 방법은 재귀(recursion)를 추가하는 것이다: 블록들은 상위 레벨 블록들로 그룹화될 수 있고, 등등. 우리는 이 방법을 _Hierarchical_라고 부른다.\n' +
      '\n' +
      '비교도표에서 두 방법의 결과를 제시하지만 세부사항을 섹션A.1.2로 축소한다. 서브샘플링과 관련하여 이러한 그룹화 기반 방법은 분포에서 노드 7의 수에 따른 스케일링 외에도 \\(\\mathcal{D}\\)의 지원 크기에 따라 비용 척도가 선형적으로 확장된다는 단점이 있다.\n' +
      '\n' +
      '각주 7: AtP*도 같은 방식으로 선형으로 확장되지만 프롬프트 쌍당 훨씬 적은 전진 패스로 확장됩니다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Setup\n' +
      '\n' +
      '모델 동작을 컴포넌트에 귀속시킬 때 중요한 선택은 모델의 계산 그래프를 분석 단위 또는 \'노드\' \\(N\\ni n\\)(cf. Section 2.1)로 분할하는 것이다. 우리는 \\(N\\), _AttentionNodes_ 및 _NeuronNodes_의 두 가지 설정을 조사한다. 뉴런노드의 경우, 각각의 MLP 뉴런8은 별개의 노드이다. AttentionNodes의 경우, 각 head에 대한 질의, 키, 값 벡터뿐만 아니라 pre-linear per-head attention output9를 고려하며, 이러한 단위를 \'사이트\'라고 한다. 각 사이트에 대해 서로 다른 토큰 위치에 있는 해당 사이트의 각 사본을 별도의 노드로 간주한다. 그 결과, TokenPosition \\(\\times\\) Site로부터 pair \\((T,S)\\)으로 각 노드 \\(n\\in N\\)을 식별할 수 있었다. 본 논문에서 제안한 \\(N\\)에 대한 두 가지 설정은 서로 다른 수준의 입도를 사용하고 있으며 노드당 효과 크기가 다를 것으로 예상되므로 이에 대한 결과를 별도로 제시한다.\n' +
      '\n' +
      '각주 8: 우리는 노드에 대해 뉴런 사후 활성화를 사용한다; 이것은 인과적으로 개입할 때 차이가 없지만 AtP의 경우 \\(n\\mapsto\\mathcal{L}(n)\\) 함수를 더 선형적으로 만들기 때문에 유익하다.\n' +
      '\n' +
      '각주 9: 특히 그의 질의들이 그의 키들 및/또는 값들에 대해 무시할 수 있는 패치 효과들을 갖는 경우에 주의 헤드가 어떤 기능을 제공하고 있는지에 대한 추가 정보를 제공하기 때문에 출력 노드를 포함한다. 이는 질의가 프롬프트에 따라 차이가 없도록 \\(x^{\\text{clean}}\\), \\(x^{\\text{noise}}\\)을 선택한 결과로 발생할 수 있다.\n' +
      '\n' +
      'ModelsWe investigate transformer language models from the Pythia suite (Biderman et al., 2023) of sizes between 410M and 12B parameters. 이를 통해 우리의 방법이 규모에 걸쳐 적용 가능함을 입증할 수 있다. 그림 1, 7 및 8의 검증 회수 비용 그림은 피티아-12B를 나타낸다. 다른 모델 크기에 대한 결과는 상대 비용(cf. 섹션 4.2)을 통해 표시된다. 본문의 그림 9와 부록 B.3의 검증 비용 리콜을 통해 분해된다.\n' +
      '\n' +
      'Effect Metric \\(\\mathcal{L}\\) 모든 보고된 결과는 음의 로그 확률 10을 손실 함수 \\(\\mathcal{L}\\)로 사용한다. 우리는 깨끗한 프롬프트(x^{\\text{clean}}\\)로부터 목표물에 대한 \\(\\mathcal{L}\\)을 계산한다. 부록 B.4의 다른 메트릭을 간략하게 탐색한다.\n' +
      '\n' +
      '각주 10: 또 다른 인기 있는 메트릭은 클린 타겟과 노이즈 타겟 사이의 로짓의 차이이다. 음의 로그프로브와 달리 로짓 차이는 최종 로짓에서 선형이므로 AtP를 선호할 수 있다. 로짓 차이의 단점은 노이즈 대상에 민감하다는 것인데, 이는 IOI에서와 같이 여러 개의 그럴듯한 완성도가 있는 경우 의미가 없을 수 있다.\n' +
      '\n' +
      '###효과성 및 효율성 측정\n' +
      '\n' +
      '서론에서 언급한 바와 같이 검증된 재현율의 비용은 주로 가장 큰 효과 노드를 찾는 데 관심이 있으며 모델 및 분포에 걸친 \\(c(n)\\)의 분포는 부록 D를 참조하십시오. 주어진 방법을 통해 노드 추정치를 얻으면 한 번에 하나씩 상위 노드의 실제 효과를 직접 측정하는 것이 상대적으로 저렴하며 이를 "검증"이라고 한다. 이것을 우리의 방법론에 통합하면, 우리는 거짓 긍정이 일반적으로 큰 문제가 아니라는 것을 발견한다; 그들은 검증 중에 간단히 드러난다. 대조적으로, 위음성은 우리가 피하려고 했던 모든 노드를 검증하지 않고는 구제하기가 그리 쉽지 않다.\n' +
      '\n' +
      '본 논문에서는 실제 효과크기가 가장 큰 \\(K\\) 노드를 검증하기 위해 총 계산비용(순방향 통행의 #)을 기준으로 방법을 비교한다. 측정되는 절차는 먼저 추정치를 계산하고(추정 비용을 발생), 추정된 크기의 순으로 노드를 스윕하여 개별 효과를 측정(c(n)\\)(즉, 검증)하고 검증 비용을 발생시킨다. 그러면 총 비용은 이 두 비용의 합입니다.\n' +
      '\n' +
      '역순위 가중 기하 평균 비용은 때때로 스칼라로 방법 성능을 요약하는 것이 유용하다는 것을 발견하는데, 이는 다른 설정(예: 그림 2에서와 같이 모델 크기)에 걸쳐 방법을 한 눈에 비교하거나 하이퍼파라미터(cf. 부록 B.5)를 선택하는 데 유용하다. 상단(K\\) 노드에 대한 재현율 검증 비용은 다양한 크기에서 관심의 대상이 된다. 성능 메트릭이 작거나 큰 \\(K\\)에 의해 지배되는 것을 피하기 위해, 우리는 상위 \\(K\\) 노드의 비용에 대해 가중치 \\(1/K\\)을 갖는 가중 평균을 사용한다. 유사하게, 비용 자체는 크기가 다를 수 있기 때문에 로그 척도로 평균을 내는데, 즉 기하 평균을 취한다.\n' +
      '\n' +
      '이 메트릭은 또한 그림 1과 같은 도표에서 곡선 아래의 면적에 비례한다. 더 이해할 수 있는 결과를 생성하기 위해, 우리는 항상 동일한 메트릭 상의 오라클 검증 비용에 대해 보고하며(즉, 대각선은 오라클이고, 상대 비용은 1이다. 이를 IRWRGM(역 순위 가중 상대 기하 평균) 비용 또는 상대 비용으로 지칭한다.\n' +
      '\n' +
      '개별 실무자의 선호도는 이 메트릭이 더 이상 중요한 순위 레짐을 정확하게 측정하지 않도록 상이할 수 있다는 점에 유의한다. 예를 들어, AtP*는 AtP 또는 AtP+QKfix에 비해 주목할만한 초기 비용을 지불하며, 이는 추가 위음성을 찾지 못할 때 불리하게 설정하지만, 이는 실질적으로 중요할 수도 있고 그렇지 않을 수도 있다. 성능을 더 자세히 이해하려면 그림 1(부록 B.3)과 같이 검증된 재현율 그림의 비용을 참조하는 것이 좋다.\n' +
      '\n' +
      '### 단일 프롬프트 쌍 대 분포\n' +
      '\n' +
      '우리는 단일 프롬프트 쌍에 많은 실험을 집중한다. 이는 주로 지상 진실 데이터를 설정하고 얻는 것이 더 쉽기 때문입니다. 또한 질문을 조사할 수 있는 더 간단한 설정이며 일반화할 분포가 항상 사용 가능한 것은 아니기 때문에 보다 보편적으로 적용할 수 있는 설정이다.\n' +
      '\n' +
      '클리닝 단일 프롬프트 쌍들은 우리가 비교적 깨끗한 회로11을 가질 것으로 기대하는 단일 프롬프트 쌍들에 대한 결과들을 보고한다. 모든 단일 프롬프트 쌍들은 표 1에 도시된다. IOI-PP는 주로 주의 헤드들을 포함하는 태스크인 간접 객체 식별(IOI) 태스크(Wang et al., 2022)로부터 인스턴스와 유사하도록 선택된다. CITY-PP는 이전의 연구가 초기 MLP 및 소수의 후기 주의 헤드(Geva et al., 2023; Meng et al., 2023; Nanda et al., 2023)를 포함하는 사실적 리콜을 이끌어내기 위해 선택된다. 국가/도시 조합은 Pythia-410M이 \\(x^{\\text{clean}}\\)과 \\(x^{\\text{noise}}\\) 모두에서 낮은 손실을 달성하고 모든 장소가 단일 토큰으로 표현되도록 선택되었다.\n' +
      '\n' +
      '각주 11: 형식적으로는 델타 분포(p(x^{\\text{clean},x^{\\text{noise})=\\delta_{x^{\\text{clean}}_{1}}x^{\\text{noise}}_{1}(x^{\\text{clean},x^{\\text{noise}}))를 통한 프롬프트 분포를 나타내며, 여기서 \\(x^{\\text{clean}_{1},x^{\\text{noise}}_{1}\\은 특이 프롬프트 쌍이다.\n' +
      '\n' +
      'CITY-PP의 경우 NeuronNodes, IOI-PP의 경우 AttentionNodes에 초점을 맞춘 그림 1에서 다양한 방법에 대해 검증된 100% 리콜 비용을 보여준다. 더 작은 피티아 모델에 대한 철저한 결과는 부록 B.3에 나와 있다. 그림 2는 CITY-PP 및 IOI-PP의 모든 모델에 대한 집계된 상대 비용을 보여준다.\n' +
      '\n' +
      '모든 중요한 노드를 회상하는 엄격한 기준을 적용하는 대신, 우리는 또한 이러한 제약을 완화할 수 있다. 그림 7에서 두 가지 깨끗한 프롬프트 쌍 설정에서 확인된 90% 리콜 비용을 보여준다.\n' +
      '\n' +
      '랜덤 프롬프트 쌍 이전 프롬프트 쌍은 사실 최상의 시나리오일 수 있으며, 이들이 생성하는 개입은 특정 회로에 상당히 국한될 것이며, 이는 AtP가 기여도를 근사화하는 것을 용이하게 할 수 있다. 따라서 방법이 개입이 덜 외과적인 설정으로 일반화되는 방법을 보는 것이 유익할 수 있다. 이를 위해, 우리는 또한 RAND-PP라고 지칭하는 The Pile(Gao et al., 2020)의 비저작권 보호 섹션으로부터 선택된 랜덤 프롬프트 쌍에 대한 도 8(상단) 및 도 9의 결과를 보고한다. 피티아-410M이 두 프롬프트에서 여전히 낮은 손실을 달성하도록 프롬프트 쌍을 선택했다.\n' +
      '\n' +
      '우리는 AtP/AtP*가 여기에서 다소 덜 효과적이라는 것을 발견했으며, 이는 AtP/AtP*의 강력한 성능이 특히 선명한 회로를 사용하는 깨끗한 프롬프트나 정확한 제어인 노이즈 프롬프트에 의존하지 않는다는 잠정적인 증거를 제공한다.\n' +
      '\n' +
      '분포의 인과적 속성은 섹션 2에 제시된 바와 같이 분포에 걸쳐 평가될 때 종종 가장 흥미롭다. 방법 중 AtP, AtP* 및 서브샘플링 척도는 분포에 합리적으로 평가되며, 전자 2는 값이 저렴하기 때문에 \\(|\\mathcal{D}|\\) 시간을 실행하는 것이 금지되지 않으며, 서브샘플링은 본질적으로 분포에 걸쳐 평균을 내고 따라서 활성화 패칭을 통한 검증에 비해 비례적으로 저렴해지기 때문이다. 또한, 분포를 갖는 것은 섹션 3.3에 기술된 바와 같이, 보다 수행적인 반복적 방법을 가능하게 한다.\n' +
      '\n' +
      '우리는 두 가지 분포 설정에서 이러한 방법의 비교를 제시한다. 첫 번째는 IOI (Wang et al., 2022)를 6개의 이름으로 축소한 버전으로, \\(6\\times 5\\times 4=120\\)의 프롬프트 쌍을 생성하여 AttentionNodes를 평가한다. 다른 분포는 모델이 뉴런노드를 평가하는 부정관사 \'a\' 또는 \'an\'을 출력하도록 프롬프트한다. 이러한 분포를 구성하는 자세한 내용은 부록 B.1을 참조하십시오. 결과는 피티아 12B에 대해 그림 8에, 모델에 걸쳐 그림 9에 나와 있다. 결과는 AtP가 특히 QK 픽스와 함께 양호하게 계속 수행됨을 보여준다; 또한, 취소 실패 모드는 특정 입력 프롬프트 쌍에 민감한 경향이 있고, 그 결과, 전체에 걸쳐 평균화하는\n' +
      '\n' +
      '그림 7: 90% 목표 재현율과 함께 깨끗한 프롬프트 쌍에서 다른 방법을 사용하여 피티아-12B에서 가장 인과적으로 중요한 노드를 찾는 비용. 이것은 그림 1의 AtP* 위음성이 소수의 노드임을 강조한다.\n' +
      '\n' +
      '도 8 | 다른 방법을 사용하여 피티아-12B에서 가장 인과적으로 중요한 노드를 랜덤 프롬프트 쌍(표 1 참조) 및 분포에서 찾는 비용. 음영은 기하학적 표준 편차를 나타냅니다. 비용은 분배 케이스의 프롬프트 쌍당 전진 패스 또는 전진 패스로 측정됩니다.\n' +
      '\n' +
      '그림 9 | 랜덤 프롬프트 쌍 및 분포에 대한 모델 간 방법의 비용입니다. 비용은 오라클(따라서 진정한 기여 크기의 감소 순서로 노드를 확인하는 것)을 갖는 것과 관련이 있으며 역순위 가중 기하 평균을 사용하여 집계된다. 이는 그림 8의 각 곡선에 대한 대각선 위의 영역에 해당함을 의미한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '맥락 - 이것은 성과 문제를 제기하지 않으며, 따라서 우리의 작업은 여기서 어떤 이익도 제공하지 않을 것이다.\n' +
      '\n' +
      '이 연구에서 중요도 척도로 \\(c(n)\\)의 동굴은 식 (1)과 같이 활성화 패칭의 지상 진리를 평가 대상으로 삼았다. McGrath et al.(2023)에 의해 논의된 바와 같이, 식 (1)은 종종 다른 평가 목표인 "직접 효과"에 크게 동의하지 않는데, 이는 나중에 구성 요소가 더 일찍 패치된 구성 요소를 보상하기 위해 행동을 이동할 때 일부 기여에 더 낮은 가중치를 둡니다. 최악의 경우 이것은 우리의 측정 기준에 의해 설명되지 않은 추가 거짓 음을 생성하는 것으로 볼 수 있다. 어느 정도 이것은 식에서 GradDrop 공식에 의해 완화될 가능성이 있다. (11)은 그 하류 이동의 효과를 제거하는 항을 포함할 것이다.\n' +
      '\n' +
      '그러나, 우리가 고-직접-효과 노드를 찾는 것에 관심을 가져야 하는지도 의문이다. 예를 들어, 직접 효과는 노스탈 대수학자(2020)가 탐구한 바와 같이 모든 노드에 대해 효율적으로 계산하기 쉬우므로 직접 효과가 관심의 양이라면 AtP와 같은 빠른 근사치가 필요하지 않다. 그러나 직접적인 효과는 인과적으로 중요한 노드를 찾기 위한 도구로서 더 제한적이기 때문에 계산의 용이성은 무료 점심 식사가 아니다. 예를 들어, 비-최종 토큰 위치들에서 노드들로부터 직접적인 영향이 없다. 우리는 섹션 3.1.2와 부록 A.2.2에서 직접 효과에 대해 더 논의한다.\n' +
      '\n' +
      '우리의 지상-진실 정의의 또 다른 뉘앙스는 분포 설정에서 발생한다. 일부 노드들은 실제적이고 유의미한 효과를 가질 수 있지만, 단지 단일 클린 프롬프트(예를 들어, 이들은 IOI12 내의 특정 이름 또는 A-AN 내의 객체에만 응답함)에 대해서만 응답한다. 효과가 분포에 대해 평균화되기 때문에, 지상 진리는 이러한 노드들을 큰 인과적 중요성을 부여하지 않을 것이다. 실무자의 목표에 따라 이것이 바람직할 수도 있고 바람직하지 않을 수도 있다.\n' +
      '\n' +
      '각주 12: 우리는 몇 가지 사례에서 이러한 특정 행동을 관찰했습니다.\n' +
      '\n' +
      '다양한 추정기의 성능을 평가할 때 효과 크기 대 순위 추정의 상대적 순위를 평가하는 데 중점을 두었으며, 주요 목표는 중요한 구성 요소를 식별하는 것이었고(이를 위해 도구적으로만 유용한 효과 크기), 효과 크기를 아는 것이 중요한 상황에서 한 번에 가장 높은 추정 효과를 가진 노드에 대한 추가 검증 단계를 가정했다. 따라서 AtP 또는 AtP*에서 추정된 효과 크기가 지상 진실과 얼마나 밀접하게 일치하는지 여부에 대한 증거를 제시하지 않는다. 유사하게, 우리는 검증 과정을 통해 걸러낼 수 있기 때문에 분석에서 거짓 양성의 유병률을 평가하지 않았다. 마지막으로, 이전 작업에서 수행된 것처럼, 우리의 방법이 인간 연구자들에 의해 발견된 것과 인과적으로 중요한 동일한 노드를 발견하는지 확인하기 위해 과거의 수동 해석 가능성 작업과 비교하지 않았다(Conmy et al., 2023; Syed et al., 2023).\n' +
      '\n' +
      'Pythia 모델 패밀리(Biderman et al., 2023)에 대한 우리의 결과가 다른 LLM 패밀리로 전달될 가능성이 있다고 생각하지만, 특히 표준 디코더 전용 변압기 아키텍처에서 상당히 벗어난 SotA 규모 모델 또는 모델에 대한 추가 증거 없이는 질적으로 다른 행동을 배제할 수 없다.\n' +
      '\n' +
      '### Extensions/Variants\n' +
      '\n' +
      '에지 패칭은 개별 노드의 효과를 계산하는 데 초점을 맞추지만, 에지 활성화 패칭은 계산 그래프의 어떤 경로에 대한 더 미세한 정보를 제공할 수 있다. 그러나, 그것은 순진하게 행해질 경우 전방 패스의 수에서 훨씬 더 큰 폭발로 고통받는다. 다행히도, AtP는 노드들(Nanda, 2022; Syed et al., 2023) 사이의 에지의 효과를 추정하는 것으로 일반화하기 쉬운 반면, AtP*는 추가적인 개선을 제공할 수 있다. 에지-AtP와 부록 C.2에서 AtP*의 통찰력을 효율적으로 전달하는 방법에 대해 논의한다.\n' +
      '\n' +
      'Coarser 노드 \\(N\\)는 전체 레이어나 슬라이딩 윈도우가 아닌 세밀한 속성에 초점을 맞추었다 (Geva et al., 2023; Meng et al., 2023). 후자의 경우 해결해야 할 계산적 폭발이 적지만 긴 맥락의 경우 우리와 같은 속도 향상을 고려하는 데 여전히 이점이 있을 수 있으며, 반면 선형적이지 않을 수 있으므로 AtP*보다 다른 방법을 선호한다. 우리는 이것에 대한 조사를 미래의 일에 맡긴다.\n' +
      '\n' +
      '레이어 정규화난다(2022)는 레이어 정규화에 대한 AtP* 근사치가 더 큰/더 거친 노드를 패치하는 경우 더 나쁜 근사치일 수 있음을 관찰했다: 평균적으로 패치된 및 깨끗한 활성화는 유사한 규범을 가질 가능성이 높지만 코사인 유사성은 높지 않을 수 있다. 그들은 계층 정규화의 분모를 예를 들어 구현에서 정지 구배 연산자를 사용하여 고정된 것으로 처리할 것을 권장한다. 부록 C.1에서 우리는 이것의 효과를 조사하고 이 대체 형태의 AtP의 행동을 설명한다. 이 변형은 특히 잔차 스트림 노드를 패치할 때 더 나은 결과를 생성할 가능성이 있는 것으로 보이지만 이에 대한 경험적 조사는 향후 작업에 맡긴다.\n' +
      '\n' +
      'DenoisingDenoising(Lieberum et al., 2023; Meng et al., 2023)은 패치를 위한 다른 사용 케이스로서, 중간 정도의 다른 결과를 생성할 수 있다: 차이는 각 순방향 패스가 \\(x^{\\text{clean}}\\)에서 가져온 패치로의 활성화와 함께 \\(x^{\\text{noise}}\\)에서 실행된다는 것이다. 이것은 패치가 필요한 것이 아니라 \\(x^{\\text{clean}\\)에서 모델 성능을 회복하기에 충분한지를 테스트한다. 부록 B.4에서 이 선택의 효과에 대한 몇 가지 예비 증거를 제공하지만 향후 작업에 더 철저한 조사를 남겨둔다.\n' +
      '\n' +
      '다른 형태의 절제 또한 일부 설정에서 평균 절제 또는 제로 절제를 수행하는 것이 흥미로울 수 있으며 우리의 조정은 여기에 적용 가능한 상태로 남아 있으며 무작위 프롬프트 쌍 결과는 AtP*가 노이즈 분포에 과도하게 민감하지 않음을 시사하므로 결과가 이월될 가능성이 있다고 추측한다.\n' +
      '\n' +
      '### Applications\n' +
      '\n' +
      '본 연구에서 논의한 방법의 자동 회로 찾기 자연 적용은 희소 서브그래프 또는 \'회로\'의 자동 식별 및 국소화이다(Cammarata et al., 2020). 이의 변형은 ACDC 알고리즘과 에지 속성 패칭을 결합한 Syed et al.(2023)에 의해 동시 작업에서 이미 논의되었다(Conmy et al., 2023). 에지 패치 논의에서 언급했듯이 AtP*는 에지 속성 패치로 일반화될 수 있으며, 이는 자동화된 회로 발견에 추가적인 이점을 가져올 수 있다.\n' +
      '\n' +
      '또 다른 접근법은 Cao et al.(2021); Louizos et al.(2018)과 유사한 노드들에 대한 (확률적) 마스크를 학습하는 것이다; 여기서 확률은 현재 추정된 노드 기여도 \\(c(n)\\으로 스케일링된다. 이 접근법의 경우, 현재 마스크 확률이 주어진 모든 노드 효과를 추정하는 빠른 방법이 중요함을 증명할 수 있다.\n' +
      '\n' +
      'Sparse Autoencoders(SAEs)를 사용하여 뉴런과 같은 트랜스포머-네이티브 유닛보다 잠재적으로 더 의미적 일관성을 갖는 단절된 희소 표현들을 구성하는 것에 대한 커뮤니티에 의해 최근 관심이 증가하고 있다(Bricken et al., 2023; Cunningham et al., 2023). SAE는 일반적으로 그들이 적용되는 대응하는 트랜스포머 블록보다 훨씬 더 많은 노드를 갖는다. 이것은 활성화 패치 효과 측면에서 더 큰 문제를 제기하여 AtP*의 속도를 더 가치 있게 만들 수 있다. 그러나 SAE의 희소성으로 인해 주어진 전진 패스에서는 대부분의 특징의 효과가 0이 된다. 예를 들어, Bricken et al.(2023)에 의한 일부 성공적인 SAE는 주어진 토큰 위치에 대해 500개의 뉴런에 대해 10-20개의 활성 특징을 가지며, 이는 MLP 설정에 대해 노드 수를 20-50배 감소시키며, 기존의 반복적 방법이 실용적으로 유지되는 스케일을 증가시킨다. 그러나 이 작업에서 논의된 방법이 다시 더 중요해질 수 있는 실제 관련 또는 SOTA 규모 모델에 대해 허용 가능한 재구성 오류로 어느 정도의 희소성이 실현 가능한지 여부는 여전히 열린 연구 질문이다.\n' +
      '\n' +
      '조향 LLMsAtP*는 모델의 행동을 제어하기 위해 표적 추론 시간 개입에 활용될 수 있는 모델의 단일 노드를 발견하는 데 사용될 수 있다. 이전 작업(Li et al., 2023; Turner et al., 2023; Zou et al., 2023)과 대조적으로, 모델의 나머지 계산에 덜 영향을 미치는 더 국부적인 개입을 제공할 수 있다. 한 가지 잠재적인 흥미진진한 방향은 AtP*(또는 다른 구배 기반 근사치)를 사용하여 활성화된 경우 어떤 희소 오토인코더 특징이 상당한 효과를 갖는지 확인하는 것이다.\n' +
      '\n' +
      '### Recommendation\n' +
      '\n' +
      '우리의 결과는 실무자가 빠른 인과적 귀인을 시도하려는 경우 고려해야 할 두 가지 주요 요인이 있음을 시사한다. (i) 국산화의 원하는 세분화, (ii) 신뢰 대 계산 절충.\n' +
      '\n' +
      '(i)와 관련하여, 원하는 과립성, 더 작은 성분(예를 들어, MLP 뉴런 또는 주의 헤드)은 더 많지만 더 선형적이며, AtP와 같은 구배 기반 방법에서 더 나은 결과를 얻을 가능성이 있다. 우리는 AtP가 층들을 패치하거나 층들의 슬라이딩 윈도우들을 패치하는 경우 좋은 근사치가 될 것이라고 덜 확신하며, 이 경우 실무자들은 정상적인 패치를 하기를 원할 수 있다. 포워드 패스가 필요한 횟수(예: 긴 컨텍스트 곱하기 여러 계층, 토큰 \\(\\times\\) 계층 패칭 시)가 금지된 상태로 유지된다면, 우리의 다른 기준선이 유용할 수 있다. 단일 프롬프트 쌍의 경우 이해하기 쉽기 때문에 블록을 시도하는 것이 특히 좋습니다. 분포의 경우 많은 프롬프트 쌍에 더 잘 확장되기 때문에 서브샘플링을 권장합니다.\n' +
      '\n' +
      '(ii)와 관련하여, 신뢰 대 계산 트레이드오프는, 애플리케이션에 따라, 신뢰도를 증가시키기 위해 진단을 실행한 후 활성화 패치 프리필터로서 AtP를 실행하는 것이 바람직할 수 있다. 반면에, 위음성이 큰 문제가 아니라면 진단을 건너뛰는 것이 바람직할 수 있으며, 위양성도 그렇지 않다면, 특정 경우에 의사들은 활성화 패치 검증을 완전히 건너뛰기를 원할 수 있다. 또한, 프롬프트 쌍 분포가 관심의 특정 회로/행동을 적절하게 강조하지 않으면, 이것은 또한 임의의 로컬화 방법들로부터 학습될 수 있는 것을 제한할 수 있다.\n' +
      '\n' +
      'AtP가 적절한 경우, 우리의 결과는 단일 프롬프트 쌍에 대한 AtP*, 분포에 대한 AttentionNodes에 대한 AtP+QKFix 및 분포에 대한 NeuronNodes(또는 비선형성 직전에 있지 않은 다른 사이트)에 대한 AtP가 사용하기에 가장 좋은 변형임을 시사한다.\n' +
      '\n' +
      '물론, 이러한 권장 사항은 우리가 연구한 것과 유사한 설정에서 가장 잘 입증된다: 집중 프롬프트 쌍/분포, 주의 노드 또는 뉴런 사이트, 노드별 귀속, 클린 프롬프트 다음 토큰 상의 교차 엔트로피 손실 측정. 이러한 가정에서 벗어나는 경우 도약하기 전에 살펴보는 것이 좋습니다.\n' +
      '\n' +
      '##6 관련 사항\n' +
      '\n' +
      '지역화 및 매개 분석 이 작업은 그래프가 언어 모델의 계산을 나타내는 특정 경우에 인과 그래프(Pearl, 2000)에서 모든(중요한) 노드의 영향을 식별하는 것과 관련이 있다. 인과 그래프에서 중요한 중간 노드를 찾기 위한 핵심 방법은 이들 노드에 개입하여 효과를 관찰하는 것인데, 이는 펄(2001); 로빈스 앤 그린란드(1992)에 의해 인과 매개 분석이라는 이름으로 처음 논의되었다.\n' +
      '\n' +
      '활성화 패칭(Activation Patching)은 최근 심층 신경망에서 인과적으로 중요한 노드를 식별하기 위해, 특히 모델 컴포넌트의 출력이 개입되는 활성화 패칭의 방법을 통해 인과 매개 분석의 아이디어를 적용하는 데 성공하는 것이 증가하고 있다. 이 기술은 커뮤니티에 의해 널리 사용되어 왔으며 콘텍스트의 범위에서 성공적으로 적용되었다(Connry et al., 2023; Cunningham et al., 2023; Feng and Steinhardt, 2023; Finlayson et al., 2021; Geva et al., 2023; Goldowsky-Dill et al., 2023; Hanna et al., 2023; Hase et al., 2023; Hendel et al., 2023; Huang et al., 2023; Lieberum et al., 2023; McDougall et al., 2023; Meng et al., 2023; Merullo et al., 2023; Nanda et al., 2023; Olsson et al., 2022; Soulos et al., 2020; Stolfo et al., 2023; Tigges et al., 2023; Vig et al., 2022).\n' +
      '\n' +
      'Chan et al.(2022)은 모델의 행동의 기초가 되는 내부 메커니즘에 대한 가설을 검증하기 위한 일반화된 알고리즘인 인과적 스크러빙을 도입하고, 평균 또는 제로 절제를 사용하지 않고 잡음 제거 및 재샘플 절제를 수행하는 이들의 동기를 상세히 설명한다 - 그들은 그 가설이 일부 큰 섭동 세트에 불변한다는 것을 암시하는 것으로 해석하며, 따라서 그들의 시작점은 깨끗한 교란되지 않은 전방 통과.13\n' +
      '\n' +
      '각주 13: 잡음 제거보다는 잡음 제거에 초점을 맞춘 우리의 동기는 밀접하게 관련된 것이었다 – 우리는 자동화된 회로 발견에 의해 동기 부여되었으며, 여기서 점점 더 많은 모델의 잡음 제거는 섹션 5.3에서 논의된 두 접근법의 기본 방법론이다.\n' +
      '\n' +
      '인과적 추상화를 공식화하는 것과 관련된 또 다른 연구 라인은 하위 변수의 상위 인과적 추상화를 찾고 검증하는 데 중점을 둔다(Geiger et al., 2020, 2021, 2022, 2023). 이러한 상이한 프레임워크들이 어떻게 동의하고 상이한지에 대한 더 상세한 내용은 제너 등(2022)을 참조한다. 이러한 작업과 달리 우리는 주로 계산 그래프에서 중요한 하위 수준 변수를 식별하는 데 관심이 있으며 하위 수준을 상위 수준으로 분류하는 의미 또는 잠재적 그룹을 조사하지 않는다.\n' +
      '\n' +
      '인과적 매개 분석에 더하여, 모델 순방향 패스에서 노드 활성화들에 대한 개입은 또한 바람직한 행동을 향한 조향 모델들의 방법으로서 연구되었다(Belrose et al., 2023; Jorgensen et al., 2023; Li et al., 2023; Rimsky et al., 2023; Turner et al., 2023; Zou et al., 2023).\n' +
      '\n' +
      'Attribution Patching/ Gradient-based Masking, 우리가 NAND(2022)에서 공식화된 바와 같은 AtP의 리샘플-ablation 변이체를 사용하는 동안, 유사한 제형이 심층 신경망을 성공적으로 프루닝하기 위해 과거에 사용되었다(Figurnov et al., 2016; Michel et al., 2019; Molchanov et al., 2017), 또는 심지어 해석가능성을 위해 인과적으로 중요한 노드들을 식별한다(Cao et al., 2021). Syed et al.(2023)에 의한 동시 작업은 또한 AtP가 이전의 수동 회로 식별 작업과 일치하는 방식으로 인과적으로 중요한 회로를 자동으로 찾는 데 도움이 될 수 있음을 입증한다. Syed et al.(2023)과 달리, AtP의 고장모드에 대한 추가적인 분석을 제공하고, AtP\\({}^{*}\\) 형태의 개선을 제공하며, 인간 연구자들의 판단과 무관한 지상진실에 대해 더 큰 모델들의 집합에 대한 몇 가지 기준선뿐만 아니라 두 방법 모두를 평가한다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '본 논문에서는 노드 패치 효과 평가를 위한 속성 패칭의 활용 방안을 탐색하였다. 우리는 속성 패칭을 대안 및 보완과 비교하고 고장 모드를 특성화하고 신뢰성 진단을 제시했다. 우리는 또한 회로 발견, 에지 국소화, 거친 입자 국소화 및 인과적 추상화와 같이 패치가 흥미로울 수 있는 다른 설정에 대한 기여의 의미에 대해 논의했다.\n' +
      '\n' +
      '우리의 결과는 AtP*가 대안보다 노드 패치 효과 평가에 더 신뢰할 수 있고 확장 가능한 접근법이 될 수 있음을 보여준다. 그러나 취소 및 포화와 같은 속성 패칭의 실패 모드를 인식하는 것이 중요하다. 이를 자세히 조사하고 완화와 결과가 신뢰할 수 있는지 확인하기 위한 진단에 대한 권장 사항을 제공했다.\n' +
      '\n' +
      '우리는 우리의 연구가 기계론적 해석 가능성 분야에 중요한 기여를 하고 심층 신경망의 행동을 이해하기 위한 보다 신뢰할 수 있고 확장 가능한 방법의 개발을 발전시키는 데 도움이 될 것이라고 믿는다.\n' +
      '\n' +
      '## 8 저자 기부금\n' +
      '\n' +
      'Janos Kramar는 연구 책임자였고, Tom Lieberum 또한 핵심 기여자였다 - 둘 다 프로젝트의 대부분의 측면에 매우 관여했다. 로힌 샤와 닐 난다는 고문으로 일했고, 내내 피드백과 지도를 했다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Belrose et al.(2023) N. 벨로즈, 슈나이더-조셉, S 라브포겔 코터렐, E. 래프, S. 바이더만 리스: 닫힌 형태의 완벽한 선형 개념 소거__ arXiv preprint arXiv:2306.03819_, 2023.\n' +
      '* Biderman et al.(2023) S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. 오브라이언, E. 한라한, M. A. 칸, S. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. 수타위카, O. 반 더 월 피티아: 훈련과 스케일링을 통해 대규모 언어 모델을 분석하기 위한 스위트룸입니다. A. 크라우스, E. 브런스킬, K. 조병하르트 Sabato, and J. Scarlett, editors, _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 2397-2430. PMLR, 2023. URL[https://proceedings.mlr.press/v202/biderman23a.html](https://proceedings.mlr.press/v202/biderman23a.html).\n' +
      '* Bricken et al.(2023) T. Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T. 코널리, N 터너, C. 아닐, C. 데니슨, A. 애스켈, R. 라센비 우승 크레이브크 티쉬퍼 맥스웰 조셉, 조셉 Hatfield-Dodds, A. Tamkin, K 응웬, B. 맥클린, J. E. 버크, T. 흄수 카터, T 헤니건과 C. 올라 일원성에 대한: 사전 학습으로 언어 모델을 분해하는 것. _ Transformer Circuits Thread_, 2023. [https://transformer-circuits.pub/2023/monosemantic-features/index.html](https://transformer-circuits.pub/2023/monosemantic-features/index.html)\n' +
      '* Cammarata et al.(2020) N. Cammarata S. C. Olah, M. 페트로프 L. 슈베르트, C. 보스, B. Egan, S. K. 임 나사산: 회로. _ Distill_, 2020. doi: 10.23915/distill.00024. [https://distill.pub/2020/circuits](https://distill.pub/2020/circuits)\n' +
      '* Cao et al. (2021) N. D. Cao, L. 슈미드, D. 허크스, 그리고 나 티토프 차별화할 수 있는 마스킹이 있는 언어 모델에 대한 희박한 개입, 2021.\n' +
      '* Chan et al.(2022) L. 챤, A. 개리가-알론소, N. (주)골드워스키딜 Greenblatt, J. Nitishinskaya, A. Radhakrishnan, B. Shlegeris 및 N. 토마스 인과적 스크러빙, 해석가능 가설을 엄격하게 테스트하는 방법. _ AI Alignment Forum_, 2022. [https://www.alignmentforum.org/posts/JvZhhzyChu2Yd57RN/causal-scrubbing-a-rigorously-testing](https://www.alignmentforum.org/posts/JvZhhzyChu2Yd57RN/causal-scrubbing-a-rigorously-testing)\n' +
      '\n' +
      '* Conmy et al. (2023) A. Conmy, A. N. Mavor-Parker, A. Lynch, S. 하이미르하임과 A. 가리가 알론소 기계론적 해석 가능성에 대한 자동화된 회로 발견, 2023년.\n' +
      '* Cunningham et al. (2023) H. Cunningham, A. Ewart, L. 릭스, R 휴벤과 L. 샤키 희박한 오토인코더는 2023년 언어 모델에서 매우 해석 가능한 특징을 발견한다.\n' +
      '* Feng and Steinhardt (2023) J. Feng and J. Steinhardt. 언어 모델은 컨텍스트에서 엔티티를 어떻게 바인딩합니까?, 2023.\n' +
      '* Figurnov et al.(2016) M. 피구노프, A. 이브라이모바, D. P. 베트로프, P. 콜리. 천공된 cnns: 중복 회선을 제거하여 가속합니다. 이민일 우기야마 룩스버그, I. 가욘, R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016. URL[https://proceedings.neurips.cc/paper_files/paper/2016/file/f0e52b27a7a7a5d6a1a87373dffa53dbe5-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2016/file/f0e52b27a7a5a1a87373dffa53dbe5-Paper.pdf].\n' +
      '* Finlayson et al.(2021) M. 핀레이슨, A. 뮬러, S 게르만 시버, T 린젠, Y 벨린코프 신경 언어 모델의 구문 일치 메커니즘에 대한 인과 분석. C. Zong, F. Xia, W. 리, R. Navigli, 편집자, _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 1828-1843, Online, Aug. 2021. 컴퓨팅 언어학 협회. doi: 10.18653/v1/2021.acl-long.144. URL[https://aclanthology.org/2021.acl-long.144](https://aclanthology.org/2021.acl-long.144)\n' +
      '* Gao et al.(2020) L. 가오성 비더만 블랙 L. 골딩, T 호피, C. 포스터, J. Phang, H. He, A. Thite, N. 나베시마 기자랑 C. 리히 파일: 언어 모델링을 위한 다양한 텍스트의 800gb 데이터세트. _ ArXiv:2101.00027_, 2020.\n' +
      '* Geiger 등(2020) A. Geiger, K. 리처드슨과 C. 팟츠 신경 자연 언어 추론 모델은 2020년 어휘 수반 및 부정 이론을 부분적으로 포함한다.\n' +
      '* Geiger et al. (2021) A. Geiger, H. Lu, T. 카드랑 C 포츠요 신경망의 인과적 추상화, 2021년\n' +
      '* Geiger et al. (2022) A. Geiger, Z. Wu, H. Lu, J. Rozner, E. Kreiss, T. Icard, N. D. Goodman, C. Potts. 2022년, 해석 가능한 신경망에 대한 인과 구조를 유도한다.\n' +
      '* Geiger et al. (2023) A. Geiger, C. Potts, and T. 카드 충실한 모델 해석을 위한 인과적 추상화, 2023.\n' +
      '* Geva et al. (2023) M. 게바, 제이 배스팅스, 케이 필리포바와 A. 글로버슨 자동 회귀 언어 모델에서 사실적 연관성에 대한 회상, 2023.\n' +
      '* Goldowsky-Dill et al.(2023) N. 골도스키딜, 맥레오드, L. 사토와 A. 아로라 경로 패칭으로 모델 동작을 로컬화하는 단계, 2023.\n' +
      '* Gurnee et al.(2023) W. 구니노 난다, M 폴리경 하비, 디. 트로이트키, 디. 베르티마스 건초더미에서 뉴런을 찾는 것: 2023년 희박한 탐침을 사용한 사례 연구.\n' +
      '* Hanna et al.(2023) M. 한나오 류랑 A. 버라이어티엔 gpt-2는 어떻게 그 이상을 계산합니까?: 미리 훈련된 언어 모델에서 수학적 능력을 해석하는 것, 2023.\n' +
      '* Hase et al.(2023) P. Hase, M. 반살, B. Kim, A. Ghandeharioun. 로컬리제이션이 편집에 도움이 되나요? 인과관계 기반 현지화 대 놀라운 차이. 2023년 언어 모델에서의 지식 편집.\n' +
      '* Hendel et al.(2023) R. 헨델 게바와 A. 글로버슨 상황 내 학습은 2023년 태스크 벡터를 생성한다.\n' +
      '* Hoffmann et al. (2020) J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. 헤니건, E. 놀랜드, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. 오신데로 시모니안, E. 엘슨, O. Vinyals, J. Rae, L. 시프 계산 최적 대형 언어 모델 훈련에 대한 경험적 분석. 인석 고예조 모하메드, A. 아가왈, D. 벨그레이브, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 30016-30030. Curran Associates, Inc., 2022. URL[https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf]).\n' +
      '* Huang et al. (2023) J. Huang, A. Geiger, K. 지우스터린크 Wu, C. Potts. 뉴런에 대한 자연 언어 설명을 엄격하게 평가한다, 2023.\n' +
      '* Jenner et al. (2022) E. Jenner, A. Garriga-Alonso, and E. Zverev. 인과적 스크러빙, 인과적 추상화 및 관련 방법의 비교. _ AI Alignment Forum_, 2022. [https://www.alignmentform.org/posts/uLMWMeBG3ruoBRhMW/a-comparison-of-causal-scrubbing-causal-abstractions-and](https://www.alignmentform.org/posts/uLMWMeBG3ruoBRhMW/a-comparison-of-causal-scrubbing-causal-abstractions-and]\n' +
      '* Jorgensen et al.(2023) O. 조겐슨, D. 코프, N. Schoots and M 섀너핸 평균 중심(mean-centring, 2023)으로 언어 모델의 활성화 스티어링 개선\n' +
      '* Li 등(2023) K. 이오 파텔, F. 비에가스, H. 피스터, M. 와텐버그 추론-시간 개입: 2023년 언어 모델로부터 진실된 답변을 얻는 것.\n' +
      '* Lieberum et al.(2023) T. 리베럼 라츠, J. 크라마, N. 난다, G. 어빙, R 샤와 브이 미쿨릭 회로 분석 해석 가능성은 어느 정도입니까? 2023년 친칠라에서의 객관식 능력의 증거.\n' +
      '* Louizos et al. (2018) C. Louizos, M. 웰링, 그리고 D. P. 킹마 \\(l_{0}\\) 정규화, 2018을 통해 희소 신경망을 학습한다.\n' +
      '* McDougall et al. (2023) C. McDougall, A. Conmy, C. Rushing, T. 맥그래스, N. 낸다 복사 억제: 주의 헤드, 2023을 종합적으로 이해합니다.\n' +
      '* McGrath et al. (2023) T. 맥그래스 라츠, J. 크라마, V. 미쿨릭, S. 레그 히드라 효과: 언어 모델 계산에서 새로운 자가 복구, 2023.\n' +
      '* Meng et al.(2023) K. Meng, D. Bau, A. Andonian and Y. 벨린코프 2023년 gpt에서 사실 관계를 찾고 편집합니다.\n' +
      '* Merullo et al. (2023) J. Merullo, C. Eickhoff, and E. Pavlick. 트랜스포머 언어 모델의 작업 전반에 걸쳐 회로 구성요소 재사용, 2023.\n' +
      '* Michel et al. (2019) P. Michel, O. 레비랑 G. 노비그요 16명의 머리가 한 명보다 더 나은가요? H. Wallach, H. Larochelle, A. Beygelzimer, F. d\'Alche-Buc, E. Fox, R. Garnett, Editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL[https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9b282670cdd54f69f-Paper.pdf].\n' +
      '* Molchanov et al. (2017) P. Molchanov, S. 티리 카라스, T. 아일라와 J. 카우츠 자원 효율적인 추론을 위해 컨볼루션 신경망을 프루닝한다. _International Conference on Learning Representations_, 2017. URL[https://openreview.net/forum?id=SJGCiw5gl](https://openreview.net/forum?id=SJGCiw5gl)에서,\n' +
      '*난다(2022) N. 낸다 귀인 패칭: 산업 규모에서의 활성화 패칭. 2022. URL[https://www.neelnanda.io/mechanistic-interpretability/attribution-patching](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching)\n' +
      '* Nanda et al.(2023) N. 난다, S 라자마노하라, J. 크라마, R. 샤 사실 발견: 뉴런 레벨에서 역-엔지니어 사실적 회상을 시도, 2023년 12월. URL[https://www.alignmentform.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall](https://www.alignmentform.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall].\n' +
      '\n' +
      '노스트 대수학자 gpt 해석: 로짓 렌즈. 2020. URL[https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/해석-gpt-the-logit-lens](https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/해석-gpt-the-logit-lens)\n' +
      '* Olsson et al.(2022) C. Olsson, N. 엘하이지 낸다 조셉 다사르마 Henighan, B. Mann, A. Askell, Y. 배아천 Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds D. Hernandez S. 존스턴, A. 존스, J. 커니언, L. 로빗 Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. 맥캔들시, 그리고 C.올라 In-context learning and induction head. _ Transformer Circuits Thread_, 2022. [https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)\n' +
      '* Pearl(2000) J. Pearl. _ 인과관계: 모델, 추론 및 추론_. 케임브리지 대학 출판사, 2000년\n' +
      '* Pearl(2001) J. Pearl. 직접 및 간접 효과, 2001.\n' +
      '* Radford et al. (2018) A. Radford, K. 나라심한 살리만과 나, 서츠키버 생성적 사전 훈련에 의한 언어 이해력 향상, 2018.\n' +
      '* Rimsky et al.(2023) N. 림스키 Gabrieli, J. Schulz, M. Tong, E. Hubinger and A. M. Turner. 조영 활성화 추가, 2023을 통해 라마 2를 조향한다.\n' +
      '* Robins and Greenland (1992) J. M. Robins and S. 그린란드 직접 효과와 간접 효과에 대한 식별 가능성 및 교환 가능성. _ Epidemiology_, 3:143-155, 1992. URL[https://api.semanticscholar.org/CorpusID:10757981](https://api.semanticscholar.org/CorpusID:10757981).\n' +
      '* Soulos et al. (2020) P. Soulos, R. T. McCoy, T. 린젠, P. 스몰렌스키 역할 학습 네트워크를 사용하여 벡터 표현의 구성 구조를 발견합니다. A. 알리샤히 벨린코프, G. 크리팔라, D. 허크스, Y. Pinter, and H. Sajjad, editor, _Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP_, pages 238-254, Online, Nov. 2020. Computational Linguistics Association. doi: 10.18653/v1/2020.blackboxnlp-1.23. URL[https://aclanthology.org/2020.blackboxnlp-1.23](https://aclanthology.org/2020.blackboxnlp-1.23)\n' +
      '* Stolfo et al. (2023) A. Stolfo, Y. 벨린코프와 M 사찬 인과 매개 분석을 이용한 언어 모델의 산술 추론 기계론적 해석, 2023.\n' +
      '* Syed et al. (2023) A. Syed, C. Rager, and A. Conmy. 귀인 패칭은 자동화된 회로 발견, 2023보다 우수하다.\n' +
      '* Tigges et al. (2023) C. Tigges, O. J. Hollinsworth, A. Geiger, and N. 낸다 2023년, 대규모 언어 모델에서 정서의 선형 표현.\n' +
      '* Todd et al. (2023) E. Todd, M. L. Li, A. S. Sharma, A. Mueller, B. C. Wallace, and D. Bau. 큰 언어 모델의 함수 벡터, 2023.\n' +
      '* Turner et al.(2023) A. M. Turner, L. 티어가트, D. 우델, G. 리치, U. 미니, 엠. 맥디아미드 활성화 추가: 최적화 없이 스티어링 언어 모델, 2023.\n' +
      '* Vaswani et al. (2017) A. Vaswani, N. N. 쉐이저 파마르, J. 우즈코리트, L. 존스, A. N. 고메즈, L. 카이저, 나 폴로수킨 2017년엔 관심만 있으면 돼\n' +
      '* Veit et al. (2016) A. Veit, M. J. Wilber, and S. 신부님 잔여 네트워크는 비교적 얕은 네트워크의 앙상블처럼 행동한다. 이민일 우기야마 룩스버그, I. 가욘, R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016. URL[https://proceedings.neurips.cc/paper_files/paper/2016/file/37bc2f75bf1bf1bcfe8450a1a41c200364c-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2016/file/37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf).\n' +
      '\n' +
      '* Vig et al. (2020) J. Vig, S. 게르만 벨린코프 기안대보 가수, S. 쉬버 인과 매개 분석을 사용하여 언어 모델의 성별 편향을 조사합니다. M. Larochelle 란자토 해델 Balcan, and H. Lin, Editors, _Advances in Neural Information Processing Systems_, volume 33, pages 12388-12401. Curran Associates, Inc., 2020. URL[https://proceedings.neurips.cc/paper_files/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf]).\n' +
      '* Wang et al.(2022) K. 왕, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. 야생에서의 해석 가능성: 2022년 gpt-2 소형에서 간접 객체 식별을 위한 회로.\n' +
      '* Welch(1947) B. L. Welch. 여러 다른 모집단 분산이 관련될 때 \'학생의 문제\'의 일반화. _ Biometrika_, 34(1-2):28-35, 01 1947. ISSN 0006-3444. doi: 10.1093/biomet/34.1-2.28. URL[https://doi.org/10.1093/biomet/34.1-2.28](https://doi.org/10.1093/biomet/34.1-2.28).\n' +
      '* Zou et al.(2023) A. Zou, L. 판승 첸, J. 캠벨, P.궈, R. 렌아판 엑스 음문 마지카, A-K S. 돔브로스키 고남 이명준 왕아멜렌 바사르트 고예조 프레드릭슨, J. Z. 콜터 그리고 D. 헨드릭스 대표 공학: Ai 투명성에 대한 하향식 접근, 2023.\n' +
      '\n' +
      '## 부록 메소드 상세\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '서브샘플링의 특성\n' +
      '\n' +
      '여기서는 3.3절의 서브샘플링 추정량\\(\\hat{\\mathcal{I}}_{\\text{SS}(n)\\)이 상호작용 효과가 없는 경우 편향되지 않음을 증명한다. 또한, 간단한 상호작용 모델을 가정하면, 우리는 \\(\\hat{\\mathcal{I}}_{\\text{SS}(n)\\)의 편향이 다른 노드와의 총 상호작용 효과의 \\(p\\)배임을 보여준다. 우리는 쌍대 상호작용 모델을 가정한다. 즉, 노드 집합\\(\\eta\\)이 주어지면, 우리는\n' +
      '\n' +
      '\\sigma_{n,n^{\\prime}}(x)\\sum_{n\\in\\eta}\\mathcal{I}(n;x)+\\sum_{\\begin{subarray}{c}n, n^{\\prime}\\in\\eta\\n\\neq n\\end{subarray}\\sigma_{n,n^{\\prime}}(x)\\tag{14}\\tag{c}n, n^{\\prime}\\sigma_{n,n^{\\prime}}(x)\n' +
      '\n' +
      '각 프롬프트 쌍에 대해 고정된 상수 \\(\\sigma_{n,n^{\\prime}}(x)\\in\\mathbb{R}\\(x\\in\\text{support}(\\mathcal{D}))를 갖는 \\(\\sigma_{n,n^{\\prime}=\\mathbb{E}_{x\\sim\\mathcal{D}}\\left[\\sigma_{n,n^{\\prime}}(x)\\right]\\)\n' +
      '\n' +
      '주어진 \\(\\eta\\)에 각 노드를 포함시킬 확률을 \\(p\\)이라 하고 \\(\\text{Bernoulli}^{|N|}(p)\\)와 \\(\\mathcal{D}\\)에서 샘플링된 프롬프트 쌍 \\(x\\)을 \\(M\\)으로 한다. 그러면,\n' +
      '\n' +
      'b{E}\\left[\\frac{1}{|\\eta^{+}(n);x_{k}^{-}(n)|}\\frac{1}{|\\eta^{-}(n)|}\\mathcal{I}(\\frac{1}{|\\eta^{+}(n)|}\\frac{1}{|\\eta^{-}(n)|}\n' +
      '\n' +
      '식 (15g)에서 우리는 상호작용항 \\(\\sigma_{m^{\\prime}}\\)이 모두 0이면 추정량이 편향되지 않음을 관측한다. 그렇지 않으면, 편향은 예상대로 상호작용 효과의 합과 \\(p\\)으로 모두 스케일링된다.\n' +
      '\n' +
      '블록과 계층적 기준선을 위한 Pseudocode a.1.2\n' +
      '\n' +
      '알고리즘 2에서 우리는 블록 베이스라인 알고리즘을 자세히 설명한다. 섹션 3.3에서 설명한 바와 같이 "블록 크기" 하이퍼파라미터 \\(B\\): 작은 블록 크기는 모든 블록을 평가하는 데 많은 시간이 필요한 반면 큰 블록 크기는 각 기여도가 높은 블록에서 평가하는 많은 관련 없는 노드를 의미한다.\n' +
      '\n' +
      '계층적 베이스라인 알고리즘은 작은 블록들을 사용함으로써 이러한 트레이드오프를 해결하는 것을 목표로 하지만, 슈퍼블록들로 그룹화되므로 키노드를 찾기 전에 모든 작은 블록들을 트래버스할 필요가 없다. 알고리즘 3에서 우리는 배치 크기 1에 해당하는 반복적인 형태의 계층적 알고리즘을 자세히 설명한다.\n' +
      '\n' +
      '놀랄 수 있는 한 가지 측면은 라인 21에서 하위 블록이 조상 슈퍼블록보다 더 높은 우선 순위로 우선순위 큐에 추가되지 않도록 보장한다는 것이다. 이를 수행하는 이유는 실제로 한 번에 하나의 블록을 패치하는 것이 아니라 배치 추론을 사용하기 때문에 배치 크기에 따라 가장 우선순위가 높은 평가되지 않은 블록을 평가하며, 이는 일부 블록이 평가될 때 상당한 지연을 초래할 수 있기 때문이다. 배치 크기 하이퍼파라미터에 대한 이러한 의존성을 줄이기 위해 라인 21은 모든 블록이 배치 크기 1보다 더 늦게 최대 \\(L\\) 배치에서 평가되도록 한다.\n' +
      '\n' +
      '### AtP improvements\n' +
      '\n' +
      'Attention 키에 대한 수정된 AtP에 대한 a.2.1 Pseudocode\n' +
      '\n' +
      '섹션 3.1.1에 설명된 바와 같이 모든 노드에 대해 식 (10)을 순진하게 계산하려면 각 주의 헤드 및 프롬프트 쌍에서 \\(\\mathsf{O}(T^{3})\\) 플롭이 필요하다. 여기서 우리는 \\(\\mathsf{O}(T^{2})\\에서 보다 효율적인 알고리즘을 제공한다. 키들, 질의들 및 주의 확률들 외에도, 우리는 이제 주의 로짓들(사전-소프트맥스 스케일링된 키-질의 내적들)을 캐시한다.\n' +
      '\n' +
      '우리는 식 (8)과 식 (9)와 유사하게 \\(\\mathsf{attnLogits}^{t}_{\\mathsf{patch}}(n^{q})\\)와 \\(\\Delta_{t}\\,\\mathsf{attnLogits}(n^{q})\\을 정의한다. 간결함을 위해 우리는 또한 \\(\\mathsf{attnLogits}_{\\mathsf{patch}(n^{q})_{t}:=\\mathsf{attnLogits}_{t}(n^{q})_{t})와 \\(\\Delta\\,\\mathsf{attnLogits}(n^{q})_{t}:=\\Delta_{t}\\,\\mathsf{attnLogits}(n^{q})_{t}\\,\\mathsf{attnLogits}(n^{q})_{t}\\)을 정의할 수 있다.\n' +
      '\n' +
      '시퀀스의 위치 \\(t\\)에서 키 \\(n^{k}_{t}\\)에 대해 \\(\\mathsf{attnLogits}(n^{q})_{t}\\)이 바뀔 때 \\(\\mathsf{attn}(n^{q})_{t}\\)의 non-\\(t\\) 성분의 비율이 변하지 않으므로 \\(\\Delta_{t}\\,\\mathsf{attn}(n^{q})\\)이 실제로 \\(\\mathsf{onehot}(t)-\\mathsf{attn}(n^{q})\\)에 약간의 스칼라 \\(n^{k}\\)을 곱하기 위해서는 스칼라 \\(s^{t}:=\\frac{\\Delta\\,\\mathsf{attn}(n^{q})_{t}}{1-\\mathsf{attn}(n^{q})_{t}\\)이 되어야 한다. 또한, \\(\\log\\left(\\frac{\\mathsf{attn}_{\\mathsf{patch}}(n^{q})_{t}{1-\\mathsf{attn}_{\\mathsf{patch}}(n^{q})_{t}\\right)=\\log\\left(\\frac{\\mathsf{attn}(n^{q})_{t}{1-\\mathsf{attn}(n^{q})_{t}(n^{q}))_{t}{1-\\mathsf{attn}(n^{q})_{t}\\right)+\\Delta\\,\\mathsf{attnLogits}(n^{q})_{t}\\t)는 시그모이드 함수의 역수이므로, \\(\\mathsf{attn}^{t}_{\\mathsf{patch}(n^{q})=\\mathsf{attnLogits}(n^{q})_{t}\\t}\n' +
      '\n' +
      '#### a.2.2 The \\(\\mathsf{attnLogits}\\)\n' +
      '\n' +
      '우리는 이제 섹션 3.1.1에서 \\(\\mathsf{attnLogits}\\) 알고리즘을 자세히 설명한다.\n' +
      '\n' +
      '```\n' +
      '1:block size \\(B\\), compute budget \\(M\\), node \\(N=\\{n_{i}\\}), prompts \\(x^{\\text{clean}\\), \\(x^{\\text{noise}\\), intervention function \\(\\bar{I}:\\eta\\mapsto\\mathcal{I}(\\eta;x^{\\text{clean}},x^{\\text{noise}})})\n' +
      '2:numBlocks \\(\\leftarrow\\lceil|N|/B\\rceil\\)\n' +
      '3:\\(\\pi\\leftarrow\\text{shuffle}\\left(\\{|\\text{numBlocks}\\cdot iB/|N|\\mid i\\in\\{0, \\ldots,|N|-1\\}\\}\\right)\\)\\(\\triangleright\\) 각 노드를 블록에 할당한다.\n' +
      '4:for\\(i\\gets 0\\) to \\(\\text{numBlocks}-1\\)do\n' +
      '5:\\(\\text{blockContribution}[i]\\leftarrow\\lfloor\\bar{I}(\\pi^{-1}(\\{i\\}))\\rfloor\\)\\(\\triangleright\\)\\(\\pi^{-1}(\\{i\\}):=\\{n:\\pi(n)=i\\mid n\\in N\\}\\})\n' +
      '6:\\(\\text{pspentBudget}\\gets M-\\text{numBlocks}\\)\n' +
      '7:topNodeContiribs\\(\\leftarrow\\) CreateEmptyDictionary()\n' +
      '8:forall\\(i\\in\\{0\\) to \\(\\text{numBlocks}-1\\}\\)의 블록 기여도[\\(i\\)]do의 순으로\n' +
      '9:forall\\(n\\in\\pi^{-1}(\\{i\\})\\)do\n' +
      '10:if\\(\\text{pspentBudget}<M\\)then\n' +
      '11:\\(\\text{topNodeContiribs}[n]\\leftarrow\\lfloor\\bar{I}(\\{n\\})\\rfloor\\)\n' +
      '12:\\(\\text{spentBudget}\\leftarrow\\text{spentBudget}+1\\)\n' +
      '13:else\n' +
      '14:return topNodeContiribs\n' +
      '```\n' +
      '\n' +
      '**알고리즘 2** 인과적 귀속을 위한 블록 알고리즘.\n' +
      '\n' +
      'Attention 키에 대한 AtP 보정을 위한### Pseudocode\n' +
      '\n' +
      '섹션 3.1.1에 설명된 바와 같이 모든 노드에 대해 식 (10)을 순진하게 계산하려면 각 주의 헤드 및 프롬프트 쌍에서 \\(\\mathsf{O}(T^{3})\\) 플롭이 필요하다. 여기서 우리는 \\(\\mathsf{O}(T^{2})\\에서 보다 효율적인 알고리즘을 제공한다. 키들, 질의들 및 주의 확률들 외에도, 우리는 이제 주의 로짓들(사전-소프트맥스 스케일링된 키-질의 내적들)을 캐시한다.\n' +
      '\n' +
      '우리는 식 (8)과 식 (9)와 유사하게 \\(\\mathsf{attnLogits}^{t}_{\\mathsf{patch}}(n^{q})\\)와 \\(\\Delta_{t}\\,\\mathsf{attnLogits}(n^{q})\\을 정의한다. 간결함을 위해 우리는 또한 \\(\\mathsf{attnLogits}_{\\mathsf{patch}(n^{q})_{t}:=\\mathsf{attnLogits}_{t}(n^{q})_{t})와 \\(\\Delta\\,\\mathsf{attnLogits}(n^{q})_{t}:=\\Delta_{t}\\,\\mathsf{attnLogits}(n^{q})_{t}\\,\\mathsf{attnLogits}(n^{q})_{t}\\)을 정의할 수 있다.\n' +
      '\n' +
      '시퀀스의 위치 \\(t\\)에서 키 \\(n^{k}_{t}\\)에 대해 \\(\\mathsf{attnLogits}(n^{q})_{t}\\)이 바뀔 때 \\(\\mathsf{attn}(n^{q})_{t}\\)의 non-\\(t\\) 성분의 비율이 변하지 않으므로 \\(\\Delta_{t}\\,\\mathsf{attn}(n^{q})\\)이 실제로 \\(\\mathsf{onehot}(t)-\\mathsf{attn}(n^{q})\\)에 약간의 스칼라 \\(n^{k}\\)을 곱하기 위해서는 스칼라 \\(s^{t}:=\\frac{\\Delta\\,\\mathsf{attn}(n^{q})_{t}}{1-\\mathsf{attn}(n^{q})_{t}\\)이 되어야 한다. 또한, \\(\\log\\left(\\frac{\\mathsf{attn}^{t}_{\\mathsf{patch}}(n^{q})_{t}}{1-\\mathsf{attn}_{\\mathsf{patch}(n^{q})_{t}\\right)=\\log\\left(\\frac{\\mathsf{attn}(n^{q})_{t}{1-\\mathsf{attn}(n^{q})_{t}(n^{q}))_{t}{1-\\mathsf{attn}(n^{q})_{t}\\right)+\\Delta\\,\\mathsf{attnLogits}(n^{q})_{t}\\t)는 시그모이드 함수의 역수이므로, \\(\\mathsf{attn}^{t}_{\\mathsf{patch}(n^{q})=\\mathsf{attnLogits}(n^{q})_{t\n' +
      '```\n' +
      '1: 브랜칭 인자 \\(B\\), num level \\(L\\), 계산 예산 \\(M\\), 노드 \\(N=\\{n_{i}\\}\\), 개입 함수 \\(I\\)\n' +
      '2:numTopLevelBlocks \\(\\leftarrow\\lceil|N|/B^{L}\\rceil\\)\n' +
      '3:\\(\\pi\\leftarrow\\) shuffle (\\(\\left\\big{\\{}|\\text{numTopLevelBlocks}\\cdot iB^{L}/|N|\\big{|}\\big{|}i\\in\\{0, \\ldots,|N|-1\\}\\big{\\}\\right\\}\\)\n' +
      '4: all\\(n_{i}\\in N\\)do\n' +
      '5:\\((d_{L-1},d_{L-2},\\ldots,d_{0})\\leftarrow\\) zero-padded final \\(L\\) base-\\(B\\) digits of \\(\\pi_{i}\\)\n' +
      '6:address\\((n_{i})=(\\lfloor\\pi_{i}/B^{L}\\rfloor,d_{L-1},\\ldots,d_{0})\\)\n' +
      '7:\\(Q\\leftarrow\\) CreateEmptyPriorityQueue()\n' +
      '8:for\\(i\\gets 0\\ to numTopLevelBlocks - \\(1\\)do\n' +
      '9: PriorityQueueInsert(\\(Q,[i],\\infty\\))\n' +
      '10:spentBudget \\(\\gets 0\\)\n' +
      '11:topNodeContrits \\(\\leftarrow\\) CreateEmptyDictionary()\n' +
      '12:repeat\n' +
      '13: (주소 Prefix, priority) \\(\\leftarrow\\) PriorityQueuePop(\\(Q\\))\n' +
      '14 : 블록노드 \\(\\leftarrow\\{n\\in N|\\text{StartsWith}(address(n),\\text{address Prefix})\\}\\)\n' +
      '15: blockContribution \\(\\leftarrow\\)\\(|\\mathcal{I}\\)(blockNodes) \\(|\\)\n' +
      '16 : spentBudget \\(\\leftarrow\\) spentBudget + \\(1\\)\n' +
      '17:ifblockNodes = \\(\\{n\\}\\ for some \\(n\\in N\\)then\n' +
      '18 : topNodeContribs\\([n]\\leftarrow\\) 블록 기여도\n' +
      '19:else\n' +
      '20:for\\(i\\gets 0\\~\\(B-1\\)do\n' +
      '21:if\\(\\{n\\in\\text{blockNodes}|\\text{StartsWith}(address(n),\\text{addressPrefix}+[i]\\} \\neq\\emptyset\\)then\n' +
      '22: PriorityQueueInsert(\\(Q,\\text{addressPrefix}+[i],\\min(\\text{blockContribution, priority}))\n' +
      '23:untilspentBudget = \\(M\\) 또는 PriorityQueueEmpty(\\(Q\\))\n' +
      '24:return topNodeContribs\n' +
      '```\n' +
      '\n' +
      '**알고리즘 3** 인과적 귀속을 위한 계층적 알고리즘, 반복적 형태이다. 실제로 우리는 라인 14에서 한 번에 단일 블록을 평가하기보다는 추가 배치를 수행한다.\n' +
      '\n' +
      '```\n' +
      '1: 브랜칭 인자 \\(B\\), num level \\(L\\), 계산 예산 \\(M\\), 노드 \\(N=\\{n_{i}\\}\\), 개입 함수 \\(I\\)\n' +
      '2:numTopLevelBlocks \\(\\leftarrow\\lceil|N|/B^{L}\\rceil\\)\n' +
      '3:\\(\\pi\\leftarrow\\) shuffle (\\(\\left\\big{\\{}|\\text{numTopLevelBlocks}\\cdot iB^{L}/|N|\\big{|}\\big{|}i\\in\\{0, \\ldots,|N|-1\\}\\big{\\}\\right\\}\\)\n' +
      '4: all\\(n_{i}\\in N\\)do\n' +
      '5:\\((d_{L-1},d_{L-2},\\ldots,d_{0})\\leftarrow\\) zero-padded final \\(L\\) base-\\(B\\) digits of \\(\\pi_{i}\\)\n' +
      '6:address\\((n_{i})=(\\lfloor\\pi_{i}/B^{L}\\rfloor,d_{L-1},\\ldots,d_{0})\\)\n' +
      '7:\\(Q\\leftarrow\\) CreateEmptyPriorityQueue()\n' +
      '8:for\\(i\\gets 0\\ to numTopLevelBlocks - \\(1\\)do\n' +
      '9: PriorityQueueInsert(\\(Q,[i],\\infty\\))\n' +
      '10:\\(\\text{spentBudget}\\gets 0\\)\n' +
      '11:topNodeContrits \\(\\leftarrow\\) CreateEmptyDictionary()\n' +
      '12:repeat\n' +
      '13: (주소 Prefix, priority) \\(\\leftarrow\\) PriorityQueuePop(\\(Q\\))\n' +
      '14 : 블록노드 \\(\\leftarrow\\{n\\in N|\\text{StartsWith}(address(n),\\text{address Prefix})\\}\\)\n' +
      '15: blockContribution \\(\\leftarrow\\)\\(|\\mathcal{I}\\)(blockNodes) \\(|\\)\n' +
      '16:\\(\\text{spentBudget}\\leftarrow\\) spentBudget + \\(1\\)\n' +
      '17:ifblockNodes = \\(\\{n\\}\\ for some \\(n\\in N\\)then\n' +
      '18 : topNodeContribs\\([n]\\leftarrow\\) 블록 기여도\n' +
      '19:else\n' +
      '20:for\\(i\\gets 0\\~\\(B-1\\)do\n' +
      '21:if\\(\\{n\\in\\text{blockNodes}|\\text{StartsWith}(address(n),\\text{addressPrefix}+[i]\\} \\neq\\emptyset\\)then\n' +
      '23: PriorityQueueInsert(\\(Q,\\text{addressPrefix}+[i],\\min(\\text{blockContribution, priority}))\n' +
      '24:untilspentBudget = \\(M\\) 또는 PriorityQueueEmpty(\\(Q\\))\n' +
      '25:return topNodeContribs\n' +
      '```\n' +
      '\n' +
      '**알고리즘 4** 인과적 귀속을 위한 계층적 알고리즘, 반복적 형태이다. 실제로 우리는 라인 14에서 한 번에 단일 블록을 평가하기보다는 추가 배치를 수행한다.\n' +
      '\n' +
      '\\(\\log\\left(\\frac{\\operatorname{attr}_{\\text{patch}}^{\\prime}(n^{\\ell})_{k}}{1-\\operatorname{attr}_{\\text{patch}}^{\\prime}(n^{\\ell})_{l}\\right)\\right)\\. 이를 종합하면, 주의 헤드당 \\(\\operatorname{attnLogits}_{\\text{patch}(n^{q})\\) 플롭을 이용하여 \\(\\hat{I}_{\\text{APflx}}^{K}(n^{t};x^{noise}})\\)의 모든 키를 \\(x^{text{clean}})의 모든 쿼리와 결합하여 \\(\\Delta\\operatorname{attnLogits}(n^{q})\\), 그리고 \\(\\Delta_{t}\\operatorname{attn}(n^{q})_{t}) 플롭을 연산할 수 있고, 따라서 \\(\\hat{I}_{\\text{APflx}}^{K}(n^{t};x^{text{noise}})\\(\\hat{I}_{\\text{APflx}}^{K}(n^{t};x^{q})\\)의 모든 키를 \\(x^{clean}})의 모든 쿼리와\n' +
      '\n' +
      '알고리즘 4는 단일 주의 헤드에서 키 노드(n_{1}^{k},\\dots,n_{t}^{k}\\)에 대한 수정 AtP 추정치(\\hat{\\epsilon}_{\\text{APflx}}^{K}(n_{t}^{k})\\)에 대한 일부 질의 노드(n^{q}\\)와 프롬프트 쌍(x^{text{clean},x^{noise}\\)의 기여도를 계산하며, 숫자 오버플로를 피하면서 \\(O(T)\\) 플롭스를 사용한다. 우리는 섹션 3.1.1에서 \\(\\operatorname{attn}(n^{q})\\), \\(\\operatorname{attn}_text{patch}}^{t}(n^{q})\\), \\(\\Delta_{t}\\operatorname{attn}(n^{q})\\), \\(\\operatorname{attnLogits}(n^{q})\\), \\(\\operatorname{attnLogits}(n^{q})\\), \\(s_{t}\\)의 표기법을 재사용하여 프롬프트 쌍을 암묵적으로 남긴다.\n' +
      '\n' +
      '```\n' +
      '0:\\(\\mathbf{a}:=\\operatorname{attnLogits}(n^{q})\\), \\(\\mathbf{a}^{\\text{patch}:=\\operatorname{attnLogits}_{\\text{patch}}(n^{q})\\), \\(\\mathbff{g}:=\\frac{\\partial\\mathcal{L}\\left(\\mathcal{M}(x^{\\text{clean}}) \\right)}{\\partial\\operatorname{attn}(n^{q}})\\)\n' +
      '1:\\(t^{*}\\leftarrow\\operatorname{argmax}_{t}(a_{t})\\)\n' +
      '2:\\(\\ell\\leftarrow\\mathbf{a}-a_{t^{*}}-\\log\\left(\\sum_{t}e^{a_{t}-a_{t^{*}}}\\right)\\)\\(\\triangleright\\) Clean log attn 가중치, \\(\\ell=\\log(\\operatorname{attn}(n^{q}))\\)\n' +
      '3:\\(\\mathbf{d}\\leftarrow\\ell-\\log(1-e^{\\ell})\\)\\(\\triangleright\\) Clean Logodds, \\(d_{t}=\\log\\left(\\frac{\\operatorname{attn}(n^{q})_{t}}{1-\\operatorname{attn}(n^{q})_{t}}\\right)\\(d_{t}=\\log\\left(\\frac{\\operatorname{attn}(n^{q})_{t}}\\right)\n' +
      '4:\\(d_{t^{*}}\\gets d_{t^{*}}-\\max_{t\\neq t^{*}}a_{t}-\\log\\left(\\sum_{t^{*}\\neq t^{*}}e^{a_{t^{*}}-\\max_{t\\neq t^{*}}a_{t}}\\right)\\(\\triangleright\\) Adjust \\(\\mathbf{d}})\\(a_{t^{*}}\\gg\\max_{t\\neq t^{*}}a_{t}\\right)에 대해 더 안정됨\n' +
      '5:\\(\\ell^{\\text{patch}}\\leftarrow\\operatorname{logigmoid}(\\mathbf{d}+\\mathbf{a}^{text{patch}-\\mathbf{a})\\)\\(\\triangleright\\) Patched log attn weight, \\(\\ell_{t}^{\\text{patch}}=\\log(\\operatorname{attn}_{\\text{patch}}^{t}(n^{q})_{t})\\(\\triangleright\\) Patched log attn weight, \\(\\ell_{t}^{t}}}\\log(\\operatorname{attn}_text{patch}}^{t}(n^{q})_{t})\\)\n' +
      '6:\\(\\Delta\\ell\\leftarrow\\ell^{\\text{patch}}-\\ell\\)\\(\\triangleright\\)\\(\\Delta\\ell_{t}=\\log\\left(\\frac{\\operatorname{attn}_{\\text{patch}}^{t}(n^{q})_{t}}{ \\operatorname{attn}(n^{q})_{t}}\\right)\\)\n' +
      '7:\\(b\\leftarrow\\operatorname{softmax}(\\mathbf{a})^{\\top}\\mathbf{g}\\)\\(\\triangleright\\)\\(b=\\operatorname{attn}(n^{q})^{\\top}\\mathbf{g}\\)\n' +
      '8:for\\(t\\gets 1\\~\\(T\\)do\\(\\triangleright\\) Compute scaling factor \\(s_{t}:=\\frac{\\Delta_{t}\\operatorname{attn}(n^{q})_{t}}{1-\\operatorname{attn}(n^{q})_{t}}\\)\n' +
      '9:if\\(\\ell_{t}^{\\text{patch}}>\\ell_{t}\\)then\\(\\triangleright\\)\\(\\ell_{t}^{\\text{patch}\\gg\\ell_{t}\\)일 때 오버플로우를 피한다.\n' +
      '10:\\(s_{t}\\gets e^{d_{t}+\\Delta\\ell_{t}+\\log(1-e^{-\\Delta\\ell_{t}})}\\(\\triangleright\\)\\(s_{t}=\\frac{\\operatorname{attn}(n^{q})_{t}}{1-\\operatorname{attn}(n^{q})_{t}}}}\\operatorname{attn}(n^{q})_{t}}{left(1-\\frac{\\operatorname{attn}(n^{q})_{t}}{\\operatorname{attn}_{\\text{patch}}^{t}(n^{q})_{t}}}{t}(n^{q})_{t}}}\\right)\\(s_{t}=\\frac{\\operatorname{attn}(n^{q})_{t}}{{t}{{t\n' +
      '11:else\\(\\triangleright\\) \\(\\ell_{t}^{\\text{patch}}\\ll\\ell_{t}\\)일 때 오버플로우를 피한다.\n' +
      '12:\\(s_{t}\\leftarrow-e^{d}+\\log(1-e^{\\Delta\\ell_{t}})\\)\\(\\triangleright\\)\\(s_{t}=-\\frac{\\operatorname{attn}(n^{q})_{t}{1-\\operatorname{attn}(n^{q})_{t}}}\\left(1-\\frac{\\operatorname{attn}_{\\text{patch}}^{t}(n^{q})_{t}}{\\operatorname{attn}(n^{q})_{t}}{\\operatorname{attn}(n^{q})_{t}\\right)\\(s_{t}=-\\frac{\\operatorname{attn}(n^{q})_{t}}{1-\\operatorname{attn}(n^{q})_{t}}{\\operatorname{attn}(n^{q})_{\n' +
      '13:\\(r_{t}\\gets s_{t}(g_{t}-b)\\)\\(r_{t}=s_{t}(\\text{onehot}(t)-\\operatorname{attn}(n^{q}))^{\\top}\\mathbf{g}=\\Delta_{t}\\operatorname{attn}(n^{q})\\cdot\\frac{\\partial\\mathcal{L}\\left(\\mathcal{M}(x^{\\text{clean}})\\right}{\\partial\\operatorname{attn}(n^{q}})}\n' +
      '14:return\\(\\mathbf{r}\\)\n' +
      '```\n' +
      '\n' +
      '**알고리즘 4** 주의키 AtP 보정\n' +
      '\n' +
      '보정된 AtP 추정치 \\(\\hat{\\epsilon}_{\\text{APflx}}^{K}(n_{t}^{k})\\)는 식 (10)을 사용하여 계산될 수 있다. 즉, 이 어텐션 헤드에 대해 알고리즘 4 over 질의에서 반환 \\(r_{t}\\)을 합산하고 \\(x^{\\text{clean}},x^{\\text{noise}}\\sim\\mathcal{D}\\을 평균함으로써 계산된다.\n' +
      '\n' +
      '그래드롭의 특성\n' +
      '\n' +
      '3.1.2절에서는 직접 효과와 간접 효과 사이의 상쇄에 의해 발생하는 AtP 고장 모드를 해결하기 위해 GradDrop을 도입하였다. 즉, 전체 효과(일부 프롬프트 쌍에 대한 총 효과)가 \\(\\mathcal{I}\\text{direct}}(n)+\\mathcal{I}^{\\text{ indirect}}(n)\\)이면, 이는 상쇄에 가깝고, 비선형성으로 인해 \\(\\hat{I}\\text{AP}^{\\text{indirect}(n)\\)에서 작은 곱셈 근사 오차가 \\(|\\mathcal{I}\\text{direct}(n)+\\hat{I}^{\\text{indirect}(n)|\\)보다 작은 크기만큼 발생할 수 있다.\n' +
      '\n' +
      '개선된 추정량\\(\\hat{\\epsilon}_{\\text{AP}+\\text{GD}}(n)\\)으로 이 고장 모드를 해결하기 위해, GradDrop에는 3개의 desiderata가 있다:\n' +
      '\n' +
      '1. \\(\\hat{\\epsilon}_{\\text{AP}+\\text{GD}}(n)\\)은 \\(\\hat{\\epsilon}_{\\text{AP}}(n)\\)보다 작지 않아야 한다.\n' +
      '2. \\(\\hat{\\epsilon}_{\\text{AP}+\\text{GD}(n)\\)은 일반적으로 \\(\\hat{\\epsilon}_{\\text{AP}}(n)\\)보다 크지 않아야 하는데, 이는 오탐을 생성하므로 검증이 느려지고 주어진 예산에서 효과적으로 오탐을 생성할 수 있기 때문이다.\n' +
      '3. \\(\\hat{\\epsilon}_{\\text{AP}}(n)\\)이 취소 실패 모드를 겪고 있다면 \\(\\hat{\\epsilon}_{\\text{AP}+\\text{GD}(n)\\)이 \\(\\hat{\\epsilon}_{\\text{AP}(n)\\)보다 상당히 커야 한다.\n' +
      '\n' +
      '계층\\(\\ell\\)의 잔차 스트림 기여도를 나타내기 위해 가상 노드\\(n^{\\text{out}}_{\\ell}\\)을 사용하여 섹션 3.1.2에서 그래드롭이 어떻게 정의되었는지 회상해 보자.\n' +
      '\n' +
      'n:=\\mathbb{E}_{x^{\\text{AP}+\\text{GD}(n)}\\left[\\frac{1}\\sum_{\\ell=1}^{L}\\left|\\hat{I}}_{\\text{AP}+\\text{GD}_{\\ell}(n;x^{\\text{clean}},x^{\\text{noise}}}\\left[\\frac{1}\\sum_{\\ell}\\l}\\left|(n(x^{\\text{noise}})-n(x^{\\text{l}\\right}}\\left[\\frac{1}\\sum_{\\ell}\\l}\\left|(n(x^{\\text{noise})-n(x^{\\text{noise}}}\\text{L}\\left|\\right}}\\mathbb{E}{L}\\text{l}\\text{l}\\text{l}\\text{l}\\text{l}\\left|(n(x^{\\text{noise})-n(x^{\\text [=\\mathbb{E}_{x^{\\text{clean},x^{\\text{noise}}\\left[\\frac{1}{L-1}\\sum_{\\ell}}\\left|(n(x^{\\text{noise}})-n(x^{\\text{clean}))\\right.^{\\intercal}\\frac{\\partial\\mathcal{L}{\\partial n}(\\mathcal{M}(x^{\\text{clean}}\\mid\\text{do}(n^{\\text{out}}_{\\ell}}_{\\ell}(x^{\\text{clean}}))\\right|\\right]\\frac{L}{\\partial n}(\\mathcal{M}(x^{\\text{clean}}\\mid\\text{do}(n^{\\text{out}}_{\\ell}(x^{\\text{clean}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\n' +
      '\n' +
      '그레이드드롭의 행동을 더 잘 이해하기 위해 그라디언트\\(\\frac{\\partial\\mathcal{L}{\\partial n}\\)을 더 주의 깊게 살펴보자. 전체 기울기\\(\\frac{\\partial\\mathcal{L}{\\partial n}\\)는 노드\\(n\\)에서 출력까지의 모든 경로 기울기의 합으로 표현될 수 있다. 각각의 경로는 (스킵 접속을 통한 라우팅과 대조적으로) 그것이 통과하는 층들의 세트에 의해 특징지어진다. 우리는 경로\\(s\\)에 따른 기울기를 \\(\\frac{\\partial\\mathcal{L}_{s}}{\\partial n}으로 쓴다.\n' +
      '\n' +
      '(\\(\\mathcal{S}\\)은 층 \\(n\\)이 들어간 후 층들의 모든 부분집합들의 집합이라고 하자. 예를 들어, 직접효과 경로는 \\(\\emptyset\\in\\mathcal{S}\\)에 의해 주어진다. 그러면 전체 그래디언트는 다음과 같이 나타낼 수 있다.\n' +
      '\n' +
      '\\[\\frac{\\partial\\mathcal{L}{\\partial n}=\\sum_{s\\in\\mathcal{S}\\frac{\\partial\\mathcal{L}_{s}{\\partial n}. \\tag{16}\\\n' +
      '\n' +
      '우리는 \\(\\hat{\\hat{I}}^{s}_{\\text{AP}(n)=(n(x^{\\text{noise}})-n(x^{\\text{clean}))^{\\intercal}\\frac{\\partial\\mathcal{L}_{s}{\\partial n}\\)을 유추하고, \\(\\hat{\\hat{I}}_{\\text{AP}(n)=\\sum_{s\\in\\mathcal{S}}\\hat{\\hat{I}}^{s}_{\\text{AP}(n))을 분해할 수 있다. 어떤 층에서 GradDrop을 할 때의 효과는 \\(\\ell\\in s\\)으로 모든 항을 드롭하는 것이다. 즉,\n' +
      '\n' +
      '\\hat{\\hat{I}}_{\\text{GD}_{\\ell}}(n)=\\sum_{\\begin{subarray}{c}s\\in\\mathcal{S}\\\\ell\\notin s\\end{subarray}\\hat{\\hat{I}}^{s}_{\\text{AP}}(n). \\tag{17}\\mathcal{S}\\\\ell\\notin s\\end{subarray}\\hat{\\hat{I}}^{s}_{\\text{AP}}(n)\n' +
      '\n' +
      '이제 우리는 3 데시데라타에 대해 논의하기 위해 이 이해도를 사용할 것입니다.\n' +
      '\n' +
      '첫째, 대부분의 노드 효과는 대부분의 계층(예: Veit et al. (2016) 참조)과 거의 무관하며, 임의의 계층(\\(\\ell\\)에 대해\\(\\hat{\\hat{I}}_{\\text{AP}+\\text{GD}_{\\ell}(n)=\\hat{\\hat{I}}_{\\text{AP}(n)\\)을 갖는다. (K)를 중요한 하류층의 집합으로 하면, 이것은 첫 번째 데시데라툼을 충족시키는 \\(\\frac{1}{L-1}\\sum_{\\ell=1}^{L}\\left|\\hat{\\hat{I}}_{\\text{AP}+\\text{GD}_{\\ell}(n;x^{\\text{clean},x^{\\text{noise}})\\right|\\geq\\frac{L-|K|-1}\\left|\\hat{I}}_{\\text{AP}(n;x^{\\text{clean},x^{\\text{noise}})\\right|\\eq\\frac{L-|K|-1}\\left|\\hat{I}}_{\\text{AP}}(n;x^{\\text{noise}})\\right|\\eq\\frac{L-|K|-1}\\left|\\hat{I}}_{\\text{AP}}(n;x^{\\text{clean},x^{\\text{noise}}\n' +
      '\n' +
      '두 번째 데시데라툼에 대해: 각 \\(\\ell\\)에 대해 우리는 \\(\\left|\\hat{I}}_{\\text{GD}_{\\ell}}(n)\\right|\\leq\\sum_{s\\in \\mathcal{S}\\left|\\hat{\\hat{I}}^{s}^{n)\\text{AP}}(n)\\right|\\hat{I}}_{\\text{I}}}\\text{GD}\\right|\\lq\\frac{L-1}\\sum_\\lq\\frac{L-1}\\right|\\hat{\\hat{I}}^{s}(n)\\right|\\hat{I}}\\text{I}}\\text{GD}\\right|\\lq\\frac{L-1}\\right|\\hat{L-1}\\text{I}}_{\\text{I}}^{s}(n)\\right|\\hat{I}}\\text{I}}\\text RHS가 \\(\\alpha\\)배 크기(\\left|\\sum_{s}in S}\\hat{\\mathcal{I}}_{\\text{AP}}^{s}(n)\\right|=|\\hat{\\mathcal{I}}_{I}}(n)|\\)보다 훨씬 더 크려면, 서로 다른 경로들 사이에 상당한 상쇄가 존재해야 하며, 따라서 \\(\\sum_{s\\in S}\\left|\\hat{\\mathcal{I}}{\\text{AP}}(n)\\right|\\geq\\frac{(L-1)\\sigma\\left|\\sum_K}\\right\\left|\\sum_{s}in S}\\hat{\\mathcal{I}}^{s}(n)\\right|\\hat{\\mathcal{I}}{s}(n)\\right|\\hat{\\mathcal{I}}{s}(n)\\right|\\hat{\\mathcal{I}}{s}(n)\\right|\\hat{\\mathcal{I}} 이것은 가능하지만, 예를 들어 \\(\\alpha>3\\)에 대해서는 일반적으로 가능성이 없어 보인다.\n' +
      '\n' +
      '세 번째 데시데라툼을 생각해 보자. 즉, \\(|\\hat{\\mathcal{I}}_{\\text{AP}}(n)|\\ll|\\mathcal{I}\\left(n\\right)|\\ll|\\mathcal{I}\\left(n\\right)|\\ll|\\mathcal{I}^{\\text{direct}}(n)|\\approx|\\hat{\\mathcal{I}}_text{direct}(n)||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 그리고 나서, \\(\\left|\\sum_{s\\in S\\setminus\\emptyset}\\hat{\\mathcal{I}}_{\\text{AP}}^{s}(n)\\right|=\\left|\\hat{\\mathcal{I}}_{\\text{AP}}(n)-\\hat{\\mathcal{I}}_{\\text{I}}^{text{direct}}(n)\\right|\\gg|\\mathcal{I}\\left(n\\right)|\\t. \\(\\sum_{s\\in S\\setminus\\emptyset}\\hat{\\mathcal{I}}_{\\text{AP}}^{s(n)\\)의 합집합은 S\\(\\sum_{s\\in S\\hat{\\mathcal{I}}_{\\text{AP}}^{s(n)=\\hat{\\mathcal{I}}_{\\text{AP}}(n)-\\hat{\\mathcal{I}+\\text{GD}_{\\ell}(n)\\)의 합집합이다.\n' +
      '\n' +
      '그러면 \\(\\sum_{\\ell}\\left|\\hat{\\mathcal{I}}_{\\text{I}}(n)-\\hat{\\mathcal{I}}_{\\text{I}+\\text{GD}_{\\ell}(n)\\right|\\\\(\\left|\\hat{\\mathcal{I}}_{\\text{I}}(n)-\\hat{\\mathcal{I}}^{\\text{direct}}(n)\\right|\\)보다 훨씬 작을 수 있지만 직관적으로 가능성이 낮다. 그 비율이 \\(\\alpha\\), 즉 \\(\\sum_{\\ell}\\left|\\hat{\\mathcal{I}}_{\\text{AP}}(n)-\\hat{\\mathcal{I}}_{\\text{I}+\\text{GD}_{\\ell}(n)\\right|=\\alpha\\left|\\hat{\\mathcal{I}}_{\\text{I}}(n)-\\hat{\\mathcal{I}}^{\\text{direct}}(n)\\right|\\text{I}}+\\text{GD}_{\\text{I}}(n)\\right|라고 가정하자. 예를 들어, 모든 간접 효과들이 길이\\(1\\)의 경로들을 사용한다면, 그 결합은 디조인트 결합이므로, \\(\\sum_{\\ell}\\left|\\hat{\\mathcal{I}}(n)-\\hat{\\mathcal{I}+\\text{GD}_{\\ell}(n)\\right|\\geq\\left|\\sum_{\\ell}\\left(\\hat{\\mathcal{I}}{I}}(n)-\\hat{\\mathcal{I}}(n)\\right|=\\left|\\hat{\\mathcal{I}}^{\\text{Direct}}(n)\\text{I}}^{\\text{I}}(n)\\hat{\\mathcal{I}}^{\\text{I}}(n)\\hat{\\mathcal{I}}^{\\text{I}}(n)\\hat{\\mathcal{I}}+\\text{GD}_{\\ell}(n)\\right|=\\left|\\hat{\\mathcal{I}}^{\\text{I}}(n 이제\n' +
      '\n' +
      '\\text{GD}_{\\ell\\in K}\\left|\\hat{\\mathcal}(n)}\\text{GD}_{\\ell\\in K}\\left|\\hat{\\mathcal}(n)}\\text{I}\\text{I}\\right|\\hat{\\mathcal}(n)\\text{I}\\text{I}\\right|\\hat{L}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\\text{I}\n' +
      '\n' +
      '그리고 RHS는 \\(\\alpha\\left|\\hat{\\mathcal{I}}_{\\text{AP}}(n)\\right|\\)이상 개선되어 \\(\\alpha\\left|\\hat{\\mathcal{I}}_{\\text{direct}}(n)\\right|>(2|K|+\\alpha)\\left|\\hat{\\mathcal{I}}(n)\\right|\\)으로 가정될 수 있다.\n' +
      '\n' +
      '그러나 궁극적으로 데시데라타는 GradDrops가 위음성의 수를 감소시키거나 그대로 둠으로써 추가 후방 패스의 초기 초기 비용 외에 성능을 향상시키는 실험을 통해 검증된다.\n' +
      '\n' +
      '컴퓨팅 진단을 위한### 알고리즘\n' +
      '\n' +
      '알고리즘 1에서 구한 모든 노드 \\(n\\)에 대한 요약통계 \\(\\tilde{i}_{\\pm}\\), \\(s_{\\pm}\\) 및 \\(\\text{count}_{\\pm}\\)과 임계값 \\(\\theta>0\\)이 주어지면, 우리는 \\(|\\tilde{i}_{+}-\\tilde{i}_{-}|\\geq\\theta\\)라는 가설을 검정하기 위해 Welch의 \\(t\\)-검정 Welch(1947)를 사용할 수 있다. 구체적으로 우리는 \\(t\\)-통계적 비아를 계산한다.\n' +
      '\n' +
      '\\frac{s}_{i_{pm} =\\frac{s_{\\pm}{\\sqrt{\\text{count}_{\\pm}} \\tag{24}\\[t =\\frac{\\theta-|\\tilde{i}_{+}-\\tilde{i}_{-}|}{\\sqrt{s_{i_{+}}^{2}+s_{i_{-}}^{2}}}. \\tag{25}\\tag{25}}} 유효자유도\\(\\nu\\)는 Welch-Satterthwaite 방정식으로 근사할 수 있다.\n' +
      '\n' +
      '\\frac{\\left(\\nu_{\\text{c}}^{2}}{\\text{count}_{c}}^{2}}{\\text{count}_{*}}\\right)^{2}{\\frac{s_{\\text{c}}^{4}{\\text{ count}_{*}^{2}(\\text{count}_{*}-1}}+\\frac{s_{*}^{2}(\\text{count}_{*}-1}}{tag{26}}}\n' +
      '\n' +
      '그런 다음 적절한 지점에서 학생의 \\(t\\left(x;\\nu_{\\text{Welch}}\\right)의 누적 분포 함수를 사용하여 관측된 것만큼 큰 \\(t\\)을 얻을 확률(\\(p\\)-값)을 계산한다. 우리는 집합 상한을 구하기 위해 모든 노드의 개별 \\(p\\)-값의 최대값을 취한다. 마지막으로 이진 탐색을 사용하여 주어진 목표 값보다 작은 집합(p\\(p\\) 값을 갖는 가장 큰 임계값(\\theta\\)을 찾는다. 우리는 부록 B.3에서 서로 다른 신뢰 수준(\\(1-p_{\\text{target}}\\))에 대해 여러 가지 진단 곡선을 보여준다.\n' +
      '\n' +
      '## 부록 B 실험\n' +
      '\n' +
      '### Prompt Distributions\n' +
      '\n' +
      '#### b.1.1 Iqi\n' +
      '\n' +
      '다음 프롬프트 템플릿을 사용합니다.\n' +
      '\n' +
      '```\n' +
      'BOS : _[A] _and _[B] _went _to _the _bar, _[A/C] _gave _a _drink _to _[B/A]\n' +
      '```\n' +
      '\n' +
      '각 클린 프롬프트\\(x^{\\text{clean}}\\)은 완료 B가 있는 A와 B 두 개의 이름을 사용하고, 노이즈 프롬프트\\(x^{\\text{noise}}\\)은 완료 A가 있는 A, B, C 이름을 사용한다. {Michael, Jessica, Ashley, Joshua, David, Sarah}의 집합에서 이름을 선택하여 120개의 프롬프트 쌍을 생성한다.\n' +
      '\n' +
      '#### b.1.2 A-Asn\n' +
      '\n' +
      '다음 프롬프트 템플릿을 사용하여 부정관사의 예측을 유도한다.\n' +
      '\n' +
      '```\n' +
      'BOS I _want _one _pear _Can _you _pick _up _a _pear _for _me?\n' +
      '```\n' +
      '\n' +
      'LJ _want _one _orange _Can _you _pick _up _an _orange _for _me?\n' +
      'LJ _want _one _[OBJECT] _Can _you _pick _up _[a/an]\n' +
      '```\n' +
      '\n' +
      '우리는 소형 모델의 제로 샷 성능이 상대적으로 낮았지만 각 사례의 단일 예를 제공할 때 성능이 급격히 향상되었음을 발견했다. 모델 성능은 두 예제의 순서에 민감했지만 모든 경우에 무작위보다 우수했다. 소수 샷 순서의 영향의 크기와 부호는 일관성이 없었다.\n' +
      '\n' +
      '깨끗한 프롬프트 \\(x^{\\text{clean}}\\)에는 {보트, 코트, 드럼, 뿔, 지도, 파이프, 나사, 스탬프, 텐트, 벽} 중 하나인 \'_a\'를 유도하는 물체가 포함되어 있다. 소음 프롬프트(x^{\\text{noise}}\\)는 사과, 개미, 도끼, 상, 코끼리, 달걀, 오렌지, 오븐, 양파, 우산 중 하나인 \'_an\'을 유도하는 물체를 포함한다. 그러면 총 100개의 프롬프트 쌍이 생성됩니다.\n' +
      '\n' +
      '### 배포에 걸친 취소\n' +
      '\n' +
      '섹션 2에서 언급했듯이 평균 효과의 크기를 취하기보다는 분포에 걸쳐 효과의 크기를 평균화한다. 우리는 효과의 취소가 분포에 걸쳐 자주 발생하고 있기 때문에 부정확한 추정치와 함께 상당한 위음성으로 이어질 수 있기 때문에 이 작업을 수행한다. 이 효과를 정확하게 정량화하기 위한 적절한 절제 연구는 이 작업의 범위를 벗어난다. 그림 10에서 우리는 다양한 모델 크기에 대한 IOI 분포에 걸친 상쇄 정도를 보여준다. 이를 위해 노드\\(n\\)의 _Cancellation Ratio_를 다음과 같이 정의한다.\n' +
      '\n' +
      '\\mathcal{I}\\left(n;x^{x^{text{clean}},x^{text{noise}}\\mathcal{I}\\left(n;x^{text{clean}},x^{text{noise}}\\right}\\mathcal{I}\\left(n;x^{text{clean}},x^{text{noise}}\\right}}\\mathcal{I}\\left(n;x^{text{clean}},x^{text{noise}}\\right}}\\mathcal{I}\\left(n;x^{text{clean}},x^{text{noise}}\\right}}\\mathcal{I}\\left(n;x^{clean}},x^{text{noise}}\\right}\\right}\\mathcal{I}\\left(n;x^{clean}},x^{text{noise}}\\right}\\mathcal{I}\\left(n;x^{clean}},x^{text{noise}}\\right}\n' +
      '\n' +
      '### 추가 상세결과\n' +
      '\n' +
      '그림 11의 조사된 모든 분포에 걸쳐 피티아-12B에 대한 진단 측정과 그림 12와 13의 모든 모델과 설정에 대해 검증된 100% 재현율 곡선의 비용을 보여준다.\n' +
      '\n' +
      '### Metrics\n' +
      '\n' +
      '본 논문에서는 손실의 차이(음수 로그 확률)를 메트릭 \\(\\mathcal{L}\\)으로 집중한다. 우리는 AtP(\\({}^{*}\\))가 \\(\\mathcal{L}\\)의 선택에 민감하지 않다는 몇 가지 증거를 제공한다. 피티아-12B의 경우 IOI-PP 및 IOI에서 세 가지 다른 메트릭에 대해 그림 14의 순위 산점도를 보여준다.\n' +
      '\n' +
      '또한 IOI의 경우 효과가 평가될 때 AtP\\({}^{*}\\)의 성능이 현저하게 나빠짐을 보여준다.\n' +
      '\n' +
      '그림 10: 다양한 모델 크기에 대한 IOI에 걸친 취소 비율. 1의 비율은 분포에 걸쳐 양의 효과와 음의 효과가 상쇄되는 반면, 0의 비율은 분포에 걸쳐 음의 효과 또는 양의 효과만 존재한다는 것을 의미한다. 우리는 \\(\\sum_{x^{\\text{clean}},x^{\\text{noise}}\\left|\\mathcal{I}\\left(n;x^{\\text{clean}},x^{\\text{noise}\\right)\\right||에 기초하여 노드의 백분위수에 따른 제거율을 보고한다.\n' +
      '\n' +
      '잡음제거 대신에 잡음제거를 통해(cf. Section 2.1). 현재로서는 이 관찰에 대한 만족스러운 설명이 없다.\n' +
      '\n' +
      '### Hyperparameter selection\n' +
      '\n' +
      '_iterative_ baseline 및 _AtP_ 기반 방법은 하이퍼파라미터를 갖지 않는다. 일반적으로 각 하이퍼파라미터 설정에 대해 5개의 무작위 시드를 사용하고 가장 낮은 IRWRGM 비용을 생성하는 설정을 선택했다(섹션 4.2 참조).\n' +
      '\n' +
      '_Subsampling_의 경우 두 하이퍼파라미터는 Bernoulli sampling 확률\\(p\\), 노드 검증 전에 수집해야 할 샘플 수는 \\(\\hat{\\epsilon}_{\\text{SS}\\)의 순으로 나타났다. (p\\)는 {0.01, 0.03}14에서 선택되었으며, 배치 크기는 설정에 따라 달라지는 2개의 배치 수 중에서 단계 수를 선택했다.\n' +
      '\n' +
      '각주 14: 우리는 \\(p\\)의 더 큰 값이 일관되게 성능이 떨어지는 것을 초기에 관찰했다. 우리는 \\(p\\)에 대해 더 세분화되고 더 작은 값을 조사하기 위해 향후 작업에 맡긴다.\n' +
      '\n' +
      '_Blocks_의 경우 블록 크기 2, 6, 20, 60, 250에 걸쳐 스위핑하였으며, _Hierarchical_의 경우 다음과 같은 휴리스틱 인수 때문에 \\(B=3\\)의 분기 계수를 사용하였다. 하나의 노드를 제외한 모든 노드가 제로 효과를 갖는다면, 그 노드를 발견하는 것은 계층 레벨들을 통해 반복하는 문제가 될 것이다. 우리는 몇 개의 레벨(\\log_{B}|N|\\)을 가질 것이고, 각 레벨에서 어떤 하위 레벨 블록이 특수 노드에 있는지 찾기 위해서는 \\(B\\) 순방향 패스가 필요할 것이며, 따라서 노드를 찾는 비용은 \\(B\\log_{B}|N|=\\frac{B}{\\logB}\\log|N|\\)일 것이다. (\\frac{B}{\\log B}\\)는 \\(B=e\\)에서, 또는 \\(B\\)이 정수여야 하는 경우 \\(B=3\\)에서 최소화된다. 다른 하이퍼모수는 수준의 수이며, 우리는 이것을 2에서 12로 쓸었다.\n' +
      '\n' +
      '도 11: 분포 전체에 걸쳐 12B에 대한 위음성의 진단.\n' +
      '\n' +
      '그림 12 | 검증된 100% 재현 곡선 비용, 뉴런 노드에 대한 모델 및 설정에 걸쳐 스위핑\n' +
      '\n' +
      '도 12: | 검증된 100% 재현 곡선, 뉴런노드에 대한 모델 및 설정에 걸친 스위핑 비용\n' +
      '\n' +
      '도 13 | 검증된 100% 재현 곡선 비용, AttentionNodes에 대한 모델 및 설정에 걸친 스위핑\n' +
      '\n' +
      '도 13: | 검증된 100% 재현 곡선, AttentionNodes에 대한 모델 및 설정에 걸친 스위핑 비용\n' +
      '\n' +
      '그림 14 \\(|\\) 다양한 메트릭 \\(\\mathcal{L}\\)을 사용하여 피티아-12B에서 AtP\\({}^{*}\\)에 대한 참 순위입니다. 마지막 행은 잡음 제거(잡음 제거) 설정에서 효과를 보여주는데, IOI가 정확하고 대체적인 다음 토큰에 대한 바이모달 분포를 생성하기 때문에 오른쪽 하위 플롯(로그-odds 잡음 제거)이 중간 하위 플롯(로짓-diff 잡음 제거)과 유사하다고 추측한다.\n' +
      '\n' +
      '## 부록 CatP 변종\n' +
      '\n' +
      '### 잔류 사이트 AtP 및 레이어 정규화\n' +
      '\n' +
      '잔류 스트림 사이트와 같이 잔류 스트림에서 전체 신호의 대부분 또는 전부를 포함하는 사이트에서 AtP의 행동을 고려해보자. 난다(2022)는 이 행동에 대한 우려를 설명했다: 패치된 값이 깨끗한 값과 크게 다르지만 유사한 규범을 가진 경우 계층 정규화의 선형 근사치가 제대로 작동하지 않을 것이다. 이를 설명하기 위해 제안된 AtP 수정은 역방향 패스를 계산할 때 고정된 스케일링 팩터(분모 내)를 유지하는 것이었다. 여기에서 우리는 이 수정이 AtP의 근사 오차에 어떻게 영향을 미칠지에 대한 분석을 제시할 것이다. (이 문제에 대한 실증조사는 본 논문의 범위를 벗어난다.)\n' +
      '\n' +
      '구체적으로, 고려 중인 노드를 \\(n\\), 깨끗하고 대체적인 값 \\(n^{\\text{clean}}\\) 및 \\(n^{\\text{noise}}\\)으로 하고, 단순화를 위해 모델이 파라메트릭되지 않은 RM-SNorm \\(\\mathcal{M}(n):=n/|n|\\)에 지나지 않는다고 가정하자. (n^{\\text{noise}})\\(\\hat{\\mathcal{M}}_{\\text{AP}}(n^{\\text{noise}}):=\\mathcal{M}(n^{\\text{clean})+\\mathcal{M}(n^{\\text{clean})^{\\perp}(n^{\\text{noise}}))\\(\\hat{\\mathcal{M}}_{\\text{atP}+\\text{foreanLN}(n^{\\text{clean}}))에 직교하는 초평면으로의 투영을 구하는 변형에 의해 계산된다.\n' +
      '\n' +
      '위의 오차를 정량화하기 위해 유클리드 거리 관점에서 오차\\(\\epsilon\\)을 측정할 것이다. 또한 일반성을 잃지 않고 \\(|n^{\\text{clean}}|=1\\)이라고 가정하자. 기하학적으로, \\(\\mathcal{M}(n)\\)은 단위 초구 위의 투영이고, \\(\\mathcal{M}_{\\text{AP}(n)\\)은 \\(n^{\\text{clean}}\\)에서 접선 초평면 위의 투영이며, \\(\\mathcal{M}_{\\text{atP}+\\text{foreanLN}\\)은 아이덴티티 함수이다.\n' +
      '\n' +
      '이제 \\(n^{\\text{clean}\\), \\(n^{\\text{noise}\\)에 의해 스팬된 평면상의 직교좌표 \\(x,y)\\)을 정의하여 \\(n^{\\text{clean}\\)이 \\(1,0)\\)에 매핑되고 \\(n^{\\text{noise}\\)이 \\(x,y)\\), \\(y\\geq 0\\)에 매핑되도록 하자. 그 다음, \\(\\epsilon_{\\text{atP}:=\\left|\\hat{\\mathcal{M}(n^{\\text{noise}})-\\mathcal{M}(n^{\\text{noise})\\right|=\\sqrt{x^{2}-2\\frac{\\kappa+y^{2}}{\\sqrt{x^{2}+y^{2}}}), \\(\\epsilon_{\\text{atP}+\\text{foreanLN}:=\\left|\\hat{\\mathcal{M}(n^{\\text{noise}})-\\mathcal{M}(n^{\\text{noise}})\\right|=\\left|\\sqrt{x^{2}+y^{2}-1\\right|\\tqrt{x^{2}+y^{2}}}), \\(\\epsilon_{\\text{atP}+\\text{foreanLN}:=\\left|\\hat{\\mathcal{M}\n' +
      '\n' +
      '그림 15의 오류를 그림으로 나타내면 예상대로 층노말분모를 동결하면 코사인 유사도가 \\(n^{\\text{clean}\\)보다 작을 때 \\(n^{\\text{noise}\\)과 동일한 규범을 가질 때, 그리고 (x>1\\)인 이상한 경우를 제외하고 cosine 유사도가 \\(\\frac{1}{2}\\)보다 작을 때 도움이 되지만, \\(n^{\\text{noise}\\)이 \\(n^{\\text{clean}\\)에 가까울 때 큰 고통을 받는다는 것을 알 수 있다. 이것은 분모를 동결하는 동안 전체 잔류 신호에 비해 패치 거리가 작을 때 일반적으로 도움이 되지 않을 것이라는 것을 보여준다(이 논문에서 고려된 거의 모든 노드와 마찬가지로), 상당히 정렬되지 않을 수 있지만 유사한 규범을 갖는 패치 잔류 스트림의 다른 설정에서 도움이 될 것이다.\n' +
      '\n' +
      '### Edge AtP 및 AtP*\n' +
      '\n' +
      '여기에서 우리는 에지 속성 패칭과 GradDrop 및/또는 QK 픽스를 사용하는 경우 비용 규모가 어떻게 되는지 조사할 것이다. (이 섹션에서는 단일 프롬프트 쌍에 초점을 맞출 것입니다.)\n' +
      '\n' +
      '먼저, 에지 속성 패치가 근사화하려고 하는 것과 그것이 어떻게 작동하는지 검토해보자.\n' +
      '\n' +
      '###### c.2.1 에지 개입 효과\n' +
      '\n' +
      '주어진 노드\\(n_{1},n_{2}\\)에서 \\(n_{1}\\)의 상류에 있는 \\(n_{1},n_{2}\\)이 주어지면 \\(n_{1}\\)에 대한 대체 값으로 패치하면 복잡한 비선형 방식으로 \\(n_{2}\\)에 영향을 미칠 수 있다. 3.1.2에서 논의된 바와 같이, LLM은 잔차 스트림을 가지고 있기 때문에, "직접 효과"는 잔차 스트림 기여에 대한 대체 값\\(n_{1}(x^{\\text{noise})\\(r_{\\text{out},\\ell_{1}}(x^{\\text{clean}})을 변형하는 비교적 간단한 함수로서 이해될 수 있다. 그리고, 변환 그림 15는 근사화하려는 모델이 단지 \\(\\mathcal{M}(n):=n/|n|)인 완구 설정에서 AtP와 AtP가 어떻게 작용하는지를 비교한 것이다. 빨간색 영역은 동결된 레이어노어 스케일링이 도움이 되는 부분이고 파란색 영역은 아픈 부분입니다. 우리는 \\(x>1\\)이 아니면, \\(n^{\\text{noise}}\\)과 \\(n^{\\text{clean}}\\) 사이의 코사인 유사성이 \\(<\\frac{1}{2}\\)일 때 동결층수 스케일링은 항상 더 낮은 오차를 갖지만, 그렇지 않으면 종종 더 높은 오차를 갖는다.\n' +
      '\n' +
      '(n_{2}^{\\text{direct}}\\).\n' +
      '\n' +
      '상기에서 \\(\\ell_{1}\\)과 \\(\\ell_{2}\\)은 각각 \\(n_{1}\\)과 \\(n_{2}\\)을 포함하는 반층이다. [\\(\\mathbf{n}_{(\\ell_{1},\\ell_{2})}\\)을 semilayers \\(\\ell_{1}\\)와 \\(\\ell_{2}\\) 사이의 non-residual node의 집합으로 정의하자. 그리고 생성된 \\(n_{2}^{\\text{direct}}\\)을 다음과 같이 정의할 수 있다.\n' +
      '\n' +
      '\\[n_{2}^{\\text{direct}^{\\ell_{1}}(x^{\\text{noise}}))):=n_{2}(x^{\\text{clean}}|\\operatorname{do}(n_{1}\\gets n_{1}(x^{\\text{noise}})),\\operatorname{do}(\\mathbf{n}_{(\\ell_{1},\\ell_{2}}\\leftarrow\\mathbf{n}_{(\\ell_{1},\\ell_{2}}}(x^{\\text{ clean})))))) =n_{2}(x^{\\text{clean}|\\operatorname{do}(n_{1}\\gets n_{1}(x^{\\text{noise}})),\\operatorname{do}(\\mathbf{n}_{(\\ell_{1},\\ell_{2}}\\leftarrow\\mathbf{n}_{(\\ell_{1},\\ell_\n' +
      '\n' +
      '잔류스트림 입력 \\(r_{\\text{in},\\ell_{2}}^{\\text{direct}^{\\ell_{1}}(x^{\\text{clean}}|\\operatorname{do}(n_{1}\\gets n_{1}(x^{\\text{noise}))))도 유사하게 정의된다.\n' +
      '\n' +
      '마지막으로, \\(n_{2}\\) 자체는 \\(\\mathcal{L}\\)을 계산하기에 충분하지 않으며, 또한 \\(\\mathcal{M}(x^{\\text{clean})\\)을 수정 \\(n_{2}^{\\text{direct}^{\\ell_{1}}(x^{\\text{clean}}|\\operatorname{do}(n_{1}\\gets n_{1}(x^{\\text{noise}))을 사용하여 전진 통과하도록 해야 한다.\n' +
      '\n' +
      '이걸 써내면 엣지 개입 효과가 있어\n' +
      '\n' +
      '\\[\\mathcal{I}(n_{1}\\to n_{2};x^{\\text{clean},x^{\\text{noise}}):= \\mathcal{L}(\\mathcal{M}(x^{\\text{clean}}|\\operatorname{do}(n_{2} \\gets n_{2}^{\\text{direct}^{\\ell_{1}}}(x^{\\text{clean}}}|\\operatorname{do}(n_{1}\\gets n_{1}(x^{\\text{noise}))}\\[-\\mathcal{L}(\\mathcal{M}(x^{\\text{clean}))}\\mathcal{L}(\\tag{27}\\text{n}}}}}}}\\text{m}(x^{\\text{n}}}}}}}}\\text{m}(x^{\\text{n}}}}}}}}}\\mathcal{L}(\\mathcal{M}(x^{\\text{\n' +
      '\n' +
      '###### 0.c.2. 노드와 에지\n' +
      '\n' +
      '이것을 평가하고자 하는 모서리가 무엇인지 간단히 생각해 봅시다. 섹션 4.1에서 우리는 MLP 뉴런에서 주의 노드를 편리하게 분리할 수 있었고, 두 종류의 노드를 모두 처리하려면 각 종류의 노드를 스스로 처리할 수 있어야 한다는 것을 알고, 결과를 결합할 수 있었다. 에지 개입의 경우 에지가 MLP 뉴런에서 주의 노드로 이동할 수 있고 그 반대도 마찬가지이기 때문에 물론 이것은 사실이 아니다. 이 섹션의 목적을 위해, 우리는 노드 집합 \\(N\\)이 주의 노드를 포함하고, MLP에 대해 계층당 노드(Syed et al. (2023)에서와 같이) 또는 뉴런당 노드(NeuronNodes 설정에서와 같이)라고 가정할 것이다.\n' +
      '\n' +
      '에지와 관련하여, MLP 노드는 임의의 업스트림 또는 다운스트림 노드와 합리적으로 연결될 수 있지만, 어텐션 노드에 대해서는 그렇지 않다: 어텐션 헤드의 키, 쿼리 및 값 노드는 해당 헤드의 어텐션 출력 노드를 통해서만 다운스트림 노드에 영향을 미칠 수 있고, 그 반대의 경우도 마찬가지이다. 결과적으로, 서로 다른 반계층 사이의 에지에서, 업스트림 어텐션 노드는 어텐션 헤드 출력이어야 하고, 다운스트림 어텐션 노드는 키, 쿼리 또는 값이어야 한다. 또한, 각 쿼리 노드를 동일한 위치의 출력 노드에 연결하고, 각 키 및 값 노드를 인과적으로 영향을 줄 수 있는 위치의 출력 노드에 연결하는 일부 주의 내 헤드 에지가 있다.\n' +
      '\n' +
      '###### 0.c.2.3 Edge AtP\n' +
      '\n' +
      '노드 활성화 패칭과 마찬가지로 에지 개입 효과\\(\\mathcal{I}(n_{1}\\to n_{2};x^{\\text{clean}},x^{\\text{noise})\\)는 매번 순방향 패스가 필요하기 때문에 모든 에지에 대해 직접 평가하는데 비용이 많이 든다. 그러나 AtP와 마찬가지로 우리는 1차 근사치를 적용할 수 있다: 우리는 정의한다\n' +
      '\n' +
      '\\mathcal{M}(\\hat{\\mathcal{I}}_{2};x^{\\text{clean},x^{\\text{noise}}):=\\left(\\operatorname{Jac}_{n_{1}}^{\\text{2}}(r_{2})(r_{1}})(n_{1}(x^{\\text{clean})),\\tag{30}\\nabla_{n_{2}}(n_{l}}(x^{\\text{clean}))\n' +
      '\n' +
      '이것은 \\(n_{1}(x^{\\text{noise}})\\approx n_{1}(x^{\\text{clean}})일 때 근사치이다.\n' +
      '\n' +
      '이 분해의 주요 이점은 첫 번째 항은 \\(n_{1}\\)에만 의존하고 두 번째 항은 \\(n_{2}\\)에만 의존한다는 것이며, 두 항 모두 AtP 자체처럼 \\(x^{\\text{clean}\\)에 대한 순방향 통과와 \\(x^{\\text{noise}\\)에 대한 순방향 통과로부터 계산하기 쉽다.\n' +
      '\n' +
      '그런 다음, 에지-AtP 평가를 완료하기 위해, 계산적으로 남아 있는 것은 각각의 토큰 위치에서, 상이한 반층들 내의 노드들 사이의 모든 내적을 평가하는 것이다. 이것은 총 15에서 \\(d_{\\text{resid}}\\mathcal{I}(1-\\frac{1}{L})|N|^{2}/2\\) 곱셈을 필요로 하며, 여기서 \\(L\\)은 레이어의 수, \\(T\\)은 토큰의 수, \\(|N|\\)은 노드의 총 수이다. 이 비용은 Pythia 2.8B에서 모든 \\(\\Delta\\alpha_{n_{1}}^{\\text{AtP}}(x^{\\text{clean},x^{\\text{noise})\\)와 \\(\\nabla_{r_{n_{2}}}^{\\text{AtP}}\\mathcal{L}(\\mathcal{M}(x^{\\text{clean}))\\)을 계산하는 비용을 초과한다. MLP 계층당 단일 노드라도 피티아 2.8B에서 더 큰 모델을 살펴보거나 특히 작은 모델이라도 단일 뉴런 노드를 고려할 경우 격차가 크게 증가한다.\n' +
      '\n' +
      '각주 15: 이 공식은 단순화를 위해 단일 층 내의 모서리를 생략하지만 작은 소수이다.\n' +
      '\n' +
      '이러한 관찰로 인해, 우리는 계산 비용의 2차 부분에 초점을 맞출 것이다. 즉, 모든 계산에서 하나의 노드(\\Delta r_{n_{1}}^{\\text{AtP}}(x^{\\text{clean},x^{\\text{noise}))가 아닌 두 노드(\\intercal}}\\nabla_{r_{n_{2}}}^{\\text{AtP}}\\mathcal{L}(\\mathcal{M}(x^{\\text{clean}))에 대한 계산 비용이다. 특히, 우리는 또한 "2차 비용"에서 주의 내 헤드 에지를 제외할 것이다: 이러한 에지, 일부 키, 쿼리 또는 값 노드에서 주의 출력 노드로의 이러한 에지는 해당 키, 쿼리 또는 값 노드에 대한 노드별 AtP 또는 AtP* 방법의 사소한 변형에 의해 처리될 수 있다.\n' +
      '\n' +
      '#### _C.2.4 MLPs_\n' +
      '\n' +
      'MLP 노드 주변에서 발생할 수 있는 몇 가지 문제가 있습니다. 하나는 섹션 3.1.1에 설명된 주의 포화 문제와 유사하게 MLP에 대한 선형 근사치가 경우에 따라 상당히 불량하여 \\(n_{2}\\)이 MLP 노드인 경우 상당한 위음성을 생성할 수 있다는 것이다. 또 다른 문제는 단일 뉴런 노드를 사용하면 매우 많아 에지당 \\(d_text{resid}})차원 내적이 상당히 비싸다는 것이다.\n' +
      '\n' +
      '주의 확률을 포화시키는 깨끗한 활성화가 영향을 크게 과소평가하는 작은 기울기를 가질 수 있듯이 MLP 포화 및 고정도 MLP 비선형성에 대해서도 마찬가지이다. 비슷한 수정이 적용 가능하다. \\(n_{1}\\)에서 \\(n_{2}\\)까지의 함수에 대한 선형 근사법을 사용하는 대신, 우리는 \\(n_{1}\\)에서 사전 활성화 \\(n_{2,\\text{pre}}\\)까지의 함수를 선형 근사하고, 그 다음 구배를 곱하기 전에 그것을 사용하여 다시 계산할 수 있다.\n' +
      '\n' +
      '구배-델타-활성화 내적이 \\(d_{\\text{resid}}\\)이 아닌 \\(d_{n_{2}}\\)차원으로 계산되는 이러한 종류의 재배열은 다시 나타날 것이다 - 우리는 이것을 AtP의 _factored_ 형태라고 부를 것이다.\n' +
      '\n' +
      '노드가 뉴런일 경우 인수화된 형태는 곱셈 횟수에 변화가 없으나, MLP 층일 경우 비용(d_{\\text{neurons}}\\)이 크게 증가한다. 이 증가는 두 가지 요인에 의해 완화된다: 하나는 주의 노드에서 끝나는 에지의 수가 \\(3\\times(\\#\\text{ head per layer})\\)만큼 수적으로 적은 소수의 에지이며, 다른 하나는 파라미터 공유의 가능성이다.\n' +
      '\n' +
      '뉴런 에지와 매개변수를 공유하는 유용한 관찰은 각 에지가 서로 다른 토큰16 위치에 걸쳐 \\(\\text{Jac}_{n_{1}}(r_{\\text{out},\\ell_{1})(n_{1}(x^{\\text{clean}))) 및 \\(\\text{Jac}_{r_{n_{1},\\ell_{2}}(n_{2})(r_{\\text{in},\\ell_{2}}(x^{\\text{clean}))에서 동일한 매개변수 행렬을 재사용한다는 것이다. 실제로, MLP 활성화 함수를 제외하고, 이러한 함수에서 유일한 다른 비선형성은 계층 정규화이며, 섹션 3.1에서와 같이 스케일링 팩터를 깨끗한 값으로 동결하면 자코비안은 해당 매개변수 행렬의 곱과 동일하며, 이를 깨끗한 스케일링 팩터로 나눈다.\n' +
      '\n' +
      '각주 16: 하나 이상의 프롬프트 쌍에서 이 작업을 수행하는 경우 다른 배치 항목에도 적용됩니다.\n' +
      '\n' +
      '따라서, 각 토큰에서 파라미터 행렬들을 미리 곱하면 뉴런-뉴런 에지에 대한 token당 2차 비용을 \\(d_{\\text{resid}}\\)(즉, 스칼라 곱셈)으로 줄이거나 뉴런과 일부 주의 부위 사이의 에지에 대한 \\(d_{\\text{resid}/d_{\\text{site}}\\)(즉, 어떤 주의 부위 사이의 에지에 대한 \\(d_{\\text{site}}}}) 차원 내적만큼 줄일 수 있다.\n' +
      '\n' +
      '그러나 이러한 사전 곱셈 매개변수 행렬(또는 실제로 뉴런 사이트를 사용하는 경우 에지-Atp 추정치)은 MLP 가중치 자체보다 여러 배(구체적으로, \\((L-1)\\frac{d_{\\text{resid}}}{4d_{\\text{resid}}}}\\) 더 클 것이므로 저장을 신중하게 고려해야 할 필요가 있다. 모든 모서리에 대한 전체 추정치보다는 가장 큰 추정치 또는 일부 임계값 이상의 추정치만 찾는 방법을 고려할 가치가 있을 수 있다.\n' +
      '\n' +
      '###### c.2.5 Edge AtP* 비용\n' +
      '\n' +
      '이제 섹션 3.1의 AtP* 제안을 이 설정에 적용하는 방법을 고려해보자. 우리는 이미 QK 픽스와 유사하게 동기화된 MLP 픽스가 뉴런 노드에서는 무시할 수 있는 비용을 갖지만, 노드당 MLP 레이어를 사용하는 경우 2차 비용에서 최소한 가장자리에서 MLP 노드로의 오버헤드가 발생한다는 것을 알고 있다. 우리는 MLP 픽스가 edge-AtP*의 일부라고 간주할 것이다. 이제 정규 AtP*에서 두 가지 수정, 즉 GradDrops와 QK 수정에 대해 조사해보자.\n' +
      '\n' +
      'GradDropsGradDrops는 AtP 공식의 단일 백워드 패스를 \\(L\\) 백워드 패스로 대체함으로써 작동한다. 이것은 실제로 승산(\\nabla_{r_{2}}^{\\text{AP}}\\mathcal{L}(\\mathcal{M}(x^{\\text{clean}))에 대한 \\(L\\) 값을 의미하므로, 이것은 2차 비용에 대한 \\(L\\)의 승산 인자이다(실제로 이들 중 일부는 중복될 것이지만, 이를 고려하여 승산 인자를 \\((L+1)/2\\)으로 구동시킬 수 있다). 특히 이것은 뉴런 에지에 사용되는 "팩터링된 AtP"와 동등하게 잘 작동하며, 특히 \\(n_{2}\\)이 뉴런이라면 \\(n_{1}\\)에 걸쳐 구배를 쉽게 결합하고 공유할 수 있어 \\((L+1)/2\\)의 2차 비용 오버헤드를 제거한다.\n' +
      '\n' +
      '그러나 GradDrops에 대한 동기는 효과가 상쇄될 수 있는 여러 경로를 설명하는 것이었는데, edge-interventions 설정에서 이는 이미 다른 방식으로 발견될 수 있다(n_{2}\\ 중 책임 있는 edge를 식별함으로써). 따라서 GradDrops의 이점은 줄어든다. 동시에, 비용은 여전히 상당하다. 따라서 권장 절차 edge-AtP*에서 GradDrop을 생략합니다.\n' +
      '\n' +
      'QK 픽스는 서로 다른 입력\\(\\nabla_{n_{n_{2}}(\\mathcal{L}(\\mathcal{M}(x^{\\text{clean})))(n_{2}(x^{\\text{clean}))) 항, 즉 소프트맥스에 대한 선형 근사치를 소프트맥스의 변화에 대한 정확한 계산으로 대체하는 것에 적용된다. 섹션 3.1.1에서와 같이, 질의 노드인 \\(n_{2}\\)을 더 간단하게 설명하는 경우와 알고리즘 4를 사용하여 핵심 노드인 \\(n_{2}\\)을 더 복잡하게 설명하는 경우가 있지만, 이들 모두는 \\(n_{2}\\)에 해당하는 \\(\\Delta\\operatorname{attnLogits}\\)을 계산한 후에 하기에는 저렴하다.\n' +
      '\n' +
      '"팩터화된 AtP" 방식은 키 또는 질의 가중치를 갖는 행렬 다중(\\Delta r_{n_{1}}^{\\text{AP}}(x^{\\text{clean}},x^{\\text{noise}})과 깨끗한 질의 또는 키를 갖는 행렬 다중(\\Delta r_{n_{1}}^{\\text{AP}}(x^{\\text{clean}},x^{\\text{noise}})이다. 이것은 AtP를 사용하여 각 에지\\(n_{1}\\to n_{2}\\)에 필요한 \\(d_{\\text{resid}\\) 곱셈 대신, \\(d_{\\text{resid}d_{\\text{key}}+Td_{\\text{key}\\) 곱셈(이는 인과 마스크 덕분에 \\(d_{\\text{key}}(d_{\\text{resid}}+(T+1)/2))의 평균으로 줄일 수 있음을 의미한다.\n' +
      '\n' +
      '"unfactored" option은 \\(r_text{in},\\ell_{2}}) 공간에 머무르는 것이다. 즉, 깨끗한 질의 또는 키를 각각의 키 또는 질의 가중치 행렬로 미리 곱한 후, 각각 \\(\\Delta r_{n_{1}}^{\\text{AP}(x^{\\text{clean},x^{\\text{noise}))의 내적을 취한다. 이러한 방법으로 계산 비용의 2차 부분은 \\(d_{\\text{resid}}(T+1)/2\\) 곱셈을 포함하고, 이는 짧은 시퀀스 길이에 대해 더 효율적일 것이다.\n' +
      '\n' +
      '이는 키 노드와 질의 노드로의 에지에 대해 2차 비용에서 AtP+QKfix를 수행하는 오버헤드가 \\(\\min\\left(\\frac{T+1}{2},d_{\\text{key}}\\left(1+\\frac{T+1}{2d_{\\text{resid}}} \\right)\\right)\\의 곱셈 인자임을 의미한다.\n' +
      '\n' +
      'QK fix + GradDrops QK fix가 GradDrops와 결합된다면, 첫 번째 곱셈인 d(d_{\\text{resid}}\\times d_{\\text{key}}\\) 행렬은 서로 다른 그라디언트들 사이에서 공유될 수 있다. 따라서 QKfix + GradDrops의 2차 비용에 대한 오버헤드는 팩토링된 방법을 사용하여 쿼리와 키로의 에지에 대해 \\(d_{\\text{key}}\\left(1+\\frac{(T+1)(L+1)}{4d_{\\text{resid}}\\right)이다.\n' +
      '\n' +
      '### Conclusion\n' +
      '\n' +
      '위의 모든 가능성을 고려할 때 모든 상황에서 정확성과 계산 비용 사이에 가장 좋은 절충점이 어디에 있는지 분명하지 않다. 표 2에서 우리는 우리가 언급한 변화에 걸쳐 각 종류의 에지에 대한 2차 비용에서 곱셈의 수를 측정하는 공식을 제공한다. 그림 16에서 수치 비교를 가능하게 하기 위해 그림 2와 같은 논문의 다른 곳에서 사용되는 4가지 크기의 피티아 모델을 연결한다.\n' +
      '\n' +
      '## 부록 D 참효과 분포\n' +
      '\n' +
      '그림 17에서 우리는 모형과 분포에 걸친 \\(c(n)\\)의 분포를 보여준다.\n' +
      '\n' +
      '그림 16 | 모델 크기 및 프롬프트 길이에 걸친 에지-AtP 변형의 비교. 여기서 AtP*는 QKfix 및 MLPfix를 포함하도록 정의되지만 GradDrops는 포함하지 않는다. 비용은 각 설정에 따라 수십 배마다 다릅니다.\n' +
      '\n' +
      '전체-MLP 노드를 갖는 설정에서, MLPfix는 짧은 프롬프트에 대해 상당한 비용을 수반하지만, 긴 프롬프트에 대해서는 거의 중요하지 않다.\n' +
      '\n' +
      '뉴런-노드 설정에서, MLPfix는 비용이 들지 않는다. 그러나 이러한 환경에서 GradDrops는 계속해서 많은 비용을 부과한다; 비록 MLP\\(\\rightarrow\\)MLP 에지에 영향을 미치지는 않지만, QKfix로 비용을 지배하는 MLP\\(\\rightarrow\\)Q,K 에지에 영향을 미친다.\n' +
      '\n' +
      '도 17 | 모델 및 프롬프트 쌍 분포에 걸친 실제 효과 분포\n' +
      '\n' +
      '도 17: | 모델 및 프롬프트 쌍 분포에 걸친 참 효과 분포\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>