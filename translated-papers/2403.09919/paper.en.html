<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Recurrent Drafter for Fast Speculative Decoding\n' +
      '\n' +
      'in Large Language Models\n' +
      '\n' +
      'Aonan Zhang\\({}^{*}\\) Chong Wang\\({}^{*}\\), Yi Wang\\({}^{*}\\), Xuanyu Zhang and Yunfei Cheng\n' +
      '\n' +
      'Apple\n' +
      '\n' +
      '{aonan_zhang,mr.chongwang,wyi,xuanyu_zhang,yunfei_cheng}@apple.com\n' +
      '\n' +
      'Equal contributions.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attention structure only for inference in Medusa. We empirically demonstrate the effectiveness of the proposed method on several popular open source language models, along with a comprehensive analysis of the trade-offs involved in adopting this approach.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large language models (LLM) (Anil et al., 2023; Brown et al., 2020) represent a rapidly evolving field within machine learning, offering tremendous promise and potential for advancements in various domains. These models are usually large, with billions of parameters and it is believed that the size of models is one of the most important factors responsible for the capabilities of these language models. Primarily, LLMs use auto-regressive methods to generate token-by-token responses given the prompts, meaning each forward run can generate only one token. The latency of the single token generation step significantly increases when the model size is large. This issue assumes critical importance in contemporary hardware environments, where memory bandwidth constraints often pose limitations, while computational resources remain abundant.\n' +
      '\n' +
      'Recently, speculative decoding (Leviathan et al., 2023; Chen et al., 2023; Spector and Re, 2023; Cai et al., 2024; Bhendawade et al., 2024) has emerged as a promising strategy to accelerate LLM inference, aiming to mitigate the challenges mentioned earlier. Speculative decoding uses a smaller draft model to generate preliminary candidate tokens more efficiently. Subsequently, these candidate tokens undergo verification (usually through a rejection sampling procedure) by the larger target model in a single forward pass, optimizing memory bandwidth usage and overall computational cost. This often can significantly decrease the latency of LLM inference. Among these methods, Leviathan et al. (2023); Chen et al. (2023a); Spector and Re (2023) choose to use separate small model(s) as the draft models. Conversely, the most recent approach, Medusa (Cai et al., 2024), adopts a single, unified model strategy by training multiple draft heads using the output from the target model as input. The single-model approach, exemplified by Medusa, holds promise for easier integration into existing LLM serving systems. More recently, speculative streaming (Bhendawade et al.,2024) also took a single model approach without using additional heads and they tune the target model (instead of simply keep the its weight frozen) by predicting n-grams instead of just next tokens. In this regard, speculative streaming is similar to the updated Medusa 2 approach in Cai et al. (2024), which jointly trains the target model and draft heads to obtain a more speculative-decoding friendly target model with good improvement.\n' +
      '\n' +
      'In this paper, we extend the principle established in Medusa (Cai et al., 2024), adopting a single-model strategy to enhance speculative decoding. Our approach, termed the _recurrent drafter_ (ReDrafter), addresses limitations presented in the Medusa framework. Unlike Medusa, which necessitates multiple draft heads with distinct parameters to indicate predictive positions, our recurrent drafter introduces dependencies among predictive heads, drawing inspirations from recurrent neural network (RNN) language models (Mikolov and Zweig, 2012). We only have one set of parameters for the draft head and these parameters are shared across different predictive positions. Additionally, Medusa\'s independent prediction mechanism results in an exponentially large set of feasible candidate token sequences. To mitigate this issue, the authors of Medusa introduce a clever tree attention approach. This is optimized via a separate greedy procedure to identify predefined tree paths using validation data. In contrast, with the introduced dependencies among draft heads, our approach allows for direct use of beam search to filter out low-quality candidates, significantly reducing the number of candidate token sequences for verification by the target model. Furthermore, we present an efficient tree attention algorithm based on beam search results, dynamically constructed during runtime rather than predetermined. Unlike the tree attention construction in Medusa which depends on validate data sets, our procedure does not rely on additional data. This makes it easier to be deployed after training is done. Finally, we empirically demonstrate the effectiveness of the proposed methodology.\n' +
      '\n' +
      '## 2 Recurrent Drafter for Speculative Decoding\n' +
      '\n' +
      'Model definition.We now outline the formulation of the proposed recurrent drafter for speculative decoding. Similar to the Medusa approach, we use the last layer\'s output of the transformer from the target model as input for the recurrent draft heads. Additionally, we incorporate the embeddings of historical tokens as recurrent inputs to the draft head, as depicted in Figure 1.\n' +
      '\n' +
      'We use the standard RNN design to predict the next token. For instance, considering the token sequence, This is very good news, as illustrated in Figure 1, once the target model generates the token is with last layer\'s output \\(x\\), the draft head uses the embedding of token is to update its RNN hidden state and combine with output \\(x\\) to predict the next token very. This strategy is recurrently applied for subsequent tokens. In this paper, we opt for a simple recurrent design to model the connections among the shared draft heads, deferring more complex model choices to future investigations. In particular, we initialize the hidden state of the draft head as \\(s_{0}=e_{0}\\), where \\(e_{0}\\) is the embedding of the last token that target model has already produced. To predict the \\(i\\)th token using the draft head, we first update the hidden state of the draft head as,\n' +
      '\n' +
      '\\[s_{i} =f(Us_{i-1}+We_{i}+b)\\] \\[h_{i} =[s_{i},x],\\]\n' +
      '\n' +
      'where \\(f\\) is the activation function and \\(U\\), \\(W\\) and \\(b\\) is the parameter for the RNN (Mikolov and Zweig, 2012). We only use one layer RNN to make the model simple. Then we apply a few layers of ResNet (He et al., 2016) with a standard softmax layer at the end.\n' +
      '\n' +
      'Since the parameters of the draft heads are shared, the number of parameters remains constant even if the model is trained to predict more than one token. During the inference process, the initial token is supplied by the target model. Subsequent tokens are selected through beam search using the draft head, which we will discuss next.\n' +
      '\n' +
      'Beam search.To generate candidate tokens for verification, we can resort to using beam search due to the dependencies in the recurrent drafter. This enables us to first filter out the low-quality candidate token sequences, yielding a substantially smaller set for target model verification. Denote \\(K\\) beams as \\(B=[b_{1},...,b_{K}]\\), and each candidate with length \\(L\\) as \\(b_{k}=[i_{k}^{1},...,i_{k}^{L}]\\). We would need to verify \\(LK\\) tokens linearly to both \\(K\\) and \\(L\\). Comparing with Medusa, the additional overhead of our approach stems from the beam search through recurrent connections, a sequential process, whereas Medusa can predict each head independently in parallel. Through our experiments, we discovered that the overhead is minimal due to the simple structure of the draft head, when compared to the more complex structure of the target model. Moreover, we introduce an algorithmic optimization to further reduce the computation as described below.\n' +
      '\n' +
      'Dynamic tree attention after beam search.We can enhance the computation efficiency based on the results of beam search, as beam search typically reveals a tree structure by sharing some common prefixes. We refer this mechanism as "dynamic tree attention," as depicted in Figure 2. In this example, for the first candidate: _1 watch the sunset_ and the third candidate: _1 watch as leaves_, they share prefix _1 watch_ and we do not need to repeat the computation of this shared prefix. The saving of computation is achieved by dedupplicating the shared token prefixes and placing attention masks at appropriate locations. The use of a tree structure to save computation resembles approaches seen in Cai et al. (2024); Miao et al. (2023); Spector & Re (2023), where similar methods were employed to avoid redundant calculations for shared prefixes.\n' +
      '\n' +
      'However, unlike the use of tree structures mentioned above, we must determine ours _dynamically_ as it relies on individual beam search results at runtime. A standard trie-based algorithm is slow as it is difficult to be parallelized. We notice that a unique property of our problem is that all candidate sequences have the same length. With this insight, we discovered that our dynamic tree attention can be efficiently constructed using standard tensor operators, a critical aspect for leveraging modern-day accelerators with little overhead.\n' +
      '\n' +
      'The key observation is that we can find all shared prefixes in the beam search result using standard tensor operations without building a prefix trie. We have developed the function called get_prefix_match (illustrated in Figure 2(a\\(\\rightarrow\\)c)). This function processes the initial tensor beams, generating the prefix_match tensor as its output. In this ten\n' +
      '\n' +
      'Figure 1: The recurrent drafter design, where the draft head is repeated 3 times with recurrent connections for predicting 3 candidate tokens during one forward pass.\n' +
      '\n' +
      'sor, prefix_match[i][j]=k indicates that the candidate at the smallest index \\(k\\) shares an identical prefix beams[i][:j+1]. For instance, prefix_match[2][1]=0 signifies a shared prefix _"I watch"_ between beams[0] and beams[2]. It is possible to condense tokens where prefix_match[i][j]<i. We demonstrate the pseudo-code of get_prefix_match in Listing 1. Subsequent operations can be similarly designed to use the prefixes found here to construct dynamic tree attention. This leads to further acceleration of LLM inference in speculative decoding. Finally, the use of dynamic tree attention is not limited to ReDrafter. It can also be used in the standard two-model speculative decoding approach while a separate draft model performs beam search and then apply dynamic tree attention.\n' +
      '\n' +
      'Speculative decoding with ReDrafter.Here we briefly describe the steps of using ReDrafter for speculative decoding. In each generation step during inference, we start with all previously generated tokens with the last hidden state \\(s_{0}\\). We employ beam search to generate a set of candidate sequences \\(B=[b_{1},...,b_{K}]\\). Subsequently, dynamic tree attention is applied to flatten and compress \\(B\\) into a sequence \\(B_{c}\\), while formulating an appropriate attention mask, \\(M_{c}\\). The base model then proceeds with a forward pass to compute the log probabilities for all proposed tokens. Then, we select the best candidate with the longest accepted prefix. The selection method can range from a greedy approach (aka. token matches), to more sophisticated techniques like rejection sampling or typical sampling. We append accepted tokens to the end of previously generated tokens and run the next iteration until the stopping criteria are met.\n' +
      '\n' +
      'Discussions of the tree attention in Medusa.Here we briefly describe the tree attention proposed in Medusa to highlight its differences with our approach. The authors have the challenge of managing an exponentially large set of candidate token sequences resulting from the independent predictions of draft heads. They address this issue by constructing a predetermined sparse tree attention structure, crucial for maintaining a reasonable small number of tokens for verification. Suppose there are totally \\(L\\) independent Medusa heads, and each head has top \\(K\\) predictions, denoted as \\((i^{\\ell}_{1},...,i^{\\ell}_{k},...,i^{\\ell}_{K})\\), where \\(\\ell=1,...L\\). Then a feasible candidate token sequence is constructed by selecting one token from each head as \\(p=[i^{1}_{k_{1}},...,i^{\\ell}_{k_{\\ell}},...,i^{L}_{k_{L}}]\\), where \\(k_{\\ell}\\) indicates the \\(k_{\\ell}\\)th element in the top \\(K\\) from the \\(\\ell\\)th head. Altogether, this results in \\(O(K^{L})\\) potential tokens to be verified compared to our \\(KL\\) after beam search. In Medusa, a greedy algorithm is designed to select those paths that would give overall high acceptance rates using a validation data set. In practice,\n' +
      '\n' +
      'Figure 2: An illustration of the different components for building the dynamic tree attention for batch size equals to 1. (Extending to batch size larger than 1 is straightforward.) In this example, the beam size is 3, with each candidate sequence containing 4 tokens. (a) The beam search result. (b) The total token count is reduced from \\(3\\times 4=12\\) to 9 by compressing shared prefixes in the beam search result. (c) A tensor-based, GPU-friendly algorithm is used to detect repeating prefixes within each beam (illustrated as gray boxes). (d) The tensor-based approach also extends to the encoding of child-parent relationships in the tree attention. The attention masks can be applied in a similar way.\n' +
      '\n' +
      'paths that are shorter than \\(L\\) are also considered in Medusa through a pruning approach. The result is a sparse tree structure that effectively restricts verification up to a certain number, which was fixed at 64 in Cai et al. (2024). When the tree is flattened as a tensor for verification, tokens not on the same path will be effectively masked out and this is named as "tree attention". While prioritizing overall accuracy using a validate dataset is a sensible approach, it may inadvertently impact individual data performance, as the best paths overall may not necessarily be optimal for individual data points. In addition, as it replies on a separate validation data, this may pose another issue of how to select the optimal validation set to use.\n' +
      '\n' +
      '```\n' +
      'defget_prefix_match(beams): """ Computsnecessaryinformationforefirmatching. Addanadditionaldimensionattheforefrontforbatchsizes>1. bw=beam_width,cl=candidate_seq_length Args: -beams:[bw,cl] Batchedinputcandidates. Returns: -prefix_match:[bw,cl] prefix_match[i][j]=kmeansbeamsiandksharethesame firstj+1tokens. Examples: beams=tensor([[91,92,93,95], [91,92,94,96], [91,92,93,97]]) prefix_match=tensor([[0,0,0,0], [0,0,1,1], [0,0,0,2]] """ cl=beams.shape[1]\n' +
      '#Indexeachcandidate. beam_index=torch.arange(1,cl+1) #Matchtokensacrossbeams. matches=beams[:,:,None]==beams[:,None,:]\n' +
      '#Matchallprefixes. seq_matches=(torch.cumsum(matches,dim=2) ==beam_index[None,None,:])\n' +
      '#Previouscandidatewithsmallestindexthatasharesthesame prefix. match=torch.argmax(seq_matches,dim=2) returnprefix_match\n' +
      '```\n' +
      '\n' +
      'Listing 1: An example implementation for get_prefix_match.\n' +
      '\n' +
      'Related work.Since speculative decoding (Chen et al., 2023; Leviathan et al., 2023) was introduced, various improvements and extensions have been proposed and studied. For example, draft models can be either through training (Cai et al., 2024; Liu et al., 2023; Chen et al., 2023; Li et al., 2024; Bhendawade et al., 2024) or training-free approaches (He et al., 2023; Yang et al., 2023; Fu et al., 2024). Spector & Re (2023) restructure speculative decoding candidate tokens as a tree. The idea has also been explored in Cai et al. (2024); Miao et al. (2023); Li et al. (2024) in different contexts, and the tree structures are typically predetermined before inference to allow efficient token drafting.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      'In our experiments, we focus on the scenario where the target model is frozen and only the draft head is learned. Although as shown in Cai et al. (2024); Bhendawade et al. (2024),jointly training can improve speculative decoding performance, keeping the parameters of the target model frozen can be more beneficial in practical deployment. With a fixed target model, the preservation of its output distribution when using speculative decoding allows practitioners to freely choose draft models from a variety of options to suit their specific requirements. This eliminates concerns about potential changes to the target model that could require additional evaluations. We leave the exploration of joint training of target model and the draft head in our method as future work. Finally, our experimental setup consists of a single host equipped with 96 CPUs and 8 Nvidia H100 80GB HBM3 GPUs. We use 8 GPUs for training and use a single GPU for inference.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      'We used pre-existing Medusa checkpoints from the authors\' repository, which use Vicuna 7B base model (Touvron et al., 2023; Chiang et al., 2023), to assess training efficacy. For our method, ReDrafter, we adopted an identical training scheme, using a two-epoch training of the drafting head based on ShareGPT (2023). We subsequently performed evaluations using Alpaca Eval (Dubois et al., 2023).\n' +
      '\n' +
      'While Medusa used two ResNet blocks totaling 0.74B parameters, we evaluated two options for ReDrafter: a leaner setup with two ResNet blocks comprising 0.33B parameters, and a heavier setup with four ResNet blocks encompassing 0.56B parameters, aiming to enhance prediction performance. Both ReDrafter variants maintain a lower memory footprint than Medusa. Results are shown in Figure 3. We found that even ReDrafter\'s less complex version outperforms Medusa due to its design advantages: a) ReDrafter leverages prior token data alongside the hidden state for more informed predictions, and b) ReDrafter\'s unified head promotes precise learning at each position through shared information. ReDrafter is particularly good at longer range predictions.\n' +
      '\n' +
      'Figure 4: Analysis of 30 drafters where half used simplified settings without RNN (\\(U=I,W=I,b=0,f(x)=x\\)), and the other half incorporated RNN. The comparison reveals a trade-off between model simplicity and complexity, illustrated by the gap between blue (non-RNN) and yellow (RNN) dots. Despite the added complexity and inference speed impact, RNN models achieve better accuracy, demonstrating the benefits of incorporating the recurrent structure.\n' +
      '\n' +
      'Figure 3: Comparisons of the training accuracy of Medusa and recurrent drafters, both based on Vicuna 7B, with a focus on predicting the next five tokens. Medusa uses five heads to forecast tokens from \\(t+1\\) to \\(t+5\\), whereas the recurrent drafter relies on a single head, benefiting from reduced memory usage. Despite this, the recurrent drafter achieves higher predictive accuracy on both the training and validation sets.\n' +
      '\n' +
      'We analyzed the relationship between training accuracy and inference speed across 30 trafters, divided into two categories: drafters without recurrent neural networks (RNN) and with RNN integration. For each category, we adjust the number of ResNet layers and the learning rate, yielding 15 distinct configurations per category. For the non-RNN drafters, we adopted a simplified model setting where \\(U=I\\), \\(W=I\\), \\(b=0\\), and applying an identity function \\(f(x)=x\\). During inference, we use greedy decoding and set beam width equals to 15 for a fair comparison. Results are shown in Figure 4. For the drafters without RNN, we observed a high Pearson correlation coefficient 0.907, between validation accuracy at \\(t+5\\) and inference speed within this group. Drafters equipped with RNN showed a Pearson correlation coefficient of 0.798.\n' +
      '\n' +
      'The difference between the two groups is visually represented by the gap between blue and yellow dots in our scatter plots. This gap underscores the trade-off associated with employing a more complex modeling approach (with RNN). Although the use of RNN introduces additional computational complexity, it also enables the models to potentially achieve a higher ceiling in terms of both accuracy and efficiency during inference.\n' +
      '\n' +
      '### Inference\n' +
      '\n' +
      'In our study, we evaluated auto-regressive (AR) generation, Medusa, and recurrent drafters (ReDrafter) using the MT-bench (Zheng et al., 2024) on Vicuna models of both 7B and 13B sizes. To optimize memory usage, we incorporated the KV Cache strategy as used in the Medusa approach. For Medusa, we use author-provided checkpoints. For ReDrafter, we select the highest accuracy checkpoint on validation set through a grid search over number of ResNet blocks and keep its size smaller than Medusa\'s drafter. In order to have a fair comparison, we set batch size to be one and show experimental results in Table 1. ReDrafter demonstrated superior speed, outpacing AR by 2.67x in greedy decoding, compared to Medusa\'s 2.28x on Vicuna 7B. Incorporating the typical acceptance algorithm (Hewitt et al., 2022; Cai et al., 2024) requires tuning temperature and hyperparameters \\((\\epsilon,\\delta)\\). We employed the quality score as a benchmarking standard and chose the configuration with the highest speed-up, provided its quality score was within the standard deviation comparing with AR\'s quality score. With typical acceptance, ReDrafter achieved even greater efficiencies, up to 3.28x. This trend extends to Vicuna 13B models, where ReDrafter\'s advanced predictive ability yielded a higher acceptance rate per forward pass and leveraged GPU optimizations to minimize computational load, particularly at larger beam sizes.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c} \\hline \\hline  & Base & Drafter Size & Tokens/Sec. & Tokens/Step & Speed-up & Quality \\\\ \\hline AR Greedy & Vicuna 7B & 0 & 35.63 & 1.00 & 1.00\\(\\times\\) & 5.84 \\\\ Medusa Greedy & Vicuna 7B & 0.74B & 81.32 & 2.52 & 2.28\\(\\times\\) & 5.88 \\\\ Medusa Typical & Vicuna 7B & 0.74B & 90.12 & 2.77 & 2.53\\(\\times\\) & 5.82 \\\\ ReDrafter Greedy & Vicuna 7B & 0.56B & **95.16** & **3.41** & **2.67\\(\\times\\)** & 5.92 \\\\ ReDrafter Typical & Vicuna 7B & 0.56B & **117.12** & **4.39** & **3.28\\(\\times\\)** & 5.87 \\\\ \\hline AR Greedy & Vicuna 13B & 0 & 28.41 & 1.00 & 1.00\\(\\times\\) & 6.37 \\\\ Medusa Greedy & Vicuna 13B & 0.95B & 67.45 & 2.61 & 2.37\\(\\times\\) & 6.36 \\\\ Medusa Typical & Vicuna 13B & 0.95B & 73.92 & 2.83 & 2.60\\(\\times\\) & 6.34 \\\\ ReDrafter Greedy & Vicuna 13B & 0.90B & **83.08** & **3.71** & **2.92\\(\\times\\)** & 6.35 \\\\ ReDrafter Typical & Vicuna 13B & 0.90B & **92.22** & **4.33** & **3.24\\(\\times\\)** & 6.35 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Evaluations of AR, Medusa, and our ReDrafter approach on MT-bench. Our findings replicated Medusa’s acceleration benefits over AR. Notably, ReDrafters, with their smaller sizes, achieved a greater speed increase (2.67x for Vicuna 7B and 2.92x for 13B) compared to Medusa (2.28x for Vicuna 7B and 2.37x for 13B) during greedy decoding. Using typical acceptance, ReDrafter also demonstrated the potential for even greater speed improvements (3.28x for Vicuna 7B and 3.24x for 13B) without compromising quality, largely due to their ability to predict more extended sequences, with more tokens accepted per step.\n' +
      '\n' +
      '### Analysis\n' +
      '\n' +
      'Number of accepted tokens per step.In Figure 5(a), the impact of varying beam sizes and generated output lengths is examined. As the beam size increases, the chance of finding a candidate with a greater number of accepted tokens also rises, leading to an increase in tokens generated per forward step. And the number of generated tokens per-step is pretty stable for various output lengths. This indicates a consistent generation of tokens per forward step across different output lengths for each beam size, showcasing ReDrafter\'s robust performance in handling long generations.\n' +
      '\n' +
      'Dynamic tree attention.The implementation of dynamic tree attention becomes crucial for larger beam sizes, enabling significant reductions in computational and memory demands by efficiently compressing candidates. Figure 5(b) highlights a consistent compression ratio despite variations in beam size, with the compression ratio remaining predictable even in outlier scenarios (99th and 1st percentiles).\n' +
      '\n' +
      'Speed and throughput.We conduct an analysis of our ReDrafter approach, across various beam sizes, optimizing the batch size to maximize GPU utilization. The ReDrafter is equipped with 4 ResNet layers, using Vicuna 7B as the base model. The analysis was done over 128 random chosen prompts. Results are demonstrated in Figure 6(a). In terms of tokens/second (depicted by the blue line and the higher the better), ReDrafter outperforms AR significantly. As the beam size expands, ReDrafter has the potential to achieve even greater tokens/second, given sufficient computational resources. However, when the batch size exceeds 4, we encounter a computational bottleneck, leading to a decline in tokens/second. Despite this, overall throughput (illustrated by the red lines, calculated as batch size multiplied by tokens/second) continues to rise. Beyond a batch size of 16, computational constraints become the limiting factor, and further increases in batch size no longer enhance overall throughput. Pushing the batch size beyond this point risks making overall inference slower. In Figure 6(b), we show that implementing tree attention can mitigate both computational and memory bottlenecks for ReDrafter, thereby enabling higher throughput and higher tokens/second. In practical deployment, one might consider both speed and throughput to make balanced decisions.\n' +
      '\n' +
      'Figure 5: (a) The count of accepted tokens per forward step is stable when the generated output length changes. Note that we train the drafter to predict next five tokens. (b) Dynamic tree attention facilitates sequence compression during the verification process, where x- and y-axes indicates the number of tokens to be verified by the target model.\n' +
      '\n' +
      'Rejection sampling vs. typical acceptance.We conduct an analysis of rejection sampling versus typical acceptance on their speed and quality at different sampling temperatures, shown in Table 2. Unlike rejection sampling (Leviathan et al., 2023; Chen et al., 2023a), typical acceptance (Cai et al., 2024) does not guarantee a distribution match with the target model. Instead, it approves a proposed token \\(\\hat{x}_{t+1}\\) if its conditional probability \\(p(\\hat{x}_{t+1}|x_{\\leq t})\\) exceeds a certain threshold, defined as \\(\\min(\\epsilon,\\delta\\exp(-H(p(\\cdot|x_{\\leq t}))))\\), with \\(\\epsilon,\\delta\\) being adjustable hyper-parameters and \\(H(\\cdot)\\) being the entropy function. Adjustments to these hyperparameters can significantly influence the sampling distribution, and, as a result, we observe notable impacts on the quality of generation. Following the recommendations in Cai et al. (2024), we set \\(\\epsilon=0.3\\) and \\(\\delta=0.09\\), aligning these settings with rejection sampling at equivalent temperatures. Our empirical investigation reveals comparable performance between the two sampling approaches for our method when the temperature is lower than 0.7. At higher temperature settings, such as 0.9, typical sampling may underperform, resulting in a significant drop in quality scores relative to rejection sampling. This discrepancy is likely attributed to the less than ideal choice of hyperparameters.\n' +
      '\n' +
      '## 4 Conclusions\n' +
      '\n' +
      'In this paper, we have introduced a novel approach to improve the generation efficiency of large language models. Our method, recurrent drafter, uses a single draft head with\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline  & Temp. & Speed-up & Quality \\\\ \\hline Greedy Decode & 0.0 & 1.00\\(\\times\\) & 1.00\\(\\times\\) \\\\ \\hline Rejection Sampling & 0.1 & 2.58 \\(\\pm\\) 0.06\\(\\times\\) & 1.00 \\(\\pm\\) 0.02\\(\\times\\) \\\\  & 0.3 & 2.93 \\(\\pm\\) 0.06\\(\\times\\) & 1.01 \\(\\pm\\) 0.02\\(\\times\\) \\\\  & 0.5 & 3.02 \\(\\pm\\) 0.05\\(\\times\\) & 0.99 \\(\\pm\\) 0.02\\(\\times\\) \\\\  & 0.7 & 3.13 \\(\\pm\\) 0.06\\(\\times\\) & 0.97 \\(\\pm\\) 0.03\\(\\times\\) \\\\  & 0.9 & 3.43 \\(\\pm\\) 0.05\\(\\times\\) & 0.93 \\(\\pm\\) 0.03\\(\\times\\) \\\\ \\hline Typical Acceptance & 0.1 & 2.62 \\(\\pm\\) 0.05\\(\\times\\) & 1.00 \\(\\pm\\) 0.02\\(\\times\\) \\\\  & 0.3 & 2.88 \\(\\pm\\) 0.06\\(\\times\\) & 0.99 \\(\\pm\\) 0.03\\(\\times\\) \\\\  & 0.5 & 3.02 \\(\\pm\\) 0.04\\(\\times\\) & 0.98 \\(\\pm\\) 0.01\\(\\times\\) \\\\  & 0.7 & 3.17 \\(\\pm\\) 0.04\\(\\times\\) & 0.97 \\(\\pm\\) 0.02\\(\\times\\) \\\\  & 0.9 & 3.49 \\(\\pm\\) 0.05\\(\\times\\) & 0.86 \\(\\pm\\) 0.03\\(\\times\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Speed comparisons, tokens/second, and quality of rejection sampling and typical acceptance at different temperatures. We use speed and quality score of greedy decode of auto-regressive generation as the baseline.\n' +
      '\n' +
      'Figure 6: (a) Speed (tokens/second) and throughput (batch size * tokens/second) for AR and ReDrafter with various beams. (b) Speed and throughput for ReDrafter w, w/o tree attentaion. RD stands for ReDrafter.\n' +
      '\n' +
      'a recurrent dependency design. This simplifies the inference process while maintaining effectiveness, as demonstrated empirically on several popular language models. Our method offers a practical and efficient solution for serving large language models under the speculative decoding framework.\n' +
      '\n' +
      'Acknowledgments.We would like to thank Barry Theobald, Frank Chu, Liangliang Cao, Ruoming Pang, Sam Wiseman, Xiujun Li and Zhiyun Lu for their comments and suggestions on the manuscript.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* Bhendawade et al. (2024) Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, and Mahyar Najibi. Speculative streaming: Fast llm inference without auxiliary models. _arXiv preprint arXiv:2402.11131_, 2024.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* Cai et al. (2024) Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. _arXiv preprint arXiv:2401.10774_, 2024.\n' +
      '* Chen et al. (2023a) Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. _arXiv preprint arXiv:2302.01318_, 2023a.\n' +
      '* Chen et al. (2023b) Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, and Kevin Chen-Chuan Chang. Cascade speculative drafting for even faster llm inference. _arXiv preprint arXiv:2312.11462_, 2023b.\n' +
      '* Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).\n' +
      '* Dubois et al. (2023) Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacaafarm: A simulation framework for methods that learn from human feedback. _Advances in Neural Information Processing Systems_, 36, 2023.\n' +
      '* Fu et al. (2024) Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm inference using lookahead decoding. _arXiv preprint arXiv:2402.02057_, 2024.\n' +
      '* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 770-778, 2016.\n' +
      '* He et al. (2023) Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. Rest: Retrieval-based speculative decoding. _arXiv preprint arXiv:2311.08252_, 2023.\n' +
      '* Hewitt et al. (2022) John Hewitt, Christopher D Manning, and Percy Liang. Truncation sampling as language model desmoothing. _arXiv preprint arXiv:2210.15191_, 2022.\n' +
      '* Leviathan et al. (2023) Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In _International Conference on Machine Learning_, pp. 19274-19286. PMLR, 2023.\n' +
      '* Li et al. (2020)* Li et al. (2024) Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. _arXiv preprint arXiv:2401.15077_, 2024.\n' +
      '* Liu et al. (2023) Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. Online speculative decoding. _arXiv preprint arXiv:2310.07177_, 2023.\n' +
      '* Miao et al. (2023) Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyau Anfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. _arXiv preprint arXiv:2305.09781_, 2023.\n' +
      '* Mikolov and Zweig (2012) Tomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model. In _2012 IEEE Spoken Language Technology Workshop (SLT)_, pp. 234-239, 2012. doi: 10.1109/SLT.2012.6424228.\n' +
      '* Sharegpt (2023) Sharegpt. Sharegpt, 2023. URL [https://huggingface.co/datasets/anon8231489123/Sharegpt_Vicuna_unfiltered](https://huggingface.co/datasets/anon8231489123/Sharegpt_Vicuna_unfiltered).\n' +
      '* Spector and Re (2023) Benjamin Spector and Chris Re. Accelerating llm inference with staged speculative decoding. _arXiv preprint arXiv:2308.04623_, 2023.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* Yang et al. (2023) Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei. Inference with reference: Lossless acceleration of large language models. _arXiv preprint arXiv:2304.04487_, 2023.\n' +
      '* Zheng et al. (2024) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _Advances in Neural Information Processing Systems_, 36, 2024.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>