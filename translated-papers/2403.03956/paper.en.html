<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Backtracing: Retrieving the Cause of the Query\n' +
      '\n' +
      'Rose E. Wang Pawan Wirawarn Omar Khattab\n' +
      '\n' +
      '**Noah Goodman Dorottya Demszky**\n' +
      '\n' +
      'Stanford University\n' +
      '\n' +
      'rewang@cs.stanford.edu, ddemszky@stanford.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Many online content portals allow users to ask questions to supplement their understanding (e.g., of lectures). While information retrieval (IR) systems may provide answers for such user queries, they do not directly assist content creators--such as lecturers who want to improve their content--identify segments that _caused_ a user to ask those questions. We introduce the task of _backtracing_, in which systems retrieve the text segment that most likely caused a user query. We formalize three real-world domains for which backtracing is important in improving content delivery and communication: understanding the cause of (a) student confusion in the Lecture domain, (b) reader curiosity in the News Article domain, and (c) user emotion in the Conversation domain. We evaluate the zero-shot performance of popular information retrieval methods and language modeling methods, including bi-encoder, re-ranking and likelihood-based methods and ChatGPT. While traditional IR systems retrieve semantically relevant information (e.g., details on "projection matrices" for a query "does projecting multiple times still lead to the same point?"), they often miss the causally relevant context (e.g., the lecturer states "projecting twice gets me the same answer as one projection"). Our results show that there is room for improvement on backtracing and it requires new retrieval approaches. We hope our benchmark serves to improve future retrieval systems for backtracing, spawning systems that refine content generation and identify linguistic triggers influencing user queries.1\n' +
      '\n' +
      'Footnote 1: Our code and data are opensourced: [https://github.com/rosewang208/backtracing](https://github.com/rosewang208/backtracing).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Content creators and communicators, such as lecturers, greatly value feedback on their content to address confusion and enhance its quality (Evans and Guymon, 1978; Hativa, 1998). For example, when a student is confused by a lecture content, they post questions on the course forum seeking clarification. Lecturers want to determine _where_ in the lecture the misunderstanding stems from in order to improve their teaching materials (McKone, 1999; Harvey, 2003; Gormally et al., 2014). The needs of these _content creators_ are different than the needs of _information seekers_ like students, who may directly rely on information retrieval (IR) systems such as Q&A methods to satisfy their information needs (Schutze et al., 2008; Yang et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018).\n' +
      '\n' +
      'Identifying the cause of a query can be challenging because of the lack of explicit labeling, implicit nature of additional information need, large size of corpus, and required domain expertise to understand both the query and corpus. Consider the example shown in Figure 1. First, the student does not explicitly flag what part of the lecture causes\n' +
      '\n' +
      'Figure 1: The task of backtracing takes a query and identifies the context that triggers this query. Identifying the cause of a query can be challenging because of the lack of explicit labeling, large corpus size, and domain expertise to understand both the query and corpus.\n' +
      '\n' +
      'their question, yet they express a latent need for additional information outside of the lecture content. Second, texts like lecture transcripts are long documents; a lecturer would have a difficult time pinpointing the precise source of confusion for every student question they receive. Finally, some queries require domain expertise for understanding the topic and reason behind the student\'s confusion; not every student question reflects the lecture content verbatim, which is what makes backtracing interesting and challenging.\n' +
      '\n' +
      'To formalize this task, we introduce a novel retrieval task called _backtracing_. Given a query (e.g., a student question) and a corpus (e.g., a lecture transcript), the system must identify the sentence that most likely provoked the query. We formalize three real-world domains for which backtracing is important for improving content delivery and communication. First is the Lecture domain where the goal is to retrieve the cause of student confusion; the query is a student\'s question and the corpus is the lecturer\'s transcript. Second is the News Article domain where the goal is to retrieve the cause of a user\'s curiosity in the news article domain; the query is a user\'s question and the corpus is the news article. Third is the Conversation domain where the goal is to retrieve the cause of a user\'s emotion (e.g., anger); the query is the user\'s conversation turn expressing that emotion and the corpus is the complete conversation. Figure 2 illustrates an example for each of these domains. These diverse domains showcase the applicability and common challenges of backtracing for improving content generation, similar to heterogeneous IR datasets like BEIR Thakur et al. (2021).\n' +
      '\n' +
      'We evaluate a suite of popular retrieval systems, like dense retriever-based Reimers and Gurevych (2019); Guo et al. (2020); Karpukhin et al. (2020) or re-ranker-based systems Nogueira and Cho (2019); Craswell et al. (2020); Ren et al. (2021). Additionally, we evaluate likelihood-based retrieval methods which use pre-trained language models (PLMs) to estimate the probability of the query conditioned on variations of the corpus Sachan et al. (2022), such as measuring the query likelihood conditioned on the corpus with and without the candidate segment. Finally, we also evaluate the long context window gpt-3.5-turbo-16k ChatGPT model because of its ability to process long texts and perform instruction following. We find that there is room\n' +
      '\n' +
      'Figure 2: Retrieving the correct triggering context can provide insight into how to better satisfy the user’s needs and improve content delivery. We formalize three real-world domains for which backtracing is important in providing context on a user’s query: (a) The Lecture domain where the objective is to retrieve the cause of student confusion; (b) The News Article domain where the objective is to retrieve the cause of reader curiosity; (c) The Conversation domain where the objective is to retrieve the cause of user emotion (e.g., anger). The user’s query is shown in the gray box and the triggering context is the green -highlighted sentence. Popular retrieval systems such as dense retriever-based and re-ranker based systems retrieve incorrect contexts shown in red.\n' +
      '\n' +
      'for improvement on backtracing across all methods. For example, the bi-encoder systems Reimers and Gurevych (2019) struggle when the query is not semantically similar to the text segment that causes it; this often happens in the Conversation and Lecture domain, where the query may be phrased differently than the original content. Overall, our results indicate that backtracing is a challenging task which requires new retrieval approaches to take in _causal_ relevance into account; for instance, the top-3 accuracy of the best model is only \\(44\\%\\) on the Lecture domain.\n' +
      '\n' +
      'In summary, we make the following contributions in this paper:\n' +
      '\n' +
      '* We propose a new task called backtracing where the goal is to retrieve the cause of the query from a corpus. This task targets the information need of _content creators_ who wish to improve their content in light of questions from _information seekers_.\n' +
      '* We formalize a benchmark consisting of three domains for which backtracing plays an important role in identifying the context triggering a user\'s query: retrieving the cause of student confusion in the Lecture setting, reader curiosity in the News Article setting, and user emotion in the Conversation setting.\n' +
      '* We evaluate a suite of popular retrieval systems, including bi-encoder and re-ranking architectures, as well as likelihood-based methods that use pretrained language models to estimate the probability of the query conditioned on variations of the corpus.\n' +
      '* We show that there is room for improvement and limitations in current retrieval methods for performing backtracing, suggesting that the task is not only challenging but also requires new retrieval approaches.\n' +
      '\n' +
      '## 2 Related works\n' +
      '\n' +
      'The task of information retrieval (IR) aims to retrieve relevant documents or passages that satisfy the information need of a user Schutze et al. (2008); Thakur et al. (2021). Prior IR techniques involve neural retrieval methods like ranking models Guo et al. (2016); Xiong et al. (2017); Khattab and Zaharia (2020) and representation-focused language models Peters et al. (2018); Devlin et al. (2018); Reimers and Gurevych (2019). Recent works also use PLMs for ranking texts in performing retrieval Zhuang and Zuccon (2021); Zhuang et al. (2021); Sachan et al. (2022); an advantage of using PLMs is not requiring any domain- or task-specific training, which is useful for settings where there is not enough data for training new models. These approaches have made significant advancements in assisting _information seekers_ in accessing information on a range of tasks. Examples of these tasks include recommending news articles to read for a user in the context of the current article they\'re reading Voorhees (2005); Soboroff et al. (2018), retrieving relevant bio-medical articles to satisfy health-related concerns Tsatsaronis et al. (2015); Boteva et al. (2016); Roberts et al. (2021); Soboroff (2021), finding relevant academic articles to accelerate a researcher\'s literature search Voorhees et al. (2021), or extracting answers from texts to address questions Yang et al. (2015); Rajpurkar et al. (2016); Joshi et al. (2017); Yang et al. (2018).\n' +
      '\n' +
      'However, the converse needs of _content creators_ have received less exploration. For instance, understanding what aspects of a lecture cause students to be confused remains under-explored and marks areas for improvement for content creators. Backtracing is related to work on predicting search intents from previous user browsing behavior for understanding why users issue queries in the first place and what trigger their information needs Cheng et al. (2010); Kong et al. (2015); Koskela et al. (2018). The key difference between our approach and prior works is the nature of the input data and prediction task. While previous methods rely on observable user browsing patterns (e.g., visited URLs and click behaviors) for ranking future search results, our backtracing framework leverages the language in the content itself as the context for the user query and the output space for prediction. This shift in perspective allows content creators to get granular insights into specific contextual, linguistic triggers that influence user queries, as opposed to behavioral patterns.\n' +
      '\n' +
      'Another related task is question generation, which also has applications to education Heilman and Smith (2010); Duan et al. (2017); Pan et al. (2019). While question generation settings assume the answer can be identified in the source document, backtracing is interested in the triggers for the questions rather than the answers themselves. In many cases, including our domains, the answer to the question may exist outside of the provided \n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      'for students to ask questions to express confusion about novel concepts. Lecturers can benefit from knowing what parts of their lecture cause confusion. We adapt the paired comment-lecture dataset from SightWang et al. (2023), which contains lecture transcripts from MIT OpenCourseWare math videos and real user comments from YouTube expressing confusion. While these comments naturally act as queries in the backtracing framework, the comments do not have ground-truth target annotations on what _caused_ the comment in the first place. Our work contributes these annotations. Two annotators (co-authors of this paper) familiar with the task of backtracing and fluent in the math topics at a university-level annotate the queries2. They select up to 5 sentences and are allowed to use the corresponding video to perform the task. \\(20\\) queries are annotated by both annotators and these annotations share high agreement: the annotators identified the same target sentences for \\(70\\%\\) of the queries, and picked target sentences close to each other. _These annotation results indicate that performing backtracing with consensus is possible._ Appendix B includes more detail on the annotation interface and agreement. The final dataset contains 210 annotated examples, comparable to other IR datasets Craswell et al. (2020, 2021); Sobroff (2021).3 In the case where a query has more than one target sentence, the accuracy criterion is whether there\'s overlap between the target sentences and predicted sentence (see task definition in Section 3).\n' +
      '\n' +
      'Footnote 2: The annotators must be fluent in the math topics to understand both the lecture and query, and backtrace accordingly.\n' +
      '\n' +
      'News ArticleWe use real-world news articles and questions written by crowdworkers as they read through the articles to construct the News Article domain. News articles are a natural setting for readers to ask curiosity questions, expressing a need for more information. We adapt the dataset from Ko et al. (2020) which contains news articles and questions indexed by the article sentences that provoked curiosity in the reader. We modify the dataset by filtering out articles that cannot fit within the smallest context window of models used in the likelihood-based retrieval methods (i.e., \\(1024\\) tokens). This adapted dataset allows us to assess the ability of methods to incorporate more contextual information and handling more distractor sentences, while maintaining a manageable length of text. The final dataset contains 1382 examples.\n' +
      '\n' +
      'ConversationWe use two-person conversations which have been annotated with emotions, such as _anger_ and _fear_, and cause of emotion on the level of conversation turns. Conversations are natural settings for human interaction where a speaker may accidentally say something that evokes strong emotions like anger. These emotions may arise from cumulative or non-adjacent interactions, such as the example in Figure 2. While identifying content that evokes the emotion expressed via a query differs from content that causes confusion, the ability to handle both is key to general and effective backtracing systems that retrieve information based on causal relevance. Identifying utterances that elicit certain emotions can pave the way for better emotional intelligence in systems and refined conflict resolution tools. We adapt the conversation dataset from Poria et al. (2021) which contain turn-level annotations for the emotion and its cause, and is designed for recognizing the cause of emotions. The query is one of the speaker\'s conversation turn annotated with an emotion and the corpus is all of the conversation turns. To ensure there are enough distractor sentences, we use conversations with at least 5 sentences and use the last annotated utterance in the conversation. The final dataset contains 671 examples.\n' +
      '\n' +
      '### Domain Analysis\n' +
      '\n' +
      'To contextualize the experimental findings in Section 6, we first analyze the structural attributes of our datasets in relation to backtracing.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r|c|c|c} \\hline \\hline  & & **Lec** & **News** & **Conv** \\\\ \\hline Query & Total & \\(210\\) & \\(1382\\) & \\(671\\) \\\\  & Avg. words & \\(30.9\\) & \\(7.1\\) & \\(11.6\\) \\\\  & Max words & \\(233\\) & \\(27\\) & \\(62\\) \\\\  & Min words & \\(4\\) & \\(1\\) & \\(1\\) \\\\ \\hline Corpus & Total & \\(11042\\) & \\(2125\\) & \\(8263\\) \\\\  & Avg. size & \\(525.8\\) & \\(19.0\\) & \\(12.3\\) \\\\  & Max size & \\(948\\) & \\(45\\) & \\(6110\\) \\\\  & Min size & \\(273\\) & \\(7\\) & \\(6\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Dataset statistics on the query and corpus sizes for backtracing. Lec is the Lecture domain, News is the News Article domain, and Conv is the Conversation domain. The corpus size is measured on the level of sentences for Lecture and News Article, and of conversation turns for Conversation.\n' +
      '\n' +
      'How similar is the query to the cause?To answer this question, we plot the semantic similarity of the query to the ground-truth cause sentence (GT) in Figure 4. We additionally plot the maximal similarity of the query to any corpus sentence (Max) and the difference between the ground-truth and maximal similarity (Diff). This compares the distractor sentences to the ground-truth sentences; the larger the difference is, the less likely semantic relevance can be used as a proxy for _causal_ relevance needed to perform backtracing. This would also indicate that poor performance of similarity-based methods because the distractor sentences exhibit higher similarity. We use the all-MiniLM-L12-v2 S-BERT model to measure semantic similarity Reimers and Gurevych (2019).\n' +
      '\n' +
      'Notably, the queries and their ground-truth cause sentences exhibit low semantic similarity across domains, indicated by the low blue bars. Additionally, indicated by the green bars, Conversation and Lecture have the largest differences between the ground-truth and maximal similarity sentences, whereas News Article has the smallest. This suggests that there may be multiple passages in a given document that share a surface-level resemblance with the query, but a majority do not cause the query in the Conversation and Lecture domains. In the News Article domain, the query and cause sentence exhibit higher semantic similarity because the queries are typically short and mention the event or noun of interest. Altogether, this analysis brings forth a key insight: Semantic relevance doesn\'t always equate causal relevance.\n' +
      '\n' +
      'Where are the causes located in the corpus?Understanding the location of the cause provides insight into how much context is needed in identifying the cause to the query. Figure 5 visualizes the distribution of cause sentence locations within the corpus documents. These plots show that while some domains have causes concentrated in specific sections, others exhibit a more spread-out pattern. For the News Article domain, there is a noticeable peak at the beginning of the documents which suggests little context is needed to identify the cause. This aligns with the typical structure of news articles where crucial information is introduced early to capture the reader\'s interest. As a result, readers may have immediate questions from the onset. Conversely, in the Conversation domain, the distribution peaks at the end, suggesting that more context from the conversation is needed to identify the cause. Finally, in the Lecture domain, the distribution is relatively uniform which suggests a broader contextual dependence. The causes of confusion arise from any section, emphasizing the importance of consistent clarity throughout an educational delivery.\n' +
      '\n' +
      'An interesting qualitative observation is that there are shared cause locations for different queries. An example from the Lecture domain is shown in Figure 6 where different student questions are mapped to the same cause sentence. This shows the potential for models to effectively perform backtracing and automatically identify common locations of confusion for lecturers to revise\n' +
      '\n' +
      'Figure 4: Each dataset plot shows the query similarity to the ground truth cause sentence (GT), to the corpus sentence with maximal similarity (Max), and the difference between the maximal and ground-truth similarity sentences (Diff).\n' +
      '\n' +
      'Figure 5: Each row plot is a per-domain histogram of where the ground-truth cause sentence lies in the corpus document. The x-axis reports the location of the cause sentence; 0 means the cause sentence is the first sentence and 1 the last sentence. The y-axis reports the count of cause sentences at that location.\n' +
      '\n' +
      '## 5 Methods\n' +
      '\n' +
      'We evaluate a suite of existing, state-of-the-art retrieval methods and report their top-1 and top-3 accuracies: Do the top 1 and 3 candidate sentences include the ground-truth sentences? Reporting top-k accuracy is a standard metric in the retrieval setting. We also report their minimum distance within the top-1 and top-3 candidates: What is the minimum distance between the method\'s candidates and the ground-truth sentences? The methods can be broadly categorized into similarity-based (i.e., using sentence similarity) and likelihood-based retrieval methods. Similar to Sachan et al. (2022), the likelihood-based retrieval methods use PLMs to measure the probability of the query conditioned on variations of the corpus and can be more expressive than the similarity-based retrieval methods; we describe these variations in detail below. We use GPT-2 Radford et al. (2019), GPT-J Wang and Komatsuzaki (2021), and OPT-6.7B Zhang et al. (2022) as the PLMs. We additionally evaluate with gpt-3.5-turbo-16k, a new model that has a long context window ideal for long text settings like Sight. However, because this model does not output probability scores, we cast only report its top-1 results.\n' +
      '\n' +
      'Random.This method randomly retrieves a sentence from the corpus.\n' +
      '\n' +
      'Edit distance.This method retrieves the sentence with the smallest edit distance from the query.\n' +
      '\n' +
      'Bi-encoders.This method retrieves the sentence with the highest semantic similarity using the best performing S-BERT models Reimers and Gurevych (2019). We use multi-qa-MiniLM-L6-cos-v1 trained on a large set of question-answer pairs and all-MiniLM-L12-v2 trained on a diversity of text pairs from sentence-transformers as the encoders.\n' +
      '\n' +
      'Cross-encoder.This method picks the sentence with the highest predicted similarity score by the cross-encoder. We use ms-marco-MiniLM-L-6-v2 Thakur et al. (2021).\n' +
      '\n' +
      'Re-ranker.This method uses a bi-encoder to retrieve the top \\(k\\) candidate sentences from the corpus, then uses a cross-encoder to re-rank the \\(k\\) sentences. We use all-MiniLM-L12-v2 as the bi-encoder and ms-marco-MiniLM-L-6-v2 as the cross-encoder. Since the smallest dataset--Daily Dialog--has a minimum of 5 sentences, we use \\(k=5\\) for all datasets.\n' +
      '\n' +
      'gpt-3.5-turbo-16k.This method is provided a line-numbered corpus and the query, and generates the line number that most likely caused the query. The prompt used for gpt-3.5-turbo-16k is in Appendix C.\n' +
      '\n' +
      'Single-sentence likelihood-based retrieval \\(p(q|x_{t})\\).This method retrieves the sentence \\(x_{t}\\in X\\) that maximizes \\(p(q|x_{t})\\). To contextualize the corpus and query, we add domain-specific prefixes to the corpus and query. For example, in Sight, we prepend "Teacher says: " to the corpus sentence and "Student asks: " to the query. Due to space constraints, Appendix C contains all the prefixes used.\n' +
      '\n' +
      'Auto-regressive likelihood-based retrieval \\(p(q|x_{\\leq t})\\).This method retrieves the sentence \\(x_{t}\\) which maximizes \\(p(q|x_{\\leq t})\\). This method evaluates the importance of preceding context in performing backtracing. Lecture is the only domain where the entire corpus cannot fit into the context window. This means that we cannot always evaluate \\(p(q|x_{\\leq t})\\) for \\(x_{t}\\) when \\(|x_{\\leq t}|\\) is longer than the context window limit. For this reason, we split the corpus \\(X\\) into chunks of \\(k\\) sentences, (i.e., \\(X_{0:k-1},X_{k:2k-1},\\dots\\)) and evaluate each \\(x_{t}\\) within their respective chunk. For example, if \\(x_{t}\\in X_{k:2k-1}\\), the auto-regressive likelihood score for \\(x_{t}\\) is \\(p(q|X_{k:t})\\). We evaluate with \\(k=20\\) because it is the maximum number of sentences (in addition to the query) that can fit in the smallest model context window.\n' +
      '\n' +
      'Average Treatment Effect (ATE) likelihood-based retrieval \\(p(q|X)-p(q|X\\setminus x_{t})\\).This method takes inspiration from treatment effects in causal inference Holland (1986). We describe how ATE can be used as a retrieval criterion. In our setting, the treatment is whether the sentence\n' +
      '\n' +
      'Figure 6: An example of a common confusion point where several students posed questions concerning a particular part of the lecture.\n' +
      '\n' +
      '\\(x_{t}\\) is included in the corpus. We\'re interested in the effect the treatment has on the query likelihood:\n' +
      '\n' +
      '\\[\\texttt{ATE}(x_{t})=p_{\\theta}(q|X)-p_{\\theta}(q|X\\setminus\\{x_{t}\\}). \\tag{2}\\]\n' +
      '\n' +
      'ATE likelihood methods retrieve the sentence that maximizes \\(\\texttt{ATE}(x_{t})\\). These are the sentences that have the largest effect on the query\'s likelihood. We directly select the sentences that maximize Equation 2 for News Article and Conversation. We perform the same text chunking for Lecture as in the auto-regressive retrieval method: If \\(x_{t}\\in X_{k:2k-1}\\), the ATE likelihood score for \\(x_{t}\\) is measured as \\(p(q|X_{k:2k-1})-p(q|X_{k:2k-1}\\setminus\\{x_{t}\\})\\).\n' +
      '\n' +
      '## 6 Results\n' +
      '\n' +
      'The accuracy results are summarized in Table 2, and distance results in Table 3.\n' +
      '\n' +
      'The best-performing models achieve modest accuracies.For example, on the Lecture domain with many distractor sentences, the best-performing model only achieves top-3 \\(44\\%\\) accuracy. On the Conversation domain with few distractor sentences, the best-performing model only achieves top-3 \\(65\\%\\) accuracy. This underscores that measuring causal relevance is challenging and markedly different from existing retrieval tasks.\n' +
      '\n' +
      'No model performs consistently across domains.For instance, while a similarity-based method like the Bi-Encoder (all-MiniLM) performs well on the News Article domain with top-3 \\(75\\%\\) accuracy, it only manages top-3 \\(37\\%\\) accuracy on the Conversation domain. These results complement the takeaway from the domain analysis in Section 4 that semantic relevance is not a reliable proxy for causal relevance. Interestingly, on the long document domain Lecture, the long-context model gpt-3.5-turbo-16k performs worse than non-contextual methods like single-sentence likelihood methods. This suggests that accounting for context is challenging for current models.\n' +
      '\n' +
      'Single-sentence methods generally outperform their autoregressive counterparts except on Conversation.This result complements the observations made in Section 4\'s domain analysis where the location of the causes concentrates at the start for News Article and uniformly for Lecture, suggesting that little context is needed to identify the cause. Conversely, conversations require more context to distinguish the triggering contexts, which suggests why the autoregressive methods perform generally better than the single-sentence methods.\n' +
      '\n' +
      'ATE likelihood methods does not signicantly improve upon other methods.Even though the ATE likelihood method is designed the calculate the effect of the cause sentence, it competes with noncontextual methods such as the single-sentence likelihood methods. This suggest challenges in using likelihood methods to measure the counter\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c l|c c|c c|c c} \\hline \\hline  & \\multicolumn{2}{c}{**Lecture**} & \\multicolumn{2}{c}{**News Article**} & \\multicolumn{2}{c}{**Conversation**} \\\\  & & @1 & @3 & @1 & @3 & @1 & @3 \\\\ \\hline Random & 0 & 3 & 6 & 21 & 11 & 31 \\\\ Edit & 4 & 8 & 7 & 18 & 1 & 16 \\\\ BM25 & 8 & 15 & 43 & 65 & 1 & 35 \\\\ Bi-Encoder (Q\\&A) & 23 & 37 & 48 & 71 & 1 & 32 \\\\ Bi-Encoder (all-MiniLM) & 26 & 40 & 49 & 75 & 1 & 37 \\\\ Cross-Encoder & 22 & 39 & 66 & **85** & 1 & 15 \\\\ Re-ranker & **30** & **44** & 66 & **85** & 1 & 21 \\\\ gpt-3.5-turbo-16k & 15 & N/A & **67** & N/A & **47** & N/A \\\\ \\hline\n' +
      '**Single-sentence** & GPT2 & 21 & 34 & 43 & 64 & 3 & 46 \\\\ \\(p(q|s_{t})\\) & GPTJ & \\(23\\) & 42 & **67** & **85** & 5 & **65** \\\\  & OPT 6B & **30** & 43 & 66 & 82 & 2 & 56 \\\\ \\hline\n' +
      '**Autoregressive** & GPT2 & 11 & 16 & 9 & 18 & 5 & 54 \\\\ \\(p(q|s_{\\leq t})\\) & GPTJ & 14 & 24 & 55 & 76 & 8 & 60 \\\\  & OPT 6B & 16 & 26 & 52 & 73 & 18 & **65** \\\\ \\hline\n' +
      '**ATE** & GPT2 & \\(13\\) & 21 & 51 & 68 & 2 & 24 \\\\ \\(p(q|S)-p(q|S/\\{s_{t}\\})\\) & GPTJ & 8 & 18 & **67** & 79 & 3 & 18 \\\\  & OPT 6B & 2 & 6 & 64 & 76 & 3 & 22 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Accuracy (\\(\\uparrow\\) % betterd).** The best models in each column are bolded. For each dataset, we report the top-1 and 3 accuracies. gpt-3.5-turbo-16k reports N/A for top-3 accuracy because it does not output deterministic continuous scores for ranking sentences.\n' +
      '\n' +
      'factual effect of a sentence on a query.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'In this paper, we introduce the novel task of back-tracing, which aims to retrieve the text segment that most likely provokes a query. This task addresses the information need of _content creators_ who want to improve their content, in light of queries from information seekers. We introduce a benchmark that covers a variety of domains, such as the news article and lecture setting. We evaluate a series of methods including popular IR methods, likelihood-based retrieval methods and gpt-3.5-turbo-16k. Our results indicate that there is room for improvement across existing retrieval methods. These results suggest that backtracing is a challenging task that requires new retrieval approaches with better contextual understanding and reasoning about causal relevance. We hope our benchmark serves as a foundation for improving future retrieval systems for backtracing, and ultimately, spawns systems that empower content creators to understand user queries, refine their content and provide users with better experiences.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'Single-sentence focus.Our approach primarily focuses on identifying the most likely single sentence that caused a given query. However, in certain scenarios, the query might depend on groups or combinations of sentences. Ignoring such dependencies can limit the accuracy of the methods.\n' +
      '\n' +
      'Content creators in other domains.Our evaluation primarily focuses on the dialog, new article and lecture settings. While these domains offer valuable insights, the performance of backtracing methods may vary in other contexts, such as scientific articles and queries from reviewers. Future work should explore the generalizability of backtracing methods across a broader range of domains and data sources.\n' +
      '\n' +
      'Long text settings.Due to the length of the lecture transcripts, the transcripts had to be divided and passed into the likelihood-based retrieval methods. This approach may result in the omission of crucial context present in the full transcript, potentially affecting the accuracy of the likelihood-based retrieval methods. Exploring techniques to effectively handle larger texts and overcome model capacity constraints would be beneficial for improving backtracing performance in long text settings, where we would imagine backtracing to be useful in providing feedback for.\n' +
      '\n' +
      'Multimodal sources.Our approach identifies the most likely text segment in a corpus that caused\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c l|c c|c c|c c} \\hline \\hline  & \\multicolumn{2}{c}{**Lecture**} & \\multicolumn{2}{c}{**News Article**} & \\multicolumn{2}{c}{**Conversation**} \\\\  & & @1 & @3 & @1 & @3 & @1 & @3 \\\\ \\hline Random & 167.5 & 67.8 & 7.6 & 3.0 & 3.7 & 1.7 \\\\ Edit & 157.9 & 70.7 & 7.7 & 3.4 & 1.3 & 0.9 \\\\ BM25 & 122.7 & 50.7 & 4.6 & 1.4 & 1.3 & 0.7 \\\\ Bi-Encoder (Q&A) & 91.9 & 35.2 & 4.1 & 1.2 & 1.3 & 0.8 \\\\ Bi-Encoder (all-MiniLM) & 84.7 & 38.6 & 3.7 & 1.0 & 1.3 & 0.7 \\\\ Cross-Encoder & 96.6 & 33.8 & 2.5 & **0.6** & 1.3 & 0.9 \\\\ Re-ranker & 92.2 & 41.4 & 2.7 & **0.6** & 1.3 & 0.9 \\\\ gpt-3.5-turbo-16k & 73.9 & N/A & **1.5** & N/A & **1.0** & N/A \\\\ \\hline\n' +
      '**Single-sentence** & GPT2 & \\(5.4^{*}\\) & \\(2.1^{*}\\) & 4.6 & 1.5 & 1.5 & 0.6 \\\\ \\(p(q|s_{t})\\) & GPTJ & \\(\\mathbf{5.0^{*}}\\) & \\(\\mathbf{1.9^{*}}\\) & 2.5 & 0.7 & 1.4 & **0.4** \\\\ OPT 6B & \\(5.2^{*}\\) & \\(2.3^{*}\\) & 2.7 & 0.8 & 1.3 & 0.5 \\\\ \\hline\n' +
      '**Autoregressive** & GPT2 & \\(5.6^{*}\\) & \\(3.4^{*}\\) & 7.2 & 4.8 & 2.0 & 0.8 \\\\ \\(p(q|s_{\\leq t})\\) & GPTJ & \\(5.5^{*}\\) & \\(3.4^{*}\\) & 1.8 & 0.8 & 2.0 & 0.8 \\\\ OPT 6B & \\(5.1^{*}\\) & \\(3.5^{*}\\) & 1.9 & 1.0 & 1.9 & 0.7 \\\\ \\hline\n' +
      '**ATE** & GPT2 & \\(7.4^{*}\\) & \\(2.8^{*}\\) & 4.7 & 1.3 & 1.5 & 0.9 \\\\ \\(p(q|S)-p(q|S/\\left\\{s_{t}\\right\\})\\) & GPTJ & \\(7.2^{*}\\) & \\(3.2^{*}\\) & 2.9 & 0.9 & 1.6 & 1.0 \\\\ OPT 6B & \\(7.1^{*}\\) & \\(\\mathbf{1.9^{*}}\\) & \\(3.2\\) & 1.1 & 2.4 & 1.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **Minimum Sentence Distance from Ground Truth (\\(\\downarrow\\) better)** The best models in each column are bolded. For each dataset, we report the minimum sentence distance from the ground truth cause sentence of the method’s top-\\(1\\) and \\(3\\) candidates; 0 meaning that the method always predicts the ground truth candidate sentence. Note for the likelihood-based methods on the Lecture domain were evaluated on 20-sentence chunks of the original text due to the context window limitation. If the top sentence is not in the top-chunk, it is excluded in distance metric. We’ve marked the affected metrics with an asterisk \\({}^{*}\\).\n' +
      '\n' +
      'a given query. However, in multimodal settings, a query may also be caused by other data types, e.g., visual cues that are not captured in the transcripts. Ignoring such non-textual data can limit the accuracy of the methods.\n' +
      '\n' +
      '## Ethics Statement\n' +
      '\n' +
      'Empowering content creators to refine their content based on user feedback contributes to the production of more informative materials. Therefore, our research has the potential to enhance the educational experiences of a user, by assisting content creators through backtracing. Nonetheless, we are mindful of potential biases or unintended consequences that may arise through our work and future work. For example, the current benchmark analyzes the accuracy of backtracing on English datasets and uses PLMs trained predominantly on English texts. As a result, the inferences drawn from the current backtracing results or benchmark may not accurately capture the causes of multilingual queries, and should be interpreted with caution. Another example is that finding the cause for a user emotion can be exploited by content creators. We consider this as an unacceptable use case of our work, in addition to attempting to identify users in the dataset or using the data for commercial gain.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'We\'d like thank Michael Zhang and Dilip Arumugam for the fruitful conversations at the start of this project. We\'d also like to thank Gabriel Poesia for helpful feedback on the paper.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* J. Devlin, M. Chang, K. Lee, and K. Toutanova (2018)Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Cited by: SS1.\n' +
      '* N. Chaswell, B. Mitra, E. Yilmaz, and D. Campos (2021)Overview of the trec 2020 deep learning track. Cited by: SS1.\n' +
      '* N. Craswell, B. Mitra, E. Yilmaz, D. Campos, and E. M. Voorhees (2020)Overview of the trec 2019 deep learning track. Cited by: SS1.\n' +
      '* J. Devlin, M. Chang, K. Lee, and K. Toutanova (2018)Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Cited by: SS1.\n' +
      '* N. Duan, D. Tang, P. Chen, and M. Zhou (2017)Question generation for question answering. In Proceedings of the 2017 conference on empirical methods in natural language processing, pp. 866-874. Cited by: SS1.\n' +
      '* W. E. Evans and R. E. Guymon (1978)Clarity of explanation: a powerful indicator of teacher effectiveness. Cited by: SS1.\n' +
      '* N. Fuhr (2018)Some common mistakes in ir evaluation, and how they can be avoided. In Acm sigir forum, Vol. 51, pp. 32-41. Cited by: SS1.\n' +
      '* C. Gormally, M. Evans, and P. Brickman (2014)Feedback about teaching in higher ed: Neglected opportunities to promote change. CBE--Life Sciences Education13 (2), pp. 187-199. Cited by: SS1.\n' +
      '* J. Guo, Y. Fan, Q. Ai, and W. Bruce Croft (2016)A deep relevance matching model for ad-hoc retrieval. In Proceedings of the 25th ACM international on conference on information and knowledge management, pp. 55-64. Cited by: SS1.\n' +
      '* M. Guo, Y. Yang, D. Cer, Q. Shen, and N. Constant (2020)Multireqa: a cross-domain evaluation for retrieval question answering models. Cited by: SS1.\n' +
      '* L. Harvey (2003)Student feedback [1]. Quality in higher education9 (1), pp. 3-20. Cited by: SS1.\n' +
      '* N. Hativa (1998)Lack of clarity in university teaching: a case study. Higher Education, pp. 353-381. Cited by: SS1.\n' +
      '* M. Heilman and N. A. Smith (2010)Good question! statistical ranking for question generation. In Human language technologies: The 2010 annual conference of the North American Chapter of the Association for Computational Linguistics, pp. 609-617. Cited by: SS1.\n' +
      '* P. W. Holland (1986)Statistics and causal inference. Journal of the American statistical Association81 (396), pp. 945-960. Cited by: SS1.\n' +
      '* M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer (2017)Triviaqa: a large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601-1611. Cited by: SS1.\n' +
      '* V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W. Yih (2020)Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online, pp. 6769-6781. External Links: Link, Document Cited by: SS1, SS2.1.\n' +
      '* O. Khattab and M. Zaharia (2020)Colbert: efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pp. 39-48. Cited by: SS1.\n' +
      '* W. Ko, T. Chen, Y. Huang, G. Durrett, and J. J. Li (2020)Inquisitive question generation for high level text comprehension. arXiv preprint arXiv:2010.01657. Cited by: SS1.\n' +
      '* W. Kong, R. Li, J. Luo, A. Zhang, Y. Chang, and J. Allan (2015)Predicting search intent based on pre-search context. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 503-512. Cited by: SS1.\n' +
      '* M. Koskela, P. Luukkonen, T. Ruotsalo, M. Sjoberg, and P. Floreen (2018)Proactive information retrieval by capturing search intent from primary task context. ACM Transactions on Interactive Intelligent Systems (TiiS)8 (3), pp. 1-25. Cited by: SS1.\n' +
      '* M. Library (2023)MiniChain Library. Note: [https://github.com/srush/minichain#typed-prompts](https://github.com/srush/minichain#typed-prompts) External Links: Link Cited by: SS1.\n' +
      '* I. McKenzie (2023)Inverse scaling prize: first found winners. Note: [https://irmckenzie.co.uk/round1#](https://irmckenzie.co.uk/round1#): -?:text=model%20should%20answer.-, Using%20newlines.-We%20saw%20many External Links: Link Cited by: SS1.\n' +
      '* K. E. McKone (1999)Analysis of student feedback improves instructor effectiveness. Journal of Management Education23 (4), pp. 396-415. Cited by: SS1.\n' +
      '* R. Nogueira and K. Cho (2019)Passage re-ranking with bert. arXiv preprint arXiv:1901.04085. Cited by: SS1.\n' +
      '* L. Pan, W. Lei, T. Chua, and M. Kan (2019)Recent advances in neural question generation. arXiv preprint arXiv:1905.08949. Cited by: SS1.\n' +
      '* E. Perez, D. Kiela, and K. Cho (2021)True few-shot learning with language models. Advances in neural information processing systems34, pp. 11054-11070. Cited by: SS1.\n' +
      '* M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer (2018)Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2227-2237. External Links: Link, Document Cited by: SS1.\n' +
      '* S. Poria, N. Majumder, D. Hazarika, D. Ghosal, R. Bhardwaj, S. P. Bai, J. Hong, R. Ghosh, A. Roy, N. Chhaya, et al. (2021)Recognizing emotion cause in conversations. Cognitive Computation13, pp. 1317-1332. Cited by: SS1.\n' +
      '* A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. (2019)Language models are unsupervised multitask learners. OpenAI blog1 (8), pp. 9. Cited by: SS1.\n' +
      '* P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang (2016)Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383-2392. Cited by: SS1.\n' +
      '* N. Reimers and I. Gurevych (2019)Sentence-bert: sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. Cited by: SS1.\n' +
      '* N. Reimers and I. Gurevych (2019)Sentence-bert: sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. Cited by: SS1.\n' +
      '* R. Ren, Y. Qu, J. Liu, W. X. Zhao, Q. She, H. Wu, H. Wang, and J. Wen (2021)Rocketqav2: a joint training method for dense passage retrieval and passage re-ranking. arXiv preprint arXiv:2110.07367. Cited by: SS1.\n' +
      '* K. Roberts, D. Demner-Fushman, E. M. Voorhees, S. Bedrick, and W. R. Hersh (2021)Overview of the trec 2021 clinical trials track. In Proceedings of the Thirtieth Text REtrieval Conference (TREC 2021), Cited by: SS1.\n' +
      '* D. S. Sachan, M. Lewis, M. Joshi, A. Aghajanyan, W. Yih, J. Pineau, and L. Zettlemoyer (2022)Improving passage retrieval with zero-shot question generation. arXiv preprint arXiv:2204.07496. Cited by: SS1.\n' +
      '* H. Schutze, C. D. Manning, and P. Raghavan (2008)Introduction to information retrieval, volume 39. Cambridge University Press Cambridge. Cited by: SS1.\n' +
      '* I. Soboroff (2021)Overview of trec 2021. In 30th Text REtrieval Conference. Gaithersburg, Maryland, Cited by: SS1.\n' +
      '* I. Soboroff, S. Huang, and D. Harman (2018)Trec 2018 news track overview. In TREC, Vol. 409, pp. 410. Cited by: SS1.\n' +
      '* N. Thakur, N. Reimers, A. Ruckle, A. Srivastava, and I. Gurevych (2021)Beir: a heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663. Cited by: SS1.\n' +
      '* G. Tsatsaronis, G. Balikas, P. Malakasiotis, I. Pattalas, M. Zschunke, M. R. Alvers, D. Weissenborn, A.\n' +
      '\n' +
      'Krishara, Sergios Petridis, Dimitris Polychronopoulos, et al. 2015. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. _BMC bioinformatics_, 16(1):1-28.\n' +
      '* Voorhees et al. (2021) Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2021. Trec-covid: constructing a pandemic information retrieval test collection. In _ACM SIGIR Forum_, volume 54, pages 1-12. ACM New York, NY, USA.\n' +
      '* Voorhees (2005) Ellen M Voorhees. 2005. The trec robust retrieval track. In _ACM SIGIR Forum_, volume 39, pages 11-20. ACM New York, NY, USA.\n' +
      '* Wang and Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. 2021. Gpt-j-6b: A 6 billion parameter autoregressive language model.\n' +
      '* Wang et al. (2023) Rose Wang, Pawan Wirawarn, Noah Goodman, and Dorottya Demszky. 2023. Sight: A large annotated dataset on student insights gathered from higher education transcripts. In _Proceedings of Innovative Use of NLP for Building Educational Applications_.\n' +
      '* Xiong et al. (2017) Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-end neural ad-hoc ranking with kernel pooling. In _Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval_, pages 55-64.\n' +
      '* Yang et al. (2015) Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. Wikiqa: A challenge dataset for open-domain question answering. In _Proceedings of the 2015 conference on empirical methods in natural language processing_, pages 2013-2018.\n' +
      '* Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2369-2380.\n' +
      '* Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models.\n' +
      '* Zhuang et al. (2021) Shengyao Zhuang, Hang Li, and Guido Zuccon. 2021. Deep query likelihood model for information retrieval. In _Advances in Information Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28-April 1, 2021, Proceedings, Part II 43_, pages 463-470. Springer.\n' +
      '* Zhuang and Zuccon (2021) Shengyao Zhuang and Guido Zuccon. 2021. Tilde: Term independent likelihood model for passage reranking. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 1483-1492.\n' +
      '* Ziems et al. (2023) Caleb Ziems, William Held, Omar Shaikh, Jiao Chen, Zhehao Zhang, and Diyi Yang. 2023. Can large language models transform computational social science? _arXiv preprint arXiv:2305.03514_.\n' +
      '\n' +
      '## Appendix A Computational Setup\n' +
      '\n' +
      'We ran our experiments on a Slurm-based university compute cluster on A6000 machines. The experiments varied in length in time--some took less than an hour to run (e.g., the random baselines), while others took a few days to run (e.g., the ATE likelihood-based methods on Lecture).\n' +
      '\n' +
      '## Appendix B Lecture annotation interface\n' +
      '\n' +
      'Figure 7 shows the interface used for annotating the Lecture dataset.\n' +
      '\n' +
      '## Appendix C Contextualized prefixes for scoring\n' +
      '\n' +
      'This section describes the prompts used for the likelihood-based retrieval methods and gpt-3.5-turbo-16k.\n' +
      '\n' +
      'The prompts used for gpt-3.5-turbo-16k follow the practices in works from NLP, education and social sciences McKenzie (2023); Library (2023); Ziems et al. (2023); Wang et al. (2023). Specifically, we enumerate the sentences in the corpus as multiple-choice options and each option is separated by a newline. We add context for the task at the start of the prompt, and the constraints of outputting a JSON-formatted text for the task at the end of the prompt. We found the model to be reliable in outputting the text in the desirable format.\n' +
      '\n' +
      '### Lecture\n' +
      '\n' +
      'For the likelihood-based retrieval methods, the sentences are concatenated by spaces and "A teacher is teaching a class, and a student asks a question.\\nTeacher: " is prepended to the corpus. Because the text comes from transcribed audio which is not used in training dataset of the PLMs we use in our work, we found it important for additional context to be added in order for the probabilities to be slightly better calibrated. For the query, "Student: " is prepended to the text. For example, \\(X=\\) "A teacher is teaching a class, and a student asks a question.\\nTeacher: [sentence 1] [sentence 2]...", and \\(q=\\) "Student: [query]".\n' +
      '\n' +
      'The prompt used for gpt-3.5-turbo-16k is in Figure 8.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '**gpt-3.5-turbo-16k prompt for News Article**\n' +
      '\n' +
      'Consider the following article:\n' +
      '\n' +
      '{line-numbered article}\n' +
      '\n' +
      'Now consider the following question:\n' +
      '\n' +
      '{query}\n' +
      '\n' +
      'Which of the article lines most likely provoked this question? If there are multiple possible answers, list them out. Format your answer as: ["line number": integer, "reason": "reason for why this line most likely caused this query",...]\n' +
      '\n' +
      'Figure 10: gpt-3.5-turbo-16k prompt for Conversation. For the line-numbered conversation, the speaker is added to each turn, the turns are separated by line breaks, and each line begins with its line number. For the query, the speaker is also added. For example, a line-numbered conversation may look like “0. Speaker A: [utterance]\\n 1. Speaker B: [utterance]\\n 2. Speaker A: [utterance]...”, and the query may look like “Speaker A: [query]”.\n' +
      '\n' +
      'Figure 9: gpt-3.5-turbo-16k prompt for News Article. For the line-numbered article, “Text: ” is prepended to each sentence, the sentences are separated by line breaks, and each line begins with its line number. For the query, “Question: ” is prepended to the text. For example, a line-numbered article looks like “0. Text: [sentence 1]\\n 1. Text: [sentence 2]\\n 2. Text: [sentence 3]...”, and the query looks like “Question: [question]”.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>