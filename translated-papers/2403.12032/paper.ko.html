<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '제어된 다시점 편집을 이용한 #일반 3차원 확산 어댑터\n' +
      '\n' +
      'HANSHENG CHEN\n' +
      '\n' +
      ' 스탠포드 대학교\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'RUOXI SHI\n' +
      '\n' +
      ' UC 샌디에이고\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'YULIN LIU\n' +
      '\n' +
      ' UC 샌디에이고\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'BOKUI SHEN\n' +
      '\n' +
      ' Apparate Lab.\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'JIAYUAN GU\n' +
      '\n' +
      ' UC 샌디에이고\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'GORDON WETZSTEIN\n' +
      '\n' +
      ' 스탠포드 대학교\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'HAO SU\n' +
      '\n' +
      ' UC 샌디에이고\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'LEONIDAS GUIBAS\n' +
      '\n' +
      ' 스탠포드 대학교\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'Demo & code: [https://lakonik.github.io/mvedit/](https://lakonik.github.io/mvedit/]\n' +
      '\n' +
      '오픈 도메인 3D 객체 합성은 제한된 데이터와 높은 계산 복잡성으로 인해 이미지 합성에 뒤쳐져 왔다. 이러한 간극을 메우기 위해 최근 연구는 다시점 확산을 조사했지만 3D 일관성, 시각적 품질 또는 효율성에 미치지 못하는 경우가 많다. 본 논문에서는 SDEdit의 3차원 대응물로서 동작하는 _MVEdit_를 제안한다. _MVEdit_는 다시점 영상을 공동으로 잡음제거하고 고품질의 텍스처 메쉬를 출력하기 위해 조상 샘플링을 사용한다. 외장형 2D 확산 모델에서 구축된 MVEdit은 트레이닝이 없는 _3D Adapter_를 통해 3D 일관성을 달성하며, 이는 마지막 타임스테프의 2D 뷰를 일관된 3D 표현으로 들어올린 다음, 타협하지 않는 시각적 품질 없이 렌더링된 뷰를 사용하여 다음 타임스테프의 2D 뷰를 컨디셔닝한다. 2-5분의 추론 시간으로, 이 프레임워크는 점수 증류보다 품질과 속도 사이의 더 나은 절충을 달성한다. MVEdit은 텍스트/이미지-대-3D 생성, 3D-대-3D 편집 및 고품질 질감 합성을 포함한 광범위한 응용 프로그램으로 매우 다양하고 확장 가능하다. 특히, 평가들은 이미지-투-3D 및 텍스트-유도 텍스처 생성 태스크들 모두에서 최첨단 성능을 입증한다. 또한 제한된 자원을 가진 작은 3차원 데이터 세트에서 2차원 잠재 확산 모델을 미세 조정함으로써 빠른 저해상도 텍스트-3차원 초기화가 가능하도록 하는 방법을 소개한다.\n' +
      '\n' +
      '그림 1. ** 예는 관련 추론 시간(RTX A6000에서) 및 타임스텝 수와 함께 다양한 3D 작업에 걸쳐 MVEdit의 일반성을 보여준다. 이미지 대 3D의 경우, Zero123++에 의한 초기 뷰들은 엄격하게 3D 일관성이 없다(도 9의 고장을 야기함). MVEdit에서 해결한 문제입니다.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:2]\n' +
      '\n' +
      '손실 함수로서 미리 훈련된 이미지 확산 모델을 사용하여 NeRF를 최적화하는, 방법. 제한된 해상도, 야누스 문제, 과포화 색상들, 및 모드-추구 행동과 같은 그것의 이슈들 중 일부는 후속 작업들에서 다루어졌다(Chen et al., 2023; Lin et al., 2023; Qian et al., 2024; Sun et al., 2024; Wang et al., 2023a). 개선에도 불구하고 SDS와 그 변이체는 여전히 시간이 많이 걸리고 종종 조상 샘플링과 비교하여 저하된 분포를 산출한다. (Haque et al., 2023; Zhou and Tulsiani, 2023)는 조상 샘플링과 최적화를 번갈아 가며, 이는 또한 비효율적이다. 조상의 샘플링을 한 번만 수행하고 각 타임스텝 내에서 NeRF를 최적화하는 NerffDiff(Gu et al., 2023)에서 더 빠른 접근법이 보인다. 그러나 다양한 오픈 도메인 객체를 다루는 경우 샘플링 프로세스를 방해하는 NeRF로 인해 동일한 흐림 문제를 직면하게 되며, 이는 이 작업에서 해결해야 할 과제이다.\n' +
      '\n' +
      '##3. MVedit : 제어된 다시점 편집\n' +
      '\n' +
      '섹션 2.2 및 2.3에서 논의된 바와 같이, 잡음 제거 네트워크에 3D NeRF를 추가하지만(그림). 2(b))는 3D 일관성을 보장하며, NeRF가 일반적으로 불일치하는 멀티뷰 입력을 평균하여 불가피한 손실을 초래하기 때문에 종종 흐릿한 결과를 초래한다. 잠재 확산 모델들(Rombach et al., 2022)의 경우, 추가적인 VAE 디코딩 및 인코딩 프로세스는 이 이슈를 더욱 악화시킬 수 있다.\n' +
      '\n' +
      '입력 잡음 뷰에서 잡음 제거된 뷰로의 정보 흐름을 방해하지 않으면서 3D 일관성 챌린지를 해결하기 위해, 우리는 3D 모델 주위의 스킵 연결을 포함하는 새로운 아키텍처를 제안한다(그림). 도 2(c) 및 그 간략화 버전(도 2). 2(d)). 단순화된 아키텍처를 기반으로 그림 1에 표시된 MVEdit 프레임워크를 소개한다. 도 4를 참조하여 상세히 설명하면 다음과 같다.\n' +
      '\n' +
      '### Framework Overview\n' +
      '\n' +
      '1.1.1. 예비: 단일 영상 확산을 이용한 SDEdit\n' +
      '\n' +
      '그림 1의 빨간색과 주황색 흐름을 무시합니다. 도 4에 도시된 바와 같이, 나머지 청색 흐름은 베이스 텍스트-이미지 2D 확산 모델을 사용하는 원래의 SDEdit 샘플링 프로세스를 묘사한다. 잠재 확산 모델의 경우 간결성을 위해 VAE 인코딩/디코딩 프로세스를 생략한다. 초기 RGB 이미지\\(x^{\\text{init}}\\in\\mathbb{R}^{C\\times H\\times W}\\)이 주어지면, SDEdit은 먼저 가우시안 확산 과정을 따라 랜덤 잡음\\(\\epsilon\\sim\\mathcal{N}(0,I)\\)으로 이미지를 교란시킨다:\n' +
      '\n' +
      '\\[x^{(t)}=\\alpha^{(t)}x^{\\text{init}}+\\sigma^{(t)}\\epsilon, \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(t\\gets t_{\\text{start}}\\in[0,T]\\)는 사용자가 지정한 시작 타임스템이고, \\(\\alpha^{(t)},\\sigma^{(t)}\\)는 잡음 스케줄에 의해 결정되는 스칼라이며, \\(x^{(t)}\\)은 잡음 영상을 나타낸다. 잡음제거 단계를 위해 UNet\\(\\hat{\\epsilon}\\Big{(}x^{(t)},c,t\\Big{)}\\)은 잡음영상\\(x^{(t)}\\), 조건\\(c\\)(즉, 텍스트 프롬프트), 타임스텝\\(t\\)으로부터 잡음성분\\(\\hat{\\epsilon}\\)을 예측한다. 이후, 예측된 잡음으로부터 잡음 제거된 영상\\(\\hat{\\epsilon}\\)을 얻을 수 있다.\n' +
      '\n' +
      '\\frac{\\Big{(}x^{(t)}-\\sigma^{(t)}\\hat{\\epsilon}\\Big{(}x^{(t)},c,t\\Big{)}\\Big{}}{\\alpha^{(t)}. \\tag{2}\\\n' +
      '\n' +
      '다음 단계로 나아가기 위해, 일반적인 확산 ODE 또는 SDE 솔버(Song et al., 2021)는 이전 타임스텝 \\(t-\\Delta t\\)에서 덜 잡음인 이미지 \\(x^{(t-M)}\\)를 산출하기 위해 적용될 수 있다. 본 논문에서는 DPMSolver++ (Lu et al., 2022)를 채택하여 Solver 단계를 다음과 같이 작성할 수 있다.\n' +
      '\n' +
      '\\[x^{(t-\\Delta t)}\\gets DPMSolver_{2}\\Big{(}\\hat{x},t,x^{(t)}\\Big{}, \\tag{3}\\gets\n' +
      '\n' +
      '여기서 \\(z\\)는 솔버의 내부 상태를 나타낸다. 재귀적 잡음 제거는 Eq를 반복함으로써 실행될 수 있다. (2) 및 Eq. (3) 잡음 제거된 상태 \\(x^{(0)}\\)에 도달할 때까지, 따라서 조상 샘플링 프로세스를 완료한다.\n' +
      '\n' +
      'Multi-View Diffusion을 이용한 MVEdit\n' +
      '\n' +
      'MVEdit에서 우리는 그림 4의 빨간색 흐름으로 묘사된 새로운 3D 어댑터를 통해 단일 이미지 확산 모델을 3D 일관된 다중 뷰 확산 모델에 적용한다. 각 타임스텝에 대해 먼저 알려진 카메라 매개변수로 미리 정의된 모든 뷰의 잡음 제거된 이미지\\(\\hat{x}_{i}\\}\\)를 얻으며, 여기서 \\(i\\)은 뷰 인덱스를 나타낸다. 그런 다음, 이 잡음 제거된 뷰들로부터 \\(\\hat{\\phi}\\)에 의해 파라미터화된 3D 표현을 재구성할 수 있다. 본 논문에서는 NeRF를 위한 InstantNGP (Muller et al., 2022) 또는 메쉬를 위한 DMTet (Shen et al., 2021)을 사용하여 최적화 기반 재구성 접근법을 사용한다. 따라서, 3차원 매개변수\\(\\hat{\\phi}\\)를 추정할 수 있다.\n' +
      '\n' +
      '그림 3. **Text-Guided 3D-to-3D 파이프라인 기반의 두 아키텍처를 \\(t^{\\text{start}}=0.78T\\)로 비교. 서로 다른 시간에 걸쳐 렌더링된 RGB 이미지\\(x^{\\text{rend}}_{\\text{RGB}}\\)는 샘플링 프로세스를 시각화하기 위해 표시된다.\n' +
      '\n' +
      '그림 2. **3D 인식 멀티뷰 노이즈 제거 아키텍처 간의 비교**. (c)에서 3D NeRF 주위에 스킵 연결을 추가하면 (b)의 잠재적인 흐림 문제를 완화하지만, 기성품 2D 안정확산을 확장할 때 동일한 잡음제거 타임스텝 내에서 두 개의 2D UNet 패스를 필요로 한다; (d)에서 우리의 단순화된 아키텍처는 3D NeRF를 재구성하기 위해 마지막 잡음제거 타임스텝으로부터 잡음제거된 다시점 이미지를 재사용한다.\n' +
      '\n' +
      '(\\{\\hat{x}_{i}\\}\\})에 대한 렌더링 손실을 최소화하는 단계:\n' +
      '\n' +
      '\\[\\hat{\\phi}=\\operatorname*{arg\\,min}_{\\phi}\\mathcal{L}_{\\text{rend}(\\{\\hat{x}_{i},p_{i}\\,\\phi}\\tag{4}\\)\n' +
      '\n' +
      '손실 및 최적화에 대한 자세한 내용은 섹션 3.2에서 설명되며, 재구성된 3D 표현으로 RGBD 채널(\\(\\{x_{i}^{\\text{rend}}\\})을 갖는 새로운 이미지 세트가 뷰들로부터 렌더링될 수 있다. 이러한 엄격하게 3D-일관된 렌더링은 멀티뷰 집계의 결과이며, 초기 잡음 제거 단계에서 흐릿한 경향이 있다. 컨디셔닝 신호로서 제어넷(Zhang et al., 2023)에 \\(x_{i}^{\\text{rend}\\)을 공급함으로써, 제어된 UNet \\(\\hat{\\epsilon}^{\\text{ctl}}\\Big{(}x^{(t),c_{t,t,x_{i}^{\\text{rend}\\Big{)}을 통과하는 두 번째 패스를 통해 더 선명한 이미지 \\(\\hat{x}^{i}^{\\text{ctl}\\)를 얻을 수 있다:\n' +
      '\n' +
      '{x}_{i}^{\\text{ctrl}=\\frac{\\Big{(}x_{i}^{(t)}-\\sigma^{(t)}\\hat{\\epsilon}^{\\text{ctl}\\Big{(}x_{i}^{(t)},c_{t},t,x_{i}^{\\text{rend}\\Big{}}{ \\alpha^{(t)}}}. \\tag{5}\\t}\n' +
      '\n' +
      '따라서 Eq의 솔버 단계에서 \\(\\hat{x}_{i}\\)을 \\(\\hat{x}_{i}^{\\text{ctl}\\)으로 대체함으로써 3D 일관성 있는 샘플링을 달성할 수 있다. (3). Eq. (5)는 그림 2-패스 아키텍처를 효과적으로 공식화한다. 도 2의 (c)에서, 스킵 연결은 본질적으로 시끄러운 멀티뷰를 제2 UNet으로 재공급하고 있다. 실제로, 단일 잡음 제거 단계 내에서 두 개의 패스를 실행하는 것은 중복으로 보인다. 따라서 마지막 잡음 제거 단계에서 렌더링된 뷰를 사용하여 그림의 단순화된 아키텍처에 해당하는 다음 잡음 제거 단계의 UNet을 조정한다. 2(d).\n' +
      '\n' +
      '실증적으로, Stable Diffusion (Rombach et al., 2022)을 기본 모델로 하여, Off-the-shelf _Tile_ (conditioned on blurry RGB image)와 _Depth_ (conditioned on depth map) ControlNets가 일관된 다시점 생성을 위해 이미 RGB와 depth conditioning을 처리할 수 있음을 발견하여, 커스텀 ControlNet의 트레이닝의 필요성을 배제하였다. 그러나, 재귀적 자기-조절은 컬러 드리프팅 또는 오버-샤프닝/평활화와 같은 안정한 확산 내의 일부 불리한 바이어스를 증폭시킬 수 있다. 따라서 시간 의존적인 동적 제어넷 가중치를 적용한다. 특히, Eq에서 \\(t\\)이 크면 \\(Tile\\) ControlNet의 무게를 줄이고, 그렇지 않으면 작은 분모 \\(\\alpha^{(t)}\\)을 줄인다. (5) 이때 분자의 임의의 편향을 상당히 증폭시킬 것이다. 그러나 ControlNet 가중치를 줄이면 3D 일관성이 더 나빠진다. 일관성 문제를 완화하기 위해 \\(t>0.4T\\)에 대해서만 추가 가중 혼합 연산을 도입한다.\n' +
      '\n' +
      '\\hat{x}_{i}^{\\text{blend}=w^{(t)}x_{\\text{RGB}i}^{\\text{rend}+(1-w^{(t)}) \\hat{x}_{i}^{\\text{ctrl}, \\tag{6}\\\n' +
      '\n' +
      '여기서 \\(x_{\\text{RGB}i}^{\\text{rend}\\)는 렌더링된 이미지의 RGB 채널을 나타내고, \\(\\hat{x}_{i}^{\\text{ctl}\\)은 ControlNet 가중치가 감소된 잡음 제거된 이미지이고, \\(w^{(t)}\\)은 시간 의존적인 블렌딩 가중치이다. 혼합된 이미지\\(\\hat{x}_{i}^{\\text{blend}\\)는 DPMSolver에 공급될 잡음 제거된 이미지로 처리된다.\n' +
      '\n' +
      '### 강력한 NeRF/메쉬 최적화\n' +
      '\n' +
      '3D 어댑터는 특히 초기 잡음 제거 단계에서 잠재적으로 일관성이 없는 멀티뷰 입력의 도전에 직면한다. Neus(Wang et al., 2021)와 같은 기존의 표면 최적화 접근법은 불일치를 해결하기 위해 설계되지 않았다. 따라서, 우리는 향상된 정규화 및 점진적 해상도를 사용하여 InstANGP NeRF(Muller et al., 2022) 및 DMTet 메쉬(Shen et al., 2021)의 강건한 최적화를 위한 다양한 기술을 개발했다.\n' +
      '\n' +
      '#### 3.2.1. Rendering\n' +
      '\n' +
      '각 NeRF 최적화 반복에 대해 모든 카메라 뷰에서 128\\(\\times\\)128 이미지 패치를 무작위로 샘플링한다. NeRF 밀도 구배로부터 법선을 계산하는 (Poole et al., 2023)과 달리, 우리는 렌더링된 깊이 맵들로부터 패치-와이즈 법선 맵들을 계산하며, 이는 더 빠르고 더 견고하다는 것을 발견한다. 메쉬 렌더링을 위해 NeRF에 사용되는 동일한 Instant-NGP 신경 필드를 질의하여 표면 색상을 얻는다. NeRF와 메시 모두에 대해 람베르시안 음영은 톤 매핑 전에 선형 색상 공간에 적용되며 각 뷰에 무작위 점 조명이 할당된다.\n' +
      '\n' +
      'RGBA Losses\n' +
      '\n' +
      'NeRF와 메쉬 모두에 대해 RGB와 Alpha 렌더링 손실을 사용하여 3D 매개변수\\(\\phi\\)를 최적화함으로써 렌더링된 뷰\\(\\x_{i}^{\\text{rend}}\\}\\)이 목표 잡음 제거된 뷰\\(\\hat{x}_{i}\\}\\)과 일치하도록 한다. RGB는 픽셀-와이즈 L\\({}_{1}\\) 손실과 패치-와이즈 LPIPS 손실의 조합을 사용한다(Zhang et al., 2018). Alpha의 경우, Magic123(Qian et al., 2024)에서와 같이 Off-the-shelf 배경 제거 네트워크(Lee et al., 2022)를 사용하여 \\(\\hat{x}_{i}\\}\\)로부터 목표 Alpha 채널을 예측한다. 또한, NeRF가 초기화에 과도하게 맞지 않도록 가우시안 블러를 사용하여 예측된 알파 맵을 부드럽게 한다.\n' +
      '\n' +
      '####3.2.3. 정상 손실\n' +
      '\n' +
      '울퉁불퉁한 표면을 피하기 위해 렌더링된 정규 지도에 L\\({}_{1.5}\\) 총 변동(TV) 정규화 손실을 적용한다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{N}=\\sum_{chw}\\left\\|w_{h\\cdot w}\\cdot\\nabla_{hw}x_{chw}^{\\text{rend}}\\right\\|^{1.5}, \\tag{7}\\\\tag{\n' +
      '\n' +
      '그림 4. **MVEdit.**의 초기화 및 조상 샘플링 프로세스 원본 단일 이미지 SDEdit는 파란색, 추가 3D 어댑터는 빨간색, 추가 컨디셔닝은 주황색으로 표시된다. 간결함을 위해, 제1 뷰만이 묘사되고, 잠재 확산과 관련된 경우 VAE 인코딩/디코딩은 생략된다.\n' +
      '\n' +
      '여기서 \\(r\\)은 광선 인덱스이고 \\(d\\)은 전경 대 배경 비율의 균형을 맞추기 위해 조정될 수 있는 상상력 배경 쉘의 사용자 정의 "두께"이다.\n' +
      '\n' +
      '#### 3.2.5. 메쉬 스무딩 손실\n' +
      '\n' +
      '일반적인 관행에 따라, 우리는 DMTet으로부터 추출된 메쉬를 추가로 정규화하기 위해 라플라시안 평활 손실(Sorkine et al., 2004) 및 정규 일관성 손실을 사용한다.\n' +
      '\n' +
      '3.2.6. 구현 세부사항\n' +
      '\n' +
      '전술한 손실 함수들의 가중 합은 3D 표현을 최적화하기 위해 활용된다. 각각의 잡음 제거 단계에서, 우리는 이전 단계로부터 3D 표현을 전달하고 Adam(Kingma and Ba, 2015) 최적화(3D에 대해 96 또는 텍스쳐에만 대해 48)의 추가 반복을 수행한다. 선조 샘플링 프로세스 동안, 렌더링 분해능은 128에서 256으로 점진적으로 증가하고, NeRF가 메시로 변환될 때(텍스쳐-전용 분해능은 일관되게 512) 최종적으로 512로 증가한다. 렌더링 해상도가 확산 해상도 512보다 낮은 경우, 효율적인 슈퍼-해상도를 위해 RealESRGAN-small(Wang et al., 2021)를 채용한다.\n' +
      '\n' +
      '##4. Mvedit Application and Pipelines\n' +
      '\n' +
      '이 섹션에서는 다양한 MVEdit 파이프라인에 대한 세부 사항을 제시한다. 그들의 각각의 응용 프로그램은 그림 1에 나와 있다. 1, 추론 시간 및 타임스텝 수에 대한 세부 정보를 포함한다. SDEdit과 마찬가지로 이러한 파이프라인의 초기 타임스텝\\(t^{\\text{start}}\\)을 조정할 수 있어 그림 5와 같이 편집 범위를 제어할 수 있다.\n' +
      '\n' +
      '### 3D 합성 파이프라인\n' +
      '\n' +
      '강력한 NeRF/메쉬 최적화 기술을 완전히 활용하는 3D 합성 파이프라인은 물체를 둘러싼 32개의 뷰로 시작한다. 이것들은 9개의 뷰들로 점진적으로 감소되어, 후기 스테이지들에서 멀티뷰 잡음 제거의 계산 비용을 완화하는 데 도움이 된다. NeRF는 항상 초기 3D 표현으로 채택되며 밀도 필드는 60% 완료 시 DMTet 메쉬 표현으로 변환된다. 그런 다음 고유한 입력 양식 및 컨디셔닝 메커니즘으로 다양한 파이프라인 변형을 구성할 수 있다.\n' +
      '\n' +
      '######4.1.1. 문자 안내 3D-to-3D\n' +
      '\n' +
      '입력된 3D 객체가 주어지면, 우리는 무작위로 32개의 주변 카메라를 샘플링하고 NeRF를 초기화하기 위해 초기 멀티뷰 이미지를 렌더링한다. 텍스트 프롬프트에 본질적으로 안정적인 확산이 조건화되어 있으므로 추가 모듈이 필요하지 않습니다.\n' +
      '\n' +
      '###### 4.1.2. 3D-to-3D 지시\n' +
      '\n' +
      'Instruct-NeRF2NeRF(Haque et al., 2023)에 의해 영감을 받아, 우리는 메쉬 기반 Instruct 3D-to-3D 파이프라인을 소개한다. Extra Image-conditioning은 InstructPix2Pix ControlNet(Brooks et al., 2023; Zhang et al., 2023)에 초기 다시점 영상을 공급함으로써 채용된다.\n' +
      '\n' +
      '#### 4.1.3. Image-to-3D\n' +
      '\n' +
      'Zero123++(Shi et al., 2023)를 사용하여 초기 멀티뷰 이미지들을 생성하고, MVEdit는 초기 3D 불일치를 해결함으로써 이러한 뷰들을 고품질 메시로 들어올릴 수 있다. 원래의 외관은 IP-어댑터(Ye et al., 2023) 및 크로스-이미지 어텐션(Alauf et al., 2023; Shi et al., 2023)을 이용한 이미지 컨디셔닝을 통해 보존될 수 있다. Zero123++는 고정된 6개의 뷰 집합만을 생성할 수 있기 때문에, 입력을 미러링하고 생성 과정을 3회 반복함으로써 초기화를 증강하여 총 36개의 이미지를 생성한다. 또한 생성된 뷰에 대한 대응관계를 이용하여 입력 뷰의 포즈를 추정할 수 있어 총 36+1\\의 초기 영상을 얻을 수 있다. 샘플링 프로세스가 시작됨에 따라, 이 수는 32로 감소된다.\n' +
      '\n' +
      '### Re-Texturing Pipelines\n' +
      '\n' +
      '동결된 3D 메시가 주어지면, MVEdit은 처음부터 (랜덤 가우시안 잡음으로 초기화되고) 고품질 텍스처를 생성할 수 있다.\n' +
      '\n' +
      '도 5. **동일한 시드이지만 다른 \\(t^{\\text{start}}\\)을 사용하는 **텍스트-유도 3D-to-3D.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '평가된 방법으로 비용이 많이 드는 사용자 연구에 대한 자동화된 대안을 제공합니다.\n' +
      '\n' +
      '표 1에서, 우리는 GSO 테스트 세트에 대한 One-2-3-45 (Liu et al., 2023), DreamGaussian (Tang et al., 2024), Wonder3D (Long et al., 2024), One-2-3-45+ (Liu et al., 2024) 및 우리의 MVEdit (image-to-3D 및 texture super-resolution를 모두 포함)에 대한 결과를 제시한다. 이 비교는 MVEdit가 모든 메트릭에서 다른 방법을 훨씬 능가하는 동시에 합리적인 런타임을 제공한다는 것을 보여준다. 인-더-와일드 이미지의 경우 SyncDreamer(Liu et al., 2024) 및 DreamCraft3D(Sun et al., 2024)를 포함하도록 비교를 확장한다. 여기에서 GPT-4V는 두 시간 이상의 광범위한 객체 생성 시간에도 불구하고 MVEdit이 SDS 방법 DreamCraft3D보다 높은 Elo 점수를 달성하면서 우리 방법에 대한 뚜렷한 선호도를 보여준다.\n' +
      '\n' +
      '도. 7은 상위 경쟁자들 간의 질적 비교를 추가로 제시한다. Wonder3D(Long et al., 2024)는 InstantNGP 기반 표면 최적화를 위한 멀티뷰 이미지 및 정규 맵을 생성하는데, 이는 멀티뷰 불일치로 인해 파손된 구조를 초래할 수 있다. One-2-3-45++(Liu et al., 2024)는 우리와 동일한 다중 뷰 생성기(즉, Zero123++)를 사용하지만, 표면 추출을 위해 부호 거리 함수(signed distance function, SDF)를 생성하기 위해 다중 뷰-조건 3D-네이티브 확산 모델을 사용하지만, 이는 때때로 누락된 부분들을 갖는 지나치게 매끄러운 표면을 생성한다. DreamCraft3D(Sun et al., 2024)는 몇 시간 동안의 증류를 통해 인상적인 기하학적 세부 사항을 생성할 수 있지만 일반적으로 시끄러운 기하학 및 텍스처, 때로는 강력한 인공물 및 야누스 문제를 생성한다. 대조적으로, 우리의 접근법은 SDS에 비해 기하학에서 덜 상세하지만 일반적으로 더 강력하고 더 적은 인공물 또는 실패를 나타낸다. 이로 인해 시각적으로 더 즐거운 렌더링을 얻을 수 있습니다.\n' +
      '\n' +
      '텍스트 유도 텍스쳐 생성의### 비교\n' +
      '\n' +
      '우리는 Obi-javerse(Deitke et al., 2023)의 고품질 하위 집합에서 92개의 객체를 무작위로 선택하고 BLIP(Li et al., 2022)를 채택했다.\n' +
      '\n' +
      'Figure 7. in-the-wild 영상에서 mesh 기반 영상-to-3D 방법의 비교. 자세히 보려면 확대하십시오.\n' +
      '\n' +
      '렌더링된 이미지에서 텍스트 프롬프트를 생성합니다. 이러한 텍스쳐 없는 메쉬와 이러한 객체들의 생성된 프롬프트를 이용하여 TEXTure[14]와 Text2Tex[15]에 대한 MVEdit 재 텍스쳐링 파이프라인을 평가한다. TextFusion[16]은 공식 코드를 사용할 수 없기 때문에 직접 비교되지는 않지만, 섹션 6.3.1에서 논의될 절제 연구의 시나리오와 매우 유사하다. 우리는 렌더링된 이미지를 통해 생성된 텍스처 메쉬의 품질을 평가하고 미적 [14] 및 CLIP [13, 15] 점수를 메트릭으로 계산한다. [21]의 사용자 연구에서 볼 수 있듯이 미적 점수는 텍스처 세부 사항에 대한 인간의 선호도와 더 밀접하게 일치하는 반면 CLIP 점수는 덜 민감하다는 점에 주목하는 것이 중요하다. 표 2는 MVEdit가 두 메트릭 모두에서 TEXTure 및 Text2Tex를 뚜렷한 마진으로 능가하고 더 빠른 속도로 달성함을 보여준다.\n' +
      '\n' +
      '도. 도 8은 시험된 방법 간의 정량적 비교를 제시한다. TEXTure와 Text2Tex는 모두 약간 과포화 색상을 생성하고 잡음이 많은 아티팩트를 생성한다. 대조적으로, MVEdit는 사실적인 외관과 강한 텍스트-이미지 정렬로 깨끗하고 상세한 질감을 생성한다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      'Skip Connection을 갖는 3D Adapter의 효과\n' +
      '\n' +
      'ControlNet 기반 3D 어댑터의 유효성을 검증하기 위해 ControlNet을 제거하여 절제 연구를 수행하고, Eq. \\(w^{(t)}\\)에 블렌딩 가중치 \\(w^{(t)}\\)를 설정한다. (\\delta\\) 내지 \\(1\\)은 모든 타임스테프에 대해, 그림과 같이 스킵 연결 없이 효과적으로 아키텍처를 구성한다. 2(b). 텍스트 유도 텍스처 생성의 경우, 스킵 연결 없이 샘플링하는 것은 근본적으로 텍스퓨전 [16]과 유사하며, 이는 정보 손실로 인해 더 적은 디테일을 갖는 텍스처를 산출하는 것으로 알려져 있다. 이는 표 2에 제시된 정량적 결과에 의해 확인되며, 이는 미적 점수와 총 변화에서 현저한 감소를 보여준다. 그림 1의 정성적 비교. 8 더\n' +
      '\n' +
      '도 8. **텍스트 유도 텍스처 생성에 대한 비교.** 상세 보기를 위해 확대해주십시오. BLIP 생성 텍스트 프롬프트는 실제 지오메트리를 정확하게 반영하지 않을 수 있으므로 프롬프트와 완벽하게 정렬되는 텍스처 맵을 생성하는 것은 불가능하다는 점에 유의한다.\n' +
      '\n' +
      '그림 9. **다중 뷰 불일치를 해결하는 MVEdit의 효과에 대한 절제 연구.** MVEdit 확산 없이 재구성 전용 접근법은 부서진 얇은 구조와 모호한 질감으로 이어진다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Methods & \\begin{tabular}{c} \\begin{tabular}{c} Img-3D\\(\\uparrow\\) \\\\ Align. \\\\ \\end{tabular} \\\\ \\end{tabular} & 3D Plaus\\(\\uparrow\\) &\n' +
      '\\begin{tabular}{c} Texture\\(\\uparrow\\) \\\\ Details\\(\\uparrow\\) \\\\ \\end{tabular} \\\\ \\hline Ours (MVEdit) & 1340 & 1339 & 1268 \\\\ Ours (Reconstruction-only) & 1275 & 1252 & 1241 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3. **다중 뷰 불일치를 해결하는 MVEdit의 효과에 대한 정량적 절제 연구.**두 아키텍처 간의 시각적 격차를 보여준다. 3D-to-3D 편집을 위해, Fig. 도 3은 스킵 연결이 선명한 텍스처를 생성하는 것뿐만 아니라 기하학적 디테일(예를 들어, 얼룩말의 귀 및 무릎)을 향상시키는 데 중요한 역할을 한다는 것을 보여준다.\n' +
      '\n' +
      '####6.3.2. Image-to-3D: MVEdit vs. 재구성 전용\n' +
      '\n' +
      '본 논문에서 제안한 이미지-투-3D 파이프라인이 Zero123++에 의해 생성된 초기 뷰에서 3D 불일치를 효과적으로 해결한다는 것을 검증하기 위해, 강건한 NeRF/mesh 최적화를 위해 초기 뷰만을 사용하여 노이즈 제거 UNet/DPMSolver를 우회하고 재구성 측면만을 남기는 절제 연구를 수행한다. 정량적으로 표 3의 GPT-4V 평가 결과는 MVEdit와 재구성 전용 방법 사이의 명확한 격차를 드러내어 MVEdit의 효과를 강조한다. 질적으로 그림 9에서 관찰된 바와 같이 재구성 전용 방법은 다중 뷰 정렬의 일반적인 결과인 부서진 얇은 구조와 덜 정의된 질감을 초래하는 경향이 있다.\n' +
      '\n' +
      '정규화 손실함수의 효과\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 11에서는 전체 MVEdit, 광선 엔트로피 손실이 없는 것, 일반 TV 손실이 없는 것의 세 가지 설정 하에서 3D-to-3D 편집을 지시하는 결과를 보여준다. 광선 엔트로피 손실을 제거하면 팽창된 기하 구조와 덜 정의된 텍스처가 생성되며, 퍼지 밀도 필드를 사용하여 DMTet을 초기화한 결과, 정상적인 TV 손실을 제거하면 텍스처 품질에 거의 영향을 미치지 않지만 기하 구조에 수많은 구멍이 발생하는 것을 알 수 있다. 이러한 절제에서 품질의 저하는 특히 3D에서 대화식으로 볼 때 인간에게 분명하지만, 우리는 미적 점수, CLIP 점수 및 GPT-4V 메트릭을 포함한 기존 메트릭이 이러한 차이를 포착하는 데 어려움을 겪는다. 따라서 이러한 절제 연구에 대한 정량적 평가는 포함하지 않는다.\n' +
      '\n' +
      '###3D-to-3D 편집 결과 및 논의\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 10에서는 텍스쳐가 없는 낮은 폴리 메쉬, 이미지-대-3D 파이프라인에서 생성된 메쉬, 복셀 캐릭터 메쉬, 스타일화된 캐릭터 메쉬의 네 가지 입력 유형에서 편집된 텍스트 유도 3D-대-3D 파이프라인 및 지시 3D-대-3D 파이프라인(텍스쳐 초해상도 포함)의 결과를 보여준다. 그림에서 볼 수 있듯이 모든 입력이 능숙하게 처리되어 신속하게 정확한 모양, 복잡한 질감 및 자세한 지오메트리를 생성하여 3D-3D 파이프라인의 범용성을 강조한다.\n' +
      '\n' +
      '### 텍스트-투-3D 생성 결과 및 논의\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 12에서는 StableSSDNeRF와 MVEdit 파이프라인 조합을 이용한 text-to-3D 생성 결과를 제시한다. 덕분에\n' +
      '\n' +
      '그림 11. **절제 연구는 동일한 시드를 사용하여 \\(r^{\\text{start}}=1.0T\\)의 지시 3D-to-3D 파이프라인을 기반으로 하는 정규화 손실 함수**에 대한 연구이다.\n' +
      '\n' +
      '그림 10. **텍스트가 안내하는 3D-to-3D 및 3D-to-3D 파이프라인의 결과**.\n' +
      '\n' +
      '대형 이미지 확산 모델로부터의 지식 전달, StableSSD-NeRF는 2458 ShapeNet 3D Cars의 저해상도 렌더링에서만 미세 조정됨에도 불구하고 보이지 않는 프롬프트를 따를 수 있어 색상과 스타일의 올바른 조합을 생성한다. 특히, 완전히 보이지 않는 개념(_NASCAR_) 또는 특이한 조합(_화학식 1_ 및 _truck_)으로 일반화할 수도 있다. 동일한 입력 프롬프트에 맞춰 텍스트 유도 3D-to-3D 및 재 텍스쳐링 파이프라인을 사용하여 추가 처리했을 때, 본 방법은 단 4분 이내에 다양하고 고품질의 실사 차량을 성공적으로 생산했다.\n' +
      '\n' +
      '### Sample Diversity\n' +
      '\n' +
      '모드 추적 행동을 나타내는 SDS 접근법과 달리 MVEdit는 다른 무작위 시드를 사용하여 정확히 동일한 입력에서 변형을 생성할 수 있다. 예를 들어 도 1에 도시하였다. 13.\n' +
      '\n' +
      '##7. 결론 및 한계\n' +
      '\n' +
      '이 연구에서는 2D 확산 모델을 3D 확산 파이프라인에 적용하기 위한 일반적인 접근 방법인 MVEdit의 도입으로 2D와 3D 콘텐츠 생성 사이의 격차를 좁혔다. 우리의 새로운 훈련 없는 3D 어댑터는 기성 컨트롤넷과 강력한 NeRF/메쉬 최적화 스킴을 활용하여 날카로운 세부 사항을 생성하면서 3D 일관된 다중 뷰 조상 샘플링을 달성하는 문제를 효과적으로 해결한다. 또한, 도메인별 3D 초기화를 위한 StableSSDNeRF를 개발하였다. 다양한 작업에 걸친 광범위한 정량적 및 정성적 평가는 3D 어댑터 설계의 효율성과 관련 파이프라인의 범용성을 검증하여 이미지 대 3D 및 텍스처 생성 작업 모두에서 최첨단 성능을 보여준다.\n' +
      '\n' +
      '도 12. **StableSSDNeRF 및 MVEdit 파이프라인들을 이용한 텍스트-to-3D 생성의 결과들**\n' +
      '\n' +
      '도 13. **는 \\(t^{\\text{start}}=1.0T\\)을 갖는 지시 3D-to-3D 파이프라인에 기초하여 생성된 샘플의 다양성을 보여주는 예이다.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '* Richardson et al. (2023) Elad Richardson, Cal Meteer, Yuval Alalarf, Raja Giryes, and Daniel Cohen-Or. 2023. 텍스쳐: 3d 형상의 텍스쳐링. _SIGGRAPH_에서.\n' +
      '* Bombach et al. (2022) Robin Bombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ominer. 2022. 잠재 확산 모델을 이용한 고해상도 영상 합성. _CVPR_에서.\n' +
      '* Schulmann et al. (2022) Christophu Schulmann, Romanu Beaumont, Richard Veness, Cade Gordon, Ross Weighman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kut Kutindry, Kathtische Crowson, Ludwig Schmidt, Robert Kacarznyczyk, and Jennie Jisev. 2022. IAUCDN-5B: 훈련 및 생성 이미지-텍스트 모델을 위한 개방형 대규모 데이터세트. _NeurIPS Workshop_에서.\n' +
      '* Shen et al.(2021) Tianchang Shen, Jun Gao, Angunge Yin, Ming-Yu Liu, and Sanja Fidler. 2021. 딥 마칭 사면체: 고해상도 및 3차원 형상 합성을 위한 하이브리드 표현. _NeurIPS_에서.\n' +
      '* Shi et al. (2023) Ruwei Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinwei Wei, Linghao Chen, Chong Zeng, and Hao Su. 2023. Zero12++: 단일 이미지 대 일관성 있는 다시점 확산 기반 모델. arXiv:2310.15110\n' +
      '* Shi et al. (2024) Yichun Shi, Peng Wang, Jiangbo Ye, Mai Long, Kejie Li, and Xiao Yang. 2024. MV-Dream: 3D 생성을 위한 다시점 확산. _ICLR_에서.\n' +
      '* Shue et al. (2023) J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, Gordon Weitzstein. 2023. Triplane Diffusion을 이용한 3차원 신경계 생성. _CVPR_에서.\n' +
      '* Stierman et al.(2019) Vincent Stierman, Michael Zollhofer, and Gordon Weitzstein. 2019. 장면 표현 네트워크: 연속 3D 구조 인식 신경망 장면 표현. _NeurIPS_에서.\n' +
      '* Song et al. (2021) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2021. 확률적 미분방정식을 통한 점수 기반 생성 모델링 _ICLR_에서.\n' +
      '* Socrin et al. (2004) Sorin, D. Cohen-Or, Y. 립만 알렉사, C. 로스, 그리고 H.P. 세이델 2004. 라플라시안 표면 편집. 2004 Eurographics/ACM SIGGRAPH Symposium on Geometry Processing_, volume _Faster(SOF\'04)_의 _Proceedings. Association for Computing Machinery, New York, NY, USA, 175-184. [https://doi.org/10.1145/105732.1057456](https://doi.org/10.1145/105732.1057456)\n' +
      '* Sun et al. (2024) Jingxiang Sun, Bo Zhang, Ruhti Shaho, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. 2024. DreamcarM3d: 부트스트랩된 확산을 갖는 계층적 3d 세대 이전. _ICLR_에서.\n' +
      '* Tang et al. (2024) Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. 2024. Dream-Gaussian:Generative Gaussian Splitting for Efficient 3D Content Creation _ICLR_에서.\n' +
      '* Teswar et al. (2023) Ayush Teswar, Tianwei Yin, George Cazenavette, Semen Rezchikov, Joshua B. Tenenbaum, Fredo Durand, William T. 프리먼, 빈센트 시츠만 2023. Forward Models과의 확산: 직접 감독 없이 확률적 역문제 해결. _NeurIPS_에서.\n' +
      '* Wang et al. (2021) Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. 2021a. Neus: Multi-view Reconstruction을 위한 볼륨 렌더링을 통한 Neural Implicit Surface 학습 _NeurIPS_에서.\n' +
      '* Wang et al. (2022) Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingting Shen, Dong Chen, Fang Wen, Qifeng Chen, and Baining Ging. 2023. Radio: Diffusion을 이용한 D3 디지털 아바타의 Solupping 생성 모델 _CVPR_에서.\n' +
      '* Wang et al. (2021) Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. 2021b. Real-ESRGAN: 순수한 합성 데이터로 실세계 블라인드 슈퍼 해상도를 훈련합니다. _ICCV Workshop_에서.\n' +
      '* Wang et al. (2022a) Zhenyi Wang, Cheng Liu, Yixian Wang, Fan Bao, Chongzuan Li, Hang Su, and Jun Zhu. 2022a. ProfileDreamer: Variational Score Distillation을 이용한 높은 충실도와 다양한 Text-to-3D Generation. _NeurIPS_에서.\n' +
      '* Watson et al.(2023) Daniel Watson, William Chan, Ricardo Martin-Brualis, Jonathan Ho, Andrea Taglascachi, and Mohammad Norouzi. 2023. 확산 모델을 사용한 새로운 뷰 합성. _ICLR_에서.\n' +
      '* Wu et al. (2024) Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and Gordon Weitzstein. 2024. GPT-IV(vision)는 Text-to-3D 생성을 위한 Human-Aligned Evaluator이다. _CVPR_에서.\n' +
      '* Xu et al. (2024) Yinghao Xu, Hao Tan, Fujun Luan, Si Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sankarski, Gordon Wettstein, Zexiang Xu, and Kai Zhang. 2024. DMV3D: 3D Large Reconstruction Model을 이용한 Denoising Multi-View Diffusion _ICLR_에서.\n' +
      '* Yu et al. (2023) He, Ju2 Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. IP-적응: 텍스트 대 이미지 확산 모델을 위한 텍스트 호환 이미지 프롬프트 어댑터. arXiv:2308.06721\n' +
      '* Zhang et al. (2023) Lvmin Zhang, Anyi Rao, and Maeness Agrawala. 2023. 텍스트-이미지 확산 모델에 조건 제어를 추가한다. _ICCV_에서.\n' +
      '* Zhang et al. (2018) Richard Zhang, Phillio Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The Unasonable Effectiveness of Deep Feature as a Perceptual Metric. _CVPR_에서.\n' +
      '* Zheng et al. (2023) Xin-Yang Zheng, Hao Pan, Peng-Shui Wang, Xin Tong, Yang Liu, and Heung-Yeung Shum. 2023. 제어 가능한 3차원 형상 생성을 위한 국부적 주의력 SDF 확산_ ACM Transactions on Graphics_42, 4 (2023).\n' +
      '* Zhou and Tulsiani (2023) Zhirhou Zhou and Shububububub Tulsiani. 2023. SparsePusion: Distilling View-conditioned Diffusion for 3D reconstruction. _CVPR_에서.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>