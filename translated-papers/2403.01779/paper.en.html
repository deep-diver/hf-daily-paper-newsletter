<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on\n' +
      '\n' +
      'Yuhao Xu\n' +
      '\n' +
      'Xiao-i Research\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Tao Gu\n' +
      '\n' +
      'Xiao-i Research\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Weifeng Chen\n' +
      '\n' +
      'Xiao-i Research\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Chengai Chen\n' +
      '\n' +
      'Xiao-i Research\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Footnote 1: email: {yuhao.xu,tao.gu,weifeng.chen,arlenec}@xiaoi.com\n' +
      '\n' +
      'Xiao-i Research\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Footnote 2: email: {yuhao.xu,tao.gu,weifeng.chen,arlenec}@xiaoi.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Image-based virtual try-on (VION), which aims to generate an outfitted image of a target human wearing an in-shop garment, is a challenging image-synthesis task calling for not only high fidelity of the outfitted human but also full preservation of garment details. To tackle this issue, we propose Outfitting over Try-on Diffusion (OOTDiffusion), leveraging the power of pretrained latent diffusion models and designing a novel network architecture for realistic and controllable virtual try-on. Without an explicit warping process, we propose an outfitting UNet to learn the garment detail features, and merge them with the target human body via our proposed outfitting fusion in the denoising process of diffusion models. In order to further enhance the controllability of our outfitting UNet, we introduce outfitting dropout to the training process, which enables us to adjust the strength of garment features through classifier-free guidance. Our comprehensive experiments on the VITON-HD and Dress Code datasets demonstrate that OOTDiffusion efficiently generates high-quality outfitted images for arbitrary human and garment images, which outperforms other VTON methods in both fidelity and controllability, indicating an impressive breakthrough in virtual try-on. Our source code is available at [https://github.com/levihsu/OOTDiffusion](https://github.com/levihsu/OOTDiffusion).\n' +
      '\n' +
      'Keywords:Virtual try-on Latent diffusion Outfitting fusion\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Image-based virtual try-on (VTON) is a popular and promising image-synthesis technology for the current e-commerce industry, which is able to dramatically improve the shopping experience of consumers and reduce the advertising cost of clothing merchants. As its name suggests, the objective of the VTON task is to generate an image of a target human wearing a given garment, which took tremendous efforts from numerous researchers [15, 16, 17, 12, 17, 24, 30, 49, 52, 54] for more natural and accurate virtual try-on results in the past few years.\n' +
      '\n' +
      'Image-based VTON is currently facing two main challenges. First, the generated images should be realistic and natural enough to avoid dissonance. Most of recent researches on virtual try-on leverage generative adversarial networks [13] (GANs) or latent diffusion models [40] (LDMs) for image generation. Previous GAN-based methods [15, 16, 17, 52, 6, 27] usually have difficulty in generating correct garment folds, natural light and shadow, and realistic human body, and are prone to overfitting. Hence more recent work favors LDM-based methods [14, 24, 32, 60], which effectively enhance the fidelity of outfitted images. The second critical challenge is how to preserve as much as possible the detail features of garments, such as complicated text, textures, colors, patterns and lines, etc. Previous researches [14, 27, 32, 52, 6] usually perform an explicit warping process to align the garment features with the target human body, and then feed the warped garment into generative models (i.e., GANs and LDMs, etc.). Thus the performance of such VTON methods is extremely dependent on the efficacy of the lossy warping process. On the other hand, some LDM-based methods [14, 24, 32] attempt to learn garment features via CLIP textual-inversion [10], which fail to preserve fine-grained garment details.\n' +
      '\n' +
      'Motivated by the aforementioned prospects and challenges of image-based VTON, we propose a novel LDM-based virtual try-on method, namely Outfitting over Try-on Diffusion (OOTDiffusion; see Fig. 2). First, we make full use of the advantages of pretrained latent diffusion models [40] to ensure high fidelity of generated images and natural try-on effects. Then we introduce a novel outfitting UNet to learn the precise garment features in the latent space, and merge them with the noisy human body in the denoising UNet through an outfitting fusion process in the self-attention layers [48]. In this way, the garment features are smoothly adapted to various target human body types and postures, without suffering information loss or feature distortion caused by warping. Meanwhile, we also conduct CLIP textual-inversion [10] for the garment image as the auxiliary semantic information fed into both outfitting and denoising UNets via the cross-attention mechanism [48]. Furthermore, we perform an outfitting dropout process, which randomly drops a handful of garment features in training to enable classifier-free guidance [20] with respect to the garment features. Through this approach, the strength of garment control over the generated result can be simply adjusted by a guidance scale, which further enhances the controllability of our VTON method.\n' +
      '\n' +
      'Our contributions are summarized as follows:\n' +
      '\n' +
      '* We propose OOTDiffusion, a novel LDM-based network architecture for realistic and controllable virtual try-on.\n' +
      '* We design an outfitting UNet to learn the precise garment features and align them with the target human body via our outfitting fusion in the self-attention layers without an explicit warping process.\n' +
      '* We introduce outfitting dropout to the training process, which further improves the controllability of our outfitting UNet.\n' +
      '* We train our OOTDiffusion on two broadly-used high-resolution benchmark datasets, i.e., VITON-HD [6] and Dress Code [33], respectively. Extensive qualitative and quantitative evaluations demonstrate our superiority over state-of-the-art VTON methods in both fidelity and controllability for various target human and garment images (see Fig. 1), which implies an impressive breakthrough in image-based virtual try-on.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '**Image-based Virtual Try-on.** Image-based virtual try-on has been investigated for many years as a promising and challenging task [6, 9, 14, 22, 24, 27, 32, 49, 52, 60]. Aiming at more natural and accurate results, recent researches are mainly based on generative adversarial networks [13] (GANs) or latent diffusion models [40] (LDMs) for image generation. Among the GAN-based VTON methods [6, 27, 52], VITON-HD [6] collected a high-resolution dataset and proposed ALIAS normalization and generator to address the misalignment between warped clothes and target regions. HR-VITON [27] simultaneously performed warping and segmentation to handle the body occlusion and garment misalignment. GP-VTON [52] proposed a LFGP warping module to generate deformed garments and introduced a DGT training strategy for the warping network. As introduced above, GAN-based methods usually rely on an explicit warping process neglecting realistic garment folds and natural light and shadow, which seriously degrades the fidelity and realism of outfitted images. Meanwhile, GAN-based methods are prune to overfitting and causing severe performance degradation on out-of-distribution images.\n' +
      '\n' +
      'With respect to the LDM-based approaches [14, 24, 32], LaDI-VTON [32] and DCI-VTON [14] also require an explicit warping process. In specific, LaDI-VTON [32] performed textual-inversion to map the visual garment features to the CLIP [39] token embedding space and condition the latent diffusion model along with the warped input. DCI-VTON [14] directly combined the warped clothes with the masked person image to get a coarse result, and then refined it by the diffusion model. Neither of these methods succeeded in fully preserving garment details like complicated pattern and text due to the information loss caused by the CLIP encoder. More recently, StableVITON [24] discarded independent warping and proposed a zero cross-attention block to learn semantic correlation between the clothes and human body. However, information loss still exists in cross-attention and the extra zero-initialized blocks heavily increase its training and inference cost. In contrast, again without warping, our LDM-based OOTDiffusion finetunes the pretrained outfitting UNet to learn garment details in one step and efficiently merges them into the denoising UNet via our outfitting fusion with negligible information loss.\n' +
      '\n' +
      '#### 2.0.2 LDM-based Controllable Image Generation.\n' +
      '\n' +
      'Latent diffusion models [40] have achieved great success in text-to-image [1, 26, 42, 38, 44] and image-to-image [23, 36, 43, 45, 47] generation in recent years. For the purpose of more controllable generated results, Prompt-to-Prompt [18] and Null-text Inversion [31] controlled the cross-attention layers to finely edit images by modifying the input captions without extra model training. InstructPix2Pix [3] created paired data to train diffusion models that generate the edited image given an input image and a text instruction. Paint-by-Example [53] trained image-conditioned diffusion models in a self-supervised manner to offer fine-grained image control. ControlNet [57] and T2I-Adapter [34] incorporated additional blocks into pre-trained diffusion models to enable spatial conditioning controls. IP-Adapter [55] adopted a decoupled cross-attention mechanism for text and image features to enable controllable generation with image prompt and additional structural conditions. In this paper, we focus on the image-based VTON task, employing outfitting fusion in the self-attention layers of the denoising UNet and performing outfitting dropout at training time to enable latent diffusion models to generate more controllable outfitted images with respect to the garment features.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      '#### 3.1.1 Stable Diffusion.\n' +
      '\n' +
      'Our OOTDiffusion is an extension of Stable Diffusion [40], which is one of the most commonly-used latent diffusion models. Stable Diffusion employs a variational autoencoder [25] (VAE) that consists of an encoder \\(\\mathcal{E}\\) and a decoder \\(\\mathcal{D}\\) to enable image representations in the latent space. And a UNet [41]\\(\\epsilon_{\\theta}\\) is trained to denoise a Gaussian noise \\(\\epsilon\\) with a conditioning input encoded by a CLIP text encoder [39]\\(\\tau_{\\theta}\\). Given an image \\(\\mathbf{x}\\) and a text prompt \\(\\mathbf{y}\\), the training of the denoising UNet \\(\\epsilon_{\\theta}\\) is performed by minimizing the following loss function:\n' +
      '\n' +
      '\\[\\mathcal{L}_{LDM}=\\mathbb{E}_{\\mathcal{E}(\\mathbf{x}),\\mathbf{y},\\epsilon \\sim\\mathcal{N}(0,1),t}\\left[\\|\\epsilon-\\epsilon_{\\theta}(\\mathbf{z}_{t},t, \\tau_{\\theta}(\\mathbf{y}))\\|_{2}^{2}\\right], \\tag{1}\\]\n' +
      '\n' +
      'where \\(t\\in\\{1,...,T\\}\\) denotes the time step of the forward diffusion process, and \\(\\mathbf{z}_{t}\\) is the encoded image \\(\\mathcal{E}(\\mathbf{x})\\) with the added Gaussian noise \\(\\epsilon\\sim\\mathcal{N}(0,1)\\) (i.e., the noise latent). Note that the conditioning input \\(\\tau_{\\theta}(\\mathbf{y})\\) is correlated with the denoising UNet by the cross-attention mechanism [48].\n' +
      '\n' +
      '### OOTDiffusion\n' +
      '\n' +
      '**Overview.** Fig. 2 illustrates the overview of our method. Given a target human image \\(\\mathbf{x}\\in\\mathbb{R}^{3\\times H\\times W}\\) and an input garment image \\(\\mathbf{g}\\in\\mathbb{R}^{3\\times H\\times W}\\), our OOTDiffusion is capable of generating a realistic outfitted image \\(\\mathbf{x}_{\\mathbf{g}}\\in\\mathbb{R}^{3\\times H\\times W}\\). We employ OpenPose [4, 5, 46, 51] and HumanParsing [28] to generate a masked human image \\(\\mathbf{x}_{\\mathbf{m}}\\in\\mathbb{R}^{3\\times H\\times W}\\), and use a VAE encoder \\(\\mathcal{E}\\) to transform it into the latent space as \\(\\mathcal{E}(\\mathbf{x}_{\\mathbf{m}})\\in\\mathbb{R}^{4\\times h\\times w}\\), where \\(h=\\frac{H}{8}\\) and \\(w=\\frac{W}{8}\\). Then we concate\n' +
      '\n' +
      'Figure 2: Overview of our proposed OOTDiffusion model. On the left side, the garment image is encoded into the latent space and fed into the outfitting UNet for a single step process. Along with the auxiliary conditioning input generated by CLIP encoders, the garment features are incorporated into the denoising UNet via outfitting fusion. Outfitting dropout is performed for the garment latents particularly in training to enable classifier-free guidance. On the right side, the input human image is masked with respect to the target region and concatenated with a Gaussian noise as the input to the denoising UNet for multiple sampling steps. After denoising, the feature map is decoded back into the image space as our try-on result.\n' +
      '\n' +
      'nate \\(\\mathcal{E}(\\mathbf{x_{m}})\\) with a Gaussian noise \\(\\epsilon\\in\\mathbb{R}^{4\\times h\\times w}\\) as the input latent \\(\\mathbf{z}_{T}\\in\\mathbb{R}^{8\\times h\\times w}\\) for the denoising UNet. Note that we add 4 zero-initialized channels to the first convolutional layer of the denoising UNet to support our input with 8 channels.\n' +
      '\n' +
      'On the other side, we feed the encoded garment latent \\(\\mathcal{E}(\\mathbf{g})\\in\\mathbb{R}^{4\\times h\\times w}\\) into an (i) outfitting UNet to learn the garment features in a single step, and integrate them into the denoising UNet via our (ii) outfitting fusion. And we perform (iii) outfitting dropout for \\(\\mathcal{E}(\\mathbf{g})\\) particularly in the training process. In addition, we also conduct CLIP textual-inversion [10] for the garment image \\(\\mathbf{g}\\), and optionally concatenate it with a text embedding of the garment label \\(\\mathbf{y}\\in\\{``upperbody",``lowerbody",``dress"\\}\\) as an auxiliary conditioning input, which is fed into both outfitting and denoising UNets via the cross-attention mechanism [48]. Finally, after multiple steps of the denoising process, we use a VAE decoder \\(\\mathcal{D}\\) to transform the denoised latent \\(\\mathbf{z}_{0}\\in\\mathbb{R}^{4\\times h\\times w}\\) back into the image space as the output image \\(\\mathbf{x_{g}}=\\mathcal{D}(\\mathbf{z}_{0})\\in\\mathbb{R}^{3\\times H\\times W}\\). We will elaborate the key technologies (i.e., (i) outfitting UNet, (ii) outfitting fusion, and (iii) outfitting dropout) of our OOTDiffusion in the following sections.\n' +
      '\n' +
      '#### 3.1.2 Outfitting UNet.\n' +
      '\n' +
      'As introduced above, we propose an outfitting UNet to efficiently learn the detail features of the garment image \\(\\mathbf{g}\\). The left side of Fig. 2 shows the architecture of our outfitting UNet, which is essentially identical to the denoising UNet of Stable Diffusion. The encoded garment latent \\(\\mathcal{E}(\\mathbf{g})\\in\\mathbb{R}^{4\\times h\\times w}\\) is fed into the outfitting UNet \\(\\omega_{\\theta^{\\prime}}\\), and then incoporated into the denoising UNet \\(\\epsilon_{\\theta}\\) via our outfitting fusion (see the next section). Along with the aforementioned auxiliary conditioning input, the outfitting and denoising UNets are jointly trained by minimizing the following loss function:\n' +
      '\n' +
      '\\[\\mathcal{L}_{OOTD}=\\mathbb{E}_{\\mathcal{E}(\\mathbf{x_{m}}),\\mathcal{E}( \\mathbf{g}),\\psi,\\epsilon\\sim\\mathcal{N}(0,1),t}\\left[\\|\\epsilon-\\epsilon_{ \\theta}(\\mathbf{z}_{t},t,\\omega_{\\theta^{\\prime}}(\\mathcal{E}(\\mathbf{g}), \\psi),\\psi)\\|_{2}^{2}\\right], \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\psi=\\tau_{g}(\\mathbf{g})\\) (c) \\(\\tau_{y}(\\mathbf{y})\\) represents the auxiliary conditioning input for both \\(\\omega_{\\theta^{\\prime}}\\) and \\(\\epsilon_{\\theta}\\). While \\(\\tau_{g}\\) and \\(\\tau_{y}\\) refer to the pretrained CLIP image encoder and text encoder respectively, and (c) denotes concatenation.\n' +
      '\n' +
      'In practice, we directly duplicate the pretrained UNet weights of Stable Diffusion [40] for the initialization of both our outfitting and denoising UNets (except for the zero-initialized channels added to the first convolutional layer), and jointly finetune them on the high-resolution VTON datasets [6, 33]. Note that \\(\\omega_{\\theta^{\\prime}}\\) and \\(\\epsilon_{\\theta}\\) do not share any weights in the training process. We claim that our tactical utilization of the pretrained models dramatically improves the training efficiency and reduces the training cost. Moreover, compared with the denoising UNet, a significant difference in our outfitting UNet is that it requires only one step forward process before the multiple denoising steps in inference, causing a minimal amount of extra computational cost to the original Stable Diffusion [40].\n' +
      '\n' +
      '#### 3.1.3 Outfitting Fusion.\n' +
      '\n' +
      'Based on our proposed outfitting UNet and inspired by the spatial-attention mechanism [21, 48], we introduce an outfitting fusion process to incorporate the learned garment features into the denoising UNet. First, we tive into the transformer blocks [48] of two UNets, finding each pair of feature maps used as input to the corresponding self-attention layers [48]. Given the \\(n\\)th pair of feature maps \\(\\mathbf{g}_{n},\\mathbf{x}_{n}\\in\\mathbb{R}^{c_{n}\\times h_{n}\\times w_{n}}\\), we concatenate them in the spatial domain as:\n' +
      '\n' +
      '\\[\\mathbf{x}_{\\mathbf{g}_{n}}=\\mathbf{x}_{n}\\text{ \\textcircled{C}}\\mathbf{g}_{n}\\in \\mathbb{R}^{c_{n}\\times h_{n}\\times 2w_{n}}. \\tag{3}\\]\n' +
      '\n' +
      'Then we replace \\(\\mathbf{x}_{n}\\) with the concatenated feature map \\(\\mathbf{x}_{\\mathbf{g}_{n}}\\) as the input to the self-attention layer of the denoising UNet. After the self-attention process, we crop out the fist half of the feature map as the output of the self-attention layer. Through this approach, the garment features are effectively correlated with the noisy human body with negligible information loss. Hence the denoising UNet is made capable of learning the precise features from the outfitting UNet for preserving garment details and naturally adapting them to the target human body in the generated image.\n' +
      '\n' +
      '#### 3.3.2 Outfitting Dropout.\n' +
      '\n' +
      'In order to further enhance the controllability of our VTON method, we employ an outfitting dropout operation in training to enable classifier-free guidance [20] for the garment features. Classifier-free guidance has been broadly used in conditional image generation [3; 35; 44; 56] for trading off the quality and diversity of images generated by latent diffusion models. Specifically in the training process of our outfitting UNet, we randomly drop the input garment latent as \\(\\mathcal{E}(\\mathbf{g})=\\varnothing\\), where \\(\\varnothing\\in\\mathbb{R}^{4\\times h\\times w}\\) refers to an all-zero latent. In this way, the denoising UNet is trained both conditionally and unconditionally, i.e., with and without the outfitting fusion. Then at inference time, we simply use a guidance scale \\(s_{\\mathbf{g}}\\geq 1\\) to adjust the strength of conditional control over the predicted noise \\(\\hat{\\epsilon}_{\\theta}\\) as:\n' +
      '\n' +
      '\\[\\hat{\\epsilon}_{\\theta}(\\mathbf{z}_{t},\\omega_{\\theta^{\\prime}}(\\mathcal{E}( \\mathbf{g})))=\\epsilon_{\\theta}(\\mathbf{z}_{t},\\varnothing)+s_{\\mathbf{g}} \\cdot(\\epsilon_{\\theta}(\\mathbf{z}_{t},\\omega_{\\theta^{\\prime}}(\\mathcal{E}( \\mathbf{g})))-\\epsilon_{\\theta}(\\mathbf{z}_{t},\\varnothing)), \\tag{4}\\]\n' +
      '\n' +
      'where we omit some minor terms compared with Eq. (2) for the sake of brevity.\n' +
      '\n' +
      'In practice, we empirically set the outfitting dropout ratio to 10% in training, i.e., 10% of garment latents \\(\\mathcal{E}(\\mathbf{g})\\) are set to \\(\\varnothing\\). And the optimal value of the guidance scale \\(s_{\\mathbf{g}}\\) is usually around \\(1.5\\sim 2.0\\) according to our ablation study (see Sec. 4.3). Fig. 3 and Tab. 1 demonstrate the effects of our outfitting dropout and different guidance scale values.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '#### 4.1.1 Datasets.\n' +
      '\n' +
      'Our experiments are performed on two high-resolution (\\(1024\\times 768\\)) virtual try-on datasets, i.e., VITON-HD [6] and Dress Code [33]. The VITON-HD dataset consists of 13,679 image pairs of frontal half-body models and corresponding upper-body garments, where 2032 pairs are used as the test set. The Dress Code dataset consists of 15,363/8,951/2,947 image pairs of full-body models and corresponding upper-body garments/lower-body garments/dresses, where 1,800 pairs for each garment category are used as the test set.\n' +
      '\n' +
      '**Compared methods.** On the VITON-HD dataset [6], we compare our OOTDiffusion with multiple state-of-the-art VTON methods, including the GAN-based VITON-HD [6], HR-VITON [27] and GP-VTON [52], as well as the LDM-based LaDI-VTON [32] and StableVITON [24].\n' +
      '\n' +
      'While for the evaluation on the Dress Code dataset [33], since VITON-HD [6], HR-VITON [27] and StableVITON [24] are not designed for the entire dataset beyond upper-body garments, we select two VTON methods (i.e., GP-VTON [52] and LaDI-VTON [32]) and another LDM-based inpainting method (i.e., Paint-by-Example [53]) for fair comparison.\n' +
      '\n' +
      '**Evaluation Metrics.** We evaluate the results in both the paired and unpaired settings, where the paired setting provides the target human and the corresponding garment images for reconstruction, and the unpaired setting provides the different garment images for virtual try-on. Specifically for Dress Code [33], we note that the evaluation are performed on the entire dataset rather than being limited to upper-body garments, which more effectively validates the feasibility of each method in real-world applications with various garment types. In addition, we conduct cross-dataset evaluation, in which the models are trained on one dataset and evaluated on the other, to further investigate the generalization ability of each method.\n' +
      '\n' +
      'In the quantitative evaluation, though our OOTDiffusion supports higher-resolution (\\(1024\\times 768\\)) virtual try-on, all the experiments are conducted at the resolution of \\(512\\times 384\\) for fair comparison with previous VTON methods. For the paired setting, we use LPIPS [58] and SSIM [50] to measure the quality of the generated image in terms of restoring the original image. For the unpaired setting, we employ FID [19] and KID [2] for realism and fidelity assessment. We follow the previous work [7, 32, 37] to implement all of these metrics.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'In our experiments, we initialize the OOTDiffusion models by inheriting the pretrained weights of Stable Diffusion v1.5 [40]. Then we finetune the outfitting and denoising UNets using an AdamW optimizer [29] with a fixed learning rate of 5e-5. Note that we train four types of models on VITON-HD [6] and Dress Code [33] datasets at resolutions of \\(512\\times 384\\) and \\(1024\\times 768\\), separately. All the models are trained for 36000 iterations on a single NVIDIA A100 GPU, with a batch size of 64 for the \\(512\\times 384\\) resolution and 16 for the \\(1024\\times 768\\) resolution. At inference time, we run our OOTDiffusion on a single NVIDIA RTX 4090 GPU for 20 sampling steps using the UniPC sampler [59].\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'We investigate the effects of our proposed outfitting dropout as well as the different values of the guidance scale \\(s_{\\mathbf{g}}\\) on the VITON-HD dataset [6]. First, we train two variants of our OOTDiffusion models without/with outfitting dropout,respectively. Then for the model trained with outfitting dropout, we set \\(s_{\\mathbf{g}}=1.0,1.5,2.0,2.5,3.0,5.0\\) for classifier-free guidance. At inference time, we guarantee all of other parameters (including the random seed) are consistent for fair comparison. As Fig. 3 shows, without outfitting dropout, classifier-free guidance is not supported and the generated result is obviously the worst. While for the model trained with outfitting dropout, when \\(s_{\\mathbf{g}}=1.0\\), the inference process is\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c c} \\hline \\hline\n' +
      '**Outfitting** & **Guidance** & \\multirow{2}{*}{**LPIPS \\(\\downarrow\\) SSIM \\(\\uparrow\\) FID \\(\\downarrow\\) KID \\(\\downarrow\\)**} \\\\\n' +
      '**Dropout** & **Scale** & & & & \\\\ \\hline � & - & 0.0750 & 0.8699 & 8.91 & 0.89 \\\\ ✓ & 1.0 & 0.0749 & 0.8705 & 8.99 & 0.89 \\\\ ✓ & 1.5 & **0.0705** & **0.8775** & 8.81 & **0.82** \\\\ ✓ & 2.0 & 0.0708 & 0.8766 & **8.80** & 0.86 \\\\ ✓ & 2.5 & 0.0746 & 0.8691 & 8.84 & 0.89 \\\\ ✓ & 3.0 & 0.0753 & 0.8684 & 8.95 & 0.96 \\\\ ✓ & 5.0 & 0.0788 & 0.8640 & 9.28 & 1.22 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Ablation study of outfitting dropout and different guidance scale values on the VITON-HD dataset. The best and second best results are reported in **bold** and underline, respectively.\n' +
      '\n' +
      'Figure 3: Qualitative comparison of outfitted images generated by OOTDiffusion models trained without/with outfitting dropout and using different values of the guidance scale \\(s_{\\mathbf{g}}\\). Please zoom in for more details.\n' +
      '\n' +
      'identical to the model without outfitting dropout (see Eq. (4)), which gets a similarly bad result. When \\(s_{\\mathbf{g}}>1.0\\), we see that the fine-grained garment features become clearer as \\(s_{\\mathbf{g}}\\) increases. However, color distortion occurs when \\(s_{\\mathbf{g}}\\geq 2.5\\) and becomes extremely significant when \\(s_{\\mathbf{g}}=5.0\\). Furthermore, Tab. 1 quantitatively proves the efficacy of our outfitting dropout which enables classifier-free guidance with respect to the garment features, and find the optimal guidance scale value is around \\(1.5\\sim 2.0\\) in most cases. According to this study, we consistently conduct outfitting dropout for OOTDiffusion, and empirically set \\(s_{\\mathbf{g}}=1.5\\) for the VITON-HD dataset [6] and \\(s_{\\mathbf{g}}=2.0\\) for the Dress Code dataset [33] in the following experiments.\n' +
      '\n' +
      '### Experimental Results\n' +
      '\n' +
      '**Qualitative Results.** Fig. 4 visually shows some example results of our method and other VTON methods on the test set of VITON-HD [6]. We observe that compared with other methods, our OOTDiffusion consistently achieves the best\n' +
      '\n' +
      'Figure 4: Qualitative comparison on the VITON-HD dataset [6] (half-body models with upper-body garments). Please zoom in for more details.\n' +
      '\n' +
      'try-on effect for various upper-body garments. More specifically, GAN-based methods like GP-VTON [52] often fail to generate realistic human bodies (1st and 4th rows) and natural garment folds (2nd and 3rd rows), which makes the outfitted images look unrealistic. While other LDM-based methods like LaDI-VTON [32] and StableVITON [24] tend to lose some garment details such as the 2nd and 3rd rows, respectively.\n' +
      '\n' +
      'Figure 5: Qualitative comparison on the Dress Code dataset [33] (full-body models with upper-body garments/lower-body garments/dresses). Please zoom in for more details.\n' +
      '\n' +
      'complicated text (2nd and 4th rows) and patterns (1st and 3rd rows). In contrast, our OOTDiffusion not only generates realistic images but also preserves most of the garment details.\n' +
      '\n' +
      'Regarding the more complicated Dress Code dataset [33], which consists of full-body models and various garment categories, our OOTDiffusion still visually outperforms other VTON methods. As illustrated in Fig. 5, Paint-by-Example [53] and LaDI-VTON [32] fail to preserve the garment features, and GP-VTON [52] tends to cause severe body and background distortion. On the contrary, our OOTDiffusion consistently shows very stable performance on different garment categories including upper-body garments (1st row), lower-body garments (2nd row) and dresses (3rd and 4th rows).\n' +
      '\n' +
      'In order to evaluate the generalization ability of our method, we conduct an additional cross-dataset experiment, i.e., training on one dataset and testing on the other. Fig. 6 demonstrates that among all the models trained on the VITON-HD dataset [6], our OOTDiffusion is optimally adapted to the test examples in the Dress Code dataset [33], generating more realistic outfitted images and preserving much more garment details. In summary, the observations above (Figs. 4 to 6) qualitatively prove the superiority and generalization capability of our OOTDiffusion in generating natural and accurate outfitted results for various human and garment images.\n' +
      '\n' +
      'Figure 6: Qualitative results of the cross-dataset evaluation. The models are trained on the VITON-HD dataset [6] and tested on the Dress Code dataset [33]. Please zoom in for more details.\n' +
      '\n' +
      'Quantitative Results.Tab. 2 presents the quantitative evaluation results on the VITON-HD dataset [6]. We observe that some GAN-based models like HR-VITON [27] and GP-VTON [52] achieve relatively high SSIM scores, indicating that they are able to retain the structural information of the original images. However, their generated images lack detail fidelity, and thus drop behind ours on LPIPS. The previous LDM-based methods including LaDI-VTON [32] and StableVITON [24] generate more realistic images according to their FID and KID scores, but they fail to restore the detail features due to their lossy feature\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Train/Test**} & \\multicolumn{3}{c}{**VITON-HD/Dress Code**} & \\multicolumn{3}{c}{**Dress Code/VITON-HD**} \\\\ \\cline{2-9}  & **LPIPS \\(\\downarrow\\)** & **SSIM \\(\\uparrow\\)** & **FID \\(\\downarrow\\)** & **KID \\(\\downarrow\\)** & **LPIPS \\(\\downarrow\\)** & **SSIM \\(\\uparrow\\)** & **FID \\(\\downarrow\\)** & **KID \\(\\downarrow\\)** \\\\ \\hline VITON-HD* [6] & 0.187 & 0.853 & 44.26 & 28.82 & - & - & - & - \\\\ HR-VITON* [27] & 0.108 & 0.909 & 19.97 & 7.35 & - & - & - & - \\\\ LaDI-VTON [32] & 0.154 & 0.908 & 14.58 & 3.59 & 0.235 & 0.812 & 29.66 & 20.58 \\\\ GP-VTON [52] & 0.291 & 0.820 & 74.36 & 80.49 & 0.266 & 0.811 & 52.69 & 49.14 \\\\ StableVITON [24] & 0.065 & 0.914 & 13.18 & 2.26 & - & - & - & - \\\\ \\hline\n' +
      '**OOTDiffusion (Ours)** & **0.061** & **0.915** & **11.96** & **1.21** & **0.123** & **0.839** & **11.22** & **2.72** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Quantitative results of the cross-dataset evaluation. Each model is trained on one of the VITON-HD [6] and Dress Code [33] datasets and evaluated on the other. The best and second best results are reported in **bold** and underline, respectively. The * marker refers to the results reported in previous works.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & **LPIPS \\(\\downarrow\\)** & **SSIM \\(\\uparrow\\)** & **FID \\(\\downarrow\\)** & **KID \\(\\downarrow\\)** \\\\ \\hline VITON-HD [6] & 0.116 & 0.863 & 12.13 & 3.22 \\\\ HR-VITON [27] & 0.097 & 0.878 & 12.30 & 3.82 \\\\ LaDI-VTON [32] & 0.091 & 0.875 & 9.31 & 1.53 \\\\ GP-VTON [52] & 0.083 & **0.892** & 9.17 & 0.93 \\\\ StableVITON [24] & 0.084 & 0.862 & 9.13 & 1.20 \\\\ \\hline\n' +
      '**OOTDiffusion (Ours)** & **0.071** & 0.878 & **8.81** & **0.82** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Quantitative results on the VITON-HD dataset [6]. The best and second best results are reported in **bold** and underline, respectively.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multicolumn{3}{c}{**All**} & \\multicolumn{3}{c}{**Upper-body**} & \\multicolumn{3}{c}{**Lower-body**} & \\multicolumn{3}{c}{**Dresses**} \\\\ \\cline{2-9}  & **LPIPS \\(\\downarrow\\)** & **SSIM \\(\\uparrow\\)** & **FID \\(\\downarrow\\)** & **KID \\(\\downarrow\\)** & **FID \\(\\downarrow\\)** & **KID \\(\\downarrow\\)** & **FID \\(\\downarrow\\)** & **KID \\(\\downarrow\\)** \\\\ \\hline PSAD* [33] & 0.058 & 0.918 & 10.61 & 6.17 & 17.51 & 7.15 & 19.68 & 8.90 & 17.07 & 6.66 \\\\ Paint-by-Example [53] & 0.142 & 0.851 & 9.57 & 3.63 & 18.63 & 4.81 & 15.89 & 4.12 & 19.15 & 5.88 \\\\ LaDI-VTON [32] & 0.067 & 0.910 & 5.66 & 1.21 & 12.30 & 1.30 & 13.38 & 1.98 & 13.12 & 1.85 \\\\ GP-VTON [52] & 0.051 & 0.921 & 5.88 & 1.28 & 12.20 & 1.22 & 16.65 & 2.86 & 12.65 & 1.84 \\\\ \\hline\n' +
      '**OOTDiffusion (Ours)** & **0.045** & **0.927** & **4.20** & **0.37** & **11.03** & **0.29** & **9.72** & **0.64** & **10.65** & **0.54** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Quantitative results on the Dress Code dataset [33]. The best and second best results are reported in **bold** and underline, respectively. The * marker refers to the results reported in previous works.\n' +
      '\n' +
      'fusion. In comparison, our OOTDiffusion not only generates realistic outfitted images but also preserves the precise details, and thus substantially outperforms other methods on the other three metrics (LPIPS, FID and KID) while obtaining comparable SSIM scores to the GAN-based methods.\n' +
      '\n' +
      'Tab. 3 demonstrates the state-of-the-art performance of our method on the Dress Code dataset [33], which outperforms others on all the metrics for all the garment categories (upper-body/lower-body/dresses), confirming our feasibility in more complicated cases. Note that GP-VTON [52] applies extra data modifications such as background removal and pose normalization to Dress Code, and only provides a part of their test data. Despite this, our OOTDiffusion still achieves the best results on the more challenging original test dataset.\n' +
      '\n' +
      'Furthermore, the generalization capability of our method is quantitatively verified by the results of the cross-dataset evaluation listed in Tab. 4. We see that GP-VTON [52] falls far behind other methods on all the metrics since its warping module severely overfits the training data. While our method leads again on all the metrics for the out-of-distribution test dataset. Overall, the observations above (Tabs. 2 to 4) further demonstrate that our OOTDiffusion significantly outperforms other VTON methods in both fidelity and controllability for all kinds of scenarios and conditions.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'Despite the state-of-the-art performance achieved in the image-based virtual try-on task, limitations still exist in our OOTDiffusion which demand further improvement. First, since our model is trained on paired human and garment images, it may fail to get perfect results for cross-category virtual try-on, e.g., to put a T-shirt on a woman in a long dress, or to let a man in pants wear a skirt. This issue can be partially solved in the future by collecting datasets of each model wearing different clothes in the same pose. Another limitation is that some details in the original human image might be altered after virtual try-on, such as muscles, watches and tattoos, etc. This is because the relevant body area is masked and repainted by the diffusion model. Thus more practical pre- and post-processing methods are required for addressing such problems.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'In this paper, we present OOTDiffusion, a novel LDM-based network architecture for image-based virtual try-on. The proposed outfitting UNet efficiently learns the garment features and incorporates them into the denoising UNet via the proposed outfitting fusion process with negligible information loss. Classifier-free guidance for the garment features is enabled by the proposed outfitting dropout in training, which further enhances the controllability of our method. Extensive experiments on high-resolution datasets show our superiority over other VTON methods in both fidelity and controllability, indicating that our OOTDiffusion has broad application prospects for virtual try-on.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'We sincerely thank our colleagues including Wei Du, Xuping Su, Yilan Ye and Chi Zhang, etc., for kindly supporting and promoting our work.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. (2023) Improving image generation with better captions. Computer Science. Note: https://cdn. openai. com/papers/dall-e-3. pdf2 Cited by: SS1.\n' +
      '* [2]M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton (2018) Demystifying mmd gans. arXiv preprint arXiv:1801.01401. Cited by: SS1.\n' +
      '* [3]T. Brooks, A. Holynski, and A. A. Efros (2023) Instructpix2pix: learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18392-18402. Cited by: SS1.\n' +
      '* [4]Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh (2019) OpenPose: real-time multi-person 2d pose estimation using part affinity fields. IEEE Transactions on Pattern Analysis and Machine Intelligence. Cited by: SS1.\n' +
      '* [5]Z. Cao, T. Simon, S. E. Wei, and Y. Sheikh (2017) Realtime multi-person 2d pose estimation using part affinity fields. In CVPR Cited by: SS1.\n' +
      '* [6]S. Choi, S. Park, M. Lee, and J. Choo (2021) Viton-hd: high-resolution virtual try-on via misalignment-aware normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14131-14140. Cited by: SS1.\n' +
      '* [7]N. S. Detlefsen, J. Borovec, J. Schock, A. H. Jha, T. Koker, L. Di Liello, D. Stancl, C. Quan, M. Grechkin, and W. Falcon (2022) Torchmetrics-measuring reproducibility in pytorch. Journal of Open Source Software7 (70), pp. 4101-4102. Cited by: SS1.\n' +
      '* [8]H. Dong, X. Liang, X. Shen, B. Wang, H. Lai, J. Zhu, Z. Hu, and J. Yin (2019) Towards multi-pose guided virtual try-on network. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9026-9035. Cited by: SS1.\n' +
      '* [9]E. Fenocchi, D. Morelli, M. Cornia, L. Baraldi, F. Cesari, R. Cucchiara, and R. Dual-branch collaborative transformer for virtual try-on. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2247-2251. Cited by: SS1.\n' +
      '* [10]R. Gal, Y. Alaluf, Y. Atzmon, O. Patashnik, A. H. Bermano, G. Chechik, and D. Cohen-Or (2022) An image is worth one word: personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618. Cited by: SS1.\n' +
      '* [11]C. Ge, Y. Song, Y. Ge, H. Yang, W. Liu, and P. Luo (2021) Disentangled cycle consistency for highly-realistic virtual try-on. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16928-16937. Cited by: SS1.\n' +
      '* [12]Y. Ge, Y. Song, R. Zhang, C. Ge, W. Liu, and P. Luo (2021) Parser-free virtual try-on via distilling appearance flows. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8485-8493. Cited by: SS1.\n' +
      '* [13]I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio (2020) Generative adversarial networks. Communications of the ACM63 (11), pp. 139-144. Cited by: SS1.\n' +
      '* [14]J. Gou, S. Sun, J. Zhang, J. Si, C. Qian, and L. Zhang (2023) Taming the power of diffusion models for high-quality virtual try-on with appearance flow. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 7599-7607. Cited by: SS1.\n' +
      '\n' +
      '* [15] Han, X., Hu, X., Huang, W., Scott, M.R.: Clothflow: A flow-based model for clothed person generation. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 10471-10480 (2019)\n' +
      '* [16] Han, X., Wu, Z., Wu, Z., Yu, R., Davis, L.S.: Viton: An image-based virtual try-on network. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7543-7552 (2018)\n' +
      '* [17] He, S., Song, Y.Z., Xiang, T.: Style-based global appearance flow for virtual try-on. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3470-3479 (2022)\n' +
      '* [18] Hertz, A., Mokady, R., Tenenbaum, J., Aherman, K., Pritch, Y., Cohen-Or, D.: Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626 (2022)\n' +
      '* [19] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems **30** (2017)\n' +
      '* [20] Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022)\n' +
      '* [21] Hu, L., Gao, X., Zhang, P., Sun, K., Zhang, B., Bo, L.: Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv preprint arXiv:2311.17117 (2023)\n' +
      '* [22] Issenhuth, T., Mary, J., Calauzenes, C.: Do not mask what you do not need to mask: a parser-free virtual try-on. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XX 16. pp. 619-635. Springer (2020)\n' +
      '* [23] Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani, M.: Imagic: Text-based real image editing with diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6007-6017 (2023)\n' +
      '* [24] Kim, J., Gu, G., Park, M., Park, S., Choo, J.: Stableviton: Learning semantic correspondence with latent diffusion model for virtual try-on. arXiv preprint arXiv:2312.01725 (2023)\n' +
      '* [25] Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013)\n' +
      '* [26] Kumari, N., Zhang, B., Zhang, R., Shechtman, E., Zhu, J.Y.: Multi-concept customization of text-to-image diffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1931-1941 (2023)\n' +
      '* [27] Lee, S., Gu, G., Park, S., Choi, S., Choo, J.: High-resolution virtual try-on with misalignment and occlusion-handled conditions. In: European Conference on Computer Vision. pp. 204-219. Springer (2022)\n' +
      '* [28] Li, P., Xu, Y., Wei, Y., Yang, Y.: Self-correction for human parsing. IEEE Transactions on Pattern Analysis and Machine Intelligence **44**(6), 3260-3271 (2020)\n' +
      '* [29] Loshchilov, I., Hutter, F.: Fixing weight decay regularization in adam (2018)\n' +
      '* [30] Minar, M.R., Tuan, T.T., Ahn, H., Rosin, P., Lai, Y.K.: Cp-vton+: Clothing shape and texture preserving image-based virtual try-on. In: CVPR Workshops. vol. 3, pp. 10-14 (2020)\n' +
      '* [31] Mokady, R., Hertz, A., Aherman, K., Pritch, Y., Cohen-Or, D.: Null-text inversion for editing real images using guided diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6038-6047 (2023)* [32] Morelli, D., Baldrati, A., Cartella, G., Cornia, M., Bertini, M., Cucchiara, R.: Ladivton: Latent diffusion textual-inversion enhanced virtual try-on. arXiv preprint arXiv:2305.13501 (2023)\n' +
      '* [33] Morelli, D., Fincato, M., Cornia, M., Landi, F., Cesari, F., Cucchiara, R.: Dress code: High-resolution multi-category virtual try-on. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2231-2235 (2022)\n' +
      '* [34] Mou, C., Wang, X., Xie, L., Wu, Y., Zhang, J., Qi, Z., Shan, Y., Qie, X.: T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453 (2023)\n' +
      '* [35] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021)\n' +
      '* [36] Parmar, G., Kumar Singh, K., Zhang, R., Li, Y., Lu, J., Zhu, J.Y.: Zero-shot image-to-image translation. In: ACM SIGGRAPH 2023 Conference Proceedings. pp. 1-11 (2023)\n' +
      '* [37] Parmar, G., Zhang, R., Zhu, J.Y.: On aliased resizing and surprising subtleties in gan evaluation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11410-11420 (2022)\n' +
      '* [38] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023)\n' +
      '* [39] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [40] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [41] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. pp. 234-241. Springer (2015)\n' +
      '* [42] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aherman, K.: Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22500-22510 (2023)\n' +
      '* [43] Saharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans, T., Fleet, D., Norouzi, M.: Palette: Image-to-image diffusion models. In: ACM SIGGRAPH 2022 Conference Proceedings. pp. 1-10 (2022)\n' +
      '* [44] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems **35**, 36479-36494 (2022)\n' +
      '* [45] Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D.J., Norouzi, M.: Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence **45**(4), 4713-4726 (2022)\n' +
      '* [46] Simon, T., Joo, H., Matthews, I., Sheikh, Y.: Hand keypoint detection in single images using multiview bootstrapping. In: CVPR (2017)* [47] Tumanyan, N., Geyer, M., Bagon, S., Dekel, T.: Plug-and-play diffusion features for text-driven image-to-image translation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1921-1930 (2023)\n' +
      '* [48] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems **30** (2017)\n' +
      '* [49] Wang, B., Zheng, H., Liang, X., Chen, Y., Lin, L., Yang, M.: Toward characteristic-preserving image-based virtual try-on network. In: Proceedings of the European conference on computer vision (ECCV). pp. 589-604 (2018)\n' +
      '* [50] Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing **13**(4), 600-612 (2004)\n' +
      '* [51] Wei, S.E., Ramakrishna, V., Kanade, T., Sheikh, Y.: Convolutional pose machines. In: CVPR (2016)\n' +
      '* [52] Xie, Z., Huang, Z., Dong, X., Zhao, F., Dong, H., Zhang, X., Zhu, F., Liang, X.: Gp-vton: Towards general purpose virtual try-on via collaborative local-flow global-parsing learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 23550-23559 (2023)\n' +
      '* [53] Yang, B., Gu, S., Zhang, B., Zhang, T., Chen, X., Sun, X., Chen, D., Wen, F.: Paint by example: Exemplar-based image editing with diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18381-18391 (2023)\n' +
      '* [54] Yang, H., Zhang, R., Guo, X., Liu, W., Zuo, W., Luo, P.: Towards photo-realistic virtual try-on by adaptively generating-preserving image content. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 7850-7859 (2020)\n' +
      '* [55] Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023)\n' +
      '* [56] Yu, J., Xu, Y., Koh, J.Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B.K., et al.: Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789 **2**(3), 5 (2022)\n' +
      '* [57] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3836-3847 (2023)\n' +
      '* [58] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 586-595 (2018)\n' +
      '* [59] Zhao, W., Bai, L., Rao, Y., Zhou, J., Lu, J.: Unipc: A unified predictor-corrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [60] Zhu, L., Yang, D., Zhu, T., Reda, F., Chan, W., Saharia, C., Norouzi, M., Kemelmacher-Shlizerman, I.: Tryondiffusion: A tale of two unets. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4606-4615 (2023)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>