<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# OOTDiffusion: 제어 가능한 가상 트라이온을 위한 아웃피팅 융합 기반 잠재 확산\n' +
      '\n' +
      'Yuhao Xu\n' +
      '\n' +
      'Xiao-i Research\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Tao Gu\n' +
      '\n' +
      'Xiao-i Research\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Weifeng Chen\n' +
      '\n' +
      'Xiao-i Research\n' +
      '\n' +
      '1\n' +
      '\n' +
      'Chengai Chen\n' +
      '\n' +
      'Xiao-i Research\n' +
      '\n' +
      '1\n' +
      '\n' +
      '각주 1: 이메일: {yuhao.xu,tao.gu,weifeng.chen,arlenec}@xiaoi.com\n' +
      '\n' +
      'Xiao-i Research\n' +
      '\n' +
      '1\n' +
      '\n' +
      '각주 2: 이메일: {yuhao.xu,tao.gu,weifeng.chen,arlenec}@xiaoi.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '인샵 의류를 착용한 대상 사람의 의상 이미지를 생성하는 것을 목표로 하는 이미지 기반 가상 트라이온(VION)은 의상된 사람의 높은 충실도뿐만 아니라 의상 디테일의 완전한 보존을 요구하는 도전적인 이미지 합성 작업이다. 이 문제를 해결하기 위해, 우리는 미리 훈련된 잠재 확산 모델의 힘을 활용하고 현실적이고 제어 가능한 가상 트라이온을 위한 새로운 네트워크 구조를 설계하는 OOTDiffusion(Outfitting over Try-on Diffusion)을 제안한다. 명시적 워핑 과정 없이, 우리는 확산 모델의 잡음 제거 과정에서 제안된 아웃피팅 융합을 통해 의복의 세부 특징을 학습할 수 있는 아웃피팅 UNet을 제안하고, 이를 대상 인체와 병합한다. 외피팅 UNet의 제어 가능성을 더욱 높이기 위해 훈련 과정에 외피팅 드롭아웃을 도입하여 분류기 없는 안내를 통해 의복 특징의 강도를 조정할 수 있다. VITON-HD 및 드레스 코드 데이터 세트에 대한 포괄적인 실험은 OOTDiffusion이 임의의 인간 및 의류 이미지에 대해 고품질 의상 이미지를 효율적으로 생성한다는 것을 보여주며, 이는 충실도와 제어성 모두에서 다른 VTON 방법을 능가하여 가상 시도에서 인상적인 돌파구를 나타낸다. 우리의 소스 코드는 [https://github.com/levihsu/OOTDiffusion](https://github.com/levihsu/OOTDiffusion)에서 사용할 수 있다.\n' +
      '\n' +
      '키워드: 가상 시도 잠재 확산 아웃피팅 융합\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '이미지 기반 가상 트라이온(VTON)은 현재 전자상거래 산업에서 인기 있고 유망한 이미지 합성 기술로 소비자의 쇼핑 경험을 획기적으로 개선하고 의류 가맹점의 광고 비용을 절감할 수 있다. VTON 작업의 목적은 이름에서 알 수 있듯이 주어진 의복을 입은 대상 인간의 이미지를 생성하는 것이며, 이는 지난 몇 년 동안 보다 자연스럽고 정확한 가상 시도 결과를 위해 수많은 연구자[15, 16, 17, 12, 17, 24, 30, 49, 52, 54]로부터 엄청난 노력을 기울였다.\n' +
      '\n' +
      '이미지 기반 VTON은 현재 두 가지 주요 문제에 직면해 있다. 첫째, 생성된 이미지는 부조화를 피할 수 있을 만큼 사실적이고 자연스러워야 한다. 최근 가상 트라이온에 대한 대부분의 연구는 이미지 생성을 위한 생성적 적대 네트워크[13] (GANs) 또는 잠재 확산 모델[40] (LDMs)를 활용한다. 기존의 GAN 기반 방법[15, 16, 17, 52, 6, 27]은 보통 올바른 의복 주름, 자연광 및 그림자, 사실적인 인체를 생성하는 데 어려움이 있으며 과적합하기 쉽다. 따라서 보다 최근의 연구는 LDM 기반 방법[14, 24, 32, 60]을 선호하며, 이는 의상 이미지의 충실도를 효과적으로 향상시킨다. 두 번째 중요한 과제는 복잡한 텍스트, 질감, 색상, 패턴 및 라인 등과 같은 의류의 세부 기능을 가능한 한 보존하는 방법이다. 기존 연구들[14, 27, 32, 52, 6]은 의복의 특징을 대상 인체와 정렬하기 위해 명시적인 워핑 프로세스를 수행한 다음, 워핑된 의복을 생성 모델(즉, GAN 및 LDM 등)로 공급한다. 따라서, 이러한 VTON 방법의 성능은 손실 워핑 프로세스의 효능에 매우 의존한다. 한편, 일부 LDM 기반 방법[14, 24, 32]은 CLIP 텍스트 반전[10]을 통해 의복 특징을 학습하려고 시도하며, 이는 세립 의복 세부 사항을 보존하지 못한다.\n' +
      '\n' +
      '앞서 언급한 이미지 기반 VTON의 전망과 과제에 자극을 받아 새로운 LDM 기반 가상 시도 방법, 즉 Outfitting over Try-on Diffusion(OOTDiffusion; 그림 2 참조)을 제안한다. 먼저, 생성된 이미지의 높은 충실도와 자연스러운 시도 효과를 보장하기 위해 사전 훈련된 잠재 확산 모델[40]의 장점을 최대한 활용한다. 그런 다음 잠재 공간의 정확한 의복 특징을 학습하기 위한 새로운 외투형 UNet을 소개하고, 자기 주의 계층에서 외투형 융합 과정을 통해 잡음 제거 UNet에서 시끄러운 인체와 병합한다[48]. 이러한 방식으로, 의복 피처들은 워핑에 의해 야기되는 정보 손실 또는 피처 왜곡을 겪지 않고 다양한 타겟 인체 타입들 및 자세들에 부드럽게 적응된다. 한편, 의류 이미지에 대한 CLIP 텍스트-인버전(10)을 교차-어텐션 메커니즘(48)을 통해 아웃피팅 및 디노이징 UNET 모두에 공급되는 보조 의미 정보로 수행한다. 또한, 의류 특징에 대한 분류기 없는 안내[20]를 가능하게 하기 위해 트레이닝에서 소수의 의류 특징을 무작위로 떨어뜨리는 아웃핏 드롭아웃 프로세스를 수행한다. 이 방법을 통해 생성된 결과에 대한 의복 제어의 강도는 안내 척도로 간단히 조정할 수 있으며, 이는 VTON 방법의 제어 가능성을 더욱 향상시킨다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '* 우리는 현실적이고 제어 가능한 가상 트라이온을 위한 새로운 LDM 기반 네트워크 아키텍처인 OOTDiffusion을 제안한다.\n' +
      '* 명시적인 워핑 과정 없이 셀프 어텐션 레이어에서 아웃핏 융합을 통해 정확한 의류 특징을 학습하고 대상 인체와 정렬할 수 있도록 아웃핏 UNet을 설계한다.\n' +
      '* 트레이닝 과정에 아웃피팅 드롭아웃을 도입하며, 이는 아웃피팅 UNet의 제어 가능성을 더욱 향상시킨다.\n' +
      '* 광범위하게 사용되는 두 가지 고해상도 벤치마크 데이터 세트, 즉 VITON-HD [6] 및 드레스 코드 [33]에서 OOTDiffusion을 각각 훈련한다. 광범위한 정성적 및 정량적 평가는 다양한 표적 인간 및 의복 이미지에 대한 충실도와 제어 가능성 모두에서 최첨단 VTON 방법보다 우리의 우월성을 입증한다(도 1 참조). 이는 이미지 기반 가상 시도에서 인상적인 돌파구를 의미합니다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**이미지 기반 가상 트라이온.** 이미지 기반 가상 트라이온은 유망하고 도전적인 작업으로서 수년 동안 조사되었다[6, 9, 14, 22, 24, 27, 32, 49, 52, 60]. 보다 자연스럽고 정확한 결과를 목표로 최근의 연구는 주로 이미지 생성을 위한 생성적 적대 네트워크[13] (GANs) 또는 잠재 확산 모델[40] (LDMs)를 기반으로 한다. GAN 기반 VTON 방법[6, 27, 52] 중 VITON-HD [6]은 고해상도 데이터 세트를 수집하고 뒤틀린 의상과 대상 영역 간의 정렬 오류를 해결하기 위해 ALIAS 정규화 및 생성기를 제안했다. HR-VITON[27]은 신체 폐색 및 의복 오정렬을 처리하기 위해 워핑 및 분할을 동시에 수행했다. GP-VTON[52]은 변형된 의류를 생성하기 위한 LFGP 워핑 모듈을 제안하고 워핑 네트워크를 위한 DGT 훈련 전략을 도입하였다. 위에서 소개한 바와 같이 GAN 기반 방법은 일반적으로 사실적인 의복 주름과 자연광 및 그림자를 무시하는 명시적인 워핑 프로세스에 의존하며, 이는 의상 이미지의 충실도와 사실성을 심각하게 저하시킨다. 한편, GAN 기반 방법은 과적합(overfitting)을 위한 프루닝(prune) 방법이며, 아웃-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오\n' +
      '\n' +
      'LDM 기반 접근법[14, 24, 32]과 관련하여, LaDI-VTON[32] 및 DCI-VTON[14]도 명시적인 워핑 프로세스를 필요로 한다. 특히, LaDI-VTON[32]는 텍스트 역산을 수행하여 시각적 의복 특징을 CLIP[39] 토큰 임베딩 공간에 매핑하고 워핑된 입력과 함께 잠재 확산 모델을 조건화한다. DCI-VTON[14]는 가면을 쓴 사람 이미지에 휘어진 옷을 직접 결합하여 거친 결과를 얻은 후 확산 모델에 의해 정제하였다. 이 두 방법 모두 CLIP 인코더로 인한 정보 손실로 인해 복잡한 패턴 및 텍스트와 같은 의류 세부 정보를 완전히 보존하는 데 성공하지 못했다. 보다 최근에, StableVITON[24]은 독립적인 워핑을 버리고 의상과 인체 사이의 의미적 상관 관계를 학습하기 위해 제로 크로스 어텐션 블록을 제안했다. 그러나, 정보 손실은 여전히 교차 주의에 존재하며, 추가 제로 초기화 블록은 학습 및 추론 비용을 크게 증가시킨다. 대조적으로, 다시 뒤틀림 없이 LDM 기반 OOT디퓨전은 미리 훈련된 아웃피팅 UNet을 세분화하여 의류 세부 정보를 한 단계에서 배우고 무시할 수 있는 정보 손실과 아웃피팅 융합을 통해 디노이징 UNet으로 효율적으로 통합한다.\n' +
      '\n' +
      '####2.0.2 LDM 기반 제어 가능한 이미지 생성.\n' +
      '\n' +
      '잠재 확산 모델[40]은 최근 몇 년 동안 텍스트 대 이미지[1, 26, 42, 38, 44] 및 이미지 대 이미지[23, 36, 43, 45, 47] 세대에서 큰 성공을 거두었다. 보다 제어 가능한 생성 결과를 위해, 프롬프트-투-프롬프트[18] 및 널-텍스트 반전[31]은 추가 모델 학습 없이 입력 캡션을 수정하여 이미지를 미세하게 편집하도록 교차-어텐션 레이어를 제어했다. InstructPix2Pix[3]은 입력 이미지와 텍스트 명령이 주어지면 편집 이미지를 생성하는 확산 모델을 훈련하기 위해 쌍을 이루는 데이터를 생성했다. 예시에 따라 [53] 훈련된 이미지 조절 확산 모델을 자체 감독 방식으로 학습하여 세밀한 이미지 제어를 제공한다. ControlNet[57] 및 T2I-Adapter[34]는 공간 컨디셔닝 제어를 가능하게 하기 위해 사전 훈련된 확산 모델에 추가 블록을 통합하였다. IP-Adapter[55]는 텍스트 및 이미지 특징에 대해 분리 교차 주의 메커니즘을 채택하여 이미지 프롬프트 및 추가 구조적 조건으로 제어 가능한 생성을 가능하게 했다. 본 논문에서는 영상 기반 VTON 태스크에 초점을 맞추어, 노이즈 제거 UNet의 self-attention layer에 아웃피팅 융합을 적용하고 트레이닝 시간에 아웃피팅 드롭아웃을 수행하여 잠재 확산 모델이 의복 특징에 대해 보다 제어 가능한 의상 이미지를 생성할 수 있도록 한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      '####3.1.1 안정적인 확산\n' +
      '\n' +
      '우리의 OOTDiffusion은 가장 일반적으로 사용되는 잠재 확산 모델 중 하나인 Stable Diffusion[40]의 확장이다. 안정적 확산은 잠재 공간에서 이미지 표현이 가능하도록 인코더\\(\\mathcal{E}\\)와 디코더\\(\\mathcal{D}\\)으로 구성된 가변 오토인코더[25] (VAE)를 사용한다. 그리고 UNet[41]\\(\\epsilon_{\\theta}\\)은 CLIP 텍스트 인코더[39]\\(\\tau_{\\theta}\\)에 의해 인코딩된 컨디셔닝 입력으로 가우시안 잡음\\(\\epsilon\\)을 제거하도록 학습된다. 이미지\\(\\mathbf{x}\\)와 텍스트 프롬프트\\(\\mathbf{y}\\)이 주어지면, 디노이징 UNet\\(\\epsilon_{\\theta}\\)의 트레이닝은 다음과 같은 손실 함수를 최소화함으로써 수행된다:\n' +
      '\n' +
      '\\mathcal{L}_{LDM}=\\mathbb{E}_{\\mathcal{E}(\\mathbf{x}),\\mathbf{y},\\epsilon\\sim\\mathcal{N}(0,1),t}\\left[\\|\\epsilon-\\epsilon_{\\theta}(\\mathbf{z}_{t},t, \\tau_{\\theta}(\\mathbf{y}))\\|_{2}^{2}\\right], \\tag{1}\\het}\\het}\n' +
      '\n' +
      '여기서 \\(t\\in\\{1,...,T\\}\\)은 순방향 확산 과정의 시간 단계를 나타내고, \\(\\mathbf{z}_{t}\\)은 가우시안 잡음 \\(\\epsilon\\sim\\mathcal{N}(0,1)\\)(즉, 잡음 잠재)이 추가된 인코딩된 이미지 \\(\\mathcal{E}(\\mathbf{x})\\)이다. 컨디셔닝 입력\\(\\tau_{\\theta}(\\mathbf{y})\\)는 교차-어텐션 메커니즘에 의해 잡음 제거 UNet과 상관된다는 점에 유의한다[48].\n' +
      '\n' +
      '### OOTDiffusion\n' +
      '\n' +
      '**개요.** 도 2는 우리의 방법의 개요를 예시한다. 목표 인체 이미지\\(\\mathbf{x}\\in\\mathbb{R}^{3\\times H\\times W}\\)와 입력 의류 이미지\\(\\mathbf{g}\\in\\mathbb{R}^{3\\times H\\times W}\\)이 주어지면, 우리의 OOTDiffusion은 사실적인 의상 이미지\\(\\mathbf{x}\\in\\mathbbb{R}^{3\\times H\\times W}\\)을 생성할 수 있다. 본 논문에서는 OpenPose[4, 5, 46, 51]와 HumanParsing[28]을 사용하여 마스킹된 인간 이미지 \\(\\mathbf{x}_{\\mathbf{m}}\\in\\mathbb{R}^{3\\times H\\times W}\\)를 생성하고, VAE 인코더 \\(\\mathcal{E}\\)을 사용하여 잠재 공간으로 \\(\\mathcal{E}(\\mathbf{x}_{\\mathbf{m}})\\in\\mathbb{R}^{4\\times h\\times w}\\)을 변환하며, 여기서 \\(h=\\frac{H}{8}\\)과 \\(w=\\frac{W}{8}\\)을 생성한다. 그리고 나서 우리는 양보한다.\n' +
      '\n' +
      '그림 2: 제안된 OOTDiffusion 모델의 개요. 왼쪽에서 의류 이미지는 잠재 공간으로 인코딩되고 단일 단계 프로세스를 위해 아웃피팅 UNet으로 공급된다. CLIP 인코더에 의해 생성된 보조 컨디셔닝 입력과 함께, 의복 특징은 아웃피팅 융합을 통해 노이즈 제거 UNet에 통합된다. 특히 분류기 없는 안내를 가능하게 하기 위해 트레이닝에서 의복 래턴트에 대해 아웃핏 드롭아웃이 수행된다. 우측에서, 입력 인간 이미지는 타겟 영역에 대해 마스킹되고, 다수의 샘플링 단계를 위해 잡음 제거 UNet에 대한 입력으로서 가우시안 노이즈와 연결된다. 디노이징 후에, 특징 맵은 우리의 시도 결과로서 이미지 공간으로 다시 디코딩된다.\n' +
      '\n' +
      'nate \\(\\mathcal{E}(\\mathbf{x_{m}})\\)에 가우시안 잡음 \\(\\epsilon\\in\\mathbb{R}^{4\\times h\\times w}\\)을 갖는 입력 잠재 \\(\\mathbf{z}_{T}\\in\\mathbb{R}^{8\\times h\\times w}\\)을 갖는 잡음 제거 UNet. 우리는 8개의 채널로 우리의 입력을 지원하기 위해 잡음 제거 UNet의 첫 번째 컨볼루션 계층에 4개의 제로 초기화 채널을 추가한다는 점에 유의한다.\n' +
      '\n' +
      '반면에, 우리는 (i)외투유니트(outfitting unet)에 인코딩된 의류 잠재 \\(\\mathcal{E}(\\mathbf{g})\\in\\mathbb{R}^{4\\times h\\times w}\\)을 한 단계로 학습하고, (ii)외투융합을 통해 잡음제거 유니트에 통합한다. 그리고 훈련 과정에서 특히 \\(\\mathcal{E}(\\mathbf{g})\\)에 대해 (iii) 아웃피팅 드롭아웃을 수행한다. 또한, 의류 이미지\\(\\mathbf{g}\\)에 대한 CLIP 텍스트-인버전 [10]을 수행하고, 보조 컨디셔닝 입력으로서 의류 라벨\\(\\mathbf{y}\\in\\{upperbody", "lowerbody", "dress"\\}\\)의 텍스트 임베딩과 선택적으로 연결하며, 교차-어텐션 메커니즘을 통해 외부 피팅 및 노이즈 제거 UNET 모두에 공급된다[48]. 마지막으로, 여러 단계의 잡음 제거 과정을 거친 후 VAE 디코더를 사용하여 잡음 제거된 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 잠재 우리는 다음 섹션에서 OOTDiffusion의 핵심 기술(즉, (i)outfitting UNet, (ii)outfitting 융합 및 (iii)outfitting dropout)을 자세히 설명할 것이다.\n' +
      '\n' +
      '3.1.2 아웃핏 UNet.\n' +
      '\n' +
      '앞서 소개한 바와 같이 의류 이미지 \\(\\mathbf{g}\\)의 세부 특징을 효율적으로 학습할 수 있는 아웃핏 UNet을 제안한다. 도 4의 좌측면이다. 도 2는 스테이블 확산의 노이즈 제거 UNet과 본질적으로 동일한 우리의 아웃핏 UNet의 아키텍처를 보여준다. 인코딩된 의복 잠재성\\(\\mathcal{E}(\\mathbf{g})\\in\\mathbb{R}^{4\\times h\\times w}\\)을 외접 UNet\\(\\omega_{\\theta^{\\prime}\\)에 투입한 후, 외접 융합을 통해 잡음제거 UNet\\(\\epsilon_{\\theta}\\)에 삽입한다(다음 절 참조). 전술한 보조 컨디셔닝 입력과 함께, 아웃피팅 및 디노이징 UNets은 다음의 손실 함수를 최소화함으로써 공동으로 트레이닝된다:\n' +
      '\n' +
      '\\mathcal{L}_{OOTD}=\\mathbb{E}_{\\mathcal{E}(\\mathbf{x_{m}}),\\mathcal{E}(\\mathbf{g}),\\psi,\\epsilon\\sim\\mathcal{N}(0,1),t}\\left[\\|\\epsilon-\\epsilon_{\\theta}(\\mathbf{z}_{t},t,\\omega_{\\theta^{\\prime}}(\\mathcal{E}(\\mathbf{g}), \\psi),\\psi\\epsilon\\sim\\mathcal{N}(0,1),t}\\left[\\|\\epsilon-\\epsilon_{\\theta}(\\mathbf{g}), \\psi),\\psi\\epsilon\\sim\\mathcal{N}(0,1),t}\\left[\\|\\epsilon-\\epsilon_{\\theta}(\\mathbf{z}_{t\n' +
      '\n' +
      '여기서 \\(\\psi=\\tau_{g}(\\mathbf{g})\\) (c) \\(\\tau_{y}(\\mathbf{y})\\)는 \\(\\omega_{\\theta^{\\prime}\\) 및 \\(\\epsilon_{\\theta}\\) 모두에 대한 보조 컨디셔닝 입력을 나타낸다. 한편, \\(\\tau_{g}\\)와 \\(\\tau_{y}\\)는 각각 미리 훈련된 CLIP 영상 인코더와 텍스트 인코더를 나타내며, (c)는 연접을 나타낸다.\n' +
      '\n' +
      '실제로, 우리는 아웃피팅 및 디노이징 UNets(첫 번째 컨볼루션 계층에 추가된 제로 초기화 채널을 제외)의 초기화를 위해 미리 훈련된 Stable Diffusion [40]의 UNet 가중치를 직접 복제하고 고해상도 VTON 데이터 세트 [6, 33]에서 공동으로 미세 조정한다. 훈련 과정에서 \\(\\omega_{\\theta^{\\prime}\\)와 \\(\\epsilon_{\\theta}\\)은 어떠한 가중치도 공유하지 않는다는 점에 유의한다. 우리는 사전 훈련된 모델을 전술적으로 활용하면 훈련 효율이 크게 향상되고 훈련 비용이 감소한다고 주장한다. 또한, unoising UNet과 비교하여, 우리의 아웃피팅 UNet에서 중요한 차이점은 추론에서 다수의 denoising 단계 이전에 단지 한 단계 전진 과정을 필요로 한다는 것이며, 이는 원래의 안정 확산[40]에 최소한의 추가 계산 비용을 야기한다는 것이다.\n' +
      '\n' +
      '#####3.1.3 아웃핏 퓨전.\n' +
      '\n' +
      '제안된 아웃피팅 UNet을 기반으로 공간-어텐션 메커니즘[21, 48]에서 영감을 얻은 아웃피팅 융합 프로세스를 도입하여 학습된 의류 특징을 노이즈 제거 UNet에 통합한다. 먼저, 두 개의 UNET의 트랜스포머 블록[48]에 접근하여, 대응하는 자기 주의 층에 대한 입력으로 사용되는 특징 맵의 각 쌍을 찾는다[48]. \\(n\\)번째 특징맵 쌍 \\(\\mathbf{g}_{n},\\mathbf{x}_{n}\\in\\mathbb{R}^{c_{n}\\times h_{n}\\times w_{n}\\)을 주어 공간 도메인에서 다음과 같이 연결한다.\n' +
      '\n' +
      '\\mathbf{x}_{\\mathbf{g}_{n}=\\mathbf{x}_{n}\\text{\\textcircled{C}}\\mathbb{g}_{n}\\in\\mathbb{R}^{c_{n}\\times h_{n}\\times 2w_{n}}. \\tag{3}\\times\n' +
      '\n' +
      '그리고 잡음제거 UNet의 자기관심계층에 대한 입력으로 \\(\\mathbf{x}_{n}\\)을 연접된 특징맵 \\(\\mathbf{x}_{\\mathbf{g}_{n}\\)으로 대체한다. 셀프-어텐션 프로세스 후에, 우리는 피처 맵의 주먹 절반을 셀프-어텐션 레이어의 출력으로서 크롭아웃한다. 이러한 접근법을 통해, 의복 특징들은 무시할 수 있는 정보 손실로 시끄러운 인체와 효과적으로 상관된다. 따라서 디노이징 UNet은 의복의 세부 정보를 보존하고 생성된 이미지에서 목표 인체에 자연스럽게 적응하기 위해 아웃피팅 UNet으로부터 정확한 특징을 학습할 수 있게 한다.\n' +
      '\n' +
      '#### 3.3.2 아웃핏 드롭아웃.\n' +
      '\n' +
      '본 논문에서 제안한 VTON 방법의 제어성을 더욱 향상시키기 위해, 의류 특징들에 대한 분류기 없는 안내[20]를 가능하게 하기 위해 트레이닝에서 아웃핏 드롭아웃 동작을 채용한다. 분류자 없는 지침은 잠재 확산 모델에 의해 생성된 이미지의 품질과 다양성을 거래하기 위해 조건부 이미지 생성[3; 35; 44; 56]에서 광범위하게 사용되었다. 특히, UNet의 트레이닝 과정에서 잠재된 입력 의류는 \\(\\mathcal{E}(\\mathbf{g})=\\varnothing\\으로 랜덤하게 드롭하며, 여기서 \\(\\varnothing\\in\\mathbb{R}^{4\\times h\\times w}\\)은 모두 0의 잠재성을 의미한다. 이러한 방식으로, 잡음 제거 UNet은 조건적 및 무조건적으로, 즉 아웃피팅 융합의 유무에 관계없이 훈련된다. 그리고 추론 시간에 예측 잡음에 대한 조건부 제어의 강도를 조정하기 위해 유도 척도(s_{\\mathbf{g}\\geq 1\\)를 사용한다.\n' +
      '\n' +
      '\\epsilon_{\\theta}(\\hat{\\epsilon}_{t},\\omega_{\\theta^{\\prime}(\\mathbf{g})))=\\epsilon_{\\theta}(\\mathbf{z}_{t},\\varnothing)+s_{\\mathbf{g}}\\cdot(\\epsilon_{\\theta}(\\mathbf{z}_{t},\\omega_{\\theta^{\\prime}(\\mathcal}E}(\\mathbf{g})))-\\epsilon_{\\theta}(\\mathbf{z}_{t},\\varnothing)),\\tag{4}\\tag{g}}\\cdot(\\epsilon_{\\theta}(\\mathbf{z}_{t},\\omega_{\\theta^{\\prime}))-\\epsilon_{\\theta}(\\mathbf{g})\n' +
      '\n' +
      '여기서 우리는 식에 비해 몇 가지 작은 항을 생략합니다. (2) 간결함을 위해.\n' +
      '\n' +
      '실제로, 우리는 훈련에서 아웃핏 탈락률을 10%로 경험적으로 설정했는데, 즉 의복 래턴트(\\mathcal{E}(\\mathbf{g})\\)의 10%를 \\(\\varnothing\\)으로 설정하였다. 그리고 절제 연구에 따르면 유도 척도(s_{\\mathbf{g}}\\)의 최적값은 보통 약 \\(1.5\\sim 2.0\\)이다(Sec. 4.3 참조). 도. 3 및 탭. 1은 아웃핏 탈락과 다양한 유도 척도 값의 효과를 보여준다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '#### 4.1.1 Datasets.\n' +
      '\n' +
      '본 논문에서는 VITON-HD[6]와 Dress Code[33]의 두 가지 고해상도 가상 데이터셋(\\(1024\\times 768\\))을 대상으로 실험을 수행하였다. VITON-HD 데이터 세트는 13,679개의 정면 반신체 모델과 해당 상반신 의상으로 구성되며, 여기서 2032쌍이 테스트 세트로 사용된다. 드레스 코드 데이터 세트는 전신 모델의 15,363/8,951/2,947 이미지 쌍과 해당 상반신 의류/하반신 의류/드레스로 구성되며, 각 의류 카테고리에 대해 1,800쌍이 테스트 세트로 사용된다.\n' +
      '\n' +
      '**비교 방법.** VITON-HD 데이터셋 [6]에서 OOTDiffusion을 GAN 기반 VITON-HD [6], HR-VITON [27] 및 GP-VTON [52], LDM 기반 LaDI-VTON [32] 및 StableVITON [24]를 포함한 여러 최신 VTON 방법과 비교한다.\n' +
      '\n' +
      '드레스 코드 데이터세트[33]에 대한 평가는 VITON-HD[6], HR-VITON[27] 및 StableVITON[24]이 상반신 의류를 넘어 전체 데이터세트에 대해 설계되지 않았기 때문에 공정한 비교를 위해 GP-VTON[52] 및 LaDI-VTON[32]) 두 가지 VTON 방법과 다른 LDM 기반 인페인팅 방법(즉, Paint-by-Example[53])을 선택한다.\n' +
      '\n' +
      '**평가 메트릭.** 페어링된 설정 및 페어링되지 않은 설정 모두에서 결과를 평가하며, 여기서 페어링된 설정은 대상 인간 및 재구성을 위한 해당 의류 이미지를 제공하고, 페어링되지 않은 설정은 가상 트라이온을 위한 다른 의류 이미지를 제공한다. 특히 드레스 코드[33]의 경우 상체 의류에 국한되지 않고 전체 데이터 세트에 대해 평가가 수행되므로 다양한 의류 유형을 가진 실제 응용 프로그램에서 각 방법의 타당성을 보다 효과적으로 검증한다. 또한, 각 방법의 일반화 능력을 추가로 조사하기 위해 한 데이터 세트에 대해 모델을 학습하고 다른 데이터 세트에 대해 평가하는 교차 데이터 세트 평가를 수행한다.\n' +
      '\n' +
      '정량적 평가에서는 OOTDiffusion이 더 높은 해상도(\\(1024\\times 768\\))의 가상 트라이온을 지원하지만, 모든 실험은 이전의 VTON 방법과 공정하게 비교하기 위해 \\(512\\times 384\\)의 해상도로 수행되었다. 페어링 설정을 위해 LPIPS[58]와 SSIM[50]을 사용하여 원본 이미지의 복원 측면에서 생성된 이미지의 품질을 측정한다. 비쌍 설정의 경우, 현실감과 충실도 평가를 위해 FID[19]와 KID[2]를 사용한다. 우리는 이러한 모든 메트릭을 구현하기 위해 이전 작업 [7, 32, 37]을 따른다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '실험에서, 우리는 안정 확산 v1.5[40]의 미리 훈련된 가중치를 상속하여 OOTDiffusion 모델을 초기화한다. 그리고 5e-5의 고정 학습률을 갖는 AdamW 최적화기[29]를 사용하여 UNET의 아웃피팅과 디노이징을 세분화하고, VITON-HD[6] 및 Dress Code[33] 데이터세트에서 각각 \\(512\\times 384\\) 및 \\(1024\\times 768\\)의 해상도로 4가지 모델을 학습한다. 모든 모델들은 단일 NVIDIA A100 GPU에서 36000번의 반복을 위해 학습되며, 배치 크기는 \\(512\\times 384\\) 해상도에 대해 64, \\(1024\\times 768\\) 해상도에 대해 16이다. 추론 시간에 UniPC 샘플러를 사용하여 20개의 샘플링 단계에 대해 단일 NVIDIA RTX 4090 GPU에서 OOTDiffusion을 실행한다[59].\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'VITON-HD 데이터세트 [6]에서 제안된 아웃피팅 탈락과 유도 척도 \\(s_{\\mathbf{g}}\\)의 다른 값이 미치는 영향을 조사한다. 먼저, 우리는 OOT확산 모델의 두 가지 변형을 각각 아웃핏 탈락 없이 훈련한다. 그런 다음 아웃피팅 드롭아웃으로 훈련된 모델에 대해 분류기 없는 안내를 위해 \\(s_{\\mathbf{g}}=1.0,1.5,2.0,2.5,3.0,5.0\\)을 설정했다. 추론 시간에 우리는 공정한 비교를 위해 다른 모든 매개변수(랜덤 시드 포함)가 일관성을 보장한다. 를 포함하는 것을 특징으로 하는 반도체 소자의 제조 방법. 도 3은 아웃피팅 드롭아웃 없이, 분류기 없는 안내는 지원되지 않으며 생성된 결과는 명백히 최악이다. outfitting dropout으로 학습된 모델의 경우 \\(s_{\\mathbf{g}}=1.0\\)일 때 추론 과정은\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c c} \\hline \\hline\n' +
      '*outitting** & **Guidance** &\\multirow{2}}{**LPIPS \\(\\downarrow\\) SSIM \\(\\uparrow\\) FID \\(\\downarrow\\) KID \\(\\downarrow\\)**} \\\\\\\n' +
      '**Dropout** & **Scale** & & & & \\\\ \\hline � & - & 0.0750 & 0.8699 & 8.91 & 0.89 \\\\ ✓ & 1.0 & 0.0749 & 0.8705 & 8.99 & 0.89 \\\\ ✓ & 1.5 & **0.0705** & **0.8775** & 8.81 & **0.82** \\\\ ✓ & 2.0 & 0.0708 & 0.8766 & **8.80** & 0.86 \\\\ ✓ & 2.5 & 0.0746 & 0.8691 & 8.84 & 0.89 \\\\ ✓ & 3.0 & 0.0753 & 0.8684 & 8.95 & 0.96 \\\\ ✓ & 5.0 & 0.0788 & 0.8640 & 9.28 & 1.22 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: VITON-HD 데이터 세트의 아웃피팅 드롭아웃 및 상이한 안내 스케일 값에 대한 절제 연구. 최고 및 두 번째 최고 결과는 각각 **굵은** 및 밑줄로 보고된다.\n' +
      '\n' +
      '그림 3: 아웃핏 드롭아웃 없이/아웃핏으로 훈련된 OOTDiffusion 모델에 의해 생성된 의상 이미지의 정성적 비교 및 안내 척도 \\(s_{\\mathbf{g}}\\)의 다른 값을 사용한다. 자세한 내용은 확대하십시오.\n' +
      '\n' +
      '중단 제외 없이 모형과 동일합니다. 식 참조 (4)), 유사하게 나쁜 결과를 얻는다. \\(s_{\\mathbf{g}}>1.0\\)일 때, 우리는 \\(s_{\\mathbf{g}}\\)이 증가함에 따라 세립 의복 특징이 더 명확해지는 것을 알 수 있다. 그러나 색 왜곡은 \\(s_{\\mathbf{g}}\\geq 2.5\\)일 때 발생하며 \\(s_{\\mathbf{g}}=5.0\\)일 때 매우 현저해진다. 또한 탭입니다. 1은 의류 특징에 대한 분류기 없는 안내를 가능하게 하는 아웃핏 드롭아웃의 효과를 정량적으로 입증하며, 대부분의 경우 최적의 안내 척도 값은 \\(1.5\\sim 2.0\\) 정도이다. 본 연구에 따르면, OOTDiffusion에 대한 Outfitting dropout을 지속적으로 수행하였고, VITON-HD 데이터셋 [6]은 \\(s_{\\mathbf{g}}=1.5\\), Dress Code 데이터셋 [33]은 \\(s_{\\mathbf{g}}=2.0\\)을 실증적으로 설정하였다.\n' +
      '\n' +
      '### Experimental Results\n' +
      '\n' +
      '**정성적 결과** 도 4는 VITON-HD [6]의 테스트 세트에서 본 방법 및 다른 VTON 방법의 일부 예시적인 결과를 시각적으로 보여준다. 우리는 다른 방법과 비교하여 우리의 OOT확산이 일관되게 최고를 달성한다는 것을 관찰한다.\n' +
      '\n' +
      '도 4: VITON-HD 데이터세트 [6](상체 의복이 있는 반체 모델)에 대한 정성적 비교. 자세한 내용은 확대하십시오.\n' +
      '\n' +
      '다양한 상체 의류에 대한 시도 효과. 보다 구체적으로 GP-VTON[52]과 같은 GAN 기반 방법은 사실적인 인체(1열과 4열)와 자연스러운 의복 주름(2열과 3열)을 생성하지 못하는 경우가 많으며, 이는 의상 이미지를 비현실적으로 보이게 한다. LaDI-VTON[32] 및 StableVITON[24]와 같은 다른 LDM 기반 방법은 각각 2열 및 3열과 같은 일부 의류 세부 정보를 잃는 경향이 있다.\n' +
      '\n' +
      '도 5: 드레스 코드 데이터세트 [33](상체 의류/하체 의류/드레스를 갖는 전신 모델)에 대한 정성적 비교. 자세한 내용은 확대하십시오.\n' +
      '\n' +
      '복잡한 텍스트(2행과 4행)와 패턴(1행과 3행)입니다. 대조적으로, 우리의 OOTDiffusion은 사실적인 이미지를 생성할 뿐만 아니라 대부분의 의류 디테일을 보존합니다.\n' +
      '\n' +
      '전신 모델과 다양한 의류 카테고리로 구성된 보다 복잡한 드레스 코드 데이터 세트[33]와 관련하여, 우리의 OOTDiffusion은 여전히 다른 VTON 방법보다 시각적으로 우수하다. 도 1에 도시된 바와 같다. 도 5에 도시된 바와 같이, Paint-by-Example [53] 및 LaDI-VTON [32]는 의복 특징들을 보존하지 못하고, GP-VTON [52]는 심각한 신체 및 배경 왜곡을 야기하는 경향이 있다. 반면, OOTDiffusion은 상반신 의류(1열), 하반신 의류(2열), 드레스(3열, 4열) 등 다양한 의류 카테고리에서 지속적으로 매우 안정적인 성능을 보인다.\n' +
      '\n' +
      '우리의 방법의 일반화 능력을 평가하기 위해 한 데이터 세트에 대한 훈련과 다른 데이터 세트에 대한 테스트와 같은 추가 교차 데이터 세트 실험을 수행한다. 도. 도 6은 VITON-HD 데이터세트[6]에서 훈련된 모든 모델 중에서 우리의 OOTDiffusion이 Dress Code 데이터세트[33]의 테스트 예제에 최적으로 적응되어 보다 사실적인 의상 이미지를 생성하고 훨씬 더 많은 의상 세부 정보를 보존한다는 것을 보여준다. 요약하면, 상기 관측치(도 4 내지 도 6) 다양한 인체 및 의복 이미지에 대한 자연스럽고 정확한 의상 결과를 생성하는 데 있어 OOTDiffusion의 우수성과 일반화 능력을 질적으로 입증한다.\n' +
      '\n' +
      '그림 6: 교차 데이터 세트 평가의 질적 결과. 모델은 VITON-HD 데이터세트[6]에서 훈련되고 드레스 코드 데이터세트[33]에서 테스트된다. 자세한 내용은 확대하십시오.\n' +
      '\n' +
      '정량적 결과.탭. 2는 VITON-HD 데이터셋에 대한 정량적 평가 결과를 제시한다[6]. 우리는 HR-VITON[27] 및 GP-VTON[52]와 같은 일부 GAN 기반 모델이 상대적으로 높은 SSIM 점수를 달성하여 원본 이미지의 구조적 정보를 유지할 수 있음을 관찰한다. 그러나 생성된 이미지는 상세 충실도가 부족하여 LPIPS에서 우리보다 떨어진다. LaDI-VTON[32]과 StableVITON[24]를 포함한 기존의 LDM 기반 방법들은 FID와 KID 점수에 따라 보다 사실적인 영상을 생성하지만, 손실된 특징 때문에 디테일 특징을 복원하지 못한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Train/Test**} & \\multicolumn{3}{c}{**VITON-HD/Dress Code**} & \\multicolumn{3}{c}{**Dress Code/VITON-HD**} \\\\ \\cline{2-9}  & **LPIPS \\(\\downarrow\\)** & **SSIM \\(\\uparrow\\)** & **FID \\(\\downarrow\\)** & **KID \\(\\downarrow\\)** & **LPIPS \\(\\downarrow\\)** & **SSIM \\(\\uparrow\\)** & **FID \\(\\downarrow\\)** & **KID \\(\\downarrow\\)** \\\\ \\hline VITON-HD* [6] & 0.187 & 0.853 & 44.26 & 28.82 & - & - & - & - \\\\ HR-VITON* [27] & 0.108 & 0.909 & 19.97 & 7.35 & - & - & - & - \\\\ LaDI-VTON [32] & 0.154 & 0.908 & 14.58 & 3.59 & 0.235 & 0.812 & 29.66 & 20.58 \\\\ GP-VTON [52] & 0.291 & 0.820 & 74.36 & 80.49 & 0.266 & 0.811 & 52.69 & 49.14 \\\\ StableVITON [24] & 0.065 & 0.914 & 13.18 & 2.26 & - & - & - & - \\\\ \\hline\n' +
      '**OOTDiffusion (Ours)** & **0.061** & **0.915** & **11.96** & **1.21** & **0.123** & **0.839** & **11.22** & **2.72** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 교차 데이터세트 평가의 정량적 결과. 각 모델은 VITON-HD [6] 및 Dress Code [33] 데이터 세트 중 하나에 대해 훈련되고 다른 하나에 대해 평가된다. 최고 및 두 번째 최고 결과는 각각 **굵은** 및 밑줄로 보고된다. *마커는 이전 작업에서 보고된 결과를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & **LPIPS \\(\\downarrow\\)** & **SSIM \\(\\uparrow\\)** & **FID \\(\\downarrow\\)** & **KID \\(\\downarrow\\)** \\\\ \\hline VITON-HD [6] & 0.116 & 0.863 & 12.13 & 3.22 \\\\ HR-VITON [27] & 0.097 & 0.878 & 12.30 & 3.82 \\\\ LaDI-VTON [32] & 0.091 & 0.875 & 9.31 & 1.53 \\\\ GP-VTON [52] & 0.083 & **0.892** & 9.17 & 0.93 \\\\ StableVITON [24] & 0.084 & 0.862 & 9.13 & 1.20 \\\\ \\hline\n' +
      '**OOTDiffusion (Ours)** & **0.071** & 0.878 & **8.81** & **0.82** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: VITON-HD 데이터 세트에 대한 정량적 결과[6]. 최고 및 두 번째 최고 결과는 각각 **굵은** 및 밑줄로 보고된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multicolumn{3}{c}{**All**} & \\multicolumn{3}{c}{**Upper-body**} & \\multicolumn{3}{c}{**Lower-body**} & \\multicolumn{3}{c}{**Dresses**} \\\\ \\cline{2-9}  & **LPIPS \\(\\downarrow\\)** & **SSIM \\(\\uparrow\\)** & **FID \\(\\downarrow\\)** & **KID \\(\\downarrow\\)** & **FID \\(\\downarrow\\)** & **KID \\(\\downarrow\\)** & **FID \\(\\downarrow\\)** & **KID \\(\\downarrow\\)** \\\\ \\hline PSAD* [33] & 0.058 & 0.918 & 10.61 & 6.17 & 17.51 & 7.15 & 19.68 & 8.90 & 17.07 & 6.66 \\\\ Paint-by-Example [53] & 0.142 & 0.851 & 9.57 & 3.63 & 18.63 & 4.81 & 15.89 & 4.12 & 19.15 & 5.88 \\\\ LaDI-VTON [32] & 0.067 & 0.910 & 5.66 & 1.21 & 12.30 & 1.30 & 13.38 & 1.98 & 13.12 & 1.85 \\\\ GP-VTON [52] & 0.051 & 0.921 & 5.88 & 1.28 & 12.20 & 1.22 & 16.65 & 2.86 & 12.65 & 1.84 \\\\ \\hline\n' +
      '**OOTDiffusion (Ours)** & **0.045** & **0.927** & **4.20** & **0.37** & **11.03** & **0.29** & **9.72** & **0.64** & **10.65** & **0.54** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 드레스 코드 데이터 세트에 대한 정량적 결과[33]. 최고 및 두 번째 최고 결과는 각각 **굵은** 및 밑줄로 보고된다. *마커는 이전 작업에서 보고된 결과를 나타낸다.\n' +
      '\n' +
      '융합. 이에 비해 OOTDiffusion은 사실적인 의상 이미지를 생성할 뿐만 아니라 정확한 세부 정보를 보존하므로 GAN 기반 방법과 유사한 SSIM 점수를 얻으면서 다른 세 가지 메트릭(LPIPS, FID 및 KID)에서 다른 방법보다 훨씬 우수하다.\n' +
      '\n' +
      '탭 도 3은 드레스 코드 데이터셋 [33]에서 본 방법의 최신 성능을 보여주며, 모든 의류 카테고리(상체/하체/드레스)에 대한 모든 메트릭에서 다른 메트릭보다 우수한 성능을 보여 더 복잡한 경우 우리의 실현 가능성을 확인한다. GP-VTON[52]은 Dress Code에 배경 제거 및 포즈 정규화와 같은 추가 데이터 수정을 적용하고, 그들의 테스트 데이터의 일부만을 제공한다는 점에 유의한다. 그럼에도 불구하고, 우리의 OOTDiffusion은 여전히 더 도전적인 원본 테스트 데이터 세트에서 최상의 결과를 달성한다.\n' +
      '\n' +
      '또한, 제안된 방법의 일반화 성능은 Tab. 4에 제시된 교차 데이터세트 평가 결과를 통해 정량적으로 검증되었으며, GP-VTON[52]는 워핑 모듈이 학습 데이터에 심하게 적합하기 때문에 모든 메트릭에서 다른 방법보다 훨씬 뒤처짐을 알 수 있다. 우리의 방법은 배포되지 않은 테스트 데이터 세트에 대한 모든 메트릭을 다시 이끕니다. 전체적으로 위의 관측치(탭. 2~4) 또한 OOTDiffusion이 모든 종류의 시나리오 및 조건에 대해 충실도와 제어성 모두에서 다른 VTON 방법보다 훨씬 우수하다는 것을 보여준다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '이미지 기반 가상 시도 작업에서 달성된 최첨단 성능에도 불구하고, 추가적인 개선을 요구하는 OOTDiffusion에는 여전히 한계가 존재한다. 첫째, 모델이 짝을 이루는 인간과 의복 이미지에 대해 훈련되기 때문에, 긴 드레스를 입은 여성에게 티셔츠를 입히거나, 바지를 입은 남성이 치마를 입히도록 하는 것과 같은 교차 카테고리 가상 트라이온에 대한 완벽한 결과를 얻지 못할 수 있다. 이 문제는 동일한 포즈로 서로 다른 옷을 입은 각 모델의 데이터 세트를 수집함으로써 향후 부분적으로 해결될 수 있다. 또 다른 한계는 원래 인간 이미지의 일부 세부 사항이 근육, 시계 및 문신 등과 같은 가상 시도 후에 변경될 수 있다는 것이다. 확산 모델에 의해 관련 신체 영역이 마스킹되고 재도장되기 때문이다. 따라서 이러한 문제를 해결하기 위해 보다 실용적인 사전 및 사후 처리 방법이 필요하다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 이미지 기반 가상 트라이온을 위한 새로운 LDM 기반 네트워크 구조인 OOTDiffusion을 제안한다. 제안된 아웃피팅 UNet은 의상 특징을 효율적으로 학습하고, 제안된 아웃피팅 융합 프로세스를 통해 노이즈 제거 UNet에 통합하며, 정보 손실은 무시할 수 있다. 의복 특징에 대한 분류기 없는 지침은 훈련에서 제안된 아웃핏 드롭아웃에 의해 활성화되며, 이는 본 방법의 제어 가능성을 더욱 향상시킨다. 고해상도 데이터 세트에 대한 광범위한 실험은 충실도와 제어 가능성 모두에서 다른 VTON 방법보다 우수함을 보여주며, 이는 OOTDiffusion이 가상 트라이온에 대한 광범위한 응용 가능성을 가지고 있음을 나타낸다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '저희 업무를 친절하게 지원하고 홍보해 주신 웨이두, 쉬핑수, 이란예, 치장 등 동료분들께 진심으로 감사드립니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]J. 베커고 징태 브룩스 J. Wang, L. 이림 오양, 장정, 이영 Guo, et al.(2023) 더 나은 캡션을 갖는 이미지 생성 개선. 컴퓨터 과학 참고: https://cdn. openai. com/papers/dall-e-3. pdf2 Cited by: SS1.\n' +
      '*[2]M. 빈코스키, D. J. 서덜랜드, M. Arbel, and A. Gretton (2018) Demystifying mmd gans. ArXiv:1801.01401. 인용: SS1.\n' +
      '*[3]T. Brooks, A. Holynski, and A. A. Efros (2023) Instructpix2pix: 이미지 편집 지시를 따르는 것을 배우는 것. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18392-18402. Cited by: SS1.\n' +
      '*[4]Z. 조달고 마르티네스 사이먼 Wei, and Y. A. Sheikh (2019) OpenPose: part affinity field를 이용한 실시간 다인 2d 포즈 추정. IEEE의 패턴분석 및 기계지능에 관한 연구 인용: SS1.\n' +
      '*[5]Z. 조태 사이먼, S. E. Wei, Y. Sheikh(2017) Part affinity field를 이용한 실시간 다인 2d 자세 추정. CVPR에서 인용: SS1.\n' +
      '*[6]S. 최승 박민 Lee, and J. Choo(2021) Viton-hd: high-resolution virtual try-on via misalignment-aware normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14131-14140. Cited by: SS1.\n' +
      '*[7]N. S. Detlefsen, J. Borovec, J. Schock, A. H. Jha, T. Koker L. Di Liello, D. Stancl, C. Quan, M. 그렉킨과 W. Falcon (2022) Torchmetrics-asuring reproducibility in pytorch. Journal of Open Source Software7 (70), pp. 4101-4102. Cited by: SS1.\n' +
      '*[8]H. 동진 량선 Shen B. Wang H. Lai J. Zhu, Z. Hu, and J. Yin(2019) Towards the multi-pose guided virtual try-on network. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9026-9035. Cited by: SS1.\n' +
      '*[9]E. 페노키, 모렐리, M. 코니아 L. 바랄디, F. 세사리, R. 쿠키아라, R. 가상 트라이온을 위한 이중 분기 협업 변압기입니다. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2247-2251. Cited by: SS1.\n' +
      '*[10]R. 갈영 알라루프 아츠몬, 오 Patashnik, A. H. Bermano, G. Chechik, and D. Cohen-Or (2022) 이미지는 텍스트 역산을 사용하여 텍스트 대 이미지 생성을 개인화하는 하나의 단어 가치가 있다. ArXiv:2208.01618. 인용: SS1.\n' +
      '*[11]C. 지영 송영 게현양 Liu and P. Luo (2021) Disentangled cycle consistency for high-realistic virtual try-on. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16928-16937. Cited by: SS1.\n' +
      '*[12]Y. 지영 송래 장창게 Liu, P. Luo(2021) 파서가 없는 가상 트라이온은 증류 외관 흐름을 통해 이루어진다. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8485-8493. Cited by: SS1.\n' +
      '*[13]I. 좋은 친구, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio(2020) Generative Adversarial Networks. communications of the ACM63(11), pp. 139-144. Cited by: SS1.\n' +
      '*[14]J. 고승 Sun, J. Zhang, J. Si, C. Qian, 및 L. Zhang(2023) 외형 흐름과 함께 고품질 가상 트라이온을 위한 확산 모델의 파워를 길들이기. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 7599-7607. Cited by: SS1.\n' +
      '\n' +
      '* [15] Han, X., Hu, X., Huang, W., Scott, M.R.: Clothflow: A flow-based model for clothed person generation. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 10471-10480 (2019)\n' +
      '* [16] Han, X., Wu, Z., Wu, Z., Yu, R., Davis, L.S.: Viton: An image-based virtual try-on network. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7543-7552 (2018)\n' +
      '* [17] He, S., Song, Y.Z., Xiang, T.: Style-based global appearance flow for virtual try-on. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3470-3479 (2022)\n' +
      '* [18] Hertz, A., Mokady, R., Tenenbaum, J., Aherman, K., Pritch, Y., Cohen-Or, D.: Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626 (2022)\n' +
      '* [19] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems **30** (2017)\n' +
      '* [20] Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022)\n' +
      '* [21] Hu, L., Gao, X., Zhang, P., Sun, K., Zhang, B., Bo, L.: Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv preprint arXiv:2311.17117 (2023)\n' +
      '* [22] Issenhuth, T., Mary, J., Calauzenes, C.: Do not mask what you do not need to mask: a parser-free virtual try-on. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XX 16. pp. 619-635. Springer (2020)\n' +
      '* [23] Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani, M.: Imagic: Text-based real image editing with diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6007-6017 (2023)\n' +
      '* [24] Kim, J., Gu, G., Park, M., Park, S., Choo, J.: Stableviton: Learning semantic correspondence with latent diffusion model for virtual try-on. arXiv preprint arXiv:2312.01725 (2023)\n' +
      '* [25] Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013)\n' +
      '* [26] Kumari, N., Zhang, B., Zhang, R., Shechtman, E., Zhu, J.Y.: Multi-concept customization of text-to-image diffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1931-1941 (2023)\n' +
      '* [27] Lee, S., Gu, G., Park, S., Choi, S., Choo, J.: High-resolution virtual try-on with misalignment and occlusion-handled conditions. In: European Conference on Computer Vision. pp. 204-219. Springer (2022)\n' +
      '* [28] Li, P., Xu, Y., Wei, Y., Yang, Y.: Self-correction for human parsing. IEEE Transactions on Pattern Analysis and Machine Intelligence **44**(6), 3260-3271 (2020)\n' +
      '* [29] Loshchilov, I., Hutter, F.: Fixing weight decay regularization in adam (2018)\n' +
      '* [30] Minar, M.R., Tuan, T.T., Ahn, H., Rosin, P., Lai, Y.K.: Cp-vton+: Clothing shape and texture preserving image-based virtual try-on. In: CVPR Workshops. vol. 3, pp. 10-14 (2020)\n' +
      '* [31] Mokady, R., Hertz, A., Aherman, K., Pritch, Y., Cohen-Or, D.: Null-text inversion for editing real images using guided diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6038-6047 (2023)* [32] Morelli, D., Baldrati, A., Cartella, G., Cornia, M., Bertini, M., Cucchiara, R.: Ladivton: Latent diffusion textual-inversion enhanced virtual try-on. arXiv preprint arXiv:2305.13501 (2023)\n' +
      '* [33] Morelli, D., Fincato, M., Cornia, M., Landi, F., Cesari, F., Cucchiara, R.: Dress code: High-resolution multi-category virtual try-on. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2231-2235 (2022)\n' +
      '* [34] Mou, C., Wang, X., Xie, L., Wu, Y., Zhang, J., Qi, Z., Shan, Y., Qie, X.: T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453 (2023)\n' +
      '* [35] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021)\n' +
      '* [36] Parmar, G., Kumar Singh, K., Zhang, R., Li, Y., Lu, J., Zhu, J.Y.: Zero-shot image-to-image translation. In: ACM SIGGRAPH 2023 Conference Proceedings. pp. 1-11 (2023)\n' +
      '* [37] Parmar, G., Zhang, R., Zhu, J.Y.: On aliased resizing and surprising subtleties in gan evaluation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11410-11420 (2022)\n' +
      '* [38] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023)\n' +
      '* [39] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [40] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [41] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. pp. 234-241. Springer (2015)\n' +
      '* [42] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aherman, K.: Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22500-22510 (2023)\n' +
      '* [43] Saharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans, T., Fleet, D., Norouzi, M.: Palette: Image-to-image diffusion models. In: ACM SIGGRAPH 2022 Conference Proceedings. pp. 1-10 (2022)\n' +
      '* [44] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems **35**, 36479-36494 (2022)\n' +
      '* [45] Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D.J., Norouzi, M.: Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence **45**(4), 4713-4726 (2022)\n' +
      '* [46] Simon, T., Joo, H., Matthews, I., Sheikh, Y.: Hand keypoint detection in single images using multiview bootstrapping. In: CVPR (2017)* [47] Tumanyan, N., Geyer, M., Bagon, S., Dekel, T.: Plug-and-play diffusion features for text-driven image-to-image translation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1921-1930 (2023)\n' +
      '* [48] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems **30** (2017)\n' +
      '* [49] Wang, B., Zheng, H., Liang, X., Chen, Y., Lin, L., Yang, M.: Toward characteristic-preserving image-based virtual try-on network. In: Proceedings of the European conference on computer vision (ECCV). pp. 589-604 (2018)\n' +
      '* [50] Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing **13**(4), 600-612 (2004)\n' +
      '* [51] Wei, S.E., Ramakrishna, V., Kanade, T., Sheikh, Y.: Convolutional pose machines. In: CVPR (2016)\n' +
      '* [52] Xie, Z., Huang, Z., Dong, X., Zhao, F., Dong, H., Zhang, X., Zhu, F., Liang, X.: Gp-vton: Towards general purpose virtual try-on via collaborative local-flow global-parsing learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 23550-23559 (2023)\n' +
      '* [53] Yang, B., Gu, S., Zhang, B., Zhang, T., Chen, X., Sun, X., Chen, D., Wen, F.: Paint by example: Exemplar-based image editing with diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18381-18391 (2023)\n' +
      '* [54] Yang, H., Zhang, R., Guo, X., Liu, W., Zuo, W., Luo, P.: Towards photo-realistic virtual try-on by adaptively generating-preserving image content. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 7850-7859 (2020)\n' +
      '* [55] Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023)\n' +
      '* [56] Yu, J., Xu, Y., Koh, J.Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B.K., et al.: Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789 **2**(3), 5 (2022)\n' +
      '* [57] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3836-3847 (2023)\n' +
      '* [58] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 586-595 (2018)\n' +
      '* [59] Zhao, W., Bai, L., Rao, Y., Zhou, J., Lu, J.: Unipc: A unified predictor-corrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [60] Zhu, L., Yang, D., Zhu, T., Reda, F., Chan, W., Saharia, C., Norouzi, M., Kemelmacher-Shlizerman, I.: Tryondiffusion: A tale of two unets. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4606-4615 (2023)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>