<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# The FinBen: An Holistic Financial Benchmark for Large Language Models\n' +
      '\n' +
      'Qianqian Xie\\({}^{a}\\), Weiguang Han\\({}^{b}\\), Zhengyu Chen\\({}^{b}\\), Ruoyu Xiang\\({}^{a}\\), Xiao Zhang\\({}^{a}\\), Yueru He\\({}^{a}\\), Mengxi Xiao\\({}^{b}\\), Dong Li\\({}^{b}\\), Yongfu Dai\\({}^{g}\\), Duanyu Feng\\({}^{g}\\), Yijing Xu\\({}^{a}\\), Haoqiang Kang\\({}^{e}\\), Ziyan Kuang\\({}^{j}\\), Chenhan Yuan\\({}^{c}\\), Kailai Yang\\({}^{c}\\), Zheheng Luo\\({}^{c}\\), Tianlin Zhang\\({}^{c}\\), Zhiwei Liu\\({}^{c}\\), Guojun Xiong\\({}^{j}\\), Zhiyang Deng\\({}^{i}\\), Yuechen Jiang\\({}^{i}\\), Zhiyuan Yao\\({}^{i}\\), Haohang Li\\({}^{i}\\), Yangyang Yu\\({}^{i}\\), Gang Hu\\({}^{h}\\), Jiajia Huang\\({}^{k}\\), Xiao-Yang Liu\\({}^{e}\\), Alejandro Lopez-Lira\\({}^{d}\\), Benyou Wang\\({}^{f}\\), Yanzhao Lai\\({}^{m}\\), Hao Wang\\({}^{g}\\), Min Peng\\({}^{b*}\\), Sophia Ananiadou\\({}^{c}\\), Jimin Huang\\({}^{a}\\)\n' +
      '\n' +
      '\\({}^{a}\\)The Fin AI, \\({}^{b}\\)Wuhan University, \\({}^{c}\\)The University of Manchester, \\({}^{d}\\)University of Florida,\n' +
      '\n' +
      '\\({}^{e}\\)Columbia University, \\({}^{f}\\)The Chinese University of Hong Kong, Shenzhen,\n' +
      '\n' +
      '\\({}^{g}\\)Sichuan University, \\({}^{h}\\)Yunnan University, \\({}^{i}\\)Stevens Institute of Technology\n' +
      '\n' +
      '\\({}^{j}\\)Stony Brook University, \\({}^{k}\\)Nanjin Audit University,\n' +
      '\n' +
      '\\({}^{l}\\)Jiangxi Normal University, \\({}^{m}\\)Southwest Jiaotong University\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of thorough evaluations and the complexity of financial tasks. This along with the rapid development of LLMs, highlights the urgent need for a systematic financial evaluation benchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive open-sourced evaluation benchmark, specifically designed to thoroughly assess the capabilities of LLMs in the financial domain. FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs\' cognitive abilities in inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, and more. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals insights into their strengths and limitations within the financial domain. The findings indicate that GPT-4 leads in quantification, extraction, numerical reasoning, and stock trading, while Gemini shines in generation and forecasting; however, both struggle with complex extraction and forecasting, showing a clear need for targeted enhancements. Instruction tuning boosts simple task performance but falls short in improving complex reasoning and forecasting abilities. FinBen seeks to continuously evaluate LLMs in finance, fostering AI development with regular updates of tasks and models1.\n' +
      '\n' +
      'Footnote 1: [https://github.com/The-FinAI/PIXIU](https://github.com/The-FinAI/PIXIU)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Recently, Large Language Models (LLMs) Brown et al. (2020) such as ChatGPT2 and GPT-4 (OpenaAI, 2023), have reshaped the field of natural language processing (NLP) and exhibited remarkable capabilities in specialized domains across mathematics, coding, medicine, law, and finance Bubeck et al. (2023). With their increasing model size and extensive pre-training data, LLMs have developed the emergent capacity for in-context learning, enabling them to perform remarkably across a wide range of domain-specific tasks in zero-shot and few-shot settings Wei et al. (2023). Within the financial domain, recent several studies Xie et al. (2023); Lopez-Lira and Tang (2023); Li et al. (2023); Xie et al. (2023) have shown the great potential of advanced LLMs such as GPT-4 on financial text analysis and prediction tasks. While their potential is evident, a comprehensive understanding of their capabilities and limitations for finance, remains largely unexplored. This is due to a lack of extensive evaluation studies and benchmarks, and the inherent complexities associated with the professional nature of financial tasks.\n' +
      '\n' +
      'Footnote 2: [https://openai.com/chatgpt](https://openai.com/chatgpt)\n' +
      '\n' +
      'Existing financial domain evaluation benchmarks including FLUE Shah et al. (2022), BBT-CFLEB Lu et al. (2023), and PIXIU Xie et al. (2023), have a limited scope and are solely focused on financial NLP tasks, primarily targeting language understanding abilities where LLMs have already been extensively evaluated. As shown in Table 1, they fail to capture other crucial facets of the financial domain, such as comprehending and extracting domain-specific financial knowledge andresolving realistic financial tasks. As such, their efficacy in evaluating and understanding LLM performance is limited.\n' +
      '\n' +
      'Furthermore, while there are newly released benchmarks in the general domain, such as MMLU Hendrycks et al. (2020), HELM Liang et al. (2022) and BIG-bench Srivastava et al. (2023) compiling massive tasks across numerous institutions, they do not extend to the financial domain. The fast progression of LLMs, coupled with an incomplete understanding of their abilities and behavior, highlights the need for a systematic financial evaluation benchmark dedicated to these models.\n' +
      '\n' +
      'How should an effective systematic financial evaluation benchmark be designed? We believe it should fulfill the following criteria: 1) Broad coverage: It should cover a broad spectrum of tasks to capture the financial domain\'s complexity, incorporating both linguistic understanding and diverse skills like knowledge extraction, text generation, and numerical reasoning et al. 2) Real-world application orientation: The benchmark should focus on real-world scenarios, including stock market analysis and trading, highlighting LLMs\' practical application capabilities. 3) Inclusion of financial domain-specific characteristics: It also needs to address the unique aspects of finance, embedding tasks that demand specific knowledge, terminology, and concepts, demonstrating LLMs\' proficiency in the field. 4) Consideration of human-level cognition: It should gauge human-like cognitive abilities, evaluating LLMs on decision-making, problem-solving, and abstract reasoning within financial contexts.\n' +
      '\n' +
      'To bridge this gap, we propose FinBen, the first open-sourced3 comprehensive evaluation benchmark designed for assessing the capabilities of LLMs in the financial domain. As shown in Figure 1, FinBen includes 35 datasets spanning 23 financial tasks organized into three Spectrums of difficulty inspired by the Cattell-Horn-Carroll (CHC) theory Schneider and McGrew (2012) in the fields of psychology and education, to assess LLMs across various cognitive domains, including inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, fluid intelligence, and general intelligence. Spectrum I is comprised of foundational tasks including Quantification, Extraction, and Numerical Understanding, laying the groundwork for basic cognitive skills. Moving up, Spectrum II delves into more complex Generation and Forecasting tasks, demanding enhanced cognitive involvement. At the apex, Spectrum III focuses on the sophisticated stock trading task, exemplifying the application of General Intelligence.\n' +
      '\n' +
      'Footnote 3: We will release all resources to the research community.\n' +
      '\n' +
      'In align with the above criteria, FinBen distinguishes from existing benchmarks from the breadth and depth of its coverage, as well as its uniquely tailored focus on the financial domain: 1) **Wide coverage**: FinBen integrates classic NLP tasks (text analysis, knowledge extraction, question answering) with finance-specific challenges (numeric labeling) and innovates by assessing LLMs on real-world financial applications (stock prediction, credit scoring) and for the **first time** directly assess the trading performance of LLMs. This broad approach unveils LLMs\' strengths and limitations in finance comprehensively. 2) **Multi-data modality and diversity of text types**: FinBen distinguishes itself by embracing diverse data forms and text types for its tasks, including news, tweets, earnings calls, financial documents, tables, and time-series data. This variety facilitates a thorough assessment of LLMs\' comprehension and generation of financial content, highlighting their real-world utility. 3) **Diverse difficulty levels**: the FinBen incorporates tasks of varying difficulty levels, from simpler fundamental tasks like news headline classification, to advanced cognitive engagement tasks such as the stock movement prediction, and even more complex general intelligence tasks that even challenge humans, such as stock trading. This range enables a nuanced evaluation of LLMs, fully mapping their strengths and weaknesses in finance.\n' +
      '\n' +
      'We test 15 representative general LLMs such as\n' +
      '\n' +
      'Figure 1: Evaluation framework of FinBen.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      'sentiments within financial narratives. _1) Sentiment analysis_ focuses on extracting sentiment information from financial texts. We utilize two datasets: the Financial Phrase Bank (FPB) (Malo et al., 2014), FiQA-SA (Maia et al., 2018), and TSA (Cortis et al., 2017) dataset. _2) News headline classification_ analyzes additional information, like price movements in financial texts, using the Headlines dataset (Sinha and Khandait, 2021), which includes news about "gold" from 2000 to 2019 and their 9 corresponding tags. _3) Hawkish-Dovish classification_ aims to classify sentences from monetary policy texts as \'hawkish\' or \'dovish,\' focusing on the nuanced language and economic implications of financial texts, using the FOMC (Shah et al., 2023) dataset. _4) Argument unit classification_ categorizes sentences as claims or premises using the FinArg AUC dataset (Sy et al., 2023). _5) Argument relation detection_ identifies relationships (attack, support, or irrelevant) between social media posts using the FinArg ARC dataset (Sy et al., 2023). _6) Multi-class classification_ targets categorizing a variety of financial texts, including analyst reports, news articles, and investor comments, utilizing the MultiFin dataset (Jorgensen et al., 2023). _7) Deal completeness classification_ predicts if mergers and acquisitions events are "completed" or remain "rumors" based on news and tweets, employing the MA dataset (Yang et al., 2020). _8) ESG issue identification_ focuses on detecting Environmental, Social, and Governance (ESG) concerns in financial documents using the MLESG dataset (Chen et al., 2023). For all datasets, evaluation utilizes the accuracy and F1 Score.\n' +
      '\n' +
      '**Extraction.** The extraction task including 5 datasets from 4 information extraction tasks, evaluating LLMs\' ability to accurately retrieve specific financial information from large datasets, a process tied closely to Associative Memory (Ma). _1) Named entity recognition_ extracts entities like LOCATION, ORGANIZATION, and PERSON from financial agreements and SEC filings, using the NER (Alvarado et al., 2015) and FINERORD (Shah et al., 2023) datasets. _2) Relation extraction_ identifies relationships such as "product/material produced" and "manufacturer" in financial news and earnings transcripts with the FINRED dataset (Sharma et al., 2022). _3) Causal classification_ discerns whether sentences from financial news and SEC filings convey causality us\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l} \\hline \\hline\n' +
      '**Data** & **Task** & **Valid** & **Test** & **Evaluation** & **License** \\\\ \\hline FPB (Malo et al., 2014) & sentiment analysis & 775 & 970 & F1, Accuracy & CC BY-S A 3.0 \\\\ FiQA-SA (Maia et al., 2018) & sentiment analysis & 188 & 235 & F1 & Public \\\\ TSA (Cortis et al., 2017) & sentiment analysis & - & 561 & F1, Accuracy & CC BY-NC-S A 4.0 \\\\ Headlines (Sinha and Khandait, 2021) & news headline classification & 1,141 & 2,283 & Avg F1 & CC BY-S A 3.0 \\\\ FOMC (Shah et al., 2023) & hawkish-dovish classification & - & 496 & F1, Accuracy & CC BY-NC-4.0 \\\\ FinArg-ACy (Sy et al., 2023) & argument unit classification & - & 969 & F1, Accuracy & CC BY-NC-S A 4.0 \\\\ FinArg-ARC (Sy et al., 2023) & argument relation classification & - & 496 & F1, Accuracy & CC BY-NC-S A 4.0 \\\\ MultiFiz (Jorgensen et al., 2023) & multi-class classification & - & 690 & F1, Accuracy & Public \\\\ MA (Yang et al., 2020) & deal completeness classification & - & 500 & accuracy\\({}_{\\text{F1}}\\) & Public \\\\ MLESG (Chen et al., 2023) & ESG issue identification & - & 300 & accuracy\\({}_{\\text{F1}}\\) & CC BY-NC-ND \\\\ \\hline NER (Alvarado et al., 2015) & named entity recognition & 103 & 980 & Entity F1 & CC BY-NA 3.0 \\\\ FINER-ORD (Shah et al., 2023) & named entity recognition & - & 1080 & Entity F1 & CC BY-NC-4.0 \\\\ FinRED (Sharma et al., 2022) & relation extraction & - & 1,068 & F1, Entity F1 & Public \\\\ SC (Manila et al., 2020) & causal classification & - & 8,630 & F1,Failor F1 & CC BY 4.0 \\\\ CD (Marako et al., 2020) & causal detection & - & 226 & F1,Entity F1 & CC BY 4.0 \\\\ \\hline FiQA (Chen et al., 2021) & question answering & 883 & 1,147 & EM Accuracy & MIT License \\\\ TAPQA (Zhu et al., 2021) & question answering & - & 1,668 & F1,EM Accuracy & MIT License \\\\ ConFifQA (Chen et al., 2022) & question answering & - & 2,210 & 1,490 & EM Accuracy & MIT License \\\\ FNNL (Sharma et al., 2023) & numeric labeling & - & 318 & F1,EM Accuracy & Public \\\\ FSRL (Lamm et al., 2018) & token classification & - & 97 & F1, EM Accuracy & MIT License \\\\ \\hline ECTN (Mukherjee et al., 2022) & text summarization & - & 495 & ROUGE, BERTScore, BARTScore & Public \\\\ EDTSum (Zhu et al., 2021) & text summarization & - & 2000 & ROUGE, BERTScore, BARTScore & Public \\\\ \\hline BigData22 (Soun et al., 2022) & stock movement prediction & 798 & 1,470 & Accuracy, MCC & Public \\\\ ACL18 (Xu and Cohen, 2018) & stock movement prediction & 2,560 & 3,720 & Accuracy, MCC & MIT License \\\\ CIKMish (Wu et al., 2018) & stock movement prediction & 431 & 1,140 & Accuracy, MCC & Public \\\\ German (Hofmann, 1994) & credit scoring & - & 1000 & F1, MCC & CC BY 4.0 \\\\ Australian (Quinlan) & credit scoring & - & 690 & F1, MCC & CC BY 4.0 \\\\ LendingClub (Feng et al., 2023) & credit scoring & 1,344 & 2,690 & F1, MCC & CC1.0 \\\\ ccf (Feng et al., 2023) & fraud detection & 1,138 & 2,278 & F1, MCC & (DDcl) v1.0 \\\\ ccfand (Feng et al., 2023) & fraud detection & 1,907 & 2,697 & F1, MCC & Public \\\\ polish (Feng et al., 2023) & financial distress identification & 868 & 1,736 & F1, MCC & CC BY 4.0 \\\\ Taiwan (Feng et al., 2023) & financial distress identification & 681 & 1,364 & F1, MCC & CC BY 4.0 \\\\ ProtoSegur (Feng et al., 2023) & chain analysis & 1,189 & 2,381 & F1, MCC & Public \\\\ travelinsurance (Feng et al., 2023) & chain analysis & - & 3,800 & F1, MCC & (ODbl) v1.0 \\\\ fintrade (Yu et al., 2023) & stock trading & - & 3,384 & CR, SR, DV, AV, MD & MIT License \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: The tasks, datasets, data statistics, and evaluation metrics included in FinBen.\n' +
      '\n' +
      'ing the SC dataset (Mariko et al., 2020). _4) Causal detection_ identifies cause and effect spans in financial texts with the CD dataset (Mariko et al., 2020). The evaluation of these tasks is focused on the F1 score (Goutte and Gaussier, 2005) and Entity F1 score (Derczynski, 2016).\n' +
      '\n' +
      '**Understanding.** The understanding task includes 5 datasets from 4 numerical reasoning tasks, challenging LLMs to interpret and analyze complex numerical data and intricate financial statistics, associted with the Quantitative Reasoning (Gq) ability. _1) Question answering_ focuses on solving questions through multi-step numerical reasoning with financial reports and tables, utilizing the FinQA (Chen et al., 2021) and TATQA (Zhu et al., 2021) dataset. _2) Multi-turn question answering_ is an extension on QA with multi-turn questions and answers based on financial earnings reports and tables, using the ConvFinQA dataset (Chen et al., 2022). _3) Numeric labeling_ aims at tagging numeric spans in financial documents using 2,794 labels with the FNXL dataset (Sharma et al., 2023). _4) Token classification_ aims at identifying common attributes and comparative elements in textual analogies by extracting analogy frames, utilizing the FSRL dataset (Lamm et al., 2018). Entity F1 score (Derczynski, 2016) and the Exact Match Accuracy (EMAcc) metric (Kim et al., 2023) are used to evaluate these tasks.\n' +
      '\n' +
      '### Spectrum II: Advanced Cognitive Engagement\n' +
      '\n' +
      'Spectrum II has 14 datasets across 6 tasks designed to assess the Generation (Crystallized Intelligence) and Forecasting (Fluid Intelligence) capabilities of LLMs, requiring deeper cognitive engagement.\n' +
      '\n' +
      '**Generation.** The generation task gauges the models\' proficiency in producing coherent, informative, and relevant text outputs, involving the Crystallized Intelligence (Gc). We focus on the _text summarization_ task utilizing the ECTSUM (Mukherjee et al., 2022) dataset for summarizing earnings call transcripts and the EDTSUM (Zhou et al., 2021) dataset for abstracting financial news articles into concise summaries. It\'s evaluated using ROUGE scores (Lin, 2004), BERTScore (Zhang et al., 2019), and BART Score (Yuan et al., 2021), metrics that quantify to measure the alignment, factual consistency, and information retention between machine-generated and expert summaries.\n' +
      '\n' +
      '**Forecasting.** The forecasting task leverages Fluid Intelligence (Gf), challenging models to adaptively predict future market and investor behaviors from emerging patterns. It includes 12 datasets from 5 forecasting tasks. _1) Stock movement prediction_ focuses on forecasting stock directions as either positive or negative, based on historical prices and tweets, utilizing three datasets: BigData22 (Soun et al., 2022), ACL18 (Xu and Cohen, 2018) and CIKM18 (Wu et al., 2018). _2) Credit scoring_ classifies individuals as "good" or "bad" credit risks using historical customer data, employing datasets including: German (Hofmann, 1994), Australia (Quinlan) and LendingClub (Feng et al., 2023). _3) Fraud detection_ involve categorizes transactions as "fraudulent" or "non-fraudulent", using two datasets: ccf (Feng et al., 2023) and ccfraud (Feng et al., 2023). _4) Financial distress identification_ aims to predict a company\'s bankruptcy risk, using the polish (Feng et al., 2023) and taiwan dataset (Feng et al., 2023). _5) Claim analysis_ anonymizes client data for privacy, labeling a "target" to indicate claim status, using two datasets: PortoSeguro (Feng et al., 2023) and travelinsurance (Feng et al., 2023). F1 score and Matthews correlation coefficient (MCC) (Chicco and Jurman, 2020) are used for evaluating these tasks.\n' +
      '\n' +
      '### Spectrum III: General Intelligence\n' +
      '\n' +
      '**Trading.** Strategic decision-making in Trading (Punt, 2017), categorized under Spectrum III, is the pinnacle task for financial LLMs, emphasizing their use of General Intelligence (g). This task evaluates the model\'s proficiency in synthesizing diverse information to formulate and implement trading strategies, a challenge even for experts, representing the highest level of cognitive capability in financial analysis. The SOTA financial LLM agent _FinMem_(Yu et al., 2023) are used to evaluate LLMs on sophisticated stock decisions, based on the FinTrade dataset we curated of seven major stocks, simulating real-world trading through historical prices, news, and sentiment analysis. Performance is measured by Cumulative Return (CR) (Ariel, 1987), Sharpe Ratio (SR) (Sharpe, 1998), Daily (DV) and Annualized volatility (AV) (Zhou et al., 2023), and Maximum Drawdown (MD) (Magdon-Ismail and Atiya, 2004), offering a comprehensive assessment of profitability, risk management, and decision-making prowess.\n' +
      '\n' +
      'Evaluation\n' +
      '\n' +
      'We evaluate the zero-shot and few-shot performance of 15 representative general LLMs and financial LLMs on the FinBen benchmark, including: 1) ChatGPT: An instruction-following LLM with 175B parameters developed by OpenAI. 2) GPT-4 (OpenAI, 2023b): A powerful instruction-following LLM with approximately 1T parameters, proposed by OpenAI. 3) Gemini Pro (Team et al., 2023): A multimodal AI LLM with 50T parameters, released by Google. 4) LLaMA2-70B (Touvron et al., 2023): An instruction-following LLM with 70B parameters developed by MetaAI. 5) ChatGLM3-6B (Du et al., 2022): A conversational LLM with 6B parameters, jointly released by Zhipu AI and Tsinghua KEG. 6) Baichuan2-6B (Baichuan, 2023): An open-source LLM with 6B parameters, launched by Baichuan Intelligent Technology. 7) InternLM-7B (Team, 2023b): An open-sourced 7B parameter base model tailored for practical scenarios, proposed by SenseTime. 9) Falcon7B (Almazrouei et al., 2023): A 7B parameter causal decoder-only LLM model trained on 1500B tokens of RefinedWeb enhanced with curated corpora. 10) Mixrtal 8\\(\\times\\)7B (Jiang et al., 2024): A LLM with the Sparse Mixture of Experts (SMoE) architecture. 11) Code Llama-7B (Roziere et al., 2023): An open-source LLM model for generating programming code, launched by Meta AI with 7B parameters. 12) FinGPT (Yang et al., 2023a): An 7B instruction finetuned financial LLM with sentiment analysis tasks. 13) FinMA-7B (Xie et al., 2023b): An 7B instruction finetuned financial LLM with multiple NLP and forecasting tasks. 14) DISCFinLLM (Chen et al., 2023c): An open-sourced financial LLM, fine-tuned from Baichuan-13B-Chat (Baichuan, 2023). 15) CFGPT (Li et al., 2023a): An open-source LLM, specifically designed for the financial sector and trained on Chinese financial datasets, which comprises 7 billion parameters. All experiments are conducted exclusively using 5 NVIDIA TITAN RTX graphics GPUs and 2 NVIDIA GeForce RTX 3090 GPUs, taking approximately 20 hours to complete. On average, 2 GPUs are allocated per experiment, amounting to a total of approximately 20400 GPU hours.\n' +
      '\n' +
      '## 4 Results\n' +
      '\n' +
      'Table 3 and Table 4 shows the performance of 12 representative LLMs on all datasets in the FinBen.\n' +
      '\n' +
      '### Foundamental Tasks Analysis\n' +
      '\n' +
      'From Table 3, for fundamental tasks, we can see that GPT-4 stands out with the highest average performance, closely followed by ChatGPT, and Gemini. Among all open-sourced LLMs, the financial LLM FinMA-7B showcases superior performance on several classification tasks, such as FPB, even exceeding larger models like GPT-4. This is attributed to its tailored instruction tuning on the training datasets. For general-purpose LLMs, LLaMA2 70B leads in average performance, due to the large model size. Among models tailored for the Chinese language, ChatGLM2-6B outperforms InternLM 7B in average performance, indicating its effectiveness in handling financial tasks. However, CFGPT sft-7B-Full, fine-tuned on Chinese financial data, exhibits limited improvement on a few datasets and even declining performance on others like MultiFin compared to its base model InternLM 7B. This trend suggests a language-based discrepancy, highlighting that fine-tuning with Chinese data may adversely affect performance on English tasks, underscoring the complexities of cross-lingual adaptation in model training.\n' +
      '\n' +
      'Notably, in **quantification** datasets such as Headlines, models like Gemini and other financially tuned LLMs, including FinMA-7B, perform on par with or even better than GPT-4. However, when tackling **understanding** tasks in datasets like FinQA and ConvFinQA, GPT-4 and ChatGPT significantly outperform others, highlighting the limited numerical reasoning capabilities of models like Gemini and LLaMA2-70B. Challenges persist in **extraction** datasets requiring complex information extraction and numeric labeling, such as FinRED, CD, FNXL, and FSRL, where all models, including GPT-4, fall short, indicating a need for further enhancement in these areas.\n' +
      '\n' +
      'In conclusion, SOTA LLMs like GPT-4 exhibit strong performance across quantification tasks. However, there\'s a clear gap in numerical reasoning and complex information extraction tasks, pinpointing the necessity for further development. Instruction tuning has shown to enhance performance significantly, suggesting a valuable approach for improving model capabilities in specialized financial tasks. The results highlight the complexity of cross-lingual model tuning and the importance of careful language consideration in enhancing LLMs\' effectiveness across diverse financial tasks.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      'summarization dataset, illustrating its prowess in generating coherent summaries. Nevertheless, all models face challenges with extractive summarization, which demands the generation of precise label sequences for sentences. In the **forecasting** task, Gemini distinguishes itself across most datasets, except in the Australian credit scoring dataset, where GPT-4 demonstrates superior performance.\n' +
      '\n' +
      'Among open-source LLMs, LLaMA2 70B stands out in text summarization, whereas LLaMA2-7B-chat excels in forecasting tasks. Despite instruction tuning with datasets like BigData22 and ACL18, FinMA 7B lags behind peers such as Falcon 7B in forecasting performance, underscoring the need for more effective improvement strategies. CFGPT sft-7B-Full consistently shows a decrease in performance compared to its foundational model, InternLM 7B. For forecasting, it is crucial to acknowledge that all LLMs do not meet expected outcomes and fall behind traditional methodologies. This consistent observation with existing studies [23, 24] underlines a notable deficiency in LLMs\' capacity to tackle advanced cognitive tasks as effectively as conventional methods.\n' +
      '\n' +
      'This analysis reveals significant potential for enhancement in LLMs, including industry leaders like GPT-4 and Gemini, particularly in text generation and forecasting tasks that demand higher cognitive skills.\n' +
      '\n' +
      '### General Intelligence Tasks Analysis\n' +
      '\n' +
      'The comparative analysis of various Large Language Models (LLMs) on the complex task of stock trading, which demands a high degree of general intelligence, is presented in Table 45. The results indicate a superior performance of all LLMs over the traditional Buy & Hold strategy, highlighting their efficacy in formulating more advantageous trading decisions. Among the evaluated LLMs, GPT-4 distinguishes itself by attaining the highest Sharpe Ratio (SR), exceeding 1. This achievement underscores GPT-4\'s proficiency in optimizing profit against the risk, a capability that appears somewhat diminished in other LLMs, which tend to expose investors to higher risk for lesser returns. Additionally, GPT-4 demonstrates the minimal Max Drawdown (MDD), suggesting that it limits potential losses more effectively than its counterparts, thereby offering a more secure investment avenue.\n' +
      '\n' +
      'Footnote 5: For detail trading performance, please see Appendix E\n' +
      '\n' +
      'In contrast, ChatGPT exhibits significantly lower performance metrics, indicating limitations in its financial decision-making capabilities. Gemini, on the other hand, secures the position of second-best performer, showcasing lower risk and volatility in comparison to GPT-4, yet maintaining commendable returns. When considering open-source models, it is observed that LLaMA-70B, despite its lower volatility, yields the least profit among the LLMs, highlighting a trade-off between risk management and profitability.\n' +
      '\n' +
      'For smaller models with parameters less than 70 billion, a marked inability to adhere to trading instructions consistently across transactions is noted, attributed to their limited comprehension, extraction capabilities, and constrained context windows. This limitation underscores the critical challenges smaller LLMs face in tasks requiring intricate financial reasoning and decision-making, thereby spotlighting the necessity for more advanced models to tackle such high-level cognitive tasks effectively.\n' +
      '\n' +
      'In essence, the exceptional performance of LLMs in the stock trading task illuminates their capacity to embody general intelligence within the financial domain. This capacity, rooted in the integration of diverse cognitive skills and the application of these skills to real-world financial challenges, heralds a new era of financial analysis and decision-making. Our findings, thereby, not only affirm the significant potential of LLMs in navigating the complexities of financial markets but also suggest a promising trajectory for their further development and application in tasks demanding a high level of general intelligence.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'In this work, we introduce a comprehensive financial benchmark the FinBen specifically designed for evaluating LLMs in the financial domain. This benchmark encompasses 35 diverse datasets from 23 tasks, organized into three spectrums of difficulty. Unlike previous benchmarks in the financial domain, the FinBen extends its evaluation to encompass a broad spectrum of tasks, including quantification, extraction, understanding, generation, forecasting. Notably, for the first time, it incorporates a direct trading task through an agent-based evaluation framework. Our comprehensive evaluation of 15 representative LLMs yields several key insights: 1) GPT-4 emerges as the top performer in tasks related to quantification, extraction, un derstanding, and trading, whereas Gemini leads in generation and forecasting tasks. 2) While existing LLMs demonstrate commendable performance on foundational tasks, their effectiveness on more cognitively demanding tasks and those requiring general intelligence appears constrained. 3) The findings highlight the capacity of LLMs to directly inform trading decisions, suggesting a promising avenue for future research. Moving forward, we aim to expand FinBen to encompass additional languages and a wider array of financial trading tasks, further broadening the benchmark\'s applicability and utility in advancing the field of financial LLMs.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'Despite the groundbreaking efforts to benchmark LLMs in the financial domain through the FinBen, we acknowledge several inherent limitations that could impact the benchmark\'s effectiveness and applicability:\n' +
      '\n' +
      '**Dataset Size Limitations**: A primary challenge faced in the development of the FinBen is the restricted size of available datasets, a common issue in the niche field of open-source financial data. This limitation may affect the depth of the models\' financial understanding and their ability to generalize across the full spectrum of financial contexts.\n' +
      '\n' +
      '**Model Size Limitations**: Due to computational resource constraints, our evaluation was limited to the LLaMA 70B model. This restriction potentially overlooks the capabilities and performance nuances that larger or differently architected models might demonstrate on FinBen\'s comprehensive task suite. **Generalizability**: The tasks, particularly those involving trading and forecasting, are predominantly based on data from American markets and English-language texts. This focus may limit the benchmark\'s applicability to global financial markets, where linguistic diversity and unique market dynamics play a crucial role. **Potential Negative Impacts**: While the FinBen aims to propel the field of financial language understanding forward, it is crucial to consider the potential for misuse, such as the propagation of financial misinformation or the exertion of unethical influence on markets. These risks underscore the importance of responsible usage and further safeguards in the deployment of LLMs trained or evaluated with the FinBen6.\n' +
      '\n' +
      'Footnote 6: For a detailed ethical and legal statement concerning this work, please see Appendix.\n' +
      '\n' +
      '### Ethical Statement\n' +
      '\n' +
      'The development and dissemination of the FinBen by the authors carry full responsibility for any potential violation of rights or arising legal issues. Diligent efforts have been undertaken to ensure the construction of the FinBen respects privacy and conforms to established ethical guidelines. The datasets compiled within FinBen are shared under the MIT license, with the expectation that users agree to adhere to its conditions.\n' +
      '\n' +
      'This manuscript, inclusive of any associated source codes, datasets, and appendices ("Material"), is designated exclusively for academic and educational pursuits. It is crucial to acknowledge that the Material does not provide financial, legal, or investment counsel, nor should it be utilized as a foundation for any form of decision-making.\n' +
      '\n' +
      'While the authors have exerted reasonable diligence to verify the accuracy and reliability of the Material, no explicit or implied warranty is extended regarding its completeness or suitability for any specific application. The authors, along with their affiliated entities, absolve themselves of liability for any losses, damages, or other consequences, whether direct or indirect, that may emanate from the employment or reliance upon the Material. It is incumbent upon the user to seek professional consultation for financial, legal, or investment determinations.\n' +
      '\n' +
      'By referencing or employing this Material, individuals consent to indemnify, defend, and hold the authors, along with any affiliated organizations or persons, harmless against any claims or damages that may arise from such utilization.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The Falcon series of open language models. _arXiv preprint arXiv:2311.16867_.\n' +
      '* Alvarado et al. (2015) Julio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. 2015. Domain adaption of named entity recognition to support credit risk assessment. In _Proceedings of the Australasian Language Technology Association Workshop 2015_, pages 84-90.\n' +
      '* Araci (2019) Dogu Araci. 2019. Finbert: Financial sentiment analysis with pre-trained language models.\n' +
      '* Ariel (1987) Robert Ariel. 1987. A monthly effect in stock returns. _Journal of financial economics_, 18(1):161-174.\n' +
      '* Arizriz et al. (2017)Baichuan. 2023. Baichuan 2: Open large-scale language models. _arXiv preprint arXiv:2309.10305_.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901.\n' +
      '* Bubeck et al. (2023) Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_.\n' +
      '* Chen et al. (2023a) Chung-Chi Chen, Yu-Min Tseng, Juyeon Kang, Anais Lhuissier, Min-Yuh Day, Teng-Tsai Tu, and Hsin-Hsi Chen. 2023a. Multi-lingual esg issue identification. In _Proceedings of the Fifth Workshop on Financial Technology and Natural Language Processing and the Second Multimodal AI For Financial Forecasting_, pages 111-115.\n' +
      '* Chen et al. (2023b) Wei Chen, Qiushi Wang, Zefei Long, Xianyin Zhang, Zhongtian Lu, Bingxuan Li, Siyuan Wang, Jiarong Xu, Xiang Bai, Xuanjing Huang, and Zhongyu Wei. 2023b. Disc-finllm: A chinese financial large language model based on multiple experts fine-tuning.\n' +
      '* Chen et al. (2023c) Wei Chen, Qiushi Wang, Zefei Long, Xianyin Zhang, Zhongtian Lu, Bingxuan Li, Siyuan Wang, Jiarong Xu, Xiang Bai, Xuanjing Huang, et al. 2023c. Disc-finllm: A chinese financial large language model based on multiple experts fine-tuning. _arXiv preprint arXiv:2310.15205_.\n' +
      '* Chen et al. (2021) Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge, et al. 2021. Finqa: A dataset of numerical reasoning over financial data. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 3697-3711.\n' +
      '* Chen et al. (2022) Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. Conv/finqa: Exploring the chain of numerical reasoning in conversational finance question answering.\n' +
      '* Chicco and Jurman (2020) Davide Chicco and Giuseppe Jurman. 2020. The advantages of the matthews correlation coefficient (mcc) over f1 score and accuracy in binary classification evaluation. _BMC genomics_, 21(1):1-13.\n' +
      '* Cortis et al. (2017) Keith Cortis, Andre Freitas, Tobias Daudert, Manuela Huerlimann, Manel Zarrouk, Siegfried Handschuh, and Brian Davis. 2017. Semeval-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news. In _Proceedings of the 11th international workshop on semantic evaluation (SemEval-2017)_, pages 519-535.\n' +
      '* Dai et al. (2024) Yongfu Dai, Duanyu Feng, Jimin Huang, Haochen Jia, Qianqian Xie, Yifang Zhang, Weiguang Han, Wei Tian, and Hao Wang. 2024. Liaw: A chinese legal large language models benchmark.\n' +
      '* Derczynski (2016) Leon Derczynski. 2016. Complementarity, F-score, and NLP evaluation. In _Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\'16)_, pages 261-266, Portoroz, Slovenia. European Language Resources Association (ELRA).\n' +
      '* Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335.\n' +
      '* Feng et al. (2023) Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang Han, Alejandro Lopez-Lira, and Hao Wang. 2023. Empowering many, biasing a few: Generalist credit scoring through large language models. _arXiv preprint arXiv:2310.00566_.\n' +
      '* Goutte and Gaussier (2005) Cyril Goutte and Eric Gaussier. 2005. A probabilistic interpretation of precision, recall and f-score, with implication for evaluation. In _European conference on information retrieval_, pages 345-359. Springer.\n' +
      '* Han et al. (2023a) Weiguang Han, Jimin Huang, Qianqian Xie, Boyi Zhang, Yanzhao Lai, and Min Peng. 2023a. Mastering pair trading with risk-aware recurrent reinforcement learning.\n' +
      '* Han et al. (2023b) Weiguang Han, Boyi Zhang, Qianqian Xie, Min Peng, Yanzhao Lai, and Jimin Huang. 2023b. Select and trade: Towards unified pair trading with hierarchical reinforcement learning. _arXiv preprint arXiv:2301.10724_.\n' +
      '* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_.\n' +
      '* Hofmann (1994) Hans Hofmann. 1994. Statlog (German Credit Data). UCI Machine Learning Repository. DOI: [https://doi.org/10.24432/CSNC77](https://doi.org/10.24432/CSNC77).\n' +
      '* Hongyuan et al. (2023) Dong Hongyuan, Che Wanxiang, He Xiaoyu, Zheng Guudong, and Wen Junjie. 2023. FinBART: A pre-trained seq2seq language model for Chinese financial tasks. In _Proceedings of the 22nd Chinese National Conference on Computational Linguistics_, pages 906-917, Harbin, China. Chinese Information Processing Society of China.\n' +
      '* Islam et al. (2023) Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. 2023. Financebench: A new benchmark for financial question answering. _arXiv preprint arXiv:2311.11944_.\n' +
      '* Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. _arXiv preprint arXiv:2401.04088_.\n' +
      '\n' +
      'Rasmus Jorgensen, Oliver Brandt, Mareike Hartmann, Xiang Dai, Christian Igel, and Desmond Elliott. 2023. Multifin: A dataset for multilingual financial nlp. In _Findings of the Association for Computational Linguistics: EACL 2023_, pages 864-879.\n' +
      '* Kim et al. (2023) Kisub Kim, Xin Zhou, Dongsun Kim, Julia Lawall, Kui Liu, Tegawende F Bissyande, Jacques Klein, Jaekwon Lee, and David Lo. 2023. How are we detecting inconsistent method names? an empirical study from code review perspective. _arXiv preprint arXiv:2308.12701_.\n' +
      '* Koncel-Kedziorski et al. (2023) Rik Koncel-Kedziorski, Michael Krumdick, Viet Lai, Varshini Reddy, Charles Lovering, and Chris Tanner. 2023. Bizbench: A quantitative reasoning benchmark for business and finance. _arXiv preprint arXiv:2311.06602_.\n' +
      '* Lamm et al. (2018) Matthew Lamm, Arun Tejasvi Chaganty, Christopher D Manning, Dan Jurafsky, and Percy Liang. 2018. Textual analogy parsing: What\'s shared and what\'s compared among analogous facts. _arXiv preprint arXiv:1809.02700_.\n' +
      '* Lee et al. (2024) Jean Lee, Nicholas Stevens, Soyeon Caren Han, and Minseok Song. 2024. A survey of large language models in finance (fnllms).\n' +
      '* Lei et al. (2023) Yang Lei, Jiangtong Li, Ming Jiang, Junjie Hu, Dawei Cheng, Zhijun Ding, and Changjun Jiang. 2023. Cfbenchmark: Chinese financial assistant benchmark for large language model.\n' +
      '* Li et al. (2023a) Jiangtong Li, Yuxuan Bian, Guoxuan Wang, Yang Lei, Dawei Cheng, Zhijun Ding, and Changjun Jiang. 2023a. Cfgpt: Chinese financial assistant with large language model.\n' +
      '* Li et al. (2023b) Xianzhi Li, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu, and Sameena Shah. 2023b. Are chatgpt and gpt-4 general-purpose solvers for financial text analytics? an examination on several typical tasks. _arXiv preprint arXiv:2305.05862_.\n' +
      '* Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. _arXiv preprint arXiv:2211.09110_.\n' +
      '* Lin (2004) Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pages 74-81.\n' +
      '* Liu et al. (2023a) Xiao-Yang Liu, Guoxuan Wang, and Daochen Zha. 2023a. Fingpt: Democratizing internet-scale data for financial large language models. _arXiv preprint arXiv:2307.10485_.\n' +
      '* Liu et al. (2022) Xiao-Yang Liu, Ziyi Xia, Jingyang Rui, Jiechao Gao, Hongyang Yang, Ming Zhu, Christina Dan Wang, Zhaoran Wang, and Jian Guo. 2022. Finfl-meta: Market environments and benchmarks for data-driven financial reinforcement learning.\n' +
      '* Liu et al. (2023b) Xiao-Yang Liu, Ziyi Xia, Hongyang Yang, Jiechao Gao, Daochen Zha, Ming Zhu, Christina Dan Wang, Zhaoran Wang, and Jian Guo. 2023b. Dynamic datasets and market environments for financial reinforcement learning.\n' +
      '* Liu et al. (2020) Zhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li, and Jun Zhao. 2020. FinBERT: A pre-trained financial language representation model for financial text mining. In _Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20_, pages 4513-4519. International Joint Conferences on Artificial Intelligence Organization. Special Track on AI in FinTech.\n' +
      '* Lopez-Lira and Tang (2023) Alejandro Lopez-Lira and Yuehua Tang. 2023. Can chatgpt forecast stock price movements? return predictability and large language models. _arXiv preprint arXiv:2304.07619_.\n' +
      '* Lu et al. (2023) Dakuan Lu, Jiaqing Liang, Yipei Xu, Qianyu He, Yipeng Geng, Mengkun Han, Yingsi Xin, Hengkui Wu, and Yanghua Xiao. 2023. Bbt-fin: Comprehensive construction of chinese financial domain pre-trained language model, corpus and benchmark. _arXiv preprint arXiv:2302.09432_.\n' +
      '* Magdon-Ismail and Atiya (2004) Malik Magdon-Ismail and Amir F Atiya. 2004. Maximum drawdown. _Risk Magazine_, 17(10):99-102.\n' +
      '* Maia et al. (2018a) Macedo Maia, Siegfried Handschuh, Andre Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. 2018a. Www\'18 open challenge: financial opinion mining and question answering. In _Companion proceedings of the the web conference 2018_, pages 1941-1942.\n' +
      '* Maia et al. (2018b) Macedo Maia, Siegfried Handschuh, Andre Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. 2018b. Www\'18 open challenge: Financial opinion mining and question answering. pages 1941-1942.\n' +
      '* Malo et al. (2014) Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala. 2014. Good debt or bad debt: Detecting semantic orientations in economic texts. _Journal of the Association for Information Science and Technology_, 65(4):782-796.\n' +
      '* Mariko et al. (2020) Dominique Mariko, Hanna Abi Akl, Estelle Labidurie, Stephane Durfort, Hugues De Mazancourt, and Mahmoud El-Haj. 2020. Financial document causality detection shared task (fincausal 2020). _arXiv preprint arXiv:2012.02505_.\n' +
      '* McGrew (2009) Kevin S McGrew. 2009. Chc theory and the human cognitive abilities project: Standing on the shoulders of the giants of psychometric intelligence research.\n' +
      '* Mukherjee et al. (2022) Rajdeep Mukherjee, Abhinav Bohra, Akash Banerjee, Soumya Sharma, Manjunath Hegde, Afreen Shaikh, Shivani Shrivastava, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, et al. 2022. Ectsum: A new benchmark dataset for bullet point summarization of long earnings call transcripts. _arXiv preprint arXiv:2210.12467_.\n' +
      '* Maia et al. (2018)OpenAI. 2023a. Gpt-4 technical report.\n' +
      '* OpenAI (2023) R OpenAI. 2023b. Gpt-4 technical report. arxiv 2303.08774. _View in Article_, 2:13.\n' +
      '* Punt (2017) Andre E Punt. 2017. Strategic management decision-making in a complex world: quantifying, understanding, and using trade-offs. _ICES Journal of Marine Science_, 74(2):499-510.\n' +
      '* Quinlan (2012) Ross Quinlan. Statlog (Australian Credit Approval). UCI Machine Learning Repository. DOI: [https://doi.org/10.24432/C59012](https://doi.org/10.24432/C59012).\n' +
      '* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. 2023. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_.\n' +
      '* Alvarado et al. (2015) Julio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. 2015. Domain adaption of named entity recognition to support credit risk assessment. In _Proceedings of the Australasian Language Technology Association Workshop 2015_, pages 84-90, Parramatta, Australia.\n' +
      '* Schneider and McGrew (2012) W Joel Schneider and Kevin S McGrew. 2012. The cattell-horn-carroll model of intelligence.\n' +
      '* Shah et al. (2023a) Agam Shah, Suvan Paturi, and Sudheer Chava. 2023a. Trillion dollar words: A new financial dataset, task & market analysis. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 6664-6679, Toronto, Canada. Association for Computational Linguistics.\n' +
      '* Shah et al. (2023b) Agam Shah, Ruchit Vithani, Abhinav Gullapalli, and Sudheer Chava. 2023b. Finer: Financial named entity recognition dataset and weak-supervision model. _Neural preprint arXiv:2302.11157_.\n' +
      '* Shah et al. (2022) Raj Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman, Charese Smiley, Jiaoa Chen, and Diyi Yang. 2022. When flue meets flang: Benchmarks and large pretrained language model for financial domain. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 2322-2335.\n' +
      '* Sharma et al. (2023) Soumya Sharma, Subhendu Khatuya, Manjunath Hegde, Afreen Shaikh, Koustuv Dasgupta, Pawan Goyal, and Niloy Ganguly. 2023. Financial numeric extreme labelling: A dataset and benchmarking. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 3550-3561.\n' +
      '* Sharma et al. (2022) Soumya Sharma, Tapas Nayak, Arusarka Bose, Ajay Kumar Meena, Koustuv Dasgupta, Niloy Ganguly, and Pawan Goyal. 2022. Finred: A dataset for relation extraction in financial domain. In _Companion Proceedings of the Web Conference 2022_, pages 595-597.\n' +
      '* Sharpe (1998) William F Sharpe. 1998. The sharpe ratio. _Streetwise-the Best of the Journal of Portfolio Management_, 3:169-85.\n' +
      '* Sinha and Khandait (2020) Ankur Sinha and Tanmay Khandait. 2020. Impact of news on the commodity market: Dataset and results.\n' +
      '* Sinha and Khandait (2021) Ankur Sinha and Tanmay Khandait. 2021. Impact of news on the commodity market: Dataset and results. In _Advances in Information and Communication: Proceedings of the 2021 Future of Information and Communication Conference (FICC), Volume 2_, pages 589-601. Springer.\n' +
      '* Soun et al. (2022) Yejun Soun, Jaemin Yoo, Minyong Cho, Jihyeong Jeon, and U Kang. 2022. Accurate stock movement prediction with self-supervised learning from sparse noisy tweets. In _2022 IEEE International Conference on Big Data (Big Data)_, pages 1691-1700. IEEE.\n' +
      '* Srivastava et al. (2023) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _Transactions on Machine Learning Research_.\n' +
      '* Sy et al. (2023) Eugene Sy, Tzu-Cheng Peng, Shih-Hsuan Huang, Heng-Yu Lin, and Yung-Chun Chang. 2023. Fine-grained argument understanding with bert ensemble techniques: A deep dive into financial sentiment analysis. In _Proceedings of the 35th Conference on Computational Linguistics and Speech Processing (ROCLING 2023)_, pages 242-249.\n' +
      '* Team (2023a) Fin-Eva Team. 2023a. Fin-eva version 1.0.\n' +
      '* Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_.\n' +
      '* Team (2023b) InternLM Team. 2023b. Internlm: A multilingual language model with progressively enhanced capabilities.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.\n' +
      '* Wang et al. (2023) Neng Wang, Hongyang Yang, and Christina Dan Wang. 2023. Fingpt: Instruction tuning benchmark for open-source large language models in financial datasets.\n' +
      '* Wei et al. (2023) Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. 2023. Larger language models do in-context learning differently. _arXiv preprint arXiv:2303.03846_.\n' +
      '* Wang et al. (2023)Huizhe Wu, Wei Zhang, Weiwei Shen, and Jun Wang. 2018. Hybrid deep sequential modeling for social text-driven stock prediction. In _Proceedings of the 27th ACM international conference on information and knowledge management_, pages 1627-1630.\n' +
      '* Wu et al. (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: A large language model for finance.\n' +
      '* Xie et al. (2023) Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, and Jimin Huang. 2023a. The wall street neophyte: A zero-shot analysis of chatgpt over multimodal stock movement prediction challenges. _arXiv preprint arXiv:2304.05351_.\n' +
      '* Xie et al. (2023b) Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023b. Pixiu: A large language model, instruction data and evaluation benchmark for finance. _arXiv preprint arXiv:2306.05443_.\n' +
      '* Xu and Cohen (2018) Yumo Xu and Shay B Cohen. 2018. Stock movement prediction from tweets and historical prices. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1970-1979.\n' +
      '* Yang et al. (2023a) Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023a. Fingpt: Open-source financial large language models.\n' +
      '* Yang et al. (2020a) Linyi Yang, Eoin M Kenny, Tin Lok James Ng, Yi Yang, Barry Smyth, and Ruihai Dong. 2020a. Generating plausible counterfactual explanations for deep transformers in financial text classification. _arXiv preprint arXiv:2010.12512_.\n' +
      '* Yang et al. (2023b) Yi Yang, Yixuan Tang, and Kar Yan Tam. 2023b. Investlm: A large language model for investment using financial domain instruction tuning.\n' +
      '* Yang et al. (2020b) Yi Yang, Mark Christopher Siy UY, and Allen Huang. 2020b. Finbert: A pretrained language model for financial communications.\n' +
      '* Yu et al. (2023) Yangyang Yu, Haohang Li, Zhi Chen, Yuchen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W. Suchow, and Khaldoun Khashanah. 2023. Finnmem: A performance-enhanced llm trading agent with layered memory and character design.\n' +
      '* Yuan et al. (2021) Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. _Advances in Neural Information Processing Systems_, 34:27263-27277.\n' +
      '* Zhang et al. (2023a) Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. 2023a. Instruction tuning for large language models: A survey.\n' +
      '* Zhang et al. (2019) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. _arXiv preprint arXiv:1904.09675_.\n' +
      '* Zhang et al. (2024) Xiao Zhang, Ruoyu Xiang, Chenhan Yuan, Duanyu Feng, Weiguang Han, Alejandro Lopez-Lira, Xiaofeng Liu, Sophia Ananiadou, Min Peng, Jimin Huang, and Qianqian Xie. 2024. Dolares or dollars? unraveling the bilingual prowess of financial llms between spanish and english.\n' +
      '* Zhang et al. (2023b) Xuanyu Zhang, Bingbing Li, and Qing Yang. 2023b. Cgce: A chinese generative chat evaluation benchmark for general and financial domains.\n' +
      '* Zhang et al. (2023c) Xuanyu Zhang, Qing Yang, and Dongliang Xu. 2023c. Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters.\n' +
      '* Zhou et al. (2023) Xianzheng Zhou, Hui Zhou, and Huaigang Long. 2023. Forecasting the equity premium: Do deep neural network models work? _Modern Finance_, 1(1):1-11.\n' +
      '* Zhou et al. (2021) Zhihan Zhou, Liqian Ma, and Han Liu. 2021. Trade the event: Corporate events detection for news-based event-driven trading.\n' +
      '* Zhu et al. (2021) Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. _arXiv preprint arXiv:2105.07624_.\n' +
      '\n' +
      '## Appendix A Contributions\n' +
      '\n' +
      '**Science Leadership**: Qianqian Xie, Min Peng, Sophia Ananiadou, Alejandro Lopez-Lira, Hao Wang, Yanzhao Lai, Benyou Wang, Xiao-yang Liu, Gang Hu, Jiajia Huang, Jimin Huang. **Contributors**: Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, Yijing Xu, Haoqiang Kang, Ziyan Kuang, Chenhan Yuan, Kailai Yang, Zheheng Luo, Tianlin Zhang, Zhiwei Liu, Guojun Xiong, Zhiyang Deng, Yuechen Jiang, Zhiyuan Yao, Haohang Li, Yangyang Yu\n' +
      '\n' +
      '## Appendix B Other LLMs Performance\n' +
      '\n' +
      'Table 5 presents other LLMs\' performance in the FinBen.\n' +
      '\n' +
      '## Appendix C Instructions\n' +
      '\n' +
      'For detail instruction of each dataset, please see Table 6 and Table 7.\n' +
      '\n' +
      '## Appendix D Related Work\n' +
      '\n' +
      '### Financial Large Language Models\n' +
      '\n' +
      'Recent years have seen a significant surge in research on finance-specific LLMs, expanding on the groundwork laid by general-purpose language models (Lee et al., 2024; Liu et al., 2023b;Xie et al., 2023; Zhang et al., 2024; Dai et al., 2024). Financial pre-trained language models (FinPLMs) like FinBERT (Araci, 2019; Yang et al., 2020; Liu et al., 2020), derived from BERT, and FLANG (Shah et al., 2022), based on ELECTRA, have been developed using domain-specific data for enhanced performance in tasks like sentiment analysis and stock prediction. The open-source release of Meta AI\'s LLaMA (Touvron et al., 2023) has fueled further innovation in Financial LLMs (FinLLMs), with models like FinMA (Xie et al., 2023), InvestLM (Yang et al., 2023), and Fin GPT (Wang et al., 2023; Liu et al., 2023) leveraging advanced tuning strategies (Zhang et al., 2023) for financial applications. BloombergGPT (Wu et al., 2023) stands out as a BLOOM-based, closed-source models tailored for the financial industry. Additionally, the Chinese financial sector has seen the emergence of models like XuanYuan 2.0 (Zhang et al., 2023), integrating broad and specialized knowledge, FinBART (Hongyuan et al., 2023) for financial communication, and CFGPT (Li et al., 2023), which includes a comprehensive dataset for targeted pre-training and fine-tuning.\n' +
      '\n' +
      '### Financial Evaluation Benchmarks\n' +
      '\n' +
      'Financial evaluation benchmarks, such as the pioneering FLUE (Shah et al., 2022), have been in\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c} \\hline \\hline \\multirow{2}{*}{**Dataset**} & \\multirow{2}{*}{**Metrics**} & **Bialchuan** & **CodeLLaMA** & **DISC-** \\\\  & & **7B** & **7B** & **FinLLM** \\\\ \\hline \\multirow{2}{*}{FPB} & F1 & **0.36** & 0.34 & 0.29 \\\\  & Acc & 0.32 & **0.39** & 0.26 \\\\  & F1Q-SA & F1 & 0.17 & **0.66** & 0.32 \\\\  & RMSE1 & 1.07 & 0.43 & **0.32** \\\\  & Headlines & AvgF1 & **0.60** & 0.60 & 0.60 \\\\  & F1 & 0.16 & 0.14 & **0.19** \\\\  & Acc & 0.25 & 0.27 & **0.28** \\\\  & MicroF1 & **0.34** & 0.28 & 0.29 \\\\  & FinAP-ARC & MicroF1 & 0.17 & 0.25 & **0.29** \\\\  & Multirin & MicroF1 & 0.06 & 0.21 & **0.29** \\\\  & MicroF1 & 0.02 & **0.54** & 0.29 \\\\  & MLESG & MicroF1 & 0.00 & 0.10 & **0.29** \\\\ \\hline \\hline NER & Entity1 & 0.00 & 0.07 & **0.12** \\\\  & FinRep1 & **0.00** & **0.00** & 0.00 \\\\  & FinRep1 & **0.00** & 0.00 & 0.00 \\\\  & Sc & F1 & 0.57 & **0.85** & 0.00 \\\\  & F1 & **0.00** & 0.00 & 0.00 \\\\ \\hline \\hline \\multirow{2}{*}{FMQ} & EmAcc & **0.00** & 0.00 & 0.00 \\\\  & EmAcc & **0.00** & 0.00 & 0.00 \\\\  & ComFit-DA & EmAcc & **0.00** & 0.00 & 0.00 \\\\  & FNXL & EntityF1 & **0.00** & 0.00 & 0.00 \\\\  & FSRL & EntityF1 & **0.00** & 0.00 & 0.00 \\\\ \\hline \\hline \\multirow{2}{*}{EDTSUM} & Rouge-1 & **0.22** & 0.10 & 0.22 \\\\  & BertScore & 0.54 & **0.67** & 0.61 \\\\  & BartScore & -4.57 & **-3.62** & -4.13 \\\\  & Rouge-1 & **0.00** & 0.00 & 0.00 \\\\  & BertScore & **0.00** & 0.00 & 0.00 \\\\  & BartScore & **-5.18** & -5.18 & -5.18 \\\\ \\hline \\hline \\multirow{2}{*}{BigData22} & Acc & **0.53** & 0.52 & 0.44 \\\\  & MCC & **-0.01** & -0.01 & -0.05 \\\\  & Acc & 0.50 & **0.51** & 0.50 \\\\  & MCC & 0.00 & 0.00 & **0.02** \\\\  & Acc & **0.53** & 0.51 & 0.44 \\\\  & MCC & -0.05 & **0.02** & -0.03 \\\\  & F1 & 0.52 & **0.66** & 0.52 \\\\  & MCC & 0.00 & 0.00 & 0.00 \\\\  & F1 & 0.26 & **0.43** & 0.26 \\\\  & MCC & 0.00 & 0.00 & 0.00 \\\\  & MCC & 0.00 & 0.00 & 0.00 \\\\  & IndlingCub & F1 & 0.72 & **0.81** & 0.72 \\\\  & MCC & -0.01 & **0.00** & 0.00 \\\\  & F1 & **0.97** & 0.00 & 0.66 \\\\  & MCC & **0.00** & 0.00 & -0.04 \\\\  & F1 & 0.00 & 0.06 & **0.46** \\\\  & MCC & 0.00 & 0.00 & **0.02** \\\\  & F1 & 0.91 & 0.47 & **0.92** \\\\  & MCC & 0.02 & **0.04** & 0.00 \\\\  & F1 & 0.70 & 0.36 & **0.95** \\\\  & MCC & -0.02 & -0.03 & **0.00** \\\\  & F1 & 0.01 & **0.88** & 0.63 \\\\  & MCC & **0.01** & -0.01 & -0.02 \\\\  & F1 & **0.03** & 0.02 & 0.00 \\\\  & MCC & -0.09 & **0.00** & 0.00 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: The zero-shot and few-shot performance of other LLMs on the FinBen.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      'Figure 8: Accumulative Returns of LLM Trading Strategies on NFLX\n' +
      '\n' +
      'Figure 10: Accumulative Returns of LLM Trading Strategies on TSLA\n' +
      '\n' +
      'Figure 7: Accumulative Returns of LLM Trading Strategies on MSFT\n' +
      '\n' +
      'Figure 9: Accumulative Returns of LLM Trading Strategies on NIO\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l p{284.5pt}} \\hline \\hline\n' +
      '**Data** & **Prompt** \\\\ \\hline \\multirow{2}{*}{FPB} & “Analyze the sentiment of this statement extracted from a financial news article. \\\\  & Provide your answer as either negative, positive or neutral. \\\\  & For instance.” The company’s stocks plummeted following the scandal.’ would be classified as negative.” \\\\ \\hline \\multirow{2}{*}{FQA-SA} & “What is the sentiment of the following financial (category): Positive, Negative, or Neutral” \\\\ \\hline \\multirow{2}{*}{Headlines} & “Consider whether the headline mentions the price of gold. \\\\  & Is there a Price or Not in the gold commodity market indicated in the news headline? \\\\  & Please answer Yes or No.” \\\\ \\hline \\multirow{2}{*}{NER} & “In the sentences extracted from financial agreements in U.S. SEC filings, identify the named entities that represent a person (‘PER’), an organization (‘ORG’), or a location (‘LOC’). The required answer format is \'entity name, entity type’. \\\\  & For instance, in ‘Elon Music, CEO of Spocx, announced the launch from Cape Canversal’, the entities would be: ‘Elon Musk, PER; Spocx, OCR; Cape Canversal, LOC” \\\\ \\hline \\multirow{4}{*}{FNER-ORD} & “In the list of tokens, identify (‘id’) each accordingly. \\\\  & If the entity spans multiple tokens, use the prefix B-PER, B-LOC, or B-ORG for the first token, and I-PER, I-LOC, or I-ORG for the subsequent tokens of that entity. \\\\ \\cline{1-1}  & The beginning of each separate entity should always be labeled with a B-PER, B-LOC, or B-ORG prefix. If the token does not into any of the three named categories, or is not a named entity, label it as ‘O’.” \\\\ \\hline \\multirow{2}{*}{FinQA} & “Given the financial data and experts analysis, please answer this question.” \\\\ \\cline{2-2}  & “In the context of this series of interconnected finance-related queries and the additional information provided by the pretext, table data, and post text from a company’s financial filings, please provide a response to the final question. This may require extracting information from the context and performing mathematical calculations. Please take into account the information provided in the preceding questions and their answers when formulating your response.” \\\\ \\hline \\multirow{2}{*}{BigData22} & “Contemplate the data and tweets to guess whether the closing price of (\\(\\mathrm{iid}\\)) will surge or decline at (\\(\\mathrm{point}\\)). \\\\  & Please declare which either Rise or Fall.” \\\\ \\hline \\multirow{2}{*}{ACL18} & “Scrutinize the data and tweets to envisage if the closing price of (\\(\\mathrm{iid}\\)) will well or contract at (\\(\\mathrm{point}\\)). \\\\  & Respond with either Rise or Fall.” \\\\ \\hline \\multirow{2}{*}{CIRM18} & “Reflect on the provided data and tweets to anticipate if the closing price of (\\(\\mathrm{iid}\\)) is going to increase or decrease at (\\(\\mathrm{point}\\)). \\\\  & Respond with either Rise or Fall.” \\\\ \\hline \\multirow{2}{*}{ECTSem} & “Given the following article, please produce a list of 0 and 1, each separated by \\(\\,\\) - \\(\\,\\) to indicate which sentences should be included in the final summary. The article’s sentences have been split by \\(\\,\\) - \\(\\,\\) Please mark each sentence with 1 if it should be included in the summary and 0 if it should not.” \\\\ \\hline \\multirow{2}{*}{EDTSum} & “You are given a text that consists of multiple sentences. Your task is to perform abstractive summarization on this text. Use your understanding of the content to express the main ideas and crucial details in a shorter, coherent, and natural sounding text.” \\\\ \\hline \\multirow{2}{*}{German} & “Asses the creditworthiness of a customer using the following table attributes for financial states. Respond with either ‘good’ or "bad. And the table attributes including 13 categorical attributes and 7 numerical attributes are as follows.” \\\\ \\hline \\multirow{2}{*}{Australian} & “Asses the creditworthiness of a customer using the following table attributes for financial status. Respond with either ‘good’ or "bad.” And the table attributes including 13 categorical attributes and 7 numerical attributes and values have been changed to meaningless symbols to protect confidentiality of the data..” \\\\ \\hline \\multirow{2}{*}{FOMC} & “Examine the excerpt from a central bank’s release below. Classify is a HAWKISH if it advocates for a tightening of monetary policy, DVSWIFI if it suggests an easing of monetary policy, or NEUTRAL if the stance is unbiased. Your response should return only HAWKISH, DOVISI, or NEUTRAL.” \\\\ \\hline \\multirow{2}{*}{TSA} & “Given the following financial text, return a sentiment score for Ahstead as a floating-point number ranging from -1 (indicating a very negative or beurish sentiment) to 1 (indicating a very positive or bullish sentiment), with 0 designating neutral sentiment. Return only the numerical score first, follow it with a brief reasoning behind your score.” \\\\ \\hline \\multirow{2}{*}{FinArg - ACC} & “Analyze sentences from earnings conference calls and identify their argumentative function. \\\\  & Each sentence is either a premise, offering evidence or reasoning, or a claim, asserting a conclusion or viewpoint. Return only premise or claim.” \\\\ \\hline \\multirow{2}{*}{FinArg - ARC} & “In this task, you are given a pair of sentences. \\\\  & Your objective is to ascertain the type of argumentative relation between these two sentences. \\\\ \\cline{2-2}  & The relation could either be ‘NoRelation’, indicating to discernible relation between the sentences, \\\\ \\cline{2-2}  & “Superior, indicating that the first sentence supports the second, or “attack”: indicating that the first sentence disputes or contradicts the second. Return only on of the three classifications: ‘notation’, ‘superior’, or ‘attack’.” \\\\ \\hline \\multirow{4}{*}{MultiFin} & “In this task, you’re working with English headlines from the MULTIFIN dataset. \\\\  & This dataset is made of peak-world article headlines from a large accounting firm’s websites. \\\\ \\cline{2-2}  & Your objective is to categorize each headline according to its primary topic. \\\\ \\cline{2-2}  & The potential categories are \\(\\lfloor\\)category\\(\\rfloor\\). \\\\ \\cline{2-2}  & Your response should only include the category that best fits the headline.” \\\\ \\hline \\multirow{2}{*}{MA} & “In this task, you will be given Mergers and Acquisition news articles or tweets. \\\\ \\cline{2-2}  & Your task is to classify each article or tweet based on whether the mentioned deal was completed or remained a rumour. \\\\ \\cline{2-2}  & Your response should be a single word - either ‘complete’ or ‘rumour’, representing the outcome of the deal mentioned in the provided text.” \\\\ \\hline \\multirow{2}{*}{MLESG} & “You’re given English news articles related to Environmental, Social, and Corporate Governance (ESG) issues. \\\\ \\cline{2-2}  & Your task is to classify each article based on the ESG issue it pertains to, according to the MSCI ESG rating guidelines. \\\\ \\cline{2-2}  & The ESG issues include \\(\\lfloor\\)category\\(\\rfloor\\). \\\\ \\cline{2-2}  & Your output should be the most relevant ESG issue label, followed by a brief rationale based on the article content.” \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: The prompt of each dataset.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Data** & **Prompt** \\\\ \\hline \\hline \\multirow{3}{*}{FinkBED} & “Given the following sentence, identify the head, tail, and relation of each triplet present in the sentence. \\\\  & The relations you should be looking for the car (category). \\\\  & If a relation exists between two entities, provide your answer in the format [category]. \\\\  & If there are multiple triplets in a sentence, provide each one on a new line.” \\\\  & “In this task, you are provided with sentences extracted from financial news and SEC data. \\\\  & Your goal is to classify each sentence into either ‘causal’ or ‘noise’ based on whether or not it indicates a causal relationship between financial events. \\\\  & Please return only the category ‘causal’ or ‘noise’.” \\\\ \\hline \\multirow{3}{*}{CD} & “Your job in this task is to perform sequence labeling on a provided text section, marking the chunks that represent the cause of an event and the effects that result from it. For each token in the text, asking a label to indicate its role in representing cause or effect. \\\\  & The labels you should use are “B-CAUSE; 1-CAUSE; 2-BEFTECT; 1-BEFTECT, and ‘O’. \\\\  & A ‘B-’ prefix is used to denote the beginning of a cause or effect sequence. \\\\  & while an ‘1:” prefix is used for continuation of a cause or effect sequence. \\\\  & If a token is not part of either a cause or effect sequence, label it as ’O’. \\\\  & Provide your answer as a sequence of ‘tokenlabel’ pairs, with each pair on a new line.” \\\\ \\hline TATQA & “Please answer the given financial question based on the context. Context: {context}[Question: What is the amount of total sales in 2019?” \\\\ \\hline \\multirow{3}{*}{FNXL} & “In the task of Financial Numeric Extreme Labelling (FNXL), \\\\  & your job is to identify and label the semantic role of each token in a sentence. \\\\  & The labels can include [category]” \\\\ \\hline \\multirow{3}{*}{FSRL} & “In the task of Textual Analogy Parsing (TAP), your job is to identify and label the semantic role of each token in a sentence. \\\\  & The labels can include [category].” \\\\ \\hline \\multirow{3}{*}{LendingClub} & “Asses the client’s loan status based on the following loan records from Lending Club. \\\\  & Respond with only ‘good’ or ‘bad’, and do not provide any additional information. \\\\  & For instance, “The client is a stable income, no previous debts, and owns a property.’ should be classified as ‘good’.” \\\\ \\hline \\multirow{3}{*}{ccf} & “Detect the credit card fraud using the following financial table attributes. \\\\  & Respond with only ‘yes’ or ‘no’, and do not provide any additional information. \\\\  & Therein, the data contains 28 numerical input variables V1, V2,... \\\\  & and V28 which are the result of a PCA transformation and 1 input variable Amount which has not been transformed with PCA. \\\\  & The feature ‘Amount’ is the transaction Amount, this feature can be used for example-dependent cost-sensitive learning. \\\\  & For instance, “The client has attributes: [category]” \\\\ \\hline \\multirow{3}{*}{ccfraud} & “Detect the credit card fraud with the following financial profile. \\\\  & Respond with only ‘good’ or ‘bad’, and do not provide any additional information. \\\\  & The client is a female, the state number is 25, the number of cards is 1, the credit balance is 7000, \\\\  & the number of transactions is 16, the number of international transactions is 0, \\\\  & the credit limit is 6.’ should be classified as ‘good.” \\\\ \\hline \\multirow{3}{*}{polish} & “Predict whether the company will face bankruptcy based on the financial profile attributes provided in the following text. \\\\  & Respond with only ‘no’ or ‘yes’, and do not provide any additional information.” \\\\ \\hline \\multirow{3}{*}{P Porto-Seguro} & “Identify whether or not to files a claim for the auto insurance policy holder using the following table attributes about individual financial profile. \\\\  & Respond with only ‘yes’ or ‘no’, and do not provide any additional information. \\\\  & And the table attributes that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc). \\\\  & In addition, feature names include the positiva bin to indicate binary features and cat to indicate categorical features. \\\\  & Features without these designations are either continuous or ordinal. \\\\  & Values of -1 indicate that the feature was missing from the observation.” \\\\ \\hline \\multirow{3}{*}{travelinsurance} & “Identify the claim status of insurance companies using the following table attributes for travel insurance status. \\\\  & Respond with only ‘yes’ or ‘no’, and do not provide any additional information. \\\\  & And the table attributes including 5 categorical attributes and 4 numerical attributes are as follows:[category]" \\\\ \\hline \\multirow{3}{*}{fintrade} & “Given the information, can you make an investment decision?” last summarize the reason of the decision. \\\\  & please consider only the available short-term information, the mid-term information, the long-term information, the \\\\  & reflection-term information. \\\\  & please consider the momentum of the historical stock price. \\\\  & When cumulative return is positive or zero, you are a risk-seeking investor. \\\\  & But when cumulative return is negative, you are a risk-exercise investor. \\\\  & please consider how much share of the stock the inverse holds now. \\\\  & You should provide exactly one of the following investment decisions: buy or sell. \\\\  & When it is really hard to make a ‘buy’-or-’sell’ decision, you could go with ‘hold’ option. \\\\  & You also need to provide the id of the information to support your decision. \\\\  & [ investment,info] \\\\  & [gropule\\_json\\_suffix,v2] \\\\  & Your output should strictly conforms the following json format without any additional contents: \\\\  & [’investment\\_decision’\\_ stringing, “summary\\_reason’: stringing, “short\\_memory\\_intax”: number, \\\\  &\'middle\\_memory\\_index’: number, “long\\_memory\\_index’: number, “reflection\\_memory\\_intax’: number]" \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: The example prompt for each dataset. FiQA-SA has two types of text, including news headlines and tweets. We will fill the detailed text type into {category} for each data sample. For stock movement prediction data such as BigData22, we will fill {tid} and {point} with the detailed stock name and time from each data sample.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l} \\hline \\hline\n' +
      '**Ticker** & **Model** & **CR (\\%)** & **SR** & **DV (\\%)** & **AV (\\%)** & **MD (\\%)** \\\\ \\hline \\multirow{6}{*}{TSLA} & Buy and Hold & -25.2137 & -0.7203 & 4.4099 & 70.0043 & 57.6765 \\\\  & GPT-4 & 68.3089 & 2.8899 & 2.9780 & 47.2739 & 10.7996 \\\\  & GPT3.5-Turbo & 25.2137 & 0.7203 & 4.4099 & 70.0043 & 51.3186 \\\\  & Ilama2-70B & -31.4144 & -1.0412 & 3.8014 & 60.3450 & 48.6173 \\\\  & gemini & -0.3790 & -0.0148 & 3.2271 & 51.2280 & 35.6707 \\\\ \\hline \\multirow{6}{*}{NFLEX} & Buy and Hold & 34.6251 & 1.3696 & 3.1852 & 50.5634 & 20.9263 \\\\  & GPT-4 & 36.4485 & 2.0088 & 2.2860 & 36.2894 & 15.8495 \\\\  & GPT3.5-Turbo & 7.9337 & 0.4610 & 2.1680 & 34.4160 & 17.9578 \\\\  & Ilama2-70B & 33.8460 & 1.4741 & 2.8928 & 45.9216 & 20.3910 \\\\  & gemini & 11.6298 & 1.0073 & 1.4546 & 23.0906 & 16.5106 \\\\ \\hline \\multirow{6}{*}{AMZN} & Buy and Hold & -16.4428 & -0.7448 & 2.7812 & 44.1508 & 33.8847 \\\\  & GPT-4 & 10.5539 & 0.4923 & 2.7012 & 42.8802 & 22.9294 \\\\  & GPT3.5-Turbo & 19.9636 & 0.9611 & 2.6171 & 41.5454 & 19.2191 \\\\  & Ilama2-70B & 8.3595 & 1.9715 & 0.5342 & 8.4804 & 0.0000 \\\\  & gemini & -2.3838 & -0.5321 & 0.5645 & 8.9605 & 6.4291 \\\\ \\hline \\multirow{6}{*}{MSFT} & Buy and Hold & 17.2161 & 0.9709 & 2.2339 & 35.4623 & 15.0097 \\\\  & GPT-4 & 25.7826 & 1.5818 & 2.0535 & 25.989 & 14.9889 \\\\  & GPT3.5-Turbo & 20.4179 & 1.3600 & 1.8915 & 30.0259 & 20.3211 \\\\  & Ilama2-70B & 27.7664 & 1.5708 & 2.2270 & 35.3524 & 15.0097 \\\\  & gemini & 21.5082 & 1.3701 & 1.9777 & 31.3957 & 17.5051 \\\\ \\hline \\multirow{6}{*}{COIN} & Buy and Hold & -18.4787 & -0.3369 & 6.9098 & 109.6904 & 60.5084 \\\\  & GPT4-4 & 25.7631 & 0.5619 & 5.7761 & 91.6934 & 35.7526 \\\\  & GPT3.5-Turbo & 25.1141 & 0.4772 & 6.6312 & 105.2669 & 53.9628 \\\\  & Ilama2-70B & 15.1836 & 0.4395 & 4.3528 & 60.0979 & 35.3249 \\\\  & gemini & 89.4782 & 1.7648 & 6.3879 & 101.4048 & 40.3246 \\\\ \\hline \\multirow{6}{*}{AAPL} & Buy and Hold & 12.7371 & 0.7759 & 2.0682 & 32.8323 & 20.6591 \\\\  & GPT4-4 & 21.2334 & 1.9274 & 1.3879 & 22.0328 & 6.4237 \\\\  & Ilama2-70B & 27.0152 & -1.9193 & 1.7734 & 28.1517 & 33.1619 \\\\  & Ilama2-70B & 11.4855 & 1.1550 & 1.2529 & 19.8855 & 92.776 \\\\  & gemini & -5.3097 & -0.3637 & 1.8392 & 29.1971 & 26.6450 \\\\ \\hline \\multirow{6}{*}{GOOG} & Buy and Hold & 6.3107 & 0.3081 & 2.5806 & 40.9660 & 21.1907 \\\\  & GPT-4 & 13.2811 & 0.9667 & 1.7308 & 27.4762 & 12.2209 \\\\  & GPT3.5-Turbo & 0.9990 & 0.0614 & 0.20490 & 32.5265 & 20.9316 \\\\  & Ilama2-70B & 17.0030 & 1.1057 & 1.9374 & 30.7546 & 13.2088 \\\\  & gemini & 38.7956 & 3.0341 & 1.6110 & 25.5732 & 13.7311 \\\\ \\hline \\multirow{6}{*}{NO} & Buy and Hold & -49.4263 & -1.1895 & 5.2351 & 83.1048 & 52.2083 \\\\  & GPT-4 & 24.7684 & 0.9438 & 3.3063 & 52.4861 & 29.3384 \\\\ \\cline{1-1}  & GPT3.5-Turbo & -28.9321 & -1.0096 & 3.6105 & 57.3149 & 39.5907 \\\\ \\cline{1-1}  & Ilama2-70B & -49.6947 & -2.7868 & 2.2466 & 35.0639 & 42.6221 \\\\ \\cline{1-1}  & gemini & 14.5673 & 0.6212 & 2.9543 & 46.8977 & 23.0110 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: The overall trading performance comparison for different LLMs across various stocks. The results include large LLMs only (\\(\\geq 70B\\)), as models with smaller contexts have difficulty understanding the instructions and producing a static strategy of holding.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>