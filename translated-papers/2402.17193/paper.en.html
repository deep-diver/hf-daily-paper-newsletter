<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# When Scaling Meets LLM Finetuning:\n' +
      '\n' +
      'The Effect of Data, Model and Finetuning Method\n' +
      '\n' +
      'Biao Zhang\\({}^{\\dagger}\\) Zhongtao Liu\\({}^{\\diamond}\\) Colin Cherry\\({}^{\\diamond}\\) Orhan Firat\\({}^{\\dagger}\\)\n' +
      '\n' +
      '\\({}^{\\dagger}\\)Google DeepMind \\({}^{\\diamond}\\)Google Research\n' +
      '\n' +
      '{biaojiaxing,zhongtao,colincherry,orhanf}@google.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'While large language models (LLMs) often adopt _finetuning_ to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning - full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a power-based multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Leveraging and transferring the knowledge encoded in large-scale pretrained models for downstream applications has become the standard paradigm underlying the recent success achieved in various domains (Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020; Dosovitskiy et al., 2021; Baevski et al., 2020), with the remarkable milestone set by large language models (LLMs) that have yielded ground-breaking performance across language tasks (Brown et al., 2020; Zhang et al., 2022; Scao et al., 2022; Touvron et al., 2023). Advanced LLMs, such as GPT-4 (OpenAI, 2023) and PaLM 2 (Anil et al., 2023), often show emergent capabilities and allow for in-context learning that could use just a few demonstration examples to perform complex reasoning and generation tasks (Wei et al., 2022; Zhang et al., 2023; Fu et al., 2023; Shen et al., 2023). Still, LLM finetuning is required and widely adopted to unlock new and robust capabilities for creative tasks, get the most for focused downstream tasks, and align its value with human preferences (Ouyang et al., 2022; Yang et al., 2023; Gong et al., 2023; Schick et al., 2023). This becomes more significant in traditional industrial applications due to the existence of large-scale annotated task-specific data accumulated over years.\n' +
      '\n' +
      'There are many potential factors affecting the performance of LLM finetuning, including but not limited to 1) pretraining conditions, such as LLM model size and pretraining data size; and 2) finetuning conditions, such as downstream task, finetuning data size and finetuning methods. Intuitively, the pretraining controls the quality of the learned representation and knowledge in pretrained LLMs, and the finetuning affects the degree of transfer to the downstream task. While previous studies have well explored the scaling for LLM pretraining or training from scratch (Kaplan et al., 2020; Hoffmann et al., 2022) and the development of advanced efficient finetuning methods (Hu et al., 2021; He et al., 2022), the question of whether and how LLM finetuning scales with the above factors unfortunately receives very little attention (Hernandez et al., 2021), which is the focus of our study.\n' +
      '\n' +
      'Note, apart from improving finetuning performance, studying the scaling for LLM finetuning could help us to understand the impact of different pretraining factors from the perspective of finetuning, which may offer insights for LLM pretraining.\n' +
      '\n' +
      'In this paper, we address the above question by systematically studying the scaling for two popular ways of LLM finetuning: _full-model tuning_ (FMT) that updates all LLM parameters and _parameter-efficient tuning_ (PET) that only optimizes a small amount of (newly added) parameters, such as prompt tuning (Lester et al., 2021, Prompt) and low-rank adaptation (Hu et al., 2021, LoRA). We first examine finetuning data scaling (Hernandez et al., 2021), on top of which we further explore its scaling relationship with other scaling factors, including LLM model size, pretraining data size, and PET parameter size. We focus on the data-limited regime, where the finetuning data is much smaller than the LLM model, better reflecting the situation in the era of LLM. For experiments, we pretrained two sets of bilingual LLMs (English&German, English&Chinese) with model size ranging from 1B to 16B, and performed large-scale study on WMT machine translation (English-German, English-Chinese) and multilingual summarization (English, German, French and Spanish) tasks with up to 20M finetuning examples. Our main findings are summarized below:\n' +
      '\n' +
      '* We propose the following multiplicative joint scaling law for LLM finetuning: \\[\\hat{\\mathcal{L}}(X,D_{f})=A*\\frac{1}{X^{\\alpha}}*\\frac{1}{D_{f}^{\\beta}}+E,\\] (1) where \\(\\{A,E,\\alpha,\\beta\\}\\) are data-specific parameters to be fitted, \\(D_{f}\\) denotes finetuning data size, and \\(X\\) refer to each of the other scaling factors. We show empirical evidence that this joint law generalizes to different settings.\n' +
      '* Scaling LLM model benefits LLM finetuning more than scaling pretraining data.\n' +
      '* Increasing PET parameters doesn\'t scale well for LoRA and Prompt, although LoRA shows better training stability.\n' +
      '* The scaling property for LLM finetuning is highly task- and data-dependent, making the selection of optimal finetuning method for a downstream task non-trivial.\n' +
      '* LLM-based finetuning could encourage zero-shot generalization to relevant tasks, and PET performs much better than FMT.\n' +
      '\n' +
      '## 2 Setup\n' +
      '\n' +
      'Downstream TasksWe consider machine translation and multilingual summarization as the downstream tasks for the finetuning, because 1) these tasks require resolving cross-lingual understanding and generation, which represent high complexity and are challenging; and 2) they are well established in NLP with rich amount of available finetuning corpora. Specially, we adopt WMT14 English-German (En-De) and WMT19 English-Chinese (En-Zh) (Kocmi et al., 2022) for translation. We combine the De, Spanish (Es) and French (Fr) portion of the multilingual summarization dataset (Scialom et al., 2020) with CNN/Daily-Mail (Hermann et al., 2015, En) for summarization and denote it as MLSum. Details about each task are listed in Table 1a. Note for MLSum, we directly concatenate the datasets of different languages for training and evaluation, where each article is prepended a prompt indicating its language "_Summarize the following document in [lang]:_".\n' +
      '\n' +
      'LLMs and PrerainingWe adopt the exact setup as in Garcia et al. (2023) for LLM pretraining. The model is a decoder-only Transformer with multi-query attention (Chowdhery et al., 2022) and trained with the modified UL2 objective (Tay et al., 2022). Considering the focused downstream tasks and also to ensure the generalization of our study, we pretrained two sets of bilingual LLMs, i.e. En-De LLM and En-Zh LLM. The pretraining data is a mix of monolingual data from two languages: we use En/De (En/Zh) data with about 280B (206B) tokens to pretrain the En-De (En-Zh) LLM. We train LLMs with parameter sizes from 1B to 16B by varying model configurations as in Table 3 and keep all other settings intact. All LLMs are optimized using Adafactor (Shazeer & Stern, 2018) for one training epoch under a cosine learning rate decay schedule (from 0.01 to 0.001). We refer the readers to (Garcia et al., 2023) for more details about the pretraining.\n' +
      '\n' +
      'Finetuning SettingsWe mainly study the scaling for the following three finetuning methods:\n' +
      '\n' +
      '* **Full-Model Tuning (FMT)**: This is the vanilla way of finetuning which simply optimizes all LLM parameters;\n' +
      '* **Prompt Tuning (Prompt)**: Prompt prepends the input embedding \\(X\\in\\mathbb{R}^{|X|\\times d}\\) with a tunable "soft-prompt" \\(P\\in\\mathbb{R}^{|P|\\times d}\\), and feeds their concatenation \\([P;X]\\in\\mathbb{R}^{(|P|+|X|)\\times d}\\) to LLM. \\(|\\cdot|\\) and \\(d\\) denote sequence length and model dimension, respectively. During finetuning, only the prompt parameter \\(P\\) is optimized. We initialize \\(P\\) from sampled vocabulary, and set the prompt length \\(|P|\\) to 100 by default (Lester et al., 2021).\n' +
      '* **Low-Rank Adaptation (LoRA)**: Rather than modifying LLM inputs, LoRA updates pretrained model weights \\(W\\in\\mathbb{R}^{m\\times n}\\) with trainable pairs of rank decomposition matrices \\(B\\in\\mathbb{R}^{m\\times r},A\\in\\mathbb{R}^{r\\times n}\\), and uses \\(W+BA\\) instead during finetuning. \\(m,n\\) are dimensions and \\(r\\) is LoRA rank. Only \\(B\\)s and \\(A\\)s are optimized. We apply LoRA to both attention and feed-forward layers in LLMs, and set the rank \\(r\\) to 4 by default (Hu et al., 2021).\n' +
      '\n' +
      'We explore 4 different factors for the scaling, which are summarized in Table 0(b). Except LLM model scaling, all experiments are based on the corresponding 1B LLM. For pretraining data scaling, we adopt intermediate pretrained checkpoints as the proxy due to computational budget constraint while acknowledge its sub-optimality. Details for optimization are given in Appendix.\n' +
      '\n' +
      'EvaluationWe use the best checkpoint based on token-level perplexity (PPL) on the dev set for evaluation. For scaling laws, we report PPL on test sets; for general generation, we use greedy decoding, and report BLEURT (Sellam et al., 2020) and Rougel (Lin, 2004) for translation and summarization, respectively. For zero-shot evaluation, we adopt Flores200 (NLLB Team, 2022) and evaluate on (Fr, De, Hindi (Hi), Turkish (Tr), Polish (Po)\\(\\rightarrow\\)Zh) and (Fr, Zh, Hi, Tr, Po\\(\\rightarrow\\)De) for En-Zh and En-De translation respectively. For scaling law evaluation, we split empirical data points into two sets, _empirical fitting_ and _held-out_ set, where the former is used for fitting scaling parameters and the latter is used for evaluation. We report mean absolute deviation. To reduce noise, we perform three runs, each with a different random subset of the finetuning data, and report average performance. When sampling for MLSum, we keep the mixing ratio over different languages fixed.\n' +
      '\n' +
      '## 3 Why Multiplicative Joint Scaling Law?\n' +
      '\n' +
      'We consider 4 scaling factors in this study but jointly modeling all of them is time and resource consuming. Instead, we treat finetuning data as the pivoting factor and perform joint scaling analysis\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 1: Setups for finetuning. “K/B/M”: thousand/billion/million; “#Train”: the number of training examples; “Length”: maximum source/target sequence length cut at training. Note pretraining data size is for token count. **Bold** numbers denote the held-out settings we leave for scaling law verification.\n' +
      '\n' +
      'between it and every other factor separately. Below, we start with finetuning experiments for FMT, Prompt and LoRA on WMT14 En-De, and then explore the formulation for the joint scaling.\n' +
      '\n' +
      'Finetuning data scaling follows a power law.We first examine the scaling over finetuning data size for each LLM model size independently, with a single variable formulation: \\(\\hat{\\mathcal{L}}(D_{f})=\\nicefrac{{A}}{{D_{f}^{s}}}+E\\). Following Hoffmann et al. (2022), we estimate \\(\\{A,\\beta,E\\}\\) using the Huber loss (\\(\\delta=0.001\\)) and the L-BFGS algorithm, and select the best fit from a grid of initializations. Figure 1 shows that the above formulation well describes LLM finetuning data scaling with small predictive errors across model sizes and methods, echoing with the findings of Hernandez et al. (2021). Such scaling trend also implies that, while finetuning with small amount of examples could achieve decent results (Zhou et al., 2023; Gao et al., 2023), larger scale finetuning data still contributes to improved downstream performance, especially when the downstream application is well defined.\n' +
      '\n' +
      'Additive or multiplicative joint scaling law for LLM finetuning?Figure 1 also shows some scaling pattern over LLM model sizes, suggesting the existence of a joint scaling law. We explore two formulations: _multiplicative_ as in Eq. (1) and _additive_: \\(\\hat{\\mathcal{L}}(X,D_{f})=\\nicefrac{{A}}{{X^{\\alpha}}}+\\nicefrac{{B}}{{D_{f} ^{s}}}+E\\) (Hoffmann et al., 2022), and compare them via empirical experiments.1\n' +
      '\n' +
      'Footnote 1: For LLM model scaling, we omitted the newly added parameters in PET because 1) the added parameters only take a very tiny proportion, and 2) the proportion across LLM model sizes is similar. Take the 1B LLM as example. \\(|P|=100\\) in Prompt adds 0.017% parameters; \\(r=4\\) in LoRA adds 0.19% parameters. We also explored different formulations for the new parameters for PET, which don’t make a substantial difference.\n' +
      '\n' +
      'In both formulations, \\(\\alpha\\) and \\(\\beta\\) reflect the impact of factor \\(X\\) and finetuning data size on the performance, respectively, which are factor-specific. \\(E\\) is a model- and task-dependent term, describing irreducible loss (Ghorbani et al., 2021). We notice that the meaning for \\(\\beta\\) and \\(E\\) generalizes over different factors \\(X\\), and thus propose to estimate them first based on results for both LLM model and pretraining data scaling.2 Such joint fitting could also reduce overfitting and improve extrapolation ability. We apply the following joint fitting loss:\n' +
      '\n' +
      'Footnote 2: We didn’t consider PET parameter scaling when estimating \\(\\beta\\) and \\(E\\) because this scaling is pretty weak and ineffective, as shown in Section 4.\n' +
      '\n' +
      '\\[\\min_{a_{X},b_{X},a_{X},\\beta,e}\\sum_{\\text{run 1 in factor }X}\\text{ Huber}_{\\delta}\\left(\\hat{\\mathcal{L}}\\left(X^{i},D_{f}^{i}|a_{X},b_{X}, \\alpha_{X},\\beta,e\\right)-\\mathcal{L}^{i}\\right), \\tag{2}\\]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Scaling Factor} & \\multicolumn{4}{c}{Multiplicative} & \\multicolumn{4}{c}{Additive} \\\\ \\cline{2-10}  & FMT & Prompt & LoRA & Avg & FMT & Prompt & LoRA & Avg \\\\ \\hline LLM Model Size & \\(0.0052\\) & \\(0.0043\\) & \\(0.0047\\) & **0.0048** & \\(0.012\\) & \\(0.0076\\) & \\(0.0045\\) & \\(0.0079\\) \\\\ Pretraining Data Size & \\(0.0057\\) & \\(0.0061\\) & \\(0.0084\\) & **0.0068** & \\(0.0048\\) & \\(0.0075\\) & \\(0.0082\\) & \\(0.0069\\) \\\\ PET parameter size & - & \\(0.005\\) & \\(0.0031\\) & **0.004** & - & \\(0.0069\\) & \\(0.0032\\) & \\(0.005\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Held-out fitting errors (\\(\\downarrow\\)) for the additive and multiplicative scaling formulation over different finetuning methods on WMT14 En-De. Multiplicative scaling law generalizes better.\n' +
      '\n' +
      'Figure 1: Fitted single-variable scaling laws for finetuning data scaling over different LLM model sizes on WMT14 En-De. Solid lines denote fitted scaling curves. Filled circles and triangles denote fitting and held-out data points. \\(\\Delta_{h}\\): mean absolute deviation on the held-out data.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:5]\n' +
      '\n' +
      'dev-set overfitting, challenging issues when tuning on small datasets. We observe high mismatch when extrapolating to 16B, particularly for LoRA and Prompt on WMT19 En-Zh in Figure 2. We ascribe this to 1) the insufficiency of empirical data over LLM model sizes (i.e. only 4 points) - the prediction by the fitted scaling law makes sense intuitively based on 1B-8B results, and 2) the inferior of the 16B En-Zh LLM due to pretraining instability, where its pretraining performance is not well predicted by even single-variable scaling laws as in Figure 10, Appendix.\n' +
      '\n' +
      'LLM finetuning benefits more from LLM model scaling than pretraining data scaling across tasks and methods.While LLM model size and pretraining data size show similar impact on the pretraining scaling following the optimal scaling under a computational budget constraint (Hoffmann et al., 2022; Muenninghoff et al., 2023), they show slightly different roles in finetuning scaling. Intuitively, finetuning heavily relies on the knowledge encoded in the LLM, where LLM model size and pretraining data size both matter. However, results in Figures 2, 3 and Table 4 show that the scaling exponent for LLM model size \\(\\alpha_{m}\\) often outnumbers that for pretraining data size \\(\\alpha_{p}\\) across finetuning methods and tasks, i.e. \\(\\alpha_{m}>\\alpha_{p}\\). This suggests that using a larger LLM model is preferred over pretraining on a larger dataset, but we also notice that the difference in scaling is highly task-dependent. Our selection of closed generation tasks, i.e. translation and summarization, might deliver biased observations and for more creative generation tasks, larger and diverse pretraining data could be more crucial.\n' +
      '\n' +
      'Scaling PET parameters is ineffective, delivering limited gains for both LoRA and Prompt.The amount of newly added trainable parameters often forms a bottleneck for the expressivity of\n' +
      '\n' +
      'Figure 3: Fitted multiplicative joint scaling laws for **pretraining data size and finetuning data size** on WMT14 En-De, WMT19 En-Zh and MLSum(LLM model size: 1B). \\(\\alpha_{p}\\): scaling exponent for pretraining data size.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      'empirical methods by extrapolating the fitted scaling law. Figure 5 shows the critical points as a function of LLM model size and pretraining data size over different tasks.\n' +
      '\n' +
      'The scaling trend and actual value are highly dependent on the downstream task: critical points for one task can hardly generalize to other tasks. Still, the existence of such points suggests that the selection of finetuning methods should be based on the availability of finetuning examples. When only few thousands of finetuning examples are available, PET should be considered first, either Prompt or LoRA. With sightly larger datasets, LoRA would be preferred due to its stability and slightly better finetuning data scalability. For million-scale datasets, FMT would be good.\n' +
      '\n' +
      'How does finetuning affect the generalization capability of the base LLM?While finetuning on task-specific data improves task-specific performance, it may specialize the base LLM towards the task and hurt the models\' generalization. We examine this for different finetuning methods by performing zero-shot translation for LLMs finetuned on WMT14 En-De and WMT19 En-Zh (Few-shot results are in Appendix). We focus on generalization to related tasks, where the target language is shared, i.e. De and Zh, and generalization should be relatively easier (Johnson et al., 2017). We report average performance for translation from a diverse set of source languages other than English.\n' +
      '\n' +
      'Figure 5: Critical finetuning data sizes between different finetuning methods estimated by the fitted joint scaling law on WMT14 En-De, WMT19 En-Zh and MLSum. We use _scipy.optimize.fsolve_ for the estimation. Critical point for “A vs. B”: the finetuning data size (y-axis) at which A performs equal to B under the base model condition at x-axis. The value varies greatly across tasks.\n' +
      '\n' +
      'Figure 6: Zero-shot evaluation for LLM model size and finetuning data size scaling. The score is averaged over {Fr, De, Hi, Tr, Po\\(\\rightarrow\\)Zh} and {Fr, Zh, Hi, Tr, Po\\(\\rightarrow\\)De} for WMT19 En-Zh and WMT14 En-De, respectively.\n' +
      '\n' +
      'Figure 6 shows the results. While specializing on a downstream task, finetuning could still elicit and improve the generalization for closely related tasks, although the overall zero-shot translation quality is inferior. Note whether finetuning benefits generalization is method- and task-dependent. Overall, Prompt and LoRA achieve relatively better results than FMT particularly when the base LLM is large, mostly because LLM parameters are frozen and the learned knowledge get inherited. This also suggests that when generalization capability is a big concern, PET should be considered.\n' +
      '\n' +
      '## 6 Related Work\n' +
      '\n' +
      'LLM finetuningWith the significant increase of model size, updating all LLM parameters becomes computationally inefficient and unaffordable. Researchers thus resort to parameter efficient tuning methods that target achieving the best performance with minimal tunable parameters. Efforts in this direction mainly focus on developing efficient tunable modules for LLMs, such as adapters that insert small feed-forward layers (Houlsby et al., 2019; Bapna et al., 2019), prefix and prompt tuning that appends tunable embeddings to the input (Li and Liang, 2021; Lester et al., 2021), LoRA and compacter that adopts low-rank decomposition (Hu et al., 2021; Mahabadi et al., 2021), Bitfit that adds tunable bias vectors (Zaken et al., 2021), IA3 that scales model activations (Liu et al., 2022) and QLoRA that leverages quantization (Dettmers et al., 2023), to name a few. While previous studies reported encouraging performance with PET, e.g. reaching and even surpassing FMT across various domains (He et al., 2022; Ding et al., 2022; Liu et al., 2022; Dettmers et al., 2023), they mainly focus on one or few experimental setups, leaving the question of how scaling affects the performance of different finetuning methods under-explored.\n' +
      '\n' +
      'Scaling LawsRecent research has shown that the performance of neural models can be predicted by a power-law of model and/or data sizes (Hestness et al., 2017; Kaplan et al., 2020). Such pattern widely exists across different domains and model architectures, such as computer vision (Zhai et al., 2021), autoregressive generative modeling (Henighan et al., 2020), neural machine translation (Gordon et al., 2021; Ghorbani et al., 2021; Bansal et al., 2022; Zhang et al., 2022a), multilingual translation (Fernades et al., 2023), multi-modal modeling (Aghajanyan et al., 2023) and sparse neural architectures (Frantar et al., 2023). These laws provide a valuable tool for guiding training decisions (Hoffmann et al., 2022) and model development by understanding how model performance evolves with scale, which greatly facilitates the development of LLMs (OpenAI, 2023). Unfortunately, the study of scaling for LLM finetuning lags behind badly, and our study fills this gap.\n' +
      '\n' +
      'The most closely related work to ours is (Hernandez et al., 2021) which explored the scaling for knowledge transfer by comparing finetuning with training from scratch. Our study is orthogonal to theirs with significant difference as our key focus is understanding the scaling of different factors for LLM finetuning, rather than the transfer.\n' +
      '\n' +
      '## 7 Conclusion and Future Work\n' +
      '\n' +
      'In this paper, we systematically studied the scaling for LLM finetuning, considering different factors including LLM model size, pretraining data size, finetuning data size, PET parameter size and diverse finetuning methods. To ensure the generality, we worked on two sets of LLMs, three different downstream tasks (translation and summarization), and three finetuning methods (FMT, Prompt and LoRA). We proposed a multiplicative joint scaling law that could describe the scaling relationship between finetuning data size and each other scaling factor. Extensive results show that increasing LLM model size has a higher impact on finetuning than pretraining data scaling, and that scaling PET parameter is ineffective. In addition, finetuning scaling is highly task- and data-dependent, making the selection of best finetuning method for a downstream task less conclusive.\n' +
      '\n' +
      'We acknowledge that our work suffers from some limitations. The proposed joint scaling law is mostly based on empirical results on closed generation tasks without theoretical groundings. Whether it could generalize to different finetuning scenarios requires more experimentation, which however is beyond our current computing budget. Besides, we understand the imperfection of the optimization and evaluation for Prompt and LoRA in some setups. In the future, we would like to extend our study to multi-modal LLMs, explore the impact of finetuning data quality and consider open and creative generation tasks as well as multi-task setup for finetuning.\n' +
      '\n' +
      '## 8 Acknowledgements\n' +
      '\n' +
      'We thank the reviewers for their insightful comments. We thank Yamini Bansal for providing valuable feedback on the scaling laws, Xavier Garcia for reviewing this work with constructive comments, Frederick Liu for helpful discussion on PET optimization, and Quoc Le, Apu Shah and Google Translate team for supporting this research.\n' +
      '\n' +
      'We also thank the colleagues building the training infrastructure used in this paper: Brian Lester, Rami Al-Rfou and Noah Constant for prompt tuning, Chu-Cheng Lin for LoRA, Xavier Garcia and the TSX team (Roberts et al., 2023) for the training framework.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Aghajanyan et al. (2023) Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal language models. _arXiv preprint arXiv:2301.03728_, 2023.\n' +
      '* Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* Baevski et al. (2020) Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. Wav2vec 2.0: A framework for self-supervised learning of speech representations. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS\'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\n' +
      '* Bansal et al. (2022) Yamini Bansal, Behrooz Ghorbani, Ankush Garg, Biao Zhang, Colin Cherry, Behnam Neyshabur, and Orhan Firat. Data scaling laws in NMT: The effect of noise and architecture. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 1466-1482. PMLR, 17-23 Jul 2022. URL [https://proceedings.mlr.press/v162/bansal22b.html](https://proceedings.mlr.press/v162/bansal22b.html).\n' +
      '* Bapna et al. (2019) Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. Simple, scalable adaptation for neural machine translation. _arXiv preprint arXiv:1909.08478_, 2019.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n' +
      '* Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. _arXiv preprint arXiv:2305.14314_, 2023.\n' +
      '* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).\n' +
      '* Ding et al. (2022) Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models, 2022.\n' +
      '* Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=YicbFdNTTY](https://openreview.net/forum?id=YicbFdNTTY).\n' +
      '\n' +
      '* Fernandes et al. (2023) Patrick Fernandes, Behrooz Ghorbani, Xavier Garcia, Markus Freitag, and Orhan Firat. Scaling laws for multilingual neural machine translation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 10053-10071. PMLR, 23-29 Jul 2023. URL [https://proceedings.mlr.press/v202/fernandes23a.html](https://proceedings.mlr.press/v202/fernandes23a.html).\n' +
      '* Frantar et al. (2023) Elias Frantar, Carlos Riquelme, Neil Houlsby, Dan Alistarh, and Utku Evci. Scaling laws for sparsely-connected foundation models, 2023.\n' +
      '* Fu et al. (2023) Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language model negotiation with self-play and in-context learning from ai feedback. _arXiv preprint arXiv:2305.10142_, 2023.\n' +
      '* Gao et al. (2023) Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model. _arXiv preprint arXiv:2304.15010_, 2023.\n' +
      '* Garcia et al. (2023) Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Melvin Johnson, and Orhan Firat. The unreasonable effectiveness of few-shot learning for machine translation. In _International Conference on Machine Learning_, pp. 10867-10878. PMLR, 2023.\n' +
      '* Ghorbani et al. (2021) Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation. _CoRR_, abs/2109.07740, 2021. URL [https://arxiv.org/abs/2109.07740](https://arxiv.org/abs/2109.07740).\n' +
      '* Gong et al. (2023) Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans. _arXiv preprint arXiv:2305.04790_, 2023.\n' +
      '* Gordon et al. (2021) Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws for neural machine translation. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 5915-5922, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.478. URL [https://aclanthology.org/2021.emnlp-main.478](https://aclanthology.org/2021.emnlp-main.478).\n' +
      '* He et al. (2022) Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning, 2022.\n' +
      '* Henighan et al. (2020) Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. _arXiv preprint arXiv:2010.14701_, 2020.\n' +
      '* Hermann et al. (2015) Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In _NIPS_, pp. 1693-1701, 2015. URL [http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend](http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend).\n' +
      '* Hernandez et al. (2021) Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. _arXiv preprint arXiv:2102.01293_, 2021.\n' +
      '* Hestness et al. (2017) Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory F. Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. _CoRR_, abs/1712.00409, 2017. URL [http://arxiv.org/abs/1712.00409](http://arxiv.org/abs/1712.00409).\n' +
      '* Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '* Houlsby et al. (2019) Neil Houlsby, Andrei Giurgui, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pp. 2790-2799. PMLR, 09-15 Jun 2019. URL [https://proceedings.mlr.press/v97/houlsby19a.html](https://proceedings.mlr.press/v97/houlsby19a.html).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), _Advances in Neural Information Processing Systems_, 2022. URL [https://openreview.net/forum?id=TG8KACxEON](https://openreview.net/forum?id=TG8KACxEON).\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. 21(1), jan 2020. ISSN 1532-4435.\n' +
      '* Roberts et al. (2023) Adam Roberts, Hyung Won Chung, Gaurav Mishra, Anselm Levskaya, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. Scaling up models and data with 15x and seqio. _Journal of Machine Learning Research_, 24(377):1-8, 2023.\n' +
      '* Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.\n' +
      '* Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. _arXiv preprint arXiv:2302.04761_, 2023.\n' +
      '* Scialom et al. (2020) Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. MLSUM: The multilingual summarization corpus. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 8051-8067, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.647. URL [https://aclanthology.org/2020.emnlp-main.647](https://aclanthology.org/2020.emnlp-main.647).\n' +
      '* Sellam et al. (2020) Thibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 7881-7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL [https://aclanthology.org/2020.acl-main.704](https://aclanthology.org/2020.acl-main.704).\n' +
      '* Shazeer & Stern (2018) Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In _International Conference on Machine Learning_, pp. 4596-4604. PMLR, 2018.\n' +
      '* Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. _arXiv preprint arXiv:2303.17580_, 2023.\n' +
      '* Tay et al. (2022) Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. Ul2: Unifying language learning paradigms. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.\n' +
      '* Yang et al. (2023) Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages. _arXiv preprint arXiv:2305.18098_, 2023.\n' +
      '* Zaken et al. (2021) Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. _CoRR_, abs/2106.10199, 2021. URL [https://arxiv.org/abs/2106.10199](https://arxiv.org/abs/2106.10199).\n' +
      '\n' +
      '* Zhai et al. (2021) Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. _CoRR_, abs/2106.04560, 2021. URL [https://arxiv.org/abs/2106.04560](https://arxiv.org/abs/2106.04560).\n' +
      '* Zhang et al. (2022a) Biao Zhang, Behrooz Ghorbani, Ankur Bapna, Yong Cheng, Xavier Garcia, Jonathan Shen, and Orhan Firat. Examining scaling and transfer of language model architectures for machine translation. _CoRR_, abs/2202.00528, 2022a. URL [https://arxiv.org/abs/2202.00528](https://arxiv.org/abs/2202.00528).\n' +
      '* Zhang et al. (2023) Biao Zhang, Barry Haddow, and Alexandra Birch. Prompting large language model for machine translation: A case study. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 41092-41110. PMLR, 23-29 Jul 2023. URL [https://proceedings.mlr.press/v202/zhang23m.html](https://proceedings.mlr.press/v202/zhang23m.html).\n' +
      '* Zhang et al. (2022b) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022b.\n' +
      '* Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. _arXiv preprint arXiv:2305.11206_, 2023.\n' +
      '\n' +
      '## Appendix A Appendix\n' +
      '\n' +
      'Optimization for LLM finetuning.For optimization, we continue the pretraining from the given pretrained checkpoint on finetuning data but with the standard conditional log-likelihood loss. More specifically, for each finetuning example, we concatenate the _input_ and _target_ into a single sequence and compute the log-likelihood on the _target_ alone. Adafactor and cosine learning rate schedule are reused. Note En-De and En-Zh LLM are pretrained for 135K and 98K steps, respectively. All LLMs are further finetuned for up to 200K steps (except for WMT En-Zh (FMT) which is 300K steps) or 100 epochs, whichever comes first. To get the best performance, we optimize the initial learning rate and batch size for different finetuning methods based on the 1B LLM via grid search. Finally, we set the learning rate to \\(3e^{-1},1e^{-2}\\) and \\(1e^{-3}\\) for Prompt, LoRA and FMT, respectively, and set the batch size to 16 and 128 for PET and FMT, respectively.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline LLM Model Size & \\#Layers & \\#Heads & Head Dim & FFN Dim & Model Dim \\\\ \\hline\n' +
      '1B & 16 & 8 & 256 & 8192 & 2048 \\\\\n' +
      '2B & 20 & 10 & 256 & 10240 & 2560 \\\\\n' +
      '4B & 24 & 12 & 256 & 12288 & 3072 \\\\\n' +
      '8B & 32 & 16 & 256 & 16384 & 4096 \\\\\n' +
      '16B & 40 & 20 & 256 & 20480 & 5120 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Hyperparameters for different-sized LLMs. “B”: billion; “#Layers, #Heads”: the number of layers and attention heads, respectively; “Head Dim, FFN Dim, Model Dim”: the dimension for each attention head, the feed-forward layer and the hidden representation, respectively.\n' +
      '\n' +
      'Figure 7: Generation quality (BLEURT/RougeL) for scaling **LLM model size and finetuning data size** on WMT14 En-De, WMT19 En-Zh and MLSum. Overall, BLEURT/RougeL correlates positively with PPL with few exceptions.\n' +
      '\n' +
      'Analyzing the critical finetuning data size \\(D_{f}^{c}\\).While Eq. (1) hinders us from computing \\(D_{f}^{c}\\) directly, it still allows for theoretical analysis between two finetuning methods when their performance gap is a constant:\n' +
      '\n' +
      '\\[\\hat{\\mathcal{L}}_{1}-\\hat{\\mathcal{L}}_{2}=E_{1}-E_{2}\\quad\\Longrightarrow\\quad \\hat{D_{f}^{c}}=H*X^{\\gamma},\\quad H=\\left(\\nicefrac{{A_{1}}}{{A_{2}}}\\right)^{ \\frac{1}{p_{1}-\\gamma_{2}}},\\gamma=\\frac{\\alpha_{2}-\\alpha_{1}}{\\beta_{1}- \\beta_{2}} \\tag{3}\\]\n' +
      '\n' +
      ', which follows another power-law. Intuitively, the exponent \\(\\gamma\\) captures the transferability difference of the two methods to the downstream task as scaling factor \\(X\\). We summarize the coefficients for\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Params} & \\multicolumn{2}{c}{WMT14 En-De} & \\multicolumn{4}{c}{WMT19 En-Zh} & \\multicolumn{4}{c}{MLSum} \\\\ \\cline{2-10}  & FMT & Prompt & LoRA & FMT & Prompt & LoRA & FMT & Prompt & LoRA \\\\ \\hline Scaling for LLM model size & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) \\\\ \\(A_{m}\\) & \\(1.2\\times 10^{5}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) \\\\ \\(\\alpha_{m}\\) & \\(0.52\\) & \\(0.4\\) & \\(0.36\\) & \\(0.34\\) & \\(0.33\\) & \\(0.31\\) & \\(0.24\\) & \\(0.1\\) & \\(0.11\\) \\\\ \\hline Scaling for pretraining data size and finetuning data size & & & & & & & & \\\\ \\(A_{p}\\) & \\(6.3\\times 10^{2}\\) & \\(2.7\\times 10^{2}\\) & \\(1.4\\times 10^{2}\\) & \\(2.4\\times 10^{2}\\) & \\(2\\times 10^{2}\\) & \\(1.3\\times 10^{2}\\) & \\(42\\) & \\(16\\) & \\(17\\) \\\\ \\(\\alpha_{p}\\) & \\(0.21\\) & \\(0.21\\) & \\(0.18\\) & \\(0.17\\) & \\(0.2\\) & \\(0.18\\) & \\(0.11\\) & \\(0.069\\) & \\(0.073\\) \\\\ \\hline Scaling for PET parameter size and finetuning data size & & & & & & & & \\\\ \\(A_{t}\\) & - & \\(1\\) & \\(1.4\\) & - & \\(1\\) & \\(1.2\\) & - & \\(2.6\\) & \\(2.4\\) \\\\ \\(\\alpha_{t}\\) & - & \\(0.0027\\) & \\(-0.0017\\) & - & \\(0.0019\\) & \\(0.0044\\) & - & \\(0.0026\\) & \\(0.000\\,22\\) \\\\ \\hline \\(E\\) & \\(0.75\\) & \\(0.62\\) & \\(0.62\\) & \\(1\\) & \\(0.77\\) & \\(0.73\\) & \\(0.98\\) & \\(0.000\\,51\\) & \\(0.2\\) \\\\ \\(\\beta\\) & \\(0.15\\) & \\(0.051\\) & \\(0.081\\) & \\(0.14\\) & \\(0.015\\) & \\(0.025\\) & \\(0.087\\) & \\(0.025\\) & \\(0.03\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Fitted scaling parameters for different settings.\n' +
      '\n' +
      'Figure 8: Generation quality (BLEURT/RougeL) for scaling **pretraining data size** and finetuning data size on WMT14 En-De, WMT19 En-Zh and MLSum.\n' +
      '\n' +
      'different tasks in Table 5, where the value differs greatly over tasks and there are no clear patterns across settings.\n' +
      '\n' +
      'How does finetuning affect the few-shot capability of the base LLM?Apart from zero-shot translation, we also explore LLM\'s few-shot capability after finetuning. Few-shot generation not\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Params} & \\multicolumn{3}{c}{WMT14 En-De} & \\multicolumn{3}{c}{WMT19 En-Zh} & \\multicolumn{3}{c}{MLSum} \\\\ \\cline{2-10}  & F vs. P & F vs. L & P vs. L & F vs. P & F vs. L & P vs. L & F vs. P & F vs. L & P vs. L \\\\ \\hline Scaling LLM model size and finetuning data size & & & & & & & & & \\\\ \\(H\\) & \\(3.7\\times 10^{14}\\) & \\(2\\times 10^{14}\\) & \\(1.6\\times 10^{-9}\\) & \\(6.1\\times 10^{4}\\) & \\(1.6\\times 10^{6}\\) & \\(1.8\\times 10^{-11}\\) & \\(3.6\\times 10^{18}\\) & \\(1.2\\times 10^{17}\\) & \\(0.000\\) 45 \\\\ \\(\\gamma\\) & \\(-1.2\\) & \\(-2.4\\) & \\(1.5\\) & \\(-0.12\\) & \\(-0.3\\) & \\(1.9\\) & \\(-2.1\\) & \\(-1.8\\) & \\(2.3\\) \\\\ \\hline Scaling pretraining data size and finetuning data size & & & & & & & & & \\\\ \\(H\\) & \\(3.7\\times 10^{3}\\) & \\(6.9\\times 10^{8}\\) & \\(7.7\\times 10^{-10}\\) & \\(5\\) & \\(2.7\\times 10^{2}\\) & \\(8.6\\times 10^{-19}\\) & \\(3.7\\times 10^{6}\\) & \\(1\\times 10^{7}\\) & \\(1.6\\times 10^{2}\\) \\\\ \\(\\gamma\\) & \\(-0.0015\\) & \\(-0.5\\) & \\(1.2\\) & \\(0.26\\) & \\(0.093\\) & \\(2.1\\) & \\(-0.63\\) & \\(-0.63\\) & \\(-0.63\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Coefficients in Eq. (3) by comparing different methods over setups. “F/P/L”: FMT/Prompt/LoRA.\n' +
      '\n' +
      'Figure 10: Fitted single-variable scaling laws for En-De and En-Zh LLM pretraining. We evaluate the model on a held-out validation set and fit the scaling law based on PPL. Note that the scaling law doesn’t well extrapolate to 16B for En-Zh LLM whose actual performance is worse than the expectation (This might be caused by pretraining instabilities.). Such mismatch we argue is amplified after finetuning as shown in Figure 2.\n' +
      '\n' +
      'Figure 9: Generation quality (BLEURT/RougeL) for scaling **PET parameter size and finetuning data size** on WMT14 En-De, WMT19 En-Zh and MLSum.\n' +
      '\n' +
      'only offers a way to inspect LLM\'s capability but also is of interest to downstream applications as it provides an effective way to adapt models over domains. Figures 11, 12, 13 and 14 shows the impact of finetuning on few-shot generation.\n' +
      '\n' +
      'We note that FMT degenerates LLM\'s few-shot capability in most cases, where adding more fine-tuning data often reduces the few-shot performance. By contrast, PET behaves more robustly which retains most of LLM\'s few-shot capability regardless of model size and pretraining data size.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c c c} \\hline \\hline  & Scaling Factor & \\multicolumn{4}{c}{Multiplicative} & \\multicolumn{4}{c}{Additive} \\\\ \\cline{3-10}  & & FMT & Prompt & LoRA & Avg & FMT & Prompt & LoRA & Avg \\\\ \\hline \\multirow{3}{*}{WMT En-De} & LLM Model Size & \\(0.0052\\) & \\(0.0043\\) & \\(0.0047\\) & **0.0048** & \\(0.012\\) & \\(0.0076\\) & \\(0.0045\\) & \\(0.0079\\) \\\\  & Pretraining Data Size & \\(0.0057\\) & \\(0.0061\\) & \\(0.0084\\) & **0.0068** & \\(0.0048\\) & \\(0.0075\\) & \\(0.0082\\) & \\(0.0069\\) \\\\  & PET parameter size & - & \\(0.005\\) & \\(0.0031\\) & **0.004** & - & \\(0.0069\\) & \\(0.0032\\) & \\(0.005\\) \\\\ \\hline \\multirow{3}{*}{WMT En-Zh} & LLM Model Size & \\(0.0075\\) & \\(0.019\\) & \\(0.026\\) & **0.018** & \\(0.021\\) & \\(0.018\\) & \\(0.029\\) & \\(0.022\\) \\\\  & Pretraining Data Size & \\(0.002\\) & \\(0.0071\\) & \\(0.0056\\) & **0.0049** & \\(0.0026\\) & \\(0.0069\\) & \\(0.0058\\) & \\(0.0051\\) \\\\  & PET parameter size & - & \\(0.0075\\) & \\(0.0051\\) & \\(0.0063\\) & - & \\(0.0076\\) & \\(0.0044\\) & **0.006** \\\\ \\hline \\multirow{3}{*}{MLSum} & LLM Model Size & \\(0.0066\\) & \\(0.013\\) & \\(0.022\\) & \\(0.014\\) & \\(0.0072\\) & \\(0.015\\) & \\(0.017\\) & **0.013** \\\\  & Pretraining Data Size & \\(0.009\\) & \\(0.0083\\) & \\(0.0039\\) & \\(0.007\\) & \\(0.0062\\) & \\(0.0046\\) & \\(0.0043\\) & **0.005** \\\\  & PET parameter size & - & \\(0.0081\\) & \\(0.003\\) & \\(0.0055\\) & - & \\(0.0053\\) & \\(0.0027\\) & **0.004** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Held-out fitting errors (\\(\\downarrow\\)) for the additive and multiplicative scaling formulation over different tasks. Overall, multiplicative scaling law generalizes better.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c} \\hline \\hline  & Scaling Factor & FMT & Prompt & LoRA \\\\ \\hline \\multirow{3}{*}{WMT En-De} & LLM Model Size & -0.184 & -0.986\\({}^{\\dagger}\\) & -0.988\\({}^{\\ddagger}\\) \\\\  & Pretraining Data Size & -0.792\\({}^{\\dagger}\\) & -0.967\\({}^{\\dagger}\\) & -0.980\\({}^{\\ddagger}\\) \\\\  & PET parameter size & - & -0.841\\({}^{\\ddagger}\\) & -0.975\\({}^{\\ddagger}\\) \\\\ \\hline \\multirow{3}{*}{WMT En-Zh} & LLM Model Size & -0.984\\({}^{\\ddagger}\\) & -0.994\\({}^{\\dagger}\\) & -0.995\\({}^{\\ddagger}\\) \\\\  & Pretraining Data Size & -0.994\\({}^{\\dagger}\\) & -0.979\\({}^{\\ddagger}\\) & -0.978\\({}^{\\ddagger}\\) \\\\  & PET parameter size & - & -0.643\\({}^{\\ddagger}\\) & -0.968\\({}^{\\ddagger}\\) \\\\ \\hline \\multirow{3}{*}{MLSum} & LLM Model Size & -0.965\\({}^{\\dagger}\\) & -0.909\\({}^{\\ddagger}\\) & -0.890\\({}^{\\ddagger}\\) \\\\  & Pretraining Data Size & -0.941\\({}^{\\ddagger}\\) & -0.833\\({}^{\\ddagger}\\) & -0.838\\({}^{\\ddagger}\\) \\\\  & PET parameter size & - & -0.924\\({}^{\\dagger}\\) & -0.986\\({}^{\\ddagger}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Pearson correlation between PPL and BLEURT/RougeL for different finetuning methods and setups. “\\({}^{\\ddagger}\\)”: the correlation is significant at \\(p<0.01\\). Note lower PPL and higher BLEURT/RougeL denote better quality, thus their correlation values are negative. In general, PPL and BLEURT/RougeL are highly correlated.\n' +
      '\n' +
      'Figure 11: One-shot performance (BLEURT/RougeL) for LLM model size and finetuning data size scaling on WMT14 En-De, WMT19 En-Zh and MLSum. ‘Baseline’: performance without finetuning.\n' +
      '\n' +
      'Figure 12: Five-shot performance (BLEURT/RougeL) for LLM model size and finetuning data size scaling on WMT14 En-De and WMT19 En-Zh.\n' +
      '\n' +
      'Figure 14: Five-shot performance (BLEURT/RougeL) for pretraining and finetuning data size scaling on WMT14 En-De and WMT19 En-Zh.\n' +
      '\n' +
      'Figure 13: One-shot performance (BLEURT/RougeL) for pretraining and finetuning data size scaling on WMT14 En-De, WMT19 En-Zh and MLSum. ‘Baseline’: performance without finetuning.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>