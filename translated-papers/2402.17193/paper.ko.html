<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#Scaling이 LLM Finetununing을 만났을 때:\n' +
      '\n' +
      '데이터, 모델 및 Finetuning 방법의 효과\n' +
      '\n' +
      'Biao Zhang\\({}^{\\dagger}\\) Zhongtao Liu\\({}^{\\diamond}\\) Colin Cherry\\({}^{\\diamond}\\) Orhan Firat\\({}^{\\dagger}\\)\n' +
      '\n' +
      'Google DeepMind\\({}^{\\dagger}\\)Google DeepMind\\({}^{\\diamond}\\)Google Research Google DeepMind\\({}^{\\diamond}\\)\n' +
      '\n' +
      '{biaojiaxing,zhongtao,colincherry,orhanf}@google.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)은 다운스트림 애플리케이션에 대한 기능을 잠금 해제하기 위해 종종 _finetuning_를 채택하지만, 상이한 finetuning 방법의 유도 편향(특히 스케일링 속성)에 대한 우리의 이해는 여전히 제한적이다. 이 격차를 메우기 위해 LLM 모델 크기, 사전 훈련 데이터 크기, 새로운 미세 조정 매개변수 크기 및 미세 조정 데이터 크기를 포함한 다양한 스케일링 인자가 미세 조정 성능에 영향을 미치는지 여부와 방법을 연구하는 체계적인 실험을 수행한다. 본 논문에서는 FMT(Full-Model Tuning)와 LoRA(Parameter efficient Tuning)의 두 가지 방법을 고려하고, LLM 모델 크기가 Finetuning 데이터 크기를 크게 능가하는 데이터 제한 영역에서의 스케일링 동작을 탐색한다. 1B에서 16B까지의 두 가지 사전 훈련된 이중언어 LLM 집합과 이중언어 기계번역 및 다국어 요약 벤치마크에 대한 실험을 바탕으로, 1) LLM Finetuning 데이터 크기와 서로 다른 스케일링 인자 사이의 power-based multiplicative joint scaling 법칙을 따르는 LLM Finetuning 방법, 2) LLM Finetuning 데이터 크기보다 LLM 모델 스케일링으로 얻는 이득이 더 크며, PET 파라미터 스케일링은 일반적으로 효과적이지 않으며, 3) 최적의 Finetuning 방법은 작업 및 Finetuning 데이터 의존성이 높다. 우리의 연구 결과가 LLM 미세 조정 방법을 이해하고 선택하고 개발하는 데 도움이 되기를 바란다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '다운스트림 응용을 위해 대규모 사전 훈련된 모델에서 인코딩된 지식을 활용하고 전달하는 것은 다양한 도메인에서 달성된 최근 성공의 기초가 되는 표준 패러다임이 되었다(Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020; Dosovitskiy et al., 2021; Baevski et al., 2020), 언어 태스크 전반에 걸쳐 획기적인 성능을 산출한 대규모 언어 모델(LLM)에 의해 설정된 현저한 이정표이다(Brown et al., 2020; Zhang et al., 2022; Scao et al., 2022; Touvron et al., 2023). GPT-4(OpenAI, 2023) 및 PaLM 2(Anil et al., 2023)와 같은 진보된 LLMs는 종종 창발 능력을 보여주고 복잡한 추론 및 생성 작업을 수행하기 위해 소수의 시연 예만을 사용할 수 있는 상황 내 학습을 허용한다(Wei et al., 2022; Zhang et al., 2023; Fu et al., 2023; Shen et al., 2023). 여전히, LLM 미세조정은 창조적 작업에 대한 새롭고 강력한 능력을 잠금해제하고, 집중된 다운스트림 작업에 대해 가장 많이 획득하며, 그 가치를 인간 선호도에 정렬하기 위해 요구되고 널리 채택된다(Ouyang et al., 2022; Yang et al., 2023; Gong et al., 2023; Schick et al., 2023). 이는 수년에 걸쳐 축적된 대규모 주석이 달린 작업별 데이터의 존재로 인해 전통적인 산업 응용 분야에서 더욱 중요해진다.\n' +
      '\n' +
      '1) LLM 모델 크기 및 사전 훈련 데이터 크기와 같은 사전 훈련 조건, 2) 다운스트림 작업, 미세 조정 데이터 크기 및 미세 조정 방법과 같은 미세 조정 조건을 포함하여 LLM 미세 조정 성능에 영향을 미치는 많은 잠재적 요인이 있다. 직관적으로 사전 훈련은 사전 훈련된 LLM에서 학습된 표현과 지식의 질을 조절하고, 미세 조정은 다운스트림 작업으로의 전이 정도에 영향을 미친다. 선행 연구들이 LLM 사전 훈련 또는 훈련을 위한 스케일링(Kaplan et al., 2020; Hoffmann et al., 2022)과 고급 효율적인 미세 조정 방법의 개발(Hu et al., 2021; He et al., 2022)을 잘 탐구했지만, LLM 미세 조정 척도가 위와 같은 요인으로 불행히도 거의 관심을 받지 않는지 여부와 방법에 대한 질문(Hernandez et al., 2021)은 우리 연구의 초점이다.\n' +
      '\n' +
      '주목할 점은, 미세조정의 성능 향상과는 별도로, LLM 미세조정에 대한 스케일링을 연구하는 것은 미세조정의 관점에서 다양한 사전 훈련 요인의 영향을 이해하는 데 도움이 될 수 있으며, 이는 LLM 사전 훈련에 대한 통찰력을 제공할 수 있다.\n' +
      '\n' +
      '본 논문에서는 LLM finetuning의 대표적인 두 가지 방법 즉, 모든 LLM 파라미터를 업데이트하는 _full-model tuning_ (FMT)와 프롬프트 튜닝(Lester et al., 2021, Prompt)과 저순위 적응(Hu et al., 2021, LoRA)과 같이 소량의 (새로 추가된) 파라미터만을 최적화하는 _parameter-efficient tuning_ (PET)에 대한 스케일링을 체계적으로 연구하여 위의 문제를 해결한다. 우리는 먼저 미세 조정 데이터 스케일링(Hernandez et al., 2021)을 조사하고, 그 위에 LLM 모델 크기, 사전 훈련 데이터 크기 및 PET 파라미터 크기를 포함한 다른 스케일링 인자들과의 스케일링 관계를 추가로 탐구한다. 우리는 LLM 모델보다 미세화 데이터가 훨씬 작아 LLM 시대의 상황을 더 잘 반영하는 데이터 제한 체제에 초점을 맞춘다. 실험을 위해 모델 크기가 1B에서 16B 사이인 두 개의 이중언어 LLM(English&German, English&Chinese) 세트를 사전 훈련하고, 최대 20M 미세화 예제를 사용하여 WMT 기계 번역(English-German, English-Chinese)과 다국어 요약(English, German, French, Spanish) 작업에 대한 대규모 연구를 수행했다. 주요 연구 결과는 다음과 같다.\n' +
      '\n' +
      'LLM finetuning을 위한 곱셈적 절리 스케일링 법칙을 제안한다. \\[\\hat{\\mathcal{L}(X,D_{f})=A*\\frac{1}{X^{\\alpha}*\\frac{1}{D_{f}^{\\beta}+E,\\(1) 여기서 \\(A,E,\\alpha,\\beta\\}\\)은 피팅될 데이터 특정 파라미터이고, \\(D_{f}\\)은 finetuning 데이터 크기를 나타내며, \\(X\\)은 다른 스케일링 인자 각각을 나타낸다. 우리는 이 공동법이 다른 설정으로 일반화되고 있다는 실증적 증거를 보여준다.\n' +
      '* 스케일링 LLM 모델은 스케일링 프리트레이닝 데이터를 스케일링하는 것보다 LLM 미세조정에 더 유리하다.\n' +
      '* LoRA가 더 나은 훈련 안정성을 보여주지만, PET 파라미터들의 증가는 LoRA 및 프롬프트에 대해 잘 스케일링되지 않는다.\n' +
      '* LLM 피네튜닝에 대한 스케일링 특성은 태스크 및 데이터 의존성이 높아 다운스트림 태스크에 대한 최적의 피네튜닝 방법의 선택이 자명하지 않다.\n' +
      '* LLM 기반 미세 조정은 관련 작업에 대한 제로 샷 일반화를 장려할 수 있으며 PET는 FMT보다 훨씬 더 나은 성능을 보인다.\n' +
      '\n' +
      '## 2 Setup\n' +
      '\n' +
      '다운스트림 태스크는 기계 번역 및 다국어 요약이 피네튜닝의 다운스트림 태스크로 간주되는데, 1) 이러한 태스크는 복잡도가 높고 어려운 교차 언어 이해 및 생성을 해결해야 하고 2) 사용 가능한 피네튜닝 코퍼스의 양이 풍부한 NLP에서 잘 확립되어 있기 때문이다. 특히, 번역을 위해 WMT14 English-German(En-De)과 WMT19 English-Chinese(En-Zh)(Kocmi et al., 2022)를 채택하였다. 다국어 요약 데이터셋(Scialom et al., 2020)의 De, Spanish(Es) 및 French(Fr) 부분을 CNN/Daily-Mail(Hermann et al., 2015, En)과 결합하여 MLSum으로 나타낸다. 각 작업에 대한 세부 사항은 표 1a에 나열되어 있다. MLSum의 경우, 우리는 훈련 및 평가를 위해 서로 다른 언어의 데이터 세트를 직접 연결하며, 여기서 각 기사는 "_lang]:_에서 다음 문서를 요약한다"라는 언어를 나타내는 프롬프트를 준비한다.\n' +
      '\n' +
      'LLM 및 PrerainingWe는 LLM 사전 훈련을 위해 Garcia et al.(2023)에서와 같이 정확한 설정을 채택한다. 모델은 다중 질의 어텐션을 갖는 디코더 전용 트랜스포머(Chowdhery et al., 2022)이고 수정된 UL2 대물렌즈(Tay et al., 2022)로 트레이닝된다. 집중된 다운스트림 작업을 고려하고 연구의 일반화를 보장하기 위해 En-De LLM과 En-Zh LLM이라는 두 세트의 이중 언어 LLM을 사전 훈련했다. 사전 훈련 데이터는 두 언어의 단일 언어 데이터를 혼합한 것으로, En-De(En-Zh) LLM을 사전 훈련하기 위해 약 280B(206B) 토큰과 En/De(En/Zh) 데이터를 사용한다. 표 3과 같이 모델 구성을 변경하여 매개변수 크기가 1B에서 16B인 LLM을 훈련하고 다른 모든 설정을 그대로 유지한다. 모든 LLM은 코사인 학습 속도 감쇠 스케줄(0.01에서 0.001) 하에서 하나의 훈련 에폭에 대해 Adafactor(Shazeer & Stern, 2018)를 사용하여 최적화된다. 사전 훈련에 대한 자세한 내용은 독자들(Garcia et al., 2023)에게 문의한다.\n' +
      '\n' +
      'Finetuning SettingsWe 주로 다음의 세 가지 Finetuning 방법에 대한 scaling을 연구한다:\n' +
      '\n' +
      '**Full-Model Tuning(FMT)**: 이는 모든 LLM 파라미터를 단순히 최적화하는 파인튜닝의 바닐라 방식;\n' +
      '입력 임베딩 \\(X\\in\\mathbb{R}^{|X|\\times d}\\)을 튜닝 가능한 "소프트-프롬프트"\\(P\\in\\mathbb{R}^{|P|\\times d}\\)으로 수정하고, 이들의 연결 \\([P;X]\\in\\mathbb{R}^{(|P|+|X|)\\times d}\\)을 LLM에 공급한다. \\(X\\in\\mathbb{R}^{(|P|+|X|)\\times d}\\) (|\\cdot|\\) 및 \\(d\\)는 각각 서열 길이 및 모델 차원을 나타낸다. 미세 조정 중에는 프롬프트 매개변수 \\(P\\)만 최적화됩니다. 샘플링된 어휘로부터 \\(P\\)을 초기화하고, 기본값으로 프롬프트 길이 \\(|P|\\)을 100으로 설정한다(Lester et al., 2021).\n' +
      '**Low-Rank Adaptation (LoRA)**: LLM 입력을 수정하는 것이 아니라, LoRA는 트레이닝 가능한 쌍의 랭크 분해 행렬 \\(B\\in\\mathbb{R}^{m\\times r},A\\in\\mathbb{R}^{r\\times n}\\)으로 사전 트레이닝된 모델 가중치 \\(W\\in\\mathbb{R}^{m\\times n}\\)을 업데이트하고, finetuning 동안 \\(W+BA\\)을 대신 사용한다. \\in\\mathbb{R}^{m\\times r},A\\in\\mathbb{R}^{r\\times n}\\. (m,n\\)은 차원이고 \\(r\\)은 LoRA 순위이다. 단지 \\(B\\)s와 \\(A\\)s만이 최적화된다. LLM의 attention layer와 feed-forward layer에 LoRA를 적용하였고, rank \\(r\\)을 기본값으로 4로 설정하였다 (Hu et al., 2021).\n' +
      '\n' +
      '우리는 표 0(b)에 요약된 스케일링에 대한 4가지 다른 요인을 탐색한다. LLM 모델 스케일링을 제외하고 모든 실험은 해당 1B LLM을 기반으로 한다. 사전 훈련 데이터 스케일링을 위해 계산 예산 제약으로 인해 중간 사전 훈련 체크포인트를 프록시로 채택하는 동시에 하위 최적화를 인정한다. 최적화에 대한 자세한 내용은 부록에 나와 있습니다.\n' +
      '\n' +
      '평가 우리는 평가를 위해 DEV 집합에서 토큰 수준 복잡도(PPL)를 기반으로 하는 최상의 체크포인트를 사용한다. 스케일링 법칙을 위해 테스트 세트에 대한 PPL을 보고하며, 일반 생성을 위해 그리디 디코딩을 사용하고, 번역 및 요약에 대해 각각 BLEURT(Sellam et al., 2020) 및 Rougel(Lin, 2004)을 보고한다. 0-shot 평가를 위해 Flores200(NLLB Team, 2022)을 채택하여 En-Zh와 En-De 번역을 각각 Fr, De, Hindi(Hi), Turkey(Tr), Polish(Po)\\(\\rightarrow\\)Zh)와 Fr, Zh, Hi, Tr, Po\\(\\rightarrow\\)De에 대해 평가한다. 스케일링 법칙 평가를 위해 경험적 데이터 포인트를 _empirical fitting_와 _held-out_ set의 두 집합으로 나누었으며, 여기서 전자는 스케일링 파라미터의 피팅에 사용되고 후자는 평가에 사용된다. 우리는 평균 절대 편차를 보고한다. 잡음을 줄이기 위해 세 번의 실행을 수행하는데, 각각은 미세 조정 데이터의 다른 랜덤 서브세트를 가지고 평균 성능을 보고한다. ML섬을 샘플링할 때 서로 다른 언어에 대한 혼합 비율을 고정한다.\n' +
      '\n' +
      '#3 왜 곱셈적 합동 척도법인가?\n' +
      '\n' +
      '본 연구에서는 4개의 스케일링 요인을 고려하지만, 이들 모두를 공동으로 모델링하는 것은 시간과 자원이 많이 소모된다. 대신, 우리는 피벗팅 팩터로 미세화 데이터를 취급하고 조인트 스케일링 분석을 수행한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 1: 미세조정을 위한 설정. “K/B/M”: 천/억/백만; “#Train”: 훈련 예제 수; “Length”: 훈련 시 절단된 최대 소스/타겟 시퀀스 길이. 노트 사전 훈련 데이터 크기는 토큰 카운트에 대한 것입니다. **굵은** 숫자는 스케일링법 검증을 위해 남겨두는 고정 설정을 나타냅니다.\n' +
      '\n' +
      '그것과 다른 모든 요인들 사이에 개별적으로. 아래에서는 WMT14 En-De에서 FMT, Prompt 및 LoRA에 대한 미세 조정 실험으로 시작하여 조인트 스케일링을 위한 제형을 탐구한다.\n' +
      '\n' +
      '데이터 크기 조정은 멱 법칙을 따른다. 먼저 각 LLM 모델 크기에 대한 데이터 크기 조정에 대한 스케일링을 단일 변수 공식인 \\(\\hat{\\mathcal{L}}(D_{f})=\\nicefrac{A}{D_{f}^{s}}+E\\)로 독립적으로 검토한다. Hoffmann et al. (2022)에 이어 Huber loss (\\(\\delta=0.001\\))와 L-BFGS 알고리즘을 이용하여 \\(\\{A,\\beta,E\\}\\)을 추정하고, 초기화의 그리드에서 최적의 적합도를 선택한다. 도 1은 상기 공식이 Hernandez et al.(2021)의 발견에 반향하는, 모델 크기 및 방법에 걸쳐 작은 예측 오차를 갖는 LLM 미세조정 데이터 스케일링을 잘 기술하고 있음을 보여준다. 이러한 스케일링 경향은 또한 적은 양의 예제를 갖는 미세조정이 양호한 결과를 달성할 수 있는 반면(Zhou et al., 2023; Gao et al., 2023), 더 큰 스케일 미세조정 데이터는 특히 다운스트림 애플리케이션이 잘 정의될 때 여전히 개선된 다운스트림 성능에 기여함을 의미한다.\n' +
      '\n' +
      'LLM 미세조정에 대한 덧셈 또는 곱셈 조인트 스케일링 법칙?그림 1은 또한 LLM 모델 크기에 대한 일부 스케일링 패턴을 보여 조인트 스케일링 법칙의 존재를 시사한다. 우리는 Eq에서와 같이 _곱셈_의 두 가지 공식을 탐구한다. (1) 및 _additive_: \\(\\hat{\\mathcal{L}}(X,D_{f})=\\nicefrac{A}}{X^{\\alpha}}+\\nicefrac{B}}{D_{f}}^{s}}+E\\)(Hoffmann et al., 2022)을 사용하여 경험적 실험을 통해 비교.1\n' +
      '\n' +
      '각주 1: LLM 모델 스케일링의 경우, 1) 추가된 매개변수는 매우 작은 비율만 취하고 2) LLM 모델 크기에 걸친 비율은 유사하기 때문에 PET에서 새로 추가된 매개변수를 생략했다. 1B LLM을 예로 들어보자. \\ (|P|=100\\) in Prompt는 0.017%의 파라미터를 추가하고; \\(r=4\\) in LoRA는 0.19%의 파라미터를 추가한다. 또한 PET에 대한 새로운 매개변수에 대한 다양한 제형을 조사했는데, 이는 실질적인 차이를 만들지 않는다.\n' +
      '\n' +
      '두 공식 모두에서 \\(\\alpha\\) 및 \\(\\beta\\)은 인자별 성능인 인자\\(X\\) 및 미세조정 데이터 크기가 성능에 미치는 영향을 각각 반영한다. \\\\ (E\\)은 기약 손실(Ghorbani et al., 2021)을 설명하는 모델 및 태스크 종속 용어이다. 우리는 \\(\\beta\\)와 \\(E\\)에 대한 의미가 서로 다른 요인 \\(X\\)에 대해 일반화된다는 것을 알아내고, 따라서 LLM 모델과 사전 훈련 데이터 스케일링에 대한 결과를 기반으로 먼저 추정하도록 제안한다.2 이러한 관절 피팅은 또한 과적합을 줄이고 외삽 능력을 향상시킬 수 있다. 다음과 같은 조인트 피팅 손실을 적용한다:\n' +
      '\n' +
      '각주 2: 우리는 \\(\\beta\\)와 \\(E\\)을 추정할 때 PET 파라미터 스케일링을 고려하지 않았다. 왜냐하면 이 스케일링은 섹션 4에 나타낸 바와 같이 상당히 약하고 효과적이지 않기 때문이다.\n' +
      '\n' +
      '요인×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Scaling Factor} & \\multicolumn{4}{c}{Multiplicative} & \\multicolumn{4}{c}{Additive} \\\\ \\cline{2-10}  & FMT & Prompt & LoRA & Avg & FMT & Prompt & LoRA & Avg \\\\ \\hline LLM Model Size & \\(0.0052\\) & \\(0.0043\\) & \\(0.0047\\) & **0.0048** & \\(0.012\\) & \\(0.0076\\) & \\(0.0045\\) & \\(0.0079\\) \\\\ Pretraining Data Size & \\(0.0057\\) & \\(0.0061\\) & \\(0.0084\\) & **0.0068** & \\(0.0048\\) & \\(0.0075\\) & \\(0.0082\\) & \\(0.0069\\) \\\\ PET parameter size & - & \\(0.005\\) & \\(0.0031\\) & **0.004** & - & \\(0.0069\\) & \\(0.0032\\) & \\(0.005\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: WMT14 En-De의 다른 미세 조정 방법에 대한 첨가제 및 승법 스케일링 제형에 대한 헬드 아웃 피팅 오차(\\(\\downarrow\\))이다. 곱셈법칙은 더 잘 일반화한다.\n' +
      '\n' +
      '그림 1: WMT14 En-De에서 서로 다른 LLM 모델 크기에 대한 데이터 스케일링을 미세 조정하기 위한 적합 단일 변수 스케일링 법칙. 실선은 적합된 축척 곡선을 나타냅니다. 채워진 원과 삼각형은 피팅 및 고정 데이터 점을 나타냅니다. \\ (\\Delta_{h}\\): 보류된 데이터에 대한 평균 절대 편차.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:5]\n' +
      '\n' +
      '작은 데이터 세트를 조정할 때 문제가 되는 데브 세트 오버피팅. 특히 그림 2에서 LoRA와 WMT19 En-Zh에 대한 프롬프트의 경우 16B로 외삽할 때 높은 불일치를 관찰한다. 1) LLM 모델 크기에 대한 경험적 데이터의 부족(즉, 4점 만) - 적합 스케일링 법칙에 의한 예측은 1B-8B 결과를 기반으로 직관적으로 의미가 있으며, 2) 16B En-Zh LLM은 사전 훈련 불안정성으로 인해 열등하며, 이는 그림 10과 같이 단일 변수 스케일링 법칙으로도 사전 훈련 성능이 잘 예측되지 않는다.\n' +
      '\n' +
      'LLM 미세 조정은 작업 및 방법에 걸쳐 사전 훈련 데이터 스케일링보다 LLM 모델 스케일링에서 더 많은 이점을 제공한다. LLM 모델 크기와 사전 훈련 데이터 크기는 계산 예산 제약 하에서 최적의 스케일링에 따른 사전 훈련 스케일링에 유사한 영향을 나타내지만(Hoffmann et al., 2022; Muenninghoff et al., 2023), 미세 조정 스케일링에서 약간 다른 역할을 나타낸다. 직관적으로 미세 조정은 LLM 모델 크기와 사전 훈련 데이터 크기가 모두 중요한 LLM에 인코딩된 지식에 크게 의존한다. 그러나 그림 2, 3 및 표 4의 결과는 LLM 모델 크기\\(\\alpha_{m}\\)에 대한 스케일링 지수가 미세 조정 방법 및 작업에 걸쳐 데이터 크기\\(\\alpha_{p}\\)을 사전 훈련하는 경우, 즉 \\(\\alpha_{m}>\\alpha_{p}\\)보다 종종 더 크다는 것을 보여준다. 이것은 더 큰 데이터 세트에 대한 사전 훈련보다 더 큰 LLM 모델을 사용하는 것이 선호된다는 것을 시사하지만, 우리는 또한 스케일링의 차이가 작업 의존성이 높다는 것을 알아차린다. 폐쇄형 생성 작업, 즉 번역 및 요약의 선택은 편향된 관찰을 전달할 수 있으며 더 창의적인 생성 작업의 경우 더 크고 다양한 사전 훈련 데이터가 더 중요할 수 있다.\n' +
      '\n' +
      '확장 PET 매개변수는 비효율적이며 LoRA와 Prompt 모두에 제한된 이득을 제공한다. 새로 추가된 훈련 가능한 매개변수의 양은 종종 의 표현성에 병목 현상을 형성한다.\n' +
      '\n' +
      '도 3: WMT14 En-De, WMT19 En-Zh 및 MLSum(LLM 모델 크기: 1B)의 **사전 훈련 데이터 크기 및 미세 조정 데이터 크기**에 대한 적합 곱셈 조인트 스케일링 법칙. \\ (\\alpha_{p}\\): 데이터 크기를 사전 훈련하기 위한 스케일링 지수.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '적합된 스케일링 법칙을 외삽하여 경험적 방법. 그림 5는 다양한 작업에 대한 LLM 모델 크기와 사전 훈련 데이터 크기의 함수로 임계점을 보여준다.\n' +
      '\n' +
      '스케일링 추세와 실제 값은 다운스트림 작업에 크게 의존하며, 한 작업에 대한 임계점은 다른 작업에 거의 일반화할 수 없다. 그럼에도 불구하고, 그러한 점들의 존재는 미세조정 방법의 선택이 미세조정 예제의 가용성에 기초해야 함을 시사한다. 수천 개의 미세 조정 예만 사용할 수 있는 경우, 프롬프트 또는 LoRA 중 하나를 먼저 PET를 고려해야 한다. 시각적으로 더 큰 데이터 세트를 사용하면 안정적이고 데이터 확장성을 약간 더 잘 조정하기 때문에 LoRA가 선호될 것이다. 백만 개의 데이터 세트의 경우 FMT가 좋습니다.\n' +
      '\n' +
      '피네튜닝은 기본 LLM의 일반화 능력에 어떤 영향을 미치는가? 태스크별 데이터에 대한 피네튜닝은 태스크별 성능을 향상시키지만, 기본 LLM을 태스크에 특화시키고 모델의 일반화에 타격을 줄 수 있다. 우리는 WMT14 En-De 및 WMT19 En-Zh에서 미세 조정된 LLM에 대해 제로 샷 변환을 수행하여 다양한 미세 조정 방법에 대해 이를 조사한다(Few-샷 결과는 부록에 있다). 우리는 대상 언어가 공유되는 관련 과제, 즉 De와 Zh에 대한 일반화에 초점을 맞추고, 일반화가 상대적으로 용이해야 한다(Johnson et al., 2017). 우리는 영어 이외의 다양한 소스 언어의 번역에 대한 평균 성능을 보고한다.\n' +
      '\n' +
      '그림 5: WMT14 En-De, WMT19 En-Zh 및 MLSum에 대한 적합 조인트 스케일링 법칙에 의해 추정된 상이한 미세조정 방법들 사이의 임계 미세조정 데이터 크기들. 추정에는 _scipy.optimize.fsolve_를 사용한다. "A vs. B"의 임계점 : x축의 기본 모델 조건에서 A가 B와 동일하게 수행하는 미세 조정 데이터 크기(y축)입니다. 그 가치는 업무마다 크게 다르다.\n' +
      '\n' +
      '그림 6: LLM 모델 크기와 미세 조정 데이터 크기 스케일링에 대한 제로샷 평가. 점수는 WMT19 En-Zh와 WMT14 En-De에 대해 각각 {Fr, De, Hi, Tr, Po\\(\\rightarrow\\)Zh}와 {Fr, Zh, Hi, Tr, Po\\(\\rightarrow\\)De}에 대해 평균된다.\n' +
      '\n' +
      '도 6은 그 결과를 나타낸다. 다운스트림 작업을 전문으로 하는 동안 전체 제로 샷 번역 품질이 열등하지만 미세 조정은 여전히 밀접하게 관련된 작업에 대한 일반화를 이끌어내고 개선할 수 있다. 피네튜닝 혜택 일반화가 방법 및 작업 의존적인지 여부에 주목하십시오. 전반적으로 프롬프트 및 LoRA는 특히 기본 LLM이 클 때 FMT보다 상대적으로 더 나은 결과를 얻는데, 이는 대부분 LLM 매개변수가 동결되고 학습된 지식이 상속되기 때문이다. 이것은 또한 일반화 능력이 큰 관심사일 때 PET를 고려해야 함을 시사한다.\n' +
      '\n' +
      '##6 관련 업무\n' +
      '\n' +
      '모델 크기가 크게 증가함에 따라 모든 LLM 매개변수를 업데이트하는 것은 계산적으로 비효율적이고 감당할 수 없게 된다. 따라서 연구자들은 최소한의 조정 가능한 매개변수로 최상의 성능을 달성하는 것을 목표로 하는 매개변수 효율적인 조정 방법에 의존한다. 이러한 방향으로의 노력은 주로 LLM에 대한 효율적인 튜닝 가능한 모듈 개발에 초점을 맞추는데, 예를 들어, 작은 피드 포워드 레이어를 삽입하는 어댑터(Houlsby et al., 2019; Bapna et al., 2019), 입력에 튜닝 가능한 임베딩을 추가하는 프리픽스 및 프롬프트 튜닝(Li and Liang, 2021; Lester et al., 2021), 낮은 랭크 분해를 채택하는 LoRA 및 컴팩터(Hu et al., 2021; Mahabadi et al., 2021), 튜닝 가능한 바이어스 벡터를 추가하는 Bitfit(Zaken et al., 2021), 모델 활성화를 스케일링하는 IA3(Liu et al., 2022) 및 양자화를 활용하는 QLoRA(Dettmers et al., 2023) 등이 있다. 이전 연구에서는 다양한 도메인(He et al., 2022; Ding et al., 2022; Liu et al., 2022; Dettmers et al., 2023)에서 FMT에 도달하고 심지어 능가하는 것과 같은 PET를 사용한 고무적인 성능을 보고했지만, 이들은 주로 하나 또는 소수의 실험 설정에 초점을 맞추고 있으며, 스케일링이 다양한 미세 조정 방법의 성능에 어떻게 영향을 미치는지에 대한 질문을 남긴다.\n' +
      '\n' +
      '척도 법칙 최근 연구에 따르면 신경망 모델의 성능은 모델 및/또는 데이터 크기의 멱법칙에 의해 예측될 수 있다(Hestness et al., 2017; Kaplan et al., 2020). 이러한 패턴은 컴퓨터 비전(Zhai et al., 2021), 자기회귀 생성 모델링(Henighan et al., 2020), 신경 기계 번역(Gordon et al., 2021; Ghorbani et al., 2021; Bansal et al., 2022; Zhang et al., 2022a), 다국어 번역(Fernades et al., 2023), 다중-모달 모델링(Aghajanyan et al., 2023) 및 희소 신경 아키텍처(Frantar et al., 2023)와 같은 상이한 도메인 및 모델 아키텍처에 광범위하게 존재한다. 이러한 법률은 훈련 결정을 안내하는 귀중한 도구(Hoffmann et al., 2022)와 모델 성능이 규모로 어떻게 진화하는지를 이해함으로써 모델 개발을 제공하며, 이는 LLMs(OpenAI, 2023)의 개발을 크게 촉진한다. 불행히도 LLM 미세조정에 대한 스케일링 연구는 심하게 뒤처지고 있으며, 우리의 연구는 이 격차를 메운다.\n' +
      '\n' +
      '우리와 가장 밀접한 관련이 있는 작업은 (Hernandez et al., 2021)인데, 이는 피네튜닝을 처음부터 훈련과 비교하여 지식 전달을 위한 스케일링을 탐구했다. 우리의 주요 초점은 이전이 아닌 LLM 미세 조정을 위한 다양한 요인의 스케일링을 이해하는 것이기 때문에 우리의 연구는 상당한 차이로 그들의 연구와 직교한다.\n' +
      '\n' +
      '##7 결론 및 향후 과제\n' +
      '\n' +
      '본 논문에서는 LLM 모델 크기, 사전 훈련 데이터 크기, 미세 조정 데이터 크기, PET 파라미터 크기 및 다양한 미세 조정 방법을 포함한 다양한 요인을 고려하여 LLM 미세 조정에 대한 스케일링을 체계적으로 연구하였다. 일반성을 보장하기 위해 두 세트의 LLM, 세 가지 다른 다운스트림 작업(번역 및 요약), 세 가지 미세 조정 방법(FMT, 프롬프트 및 LoRA)을 수행했다. 우리는 미세 조정 데이터 크기와 서로 다른 스케일링 인자 사이의 스케일링 관계를 설명할 수 있는 곱셈 합동 스케일링 법칙을 제안했다. 광범위한 결과는 LLM 모델 크기가 증가하는 것이 사전 훈련 데이터 스케일링보다 미세 조정에 더 큰 영향을 미치며, 스케일링 PET 매개변수가 비효율적이라는 것을 보여준다. 또한, 피네튜닝 스케일링은 태스크 및 데이터 의존성이 높아 다운스트림 태스크에 대한 최상의 피네튜닝 방법의 선택이 덜 결정적이다.\n' +
      '\n' +
      '우리는 우리의 일이 몇 가지 한계를 겪고 있다는 것을 인정한다. 제안된 공동스케일링 법칙은 대부분 이론적인 접지가 없는 폐쇄형 발전 과제에 대한 경험적 결과에 기초한다. 다양한 미세 조정 시나리오로 일반화할 수 있는지 여부는 현재 컴퓨팅 예산을 초과하는 더 많은 실험을 필요로 한다. 또한, 일부 설정에서 프롬프트 및 LoRA에 대한 최적화 및 평가의 불완전성을 이해합니다. 향후, 본 연구를 멀티모달 LLM으로 확장하고, 데이터 품질 개선의 영향을 탐색하며, 오픈 및 크리에이티브 생성 작업뿐만 아니라 멀티 태스크 설정을 고려하고자 한다.\n' +
      '\n' +
      '## 8 Acknowledgements\n' +
      '\n' +
      '우리는 비평가들의 통찰력 있는 논평에 감사한다. 스케일링 법칙에 대한 귀중한 피드백을 제공한 야미니 밴살, 건설적인 논평으로 이 작업을 검토한 사비에르 가르시아, PET 최적화에 대한 유용한 토론을 한 프레데릭 류, 이 연구를 지원하는 콕 르, 아푸 샤 및 구글 번역 팀에 감사드린다.\n' +
      '\n' +
      '우리는 또한 본 논문에서 사용된 훈련 인프라를 구축하는 동료들에게 감사한다: Brian Lester, Rami Al-Rfou and Noah Constant for prompt tuning, Chu-Cheng Lin for LoRA, Xavier Garcia and TSX Team (Roberts et al., 2023) for the training framework.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Aghajanyan et al. (2023) Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. 생성 혼합 모달 언어 모델의 척도법 _ arXiv preprint arXiv:2301.03728_, 2023.\n' +
      '* Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 기술 보고서. _ arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* Baevski et al. (2020) Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. Wav2vec 2.0: 음성 표현의 자기 지도 학습을 위한 프레임워크. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS\'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546\n' +
      '*Bansal et al.(2022) Yamini Bansal, Behrooz Ghorbani, Ankush Garg, Biao Zhang, Colin Cherry, Behnam Neyshabur, and Orhan Firat. NMT의 데이터 스케일링 법칙: 잡음과 아키텍처의 영향. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato(eds.), _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 1466-1482. PMLR, 17-23 Jul 2022. URL[https://proceedings.mlr.press/v162/bansal22b.html](https://proceedings.mlr.press/v162/bansal22b.html).\n' +
      '* Bapna et al.(2019) Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. 신경 기계 번역을 위한 간단하고 확장 가능한 적응 ArXiv preprint arXiv:1909.08478_, 2019.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 소수의 학습자를 의미한다. _ 신경 정보 처리 시스템_, 33:1877-1901, 2020의 발전.\n' +
      '* Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _ ArXiv:2204.02311_, 2022.\n' +
      '* Dettmers et al.(2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: 양자화된 llms의 효율적인 미세조정 arXiv preprint arXiv:2305.14314_, 2023.\n' +
      '* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. In _Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers)_, pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL[https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).\n' +
      '* Ding et al. (2022) Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. 델타 튜닝: 2022년 사전 훈련된 언어 모델을 위한 파라미터 효율적인 방법에 대한 포괄적인 연구.\n' +
      '* Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 이미지는 16x16 단어의 가치가 있습니다: 스케일에서 이미지 인식을 위한 트랜스포머입니다. _International Conference on Learning Representations_, 2021. URL[https://openreview.net/forum?id=YicbFdNTTY](https://openreview.net/forum?id=YicbFdNTTY)에 있어서,\n' +
      '\n' +
      '* Fernandes et al. (2023) Patrick Fernandes, Behrooz Ghorbani, Xavier Garcia, Markus Freitag, and Orhan Firat. 다국어 신경 기계 번역을 위한 척도법. In Andreas Krause, Emma Brunskill, Kyungyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett(eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 10053-10071. PMLR, 23-29 Jul 2023. URL[https://proceedings.mlr.press/v202/fernandes23a.html](https://proceedings.mlr.press/v202/fernandes23a.html).\n' +
      '* Frantar et al. (2023) Elias Frantar, Carlos Riquelme, Neil Houlsby, Dan Alistarh, and Utku Evci. 희박하게 연결된 기반 모델에 대한 척도법, 2023.\n' +
      '* Fu et al. (2023) Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. ai 피드백으로부터 자기 재생 및 상황 내 학습을 통한 언어 모델 협상 개선. _ arXiv preprint arXiv:2305.10142_, 2023.\n' +
      '* Gao et al. (2023) Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameterefficient visual instruction model. _ arXiv preprint arXiv:2304.15010_, 2023.\n' +
      '* Garcia et al. (2023) Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Melvin Johnson, and Orhan Firat. 기계 번역을 위한 소수의 샷 학습의 불합리한 효과. In _International Conference on Machine Learning_, pp. 10867-10878. PMLR, 2023.\n' +
      '* Ghorbani et al. (2021) Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. 신경 기계 번역을 위한 법칙의 스케일링. _ CoRR_, abs/2109.07740, 2021. URL[https://arxiv.org/abs/2109.07740](https://arxiv.org/abs/2109.07740).\n' +
      '* Gong et al. (2023) Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, 및 Kai Chen. 멀티모달-gpt: 인간과 대화를 위한 비전과 언어 모델. _ arXiv preprint arXiv:2305.04790_, 2023.\n' +
      '* Gordon et al. (2021) Mitchell A Gordon, Kevin Duh, and Jared Kaplan. 신경 기계 번역을 위한 데이터 및 매개변수 스케일링 법칙. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 5915-5922, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.478. URL[https://aclanthology.org/2021.emnlp-main.478](https://aclanthology.org/2021.emnlp-main.478)\n' +
      '* He et al. (2022) Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 매개변수 효율적 전이학습의 통일된 관점을 살펴보면, 2022년이다.\n' +
      '* Henighan et al. (2020) Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive Generative Modeling. _ arXiv preprint arXiv:2010.14701_, 2020.\n' +
      '* Hermann et al. (2015) Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 읽고 이해하는 기계들을 가르치는 것. _NIPS_, pp. 1693-1701, 2015. URL[http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend](http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend].\n' +
      '* Hernandez et al. (2021) Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 전송을 위한 법률 확대. _ ArXiv:2102.01293_, 2021.\n' +
      '* Hestness et al. (2017) Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory F. Diamos, Heewoo Jun, Hassan Kianinejad, Md. 모스토파 알리 팻와리, 양양, 옌치 저우. 딥러닝 스케일링은 경험적으로 예측가능하다. _ CoRR_, abs/1712.00409, 2017. URL[http://arxiv.org/abs/1712.00409](http://arxiv.org/abs/1712.00409).\n' +
      '* Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al training compute-optimal large language models. _ arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '* Houlsby et al. (2019) Neil Houlsby, Andrei Giurgui, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. NLP를 위한 파라미터 효율적인 전이 학습. Kamalika Chaudhuri and Ruslan Salakhutdinov(eds.), _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pp. 2790-2799. PMLR, 09-15 Jun 2019. URL[https://proceedings.mlr.press/v97/houlsby19a.html](https://proceedings.mlr.press/v97/houlsby19a.html).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 인간의 피드백으로 지침을 따르도록 언어 모델을 훈련합니다. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and KyungHyun Cho(eds.), _Advances in Neural Information Processing Systems_, 2022. URL[https://openreview.net/forum?id=TG8KACxEON](https://openreview.net/forum?id=TG8KACxEON).\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 통합된 텍스트-텍스트 변환기를 이용한 전이학습의 한계를 탐색한다. 21(1), jan 2020. ISSN 1532-4435.\n' +
      '* Roberts et al. (2023) Adam Roberts, Hyung Won Chung, Gaurav Mishra, Anselm Levskaya, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. Scaling up models and data with 15x and seqio. _ Journal of Machine Learning Research_, 24(377):1-8, 2023.\n' +
      '* Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: 176b-parameter open-access multilingual language model. _ ARXiv 프리프린트 arXiv:2211.05100_, 2022.\n' +
      '* Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 도구 형성기: 언어 모델은 스스로 도구를 사용하는 법을 배울 수 있습니다. _ arXiv preprint arXiv:2302.04761_, 2023.\n' +
      '* Scialom et al. (2020) Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. MLSUM: 다국어 요약 코퍼스. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 8051-8067, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.647. URL[https://aclanthology.org/2020.emnlp-main.647](https://aclanthology.org/2020.emnlp-main.647)\n' +
      '* Sellam et al.(2020) Thibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: 텍스트 생성을 위한 강건 메트릭 학습. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 7881-7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL[https://aclanthology.org/2020.acl-main.704](https://aclanthology.org/2020.acl-main.704).\n' +
      '* Shazeer & Stern (2018) Noam Shazeer and Mitchell Stern. 보조인자: 하위 선형 메모리 비용을 갖는 적응형 학습 속도. In _International Conference on Machine Learning_, pp. 4596-4604. PMLR, 2018.\n' +
      '* Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: chatgpt 및 그 친구들과 함께 ai 태스크를 포옹 얼굴에서 해결하는 단계; _ arXiv preprint arXiv:2303.17580_, 2023.\n' +
      '* Tay et al. (2022) Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. Ul2: Unifying language learning paradigms. _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting reasoning in large language models. _ 신경 정보 처리 시스템_, 35:24824-24837, 2022에서의 발전.\n' +
      '* Yang et al. (2023) Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. 빅트랜스: 100개 이상의 언어에 대한 다국어 번역 능력을 갖춘 대형 언어 모델을 증강하는 것. _ arXiv preprint arXiv:2305.18098_, 2023.\n' +
      '* Zaken et al. (2021) Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 비트핏: 트랜스포머 기반 마스킹 언어 모델을 위한 간단한 파라미터 효율적인 미세 조정 CoRR_, abs/2106.10199, 2021. URL[https://arxiv.org/abs/2106.10199](https://arxiv.org/abs/2106.10199).\n' +
      '\n' +
      '* Zhai et al. (2021) Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 비전 트랜스포머를 확장하고 있어요 CoRR_, abs/2106.04560, 2021. URL[https://arxiv.org/abs/2106.04560](https://arxiv.org/abs/2106.04560).\n' +
      '* Zhang et al. (2022a) Biao Zhang, Behrooz Ghorbani, Ankur Bapna, Yong Cheng, Xavier Garcia, Jonathan Shen, and Orhan Firat. 기계 번역을 위한 언어 모델 아키텍처의 스케일링 및 전송을 검토하고 있다. _ CoRR_, abs/2202.00528, 2022a. URL[https://arxiv.org/abs/2202.00528](https://arxiv.org/abs/2202.00528)\n' +
      '* Zhang et al. (2023) Biao Zhang, Barry Haddow, and Alexandra Birch. 기계 번역을 위한 대형 언어 모델 요청: 사례 연구. Andreas Krause, Emma Brunskill, Kyungyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett(eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 41092-41110. PMLR, 23-29 Jul 2023. URL[https://proceedings.mlr.press/v202/zhang23m.html](https://proceedings.mlr.press/v202/zhang23m.html).\n' +
      '* Zhang et al. (2022b) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _ arXiv preprint arXiv:2205.01068_, 2022b.\n' +
      '* Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Lessly is more for alignment. _ arXiv preprint arXiv:2305.11206_, 2023.\n' +
      '\n' +
      '## 부록, 부록\n' +
      '\n' +
      'LLM finetuning을 위한 최적화.최적화를 위해, finetuning 데이터에 대해 주어진 사전 훈련된 체크포인트로부터 표준 조건부 로그 우도 손실로 사전 훈련을 계속한다. 보다 구체적으로, 각 미세 조정 예에 대해 _input_ 및 _target_를 단일 시퀀스로 연결하고 _target_ 단독의 로그 우도를 계산한다. 보조인자 및 코사인 학습 속도 스케줄이 재사용됩니다. 참고 En-De 및 En-Zh LLM은 각각 135K 및 98K 단계에 대해 사전 훈련된다. 모든 LLM은 최대 200K 단계(300K 단계인 WMT En-Zh(FMT)를 제외하고) 또는 100시대에 대해 추가로 미세 조정된다. 최상의 성능을 얻기 위해 그리드 검색을 통해 1B LLM 기반 다양한 미세 조정 방법에 대한 초기 학습 속도와 배치 크기를 최적화한다. 마지막으로, Prompt, LoRA 및 FMT의 학습률을 각각 \\(3e^{-1},1e^{-2}\\) 및 \\(1e^{-3}\\)으로 설정하고, PET 및 FMT의 배치 크기를 각각 16 및 128로 설정하였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline LLM Model Size & \\#Layers & \\#Heads & Head Dim & FFN Dim & Model Dim \\\\ \\hline\n' +
      '1B & 16 & 8 & 256 & 8192 & 2048\\\\\n' +
      '2B & 20 & 10 & 256 & 10240 & 2560\\\\\n' +
      '4B & 24 & 12 & 256 & 12288 & 3072\\\\\n' +
      '8B & 32 & 16 & 256 & 16384 & 4096\\\\\n' +
      '16B & 40 & 20 & 256 & 20480 & 5120 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 상이한 크기의 LLM에 대한 하이퍼파라미터. "B": billion; "#Layers, #Heads": 각각 레이어 및 어텐션 헤드의 수; "Head Dim, FFN Dim, Model Dim": 각각 어텐션 헤드, 피드-포워드 레이어 및 히든 표현에 대한 차원.\n' +
      '\n' +
      '도 7: WMT14 En-De, WMT19 En-Zh 및 MLSum 상의 **LLM 모델 크기 및 미세조정 데이터 크기**를 스케일링하기 위한 생성 품질(BLEURT/RougeL). 전반적으로 BLEURT/RougeL은 몇 가지 예외를 제외하고 PPL과 양의 상관관계가 있다.\n' +
      '\n' +
      '임계 미세조정 데이터 크기\\(D_{f}^{c}\\)를 분석하는 한편, Eq. (1) 우리는 직접 계산(D_{f}^{c}\\)을 하는 것을 방해하지만, 여전히 그들의 성능 갭이 상수일 때 두 미세조정 방법 사이의 이론적인 분석이 가능하다:\n' +
      '\n' +
      '[\\hat{\\mathcal{L}}_{1}-\\hat{\\mathcal{L}}_{2}=E_{1}-E_{2}\\quad\\longrightarrow\\quad\\hat{D_{f}^{c}}=H*X^{\\gamma},\\quad H=\\left(\\nicefrac{{A_{1}}}{A_{2}}}}{{p_{1}-\\gamma_{2}},\\gamma=\\frac{\\alpha_{2}-\\beta_{1}}{\\beta_{3}\\tag{3}}}}\\tamma}},\\gamma=\\frac{\\alpha_{2}-\\beta_{2}}}{\\beta_{2}}}{\\beta_{2}}}{\\beta_{2}}}{\\beta_{2}}}{\\beta_{2}}{\\beta_{2}}}{\\beta_{\n' +
      '\n' +
      '다른 권력 법칙을 따르는 것. 직관적으로 지수\\(\\gamma\\)는 두 방법의 다운스트림 작업으로의 전달 가능성 차이를 스케일링 인자\\(X\\)로 포착한다. 의 계수를 요약합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Params} & \\multicolumn{2}{c}{WMT14 En-De} & \\multicolumn{4}{c}{WMT19 En-Zh} & \\multicolumn{4}{c}{MLSum} \\\\ \\cline{2-10}  & FMT & Prompt & LoRA & FMT & Prompt & LoRA & FMT & Prompt & LoRA \\\\ \\hline Scaling for LLM model size & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) \\\\ \\(A_{m}\\) & \\(1.2\\times 10^{5}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) & \\(\\nicefrac{{2}}{{2}}\\) \\\\ \\(\\alpha_{m}\\) & \\(0.52\\) & \\(0.4\\) & \\(0.36\\) & \\(0.34\\) & \\(0.33\\) & \\(0.31\\) & \\(0.24\\) & \\(0.1\\) & \\(0.11\\) \\\\ \\hline Scaling for pretraining data size and finetuning data size & & & & & & & & \\\\ \\(A_{p}\\) & \\(6.3\\times 10^{2}\\) & \\(2.7\\times 10^{2}\\) & \\(1.4\\times 10^{2}\\) & \\(2.4\\times 10^{2}\\) & \\(2\\times 10^{2}\\) & \\(1.3\\times 10^{2}\\) & \\(42\\) & \\(16\\) & \\(17\\) \\\\ \\(\\alpha_{p}\\) & \\(0.21\\) & \\(0.21\\) & \\(0.18\\) & \\(0.17\\) & \\(0.2\\) & \\(0.18\\) & \\(0.11\\) & \\(0.069\\) & \\(0.073\\) \\\\ \\hline Scaling for PET parameter size and finetuning data size & & & & & & & & \\\\ \\(A_{t}\\) & - & \\(1\\) & \\(1.4\\) & - & \\(1\\) & \\(1.2\\) & - & \\(2.6\\) & \\(2.4\\) \\\\ \\(\\alpha_{t}\\) & - & \\(0.0027\\) & \\(-0.0017\\) & - & \\(0.0019\\) & \\(0.0044\\) & - & \\(0.0026\\) & \\(0.000\\,22\\) \\\\ \\hline \\(E\\) & \\(0.75\\) & \\(0.62\\) & \\(0.62\\) & \\(1\\) & \\(0.77\\) & \\(0.73\\) & \\(0.98\\) & \\(0.000\\,51\\) & \\(0.2\\) \\\\ \\(\\beta\\) & \\(0.15\\) & \\(0.051\\) & \\(0.081\\) & \\(0.14\\) & \\(0.015\\) & \\(0.025\\) & \\(0.087\\) & \\(0.025\\) & \\(0.03\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 상이한 설정에 대한 피팅된 스케일링 파라미터.\n' +
      '\n' +
      '도 8: 스케일링 ** 전처리 데이터 크기**를 위한 생성 품질(BLEURT/RougeL) 및 WMT14 En-De, WMT19 En-Zh 및 MLSum 상의 데이터 크기를 미세조정한다.\n' +
      '\n' +
      '표 5의 다른 작업에서 값이 작업에 비해 크게 다르고 설정에 따라 명확한 패턴이 없습니다.\n' +
      '\n' +
      '파인튜닝이 베이스 LLM의 소수 샷 능력에 어떤 영향을 미치는가? 제로 샷 번역 외에도, 우리는 또한 파인튜닝 후 LLM의 소수 샷 능력을 탐구한다. 퓨샷 생성하지 않음\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Params} & \\multicolumn{3}{c}{WMT14 En-De} & \\multicolumn{3}{c}{WMT19 En-Zh} & \\multicolumn{3}{c}{MLSum} \\\\ \\cline{2-10}  & F vs. P & F vs. L & P vs. L & F vs. P & F vs. L & P vs. L & F vs. P & F vs. L & P vs. L \\\\ \\hline Scaling LLM model size and finetuning data size & & & & & & & & & \\\\ \\(H\\) & \\(3.7\\times 10^{14}\\) & \\(2\\times 10^{14}\\) & \\(1.6\\times 10^{-9}\\) & \\(6.1\\times 10^{4}\\) & \\(1.6\\times 10^{6}\\) & \\(1.8\\times 10^{-11}\\) & \\(3.6\\times 10^{18}\\) & \\(1.2\\times 10^{17}\\) & \\(0.000\\) 45 \\\\ \\(\\gamma\\) & \\(-1.2\\) & \\(-2.4\\) & \\(1.5\\) & \\(-0.12\\) & \\(-0.3\\) & \\(1.9\\) & \\(-2.1\\) & \\(-1.8\\) & \\(2.3\\) \\\\ \\hline Scaling pretraining data size and finetuning data size & & & & & & & & & \\\\ \\(H\\) & \\(3.7\\times 10^{3}\\) & \\(6.9\\times 10^{8}\\) & \\(7.7\\times 10^{-10}\\) & \\(5\\) & \\(2.7\\times 10^{2}\\) & \\(8.6\\times 10^{-19}\\) & \\(3.7\\times 10^{6}\\) & \\(1\\times 10^{7}\\) & \\(1.6\\times 10^{2}\\) \\\\ \\(\\gamma\\) & \\(-0.0015\\) & \\(-0.5\\) & \\(1.2\\) & \\(0.26\\) & \\(0.093\\) & \\(2.1\\) & \\(-0.63\\) & \\(-0.63\\) & \\(-0.63\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: Eq.의 계수. (3) 설정과 다른 방법을 비교하여. “F/P/L”: FMT/Prompt/LoRA.\n' +
      '\n' +
      '그림 10: En-De 및 En-Zh LLM 사전 훈련에 대한 적합 단일 변수 스케일링 법칙. 우리는 보류된 검증 세트에 대해 모델을 평가하고 PPL에 기반한 스케일링 법칙에 맞는다. En-Zh LLM에 대한 스케일링 법칙은 실제 성능이 예상보다 좋지 않은 경우 16B로 잘 외삽되지 않는다(이는 사전 훈련 불안정으로 인해 발생할 수 있다). 이러한 불일치는 그림 2와 같이 미세 조정 후 증폭된다.\n' +
      '\n' +
      '도 9: WMT14 En-De, WMT19 En-Zh 및 MLSum 상의 **PET 파라미터 크기 및 미세조정 데이터 크기**를 스케일링하기 위한 생성 품질(BLEURT/RougeL).\n' +
      '\n' +
      'LLM의 기능을 검사하는 방법만 제공하지만 도메인에서 모델을 적용할 수 있는 효과적인 방법을 제공하기 때문에 다운스트림 응용 프로그램에도 관심이 있습니다. 그림 11, 12, 13 및 14는 소수 샷 생성에 대한 미세조정의 영향을 보여준다.\n' +
      '\n' +
      '우리는 FMT가 대부분의 경우 LLM의 소수 샷 성능을 퇴화시키는데, 여기서 더 많은 미세 조정 데이터를 추가하면 종종 소수 샷 성능이 감소한다는 점에 주목한다. 대조적으로, PET는 모델 크기 및 사전 훈련 데이터 크기에 관계없이 LLM의 대부분의 소수의 샷 능력을 유지하는 더 강건하게 행동한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c c c} \\hline \\hline  & Scaling Factor & \\multicolumn{4}{c}{Multiplicative} & \\multicolumn{4}{c}{Additive} \\\\ \\cline{3-10}  & & FMT & Prompt & LoRA & Avg & FMT & Prompt & LoRA & Avg \\\\ \\hline \\multirow{3}{*}{WMT En-De} & LLM Model Size & \\(0.0052\\) & \\(0.0043\\) & \\(0.0047\\) & **0.0048** & \\(0.012\\) & \\(0.0076\\) & \\(0.0045\\) & \\(0.0079\\) \\\\  & Pretraining Data Size & \\(0.0057\\) & \\(0.0061\\) & \\(0.0084\\) & **0.0068** & \\(0.0048\\) & \\(0.0075\\) & \\(0.0082\\) & \\(0.0069\\) \\\\  & PET parameter size & - & \\(0.005\\) & \\(0.0031\\) & **0.004** & - & \\(0.0069\\) & \\(0.0032\\) & \\(0.005\\) \\\\ \\hline \\multirow{3}{*}{WMT En-Zh} & LLM Model Size & \\(0.0075\\) & \\(0.019\\) & \\(0.026\\) & **0.018** & \\(0.021\\) & \\(0.018\\) & \\(0.029\\) & \\(0.022\\) \\\\  & Pretraining Data Size & \\(0.002\\) & \\(0.0071\\) & \\(0.0056\\) & **0.0049** & \\(0.0026\\) & \\(0.0069\\) & \\(0.0058\\) & \\(0.0051\\) \\\\  & PET parameter size & - & \\(0.0075\\) & \\(0.0051\\) & \\(0.0063\\) & - & \\(0.0076\\) & \\(0.0044\\) & **0.006** \\\\ \\hline \\multirow{3}{*}{MLSum} & LLM Model Size & \\(0.0066\\) & \\(0.013\\) & \\(0.022\\) & \\(0.014\\) & \\(0.0072\\) & \\(0.015\\) & \\(0.017\\) & **0.013** \\\\  & Pretraining Data Size & \\(0.009\\) & \\(0.0083\\) & \\(0.0039\\) & \\(0.007\\) & \\(0.0062\\) & \\(0.0046\\) & \\(0.0043\\) & **0.005** \\\\  & PET parameter size & - & \\(0.0081\\) & \\(0.003\\) & \\(0.0055\\) & - & \\(0.0053\\) & \\(0.0027\\) & **0.004** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 상이한 작업들에 걸친 가산 및 승법 스케일링 제형에 대한 헬드 아웃 피팅 오차들(\\(\\downarrow\\))이다. 전반적으로 곱셈적 스케일링 법칙은 더 잘 일반화된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c} \\hline \\hline  & Scaling Factor & FMT & Prompt & LoRA \\\\ \\hline \\multirow{3}{*}{WMT En-De} & LLM Model Size & -0.184 & -0.986\\({}^{\\dagger}\\) & -0.988\\({}^{\\ddagger}\\) \\\\  & Pretraining Data Size & -0.792\\({}^{\\dagger}\\) & -0.967\\({}^{\\dagger}\\) & -0.980\\({}^{\\ddagger}\\) \\\\  & PET parameter size & - & -0.841\\({}^{\\ddagger}\\) & -0.975\\({}^{\\ddagger}\\) \\\\ \\hline \\multirow{3}{*}{WMT En-Zh} & LLM Model Size & -0.984\\({}^{\\ddagger}\\) & -0.994\\({}^{\\dagger}\\) & -0.995\\({}^{\\ddagger}\\) \\\\  & Pretraining Data Size & -0.994\\({}^{\\dagger}\\) & -0.979\\({}^{\\ddagger}\\) & -0.978\\({}^{\\ddagger}\\) \\\\  & PET parameter size & - & -0.643\\({}^{\\ddagger}\\) & -0.968\\({}^{\\ddagger}\\) \\\\ \\hline \\multirow{3}{*}{MLSum} & LLM Model Size & -0.965\\({}^{\\dagger}\\) & -0.909\\({}^{\\ddagger}\\) & -0.890\\({}^{\\ddagger}\\) \\\\  & Pretraining Data Size & -0.941\\({}^{\\ddagger}\\) & -0.833\\({}^{\\ddagger}\\) & -0.838\\({}^{\\ddagger}\\) \\\\  & PET parameter size & - & -0.924\\({}^{\\dagger}\\) & -0.986\\({}^{\\ddagger}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 상이한 미세 조정 방법 및 설정에 대한 PPL과 BLEURT/RougeL 사이의 피어슨 상관 관계. “\\({}^{\\ddagger}\\)”: 상관은 \\(p<0.01\\)에서 유의하다. 더 낮은 PPL과 더 높은 BLEURT/RougeL은 더 나은 품질을 나타내므로 상관 값은 음수이다. 일반적으로 PPL과 BLEURT/RougeL은 상관관계가 높다.\n' +
      '\n' +
      '도 11: LLM 모델 크기에 대한 원샷 성능(BLEURT/RougeL) 및 WMT14 En-De, WMT19 En-Zh 및 MLSum에 대한 미세 조정 데이터 크기 스케일링. ‘기준선’ : 미세 조정 없이 수행되는 것.\n' +
      '\n' +
      '도 12: LLM 모델 크기에 대한 5-shot 성능(BLEURT/RougeL) 및 WMT14 En-De 및 WMT19 En-Zh에 대한 미세조정 데이터 크기 스케일링.\n' +
      '\n' +
      '도 14: WMT14 En-De 및 WMT19 En-Zh에 대한 사전 훈련 및 미세 조정 데이터 크기 스케일링을 위한 5-샷 성능(BLEURT/RougeL).\n' +
      '\n' +
      '도 13: WMT14 En-De, WMT19 En-Zh 및 MLSum에 대한 사전 훈련 및 미세 조정 데이터 크기 스케일링을 위한 원샷 성능(BLEURT/RougeL). ‘기준선’ : 미세 조정 없이 수행되는 것.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>