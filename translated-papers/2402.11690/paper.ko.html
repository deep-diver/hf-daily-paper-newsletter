<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Vision-Flan: Visual Instruction Tuning에서 Human-Labeled Task의 스케일링\n' +
      '\n' +
      ' Zhiyang Xu\\({}^{\\spadesuit}\\) Chao Feng\\({}^{\\spadesuit}\\) Rulin Shao\\({}^{\\heartsuit}\\) Trevor Ashby\\({}^{\\spadesuit}\\) Ying Shen\\({}^{\\spadesuit}\\)\n' +
      '\n' +
      'Di Jin\\({}^{\\diamond}\\) Yu Cheng\\({}^{\\spadesuit}\\) Qifan Wang\\({}^{\\diamond}\\) Liftu Huang\\({}^{\\spadesuit}\\)\n' +
      '\n' +
      '워싱턴대학교 버지니아텍대학교\n' +
      '\n' +
      '({}^{\\diamond}\\)Amazon Inc. \\\\({^{\\diamond}\\)Amazon Inc. ({}^{\\spadesuit}\\)Microsoft \\({}^{\\diamond}\\)Meta AI\n' +
      '\n' +
      '{zhiyangx,lifuh}@vt.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '비젼-언어 모델(VLMs)의 뛰어난 성능에도 불구하고, 기존의 VLM 프레임워크 내에서 (1) GPT-4 합성 데이터가 VLMs의 성능을 실질적으로 향상시키는 것이 아니라, (2) GPT-4 합성 데이터의 _annotation error_와 _bias_를 효과적으로 조절할 수 있다는 것을 발견한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 비전-언어 모델(VLMs) Liu 등(2023); Li 등(2023); Dai 등(2023); 사전 훈련된 대형-언어 모델(LLMs) Chiang 등(2023); Gao 등(2023) 및 사전 훈련된 이미지 인코더 Sun 등(2023)은 일반적인 시각적 어시스턴트로서 인상적인 능력을 보여주었다. 유니모달 인코더 외에도, 이러한 VLM 프레임워크의 주요 성분들은 (1) LLaVA 모델 Liu et al. (2023); 미리 훈련된 이미지 인코더와 LLMs 사이의 연결을 확립하는 Li et al. (2023), (2) 브리징 모듈을 사전 훈련하기 위해 사용되는 대규모 텍스트-이미지 쌍 슈만 et al. (2022), 및 (3) GPT-4 합성 시각적 명령어 튜닝 데이터세트 Liu et al. (2023); Li et al. (2023)은 VLM의 응답을 인간 선호도와 정렬(즉, 상세하고 유용한 응답을 생성하기 위한 사용자의 지시에 따르기)하기 위해 VLM의 응답을 인간 선호도와 정렬한다. 그들의 주목할만한 성공에도 불구하고, 우리는 추가 조사가 필요한 두 가지 남은 과제를 식별한다.\n' +
      '\n' +
      '첫째, 사전 훈련 단계에서 사용되는 데이터는 다양성이 결여된 이미지 캡션 작업에 의해 지배되어 VLMs Chen et al.(2023); Zhang et al.(2023)의 일반화 가능성이 제한된다. 예를 들어, LLaVA 모델 Liu et al.(2023)은 Zhang et al.(2023)을 사전 트레이닝하는 동안 텍스트 검출과 관련된 인스턴스들의 부재로 인해 광학 문자 인식(OCR) 태스크를 제대로 수행하지 못한다. 최근 몇몇 연구들은 Zhang et al. (2023); Hu et al. (2023); Liu et al. (2023)과 같은 시각적 질문 응답 및 OCR과 같은 명령어 튜닝 데이터 세트에 대해 VLM을 추가로 미세 조정함으로써 이 문제를 해결하지만, 태스크들의 커버리지는 여전히 제한적이다.\n' +
      '\n' +
      '둘째, 대부분의 기존의 시각적 명령 튜닝 데이터세트 Liu et al.(2023); Li et al.(2023); Yin et al.(2023)은 시각적 대화, 컴플렉스 VQA 및 디테일 캡션과 같은 새로운 태스크를 생성하기 위해 기존의 컴퓨터-비전 데이터세트로부터 캡션 또는 밀집 캡션과 같은 텍스트 주석을 용도 변경함으로써 GPT-4를 통해 합성적으로 생성된다. VLM이 인간의 선호도에 맞춰 유창하고 상세한 응답을 생성할 수 있도록 하지만, 태스크 다양성, 객체들 간의 거짓된 동시 발생 패턴 및 긴 형태의 출력의 부족은 심각한 환각 Liu et al.(2023); Liet al., 2023; Liu et al., 2023; Zhou et al., 2023) 및 치명적인 망각 - VLM은 그들의 비전 인코더들의 제로-샷 성능에 비해 MNIST(LeCun, 1998) 및 CIFAR-10(Krizhevsky et al., 2009)과 같은 기본 검출 태스크들에 대해 유사한 분류 성능을 유지하지 못한다(Zhai et al., 2023).\n' +
      '\n' +
      '두 가지 문제를 해결하기 위해, 우리는 학술 데이터 세트에서 추출한 187개의 작업, 객체 탐지 및 OCR과 같은 _perception_ 작업, 이미지 품질 분류 및 이미지 스타일 분류와 같은 _domain-specific_ 작업, 그래프 이해 및 기하학적 질문 응답과 같은 _complex reasoning_ 작업 등으로 구성된 가장 다양한 공개 가능한 시각적 명령어 튜닝 데이터 세트인 Vision-Flan을 소개한다. 비전 플란의 각 작업에는 전문가가 작성한 지침이 수반됩니다. 우리는 그림 1의 Vision-Flan의 일부 샘플 작업을 보여주고 부록 J의 전체 작업 목록을 제공한다.\n' +
      '\n' +
      '또한 2단계 명령어 튜닝 프레임워크를 소개한다. 첫 번째 단계에서는 미리 학습된 LLaVA 모델(Liu et al., 2023)을 초기 모델로 활용하고, 이를 Vision-Flan 상에서 미세조정하여 다양한 역량을 확보하여 Vision-Flan Base 모델을 생성한다. 그러나, 학술 데이터 세트에서 타겟 출력의 간결성 때문에, Vision-Flan Base에 의해 생성된 응답은 간단하고 인간의 선호도와 정렬되지 않는 경향이 있다. 따라서 두 번째 단계에서는 최소량의 GPT-4 합성 데이터를 사용하여 Vision-Flan Base를 더 세분화한다. 이 단계는 모델의 출력을 인간의 선호도에 더 부합하도록 조정하여 Vision-Flan Chat 모델을 생성하는 것을 목표로 한다.\n' +
      '\n' +
      '실험 결과는 Vision-Flan의 고품질 인간 주석이 Vision-Flan 베이스와 Vision-Flan 채팅의 기능을 크게 향상시키면서 환각과 치명적인 망각의 위험을 감소시킨다는 것을 보여준다. 2단계 명령어 튜닝 프레임워크는 Vision-Flan Chat이 최첨단 VLM에 비해 훨씬 적은 GPT-4 합성 데이터를 사용하여 더 나은 인간 선호도 정렬을 달성할 수 있도록 한다. 마지막으로, 인간 표지 및 GPT-4 합성 데이터의 역할과 다양한 훈련 전략의 영향을 포함한 시각적 지시 튜닝을 이해하기 위해 광범위한 분석을 수행한다. 우리의 조사는 몇 가지 핵심 통찰력을 제공합니다.\n' +
      '\n' +
      '* 시각적 명령어 튜닝에서 인간-표지된 태스크의 수를 증가시키는 것은 광범위한 평가 벤치마크에 걸쳐 VLM의 능력을 실질적으로 향상시킬 수 있다.\n' +
      '* GPT-4 합성 데이터는 VLMs 능력을 실질적으로 향상시키지 않으며 MME(Fu et al., 2023) 및\n' +
      '\n' +
      '그림 1: Vision-Flan의 샘플 작업. **인스트럭션**은 주석자에 의해 조작된 태스크 인스트럭션을 나타낸다. **입력**은 주어진 태스크에서 텍스트 입력을 의미하며, **Target**은 명령어를 기반으로 하는 타겟 응답이다.\n' +
      '\n' +
      ' MM-Bench Liu et al.(2023)\n' +
      '* GPT-4 합성 데이터의 최소량(1,000)은 VLM의 반응을 인간의 선호도와 정렬하기에 충분하다. 특히, GPT-4 합성 데이터를 증가시키는 것은 정렬의 비례적 향상에 해당하지 않으며 환각과 바이어스를 VLM에 도입한다.\n' +
      '* 시각적 명령어 튜닝은 주로 시각적 특징을 처리하고 이해하는 대규모 언어 모델(LLM)의 능력을 향상시킨다. LLM의 임베딩 공간에 시각적 특징을 매핑하는 브리징 모듈의 트레이닝은 주로 사전 트레이닝 단계 동안 달성된다.\n' +
      '\n' +
      '## 2 Vision-Flan\n' +
      '\n' +
      '### Collection Pipeline\n' +
      '\n' +
      '우리는 훈련과 테스트의 두 번의 반복을 포함하는 자격을 갖춘 주석자를 식별하기 위해 주석자 선택 프로세스를 신중하게 설계한다. 선택 과정과 보상에 대한 자세한 내용은 부록 A.1에서 확인할 수 있으며, 결국 21명의 후보자 중 7명을 주석자로 채용하고 모두 컴퓨터 과학 대학원생이다. 비전 플란에서 작업의 다양성과 품질을 보장하기 위해 4가지 주요 단계로 엄격한 주석 파이프라인을 설계한다.\n' +
      '\n' +
      '**기존 데이터셋 수집 및 전처리:** 두 명의 전문 연구자(즉, 자연어 처리 및 컴퓨터 비전 분야의 수석 박사과정 학생)가 온라인 검색을 통해 양질의 비전 언어 데이터셋을 파악한다. 그런 다음 데이터 세트는 7개의 주석자에게 균등하게 분배되어 데이터 세트를 다운로드하고 전처리한다. 각각의 처리된 인스턴스는 이미지, 명령어(마이너 수정을 갖는 원래의 데이터세트로부터의 태스크 정의), 적용 가능한 경우 텍스트 입력, 및 타겟 출력으로 구성된다.\n' +
      '\n' +
      '새로운 작업을 만드는 것: 두 명의 전문가 연구자와 주석자는 또한 기존의 주석에서 파생될 수 있는 잠재적인 새로운 작업에 대해 논의한다. 우리는 데이터셋에 존재하는 두 개 이상의 태스크의 주석을 결합하여 새로운 태스크를 도출한다. 예를 들어, Concadia dataset Kreiss et al.(2022)에서, 각각의 인스턴스는 이미지 캡션 및 이미지와 관련된 지식 스니펫으로 구성된다. 본 논문에서는 자유 형태 생성 태스크인 이미지에 주어진 캡션과 배경 지식을 모두 예측하는 새로운 태스크를 제안한다. 새로운 타겟 출력은 캡션을 지식 스니펫과 연결함으로써 형성된다. 또한 원본 작업의 보다 기본적인 버전을 만들어 새로운 작업을 개발합니다. 예를 들어, MSCOCO Lin et al.(2014)에서 객체 검출 주석이 주어졌을 때, 우리는 객체의 목록을 제공하고 모델에 이미지에 나타나는 객체를 선택하도록 요청하는 객체 선택 태스크를 제안한다(부정적인 옵션은 다른 이미지에는 나타나지만 주어진 이미지에는 나타나지 않는 객체를 샘플링함으로써 생성됨). 전문가 연구자와 주석자는 새로 개발된 과제마다 20개의 인스턴스를 수작업으로 해결한다. 인간 예측이 목표 출력과 일치하면, 이 새로운 작업이 유효한 것으로 간주된다.\n' +
      '\n' +
      '**작업 명령어와 출력 템플릿을 반복적으로 정제합니다.** 기존 작업의 경우 주석자에게 사소한 수정으로 원래 작업 정의에 따라 명령어를 작성하도록 요청합니다. 새로 개발된 과제에 대해서는 주석이 전문가 연구자와 협의하여 지시사항을 작성한다. 주석자가 새로운 명령어 작성을 마치면, 두 명의 전문가 연구자 중 한 명이 무작위로 할당되어 인스턴스를 조사하고 명령어 수정을 위한 피드백을 제공한다. 이 단계는 명령이 우리의 요구 사항을 충족할 때까지 반복적으로 반복된다. 우리는 명령어가 _clear_, _easy to understand_, 그리고 human_에 의해 올바르게 실행될 수 있어야 한다. 각 태스크는 연관된 데이터세트 및 명령어와 함께 Vision-Flan에 대한 후보 태스크 풀에 추가된다.\n' +
      '\n' +
      '**각 태스크의 품질을 검증하는 단계:** 후보 태스크 풀로부터 원어민 영어 화자를 포함한 두 명의 전문가 연구자가 함께 협력하여 지시가 유창하고 의도한 태스크를 효과적으로 전달하고 태스크가 다른 태스크와 중복되지 않는 고품질 태스크를 선택한다.\n' +
      '\n' +
      '이 네 가지 단계를 기반으로 최종적으로 187개의 고품질 작업을 수집하고 각 작업에 대해 해당 데이터 세트에서 10,000개의 인스턴스를 무작위로 샘플링한다. 데이터 집합에 10,000개 미만의 인스턴스가 포함된 경우 모든 인스턴스를 포함합니다. 데이터 세트는 총 187개의 작업에 대해 1,664,261개의 인스턴스로 구성된 Vision-Flan으로 명명한다. 부록 H의 Vision-Flan에 사용된 모든 데이터 세트에 대한 참조를 포함하고 부록 J의 각 작업에 대한 인스턴스를 보여준다.\n' +
      '\n' +
      '기존 데이터 세트의### 비교\n' +
      '\n' +
      '표 1은 기존의 시각 명령어 튜닝 데이터셋과 Vision-Flan의 비교를 제시하고 있다. 기존 시각 명령어 튜닝 데이터셋의 경우,\n' +
      '\n' +
      '원래 논문에서 보고된 작업 및 인스턴스의 수를 직접 채택합니다. 이러한 데이터 세트의 대부분은 ChatGPT1 및 GPT-42와 같은 독점 언어 모델을 사용하여 생성되며 좁은 범위의 작업 다양성을 나타낸다. VL-Qwen Bai 등(2023)은 최근 인간이 주석을 달았지만 대중이 접근할 수 없는 대규모 데이터세트이다. Multi-Instruct Xu et al.(2023)은 공개된 데이터 셋을 기반으로 하지만 주로 시각적 접지 작업에 중점을 두고 있으며 지역별 정보를 포함하지 않는 29개의 작업만 포함하고 있다. 대조적으로, Vision-Flan은 멀티인스트럭트의 태스크 수에 비해 3배 증가를 제공하는 훨씬 더 다양한 태스크 배열을 포함한다.\n' +
      '\n' +
      '각주 1: [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)\n' +
      '\n' +
      '각주 2: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)\n' +
      '\n' +
      '그림 2에서 우리는 Vision-Flan과 다른 데이터 세트로 다루는 작업 범주를 비교한다. Vision-Flan 내의 태스크는 먼저 _Question Answering_, _Classification_ 및 _Generation_의 세 가지 기본 그룹으로 분류되며, 이러한 기본 그룹 각각은 더 세분화된 특정 범주로 나뉜다. 예를 들어, _Classification_ 그룹 내에서, _General Object_ 카테고리는 이미지들 내의 오브젝트들을 "fish", "car", 및 "dog"와 같은 다양한 개념들로 분류하는 것을 포함한다. 대조적으로, _Vehicle Model_ 카테고리는 모델들에게 "도요타" 및 "캠리"와 같은 특정 자동차 브랜드 또는 모델을 정확하게 식별할 것을 요구한다. 그림 2의 시각화는 기존 데이터 세트에 비해 Vision-Flan에서 작업의 우수한 다양성과 볼륨을 명확하게 보여준다. 부록 I의 각 범주에 있는 작업을 나열합니다.\n' +
      '\n' +
      '## 3 Vision-Flan Finetuning\n' +
      '\n' +
      'Model ArchitectureWe adopt the same VLM architecture with LLaVA Liu et al.(2023) and indicate it as LLaVA-Architecture. 그림 3과 같이 사전 훈련된 비전 인코더, 사전 훈련된 대형 언어 모델, 그리고 이들을 연결하기 위한 MLP의 두 계층으로 구성되어 있다. LLaVA-Architecture의 비전-언어 사전-트레이닝 단계에서, 사전-트레이닝된 비전 인코더 및 대형 언어 모델 둘 다 동결된 상태로 유지되고, MLP 계층만이 대규모 이미지 캡션 데이터세트 Schuhmann 등에 대해 트레이닝된다(2022). 우리는 시각적 지시 튜닝 없이 미리 훈련된 이 LLaVA 모델을 초기 모델로 활용하고 Vision-Flan에서 미세 조정합니다. 시각적 명령어 튜닝 동안, 우리는 비전 인코더를 동결 상태로 유지하면서 MLP 레이어와 언어 모델을 모두 미세 조정한다.\n' +
      '\n' +
      '2단계 Visual Instruction Tuning은 기존의 Liu et al. (2023); Dai et al. (2023)은 시각적 명령어 튜닝을 위해 인간 라벨링된 데이터와 GPT-4 합성 데이터를 혼합한 2단계 명령어 튜닝 파이프라인을 소개한다. 그림 3에 나타난 바와 같이, 첫 번째 단계에서는 미세하게 된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Dataset** & Instances \\# & Tasks \\# & Source \\\\ \\hline LLaVA Liu et al. (2023) & 150K & 3 & Synthetic \\\\ LAMM Yin et al. (2023) & 196K & 8 & Synthetic \\\\ VL-Qwen Bai et al. (2023) & 350K & Unknown & Private \\\\ M\\({}^{\\dagger}\\)IT Li et al. (2023) & 2.4M & 40 & Synthetic \\\\ mPu-Qw Yeh et al. (2023) & 150K & 3 & Synthetic \\\\ Shikra Chen et al. (2023) & 156K & 4 & Synthetic \\\\ SVIT Zhao et al. (2023) & 4.2M & 4 & Synthetic \\\\ MultiInstruct Xu et al. (2023) & 510K & 62 & Public \\\\ Vision-Flan (Ours) & 1.6M & 196 & Public \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: Vision-Flan과 기존 시각 명령어 튜닝 데이터셋의 비교.\n' +
      '\n' +
      '그림 2: Vision-Flan과 이전 시각 명령어 튜닝 데이터 세트 간의 작업 다양성 비교. LLaVA와 SVIT는 매우 거친 분류의 작업을 보고한다. 각 원은 작업 범주를 나타내며 반지름은 해당 범주의 작업 수에 비례합니다. 다른 데이터 세트에 대한 원의 반지름은 비슷합니다.\n' +
      '\n' +
      'VLM은 Vision-Flan의 모든 187개 작업에서 조정되어 다양한 기능을 획득하고 결과 모델을 Vision-Flan 베이스로 명명합니다. 그러나 학술 데이터 세트에 제시된 목표 출력의 간결성으로 인해 Vision-Flan Base의 응답은 인간이 선호하는 형식이 아니다. 따라서 GPT-4 합성 데이터를 기반으로 Vision-Flan Base를 세분화하여 모델의 출력을 인간의 선호도와 정렬한다. 우리는 계산된 모델을 Vision-Flan 채팅으로 나타낸다. 이 훈련 프레임워크는 시각적 명령 튜닝에서 인간 표지 및 GPT-4 합성 데이터의 뚜렷한 기여에 대한 깊은 통찰력을 제공하면서 최소한의 GPT-4 합성 데이터를 필요로 한다.\n' +
      '\n' +
      '구현 세부사항 Vicuna-13B v1.5[14], CLIP-ViT-L-336px[20] 및 MLP의 두 계층을 VLM으로 사용하여 LLaVA-아키텍처를 활용합니다. 1단계 명령어 튜닝을 위해 8개의 A100 GPU에서 학습률 2e-5와 장치 배치 크기 16에 대해 1 에폭 동안 Vision-Flan에서 MLP 레이어와 언어 모델을 미세 조정한다. 2단계 명령어 튜닝을 위해 LLaVA 데이터셋[13]에서 무작위로 샘플링된 1,000개의 인스턴스에 대한 MLP 레이어와 언어 모델을 학습률 1e-5 및 8개의 GPU에서 128단계에 대해 장치 배치 크기 8에 따라 추가로 미세 조정한다. 다음 섹션에서는 LLaVA 데이터 세트와 GPT-4 합성 데이터를 상호 교환하여 사용한다.\n' +
      '\n' +
      '##4 실험 설정\n' +
      '\n' +
      '평가 데이터세트 _multiple-choice_ 벤치마크: **MMbench**[15], **MME**[11], **MMMU**; _free-form generation_ 벤치마크: **MM-Vet**[11] 및 **LLaVA-Bench**; _hallucination_ 벤치마크: **POPE**[15], _catastrophic forgetting_ 벤치마크: **CIFAR-10 및 CIFAR-100**[16], **MNIST**[12], **miniImageNet**[13]을 포함하여 널리 채택된 여러 멀티모달 평가 벤치마크 데이터셋에 대한 모델을 평가한다. 평가 데이터셋에 대한 자세한 내용은 부록 B에서 확인할 수 있다.\n' +
      '\n' +
      'MMbench, MME, MM-Vet, LLaVA-Bench, POPE 및 MMMU에 대한 평가 프로토콜은 각 모델의 성능을 평가하기 위해 평가 코드의 공식 구현을 엄격하게 따른다. CIFAR-10, CIFAR-100, MNIST 및 miniImageNet을 포함한 공식 평가 코드가 없는 데이터셋의 경우 최첨단 오픈소스 LLM, Vicuna 1.5 13B를 활용하여 평가를 수행하고 표 2의 CF 컬럼에서 이 네 가지 데이터셋에 대한 평균 성능을 보고한다. 평가 프로토콜에 대한 자세한 내용은 부록 C에서 찾을 수 있다.\n' +
      '\n' +
      '기준선 우리는 모델을 **BLIP-2**[15], **Instruct-BLIP**[15], **Shikra**[17], **LLaVA**[15], **Qwen-VL**, **Qwen-VL-Chat**[14], **LLaVA-1.5**[15]를 포함한 여러 최신 최신 비전 언어 모델과 비교한다. 모든 기준선에 사용된 LLM 및 이미지 인코더는 표 2와 같다. 기준선의 상세는 부록 D에서 확인할 수 있다.\n' +
      '\n' +
      '##5 결과 및 분석\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '표 2에서 입증된 바와 같이, Vision-Flan Base는 환각 및 치명적인 망각을 감소시키면서 MME, MM-Bench 및 MMMU를 포함한 종합 평가 벤치마크에서 최첨단 성능을 달성한다. 그러나 우리는 GPT-4 합성 데이터를 사용하여 훈련된 VLM에 비해 LLaVA-벤치 데이터 세트에서 Vision-Flan Base 점수가 상당히 낮다는 것을 관찰한다. 우리는 이러한 불일치를 학술 데이터 세트 내의 목표 출력의 간결성과 간결성으로 돌린다. 도 1에 도시된 바와 같이, VQA 태스크들은 종종 단일 또는 소수의 단어들을 포함하는 출력들을 산출한다. 많은 생성 작업의 출력조차도 일반적으로 하나 또는 두 개의 간결한 문장으로 제한된다. 이러한 작업에 대한 훈련은 Vision-Flan Base가 인간의 선호도와 일치하지 않는 간략한 응답을 생성하도록 이끈다. 반대로, 1,000개의 GPT-4 합성 데이터 인스턴스에 대한 2단계 튜닝을 통해, Vision-Flan Chat은 LLaVA-Bench에서 상당한 성능 향상을 달성하고,\n' +
      '\n' +
      '그림 3: 그림의 왼쪽은 LLaVA-Architecture를 나타내고 그림의 오른쪽은 2단계 시각적 명령어 튜닝 파이프라인을 나타낸다.\n' +
      '\n' +
      '상대적으로 더 낮은 환각과 치명적인 망각의 비율을 유지하면서 인간 선호 정렬을 측정하는 벤치마크. Vision-Flan 모델이 현재 VLM보다 나은 이유를 더 잘 이해하기 위해 OCR 및 개체 인식에 초점을 맞춘 두 가지 사례 연구를 수행하고 부록 E.2에서 양적 및 질적 결과를 모두 분석한다.\n' +
      '\n' +
      '표 2의 또 다른 발견은 Vision-Flan Base와 비교하여 Vision-Flan Chat이 GPT-4 합성 데이터에 의해 필연적으로 도입되는 편향과 환각을 입증하는 종합 평가 벤치마크에서 약간 열등한 성능을 달성한다는 것이며, 이는 섹션 5.2에서 자세히 논의된다.\n' +
      '\n' +
      '인간표지 및 GPT-4 합성 데이터셋의### 효과\n' +
      '\n' +
      '비전-플란 그림 4에서 태스크 다양성의 효과는 시각적 지시 조정 동안 사용된 비전-플란의 태스크 수와 4가지 종합 평가 벤치마크에 걸친 비전-플란 베이스의 성능 사이의 관계를 보여준다. 작업의 수가 증가함에 따라 모든 데이터 세트에 대한 Vision-Flan Base의 성능이 향상됨을 알 수 있다. 다양한 작업에서 다양한 수의 인스턴스가 미치는 영향을 평가하기 위해 시각적 지시 조정에 사용되는 총 인스턴스의 양을 고정하고 다른 수의 작업으로 실험을 수행한다. 표 3에서 입증된 바와 같이, 트레이닝 인스턴스들의 수가 일정할 때, 태스크들의 수를 증가시키는 것은 모델 성능을 상당히 향상시킨다. 이러한 발견은 Vision-Flan_ 내의 다양한 인간 표지 작업 배열이 VLMs_의 능력을 개선하는 데 필수적이라는 가설을 입증한다.\n' +
      '\n' +
      'GPT-4 합성 데이터가 종합 평가 벤치마크에 미치는 영향 또한 GPT-4 합성 데이터가 종합 평가 벤치마크에 대한 모델의 성능을 향상시킬 수 있는지 분석하고 그림 5의 결과를 보여준다. GPT-4 합성 데이터 인스턴스에 Vision-Flan Base를 추가로 조정해도 성능 향상으로 이어지지 않는다. 적은 양의 GPT-4 합성 데이터(100) 상에서 미리 훈련된 LLaVA 모델을 튜닝하는 것은 MME 상에서 그 성능을 향상시킬 수 있지만, 훈련 인스턴스의 수를 더 증가시키는 것은 어떠한 개선으로도 이어지지 않는다. 우리는 또한 MM-Bench 데이터세트에서 유사한 경향을 관찰하고 그 결과를 부록 E.1에 보고한다. 이러한 관찰은 LLMs에서의 최근 발견과 일치한다: _GPT-4 합성 데이터는 모델의 능력을 향상시키지 않고 오히려 인간 선호 포맷_Jain 등(2023); G\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline\n' +
      '**Model** & LLM & Image Encoder & MM-Bench & MME & MMMU & LLaVA-Bench & MM-Vet & Pope & CF \\\\ \\hline BLIP-2 & FlanT5-XXL & ViT-g/14 & - & 1293.8 & 34.0 & - & 22.4 & 85.3 & - \\\\ InstructBlip & Vicuna-13B & ViT-g/14 & 36.0 & 1212.8 & 33.8 & 58.2 & 25.6 & 78.9 & - \\\\ Mini-GPT4 & Vicuna-13B & ViT-g/14 & 24.3 & 581.67 & 27.6 & - & - & - & - \\\\ Shixra & Vicuna-13B & ViT-g/14 & 58.8 & - & - & - & - & - & - \\\\ LLaVA & Vicuna-13B v1.5 & CLIP-ViT-336px & 38.7 & 1151.6 & - & 70.8 & 33.4 & 75.3 & - \\\\ Qwen-VL & Qwen-7B & ViT-bigG & 38.2 & - & - & - & - & - & - \\\\ Qwen-VL-Chat & Qwen-7B & ViT-bigG & 60.6 & 1487.5 & 32.9 & 73.6 & - & - & 72.1 \\\\ LLaVA 1.5 & Vicuna-13B v1.5 & CLIP-ViT-L-336px & 66.7 & 1531.3 & 33.6 & 70.7 & 35.4 & 83.6 & 73.3 \\\\ \\hline \\hline Vision-Flan Base & Vicuna-13B v1.5 & CLIP-ViT-L-336px & **69.8** & **1537.8** & **34.4** & 38.5 & 33.4 & 85.9 & **87.2** \\\\ \\hline \\hline \\multicolumn{10}{l}{**Second-Stage Tuning with 1,000 GPT-4 Synthesized Instances**} \\\\ Vision-Flan Chat & Vicuna-13B v1.5 & CLIP-ViT-L-336px & 67.6 & 1490.6 & 34.3 & **78.3** & **38.0** & **86.1** & 84.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 널리 채택된 벤치마크 데이터 세트에 대한 VLM의 포괄적인 평가. CF는 4개의 치명적인 망각 벤치마크에서 VLM의 평균 성능을 나타낸다.\n' +
      '\n' +
      '그림 4: 4개의 포괄적인 벤치마크에 대한 성과 대 훈련 과제 수.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline \\# of Tasks & \\# of Instances per Task & MMB & MME & Pope & MMMU \\\\ \\hline\n' +
      '**Training with 100,000 Instances** & & & & \\\\ \\hline\n' +
      '10 & 10,000 & 58.3 & 723.9 & 81.0 & 32.6\\\\\n' +
      '187 & 500 & 58.8 & 1314.3 & 83.3 & 33.3\\\\\n' +
      '**Training with 200,000 Instances** & & & & \\\\ \\hline\n' +
      '20 & 10,000 & 58.8 & 897.3 & 83.4 & 31.8\\\\\n' +
      '187 & 1,000 & 63.5 & 1373.5 & 83.6 & 33.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 고정된 총 데이터 인스턴스로 훈련된 Vision-Flan Base의 비교.\n' +
      '\n' +
      'GPT-4 합성 데이터가 인간 선호 정렬에 미치는 영향은 제안된 2단계 튜닝 프레임워크를 사용할 때 LLaVA 데이터 세트에서 1,000개의 GPT-4 합성 인스턴스에 대해 2단계 미세 조정을 수행함으로써 Vision-Flan Chat이 LLaVA-Bench 데이터 세트에서 훨씬 더 나은 성능(78.5 v.s. 38.5)을 달성한다는 것을 발견했다. 이러한 관찰은 VLM을 인간의 선호도와 정렬하기 위해 필요한 대규모 GPT-4 합성 데이터 세트에 대한 광범위한 미세 조정이라는 질문을 제기하게 한다. 이를 해결하기 위해 Vision-Flan Base와 사전 훈련된 LLaVA 모델을 100에서 158,000까지의 다양한 수의 GPT-4 합성 인스턴스에 대해 세분화하고 그림 6의 결과를 보여준다. 1,000개의 인스턴스에서 Vision-Flan Base는 78.3의 점수를 얻었고 훈련 인스턴스의 수를 더 증가시키면 성능 저하로 이어진다. 사전 훈련된 LLaVA 모델에서도 유사한 경향을 볼 수 있다.\n' +
      '\n' +
      'GPT-4 합성 데이터 원인 환각 및 바이어스 동시 작업[14]은 현재의 VLM에서의 환각이 긍정적인 답변(즉, "예")에 대한 편향에 의해 야기될 수 있음을 식별한다. 그림 7에서 우리는 GPT-4 합성 데이터 세트의 환각, "예" 비율 및 훈련 인스턴스 수 사이의 관계를 명시적으로 보여준다. GPT-4 합성 인스턴스의 수가 증가함에 따라 이미지에 객체가 없더라도 모델의 응답이 "예"라는 답변에 치우쳐 모델이 환각을 느끼게 된다. 이 관찰은 VLM의 환각과 편향을 피하기 위해 소량의 GPT-4 합성 훈련 사례가 선호됨을 시사한다.\n' +
      '\n' +
      '혼합 데이터 Vs에 대한 단일 단계 튜닝 2단 튜닝\n' +
      '\n' +
      '이 섹션에서는 동일한 사전 훈련된 LLaVA 모델을 기반으로 두 가지 훈련 전략의 성능을 비교한다: (1) Vision-Flan과 LLaVA 데이터 세트의 혼합에서 미세 조정한다; (2) Vision-Flan과 LLaVA 데이터 세트의 1,000 인스턴스를 사용하여 미세 조정한다. 표 4에 도시된 바와 같이, Vision-Flan과 GPT-4 합성 데이터의 혼합에 대해 미세 조정된 VLM의 성능은 우리의 2단계 튜닝 프레임워크를 통해 훈련된 Vision-Flan 채팅에 비해 현저하게 떨어진다.\n' +
      '\n' +
      'Visual Instruction Tuning 시 VLM에서 필수적으로 개선되는 사항\n' +
      '\n' +
      'LLaVA-Architecture에서 MLP 레이어는 비전 인코더에서 LLM의 임베딩 공간으로 시각적 특징을 매핑한다. 그런 다음 LLM은 시각적 특징을 해석하고 텍스트 지침을 따라 응답을 생성한다. 표 5에서, 우리는 시각적 명령어 튜닝 동안 상이한 모듈들을 트레이닝한 결과를 보여주고 MLP들만을 튜닝하는 것이 야기하는 것을 관찰한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & \\# of LLaVA & MME & LLaVA-Bench & MM-Vet \\\\ \\hline Mixed Data & 1,000 & 1364.0 & 52.7 & 36.6 \\\\ Mixed Data & 158,000 & 1317.9 & 63.9 & 36.8 \\\\ Two-stage & 1,000 & 1490.6 & 78.3 & 38.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 혼합 데이터에 대한 단일 단계 미세 조정과 2단계 미세 조정 간의 비교.\n' +
      '\n' +
      '그림 5: GPT-4 합성 훈련 인스턴스 수가 MME에 미치는 영향. 회색 점선은 LLaVA 1.5의 성능을 나타낸다.\n' +
      '\n' +
      '그림 6: GPT-4 합성 인스턴스 수가 인간 선호도 정렬에 미치는 영향. 회색 점선은 LLaVA 1.5의 성능을 나타낸다.\n' +
      '\n' +
      '그림 7: GPT-4 합성 훈련 사례 수가 환각 벤치마크와 "예"의 비율에 미치는 영향. 파선은 최첨단 LLaVA 1.5 모델의 성능을 나타낸다.\n' +
      '\n' +
      '시각적 명령어 튜닝 동안 MLP 및 LLM을 모두 튜닝하는 것과 비교하여 상당한 성능 저하. 그러나, 동결된 MLP로 LLM을 튜닝하는 것은 두 모듈을 튜닝하는 것과 유사한 성능을 초래하여, 시각적 명령어 튜닝이 MLP가 사전 트레이닝 동안 충분히 학습되는 동안 주로 LLM이 시각적 특징을 더 잘 이해할 수 있게 함을 입증한다. 이 주장을 뒷받침하기 위해 Vision-Flan Base와 Vision-Flan Chat의 명령어 조정 MLP를 사전 훈련된 LLaVA 모델로 교체하고, 사전 훈련된 MLP를 사용하면 표 6과 같이 대부분의 태스크에서 두 모델 모두 90% 이상의 성능을 유지할 수 있음을 보여준다. 또한 사전 훈련된 MLP와 명령어 조정 MLP의 매개변수 사이의 피어슨 상관 계수를 계산하고 상관 계수가 0.99보다 높다는 것을 발견했다.\n' +
      '\n' +
      '##6 관련 업무\n' +
      '\n' +
      '명령어 튜닝 Wei et al.(2022)은 NLP에서 처음 도입되고 시각-언어 도메인에 적응되었다. MultiInstruct Xu et al. (2023)은 미리 훈련된 VLM들의 제로-샷 성능을 향상시키기 위한 최초의 인간-라벨 멀티-모달 명령어 튜닝 데이터세트를 제안한다. LLaVA Liu et al. (2023)은 GPT-4를 레버리지하여 비주얼 다이얼로그들을 생성하기 위해 기존의 컴퓨터-비전 데이터세트들로부터 캡션들 또는 밀집 캡션들과 같은 텍스트 주석들을 재목적화하고, 비주얼 명령어 튜닝을 위한 컴플렉스 VQA 및 디테일 캡션들을 생성한다. LLaVA에 이어, mPLUG-Owl Ye et al. (2023), LAMM Yin et al. (2023), MIMIC-IT Li et al. (2023) 및 MacawLLM Lyu et al. (2023)은 명령어 튜닝 태스크를 3D-도메인, 다중 이미지 및 비디오로 더 확장하고 트레이닝 인스턴스의 양을 증가시키기 위해 GPT-4 및 ChatGPT와 같은 독점 LLM을 레버리지한다. MiniGPT-4 Zhu et al.(2023)은 ChatGPT를 활용하여 사전 훈련된 VLM 자체로부터의 출력을 정제한다. InstructBLIP Dai et al. (2023) 및 LLaVA-1.5 Liu et al. (2023)은 시각적 명령어 튜닝을 향상시키기 위해 인간-주석 및 GPT4 합성 데이터 세트를 혼합한다.\n' +
      '\n' +
      '최근 몇 가지 연구는 시각적 지시 조정을 개선하기 위한 다양한 전략을 탐구한다. StableLLaVA Li 등(2023) 및 VPG-C Li 등(2023)은 도메인 바이어스를 완화하고 VLM들이 시각적 세부사항들에 참석하도록 장려하기 위해 Stable Diffusion Rombach 등(2022) 또는 Blended Diffusion Avrahami 등(2022)을 사용하여 이미지 및 텍스트 둘 다를 생성한다. Liu et al. (2023)은 포지티브 명령들에 의해 도입되는 바이어스를 입증하고 강건성을 향상시키기 위한 네거티브 명령 예들을 도입한다. Shikra Chen et al. (2023)은 VLM의 참조 능력을 향상시키기 위해 시각적 명령 튜닝에 시각적 접지 작업을 통합한다. LLaVAR Zhang et al.(2023) 및 BLIVA Hu et al.(2023)은 OCR 툴 및 GPT-4를 레버리지하여 VLM들이 이미지에서 텍스트를 이해하는 것을 돕는 태스크들을 생성한다. Lu et al. (2023) 및 SVIT Zhao et al. (2023)은 VLM의 크기 및 GPT-4 합성 데이터세트의 크기를 스케일링하는 효과를 경험적으로 연구한다. 두 개의 동시 작업 Wang et al. (2023); Chen et al. (2023)은 시각적 명령 튜닝 데이터를 생성하고 우수한 성능을 달성하기 위해 이미지로서 GPT-4V를 직접 프롬프트한다. 추가적인 관련 작업은 부록 G에서 확인할 수 있다.\n' +
      '\n' +
      '우리의 작업은 모든 이전 작업과 달리 VLM의 능력을 향상시키기 위해 시각적 지시 조정에서 인간 라벨 작업을 스케일링하는 데 주로 중점을 둔다. 또한 인간 표지 및 GPT-4 합성 데이터의 특성을 이해하고 의미 있는 결론을 도출하기 위해 광범위한 분석을 수행한다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '우리는 187개의 다양한 태스크와 1,664,261개의 학술 데이터 세트에서 수집된 인스턴스로 구성된 가장 다양한 공개 가능한 시각적 명령어 튜닝 데이터 세트인 Vision-Flan을 구성하고 각 태스크에는 전문가가 작성한 명령이 수반된다. 제안된 2단계 튜닝 프레임워크를 사용하여 Vision-Flan에서 훈련된 VLM이 종합 평가 벤치마크에서 최첨단 성능을 달성함을 보여준다. 또한, 우리는 광범위한 분석을 수행하고 공개한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Model** & MMB & MME & LLaVA-Bench & Pope \\\\ \\hline\n' +
      '**Vision-Flan Base** & 69.8 & 1537.8 & 38.5 & 85.9 \\\\ \\hline\n' +
      '**+**Prerained MLP** & 68.0 & 1403.1 & 36.4 & 84.0 \\\\\n' +
      '**Vision-Flan Chat** & 67.6 & 1490.6 & 78.3 & 86.1\\\\\n' +
      '**+**Pretrained MLP** & 65.7 & 1332.2 & 73.8 & 85.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 시각적 명령 튜닝된 MLP를 사전 훈련된 MLP로 교체한 결과. 회색 행은 원래 모델의 성능을 보여주고 노란색 행은 명령어 조정된 MLP를 사전 훈련된 MLP로 대체한 후 성능을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline LLM & MLPs & MM-Bench & MME & LLaVA-Bench & Pope \\\\ \\hline ✗ & ✗ & 45.0 & 936.3 & 32.4 & 51.9 \\\\ ✗ & ✓ & 52.4 & 1107.3 & 39.1 & 83.3 \\\\ ✓ & ✗ & 69.2 & 1495.5 & 39.3 & 85.6 \\\\ ✓ & ✓ & 69.8 & 1537.8 & 38.5 & 85.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: Vision-Flan Base에서 서로 다른 모듈의 튜닝 효과. ✓ ● 모듈은 튜닝되고 ✗는 모듈이 시각적 명령 튜닝 동안 동결됨을 나타낸다.\n' +
      '\n' +
      '시각적 명령 조정에서 인간 표지 및 GPT-4 합성 데이터의 뚜렷한 기여.\n' +
      '\n' +
      '## 8 Limitations\n' +
      '\n' +
      'Vision-Flan에 포함된 모든 작업은 영어로 되어 있으며, 이는 데이터 세트 및 모델의 사용을 영어 말하기 모집단으로 제한한다. 향후 작업은 다국어 작업으로 Vision-Flan을 확장해야 한다. 또한 Vision-Flan의 모든 작업은 단일 이미지로만 구성됩니다. 많은 실세계 비전 언어 작업은 모델이 여러 이미지를 입력으로 가져가야 한다. 따라서 향후 작업은 여러 이미지 또는 비디오를 포함하는 비전 언어 작업을 탐구해야 한다.\n' +
      '\n' +
      '우리의 분석은 주로 GPT-4 합성 시각 명령어 튜닝 데이터 세트에 초점을 맞춘다. 최근, GPT-4V 3이 공개적으로 이용가능해짐에 따라, 시각적 명령 튜닝 데이터를 생성하기 위해 이미지를 입력으로서 GPT-4V를 프롬프트하는 일부 동시 작업[22, 23]이 있다. 향후 작업은 이러한 데이터 세트에 대한 VLM의 튜닝 효과를 분석하고 장점과 단점을 식별할 수 있다.\n' +
      '\n' +
      '각주 3: [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card)\n' +
      '\n' +
      '실험에서, 우리는 LLaVA-Architecture[10]의 강한 성능과 높은 효율로 인해 주로 LLaVA-Architecture[10]에 초점을 맞춘다. 그러나 BLIP2[11]의 Q-former 및 Flamingo[1]의 Perceiver Resampler와 같은 다른 기초 아키텍처가 존재한다. 보다 일반적인 결론을 도출하기 위해 향후 보다 다양한 VLM 아키텍처를 탐색할 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* November 2, 2019, pp. 8947-8956. Cited by: SS1.\n' +
      '*[2]J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. 하손기 Lenc, A. Mensch, K 밀리컨, M. 레이놀즈, R 링, E. 러더포드, S. 카비태 한진 공성호 사만귀에 몬테이로, J. L. 메닉, S. Borgeaud, A. Brock, A. Nematzadeh, S. 샤리프자데 빈코스키, R 바레이라, 오 빈얼스, A. 지서먼, K. Simonyan(2022) Flamingo: 수 샷 학습을 위한 시각적 언어 모델. 인용: SS1.\n' +
      '*[3]S. 안톨, A. 아그라왈, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and D. Parikh (2015) Vqa: 시각적 질문 응답. In Proceedings of the IEEE international conference on computer vision, pp. 2425-2433. Cited by: SS1.\n' +
      '*[4]O. 아브라함, D. 리친스키, O. 튀김(2022) 자연 이미지의 텍스트 기반 편집을 위한 블렌디드 확산. IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 18187-18197. Cited by: SS1.\n' +
      '*[5]J. 배승 배영 추주영 최경 젠장, X 등영 판원 지영 한필황 지명 이정린 Lin D. Liu G. Liu C. Lu K. 루정마 Men, X 렌철탄 탄준두 왕욱 왕승 Wu, B. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. 양영 야오비위안 위안장 장영 장영 장창주 저우, T. Zhu(2023) Qwen 기술 보고서. CoRRabs/2309.16609. External Links: 2309.16609 Cited by: SS1.\n' +
      '*[6]J. 배승 배승 양승 왕승 Tan, P. Wang, J. Lin, C. Zhou, 및 J. Zhou(2023) Qwen-vl: 이해, 현지화, 텍스트 판독, 및 그 이상을 위한 다목적 비전-언어 모델. 인용: SS1.\n' +
      '*[7]J. 바부, D. 마요, J. 알베리오, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz(2019) ObjectNet: a large-scale bias-controlled dataset for pushing the object recognition models. In Advances in Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 9448-9458. External Links: 1905.02192 Cited by: SS1.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      'IEEE International Conference on Computer Vision_의 진행.\n' +
      '* Chen et al. (2023a) Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. 2023a. Shikra: Unleashing multimodal llm\'s referential dialogue magic. _ CoRR_, abs/2306.15195.\n' +
      '* Chen et al. (2023b) Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023b. Sharegpt4v: 캡션이 더 좋은 대형 멀티모달 모델 개선. _ CoRR_, abs/2311.12793.\n' +
      '* Chen et al. (2023c) Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. 2023c. 미리 훈련된 시각과 언어 모델이 시각 정보 추구 질문에 답할 수 있는가? 14948-14968 페이지.\n' +
      '* Chen et al. (2020) Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, 및 Jingjing Liu. 2020. Uniter: Universal image-text representation learning. 유럽 컴퓨터 비전 회의에서 104-120쪽, 스프링어\n' +
      '* Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Liamin Zhang, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: 90%* chatgpt 품질의 gpt-4를 인상하는 오픈소스 챗봇.\n' +
      '* Chng et al. (2020) Chee-Kheng Chng, Chee Seng Chan, and Cheng-Lin Liu. 2020. Total-text: toward orientation robustness in scene text detection. _ Int. J. Document Anal. 인식._ , 23(1):31-52.\n' +
      '* Chua et al. (2009) Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yantao Zheng. 2009. NUS-WIDE: 국립 싱가포르 대학의 실제 웹 이미지 데이터베이스. [Proceedings of the 8th ACM International Conference on Image and Video Retrieval, CIVR 2009, Santorini Island, Greece, July 8-10, 2009_]. ACM\n' +
      '* Cimpoi et al.(2014) M. 심포이 마지일옥키노스 모하메드와 A. 베달디 2014. 야생의 질감을 묘사하고 있습니다. In _Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_.\n' +
      '* Coates et al. (2011) Adam Coates, Andrew Y. 응, 이홍락 2011. 비지도 특징 학습에서 단일 계층 네트워크의 분석. In _Proceedings of the Fourth International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Laudendale, USA, April 11-13, 2011_, volume 15 of _JMLR Proceedings_, pages 215-223. JMLR.org.\n' +
      '* Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. H. Hoi. 2023. 명령어: 명령어 튜닝을 갖는 범용 비전-언어 모델들을 향함. _ CoRR_, abs/2305.06500\n' +
      '* Darlow et al. (2018) Luke Nicholas Darlow, Elliot J. Crowley, Antreas Antoniou, and Amos J. Storkey. 2018. CINIC-10은 이미제넷 또는 CIFAR-10이 아니다. _CoRR_, abs/1810.03505.\n' +
      '* Das et al. (2019) Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Stefan Lee, Jose M. F. Moura, Devi Parikh, and Dhruv Batra. 2019. 비주얼 다이얼로그. 41권, 페이지 1242-1256.\n' +
      '* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. _Proceedings of the 2019 Conference of the North American chapter of Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1(Long and Short Papers)_, pages 4171-4186. Association for Computational Linguistics.\n' +
      '* Eitz et al. (2012) Mathias Eitz, James Hays, and Marc Alexa. 2012. 인간은 물체를 어떻게 스케치하는가? _ ACM Trans. 그래프. (Proc. SIGGRAPH)_, 31(4):44:1-44:10이다.\n' +
      '* Everingham et al. (2010) Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. 윈과 앤드류 지서먼 2010. 파스칼 비주얼 오브젝트 클래스(VOC) 챌린지.\n' +
      '* Farhadi et al. (2009) Ali Farhadi, Ian Endres, Derek Hoiem, and David A. Forsyth. 2009. 객체들을 그들의 속성들로 기술하는 단계. 2009년 _2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA_, pages 1778-1785. IEEE Computer Society.\n' +
      '* Fei-Fei et al.(2004) Li Fei-Fei, Rob Fergus, and Pietro Perona. 2004. learning generatingative visual models from few training examples: a incremental bayesian approach tested on 101 object categories. 178쪽\n' +
      '* Fu et al. (2023) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. 2023. MME: 멀티모달 대형 언어 모델에 대한 종합 평가 벤치마크. _ CoRR_, abs/2306.13394.\n' +
      '* Ganin et al. (2017) Yaroslav Ganin, Evgeniya Ustinova, 하나 Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor S. 렘피츠키 2017. Domain-adversarial training of neural networks. Gabriela Csurka, editor, _Domain Adaptation in Computer Vision Applications_, Advances in Computer Vision and Pattern Recognition, pages 189-209. Springer.\n' +
      '* Gao et al. (2023) Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. 2023. 라마-어댑터 V2: 파라미터-효율적 시각 지시 모델 _ CoRR_, abs/2304.15010.\n' +
      '* ECCV 2018\n' +
      '워크숍 - 독일 뮌헨, September 8-14, 2018, Proceedings, Part II_, vol 11130 of _Lecture Notes in Computer Science_, pages 676-691. Springer.\n' +
      '* Geirhos et al. (2019) Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. 2019. Imagenet-trained cnns는 텍스쳐에 치우쳐 있으며, 형상 바이어스를 증가시키면 정확성과 견고성이 향상된다. _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net.\n' +
      '* Goyal et al. (2017) Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. vqa matter에서 v를 만드는 것: 시각적 질문 응답에서 이미지 이해의 역할을 높이는 것. IEEE conference on computer vision and pattern recognition_의 _Proceedings, pages 6904-6913.\n' +
      '* 그리핀 등 (2007) Gregory Griffin, Alex Holub, and Pietro Perona. 2007. Caltech-256 객체 카테고리 데이터세트.\n' +
      '* Gudibande et al. (2023) Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. 전매특허 lms를 모방한다는 잘못된 약속 CoRR_, abs/2305.15717.\n' +
      '* Guillaume Jaume et al.(2019) Jean-Philippe Thiran Guillaume Jaume, Hazim Kemal Ekenel. 2019. Funsd: 시끄러운 스캔 문서에서의 형태 이해를 위한 데이터셋. In _Accepted to ICDAR-OST_\n' +
      '* ECCV 2020\n' +
      '- 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVII_, volume 12362 of _Lecture Notes in Computer Science_, pages 417-434. Springer.\n' +
      '* Hendrycks et al. (2021a) Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, 및 Justin Gilmer. 2021a. 강건성의 많은 면: 유통 외 일반화에 대한 비판적 분석. 페이지 8320-8329.\n' +
      '* Hendrycks and Dietterich (2019) Dan Hendrycks and Thomas G. Dietterich. 2019. Benchmarking neural network robustness to common corruption and perturbations.\n' +
      '* Hendrycks et al. (2021b) Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. 2021b. 자연적 적대적 예지 페이지 15262-15271.\n' +
      '* Van Horn et al. (2018) Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alexander Shepard, Hartwig Adam, Pietro Perona, and Serge J. Belongie. 2018. inaturalist species classification and detection dataset.\n' +
      '* Horn et al. (2015) Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge J. Belongie. 2015. 시민 과학자들과 함께 조류 인식 앱과 대규모 데이터 세트를 구축: 세밀한 데이터 세트 컬렉션의 세밀한 인쇄. _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015_, pages 595-604. IEEE Computer Society.\n' +
      '* Hou et al. (2021) Qiang Hou, Weiqing Min, Jing Wang, Sujuan Hou, Yuanjie Zheng, and Shuqiang Jiang. 2021. Foodogodet-1500: 멀티 스케일 피처 디커플링 네트워크를 통한 대규모 식품 로고 검출을 위한 데이터세트. _Proceedings of the 29th ACM International Conference on Multimedia_, pages 4670-4679.\n' +
      '* Hsu et al. (2021) Ting-Yao Hsu, C. Lee Giles, and Ting-Hao Kenneth Huang. 2021. Scicap: 과학적 수치에 대한 캡션 생성. 3258-3264 페이지.\n' +
      '* Hu et al. (2023) Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, Zhuowen Tu. 2023. BLIVA: 텍스트가 풍부한 시각적 질문의 더 나은 처리를 위한 간단한 멀티모달 LLM. _ CoRR_, abs/2308.09936.\n' +
      '* Huang et al. (2007) Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. 2007. Labeled face in the wild: a database for studying face recognition in nonstrained environment. 암허스트 매사추세츠 대학의 기술 보고서 07-49\n' +
      '* Hudson and Manning (2019) Drew A Hudson and Christopher D Manning. 2019. Gqa: real-world visual reasoning and compositional question answering을 위한 새로운 데이터셋. IEEE/CVF Conference on computer vision and pattern recognition_의 _Proceedings, pages 6700-6709.\n' +
      '*황과 슈워츠(2023) 은정황과 베어드 슈워츠. 2023. MemeCap: memes의 캡션 및 해석을 위한 데이터세트.\n' +
      '* Jain et al. (2023) Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rocktaschel, and David Scott Krueger. 2023. 세부조정이 절차적으로 정의된 업무에 미치는 영향을 기계적으로 분석한다.\n' +
      '* Johnson et al. (2017) Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. 2017. CLEVR: 작곡 언어 및 초등 시각적 추론을 위한 진단 데이터세트. _2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017_, pages 1988-1997. IEEE Computer Society.\n' +
      '* Kafle et al. (2018) Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. 2018. Dvqa: 질문 응답을 통한 데이터 시각화의 이해. IEEE conference on computer vision and pattern recognition_의 _Proceedings, pages 5648-5656.\n' +
      '*20, 2011_, page 20. ACM.\n' +
      '* Karkkanen and Joo(2021) Kimmo Karkkanen and Jungseock Joo. 2021. Fairface: 편향 측정 및 완화를 위한 균형 잡힌 인종, 성별, 연령에 대한 얼굴 속성 데이터세트. IEEEWinter Conference on Applications of Computer Vision, WACV 2021, Waikoloa, HI, USA, January 3-8, 2021_, pages 1547-1557. IEEE.\n' +
      '* ECCV 2016\n' +
      '- 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV_, volume 9908 of _Lecture Notes in Computer Science_, pages 235-251. Springer.\n' +
      '* Khosla et al. (2011) Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. 2011. 세밀한 이미지 분류를 위한 새로운 데이터 세트. _First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition_, Colorado Springs, CO.\n' +
      '* Kirstain et al. (2023) Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. 2023. Pick-a-pic: 텍스트 대 이미지 생성을 위한 사용자 선호도의 오픈 데이터세트. 부피 abs/2305.01569입니다.\n' +
      '* Krause et al. (2013) Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 2013. 3d object representation for fine-grained categorization. IEEE International Conference on Computer Vision Workshops, ICCV Workshops 2013, Sydney, Australia, December 1-8, 2013_, pages 554-561. IEEE Computer Society.\n' +
      '* Kreiss et al. (2022) Elisa Kreiss, Fei Fang, Noah D. Goodman, and Christopher Potts. 2022. Concaldia: 목적을 갖는 이미지 기반 텍스트 생성을 향함.\n' +
      '* Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _ International journal of computer vision_, 123:32-73.\n' +
      '* Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layer of features from tiny images.\n' +
      '* Kumar et al. (2022) Anurendra Kumar, Keval Morabia, William Wang, Kevin Chang, and Alex Schwing. 2022. CoVA: 웹 페이지 정보 추출을 위한 상황 인식 시각 주의. _Proceedings of the Fifth Workshop on e-Commerce and NLP(ECNLP 5)_, pages 80-90, Dublin, Ireland. 컴퓨터 언어학과의 연관성\n' +
      '* Lau et al. (2019) Jason J Lau, Soumya Gayen, Dina Demner, and Asma Ben Abacha. 2019. Visual question answering in radiology (vqa-rad).\n' +
      '* LeCun(1998) Yann LeCun. 1998. mnist database of handwritten digits. _ http://yann. 레쿤 com/exdb/mnist/_.\n' +
      '*15, 2022_, pages 3108-3120. ACM.\n' +
      '* Li 등(2023a) Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, 및 Ziwei Liu. 2023a. IMIC-IT: Multi-modal in-context instruction tuning. _ CoRR_, abs/2306.05425.\n' +
      '* Li 등(2023b) Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, 및 Ziwei Liu. 2023b. Otter: in-context 명령어 튜닝을 갖는 멀티모달 모델. _ CoRR_, abs/2305.03726.\n' +
      '* Li et al.(2017) Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. 호스피데일스 2017. 더 깊고, 더 넓고, 더 예술적인 영역 일반화. IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017_, pages 5543-5551. IEEE Computer Society.\n' +
      '* Li et al. (2023) Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, and Yueting Zhuang. 2023c. 무샷 시연 지침을 따르도록 멀티모달 llms를 미세 조정합니다.\n' +
      '* Li et al. (2023) Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023d. BLIP-2: 냉동 이미지 인코더 및 대형 언어 모델로 부트스트래핑 언어-이미지 사전 트레이닝. 202:19730-19742.\n' +
      '* Li 등(2023) Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Pei Yi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu. 2023e. M\\({}^{3}\\)it: 멀티모달 멀티언어 명령어 튜닝을 위한 대규모 데이터세트. _ CoRR_, abs/2306.04387.\n' +
      '* Li et al. (2019) Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. Visualbert: 시각 및 언어에 대한 단순하고 수행 가능한 기준선 _ CoRR_, abs/1908.03557.\n' +
      '* Li et al. (2018) Qing Li, Qingyi Tao, Shafiq R. 조티, 지안페이 카이, 지보 루오 2018. VQA-E: 시각적 질문에 대한 답변 설명, 정교화 및 향상. 11211:570-586\n' +
      '* Li and Deng(2019) Shan Li and Weihong Deng. 2019. 신뢰할 수 있는 크라우드소싱 및 제한 없는 표정 인식을 위한 심층 지역성 보존 학습_ IEEE Transactions on Image Processing_, 28(1):356-370.\n' +
      '* Li et al. (2023) Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin Fu, Guosheng Lin, Chunhua Shen, Ling Chen, and Yunchao Wei. 2023f. 스타벨라바: 합성된 영상-대화 데이터를 이용한 향상된 시각적 명령어 튜닝. _ CoRR_, abs/2308.10253.\n' +
      '* Li et al. (2023) Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023g. 대용량 시각 언어 모델에서 대상 환각을 평가합니다. 292-305페이지\n' +
      '\n' +
      'T 린민 Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. Lawrence Zitnick(2014)Microsoft COCO: common objects in context. In Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, Vol. 8693 of Lecture Notes in Computer Science, pp. 740-755. Cited by: SS1.\n' +
      '*Z. 린자 Chen, D. Pathak, P. Zhang, D. Ramanan(2023)은 비전 언어 모델에서 언어 전적의 역할을 재방문한다. 외부 링크: 인용된 링크: SS1입니다.\n' +
      '* November 2, 2019, pp. 2152-2161. Cited by: SS1.\n' +
      '* F. Liu, T. 관주 이림 천영 야쿱, D. 마노차, T. Zhou (2023)HallusionBench: 당신은 어떻게 생각하는지 보았나요? 아니면 네가 본 걸 생각하는 거야? gpt-4v(ision), llava-1.5 및 기타 다중 모달리티 모델에 도전하는 이미지 컨텍스트 추론 벤치마크. CoRRabs/2310.14566. External Links: Link Cited by: SS1.\n' +
      '* F. Liu, K. 린, L 이정왕 야쿱, L. Wang(2023) 강건한 명령어 튜닝으로 대형 멀티모달 모델을 정렬한다. CoRRabs/2306.14565. External Links: Link Cited by: SS1.\n' +
      '* F. Liu, K. 린, L 이정왕 야쿱, L. 왕(2023) 강력한 명령어 튜닝을 통해 대형 멀티모달 모델에서 환각을 완화한다. 외부 링크: 인용된 링크: SS1입니다.\n' +
      '* H. Liu, C. Li, Y. Li, 및 Y. J. Lee(2023)는 시각적 명령 튜닝으로 기준선을 개선했다. CoRRabs/2310.03744. External Links: Link Cited by: SS1.\n' +
      '*Y. 류현완 장병리 장원 조영 위안제왕 류경 첸과 D. 린(2023) 음벤치: 당신의 멀티모달 모델은 만능 선수입니까? CoRRabs/2307.06281. External Links: Link Cited by: SS1.\n' +
      '*Y. 류룡 진승 장승 Zhang(2017) 야생에서 곡선 텍스트 탐지: 새 데이터 세트 및 새 솔루션. CoRRabs/1712.02170. External Links: Link Cited by: SS1.\n' +
      '*Z. 류필루 추익 왕, X 탕(2016) 딥패션: 풍부한 주석으로 강력한 옷 인식 및 검색에 힘을 실어줍니다. 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 1096-1104. External Links: Link Cited by: SS1.\n' +
      '*Y. Peng Loh and C. S. Chan(2019)은 독점적으로 어두운 데이터셋으로 저조도 이미지를 알고 있습니다. 컴퓨터 비지니스 Image Underst.178, pp.30-42. External Links: Link Cited by: SS1.\n' +
      '* V. Lomonaco and D. Maltoni (2017)Core50: 새로운 데이터셋 및 연속 객체 인식을 위한 벤치마크. In 1st Annual Conference on Robot Learning, CoRL 2017, Mountain View, California, USA, November 13-15, 2017, Proceedings, Vol. 78, pp. 17-26. External Links: Cited by: SS1.\n' +
      '* P. Lu, R. 공성호 장룡 추승 황철 양, S. Zhu(2021)Intergrp: 형식적 언어와 상징적 추론으로 해석할 수 있는 기하학 문제 해결. 제59회 컴퓨터 언어학 협회 연례 회의에서, SS1이 인용했다.\n' +
      '* P. Lu, L. 주진태 시아영 조원 장장 유진 양, S. Zhu(2021)Iconqa: 추상적 다이어그램 이해와 시각적 언어 추론의 새로운 벤치마크. The Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Cited by: SS1.\n' +
      '*Y. Lu, C. Li, H. Liu, J. Yang, J. Gao, 및 Y. Shen (2023) scaling에 대한 경험적 연구: 지시-조정된 대형 멀티모달 모형을 중심으로 ArXiv:2309.09958. External Links: Link Cited by: SS1.\n' +
      '* C. Lyu, M. 우림 왕욱 황병주 두성 시, 지 Tu(2023)Macaw-llm: 이미지, 오디오, 비디오 및 텍스트 통합을 갖는 멀티모달 언어 모델링. CoRRabs/2306.09093. External Links: Link Cited by: SS1.\n' +
      '* S. 마지, E. Rahtu, J. Kannala, M. B. Blaschko, 그리고 A. Vedaldi (2013)의 세립 시각 분류이다. 기술 보고서 외부 링크: 인용 링크: SS1.\n' +
      '* M. 말리노스키와 M. Fritz (2014) 불확실한 입력에 기초하여 실제 세계 장면에 대한 질문에 답하는 멀티 월드 접근법. In Advances in Neural Information Processing Systems 27, pp. 1682-1690. External Links: Link, Document Cited by: SS1.\n' +
      '*K. 마리노 라스테가리, A. 파하디, R. 모타기(2019)Ok-vqa: 외부 지식이 필요한 시각적 질문 응답 벤치마크. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pp. 3195-3204. External Links: Link, Document Cited by: SS1.\n' +
      '* M. 매튜, V. R. 바갈 Tito, D. Karatzas, E. Valveny, and C. V. Jawahar (2022)Infographicvqa. IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022, Waikoloa, HI, USA, January 3-8, 2022, pp.\n' +
      '\n' +
      'Minesh Mathew, Dimosthenis Karatzas, R. 만마사, 그리고 C. V. 자와하르 2020. Docvqa: 문서 이미지 상의 VQA에 대한 데이터셋 _ CoRR_, abs/2007.00398.\n' +
      '* Mathews et al. (2016) Alexander Patrick Mathews, Lexing Xie, and Xuming He. 2016. Senticap: 감성으로 이미지 묘사를 생성하는 단계. [Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, 2월 12-17, Phoenix, Arizona, USA_, pages 3574-3580. AAAI Press.\n' +
      '* Methani et al. (2020) Nitesh Methani, Pritha Ganguly, Mitesh M. 카프라, 프라츄시 쿠마르 2020. Plotqa: 과학적 플롯에 대한 추론. IEEE Winter Conference on Applications of Computer Vision, WACV 2020, Snowmass Village, CO, USA, March 1-5, 2020_, pages 1516-1525. IEEE.\n' +
      '* Mishra et al. (2019) Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. 2019. Ocr-vqa: 이미지 내의 텍스트를 판독하여 시각적 질문 응답. _ICDAR_에서.\n' +
      '* Mostafazadeh et al. (2016) Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He, and Lucy Vanderwende. 2016. 이미지에 대한 자연스러운 질문 생성.\n' +
      '* Olsen et al. (2018) Alex Olsen, Dmitry A. Konovalov, Bronson Philippa, Peter Ridd, Jake C. Wood, Jamie Johns, Wesley Banks, Benjamin Girgenti, Owen Kenny, James Whinney, Brendan Calvert, Mostafa Rahimi Azghadi, and Ronald D. White. 2018. 딥위드: 딥러닝을 위한 다등급 잡초종 이미지 데이터셋. _ CoRR_, abs/1810.05726.\n' +
      '* Peng et al. (2019) Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. 2019. Moment matching for multi-source domain adaptation. _Proceedings of the IEEE International Conference on Computer Vision_, pages 1406-1415.\n' +
      '* Peng et al. (2017) Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. 2017. Visda: visual domain adaptation challenge. _ CoRR_, abs/1710.06924.\n' +
      '* Plummer et al. (2017) Bryan A. Plummer, Liwei Wang, Christopher M. 세르반테스, 후안 C. 카이스토, 줄리아 호켄마에, 스베틀라나 라제브닉 2017. Flickr30k entity: 보다 풍부한 이미지-문장 모델에 대한 영역-문장 대응 관계를 수집. _ IJCV_, 123(1):74-93.\n' +
      '* Pont-Tuset et al. (2020) Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. 2020. 비전과 언어를 지역화된 내러티브와 연결. _ECCV_에서.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. IMT-2000 3GPP-기계학습에 관한 국제학술대회 - 페이지 8748-8763. PMLR.\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. 잠재 확산 모델을 이용한 고해상도 영상 합성. _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 10674-10685. IEEE.\n' +
      '* ECCV 2010, 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV_, vol 6314 of _Lecture Notes in Computer Science_, pages 213-226. Springer.\n' +
      '* Sagonas et al. (2016) Christos Sagonas, Epameinondas Antonakos, Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic. 2016. 300 face in the-the-wild challenge: database and results. _ 이미지 Vis. Comput._ 47:3-18.\n' +
      '* Sakaridis et al. (2019) Christos Sakaridis, Dengxin Dai, and Luc Van Gool. 2019. 시맨틱 야간 영상 분할을 위한 안내된 교육과정 모델 적응 및 불확실성 인식 평가. IEEE/CVF International Conference on Computer Vision_의 _Proceedings, pages 7374-7383.\n' +
      '* 2022년 12월 9일_.\n' +
      '* ECCV 2022\n' +
      '- 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII_, volume 13668 of _Lecture Notes in Computer Science_, pages 146-162. Springer.\n' +
      '* 2019년 2월 1일_, 페이지 8876-8884. AAAI Press.\n' +
      '* Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual 캡션: A cleaned, hypermymed, image alt-text dataset for automatic image captioning. 제56회 연차보고서\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      'Zhiyang Xu, Ying Shen, Lifu Huang. 2023. Multi-Instruct: 명령어 튜닝을 통한 멀티모달 제로샷 학습 개선. _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 11445-11465, Canada, Toronto. 컴퓨터 언어학과의 연관성\n' +
      '* Yang et al. (2021) Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar, and Chris Callison-Burch. 2021. 위키호를 이용한 시각적 목표-단계 추론. 페이지 2167-2179.\n' +
      '* Yao et al. (2023) Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. 2023. End-to-End 멀티모달 팩트 체킹 및 설명 생성: 도전적인 데이터셋 및 모델. _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023_, pages 2733-2743. ACM.\n' +
      '* Ye et al. (2023) Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. 2023. mplug-owl: Modularization empowers large language models with multimodality. _ CoRR_, abs/2304.14178.\n' +
      '* Yin et al. (2023) Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, Jing Shao, and Wanli Ouyang. 2023. LAMM: 언어 지원 멀티모달 명령어-튜닝 데이터세트, 프레임워크 및 벤치마크_ CoRR_, abs/2306.06687.\n' +
      '* Yu et al. (2015) Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. 2015. LSUN: Loop에서 인간과 딥러닝을 이용한 대규모 이미지 데이터셋 구축. _ CoRR_, abs/1506.03365.\n' +
      '* Yu et al. (2023) Weihao Yu, Zhen규안 Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. Mm-vet: 통합 능력에 대한 대형 멀티모달 모델 평가 _ CoRR_, abs/2308.02490.\n' +
      '* Yue et al. (2023) Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Reniliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, 및 Wenhu Chen. 2023. MMMU: 전문가 AGI를 위한 대규모 다중 훈련 멀티모달 이해 및 추론 벤치마크.\n' +
      '* Zhai et al. (2023) Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. 2023. 멀티모달 대형 언어 모델에서의 치명적인 망각 조사. _ CoRR_, abs/2309.10313.\n' +
      '*Zhang et al. (2019) Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. 2019. Raven: 관계적 및 유추적 시각적 추론을 위한 데이터셋. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_.\n' +
      '* Zhang et al. (2023) Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. 2023. 라바: 텍스트가 풍부한 이미지 이해를 위한 향상된 시각적 명령어 튜닝. _ CoRR_, abs/2306.17107.\n' +
      '* Zhao et al. (2019) Bo Zhao, Yanwei Fu, Rui Liang, Jiahong Wu, Yonggang Wang, and Yizhou Wang. 2019. Zero-shot 학습을 위한 대규모 속성 데이터셋. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_, pages 0-0.\n' +
      '* Zhao et al. (2023) Bo Zhao, Boya Wu, and Tiejun Huang. 2023. SVIT : 스케일링 업 비주얼 명령어 튜닝 _ CoRR_, abs/2307.04087.\n' +
      '* Zhou et al. (2018) Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. 2018. 장소: 장면 인식을 위한 1,000만 영상 데이터베이스_ IEEE Trans. Pattern Anal. 마흐 Intell._ , 40(6):1452-1464.\n' +
      '* Zhou et al. (2014) Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. 2014. 장소 데이터베이스를 이용하여 장면 인식을 위한 심층 특징을 학습한다. 487-495 페이지\n' +
      '* Zhou et al. (2023) Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2023. 대형 시각 언어 모델에서 객체 환각을 분석하고 완화한다.\n' +
      '* Zhou and Shimada(2021) Yutong Zhou and Nobutaka Shimada. 2021. 사전 훈련된 BERT 모델을 이용한 텍스트-대-면 합성 및 조작을 위한 생성적 적대 네트워크. _16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021, Jodhpur, India, December 15-18, 2021_, pages 1-8. IEEE.\n' +
      '* Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: 고급 대형 언어 모델로 비젼-언어 이해력 향상. _ CoRR_, abs/2304.10592.\n' +
      '* Zhu et al.(2016) Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. 2016. Visual7w: Grounded question answering in images. IEEE conference on computer vision and pattern recognition_의 _Proceedings, pages 4995-5004.\n' +
      '\n' +
      '## Vision-Flan의 Annotation 과정에 대한 자세한 설명\n' +
      '\n' +
      '### Annotator Selection\n' +
      '\n' +
      '주석 작업의 복잡성으로 인해 우리는 자격을 갖춘 주석자를 선택하기 위한 선택 프로세스를 신중하게 설계한다. 특히, 처음에는 저자들이 NLP와 멀티모달 학습에 관심이 있는 컴퓨터 과학 대학원생을 찾는 이메일을 보낸다. 21명의 컴퓨터 과학 대학원생 그룹이 튜토리얼 섹션에 등록했다. 튜토리얼 섹션에서는 NLP의 두 박사 학생이 데이터 세트를 다운로드하고 데이터 세트를 통일된 형식으로 처리하는 명령어 작성 요구 사항을 설명한다. 튜토리얼 후 각 후보에는 3개의 데이터 세트가 할당되며 원시 데이터 세트를 처리하고 지침을 작성하는 데 총 3일이 소요된다. 결국 각 후보자는 자신의 주석을 제출하고 두 명의 박사학위 학생이 각 후보자에게 피드백을 제공한다. 그런 다음 후보자들은 피드백에 따라 지침이나 형식을 수정하는 데 이틀이 걸립니다. 이틀 후, 후보자들은 최종 버전의 주석을 제출하고 두 명의 박사 학위 학생들이 사례별로 주석의 품질에 대해 논의한다. 결국 21명의 학생 중 7명이 자격 있는 주석자로 선정되었다. 보상은 시간당 15달러입니다.\n' +
      '\n' +
      '## 부록 B 평가 데이터 세트\n' +
      '\n' +
      '우리는 널리 사용되는 여러 멀티모달 평가 벤치마크 데이터 세트에 대해 모델을 평가한다: (1) **MMbench**Liu et al. (2023)은 20차원에서 VLM의 능력을 측정하는 포괄적인 평가 벤치마크이다. (2)**MME**Fu et al. (2023)은 14개의 다양한 태스크를 기반으로 VLM의 인식 및 인식 능력을 측정한다. (3) **MM-Vet**Yu 등(2023)은 OCR, 인식, 지식, 공간 인식, 수학, 언어 생성을 포함한 VLM의 다양한 능력의 통합을 측정하는 데 중점을 둔다. (4) **LLaVA-Bench**Liu et al. (2023)은 다양한 일상 생활 시각 작업에서 VLM의 명령어 팔로우 및 채팅 능력을 평가한다. (5) **POPE**Li 등(2023)은 VLM에서 물체 환각을 조사하는 평가 벤치마크이다. (6) **MMMU**Yue et al. (2023)은 대학 수준의 교과 지식과 의도적인 추론이 필요한 다학문 과제에 대해 VLM을 평가한다.\n' +
      '\n' +
      '또한, 4개의 데이터세트: **CIFAR-10 및 CIFAR-100**Krizhevsky et al. (2009), **MNIST**LeCun (1998) 및 **miniImageNet**Vinyals et al. (2016)에 대해 VLM의 새롭게 제안된 재난적 망각 문제 Zhai et al. (2023)을 평가한다. 표 2의 CF 열에 있는 4개의 벤치마크에 대한 VLM의 평균 성능을 보고한다.\n' +
      '\n' +
      '## 부록 C 평가 프로토콜\n' +
      '\n' +
      'MM-Bench, MME, MM-Vet, LLaVA-Bench, POPE 및 MMMU의 경우, 성능 평가를 위해 평가 코드4의 공식 구현을 사용한다. 구체적으로, MM-벤치와 MM-Vet의 평가 스크립트는 GPT-4 API를 호출하여 목표 출력이 주어진 예측의 정확성을 평가하고 이진 점수(0 또는 1)를 생성한다. 마찬가지로 LLaVA-Bench의 평가도 GPT-4를 활용하며, 평가 방법은 목표 출력 외에 영상의 상세 설명을 고려한다. 평가 결과는 정확성뿐만 아니라 예측의 인간 선호도를 나타내는 점수이다. MME와 POPE는 이진 분류 작업이며 예측과 목표 레이블 간의 문자열 매칭을 기반으로 평가한다.\n' +
      '\n' +
      '각주 4: [https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)\n' +
      '\n' +
      '[https://mbmech.opencompass.org.cn/leaderboard](https://mbmech.opencompass.org.cn/leaderboard)\n' +
      '\n' +
      '[https://github.com/yuweihao/MM-Vet](https://github.com/yuweihao/MM-Vet)\n' +
      '\n' +
      '[https://github.com/haotian-liu/LLaVA/blob/](https://github.com/haotian-liu/LLaVA/blob/)\n' +
      '\n' +
      '## 부록 D 기준\n' +
      '\n' +
      '우리는 우리의 방법을 최근의 비전 언어 모델과 비교한다. 아래 나열된 모든 기준선은 사전 훈련된 LLM, 사전 훈련된 이미지 인코더 및 이를 연결하는 브리징 모듈로 구성된 유사한 아키텍처를 가지고 있다. **BLIP-2**Li 등(2023)은 Q-전자를 활용하여 사전 훈련된 LLM과 함께 사전 훈련된 이미지 인코더를 브릿지하고, 강한 제로-샷 능력을 달성한다. **Instruct-BLIP**Dai et al.(2023)은 visual-instruction-tuned BLIP-2 Li et al.(2023) 모델이다. 명령어 튜닝 데이터세트는 13개의 학술 데이터세트와 LLaVA Liu et al.(2023) 데이터세트를 혼합한 것이다. **Shikra**Chen et al. (2023)은 객체 접지 능력에 더 초점을 맞추고 지시 대화 데이터세트 및 LLaVA 데이터세트 Liu et al. (2023)에 의해 조정되며, 이들 모두는 GPT-4를 통해 합성적으로 생성된다. **LLaVA**Liu et al. (2023)은 GPT-4 합성 시각적 명령 조정 데이터세트 상에서 최초의 VLM 피니튜닝되고 범용 시각적 챗봇으로서 현저한 성능을 달성한다. **Qwen-VL** 및 **Qwen-VL-Chat**Bai 등(2023)은 최근 Qwen Bai 등(2023) 언어 모델에 기초하여 제안된 VLM이며 대규모(5천만 인스턴스) 개인 시각적 명령어 튜닝 데이터세트에 대해 트레이닝된다. **LLaVA-1.5**Liu 등(2023)은 shareGPT 5, LLaVA Liu 등(2023) 및 8개의 학술 이미지-텍스트 데이터세트의 혼합물에 대해 트레이닝된 LLaVA 모델이다.\n' +
      '\n' +
      '## 부록 추가 결과\n' +
      '\n' +
      'GPT-4 합성 데이터가 종합평가 벤치마크에 미치는 영향\n' +
      '\n' +
      'Vision-Flan에서 훈련된 VLMs가 왜 최첨단 VLMs보다 더 나은가?\n' +
      '\n' +
      '이 섹션에서는 Vision-Flan에서 훈련된 모델이 최첨단 VLM에 비해 더 나은 성능을 발휘할 수 있는 이유를 설명하기 위해 두 가지 사례 연구를 수행한다.\n' +
      '\n' +
      'OCR에 관한 사례연구\n' +
      '\n' +
      'Vision-Flan Chat의 예측을 수동으로 확인하고 다른 VLM과 비교할 때, 우리가 관찰하는 첫 번째 경향은 Vision-Flan Chat이 그림 10과 같이 OCR을 더 잘 수행할 수 있다는 것이다. 이러한 관찰을 정량화하기 위해 도전적인 TextOCR 데이터셋(Singh et al., 2021)에서 LLaVA, LLaVA 1.5 및 우리의 모델을 평가한다. VLM에 각 이미지의 모든 텍스트를 예측하고 텍스트 조각의 대상 목록과 예측 사이의 중첩을 확인하도록 요청한다. 그림 9와 같이 Vision-Flan Base와 Vision-Flan Chat의 리콜은 LLaVA 1.5에 비해 훨씬 높은 반면 응답당 예측 토큰의 평균 수는 비슷하다.\n' +
      '\n' +
      '개체 인식 사례 연구\n' +
      '\n' +
      '또한 Vision-Flan에서 학습된 모델이 이미지 내의 엔티티를 더 잘 식별할 수 있는 반면 LLaVA 1.5는 단순히 이미지 내의 엔티티의 출현을 캡션한다는 것을 발견했다. 질적인 예는 그림 11에 나와 있다.\n' +
      '\n' +
      '정량적 결과를 계산하기 위해, 우리는 WIT 데이터세트(Srinivasan et al., 2021)에서 1,000개의 이미지를 무작위로 샘플링하며, 여기서 이미지는 위키피디아 페이지에서 가져온 것이고 캡션에는 일반적으로 이미지에 나타나는 엔티티가 포함된다. 우리는 VLM에 엔티티를 도입하도록 촉구한다.\n' +
      '\n' +
      '그림 8: 종합 평가 벤치마크, 즉 MM-Bench에 GPT-4 합성 훈련 인스턴스의 수를 증가시키는 효과. 회색 점선은 최첨단 LLaVA 1.5 모델의 성능을 나타낸다.\n' +
      '\n' +
      '도 10: Vision-Flan이 VLM들이 텍스트를 더 잘 인식할 수 있게 한다는 것을 보여주기 위한 TextCap으로부터의 예시.\n' +
      '\n' +
      '도 9: TextOCR 상의 다양한 VLM의 성능. 회색 막대는 예측당 평균 토큰 수를 보여주고 주황색 선은 예측의 재현율을 보여준다.\n' +
      '\n' +
      '배경 지식이 있는 이미지 우리는 spaCy 6의 EntityRecognizer를 활용하여 예측 및 지상 진실 캡션 모두에서 개체를 인식하고 예측에 나타나는 대상 개체의 백분율을 계산합니다. 그림 12와 같이 Vision-Flan Base와 Vision-Flan Chat은 LLaVA 1.5에 비해 응답(회색 막대)에서 더 많은 개체를 예측하고 개체(주황색 선)의 커버리지가 더 높다는 것이 분명하다.\n' +
      '\n' +
      '각주 6: [https://spacy.io/api/entityrecognizer](https://spacy.io/api/entityrecognizer)\n' +
      '\n' +
      '## 부록 F 추가 분석\n' +
      '\n' +
      '### 브리지 모듈이 동일한 아키텍처를 가진 LLM에 걸쳐 공유될 수 있음\n' +
      '\n' +
      'LLMs들을 정렬하고 미세조정하는 최근의 연구들 Jain et al.(2023)은 정렬이 LLMs들로부터 출력들의 스타일 및 포맷을 변경하기 위해 몇 개의 가중치들 또는 뉴런들의 가지치기로 매우 국부적으로 발생함을 시사하며, LLMs들의 파라미터 공간을 실질적으로 변화시키지 않는다. 이 발견에 따라, 우리는 시각적 특징을 LLM의 임베딩 공간에 매핑하는 MLP 레이어가 동일한 아키텍처를 가진 LLM에 걸쳐 공유될 수 있지만 서로 다른 텍스트 정렬 데이터세트_에서 조정된다고 가정한다. 표 7과 같이 Vision-Flan에서 미세 조정되지만 냉동 보관된 LLM을 사례 연구로 사용하는 Vision-Flan Base w/냉동 LLM을 포함한 4가지 모델을 선택하고 LLM(Vicuna v1.5)을 기성 LLaMA 2 Chat 모델로 직접 교체한다. 추론 과정에서 Vicuna v1.5 대신 LLaMA 2 채팅의 공식 프롬프트 템플릿을 사용했으며, 결과는 MLP가 동일한 아키텍처를 가진 LLM 간에 공유될 수 있지만 다른 정렬 데이터 세트에 대해 훈련될 수 있음을 보여준다. 흥미로운 관찰은 우리가 LLaMA 2 채팅에서 교체한 후 LLaVA-벤치에 상당한 성능 향상이 있다는 것이다. Vision-Flan Base와 Vision-Flan 채팅에서 MLP와 LLM을 모두 세밀하게 조정하면 LLaMA 2 채팅에서 스왑할 때 놀라운 성능 저하를 관찰할 수 있다. 이는 LLaMA 2 채팅이 시각적 명령어 조정 Vicuna v1.5에 비해 시각적 특징을 효과적으로 해석할 수 없기 때문에 이해할 수 있다.\n' +
      '\n' +
      '평가 벤치마크 간의 불일치\n' +
      '\n' +
      '표 2 및 7에서 우리는 여러 모델에서 객관식 벤치마크(예: MME 및 MM-벤치)와 LLaVA-벤치 간의 큰 성능 불일치를 식별한다. 구체적으로, 표 2에서, LLaVA는 LLaVA 1.5의 성능 레벨에 필적하는, LLaVA-벤치 상에서 70.8의 점수를 달성한다. 대조적으로, MME 및 MM-벤치 상에서 LLaVA의 성능은 LLaVA 1.5에 비해 각각 1151.6 및 38.7로 1531.3 및 66.7로 현저하게 낮다. 더 나아가, 이러한 경향은 표 7에서도 명백하다. MME 및 MM-벤치 상에서 LLaVA의 성능을 LLaVA 1.5에 대체하면, MME 및 MM-벤치 상에서 LLaVA의 성능은 LLaVA 1.5에 비해 각각 1151.6 및 38.7로 현저하게 낮다.\n' +
      '\n' +
      '도 11: Vision-Flan이 VLM들이 엔티티들을 더 잘 인식할 수 있게 한다는 것을 보여주기 위한 MM-Vet으로부터의 예시.\n' +
      '\n' +
      '그림 12: 개체 인식에 대한 다양한 VLM의 성능. 회색 막대는 반응당 평균 개체 수를 나타내고 주황색 선은 예측에 나타나는 대상 반응의 개체 비율을 나타냅니다.\n' +
      '\n' +
      'Vision-Flan Base 및 Off-the-shelf LLaMA 2 Chat의 LLM은 두 모델 모두 MME 및 MM-Bench에서 현저한 성능 감소를 나타내는 반면 LLaVA-Bench에서는 유사한 성능을 유지한다. 우리의 가설은 LLaVA-벤치가 시각적 특징에 대한 LLM의 강한 이해를 필요로 하지 않고, 오히려 LLM의 언어-우선 순위에 의존한다고 가정한다(Lin et al., 2023). 또한 GPT-4에 의해 합성된 데이터는 평가 메트릭, 즉 GPT-4 자체의 선호도와 일치하여 긴 형태의 응답을 생성하는 모델의 능력을 촉진한다.\n' +
      '\n' +
      '## 부록 G 추가 관련 업무\n' +
      '\n' +
      'Vision-Language Models. 이전 작업들(Li et al., 2019; Chen et al., 2020; Tan and Bansal, 2019; Su et al., 2020; Wang et al., 2023)은 주로 MMLM(Unified masked-language modeling) 목적(Devlin et al., 2019)으로 비젼-언어 모델들(VLMs)을 처음부터 사전 훈련하는데, 이는 상당한 훈련 비용과 열등한 성능을 부과할 수 있다. 최근 한 라인에서는 LLM의 임베딩 공간에 시각적 특징을 매핑하는 소량의 브리징 파라미터를 도입하여 기성 비주얼 인코더와 LLM으로부터 VLM을 구축하는 것을 제안한다. 플라밍고(Alayrac et al., 2022)는 인터리빙된 이미지-텍스트 입력을 처리하고 시각적 콘텐츠에 기초하여 응답을 생성할 수 있는 VLM을 제시한다. 본 논문에서는 냉동 LLM과 비주얼 인코더를 연결하기 위한 브리지 모듈로 Perceiver Resampler를 제안한다. OFA(Wang et al., 2022)는 이미지를 이산 비주얼 토큰에 매핑하고 이산 비주얼 토큰을 LLM에 공급하는 시퀀스 투 시퀀스 학습 프레임워크를 제안한다. BLIP-2(Li 등, 2023)는 미리 훈련되고 동결된 시각 및 언어 모델들을 브릿지하기 위해 Q-전이를 도입하고, 이에 기초하여 MiniGPT-4(Zhu 등, 2023)는 시각 인코더와 언어 모델 인코더 사이의 갭을 브릿지하기 위해 선형 프로젝터를 추가로 추가한다. LLaVA(Liu et al., 2023)는 시각적 정보를 큰 언어 모델에 융합하고 시각적 명령어 튜닝 동안 언어 모델을 동결해제하기 위해 프로젝터를 도입한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Model** & MM-Bench & MME & LLaVA-Bench & Pope \\\\ \\hline\n' +
      '**Pretrained LLaVA-Architecture** & 45.0 & 936.3 & 32.4 & 51.9 \\\\ \\hline + LLaMA 2 Chat & 45.3 (100.6) & 557.0 (59.5) & 59.2 (182.7) & 66.9 (128.9) \\\\ \\hline Vision-Flan Base w/ frozen LLM & 52.4 & 1107.3 & 41.6 & 83.3 \\\\ \\hline + LLaMA 2 Chat & 46.6 (88.9) & 1095.8 (99.0) & 56.4 (135.6) & 80.9 (97.1) \\\\ \\hline Vision-Flan Base & 69.8 & 1537.8 & 38.5 & 85.9 \\\\ \\hline + LLaMA 2 Chat & 47.2 (67.6) & 852.6 (55.4) & 69.9 (181.6) & 66.1 (76.9) \\\\ \\hline Vision-Flan Chat & 67.6 & 1490.6 & 78.3 & 86.1 \\\\ \\hline + LLaMA 2 Chat & 47.0 (69.5) & 869.6 (59.3) & 74.6 (95.3) & 65.8 (76.4) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 4개의 VLM에서 Vicuna 1.5를 LLaMA 2 채팅으로 교체한 결과. 회색 행은 원래 모델의 성능을 나타내고 파란색 행은 LLM을 교체한 후 VLM의 성능을 나타낸다. 각 괄호의 숫자는 원래 성능과 비교하여 LLaMA 2 Chat의 통합 후 VLM의 성능 백분율을 나타낸다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:21]\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Dataset \\& Reference & Tasks \\\\ \\hline \\hline CINIC-10 (Darlow et al., 2018) & 1. animal recognition in low resolution image \\\\  & 2. shipping method recognition in low resolution image \\\\  & 3. transportation option recognition in low resolution image \\\\  & 4. animal presence classification in low resolution \\\\  &  image \\\\  & 5. object shipping object presence in low resolution image \\\\ \\hline MSCOCO (Lin et al., 2014) & 1. multiple choice VQA \\\\  & 2. short image captioning \\\\  & 3. appliance recognition \\\\  & 4. furniture recognition \\\\  & 5. kitchen object recognition \\\\  & 6. vehicle recognition \\\\  & 7. animal recognition \\\\  & 8. sports object recognition \\\\  & 9. image text matching \\\\  & 10. image text selection \\\\ \\hline FairFace (Karkkainen and Joo, 2021) & 1. human age classification \\\\  & 2. human gender classification \\\\  & 3. human race classification \\\\ \\hline IconQA (Lu et al., 2021b) & 1. abstract diagram understanding \\\\  & 2. fill in blank in abstract diagram understanding \\\\ \\hline ImageNet-A (Hendrycks et al., 2021b) & 1. object recognition of natural adversarial examples \\\\ \\hline ImageNet-C (Hendrycks and Dietterich, 2019) & 1. blur type classification \\\\  & 2. coarse-grained image corruption classification \\\\  & 3. weather type classification \\\\  & 4. fine-grained image corruption classification \\\\ \\hline InfographicVQA (Mathew et al., 2022) & 1. VQA \\\\  & 2. document level VQA \\\\ \\hline SemArt (Garcia and Vogiatzis, 2018) & 1. painting time frame recognition \\\\  & 2. painting type recognition \\\\  & 3. painting school recognition \\\\  & 4. painting technique recognition \\\\  & 5. detailed image description \\\\ \\hline Set5 (Bevilacqua et al., 2012) & 1. object recognition in low resolution image \\\\ \\hline TextCaps (Sidorov et al., 2020) & 1. image captioning with reading comprehension \\\\ \\hline VisDial (Das et al., 2019) & 1. visual dialogue with short context \\\\  & 2. visual dialogue with medium context \\\\  & 3. visual dialogue with long context \\\\  & 4. visual dialogue with very long context \\\\ \\hline STL-10 (Coates et al., 2011) & 1. object recognition \\\\ \\hline Places365 (Zhou et al., 2018) & 1. scene classification \\\\ \\hline Office-31 (Saenko et al., 2010) & 1. image domain and office object classification \\\\  & 2. office object recognition \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Dataset \\& Reference & Tasks \\\\ \\hline \\hline LSUN (Yu et al., 2015) & 1. scene classification \\\\ \\hline FGVC-Aircraft (Maji et al., 2013) & 1. aircraft family classification \\\\  & 2. aircraft manufacturer classification \\\\  & 3. aircraft variant classification \\\\  & 4. aircraft model classification \\\\ \\hline DeepFashion (Liu et al., 2016) & 1. cloth texture classification \\\\ \\hline CUB-200-2011 (Wah et al., 2011) & 1. bird species recognition \\\\ \\hline CLEVR (Johnson et al., 2017) & 1. VQA in 3D rendered images \\\\  & 2. question answer matching \\\\  & 3. visual dialogue in 3D rendered images \\\\  & 4. VQA in 3D rendered images with multiple questions \\\\ \\hline CLEVR-CoGenT (Johnson et al., 2017) & 1. VQA in 3D rendered images \\\\  & 2. question answer matching \\\\  & 3. VQA in 3D rendered images with multiple questions \\\\ \\hline A-OKVQA (Schwenk et al., 2022) & 1. rationales generation \\\\  & 2. answer rationale generation \\\\  & 3. outside knowledge VQA \\\\ \\hline AI2D (Kembhavi et al., 2016) & 1. diagram VQA \\\\ \\hline AID (Xia et al., 2017) & 1. aerial scene classification \\\\ \\hline Caltech-256 (Griffin et al., 2007) & 1. object recognition \\\\ \\hline CoVA (Kumar et al., 2022) & 1. webpage recognition \\\\ \\hline DeepWeeds (Olsen et al., 2018) & 1. weed species recognition \\\\ \\hline ExDark (Loh and Chan, 2019) & 1. object recognition in low light environments \\\\ \\hline FFHQ-Text (Zhou and Shimada, 2021) & 1. facial attribute textual descriptions generation \\\\ \\hline FlickrLogos-27 (Kalantidis et al., 2011) & 1. logo recognition \\\\ \\hline FoodLogoDet-1500 (Hou et al., 2021) & 1. food logo recognition \\\\ \\hline ImageNet-R (Hendrycks et al., 2021) & 1. object recognition in diverse image domain \\\\  & 2. image style classification \\\\ \\hline ImageNet-Sketch (Wang et al., 2019) & 1. object recognition in sketch \\\\ \\hline JHU-CROWD++ (Sindagi et al., 2019) & 1. scene classification \\\\ \\hline MNIST-M (Ganin et al., 2017) & 1. number recognition \\\\ \\hline MVTecAD (Bergmann et al., 2021) & 1. object anomaly detection \\\\  & 2. industrial item recognition \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Dataset \\& Reference & Tasks \\\\ \\hline \\hline NABirds (Horn et al., 2015) & 1. bird species recognition in north America \\\\  & 2. bird body parts detection \\\\ \\hline Road-Anomaly (Lis et al., 2019) & 1. road anomaly detection \\\\ \\hline SCUT-CTW1500 (Liu et al., 2017) & 1. curve text detection in the wild \\\\ \\hline Total-Text (Chng et al., 2020) & 1. scene text detection and recognition \\\\ \\hline VisDA-2017 (Peng et al., 2017) & 1. object recognition in 3D rendered image \\\\  & 2. multiple choice object recognition in 3D rendered image \\\\ \\hline Yoga-82 (Verma et al., 2020) & 1. yoga pose recognition \\\\ \\hline Caltech101 (Fei-Fei et al., 2004) & 1. object recognition \\\\  & 2. living organism classification \\\\ \\hline Cars (Krause et al., 2013) & 1. car brand maker and year classification \\\\  & 2. car brand classification \\\\ \\hline Core50 (Lomonaco and Maltoni, 2017) & 1. object recognition \\\\ \\hline NUS-WIDE (Chua et al., 2009) & 1. animal presence classification \\\\ \\hline ObjectNet (Barbu et al., 2019) & 1. object recognition \\\\ \\hline Places205 (Zhou et al., 2014) & 1. indoor outdoor classification \\\\ \\hline\n' +
      '300w (Sagonas et al., 2016) & 1. indoor outdoor classification \\\\ \\hline Yahoo (Farhadi et al., 2009) & 1. object recognition \\\\ \\hline LFW (Huang et al., 2007) & 1. celebrity recognition \\\\ \\hline model-vs-human (Geirhos et al., 2019) & 1. image-style classification \\\\ \\hline Office-Home (Venkateswara et al., 2017) & 1. object recognition \\\\ \\hline Winoground (Thrush et al., 2022) & 1. image caption matching \\\\ \\hline ConceptualCaptions (Sharma et al., 2018) & 1. conceptual image captioning \\\\ \\hline KVQA+image question answer (Shah et al., 2019) & 1. knowledge-aware VQA \\\\  & 2. visual entity recognition \\\\ \\hline MemeCap (Hwang and Shwartz, 2023) & 1. meme understanding \\\\ \\hline PlotQA (Methani et al., 2020) & 1. VQA over scientific plots \\\\ \\hline SentiCap (Mathews et al., 2016) & 1. image captioning conditioned on sentiment \\\\ \\hline VQA-E (Li et al., 2018) & 1. VQA \\\\  & 2. short image captioning \\\\ \\hline VQG (Mostafazadeh et al., 2016) & 1. visual question generation \\\\  & 2. short image captioning \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Dataset \\& Reference & Tasks \\\\ \\hline \\hline WIT (Srinivasan et al., 2021) & 1. background knowledge extraction \\\\ \\hline WikiArt (Tan et al., 2019) & 1. artist genre style recognition \\\\ \\hline VQA-RAD (Lau et al., 2019) & 1. VQA in radiology \\\\ \\hline VOC2007 (Everingham et al., 2010) & 1. multiple object recognition \\\\ \\hline VizWiz (Gurari et al., 2020) & 1. answering visual questions from blind people \\\\  & 2. captioning image taken by blind people \\\\  & 3. quality issue classification of image taken by blind people \\\\ \\hline ViQuAE (Lerner et al., 2022) & 1. knowledge based VQA about entities \\\\ \\hline ST-VQA (Biten et al., 2019) & 1. scene text VQA \\\\ \\hline Stanford Dogs (Khosla et al., 2011) & 1. dog species classification \\\\ \\hline Sketch (Eitz et al., 2012) & 1. living organism classification in sketch \\\\  & 2. object recognition in sketch \\\\ \\hline RAVEN (Zhang et al., 2019) & 1. relational and analogical visual reasoning \\\\ \\hline PICKAPIC (Kirstain et al., 2023) & 1. image prompt generation \\\\ \\hline PACS (Li et al., 2017) & 1. object recognition in art painting \\\\  & 2. object recognition in cartoon \\\\  & 3. object recognition in photograph \\\\  & 4. dog image style classification \\\\  & 5. elephant image style classification \\\\  & 6. giraffe image style classification \\\\  & 7. guitar image style classification \\\\  & 8. horse image style classification \\\\  & 9. house image style classification \\\\  & 10. person image style classification \\\\ \\hline NOCAPS (Agrawal et al., 2019) & 1. multiple short captions generation \\\\ \\hline Localized Narratives (Pont-Tuset et al., 2020) & 1. COCO detailed image captioning \\\\  & 2. flickr30k detailed image captioning \\\\  & 3. open images detailed image captioning \\\\  & 4. ade20k detailed image captioning \\\\ \\hline INATURALIST (Horn et al., 2018) & 1. class classification \\\\  & 2. family classification \\\\  & 3. genus classification \\\\  & 4. Latin English name classification \\\\  & 5. order classification \\\\  & 6. phylum classification \\\\  & 7. supercategory classification \\\\ \\hline HICO (Chao et al., 2015) & 1. human activity detection \\\\ \\hline GEOMETRY3K (Lu et al., 2021a) & 1. geometry question answering \\\\ \\hline FUNSD (Guillaume Jaume, 2019) & 1. text detection in noisy scanned documents \\\\ \\hline FLICKR30K (Plummer et al., 2017) & 1. multiple captions generation \\\\ \\hline DVQA (Kafle et al., 2018) & 1. chart question answering \\\\ \\hline DTD (Cimpoi et al., 2014) & 1. coarse grained texture classification \\\\  & 2. multiple texture detection \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Dataset \\& Reference & Tasks \\\\ \\hline \\hline DOMAIN NET (Peng et al., 2019) & 1. object recognition in clip art \\\\  & 2. object recognition in infograph \\\\  & 3. object recognition in painting \\\\  & 4. object recognition in quickdraw \\\\  & 5. object recognition in real image \\\\  & 6. image style classification \\\\ \\hline DOCVQA (Mathew et al., 2020) & 1. document level VQA \\\\ \\hline DAQUAR (Malinowski and Fritz, 2014) & 1. VQA \\\\ \\hline CONCADIA (Kreiss et al., 2022) & 1. caption with background knowledge \\\\  & 2. short image captioning \\\\ \\hline Visual7W (Zhu et al., 2016) & 1. VQA object attribute \\\\ \\hline VQAv2 (Goyal et al., 2017) & 1. general VQA \\\\  & 2. question image matching \\\\ \\hline Visual Genome(Krishna et al., 2017) & 1. spatial relationship question answering \\\\ \\hline OK-VQA(Marino et al., 2019) & 1. outside knowledge VQA \\\\ \\hline ScienceQA (Lu et al., 2022) & 1. VQA \\\\  & 2. explanation generation \\\\ \\hline OCR-VQA (Mishra et al., 2019) & 1. VQA by reading text in image \\\\ \\hline wikiHow-image (Yang et al., 2021) & 1. next step generation \\\\  & 2. image text step ordering \\\\  & 3. immediate next step selection \\\\  & 4. text image step ordering \\\\ \\hline SciCap (Hsu et al., 2021) & 1. figure captioning \\\\ \\hline LAD (Zhao et al., 2019) & 1. detailed object description generation \\\\ \\hline Dark Zurich (Sakaridis et al., 2019) & 1. time of the day classification \\\\ \\hline RAF-DB (Li and Deng, 2019) & 1. human emotion detection \\\\ \\hline GQA (Hudson and Manning, 2019) & 1. spatial relationship question answering \\\\ \\hline VQA (Antol et al., 2015) & 1. color \\\\  & 2. activity recognition \\\\  & 3. counting \\\\  & 4. object presence \\\\  & 5. object recognition \\\\  & 6. positional reasoning \\\\  & 7. scene recognition \\\\  & 8. sentiment understanding \\\\  & 9. sport recognition \\\\  & 10. utility affordance \\\\ \\hline Multimodal Factual Checking (Yao et al., 2023) & 1. multimodal factual checking \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '## Part I: Vision-Flam의 과제 분류\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Category & Tasks \\\\ \\hline \\hline Perception & 1. CLEVR-CoGenT VQA in 3D rendered images \\\\  & 2. CLEVR-CoGenT question answer matching \\\\  & 3. CLEVR-CoGenT VQA in 3D rendered images \\\\  & with multiple questions \\\\  & 4. CLEVR VQA in 3D rendered images with multiple questions \\\\  & 5. GQA spatial relationship question answering \\\\  & 6. VQA color \\\\  & 7. VQA activity recognition \\\\  & 8. VQA counting \\\\  & 9. VQA object presence \\\\  & 10. VQA object recognition \\\\  & 11. VQA positional reasoning \\\\  & 12. VQA scene recognition \\\\  & 13. VQA sentiment understanding \\\\  & 14. VQA sport recognition \\\\  & 15. VQA utility affordance \\\\  & 16. VQA-E VQA \\\\  & 17. VQAv2 general VQA \\\\  & 18. Visual Genome spatial relationship question answering \\\\  & 19. CLEVR question answer matching \\\\  & 20. VizWiz answering visual questions from blind \\\\  & people \\\\  & 21. DAQUAR VQA \\\\  & 22. MSCOCO multiple choice VQA \\\\  & 23. Visual7W VQA object attribute \\\\  & 24. CLEVR VQA in 3D rendered images \\\\ \\hline Outside Knowledge & 1. KVQA knowledge aware VQA \\\\  & 2. VIQUAE knowledge based VQA about entities \\\\  & 3. VQARAD VQA in radiology \\\\  & 4. OK-VQA outside knowledge VQA \\\\  & 5. A-OKVQA outside knowledge VQA \\\\ \\hline Reasoning & 1. GEOMETRY3K geometry question answering \\\\  & 2. IconQA abstract diagram understanding \\\\  & 3. IconQA fill in blank in abstract diagram understanding \\\\  & 4. InfographicVQA VQA \\\\  & 5. InfographicVQA document level VQA \\\\  & 6. ScienceQA VQA \\\\  & 7. AI2D diagram VQA \\\\ \\hline OCR & 1. DOCVQA document level VQA \\\\  & 2. DVQA chart question answering \\\\  & 3. PlotQA VQA over scientific plots \\\\  & 4. OCR-VQA VQA by reading text in image \\\\  & 5. ST-VQA scene text VQA \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|p{113.8pt}|p{113.8pt}|} \\hline Category & Tasks \\\\ \\hline \\hline Document-Level OCR & 1. FUNSD text detection in noisy scanned documents \\\\  & 2. SCUT-CTW1500 curve text detection in the wild \\\\  & 3. Total-Text scene text detection and recognition \\\\ \\hline Phrase-Level OCR & 1. CoVA webpage recognition \\\\  & 2. FlickrLogos-27 logo recognition \\\\  & 3. FoodLogoDet-1500 food logo recognition \\\\ \\hline Knowledge Extraction & 1. CONCADIA caption with background knowledge \\\\  & edge \\\\  & 2. KVQA visual entity recognition \\\\  & 3. WIT background knowledge extraction \\\\ \\hline Semantic Art Understanding & 1. Semart painting time frame recognition \\\\  & 2. Semart painting type recognition \\\\  & 3. Semart painting school recognition \\\\  & 4. Semart painting technique recognition \\\\  & 5. Semart detailed image description \\\\  & 6. WikiArt artist genre style recognition \\\\ \\hline Visual Dialogue & 1. CLEVR visual dialogue in 3D rendered images \\\\  & 2. Visdial visual dialogue with short context \\\\  & 3. Visdial visual dialogue with medium context \\\\  & 4. Visdial visual dialogue with long context \\\\  & 5. Visdial visual dialogue with very long context \\\\ \\hline Rational and Script Generation & 1. ScienceQA explanation generation \\\\  & 2. A-OKVQA rationales generation \\\\  & 3. A-OKVQA answer rationale generation \\\\  & 4. MemeCap meme understanding \\\\  & 5. wikiHow-image next step generation \\\\  & 6. VQG visual question generation \\\\ \\hline Coarse-grained Captioning & 1. ConceptualCaptions conceptual image captioning \\\\  & 2. FLICKR30K multiple captions generation \\\\  & 3. NOCAPS multiple short captions generation \\\\  & 4. PICKAPIC image prompt generation \\\\  & 5. VizWiz captioning image taken by blind people \\\\  & 6. VQA-E short image captioning \\\\  & 7. VQG short image captioning \\\\  & 8. MSCOCO short image captioning \\\\  & 9. CONCADIA short image captioning \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:30]\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Category & Tasks \\\\ \\hline \\hline Vehicle Classification & 1. Cars car brand maker and year classification \\\\  & 2. Cars car brand classification \\\\  & 3. FGVC-Aircraft aircraft family classification \\\\  & 4. FGVC-Aircraft aircraft manufacturer classification \\\\  & 5. FGVC-Aircraft aircraft variant classification \\\\  & 6. FGVC-Aircraft aircraft model classification \\\\ \\hline Human Activity & 1. HICO human activity detection \\\\  & 2. RAF-DB human emotion detection \\\\  & 3. Yoga-82 yoga pose recognition \\\\ \\hline Facial Recognition & 1. LFW celebrity recognition \\\\  & 2. Fairface human age classification \\\\  & 3. Fairface human gender classification \\\\  & 4. Fairface human race classification \\\\ \\hline Anomaly Detection & 1. Road-Anomaly road anomaly detection \\\\  & 2. MVTecAD object anomaly detection \\\\ \\hline General Object & 1. Caltech-256 object recognition \\\\  & 2. Caltech101 object recognition \\\\  & 3. Caltech101 living organism classification \\\\  & 4. Core50 object recognition \\\\  & 5. ImageNet-A object recognition of natural adversarial examples \\\\  & 6. MNIST-M number recognition \\\\  & 7. MVTecAD industrial item recognition \\\\  & 8. ObjectNet object recognition \\\\  & 9. Office-Home object recognition \\\\  & 10. Office-31 image domain and office object classification \\\\  & 11. Office-31 office object recognition \\\\  & 12. STL-10 object recognition \\\\  & 13. Set5 object recognition in low resolution image \\\\  & 14. VOC2007 multiple object recognition \\\\  & 15. MSCOCO appliance recognition \\\\  & 16. MSCOCO furniture recognition \\\\  & 17. MSCOCO kitchen object recognition \\\\  & 18. MSCOCO vehicle recognition \\\\  & 19. MSCOCO animal recognition \\\\  & 20. MSCOCO sports object recognition \\\\  & 21. Yahoo object recognition \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|p{142.3pt}|p{142.3pt}|} \\hline Category & Tasks \\\\ \\hline \\hline Complex Reasoning & 1. RAVEN relational and analogical visual reasoning \\\\  & 2. Multimodal Factual Checking multimodal factual checking \\\\  & 3. wikiHow-image image text step ordering \\\\  & 4. wikiHow-image immediate next step selection \\\\  & 5. wikiHow-image text image step ordering \\\\ \\hline Image Text Matching & 1. MSCOCO image text matching \\\\  & 2. Winoground image caption matching \\\\  & 3. MSCOCO image text selection \\\\  & 4. MSCOCO question image matching \\\\ \\hline General Object Classification in Special Image Domain & 1. DOMAIN NET object recognition in clip art \\\\  & 2. DOMAIN NET object recognition in infograph \\\\  & 3. DOMAIN NET object recognition in painting \\\\  & 4. DOMAIN NET object recognition in quick-draw \\\\  & 5. DOMAIN NET object recognition in real image \\\\  & 6. ExDark object recognition in low light environments \\\\  & 7. ImageNet-R object recognition in diverse image domain \\\\  & 8. ImageNet-Sketch object recognition in sketch \\\\  & 9. PACS object recognition in art painting \\\\  & 10. PACS object recognition in cartoon \\\\  & 11. PACS object recognition in photograph \\\\  & 12. SKETCH living organism classification in sketch \\\\  & 13. SKETCH object recognition in sketch \\\\  & 14. Cinic-10 animal recognition in low resolution image \\\\  & 15. Cinic-10 shipping method recognition in low resolution image \\\\  & 16. Cinic-10 transportation option recognition in low resolution image \\\\  & 17. Cinic-10 animal presence classification in low resolution image \\\\  & 18. Cinic-10 object shipping object presence in low resolution image \\\\  & 19. VisDA-2017 object recognition in 3D rendered image \\\\  & 20. VisDA-2017 multiple choice object recognition in 3D rendered image \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Category & Tasks \\\\ \\hline \\hline Image-Style Classification & 1. DOMAIN-NET image style classification \\\\  & 2. ImageNet-R image style classification \\\\  & 3. PACS dog image style classification \\\\  & 4. PACS elephant image style classification \\\\  & 5. PACS giraffe image style classification \\\\  & 6. PACS guitar image style classification \\\\  & 7. PACS horse image style classification \\\\  & 8. PACS house image style classification \\\\  & 9. PACS person image style classification \\\\  & 10. Model-vs-human image style classification \\\\ \\hline Image Quality Classification & 1. ImageNet-C blur type classification \\\\  & 2. ImageNet-C coarse-grained image corruption \\\\  & classification \\\\  & 3. ImageNet-C weather type classification \\\\  & 4. ImageNet-C fine-grained image corruption \\\\  & classification \\\\  & 5. VizWiz quality issue classification of image \\\\  & taken by blind people \\\\ \\hline Texture Classification & 1. DTD coarse grained texture classification \\\\  & 2. DTD multiple texture detection \\\\  & 3. DeepFashion cloth texture classification \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      'Visco-FLAN Tasks\n' +
      '\n' +
      '모든 "봉쇄 작업들" 도 14.\n' +
      '\n' +
      'Figure 13:\n' +
      '\n' +
      'Figure 15:\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:36]\n' +
      '\n' +
      'Figure 18:\n' +
      '\n' +
      'Figure 19:\n' +
      '\n' +
      'Figure 20:\n' +
      '\n' +
      '**Task:**: **Visdial_visual_dialogue_with_long_context**\n' +
      '\n' +
      '**인스트럭션**: 여기 두 사람 사이의 대화가 있습니다. 그들은 그림에 대해 이야기하고 그것에 대한 몇 가지 질문과 대답을 논의하고 있다. 대화를 읽고 다음 사람으로 대답하세요.\n' +
      '\n' +
      '1번, 오토바이는 몇 대죠? 20명: 20명: 그것들은 무슨 색인가요? 2번 사람: 흑백 1번 사람: 몇 명인가요? 20명: 이것은 도시에 있나요? 사람 2: 사람 1: 여자 없어요? 사람 2: 사람 1이 없음: 마을인가요? Person2: 질문 없음: 가죽으로 되어 있나요?\n' +
      '\n' +
      '**타깃** : Person2 : 예\n' +
      '\n' +
      '**Task:**: **Visdial_visual_dialogue_with_very_long_context**\n' +
      '\n' +
      '**인스트럭션**: 여기 사람과 로봇의 상세한 대화가 있습니다. 그들은 그림에 대한 몇 가지 질문과 답을 논의하고 있다. 대화를 통해 로봇처럼 질문에 답하십시오.\n' +
      '\n' +
      '남자들은 어떤 유니폼을 입고 있나요? 로봇: 그들은 군용 인간처럼 보인다: 얼마나 많은 사람들이 있다고 추측할 수 있는가? 로봇: 세 명의 남자와 한 명의 민간인 여성이 있습니다: 비행기는 어떤 색인가요? 로봇:백인:여자는 무엇을 하고 있나요? 로봇: 남자들 중 한 명과 대화하고 비행기의 측면을 가리킵니다. 인간: 비행기에 글씨가 있나요? 로봇: 몇개의 숫자들, 하지만 나는 그것들을 인간으로 표현할 수 없다: 그것은 군용 비행기처럼 보이나? 로봇: 그렇게 생각하지만 거의 다 왔어, 인간에게 말하기가 너무 어려워: 이미지에 다른 비행기가 있어? 로봇: 네, 저는 배경 질문에서 다른 두 개의 부분들을 봅니다: 여성은 무엇을 입고 있나요?\n' +
      '\n' +
      '**Target**: 로봇: 흰색과 파란색 드레스\n' +
      '\n' +
      '**Task:**: **CLEVR_visual_dialogue_in_3D_rendered_images**\n' +
      '\n' +
      '**인스트럭션**: 대화 히스토리로서 몇 개의 질의응답 쌍이 주어지며, 이미지에 기초하여 마지막에 질문에 답하게 된다.\n' +
      '\n' +
      '**입력** : 큰 파란색 무광 물체가 큰 녹색 무광 물체와 같은 모양으로 몇 개나 되는가? 0 \\(\\backslash\\)n은 작은 회색 금속 구의 수보다 작은 빨간색 반짝이는 물체의 왼쪽 빨간색 반짝이는 물체의 수가 더 많은가? no\\(\\backslash\\)no 녹색 물체가 빨간색 물체의 왼쪽에 있고 작은 빨간색 물체의 오른쪽에 있는 큰 물체가 같은 모양을 가지고 있나요? 빨간 금속 물체와 같은 크기의 다른 것은 없나요? 네\\(\\backslash\\)n 몇 개의 정육면체가 큰 고무 물체이거나 작은 노란색 고무 물체인가요? 0 \\(\\backslash\\)noes 큰 구는 녹색 원기둥과 같은 물질을 가지고 있는가? no\\(\\backslash\\)n 큰 녹색 금속 실린더의 오른쪽에 있는 무광택 물체 앞에 있는 무광택 물체 뒤에 있는 큰 물체가 어떤 모양일까요?\n' +
      '\n' +
      '**Target**: cylinder\n' +
      '\n' +
      '도 21 : 도 23\n' +
      '\n' +
      'Figure 23\n' +
      '\n' +
      'Figure 24\n' +
      '\n' +
      'Figure 23\n' +
      '\n' +
      'Figure 25\n' +
      '\n' +
      '**Task : PICKAPIC_image_prompt_generation Instruction : 주어진 이미지를 생성하기 위한 프롬프트를 작성하고 있는 이미지. 프롬프트를 생성하십시오. 제다이 오리가 광선검을 들고 있다\n' +
      '\n' +
      '**Task: VQA-E+short_image_captioning Instruction: 이미지를 제공받는다. 이미지에서 무슨 일이 일어나고 있는지 간단히 설명합니다. 목표: 식탁에 앉아 말하는 두 사람이 있다.\n' +
      '\n' +
      '**Task: VIZWIZ+captioning_image_taken_by_blind_people Instruction: 시각 장애인에 의해 촬영된 이미지를 주고, 이 이미지에 대한 다중 캡션을 생성한다. 타겟: 캡션 1: 한 여성의 손이 두꺼운 녹색 울퉁불퉁한 천으로 쉬고 있고, 그녀의 손가락은 오렌지 식품 라벨 아래에 있다. 캡션 2: 손가락 끝이 어두운 니트 천 위에 앉아 있는 오렌지 주스 상자를 만지고 있다. 캡션 3: 오렌지 주스는 당신의 손끝 바로 위에 있는 상자 안에 있다. 캡션 4: 한 사람이 음식 꾸러미를 만지고 있다.**\n' +
      '\n' +
      '**Task: ConceptualCaptions_conceptual_image_captioning Instruction**: 이 태스크에서, 주어진 이미지에 대한 짧은 캡션을 생성할 것이다. 이미지는 다양한 장면, 사물 또는 사람을 묘사할 수 있습니다. 목표는 이미지의 주요 요소를 설명하는 짧고 정확한 캡션을 제공하는 것입니다. 색상, 객체, 동작, 요소 간 상호 작용 등 세부 사항에 주의를 기울여 의미 있고 유익한 캡션을 만듭니다.\n' +
      '\n' +
      '타겟: 배우가 시사회에 도착합니다\n' +
      '\n' +
      '**Task: FICKR30K_multiple_captions_generation Instruction**: 각 이미지는 어떤 일이 일어나고 있다. 이미지를 주의 깊게 분석하고 각 이미지에 대해 5개의 짧은 캡션을 생성합니다.\n' +
      '\n' +
      '**대상: 캡션 1: 한 젊은 여성이 모자를 쓴 다른 여성이 쳐다보는 동안 무엇인가를 열심히 쳐다보고 있다. 캡션 2: 안경을 쓴 어린 소녀가 멀리서 무언가를 관찰한다. 캡션 3: 십대 소녀와 그녀의 엄마가 공공장소에 있다. 캡션 4: 금발 소녀가 선글라스를 고치고 있다. 캡션 5: 한 여성이 선글라스를 조정한다.\n' +
      '\n' +
      '**Task: VIZWIZ_captioning_image_taken_by_blind_people Instruction**: 시각 장애인에 의해 촬영된 이미지를 주고, 이 이미지에 대한 다수의 캡션을 생성한다.\n' +
      '\n' +
      '**대상: 캡션 1: 한 여성의 손이 두꺼운 녹색 울퉁불퉁한 천으로 쉬고 있고, 그녀의 손가락은 오렌지 식품 라벨 아래에 있다. 캡션 2: 손가락 끝이 어두운 니트 천 위에 앉아 있는 오렌지 주스 상자를 만지고 있다. 캡션 3: 오렌지 주스는 당신의 손끝 바로 위에 있는 상자 안에 있다. 캡션 4: 한 사람이 음식 꾸러미를 만지고 있다.\n' +
      '\n' +
      '**Task: NOCAPS_multiple_short_captions_generation Instruction**: 이 이미지에 대한 10개의 짧은 캡션의 리스트를 주세요.\n' +
      '\n' +
      '**대상: 캡션 1: 블랙탑에서 자전거로 폴로를 연주하는 사람들 그룹 캡션 2: 모자를 쓰고 자전거를 타는 남자. 캡션 3: 자전거를 타고 몰레로 공을 치는 것으로 구성된 게임을 하는 개인들. 캡션 4: 재킷을 입은 세 남자가 자전거에서 크로케를 하고 있다. Caption 5: 몇몇 사람들이 자전거로 하키를 하고 있다. 옵션 6: 농구 코트에서 자전거를 타고 돌아다니는 세 사람. Caption 7: 몇몇 사람들이 공원 아스팔트로 보이는 곳에서 자전거를 탄다. 캡션 8: 한 무리의 사람들이 자전거를 타고 하키를 하고 있다. 캡션 9: 4명의 사람들이 많은 집 근처의 콘크리트 패드 위에서 자전거를 타고 폴로를 하고 있다. 캡션 10: 4명의 사람들이 도로에서 자전거 위에서 폴로를 하고 있다.\n' +
      '\n' +
      'Figure 27:\n' +
      '\n' +
      '**Task: LOC-NARRATIVES_COCO_detailed_image_captioning Instruction**: 이 이미지에 대해 더 알고 싶습니다. 자세히 설명해 주시겠어요?\n' +
      '\n' +
      '**대상: 이 그림에는 이미지의 오른쪽과 왼쪽에 코끼리가 있고, 이미지의 오른쪽과 왼쪽에 물, 풀밭과 바위가 있으며, 이미지의 배경 영역에는 나무와 말린 풀이 있다.\n' +
      '\n' +
      '**Task: LOC-NARRATIVES_flickr30k_detailed_image_captioning Instruction**: 입력 영상에 대한 상세한 캡션을 주세요.\n' +
      '\n' +
      '**대상: 이 이미지에는 서로 다른 색상의 티셔츠를 입은 두 아이가 공기주입기에서 놀고 있습니다. 이 아이들 중 한 명이 장난감을 들고 웃고 있습니다. 배경에는 다른 장난감, 나무 물건, 사람이 있습니다.\n' +
      '\n' +
      '**Task: LOC-NARRATIVES_open_images_detailed_image_captioning Instruction**: 이 이미지에 무슨 일이 일어나고 있는가? 자세히 설명해 주세요.\n' +
      '\n' +
      '**대상: 이 사진에서 나무 물체 위에 있는 뚜껑이 있는 항아리에 있는 음식물을 볼 수 있고 흐릿한 배경이 있습니다.\n' +
      '\n' +
      '**Task: SciCap+figure_captioning Instruction**: 논문으로부터 주어진 과학 도형에 대한 상세 캡션을 생성한다. 캡션은 그림의 통계를 언급해야 하며 언어는 형식적이고 명확해야 합니다.\n' +
      '\n' +
      '**Target:** n의 다른 값에 대한 m = [BRACKET] 에지의 삽입을 위한 가장 효율적인 알고리즘에 의한 lca/la 데이터 구조에 의한 총 소요 시간과 소요 시간의 비교.\n' +
      '\n' +
      '**Task: SentiCap_image_captioning_conditioned_on_sentiment Instruction**: 사진을 제공받고, 사진과 관련된 특정 감성(긍정 또는 부정)을 가진 캡션을 작성한다. 캡션의 감성이 요청된 감성과 일치해야 합니다. 주어진 이미지에 대해 긍정적인 감성을 가진 캡션을 쓰세요.\n' +
      '\n' +
      '잘생긴 음식으로 가득 찬 아주 멋진 시도\n' +
      '\n' +
      '**Task: textcaps_image_captioning_with_reading_comprehension Instruction**: 이미지에 대한 캡션을 작성한다. 캡션을 작성할 때 이미지의 텍스트를 고려하여 캡션에 가장 적합한 방법을 결정합니다.\n' +
      '\n' +
      '**Target:** 여기 이 이미지에 대한 캡션이 있습니다: \'숫자가 50인 파란색 매직 저지는 회색 배경에 반대편입니다\'\n' +
      '\n' +
      'Figure 29:\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:43]\n' +
      '\n' +
      '**Task: LSUN_scene_classification**\n' +
      '\n' +
      '**명령: 이 작업에서는 장면(식탁방, 침실, 주방, 야외 교회 등)의 사진이 제공되며 이미지를 해당 장면 범주로 분류해야 합니다. 대답은 장소 이름이어야 합니다. 옵션: (a) 타워(b) 교실(c) 식당(d) 침실(e) 주방(f) 교회 외부(g) 거실(h) 회의실(i) 식당 목표: (h) 회의실**\n' +
      '\n' +
      '**Task: Places205_indoor_outdoor_classification**\n' +
      '\n' +
      '**인스트럭션: 이 작업에서, 당신은 묘사된 장소 또는 장면이 실내 또는 실외인지를 식별해야 한다. 이미지에는 병원, 브릿지, 코트야드, 모텔 등 총 205개 클래스가 포함되어 있습니다. 이미지의 클래스는 다양한 장소 또는 장면 집합입니다. 일부 이미지는 특정 장소와 관련된 객체를 포함할 수 있지만 일부 이미지는 장소 또는 경치를 직접 보여줄 수 있으므로 세부 사항에 주의하십시오. 따라서, 당신의 대답은 이미지 옵션에 표시된 장소 또는 장면이어야 한다: (a) 실외 (b) 실내 목표: (b) 실외**\n' +
      '\n' +
      '**Task: JHU-CROWD_scene_classification**\n' +
      '\n' +
      '**명령: 이미지 내의 장면의 위치를 제공한다. 워터파크, 마라톤, 시위, 경기장 또는 기타 가능한 장소가 될 수 있습니다. 현장은 경기장에 있습니다\n' +
      '\n' +
      '**Task: AID+aerial_scene_classification**\n' +
      '\n' +
      '**지시: 항공 이미지가 제공됩니다. 이미지에 있는 장면을 알려주세요. 잠재적인 장면은 해변, 산업, 초원 등이다. 공중현장은 공항이다\n' +
      '\n' +
      '**Task: Dark-Zurich_time_of_the_day_classification**\n' +
      '\n' +
      '**지시: 이미지가 캡처된 날짜의 시간을 식별합니다. 선택사항은 낮, 밤, 황혼이다. 목표: 오늘의 시간은 황혼입니다.\n' +
      '\n' +
      'Figure 33:\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:46]\n' +
      '\n' +
      '**Task: NATURALIST_family_classification**\n' +
      '\n' +
      '**명령**: 과는 속 위 및 순서 아래의 분류학적 순위이다. 이미지에서 유기체의 패밀리를 식별합니다.\n' +
      '\n' +
      '**표적**: 이미지 내 유기체의 계열은 Ranunculaceae이다.\n' +
      '\n' +
      '**Task: NATURALIST_genus_classification**\n' +
      '\n' +
      '**인스트럭션**: 속은 종 위 및 과 아래에 있는 분류학적 순위이다. 이미지에서 유기체의 속을 식별합니다.\n' +
      '\n' +
      '**표적**: 이미지 내의 유기체의 속은 Esox이다.\n' +
      '\n' +
      '**Task: NATURALIST_tain_English_name_classification**\n' +
      '\n' +
      '**지시**: 이미지 내의 유기체를 식별한다. 영어 이름(공통 이름이라고도 함) 다음에 학명(라틴 이름이라고도 함)을 지정합니다. 예를 들어: "이미지 속의 유기체는 일반적인 지렁이입니다. 그것의 학명은 Lumphicus terrestris이다.\n' +
      '\n' +
      '**표적**: 이미지 내의 유기체는 블루-브레스트 코돈비유이다. 그것의 학명은 Uraeginthus angolensis입니다.\n' +
      '\n' +
      '**Task: NATURALIST_order_classification**\n' +
      '\n' +
      '**명령**: 분류학적 범주는 기본적인 특성, 유사성 및 비유사성에 기초하여 개발된 유기체의 순위 또는 그룹이다. 그 순서는 분류학적 순위로서 가족 위와 계급 아래이다. 이미지에서 유기체의 순서를 식별합니다.\n' +
      '\n' +
      '**대상**: 이미지 내의 유기체의 순서는 스쿼마타이다.\n' +
      '\n' +
      '**Task: NATURALIST_phylum_classification**\n' +
      '\n' +
      '**인스트럭션**: 필럼은 계급 이상 왕국 이하의 순위를 매기는 주요 분류학적 범주로 정의된다. 이미지에서 유기체의 문을 식별합니다.\n' +
      '\n' +
      '**표적**: 이미지 내의 유기체의 문은 트라체오피타이다.\n' +
      '\n' +
      '**Task: NATURALIST_supercategory_classification**\n' +
      '\n' +
      '**인스트럭션**: 유기체의 이미지가 주어질 것입니다. 이미지를 분석하고 제공된 옵션에서 이 유기체의 슈퍼 범주를 선택합니다. 선택사항: (a) 동물 (b) 파충류 (c) 곤충 (d) 산란어 (e) 곰팡이 (f) 양서류 (g) 새 (h) 식물 (i) 몰럭 (j) 포유류 (k) 거미류\n' +
      '\n' +
      '**: (c) 곤충\n' +
      '\n' +
      'Figure 37:\n' +
      '\n' +
      'Figure 38:\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:48]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:49]\n' +
      '\n' +
      '**Task: Fairface_human_age_classification**\n' +
      '\n' +
      '**지시: 당신은 사람의 얼굴을 보게 된다. 이 사람은 나이가 다를 수 있고, 당신의 임무는 그 사람의 나이를 식별하는 것입니다**\n' +
      '\n' +
      '**타겟 : 사람의 나이는 10-19**\n' +
      '\n' +
      '**Task: Fairface_human_gender_classification**\n' +
      '\n' +
      '**설명: 여기 사람의 사진이 있습니다. 이 사진으로만 봤을 때 이 사람의 성별이 뭐라고 생각하십니까?\n' +
      '\n' +
      '**대상: 그 사람의 성별은 여성**\n' +
      '\n' +
      '**Task: Fairface_human_race_classification**\n' +
      '\n' +
      '**지시: 주어진 이미지에서 이 사람의 인종에 대한 좋은 추측은 무엇일까요?**\n' +
      '\n' +
      '**타겟: 그 사람의 인종은 동남아시아**\n' +
      '\n' +
      '**Task: LFW_human_face_recognition**\n' +
      '\n' +
      '**인스트럭션: 이 작업에서, 당신은 개인의 얼굴 이미지를 제시받을 것이다. 당신의 목표는 그것이 나타내는 사람의 신원을 식별함으로써 이미지를 정확하게 분류하는 것이다. 이를 달성하기 위해서는 얼굴의 모양과 구조, 눈, 코, 입, 머리카락 등 이미지에 존재하는 얼굴 특징과 신원을 결정하기 위한 귀중한 단서를 제공할 수 있는 두더지, 흉터 또는 모반과 같은 다른 구별되는 특징을 꼼꼼히 살펴야 한다. 예를 들어, 특정 얼굴 비율, 뚜렷한 눈 색깔 또는 독특한 헤어 스타일은 개인의 정체성의 특징을 정의할 수 있다. 다른 데이터 세트에서 바퀴로 자전거를 식별하거나 꽃잎으로 해바라기를 식별할 수 있듯이 이 경우 사람은 고유한 얼굴 특징 세트로 식별할 수 있다. 이러한 시각적 단서를 기반으로 정보에 입각한 결정을 내리면 그 사람의 정체로 답변을 제공한다.**\n' +
      '\n' +
      '**Target: Pete Sampras**\n' +
      '\n' +
      'Figure 45:\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:51]\n' +
      '\n' +
      '**Task: Core50_object_recognition**\n' +
      '\n' +
      '**명령:** 당신의 임무는 그림에 표시된 항목을 식별하는 것이다. 이미지에는 플러그 어댑터, 휴대폰, 가위 등과 같은 일상적인 객체가 포함되어 있습니다. 영상을 정확하게 분류하기 위해서는 물체의 모양, 크기, 색상 특성을 세심하게 고려하는 것이 중요하다.\n' +
      '\n' +
      '**Target:** cup\n' +
      '\n' +
      '**Task: Office-Home_object_recognition**\n' +
      '\n' +
      '**명령:** 당신의 작업은 객체 이미지를 침대, 싱크, 운동화, 테이블, TV 등과 같은 각각의 카테고리로 분류하는 것을 포함한다; 예를 들어, 모델이 랩톱의 이미지와 함께 제시된다면, 그것은 이미지를 \'랩톱\'으로 올바르게 식별하고 분류해야 한다.\n' +
      '\n' +
      '**Target:** Shelf\n' +
      '\n' +
      '**Task: MNIST_M_number_recognition**\n' +
      '\n' +
      '**명령:** 이 작업에서, 자연 이미지 배경에 겹쳐진 필기 디지트를 포함하는 그레이스케일 이미지가 제시될 것이다. 목표는 이미지에서 숫자를 정확하게 식별하는 것입니다.\n' +
      '\n' +
      '**Target:** 1\n' +
      '\n' +
      '**Task: ImageNet-A_object_recognition_of_natural_adversarial_examples**\n' +
      '\n' +
      '**Instruction:** 이 과제에서, 이미지가 어떤 것을 포함하고 있는지 식별해 주세요. 이미지는 다른 것들 중에서, 동물, 새, 일상 물체, 곤충 옵션을 포함할 수 있다: (a) 제공된 이미지는 로리케트를 포함한다(b) 제공된 이미지는 사자를 포함한다(c) 제공된 이미지는 아마딜로를 포함한다(d) 제공된 이미지는 야구 선수를 포함한다(e) 제공된 이미지는 세발자전거를 포함한다(f) 제공된 이미지는 럭비공을 포함한다(g) 제공된 이미지는 잭-오-랜턴을 포함한다(h) 제공된 이미지는 카누를 포함한다.\n' +
      '\n' +
      '**Target:**(a) 상기 제공된 이미지는 로리케트를 포함하는\n' +
      '\n' +
      '**Task: MVTecAD_industrial_item_recognition**\n' +
      '\n' +
      '**명령:** 당신의 목적은 이미지를 대응하는 객체 카테고리에 기초하여 분류하는 것이다. 제공된 이미지는 병, 케이블, 카펫 등을 포함한 다양한 산업 품목을 포함합니다. 선, 음영, 색상 구성, 세부 수준 등의 세부 사항에 주의하여 이미지의 전체적인 시각적 외관에 집중합니다. 이러한 특징은 다른 객체 범주 간에 크게 다를 수 있으므로 객체의 모양, 색상 및 질감과 같은 고유한 특성을 분석하는 것이 중요하다. 분류 프로세스가 완료되면 분석에 따라 적절한 개체 이름을 출력합니다.\n' +
      '\n' +
      '**대상:**대상물은 알약이다.\n' +
      '\n' +
      'Figure 49:\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:53]\n' +
      '\n' +
      'Task: Set5_object_recognition_in_low_resolution_image\n' +
      '\n' +
      '교수: 이 과제에서, 5명의 피험자, 즉 아기, 새, 나비, 머리, 여자 중에서 이미지에서 피험자를 인식한다.\n' +
      '\n' +
      '타겟: 이미지 내의 피사체는 새이다.\n' +
      '\n' +
      'Task: Yahoo_object_recognition\n' +
      '\n' +
      '명령: 이 작업에서는 동물, 객체 및 차량의 다른 범주의 이미지가 포함된 데이터 집합의 이미지가 제공됩니다. 이러한 범주는 하위 범주로 더 나뉜다. 당신의 일은 주어진 이미지를 이러한 하위 범주 중 하나로 분류하는 것입니다. 그것은 비행기에서 얼룩말까지 무엇이든 될 수 있습니다. 분류는 크기, 모양, 색상, 고유한 특징 및 이미지에 표시된 컨텍스트 또는 환경과 같은 주요 식별자에 기초해야 합니다. 예를 들어, 얼룩말의 이미지가 주어지면, 여러분의 대답은 단순히 얼룩말일 것입니다. 이미지도 물체나 차량의 것일 수 있다는 것을 기억하세요. 대답은 이미지에 대한 적절한 하위 범주를 나타내는 단일 단어여야 하며, 광범위한 범주를 넘어서는 특수성을 강조해야 한다.\n' +
      '\n' +
      'Target: building\n' +
      '\n' +
      'Task: MSCOC_appliance_recognition\n' +
      '\n' +
      '명령: 집 주변에서 일반 전자 기기의 이미지가 주어지면 해당 객체의 유형을 식별합니다. 부엌에서 음식을 요리하거나 보관하는 데 일반적으로 사용되는 가전 제품일 수 있습니다. 옵션: (a) 이 이미지는 오븐을 포함하고 (b) 이 이미지는 마이크로파를 포함하고 (c) 이 이미지는 토스터를 포함하고 (d) 이 이미지는 냉장고를 포함하고 (e) 이 이미지는 싱크대를 포함한다\n' +
      '\n' +
      'Target: (e) 이 이미지에는 싱크가 포함되어 있음\n' +
      '\n' +
      'Task: MSCOC_furniture_recognition\n' +
      '\n' +
      '지시: 집에 있는 가구의 이미지가 주어지면 가구의 유형을 식별합니다. 일반적으로 집을 더 잘 보이게 하기 위해 사용되며 다양한 종류의 재료로 만들 수 있습니다. 옵션: (a) 이 이미지는 식탁을 포함하고 (b) 이 이미지는 침대를 포함하고 (c) 이 이미지는 화장실을 포함하고 (d) 이 이미지는 의자를 포함하고 (e) 이 이미지는 소파를 포함하고 (f) 이 이미지는 화분을 포함하는 식물\n' +
      '\n' +
      'Target: (d) 이 이미지에는 의자가 포함되어 있음\n' +
      '\n' +
      'Task: MSCOCOC_kitchen_object_recognition\n' +
      '\n' +
      '지시: 주방에 있는 어떤 것의 이미지가 주어지면, 그것이 무엇일 수 있는지 식별하세요. 이미지는 식사에 사용되는 요리 도구 또는 품목이 될 수 있습니다. 음식을 제공하거나 보관하는 데에도 사용할 수 있습니다. 옵션: (a) 이 이미지는 병(b) 이 이미지는 컵(c) 이 이미지는 와인잔(d) 이 이미지는 포크(e) 이 이미지는 나이프(f) 이 이미지는 그릇(g) 이 이미지는 숟가락을 포함한다.\n' +
      '\n' +
      'Target: (a) 이 이미지에는 병이 들어 있다\n' +
      '\n' +
      'Task: MSCOCOC_vehicle_recognition\n' +
      '\n' +
      '명령: 차량의 이미지가 주어지면 차량의 종류를 식별합니다. 차량은 다양한 종류가 있을 수 있습니다; 그것은 중고, 개인 또는 대중 교통일 수 있습니다. 하나 이상의 사람을 동시에 태울 수 있습니다. 옵션: (a) 이 이미지는 버스(b) 이 이미지는 자전거를 포함한다(c) 이 이미지는 보트(d) 이 이미지는 비행기(e) 이 이미지는 오토바이를 포함한다(f) 이 이미지는 기차(g) 이 이미지는 트럭(h) 이 이미지는 자동차를 포함한다.\n' +
      '\n' +
      'Target: (b) 이 이미지에는 자전거가 포함되어 있음\n' +
      '\n' +
      'Figure 54:\n' +
      '\n' +
      'Figure 53:\n' +
      '\n' +
      '**Task: MSCOCO_sports_object_recognition**\n' +
      '\n' +
      '**지시: 상품을 정렬하는 이미지가 주어지면 객체가 무엇인지 식별합니다. 팀 스포츠나 개인 활동에 사용할 수 있습니다. 물체는 또한 다양한 종류의 스포츠에서 사용될 수 있고 때때로 착용자가 스포츠를 더 쉽게 플레이할 수 있게 한다. Options: (a) 이 이미지는 스키(b) 이 이미지는 서프보드를 포함한다(c) 이 이미지는 프리스비를 포함한다(d) 이 이미지는 야구 방망이를 포함한다(e) 이 이미지는 테니스 라켓(f) 이 이미지는 야구 글러브를 포함한다(g) 이 이미지는 스노우보드를 포함한다(i) 이 이미지는 스케이트보드를 포함한다(j) 이 이미지는 스포츠볼을 포함한다: (j) 이 이미지는 스포츠볼을 포함한다.\n' +
      '\n' +
      'Figure 56:\n' +
      '\n' +
      '**Task: Wikihow_image_text_step_ordering**\n' +
      '\n' +
      '**지시: 당신은 페인트로 파인 코네스를 담그고 있습니다. "대나무 꼬치 끝을 솔방울 꼭대기에 비틀기" 단계가 이미지 속 단계 다음 또는 이전 단계인가요? 옵션: (a) 다음 (b) 이전 타겟: (b) 이전**\n' +
      '\n' +
      '**Task: Wikihow_immediate_next_step_selection**\n' +
      '\n' +
      '**설명: 오븐에서 드라이 실란트로를 사용하고 있습니다. 이미지에서 다음 단계는 무엇입니까? 옵션: (a) 건조된 고수 잎을 밀폐 용기에 보관한다. (b) 오븐을 250"F(121"C)로 예열한다. (c) 오븐에서 트레이를 꺼내 10분간 식힌다. (d) 베이킹 트레이에 잎을 펴서 1층을 형성한다. (e) 고수를 세척하여 먼지와 찌꺼기를 제거한다. 목표: (c) 오븐에서 트레이를 꺼내고 10분 동안 식히세요.**\n' +
      '\n' +
      '**Task: Wikihow_text_image_step_ordering**\n' +
      '\n' +
      '**설명: 목표는 "전자레인지로 해동"입니다. 현재 단계인 "비닐 랩을 제거하고 반죽을 검사합니다."가 주어지면, 사진은 다음 단계입니까 아니면 이전 단계입니까? 옵션: 이전 다음 타겟: 다음**\n' +
      '\n' +
      '**Task: multimodal_factual_checking Instruction**: Context: 우리의 평점 A 널리 공유된 페이스북 게시물은 캘리포니아가 \'소아성애\'를 합법화했다고 주장했고, \'이제 21살은 11살과 성관계를 가질 수 있고, 성관계자로 성등록부에 등재되지 않을 수 있다.\' 그 게시물과 많은 사람들은 단순히 틀렸다. 그들은 캘리포니아의 성결정권자 등록부에서 LGBT 젊은이들이 어떻게 대우받는지에 대한 격차를 없애는 것을 목표로 하는 주 SB 145의 제안을 심각하게 왜곡한다. 이 법안은 미성년자와 자발적, 구두 성관계를 한 것으로 유죄 판결을 받고 피해자와 10세 이내인 청년에 대한 자동 성 제공 등록을 없앨 것이다. 대신, 기존 법이 법관이 등록부에 질적인 성교를 포함하는 사건에 범죄자를 배치할지 여부를 결정할 수 있는 것처럼 판사는 그러한 결정을 내릴 것이다. 그 법안은 어떤 식으로든 성인이 미성년자와 어떤 종류의 성관계를 갖는 것을 합법화하지 않을 것이다. 유일한 변화는 특정 성행위에 대해 성범죄자를 성등록부에 등재할 것인지에 대한 판사의 재량권을 부여하는 것을 포함한다. 우리는 페이스북 포스트 팬츠 온 파이어의 주장을 평가한다. 바지 ON FIRE - 그 진술은 정확하지 않고 터무니없는 주장을 한다. 그 문맥이 "PEDOPHILA는 이제 캘리포니아에서 합법이다. 이제 21살은 11살과 성관계를 가질 수 있고, 성범죄자로 성등록부에 등재되지 않는다"를 지지하는가? 옵션: (a) 확실하지 않다 (b) 아니오 (c) 예\n' +
      '\n' +
      '**Target:**A1: (b)no\n' +
      '\n' +
      'Figure 58\n' +
      '\n' +
      '**Task:**RAVEN_relational_and_analogical_visual_reasoning Instruction**: 각 이미지는 이미지 1 내지 이미지 8로 라벨링된 8개의 이미지를 갖는다. 이들 8개의 이미지는 특정 패턴을 따른다. 패턴을 감지하고 사용 가능한 8가지 옵션에서 시퀀스의 다음 이미지를 선택합니다.\n' +
      '\n' +
      '**타깃:**옵션 4\n' +
      '\n' +
      'Figure 59\n' +
      '\n' +
      '**Task:**image_text_matching Instruction**: "파란색과 보라색을 입은 여성이 눈 속에 서 있는 동안 스노우보드를 들고 있다."는 이미지를 묘사하고 있는가? 옵션: (a) 설명이 이미지와 일치함 (b) 텍스트는 이미지에 대한 설명이 아님\n' +
      '\n' +
      '**Target:**(a) 상기 설명이 상기 이미지와 일치하는\n' +
      '\n' +
      '**Task:**Winoground+image_caption_matching Instruction**: 이 작업에서, 당신은 이미지와 두 캡션을 제공받을 것이다. 두 캡션 중 어떤 캡션이 이미지를 올바르게 설명하는지 확인하는 작업을 수행합니다. 옵션: (a) 백색 벽은 곧 청색으로 칠해질 것이고 (b) 청색 벽은 곧 백색으로 칠해질 것이다\n' +
      '\n' +
      '**대상:**(a) 흰색 벽이 곧 파란색으로 칠해질 것\n' +
      '\n' +
      '**Task:**image_text_selection Instruction**: 옵션들 중 어떤 옵션이 이미지의 캡션인지. 옵션: (a) 전자레인지에 앉아 있는 노트북 두 대. (b) 두 명의 나이든 여성들이 저녁 식사를 준비하고 있다. (c) 컴퓨터 모니터, 프린터 및 cd 랙이 구비된 책상. (d) 식사 접시에 조건을 넣을 준비를 하고 있는 소녀. (e) 한 남자가 버스의 휴대전화로 사진을 찍고 있다.\n' +
      '\n' +
      '(d) 식사 접시에 조건을 넣을 준비를 하고 있는 소녀.\n' +
      '\n' +
      'Figure 61\n' +
      '\n' +
      '**Task: DOMAIN-NET_object_recognition_in_clip_art Instruction: Clip art는 문서 및 프리젠테이션에 사용되는 간단한 그림 또는 기호로 정의된다. 입력은 클립 아트 이미지입니다. 이미지의 주요 객체를 식별합니다. 타겟: 지그재그\n' +
      '\n' +
      '**Task: DOMAIN-NET_object_recognition_in_infograph Instruction: info 그래프는 임의의 객체에 대한 정보 또는 데이터를 나타내기 위해 사용되는 포스터와 같은 시각적 이미지이다. 이 작업의 경우 입력은 정보 그래프가 됩니다. 정보 그래프의 주요 객체를 식별합니다. 타깃: 토스터**\n' +
      '\n' +
      '**Task: DOMAIN-NET_object_recognition_in_painting Instruction: 이 작업에 대한 입력은 그림이다. 그림의 주요 객체를 식별합니다. Target, see saw**\n' +
      '\n' +
      'Figure 62\n' +
      '\n' +
      '**Task: DOMAIN-NET_object_recognition_in_quickdraw Instruction: 이 작업에서 입력은 어떤 것의 대략적인 스케치가 될 것이다. 대략적인 스케치에 표시된 주요 객체를 식별합니다. 목표물 덤벨**\n' +
      '\n' +
      '**Task: DOMAIN-NET_object_recognition_in_real_image Instruction: 이미지 내의 주요 객체를 식별한다. 목표물 블루베리**\n' +
      '\n' +
      '**Task: ExDark_object_recognition_in_low_light_environment Instruction: 주어진 이미지는 저조도 환경에서 촬영된다. 자전거, 보트, 병, 버스, 자동차 및 기타 객체를 포함하여 이미지에서 객체를 식별합니다. 목표물은 자전거다\n' +
      '\n' +
      'Figure 61\n' +
      '\n' +
      '**Task: ImageNet-R_object_recognition_in_diverse_image_domain**\n' +
      '\n' +
      '**Instruction**: 당신의 임무는 다양한 카테고리를 이용하여 이미지를 분류하는 것이다. 이러한 특성은 렌디션에 따라 다를 수 있으므로 이미지 내 객체의 모양, 색상 및 질감을 포함한 세부 정보를 주의 깊게 관찰해야 합니다. 분류 프로세스의 결과로 적절한 개체 이름을 출력합니다.\n' +
      '\n' +
      '**대상: 백상아리 대왕\n' +
      '\n' +
      '**Task: ImageNet_object_recognition_in_sketch**\n' +
      '\n' +
      '**인스트럭션**: 객체에 대한 스케치가 주어집니다. 이미지에 있는 객체의 이름을 알려주세요.\n' +
      '\n' +
      '목표물은 다리미야\n' +
      '\n' +
      '**Task: PACS_object_recognition_in_art_painting**\n' +
      '\n' +
      '**인스트럭션**: 입력으로서 미술 회화 이미지가 주어질 것이다. 이미지의 주요 객체를 식별합니다.\n' +
      '\n' +
      '**Target: dog\n' +
      '\n' +
      '**Task: PACS_object_recognition_in_cartoon**\n' +
      '\n' +
      '**인스트럭션**: 만화의 이미지를 받게 될 것입니다. 이미지의 주요 객체를 식별합니다.\n' +
      '\n' +
      '**Target: horse\n' +
      '\n' +
      '**Task: PACS_object_recognition_in_photograph**\n' +
      '\n' +
      '**인스트럭션** : 입력은 물체의 사진이다. 이미지의 주요 객체를 식별합니다.\n' +
      '\n' +
      '**Target: elephant\n' +
      '\n' +
      '**Task: SKETCH_living_organism_classification_in_sketch**\n' +
      '\n' +
      '**인스트럭션**: 이 작업에서, 당신은 사진이 살아있는 유기체를 포함하는지 여부를 식별할 것이다. 주어진 이미지는 인간이 그린 흑백 스케치입니다. 그림이 살아있는 유기체 또는 살아있는 유기체의 일부를 묘사한다면, 출력은 "살아있는"이어야 한다. 그렇지 않으면 "무생활"을 인쇄합니다.\n' +
      '\n' +
      '**Target: Living\n' +
      '\n' +
      '**Task:**\n' +
      '\n' +
      '**Task:****SXETCH_object_recognition_in_sketch**\n' +
      '\n' +
      '**인스트럭션: 각각의 이미지는 물체의 인간이 그린 스케치이다. 이미지의 주요 객체를 식별합니다. 타겟: 마이크**\n' +
      '\n' +
      '**Task:****Cinic-10_animal_recognition_in_low_resolution_image**\n' +
      '\n' +
      '**지시: 주어진 이미지는 다양한 종류의 동물을 포함할 수 있다. 이들 동물 중 일부는 숲, 건조지 또는 기타 자연 지역에서 발견된다. 그들 중 일부는 길들여진 애완동물일 수도 있습니다. 사진 속의 동물을 확인해 주세요. 목표: 이미지에는 새**\n' +
      '\n' +
      '**Task:****Cinic-10_shipping_method_recognition_in_low_resolution_image**\n' +
      '\n' +
      '**지시: 주어진 이미지는 상이한 유형의 운송 장비를 포함할 수 있다. 물건이나 육지를 가로질러 물건을 운반할 수 있으며 전 세계적으로 필요한 모든 종류의 재료를 운반합니다. 사진에서 배송 옵션 유형을 확인하십시오. 타겟: 이미지가 선박**을 포함하고 있음\n' +
      '\n' +
      '**Figure 66**\n' +
      '\n' +
      '**Task:****Cinic-10_transportation_option_recognition_in_low_resolution_image**\n' +
      '\n' +
      '**명령: 주어진 이미지는 상이한 유형의 운송 차량을 포함할 수 있다. 사람들은 일상 생활에서 이 차량들을 타고 돌아다닌다. 항공 여행이나 지상에서 더 느린 운송 수단일 수 있습니다. 사진에서 전송 옵션 유형을 확인하십시오. 타겟: 이미지가 자동차**를 포함하고 있음\n' +
      '\n' +
      '**Task:****Cinic-10_animal_presence_classification_in_low_resolution_image**\n' +
      '\n' +
      '**지시: 주어진 이미지는 일부 동물들을 포함할 수 있다; 그것들은 일반적으로 야생 또는 길들여진 동물들에서 발견되는 동물들일 수 있다. 그림에는 이 설명에 맞지 않는 내용도 포함될 수 있습니다. 당신의 일은 이미지의 주체가 동물인지 아닌지를 확인하는 것이다. 목표물: 그 대상은 동물**\n' +
      '\n' +
      '**Task:****Cinic-10+object_shipping_object_presence_in_low_resolution_image**\n' +
      '\n' +
      '**지시: 주어진 이미지는 전 세계에서도 먼 거리를 가로질러 상품 및 재료를 운송하는 데 사용되는 일부 차량을 포함할 수 있다. 그림에는 이 설명에 맞지 않는 내용도 포함될 수 있습니다. 귀하의 업무는 이미지의 주체가 상품을 배송하는 데 사용할 수 있는지 여부를 식별하는 것입니다. Target: object can used for shipping**\n' +
      '\n' +
      '**Task:** **ViDA-2017_object_recognition_in_3D_rendered_image**\n' +
      '\n' +
      '**명령:** 당신의 임무는 대응하는 객체 카테고리에 기초하여 이미지를 분류하는 것이다. 이미지에는 비행기, 말, 칼, 사람, 식물 등 12개 카테고리에 분포된 다양한 객체가 포함되어 있다. 이미지를 정확하게 분류하려면 모양, 색상, 질감 및 공간 컨텍스트 관계와 같은 시각적 특성을 주의 깊게 분석하십시오. 이미지의 객체 범주를 식별한 후 분류에 적합한 레이블을 출력합니다.\n' +
      '\n' +
      '**Target:** plant\n' +
      '\n' +
      '**Task:** **ViSDA-2017_multiple_choice_object_recognition_in_3D_rendered_image**\n' +
      '\n' +
      '**명령:**3D 렌더링된 객체를 포함하는 이미지가 주어집니다. 목표는 주어진 옵션에서 이미지에 존재하는 객체의 범주를 식별하는 것입니다. 옵션: (a) 나이프(b) 말(c) 열차(d) 버스(e) 플랜트(f) 스케이트보드(g) 카(h) 자전거(i) 트럭(j) 비행기\n' +
      '\n' +
      '**Target:**(i) 트럭\n' +
      '\n' +
      '**Task:** **DOMAIN-NET_image_style_classification**\n' +
      '\n' +
      '**지시:** 이미지를 받게 됩니다. 두 가지 질문에 대답하세요. 이게 무슨 이미지야? 클립 아트, 정보 그래프, 그림, 대략적인 스케치, 그림, 실제 및 스케치 중에서 선택합니다. 두 번째 질문, 이미지의 주요 대상은 무엇인가? \'이건 사과 클립아트야\'\n' +
      '\n' +
      '이건 트럼펫 그림이야\n' +
      '\n' +
      '**Task:** **ImageNet-R_image_style_classification**\n' +
      '\n' +
      '**지도:** 자신의 영역을 기준으로 이미지를 분류하는 것이 목표이며, \'비디오게임\', \'그림 그리기\', \'스케치\', \'만화\', \'예술\', \'토이\', \'데비안타르트\', \'그래픽\', \'조각\', \'미스\', \'자수\', \'스티커\', \'그라피티\', \'오리가미\', 또는 \'문신\'일 수 있다. 최종 출력은 이미지의 식별된 도메인을 지정해야 합니다.\n' +
      '\n' +
      '**Target:** misc\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명: 반려견의 이미지를 드립니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명: 반려견의 이미지를 드립니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_dog_image_style_classification**\n' +
      '\n' +
      '**설명:**개의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**설명: 반려견의 이미지를 드립니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다. 옵션 : (a) 미술화 (b) 만화 (c) 스케치 (d) 사진\n' +
      '\n' +
      '**타깃:**(b) 만화\n' +
      '\n' +
      '**Task:** **PACS_dog_dog_image_style_classification**\n' +
      '\n' +
      '**명령:**\n' +
      '\n' +
      '**Task: PACS_elephant_image_style_classification**\n' +
      '\n' +
      '당신은 코끼리의 이미지를 얻게 될 것입니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다.\n' +
      '\n' +
      '옵션: (a) 만화(b) 미술도화(c) 사진(d) 스케치\n' +
      '\n' +
      '**Target:**(d) 스케치\n' +
      '\n' +
      '**Task: PACS_giraffe_image_style_classification**\n' +
      '\n' +
      '**설명:** 기타의 이미지를 받게 됩니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다.\n' +
      '\n' +
      '옵션: (a) 스케치(b) 만화(c) 미술화(d) 사진\n' +
      '\n' +
      '**Target:**(a) 스케치\n' +
      '\n' +
      '**Task: PACS_guitar_image_style_classification**\n' +
      '\n' +
      '**설명:** 기타의 이미지를 받게 됩니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다.\n' +
      '\n' +
      '옵션: (a) 만화(b) 사진(c) 스케치(d) 미술 그림\n' +
      '\n' +
      '**Target:**(b) 사진\n' +
      '\n' +
      '**Task: PACS_hose_image_style_classification**\n' +
      '\n' +
      '**설명:**집의 이미지가 주어집니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다.\n' +
      '\n' +
      '옵션: (a) 스케치(b) 사진(c) 아트페인팅(d) 스케치\n' +
      '\n' +
      '**Target:**(b) 사진\n' +
      '\n' +
      '**Task: PACS_person_image_style_classification**\n' +
      '\n' +
      '**설명:** 사람의 이미지가 주어질 것입니다. 이미지는 그림, 만화, 사진 또는 스케치와 같은 다양한 카테고리로 구성될 수 있습니다. 이미지 범주를 식별합니다.\n' +
      '\n' +
      '**Target:** Cartoon\n' +
      '\n' +
      '**Task:** **Model-vs-human_image_style_classification**\n' +
      '\n' +
      '**설명:** 이 이미지의 예술적 스타일은 무엇입니까?\n' +
      '\n' +
      '**Target:** power-equalsiat\n' +
      '\n' +
      'Figure 71:\n' +
      '\n' +
      'Figure 72:\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:62]\n' +
      '\n' +
      '**Task: DTD+coarse_grained_texture_classification**\n' +
      '\n' +
      '**지시: 텍스쳐는 인간의 관점에서 표면 또는 물질의 느낌, 외관 또는 일관성으로 정의된다. 이미지에 표시된 기본 텍스처를 탐지합니다. 목표물, 금이 갔다\n' +
      '\n' +
      '**Task: DeepFashion_cloth_texture_classification**\n' +
      '\n' +
      '**설명: 천에 대한 아주 간단한 설명을 써주실 수 있나요? 목표: 그 옷은 추상 거울 인쇄 드레스입니다.**\n' +
      '\n' +
      '**Task: DTD_multiple_texture_detection**\n' +
      '\n' +
      '**지시: 텍스쳐는 인간의 관점에서 표면 또는 물질의 느낌, 외관 또는 일관성으로 정의된다. 이미지의 모든 텍스처를 탐지합니다. 쉼표로 구분된 목록으로 제시합니다 목표: 다공성**\n' +
      '\n' +
      'Figure 75:\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:64]\n' +
      '\n' +
      'Figure 76\n' +
      '\n' +
      '**Task:**VQA_color\n' +
      '\n' +
      '**명령:** 이 작업에서 이미지 내의 일부 객체의 색상을 묻는다. 당신의 대답은 구절이 되어야 한다.\n' +
      '\n' +
      '**입력:**질문: 우산은 주로 어떤 색인가요?\n' +
      '\n' +
      '**Target:** orange\n' +
      '\n' +
      '**Task:**Visual7W_VQA_object_attribute\n' +
      '\n' +
      '**Instruction:** 이 작업에서, 당신은 어떤 객체의 속성에 대해 질문을 받게 될 것이다. 당신의 대답은 매우 간결해야 합니다.\n' +
      '\n' +
      '**입력:**질문: 욕조의 벽은 무엇으로 만들어졌는가?\n' +
      '\n' +
      '**Target:** tile\n' +
      '\n' +
      '**Task:**VQA_activity_recognition\n' +
      '\n' +
      '**지시:** 이 작업에서 이미지에서 일어나는 주요 활동에 대한 질문에 답해야 합니다. 대답은 한두 단어여야 합니다.\n' +
      '\n' +
      '여자애는 뭐 하는 거야?\n' +
      '\n' +
      '**Target:** Eating.\n' +
      '\n' +
      'Figure 77\n' +
      '\n' +
      '**Task:**VQA_activity_recognition\n' +
      '\n' +
      '**지시:** 이 작업에서 이미지에서 일어나는 주요 활동에 대한 질문에 답해야 합니다. 대답은 한두 단어여야 합니다.\n' +
      '\n' +
      '여자애는 뭐 하는 거야?\n' +
      '\n' +
      '**Target:** Eating.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:66]\n' +
      '\n' +
      '**Task:** VQA_positional_reasoning\n' +
      '\n' +
      '**명령:** 이 작업에서, 목표는 제시된 이미지 내의 객체들의 위치를 이해하고 질문에 대한 답변을 제공하는 것이다. 나에게 아주 짧은 대답을 주었다.\n' +
      '\n' +
      '**입력:** 케익의 오른쪽에 뭐가 있죠?\n' +
      '\n' +
      '**Target:** fork\n' +
      '\n' +
      '**Task:** VQA_scene_recognition\n' +
      '\n' +
      '**지시:** 이미지 내의 장면에 대한 질문을 받게 된다. 질문에 한 두 단어로 대답하세요.\n' +
      '\n' +
      '**입력 :** 여기가 실내인가요 실외인가요?\n' +
      '\n' +
      '**Target:** indoor\n' +
      '\n' +
      '**Task:** VQA_sentiment_understanding\n' +
      '\n' +
      '**지시:** 이 작업에서, 이미지 내에 전달되는 감정에 관한 질문을 받게 될 것이다. 나는 짧고 간결한 대답이 필요하다.\n' +
      '\n' +
      '**입력:** 문제는 이 개가 행복하다는 거야?\n' +
      '\n' +
      '**Target:** yes\n' +
      '\n' +
      '**Task:** VQA_sport_recognition\n' +
      '\n' +
      '**설명:**스포츠에 관한 사진이 주어지면 다음 질문에 답하시오. 질문에 한 두 단어로 대답하세요.\n' +
      '\n' +
      '어떤 스포츠를 할 것인가?\n' +
      '\n' +
      '**Target:** surfing\n' +
      '\n' +
      '**Task:** VQA_utility_affordance\n' +
      '\n' +
      '**인스트럭션:** 사진을 보시고 사진 속 각 객체들이 어떤 용도로 사용될 수 있는지 생각하여 다음 질문에 답해 주시기 바랍니다. 출력에는 한두 개의 단어가 포함될 수 있습니다.\n' +
      '\n' +
      '입력:**남자가 자르고 있는 것은 무엇인가?\n' +
      '\n' +
      '**Target:** garlic\n' +
      '\n' +
      '**Task:** VQAv2_general_VQA\n' +
      '\n' +
      '**인스트럭션:****인스트럭션:**사진을 보시고 사진에 대한 일반적인 질문에 답변해 주시기 바랍니다. 출력에는 한두 개의 단어가 포함될 수 있습니다.\n' +
      '\n' +
      '**입력 :** 기호가 무슨 색인가요?\n' +
      '\n' +
      '**대상:** 적색 및 백색\n' +
      '\n' +
      '**Figure 81**\n' +
      '\n' +
      '**Task:Visual-Genome_spatial_relationship_question_answering Instruction: 이미지 내 객체들의 공간적 관계에 대한 질문을 받는다. 짧은 문구로 질문에 대답하세요. 입력: 피자에 무엇이 들어있나요? 목표물함\n' +
      '\n' +
      '**Task: CLEVR-CoGenT_question_answer_matching Instruction: 이 작업에서, 당신은 질문들의 세트 및 대응하는 답변들과 함께 3D-렌더링된 객체들을 포함하는 이미지를 제시할 것이다. 이미지의 시각적 내용을 기반으로 각 질문과 해당 답변을 올바르게 일치시키는 것이 목표입니다. 출력 형식은 Q1A3, Q2A5, Q3A2, Q4A1, Q5A1이라는 패턴을 따라야 하며, 이는 질문 번호와 해당 답변 번호를 나타낸다. 입력: Q1: 고무공과 같은 색상의 다른 물체는 몇 개입니까? Q2: 시안 고무 원통의 오른쪽에 있는 반짝이는 물체의 색깔이 큰 원통과 같은가? Q3: 작은 청록색 실린더 앞에 있는 노란색 물체는 무엇으로 만들어졌나요? Q4: 큰 보라색 물체의 물질이 큰 구와 같은가? Q5: 시안 고무 물체 뒤에 노란색 금속 블록이 있는데, 작은 시안 원통과 같은 크기를 가지고 있나요? 타겟: Q1A5 Q2A7 Q3A8 Q4A6 Q5A6 Q6A3 Q7A2 Q8A1 Q9A4\n' +
      '\n' +
      '**Task: Viewiz_answering_visual_questions_from_blind_people Instruction: 시각 장애인은 이 이미지에 대해 질문을 하고, 질문에 가능한 최선의 방법으로 답변한다. 입력: 이것은 어떤 종류의 음식인가요? 옵션: (a) 통조림 콩(b) 부시가 소듐 다크 레드 강낭콩(c) 다크 레드 강낭콩(d) 강낭콩(e) 감소 소듐 강낭콩 목표: (b) 부시가 소듐 다크 레드 강낭콩**\n' +
      '\n' +
      '**Task: DAQUAR_VQA Instruction: 입력 텍스트는 이미지에 대한 질문을 포함할 것이다. 질문에 대답해 출력은 한두 단어여야 합니다. 입력: 공장의 오른쪽에 무엇이 있습니까? 목표물 내각**\n' +
      '\n' +
      '**Task: CLEVR-CoGenT_VQA_in_3D_rendered_images Instruction: 이 작업에 대한 입력은 3D-rendered 객체들의 이미지 및 상이한 카테고리들에 속하는 질문이다. 질문은 기존, 카운트, 정수 비교, 쿼리 속성 및 속성 비교의 다섯 가지 작업 클래스로 분류됩니다. 여기서의 과제는 질문에 답하는 것이고 당신의 대답은 한 두 개의 토큰이어야 한다. 입력: 청록색 실린더의 크기가 어떻게 되나요? 목표물: 대형**\n' +
      '\n' +
      '**Task: CLEVR_question_answer_matching**\n' +
      '\n' +
      '**인스트럭션: 3D-렌더링된 객체들, 다수의 질문들 및 동일한 수의 답변들의 이미지가 주어질 것이다. 여기서의 과제는 당신이 보는 이미지에 따라 정답에 맞는 질문들을 맞추는 것이다. 출력의 형식은 다음과 같다: Q1A3,Q2A5,Q3A2,Q4A1,Q5A1**\n' +
      '\n' +
      '**입력:**Q1: 무광택 실린더 앞의 무광택 물체 앞에 있는 큰 공의 바로 앞에 반짝이는 것이 있는데, 그 모양이 어떻게 되나요? Q2: 큐브와 같은 크기의 다른 것이 있나요? Q3: 회색 무광택 공의 오른쪽 고무 물체보다 녹색 금속 구 뒤에 있는 공이 더 많나요? Q4: 노란색 고무 물체의 왼쪽에 있는 고무 물체의 오른쪽에 금속 구가 있나요?\n' +
      '\n' +
      '**대상 : Q1A2 Q2A3 Q3A3 Q4A3 Q5A6 Q6A3 Q7A1 Q8A5 Q9A4\n' +
      '\n' +
      '**Task: CLEVR_VQA_in_30_rendered_images_with_multiple_questions**\n' +
      '\n' +
      '**인스트럭션: 이 작업에 대한 입력은 3D-렌더링된 객체들의 이미지 및 상이한 카테고리들에 속하는 질문들의 세트이다. 질문은 기존, 카운트, 정수 비교, 쿼리 속성 및 속성 비교의 다섯 가지 작업 클래스로 분류됩니다. 이 작업의 출력은 각 이미지에 대해 주어진 질문에 대한 답변 집합입니다. 답변은 이미지의 내용과 질문의 범주를 기반으로 생성되어야 한다. 출력은 텍스트 형태로 되어 있어야 합니다.**\n' +
      '\n' +
      '**입력:**Q1: 금속 큐브와 같은 색인 다른 것이 있습니까? Q2: 작은 노란색 금속 물체 앞에 있는 무광택 물체가 어떤 모양을 가지고 있나요? Q3: 노란색 무광택물 뒤에 있는 시안 무광택물 실린더에 남아 있는 시안 무광택물의 크기는 어떻게 되나요? Q4: 작은 노란색 고무 블록 앞에 있는 노란 것이 작은 금속 블록 앞에 있는 작은 것과 같은 모양일까요?\n' +
      '\n' +
      '**Target:**A1: 예 A2: 실린더 A3: 소형 A4: 노 A5: 예 A6: 2A7: 대형 A8: 노 A9: 고무\n' +
      '\n' +
      '**Task:** KVQA_world_knowledge_enabled_VQA**\n' +
      '\n' +
      '**인스트럭션: 사진 및 사진과 관련된 질문을 제공받는다. 당신의 직업은 당신의 배경지식으로 질문에 정확하게 답하는 것이다. 질문들에서 방향들(좌측, 우측 등)에 대한 임의의 참조들은 이미지에 묘사된 사람의 관점에서 나온다는 것에 유의한다. 답변은 개체명으로 구성되어야 합니다.**\n' +
      '\n' +
      '**입력:** 어떤 대륙에서 태어난 인물인가요?\n' +
      '\n' +
      '**타깃 *** 북미지역\n' +
      '\n' +
      '**Task:** VIQAF_knowledge_based_VQA_about_entities**\n' +
      '\n' +
      '**인스트럭션: 이 이미지의 도움으로, 이미지 내의 시각적 특징들을 명명된 엔티티들과 연결함으로써 입력 텍스트에서 주어진 질문에 답변할 수 있다. 대답은 명명된 개체여야 합니다.**\n' +
      '\n' +
      '**입력:** 이 산이 어느 나라에서 가장 높은 지점인가요?\n' +
      '\n' +
      '**표적**남조선\n' +
      '\n' +
      '**Task:** VOARAD_VQA_in_radiology**\n' +
      '\n' +
      '**설명: 영상의학 이미지(신체 부위 스캔)를 드리겠습니다. 분석하여 입력된 텍스트에 주어진 질문에 답한다.**\n' +
      '\n' +
      '**입력: 환자가 중앙선을 배치했습니까?**\n' +
      '\n' +
      '**Target:** Yes\n' +
      '\n' +
      '**Task:** VOARAD_VQA_in_radiology**\n' +
      '\n' +
      '**설명: 영상의학 이미지(신체 부위 스캔)를 드리겠습니다. 분석하여 입력된 텍스트에 주어진 질문에 답한다.**\n' +
      '\n' +
      '**출력:** 작업의 출력은 각 이미지에 대해 주어진 질문에 대한 답변의 집합이다. 답변은 이미지의 내용과 질문의 범주를 기반으로 생성되어야 한다. 출력은 텍스트 형태로 되어 있어야 합니다.**\n' +
      '\n' +
      '**입력:**Q1: 금속 큐브와 같은 색인 다른 것이 있습니까? Q2: 작은 노란색 금속 물체 앞에 있는 무광택 물체가 어떤 모양을 가지고 있나요? Q3: 노란색 무광택물 뒤에 있는 시안 무광택물 실린더에 남아 있는 시안 무광택물의 크기는 어떻게 되나요? Q4: 작은 노란색 고무 블록 앞에 있는 노란 것이 작은 금속 블록 앞에 있는 작은 것과 같은 모양일까요?\n' +
      '\n' +
      '**Target:**A1: 예 A2: 실린더 A3: 소형 A4: 노 A5: 예 A6: 2A7: 대형 A8: 노 A9: 고무\n' +
      '\n' +
      '**Task:** KVQA_world_knowledge_enabled_VQA**\n' +
      '\n' +
      '**인스트럭션: 사진 및 사진과 관련된 질문을 제공받는다. 당신의 직업은 당신의 배경지식으로 질문에 정확하게 답하는 것이다. 질문들에서 방향들(좌측, 우측 등)에 대한 임의의 참조들은 이미지에 묘사된 사람의 관점에서 나온다는 것에 유의한다. 답변은 개체명으로 구성되어야 합니다.**\n' +
      '\n' +
      '**입력:** 어떤 대륙에서 태어난 인물인가요?\n' +
      '\n' +
      '**타깃 *** 북미지역\n' +
      '\n' +
      '**Task:** VIQAF_knowledge_based_VQA_about_entities**\n' +
      '\n' +
      '**인스트럭션: 이 이미지의 도움으로, 이미지 내의 시각적 특징들을 명명된 엔티티들과 연결함으로써 입력 텍스트에서 주어진 질문에 답변할 수 있다. 대답은 명명된 개체여야 합니다.**\n' +
      '\n' +
      '이 산이 어느 나라에서 가장 높은 곳인가요?\n' +
      '\n' +
      '**표적**남조선\n' +
      '\n' +
      '**Task:** VOARAD_VQA_in_radiology**\n' +
      '\n' +
      '**설명: 영상의학 이미지(신체 부위 스캔)를 드리겠습니다. 분석하여 입력된 텍스트에 주어진 질문에 답한다.**\n' +
      '\n' +
      '**입력: 환자가 중앙선을 배치했습니까?**\n' +
      '\n' +
      '**Target:** Yes\n' +
      '\n' +
      '**Task:** VOARAD_VQA_in_radiology**\n' +
      '\n' +
      '**설명: 영상의학 이미지(신체 부위 스캔)를 드리겠습니다. 분석하여 입력된 텍스트에 주어진 질문에 답한다.**\n' +
      '\n' +
      '**출력:** 태스크는 각 이미지에 대해 주어진 질문에 대한 답변의 집합이다. 답변은 이미지의 내용과 질문의 범주를 기반으로 생성되어야 한다. 출력은 텍스트 형태로 되어 있어야 합니다.**\n' +
      '\n' +
      '**입력:**Q1: 금속 큐브와 같은 색인 다른 것이 있습니까? Q2: 작은 노란색 금속 물체 앞에 있는 무광택 물체가 어떤 모양을 가지고 있나요? Q3: 노란색 무광택물 뒤에 있는 시안 무광택물 실린더에 남아 있는 시안 무광택물의 크기는 어떻게 되나요? Q4: 작은 노란색 고무 블록 앞에 있는 노란 것이 작은 금속 블록 앞에 있는 작은 것과 같은 모양일까요?\n' +
      '\n' +
      '**Target:**A1: 예 A2: 실린더 A3: 소형 A4: 노 A5: 예 A6: 2A7: 대형 A8: 노 A9: 고무\n' +
      '\n' +
      '**Task:** KVQA_world_knowledge_enabled_VQA**\n' +
      '\n' +
      '**인스트럭션: 사진 및 사진과 관련된 질문을 제공받는다. 당신의 직업은 당신의 배경지식으로 질문에 정확하게 답하는 것이다. 질문들에서 방향들(좌측, 우측 등)에 대한 임의의 참조들은 이미지에 묘사된 사람의 관점에서 나온다는 것에 유의한다. 답변은 개체명으로 구성되어야 합니다.**\n' +
      '\n' +
      '**입력:** 어느 대륙에서 탄생한 인물인가요?\n' +
      '\n' +
      '**타깃 *** 북미지역\n' +
      '\n' +
      '**Task:** VIQAF_knowledge_based_VQA_about_entities**\n' +
      '\n' +
      '**인스트럭션: 이 이미지의 도움으로, 이미지 내의 시각적 특징들을 명명된 엔티티들과 연결함으로써 입력 텍스트에서 주어진 질문에 답변할 수 있다. 대답은 명명된 개체여야 합니다.**\n' +
      '\n' +
      '이 산이 어느 나라에서 가장 높은 곳인가요?\n' +
      '\n' +
      '**표적**남조선\n' +
      '\n' +
      '**Task:** VOARAD_VQA_in_radiology**\n' +
      '\n' +
      '**설명: 영상의학 이미지(신체 부위 스캔)를 드리겠습니다. 분석하여 입력된 텍스트에 주어진 질문에 답한다.**\n' +
      '\n' +
      '**입력 : 환자가 중심선이 배치되어 있는가? 도 87\n' +
      '\n' +
      '**Task: GEDMETRY3K_geometry_question_answering Instruction: 몇 가지 기하학적 정보가 있는 도형을 드리겠습니다. 입력된 텍스트의 이미지와 데이터를 분석하여 질문에 답한다. 입력: a=8, b=15, c=17, tan B. Options: (a)2.43(b)1.88(c)1.67(d)1.23 Target: (b)1.88 Task: Iconqa_abstract_diagram_understanding Instruction: 주어진 추상도에 대한 질문이 있는데, 짧은 답변을 주실 수 있나요? 입력: 엘라가 어느 날 아침 자기 침대를 정리하고 있다. 시계가 시간을 보여준다. 지금 몇 시야? 정답은 오전 6시입니다\n' +
      '\n' +
      '**Task: Iconqa_fill_in_blank_in_abstract_diagram_understanding Instruction: Hey, 여기 그것을 설명하는 추상도 및 문장이 있다. 주어진 문장에서 누락된 부분을 채우는 것을 도와줄 수 있나요? 입력: 숫자_가 표시된다. 목표물: 22\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:71]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>