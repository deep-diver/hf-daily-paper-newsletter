<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning\n' +
      '\n' +
      ' Zhiyang Xu\\({}^{\\spadesuit}\\) Chao Feng\\({}^{\\spadesuit}\\) Rulin Shao\\({}^{\\heartsuit}\\) Trevor Ashby\\({}^{\\spadesuit}\\) Ying Shen\\({}^{\\spadesuit}\\)\n' +
      '\n' +
      'Di Jin\\({}^{\\diamond}\\) Yu Cheng\\({}^{\\spadesuit}\\) Qifan Wang\\({}^{\\diamond}\\) Liftu Huang\\({}^{\\spadesuit}\\)\n' +
      '\n' +
      '\\({}^{\\spadesuit}\\)Virginia Tech \\({}^{\\heartsuit}\\)University of Washington \\({}^{\\spadesuit}\\)University of Michigan\n' +
      '\n' +
      '\\({}^{\\diamond}\\)Amazon Inc. \\({}^{\\spadesuit}\\)Microsoft \\({}^{\\diamond}\\)Meta AI\n' +
      '\n' +
      '{zhiyangx,lifuh}@vt.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Despite vision-language models\' (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) _lacking task diversity_ in pretraining and visual instruction tuning, and (2) _annotation error_ and _bias_ in GPT-4 synthesized instruction tuning data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct Vision-Flan, the most diverse publicly available visual instruction tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written instruction. In addition, we propose a two-stage instruction tuning framework, in which VLMs are firstly finetuned on Vision-Flan and further tuned on GPT-4 synthesized data. We find this two-stage tuning framework significantly outperforms the traditional single-stage visual instruction tuning framework and achieves the state-of-the-art performance across a wide range of multi-modal evaluation benchmarks. Finally, we conduct in-depth analyses to understand visual instruction tuning and our findings reveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs\' capabilities but rather modulates the model\'s responses to human-preferred formats; (2) A minimal quantity (e.g., 1,000) of GPT-4 synthesized data can effectively align VLM responses with human-preference; (3) Visual instruction tuning mainly helps large-language models (LLMs) to understand visual features.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Recent vision-language models (VLMs) Liu et al. (2023); Li et al. (2023); Dai et al. (2023), built upon pre-trained large-language models (LLMs) Chiang et al. (2023); Gao et al. (2023) and pretrained image encoders Sun et al. (2023), have shown impressive capabilities as general visual assistants. Besides the unimodal encoders, the main ingredients of these VLM frameworks encompass: (1) a bridging module, such as the MLP layers in the LLaVA model Liu et al. (2023); Li et al. (2023), that establishes connections between the pretrained image encoders and LLMs, (2) large-scale text-image pairs Schuhmann et al. (2022) used for pre-training the bridging module, and (3) GPT-4 synthesized visual instruction tuning datasets Liu et al. (2023); Li et al. (2023) to align the responses of VLMs with human preferences (i.e., following users\' instruction to generate detailed and helpful responses). Despite their notable successes, we identify two remaining challenges that merit further investigation.\n' +
      '\n' +
      'Firstly, the data used in the pre-training stage is dominated by the image captioning task, which lacks diversity, resulting in limited generalizability of VLMs Chen et al. (2023); Zhang et al. (2023). For instance, the LLaVA model Liu et al. (2023) performs poorly on the optical character recognition (OCR) task due to the absence of instances related to text detection during pre-training Zhang et al. (2023). Several recent studies address this problem by further fine-tuning VLMs on instruction tuning datasets covering more tasks Zhang et al. (2023); Hu et al. (2023); Liu et al. (2023) such as visual question answering and OCR but the coverage of the tasks is still limited.\n' +
      '\n' +
      'Secondly, most of the existing visual instruction tuning datasets Liu et al. (2023); Li et al. (2023); Yin et al. (2023) are synthetically generated via GPT-4 by repurposing text annotations such as captions or dense captions from existing computer-vision datasets to generate new tasks, such as visual dialogue, Complex VQA and detail captions. While they enable VLMs to generate fluent and detailed responses aligned with human preferences, the lack of task diversity, spurious co-occurring patterns between objects, and long-form outputs may cause severe hallucination Liu et al. (2023); Liet al., 2023; Liu et al., 2023; Zhou et al., 2023), and catastrophic forgetting - VLMs fail to maintain a similar classification performance on basic detection tasks, such as MNIST (LeCun, 1998) and CIFAR-10 (Krizhevsky et al., 2009), compared to the zero-shot performance of their vision encoders (Zhai et al., 2023).\n' +
      '\n' +
      'To address both challenges, we introduce Vision-Flan, the most diverse public-available visual instruction tuning dataset consisting of 187 tasks drawn from academic datasets, covering _perception_ tasks such as object detection and OCR, _domain-specific_ tasks such as image-quality classification and image-style classification, _complex reasoning_ tasks such as graph understanding and geometric question answering, and many more. Each task in Vision-Flan is accompanied by an expert-written instruction. We show some sample tasks from Vision-Flan in Figure 1 and provide the full list of tasks in Appendix J.\n' +
      '\n' +
      'In addition, we introduce a two-stage instruction tuning framework. In the first stage, we utilize the pre-trained LLaVA model (Liu et al., 2023) as our initial model, and finetune it on Vision-Flan to gain diverse capabilities, resulting in the Vision-Flan Base model. However, due to the concise nature of target outputs in academic datasets, the responses generated by Vision-Flan Base tend to be brief and not aligned with human preferences. Therefore, in the second stage, we further finetune Vision-Flan Base using a minimal amount of GPT-4 synthesized data. This step aims to adjust the model\'s outputs to be more in line with human preferences, resulting in the Vision-Flan Chat model.\n' +
      '\n' +
      'Our experimental results demonstrate that high-quality human annotations from Vision-Flan significantly enhance the capabilities of both Vision-Flan Base and Vision-Flan Chat while reducing the risk of hallucination and catastrophic forgetting. The two-stage instruction tuning framework enables Vision-Flan Chat to achieve better human preference alignment using much less GPT-4 synthesized data compared to the state-of-the-art VLMs. Finally, we perform extensive analysis to understand visual instruction tuning including the roles of human-labeled and GPT-4 synthesized data, and the impacts of various training strategies. Our investigation yields several key insights:\n' +
      '\n' +
      '* Increasing the number of human-labeled tasks in visual instruction tuning can substantially enhance VLMs\' capabilities across extensive evaluation benchmarks.\n' +
      '* GPT-4 synthesized data does not substantially enhance VLMs capabilities and yields marginal improvements in the VLMs\' performance on comprehensive evaluation benchmarks, such as MME (Fu et al., 2023) and\n' +
      '\n' +
      'Figure 1: Sample tasks in Vision-Flan. **Instruction** denotes a task instruction crafted by annotators. **Input** means text input in the given task, and **Target** is the target response based on the instruction.\n' +
      '\n' +
      ' MM-Bench Liu et al. (2023).\n' +
      '* A minimal quantity (1,000) of GPT-4 synthesized data is sufficient to align VLMs\' responses with human preferences. Notably, increasing GPT-4 synthesized data does not correspond to a proportional enhancement in alignment and introduces hallucination and bias into the VLMs.\n' +
      '* Visual instruction tuning mainly enhances the ability of large-language models (LLMs) to process and understand visual features. The training of the bridging module, which maps visual features to the embedding space of LLMs, is predominantly achieved during the pre-training phase.\n' +
      '\n' +
      '## 2 Vision-Flan\n' +
      '\n' +
      '### Collection Pipeline\n' +
      '\n' +
      'We carefully design an annotator selection process to identify qualified annotators, which involves 2 iterations of training and testing. More details of the selection process and compensation can be found in Appendix A.1. In the end, we hire 7 out of 21 candidates as our annotators and all of them are graduate students in computer science. To ensure the diversity and quality of the tasks in Vision-Flan, we design a rigorous annotation pipeline with four major steps:\n' +
      '\n' +
      '**Existing dataset collection and pre-processing:** Two expert researchers (i.e., senior Ph.D. students in the fields of natural language processing and computer vision) search online and identify high-quality vision-language datasets. The datasets are then equally distributed to 7 annotators to download and preprocess the datasets. Each processed instance consists of an image, an instruction (the task definition from the original dataset with minor modifications), a text input if applicable, and a target output.\n' +
      '\n' +
      'Creating new tasks:The two expert researchers and annotators also discuss potential new tasks that could be derived from the existing annotations. We derive new tasks by combining the annotations of two or more existing tasks on a dataset. For example, in the Concadia dataset Kreiss et al. (2022), each instance consists of an image caption and a knowledge snippet related to the image. We propose a new task to predict both the caption and the background knowledge given an image, which is a free-form generation task. The new target output is formed by concatenating the caption with the knowledge snippet. We also develop new tasks by creating more basic versions of the original tasks. For example, given the object detection annotations in MSCOCO Lin et al. (2014), we propose an object selection task in which we provide a list of objects and ask the model to select the object that appears in the image (the negative options are created by sampling objects that appear in other images but not in the given image). The expert researchers and annotators manually solve 20 instances for each newly developed task. If the human predictions match the target outputs, this new task is considered valid.\n' +
      '\n' +
      '**Iteratively refining the task instructions and output templates:** For existing tasks, we ask annotators to write instructions based on the original task definitions with minor modifications. For newly developed tasks, the annotators write instructions by discussing with the expert researchers. Once an annotator finishes writing a new instruction, one of the two expert researchers is randomly assigned to examine the instances and provide feedback for revising the instruction. This step iterates repeatedly until the instruction meets our requirements. We require the instruction to be _clear_, _easy to understand_, and can _be correctly executed by a human_. Each task together with its associated dataset and instruction is then added to the pool of candidate tasks for Vision-Flan.\n' +
      '\n' +
      '**Verifying the quality of each task:** From the candidate task pool, two expert researchers, including a native English speaker, work together to select the high-quality tasks where the instruction is fluent and effectively conveys the intended task and the task does not overlap with other tasks.\n' +
      '\n' +
      'Based on these four steps, we finally collect 187 high-quality tasks, and for each task, we randomly sample 10,000 instances from its corresponding dataset. If a dataset contains less than 10,000 instances, we include all of them. We name the dataset as Vision-Flan, consisting of 1,664,261 instances for 187 tasks in total. We include references to all the datasets used in Vision-Flan in Appendix H and show an instance for each task in Appendix J.\n' +
      '\n' +
      '### Comparison with Existing Datasets\n' +
      '\n' +
      'Table 1 presents a comparison between existing visual instruction tuning datasets and Vision-Flan. For existing visual instruction tuning datasets, we\n' +
      '\n' +
      'directly adopt the numbers of tasks and instances reported in their original papers. The majority of these datasets are generated using proprietary language models, such as ChatGPT1 and GPT-42, and exhibit a narrow range of task diversity. VL-Qwen Bai et al. (2023) is a recently introduced large-scale dataset annotated by humans but remains inaccessible to the public. Although Multi-Instruct Xu et al. (2023) is based on publicly available datasets, it mainly focuses on visual grounding tasks and only contains 29 tasks that do not involve region-specific information. In contrast, Vision-Flan encompasses a significantly more diverse array of tasks, offering a three-times increase compared to the number of tasks in MultiInstruct.\n' +
      '\n' +
      'Footnote 1: [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)\n' +
      '\n' +
      'Footnote 2: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)\n' +
      '\n' +
      'In Figure 2, we compare the task categories covered by Vision-Flan and other datasets. Tasks within Vision-Flan are first categorized into three primary groups: _Question Answering_, _Classification_, and _Generation_, and each of these primary groups is further divided into specific, fine-grained categories. For instance, within the _Classification_ group, the _General Object_ category involves classifying objects in images into various concepts, such as "fish", "car", and "dog". Contrastingly, the _Vehicle Model_ category demands the models to accurately identify specific car brands or models, like "Toyota" and "Camry". The visualization in Figure 2 clearly demonstrates the superior diversity and volume of tasks in Vision-Flan compared to existing datasets. We list tasks in each category in Appendix I.\n' +
      '\n' +
      '## 3 Vision-Flan Finetuning\n' +
      '\n' +
      'Model ArchitectureWe adopt the same VLM architecture as LLaVA Liu et al. (2023) and denote it as LLaVA-Architecture. As shown in Figure 3, it consists of a pre-trained vision encoder, a pre-trained large language model, and two layers of MLPs to connect them. In the vision-language pre-training phase of the LLaVA-Architecture, both the pre-trained vision encoder and large language model remain frozen, and only the MLP layers are trained on a large-scale image captioning dataset Schuhmann et al. (2022). We leverage this pre-trained LLaVA model, without any visual instruction tuning, as our initial model and finetune it on Vision-Flan. During visual instruction tuning, we finetune both the MLP layers and the language model while keeping the vision encoder frozen.\n' +
      '\n' +
      'Two-stage Visual Instruction TuningContrary to prior approaches Liu et al. (2023); Dai et al. (2023) that mix human-labeled data with GPT-4 synthesized data for visual instruction tuning, our study introduces a two-stage instruction tuning pipeline. As shown in Figure 3, in the first stage, we fine\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Dataset** & Instances \\# & Tasks \\# & Source \\\\ \\hline LLaVA Liu et al. (2023) & 150K & 3 & Synthetic \\\\ LAMM Yin et al. (2023) & 196K & 8 & Synthetic \\\\ VL-Qwen Bai et al. (2023) & 350K & Unknown & Private \\\\ M\\({}^{\\dagger}\\)IT Li et al. (2023) & 2.4M & 40 & Synthetic \\\\ mPu-Qw Yeh et al. (2023) & 150K & 3 & Synthetic \\\\ Shikra Chen et al. (2023) & 156K & 4 & Synthetic \\\\ SVIT Zhao et al. (2023) & 4.2M & 4 & Synthetic \\\\ MultiInstruct Xu et al. (2023) & 510K & 62 & Public \\\\ Vision-Flan (Ours) & 1.6M & 196 & Public \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Comparison between Vision-Flan and existing visual instruction tuning datasets.\n' +
      '\n' +
      'Figure 2: Comparison of task diversity between Vision-Flan and previous visual instruction tuning datasets. LLaVA and SVIT report very coarse-grained categories of tasks. Each circle represents a task category and the radius is proportional to the number of tasks in that category. The radius of circles for different datasets are comparable.\n' +
      '\n' +
      'tune the VLM on all 187 tasks of Vision-Flan to acquire diverse capabilities and name the resulting model as Vision-Flan Base. However, due to the brevity of target outputs presented in academic datasets, the responses from Vision-Flan Base are not in human-preferred formats. Hence, we further finetune Vision-Flan Base on GPT-4 synthesized data to align the model\'s outputs with human preference. We denote the yielded model as Vision-Flan Chat. This training framework requires minimal GPT-4 synthesized data while providing deep insights into the distinct contributions of human-labeled and GPT-4 synthesized data in visual instruction tuning.\n' +
      '\n' +
      'Implementation DetailsWe leverage LLaVA-Architecture with Vicuna-13B v1.5 [14], CLIP-ViT-L-336px [20] and two layers of MLP as our VLM. For the first-stage instruction tuning, we finetune the MLP layers and the language model on Vision-Flan for 1 epoch with a learning rate 2e-5 and per device batch size 16 on 8 A100 GPUs. For the second-stage instruction tuning, we further finetune the MLP layers and the language model on 1,000 instances randomly sampled from the LLaVA dataset [13] with learning rate 1e-5 and per device batch size 8 on 8 GPUs for 128 steps. In the following sections, we use LLaVA dataset and GPT-4 synthesized data interchangeably.\n' +
      '\n' +
      '## 4 Experiment Setup\n' +
      '\n' +
      'Evaluation DatasetsWe evaluate the models on several widely adopted multimodal evaluation benchmark datasets including _multiple-choice_ benchmarks: **MMbench**[15], **MME**[11], and **MMMU**; _free-form generation_ benchmarks: **MM-Vet**[11] and **LLaVA-Bench**; the _hallucination_ benchmark: **POPE**[15], and _catastrophic forgetting_ benchmarks: **CIFAR-10 and CIFAR-100**[16], **MNIST**[12], and **miniImageNet**[13]. More details of the evaluation datasets can be found in Appendix B.\n' +
      '\n' +
      'Evaluation ProtocolsFor MMbench, MME, MM-Vet, LLaVA-Bench, POPE and MMMU, we strictly follow their official implementations of evaluation code to evaluate the performance of each model. For datasets that do not have official evaluation codes including CIFAR-10, CIFAR-100, MNIST, and miniImageNet, we leverage the state-of-the-art open-source LLM, Vicuna 1.5 13B, to perform the evaluation and report the averaged performance on these four datasets in the CF column in Table 2. More details of evaluation protocols can be found in Appendix C.\n' +
      '\n' +
      'BaselinesWe compare our models with several recent state-of-the-art vision-language models, including **BLIP-2**[15], **Instruct-BLIP**[15], **Shikra**[17], **LLaVA**[15], **Qwen-VL**, **Qwen-VL-Chat**[14], and **LLaVA-1.5**[15]. The LLMs and image encoders used in all baselines are shown in Table 2. Details of baselines can be found in Appendix D.\n' +
      '\n' +
      '## 5 Results and Analysis\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      'As demonstrated in Table 2, Vision-Flan Base achieves state-of-the-art performance on comprehensive evaluation benchmarks including MME, MM-Bench and MMMU, while reducing hallucination and catastrophic forgetting. However, we observe that Vision-Flan Base scores significantly lower on the LLaVA-Bench dataset in comparison to VLMs trained using GPT-4 synthesized data. We attribute this discrepancy to the conciseness and brevity of target outputs within academic datasets. As shown in Figure 1, VQA tasks frequently yield outputs comprising a single or a few words. Even outputs of many generation tasks are typically confined to one or two succinct sentences. Training on these tasks leads Vision-Flan Base to generate brief responses, which are not aligned with human preferences. Conversely, through the second-stage tuning on a mere 1,000 GPT-4 synthesized data instances, Vision-Flan Chat achieves significant performance improvement on LLaVA-Bench,\n' +
      '\n' +
      'Figure 3: The left of the figure shows the LLaVA-Architecture and the right of the figure shows the two-stage visual instruction tuning pipeline.\n' +
      '\n' +
      'a benchmark measuring human-preference alignment, while maintaining a relatively lower rate of hallucination and catastrophic forgetting. To better understand why Vision-Flan models are better than current VLMs, we conduct two case studies focusing on OCR and Entity Recognition and analyze both quantitative and qualitative results in Appendix E.2.\n' +
      '\n' +
      'Another finding in Table 2 is that compared to Vision-Flan Base, Vision-Flan Chat achieves slightly inferior performance on comprehensive evaluation benchmarks demonstrating the bias and hallucination inevitably introduced by the GPT-4 synthesized data, which is discussed in detail in Section 5.2.\n' +
      '\n' +
      '### Effect of Human-Labeled and GPT-4 Synthesized Datasets\n' +
      '\n' +
      'Effect of Task Diversity in Vision-FlanFigure 4 illustrates the relationship between the number of tasks from Vision-Flan employed during visual instruction tuning and the performance of Vision-Flan Base across four comprehensive evaluation benchmarks. It\'s apparent that as the number of tasks increases, the performance of Vision-Flan Base on all datasets is improved. To evaluate the impact of varying numbers of instances from different tasks, we fix the total amount of instances used for visual instruction tuning and experiment with different numbers of tasks. As demonstrated in Table 3, when the number of training instances is constant, augmenting the number of tasks significantly enhances model performance. These findings substantiate our hypothesis that _the diverse array of human-labeled tasks within_ Vision-Flan _is essential for improving the capabilities of VLMs_.\n' +
      '\n' +
      'Effect of GPT-4 Synthesized Data on Comprehensive Evaluation BenchmarksFurthermore, we analyze if GPT-4 synthesized data can improve the model\'s performance on comprehensive evaluation benchmarks and show the results in Figure 5. Further tuning Vision-Flan Base on GPT-4 synthesized data instances does not lead to performance improvement. Tuning pretrained LLaVA model on a small amount of GPT-4 synthesized data (100) can improve its performance on MME but further increasing the number of training instances does not lead to any improvement. We also observe a similar trend on the MM-Bench dataset and report the result in Appendix E.1. These observations are in line with recent findings in LLMs: _GPT-4 synthesized data does not improve model\'s capability but rather modulates the responses towards human-preferred formats_Jain et al. (2023); G\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline\n' +
      '**Model** & LLM & Image Encoder & MM-Bench & MME & MMMU & LLaVA-Bench & MM-Vet & Pope & CF \\\\ \\hline BLIP-2 & FlanT5-XXL & ViT-g/14 & - & 1293.8 & 34.0 & - & 22.4 & 85.3 & - \\\\ InstructBlip & Vicuna-13B & ViT-g/14 & 36.0 & 1212.8 & 33.8 & 58.2 & 25.6 & 78.9 & - \\\\ Mini-GPT4 & Vicuna-13B & ViT-g/14 & 24.3 & 581.67 & 27.6 & - & - & - & - \\\\ Shixra & Vicuna-13B & ViT-g/14 & 58.8 & - & - & - & - & - & - \\\\ LLaVA & Vicuna-13B v1.5 & CLIP-ViT-336px & 38.7 & 1151.6 & - & 70.8 & 33.4 & 75.3 & - \\\\ Qwen-VL & Qwen-7B & ViT-bigG & 38.2 & - & - & - & - & - & - \\\\ Qwen-VL-Chat & Qwen-7B & ViT-bigG & 60.6 & 1487.5 & 32.9 & 73.6 & - & - & 72.1 \\\\ LLaVA 1.5 & Vicuna-13B v1.5 & CLIP-ViT-L-336px & 66.7 & 1531.3 & 33.6 & 70.7 & 35.4 & 83.6 & 73.3 \\\\ \\hline \\hline Vision-Flan Base & Vicuna-13B v1.5 & CLIP-ViT-L-336px & **69.8** & **1537.8** & **34.4** & 38.5 & 33.4 & 85.9 & **87.2** \\\\ \\hline \\hline \\multicolumn{10}{l}{**Second-Stage Tuning with 1,000 GPT-4 Synthesized Instances**} \\\\ Vision-Flan Chat & Vicuna-13B v1.5 & CLIP-ViT-L-336px & 67.6 & 1490.6 & 34.3 & **78.3** & **38.0** & **86.1** & 84.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Comprehensive evaluation of VLMs on widely adopted benchmark datasets. CF denotes the averaged performance of VLMs on four catastrophic forgetting benchmarks.\n' +
      '\n' +
      'Figure 4: Performance on four comprehensive benchmarks versus the number of training tasks.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline \\# of Tasks & \\# of Instances per Task & MMB & MME & Pope & MMMU \\\\ \\hline\n' +
      '**Training with 100,000 Instances** & & & & \\\\ \\hline\n' +
      '10 & 10,000 & 58.3 & 723.9 & 81.0 & 32.6 \\\\\n' +
      '187 & 500 & 58.8 & 1314.3 & 83.3 & 33.3 \\\\\n' +
      '**Training with 200,000 Instances** & & & & \\\\ \\hline\n' +
      '20 & 10,000 & 58.8 & 897.3 & 83.4 & 31.8 \\\\\n' +
      '187 & 1,000 & 63.5 & 1373.5 & 83.6 & 33.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Comparison of Vision-Flan Base trained with a fixed total amount of data instances.\n' +
      '\n' +
      'Effect of GPT-4 Synthesized Data on Human-Preference AlignmentWhen utilizing our proposed two-stage tuning framework, we find that by performing a second-stage finetuning on a mere 1,000 GPT-4 synthesized instances from the LLaVA dataset, Vision-Flan Chat achieves significantly better performance (78.5 v.s. 38.5) on the LLaVA-Bench dataset. This observation leads us to raise the question: _Is extensive finetuning on large-scale GPT-4 synthesized datasets necessary for aligning VLMs with human preferences?_ To answer it, we finetune both Vision-Flan Base and pretrained LLaVA model on different numbers of GPT-4 synthesized instances ranging from 100 to 158,000, and show the results in Figure 6. As we can see, with 1,000 instances, Vision-Flan Base achieves a score of 78.3 and further increasing the number of training instances leads to a performance drop. A similar trend can also be seen for the pretrained LLaVA model.\n' +
      '\n' +
      'GPT-4 Synthesized Data Causes Hallucination and BiasConcurrent work [14] identifies that hallucination in current VLMs can be caused by their bias toward positive answers (i.e., "Yes"). In Figure 7, we explicitly show the relationship between hallucination, the ratio of "Yes", and the number of training instances from GPT-4 synthesized dataset. As the number of GPT-4 synthesized instances increases, the model\'s responses are biased towards the answer "Yes" even if the objects are not in the images, causing the model to hallucinate. This observation suggests that a small amount of GPT-4 synthesized training instances is preferred to avoid hallucination and bias in VLMs.\n' +
      '\n' +
      '### Single-stage Tuning on Mixed Data Vs. Two-stage Tuning\n' +
      '\n' +
      'In this section, we compare the performance of two training strategies based on the same pretrained LLaVA model: (1) finetuning it on the mix of Vision-Flan and the LLaVA dataset; (2) finetuning it utilizing Vision-Flan and 1,000 instances from the LLaVA dataset with our two-stage tuning method. As illustrated in Table 4, the performance of VLMs finetuned on the mix of Vision-Flan and GPT-4 synthesized data is notably inferior compared to Vision-Flan Chat trained through our two-stage tuning framework.\n' +
      '\n' +
      '### What is Essentially Improved in VLMs during Visual Instruction Tuning\n' +
      '\n' +
      'In LLaVA-Architecture, the MLP layers map the visual features from a vision encoder into the embedding space of LLMs. The LLMs then interpret the visual features and follow text instructions to generate responses. In Table 5, we show the results of training different modules during visual instruction tuning and observe that solely tuning MLPs causes\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & \\# of LLaVA & MME & LLaVA-Bench & MM-Vet \\\\ \\hline Mixed Data & 1,000 & 1364.0 & 52.7 & 36.6 \\\\ Mixed Data & 158,000 & 1317.9 & 63.9 & 36.8 \\\\ Two-stage & 1,000 & 1490.6 & 78.3 & 38.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Comparison between single-stage finetuning on mixed data and two-stage finetuning.\n' +
      '\n' +
      'Figure 5: Effect of the number of GPT-4 synthesized training instances on MME. The dashed gray line indicates the performance of LLaVA 1.5.\n' +
      '\n' +
      'Figure 6: Effect of the number of GPT-4 synthesized instances on human preference alignment. The dashed gray line indicates the performance of LLaVA 1.5.\n' +
      '\n' +
      'Figure 7: Effect of the number of GPT-4 synthesized training instances on the hallucination benchmark and the ratio of “Yes”. The dashed lines indicate the performance of the state-of-the-art LLaVA 1.5 model.\n' +
      '\n' +
      'a significant performance drop compared to tuning both MLPs and LLMs during visual instruction tuning. However, tuning LLMs with frozen MLPs results in similar performance as tuning both modules, demonstrating that visual instruction tuning mainly enables LLMs to better understand visual features while MLPs have been sufficiently learned during pretraining. To further support this claim, we replace the instruction-tuned MLPs in Vision-Flan Base and Vision-Flan Chat with the pre-trained LLaVA model, and show that with the pretrained MLPs, both models can retain more than 90% of performance on most tasks as shown in Table 6. We also compute the Pearson Correlation Coefficient between the parameters of pretrained MLPs and instruction-tuned MLPs, and find that their correlation coefficient is higher than 0.99.\n' +
      '\n' +
      '## 6 Related Work\n' +
      '\n' +
      'Instruction tuning Wei et al. (2022) is first introduced in NLP and has been adapted to the visual-language domain. MultiInstruct Xu et al. (2023) propose the first human-label multi-modal instruction tuning dataset for improving the zero-shot performance of pre-trained VLMs. LLaVA Liu et al. (2023) leverage GPT-4 to repurpose text annotations such as captions or dense captions from existing computer-vision datasets to generate visual dialogues, Complex VQA and detail captions for visual instruction tuning. Following LLaVA, mPLUG-Owl Ye et al. (2023), LAMM Yin et al. (2023), MIMIC-IT Li et al. (2023) and MacawLLM Lyu et al. (2023) leverage proprietary LLMs such as GPT-4 and ChatGPT to further extend the instruction tuning tasks into 3D-domain, multiple-images and videos, and increase the amount of training instances. MiniGPT-4 Zhu et al. (2023) utilizes ChatGPT to refine output from the pretrained VLM itself. InstructBLIP Dai et al. (2023) and LLaVA-1.5 Liu et al. (2023) mix the human-annotated and GPT4 synthesized datasets to enhance visual instruction tuning.\n' +
      '\n' +
      'Several recent work explores different strategies to improve visual instruction tuning. StableLLaVA Li et al. (2023) and VPG-C Li et al. (2023) generate both images and texts using Stable Diffusion Rombach et al. (2022) or Blended Diffusion Avrahami et al. (2022) to alleviate domain bias and encourage VLMs attend to visual details. Liu et al. (2023) demonstrate the bias introduced by positive instructions and introduce negative instruction examples for improving robustness. Shikra Chen et al. (2023) incorporate visual grounding tasks in visual instruction tuning to improve the VLM\'s referential capability. LLaVAR Zhang et al. (2023) and BLIVA Hu et al. (2023) leverage OCR tools and GPT-4 to generate tasks helping VLMs to understand text in images. Lu et al. (2023) and SVIT Zhao et al. (2023) empirically study the effect of scaling the size of VLMs and the size of GPT-4 synthesized dataset. Two concurrent works Wang et al. (2023); Chen et al. (2023) directly prompt GPT-4V with images as input to generate visual instruction tuning data and achieve superior performance. Additional related work can be found in Appendix G.\n' +
      '\n' +
      'Unlike all prior work, our work mainly focuses on scaling human-labeled tasks in visual instruction tuning to improve VLMs\' capabilities. Additionally, we perform extensive analysis to understand the characteristics of human-labeled and GPT-4 synthesized data and draw meaningful conclusions.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'We construct Vision-Flan, the most diverse public-available visual instruction tuning dataset, consisting of 187 diverse tasks and 1,664,261 instances collected from academic datasets, and each task is accompanied by an expert-written instruction. We demonstrate that VLMs trained on Vision-Flan with proposed two-stage tuning framework achieve state-of-the-art performance on comprehensive evaluation benchmarks. Additionally, we perform extensive analysis and reveal the\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Model** & MMB & MME & LLaVA-Bench & Pope \\\\ \\hline\n' +
      '**Vision-Flan Base** & 69.8 & 1537.8 & 38.5 & 85.9 \\\\ \\hline\n' +
      '**+**Pretrained MLP** & 68.0 & 1403.1 & 36.4 & 84.0 \\\\\n' +
      '**Vision-Flan Chat** & 67.6 & 1490.6 & 78.3 & 86.1 \\\\\n' +
      '**+**Pretrained MLP** & 65.7 & 1332.2 & 73.8 & 85.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Results of replacing visual instruction tuned MLPs with pretrained MLPs. Gray rows show the performance of the original models and yellow rows show the performance after replacing instruction-tuned MLPs with pretrained MLPs.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline LLM & MLPs & MM-Bench & MME & LLaVA-Bench & Pope \\\\ \\hline ✗ & ✗ & 45.0 & 936.3 & 32.4 & 51.9 \\\\ ✗ & ✓ & 52.4 & 1107.3 & 39.1 & 83.3 \\\\ ✓ & ✗ & 69.2 & 1495.5 & 39.3 & 85.6 \\\\ ✓ & ✓ & 69.8 & 1537.8 & 38.5 & 85.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Effect of tuning different modules in Vision-Flan Base. ✓ denotes the module is tuned and ✗ denotes the module is frozen during visual instruction tuning.\n' +
      '\n' +
      'distinct contributions of human-labeled and GPT-4 synthesized data in visual instruction tuning.\n' +
      '\n' +
      '## 8 Limitations\n' +
      '\n' +
      'All the tasks included in Vision-Flan are in English, which confines the usage of our dataset and models to English speaking populations. Future work should extend Vision-Flan with multilingual tasks. In addition, all the tasks in Vision-Flan only consists of a single image. Many real-world vision-language tasks require the model to take multiple images as inputs. Thus, future work should explore vision-language tasks that involve multiple images or videos.\n' +
      '\n' +
      'Our analysis mainly focuses on the GPT-4 synthesized visual instruction tuning dataset. Recently, as the GPT-4V 3 becomes publicly available, there are some concurrent works [22, 23] prompting GPT-4V with images as inputs to generate visual instruction tuning data. Future work can analyze the effect of tuning VLMs on such datasets and identify the advantages and disadvantages.\n' +
      '\n' +
      'Footnote 3: [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card)\n' +
      '\n' +
      'In our experiments, we mainly focus on the LLaVA-Architecture [10] due to its strong performance and high efficiency. However, there are other foundation architectures such as Q-former in BLIP2 [11] and Perceiver Resampler in Flamingo [1]. More diverse VLM architectures can be explored in the future to draw more general conclusions.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* November 2, 2019, pp. 8947-8956. Cited by: SS1.\n' +
      '* [2]J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. L. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan (2022) Flamingo: a visual language model for few-shot learning. Cited by: SS1.\n' +
      '* [3]S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and D. Parikh (2015) Vqa: visual question answering. In Proceedings of the IEEE international conference on computer vision, pp. 2425-2433. Cited by: SS1.\n' +
      '* [4]O. Avrahami, D. Lischinski, and O. Fried (2022) Blended diffusion for text-driven editing of natural images. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 18187-18197. Cited by: SS1.\n' +
      '* [5]J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin, K. Lin, D. Liu, G. Liu, C. Lu, K. Lu, J. Ma, R. Men, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang, S. Wu, B. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan, Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Y. Zhang, C. Zhou, J. Zhou, X. Zhou, and T. Zhu (2023) Qwen technical report. CoRRabs/2309.16609. External Links: 2309.16609 Cited by: SS1.\n' +
      '* [6]J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou (2023) Qwen-vl: a versatile vision-language model for understanding, localization, text reading, and beyond. Cited by: SS1.\n' +
      '* [7]J. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz (2019) ObjectNet: a large-scale bias-controlled dataset for pushing the limits of object recognition models. In Advances in Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 9448-9458. External Links: 1905.02192 Cited by: SS1.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      'Proceedings of the IEEE International Conference on Computer Vision_.\n' +
      '* Chen et al. (2023a) Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. 2023a. Shikra: Unleashing multimodal llm\'s referential dialogue magic. _CoRR_, abs/2306.15195.\n' +
      '* Chen et al. (2023b) Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023b. Sharegpt4v: Improving large multi-modal models with better captions. _CoRR_, abs/2311.12793.\n' +
      '* Chen et al. (2023c) Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. 2023c. Can pre-trained vision and language models answer visual information-seeking questions? pages 14948-14968.\n' +
      '* Chen et al. (2020) Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation learning. In _European conference on computer vision_, pages 104-120. Springer.\n' +
      '* Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Liamin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.\n' +
      '* Chng et al. (2020) Chee-Kheng Chng, Chee Seng Chan, and Cheng-Lin Liu. 2020. Total-text: toward orientation robustness in scene text detection. _Int. J. Document Anal. Recognit._, 23(1):31-52.\n' +
      '* Chua et al. (2009) Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yantao Zheng. 2009. NUS-WIDE: a real-world web image database from national university of singapore. In _Proceedings of the 8th ACM International Conference on Image and Video Retrieval, CIVR 2009, Santorini Island, Greece, July 8-10, 2009_. ACM.\n' +
      '* Cimpoi et al. (2014) M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed,, and A. Vedaldi. 2014. Describing textures in the wild. In _Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_.\n' +
      '* Coates et al. (2011) Adam Coates, Andrew Y. Ng, and Honglak Lee. 2011. An analysis of single-layer networks in unsupervised feature learning. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Laudendale, USA, April 11-13, 2011_, volume 15 of _JMLR Proceedings_, pages 215-223. JMLR.org.\n' +
      '* Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. 2023. Instructible: Towards general-purpose vision-language models with instruction tuning. _CoRR_, abs/2305.06500.\n' +
      '* Darlow et al. (2018) Luke Nicholas Darlow, Elliot J. Crowley, Antreas Antoniou, and Amos J. Storkey. 2018. CINIC-10 is not imagenet or CIFAR-10. _CoRR_, abs/1810.03505.\n' +
      '* Das et al. (2019) Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Stefan Lee, Jose M. F. Moura, Devi Parikh, and Dhruv Batra. 2019. Visual dialog. volume 41, pages 1242-1256.\n' +
      '* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 4171-4186. Association for Computational Linguistics.\n' +
      '* Eitz et al. (2012) Mathias Eitz, James Hays, and Marc Alexa. 2012. How do humans sketch objects? _ACM Trans. Graph. (Proc. SIGGRAPH)_, 31(4):44:1-44:10.\n' +
      '* Everingham et al. (2010) Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. 2010. The pascal visual object classes (VOC) challenge.\n' +
      '* Farhadi et al. (2009) Ali Farhadi, Ian Endres, Derek Hoiem, and David A. Forsyth. 2009. Describing objects by their attributes. In _2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA_, pages 1778-1785. IEEE Computer Society.\n' +
      '* Fei-Fei et al. (2004) Li Fei-Fei, Rob Fergus, and Pietro Perona. 2004. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. page 178.\n' +
      '* Fu et al. (2023) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. 2023. MME: A comprehensive evaluation benchmark for multimodal large language models. _CoRR_, abs/2306.13394.\n' +
      '* Ganin et al. (2017) Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor S. Lempitsky. 2017. Domain-adversarial training of neural networks. In Gabriela Csurka, editor, _Domain Adaptation in Computer Vision Applications_, Advances in Computer Vision and Pattern Recognition, pages 189-209. Springer.\n' +
      '* Gao et al. (2023) Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. 2023. Llama-adapter V2: parameter-efficient visual instruction model. _CoRR_, abs/2304.15010.\n' +
      '* ECCV 2018\n' +
      'Workshops - Munich, Germany, September 8-14, 2018, Proceedings, Part II_, volume 11130 of _Lecture Notes in Computer Science_, pages 676-691. Springer.\n' +
      '* Geirhos et al. (2019) Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. 2019. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net.\n' +
      '* Goyal et al. (2017) Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6904-6913.\n' +
      '* Griffin et al. (2007) Gregory Griffin, Alex Holub, and Pietro Perona. 2007. Caltech-256 object category dataset.\n' +
      '* Gudibande et al. (2023) Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. The false promise of imitating proprietary lms. _CoRR_, abs/2305.15717.\n' +
      '* Guillaume Jaume et al. (2019) Jean-Philippe Thiran Guillaume Jaume, Hazim Kemal Ekenel. 2019. Funsd: A dataset for form understanding in noisy scanned documents. In _Accepted to ICDAR-OST_.\n' +
      '* ECCV 2020\n' +
      '- 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVII_, volume 12362 of _Lecture Notes in Computer Science_, pages 417-434. Springer.\n' +
      '* Hendrycks et al. (2021a) Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. 2021a. The many faces of robustness: A critical analysis of out-of-distribution generalization. pages 8320-8329.\n' +
      '* Hendrycks and Dietterich (2019) Dan Hendrycks and Thomas G. Dietterich. 2019. Benchmarking neural network robustness to common corruptions and perturbations.\n' +
      '* Hendrycks et al. (2021b) Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. 2021b. Natural adversarial examples. pages 15262-15271.\n' +
      '* Van Horn et al. (2018) Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alexander Shepard, Hartwig Adam, Pietro Perona, and Serge J. Belongie. 2018. The inaturalist species classification and detection dataset.\n' +
      '* Horn et al. (2015) Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge J. Belongie. 2015. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015_, pages 595-604. IEEE Computer Society.\n' +
      '* Hou et al. (2021) Qiang Hou, Weiqing Min, Jing Wang, Sujuan Hou, Yuanjie Zheng, and Shuqiang Jiang. 2021. Foodogodet-1500: A dataset for large-scale food logo detection via multi-scale feature decoupling network. In _Proceedings of the 29th ACM International Conference on Multimedia_, pages 4670-4679.\n' +
      '* Hsu et al. (2021) Ting-Yao Hsu, C. Lee Giles, and Ting-Hao Kenneth Huang. 2021. Scicap: Generating captions for scientific figures. pages 3258-3264.\n' +
      '* Hu et al. (2023) Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, and Zhuowen Tu. 2023. BLIVA: A simple multimodal LLM for better handling of text-rich visual questions. _CoRR_, abs/2308.09936.\n' +
      '* Huang et al. (2007) Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. 2007. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst.\n' +
      '* Hudson and Manning (2019) Drew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6700-6709.\n' +
      '* Hwang and Shwartz (2023) EunJeong Hwang and Vered Shwartz. 2023. MemeCap: A dataset for captioning and interpreting memes.\n' +
      '* Jain et al. (2023) Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rocktaschel, and David Scott Krueger. 2023. Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks.\n' +
      '* Johnson et al. (2017) Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. 2017. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In _2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017_, pages 1988-1997. IEEE Computer Society.\n' +
      '* Kafle et al. (2018) Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. 2018. Dvqa: Understanding data visualizations via question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5648-5656.\n' +
      '* 20, 2011_, page 20. ACM.\n' +
      '* Karkkanen and Joo (2021) Kimmo Karkkanen and Jungseock Joo. 2021. Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In _IEEEWinter Conference on Applications of Computer Vision, WACV 2021, Waikoloa, HI, USA, January 3-8, 2021_, pages 1547-1557. IEEE.\n' +
      '* ECCV 2016\n' +
      '- 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV_, volume 9908 of _Lecture Notes in Computer Science_, pages 235-251. Springer.\n' +
      '* Khosla et al. (2011) Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. 2011. Novel dataset for fine-grained image categorization. In _First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition_, Colorado Springs, CO.\n' +
      '* Kirstain et al. (2023) Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. 2023. Pick-a-pic: An open dataset of user preferences for text-to-image generation. volume abs/2305.01569.\n' +
      '* Krause et al. (2013) Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 2013. 3d object representations for fine-grained categorization. In _2013 IEEE International Conference on Computer Vision Workshops, ICCV Workshops 2013, Sydney, Australia, December 1-8, 2013_, pages 554-561. IEEE Computer Society.\n' +
      '* Kreiss et al. (2022) Elisa Kreiss, Fei Fang, Noah D. Goodman, and Christopher Potts. 2022. Concaldia: Towards image-based text generation with a purpose.\n' +
      '* Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _International journal of computer vision_, 123:32-73.\n' +
      '* Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images.\n' +
      '* Kumar et al. (2022) Anurendra Kumar, Keval Morabia, William Wang, Kevin Chang, and Alex Schwing. 2022. CoVA: Context-aware visual attention for webpage information extraction. In _Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5)_, pages 80-90, Dublin, Ireland. Association for Computational Linguistics.\n' +
      '* Lau et al. (2019) Jason J Lau, Soumya Gayen, Dina Demner, and Asma Ben Abacha. 2019. Visual question answering in radiology (vqa-rad).\n' +
      '* LeCun (1998) Yann LeCun. 1998. The mnist database of handwritten digits. _http://yann. lecun. com/exdb/mnist/_.\n' +
      '* 15, 2022_, pages 3108-3120. ACM.\n' +
      '* Li et al. (2023a) Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. 2023a. MIMIC-IT: multi-modal in-context instruction tuning. _CoRR_, abs/2306.05425.\n' +
      '* Li et al. (2023b) Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023b. Otter: A multi-modal model with in-context instruction tuning. _CoRR_, abs/2305.03726.\n' +
      '* Li et al. (2017) Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. 2017. Deeper, broader and artier domain generalization. In _IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017_, pages 5543-5551. IEEE Computer Society.\n' +
      '* Li et al. (2023) Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, and Yueting Zhuang. 2023c. Fine-tuning multimodal llms to follow zero-shot demonstrative instructions.\n' +
      '* Li et al. (2023) Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023d. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. 202:19730-19742.\n' +
      '* Li et al. (2023) Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu. 2023e. M\\({}^{3}\\)it: A large-scale dataset towards multi-modal multilingual instruction tuning. _CoRR_, abs/2306.04387.\n' +
      '* Li et al. (2019) Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. Visualbert: A simple and performant baseline for vision and language. _CoRR_, abs/1908.03557.\n' +
      '* Li et al. (2018) Qing Li, Qingyi Tao, Shafiq R. Joty, Jianfei Cai, and Jiebo Luo. 2018. VQA-E: explaining, elaborating, and enhancing your answers for visual questions. 11211:570-586.\n' +
      '* Li and Deng (2019) Shan Li and Weihong Deng. 2019. Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition. _IEEE Transactions on Image Processing_, 28(1):356-370.\n' +
      '* Li et al. (2023) Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin Fu, Guosheng Lin, Chunhua Shen, Ling Chen, and Yunchao Wei. 2023f. Stabellava: Enhanced visual instruction tuning with synthesized image-dialogue data. _CoRR_, abs/2308.10253.\n' +
      '* Li et al. (2023) Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023g. Evaluating object hallucination in large vision-language models. pages 292-305.\n' +
      '\n' +
      'T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. Lawrence Zitnick (2014)Microsoft COCO: common objects in context. In Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, Vol. 8693 of Lecture Notes in Computer Science, pp. 740-755. Cited by: SS1.\n' +
      '* Z. Lin, X. Chen, D. Pathak, P. Zhang, and D. Ramanan (2023)Revisiting the role of language priors in vision-language models. External Links: Link Cited by: SS1.\n' +
      '* November 2, 2019, pp. 2152-2161. Cited by: SS1.\n' +
      '* F. Liu, T. Guan, Z. Li, L. Chen, Y. Yacoob, D. Manocha, and T. Zhou (2023)HallusionBench: you see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v(ision), llava-1.5, and other multi-modality models. CoRRabs/2310.14566. External Links: Link Cited by: SS1.\n' +
      '* F. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang (2023)Aligning large multi-modal model with robust instruction tuning. CoRRabs/2306.14565. External Links: Link Cited by: SS1.\n' +
      '* F. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang (2023)Mitigating hallucination in large multi-modal models via robust instruction tuning. External Links: Link Cited by: SS1.\n' +
      '* H. Liu, C. Li, Y. Li, and Y. J. Lee (2023)Improved baselines with visual instruction tuning. CoRRabs/2310.03744. External Links: Link Cited by: SS1.\n' +
      '* Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, K. Chen, and D. Lin (2023)Mmbench: is your multi-modal model an all-around player?. CoRRabs/2307.06281. External Links: Link Cited by: SS1.\n' +
      '* Y. Liu, L. Jin, S. Zhang, and S. Zhang (2017)Detecting curve text in the wild: new dataset and new solution. CoRRabs/1712.02170. External Links: Link Cited by: SS1.\n' +
      '* Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang (2016)Deepfashion: powering robust clothes recognition and retrieval with rich annotations. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 1096-1104. External Links: Link Cited by: SS1.\n' +
      '* Y. Peng Loh and C. S. Chan (2019)Getting to know low-light images with the exclusively dark dataset. Comput. Vis. Image Underst.178, pp. 30-42. External Links: Link Cited by: SS1.\n' +
      '* V. Lomonaco and D. Maltoni (2017)Core50: a new dataset and benchmark for continuous object recognition. In 1st Annual Conference on Robot Learning, CoRL 2017, Mountain View, California, USA, November 13-15, 2017, Proceedings, Vol. 78, pp. 17-26. External Links: Link Cited by: SS1.\n' +
      '* P. Lu, R. Gong, S. Jiang, L. Qiu, S. Huang, X. Liang, and S. Zhu (2021)Intergrp: interpretable geometry problem solving with formal language and symbolic reasoning. In The 59th Annual Meeting of the Association for Computational Linguistics (ACL), Cited by: SS1.\n' +
      '* P. Lu, L. Qiu, J. Chen, T. Xia, Y. Zhao, W. Zhang, Z. Yu, X. Liang, and S. Zhu (2021)Iconqa: a new benchmark for abstract diagram understanding and visual language reasoning. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Cited by: SS1.\n' +
      '* Y. Lu, C. Li, H. Liu, J. Yang, J. Gao, and Y. Shen (2023)An empirical study of scaling instruct-tuned large multimodal models. arXiv preprint arXiv:2309.09958. External Links: Link Cited by: SS1.\n' +
      '* C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, and Z. Tu (2023)Macaw-llm: multi-modal language modeling with image, audio, video, and text integration. CoRRabs/2306.09093. External Links: Link Cited by: SS1.\n' +
      '* S. Maji, E. Rahtu, J. Kannala, M. B. Blaschko, and A. Vedaldi (2013)Fine-grained visual classification of aircraft. Technical report External Links: Link Cited by: SS1.\n' +
      '* M. Malinowski and M. Fritz (2014)A multi-world approach to question answering about real-world scenes based on uncertain input. In Advances in Neural Information Processing Systems 27, pp. 1682-1690. External Links: Link, Document Cited by: SS1.\n' +
      '* K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi (2019)Ok-vqa: a visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pp. 3195-3204. External Links: Link, Document Cited by: SS1.\n' +
      '* M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. V. Jawahar (2022)Infographicvqa. In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022, Waikoloa, HI, USA, January 3-8, 2022, pp.\n' +
      '\n' +
      'Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and C. V. Jawahar. 2020. Docvqa: A dataset for VQA on document images. _CoRR_, abs/2007.00398.\n' +
      '* Mathews et al. (2016) Alexander Patrick Mathews, Lexing Xie, and Xuming He. 2016. Senticap: Generating image descriptions with sentiments. In _Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA_, pages 3574-3580. AAAI Press.\n' +
      '* Methani et al. (2020) Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. 2020. Plotqa: Reasoning over scientific plots. In _IEEE Winter Conference on Applications of Computer Vision, WACV 2020, Snowmass Village, CO, USA, March 1-5, 2020_, pages 1516-1525. IEEE.\n' +
      '* Mishra et al. (2019) Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. 2019. Ocr-vqa: Visual question answering by reading text in images. In _ICDAR_.\n' +
      '* Mostafazadeh et al. (2016) Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He, and Lucy Vanderwende. 2016. Generating natural questions about an image.\n' +
      '* Olsen et al. (2018) Alex Olsen, Dmitry A. Konovalov, Bronson Philippa, Peter Ridd, Jake C. Wood, Jamie Johns, Wesley Banks, Benjamin Girgenti, Owen Kenny, James Whinney, Brendan Calvert, Mostafa Rahimi Azghadi, and Ronald D. White. 2018. Deepweeds: A multi-class weed species image dataset for deep learning. _CoRR_, abs/1810.05726.\n' +
      '* Peng et al. (2019) Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. 2019. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 1406-1415.\n' +
      '* Peng et al. (2017) Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. 2017. Visda: The visual domain adaptation challenge. _CoRR_, abs/1710.06924.\n' +
      '* Plummer et al. (2017) Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2017. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. _IJCV_, 123(1):74-93.\n' +
      '* Pont-Tuset et al. (2020) Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. 2020. Connecting vision and language with localized narratives. In _ECCV_.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR.\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 10674-10685. IEEE.\n' +
      '* ECCV 2010, 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV_, volume 6314 of _Lecture Notes in Computer Science_, pages 213-226. Springer.\n' +
      '* Sagonas et al. (2016) Christos Sagonas, Epameinondas Antonakos, Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic. 2016. 300 faces in-the-wild challenge: database and results. _Image Vis. Comput._, 47:3-18.\n' +
      '* Sakaridis et al. (2019) Christos Sakaridis, Dengxin Dai, and Luc Van Gool. 2019. Guided curriculum model adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7374-7383.\n' +
      '* December 9, 2022_.\n' +
      '* ECCV 2022\n' +
      '- 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII_, volume 13668 of _Lecture Notes in Computer Science_, pages 146-162. Springer.\n' +
      '* February 1, 2019_, pages 8876-8884. AAAI Press.\n' +
      '* Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypermymed, image alt-text dataset for automatic image captioning. In _Proceedings of the 56th Annual\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      'Zhiyang Xu, Ying Shen, and Lifu Huang. 2023. Multi-Instruct: Improving multi-modal zero-shot learning via instruction tuning. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 11445-11465, Toronto, Canada. Association for Computational Linguistics.\n' +
      '* Yang et al. (2021) Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar, and Chris Callison-Burch. 2021. Visual goal-step inference using wikihow. pages 2167-2179.\n' +
      '* Yao et al. (2023) Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. 2023. End-to-end multimodal fact-checking and explanation generation: A challenging dataset and models. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023_, pages 2733-2743. ACM.\n' +
      '* Ye et al. (2023) Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. 2023. mplug-owl: Modularization empowers large language models with multimodality. _CoRR_, abs/2304.14178.\n' +
      '* Yin et al. (2023) Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, Jing Shao, and Wanli Ouyang. 2023. LAMM: language-assisted multimodal instruction-tuning dataset, framework, and benchmark. _CoRR_, abs/2306.06687.\n' +
      '* Yu et al. (2015) Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. 2015. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. _CoRR_, abs/1506.03365.\n' +
      '* Yu et al. (2023) Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. _CoRR_, abs/2308.02490.\n' +
      '* Yue et al. (2023) Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Reniliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI.\n' +
      '* Zhai et al. (2023) Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. 2023. Investigating the catastrophic forgetting in multimodal large language models. _CoRR_, abs/2309.10313.\n' +
      '* Zhang et al. (2019) Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. 2019. Raven: A dataset for relational and analogical visual reasoning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_.\n' +
      '* Zhang et al. (2023) Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. 2023. Llavar: Enhanced visual instruction tuning for text-rich image understanding. _CoRR_, abs/2306.17107.\n' +
      '* Zhao et al. (2019) Bo Zhao, Yanwei Fu, Rui Liang, Jiahong Wu, Yonggang Wang, and Yizhou Wang. 2019. A large-scale attribute dataset for zero-shot learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_, pages 0-0.\n' +
      '* Zhao et al. (2023) Bo Zhao, Boya Wu, and Tiejun Huang. 2023. SVIT: scaling up visual instruction tuning. _CoRR_, abs/2307.04087.\n' +
      '* Zhou et al. (2018) Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. 2018. Places: A 10 million image database for scene recognition. _IEEE Trans. Pattern Anal. Mach. Intell._, 40(6):1452-1464.\n' +
      '* Zhou et al. (2014) Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. 2014. Learning deep features for scene recognition using places database. pages 487-495.\n' +
      '* Zhou et al. (2023) Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2023. Analyzing and mitigating object hallucination in large vision-language models.\n' +
      '* Zhou and Shimada (2021) Yutong Zhou and Nobutaka Shimada. 2021. Generative adversarial network for text-to-face synthesis and manipulation with pretrained BERT model. In _16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021, Jodhpur, India, December 15-18, 2021_, pages 1-8. IEEE.\n' +
      '* Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _CoRR_, abs/2304.10592.\n' +
      '* Zhu et al. (2016) Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. 2016. Visual7w: Grounded question answering in images. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4995-5004.\n' +
      '\n' +
      '## Appendix A More Details on the Annotation Process of Vision-Flan\n' +
      '\n' +
      '### Annotator Selection\n' +
      '\n' +
      'Due to the complexity of the annotation task, we carefully design a selection process to select qualified annotators. Specifically, at beginning, the authors send out emails looking for graduate students in computer science who are interested in NLP and multi-modal learning. A group of 21 graduate computer science students signed up for a tutorial section. In the tutorial section, two PhD students in NLP explain the requirements for writing instructions, downloading the dataset and processingraw datasets into a unified format. After the tutorial, each candidate is assigned with three datasets and they have totally three days to process the raw datasets and write instructions. In the end, each candidate submits their annotations and two PhD students provide feedback to each candidate. The candidates then have two days to modify their instructions or formats based on the feedback. After two days, the candidates submit their final version of annotations and two PhD students discuss the quality of the annotations case by case. In the end, 7 out of 21 students were selected as qualified annotators. The compensation is 15$ per hour.\n' +
      '\n' +
      '## Appendix B Evaluation Datasets\n' +
      '\n' +
      'We evaluate our models on several widely used multimodal evaluation benchmark datasets: (1) **MMbench**Liu et al. (2023) is a comprehensive evaluation benchmark measuring VLM\'s capabilities from 20 dimensions. (2)**MME**Fu et al. (2023) measures VLM\'s perception and cognition capabilities based on 14 diverse tasks. (3) **MM-Vet**Yu et al. (2023) focuses on measuring the integration of various capabilities of VLMs, including OCR, recognition, knowledge, spatial awareness, math, and language generation. (4) **LLaVA-Bench**Liu et al. (2023) evaluates the instruction following and chat ability of VLMs in diverse daily-life visual tasks. (5) **POPE**Li et al. (2023) is an evaluation benchmark that probes object hallucination in VLMs. (6) **MMMU**Yue et al. (2023) evaluates VLMs on multi-discipline tasks that require college-level subject knowledge and deliberate reasoning.\n' +
      '\n' +
      'We also evaluate the newly proposed catastrophic forgetting problem Zhai et al. (2023) of VLMs on 4 datasets: **CIFAR-10 and CIFAR-100**Krizhevsky et al. (2009), **MNIST**LeCun (1998), and **miniImageNet**Vinyals et al. (2016). We report the averaged performance of VLMs on the four benchmarks in the CF column of Table 2.\n' +
      '\n' +
      '## Appendix C Evaluation Protocols\n' +
      '\n' +
      'For MM-Bench, MME, MM-Vet, LLaVA-Bench, POPE and MMMU, we use their official implementations of evaluation code4 to evaluate the performance. Specifically, the evaluation scripts of MM-bench and MM-Vet call GPT-4 API to evaluate the correctness of a prediction given the target output and produce a binary score (0 or 1). Similarly, the evaluation of LLaVA-Bench also leverages GPT-4, and in addition to the target outputs, the evaluation method considers detail descriptions of images. The evaluation results are scores indicating not only the correctness but the human-preference of the predictions. MME and POPE are binary classification tasks and their evaluation is based on string matching between the predictions and target labels.\n' +
      '\n' +
      'Footnote 4: [https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)\n' +
      '\n' +
      '[https://mbmech.opencompass.org.cn/leaderboard](https://mbmech.opencompass.org.cn/leaderboard)\n' +
      '\n' +
      '[https://github.com/yuweihao/MM-Vet](https://github.com/yuweihao/MM-Vet)\n' +
      '\n' +
      '[https://github.com/haotian-liu/LLaVA/blob/](https://github.com/haotian-liu/LLaVA/blob/)\n' +
      '\n' +
      '## Appendix D Baselines\n' +
      '\n' +
      'We compare our method with recent vision-language models. All the baselines listed below have similar architectures which consist of a pre-trained LLM, a pretrained image encoder, and a bridging module that connects them. **BLIP-2**Li et al. (2023) utilizes the Q-Former to bridge a pre-trained image encoder with a pretrained LLM, and achieves strong zero-shot capabilities. **Instruct-BLIP**Dai et al. (2023) is a visual-instruction-tuned BLIP-2 Li et al. (2023) model. The instruction tuning dataset is a mixture of 13 academic datasets and the LLaVA Liu et al. (2023) dataset. **Shikra**Chen et al. (2023) focuses more on the object grounding capability and is instruction tuned on referential dialogue dataset and LLaVA dataset Liu et al. (2023), both of which are synthetically generated via GPT-4. **LLaVA**Liu et al. (2023) is the first VLM finetuned on GPT-4 synthesized visual instruction tuning dataset and achieves remarkable performance as a general-purpose visual chatbot. **Qwen-VL** and **Qwen-VL-Chat**Bai et al. (2023) are recently proposed VLMs based on Qwen Bai et al. (2023) language model and are trained on a large-scale (50 million instances) private visual instruction tuning dataset. **LLaVA-1.5**Liu et al. (2023) is a LLaVA model trained on a mixture of shareGPT 5, LLaVA Liu et al. (2023) and 8 academic image-text datasets.\n' +
      '\n' +
      '## Appendix E Additional Results\n' +
      '\n' +
      '### Effect of GPT-4 synthesized data on comprehensive evaluation benchmarks\n' +
      '\n' +
      '### Why VLMs Trained on Vision-Flan are Better than State-of-the-Art VLMs?\n' +
      '\n' +
      'In this section, we perform two case studies to explain why models trained on Vision-Flan can perform better compared to state-of-the-art VLMs.\n' +
      '\n' +
      '#### e.2.1 Case Study on OCR\n' +
      '\n' +
      'When we manually check the predictions of Vision-Flan Chat and compare them to other VLMs, the first trend we observe is that Vision-Flan Chat can better perform OCR as shown in Figure 10. To quantify this observation, we evaluate LLaVA, LLaVA 1.5 and our models on the challenging TextOCR dataset (Singh et al., 2021). We ask the VLMs to predict all the text on each image and check the overlap between the target list of text pieces and the predictions. As shown in Figure 9, the recall of Vision-Flan Base and Vision-Flan Chat is much higher compared to LLaVA 1.5 while the averaged numbers of predicted tokens per response are similar.\n' +
      '\n' +
      '#### e.2.2 Case Study on Entity Recognition\n' +
      '\n' +
      'We also spot that models trained on Vision-Flan can better identify entities in an image while LLaVA 1.5 simply captions the appearance of entities in an image. A qualitative example is shown in Figure 11.\n' +
      '\n' +
      'To compute quantitative results, we randomly sample 1,000 images with their captions from the WIT dataset (Srinivasan et al., 2021), in which the images are from Wikipedia pages and the captions commonly contain entities appearing in the images. We prompt VLMs to introduce the entities in\n' +
      '\n' +
      'Figure 8: Effect of increasing the number of GPT-4 synthesized training instances on the comprehensive evaluation benchmark, namely MM-Bench. The dashed gray line indicates the performance of the-state-of-the-art LLaVA 1.5 model.\n' +
      '\n' +
      'Figure 10: An example from TextCap to show that Vision-Flan allows VLMs to better recognize text.\n' +
      '\n' +
      'Figure 9: Performance of various VLMs on TextOCR. The gray bars shows the averaged number of tokens per prediction and the orange line show the recall of predictions.\n' +
      '\n' +
      'the image with some background knowledge. We leverage the EntityRecognizer from spaCy 6 to recognize the entities in both predictions and ground truth captions and compute the percentage of target entities appearing in the predictions. As shown in Figure 12, it is clear that Vision-Flan Base and Vision-Flan Chat predict more entities in their responses (gray bars) and have higher coverage of entities (orange line) compared to LLaVA 1.5.\n' +
      '\n' +
      'Footnote 6: [https://spacy.io/api/entityrecognizer](https://spacy.io/api/entityrecognizer)\n' +
      '\n' +
      '## Appendix F Additional Analysis\n' +
      '\n' +
      '### The Bridging Module Can Be Shared Across LLMs with Same Architecture\n' +
      '\n' +
      'Recent studies Jain et al. (2023) in aligning and finetuning LLMs suggest that alignment happens very localized with pruning of a few weights or neurons to alter the style and format of outputs from LLMs, and does not substantially change the parameter space of LLMs. Following this finding, we hypothesize that _the MLP layers that map visual features into LLMs\' embedding space can be shared across LLMs with identical architecture but are tuned on different text alignment datasets_. As shown in Table 7, we take four different models including Vision-Flan Base w/frozen LLM which is finetuned on Vision-Flan but with LLMs kept frozen as a case study, and directly replace their LLMs (Vicuna v1.5) with off-the-shelf LLaMA 2 Chat model. During inference, we use the official prompting template of LLaMA 2 chat instead of Vicuna v1.5. The results demonstrate that MLPs can be shared between LLMs with the same architecture but trained on different alignment datasets. An interesting observation is that there is a significant performance boost on LLaVA-Bench after we swap in LLaMA 2 Chat. If we finetune both the MLPs and the LLMs in Vision-Flan Base and Vision-Flan Chat, we observe a remarkable performance drop when we swap in LLaMA 2 chat. This is understandable because the LLaMA 2 chat can not effectively interpret visual features compared to the visual-instruction-tuned Vicuna v1.5.\n' +
      '\n' +
      '### Discrepancy Between Evaluation Benchmarks\n' +
      '\n' +
      'In Table 2 and 7, we identify large performance discrepancy between multiple-choice benchmarks (e.g., MME and MM-Bench) and LLaVA-Bench on several models. Specifically, in Table 2, LLaVA achieves a score of 70.8 on LLaVA-Bench, comparable to the performance level of LLaVA 1.5. In contrast, LLaVA\'s performance on MME and MM-Bench is markedly lower, with scores of 1151.6 and 38.7, respectively, compared to LLaVA 1.5, which scores 1531.3 and 66.7. Furthermore, this trend is also evident in Table 7. Upon substituting the\n' +
      '\n' +
      'Figure 11: An example from MM-Vet to show that Vision-Flan allows VLMs to better recognize entities.\n' +
      '\n' +
      'Figure 12: Performance of various VLMs on Entity Recognition. The gray bars show the average number of entities per response and the orange line shows the percentage of entities in the target response that appears in the prediction.\n' +
      '\n' +
      'LLMs in Vision-Flan Base and Vision-Flan Chat with off-the-shelf LLaMA 2 Chat, both models exhibit a notable decline in performance on MME and MM-Bench, while maintaining comparable performance on LLaVA-Bench. Our hypothesis posits that LLaVA-Bench does not require LLM\'s strong understanding of the visual features, but rather relies on the language-prior of LLMs (Lin et al., 2023). Furthermore, the data synthesized by GPT-4 facilitates the model\'s ability to generate long-form responses, aligning with the preferences of the evaluation metric, namely, GPT-4 itself.\n' +
      '\n' +
      '## Appendix G Additional Related Work\n' +
      '\n' +
      'Vision-Language Models.Previous works (Li et al., 2019; Chen et al., 2020; Tan and Bansal, 2019; Su et al., 2020; Wang et al., 2023) mainly pretrain vision-language models (VLMs) from scratch with a unified masked-language modeling (MLM) objective (Devlin et al., 2019), which can impose significant training cost and inferior performance. Recently, a line of works proposes to build VLMs from the off-the-shelf visual encoders and LLMs by introducing a small amount of bridging parameters that maps visual features into the embedding space of LLMs. Flamingo (Alayrac et al., 2022) presents a VLM that is capable of processing interleaved image-text inputs and generating responses based on the visual content. It proposes Perceiver Resampler as the bridging module to connect the frozen LLM and visual encoder. OFA (Wang et al., 2022) proposes a sequence-to-sequence learning framework that maps images to discrete visual tokens and feeds the discrete visual tokens into LLMs. BLIP-2 (Li et al., 2023) introduces Q-Former to bridge pre-trained and frozen vision and language models, based on which, MiniGPT-4 (Zhu et al., 2023) further adds a linear projector to bridge the gap between the visual encoder and language model encoder. LLaVA (Liu et al., 2023) introduces a projector to fuse visual information into a large language model and unfreezes language model during visual instruction tuning.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Model** & MM-Bench & MME & LLaVA-Bench & Pope \\\\ \\hline\n' +
      '**Pretrained LLaVA-Architecture** & 45.0 & 936.3 & 32.4 & 51.9 \\\\ \\hline + LLaMA 2 Chat & 45.3 (100.6) & 557.0 (59.5) & 59.2 (182.7) & 66.9 (128.9) \\\\ \\hline Vision-Flan Base w/ frozen LLM & 52.4 & 1107.3 & 41.6 & 83.3 \\\\ \\hline + LLaMA 2 Chat & 46.6 (88.9) & 1095.8 (99.0) & 56.4 (135.6) & 80.9 (97.1) \\\\ \\hline Vision-Flan Base & 69.8 & 1537.8 & 38.5 & 85.9 \\\\ \\hline + LLaMA 2 Chat & 47.2 (67.6) & 852.6 (55.4) & 69.9 (181.6) & 66.1 (76.9) \\\\ \\hline Vision-Flan Chat & 67.6 & 1490.6 & 78.3 & 86.1 \\\\ \\hline + LLaMA 2 Chat & 47.0 (69.5) & 869.6 (59.3) & 74.6 (95.3) & 65.8 (76.4) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Results of replacing Vicuna 1.5 with LLaMA 2 Chat in four VLMs. The gray rows denote the performance of original models and blue rows denote the performance of the VLMs after replacing the LLMs. The number in each bracket denotes the percentage of VLMs’ performance after integration of LLaMA 2 Chat, compared to their original performance.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:21]\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Dataset \\& Reference & Tasks \\\\ \\hline \\hline CINIC-10 (Darlow et al., 2018) & 1. animal recognition in low resolution image \\\\  & 2. shipping method recognition in low resolution image \\\\  & 3. transportation option recognition in low resolution image \\\\  & 4. animal presence classification in low resolution \\\\  &  image \\\\  & 5. object shipping object presence in low resolution image \\\\ \\hline MSCOCO (Lin et al., 2014) & 1. multiple choice VQA \\\\  & 2. short image captioning \\\\  & 3. appliance recognition \\\\  & 4. furniture recognition \\\\  & 5. kitchen object recognition \\\\  & 6. vehicle recognition \\\\  & 7. animal recognition \\\\  & 8. sports object recognition \\\\  & 9. image text matching \\\\  & 10. image text selection \\\\ \\hline FairFace (Karkkainen and Joo, 2021) & 1. human age classification \\\\  & 2. human gender classification \\\\  & 3. human race classification \\\\ \\hline IconQA (Lu et al., 2021b) & 1. abstract diagram understanding \\\\  & 2. fill in blank in abstract diagram understanding \\\\ \\hline ImageNet-A (Hendrycks et al., 2021b) & 1. object recognition of natural adversarial examples \\\\ \\hline ImageNet-C (Hendrycks and Dietterich, 2019) & 1. blur type classification \\\\  & 2. coarse-grained image corruption classification \\\\  & 3. weather type classification \\\\  & 4. fine-grained image corruption classification \\\\ \\hline InfographicVQA (Mathew et al., 2022) & 1. VQA \\\\  & 2. document level VQA \\\\ \\hline SemArt (Garcia and Vogiatzis, 2018) & 1. painting time frame recognition \\\\  & 2. painting type recognition \\\\  & 3. painting school recognition \\\\  & 4. painting technique recognition \\\\  & 5. detailed image description \\\\ \\hline Set5 (Bevilacqua et al., 2012) & 1. object recognition in low resolution image \\\\ \\hline TextCaps (Sidorov et al., 2020) & 1. image captioning with reading comprehension \\\\ \\hline VisDial (Das et al., 2019) & 1. visual dialogue with short context \\\\  & 2. visual dialogue with medium context \\\\  & 3. visual dialogue with long context \\\\  & 4. visual dialogue with very long context \\\\ \\hline STL-10 (Coates et al., 2011) & 1. object recognition \\\\ \\hline Places365 (Zhou et al., 2018) & 1. scene classification \\\\ \\hline Office-31 (Saenko et al., 2010) & 1. image domain and office object classification \\\\  & 2. office object recognition \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Dataset \\& Reference & Tasks \\\\ \\hline \\hline LSUN (Yu et al., 2015) & 1. scene classification \\\\ \\hline FGVC-Aircraft (Maji et al., 2013) & 1. aircraft family classification \\\\  & 2. aircraft manufacturer classification \\\\  & 3. aircraft variant classification \\\\  & 4. aircraft model classification \\\\ \\hline DeepFashion (Liu et al., 2016) & 1. cloth texture classification \\\\ \\hline CUB-200-2011 (Wah et al., 2011) & 1. bird species recognition \\\\ \\hline CLEVR (Johnson et al., 2017) & 1. VQA in 3D rendered images \\\\  & 2. question answer matching \\\\  & 3. visual dialogue in 3D rendered images \\\\  & 4. VQA in 3D rendered images with multiple questions \\\\ \\hline CLEVR-CoGenT (Johnson et al., 2017) & 1. VQA in 3D rendered images \\\\  & 2. question answer matching \\\\  & 3. VQA in 3D rendered images with multiple questions \\\\ \\hline A-OKVQA (Schwenk et al., 2022) & 1. rationales generation \\\\  & 2. answer rationale generation \\\\  & 3. outside knowledge VQA \\\\ \\hline AI2D (Kembhavi et al., 2016) & 1. diagram VQA \\\\ \\hline AID (Xia et al., 2017) & 1. aerial scene classification \\\\ \\hline Caltech-256 (Griffin et al., 2007) & 1. object recognition \\\\ \\hline CoVA (Kumar et al., 2022) & 1. webpage recognition \\\\ \\hline DeepWeeds (Olsen et al., 2018) & 1. weed species recognition \\\\ \\hline ExDark (Loh and Chan, 2019) & 1. object recognition in low light environments \\\\ \\hline FFHQ-Text (Zhou and Shimada, 2021) & 1. facial attribute textual descriptions generation \\\\ \\hline FlickrLogos-27 (Kalantidis et al., 2011) & 1. logo recognition \\\\ \\hline FoodLogoDet-1500 (Hou et al., 2021) & 1. food logo recognition \\\\ \\hline ImageNet-R (Hendrycks et al., 2021) & 1. object recognition in diverse image domain \\\\  & 2. image style classification \\\\ \\hline ImageNet-Sketch (Wang et al., 2019) & 1. object recognition in sketch \\\\ \\hline JHU-CROWD++ (Sindagi et al., 2019) & 1. scene classification \\\\ \\hline MNIST-M (Ganin et al., 2017) & 1. number recognition \\\\ \\hline MVTecAD (Bergmann et al., 2021) & 1. object anomaly detection \\\\  & 2. industrial item recognition \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Dataset \\& Reference & Tasks \\\\ \\hline \\hline NABirds (Horn et al., 2015) & 1. bird species recognition in north America \\\\  & 2. bird body parts detection \\\\ \\hline Road-Anomaly (Lis et al., 2019) & 1. road anomaly detection \\\\ \\hline SCUT-CTW1500 (Liu et al., 2017) & 1. curve text detection in the wild \\\\ \\hline Total-Text (Chng et al., 2020) & 1. scene text detection and recognition \\\\ \\hline VisDA-2017 (Peng et al., 2017) & 1. object recognition in 3D rendered image \\\\  & 2. multiple choice object recognition in 3D rendered image \\\\ \\hline Yoga-82 (Verma et al., 2020) & 1. yoga pose recognition \\\\ \\hline Caltech101 (Fei-Fei et al., 2004) & 1. object recognition \\\\  & 2. living organism classification \\\\ \\hline Cars (Krause et al., 2013) & 1. car brand maker and year classification \\\\  & 2. car brand classification \\\\ \\hline Core50 (Lomonaco and Maltoni, 2017) & 1. object recognition \\\\ \\hline NUS-WIDE (Chua et al., 2009) & 1. animal presence classification \\\\ \\hline ObjectNet (Barbu et al., 2019) & 1. object recognition \\\\ \\hline Places205 (Zhou et al., 2014) & 1. indoor outdoor classification \\\\ \\hline\n' +
      '300w (Sagonas et al., 2016) & 1. indoor outdoor classification \\\\ \\hline Yahoo (Farhadi et al., 2009) & 1. object recognition \\\\ \\hline LFW (Huang et al., 2007) & 1. celebrity recognition \\\\ \\hline model-vs-human (Geirhos et al., 2019) & 1. image-style classification \\\\ \\hline Office-Home (Venkateswara et al., 2017) & 1. object recognition \\\\ \\hline Winoground (Thrush et al., 2022) & 1. image caption matching \\\\ \\hline ConceptualCaptions (Sharma et al., 2018) & 1. conceptual image captioning \\\\ \\hline KVQA+image question answer (Shah et al., 2019) & 1. knowledge-aware VQA \\\\  & 2. visual entity recognition \\\\ \\hline MemeCap (Hwang and Shwartz, 2023) & 1. meme understanding \\\\ \\hline PlotQA (Methani et al., 2020) & 1. VQA over scientific plots \\\\ \\hline SentiCap (Mathews et al., 2016) & 1. image captioning conditioned on sentiment \\\\ \\hline VQA-E (Li et al., 2018) & 1. VQA \\\\  & 2. short image captioning \\\\ \\hline VQG (Mostafazadeh et al., 2016) & 1. visual question generation \\\\  & 2. short image captioning \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Dataset \\& Reference & Tasks \\\\ \\hline \\hline WIT (Srinivasan et al., 2021) & 1. background knowledge extraction \\\\ \\hline WikiArt (Tan et al., 2019) & 1. artist genre style recognition \\\\ \\hline VQA-RAD (Lau et al., 2019) & 1. VQA in radiology \\\\ \\hline VOC2007 (Everingham et al., 2010) & 1. multiple object recognition \\\\ \\hline VizWiz (Gurari et al., 2020) & 1. answering visual questions from blind people \\\\  & 2. captioning image taken by blind people \\\\  & 3. quality issue classification of image taken by blind people \\\\ \\hline ViQuAE (Lerner et al., 2022) & 1. knowledge based VQA about entities \\\\ \\hline ST-VQA (Biten et al., 2019) & 1. scene text VQA \\\\ \\hline Stanford Dogs (Khosla et al., 2011) & 1. dog species classification \\\\ \\hline Sketch (Eitz et al., 2012) & 1. living organism classification in sketch \\\\  & 2. object recognition in sketch \\\\ \\hline RAVEN (Zhang et al., 2019) & 1. relational and analogical visual reasoning \\\\ \\hline PICKAPIC (Kirstain et al., 2023) & 1. image prompt generation \\\\ \\hline PACS (Li et al., 2017) & 1. object recognition in art painting \\\\  & 2. object recognition in cartoon \\\\  & 3. object recognition in photograph \\\\  & 4. dog image style classification \\\\  & 5. elephant image style classification \\\\  & 6. giraffe image style classification \\\\  & 7. guitar image style classification \\\\  & 8. horse image style classification \\\\  & 9. house image style classification \\\\  & 10. person image style classification \\\\ \\hline NOCAPS (Agrawal et al., 2019) & 1. multiple short captions generation \\\\ \\hline Localized Narratives (Pont-Tuset et al., 2020) & 1. COCO detailed image captioning \\\\  & 2. flickr30k detailed image captioning \\\\  & 3. open images detailed image captioning \\\\  & 4. ade20k detailed image captioning \\\\ \\hline INATURALIST (Horn et al., 2018) & 1. class classification \\\\  & 2. family classification \\\\  & 3. genus classification \\\\  & 4. Latin English name classification \\\\  & 5. order classification \\\\  & 6. phylum classification \\\\  & 7. supercategory classification \\\\ \\hline HICO (Chao et al., 2015) & 1. human activity detection \\\\ \\hline GEOMETRY3K (Lu et al., 2021a) & 1. geometry question answering \\\\ \\hline FUNSD (Guillaume Jaume, 2019) & 1. text detection in noisy scanned documents \\\\ \\hline FLICKR30K (Plummer et al., 2017) & 1. multiple captions generation \\\\ \\hline DVQA (Kafle et al., 2018) & 1. chart question answering \\\\ \\hline DTD (Cimpoi et al., 2014) & 1. coarse grained texture classification \\\\  & 2. multiple texture detection \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Dataset \\& Reference & Tasks \\\\ \\hline \\hline DOMAIN NET (Peng et al., 2019) & 1. object recognition in clip art \\\\  & 2. object recognition in infograph \\\\  & 3. object recognition in painting \\\\  & 4. object recognition in quickdraw \\\\  & 5. object recognition in real image \\\\  & 6. image style classification \\\\ \\hline DOCVQA (Mathew et al., 2020) & 1. document level VQA \\\\ \\hline DAQUAR (Malinowski and Fritz, 2014) & 1. VQA \\\\ \\hline CONCADIA (Kreiss et al., 2022) & 1. caption with background knowledge \\\\  & 2. short image captioning \\\\ \\hline Visual7W (Zhu et al., 2016) & 1. VQA object attribute \\\\ \\hline VQAv2 (Goyal et al., 2017) & 1. general VQA \\\\  & 2. question image matching \\\\ \\hline Visual Genome(Krishna et al., 2017) & 1. spatial relationship question answering \\\\ \\hline OK-VQA(Marino et al., 2019) & 1. outside knowledge VQA \\\\ \\hline ScienceQA (Lu et al., 2022) & 1. VQA \\\\  & 2. explanation generation \\\\ \\hline OCR-VQA (Mishra et al., 2019) & 1. VQA by reading text in image \\\\ \\hline wikiHow-image (Yang et al., 2021) & 1. next step generation \\\\  & 2. image text step ordering \\\\  & 3. immediate next step selection \\\\  & 4. text image step ordering \\\\ \\hline SciCap (Hsu et al., 2021) & 1. figure captioning \\\\ \\hline LAD (Zhao et al., 2019) & 1. detailed object description generation \\\\ \\hline Dark Zurich (Sakaridis et al., 2019) & 1. time of the day classification \\\\ \\hline RAF-DB (Li and Deng, 2019) & 1. human emotion detection \\\\ \\hline GQA (Hudson and Manning, 2019) & 1. spatial relationship question answering \\\\ \\hline VQA (Antol et al., 2015) & 1. color \\\\  & 2. activity recognition \\\\  & 3. counting \\\\  & 4. object presence \\\\  & 5. object recognition \\\\  & 6. positional reasoning \\\\  & 7. scene recognition \\\\  & 8. sentiment understanding \\\\  & 9. sport recognition \\\\  & 10. utility affordance \\\\ \\hline Multimodal Factual Checking (Yao et al., 2023) & 1. multimodal factual checking \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '## Part I Task Categories in Vision-Flam\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Category & Tasks \\\\ \\hline \\hline Perception & 1. CLEVR-CoGenT VQA in 3D rendered images \\\\  & 2. CLEVR-CoGenT question answer matching \\\\  & 3. CLEVR-CoGenT VQA in 3D rendered images \\\\  & with multiple questions \\\\  & 4. CLEVR VQA in 3D rendered images with multiple questions \\\\  & 5. GQA spatial relationship question answering \\\\  & 6. VQA color \\\\  & 7. VQA activity recognition \\\\  & 8. VQA counting \\\\  & 9. VQA object presence \\\\  & 10. VQA object recognition \\\\  & 11. VQA positional reasoning \\\\  & 12. VQA scene recognition \\\\  & 13. VQA sentiment understanding \\\\  & 14. VQA sport recognition \\\\  & 15. VQA utility affordance \\\\  & 16. VQA-E VQA \\\\  & 17. VQAv2 general VQA \\\\  & 18. Visual Genome spatial relationship question answering \\\\  & 19. CLEVR question answer matching \\\\  & 20. VizWiz answering visual questions from blind \\\\  & people \\\\  & 21. DAQUAR VQA \\\\  & 22. MSCOCO multiple choice VQA \\\\  & 23. Visual7W VQA object attribute \\\\  & 24. CLEVR VQA in 3D rendered images \\\\ \\hline Outside Knowledge & 1. KVQA knowledge aware VQA \\\\  & 2. VIQUAE knowledge based VQA about entities \\\\  & 3. VQARAD VQA in radiology \\\\  & 4. OK-VQA outside knowledge VQA \\\\  & 5. A-OKVQA outside knowledge VQA \\\\ \\hline Reasoning & 1. GEOMETRY3K geometry question answering \\\\  & 2. IconQA abstract diagram understanding \\\\  & 3. IconQA fill in blank in abstract diagram understanding \\\\  & 4. InfographicVQA VQA \\\\  & 5. InfographicVQA document level VQA \\\\  & 6. ScienceQA VQA \\\\  & 7. AI2D diagram VQA \\\\ \\hline OCR & 1. DOCVQA document level VQA \\\\  & 2. DVQA chart question answering \\\\  & 3. PlotQA VQA over scientific plots \\\\  & 4. OCR-VQA VQA by reading text in image \\\\  & 5. ST-VQA scene text VQA \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|p{113.8pt}|p{113.8pt}|} \\hline Category & Tasks \\\\ \\hline \\hline Document-Level OCR & 1. FUNSD text detection in noisy scanned documents \\\\  & 2. SCUT-CTW1500 curve text detection in the wild \\\\  & 3. Total-Text scene text detection and recognition \\\\ \\hline Phrase-Level OCR & 1. CoVA webpage recognition \\\\  & 2. FlickrLogos-27 logo recognition \\\\  & 3. FoodLogoDet-1500 food logo recognition \\\\ \\hline Knowledge Extraction & 1. CONCADIA caption with background knowledge \\\\  & edge \\\\  & 2. KVQA visual entity recognition \\\\  & 3. WIT background knowledge extraction \\\\ \\hline Semantic Art Understanding & 1. Semart painting time frame recognition \\\\  & 2. Semart painting type recognition \\\\  & 3. Semart painting school recognition \\\\  & 4. Semart painting technique recognition \\\\  & 5. Semart detailed image description \\\\  & 6. WikiArt artist genre style recognition \\\\ \\hline Visual Dialogue & 1. CLEVR visual dialogue in 3D rendered images \\\\  & 2. Visdial visual dialogue with short context \\\\  & 3. Visdial visual dialogue with medium context \\\\  & 4. Visdial visual dialogue with long context \\\\  & 5. Visdial visual dialogue with very long context \\\\ \\hline Rational and Script Generation & 1. ScienceQA explanation generation \\\\  & 2. A-OKVQA rationales generation \\\\  & 3. A-OKVQA answer rationale generation \\\\  & 4. MemeCap meme understanding \\\\  & 5. wikiHow-image next step generation \\\\  & 6. VQG visual question generation \\\\ \\hline Coarse-grained Captioning & 1. ConceptualCaptions conceptual image captioning \\\\  & 2. FLICKR30K multiple captions generation \\\\  & 3. NOCAPS multiple short captions generation \\\\  & 4. PICKAPIC image prompt generation \\\\  & 5. VizWiz captioning image taken by blind people \\\\  & 6. VQA-E short image captioning \\\\  & 7. VQG short image captioning \\\\  & 8. MSCOCO short image captioning \\\\  & 9. CONCADIA short image captioning \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:30]\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Category & Tasks \\\\ \\hline \\hline Vehicle Classification & 1. Cars car brand maker and year classification \\\\  & 2. Cars car brand classification \\\\  & 3. FGVC-Aircraft aircraft family classification \\\\  & 4. FGVC-Aircraft aircraft manufacturer classification \\\\  & 5. FGVC-Aircraft aircraft variant classification \\\\  & 6. FGVC-Aircraft aircraft model classification \\\\ \\hline Human Activity & 1. HICO human activity detection \\\\  & 2. RAF-DB human emotion detection \\\\  & 3. Yoga-82 yoga pose recognition \\\\ \\hline Facial Recognition & 1. LFW celebrity recognition \\\\  & 2. Fairface human age classification \\\\  & 3. Fairface human gender classification \\\\  & 4. Fairface human race classification \\\\ \\hline Anomaly Detection & 1. Road-Anomaly road anomaly detection \\\\  & 2. MVTecAD object anomaly detection \\\\ \\hline General Object & 1. Caltech-256 object recognition \\\\  & 2. Caltech101 object recognition \\\\  & 3. Caltech101 living organism classification \\\\  & 4. Core50 object recognition \\\\  & 5. ImageNet-A object recognition of natural adversarial examples \\\\  & 6. MNIST-M number recognition \\\\  & 7. MVTecAD industrial item recognition \\\\  & 8. ObjectNet object recognition \\\\  & 9. Office-Home object recognition \\\\  & 10. Office-31 image domain and office object classification \\\\  & 11. Office-31 office object recognition \\\\  & 12. STL-10 object recognition \\\\  & 13. Set5 object recognition in low resolution image \\\\  & 14. VOC2007 multiple object recognition \\\\  & 15. MSCOCO appliance recognition \\\\  & 16. MSCOCO furniture recognition \\\\  & 17. MSCOCO kitchen object recognition \\\\  & 18. MSCOCO vehicle recognition \\\\  & 19. MSCOCO animal recognition \\\\  & 20. MSCOCO sports object recognition \\\\  & 21. Yahoo object recognition \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|p{142.3pt}|p{142.3pt}|} \\hline Category & Tasks \\\\ \\hline \\hline Complex Reasoning & 1. RAVEN relational and analogical visual reasoning \\\\  & 2. Multimodal Factual Checking multimodal factual checking \\\\  & 3. wikiHow-image image text step ordering \\\\  & 4. wikiHow-image immediate next step selection \\\\  & 5. wikiHow-image text image step ordering \\\\ \\hline Image Text Matching & 1. MSCOCO image text matching \\\\  & 2. Winoground image caption matching \\\\  & 3. MSCOCO image text selection \\\\  & 4. MSCOCO question image matching \\\\ \\hline General Object Classification in Special Image Domain & 1. DOMAIN NET object recognition in clip art \\\\  & 2. DOMAIN NET object recognition in infograph \\\\  & 3. DOMAIN NET object recognition in painting \\\\  & 4. DOMAIN NET object recognition in quick-draw \\\\  & 5. DOMAIN NET object recognition in real image \\\\  & 6. ExDark object recognition in low light environments \\\\  & 7. ImageNet-R object recognition in diverse image domain \\\\  & 8. ImageNet-Sketch object recognition in sketch \\\\  & 9. PACS object recognition in art painting \\\\  & 10. PACS object recognition in cartoon \\\\  & 11. PACS object recognition in photograph \\\\  & 12. SKETCH living organism classification in sketch \\\\  & 13. SKETCH object recognition in sketch \\\\  & 14. Cinic-10 animal recognition in low resolution image \\\\  & 15. Cinic-10 shipping method recognition in low resolution image \\\\  & 16. Cinic-10 transportation option recognition in low resolution image \\\\  & 17. Cinic-10 animal presence classification in low resolution image \\\\  & 18. Cinic-10 object shipping object presence in low resolution image \\\\  & 19. VisDA-2017 object recognition in 3D rendered image \\\\  & 20. VisDA-2017 multiple choice object recognition in 3D rendered image \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|l|l|} \\hline Category & Tasks \\\\ \\hline \\hline Image-Style Classification & 1. DOMAIN-NET image style classification \\\\  & 2. ImageNet-R image style classification \\\\  & 3. PACS dog image style classification \\\\  & 4. PACS elephant image style classification \\\\  & 5. PACS giraffe image style classification \\\\  & 6. PACS guitar image style classification \\\\  & 7. PACS horse image style classification \\\\  & 8. PACS house image style classification \\\\  & 9. PACS person image style classification \\\\  & 10. Model-vs-human image style classification \\\\ \\hline Image Quality Classification & 1. ImageNet-C blur type classification \\\\  & 2. ImageNet-C coarse-grained image corruption \\\\  & classification \\\\  & 3. ImageNet-C weather type classification \\\\  & 4. ImageNet-C fine-grained image corruption \\\\  & classification \\\\  & 5. VizWiz quality issue classification of image \\\\  & taken by blind people \\\\ \\hline Texture Classification & 1. DTD coarse grained texture classification \\\\  & 2. DTD multiple texture detection \\\\  & 3. DeepFashion cloth texture classification \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      'Visco-FLAN Tasks\n' +
      '\n' +
      'All "Cenceration Tasks"Figure 14:\n' +
      '\n' +
      'Figure 13:\n' +
      '\n' +
      'Figure 15:\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:36]\n' +
      '\n' +
      'Figure 18:\n' +
      '\n' +
      'Figure 19:\n' +
      '\n' +
      'Figure 20:\n' +
      '\n' +
      '**Task:**: **Visdial_visual_dialogue_with_long_context**\n' +
      '\n' +
      '**Instruction**: Here is a conversation between 2 people. They are talking about a picture and discussing some questions and answers about it. Read the conversation and then answer as the next person.\n' +
      '\n' +
      '**Input**: Person1: how many motorcycles? Person2: 20 Person1: what color are they? Person2: black and white Person1: how many men? Person2: 20 Person1: is this in a city? Person2: no Person1: any women? Person2: no Person1: is a town? Person2: no Question: are they in leather?\n' +
      '\n' +
      '**Target**: Person2: yes\n' +
      '\n' +
      '**Task:**: **Visdial_visual_dialogue_with_very_long_context**\n' +
      '\n' +
      '**Instruction**: Here is a detailed conversation between a person and a robot. They are discussing some questions and answers about a picture. From the conversation, answer the question as the robot.\n' +
      '\n' +
      '**Input**: Human: what kind of uniform are the men wearing? Robot: they look like military Human: how many men would you guess there are? Robot: there are 3 men and a civilian woman Human: what color is the plane? Robot: white Human: what is the woman doing? Robot: talking to 1 of the men and pointing at the side of the plane Human: does the plane have writing on it? Robot: some numbers but i cannot make them out Human: does it look like a military plane? Robot: i think so but it is a close up, so hard to tell Human: are there any other planes in the image? Robot: yes i see parts of 2 others in the background Question: what is the woman wearing?\n' +
      '\n' +
      '**Target**: Robot: a white and blue dress\n' +
      '\n' +
      '**Task:**: **CLEVR_visual_dialogue_in_3D_rendered_images**\n' +
      '\n' +
      '**Instruction**: You are given some question and answer pairs as dialogue history, and you will answer the question at the end based on the image.\n' +
      '\n' +
      '**Input**: What number of large blue matte things are the same shape as the big green matte object? 0 \\(\\backslash\\)n Is the number of red shiny objects left of the small red shiny object greater than the number of small gray metallic spheres? no \\(\\backslash\\)no the green thing to the left of the red object and the large thing that is right of the tiny red object have the same shape? no \\(\\backslash\\)nts there any other thing that is the same size as the red metal object? yes \\(\\backslash\\)nWhat number of cubes are either big rubber things or tiny yellow rubber objects? 0 \\(\\backslash\\)noes the large sphere have the same material as the green cylinder? no \\(\\backslash\\)nThe large thing that is behind the matte ball in front of the matte thing that is on the right side of the large green metal cylinder is what shape?\n' +
      '\n' +
      '**Target**: cylinder\n' +
      '\n' +
      'Figure 21:Figure 23\n' +
      '\n' +
      'Figure 23\n' +
      '\n' +
      'Figure 24\n' +
      '\n' +
      'Figure 23\n' +
      '\n' +
      'Figure 25\n' +
      '\n' +
      '**Task: PICKAPIC_image_prompt_generation Instruction: Image you are writing a prompt for generating the given image. Please generate the prompt. Target: Jedi duck holding a lightsaber.**\n' +
      '\n' +
      '**Task: VQA-E+short_image_captioning Instruction: You are provided with an image. Briefly Explain what is going on in the image. Target: there are two people that are seated at the table talking**\n' +
      '\n' +
      '**Task: VIZWIZ+captioning_image_taken_by_blind_people Instruction: Give an image taken by blind people, generate multiple captions for this image. Target: Caption 1: A woman\'s hand is resting of some kind of thick green bumpy fabric and her finger is under an orange food label Caption 2: Fingertip touching an orange juice carton that\'s sitting on top of dark knit fabric Caption 3: Orange juice is in the carton right at your fingertips. Caption 4: A person is touching a package of food.**\n' +
      '\n' +
      '**Task: ConceptualCaptions_conceptual_image_captioning Instruction**: In this task, you will generate a short caption for the given image. The image could depict various scenes, objects, or people. Your goal is to provide a short and accurate caption that describes the main elements the image. Pay attention to the details, such as colors, objects, actions, and interactions between elements, to create a meaningful and informative caption.\n' +
      '\n' +
      '**Target: actor arrives at the premiere.**\n' +
      '\n' +
      '**Task: FICKR30K_multiple_captions_generation Instruction**: Each image has something going on. Carefully analyze the image and generate 5 short captions for each image.\n' +
      '\n' +
      '**Target: Caption 1: A young woman intently looking at something while another woman in a hat looks on. Caption 2: A young girl in glasses observes something in the distance. Caption 3: a teenager and her mom at a public place. Caption 4: A blond girl is fixing her sunglasses. Caption 5: A woman adjusts her sunglasses.\n' +
      '\n' +
      '**Task: VIZWIZ_captioning_image_taken_by_blind_people Instruction**: Give an image taken by blind people, generate multiple captions for this image.\n' +
      '\n' +
      '**Target: Caption 1: A woman\'s hand is resting of some kind of thick green bumpy fabric and her finger is under an orange food label Caption 2: Fingertip touching an orange juice carton that\'s sitting on top of dark knit fabric Caption 3: Orange juice is in the carton right at your fingertips. Caption 4: A person is touching a package of food.\n' +
      '\n' +
      '**Task: NOCAPS_multiple_short_captions_generation Instruction**: Give me a list of 10 short captions for this image.\n' +
      '\n' +
      '**Target: Caption 1: Group of people playing polo on bicycles on blacktop. Caption 2: A man riding a bicycle wearing a hat. Caption 3: Individuals playing a game consisting of riding on bicycles and hitting a ball with mallets. Caption 4: Three men in jackets playing croquet on bicycles. Caption 5: some people are playing hockey with their bicycle. Caption 6: Three people that are riding bicycles around on a basketball court. Caption 7: Several people ride bikes on what appears to be park asphalt. Caption 8: A group of people are riding the bicycle and playing hockey. Caption 9: Four people riding bicycles and playing polo while on a concrete pad near many houses. Caption 10: Four people playing polo on their bicycles on pavement.\n' +
      '\n' +
      'Figure 27:\n' +
      '\n' +
      '**Task: LOC-NARRATIVES_COCO_detailed_image_captioning Instruction**: I want to know more about this image. Can you please describe it in details?\n' +
      '\n' +
      '**Target: In this picture there are elephants at the right and left side of the image, there is water, grass land and rocks at the right and left side of the image, there are trees and dried grass at the background area of the image.\n' +
      '\n' +
      '**Task: LOC-NARRATIVES_flickr30k_detailed_image_captioning Instruction**: Please give me a detailed caption about the input image.\n' +
      '\n' +
      '**Target: In this image, there are two children in different color t-shirts, playing on an inflatable. One of these children is holding a toy and smiling. In the background, there are other toys, wooden objects and a person.\n' +
      '\n' +
      '**Task: LOC-NARRATIVES_open_images_detailed_image_captioning Instruction**: What is going in this image? Give me a detailed description.\n' +
      '\n' +
      '**Target: In this picture I can see food items in the jars with lids, which are on the wooden object, and there is blur background.\n' +
      '\n' +
      '**Task: SciCap+figure_captioning Instruction**: Generate a detailed caption for the given scientific figure from a paper. Your caption should mention the statistics in the figure and the language should be formal and clear.\n' +
      '\n' +
      '**Target:** Comparison of total time taken and time taken by lca/la data structure by the most efficient algorithm for insertion of m = [BRACKET] edges for different values of n.\n' +
      '\n' +
      '**Task: SentiCap_image_captioning_conditioned_on_sentiment Instruction**: You are provided with a picture, write a caption with a specific sentiment (positive or negative) related to the picture. Note that the sentiment in the caption should match the requested sentiment. Write a caption with a positive sentiment for the given image.\n' +
      '\n' +
      '**Target:** a very nice looking try filled with some excellent looking food\n' +
      '\n' +
      '**Task: textcaps_image_captioning_with_reading_comprehension Instruction**: Write a caption for the image. When you write the caption, also consider the text on the image and decide the best way to fit them into your caption.\n' +
      '\n' +
      '**Target:** Here is a caption for this image: \'A blue Magic jersey with the number 50 sits against a grey background\'\n' +
      '\n' +
      'Figure 29:\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:43]\n' +
      '\n' +
      '**Task: LSUN_scene_classification**\n' +
      '\n' +
      '**Instruction: In this task you will be provided with a picture of a scene (dining room, bedroom, kitchen, outdoor church, and so on) and you have to classify images into their corresponding scene categories. Your answer should be the name of the place. Options: (a) tower (b) classroom (c) dining room (d) bedroom (e) kitchen (f) church outside (g) living room (h) conference room (i) restaurant Target: (h) conference room**\n' +
      '\n' +
      '**Task: Places205_indoor_outdoor_classification**\n' +
      '\n' +
      '**Instruction: In this task, you have to identify if the place or scene pictured is indoor or outdoor. In the image is among a total of 205 classes such as Hospital, Bridge, Courtyard, Motel,... The classes of the images are a diverse set of places or scenes. Pay attention to the details as some of the images may contain an object that relates to a specific place while some images may directly show the place or scenary. So, your answer should be the place or scene shown in the image Options: (a) Outdoor (b) Indoor Target: (b) Outdoor**\n' +
      '\n' +
      '**Task: JHU-CROWD_scene_classification**\n' +
      '\n' +
      '**Instruction: Provide the location of the scene in the image. It could be a water park, marathon, protest, stadium, or any other possible location. Target: The scene is located at a stadium.**\n' +
      '\n' +
      '**Task: AID+aerial_scene_classification**\n' +
      '\n' +
      '**Instruction: You are given an aerial image. Tell me the scene in the image. The potential scenes are beach, industrial, meadow, and so on... Target: The aerial scene is Airport.**\n' +
      '\n' +
      '**Task: Dark-Zurich_time_of_the_day_classification**\n' +
      '\n' +
      '**Instruction: Identify the time of the day when the image is captured. Options are: daytime, nighttime, twilight. Target: The time of the day is twilight.**\n' +
      '\n' +
      'Figure 33:\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:46]\n' +
      '\n' +
      '**Task: NATURALIST_family_classification**\n' +
      '\n' +
      '**Instruction**: The family is a taxonomic rank above the genus and below the order. Identify the family of the organism in the image.\n' +
      '\n' +
      '**Target**: The family of the organism in the image is Ranunculaceae.\n' +
      '\n' +
      '**Task: NATURALIST_genus_classification**\n' +
      '\n' +
      '**Instruction**: The genus is a taxonomic rank above the species and below the family. Identify the genus of the organism in the image.\n' +
      '\n' +
      '**Target**: The genus of the organism in the image is Esox.\n' +
      '\n' +
      '**Task: NATURALIST_tain_English_name_classification**\n' +
      '\n' +
      '**Instruction**: Identify the organism in the image. Give the english name(also called common name) followed by the scientific name(also called latin name). For example : "The organism in the image is Common Earthworm. Its scientific name is Lumphicus terrestris.\n' +
      '\n' +
      '**Target**: The organism in the image is Blue-breasted Cordonbieu. Its scientific name is Uraeginthus angolensis.\n' +
      '\n' +
      '**Task: NATURALIST_order_classification**\n' +
      '\n' +
      '**Instruction**: Taxonomic category is a rank or group of organisms developed on the basis of their fundamental characteristics, similarities and dissimilarities. The order is a taxonomic rank above the family and below the class. Identify the order of the organism in the image.\n' +
      '\n' +
      '**Target**: The order of the organism in the image is Squamata.\n' +
      '\n' +
      '**Task: NATURALIST_phylum_classification**\n' +
      '\n' +
      '**Instruction**: Phylum is defined as a principal taxonomic category that ranks above class and below kingdom. Identify the phylum of the organism in the image.\n' +
      '\n' +
      '**Target**: The phylum of the organism in the image is Tracheophyta.\n' +
      '\n' +
      '**Task: NATURALIST_supercategory_classification**\n' +
      '\n' +
      '**Instruction**: You will be given an image of an organism. Analyze the image and pick the super category for this organism from the options provided. Options: (a) Animalia (b) Reptiles (c) Insects (d) Ray-finned Fishes (e) Fungi (f) Amphibians (g) Birds (h) Plants (i) Molluks (j) Mammals (k) Arachnids\n' +
      '\n' +
      '**Target**: (c) Insects\n' +
      '\n' +
      'Figure 37:\n' +
      '\n' +
      'Figure 38:\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:48]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:49]\n' +
      '\n' +
      '**Task: Fairface_human_age_classification**\n' +
      '\n' +
      '**Instruction: You are given an image of a person\'s face. This person can be of different ages, your task is to identify the person\'s age**\n' +
      '\n' +
      '**Target: The person\'s age is 10-19**\n' +
      '\n' +
      '**Task: Fairface_human_gender_classification**\n' +
      '\n' +
      '**Instruction: Here is a picture of a person. Based only upon this picture, what would you guess this person\'s gender is?**\n' +
      '\n' +
      '**Target: The person\'s gender is Female**\n' +
      '\n' +
      '**Task: Fairface_human_race_classification**\n' +
      '\n' +
      '**Instruction: What could be a good guess for this person\'s race in the given image?**\n' +
      '\n' +
      '**Target: The person\'s race is Southeast Asian**\n' +
      '\n' +
      '**Task: LFW_human_face_recognition**\n' +
      '\n' +
      '**Instruction: In this task, you will be presented with a face image of an individual. Your objective is to accurately classify the image by identifying the person\'s identity it represents. To accomplish this, you must meticulously examine the facial features present in the image, such as the shape and structure of the face, eyes, nose, mouth, hair, and any other distinguishing features such as moles, scars, or birthmarks that can provide valuable clues for determining the identity. For instance, certain facial proportions, distinct eye color, or unique hair style could be defining characteristics of an individual\'s identity. Just as one might identify a bicycle by its wheels or a sunflower by its petals in other datasets, in this case, a person can be identified by their unique set of facial features. Once you\'ve made an informed determination based on these visual clues, provide your answer as the identity of the person.**\n' +
      '\n' +
      '**Target: Pete Sampras**\n' +
      '\n' +
      'Figure 45:\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:51]\n' +
      '\n' +
      '**Task: Core50_object_recognition**\n' +
      '\n' +
      '**Instruction:** Your task is to identify the item shown in the picture. The images contain everyday objects such as a plug adapter, mobile phone, scissors, and more. It is important to carefully consider the object\'s shape, size, and color characteristics in order to accurately classify the image.\n' +
      '\n' +
      '**Target:** cup\n' +
      '\n' +
      '**Task: Office-Home_object_recognition**\n' +
      '\n' +
      '**Instruction:** Your task involves classifying object images into their respective categories like Bed, Sink, Sneakers, Table, TV and so on; for instance, if the model is presented with an image of a laptop, it should correctly identify and categorize the image as \'laptop\'.\n' +
      '\n' +
      '**Target:** Shelf\n' +
      '\n' +
      '**Task: MNIST_M_number_recognition**\n' +
      '\n' +
      '**Instruction:** In this task, you will be presented with a grayscale image containing a handwritten digit overlaid on a natural image background. Your objective is to correctly identify the digit in the image.\n' +
      '\n' +
      '**Target:** 1\n' +
      '\n' +
      '**Task: ImageNet-A_object_recognition_of_natural_adversarial_examples**\n' +
      '\n' +
      '**Instruction:** In this task, given an image, please identify what the image contains a. The image could contain, among other things, animals, birds, daily objects, insects Options: (a) The provided image contains a lorikeet (b) The provided image contains a lion (c) The provided image contains an armadillo (d) The provided image contains a baseball player (e) The provided image contains a tricycle (f) The provided image contains a rugby ball (g) The provided image contains a jack-o-lantern (h) The provided image contains a canoe\n' +
      '\n' +
      '**Target:** (a) The provided image contains a lorikeet\n' +
      '\n' +
      '**Task: MVTecAD_industrial_item_recognition**\n' +
      '\n' +
      '**Instruction:** Your objective is to classify an image based on its corresponding object category. The image provided encompasses a diverse range of industrial items, including a bottle, cable, carpet, and more. Focus on the overall visual appearance of the image, paying attention to details such as lines, shading, color scheme, and level of detail. It is crucial to analyze the distinctive characteristics of the object, such as its shape, color, and texture, as these features may vary significantly between different object categories. Once you have completed the classification process, output the appropriate object name based on your analysis.\n' +
      '\n' +
      '**Target:** The object is a pill.\n' +
      '\n' +
      'Figure 49:\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:53]\n' +
      '\n' +
      'Task: Set5_object_recognition_in_low_resolution_image\n' +
      '\n' +
      'Instruction: In this task, recognize the subject in the image from among 5 subjects, namely - baby, bird, butterfly, head, woman.\n' +
      '\n' +
      'Target: The subject in the image is a bird\n' +
      '\n' +
      'Task: Yahoo_object_recognition\n' +
      '\n' +
      'Instruction: In this task, you are given an image from a dataset, which contains images from different categories of animals, objects, and vehicles. These categories further divide into subcategories. Your job is to classify the given image into one of these subcategories, which could be anything from an aeroplane to a zebra. Your classification should be based on key identifiers like size, shape, color, distinctive features, and the context or environment depicted in the image. For example, if you\'re given an image of a zebra, your answer would simply be zebra. Remember that images could be of objects or vehicles as well. Your answer should be a single word representing the appropriate subcategory for the image, emphasizing specificity beyond the broad categories.\n' +
      '\n' +
      'Target: building\n' +
      '\n' +
      'Task: MSCOC_appliance_recognition\n' +
      '\n' +
      'Instruction: Given an image of a common electronic appliance from around the house, identify the type of object it is. It could be an appliance that is commonly used in the kitchen to cook or store food. Options: (a) This image contains an oven (b) This image contains a microwave (c) This image contains a toaster (d) This image contains a refrigerator (e) This image contains a sink\n' +
      '\n' +
      'Target: (e) This image contains a sink\n' +
      '\n' +
      'Task: MSCOC_furniture_recognition\n' +
      '\n' +
      'Instruction: Given an image of a piece of furniture in a house, identify the type of furniture. It is usually used to make the house look better and can be made of different kinds of material. Options: (a) This image contains a dining table (b) This image contains a bed (c) This image contains a toilet (d) This image contains a chair (e) This image contains a couch (f) This image contains a potted plant\n' +
      '\n' +
      'Target: (d) This image contains a chair\n' +
      '\n' +
      'Task: MSCOCOC_kitchen_object_recognition\n' +
      '\n' +
      'Instruction: Given an image of something from the kitchen, identify what it could be. The image could be of cooking tools or items that are used for eating. It could also be used for serving food or storing it. Options: (a) This image contains a bottle (b) This image contains a cup (c) This image contains a wine glass (d) This image contains a fork (e) This image contains a knife (f) This image contains a bowl (g) This image contains a spoon\n' +
      '\n' +
      'Target: (a) This image contains a bottle\n' +
      '\n' +
      'Task: MSCOCOC_vehicle_recognition\n' +
      '\n' +
      'Instruction: Given an image of a vehicle, identify the kind of vehicle it is. The vehicle can be of different types; it could be something used, personal, or public transport. It could carry one or more people at the same time. Options: (a) This image contains a bus (b) This image contains a bicycle (c) This image contains a boat (d) This image contains an airplane (e) This image contains a motorcycle (f) This image contains a train (g) This image contains a truck (h) This image contains a car\n' +
      '\n' +
      'Target: (b) This image contains a bicycle\n' +
      '\n' +
      'Figure 54:\n' +
      '\n' +
      'Figure 53:\n' +
      '\n' +
      '**Task: MSCOCO_sports_object_recognition**\n' +
      '\n' +
      '**Instruction: Given an image of sorting goods, identify what the object is. It could be used to play a team sport or an individual activity. The objects can also be used in different kinds of sports and sometimes make it easier for the wearer to play the sport. Options: (a) This image contains a ski (b) This image contains a surfboard (c) This image contains a frisbee (d) This image contains a baseball bat (e) This image contains a tennis racket (f) This image contains a baseball glove (g) This image contains a kite (h) This image contains a snowboard (i) This image contains a skateboard (j) This image contains a sports ball Target: (j) This image contains a sports ball\n' +
      '\n' +
      'Figure 56:\n' +
      '\n' +
      '**Task: Wikihow_image_text_step_ordering**\n' +
      '\n' +
      '**Instruction: You are doing Dipping Pine Cones in Paint. Is the step "Twist the end of a bamboo skewer into the top of the pine cone." the next or previous step to the step in the image? Options: (a) next (b) previous Target: (b) previous**\n' +
      '\n' +
      '**Task: Wikihow_immediate_next_step_selection**\n' +
      '\n' +
      '**Instruction: You are doing Using an Oven to Dry Cilantro. What is the next step to step in the image? Options: (a) Store the dried cilantro leaves in an airtight container. (b) Preheat your oven to 250 "F (121 "C). (c) Remove the tray from the oven and let it cool for 10 minutes. (d) Spread the leaves on the baking tray to form 1 layer. (e) Wash the cilantro to remove dirt and debris. Target: (c) Remove the tray from the oven and let it cool for 10 minutes.**\n' +
      '\n' +
      '**Task: Wikihow_text_image_step_ordering**\n' +
      '\n' +
      '**Instruction: The goal is to "Thawing with a Microwave". Given the current step "Remove the plastic wrap and inspect your dough.", Is the picture the next or the previous step? Options: previous next Target: next**\n' +
      '\n' +
      '**Task: multimodal_factual_checking Instruction**: Context: Our Rating A widely-shared Facebook post claimed California had legalized \'pedophilia,\' and that \'Now a 21 year old can have sex with an 11 year old, and not be listed on the sex registry as a sex offerender.\' That post and many like it are simply wrong. They grossly distort the proposals in state SB 145, which aims to eliminate a disparity in how LGBT young people are treated on California\'s sex offerender registry. The legislation would eliminate automatic sex offer registration for young adults who are convicted of having voluntary and oral sex with a minor and are within 10 years of age of the victim. Instead, a judge would make that decision, just as existing law allows judges to decide whether to place offenders in cases involving vaginal intercourse on the registry. The bill would not, in any fashion, make it legal for any adult to have any type of sex with a minor. The only change involves giving a judge discretion over whether to list an offender on the sex registry for certain sex acts. We rate the claims in the Facebook post Pants on Fire. Pants ON FIRE - The statement is not accurate and makes a ridiculous claim. Does the context support "PEDOPHILA is now LEGAL in CALIFORNIA. Now a 21 year old can have sex with an 11 year old, and not be listed on the sex registry as a sex offender." Options: (a) not sure (b) no (c) yes\n' +
      '\n' +
      '**Target:** A1: (b) no\n' +
      '\n' +
      'Figure 58\n' +
      '\n' +
      '**Task:**RAVEN_relational_and_analogical_visual_reasoning Instruction**: Each image has 8 images labeled as image 1 to Image 8. These 8 images follow a specific pattern. Detect the pattern and select the next image in the sequence from the 8 available options.\n' +
      '\n' +
      '**Target:** Option 4\n' +
      '\n' +
      'Figure 59\n' +
      '\n' +
      '**Task:**image_text_matching Instruction**: Does "A woman in blue and purple holds a snowboard while standing in the snow." describes image? Options: (a) the description matches the image (b) the text is not a description of the image\n' +
      '\n' +
      '**Target:** (a) the description matches the image\n' +
      '\n' +
      '**Task:**Winoground+image_caption_matching Instruction**: In this task, you will be provided with an image and two captions. Your task is to identify which of the two captions correctly describes the image. Options: (a) the white wall will soon be painted blue (b) the blue wall will soon be painted white\n' +
      '\n' +
      '**Target:** (a) the white wall will soon be painted blue\n' +
      '\n' +
      '**Task:**image_text_selection Instruction**: Which option in the options that is the caption of the image. Options: (a) A couple of laptops with one sitting on a microwave. (b) Two older women are preparing for a dinner. (c) A desk with a computer monitor, printer and cd rack. (d) A girl preparing to put conditions on her dinner plate. (e) A man is taking an image on his phone of a bus.\n' +
      '\n' +
      '**Target:** (d) A girl preparing to put conditions on her dinner plate.\n' +
      '\n' +
      'Figure 61\n' +
      '\n' +
      '**Task: DOMAIN-NET_object_recognition_in_clip_art Instruction: Clip art is defined as simple pictures or symbols used in documents and presentations. The input is a clip art image. Identify the main object in the image. Target: zigzag\n' +
      '\n' +
      '**Task: DOMAIN-NET_object_recognition_in_infograph Instruction: An info graph is a visual image like a poster that is used to represent information or data about any object. For this task, the input will be a info graph. Identify the main object of the info graph. Target: toaster**\n' +
      '\n' +
      '**Task: DOMAIN-NET_object_recognition_in_painting Instruction: The input for this task is a painting. Identify the main object in the painting. Target: see saw**\n' +
      '\n' +
      'Figure 62\n' +
      '\n' +
      '**Task: DOMAIN-NET_object_recognition_in_quickdraw Instruction: In this task, the input will be a rough sketch of something. Identify the main object depicted in the rough sketch. Target: dumbbell**\n' +
      '\n' +
      '**Task: DOMAIN-NET_object_recognition_in_real_image Instruction: Identify the main object in the image. Target: blueberry**\n' +
      '\n' +
      '**Task: ExDark_object_recognition_in_low_light_environments Instruction: The given image is taken in low-light environments. Identify the object in the image, including bicycle, boat, bottle, bus, car, and other objects. Target: The object is bicycle.**\n' +
      '\n' +
      'Figure 61\n' +
      '\n' +
      '**Task: ImageNet-R_object_recognition_in_diverse_image_domain**\n' +
      '\n' +
      '**Instruction**: Your task is to classify the image using various categories. You need to carefully observe the details of the object in the image, including its shape, color, and texture, as these characteristics may vary across different renditions. Output the appropriate object name as the result of your classification process.\n' +
      '\n' +
      '**Target: great white shark\n' +
      '\n' +
      '**Task: ImageNet_object_recognition_in_sketch**\n' +
      '\n' +
      '**Instruction**: You are given a sketch of an object. Tell me the name of the object in the image.\n' +
      '\n' +
      '**TargetThe sketch is a iron.\n' +
      '\n' +
      '**Task: PACS_object_recognition_in_art_painting**\n' +
      '\n' +
      '**Instruction**: You will be given an art painting image as input. Identify the main object in the image.\n' +
      '\n' +
      '**Target: dog\n' +
      '\n' +
      '**Task: PACS_object_recognition_in_cartoon**\n' +
      '\n' +
      '**Instruction**: You will be given an image of a cartoon. Identify the main object in the image.\n' +
      '\n' +
      '**Target: horse\n' +
      '\n' +
      '**Task: PACS_object_recognition_in_photograph**\n' +
      '\n' +
      '**Instruction**: The input is a photograph of an object. Identify the main object in the image.\n' +
      '\n' +
      '**Target: elephant\n' +
      '\n' +
      '**Task: SKETCH_living_organism_classification_in_sketch**\n' +
      '\n' +
      '**Instruction**: In this task, you will identify whether the picture contains a living organism. The images given are black and white sketches drawn by human beings. If the picture depicts a living organism or part of a living organism, the output should be "Living". Otherwise, print "Non-Living"\n' +
      '\n' +
      '**Target: Living\n' +
      '\n' +
      '**Task:**\n' +
      '\n' +
      '**Task:****SXETCH_object_recognition_in_sketch**\n' +
      '\n' +
      '**Instruction: Each image is a human drawn sketch of an object. Identify the main object in the image. Target: microphone**\n' +
      '\n' +
      '**Task:****Cinic-10_animal_recognition_in_low_resolution_image**\n' +
      '\n' +
      '**Instruction: The given image can contain various types of animals. Some of these animals are found in forests, drylands, or other natural areas. Some of them could also be domesticated pets. Please identify the animal in the picture. Target: The image contains a bird**\n' +
      '\n' +
      '**Task:****Cinic-10_shipping_method_recognition_in_low_resolution_image**\n' +
      '\n' +
      '**Instruction: The given image can contain different types of shipping equipment. They can carry goods across water or land, and they carry all types of materials required around the world. Please identify the type of shipping option in the picture. Target: The image contains a ship**\n' +
      '\n' +
      '**Figure 66**\n' +
      '\n' +
      '**Task:****Cinic-10_transportation_option_recognition_in_low_resolution_image**\n' +
      '\n' +
      '**Instruction: The given image can contain different types of transport vehicles. People use these vehicles to travel around in their day-to-day lives. It could be air travel or a slower means of transport on the ground. Please identify the type of transport option in the picture. Target: The image contains an automobile**\n' +
      '\n' +
      '**Task:****Cinic-10_animal_presence_classification_in_low_resolution_image**\n' +
      '\n' +
      '**Instruction: The given image can contain some animals; they can be animals typically found in the wild or domesticated animals. The picture could also contain something that does not fit this description. Your job is to identify if the subject of the image is an animal or not. Target: The object is an animal**\n' +
      '\n' +
      '**Task:****Cinic-10+object_shipping_object_presence_in_low_resolution_image**\n' +
      '\n' +
      '**Instruction: The given image can contain some vehicles used for transporting goods and materials across large distances, even around the world. The picture could also contain something that does not fit this description. Your job is to identify if the subject of the image can be used for shipping goods or not. Target: The object can be used for shipping**\n' +
      '\n' +
      '**Task:** **ViDA-2017_object_recognition_in_3D_rendered_image**\n' +
      '\n' +
      '**Instruction:** Your task is to classify an image based on its corresponding object category. The image contains a variety of objects distributed among 12 categories, including aeroplane, horse, knife, person, plant, and others. To accurately classify the image, carefully analyze its visual characteristics, such as shape, color, texture, and spatial context relations, as these attributes can vary significantly across different domains. Once you have identified the object category of the image, output the appropriate label for your classification.\n' +
      '\n' +
      '**Target:** plant\n' +
      '\n' +
      '**Task:** **ViSDA-2017_multiple_choice_object_recognition_in_3D_rendered_image**\n' +
      '\n' +
      '**Instruction:** You are given an image which contains a 3D rendered object. Your goal is to identify the category of the object present in the image from the given options. Options: (a) knife (b) horse (c) train (d) bus (e) plant (f) skateboard (g) car (h) bicycle (i) truck (j) aeroplane\n' +
      '\n' +
      '**Target:** (i) truck\n' +
      '\n' +
      '**Task:** **DOMAIN-NET_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image. Answer 2 questions. What kind of image is this? Choose from clip art, info graph, painting, rough sketch, painting, real and sketch. Second question, what is the main object in the image? Answer it like "This is a clip art of an apple."\n' +
      '\n' +
      '**Target:** This is a painting of a trumpet.\n' +
      '\n' +
      '**Task:** **ImageNet-R_image_style_classification**\n' +
      '\n' +
      '**Instruction:** Your goal is to classify the image based on its domain, which can be \'videogame\', \'painting\',\'sketch\', \'cartoon\', \'art\', \'toy\', \'deviantart\', \'graphic\',\'sculpture\',\'misc\', \'embroidery\',\'sticker\', \'graffiti\', \'origami\', or \'tattoo\'. Your final output should specify the identified domain of the image.\n' +
      '\n' +
      '**Target:** misc\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction: You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction: You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction: You will be given an image of a dog. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category. Options: (a) Art painting (b) Cartoon (c) Sketch (d) Photograph\n' +
      '\n' +
      '**Target:** (b) Cartoon\n' +
      '\n' +
      '**Task:** **PACS_dog_dog_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will\n' +
      '\n' +
      '**Task: PACS_elephant_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of an elephant. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category.\n' +
      '\n' +
      'Options: (a) Cartoon (b) Art painting (c) Photograph (d) Sketch\n' +
      '\n' +
      '**Target:** (d) Sketch\n' +
      '\n' +
      '**Task: PACS_giraffe_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a guitar. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category.\n' +
      '\n' +
      'Options: (a) Sketch (b) Cartoon (c) Art painting (d) Photograph\n' +
      '\n' +
      '**Target:** (a) Sketch\n' +
      '\n' +
      '**Task: PACS_guitar_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a guitar. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category.\n' +
      '\n' +
      'Options: (a) Cartoon (b) Photograph (c) Sketch (d) Art painting\n' +
      '\n' +
      '**Target:** (b) Photograph\n' +
      '\n' +
      '**Task: PACS_hose_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a house. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category.\n' +
      '\n' +
      'Options: (a) Sketch (b) Photograph (c) Art painting (d) Sketch\n' +
      '\n' +
      '**Target:** (b) Photograph\n' +
      '\n' +
      '**Task: PACS_person_image_style_classification**\n' +
      '\n' +
      '**Instruction:** You will be given an image of a person. The image could be of different categories like painting, cartoon, photograph, or sketch. Identify the image category.\n' +
      '\n' +
      '**Target:** Cartoon\n' +
      '\n' +
      '**Task:** **Model-vs-human_image_style_classification**\n' +
      '\n' +
      '**Instruction:** What is the artistic style of this image?\n' +
      '\n' +
      '**Target:** power-equalsiat\n' +
      '\n' +
      'Figure 71:\n' +
      '\n' +
      'Figure 72:\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:62]\n' +
      '\n' +
      '**Task: DTD+coarse_grained_texture_classification**\n' +
      '\n' +
      '**Instruction: Texture is defined as the feel, appearance or consistency of a surface or substance from a human\'s perspective. Detect the primary texture represented in the image. Target: cracked**\n' +
      '\n' +
      '**Task: DeepFashion_cloth_texture_classification**\n' +
      '\n' +
      '**Instruction: Can you write a very short description of the cloth? Target: The cloth is an Abstract Mirrored Print Dress.**\n' +
      '\n' +
      '**Task: DTD_multiple_texture_detection**\n' +
      '\n' +
      '**Instruction: Texture is defined as the feel, appearance or consistency of a surface or substance from a human\'s perspective. Detect all the textures in the image. Present it as a comma separated list Target: porous**\n' +
      '\n' +
      'Figure 75:\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:64]\n' +
      '\n' +
      'Figure 76\n' +
      '\n' +
      '**Task:**VQA_color\n' +
      '\n' +
      '**Instruction:** In this task, you are asked the color of some object in the image. Your answer should be a phrase.\n' +
      '\n' +
      '**Input:** Question: What color is the umbrella primarily?\n' +
      '\n' +
      '**Target:** orange\n' +
      '\n' +
      '**Task:**Visual7W_VQA_object_attribute\n' +
      '\n' +
      '**Instruction:** In this task, you will be asked about the attribute of some object. Your answer should be very concise.\n' +
      '\n' +
      '**Input:** Question: What is the wall of the tub made of?\n' +
      '\n' +
      '**Target:** tile\n' +
      '\n' +
      '**Task:**VQA_activity_recognition\n' +
      '\n' +
      '**Instruction:** In this task, you need to answer a question about the main activity happening in the image. Your answer should be one or two words.\n' +
      '\n' +
      '**Input:** What is the girl doing?\n' +
      '\n' +
      '**Target:** Eating.\n' +
      '\n' +
      'Figure 77\n' +
      '\n' +
      '**Task:**VQA_activity_recognition\n' +
      '\n' +
      '**Instruction:** In this task, you need to answer a question about the main activity happening in the image. Your answer should be one or two words.\n' +
      '\n' +
      '**Input:** What is the girl doing?\n' +
      '\n' +
      '**Target:** Eating.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:66]\n' +
      '\n' +
      '**Task:** VQA_positional_reasoning\n' +
      '\n' +
      '**Instruction:** In this task, the goal is to understand the location of objects within the presented image and provide an answer to the question. Given me a very short answer.\n' +
      '\n' +
      '**Input:** What is to the right of cake?\n' +
      '\n' +
      '**Target:** fork\n' +
      '\n' +
      '**Task:** VQA_scene_recognition\n' +
      '\n' +
      '**Instruction:** You are asked a question about the scene in the image. Answer the question with one or two words.\n' +
      '\n' +
      '**Input:** Is this indoor or outdoor?\n' +
      '\n' +
      '**Target:** indoor\n' +
      '\n' +
      '**Task:** VQA_sentiment_understanding\n' +
      '\n' +
      '**Instruction:** In this task, you will be asked a question regarding the emotion conveyed in the image. I need a short and concise answer.\n' +
      '\n' +
      '**Input:** The question is is this dog happy?\n' +
      '\n' +
      '**Target:** yes\n' +
      '\n' +
      '**Task:** VQA_sport_recognition\n' +
      '\n' +
      '**Instruction:** Given a picture about sports, answer the following question. Answer the question with one or two words.\n' +
      '\n' +
      '**Input:** What sport will the man be doing?\n' +
      '\n' +
      '**Target:** surfing\n' +
      '\n' +
      '**Task:** VQA_utility_affordance\n' +
      '\n' +
      '**Instruction:** Please take a look at the picture and answer the following question by thinking about what each object in the picture can be used for. Your output can contain or two words.\n' +
      '\n' +
      '**Input:** What is the man chopping?\n' +
      '\n' +
      '**Target:** garlic\n' +
      '\n' +
      '**Task:** VQAv2_general_VQA\n' +
      '\n' +
      '**Instruction:** **Instruction:** Please take a look at the picture and answer a general question about the picture. Your output can contain one or two words.\n' +
      '\n' +
      '**Input:** What color is the sign?\n' +
      '\n' +
      '**Target:** red and white\n' +
      '\n' +
      '**Figure 81**\n' +
      '\n' +
      '**Task: Visual-Genome_spatial_relationship_question_answering Instruction: You are asked a question about the spatial relationship of objects in the image. Answer question with a short phrase. Input: What is on the pizza? Target: Ham\n' +
      '\n' +
      '**Task: CLEVR-CoGenT_question_answer_matching Instruction: In this task, you will be presented with an image containing 3D-rendered objects along with a set of questions and corresponding answers. Your goal is to correctly match each question with its corresponding answer based on the visual content of the image. The output format should follow this pattern: Q1A3, Q2A5, Q3A2, Q4A1, Q5A1, indicating the question number followed by the corresponding answer number. Input: Q1: How many other objects are there of the same color as the rubber ball? Q2: Is the color of the shiny object that is right of the cyan rubber cylinder the same as the big cylinder? Q3: What is the yellow object that is in front of the tiny cyan cylinder made of? Q4: Is the material of the large purple object the same as the large sphere? Q5: There is a yellow metal block that is behind the cyan rubber object; does it have the same size as the tiny cyan cylinder? Target: Q1A5 Q2A7 Q3A8 Q4A6 Q5A6 Q6A3 Q7A2 Q8A1 Q9A4\n' +
      '\n' +
      '**Task: Viewiz_answering_visual_questions_from_blind_people Instruction: A blind person asks you a question about this image, answer the question in the best way possible. Input: What kind of food is this? Options: (a) canned beans (b) bushs reduced sodium dark red kidney beans (c) dark red kidney beans (d) kidney beans (e) reduced sodium kidney beans Target: (b) bushs reduced sodium dark red kidney beans**\n' +
      '\n' +
      '**Task: DAQUAR_VQA Instruction: The input text will contain a question about the image. Answer the question. Your output should be one or two words. Input: What is at the right side of the plant? Target: cabinet**\n' +
      '\n' +
      '**Task: CLEVR-CoGenT_VQA_in_3D_rendered_images Instruction: The input for this task is an image of 3D-rendered objects and a question that fall into different categories. The questions fall into five classes of tasks: Exist, Count, Compare Integer, Query Attribute, and Compare Attribute. The task here is to answer the question and your answer should be one or two tokens. Input: The cyan cylinder is what size? Target: large**\n' +
      '\n' +
      '**Task: CLEVR_question_answer_matching**\n' +
      '\n' +
      '**Instruction: You will be given an image of 3D-rendered objects, a number of Questions and same number of answers. The task here is to match the questions to the right answers according to the image you see. The format of the output should be something like: Q1A3,Q2A5,Q3A2,Q4A1,Q5A1**\n' +
      '\n' +
      '**Input:** Q1: There is a shiny thing right of the big ball that is in front of the matte object in front of the matte cylinder; what is its shape? Q2: Is there any other thing that has the same size as the cube? Q3: Are there more balls that are behind the green metal sphere than rubber objects right of the gray matte ball? Q4: Is there a metal sphere on the right side of the rubber object on the left side of the yellow rubber thing?\n' +
      '\n' +
      '**Target:** Q1A2 Q2A3 Q3A3 Q4A3 Q5A6 Q6A3 Q7A1 Q8A5 Q9A4\n' +
      '\n' +
      '**Task: CLEVR_VQA_in_30_rendered_images_with_multiple_questions**\n' +
      '\n' +
      '**Instruction: The input for this task is an image of 3D-rendered objects and a set of questions that fall into different categories. The questions fall into five classes of tasks: Exist, Count, Compare Integer, Query Attribute, and Compare Attribute. The output of this task is a set of answers to the given questions for each image. The answers should be generated based on the content of the image and the category of the question. The output should be in the form of text.**\n' +
      '\n' +
      '**Input:** Q1: Are there any other things that are the same color as the metallic cube? Q2: The matte thing that is in front of the small yellow metal thing has what shape? Q3: What is the size of the cyan thing that is left of the cyan matte cylinder behind the yellow matte thing? Q4: Does the yellow thing that is in front of the tiny yellow rubber block have the same shape as the tiny thing that is in front of the tiny metal block?..\n' +
      '\n' +
      '**Target:** A1: yes A2: cylinder A3: small A4: no A5: yes A6: 2 A7: large A8: no A9: rubber\n' +
      '\n' +
      '**Task:** KVQA_world_knowledge_enabled_VQA**\n' +
      '\n' +
      '**Instruction: You are provided with a picture and a question related to the picture. Your job is to correctly answer the question with your background knowledge. Note that any references to directions (left, right, etc.) in the questions are from the perspective of the person depicted in the image. Your answer should consist of entity names.**\n' +
      '\n' +
      '**Input:** In which continent was the person in the image born?\n' +
      '\n' +
      '**Target:** North America\n' +
      '\n' +
      '**Task:** VIQAF_knowledge_based_VQA_about_entities**\n' +
      '\n' +
      '**Instruction: With the help of this image, can you answer the question given in the input text by connecting the visual features in the image with named entities. Your answer should be a named entity.**\n' +
      '\n' +
      '**Input:** this mountain is the highest point in which country?\n' +
      '\n' +
      '**Target:** Nam Chosun\n' +
      '\n' +
      '**Task:** VOARAD_VQA_in_radiology**\n' +
      '\n' +
      '**Instruction: I will give you a radiology image(scan of a body part). Analyze it and answer the question given in the input text.**\n' +
      '\n' +
      '**Input: Does the patient have a central line placed?**\n' +
      '\n' +
      '**Target:** Yes\n' +
      '\n' +
      '**Task:** VOARAD_VQA_in_radiology**\n' +
      '\n' +
      '**Instruction: I will give you a radiology image(scan of a body part). Analyze it and answer the question given in the input text.**\n' +
      '\n' +
      '**Output:** The output of the task is a set of answers to the given questions for each image. The answers should be generated based on the content of the image and the category of the question. The output should be in the form of text.**\n' +
      '\n' +
      '**Input:** Q1: Are there any other things that are the same color as the metallic cube? Q2: The matte thing that is in front of the small yellow metal thing has what shape? Q3: What is the size of the cyan thing that is left of the cyan matte cylinder behind the yellow matte thing? Q4: Does the yellow thing that is in front of the tiny yellow rubber block have the same shape as the tiny thing that is in front of the tiny metal block?..\n' +
      '\n' +
      '**Target:** A1: yes A2: cylinder A3: small A4: no A5: yes A6: 2 A7: large A8: no A9: rubber\n' +
      '\n' +
      '**Task:** KVQA_world_knowledge_enabled_VQA**\n' +
      '\n' +
      '**Instruction: You are provided with a picture and a question related to the picture. Your job is to correctly answer the question with your background knowledge. Note that any references to directions (left, right, etc.) in the questions are from the perspective of the person depicted in the image. Your answer should consist of entity names.**\n' +
      '\n' +
      '**Input:** In which continent was the person in the image born?\n' +
      '\n' +
      '**Target:** North America\n' +
      '\n' +
      '**Task:** VIQAF_knowledge_based_VQA_about_entities**\n' +
      '\n' +
      '**Instruction: With the help of this image, can you answer the question given in the input text by connecting the visual features in the image with named entities. Your answer should be a named entity.**\n' +
      '\n' +
      '**Input:** This mountain is the highest point in which country?**\n' +
      '\n' +
      '**Target:** Nam Chosun\n' +
      '\n' +
      '**Task:** VOARAD_VQA_in_radiology**\n' +
      '\n' +
      '**Instruction: I will give you a radiology image(scan of a body part). Analyze it and answer the question given in the input text.**\n' +
      '\n' +
      '**Input: Does the patient have a central line placed?**\n' +
      '\n' +
      '**Target:** Yes\n' +
      '\n' +
      '**Task:** VOARAD_VQA_in_radiology**\n' +
      '\n' +
      '**Instruction: I will give you a radiology image(scan of a body part). Analyze it and answer the question given in the input text.**\n' +
      '\n' +
      '**Output:** The task is a set of answers to the given questions for each image. The answers should be generated based on the content of the image and the category of the question. The output should be in the form of text.**\n' +
      '\n' +
      '**Input:** Q1: Are there any other things that are the same color as the metallic cube? Q2: The matte thing that is in front of the small yellow metal thing has what shape? Q3: What is the size of the cyan thing that is left of the cyan matte cylinder behind the yellow matte thing? Q4: Does the yellow thing that is in front of the tiny yellow rubber block have the same shape as the tiny thing that is in front of the tiny metal block?..\n' +
      '\n' +
      '**Target:** A1: yes A2: cylinder A3: small A4: no A5: yes A6: 2 A7: large A8: no A9: rubber\n' +
      '\n' +
      '**Task:** KVQA_world_knowledge_enabled_VQA**\n' +
      '\n' +
      '**Instruction: You are provided with a picture and a question related to the picture. Your job is to correctly answer the question with your background knowledge. Note that any references to directions (left, right, etc.) in the questions are from the perspective of the person depicted in the image. Your answer should consist of entity names.**\n' +
      '\n' +
      '**Input:** In which continent was the person in the image born?**\n' +
      '\n' +
      '**Target:** North America\n' +
      '\n' +
      '**Task:** VIQAF_knowledge_based_VQA_about_entities**\n' +
      '\n' +
      '**Instruction: With the help of this image, can you answer the question given in the input text by connecting the visual features in the image with named entities. Your answer should be a named entity.**\n' +
      '\n' +
      '**Input:** This mountain is the highest point in which country?**\n' +
      '\n' +
      '**Target:** Nam Chosun\n' +
      '\n' +
      '**Task:** VOARAD_VQA_in_radiology**\n' +
      '\n' +
      '**Instruction: I will give you a radiology image(scan of a body part). Analyze it and answer the question given in the input text.**\n' +
      '\n' +
      '**Input: Does the patient have a central line placed?Figure 87\n' +
      '\n' +
      '**Task: GEDMETRY3K_geometry_question_answering Instruction: I will give you a figure with some geometrical information. Analyze the image and data in the input text and answer the question. Input: a = 8, b = 15, and c = 17, find tan B. Options: (a) 2.43 (b) 1.88 (c) 1.67 (d) 1.23 Target: (b) 1.88 Task: Iconqa_abstract_diagram_understanding Instruction: I have a question about the given abstract diagram, can you please give me a short answer? Input: Ella is making her bed one morning. The clock shows the time. What time is it? Target: The answer is 6:00 A.M.\n' +
      '\n' +
      '**Task: Iconqa_fill_in_blank_in_abstract_diagram_understanding Instruction: Hey, here is an abstract diagram and sentence describing it. Can you help to fill in the missing part in the given sentence? Input: The number_is shown. Target: 22\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:71]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>