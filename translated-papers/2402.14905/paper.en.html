<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'Consequently, it is imperative that we downsize LLMs.\n' +
      '\n' +
      'Furthermore, considerations of _portability_ and _computational cost_ propel the necessity to deploy LLMs on smartphones and mobile devices. In the current landscape of mobile technology, integrating a LLM like the LLaMA-v2 7B (Touvron et al., 2023b) with 8-bit weights proves prohibitively expensive due to limitations in main-memory (DRAM) capacity source. A prevalent memory hierarchy in mobile devices is depicted in Figure 2. With DRAM capacities ranging from 6 GB for the iPhone 15 and 12 GB for the Google Pixel 8 Pro (Hristov, 2022; Google, 2023), a mobile app should not exceed 10% of the DRAM, since DRAM is shared with the operating system and other applications (Malladi et al., 2012). This motivates deploying sub-billion parameter LLMs. Additionally, factoring in LLM energy consumption (0.1 J/token per billion in model parameters (Han et al., 2016; Malladi et al., 2012)), a 7B-parameter LLM consumes 0.7 J/token. A fully charged iPhone, with approximately 50kJ of energy, can sustain this model in conversation for less than 2 hours at a rate of 10 tokens/s, with every 64 tokens draining 0.2% of the battery.\n' +
      '\n' +
      'These demands converge on a singular imperative: the adoption of compact models for on-device execution. By utilizing a sub-billion model, such as a 350M 8-bit model consuming only 0.035 J/token, an iPhone can support conversational use an entire day. Moreover, the decoding speed can be significantly enhanced, as exemplified by the benchmark results of the 125M model, capable of operating at of 50 tokens/s, comparing to the state-of-the-art iPhone App MLC Chat utilizing the LLaMA 7B model at 3\\(\\sim\\)6 tokens/second5. In light of these considerations, our paper is motivated by the design and implementation of LLMs with parameters less than 1 billion.\n' +
      '\n' +
      'Footnote 5: [https://llm.mlc.ai](https://llm.mlc.ai)\n' +
      '\n' +
      'We make following contributions to build the most accurate LLMs to date under 1 billion parameters.\n' +
      '\n' +
      '* Contradictory to the scaling law (Kaplan et al., 2020), we demonstrate that depth is more important than width for small LLMs. A deep-and-thin model structure excels in capturing abstract concepts, resulting in superior final performance.\n' +
      '* We revisit embedding sharing methods (Zhang et al., 2022) and implement grouped query attention (Ainslie et al., 2023) in small LLMs to maximize weight utilization.\n' +
      '* We propose immediate block-wise weight sharing. In scenarios where memory movement is the latency bottleneck, weight sharing between two adjacent blocks avoids weight movement, requiring only computing the block twice and incurring minimal latency overhead.\n' +
      '* We propose a new family of models, MobileLLM, showcasing SOTA performance. In a suite of zero-shot tasks, MobileLLM outperforms the previous SOTA 125M/350M models by 2.7%/4.3%.\n' +
      '* In downstream tasks, such as Chat and API calling, MobileLLM model family significantly outperforms equivalently-sized models. In an API calling task, MobileLLM even achieves comparable exact-match score as considerably larger LLaMA-v2 7B model.\n' +
      '\n' +
      '## 2 Improving Sub-billion Scale LLM Design\n' +
      '\n' +
      'In this section, we present the evolutionary path from a baseline sub-billion parameter model to the new state-of-the-art models (Figure 3). We explore both 125M and 350M models and demonstrate consistent improvements in both cases. For on-device use cases where model size is a major constraint, how to allocate the limited weight parameters effectively becomes more critical than ever. We first propose a strong baseline model named MobileLLM by testing out four model design techniques that are beneficial for sub-billion scale LLMs, including (1) adopting SwiGLU FFN (Dauphin et al., 2017); (2) forcing _lanky_ (deep and thin) architectures (3) revisiting embedding sharing method (Zhang et al., 2022) (4) utilizing grouped query attention (Chowdhery et al., 2023). Then we develop an immediate block-wise layer sharing method that further boosts accuracy without incurring any additional memory overhead and with only slight latency overhead in the memory-bounded LM decoding process. We denote our model with layer sharing as MobileLLM-LS.\n' +
      '\n' +
      '### Training Setup\n' +
      '\n' +
      'Our experiments are conducted on 32 A100 GPUs, with each GPU having a batch size of 32. We performed exploratory experiments with 120k iterations on 0.25T tokens. Subsequently, the top models reported in Table 3 and Table 4, are trained with 480k iterations on 1T tokens.\n' +
      '\n' +
      'Figure 2: Memory hierarchy in prevalent mobile devices. Despite adequate Flash storage, the operational memory for executing high-speed applications predominantly resides in DRAM, typically constrained to 6-12 GB.\n' +
      '\n' +
      'We evaluate the pre-trained model on zero-shot common sense reasoning tasks, including ARC-easy, ARCH-challenge (Clark et al., 2018), BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), OBQA (Mihaylov et al., 2018), WinoGrande (Sakaguchi et al., 2021), as well as question answering and reading comprehension tasks using TQA (Joshi et al., 2017) and RACE dataset (Lai et al., 2017).\n' +
      '\n' +
      '### Building a Strong Baseline\n' +
      '\n' +
      '#### 2.2.1 Feed-forward Network Choice\n' +
      '\n' +
      'We first investigate activation functions commonly used in feed-forward networks (FFNs) and find that the state-of-the-art SwiGLU (Dauphin et al., 2017) is also beneficial for small models. By changing the vanilla FFN (\\(FC\\to ReLU\\to FC\\)) to SwiGLU, The average performance on zero-shot reasoning tasks is boost from 42.6 to 43.9 for the 125M model. Therefore, we use SwiGLU in FFN for the experiments afterwards.\n' +
      '\n' +
      '#### 2.2.2 Architecture Depth vs Width\n' +
      '\n' +
      'A prevalent belief (Kaplan et al., 2020) in the field suggests that the performance of transformer models is primarily determined by number of parameters, the size of the training dataset, and the number of training iterations. This belief posits that architectural designs have negligible impact on the transformer model\'s performance. However, our findings indicate that this may not hold true for smaller models.\n' +
      '\n' +
      'Our experimental results, specifically for small models with limited model capacity, reveals that going deeper is more crucial than going wider for performance improvement. We conducted an extensive study involving the training of 19 models, including 9 models with \\(\\sim\\)125M parameters and 10 models with \\(\\sim\\)350M parameters. Each model is designed with similar size but varied in terms of depth and width. We experiment on eight zero-shot common sense reasoning tasks, as well as question answering and reading comprehension benchmarks. Our findings consistently demonstrate that deeper and thinner models outperform their shallower and wider counterparts. Figure 4 (a) and (b) illustrate the superior performance of deeper networks across most zero-shot reasoning tasks, including ARC-easy, ARC-challenge, PIQA, HellaSwag, OBQA, WinoGrande. Particularly, this trend is even more pronounced on the TQA and RACE datasets, as shown in Figure 4 (c)-(f). Detailed model configurations and results can be seen in appendix. Our findings suggest that models with 30 or even 42 layers perform significantly better than those with 12 layers for transformer models sized around 125M. This finding is surprising considering the number of layers in most previous 125M models (Zhang et al., 2022; Black et al., 2022) is limited to 12.\n' +
      '\n' +
      '#### 2.2.3 Embedding Sharing\n' +
      '\n' +
      'In sub-billion scale language models, the embedding layers constitute a significant portion of the parameter count. For instance, with an embedding dimension of 512 and a vocabulary size of 32k, the input and output embedding layers each comprise 16 million parameters. Together, these embedding layers account for more than 20% the total parameters of a 125M-parameter model. Contrastingly, this proportion is considerably lower in larger language models. For example, the input and output embeddings only account for 3.7% of the total number of parameters in the LLaMA-7B model (Touvron et al., 2023) and a mere 0.7% in the LLaMA-70B model. This disparity might elucidate why embedding sharing was initially proposed and implemented in OPT models (Zhang et al., 2022) but was subsequently disregarded in recent designs of LLMs.\n' +
      '\n' +
      'In the development of sub-billion scale language models, we revisit the concept of input-output embedding sharing. The input embedding in LLM models maps the token id in the vocabulary to the corresponding token embedding and has a dimension of \\((vocab\\_size,embedding\\_dim)\\). Conversely, the output fully-connected layer maps the embedding dimension back to the logits prediction across the vocabulary, with weight size of \\((vocab\\_size,embedding\\_dim)\\). By sharing\n' +
      '\n' +
      'Figure 3: Design roadmap of sub-billion sized transformer models. The foreground and background bars represents the averaged accuracy on zero-shot common sense reasoning tasks for 125M and 350M models, respectively. The 125M model, initially a 12-layer 768-dimension structure, is enhanced through improving feed-forward network design, network depth adjustments, and weight sharing strategies. The detailed accuracy on each modification can be found in the appendix.\n' +
      '\n' +
      'the embedding, we reuse the input embedding weights as the output fully connected layer weights, resulting in a more efficient and compact model architecture.\n' +
      '\n' +
      'We experiment on a 30-layer 125M model. In Table 1, we demonstrate that sharing the input and output embeddings reduces the number of parameters by 16M, approximately 11.8% of total parameters with a 0.2 points drop in average accuracy. The marginal accuracy drop can be readily restored by reallocating the saved parameters to add more layers. Increasing the depth to 32 layers produces a 0.4 points accuracy gain while still maintaining 10M fewer parameters compared to the original 135M model. Similar results are also observed in 350M models. These findings further suggest that embedding sharing is a valuable technique for maximizing weight utilization and optimizing model performance given limited model storage budget.\n' +
      '\n' +
      '#### 2.2.4 Number of Heads and KV Heads\n' +
      '\n' +
      'We now investigate the optimal head size for small transformer models. The trade-off between more semantics per head dimension and more non-linear combinations of multiple heads is a key consideration in choosing the head size. In addition, most previous studies have typically used an identical number of key-value heads to query heads in sub-billion parameter language models. Instead, we found that grouped query attention, which is initially designed for reducing key-value cache size in LLMs Chowdhery et al. (2023); Ainslie et al. (2023), that can also effectively reduce redundancy in key-value heads in small LMs. Grouped query attention can be viewed as another form of weight-sharing for weight re-utilization, where the number of key-value heads is \\(1/n\\) that of query heads, and the kv head is repeated \\(n\\) times in computing attention scores and output together with the query. Here, \\(n\\in\\mathbb{Z}^{+}\\) denotes a positive integer that the number of query heads are divisible by.\n' +
      '\n' +
      'To establish a solid foundation for a state-of-the-art small transformer model, we conducted experiments to determine the desirable head size on 125M and 350M models. Results in Figure 5 show that using 16 query heads produces the best results. Additionally, reducing the number of kv-heads from 16 to 4 resulted in comparable accuracy for the 125M model\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **\\# Params** & **ARC-e** & **ARC-c** & **BoolQ** & **PIQA** & **SIQA** & **HS** & **OBQA** & **WinoGrande** & **Avg.** \\\\ \\hline Without emb-share & 135M & 43.6 & 26.1 & 58.0 & 62.5 & 42.6 & 36.5 & 37.5 & 51.5 & 44.8 \\\\ + emb-share & 119M & 44.4 & 26.0 & 56.2 & 62.8 & 43.1 & 35.9 & 36.0 & 52.6 & 44.6 \\\\ + emb-share, \\(\\uparrow\\) depth & 125M & 43.3 & 26.4 & 54.4 & 64.7 & 43.5 & 36.9 & 38.5 & 52.6 & 45.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Ablation study on input-output embedding sharing with a 30-layer model with 512 embedding dimension, on zero-shot common-sense reasoning tasks. Increased depth (\\(\\uparrow\\) depth) model has 32 layers.\n' +
      '\n' +
      'Figure 4: Under comparable model sizes, deeper and thinner models generally outperform their wider and shallower counterparts across various tasks such as zero-shot common sense reasoning, question answering, and reading comprehension.\n' +
      '\n' +
      'Figure 5: Ablation study on number of heads and kv-heads. Here, ratio denotes the number of heads divided by number of kv-heads. Averaged accuracy on zero-shot reasoning tasks is reported.\n' +
      '\n' +
      'and only 0.2 points accuracy drop in the 350M model with almost 10% model size reduction. These results serve as a guideline in our model architecture design. By adopting the grouped query attention (GQA) and meanwhile increasing the embedding dimension to maintain the model size, the accuracy of 125M further increases 0.4 points, indicating GQA as a favorable method to further squeeze out small model\'s potential.\n' +
      '\n' +
      'In summary, we tested out four state-of-the-art techniques beneficial to small model designs, including FFN with SwiGLU, deep and thin architecture, embedding sharing, and grouped query attention. Combining these techniques, we build a strong baseline small LLM and we name it MobileLLM.\n' +
      '\n' +
      '### Layer Sharing\n' +
      '\n' +
      'The findings in Section 2.2.2 on the impact of layer depth versus width suggest deeper layers are favorable for small transformer models. This motivates us to investigate layer sharing as a strategy to increase the number of hidden layers without additional model storage cost. This approach is particularly helpful in on-device scenarios where model size is a major constraint.\n' +
      '\n' +
      'Surprisingly, the experimental findings show that accuracy enhancement can be achieved by simply replicating transformer blocks, without necessitating architectural modifications or an enlargement of the model size. We further examined three different weight sharing strategies, illustrated in Figure 6. Results in Table 2 indicate that the repeat-over layer-sharing strategy produces the best performance among immediate block-wise repeat, repeat all-over, and reverse sharing strategies. However, considering the hardware memory hierarchy (Figure 2), the SRAM for computing is typically limited to around 20MB. This capacity is usually only sufficient to hold a single transformer block. Therefore, placing shared weights in the cache and computing twice immediately can avoid the need to transfer weights between the SRAM and DRAM, resulting in improved overall execution speed for auto-regressive inference. Consequently, we have opted for the immediate block-wise sharing strategy in our model design. We denote the proposed model with layer sharing as MobileLLM-LS.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '### Experimental Settings\n' +
      '\n' +
      'We train MobileLLM from scratch using Adam optimizer (Kingma and Ba, 2014) with weight decay of 0.1. The experiments are conducted using 32 A100 GPUs, with a batch size of 32 on each GPU. The initial learning rate is set to 2e-3 and follows a cosine learning-rate decay strategy. We perform quick exploration experiments with 120k iterations on 0.25T tokens and train the best models reported in Tables 3 and 4 with 480k iterations on 1T tokens.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      'We compare the final performance on zero-shot common sense reasoning tasks, question answering and reading comprehension tasks. The results of baseline methods were evaluated using their open-source Hugging Face models to ensure consistent evaluation procedures.\n' +
      '\n' +
      '**Zero-shot Common Sense Reasoning** Table 3 presents a comparison between our proposed model, MobileLLM, and state-of-the-art sub-billion parameter models, including the early open-sourced LLMs, OPT (Zhang et al., 2022), BLOOM (Scao et al., 2022), and recent releases such as Galactic (Taylor et al., 2022), Cerebras (Dey et al., 2023), GPT-neo (Black et al., 2022) as well as the LLM analyzing suite Pythia (Biderman et al., 2023) and transformer variants RWKV (Peng et al., 2023) on zero-shot common sense reasoning tasks. For the 125M model size, MobileLLM favorably outperforms previous models such as OPT, GPT-Neo, and Calactica at the same model size by a significant margin. Additionally, MobileLLM-125M achieves 3.8 points and 2.7 points higher accuracy than Pythia-160M and RWKV-169M while being 22% and 26% smaller, respectively. Furthermore, incorporating layer-sharing in MobileLLM-LS-125M results in an additional 0.7 points improvement in accuracy. It is noteworthy that MobileLLM-LS-125M achieves comparable or even higher results than most previous 350M models. In the 350M model size category, MobileLLM surpasses previous state-of-the-art models by\n' +
      '\n' +
      'Figure 6: (a) Baseline model without layer sharing; (b) Immediate block-wise sharing; (c) Repeat-all-over sharing; (d) Reverse sharing. A transformer block contains the multi-head self-attention (MHSA) and the feed-forward network (FFN). While repeat-all-over sharing has slightly higher performance, immediate block-wise sharing best utilize the cache because the shared weights can stay in the cache and be immediately computed twice.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      'tions of chat examples in the appendix also underscore the impressive quality of responses generated by MobileLLM models.\n' +
      '\n' +
      '#### 3.3.2 API Calling\n' +
      '\n' +
      'API calling is a common on-device application, particularly in collaboration with audio-to-text models for assistant functionalities. Leveraging LLMs for API calling involves the conversion of natural language inputs into JSON configurations to invoke corresponding APIs6. For instance, given the input _"Help me set an alarm at 7:30 AM"_ the model outputs _[API: "alarm(time="7:30 am")"]_. Additionally, the model generates an agent response: _"Sure: Your alarm is set to 7:30 AM."_\n' +
      '\n' +
      'Footnote 6: [https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)\n' +
      '\n' +
      'To adapt LLMs for this task, we create a synthetic dataset with 5000 training samples and 2500 testing samples. Each sample involves 8 conversation turns on average. Detailed examples of this dataset are provided in appendix. The pre-trained models undergoes fine-tuning on training set for 4 epochs, utilizing the Adam optimizer with a linear-decay learning rate starting at 2e-5 and a weight decay of 0.01.\n' +
      '\n' +
      'Table 6 shows that MobileLLM-350M demonstrates comparable intent and structure exact match scores to LLaMA-v2 7B, where high intent scores indicate correct prediction of the API user intends to call, while structural exact match scores reflects the proficiency in predicting content within API functions. Despite lower Rouge scores in MobileLLM-350M compared to 7B models, it is crucial to note that API calling prioritize correct API invocation. The results suggest that certain common scenarios in on-device applications are not particularly challenging, and smaller models like MobileLLM-350M can adeptly handle it.\n' +
      '\n' +
      '### Compatibility with Quantization\n' +
      '\n' +
      'We further conduct per-token min-max post-training quantization (PTQ) experiments on both MobileLLM and MobileLLM-LS models with 125M and 350M model size trained on 0.25T tokens. Figure 7 shows that employing W8A8 PTQ yields a modest accuracy reduction of less than 0.5 points and remains compatible with layer-sharing.\n' +
      '\n' +
      '### Knowledge Distillation\n' +
      '\n' +
      'So far, we trained compact models from scratch using next tokens as hard labels. We explored Knowledge Distillation (KD) of 125M and 350M models with LLAMA-v2 7B as a teacher. Unfortunately KD increases training time (slowdown of \\(2.6-3.2\\times\\)) and exhibits comparable or inferior accuracy to label-based training (details in appendix).\n' +
      '\n' +
      '### On-device Profiling\n' +
      '\n' +
      'We measure the latency for MobileLLM-125M and MobileLLM-LS-125M FP16 models via ExecuTorch7 on iPhone 13 (iOS 17.2.1), with Metal Performance Shaders\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Model** & \\multicolumn{2}{c}{EM\\({}_{\\text{intent}}\\)} & \\multicolumn{2}{c}{EM\\({}_{\\text{structure}}\\)} & \\multicolumn{1}{c}{R1} & \\multicolumn{1}{c}{RL\\({}_{\\text{max}}\\)} \\\\ \\hline OPT-350M & 56.1 & 38.6 & 37.1 & 35.3 \\\\ Pythia-410M & 62.2 & 44.7 & 43.1 & 41.1 \\\\ BLOOM-560M & **64.7** & 37.9 & 36.9 & 34.6 \\\\ MobileLLM-350M & **65.3** & **48.8** & **46.8** & **44.6** \\\\ \\hline LLaMA-v2 7B & 62.8 & **50.9** & **56.5** & **54.3** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: API calling evaluation score. EM\\({}_{\\text{intent}}\\)/EM\\({}_{\\text{structure}}\\) measures the exact match in API calling. R1/RL refers to Rouge-1/L score measuring the quality of agent response.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Model** & \\multicolumn{2}{c}{**MT-Bench\\({}_{\\text{(score)}}\\)**} & \\multicolumn{1}{c}{**Alpaca Eval\\({}_{\\text{(win %)}}\\)**} \\\\ \\hline \\multicolumn{4}{c}{_number of parameters \\textless{} 200M_} \\\\ \\hline OPT-125M & 1.21 & 3.91 \\\\ GPT-Neo-125M & 1.06 & 1.01 \\\\ Pythia-160M & 1.01 & 0.63 \\\\ MobileLLM-125M & **2.33** & **24.07** \\\\ MobileLLM-LS-125M & **2.52** & **23.79** \\\\ \\hline \\multicolumn{4}{c}{_200M \\textless{} number of parameters \\textless{} 1B_} \\\\ \\hline OPT-350M & 1.37 & 6.80 \\\\ Pythia-410M & 1.62 & 13.87 \\\\ BLOOM-560M & 1.73 & 10.29 \\\\ MobileLLM-350M & **3.28** & **47.08** \\\\ MobileLLM-LS-350M & **3.16** & **48.20** \\\\ \\hline \\multicolumn{4}{c}{_number of parameters \\textgreater{} 1B_} \\\\ \\hline Pythia-1B & 1.70 & 16.62 \\\\ BLOOM-1.1B & 2.37 & 19.90 \\\\ Falcon-1.3B & 2.54 & 30.38 \\\\ OPT-1.3B & 2.24 & 38.84 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Benchmark results on AlpacaEvaluator: GPT-4; Reference model: text-davinci-001) and MT-Bench.\n' +
      '\n' +
      'Figure 7: Comparison between BFloat16 model and 8-bit weight 8-bit activation post-training quantized model.\n' +
      '\n' +
      '(MPS) backend8. Model loading, initialization and execution time are reported in Table 7. Specifically, execution time is averaged over 50 iterations.\n' +
      '\n' +
      'Footnote 8: [https://pytorch.org/executorch/stable/build-run-mps.html](https://pytorch.org/executorch/stable/build-run-mps.html)\n' +
      '\n' +
      'Results in Table 7 reflects that through weight sharing and doubling the number of layers, MobileLLM-LS incurs only a 2.2% increase in loading and initialization time compared to MobileLLM, attributable to their similar model sizes. Execution time also experiences a mere 2.6% overhead, benefitting from data locality. In contrast, a model with doubled number of layers without weight sharing exhibits a substantial 143% rise in loading and initialization time and an 86% increase in execution time.\n' +
      '\n' +
      '## 4 Related Work\n' +
      '\n' +
      'The excellent performance of LLMs has fostered its wide applications. Considering computational cost and energy consumption of LLMs, an new stream of research direction have emerged to downsize LLMs to enable on-device inference. These methods include:\n' +
      '\n' +
      '**Model Compression.** Numerous model compression methods are developed for LLMs, including pruning(Xia et al., 2023b), sparsity (Sun et al., 2023; Xia et al., 2023a; Frantar and Alistarh, 2023), and quantization (Liu et al., 2023a; Dettmers et al., 2022; Kim et al., 2023; Frantar et al., 2022; Xiao et al., 2023; Yao et al., 2022; Liu et al., 2023c;b; Frantar et al., 2022). Our research is complementary to these techniques. As also substantiated in Section 3.4, our methodology is compatible with quantization.\n' +
      '\n' +
      '**Small Model Design.** A limited number of studies have explored compact model architectures, such as TinyLLaMA (Timiryasov and Tastet, 2023). However, even the smallest TinyLLaMA exceeds 1 billion parameters, making them still prohibitive for many on-device applications. Some research proposes large model architectures alongside their smaller LLM variants in a model family (Zhang et al., 2022; Scao et al., 2022; Black et al., 2022; Dey et al., 2023) or a analytical suite containing small LLM variants (Biderman et al., 2023). However, these models are not optimized under the constraint of sub-billion parameters and therefore may not be optimal.\n' +
      '\n' +
      '**Neural Architecture Search.** NAS has garnered substantial attention in the realm of convolutional neural networks, particularly in the context of vision tasks (Tan and Le, 2019; Zoph and Le, 2016; Wu et al., 2019; Guo et al., 2020). In contrast, within the transformer domain, the prevailing consensus posits that the model architecture exerts minimal influence on accuracy, provided the total number of parameters remains consistent (Kaplan et al., 2020). Only a limited number of studies have developed NAS algorithm for language transformer, targeting at BERT models (Xu et al., 2021; Jawahar et al., 2023; Ganesan et al., 2021). Our current investigation, focusing on the interplay between depth and width, can be conceptualized as a meticulous grid search within the depth space. The outcomes of that study challenge the prevalent orthodoxy surrounding scaling laws, proposing that deep and thin architectures demonstrate higher performance for compact LLMs.\n' +
      '\n' +
      '**Weight Sharing.** Weight sharing is an intuitive strategy for optimizing model weight utilization within fixed parameter constraints. While the OPT family (Zhang et al., 2022) and subsequent works (Black et al., 2022) leverage weight sharing between input and output embeddings, limited research has explored weight sharing for intermediate layers in transformers (Shen et al., 2022; Reid et al., 2021). Prior efforts often entail specialized designs for shared layers. In contrast, our contribution highlights a more straightforward yet effective way of simply repeating transformer blocks, yielding improved accuracy with a fixed model size and minimal latency increase.\n' +
      '\n' +
      '**Efficient Attention and Implementation.** In the realm of efficient transformer design, much research has focused on optimizing attention computation through methods like low-rank approximation (Wang et al., 2020; Katharopoulos et al., 2020; Xiong et al., 2021) and sparse attention (Kitaev et al., 2020; Roy et al., 2021). Another line of work explores hardware scheduling and weight movement, exemplified by works such as FlashAttention (Dao et al., 2022) and FlexGen (Sheng et al., 2023). In contrast, our primary goal is to optimize model size without introducing new attention computation or efficient hardware implementation methods.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'This study focuses on optimizing sub-billion scale models for on-device applications. Our findings indicate that, for smaller models, prioritizing depth over width enhances model performance. Furthermore, by leveraging advanced weight-sharing techniques, including embedding sharing, grouped query attention, and block-wise weight sharing, we achieve significant enhancements in weight utilization within storage-constrained scenarios. The resulting models\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & Load & Init & Execute \\\\ \\hline MobileLLM & 39.2 ms & 1361.7 ms & 15.6 ms \\\\ MobileLLM-LS & 43.6 ms & 1388.2 ms & 16.0 ms \\\\\n' +
      '60-layer non-shared & 68.6 ms & 3347.7 ms & 29.0 ms \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Latency analysis of MobileLLM-125M (30 layers), MobileLLM-LS-125M (2\\(\\times\\)30 layers, adjacent blocks sharing weights), and a 60-layer non-shared weight model, with consistent configurations in all other aspects.\n' +
      '\n' +
      'denoted as MobileLLM exhibit substantial advancements in zero-shot commonsense reasoning, question answering, and reading comprehension tasks compared to previous SoTA methods. Last but not least, we demonstrate the effectiveness of the fine-tuned MobileLLM models in two prevalent on-device use cases: chat and API calling, underscoring their adeptness in handling such tasks.\n' +
      '\n' +
      '## Acknowledgment\n' +
      '\n' +
      'We thank Hansong Zhang for his valuable contribution in setting up latency measurement environment on iOS, and the full support from PyTorch Edge team.\n' +
      '\n' +
      '## Broader Impact\n' +
      '\n' +
      'This paper advocates for the adoption of sub-billion scale large language models in on-device applications, aiming to mitigate energy consumption during LLM inference. The proposed approach is promising in alleviating computational costs associated with LLM deployment.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Ainslie et al. (2023) Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In EMNLP, Cited by: SS1.\n' +
      '* S. Biderman, H., M. G., H. Bradley, K. O\'Brien, E., M. A., S., Prabanth, U. S., E., et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397-2430. Cited by: SS1.\n' +
      '* Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. P. P. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34, pp. 7432-7439. Cited by: SS1.\n' +
      '* S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J., et al. Gpt-neox-20b: an open-source autoregressive language model. arXiv preprint arXiv:2204.06745. Cited by: SS1.\n' +
      '* A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research24 (240), pp. 1-113. Cited by: SS1.\n' +
      '* C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova (2019)EOLO: exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044. Cited by: SS1.\n' +
      '* P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord (2018)Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Cited by: SS1.\n' +
      '* T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Re (2022)Flashattention: fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems35, pp. 16344-16359. Cited by: SS1.\n' +
      '* Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier (2017)Language modeling with gated convolutional networks. In International conference on machine learning, pp. 933-941. Cited by: SS1.\n' +
      '* T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer (2022)LLM. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339. Cited by: SS1.\n' +
      '* N. Dey, G. Gosal, H. Khachane, W. Marshall, R. Pathria, M. Tom, J. Hestness, et al. Cerebras-gpt: open compute-optimal language models trained on the cerebras wafer-scale cluster. arXiv preprint arXiv:2304.03208. Cited by: SS1.\n' +
      '* E. Frantar and D. Alistarh (2023)SparseGPT: massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pp. 10323-10337. Cited by: SS1.\n' +
      '* E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh (2022)Gptq: accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323. Cited by: SS1.\n' +
      '* V. Ganesan, G. Ramesh, W. Heng, Z. Liu, Y. Wei, and J. Sun (2020)Single path one-shot neural architecture search with uniform sampling. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVI 16, pp. 544-560. Cited by: SS1.\n' +
      '* S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J. Dally (2016)EIE: efficient inference engine on compressed deep neural network. ACM SIGARCH Computer Architecture News44 (3), pp. 243-254. Cited by: SS1.\n' +
      '* G. Hinton, O. Vinyals, J. Dean, et al. (2015)Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531. Cited by: SS1.\n' +
      '* G.\n' +
      '\n' +
      '* [14] Hristov, V. A16 bionic explained: what\'s new in apple\'s pro-grade mobile chip? [https://www.phonearena.com/news/A16-Bionic-explained-whats-new_id142438](https://www.phonearena.com/news/A16-Bionic-explained-whats-new_id142438), 2022.\n' +
      '* [15] Jawahar, G., Yang, H., Xiong, Y., Liu, Z., Wang, D., Sun, F., Li, M., Pappu, A., Oguz, B., Abdul-Mageed, M., et al. Mixture-of-supernets: Improving weight-sharing supernet training with architecture-routed mixture-of-experts. _arXiv preprint arXiv:2306.04845_, 2023.\n' +
      '* [16] Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. _arXiv preprint arXiv:1705.03551_, 2017.\n' +
      '* [17] Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* [18] Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In _International conference on machine learning_, pp. 5156-5165. PMLR, 2020.\n' +
      '* [19] Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M. W., and Keutzer, K. Squeezellm: Dense-and-sparse quantization. _arXiv preprint arXiv:2306.07629_, 2023.\n' +
      '* [20] Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.\n' +
      '* [21] Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020.\n' +
      '* [22] Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. Race: Large-scale reading comprehension dataset from examinations. _arXiv preprint arXiv:1704.04683_, 2017.\n' +
      '* [23] Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval), 2023.\n' +
      '* [24] Liu, J., Gong, R., Wei, X., Dong, Z., Cai, J., and Zhuang, B. Qllm: Accurate and efficient low-bitwidth quantization for large language models. _arXiv preprint arXiv:2310.08041_, 2023a.\n' +
      '* [25] Liu, S.-y., Liu, Z., Huang, X., Dong, P., and Cheng, K.-T. Llm-fp4: 4-bit floating-point quantized transformers. _arXiv preprint arXiv:2310.16836_, 2023b.\n' +
      '* [26] Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. Llm-qat: Data-free quantization aware training for large language models. _arXiv preprint arXiv:2305.17888_, 2023c.\n' +
      '* [27] Malladi, K. T., Lee, B. C., Nothaft, F. A., Kozyrakis, C., Periyathambi, K., and Horowitz, M. Towards energy-proportional datacenter memory with mobile dram. _ACM SIGARCH Computer Architecture News_, 40(3):37-48, 2012.\n' +
      '* [28] Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. _arXiv preprint arXiv:1809.02789_, 2018.\n' +
      '* [29] Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., et al. Rwkv: Reinventing rnns for the transformer era. _arXiv preprint arXiv:2305.13048_, 2023.\n' +
      '* [30] Reid, M., Marrese-Taylor, E., and Matsuo, Y. Subformer: Exploring weight sharing for parameter efficiency in generative transformers. _arXiv preprint arXiv:2101.00234_, 2021.\n' +
      '* [31] Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient content-based sparse attention with routing transformers. _Transactions of the Association for Computational Linguistics_, 9:53-68, 2021.\n' +
      '* [32] Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106, 2021.\n' +
      '* [33] Sap, M., Rashkin, H., Chen, D., LeBras, R., and Choi, Y. Socialiqa: Commonsense reasoning about social interactions. _arXiv preprint arXiv:1904.09728_, 2019.\n' +
      '* [34] Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A. S., Yvon, F., Galle, M., et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.\n' +
      '* [35] Shen, Z., Liu, Z., and Xing, E. Sliced recursive transformer. In _European Conference on Computer Vision_, pp. 727-744. Springer, 2022.\n' +
      '* [36] Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen, B., Liang, P., Re, C., Stoica, I., and Zhang, C. Flexgen: High-throughput generative inference of large language models with a single gpu. In _International Conference on Machine Learning_, pp. 31094-31116. PMLR, 2023.\n' +
      '* [37] Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and effective pruning approach for large language models. _arXiv preprint arXiv:2306.11695_, 2023.\n' +
      '\n' +
      '* Tan and Le (2019) Tan, M. and Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In _International conference on machine learning_, pp. 6105-6114. PMLR, 2019.\n' +
      '* Taylor et al. (2022) Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R. Galactica: A large language model for science. _arXiv preprint arXiv:2211.09085_, 2022.\n' +
      '* Timiryasov and Tastet (2023) Timiryasov, I. and Tastet, J.-L. Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty. _arXiv preprint arXiv:2308.02019_, 2023.\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al. (2021) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, Kardas, M., Kerkez, V., Khabas, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models, 2023b.\n' +
      '* Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.\n' +
      '* Wu et al. (2019) Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y., and Keutzer, K. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10734-10742, 2019.\n' +
      '* Xia et al. (2023a) Xia, H., Zheng, Z., Li, Y., Zhuang, D., Zhou, Z., Qiu, X., Li, Y., Lin, W., and Song, S. L. Flash-llm: Enabling cost-effective and highly-efficient large generative model inference with unstructured sparsity. _arXiv preprint arXiv:2309.10285_, 2023a.\n' +
      '* Xia et al. (2023b) Xia, M., Gao, T., Zeng, Z., and Chen, D. Sheared llama: Accelerating language model pre-training via structured pruning. _arXiv preprint arXiv:2310.06694_, 2023b.\n' +
      '* Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models. In _International Conference on Machine Learning_, pp. 38087-38099. PMLR, 2023.\n' +
      '* Xiong et al. (2021) Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V. Nystromformer: A nystrom-based algorithm for approximating self-attention. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pp. 14138-14148, 2021.\n' +
      '* Xu et al. (2021) Xu, J., Tan, X., Luo, R., Song, K., Li, J., Qin, T., and Liu, T.-Y. Nas-bert: task-agnostic and adaptive-size bert compression with neural architecture search. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pp. 1933-1943, 2021.\n' +
      '* Yao et al. (2022) Yao, Z., Yazdani Aminabadi, R., Zhang, M., Wu, X., Li, C., and He, Y. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. _Advances in Neural Information Processing Systems_, 35:27168-27183, 2022.\n' +
      '* Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence? _arXiv preprint arXiv:1905.07830_, 2019.\n' +
      '* Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.\n' +
      '* Zheng et al. (2023) Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _arXiv preprint arXiv:2306.05685_, 2023.\n' +
      '* Zoph and Le (2016) Zoph, B. and Le, Q. V. Neural architecture search with reinforcement learning. _arXiv preprint arXiv:1611.01578_, 2016.\n' +
      '\n' +
      '## Appendix A Impact of Each Design Choice\n' +
      '\n' +
      'This section presents comprehensive tabulated results for the _improving sub-billion scale LLM design_ experiments, at the model sizes of 125M and 350M. Looking at the results in Table 8, transitioning from the traditional Feedforward Network (\\(FC\\to ReLU\\to FC\\)) to SwiGLU yields a accuracy improvement of 1.3% for both model sizes. Further increasing the model depth enhances accuracy by 0.9%/1.1% for 125M/350M models, respectively. Then, introducing input and output embedding sharing achieves a parameter reduction of approximately 10%, while with only marginal accuracy drops of 0.2% for the 125M and 0.6% for 350M models. Additionally, following in findings in Section C, we incorporate grouped query attention with head dimension equals to 64, and head number to be near 4\\(\\times\\) to the kv head number, while increasing the embedding dimension to preserve model size. This modification further results in a performance boost of 0.4%/0.7% for 125M/350M models. Combining these techniques establishes a strong baseline network denoted as MobileLLM. Finally, the immediate block-wise weight-sharing technique contributes an additional accuracy gain of 1.1% for models trained on 0.25 trillion tokens, and resulting in the model MobileLLM-LS. Final models including MobileLLM and MobileLLM-LS are trained with 1 trillion tokens.\n' +
      '\n' +
      '## Appendix B Depth vs Width\n' +
      '\n' +
      'We provide network depth versus width exploration results on zero-shot reasoning tasks in Table 9, as well as results on question answering and reading comprehension tasks in Table 10. The findings indicate that shallow architectures with fewer than 10 layers perform poorly in reasoning or handling comprehension tasks. Models with 10-20 layers exhibit improved accuracy, while further increasing depth continues to provide significant benefits across all three tasks. Notably, the optimal depth is found to be near 30 layers for sub-billion scale models.\n' +
      '\n' +
      '## Appendix C Number of Heads and Key-Value Heads\n' +
      '\n' +
      'We provide detailed experimental results assessing the impact of the number of attention heads and key-value heads on zero-shot reasoning accuracy in Table 11. Our study involves two baseline architectures: a 8-layer 125M model with a embedding dimension of 896, and a 15-layer 350M model with a embedding dimension of 1280. We conduct head size sweeps in {8, 16, 32}. The findings shown in Table 11 indicate that using 16 heads, with a head dimension close to 64, and 4 key-value heads, yields the best accuracy and memory trade-off. This setting serves as a guiding principle in our model architecture design.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c c} \\hline \\hline\n' +
      '**Techniques** & **L** & **H** & **H\\({}_{\\text{KV}}\\)** & **Dim** & **\\#Params(M)** & **ARC-e** & **ARC-c** & **Bool** & **PIQA** & **SIQA** & **HellaSwag** & **OBQA** & **WinoGrande** & **Avg.** \\\\ \\hline\n' +
      '**125M** & & & & & & & & & & & & & & \\\\ \\hline Baseline model & 12 & 12 & 12 & 768 & 134.1 & 41.3 & 25.2 & 57.5 & 62.0 & 41.9 & 31.1 & 31.2 & 50.8 & 42.6 \\\\ + SwiGLU in FFN & 12 & 12 & 12 & 768 & 134.1 & 43.1 & 28.9 & 58.1 & 62.3 & 42.3 & 34.6 & 31.5 & 50.1 & 43.9 \\\\ + Use deep-thin structure & 30 & 8 & 8 & 512 & 135.0 & 43.6 & 26.1 & 58.0 & 62.5 & 42.6 & 36.5 & 37.5 & 51.5 & 44.8 \\\\ + Embedding share & 30 & 8 & 8 & 512 & 118.6 & 44.4 & 26.0 & 56.2 & 62.8 & 43.1 & 35.9 & 36.0 & 52.6 & 44.6 \\\\ + Multi-query attention & 30 & 9 & 3 & 576 & 124.6 & 45.5 & 27.7 & 58.3 & 64.6 & 41.9 & 36.4 & 35.4 & 50.4 & 45.0 \\\\ (Train on 1T token) & 30 & 9 & 3 & 576 & 124.6 & 43.9 & 27.1 & 60.2 & 65.3 & 42.4 & 38.9 & 39.5 & 53.1 & **46.3** \\\\ + Layer sharing & 30 & 9 & 3 & 576 & 124.6 & 44.4 & 27.0 & 61.5 & 65.1 & 43.0 & 37.6 & 37.8 & 52.0 & 46.1 \\\\ (Train on 1T token) & 30 & 9 & 3 & 576 & 124.6 & 45.8 & 28.7 & 60.4 & 65.7 & 42.9 & 39.5 & 41.1 & 52.1 & **47.0** \\\\ \\hline\n' +
      '**350M** & & & & & & & & & & & & & & \\\\ \\hline Baseline model & 15 & 20 & 20 & 1280 & 376.8 & 50.3 & 27.6 & 53.8 & 68.1 & 44.1 & 42.6 & 40.1 & 52.4 & 47.4 \\\\ + SwiGLU in FFN & 15 & 20 & 20 & 1280 & 386.7 & 49.2 & 30.6 & 59.1 & 67.7 & 44.3 & 43.2 & 41.0 & 54.2 & 48.7 \\\\ + Use deep-thin structure & 32 & 14 & 14 & 896 & 380.3 & 50.7 & 31.4 & 59.4 & 67.8 & 43.3 & 46.2 & 43.8 & 56.2 & 49.8 \\\\ + Embedding share & 32 & 14 & 14 & 896 & 351.6 & 49.9 & 32.0 & 60.3 & 67.9 & 43.2 & 47.0 & 38.9 & 54.8 & 49.2 \\\\ + Multi-query attention & 32 & 15 & 5 & 966 & 345.3 & 51.4 & 31.3 & 61.0 & 68.1 & 43.6 & 47.2 & 41.6 & 55.4 & 49.9 \\\\ (Train on 1T token) & 32 & 15 & 5 & 960 & 345.3 & 53.8 & 33.5 & 62.4 & 68.6 & 44.7 & 49.6 & 40.0 & 57.6 & **51.3** \\\\ + Layer sharing & 32 & 15 & 5 & 960 & 345.3 & 51.9 & 35.2 & 59.6 & 68.9 & 43.4 & 47.2 & 43.3 & 58.4 & 51.0 \\\\ (Train on 1T token) & 32 & 15 & 5 & 960 & 345.3 & 54.4 & 32.5 & 62.8 & 69.8 & 44.1 & 50.6 & 45.8 & 57.2 & **52.1** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Ablation study on the impact of each design choice to the model accuracy on zero-shot common sense reasoning tasks. Corresponding to the bar chart in Figure 3. Here, L, H, H\\({}_{\\text{KV}}\\) denotes number of layers, heads, kv-heads, respectively and dim denotes embedding dimension.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '## Appendix F Knowledge Distillation\n' +
      '\n' +
      'The results of integrating knowledge distillation (KD) (Hinton et al., 2015) into small model pre-training are presented in Table 14. LLaMA-v2 7B models serve as the teacher, and the KD loss is computed using cross-entropy between the logits from the large pre-trained teacher model (_i.e._, LLaMA-v2 7B) and the small student network (_i.e._, 125M or 350M models):\n' +
      '\n' +
      '\\[\\mathcal{L}_{CE}=-\\frac{1}{n}\\sum_{c}\\sum_{i=1}^{n}p_{c}^{\\mathcal{T}}(X_{i}) \\log(p_{c}^{\\mathcal{S}}(X_{i})), \\tag{1}\\]\n' +
      '\n' +
      'Here, \\(i\\) denotes the \\(i^{th}\\) sample in the current batch with \\(n\\) total samples in the batch, and \\(c\\) represents the number of classes, which, in our case, equals the size of the vocabulary. \\(\\mathcal{T}\\) and \\(\\mathcal{S}\\) are the teacher and student networks, respectively.\n' +
      '\n' +
      'The results in Table 14 indicate that adding KD loss is comparable or even lower than solely using the next token as labels. However, it\'s noteworthy that the training time using KD is \\(2.6-3.2\\times\\) slower than training from scratch using labels. All models are trained on 32 A100 80G GPUs with a batch size of 32 for 120k iterations. Consequently, we opt to use labels in our experiments.\n' +
      '\n' +
      '## Appendix G Datasets and Benchmarks\n' +
      '\n' +
      'MobileLLM is assessed on zero-shot common sense reasoning (BoolQ, PIQA, SIQA, HellaSwag, Winogrande, ARC, OBQA), question answering (TriviaQA), and reading comprehension (RACE) tasks. Additionally, we evaluate our chat models on MT-Bench and AlpacaEval benchmarks. We also generated a API calling dataset to fine-tune and evaluate models\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **\\#Heads \\#KV-Heads** & **\\#Params(M)** & **ARC-e** & **ARC-c** & **BoolQ** & **PIQA** & **SIQA** & **HellaSwag** & **OBQA** & **WinoGrande** & **Avg.** \\\\ \\hline \\multirow{9}{*}{125M} & 32 & 32 & 138.1 & 42.1 & 26.9 & 58.4 & 62.2 & 42.1 & 33.8 & 36.7 & 52.4 & 44.3 \\\\  & 32 & 16 & 131.7 & 41.2 & 26.4 & 57.7 & 62.5 & 42.3 & 33.3 & 34.2 & 52.9 & 43.8 \\\\  & 32 & 8 & 128.5 & 42.6 & 27.3 & 61.1 & 61.9 & 41.9 & 32.2 & 35.0 & 52.0 & 44.2 \\\\  & 32 & 4 & 126.8 & 43.1 & 26.8 & 59.8 & 62.7 & 41.4 & 32.5 & 34.4 & 51.1 & 44.0 \\\\  & 32 & 2 & 126.0 & 39.8 & 26.7 & 59.4 & 59.4 & 42.0 & 31.3 & 32.6 & 52.9 & 43.0 \\\\  & 32 & 1 & 125.6 & 41.0 & 24.3 & 59.1 & 60.8 & 41.2 & 31.4 & 35.4 & 52.0 & 43.1 \\\\\n' +
      '**125M** & 16 & 16 & 138.1 & 41.6 & 25.7 & 61.1 & 62.4 & 43.1 & 34.4 & 36.9 & 51.6 & **44.6** \\\\ \\# layers=8 & 16 & 8 & 131.7 & 42.4 & 26.4 & 60.7 & 63.4 & 41.9 & 33.5 & 34.7 & 51.5 & 44.3 \\\\ dim=896 & 16 & 4 & 128.5 & 42.5 & 25.6 & 62.3 & 62.4 & 41.8 & 33.0 & 35.9 & 54.5 & **44.7** \\\\  & 16 & 1 & 126.8 & 41.7 & 25.3 & 56.9 & 61.7 & 42.0 & 32.9 & 32.6 & 54.5 & 43.5 \\\\  & 16 & 1 & 126.0 & 40.4 & 26.3 & 61.8 & 63.2 & 41.7 & 32.0 & 34.0 & 50.4 & 43.7 \\\\  & 8 & 8 & 138.1 & 41.4 & 25.0 & 58.3 & 61.7 & 41.7 & 33.3 & 35.9 & 53.2 & 43.8 \\\\  & 8 & 4 & 131.7 & 43.3 & 28.2 & 58.3 & 61.8 & 42.8 & 33.8 & 30.9 & 53.0 & 44.0 \\\\  & 8 & 2 & 128.5 & 40.7 & 26.2 & 58.1 & 62.4 & 41.6 & 32.8 & 34.8 & 51.5 & 43.4 \\\\  & 8 & 1 & 126.8 & 42.5 & 24.8 & 59.4 & 62.3 & 42.0 & 32.0 & 36.3 & 51.3 & 43.8 \\\\ \\hline \\multirow{9}{*}{350M} & 32 & 32 & 386.7 & 48.6 & 30.4 & 59.7 & 67.2 & 43.9 & 44.0 & 40.9 & 53.9 & 48.6 \\\\  & 32 & 16 & 362.1 & 48.9 & 31.6 & 57.6 & 68.4 & 43.4 & 43.8 & 38.6 & 54.9 & 48.4 \\\\  & 32 & 8 & 349.8 & 48.3 & 33.1 & 61.0 & 67.2 & 42.6 & 42.1 & 39.0 & 53.9 & 48.4 \\\\  & 32 & 4 & 343.7 & 47.2 & 29.8 & 59.4 & 67.2 & 43.5 & 42.5 & 42.5 & 54.1 & 48.3 \\\\  & 32 & 2 & 340.6 & 47.6 & 30.3 & 62.4 & 69.9 & 42.6 & 41.6 & 38.6 & 52.0 & 47.7 \\\\  & 32 & 1 & 339.0 & 48.5 & 27.3 & 56.3 & 67.1 & 42.9 & 40.9 & 36.7 & 53.3 & 46.6 \\\\\n' +
      '**350M** & 16 & 16 & 386.7 & 50.8 & 30.6 & 62.3 & 68.6 & 43.5 & 45.1 & 43.8 & 52.4 & **49.6** \\\\ \\# layers=15 & 16 & 8 & 362.1 & 48.5 & 30.7 & 59.4 & 67.3 & 43.8 & 43.8 & 41.3 & 53.3 & 48.5 \\\\ dim=1280 & 16 & 4 & 349.8 & 49.9 & 30.6 & 60.0 & 69.2 & 43.5 & 44.2 & 41.8 & 55.8 & **49.4** \\\\  & 16 & 2 & 343.7 & 49.3 & 28.4 & 55.0 & 67.3 & 42.7 & 42.6 & 40.3 & 54.5 & 47.5 \\\\  & 16 & 1 & 340.6 & 49.2 & 29.3 & 58.8 & 67.4 & 43.5 & 42.1 & 39.9 & 52.8 & 47.9 \\\\  & 8 &for this particular task.\n' +
      '\n' +
      '### Zero-shot Common Sense Reasoning tasks\n' +
      '\n' +
      '**BoolQ**Clark et al. (2019) is a reading comprehension dataset focused on naturally occurring yes/no questions. Each instance includes a question (Q), an excerpt from a passage (P), and an answer (A), with an added explanation for enhanced clarity.\n' +
      '\n' +
      '**PIQA**Bisk et al. (2020), abbreviated for Physical Interaction: Question Answering, serves as a benchmark for evaluating and studying the capacity of natural language models in comprehending physical commonsense understanding.\n' +
      '\n' +
      '**SIQA**Sap et al. (2019), abbreviated for Social Interaction Question Answering, is designed to measure the social and emotional intelligence of computational models through multiple-choice question answering.\n' +
      '\n' +
      '**HellaSwag**Zellers et al. (2019) serves as a benchmark for physically situated commonsense natural language inference. It comprises four-way multiple-choice problems that are considered trivial for humans (>95% accuracy) but pose a challenge for language models.\n' +
      '\n' +
      '**WinoGrande**Sakaguchi et al. (2021) is a benchmark for commonsense reasoning. It consists of a set of 273 expert-crafted pronoun resolution problems, deliberately designed to be unsolvable for statistical models relying on selectional preferences or word associations.\n' +
      '\n' +
      '**ARC**Clark et al. (2018), the AI2 Reasoning Challenge, is a compilation of 7787 natural science questions. It is divided into a Challenge Set and an Easy Set, with the Challenge Set exclusively comprising questions that were answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm.\n' +
      '\n' +
      '**OBQA**Mihaylov et al. (2018) is a dataset consisting of approximately 6000 questions designed for open book question answering. The task involves integrating a corpus of provided science facts (open book) with external broad common knowledge. Providing correct answers necessitates leveraging simple common knowledge beyond the core facts provided.\n' +
      '\n' +
      '### Question Answering Tasks\n' +
      '\n' +
      '**TriviaQA**Joshi et al. (2017) is a closed-book question answering benchmark. It encompasses over 650,000 question-answer evidence triples derived by combining 95,000 question-answer pairs authored by Trivia enthusiasts. Each question is supported by an average of six evidence documents.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l l l l l} \\hline \\hline\n' +
      '**Model** & \\multicolumn{2}{c}{**Precision**} & \\multicolumn{1}{c}{**ARC-e**} & \\multicolumn{1}{c}{**ARC-c**} & \\multicolumn{1}{c}{**BoolQ**} & \\multicolumn{1}{c}{**PIQA**} & \\multicolumn{1}{c}{**SIQA**} & \\multicolumn{1}{c}{**HellaSwag**} & \\multicolumn{1}{c}{**OBQA**} & \\multicolumn{1}{c}{**WinoGrande**} & \\multicolumn{1}{c}{**Avg.**} & \\multicolumn{1}{c}{**Gap**} \\\\ \\hline MobileLLM-125M & BF16 & 45.5 & 27.7 & 58.3 & 64.6 & 41.9 & 36.4 & 35.4 & 50.4 & 45.0 & 4 & 5.0 \\\\ MobileLLM-125M & W8A8 & 45.2 & 27.1 & 58.3 & 65.0 & 41.7 & 36.2 & 33.6 & 51.0 & 44.8 & 0.2 \\\\ \\hline MobileLLM-LS-125M & BF16 & 44.4 & 27.0 & 61.5 & 65.1 & 43.0 & 37.6 & 37.8 & 52.0 & 46.1 & – \\\\ MobileLLM-LS-125M & W8A8 & 44.0 & 27.5 & 60.9 & 64.6 & 43.1 & 37.7 & 37.7 & 51.0 & 45.8 & 0.3 \\\\ \\hline MobileLLM-350M & BF16 & 51.4 & 31.3 & 61.0 & 68.1 & 43.6 & 47.2 & 41.6 & 55.4 & 49.9 & – \\\\ MobileLLM-350M & W8A8 & 51.4 & 32.1 & 61.1 & 68.8 & 43.1 & 47.1 & 40.6 & 55.1 & 49.9 & 0.0 \\\\ \\hline MobileLLM-LS-350M & BF16 & 51.9 & 35.2 & 59.6 & 68.9 & 43.4 & 47.2 & 43.3 & 58.4 & 51.0 & – \\\\ MobileLLM-LS-350M & W8A8 & 51.3 & 33.8 & 59.5 & 69.1 & 43.7 & 47.2 & 43.0 & 57.0 & 50.6 & 0.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 13: Ablation study: 8-bit weight, 8-bit activation post-training quantization results on zero-shot common sense reasoning tasks. Quantized models achieve accuracy gap of within 0.5% compared to the full-precision BF16 counterpart.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l l l l l} \\hline \\hline\n' +
      '**Model** & \\multicolumn{2}{c}{**Training Loss**} & \\multicolumn{1}{c}{**Training Time**} & \\multicolumn{1}{c}{**ARC-e**} & \\multicolumn{1}{c}{**ARC-c**} & \\multicolumn{1}{c}{**BoolQ**} & \\multicolumn{1}{c}{**PIQA**} & \\multicolumn{1}{c}{**SIQA**} & \\multicolumn{1}{c}{**HellaSwag**} & \\multicolumn{1}{c}{**OBQA**} & \\multicolumn{1}{c}{**WinoGrande**} & \\multicolumn{1}{c}{**Avg.**} \\\\ \\hline\n' +
      '125M model & Label & 29h & 43.1 & 28.9 & 58.1 & 62.3 & 42.3 & 34.6 & 31.5 & 50.1 & 43.9 \\\\\n' +
      '125M model & Label + KD & 93h & 41.8 & 28.5 & 58.5 & 61.6 & 41.1 & 34.5 & 32.7 & 51.6 & 43.8 \\\\ \\hline\n' +
      '350M model & Label & 42h & 50.2 & 31.8 & 56.9 & 67.7 & 44.3 & 45.8 & 40.8 & 55.5 & 49.1 \\\\\n' +
      '350M model & Label + KD & 109h & 48.7 & 31.8 & 60.7 & 67.4 & 43.2 & 45.9 & 38.9 & 53.7 & 48.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 14: Ablation study on employing LLaMA-v2 7B teacher’s output as soft labels for knowledge distillation (KD). Results indicate a slight degradation in performance when incorporating KD loss compared to only using hard labels.\n' +
      '\n' +
      '### Reading Comprehension Tasks\n' +
      '\n' +
      '**RACE**[17] is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students.\n' +
      '\n' +
      '### Chat Benchmarks\n' +
      '\n' +
      '**AlpacaEval**[17] is a single-turn benchmark featuring 805 questions across various topics. In the evaluation of this paper, models generate responses and are scored by GPT-4, with the ultimate metric being the pairwise win-rate against a baseline model (text-davinci-001).\n' +
      '\n' +
      '**MT-Bench**[21] is a multi-turn benchmark comprising 160 questions spanning eight diverse knowledge domains. In this evaluation, models are tasked with answering an initial question and subsequently providing a second response to a predefined follow-up question. Each model response is rated by GPT-4 on a scale from 1 to 10, and the final score is determined by the mean over the two turns.\n' +
      '\n' +
      '### API Calling Dataset\n' +
      '\n' +
      'The API calling dataset is generated by instructing a language model to simulate a conversation between a human and an agent. This process instructs the language model to explicitly mention tasks that should be performed using designated APIs throughout the simulated conversation. Examples of a conversation turn in generated dataset are shown in the following boxes.\n' +
      '\n' +
      '[size=title=!, topsep=0pt] {"role": "USER", "message": "Can you help me send a SMS message: We are looking forward to seeing you"}, {"role": "PARSER", "message": "style_transfer(text = "We are looking forward to seeing you", style = ["sms"], contact = None"}, {"role": "AGENT", "message": "Sure! Here\'s a possible SMS message: Hey [name], we are looking forward to seeing you! Wishing you all well."},\n' +
      '\n' +
      '[size=title=!, topsep=0pt] {"role": "USER", "message": "How did the S&P 500 perform last month?"}, {"role": "PARSER", "message": "get_stock_information(company_name = "S&P 500", ticker_symbol = "GSPC", date_time = "last month", financial_metrics = "price")"}, {"role": "AGENT", "message": "The S&P 500 closed at $2,977.94 last month, with a monthly return of 2.5%."},\n' +
      '\n' +
      '[size=title=!, topsep=0pt] {"role": "USER", "message": "Is the San Diego Zoo open during the winter months?"}, {"role": "PARSER", "message": "get_local_information(point_of_interest = "San Diego Zoo", attributes = "hours of operation")"}, {"role": "AGENT", "message": "The San Diego Zoo is open year-round, but the hours of operation vary depending on the time of year. During the winter months, the zoo is open from 9:00 AM to 5:00 PM."},{"role": "USER", "message": "What is the most recent news concerning aliens?"},\n' +
      '\n' +
      '{"role": "PARSER", "message": "news(query = "latest alien news")"},\n' +
      '\n' +
      '{"role": "AGENT", "message": "According to recent news articles, NASA has announced the discovery of a new planet outside of our solar system that has the potential to support alien life. The planet, named Kepler-186f, is located in the constellation Cygnus and is the first known Earth-sized planet to be found in the habitable zone of a star. While this news does not necessarily prove the existence of aliens, it does provide hope for the possibility of finding life beyond Earth. Additionally, a recent study published in the journal Nature has suggested that the universe may be teeming with alien civilizations, with estimates suggesting that there could be tens of billions of intelligent civilizations in the observable universe. However, it\'s important to note that these findings are still being researched and debated in the scientific community."},\n' +
      '\n' +
      '{"role": "USER", "message": "Who won the last Super Bowl?"},\n' +
      '\n' +
      '{"role": "PARSER", "message": "get_sports_information(match = "", sport_type = "NFL", metric = "winner")"},\n' +
      '\n' +
      '{"role": "AGENT", "message": "The last Super Bowl was won by the Kansas City Chiefs."},\n' +
      '\n' +
      '## Appendix H Approximate calculation of envisioned future GPU consumption\n' +
      '\n' +
      '#GPUs = \\(7.88\\times 10^{9}\\) (population size) \\(\\times 5\\%\\) (individual time9) \\(\\times\\)\\(220\\times 10^{9}\\) (approximating FLOPs/token for 220B GPT-4 model with one expert activated10) \\(\\times 50\\) (token/s) \\(\\times 24\\) (h) \\(\\times 3600\\) (s) / (\\(60\\times 10^{12}\\) (FLOPs/s, H100 computation capacity ) \\(\\times 24\\) (h) \\(\\times 3600\\) (s)) \\(\\approx 1\\times 10^{8}\\)\n' +
      '\n' +
      'Footnote 9: According to the statistics that YouTube users spending an average of 23 hours per month (source) and Instagram users dedicating 12 hours monthly to the app (source), we have reason to believe the interaction with LLM can easily surpass the cumulative engagement of these platforms in the future.\n' +
      '\n' +
      'This assumes short sequence lengths (_i.e._ hundreds of tokens, not tens of thousands of tokens), where the compute is dominated by linear layers. For very long sequences, the self-attention computation would dominate, and this would necessitate a more complicated formula.\n' +
      '\n' +
      '## Appendix I Chat Generations from MobileLLM\n' +
      '\n' +
      'We show a several examples of chat output generated with MobileLLM while responding to various questions.\n' +
      '\n' +
      '**Input:**\n' +
      '\n' +
      'Are there any particular physical benefits to mindful walking, such as improved posture or increased physical fitness?\n' +
      '\n' +
      '**Output** (MobileLLM-125M):\n' +
      '\n' +
      'Yes, there are several physical benefits to mindful walking. Here are a few:\n' +
      '\n' +
      '1. Improved Posture: Mindful walking can help improve your posture by promoting good alignment and reducing stress on the joints. This can lead to better posture and reduced back pain.\n' +
      '\n' +
      '2. Increased Flexibility: Mindful walking can also help increase flexibility in your body, which can reduce the risk of injury and improve overall health.\n' +
      '\n' +
      '3. Better Sleep: Mindful walking can help you fall asleep more easily and stay asleep longer, which can improve your sleep quality.\n' +
      '\n' +
      '4. Reduced Stress: Mindful walking can help reduce stress levels by promoting relaxation and reducing tension in the body.\n' +
      '\n' +
      '5. Enhanced Mental Health: Mindful walking has been shown to have positive effects on mental health, including reducing symptoms of depression and anxiety.\n' +
      '\n' +
      'Overall, mindful walking is an excellent way to improve your physical health and well-being. It\'s a simple and effective way to get moving and enjoy the benefits it offers. If you\'re interested in trying mindful walking, I would recommend starting with shorter walks and gradually increasing the duration over time. You may also want to consider incorporating mindfulness into your daily routine to further enhance your experience.\n' +
      '\n' +
      'In conclusion, mindful walking is a great way to improve your physical health and well-being. With its numerous physical benefits, it\'s easy to see why it\'s becoming increasingly popular among people looking for a fun and sustainable way to get moving. So go ahead and give it a try!\n' +
      '\n' +
      '**Output** (MobileLLM-350M):\n' +
      '\n' +
      'Yes, there are several physical benefits of mindful walking. Firstly, it can improve your posture by reducing the amount of tension in your muscles and joints. Secondly, it can increase your physical fitness by improving your cardiovascular health, strengthening your core muscles, and increasing your flexibility. Additionally, mindful walking can help you to stay more focused on the present moment, which can lead to better sleep quality and reduced stress levels. Finally, it can also help you to develop a greater sense of awareness and appreciation for the world around you, which can enhance your overall well-being.\n' +
      '\n' +
      'Overall, mindful walking is a great way to improve your physical health and well-being, while also promoting relaxation and mindfulness. Whether you\'re looking to reduce stress, improve your posture, or simply enjoy a peaceful walk, mindful walking can be a rewarding and fulfilling experience.\n' +
      '\n' +
      'If you\'re interested in trying out mindful walking, there are many resources available that can guide you through the process. Some popular options include guided meditations, apps that offer mindful walking exercises, and classes that teach you how to practice mindful walking. With a little bit of effort and dedication, anyone can start incorporating mindful walking into their daily routine and reap the many benefits it offers.\n' +
      '\n' +
      'In conclusion, mindful walking is a simple yet powerful tool that can help you to improve your physical health, reduce stress, and cultivate a deeper sense of awareness and appreciation for the world around you. So why not give it a try and see what kind of benefits you can achieve? Your body and mind will thank you!\n' +
      '\n' +
      '**Output** (llama-v2-7b-chat):\n' +
      '\n' +
      'Mindful walking can have a range of physical benefits, including:\n' +
      '\n' +
      '1. Improved posture: By focusing on the sensation of each step and the movement of the body, mindful walking can help improve posture and reduce slouching or hunching.\n' +
      '\n' +
      '2. Increased physical fitness: Mindful walking can help improve cardiovascular fitness, increase muscle strength and endurance, and boost overall physical fitness.\n' +
      '\n' +
      '3. Better balance and coordination: By paying attention to the body\'s position in space and the movement of the legs and feet, mindful walking can help improve balance and coordination.\n' +
      '\n' +
      '4. Reduced stress and tension: Mindful walking can help reduce stress and tension in the body, which can lead to improved physical relaxation and well-being.\n' +
      '\n' +
      '5. Improved flexibility: By focusing on the movement of the body and the sensation of each step, mindful walking can help improve flexibility and range of motion.\n' +
      '\n' +
      '6. Boosted immune system: Studies have shown that mindfulness practices, including mindful walking, can help boost the immune system and reduce inflammation.\n' +
      '\n' +
      '7. Improved sleep: Mindful walking can help improve sleep quality and duration by reducing stress and promoting relaxation. 8. Reduced pain: Mindful walking can help reduce pain by focusing on the sensation of each step and the movement of the body, rather than the pain itself.\n' +
      '\n' +
      '9. Improved mood: Mindful walking can help improve mood and reduce symptoms of depression and anxiety by promoting relaxation and reducing stress.\n' +
      '\n' +
      'It\'s important to note that the physical benefits of mindful walking may vary depending on the individual and the frequency and duration of the practice. However, with regular practice, mindful walking can have a range of positive effects on physical health and well-being.\n' +
      '\n' +
      '**Input:**\n' +
      '\n' +
      'How do I wrap a present neatly?\n' +
      '\n' +
      '**Output** (MobileLLM-125M):\n' +
      '\n' +
      '1. Choose the right size gift: The size of the present you want to give will depend on your budget and personal preference. If you are looking for a small gift, consider buying a small box or card with a single-use plastic bag.\n' +
      '\n' +
      '2. Prepare the present: Make sure all the items in the present are clean and tidy. You can use a cleaning kit or a tissue paper to remove any dirt or debris from the present.\n' +
      '\n' +
      '3. Arrange the presents: Place the present in a place where it is easy to see and touch. Use a pencil to draw a line around the present and arrange it in a way that makes sense to you.\n' +
      '\n' +
      '4. Add decorations: Decorate the present with stickers, ribbons, or other decorative items. This will add some personality to the present and make it more visually appealing.\n' +
      '\n' +
      '5. Enjoy the present: Take time to enjoy the present and let yourself feel the warmth and comfort of the present. Smile at yourself and take a moment to appreciate the present.\n' +
      '\n' +
      'Remember, the most important thing is to have fun and enjoy the present! Happy gift-giving!\n' +
      '\n' +
      'If you\'re feeling adventurous, try making a surprise gift for someone special by wrapping the present in a cute and colorful gift basket. Or, if you\'re feeling adventurous, try making a gift for a friend who needs a little extra help with their homework. Whatever you choose, remember to be creative and have fun with your present!\n' +
      '\n' +
      'With these tips, you\'ll be able to create a beautiful and thoughtful present that will last for years to come. Happy gift-giving!\n' +
      '\n' +
      '**Output** (MobileLLM-350M):\n' +
      '\n' +
      '1. Start by wrapping the present in a decorative gift wrap or ribbon.\n' +
      '\n' +
      '2. Use a small amount of glue or tape to secure the gift wrap or ribbon to the surface of the present.\n' +
      '\n' +
      '3. Place the gift wrap or ribbon around the present, making sure it is centered and evenly distributed.\n' +
      '\n' +
      '4. Use a small amount of glue or tape to secure the gift wrap or ribbon to the surface of the present.\n' +
      '\n' +
      '5. Once the gift wrap or ribbon is securely attached, you can add any additional decorations or embellishments to the present.\n' +
      '\n' +
      '6. If you want to add a personal touch, you can write a message or a special message on the gift wrap or ribbon.\n' +
      '\n' +
      '7. Finally, enjoy your beautifully wrapped present!\n' +
      '\n' +
      'Remember, wrapping a gift is a fun and creative way to show your loved one how much you care. Have fun and enjoy the process!\n' +
      '\n' +
      '**Output** (llama-v2-7b-chat):\n' +
      '\n' +
      'Wrapping a present neatly can be a bit of a challenge, but with a few simple tips and tricks, you can create a beautifully wrapped gift that your recipient will love. Here are some tips to help you wrap a present neatly:\n' +
      '\n' +
      '1. Use the right wrapping paper: Choose a wrapping paper that complements the gift and is the right size for the present. You can also use a patterned paper or a solid color, depending on your preference.\n' +
      '\n' +
      '2. Measure the gift: Before you start wrapping, measure the gift to make sure you have enough wrapping paper. You can also use a piece of scrap paper to measure the gift and mark the edges with a pencil.\n' +
      '\n' +
      '3. Use a flat surface: Make sure you have a flat surface to work on, such as a table or countertop. This will help you keep the wrapping paper flat and prevent it from wrinkling.\n' +
      '\n' +
      '4. Start with the edges: Begin by folding the edges of the wrapping paper over the gift, making sure they are even and smooth. You can use tape to secure the edges if needed.\n' +
      '\n' +
      '5. Roll the gift: Once you have the edges in place, roll the gift into the center of the wrapping paper. Make sure the gift is securely in place before moving on to the next step.\n' +
      '\n' +
      '6. Tuck in the edges: As you wrap the gift, tuck the edges of the wrapping paper under the gift, smoothing them out as you go. Make sure the edges are even and smooth, and use tape to secure them if needed.\n' +
      '\n' +
      '7. Add a ribbon or bow: Once you have the gift wrapped, add a ribbon or bow to the top. This will not only make the gift look more festive, but it will also help to keep the wrapping paper in place.\n' +
      '\n' +
      '8. Practice makes perfect: Wrapping a present neatly takes a bit of practice, so don\'t be discouraged if it doesn\'t come out perfectly the first time. Keep practicing, and you\'ll get the hang of it in no time!\n' +
      '\n' +
      'By following these tips, you can create a beautifully wrapped present that your recipient will love. Happy wrapping!\n' +
      '\n' +
      '**Input:**\n' +
      '\n' +
      'What are the best exercises for beginners?\n' +
      '\n' +
      '**Output** (MobileLLM-125M):\n' +
      '\n' +
      '1. Push-ups: This is a classic exercise that can be done anywhere, anytime. It\'s easy to do and requires no equipment.\n' +
      '\n' +
      '2. Dumbbell Rows: These rows are great for building strength in your legs and core.\n' +
      '\n' +
      '3. Squarks: Squarks are a great way to build strength in your legs and core.\n' +
      '\n' +
      '4. Lunges: Lunges are a great way to improve balance and stability.\n' +
      '\n' +
      '5. Plank: A plank is a great way to build strength in your back and core.\n' +
      '\n' +
      '6. Jumping Jacks: Jumping jacks are a fun and effective way to build strength in your legs and core.\n' +
      '\n' +
      '7. Burpees: Burpees are a great way to build strength in your legs and core.\n' +
      '\n' +
      '8. Mountain Climbers: Mountain climbers are a great way to build strength in your legs and core.\n' +
      '\n' +
      '9. Deadlifts: Deadlifts are a great way to build strength in your legs and core.\n' +
      '\n' +
      '10. Tricep Dips: Tricep dips are a great way to build strength in your triceps.\n' +
      '\n' +
      'Remember, it\'s important to start with simple exercises and gradually increase the intensity as you get stronger. Don\'t push yourself too hard or you may find it difficult to maintain proper form. With consistent practice, you will see improvements in your strength and overall fitness.\n' +
      '\n' +
      'If you have any questions about exercises or healthy living, don\'t hesitate to talk to a doctor or physical therapist.\n' +
      '\n' +
      'They can help you develop a personalized plan that works best for you.\n' +
      '\n' +
      'In conclusion, there are many exercises that can be done at home, but it\'s important to start slowly and gradually increase the intensity over time. Remember to listen to your body and take breaks when needed. And most importantly, enjoy the process!\n' +
      '\n' +
      '**Output** (MobileLLM-350M):\n' +
      '\n' +
      'Beginners should start with exercises that are easy to do and don\'t require a lot of strength or endurance. Here are some exercises that are great for beginners:\n' +
      '\n' +
      '1. Push-ups: This is a great exercise for beginners because it\'s a simple exercise that can be done anywhere. Start with a push-up on your knees and then lower yourself down to the ground.\n' +
      '\n' +
      '2. Squarks: Squarks are a great exercise for building leg strength and endurance. Start with a squat on your knees and then lower yourself down to the ground.\n' +
      '\n' +
      '3. Lunges: Lunges are a great exercise for building leg strength and endurance. Start with a lunge on your front foot and then lower yourself down to the ground.\n' +
      '\n' +
      '4. Plank: Planks are a great exercise for building core strength and stability. Start with a plank on your forearms and then lower yourself down to the ground.\n' +
      '\n' +
      '5. Bicycle crunches: Bicycle crunches are a great exercise for building upper body strength and stability. Start with a bicycle crunch on your upper back and then lower yourself down to the ground.\n' +
      '\n' +
      '6. Plank with leg lifts: This exercise is great for building leg strength and stability. Start with a plank on your forearms and then lift your legs off the ground and hold for a few seconds before lowering them back down.\n' +
      '\n' +
      '7. Jumping jacks: Jumping jacks are a great exercise for building cardiovascular endurance and strength. Start with a jumping jack on your feet and then jump up and spread your legs.\n' +
      '\n' +
      'Remember to start with a weight that is comfortable for you and gradually increase the weight as you get stronger.\n' +
      '\n' +
      'It\'s also important to warm up before exercising and to stretch after your workout to prevent injury.\n' +
      '\n' +
      'In conclusion, beginners should start with exercises that are easy to do and don\'t require a lot of strength or endurance. These exercises are great for building leg strength and endurance, and can be done anywhere. Remember to start with a weight that is comfortable for you and gradually increase the weight as you get stronger.\n' +
      '\n' +
      'Warming up and stretching after your workout are also important to prevent injury.\n' +
      '\n' +
      'Following these exercises can help you build strength, endurance, and improve your overall fitness level.\n' +
      '\n' +
      'Remember to always consult with a healthcare professional before starting any new exercise program.\n' +
      '\n' +
      '**Output** (llama-v2-7b-chat):\n' +
      '\n' +
      'Exercise is an essential part of a healthy lifestyle, but for beginners, it can be intimidating and overwhelming. Here are some of the best exercises for beginners:\n' +
      '\n' +
      '1. Bodyweight exercises: These exercises are great for beginners because they don\'t require any equipment. Examples include push-ups, squats, lunges, and planks.\n' +
      '\n' +
      '2. Yoga: Yoga is a low-impact exercise that can help improve flexibility, balance, and strength. It\'s also great for reducing stress and improving overall well-being.\n' +
      '\n' +
      '3. Brisk walking: Walking is a simple and accessible form of exercise that can be done anywhere. It\'s a great way to get your heart rate up and improve cardiovascular health.\n' +
      '\n' +
      '4. Swimming: Swimming is a low-impact exercise that can help improve cardiovascular health and strengthen muscles. It\'s also easy on the joints, making it a great option for beginners.\n' +
      '\n' +
      '5. Cycling: Cycling is another low-impact exercise that can be done indoors or outdoors. It\'s a great way to improve cardiovascular health and build leg strength.\n' +
      '\n' +
      '6. Resistance band exercises: Resistance bands are lightweight and portable, making them easy to use at home or on the go. They can help improve strength and flexibility in the upper body, lower body, and core.\n' +
      '\n' +
      '7. Pilates: Pilates is a form of exercise that focuses on core strength, flexibility, and body control. It\'s a great option for beginners because it\'s low-impact and can help improve posture and balance.\n' +
      '\n' +
      '8. Dance-based workouts: Dance-based workouts, such as Zumba or Hip Hop Abs, are fun and energetic ways to get in shape. They often involve aerobic exercise and strength training, making them a great option for beginners.\n' +
      '\n' +
      '9. Jumping rope: Jumping rope is a high-intensity exercise that can help improve cardiovascular health and burn calories. It\'s also easy to do at home with a simple jump rope.\n' +
      '\n' +
      '10. Kettlebell exercises: Kettlebells are small weights that can be used to perform a variety of exercises, including swings, squats, and presses. They\'re a great way to improve strength and cardiovascular health.\n' +
      '\n' +
      'Remember to always consult with a doctor before starting any new exercise program, especially if you\'re a beginner. It\'s also important to start slowly and gradually increase the intensity and duration of your workouts as you become more comfortable.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>