<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '결과적으로, 우리는 LLM을 축소하는 것이 필수적이다.\n' +
      '\n' +
      '또한, _portability_와 _computational cost_에 대한 고려는 스마트폰 및 모바일 장치에 LLM을 배치해야 하는 필요성을 촉진한다. 현재 모바일 기술의 환경에서 LLaMA-v2 7B(Touvron et al., 2023b)와 같은 LLM을 8비트 가중치와 통합하는 것은 DRAM(main-memory) 용량 소스의 한계로 인해 엄청나게 비싸다는 것을 증명한다. 모바일 장치에서 널리 퍼진 메모리 계층이 그림 2에 묘사되어 있다. DRAM 용량이 아이폰 15의 경우 6GB, 구글 픽셀 8 프로의 경우 12GB(Hristov, 2022; Google, 2023)인 경우, 모바일 앱은 DRAM이 운영 체제 및 다른 애플리케이션과 공유되기 때문에 DRAM의 10%를 초과하지 않아야 한다(Malladi et al., 2012). 이것은 10억 미만의 매개변수 LLM을 배포하는 동기를 부여합니다. 추가로, LLM 에너지 소비(모델 파라미터에서 10억 명당 0.1 J/토큰), 7B-파라미터 LLM은 0.7 J/토큰을 소비한다(Han et al., 2016; Malladi et al., 2012). 약 50kJ의 에너지를 가진 완전히 충전된 아이폰은 10토큰/s의 속도로 2시간 이내에 이 모델을 대화에서 유지할 수 있으며 64토큰마다 배터리의 0.2%가 소모된다.\n' +
      '\n' +
      '이러한 요구는 온-디바이스 실행을 위한 콤팩트 모델의 채택이라는 단일 필수 사항에 수렴한다. 0.035 J/토큰만 소비하는 350M 8비트 모델과 같은 10억 이하의 모델을 활용하면 아이폰은 하루 종일 대화 사용을 지원할 수 있다. 또한, LLaMA 7B 모델을 3\\(\\sim\\)6tokens/second5로 사용하는 최신 아이폰 앱 MLC Chat에 비해 50tokens/s에서 동작할 수 있는 125M 모델의 벤치마크 결과에 의해 예시된 바와 같이 디코딩 속도가 크게 향상될 수 있다. 이러한 고려 사항들에 비추어, 본 논문은 10억 미만의 파라미터를 갖는 LLM의 설계 및 구현에 의해 동기 부여된다.\n' +
      '\n' +
      '각주 5: [https://llm.mlc.ai](https://llm.mlc.ai)\n' +
      '\n' +
      '우리는 10억 매개 변수 하에서 현재까지 가장 정확한 LLM을 구축하기 위해 다음과 같은 기여를 한다.\n' +
      '\n' +
      '* 스케일링 법칙(Kaplan et al., 2020)에 모순되는, 우리는 작은 LLMs에 대해 깊이가 폭보다 더 중요하다는 것을 입증한다. 심층적이고 얇은 모델 구조는 추상적인 개념을 포착하는 데 탁월하여 최종 성능이 우수합니다.\n' +
      '* We revisit embedding sharing methods (Zhang et al., 2022) and implement grouped query attention (Ainslie et al., 2023) in small LLMs to maximize weight utilization.\n' +
      '* 즉각적인 블록별 가중치 공유를 제안한다. 메모리 이동이 레이턴시 병목 현상인 시나리오에서, 두 개의 인접한 블록들 사이의 가중치 공유는 가중치 이동을 회피하여, 블록을 두 번만 컴퓨팅하고 최소 레이턴시 오버헤드를 발생시킨다.\n' +
      '* 우리는 SOTA 성능을 보여주는 모바일LLM의 새로운 모델 패밀리를 제안한다. 제로샷 태스크 집합에서 MobileLLM은 기존 SOTA 125M/350M 모델보다 2.7%/4.3% 더 우수하다.\n' +
      '* Chat 및 API 호출과 같은 다운스트림 작업에서 MobileLLM 모델 패밀리는 동등 크기 모델보다 상당히 우수하다. API 호출 태스크에서, 모바일LLM은 심지어 상당히 큰 LLaMA-v2 7B 모델로서 비교가능한 정확한 매칭 스코어를 달성한다.\n' +
      '\n' +
      '##2 Subbillion Scale LLM 설계 개선\n' +
      '\n' +
      '이 절에서는 기준선 하위 10억 매개변수 모델에서 새로운 최첨단 모델(그림 3)로의 진화 경로를 제시한다. 우리는 125M 및 350M 모델을 모두 탐색하고 두 경우 모두에서 일관된 개선을 보여준다. 모델 크기가 주요 제약 조건인 온디바이스 사용 사례의 경우 제한된 가중치 매개변수를 효과적으로 할당하는 방법이 그 어느 때보다 중요해진다. 본 논문에서는 먼저, (1) SwiGLU FFN (Dauphin et al., 2017), (2) forcing _lanky_ (deep and thin) architectureures (3) 재방문 임베딩 공유 방법 (Zhang et al., 2022) (4) grouped query attention (Chowdhery et al., 2023)을 포함하여 10억 이하의 스케일 LLM에 유용한 4가지 모델 설계 기법을 테스트하여 MobileLLM이라는 강력한 베이스라인 모델을 제안한다. 그리고 메모리 제한 LM 디코딩 과정에서 추가적인 메모리 오버헤드가 발생하지 않고 약간의 지연 오버헤드만으로도 정확도를 더욱 높일 수 있는 즉각적인 블록별 계층 공유 방법을 개발한다. 우리는 계층 공유를 가진 모델을 MobileLLM-LS로 나타낸다.\n' +
      '\n' +
      '### Training Setup\n' +
      '\n' +
      '실험은 32개의 A100 GPU에 대해 수행되었으며, 각 GPU의 배치 크기는 32이며, 0.25T 토큰에 대해 120k 반복으로 탐색적 실험을 수행하였다. 이어서, 표 3 및 표 4에 보고된 상위 모델들은 1T 토큰들에 대해 480k 반복으로 트레이닝된다.\n' +
      '\n' +
      '도 2: 널리 퍼진 모바일 디바이스에서의 메모리 계층구조. 적절한 플래시 저장에도 불구하고, 고속 애플리케이션을 실행하기 위한 동작 메모리는 주로 DRAM에 상주하며, 일반적으로 6-12GB로 제한된다.\n' +
      '\n' +
      '본 논문에서는 ARC-easy, ARCH-challenge(Clark et al., 2018), BoolQ(Clark et al., 2019), PIQA(Bisk et al., 2020), SIQA(Sap et al., 2019), HellaSwag(Zellers et al., 2019), OBQA(Mihaylov et al., 2018), WinoGrande(Sakaguchi et al., 2021), TQA(Joshi et al., 2017) 및 RACE 데이터셋(Lai et al., 2017)을 사용하여 질문 응답 및 읽기 이해 작업을 포함한 제로샷 상식 추론 작업에 대한 사전 학습된 모델을 평가한다.\n' +
      '\n' +
      '튼튼한 기준선 구축\n' +
      '\n' +
      '###### 2.2.1 전향적 네트워크 선택\n' +
      '\n' +
      '우리는 먼저 피드포워드 네트워크(FFN)에서 일반적으로 사용되는 활성화 함수를 조사하고 최첨단 SwiGLU(Dauphin et al., 2017)가 소형 모델에도 유용하다는 것을 발견했다. 바닐라 FFN(\\(FC\\to ReLU\\to FC\\))을 SwiGLU로 변경함으로써, 125M 모델의 경우 제로 샷 추론 작업에 대한 평균 성능이 42.6에서 43.9로 향상된다. 따라서 이후 실험을 위해 FFN에서 SwiGLU를 사용한다.\n' +
      '\n' +
      '###### 2.2.2 건축 깊이 대 폭\n' +
      '\n' +
      '현장에 널리 퍼져 있는 믿음(Kaplan et al., 2020)은 변압기 모델의 성능이 주로 매개변수 수, 훈련 데이터 세트의 크기 및 훈련 반복 횟수에 의해 결정됨을 시사한다. 이러한 믿음은 건축 설계가 변압기 모델의 성능에 거의 영향을 미치지 않는다고 가정한다. 그러나 우리의 연구 결과는 이것이 더 작은 모델에 적용되지 않을 수 있음을 나타낸다.\n' +
      '\n' +
      '특히 모델 용량이 제한된 소형 모델에 대한 실험 결과는 성능 향상을 위해 더 넓게 가는 것보다 더 깊게 가는 것이 더 중요하다는 것을 보여준다. 우리는 \\(\\sim\\)125M 매개변수가 있는 9개 모델과 \\(\\sim\\)350M 매개변수가 있는 10개 모델을 포함하여 19개 모델의 훈련을 포함하는 광범위한 연구를 수행했다. 각 모델은 비슷한 크기로 설계되었지만 깊이 및 너비 측면에서 다양했다. 우리는 8개의 제로샷 상식 추론 과제와 질의 응답 및 읽기 이해 벤치마크를 실험한다. 우리의 연구 결과는 더 깊고 얇은 모델이 더 얕고 더 넓은 모델을 능가한다는 것을 일관되게 보여준다. 그림 4의 (a)와 (b)는 ARC-easy, ARC-challenge, PIQA, HellaSwag, OBQA, WinoGrande를 포함한 대부분의 제로 샷 추론 작업에 걸쳐 더 깊은 네트워크의 우수한 성능을 보여준다. 특히 이러한 경향은 그림 4(c)-(f)와 같이 TQA 및 RACE 데이터 세트에서 훨씬 더 두드러진다. 자세한 모델 구성과 결과는 부록에서 확인할 수 있다. 본 연구 결과는 약 125M 크기의 변압기 모델에 대해 30층 또는 42층의 모델이 12층의 모델보다 훨씬 더 나은 성능을 보인다는 것을 시사한다. 이 발견은 대부분의 이전 125M 모델들(Zhang et al., 2022; Black et al., 2022)에서 층들의 수를 고려하면 놀라운 것이다(Zhang et al., 2022).\n' +
      '\n' +
      '###### 2.2.3 임베딩 공유\n' +
      '\n' +
      '10억 미만의 스케일 언어 모델들에서, 임베딩 계층들은 파라미터 카운트의 상당한 부분을 구성한다. 예를 들어, 512의 임베딩 차원과 32k의 어휘 크기를 갖는, 입력 및 출력 임베딩 레이어들은 각각 1,600만 개의 파라미터들을 포함한다. 함께, 이러한 임베딩 레이어는 125M-파라미터 모델의 전체 파라미터의 20% 이상을 차지한다. 대조적으로, 이 비율은 더 큰 언어 모델에서 상당히 낮다. 예를 들어, 입력 및 출력 임베딩은 LLaMA-7B 모델(Touvron et al., 2023)에서 파라미터들의 총 수의 3.7%만을 설명하고 LLaMA-70B 모델에서는 단지 0.7%만을 설명한다. 이러한 불일치는 임베딩 공유가 처음에 OPT 모델(Zhang et al., 2022)에서 제안되고 구현되었지만 이후 LLM의 최근 설계에서 무시된 이유를 설명할 수 있다.\n' +
      '\n' +
      '10억 미만의 규모 언어 모델 개발에서 우리는 투입산출 임베딩 공유의 개념을 재조명한다. LLM 모델에서의 입력 임베딩은 어휘 내의 토큰 id를 대응하는 토큰 임베딩에 매핑하고 \\((vocab\\_size,embedding\\_dim)\\)의 차원을 갖는다. 반대로, 출력이 완전히 연결된 계층은 임베딩 차원을 어휘 전체에 걸친 로짓 예측에 다시 매핑하며, 가중치 크기는 \\((vocab\\_size,embedding\\_dim)\\이다. 공유\n' +
      '\n' +
      '그림 3: 수십억 이하의 크기의 변압기 모델의 설계 로드맵. 전경 및 배경 막대는 각각 125M 및 350M 모델에 대한 제로 샷 상식 추론 작업에 대한 평균 정확도를 나타낸다. 처음에 12-레이어 768-차원 구조인 125M 모델은 피드-포워드 네트워크 설계, 네트워크 깊이 조정 및 가중치 공유 전략 개선을 통해 향상된다. 각 수정에 대한 자세한 정확도는 부록에서 확인할 수 있다.\n' +
      '\n' +
      '임베딩은 입력 임베딩 가중치를 출력 완전 연결 레이어 가중치로 재사용하여 보다 효율적이고 컴팩트한 모델 아키텍처를 생성한다.\n' +
      '\n' +
      '우리는 30층 125M 모델에 대해 실험한다. 표 1에서 입력 및 출력 임베딩을 공유하면 평균 정확도가 0.2포인트 떨어지는 총 매개변수의 약 11.8%인 매개변수 수가 16M 감소한다는 것을 보여준다. 저장된 매개변수를 다시 할당하여 더 많은 레이어를 추가함으로써 한계 정확도 저하를 쉽게 복원할 수 있다. 32개의 레이어로 깊이를 증가시키는 것은 원래의 135M 모델에 비해 여전히 10M 더 적은 파라미터를 유지하면서 0.4 포인트 정확도 이득을 생성한다. 350M 모델에서도 유사한 결과가 관찰된다. 이러한 연구 결과는 임베딩 공유가 제한된 모델 스토리지 예산이 주어진 경우 가중치 활용을 극대화하고 모델 성능을 최적화하는 데 유용한 기법임을 시사한다.\n' +
      '\n' +
      'Heads and KV Heads Number 2.2.4\n' +
      '\n' +
      '이제 소형 변압기 모델의 최적 헤드 크기를 조사합니다. 헤드 차원당 더 많은 의미론과 여러 헤드의 더 많은 비선형 조합 사이의 절충은 헤드 크기를 선택하는 데 중요한 고려 사항이다. 또한, 대부분의 선행 연구들은 일반적으로 동일한 수의 키-값 헤드를 사용하여 수십억 이하의 파라미터 언어 모델에서 헤드를 쿼리했다. 그 대신, 초기에 LLMs Chowdhery et al. (2023); Ainslie et al. (2023)에서 키-값 캐시 크기를 줄이기 위해 설계된 그룹화된 쿼리 어텐션이 작은 LMs에서 키-값 헤드의 중복성을 효과적으로 감소시킬 수 있다는 것을 발견했다. 그룹화된 질의어텐션은 가중치 재사용을 위한 가중치 공유의 또 다른 형태로 볼 수 있는데, 키-값 헤드의 수는 질의 헤드의 수보다 \\(1/n\\)이고, kv 헤드는 어텐션 스코어 계산에서 \\(n\\)회 반복되어 질의와 함께 출력된다. 여기서, \\(n\\in\\mathbb{Z}^{+}\\)는 질의 헤드의 개수가 나눌 수 있는 양의 정수를 의미한다.\n' +
      '\n' +
      '최첨단 소형 변압기 모델의 견고한 기반을 구축하기 위해 125M 및 350M 모델에 대해 바람직한 헤드 크기를 결정하기 위한 실험을 수행했다. 그림 5의 결과는 16개의 쿼리 헤드를 사용하는 것이 가장 좋은 결과를 생성함을 보여준다. 또한 kv-헤드의 수를 16개에서 4개로 줄이면 125M 모델에 대해 비슷한 정확도를 보였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **\\# Params** & **ARC-e** & **ARC-c** & **BoolQ** & **PIQA** & **SIQA** & **HS** & **OBQA** & **WinoGrande** & **Avg.** \\\\ \\hline Without emb-share & 135M & 43.6 & 26.1 & 58.0 & 62.5 & 42.6 & 36.5 & 37.5 & 51.5 & 44.8 \\\\ + emb-share & 119M & 44.4 & 26.0 & 56.2 & 62.8 & 43.1 & 35.9 & 36.0 & 52.6 & 44.6 \\\\ + emb-share, \\(\\uparrow\\) depth & 125M & 43.3 & 26.4 & 54.4 & 64.7 & 43.5 & 36.9 & 38.5 & 52.6 & 45.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 제로샷 상식 추론 작업에 512개의 임베딩 차원이 있는 30-레이어 모델을 사용한 입출력 임베딩 공유에 대한 절제 연구. 증가된 깊이(\\(\\uparrow\\) 깊이) 모델은 32개의 층을 갖는다.\n' +
      '\n' +
      '그림 4: 비교 가능한 모델 크기에서 더 깊고 얇은 모델은 일반적으로 제로 샷 상식 추론, 질문 응답 및 읽기 이해와 같은 다양한 작업에 걸쳐 더 넓고 얕은 모델을 능가한다.\n' +
      '\n' +
      '그림 5: 머리 수와 kv-머리 수에 대한 절제 연구. 여기서, 비율은 kv-헤드의 수로 나눈 헤드의 수를 나타낸다. 제로 샷 추론 작업에 대한 평균 정확도가 보고된다.\n' +
      '\n' +
      '그리고 모델 크기 감소가 거의 10%인 350M 모델에서 정확도가 0.2포인트만 떨어진다. 이러한 결과는 모델 아키텍처 설계에 지침이 된다. 그룹화된 질의어텐션(GQA)을 채택하고, 모델 크기를 유지하기 위해 임베딩 차원을 증가시킴으로써, 125M의 정확도는 0.4 포인트 더 증가하며, 이는 작은 모델의 잠재력을 추가로 스퀴즈 아웃하기 위한 유리한 방법으로서 GQA를 나타낸다.\n' +
      '\n' +
      '요약하면, 우리는 SwiGLU가 있는 FFN, 깊고 얇은 아키텍처, 임베딩 공유 및 그룹화된 쿼리 주의를 포함하여 소규모 모델 설계에 유익한 4가지 최신 기술을 테스트했습니다. 이러한 기술을 결합하여 강력한 기준선 작은 LLM을 구축하고 이를 MobileLLM이라고 명명한다.\n' +
      '\n' +
      '### Layer Sharing\n' +
      '\n' +
      '2.2.2절에서 층 깊이 대 폭의 영향에 대한 결과는 더 깊은 층이 소형 변압기 모델에 유리함을 시사한다. 이는 추가적인 모델 저장 비용 없이 은닉층 수를 늘리기 위한 전략으로 계층 공유를 조사하도록 동기를 부여한다. 이 접근법은 모델 크기가 주요 제약 조건인 온-디바이스 시나리오에서 특히 유용하다.\n' +
      '\n' +
      '놀랍게도, 실험 결과는 구조적 수정이나 모델 크기의 확대가 필요 없이 단순히 변압기 블록을 복제함으로써 정확도 향상을 달성할 수 있음을 보여준다. 우리는 그림 6에 설명된 세 가지 다른 가중치 공유 전략을 추가로 조사했다. 표 2의 결과는 반복-오버 레이어 공유 전략이 즉각적인 블록별 반복, 반복 전체 및 역방향 공유 전략 중 최고의 성능을 생성함을 나타낸다. 그러나, 하드웨어 메모리 계층구조(도 2)를 고려하면, 컴퓨팅을 위한 SRAM은 통상적으로 약 20MB로 제한된다. 이 용량은 보통 단일 변압기 블록을 보유하기에 충분하다. 따라서, 공유된 가중치를 캐쉬에 배치하고 컴퓨팅을 즉시 두 번 수행하는 것은 SRAM과 DRAM 사이에서 가중치를 전달할 필요성을 피할 수 있으며, 결과적으로 자동 회귀 추론을 위한 전체 실행 속도가 향상된다. 결과적으로, 우리는 모델 설계에서 즉각적인 블록별 공유 전략을 선택했다. 제안된 모델은 MobileLLM-LS로 계층 공유를 나타낸다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '### Experimental Settings\n' +
      '\n' +
      '본 논문에서는 무게감소가 0.1인 Adam Optimizer (Kingma and Ba, 2014)를 이용하여 MobileLLM을 처음부터 훈련하고, 각 GPU에서 32개의 A100 GPU를 사용하여 배치크기가 32인 실험을 수행하였다. 초기 학습률은 2e-3으로 설정되며 코사인 학습률 감쇠 전략을 따른다. 0.25T 토큰에 대해 120k 반복으로 빠른 탐색 실험을 수행하고 1T 토큰에 대해 480k 반복으로 표 3 및 4에 보고된 최상의 모델을 훈련한다.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '0-shot 상식 추론 과제, 질의 응답 과제, 읽기 이해 과제에 대한 최종 성과를 비교한다. 기준선 방법의 결과는 일관된 평가 절차를 보장하기 위해 오픈 소스 허깅 페이스 모델을 사용하여 평가되었다.\n' +
      '\n' +
      '**Zero-shot Common Sense Reasoning** 표 3은 초기 오픈소싱 LLMs, OPT(Zhang et al., 2022), BLOOM(Scao et al., 2022) 및 Galactic(Taylor et al., 2022), Cerebras(Dey et al., 2023), GPT-neo(Black et al., 2022)와 같은 최근 출시를 포함한 우리의 제안된 모델, MobileLLM 및 최신 서브-billion 파라미터 모델들 간의 비교 및 제로-샷 상식 추론 태스크들에 대한 LLM 분석 스위트 피티아(Biderman et al., 2023) 및 변압기 변형 RWKV(Peng et al., 2023)를 나타낸다. 125M 모델 크기에 대해 MobileLLM은 동일한 모델 크기에서 OPT, GPT-Neo 및 칼락티카와 같은 이전 모델보다 상당한 마진만큼 유리하게 우수하다. 또한 MobileLLM-125M은 Pythia-160M과 RWKV-169M보다 각각 22%와 26% 작은 3.8점과 2.7점의 높은 정확도를 보였다. 또한, MobileLLM-LS-125M에 계층 공유를 통합하면 정확도가 0.7 포인트 더 향상된다. 모바일LLM-LS-125M은 대부분의 이전 350M 모델보다 비슷하거나 심지어 더 높은 결과를 달성한다는 점은 주목할 만하다. 350M 모델 크기 카테고리에서 MobileLLM은 이전 최신 모델을 능가합니다.\n' +
      '\n' +
      '도 6: (a) 레이어 공유가 없는 기준선 모델; (b) 즉시 블록별 공유; (c) 반복 전체 공유; (d) 역방향 공유. 변압기 블록은 다중 헤드 자기 주의(multi-head self-attention, MHSA)와 피드 포워드 네트워크(feed-forward network, FFN)를 포함한다. 반복 전체 공유가 약간 더 높은 성능을 갖는 반면, 즉시 블록별 공유는 공유된 가중치가 캐시에 남아서 즉시 두 번 계산될 수 있기 때문에 캐시를 가장 잘 활용한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '부록의 채팅 예제들은 또한 모바일LLM 모델들에 의해 생성된 인상적인 응답의 품질을 강조한다.\n' +
      '\n' +
      '###### 3.3.2 API 호출\n' +
      '\n' +
      'API 호출은 특히 어시스턴트 기능을 위한 오디오-텍스트 모델과의 협업에서 일반적인 온-디바이스 애플리케이션이다. API 호출을 위한 LLMs를 활용하는 것은 자연 언어 입력을 JSON 구성으로 변환하여 대응하는 API6을 호출하는 것을 포함한다. 예를 들어, 입력 _"Help me set a alarm at 7:30 AM"_이 주어지면 모델은 _[API: "alarm(time="7:30 am")"]_를 출력한다. 또한, 모델은 에이전트 응답을 생성합니다: _"Sure: 귀하의 알람은 오전 7시 30분으로 설정됩니다."_\n' +
      '\n' +
      '각주 6: [https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)\n' +
      '\n' +
      '이 작업을 위해 LLM을 적용하기 위해 5000개의 훈련 샘플과 2500개의 테스트 샘플이 있는 합성 데이터 세트를 생성한다. 각 샘플에는 평균 8번의 대화가 포함됩니다. 이 데이터 세트의 자세한 예는 부록에 나와 있습니다. 사전 훈련된 모델은 2e-5에서 시작하는 선형 감쇠 학습 속도와 0.01의 가중치 감쇠를 갖는 아담 최적화기를 사용하여 4개의 에폭에 대한 훈련 세트를 미세 조정한다.\n' +
      '\n' +
      '표 6은 MobileLLM-350M이 LLaMA-v2 7B와 유사한 인텐트 및 구조 정확한 매치 스코어를 보여주며, 여기서 높은 인텐트 스코어는 호출하려는 API 사용자의 정확한 예측을 나타내는 반면, 구조적 정확한 매치 스코어는 API 함수 내의 콘텐츠를 예측하는 숙련도를 반영한다. 모바일LLM-350M의 루지 점수가 7B 모델에 비해 낮음에도 불구하고 API 호출이 올바른 API 호출을 우선시한다는 점에 유의해야 한다. 결과는 온-디바이스 애플리케이션에서 특정 공통 시나리오가 특별히 도전적이지 않으며 모바일LLM-350M과 같은 더 작은 모델이 이를 능숙하게 처리할 수 있음을 시사한다.\n' +
      '\n' +
      '### 양자화와 호환성\n' +
      '\n' +
      '우리는 추가로 0.25T 토큰에 대해 125M 및 350M 모델 크기를 훈련한 MobileLLM 및 MobileLLM-LS 모델 모두에 대해 토큰당 최소-최대 사후 훈련 양자화(PTQ) 실험을 수행한다. 그림 7은 W8A8 PTQ를 사용하면 0.5점 미만의 적당한 정확도 감소를 얻을 수 있으며 계층 공유와 호환성을 유지한다는 것을 보여준다.\n' +
      '\n' +
      '### Knowledge Distillation\n' +
      '\n' +
      '지금까지 우리는 다음 토큰을 하드 레이블로 사용하여 처음부터 컴팩트 모델을 교육했습니다. 우리는 LLAMA-v2 7B를 교사로 사용하여 125M 및 350M 모델의 지식 증류(KD)를 조사했다. 불행히도 KD는 훈련 시간을 증가시키고(2.6-3.2\\times\\) 레이블 기반 훈련(부록의 세부사항)과 비슷하거나 낮은 정확도를 나타낸다.\n' +
      '\n' +
      '### On-device Profiling\n' +
      '\n' +
      '아이폰 13(iOS 17.2.1)에서 ExecuTorch7을 이용한 MobileLLM-125M과 MobileLLM-LS-125M FP16 모델의 대기시간을 측정하였고, Metal Performance Shaders를 사용하였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Model** & \\multicolumn{2}{c}{EM\\({}_{\\text{intent}}\\)} & \\multicolumn{2}{c}{EM\\({}_{\\text{structure}}\\)} & \\multicolumn{1}{c}{R1} & \\multicolumn{1}{c}{RL\\({}_{\\text{max}}\\)} \\\\ \\hline OPT-350M & 56.1 & 38.6 & 37.1 & 35.3 \\\\ Pythia-410M & 62.2 & 44.7 & 43.1 & 41.1 \\\\ BLOOM-560M & **64.7** & 37.9 & 36.9 & 34.6 \\\\ MobileLLM-350M & **65.3** & **48.8** & **46.8** & **44.6** \\\\ \\hline LLaMA-v2 7B & 62.8 & **50.9** & **56.5** & **54.3** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: API 호출 평가 점수. EM\\({}_text{intent}\\)/EM\\({}_text{structure}\\)는 API 호출에서 정확한 일치를 측정한다. R1/RL은 에이전트 반응의 품질을 측정하는 Rouge-1/L 점수를 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Model** & \\multicolumn{2}{c}{**MT-Bench\\({}_{\\text{(score)}}\\)**} & \\multicolumn{1}{c}{**Alpaca Eval\\({}_{\\text{(win %)}}\\)**} \\\\ \\hline \\multicolumn{4}{c}{_number of parameters \\textless{} 200M_} \\\\ \\hline OPT-125M & 1.21 & 3.91 \\\\ GPT-Neo-125M & 1.06 & 1.01 \\\\ Pythia-160M & 1.01 & 0.63 \\\\ MobileLLM-125M & **2.33** & **24.07** \\\\ MobileLLM-LS-125M & **2.52** & **23.79** \\\\ \\hline \\multicolumn{4}{c}{_200M \\textless{} number of parameters \\textless{} 1B_} \\\\ \\hline OPT-350M & 1.37 & 6.80 \\\\ Pythia-410M & 1.62 & 13.87 \\\\ BLOOM-560M & 1.73 & 10.29 \\\\ MobileLLM-350M & **3.28** & **47.08** \\\\ MobileLLM-LS-350M & **3.16** & **48.20** \\\\ \\hline \\multicolumn{4}{c}{_number of parameters \\textgreater{} 1B_} \\\\ \\hline Pythia-1B & 1.70 & 16.62 \\\\ BLOOM-1.1B & 2.37 & 19.90 \\\\ Falcon-1.3B & 2.54 & 30.38 \\\\ OPT-1.3B & 2.24 & 38.84 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: AlpacaEvaluator에 대한 벤치마크 결과: GPT-4; 참조 모델: text-davinci-001) 및 MT-Bench.\n' +
      '\n' +
      '그림 7: BFloat16 모델과 8비트 가중치 8비트 활성화 후 양자화 모델 간의 비교.\n' +
      '\n' +
      '(MPS) 백엔드8. 모델 로딩, 초기화 및 실행 시간이 표 7에 보고된다. 구체적으로, 실행 시간은 50회 반복에 걸쳐 평균화된다.\n' +
      '\n' +
      '각주 8: [https://pytorch.org/executorch/stable/build-run-mps.html](https://pytorch.org/executorch/stable/build-run-mps.html)\n' +
      '\n' +
      '표 7의 결과는 가중치 공유와 레이어 수를 두 배로 늘리면 모바일LLM-LS가 유사한 모델 크기에 기인하여 모바일LLM에 비해 로딩 및 초기화 시간이 2.2%만 증가한다는 것을 반영한다. 실행 시간은 또한 데이터 지역성으로 인해 2.6%의 오버헤드에 불과합니다. 이와는 대조적으로, 가중치 공유 없이 두 배의 레이어 수를 갖는 모델은 로딩 및 초기화 시간에서 상당한 143% 상승 및 실행 시간에서 86% 증가를 나타낸다.\n' +
      '\n' +
      '##4 관련 업무\n' +
      '\n' +
      'LLM의 우수한 성능은 광범위한 응용 프로그램을 육성했다. LLM의 계산 비용과 에너지 소비를 고려할 때, LLM을 소형화하여 온-디바이스 추론을 가능하게 하는 새로운 연구 방향이 등장했다. 상기 방법들은,\n' +
      '\n' +
      '**Model Compression.** LLMs에 대하여 프루닝(Xia et al., 2023b), 희소성(Sun et al., 2023; Xia et al., 2023a; Frantar and Alistarh, 2023), 양자화(Liu et al., 2023a; Dettmers et al., 2022; Kim et al., 2023; Frantar et al., 2022; Xiao et al., 2023; Yao et al., 2022; Liu et al., 2023c;b; Frantar et al., 2022). 우리의 연구는 이러한 기술과 보완적이다. 섹션 3.4에서도 입증된 바와 같이, 우리의 방법론은 양자화와 호환된다.\n' +
      '\n' +
      '**Small Model Design.** 제한된 수의 연구에서 TinyLLaMA(Timiryasov and Tastet, 2023)와 같은 컴팩트 모델 아키텍처를 탐색하였다. 그러나 가장 작은 TinyLLaMA조차도 10억 개의 매개 변수를 초과하여 많은 온-디바이스 애플리케이션에서 여전히 금지된다. 일부 연구는 모델 패밀리(Zhang et al., 2022; Scao et al., 2022; Black et al., 2022; Dey et al., 2023) 또는 작은 LLM 변형을 포함하는 분석 제품군과 함께 큰 모델 아키텍처를 제안한다(Biderman et al., 2023). 그러나, 이러한 모델들은 10억 이하의 파라미터들의 제약 하에서 최적화되지 않으며, 따라서 최적이 아닐 수 있다.\n' +
      '\n' +
      '**Neural Architecture Search.** NAS는 컨볼루션 신경망의 영역, 특히 비전 태스크의 맥락에서 상당한 관심을 받았다(Tan and Le, 2019; Zoph and Le, 2016; Wu et al., 2019; Guo et al., 2020). 이와는 대조적으로, 트랜스포머 도메인 내에서, 지배적인 합의는 모델 아키텍처가 총 파라미터 수가 일관성을 유지한다면 정확도에 최소한의 영향을 미친다고 가정한다(Kaplan et al., 2020). 오직 제한된 수의 연구들만이 BERT 모델들을 대상으로 언어 트랜스포머를 위한 NAS 알고리즘을 개발하였다 (Xu et al., 2021; Jawahar et al., 2023; Ganesan et al., 2021). 깊이와 너비 사이의 상호 작용에 초점을 맞춘 우리의 현재 조사는 깊이 공간 내에서 세심한 그리드 검색으로 개념화될 수 있다. 그 연구의 결과는 확장 법칙을 둘러싼 널리 퍼진 정통성에 도전하며, 깊고 얇은 아키텍처가 소형 LLM에 대해 더 높은 성능을 입증한다고 제안한다.\n' +
      '\n' +
      '**가중치 공유.**가중치 공유는 고정된 파라미터 제약 조건 내에서 모델 가중치 활용을 최적화하기 위한 직관적인 전략이다. OPT 패밀리(Zhang et al., 2022) 및 후속 작업(Black et al., 2022)이 입력 및 출력 임베딩 사이의 가중치 공유를 레버리지하는 동안, 제한된 연구가 트랜스포머에서 중간 레이어에 대한 가중치 공유를 탐구했다(Shen et al., 2022; Reid et al., 2021). 이전 작업에는 종종 공유 레이어에 대한 특수 설계가 수반됩니다. 대조적으로, 우리의 기여는 변압기 블록을 단순히 반복하는 보다 간단하면서도 효과적인 방법을 강조하여 고정된 모델 크기와 최소 지연 증가로 향상된 정확도를 제공한다.\n' +
      '\n' +
      '**Efficient Attention and Implementation.** 효율적인 변압기 설계의 영역에서 많은 연구가 저순위 근사(Wang et al., 2020; Katharopoulos et al., 2020; Xiong et al., 2021) 및 희소 주의(Kitaev et al., 2020; Roy et al., 2021)와 같은 방법을 통해 주의 계산을 최적화하는 데 초점을 맞추고 있다. 또 다른 작업 라인은 플래시 어텐션(Dao et al., 2022) 및 플렉스젠(Sheng et al., 2023)과 같은 작업으로 예시된 하드웨어 스케줄링 및 웨이트 이동을 탐구한다. 대조적으로, 우리의 주요 목표는 새로운 주의 계산 또는 효율적인 하드웨어 구현 방법을 도입하지 않고 모델 크기를 최적화하는 것이다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 연구는 온-디바이스 애플리케이션을 위한 10억 이하의 스케일 모델을 최적화하는 것에 초점을 맞추고 있다. 우리의 연구 결과는 더 작은 모델의 경우 너비보다 깊이를 우선시하는 것이 모델 성능을 향상시킨다는 것을 나타낸다. 또한, 임베딩 공유, 그룹화된 질의 주의 및 블록별 가중치 공유를 포함한 고급 가중치 공유 기법을 활용하여 스토리지 제약 시나리오 내에서 가중치 활용도를 크게 향상시킵니다. 결과 모델\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & Load & Init & Execute \\\\ \\hline MobileLLM & 39.2 ms & 1361.7 ms & 15.6 ms \\\\ MobileLLM-LS & 43.6 ms & 1388.2 ms & 16.0 ms \\\\\n' +
      '60-layer non-shared & 68.6 ms & 3347.7 ms & 29.0 ms \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: MobileLLM-125M(30개 층), MobileLLM-LS-125M(2\\(\\times\\)30개 층, 가중치를 공유하는 인접 블록), 및 60-계층 비공유 가중치 모델의 지연 분석, 다른 모든 측면에서 일관된 구성을 갖는다.\n' +
      '\n' +
      'MobileLLM으로 명명된 것은 이전의 SoTA 방법에 비해 제로 샷 상식 추론, 질문 응답 및 읽기 이해 작업에서 상당한 발전을 보여준다. 마지막으로, 채팅 및 API 호출과 같은 두 가지 일반적인 온-디바이스 사용 사례에서 미세 조정된 모바일LLM 모델의 효율성을 입증하여 이러한 작업을 처리하는 데 능숙함을 강조한다.\n' +
      '\n' +
      '## Acknowledgment\n' +
      '\n' +
      'iOS의 대기시간 측정 환경 설정에 귀중한 공헌과 파이토치 엣지팀의 전폭적인 지원에 한송 장씨에게 감사드린다.\n' +
      '\n' +
      '## Broader Impact\n' +
      '\n' +
      '본 논문은 LLM 추론 시 에너지 소비를 줄이기 위해 온-디바이스 애플리케이션에서 수십억 이하의 대규모 언어 모델을 채택하는 것을 옹호한다. 제안된 접근법은 LLM 배치와 관련된 계산 비용을 완화하는 데 유망하다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Ainslie et al. (2023) Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. GQA: 다중 헤드 체크포인트에서 일반화된 다중 쿼리 변압기 모델을 훈련합니다. EMNLP에서 인용됨: SS1.\n' +
      '* S. Biderman, H., M. G., H. Bradley, K. O\'Brien, E., M. A., S., Prabanth, U. S., E., et al. Pythia: 훈련 및 스케일링에 걸쳐 큰 언어 모델들을 분석하기 위한 스위트. In International Conference on Machine Learning, pp. 2397-2430. Cited by: SS1.\n' +
      '*Y. 비스크 젤러스, J. 가오, Y. Choi, et al. P. P. Piqa: 자연어로 물리적 상식에 대한 추론. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34, pp. 7432-7439. Cited by: SS1.\n' +
      '* S. 검은색 바이더만, E. 할라한, Q. 안소니 가오락 골딩 H. He, C. Leahy, K McDonell, J., et al. Gpt-neox-20b: a open-source autoregressive language model. ArXiv:2204.06745. 인용: SS1.\n' +
      '* A. Chowdhery, S. 나랑, J 데블린, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: 경로를 갖는 스케일링 언어 모델링. Journal of Machine Learning Research24(240), pp. 1-113. Cited by: SS1.\n' +
      '* C. Clark, K. 이명 장태 Kwiatkowski 콜린스, K Toutanova (2019)EOLO: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구합니다. ArXiv:1905.10044. 인용: SS1.\n' +
      '* P. Clark, I. Cowhey, O. 에치오니, T. Khot, A. Sabharwal, C. Schoenick, O. 타피오르(2018) 질의응답을 해결했다고 생각하십니까? try arc, the ai2 reasoning challenge. ArXiv:1803.05457. 인용: SS1.\n' +
      '*T. 다오대복 Ermon, A. Rudra, and C. Re(2022)Flashattention: io-awareness와 함께 빠르고 기억력이 효율적인 정확한 주의력. The Advances in Neural Information Processing Systems35, pp. 16344-16359. Cited by: SS1.\n' +
      '* Y. N. Duphin, A. Fan, M. Auli, and D. Grangier (2017)Language modeling with gated convolutional networks. In International conference on machine learning, pp. 933-941. Cited by: SS1.\n' +
      '*T. 디트머스 루이스 벨카다, L. 제틀모이어(2022)LLM. int8(): 스케일에서 트랜스포머에 대한 8비트 매트릭스 곱셈. ArXiv:2208.07339. 인용: SS1.\n' +
      '*N. Dy, G. Gosal, H. Khachane, W. 마샬, R 파트리아 Tom, J. Hestness, et al. Cerebras-gpt: open compute-optimal language models trained on the cerebras wafer-scale cluster. ArXiv:2304.03208. 인용: SS1.\n' +
      '* E. Frantar and D. Alistarh (2023)SparseGPT: 대규모 언어 모델은 원샷으로 정확하게 가지치기될 수 있다. In International Conference on Machine Learning, pp. 10323-10337. Cited by: SS1.\n' +
      '* E. Frantar, S. 애쉬부스, T Hoefler, and D. Alistarh(2022)Gptq: exact post-training quantization for generative pre-trained transformer. ArXiv:2210.17323. 인용: SS1.\n' +
      '* V. 가네산, G. 라메시, W. 횡종 유영 Wei, and J. Sun (2020) single path one-shot neural architecture search with uniform sampling. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVI 16, pp. 544-560. Cited by: SS1.\n' +
      '* S. 한익 Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J. Dally(2016)EIE: efficient inference engine on compressed deep neural network. ACM SIGARCH Computer Architecture News44(3), pp. 243-254. Cited by: SS1.\n' +
      '* G. Hinton, O. Vinyals, J. Dean, et al.(2015)Distilling the knowledge in a neural network. ArXiv:1503.02531. 인용: SS1.\n' +
      '* G.\n' +
      '\n' +
      '* [14] Hristov, V. A16 bionic explained: what\'s new in apple\'s pro-grade mobile chip? [https://www.phonearena.com/news/A16-Bionic-explained-whats-new_id142438](https://www.phonearena.com/news/A16-Bionic-explained-whats-new_id142438), 2022.\n' +
      '* [15] Jawahar, G., Yang, H., Xiong, Y., Liu, Z., Wang, D., Sun, F., Li, M., Pappu, A., Oguz, B., Abdul-Mageed, M., et al. Mixture-of-supernets: Improving weight-sharing supernet training with architecture-routed mixture-of-experts. _arXiv preprint arXiv:2306.04845_, 2023.\n' +
      '* [16] Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. _arXiv preprint arXiv:1705.03551_, 2017.\n' +
      '* [17] Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* [18] Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In _International conference on machine learning_, pp. 5156-5165. PMLR, 2020.\n' +
      '* [19] Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M. W., and Keutzer, K. Squeezellm: Dense-and-sparse quantization. _arXiv preprint arXiv:2306.07629_, 2023.\n' +
      '* [20] Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.\n' +
      '* [21] Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020.\n' +
      '* [22] Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. Race: Large-scale reading comprehension dataset from examinations. _arXiv preprint arXiv:1704.04683_, 2017.\n' +
      '* [23] Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval), 2023.\n' +
      '* [24] Liu, J., Gong, R., Wei, X., Dong, Z., Cai, J., and Zhuang, B. Qllm: Accurate and efficient low-bitwidth quantization for large language models. _arXiv preprint arXiv:2310.08041_, 2023a.\n' +
      '* [25] Liu, S.-y., Liu, Z., Huang, X., Dong, P., and Cheng, K.-T. Llm-fp4: 4-bit floating-point quantized transformers. _arXiv preprint arXiv:2310.16836_, 2023b.\n' +
      '* [26] Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. Llm-qat: Data-free quantization aware training for large language models. _arXiv preprint arXiv:2305.17888_, 2023c.\n' +
      '* [27] Malladi, K. T., Lee, B. C., Nothaft, F. A., Kozyrakis, C., Periyathambi, K., and Horowitz, M. Towards energy-proportional datacenter memory with mobile dram. _ACM SIGARCH Computer Architecture News_, 40(3):37-48, 2012.\n' +
      '* [28] Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. _arXiv preprint arXiv:1809.02789_, 2018.\n' +
      '* [29] Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., et al. Rwkv: Reinventing rnns for the transformer era. _arXiv preprint arXiv:2305.13048_, 2023.\n' +
      '* [30] Reid, M., Marrese-Taylor, E., and Matsuo, Y. Subformer: Exploring weight sharing for parameter efficiency in generative transformers. _arXiv preprint arXiv:2101.00234_, 2021.\n' +
      '* [31] Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient content-based sparse attention with routing transformers. _Transactions of the Association for Computational Linguistics_, 9:53-68, 2021.\n' +
      '* [32] Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106, 2021.\n' +
      '* [33] Sap, M., Rashkin, H., Chen, D., LeBras, R., and Choi, Y. Socialiqa: Commonsense reasoning about social interactions. _arXiv preprint arXiv:1904.09728_, 2019.\n' +
      '* [34] Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A. S., Yvon, F., Galle, M., et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.\n' +
      '* [35] Shen, Z., Liu, Z., and Xing, E. Sliced recursive transformer. In _European Conference on Computer Vision_, pp. 727-744. Springer, 2022.\n' +
      '* [36] Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen, B., Liang, P., Re, C., Stoica, I., and Zhang, C. Flexgen: High-throughput generative inference of large language models with a single gpu. In _International Conference on Machine Learning_, pp. 31094-31116. PMLR, 2023.\n' +
      '* [37] Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and effective pruning approach for large language models. _arXiv preprint arXiv:2306.11695_, 2023.\n' +
      '\n' +
      '* Tan and Le(2019) Tan, M. 및 Le, Q. Efficientnet: 컨볼루션 신경망에 대한 모델 스케일링을 재싱한다. In _International conference on machine learning_, pp. 6105-6114. PMLR, 2019.\n' +
      '* Taylor 등(2022) Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R. 갈락티카: 과학을 위한 큰 언어 모델. _ arXiv preprint arXiv:2211.09085_, 2022.\n' +
      '* Timiryasov and Tastet (2023) Timiryasov, I. and Tastet, J.-L. Baby llama: Knowledge distillation from ensemble of teachers trained on a small dataset with no performance penalty. _ arXiv preprint arXiv:2308.02019_, 2023.\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. - A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al. (2021) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, Kardas, M., Kerkez, V., Khabas, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M. - A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, E. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Zu, P., Yan, Z., Zarov, I., Zambadur, M., Fan, A., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. 라마 2: 오픈 파운데이션 및 미세 조정된 채팅 모델, 2023b.\n' +
      '* Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. _ arXiv preprint arXiv:2006.04768_, 2020.\n' +
      '* Wu et al. (2019) Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y., and Keutzer, K. Fbnet: 미분 가능한 신경 아키텍처 검색을 통한 하드웨어 인식 효율적인 콘넷 설계. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10734-10742, 2019.\n' +
      '* Xia et al. (2023a) Xia, H., Zheng, Z., Li, Y., Zhuang, D., Zhou, Z., Qiu, X., Li, Y., Lin, W., and Song, S. L. Flash-llm: Enabling cost-effective and high-efficient large generative model inference with unstructured sparsity. _ arXiv preprint arXiv:2309.10285_, 2023a.\n' +
      '* Xia et al. (2023b) Xia, M., Gao, T., Zeng, Z., and Chen, D. Sheared llama: Accelerating language model pre-training via structured pruning. _ arXiv preprint arXiv:2310.06694_, 2023b.\n' +
      '* Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: 대형 언어 모델을 위한 정확하고 효율적인 훈련 후 양자화. In _International Conference on Machine Learning_, pp. 38087-38099. PMLR, 2023.\n' +
      '* Xiong et al. (2021) Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V. Nystromformer: self-attention을 근사화하는 nystrom 기반 알고리즘. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pp. 14138-14148, 2021.\n' +
      '* Xu et al. (2021) Xu, J., Tan, X., Luo, R., Song, K., Li, J., Qin, T., and Liu, T. -Y. Nas-bert: 신경 아키텍처 검색을 사용한 작업 방지 및 적응형 크기 버트 압축. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pp. 1933-1943, 2021.\n' +
      '* Yao et al. (2022) Yao, Z., Yazdani Aminabadi, R., Zhang, M., Wu, X., Li, C., and He, Y. 제로양자: 대규모 변압기를 위한 효율적이고 저렴한 훈련 후 양자화 신경 정보 처리 시스템_, 35:27168-27183, 2022에서의 발전.\n' +
      '* Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. 헬라스바그: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? _ ArXiv preprint arXiv:1905.07830_, 2019.\n' +
      '* Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. _ arXiv preprint arXiv:2205.01068_, 2022.\n' +
      '* Zheng et al. (2023) Zheng, L., Chiang, W. L., Sheng, Y., Zhu, S., Wu, Z., Zhu, Y., Lin, Z., Li, Z., Li, Z., Li, D., Xing, E., et al., Judging llm-as-a-judge with mt-bench and chatbot arena. _ arXiv preprint arXiv:2306.05685_, 2023.\n' +
      '* Zoph and Le(2016) Zoph, B. and Le, Q. V. 강화학습을 적용한 Neural architecture search. _ ArXiv:1611.01578_, 2016.\n' +
      '\n' +
      '## 부록이 각 디자인 선택에 미치는 영향\n' +
      '\n' +
      '이 섹션에서는 125M 및 350M의 모델 크기에서 10억 미만의 규모 LLM 설계_ 실험을 개선하기 위한 포괄적인 표로 작성된 결과를 제시한다. 표 8의 결과를 살펴보면, 전통적인 Feedforward Network(\\(FC\\to ReLU\\to FC\\))에서 SwiGLU로 전환하면 두 모델 크기 모두 1.3%의 정확도 향상을 얻을 수 있다. 모델 깊이를 더 증가시키면 125M/350M 모델의 정확도가 각각 0.9%/1.1% 향상된다. 그런 다음 입출력 임베딩 공유를 도입하면 약 10%의 매개변수 감소를 달성하지만 125M 모델의 경우 0.2%, 350M 모델의 경우 0.6%의 한계 정확도만 떨어진다. 또한, 섹션 C의 결과에 따라, 모델 크기를 보존하기 위해 임베딩 차원을 증가시키면서, 헤드 차원이 64와 같고, 헤드 번호가 kv 헤드 번호에 4(\\times\\)에 가까운 그룹화된 쿼리 주의를 통합한다. 이 수정은 125M/350M 모델에 대해 0.4%/0.7%의 성능 향상을 추가로 초래한다. 이러한 기술을 결합하면 MobileLLM으로 표시된 강력한 베이스라인 네트워크가 설정된다. 마지막으로, 즉시 블록별 가중치 공유 기법은 0.25조 토큰으로 훈련된 모델에 대해 1.1%의 추가 정확도 증가를 기여하여 모델 MobileLLM-LS를 생성한다. MobileLLM과 MobileLLM-LS를 포함한 최종 모델들은 1조 개의 토큰으로 훈련된다.\n' +
      '\n' +
      '## 부록 B 깊이 대 폭\n' +
      '\n' +
      '우리는 표 9의 제로 샷 추론 태스크에 대한 네트워크 깊이 대 폭 탐색 결과와 표 10의 질문 응답 및 읽기 이해 태스크에 대한 결과를 제공한다. 연구 결과는 10개 미만의 계층을 가진 얕은 아키텍처가 추론 또는 이해 태스크를 처리하는 데 실패했음을 나타낸다. 10-20 층들을 갖는 모델들은 향상된 정확도를 나타내는 반면, 더 증가된 깊이는 세 가지 작업들 모두에 걸쳐 상당한 이점들을 계속해서 제공한다. 특히, 최적 깊이는 10억 미만의 규모 모델에 대해 30개 층에 가까운 것으로 나타났다.\n' +
      '\n' +
      '## 부록 C 헤드 수와 키 값 헤드 수\n' +
      '\n' +
      '본 연구에서는 896의 임베딩 차원을 갖는 8-layer 125M 모델과 1280의 임베딩 차원을 갖는 15-layer 350M 모델, {8, 16, 32}에서 헤드 크기 스윕을 수행한다. 표 11에 표시된 결과는 헤드 치수가 64에 가까운 16개의 헤드 및 4개의 키 값 헤드를 사용하여 최상의 정확도와 메모리 트레이드오프를 산출함을 나타낸다. 이 설정은 모델 아키텍처 설계에서 지침 원리로 사용됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c c} \\hline \\hline\n' +
      '**Techniques** & **L** & **H** & **H\\({}_{\\text{KV}}\\)** & **Dim** & **\\#Params(M)** & **ARC-e** & **ARC-c** & **Bool** & **PIQA** & **SIQA** & **HellaSwag** & **OBQA** & **WinoGrande** & **Avg.** \\\\ \\hline\n' +
      '**125M** & & & & & & & & & & & & & & \\\\ \\hline Baseline model & 12 & 12 & 12 & 768 & 134.1 & 41.3 & 25.2 & 57.5 & 62.0 & 41.9 & 31.1 & 31.2 & 50.8 & 42.6 \\\\ + SwiGLU in FFN & 12 & 12 & 12 & 768 & 134.1 & 43.1 & 28.9 & 58.1 & 62.3 & 42.3 & 34.6 & 31.5 & 50.1 & 43.9 \\\\ + Use deep-thin structure & 30 & 8 & 8 & 512 & 135.0 & 43.6 & 26.1 & 58.0 & 62.5 & 42.6 & 36.5 & 37.5 & 51.5 & 44.8 \\\\ + Embedding share & 30 & 8 & 8 & 512 & 118.6 & 44.4 & 26.0 & 56.2 & 62.8 & 43.1 & 35.9 & 36.0 & 52.6 & 44.6 \\\\ + Multi-query attention & 30 & 9 & 3 & 576 & 124.6 & 45.5 & 27.7 & 58.3 & 64.6 & 41.9 & 36.4 & 35.4 & 50.4 & 45.0 \\\\ (Train on 1T token) & 30 & 9 & 3 & 576 & 124.6 & 43.9 & 27.1 & 60.2 & 65.3 & 42.4 & 38.9 & 39.5 & 53.1 & **46.3** \\\\ + Layer sharing & 30 & 9 & 3 & 576 & 124.6 & 44.4 & 27.0 & 61.5 & 65.1 & 43.0 & 37.6 & 37.8 & 52.0 & 46.1 \\\\ (Train on 1T token) & 30 & 9 & 3 & 576 & 124.6 & 45.8 & 28.7 & 60.4 & 65.7 & 42.9 & 39.5 & 41.1 & 52.1 & **47.0** \\\\ \\hline\n' +
      '**350M** & & & & & & & & & & & & & & \\\\ \\hline Baseline model & 15 & 20 & 20 & 1280 & 376.8 & 50.3 & 27.6 & 53.8 & 68.1 & 44.1 & 42.6 & 40.1 & 52.4 & 47.4 \\\\ + SwiGLU in FFN & 15 & 20 & 20 & 1280 & 386.7 & 49.2 & 30.6 & 59.1 & 67.7 & 44.3 & 43.2 & 41.0 & 54.2 & 48.7 \\\\ + Use deep-thin structure & 32 & 14 & 14 & 896 & 380.3 & 50.7 & 31.4 & 59.4 & 67.8 & 43.3 & 46.2 & 43.8 & 56.2 & 49.8 \\\\ + Embedding share & 32 & 14 & 14 & 896 & 351.6 & 49.9 & 32.0 & 60.3 & 67.9 & 43.2 & 47.0 & 38.9 & 54.8 & 49.2 \\\\ + Multi-query attention & 32 & 15 & 5 & 966 & 345.3 & 51.4 & 31.3 & 61.0 & 68.1 & 43.6 & 47.2 & 41.6 & 55.4 & 49.9 \\\\ (Train on 1T token) & 32 & 15 & 5 & 960 & 345.3 & 53.8 & 33.5 & 62.4 & 68.6 & 44.7 & 49.6 & 40.0 & 57.6 & **51.3** \\\\ + Layer sharing & 32 & 15 & 5 & 960 & 345.3 & 51.9 & 35.2 & 59.6 & 68.9 & 43.4 & 47.2 & 43.3 & 58.4 & 51.0 \\\\ (Train on 1T token) & 32 & 15 & 5 & 960 & 345.3 & 54.4 & 32.5 & 62.8 & 69.8 & 44.1 & 50.6 & 45.8 & 57.2 & **52.1** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 각 설계 선택이 모델 정확도에 미치는 영향이 제로샷 상식 추론 과제에 미치는 영향에 대한 절제 연구. 도 3의 막대 차트에 대응한다. 여기서, L, H, H\\({}_{\\text{KV}}\\)는 각각 층수, 헤드, kv-헤드를 나타내고, 딤은 임베딩 차원을 나타낸다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '## 부록 F 지식 증류\n' +
      '\n' +
      '지식 증류(KD)(Hinton et al., 2015)를 소규모 모델 사전 훈련에 통합한 결과는 표 14에 제시되어 있다. LLaMA-v2 7B 모델은 교사 역할을 하고, KD 손실은 대규모 사전 훈련된 교사 모델(_i.e._, LLaMA-v2 7B)로부터의 로짓과 소규모 학생 네트워크(_i.e._, 125M 또는 350M 모델) 사이의 교차 엔트로피(cross-entropy)를 사용하여 계산된다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{CE}=-\\frac{1}{n}\\sum_{c}\\sum_{i=1}^{n}p_{c}^{\\mathcal{T}(X_{i}}) \\log(p_{c}^{\\mathcal{S}(X_{i})), \\tag{1}\\\n' +
      '\n' +
      '여기서 \\(i\\)은 현재 배치에서 \\(i^{th}\\)의 표본과 배치에서 \\(n\\)의 총 표본을 나타내고 \\(c\\)은 클래스 수를 나타내며, 이는 우리의 경우 어휘의 크기와 같다. \\(i^{th}\\) (\\mathcal{T}\\)와 \\(\\mathcal{S}\\)는 각각 교사와 학생 네트워크이다.\n' +
      '\n' +
      '표 14의 결과는 KD 손실을 추가하는 것이 다음 토큰을 라벨로 사용하는 것보다 비슷하거나 심지어 더 낮다는 것을 나타낸다. 그러나 KD를 이용한 훈련 시간이 레이블을 이용한 처음부터 훈련하는 것보다 \\(2.6-3.2\\times\\) 더 느리다는 점은 주목할 만하다. 모든 모델은 120k 반복에 대해 배치 크기가 32인 32개의 A100 80G GPU에 대해 훈련된다. 결과적으로 우리는 실험에 라벨을 사용하기로 결정했다.\n' +
      '\n' +
      '## 부록 G 데이터세트 및 벤치마크\n' +
      '\n' +
      '모바일LLM은 제로샷 상식 추론(BoolQ, PIQA, SIQA, HellaSwag, Winogrande, ARC, OBQA), 질문 응답(TriviaQA), 읽기 이해(RACE) 과제에 대해 평가된다. 또한 MT-벤치와 AlpacaEval 벤치마크에서 채팅 모델을 평가한다. 또한 모델을 미세 조정하고 평가하기 위해 API 호출 데이터 세트를 생성했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **\\#Heads \\#KV-Heads** & **\\#Params(M)** & **ARC-e** & **ARC-c** & **BoolQ** & **PIQA** & **SIQA** & **HellaSwag** & **OBQA** & **WinoGrande** & **Avg.** \\\\ \\hline \\multirow{9}{*}{125M} & 32 & 32 & 138.1 & 42.1 & 26.9 & 58.4 & 62.2 & 42.1 & 33.8 & 36.7 & 52.4 & 44.3 \\\\  & 32 & 16 & 131.7 & 41.2 & 26.4 & 57.7 & 62.5 & 42.3 & 33.3 & 34.2 & 52.9 & 43.8 \\\\  & 32 & 8 & 128.5 & 42.6 & 27.3 & 61.1 & 61.9 & 41.9 & 32.2 & 35.0 & 52.0 & 44.2 \\\\  & 32 & 4 & 126.8 & 43.1 & 26.8 & 59.8 & 62.7 & 41.4 & 32.5 & 34.4 & 51.1 & 44.0 \\\\  & 32 & 2 & 126.0 & 39.8 & 26.7 & 59.4 & 59.4 & 42.0 & 31.3 & 32.6 & 52.9 & 43.0 \\\\  & 32 & 1 & 125.6 & 41.0 & 24.3 & 59.1 & 60.8 & 41.2 & 31.4 & 35.4 & 52.0 & 43.1 \\\\\n' +
      '**125M** & 16 & 16 & 138.1 & 41.6 & 25.7 & 61.1 & 62.4 & 43.1 & 34.4 & 36.9 & 51.6 & **44.6** \\\\ \\# layers=8 & 16 & 8 & 131.7 & 42.4 & 26.4 & 60.7 & 63.4 & 41.9 & 33.5 & 34.7 & 51.5 & 44.3 \\\\ dim=896 & 16 & 4 & 128.5 & 42.5 & 25.6 & 62.3 & 62.4 & 41.8 & 33.0 & 35.9 & 54.5 & **44.7** \\\\  & 16 & 1 & 126.8 & 41.7 & 25.3 & 56.9 & 61.7 & 42.0 & 32.9 & 32.6 & 54.5 & 43.5 \\\\  & 16 & 1 & 126.0 & 40.4 & 26.3 & 61.8 & 63.2 & 41.7 & 32.0 & 34.0 & 50.4 & 43.7 \\\\  & 8 & 8 & 138.1 & 41.4 & 25.0 & 58.3 & 61.7 & 41.7 & 33.3 & 35.9 & 53.2 & 43.8 \\\\  & 8 & 4 & 131.7 & 43.3 & 28.2 & 58.3 & 61.8 & 42.8 & 33.8 & 30.9 & 53.0 & 44.0 \\\\  & 8 & 2 & 128.5 & 40.7 & 26.2 & 58.1 & 62.4 & 41.6 & 32.8 & 34.8 & 51.5 & 43.4 \\\\  & 8 & 1 & 126.8 & 42.5 & 24.8 & 59.4 & 62.3 & 42.0 & 32.0 & 36.3 & 51.3 & 43.8 \\\\ \\hline \\multirow{9}{*}{350M} & 32 & 32 & 386.7 & 48.6 & 30.4 & 59.7 & 67.2 & 43.9 & 44.0 & 40.9 & 53.9 & 48.6 \\\\  & 32 & 16 & 362.1 & 48.9 & 31.6 & 57.6 & 68.4 & 43.4 & 43.8 & 38.6 & 54.9 & 48.4 \\\\  & 32 & 8 & 349.8 & 48.3 & 33.1 & 61.0 & 67.2 & 42.6 & 42.1 & 39.0 & 53.9 & 48.4 \\\\  & 32 & 4 & 343.7 & 47.2 & 29.8 & 59.4 & 67.2 & 43.5 & 42.5 & 42.5 & 54.1 & 48.3 \\\\  & 32 & 2 & 340.6 & 47.6 & 30.3 & 62.4 & 69.9 & 42.6 & 41.6 & 38.6 & 52.0 & 47.7 \\\\  & 32 & 1 & 339.0 & 48.5 & 27.3 & 56.3 & 67.1 & 42.9 & 40.9 & 36.7 & 53.3 & 46.6 \\\\\n' +
      '**350M** & 16 & 386.7 & 50.6 & 68.6 & 43.5 & 45.1 & 43.5 & 45.1 & 362.1 & 48.5 & 30.7 & 59.4 & 67.3 & 43.8 & 43.8 & 43.3 & 43.8 & 41.3 & 53.3 & 4 & 349.8 & 49.9 & 30.5 & 60.5 & 41.8 & 44.2 & 44.5 & 44.3 & 44.3 & 44.3 & 44.3 & 44.3 & 44.3 & 44.3 & 42.4 & 47.5 & 34.3 & 44.3 & 44.3 & 44.3 & 42.3 & 47.4 & 47.5 & 40.3 & 34.3 & 58.7 & 42.5 & 47.9 & 8\n' +
      '\n' +
      '# Zero-shot Common Sense 추론 과제\n' +
      '\n' +
      '**BoolQ**Clark 등(2019)은 자연 발생 예/아니오 질문에 초점을 맞춘 읽기 이해 데이터세트이다. 각 인스턴스는 질문(Q), 구절(P)에서 발췌한 부분 및 답변(A)을 포함하며, 향상된 명확성을 위한 설명이 추가된다.\n' +
      '\n' +
      'Physical Interaction: Question Answering으로 약칭된**PIQA**Bisk 등(2020)은 물리적 상식 이해를 이해하는데 있어서 자연어 모델의 능력을 평가하고 연구하기 위한 벤치마크 역할을 한다.\n' +
      '\n' +
      'Social Interaction Question Answering으로 약칭되는**SIQA**Sap 등(2019)은 객관식 질문 답변을 통해 계산 모델의 사회적 감성 지능을 측정할 수 있도록 설계되어 있다.\n' +
      '\n' +
      '**HellaSwag**Zellers et al.(2019)은 물리적으로 위치한 상식적인 자연어 추론을 위한 벤치마크 역할을 한다. 그것은 인간에게는 사소한 것으로 간주되지만(>95% 정확도) 언어 모델에 대한 도전을 제기하는 4원 객관식 문제로 구성된다.\n' +
      '\n' +
      '**WinoGrande**Sakaguchi 등(2021)은 상식 추론의 벤치마크이다. 그것은 273개의 전문가 조작 대명사 해결 문제 세트로 구성되며, 선택 선호 또는 단어 연관성에 의존하는 통계 모델에 대해 해결할 수 없도록 의도적으로 설계되었다.\n' +
      '\n' +
      '**ARC**Clark 등(2018), AI2 Reasoning Challenge는 7787개의 자연과학 문제를 정리한 것이다. 그것은 Challenge Set과 Easy Set으로 나뉘며 Challenge Set은 검색 기반 알고리즘과 단어 동시 발생 알고리즘 모두에 의해 잘못 대답된 질문을 독점적으로 포함한다.\n' +
      '\n' +
      '**OBQA**Mihaylov et al.(2018)은 오픈북 질의응답을 위해 설계된 대략 6000개의 질문으로 구성된 데이터셋이다. 이 작업은 제공된 과학 사실(공개된 책)의 말뭉치를 외부의 광범위한 상식과 통합하는 것을 포함한다. 정답을 제공하는 것은 제공된 핵심 사실을 넘어 단순한 상식을 활용하는 것을 필요로 한다.\n' +
      '\n' +
      '질문 응답 작업\n' +
      '\n' +
      '**TriviaQA**Joshi et al.(2017)은 비공개북 질의응답 벤치마크이다. 그것은 트리비아 마니아들이 저술한 95,000개의 질문-답변 쌍을 결합하여 파생된 65만 개 이상의 질문-답변 증거 트리플을 포함한다. 각 질문은 평균 6개의 증거 문서에 의해 뒷받침된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l l l l l} \\hline \\hline\n' +
      '**Model** & \\multicolumn{2}{c}{**Precision**} & \\multicolumn{1}{c}{**ARC-e**} & \\multicolumn{1}{c}{**ARC-c**} & \\multicolumn{1}{c}{**BoolQ**} & \\multicolumn{1}{c}{**PIQA**} & \\multicolumn{1}{c}{**SIQA**} & \\multicolumn{1}{c}{**HellaSwag**} & \\multicolumn{1}{c}{**OBQA**} & \\multicolumn{1}{c}{**WinoGrande**} & \\multicolumn{1}{c}{**Avg.**} & \\multicolumn{1}{c}{**Gap**} \\\\ \\hline MobileLLM-125M & BF16 & 45.5 & 27.7 & 58.3 & 64.6 & 41.9 & 36.4 & 35.4 & 50.4 & 45.0 & 4 & 5.0 \\\\ MobileLLM-125M & W8A8 & 45.2 & 27.1 & 58.3 & 65.0 & 41.7 & 36.2 & 33.6 & 51.0 & 44.8 & 0.2 \\\\ \\hline MobileLLM-LS-125M & BF16 & 44.4 & 27.0 & 61.5 & 65.1 & 43.0 & 37.6 & 37.8 & 52.0 & 46.1 & – \\\\ MobileLLM-LS-125M & W8A8 & 44.0 & 27.5 & 60.9 & 64.6 & 43.1 & 37.7 & 37.7 & 51.0 & 45.8 & 0.3 \\\\ \\hline MobileLLM-350M & BF16 & 51.4 & 31.3 & 61.0 & 68.1 & 43.6 & 47.2 & 41.6 & 55.4 & 49.9 & – \\\\ MobileLLM-350M & W8A8 & 51.4 & 32.1 & 61.1 & 68.8 & 43.1 & 47.1 & 40.6 & 55.1 & 49.9 & 0.0 \\\\ \\hline MobileLLM-LS-350M & BF16 & 51.9 & 35.2 & 59.6 & 68.9 & 43.4 & 47.2 & 43.3 & 58.4 & 51.0 & – \\\\ MobileLLM-LS-350M & W8A8 & 51.3 & 33.8 & 59.5 & 69.1 & 43.7 & 47.2 & 43.0 & 57.0 & 50.6 & 0.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: 절제 연구: 제로 샷 상식 추론 작업에 대한 8비트 가중치, 8비트 활성화 후 양자화 결과. 양자화된 모델은 완전 정밀 BF16 대응물과 비교하여 0.5% 이내의 정확도 격차를 달성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l l l l l} \\hline \\hline\n' +
      '**Model** & \\multicolumn{2}{c}{**Training Loss**} & \\multicolumn{1}{c}{**Training Time**} & \\multicolumn{1}{c}{**ARC-e**} & \\multicolumn{1}{c}{**ARC-c**} & \\multicolumn{1}{c}{**BoolQ**} & \\multicolumn{1}{c}{**PIQA**} & \\multicolumn{1}{c}{**SIQA**} & \\multicolumn{1}{c}{**HellaSwag**} & \\multicolumn{1}{c}{**OBQA**} & \\multicolumn{1}{c}{**WinoGrande**} & \\multicolumn{1}{c}{**Avg.**} \\\\ \\hline\n' +
      '125M 모델 & 라벨 & 29h & 43.1 & 28.9 & 58.1 & 62.3 & 42.3 & 34.6 & 31.5 & 50.1 & 43.9\\\\\n' +
      '125M model & Label + KD & 93h & 41.8 & 28.5 & 58.5 & 61.6 & 41.1 & 34.5 & 32.7 & 51.6 & 43.8 \\\\ \\hline\n' +
      '350M 모델 & 라벨 & 42h & 50.2 & 31.8 & 56.9 & 67.7 & 44.3 & 45.8 & 40.8 & 55.5 & 49.1\\\\\n' +
      '350M model & Label + KD & 109h & 48.7 & 31.8 & 60.7 & 67.4 & 43.2 & 45.9 & 38.9 & 53.7 & 48.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 14: 지식 증류(KD)를 위한 소프트 라벨로서 LLaMA-v2 7B 교사의 출력을 사용하는 것에 대한 절제 연구. 결과는 하드 라벨만 사용하는 것과 비교하여 KD 손실을 통합할 때 성능이 약간 저하되었음을 나타낸다.\n' +
      '\n' +
      '읽기 이해 과제\n' +
      '\n' +
      '**RACE**[17]은 28,000개 이상의 구절과 10만 개에 가까운 문항의 대규모 읽기 이해 데이터셋이다. 이 데이터 세트는 중학생과 고등학생을 위해 설계된 중국의 영어 시험에서 수집된다.\n' +
      '\n' +
      '### Chat Benchmarks\n' +
      '\n' +
      '**AlpacaEval**[17]은 다양한 주제에 걸쳐 805개의 질문을 특징으로 하는 단일 턴 벤치마크이다. 이 논문의 평가에서 모델은 응답을 생성하고 GPT-4에 의해 점수가 매겨지며 최종 메트릭은 기본 모델(텍스트 다빈치-001)에 대한 쌍별 승률이다.\n' +
      '\n' +
      '**MT-Bench**[21]은 8개의 다양한 지식 영역에 걸쳐 있는 160개의 질문으로 구성된 다중 턴 벤치마크이다. 이 평가에서, 모델들은 초기 질문에 응답하고 후속적으로 미리 정의된 후속 질문에 대한 제2 응답을 제공하는 작업을 수행한다. 각 모델 반응은 1에서 10까지의 척도로 GPT-4로 평가되며 최종 점수는 두 차례에 걸친 평균에 의해 결정된다.\n' +
      '\n' +
      'API Calling Dataset\n' +
      '\n' +
      'API 호출 데이터세트는 인간과 에이전트 사이의 대화를 시뮬레이션하도록 언어 모델에 명령함으로써 생성된다. 이 프로세스는 언어 모델에 시뮬레이트된 대화 전반에 걸쳐 지정된 API들을 사용하여 수행되어야 하는 태스크들을 명시적으로 언급하도록 지시한다. 생성된 데이터 세트에서 대화 전환의 예는 다음 상자에 나와 있다.\n' +
      '\n' +
      '[size=title=!, topsep=0pt]{"role": "USER", "message": "Do you help me send a SMS message: We will see you"}, {"role": "PARSER", "message": "style_transfer(text="We are expect to see you", style="sms"", contact="None"}, {"role": "AGENT", "message": "Sure! Here\'s possible SMS message: Hey[name], We\'re expected you all well!\n' +
      '\n' +
      '[size=title=!, topsep=0pt]{"role": "USER", "message": "How did the S&P 500 was last month?"}, {"role": "PARSER", "message": "get_stock_information(company_name="S&P 500", ticker_symbol="GSPC", date_time="last month", financial_metrics="price")}, {"role": "AGENT", "message": "The S&P 500 closed with $2,977.94 with monthly return of 2.5%},\n' +
      '\n' +
      '[size=title=!, topsep=0pt]{"role": "USER", "message": "The San Diego Zoo during the winter months"), "PARSER", "message": "get_local_information(point_of_interest="San Diego Zoo", attribute="hours of operation")}, {"role": "AGENT", "message": "The San Diego Zoo is open one year-round, but the hours of operation": "The San Diego Zoo is open of year-month}, {"role": "USER", "message": "What is the most recent news concerning the aliens?"}, {"role": "USER", "message": "What is a latest news concerning the aliens?"}, {"message": "get_local_information(point_of_interest="San Diego Zoo", attribute="hours of operation")}, {"role": "AGENT", "message": "The San Diego Zoo is open for year-time, but the hours of operation": "The\n' +
      '\n' +
      '{"role": "PARSER", "message": "news(query="latest alien news")},\n' +
      '\n' +
      '"역할": "AGENT", "메시지": "최근 뉴스 기사에 따르면, NASA는 외계 생명체를 지탱할 가능성이 있는 태양계 밖의 새로운 행성의 발견을 발표했다. 케플러-186f라는 행성은 별의 거주 가능한 구역에서 발견된 최초의 알려진 지구 크기의 행성이다. 이 뉴스는 외계인의 존재를 반드시 증명하는 것은 아니지만, 지구 너머의 생명체를 찾을 가능성에 대한 희망을 제공한다. 또한, 네이처에 발표된 최근 연구는 관측 가능한 우주에 수백억 개의 지능적인 문명이 있을 수 있다는 추정과 함께 우주가 외계 문명으로 가득 차 있을 수 있다는 추정과 함께, 이러한 발견들이 과학계에서 여전히 연구 및 논의되고 있다는 것에 주목하는 것이 중요하다."\n' +
      '\n' +
      '{"role": "USER", "message": "누가 마지막 슈퍼볼에서 이겼나요?"}\n' +
      '\n' +
      '{"role": "PARSER", "message": "get_sports_information(match="", sport_type="NFL", metric="winner")},\n' +
      '\n' +
      '{"role": "AGENT", "message": "The Last Super Bowl was win the Kansas City Chiefs,}\n' +
      '\n' +
      '## 부록 H 예상 미래 GPU 소비량 근사 계산\n' +
      '\n' +
      '#GPU = \\(7.88\\times 10^{9}\\)(개체수 크기) \\(\\times 5\\%\\)(개체 시간9) \\(\\times\\)\\(220\\times 10^{9}\\)(한 전문가가 활성화된 220B GPT-4 모델에 대한 근사 FLOPs/token) \\(\\times 50\\)(token/s) \\(\\times 24\\)(h) \\(\\times 3600\\)(s)/(\\(60\\times 10^{12}\\)(FLOPs/s, H100 계산 용량) \\(\\times 24\\)(h) \\(\\times 3600\\)(s)) \\(\\approx 1\\times 10^{8}\\)\n' +
      '\n' +
      '각주 9: 유튜브 사용자들이 매달 평균 23시간을 소비한다는 통계(출처)와 인스타그램 사용자들이 매달 12시간을 앱(출처)에 헌납한다는 통계에 따르면, 우리는 LLM과의 상호작용이 향후 이러한 플랫폼의 누적 참여를 쉽게 능가할 수 있다고 믿을 만한 이유가 있다.\n' +
      '\n' +
      '이는 계산이 선형 계층에 의해 지배되는 짧은 시퀀스 길이(_i.e._ 수만 개의 토큰이 아닌 수백 개의 토큰)를 가정한다. 매우 긴 시퀀스의 경우 자기 주의 계산이 우세할 것이며, 이는 더 복잡한 공식을 필요로 할 것이다.\n' +
      '\n' +
      '## 부록 I Chat Generation from MobileLLM\n' +
      '\n' +
      '다양한 질문에 응답하면서 모바일LLM으로 생성된 채팅 출력의 몇 가지 예를 보여준다.\n' +
      '\n' +
      '**Input:**\n' +
      '\n' +
      '자세 개선이나 체력 향상과 같은 유념하는 보행에 특별한 신체적 이점이 있나요?\n' +
      '\n' +
      '**Output** (MobileLLM-125M):\n' +
      '\n' +
      '네, 유념하는 보행에는 몇 가지 신체적 이점이 있습니다. 여기 몇 가지가 있습니다.\n' +
      '\n' +
      '1. 개선된 자세: 마음 깊은 보행은 좋은 정렬을 촉진하고 관절에 가해지는 스트레스를 줄여 자세를 개선하는 데 도움을 줄 수 있다. 이것은 더 나은 자세와 허리 통증을 감소시킬 수 있다.\n' +
      '\n' +
      '2. 유연성 증가: 마음가짐 보행은 또한 신체의 유연성을 증가시키는 데 도움이 될 수 있으며, 이는 부상의 위험을 줄이고 전반적인 건강을 향상시킬 수 있다.\n' +
      '\n' +
      '3. 더 나은 수면: 마음 깊은 걷기는 여러분이 더 쉽게 잠들게 하고 더 오래 잠들게 할 수 있으며, 이것은 여러분의 수면의 질을 향상시킬 수 있습니다.\n' +
      '\n' +
      '4. 스트레스 감소: 마음 깊은 보행은 이완을 촉진하고 신체의 긴장을 감소시킴으로써 스트레스 수준을 감소시키는데 도움을 줄 수 있다.\n' +
      '\n' +
      '5. 정신건강증진: 마음가짐걷기는 우울과 불안 증상을 감소시키는 등 정신건강에 긍정적인 영향을 미치는 것으로 나타났다.\n' +
      '\n' +
      '전반적으로, 주의 깊게 걷는 것은 여러분의 신체적 건강과 웰빙을 향상시키는 훌륭한 방법입니다. 이동하고 제공하는 혜택을 누릴 수 있는 간단하고 효과적인 방법입니다. 유념하는 걷기를 시도하는 데 관심이 있다면 더 짧은 걷기로 시작하여 시간이 지남에 따라 점차적으로 지속 시간을 늘리는 것이 좋습니다. 또한 경험을 더욱 향상시키기 위해 마음챙김을 일상에 통합하는 것을 고려하고 싶을 수 있습니다.\n' +
      '\n' +
      '결론적으로, 주의 깊게 걷는 것은 여러분의 신체적 건강과 웰빙을 향상시키는 좋은 방법입니다. 수많은 물리적 이점으로 인해 재미 있고 지속 가능한 이동 방법을 찾는 사람들 사이에서 점점 더 인기를 얻고 있는 이유를 쉽게 알 수 있습니다. 그러니 한 번 시도해 보세요!\n' +
      '\n' +
      '**Output** (MobileLLM-350M):\n' +
      '\n' +
      '네, 마음가짐 걷기에는 몇 가지 신체적 이점이 있습니다. 첫째, 근육과 관절의 긴장량을 줄여 자세를 개선할 수 있습니다. 둘째, 심혈관 건강을 개선하고 코어 근육을 강화하며 유연성을 증가시켜 체력을 높일 수 있습니다. 또한, 유념하는 보행은 여러분이 현재의 순간에 더 집중하도록 도울 수 있으며, 이는 더 나은 수면의 질과 감소된 스트레스 수준으로 이어질 수 있다. 마지막으로, 그것은 또한 여러분이 전반적인 웰빙을 향상시킬 수 있는 주변 세계에 대한 더 큰 인식 및 감상을 개발하는 데 도움이 될 수 있습니다.\n' +
      '\n' +
      '전반적으로 마음가짐 걷기는 신체 건강과 웰빙을 향상시키는 동시에 휴식과 마음챙김을 촉진하는 좋은 방법입니다. 스트레스를 줄이고 자세를 개선하거나 단순히 평화로운 산책을 즐기려는 경우에도 마음 깊은 산책은 보람 있고 만족스러운 경험이 될 수 있습니다.\n' +
      '\n' +
      '유념하는 걷기를 시도하는 데 관심이 있다면, 그 과정을 안내할 수 있는 많은 자원이 있습니다. 일부 인기 있는 옵션에는 안내된 명상, 유념하는 걷기 운동을 제공하는 앱, 유념하는 걷기 연습 방법을 알려주는 수업이 포함됩니다. 약간의 노력과 헌신으로 누구나 마음 깊은 걸음걸이를 일상 생활에 통합하기 시작하고 제공되는 많은 이점을 얻을 수 있습니다.\n' +
      '\n' +
      '결론적으로 유념 보행은 신체 건강 증진, 스트레스 감소, 주변 세계에 대한 더 깊은 인식 및 감사 의식을 함양하는 데 도움이 될 수 있는 간단하면서도 강력한 도구이다. 그렇다면 어떤 혜택을 얻을 수 있는지 한 번 시도해 보는 것은 어떨까요? 여러분의 몸과 마음이 감사할 것입니다!\n' +
      '\n' +
      '**Output** (llama-v2-7b-chat):\n' +
      '\n' +
      '마음가짐 보행은 다음을 포함하는 다양한 신체적 이점을 가질 수 있다:\n' +
      '\n' +
      '1. 자세 개선: 각 스텝의 감각과 신체의 움직임에 집중함으로써 유심 보행이 자세 개선에 도움을 주고, 구부리거나 구부리는 것을 줄일 수 있다.\n' +
      '\n' +
      '2. 체력 증진: 마음가짐 보행은 심혈관계 체력 증진, 근력 및 지구력 증진, 전반적인 체력 증진에 도움을 줄 수 있다.\n' +
      '\n' +
      '3. 균형과 협응력 향상: 공간에서의 신체의 위치와 다리와 발의 움직임에 주의를 기울임으로써 유념한 보행이 균형과 협응력 향상에 도움을 줄 수 있다.\n' +
      '\n' +
      '4. 스트레스 및 긴장 감소: 마음 깊은 보행은 신체의 스트레스 및 긴장을 감소시키는데 도움을 줄 수 있으며, 이는 신체 이완 및 웰빙을 향상시킬 수 있다.\n' +
      '\n' +
      '5. 유연성 향상: 신체의 움직임과 각 스텝의 감각에 집중함으로써 유심 보행이 유연성 및 운동 범위 향상에 도움을 줄 수 있다.\n' +
      '\n' +
      '6. 강화된 면역 체계: 연구에 따르면 유념 보행을 포함한 마음챙김 관행은 면역 체계를 강화하고 염증을 줄이는 데 도움이 될 수 있다.\n' +
      '\n' +
      '7. 수면 개선: 마음가짐 걷기는 스트레스 감소 및 이완 촉진으로 수면의 질과 지속시간 향상에 도움을 줄 수 있다. 8. 통증 감소: 마음가짐 걷기는 통증 자체보다는 각 단계의 감각과 몸의 움직임에 집중하여 통증 감소에 도움을 줄 수 있다.\n' +
      '\n' +
      '9. 기분개선: 마음가짐걷기는 이완을 촉진하고 스트레스를 감소시킴으로써 기분개선과 우울과 불안의 증상 감소에 도움을 줄 수 있다.\n' +
      '\n' +
      '유념하는 걷기의 신체적 이점은 개인과 연습의 빈도와 기간에 따라 달라질 수 있다는 점에 유의하는 것이 중요하다. 그러나 규칙적인 연습을 통해 유념하는 보행은 신체 건강과 웰빙에 다양한 긍정적인 영향을 미칠 수 있다.\n' +
      '\n' +
      '**Input:**\n' +
      '\n' +
      '선물은 어떻게 깔끔하게 포장하나요?\n' +
      '\n' +
      '**Output** (MobileLLM-125M):\n' +
      '\n' +
      '1. 적당한 크기의 선물을 선택하라: 당신이 주고 싶은 선물의 크기는 당신의 예산과 개인적인 선호도에 따라 달라질 것이다. 작은 선물을 찾고 있다면 일회용 비닐봉지로 작은 상자나 카드를 사는 것을 고려해 보세요.\n' +
      '\n' +
      '2. 현재를 준비한다: 현재에 있는 모든 물품이 깨끗하고 정돈되어 있는지 확인한다. 클리닝 키트나 티슈 페이퍼를 사용하여 현재 먼지나 이물질을 제거할 수 있습니다.\n' +
      '\n' +
      '3. 선물 정렬: 현재를 보고 만지기 쉬운 곳에 배치한다. 연필로 현재를 중심으로 선을 긋고 당신에게 이치에 맞는 방법으로 배열한다.\n' +
      '\n' +
      '4. 장식 추가: 스티커, 리본 또는 기타 장식 아이템으로 현재를 장식합니다. 이것은 현재에 약간의 개성을 더하고 시각적으로 더 매력적으로 만들 것입니다.\n' +
      '\n' +
      '5. 현재를 즐기다: 시간을 내어 현재를 즐기고 자신이 현재가 주는 따뜻함과 편안함을 느낄 수 있도록 한다. 자신에게 미소를 지으며 현재를 감상하는 시간을 가져보세요.\n' +
      '\n' +
      '기억해, 가장 중요한 것은 현재를 즐기고 즐기는 거야! 선물 잘 받으세요!\n' +
      '\n' +
      '모험을 느끼고 있다면, 귀엽고 다채로운 선물 바구니에 선물을 포장하여 특별한 사람을 위한 깜짝 선물을 만들어 보세요. 혹은, 모험심이 느껴진다면, 숙제에 약간의 도움이 필요한 친구를 위해 선물을 만들어 보세요. 여러분이 무엇을 선택하든, 창의적이고 여러분의 현재와 함께 즐거운 시간을 보내는 것을 기억하세요!\n' +
      '\n' +
      '이러한 팁을 통해 앞으로 몇 년 동안 지속될 아름답고 사려 깊은 선물을 만들 수 있습니다. 선물 잘 받으세요!\n' +
      '\n' +
      '**Output** (MobileLLM-350M):\n' +
      '\n' +
      '1. 장식용 선물용 랩 또는 리본에 선물을 감싸는 것으로 시작한다.\n' +
      '\n' +
      '2. 선물용 랩 또는 리본을 현재의 표면에 고정시키기 위해 소량의 접착제 또는 테이프를 사용한다.\n' +
      '\n' +
      '3. 선물용 랩이나 리본을 현재 주위에 놓아 중심을 잡고 고르게 분포되도록 한다.\n' +
      '\n' +
      '4. 선물용 랩 또는 리본을 현재의 표면에 고정시키기 위해 소량의 접착제 또는 테이프를 사용한다.\n' +
      '\n' +
      '5. 선물용 랩 또는 리본이 단단히 부착되면, 현재에 임의의 추가 장식 또는 장식을 추가할 수 있다.\n' +
      '\n' +
      '6. 개인 터치를 추가하고자 하는 경우에는 선물용 랩이나 리본에 메시지나 특별한 메시지를 작성할 수 있다.\n' +
      '\n' +
      '7. 마지막으로 아름답게 포장된 선물을 즐겨보세요!\n' +
      '\n' +
      '선물을 포장하는 것은 사랑하는 사람에게 얼마나 신경을 쓰는지 보여주는 재미있고 창의적인 방법이라는 것을 기억하세요. 즐거운 시간 보내시고 그 과정을 즐기세요!\n' +
      '\n' +
      '**Output** (llama-v2-7b-chat):\n' +
      '\n' +
      '선물을 깔끔하게 포장하는 것은 약간의 도전이 될 수 있지만 몇 가지 간단한 팁과 요령으로 수령인이 좋아할 아름답게 포장된 선물을 만들 수 있습니다. 선물을 깔끔하게 포장하는 데 도움이 되는 몇 가지 팁이 있습니다:\n' +
      '\n' +
      '1. 올바른 포장지를 사용: 선물을 보완하고 현재에 알맞은 크기의 포장지를 선택한다. 또한 취향에 따라 무늬 종이 또는 단색을 사용할 수 있습니다.\n' +
      '\n' +
      '2. 선물 측정: 포장을 시작하기 전에, 충분한 포장지가 있는지 확인하기 위해 선물을 측정한다. 또한 스크랩지를 사용하여 선물을 측정하고 가장자리를 연필로 표시할 수 있습니다.\n' +
      '\n' +
      '3. 평평한 표면을 사용: 테이블이나 조리대와 같이 작업할 평평한 표면이 있는지 확인합니다. 이렇게 하면 포장지를 평평하게 유지하고 구겨지는 것을 방지할 수 있습니다.\n' +
      '\n' +
      '4. 가장자리부터 시작: 선물 위에 포장지의 가장자리를 접어서, 평평하고 매끄러운지 확인하세요. 테이프를 사용하여 필요한 경우 가장자리를 고정할 수 있습니다.\n' +
      '\n' +
      '5. 선물을 굴려라: 모서리가 제자리에 놓이면, 선물을 포장지의 중앙으로 굴려라. 다음 단계로 넘어가기 전에 선물이 제자리에 안전하게 있는지 확인하십시오.\n' +
      '\n' +
      '6. 가장자리를 끼우세요: 선물을 포장할 때, 포장지의 가장자리를 선물 아래에 끼우고, 갈 때 매끄럽게 합니다. 가장자리가 균일하고 매끄러운지 확인하고 필요한 경우 테이프를 사용하여 고정하십시오.\n' +
      '\n' +
      '7. 리본 또는 활을 추가: 선물을 포장한 후, 상단에 리본 또는 활을 추가한다. 이렇게 하면 선물이 더 축제처럼 보일 뿐만 아니라 포장지를 제자리에 보관하는 데 도움이 됩니다.\n' +
      '\n' +
      '8. 연습이 완벽을 만든다: 선물을 깔끔하게 포장하는 것은 약간의 연습이 필요하므로, 처음에 완벽하게 나오지 않으면 낙담하지 마세요. 계속 연습하면 금방 요령을 터득할 수 있을 거야!\n' +
      '\n' +
      '이 팁을 따라 수령인이 좋아할 아름답게 포장된 선물을 만들 수 있습니다. 즐거운 포장!\n' +
      '\n' +
      '**Input:**\n' +
      '\n' +
      '초보자에게 가장 좋은 운동은 무엇인가요?\n' +
      '\n' +
      '**Output** (MobileLLM-125M):\n' +
      '\n' +
      '1. 푸시업: 언제 어디서나 할 수 있는 클래식한 운동입니다. 하기 쉽고 장비가 필요하지 않습니다.\n' +
      '\n' +
      '2. 덤벨 행: 이 행은 다리와 코어에 힘을 키우는 데 좋습니다.\n' +
      '\n' +
      '3. 스쿼크: 스쿼크는 다리와 코어에 힘을 키우는 좋은 방법입니다.\n' +
      '\n' +
      '4. 폐: 폐는 균형과 안정성을 향상시키는 훌륭한 방법이다.\n' +
      '\n' +
      '5. 플랭크: 플랭크는 등과 코어에 힘을 기르는 좋은 방법입니다.\n' +
      '\n' +
      '6. 점핑 잭: 점핑 잭은 여러분의 다리와 코어에 힘을 기르는 재미있고 효과적인 방법입니다.\n' +
      '\n' +
      '7. 버피: 버피는 여러분의 다리와 코어에 힘을 기르는 좋은 방법입니다.\n' +
      '\n' +
      '8. 등산객: 등산객은 다리와 코어에 힘을 기르는 좋은 방법입니다.\n' +
      '\n' +
      '9. Deadlifts: Deadlifts는 여러분의 다리와 코어에 힘을 기르는 좋은 방법입니다.\n' +
      '\n' +
      '10. 삼두근 딥: 삼두근 딥은 삼두근에 힘을 키우는 좋은 방법입니다.\n' +
      '\n' +
      '간단한 운동부터 시작하고 강해질수록 점차 강도를 높이는 것이 중요하다는 것을 기억하세요. 자신을 너무 몰아붙이지 마세요, 그렇지 않으면 적절한 형태를 유지하는 것이 어려울 수 있습니다. 일관된 연습으로 힘과 전반적인 체력이 향상됩니다.\n' +
      '\n' +
      '운동이나 건강한 생활에 대해 궁금한 점이 있으면 주저하지 말고 의사나 물리 치료사와 상담하세요.\n' +
      '\n' +
      '그들은 당신에게 가장 잘 맞는 개인화된 계획을 개발하는 데 도움을 줄 수 있습니다.\n' +
      '\n' +
      '결론적으로 가정에서도 할 수 있는 운동이 많지만 시간이 지남에 따라 서서히 시작하여 점차 강도를 높이는 것이 중요하다. 여러분의 몸에 귀를 기울이고 필요할 때 휴식을 취하는 것을 기억하세요. 그리고 가장 중요한 것은, 그 과정을 즐겨요!\n' +
      '\n' +
      '**Output** (MobileLLM-350M):\n' +
      '\n' +
      '초보자는 하기 쉽고 많은 힘이나 지구력을 필요로 하지 않는 운동부터 시작해야 한다. 초보자에게 좋은 몇 가지 운동이 있습니다.\n' +
      '\n' +
      '1. 팔굽혀펴기: 어디서든 할 수 있는 간단한 운동이기 때문에 초보자에게 좋은 운동입니다. 무릎에 팔굽혀펴기부터 시작해서 바닥에 엎드리세요.\n' +
      '\n' +
      '2. Squarks: Squarks는 다리 근력과 지구력을 길러주는 훌륭한 운동이다. 무릎을 꿇고 쪼그려 앉는 것부터 시작해서 바닥에 엎드리세요.\n' +
      '\n' +
      '3. 폐: 폐는 다리 근력과 지구력을 길러주는 훌륭한 운동이다. 앞발에 뛰어내리는 것부터 시작해서 땅으로 몸을 내려라.\n' +
      '\n' +
      '4. 플랭크: 플랭크는 코어 강도와 안정성을 구축하기 위한 훌륭한 운동이다. 팔뚝에 널빤지로 시작해서 바닥에 엎드려라.\n' +
      '\n' +
      '5. 자전거 크런치: 자전거 크런치는 상체 근력과 안정성을 구축하기 위한 훌륭한 운동이다. 등 윗부분에 자전거 바삭거리는 것부터 시작해서 바닥에 엎드리세요.\n' +
      '\n' +
      '6. 다리 리프트가 있는 플랭크: 이 운동은 다리 힘과 안정성을 구축하는 데 탁월합니다. 팔뚝에 널빤지로 시작해서 다리를 땅에서 들어올리고 몇 초 동안 기다렸다가 다시 내려놓는다.\n' +
      '\n' +
      '7. 점핑잭: 점핑잭은 심혈관계 지구력과 힘을 기르기 위한 훌륭한 운동이다. 발에 점핑잭으로 시작해서 점프하고 다리를 벌리세요.\n' +
      '\n' +
      '몸에 편한 무게부터 시작하고 강해질수록 점차 무게를 늘려가는 것을 기억하세요.\n' +
      '\n' +
      '운동하기 전에 몸을 풀고 운동 후에 스트레칭을 하는 것도 부상을 예방하는 데 중요합니다.\n' +
      '\n' +
      '결론적으로, 초보자들은 하기 쉽고 많은 힘이나 지구력을 필요로 하지 않는 운동부터 시작해야 한다. 이 운동은 다리 힘과 지구력을 키우는 데 탁월하며 어디에서나 할 수 있습니다. 몸에 편한 무게부터 시작하고 강해질수록 점차 무게를 늘려가는 것을 기억하세요.\n' +
      '\n' +
      '운동 후 워밍업과 스트레칭도 부상을 예방하는 데 중요합니다.\n' +
      '\n' +
      '이러한 운동을 따라 하면 근력, 지구력 및 전반적인 체력 수준을 향상시키는 데 도움이 될 수 있습니다.\n' +
      '\n' +
      '새로운 운동 프로그램을 시작하기 전에 항상 의료 전문가와 상의해야 합니다.\n' +
      '\n' +
      '**Output** (llama-v2-7b-chat):\n' +
      '\n' +
      '운동은 건강한 생활 방식에서 필수적인 부분이지만, 초보자들에게는 위협적이고 압도적일 수 있습니다. 여기 초보자를 위한 가장 좋은 운동들이 있습니다:\n' +
      '\n' +
      '1. 체중 운동: 이 운동들은 어떤 장비도 필요하지 않기 때문에 초보자들에게 좋습니다. 예를 들면 팔굽혀펴기, 스쿼트, 런지, 플랭크 등이 있다.\n' +
      '\n' +
      '2. 요가: 요가는 유연성, 균형 및 근력 향상에 도움을 줄 수 있는 저충격 운동이다. 스트레스를 줄이고 전반적인 웰빙을 개선하는 데에도 좋습니다.\n' +
      '\n' +
      '3. 활기찬 걷기: 걷기는 어디서나 할 수 있는 간단하고 접근 가능한 형태의 운동이다. 심박수를 높이고 심혈관 건강을 개선하는 좋은 방법입니다.\n' +
      '\n' +
      '4. 수영: 수영은 심혈관계 건강 증진과 근육 강화에 도움을 줄 수 있는 저충격 운동이다. 관절도 쉬워 초보자에게 좋은 선택입니다.\n' +
      '\n' +
      '5. 사이클링: 사이클링은 실내 또는 실외에서 수행될 수 있는 또 다른 저충격 운동이다. 심혈관 건강을 개선하고 다리 근력을 키우는 좋은 방법입니다.\n' +
      '\n' +
      '6. 저항밴드 운동: 저항밴드는 가볍고 휴대가 용이하여 가정이나 이동 중에도 쉽게 사용할 수 있다. 상체, 하체, 코어의 힘과 유연성을 향상시키는 데 도움이 될 수 있습니다.\n' +
      '\n' +
      '7. 필라테스: 필라테스는 코어 근력, 유연성, 신체 조절에 초점을 맞춘 운동의 한 형태이다. 충격은 적고 자세와 균형을 개선하는 데 도움이 될 수 있기 때문에 초보자에게 좋은 선택입니다.\n' +
      '\n' +
      '8. 댄스 기반 운동: 줌바 또는 힙합 앱과 같은 댄스 기반 운동은 몸매를 가꾸는 재미있고 활기찬 방법입니다. 그들은 종종 유산소 운동과 근력 훈련을 포함하기 때문에 초보자들에게 좋은 선택입니다.\n' +
      '\n' +
      '9. 점핑로프: 점핑로프는 심혈관계 건강 증진과 칼로리 소모에 도움을 줄 수 있는 고강도 운동이다. 간단한 줄넘기로 집에서 쉽게 할 수 있습니다.\n' +
      '\n' +
      '10. 케틀벨 운동: 케틀벨은 그네, 스쿼트, 프레스 등 다양한 운동을 수행하기 위해 사용될 수 있는 작은 무게이다. 그것들은 힘과 심혈관 건강을 향상시키는 좋은 방법입니다.\n' +
      '\n' +
      '특히 초보자인 경우 새로운 운동 프로그램을 시작하기 전에 의사와 항상 상의하는 것을 잊지 마십시오. 또한 더 편안해짐에 따라 운동의 강도와 지속 시간을 천천히 그리고 점진적으로 증가시키는 것이 중요합니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>