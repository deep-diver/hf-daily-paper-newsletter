<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'instruction-following (Fu et al., 2019; Zhang et al., 2023a). Besides, an agent can be equipped with various tools, but those tools introduce additional uncertainties (e.g., accessibility to a database or the internet). From the perspective of multi-agent system robustness, a single unexpected error or response can propagate to the whole system, causing a series of cascading effects if not handled properly. Thus, it is crucial for multi-agent applications to autonomously detect and handle incorrect responses from LLMs. While LLMs may assist in identifying and managing these errors, it remains a challenge to determine whether they can resolve errors on their own and to automatically provide the necessary information for error correction. Consequently, designing fault-tolerant that incorporate LLMs is a key challenge in the development of multi-agent applications.\n' +
      '\n' +
      '_Compatible multi-modal data is highly systematic._ There is increasing number of agent or LLM applications targeting on multi-modal content generation. Supporting multi-modal data (Su et al., 2023; Betker et al., 2023) in multi-agent applications requires a comprehensive and systematic approach. This includes considerations for data storage, presentation, user interaction, message transmission, communication, etc. However, meeting these requirements presents new challenges, includes ensuring data consistency across different formats, maintaining high performance during data transmission and agent communication, and avoiding the introduction of complex concepts for both developers and users. Although there are application-specific solutions, there are no general platform-level programming interfaces to support multi-modal applications.\n' +
      '\n' +
      '_Distributed applications bring extra programming difficulties and system design challenges._ An industrial-oriented scenario for multi-agent applications is that the agents are owned by different organizations and run on different machines because the agents are equipped with unique private knowledge or patented tools. To support such applications, it usually requires the application developers to have professional knowledge on distributed system programming and optimization in the design phase. Besides, distributed applications usually require great extra effort in the development and testing, especially when debugging and diagnosing issues spread across distributed processes or agents. Moreover, integrating advanced features like multi-modal data processing pose additional challenges in a distributed setting, when the agents requires different time to accomplish the sub-tasks or the generated contents have are very heterogeneous. Poor design decisions can result in excessive communication overhead between agents. Therefore, it\'s challenging for developers to address these issues effectively to ensure effective system operations.\n' +
      '\n' +
      'To address these challenges, we introduce **AgentScope**, a novel multi-agent platform designed for developers with varying levels of expertise. AgentScope is well designed with a message exchange communication mechanism that embodies great usability, robustness and efficiency. We underscore the salient features of AgentScope as follows:\n' +
      '\n' +
      '**Exceptional Usability for Developers**: AgentScope is designed with a fundamental emphasis on ease of use, particularly for developers with varying levels of expertise. By implementing a procedure-oriented message exchange mechanism, AgentScope ensures a smooth learning curve. To further alleviate the programming burdens, AgentScope offers an extensive suite of syntactic utilities, including various pipelines and an information sharing mechanism. Coupled with rich built-in resources and integrated user interaction modules, AgentScope makes programming a multi-agent application much more enjoyable than ever.\n' +
      '\n' +
      '**Robust Fault Tolerance for Diverse LLMs and APIs**: As the scale and scope of models and APIs expand, a robust fault-tolerance mechanism in multi-agent application becomes paramount. AgentScope integrates a comprehensive service-level retry mechanism to maintain API reliability. AgentScope equips with a set of rule-based correction tools to handle some obvious formatting problem in the responses of LLMs. Moreover, AgentScope offers customizable fault tolerance configurations, enabling developers to tailor their own fault tolerance mechanism through parameters like parse_func, fault_handler, and max_retries. While admittedly, not all the errors can be handled by the aforementioned mechanism, we propose a logging system with customized features for multi-agent applications as the last safeguard for AgentScope.\n' +
      '\n' +
      '**Extensive Compatibility for Multi-Modal Applications**: With remarkable progress of large-scale multi-modal models, AgentScope supports multi-modal data (_e.g._, files, images, audio and videos) in both dialog presentation, message transmission and data storage. Specifically, AgentScope decouples multi-modal data transmission from storage by a unified URL-based attribute in message. During message transmission, AgentScope only attaches a URL to the message, thereby minimizing memory usage incurred by message copies in each agent\'s memory. This strategy ensures that multi-modal data is loaded only when necessary, such as when being rendered in web UI or invoked by model wrappers.\n' +
      '\n' +
      '**Optimized Efficiency for Distributed Multi-Agent Operations**: Recognizing the vital importance of distributed deployment, AgentScope introduces an actor-based distributed mechanism that enables centralized programming of complex distributed workflows, and automatic parallel optimization. Particularly, the workflow for local and distributed deployments are exactly the same, indicating negligible overhead when migrating application between centralized and distributed environments. With such distribution framework, AgentScope empowers developers to concentrate on the application design rather than implementation details.\n' +
      '\n' +
      '**Summary.** To summarize, our AgentScope multi-agent platform provides a procedure-oriented message exchange mechanism with a set of syntactic features to facilitate the multi-agent programming; the fault tolerance designs of AgentScope enable developers to handle errors elegantly for their applications; the support for the multi-modal applications reduces the overheads of heterogeneous data generation and transmission; the actor-based distributed mode of AgentScope can help develop efficient and reliable distributed multi-agent applications seamlessly.\n' +
      '\n' +
      '**Roadmap.** In the following sections, we navigate through the core components and capabilities of AgentScope, showcasing its role in advancing the development and deployment of multi-agent applications. Section 2 provides an overview, while Section 3 focuses on the user experience. Section 6 examines our platform\'s distributed efficiency. Sections 5 and 4 cover multi-modal support and fault tolerance. Use cases are presented in Section 7, related work in Section 8, and concluding thoughts in Section 9.\n' +
      '\n' +
      '## 2 Overview\n' +
      '\n' +
      '### Basic Concepts in AgentScope\n' +
      '\n' +
      'This section introduces the primary concepts in AgentScope: message, agent, service, and workflow. These four concepts are throughout the platform and all multi-agent applications based on it.\n' +
      '\n' +
      '* **Message**: Messages serve as the carriers for information exchange in multi-agent conversations, encapsulating the source and content of the information. In AgentScope, messages are implemented as Python dictionaries with two mandatory fields (_name_ and _content_) and an optional field (_url_). The _name_ field records the name of the agent who generated the message, and the _content_ field contains the text-based information generated by the agent. The _url_ field is designed to hold Uniform Resource Locator (URL), which typically links to multi-modal data, such as images or videos. Messages with this field is particularly relevant for interactions with agents that can process and generate multi-modal\n' +
      '\n' +
      'Figure 1: Architecture of AgentScope.\n' +
      '\n' +
      'content. Each message is uniquely identified by an auto-generated UUID and timestamp, ensuring traceability. Example 1 shows how the messages can be created, serving as atoms in the inter-agent communication of AgentScope.\n' +
      '\n' +
      '```\n' +
      '1fromagentscope.messageimportMsg\n' +
      '2\n' +
      '3msg1=Msg("Alice","Hello1")\n' +
      '4msg2=Msg(\n' +
      '5name="Bob",\n' +
      '6content="HowdoyoufindthispictureIcapturedyesterday?",\n' +
      '7url="[https://xxx.png](https://xxx.png)")\n' +
      '```\n' +
      '\n' +
      'Example 1: Illustrative examples of message creation in AgentScope.\n' +
      '\n' +
      '* **Agent**: Agents are the primary actors within multi-agent applications, acting as conversational participants and executors of tasks. In AgentScope, agent behaviors are abstracted through two interfaces: the _reply_ and _observe_ functions. The _reply_ function takes a message as input and produces a response, while the _observe_ function processes incoming messages without generating a direct reply. The interplay between agents and messages, as shown in Example 2, forms the operational basis of AgentScope and is essential for developers to model complex interactions in multi-agent LLMs. [leftmargin=0cm]\n' +
      '* **Service**: Services in AgentScope refer to the functional APIs that enable agents to perform specific tasks. These services are categorized into model API services, which are channels to use the LLMs, and general API services, which provide a variety of tool functions. The integration of these services into agents is key for executing a wide range of tasks, especially when interfacing with LLMs that may require external data or computation services.\n' +
      '* **Workflow**: Workflows represent ordered sequences of agent executions and message exchanges between agents, analogous to computational graphs in TensorFlow, but with the flexibility to accommodate non-DAG structures. Workflows define the flow of information and task processing among agents, facilitating parallel execution and efficiency improvements. This concept is essential for designing multi-agent systems that interact with LLMs, as it allows for the coordination of complex, interdependent tasks.\n' +
      '\n' +
      '### Architecture of AgentScope\n' +
      '\n' +
      'We present AgentScope as an infrastructural platform to facilitate the creation, management, and deployment of multi-agent applications integrated with LLMs. The architecture of AgentScope comprises three hierarchical layers, as shown in Figure 1. The layers provide supports for multi-agent applications from different levels, including elementary and advanced functionalities of a single agent (utility layer), resources and runtime management (manager and wrapper layer), and agent-level to workflow-level programming interfaces (agent layer). AgentScope introduces intuitive abstractions designed to fulfill the diverse functionalities inherent to each layer and simplify the complicated inter-layer dependencies when building multi-agent systems. Furthermore, we offer programming interfaces and default mechanisms to strengthen the resilience of multi-agent systems against faults within different layers.\n' +
      '\n' +
      '**Utility Layer:** As the platform\'s foundation, the utility layer in AgentScope provides essential services to support the core functionalities of agents. This layer abstracts the complexity of underlying operations,such as API invocation, data retrieval, and code execution, allowing agents to focus on their primary tasks. AgentScope\'s utility layer is designed with ease of use and robustness as its utmost priority, supporting versatile operations in multi-agent systems and providing _built-in_ autonomous retry mechanisms for exception and error handling against unexpected interruptions.\n' +
      '\n' +
      '**Manager and Wrapper Layer:** As an intermediary, the manager and wrapper abstraction layer manages the resources and API services, ensuring high availability of resources and providing resistance to undesired responses from LLMs. Unlike the utility layer, which provides default handlers, the manager and wrapper layer also offers customizable interfaces for fault tolerance controls depending on developers\' needs and the specific requirements of the application. This layer is responsible for maintaining the operational integrity of the agents, a crucial aspect for LLMs to perform consistently under diverse conditions. Detailed elaboration on the fault tolerance mechanisms is provided in Section 4.\n' +
      '\n' +
      '**Agent Layer:** At the core of AgentScope lies the agent abstraction, which forms the backbone of the multi-agent workflow and is the primary entity responsible for interaction and communication. This layer is designed to facilitate the construction of intricate workflows and enhance usability, reducing the programming burden on developers. By integrating streamlined syntax and tools, AgentScope empowers developers to concentrate on the implementation and optimization of agent-based applications that leverage the capabilities of LLMs. The programming features and syntactic sugars are introduced in Section 3 with more details.\n' +
      '\n' +
      '**User interaction:** In addition to the layered architecture, AgentScope provides multi-agent oriented interfaces such as terminal and Web UI. These interfaces allow developers to effortlessly monitor the status and metrics of the application, including agent communication, execution timing, and financial costs.\n' +
      '\n' +
      'Collectively, the layered constructs of AgentScope provide the essential building blocks for developers to craft bespoke multi-agent applications that leverage the advanced capabilities of large language models. The subsequent section will delve into the features of AgentScope that enhance the programming experience for multi-agent application development.\n' +
      '\n' +
      '## 3 High Usability in AgentScope\n' +
      '\n' +
      'The design of AgentScope prioritizes usability, aiming to streamline the development process for multi-agent with LLMs and ensure a smooth interaction experience for both users and developers. This section delves into how AgentScope flattens the learning curve and enhances the programmer\'s experience by introducing intuitive concepts and features that facilitate the creation of complex multi-agent applications.\n' +
      '\n' +
      '### Syntactic Sugar for Multi-Agent Workflows\n' +
      '\n' +
      'Leveraging basic concepts introduced in Section 2.1, developers are empowered to construct sophisticated multi-agent applications. Nonetheless, directly coding each agent\'s message exchange can become cumbersome, as shown in Example 3. Recognizing this, AgentScope introduces two syntactic utilities: pipelines and message hubs, to abstract away the complexity and minimize repetition.\n' +
      '\n' +
      '```\n' +
      '1#setupagents:agent1toagent5\n' +
      '2#...\n' +
      '3\n' +
      '4x=agent1(x)\n' +
      '5x=agent2(x)\n' +
      '6x=agent3(x)\n' +
      '7x=agent4(x)\n' +
      '8x=agent5(x)\n' +
      '```\n' +
      '\n' +
      'Example 3: Example of programming a sequential workflow with basic concepts in AgentScope.\n' +
      '\n' +
      '**Pipeline Abstraction:** The pipeline abstraction reduces repetitive coding by encapsulating patterns of message transmission, including sequential, conditional, and iterative exchanges, into simple, reusable components. With pipelines, developers can focus on the logic of agent interactions rather than the boilerplate code. Example 4 illustrates how pipelines can be employed in both functional and object-oriented styles to create a clear and concise agent workflow. Besides the sequential pipeline in the example, AgentScope also provides if-else, switch, while-loop and for-loop pipelines, facilitating the programming of the multi-agent interactions.\n' +
      '\n' +
      '```\n' +
      '1#setupagents:agent1toagent5\n' +
      '2#...\n' +
      '3fromagentscope.pipelinesimportSequentialPipeline\n' +
      '4fromagentscope.pipelines.functionalimportsequentialpipeline\n' +
      '5\n' +
      '6#usingfunctionalpipeline\n' +
      '7x=sequentialpipeline([agent1,agent2,agent3,agent4,agent5],x)\n' +
      '8\n' +
      '9#usingobjectpipeline\n' +
      '10pipe=SequentialPipeline([agent1,agent2,agent3,agent4,agent5])\n' +
      '11x=pipe(x)\n' +
      '```\n' +
      '\n' +
      'Example 4: Using functional and object sequential pipeline to construct workflow in AgentScope.\n' +
      '\n' +
      '**Message Hub for Agent Communication:** In multi-agent systems, especially when integrated with LLMs, efficiently managing communication among a group of agents is essential. The message hub in AgentScope serves as a broadcast mechanism that simplifies group interactions. Developers can initiate a message hub by defining participating agents and can include initial broadcast messages. When new messages are generated by the agents within the message hub, they are automatically disseminated to other participants, as demonstrated in Example 5. This abstraction is particularly useful for multi-agent scenarios involving LLMs, where dynamic and contextually rich conversations are common Du et al. (2023).\n' +
      '\n' +
      '```\n' +
      '1#setupagents:agent1toagent4\n' +
      '2#...\n' +
      '3\n' +
      '4greeting=Mag("host","Welcometothemessagehub!")\n' +
      '5\n' +
      '6withmsghub(participant=[agent1,agent2,agent3],\n' +
      '7announcement=greeting)ashub:\n' +
      '8#Messagewillbebroadcasttoagent2andagent3automatically\n' +
      '9agent1()\n' +
      '10\n' +
      '11#Deleteagent2fromthemessagehub\n' +
      '12hub.delete(agent2)\n' +
      '13\n' +
      '14#Addagent4intothemessagehub\n' +
      '15hub.add(agent4)\n' +
      '16\n' +
      '17#Broadcastmessage\n' +
      '18hub.broadcast(Mag("host","Welcomagent4tojointthehub!"))\n' +
      '```\n' +
      '\n' +
      'Example 5: Using message hub with AgentScope.\n' +
      '\n' +
      '### Resource-Rich Environment for Agent Development\n' +
      '\n' +
      'To further enhance usability, AgentScope is equipped with a rich set of built-in resources, including services, dedicated agents, and pre-configured examples. These resources are designed to reduce the initial setup effort and enable rapid prototyping and deployment of multi-agent LLM systems.\n' +
      '\n' +
      '**Comprehensive Service Integration:** AgentScope integrates a variety of services, such as web search, database querying, and code execution, to support the capabilities of agents. These services are essential for building useful agents with LLMs, as they often need to draw information from external sources or execute tasks that go beyond the equipped LLMs\' internal knowledge. Example 6 showcases the seamless conversion of a service into an OpenAI-Compatible JSON format, simplifying the integration process for developers.\n' +
      '\n' +
      '**Pre-built Agent Templates:** As cataloged in Table 1, AgentScope offers pre-built agents and ready-to-use components for tasks like dialogue management, user proxying, multi-modal data handling, and distributed deployment. These templates serve as starting points for developers to customize and extend, significantly accelerating the development of multi-agent LLM applications.\n' +
      '\n' +
      '### Multi-Agent Oriented Interaction Interfaces\n' +
      '\n' +
      'Furthermore, AgentScope introduces interaction interfaces tailored for multi-agent systems, as illustrated in Figures 2, 3 and 4. These interfaces provide a rich multi-modal experience, crucial for systems incorporating LLMs that handle diverse data types.\n' +
      '\n' +
      '**Agent Differentiation in User Interfaces:** To facilitate user interaction with multiple agents, AgentScope assigns unique colors and icons to each agent, enhancing clarity and visual distinction in both terminal and web UI. The "first-person perspective" feature allows users to experience interactions from the viewpoint of a specified agent, aligning with their role in the application, such as in a game scenario. This feature not only enriches the multi-agent experience but also mirrors the nuanced interactions that occur in human-agent and agent-agent dialogues within LLM systems.\n' +
      '\n' +
      '**Monitoring and Cost Management:** A vital aspect of deploying LLMs at scale is resource management. AgentScope includes a monitoring module that tracks model and API usage, as well as calculating financial costs. Developers can customize metrics and set budget limits, receiving automatic alerts when thresholds\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} \\hline \\hline Agent Name & Function \\\\ \\hline UserAgent & The proxy of the user. \\\\ DialogAgent & A general dialog agent, whose role can be set by system prompt. \\\\ DictDialogAgent & A dictionary version dialog agent, who responds in Python dictionary format. \\\\ ProgrammerAgent & An agent that can write and execute Python code. \\\\ TextToImageAgent & An agent that generates images according to the requirements. \\\\ AudioDialogAgent & An agent that can interact in speech. \\\\ RpcUserAgent & A distributed version user proxy. \\\\ RpcDialogAgent & A distributed version DialogAgent. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Built-in agents and their functions in AgentScope.\n' +
      '\n' +
      'are approached or exceeded. This proactive cost management is particularly important for LLMs that may incur high computational expenses.\n' +
      '\n' +
      'In essence, AgentScope offers an environment where developers can efficiently build and deploy fault-tolerant multi-agent LLM applications. By providing syntactic abstractions, a rich resource pool, and multi-agent interactive interfaces, AgentScope ensures that the intricacies of multi-agent systems are abstracted away, allowing developers to focus on creating innovative solutions.\n' +
      '\n' +
      '## 4 Supporting Fault-Tolerant Mechanism\n' +
      '\n' +
      'In the realm of multi-agent systems, particularly those interfacing with diverse open-source LLMs, fault tolerance is a key property to ensure seamless operation. AgentScope is engineered to autonomously handle a wide range of errors with minimal human intervention required, drawing upon a comprehensive fault-tolerant infrastructure that is acutely aware of the complexities involved in multi-agent coordination and LLM dependencies.\n' +
      '\n' +
      'Error Classification and Handling Strategies.Our approach begins with a methodical classification of errors into distinct levels, each with tailored handling strategies:\n' +
      '\n' +
      '* _Accessibility errors:_ In AgentScope, an agent\'s functionalities rely on different kinds of services, but those services may be subject to temporary inaccessible errors. These errors may be caused by model instability or network conditions. For example, the model APIs may return timeout error when there is\n' +
      '\n' +
      'Figure 3: The dialogue history of a werewolf game in AgentScope.\n' +
      '\n' +
      'Figure 2: The generation, storage and transmission of Multi-modal data in AgentScope.\n' +
      '\n' +
      'traffic congestion in the busy hours, or a database on a remote machine may be inaccessible because of transient network outages.\n' +
      '* _Rule-resolvable errors:_ As many multi-agent applications require information exchange between services or agents, it is essential to follow the protocols for those communications, e.g., in JSON format. However, as the responses of LLMs are not fully controllable yet, their return may not follow the format required in the prompts. For example, we may expect a response from an LLM in JSON, but a right brace is missed at the end of the return, leading to parsing failure.\n' +
      '* _Model-resolvable errors:_ When a multi-agent system handles some complicated tasks, the ability of the agent to understand the input, make decisions, and deliver outputs mostly depends on the capability of LLMs. In some cases, the responses of LLMs are in the expected format, but the content has problems, such as argument errors, semantic errors, or programming mistakes. It is hard to have pre-defined rules to regularize those responses for diverse tasks, but it has also been shown that such errors may be detected and recovered by further interaction with the LLMs.\n' +
      '* _Unresolvable errors:_ Eventually, there must be some errors that cannot be detected or solved. A typical example is that the API key of an LLM is expired or unauthorized. The agents relying on it or the system can do nothing to resolve such errors.\n' +
      '\n' +
      'Fault Tolerance mechanisms in AgentScope.In AgentScope, we provide different mechanisms to encounter the above mentioned errors.\n' +
      '\n' +
      '* _Basic auto-retry mechanisms._ To combat accessibility errors, AgentScope\'s API services and model wrappers are fortified with retry logic that developers can customize, such as setting the maximum retry count. This ensures that agents can recover from sporadic disruptions and maintain their operational continuity.\n' +
      '* _Rule-based correction tools._ The rule-based correction tools are introduced into AgentScope to efficiently and economically handle some easy-to-fix format errors in the responses of LLMs. For example, we establish a set of default rules in AgentScope that can complete unmatchable braces and extract JSON data from strings. Such rule-based correction tools can correct some of the common rule-resolvable errors without calling LLM APIs again, which means shorter processing time and no LLM API call cost.\n' +
      '\n' +
      'Figure 4: Multi-modal interactions between agents in web UI.\n' +
      '\n' +
      '* _Customizable fault handlers._ AgentScope also integrates flexible fault handlers interfaces in model wrappers for developers to define how to parsing the responses from LLMs and handling the unexpected outputs. Application developers can configure their fault handling mechanism by providing parsing function, fault handling funtion and number of chances giving to LLMs through configurable parameters, parse_func and fault_handler and max_retries when invoking LLMs. With such developer-friendly design, AgentScope can be configurably robust to rule-resolvable errors (when the build-in rules fail to handle) and some model-resolvable errors that can be detected and handled by a single agent (e.g., distilling a verbose summary to a more concise one).\n' +
      '* _Agent-level fault handling._ There are model-resolvable errors that require more advanced LLM usages or agent-level interaction to recover. For example, detecting semantic errors, which usually include factual inaccuracy, logical inconsistency, contextual incoherence, unreasonable inference, and inappropriate vocabulary usage, is challenging since they may not necessarily trigger immediate red flags within the system\'s existing validation processes. Developers can utilize the agent\'s ability in AgentScope (e.g., memory module and message hub) to critique for semantic error checking such as self-critique, pairwise critique, and human-augmented critique.\n' +
      '* _Logging system._ Although the unsolvable errors are too tricky for the system to handle, AgentScope provides an improved logging system for developers to quickly monitor and identify the problems in multi-agent applications. The logging system in AgentScope has customized features for the multi-agent application scenarios, including adding a logging level called CHAT for logging conversations between agents, providing formatted logs with various execution information, and a WebUI user interface to facilitate monitoring.\n' +
      '\n' +
      '## 5 Supporting Multi-Modal Application\n' +
      '\n' +
      'The integration of multi-modal data is indispensable for advancing the capabilities and applications of multi-agent with LLMs. AgentScope is designed to seamlessly support various data modalities, leveraging the diverse inputs and outputs that contemporary LLMs can process and produce.\n' +
      '\n' +
      'Management of Multi-Modal DataIn a running AgentScope application, the lifecycle of multi-modal data is carefully managed. This management includes the generation, transmission, and storage of multi-modal data--all facilitated through a decoupled architecture using URLs and a local file manager system. Fig. 2 exemplifies this process, including data originating from user inputs or model generations, data storage and retrieval and data sharing.\n' +
      '\n' +
      '* _Multi-modal data generation._ There are two primary sources of multi-modal data in AgentScope. One source is simply the locally stored multi-modal files, which can be used by either user proxy agents or general agents with access to the local file system. Another source is the model-modal content generation models. Our model APIs and the model wrappers integrate the most popular multi-modal models, such as the text-to-image content generation models like OpenAI\'s DALL-E, and conversely, the image-to-text image analysis models, e.g., GPT-4V. Besides the built-in APIs, developers can introduce their favorite multi-modal models and customize their own model wrappers, with our ready-to-use examples as the starting points. This customization process is streamlined in AgentScope and benefits from our modular design, allowing developers to connect their multi-modal services with minimal effort.\n' +
      '* _Multi-modal data storage._ As mentioned above, multi-modal data in the multi-agent application can be either from ready-to-use local files or generated by multi-modal models. When a multi-modal model wrapper is invoked to generate multi-modal data, it first saves the data locally with the help of the file manager and returns a local URL when it receives multi-modal data from the model API service.\n' +
      '* _Multi-modal data transmission._ AgentScope simplifies the process of multi-modal data sharing between agents by allowing agents to encapsulate local or remote URLs in multi-modal messages to indicate the actual storage locations of the data. The receiver agents can load the multi-modal data through the URLs when ready to process those.\n' +
      '\n' +
      'The benefits of introducing URLs in the messages when agents share multi-modal data are three-fold. 1) It can minimize the message size to avoid potential errors or delays because of the network bandwidth and enable the receiver agent to load the data on demand. 2) If there is other text information in the message, the downstream agents can potentially prioritize or parallel the processing of the text information to/and the processing of multi-modal information. 3) Such URL-attached messages can also facilitate the multi-modal data demonstration, which will be introduced in the following section.\n' +
      '\n' +
      'Multi-Modal Interaction ModesWith the implementation of URL-attached messages, AgentScope empowers users to interact with multi-modal systems via accessible interfaces such as terminal and web UI. Figure 4 showcases the user\'s ability to interact with multi-modal data within interaction modes. In the terminal, users can conveniently access locally stored data by activating the provided URLs. The web UI further enhances user experience by providing an intuitive platform to view and analyze multi-modal content, aligning with the expectations of modern web applications.\n' +
      '\n' +
      'Through AgentScope, developers are equipped to tailor model API services and wrappers to their individual needs, forge applications that handle diverse data modalities, and provide users with the necessary tools to engage with multi-modal agents effectively. This comprehensive support for multi-modal applications positions AgentScope as a versatile and powerful framework for harnessing the full potential of multi-agent LLMs, broadening the horizons for developers and researchers alike in creating sophisticated and interactive AI systems.\n' +
      '\n' +
      '## 6 Actor-based Distributed Framework\n' +
      '\n' +
      'Efficiency and extensibility are essential when building industry-level applications on multi-agent systems. The inference speed of the agents in multi-agent applications may vary dramatically. For example, suppose an agent in a multi-modal application employs a text-to-video model. In that case, its response time may be significantly longer than that of an agent designed to fill in details of stories. _Parallelization_, as a classic idea, should be introduced to boost efficiency. Besides, multi-agent applications can comprise agents physically distributed on different machines. A typical use case is that a company can wrap its patented techniques or private knowledge bases into an agent on their local machines connected to the internet and provide autonomous services to other entities via agent interactions.\n' +
      '\n' +
      'However, when it comes to multi-agent system, a challenge is that developers need to make decisions between the following two pairs of technology roadmaps. As there is no free lunch, any combinations have their benefits and drawbacks.\n' +
      '\n' +
      '* _Centralized v.s. decentralized coordination._ In the context of the distributed system, centralized coordination means multiple computation nodes being managed by a central node, such as the server-client model. A multi-agent mechanism with centralized coordination means the execution of the agents is scheduled by, and the messages between agents are forwarded by a central coordination component. On the contrary, decentralized coordination does not rely on any central component to schedule or forward messages, but the agents in such a system can be invoked automatically and send messages directly to the downstream agents for further processing. While centralized coordination is a straightforward style that can be understood and is easy to debug, its disadvantages include vulnerability to central node failures, imposing heavy traffic on the central node, and difficulty in scaling or extending to complicated applications. In contrast, the decentralized coordination may require extra effort to develop and maintain but has a higher robustness against failure of any single node.\n' +
      '* _Static vs. dynamic workflow design._ A similar comparison can be found between the static computational graph employed in early versions of TensorFlow (Abadi et al., 2016) and the dynamic computation graph used in PyTorch Paszke et al. (2019). In the context of multi-agent applications, the choice between a static and dynamic workflow is akin to choosing between pre-compiled and interpreted execution. The static workflow design can enable the optimization of the workflow graph level for running time and resource allocation. However, static workflow design requires the workflow graph to be known before execution, which limits the adaptation into applications, especially the ones with loop structures in the design. In contrast, dynamic workflows offer greater flexibility at the expense of optimization potential. This is particularly relevant when dealing with large language models where execution paths can change based on the input data or model inference results.\n' +
      '\n' +
      'Distributed mode in AgentScope.AgentScope balances these technology roadmaps by implementing an actor-based distributed mode that is mindful of the unique needs of multi-agent LLM systems, with the following important features:\n' +
      '\n' +
      '* _Automatic parallel optimization without static graphs._ AgentScope leverages the actor model to enable automatic parallel optimization, allowing developers to circumvent the intricacies of static graph programming. This approach seamlessly aligns with the dynamic and often unpredictable nature of LLMs, where the computational graph can alter based on evolving contexts and dialogue states.\n' +
      '* _Programming workflows with minimal complexity._ In contrast to traditional actor models and peer-to-peer (P2P) implementations that require intricate execution ordering for distributed agents, AgentScope simplifies workflow programming to a single procedural style within a Python function. This design significantly flatten the learning curve for developers, making the construction of sophisticated multi-agent LLMs more accessible.\n' +
      '* _Hybrid local and distributed agent support._ AgentScope\'s flexibility extends to supporting a hybrid mode where some agents operate locally while others are distributed. This feature is particularly beneficial when integrating LLMs with varying computational requirements, allowing for resource-intensive models to be distributed while less demanding agents remain local, all without the developer needing to differentiate between the two during implementation.\n' +
      '\n' +
      'Specifically, we can concisely describe how AgentScope incorporates the actor model as the following. In this conceptual framework, an "actor" acts as a stand-alone entity that processes computation upon receipt of all necessary messages. This paradigm ensures that each agent, corresponding to an actor, only engages in computation once the required input messages are ready, thus achieving automatic parallel optimization.\n' +
      '\n' +
      'However, the actor-model-based workflow presents a programming challenge: the variable (i.e., messages) passing between actors (i.e., agents) may be placeholders without any practical meaning at the beginning. To\n' +
      '\n' +
      'Figure 5: An example of a distributed application in AgentScope, illustrating various processes as denoted by different colors.\n' +
      '\n' +
      'alleviate this, AgentScope introduces the "placeholder" message, a novel data structure that allows the main process to continue without blocking, while preserving the necessary information to retrieve real values later (Figure 5). This mechanism is particularly advantageous for multi-agent LLM systems, where execution flow must adapt to the variable output of language models.\n' +
      '\n' +
      '```\n' +
      '1#setupdistributedagent:agent\n' +
      '2...\n' +
      '3\n' +
      '4input_msg=Mag("system","Whichagentshouldrespondnext,agent2oragent37")\n' +
      '5\n' +
      '6#thevariablechoceisaplecholder\n' +
      '7choice:placeholder=host_agent(input_msg)\n' +
      '8\n' +
      '9ifchoice["content"]=="agent2":\n' +
      '10response=agent2()\n' +
      '11elifchoice["content"]=="agent3":\n' +
      '12response=agent3()\n' +
      '```\n' +
      '\n' +
      'Example 7: Demonstrating the use of placeholders in control flow within AgentScope.\n' +
      '\n' +
      'Another series of challenges arise when placeholders are used within control flow statements (e.g., if-else, loops) without their real values. An example is shown in Example 7, where a placeholder is required to make decisions. In these circumstances, AgentScope temporarily blocks the process to retrieve its actual value, thus ensuring the continuity of the control flow.\n' +
      '\n' +
      'The actor-based distributed mode in AgentScope not only provides automatic parallel optimization and simplifies the developer experience but also demonstrates high efficiency for distributed multi-agent LLM applications. It enables developers to focus on implementing agent logic, particularly the "reply" function, without concern for underlying distributed complexities. This streamlined approach to distributed multi-agent systems can advance the field of LLMs by making it easier to develop, run, and debug sophisticated and scalable multi-agent architectures.\n' +
      '\n' +
      '## 7 Applications\n' +
      '\n' +
      'AgentScope stands as a multi-agent platform optimized for integrating and coordinating large-scale models in a user-friendly and fault-tolerant manner. With the features mentioned in above sections, AgentScope is an ideal platform for a diverse range of applications. These range from simple dialogues to intricate, rule-based games like werewolf, extending to distributed conversations that involve parallel operations across multiple machines. In this section, we will expand upon three primary applications of AgentScope, with each instance illustrating the framework\'s distinct capabilities. All examples referenced herein are accessible in our GitHub repository for community use and contribution.\n' +
      '\n' +
      '### Standalone Conversation\n' +
      '\n' +
      'The fundamental application of AgentScope lies in facilitating standalone conversations, where all agents are operating in a main process. This use case serves as an excellent starting point for users new to AgentScope, allowing them to familiarize themselves with the framework\'s basic functionalities.\n' +
      '\n' +
      'The initial step in launching an application involves initializing agent objects. In this example, we utilize two built-in agents within AgentScope, DialogAgent and UserAgent, to facilitate a dialogue between a user and an AI assistant. Prior to instantiating these agents, model configurations must be loaded through the init interface of AgentScope. Currently, AgentScope is compatible with a variety of platforms, including the standard OpenAI chat, embedding and DALL-E API, HuggingFace and ModelScope inference APIs, as well as locally hosted models with FastChat, vllm, or Flask. Additionally, the init interface allows to specify the file storage directories, storage options, logging level, and agent configurations, etc. After setting the model configurations, developers can instantiate the agents with their respective models, as illustrated in Example 8.\n' +
      '\n' +
      '```\n' +
      '1importagentscope\n' +
      '2fromagentscope.agentsimportDialogAgent,UserAgent\n' +
      '3\n' +
      '4#readmodelconfigs\n' +
      '5agentscope.init(model_configs="./openai_model_configs.json")\n' +
      '6\n' +
      '7#Createadiologagentandauseragent\n' +
      '8assistant_agent=DialogAgent(\n' +
      '9name="Assistant",\n' +
      '10sys_prompt="Youareahelpfullassistant",\n' +
      '11model="gpt-4"\n' +
      '12}\n' +
      '13user_agent=UserAgent()\n' +
      '```\n' +
      '\n' +
      'Example 8: The simple initiilization of standalone conversation example in AgentScope.\n' +
      '\n' +
      '```\n' +
      '1#Basicversion\n' +
      '2x=None\n' +
      '3whilexisNoneorx.content!="exit":\n' +
      '4x=assistant_agent(x)\n' +
      '5x=user_agent(x)\n' +
      '```\n' +
      '\n' +
      'Example 9: A standalone conversation application in AgentScope.\n' +
      '\n' +
      'Subsequently, we construct the conversation by exchanging message between the agents. Specifically, the dialogue process is designed to be a loop, allowing continuous interaction until the user opts to conclude the conversation. Example 9 illustrates the basic implementations within AgentScope.\n' +
      '\n' +
      'To cater to more advanced applications, AgentScope incorporates pipelines to manage the messages exchanges, thereby providing a structured and scalable framework for complex agent interactions. Here the implementation of this standalone conversation application can be simplified with sequential pipeline and loop pipeline as shown in Example 10). Furthermore, Appendix A presents a conversation history by running the above code.\n' +
      '\n' +
      '```\n' +
      '1#Advancedversionwithsequentialpipeline\n' +
      '2fromagentscope.pipelines.functionalimportsequentialpipeline\n' +
      '3x=None\n' +
      '4whilexisNoneorx.content!="exit":\n' +
      '5x=sequentialpipeline([dialog_agent,user_agent],x)\n' +
      '6\n' +
      '7#Advancedversionwithwhilelooppipeline\n' +
      '8fromagentscope.pipelines.functionalimportwhilelooppipeline\n' +
      '9x=whilelooppipeline(\n' +
      '10[assistant_agent,user_agent],\n' +
      '11condition_func=lambda_,x:xisNoneorx.content!="exit",\n' +
      '12x=None)\n' +
      '```\n' +
      '\n' +
      'Example 10: Standalone conversation with pipelines.\n' +
      '\n' +
      '### Werewolf\n' +
      '\n' +
      'Advancing to more complex application, in this subsection we show how to program the workflow of werewolf game in AgentScope with about one hundred lines of code. The werewolf game is a social deduction game, where six players are divided into two opposing teams, werewolves and villagers. The game ends when either all werewolves are eliminated (villager victory) or the number of Werewolves equals or outnumbers the villagers (werewolf victory).\n' +
      '\n' +
      'Setting up the game involves roles allocation and agents initialization. AgentScope supports quick startup with pre-set agent configurations, which contains the required parameters to instantiate the agent objects.\n' +
      '\n' +
      'Example 11 presents how to load agents from configurations and assign roles.\n' +
      '\n' +
      'In werewolf game, one of the most striking features is group conversation, including werewolf discussion in night phase and daytime discussion, which requires to involve multiple different players. To tackle such requirement, we utilize message hub in AgentScope to create group conversation easily. Example 12 presents the werewolf discussion implemented in AgentScope. In this discussion, the message hub starts with an announcement from moderator. After that, the werewolves discuss for at most MAX_WEREWOLF_DISCUSSION_ROUND rounds, and conclude once they reach an agreement. Note in werewolf game, the used agent class is DictDialogAgent, which responds in Python dictionary. With prompt asking agents to respond with "agreement" field, we can directly use this attribute in the response message. For the complete code and example dialogue history, please refer to Appendix B.\n' +
      '\n' +
      '### Distributed Conversation\n' +
      '\n' +
      'Allowing distributed agents are one of the most striking features in AgentScope. In this subsection, we elaborate how to set up a distributed application in two modes: Single-Machine Multi-Process mode and Multi-Machine Multi-Process mode.\n' +
      '\n' +
      'In single-machine multi-process mode, all agents are deployed on a single machine, each running in separate processes. For comparison, we implement the same example with standalone conversation, but with the assistant agent deployed within its own process. Example 13 shows the complet code, where the only difference from local deployment is the invocation of the to_dist function. Subsequently, the agent is deployed on the local host with an automatically allocated port. Beyond this, the Single-Machine Multi-Process mode is essentially identical to local deployment, yet it has been optimized for parallel execution.\n' +
      '\n' +
      'In contrast, the Multi-Machine Multi-Process mode requires developers to initiate the agent service on a remote machine. Example 14 demonstrates an example of DialogAgent deployment on a remote machine, and Example 15 elaborates how to construct the workflow in Multi-Machine Multi-Process mode. In this case, the developer must connect to the agent server using the specified URL and port, and then construct the workflows. Similar to Single-Machine Multi-Process mode, the code for workflow is identical to local deployment.\n' +
      '\n' +
      '```\n' +
      '1fromagentscope.agentsimportUserAgent,DialogAgent\n' +
      '2importagentscope\n' +
      '3\n' +
      '4agentscope.init(model_configs=model_configs)\n' +
      '5\n' +
      '6assistant_agent=DialogAgent(\n' +
      '7name="Assistant",\n' +
      '8sys_prompt="Youareahelpfullassistant",\n' +
      '9model="gpt-4"\n' +
      '10).to_dist()\n' +
      '11user_agent=UserAgent()\n' +
      '12\n' +
      '13x=None\n' +
      '14whilexisNoneornotx.content!="exit:\n' +
      '15x=sequentialpipeline([assistant_agent,user_agent],x)\n' +
      '```\n' +
      '\n' +
      'Example 13: A distributed conversation application in AgentScope.\n' +
      '\n' +
      '```\n' +
      '1fromagentscope.agents:rpc_agentimportRpcAgentServerLauncher\n' +
      '2fromagentscope.agentsimportDialogAgent\n' +
      '3\n' +
      '4#Loadmodelconfigurations\n' +
      '5agentscope.init(model_configs="configs/model_configs.json")\n' +
      '6\n' +
      '7server_launcher=RpcAgentServerLauncher(\n' +
      '8agent_class=DialogAgent,\n' +
      '9agent_kwargs={\n' +
      '10"name":"Assistant",\n' +
      '11"sys_prompt":"Youareahelpfullassistant.",\n' +
      '12"model":"gpt-4"\n' +
      '13},host="xxx.xxx.xxx",\n' +
      '15port=12010,\n' +
      '16)\n' +
      '17\n' +
      '18#Starttheserver\n' +
      '19server_launcher.launch()\n' +
      '20server_launcher.wait_until_terminate()\n' +
      '```\n' +
      '\n' +
      'Example 14: Start an agent server in remote machine.\n' +
      '\n' +
      'In summary, our framework enables distributed deployments to utilize the same workflow construction as local deployments, thereby simplifying the development of distributed applications and facilitating the seamless transition from local to distributed modes.\n' +
      '\n' +
      '## 8 Related Works\n' +
      '\n' +
      'The development of AgentScope aligns with the rapidly evolving landscape of frameworks that leverage large language models (LLMs) for the creation of language agents and multi-agent systems. Here, we briefly introduce works closely related to AgentScope from two sub-domains pertinent : Language Agent Frameworks, focusing on individual agent capabilities, and Multi-Agent Frameworks, emphasizing collaboration among multiple agents. For broader related works, readers can refer to (Wang et al., 2023; Xi et al., 2023).\n' +
      '\n' +
      'Language Agent FrameworksLanguage agent frameworks are pivotal for developing applications that can interpret and interact using human language.\n' +
      '\n' +
      'The Transformers library (Huggingface, 2023) has introduced a natural language API to interface with transformer models in its recent updates (_Transformers-Agents_). This API utilizes a set of customizable tools, allowing the model to interpret instructions and generate code snippets accordingly. It offers support for various open-source and proprietary model endpoints, catering to diverse developer needs. _LangChain_(Langchain-AI, 2023) provides a framework for building applications that are context-aware and capable of reasoning. It includes libraries and templates that facilitate the integration of multiple components into a unified cognitive architecture. LangServe and LangSmith extend the framework\'s capabilities by enabling deployment as a REST API and offering developer tools for debugging and monitoring chains built on any LLM framework. _AutoGPT_(AutoGPT-Team, 2023) illustrates a different approach, allowing an LLM to iteratively execute actions and make decisions. As a generalist agent, AutoGPT is not task-specific; it is designed to perform a variety of computer-based tasks, reflecting the adaptive nature of LLMs. _ModelScope-Agent_(Li et al., 2023a) is a customizable agent framework that harnesses open-source LLMs to perform tasks and connect with external APIs. It facilitates seamless integration with model APIs and common APIs while providing a comprehensive infrastructure for data collection, tool retrieval, and customized model training, all aiming to realize practical real-world applications.\n' +
      '\n' +
      'Multi-Agent FrameworksBuilding on the capabilities of individual agents, multi-agent frameworks explore collaboration and interaction among multiple agents to address complex tasks.\n' +
      '\n' +
      '_AutoGen_(Wu et al., 2023) provides a generic infrastructure that allows developers to program interaction patterns using both natural language and code. This framework enables the development of diverse applications by facilitating conversation among agents that are customizable and can utilize various combinations of LLMs, human inputs, and tools. _MetaGPT_(Hong et al., 2023) incorporates meta-programming to enhance multi-agent collaborations. By encoding Standardized Operating Procedures (SOP) into prompts, this framework ensures streamlined workflows and reduced errors, exemplifying effective task decomposition among agents. _AGENTS_(Zhou et al., 2023) is an open-source library that supports autonomous language agents with features like planning, memory, and multi-agent communication. It is designed to be user-friendly, helping non-specialists to deploy state-of-the-art language agents, and research-friendly, with a modularized design for extensibility. _OpenAgents_(Xie et al., 2023) provides an open platform for using language agents with practical functionalities accessible through a web interface. This framework emphasizes facilitating real-world agent interactions and includes specialized agents for different tasks, such as data analysis and web browsing. _ChatDev_(Qian et al., 2023) exploits LLMs for software development, creating a virtual chat-powered company that follows a waterfall model. It engages "software agents" at different stages of the development process, facilitating collaboration and context-aware communication. _CAMEL_(Li et al., 2023b) proposes a novel framework for autonomous cooperation among communicative agents using role-playing techniques, which allows for the generation of conversational data for studying agent behaviors and capabilities. Lastly, _AgentSims_(Lin et al., 2023) introduces a sandbox environment to evaluate LLMs in task-based scenarios, offering an infrastructure for researchers to test specific LLM capacities within a simulated environment.\n' +
      '\n' +
      'These frameworks represent significant strides in the use of LLMs for both individual and collaborative agent tasks. AgentScope is situated within this context, contributing by addressing the need for a user-friendly, fault-tolerant and versatile framework designed to manage complex interactions and processes inherent in multi-agent LLM systems. By focusing on ease of use and reliability, AgentScope aims to facilitate the creation of robust and versatile applications across diverse domains.\n' +
      '\n' +
      'Conclusion\n' +
      '\n' +
      'In this work, we propose AgentScope, a platform that stands at the forefront of multi-agent system development, synergizing user-centric design with the advanced capabilities of LLMs. Through its innovative communication and distributed mechanisms, AgentScope demonstrates its potential to boost collaboration amongst agents, enabling efficient, fault-tolerant operations and multi-modal interactions. By abstracting complexities and offering an array of development utilities, AgentScope substantially lowers the barriers to entry, fostering a more inclusive and creative community of developers.\n' +
      '\n' +
      'Looking forward, AgentScope opens numerous avenues for further research and development. Future work could delve into deeper integration of retrieval augmented generation, and explore adaptive communication protocols and interactive modals that evolve alongside task requirements. The platform\'s impact on accelerating the deployment of multi-agent systems across industries, from healthcare to customer service, promises to be profound, potentially leading to smarter and more responsive technologies that enhance human-machine collaboration. With AgentScope, we invite the broader research and development community to build upon our foundation, driving innovations that will shape the next generation of intelligent multi-agent applications.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Abadi et al. (2016) Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. {TensorFlow}: a system for {Large-Scale} machine learning. In _12th USENIX symposium on operating systems design and implementation (OSDI 16)_, pages 265-283, 2016.\n' +
      '* Autogpt (2023) AutoGPT-Team. Autogpt, 2023. URL [https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT).\n' +
      '* Betker et al. (2023) James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. _Computer Science_, 2(3):8, 2023.\n' +
      '* Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. _arXiv preprint arXiv:2305.14325_, 2023.\n' +
      '* Fu et al. (2019) Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. From language to goals: Inverse reinforcement learning for vision-based instruction following. In _7th International Conference on Learning Representations_, 2019.\n' +
      '* Hong et al. (2023) Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. _arXiv preprint arXiv:2308.00352_, 2023.\n' +
      '* Huggingface (2023) Huggingface. Transformers-agents, 2023. URL [https://huggingface.co/docs/transformers/transformers_agents](https://huggingface.co/docs/transformers/transformers_agents).\n' +
      '* Langchain (2023) Langchain-AI. Langchain, 2023. URL [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain).\n' +
      '* Li et al. (2023a) Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, et al. Modelscope-agent: Building your customizable agent system with open-source large language models. _arXiv preprint arXiv:2309.00986_, 2023a.\n' +
      '* Li et al. (2023b) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large scale language model society. _arXiv preprint arXiv:2303.17760_, 2023b.\n' +
      '* Lin et al. (2023) Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen. Agentsims: An open-source sandbox for large language model evaluation. _arXiv preprint arXiv:2308.04026_, 2023.\n' +
      '* OpenAI (2023) OpenAI. GPT-4 technical report. _CoRR_, abs/2303.08774, 2023.\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In _Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems_, 2022.\n' +
      '* Park et al. (2023) Joon Sung Park, Joseph C. O\'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, pages 2:1-2:22, 2023.\n' +
      '* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* Qian et al. (2023) Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. _arXiv preprint arXiv:2307.07924_, 2023.\n' +
      '* Qian et al. (2023)* Rawte et al. (2023) Vipula Rawte, Amit P. Sheth, and Amitava Das. A survey of hallucination in large foundation models. _CoRR_, abs/2309.05922, 2023.\n' +
      '* Su et al. (2023) Weijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li, Gao Huang, Yu Qiao, Xiaogang Wang, Jie Zhou, and Jifeng Dai. Towards all-in-one pre-training via maximizing multi-modal mutual information. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15888-15899, 2023.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _CoRR_, abs/2302.13971, 2023a.\n' +
      '* Touvron et al. (2021) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. _CoRR_, abs/2307.09288, 2023b.\n' +
      '* Wang et al. (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. _arXiv preprint arXiv:2308.11432_, 2023.\n' +
      '* Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. _arXiv preprint arXiv:2308.08155_, 2023.\n' +
      '* Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. _arXiv preprint arXiv:2309.07864_, 2023.\n' +
      '* Xie et al. (2023) Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. Openagents: An open platform for language agents in the wild. _arXiv preprint arXiv:2310.10634_, 2023.\n' +
      '* Zhang et al. (2023a) Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. Instruction tuning for large language models: A survey. _CoRR_, abs/2308.10792, 2023a.\n' +
      '* Zhang et al. (2023b) Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren\'s song in the AI ocean: A survey on hallucination in large language models. _CoRR_, abs/2309.01219, 2023b.\n' +
      '* Zhou et al. (2023) Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, et al. Agents: An open-source framework for autonomous language agents. _arXiv preprint arXiv:2309.07870_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:21]\n' +
      '\n' +
      '* [8] player1: I see your point about Player4. They could be at Threat if they are the user or witch. But I still think Player3 is a strong player and could figure us out.\n' +
      '* [9] Player2: Alright, let\'s go with Player3. We can consider Player4 next time.\n' +
      '* [10] Moderator: Which player do you vote to kill? Response in the following format\n' +
      '* [11] {\n' +
      '* [12] "thought": "thought".\n' +
      '* [13] "speak": "player_name"\n' +
      '* [14] }\n' +
      '* [15] player1: Player3\n' +
      '* [16] Player2: Player3\n' +
      '* [17] Moderator: Theplayer with the most votes is Player3.\n' +
      '* [18] Moderator: Player6, you\'re switch. Tonight Player3 is eliminated. Would you like to resurrect Player3? Response in the following format which can be loaded by python json.loads()\n' +
      '* [19] {\n' +
      '* [20] "thought": "thought",\n' +
      '* [21] "speak": "thoughts summary tosay",\n' +
      '* [22] "resurrect": "true/false"\n' +
      '* [23] }\n' +
      '* [24] player6: I\'m deeply saddened by the loss of Player3. It\'s a tough decision to make.\n' +
      '* [25] Moderator: Player5, you\'reeser. Which player in Player1, Player2, Player3,\n' +
      '* [26] Player4, Player5 and Player6 would you like to check tonight? Response in the following json format which can be loaded by python json.loads()\n' +
      '* [27] "thought": "thought",\n' +
      '* [28] "speak": "player_name"\n' +
      '* [29] }\n' +
      '* [30] Player5: Player1\n' +
      '* [31] Moderator: Okay, the role of Player1 is werewolf.\n' +
      '* [32] Moderator: The day is coming, all theplayersoppenyoureyes. Lastnight is peaceful, no player is eliminated.\n' +
      '* [33] Moderator: Now the alive players are Player1, Player2, Player3, Player4, Player5 and Player6. Given the game rules and your role, based on the situation and the information you gain, to vote a player eliminated among aliveplayers and to win the game, what do your want to say to others? You can decide whether to reveal your role. Response in the following JSON format which can be loaded by python json.loads()\n' +
      '* [34] {\n' +
      '* [35] "thought": "thought",\n' +
      '* [36] "speak": "thought summary tosay toothers"\n' +
      '* [37] }\n' +
      '* [38] Player1: I\'m glad that we all made it through the night. I think we should keep an eyeon Player4. They\'ve been pretty quiet, which could mean they\'re trying to avoid suspicion.\n' +
      '* [39] Player2: I agree with Player1. Player4 has been unusually quiet. Maybethey\'re trying to avoid drawing attention to themselves.\n' +
      '* [40] players: I see your point about Player4 but it\'s still early in the game. We should be careful not to jump to conclusions without more evidence.\n' +
      '* [41] Player4: I understand your concerns, but I assure you, I\'m just a willager. I think we should focus on gathering more information before we start pointing / fingers. If the serm has any insights, now might be a good time to share.\n' +
      '* [42] Player5: I understand everyone\'s concerns about Player4, but I think we should / also keep an eyeon Player1. They were quick to cast suspicion onothers, which could be a strategy to divert attention from themselves.\n' +
      '* [43] Player6: I understand the concerns about Player4, but I also think we should be cautious about Player1 and Player2. They were quick to agree with each other, which could be a strategy to divert attention from themselves. Let\'s gather more information before we make any decisions.\n' +
      '* [44] Moderator: Now the alive players are Player1, Player2, Player3, Player4, Player5\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:23]\n' +
      '\n' +
      '* [92] Player8: I agree with Player3 and Player8. Player2 was quick to cast suspicion on Player4, who turned out to be innocent. I also noticed that Player2 and 4 Player1 were often in agreement, which could mean they were both werewolves\n' +
      '* [93] trying to protect each other. I think we should vote out Player2.\n' +
      '* [94] Moderator: Now the alive players are Player2, Player3, Player5 and Player6.\n' +
      '* [95] Given the game rules and your role, based on the situation and the information\n' +
      '* [96] you gain, to vote a player among alive players and to win the game, it\'s time\n' +
      '* [97] to vote one player among the alive players, please cast your vote on who you\n' +
      '* [98] believe is a werewolf. Response in the following format which can be loaded by\n' +
      '* [99] python json.loads()\n' +
      '* [99]\n' +
      '* [99] "thought": "thought",\n' +
      '* [99] "speak": "player_name"\n' +
      '* [99] "\n' +
      '* [99] Player2: Player5\n' +
      '* [99] Player3: Player2\n' +
      '* [99] Player6: Player2\n' +
      '* [99] "Moderator: Player2 has beenvoted out.\n' +
      '* [100] Moderator: The game is over. The werewolves have been defeated, and the village\n' +
      '* [99] "is safe once again!"\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>