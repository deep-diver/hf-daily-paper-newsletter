<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Eva-clip-18b: Scaling CLIP to 18 Billion Parameters\n' +
      '\n' +
      'Quan Sun1\n' +
      '\n' +
      'Correspondence to _wangxinlong@baai.ac.cn_\n' +
      '\n' +
      'Jinsheng Wang1\n' +
      '\n' +
      'Correspondence to _wangxinlong@baai.ac.cn_\n' +
      '\n' +
      'Qiying Yu1,2\n' +
      '\n' +
      'Correspondence to _wangxinlong@baai.ac.cn_\n' +
      '\n' +
      'Yufeng Cui1\n' +
      '\n' +
      'Fan Zhang1\n' +
      '\n' +
      'Xiaosong Zhang1\n' +
      '\n' +
      'Xinlong Wang1\n' +
      '\n' +
      '1 Beijing Academy of Artificial Intelligence\n' +
      '\n' +
      '2 Tsinghua University\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Scaling up contrastive language-image pretraining (CLIP) is critical for empowering both vision and multimodal models. We present EVA-CLIP-18b, the largest and most powerful open-source CLIP model to date, with 18-billion parameters. With only 6-billion training samples seen, EVA-CLIP-18b achieves an exceptional **80.7**%_ zero-shot top-1 accuracy averaged across 27 widely recognized image classification benchmarks, outperforming its forerunner _EVA-CLIP (5-billion parameters) and other open-source CLIP models by a large margin. Remarkably, we observe a consistent performance improvement with the model size scaling of EVA-CLIP, despite maintaining a constant training dataset of 2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B) employed in other state-of-the-art CLIP models. EVA-CLIP-18b demonstrates the potential of EVA-style [30, 29, 63] weak-to-strong visual model scaling. With our model weights made publicly available, we hope to facilitate future research in vision and multimodal foundation models.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Recent years witnessed the rapid growth of Large Multimodal Models (LMMs) [3, 64, 62, 69, 5, 46], with CLIP models [53, 19, 63, 43, 75, 28, 17] serving as a foundational vision encoder to deliver robust and transferable visual representations, and Large Language Models (LLMs) [65, 54] serving as a general interface for reasoning across different modalities. However, as LLMs have scaled up to around 100B parameters or higher [11, 20, 65], the adopted vision foundation models continue to operate at a much smaller scale, lagging far behind the LLMs.\n' +
      '\n' +
      'This paper introduces EVA-CLIP-18b, the largest open-source CLIP model with 18-billion parameters to narrow this gap. EVA-CLIP[63] open-sources a series of effective and efficient CLIP models, which have been leveraged as the vision foundation by numerous impactful works across 2D / 3D vision and multimodal modeling [42, 78, 77, 50, 69, 64]. We further scale up EVA-CLIP to this significant parameter size building upon the scaling philosophy of EVA[30, 29] and EVA-CLIP[63]. With merely 6-billion training samples seen and trained on publicly available datasets, EVA-CLIP-18b achieves the exceptional **80.7**%_ average zero-shot top-1 accuracy on 27 widely recognized image classification benchmarks, significantly surpassing its forerunner -C1IP-E/14+ (5-billion parameters) and other open-source CLIP models. Besides, the models _have not exhibited any\n' +
      '\n' +
      'Figure 1: **Scaling behavior of EVA-CLIP with zero-shot classification performance averaged across 27 image classification benchmarks, compared with the current state-of-the-art and largest CLIP models (224px). The diameter of each circle demonstrates the forward GFLOPs \\(\\times\\) the number of training samples seen. The performance of EVA-CLIP consistently improves as scaling up.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:2]\n' +
      '\n' +
      'cally, we employ a pre-trained EVA-18B [30, 29] as the vision encoder and EVA-02-CLIP-E/14+ [63] for the text encoder. We adopt the LAMB optimizer [73] with \\(\\beta_{1}\\) = 0.9, \\(\\beta_{2}\\)=0.95, and a weight decay of 0. We apply different learning rates and layer decay rates to the vision encoder and text encoder to ensure optimal training. We set the peak learning rate as 4e-4 and 4e-5 for the vision encoder and the text encoder respectively, with 2000 warm-up steps. Afterwards, the learning rates decay to 0 with a cosine schedule. The learning rate layer decay rates are configured as 0.9 and 0.75 for the vision and text encoders. The temperature parameter remains constant at 0.01. Further, we use the DeepSpeed optimization library [56] with ZeRO stage-3 partition [55], gradient checkpointing [16] and flash attention [24] to optimize the training cost.\n' +
      '\n' +
      '**Dataset.** Our Merged-2B dataset consists of 1.6 billion samples from LAION-2B [58] and 0.4 billion samples from COYO-700M [12]. Note that the use of a subset from LAION-2B is not the result of deliberate filtering, but rather due to image downloading failures. The use of 0.4 billion COYO-700M samples aims to complement the number of training samples to nearly the same as LAION-2B. Merged-2B+ consists of all samples from Merged-2B, along with additional 20 million samples from LAION-COCO [1] and 23 million samples from Merged-video including VideoCC [48], InternVid [70] and WebVid-10M [6]. Merged-video is included at the end of the training process.\n' +
      '\n' +
      'EVA-CLIP-18B pre-trains with 5.4 billion samples from Merged-2B seen with 50% of patch dropout ratio [44], 0.6 billion samples from Merged-2B and 20 million samples from LAION-COCO without patch dropout, and 24 million samples from Merged-video with 50% of patch dropout ratio.\n' +
      '\n' +
      '**Evaluation.** We evaluate on 33 widely used datasets across image, video classification and image-text retrieval. All datasets used to evaluate EVA-CLIP-18B are reported in Table 11. We utilize the specified prompt templates following [53, 38].\n' +
      '\n' +
      '**Zero-Shot Image Classification.** We show the exceptional performance of EVA-CLIP on all 27 zero-shot image classification benchmarks in Table 2. EVA-CLIP-18B achieves **80.7%** top-1 accuracy averaged across all 27 benchmarks. These results significantly outperform the previous best open-sourced DFN5B-CLIP-H/14+ [28] by +1.5%, and the largest existing CLIP model, InternVL-C [17], by +2.7%. For Bird-snap dataset, the download was limited to 2195 test images due to broken links.\n' +
      '\n' +
      '**Zero-Shot Video Classification.** We report the top-1 accuracy for UCF-101 [60] and the mean of top-1 and top-5 accuracy for Kinetics-400 [15], Kinetics-600 [13] and Kinetics-700 [14]. In Table 5 we demonstrate that EVA-CLIP-18B also outperforms other CLIP models on zero-shot video clas\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c|c c c}  & \\multicolumn{3}{c|}{image encoder} & \\multicolumn{3}{c|}{text encoder} & \\multicolumn{3}{c}{\\# params} \\\\ method & \\multicolumn{3}{c|}{layers with head} & \\multicolumn{3}{c|}{layers with heads} & \\multicolumn{3}{c}{image text} & \\multicolumn{3}{c}{total} \\\\ \\hline EVA-CLIP-SB & 32 & 4096 & 32 & 32 & 1280 & 20 & 7.5B & 695M & 8.1B \\\\ EVA-CLIP-18B & 48 & 5120 & 40 & 32 & 1280 & 20 & 17.5B & 695M & 18.1B \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Architecture configurations.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c|c c c c|c c c}  & \\multicolumn{4}{c|}{zero-shot **text** retrieval} & \\multicolumn{4}{c|}{zero-shot **image** retrieval} & \\multicolumn{4}{c}{} \\\\  & \\multicolumn{3}{c|}{Flickr30K} & \\multicolumn{3}{c|}{COCO} & \\multicolumn{3}{c|}{Flickr30K} & \\multicolumn{3}{c}{COCO} \\\\ method & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & MR \\\\ \\hline EVA-01-CLIP-9/14 & 87.9 & 98.0 & 99.5 & 61.7 & 83.2 & 89.9 & 72.5 & 91.5 & 95.4 & 44.5 & 69.1 & 77.7 & 80.9 \\\\ EVA-01-CLIP-9/14+ & 93.3 & 99.5 & 99.9 & 69.4 & 88.3 & 93.2 & 79.2 & 95.2 & 97.3 & 51.1 & 74.7 & 82.5 & 85.3 \\\\ OpenCLIP-G/14 & 93.5 & 99.3 & 99.7 & 69.0 & 87.8 & 93.1 & 80.9 & 95.1 & 97.2 & 52.6 & 76.1 & 83.6 & 85.7 \\\\ EVA-02-CLIP-E/14+ & 94.3 & 99.6 & 99.8 & 69.4 & 88.6 & 93.3 & 79.7 & 94.9 & 97.3 & 52.5 & 75.9 & 83.4 & 85.7 \\\\ EVA-CLIP-8B & 95.6 & 99.6 & 99.9 & 70.3 & 89.3 & 93.9 & 80.8 & 95.5 & 97.6 & 53.0 & 76.0 & 83.4 & 86.2 \\\\ DFN5B-CLIP-H/14 & 92.9 & 99.3 & 99.9 & 72.3 & 90.2 & 94.6 & 80.1 & 95.2 & 97.3 & 53.9 & 78.0 & 85.6 & 86.6 \\\\ InternVL-C & 93.8 & **99.7** & **100.0** & 70.3 & 89.2 & 93.8 & 82.1 & 96.0 & 98.1 & 54.1 & 77.1 & 84.8 & 86.6 \\\\ DFN5B-CLIP-H/14+ & 93.6 & 99.3 & 99.6 & 71.8 & 90.4 & 94.9 & 82.1 & 96.0 & 97.9 & 55.6 & **79.2** & **86.3** & 87.2 \\\\ EVA-CLIP-18B & **96.7** & **99.7** & **100.0** & **73.6** & **90.9** & **95.0** & **83.3** & **96.3** & **98.3** & **56.2** & 78.5 & 85.6 & **87.8** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Zero-shot retrieval performance on Flickr30K [74] and COCO [45].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c|c}  & \\multicolumn{3}{c|}{image encoder} & \\multicolumn{3}{c|}{text encoder} & \\multicolumn{3}{c}{\\# params} \\\\ method & \\multicolumn{3}{c|}{layers with heads} & \\multicolumn{3}{c|}{layers with heads} & \\multicolumn{3}{c}{image text} \\\\ \\hline EVA-CLIP-SB & 32 & 4096 & 32 & 32 & 1280 & 20 & 7.5B & 695M & 8.1B \\\\ EVA-CLIP-18B & 48 & 5120 & 40 & 32 & 1280 & 20 & 17.5B & 695M & 18.1B \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Architecture configurations.\n' +
      '\n' +
      'sification benchmarks by a large margin. When sampling a single center frame per video, EVA-CLIP-18B achieves accuracies of 86.0%, 72.9%, 72.9%, and 68.2% across the four evaluated benchmarks. Further, when uniformly sample 8 or 16 frames per video, we observe an improvement of +4.7% / +4.8% averaged across four benchmarks compared to the single-frame setting.\n' +
      '\n' +
      '**Zero-Shot Image-Text Retrieval.** In Table 3, we report the zero-shot image and text retrieval results on Flickr30K [74] and COCO [45]. EVA-CLIP-18B achieves an average recall of 87.8% across all retrieval benchmarks, significantly outperforming competitors.\n' +
      '\n' +
      '**Robustness.** In Table 6, we demonstrate that scaling up EVA-CLIP significantly enhances the robustness of visual representations. EVA-CLIP suffers from the smallest performance drop (\\(\\Delta\\downarrow\\)) between ImageNet-1K and ImageNet variants including adversarial ones, with merely 0.2% top-1 accuracy gap for EVA-CLIP-18B.\n' +
      '\n' +
      'For a more robust and comprehensive evaluation of robustness and zero-shot performance, it is advisable to include more benchmarks covering more image distributions. However, we want to note that higher ImageNet top-1 accuracy does not necessarily lead to better overall performance, as evidenced in Table 6, where BASIC-L [52] exhibits higher ImageNet-related top-1 accuracy but considerably lower overall average top-1 accuracy compared to EVA-CLIP-18B across a broader range of datasets and distributions, showing a difference of -8.2%.\n' +
      '\n' +
      '**Linear Probing on ImageNet-1K.** In Table 7, we present the results of linear probing on ImageNet-1K [26]. EVA-CLIP-18B achieves an average top-1 accuracy of 88.9%, surpassing InternVL-C [17] by 0.7%.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c c c c c c} method & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline BASIC-L [52] (reported) & 85.7 & 80.6 & 85.6 & 95.7 & 76.1 & 82.3 & 97.5 & 82.3 & 40.3 & 76.2 & 59.2 & 64.6 & 51.0 & 95.1 & 59.6 & 72.7 & 99.6 & 76.7 (77.8) \\\\ EVA-CLIP-18B & 83.8 & 77.9 & 87.3 & 95.7 & 74.7 & 82.2 & 99.4 & 93.8 & 83.0 & 77.7 & 79.9 & 72.1 & 79.8 & 95.8 & 65.2 & 76.9 & 99.6 & 84.9 (84.1) \\\\  & **-1.9** & **-2.7** & **+1.7** & **+0.0** & **-1.4** & **-0.1** & **+1.9** & **+11.5** & **+42.7** & **+1.5** & **+20.7** & **+7.5** & **+28.8** & **+0.7** & **+5.6** & **+4.2** & **+0.0** & **+8.2 (+6.3)** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: **Robustness evaluation of CLIP models and comparison with BASIC-L [52] on 17 Benchmarks.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c} method & \\#param & \\(\\text{wpl acc.}\\) \\\\ \\hline OpenCLIP-G/14 (reported) & 1.8B & 86.2 \\\\ EVA-01-CLIP-g/14 & 1.0B & 86.5 \\\\ EVA-02-CLIP-E/14+ & 4.4B & 88.1 \\\\ InternVL-C (reported) & 5.9B & 88.2 \\\\ EVA-CLIP-SB & 7.5B & 88.5 \\\\ EVA-CLIP-18B & 17.5B & 88.9 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: **Linear Probing on ImageNet-1K [26].** The top-1 accuracy shows a continuous improvement with the scaling up of EVA-CLIP.\n' +
      '\n' +
      '**3D Representation.** We adopt the Uni3D [77] setting to explore the effectiveness of scaling up teachers. With the scaling up of EVA-CLIP in Table 8, we observe consistent improvements in 3D representation learning capabilities. Further, Uni3D-base equipped with EVA-CLIP-18B sets new records on ModelNet [71] and ScanObjectNN [66] benchmarks.\n' +
      '\n' +
      '## 4 Ablation Studies\n' +
      '\n' +
      '**Video Data.** In Table 9, we conduct ablations on EVAC-CLIP-18B\'s zero-shot performance, comparing results when trained with and without Merged-Video. The training objective for the video data aligns with that of images, encompassing the extraction of features from video where 8 frames are uniformly sampled. The mean of all [CLS] embeddings serves as a representation for the video. The outcomes reveal substantial performance improvements associated with training using Merged-Video. The zero-shot performance, averaged across UCF-101 [60] and Kinetics-400 [15] / 600 [13] / 700 [14], indicates a gain of +0.7 for evaluation with one middle frame and +0.8 for evaluation with 8 frames.\n' +
      '\n' +
      '**Image Resolution.** In Table 10, we investigate the impact of larger image resolutions on zero-shot performance. Notably, there is an average top-1 accuracy gain of +0.9 when the resolution increases from \\(224^{2}\\) to \\(448^{2}\\) for EVA-CLIP-8B. Similarly, an increase from \\(224^{2}\\) to \\(336^{2}\\) results in a gain of +0.5, even when trained with low global batch sizes of 24k for EVA-CLIP-8B + and 23k for EVA-CLIP-18B+.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'We present EVA-CLIP-18B, the currently largest and most performant open-sourced CLIP model with 18-billion parameters. We show that following EVA\'s weak-to-strong vision scaling principle, we can further scale up CLIP models to a new record and advance SOTA on multiple prevalent benchmarks across image, video and 3D domains. Importantly, we demonstrate that scaling up the size of EVA-CLIP models consistently boosts performance with no sign of saturation, shedding light on future vision model scaling.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] L. coco: 600m synthetic captions from laino2b-en. [https://laino.ai/blog/lain-coco/](https://laino.ai/blog/lain-coco/). Cited by: SS1.\n' +
      '* [2]R. 80 zero-shot accuracy with openclip: Vit-g/14 trained on laino-2b. [https://laino.ai/blog/giant-openclip/](https://laino.ai/blog/giant-openclip/). Cited by: SS1.\n' +
      '* [3]J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou (2023) Qowen-vl: a versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966. Cited by: SS1.\n' +
      '* [4]M. Bain, A. Nagrani, G. Varol, and A. Zisserman (2021) Frozen in time: a joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1728-1738. Cited by: SS1.\n' +
      '* [5]H. Bao, L. Dong, and F. Wei (2021) Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254. Cited by: SS1.\n' +
      '* [6]A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz (2019) ObjectNet: a large-scale bias-controlled dataset for pushing the limits of object recognition models. In NeurIPS, Cited by: SS1.\n' +
      '* [7]T. Berg, J. Liu, S. W. Lee, M. L. Alexander, D. W. Jacobs, and P. N. Belthumeur (2014) BirdsMap: large-scale fine-grained visual categorization of birds. In CVPR, Cited by: SS1.\n' +
      '* [8]L. Bossard, M. Guillaumin, and L. Van Gool (2014) Food-101-mining discriminative components with random forests. In ECCV, Cited by: SS1.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c|c} \\hline method & resolution & IN-1k & IN-A & IN-B & IN-V2 & IN-Ske. & ObjectNet & **avg.** \\\\ \\hline \\hline EVA-CLIP-8B & 224\\(\\times\\)224 & 83.5 & 85.2 & 95.3 & 77.7 & 74.3 & 81.2 & 82.9 \\\\ EVA-CLIP-8B + & 448\\(\\times\\)448 & 83.8 & 88.7 & 95.4 & 77.7 & 74.1 & 82.9 & 83.8 \\\\  & & +0.3 & +3.5 & +0.1 & +0.0 & **-0.2** & +1.7 & +0.9 \\\\ \\hline \\hline EVA-CLIP-18B & 224\\(\\times\\)2224 & 83.8 & 87.3 & 95.7 & 77.9 & 74.7 & 82.2 & 83.6 \\\\ EVA-CLIP-18B+ & 336\\(\\times\\)336 & 83.9 & 88.9 & 95.6 & 78.2 & 74.3 & 83.6 & 84.1 \\\\  & & +0.1 & +1.6 & **-0.1** & +0.3 & **-0.4** & +1.4 & +0.5 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 10: **Increasing resolution.** We report zero-shot performance on ImageNet variants and ObjectNet.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c}  & \\multicolumn{3}{c|}{classification} & retrieval \\\\  & image video (\\#F 1) & video (\\#F 8) & avg. recall \\\\ \\hline w/o video data & 80.7 & 74.3 & 78.9 & 87.9 \\\\ w/ video data & 80.7 & 75.0 (+0.7) & 79.7 (+0.8) & 87.8 (-0.1) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: **Video data enhances zero-shot video classification performance**. We respectively report performances averaged on 27 image classification benchmarks, 4 video benchmarks and 2 image-text retrieval benchmarks.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c}  & \\multicolumn{3}{c|}{classification} & retrieval \\\\  & image video (\\#F 1) & video (\\#F 8) & avg. recall \\\\ \\hline w/o video data & 80.7 & 74.3 & 78.9 & 87.9 \\\\ w/ video data & 80.7 & 75.0 (+0.7) & 79.7 (+0.8) & 87.8 (-0.1) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: **Video data enhances zero-shot video classification performance**. We respectively report performances averaged on 27 image classification benchmarks, 4 video benchmarks and 2 image-text retrieval benchmarks.\n' +
      '\n' +
      '* [11] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. _arXiv preprint arXiv:2005.14165_, 2020.\n' +
      '* [12] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. [https://github.com/kakaobrain/coyo-dataset](https://github.com/kakaobrain/coyo-dataset), 2022.\n' +
      '* [13] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600. _arXiv preprint arXiv:1808.01340_, 2018.\n' +
      '* [14] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700 human action dataset. _arXiv preprint arXiv:1907.06987_, 2019.\n' +
      '* [15] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In _CVPR_, 2017.\n' +
      '* [16] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost, 2016.\n' +
      '* [17] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Intervl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. _arXiv preprint arXiv:2312.14238_, 2023.\n' +
      '* [18] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. _Proceedings of the IEEE_, 2017.\n' +
      '* [19] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning, 2022.\n' +
      '* [20] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sash Tsysshchenko, Joshua Maynez, Abhishek Rao, Parser Barnes, Yi Tay, Noam Shazezev, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denmy Zhou, Daphne Ippolito, David Luan, Hyonnetk Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanuulayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slay Petrov, and Noah Friedel. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n' +
      '* [21] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed,, and A. Vedaldi. Describing textures in the wild. In _CVPR_, 2014.\n' +
      '* [22] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. _arXiv preprint arXiv:2003.10555_, 2020.\n' +
      '* [23] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In _AISTAT_, 2011.\n' +
      '* [24] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022.\n' +
      '* [25] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Anirududha Kembhavi, and Ali Farhadi. Obiayverse: A universe of annotated 3d objects. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13142-13153, 2023.\n' +
      '* [26] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.\n' +
      '* [27] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. "The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. "[http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html](http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html)", 2007.\n' +
      '* [28] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. _arXiv preprint arXiv:2309.17425_, 2023.\n' +
      '* [29] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A visual representation for neon genesis. _arXiv preprint arXiv:2303.11331_, 2023.\n' +
      '* [30] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. _arXiv preprint arXiv:2211.07636_, 2022.\n' +
      '* [31] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In _CVPRW_, 2004.\n' +
      '* [32] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Emetzari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shujaran Song, Hannaneh Hajishirzi, Ali Farhadi, Roman Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets. _arXiv preprint arXiv:2304.14108_, 2023.\n' +
      '* [33] Ian J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun Lee, et al. Challenges in representation learning: A report on three machine learning contests. In _ICONIP_, 2013.\n' +
      '* [34] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. _IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens._, 2019.\n' +
      '* [35] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorndon, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In _CVPR_, 2021.\n' +
      '* [36] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In _CVPR_, 2021.\n' +
      '\n' +
      '* [37] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In _ECCV_, 2016.\n' +
      '* [38] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip. [https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip), 2021.\n' +
      '* [39] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _ICCVW_, 2013.\n' +
      '* [40] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n' +
      '* [41] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 1998.\n' +
      '* [42] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [43] Xianhang Li, Zeyu Wang, and Cihang Xie. Clipa-v2: Scaling clip training with 81.1 _arXiv preprint arXiv:2306.15658_, 2023.\n' +
      '* [44] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking, 2022.\n' +
      '* [45] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _European conference on computer vision_, pages 740-755. Springer, 2014.\n' +
      '* [46] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '* [47] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. _arXiv preprint arXiv:1306.5151_, 2013.\n' +
      '* [48] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja Hauth, Santiago Manen, Chen Sun, and Cordelia Schmid. Learning audiovideo modalities from image captions, 2022.\n' +
      '* [49] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _ICVGIP_, 2008.\n' +
      '* [50] Ting Pan, Luh Tang, Xinlong Wang, and Shiguang Shan. Tokenize anything via prompting. _arXiv preprint arXiv:2312.09128_, 2023.\n' +
      '* [51] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In _CVPR_, 2012.\n' +
      '* [52] Hieu Pham, Zihang Dai, Golnar Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and Quoc V Le. Combined scaling for zero-shot transfer learning. _arXiv preprint arXiv:2111.10050_, 2021.\n' +
      '* [53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* [54] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. _JMLR_, 2020.\n' +
      '* [55] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In _SC20_, 2020.\n' +
      '* [56] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In _KDD_, 2020.\n' +
      '* [57] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet?, 2019.\n' +
      '* [58] Christoph Schulmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _arXiv preprint arXiv:2210.08402_, 2022.\n' +
      '* [59] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatuszaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [60] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. _arXiv preprint arXiv:1212.0402_, 2012.\n' +
      '* [61] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. _Neural networks_, 2012.\n' +
      '* [62] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. _arXiv preprint arXiv:2312.13286_, 2023.\n' +
      '* [63] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. _arXiv preprint arXiv:2303.15389_, 2023.\n' +
      '* [64] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. _arXiv preprint arXiv:2307.05222_, 2023.\n' +
      '* [65] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [66] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1588-1597, 2019.\n' +
      '* [67] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. In _MICCAI_, 2018.\n' +
      '* [68] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. _NeurIPS_, 2019.\n' +
      '* [69] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xiauxuan Song, et al. Cogytlm: Visual expert for pretrained language models. _arXiv preprint arXiv:2311.03079_, 2023.\n' +
      '* [70] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Conghui He, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: A large-scale video-text dataset for multimodal understanding and generation, 2024.\n' +
      '\n' +
      '* [71] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1912-1920, 2015.\n' +
      '* [72] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In _CVPR_, 2010.\n' +
      '* [73] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes, 2019.\n' +
      '* [74] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _TACL_, 2014.\n' +
      '* [75] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. _arXiv preprint arXiv:2303.15343_, 2023.\n' +
      '* [76] Biao Zhang and Rico Sennrich. Root mean square layer normalization. _arXiv preprint arXiv:1910.07467_, 2019.\n' +
      '* [77] Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. Uniq: Exploring unified 3d representation at scale. _arXiv preprint arXiv:2310.06773_, 2023.\n' +
      '* [78] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minipt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:9]\n' +
      '\n' +
      'we present results obtained by selecting the best-performing transformation between the two. In the case of zero-shot retrieval tasks, we specifically choose the transformation that involves direct resizing of images to a fixed size.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 14: **Impact of image transformations on zero-shot evaluation. †\\(\\dagger\\)** denotes the direct resizing of images to a fixed size, while * indicates resizing images based on the shortest side and subsequently center cropping to achieve a fixed size.**\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>