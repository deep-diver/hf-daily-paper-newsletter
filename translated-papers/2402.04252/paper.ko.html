<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Eva-clip-18b: CLIP를 180억 파라미터로 스케일링\n' +
      '\n' +
      'Quan Sun1\n' +
      '\n' +
      '_wangxinlong@baai.ac.cn_에 대한 대응\n' +
      '\n' +
      'Jinsheng Wang1\n' +
      '\n' +
      '_wangxinlong@baai.ac.cn_에 대한 대응\n' +
      '\n' +
      'Qiying Yu1,2\n' +
      '\n' +
      '_wangxinlong@baai.ac.cn_에 대한 대응\n' +
      '\n' +
      'Yufeng Cui1\n' +
      '\n' +
      'Fan Zhang1\n' +
      '\n' +
      'Xiaosong Zhang1\n' +
      '\n' +
      'Xinlong Wang1\n' +
      '\n' +
      '1 베이징 인공지능 아카데미\n' +
      '\n' +
      '칭화대학교\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대비 언어 이미지 사전 훈련(CLIP)을 확장하는 것은 시각 및 멀티모달 모델 모두에 권한을 부여하는 데 중요하다. 우리는 180억 개의 매개변수를 가진 현재까지 가장 크고 강력한 오픈 소스 CLIP 모델인 EVA-CLIP-18b를 제시한다. 60억 개의 훈련 샘플만 볼 수 있는 EVA-CLIP-18b는 널리 인식된 27개의 이미지 분류 벤치마크에서 평균화된 예외적인 **80.7**%_ 제로 샷 탑-1 정확도를 달성하여 선도자 _EVA-CLIP(50억 매개 변수) 및 기타 오픈 소스 CLIP 모델을 큰 마진으로 능가한다. 놀랍게도, 우리는 LAION-2B와 COYO-700M에서 20억 개의 이미지-텍스트 쌍의 일정한 훈련 데이터 세트를 유지함에도 불구하고 EVA-CLIP의 모델 크기 스케일링으로 일관된 성능 향상을 관찰했다. 이 데이터 세트는 공개적으로 사용할 수 있으며 다른 최신 CLIP 모델에 사용되는 사내 데이터 세트(예: DFN-5B, WebLI-10B)보다 훨씬 작다. EVA-CLIP-18b는 EVA 스타일 [30, 29, 63] 약한 대 강한 시각적 모델 스케일링의 잠재력을 보여준다. 공개적으로 사용할 수 있는 모델 가중치를 사용하여 비전 및 멀티모달 기반 모델에 대한 향후 연구를 촉진하기를 바랍니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 몇 년 동안 LMM(Large Multimodal Models)[3, 64, 62, 69, 5, 46]의 급속한 성장을 목격했으며 CLIP 모델[53, 19, 63, 43, 75, 28, 17]은 견고하고 전달 가능한 시각적 표현을 전달하는 기초 비전 인코더 역할을 하고 LLM(Large Language Models)[65, 54]는 다양한 모달리티에 걸쳐 추론을 위한 일반적인 인터페이스 역할을 한다. 그러나 LLM이 약 100B 매개변수 이상[11, 20, 65]으로 확장됨에 따라 채택된 비전 기반 모델은 LLM보다 훨씬 뒤처져 훨씬 더 작은 규모로 계속 작동한다.\n' +
      '\n' +
      '본 논문은 이러한 격차를 줄이기 위해 180억 매개 변수를 가진 가장 큰 오픈 소스 CLIP 모델인 EVA-CLIP-18b를 소개한다. EVA-CLIP[63] 오픈 소스는 2D/3D 비전과 멀티모달 모델링[42, 78, 77, 50, 69, 64]에 걸쳐 수많은 임팩트 작업에 의해 비전 기반으로서 활용되어 온 일련의 효과적이고 효율적인 CLIP 모델을 제공한다. 우리는 EVA[30, 29] 및 EVA-CLIP[63]의 스케일링 철학을 기반으로 EVA-CLIP를 이 중요한 매개변수 크기 구축으로 추가로 확장한다. 공개된 데이터 세트에서 보여지고 훈련된 60억 개의 훈련 샘플만으로 EVA-CLIP-18b는 널리 인식된 27개의 이미지 분류 벤치마크에서 예외적인 **80.7**%_ 평균 제로 샷 상위 1 정확도를 달성하여 선구자 -C1IP-E/14+(50억 매개 변수) 및 기타 오픈 소스 CLIP 모델을 크게 능가한다. 게다가, 모델들은 아무것도 보여주지 않았다.\n' +
      '\n' +
      '그림 1: 현재 최첨단 및 최대 CLIP 모델(224px)과 비교하여 27개의 이미지 분류 벤치마크에 걸쳐 평균화된 제로 샷 분류 성능을 갖는 EVA-CLIP의 **스케일링 거동. 각 원의 직경은 정방향 GFLOPs\\(\\times\\)의 훈련 샘플 수를 보여준다. EVA-CLIP의 성능은 스케일링 업에 따라 지속적으로 개선된다.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:2]\n' +
      '\n' +
      '시각 인코더는 사전 훈련된 EVA-18B [30, 29]를 사용하고 텍스트 인코더는 EVA-02-CLIP-E/14+ [63]을 사용한다. 본 논문에서는 \\(\\beta_{1}\\)=0.9, \\(\\beta_{2}\\)=0.95의 가중치 감쇠율을 갖는 LAMB 최적화기[73]를 채택하였으며, 최적의 학습을 보장하기 위해 비젼 인코더와 텍스트 인코더에 서로 다른 학습율과 레이어 감쇠율을 적용한다. 비젼 인코더와 텍스트 인코더의 최대 학습률을 각각 4e-4와 4e-5로 설정하였으며, 2000번의 워밍업 단계를 거쳤다. 그 후, 학습 속도는 코사인 스케줄로 0으로 감소한다. 학습률 계층 붕괴율은 비전 및 텍스트 인코더에 대해 0.9 및 0.75로 구성된다. 온도 파라미터는 0.01로 일정하게 유지되며, 학습 비용을 최적화하기 위해 ZeRO stage-3 partition[55], gradient checkpointing[16], flash attention[24]과 함께 DeepSpeed 최적화 라이브러리[56]를 사용한다.\n' +
      '\n' +
      '**Dataset.** 우리의 병합-2B 데이터 세트는 LAION-2B [58]의 16억 샘플 및 COYO-700M [12]의 0.4억 샘플로 구성된다. LAION-2B로부터의 서브세트의 사용은 의도적인 필터링의 결과가 아니라, 오히려 이미지 다운로드 실패로 인한 것이라는 점에 유의한다. 04억 COYO-700M 샘플의 사용은 훈련 샘플의 수를 LAION-2B와 거의 동일한 것으로 보완하는 것을 목표로 한다. Merged-2B+는 Merged-2B의 모든 샘플과 LAION-COCO[1]의 샘플 2,000만 개와 VideoCC[48], InternVid[70], WebVid-10M[6]을 포함한 Merged-video의 샘플 2,300만 개로 구성된다. 병합 비디오는 트레이닝 프로세스의 마지막에 포함된다.\n' +
      '\n' +
      'EVA-CLIP-18B는 패치 드롭아웃 비율의 50%에서 볼 수 있는 Merged-2B의 샘플 54억 개, Merged-2B의 샘플 66억 개 및 패치 드롭아웃이 없는 LAION-COCO의 샘플 2000만 개, Merged-비디오의 샘플 2400만 개, 패치 드롭아웃 비율의 50%에서 볼 수 있다.\n' +
      '\n' +
      '**평가.** 이미지, 비디오 분류 및 이미지-텍스트 검색 전반에 걸쳐 널리 사용되는 33개의 데이터 세트에 대해 평가한다. EVA-CLIP-18B를 평가하기 위해 사용된 모든 데이터 세트는 표 11에 보고되어 있으며 [53, 38]에 따라 지정된 프롬프트 템플릿을 활용한다.\n' +
      '\n' +
      '**제로 샷 이미지 분류.** 표 2의 모든 27개의 제로 샷 이미지 분류 벤치마크에서 EVA-CLIP의 탁월한 성능을 보여준다. EVA-CLIP-18B는 모든 27개의 벤치마크에서 평균한 **80.7%** 상위 1의 정확도를 달성한다. 이러한 결과는 이전의 가장 우수한 개방형 DFN5B-CLIP-H/14+[28]을 +1.5% 능가하고 가장 큰 기존 CLIP 모델인 InternVL-C[17]을 +2.7% 능가한다. 버드 스냅 데이터 세트의 경우 링크가 끊어져 2195개의 테스트 이미지로 다운로드가 제한되었다.\n' +
      '\n' +
      '**Zero-Shot Video Classification.** UCF-101[60]에 대한 Top-1 정확도 및 Kinetics-400[15], Kinetics-600[13] 및 Kinetics-700[14]에 대한 Top-1 및 Top-5 정확도의 평균을 보고한다. 표 5에서 EVA-CLIP-18B가 제로 샷 비디오 클래스에서 다른 CLIP 모델보다 우수하다는 것을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c|c c c}  & \\multicolumn{3}{c|}{image encoder} & \\multicolumn{3}{c|}{text encoder} & \\multicolumn{3}{c}{\\# params} \\\\ method & \\multicolumn{3}{c|}{layers with head} & \\multicolumn{3}{c|}{layers with heads} & \\multicolumn{3}{c}{image text} & \\multicolumn{3}{c}{total} \\\\ \\hline EVA-CLIP-SB & 32 & 4096 & 32 & 32 & 1280 & 20 & 7.5B & 695M & 8.1B \\\\ EVA-CLIP-18B & 48 & 5120 & 40 & 32 & 1280 & 20 & 17.5B & 695M & 18.1B \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 아키텍처 구성.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c|c c c c|c c c}  & \\multicolumn{4}{c|}{zero-shot **text** retrieval} & \\multicolumn{4}{c|}{zero-shot **image** retrieval} & \\multicolumn{4}{c}{} \\\\  & \\multicolumn{3}{c|}{Flickr30K} & \\multicolumn{3}{c|}{COCO} & \\multicolumn{3}{c|}{Flickr30K} & \\multicolumn{3}{c}{COCO} \\\\ method & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & MR \\\\ \\hline EVA-01-CLIP-9/14 & 87.9 & 98.0 & 99.5 & 61.7 & 83.2 & 89.9 & 72.5 & 91.5 & 95.4 & 44.5 & 69.1 & 77.7 & 80.9 \\\\ EVA-01-CLIP-9/14+ & 93.3 & 99.5 & 99.9 & 69.4 & 88.3 & 93.2 & 79.2 & 95.2 & 97.3 & 51.1 & 74.7 & 82.5 & 85.3 \\\\ OpenCLIP-G/14 & 93.5 & 99.3 & 99.7 & 69.0 & 87.8 & 93.1 & 80.9 & 95.1 & 97.2 & 52.6 & 76.1 & 83.6 & 85.7 \\\\ EVA-02-CLIP-E/14+ & 94.3 & 99.6 & 99.8 & 69.4 & 88.6 & 93.3 & 79.7 & 94.9 & 97.3 & 52.5 & 75.9 & 83.4 & 85.7 \\\\ EVA-CLIP-8B & 95.6 & 99.6 & 99.9 & 70.3 & 89.3 & 93.9 & 80.8 & 95.5 & 97.6 & 53.0 & 76.0 & 83.4 & 86.2 \\\\ DFN5B-CLIP-H/14 & 92.9 & 99.3 & 99.9 & 72.3 & 90.2 & 94.6 & 80.1 & 95.2 & 97.3 & 53.9 & 78.0 & 85.6 & 86.6 \\\\ InternVL-C & 93.8 & **99.7** & **100.0** & 70.3 & 89.2 & 93.8 & 82.1 & 96.0 & 98.1 & 54.1 & 77.1 & 84.8 & 86.6 \\\\ DFN5B-CLIP-H/14+ & 93.6 & 99.3 & 99.6 & 71.8 & 90.4 & 94.9 & 82.1 & 96.0 & 97.9 & 55.6 & **79.2** & **86.3** & 87.2 \\\\ EVA-CLIP-18B & **96.7** & **99.7** & **100.0** & **73.6** & **90.9** & **95.0** & **83.3** & **96.3** & **98.3** & **56.2** & 78.5 & 85.6 & **87.8** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: Flickr30K [74] 및 COCO [45]에 대한 제로샷 검색 성능.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c|c}  & \\multicolumn{3}{c|}{image encoder} & \\multicolumn{3}{c|}{text encoder} & \\multicolumn{3}{c}{\\# params} \\\\ method & \\multicolumn{3}{c|}{layers with heads} & \\multicolumn{3}{c|}{layers with heads} & \\multicolumn{3}{c}{image text} \\\\ \\hline EVA-CLIP-SB & 32 & 4096 & 32 & 32 & 1280 & 20 & 7.5B & 695M & 8.1B \\\\ EVA-CLIP-18B & 48 & 5120 & 40 & 32 & 1280 & 20 & 17.5B & 695M & 18.1B \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 아키텍처 구성.\n' +
      '\n' +
      '시피케이션 벤치마크가 큰 폭으로 상승했습니다. 비디오당 단일 중심 프레임을 샘플링할 때 EVA-CLIP-18B는 평가된 4개의 벤치마크에서 86.0%, 72.9%, 72.9% 및 68.2%의 정확도를 달성한다. 또한, 비디오당 8 프레임 또는 16 프레임을 균일하게 샘플링할 때, 단일 프레임 설정과 비교하여 4개의 벤치마크에 걸쳐 평균 +4.7%/+4.8%의 개선을 관찰한다.\n' +
      '\n' +
      '**Zero-Shot Image-Text Retrieval.** 표 3에서 Flickr30K[74]와 COCO[45]에 제로샷 이미지와 텍스트 Retrieval 결과를 보고한다. EVA-CLIP-18B는 모든 검색 벤치마크에서 평균 87.8%의 리콜을 달성하여 경쟁업체보다 크게 뛰어납니다.\n' +
      '\n' +
      '**견고성.** 표 6에서, 우리는 EVA-CLIP를 스케일링하는 것이 시각적 표현들의 견고성을 상당히 향상시킨다는 것을 입증한다. EVA-CLIP는 EVA-CLIP-18B의 경우, ImageNet-1K와 ImageNet 변이체 사이의 성능 저하(\\(\\Delta\\downarrow\\))가 가장 작으며, EVA-CLIP-18B의 경우 0.2%의 Top-1 정확도 차이를 보인다.\n' +
      '\n' +
      '강건성과 제로샷 성능에 대한 보다 강력하고 포괄적인 평가를 위해서는 더 많은 이미지 분포를 포함하는 벤치마크를 더 포함하는 것이 좋다. 그러나, 우리는 BASIC-L [52]가 더 넓은 범위의 데이터 세트 및 분포에서 EVA-CLIP-18B에 비해 더 높은 이미지넷 상위-1 정확도가 더 나은 전체 성능으로 반드시 이어지는 것은 아니라는 점에 주목하고 싶다.\n' +
      '\n' +
      '**ImageNet-1K에 대한 선형 프로빙.** 표 7에서 ImageNet-1K에 대한 선형 프로빙 결과를 제시한다[26]. EVA-CLIP-18B는 평균 상위 1의 정확도 88.9%를 달성하여 InternVL-C[17]를 0.7% 능가한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c c c c c c} method & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline BASIC-L [52] (reported) & 85.7 & 80.6 & 85.6 & 95.7 & 76.1 & 82.3 & 97.5 & 82.3 & 40.3 & 76.2 & 59.2 & 64.6 & 51.0 & 95.1 & 59.6 & 72.7 & 99.6 & 76.7 (77.8) \\\\ EVA-CLIP-18B & 83.8 & 77.9 & 87.3 & 95.7 & 74.7 & 82.2 & 99.4 & 93.8 & 83.0 & 77.7 & 79.9 & 72.1 & 79.8 & 95.8 & 65.2 & 76.9 & 99.6 & 84.9 (84.1) \\\\  & **-1.9** & **-2.7** & **+1.7** & **+0.0** & **-1.4** & **-0.1** & **+1.9** & **+11.5** & **+42.7** & **+1.5** & **+20.7** & **+7.5** & **+28.8** & **+0.7** & **+5.6** & **+4.2** & **+0.0** & **+8.2 (+6.3)** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: **CLIP 모델의 강건성 평가 및 17개의 벤치마크에 대한 BASIC-L[52]과의 비교.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c} method & \\#param & \\(\\text{wpl acc.}\\) \\\\ \\hline OpenCLIP-G/14 (reported) & 1.8B & 86.2 \\\\ EVA-01-CLIP-g/14 & 1.0B & 86.5 \\\\ EVA-02-CLIP-E/14+ & 4.4B & 88.1 \\\\ InternVL-C (reported) & 5.9B & 88.2 \\\\ EVA-CLIP-SB & 7.5B & 88.5 \\\\ EVA-CLIP-18B & 17.5B & 88.9 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: **ImageNet-1K [26].** top-1 정확도는 EVA-CLIP의 스케일링 업과 함께 지속적인 개선을 보여준다.\n' +
      '\n' +
      '**3D Representation.** Uni3D[77] 설정을 채택하여 교사 스케일업의 효과를 탐색한다. 표 8의 EVA-CLIP의 스케일 업으로, 우리는 3D 표현 학습 능력에서 일관된 개선을 관찰한다. 또한, EVA-CLIP-18B가 탑재된 Uni3D-base는 ModelNet[71] 및 ScanObjectNN[66] 벤치마크에 새로운 레코드를 설정한다.\n' +
      '\n' +
      '##4 : 연구\n' +
      '\n' +
      '**Video Data.** 표 9에서 우리는 EVAC-CLIP-18B의 제로샷 성능에 대한 어블레이션을 수행하여 Merged-Video가 있거나 없는 경우 결과를 비교한다. 비디오 데이터에 대한 트레이닝 목표는 8개의 프레임이 균일하게 샘플링되는 비디오로부터 특징의 추출을 포함하는 이미지의 것과 정렬된다. 모든 [CLS] 임베딩의 평균은 비디오에 대한 표현으로서 작용한다. 결과는 Merged-Video를 이용한 훈련과 관련된 실질적인 성능 향상을 보여준다. UCF-101[60] 및 Kinetics-400[15]/600[13]/700[14]에 걸쳐 평균화된 제로 샷 성능은 하나의 중간 프레임으로 평가를 위한 경우 +0.7, 8 프레임으로 평가를 위한 경우 +0.8의 이득을 나타낸다.\n' +
      '\n' +
      '**이미지 해상도.** 표 10에서 더 큰 이미지 해상도가 제로 샷 성능에 미치는 영향을 조사한다. 특히, EVA-CLIP-8B의 경우 해상도가 \\(224^{2}\\)에서 \\(448^{2}\\)으로 증가할 때 평균 최고 1 정확도 이득은 +0.9이다. 마찬가지로, EVA-CLIP-8B+의 경우 24k, EVA-CLIP-18B+의 경우 23k의 낮은 전역 배치 크기로 훈련하더라도 \\(224^{2}\\)에서 \\(336^{2}\\)으로 증가하면 +0.5의 이득을 얻는다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리는 현재 180억 개의 매개변수를 가진 가장 크고 가장 성능이 뛰어난 개방형 CLIP 모델인 EVA-CLIP-18B를 제시한다. 우리는 EVA의 약한 시각 확장 원리에 따라 CLIP 모델을 새로운 기록으로 확장하고 이미지, 비디오 및 3D 도메인에 걸쳐 널리 퍼진 여러 벤치마크에서 SOTA를 발전시킬 수 있음을 보여준다. 중요하게도, 우리는 EVA-CLIP 모델의 크기를 확장하는 것이 포화 징후 없이 지속적으로 성능을 향상시켜 미래 비전 모델 축소에 빛을 발한다는 것을 보여준다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] L. coco: 600m synthetic captions from laino2b-en. [https://laino.ai/blog/lain-coco/](https://laino.ai/blog/lain-coco/). Cited by: SS1.\n' +
      '*[2]R. 80 zero-shot accuracy with openclip: Vit-g/14 trained on laino-2b. [https://laino.ai/blog/giant-openclip/] (https://laino.ai/blog/giant-openclip/). 인용: SS1.\n' +
      '*[3]J. 배승 배승 양승 왕승 Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou (2023) Qowen-vl: understanding, localization, text reading, and beyond. ArXiv:2308.12966. 인용: SS1.\n' +
      '*[4]M. Bain, A. Nagrani, G. Varol, and A. Zisserman(2021) Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1728-1738. Cited by: SS1.\n' +
      '*[5]H. 바오락 Dong, and F. Wei (2021) Beit: Bert pre-training of image transformer. ArXiv:2106.08254. 인용: SS1.\n' +
      '*[6]A. 바부, D. 마요, J. 알베리오, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz(2019) ObjectNet: a large-scale bias-controlled dataset for pushing the object recognition models. NeurIPS에서 인용: SS1.\n' +
      '*[7]T. Berg, J. Liu, S. W. Lee, M. L. Alexander, D. W. Jacobs, and P. N. Belthumeur (2014) BirdsMap: large-scale fine-grained visual classification of birds. CVPR에서 인용됨: SS1.\n' +
      '*[8]L. 보사드 기야민, L. 밴 굴(2014) 푸드-101 채굴 랜덤 포레스트가 있는 차별적 구성 요소. ECCV에서 인용: SS1.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c|c} \\hline method & resolution & IN-1k & IN-A & IN-B & IN-V2 & IN-Ske. & ObjectNet & **avg.** \\\\ \\hline \\hline EVA-CLIP-8B & 224\\(\\times\\)224 & 83.5 & 85.2 & 95.3 & 77.7 & 74.3 & 81.2 & 82.9 \\\\ EVA-CLIP-8B + & 448\\(\\times\\)448 & 83.8 & 88.7 & 95.4 & 77.7 & 74.1 & 82.9 & 83.8 \\\\  & & +0.3 & +3.5 & +0.1 & +0.0 & **-0.2** & +1.7 & +0.9 \\\\ \\hline \\hline EVA-CLIP-18B & 224\\(\\times\\)2224 & 83.8 & 87.3 & 95.7 & 77.9 & 74.7 & 82.2 & 83.6 \\\\ EVA-CLIP-18B+ & 336\\(\\times\\)336 & 83.9 & 88.9 & 95.6 & 78.2 & 74.3 & 83.6 & 84.1 \\\\  & & +0.1 & +1.6 & **-0.1** & +0.3 & **-0.4** & +1.4 & +0.5 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: **해상도 증가.** ImageNet 변종 및 ObjectNet에 대한 제로샷 성능을 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c}  & \\multicolumn{3}{c|}{classification} & retrieval \\\\  & image video (\\#F 1) & video (\\#F 8) & avg. recall \\\\ \\hline w/o video data & 80.7 & 74.3 & 78.9 & 87.9 \\\\ w/ video data & 80.7 & 75.0 (+0.7) & 79.7 (+0.8) & 87.8 (-0.1) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: **비디오 데이터는 제로-샷 비디오 분류 성능**을 향상시킨다. 우리는 각각 27개의 이미지 분류 벤치마크, 4개의 비디오 벤치마크 및 2개의 이미지 텍스트 검색 벤치마크에 대해 평균 성능을 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c}  & \\multicolumn{3}{c|}{classification} & retrieval \\\\  & image video (\\#F 1) & video (\\#F 8) & avg. recall \\\\ \\hline w/o video data & 80.7 & 74.3 & 78.9 & 87.9 \\\\ w/ video data & 80.7 & 75.0 (+0.7) & 79.7 (+0.8) & 87.8 (-0.1) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: **비디오 데이터는 제로-샷 비디오 분류 성능**을 향상시킨다. 우리는 각각 27개의 이미지 분류 벤치마크, 4개의 비디오 벤치마크 및 2개의 이미지 텍스트 검색 벤치마크에 대해 평균 성능을 보고한다.\n' +
      '\n' +
      '* [11] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. _arXiv preprint arXiv:2005.14165_, 2020.\n' +
      '* [12] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. [https://github.com/kakaobrain/coyo-dataset](https://github.com/kakaobrain/coyo-dataset), 2022.\n' +
      '* [13] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600. _arXiv preprint arXiv:1808.01340_, 2018.\n' +
      '* [14] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700 human action dataset. _arXiv preprint arXiv:1907.06987_, 2019.\n' +
      '* [15] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In _CVPR_, 2017.\n' +
      '* [16] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost, 2016.\n' +
      '* [17] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Intervl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. _arXiv preprint arXiv:2312.14238_, 2023.\n' +
      '* [18] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. _Proceedings of the IEEE_, 2017.\n' +
      '* [19] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning, 2022.\n' +
      '* [20] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sash Tsysshchenko, Joshua Maynez, Abhishek Rao, Parser Barnes, Yi Tay, Noam Shazezev, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denmy Zhou, Daphne Ippolito, David Luan, Hyonnetk Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanuulayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slay Petrov, and Noah Friedel. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n' +
      '* [21] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed,, and A. Vedaldi. Describing textures in the wild. In _CVPR_, 2014.\n' +
      '* [22] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. _arXiv preprint arXiv:2003.10555_, 2020.\n' +
      '* [23] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In _AISTAT_, 2011.\n' +
      '* [24] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022.\n' +
      '* [25] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Anirududha Kembhavi, and Ali Farhadi. Obiayverse: A universe of annotated 3d objects. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13142-13153, 2023.\n' +
      '* [26] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.\n' +
      '* [27] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. "The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. "[http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html](http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html)", 2007.\n' +
      '* [28] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. _arXiv preprint arXiv:2309.17425_, 2023.\n' +
      '* [29] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A visual representation for neon genesis. _arXiv preprint arXiv:2303.11331_, 2023.\n' +
      '* [30] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. _arXiv preprint arXiv:2211.07636_, 2022.\n' +
      '* [31] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In _CVPRW_, 2004.\n' +
      '* [32] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Emetzari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shujaran Song, Hannaneh Hajishirzi, Ali Farhadi, Roman Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets. _arXiv preprint arXiv:2304.14108_, 2023.\n' +
      '* [33] Ian J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun Lee, et al. Challenges in representation learning: A report on three machine learning contests. In _ICONIP_, 2013.\n' +
      '* [34] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. _IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens._, 2019.\n' +
      '* [35] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorndon, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In _CVPR_, 2021.\n' +
      '* [36] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In _CVPR_, 2021.\n' +
      '\n' +
      '* [37] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In _ECCV_, 2016.\n' +
      '* [38] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip. [https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip), 2021.\n' +
      '* [39] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _ICCVW_, 2013.\n' +
      '* [40] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n' +
      '* [41] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 1998.\n' +
      '* [42] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [43] Xianhang Li, Zeyu Wang, and Cihang Xie. Clipa-v2: Scaling clip training with 81.1 _arXiv preprint arXiv:2306.15658_, 2023.\n' +
      '* [44] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking, 2022.\n' +
      '* [45] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _European conference on computer vision_, pages 740-755. Springer, 2014.\n' +
      '* [46] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '* [47] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. _arXiv preprint arXiv:1306.5151_, 2013.\n' +
      '* [48] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja Hauth, Santiago Manen, Chen Sun, and Cordelia Schmid. Learning audiovideo modalities from image captions, 2022.\n' +
      '* [49] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _ICVGIP_, 2008.\n' +
      '* [50] Ting Pan, Luh Tang, Xinlong Wang, and Shiguang Shan. Tokenize anything via prompting. _arXiv preprint arXiv:2312.09128_, 2023.\n' +
      '* [51] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In _CVPR_, 2012.\n' +
      '* [52] Hieu Pham, Zihang Dai, Golnar Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and Quoc V Le. Combined scaling for zero-shot transfer learning. _arXiv preprint arXiv:2111.10050_, 2021.\n' +
      '* [53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* [54] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. _JMLR_, 2020.\n' +
      '* [55] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In _SC20_, 2020.\n' +
      '* [56] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In _KDD_, 2020.\n' +
      '* [57] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet?, 2019.\n' +
      '* [58] Christoph Schulmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _arXiv preprint arXiv:2210.08402_, 2022.\n' +
      '* [59] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatuszaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [60] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. _arXiv preprint arXiv:1212.0402_, 2012.\n' +
      '* [61] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. _Neural networks_, 2012.\n' +
      '* [62] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. _arXiv preprint arXiv:2312.13286_, 2023.\n' +
      '* [63] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. _arXiv preprint arXiv:2303.15389_, 2023.\n' +
      '* [64] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. _arXiv preprint arXiv:2307.05222_, 2023.\n' +
      '* [65] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [66] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1588-1597, 2019.\n' +
      '* [67] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. In _MICCAI_, 2018.\n' +
      '* [68] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. _NeurIPS_, 2019.\n' +
      '* [69] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xiauxuan Song, et al. Cogytlm: Visual expert for pretrained language models. _arXiv preprint arXiv:2311.03079_, 2023.\n' +
      '* [70] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Conghui He, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: A large-scale video-text dataset for multimodal understanding and generation, 2024.\n' +
      '\n' +
      '* [71] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1912-1920, 2015.\n' +
      '* [72] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In _CVPR_, 2010.\n' +
      '* [73] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes, 2019.\n' +
      '* [74] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _TACL_, 2014.\n' +
      '* [75] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. _arXiv preprint arXiv:2303.15343_, 2023.\n' +
      '* [76] Biao Zhang and Rico Sennrich. Root mean square layer normalization. _arXiv preprint arXiv:1910.07467_, 2019.\n' +
      '* [77] Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. Uniq: Exploring unified 3d representation at scale. _arXiv preprint arXiv:2310.06773_, 2023.\n' +
      '* [78] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minipt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:9]\n' +
      '\n' +
      '우리는 둘 중 가장 성능이 좋은 변환을 선택하여 얻은 결과를 제시한다. 0-shot 검색 태스크의 경우, 특히 고정된 크기로 이미지를 직접 리사이징하는 변환을 선택한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 14: **이미지 변환이 제로-샷 평가에 미치는 영향. †\\(\\dagger\\)**는 고정된 크기로 이미지를 직접 리사이징하는 것을 나타내는 반면, *는 고정된 크기를 달성하기 위해 가장 짧은 측면을 기준으로 이미지를 리사이징하고 후속적으로 중앙 크롭하는 것을 나타낸다.**\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>