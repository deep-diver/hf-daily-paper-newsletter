<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#Open-ended Visual Quality 비교를 위한\n' +
      '\n' +
      'Haoning Wu\n' +
      '\n' +
      '동등한 기여도. \\ ({}^{1}\\)난양기술대학교 ({}^{2}\\)City University of Hong Kong. \\ ({}^{3}\\) 상하이 자오동대학교 ({}^{4}\\)Sensetime Research.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '이미지 품질 평가(IQA)는 고품질 시각적 콘텐츠에 대한 효과적인 추천[58]과 잠재적인 개선을 위한 귀중한 지침[54, 62, 66]을 제공하기 때문에 시각적 컴퓨팅에서 중요한 영역이었다. 가장 최근에, 몇몇 선구적인 연구들[50, 51, 52]은 대형 멀티모달리티 모델들(LMMs, _e.g_)을 탐구했다. GPT-4V) [57, 4, 5, 9, 25, 34], IQA를 스칼라 점수(_e.g_. 3.457)를 제공하는 것으로부터 개방형 시나리오로 확장하는데, 이는 _open-range question_에 대한 응답을 허용하고, 전체 점수를 넘어서는 _detailed reasonings_를 제공한다.\n' +
      '\n' +
      '이러한 연구는 IQA에 대한 인간의 능력을 충분히 모방하지만, 또한 인간과 동일한 결점, 즉 절대 평가에 대한 ** 모호성을 겪는다. 예를 들어, 도 1에 도시된 바와 같다. 도 2의 (a)에 도시된 바와 같이, 상이한 인간 관찰자는 단일 이미지 상의 노출 레벨에 대해 상이한 기준을 보유하며, 이후 다양한 절대 평가를 제공한다. 그럼에도 불구하고 이미지의 노출 수준을 비교하도록 요청했지만 모든 관찰자는 순위 (1)>(2)>(3)>(4)에 동의한다(그림). 2(b)); 모든 관찰자는 또한 (1)이 가장 높은 노출을 갖는다는 것에 동의하지만, 모두가 독립적으로 평가하면서 옵션 _high_를 선택하는 것은 아니다. 이러한 관찰을 감안할 때 비교는 품질 평가를 위한 전통적인 인간 연구 설정[43]이었고 다양한 기존 주관적 연구[10, 35, 53, 54, 63]에 의해 채택되었다. 또한, 모호성을 피하기 위해 비교 설정도 개선 안내를 위해 IQA 알고리즘을 적용하면서 주로 [54, 66]을 채택한다.\n' +
      '\n' +
      'IQA에 대한 비교는 널리 인식되었지만, 기존의 관련 데이터 세트[35] 및 방법[28, 64]은 일반적으로 전반적인 품질 비교를 기반으로 하며 개방형 시나리오로 확장되지 않았다. 반면, 오픈 소스 LMM [51, 4, 25]는 보통 _single image_ 명령어 튜닝 데이터 세트[1, 13, 26]로 미세 조정되고 두 이미지 비교 설정[68, 70]에서도 충분한 능력이 부족한 것으로 입증되었다. 이러한 간극은 시각적 품질 비교를 위한 특정 명령어 튜닝 데이터세트의 필요성을 분명히 나타냈지만, 인간으로부터 그러한 데이터세트를 수집하는 것은 너무 비싸다. 본 논문에서는 비용이 많이 드는 인간 노동을 피하기 위해, Weak Supervisors_(_Co-Instruct_)로부터 _Collaborative Instruction Tuning이라는 학습 데이터셋을 수집하는 대체 전략을 제안한다. (주)특집\n' +
      '\n' +
      '그림 2: **개방형 시각적 품질 비교의 동기: 비교 설정은 단일 이미지에 대한 절대 평가**에 대한 ** 모호성을 효과적으로 피할 수 있으며 다운스트림 지침 역할을 하기 위해 보다 명확한 판단을 제공할 수 있다[54, 66].\n' +
      '\n' +
      'cally, 우리는 두명의 완벽하지 않은 감독자를 채택한다: **1)****Merge2Compare**. Q-Pathway(Q-Pathway)에서 19K개의 영상에 대한 단일 영상의 인간 품질 기술로부터 생성된 [51]을 텍스트 임베딩 모델(44)로 가장 유사한 기술들을 제거하고 100K 그룹으로 랜덤하게 매칭한다. 그런 다음 LLaVA-150K [26]의 구성과 유사하게 단일 모달 LLM [16]을 프롬프트하여 그룹 내 다중 인간 설명을 비교하고 "_merge_"를 100K 의사 비교(도 3(a)에 도시된 바와 같이)로 만든다. . **2)** ***Teach2Compare*** GPT-4V가 기존 관행 [2, 17]에 따라 기존 LMM 중 쌍별 설정 [70, 68]에서 특히 높은 정확도를 갖는 것을 관찰하면 GPT-4V 응답을 활용하여 데이터 세트를 더욱 확장한다. 우리는 9K개의 레이블이 지정되지 않은 이미지를 수집하고 30K 이미지 그룹(그룹_당_2-4 이미지)으로 무작위로 매칭하고 비교를 위해 _caption-like_ 일반 비교 및 _question-answer pair_ 모두에 대해 GPT-4V 응답을 얻는다. Q-Instruct-200K[51] (_on single image_), **Merge2Compare**, **Teach2Compare**을 통합하여 개방형 다중 영상 품질 비교를 위해 설계된 첫 번째 명령어 튜닝 데이터셋인 Co-Instruct-562K를 구성한다.\n' +
      '\n' +
      '대화 중에 각각의 특정 이미지를 올바르게 참조하기 위해, 우리는 다중 이미지 케이스들을 처리하기 위한 특정 이미지-텍스트 인터리빙 포맷[5]을 정의한다: 사용자: 첫 번째 이미지: <img0> 두 번째 이미지: <img1>... <query> Assistant:<response>\n' +
      '\n' +
      '명령어 튜닝 동안 여러 이미지를 함께 공급해야 하므로 시각적 임베딩을 선형적으로 투영하는 가장 인기 있는 LLaVA[25, 26, 39] 구조를 채택하면 언어 모델[42]의 컨텍스트 창을 초과하고 오류가 발생한다. 이후, 시각적 토큰 길이(이미지_당 1025에서 65개의 토큰으로)를 먼저 줄이기 위해 대안적인 시각적 추상기 구조[57]를 채택한 다음 언어 디코더로 전달하기 위해 텍스트 임베딩과 연결한다. Co-Instruct-562K 데이터 세트와 특별히 설계된 입력 구조에서 학습하여 기준 [57]보다 최대 **86%** 개선 및 최신 오픈 소스 LMM보다 **61%** 더 나은 **Co-Instruct**를 제시한다. 더 중요한 것은 GPT-4V를 교사 중 하나로 사용함에도 불구하고, 여전히 다양한 MCQ(Multi-Choice Question) 벤치마크에서 GPT-4V _teacher_를 능가하고, 상세한 언어 추론이 필요한 시나리오에서 GPT-4V 능력과 일치한다는 것이다.\n' +
      '\n' +
      '도 3: **Co-Instruct-562K의 구성 방법론, (a)****Merge2Compare**(인간-표지된 단일 이미지 품질 설명으로부터의_LLM 비교_) 및 (b) **Teach2Compare**(다중 표지되지 않은 이미지 상의_GPT-4V 비교_)의 조합.\n' +
      '\n' +
      '모델 **Co-Instr**을 학습한 후, 우리의 또 다른 관심사는 다중 이미지 비교에 대한 풍부한 평가 설정의 부족이다: Q-Bench [50, 68] 시리즈는 단일 이미지와 이미지 쌍에 대해 여러 형식을 다루었지만, 품질 비교를 위한 기존 평가 시나리오는 두 이미지 외에도 없다. 이 평가 설정을 보완하기 위해 **MICBench**를 구성한다. 구체적으로, **MICBench**는 3개 또는 4개의 이미지(_each half_)의 그룹 중 품질 또는 관련 속성을 비교하는 2,000개의 다중 선택 질문(MCQ)을 포함하며, 여기서 질문의 절반 이상이 _Which_ 질문(_e. 어느 이미지가 가장 선명도?_)이다. 그룹으로부터 특정 품질 관련 외형을 가진 이미지를 추출하는 것을 목표로 하는 _Which_ 문항은 이미지 비교와 관련된 가장 중요한 문항이다. _Which_ 질문에도 불구하고, **MICBench**는 또한 다중-이미지 품질 비교를 위한 전체론적 벤치마크 설정을 제공하기 위해 _Yes-or-No_ 질문 및 다른 질문 유형(_What/How/How-Many 등)을 포함한다.\n' +
      '\n' +
      '요약하면, 우리는 _open-ended_ 시각 품질 비교를 위한 체계적인 연구를 수행한다. 우리의 기여는 세 가지로 요약될 수 있다:\n' +
      '\n' +
      '1. 시각적 품질 비교를 위한 첫 번째 명령어-튜닝 **dataset**인 Co-Instruct-562K를 구성한다. 두 개의 "_약한 감독자_", **Merge2Compare**(LLM 병합 비교) 및 **Teach2Compare**(GPT-4V 의사 레이블 비교)에서 수집된 데이터를 사용하여 공개 데이터 세트는 시각적 비교 설정에서 오픈 소스 LMM의 기능을 크게 확장한다.\n' +
      '2. 개방형 시각적 비교를 위한 가장 가능성 있는 **모델**인 **Co-Instruct**를 제안한다. 이미지-텍스트 인터리브 입력 포맷과 Co-Instruct-562K 데이터세트로 미세 조정함으로써, 다수의 개방형 시각적 비교 작업에서 기존 방법(및 심지어 GPT-4V)보다 상당히 우수하다. 개방형 가중치로 독점 모델보다 광범위한 적용이 가능합니다.\n' +
      '3. 여러 이미지(2개 이상)에 대한 품질 비교를 위해 LMM을 평가하는 첫 번째 벤치마크로 **benchmark**, **MICBench**를 구성한다. 3~4개의 이미지 중 시각적 품질 비교와 관련된 2,000개의 다양한 유형의 개방형 MCQ를 다룬다. **MICBench**는 시각적 품질 비교 문제에 대한 보다 총체적인 평가 연구에 기여한다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '시각적 품질 비교\n' +
      '\n' +
      '시각적 품질 비교(_especially_ pair comparison)는 널리 사용되는 주관적 품질 평가 방법론으로, 인간의 의견을 수렴하는 가장 신뢰할 수 있는 방법으로 작용한다[31]. 그러나 이미지의 수가 증가하면 쌍별 비교의 기하급수적 증가로 인해 실험이 불가능해진다[14]. 쌍 수를 줄이기 위해 많은 활성 샘플링 방법이 제안되었지만 [21, 33, 55]는 계산 비용이 많이 들고 대규모 실험에 적합하지 않다. 주관적 연구에도 불구하고 품질 순위를 매기는 학습은 많은 객관적인 접근법[8, 27, 30, 46, 48, 64, 65]에 의해 효과적인 것으로 널리 입증된다. 그럼에도 불구하고 일반적으로 전체 비교를 위한 스칼라 점수 또는 이진 판단만을 예측하여 의미 있는 피드백을 특정 유형의 왜곡 또는 선호도로 제공하는 능력을 제한한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n' +
      '단계 1: 쌍/그룹 매칭(도). 4(a)). 기존의 단일 이미지 품질 기술에서 얻은 정보를 가장 잘 활용하기 위해 실험 규칙에 따라 \\(O(n\\log n)\\) 쌍별 조합을 샘플링하여 집합 [37, 45]에서 모든 개별 항목 간에 효과적으로 순위를 매기기 위해 Q-Pathway의 19K 이미지 모두에서 81K 이미지 쌍을 무작위로 샘플링한다. 쌍에도 불구하고, 우리는 더 많은 이미지의 시나리오를 커버하기 위해 3개의 이미지가 있는 27K 그룹과 4개의 이미지가 있는 18K 그룹을 추가로 샘플링한다.\n' +
      '\n' +
      '단계 2: Top-Similarity Pair Removal(도) 4(b))._merge_의 유효성은 설명들 간의 차이에서 비롯되며, _e.g._ 사이의_e.g._는 <img>에 대해 수용가능하고 _The quality is poor_ for <img1>에 대해 수용가능하다. 그러나, 쌍/그룹 내의 설명들이 거의 동일한 정보를 포함하는 경우(_e.g._명확도는 양호하지만 조명은 어둡다), _merged_ 비교들은 충분한 정보가 부족하거나 심지어 잘못된 예측으로도 부족할 것이다. 이후, E5-Mistral [44] 텍스트 임베딩 모델을 사용하여 기술 간의 유사성을 계산하고, _any_ high-similarity 기술 쌍이 그룹에 존재하는 경우 제거한다. 제거 후 70K 이미지 쌍(초기 샘플의 86%), 3개(초기 샘플의 74%)의 20K 그룹 및 4개(초기 샘플의 55%)의 10K 그룹을 보존하고 _merging_를 위해 LLM에 공급한다.\n' +
      '\n' +
      '단계 3: LLM 병합(도. 도 4(c)). **Merge2Compare**에 대한 핵심 단계는 LLM들이 단일 이미지 평가들을 비교 텍스트들로 변환하도록 프롬프트하는 것이다. 특히, 기존의 많은 관행[69, 26, 59]에 따라 컨텍스트에서 이미지의 대체물로 설명을 넣는다. 이미지 <img1>에 대한 설명을 <desc1>로 표현하면, LLM _merging_에 대한 사용자 질의는 다음과 같이 공식화된다:\n' +
      '\n' +
      '(쌍) 첫 번째 이미지: <desc0> 두 번째 이미지: <desc1> 어떤 이미지가 더 좋은 품질을 가지며, 왜?\n' +
      '\n' +
      '(Groups of Three/Four) {The \\(K_{i+1}\\) image: <desc1> \\(|_{i=0}^{N-1}\\)}\n' +
      '\n' +
      '이미지의 품질을 평가하고 순위를 정당화하십시오.\n' +
      '\n' +
      '여기서 \\(K_{i}\\)는 \\(i+1\\)의 서수 형태를 나타내며, _e.g._\\(K_{1}\\)은 첫째, \\(K_{2}\\)은 둘째이다.\n' +
      '\n' +
      '_merged_ 비교는 추론**(도 4_right_)와의 **전체 비교이다. 신뢰성을 검증하기 위해 무작위 제거된 250개 그룹의 병합 비교와 혼합된 **Merge2Compare**의 무작위 샘플 250개에 대해 인간 검사를 수행했다. *Merge2Compare**의 정확률은 96%인 반면 제거된 그룹에서는 72%에 불과하여 최상위 유사도 제거 과정이 훈련 데이터의 품질을 보장하는 데 미치는 영향을 보여준다.\n' +
      '\n' +
      '도 4: **Merge2Compare**를 구성하는 파이프라인: 이미지를 먼저 그룹 **(a)**로 매칭한 후, 최상위 유사도 제거 **(b)**를 통해 필터링한다. 필터링 후, 단일 이미지 품질 설명들은 LLM[16]에 의한 비교들로 _merged_**(c)**이다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '\'합성의 합리성\'\n' +
      '\n' +
      '원칙 3에서 논의한 바와 같이, 우리의 동기는 서로를 보완할 수 있는 부분 집합을 모으는 것이다. 이러한 보완성은 다음의 두 가지 측면에서 반영된다. 첫째, 일반 비교의 측면에서 **Merge2Compare**는 정확도가 높지만 세밀한 비교(_Top-similarity Pair Removal_ 제외)가 부족한 반면, **Teach2Compare**-_general_는 약간 덜 정확하지만 보다 다양한 시나리오를 제공하고 컨텐츠 정보를 배경으로 포함한다. 양자의 합동 훈련은 우리 모델에 의한 보다 포괄적인 품질 비교에 기여한다. 또한 **Teach2Compare**에는 고유한 _Q&A_ 하위 집합이 포함되어 있어 모델의 개방형 범위 질문에 대한 답변 능력이 크게 향상됩니다.\n' +
      '\n' +
      '##4 : 공동 지시 모델\n' +
      '\n' +
      '본 절에서는 제안된 모델인 **Co-Instruct**에 대해 논의한다. 구체적으로, 다중 이미지 비교 설정을 위한 두 가지 자명하지 않은 적응을 만들었다.\n' +
      '\n' +
      '시각적 토큰 감소(도. 도 6의 (a)).대부분의 최신 LMM[5, 25, 39]은 많은 수의 토큰(_e.g._1025)을 유지하는 단순 프로젝터를 채택하였다. 이 구조는 다중-이미지 시나리오들에 우호적이지 않다: 두 개의 이미지들만을 통과시키는 것은 LLaVA[26]의 최대 길이를 초과할 것이고(2048), 네 개의 이미지들은 LLaMA-2[42]의 컨텍스트 윈도우를 초과할 것이다(4096). 따라서 다중 이미지 시나리오에 쉽게 적응할 수 있도록 LLM에 시각적 임베딩을 공급하기 전에 토큰 번호를 줄이기 위해 널리 사용되는 또 다른 추상기 [56, 4, 57] 구조를 채택한다.\n' +
      '\n' +
      '이미지-텍스트 인터리브드 형식(그림) 도 6의 (c)).전형적인 단일-이미지 명령 튜닝은 보통 "이미지들의_position_"에 대해 신경 쓰지 않는다. 대부분의 접근법 [4, 25, 69]는 모든 이미지를 텍스트(<img0>(<img1>...)<text>) 앞에 직접 쌓아 올린다. 여러 이미지에서 **joint** 정보를 조회하면 큰 문제가 발생하지 않지만, 비교 중에 이미지 전반에 환각을 도입하는 것이 관찰된다. 이를 해결하기 위해, 우리는 다중 이미지 트레이닝을 위한 이미지-텍스트 인터리빙된 포맷을 제안하며, 각각의 이미지는 명목상 식별을 위해 명시적 텍스트로 시작된다:\n' +
      '\n' +
      'User: 첫 번째 이미지: <img0> 두 번째 이미지: <img1>(...) <query> Assistant: <response>\n' +
      '\n' +
      '우리의 실험에서, 우리는 이 인터리빙된 포맷이 **Co-Instruct**(Tab)의 성능을 상당히 향상시킨다는 것을 입증했다. 8), 특히 이미지를 분할하기 위해 학습 가능한 특수 토큰(<img_st> 및 <img_end>)을 사용하는 것보다 우수하다.\n' +
      '\n' +
      '도 6: **Co-Instruct.**(a) 이미지의 구조는 시각적 임베딩 레이어에 의해 인코딩된 다음, 토큰 번호를 줄이기 위해 추상기 모듈을 통과하고, 이어서 이미지-텍스트 인터리빙된 포맷 하에서 텍스트 임베딩과 융합된 **(c)**.\n' +
      '\n' +
      '##5: MICBench\n' +
      '\n' +
      '이 섹션에서는 기존 평가 설정(Sec. 6.3)의 보완으로 3개 또는 4개의 이미지 그룹에 대한 개방형 평가 설정을 다루기 위해 제안된 **MICBench**에 대해 논의한다. 여기에는 여러 후보가 장착된 2,000개의 _open-range_ 질문 그룹이 포함되어 있으며 자세한 내용은 다음과 같다.\n' +
      '\n' +
      '다양한 이미지 그룹을 소싱합니다. 벤치마크의 다양성을 개선하기 위해 **MICBench**에서 LLVMQA[49]의 이미지에서 **(1)** 세 그룹으로 구성된 400 그룹과 네 그룹으로 구성된 400 그룹의 두 소스에서 이미지 그룹을 샘플링하고 원래 9개의 데이터 세트 [3, 6, 7, 12, 15, 18, 24, 60, 67]에서 소스하고 **(2)** 레이블이 지정되지 않은 데이터베이스 [3, 24, 41, 54]에서 샘플링된 1,000개의 무작위 이미지에서 **(1)** 세 그룹으로 구성된 600 그룹과 네 그룹으로 구성된 600 그룹을 샘플링한다. 총 2,000개 그룹의 **MICBench**에는 다양한 품질 문제와 낮은 수준의 출현이 포함되어 있어 품질 비교에 대한 편향되지 않은 평가를 제공한다.\n' +
      '\n' +
      '평가 양식: Multi-choice Questions(MCQs). LLM/LMM 벤치마크에 대한 가장 인기 있는 평가 양식으로서, **MICBench**의 평가 양식으로 Multi-choice Questions(MCQ)가 채택된다. 도 1에 도시된 바와 같다. 도 7에 도시된 바와 같이, 각각의 이미지 그룹은 이미지들 중 품질 또는 관련 속성들을 비교하는 전문가-조작된 질문과 연관된다. 일반적인 질문 유형(_Yes-or-No/What/How 등)에도 불구하고, **MICBench**는 또한 특별한 유형의 질문인 _Which_ 질문을 도입한다(Fig). 도 7(a)), 비교에 대한 이러한 일반적인 유형의 인간 질의를 커버하기 위해. 총 10명의 인간 전문가가 **MICBench**에 주석을 달기 위해 참여하며 각 MCQ의 답변은 다른 전문가가 교차 검토한다. 기존 벤치마크 [29, 49]와 유사하게 **MICBench**는 방법 개발을 위한 _dev_ 집합(1,004)과 LMM의 성능을 평가하기 위한 _test_ 집합(996)으로 더 나누어진다 (_answers will hidden from public_).\n' +
      '\n' +
      '도 7: **Dataset Card of MICBench**, for (a) _Which_ questions (60%), (b) _Yes-or-No_ questions (22%), 및 (c) _Other_ types of questions (18%)로 구성된, 3/4 이미지상의 데이터세트 카드.\n' +
      '\n' +
      '## 6 Evaluation\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '**Co-Instr**는 mPLUG-Owl2[57]의 공개된 체크포인트 후에 미세 조정되며, LLaMA-2[42]는 LLM으로, CLIP-ViT-L14[36]는 시각적 임베딩 모듈로 사용된다. 이미지는 정사각형으로 패딩한 후 모델에 투입하기 전에 \\(448\\times 448\\)으로 크기를 조정한다. 학습률은 \\(2e-5\\)으로 설정하였으며, 회분식 크기 192에서 두 개의 에폭으로 최종 체크포인트를 사용하여 평가하였다. 과적합을 피하기 위해 평가 데이터 세트의 _dev_ 서브세트만 최상의 훈련 하이퍼-파라미터를 선택하는 데 사용되며, 여기서 최종 보고된 결과는 중첩되지 않은 _test_ 서브세트에서 비롯된다. 모든 매개변수는 훈련 중에 업데이트되며 8*NVIDIA A100 GPU에서 총 25시간 동안 비용이 듭니다.\n' +
      '\n' +
      '### Baseline Models\n' +
      '\n' +
      'LLaVA-v1.5-13B[25], InternLM-XComposer2[5], BakLLaVA[39], EMU2-Chat[40], mPLUG-Owl2[57] (_*Co-Instr**의_baseline of_ **Co-Instr**)와 비교하여 다중 이미지 입력을 지원하는 5개의 오픈 소스 최신 LMM을 선택한다. 또한 Qwen-VL-Max, Gemini-Pro 및 GPT-4V(_*Co-Instr**의_teacher of_ **Co-Instr**)의 세 가지 잘 알려진 독점 근접 소스 모델과 비교한다.\n' +
      '\n' +
      '### 기존 평가 설정에 대한 결과\n' +
      '\n' +
      '*MICBench**(Sec. 5)에도 불구하고 또한 LMM에 대한 기존의 몇 가지 시각적 품질 평가/비교 벤치마크에서 기준 모델에 대해 제안된 **Co-Instr**를 평가한다. 평가 설정 및 결과는 다음과 같다.\n' +
      '\n' +
      '**Q-Bench\\({}^{\\tt PAIR}\\)-A1**[68]은 1,999개의 전문가 조작된 _open-range_ 품질 관련 MCQ와 _image pair_ 상의 시각적 품질 비교를 위한 벤치마크이다. 탭에서 2, 우리는 이 벤치마크에서 **Co-Instr**를 기존 오픈 소스 및 독점 모델과 비교한다. **Co-Instr**는 오픈 소스 LMM보다 훨씬 우수한 정확도를 보여준다: 그것은 기준선(mPLUG-Owl2)보다 더 나은 **64%**, 다중 이미지 하위 집합이 없는 변형(**Merge2Compare** 및 **Teach2Compare**)보다 더 나은 **51%**, 그리고 그들 중 최고보다 23% 더 우수하다. 또한 Qwen-VL-Max보다 성능이 우수합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c c|c c} \\hline \\multicolumn{1}{l|}{**Sub-categories**} & \\multicolumn{3}{c|}{**Question Types**} & \\multicolumn{3}{c|}{**Low-level Concern**} & \\multicolumn{3}{c|}{**Pairing Setting**} & \\multicolumn{1}{c}{} \\\\ \\multicolumn{1}{l|}{**Model**} & \\multicolumn{1}{c}{**Yoc-over-Net**} & \\multicolumn{1}{c}{**Whaf**} & \\multicolumn{1}{c|}{**Houf**} & \\multicolumn{1}{c|}{**Dideronf**} & \\multicolumn{1}{c|}{**Other**} & \\multicolumn{1}{c|}{**Compens**} & \\multicolumn{1}{c|}{**Joint**} & \\multicolumn{1}{c}{**Overall**} \\\\ \\hline random sparse accuracy & 50.00\\% & 32.03\\% & 33.16\\% & 38.99\\% & 41.95\\% & 38.69\\% & 43.70\\% & 39.82\\% \\\\ (Seg/2023) LLAVA+15-13B & 57.24\\% & 47.50\\% & 44.31\\% & 49.01\\% & 59.51\\% & 52.00\\% & 52.60\\% \\\\ (Det/2023) BakLLAVA & 60.00\\% & 44.42\\% & 50.86\\% & 53.00\\% & 58.82\\% & 54.82\\% & 56.55\\% & 52.75\\% \\\\ (Net/2023) mPLUG-Owl2 (_baseline of_ **Co-Int****)** & 58.07\\% & 36.61\\% & 48.44\\% & 47.27\\% & 51.90\\% & 45.73\\% & 60.00\\% & 48.94\\% \\\\ (Det/2023) Erm-Chat & 51.94\\% & 29.28\\% & 58.54\\% & 42.01\\% & 58.71\\% & 46.28\\% & 40.00\\% & 47.08\\% \\\\ (Det/2023) HendLM-XComposer2-VL & 74.13\\% & 58.64\\% & 62.28\\% & 67.73\\% & 64.34\\% & 48.00\\% & 65.19\\% \\\\ (Det/2023) U-VL-Max (_Propitared_) & 67.68\\% & 67.56\\% & 55.53\\% & 60.00\\% & 61.18\\% & 68.65\\% & 61.29\\% & 69.37\\% \\\\ Gemini-Pro (_Propitared_) & 66.78\\% & 56.61\\% & 56.74\\% & 60.42\\% & 60.55\\% & 60.04\\% & 60.44\\% & 60.46\\% \\\\ \\hline _Non-segert Human_ & **79.75\\%** & **69.49\\%** & **84.42\\%** & **77.32\\%** & **79.93\\%** & **81.00\\%** & **68.00\\%** & **78.07\\%** \\\\ \\hline \\multicolumn{1}{l|}{without _Multi-image Competitive Data_} & **78.11\\%** & 77.06\\% & 82.33\\% & 78.17\\% & 77.22\\% & 80.20\\% & 76.30\\% & 80.12\\% \\\\ \\hline \\multicolumn{1}{l|}{**Co-Int****)** (Ours)} & **78.05\\%** & **77.20\\%** & **79.23\\%** & **80.00\\%** & **80.62\\%** & **81.91\\%** & **74.22\\%** & **80.18\\%** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: _Q-Bench\\({}^{\\tt PAIR}\\)-A1_에 대한 결과. **Co-Instr**는 비교 데이터_가 없는 변이체 _*51%**보다 현저히 우수하며, 인간의 능력을 능가하는 유일한 LMM이다.\n' +
      '\n' +
      '그리고 Gemini-Pro는 큰 마진(21%/33%)으로 나타났다. 또한, 모든 MCQ 교육 데이터는 GPT-4V에서 가져온 것이지만, 학생(**Co-Instruct**)은 여전히 주목할 만한 **2.7%**로 이 MCQ 평가 세트에서 교사보다 우수하므로 협력적 교수 전략의 효과를 시사한다. 우리의 모델은 또한 비전문가 인간(_esp._ on Compare subset)의 정확도를 능가하는 유일한 LMM_이다. 이 벤치마크에서는 향후 실제 시각적 품질 비교에 대한 인간 작업을 완화하기 위해 모델을 사용하는 의미 있는 비전을 강력하게 지원한다.\n' +
      '\n' +
      '**Q-Bench\\({}^{\\text{PAIR}}\\)-A2**는 일반적이고 상세한 시각적 품질 비교를 위한 벤치마크 설정으로 _image pair_에 대한 상세 추론_with detailed reasonings_이다. 499개의 이미지 쌍으로 구성된 GPT는 GPT를 사용하여 **완전성**, **정밀** 및 **관련성**에 대한 _golden_ 전문가 라벨 비교에 대한 LMM 반응을 평가한다. 탭에 나열된 대로입니다. 도 3을 참조하면, **Co-Instruct**는 비교 출력의 **Completeness**(+57%) 및 **Precision**(+59%)를 _w/o 비교 데이터_ 버전보다 현저하게 향상시킬 수 있지만, 여전히 정량적 메트릭에서 GPT-4V에 약간 뒤쳐진다. 이는 GPT-4V로부터의 출력이 **Co-Instruct** 출력보다 _2배 이상 긴_인 반면 GPT 평가는 _더 긴 텍스트 출력보다 유리한 것으로 관찰되기 때문일 수 있다. 여기서 이러한 잠재적 바이어스가 발생하는 위치를 검증하기 위해 그림 8에서 서로 다른 LMM의 결과를 정성적으로 시각화한다. 그림과 같이 그림\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c c|c c c|c} \\hline\n' +
      '**Dimensions** &\\multicolumn{3}{c|}{**Completeness**} &\\multicolumn{3}{c|}{**Precision**} &\\multicolumn{3}{c|}{**Relevance**} &\\multirow{2}{*}{\\(S_{\\text{un}\\}}}}}\\\\\n' +
      '**Model** & \\(F_{\\text{s}}\\) & \\(F_{\\text{s}}\\) & \\(F_{\\text{s}}\\) & & & \\(F_{\\text{s}}\\) & \\(F_{\\text{s}}\\) & \\(F_{\\text{s}}\\) & \\(F_{\\text{s}}\\) & \\(F_{\\text{s}}\\) \\\\ \\hline (**exp/2022) LLAVA+1-5-3IB & 18:77\\% & 73.44\\% & 7.99\\% & 0.80 & 34.00\\% & 38.72\\% & 24.62\\% & 0.20 & 1.02\\% & 34.39\\% & 64.39\\% & 1.63 & 3.44 \\\\ (**fc1022**) BNLLAVA & 29.46\\% & 50.77\\% & 10.57\\% & 0.80 & 40.85\\% & 38.06\\% & 21.33\\% & 0.80 & 2.26\\% & 15.04\\% & 58.20\\% & 1.79 & 3.40 \\\\ (**fc1022**) BNLLAVA & 29.46\\% & 50.77\\% & 10.57\\% & 0.80 & 40.85\\% & 30.86\\% & 21.33\\% & 0.80 & 2.26\\% & 15.04\\% & 58.20\\% & 1.79 & 3.40 \\\\ (**fc1022**) BNLLAVA & 29.46\\% & 65.54\\% & 14.45\\% & 0.94 & 30.94\\% & 31.71\\% & 24.64\\% & 0.82 & 3.70\\% & 20.94\\% & 64.88\\% & 1.63 & 3.50 \\\\ (**fc1022**) BNLLAVA & 41.25\\% & 54.33\\% & 4.42\\% & 0.63 & 13.18\\% & 31.84\\% & 24.84\\% & 0.82 & 4.12\\% & 38.61\\% & 57.27\\% & 1.85 & 3.60 \\\\ (**fc1022**) BNLLAVA & 13.07\\% & 72.17\\% & 14.13\\% & 1.00 & 31.26\\% & 24.13\\% & 23.75\\% & 0.93 & 1.60\\% & 24.17\\% & 22.98\\% & 1.70 & 3.64 \\\\ (**fc1022**) BNLLAVA (_Propitency_) & 11.64\\% & 54.08\\% & 54.08\\% & 1.22 & 24.26\\% & 20.15\\% & 25.62\\% & 1.11 & 2.23\\% & 10.95\\% & 58.56\\% & 1.52 & 4.16 \\\\ Gemini-Pro (_Propitency_) & 18.28\\% & 44.48\\% & 88.64\\% & 1.18 & 13.37\\% & 23.96\\% & 20.02\\% & 0.65\\% & 5.91\\% & 92.22\\% & 1.90 & 4.60 \\\\ GPT-4V (_Propitency_, _twicealer of **Ours**) & 4.00\\% & 31.82\\% & 64.09\\% & **1.00** & 40.45\\% & 41.25\\% & 44.44\\% & **1.34** & 0.18\\% & 1.00\\% & 69.35\\% & **1.94** & **4.89** \\\\ \\hline _w/o Multi-Image Computer Data_ & 15.25\\% & 63.76\\% & 18.32\\% & 1.02 & 30.44\\% & 40.18\\% & 19.62\\% & 0.79 & 0.06\\% & 9.86\\% & 80.02\\% & 1.87 & 3.60 \\\\ _w/o Multi-Image Computer Data_ & 15.25\\% & 63.76\\% & 18.32\\% & 1.02 & 30.44\\% & 40.18\\% & 19.62\\% & 0.79 & 0.06\\% & 9.86\\% & 80.02\\% & 1.87 & 3.60 \\\\ _**Co-Instrect** (Ours) & 4.04\\% & 51.55\\% & 63.55\\% & **1.58** & 15.68\\% & 43.60\\% & 41.37\\% & **1.26** & 0.05 & 0.44\\% & 58.22\\% & **1.56** & **4.82** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: _Q-Bench\\({}^{\\text{PAIR}\\)-A2_. \\ (P_{i}\\)는 스코어 \\(i\\)에 대한 빈도(score in \\([0,2]\\))를 나타낸다. GPT-4V보다 약간 열등한 반면, **Co-Instr**는 비교 데이터 없이_(+31%), 특히 **정밀** 메트릭(+59%)**의 경우 두 변형 모두에 비해 크게 개선되었다.\n' +
      '\n' +
      '그림 8: _Q-Bench\\({}^{\\text{PAIR}}\\)-A2_ 상의 **정성적 시각화. GPT-4V는 여기서 잘못된 비교를 하더라도 가장 긴 출력을 제공하고 가장 높은 정밀도 점수를 달성한다.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.: VQA: Visual Question Answering. In: IEEE ICCV. pp. 2425-2433 (2015)\n' +
      '* [2] Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: ShareGPT4V: Improving large multi-modal models with better captions. CoRR **abs/2311.12793** (2023)\n' +
      '* [3] Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollar, P., Zitnick, C.L.: Microsoft COCO captions: Data collection and evaluation server. CoRR **abs/1504.00325** (2015)\n' +
      '* [4] Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: InstructBLIP: Towards general-purpose vision-language models with instruction tuning. CoRR **abs/2305.06500** (2023)\n' +
      '* [5] Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., Zhang, W., Li, Y., Yan, H., Gao, Y., Zhang, X., Li, W., Li, J., Chen, K., He, C., Zhang, X., Qiao, Y., Lin, D., Wang, J.: InternLM-XComposer2: Mastering free-form text-image composition and comprehension in vision-language large model. CoRR **abs/2401.16420** (2024)\n' +
      '* [6] Fang, Y., Zhu, H., Zeng, Y., Ma, K., Wang, Z.: Perceptual quality assessment of smartphone photography. In: IEEE CVPR. pp. 3677-3686 (2020)\n' +
      '* [7] Ghadiyaram, D., Bovik, A.C.: Massive online crowdsourced study of subjective and objective picture quality. IEEE TIP **25**(1), 372-387 (2016)\n' +
      '* [8] Golestaneh, S.A., Dadsetan, S., Kitani, K.M.: No-reference image quality assessment via transformers, relative ranking, and self-consistency. In: IEEE WACV. pp. 3209-3218 (2022)\n' +
      '* [9] Google: Gemini Pro (2023), [https://deepmind.google/technologies/gemini](https://deepmind.google/technologies/gemini)\n' +
      '* [10] Gu, J., Cai, H., Chen, H., Ye, X., Ren, J., Dong, C.: PIPAL: A large-scale image quality assessment dataset for perceptual image restoration. In: ECCV. pp. 633-651 (2020)\n' +
      '* [11] Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., Steinhardt, J.: Measuring massive multitask language understanding. In: ICLR. pp. 1-10 (2021)\n' +
      '* [12] Hosu, V., Lin, H., Sziranyi, T., Saupe, D.: Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE TIP **29**, 4041-4056 (2020)\n' +
      '* [13] Hudson, D.A., Manning, C.D.: GQA: A new dataset for real-world visual reasoning and compositional question answering. In: IEEE CVPR. pp. 6700-6709 (2019)\n' +
      '* [14] ITU-R, B.T.: Methodology for the subjective assessment of the quality of television pictures. [https://www.itu.int/rec/R-REC-BT.500](https://www.itu.int/rec/R-REC-BT.500) (2002)\n' +
      '* [15] Jayaraman, D., Mittal, A., Moorthy, A.K., Bovik, A.C.: Objective quality assessment of multiply distorted images. In: ASILOMAR. pp. 1693-1697 (2012)\n' +
      '* [16] Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.S., de las Casas, D., Hanna, E.B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L.R., Saulnier, L., Lachaux, M.A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T.L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., Sayed, W.E.: Mixrl of experts. CoRR **abs/2401.04088** (2024)\n' +
      '* [17] LAION: LAION GPT-4V dataset. [https://huggingface.co/datasets/laion/gpt4v-dataset](https://huggingface.co/datasets/laion/gpt4v-dataset) (2023)\n' +
      '* [18] Li, C., Zhang, Z., Wu, H., Sun, W., Min, X., Liu, X., Zhai, G., Lin, W.: AGIQA-3K: An open database for ai-generated image quality assessment. CoRR **2306.04717** (2023)* [19] Li, D., Jiang, T., Jiang, M.: Quality assessment of in-the-wild videos. In: ACM MM. pp. 2351-2359 (2019)\n' +
      '* [20] Li, D., Jiang, T., Lin, W., Jiang, M.: Which has better visual quality: The clear blue sky or a blurry animal? IEEE TMM **21**(5), 1221-1234 (2019)\n' +
      '* [21] Li, J., Mantiuk, R., Wang, J., Ling, S., Le Callet, P.: Hybrid-MST: A hybrid active sampling strategy for pairwise preference aggregation. In: NeurIPS. pp. 1-11 (2018)\n' +
      '* [22] Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., Hashimoto, T.B.: AlpacaEval: An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval) (2023)\n' +
      '* [23] Li, Y., Wang, S., Zhang, X., Wang, S., Ma, S., Wang, Y.: Quality assessment of end-to-end learned image compression: The benchmark and objective measure. In: ACM MM. pp. 4297-4305 (2021)\n' +
      '* [24] Lin, H., Hosu, V., Saupe, D.: KADID-10k: A large-scale artificially distorted iqa database. In: QoMEX. pp. 1-3 (2019)\n' +
      '* [25] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning. CoRR **abs/2310.03744** (2023)\n' +
      '* [26] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. CoRR **abs/2304.08485** (2023)\n' +
      '* [27] Liu, X., Van De Weijer, J., Bagdanov, A.D.: RankIQA: Learning from rankings for no-reference image quality assessment. In: IEEE ICCV. pp. 1040-1049 (2017)\n' +
      '* [28] Liu, X., Van De Weijer, J., Bagdanov, A.D.: Exploiting unlabeled data in cnns by self-supervised learning to rank. IEEE TPAMI **41**(8), 1862-1878 (2019)\n' +
      '* [29] Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., Chen, K., Lin, D.: MMBench: Is your multi-modal model an all-around player? CoRR **abs/2307.06281** (2023)\n' +
      '* [30] Ma, K., Liu, W., Liu, T., Wang, Z., Tao, D.: dipIQ: Blind image quality assessment by learning-to-rank discriminable image pairs. IEEE TIP **26**(8), 3951-3964 (2017)\n' +
      '* [31] Mantiuk, R.K., Tomaszewska, A., Mantiuk, R.: Comparison of four subjective methods for image quality assessment. In: Computer Graphics Forum. vol. 31, pp. 2478-2491 (2012)\n' +
      '* [32] Michaelis, C., Mitzkus, B., Geirhos, R., Rusak, E., Bringmann, O., Ecker, A.S., Bethge, M., Brendel, W.: Benchmarking robustness in object detection: Autonomous driving when winter is coming. CoRR **abs/1907.07484** (2019)\n' +
      '* [33] Mikhailiuk, A., Wilmot, C., Perez-Ortiz, M., Yue, D., Mantiuk, R.K.: Active sampling for pairwise comparisons via approximate message passing and information gain maximization. In: IEEE ICPR. pp. 2559-2566 (2021)\n' +
      '* [34] OpenAI: Gpt-4 technical report (2023)\n' +
      '* [35] Prashnani, E., Cai, H., Mostofi, Y., Sen, P.: PieAPP: Perceptual image-error assessment through pairwise preference. In: IEEE CVPR. pp. 1808-1817 (2018)\n' +
      '* [36] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML. pp. 8748-8763 (2021)\n' +
      '* [37] Rajkumar, A., Agarwal, S.: When can we rank well from comparisons of o (nlog (n)) non-actively chosen pairs? In: Conference on Learning Theory. pp. 1376-1401 (2016)\n' +
      '* [38] Schwenk, D., Khandelwal, A., Clark, C., Marino, K., Mottaghi, R.: A-OKVQA: A benchmark for visual question answering using world knowledge. In: ECCV. pp. 146-162 (2022)* [39] SkunkworksAI: BakLLaVA (2024), [https://github.com/SkunkworksAI/BakLLaVA](https://github.com/SkunkworksAI/BakLLaVA) 3, 8, 10\n' +
      '* [40] Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Luo, Z., Wang, Y., Rao, Y., Liu, J., Huang, T., et al.: Generative multimodal models are in-context learners. CoRR **abs/2312.13286** (2023)\n' +
      '* [41] Thomee, B., Shamma, D.A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., Li, L.J.: YFCC100M: The new data in multimedia research. Commun. ACM **59**(2), 64-73 (2016)\n' +
      '* [42] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.S., Lachaux, M.A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.M., Subramanian, R., Tan, X.E., Tang, B., Taylor, R., Williams, A., Kuan, J.X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., Scialom, T.: Llama 2: Open foundation and fine-tuned chat models. CoRR **abs/2307.09288** (2023)\n' +
      '* [43] Tsukida, K., Gupta, M.R.: How to analyze paired comparison data (Technical Report UWEETR-2011-0004, University of Washington, 2011), [https://api.semanticscholar.org/CorpusID:15425240](https://api.semanticscholar.org/CorpusID:15425240)\n' +
      '* [44] Wang, L., Yang, N., Huang, X., Yang, L., Majumder, R., Wei, F.: Improving text embeddings with large language models. CoRR **abs/2401.00368** (2024)\n' +
      '* [45] Wauthier, F., Jordan, M., Jojic, N.: Efficient ranking from pairwise comparisons. In: ICML. pp. 109-117 (2013)\n' +
      '* [46] Wu, H., Chen, C., Hou, J., Liao, L., Wang, A., Sun, W., Yan, Q., Lin, W.: FAST-VQA: Efficient end-to-end video quality assessment with fragment sampling. In: ECCV. pp. 538-554 (2022)\n' +
      '* [47] Wu, H., Chen, C., Liao, L., Hou, J., Sun, W., Yan, Q., Gu, J., Lin, W.: Neighbourhood representative sampling for efficient end-to-end video quality assessment. IEEE TPAMI (2023)\n' +
      '* [48] Wu, H., Zhang, E., Liao, L., Chen, C., Hou, J., Wang, A., Sun, W., Yan, Q., Lin, W.: Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In: IEEE ICCV (2023)\n' +
      '* [49] Wu, H., Zhang, Z., Zhang, E., Chen, C., Liao, L., Wang, A., Li, C., Sun, W., Yan, Q., Zhai, G., Lin, W.: Q-bench: A benchmark for general-purpose foundation models on low-level vision. In: ICLR. pp. 1-13 (2023)\n' +
      '* [50] Wu, H., Zhang, Z., Zhang, E., Chen, C., Liao, L., Wang, A., Li, C., Sun, W., Yan, Q., Zhai, G., Lin, W.: Q-Bench: A benchmark for general-purpose foundation models on low-level vision. In: ICLR (2024)\n' +
      '* [51] Wu, H., Zhang, Z., Zhang, E., Chen, C., Liao, L., Wang, A., Xu, K., Li, C., Hou, J., Zhai, G., et al.: Q-instruct: Improving low-level visual abilities for multi-modality foundation models. CoRR **abs/2311.06783** (2023)\n' +
      '* [52] Wu, H., Zhang, Z., Zhang, W., Chen, C., Liao, L., Li, C., Gao, Y., Wang, A., Zhang, E., Sun, W., et al.: Q-Align: Teaching lmms for visual scoring via discrete text-defined levels. CoRR **abs/2312.17090** (2023)\n' +
      '* [53] Wu, X., Sun, K., Zhu, F., Zhao, R., Li, H.: Better aligning text-to-image models with human preference. CoRR **abs/2303.14420** (2023)\n' +
      '*[*[54] Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., Dong, Y.: ImageReward: Text-to-Image 생성에 대한 인간의 선호도를 학습하고 평가한다. CoRR **abs/2304.05977** (2023)\n' +
      '* [55] Ye, P., Doermann, D.: Active sampling for subjective image quality assessment. In: IEEE CVPR. pp. 4249-4256 (2014)\n' +
      '* [56] Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., Jiang, C., Li, C., Xu, Y., Chen, H., Tian, J., Qi, Q., Zhang, J., Huang, F.: mPLUG-Owl: Modularization empowers large language models with multimodality. CoRR **abs/2304.14178** (2023)\n' +
      '* [57] Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou, J.: mPLUG-Owl2: Revolutionizing multi-modal large language model with modality collaboration. CoRR **abs/2311.04257** (2023)\n' +
      '* [58] Yim, J.G., Wang, Y., Birkbeck, N., Adsumilli, B.: Subjective quality assessment for youtube UGC dataset. In: IEEE ICIP. pp. 1-5 (2020)\n' +
      '* [59] Yin, Z., Wang, J., Cao, J., Shi, Z., Liu, D., Li, M., Sheng, L., Bai, L., Huang, X., Wang, Z., et al.: LAMM: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. CoRR **abs/2306.06687** (2023)\n' +
      '* [60] Ying, Z., Niu, H., Gupta, P., Mahajan, D., Ghadiyaram, D., Bovik, A.: From patches to pictures (PaQ-2-PiQ): Mapping the perceptual space of picture quality. In: IEEE CVPR. pp. 3575-3585 (2020)\n' +
      '* [61] Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al.: MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. CoRR **abs/2311.16502** (2023)\n' +
      '* [62] Zhang, C., Su, S., Zhu, Y., Yan, Q., Sun, J., Zhang, Y.: Exploring and evaluating image restoration potential in dynamic scenes. In: IEEE CVPR. pp. 2057-2066 (2022)\n' +
      '* [63] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: IEEE CVPR. pp. 586-595 (2018)\n' +
      '* [64] Zhang, W., Ma, K., Zhai, G., Yang, X.: Uncertainty-aware blind image quality assessment in the laboratory and wild. IEEE TIP **30**, 3474-3486 (2021)\n' +
      '* [65] Zhang, W., Zhai, G., Wei, Y., Yang, X., Ma, K.: Blind image quality assessment via vision-language correspondence: A multitask learning perspective. In: IEEE CVPR. pp. 14071-14081 (2023)\n' +
      '* [66] Zhang, W., Liu, Y., Dong, C., Qiao, Y.: RankSRGan: Generative adversarial networks with ranker for image super-resolution. In: ICCV. pp. 3096-3105 (2019)\n' +
      '* [67] Zhang, Z., Sun, W., Wang, T., Lu, W., Zhou, Q., Wang, Q., Min, X., Zhai, G., et al.: Subjective and objective quality assessment for in-the-wild computer graphics images. ACM TOMM **20**(4), 1-22 (2023)\n' +
      '* [68] Zhang, Z., Wu, H., Zhang, E., Zhai, G., Lin, W.: A benchmark for multi-modal foundation models on low-level vision: from single images to pairs. CoRR **abs/2402.07116** (2024)\n' +
      '* [69] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: MiniGPT-4: Enhancing vision-language understanding with advanced large language models. CoRR **abs/2304.10592** (2023)\n' +
      '* [70] Zhu, H., Sui, X., Chen, B., Liu, X., Chen, P., Fang, Y., Wang, S.: 2AFC prompting of large multimodal models for image quality assessment. CoRR **abs/2402.01162** (2024)\n' +
      '* [\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>