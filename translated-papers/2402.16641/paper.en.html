<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Towards Open-ended Visual Quality Comparison\n' +
      '\n' +
      'Haoning Wu\n' +
      '\n' +
      'Equal contribution. \\({}^{1}\\)Nanyang Technological University. \\({}^{2}\\)City University of Hong Kong. \\({}^{3}\\)Shanghai Jiao Tong University. \\({}^{4}\\)Sensetime Research.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Image quality assessment (IQA) has been an important domain in visual computing, as it provides effective recommendation [58] on high-quality visual contents and valuable guidance [54, 62, 66] for potential improvements. Most recently, several pioneer studies [50, 51, 52] have explored large multi-modality models (LMMs, _e.g_. GPT-4V) [57, 4, 5, 9, 25, 34], on expanding IQA from giving a scalar score (_e.g_. 3.457) to the open-ended scenarios, that allows evaluations in response to _open-range questions_, and provide _detailed reasonings_ beyond an overall score.\n' +
      '\n' +
      'While these studies sufficiently _emulate human ability_ on IQA, they also suffer from the same drawback as human: **ambiguity on absolute evaluations**. For instance, as shown in Fig. 2 (a), different human observers hold different standards on the exposure level on single images, and henceforth provide diversified absolute evaluations. Nevertheless, while asked to compare the exposure level of the images, all observers agree with the rank (1)>(2)>(3)>(4) (Fig. 2(b)); all observers also agree that (1) has the highest exposure, though not all choose the option _high_ while evaluating it independently. Given this observation, comparison has been a traditional human study setting for quality evaluation [43] and adopted by a wide range of existing subjective studies [10, 35, 53, 54, 63]. Furthermore, to avoid the ambiguity, the comparative settings are also predominantly adopted [54, 66] while applying IQA algorithms for improvement guidance.\n' +
      '\n' +
      'While comparison has widely-recognized significance for IQA, existing related datasets [35] and methods [28, 64] are generally based on overall quality comparison and have not extended to the open-ended scenarios; on the other hand, open-source LMMs [51, 4, 25] are usually only fine-tuned with _single image_ instruction tuning datasets [1, 13, 26] and proved to lack enough capability even on two image comparison settings [68, 70]. While these gaps have clearly indicated the need of a specific instruction tuning dataset for visual quality comparison, it is too expensive to collect such a dataset from human. To avoid costly human labors, we propose an alternative strategy to collect the training dataset, named _Collaborative Instruction Tuning from Weak Supervisors_ (_Co-Instruct_). Specifi\n' +
      '\n' +
      'Figure 2: **The motivation of open-ended visual quality comparison: comparative settings can effectively avoid the **ambiguity on absolute evaluations** for single images, and provide more clear-cut judgements to serve as downstream guidances [54, 66].\n' +
      '\n' +
      'cally, we adopt two non-perfect supervisors: **1)****Merge2Compare**. Originated from single image human quality descriptions of 19K images in Q-Pathway [51], we randomly match them into 100K groups with removing the most similar descriptions with an text embedding model [44]. Then, similar as the construction of LLaVA-150K [26], we prompt a single-modal LLM [16] to compare multiple human descriptions in a group, and "_merge_" them into 100K pseudo comparisons (as illustrated in Fig. 3(a)). **2)****Teach2Compare**. Observing that GPT-4V has especially high accuracy on pairwise settings [70, 68] among existing LMMs, following existing practices [2, 17], we leverage GPT-4V responses to further expand our dataset. We collect 9K unlabeled images and randomly match into 30K image groups (_2-4 images per group_), and obtain GPT-4V responses on both _caption-like_ general comparisons and _question-answer pairs_ for comparisons. By integrating Q-Instruct-200K [51] (_on single images_), **Merge2Compare**, and **Teach2Compare**, we construct the Co-Instruct-562K, the first instruction tuning dataset designed for open-ended multi-image quality comparison.\n' +
      '\n' +
      'To correctly refer to each specific image during conversation, we define a specific image-text interleaved format [5] to handle multi-image cases, as follows: User: The first image: <img0> The second image: <img1>... <query> Assistant: <response>\n' +
      '\n' +
      'As we need to feed multiple images together during instruction tuning, adopting the most popular LLaVA [25, 26, 39] structure that linearly projects visual embeddings will exceed the context window of the language models [42] and cause errors. Henceforth, we adopt an alternative visual abstractor structure [57] to first reduce visual token length (_from 1025 to 65 tokens per image_), and then concatenate them with text embeddings to pass to language decoders. By learning from the Co-Instruct-562K dataset and the specially-designed input structure, we present the **Co-Instruct**, with up to **86%** improvements than its baseline [57], and **61%** better than the state-of-the-art open-source LMM. More importantly, despite using GPT-4V as one of its teachers, it still surpasses the GPT-4V _teacher_ in a variety of multi-choice question (MCQ) benchmarks, and matches GPT-4V ability in scenarios requiring detailed language reasonings.\n' +
      '\n' +
      'Figure 3: **The construction methodology of Co-Instruct-562K, a combination of (a)****Merge2Compare** (_LLM comparison from human-labeled single image quality descriptions_) and (b) **Teach2Compare** (_GPT-4V comparison on multiple unlabeled images_).\n' +
      '\n' +
      'After training the model **Co-Instr**, our another concern is the lack of abundant evaluation settings on multi-image comparison: while Q-Bench [50, 68] series have covered multiple formats on single images and image pairs, there is no existing evaluation scenario for quality comparison **beyond two images**. To complement this evaluation setting, we construct the **MICBench**. Specifically, the **MICBench** contains 2,000 multi-choice questions (MCQ) comparing the quality or related attributes among a group of three or four images (_each half_), in which over half of the questions are _Which_ questions (_e.g. which image has highest clarity?_). Aiming to extract an image with a specific quality-related appearance from a group, _Which_ questions are the most important questions related to image comparison. Despite _Which_ questions, the **MICBench** also includes _Yes-or-No_ questions and other question types (_What/How/How-Many, etc_) to provide a holistic benchmark setting for multi-image quality comparison.\n' +
      '\n' +
      'In summary, we conduct a systematical study towards _open-ended_ visual quality comparison. Our contributions can be summarized as three-fold:\n' +
      '\n' +
      '1. We construct the first instruction-tuning **dataset** for visual quality comparison, the Co-Instruct-562K. With data collected from two "_weak supervisors_", **Merge2Compare** (LLM-merged comparisons) and **Teach2Compare** (GPT-4V pseudo-labeled comparisons), our public dataset significantly expands the capabilities of open-source LMMs on visual comparative settings.\n' +
      '2. We propose the most capable **model** for open-ended visual comparison, the **Co-Instruct**. With image-text interleaved input format and fine-tuned with the Co-Instruct-562K dataset, it significantly outperforms existing methods (and even GPT-4V) in multiple open-ended visual comparison tasks. With open weights, it allows for broader application than proprietary models.\n' +
      '3. We construct the **benchmark**, the **MICBench**, as the first benchmark to evaluate LMMs for quality comparison on multiple images (more than two). It covers 2,000 diverse-type open-range MCQs related to visual quality comparison among three or four images. The **MICBench** contributes to more holistic evaluation studies on the visual quality comparison problem.\n' +
      '\n' +
      '## 2 Related Works\n' +
      '\n' +
      '### Visual Quality Comparison\n' +
      '\n' +
      'Visual quality comparison (_especially_ paired comparison) is a widely used subjective quality assessment methodology, serving as the most reliable way to collect human opinions [31]. However, when the number of images increases, the experiments become infeasible because of the exponential growth of pairwise comparisons [14]. While many active sampling methods have been proposed to reduce the number of pairs [21, 33, 55], they are computationally expensive and unsuitable for large-scale experiments. Despite subjective studies, learning to rank quality is widely proven as effective by many objective approaches [8, 27, 30, 46, 48, 64, 65]. Nonetheless, they typically only predict a scalar score or a binary judgement for overall comparison, limiting their ability to provide meaningful feedbacks into specific types of distortions or preferences.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n' +
      'Step 1: Pair/Group Matching (Fig. 4(a)).To best utilize information from existing single image quality descriptions, following the empirical rule to sample \\(O(n\\log n)\\) pairwise combinations to effectively rank among all individual items in a set [37, 45], we randomly sample 81K image pairs from all 19K images in the Q-Pathway. Despite pairs, we further sample 27K groups with three images and 18K groups with four images to cover the scenarios of more images.\n' +
      '\n' +
      'Step 2: Top-Similarity Pair Removal (Fig. 4(b)).The effectiveness of the _merge_ comes from the differences among descriptions, _e.g._ between _The quality is acceptable_ for <img> and _The quality is poor_ for <img1>. However, if descriptions in a pair/group contains almost the same information (_e.g._ both images with _The clarity is good, but lighting is dark_), the _merged_ comparisons will lack enough information or even with false predictions. Henceforth, we use E5-Mistral [44] text embedding model to compute similarities among descriptions, and remove if _any_ high-similarity description pairs exist in the group. After removal, 70K image pairs (86% of initial samples), 20K groups of three (74% of initial) and 10K groups of four (55% of initial) are preserved and fed into the LLM for _merging_.\n' +
      '\n' +
      'Step 3: LLM Merging (Fig. 4(c)).The key step for the **Merge2Compare** is to prompt LLMs to convert the single image evaluations to comparative texts. Specifically, following many existing practices [69, 26, 59], we put the descriptions as alternates of images in the context. Denote the description for image <img1> as <desc1>, the user query for LLM _merging_ is formulated as follows:\n' +
      '\n' +
      '(Pairs) The first image: <desc0> The second image: <desc1> Which image has better quality, and why?\n' +
      '\n' +
      '(Groups of Three/Four) {The \\(K_{i+1}\\) image: <desc1> \\(|_{i=0}^{N-1}\\)}\n' +
      '\n' +
      'Please rank the quality of the images and justify your rankings.\n' +
      '\n' +
      'where \\(K_{i}\\) represents the ordinal form of \\(i+1\\), _e.g._\\(K_{1}\\) is first, \\(K_{2}\\) is second.\n' +
      '\n' +
      'The _merged_ comparisons are **overall comparisons with reasonings** (Fig. 4_right_). To validate their reliability, we conducted a human examination on 250 random samples from **Merge2Compare** mixed with merged comparisons from 250 random removed groups. The correctness rate in **Merge2Compare** is 96%, while it is only 72% in the removed groups, demonstrating the effects of the top-similarity removal process in ensuring the quality of training data.\n' +
      '\n' +
      'Figure 4: The pipeline of constructing **Merge2Compare**: images are first matched into groups **(a)**, and then filtered via top-similarity removal **(b)**. After filtering, the single image quality descriptions are _merged_**(c)** into comparisons by the LLM [16].\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '### Rationale of Combinations.\n' +
      '\n' +
      'As discussed in principle 3, our motivation is to collect subsets that can complement each other. This complementarity is reflected in the following two aspects. Firstly, in terms of general comparisons, **Merge2Compare** has higher accuracy but lacks fine-grained comparison (excluded by _Top-similarity Pair Removal_), while **Teach2Compare**-_general_, although slightly less accurate, offers more diverse scenarios and includes content information as background. Joint training of both contributes to a more comprehensive quality comparison by our model. Additionally, **Teach2Compare** includes a unique _Q&A_ subset, which significantly enhances the model\'s ability to answer open-range questions.\n' +
      '\n' +
      '## 4 The Co-Instruct Model\n' +
      '\n' +
      'In this section, we discuss the proposed model, **Co-Instruct**. Specifically, we have made two non-trivial adaptations for the multi-image comparative setting:\n' +
      '\n' +
      'Visual Token Reduction (Fig. 6 (a)).Most state-of-the-art LMMs [5, 25, 39] have adopted the simple projector that keeps a large number of tokens (_e.g._ 1025). This structure is not friendly to multi-image scenarios: passing only two images will exceed the max length of LLaVA [26] (2048), and four images will exceed the context window of LLaMA-2 [42] (4096). Thus, we adopt another widely-used abstractor [56, 4, 57] structure to reduce token numbers before feeding the visual embeddings to LLM, so as to easily adapt to multi-image scenarios.\n' +
      '\n' +
      'Image-text Interleaved Format (Fig. 6 (c)).Typical single-image instruction tuning usually does not care about "_position of images_". Most approaches [4, 25, 69] directly pile all images before texts (<img0>(<img1>...)<text>). While this will not cause much troubles if we query about **joint** information from multiple images, it is observed to introduce hallucinations across images during comparison. To solve this, we propose an image-text interleaved format for multi-image training, that each image is started with explicit text to identify its nominal:\n' +
      '\n' +
      'User: The first image: <img0> The second image: <img1> (...) <query> Assistant: <response>\n' +
      '\n' +
      'In our experiments, we demonstrated that this interleaved format significantly enhances the performance of **Co-Instruct** (Tab. 8), notably better than using learnable special tokens (<img_st> and <img_end>) to divide images.\n' +
      '\n' +
      'Figure 6: **The structure of **Co-Instruct.** (a) Images are encoded by visual embedding layers and then passsed through an abstractor module to reduce token numbers, and then **(c)** fused with text embeddings into under the image-text interleaved format.\n' +
      '\n' +
      '## 5 The MICBench\n' +
      '\n' +
      'In this section, we discuss the proposed **MICBench** to cover the open-ended evaluation settings on groups of three or four images, as a complementary of existing evaluation settings (Sec. 6.3). It contains 2,000 groups of _open-range_ questions equipped with multiple candidates, with details elaborated as follows:\n' +
      '\n' +
      'Sourcing Diverse Image Groups.To improve the diversity of the benchmark, in the **MICBench**, we sample image groups from two sources: **(1)** 400 groups of three and 400 groups of four from the images in LLVMQA [49], which are originally sourced from 9 datasets [3, 6, 7, 12, 15, 18, 24, 60, 67]; **(2)** 600 groups of three and 600 groups of four on 1,000 random images sampled from unlabeled databases [3, 24, 41, 54] (_zero overlap with training-set images_). With in-total 2,000 groups, the **MICBench** contains a wide variety of quality issues and low-level appearances, providing a non-biased evaluation on quality comparison.\n' +
      '\n' +
      'Evaluation Form: Multi-choice Questions (MCQs).As the most popular evaluation form for LLM/LMM benchmarks [11, 29, 49, 61], multi-choice question (MCQ) is adopted as the evaluation form of **MICBench**. As is shown in Fig. 7, each image group is associated with a expert-crafted question that compare quality or related attributes among the images. Despite common question types (_Yes-or-No/What/How, etc_), the **MICBench** also introduces a special type of question, the _Which_ questions (Fig. 7(a)), to cover this common type of human query on comparison. In total 10 human experts participate in annotating the **MICBench**, and the answer of each MCQ is cross-examined by another expert. Similar as existing benchmarks [29, 49], **MICBench** is further divided into a _dev_ set (1,004) for method development (_answers will be public_), and a _test_ set (996) to evaluate performance of LMMs (_answers will be hidden from public_).\n' +
      '\n' +
      'Figure 7: **Dataset Card of MICBench**, made up of (a) _Which_ questions (60%), (b) _Yes-or-No_ questions (22%), and (c) _Other_ types of questions (18%) on three/four images.\n' +
      '\n' +
      '## 6 Evaluation\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'The **Co-Instr** is fine-tuned after the released checkpoint of mPLUG-Owl2 [57], with LLaMA-2 [42] as LLM and CLIP-ViT-L14 [36] as visual embedding module. Images are padded to square and then resized to \\(448\\times 448\\) before fed into the model. The learning rate is set as \\(2e-5\\), with two epochs under batch size 192. The final checkpoint is used for evaluation. To avoid over-fitting, only _dev_ subsets of evaluation datasets are used to choose best training hyper-parameters, where the final reported results are from non-overlapped _test_ subsets. All parameters are updated during training, costing in total 25 hours on 8*NVIDIA A100 GPUs.\n' +
      '\n' +
      '### Baseline Models\n' +
      '\n' +
      'We choose five open-source recent state-of-the-art LMMs that supports multi-image inputs to compare with: LLaVA-v1.5-13B [25], InternLM-XComposer2 [5], BakLLaVA [39], EMU2-Chat [40], mPLUG-Owl2 [57] (_baseline of_ **Co-Instr**). Additionally, we also compare with three well-recognized proprietary close-source models: Qwen-VL-Max, Gemini-Pro, and GPT-4V (_teacher of_ **Co-Instr**).\n' +
      '\n' +
      '### Results on Existing Evaluation Settings\n' +
      '\n' +
      'Despite the **MICBench** (Sec. 5), we also evaluate the proposed **Co-Instr** against baseline models on several existing visual quality evaluation/comparison benchmarks for LMMs. The evaluation settings and results are as follows.\n' +
      '\n' +
      '**Q-Bench\\({}^{\\tt PAIR}\\)-A1**[68] is a benchmark for visual quality comparison with 1,999 expert-crafted _open-range_ quality-related MCQs on _image pairs_. In Tab. 2, we compare **Co-Instr** against existing open-source and proprietary models on this benchmark. **Co-Instr** shows far superior accuracy than open-source LMMs: it is **64%** better than its baseline (mPLUG-Owl2), **51%** better than the variant without our multi-image subsets (**Merge2Compare** and **Teach2Compare**), and also 23% better than the best of them. It also outperforms Qwen-VL-Max\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c c|c c} \\hline \\multicolumn{1}{l|}{**Sub-categories**} & \\multicolumn{3}{c|}{**Question Types**} & \\multicolumn{3}{c|}{**Low-level Concern**} & \\multicolumn{3}{c|}{**Pairing Setting**} & \\multicolumn{1}{c}{} \\\\ \\multicolumn{1}{l|}{**Model**} & \\multicolumn{1}{c}{**Yoc-over-Net**} & \\multicolumn{1}{c}{**Whaf**} & \\multicolumn{1}{c|}{**Houf**} & \\multicolumn{1}{c|}{**Dideronf**} & \\multicolumn{1}{c|}{**Other**} & \\multicolumn{1}{c|}{**Compens**} & \\multicolumn{1}{c|}{**Joint**} & \\multicolumn{1}{c}{**Overall**} \\\\ \\hline random sparse accuracy & 50.00\\% & 32.03\\% & 33.16\\% & 38.99\\% & 41.95\\% & 38.69\\% & 43.70\\% & 39.82\\% \\\\ (Seg/2023) LLAVA+15-13B & 57.24\\% & 47.50\\% & 44.31\\% & 49.01\\% & 59.51\\% & 52.00\\% & 52.60\\% \\\\ (Det/2023) BakLLAVA & 60.00\\% & 44.42\\% & 50.86\\% & 53.00\\% & 58.82\\% & 54.82\\% & 56.55\\% & 52.75\\% \\\\ (Net/2023) mPLUG-Owl2 (_baseline of_ **Co-Int****)** & 58.07\\% & 36.61\\% & 48.44\\% & 47.27\\% & 51.90\\% & 45.73\\% & 60.00\\% & 48.94\\% \\\\ (Det/2023) Erm-Chat & 51.94\\% & 29.28\\% & 58.54\\% & 42.01\\% & 58.71\\% & 46.28\\% & 40.00\\% & 47.08\\% \\\\ (Det/2023) HendLM-XComposer2-VL & 74.13\\% & 58.64\\% & 62.28\\% & 67.73\\% & 64.34\\% & 48.00\\% & 65.19\\% \\\\ (Det/2023) U-VL-Max (_Propitared_) & 67.68\\% & 67.56\\% & 55.53\\% & 60.00\\% & 61.18\\% & 68.65\\% & 61.29\\% & 69.37\\% \\\\ Gemini-Pro (_Propitared_) & 66.78\\% & 56.61\\% & 56.74\\% & 60.42\\% & 60.55\\% & 60.04\\% & 60.44\\% & 60.46\\% \\\\ \\hline _Non-segert Human_ & **79.75\\%** & **69.49\\%** & **84.42\\%** & **77.32\\%** & **79.93\\%** & **81.00\\%** & **68.00\\%** & **78.07\\%** \\\\ \\hline \\multicolumn{1}{l|}{without _Multi-image Competitive Data_} & **78.11\\%** & 77.06\\% & 82.33\\% & 78.17\\% & 77.22\\% & 80.20\\% & 76.30\\% & 80.12\\% \\\\ \\hline \\multicolumn{1}{l|}{**Co-Int****)** (Ours)} & **78.05\\%** & **77.20\\%** & **79.23\\%** & **80.00\\%** & **80.62\\%** & **81.91\\%** & **74.22\\%** & **80.18\\%** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Results on _Q-Bench\\({}^{\\tt PAIR}\\)-A1_. **Co-Instr** is remarkably remarkably **51%** better than the variant _without comparative data_, and the only LMM that surpasses human capability.\n' +
      '\n' +
      'and Gemini-Pro by a large margin (21%/33%). Additionally, though its all MCQ training data are from GPT-4V, the student (**Co-Instruct**) still outperforms its teacher on this MCQ evaluation set by notable **2.7%**, suggesting the effectiveness of the collaborative teaching strategy. Our model is also _the only LMM_ that surpasses the accuracy of a non-expert human (_esp._ on Compare subset) in this benchmark, strongly supporting the meaningful vision of using models to relieve human labors on real-world visual quality comparisons in the future.\n' +
      '\n' +
      '**Q-Bench\\({}^{\\text{PAIR}}\\)-A2** is a benchmark setting for general and detailed visual quality comparison _with detailed reasonings_ on _image pairs_. Consisting of 499 image pairs, it employs GPT to evaluate LMM responses against the _golden_ expert-labeled comparisons on **Completeness**, **Precision**, and **Relevance**. As listed in Tab. 3, the **Co-Instruct** can remarkably improve the **Completeness** (+57%) and **Precision** (+59%) of the comparative outputs than the _w/o comparative data_ version, but still falls a little bit behind GPT-4V on the quantitative metrics. This might be because outputs from GPT-4V are more than _twice as long_ as **Co-Instruct** outputs, while GPT evaluation is observed [22] to be in favor of _longer text outputs_. To validate where this potential bias occurs here, we qualitatively visualize the result of different LMMs in Fig. 8. As shown in the figure, the\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c c|c c c|c} \\hline\n' +
      '**Dimensions** & \\multicolumn{3}{c|}{**Completeness**} & \\multicolumn{3}{c|}{**Precision**} & \\multicolumn{3}{c|}{**Relevance**} & \\multirow{2}{*}{\\(S_{\\text{un}}\\)} \\\\\n' +
      '**Model** & \\(F_{\\text{s}}\\) & \\(F_{\\text{s}}\\) & \\(F_{\\text{s}}\\) & & & \\(F_{\\text{s}}\\) & \\(F_{\\text{s}}\\) & \\(F_{\\text{s}}\\) & \\(F_{\\text{s}}\\) & \\(F_{\\text{s}}\\) \\\\ \\hline (**exp/2022) LLAVA+1-5-3IB & 18:77\\% & 73.44\\% & 7.99\\% & 0.80 & 34.00\\% & 38.72\\% & 24.62\\% & 0.20 & 1.02\\% & 34.39\\% & 64.39\\% & 1.63 & 3.44 \\\\ (**fc1022**) BNLLAVA & 29.46\\% & 50.77\\% & 10.57\\% & 0.80 & 40.85\\% & 38.06\\% & 21.33\\% & 0.80 & 2.26\\% & 15.04\\% & 58.20\\% & 1.79 & 3.40 \\\\ (**fc1022**) BNLLAVA & 29.46\\% & 50.77\\% & 10.57\\% & 0.80 & 40.85\\% & 30.86\\% & 21.33\\% & 0.80 & 2.26\\% & 15.04\\% & 58.20\\% & 1.79 & 3.40 \\\\ (**fc1022**) BNLLAVA & 29.46\\% & 65.54\\% & 14.45\\% & 0.94 & 30.94\\% & 31.71\\% & 24.64\\% & 0.82 & 3.70\\% & 20.94\\% & 64.88\\% & 1.63 & 3.50 \\\\ (**fc1022**) BNLLAVA & 41.25\\% & 54.33\\% & 4.42\\% & 0.63 & 13.18\\% & 31.84\\% & 24.84\\% & 0.82 & 4.12\\% & 38.61\\% & 57.27\\% & 1.85 & 3.60 \\\\ (**fc1022**) BNLLAVA & 13.07\\% & 72.17\\% & 14.13\\% & 1.00 & 31.26\\% & 24.13\\% & 23.75\\% & 0.93 & 1.60\\% & 24.17\\% & 22.98\\% & 1.70 & 3.64 \\\\ (**fc1022**) BNLLAVA (_Propitency_) & 11.64\\% & 54.08\\% & 54.08\\% & 1.22 & 24.26\\% & 20.15\\% & 25.62\\% & 1.11 & 2.23\\% & 10.95\\% & 58.56\\% & 1.52 & 4.16 \\\\ Gemini-Pro (_Propitency_) & 18.28\\% & 44.48\\% & 88.64\\% & 1.18 & 13.37\\% & 23.96\\% & 20.02\\% & 0.65\\% & 5.91\\% & 92.22\\% & 1.90 & 4.60 \\\\ GPT-4V (_Propitency_, _twicealer of **Ours**) & 4.00\\% & 31.82\\% & 64.09\\% & **1.00** & 40.45\\% & 41.25\\% & 44.44\\% & **1.34** & 0.18\\% & 1.00\\% & 69.35\\% & **1.94** & **4.89** \\\\ \\hline _w/o Multi-Image Computer Data_ & 15.25\\% & 63.76\\% & 18.32\\% & 1.02 & 30.44\\% & 40.18\\% & 19.62\\% & 0.79 & 0.06\\% & 9.86\\% & 80.02\\% & 1.87 & 3.60 \\\\ _w/o Multi-Image Computer Data_ & 15.25\\% & 63.76\\% & 18.32\\% & 1.02 & 30.44\\% & 40.18\\% & 19.62\\% & 0.79 & 0.06\\% & 9.86\\% & 80.02\\% & 1.87 & 3.60 \\\\ _**Co-Instrect** (Ours) & 4.04\\% & 51.55\\% & 63.55\\% & **1.58** & 15.68\\% & 43.60\\% & 41.37\\% & **1.26** & 0.05 & 0.44\\% & 58.22\\% & **1.56** & **4.82** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Results on _Q-Bench\\({}^{\\text{PAIR}}\\)-A2_. \\(P_{i}\\) denotes frequency for score \\(i\\) (score in \\([0,2]\\)). While slightly inferior to GPT-4V, **Co-Instr** has significantly improved over both the variant _without comparative data_ (+31%), especially for **Precision** metric (+59%)**.\n' +
      '\n' +
      'Figure 8: **Qualitative Visualization on _Q-Bench\\({}^{\\text{PAIR}}\\)-A2_. GPT-4V gives longest outputs and achieves highest precision score even if it makes incorrect comparison here.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.: VQA: Visual Question Answering. In: IEEE ICCV. pp. 2425-2433 (2015)\n' +
      '* [2] Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: ShareGPT4V: Improving large multi-modal models with better captions. CoRR **abs/2311.12793** (2023)\n' +
      '* [3] Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollar, P., Zitnick, C.L.: Microsoft COCO captions: Data collection and evaluation server. CoRR **abs/1504.00325** (2015)\n' +
      '* [4] Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: InstructBLIP: Towards general-purpose vision-language models with instruction tuning. CoRR **abs/2305.06500** (2023)\n' +
      '* [5] Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., Zhang, W., Li, Y., Yan, H., Gao, Y., Zhang, X., Li, W., Li, J., Chen, K., He, C., Zhang, X., Qiao, Y., Lin, D., Wang, J.: InternLM-XComposer2: Mastering free-form text-image composition and comprehension in vision-language large model. CoRR **abs/2401.16420** (2024)\n' +
      '* [6] Fang, Y., Zhu, H., Zeng, Y., Ma, K., Wang, Z.: Perceptual quality assessment of smartphone photography. In: IEEE CVPR. pp. 3677-3686 (2020)\n' +
      '* [7] Ghadiyaram, D., Bovik, A.C.: Massive online crowdsourced study of subjective and objective picture quality. IEEE TIP **25**(1), 372-387 (2016)\n' +
      '* [8] Golestaneh, S.A., Dadsetan, S., Kitani, K.M.: No-reference image quality assessment via transformers, relative ranking, and self-consistency. In: IEEE WACV. pp. 3209-3218 (2022)\n' +
      '* [9] Google: Gemini Pro (2023), [https://deepmind.google/technologies/gemini](https://deepmind.google/technologies/gemini)\n' +
      '* [10] Gu, J., Cai, H., Chen, H., Ye, X., Ren, J., Dong, C.: PIPAL: A large-scale image quality assessment dataset for perceptual image restoration. In: ECCV. pp. 633-651 (2020)\n' +
      '* [11] Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., Steinhardt, J.: Measuring massive multitask language understanding. In: ICLR. pp. 1-10 (2021)\n' +
      '* [12] Hosu, V., Lin, H., Sziranyi, T., Saupe, D.: Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE TIP **29**, 4041-4056 (2020)\n' +
      '* [13] Hudson, D.A., Manning, C.D.: GQA: A new dataset for real-world visual reasoning and compositional question answering. In: IEEE CVPR. pp. 6700-6709 (2019)\n' +
      '* [14] ITU-R, B.T.: Methodology for the subjective assessment of the quality of television pictures. [https://www.itu.int/rec/R-REC-BT.500](https://www.itu.int/rec/R-REC-BT.500) (2002)\n' +
      '* [15] Jayaraman, D., Mittal, A., Moorthy, A.K., Bovik, A.C.: Objective quality assessment of multiply distorted images. In: ASILOMAR. pp. 1693-1697 (2012)\n' +
      '* [16] Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.S., de las Casas, D., Hanna, E.B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L.R., Saulnier, L., Lachaux, M.A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T.L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., Sayed, W.E.: Mixrl of experts. CoRR **abs/2401.04088** (2024)\n' +
      '* [17] LAION: LAION GPT-4V dataset. [https://huggingface.co/datasets/laion/gpt4v-dataset](https://huggingface.co/datasets/laion/gpt4v-dataset) (2023)\n' +
      '* [18] Li, C., Zhang, Z., Wu, H., Sun, W., Min, X., Liu, X., Zhai, G., Lin, W.: AGIQA-3K: An open database for ai-generated image quality assessment. CoRR **2306.04717** (2023)* [19] Li, D., Jiang, T., Jiang, M.: Quality assessment of in-the-wild videos. In: ACM MM. pp. 2351-2359 (2019)\n' +
      '* [20] Li, D., Jiang, T., Lin, W., Jiang, M.: Which has better visual quality: The clear blue sky or a blurry animal? IEEE TMM **21**(5), 1221-1234 (2019)\n' +
      '* [21] Li, J., Mantiuk, R., Wang, J., Ling, S., Le Callet, P.: Hybrid-MST: A hybrid active sampling strategy for pairwise preference aggregation. In: NeurIPS. pp. 1-11 (2018)\n' +
      '* [22] Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., Hashimoto, T.B.: AlpacaEval: An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval) (2023)\n' +
      '* [23] Li, Y., Wang, S., Zhang, X., Wang, S., Ma, S., Wang, Y.: Quality assessment of end-to-end learned image compression: The benchmark and objective measure. In: ACM MM. pp. 4297-4305 (2021)\n' +
      '* [24] Lin, H., Hosu, V., Saupe, D.: KADID-10k: A large-scale artificially distorted iqa database. In: QoMEX. pp. 1-3 (2019)\n' +
      '* [25] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning. CoRR **abs/2310.03744** (2023)\n' +
      '* [26] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. CoRR **abs/2304.08485** (2023)\n' +
      '* [27] Liu, X., Van De Weijer, J., Bagdanov, A.D.: RankIQA: Learning from rankings for no-reference image quality assessment. In: IEEE ICCV. pp. 1040-1049 (2017)\n' +
      '* [28] Liu, X., Van De Weijer, J., Bagdanov, A.D.: Exploiting unlabeled data in cnns by self-supervised learning to rank. IEEE TPAMI **41**(8), 1862-1878 (2019)\n' +
      '* [29] Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., Chen, K., Lin, D.: MMBench: Is your multi-modal model an all-around player? CoRR **abs/2307.06281** (2023)\n' +
      '* [30] Ma, K., Liu, W., Liu, T., Wang, Z., Tao, D.: dipIQ: Blind image quality assessment by learning-to-rank discriminable image pairs. IEEE TIP **26**(8), 3951-3964 (2017)\n' +
      '* [31] Mantiuk, R.K., Tomaszewska, A., Mantiuk, R.: Comparison of four subjective methods for image quality assessment. In: Computer Graphics Forum. vol. 31, pp. 2478-2491 (2012)\n' +
      '* [32] Michaelis, C., Mitzkus, B., Geirhos, R., Rusak, E., Bringmann, O., Ecker, A.S., Bethge, M., Brendel, W.: Benchmarking robustness in object detection: Autonomous driving when winter is coming. CoRR **abs/1907.07484** (2019)\n' +
      '* [33] Mikhailiuk, A., Wilmot, C., Perez-Ortiz, M., Yue, D., Mantiuk, R.K.: Active sampling for pairwise comparisons via approximate message passing and information gain maximization. In: IEEE ICPR. pp. 2559-2566 (2021)\n' +
      '* [34] OpenAI: Gpt-4 technical report (2023)\n' +
      '* [35] Prashnani, E., Cai, H., Mostofi, Y., Sen, P.: PieAPP: Perceptual image-error assessment through pairwise preference. In: IEEE CVPR. pp. 1808-1817 (2018)\n' +
      '* [36] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML. pp. 8748-8763 (2021)\n' +
      '* [37] Rajkumar, A., Agarwal, S.: When can we rank well from comparisons of o (nlog (n)) non-actively chosen pairs? In: Conference on Learning Theory. pp. 1376-1401 (2016)\n' +
      '* [38] Schwenk, D., Khandelwal, A., Clark, C., Marino, K., Mottaghi, R.: A-OKVQA: A benchmark for visual question answering using world knowledge. In: ECCV. pp. 146-162 (2022)* [39] SkunkworksAI: BakLLaVA (2024), [https://github.com/SkunkworksAI/BakLLaVA](https://github.com/SkunkworksAI/BakLLaVA) 3, 8, 10\n' +
      '* [40] Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Luo, Z., Wang, Y., Rao, Y., Liu, J., Huang, T., et al.: Generative multimodal models are in-context learners. CoRR **abs/2312.13286** (2023)\n' +
      '* [41] Thomee, B., Shamma, D.A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., Li, L.J.: YFCC100M: The new data in multimedia research. Commun. ACM **59**(2), 64-73 (2016)\n' +
      '* [42] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.S., Lachaux, M.A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.M., Subramanian, R., Tan, X.E., Tang, B., Taylor, R., Williams, A., Kuan, J.X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., Scialom, T.: Llama 2: Open foundation and fine-tuned chat models. CoRR **abs/2307.09288** (2023)\n' +
      '* [43] Tsukida, K., Gupta, M.R.: How to analyze paired comparison data (Technical Report UWEETR-2011-0004, University of Washington, 2011), [https://api.semanticscholar.org/CorpusID:15425240](https://api.semanticscholar.org/CorpusID:15425240)\n' +
      '* [44] Wang, L., Yang, N., Huang, X., Yang, L., Majumder, R., Wei, F.: Improving text embeddings with large language models. CoRR **abs/2401.00368** (2024)\n' +
      '* [45] Wauthier, F., Jordan, M., Jojic, N.: Efficient ranking from pairwise comparisons. In: ICML. pp. 109-117 (2013)\n' +
      '* [46] Wu, H., Chen, C., Hou, J., Liao, L., Wang, A., Sun, W., Yan, Q., Lin, W.: FAST-VQA: Efficient end-to-end video quality assessment with fragment sampling. In: ECCV. pp. 538-554 (2022)\n' +
      '* [47] Wu, H., Chen, C., Liao, L., Hou, J., Sun, W., Yan, Q., Gu, J., Lin, W.: Neighbourhood representative sampling for efficient end-to-end video quality assessment. IEEE TPAMI (2023)\n' +
      '* [48] Wu, H., Zhang, E., Liao, L., Chen, C., Hou, J., Wang, A., Sun, W., Yan, Q., Lin, W.: Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In: IEEE ICCV (2023)\n' +
      '* [49] Wu, H., Zhang, Z., Zhang, E., Chen, C., Liao, L., Wang, A., Li, C., Sun, W., Yan, Q., Zhai, G., Lin, W.: Q-bench: A benchmark for general-purpose foundation models on low-level vision. In: ICLR. pp. 1-13 (2023)\n' +
      '* [50] Wu, H., Zhang, Z., Zhang, E., Chen, C., Liao, L., Wang, A., Li, C., Sun, W., Yan, Q., Zhai, G., Lin, W.: Q-Bench: A benchmark for general-purpose foundation models on low-level vision. In: ICLR (2024)\n' +
      '* [51] Wu, H., Zhang, Z., Zhang, E., Chen, C., Liao, L., Wang, A., Xu, K., Li, C., Hou, J., Zhai, G., et al.: Q-instruct: Improving low-level visual abilities for multi-modality foundation models. CoRR **abs/2311.06783** (2023)\n' +
      '* [52] Wu, H., Zhang, Z., Zhang, W., Chen, C., Liao, L., Li, C., Gao, Y., Wang, A., Zhang, E., Sun, W., et al.: Q-Align: Teaching lmms for visual scoring via discrete text-defined levels. CoRR **abs/2312.17090** (2023)\n' +
      '* [53] Wu, X., Sun, K., Zhu, F., Zhao, R., Li, H.: Better aligning text-to-image models with human preference. CoRR **abs/2303.14420** (2023)\n' +
      '* [* [54] Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., Dong, Y.: ImageReward: Learning and evaluating human preferences for text-to-image generation. CoRR **abs/2304.05977** (2023)\n' +
      '* [55] Ye, P., Doermann, D.: Active sampling for subjective image quality assessment. In: IEEE CVPR. pp. 4249-4256 (2014)\n' +
      '* [56] Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., Jiang, C., Li, C., Xu, Y., Chen, H., Tian, J., Qi, Q., Zhang, J., Huang, F.: mPLUG-Owl: Modularization empowers large language models with multimodality. CoRR **abs/2304.14178** (2023)\n' +
      '* [57] Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou, J.: mPLUG-Owl2: Revolutionizing multi-modal large language model with modality collaboration. CoRR **abs/2311.04257** (2023)\n' +
      '* [58] Yim, J.G., Wang, Y., Birkbeck, N., Adsumilli, B.: Subjective quality assessment for youtube UGC dataset. In: IEEE ICIP. pp. 1-5 (2020)\n' +
      '* [59] Yin, Z., Wang, J., Cao, J., Shi, Z., Liu, D., Li, M., Sheng, L., Bai, L., Huang, X., Wang, Z., et al.: LAMM: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. CoRR **abs/2306.06687** (2023)\n' +
      '* [60] Ying, Z., Niu, H., Gupta, P., Mahajan, D., Ghadiyaram, D., Bovik, A.: From patches to pictures (PaQ-2-PiQ): Mapping the perceptual space of picture quality. In: IEEE CVPR. pp. 3575-3585 (2020)\n' +
      '* [61] Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al.: MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. CoRR **abs/2311.16502** (2023)\n' +
      '* [62] Zhang, C., Su, S., Zhu, Y., Yan, Q., Sun, J., Zhang, Y.: Exploring and evaluating image restoration potential in dynamic scenes. In: IEEE CVPR. pp. 2057-2066 (2022)\n' +
      '* [63] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: IEEE CVPR. pp. 586-595 (2018)\n' +
      '* [64] Zhang, W., Ma, K., Zhai, G., Yang, X.: Uncertainty-aware blind image quality assessment in the laboratory and wild. IEEE TIP **30**, 3474-3486 (2021)\n' +
      '* [65] Zhang, W., Zhai, G., Wei, Y., Yang, X., Ma, K.: Blind image quality assessment via vision-language correspondence: A multitask learning perspective. In: IEEE CVPR. pp. 14071-14081 (2023)\n' +
      '* [66] Zhang, W., Liu, Y., Dong, C., Qiao, Y.: RankSRGan: Generative adversarial networks with ranker for image super-resolution. In: ICCV. pp. 3096-3105 (2019)\n' +
      '* [67] Zhang, Z., Sun, W., Wang, T., Lu, W., Zhou, Q., Wang, Q., Min, X., Zhai, G., et al.: Subjective and objective quality assessment for in-the-wild computer graphics images. ACM TOMM **20**(4), 1-22 (2023)\n' +
      '* [68] Zhang, Z., Wu, H., Zhang, E., Zhai, G., Lin, W.: A benchmark for multi-modal foundation models on low-level vision: from single images to pairs. CoRR **abs/2402.07116** (2024)\n' +
      '* [69] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: MiniGPT-4: Enhancing vision-language understanding with advanced large language models. CoRR **abs/2304.10592** (2023)\n' +
      '* [70] Zhu, H., Sui, X., Chen, B., Liu, X., Chen, P., Fang, Y., Wang, S.: 2AFC prompting of large multimodal models for image quality assessment. CoRR **abs/2402.01162** (2024)\n' +
      '* [\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>