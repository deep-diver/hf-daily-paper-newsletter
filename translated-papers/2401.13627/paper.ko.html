<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 스칼링 업 우수.\n' +
      '\n' +
      '포토-현실적 이미지 복구를 위한 모델 추출\n' +
      '\n' +
      '진진유\\({}^{4}}\\), 진피오콩\\({}^{4,3,*}\\), 진파오콩\\({}^{4,*}\\), 진파오콩\\({}^{4,*}\\), 진피오콩\\({}^{1,{.\n' +
      '\n' +
      'Xintao Wang\\({}^{5}\\), 진원헤\\({}^{2,6}\\), 유시아오\\({}^{2}\\), 차오동\\({}^{1,2,\\)\n' +
      '\n' +
      '하이하이 AI 연구소({}^{1}\\)와 중국 과학원({}^{2}}\\)의 민주기술원.\n' +
      '\n' +
      '홍콩 폴리테크닉대 \\({}^{5}\\) 홍콩대학, 텐센트 PCG\\({}^{3}\\,{}^{5}\\) 중국 시드니대학({}^{4}\\)\n' +
      '\n' +
      '프로젝트 페이지: [https://supir.x픽셀](https://supirx픽셀) (https://supir.x픽셀)].\n' +
      '\n' +
      '합법적으로 형평으로 내용한다. \\. \\. \\. ({}^{\\dagger}\\) 해당 권한.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '_우리는 생성 이전의 발전적 이미지 복원 방식과 모델 스케일링의 힘을 이용하는 획기적인 이미지 복원 방법인 SUPIR(Scaling-UP 이미지 복원)을 소개한다. 다중 모달 기술과 첨단 생성 이전에 SUPIR은 지능적이고 현실적인 이미지 복원에서 상당한 발전을 나타낸다. SUPIR 내의 중추 촉매로서 모델 스케일링은 능력을 극적으로 향상시키고 이미지 복원의 새로운 잠재력을 보여준다. 우리는 모델 학습을 위한 2천만 개의 고해상도 고품질 이미지를 포함하는 데이터 세트를 수집하며, 각각 기술 텍스트 주석이 풍부하다. SUPIR은 텍스트 프롬프트에 의해 유도되는 이미지를 복원하여 적용 범위와 잠재력을 넓히는 능력을 제공한다. 또한, 지각 품질을 더욱 향상시키기 위해 부정적인 품질의 프롬프트를 도입합니다. 또한 생성 기반 복구에서 발생하는 충실도 문제를 억제하기 위해 복원 유도 샘플링 방법을 개발한다. 실험은 SUPIR의 예외적인 복원 효과와 텍스트 프롬프트를 통해 복구를 조작하는 새로운 능력을 보여준다.___ 실험은 SUPIR의 예외적인 복원 효과를 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '이미지 복원(IR)의 발달로 IR 결과의 지각 효과 및 지능에 대한 기대가 크게 증가하였다. 생성 사제[42, 49, 67, 82]를 기반으로 한IR 방법은 고품질 생성과 사전 지식을 IR에 도입하기 위해 강력한 사전 훈련 생성 모델을 레버리지하여 이러한 측면에서 상당한 진전을 가져왔다. 지속적으로 생성 이전의 능력을 향상시키는 것은 모델 스케일링이 중요하고 효과적인 접근법으로 보다 지능화된 IR 결과를 달성하는 데 중요하다. SAM[44]과 대형 언어 모델[7, 73, 74] 등 스케일링에서 놀라운 개선을 얻은 과제가 많다. 이는 초고품질 이미지를 생산할 수 있는 대규모 지능형 IR 모델을 구축하기 위한 우리의 노력에 더욱 동기를 부여한다. 그러나 컴퓨팅 자원, 모델 아키텍처, 학습 데이터, 생성 모델 및 IR의 협력과 같은 엔지니어링 제약으로 인해 IR 모델을 스케일링하는 것은 어렵다.\n' +
      '\n' +
      '본 연구에서는 시각효과와 지능에서 더 큰 잠재력을 탐색하는 것을 목표로 가장 큰 IR 방식인 SUPIR(Scaling-UP IR)을 소개한다. 구체적으로, SUPIR은 60억 개의 매개변수를 포함하는 강력한 생성 이전에 StableDiffusion-XL(SDXL) [63]을 사용한다. 이 모델을 효과적으로 적용하기 위해 6억 개 이상의 매개변수를 가진 어댑터를 설계하고 훈련합니다. 또한, 모델 스케일링으로 제공되는 잠재력을 완전히 실현하기 위해 2천만 개 이상의 고품질 고해상도 이미지를 수집했습니다. 각 이미지는 상세한 서술형 텍스트를 동반하여 텍스트 프롬프트를 통한 복원 제어를 가능하게 한다. 또한 13억 매개변수 다중 모달 언어 모델을 사용하여 이미지 콘텐츠 프롬프트를 제공하여 방법의 정확성과 지능을 크게 개선합니다. 제안된 SUPIR 모델은 특히 복잡하고 도전적인 실제 시나리오에서 최고의 시각적 품질을 달성하는 다양한 IR 작업에서 탁월한 성능을 보여준다. 또한 모델은 텍스트 프롬프트를 통해 복원 과정에 대한 유연한 제어를 제공하여 IR의 가능성을 크게 넓힌다. 그림. 1은 우리의 모델에 의한 효과를 보여주고 우수한 성능을 보여줍니다.\n' +
      '\n' +
      '우리의 작업은 단순히 스케일링을 훨씬 넘어갑니다. 모델 규모의 증가를 추구하면서 우리는 일련의 복잡한 도전에 직면한다. 먼저 IR에 SDXL을 적용할 때 기존의 Adaptor는 IR[59]의 복잡한 요구 사항을 충족하기에 너무 간단하거나 SDXL[95]와 함께 훈련하기에는 너무 크다. 이 문제를 해결하기 위해 컨트롤넷을 다듬고 제로SFT라는 새로운 커넥터를 설계하여 미리 훈련된 SDXL과 협력하여 컴퓨팅 비용을 절감하면서 IR 태스크를 효율적으로 구현하기 위해 노력했다. 낮은 품질의 이미지의 내용을 정확하게 해석할 수 있는 모델의 능력을 향상시키기 위해 이미지 열화의 변화에 대한 견고성을 향상시키기 위해 이미지 인코더를 미세 조정했다. 이러한 조치는 모델을 실행 가능하고 효과적으로 스케일링하고 안정성을 크게 향상시킵니다. 둘째, 기술 텍스트 주석을 가진 2천만 개의 고품질 고해상도 이미지를 수집하여 모델의 훈련에 대한 견고한 기초를 제공합니다. 우리는 좋지 않은 품질, 부정적인 샘플을 훈련에 통합함으로써 반직관적인 전략을 채택한다. 이러한 방식으로, 우리는 시각적 효과를 더욱 개선하기 위해 부정적인 품질 프롬프트를 사용할 수 있다. 우리의 결과는 이 전략이 고품질 양성 샘플만을 사용하는 것에 비해 화질을 크게 향상시킨다는 것을 보여준다. 마지막으로 강력한 생성 전에 두 배의 칼이 있습니다. 통제되지 않은 생성은 복원 충실도를 감소시켜 IR이 더 이상 입력 이미지에 충실하지 않게 할 수 있다. 이러한 저충실성 문제를 완화하기 위해 새로운 복원 유도 샘플링 방법을 제안한다. 효율적인 엔지니어링 구현과 결합된 이러한 모든 전략은 첨단 IR의 경계를 밀어내는 SUPIR의 스케일링을 가능하게 하는 열쇠이다. 모델 아키텍처부터 데이터 수집, 이미지 복원 기술의 최전선에 있는 위치 SUPIR까지 모든 것을 포괄하는 이 포괄적인 접근법은 향후 발전을 위한 새로운 벤치마크를 설정합니다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      'IR의 목표는 분해된 이미지를 고품질 분해 없는 버전[22, 26, 89, 91, 98, 99]으로 변환하는 것이다. 초기 연구자들은 초해상도(SR)[13, 19, 20], 데모화[11, 90, 92], 디클로링[14, 60, 72] 등 다양한 형태의 이미지 열화를 독립적으로 탐색하였다. 그러나 이러한 방법은 종종 특정 분해 가정[25, 50, 58]을 기반으로 하므로 다른 분해 [29, 53, 97]에 대한 일반화 능력이 부족하다. 시간이 지남에 따라 특정 분해 가정을 기반으로 하지 않는 블라인드 복원 방법의 필요성은 [5, 10, 34, 35, 46, 47, 48, 94] 증가했다. 이러한 추세에서 일부 방법[81, 93]은 보다 복잡한 분해 모델에 의해 실제 분해를 근사적으로 합성하며 단일 모델로 다중 분해를 처리하는 것으로 잘 알려져 있다. DiffBIR[49]와 같은 최근 연구는 서로 다른 복원 문제를 단일 모델로 통일한다. 본 논문에서는 DiffBIR과 유사한 설정을 채택하여 단일 모델을 사용하여 다양한 심각한 분해의 효과적인 처리를 달성했다.\n' +
      '\n' +
      '생성 우선 생성 사제들은 이미지의 고유한 구조를 캡처하는 데 능숙하여 자연 이미지 분포를 따르는 이미지의 생성을 가능하게 한다. GAN[23, 39, 40, 64]의 출현은 IR에서 생성 사제들의 중요성을 강조한다. GAN 반전 [2, 4, 27, 57, 57, GAN 반전[2, 4, 27, 57, 5]를 포함하여 다양한 접근법이 이러한 사제들을 고용한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      '공학적 적절성 구현이 가능합니다. SDXL의 인코더 모듈 내의 각 블록은 주로 여러 비전 트랜스포머(ViT) [21] 블록으로 구성된다. 제어망의 효과에 기여하는 두 가지 주요 요인인 대형 네트워크 용량과 훈련 가능한 복사본의 효율적인 초기화를 확인했다. 특히, 훈련 가능한 복사본에서 블록의 부분 트리밍조차도 어댑터에서 이러한 중요한 특성을 유지한다. 따라서 우리는 그림과 같이 각 인코더 블록에서 ViT 블록의 절반을 간단하게 트리밍한다. 3(b) 둘째, 어댑터를 SDXL에 연결하는 커넥터를 재설계합니다. SDXL의 생성 능력은 우수한 시각적 효과를 전달하지만 픽셀 수준의 정확한 제어를 어렵게 만든다. 제어넷은 생성 지침을 위해 제로 콘볼루션을 사용하지만, IR에 의해 요구되는 제어에만 의존하는 것은 불충분하다. LQ 지도의 영향을 증폭하기 위해 그림과 같이 제로SFT 모듈을 도입했다. 3(c). 제로 컨볼루션 기반의 구축 제로SFT는 추가 공간 특징 전달(SFT) [79] 연산 및 그룹 정규화 [84]를 포함한다.\n' +
      '\n' +
      '업트레이닝 데이터 계산.\n' +
      '\n' +
      '이미지 수집 모델 스케일링은 트레이닝 데이터(38])의 대응하는 스케일링을 필요로 한다. 그러나 IR에 사용할 수 있는 대규모 고품질 이미지 데이터 세트는 아직 없습니다. DIV2K[3]와 LSDIR[1]은 높은 화질을 제공하지만 수량이 제한되어 있다. 이미지넷(IN)[17], LAION-5B[69], SA-1B[44]과 같은 더 큰 데이터 세트는 더 많은 이미지를 포함하지만, 그들의 이미지 품질은 우리의 높은 기준에 부합하지 않는다. 이를 위해 2천만 1024\\(기업\\)1024의 고품질, 질감이 풍부하고, 콘텐츠가 선명한 이미지를 포함하는 고해상도 이미지의 새로운 대규모 데이터세트를 수집한다. 수집된 데이터셋과 기존 데이터셋의 스케일에 대한 비교는 그림 3에 나와 있으며, 또한 모델의 얼굴 복원 성능을 향상시키기 위해 FFHQ-raw 데이터세트[40]의 추가 70K 비설계형 고해상도 안면 이미지를 포함했다. 그림에서. 5(a)는 잘 알려진 다른 데이터 세트와 비교하여 데이터의 상대적 크기를 보여준다.\n' +
      '\n' +
      '다중 수정 언어 지침 Diffusion 모델은 텍스트 프롬프트에 기초하여 이미지를 생성하는 능력으로 유명합니다. 텍스트 프롬프트는 또한 다음과 같은 이유로 IR을 크게 지원할 수 있다고 믿습니다. 기존의 프레임워크는 종종 이러한 이해를 간과하거나 암묵적으로 처리하며 [24, 29]한다. 텍스트 프롬프트를 통합함으로써 LQ 이미지에 대한 이해도를 IR 모델에 명시적으로 전달하여 누락된 정보의 표적 복원을 용이하게 한다. (2) 심각한 분해의 경우 최고의 IR 모델조차도 완전히 손실된 정보를 복구하는 데 어려움을 겪을 수 있다. 이러한 경우 텍스트 프롬프트는 제어 메커니즘으로 작용할 수 있어 사용자 선호도에 따른 누락된 정보의 표적 완성이 가능하다. (3) 또한 텍스트를 통해 원하는 화질을 기술하여 출력의 지각 품질을 더욱 높일 수 있다. 그림을 참조하세요. 일부 예들에 대한 1(b) 이를 위해 우리는 두 가지 주요 수정을 합니다. LLaVA는 입력으로서 분해 처리된 LQ 이미지 \\(x^{\\prime}_{LQ}(x_{\\mathrm{D})를 입력하고 이미지 내에서 콘텐츠를 명시적으로 이해하고, 먼저 LLaVA 다중 모델 [52]를 파이프라인에 통합하기 위해 전체 프레임워크를 수정한다.\n' +
      '\n' +
      '그림 4: CFG는 음성 훈련 샘플 없이 유물을 도입하여 시각적 품질 개선을 방해한다. 음성 샘플을 추가하면 CFG를 통해 추가 품질 개선이 가능합니다.\n' +
      '\n' +
      '그림 3은 사용된 SDXL 및 제안된 어댑터의 전체 아키텍처, (b) 효율화를 위한 ViT 블록이 감소된 SDXL 인코더의 트리밍 가능한 복사본, (c) IR에서 강화된 제어를 위한 새로운 제로SFT 커넥터를 보여주고, 여기서 \\(X_{f}\\) 및 \\(X_{s}\\)는 각각 앰코더 및 엔코더 바로가기(X_{c}\\)는 어댑터로부터의 입력 특징 맵을 나타내는 IR에서 강화된 제어를 위한 새로운 제로SFT 커넥터를 보여준다. 이 모델은 대규모 SDXL을 생성 이전에 효과적으로 사용하도록 설계되었습니다.\n' +
      '\n' +
      '텍스트 설명의 형태로 입력하세요. 그런 다음 이러한 설명은 복원을 안내하는 프롬프트로 사용된다. 이 과정은 테스트 중에 자동화되어 수동 개입의 필요성을 제거할 수 있다. 둘째, PixART[12]의 접근에 따라 모든 훈련 이미지에 대한 텍스트 주석을 수집하여 아웃 모델 훈련 동안 텍스트 제어의 역할을 강화한다. 이 두 가지 변화는 이미지 콘텐츠를 이해하고 텍스트 프롬프트에 기초하여 이미지를 복원하는 능력으로 SUPIR을 변경한다.\n' +
      '\n' +
      '음성 품질 샘플 및 프롭트 분류기 없는 안내(CFG) [30]은 모델에 대해 원치 않는 콘텐츠를 특정하는 음성 프롬프트를 사용하여 또 다른 제어 방법을 제공한다. 우리는 이 기능을 사용하여 모델 NOT를 특정하여 낮은 품질의 이미지를 생성할 수 있다. 구체적으로 확산의 각 단계에서 양성 프롬프트 \\(\\mathrm{pos}\\)와 음의 프롬프트 \\(\\mathrm{neg}\\)를 사용하여 두 가지 예측을 하고, 이 두 결과를 최종 출력 \\(z_{t-1}\\)으로 융합한다.\n' +
      '\n' +
      '}(z_{t})\\mathold{H}(z_{mathrm{t},\\mathrm{t})\\mathrm{H}(z_{mathrm{t})\\mathrm{H}(z_{t},\\mathrm{t},\\mathrm{t}.\n' +
      '\n' +
      'H\\(\\mathcal{H}(\\cdot)\\가 어댑터를 사용한 확산 모델인 경우, \\(\\sigma_{t}\\)는 시간 단계 \\(t\\)에서의 노이즈의 분산이며, \\(\\lambda_{\\mathrm{cfg}}\\)는 초매개변수이다. 우리의 틀에서 \\(\\mathrm{pos}\\)는 품질의 긍정적인 단어들로 이미지 설명이 될 수 있으며, \\(\\mathrm{neg}\\)는 품질, _e.g._oil 페인팅, 만화, 블러러, 더러운, 지저분한, 낮은 품질, 변형, 낮은 해상도, 과-smooth_의 부정적인 단어이다. 긍정적인 방향과 부정적인 방향 모두를 예측하는 정확성은 CFG 기술에 중요하다. 그러나, 우리의 학습 데이터에 음성 품질의 샘플과 프롬프트가 없기 때문에 음성 프롬프트를 이해하는 데 미세 조정 SUPIR이 실패할 수 있다. 따라서 샘플링 중 음성 품질의 프롬프트를 사용하면 유물을 도입할 수 있다. 예를 들어, 4는. 이 문제를 해결하기 위해 SDXL을 사용하여 음성 품질의 프롬프트에 대응하는 100K 이미지를 생성했다. 우리는 제안된 SUPIR 모델에 의해 부정적인 품질의 개념이 학습될 수 있도록 훈련 데이터에 이러한 저품질 이미지를 대항적으로 추가한다.\n' +
      '\n' +
      '### Restoration-Guided Sampling\n' +
      '\n' +
      '강력한 생성 전에 너무 많은 생성 용량이 차례로 회복된 이미지의 충실도에 영향을 미치기 때문에 이중 처리된 검이다. 이는 IR 과제와 생성 과제의 근본적인 차이를 강조한다. 우리는 이미지 회복이 LQ 이미지에 충실하도록 생성을 제한하는 수단이 필요하다. 우리는 EDM 샘플링 방법[41]을 수정하고 이 문제를 해결하기 위해 복원 유도 샘플링 방법을 제안했다. 우리는 각 확산 단계에서 예측 결과 \\(z_{t-1}\\)를 LQ 이미지 \\(z_{LQ}\\)에 근접하도록 선택적으로 안내하기를 바란다. 특정 알고리즘은 알고리즘 1에서 나타나는데, 여기서 \\(T\\)는 총 단계 수, \\(\\{\\sigma_{t}\\}_{t=1}^{T}\\)는 \\(T\\) 단계의 소음 분산, \\(T\\) 단계는 추가 텍스트 프롬프트 조건이다. MS(S_{\\mathrm{churn}}\\), \\(S_{\\mathrm{noise}}}), \\(S_{\\mathrm{noise}}}), \\(S_{\\mathrm{min}}}), \\(S_{\\mathrm{min}}})는 5개의 하이퍼 파라미터이지만, \\(S_{\\mathrm{max{max}}}})만이 복원 안내와 관련이 있지만,\\(S_{\\mathrm{mathrm{mathrm{mathrm{mathrm{mathrm{mathrm{mathrm{mathrm{mathrm{max{max{max{max{max{max{max{max}}}}(S_{\\)는 5개의 하이퍼-파라미터(S_{\\mathrm{max{max{max{max}}}}}<S_{mathrm{max{max}}}}})는 원래 EDM 방법[41]과 관련이 있지만,\\_{mathrm{max}}}}} 더 나은 이해를 위해 간단한 다이어그램을 그림 1에 나타내었다. 5(b). 예측된 출력 \\(\\hat{z}_{t-1}\\)와 LQ 잠재 \\(z_{LQ}\\) 사이의 가중 보간(z_{t-1}\\)을 복원 유도 출력 \\(z_{t-1}\\)으로 수행한다. 영상의 저주파 정보는 확산 예측(67] 초기 단계(t\\)와 \\(\\sigma_{t}\\)에서 주로 생성되며, 가중치 \\(k=(\\sigma_{t}/\\sigma_{T}})도 크기 때문에 예측 결과는 \\(z_{LQ}\\)에 가까워서 충실도를 높인다. 확산 예측의 후기 단계에서는 주로 고주파 디테일이 생성된다. 세부적이고 질감이 적절하게 생성될 수 있도록 이 시기에 너무 많은 제약이 없어야 한다. 이때 \\(t\\)와 \\(\\sigma_{t}\\)는 상대적으로 작고, 중량 \\(k\\)도 작다. 따라서 이 방법을 통해 예측된 결과가 크게 영향을 받지 않을 것이며, 우리는 충실도를 보장하기 위해 확산 샘플링 과정에서 생성을 제어할 수 있다.\n' +
      '\n' +
      '```\n' +
      '\\(\\tau_{mathrm{churn}\\),\\(S_{\\mathrm{t}}\\), \\(S_{\\mathrm{max}}), \\(S_{\\mathrm{max}}), \\(S_{\\mathrm{max}}) \\(S_{\\mathrm{max}}\\), \\(S_{\\mathrm{mathrm{max}}\\), \\(S_{\\mathrm{mathrm{max}}}\\) \\(S_{\\) \\(S_{\\mathrm{mathrm{max}}}}}}}}\\) \\) \\(S_{\\) \\(S_{max}}}}}}}}}}}}}}}}}}} <\\) \\) \\:\\(S_{ a\\) \\(S_{max}}}}}}}}}}}}}} <\\) \\) \\:\\:\\:\\\n' +
      '1:sample\\(z_{T}\\sim\\mathcal{N}(\\mathbf{0},\\sigma_{T}^{2}\\mathbf{I})\\)\n' +
      '2:for\\(t\\in\\{T,\\ldots,1\\}\\)do\n' +
      '3:sample\\(\\mathbf{\\epsilon}_{t}\\sim\\mathcal{N}\\left(\\mathbf{0},S_{\\mathrm{noise}}^{2} \\mathbf{I}\\right)\\)\n' +
      '\\(\\gamma_{\\mathrm{churn}}}: 티피무륨_{\\mathrm{min}}.\n' +
      't}.\\(k){t}:\\math{t}.\n' +
      '6:\\(\\hat{z}_{t-1}\\leftarrow\\mathcal{H}\\left(\\hat{z}_{t},z_{LQ},\\hat{\\sigma}_{t},c\\right)\\)\n' +
      '7:\\(d_{t}\\leftarrow(\\hat{z}_{t}-(\\hat{z}_{t-1}+k_{t}(z_{LQ}-\\hat{z}_{t-1})))/\\hat{ \\sigma}_{t}\\)\n' +
      '8:\\(z_{t-1}\\leftarrow\\hat{z}_{t}+(\\sigma_{t-1}-\\hat{\\sigma}_{t})\\,d_{t}\\)\n' +
      '9:endfor\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 회복 유도 샘플링.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '모델 훈련과 샘플링 설정.\n' +
      '\n' +
      '학습의 경우, 전체 학습 데이터는 텍스트 설명이 있는 2천만 개의 고품질 이미지, 70K 얼굴 이미지 및 100K 음성 품질의 샘플 및 대응하는 음성 프롬프트를 포함한다. 더 큰 배치 크기를 가능하게 하기 위해 훈련 중 512\\(\\tot\\)512 패치로 작제한다. 실시간-ESRGAN[81]이 사용한 설정에 따라 합성 분해 모델을 사용하여 모델을 훈련하며, 유일한 차이점은 생성된 LQ 이미지를 교육을 위해 512\\(\\touthern\\)512으로 재구성한다는 것이다. 우리는 \\(0.00001\\)의 학습률로 AdamW 최적화기[54]를 사용한다. 훈련 과정은 열흘에 걸쳐 있으며, 훈련 과정은 10일이다.\n' +
      '\n' +
      '그림 5: (a) 우리는 잘 알려진 다른 데이터 세트와 비교하여 데이터의 상대적 크기를 보여준다. SA-1B[44]와 비교하여 데이터 세트는 더 높은 품질과 더 많은 이미지 다양성을 가지고 있습니다. (b) 복구 유도 샘플링 메커니즘을 보여준다.\n' +
      '\n' +
      '6개의 Nvidia A6000 GPU에서 배치 크기가 256인 전도되어 테스트 시 하이퍼 파라미터는 \\(T\\)=100, \\(\\lambda_{\\mathrm{cfg}}\\)=7.5 및 \\(\\tau_{r}=4\\)이다. 우리의 방법은 1024\\(CSI)1024의 크기로 이미지를 처리할 수 있다. 우리는 입력 이미지의 짧은 측면을 1024로 재구성하고 테스트를 위해 1024\\(CSI)1024 하위 이미지를 작물로 재구성한 다음 복원 후 원래의 크기로 다시 재구성한다. 달리 명시되지 않는 한, 프롬프트는 수동으로 제공되지 않으며, 처리는 완전히 자동화될 것이다.\n' +
      '\n' +
      '폐지\n' +
      '\n' +
      '우리의 방법은 광범위한 분해를 처리할 수 있으며 BSRGAN[93], 리얼-ESRGAN[81], StableSR[77], DiffBIR[49] 및 PASD[88]을 포함한 동일한 능력으로 최첨단 방법과 비교한다. 그들 중 일부는 512\\(종량)512 크기의 이미지를 생성하는 데 제약을 받는다. 우리의 비교에서 우리는 이 요구 사항을 충족하기 위해 테스트 이미지를 작화하여 공정한 비교를 용이하게 하기 위해 결과를 하향 조정한다. 우리는 합성 데이터와 실제 데이터 모두에 대한 비교를 수행한다.\n' +
      '\n' +
      '테스트용 LQ 이미지를 합성하기 위해 합성 데이터는 이전 작업 [45, 97]을 따르고 단일 분해 및 복잡한 혼합물 분해를 포함하여 여러 대표적인 분해에 대한 영향을 보여준다. 구체적인 내용은 Tab. 1에서 확인할 수 있으며, 정량적 비교를 위해 전체 준수 메트릭 PSNR, SSIM, LPIPS [96], 비수지 메트릭 ManIQA[86], ClipIQA[76], MUSIQ [43]를 선택했다. 우리의 방법은 결과의 우수한 이미지 품질을 반영하는 모든 비수지 메트릭에 대한 최상의 결과를 달성한다는 것을 알 수 있다. 동시에 우리는 또한 전체 준수 메트릭에서 우리의 방법의 단점을 주목한다. 우리는 그림 7과 같이 이러한 전면적 측정값의 한계를 강조하는 간단한 실험을 제시하며, 이러한 전면적 측정값의 한계를 강조하는 간단한 실험을 제시한다.\n' +
      '\n' +
      '그림 6: 다양한 방법과의 정성적 비교이다. 우리의 방법은 도전적인 열화에 따라 해당 물체의 질감과 세부 사항을 정확하게 복원할 수 있다. 다른 방법은 깨진 부리와 불규칙한 얼굴과 같은 의미적으로 올바른 세부 사항을 복구하지 못한다.\n' +
      '\n' +
      '그림 7: 이 예들은 메트릭 평가와 인간 평가 사이의 오정렬을 보여준다. SUIPR은 충실도가 높은 질감을 가진 이미지를 생성하지만 더 낮은 메트릭을 얻는다.\n' +
      '\n' +
      '우리의 결과는 더 나은 시각적 효과를 가지고 있지만 이러한 메트릭에서는 이점이 없다. 이러한 현상은 [6, 26, 28]도 많은 연구에서 언급되었다. IR의 품질 향상으로 기존 메트릭의 기준값을 재고할 필요가 있으며, 첨단 IR 방법을 평가하는 보다 효과적인 방법을 제시할 필요가 있다고 주장한다. 우리는 또한 그림 6의 일부 질적 비교 결과를 보여주는데, 심각한 분해 하에도 우리의 방법은 LQ 이미지의 함량을 충실히 나타내는 매우 합리적이고 고품질 이미지를 일관되게 생성한다.\n' +
      '\n' +
      '야생 동물의 복원은 또한 실제 LQ 이미지에 대한 방법을 테스트한다. 우리는 리얼SR[8], 디리얼SR[83], 리얼47[49], 온라인 소스에서 총 60개의 실세계 LQ 영상을 수집하여 동물, 식물, 얼굴, 건물, 풍경 등 다양한 콘텐츠를 선보인다. 우리는 그림의 질적 결과를 보여준다. 10과 정량적 결과는 Tab에 나와 있다. 1. 이러한 결과는 우리 방법에 의해 생성된 이미지가 가장 좋은 지각 품질을 가지고 있음을 나타낸다. 우리는 또한 20명의 참가자와 실제 LQ 이미지에 대한 방법을 비교하는 사용자 연구를 수행한다. 각 비교 이미지 세트에 대해 참가자들에게 이러한 테스트 방법 중 최고 품질의 복원 결과를 선택하도록 지시했다. 결과는 그림 1에 나와 있다. 8, 우리의 접근법이 지각적 품질에서 최첨단 방법을 상당히 능가했음을 보여준다.\n' +
      '\n' +
      '원형 프로빗으로 복원하는 것을 목표로 합니다.\n' +
      '\n' +
      '이미지-텍스트 쌍의 큰 데이터셋을 학습하고 확산 모델의 특징을 레버리징한 후, 우리의 방법은 인간 프롬프트에 기초하여 이미지를 선택적으로 복원할 수 있다. 그림. 1(b)은 몇 가지 예를 보여준다. 첫 번째 경우, 자전거 복구가 신속하지 않고 어려운 상황이지만, 빠른 시일 내에 수신하면 모델이 정확하게 재구성된다. 두 번째 경우, 프롬프트를 통해 모자의 재료 질감을 조절할 수 있습니다. 세 번째 경우, 하이 레벨 시맨틱 프롬프트라도 얼굴 속성에 대한 조작을 허용한다. 이미지 콘텐츠를 프롬프트하는 것 외에도 음성 품질의 프롬프트를 통해 더 높은 품질의 이미지를 생성하도록 모델을 프롬프트할 수도 있습니다. 그림. 11(a)는 두 가지 예를 보여준다. 음성 프롬프트는 출력 이미지의 전반적인 품질을 향상시키는 데 매우 효과적임을 알 수 있다. 우리는 또한 우리의 방법에서 빠른 것이 항상 효과적이지 않다는 것을 관찰했다. 제공된 프롬프트가 LQ 이미지와 정렬되지 않을 때 프롬프트는 효과가 없으며 그림과 같다. 11(b) 우리는 IR 방법이 제공된 LQ 이미지에 충실하도록 유지하는 것이 이러한 합리적이라고 생각한다. 이것은 텍스트 대 이미지 생성 모델과 상당한 차이를 반영하고 접근법의 견고성을 강조한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline \\multicolumn{1}{c}{\\multirow{2}{*}{Degradation}} & \\multicolumn{1}{c}{Method} & \\multicolumn{1}{c}{PSNR} & \\multicolumn{1}{c}{SSIM} & \\multicolumn{1}{c}{LPIPS\\(\\downarrow\\)} & \\multicolumn{1}{c}{ManlQA} & \\multicolumn{1}{c}{ClipQA} & \\multicolumn{1}{c}{MUSIQ} \\\\ \\hline \\multirow{5}{*}{Single-SR (\\(\\times\\))} & \\multicolumn{1}{c}{BSRAAN} & **25.66** & **0.674** & 0.2159 & 0.2214 & 0.6169 & 70.38 \\\\  & Real-ESGAN & 24.26 & 0.6567 & 0.2116 & 0.2287 & 0.5884 & 69.51 \\\\  & \\multirow{2}{*}{Single-SR (\\(\\times\\))} & \\multirow{2}{*}{Shuffle} & 22.59 & 0.6019 & 0.2130 & 0.3304 & 0.7520 & 72.94 \\\\  & & DiffRIR & 23.44 & 0.5841 & 0.2337 & 0.2579 & 0.7417 & 71.64 \\\\  & PASD & 24.90 & 0.6653 & **1.0893** & 0.2607 & 0.6466 & 71.39 \\\\  & SUPIR (ours) & 22.66 & 0.5763 & 0.2662 & **0.4738** & **0.8049** & **73.38** \\\\ \\hline \\hline \\multirow{5}{*}{Single-SR (\\(\\times\\))} & \\multicolumn{1}{c}{BSRAAN} & **22.62** & 0.5121 & 0.3523 & 0.2069 & 0.5836 & 67.04 \\\\  & Real-ESGAN & 21.79 & **0.5280** & 0.3276 & 0.2501 & 0.5349 & 63.80 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Single-SR (\\(\\times\\))} & \\multirow{2}{*}{Shuffle} & 21.72 & 0.4857 & 0.3118 & 0.3029 & 0.7131 & 71.24 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 21.86 & 0.4957 & 0.3106 & 0.2845 & 0.7080 & 70.26 \\\\ \\cline{1-1}  & & PASD & 21.97 & 0.5149 & **0.3044** & 0.2412 & 0.6402 & 70.20 \\\\ \\cline{1-1}  & & SUPIR (ours) & 20.68 & 0.4488 & 0.3249 & **0.4467** & **0.8099** & **73.16** \\\\ \\hline \\hline \\multirow{5}{*}{Misure-Blur (\\(\\times\\)2)} & \\multicolumn{1}{c}{BSRAAN} & **24.947** & **0.6621** & 0.2261 & 0.2127 & 0.5984 & 69.44 \\\\ \\cline{1-1}  & Real-ESGAN & 24.08 & 0.6496 & 0.2028 & 0.2857 & 0.5853 & 69.27 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 22.26 & 0.5721 & 0.2301 & 0.3248 & 0.7458 & 72.82 \\\\ \\cline{1-1}  & & DiffRIR & 23.28 & 0.5741 & 0.2395 & 0.2829 & 0.7055 & 71.22 \\\\ \\cline{1-1}  & & PASD & 24.85 & 0.6500 & **0.1952** & 0.2500 & 6.0335 & 71.07 \\\\ \\cline{1-1}  & SUPIR (ours) & 22.43 & 0.5626 & 0.2771 & **0.4757** & **0.8110** & **73.58** \\\\ \\hline \\hline \\multirow{5}{*}{Misure-Blur (\\(\\times\\)4)} & \\multicolumn{1}{c}{BSRAAN} & 17.74 & 0.83616 & 0.5699 & 0.1006 & 0.4166 & 51.25 \\\\ \\cline{1-1}  & Real-ESGAN & 24.46 & **0.5129** & 0.4636 & 0.1236 & 0.4536 & 52.23 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 20.88 & 0.4174 & 0.4668 & 0.2365 & 0.5833 & 63.54 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 22.08 & 0.4918 & **0.4978** & 0.2403 & 0.6415 & 65.97 \\\\ \\cline{1-1}  & & PASD & 21.29 & 0.4983 & 0.3624 & 0.5909 & 69.09 \\\\ \\cline{1-1}  & & PASD & 22.07 & 0.4982 & 0.3612 & **0.4951** & **0.5935** & 69.20 \\\\ \\cline{1-1}  & SUPIR (ours) & 20.77 & 0.4571 & 0.3945 & **0.4647** & **0.7840** & **73.736** \\\\ \\hline \\hline \\multirow{5}{*}{Misure-Blur (\\(\\times\\)2)} & \\multicolumn{1}{c}{BSRAAN} & **22.88** & **0.5397** & 0.3445 & 0.1383 & 0.5402 & 64.81 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 22.01 & 0.5322 & 0.3549 & 0.2115 & 0.5730 & 64.76 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 21.93 & 0.4744 & **0.3422** & 0.2024 & 0.7354 & 70.94 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 21.79 & 0.4895 & 0.3465 & 0.2261 & 0.7059 & 69.28 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 21.90 & 0.5118 & 0.4893 & 0.2597 & 0.6326 & 70.43 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 20.84 & 0.4660 & 0.3806 & **0.4688** & **0.5021** & **73.58** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 정량적 비교는 표 1이다. ***Red************************ 색상과 **blue****** 색상은 최고와 두 번째 최고의 성능을 나타낸다. (하행형)은 더 작은 것을 나타내고, 다른 사람들의 경우 더 나은 것을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l} \\hline \\hline \\multicolumn{1}{c}{\\multirow{2}{*}{Negative}} & \\multicolumn{1}{c}{P} & \\multirow{2}{*}{PSNR} & \\multicolumn{1}{c}{SSIM} & \\multicolumn{1}{c}{LPIPS\\(\\downarrow\\)} & \\multicolumn{1}{c}{ManlQA} & \\multicolumn{1}{c}{ClipQA} & \\multicolumn{1}{c}{MUSIQ} \\\\ \\cline{1-1}  & Samples & Positive & Negative & & & & & & \\\\ \\hline \\hline \\multirow{5}{*}{\\(\\checkmark\\)} &\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '콘덴서(Connector)는 제안된 제로SFT 커넥터와 0 컨볼루션(95])을 비교한다. 정량적 결과는 Tab에 나와 있다. 2c. 제로SFT에 비해 제로 컨벌루션은 비수지 메트릭에 대한 비교 가능한 성능과 훨씬 더 낮은 전체 참조 성능을 생성한다. 그림에서. 9, 우리는 비지연 측정값의 감소가 저충실도 콘텐츠를 생성함으로써 발생한다는 것을 발견했다. 따라서 IR 작업을 위해 제로SFT는 지각 효과를 잃지 않고 충실도를 보장한다.\n' +
      '\n' +
      '데이터 스케일링을 훈련하는 우리는 IR, DIV2K [3] 및 LSDIR [1]에 대한 두 개의 더 작은 데이터 세트에서 대규모 모델을 훈련했다. 질적 결과는 그림 1에 나와 있다. 대규모 고품질 데이터에 대한 훈련의 중요성과 필요성을 분명히 보여주는 12이다.\n' +
      '\n' +
      '음성 품질의 샘플 및 신속한 타브. 2b는 다른 환경에서 몇 가지 정량적 결과를 보여준다. 여기에서 이미지 품질을 "긍정적 프롬프트"로 설명하는 긍정적인 단어를 사용하고, Sec 3.2에 기술된 음질 단어와 CFG 방법을 부정적인 프롬프트로 사용한다. 긍정적인 프롬프트나 부정적 프롬프트만을 추가하는 것은 이미지의 지각적 품질을 향상시킬 수 있음을 알 수 있다. 둘 다 동시에 사용하면 최고의 지각 결과를 얻을 수 있습니다. 연수를 위해 음성 샘플을 포함하지 않으면 이 두 가지 프롬프트는 지각 품질을 향상시킬 수 없다. 그림. 4 및 그림 4. 11(a)는 부정적인 프롬프트를 사용하여 가져온 화질 향상을 보여준다.\n' +
      '\n' +
      '제안된 복구 유도 샘플링 방법은 주로 하이퍼 매개변수 \\(\\tau_{r}\\)에 의해 제어된다. 더 큰 \\(\\tau_{r}\\)는 각 단계에서 생성에 대한 수정이 더 적다. 더 작은 \\(\\tau_{r}\\)는 더 많은 생성 콘텐츠가 LQ 이미지에 더 가깝게 이루어질 수밖에 없다. 그림을 참조해 주세요. 질적 비교를 위한 13. i\\(\\tau_{r}=0.5\\)의 경우, 그 출력이 LQ 이미지에 의해 제한되고 질감과 디테일을 생성할 수 없기 때문에 이미지가 흐릿하다. \\(\\tau_{r}=6\\)의 경우, 세대 중 지도가 많지 않다. 모델은 LQ 이미지, 특히 평평한 영역에 존재하지 않는 많은 질감을 생성한다. 그림. 8(a)은 가변 \\(\\tau_{r}\\)의 함수로서 회복의 정량적 결과를 보여준다. 그림과 같이. 8(a), 6에서 4로 감소하는 \\(\\tau_{r}\\)는 시각적 품질이 크게 감소하지 않는 반면 충실도 성능은 향상된다. 복원 지도가 지속적으로 강화되면서 PSNR이 계속 개선되지만 그림과 같이 세부 사항이 손실되면서 이미지가 점차 흐릿해진다. 13.13. 그래서 영상 품질을 크게 손상시키지 않으면서 효과적으로 이미지 품질을 손상시키지 못하면서 기본 파라미터로서 \\(\\tau_{r}=4\\)를 선택하게 된다.\n' +
      '\n' +
      '그림 11: 텍스트 프롬프트 인사이트입니다. (a) 네거티브 프롬프트는 상세하고 날카로운 복원 결과로 이어진다. (b)는 환각으로 긍정적인 프롬프트를 제공하며, SUPIR은 LQ 이미지에 없는 콘텐츠를 생성하는 것을 피한다. 더 나은 시야를 위해 줌입니다.\n' +
      '\n' +
      '그림 12: 스케일이 다른 데이터셋에 대한 SUPIR 학습에 대한 정성적 비교는 그림 12:였다. 더 나은 시야를 위해 줌입니다.\n' +
      '\n' +
      '그림 10: 실세계 LQ 이미지에 대한 정성적 비교는 그림이다. SUPIR은 구조화된 건물 및 라이텔리크 강을 성공적으로 회수한다. 또한 해변 의자에 수평 행성 등 LQ에 존재하는 디테일을 유지하고 있다. 더 나은 시야를 위해 줌입니다.\n' +
      '\n' +
      'hancing fidelity.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리는 SUPIR을 선구적인 IR 방법으로 제안하고 모델 스케일링, 데이터세트 농축 및 고급 설계 특징에 의해 구동되며 지각 품질이 향상되고 텍스트 프롬프트가 제어되는 IR의 지평을 확장한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Lsdir dataset: A large scale dataset for image restoration. [https://data.vision.ee.ethz.ch/yawli/index.html](https://data.vision.ee.ethz.ch/yawli/index.html), 2023. Accessed: 2023-11-15.\n' +
      '* [2] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 4432-4441, 2019.\n' +
      '* [3] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In _The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops_, 2017.\n' +
      '* [4] David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, and Antonio Torralba. Semantic photo manipulation with a generative image prior. _arXiv preprint arXiv:2005.07727_, 2020.\n' +
      '* [5] Sefi Bell-Kligler, Assaf Shocher, and Michal Irani. Blind super-resolution kernel estimation using an internal-gan. _Advances in Neural Information Processing Systems_, 32, 2019.\n' +
      '* [6] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6228-6237, 2018.\n' +
      '* [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [8] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image super-resolution: A new benchmark and a new model. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3086-3095, 2019.\n' +
      '* [9] Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. Glean: Generative latent bank for large-factor image super-resolution. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 14245-14254, 2021.\n' +
      '* [10] Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo. Real-world blind super-resolution via feature matching with implicit high-resolution priors. In _Proceedings of the 30th ACM International Conference on Multimedia_, pages 1329-1338, 2022.\n' +
      '* [11] Haoyu Chen, Jinjin Gu, Yihao Liu, Salma Abdel Magid, Chao Dong, Qiong Wang, Hanspeter Pfister, and Lei Zhu. Masked image training for generalizable deep image denoising. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1692-1703, 2023.\n' +
      '* [12] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-\\(\\alpha\\): Fast training of diffusion transformer for photorealistic text-to-image synthesis. _arXiv preprint arXiv:2310.00426_, 2023.\n' +
      '* [13] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang, and Fisher Yu. Dual aggregation transformer for image super-resolution. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 12312-12321, 2023.\n' +
      '* [14] Zheng Chen, Yulun Zhang, Ding Liu, Bin Xia, Jinjin Gu, Linghe Kong, and Xin Yuan. Hierarchical integration diffusion model for realistic image deblurring. _Advances in Neural Information Processing Systems_, 2023.\n' +
      '* [15] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. _arXiv preprint arXiv:2108.02938_, 2021.\n' +
      '* [16] DeepFloyd. Deepfloyd inference framework. [https://www.deepfloyd.ai/deepfloyd-if](https://www.deepfloyd.ai/deepfloyd-if), 2023. Accessed: 2023-11-14.\n' +
      '* [17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.\n' +
      '* [18] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.\n' +
      '* [19] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. _IEEE transactions on pattern analysis and machine intelligence_, 38(2):295-307, 2015.\n' +
      '* [20] Chao Dong, Chen Change Loy, and Xiaoou Tang. Accelerating the super-resolution convolutional neural network. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14_, pages 391-407. Springer, 2016.\n' +
      '* [21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* [22] Yuchen Fan, Jiahui Yu, Yiqun Mei, Yulun Zhang, Yun Fu, Ding Liu, and Thomas S Huang. Neural sparse representation for image restoration. _Advances in Neural Information Processing Systems_, 33:15394-15404, 2020.\n' +
      '* [23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _Advances in neural information processing systems_, 27, 2014.\n' +
      '* [24] Jinjin Gu and Chao Dong. Interpreting super-resolution networks with local attribution maps. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9199-9208, 2021.\n' +
      '* [25] Jinjin Gu, Hannan Lu, Wangmeng Zuo, and Chao Dong. Blind super-resolution with iterative kernel correction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1604-1613, 2019.\n' +
      '* [26] Jinjin Gu, Haoming Cai, Haoyu Chen, Xiaoxing Ye, Jimmy Ren, and Chao Dong. Pipal: a large-scale image quality assessment dataset for perceptual image restoration. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI 16_, pages 633-651, 2020.\n' +
      '* [27] Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code gan prior. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 3012-3021, 2020.\n' +
      '* [28] Jinjin Gu, Haoming Cai, Chao Dong, Jimmy S Ren, Radu Timofte, Yuan Gong, Shanshan Lao, Shuwei Shi, Jiahao Wang, Sidi Yang, et al. Ntire 2022 challenge on perceptual image quality assessment. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 951-967, 2022.\n' +
      '* [29] Jinjin Gu, Xianzheng Ma, Xiangtao Kong, Yu Qiao, and Chao Dong. Networks are slacking off: Understanding generalization problem in image deraining. _Advances in Neural Information Processing Systems_, 2023.\n' +
      '* [30] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [32] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [33] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven neuron pruning approach towards efficient deep architectures. _arXiv preprint arXiv:1607.03250_, 2016.\n' +
      '* [34] Yan Huang, Shang Li, Liang Wang, Tieniu Tan, et al. Unfolding the alternating optimization for blind super resolution. _Advances in Neural Information Processing Systems_, 33:5632-5643, 2020.\n' +
      '* [35] Zheng Hui, Jie Li, Xiumei Wang, and Xinbo Gao. Learning the non-differentiable optimization for blind super-resolution. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2093-2102, 2021.\n' +
      '* [36] Younghyun Jo, Sejong Yang, and Seon Joo Kim. Srflow-da: Super-resolution using normalizing flow with deep convolutional block. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 364-372, 2021.\n' +
      '* [37] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10124-10134, 2023.\n' +
      '* [38] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* [39] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. _arXiv preprint arXiv:1710.10196_, 2017.\n' +
      '* [40] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4401-4410, 2019.\n' +
      '* [41] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35:26565-26577, 2022.\n' +
      '* [42] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. _Advances in Neural Information Processing Systems_, 35:23593-23606, 2022.\n' +
      '* [43] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5148-5157, 2021.\n' +
      '* [44] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* [45] Xiangtao Kong, Xin Liu, Jinjin Gu, Yu Qiao, and Chao Dong. Refhash dropout in image super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6002-6012, 2022.\n' +
      '* [46] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1833-1844, 2021.\n' +
      '* [47] Jingyun Liang, Kai Zhang, Shuhang Gu, Luc Van Gool, and Radu Timofte. Flow-based kernel prior with application to blind super-resolution. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10601-10610, 2021.\n' +
      '* [48] Jie Liang, Hui Zeng, and Lei Zhang. Efficient and degradation-adaptive network for real-world image super-resolution. In _European Conference on Computer Vision_, pages 574-591. Springer, 2022.\n' +
      '* [49] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei, Bo Dai, Wanli Ouyang, Yu Qiao, and Chao Dong. Diff-bir: Towards blind image restoration with generative diffusion prior. _arXiv preprint arXiv:2308.15070_, 2023.\n' +
      '* [50] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong. Blind image super-resolution: A survey and beyond. _IEEE transactions on pattern analysis and machine intelligence_, 45(5):5461-5480, 2022.\n' +
      '**[51] 해토피아 류, 춘유안 리, 유정 리, 용재 이. 시각적 지시 튜닝이 있는 __ 개선 바구민은 시각적 지시 튜닝으로 개선되었습니다. _. arXiv 프리프린트 arXiv:2310.03744_, 2023.\n' +
      '* [52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _NeurIPS_, 2023.\n' +
      '* [53] Yihao Liu, Anran Liu, Jinjin Gu, Zhipeng Zhang, Wenhao Wu, Yu Qiao, and Chao Dong. Discovering distinctive" semantics in super-resolution networks. _arXiv preprint arXiv:2108.00406_, 2021.\n' +
      '* [54] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* [55] A Lugmayr, M Danelljan, L Van Gool, and R Timofte. Learning the super-resolution space with normalizing flow. _ECCV, Sydney_, 2020.\n' +
      '* [56] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. _arXiv preprint arXiv:2108.01073_, 2021.\n' +
      '* [57] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-supervised photo upsampling via latent space exploration of generative models. In _Proceedings of the ieee/cvf conference on computer vision and pattern recognition_, pages 2437-2445, 2020.\n' +
      '* [58] Tomer Michaeli and Michal Irani. Nonparametric blind super-resolution. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 945-952, 2013.\n' +
      '* [59] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongqing Qi, Ying Shan, and Xiaohu Qie. Tzi-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_, 2023.\n' +
      '* [60] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3883-3891, 2017.\n' +
      '* [61] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.\n' +
      '* [62] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep generative prior for versatile image restoration and manipulation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(11):7474-7489, 2021.\n' +
      '* [63] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SdxI: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [64] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. _arXiv preprint arXiv:1511.06434_, 2015.\n' +
      '* [65] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.\n' +
      '* [66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* [67] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '* [68] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* [69] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.\n' +
      '* [70] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* [71] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '* [72] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Jiaya Jia. Scale-recurrent network for deep image deblurring. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 8174-8182, 2018.\n' +
      '* [73] InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. [https://github.com/InternLM/InternLM](https://github.com/InternLM/InternLM), 2023.\n' +
      '* [74] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [75] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [76] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 2555-2563, 2023.\n' +
      '* [77] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. _arXiv preprint arXiv:2305.07015_, 2023.\n' +
      '* [78] Longguang Wang, Yingqian Wang, Xiaoyu Dong, Qingyu Xu, Jungang Yang, Wei An, and Yulan Guo. Unsupervised degradation representation learning for blind super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10581-10590, 2021.\n' +
      '* [79] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Recovering realistic texture in image super-resolution by deep spatial feature transform. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 606-615, 2018.\n' +
      '* [80] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. Towards real-world blind face restoration with generative facial prior. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9168-9178, 2021.\n' +
      '* [81] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1905-1914, 2021.\n' +
      '* [82] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. _arXiv preprint arXiv:2212.00490_, 2022.\n' +
      '* [83] Pengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, Qixiang Ye, Wangmeng Zuo, and Liang Lin. Component divide-and-conquer for real-world image super-resolution. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VIII 16_, pages 101-117. Springer, 2020.\n' +
      '* [84] Yuxin Wu and Kaiming He. Group normalization. In _Proceedings of the European conference on computer vision (ECCV)_, pages 3-19, 2018.\n' +
      '* [85] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image generation via large mixture of diffusion paths. _arXiv preprint arXiv:2305.18295_, 2023.\n' +
      '* [86] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1191-1200, 2022.\n' +
      '* [87] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. Gan prior embedded network for blind face restoration in the wild. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 672-681, 2021.\n' +
      '* [88] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. Pixel-aware stable diffusion for realistic image super-resolution and personalized stylization. _arXiv preprint arXiv:2308.14469_, 2023.\n' +
      '* [89] Jiale Zhang, Yulun Zhang, Jinjin Gu, Yongbing Zhang, Linghe Kong, and Xin Yuan. Accurate image restoration with attention retractable transformer. In _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* [90] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. _IEEE transactions on image processing_, 26(7):3142-3155, 2017.\n' +
      '* [91] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep cnn denoiser prior for image restoration. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3929-3938, 2017.\n' +
      '* [92] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-based image denoising. _IEEE Transactions on Image Processing_, 27(9):4608-4622, 2018.\n' +
      '* [93] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation model for deep blind image super-resolution. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4791-4800, 2021.\n' +
      '* [94] Kai Zhang, Yawei Li, Jingyun Liang, Jiezhang Cao, Yulun Zhang, Hao Tang, Deng-Ping Fan, Radu Timofte, and Luc Van Gool. Practical blind image denoising via swin-conv-unet and data synthesis. _Machine Intelligence Research_, pages 1-14, 2023.\n' +
      '* [95] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3836-3847, 2023.\n' +
      '* [96] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 586-595, 2018.\n' +
      '* [97] Ruofan Zhang, Jinjin Gu, Haoyu Chen, Chao Dong, Yulun Zhang, and Wenming Yang. Crafting training degradation distribution for the accuracy-generalization trade-off in real-world super-resolution. _International Conference on Machine Learning (ICML)_, 2023.\n' +
      '* [98] Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and Yun Fu. Residual non-local attention networks for image restoration. In _International Conference on Learning Representations (ICLR)_, 2019.\n' +
      '* [99] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image restoration. _IEEE transactions on pattern analysis and machine intelligence_, 43(7):2480-2495, 2020.\n' +
      '* [100] Yi Zhang, Xiaoyu Shi, Dasono Li, Xiaogang Wang, Jian Wang, and Hongsheng Li. A unified conditional framework for diffusion-based image restoration. _arXiv preprint arXiv:2305.20049_, 2023.\n' +
      '* [101] Yang Zhao, Yu-Chuan Su, Chun-Te Chu, Yandong Li, Marius Renn, Yukun Zhu, Changyou Chen, and Xuhui Jia. Rethinking deep face restoration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7652-7661, 2022.\n' +
      '* [102] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy. Towards robust blind face restoration with codebook lookup transformer. _Advances in Neural Information Processing Systems_, 35:30599-30611, 2022.\n' +
      '* [103] Jiapeng Zhu, Deli Zhao, Bo Zhang, and Bolei Zhou. Disentangled inference for gans with latently invertible autoencoder. _International Journal of Computer Vision_, 130(5):1259-1276, 2022.\n' +
      '\n' +
      '부록.\n' +
      '\n' +
      '### Degradation-Robust Encoder\n' +
      '\n' +
      '그림과 같이. 본문의 2는 어댑터로 낮은 품질의 입력을 먹이기 전에 열화-로부스트 인코더를 학습하여 배치한다. 제안된 분해-부스트 인코더의 효과를 입증하기 위해 합성 데이터를 사용하여 실험을 수행한다. 그림에서. 14, 우리는 동일한 디코더를 사용하여 다른 인코더로부터의 잠재 표현을 디코딩하는 결과를 보여준다. 원래 인코더는 열화에 저항할 능력이 없고 디코딩된 이미지에는 여전히 노이즈와 블러가 포함되어 있음을 알 수 있다. 제안된 열화-로부스트 인코더는 열화의 영향을 줄일 수 있으며, 이는 생성 모델이 이미지 콘텐츠[49]로서 유물을 오인하는 것을 더욱 방지한다.\n' +
      '\n' +
      '### LLaVA Annotation\n' +
      '\n' +
      '우리의 확산 모델은 복원 과정에서 텍스트 프롬프트를 수용할 수 있습니다. 우리가 사용하는 신속한 전략은 두 가지 구성 요소로 구성되며, 하나는 LLaVA-v1.5-13B[51]에 의해 자동으로 주석이 달린 것이며 다른 하나는 표준화된 디폴트 양의 품질 프롬프트이다. 프롬프트 전략의 고정된 부분은 "시네마틱, 하이 콘트라스트, 고도로 상세하고 비현실적인 엔진, 칸톤 EOS R 카메라를 사용하여 촬영한 하이퍼 상세 사진-현실적 최대 디테일, 32k, 컬러 그레이딩, 초HD, 극단적인 꼼꼼한 디테일, 피부 기공 디테일, 하이퍼 선명도, 변형 없이 완벽한 유니리얼 엔진 5, 4k 렌더링"과 같은 단어를 포함하여 품질에 대한 긍정적인 설명을 제공한다. LLaVA 구성요소의 경우, 우리는 그림에서 예시된 바와 같이 상세한 이미지 캡션을 생성하기 위해 "이 이미지와 그 스타일을 매우 상세한 방식으로 설명"하는 명령을 사용한다. 18. 간혹 부정확성이 발생할 수 있으며 LLaVA-v1.5-13B는 일반적으로 주목할만한 정밀도로 저품질 입력의 본질을 포착한다. 재구성된 버전의 입력을 사용하여 이러한 부정확성을 보정하는 데 효과적인 것으로 입증되어 LLaVA가 이미지의 대부분의 콘텐츠에 대한 정확한 설명을 제공할 수 있다. 또한 SUPIR은 [52]에 자세히 설명된 바와 같이 잠재적인 환각 프롬프트의 영향을 완화하는 데 효과적이다.\n' +
      '\n' +
      '2.1.2.\n' +
      '\n' +
      '그림 23은 음질 프롬프트의 사용이 복원 이미지의 화질을 실질적으로 향상시킨다는 증거를 제시한다. 그러나 그림에서 관찰된 바와 같이. 15, 음의 프롬프트는 복원 대상이 명확한 의미론적 정의가 없는 경우 유물을 도입할 수 있다. 이 문제는 낮은 품질의 입력과 언어 개념 사이의 오정렬에서 비롯되었을 가능성이 있다.\n' +
      '\n' +
      '완성된 샘이 탄생합니다.\n' +
      '\n' +
      '음성 프롬프트는 품질을 향상시키는 데 매우 효과적이지만, 음성 품질의 샘플 부족과 신속성의 부족은 품질을 향상시키는 데 매우 효과적입니다.\n' +
      '\n' +
      '그림 14: 분해-부스트 인코더(DR Encoder)의 효과는 결과를 통해 입증되며, 이는 초기에 다양한 인코더와 인코딩되고 후속적으로 디코딩함으로써 달성된다. 이 과정은 확산 모델에 도입되기 전에 저품질 입력에서의 분해를 효과적으로 감소시킨다.\n' +
      '\n' +
      '그림 15: Negative 프롬프트는 낮은 품질의 입력이 명확한 의미론을 갖지 않을 때 유물을 유발한다.\n' +
      '\n' +
      '훈련 데이터는 미세 조정 SUPIR이 이러한 프롬프트를 효과적으로 이해할 수 없음을 초래한다. 이 문제를 해결하기 위해 본문의 Sec 3.2에서 SDXL 모델에서 부정적인 개념을 증류하는 방법을 소개한다. 음성 샘플을 생성하기 위한 과정은 그림 1에 나와 있다. 19. 텍스트 대 이미지 접근법을 통한 음성 샘플의 다이렉트 샘플링은 종종 무의미한 이미지를 초래한다. 이 문제를 해결하기 위해 데이터 세트의 훈련 샘플도 소스 이미지로 사용한다. 우리는 \\(0.5\\)의 강도 설정으로 [56]에서 제안된 바와 같이 이미지 대 이미지 방식으로 음성 샘플을 생성한다.\n' +
      '\n' +
      '적절한 결과.\n' +
      '\n' +
      '우리는 이 섹션에서 더 많은 결과를 제공합니다. 그림. 16건은 전면적인 측정값이 인간 평가와 일치하지 않는 추가 사례를 제시한다. 그림에서. 17, 우리는 학습에 음성 샘플을 포함하지 않고 부정적인 품질의 프롬프트를 사용하면 유물을 유발할 수 있음을 보여준다. 그림에서. 20~22, 우리는 다른 방법과 더 많은 시각적 비교를 제공한다. 70개의 예는 SUPIR의 강력한 복원 능력과 복원된 이미지의 가장 현실적인 것을 증명한다. 텍스트 프롬프트로 제어 가능한 이미지 복원의 더 많은 예는 그림에서 찾을 수 있다. 23.\n' +
      '\n' +
      '그림 16: 조건 샘플은 메트릭 평가와 인간 평가 사이의 오정렬을 강조한다. SUPIR은 충실도가 높은 텍스처를 가진 이미지를 생성하지만 메트릭 평가에서 더 낮은 점수를 받는 경향이 있다.\n' +
      '\n' +
      '그림 17: 그림 그림에 대한 모기 시각적 결과. 본문 4번. CFG는 교육에 부정적인 품질의 샘플을 포함하지 않으면 유물을 소개한다. 음성 품질의 샘플을 추가하면 CFG를 통한 추가 품질 개선이 가능합니다.\n' +
      '\n' +
      '그림 18: Snapshots는 LLaVA 주석은 LLaVA가 낮은 품질의 입력에도 대부분의 콘텐츠를 정확하게 예측한다는 것을 보여준다. 좀 더 자세한 시각은 줌으로 해주세요.\n' +
      '\n' +
      '그림 19: 음성 샘플 생성의 Pipeline. 잡음 대 이미지 접근에서 (a) 샘플링은 무의미한 출력으로 이어진다. (b) 고품질 이미지의 합성 음성 샘플입니다. 더 나은 시야를 위해 줌입니다.\n' +
      '\n' +
      '그림 20: 다양한 방법과의 정성적 비교이다. 우리의 방법은 도전적인 열화에 따라 해당 물체의 질감과 세부 사항을 정확하게 복원할 수 있다. 더 나은 시야를 위해 줌입니다.\n' +
      '\n' +
      '그림 21: 다양한 방법과의 정성적 비교이다. 우리의 방법은 도전적인 열화에 따라 해당 물체의 질감과 세부 사항을 정확하게 복원할 수 있다. 더 나은 시야를 위해 줌입니다.\n' +
      '\n' +
      '그림 22: 서로 다른 방법과 정성적 비교를 하였다. 우리의 방법은 도전적인 열화에 따라 해당 물체의 질감과 세부 사항을 정확하게 복원할 수 있다. 더 나은 시야를 위해 줌입니다.\n' +
      '\n' +
      '그림 23: 텍스트의 모식 시각 결과는 영향을 촉발한다. (a)와 (b)는 각각 양성 프롬프트와 음성 프롬프트의 예를 보여준다. 더 나은 시야를 위해 줌입니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>