<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Scaling Up to Excellence:\n' +
      '\n' +
      'Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild\n' +
      '\n' +
      'Fanghua Yu\\({}^{1,*}\\), Jinjin Gu\\({}^{2,3,*}\\), Zheyuan Li\\({}^{1}\\), Jinfan Hu\\({}^{1}\\), Xiangtao Kong\\({}^{4}\\),\n' +
      '\n' +
      'Xintao Wang\\({}^{5}\\), Jingwen He\\({}^{2,6}\\), Yu Qiao\\({}^{2}\\), Chao Dong\\({}^{1,2,\\dagger}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences \\({}^{2}\\)Shanghai AI Laboratory\n' +
      '\n' +
      '\\({}^{3}\\)University of Sydney \\({}^{4}\\)The Hong Kong Polytechnic University \\({}^{5}\\)ARC Lab, Tencent PCG \\({}^{5}\\)The Chinese University of Hong Kong\n' +
      '\n' +
      'Project Page: [https://supir.xpixel.group](https://supir.xpixel.group)\n' +
      '\n' +
      'Contribute Equally. \\({}^{\\dagger}\\) Corresponding Author.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '_We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image restoration method that harnesses generative prior and the power of model scaling up. Leveraging multi-modal techniques and advanced generative prior, SUPIR marks a significant advance in intelligent and realistic image restoration. As a pivotal catalyst within SUPIR, model scaling dramatically enhances its capabilities and demonstrates new potential for image restoration. We collect a dataset comprising 20 million high-resolution, high-quality images for model training, each enriched with descriptive text annotations. SUPIR provides the capability to restore images guided by textual prompts, broadening its application scope and potential. Moreover,we introduce negative-quality prompts to further improve perceptual quality. We also develop a restoration-guided sampling method to suppress the fidelity issue encountered in generative-based restoration. Experiments demonstrate SUPIR\'s exceptional restoration effects and its novel capacity to manipulate restoration through textual prompts._\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'With the development of image restoration (IR), expectations for the perceptual effects and intelligence of IR results have significantly increased. IR methods based on generative priors [42, 49, 67, 82] leverage powerful pre-trained generative models to introduce high-quality generation and prior knowledge into IR, bringing significant progress in these aspects. Continuously enhancing the capabilities of the generative prior is key to achieving more intelligent IR results, with model scaling being a crucial and effective approach. There are many tasks that have obtained astonishing improvements from scaling, such as SAM [44] and large language models [7, 73, 74]. This further motivates our effort to build large-scale, intelligent IR models capable of producing ultra-high-quality images. However, due to engineering constraints such as computing resources, model architecture, training data, and the cooperation of generative models and IR, scaling up IR models is challenging.\n' +
      '\n' +
      'In this work, we introduce SUPIR (Scaling-UP IR), the largest-ever IR method, aimed at exploring greater potential in visual effects and intelligence. Specifically, SUPIR employs StableDiffusion-XL (SDXL) [63] as a powerful generative prior, which contains 2.6 billion parameters. To effectively apply this model, we design and train a adaptor with more than 600 million parameters. Moreover, we have collected over 20 million high-quality, high-resolution images to fully realize the potential offered by model scaling. Each image is accompanied by detailed descriptive text, enabling the control of restoration through textual prompts. We also utilize a 13-billion-parameter multi-modal language model to provide image content prompts, greatly improving the accuracy and intelligence of our method. The proposed SUPIR model demonstrates exceptional performance in a variety of IR tasks, achieving the best visual quality, especially in complex and challenging real-world scenarios. Additionally, the model offers flexible control over the restoration process through textual prompts, vastly broadening the possibility of IR. Fig. 1 illustrates the effects by our model, showcasing its superior performance.\n' +
      '\n' +
      'Our work goes far beyond simply scaling. While pursuing an increase in model scale, we face a series of complex challenges. First, when applying SDXL for IR, existing Adaptor designs either too simple to meet the complex requirements of IR [59] or are too large to train together with SDXL [95]. To solve this problem, we trim the ControlNet and designed a new connector called ZeroSFT to work with the pre-trained SDXL, aiming to efficiently implement the IR task while reducing computing costs. In order to enhance the model\'s ability to accurately interpret the content of low-quality images, we fine-tune the image encoder to improve its robustness to variations in image degradation. These measures make scaling the model feasible and effective, and greatly improve its stability. Second, we amass a collection of 20 million high-quality, high-resolution images with descriptive text annotations, providing a solid foundation for the model\'s training. We adopt a counter-intuitive strategy by incorporating poor quality, negative samples into training. In this way, we can use negative quality prompts to further improve visual effects. Our results show that this strategy significantly improves image quality compared to using only high-quality positive samples. Finally, powerful generative prior is a double-edged sword. Uncontrolled generation may reduce restoration fidelity, making IR no longer faithful to the input image. To mitigate this low-fidelity issue, we propose a novel restoration-guided sampling method. All these strategies, coupled with efficient engineering implementation, are key to enabling the scaling up of SUPIR, pushing the boundaries of advanced IR. This comprehensive approach, encompassing everything from model architecture to data collection, positions SUPIR at the forefront of image restoration technology, setting a new benchmark for future advancements.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Image Restoration.The goal of IR is to convert degraded images into high-quality degradation-free versions [22, 26, 89, 91, 98, 99]. In the early stage, researchers independently explored different types of image degradation, such as super-resolution (SR) [13, 19, 20], denoising [11, 90, 92], and deblurring [14, 60, 72]. However, these methods are often based on specific degradation assumptions [25, 50, 58] and therefore lack generalization ability to other degradations [29, 53, 97]. Over time, the need for blind restoration methods that are not based on specific degradation assumptions has grown [5, 10, 34, 35, 46, 47, 48, 94]. In this trend, some methods [81, 93] approximate synthesize real-world degradation by more complex degradation models, and are well-known for handling multiple degradation with a single model. Recent research, such as DiffBIR [49], unifies different restoration problems into a single model. In this paper, we adopt a similar setting to DiffBIR and use a single model to achieve effective processing of various severe degradations.\n' +
      '\n' +
      'Generative Prior.Generative priors are adept at capturing the inherent structures of the image, enabling the generation of images that follow natural image distribution. The emergence of GANs [23, 39, 40, 64] has underscored the significance of generative priors in IR. Various approaches employ these priors, including GAN inversion [2, 4, 27, 57, \n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      'ing an engineering-feasible implementation. Each block within the encoder module of SDXL is mainly composed of several Vision Transformer (ViT) [21] blocks. We identified two key factors contributing to the effectiveness of ControlNet: large network capacity and efficient initialization of the trainable copy. Notably, even partial trimming of blocks in the trainable copy retains these crucial characteristics in the adaptor. Therefore, we simply trim half of the ViT blocks from each encoder block, as shown in Fig. 3(b). Second, we redesign the connector that links the adaptor to SDXL. While SDXL\'s generative capacity delivers excellent visual effects, it also renders pixel-level precise control challenging. ControlNet employs zero convolution for generation guidance, but relying solely on residuals is insufficient for the control required by IR. To amplify the influence of LQ guidance, we introduced a ZeroSFT module, as depicted in Fig. 3(c). Building based on zero convolution, ZeroSFT encompasses an additional spatial feature transfer (SFT) [79] operation and group normalization [84].\n' +
      '\n' +
      '### Scaling Up Training Data\n' +
      '\n' +
      'Image Collection.The scaling of the model requires a corresponding scaling of the training data [38]. But there is no large-scale high-quality image dataset available for IR yet. Although DIV2K [3] and LSDIR [1] offer high image quality, they are limited in quantity. Larger datasets like ImageNet (IN) [17], LAION-5B[69], and SA-1B [44] contain more images, but their image quality does not meet our high standards. To this end, we collect a new large-scale dataset of high-resolution images, which includes 20 million 1024\\(\\times\\)1024 high-quality, texture-rich, and content-clear images. A comparison on scales of the collected dataset and the existing dataset is shown in Fig. 3. We also included an additional 70K unaligned high-resolution facial images from FFHQ-raw dataset [40] to improve the model\'s face restoration performance. In Fig. 5(a), we show the relative size of our data compared to other well-known datasets.\n' +
      '\n' +
      'Multi-Modality Language Guidance.Diffusion models are renowned for their ability to generate images based on textual prompts. We believe that textual prompts can also significantly aid IR for the following reasons: (1) Understanding image content is crucial for IR. Existing frameworks often overlook or implicitly handle this understanding [24, 29]. By incorporating textual prompts, we explicitly convey the understanding of LQ images to the IR model, facilitating targeted restoration of missing information. (2) In cases of severe degradation, even the best IR models may struggle to recover completely lost information. In such cases, textual prompts can serve as a control mechanism, enabling targeted completion of missing information based on user preferences. (3) We can also describe the desired image quality through text, further enhancing the perceptual quality of the output. See Fig. 1(b) for some examples. To this end, we make two main modifications. First, we revise the overall framework to incorporate the LLaVA multi-modal large language model [52] into our pipeline, as shown in Fig. 2. LLaVA takes the degradation-robust processed LQ images \\(x^{\\prime}_{LQ}=\\mathcal{D}(\\mathcal{E}_{\\mathrm{dr}}(x_{LQ}))\\) as input and explicitly understands the content within the images, out\n' +
      '\n' +
      'Figure 4: CFG introduces artifacts without negative training samples, hindering visual quality improvement. Adding negative samples allows further quality enhancement through CFG.\n' +
      '\n' +
      'Figure 3: This figure illustrates (a) the overall architecture of the used SDXL and the proposed adaptor, (b) a trimmed trainable copy of the SDXL encoder with reduced ViT blocks for efficiency, and (c) a novel ZeroSFT connector for enhanced control in IR, where \\(X_{f}\\) and \\(X_{s}\\) denote the input feature maps from the Decoder and Encoder shortcut, respectively, \\(X_{c}\\) is the input from the adaptor, and \\(X_{fo}\\) is the output. The model is designed to effectively use the large-scale SDXL as a generative prior.\n' +
      '\n' +
      'putting in the form of textual descriptions. These descriptions are then used as prompts to guide the restoration. This process can be automated during testing, eliminating the need for manual intervention. Secondly, following the approach of PixART [12], we also collect textual annotations for all the training images, to reinforce the role of textual control during the training of out model. These two changes endow SUPIR with the ability to understand image content and to restore images based on textual prompts.\n' +
      '\n' +
      'Negative-Quality Samples and Prompt.Classifier-free guidance (CFG) [30] provides another way of control by using negative prompts to specify undesired content for the model. We can use this feature to specify the model NOT to produce low-quality images. Specifically, at each step of diffusion, we will make two predictions using positive prompts \\(\\mathrm{pos}\\) and negative prompts \\(\\mathrm{neg}\\), and take the fusion of these two results as the final output \\(z_{t-1}\\):\n' +
      '\n' +
      '\\[z_{t-1}^{\\mathrm{pos}} =\\mathcal{H}(z_{t},z_{LQ},\\sigma_{t},\\mathrm{pos}),z_{t-1}^{ \\mathrm{neg}}=\\mathcal{H}(z_{t},z_{LQ},\\sigma_{t},\\mathrm{neg}),\\] \\[z_{t-1} =z_{t-1}^{\\mathrm{pos}}+\\lambda_{\\mathrm{cfg}}\\times(z_{t-1}^{ \\mathrm{pos}}-z_{t-1}^{\\mathrm{neg}}),\\]\n' +
      '\n' +
      'where \\(\\mathcal{H}(\\cdot)\\) is our diffusion model with adaptor, \\(\\sigma_{t}\\) is the variance of the noise at time-step \\(t\\), and \\(\\lambda_{\\mathrm{cfg}}\\) is a hyper-parameter. In our framework, \\(\\mathrm{pos}\\) can be the image description with positive words of quality, and \\(\\mathrm{neg}\\) is the negative words of quality, _e.g._, "_oil painting, cartoon, blur, dirty, messy, low quality, deformation, low resolution, over-smooth_". Accuracy in predicting both positive and negative directions is crucial for the CFG technique. However, the absence of negative-quality samples and prompts in our training data may lead to a failure of the fine-tuned SUPIR in understanding negative prompts. Therefore, using negative-quality prompts during sampling may introduce artifacts, see Fig. 4 for an example. To address this problem, we used SDXL to generate 100K images corresponding to the negative-quality prompts. We counter-intuitively add these low-quality images to the training data to ensure that negative-quality concept can be learned by the proposed SUPIR model.\n' +
      '\n' +
      '### Restoration-Guided Sampling\n' +
      '\n' +
      'Powerful generative prior is a double-edged sword, as too much generation capacity will in turn affect the fidelity of the recovered image. This highlights the fundamental difference between IR tasks and generation tasks. We need means to limit the generation to ensure that the image recovery is faithful to the LQ image. We modified the EDM sampling method [41] and proposed a restoration-guided sampling method to solve this problem. We hope to selectively guide the prediction results \\(z_{t-1}\\) to be close to the LQ image \\(z_{LQ}\\) in each diffusion step. The specific algorithm is shown in Algorithm 1, where \\(T\\) is the total step number, \\(\\{\\sigma_{t}\\}_{t=1}^{T}\\) are the noise variance for \\(T\\) steps, \\(c\\) is the additional text prompt condition. \\(\\tau_{r}\\), \\(S_{\\mathrm{churn}}\\), \\(S_{\\mathrm{noise}}\\), \\(S_{\\mathrm{min}}\\), \\(S_{\\mathrm{max}}\\) are five hyper-parameters, but only \\(\\tau_{r}\\) is related to the restoration guidance, the others remain unchanged compared to the original EDM method [41]. For better understanding, a simple diagram is shown in Fig. 5(b). We perform weighted interpolation between the predicted output \\(\\hat{z}_{t-1}\\) and the LQ latent \\(z_{LQ}\\) as the restoration-guided output \\(z_{t-1}\\). Since the low-frequency information of the image is mainly generated in the early stage of diffusion prediction [67] (where \\(t\\) and \\(\\sigma_{t}\\) are relatively large, and the weight \\(k=(\\sigma_{t}/\\sigma_{T})^{\\tau_{r}}\\) is also large), the prediction result is closer to \\(z_{LQ}\\) to enhance fidelity. In the later stages of diffusion prediction, mainly high-frequency details are generated. There should not be too many constraints at this time to ensure that detail and texture can be adequately generated. At this time, \\(t\\) and \\(\\sigma_{t}\\) are relatively small, and weight \\(k\\) is also small. Therefore, the predicted results will not be greatly affected Through this method, we can control the generation during the diffusion sampling process to ensure fidelity.\n' +
      '\n' +
      '```\n' +
      'Input:\\(\\mathcal{H}\\), \\(\\{\\sigma_{t}\\}_{t=1}^{T}\\), \\(z_{LQ}\\), \\(c\\) Hyper-parameter:\\(\\tau_{r}\\), \\(S_{\\mathrm{churn}}\\), \\(S_{\\mathrm{noise}}\\), \\(S_{\\mathrm{min}}\\), \\(S_{\\mathrm{max}}\\)\n' +
      '1:sample\\(z_{T}\\sim\\mathcal{N}(\\mathbf{0},\\sigma_{T}^{2}\\mathbf{I})\\)\n' +
      '2:for\\(t\\in\\{T,\\ldots,1\\}\\)do\n' +
      '3:sample\\(\\mathbf{\\epsilon}_{t}\\sim\\mathcal{N}\\left(\\mathbf{0},S_{\\mathrm{noise}}^{2} \\mathbf{I}\\right)\\)\n' +
      '4:\\(\\gamma_{t}\\leftarrow\\begin{cases}\\min\\left(\\frac{S_{\\mathrm{churn}}}{N}, \\sqrt{2}-1\\right)&\\text{if }\\sigma_{t}\\in[S_{\\mathrm{min}},S_{\\mathrm{max}}]\\\\ 0&\\text{otherwise}\\end{cases}\\)\n' +
      '5:\\(k_{t}\\leftarrow(\\sigma_{t}/\\sigma_{T})^{\\tau_{r}}\\), \\(\\hat{z}_{t}\\gets z_{t}+\\sqrt{\\hat{\\sigma}_{t}^{2}-\\sigma_{t}^{2}}\\mathbf{ \\epsilon}_{t}\\), \\(\\hat{\\sigma}_{t}\\leftarrow\\sigma_{t}+\\gamma_{t}\\sigma_{t}\\)\n' +
      '6:\\(\\hat{z}_{t-1}\\leftarrow\\mathcal{H}\\left(\\hat{z}_{t},z_{LQ},\\hat{\\sigma}_{t},c\\right)\\)\n' +
      '7:\\(d_{t}\\leftarrow(\\hat{z}_{t}-(\\hat{z}_{t-1}+k_{t}(z_{LQ}-\\hat{z}_{t-1})))/\\hat{ \\sigma}_{t}\\)\n' +
      '8:\\(z_{t-1}\\leftarrow\\hat{z}_{t}+(\\sigma_{t-1}-\\hat{\\sigma}_{t})\\,d_{t}\\)\n' +
      '9:endfor\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1** Restoration-Guided Sampling.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Model Training and Sampling Settings\n' +
      '\n' +
      'For training, the overall training data includes 20 million high-quality images with text descriptions, 70K face images and 100K negative-quality samples and the corresponding negative prompts. To enable a larger batch size, we crop them into 512\\(\\times\\)512 patches during training. We train our model using a synthetic degradation model, following the setting used by Real-ESRGAN [81], the only difference is that we resize the produced LQ images to 512\\(\\times\\)512 for training. We use the AdamW optimizer [54] with a learning rate of \\(0.00001\\). The training process spans 10 days and is\n' +
      '\n' +
      'Figure 5: (a) We show the relative size of our data compared to other well-known datasets. Compared with SA-1B [44], our dataset has higher quality and more image diversity. (b) We demonstrate our restoration-guided sampling mechanism.\n' +
      '\n' +
      'conducted on 64 Nvidia A6000 GPUs, with a batch size of 256. For testing, the hyper-parameters are \\(T\\)=100, \\(\\lambda_{\\mathrm{cfg}}\\)=7.5, and \\(\\tau_{r}=4\\). Our method is able to process images with the size of 1024\\(\\times\\)1024. We resize the short side of the input image to 1024 and crop a 1024\\(\\times\\)1024 sub-image for testing, and then resize it back to the original size after restoration. Unless stated otherwise, prompts will not be provided manually - the processing will be entirely automatic.\n' +
      '\n' +
      '### Comparison with Existing Methods\n' +
      '\n' +
      'Our method can handle a wide range of degradations, and we compare it with the state-of-the-art methods with the same capabilities, including BSRGAN [93], Real-ESRGAN [81], StableSR [77], DiffBIR [49] and PASD [88]. Some of them are constrained to generating images of 512\\(\\times\\)512 size. In our comparison, we crop the test image to meet this requirement and downsample our results to facilitate fair comparisons. We conduct comparisons on both synthetic data and real-world data.\n' +
      '\n' +
      'Synthetic Data.To synthesize LQ images for testing, we follow previous works [45, 97] and demonstrate our effects on several representative degradations, including both single degradations and complex mixture degradations. Specific details can be found in Tab. 1. We selected the following metrics for quantitative comparison: full-reference metrics PSNR, SSIM, LPIPS [96], and the non-reference metrics ManIQA [86], ClipIQA [76], MUSIQ [43]. It can be seen that our method achieves the best results on all non-reference metrics, which reflects the excellent image quality of our results. At the same time, we also note the disadvantages of our method in full-reference metrics. We present a simple experiment that highlights the limitations of these full-reference metrics, see Fig. 7. It can be seen\n' +
      '\n' +
      'Figure 6: Qualitative comparison with different methods. Our method can accurately restore the texture and details of the corresponding object under challenging degradation. Other methods fail to recover semantically correct details such as broken beaks and irregular faces.\n' +
      '\n' +
      'Figure 7: These examples show the misalignment between metric evaluation and human evaluation. SUIPR generates images with high-fidelity textures, but obtains lower metrics.\n' +
      '\n' +
      'that our results have better visual effects, but they do not have an advantage in these metrics. This phenomenon has also been noted in many studies as well [6, 26, 28]. We argue that with the improving quality of IR, there is a need to reconsider the reference values of existing metrics and suggest more effective ways to evaluate advanced IR methods. We also show some qualitative comparison results in Fig. 6. Even under severe degradation, our method consistently produces highly reasonable and high-quality images that faithfully represent the content of the LQ images.\n' +
      '\n' +
      'Restoration in the Wild.We also test our method on real-world LQ images. We collect a total of 60 real-world LQ images from RealSR [8], DRealSR [83], Real47 [49], and online sources, featuring diverse content including animals, plants, faces, buildings, and landscapes. We show the qualitative results in Fig. 10, and the quantitative results are shown in Tab. 1(a). These results indicate that the images produced by our method have the best perceptual quality. We also conduct a user study comparing our method on real-world LQ images, with 20 participants involved. For each set of comparison images, we instructed participants to choose the restoration result that was of the highest quality among these test methods. The results are shown in Fig. 8, revealing that our approach significantly outperformed state-of-the-art methods in perceptual quality.\n' +
      '\n' +
      '### Controlling Restoration with Textual Prompts\n' +
      '\n' +
      'After training on a large dataset of image-text pairs and leveraging the feature of the diffusion model, our method can selectively restore images based on human prompts. Fig. 1(b) illustrates some examples. In the first case, the bike restoration is challenging without prompts, but upon receiving the prompt, the model reconstructs it accurately. In the second case, the material texture of the hat can be adjusted through prompts. In the third case, even high-level semantic prompts allow manipulation over face attributes. In addition to prompting the image content, we can also prompt the model to generate higher-quality images through negative-quality prompts. Fig. 11(a) shows two examples. It can be seen that the negative prompts are very effective in improving the overall quality of the output image. We also observed that prompts in our method are not always effective. When the provided prompts do not align with the LQ image, the prompts become ineffective, see Fig. 11(b). We consider this reasonable for an IR method to stay faithful to the provided LQ image. This reflects a significant distinction from text-to-image generation models and underscores the robustness of our approach.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline \\multicolumn{1}{c}{\\multirow{2}{*}{Degradation}} & \\multicolumn{1}{c}{Method} & \\multicolumn{1}{c}{PSNR} & \\multicolumn{1}{c}{SSIM} & \\multicolumn{1}{c}{LPIPS\\(\\downarrow\\)} & \\multicolumn{1}{c}{ManlQA} & \\multicolumn{1}{c}{ClipQA} & \\multicolumn{1}{c}{MUSIQ} \\\\ \\hline \\multirow{5}{*}{Single-SR (\\(\\times\\))} & \\multicolumn{1}{c}{BSRAAN} & **25.66** & **0.674** & 0.2159 & 0.2214 & 0.6169 & 70.38 \\\\  & Real-ESGAN & 24.26 & 0.6567 & 0.2116 & 0.2287 & 0.5884 & 69.51 \\\\  & \\multirow{2}{*}{Single-SR (\\(\\times\\))} & \\multirow{2}{*}{Shuffle} & 22.59 & 0.6019 & 0.2130 & 0.3304 & 0.7520 & 72.94 \\\\  & & DiffRIR & 23.44 & 0.5841 & 0.2337 & 0.2579 & 0.7417 & 71.64 \\\\  & PASD & 24.90 & 0.6653 & **1.0893** & 0.2607 & 0.6466 & 71.39 \\\\  & SUPIR (ours) & 22.66 & 0.5763 & 0.2662 & **0.4738** & **0.8049** & **73.38** \\\\ \\hline \\hline \\multirow{5}{*}{Single-SR (\\(\\times\\))} & \\multicolumn{1}{c}{BSRAAN} & **22.62** & 0.5121 & 0.3523 & 0.2069 & 0.5836 & 67.04 \\\\  & Real-ESGAN & 21.79 & **0.5280** & 0.3276 & 0.2501 & 0.5349 & 63.80 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Single-SR (\\(\\times\\))} & \\multirow{2}{*}{Shuffle} & 21.72 & 0.4857 & 0.3118 & 0.3029 & 0.7131 & 71.24 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 21.86 & 0.4957 & 0.3106 & 0.2845 & 0.7080 & 70.26 \\\\ \\cline{1-1}  & & PASD & 21.97 & 0.5149 & **0.3044** & 0.2412 & 0.6402 & 70.20 \\\\ \\cline{1-1}  & & SUPIR (ours) & 20.68 & 0.4488 & 0.3249 & **0.4467** & **0.8099** & **73.16** \\\\ \\hline \\hline \\multirow{5}{*}{Misure-Blur (\\(\\times\\)2)} & \\multicolumn{1}{c}{BSRAAN} & **24.947** & **0.6621** & 0.2261 & 0.2127 & 0.5984 & 69.44 \\\\ \\cline{1-1}  & Real-ESGAN & 24.08 & 0.6496 & 0.2028 & 0.2857 & 0.5853 & 69.27 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 22.26 & 0.5721 & 0.2301 & 0.3248 & 0.7458 & 72.82 \\\\ \\cline{1-1}  & & DiffRIR & 23.28 & 0.5741 & 0.2395 & 0.2829 & 0.7055 & 71.22 \\\\ \\cline{1-1}  & & PASD & 24.85 & 0.6500 & **0.1952** & 0.2500 & 6.0335 & 71.07 \\\\ \\cline{1-1}  & SUPIR (ours) & 22.43 & 0.5626 & 0.2771 & **0.4757** & **0.8110** & **73.58** \\\\ \\hline \\hline \\multirow{5}{*}{Misure-Blur (\\(\\times\\)4)} & \\multicolumn{1}{c}{BSRAAN} & 17.74 & 0.83616 & 0.5699 & 0.1006 & 0.4166 & 51.25 \\\\ \\cline{1-1}  & Real-ESGAN & 24.46 & **0.5129** & 0.4636 & 0.1236 & 0.4536 & 52.23 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 20.88 & 0.4174 & 0.4668 & 0.2365 & 0.5833 & 63.54 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 22.08 & 0.4918 & **0.4978** & 0.2403 & 0.6415 & 65.97 \\\\ \\cline{1-1}  & & PASD & 21.29 & 0.4983 & 0.3624 & 0.5909 & 69.09 \\\\ \\cline{1-1}  & & PASD & 22.07 & 0.4982 & 0.3612 & **0.4951** & **0.5935** & 69.20 \\\\ \\cline{1-1}  & SUPIR (ours) & 20.77 & 0.4571 & 0.3945 & **0.4647** & **0.7840** & **73.736** \\\\ \\hline \\hline \\multirow{5}{*}{Misure-Blur (\\(\\times\\)2)} & \\multicolumn{1}{c}{BSRAAN} & **22.88** & **0.5397** & 0.3445 & 0.1383 & 0.5402 & 64.81 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 22.01 & 0.5322 & 0.3549 & 0.2115 & 0.5730 & 64.76 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 21.93 & 0.4744 & **0.3422** & 0.2024 & 0.7354 & 70.94 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 21.79 & 0.4895 & 0.3465 & 0.2261 & 0.7059 & 69.28 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 21.90 & 0.5118 & 0.4893 & 0.2597 & 0.6326 & 70.43 \\\\ \\cline{1-1}  & \\multirow{2}{*}{Shuffle} & 20.84 & 0.4660 & 0.3806 & **0.4688** & **0.5021** & **73.58** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Quantitative comparison. **Red** and **blue** colors represent the best and second best performance. \\(\\downarrow\\) represents the smaller the better, and for the others, the bigger the better.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l} \\hline \\hline \\multicolumn{1}{c}{\\multirow{2}{*}{Negative}} & \\multicolumn{1}{c}{P} & \\multirow{2}{*}{PSNR} & \\multicolumn{1}{c}{SSIM} & \\multicolumn{1}{c}{LPIPS\\(\\downarrow\\)} & \\multicolumn{1}{c}{ManlQA} & \\multicolumn{1}{c}{ClipQA} & \\multicolumn{1}{c}{MUSIQ} \\\\ \\cline{1-1}  & Samples & Positive & Negative & & & & & & \\\\ \\hline \\hline \\multirow{5}{*}{\\(\\checkmark\\)} &\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'Connector.We compare the proposed ZeroSFT connector with zero convolution [95]. Quantitative results are shown in Tab. 2c. Compared to ZeroSFT, zero convolution yields comparable performance on non-reference metrics and much lower full-reference performance. In Fig. 9, we find that the drop in non-reference metrics is caused by generating low-fidelity content. Therefore, for IR tasks, ZeroSFT ensures fidelity without losing the perceptual effect.\n' +
      '\n' +
      'Training data scaling.We trained our large-scale model on two smaller datasets for IR, DIV2K [3] and LSDIR [1]. The qualitative results are shown in Fig. 12, which clearly demonstrate the importance and necessity of training on large-scale high-quality data.\n' +
      '\n' +
      'Negative-quality samples and prompt.Tab. 2b shows some quantitative results under different settings. Here, we use positive words describing image quality as "positive prompt", and use negative quality words and the CFG methods described in Sec. 3.2 as negative prompt. It can be seen that adding positive prompts or negative prompts alone can improve the perceptual quality of the image. Using both of them simultaneously yields the best perceptual results. If negative samples are not included for training, these two prompts will not be able to improve the perceptual quality. Fig. 4 and Fig. 11(a) demonstrate the improvement in image quality brought by using negative prompts.\n' +
      '\n' +
      'Restoration-guided sampling method.The proposed restoration-guided sampling method is mainly controlled by the hyper-parameter \\(\\tau_{r}\\). The larger \\(\\tau_{r}\\) is, the fewer corrections are made to the generation at each step. The smaller \\(\\tau_{r}\\) is, the more generated content will be forced to be closer to the LQ image. Please refer to Fig. 13 for a qualitative comparison. When \\(\\tau_{r}=0.5\\), the image is blurry because its output is limited by the LQ image and cannot generate texture and details. When \\(\\tau_{r}=6\\), there is not much guidance during generation. The model generates a lot of texture that is not present in the LQ image, especially in flat area. Fig. 8(a) illustrates the quantitative results of restoration as a function of the variable \\(\\tau_{r}\\). As shown in Fig. 8(a), decreasing \\(\\tau_{r}\\) from 6 to 4 does not result in a significant decline in visual quality, while fidelity performance improves. As restoration guidance continues to strengthen, although PSNR continues to improve, the images gradually become blurry with loss of details, as depicted in Fig. 13. Therefore, we choose \\(\\tau_{r}=4\\) as the default parameter, as it doesn\'t significantly compromise image quality while effectively en\n' +
      '\n' +
      'Figure 11: Influences of text prompts. (a) Negative prompts lead to detailed and sharp restoration results. (b) Given a positive prompt with hallucinations, SUPIR avoids generating content absent in the LQ images. Zoom in for better view.\n' +
      '\n' +
      'Figure 12: Qualitative comparison for SUPIR training on datasets with different scales. Zoom in for better view.\n' +
      '\n' +
      'Figure 10: Qualitative comparison on real-world LQ images. SUPIR successfully recovers structured buildings and lifelike rivers. It also maintains the details existing in LQ, such as the horizontal planks in the beach chairs. Zoom in for better view.\n' +
      '\n' +
      'hancing fidelity.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'We propose SUPIR as a pioneering IR method, empowered by model scaling, dataset enrichment, and advanced design features, expanding the horizons of IR with enhanced perceptual quality and controlled textual prompts.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Lsdir dataset: A large scale dataset for image restoration. [https://data.vision.ee.ethz.ch/yawli/index.html](https://data.vision.ee.ethz.ch/yawli/index.html), 2023. Accessed: 2023-11-15.\n' +
      '* [2] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 4432-4441, 2019.\n' +
      '* [3] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In _The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops_, 2017.\n' +
      '* [4] David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, and Antonio Torralba. Semantic photo manipulation with a generative image prior. _arXiv preprint arXiv:2005.07727_, 2020.\n' +
      '* [5] Sefi Bell-Kligler, Assaf Shocher, and Michal Irani. Blind super-resolution kernel estimation using an internal-gan. _Advances in Neural Information Processing Systems_, 32, 2019.\n' +
      '* [6] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6228-6237, 2018.\n' +
      '* [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [8] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image super-resolution: A new benchmark and a new model. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3086-3095, 2019.\n' +
      '* [9] Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. Glean: Generative latent bank for large-factor image super-resolution. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 14245-14254, 2021.\n' +
      '* [10] Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo. Real-world blind super-resolution via feature matching with implicit high-resolution priors. In _Proceedings of the 30th ACM International Conference on Multimedia_, pages 1329-1338, 2022.\n' +
      '* [11] Haoyu Chen, Jinjin Gu, Yihao Liu, Salma Abdel Magid, Chao Dong, Qiong Wang, Hanspeter Pfister, and Lei Zhu. Masked image training for generalizable deep image denoising. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1692-1703, 2023.\n' +
      '* [12] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-\\(\\alpha\\): Fast training of diffusion transformer for photorealistic text-to-image synthesis. _arXiv preprint arXiv:2310.00426_, 2023.\n' +
      '* [13] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang, and Fisher Yu. Dual aggregation transformer for image super-resolution. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 12312-12321, 2023.\n' +
      '* [14] Zheng Chen, Yulun Zhang, Ding Liu, Bin Xia, Jinjin Gu, Linghe Kong, and Xin Yuan. Hierarchical integration diffusion model for realistic image deblurring. _Advances in Neural Information Processing Systems_, 2023.\n' +
      '* [15] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. _arXiv preprint arXiv:2108.02938_, 2021.\n' +
      '* [16] DeepFloyd. Deepfloyd inference framework. [https://www.deepfloyd.ai/deepfloyd-if](https://www.deepfloyd.ai/deepfloyd-if), 2023. Accessed: 2023-11-14.\n' +
      '* [17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.\n' +
      '* [18] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.\n' +
      '* [19] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. _IEEE transactions on pattern analysis and machine intelligence_, 38(2):295-307, 2015.\n' +
      '* [20] Chao Dong, Chen Change Loy, and Xiaoou Tang. Accelerating the super-resolution convolutional neural network. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14_, pages 391-407. Springer, 2016.\n' +
      '* [21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* [22] Yuchen Fan, Jiahui Yu, Yiqun Mei, Yulun Zhang, Yun Fu, Ding Liu, and Thomas S Huang. Neural sparse representation for image restoration. _Advances in Neural Information Processing Systems_, 33:15394-15404, 2020.\n' +
      '* [23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _Advances in neural information processing systems_, 27, 2014.\n' +
      '* [24] Jinjin Gu and Chao Dong. Interpreting super-resolution networks with local attribution maps. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9199-9208, 2021.\n' +
      '* [25] Jinjin Gu, Hannan Lu, Wangmeng Zuo, and Chao Dong. Blind super-resolution with iterative kernel correction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1604-1613, 2019.\n' +
      '* [26] Jinjin Gu, Haoming Cai, Haoyu Chen, Xiaoxing Ye, Jimmy Ren, and Chao Dong. Pipal: a large-scale image quality assessment dataset for perceptual image restoration. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI 16_, pages 633-651, 2020.\n' +
      '* [27] Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code gan prior. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 3012-3021, 2020.\n' +
      '* [28] Jinjin Gu, Haoming Cai, Chao Dong, Jimmy S Ren, Radu Timofte, Yuan Gong, Shanshan Lao, Shuwei Shi, Jiahao Wang, Sidi Yang, et al. Ntire 2022 challenge on perceptual image quality assessment. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 951-967, 2022.\n' +
      '* [29] Jinjin Gu, Xianzheng Ma, Xiangtao Kong, Yu Qiao, and Chao Dong. Networks are slacking off: Understanding generalization problem in image deraining. _Advances in Neural Information Processing Systems_, 2023.\n' +
      '* [30] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [32] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [33] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven neuron pruning approach towards efficient deep architectures. _arXiv preprint arXiv:1607.03250_, 2016.\n' +
      '* [34] Yan Huang, Shang Li, Liang Wang, Tieniu Tan, et al. Unfolding the alternating optimization for blind super resolution. _Advances in Neural Information Processing Systems_, 33:5632-5643, 2020.\n' +
      '* [35] Zheng Hui, Jie Li, Xiumei Wang, and Xinbo Gao. Learning the non-differentiable optimization for blind super-resolution. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2093-2102, 2021.\n' +
      '* [36] Younghyun Jo, Sejong Yang, and Seon Joo Kim. Srflow-da: Super-resolution using normalizing flow with deep convolutional block. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 364-372, 2021.\n' +
      '* [37] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10124-10134, 2023.\n' +
      '* [38] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* [39] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. _arXiv preprint arXiv:1710.10196_, 2017.\n' +
      '* [40] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4401-4410, 2019.\n' +
      '* [41] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35:26565-26577, 2022.\n' +
      '* [42] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. _Advances in Neural Information Processing Systems_, 35:23593-23606, 2022.\n' +
      '* [43] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5148-5157, 2021.\n' +
      '* [44] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* [45] Xiangtao Kong, Xin Liu, Jinjin Gu, Yu Qiao, and Chao Dong. Refhash dropout in image super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6002-6012, 2022.\n' +
      '* [46] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1833-1844, 2021.\n' +
      '* [47] Jingyun Liang, Kai Zhang, Shuhang Gu, Luc Van Gool, and Radu Timofte. Flow-based kernel prior with application to blind super-resolution. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10601-10610, 2021.\n' +
      '* [48] Jie Liang, Hui Zeng, and Lei Zhang. Efficient and degradation-adaptive network for real-world image super-resolution. In _European Conference on Computer Vision_, pages 574-591. Springer, 2022.\n' +
      '* [49] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei, Bo Dai, Wanli Ouyang, Yu Qiao, and Chao Dong. Diff-bir: Towards blind image restoration with generative diffusion prior. _arXiv preprint arXiv:2308.15070_, 2023.\n' +
      '* [50] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong. Blind image super-resolution: A survey and beyond. _IEEE transactions on pattern analysis and machine intelligence_, 45(5):5461-5480, 2022.\n' +
      '** [51] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.\n' +
      '* [52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _NeurIPS_, 2023.\n' +
      '* [53] Yihao Liu, Anran Liu, Jinjin Gu, Zhipeng Zhang, Wenhao Wu, Yu Qiao, and Chao Dong. Discovering distinctive" semantics in super-resolution networks. _arXiv preprint arXiv:2108.00406_, 2021.\n' +
      '* [54] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* [55] A Lugmayr, M Danelljan, L Van Gool, and R Timofte. Learning the super-resolution space with normalizing flow. _ECCV, Sydney_, 2020.\n' +
      '* [56] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. _arXiv preprint arXiv:2108.01073_, 2021.\n' +
      '* [57] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-supervised photo upsampling via latent space exploration of generative models. In _Proceedings of the ieee/cvf conference on computer vision and pattern recognition_, pages 2437-2445, 2020.\n' +
      '* [58] Tomer Michaeli and Michal Irani. Nonparametric blind super-resolution. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 945-952, 2013.\n' +
      '* [59] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongqing Qi, Ying Shan, and Xiaohu Qie. Tzi-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_, 2023.\n' +
      '* [60] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3883-3891, 2017.\n' +
      '* [61] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.\n' +
      '* [62] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep generative prior for versatile image restoration and manipulation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(11):7474-7489, 2021.\n' +
      '* [63] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SdxI: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [64] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. _arXiv preprint arXiv:1511.06434_, 2015.\n' +
      '* [65] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.\n' +
      '* [66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* [67] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '* [68] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* [69] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.\n' +
      '* [70] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* [71] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '* [72] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Jiaya Jia. Scale-recurrent network for deep image deblurring. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 8174-8182, 2018.\n' +
      '* [73] InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. [https://github.com/InternLM/InternLM](https://github.com/InternLM/InternLM), 2023.\n' +
      '* [74] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [75] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [76] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 2555-2563, 2023.\n' +
      '* [77] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. _arXiv preprint arXiv:2305.07015_, 2023.\n' +
      '* [78] Longguang Wang, Yingqian Wang, Xiaoyu Dong, Qingyu Xu, Jungang Yang, Wei An, and Yulan Guo. Unsupervised degradation representation learning for blind super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10581-10590, 2021.\n' +
      '* [79] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Recovering realistic texture in image super-resolution by deep spatial feature transform. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 606-615, 2018.\n' +
      '* [80] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. Towards real-world blind face restoration with generative facial prior. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9168-9178, 2021.\n' +
      '* [81] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1905-1914, 2021.\n' +
      '* [82] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. _arXiv preprint arXiv:2212.00490_, 2022.\n' +
      '* [83] Pengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, Qixiang Ye, Wangmeng Zuo, and Liang Lin. Component divide-and-conquer for real-world image super-resolution. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VIII 16_, pages 101-117. Springer, 2020.\n' +
      '* [84] Yuxin Wu and Kaiming He. Group normalization. In _Proceedings of the European conference on computer vision (ECCV)_, pages 3-19, 2018.\n' +
      '* [85] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image generation via large mixture of diffusion paths. _arXiv preprint arXiv:2305.18295_, 2023.\n' +
      '* [86] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1191-1200, 2022.\n' +
      '* [87] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. Gan prior embedded network for blind face restoration in the wild. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 672-681, 2021.\n' +
      '* [88] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. Pixel-aware stable diffusion for realistic image super-resolution and personalized stylization. _arXiv preprint arXiv:2308.14469_, 2023.\n' +
      '* [89] Jiale Zhang, Yulun Zhang, Jinjin Gu, Yongbing Zhang, Linghe Kong, and Xin Yuan. Accurate image restoration with attention retractable transformer. In _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* [90] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. _IEEE transactions on image processing_, 26(7):3142-3155, 2017.\n' +
      '* [91] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep cnn denoiser prior for image restoration. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3929-3938, 2017.\n' +
      '* [92] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-based image denoising. _IEEE Transactions on Image Processing_, 27(9):4608-4622, 2018.\n' +
      '* [93] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation model for deep blind image super-resolution. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4791-4800, 2021.\n' +
      '* [94] Kai Zhang, Yawei Li, Jingyun Liang, Jiezhang Cao, Yulun Zhang, Hao Tang, Deng-Ping Fan, Radu Timofte, and Luc Van Gool. Practical blind image denoising via swin-conv-unet and data synthesis. _Machine Intelligence Research_, pages 1-14, 2023.\n' +
      '* [95] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3836-3847, 2023.\n' +
      '* [96] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 586-595, 2018.\n' +
      '* [97] Ruofan Zhang, Jinjin Gu, Haoyu Chen, Chao Dong, Yulun Zhang, and Wenming Yang. Crafting training degradation distribution for the accuracy-generalization trade-off in real-world super-resolution. _International Conference on Machine Learning (ICML)_, 2023.\n' +
      '* [98] Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and Yun Fu. Residual non-local attention networks for image restoration. In _International Conference on Learning Representations (ICLR)_, 2019.\n' +
      '* [99] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image restoration. _IEEE transactions on pattern analysis and machine intelligence_, 43(7):2480-2495, 2020.\n' +
      '* [100] Yi Zhang, Xiaoyu Shi, Dasono Li, Xiaogang Wang, Jian Wang, and Hongsheng Li. A unified conditional framework for diffusion-based image restoration. _arXiv preprint arXiv:2305.20049_, 2023.\n' +
      '* [101] Yang Zhao, Yu-Chuan Su, Chun-Te Chu, Yandong Li, Marius Renn, Yukun Zhu, Changyou Chen, and Xuhui Jia. Rethinking deep face restoration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7652-7661, 2022.\n' +
      '* [102] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy. Towards robust blind face restoration with codebook lookup transformer. _Advances in Neural Information Processing Systems_, 35:30599-30611, 2022.\n' +
      '* [103] Jiapeng Zhu, Deli Zhao, Bo Zhang, and Bolei Zhou. Disentangled inference for gans with latently invertible autoencoder. _International Journal of Computer Vision_, 130(5):1259-1276, 2022.\n' +
      '\n' +
      '## Appendix A Discussions\n' +
      '\n' +
      '### Degradation-Robust Encoder\n' +
      '\n' +
      'As shown in Fig. 2 of the main text, a degradation-robust encoder is trained and deployed prior to feeding the low-quality input into the adaptor. We conduct experiments using synthetic data to demonstrate the effectiveness of the proposed degradation-robust encoder. In Fig. 14, we show the results of using the same decoder to decode the latent representations from different encoders. It can be seen that the original encoder has no ability to resist degradation and its decoded images still contain noise and blur. The proposed degradation-robust encoder can reduce the impact of degradation, which further prevents generative models from misunderstanding artifacts as image content [49].\n' +
      '\n' +
      '### LLaVA Annotation\n' +
      '\n' +
      'Our diffusion model is capable of accepting textual prompts during the restoration process. The prompt strategy we employ consists of two components: one component is automatically annotated by LLaVA-v1.5-13B [51], and the other is a standardized default positive quality prompt. The fixed portion of the prompt strategy provides a positive description of quality, including words like "cinematic, High Contrast, highly detailed, unreal engine, taken using a Canon EOS R camera, hyper detailed photo-realistic maximum detail, 32k, Color Grading, ultra HD, extreme meticulous detailing, skin pore detailing, hyper sharpness, perfect without deformations, Unreal Engine 5, 4k render". For the LLaVA component, we use the command "Describe this image and its style in a very detailed manner" to generate detailed image captions, as exemplified in Fig. 18. While occasional inaccuracies may arise, LLaVA-v1.5-13B generally captures the essence of the low-quality input with notable precision. Using the reconstructed version of the input proves effective in correcting these inaccuracies, allowing LLaVA to provide an accurate description of the majority of the image\'s content. Additionally, SUPIR is effective in mitigating the impact of potential hallucination prompts, as detailed in [52].\n' +
      '\n' +
      '### Limitations of Negative Prompt\n' +
      '\n' +
      'Figure 23 presents evidence that the use of negative quality prompts [30] substantially improves the image quality of restored images. However, as observed in Fig. 15, the negative prompt may introduce artifacts when the restoration target lacks clear semantic definition. This issue likely stems from a misalignment between low-quality inputs and language concepts.\n' +
      '\n' +
      '### Negative Samples Generation\n' +
      '\n' +
      'While negative prompts are highly effective in enhancing quality, the lack of negative-quality samples and prompts in\n' +
      '\n' +
      'Figure 14: The effectiveness of the degradation-robust encoder (DR Encoder) is demonstrated by the results, which are achieved by initially encoding with various encoders and subsequently decoding. This process effectively reduces the degradations in low-quality inputs before they are introduced into the diffusion models.\n' +
      '\n' +
      'Figure 15: Negative prompt causes artifacts when low-quality inputs do not have clear semantics.\n' +
      '\n' +
      'the training data results in the fine-tuned SUPIR\'s inability to comprehend these prompts effectively. To address this problem, in Sec. 3.2 of the main text, we introduce a method to distill negative concepts from the SDXL model. The process for generating negative samples is illustrated in Fig. 19. Direct sampling of negative samples through a text-to-image approach often results in meaningless images. To address this issue, we also utilize training samples from our dataset as source images. We create negative samples in an image-to-image manner as proposed in [56], with a strength setting of \\(0.5\\).\n' +
      '\n' +
      '## Appendix B More Visual Results\n' +
      '\n' +
      'We provide more results in this section. Fig. 16 presents additional cases where full-reference metrics do not align with human evaluation. In Fig. 17, we show that using negative-quality prompt without including negative samples in training may cause artifacts. In Figs. 20 to 22, we provide more visual comparisons with other methods. Plenty of examples prove the strong restoration ability of SUPIR and the most realistic of restored images. More examples of controllable image restoration with textual prompts can be found in Fig. 23.\n' +
      '\n' +
      'Figure 16: Additional samples highlight the misalignment between metric evaluations and human assessments. While SUPIR produces images with high-fidelity textures, it tends to receive lower scores in metric evaluations.\n' +
      '\n' +
      'Figure 17: More visual results for Fig. 4 of the main text. CFG introduces artifacts if we do not include negative-quality samples in training. Adding negative-quality samples allows further quality enhancement through CFG.\n' +
      '\n' +
      'Figure 18: Snapshots showcasing LLaVA annotations demonstrate that LLaVA accurately predicts most content even with low-quality inputs. Please zoom in for a more detailed view.\n' +
      '\n' +
      'Figure 19: Pipeline of negative sample generation. (a) Sampling in a noise-to-image approach leads to meaningless outputs. (b) We synthetic negative samples from high quality images. Zoom in for better view.\n' +
      '\n' +
      'Figure 20: Qualitative comparison with different methods. Our method can accurately restore the texture and details of the corresponding object under challenging degradation. Zoom in for better view.\n' +
      '\n' +
      'Figure 21: Qualitative comparison with different methods. Our method can accurately restore the texture and details of the corresponding object under challenging degradation. Zoom in for better view.\n' +
      '\n' +
      'Figure 22: Qualitative comparison with different methods. Our method can accurately restore the texture and details of the corresponding object under challenging degradation. Zoom in for better view.\n' +
      '\n' +
      'Figure 23: More visual results of the text prompts influences. (a) and (b) show the examples of positive prompts and negative prompts, respectively. Zoom in for better view.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>