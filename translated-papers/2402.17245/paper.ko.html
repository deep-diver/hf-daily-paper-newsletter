<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 연구에서는 텍스트-이미지 생성 모델에서 최첨단 미적 품질을 달성하기 위한 세 가지 통찰력을 공유한다. 모델 개선을 위한 세 가지 중요한 측면, 즉 색상 및 대비 향상, 다중 종횡비에 걸친 생성 개선, 인간 중심 미세 세부 사항 개선에 중점을 둔다. 먼저, 확산 모델의 훈련에 있어서 잡음 스케줄의 중요성을 파헤치고, 그것이 사실성과 시각적 충실도에 지대한 영향을 미친다는 것을 증명한다. 둘째, 균형 잡힌 버킷 데이터 세트의 준비의 중요성을 강조하면서 이미지 생성에서 다양한 종횡비를 수용하는 문제를 해결한다. 마지막으로, 생성된 이미지가 인간의 지각적 기대와 공명하도록 모델 출력을 인간의 선호도와 정렬하는 중요한 역할을 조사한다. 광범위한 분석과 실험을 통해 플레이그라운드 v2.5는 SDXL[28] 및 플레이그라운드 v2[20]와 같이 널리 사용되는 오픈 소스 모델과 DALL-E 3[2] 및 Midjourney v5.2와 같은 폐쇄 소스 상용 시스템을 능가하는 다양한 조건 및 종횡비 하에서 미적 품질 측면에서 최첨단 성능을 보여준다. 본 모델은 오픈 소스이며, 플레이그라운드 v2.5의 개발은 확산 기반 이미지 생성 모델의 미적 품질을 높이는 것을 목표로 하는 연구자들에게 귀중한 지침을 제공한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '이전에 생성적 적대 네트워크(GAN)의 지배적인 프레임워크에 대한 성능과 비교하여 이미지넷을 사용한 더 나은 이미지 모델링 성능[12, 31, 6]의 성공 이후 확산 기반 생성 모델에서 큰 진전이 있었다. SDXL[28]과 같은 오픈 소스 모델은 텍스트에서 이미지 사전 훈련 데이터 세트[30]와 잠재 UNet[6] 아키텍처를 확장함으로써 잠재 확산 모델(LDM)[29] 위에 구축되었다. 반면, PixArt-alpha[3]은 잠재 백본으로 Diffusion Transformer(DiT)[27]을 탐색하여 보다 우수한 훈련 효율과 화질을 보인다. 최근 우리가 개발한 오픈소스 모델인 플레이그라운드 v2[20]는 훈련 레시피와 미적 품질에 중점을 두어 SDXL[28]에 비해 사용자 선호도가 \\(2.5\\times\\) 더 높다.\n' +
      '\n' +
      '놀이터 v2[20]는 2023년 12월에 오픈소싱되었으며 오픈 소스 및 연구 커뮤니티가 우리의 작업을 차지하고 참조하는 것을 보고 기뻤다. 특히, 플레이그라운드 v2는 허깅페이스에서 불과 한 달 동안 135,000건 이상의 다운로드를 축적했으며, 우리의 작업은 스테이블 캐스케이드[1]와 같은 최첨단 이미지 모델에 대한 최근 논문에서 인용되었다. 놀이터 v2[20]에 이어 이 작업의 기본 모델 아키텍처를 변경하지 않기로 선택했으며 대신 훈련 레시피를 분석하고 개선하고 모델의 미적 품질을 새로운 수준으로 밀어붙이는 데 중점을 두었다.\n' +
      '\n' +
      '색상 및 콘트라스트 향상(sec. 2.1), 다중 종횡비에 걸친 생성 개선(sec. 2.2), 인간 중심 미세 세부 사항 개선(sec. 2.3)의 세 가지 중요한 문제에 초점을 맞춘다. 보다 일반적으로 모델의 기능을 개선하여 보다 사실적이고 시각적으로 설득력 있는 출력을 생성하는 것을 목표로 합니다. 향상된 기능의 효과를 평가하기 위해 광범위한 사용자 연구를 수행하고 이전 최신 모델(초 3)에 대해 모델을 벤치마킹했다. 또한 10개의 고유 카테고리에 대한 모델의 성능을 평가하기 위한 새로운 자동 평가 벤치마크 _MJHQ-30K_(sec. 3.5)를 제안한다. 인간 선호도에 대한 모델의 출력을 평가할 때 플레이그라운드 v2.5가 중저니 5.2, DALL-E 3[2], 플레이그라운드 v2[20], PIXART-\\(\\알파\\)[3] 및 SDXL[28]을 포함한 최신 모델을 능가한다고 보고하게 되어 기쁩니다(그림 10). 자세한 내용은 sec. 3.2를 참조하십시오. Playground v2.5는 텍스트-이미지 생성 모델의 공간에서 전임자의 성능을 능가하고 선도적인 경쟁자로 자리매김하기 위해 노력한다.\n' +
      '\n' +
      '연구팀이 쉽게 사용할 수 있는 라이센스2가 있는 허깅페이스1에서 플레이그라운드 v2.5에 대한 가중치를 오픈소스화한다. 또한 확산 모델을 사용하기 위한 두 가지 인기 커뮤니티 도구인 A1111 및 ComfyUI에서 모델을 사용하기 위한 확장을 제공할 것이다. 연구 및 오픈 소스 커뮤니티의 혜택을 얼마나 받았는지 감안할 때 플레이그라운드 v2.5에 대한 작업의 여러 측면을 커뮤니티에서 사용할 수 있도록 하는 것이 중요하다.\n' +
      '\n' +
      '각주 1: [https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic)\n' +
      '\n' +
      '각주 2: [https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic/blob/main/LICENSE.md](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic/blob/main/LICENSE.md) 그림 2: 생동감 있는 색상과 대비로 이미지를 생성함에 있어 SDXL과 Playground v2.5를 ** 비교하는 것.**\n' +
      '\n' +
      '## 2 Methods\n' +
      '\n' +
      '### 향상된 색상과 대비\n' +
      '\n' +
      '잠재 확산 모델은 SD1.5의 방출 이후 높은 색상 대비와 생생한 색상 범위를 갖는 이미지를 생성하는데 어려움을 겪고 있다. 이는 알려진 제한 사항[21; 4; 13]이다. 예를 들어, SDXL은 순수한 블랙 이미지 또는 순수한 화이트 이미지를 생성할 수 없고, 피사체를 솔리드 백그라운드 상에 배치하지 못한다(도 2의 (a) 참조).\n' +
      '\n' +
      '이 문제는 [21]에서 지적한 바와 같이 확산 과정의 잡음 스케줄링에서 비롯된다. 안정 확산의 신호 대 잡음비[17]는 이산 잡음 레벨이 최대값에 도달하더라도 너무 높다. 몇몇 작품들이 이 결함을 고치려고 시도했다. Guttenberg와 CrossLabs[8]는 오프셋 노이즈를 제안한다. Lin et al. [21]은 마지막 잡음 제거 단계가 순수한 가우시안 잡음임을 보장하기 위해 Zero Terminal SNR을 제안한다. SDXL[28]은 Playground v2와 같이 훈련의 마지막 단계에서 오프셋 노이즈를 추가하는 전략을 채택한다. 그러나 그림 2(b)에서 볼 수 있듯이 SDXL은 여전히 음소거된 색상과 대비를 나타낸다.\n' +
      '\n' +
      '도 3: **Playground v2[20] 및 v2.5를 보다 복잡한 프롬프트와 색상 및 대비에 대해 비교하는 것.** 상단 행은 Playground v2이고, 하단 행은 Playground v2.5이다. v2에 비해, v2.5는 색상 및 대비, 및 스타일 관련 프롬프트를 따르는 능력을 극적으로 향상시킨다.\n' +
      '\n' +
      '도 4: **인물 종횡비의 정성적 비교.** 종횡비 3:4(\\(876\\times 1168\\)), 상단은 SDXL, 하단은 Playground v2.5. 본 모델은 원하는 종횡비에 맞는 구도로 이미지를 생성할 수 있다\n' +
      '\n' +
      '놀이터 v2.5의 경우 이 문제를 극적으로 개선하는 것을 목표로 했다. 우리는 이미지에서 선명한 색상과 대비를 달성하고 순수한 색상의 배경을 생성할 수 있기를 원했다. 이를 위해 우리는 보다 원칙적인 접근법을 취했으며 Karras 등이 제안한 EDM 프레임워크를 사용하여 처음부터 모델을 훈련시켰다[14].\n' +
      '\n' +
      'EDM은 (1) Zero Terminal SNR과 마찬가지로, EDM 잡음 스케줄은 최종 "timestep"에 대해 거의 0에 가까운 신호 대 잡음비를 나타낸다. 이렇게 하면 오프셋 노이즈의 필요성을 제거하고 음소거된 색상을 수정합니다. (2) EDM은 UNet의 사전 조건화뿐만 아니라 훈련 및 샘플링 프로세스를 설계하기 위한 첫 번째 원칙 접근법을 취한다. 이를 통해 EDM 저자는 더 나은 이미지 품질과 더 빠른 모델 수렴을 유도하는 명확한 설계 선택을 할 수 있다.\n' +
      '\n' +
      '우리는 또한 후게붐 등[13]에 영감을 받아 고해상도 이미지에 대해 훈련할 때 잡음 스케줄을 전반적으로 더 시끄러운 것으로 왜곡했다.\n' +
      '\n' +
      '그림 3에서 플레이그라운드 v2.5와 플레이그라운드 v2의 질적 비교를 보여주며, 후자는 오프셋 노이즈와 DDPM[12] 노이즈 스케줄을 사용한다. 첫 번째 열에서 플레이그라운드 v2.5는 색상 범위가 향상된 생생한 초상화를 생성할 수 있으며 v2.5가 순수한 검은색 배경을 생성할 수 있는 더 나은 프롬프트 이미지 정렬을 나타낸다.\n' +
      '\n' +
      '여러 측면 비율에 걸친### 생성\n' +
      '\n' +
      '다양한 상이한 종횡비의 이미지를 생성하는 능력은 텍스트 대 이미지 모델의 실제 응용에서 중요한 특징이다. 그러나 이러한 모델에 대한 일반적인 사전 훈련 절차[29; 28]는 무작위 또는 중앙 절단으로 초기 단계에서 정사각형 이미지에 대해서만 훈련하는 것으로 시작한다. 이 기술은 ImageNet[6; 23; 14]에서 훈련된 조건부 생성 모델로부터의 표준 관행이다.\n' +
      '\n' +
      '도 5: **경관 종횡비의 정성적 비교.** 종횡비 4:3(\\(1168\\times 876\\)), 상단은 SDXL, 하단은 Playground v2.5이다. 본 모델은 극단적인 종횡비 하에서 지속적으로 프롬프트를 따르는 콘텐츠를 생성할 수 있는 반면, SDXL은 때때로 프롬프트를 따르지 않거나 넓은 종횡비 하에서 다수의 객체를 생성할 수 있다.\n' +
      '\n' +
      '이론적으로 이것은 문제가 되어서는 안 된다. 대부분의 컨볼루션 레이어로 구성된 SDXL[29; 28]과 같은 확산 모델은 컨볼루션 신경망(CNN)을 모방하여 특정 해상도에 대해 훈련되지 않았더라도 추론 시간에 모든 입력 해상도로 작동해야 한다. 이는 CNN의 전이 불변 [19; 9] 속성 때문이다. 불행히도, 실제로 확산 모델은 NovelAI[24]에 의해 지적된 바와 같이 정사각형 이미지에 대해서만 훈련될 때 다른 종횡비에 잘 일반화되지 않는다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 NovelAI는 유사한 종횡비를 가진 이미지가 동일한 순방향 패스에서 함께 버킷되는 버킷 샘플링 작업을 제안한다. SDXL[28]은 이 버킷팅 전략을 채택했으며 소스 및 목표 이미지 크기를 지정하기 위해 추가 컨디셔닝을 추가했다.\n' +
      '\n' +
      'SDXL의 컨디셔닝 전략은 모델이 이미지의 주제를 다른 종횡비 하에서 중심에 배치하는 것을 배우도록 강요했다. 그러나, SDXL의 데이터 세트에서 종횡비 버킷[28]의 불균형 분포, 즉 데이터 세트의 이미지의 대부분은 정사각형이기 때문에, SDXL은 또한 그 컨디셔닝에서 특정 종횡비의 편향을 학습했다. 또한, 비정방형 종횡비로 생성된 이미지는 통상적으로 정방형 이미지보다 훨씬 낮은 품질을 나타낸다.\n' +
      '\n' +
      '플레이그라운드 v2.5에서 명시적인 목표 중 하나는 사용자에게 이것이 고품질 생산 등급 모델에 중요한 요소라는 것을 배웠듯이 모델이 여러 종횡비에서 고품질 이미지를 안정적으로 생산하도록 하는 것이었다. SDXL과 유사한 버킷 전략을 따르는 동안 다양한 종횡비에 걸쳐 보다 균형 잡힌 버킷 샘플링 전략을 보장하기 위해 데이터 파이프라인을 신중하게 제작했다. 우리의 전략은 재앙적인 망각을 피하고 모델이 한 비율 또는 다른 비율에 치우치지 않도록 돕는다.\n' +
      '\n' +
      '그림 4와 그림 5는 각각 세로 및 가로 종횡비에 걸쳐 SDXL과 플레이그라운드 v2.5의 질적 비교를 보여준다. 본 논문에서 제안하는 모델은 다양한 종횡비 하에서 다양한 객체 생성이나 잘못된 구도 생성과 같은 오류 없이 고품질의 영상을 생성할 수 있다.\n' +
      '\n' +
      '### 인간 선호도 정렬\n' +
      '\n' +
      '인간은 특히 손, 얼굴, 비틀기와 같은 인간의 특징에 대한 시각적 오류에 민감하다. 완벽한 조명, 구성, 스타일을 가진 이미지는 인간의 얼굴이 기형적이거나 신체의 구조가 일그러질 경우 저품질로 투표될 가능성이 높다.\n' +
      '\n' +
      '언어나 이미지 모두 생성 모델은 환각을 일으키기 쉽다. 이미지 모델에서, 이것은 기형화된 인간 특징으로 나타날 수 있다. 환각에 대한 여러 가지 이유가 있지만 한 가지 분명한 설명은 잘못된 훈련 목표이다. 생성 모델은 인간의 선호도를 극대화하기보다는 데이터의 로그 우도를 최대화하기 위해 훈련된다. LLM에서 미리 훈련된 생성 모델을 인간의 선호도와 정렬하기 위한 일반적인 전략을 감독 미세 조정 또는 SFT라고 한다. 간단히 말해서, SFT는 작지만 매우 고품질의 데이터 세트로 미리 훈련된 기본 모델을 미세 조정한다. 이 간단한 기술은 종종 RLHF[33]와 같은 보다 복잡한 접근법을 능가한다. 그러나 다운스트림 작업에서 성능을 최대화하기 위해 서로 다른 소스에서 SFT 정렬 데이터 세트를 가장 잘 큐레이션하는 방법에 대한 질문은 진행 중인 연구 문제로 남아 있다[32].\n' +
      '\n' +
      '플레이그라운드 v2.5의 목표 중 하나는 미드저니와 같은 폐쇄 소스 모델에 비해 오픈 소스 확산 모델에 대한 일반적인 비판인 인간 특징의 시각적 오류 가능성을 줄이는 것이었다. Emu[5]는 텍스트 대 이미지 생성 모델에 대해 SFT와 유사한 정렬 전략을 소개한다. Emu에서 영감을 받아 사용자 등급을 통해 여러 소스에서 고품질 데이터 세트를 자동으로 큐레이션할 수 있는 시스템을 개발했습니다. 그런 다음 최적의 데이터 세트 후보를 선택하기 위해 반복적 인간-인-루프[26; 22] 훈련 접근법을 취했다. 우리는 [33]과 유사하게 고정된 프롬프트 세트에서 생성된 이미지 그리드를 보고 정렬된 모델을 비교하여 경험적 평가에 의해 훈련 진행을 모니터링했다.\n' +
      '\n' +
      '우리의 새로운 정렬 전략은 적어도 네 가지 중요한 인간 중심 범주에서 SDXL보다 탁월할 수 있게 한다.\n' +
      '\n' +
      '* 얼굴 상세도, 선명도 및 생동감\n' +
      '* 눈 모양 및 시선\n' +
      '* 모발 조직감\n' +
      '* 전체 조명, 색상, 채도 및 피사계 심도\n' +
      '\n' +
      '우리는 제품의 사용 패턴과 사용자 피드백을 기반으로 이러한 범주에 초점을 맞추기로 결정했습니다.\n' +
      '\n' +
      '도 6: **인간 미적 선호도 정렬 비교 SDXL.** 상단 행은 SDXL, 하단 행은 Playground v2.5. 본 모델은 더 나은 인간 중심 얼굴 세부 사항, 전체 조명, 색상 및 필드 깊이를 생성할 수 있다.\n' +
      '\n' +
      '그림 6에서는 플레이그라운드 v2.5를 사용하여 생성된 이미지와 SDXL을 사용하여 생성된 이미지 간의 미세한 세부 사항의 차이에 대한 몇 가지 예를 보여준다. 그림 7에서 우리는 인간 중심 이미지를 생성할 때 모델을 다른 SoTA 방법과 비교한다.\n' +
      '\n' +
      '## 3 Evaluations\n' +
      '\n' +
      '#### : 사용자 스터디 인터페이스\n' +
      '\n' +
      '우리는 궁극적으로 수십만 명의 사용자가 사용할 모델을 구축하기 때문에 모델 출력에 대한 선호도를 이해하는 것이 중요합니다. 이를 위해, 우리는 제품 내에서 직접 사용자 연구를 수행한다(도 8 참조). 우리는 이것이 선호도 메트릭을 수집하는 최상의 컨텍스트라고 믿고 모델이 최종 사용자에게 가치 있는 것을 실제로 제공하는지 여부에 대한 가장 가혹한 테스트를 제공한다.\n' +
      '\n' +
      '주어진 사용자 연구를 위해 두 모델에서 고정된 프롬프트 집합과 샘플 이미지를 선택한다. 그런 다음 사용자에게 (어떤 모델이 어떤 이미지에 해당되는지 보여주지 않고) 프롬프트와 함께 이미지 쌍을 보여주고 미적 선호도와 같은 일부 속성에 따라 최상의 이미지를 선택하도록 요청한다. 단일 사용자의 등급은 편향되기 쉬우므로 각 이미지 쌍을 최소 7명의 고유한 사용자에게 보여줍니다. 편향을 더 줄이기 위해, 이미지 쌍은 그 출력이 적어도 하나 이상 선호되는 경우 주어진 모델에 대해서만 "윈"됩니다.\n' +
      '\n' +
      '도 8: 우리 제품에서 사용자에게 보여지는 이미지 쌍의 일례.\n' +
      '\n' +
      '그림 7: 방법 간의 **정성적 비교.** 맨 위 행에 대한 프롬프트: "입안에 건조함을 느끼는 사람", 맨 아래 행은 "보석상의 손, 각각 작은 원석을 잡고 맞춤 링에 맞게 완벽하게 정렬됨"이다. 본 모델은 생동감 있는 표정, 사람의 치아, 눈, 표정, 올바른 손을 생성할 수 있다.\n' +
      '\n' +
      '2표 마진. 1표 차익은 동점으로 간주된다. 마지막으로 각 사용자 스터디에 수천 명의 고유한 사용자를 포함시킵니다. 이 보고서에서 언급된 모든 사용자 연구는 이 인터페이스를 통해 수행된다.\n' +
      '\n' +
      '우리는 플레이그라운드 v2.5를 사용하여 개선하고자 하는 특정 영역, 즉 다중 종횡비와 인간 선호도 정렬에 걸친 생성과 전반적인 미적 선호도를 측정하기 위한 연구를 수행했다.\n' +
      '\n' +
      '### 다른 SoTA 모델에 대한 전반적인 심미적 선호도\n' +
      '\n' +
      '우리는 Playground v2.5를 다른 최신 모델과 비교하기 위해 Internal-1K라는 프롬프트 세트를 사용했다. Internal-1K는 Playground.com의 실제 사용자 프롬프트에서 수집된 프롬프트 집합으로 실제 사용자의 프롬프트 스타일을 대표한다. 우리는 수천 명의 사용자에게 이미지 쌍을 보여주었으며, 특히 이 연구에 대한 미적 선호도에 중점을 두었다. 이것은 플레이그라운드 v2[20]의 이전 릴리스와 동일한 연구 설정이다. 참고로, 우리의 이전 연구는 플레이그라운드 v2에서 생성된 이미지가 SDXL에서 생성된 이미지보다 2.5배 더 선호된다는 것을 보여주었다. 우리는 플레이그라운드 v2.5를 위해 이것을 능가하는 것을 목표로 했고 성공했습니다: v2.5는 SDXL보다 4.8배 선호됩니다.\n' +
      '\n' +
      '도. 도 10은 공개적으로 이용 가능한 다양한 텍스트 대 이미지 모델에 대한 우리의 결과를 보여준다. 전반적으로 플레이그라운드 v2.5는 플레이그라운드 v2[20]뿐만 아니라 SDXL[28] 및 PIXART-\\(\\alpha\\)[3]의 최신 오픈 소스 모델을 극적으로 능가한다. Playground v2.5와 SDXL의 성능 차이가 너무 커서 최첨단 폐쇄 소스에도 테스트했습니다.\n' +
      '\n' +
      '그림 10: SoTA 방법에 대한 **사용자 연구.** 공개적으로 사용 가능한 다양한 텍스트 대 이미지 모델에 대해 플레이그라운드 v2.5의 인간 미적 선호도 메트릭을 보고한다. Playground v2.5는 Midjourney 5.2, DALL-E 3[2], Playground v2[20], PIXART-\\(\\alpha\\)[3], SDXL[28]을 능가한다.\n' +
      '\n' +
      '그림 9: **방법 간의 보다 질적 비교.** 맨 위 행에 대한 프롬프트: "흐릿한 풍경, 남성 클로즈업 사진, 1800년대, 티셔츠 차림", 맨 아래 행: "돼지 머리가 스트리트웨어 패션 옷을 입은 사람, 스케이트보드 타기"\n' +
      '\n' +
      'DALL-E 3[2] 및 Midjourney 5.2와 같은 모델이 여전히 미적 품질에서 플레이그라운드 v2.5가 이러한 모델보다 우수하다는 것을 발견했다.\n' +
      '\n' +
      '다중 종횡비에 따른 생성의### 평가\n' +
      '\n' +
      '우리는 Internal-1K 프롬프트 세트를 사용하여 일반적으로 사용되는 종횡비에 대한 사용자 선호도 연구 메트릭을 보고한다. 각 화면비에 대해 9:16에서 16:9까지의 사용자 연구를 수행했으며, 주어진 연구에서는 모든 이미지에서 두 모델에 대해 동일한 화면비 컨디셔닝을 사용했다. 그림 11은 우리의 결과를 보여준다. 우리의 모델은 모든 종횡비에서 SDXL을 큰 마진으로 능가한다.\n' +
      '\n' +
      '사람중심 프롬프트에 대한 평가\n' +
      '\n' +
      '인간 선호도 정렬 개선에 대해 섹션 2.3에서 논의한 바와 같이, 사람 관련 프롬프트는 상업적 텍스트 대 이미지 모델의 중요한 실제 사용 사례이다. 실제로, 저희 제품에서 꽤 인기가 있습니다. 사람 관련 이미지를 생성하는 모델의 능력을 평가하기 위해 제품의 실제 사용자 프롬프트에서 200개의 고품질 사람 관련 프롬프트를 선별했다. 우리는 이것을 _People-200_ prompt set이라고 부른다. 우리는 벤치마킹 목적으로 이 프롬프트 세트를 커뮤니티에 공개할 것이다.\n' +
      '\n' +
      '우리는 인물 종횡비 3:2를 사용하여 사용자 연구를 수행했는데, 이는 사람을 보여주는 이미지에 대해 커뮤니티에서 가장 인기 있는 선택이기 때문이다. 우리는 플레이그라운드 v2.5를 실제 사람 데이터 세트에 대해 훈련된 SDXL의 커뮤니티 미세 조정인 SDXL 및 RealStock v2와 일반적으로 사용되는 두 가지 기준 모델과 비교했다.\n' +
      '\n' +
      '그림 12는 놀이터 v2.5가 두 기준선보다 큰 마진만큼 우수하다는 것을 보여준다.\n' +
      '\n' +
      '그림 11: 다중 종횡비에 대한 SDXL에 대한 **사용자 연구. 일반적으로 사용되는 다양한 종횡비, 9:16에서 16:9 범위의 높이 대 너비 비율로 생성된 이미지에 대한 사용자 연구를 수행했으며, 본 모델은 모든 종횡비에서 SDXL을 큰 여백으로 능가한다.**\n' +
      '\n' +
      '### 자동 평가 벤치마크\n' +
      '\n' +
      '마지막으로 모델의 심미적 품질을 자동으로 평가하기 위한 새로운 벤치마크인 _MJHQ-30K_를 소개한다. 벤치마크는 심미적 품질을 측정하기 위해 고품질 데이터 세트에서 FID(Frechet Inception Distance) [11]을 계산한다. FID를 스팟 체크하고 더 낮은 경향을 보장함으로써 사전 훈련 및 정렬의 다양한 단계 전반에 걸쳐 진행 상황을 신속하게 측정할 수 있었다.\n' +
      '\n' +
      'Midjourney 5.2에서 만든 이미지에서 고품질 데이터 세트를 선별했으며 데이터 세트는 10개의 공통 범주를 포함하고 각 범주에는 3K 샘플이 있다. 일반적인 관행에 따라 우리는 높은 이미지 품질과 높은 텍스트 대 이미지 정렬을 보장하기 위해 미적 점수[18]와 CLIP 점수[10]를 사용했다. 또한 각 카테고리 내에서 이미지 및 프롬프트를 잘 만들기 위해 각별한 주의를 기울였습니다.\n' +
      '\n' +
      '전체적인 FID(표 1)와 카테고리별 FID(그림 13)를 모두 보고한다. 모든 FID 메트릭은 해상도 1024x1024에서 계산되었으며, 본 연구의 결과는 플레이그라운드 v2.5가 전체 FID와 모든 카테고리 FID, 특히 사람과 패션 카테고리 모두에서 플레이그라운드 v2와 SDXL보다 우수함을 보여준다. 이는 MJHQ30K 벤치마크의 FID 점수와 인간의 선호도 간의 상관관계를 나타내는 사용자 연구 결과와 일맥상통한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c} \\hline \\hline Method & Overall FID \\\\ \\hline SDXL 1.0 + refiner[28] & 9.55 \\\\ \\hline Playground v2 [20] & 7.07 \\\\ \\hline Playground v2.5 & **4.48** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **MJHQ-30K overall FID.**\n' +
      '\n' +
      '그림 12: **People-200 벤치마크. 우리는 사람 생성에 중점을 둔 People-200 프롬프트 세트를 사용하여 사용자 연구를 수행한다. 우리는 플레이그라운드 v2와 SDXL[28] 및 리얼스톡 v2의 두 가지 기본 모델을 비교한다. 모든 이미지는 해상도 1254x836으로 3:2 종횡비로 생성되었다.**\n' +
      '\n' +
      '도 13: **MJHQ-30K 벤치마크. 우리는 10개의 공통 범주에 대해 플레이그라운드 v2.5, 플레이그라운드 v2[20], SDXL[28]의 FID를 보고한다. Playground v2.5는 모든 범주에서 Playground v2보다 뛰어나고, _logo_ 및 _people_ 범주에서 가장 두드러진다.**HuggingFace3의 대중에게 이 벤치마크를 공개하고 사전 훈련 및 정렬 동안 모델의 미적 품질을 벤치마킹하기 위해 커뮤니티가 이를 채택하도록 권장한다.\n' +
      '\n' +
      '각주 3: [https://huggingface.co/datasets/playgroundai/MJHQ-30K](https://huggingface.co/datasets/playgroundai/MJHQ-30K)\n' +
      '\n' +
      '## 4 Conclusion\n' +
      '\n' +
      '본 연구에서는 텍스트-이미지 생성 모델에서 최첨단 미적 품질을 달성하기 위한 세 가지 통찰력을 공유하고 다양한 조건과 설정에서 플레이그라운드 v2.5를 SoTA 모델에 대해 분석하고 경험적으로 평가한다. 플레이그라운드 v2.5는 (1) 이미지 색상 및 콘트라스트를 향상시키는 우수한 성능, (2) 다양한 종횡비 하에서 고품질 이미지를 생성하는 능력, (3) 생성된 이미지에서 심미적 품질에 대한 인간의 선호도, 특히 인간의 이미지에서 미세한 세부 사항에 대한 정렬을 보여준다.\n' +
      '\n' +
      '놀이터 v2.5를 대중에게 공개하게 되어 기쁩니다. 이 모델은 오늘날 모든 사용자를 위해 당사 제품 웹사이트4에서 사용할 수 있으며 HuggingFace5에 대한 가중치를 오픈소싱했으며, 확산 모델을 사용하기 위한 두 가지 인기 커뮤니티 도구인 A1111의 Playground v2.5와 ComfyUI의 확장을 곧 제공할 것이다.\n' +
      '\n' +
      '각주 4: [https://playground.com](https://playground.com)\n' +
      '\n' +
      '각주 5: [https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic)\n' +
      '\n' +
      '향후 작업을 위해 텍스트 대 이미지 정렬 개선, 모델의 변형 능력 향상 및 새로운 아키텍처를 탐색할 수 있기를 바랍니다.\n' +
      '\n' +
      '놀이터에서 우리의 목표는 픽셀을 깊이 이해하고 모든 기술 수준의 인간이 픽셀을 마스터하게 생성하고 편집할 수 있도록 하는 통합된 범용 비전 시스템을 구축하는 것이다. 우리는 놀이터 v2.5를 이 비전을 향한 디딤돌로 보고, 공동체가 우리와 함께 건설하도록 권장한다.\n' +
      '\n' +
      '도 14: 인기 있는 사용자 프롬프트가 있는 플레이그라운드 v2.5 랜덤 샘플.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Stability AI. Introducing stable cascade. [https://stability.ai/news/introducing-stable-cascade](https://stability.ai/news/introducing-stable-cascade), 2024. Accessed: 2024-02-20.\n' +
      '* [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. _Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf_, 2(3):8, 2023.\n' +
      '* [3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-\\(\\alpha\\): Fast training of diffusion transformer for photorealistic text-to-image synthesis. _arXiv preprint arXiv:2310.00426_, 2023.\n' +
      '* [4] Ting Chen. On the importance of noise scheduling for diffusion models, 2023.\n' +
      '* [5] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, and Devi Parikh. Emu: Enhancing image generation models using photogenic needles in a haystack, 2023.\n' +
      '* [6] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021.\n' +
      '* [7] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.\n' +
      '* [8] Nicholas Guttenberg. Diffusion with offset noise. [https://www.crosslabs.org/blog/diffusion-with-offset-noise](https://www.crosslabs.org/blog/diffusion-with-offset-noise), 2023. Accessed: 2024-02-20.\n' +
      '* [9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.\n' +
      '* [10] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning, 2022.\n' +
      '* [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium, 2018.\n' +
      '* [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.\n' +
      '* [13] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for high resolution images, 2023.\n' +
      '* [14] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models, 2022.\n' +
      '* [15] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks, 2019.\n' +
      '* [16] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan, 2020.\n' +
      '* [17] Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models, 2023.\n' +
      '* [18] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023.\n' +
      '* [19] Yann LeCun et al. Generalization and network design strategies. _Connectionism in perspective_, 19(143-155):18, 1989.\n' +
      '* [20] Daiqing Li, Aleks Kamko, Ali Sabet, Ehsan Akhgari, Linmiao Xu, and Suhail Doshi. Playground v2.\n' +
      '* [21] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed, 2024.\n' +
      '* [22] Rafid Mahmood, James Lucas, David Acuna, Daiqing Li, Jonah Philion, Jose M. Alvarez, Zhiding Yu, Sanja Fidler, and Marc T. Law. How much more data do i need? estimating requirements for downstream tasks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 275-284, June 2022.\n' +
      '\n' +
      '* [23] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models, 2021.\n' +
      '* [24] NovelAI. Novelai improvements on stable diffusion. [https://blog.novelai.net/novelai-improvements-on-stable-diffusion-e10d38db82ac](https://blog.novelai.net/novelai-improvements-on-stable-diffusion-e10d38db82ac), 2022. Accessed: 2024-02-20.\n' +
      '* [25] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.\n' +
      '* [26] Amar Parkash and Devi Parikh. Attributes for classifier feedback. In _Computer Vision-ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part III 12_, pages 354-368. Springer, 2012.\n' +
      '* [27] William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023.\n' +
      '* [28] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\n' +
      '* [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022.\n' +
      '* [30] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022.\n' +
      '* [31] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations, 2021.\n' +
      '* [32] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning, 2024.\n' +
      '* [33] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. Lima: Less is more for alignment, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>