<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '동영상 감독 해결# 인퓨전: 텍스트에 대한 효율적인 전략 적응.\n' +
      '\n' +
      '신유안\\({}^{1}\\), 진우백\\({}^{2}\\), Keyang Xu\\({}^{2}\\), 오머 토프\\({}^{2}\\), 홍량페이\\({}^{2}\\)\n' +
      '\n' +
      '구글({}^{1}\\)은 시카고({}^{2}\\)의 대학교입니다.\n' +
      '\n' +
      'yuanx@uchicago.edu {jinoo,keyangxu,omertov,hongliangfei}@google.com\n' +
      '\n' +
      '이 작업은 구글에서 첫 번째 저자의 인턴십 동안 수행되었습니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '영상 생성을 위한 공간 정보를 캡처하기 위해 쉽게 학습된 픽셀 레벨 이미지 확산 모델의 용량을 받아들이는 효율적인 확산 기반 텍스트 대 비디오 초해상도(SR) 튜닝 접근법을 제안한다. 이 목표를 달성하기 위해 텍스트 대 이미지 SR 모델의 가중치를 비디오 생성 프레임워크에 부풀려 효율적인 아키텍처를 설계한다. 또한, 비디오 프레임에 걸쳐 시간적 정합성을 보장하기 위해 시간 어댑터를 통합합니다. 우리는 팽창된 아키텍처를 기반으로 다양한 튜닝 접근법을 조사하고 계산 비용과 초해상도 품질 사이의 트레이드오프를 보고한다. Sutterstock 비디오 데이터셋에 대한 정량적 평가와 정성적 평가는 우리의 접근법이 시각적 품질과 시간적 일관성이 좋은 텍스트 대 비디오 SR 생성을 수행할 수 있음을 보여준다. 시간적 정합성을 평가하기 위해, 우리는 또한 구글 드라이브에서 비디오 포맷으로 시각화를 제시한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '딥 생성 모델로서 융합 모델[5, 14]은 생성 작업[9, 10]에서 GAN[3, 6, 16, 18]을 능가하는 새로운 최첨단 성능을 달성했다. 확산 기반 텍스트 조건 생성 시스템에서 기본 모델은 초기에 저해상도 이미지/비디오를 생성하며, 이는 이후에 초해상도 모듈[4, 10, 12]에 의해 정제되어 고품질 샘플을 생성한다. 수많은 기존 확산 기반 텍스트 대 이미지 초해상도 모델[9, 10]이 100억 규모의 텍스트 이미지 데이터셋에 대해 훈련된 것은 우수한 생성 능력을 보여주었다. 그러나 고해상도 비디오 데이터의 부족으로 인해 텍스트 대 비디오 공간 초해상도 트레이닝이 어렵다. 이 시나리오는 영상 생성 과제[2, 19, 15, 2]에 대한 외장 이미지 모델의 인플레이션에 동기를 부여한다. 또한, 비디오 생성 모델을 훈련하려면 매우 높은 계산 및 메모리 요구 사항이 필요하며, 이는 비디오 모델을 최적화하기 위해 비용 효율적인 대안을 제공하는 기술을 구동한다.\n' +
      '\n' +
      '최근 제안된 여러 방법[1]은 또한 전처리된 잠재 확산 모델을 사용하여 고품질 비디오를 생성하는 데 중점을 둔다. 시간적 주의 메커니즘은 [4, 12]에서도 일반적으로 사용된다. 그러나 미세 조정 단계에서 비디오 품질과 자원 요구 사항 사이의 상충 관계를 조사하는 것은 이러한 작업의 초점이 아니다.[1] 일반적으로 요구하려면 일반적으로 일반적으로 필요로 한다.\n' +
      '\n' +
      '우리의 접근법의 모든 아키텍처 __그림 1: 여분의 구조. 업:_ 우리는 텍스트 대 이미지 모델에서 UNet 가중치를 텍스트 대 비디오 모델에 추론하여 확산 기반 초해상도 작업을 수행한다. Bottom:_ 우리는 UNet 가중치 동결을 유지하면서 팽창된 아키텍처에 시간 어댑터를 주입하고 튜닝한다.\n' +
      '\n' +
      '영상 아키텍처에 전처리된 이미지 가중치가 부풀린 경우에도, 고품질 비디오를 생성하기 위해 모든 계산 모듈의 풀 튜닝은 고품질 비디오를 생성한다. 대조적으로, 우리의 접근법은 적용된 도메인에 있으며 튜닝 효율이 비디오 초해상도 품질에 어떻게 영향을 미치는지 조사한다. 더 중요한 것은 잠재 공간[1]에서 모델 인플레이션을 조사하는 대신 우리의 접근법은 픽셀에서 직접 작동하는 최초의 것이다. 우리의 목표는 최첨단 생성 품질을 달성하지 않는 것입니다. 대신, 우리는 합리적인 시각적 품질과 시간적 일관성을 가진 고해상도 비디오를 생성하기 위해 실용적이고 효율적인 튜닝 시스템을 구축하는 것을 목표로 한다.\n' +
      '\n' +
      '본 논문에서는 그림 1의 상단에서 볼 수 있듯이 효율적이고 효과적인 텍스트 대 비디오 초해상도를 위해 쉽게 학습된 이미지 가중치의 공간 용량을 레버리지하는 것을 목표로 하며, 비디오 프레임에 걸친 정합성을 포착하여 관심 기반 시간 어댑터를 비디오 아키텍처에 주입한다. 이 어댑터는 그림 1의 하단에 나타난 바와 같이 팽창된 가중치를 동결시키면서 독립적으로 미세 조정될 수 있다. 우리는 셰터스토크 비디오 데이터셋에 대한 공간적 초해상도 작업을 수행하고 우리의 접근법이 시각적 품질과 시간적 일관성을 가진 비디오를 생성할 수 있음을 검증한다. 우리는 또한 튜닝 복잡성과 생성 품질 사이의 상충 관계를 보여준다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '확산 기반 SR 모델은 베이스 생성 모델에 의해 생성된 저해상도 샘플에 조건화되어 고해상도 이미지[10] 또는 비디오[4]를 추가로 생성한다. 100억 규모의 영상데이터에 사전 훈련된 영상생성모델의 성공으로 최근 영상업무에 대한 장외영상모델을 직접 빌리기 위한 연구 노력이 이루어지고 있다. 예를 들어 [7]는 영상 생성을 위한 영상 가중치를 제로샷 방식으로 0샷 방식으로 로드한다[15, 2, 19]. [15, 2, 19] 텍스트 대 비디오 편집을 위한 최적화 모델 인플레이션 및 DDIM[13] 반전입니다. 이러한 연구는 비디오 공간 초해상도 작업에 직접 적용할 수 없지만 처음부터 재학습을 필요로 하지 않고 이미지 모델을 채택할 가능성에 대한 통찰력 있는 힌트를 제공한다.\n' +
      '\n' +
      '시간 축에서 작동하는 시간적 주의 메커니즘은 비디오 확산 접근법[4, 12]에서 일반적으로 채택된다. 우리의 방법은 영상 생성을 위한 영상 확산 모델을 차입하는 정신에서 [1]과 동일한 개념을 공유한다. 그러나 우리의 접근법은 텍스트 대 비디오 슈퍼 해상도에 적용된 도메인에 중점을 둔다. 더 중요한 것은 비디오 아키텍처의 부분 튜닝을 용이하게 하여 시각적 품질과 시간적 일관성을 포함하여 다양한 튜닝 방법이 생성 품질에 어떤 영향을 미치는지 질적으로 그리고 정량적으로 평가한다.\n' +
      '\n' +
      '## 3 Approach\n' +
      '\n' +
      'I:I_{1},...,I_{n}]\\로 표시된 \\(I:I_{n}]\\의 서열로 표시되는 비디오 클립을 고려하여 공간 해상도 \\(s\\)가 낮고 텍스트 설명 \\(t\\)은 동일한 길이의 새로운 비디오를 생성하지만 텍스트와 비디오 사이의 상관 관계를 보존하면서 향상된 해상도 \\(s^{\\prime}\\)를 생성하는 것을 목표로 한다. 우리는 사전 훈련되고 고정된 대규모 이미지 확산 모델에 대한 강력한 공간적 이해를 활용하여 시간적으로 일관된 영상 생성에 재조정하는 것을 목표로 한다. 이것은 시간 및 자원 소비 모두 제한된 고해상도 비디오 데이터에 대한 처음부터 광범위한 훈련의 필요성을 제거한다. 이미지 확산 모델의 가중치를 비디오 생성 아키텍처로 부풀리고(3.1절에서 자세히 설명됨) 제3.2절에서 논의된 비디오 프레임 전체에 걸쳐 연속성과 정합성을 보장하기 위해 효율적인 시간 어댑터를 튜닝함으로써 이 목표를 달성한다.\n' +
      '\n' +
      '이미지위치로 인플레이션하세요.\n' +
      '\n' +
      '영상 작업을 수용하기 위해 \'등급\' 이젠[10] 텍스트 대 이미지 초해상도 모델을 통해 이미지와 비디오 아키텍처 사이의 일대일 매핑을 구축한다. 우리는 그림 2(좌표)와 같이 잔류 및 교차 의도 블록으로 구성된 이젠의 U-Net 아키텍처를 먼저 재방문한다. 모양 \\(B\\tcer C\\tcer H\\tcer W\\)을 갖는 입력 정적 이미지의 배치를 감안할 때, 잔류 블록은 공간 정보를 캡처하는 동시에 교차 표시로 인해 생성된 이미지가 주어진 텍스트 프롬프트와 정렬되도록 보장한다. 우리의 텍스트 대 비디오 슈퍼 해상도 맥락에서 비디오 클립의 입력 배치는 \\(B\\tcer F\\tep C\\t지라도 H\\) 모양이며, 여기서 \\(F\\)는 프레임의 수이다. 그림 2(오른쪽)와 같이 각 개별 프레임은 병렬 방식을 통해 처리되며, 각 가지에는 텍스트-시각 특징 추출을 위한 잔차 블록과 교차 표시층이 포함되어 있다. UNet 블록의 마지막에는 프레임 전체에 걸쳐 일관성과 평활성을 유지하기 위한 특징 응집을 위한 시간 어댑터가 있습니다. 잔차 블록 및 교차 선택 계층의 처리 유닛은 훈련 중 동일한 가중치를 공유하며, 이 경우 비디오 데이터를 단순히 \\((BF) 시간 C\\tcer H\\tcer W\\)로 재조정할 수 있다. 이러한 가중치 공유 제도의 좋은 속성을 감안할 때, 우리는 미리 학습된 이미지 모델 가중치를 건축 수정 없이 비디오 UNet에 직접 부풀릴 수 있다.\n' +
      '\n' +
      '프론트별 의도.\n' +
      '\n' +
      '비디오 프레임 간의 정합성을 캡처하기 위해 잔류 및 교차 의도 블록 후에 시간적 어댑터를 적용한다. 그림 3은 주의 기반 시간 어댑터의 설계를 보여준다. 우리는 먼저 I\\(I\\)를 \\(I^{\\prime}\\) 형태로 \\(I^{\\prime}\\)로 재구성한 다음 기존의 자기 주의 모듈을 채택한다.\n' +
      '\n' +
      '\\(\\[\\text{ 자가 주의},Q,K,V)=\\text{Softmax}(\\frac{QK^{T}}}{\\sqrt{d}})\\cdot V.\n' +
      '\n' +
      '이러한 주의 메커니즘은 전체 구조 및 비디오 프레임의 정합성을 결정하는 데 효과적이다.\n' +
      '\n' +
      ' 구체적으로, 프레임별 상관 관계를 학습하기 위해 \'토큰 축\' \\(F\\)에 대한 가중 합을 계산한다. 우리는 DDPM[5]에서 간단한 변성 목표와 일치하여 전체 또는 부분 모델 가중치 중 어느 하나의 종말 최적화를 사용한다. 이와 같이, 모델 가중치는 저해상도 프레임에 조건화된 노이즈 예측의 MSE 손실을 최소화하여 최적화된다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '우리는 슈터스토크 데이터 세트에 대한 접근 방식을 검증한다. 8배 초해상도 내부 데이터 소스에서 사전 훈련된 이젠 확산 모델의 버전을 비디오 UNet에 주입한다. 이 UNet은 다운샘플링 및 업샘플링에 대해 각각 4단계로 구성되며, 이는 \\(2\\, \\(4\\) \\(8\\) \\(16\\)로 표시된다. T5-xxl 인코더[8]는 텍스트 임베딩을 추출하는 데 사용되며, 그 출력은 \\(16\\) 단계 내의 교차 의도 계층으로 공급된다. 우리는 8 FPS의 고해상도 및 프레임 레이트에서 7백만 개의 비디오 클립이 있는 슈터스토크 텍스트 대 비디오 데이터셋에서 비디오 모델을 훈련시킨다. 각 클립의 기간은 1초, 즉 \\(F=8\\)이다. 초해상도 척도는 \\(4\\t그램\\)로, 해상도는 \\(64\\t 64\\)에서 \\(256\\tco 256\\)로 상승한다. 우리는 (1)제로샷(ZS)을 포함한 여러 기준 최적화 접근법을 조사한다.\n' +
      '\n' +
      '그림 3: 비디오 클립에 걸쳐 시간적 정합성을 보장하는 주의를 기울이는 테포털 어댑터.\n' +
      '\n' +
      '그림 2: 텍스트 대 이미지 SR UNet에서 텍스트 대 비디오 SR UNet로의 고가 인플레이션은 그림 2이다.\n' +
      '\n' +
      '추가 훈련 없이 인플레이션을 거쳐 비디오 모델을 직접 평가합니다. (2) 풀-프트(풀): 시간 어댑터를 통합한 후, 모든 모듈은 최적화를 거치게 된다. 이 전략은 초해상도 작업에서 잠재적인 \'보일러 결합\' 성능을 보여주는 것을 목표로 한다. (3) 테스포랄: 우수한 생성 품질을 효율적으로 유지하면서 시간적 일관성을 포착하기 위해 시간 어댑터만 조정한다. 초기 LR(10^{-5}\\) 및 64 TPUv3에서 256의 배치 크기를 갖는 아다인자[11]를 사용하여 1epoch에 대해 지느러미였다.\n' +
      '\n' +
      '그림 4: 이미지 모델 인플레이션 후 서로 다른 튜닝 방법의 시각화는 텍스트 프롬프트 "크로마케이에 대한 개 다추툰"을 조건화했다.\n' +
      '\n' +
      '그림 5: 텍스트 프롬프트: 카메라는 토마토 소스에서 메제즈 기계 장비 파스타를 요리하는 것을 따른다.\n' +
      '\n' +
      '### Quantitative Results\n' +
      '\n' +
      '다양한 메트릭을 사용하여 다양한 최적화 접근법을 평가한다. 표 1에서 보는 바와 같이, 풀프트 접근법은 UNet의 모든 \\(628.89\\) 매개변수를 튜닝하여 피크 신호 대 잡음 비율(PSNR) 및 구조 지수 유사성(SSIM) 측면에서 최상의 시각적 품질을 달성한다. 효율적인 시간적 어댑터는 대략적인 \\(2\\t Woong\\) 훈련 가속도와 반열 메모리 사용량을 달성하면서 여전히 합리적인 시각적 품질을 얻는다.\n' +
      '\n' +
      '그림 6: 텍스트 프롬프트: 그린 스크린에 있는 작은 아름다운 오리 새끼.\n' +
      '\n' +
      '그림 7:텍스트 프롬프트: 브라질 북동부 해변.\n' +
      '\n' +
      '그림 8: 이미지 모델 인플레이션이 있거나 없는 방법의 수정. 텍스트 프롬프트: 투어리스트들이 옛 마을을 방문합니다.\n' +
      '\n' +
      '일반적인 파라미터 수량의 10분의 1입니다. 제로샷 접근 방식은 최악을 수행한다.\n' +
      '\n' +
      '또한 효율적인 튜닝 접근법이 시간적 일관성을 유지할 수 있다는 것을 검증하며, 즉 건설적 프레임 간의 움직임은 초해상도 결과에서 매끄럽게 유지된다. 우리는 [17]에서 양적 평가 메트릭을 채택하는데, 시간적 변화 일관성(TCC)으로 정의되고 있다.\n' +
      '\n' +
      '\\[TCC(H,G)=\\frac{\\sum_{i=1}^{n-1}SSIM(|h^{i}-h^{i+1}|,|g^{i}-g^{i+1}|)}{n-1} \\tag{2}\\]\n' +
      '\n' +
      'H=\\{h^{1},h^{2},.....,h^{n}\\}\\) 및 \\(G=\\{g^{1},g^{2},g^{n},g^{n}\\})은 각각 고해상도 접지-진 및 생성된 비디오 프레임이다. <표 1>은 여전히 효율적인 시간적 튜닝이 합리적인 결과를 얻는 훈련 효율성과 시간적 일관성 간의 명확한 상충 관계를 보여준다. 우리는 또한 제로 샷 접근법이 시간 축에서 독점적으로 작동하는 시간 모듈의 부족으로 인해 인접한 프레임들 사이에서 일관된 변화를 유지하지 못한다는 것을 관찰한다.\n' +
      '\n' +
      '### Qualitative Results\n' +
      '\n' +
      '그림 4에서 보는 바와 같이, 그라운드 진리 고해상도 비디오(GT)와 비교할 때, 풀과 테포랄은 모두 높은 시각적 품질과 시간적 평활성으로 표시된 우수한 초해상도 결과를 생성한다. ZS 접근법은 비디오 데이터에 미세 조정 없이 시각적 콘텐츠가 괜찮은 프레임을 생성하도록 관리하지만 정적 이미지에만 사전 학습하기 때문에 시간적 정합성을 유지하는 데 짧다. 이것은 비디오 프레임에 걸쳐 일관성을 포착하는 시간 어댑터의 효과를 보여준다. 그림 5, 6, 7에서 추가 시각화를 제공합니다.\n' +
      '\n' +
      '인플레이션은 매우 효율적입니다.\n' +
      '\n' +
      '이미지 가중치 인플레이션을 위한 간단한 기준선은 비디오 UNet을 무작위로 초기화하고 비디오 데이터만을 사용하여 완전히 미세 조정한다는 것이다. 그림 9에서 볼 수 있듯이 7M 영상 데이터의 \\(10\\%\\)만을 활용해도 이미지 인플레이션 기반 접근은 높은 시각적 품질을 달성할 수 있다. 이러한 경향은 그림 8에서 더욱 뚜렷해짐에 따라 이미지 가중치 인플레이션 접근법의 데이터 효율을 보여준다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 텍스트 대 이미지 모델 가중치를 텍스트 대 비디오 공간 초해상도 모델에 부풀리기 위한 실용적인 확산 시스템을 제안하였다. 화소 레벨 확산 모델에 대한 가중치 인플레이션을 연구하는 첫 번째 작업이다. 효율적인 시간적 적응을 위해 다양한 튜닝 방법을 조사했다. 우리는 또한 시간적 일관성과 튜닝 효율을 가진 초해상도 품질 사이의 좋은 상충 관계를 입증했다. 향후 조사로서 우리는 목표 해상도를 256에서 512(예: \\(4\\)에서 \\(8\\)SR로 계산하고 더 긴 시간 프레임으로 비디오를 생성하는 것을 목표로 하며, 이는 생성 품질과 계산 자원 간의 더 명백한 상충 관계를 산출한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{Visual Quality} & \\multicolumn{2}{c}{Temporal Consistency} & \\multicolumn{2}{c}{Efficiency} \\\\ \\cline{2-7}  & PSNR (\\% \\(\\uparrow\\)) & SSIM (\\(\\uparrow\\)) & TCC (\\(\\uparrow\\)) & Tunable Params (M) \\(\\downarrow\\) & Train Speed (steps/s) \\(\\uparrow\\) & Memory (G) \\(\\downarrow\\) \\\\ \\hline Zero-shot & 18.1 & 0.42 & 0.70 & - & - & - \\\\ Full-ft & 28.7 & 0.77 & 0.86 & 628.89 & 1.05 & 15 \\\\ Temporal & 24.3 & 0.62 & 0.82 & 67.24 & 2.02 & 8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 다양한 튜닝 접근법에 대한 정량적 결과는 다음과 같다.\n' +
      '\n' +
      '그림 9: PSNR 및 SSIM에 의해 평가된 훈련 데이터 효율이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In _CVPR_, 2023.\n' +
      '* [2] Duygu Ceylan, Chun-Hao Paul Huang, and Niloy J. Mitra. Pix2video: Video editing using image diffusion. _CoRR_, abs/2303.12688, 2023.\n' +
      '* [3] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In _NeurIPS_, 2014.\n' +
      '* [4] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey A. Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models. _CoRR_, abs/2210.02303, 2022.\n' +
      '* [5] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _NeurIPS_, 2020.\n' +
      '* [6] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In _CVPR_, 2020.\n' +
      '* [7] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. _CoRR_, abs/2303.13439, 2023.\n' +
      '* [8] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 2020.\n' +
      '* [9] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.\n' +
      '* [10] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In _NeurIPS_, 2022.\n' +
      '* [11] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In Jennifer G. Dy and Andreas Krause, editors, _ICML_, 2018.\n' +
      '* [12] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In _ICLR_, 2023.\n' +
      '* [13] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _ICLR_, 2021.\n' +
      '* [14] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _ICLR_, 2021.\n' +
      '* [15] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. _CoRR_, abs/2212.11565, 2022.\n' +
      '* [16] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks. In _CVPR_, 2018.\n' +
      '* [17] Haokui Zhang, Ying Li, Yuanzhouhan Cao, Yu Liu, Chunhua Shen, and Youliang Yan. Exploiting temporal consistency for real-time video depth estimation. In _ICCV_, 2019.\n' +
      '* [18] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. In _ICCV_, 2017.\n' +
      '* [19] Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han, Tiande Guo, and Luoqi Liu. Towards consistent video editing with text-to-image diffusion models. _CoRR_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>