<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'RealCompo: Dynamic Equilibrium between Realism and Compositionality Improves Text-to-Image Diffusion Models\n' +
      '\n' +
      'Xinchen Zhang\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Tsinghua University \\({}^{2}\\)Peking University \\({}^{3}\\)University of Science and Technology of China \\({}^{4}\\)PicUp.AI\n' +
      '\n' +
      'Ling Yang\n' +
      '\n' +
      'Xiaqi Cai\n' +
      '\n' +
      'Zhaochen Yu\n' +
      '\n' +
      'Jiake Xie\n' +
      '\n' +
      'Ye Tian\n' +
      '\n' +
      'Minkai Xu\n' +
      '\n' +
      'Yong Tang\n' +
      '\n' +
      'Yujiu Yang\n' +
      '\n' +
      'Bin Cui\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Diffusion models have achieved remarkable advancements in text-to-image generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we propose a new _training-free_ and _transferred-friendly_ text-to-image generation framework, namely _RealCompo_, which aims to leverage the advantages of text-to-image and layout-to-image models to enhance both realism and compositionality of the generated images. An intuitive and novel _balancer_ is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and layout-to-image models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Code is available at [https://github.com/Yang_Ling0818/RealCompo](https://github.com/Yang_Ling0818/RealCompo)\n' +
      '\n' +
      'Machine Learning, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Recently, there have been exciting developments and significant progress in diffusion models (Yang et al., 2024; Song et al., 2020; Ho et al., 2020; Song et al., 2020). Among various generative tasks, text-to-image (T2I) generation (Nichol et al., 2022; Hu et al., 2024; Yang et al., 2024) has gained considerable interest within the community. Due to extensive training on large-scale image-text paired datasets, T2I models (e.g., Stable Diffusion (Rombach et al., 2022)) have exhibited powerful semantic understanding capabilities, enabling them to generate highly realistic objects based on semantic information (Saharia et al., 2022; Betker et al., 2023; Podell et al., 2023). However, when the input text involves multiple objects or complex relationships, the generated images from T2I models sometimes do not align with the compositionality of the objects as specified by the text prompt (Lian et al., 2023; Bar-Tal et al., 2023). Fig. 2 illustrates the evaluation results of Stable Diffusion (Rombach et al., 2022) from different aspects, failing to generate the correct number of objects and lacking good compositionality.\n' +
      '\n' +
      'A practical method to optimize the compositionality of the generated images is providing the layout of each object as additional input to the diffusion model (Fan et al., 2023; Yang et al., 2023; Wu et al., 2023). Using layout as another condition to constrain T2I models, these layout-to-image (L2I) models (Li et al., 2023; Chen et al., 2024; Xie et al., 2023) have the ability to precisely control the generation of specific objects at specific locations. For instance, GLIGEN (Li et al., 2023) employs a Gated Self-Attention to train the model with comprehensive information from layouts. Although these L2I methods have improved the weaknesses of object positioning and counting errors, their realism of generation results is unsatisfactory. In contrast,\n' +
      '\n' +
      'Figure 1: On T2I-Compbench (Huang et al., 2023), RealCompo has achieved SOTA performance on all six evaluation tasks.\n' +
      '\n' +
      'T2I models can generate objects with high realism, but it is difficult to follow the text prompt regarding the number and position of objects. There is a significant complementary space between L2I models and T2I models.\n' +
      '\n' +
      'We conduct preliminary experiments to investigate the cross-attention maps of each model. As shown in Fig. 2, the T2I model Stable Diffusion struggles to comprehend multiple objects and their spatial relationships (Huang et al., 2023) in multiple-object generation tasks under uncontrolled conditions, resulting in suboptimal compositionality of the generated images. However, the L2I model GLIGEN (Li et al., 2023) primarily focuses on areas outside the object boxes within each token\'s cross-attention map. This indicates that the layout exerts an overly strong constraint on object placement. Consequently, the realism and attribute binding of L2I models leave room for improvement due to insufficient semantical attention, such as shape and color.\n' +
      '\n' +
      'To this end, we introduce a general _training-free_ text-to-image generation framework _RealCompo_ that utilizes a novel _balancer_ to achieve dynamic equilibrium between realism and compositionality in generated images. We first utilize the in-context learning (Min et al., 2022) ability of LLMs to reason out the layout of essential objects and achieve the "pre-binding" of the attributes and objects from the input text prompt. Then, we introduce an innovative _balancer_ to incorporate pre-trained L2I and T2I models. This balancer is engineered to automatically adjust the respective coefficients of the prediction combination through analyzing the cross-attention maps of each model at each denoising step. This approach can integrate the strengths of two models, dynamically balancing the realism and compositionality of the generated images. Although methods exist for merging multiple diffusion models (Xue et al., 2023; Balaji et al., 2022), they lack flexibility in their usage because they require additional training and lacking the ability to generalize to other scenarios and models. Our method is the first to perform model composition in a training-free manner, allowing for a seamless transition between any models.\n' +
      '\n' +
      'Extensive experiments are conducted to demonstrate the outstanding performance of our RealCompo in generating images with both realism and compositionality. As illustrated in Figure 2, RealCompo combines the strengths of the T2I and L2I models in a dynamically balanced manner. By incorporating layout information, it injects precise object positioning into the T2I model while maintaining its characteristics about the focus of each object in the cross-attention maps of each token. This ensures the generation of highly realistic images. At the same time, it maintains the L2I model\'s attention on the features outside the box in each token\'s cross-attention map, demonstrating strong localization ability and optimal compositionality in the generated images. RealCompo enables both T2I and L2I models to maintain their respective advantages while complementing each other\'s shortcomings.\n' +
      '\n' +
      'To the best of our knowledge, this is the first time in text-to-image generation that the quality of generated images has been enhanced by dynamically achieving an equilibrium between the realism and compositionality of the generated\n' +
      '\n' +
      'Figure 2: As shown in the first column single T2I model Stable Diffusion (Rombach et al., 2022) does not perform well in the compositionality in image generation. The second column shows that single L2I model GLIGEN (Li et al., 2023) does not perform well regarding the realism in image generation because it focuses on generating the parts outside of the bounding box of each object. In the third and fourth columns, our method achieves outstanding generation results in terms of realism and compositionality in image generation by utilizing the distinct advantages from both T2I and L2I models.\n' +
      '\n' +
      'images. With the ability to choose arbitrary L2I or T2I models, RealCompo can automatically balance them to realize a synergistic generation. We believe that RealCompo opens up a new research perspective in controllable and compositional image generation.\n' +
      '\n' +
      'Our main contributions are summarized as the following:\n' +
      '\n' +
      '* We introduce a new _training-free_ and _transferred-friendly_ text-to-image generation framework RealCompo, which enhances compositional text-to-image generation by equilibrating the realism and compositionality of generated images.\n' +
      '* In RealCompo, we design a novel _balancer_ to dynamically combine the outputs from T2I and L2I models in each denosing timestep. It provides a fresh perspective for compositional image generation.\n' +
      '* Extensive qualitative and quantitative comparisons with previous SOTA methods demonstrate that RealCompo has significantly improved the performance in generating multiple objects and complex relationships.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Text-to-Image GenerationIn recent years, the field of text-to-image generation has made significant strides (Sun et al., 2023; Xu et al., 2023; Podell et al., 2023), largely attributable to breakthroughs in diffusion models (Yang et al., 2023). By training on large-scale image-text paired datasets, T2I models such as Stable Diffusion (SD) (Rombach et al., 2022), DALL-E 2 (Ramesh et al., 2022), MDM (Gu et al., 2023), and Pixart-\\(\\alpha\\)(Chen et al., 2023), have demonstrated remarkable generative capabilities. However, there is still significant room for improvement in the compositionality of the generated images when it comes to generating multiple objects (Wu et al., 2023). Many studies have attempted to address this issue through controlled generation (Zhang et al., 2023), providing additional modalities such as semantic map (Huang et al., 2023), scene graph (Yang et al., 2022), layout (Zheng et al., 2023), etc., to constrain the model\'s generative domain to ensure the accuracy of the number and position of objects in the generated images. However, due to the constraints of the additional modalities, the realism of generation may decrease (Li et al., 2023). Furthermore, several works (Qu et al., 2023; Chen et al., 2023; Ye et al., 2023; Yang et al., 2024) have attempted to bridge the language understanding gap in models by pre-processing prompts with Large Language Models (LLMs) (Achiam et al., 2023; Touvron et al., 2023). At present, it is challenging for T2I models to strike a balance between generation realism and compositionality (Yang et al., 2024).\n' +
      '\n' +
      'Compositional Text-to-Image GenerationThe concept of compositional text-to-image generation is to make the images generated by the model visually consistent with the semantics (Huang et al., 2023). Current works focus on improving the generation quality in negation (Liu et al., 2022), generative numeracy (Lian et al., 2023), attribute binding (Chefer et al., 2023; Feng et al., 2023) and spatial reasoning (Wu et al., 2023). Recent studies can generally be divided into two types (Wang et al., 2023): one primarily uses cross-attention maps for compositional generation (Meral et al., 2023; Kim et al., 2023), while the other provides layout as a condition for generation (Gani et al., 2023). The first type of method delves into a detailed analysis of cross-attention maps, particularly emphasizing their correspondence with the input text. Attend-and-Excite (Chefer et al., 2023) dynamically intervens in the generation process to improve the model\'s generation results in terms of attribute binding (such as color). The second type of method offers layout as a constraint, enabling the model to generate images that meet this condition. This approach directly defines the area where objects are located, making it more straightforward and observable compared to the first type of methods (Li et al., 2023). Most of them utilize Large Lange Models (LLMs) to reason out layouts. LMD (Lian et al., 2023) provides an additional layout as input with the in-context learning (ICL) ability of LLMs. However, these algorithms are unsatisfactory regarding the generated images\' realism. A recent powerful framework RPG (Yang et al., 2024) utilizes Multimodal LLMs to decompose complex generation task into simpler subtask to obtain satisfactory realism and compositionality of generated images. Orthogonal to this work, we achieve dynamic equilibrium between realism and compositionality by combining T2I and L2I models.\n' +
      '\n' +
      '## 3 Preliminary\n' +
      '\n' +
      'Diffusion models (Ho et al., 2020; Sohl-Dickstein et al., 2015) are probabilistic generative models. They can perform multi-step denoising on random noise \\(\\mathbf{x}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{\\mathrm{I}})\\) to generate clean images through training. Specifically, gaussian noise \\(\\mathbf{\\epsilon}\\) is gradually added to the clean image \\(\\mathbf{x}_{0}\\) in the forward process:\n' +
      '\n' +
      '\\[\\mathbf{x}_{t}=\\sqrt{\\alpha_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\alpha_{t}}\\mathbf{\\epsilon} \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{\\mathrm{I}})\\) and \\(\\alpha_{t}\\) is the noise schedule.\n' +
      '\n' +
      'Training is performed by minimizing the squared error loss:\n' +
      '\n' +
      '\\[\\min_{\\mathbf{\\theta}}\\mathcal{L}=\\mathbb{E}_{\\mathbf{x},\\mathbf{\\epsilon}\\sim\\mathcal{N} (\\mathbf{0},\\mathbf{\\mathrm{I}}),t}\\left[\\left\\|\\mathbf{\\epsilon}-\\mathbf{\\epsilon_{\\theta}}( \\mathbf{x}_{t},t)\\right\\|_{2}^{2}\\right] \\tag{2}\\]\n' +
      '\n' +
      'The parameters of the estimated noise \\(\\mathbf{\\epsilon_{\\theta}}\\) are updated step by step by calculating the loss between the real noise \\(\\mathbf{\\epsilon}\\) and the estimated noise \\(\\mathbf{\\epsilon_{\\theta}}(\\mathbf{x}_{t},t)\\).\n' +
      '\n' +
      'The reverse process aims to start from the noise \\(\\mathbf{x}_{T}\\), and denoise it according to the predicted noise \\(\\mathbf{\\epsilon_{\\theta}}(\\mathbf{x}_{t},t)\\) at each step. DDIM (Song et al., 2020) is a deterministic sampler with denoising steps:\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{x}_{t-1}&=\\sqrt{\\bar{\\alpha}_{t-1}} \\left(\\frac{\\mathbf{x}_{t}-\\sqrt{1-\\bar{\\alpha}_{t}}\\mathbf{\\epsilon_{\\theta}}\\left(\\bm {x}_{t},t\\right)}{\\sqrt{\\bar{\\alpha}_{t}}}\\right)\\\\ &\\quad+\\sqrt{1-\\bar{\\alpha}_{t-1}-\\sigma^{2}\\mathbf{\\epsilon_{\\theta }}\\left(\\mathbf{x}_{t},t\\right)+\\sigma\\mathbf{\\epsilon}}\\end{split} \\tag{3}\\]\n' +
      '\n' +
      'Stable Diffusion (Rombach et al., 2022) is a significant advancement in this field, which conducts noise addition and removal in the latent space. Specifically, SD uses a pre-trained autoencoder that consists of an encoder \\(\\mathcal{E}\\) and a decoder \\(\\mathcal{D}\\). Given an image \\(\\mathbf{x}\\), the encoder \\(\\mathcal{E}\\) maps \\(\\mathbf{x}\\) to the latent space, and the decoder \\(\\mathcal{D}\\) can reconstruct this image, i.e., \\(\\mathbf{z}=\\mathcal{E}(\\mathbf{x})\\), \\(\\tilde{\\mathbf{x}}=\\mathcal{D}(\\mathbf{z})\\). Moreover, Stable Diffusion supports an additional text prompt \\(y\\) for conditional generation. \\(y\\) is transformed into text tokens \\(\\tau_{\\theta}(y)\\) through the pre-trained CLIP (Radford et al., 2021) text encoder. \\(\\mathbf{\\epsilon_{\\theta}}\\) is trained via:\n' +
      '\n' +
      '\\[\\min_{\\mathbf{\\theta}}\\mathcal{L}\\!=\\!\\mathbb{E}_{\\mathbf{z}\\sim\\mathcal{E}(\\mathbf{x}), \\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}),t}\\left[\\|\\mathbf{\\epsilon}\\!-\\! \\mathbf{\\epsilon_{\\theta}}(\\mathbf{z}_{t},t,\\tau_{\\theta}(y))\\|_{2}^{2}\\right] \\tag{4}\\]\n' +
      '\n' +
      'In the inference process, noise \\(\\mathbf{z}_{T}\\sim\\mathcal{N}\\left(\\mathbf{0},\\mathbf{I}\\right)\\) is sampled from the latent space. By applying Eq. 3, we perform step-by-step denoising to obtain a clean latent \\(\\mathbf{z}_{0}\\). The generative image is then reconstructed through the decoder \\(\\mathcal{D}\\).\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      'In this section, we introduce our method, RealCompo, which designs a novel balancer to achieve dynamic equilibrium between the realism and compositionality of generated images. In Section 4.1, we analyze the necessity of incorporating influence for each noise and provide a method for calculating coefficients. In Section 4.2, we provide a detailed explanation of the update rules employed by the balancer, which utilizes a training-free approach to update coefficients dynamically. In Section 4.3, we expand the application of RealCompo, designing loss functions for each category of L2I models.\n' +
      '\n' +
      '### Combination of T2I and L2I Models\n' +
      '\n' +
      'As shown in Fig. 3, we firstly leverage the powerful in-context learning (Wu et al., 2023) capability of Large Language Models to analyze the input text prompt and generate an accurate layout to achieve "pre-binding" between objects and attributes. The layout is then used as input for the L2I model. In this paper, we choose GPT-4 for layout generation. Please refer to Appendix A.1 for detailed explanation.\n' +
      '\n' +
      'It is clear from Eq. 3 that the generation of images using DDIM (Song et al., 2020) is only related to the estimation of noise \\(\\mathbf{\\epsilon_{\\theta}}\\), which reflects the model\'s guidance for the current latent \\(\\mathbf{z}_{t}\\) update. In L2I model, \\(\\mathbf{\\epsilon}_{t}^{\\text{layout}}\\)demonstrates more directive toward compositionality (Li et al., 2023), whereas in T2I model, \\(\\mathbf{\\epsilon}_{t}^{\\text{text}}\\) exhibits more directive toward realism (Rombach et al., 2022). A feasible and yet untapped solution is to inject the predictive noise of T2I model \\(\\mathbf{\\epsilon}_{t}^{\\text{text}}\\) into the predictive noise of L2I model \\(\\mathbf{\\epsilon}_{t}^{\\text{layout}}\\). However, the predictive noise from different models has its own intensity distribution, contributing differently to the generated results at different timesteps and positions. Based on this, we design a novel balancer that achieves dynamic equilibrium between the two models\' strengths at every position \\(i\\) in the noise for every timestep \\(t\\). This is accomplished by analyzing the influence of each model\'s predicted noise. Specifically, we first set the same coefficient for the predicted noise of each model to represent their influence before the first sampling step:\n' +
      '\n' +
      '\\[\\mathbf{Coe}_{T}^{\\text{text}}=\\mathbf{Coe}_{T}^{\\text{layout}}\\sim\\mathcal{N}( \\mathbf{0},\\mathbf{I}) \\tag{5}\\]\n' +
      '\n' +
      'In order to regularize the influence of each model, we per\n' +
      '\n' +
      'Figure 3: An overview of RealCompo framework for text-to-image generation. We first use LLM to obtain the corresponding layout according to the input text prompt. Next, the balancer dynamically updates the influence by analyzing the cross-attention maps derived from each model’s current influence at each denoising step.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n' +
      'noise based on the gradient of the loss function, which is a novel and stable method well-suited to our task.\n' +
      '\n' +
      '### Analysis of RealCompo\'s Extended Applications\n' +
      '\n' +
      'In order to enhance the generalization of RealCompo, we explore different combinations of L2I and T2I models. Our experiments found that not all L2I models pay attention to parts outside the object boxes as GLIGEN does. By designing energy functions to update latents, some methods\' (Chen et al., 2024; Xie et al., 2023) cross-attention maps are similar to T2I models, where each token\'s cross-attention map focuses on the entity that the token refers to. Therefore, we use the following loss function for such L2I models:\n' +
      '\n' +
      '\\[\\mathcal{L}(\\mathcal{A}_{t-1}^{\\text{text}},\\mathcal{A}_{t-1}^{\\text{layout }})=\\sum_{c}\\sum_{b}\\left(1-\\frac{\\sum_{i}\\mathcal{A}_{(ij_{b},t-1)}^{c}\\odot \\mathcal{M}_{b}}{\\sum_{i}\\mathcal{A}_{(ij_{b},t-1)}^{c}}\\right) \\tag{12}\\]\n' +
      '\n' +
      'where \\(c\\in\\{\\text{text},\\text{layout}\\}\\). We utilize this loss function for certain L2I models mentioned above to enhance its focus on generating objects within the boxes. We apply the Eq. 10 principle to improve the models\' localization capability. Similarly, we update each model\'s coefficient using Eq. 11. The complete sampling process is detailed in Algorithm 1. We have highlighted the innovations of our method in blue.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Comparison with T2I and L2I Models\n' +
      '\n' +
      'Implementation DetailsOur RealCompo is a generic, scalable framework that can achieve the complementary advantages of the model with any chosen LLMs, T2I, and L2I models. We selected GPT-4 (Achiam et al., 2023) as the layout generator in our experiments. We chose SD1.5 (Rombach et al., 2022) as the base backbone for the T2I model and GLIGEN (Li et al., 2023) as the base backbone for the L2I model. The detailed rules for generating layouts by GPT-4 are described in Appendix A.1.\n' +
      '\n' +
      'Baselines and BenchmarkWe compare our method on T2I-CompBench (Huang et al., 2023) with the outstanding T2I model Stable Diffusion v1.5 (Rombach et al., 2022), L2I models GLIGEN (Li et al., 2023) and LMD+ (Lian et al., 2023) in three main compositional scenarios: **(i) Attribute Binding**. The corresponding attribute in the text prompt should be bound to the correct object under this circumstance. **(ii) Numeric Accuracy**. The text prompt involves multiple objects with multiple quantities for each object under this circumstance. At the same time, we add here the spatial relationship between objects for testing (e.g., "in front of," "on the right," "on the top of," etc.) **(iii) Non-Spatial Relationship**. The text prompt describes the interactions between two objects, e.g. ("look at," "speak to, " wear," "hold," etc.).\n' +
      '\n' +
      'Main ResultsAs demonstrated in Table 1, RealCompo achieved SOTA performance on all six evaluation tasks. Existing methods do not give satisfactory results regarding attribute binding compared to Realcompo. This is due to insufficient information to support the text prompt input alone. When the text prompt involves multiple attributes and objects, the model will confuse the association between attributes and objects. RealCompo analyses the objects and attributes involved in the text prompt using LLM and not only adds layout as an input but also "pre-binds" attributes to objects on the input side. At the same time, combining with the L2I model\'s characteristics of "strong attribute matching and localization ability," we achieve better results in this scenario. We find that existing methods are much less capable of generating spatial relationships than non-spatial relationships, such as the outstanding T2I models Stsble Diffusion v2 and DALL-E 2. This is because it is difficult to give the model an accurate positional description in the\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{3}{c}{**Attribute Binding**} & \\multicolumn{3}{c}{**Object Relationship**} & \\multirow{2}{*}{**Complex\\(\\uparrow\\)**} \\\\ \\cline{2-2} \\cline{4-6}  & **Color**\\(\\uparrow\\) & **Shape\\(\\uparrow\\)** & **Texture\\(\\uparrow\\)** & **Spatial\\(\\uparrow\\)** & **Non-Spatial\\(\\uparrow\\)** \\\\ \\hline Stable Diffusion v1.4 (Rombach et al., 2022) & 0.3765 & 0.3576 & 0.4156 & 0.1246 & 0.3079 & 0.3080 \\\\ Stable Diffusion v2 (Rombach et al., 2022) & 0.5065 & 0.4221 & 0.4922 & 0.1342 & 0.3096 & 0.3386 \\\\ Composable Diffusion (Liu et al., 2022) & 0.4063 & 0.3299 & 0.3645 & 0.0800 & 0.2980 & 0.2898 \\\\ Structured Diffusion (Feng et al., 2023) & 0.4990 & 0.4218 & 0.4900 & 0.1386 & 0.3111 & 0.3355 \\\\ Attn-Exct v2 (Chefer et al., 2023) & 0.6400 & 0.4517 & 0.5963 & 0.1455 & 0.3109 & 0.3401 \\\\ GORS (Huang et al., 2023) & 0.6603 & 0.4785 & 0.6287 & 0.1815 & 0.3193 & 0.3328 \\\\ DALL-E 2 (Ramesh et al., 2022) & 0.5750 & 0.5464 & 0.6374 & 0.1283 & 0.3043 & 0.3696 \\\\ SDXL (Betker et al., 2023) & 0.6369 & 0.5408 & 0.5637 & 0.2032 & 0.3110 & 0.4091 \\\\ PixArt-o (Chen et al., 2023) & 0.6886 & 0.5582 & 0.7044 & 0.2082 & 0.3179 & 0.4117 \\\\ ConPreDiff (Yang et al., 2024) & 0.7019 & 0.5637 & 0.7021 & 0.2362 & 0.3195 & 0.4184 \\\\ \\hline\n' +
      '**RealCompo (Ours)** & 0.9399 & 0.5943 & 0.9206 & 0.2496 & 0.3230 & 0.4525 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Evaluation results on T2I-CompBench (Huang et al., 2023). RealCompo consistently demonstrates the best performance regarding attribute binding, object relationships, and complex compositions. We denote the best score in \\(\\overline{\\text{blue}}\\), and the second-best score in \\(\\overline{\\text{green}}\\). The baseline data is quoted from Chen et al. (2023).\n' +
      '\n' +
      'corresponding cross-attention maps of the tokens expressing the spatial relationships, suggesting that the model\'s ability to understand spatial-relationship words is inferior and that adding additional auxiliary modalities as input is necessary. RealCompo effectively fills this gap by using the L2I model to visually give the positional relationships of objects through layout. For these reasons, RealCompo dynamically combines the T2I and L2I models to generate complex situations and achieves satisfactory results. As shown in Fig. 4, compared to the current outstanding L2I models GLIGEN and LMD+, we can achieve a high level of realism while keeping the attributes of the objects matched and the number of positions generated correctly. We attribute this to the dynamic balancing approach that allows the T2I model to maximize its semantic generation capabilities independent of the layout. The quantitative results presented in Table 1 demonstrate that RealCompo achieves optimal performance for the compositional generation task in text-to-image generation.\n' +
      '\n' +
      'Furthermore, it should be noted that even when balancing two models, RealCompo increases slightly in inference time compared to a single T2I model and a single L2I model. We argue that it is worthwhile to use that time in exchange for high-quality generation results.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'Qualitative ComparisonsTo explore the generalisability of RealCompo for arbitrary models, we choose two T2I models, Stable Diffusion v1.5 (Rombach et al., 2022) and TokenCompose (Wang et al., 2023), and two L2I models, GLIGEN (Li et al., 2023) and Layout Guidance (LayGuide) (Chen et al., 2024). We combine them two by two, yielding four versions of the model RealCompo v1-v4. Our baselines select outstanding Stable Diffusion v1.5 as well as LMD+ (Lian et al., 2023). The experimental results are shown in Fig. 5. The four versions of RealCompo have a high degree of realism in generating images and achieving desirable results regarding instance composition. This is attributed to RealCompo dynamically combining the strengths of the T2I and L2I models and can seamlessly switch between models due to its simple and training-free framework. We also found that RealCompo using GLIGEN as the L2I model is better than using LayGuide in generating objects that match the layout, e.g., RealCompo v4 generates results in the first and third rows, where "popcorms" and "sunflowers" do not fill up the bounding box, which is somehow due to the better\n' +
      '\n' +
      'Figure 4: Qualitative comparison between our RealCompo and the outstanding text-to-image model Stable Diffusion v1.5 (Rombach et al., 2022), as well as the layout-to-image models, GLIGEN (Li et al., 2023) and LMD+ (Lian et al., 2023). Colored text denotes the advantages of RealCompo in generating results\n' +
      '\n' +
      'performance of the base model GLIGEN than LayGuide. Therefore, when combined with the more powerful T2I and L2I models, RealCompo will show more satisfactory results.\n' +
      '\n' +
      'Gradient AnalysisWe selected RealCompo v3 and v4 to analyze the change of gradient in the denoising process in Eq. 11. As shown in Fig. 6, we use the same prompt and random number seed to visualize the gradient magnitude changes corresponding to T2I and L2I for each model version. We observe that the gradient magnitude change of RealCompo v4 changes more in the early stages of the denoising process. We argue that TokenCompose, which enhances the composition capability of multiple-object generation by fine-tuning the model using segmentation masks, is mutually redundant with the layout-based multiple-object generation and that TokenCompose\'s positioning of objects is not necessarily within the bounding box. Therefore, RealCompo must focus on balancing the positioning of TokenCompose and layout in the pre-denoising stage, so its gradient is not stable compared to RealCompo v3. In addition, due to LayGuide being weaker than GLIGEN in terms of positioning ability, RealCompo v4 has the problem of generating objects with less filling of the bounding box in some rare cases, as mentioned before.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'In this paper, to solve the challenge of multiple-object compositional text-to-image generation, we propose the SOTA training-free and transferred-friendly framework RealCompo. In RealCompo, we use LLM to analyze the text prompt to get the layout and implement the attribute pre-binding of multiple-objects. We propose a novel balancer that dynamically combines the advantages of the T2I and L2I models to achieve realism and compositionality generation of high quality. In addition, our RealCompo can be generalized to any LLMs, T2I, and L2I models and maintains strong generation results. In future work, we will continue to improve this framework by using more powerful models as the backbone of RealCompo and will also explore its generalization to more complex modalities.\n' +
      '\n' +
      'Figure 5: Qualitative comparison of RealCompo’s generalization to different models: We select two T2I models: Stable Diffusion v1.5 (Rombach et al., 2022), TokenCompose (Wang et al., 2023b), two L2I models GLIGEN (Li et al., 2023b), Layout Guidance (LayGuide) (Chen et al., 2024), and combine them in pairs to obtain four versions of RealCompo. We demonstrate that RealCompo has strong generalization and generality to different models, achieving a remarkable level of both fidelity and precision in aligning with text prompts.\n' +
      '\n' +
      'Figure 6: Changes of gradient magnitude in Eq. 11 during denoising process for the T2I and L2I models of RealCompo v3 and v4.\n' +
      '\n' +
      '## Impact Statements\n' +
      '\n' +
      'This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Balaji et al. (2022) Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., et al. effiffi: Text-to-image diffusion models with an ensemble of expert denoisers. _arXiv preprint arXiv:2211.01324_, 2022.\n' +
      '* Bar-Tal et al. (2023) Bar-Tal, O., Yariv, L., Lipman, Y., and Dekel, T. Multi-diffusion: Fusing diffusion paths for controlled image generation. _arXiv preprint arXiv:2302.08113_, 2023.\n' +
      '* Betker et al. (2023) Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al. Improving image generation with better captions. _Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf_, 2:3, 2023.\n' +
      '* Chefer et al. (2023) Chefer, H., Alaluf, Y., Vinker, Y., Wolf, L., and Cohen-Or, D. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. _ACM Transactions on Graphics (TOG)_, 42(4):1-10, 2023.\n' +
      '* Chen et al. (2023a) Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., et al. Pixart-\\(\\alpha\\): Fast training of diffusion transformer for photorealistic text-to-image synthesis. _arXiv preprint arXiv:2310.00426_, 2023a.\n' +
      '* Chen et al. (2024) Chen, M., Laina, I., and Vedaldi, A. Training-free layout control with cross-attention guidance. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pp. 5343-5353, 2024.\n' +
      '* Chen et al. (2023b) Chen, X., Liu, Y., Yang, Y., Yuan, J., You, Q., Liu, L.-P., and Yang, H. Reason out your layout: Evoking the layout master from large language models for text-to-image synthesis. _arXiv preprint arXiv:2311.17126_, 2023b.\n' +
      '* Fan et al. (2023) Fan, W.-C., Chen, Y.-C., Chen, D., Cheng, Y., Yuan, L., and Wang, Y.-C. F. Frido: Feature pyramid diffusion for complex scene image synthesis. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pp. 579-587, 2023.\n' +
      '* Feng et al. (2023) Feng, W., He, X., Fu, T.-J., Jampani, V., Akula, A. R., Narayana, P., Basu, S., Wang, X. E., and Wang, W. Y. Training-free structured diffusion guidance for compositional text-to-image synthesis. In _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Foley et al. (2023) Foley, M., Rawat, A., Lee, T., Hou, Y., Picco, G., and Zizzo, G. Matching pairs: Attributing fine-tuned models to their pre-trained large language models. _arXiv preprint arXiv:2306.09308_, 2023.\n' +
      '* Gani et al. (2023) Gani, H., Bhat, S. F., Naseer, M., Khan, S., and Wonka, P. Llm blueprint: Enabling text-to-image generation with complex and detailed prompts. _arXiv preprint arXiv:2310.10640_, 2023.\n' +
      '* Gu et al. (2023) Gu, J., Zhai, S., Zhang, Y., Susskind, J., and Jaitly, N. Matryoshka diffusion models. _arXiv preprint arXiv:2310.15111_, 2023.\n' +
      '* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* Hu et al. (2024) Hu, H., Chan, K. C., Su, Y.-C., Chen, W., Li, Y., Sohn, K., Zhao, Y., Ben, X., Gong, B., Cohen, W., et al. Instruct-imagen: Image generation with multi-modal instruction. _arXiv preprint arXiv:2401.01952_, 2024.\n' +
      '* Huang et al. (2023a) Huang, K., Sun, K., Xie, E., Li, Z., and Liu, X. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. _arXiv preprint arXiv:2307.06350_, 2023a.\n' +
      '* Huang et al. (2023b) Huang, Z., Chan, K. C., Jiang, Y., and Liu, Z. Collaborative diffusion for multi-modal face generation and editing. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 6080-6090, 2023b.\n' +
      '* Kazemi et al. (2022) Kazemi, M., Kim, N., Bhatia, D., Xu, X., and Ramachandran, D. Lambada: Backward chaining for automated reasoning in natural language. _arXiv preprint arXiv:2212.13894_, 2022.\n' +
      '* Kim et al. (2023) Kim, Y., Lee, J., Kim, J.-H., Ha, J.-W., and Zhu, J.-Y. Dense text-to-image generation with attention modulation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 7701-7711, 2023.\n' +
      '* Li et al. (2023a) Li, X., Lv, K., Yan, H., Lin, T., Zhu, W., Ni, Y., Xie, G., Wang, X., and Qiu, X. Unified demonstration retriever for in-context learning. _arXiv preprint arXiv:2305.04320_, 2023a.\n' +
      '* Li et al. (2023b) Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., and Lee, Y. J. Gligen: Open-set grounded text-to-image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 22511-22521, 2023b.\n' +
      '\n' +
      '* Lian et al. (2023) Lian, L., Li, B., Yala, A., and Darrell, T. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. _arXiv preprint arXiv:2305.13655_, 2023.\n' +
      '* Liu et al. (2022) Liu, N., Li, S., Du, Y., Torralba, A., and Tenenbaum, J. B. Compositional visual generation with composable diffusion models. In _European Conference on Computer Vision_, pp. 423-439. Springer, 2022.\n' +
      '* Meral et al. (2023) Meral, T. H. S., Simsar, E., Tombari, F., and Yanardag, P. Conform: Contrast is all you need for high-fidelity text-to-image diffusion models. _arXiv preprint arXiv:2312.06059_, 2023.\n' +
      '* Min et al. (2022) Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? _arXiv preprint arXiv:2202.12837_, 2022.\n' +
      '* Nichol et al. (2022) Nichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., Mcgrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In _International Conference on Machine Learning_, pp. 16784-16804. PMLR, 2022.\n' +
      '* Podell et al. (2023) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* Qu et al. (2023) Qu, L., Wu, S., Fei, H., Nie, L., and Chua, T.-S. Layoutllm-t2i: Eliciting layout guidance from llm for text-to-image generation. In _Proceedings of the 31st ACM International Conference on Multimedia_, pp. 643-654, 2023.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* Si et al. (2023) Si, C., Friedman, D., Joshi, N., Feng, S., Chen, D., and He, H. Measuring inductive biases of in-context learning with underspecified demonstrations. _arXiv preprint arXiv:2305.13299_, 2023.\n' +
      '* Sohl-Dickstein et al. (2015) Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pp. 2256-2265. PMLR, 2015.\n' +
      '* Song et al. (2020) Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020a.\n' +
      '* Song et al. (2020) Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020b.\n' +
      '* Sun et al. (2023) Sun, J., Fu, D., Hu, Y., Wang, S., Rassin, R., Juan, D.-C., Alon, D., Herrmann, C., van Steenkiste, S., Krishna, R., et al. Dreamsync: Aligning text-to-image generation with image understanding feedback. _arXiv preprint arXiv:2311.17946_, 2023.\n' +
      '* Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* Wang et al. (2023a) Wang, R., Chen, Z., Chen, C., Ma, J., Lu, H., and Lin, X. Compositional text-to-image synthesis with attention map control of diffusion models. _arXiv preprint arXiv:2305.13921_, 2023a.\n' +
      '* Wang et al. (2023b) Wang, Z., Sha, Z., Ding, Z., Wang, Y., and Tu, Z. Tokencompose: Grounding diffusion with token-level supervision. _arXiv preprint arXiv:2312.03626_, 2023b.\n' +
      '* Wu et al. (2023a) Wu, H., Chang, K.-W., Wu, Y.-K., and Lee, H.-y. Speech-gen: Unlocking the generative power of speech language models with prompts. _arXiv preprint arXiv:2306.02207_, 2023a.\n' +
      '* Wu et al. (2023b) Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. _arXiv preprint arXiv:2308.08155_, 2023b.\n' +
      '* Wu et al. (2023c) Wu, Q., Liu, Y., Zhao, H., Bui, T., Lin, Z., Zhang, Y., and Chang, S. Harnessing the spatial-temporal attention of diffusion models for high-fidelity text-to-image synthesis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 7766-7776, 2023c.\n' +
      '* Wu et al. (2023d) Wu, T.-H., Lian, L., Gonzalez, J. E., Li, B., and Darrell, T. Self-correcting llm-controlled diffusion models. _arXiv preprint arXiv:2311.16090_, 2023d.\n' +
      '\n' +
      'Xie, J., Li, Y., Huang, Y., Liu, H., Zhang, W., Zheng, Y., and Shou, M. Z. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 7452-7461, 2023.\n' +
      '* Xu et al. (2023) Xu, Y., Zhao, Y., Xiao, Z., and Hou, T. Ufogen: You forward once large scale text-to-image generation via diffusion gans. _arXiv preprint arXiv:2311.09257_, 2023.\n' +
      '* Xue et al. (2023) Xue, Z., Song, G., Guo, Q., Liu, B., Zong, Z., Liu, Y., and Luo, P. Raphael: Text-to-image generation via large mixture of diffusion paths. _arXiv preprint arXiv:2305.18295_, 2023.\n' +
      '* Yang et al. (2022) Yang, L., Huang, Z., Song, Y., Hong, S., Li, G., Zhang, W., Cui, B., Ghanem, B., and Yang, M.-H. Diffusion-based scene graph to image generation with masked contrastive pre-training. _arXiv preprint arXiv:2211.11138_, 2022.\n' +
      '* Yang et al. (2023a) Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Zhang, W., Cui, B., and Yang, M.-H. Diffusion models: A comprehensive survey of methods and applications. _ACM Computing Surveys_, 56(4):1-39, 2023a.\n' +
      '* Yang et al. (2024a) Yang, L., Liu, J., Hong, S., Zhang, Z., Huang, Z., Cai, Z., Zhang, W., and Cui, B. Improving diffusion-based image synthesis with context prediction. _Advances in Neural Information Processing Systems_, 36, 2024a.\n' +
      '* Yang et al. (2024b) Yang, L., Yu, Z., Meng, C., Xu, M., Ermon, S., and Cui, B. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal l1ms. _arXiv preprint arXiv:2401.11708_, 2024b.\n' +
      '* Yang et al. (2023b) Yang, Z., Wang, J., Gan, Z., Li, L., Lin, K., Wu, C., Duan, N., Liu, Z., Liu, C., Zeng, M., and Wang, L. Reco: Region-controlled text-to-image generation. In _CVPR_, 2023b.\n' +
      '* Ye et al. (2023) Ye, Y., Cai, J., Zhou, H., Li, G., Zhang, Y., Song, Z., Gao, C., Yu, J., and Yang, W. Progressive text-to-image diffusion with soft latent direction. _arXiv preprint arXiv:2309.09466_, 2023.\n' +
      '* Zhang et al. (2023) Zhang, L., Rao, A., and Agrawala, M. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 3836-3847, 2023.\n' +
      '* Zheng et al. (2023) Zheng, G., Zhou, X., Li, X., Qi, Z., Shan, Y., and Li, X. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 22490-22499, 2023.\n' +
      '\n' +
      '## Appendix A Additional Analysis\n' +
      '\n' +
      '### LLM-based Layout Generation\n' +
      '\n' +
      'Large Language Models (LLMs) have witnessed remarkable advancements in recent years (Touvron et al., 2023; Kazemi et al., 2022). Due to their robust language comprehension, induction, reasoning, and summarization capabilities, LLMs have made significant strides in the Natural Language Processing (NLP) tasks (Foley et al., 2023; Wu et al., 2023a). In the context of multiple-object generation, generative models exhibit a relatively weaker understanding of language, this is reflected in the poor compositionality of the generated images. Consequently, exploring ways to harness the LLMs\' inferential and imaginative capacities to facilitate their collaboration with generative models, thereby producing images that adhere to the prompt, offers substantial research potential.\n' +
      '\n' +
      'In our task, we leverage the in-context learning (ICL) (Li et al., 2023; Si et al., 2023) capability of LLMs to directly infer the layout of all objects based on the user\'s input prompt. This layout is used for subsequent generations of RealCompo, eliminating the need to manually provide a layout for each prompt and achieve pre-binding of multiple objects and attributes. Specifically, as shown in Fig. 7, we construct prompt templates, which encompass descriptions of task rules (instruction), in-context examples (demonstration), and the user\'s input prompt (test). Through imitation reasoning based on the instruction, LLM can provide the layout for each object, where each layout represents the coordinates of the top-left and bottom-right corners of a respective box. We chose the highly capable GPT-4 (Achiam et al., 2023) as our LLM.\n' +
      '\n' +
      'Figure 7: Firstly, the user’s input text is embedded into the prompt template. The template is then parsed using GPT-4 with frozen parameters, which yields descriptions of the objects in the prompt as well as their corresponding layout.\n' +
      '\n' +
      '### Analysis of Gradient Existence in Eq. 11\n' +
      '\n' +
      'Here we set:\n' +
      '\n' +
      '\\[\\mathcal{L}(\\mathcal{A}_{t-1}^{\\text{text}},\\mathcal{A}_{t-1}^{\\text{ layout}})\\!=\\!\\sum_{b}\\mathcal{L}_{b}(\\mathcal{A}_{t-1}^{\\text{text}},\\mathcal{A}_{t-1}^{ \\text{layout}})\\!=\\!\\sum_{b}\\left[\\left(1\\!-\\!\\frac{\\sum_{i}\\mathcal{A}_{(ij_{b },t-1)}^{\\text{text}}\\odot\\mathcal{M}_{b}}{\\sum_{i}\\mathcal{A}_{(ij_{b},t-1)} ^{\\text{text}}}\\right)\\!+\\!\\left(1\\!-\\!\\frac{\\sum_{i}\\mathcal{A}_{(ij_{b},t-1) }^{\\text{layout}}\\odot(1\\!-\\!\\mathcal{M}_{b})}{\\sum_{i}\\mathcal{A}_{(ij_{b},t- 1)}^{\\text{layout}}}\\right)\\right] \\tag{13}\\]\n' +
      '\n' +
      'If the loss function is given by Eq. 10, the gradient in Eq. 11 can be derived as follows:\n' +
      '\n' +
      '\\[\\frac{\\partial\\mathcal{L}\\left(\\mathcal{A}_{t-1}^{\\text{text}}, \\mathcal{A}_{t-1}^{\\text{layout}}\\right)}{\\partial\\mathbf{Coe}_{t}^{c}} \\tag{14}\\] \\[= \\sum_{b}\\left[\\frac{\\partial\\mathcal{L}_{b}\\left(\\mathcal{A}_{t- 1}^{\\text{text}},\\mathcal{A}_{t-1}^{\\text{layout}}\\right)}{\\partial\\mathcal{A }_{(j_{b},t-1)}^{c}}\\frac{\\partial\\mathcal{A}_{(j_{b},t-1)}^{c}}{\\partial \\mathbf{z}_{t-1}}\\frac{\\partial\\mathbf{z}_{t-1}}{\\partial\\mathbf{\\epsilon}_{t}}\\frac{ \\partial\\mathbf{\\epsilon}_{t}}{\\partial\\mathbf{\\xi}_{t}^{c}}\\frac{\\partial\\mathbf{\\xi}_{t} ^{c}}{\\partial\\mathbf{Coe}_{t}^{c}}\\right]\\] \\[= \\sum_{b}\\left[\\frac{\\partial\\mathcal{L}_{b}\\left(\\mathcal{A}_{t- 1}^{\\text{text}},\\mathcal{A}_{t-1}^{\\text{layout}}\\right)}{\\partial\\mathcal{A }_{(j_{b},t-1)}^{c}}\\frac{\\partial\\mathcal{A}_{(j_{b},t-1)}^{c}}{\\partial\\mathbf{z }_{t-1}}\\frac{\\partial\\mathbf{z}_{t-1}}{\\partial\\mathbf{\\epsilon}_{t}}\\frac{\\partial \\mathbf{\\epsilon}_{t}}{\\partial\\mathbf{\\xi}_{t}^{c}}\\frac{\\exp\\left(\\mathbf{Coe}_{t}^{ \\text{text}}+\\mathbf{Coe}_{t}^{\\text{layout}}\\right)}{\\left(\\exp\\left(\\mathbf{Coe}_{ t}^{\\text{text}}\\right)+\\exp\\left(\\mathbf{Coe}_{t}^{\\text{layout}}\\right)\\right)^{2}}\\right]\\] \\[= \\sum_{b}\\left[\\frac{\\partial\\mathcal{L}_{b}\\left(\\mathcal{A}_{t- 1}^{\\text{text}},\\mathcal{A}_{t-1}^{\\text{layout}}\\right)}{\\partial\\mathcal{A }_{(j_{b},t-1)}^{c}}\\frac{\\partial\\mathcal{A}_{(j_{b},t-1)}^{c}}{\\partial\\mathbf{z }_{t-1}}\\frac{\\partial\\mathbf{z}_{t-1}}{\\partial\\mathbf{\\epsilon}_{t}}\\frac{\\mathbf{ \\epsilon}_{t}^{c}\\cdot\\exp\\left(\\mathbf{Coe}_{t}^{\\text{text}}+\\mathbf{Coe}_{t}^{ \\text{layout}}\\right)}{\\left(\\exp\\left(\\mathbf{Coe}_{t}^{\\text{text}}\\right)+\\exp \\left(\\mathbf{Coe}_{t}^{\\text{layout}}\\right)\\right)^{2}}\\right]\\] \\[= \\sum_{b}\\left[\\frac{\\partial\\mathcal{L}_{b}\\left(\\mathcal{A}_{t- 1}^{\\text{text}},\\mathcal{A}_{t-1}^{\\text{layout}}\\right)}{\\partial\\mathcal{A }_{(j_{b},t-1)}^{c}}\\frac{\\partial\\mathcal{A}_{(j_{b},t-1)}^{c}}{\\partial\\mathbf{z }_{t-1}}\\left(\\sqrt{1-\\bar{\\alpha}_{t-1}-\\sigma^{2}}\\!-\\!\\frac{\\sqrt{1-\\bar{ \\alpha}_{t}}}{\\sqrt{\\alpha_{t}}}\\right)\\frac{\\mathbf{\\epsilon}_{t}^{c}\\cdot\\exp \\left(\\mathbf{Coe}_{t}^{\\text{text}}+\\mathbf{Coe}_{t}^{\\text{layout}}\\right)}{\\left( \\exp\\left(\\mathbf{Coe}_{t}^{\\text{text}}\\right)+\\exp\\left(\\mathbf{Coe}_{t}^{\\text{ layout}}\\right)\\right)^{2}}\\right]\\]\n' +
      '\n' +
      'For T2I and L2I models, we have the following:\n' +
      '\n' +
      '\\[\\frac{\\partial\\mathcal{L}_{b}\\left(\\mathcal{A}_{t-1}^{\\text{ text}},\\mathcal{A}_{t-1}^{\\text{layout}}\\right)}{\\partial\\mathcal{A}_{(j_{b},t-1)}^{ \\text{text}}}=\\frac{\\mathcal{J}\\sum_{i}\\left(\\mathcal{A}_{(ij_{b},t-1)}^{\\text {text}}\\odot\\mathcal{M}_{b}\\right)-\\mathcal{M}_{b}\\sum_{i}\\mathcal{A}_{(ij_{b},t-1)}^{\\text{text}}}{\\left(\\sum_{i}\\mathcal{A}_{(ij_{b},t-1)}^{\\text{text}} \\right)^{2}} \\tag{15}\\]\n' +
      '\n' +
      '\\[\\frac{\\partial\\mathcal{L}_{b}\\left(\\mathcal{A}_{t-1}^{\\text{ text}},\\mathcal{A}_{t-1}^{\\text{layout}}\\right)}{\\partial\\mathcal{A}_{(j_{b},t-1)}^{ \\text{layout}}}=\\frac{\\mathcal{J}\\sum_{i}\\left[\\mathcal{A}_{(ij_{b},t-1)}^{ \\text{layout}}\\odot(1-\\mathcal{M}_{b})\\right]-(1-\\mathcal{M}_{b})\\sum_{i} \\mathcal{A}_{(ij_{b},t-1)}^{\\text{layout}}}{\\left(\\sum_{i}\\mathcal{A}_{(ij_{b},t-1)}^{\\text{layout}}\\right)^{2}} \\tag{16}\\]\n' +
      '\n' +
      'where \\(\\mathcal{J}\\) is a matrix with all elements equal to \\(1\\). All variables in Eq. 14 are known, indicating the existence of the gradient in Eq. 11.\n' +
      '\n' +
      'If the loss function is given by Eq. 12, the gradient in Eq. 11 can be derived as follows:\n' +
      '\n' +
      '\\[\\frac{\\partial\\mathcal{L}\\left(\\mathcal{A}_{t-1}^{\\text{text}}, \\mathcal{A}_{t-1}^{\\text{layout}}\\right)}{\\partial\\mathcal{C}\\mathbf{o}\\mathbf{e}_{t}^ {c}}\\] \\[= \\sum_{b}\\left[\\frac{\\partial\\mathcal{L}_{b}\\left(\\mathcal{A}_{t-1} ^{\\text{text}},\\mathcal{A}_{t-1}^{\\text{layout}}\\right)}{\\partial\\mathcal{A}_{( j_{b},t-1)}^{c}}\\frac{\\partial\\mathcal{A}_{(j_{b},t-1)}^{c}}{\\partial\\mathbf{z}_{t-1}} \\frac{\\partial\\mathbf{z}_{t-1}}{\\partial\\mathbf{\\epsilon}_{t}}\\frac{\\partial\\mathbf{ \\epsilon}_{t}}{\\partial\\mathbf{\\xi}_{t}^{c}}\\frac{\\partial\\mathbf{\\xi}_{t}^{c}}{ \\partial\\mathcal{C}\\mathbf{o}\\mathbf{e}_{t}^{c}}\\right] \\tag{17}\\] \\[= \\sum_{b}\\left[\\frac{\\partial\\mathcal{L}_{b}\\left(\\mathcal{A}_{t-1 }^{\\text{text}},\\mathcal{A}_{t-1}^{\\text{layout}}\\right)}{\\partial\\mathcal{A}_ {(j_{b},t-1)}^{c}}\\frac{\\partial\\mathcal{A}_{(j_{b},t-1)}^{c}}{\\partial\\mathbf{z} _{t-1}}\\left(\\sqrt{1-\\bar{\\alpha}_{t-1}-\\sigma^{2}}-\\frac{\\sqrt{1-\\bar{\\alpha} _{t}}}{\\sqrt{\\bar{\\alpha}_{t}}}\\right)\\frac{\\mathbf{\\epsilon}_{t}^{c}\\cdot\\exp \\left(\\mathbf{C}\\mathbf{o}\\mathbf{e}_{t}^{\\text{text}}+\\mathbf{C}\\mathbf{o}\\mathbf{e}_{t}^{\\text{ layout}}\\right)}{\\left(\\exp\\left(\\mathbf{C}\\mathbf{o}\\mathbf{e}_{t}^{\\text{ text}}\\right)+\\exp\\left(\\mathbf{C}\\mathbf{o}\\mathbf{e}_{t}^{\\text{layout}}\\right)\\right)^{2}}\\right]\\] \\[\\frac{\\partial\\mathcal{L}_{b}\\left(\\mathcal{A}_{t-1}^{\\text{ text}},\\mathcal{A}_{t-1}^{\\text{layout}}\\right)}{\\partial\\mathcal{A}_{(j_{b},t-1)}^{c}}= \\frac{\\mathcal{J}\\sum_{i}\\left(\\mathcal{A}_{(ij_{b},t-1)}^{c}\\odot\\mathcal{M}_ {b}\\right)-\\mathcal{M}_{b}\\sum_{i}\\mathcal{A}_{(ij_{b},t-1)}^{c}}{\\left(\\sum_ {i}\\mathcal{A}_{(ij_{b},t-1)}^{c}\\right)^{2}} \\tag{18}\\]\n' +
      '\n' +
      'where \\(c\\in\\{\\text{text},\\text{layout}\\}\\).\n' +
      '\n' +
      'Therefore, the gradient in Eq. 11 exists for the selection of different loss functions.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>