<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'RealCompo: 사실성과 합성성 사이의 동적 평형이 텍스트-이미지 확산 모델을 개선함\n' +
      '\n' +
      'Xinchen Zhang\n' +
      '\n' +
      '동등 기여도 \\({}^{1}\\) 칭화대학교 \\({}^{2}\\) 북경대학교 \\({}^{3}\\) 중국과학기술대학 \\({}^{4}\\)PicUp.AI\n' +
      '\n' +
      'Ling Yang\n' +
      '\n' +
      'Xiaqi Cai\n' +
      '\n' +
      'Zhaochen Yu\n' +
      '\n' +
      'Jiake Xie\n' +
      '\n' +
      'Ye Tian\n' +
      '\n' +
      'Minkai Xu\n' +
      '\n' +
      'Yong Tang\n' +
      '\n' +
      'Yujiu Yang\n' +
      '\n' +
      'Bin Cui\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '확산 모델은 텍스트 대 이미지 생성에서 놀라운 발전을 달성했다. 그러나 기존의 모델들은 다중 객체 구성 생성에 직면했을 때 여전히 많은 어려움을 가지고 있다. 본 논문에서는 텍스트-투-이미지 모델과 레이아웃-투-이미지 모델의 장점을 활용하여 생성된 이미지의 사실성과 합성성을 동시에 향상시키는 새로운 _training-free_와 _transferred-friendly_ text-to-image 생성 프레임워크인 _RealCompo_를 제안한다. 직관적이고 새로운 _balancer_는 노이즈 제거 과정에서 두 모델의 장점을 동적으로 균형 있게 조정하여 추가 훈련 없이 모든 모델을 플러그 앤 플레이로 사용할 수 있도록 제안한다. 광범위한 실험을 통해 본 논문에서 제안한 RealCompo가 다중 객체 합성 생성에서 기존의 텍스트-이미지 모델 및 레이아웃-이미지 모델보다 우수한 성능을 유지하면서, 생성된 이미지의 만족스러운 사실성과 합성성을 유지할 수 있음을 보인다. 코드는 [https://github.com/Yang_Ling0818/RealCompo](https://github.com/Yang_Ling0818/RealCompo)에서 이용 가능하다.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 확산 모델에는 흥미진진한 발전과 상당한 진전이 있었다(Yang et al., 2024; Song et al., 2020; Ho et al., 2020; Song et al., 2020). 다양한 생성과제 중 텍스트-투-이미지(Text-to-Image; T2I) 생성(Nichol et al., 2022; Hu et al., 2024; Yang et al., 2024)은 커뮤니티 내에서 상당한 관심을 얻고 있다. 대규모 이미지-텍스트 쌍 데이터세트에 대한 광범위한 훈련으로 인해, T2I 모델(예를 들어, Stable Diffusion(Rombach et al., 2022))은 강력한 의미 이해 능력을 나타내어, 의미 정보에 기초하여 매우 사실적인 객체를 생성할 수 있게 하였다(Saharia et al., 2022; Betker et al., 2023; Podell et al., 2023). 그러나, 입력 텍스트가 다수의 객체들 또는 복잡한 관계들을 포함할 때, T2I 모델들로부터의 생성된 이미지들은 때때로 텍스트 프롬프트에 의해 특정된 바와 같은 객체들의 조성성과 정렬되지 않는다(Lian et al., 2023; Bar-Tal et al., 2023). 도. 도 2는 서로 다른 측면에서 Stable Diffusion(Rombach et al., 2022)의 평가 결과를 도시한 것으로, 정확한 개체 수를 생성하지 못하고, 좋은 구성성이 결여되어 있다.\n' +
      '\n' +
      '생성된 이미지의 합성성을 최적화하는 실용적인 방법은 확산 모델에 대한 추가 입력으로서 각 객체의 레이아웃을 제공하는 것이다(Fan et al., 2023; Yang et al., 2023; Wu et al., 2023). T2I 모델들을 제약하기 위한 다른 조건으로서 레이아웃을 사용하면, 이러한 레이아웃-투-이미지(L2I) 모델들(Li 등, 2023; Chen 등, 2024; Xie 등, 2023)은 특정 위치들에서 특정 객체들의 생성을 정밀하게 제어하는 능력을 갖는다. 예를 들어, GLIGEN(Li et al., 2023)은 레이아웃으로부터 포괄적인 정보를 갖는 모델을 트레이닝하기 위해 Gated Self-Attention을 채용한다. 이러한 L2I 방법은 객체 위치 결정 및 카운팅 에러의 취약점을 개선하였지만, 생성 결과의 현실성은 만족스럽지 못하다. 반대로,\n' +
      '\n' +
      '도 1: T2I-Compbench(Huang et al., 2023), RealCompo는 6개의 평가 과제 모두에 대해 SOTA 성능을 달성하였다.\n' +
      '\n' +
      'T2I 모델은 사실성이 높은 객체를 생성할 수 있지만 객체의 수와 위치에 대한 텍스트 프롬프트를 따르기는 어렵다. L2I 모형과 T2I 모형 사이에는 상당한 보완적 공간이 존재한다.\n' +
      '\n' +
      '각 모델의 교차 주의 맵을 조사하기 위해 예비 실험을 수행한다. 도 1에 도시된 바와 같다. 도 2를 참조하면, T2I 모델 Stable Diffusion은 제어되지 않는 조건 하에서 다중 객체 생성 태스크들에서 다중 객체들 및 이들의 공간적 관계(Huang et al., 2023)를 이해하기 위해 고군분투하고, 생성된 이미지들의 차선의 구성성을 초래한다. 그러나, L2I 모델 GLIGEN(Li 등, 2023)은 주로 각 토큰의 교차-어텐션 맵 내의 오브젝트 박스들 외부의 영역들에 초점을 맞춘다. 이는 레이아웃이 객체 배치에 지나치게 강한 제약을 가함을 나타냅니다. 결과적으로 L2I 모델의 사실성과 속성 바인딩은 모양과 색상과 같은 의미론적 주의가 부족하여 개선의 여지가 있다.\n' +
      '\n' +
      '이를 위해, 우리는 생성된 이미지에서 사실성과 합성성 사이의 동적 평형을 달성하기 위해 새로운 _balancer_를 활용하는 일반적인 _training-free_ text-to-image 생성 프레임워크 _RealCompo_를 소개한다. 우리는 먼저 LLM의 In-Context Learning(Min et al., 2022) 능력을 활용하여 필수 객체의 레이아웃을 추론하고 입력된 텍스트 프롬프트로부터 속성과 객체의 "사전 바인딩"을 달성한다. 그런 다음 사전 훈련된 L2I 및 T2I 모델을 통합하기 위한 혁신적인 _balancer_를 소개합니다. 이 밸런서는 각 잡음 제거 단계에서 각 모델의 교차 주의 맵의 분석을 통해 예측 조합의 각 계수를 자동으로 조정하도록 조작된다. 이 접근법은 두 모델의 강도를 통합하여 생성된 이미지의 사실성과 구성성을 동적으로 밸런싱할 수 있다. 다수의 확산 모델들을 병합하기 위한 방법들이 존재하지만(Xue et al., 2023; Balaji et al., 2022), 이들은 추가적인 훈련을 필요로 하고 다른 시나리오들 및 모델들에 일반화하는 능력이 부족하기 때문에 그들의 사용에 유연성이 부족하다. 우리의 방법은 훈련 없이 모델 구성을 수행하는 첫 번째 방법으로 모든 모델 간의 원활한 전환을 가능하게 한다.\n' +
      '\n' +
      '본 논문에서 제안하는 RealCompo는 현실감과 합성성을 동시에 갖는 영상을 생성하는데 있어서 우수한 성능을 보이기 위해 다양한 실험을 수행하였다. Figure 2에 예시된 바와 같이 RealCompo는 T2I와 L2I 모델의 장점을 동적으로 균형 있게 결합한다. 레이아웃 정보를 통합하여 각 토큰의 크로스 어텐션 맵에서 각 객체의 포커스에 대한 특성을 유지하면서 T2I 모델에 정밀한 객체 포지셔닝을 주입한다. 이를 통해 매우 사실적인 이미지를 생성할 수 있습니다. 동시에 각 토큰의 교차 주의 맵에서 박스 외부의 특징에 대한 L2I 모델의 주의를 유지하여 생성된 이미지에서 강력한 현지화 능력과 최적의 합성성을 보여준다. RealCompo는 T2I와 L2I 모델 모두 서로의 단점을 보완하면서 각각의 장점을 유지할 수 있게 한다.\n' +
      '\n' +
      '우리가 아는 한, 텍스트-이미지 생성에서 생성된 이미지의 사실성과 구성성 사이의 균형을 동적으로 달성함으로써 생성된 이미지의 품질이 향상된 것은 이번이 처음이다.\n' +
      '\n' +
      '도 2: 첫 번째 열 단일 T2I 모델에 도시된 바와 같이, 안정 확산(Rombach et al., 2022)은 이미지 생성에서의 조성에서 잘 수행되지 않는다. 두 번째 열은 단일 L2I 모델 GLIGEN(Li et al., 2023)이 각 객체의 바운딩 박스 외부의 부분들을 생성하는 것에 초점을 맞추기 때문에 이미지 생성에서의 현실성에 대해 잘 수행하지 못한다는 것을 보여준다. 세 번째와 네 번째 열에서는 T2I 모델과 L2I 모델 모두에서 뚜렷한 장점을 활용하여 이미지 생성에서 사실성과 구성성 측면에서 우수한 생성 결과를 얻을 수 있다.\n' +
      '\n' +
      '이미지요 임의의 L2I 또는 T2I 모델을 선택할 수 있는 능력으로 RealCompo는 자동으로 균형을 맞춰 시너지 생성을 실현할 수 있습니다. 우리는 RealCompo가 제어 가능하고 구도적인 이미지 생성에서 새로운 연구 관점을 열어준다고 믿는다.\n' +
      '\n' +
      '우리의 주요 기여는 다음과 같이 요약된다.\n' +
      '\n' +
      '* 새로운 _training-free_ 및 _transferred-friendly_ text-to-image 생성 프레임워크 RealCompo를 소개하며, 생성된 이미지의 사실성과 합성성을 평형을 이루어 합성 텍스트-to-image 생성을 향상시킨다.\n' +
      '* RealCompo에서 우리는 각 디노징 타임스텝에서 T2I 및 L2I 모델의 출력을 동적으로 결합하기 위해 새로운 _balancer_를 설계한다. 구도 이미지 생성을 위한 신선한 관점을 제공합니다.\n' +
      '* 이전 SOTA 방법과의 광범위한 정성적 및 정량적 비교는 RealCompo가 다중 객체 및 복잡한 관계를 생성하는데 있어서 성능을 상당히 향상시켰음을 입증한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '최근 몇 년 동안, 텍스트-대-이미지 생성 분야는 상당한 진전을 이루었다(Sun et al., 2023; Xu et al., 2023; Podell et al., 2023). 크게 확산 모델에서의 돌파구에 기인한다(Yang et al., 2023). 대규모 이미지-텍스트 쌍을 이루는 데이터셋에 대한 학습을 통해, T2I 모델인 SD(Rombach et al., 2022), DALL-E 2(Ramesh et al., 2022), MDM(Gu et al., 2023), Pixart-\\(\\alpha\\)(Chen et al., 2023)은 놀라운 생성 능력을 보여주었다. 그러나, 다수의 객체를 생성하는 것에 관해서는 생성된 이미지의 합성성에 여전히 상당한 여지가 있다(Wu et al., 2023). 많은 연구가 제어된 생성을 통해 이 문제를 해결하려고 시도했다(Zhang et al., 2023), 시맨틱 맵(Huang et al., 2023), 장면 그래프(Yang et al., 2022), 레이아웃(Zheng et al., 2023) 등과 같은 추가 모달리티를 제공하여 생성된 이미지에서 객체의 수와 위치의 정확성을 보장하기 위해 모델의 생성 도메인을 제한한다. 그러나, 추가적인 모달리티들의 제약들로 인해, 생성의 현실성은 감소할 수 있다(Li 등, 2023). 나아가, 여러 작품들(Qu et al., 2023; Chen et al., 2023; Ye et al., 2023; Yang et al., 2024)은 LLM(Large Language Models)으로 전처리 프롬프트에 의해 모델들에서의 언어 이해 갭을 해소하려고 시도했다(Achiam et al., 2023; Touvron et al., 2023). 현재 T2I 모델은 세대 현실주의와 구성성 사이에서 균형을 맞추는 것이 어렵다(Yang et al., 2024).\n' +
      '\n' +
      '합성 텍스트-이미지 생성 합성 텍스트-이미지 생성의 개념은 모델에 의해 생성된 이미지들을 시맨틱스와 시각적으로 일치하게 하는 것이다(Huang et al., 2023). 현재 연구는 부정(Liu et al., 2022), 생성 수리력(Lian et al., 2023), 속성 바인딩(Chefer et al., 2023; Feng et al., 2023) 및 공간 추론(Wu et al., 2023)에서 생성 품질을 개선하는 데 중점을 두고 있다. 최근의 연구는 일반적으로 두 가지 유형으로 나눌 수 있다(Wang et al., 2023). 하나는 주로 구성 생성을 위한 교차 주의 맵을 사용하고(Meral et al., 2023; Kim et al., 2023), 다른 하나는 생성을 위한 조건으로서 레이아웃을 제공한다(Gani et al., 2023). 첫 번째 유형의 방법은 특히 입력 텍스트와의 대응을 강조하는 교차 주의 지도의 상세한 분석을 조사한다. Attend-and-Excite(Chefer et al., 2023)는 속성 바인딩(색상 등) 측면에서 모델의 생성 결과를 개선하기 위해 생성 프로세스에서 동적으로 인터벤스한다. 두 번째 유형의 메서드는 레이아웃을 제약 조건으로 제공하여 모델이 이 조건에 맞는 이미지를 생성할 수 있도록 합니다. 이 접근법은 물체가 위치하는 영역을 직접 정의하여, 제1 유형의 방법(Li 등, 2023)에 비해 더 간단하고 관찰 가능하게 한다. 대부분 대형 범위 모델(LLM)을 사용하여 레이아웃을 추론합니다. LMD(Lian et al., 2023)는 LLMs들의 ICL(in-context learning) 능력을 갖는 입력으로서 추가적인 레이아웃을 제공한다. 그러나 이러한 알고리즘은 생성된 이미지의 사실성에 대해 만족스럽지 못하다. 최근 강력한 프레임워크 RPG(Yang et al., 2024)는 멀티모달 LLM을 활용하여 복잡한 생성 작업을 보다 간단한 하위 작업으로 분해하여 생성된 이미지의 만족스러운 사실성과 구성성을 얻는다. 이 작업과 직교하여 T2I와 L2I 모델을 결합하여 사실성과 구성성 사이의 동적 평형을 달성한다.\n' +
      '\n' +
      '## 3 Preliminary\n' +
      '\n' +
      '확산 모델(Ho et al., 2020; Sohl-Dickstein et al., 2015)은 확률적 생성 모델이다. 그들은 학습을 통해 깨끗한 이미지를 생성하기 위해 랜덤 노이즈\\(\\mathbf{x}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{\\mathrm{I})\\)에 대해 다단계 잡음제거를 수행할 수 있다. 구체적으로, 정방향 프로세스에서 깨끗한 이미지\\(\\mathbf{x}_{0}\\)에 가우스 잡음\\(\\mathbf{\\epsilon}\\)이 점진적으로 추가된다:\n' +
      '\n' +
      '\\[\\mathbf{x}_{t}=\\sqrt{\\alpha_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\alpha_{t}}\\mathbf{\\epsilon} \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(\\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{\\mathrm{I})\\) 및 \\(\\alpha_{t}\\)는 잡음 스케줄이다.\n' +
      '\n' +
      '제곱 오차 손실을 최소화하여 훈련을 수행한다:\n' +
      '\n' +
      'mmathcal{L}=\\mathbb{E}_{\\mathbf{x},\\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{\\mathrm{I}),t}\\left[\\left\\|\\mathbf{\\epsilon}-\\mathbf{\\epsilon_{\\theta}(\\mathbf{x}_{t},t}\\right\\|_{2}^{2}\\right]\\tag{2}\\t}\n' +
      '\n' +
      '추정 잡음\\(\\mathbf{\\epsilon_{\\theta}\\)의 파라미터는 실제 잡음\\(\\mathbf{\\epsilon}\\)과 추정 잡음\\(\\mathbf{\\epsilon_{\\theta}(\\mathbf{x}_{t},t)\\) 사이의 손실을 계산하여 단계적으로 갱신된다.\n' +
      '\n' +
      '역과정은 각 단계에서 예측된 잡음\\(\\mathbf{\\epsilon_{\\theta}(\\mathbf{x}_{t},t)\\)에 따라 잡음\\(\\mathbf{x}_{T}\\)에서 시작하여 이를 잡음제거하는 것을 목표로 한다. DDIM(Song et al., 2020)은 잡음 제거 단계를 갖는 결정론적 샘플러이다:\n' +
      '\n' +
      'bf{x}_{t-1}&=\\sqrt{\\bar{\\alpha}_{t-1}}\\left(\\frac{\\mathbf{x}_{t}-\\sqrt{1-\\bar{\\alpha}_{t}}\\mathbf{x}_{t},t\\right}\\sqrt{1-\\bar{\\alpha}_{t-1}-\\sigma\\mathbf{x}_{t}}\\end{split}\\mathbf{x}_{t},t\\right)+\\sqrt{1-\\bar{\\alpha}_{t-1}\\sqrt{1-\\bar{\\alpha}_{t}-\\sigma^{t}\\end{split}\\mathbf{x}_{t}-\\sqrt{1-\\bar{\\alpha}_{t}\\sqrt{1-\\bar{\\alpha}_{t}-\\sigma^{2}\\mathbf\n' +
      '\n' +
      '안정적 확산(Rombach et al., 2022)은 잠재 공간에서 잡음 추가 및 제거를 수행하는 이 분야에서 중요한 발전이다. 구체적으로, SD는 인코더\\(\\mathcal{E}\\)와 디코더\\(\\mathcal{D}\\)으로 구성된 사전 훈련된 오토인코더를 사용한다. 영상(\\mathbf{x}\\)이 주어지면, 인코더(\\mathcal{E}\\)는 잠재공간에 \\(\\mathbf{x}\\)를 매핑하고, 디코더(\\(\\mathcal{D}\\)는 이 영상을 재구성할 수 있다. 즉, \\(\\mathbf{z}=\\mathcal{E}(\\mathbf{x})\\), \\(\\tilde{\\mathbf{x}=\\mathcal{D}(\\mathbf{z})\\). 또한, 안정적인 확산은 조건부 생성을 위한 추가 텍스트 프롬프트 \\(y\\)를 지원한다. \\ (y\\)는 미리 학습된 CLIP(Radford et al., 2021) 텍스트 인코더를 통해 텍스트 토큰 \\(\\tau_{\\theta}(y)\\)으로 변환된다. \\ (\\mathbf{\\epsilon_{\\theta}})는 다음을 통해 트레이닝된다:\n' +
      '\n' +
      '\\!\\\\\\[\\min_{\\mathbf{\\theta}\\mathcal{L}\\!=\\!\\\\\\[\\min_{\\mathbf{\\theta}\\mathcal{L}\\!\\ bb{E}_{\\mathbf{z}\\sim\\mathcal{E}(\\mathbf{x}), \\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}),t}\\left[\\|\\mathbf{\\epsilon}\\!-\\!\\mathbf{\\epsilon_{\\theta}(\\mathbf{z}_{t},t,\\tau_{\\theta}(y))\\|_{2}^{4}\\right]\\tag{N}(\\mathbf{0},\\mathbf{I}),t}\\left[\\|\\mathbf{\\epsilon}\\!-\\!\\mathbf{\\epsilon}\\!-\\!\\mathbf{\\epsilon_{\\theta}}(\\mathbf{z}_{t},t,\\tau_{\\theta}(y))\\|_{2}^{4}\\right]\\tag{N}(\\math\n' +
      '\n' +
      '추론 과정에서 잠재공간으로부터 잡음\\(\\mathbf{z}_{T}\\sim\\mathcal{N}\\left(\\mathbf{0},\\mathbf{I}\\right)\\)을 샘플링한다. Eq를 적용하여. 셋째, 깨끗한 잠재 \\(\\mathbf{z}_{0}\\)을 얻기 위해 단계적 디노이징을 수행한다. 그리고 생성된 영상을 복호기를 통해 복원한다.\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      '이 섹션에서는 생성된 이미지의 사실성과 구성성 사이의 동적 평형을 달성하기 위해 새로운 밸런서를 설계하는 RealCompo 방법을 소개한다. 4.1절에서는 각 잡음에 대한 영향력 통합의 필요성을 분석하고 계수 산출 방법을 제시한다. 섹션 4.2에서 우리는 계수를 동적으로 업데이트하기 위해 훈련 없는 접근법을 사용하는 밸런서에 의해 사용되는 업데이트 규칙에 대한 자세한 설명을 제공한다. 섹션 4.3에서는 L2I 모델의 각 범주에 대한 손실 함수를 설계하면서 RealCompo의 적용을 확장한다.\n' +
      '\n' +
      '### T2I와 L2I 모델의 결합\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 3을 참조하면, 먼저 대용량 언어 모델의 강력한 문맥 내 학습(Wu et al., 2023) 기능을 활용하여 입력된 텍스트 프롬프트를 분석하고 객체와 속성 간의 "사전 바인딩"을 달성하기 위한 정확한 레이아웃을 생성한다. 그런 다음 레이아웃이 L2I 모델의 입력으로 사용됩니다. 본 논문에서는 레이아웃 생성을 위해 GPT-4를 선택한다. 자세한 설명은 부록 A.1을 참조하시기 바랍니다.\n' +
      '\n' +
      '그것은 식에서 분명하다. 3. DDIM(Song et al., 2020)을 이용한 영상의 생성은 현재 잠재된 \\(\\mathbf{z}_{t}\\) 업데이트에 대한 모델의 안내를 반영하는 잡음 \\(\\mathbf{\\epsilon_{\\theta}\\)의 추정과만 관련이 있다. L2I 모델에서는 조성(Li et al., 2023)에 대한 지시가 더 높은 반면, T2I 모델에서는 실감(Rombach et al., 2022)에 대한 지시가 더 높은 것으로 나타났다. 실행가능하지만 아직 개발되지 않은 해결책은 T2I 모델\\(\\mathbf{\\epsilon}_{t}^{\\text{text}\\)의 예측 잡음을 L2I 모델\\(\\mathbf{\\epsilon}_{t}^{\\text{layout}\\)의 예측 잡음에 주입하는 것이다. 그러나 서로 다른 모델의 예측 노이즈는 자체 강도 분포를 가지고 있어 서로 다른 타임스텝과 위치에서 생성된 결과에 다르게 기여한다. 이를 바탕으로 각 타임스텝(t\\)에 대한 잡음에서 각 위치 \\(i\\)에서 두 모델의 강도 사이의 동적 평형을 이루는 새로운 밸런서를 설계하였다. 이는 각 모델의 예측 잡음의 영향을 분석하여 이루어진다. 구체적으로, 첫 번째 샘플링 단계 전에 각 모델의 예측된 노이즈에 대해 동일한 계수를 설정하여 영향을 나타낸다:\n' +
      '\n' +
      '\\[\\mathbf{Coe}_{T}^{\\text{text}=\\mathbf{Coe}_{T}^{\\text{layout}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}} \\tag{5}\\]\n' +
      '\n' +
      '각 모델의 영향을 정규화하기 위해 우리는 각 모델당\n' +
      '\n' +
      '그림 3: 텍스트 대 이미지 생성을 위한 RealCompo 프레임워크의 개요. 우리는 먼저 LLM을 사용하여 입력된 텍스트 프롬프트에 따라 해당 레이아웃을 얻는다. 다음으로, 밸런서는 각 노이즈 제거 단계에서 각 모델의 현재 영향력으로부터 도출된 교차 주의 맵을 분석하여 영향력을 동적으로 업데이트한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n' +
      '손실 함수의 기울기를 기반으로 하는 잡음은 우리의 작업에 잘 맞는 새롭고 안정적인 방법이다.\n' +
      '\n' +
      'RealCompo의 확장된 응용 프로그램의### 분석\n' +
      '\n' +
      'RealCompo의 일반화를 향상시키기 위해 L2I 및 T2I 모델의 다양한 조합을 탐색한다. 우리의 실험은 모든 L2I 모델이 GLIGEN처럼 객체 상자 외부의 부품에 주의를 기울이지 않는다는 것을 발견했다. 레이턴트를 업데이트하기 위한 에너지 함수를 설계함으로써, 일부 방법들\'(Chen et al., 2024; Xie et al., 2023) 교차-어텐션 맵들은 T2I 모델들과 유사하며, 여기서 각각의 토큰의 교차-어텐션 맵은 토큰이 참조하는 엔티티에 초점을 맞춘다. 따라서 우리는 이러한 L2I 모델에 대해 다음과 같은 손실 함수를 사용한다:\n' +
      '\n' +
      '\\mathcal{L}(\\mathcal{A}_{t-1}^{\\text{text},\\mathcal{A}_{t-1}^{\\text{layout}))=\\sum_{c}\\sum_{b}\\left(1-\\frac{\\sum_{i}\\mathcal{A}_{(ij_{b},t-1}}^{c}\\odot\\mathcal{M}_{b}{\\sum_{i}\\mathcal{A}_{(ij_{b},t-1}}^{c}\\right)\\tag{12}\\frac{\\sum_{i}\\mathcal{A}_{(ij_{b},t-1}}}\\t{lout}}}\\t{c}\\left(1-\\frac{\\sum_{i}\\mathcal{A}_{(ij_{b},t-1}}}\\t{l}}\\t{l}}\\t{c}\\left(1-\\frac{\\sum_{i\n' +
      '\n' +
      '여기서 \\(c\\in\\text{text},\\text{layout}\\}). 우리는 위에서 언급한 특정 L2I 모델에 이 손실 함수를 사용하여 상자 내에서 객체를 생성하는 데 초점을 강화한다. 우리는 Eq를 적용한다. 10원리는 모델의 현지화 능력을 향상시킵니다. 마찬가지로, 우리는 Eq를 사용하여 각 모델의 계수를 업데이트한다. 11. 전체 샘플링 프로세스는 알고리즘 1에 자세히 설명되어 있습니다. 우리는 우리의 방법의 혁신을 파란색으로 강조했습니다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### T2I 및 L2I 모델의 비교\n' +
      '\n' +
      '구현 세부사항 RealCompo는 임의의 선택된 LLMs, T2I 및 L2I 모델로 모델의 보완적인 이점을 달성할 수 있는 일반적이고 확장 가능한 프레임워크이다. 실험에서 레이아웃 생성기로 GPT-4(Achiam et al., 2023)를 선택하였다. 우리는 T2I 모델의 기본 백본으로 SD1.5(Rombach et al., 2022)를, L2I 모델의 기본 백본으로 GLIGEN(Li et al., 2023)을 선택했다. GPT-4에 의한 레이아웃 생성에 관한 상세한 규칙은 부록 A.1에 기재되어 있다.\n' +
      '\n' +
      '기준선 및 BenchmarkWe on T2I-CompBench(Huang et al., 2023)에 대한 우리의 방법을 우수한 T2I 모델 Stable Diffusion v1.5(Rombach et al., 2022), L2I 모델 GLIGEN(Li et al., 2023) 및 LMD+(Lian et al., 2023)와 세 가지 주요 구성 시나리오에서 비교한다: **(i) 속성 바인딩**. 텍스트 프롬프트의 해당 속성은 이 상황에서 올바른 개체에 바인딩되어야 합니다. **(ii) 숫자 정확성**. 텍스트 프롬프트는 이러한 상황 하에서 각각의 객체에 대해 다수의 양을 갖는 다수의 객체를 포함한다. 동시에, 우리는 여기에 테스트를 위한 객체들 사이의 공간 관계(예를 들어, "앞쪽", "오른쪽", "위쪽" 등) **(iii) 비공간 관계**를 추가한다. 텍스트 프롬프트는 두 객체 사이의 상호 작용, 예를 들어("보고", "말하기", "착용", "유지" 등을 설명한다.\n' +
      '\n' +
      '표 1에서 입증된 바와 같이, RealCompo는 6가지 평가 과제 모두에서 SOTA 성능을 달성하였다. 기존 방법들은 Realcompo에 비해 속성 바인딩에 대한 만족할 만한 결과를 제시하지 못하고 있다. 이는 텍스트 프롬프트 입력을 단독으로 지원하기에는 불충분한 정보 때문이다. 텍스트 프롬프트가 여러 속성과 객체를 포함할 때 모델은 속성과 객체 간의 연관성을 혼동합니다. RealCompo는 LLM을 사용하여 텍스트 프롬프트에 포함된 객체와 속성을 분석하고 레이아웃은 입력으로 추가할 뿐만 아니라 입력 측면의 객체에 "사전 바인딩" 속성을 추가합니다. 동시에 "강력한 속성 매칭 및 현지화 능력"이라는 L2I 모델의 특성과 결합하여 이 시나리오에서 더 나은 결과를 달성한다. T2I 모델 Stsble Diffusion v2 및 DALL-E 2와 같이 기존의 방법은 비공간적 관계에 비해 공간 관계를 생성할 수 있는 능력이 훨씬 낮다는 것을 알 수 있다. 이는 모델에서 정확한 위치 설명을 제공하기 어렵기 때문이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{3}{c}{**Attribute Binding**} & \\multicolumn{3}{c}{**Object Relationship**} & \\multirow{2}{*}{**Complex\\(\\uparrow\\)**} \\\\ \\cline{2-2} \\cline{4-6}  & **Color**\\(\\uparrow\\) & **Shape\\(\\uparrow\\)** & **Texture\\(\\uparrow\\)** & **Spatial\\(\\uparrow\\)** & **Non-Spatial\\(\\uparrow\\)** \\\\ \\hline Stable Diffusion v1.4 (Rombach et al., 2022) & 0.3765 & 0.3576 & 0.4156 & 0.1246 & 0.3079 & 0.3080 \\\\ Stable Diffusion v2 (Rombach et al., 2022) & 0.5065 & 0.4221 & 0.4922 & 0.1342 & 0.3096 & 0.3386 \\\\ Composable Diffusion (Liu et al., 2022) & 0.4063 & 0.3299 & 0.3645 & 0.0800 & 0.2980 & 0.2898 \\\\ Structured Diffusion (Feng et al., 2023) & 0.4990 & 0.4218 & 0.4900 & 0.1386 & 0.3111 & 0.3355 \\\\ Attn-Exct v2 (Chefer et al., 2023) & 0.6400 & 0.4517 & 0.5963 & 0.1455 & 0.3109 & 0.3401 \\\\ GORS (Huang et al., 2023) & 0.6603 & 0.4785 & 0.6287 & 0.1815 & 0.3193 & 0.3328 \\\\ DALL-E 2 (Ramesh et al., 2022) & 0.5750 & 0.5464 & 0.6374 & 0.1283 & 0.3043 & 0.3696 \\\\ SDXL (Betker et al., 2023) & 0.6369 & 0.5408 & 0.5637 & 0.2032 & 0.3110 & 0.4091 \\\\ PixArt-o (Chen et al., 2023) & 0.6886 & 0.5582 & 0.7044 & 0.2082 & 0.3179 & 0.4117 \\\\ ConPreDiff (Yang et al., 2024) & 0.7019 & 0.5637 & 0.7021 & 0.2362 & 0.3195 & 0.4184 \\\\ \\hline\n' +
      '**RealCompo (Ours)** & 0.9399 & 0.5943 & 0.9206 & 0.2496 & 0.3230 & 0.4525 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: T2I-CompBench(Huang et al., 2023)에 대한 평가 결과. RealCompo는 속성 바인딩, 객체 관계 및 복잡한 구성에 관한 최상의 성능을 일관되게 보여준다. 우리는 \\(\\overline{\\text{green}\\)에서 가장 좋은 점수를 나타내고, \\(\\overline{\\text{green}\\)에서 두 번째로 좋은 점수를 나타낸다. 기준 데이터는 Chen et al.(2023)로부터 인용된다.\n' +
      '\n' +
      '공간 관계를 표현하는 토큰의 대응 교차 주의 맵은 공간 관계 단어를 이해하는 모델의 능력이 떨어지고 입력으로 추가 보조 모달리티를 추가하는 것이 필요함을 시사한다. RealCompo는 L2I 모델을 사용하여 레이아웃을 통해 객체의 위치 관계를 시각적으로 제공함으로써 이러한 격차를 효과적으로 메운다. 이러한 이유로 RealCompo는 T2I와 L2I 모델을 동적으로 결합하여 복잡한 상황을 생성하고 만족스러운 결과를 얻는다. 도 1에 도시된 바와 같다. 도 4를 참조하면, 기존의 우수한 L2I 모델 GLIGEN과 LMD+에 비해 객체의 속성이 일치하고 위치가 정확하게 생성되는 것을 유지하면서 높은 수준의 현실감을 얻을 수 있다. 이를 T2I 모델이 레이아웃과 무관하게 의미 생성 기능을 최대화할 수 있도록 하는 동적 밸런싱 접근법에 기인한다. 표 1에 제시된 정량적 결과는 RealCompo가 텍스트-이미지 생성에서 구성 생성 작업에 대해 최적의 성능을 달성함을 보여준다.\n' +
      '\n' +
      '또한 두 모델의 균형을 맞출 때에도 RealCompo는 단일 T2I 모델 및 단일 L2I 모델에 비해 추론 시간이 약간 증가한다는 점에 유의해야 한다. 우리는 고품질 생성 결과의 대가로 그 시간을 사용하는 것이 가치가 있다고 주장한다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'Rombach et al., 2022)와 TokenCompose (Wang et al., 2023)의 두 가지 T2I 모델과 GLIGEN (Li et al., 2023)과 LayGuide (LayGuide)의 두 가지 L2I 모델을 정성적 비교를 통해 임의의 모델에 대한 RealCompo의 일반화 가능성을 탐색한다. 이를 2개씩 결합하여 RealCompo v1-v4 모델을 4가지 버전으로 구성하여 LMD+ 뿐만 아니라 우수한 Stable Diffusion v1.5를 선정한다 (Lian et al., 2023). 실험 결과는 그림 5와 같다. RealCompo의 4가지 버전은 이미지를 생성하고 인스턴스 구성에 대한 바람직한 결과를 달성하는 데 높은 사실성을 가지고 있다. 이는 RealCompo가 T2I와 L2I 모델의 장점을 동적으로 결합하고 단순하고 훈련 없는 프레임워크로 인해 모델 간에 원활하게 전환할 수 있기 때문이다. 또한 L2I 모델로 GLIGEN을 사용하는 RealCompo가 레이아웃과 일치하는 객체를 생성하는데 LayGuide를 사용하는 것보다 더 낫다는 것을 발견했다. 예를 들어 RealCompo v4는 첫 번째 행과 세 번째 행에서 결과를 생성하는데, 여기서 "팝콘"과 "해바라기"는 경계 상자를 채우지 않는데, 이는 어떻게든 더 나은 결과로 인한 것이다.\n' +
      '\n' +
      '도 4: 우리의 RealCompo와 우수한 텍스트-대-이미지 모델 Stable Diffusion v1.5(Rombach et al., 2022), 뿐만 아니라 레이아웃-대-이미지 모델 GLIGEN(Li et al., 2023) 및 LMD+(Lian et al., 2023) 간의 질적 비교. 컬러 텍스트는 결과 생성에서 RealCompo의 장점을 나타낸다.\n' +
      '\n' +
      '레이가이드보다 기본 모델 GLIGEN의 성능. 따라서 보다 강력한 T2I 및 L2I 모델과 결합하면 RealCompo가 보다 만족스러운 결과를 보일 것이다.\n' +
      '\n' +
      'Gradient Analysis RealCompo v3와 v4를 선택하여 Eq.의 잡음제거 과정에서 Gradient의 변화를 분석하였다. 11. 6, 우리는 각 모델 버전에 대해 T2I 및 L2I에 해당하는 기울기 크기 변화를 시각화하기 위해 동일한 프롬프트 및 난수 시드를 사용한다. 우리는 RealCompo v4의 기울기 크기 변화가 잡음 제거 과정의 초기 단계에서 더 많이 변화한다는 것을 관찰한다. 세그멘테이션 마스크를 사용하여 모델을 미세 조정함으로써 다중 객체 생성의 구성 능력을 향상시키는 TokenCompose는 레이아웃 기반 다중 객체 생성과 상호 중복적이며 TokenCompose의 객체 위치 결정이 반드시 바운딩 박스 내에 있는 것은 아니라고 주장한다. 따라서, RealCompo는 TokenCompose의 위치 결정과 배치의 균형을 사전 디노이징 단계에서 맞추는 것에 초점을 맞추어야 하므로, 그 기울기가 RealCompo v3에 비해 안정적이지 못하다. 또한, LayGuide가 위치 결정 능력 측면에서 GLIGEN보다 약하기 때문에, RealCompo v4는 앞서 언급한 바와 같이 드문 경우에 바운딩 박스의 채움이 적은 객체를 생성하는 문제가 있다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 다중 객체 합성 텍스트 대 이미지 생성의 문제를 해결하기 위해 SOTA 훈련 없는 전이 친화적 프레임워크 RealCompo를 제안한다. RealCompo에서는 LLM을 사용하여 텍스트 프롬프트를 분석하여 레이아웃을 얻고 다중 객체의 속성 사전 바인딩을 구현한다. 본 논문에서는 T2I와 L2I 모델의 장점을 동적으로 결합하여 고품질의 사실성과 구성성 생성을 구현하는 새로운 밸런서를 제안한다. 또한, 우리의 RealCompo는 모든 LLMs, T2I 및 L2I 모델로 일반화될 수 있으며 강력한 생성 결과를 유지한다. 향후 작업에서는 보다 강력한 모델을 RealCompo의 중추로 사용하여 이 프레임워크를 지속적으로 개선하고 보다 복잡한 양식으로 일반화하는 것도 탐구할 것이다.\n' +
      '\n' +
      '도 5: RealCompo의 일반화를 상이한 모델들에 대한 정성적인 비교: 우리는 두 개의 T2I 모델들: Stable Diffusion v1.5 (Rombach et al., 2022), TokenCompose (Wang et al., 2023b), 두 개의 L2I 모델 GLIGEN (Li et al., 2023b), Layout Guidance (LayGuide)(Chen et al., 2024)을 선택하고, 이들을 쌍으로 결합하여 4 버전의 RealCompo를 얻는다. 우리는 RealCompo가 다른 모델에 대한 강력한 일반화와 일반성을 가지고 있어 텍스트 프롬프트와 정렬하는 데 있어 충실도와 정확도의 현저한 수준을 달성한다는 것을 보여준다.\n' +
      '\n' +
      '그림 6: Eq에서 기울기 크기의 변화. RealCompo v3 및 v4의 T2I 및 L2I 모델에 대한 노이즈 제거 프로세스 중 11.\n' +
      '\n' +
      '## Impact Statements\n' +
      '\n' +
      '본 논문은 머신러닝 분야의 발전을 목표로 하는 작업을 제시한다. 우리 작업에는 많은 잠재적인 사회적 결과가 있으며, 여기에서 특별히 강조되어야 한다고 느끼는 것은 없습니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 기술 보고서. _ arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Balaji et al. (2022) Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., et al. effiffi: Text-to-image diffusion models with ensemble of expert denoisers. _ arXiv preprint arXiv:2211.01324_, 2022.\n' +
      '* Bar-Tal et al.(2023) Bar-Tal, O., Yariv, L., Lipman, Y., and Dekel, T. 다중 확산: 제어 영상 생성을 위한 확산 경로 융합. _ arXiv preprint arXiv:2302.08113_, 2023.\n' +
      '* Betker et al. (2023) Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al. 더 나은 캡션을 갖는 이미지 생성 개선. _ 컴퓨터 과학 https://cdn. openai. com/papers/dall-e-3. pdf_, 2:3, 2023.\n' +
      '* Chefer et al. (2023) Chefer, H., Alaluf, Y., Vinker, Y., Wolf, L., and Cohen-Or, D. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. _ ACM Transactions on Graphics (TOG)_, 42(4):1-10, 2023.\n' +
      '* Chen et al. (2023a) Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., et al. Pixart-\\(\\alpha\\): photorealistic text-to-image synthesis를 위한 확산 트랜스포머의 빠른 훈련; _ arXiv preprint arXiv:2310.00426_, 2023a.\n' +
      '* Chen et al. (2024) Chen, M., Laina, I., and Vedaldi, A. Training-free layout control with cross-attention guidance. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pp. 5343-5353, 2024.\n' +
      '* Chen et al. (2023b) Chen, X., Liu, Y., Yang, Y., Yuan, J., You, Q., Liu, L. - P., and Yang, H. Reason out your layout: Evoking the layout master from large language models for text-to-image synthesis. _ arXiv preprint arXiv:2311.17126_, 2023b.\n' +
      '* Fan et al.(2023) Fan, W. - C., Chen, Y. - C., Chen, D., Cheng, Y., Yuan, L., and Wang, Y. - C. F. 프리도: 복잡한 장면 이미지 합성을 위한 특징 피라미드 확산. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pp. 579-587, 2023.\n' +
      '* Feng et al. (2023) Feng, W., He, X., Fu, T. - J., Jampani, V., Akula, A. R., Narayana, P., Basu, S., Wang, X. E., and Wang, W. Y. Training-free structured diffusion guidance for compositional text-to-image synthesis. _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Foley et al. (2023) Foley, M., Rawat, A., Lee, T., Hou, Y., Picco, G., and Zizzo, G. Matching pairs: Attributing fine-tuned model to their pretrained large language models. _ arXiv preprint arXiv:2306.09308_, 2023.\n' +
      '* Gani et al. (2023) Gani, H., Bhat, S. F., Naseer, M., Khan, S., and Wonka, P. Llm blueprint: Enabling text-to-image generation with complex and detailed prompts. _ arXiv preprint arXiv:2310.10640_, 2023.\n' +
      '* Gu et al. (2023) Gu, J., Zhai, S., Zhang, Y., Susskind, J., and Jaitly, N. Matryoshka 확산 모델 arXiv preprint arXiv:2310.15111_, 2023.\n' +
      '* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probability models. _ 신경 정보 처리 시스템_, 33:6840-6851, 2020에서의 발전.\n' +
      '* Hu et al. (2024) Hu, H., Chan, K. C., Su, Y. -C., Chen, W., Li, Y., Sohn, K., Zhao, Y., Ben, X., Gong, B., Cohen, W., et al. Instruct-imagen: Multi-modal 명령어를 가진 이미지 생성 _ arXiv preprint arXiv:2401.01952_, 2024.\n' +
      '* Huang et al. (2023a) Huang, K., Sun, K., Xie, E., Li, Z., and Liu, X. T2i-compbench: 오픈 월드 작곡 텍스트 대 이미지 생성을 위한 포괄적인 벤치마크. _ arXiv preprint arXiv:2307.06350_, 2023a.\n' +
      '* Huang et al. (2023b) Huang, Z., Chan, K. C., Jiang, Y., and Liu, Z. 다중 모드 얼굴 생성 및 편집을 위한 협업 확산 In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 6080-6090, 2023b.\n' +
      '* Kazemi et al. (2022) Kazemi, M., Kim, N., Bhatia, D., Xu, X., and Ramachandran, D. Lambada: Backward chaining for automated reasoning in natural language. _ ArXiv:2212.13894_, 2022.\n' +
      '*Kim et al. (2023) Kim, Y., Lee, J., Kim, J.-H., Ha, J.-W., and Zhu, J.-Y. 주의 변조를 통한 조밀한 텍스트 대 이미지 생성. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 7701-7711, 2023.\n' +
      '* Li 등 (2023a) Li, X., Lv, K., Yan, H., Lin, T., Zhu, W., Ni, Y., Xie, G., Wang, X., and Qiu, X. 컨텍스트 학습을 위한 통합 데모 리트리버 _ arXiv preprint arXiv:2305.04320_, 2023a.\n' +
      '*Li 등(2023b) Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., and Lee, Y. J. Gligen: Open-set grounded text-to-image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 22511-22521, 2023b.\n' +
      '\n' +
      '* Lian et al. (2023) Lian, L., Li, B., Yala, A., and Darrell, T. Llm-grounded diffusion: 텍스트-대-이미지 확산 모델에 대한 신속한 이해도를 큰 언어 모델로 향상시킨다. _ arXiv preprint arXiv:2305.13655_, 2023.\n' +
      '* Liu et al. (2022) Liu, N., Li, S., Du, Y., Torralba, A., and Tenenbaum, J. B. Compositional visual generation with composable diffusion models. In _European Conference on Computer Vision_, pp. 423-439. Springer, 2022.\n' +
      '* Meral et al. (2023) Meral, T. H. S., Simsar, E., Tombari, F., and Yanardag, P. Conform: Contrast is all you need for high-fidelity text-to-image diffusion models. _ arXiv preprint arXiv:2312.06059_, 2023.\n' +
      '* Min et al. (2022) Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. 시연의 역할에 대한 재고: 상황 내 학습이 작동하는 이유는 무엇인가? arXiv preprint arXiv:2202.12837_, 2022.\n' +
      '*Nichol et al. (2022) Nichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., Mcgrew, B., Sutskever, I., and Chen, M. 글라이드: 텍스트 유도 확산 모델을 사용하여 실제 이미지 생성 및 편집을 진행합니다. In _International Conference on Machine Learning_, pp. 16784-16804. PMLR, 2022.\n' +
      '* Podell et al. (2023) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: 고해상도 영상 합성을 위한 잠재 확산 모델 개선. _ arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '*Qu et al.(2023) Qu, L., Wu, S., Fei, H., Nie, L., and Chua, T. - S. Layoutllm-t2i: 텍스트-이미지 생성을 위해 llm에서 레이아웃 안내를 누른다. In _Proceedings of the 31st ACM International Conference on Multimedia_, pp. 643-654, 2023.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. 클립 래턴트를 사용한 계층적 텍스트 조건 이미지 생성. _ arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. _ 신경 정보 처리 시스템_, 35:36479-36494, 2022에서의 발전.\n' +
      '* Si et al. (2023) Si, C., Friedman, D., Joshi, N., Feng, S., Chen, D., and He, H. Measuring in-context learning with underpecified demonstries. _ arXiv preprint arXiv:2305.13299_, 2023.\n' +
      '* Sohl-Dickstein et al. (2015) Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. 평형 열역학을 이용한 심층 비지도 학습 In _International conference on machine learning_, pp. 2256-2265. PMLR, 2015.\n' +
      '* Song et al. (2020) Song, J., Meng, C., and Ermon, S. 디노이징 확산 암시적 모델 arXiv preprint arXiv:2010.02502_, 2020a.\n' +
      '* Song et al. (2020) Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. _ arXiv preprint arXiv:2011.13456_, 2020b.\n' +
      '* Sun et al. (2023) Sun, J., Fu, D., Hu, Y., Wang, S., Rassin, R., Juan, D.-C., Alon, D., Herrmann, C., van Steenkiste, S., Krishna, R., et al. Dreamsync: Aligning text-to-image generation with image understanding feedback. _ arXiv preprint arXiv:2311.17946_, 2023.\n' +
      '* Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. - A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* Wang et al. (2023a) Wang, R., Chen, Z., Chen, C., Ma, J., Lu, H., and Lin, X. 확산 모델의 주의 지도 제어를 이용한 합성 텍스트-이미지 합성. _ arXiv preprint arXiv:2305.13921_, 2023a.\n' +
      '* Wang et al. (2023b) Wang, Z., Sha, Z., Ding, Z., Wang, Y., and Tu, Z. 토큰콤포즈: 토큰 수준의 감독으로 확산 접지를 한다. _ arXiv preprint arXiv:2312.03626_, 2023b.\n' +
      '* Wu et al. (2023a) Wu, H., Chang, K. - W., Wu, Y. - K and Lee H-y. 음성-gen: 음성 언어 모델의 생성 능력을 프롬프트와 함께 잠금 해제. _ arXiv preprint arXiv:2306.02207_, 2023a.\n' +
      '* Wu et al. (2023b) Wu, Q., Banal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. _ arXiv preprint arXiv:2308.08155_, 2023b.\n' +
      '* Wu et al. (2023c) Wu, Q., Liu, Y., Zhao, H., Bui, T., Lin, Z., Zhang, Y., and Chang, S. 고충실도 텍스트-이미지 합성을 위한 확산 모델의 공간적-시간적 주의를 활용한다. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 7766-7776, 2023c.\n' +
      '* Wu et al.(2023d) Wu, T. - H., Lian, L., Gonzalez, J. E., Li, B., and Darrell, T. 자체 교정 llm 제어 확산 모델 arXiv preprint arXiv:2311.16090_, 2023d.\n' +
      '\n' +
      'Xie, J., Li, Y., Huang, Y., Liu, H., Zhang, W., Zheng, Y., and Shou, M. Z. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 7452-7461, 2023.\n' +
      '* Xu et al.(2023) Xu, Y., Zhao, Y., Xiao, Z., and Hou, T. Ufogen: 확산 간스를 통해 한 번 대규모 텍스트-이미지 생성을 전달합니다. _ arXiv preprint arXiv:2311.09257_, 2023.\n' +
      '* Xue et al. (2023) Xue, Z., Song, G., Guo, Q., Liu, B., Zong, Z., Liu, Y., and Luo, P. Raphael: Text-to-image generation via large mixture of diffusion path. _ arXiv preprint arXiv:2305.18295_, 2023.\n' +
      '* Yang et al. (2022) Yang, L., Huang, Z., Song, Y., Hong, S., Li, G., Zhang, W., Cui, B., Ghanem, B., and Yang, M. -H. 확산 기반 장면 그래프를 마스킹된 대조적 사전 훈련으로 이미지 생성에 적용한다. _ ArXiv:2211.11138_, 2022.\n' +
      '* Yang et al. (2023a) Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Zhang, W., Cui, B., and Yang, M. -H. 확산 모델: 방법 및 응용 분야에 대한 포괄적인 조사. _ ACM Computing Surveys_, 56(4):1-39, 2023a.\n' +
      '* Yang et al. (2024a) Yang, L., Liu, J., Hong, S., Zhang, Z., Huang, Z., Cai, Z., Zhang, W., and Cui, B. Improving diffusion-based image synthesis with context prediction. _ 신경 정보 처리 시스템_, 36, 2024a의 발전.\n' +
      '* Yang et al. (2024b) Yang, L., Yu, Z., Meng, C., Xu, M., Ermon, S., and Cui, B. Mastering text-to-image diffusion: Recaptioning, planning and generating with multimodal l1ms. _ arXiv preprint arXiv:2401.11708_, 2024b.\n' +
      '* Yang et al. (2023b) Yang, Z., Wang, J., Gan, Z., Li, L., Lin, K., Wu, C., Duan, N., Liu, Z., Liu, C., Zeng, M., and Wang, L. Reco: 영역 제어 텍스트 대 이미지 생성. _CVPR_, 2023b에서.\n' +
      '* Ye 등(2023) Ye, Y., Cai, J., Zhou, H., Li, G., Zhang, Y., Song, Z., Gao, C., Yu, J., and Yang, W. 소프트 잠재 방향을 갖는 점진적인 텍스트-이미지 확산. _ arXiv preprint arXiv:2309.09466_, 2023.\n' +
      '* Zhang et al. (2023) Zhang, L., Rao, A., and Agrawala, M. 텍스트 대 이미지 확산 모델에 조건부 제어를 추가합니다. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 3836-3847, 2023.\n' +
      '* Zheng et al. (2023) Zheng, G., Zhou, X., Li, X., Qi, Z., Shan, Y., and Li, X. 레이아웃 확산: 레이아웃 대 이미지 생성을 위한 제어 가능한 확산 모델. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 22490-22499, 2023.\n' +
      '\n' +
      '## 부록 추가 분석\n' +
      '\n' +
      '### LLM 기반 레이아웃 생성\n' +
      '\n' +
      '대규모 언어 모델(LLMs)은 최근 몇 년 동안 괄목할 만한 발전을 목격했다(Touvron et al., 2023; Kazemi et al., 2022). 강력한 언어 이해, 유도, 추론 및 요약 능력으로 인해 LLM은 NLP(Natural Language Processing) 작업에서 상당한 진전을 이루었다(Foley et al., 2023; Wu et al., 2023a). 다중 객체 생성의 맥락에서 생성 모델은 언어에 대한 상대적으로 약한 이해도를 나타내며, 이는 생성된 이미지의 열악한 구성성에 반영된다. 결과적으로, LLM의 추론 및 상상 능력을 활용하여 생성 모델과의 협업을 촉진하여 프롬프트를 준수하는 이미지를 생성하는 방법을 탐구하는 것은 상당한 연구 잠재력을 제공한다.\n' +
      '\n' +
      '우리의 작업에서, 우리는 사용자의 입력 프롬프트에 기초하여 모든 객체들의 레이아웃을 직접 추론하기 위해 LLM들의 ICL(in-context learning)(Li et al., 2023; Si et al., 2023) 능력을 활용한다. 이 레이아웃은 RealCompo의 후속 세대에 사용되어, 각각의 프롬프트에 대한 레이아웃을 수동으로 제공하고 다수의 객체 및 속성의 사전 바인딩을 달성할 필요가 없다. 구체적으로, 도 1에 도시된 바와 같다. 도 7을 참조하면, 태스크 규칙(명령어), 컨텍스트 내 예제(시연) 및 사용자의 입력 프롬프트(테스트)에 대한 설명을 포함하는 프롬프트 템플릿을 구성한다. 명령어에 기초한 모방 추론을 통해, LLM은 각각의 오브젝트에 대한 레이아웃을 제공할 수 있으며, 여기서 각각의 레이아웃은 각각의 박스의 상부-좌측 및 하부-우측 코너들의 좌표를 나타낸다. 우리는 LLM으로 매우 유능한 GPT-4(Achiam et al., 2023)를 선택했다.\n' +
      '\n' +
      '도 7: 우선, 사용자의 입력 텍스트가 프롬프트 템플릿에 내장된다. 그런 다음 템플릿은 동결된 매개변수와 함께 GPT-4를 사용하여 파싱되며, 이는 프롬프트의 객체 및 해당 레이아웃에 대한 설명을 산출한다.\n' +
      '\n' +
      '식에서의 기울기 존재의### 분석. 11\n' +
      '\n' +
      '자, 시작합시다\n' +
      '\n' +
      '\\!=\\!\\\\cal{L}(\\mathcal{A}_{t-1}^{\\text{text}},\\mathcal{A}_{t-1}^{\\text{ layout}})\\!=\\!\\\\cal{L}(\\mathcal{A}_{t-1}^{\\text{ layout}}) sum_{b}\\mathcal{L}_{b}(\\mathcal{A}_{t-1}^{\\text{text}},\\mathcal{A}_{t-1}^{\\text{layout}})\\!=\\!\\\\ sum_{b}\\left[\\left(1\\!-\\!\\frac{\\sum_{i}\\mathcal{A}_{(ij_{b},t-1}}^{\\text{text}\\mathcal{M}_{b}}{\\sum_{i}\\mathcal{A}_{(ij_{b},t-1}}^{\\text{layout}}\\right)\\!+\\!\\left(1\\!-\\!\\frac{\\sum_{i}\\mathcal{A}_{(ij_{b},t-1}}^{\\text{layout}}\\odot(1\\!-\\!\\mathcal{M}_{b}}}^{\\text{layout}}\\right)\\tag{13}\\right]\n' +
      '\n' +
      '손실 함수가 식에 의해 주어지면. 10, Eq.의 그라디언트. 도 11은 다음과 같이 도출될 수 있다:\n' +
      '\n' +
      '(\\mathcal{A}_{t-1}^{\\text{text},\\mathcal{A}_{t-1}^{c}}{\\partial\\mathcal{A}_{(j_{b},t-1}}{\\partial\\mathbf}{t-1}}{\\partial\\mathcal{A}_{(j_{b},t-1}}{\\partial\\mathbf}{t-1}}{\\partial\\mathcal{A}{{t-1}}{\\partial\\mathbf}{t-1}}{\\partial\\mathc}\n' +
      '\n' +
      'T2I 및 L2I 모델의 경우 다음과 같습니다.\n' +
      '\n' +
      '\\mathcal{L}_{b}\\left(\\frac{\\partial\\mathcal{A}_{t-1}^{\\text{text}},\\mathcal{A}_{t-1}^{\\text{layout}\\right}=\\frac{\\mathcal{J}\\sum_{i}\\left(\\mathcal{A}_{(ij_{b},t-1}}^{\\text{text}}\\mathcal{M}_{b}\\text}\\mathcal{A}_{(ij_{b},t-1}}^{\\text{text}}}}\\text{2}}\\tag{15}\\text}}\\mathcal{A}_{(ij_{b},t-1}}\\text}\\mathcal{M}_{b}\\text}\\mathcal{A}_{(ij_{b},t-1}}}\\text}\\text}\\text}\\text}\\text}\\text}\\text}\\text}\\text}\\text}\\text}\\text\n' +
      '\n' +
      '\\mathcal{L}_{b}\\left(\\frac{\\partial\\mathcal{A}_{t-1}^{\\text{layout}},\\mathcal{A}_{(j_{b},t-1}}^{text{layout}}}=\\frac{\\mathcal{J}_sum_{i}\\left[\\mathcal{A}_{(ij_{b},t-1}}^{text{layout}}}\\odot(1-\\mathcal{M}_{b},t-1}}^{text{layout}}}{\\left(\\sum_{i}\\mathcal{A}_{(ij_{b},t-1}}}^{text{layout}}}}{\\text{16}\\right}}}\\frac{\\mathcal{J}_sum_{i}\\mathcal{A}_{(ij_{b},t-1}}}}}}\\mathcal{A}_{(ij_{b},t-1}\n' +
      '\n' +
      '여기서 \\(\\mathcal{J}\\)는 모든 원소가 \\(1\\)과 동일한 행렬이다. 모든 변수는 식에 있습니다. 도 14는 공지되어 있으며, 이는 Eq에서 구배의 존재를 나타낸다. 11.\n' +
      '\n' +
      '손실 함수가 식에 의해 주어지면. 12, Eq.의 그라디언트. 도 11은 다음과 같이 도출될 수 있다:\n' +
      '\n' +
      '(\\mathcal{A}_t-1}^{\\text{text}},\\mathcal{A}_{t-1}^{\\text{text}}\\frac{\\partial\\mathbf{t-1}}\\frac{\\partial\\mathbf{t-1}}\\frac{\\partial\\mathbf{t-1}}\\frac{\\partial\\mathbf{t-1}}\\frac{\\partial\\mathc}\n' +
      '\n' +
      'where \\(c\\in\\{\\text{text},\\text{layout}\\}\\).\n' +
      '\n' +
      '따라서, Eq에서의 그라디언트가 된다. 도 11은 상이한 손실 함수의 선택을 위해 존재한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>