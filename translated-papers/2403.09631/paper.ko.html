<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 3D-VLA: 3D Vision-Language-Action Generative World Model\n' +
      '\n' +
      ' 전하유\n' +
      '\n' +
      'Xiaowen Qiu\n' +
      '\n' +
      'Peihao Chen\n' +
      '\n' +
      'Jincheng Yang\n' +
      '\n' +
      'Xin Yan\n' +
      '\n' +
      'Yilun Du\n' +
      '\n' +
      'Yining Hong\n' +
      '\n' +
      'Chuang Gan\n' +
      '\n' +
      '[https://vis-www.cs.umass.edu/3dvla](https://vis-www.cs.umass.edu/3dvla)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근의 VLA(Vision-language-action) 모델은 3D 물리적 세계의 광범위한 영역과의 통합이 결여된 2D 입력에 의존한다. 또한, 그들은 세계의 방대한 역학, 행동과 역학 사이의 관계를 무시하고 인식에서 행동으로 직접 매핑을 학습하여 행동 예측을 수행한다. 대조적으로, 인간은 그에 따라 행동을 계획하기 위해 미래의 시나리오에 대한 상상력을 묘사하는 세계 모델을 부여받는다. 이를 위해 생성 세계 모델을 통해 3D 지각, 추론, 행동을 원활하게 연결하는 체화된 기초 모델의 새로운 패밀리를 도입하여 3D-VLA를 제안한다. 구체적으로, 3D-VLA는 3D 기반 대형 언어 모델(LLM)의 상부에 구축되고, 체화된 환경에 관여하기 위해 상호작용 토큰들의 세트가 도입된다. 또한, 생성 능력을 모델에 주입하기 위해 일련의 구체화된 확산 모델을 훈련하고 목표 이미지와 포인트 클라우드를 예측하기 위해 LLM에 정렬한다. 3D-VLA를 학습하기 위해, 우리는 기존의 로봇 데이터 세트에서 방대한 3D 관련 정보를 추출하여 대규모 3D 구현 명령어 데이터 세트를 큐레이션한다. 보유 데이터 세트에 대한 실험은 3D-VLA가 체화된 환경에서 추론, 멀티모달 생성 및 계획 기능을 크게 향상시켜 실제 응용 프로그램에서 잠재력을 보여준다.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '요즈음, 영상을 입력으로 하여 2D 공간에서 일련의 추론 작업을 수행할 수 있는 비전 언어 모델(Liu et al., 2023; Alayrac et al., 2022; Li et al., 2023b)의 확산이 있었다. 이러한 2D 기초 모델들은 또한 이미지들에 대한 상위 레벨 계획들 또는 하위 레벨 액션들을 생성할 수 있는 RT-2(Brohan et al., 2023) 및 PALM-E(Driess et al., 2023a)와 같은 최근의 구체화된 기초 모델들에 대한 기초를 마련한다. 그러나, 그들은 인간이 2D 이미지를 넘어 훨씬 더 풍부한 3D 물리적 세계 내에 위치한다는 사실을 무시한다 - 그들은 환경에 대한 그들의 3D 이해에 기초하여 추론하고 계획하며 행동한다(Palmer, 1975; Pylyshyn, 2003; Marr, 2010). 인간과 같은 지능형 체화 에이전트가 동일한 3D 이해 능력을 갖추는 것이 중요하다.\n' +
      '\n' +
      '한 걸음 더 나아가 최근 작품들(Huang et al., 2023b; Hong et al., 2024)은 3D 환경에서 기획하고 행동할 수 있는 체화된 기초 모델들을 개발한다. 그러나 이러한 모델은 주로 세계의 역학, 행동과 세계 역학 사이의 관계에 대한 광범위한 이해가 없는 인식에서 행동으로 직접 매핑을 학습한다. 한편, 인간은 3D 내적 표상에 기초하여 미래의 사건을 시뮬레이션하는 세계 모델을 축복받는다. 미래 상태에 대한 상상력과 기대감을 묘사함으로써 예측된 목표에 대한 행동을 더 잘 계획할 수 있다.\n' +
      '\n' +
      '이러한 인간과 같은 3D 세계 모델을 구축하기 위한 도전은 필연적으로 존재한다. 첫째, 기존 기반 모델은 언어 생성에 초점을 맞추고 언어를 넘어 모달리티를 상상할 수 없으며 세계 모델의 중요한 측면인 액션 생성을 촉진하기 위해 미래 상태를 시뮬레이션한다. 둘째, 기존의 구현된 데이터셋은 주로 2차원 이미지나 동영상을 포함하고 있으며, 3차원 공간에서의 추론과 기획을 위한 3차원 관련 주석이 부족하다.\n' +
      '\n' +
      '이를 위해 생성 세계 모델을 통해 3D 지각, 추론, 행동을 원활하게 연결하는 체화된 기초 모델의 새로운 패밀리를 도입하여 3D-VLA를 제안한다. 구체적으로, 3D 대형 언어 모델(Hong et al., 2023) 위에 3D-VLA를 구축하여 모델에 3D 이해 능력을 갖추도록 한다. 구현된 태스크들은 언어 생성을 통해서만 달성될 수 없었고, 동적 장면들, 조작된 객체들 및 장면들과 상호작용하기 위한 액션들을 더 깊이 파헤쳐야 하기 때문에, 우리는 LLM 어휘(예를 들어,_장면, 객체, 및 액션 토큰들)에 특별한 대화형 토큰들을 추가한다. 이러한 추가된 토큰은 우리의 모델이 더 넓은 범위의 체화된 작업을 수행하고 인터리빙된 3D 텍스트 데이터를 지원할 수 있게 한다. 구현된 기반 모델에서 멀티모달 생성 능력의 부적절함을 인식하여 목표 생성 능력을 3D-VLA에 주입할 것을 제안한다. 먼저 RGBD-to-RGBD 및 점대점 생성을 위해 구현된 확산 모델 세트를 미리 훈련한다. 다양한 모달리티의 확산 디코더와 LLM 임베딩 공간을 효율적으로 연결하기 위해 3D-VLA에서 다중 모달 목표 생성을 정렬하는 프로젝터를 사용한다. 그것은 한 세대의 촬영장비 유형을 지정하기 위해 멀티모달 신호를 전략적으로 통합한다.\n' +
      '\n' +
      '이러한 생성 세계 모델을 구축하기 위한 또 다른 과제는 데이터의 부족에 있다. 사용 중인 구체화된 데이터 세트(Padalkar et al., 2023; Brohan et al., 2022; Jang et al., 2022)는 주로 3D 관련 정보가 부족한 2D 이미지로 구성된다. 따라서, 우리는 대규모 3D 구현 명령어 튜닝 데이터세트를 큐레이션한다. 구체적으로, 먼저 로봇 조작 및 인간-객체 상호작용을 특징으로 하는 실제 및 합성 데이터를 포함하는 다양한 데이터 세트를 수집한다. 깊이 데이터가 부족한 데이터 세트의 경우 깊이 추정기를 사용하여 필요한 3D 세부 정보를 추가하고 3D 포인트 클라우드에 투영한다. 또한 기성 모델을 사용하여 3D 관련 주석을 추출하고 언어 설명을 풍부하게 하는 파이프라인을 설계한다. 이러한 방식으로, 우리는 그림 1과 같이 태스크 캡션, 액션 예측, 로컬리제이션, 멀티모달 목표 생성 등과 같은 다양한 태스크를 포괄하는 2M 3D-언어-액션 데이터 쌍을 수집한다.\n' +
      '\n' +
      '요약하자면, 우리는 다음과 같은 공헌을 가지고 있다.\n' +
      '\n' +
      '* 3D 인식, 추론, 행동을 생성 세계 모델과 통합하는 3D 비전-언어-행동 체화 기반 모델의 새로운 패밀리인 3D-VLA를 제안한다.\n' +
      '* 기존의 구체화된 데이터셋에서 3D 관련 정보의 부재를 다루는 대규모 3D 구체화된 명령어 튜닝 데이터셋을 생성한다.\n' +
      '* 우리는 환경과 더 잘 상호작용하기 위해 상호작용 토큰을 추가한다. 우리는 목표 이미지와 포인트 클라우드 생성을 위한 확산 모델을 추가로 훈련한다. 우리는 LLM 출력 피쳐와 확산 모델을 효율적으로 정렬하기 위해 프로젝터를 활용한다.\n' +
      '\n' +
      '그림 1: 3D Embodied Instruction Tuning Dataset의 예.\n' +
      '\n' +
      '* 우리의 3D-VLA는 목표 생성(이미지, 깊이 및 포인트 클라우드 측면에서), 목표 기반 계획 및 실시예 액션 예측을 포함하는 일련의 작업을 수행할 수 있다. 이 새로운 체화된 작업에서 기준선 모델을 크게 능가합니다. 또한 전통적인 언어 기반 작업에서 기본 모델을 능가합니다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '**멀티모달 언어 모델** 최근 멀티모달 언어 모델은 시각 및 언어 이해(Li et al., 2022, 2023; Liu et al., 2023; Huang et al., 2023; Peng et al., 2023; Zhu et al., 2023), 인터리브 이미지 및 텍스트 이해(Alayrac et al., 2022), 인터리브 이미지 및 텍스트 생성(Dong et al., 2023). 일부 보다 통일된 모델은 텍스트, 이미지, 비디오 및 오디오의 임의의 조합으로 입력을 지각하고 출력을 생성할 수 있다(Wu et al., 2023; Lu et al., 2023). 그러나, 이들 모델들 중 어느 것도 3D 입력에 따라 3D 입력들 또는 출력 액션들을 인지할 수 없다.\n' +
      '\n' +
      '**Vision-Language-Action Models** 액션 출력을 갖는 이전의 비전-언어 모델들은 주로 2D 피처들을 레버리지함으로써, 3D 공간 이해의 능력이 결여된다(Driess et al., 2023; Brohan et al., 2022; 2023). 대조적으로, 우리의 모델은 일반적인 세계 모델에서 목표 목표와 정렬되어 예측되는 3D 특징에 의해 안내된다. 액션 토큰 생성을 위해 포인트 클라우드와 같은 3D 기능을 최초로 활용하여 액션 플래닝 정확도를 크게 향상시켰습니다. 또한 이 파이프라인은 실제 시나리오에서 응용 프로그램을 위해 확장될 가능성이 있다.\n' +
      '\n' +
      '**3D Foundation Models** 우리의 논문은 MLLM에서 3D 특징들을 통합하는 3D 기초 모델들과 밀접하게 관련된다(Hong et al., 2023; Chen et al., 2023; Qi et al., 2023; Xu et al., 2023; Huang et al., 2023; Zhou et al., 2023; Guo et al., 2023; Li et al., 2024). 이러한 연구는 3D 기능을 이해하기 위해 기초 모델을 활용하는 데 성공적으로 발전했다. 그러나 이들은 주로 3D 장면의 현재 관찰 가능한 상태에서 분석 및 추론에 중점을 두어 즉각적인 인식을 넘어 확장되는 미래 특징을 예측하는 데 한계를 드러낸다. 이들과 대조적으로 우리는 인지할 수 있는 장면을 이해할 뿐만 아니라 특정 목표에 의해 유도되는 감지할 수 없는 멀티모달 특징을 예측하는 것을 목표로 한다. 이 기능을 통해 모델은 3D 세계와 상호 작용할 수 있는 액션 토큰을 추가로 생성할 수 있습니다.\n' +
      '\n' +
      '##3 3D 구현 명령어 튜닝 데이터 셋\n' +
      '\n' +
      '최근 인터넷에서 수십억 규모의 데이터 세트를 통해 혜택을 받은 VLM은 다양한 작업에 탁월한 능력을 보여주었다. 유사하게, 비디오-액션 쌍을 포함하는 백만-레벨 데이터 세트는 로봇 제어를 위한 체화된 VLM의 기초를 마련한다. 그러나 이들은 주로 3차원 공간 추론과 상호작용을 포함해야 하는 로봇 작업에서 깊이나 3차원 주석과 정확한 제어를 제공하지 않는다. 3D 정보가 없으면 로봇이 "가장 먼 컵을 중간 서랍에 위치"와 같은 3D 공간 추론이 필요한 명령을 이해하고 실행하는 것은 어렵다.\n' +
      '\n' +
      '이러한 격차를 해소하기 위해 우리는 쌍을 이루는 텍스트 명령뿐만 아니라 충분한 3D 관련 정보를 제공하는 대규모 3D 구현 명령어 튜닝 데이터 세트를 구축하여 학습한다.\n' +
      '\n' +
      '그림 2: 당사의 3D-VLA 파이프라인의 개요. 왼쪽 부분은 우리의 목표 생성 능력을 보여줍니다. 우리의 모델은 사용자의 입력에 기초하여 최종 상태 이미지와 포인트 클라우드를 상상할 수 있다. 이 생성된 목표 상태는 로봇 제어를 안내하기 위해 우리 모델에 피드백될 수 있다.\n' +
      '\n' +
      '모델. 구현된 데이터 세트로부터 3D 언어-동작 쌍을 추출하기 위한 파이프라인을 설계하고, 포인트 클라우드, 깊이 맵, 3D 바운딩 박스, 로봇의 7D 동작 및 텍스트 설명에 대한 주석을 획득한다. 세부사항은 다음과 같이 요약되어 있다.\n' +
      '\n' +
      '### Dataset Collection\n' +
      '\n' +
      '우리의 데이터는 다양한 소스에서 선별됩니다. 자세한 내용은 부록에서 확인할 수 있는 개요를 제공합니다.\n' +
      '\n' +
      '**로봇 데이터세트:** 12개의 데이터세트(Brohan et al., 2022; Jang et al., 2022; Walke et al., 2023; Lynch et al., 2023; Feng et al., 2023; Chen et al., 2023a; Dass et al., 2023; Mandlekar et al., 2019; Mees et al., 2023; Shah et al., 2023; Sawhney et al., 2021; Sermanet et al., 2023)를 Open-X 실시예 데이터세트(Padalkar et al., 2023)로부터 선택한다. 그들은 실제 세계에서 언어적 지시가 있는 고품질 이미지를 가지고 있지만 더 깊이 있는 정보와 3D 주석이 부족하다. 또한 Dobb-E(Shafiullah et al., 2023) 및 RH20T(Fang et al., 2023)와 같이 깊이 정보가 우수한 데이터셋을 선택한다. 또한, RLBench(James et al., 2020)와 CALVIN(Mees et al., 2022)의 두 가지 시뮬레이터 환경에서 수집된 데이터셋을 사용한다.\n' +
      '\n' +
      '**인간 객체 상호작용 데이터 세트**: 인간/손-객체 상호작용은 로봇 의사 결정 및 모방에 도움이 되는 데모를 제공할 수 있다. 따라서 우리는 Epic-Kitchens(Damen et al., 2018)과 같이 깊이 정보가 없는 데이터세트와 HOI4D(Liu et al., 2022)와 같이 3D 주석이 더 나은 데이터세트를 포함한 여러 인간-객체 상호작용 데이터세트를 활용한다.\n' +
      '\n' +
      '### Visual Annotations\n' +
      '\n' +
      '**깊이 및 광학 흐름을 추정하는 것.** 체화된 작업에 대한 비디오 데이터 세트의 95% 이상이 3D 정보를 제공하지 않는다는 점을 감안할 때, 우리는 이러한 데이터 세트로부터 비디오의 각 프레임에 ZoeDepth(Bhat et al., 2023)를 채용한다. 또한, 비디오 데이터를 더 잘 활용하기 위해 광학 흐름 추정을 위해 RAFT(Teed & Deng, 2020)를 사용한다. 광학 흐름은 우리가 생성하는 데이터를 정제하는 데 도움이 됩니다. 따라서 카메라 포즈가 변경되지 않는 비디오 세그먼트에 대해 광학 흐름을 사용하여 어떤 픽셀이 이동되지 않은 배경인지 추정한다. 동일한 비디오의 서로 다른 프레임에 걸쳐 이러한 배경의 깊이 맵을 정렬하여 각 프레임의 깊이 맵에 계수를 곱하여 깊이 일관성을 보장한다. 깊이 맵을 얻은 후, 카메라 인닉스와 포즈를 사용하여 RGB-D 이미지를 3D 포인트 클라우드로 직접 들어올릴 수 있다.\n' +
      '\n' +
      '**3D 주석을 생성하는 것.** 우리는 3D 공간에서의 로봇 동작뿐만 아니라 상상 결과로서 객체, 목표 이미지, 깊이 또는 포인트 클라우드의 3D 바운딩 박스의 여러 3D 관련 주석을 생성하는 것을 목표로 한다. 먼저 장면 내의 객체들의 3D 바운딩 박스를 추출한다. 이러한 정보는 3D 정보를 캡처하고 더 나은 의사 결정을 위해 조작된 객체에 참석할 수 있는 3D 모델의 능력에 도움이 될 수 있다. 소스 역할을 하는 구현된 데이터 세트는 로봇에 의해 실행되는 명령을 설명하는 텍스트 명령을 제공한다. 우리는 spaCy(Honnibal & Montani, 2017)를 사용하여 조작된 객체를 포함한 모든 명사 청크를 얻기 위해 명령어를 파싱한다. 우리는 각 물체의 2D 마스크를 얻기 위해 미리 훈련된 접지 모델(예를 들어, Grounded-SAM(Ren et al., 2024))을 활용한다. 이 2D 마스크는 3D로 들어올릴 때 포인트 클라우드의 일부에 해당하므로 공간에 있는 모든 물체의 3D 경계 상자를 얻을 수 있다. 마스크들을 선택할 때, 조작된 오브젝트는 상당한 광학 흐름의 영역들에서 가장 높은 신뢰도 값에 기초하여 선택된다. 깊이 및 포인트 클라우드를 재구성하기 때문에 향후 프레임에서의 이미지, 깊이 및 포인트 클라우드를 지상-진실 목표로 사용할 수 있다. 동작의 경우 제공된 데이터 세트에서 7개의 DoF 동작을 사용한다.\n' +
      '\n' +
      '### Language Annotations\n' +
      '\n' +
      '(Li et al., 2023; Peng et al., 2023)에 의해 영감을 받아, 우리는 도 2의 프롬프트들에 도시된 바와 같이, 이전에 생성된 3D 주석들(경계 박스들, 목표 이미지들/깊이들/포인트 클라우드들, 액션들)을 포괄하는 토큰들(_e.g._, <image>/image>; <pcd>/pcd>)로 구성된 조밀한 언어 주석들을 생성할 것을 제안한다.\n' +
      '\n' +
      '우리는 이러한 3D 주석을 프롬프트와 답변으로 구성하기 위해 미리 정의된 언어 템플릿을 토큰과 함께 사용한다. (Hong et al., 2023)에 이어, 우리는 프롬프트를 다양화하기 위해 ChatGPT 기반 프롬프트를 사용한다. 특히, 우리는 ChatGPT뿐만 아니라 주석이 달린 객체 및 경계 상자에 대한 지침을 제공한다. 또한 GPT가 생성하라는 데이터 유형에 대해 안내하기 위해 2-3개의 소수의 사람이 쓴 데모를 제공한다. ChatGPT는 정보를 요약하고 템플릿 생성 프롬프트를 보다 다양한 형태로 다시 작성하도록 요청받는다. 사전 정의된 템플릿이 없는 작업의 경우, ChatGPT는 또한 이러한 작업의 언어 입력 및 출력으로서 프롬프트 및 답변을 생성하도록 요청된다. 부록에 모든 유형의 데이터를 생성하라는 세부 템플릿과 프롬프트를 표시합니다.\n' +
      '\n' +
      '## 4 Methods\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      '본 절에서는 체화된 환경에서 3D 추론, 목표 생성 및 의사 결정을 위한 세계 모델인 3D-VLA를 소개한다. 도 2에 도시된 바와 같이, 우리는 먼저 3D-LLM 위에 우리의 백본을 구축하고(Hong et al., 2023), 일련의 상호작용 토큰을 추가함으로써 3D 세계와 상호작용하기 위한 모델의 능력을 더욱 향상시킨다. 다음으로, 구현된 확산 모델을 미리 학습하고, LLM과 확산 모델을 정렬하기 위한 프로젝터를 사용하여 목표 생성 능력을 3D-VLA에 주입한다.\n' +
      '\n' +
      '### 3D-VLA\n' +
      '\n' +
      '#### 4.2.1 Backbone\n' +
      '\n' +
      '첫 번째 단계에서는 3D-LLM의 방법론에 따라 3D-VLA 기반 모델을 개발한다(Hong et al., 2023). 우리가 수집한 데이터 세트는 처음부터 멀티모달 LLM을 훈련하는 데 필요한 10억 수준 스케일이 아니기 때문에, 우리는 3D 장면 특징을 생성하기 위해 멀티뷰 특징을 활용하여 3D-LLM의 접근 방식을 따른다. 이를 통해 적응할 필요 없이 미리 훈련된 VLM에 시각적 기능을 원활하게 통합할 수 있다. 한편, 3D-LLM을 위한 훈련 데이터 세트는 대부분 객체(Deitke et al., 2022) 및 실내 장면(Dai et al., 2017; Ramakrishnan et al., 2021)으로 구성되며, 이는 우리의 체화된 설정과 직접적으로 정렬되지 않는다. 따라서 우리는 3D-LLM 사전 훈련된 모델을 로드하지 않기로 선택한다. 대신, 우리는 BLIP2-FlanT5XL(Li et al., 2023)을 우리의 사전 훈련된 모델로 활용한다. 트레이닝 동안, 우리는 토큰에 대한 입력 및 출력 임베딩과 Q-전자의 가중치를 모두 동결 해제한다.\n' +
      '\n' +
      '######4.2.2 인터랙션 토큰\n' +
      '\n' +
      '3D 장면에 대한 모델의 이해도를 높이고 이러한 환경 내에서 상호 작용을 용이하게 하기 위해 새로운 상호 작용 토큰 세트를 소개한다. 첫째, 객체 명사들을 파싱된 문장들(_e.g._, <obj> a chocolate bar</obj>[loc tokens])에 묶는 객체 토큰들(<obj></obj></obj>)을 테이블 상에 통합함으로써, 모델이 어떤 객체들이 조작되거나 참조되는지를 더 잘 포착할 수 있도록 한다. 둘째, 공간 정보를 언어별로 더 잘 표현하기 위해, AABB 형태로 3D 바운딩 박스에 대한 6개의 토큰으로 표현되는 접지 참조 객체에 대한 위치 토큰 <loc0-255> 집합을 고안한다. 셋째, 동역학을 더 잘 인코딩하기 위해 정적 장면의 임베딩을 둘러싸는 <장면> </장면> 토큰을 소개한다. 3D-VLA는 장면 토큰을 통해 구성함으로써 동적 장면을 이해하고 3D 장면과 텍스트를 인터리빙하는 입력을 관리할 수 있다.\n' +
      '\n' +
      '로봇 동작을 나타내는 확장된 전문 토큰 세트로 아키텍처를 더욱 향상시킵니다. 7 자유도를 갖는 로봇의 동작은 암의 의도된 절대 위치, 회전, 그리퍼 개방을 나타내기 위해 <aloc0-255>, <arot0-255>, <gripper0/1>과 같은 이산 토큰으로 표시된다. 이러한 동작들은 토큰 <ACT_SEP>에 의해 분리된다.\n' +
      '\n' +
      '3D-VLA에 목표 생성 능력 주입###\n' +
      '\n' +
      '이 섹션에서는 이미지, 깊이 및 포인트 클라우드 측면에서 우리의 3D-VLA가 목표 생성을 수행하는 방법을 소개한다.\n' +
      '\n' +
      '인간은 세계 모델 구축의 핵심 측면인 행동 예측이나 의사 결정을 용이하게 하기 위해 장면의 최종 상태를 미리 시각화한다. 또한, 예비 실험 동안, 지상-진실 최종 상태를 제공하는 것이 모델의 추론 및 계획 능력을 향상시킬 수 있음을 발견한다. 그러나 이미지, 깊이 및 포인트 클라우드를 생성하기 위해 MLLM을 훈련하는 것은 간단하지 않다. 첫째, 최첨단 비디오 확산 모델은 체화된 설정에 맞게 조정되지 않는다. 예를 들어, 런웨이(Esser et al., 2023)에 "서랍을 열어라"라는 명령어가 주어진 미래 프레임들을 생성하도록 요청할 때, 전체 장면은 뷰 변화, 예상치 못한 객체 변형, 및 이상한 텍스처 교체뿐만 아니라 레이아웃 왜곡과 관련하여 크게 변경된다. 유사하게, 드림LLM(Dong et al., 2023)의 방법을 사용하여 인터넷 데이터에 대해 트레이닝된 안정적인 확산을 직접 동결하면, 붕괴된 출력으로 이어질 수 있다. 둘째, 다양한 양식의 확산 모델을 단일 기초 모델에 통합하는 방법은 여전히 과제로 남아 있다. 따라서 3D-VLA에 이미지, 깊이 및 포인트 클라우드를 생성할 수 있는 기능을 주입할 것을 제안한다. 먼저 구현된 확산 모델을 이미지, 깊이, 포인트 클라우드와 같은 서로 다른 모달리티 관점에서 사전 훈련하고, 이러한 확산 모델의 디코더를 정렬 단계를 통해 3D-VLA의 임베딩 공간에 정렬한다.\n' +
      '\n' +
      '목표생성을 위한 신체확산모델 4.3.1\n' +
      '\n' +
      '구현된 환경에서 목표 생성을 위한 현재 확산 모델의 한계를 해결하기 위해 RGB-D를 RGB-D로, 포인트-클라우드를 포인트-클라우드 확산 모델로 훈련한다. 우리는 큐레이션된 3D 언어 비디오 데이터를 사용하여 명령어에 따라 초기 상태 모달리티를 편집하여 해당 최종 상태 모달리티를 생성하는 조건부 확산 모델을 훈련한다. 이 모델들에 대한 구체적인 훈련 내용은 다음과 같다. RGBD에서 RGBD 생성에 대해, 사전 훈련된 VAE(Kingma and Welling, 2013)의 잠재 공간에서 작동할 때 잠재 확산에 의한 이미지 생성의 효율성과 품질로 인해 사전 훈련된 모델로 Stable Diffusion V1.4(Rombach et al., 2022)를 사용한다. 우리는 RGB 잠재량과 깊이 잠재량을 이미지 조건으로 연결한다. 마찬가지로 점대점 생성을 위해 점군 조건 입력을 추가하는 사전 훈련 모델로 점-E(Nichol et al., 2022)를 사용한다.\n' +
      '\n' +
      '교량 LLM과 목표 세대 4.3.2 교량 LLM과 목표 세대\n' +
      '\n' +
      '확산 모델을 사전 훈련한 후, 우리는 잠재 공간을 양식에서 조정함으로써 목표를 생성할 수 있는 다양한 디코더를 장착한다. 3D-VLA가 입력 지침에 조건화된 사전 훈련된 양식과 관련하여 목표를 생성할 수 있도록 사전 훈련된 디코더를 LLM에 원활하게 통합하는 방법에 대한 문제가 남아 있다. LLM과 다른 양식의 확산 모델 사이의 간격을 메우기 위해 정렬 단계를 3D-VLA로 개발한다. 먼저 <image> </image>와 <pcdb</pcdb>와 같은 추가적인 특수 토큰을 소개한다. 이러한 토큰들은 출력할 모달 콘텐츠의 타입에 대해 디코더에 알리도록 복잡하게 설계된다. 인클로징 토큰들 사이에서, 우리는 객체 토큰들 및 <이미지>와 같은 위치 토큰들을 포함할 수 있는 로봇이 실행하기 위한 명령어들을 생성하는데 있어서 LLM을 감독하며, 이는 <obj> 사과 </obj> [loc 토큰] </이미지>를 픽업한다. 이를 바탕으로 LLM(Large Language Model)의 디코더 특징과 임베딩을 DM 프레임워크의 공간에 매핑할 수 있는 트랜스포머 기반 프로젝터를 적용할 수 있다. 이는 모델의 다중 모달 데이터를 이해하고 생성할 수 있는 능력을 향상시켜 높은 수준의 언어 이해와 다중 모달 목표 생성 사이의 연결을 구축하는 데 중요한 역할을 한다. 3D-VLA 교육을 보다 효율적으로 하고, 치명적인 망각을 피하기 위해 LoRA(Hu et al., 2021)를 활용하여 다양한 확산 모델을 미세 조정한다. 동시에 새롭게 도입된 특수 토큰 임베딩, 해당 임베딩 출력 선형 레이어, 프로젝터 전체만을 학습한다. LLM과 DM 노이즈 제거 손실을 모두 최소화합니다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '3D-VLA는 3D 세계에서 추론과 접지를 수행하고 멀티모달 목표 콘텐츠를 상상하며 로봇 조작을 위한 액션을 생성할 수 있는 다용도 3D 기반 생성 세계 모델이다. 본 절에서는 3D-VLA를 3D 추론 및 현지화, 멀티모달 목표 생성, 체화된 액션 플래닝의 세 가지 측면에서 평가한다.\n' +
      '\n' +
      '### 3D 추론 및 위치 추정\n' +
      '\n' +
      '**과제** 우리의 주요 초점은 더 큰 수준의 추론과 현지화 능력을 요구하는 더 큰 역동성과 더 높은 상호작용 정도를 특징으로 하는 로봇과 관련된 장면에 있다. 로봇공학 영역에서 이러한 능력을 학습하기 위해 3D 구현된 명령어 튜닝 데이터셋을 기반으로 몇 가지 작업을 구축한다. 작업은 1) RoboVQA 데이터세트(Sermanet et al., 2023)에 구현된 QA; 2) 11개의 Open-X 데이터세트(Padalkar et al., 2023)에 대한 작업 캡션화; 3) RT-1 데이터세트(Brohan et al., 2022)에 대한 작업 캡션화; 3) RT-1 데이터세트(Brohan et al., 2022)에 대한 QA, 여기서 에이전트는 일부 특정 액션(액션 토큰에 의해 표현됨)이 실행되면 어떤 일이 일어날지에 대한 질문을 받는다; 4) 에이전트는 3d 바운딩 박스에 의해 지정된 콘텐츠를 캡션할 필요가 있는 11개의 Open-X 데이터세트에 대한 밀집 캡션화; 5) 에이전트는 로봇 조작 명령에서 언급된 객체를 로컬화하는 11개의 Open-X 데이터세트에 대한 로컬화. 본 논문에서는 고정 데이터 셋을 이용하여 3D-VLA를 평가한다.\n' +
      '\n' +
      '**Baselines.** 3D-VLA를 3D-LLM(Hong et al., 2023) 및 BLIP2(Li et al., 2023), OpenFlamingo(Alayrac et al., 2022) 및 LLAVA(Liu et al., 2023)를 포함한 2D 비전 언어 모델들과 비교한다. 우리는 이러한 베이스라인들을 두 가지 방식으로 구현한다: 1) 이 새로운 태스크들에 대해 릴리즈된 트레이닝된 모델을 테스트하는 제로-샷 트랜스퍼; 2) held-in evaluation where we trained model on 2D-image-action-language pairs (_i.e.,_, 11 dataset selected from Open\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c l|c c c c c c c} \\hline \\hline Tasks & Models & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGH-L & EM@1 \\\\ \\hline \\multirow{6}{*}{Embodied QA} & 3D-LLM\\({}^{*}\\) & 1.05 & 0.38 & 0.15 & 0.02 & 12.96 & 0.91 & 0.00 \\\\  & BLIP2 OPT\\({}_{\\text{2.7B}}\\)\\({}^{*}\\) & 7.39 & 3.17 & 0.03 & 0.02 & 3.87 & 7.40 & 3.03 \\\\  & BLIP2 FlanT5XL\\({}^{*}\\) & 22.84 & 16.17 & 12.50 & 10.11 & 11.41 & 32.01 & 10.31 \\\\  & OpenFlamingo\\({}_{\\text{4B}}\\)\\({}^{*}\\) & 9.50 & 6.51 & 5.14 & 4.29 & 6.84 & 10.40 & 1.21 \\\\  & LLAVA\\({}_{\\text{7B}}\\)\\({}^{*}\\) & 11.66 & 8.06 & 6.01 & 4.58 & 12.59 & 14.17 & 5.67 \\\\  & BLIP2 FlanT5XL & 37.31 & 27.20 & 20.32 & 15.48 & 17.80 & 38.92 & 15.35 \\\\  & **3D-VLA** & **48.34** & **38.55** & **31.72** & **26.80** & **23.72** & **49.33** & **24.53** \\\\ \\hline \\multirow{6}{*}{Task Caption} & 3D-LLM\\({}^{*}\\) & 0.78 & 0.16 & 0.07 & 0.05 & 0.57 & 1.33 & 0.00 \\\\  & BLIP2 FlanT5XL\\({}^{*}\\) & 8.50 & 2.07 & 0.35 & 0.00 & 3.40 & 8.45 & 0.00 \\\\  & OpenFlamingo\\({}_{\\text{4B}}\\)\\({}^{*}\\) & 7.61 & 1.64 & 0.37 & 0.00 & 4.74 & 9.36 & 0.00 \\\\  & LLAVA\\({}_{\\text{7B}}\\)\\({}^{*}\\) & 2.63 & 0.69 & 0.16 & 0.00 & 2.63 & 4.65 & 0.00 \\\\  & BLIP2 FlanT5XL & 22.05 & 11.40 & 5.72 & 3.16 & 8.72 & 26.12 & 7.75 \\\\  & **3D-VLA** & **55.69** & **45.88** & **39.39** & **34.88** & **27.57** & **62.01** & **29.34** \\\\ \\hline \\multirow{2}{*}{What-if QA} & BLIP2 FlanT5XL & 28.23 & 11.47 & 4.49 & 0.06 & 8.27 & 28.41 & 5.85 \\\\  & **3D-VLA** & **53.09** & **40.94** & **34.34** & **29.38** & **26.83** & **52.82** & **14.7** \\\\ \\hline \\multirow{2}{*}{Dense Caption} & 3D-LLM\\({}^{*}\\) & 0.52 & 0.22 & 0.16 & 0.13 & 0.34 & 0.64 & 0.00 \\\\  & BLIP2 FlanT5XL & 36.17 & 24.72 & 18.06 & 13.96 & 17.83 & 40.56 & 13.10 \\\\ \\cline{1-1}  & **3D-VLA** & **51.90** & **42.83** & **38.11** & **34.62** & **25.25** & **55.91** & **39.49** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 보유 데이터를 이용한 추론 능력 평가. \\ (*\\)은 사전 훈련 데이터 세트에 대한 훈련 없이 제로 샷 전송 결과를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Methods & IoU & Acc@25 & Acc@50 \\\\ \\hline Kosmos-2 (w/ GT Depth) & 10.92 & 12.73 & 3.85 \\\\ CoVLM (w/ GT Depth) & 19.81 & 25.39 & 16.61 \\\\\n' +
      '3D-VLA & **29.33** & **42.26** & **27.09** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 보유 로봇 데이터 세트에 대한 위치 확인 결과.\n' +
      '\n' +
      'X 및 RoboVQA 데이터세트. 국산화 작업을 위해, 2D 접지 MLLM, 즉 Kosmos-2(Peng et al., 2023) 및 CoVLM(Li et al., 2023a)과 비교한다. 구체적으로, 이 모델을 사용하여 제로 샷 방식으로 2D 바운딩 박스를 검출한 다음 깊이 투영을 사용하여 3D 바운딩 박스로 전달한다.\n' +
      '\n' +
      '**결과 분석** 표 1에서 3D-VLA는 언어 추론 작업에서 모든 2D VLM 방법을 능가한다. 추론을 위해 보다 정확한 공간 정보를 제공하는 3D 정보의 레버리지에 기인한다. 또한, 데이터셋에는 3차원 위치인식 주석들이 많이 포함되어 있기 때문에, 3D-VLA는 관련 객체들의 위치인식을 학습하게 되며, 이는 모델이 추론을 위한 핵심 객체들에 더 집중할 수 있도록 도와준다. 또한, 3D-LLM은 이러한 로봇 추론 작업을 제대로 수행하지 않는다는 것을 발견했으며, 이는 로봇 공학 관련 3D 데이터 세트에 대한 수집 및 훈련의 필요성을 보여준다. 표 2에서 3D-VLA는 국소화 성능 측면에서 2D 기준 방법보다 현저한 우월성을 보여준다. 이 발견은 상당한 양의 3D 주석을 공급하는 주석 프로세스의 효능에 대한 강력한 증거 역할을 하여 모델 내에서 강력한 3D 현지화 능력의 획득을 촉진한다.\n' +
      '\n' +
      '### 멀티모달 목표 생성\n' +
      '\n' +
      '**Task.** Open-X 테스트 세트에서 3D-VLA의 RGB 목표 및 포인트 클라우드 목표 생성 능력을 정량적으로 평가한다. 우리는 3D-VLA가 훈련 과정에서 보지 못하는 Open-X 테스트 세트에서 4000개의 에피소드를 무작위로 샘플링한다.\n' +
      '\n' +
      '**Baselines.** 이미지 생성을 위해, 3D-VLA와 세 가지 유형의 이미지 생성 방법을 비교한다: 1) 이미지-편집 방법 Instruct-P2P(Brooks et al., 2023); 2) 목표 이미지/비디오 생성 방법 SuSIE(Black et al., 2023); 3) 이미지 생성 능력을 갖는 LLM들(Wu et al., 2023). 포인트 클라우드 생성을 위해 텍스트-투-3D 확산 모델 Point-E(Nichol et al., 2022)와 비교한다.\n' +
      '\n' +
      '**정성적 결과.** 이미지 목표 생성 결과를 표 3에 나타내었다. 로보틱스 도메인(표 3의 1, 2, 3행)으로 직접 제로샷 전송을 하는 기존의 생성 방법과 비교할 때, 3D-VLA는 대부분의 메트릭 측면에서 유망한 성능을 달성한다. 이것은 로봇 응용 프로그램을 위해 특별히 설계된 데이터 세트를 사용하여 세계 모델을 훈련하는 것의 중요성을 강조한다. 우리가 사용한 동일한 로봇 데이터 세트에 대해 훈련된 Instruct-P2P*와 직접 비교(표의 4행)에서도 3D-VLA가 일관되게 그 성능을 능가한다. 이는 대규모 언어 모델을 3D-VLA에 통합하면 로봇 공학 조작 지침에 대한 보다 포괄적이고 통찰력 있는 이해로 이어져 목표 이미지 생성 성능이 향상된다는 것을 강조한다. 또한, 예측된 바운딩 박스를 입력 프롬프트(행 5)에서 제외할 때, 성능이 약간 감소하는 것을 관찰한다. 이 관찰은 이러한 중간 예측 바운딩 박스가 모델이 전체 장면을 이해하는 데 도움을 주기 때문에 이 중간 예측 바운딩 박스를 사용하는 것의 효과를 확인시켜, 모델이 주어진 명령에서 언급된 특정 객체에 더 많은 관심을 할당하도록 하여 궁극적으로 최종 목표 이미지를 상상하는 능력을 향상시킨다.\n' +
      '\n' +
      '포인트 클라우드 생성 결과는 표 4에 제시되어 있다. 중간 예측 바운딩 박스를 갖는 3D-VLA가 가장 잘 수행된다. 이 결과는 명령어와 장면을 모두 이해하는 맥락에서 대규모 언어 모델과 정확한 객체 현지화를 통합하는 것의 중요성을 강화한다.\n' +
      '\n' +
      '**정량적 결과.** 도 3의 첫 번째 행에서, 생성된 RGB-D 목표 이미지를 RT-1(Brohan et al., 2022) 및 Jaco Play(Dass et al., 2023) 데이터 세트의 테스트 세트 상에서 시각화한다. 이러한 샘플은 훈련 과정에서 볼 수 없다. 초기 장면들 및 명령어들이 주어지면, 3D-VLA 모델은 상호작용의 타겟 객체를 정확하게 식별하고 제공된 명령어들에 후속하여 이들 식별된 객체들의 상태들을 정확하게 수정하면서 백그라운드 엘리먼트들을 변경되지 않고 유지하는 능력을 일관되게 나타낸다. 생성된 RGB-D 목표 이미지는 시각적 외관과 의미적 내용 측면에서 모두 지상 진리 목표와 밀접하게 정렬된다. 통제된 실험 설정 외에도 인터넷이나 일상 생활에서 캡처된 장면을 포함하도록 테스트를 확장했습니다. 이러한 다양하고 통제되지 않는 환경에서 우리의 3D-VLA 모델은 일관되고 강력하게 그 효능을 입증했다.\n' +
      '\n' +
      '구현된 행동 계획\n' +
      '\n' +
      '**Task** 두 벤치마크, 즉 RLBench(James et al., 2020) 및 CALVIN(Mees et al., 2022)에 대한 로봇 암 동작 예측을 위한 3D-VLA의 능력을 평가한다. 우리는 평가를 위해 RLBench에서 세 가지 작업을 선택한다. 또한 모델의 일반화 능력을 테스트하기 위해 보이지 않는 작업으로 픽업 작업에서 var1을 선택한다. 칼빈은\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c} \\hline \\hline Models & P-FID \\(\\downarrow\\) & Chamfer-\\(L_{1}\\downarrow\\) \\\\ \\hline Point-E\\({}^{*}\\) & 5.241 & 0.159 \\\\\n' +
      '3D-VLA w/o Pred BBox & 4.914 & 0.143 \\\\\n' +
      '3D-VLA & **4.796** & **0.139** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 포인트 클라우드 목표 생성 결과. \\ (*\\)은 미리 훈련된 데이터 세트에 대해 모델이 훈련되었음을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline Method & PSNR \\(\\uparrow\\) & CLIP Sim \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & FID \\(\\downarrow\\) \\\\ \\hline Instruct-P2P & 14.41 & 0.909 & 0.389 & 0.309 \\\\ SuSIE & 15.20 & 0.898 & 0.549 & 0.182 \\\\ NeXT-GPT & 8.86 & 0.199 & 0.153 & 0.432 \\\\ Instruct-P2P\\({}^{*}\\) & 16.67 & **0.941** & 0.628 & 0.178 \\\\ \\hline\n' +
      '3D-VLA w/o Pred BBox & 17.02 & 0.919 & 0.632 & **0.173**\\\\\n' +
      '3D-VLA & **17.21** & 0.920 & **0.636** & 0.177 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: RGB 영상 목표 생성 결과 (*\\)은 미리 훈련된 데이터 세트에 대해 모델이 훈련되었음을 나타낸다.\n' +
      '\n' +
      '우리는 에이전트가 5개의 작업을 순차적으로 실행해야 하는 긴 수평 다중 작업 언어 제어 설정에서 모델을 평가한다. 우리는 A, B, C, D 장면에서 에이전트를 훈련시키고 D 장면에서 테스트한다.\n' +
      '\n' +
      '**Baselines.** RLBench에 대해, 우리는 우리의 모델 3D-VLA를 LanCon-Learn(Silva et al., 2021)과 비교하며, 이는 명령-조건 입력들에 기초하여 동작들을 예측할 수 있는 멀티-태스크 접근법이다. CALVIN의 경우 조건부 시퀀스 대 시퀀스 변량 오토인코더인 MCIL(Lynch and Sermanet, 2020)과 비교한다.\n' +
      '\n' +
      '**결과 분석.** 표 5에 나타낸 바와 같이, 3D-VLA는 RLBench 액션 예측 내의 대부분의 태스크들에서 베이스라인 성능을 능가하거나 매칭하여, 그 계획 능력을 보여준다. 기준선은 히스토리 관찰, 객체 상태 및 현재 상태 정보를 사용하는 반면 우리는 개방 루프 제어를 통해서만 실행한다는 점에 주목할 필요가 있다. 또한 픽업 작업에서 일반화 기능이 입증되었습니다. 표 6에서, 3D-VLA는 또한 CALVIN에서 유망한 결과를 달성한다. 우리는 우열을 관심 대상을 국지화하고 행동을 추론하기 위한 풍부한 정보를 제공하는 목표 상태를 상상할 수 있는 능력에 돌린다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\hline \\hline  & \\multicolumn{5}{c}{Tasks completed in a row} \\\\  & 1 & 2 & 3 & 4 & 5 \\\\ \\hline MCIL & 28.2 & 2.5 & 0.3 & 0 & 0 \\\\\n' +
      '3D-VLA & **44.7** & **16.3** & **8.1** & **1.6** & 0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: CALVIN 데이터셋에 대한 액션 플래닝 평가.\n' +
      '\n' +
      '도 3: 생성된 RGB-D 목표 이미지의 시각화. 첫 번째 행의 결과는 보류된 훈련 데이터의 테스트 세트에서 샘플링되는 반면 두 번째 행은 인터넷 또는 일상 생활에서 수집된 보이지 않는 환경이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline  & Put & Take & Pick up & Pick up \\\\  & Knite & Umbrella & Cup & Cup (unseen) \\\\ \\hline LanCon-Learn & 28.8 & 45.6 & 23.2 & - \\\\ LanCon-Learn w/ His. & 32.2 & 50.8 & **44.2** & - \\\\\n' +
      '3D-VLA & **68** & **52** & 40 & 24 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: RLBench 데이터셋에 대한 액션 플래닝 평가.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 체화된 환경에서 추론, 이해, 생성, 계획할 수 있는 생성 세계 모델인 3D-VLA를 소개한다. 본 논문에서는 모델을 학습시키기 위해 2M 3D-Language-action 데이터 쌍을 포함하는 데이터 집합을 구성하기 위한 새로운 데이터 생성 파이프라인을 제안한다. 이러한 데이터는 태스크 캡션, 로컬라이제이션, 목표 이미지/포인트 클라우드 생성, 액션 예측 등과 같은 다양한 태스크를 수행할 수 있게 한다. 우리의 모델은 3D-LLM을 백본으로 사용하고 상호 작용 토큰을 도입하여 환경과 상호 작용한다. 우리는 구현된 AI를 위해 이미지 투 이미지 및 포인트 투 포인트 확산 모델을 훈련한다. 그들은 LLM의 멀티모달 생성 능력을 향상시키기 위해 LLM과 프로젝터에 의해 추가로 정렬된다. 이 실험은 또한 우리의 3D-VLA가 2D 기준선보다 체화된 작업에서 더 강한 능력을 가지고 있음을 보여준다.\n' +
      '\n' +
      '## Impact Statement\n' +
      '\n' +
      '본 논문은 로봇 조작의 영역에서 머신 러닝의 경계를 푸는 것을 목표로 하는 연구를 소개한다. 로봇이 물리적 세계에서 작동한다는 점을 감안할 때, 로봇 시스템이 적절하게 구성되지 않았을 때 물체 및 인간과의 충돌 가능성이 발생한다. 이 문제를 완화하기 위해 우리의 접근 방식은 시뮬레이터 환경에서 초기 교육을 받은 후 부정적인 영향을 최소화하기 위해 인간 감독 하에 실제 배치를 포함한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: few-shot learning을 위한 시각적 언어 모델. _ 신경 정보 처리 시스템_, 35:23716-23736, 2022에서의 발전.\n' +
      '* Bhat et al. (2023) Bhat, S. F., Birkl, R., Wofk, D., Wonka, P., and Muller, M. Zoedepth: 상대 깊이와 미터 깊이의 조합에 의한 제로 샷 전송. _ arXiv preprint arXiv:2302.12288_, 2023.\n' +
      '* Black et al. (2023) Black, K., Nakamoto, M., Atreya, P., Walke, H., Finn, C., Kumar, A., and Levine, S. 사전 훈련된 이미지 편집 확산 모델을 사용한 제로 샷 로봇 조작, 2023.\n' +
      '* Brohan et al. (2022) Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al. Rt-1: scale at real-world control을 위한 Robotics transformer. _ ArXiv:2212.06817_, 2022.\n' +
      '* Brohan et al. (2023) Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., et al. Rt-2: Vision-language-action models transferred web knowledge to robotic control. _ arXiv preprint arXiv:2307.15818_, 2023.\n' +
      '* Brooks et al. (2023) Brooks, T., Holynski, A., and Efros, A. A. Instructpix2pix: Learning to follow image editing instructions, 2023.\n' +
      '* Chen et al. (2023a) Chen, L., Bahl, S., and Pathak, D. Playfusion: Skill acquisition via diffusion from language-annotated play. In _Conference on Robot Learning_, pp. 2012-2029. PMLR, 2023a.\n' +
      '* Chen et al. (2017) Chen, S., Chen, X., Zhang, C., Li, M., Yu, G., Fei, H., Zhu, H., Fan, J., and Chen, T. L13da: 옴니-3d 이해, 추론 및 계획을 위한 시각적 상호작용 명령어 튜닝, 2023b.\n' +
      '* Dai et al. (2017) Dai, A., Chang, A. X., Savva, M., Halber, M., Funkhouser, T., and Niessner, M. 스캐닛: 2017년, 실내 장면의 리치 주석이 달린 3D 재구성.\n' +
      '* Damen et al. (2018) Damen, D., Doughty, H., Farinella, G. M., Fidler, S., Furnari, A., Kazakos, E., Moltisanti, D., Munro, J., Perrett, T., Price, W., et al. Scaling egocentric vision: The epic-kitchens dataset. In _Proceedings of the European conference on computer vision (ECCV)_, pp. 720-736, 2018.\n' +
      '* Dass et al. (2023) Dass, S., Yapeter, J., Zhang, J., Zhang, J., Pertsch, K., Nikolaidis, S., and Lim, J. J. Clvr jaco play dataset, 2023. URL[https://github.com/clvrai/clvr_jaco_play_dataset](https://github.com/clvrai/clvr_jaco_play_dataset).\n' +
      '* Deitke et al. (2022) Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., and Farhadi, A. Obigavverse: A universe of annotated 3d objects, 2022.\n' +
      '* Dong 등(2023) Dong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao, L., Sun, J., Zhou, H., Wei, H., Kong, X., Zhang, X., Ma, K., and Yi, L. Dreamllm: Synergistic multimodal comprehension and creation. _ arXiv preprint arXiv:2309.11499_, 2023.\n' +
      '* Driess et al. (2023a) Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al. Palm-e: embodied multimodal language model. _ arXiv preprint arXiv:2303.03378_, 2023a.\n' +
      '* Driess et al. (2023b) Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. Palm-e: embodied multimodal language model, 2023b.\n' +
      '* Esser et al. (2023) Esser, P., Chiu, J., Atighechhchian, P., Granskog, J., and Germanidis, A. Structure and content-guided video synthesis with diffusion models, 2023.\n' +
      '\n' +
      '* Fang et al. (2023) Fang, H.-S., Fang, H., Tang, Z., Liu, J., Wang, J., Zhu, H., and Lu, C. Rh20t: A robotic dataset for learning various skills in one-shot. _ arXiv preprint arXiv:2307.00595_, 2023.\n' +
      '* Feng et al. (2023) Feng, Y., Hansen, N., Xiong, Z., Rajagopalan, C., and Wang, X. 실제 세계에서 오프라인 세계 모델을 재정렬합니다. _ arXiv preprint arXiv:2310.16029_, 2023.\n' +
      '* Guo et al. (2023) Guo, Z., Zhang, R., Zhu, X., Tang, Y., Ma, X., Han, J., Chen, K., Gao, P., Li, X., Li, H., and Heng, P.-A. 포인트 바인드 & 포인트-llm: 포인트 클라우드를 3D 이해, 생성 및 지시에 대한 다중 모달리티와 정렬, 2023.\n' +
      '* Hong et al. (2023) Hong, Y., Zhen, H., Chen, P., Zheng, S., Du, Y., Chen, Z., and Gan, C. 3d-llm: Injecting the 3d world in large language models. _ arXiv preprint arXiv:2307.12981_, 2023.\n' +
      '* Hong et al. (2024) Hong, Y., Zheng, Z., Chen, P., Wang, Y., Li, J., and Gan, C. Multiply: multiisensory object-centric embodied large language model in 3d world. _ arXiv preprint arXiv:2401.08577_, 2024.\n' +
      '* Honnibal & Montani (2017) Honnibal, M. and Montani, I. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. 출연하려면 2017년\n' +
      '* Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: 대형 언어 모델의 낮은 랭크 적응. _ arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* Huang et al. (2023a) Huang, H., Wang, Z., Huang, R., Liu, L., Cheng, X., Zhao, Y., Jin, T., and Zhao, Z. Chat-3d v2: 객체 식별자를 갖는 3d 장면 및 대형 언어 모델을 브리지하는 단계, 2023a.\n' +
      '* Huang et al. (2023b) Huang, J., Yong, S., Ma, X., Linghu, X., Li, P., Wang, Y., Li, Q., Zhu, S. - C., Jia, B., and Huang, S. 3D 세계의 구체화된 일반주의자 에이전트. _ arXiv preprint arXiv:2311.12871_, 2023b.\n' +
      '* Huang et al. (2023c) Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O. K., Liu, Q., et al. Language is not all you need: Aligning perception with language models. _ arXiv preprint arXiv:2302.14045_, 2023c.\n' +
      '* James et al. (2020) James, S., Ma, Z., Arrojo, D. R., and Davison, A. J. Rlbench: The robot learning benchmark & learning environment. _ IEEE Robotics and Automation Letters_, 5(2):3019-3026, 2020.\n' +
      '*Jang et al. (2022) Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., and Finn, C. Bc-z: Zero-shot task generalization with robotic imitation learning. In _Conference on Robot Learning_, pp. 991-1002. PMLR, 2022.\n' +
      '* Kingma & Welling (2013) Kingma, D. P. and Welling, M. 자동 인코딩 변량 베이지안 arXiv preprint arXiv:1312.6114_, 2013.\n' +
      '* Li 등 (2022) Li, J., Li, D., Xiong, C., and Hoi, S. Blip: 통일된 시각 언어 이해 및 생성을 위한 언어 이미지 사전 훈련. In _International Conference on Machine Learning_, pp. 12888-12900. PMLR, 2022.\n' +
      '* Li et al. (2023a) Li, J., Chen, D., Hong, Y., Chen, Z., Chen, P., Shen, Y., and Gan, C. Covlm: Composing visual entity and relationships in large language models via communicationicative decoding. _ arXiv preprint arXiv:2311.03354_, 2023a.\n' +
      '* Li 등(2023b) Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: 냉동 이미지 인코더 및 대형 언어 모델을 사용한 부트스트래핑 언어-이미지 사전 트레이닝_ arXiv preprint arXiv:2301.12597_, 2023b.\n' +
      '* Li 등(2024) Li, Z., Zhang, C., Wang, X., Ren, R., Xu, Y., Ma, R., and Liu, X. 3dmit: 장면 이해를 위한 3d 멀티모달 명령어 튜닝, 2024.\n' +
      '* Liu et al. (2023) Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. _ arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '*Liu 등(2022) Liu, Y., Liu, Y., Jiang, C., Lyu, K., Wan, W., Shen, H., Liang, B., Fu, Z., Wang, H., and Yi, L. Hoi4d: 카테고리-레벨 인간-객체 상호작용을 위한 4d 자기중심 데이터세트. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 21013-21022, 2022.\n' +
      '* Lu et al. (2023) Lu, J., Clark, C., Zellers, R., Mottaghi, R., and Kembhavi, A. UNIFIED-IO: A unified model for vision, language, and multi-modal tasks. _The Eleventh International Conference on Learning Representations_, 2023. URL[https://openreview.net/forum?id=E01k9048soZ](https://openreview.net/forum?id=E01k9048soZ).\n' +
      '* Lynch & Sermanet(2020) Lynch, C. and Sermanet, P. Language Conditioned imitation learning over nonstructured data. _ arXiv preprint arXiv:2005.07648_, 2020.\n' +
      '* Lynch et al. (2023) Lynch, C., Wahid, A., Tompson, J., Ding, T., Betker, J., Baruch, R., Armstrong, T., and Florence, P. Interactive language: 실시간으로 로봇과 대화하는 언어 IEEE Robotics and Automation Letters_, 2023.\n' +
      '* Mandlekar et al. (2019) Mandlekar, A., Booher, J., Spero, M., Tung, A., Gupta, A., Zhu, Y., Garg, A., Savarese, S., and Fei-Fei, L. 로봇 버크로 로봇 감독을 수백 시간으로 확장: 인간의 추론과 손재주를 통한 로봇 조작 데이터 세트. In _2019 IEEE/RSJ International Conference on Intelligent Robots and Systems(IROS)_, pp. 1048-1055. IEEE, 2019.\n' +
      '* Marr(2010) Marr, D. _Vision: The Computational Investigation into the Human Representation and Processing of Visual Information_. MIT Press, 07 2010. ISBN9780262514620. doi: 10.7551/mitpress/9780262514620.001.0001. URL[https://doi.org/10.7551/mitpress/9780262514620.001.0001](https://doi.org/10.7551/mitpress/9780262514620.001.0001].\n' +
      '* Mees et al. (2022) Mees, O., Hermann, L., Rosete-Beas, E., and Burgard, W. 캘빈: 긴 지평선 로봇 조작 작업에 대한 언어 조건 정책 학습의 벤치마크. _ IEEE Robotics and Automation Letters (RA-L)_, 7(3):7327-7334, 2022.\n' +
      '* Mees et al. (2023) Mees, O., Borja-Diaz, J., and Burgard, W. 비정형 데이터보다 시각적 어포던스를 갖는 접지 언어. In _Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)_, London, UK, 2023.\n' +
      '*Nichol et al. (2022) Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., and Chen, M. Point-e: 복잡한 프롬프트로부터 3d 포인트 클라우드를 생성하는 시스템. _ ARXiv 프리프린트 arXiv:2212.08751_, 2022.\n' +
      '* Padalkar et al. (2023) Padalkar, A., Pooley, A., Jain, A., Bewley, A., Herzog, A., Irpan, A., Khazatsky, A., Rai, A., Singh, A., Brohan, A., et al. Open x-embodiment: Robotic learning datasets and rt-x models. _ arXiv preprint arXiv:2310.08864_, 2023.\n' +
      '* Palmer(1975) Palmer, S. 상황적 장면이 객체 식별에 미치는 영향 Memory & Cognition_, 3:519-526, 01 1975.\n' +
      '* Peng et al. (2023) Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., and Wei, F. Kosmos-2: Grounding multimodal large language models to the world. _ arXiv preprint arXiv:2306.14824_, 2023.\n' +
      '* Pylyshyn (2003) Pylyshyn, Z. _ 보고 시각화하는 것: 그것은 당신이 생각하는 것이 아니다. 2003년 01. ISBN 9780262316316. doi: 10.7551/mitpress/6137.001.0001.\n' +
      '* Qi et al. (2023) Qi, Z., Fang, Y., Sun, Z., Wu, X., Wu, T., Wang, J., Lin, D., and Zhao, H. Gpt4point: A unified framework for point-language understanding and generation, 2023.\n' +
      '* Ramakrishnan et al. (2024) Ramakrishnan, S. K., Gokaslan, A., Wijmans, E., Maksymets, O., Clegg, A., Turner, J., Undersander, E., Galuba, W., Westbury, A., Chang, A. X., Savva, M., Zhao, Y., and Batra, D. Habitat-matterport 3d dataset (hm3d): 1000개의 대규모 3d environment for embodied ai, 2021.\n' +
      '* Ren et al. (2024) Ren, T., Liu, S., Zeng, A., Lin, J., Li, K., Cao, H., Chen, J., Huang, X., Chen, Y., Yan, F., Zeng, Z., Zhang, H., Li, F., Yang, J., Li, H., Jiang, Q., and Zhang, L. 접지된 샘: 다양한 시각적 작업을 위한 오픈 월드 모델 조립, 2024.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.\n' +
      '* Sawhney et al. (2021) Sawhney, A., Lee, S., Zhang, K., Veloso, M., and Kroemer, O. 음식을 가지고 노는 것: 상호작용적 탐구를 통해 음식 아이템 표현을 배우는 것. In _Experimental Robotics: The 17th International Symposium_, pp. 309-322. Springer, 2021.\n' +
      '* Sermanet et al. (2023) Sermanet, P., Ding, T., Zhao, J., Xia, F., Dwibedi, D., Gopalakrishnan, K., Chan, C., Dulac-Arnold, G., Maddineni, S., Joshi, N. J., Florence, P., Han, W., Baruch, R., Lu, Y., Mirchandani, S., Xu, P., Sanketi, P., Hausman, K., Shafran, I., Ichter, B., and Cao, Y. 로보브카: 로봇 공학을 위한 멀티모달 긴 지평선 추론. _arXiv preprint arXiv:2311.00899_, 2023.\n' +
      '* Shafiullah et al. (2023) Shafiullah, N. M. M., Rai, A., Ettukuru, H., Liu, Y., Misra, I., Chintala, S., and Pinto, L. 로봇을 집으로 데려오는데 arXiv preprint arXiv:2311.16098_, 2023.\n' +
      '* Shah et al. (2023) Shah, R., Martin-Martin, R., and Zhu, Y. MUTEX: 멀티모달 작업 사양에서 통일된 정책을 학습합니다. _7th Annual Conference on Robot Learning_, 2023. URL[https://openreview.net/forum?id=pwqiqaaEzJ](https://openreview.net/forum?id=pwqiqaaEzJ).\n' +
      '* Silva et al. (2021) Silva, A., Moorman, N., Silva, W., Zaidi, Z., Gopalan, N., and Gombolay, M. Lancon-learn: 멀티 태스크 조작에서 일반화를 가능하게 하는 언어로 학습. _ IEEE Robotics and Automation Letters_, 7(2):1635-1642, 2021.\n' +
      '* Teed and Deng(2020) Teed, Z. and Deng, J. Raft: Recurrent all-pairs field transforms for optical flow. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pp. 402-419. Springer, 2020.\n' +
      '* Walke et al. (2023) Walke, H. R., Black, K., Zhao, T. Z., Vuong, Q., Zheng, C., Hansen-Estruch, P., He, A. W., Myers, V., Kim, M. J., Du, M., et al. Bridgedata v2: scale에서 로봇 학습을 위한 데이터셋. In _Conference on Robot Learning_, pp. 1723-1736. PMLR, 2023.\n' +
      '* Wu et al. (2023) Wu, S., Fei, H., Qu, L., Ji, W., and Chua, T. - S. Next-gpt: Any-to-any multimodal llm. _ arXiv preprint arXiv:2309.05519_, 2023.\n' +
      '* Xu et al. (2023) Xu, R., Wang, X., Wang, T., Chen, Y., Pang, J., and Lin, D. Pointllm: Empowering large language models to understand point cloud, 2023.\n' +
      '* Zhou et al. (2023) Zhou, J., Wang, J., Ma, B., Liu, Y. - S., Huang, T., and Wang, X. Uni3d: 2023년 규모에서 통일된 3d 표현을 탐구한다.\n' +
      '* Zhu et al. (2023) Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: 고급 대형 언어 모델로 비전 언어 이해력 향상. _ arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '도 4: ChatGPT 기반 데이터 생성에 대한 프롬프트.\n' +
      '\n' +
      '도 5: 생성된 RGBD 목표 이미지의 시각화. 첫 번째 행의 결과는 보류된 훈련 데이터의 테스트 세트에서 샘플링되고 두 번째 행은 일상 생활에서 수집된 보이지 않는 환경이다.\n' +
      '\n' +
      '도 6: 생성된 RGB-D 목표 이미지 및 목표 포인트 클라우드의 시각화. (RLBench)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>