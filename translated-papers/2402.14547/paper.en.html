<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# OmniPred: Language Models as Universal Regressors\n' +
      '\n' +
      'Xingyou Song\\({}^{1}\\)1\n' +
      '\n' +
      'Oscar Li\\({}^{2*}\\)2\n' +
      '\n' +
      'Changoo Lee\\({}^{1}\\)\n' +
      '\n' +
      'Bangding (Jeffrey) Yang\\({}^{3}\\)\n' +
      '\n' +
      'Daiyi Peng\\({}^{1}\\)\n' +
      '\n' +
      'Sagi Perel\\({}^{1}\\)\n' +
      '\n' +
      'Yutian Chen\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Google DeepMind, \\({}^{2}\\)Carnegie Mellon University, \\({}^{3}\\)Google\n' +
      '\n' +
      'Footnote 1: Equal Contribution. {Work performed as a student researcher at Google DeepMind.\n' +
      '\n' +
      'Code: [https://github.com/google-research/optformer/tree/main/optformer/omnipred](https://github.com/google-research/optformer/tree/main/optformer/omnipred)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over \\((x,y)\\) evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest black-box optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Regression is a fundamental task in experimental design, over many domains such as hyperparameter tuning, computer software, industrial engineering, and chemical discovery. The goal of regression is to predict a metric \\(y\\) of a general system given a set of input features \\(x\\). Such regressors can later be used for various applications, such as offline optimization (Kumar et al., 2022; Traubacco et al., 2022), online optimization (Cai et al., 2020), low-cost benchmarking (Zela et al., 2022; Eggensperger et al., 2015) and simulation (Mendis et al., 2019; Hashemi et al., 2018; Kaufman et al., 2021).\n' +
      '\n' +
      'In recent years, large language models (LLMs) have emerged as powerful tools for processing textual representations at scale over massive heterogeneous datasets to represent complex relationships between input features and output labels. Given that LLMs have been shown to be effective for a variety of tasks beyond natural language processing, such as coding (Li et al., 2022), symbolic mathe\n' +
      '\n' +
      'Figure 1: Overview of our method. Using heterogenous offline blackbox function evaluations collected from systems optimized using Google Vizier, we train a LM-based regressor.\n' +
      '\n' +
      '2022), and scientific reasoning (Singhal et al., 2022), it is reasonable to wonder: _Can language models be used for regression?_\n' +
      '\n' +
      'Answering this question is highly important not only for the traditional field of experimental design, but also for the ever-changing field of LLM research, especially due to recent interest in the ability to forecast outcomes of complex systems (Gruver et al., 2023) and reward modeling in reinforcement learning fine-tuning (Ziegler et al., 2019). The textual processing abilities of LLMs are particularly attractive, as they can potentially bypass the need to tediously featurize inputs (i.e. the \\(x\\)\'s) into raw numerical tensors. Prior to our work, there has been no such research specifically addressing the feasibility and utility of training a "universal" metric predictor over a large heterogenous offline dataset.\n' +
      '\n' +
      'Our core contributions in summary, are as follows:\n' +
      '\n' +
      '* To the best of our knowledge, we propose OmniPred, the first scalable yet simple metric prediction framework based on constraint-independent textual representations, applicable to general input spaces.\n' +
      '* Through only these text and token-based representations, OmniPred is capable of very accurate metric predictions over experimental design data.\n' +
      '* By simultaneously multi-task learning across vastly different input spaces and objectives, in many cases OmniPred can outperform traditional regression models such as MLPs and boosted trees.\n' +
      '* These transfer learning benefits persist even on unseen tasks after locally finetuning OmniPred on small amounts of new evaluation data.\n' +
      '\n' +
      '## 2 Related Work and Motivation\n' +
      '\n' +
      'Traditional regression methods have widely used statistical techniques such as Gaussian Processes (GPs), tree-based methods, and multilayer perceptrons (MLPs), to predict a scalar objective given a fixed-length feature vector, commonly seen in tabular data settings. Multitask (Bonilla et al., 2007) and contextual (Krause and Ong, 2011) variants have been further proposed for transfer learning purposes, but still require fixed-length tensor representations of \\(x\\), and can thus only use previous \\(x\\) from the same input space. Additional recent works utilizing deep learning-based regressors include Transformers (Hollmann et al., 2023; Huang et al., 2020), recurrent neural networks (Hashemi et al., 2018), graph neural networks (Lukasik et al., 2020; Gao et al., 2023), and deep-hierarchical GPs (Fan et al., 2024), which allow length-independence. Even so, a frequent issue is still the reliance on _tensor representations_ of \\((x,y)\\).\n' +
      '\n' +
      'Tensor representations are inherently _constraint-dependent_, as each tensor element must be in a reasonable numerical range (e.g. in \\([-1,1]\\)) as inputs to a model. Thus to represent \\(x\\), every scalar feature must be normalized against user-provided bounds, and every categorical feature must be one-hot embedded against user-provided choices. Dynamic yet minor input space changes such as new bounds or additional categories, are incompatible with this static representation. To represent \\(y\\), a raw objective in \\(\\mathbb{R}\\) must also be normalized, which can be problematic at test-time when encountering outlier \\(y\\)-values. Dealing with this issue leads to implementing complicated nonlinear warpings (Daimon, 2011; Yeo and Johnson, 2000), many of which are also data-dependent (e.g. require storing min/max values from training data).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|} \\hline Regressor & Dynamic Input Spaces? & Can Multitask? & Normalize \\(x\\)? & Normalize \\(y\\)? \\\\ \\hline MLP & No & Only fixed spaces & Yes & Yes \\\\ \\hline Tree-based & No & Only fixed spaces & Yes & **No** \\\\ \\hline Gaussian Process (GP) & No & Only fixed spaces & Yes & Yes \\\\ \\hline GNN / Transformer / RNN & No & Only fixed domains & Yes & Yes \\\\ \\hline OmniPred (Ours) & **Yes** & **Yes** & **No** & **No** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Comparisons between the flexibilties of different typical regressors.\n' +
      '\n' +
      'In principle, an ideal regressor should process \\(x\\) and output \\(y\\), both **in absolute terms**, independent of changing external statistics or search constraints. For example, if the objective is \\(f(x)=\\exp(x)\\), then the regressor\'s prediction for \\(f(2)\\) should be invariant regardless if the constraint is \\(x\\in[1,5]\\) or \\(x\\in[0,100]\\). One way to accomplish this is via _token based representations_ of the data, which is instead parsed by tokens or symbols discretely (Zhou et al., 2023). This immediately unlocks a large amount of transferrability when dealing with variable-length inputs and additional contextual metadata.\n' +
      '\n' +
      'This token-based paradigm has shown great success in the case of reinforcement learning from human feedback (Ziegler et al., 2019), where regression over textual responses (the "\\(x\\)"), also known as _reward modelling_, has been crucial to the success of recent interactive LLMs such as ChatGPT (OpenAI, 2022) and Bard (Thoppilan et al., 2022). Here, LLMs are able to imitate human ratings in the form of pairwise rankings (the "\\(y\\)") or probabilistic scores \\(y\\in[0,1]\\)(Bradley and Terry, 1952).\n' +
      '\n' +
      'While the overwhelming current focus has been on subjective _human-based_ feedback needed for determining aspects such as creativity, safety, and personality, much less attention has been given towards language models for evaluating complex and natural systems common to experimental design, which consist of much more objective and numeric-based data where \\(y\\in\\mathbb{R}\\). Given multiple works which have shown the britteness and unreliability of numerical processing in language models (Hendrycks et al., 2021; Nogueira et al., 2021), it is not immediately obvious that language models are capable of high-precision numerical prediction over token-based representations. This is a crucial technical challenge which our paper resolves in the quest for a general-purpose predictor.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '### Preliminaries and Problem Definition\n' +
      '\n' +
      'Based on standard blackbox optimization terminology (Golovin et al., 2017; Liaw et al., 2018), for a given task \\(\\mathcal{T}=(\\mathcal{X},f,\\mathcal{D},m)\\), we assume there is an inherent objective function \\(f:\\mathcal{X}\\rightarrow\\mathbb{R}\\) for which we obtain _trials_\\((x,y)\\) from evaluating suggestions \\(x\\), selected from a (possibly implicit) input space \\(\\mathcal{X}\\). We define a _study_ as an offline collection of trials \\(\\mathcal{D}=\\{(x_{1},y_{1}),...,(x_{T},y_{T})\\}\\). To distinguish between different tasks, there may be observable task-level metadata \\(m\\), which can additionally characterize the task and potentially even describes the behavior of the corresponding objective \\(f(x)\\).\n' +
      '\n' +
      'The goal of standard metric prediction is to obtain a distribution function \\(s:\\mathcal{X}\\rightarrow\\mathcal{P}(\\mathbb{R})\\) such that \\(s(x)\\) accurately approximates \\(f(x)\\) over a distribution of inputs from \\(\\mathcal{X}\\), provided that a training set \\(\\mathcal{D}^{train}\\) is given. In our particular case, we also provide our language model with multi-task training data \\(\\cup\\{\\mathcal{D}^{train}_{1},\\mathcal{D}^{train}_{2},...\\}\\) from other tasks \\(\\{\\mathcal{T}_{1},\\mathcal{T}_{2},...\\}\\). While these extraneous tasks do not contain exact evaluations over the \\(f\\) at hand and may even have different input spaces, training on such additional extraneous data may still lead to transferrability, especially for similar tasks.\n' +
      '\n' +
      'A common and unified way to measure the accuracy of predictors (deterministic or stochastic) is to compute the gap between a final pointwise prediction against the true objective value, using a regressor-dependent aggregator \\(\\alpha:\\mathcal{P}(\\mathbb{R})\\rightarrow\\mathbb{R}\\) such as median or mean. However, since different studies can have vastly different objective scales (e.g. CIFAR10 accuracies are within \\([0,1]\\) while synthetic objectives are within \\([10^{2},10^{9}]\\)), we must therefore normalize the difference based on per-study statistics, i.e. for a specific task, we define the study error as a normalized mean absolute error (MAE):\n' +
      '\n' +
      '\\[\\frac{1}{y_{\\max}-y_{\\min}}\\frac{1}{|\\mathcal{D}^{test}|}\\sum_{(x,y)\\in \\mathcal{D}^{test}}|\\alpha(s(x))-y| \\tag{1}\\]\n' +
      '\n' +
      'To prevent outlier predictions from significantly swaying average errors, we further clip error maximums to \\(1.0\\), equivalent to when the regressor simply outputs boundary values from \\(\\{y_{\\min},y_{\\max}\\}\\).\n' +
      '\n' +
      '### Language Model\n' +
      '\n' +
      'In this paper, we focus on the standard multi-task regression setting, in which for a given trial \\((x,y)\\) and task-level metadata \\(m\\), the prompt is \\((x,m)\\) and response is \\(y\\), compatible with both encoder-decoder and decoder-only settings. For simplicity, we train a standard 200M parameter T5 encoder-decoder (Raffel et al., 2020) **from scratch**. We wish to learn a single set of weights \\(\\theta\\), which can be used to form a predictor \\(s_{\\theta}(\\cdot)\\) given any arbitrary task \\(\\mathcal{T}\\). In contrast to settings such as (1) traditional regression requiring training a separate model \\(\\theta_{t}\\) for each task \\(\\mathcal{D}_{t}\\) or (2) requiring completely evaluated trajectories over specialized trial tokenizations for in-context learning (Chen et al., 2022; Hollmann et al., 2023), our setting maximizes the usage of training data, much of which may contain unfinished trajectories or non-standard \\(x\\)-formats.\n' +
      '\n' +
      '**Representation:** To facilitate training a single model over multiple heterogeneous studies, a crucial guiding principle as mentioned before, is to express \\(x\\) and \\(y\\) in _absolute_ fashion, independent of the input space and numeric scaling of the specific study. Thus, we express \\(x\\) in a _key-value_ format, directly mapping parameter names to values, but do **not** represent1 the input space \\(\\mathcal{X}\\), allowing generalizability to conditional parameters and dynamic constraints. We represent \\(y\\) with **fixed-length custom tokens** to guarantee proper decoding via token logit restriction, using specific tokens to express sign, exponent, and significant digits. Ablations over different tokenizations are conducted in Appendix A.1.\n' +
      '\n' +
      'Footnote 1: In applications such as code search, it is even infeasible to express the space of all possible programs.\n' +
      '\n' +
      '**Training:** To maintain simplicity and normalization-independence, we minimize standard cross-entropy loss over the \\(y\\)-value tokens, consistent with typical language model training. The model will thus implicitly learn numeric distances from training data.\n' +
      '\n' +
      '**Sampling and Decoding:** Through regular temperature decoding, we can repeatedly sample \\(\\hat{y}\\sim s_{\\theta}(x)\\), to approximate the underlying distribution defined over \\(\\mathbb{R}\\). To remain robust to strong outliers, we aggregate samples using the empirical median, with ablations over different aggregation methods in Appendix A.2. Since the model may need to predict over unseen regions of the input space, we can also assess the model\'s uncertainty by observing the concentration of sampled \\(\\hat{y}\\) and additionally specific log probabilities across every decoded token.\n' +
      '\n' +
      '**Local Finetuning:** To adapt to an unseen task \\(\\mathcal{T}_{u}\\), the model can further be quickly finetuned over the tasks\'s corresponding training data \\(\\mathcal{D}_{u}^{train}\\). This is a common requirement during regressor-guided search, where online function evaluations should be taken into account, e.g. in Wistuba and Grabocka (2021). Finetuning may also help to refocus over seen data, when the model is not fully optimized against a specific study, e.g. if the pretraining dataset was too large.\n' +
      '\n' +
      '## 4 Data\n' +
      '\n' +
      '### Vizier Format\n' +
      '\n' +
      'The abstractions in Section 3.1 above are concretely implemented in Open Source Vizier (Song et al., 2022), a research interface for blackbox and hyperparameter optimization. Every space \\(\\mathcal{X}\\) is defined by a list of _parameters_, each of type DOUBLE, INTEGER, DISCRETE, or CATEGORICAL with an associated value\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c} \\hline \\hline  & Language Model Textual Representation \\\\ \\hline \\(x\\) & batch\\_size:128,kernel:’rbf’,learning\\_rate:0.5,model:’svm’,optimizer:’sgd’ \\\\ \\hline \\(m\\) & title:’classification’,user:’some-person’,description:’spam detection’, objective:’accuracy’ \\\\ \\hline \\(y\\) & \\(<\\)+\\(>\\)\\(<\\)1\\(>\\)\\(<\\)2\\(>\\)\\(<\\)3\\(>\\)\\(<\\)E-2\\(>\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Textual representations used for OmniPred. \\(<\\)*\\(>\\) represents a single custom token. Input space and \\(x\\) is the same as in Figure 2. Example \\(y\\) tokenization represents a value of 1.23.\n' +
      '\n' +
      'set. Every parameter may also potentially be a _child parameter_, only active when the corresponding parent parameter is a specific value (e.g. "beta" is active only if a parent categorical parameter selects "Adam", but not "SGD"). An example is shown in Figure 2.\n' +
      '\n' +
      'Task-level metadata \\(m\\) consists of a title, owner username, description, objective name, and optional free-form text. Since the Vizier API is meant to provide an optimization service for users, there can be many sources of transferrability due to user-specific settings. These include:\n' +
      '\n' +
      '* A single user or team regularly tuning similar experiments.\n' +
      '* Multiple different users tuning similar experiments (e.g. training ResNets on CIFAR10).\n' +
      '* Similar parameters used across different experiments (e.g. "learning rate").\n' +
      '* Metadata \\(m\\) describing the nature of the objective function.\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '**BBOB (Shifted):** For precise controlled experiments where we can generate synthetic datasets and perform online evaluations, we create a multi-task version of the BBOB benchmark (ElHara et al., 2019) by applying random domain shifts \\(c\\) to transform a vanilla \\(f(x)\\) into \\(f(x-c)\\), and ranging the dimension over \\([2,6]\\). Thus each task \\(\\mathcal{T}\\) is parameterized by controllable \\(m=\\) (function class, dimension, shift), and the corresponding objective can be seen as \\(f(x,m)\\), allowing evaluation over unseen \\(m\\). For a specific task \\(\\mathcal{T}_{i}\\), we minimize the _in-study training data_ size \\(\\mathcal{D}_{i}^{train}\\) but freely vary _inter-study_ training data \\(\\{\\mathcal{D}_{j}^{train}\\}_{j\\neq i}\\) from different tasks \\(\\{\\mathcal{T}_{j}\\}_{\\neq i}\\). Thus traditional regressors (e.g. MLPs) which can only train from a single \\(\\mathcal{D}_{i}^{train}\\) will struggle to regress the corresponding \\(f_{i}\\) under this limited data condition. In contrast, the LM may perform better as it will have seen trials from other tasks whose functions share similarities with \\(f_{i}\\).\n' +
      '\n' +
      '**Real World Data:** To investigate metric prediction over real world data which contain a rich variety of tasks, we naturally will use the database from Google Vizier (Golovin et al., 2017). Because we are not constrained to only training over fully completed trajectories over flat input spaces, our data usage is much larger than the 750K studies for training OptFormer (Chen et al., 2022), as seen from Table 3.\n' +
      '\n' +
      'Since Vizier only acts as a lightweight service for blackbox optimization, we for the most part do not have online access to an actual objective \\(f(x)\\), rather\n' +
      '\n' +
      'Figure 2: Common example of a (possibly nested) space and suggestions \\(x\\) in Google Vizier.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c} Property & Statistic \\\\ \\hline \\# Studies & \\(\\mathcal{O}\\)(70M+) \\\\ \\# Trials & \\(\\mathcal{O}\\)(120B+) \\\\ \\# Distinct Users & \\(\\mathcal{O}\\)(14K) \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Relevant statistics on the Google Vizier database. We provide order estimates as there may be numerous ways to define e.g. “legitimate” studies or trials. See Appendix D for further details.\n' +
      '\n' +
      'only data samples \\(\\mathcal{D}\\), and thus we must evaluate our predictor\'s accuracy via a test set \\(\\mathcal{D}^{test}\\subset\\mathcal{D}\\). We thus need to take into account how much \\(\\mathcal{D}_{train}\\) sufficiently covers the space \\(\\mathcal{X}\\), which affects the difficulty of achieving high accuracy on the task. Influencing factors include:\n' +
      '\n' +
      '* Trial count: Users can decide when to stop tuning, and thus the size of a study can be on the order of \\(10^{0}\\) to \\(10^{5}\\).\n' +
      '* Diversity of trials: By default, a study\'s trials \\(\\{(x_{1},y_{1}),...,(x_{T},y_{T})\\}\\) form the trajectory of an optimization loop, and thus later trials may converge towards a single local optimum.\n' +
      '* Space size: Approximate cardinality of a space \\(\\mathcal{X}\\) is \\(\\exp(\\text{parameter count})\\), and thus large input spaces will naturally be less explored.\n' +
      '\n' +
      'While we apply practical processing steps such as (1) setting a maximum initial trial limit per study and (2) randomly shuffling the trials and then (3) deciding on a fixed train/validation/test splitting ratio (default 0.8/0.1/0.1), we cannot fully control whether each \\(\\mathcal{D}\\) saturates its space \\(\\mathcal{X}\\), or essentially how "easy" the task is. Instead, we use a baseline regressor trained only on \\(\\mathcal{D}^{train}\\) and evaluated on corresponding \\(\\mathcal{D}^{test}\\) as a proxy metric of the difficulty of the task.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      'We answer the following key questions:\n' +
      '\n' +
      '1. Is it possible to simultaneously regress on multiple tasks of different input spaces and objective scales?\n' +
      '2. Are there benefits to multi-task training and are textual signals useful for transfer learning?\n' +
      '3. Can finetuning improve accuracy over unseen studies outside of the pretraining set?\n' +
      '\n' +
      'Appendix A contains additional ablations on OmniPred\'s capabilities. Appendix B and C respectively contain details on language model and baseline implementations.\n' +
      '\n' +
      '### Simultaneous Regression\n' +
      '\n' +
      'In Figure 3, we visually present how a BBOB-trained model captures the overall shape of analytical functions of vastly different objective scales with high precision. Furthermore, the model is capable of expressing uncertainty estimates via i.i.d. prediction samples.\n' +
      '\n' +
      'Figure 3: Model prediction samples over selected 4D shifted BBOB functions. Empirical mode (bolded) and min/max are shown from 10 samples. Over all BBOB functions, we vary the coordinate value \\(x_{i}\\) while keeping others \\(x_{j\\neq i}\\) fixed.\n' +
      '\n' +
      'In Figure 4 for a model trained over real world data, we present an analogous visualization over hand-selected studies with drastically different input spaces, representative of objectives tuned internally at Google. These include standard machine learning (e.g. image classification and language modeling), production systems (e.g. Google bid simulation, LLM inference latency), and scientific research (e.g. protein and hardware design).\n' +
      '\n' +
      '### Multi-task Transferrability\n' +
      '\n' +
      'In this subsection, we demonstrate the model\'s ability to transfer learn, i.e. improve accuracy over a specific task using knowledge gained from other similar but non-equivalent tasks, in contrast to "single-task" regressors (described in Appendix C) which only observe training data from the task being evaluated.\n' +
      '\n' +
      'In Figure 5, we clearly see that the model\'s accuracy improves with more tasks seen in training and eventually outperforms all traditional baselines. For AutoML studies encountered in Google, the error is averaged from a fixed subset of encountered studies. For BBOB, we can further demonstrate the model\'s inter-study generalization capabilities over metadata \\(m\\) (as opposed to \\(x\\)) by evaluating on unseen tasks with new shifts not encountered during training.\n' +
      '\n' +
      'Figure 4: **Left:** Diagonal fit (_l_) is better. Model’s prediction vs. ground truth target objective over varying studies. Corporate-specific objective names are redacted. **Right:** Corresponding input spaces. “#-H, $-T” is shorthand for a conditional input space with # root parameters and $ total possible parameters.\n' +
      '\n' +
      'Figure 5: Lower (\\(\\downarrow\\)) is better. Mean study prediction error of the model when varying the amount of different studies used in training (log scale). Colored horizontal lines display single-task baseline errors.\n' +
      '\n' +
      'To verify whether the model is performing transfer learning by reading textual cues, in Table 4 we compare results against the case when data is "anonymized" using a study-dependent hash function. For BBOB, we hash metadata \\(m\\) which originally displayed (function class, dimension, shift). For AutoML, we hash parameter names and string values. Each study can still be uniquely identified and trained over, but the model can no longer observe useful correlations from common textual clues. Interestingly, the model fails to train over the full anonymized BBOB dataset, a case when the data is too large and heterogeneous.\n' +
      '\n' +
      'In Figure 6, we further see that for the model, multi-task training consistently improves over single-task training, and in regimes with relatively lower input space saturation from training data, multi-task models outperform traditional baselines over several different domains. Interestingly, a single-task model trained from scratch remains a competitive choice and for certain domains, can even outperform all other single-task baselines.\n' +
      '\n' +
      '### Finetuning Analysis\n' +
      '\n' +
      'We first examine the conditions in which finetuning may be beneficial. In Table 5, we finetune various pretrained models over AutoML studies. While there is negligible benefit in finetuning the AutoML model on its data again, we see that a model pretrained over the entire Vizier dataset is able to finetune to the same level of accuracy as a pre-trained AutoML model, while a BBOB-pretrained model leads to significantly worse results than even a single-task model. This suggests that knowledge obtained from pretraining can have a large (positive or negative) influence on transferrability over specific domains such as AutoML.\n' +
      '\n' +
      'We further examine this effect by evaluating over unseen tasks, i.e. those which were newly created after the original training set was scraped, and can contain studies from new users and objectives. In Figure\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c} \\hline  & \\multicolumn{2}{c}{Mean Study Error (\\(\\downarrow\\))} \\\\ Datasets (\\# Training Studies) & Original & Anonymized \\\\ \\hline BBOB (50K) & **0.03** & 0.46 \\\\ BBOB (Full 1M) & **0.01** & FAIL \\\\ AutoML (26.3K) & **0.19** & 0.44 \\\\ AutoML (Full 540K) & **0.15** & 0.43 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Lower (\\(\\downarrow\\)) is better. Comparisons between models trained on original vs anonymized data, across BBOB-Shifted and AutoML test trials. “FAIL” means the model failed to even train.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline  & \\multicolumn{2}{c|}{Mean Study Error (\\(\\downarrow\\)) on AutoML} \\\\ Pretraining Dataset & Before Finetuning & After Finetuning \\\\ \\hline None (Single-Task) & 0.98 & 0.20 \\\\ BBOB & 0.98 & 0.45 \\\\ AutoML & **0.15** & **0.15** \\\\ Entire Vizier & 0.31 & **0.15** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Lower (\\(\\downarrow\\)) is better. Mean study errors of pretrained models and their corresponding finetuned versions.\n' +
      '\n' +
      'Figure 6: **Left:** Lower (\\(\\downarrow\\)) is better. Aggregate error across different domains. **Right:** Statistics on domains. Shorthand notation: “TpS” = Trials per Study, “SS” = Space Size, with brackets (\\(\\#\\), \\(\\$\\)) denoting conditional space with \\(\\#\\) root parameters and \\(\\$\\) total possible parameters.\n' +
      '\n' +
      '7, we compare initialization from scratch (leading to single-task training) against a pretrained model on older Vizier data. We see that knowledge obtained from pretraining can significantly transfer over and help predictions over new tasks, although as seen on the left with three studies, there are few cases of negative transfer.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'Our OmniPred framework is a first step towards a universal regressor, capable of performing high-precision predictions over objectives of any scale from vastly different input spaces and applications. Its simple and scalable design allows transfer learning from large amounts of offline diverse evaluations, while its single-task variant can still perform competitively against a wide variety of gold-standard baselines. Furthermore, it is capable of adapting to unseen data through finetuning, while still transferring knowledge from previous data. This research lays the groundwork for exciting new potential expansions in the field of experimental design.\n' +
      '\n' +
      'Figure 7: **Left:** Lower (\\(\\downarrow\\)) is better. Example LM study errors over unseen studies filtered over random distinct users. **Right:** Aggregate comparisons across different methods over 1000 unseen studies.\n' +
      '\n' +
      '## 7 Limitations and Future Work\n' +
      '\n' +
      '**Hallucinations:** By giving the model the freedom to sample \\(y\\)-values over approximately all of \\(\\mathbb{R}\\), wildly inaccurate outlier predictions are now possible. This can be exacerbated by a wrong prediction over a significant float token (e.g. leading digit or exponent). Although for convenience, we used an unweighted cross-entropy loss in which all float tokens are of equal importance, prediction accuracy can be improved by weighting more significant tokens, making the training loss more aware of numerical distances over \\(\\mathbb{R}\\).\n' +
      '\n' +
      '**Prompt-Side Numeric Tokenization:** In this work, we directly represented numeric parameter values from \\(x\\) into the default human readable format (e.g. \\(1234.5\\) is serialized simply to \'1234.5\') to be consistent with LLM literature. This may be suboptimal, as the corresponding tokens may not exactly be digit-by-digit (e.g. SentencePiece tokenization leads to tokens {\'12\',\'3\',\'4.5\'}). One may instead potentially reuse the custom tokenization for \\(y\\)-values (e.g. \\(<\\)+\\(>\\)\\(<\\)1\\(>\\)\\(<\\)2\\(>\\)\\(<\\)3\\(>\\)\\(<\\)4\\(>\\)\\(<\\)E0\\(>\\)) or in text-space, represent using other serializations which emphasize digits atomically, e.g. \'[1 10e2 2 10e1 3 10e0 4 10e-1 ]\') as in (Nogueira et al., 2021).\n' +
      '\n' +
      '**Pretrained English Encoder:** Since \\(x\\) includes parameter names and metadata which contain English words, warm-starting from a model pretrained on English text may improve accuracy. However, most checkpoints comparable to our model\'s size (\\(<\\)1B params) such as T5-small and T5-medium are not pretrained over experimental data and are unlikely to understand the numerical meaning of e.g. \'learning_rate\'. Furthermore, when using a pretrained English model, there are numerous confounding technical choices to consider (e.g. whether to freeze the encoder, tune the learning rate, embed additional custom float tokens, and use more English-based representations of \\(x\\) and \\(m\\)), but this topic is worth pursuing in the future. In this work, we have already found that training a relatively small model from scratch can still achieve regression, thus suggesting our technique\'s broad applicability even without English understanding.\n' +
      '\n' +
      '**Computational Costs:** Compared to traditional baselines, a language model requires accelerator usage and has a relatively higher computational cost for both training and finetuning, in addition to higher inference times. In this work, we purposely designed the model to minimize costs by using \\(\\approx\\) 220M params which only requires at most 8 GPUs for training and 1 GPU for inference (see Appendix B).\n' +
      '\n' +
      '**Other Input Spaces:** The Vizier API primarily focuses on hyperparameter tuning spaces. Traditionally, more complex spaces such as combinatorics and graphs require sophisticated modeling techniques to form regressors, largely in part to difficulties in representing the \\(x\\)\'s as tensors. In addition, many applications with non-expressible spaces such program synthesis are impossible to traditionally regress over. We believe that text and token-based representations are highly promising and are widely applicable to domains previously unexplored in the field of experimental design.\n' +
      '\n' +
      '**Other Metadata:** While we performed ablations which anonymized \\(m\\) and parameter names, more investigation could be made on what types of metadata are particularly useful for prediction. Such metadata could contain _proxy metrics_ introduced by previous domain-specific works, such as Jacobian Covariance for neural architecture search (Mellor et al., 2021) and neural-network norms (Jiang et al., 2020) for the study of generalization. The relevant _code_ implementing machine learning or programming tasks may be especially important.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'We would like to thank Olivier Bachem, Hado van Hasselt, John Jumper, Aviral Kumar, Yingjie Miao, Sebastian Nowozin, Mangpo Phothilimthana, Zi Wang, Scott Yak, and Amir Yazdanbakhsh for useful discussions and Daniel Golovin for continuing support.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Bonilla et al. (2007) Edwin V Bonilla, Kian Chai, and Christopher Williams. Multi-task gaussian process prediction. In _Advances in Neural Information Processing Systems_, volume 20, 2007.\n' +
      '* Bradley and Terry (1952) Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 39(3/4):324-345, 1952. ISSN 00063444. URL [http://www.jstor.org/stable/2334029](http://www.jstor.org/stable/2334029).\n' +
      '* Cai et al. (2020) Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network and specialize it for efficient deployment. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020. URL [https://openreview.net/forum?id=HylxE1HKwS](https://openreview.net/forum?id=HylxE1HKwS).\n' +
      '* Charton (2022) Francois Charton. Linear algebra with transformers. _Trans. Mach. Learn. Res._, 2022, 2022. URL [https://openreview.net/forum?id=Hp4g7FAXXG](https://openreview.net/forum?id=Hp4g7FAXXG).\n' +
      '* Chen et al. (2022) Yutian Chen, Xingyou Song, Chansoo Lee, Zi Wang, Richard Zhang, David Dohan, Kazuya Kawakami, Greg Kochanski, Arnaud Doucet, Marc\'Aurelio Ranzato, Sagi Perel, and Nando de Freitas. Towards learning universal hyperparameter optimizers with transformers. In _NeurIPS_, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/cf6501108fced72ee5c47e2151c4e153-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/cf6501108fced72ee5c47e2151c4e153-Abstract-Conference.html).\n' +
      '* Daimon (2011) Takashi Daimon. Box-cox transformation. In _Miodrag Lovric (ed.), International Encyclopedia of Statistical Science_, pp. 176-178. Springer, 2011. doi: 10.1007/978-3-642-04898-2_152. URL [https://doi.org/10.1007/978-3-642-04898-2_152](https://doi.org/10.1007/978-3-642-04898-2_152).\n' +
      '* d\'Ascoli et al. (2022) Stephane d\'Ascoli, Pierre-Alexandre Kamienny, Guillaume Lample, and Francois Charton. Deep symbolic regression for recurrent sequences. _CoRR_, abs/2201.04600, 2022. URL [https://arxiv.org/abs/2201.04600](https://arxiv.org/abs/2201.04600).\n' +
      '* Eggensperger et al. (2015) Katharina Eggensperger, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Efficient benchmarking of hyperparameter optimizers via surrogates. In Blai Bonet and Sven Koenig (eds.), _Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA_, pp. 1114-1120. AAAI Press, 2015. doi: 10.1609/AAAI.V29I1.9375. URL [https://doi.org/10.1609/aaai.v29i1.9375](https://doi.org/10.1609/aaai.v29i1.9375).\n' +
      '* EilHara et al. (2019) Ouassim Ait ElHara, Konstantinos Varelas, Duc Manh Nguyen, Tea Tusar, Dimo Brockhoff, Nikolaus Hansen, and Anne Auger. COCO: The large scale black-box optimization benchmarking (bbob-largescale) test suite. _ArXiv_, abs/1903.06396, 2019.\n' +
      '* Fan et al. (2024) Zhou Fan, Xinran Han, and Zi Wang. Transfer learning for bayesian optimization on heterogeneous search spaces. _Transactions on Machine Learning Research_, 2024. ISSN 2835-8856. URL [https://openreview.net/forum?id=emXh4M7YH](https://openreview.net/forum?id=emXh4M7YH).\n' +
      '* Gao et al. (2023) Yanjie Gao, Xianyu Gu, Hongyu Zhang, Haoxiang Lin, and Mao Yang. Runtime performance prediction for deep learning models with graph neural network. In _45th IEEE/ACM International Conference on Software Engineering: Software Engineering in Practice, SEIP@ICSE 2023, Melbourne, Australia, May 14-20, 2023_, pp. 368-380. IEEE, 2023. doi: 10.1109/ICSE-SEIP58684.2023.00039. URL [https://doi.org/10.1109/ICSE-SEIP58684.2023.00039](https://doi.org/10.1109/ICSE-SEIP58684.2023.00039).\n' +
      '* Ghahramani et al. (2019)Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D. Sculley. Google vizier: A service for black-box optimization. In _Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17, 2017_, pp. 1487-1495. ACM, 2017. doi: 10.1145/3097983.3098043. URL [https://doi.org/10.1145/3097983.3098043](https://doi.org/10.1145/3097983.3098043).\n' +
      '* Gruver et al. (2023) Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. _CoRR_, abs/2310.07820, 2023. doi: 10.48550/ARXIV.2310.07820. URL [https://doi.org/10.48550/arXiv.2310.07820](https://doi.org/10.48550/arXiv.2310.07820).\n' +
      '* Hashemi et al. (2018) Milad Hashemi, Kevin Swersky, Jamie A. Smith, Grant Ayers, Heiner Litz, Jichuan Chang, Christos Kozyrakis, and Parthasarathy Ranganathan. Learning memory access patterns. In Jennifer G. Dy and Andreas Krause (eds.), _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pp. 1924-1933. PMLR, 2018. URL [http://proceedings.mlr.press/v80/hashemi18a.html](http://proceedings.mlr.press/v80/hashemi18a.html).\n' +
      '* Havasi et al. (2021) Marton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu, Jasper Snoek, Balaji Lakshminarayanan, Andrew Mingbo Dai, and Dustin Tran. Training independent subnetworks for robust prediction. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL [https://openreview.net/forum?id=0Gg9XnKxFAH](https://openreview.net/forum?id=0Gg9XnKxFAH).\n' +
      '* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual_, 2021. URL [https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).\n' +
      '* Hollmann et al. (2023) Noah Hollmann, Samuel Muller, Katharina Eggensperger, and Frank Hutter. Tabpfn: A transformer that solves small tabular classification problems in a second. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023. URL [https://openreview.net/pdf?id=cp5Pvc16w8_](https://openreview.net/pdf?id=cp5Pvc16w8_).\n' +
      '* Huang et al. (2020) Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar S. Karnin. Tabtransformer: Tabular data modeling using contextual embeddings. _CoRR_, abs/2012.06678, 2020. URL [https://arxiv.org/abs/2012.06678](https://arxiv.org/abs/2012.06678).\n' +
      '* Jiang et al. (2020) Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020. URL [https://openreview.net/forum?id=SJgIPJBFvH](https://openreview.net/forum?id=SJgIPJBFvH).\n' +
      '* Kaufman et al. (2021) Samuel J. Kaufman, Phitchaya Mangpo Phothilimthana, Yanqi Zhou, Charith Mendis, Sudip Roy, Amit Sabne, and Mike Burrows. A learned performance model for tensor processing units. In Alex Smola, Alex Dimakis, and Ion Stoica (eds.), _Proceedings of Machine Learning and Systems 2021, MLSys 2021, virtual, April 5-9, 2021_. mlsys.org, 2021. URL [https://proceedings.mlsys.org/paper/2021/hash/85d8ce590ad8981ca2c8286f79f59954-Abstract.html](https://proceedings.mlsys.org/paper/2021/hash/85d8ce590ad8981ca2c8286f79f59954-Abstract.html).\n' +
      '* Krause and Ong (2011) Andreas Krause and Cheng Ong. Contextual gaussian process bandit optimization. In _Advances in Neural Information Processing Systems_, 2011.\n' +
      '* Krause et al. (2018)Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pp. 66-71, 2018.\n' +
      '* Kumar et al. (2022) Aviral Kumar, Amir Yazdanbakhsh, Milad Hashemi, Kevin Swersky, and Sergey Levine. Data-driven offline optimization for architecting hardware accelerators. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. URL [https://openreview.net/forum?id=GsH-K1VYyY](https://openreview.net/forum?id=GsH-K1VYyY).\n' +
      '* December 9, 2022_, 2022.\n' +
      '* Li et al. (2022) Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. _CoRR_, abs/2203.07814, 2022. doi: 10.48550/ARXIV.2203.07814. URL [https://doi.org/10.48550/arXiv.2203.07814](https://doi.org/10.48550/arXiv.2203.07814).\n' +
      '* Liaw et al. (2018) Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E. Gonzalez, and Ion Stoica. Tune: A research platform for distributed model selection and training. _CoRR_, abs/1807.05118, 2018. URL [http://arxiv.org/abs/1807.05118](http://arxiv.org/abs/1807.05118).\n' +
      '* 42nd DAGM German Conference, DAGM GCPR 2020, Tubingen, Germany, September 28\n' +
      '- October 1, 2020, Proceedings_, volume 12544 of _Lecture Notes in Computer Science_, pp. 188-201. Springer, 2020.\n' +
      '* Mellor et al. (2021) Joe Mellor, Jack Turner, Amos J. Storkey, and Elliot J. Crowley. Neural architecture search without training. In Marina Meila and Tong Zhang (eds.), _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pp. 7588-7598. PMLR, 2021. URL [http://proceedings.mlr.press/v139/mellor21a.html](http://proceedings.mlr.press/v139/mellor21a.html).\n' +
      '* Mendis et al. (2019) Charith Mendis, Alex Renda, Saman P. Amarasinghe, and Michael Carbin. Ithemal: Accurate, portable and fast basic block throughput estimation using deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pp. 4505-4515. PMLR, 2019. URL [http://proceedings.mlr.press/v97/mendis19a.html](http://proceedings.mlr.press/v97/mendis19a.html).\n' +
      '* Nogueira et al. (2021) Rodrigo Frassetto Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of the transformers with simple arithmetic tasks. _CoRR_, abs/2102.13019, 2021. URL [https://arxiv.org/abs/2102.13019](https://arxiv.org/abs/2102.13019).\n' +
      '* Nogueira et al. (2021)OpenAI. Introducing chatgpt. 2022.\n' +
      '* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21:140:1-140:67, 2020. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).\n' +
      '* Singhal et al. [2022] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Andrew Mansfield, Blaise Aguera y Arcas, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge. _CoRR_, abs/2212.13138, 2022. doi: 10.48550/ARXIV.2212.13138. URL [https://doi.org/10.48550/arXiv.2212.13138](https://doi.org/10.48550/arXiv.2212.13138).\n' +
      '* Song et al. [2022] Xingyou Song, Sagi Perel, Chansoo Lee, Greg Kochanski, and Daniel Golovin. Open source vizier: Distributed infrastructure and API for reliable and flexible blackbox optimization. In Isabelle Guyon, Marius Lindauer, Mihaela van der Schaar, Frank Hutter, and Roman Garnett (eds.), _International Conference on Automated Machine Learning, AutoML 2022, 25-27 July 2022, Johns Hopkins University, Baltimore, MD, USA_, volume 188 of _Proceedings of Machine Learning Research_, pp. 8/1-17. PMLR, 2022. URL [https://proceedings.mlr.press/v188/song22a.html](https://proceedings.mlr.press/v188/song22a.html).\n' +
      '* Thoppilan et al. [2022] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera y Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. Lamda: Language models for dialog applications. _CoRR_, abs/2201.08239, 2022. URL [https://arxiv.org/abs/2201.08239](https://arxiv.org/abs/2201.08239).\n' +
      '* Trabucco et al. [2022] Brandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine. Design-bench: Benchmarks for data-driven offline model-based optimization. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pp. 21658-21676. PMLR, 2022. URL [https://proceedings.mlr.press/v162/tabucco22a.html](https://proceedings.mlr.press/v162/tabucco22a.html).\n' +
      '* Wistuba and Grabocka [2021] Martin Wistuba and Josif Grabocka. Few-shot bayesian optimization with deep kernel surrogates. _arXiv preprint arXiv:2101.07667_, 2021.\n' +
      '* Yeo and Johnson [2000] In-Kwon Yeo and Richard A. Johnson. A new family of power transformations to improve normality or symmetry. _Biometrika_, 87(4):954-959, 2000. ISSN 00063444. URL [http://www.jstor.org/stable/2673623](http://www.jstor.org/stable/2673623).\n' +
      '* Zela et al. [2019] Arber Zela, Julien Niklas Siems, Lucas Zimmer, Jovita Lukasik, Margret Keuper, and Frank Hutter. Surrogate NAS benchmarks: Going beyond the limited search spaces of tabular NAS benchmarks. In _TheTenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022._ OpenReview.net, 2022. URL [https://openreview.net/forum?id=OnpFa95RVqs](https://openreview.net/forum?id=OnpFa95RVqs).\n' +
      '* Zhou et al. [2023] Qi-Le Zhou, Han-Jia Ye, Le-Ye Wang, and De-Chuan Zhan. Unlocking the transferability of tokens in deep models for tabular data. _CoRR_, abs/2310.15149, 2023. doi: 10.48550/ARXIV.2310.15149. URL [https://doi.org/10.48550/arXiv.2310.15149](https://doi.org/10.48550/arXiv.2310.15149).\n' +
      '* Ziegler et al. [2019] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _CoRR_, abs/1909.08593, 2019. URL [http://arxiv.org/abs/1909.08593](http://arxiv.org/abs/1909.08593).\n' +
      '\n' +
      '## Appendix A Model Ablations\n' +
      '\n' +
      'We ablate below certain settings and scenarios which affect the model\'s prediction accuracy.\n' +
      '\n' +
      '### \\(y\\)-Tokenization\n' +
      '\n' +
      'There are multiple possible ways to represent a float (e.g. 123.4) using custom tokens. Using examples from (Charton, 2022; Nogueira et al., 2021; d\'Ascoli et al., 2022), the following are all possible representations:\n' +
      '\n' +
      '* (Default) Separate Sign and Digit-by-Digit: \\(<\\)\\(+\\)\\(>\\)\\(<\\)\\(1\\)\\(>\\)\\(<\\)\\(2\\)\\(>\\)\\(<\\)\\(3\\)\\(>\\)\\(<\\)\\(4\\)\\(>\\)\\(<\\)E-\\(2\\)\\(>\\)\n' +
      '* Merged Mantissa: \\(<\\)\\(+\\)\\(234\\)\\(>\\)\\(<\\)E-\\(2\\)\\(>\\)\n' +
      '* Exponent Before Mantissa: \\(<\\)\\(+\\)\\(>\\)\\(<\\)E-\\(2\\)\\(>\\)\\(<\\)\\(1\\)\\(>\\)\\(<\\)\\(2\\)\\(>\\)\\(<\\)\\(3\\)\\(>\\)\\(<\\)\\(4\\)\\(>\\)\n' +
      '\n' +
      'In Table 6, we see that these tokenization differences do not matter with large training data (e.g. multi-task), but matter very much in low data regimes (e.g. single-task). The poor accuracy using "Merged Mantissa" is especially apparent as it requires large amounts of data to learn differences between 18K possible mantissa tokens.\n' +
      '\n' +
      '### Effect of Sampling\n' +
      '\n' +
      'The LM can output extreme outliers in its \\(y\\)-prediction, usually due to an inaccurate prediction on the exponent token or significant digits. While such issues do not occur once the model has nearly perfectly regressed on a task (e.g. BBOB), they occur frequently on realistic tasks with nontrivial error (e.g. AutoML) and thus require techniques for correction.\n' +
      '\n' +
      'One obvious method to reduce error is to increase sample count:\n' +
      '\n' +
      'We further compare aggregation methods and see that using the median considerably outperforms both max\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c} \\hline \\hline  & \\multicolumn{2}{c}{AutoML} & \\multicolumn{2}{c}{BBOB} \\\\ Tokenization Method & Single-Task & Multi-Task & Single-Task & Multi-Task \\\\ \\hline Default & 0.21 & 0.15 & 0.17 & 0.01 \\\\ Merged Mantissa & 0.73 & 0.15 & 0.41 & 0.01 \\\\ Exponent Before Mantissa & 0.24 & 0.15 & 0.17 & 0.01 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Mean Study Error (\\(\\downarrow\\)) comparisons between different tokenization methods.\n' +
      '\n' +
      'Figure 8: Lower (\\(\\downarrow\\)) is better. Mean study error when varying the samples used during inference (log scale).\n' +
      '\n' +
      'likelihood and mean. We hypothesize this is due to the median\'s robustness to hallucinated outlier samples which can occur with relatively high probability and can also skew the mean.\n' +
      '\n' +
      '### Uncertainty\n' +
      '\n' +
      'Although the main metric used throughout our work is based on pointwise prediction, an important ability for regressors is to express uncertainty when they are unable to provide an accurate prediction. This is particularly useful in applications such as Bayesian Optimization, where uncertainty can be used as an exploration proxy. In this section, we examine whether the model can quantify uncertainty even if we did not calibrate or tune any of the models for such purposes.\n' +
      '\n' +
      'We begin by demonstrating the LM\'s ability to nonparametrically express multimodal distributions in Figure 9 when we train the model against randomly sign-flipped BBOB objectives. In contrast, traditional methods such as ensembled MLPs (Havasi et al., 2021) and Gaussian Process mixtures (Bonilla et al., 2007) must specify the mixture count a priori.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c} \\hline \\hline  & \\multicolumn{2}{c}{Mean Study Error (\\(\\downarrow\\))} \\\\ Empirical Aggregation Method & AutoML (Full 540K) & BBOB (Full 1M) \\\\ \\hline Median (default) & **0.15** & 0.01 \\\\ Max-likelihood & 0.22 & 0.01 \\\\ Mean & 0.23 & 0.01 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Lower (\\(\\downarrow\\)) is better. Comparisons between different sample aggregation methods when using 64 samples.\n' +
      '\n' +
      'Figure 9: Setting similar to Figure 3 in the main section of the paper, but with bimodality.\n' +
      '\n' +
      'Furthermore, we measured the correlation between uncertainty and error on each study. In Table 8, we report the average correlation across studies. Interestingly, although Table 7 demonstrated that mean aggregation over LM samples is worse for prediction than median aggregation, the errors are well correlated with the standard deviation of the samples.\n' +
      '\n' +
      '### Ranking and Correlation Metrics\n' +
      '\n' +
      'Although our paper focuses on pointwise predictions which are maximally informative, we can trivially bootstrap our predictions into ranking-based metrics, which may be of downstream use for evolutionary algorithms which are agnostic to \\(y\\)-scaling. We see that in general, the multi-task LM generally maintains competitive ranking metrics.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c} \\hline \\hline  & & Pearson, Kendall-Tau, Spearman Correlation (\\(\\uparrow\\)) \\\\ Regressor & Uncertainty Metric & AutoML & BBOB \\\\ \\hline Gaussian Process & Predicted SD & 0.254, 0.230, 0.307 & 0.018, 0.068, 0.048 \\\\ LM w/ mean aggregation & Sample SD & 0.560, 0.487, 0.625 & 0.360, 0.366, 0.454 \\\\ LM w/ median aggregation & Harrell-Davis SE & 0.525, 0.412, 0.539 & 0.360, 0.293, 0.380 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Higher (\\(\\uparrow\\)) is better. Rank correlation between quantified uncertainty (SD = standard deviation, SE = standard error) and actual error over studies with at least 10 test trials (all BBOB studies and 641 AutoML studies)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c c c c} \\hline \\hline  & & & \\multicolumn{4}{c}{Kendall-Tau, Spearman Correlation (\\(\\uparrow\\))} \\\\ Regressor & BBOB & Bid Simulation & Google-AML & IniT2Winit & Protein Design & V-AML (Tab) & V-AML (Text) \\\\ \\hline Gaussian Process & 0.69, 0.80 & 0.80, 0.91 & 0.04, 0.06 & 0.15, **0.81** & 0.35, 0.43 & -0.03, -0.05 & 0.30, 0.39 \\\\ Random Forest & 0.59, 0.75 & 0.71, 0.84 & 0.45, 0.57 & 0.55, 0.67 & 0.40, 0.52 & 0.56, 0.71 & 0.29, 0.38 \\\\ Tree & 0.60, 0.74 & **0.82, 0.93** & 0.37, 0.48 & 0.59, 0.71 & 0.44, 0.57 & 0.55, 0.70 & 0.28, 0.36 \\\\ MLP & 0.63, 0.76 & 0.73, 0.85 & 0.37, 0.49 & 0.53, 0.63 & 0.47, 0.60 & 0.50, 0.64 & 0.25, 0.34 \\\\ Single-task LM & 0.01, 0.01 & 0.19, 0.28 & 0.21, 0.28 & 0.05, 0.08 & 0.15, 0.20 & 0.18, 0.24 & 0.11, 0.16 \\\\ Multi-task LM & **0.92, 0.96** & 0.70, 0.84 & **0.61, 0.73** & **0.65, 0.74** & **0.72, 0.81** & **0.57, 0.72** & **0.49, 0.58** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: Higher (\\(\\uparrow\\)) is better. Ranking metrics across different regressors and tasks.\n' +
      '\n' +
      '### Study Size vs. Multi-task Gain\n' +
      '\n' +
      'Intuitively, as a task\'s space becomes more saturated with trials, single-task training becomes more sufficient for accurate prediction. In Figure 10, we plot the gains from multi-task LM training over the single-task MLP baseline to validate this hypothesis. Gains are maximized at roughly \\(\\approx 50\\) training trials and diminish as the number of training trials increases. Note that maximal gains do not occur with \\(\\approx 0\\) trials, as presumably _some_ training trials are still needed to identify the structure of a task.\n' +
      '\n' +
      'Figure 10: Higher (\\(\\uparrow\\)) is better. Study error differences between MLP and multi-task LM over individual AutoML tasks. Percentiles are computed after binning the x-axis appropriately.\n' +
      '\n' +
      'Model Details\n' +
      '\n' +
      '### Pretraining\n' +
      '\n' +
      'We pretrained our model using T5X (Raffel et al., 2020), which can be found in the open-source codebase [https://github.com/google-research/t5x](https://github.com/google-research/t5x). Important hyperparameters, most of which are defaulted, include:\n' +
      '\n' +
      '* Architecture size: 12 encoder layers, 12 decoder layers, 12 heads, 64 head dimension, 768 embedding dimension, 2048 MLP dimension.\n' +
      '* Optimizer: Adafactor with base learning rate 0.01 and square root decay. Batch size 256.\n' +
      '* Vocabulary and Tokenizer: SentencePiece tokenizer (Kudo and Richardson, 2018) with a vocabulary of 32000 subword tokens, in addition to the custom tokens for representing \\(y\\)-objectives.\n' +
      '* Early stopping: We train for a maximum of 1 million steps, but early stop based on validation loss if overfitting is detected.\n' +
      '\n' +
      'The model (\\(\\approx\\) 200M parameters) was pretrained using a 4x4 TPU V2.\n' +
      '\n' +
      '### Local Training\n' +
      '\n' +
      'During local training, data is only sourced from a single study\'s limited trials (at most 1000). The training set size can be lower than the batch size (256), and thus we must define one epoch as seeing the training data once, i.e. only one gradient step if training size \\(\\leq\\) batch size, but multiple gradient steps otherwise.\n' +
      '\n' +
      'We use the same settings from pretraining for consistency, but allow a maximum of 30 epochs. For early stopping, validation loss is now measured over the entire validation set rather than sampled batches. Further specific changes:\n' +
      '\n' +
      '* **Single-task training:** Since the model is initialized randomly, we use a larger constant learning rate of \\(10^{3}\\), consistent with early learning rates encountered during pretraining.\n' +
      '* **Finetuning:** We reload the weights in addition to the optimizer state (containing e.g. momentum parameters) from a checkpoint. We use a smaller fixed learning rate of \\(10^{-5}\\), which is 10x lower than the \\(\\mathcal{O}(10^{-4})\\) learning rate normally encountered during late stages of training.\n' +
      '\n' +
      'Due to the small training set and relatively low finetuning steps, we used a single 1x1 TPU V2.\n' +
      '\n' +
      '### Inference\n' +
      '\n' +
      'At inference time, we perform temperature sampling with a temperature of 1.0. We restrict the logits to only decode the custom floating point tokens for representing \\(y\\)-values. To maximize batch size for a 1x1 TPU V2, we generate 64 samples and select the empirical median of these floating point samples as our final prediction when computing error.\n' +
      '\n' +
      'ABLE]\n' +
      '\n' +
      '## Appendix C Baseline Details\n' +
      '\n' +
      '### Vizier Input Space\n' +
      '\n' +
      'The space is defined as a list of ParameterConfigs, each of which is one of the four primitives:\n' +
      '\n' +
      '* DOUBLE: Specifies the search range \\([l,u]\\).\n' +
      '* DISCRETE: Specifies a finite subset of \\(\\mathbb{R}\\).\n' +
      '* INTEGER: Specifies an integer range \\([l,u]\\).\n' +
      '* CATEGORICAL: Specifies a set of strings.\n' +
      '\n' +
      'Numeric (DOUBLE, DISCRETE and INTEGER) parameters may specify optional log or reverse-log scaling. The log scaling is most commonly used for tuning learning rates.\n' +
      '\n' +
      '### Data Processing for Flat Space\n' +
      '\n' +
      'A _flat space_ is where every trial in the study specifies every parameter configured in the space. In this case, we convert the parameters into the unit hypercube \\([0,1]^{d}\\). For numeric parameters, we scale all values to \\([0,1]\\) range, using the linear scaling by default unless (reverse)-log scaling was configured. For CATEGORICAL parameters, we use a one-hot encoding.\n' +
      '\n' +
      '### Data Processing for Conditional Space\n' +
      '\n' +
      'A _conditional space_ is where depending on one parameter\'s value, another parameter may be unused by the blackbox function. Conditional spaces commonly appear in AutoML settings where different model classes require a different set of parameters to be tuned. Another common use case is when we wish to optimize a numeric hyperparameter in the log scale but include 0 in the search (e.g. dropout rate, regularization coefficient), i.e. \\(\\{\\texttt{UNUSED}\\}\\cup[l,u]\\) where \\(l>0\\).\n' +
      '\n' +
      'For a categorical parameter, we simply add an extra out-of-vocabulary dimension for the one hot encoding.\n' +
      '\n' +
      'For a numeric parameter, we first convert parameter values to NaN\\(\\cup[0,1]\\), using the same scaling as in the flat space but mapping all UNUSED to NaN. We then add a custom layer (one per parameter) which is defined as:\n' +
      '\n' +
      '\\[x\\mapsto\\begin{cases}v_{p}&\\text{if $x$ is NaN},\\\\ x&\\text{otherwise}\\end{cases}\\]\n' +
      '\n' +
      'where \\(v_{p}\\) is a parameter that is trained together with the rest of the model.\n' +
      '\n' +
      '### Regressor Baselines\n' +
      '\n' +
      '**Gaussian Process:** The GP regressor model is from the GP-Bandit implementation found Open Source Vizier at [https://github.com/google/vizier](https://github.com/google/vizier) and consists of the following:\n' +
      '\n' +
      '* \\(\\alpha\\sim\\text{TruncatedLogNormal}\\) controls the amplitude of Matern5/2 kernel.\n' +
      '* \\(\\lambda_{i}\\sim\\text{TruncatedLogNormal}\\) (i.i.d. for each dimension \\(i\\)) controls the length scale for the \\(i\\)-th dimension.\n' +
      '* \\(\\sigma\\sim\\text{TruncatedLogNormal}\\) controls the Gaussian noise.\n' +
      '* \\(z\\sim\\text{Normal}(0,\\sigma)\\) is the observation noise.\n' +
      '* \\(f\\sim\\text{GP}(\\lambda,\\alpha)\\) is the function.\n' +
      '* \\(y\\sim f(x)+z\\) is the noisy function.\n' +
      '\n' +
      'The algorithm then uses L-BFGS to obtain the MAP estimate of \\(\\alpha,\\lambda\\) and \\(\\sigma\\).\n' +
      '\n' +
      'One caveat here is that this model requires a non-linear preprocessing on the observations and thus predicts \\(y\\) in the preprocessed space. This preprocessing is found to be critical to achieving stable regression across Vizier studies, which have a wide variety of value ranges. Since the preprocessing is non-linear, we cannot obtain the predictive distribution over the raw observation in a closed form. Instead, we take 1000 samples from the GP, apply the inverse of the preprocessor, and then take the mean.\n' +
      '\n' +
      '**Tree and Random Forest:** We use the standard API (XGBRegressor, XGBRFRegressor) found in XGBoost ([https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)).\n' +
      '\n' +
      '**Multilayer Perceptron:** The base architecture consists of a 2-layer ReLU dense network of hidden size 256 with a final scalar output. \\(y\\)-values are normalized using tf.keras.layers.Normalization which subtracts mean and divides by standard deviation computed empirically from the training data. Training was performed with an Adam optimizer using learning rate \\(10^{-2}\\), full batch training over 100 epochs, and mean squared error.\n' +
      '\n' +
      'Google Vizier Data\n' +
      '\n' +
      '### Study Preprocessing\n' +
      '\n' +
      'Since Google Vizier is a service in which users control evaluations, much of the raw study data can be quite chaotic. We apply certain preprocessing techniques to make the data more conducive to training and evaluation.\n' +
      '\n' +
      '**Removing bad trials:** Users may ignore or fail to evaluate a proposed \\(x\\). Furthermore, during some trials, the \\(y\\)-objective may be denoted with a special "infeasible" value (e.g. if a high batch size led to GPU out-of-memory, or training encountered NaNs). We remove such trials from consideration in our work, although one can extend our \\(y\\)-tokenization to support infeasibility in later works.\n' +
      '\n' +
      '**Trial count hard limit:** Some raw studies can contain trials in upwards of \\(10^{5}\\) trials, which could dominate the data distribution. We therefore apply a hard limit and only consider the first \\(10^{3}\\) trials per study.\n' +
      '\n' +
      '**Filtering specific users:** There are specific human and automated "power users" which produce orders of magnitude more studies than the average user. Some automated users in particular are simply automatic unit tests involving the use of Vizier. We disregard studies from these users to prevent them from dominating the data distribution.\n' +
      '\n' +
      '### Real World Data Descriptions\n' +
      '\n' +
      '**(Overall) Entire Database:** No filters were applied. All studies from the database were exported on March 31, 2023. Finetuning experiments involving unseen studies consist of studies created after this date.\n' +
      '\n' +
      '**Bid Simulation:** Contains hyperparameters for one of Google\'s bid simulators. The simulator estimates how advertisements might have performed in terms of key metrics such as cost, impressions, clicks, and conversion volume.\n' +
      '\n' +
      '**Google AutoML (Internal):** A Google internal version of Vertex AI AutoML. Uses a different input space and a different ML pipeline implementation than Vertex AI AutoML.\n' +
      '\n' +
      '**Init2Winit:** A Google research project for running deterministic, scalable, and well-documented deep learning experiments, with a particular emphasis on optimization and tuning experiments (e.g. ResNets on CIFAR10, Transformers on LM1B). Public codebase can be found in [https://github.com/google/init2winit](https://github.com/google/init2winit).\n' +
      '\n' +
      '**Protein Design:** Each space consists of 50+ parameters, each of which denotes a categorical protein building block.\n' +
      '\n' +
      '**Vertex AI AutoML (Tabular and Text):** A Vertex AI platform for automated ML model selection and training for tabular or text data. For tabular data, AutoML searches over a tree of model and optimizer types, their hyperparameters, data transformation, and other components in the ML pipeline. For text, AutoML trains an ML model to classify text data, extract information, or understand the sentiment of the authors. See [https://cloud.google.com/vertex-ai?#train-models-with-minimal-ml-expertise](https://cloud.google.com/vertex-ai?#train-models-with-minimal-ml-expertise).\n' +
      '\n' +
      '### Serialization Examples\n' +
      '\n' +
      'For transparency, we provide examples of text representations seen by the model. **Disclaimer:** Due to corporate privacy policies, we redacted (in red) some parameter names and values.\n' +
      '\n' +
      '\\begin{tabular}{c|l|l} \\hline Dataset & Example \\(x\\) & Example \\(m\\) \\\\ \\hline Google AutoML & batch_size: 128 & title: "n-w597ng9917130-q40zcb001ea71" \\\\  & model_type: REDACTED & user: REDACTED \\\\  & activation_fn: "tanh" & description: "" \\\\  & batch_norm: "True" & objective: "val_categorical_cross_entropy" \\\\  & dropout: 0.143 & amc_model_version: REDACTED \\\\  & embedding_combiner: "mean" & task_type: "multi_class_classification" \\\\  & gradient_clip_norm: 1.63e+03 & task_type: "multi_class_classification" \\\\  & num_hidden_layers: 1 & \\\\  & hidden_units[0]: 359 & \\\\  & optimizer_type: "AdamOptimizer" & \\\\  & beta1: 0.9 & beta2: 0.999 \\\\  & learning_rate: 0.926 & \\\\  & nlpvocabulary_strategy: & \\\\  & "adjusted_mutual_info" & \\\\  & vocabulary_strategy: & \\\\  & "adjusted_mutual_info" & \\\\ \\hline Init2Winit & dropout_rate: 0.6 & title: "d_spl-lmlb_trfmr-b1024-2021aug20* \\\\  & decay_factor: 0.0379 & user: REDACTED \\\\  & label_smoothing: 0.378 & description: "" \\\\  & lr_bparams.baselr: 0.00285 & objective: "valid/ce_loss" \\\\  & lr_params.decay_steps_factor: 0.854 & \\\\  & lr_params.power: 1.94 & \\\\ \\hline Protein Design & p00000000:"9" & title: "871cac3095671leab5ber371aelbb25a" \\\\  & p00000001:"16" & user: REDACTED \\\\  & p00000002:"1" & description:"" \\\\  & p00000003:"11" & objective:"" \\\\  & p00000004:"16" & \\\\  & p00000006:"9" & \\\\  & p00000006:"0" & \\\\  & p00000007:"14" & \\\\  & \\\\  & p00000047:"13" & \\\\ \\hline Vertex AI Text & universalmodeltype: & title: "2022028-621c9aea-0000-2c94" \\\\  & single_dense_feature" & user: REDACTED \\\\  & token_model_type: "cnn" & description: REDACTED \\\\  & token_bow_combiner: "sqrtn" & objective:"micro-auc-pr-label0_label" \\\\  & token_model_type: "bow" & act_study_dataset_tag:"" \\\\  & rand:0 & act_study_notes:"" \\\\  & batchsize: 4 & convnet: "2:3:4*100pa" \\\\  & dropout_keep_prob: 1 & hidden_layer_dims: 50 \\\\  & max_length: 1.54e+03 & max_num_classes_for_per_class_metric: 0 \\\\  & max_token_vocab_size: 1e+05 \\\\  & merge_word_embeddings_vocab_size: 1e+05 \\\\  & token_freq_cutoff: 1 \\\\  & tokenizer_spec: "delimiter" \\\\  & word_embedding: REDACTED \\\\  & word_embedding_dim: 100 & \\\\ \\hline \\end{tabular}\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>