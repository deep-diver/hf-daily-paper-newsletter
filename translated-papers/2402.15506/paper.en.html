<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning\n' +
      '\n' +
      'Jianguo Zhang\n' +
      '\n' +
      'Equal contributions.\n' +
      '\n' +
      'Tian Lan\n' +
      '\n' +
      'Equal contributions.\n' +
      '\n' +
      'Rithesh Murthy\n' +
      '\n' +
      'Zhiwei Liu\n' +
      '\n' +
      'Weiran Yao\n' +
      '\n' +
      'Juntao Tan\n' +
      '\n' +
      'Thai Hoang\n' +
      '\n' +
      'Liangwei Yang\n' +
      '\n' +
      'Yihao Feng\n' +
      '\n' +
      'Zuxin Liu\n' +
      '\n' +
      'Tulika Awalgaonkar\n' +
      '\n' +
      'Juan Carlos Niebles\n' +
      '\n' +
      'Silvio Savarese\n' +
      '\n' +
      'Shelby Heinecke\n' +
      '\n' +
      'Huan Wang\n' +
      '\n' +
      'Caiming Xiong\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce **AgentOhana** as a comprehensive solution to address these challenges. _AgentOhana_ aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present **xLAM-v0.1**, a large action model tailored for AI agents, which demonstrates exceptional performance across various benchmarks.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large language models (LLMs) have shown strong abilities in code generation, mathematical reasoning, conversational AI, and AI agents (OpenAI, 2023; Jiang et al., 2023; Zhang et al., 2023; Liu et al., 2023; Nijkamp et al., 2023). Among these, LLM-powered autonomous agents are gaining increasing attention. Recent frameworks for LLM agents, such as AutoGPT (Gravitas, 2023), OpenAgent (Xie et al., 2023), BOLAA (Liu et al., 2023b), XAgent (Team, 2023), and LangChain (Chase, 2023), have been designed to support agent tasks, and they have attracted significant interest in the open-source community.\n' +
      '\n' +
      'Nevertheless, many existing agents are powered by closed-source LLM APIs such as GPT-4 (OpenAI, 2023) and Gemini (Team et al., 2023), mainly because most open-source models struggle to perform long-horizon reasoning and handle complex agent tasks (Liu et al., 2023a;b). Recently, there have been ongoing efforts on training open-source models instead of relying solely on commercial APIs. For instance, AgentLM (Zeng et al., 2023), Lemur (Xu et al., 2023) and Lumos (Yin et al., 2023) are trained for agents based on Llama-2 family (Touvron et al., 2023), along with special reasoning, planning and acting prompts design such as React (Yao et al., 2023), SelfReflection (Shinn et al., 2023; Madan et al., 2023; Wang et al., 2023b) to enhance the abilities. On the same, there are works to open-source agent relevant data and train agent models such as ToolLlama (Qin et al., 2023), ToolAlpaca (Tang et al., 2023) and API-bank (Li et al., 2023) to enhance abilities on reasoning, tool usages and plannings. They have shown impressive performance on agent relevant tasks.\n' +
      '\n' +
      'However, navigating the data landscape for LLM agents becomes increasingly intricate when dealing with non-standardized data formats sourced from diverse dataset collections, especially those featuring interactions of multi-turns, as commonly observed in agent-relevant data. The heterogeneity in data structures, syntaxes, labeling conventions, and processing methods across datasets posesa formidable challenge, complicating the training and fine-tuning processes of LLMs. The lack of standardized formats introduces complexities in harmonizing disparate data sources, leading to potential biases and inconsistencies. Addressing these challenges requires developing robust preprocessing pipelines, ensuring unification and compatibility across varied data formats, and implementing strategies to mitigate biases that may arise from non-standardized representations. With the increasing demand for comprehensive and diverse datasets, establishing effective methods to manage non-standardized data formats is crucial for ensuring the robust performance of LLM agents across a spectrum of applications.\n' +
      '\n' +
      'In this work, we bridge the existing gap by building the first comprehensive agent data collection and training pipeline, named _AgentOhana_. Drawing inspiration from the notable achievements of DialogStudio (Zhang et al., 2023) and FLAN (Longpre et al., 2023) in the realms of Conversational AI and instruction-based fine-tuning, _AgentOhana_ is tailored to accommodate the wide variety of data structures and formats encountered in LLM agent trajectories. It employs specialized processes to transform disparate data into a uniform format, achieving seamless integration across multiple sources. Furthermore, the collection undergoes a meticulous filtering process to ensure high-quality trajectories, thereby introducing an extra layer of quality control. Leveraging the data standardization and unification, our training pipeline preserves independent randomness across devices during both dataset partitioning and model training, thus avoiding the inadvertent introduction of biases into the training process. This comprehensive approach guarantees that AgentOhana not only unifies trajectories across environments but also enhances the overall quality and the reliability of the collected data, as well as the performance and the robustness of the model. Our approach ensures that AgentOhana serves as a versatile and accessible resource for the research community, streamlining the development process for future applications. The contributions of this paper are as follows:\n' +
      '\n' +
      '* **Innovative Solution to Data Heterogeneity**: We introduce _AgentOhana_, a pioneering platform designed to address the complex challenges associated with the consolidation of heterogeneous data sources pertaining to multi-turn LLM agent trajectories. This contribution represents a critical step forward in overcoming the obstacles of data diversity and fragmentation.\n' +
      '* **Extensive Environmental Coverage**: _AgentOhana_ distinguishes itself by incorporating agent data from ten distinct environments, spanning a comprehensive array of scenarios. This diverse collection facilitates a broad spectrum of research opportunities, enabling investigations into various aspects of agent behavior and interaction.\n' +
      '* **Data Standardization and Unification**: A core achievement of this work is the systematic standardization and unification of LLM agent data into a consistent format. This process has enabled the creation of a generic data loader, optimizing the dataset for agent training that maintains equilibrium across different data sources and preserves independent randomness across devices.\n' +
      '* **Large Agent Model**: We have developed XLAM-v0.1, a robust large action model tailored for AI agents. Demonstrating strong performance across three rigorous benchmarks, XLAM-v0.1 showcases the potential of _AgentOhana_ in facilitating the training of high-performing AI agents.\n' +
      '\n' +
      '## 2 Methodology\n' +
      '\n' +
      'As illustrated in the workflow of _AgentOhana_ shown in Figure 1, we adopt a homogeneous multi-turn data format designed to consolidate trajectories from heterogeneous data sources. Additionally, we introduce a method called _AgentRater_ to assess and filter agent trajectories based on public or close-world models. Finally, we adopt a generic dataloader as a central component to enable smooth integration of various datasets into a distributed training process.\n' +
      '\n' +
      '### Heterogeneity of various datasets\n' +
      '\n' +
      'The formats of agent data vary significantly across different environments, posing significant difficulties and challenges in unifying data, training, and analyzing models. As illustrated in row 1 of Figure 2., trajectories from two distinct environments show markedly different data organization methods, a phenomenon observed universally across different environments. For instance, the HotpotQA environment (Liu et al., 2023b) consolidates the whole target trajectory into a single string under the _prompt_ key. This design requires efforts to retrieve _user query_, _Thought_, _Model Action: i along with _Env Observation: i_ for each step \\(i^{th}\\in[1,N]\\) from a single string. Conversely, ToolAlpaca requires the identification and matching of prompt inputs, model outputs, and observations at each step, followed by the accurate aggregation of trajectory history prior to proceeding to the next step. Appendix 4 shows more examples of the original trajectories from four environments.\n' +
      '\n' +
      '### Homogeneous multi-turn agent trajectory standardization\n' +
      '\n' +
      'To address the challenges identified, we propose a unified agent data format, as depicted in row 2 of Figure 2, showcasing our proposed unified trajectory data format. We construct a homogeneous JSON dictionary format to encapsulate all relevant content of each trajectory. Concretely, our format incorporates all important elements such as _user query_ to store the initial user query, _model name_ to identify the corresponding model and _score_ to log the available model performance score. These elements can be used to differentiate between models and are poised to facilitate the development of pairwise samples for cutting-edge training methodologies such as DPO (Rafailov et al., 2023), self-reward (Yuan et al., 2024) and AI feedback (Guo et al., 2024) LLMs. Additionally, we save auxiliary trajectory information or specific notes into _other information_, providing a reference for further analysis or model improvement initiatives.\n' +
      '\n' +
      'To enhance the preservation and analysis of multi-turn agent trajectory information, we propose a structured definition of a _step_ that captures the details of each interaction turn. A step comprises three main components: _input_, _output_, and _next observation_. The _input_ component consolidates the current prompt and a historical record of past interactions, serving as a comprehensive context for the interaction. The _output_ component captures the model\'s predictions, detailing its decision-making and planning. The _next observation_ component records the environment\'s feedback, essential for the feedback loop and system adaption.\n' +
      '\n' +
      'Our framework employs a predefined method for aggregating interaction history within the _input_ component, effectively concatenating inputs and outputs from previous steps to construct a comprehensive context. Specifically, at the \\(i^{th}\\) step, the input is formatted as _input of step 1, Action: output of step 1, Observation: next observation of step 1,..., input of step i-1, Action: output of step i-1, Observation: next observation of step i-1_. This approach ensures a detailed chronological account of interactions, facilitating a nuanced understanding of the trajectory.\n' +
      '\n' +
      'While this default aggregation strategy of _input_ is integral to our framework, we also accommodate the customization of data compilation methods. Users are encouraged to explore alternative strategies that exploit the structured _input_, _output_, and _next observation_ components, tailoring the data format to their specific research or application needs. Figure 2, row 2, illustrates the transformation of trajectories from environments such as HotpotQA and ToolAlpaca using our defined step structure, where _output_ aligns with the format specifications in the initial prompt input, demonstrating the framework\'s adaptability and practical utility.\n' +
      '\n' +
      'Figure 1: Workflow of AgentOhana. A homogeneous multi-turn data format is designed to consolidate heterogeneous trajectories from diverse data sources. _AgentRater_ then assesses and filters agent trajectories. Finally, a streaming data loader enables integration of various datasets and feeds data into a distributed training process at random.\n' +
      '\n' +
      'By standardizing the capture of interactions between agents and their environments, this methodology not only facilitates a uniform approach to data documentation but also enhances the potential for in-depth analysis and refinement of AI models. This is achieved by providing a granular view of the agent interactions, decision-making process and its results, thereby enabling a more nuanced understanding and improvement of model performance.\n' +
      '\n' +
      '### AgentRater\n' +
      '\n' +
      'Agent trajectories represent a complex subset of data distinct from general and straightforward instructional data. While datasets like Alpaca (Dubois et al., 2023) features single-turn examples, and LMSYS-Chat (Zheng et al., 2023a) includes dialogues averaging around two turns, these generally encompass simpler interaction patterns. DialogStudio (Zhang et al., 2023) does offer multi-turn dialogue examples, these are primarily confined to conversations between the user and the system, lacking interactions with external environments.\n' +
      '\n' +
      'In contrast, agent trajectories delve into more intricate scenarios where an agent interacts with complex environments such as websites, APIs, and tools. This complexity is heightened by the agent\'s\n' +
      '\n' +
      'Figure 2: Example trajectories from (A) HotpotQA, (B) ToolAlpaca to AgentOhana.\n' +
      '\n' +
      'capacity to communicate with other agents, navigate through diverse interfaces, and undertake tasks that require a sequence of interactions rather than single or limited exchanges.\n' +
      '\n' +
      'The challenge with agent trajectories extends to the evaluation of performance and quality. While some environments offer rewards as feedback for an agent\'s trajectory, such rewards are often tied to the task\'s final outcome rather than reflecting the quality of the trajectory itself. Consequently, a high reward does not necessarily indicate a flawless trajectory. For example, an agent might generate invalid actions during intermediate steps of a task. Here is partial trajectory from the Webshop environment (Yao et al., 2022): _model output of step 4: "click[old world style]", observation of step 4: "You have clicked old world style."; model output of step 5: "click[rope sausage]", observation of step 5: "Invalid action"; model output of step 6: "\' ", observation of step 6: "Invalid action"; model output of step 7: "click[By Now]", observation of step 7: "Your score (min 0.0, max 1.0): 1.0"_, where the agent randomly clicks other buttons in Webshop website or generates empty responses before buying an item.\n' +
      '\n' +
      'To mitigate the issues, we design a method, named _AgentRater_ to rate the agent trajectory based on strong public models such as Mistral (Jiang et al., 2023) or close-world APIs such as ChatGPT (OpenAI, 2023). Different with approaches (Zhang et al., 2023; Chen et al., 2023) where they rate each\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Environments \\& Data** & **\\#Sampled Trajs** & **\\#Filtered Trajs** & **\\#Average Turns** \\\\ \\hline Webshop (Yao et al., 2022) & 11200 & 2063 & 6.8 \\\\ AlfWorld (Shridhar et al., 2021) & 954 & 336 & 13.5 \\\\ HotpotQA (Yang et al., 2018) & 1740 & 402 & 4.8 \\\\ ToolBench (Qin et al., 2023) & 83771 & 30319 & 3.1 \\\\ ToolAlpaca (Tang et al., 2023) & 3936 & 3399 & 2.5 \\\\ Operating System (Liu et al., 2023a) & 647 & 195 & 3.9 \\\\ APIbank (Li et al., 2023) & 33415 & 4902 & 1.0 \\\\ DataBase (Liu et al., 2023a) & 6376 & 538 & 2.0 \\\\ Mind2Web (Deng et al., 2023) & 23378 & 122 & 1.0 \\\\ Knowledge Graph (Liu et al., 2023a) & 2501 & 324 & 6.0 \\\\ AgentOhana & 167918 & 42600 & 3.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Statistics of AgentOhana. AgentOhana consists of data from 10 different environments. _#Sampled Trajs_ indicates the trajectories sampled and filtered from the original environment, _#Filtered Trajs_ indicates the filter trajectories with the AgentRater score \\(>=4.0\\), _#Average Turns_ indicates average number of turns in the filtered trajectories. Among the environments, _Operating System_, _DataBase_, _Mind2Web_ and _Knowledge Graph_ are derived from (Zeng et al., 2023).\n' +
      '\n' +
      'Figure 3: A prompt template for the AgentRater, where an open-source model (e.g., Mistral) or close-world API (e.g., ChatGPT) will rate the whole agent trajectory based on criteriaas and then assign a score from 0 - 5.\n' +
      '\n' +
      'triplet of (instruction, input, response) pair on general instruction data as there are usually single-turn or short conversations, we rate the whole trajectory on agent data. Figure 3 shows a corresponding prompt template, where we rate the trajectory with a score 0-5 and an explanation, and they can be used to further develop better AgentRater models. Table 2.3 shows the statistics of AgentOhana.\n' +
      '\n' +
      '### Generic Dataloader\n' +
      '\n' +
      'As the protocol involves loading data in the correct format for the trainer, the implementation of a generic dataloader becomes crucial in harmonizing the entire data and training pipeline. This dataloader serves as a central component, facilitating seamless integration of diverse datasets into the training process. Its generic nature ensures flexibility and compatibility across various data formats, enabling efficient data ingestion before feeding into the training framework.\n' +
      '\n' +
      '#### 2.4.1 AgentModelDatasetBase\n' +
      '\n' +
      'We have introduced the _AgentModelDatasetBase_ class to streamline common tasks such as prompt formatting while providing a virtual template for creating individual datasets. While loading data may appear straightforward at this stage, there are still several intricate issues to address. For instance, in addition to employing a machine-assisted filter as detailed in Section 2.3, users may prefer a certain level of control over data quality. Moreover, the randomness of data batching from different datasets could pose challenges, particularly when dealing with distributed training among multiple devices, which requires a a comprehensive approach to ensure robust dataset management.\n' +
      '\n' +
      '#### 2.4.2 Custom Datasets Creation\n' +
      '\n' +
      'Individual datasetAs depicted in the following example, we begin by loading individual raw data prepared from Section 2.2, typically via the streaming mode. Then, for each dataset, we can optionally introduce the filter generator to further customize the selection of data just before feeding it into the trainer. For instance, in the following example, data with relatively low scores will be further evaluated and removed. Finally, we shuffle this dataset randomly with controlled seeding to ensure randomness and reproducibility.\n' +
      '\n' +
      '```\n' +
      'classWebshopMultiTurn(AgentModelDatasetBase):... #wecanfurtherfilterouttrajectoriesatthisstage @staticmethod def_high_score_filter_generator(data,score=0.8): fordimdata: if(!"score")>=score: yield(!"prompt":d["input"],"chosen":d["output"]) defcreate_datasets(self,seed=None): train_data=load_dataset(... streaming=self.args.streaming, ) train_data=IterableDataset.from_generator( self_high_score_filter_generator, gen_kwargs=(*data":train_data) ) train_data=train_data.shuffle(seed=seed,buffer_size=1000) returntrain_data\n' +
      '```\n' +
      '\n' +
      'Combined DatasetsOur primary focus in combining datasets lies in ensuring randomness during the batching process, particularly when dealing with multiple datasets. To achieve this, we employ the _init_device_seed_ function to diversify the controlled seeds based on the process ID when data parallelism is utilized across multiple devices. By carefully managing the seeding process, we aim to maintain a balanced distribution of data by partitioning, shuffling and interleaving data across devices while preserving randomness, thus enhancing the robustness and reproducibility of our training procedure.\n' +
      '\n' +
      '```\n' +
      'toolbench_multi_turn=ToolBenchMultiTurn(tokenizer,script_args) webshop_multi_turn=WebshopMultiTurn(tokenizer,script_args)... data=[toolbench_multi_turn,webshop_multi_turn,...] sample_probs=[0.1,0.1,...]\n' +
      '#adevice-dependentseedingwillbeutilizedbasedonthecombinationofthegivendefaultseedandtheprocessID seed=init_device_seed(seed=42) train_dataset,eval_dataset=\\interleave_data( data_objects=data, sample_probs=sample_probs, seed=seed)\n' +
      '```\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      'We adopted a supervised fine-tuning approach to enhance the performance of our agent model, xLAM-v0.1, which was initially pre-trained on the Mistral-8x7B-Instruct-v0.1 model (Jiang et al., 2024). To execute this fine-tuning process, we capitalized on the capabilities of AgentOhana. Our fine-tuning procedure was conducted concurrently on 8 Nvidia H100 GPUs, utilizing the 4-bit quantized QLoRA framework (Dettmers et al., 2023).\n' +
      '\n' +
      'Our dataset compilation strategy ensured the creation of a comprehensive training corpus with a balanced distribution of data from various sources. Maintaining this equilibrium across different sources mitigated biases and ensured the robustness of our model across diverse inputs. Furthermore, we meticulously preserved independent randomness across devices during dataset partitioning and model training, preventing the introduction of unintended biases into the training process.\n' +
      '\n' +
      'Throughout the fine-tuning process, our model traversed each individual dataset approximately 3 times on average. This multi-epoch training regimen facilitated comprehensive exposure to the dataset, enabling the model to effectively learn intricate patterns present in the training data.\n' +
      '\n' +
      '### Benchmarks\n' +
      '\n' +
      'In the subsequent sections, we will present experimental evaluations conducted across three benchmarks: Webshop (Yao et al., 2022), HotpotQA (Yang et al., 2018), and MINT-Bench (Wang et al., 2023a). Webshop introduces an online shopping environment that simulates the experience of purchasing specific product items. HotpotQA encompasses multi-hop question-answering tasks, necessitating logical reasoning across multiple Wikipedia passages via the Wikipedia API. We follow the settings established in BOLAA Liu et al. (2023b) for both Webshop and HotpotQA benchmarks. BOLAA\'s framework delineates five distinct single-agent settings alongside a multi-agent scenario, providing a structured approach to assess model performance. For the Webshop benchmark, BOLAA comprises 900 user queries, of which we serve 200 as a test subset while the remainder facilitates model development. In the context of HotpotQA, BOLAA samples 300 user questions into three difficulty levels--easy, medium, and hard--with each category containing 100 questions. These questions are exclusively reserved for model testing to ensure a rigorous evaluation process.\n' +
      '\n' +
      'We adopt BOLAA\'s specified evaluation metrics, namely the average reward, to quantify model performance across benchmarks. Within the Webshop context, the reward metric evaluates the overlapping between attributes of the purchased item and the ground-truth item, thereby measuring model accuracy in identifying user preferences. For HotpotQA, the reward is quantified as the F1 score, reflecting the accuracy of the agent\'s predicted answers against the ground-truth responses.\n' +
      '\n' +
      'Regarding to the MINT-Bench (Wang et al., 2023a) benchmark, it evaluates LLMs\' ability to solve tasks with multi-turn interactions by using tools and leveraging natural language feedback. The benchmark focuses on reasoning, coding, and decision-making through a diverse set of established evaluation datasets, and carefully curate them into a compact subset for efficient evaluation. The benchmark asks LLMs to solve tasks with different interaction limits from 1 to 5 step and quantify LLMs\' tool-augmented task-solving capability by absolute performance success rate, which measures the percentage of successful task instances as a function of interaction steps.\n' +
      '\n' +
      '### Webshop\n' +
      '\n' +
      'Table 2 showcases the performance of our model within the Webshop environment. xLAM-v0.1 consistently outperforms both GPT-3.5-Turbo and GPT-3.5-Turbo-Instruct across all agent configurations. Moreover, it surpasses GPT-4-0613 in five out of six settings, with the latter demonstrating superior planning capabilities but lower performance in reasoning, self-reflection, and multi-agent interactions. These findings underscore the robust and versatile capabilities of the xLAM model across a variety of agent scenarios.\n' +
      '\n' +
      '### HotpotQA\n' +
      '\n' +
      'Table 3 details the results in the HotpotQA environment, highlighting xLAM\'s superior performance relative to GPT-3.5-Turbo and Mistral-8x7B-Instruct-v0.1 across all settings. While GPT-4-0613 exhibits a slight performance edge, our analysis on models\' predictions reveals that it typically identifies correct answers within four steps, suggesting that it may have been trained on a substantial corpus of relevant question-answering examples, thereby possessing enhanced domain-specific knowledge compared to its counterparts.\n' +
      '\n' +
      '### Mint-Bench\n' +
      '\n' +
      'Table 4 presents the testing results on the challenging and comprehensive MINT-Bench, with baseline comparisons drawn from the official leaderboard1. The xLAM-v0.1 model secures the third\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c} \\hline \\multirow{2}{*}{LLM} & \\multicolumn{8}{c}{LAA Architecture} \\\\ \\cline{2-7}  & ZS & ZST & ReAct & PlanAct & PlanReAct & BOLAA \\\\ \\hline Llama-2-70b-chat (Touvron et al., 2023) & 0.0089 & 0.0102 & 0.4273 & 0.2809 & 0.3966 & 0.4986 \\\\ Vicuna-33b (Zheng et al., 2023b) & 0.1527 & 0.2122 & 0.1971 & 0.3766 & 0.4032 & 0.5618 \\\\ Mistral-8x7B-Instruct-v0.1 (Jiang et al., 2024) & 0.4634 & 0.4592 & 0.5638 & 0.4738 & 0.3339 & 0.5342 \\\\ GPT-3.5-Turbo & 0.4851 & 0.5058 & 0.5047 & 0.4930 & 0.5436 & 0.6354 \\\\ GPT-3.5-Turbo-Instruct & 0.3785 & 0.4195 & 0.4377 & 0.3604 & 0.4851 & 0.5811 \\\\ GPT-4.0613 & 0.5002 & 0.4783 & 0.4616 & **0.7950** & 0.4635 & 0.6129 \\\\ xLAM-v0.1 & **0.5201** & **0.5268** & **0.6486** & 0.6573 & **0.6611** & **0.6556** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Average reward on the WebShop environment. **Bold** and Underline results denote the best result and the second best result for each setting, respectively.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c} \\hline \\multirow{2}{*}{LLM} & \\multicolumn{8}{c}{LAA Architecture} \\\\ \\cline{2-7}  & ZS & ZST & ReAct & PlanAct & PlanReAct \\\\ \\hline Mistral-8x7B-Instruct-v0.1 (Jiang et al., 2024) & 0.3912 & 0.3971 & 0.3714 & 0.3195 & 0.3039 \\\\ GPT-3.5-Turbo & 0.4196 & 0.3937 & 0.3868 & 0.4182 & 0.3960 \\\\ GPT-4.0613 & **0.5801** & **0.5709** & **0.6129** & **0.5778** & **0.5716** \\\\ xLAM-v0.1 & 0.5492 & 0.4776 & 0.5020 & 0.5583 & 0.5030 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Average reward on the HotpotQA environment. **Bold** and Underline results denote the best result and the second best result for each setting, respectively.\n' +
      '\n' +
      'rank in this rigorous benchmark, outperforming other agent-based models such as Lemur-70b-Chat-v1 and AgentLM-70b, as well as general large language models (LLMs) including Claude-2 and GPT-3.5-Turbo-0613. These results highlight the exceptional capability of our model to navigate the complexities of multi-turn interactions and task resolution.\n' +
      '\n' +
      '## 4 Conclusion\n' +
      '\n' +
      'In conclusion, the creation of _AgentOhana_ represents a significant step forward in addressing the challenges inherent in consolidating diverse data of the multi-turn LLM agent trajectories. Through the development of unified data and training pipelines, we have established a framework capable of handling the intricacies of various data structures and formats, thereby ensuring compatibility across a multitude of environments. By providing a comprehensive and high-quality dataset, we aim to empower researchers and practitioners to push the boundaries of AI capabilities, ultimately contributing to the advancement of autonomous agents powered by LLMs.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Chase (2023) Harrison Chase. Langchain. [https://github.com/hwchasel7/langchain](https://github.com/hwchasel7/langchain), 2023.\n' +
      '* Chen et al. (2023) Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data. _arXiv preprint arXiv:2307.08701_, 2023.\n' +
      '* Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. _arXiv preprint arXiv:2306.06070_, 2023.\n' +
      '* Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023.\n' +
      '* Dubois et al. (2023) Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaafarm: A simulation framework for methods that learn from human feedback, 2023.\n' +
      '* Gravitas (2023) Significant Gravitas. Autogpt. [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT), 2023.\n' +
      '* Guo et al. (2024) Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. _arXiv preprint arXiv:2402.04792_, 2024.\n' +
      '* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* Jiang et al. (2024)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & 1-step & 2-step & 3-step & 4-step & 5-step \\\\ \\hline GPT-4-0613 & nan & nan & nan & nan & 69.45 \\\\ Claude-Instant-1 & 12.12 & 32.25 & 39.25 & 44.37 & 45.90 \\\\ xLAM-v0.1 & 4.10 & 28.50 & 36.01 & 42.66 & 43.96 \\\\ Claude-2 & 26.45 & 35.49 & 36.01 & 39.76 & 39.93 \\\\ Lemur-70b-Chat-v1 (Xu et al., 2023) & 3.75 & 26.96 & 35.67 & 37.54 & 37.03 \\\\ GPT-3.5-Turbo-0613 & 2.73 & 16.89 & 24.06 & 31.74 & 36.18 \\\\ AgentLM-70b (Zeng et al., 2023) & 6.48 & 17.75 & 24.91 & 28.16 & 28.67 \\\\ CodeLlama-34b (Roziere et al., 2023) & 0.17 & 16.21 & 23.04 & 25.94 & 28.16 \\\\ Llama-2-70b-chat (Touvron et al., 2023) & 4.27 & 14.33 & 15.70 & 16.55 & 17.92 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Testing results on MINT-Bench.\n' +
      '\n' +
      '* Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. _arXiv preprint arXiv:2401.04088_, 2024.\n' +
      '* Li et al. (2023) Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Apibank: A benchmark for tool-augmented llms. _arXiv preprint arXiv:2304.08244_, 2023.\n' +
      '* Liu et al. (2023a) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangling Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating lms as agents, 2023a.\n' +
      '* Liu et al. (2023b) Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, et al. Bolaa: Benchmarking and orchestrating llvm-augmented autonomous agents. _arXiv preprint arXiv:2308.05960_, 2023b.\n' +
      '* Longpre et al. (2023) Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The fian collection: Designing data and methods for effective instruction tuning. _arXiv preprint arXiv:2301.13688_, 2023.\n' +
      '* Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. _arXiv preprint arXiv:2303.17651_, 2023.\n' +
      '* Nijkamp et al. (2023) Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. Codegen2: Lessons for training llms on programming and natural languages. _ICLR_, 2023.\n' +
      '* OpenAI (2023) OpenAI. Gpt-4 technical report. _ArXiv_, 2023.\n' +
      '* Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toollm: Facilitating large language models to master 16000+ real-world apis. _arXiv preprint arXiv:2307.16789_, 2023.\n' +
      '* Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _arXiv preprint arXiv:2305.18290_, 2023.\n' +
      '* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_, 2023.\n' +
      '* Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.\n' +
      '* Shridhar et al. (2021) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2021. URL [https://arxiv.org/abs/2010.03768](https://arxiv.org/abs/2010.03768).\n' +
      '* Tang et al. (2023) Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolapaca: Generalized tool learning for language models with 3000 simulated cases. _arXiv preprint arXiv:2306.05301_, 2023.\n' +
      '* Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* XAgent Team (2023) XAgent Team. XAgent: An autonomous agent for complex task solving, 2023.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* Touvron et al. (2023)* Wang et al. (2023a) Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. _arXiv preprint arXiv:2309.10691_, 2023a.\n' +
      '* Wang et al. (2023b) Yu Wang, Zhiwei Liu, Jianguo Zhang, Weiran Yao, Shelby Heinecke, and Philip S Yu. Drdt: Dynamic reflection with divergent thinking for llm-based sequential recommendation. _arXiv preprint arXiv:2312.11336_, 2023b.\n' +
      '* Xie et al. (2023) Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. Openagents: An open platform for language agents in the wild. _arXiv preprint arXiv:2310.10634_, 2023.\n' +
      '* Xu et al. (2023) Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, et al. Lemur: Harmonizing natural language and code for language agents. _arXiv preprint arXiv:2310.06830_, 2023.\n' +
      '* Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In _Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2018.\n' +
      '* Yao et al. (2022) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. _Advances in Neural Information Processing Systems_, 35:20744-20757, 2022.\n' +
      '* Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* Yin et al. (2023) Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. Lumos: Learning agents with unified data, modular design, and open-source lms. _arXiv preprint arXiv:2311.05657_, 2023.\n' +
      '* Yuan et al. (2024) Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. _arXiv preprint arXiv:2401.10020_, 2024.\n' +
      '* Zeng et al. (2023) Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. _arXiv preprint arXiv:2310.12823_, 2023.\n' +
      '* Zhang et al. (2023) Jianguo Zhang, Kun Qian, Zhiwei Liu, Shelby Heinecke, Rui Meng, Ye Liu, Zhou Yu, Silvio Savarese, and Caiming Xiong. Dialogstudio: Towards richest and most diverse unified dataset collection for conversational ai. _arXiv preprint arXiv:2307.10172_, 2023.\n' +
      '* Zheng et al. (2023a) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, et al. Lmsys-chat-1m: A large-scale real-world llm conversation dataset. _arXiv preprint arXiv:2309.11998_, 2023a.\n' +
      '* Zheng et al. (2023b) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _arXiv preprint arXiv:2306.05685_, 2023b.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:12]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>