<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '명시되지 않은 표본에 대해 교육을 받았습니다. 모델의 훈련과 실제 사용 사이의 이러한 불일치는 내장된 메시지에 따라 불량하거나 과대평가된 탐지율을 초래한다(앱 B 참조). 이 방법은 디코더가 아닌 검출기를 훈련시키는 주벨라와 왕(2023)의 동시 작업과 더 밀접하게 일치한다.\n' +
      '\n' +
      '둘째, 그들은 로컬화되지 않고 전체 오디오를 고려하므로 더 긴 오디오 클립 내에서 AI 생성 음성의 작은 세그먼트를 식별하는 것이 어렵다. 동시 WavMark의 접근법(Chen et al., 2023)은 실제 바이너리 페이로드가 뒤따르는 동기화 패턴을 1초 간격으로 반복함으로써 이를 해결한다. 이것은 몇 가지 단점이 있다. 1초 미만에서는 사용할 수 없으며 시간 편집에 취약합니다. 동기화 비트들은 또한 인코딩된 메시지에 대한 용량을 감소시키며, 전체 용량의 31%를 차지한다. 가장 중요한 것은 동기화 비트를 디코딩하기 위한 브루트 힘 검출 알고리즘은 Sec. 5.5에서 보여주듯이 특히 비-워터마킹된 콘텐츠에서 매우 느리기 때문에 대부분의 콘텐츠가 워터마킹되지 않은 소셜 미디어 플랫폼에서 AI 생성 콘텐츠의 실시간 및 대규모 추적성에 적합하지 않다는 것이다.\n' +
      '\n' +
      '이러한 한계를 해결하기 위해 본 논문에서는 국부화 음성 워터마킹 기법인 _AudioSeal_을 소개한다. 그것은 오디오 입력으로부터 부가적 워터마크 파형을 예측하는 _generator_와 입력 오디오의 각 샘플에서 워터마크가 존재할 확률을 출력하는 _detector_의 두 네트워크를 공동으로 훈련시킨다. 검출기는 신호의 랜덤 섹션들에서 워터마크를 마스킹함으로써 더 긴 오디오 클립들에 임베딩된 합성 음성을 정확하고 견고하게 검출하도록 트레이닝된다. 트레이닝 목적은 원본 오디오와 워터마킹된 오디오 사이의 지각적 차이를 최소화하면서 검출기의 정확도를 최대화하는 것이다. 또한 오디오실(AudioSeal)을 다중 비트 워터마킹으로 확장하여 오디오가 탐지 신호에 영향을 미치지 않고 특정 모델이나 버전으로 귀속될 수 있도록 한다.\n' +
      '\n' +
      '우리는 오디오실(AudioSeal)의 성능을 평가하여 AI 생성 음성을 탐지하고 로컬화한다. 오디오 실은 검출의 견고성에 대한 최첨단 결과를 달성하여 광범위한 오디오 편집에 걸쳐 거의 완벽한 검출 속도로 수동 검출을 훨씬 능가한다. 또한 샘플 레벨 검출(해상도 1/16k 초)을 수행하여 속도와 성능 모두에서 WavMark를 능가한다. 효율적인 측면에서, 우리의 검출기는 한 번 실행되고 매 시간 단계마다 검출 로짓이 산출되어 오디오 스트림에서 워터마크를 실시간으로 검출할 수 있다. 이는 검출기 내에서 워터마크를 동기화하는 것을 필요로 하는 이전의 워터마킹 방법들과 비교하여 큰 개선을 나타내며, 이에 의해 계산 시간이 실질적으로 증가한다. 마지막으로 오디오실(AudioSeal)은 이진 메시지와 결합하여 오디오 편집이 있는 경우에도 하나의 모델에 오디오를 거의 완벽하게 속성화한다.\n' +
      '\n' +
      '우리의 전반적인 기여는 다음과 같습니다.\n' +
      '\n' +
      '* 샘플 레벨까지 AI 생성 음성의 국부적 검출을 위해 설계된 최초의 오디오 워터마킹 기법인 AudioSeal을 소개하며;\n' +
      '* 오디오실(AudioSeal)이 워터마크 신호의 더 나은 불감성을 달성할 수 있게 하는 청각 마스킹에 의해 영감을 받은 새로운 지각 손실;\n' +
      '* 오디오실(AudioSeal)은 광범위한 실생활 오디오 조작들에 대한 최첨단 견고성을 달성한다(섹션 5);\n' +
      '* AudioSeal은 계산 속도에서 최첨단 오디오 워터마킹 모델을 상당히 능가하여, 최대 2배 더 빠른 검출(섹션 5.5)을 달성하는 단계;\n' +
      '* 오픈소싱(섹션 6)시 오디오 워터마킹 기술의 보안 및 무결성에 대한 통찰력.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '이 섹션에서는 오디오 데이터에 대한 탐지 및 워터마킹 방법에 대한 개요를 제공한다. 선행작품에 대한 보완적 설명은 부록 A에서 찾아볼 수 있다.\n' +
      '\n' +
      '**합성 음성 검출**합성 음성 검출은 전통적으로 포렌식 커뮤니티에서 특징을 구축하고 가짜와 실제 사이의 통계적 차이를 이용하여 수행된다. 이러한 특징들은 핸드크래프팅될 수 있다(Sahidullah et al., 2015; Janicki, 2015; AlBadawy et al., 2019; Borrelli et al., 2021) 및/또는 학습될 수 있다(Muller et al., 2022; Barrington et al., 2023). 대부분의 오디오 생성 논문들의 접근법(Borsos et al., 2022; Kharitonov et al., 2023; Borsos et al., 2023; Le et al., 2023)은 Zhang et al.(2017)과 유사하게, 그들의 모델들이 생성하는 것에 대해 종단간 딥-러닝 분류기들을 훈련시키는 것이다. 합성 오디오와 실제 오디오를 비교할 때의 정확도는 일반적으로 양호하지만, 유통 오디오(압축, 소음, 느려짐 등)에서 좋은 성능을 발휘하지는 못한다.\n' +
      '\n' +
      '**Imperceptible watermarking.** 포렌식과는 달리 워터마킹은 야생에서 한 번 식별하고자 하는 내용을 적극적으로 마킹한다. 텍스트(Kirchenbauer et al., 2023; Aaronson and Kirchner, 2023; Fernandez et al., 2023a), 이미지(Yu et al., 2021; Fernandez et al., 2023b; Wen et al., 2023) 또는 오디오/스피치(Chen et al., 2023; Juvela and Wang, 2023)에 대한 AI 생성 콘텐츠를 추적할 수 있는 수단을 제공하므로 생성 모델의 맥락에서 새로운 관심을 즐기고 있다.\n' +
      '\n' +
      '오디오 워터마킹을 위한 전통적인 방법들은 시간 또는 주파수 도메인들 중 어느 하나에 워터마크를 삽입하는 것에 의존하였다(Lie and Chang, 2006; Kalantari et al., 2009; Natgunanathan et al., 2012; Xiang et al., 2018; Su et al., 2018; Liu et al., 2019). 일반적으로 워터마크 및 그 대응하는 디코딩 기능을 설계하기 위한 도메인 특정 특징들을 포함한다. 딥-러닝 오디오 워터마킹 방법들은 멀티-비트 워터마킹에 초점을 맞추고 생성기/디-코더 프레임워크(Tai and Mansour, 2019; Qu et al., 2023; Pavlovic et al., 2022; Liu et al., 2023; Ren et al., 2023)를 따른다. AI-생성 콘텐츠의 검출에 더 잘 적응된 제로-비트 워터마킹(Wu et al., 2023; Juvela and Wang, 2023)을 탐구한 연구는 거의 없다. 우리의 근거는 메시지 페이로드가 맨 최소로 감소함에 따라 견고성이 증가한다는 것이다(Furon, 2007).\n' +
      '\n' +
      '본 논문에서는 기존의 오디오 워터마킹에 비해 우수한 성능을 보이는 WavMark(Chen et al., 2023)에 대한 최신 기술과 비교한다. 입력 오디오의 1초 스팬에 걸쳐 32비트를 숨기는 반전가능 네트워크들에 기초한다. 검출은 0.05s의 시간 스텝으로 오디오를 따라 슬라이딩하고, 각각의 슬라이딩 윈도우에 대해 메시지를 디코딩함으로써 행해진다. 10개의 첫 번째 디코딩된 비트가 동기화 패턴과 일치하면 나머지 페이로드가 저장되며(22비트), 윈도우는 1을 직접 슬라이드할 수 있다(0.05 대신). 이 무차별 힘 검출 알고리즘은 특히 워터마크가 없을 때 엄청나게 느리는데, 그 이유는 알고리즘이 (워터마크의 부재로 인해) 입력 오디오에서 각각의 슬라이딩 윈도우에 대한 워터마크를 시도하고 디코딩하지 못할 것이기 때문이다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '그 방법은 두 개의 모델을 공동으로 훈련시킨다. 생성기는 입력 오디오에 부가되는 워터마크 신호를 생성한다. 디텍터는 로컬 탐지 로짓들을 출력한다. 훈련은 원본 오디오와 워터마크된 오디오 사이의 지각 왜곡을 최소화하고 워터마크 검출을 최대화하는 두 가지 목표의 동시 클래스를 최적화한다. 신호 수정 및 로컬리제이션에 대한 견고성을 개선하기 위해 열차 시간 증가 컬렉션을 포함한다. 추론 시간에, 로짓들은 AI-생성 콘텐츠의 검출을 가능하게 하는 워터마크된 세그먼트들을 정밀하게 로컬화한다. 선택적으로, 단일 검출기를 유지하면서 워터마크된 오디오를 모델의 버전에 속성화하기 위해 짧은 이진 식별자가 검출의 상부에 추가될 수 있다.\n' +
      '\n' +
      '### Training pipeline\n' +
      '\n' +
      '그림 2는 4개의 중요한 단계를 가진 발전기와 검출기의 관절 훈련을 보여준다:\n' +
      '\n' +
      '1. 워터마크 생성기는 파형\\(s\\in\\mathbb{R}^{T}\\)을 입력으로 하고 동일한 차원의 워터마크 파형\\(\\delta\\in\\mathbb{R}^{T}\\)을 출력하는데, 여기서 \\(T\\)은 신호의 샘플 수이다. 그리고 워터마크가 삽입된 오디오는 \\(s_{w}=s+\\delta\\)가 된다.\n' +
      '2. 샘플 레벨 로컬리제이션을 가능하게 하기 위해 묵음 및 다른 오리지널 오디오와 함께 워터마크 마스킹에 초점을 맞춘 증강 전략을 채택한다. 이것은 4가지 방법 중 하나로 \\(k\\) 시작점을 무작위로 선택하고 \\(s_{w}\\에서 다음 \\(T/2k\\) 샘플을 수정함으로써 달성된다: 확률 0.4로 원래 오디오(_i.e._\\(s_{w}(t)=s(t))로 되돌리고, 0.2로 제로(_i.e._\\(s_{w}(t)=0\\))로 대체하거나, 확률 0.2로 동일한 배치(_i.e._\\(s_{w}(t)=s^{\\prime}(t))에서 다른 오디오 신호로 대체하거나, 확률 0.2로 샘플을 전혀 수정하지 않는다.\n' +
      '3. 두 번째 부류의 증강은 오디오 편집에 대한 견고성을 보장한다. 다음 신호 변경 중 하나가 적용된다: 대역통과 필터, 부스트 오디오, 오리 오디오, 에코, 하이패스 필터, 로우패스 필터, 핑크 노이즈, 가우시안 노이즈, 더 느리고, 매끄러운, 리샘플(App. C.2의 전체 상세사항). 이러한 증강의 매개변수는 최대 견고성을 강화하기 위해 공격적인 값으로 고정되며 주어진 증강을 샘플링할 확률은 평가 검출 정확도의 역수에 비례한다. 우리는 가능한 경우 미분 가능한 방식으로 이러한 증강을 구현했으며, 그렇지 않으면 기울기들이 생성기로 역 전파될 수 있도록 하는 직선 통과 추정기(Yin et al., 2019)를 사용하여 (_e.g._ MP3 압축)을 구현했다.\n' +
      '4. Detector \\(D\\)는 원본 신호와 워터마크가 삽입된 신호를 처리하여 각 연판정마다 출력하는데, 이는 \\(D(s)\\in[0,1]^{T}\\을 의미한다. 도 3은 워터마크가 존재할 때만 검출기의 출력이 하나임을 예시한다.\n' +
      '\n' +
      '도 3: (상단) 워터마크가 5 내지 7.5 초(주황색, 5만큼 확대됨) 사이에 존재하는 음성 신호(회색). (하단) 매 시간 단계에 대한 디텍터의 출력입니다. 주황색 배경 색상은 워터마크의 존재를 나타낸다.\n' +
      '\n' +
      '도 2: 생성기-검출기 훈련 파이프라인.\n' +
      '\n' +
      '모델들의 아키텍처는 EnCodec(Defossez et al., 2022)에 기초한다. 그것들은 그림 4에 나와 있고 부록 C.3에 자세히 나와 있다.\n' +
      '\n' +
      '### Losses\n' +
      '\n' +
      '우리의 설정은 다수의 지각적 손실을 포함하고, 우리는 Defossez et al.(2022)에서와 같이 그들의 기울기를 스케일링함으로써 훈련 시간 동안 균형을 맞춘다. 사용된 손실의 전체 목록은 상세하다.\n' +
      '\n' +
      '지각적 손실은 인간의 귀에 워터마크 불감성을 강제한다. 여기에는 워터마크 신호의 세기를 줄이기 위한 \\(\\ell_{1}\\) 손실, 다중 스케일 멜 스펙트로그램 손실(Gritsenko et al., 2020) 및 다중 스케일 단-푸리에-변환 스펙트로그램에서 동작하는 적대적 네트워크에 기반한 판별 손실이 포함된다. Defossez et al.(2022)은 오디오 압축을 위한 EnCodec 모델을 훈련시키기 위해 손실의 이러한 조합을 사용한다.\n' +
      '\n' +
      '또한, 전체적으로 파형 영역에서 동작하는 새로운 시간-주파수 라우드니스 손실 **TF-Loudness**를 소개한다. 이 접근법은 워터마킹의 초기에 이미 악용된 인간 청각 시스템의 심리-음향 특성인 "audotiy masking"에 기초한다(Kirovski and Attias, 2003): 인간 청각 시스템은 동시에 그리고 동일한 주파수 범위에서 발생하는 소리를 인지하지 못한다(Schnupp et al., 2011). TF-Loudness는 다음과 같이 계산된다. 첫째, 입력 신호\\(s\\)는 중첩되지 않는 주파수 대역\\(s_{0},\\dots,s_{B-1}\\)에 기초하여 \\(B\\) 신호로 분할된다. 그 후, 모든 신호는 \\(W\\) 크기의 윈도우를 사용하여 분할되며, 중첩량은 \\(r\\)으로 표시된다. 이 과정은 원래의 오디오 신호\\(s\\)와 삽입된 워터마크\\(\\delta\\)에 모두 적용된다. 그 결과, 시간-주파수 차원에서의 신호와 워터마크의 세그먼트를 각각 \\(s_{b}^{w}\\)와 \\(\\delta_{b}^{w}\\)으로 나타내었다. 모든 시간-주파수 윈도우에 대해 우리는 라우드니스 차이를 계산하는데, 여기서 라우드니스는 ITU-R BS.1770-4 추천(전기통신 연합, 2011)을 사용하여 추정된다(상세하게는 App. C.1 참조):\n' +
      '\n' +
      '\\[l_{b}^{w}=\\mathrm{Loudness}(\\delta_{b}^{w})-\\mathrm{Loudness}(s_{b}^{w}). \\tag{1}\\]\n' +
      '\n' +
      '이는 특정 시간 윈도우(w\\)와 특정 주파수 대역(b\\) 내에서 워터마크와 원 신호 사이의 라우드니스의 불일치를 정량화한다. 상기 최종 손실은 소프트맥스 함수를 이용한 라우드니스 차이의 가중 합이다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{loud}=\\sum_{b,w}\\left(\\mathrm{softmax}(l)_{b}^{w}*l_{b}^{w} \\right)\\tag{2}\\right)\n' +
      '\n' +
      '소프트맥스는 워터마크가 이미 들리지 않는 지나치게 낮은 라우드니스를 모델이 타겟팅하는 것을 방지한다.\n' +
      '\n' +
      '마스킹된 샘플-레벨 검출 손실.로컬리제이션 손실은 워터마크된 오디오의 검출이 개별 샘플들의 레벨에서 수행되는 것을 보장한다. 각 시간 단계 \\(t\\)에 대해, 검출기의 출력 \\(D(s)_{t}\\)와 지상진리 레이블 사이의 이진 교차 엔트로피 (BCE)를 계산한다. 전반적으로, 이것은 다음과 같다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{loc}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathrm{BCE}(D(s^{\\prime})_{t},y_{t}), \\tag{3}\\)\n' +
      '\n' +
      '여기서 \\(s^{\\prime}\\)은 \\(s\\) 또는 \\(s_{w}\\일 수 있고, 시간 단계 레이블 \\(y_{t}\\)은 워터마크가 있는 경우 1로 설정되고 그렇지 않은 경우 0으로 설정된다.\n' +
      '\n' +
      '### Multi-bit watermarking\n' +
      '\n' +
      '다중 비트 워터마킹을 지원하는 방법을 확장하여 특정 모델 버전으로 오디오의 귀속을 허용한다. _ 생성_에서, 우리는 생성기의 중간에 메시지 처리 계층을 추가한다. 활성화 맵은 \\(\\mathbb{R}^{h,t^{\\prime}\\)과 이진 메시지 \\(m\\in\\{0,1\\}^{b}\\)을 사용하여 원래의 활성화 맵에 추가할 새로운 활성화 맵을 출력한다. 우리는 \\(m\\)을 \\(e=\\sum_{i=0.b-1}E_{2i+m_{i}}\\in\\mathbb{R}^{h}\\in\\mathbb{R}^{h}\\에 임베딩하며, 여기서 \\(E\\in\\mathbb{R}^{2k,h}\\)은 학습 가능한 임베딩 레이어이다. \\\\(e=\\sum_{i=0.b-1}E_{2i+m_{i}}\\in\\mathbb{R}^{h}\\in\\mathbb{R}^{2k,h}\\)은 학습 가능한 임베딩 레이어이다. \\\\(e=\\sum_{i=0.b-1}E_{2i+m_{i}}\\in\\mathbb{R}^{h}\\in\\mathbb{R}^{h}\\in\\mathbb{R}^{2k,h}\\) 이어서, 활성화 맵 사이즈(\\(t,h\\))를 맞추기 위해 시간 축을 따라 (e\\)을 \\(t\\)회 반복한다. _ 검출_에서, 우리는 검출기의 맨 끝에 \\(b\\) 선형 층을 추가한다. 그들 각각은 샘플-레벨에서 메시지의 각 비트에 대해 소프트 값을 출력한다. 따라서, 검출기는 텐서 모양의 \\(\\mathbb{R}^{t,1+b}\\)(1)을 출력한다.\n' +
      '\n' +
      '도 4: ** 건축가**. _generator_는 EnCodec의 설계에서 파생된 인코더와 디코더로 구성되며, 선택적 메시지 임베딩이 있다. 인코더는 컨볼루션 블록들과 LSTM을 포함하는 반면, 디코더는 전치된 컨볼루션들로 이 구조를 미러링한다. _detector_는 인코더와 전치 컨볼루션으로 이루어지며, 샘플-와이즈 로짓들을 계산하는 선형 레이어가 뒤따른다. 선택적으로, 다수의 선형 층들은 k-비트 메시지들을 계산하기 위해 사용될 수 있다. 자세한 내용은 앱 C.3에서 확인할 수 있습니다.\n' +
      '\n' +
      '(b\\) for the message. _ 학습시 국부화 손실(\\mathcal{L}_{loc}\\)에 디코딩 손실(\\mathcal{L}_{dec}\\)을 추가한다. 이 손실\\(\\mathcal{L}_{dec}\\)은 워터마크가 존재하는 모든 부분에 대해 원본 메시지와 디텍터의 출력 사이의 BCE를 평균한다.\n' +
      '\n' +
      '### Training details\n' +
      '\n' +
      '워터마크 생성기 및 검출기는 VoxPopuli (Wang et al., 2021) 데이터세트로부터 4.5K 시간 서브세트에 대해 트레이닝된다. 우리 생성기의 유일한 목적은 음성 콘텐츠를 생성하거나 수정할 수 있는 능력 없이 입력 오디오가 주어진 지각할 수 없는 워터마크를 생성하는 것임을 강조하는 것이 중요하다. 우리는 16 kHz의 샘플링 속도와 1초의 샘플을 사용하므로 훈련에서 \\(T=16000\\)을 사용한다. 전체 훈련은 600k 단계를 필요로 하며, Adam, 1e-4의 학습률, 32의 배치 크기를 갖는다. 드롭 증강을 위해 \\(k=5\\) 초의 \\(0.1\\) 창을 사용한다. \\(k=5\\) (h\\)은 32로 설정되고, 추가 비트 수 \\(b\\) 내지 16(주, \\(h\\)은 \\(b\\)보다 높을 필요가 있음), 예를 들어, 제로-비트 경우에 충분한다. 지각적 손실은 \\(\\lambda_{\\ell_{1}}=0.1\\), \\(\\lambda_{msspec}=2.0\\), \\(\\lambda_{adv}=4.0\\), \\(\\lambda_{loud}=10.0\\). 위치화 및 워터마킹 손실은 각각 \\(\\lambda_{loc}=10.0\\) 및 \\(\\lambda_{dec}=1.0\\)으로 가중된다.\n' +
      '\n' +
      '### 탐지, 지역화 및 속성\n' +
      '\n' +
      '추론에서, 우리는 생성기와 검출기를 사용할 수 있다:\n' +
      '\n' +
      '* _Detection_: 오디오에 워터마크가 삽입되었는지 여부를 판단한다. 이를 위해 전체 오디오에 대한 평균 디텍터의 출력을 사용하고 점수가 임계값(기본값: 0.5)을 초과하는 경우 플래그를 지정합니다.\n' +
      '* _Localization_: 워터마크가 어디에 존재하는지 정확하게 식별한다. 샘플-와이즈 검출기의 출력을 이용하고, 스코어가 임계값(기본값: 0.5)을 초과하는 경우 시간 단계를 워터마크로 표시한다.\n' +
      '* _Attribution_: 오디오를 제작한 모델 버전을 식별하여 단일 디텍터로 사용자 또는 API 간의 차별화를 가능하게 한다. 검출기의 첫 번째 출력은 검출 점수를 제공하고 나머지 출력은 귀인을 위해 사용된다. 이는 검출된 샘플들에 대한 평균 메시지를 계산하고 해밍 거리가 가장 작은 식별자를 반환함으로써 수행된다.\n' +
      '\n' +
      '##4 음성/음성 품질\n' +
      '\n' +
      '먼저, 스케일 불변 신호 대 잡음비(SI-SNR: Scale Invariant Signal to Noise Ratio)를 이용하여 워터마킹된 오디오의 품질을 평가한다: \\(\\text{SI-SNR}(s,s_{w})=10\\log_{10}\\left(\\|\\alpha s\\|_{2}^{2}/\\|\\alpha s-s_{w}\\|_{2}^{2}\\right)\\), 여기서 \\(\\alpha=(s,s_{w})/\\|s\\|_{2}^{2}\\); PESQ(Rix et al., 2001), ViSQOL(Hines et al., 2012) 및 STOI(Taal et al., 2010)\n' +
      '\n' +
      '표 1은 이러한 메트릭을 보고한다. AudioSeal은 SI-SNR을 최소화하려고 하는 WavMark(Chen et al., 2023)와 같은 워터마킹 방법과는 다르게 동작한다. 실제로 높은 SI-SNR은 실제로 좋은 지각 품질과 반드시 상관관계가 있는 것은 아니다. AudioSeal은 SI-SNR에 최적화되어 있지 않고, 오히려 지각적인 음성 품질에 최적화되어 있다. 이것은 오디오실(AudioSeal)이 일관되게 더 나은 성능을 달성하는 다른 메트릭들(PESQ, STOI, ViSQOL)에 의해 더 잘 캡처된다. 다르게 말하면, 우리의 목표는 워터마크 파워를 원본과 지각적으로 구별할 수 없도록 유지하면서 가능한 한 많이 숨기는 것이다. 도 3은 또한 워터마크 신호가 음성 파형의 형태를 따르는 방법을 시각화한다.\n' +
      '\n' +
      '우리의 주관적인 평가에 사용된 메트릭은 MUSHRA 테스트(시리즈, 2014)이다. 전체 프로토콜에 대한 자세한 내용은 부록 C.4에서 찾을 수 있으며, 본 연구에서 표본은 평균 점수\\(80.49\\)을 얻은 지상진실 표본에 매우 근접한 등급을 얻었다.\n' +
      '\n' +
      '##5 실험 및 평가\n' +
      '\n' +
      '본 절에서는 수동 분류기, 워터마킹 방법, AudioSeal의 검출 성능을 평가한다. 워터마크 검출을 위한 핵심 메트릭으로 TPR(True Positive Rate) 및 FPR(False Positive Rate)을 사용한다. TPR은 워터마크된 샘플의 정확한 식별을 측정하는 반면, FPR은 거짓으로 플래그된 정품 오디오 클립의 비율을 나타낸다. 실제 시나리오에서는 FPR을 최소화하는 것이 중요합니다. 예를 들어, 매일 10억 개의 샘플을 처리하는 플랫폼에서 FPR(10^{-3}\\)과 TPR(0.5\\)은 매일 100만 개의 샘플을 수동으로 검토해야 하지만 워터마킹된 샘플의 절반만 탐지된다.\n' +
      '\n' +
      '### 수동 분류기의 비교\n' +
      '\n' +
      '우리는 먼저 Voicebox로 생성된 샘플들에 대한 검출 결과들을 비교한다(Le et al., 2023). 음성박스 생성 오디오와 실제 오디오를 구별하기 위해 분류기를 훈련시킨 수동 설정과 비교한다. 음성상자 연구의 접근법에 따라 LibriSpeech에서 약 5초 샘플 2,000개를 평가하며, 이러한 샘플은 음성상자 생성 전 프레임(음소의 90%, 50%, 30%)을 마스킹했다. 우리는 동일한 작업, 즉 원본과 생성을 구별하거나 원본과 재합성(원본 오디오에서 Mel 스펙트로그램을 추출한 후 HiFi-GAN 보코더로 보코딩하여 생성됨)에 대해 평가한다.\n' +
      '\n' +
      '능동 및 수동 설정 모두 완벽한 클래스를 달성합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Methods** & **SI-SNR** & **PESQ** & **STOI** & **ViSQOL** & **MUSHRA** \\\\ \\hline WavMark & **38.25** & 4.302 & 0.997 & 4.730 & 71.52 \\(\\pm\\) 7.18 \\\\ AudioSeal & 26.00 & **4.470** & 0.997 & **4.829** & **77.07**\\(\\pm\\) 6.35 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **오디오 품질 메트릭**. WavMark와 같은 SNR을 최소화하는 전통적인 워터마킹 방법과 비교하여 AudioSeal은 동일하거나 더 나은 지각 품질을 달성한다.\n' +
      '\n' +
      '내추럴 박스와 보이스 박스를 구별하도록 훈련된 경우에 양탄자. 반대로 탭의 두 번째 부분입니다. 도 2는 분류기가 Voicebox와 재합성된 것을 구별하도록 훈련될 때 성능의 현저한 저하를 강조한다. 이는 재합성된 샘플들이 때때로 잘못 플래그되기 때문에 분류기가 보코더 아티팩트를 검출하고 있음을 시사한다. AI 생성 샘플의 품질이 높을수록(입력이 덜 마스킹될 때) 분류 성능이 빠르게 감소한다. 반면에, 우리의 사전적 검출은 모델-특정 아티팩트에 의존하지 않고 워터마크 존재에 의존한다. 이를 통해 모든 오디오 클립에서 완벽한 검출이 가능합니다.\n' +
      '\n' +
      '워터마킹의### 비교\n' +
      '\n' +
      '검출의 강인성과 오디오 편집 공격, 즉 시간 수정(패스트, 리샘플), 필터링(대역통과, 하이패스, 로우패스), 오디오 효과(에코, 부스트 오디오, 오리 오디오), 노이즈(핑크 노이즈, 랜덤 노이즈) 및 압축(MP3, AAC, 인코더)의 광범위한 강건성을 평가한다. 이러한 공격은 오디오 편집 소프트웨어에서 일반적으로 사용되는 광범위한 변환을 포함합니다. 엔코덱 압축을 제외한 모든 편집에 대해 훈련 범위의 매개변수를 사용한 평가가 완벽할 것이다. 일반화를 보여주기 위해 훈련 중에 사용된 것보다 공격에 더 강한 매개 변수를 선택했다(App. C.2의 세부 사항).\n' +
      '\n' +
      '복스포퓰리 유효성 검사 세트의 10k 10초 오디오에서 감지가 수행됩니다. 각 편집에 대해 먼저 워터마크가 삽입된 10k/비워터마킹된 10k 편집 오디오 클립으로 구성된 균형 데이터 세트를 구축한다. 우리는 탐지 점수의 임계값을 조정하여 정확도를 최대화하는 값을 선택하여 성능을 정량화한다(이 임계값에서 해당 TPR 및 FPR을 제공한다). ROC AUC(Receiver Operating Characteristics의 Curve Under the Areaa Under the Curve)는 모든 임계 레벨에서 성능을 전역적으로 측정하고 TPR/FPR trade-off를 포착한다. 사전적 검출을 위해 데이터 은닉 방법(_e.g._WavMark)을 적용하기 위해, 우리는 릴리즈 전에 생성된 음성에 이진 메시지(무작위로 선택됨)를 삽입한다. 그런 다음 탐지 점수는 원래 메시지와 면밀히 조사된 오디오에서 추출된 메시지 사이의 해밍 거리로 계산된다.\n' +
      '\n' +
      '탭에서 관찰합니다. 오디오실(AudioSeal)은 WavMark의 평균 AUC가 0.97 대 0.84로 전반적으로 더 견고하다. 저역통과 필터와 고역통과 필터의 성능은 AudioSeal이 낮은 주파수와 높은 주파수에서 워터마크를 삽입하지 않는다는 것을 나타낸다(WavMark는 높은 주파수에 초점을 둔다).\n' +
      '\n' +
      '일반화.오디오실(AudioSeal)이 다양한 도메인과 언어에 어떻게 일반화되는지를 평가한다. 구체적으로, 우리는 SeamlessExpressive 번역 모델(Seamless Communication et al., 2023)과 함께 Expresso 데이터세트(Nguyen et al., 2023)(studio-quality recordings)의 서브세트로부터 음성 샘플을 번역한다(Seamless Communication et al., 2023). 우리는 중국어 중국어(CMN), 프랑스어(FR), 이탈리아어(IT), 스페인어(SP)의 네 가지 목표 언어를 선택한다. 또한 비음성 AI 생성 오디오: MusicGen(Copet et al., 2023)의 음악 및 AudioGen(Kreuk et al., 2023)의 환경 사운드에 대해 평가한다. 우리는 오디오 크래프트(Copet et al., 2023)에서 사용 가능한 중간 크기의 사전 훈련된 모델을 기본 샘플링 매개변수와 함께 사용한다. MusicGen의 경우 무조건적인 생성을 사용하는 반면 AudioGen의 경우 AudioCaps의 테스트 세트로부터 프롬프트를 사용한다(Kim et al., 2019). 결과는 도메인 내 테스트 세트와 매우 유사하며 앱 5에서 찾을 수 있다.\n' +
      '\n' +
      '### Localization\n' +
      '\n' +
      '샘플 레벨 검출 정확도, 즉 올바르게 라벨링된 샘플의 비율 및 IoU(Intersection over Union)를 사용하여 로컬리제이션을 평가한다. 후자는 예측된 지면과 지면 사이의 교차점으로 정의된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & \\multicolumn{3}{c}{**AudioSeal (Ours)**} & \\multicolumn{3}{c}{**Voicebox Classif.**} \\\\ \\cline{2-7}\n' +
      '**\\% Mask** & Acc. & TPR & FPR & Acc. & TPR & FPR \\\\ \\hline \\multicolumn{7}{c}{_Original audio vs AI-generated audio_} \\\\\n' +
      '30\\% & 1.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0\n' +
      '50\\% & 1.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0\n' +
      '90\\% & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 \\\\ \\hline \\multicolumn{7}{c}{_Re-synthesized audio vs AI-generated audio_} \\\\\n' +
      '30\\% & **1.0** & **1.0** & **0.0** & 0.704 & 0.680 & 0.194\\\\\n' +
      '50\\% & **1.0** & **1.0** & **0.0** & 0.809 & 0.831 & 0.170\\\\\n' +
      '90\\% & **1.0** & **1.0** & **0.0** & 0.907 & 0.942 & 0.112 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **Voicebox 바이너리 분류기와의 비교.** 퍼센티지는 마스킹된 입력 프레임들의 프랙션을 지칭한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & \\multicolumn{3}{c}{**AudioSeal (Ours)**} & \\multicolumn{3}{c}{**WavMark**} \\\\ \\cline{2-4} Edit & Acc. TPR/FPR & AUC & Acc. TPR/FPR & AUC \\\\ \\hline None & 1.00 1.000.00 & 1.00 & 1.00 1.000.00 & 1.00 \\\\ Bandpass & 1.00 1.000.00 & 1.00 & 1.00 1.000.00 & 1.00 \\\\ Highpass & 0.61 0.820.60 & 0.61 & **1.00** 1.000.00 & **1.00** \\\\ Lowpass & **0.99** 0.990.00 & **0.99** & 0.50 1.001.00 & 0.50 \\\\ Boost & 1.00 1.000.00 & 1.00 & 1.00 1.000.00 & 1.00 \\\\ Duck & 1.00 1.000.00 & 1.00 1.00 & 1.00 1.000.00 & 1.00 \\\\ Echo & **1.00** 1.000.00 & **1.00** & 0.93 0.890.03 & 0.98 \\\\ Pink & **1.00** 1.000.00 & **1.00** & 0.88 0.810.05 & 0.93 \\\\ White & **0.91** 0.860.04 & **0.95** & 0.50 0.540.54 & 0.50 \\\\ Fast (1.25x) & **0.99** 0.990.00 & **1.00** & 0.50 0.010.00 & 0.15 \\\\ Smooth & **0.99** 0.990.00 & 1.00 & 0.94 0.930.04 & 0.98 \\\\ Resample & 1.00 1.000.00 & 1.00 & 1.00 1.000.00 & 1.00 \\\\ AAC & 1.00 1.000.00 & 1.00 1.00 1.000.00 & 1.00 \\\\ MP3 & **1.00** 1.000.00 & **1.00** & 1.00 0.990.00 & 0.99 \\\\ EnCodec & **0.98** 0.980.01 & **1.00** & 0.51 0.520.50 & 0.50 \\\\ \\hline Average & **0.96** 0.980.04 & **0.97** & 0.85 0.850.14 & 0.84 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 검출 전에 적용된 상이한 편집에 대한 **검출 결과**. Acc. (TP/FPR)은 균형 잡힌 증강 샘플 세트에서 최상의 정확도를 제공하는 임계값에 대해 얻은 정확도(및 TPR/FPR)이다. AUC는 ROC 곡선 아래의 면적이다.\n' +
      '\n' +
      '진실 검출 마스크(워터마크가 찍힌 경우 1, 그렇지 않은 경우 0)를 이들의 조합으로 나눈다. IoU는 더 긴 오디오에서 짧은 워터마크의 로컬화에 대한 더 적절한 평가이다.\n' +
      '\n' +
      '이 평가는 검출과 동일한 오디오 클립에 대해 수행된다. 그 각각에 대해, 우리는 다양한 길이의 랜덤하게 배치된 세그먼트에 워터마크를 삽입한다. WavMark를 이용한 로컬라이제이션은 브루트-포스 검출(brute-force detection)이다: 1s의 윈도우는 디폴트 시프트 값이 0.05s인 음성의 10s 위로 슬라이딩한다. 16개의 패턴 비트들 사이의 Hammming 거리가 검출 스코어로서 사용된다. 윈도우가 플러스를 트리거할 때마다 16k개의 샘플을 검출 마스크에 워터마크로 표시하였다.\n' +
      '\n' +
      '도 5는 오디오 클립에서 워터마킹된 스피치의 상이한 비율에 대한 샘플-레벨 정확도 및 IoU를 플롯한다. 오디오실(AudioSeal)은 WavMark의 0.35에 비해 단 1초의 음성만 AI 조작될 때 0.99의 IoU를 달성한다. 또한, 오디오실(AudioSeal)은 사소한 오디오 변경들을 정확하게 검출할 수 있다: 오디오에서 AI 생성 세그먼트들을 샘플 레벨(보통 1/16k sec)까지 정확하게 찾아낼 수 있는 반면, 동시 WavMark는 단지 1초의 해상도를 제공하고 따라서 IoU 측면에서 뒤처진다. 이것은 특히 간단한 단어 수정이 의미를 크게 바꿀 수 있는 음성 샘플과 관련이 있다.\n' +
      '\n' +
      '### Attribution\n' +
      '\n' +
      '오디오 클립이 주어지면, 그 목적은 이제 우리 모델의 어떤 \\(N\\) 버전이 그것을 생성했는지(검출) 그리고 만약 그렇다면, 어떤 것이 그것을 생성했는지(식별)를 찾는 것이다. 이를 위해 1k 오디오 클립을 워터마킹하기 위해 5초의 음성(10초가 아님)으로 구성된 랜덤 16비트(N^{\\prime}=100\\) 메시지를 생성한다. 이로 인해 총 100k 오디오가 생성됩니다. WavMark의 경우, 처음 16비트(/32)는 고정되고 검출 점수는 잘 디코딩된 패턴 비트의 수이고, 페이로드의 후반부는 모델 버전을 숨긴다. 검출기의 평균 출력이 FPR=\\(10^{-3}\\)에 해당하는 임계값을 초과하면 오디오 클립이 플래그된다. 다음으로, 디코딩된 워터마크와 모든 원 메시지 사이의 해밍 거리를 계산한다. 해밍 거리가 가장 작은 메시지가 선택됩니다. 우리는 추가 메시지를 추가하여 \\(N>N^{\\prime}\\) 모델을 시뮬레이션할 수 있다는 점에 주목할 필요가 있다. 이는 샘플을 생성하지 않은 버전을 나타낼 수 있다.\n' +
      '\n' +
      'FAR(False Attribution Rate)은 검출된 audioios_ 중 잘못된 attribution_의 비율이고, attribution accuracy는 모든 audioios_에 걸쳐 올바른 attribution_가 뒤따르는 검출의 비율이다. AudioSeal은 FAR이 더 높지만 전반적으로 더 나은 정확도를 제공하며, 이는 궁극적으로 중요한 것이다. 요약하면, 디커플링 검출 및 귀인은 때때로 잘못된 귀인을 희생시키면서 더 나은 검출률을 달성하고 글로벌 정확도를 더 좋게 만든다.\n' +
      '\n' +
      '### Efficiency Analysis\n' +
      '\n' +
      'AudioSeal의 효율성을 강조하기 위해 성능 분석을 수행하고 WavMark와 비교한다. 단일 Nvidia Quadro GP100 GPU를 사용하여 길이 1초에서 10초 범위의 500개의 오디오 세그먼트 데이터 세트에 두 모델의 워터마크 생성기와 검출기를 적용한다. 결과는 그림 1에 나와 있다. 6 및 탭. 6. 세대 측면에서 AudioSeal은 WavMark보다 14배 빠르다. 검출을 위해, 오디오실(AudioSeal)은 워터마크가 없는 시나리오들에서 평균적으로 2배 더 빠른 성능, 특히 485배 더 빠른 성능으로 WavMark를 능가한다(탭 6). 이러한 현저한 속도 증가는 워터마크 동기화(WavMark가 1초 스니펫에 대해 200개의 포워드에 의존한다는 것을 상기)의 필요성을 우회하는 우리 모델의 고유한 국부화된 워터마크 설계 때문이다. AudioSeal의 검출기는 검출기로의 한 번의 통과만으로 각 입력 샘플에 대한 검출 로짓들을 직접 제공하여 검출의 계산 효율을 상당히 향상시킨다. 이를 통해 본 시스템은 실시간 및 대규모 애플리케이션에 매우 적합합니다.\n' +
      '\n' +
      '##6 적대적 워터마크 제거\n' +
      '\n' +
      '우리는 이제 공격자들이 (탐지 시스템을 압도하기 위해) 진정한 샘플에 워터마크를 추가함으로써 워터마크를 "위조"할 수 있는 더 해로운 고의적인 공격을 조사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & N & 1 & \\(10\\) & \\(10^{2}\\) & \\(10^{3}\\) & \\(10^{4}\\) \\\\ \\hline \\multirow{2}{*}{FAR (\\%) \\(\\downarrow\\)} & WavMark & 0.0 & **0.20** & **0.98** & **1.87** & **4.02** \\\\  & AudioSeal & 0.0 & 2.52 & 6.83 & 8.96 & 11.84 \\\\ \\hline \\multirow{2}{*}{Acc. (\\%) \\(\\uparrow\\)} & WavMark & 58.4 & 58.2 & 57.4 & 56.6 & 54.4 \\\\  & AudioSeal & **68.2** & **65.4** & **61.4** & **59.3** & **56.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: ** 귀속 결과**. 우리는 귀인(Acc.)과 거짓 귀인 비율(FAR)의 정확성을 보고한다. 검출은 FPR=\\(10^{-3}\\)에서 수행되며, 속성은 디코딩된 메시지를 \\(N\\) 버전 중 하나와 일치시킨다. 우리는 탭 3의 편집에 대한 평균 결과를 보고한다.\n' +
      '\n' +
      '도 5: **로컬라이제이션 결과** 샘플-레벨 정확도 및 인터섹션 오버 유니온(IoU) 메트릭의 관점에서 워터마크된 오디오 신호의 상이한 지속기간에 걸친 (\\(\\uparrow\\)이 더 좋다).\n' +
      '\n' +
      '또는 탐지를 피하기 위해 "제거"합니다. 본 연구 결과는 이러한 적대자에 대한 워터마킹의 효과를 유지하기 위해 워터마킹 모델을 훈련하기 위한 코드와 공개된 오디오가 워터마킹된 인식을 공개할 수 있음을 시사한다. 그러나 디텍터의 무게는 비밀로 유지되어야 합니다.\n' +
      '\n' +
      '우리는 워터마크 제거 공격에 초점을 맞추고, 상대방의 지식에 따라 세 가지 유형의 공격을 고려한다:\n' +
      '\n' +
      '* _White-box_: 적수는 검출기(_e.g._ 누출로 인해)에 대한 액세스 권한을 가지며, 그것에 대한 기울기 기반 적대적 공격을 수행한다. 최적화 목표는 디텍터의 출력을 최소화하는 것입니다.\n' +
      '* _Semi black-box_: 적대자는 어떤 가중치에도 액세스할 수 없지만, 동일한 데이터 세트에서 동일한 아키텍처를 갖는 생성기/검출기 쌍을 재훈련할 수 있다. 이전과 동일한 그래디언트 기반 공격을 수행하지만 새 디텍터를 원래 디텍터의 프록시로 사용합니다.\n' +
      '* _Black-box_: 적대자는 사용되는 워터마킹 알고리즘에 대한 어떠한 지식도 갖지 않지만, 워터마킹된 샘플들을 생성하는 API, 및 임의의 공개 데이터세트로부터의 네거티브 스피치 샘플들에 대한 액세스를 갖는다. 그들은 먼저 샘플을 수집하고 워터마킹된 것과 워터마킹되지 않은 것을 구별하기 위해 분류기를 훈련시킨다. 그들은 이 분류기가 진짜 검출기인 것처럼 공격한다.\n' +
      '\n' +
      '모든 시나리오에서 우리는 5초 동안 1k 샘플을 워터마킹한 다음 공격한다. 그래디언트 기반 공격은 100단계의 아담으로 오디오에 추가된 적대적 노이즈를 최적화합니다. 최적화 과정에서 잡음의 규준을 절충 공격 강도와 오디오 품질로 제어한다. 블랙박스 공격에 대한 분류기를 훈련할 때, 8초의 80k/80k 워터마크/정품 샘플을 사용하고, 검증 세트에서 분류기가 100% 검출 정확도를 갖도록 한다. 자세한 내용은 C.6 앱에서 확인할 수 있습니다.\n' +
      '\n' +
      '그림 7은 가우시안 노이즈를 기준으로 하여 서로 다른 강도에서 다양한 공격을 대비한다. 화이트박스 공격은 높은 오디오 품질(PESQ\\(>4\\))을 유지하면서 탐지 오류를 약 80%까지 증가시키는 가장 효과적인 공격이다. 다른 공격들은 덜 효과적이어서, 검출 오류를 증가시키는데 상당한 오디오 품질 저하가 요구되지만, 여전히 랜덤 잡음 추가에 비해 더 효과적이다. 요약하면, 워터마킹 알고리즘에 대해 많이 공개될수록 취약하다. 디텍터가 기밀로 유지되는 한 이러한 공격의 효과는 제한적이다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '본 논문에서는 AI 생성 음성의 탐지, 지역화, 귀속을 위한 사전적 방법인 오디오실(AudioSeal)을 소개한다. 오디오실(AudioSeal)은 오디오 워터마킹의 설계를 데이터 은닉보다는 국부적 검출에 특유하도록 개조한다. 오디오 샘플 레벨에서 워터마크를 생성하고 추출할 수 있는 생성기/검출기 아키텍처에 기초한다. 이것은 전통적으로 오디오 워터마크를 인코딩하고 디코딩하는 데 사용되는 느린 무차별 힘 알고리즘에 대한 의존성을 제거한다. 네트워크는 새로운 라우드니스 손실, 차별화 가능한 증강 및 마스킹된 샘플 레벨 검출 손실을 통해 공동으로 훈련된다. 결과적으로, AudioSeal은 다양한 오디오 편집 기술에 대한 최첨단 견고성, 로컬리제이션에서의 매우 높은 정밀도, 그리고 동기화에 의존하는 방법들보다 훨씬 더 빠른 실행 시간을 달성한다. 잠재적인 적대적 공격에 대한 경험적 분석을 통해, 우리는 워터마킹이 여전히 효과적인 완화가 되기 위해서는 탐지기의 가중치를 비공개로 유지해야 하며 그렇지 않으면 적대적 공격은 쉽게 위조될 수 있다는 결론을 내린다. 오디오실(AudioSeal)의 핵심 장점은 실제 적용 가능성이다. 음성 합성 API에서 워터마킹을 위한 바로 배포 가능한 솔루션으로 자리 잡고 있다. 이것은 소셜 미디어에서 대규모 콘텐츠 출처와 사건을 탐지하고 제거하는 데 중추적이며, 미국 유권자의 딥페이크 사례(Murphy et al., 2024)와 같은 사례가 확산되기 훨씬 전에 신속한 조치를 가능하게 한다.\n' +
      '\n' +
      '도 6: **평균 런타임**(\\(\\downarrow\\)이 더 좋다. AudioSeal은 워터마크 생성에 대해 1배, 동일한 오디오 입력에 대해 워터마크 검출에 대해 2배 더 빠르다. 전체 비교는 부록 D를 참조하십시오.\n' +
      '\n' +
      '도 7: **워터마크 제거 공격.** PESQ는 공격된 오디오와 정품 오디오 사이에서 측정된다(PESQ\\(<4\\)는 오디오 품질을 강하게 저하시킨다). 공격자가 워터마킹 알고리즘에 대해 더 많은 지식을 가질수록 공격은 더 잘 된다.\n' +
      '\n' +
      '윤리적 진술.이 연구는 AI 생성 콘텐츠의 투명성과 추적성을 향상시키는 것을 목표로 하지만, 일반적으로 워터마킹은 반체제 인사들에 대한 정부의 감시나 내부 고발자들의 기업 식별과 같은 잠재적인 오용들을 가질 수 있다. 또한, 워터마킹 기술은 사용자 생성 콘텐츠에 대한 저작권을 강요하기 위해 오용될 수 있으며, AI 생성 오디오를 탐지하는 능력은 디지털 통신 진정성에 대한 회의성을 증가시켜 잠재적으로 디지털 미디어 및 AI에 대한 신뢰를 손상시킬 수 있다. 그러나 이러한 위험에도 불구하고 AI 생성 콘텐츠의 탐지 가능성을 보장하는 것은 기술의 사용을 통제하기 위한 강력한 보안 조치 및 법적 프레임워크를 옹호하는 것과 함께 중요하다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Chinese ai governance rules (2023) Chinese ai governance rules, 2023. URL[http://www.cac.gov.cn/2023-07/13/c_1690898327029107.htm](http://www.cac.gov.cn/2023-07/13/c_1690898327029107.htm). 2023년 8월 29일에 접속했어요\n' +
      '* European ai act(2023) European ai act, 2023. URL[https://artificialintelligenceact.eu/](https://artificialintelligenceact.eu/) 2023년 8월 29일에 접속했어요\n' +
      '* Aaronson and Kirchner (2023) Aaronson, S. 그리고 Kirchner, H. Watermarking gpt output, 2023. URL[https://www.scottaaronson.com/talks/watermark.ppt](https://www.scottaaronson.com/talks/watermark.ppt).\n' +
      '* AlBadawy et al. (2018) AlBadawy, E. A., Lyu, S., and Farid, H. Detecting ai-synthesized speech using bispectral analysis. _CVPR workshops_, pp. 104-109, 2019에서.\n' +
      '* Arik et al. (2018) Arik, S., Chen, J., Peng, K., Ping, W., and Zhou, Y. 몇 가지 샘플이 있는 음성 복제입니다 신경 정보 처리 시스템_, 31, 2018의 발전.\n' +
      '* Bai et al.(2022) Bai, H., Zheng, R., Chen, J., Ma, M., Li, X., and Huang, L. A\\({}^{3}\\)t: 음성 합성 및 편집을 위한 정렬 인식 음향 및 텍스트 사전 훈련. Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pp. 1399-1411. PMLR, 2022. URL[https://proceedings.mlr.press/v162/bai2zd.html](https://proceedings.mlr.press/v162/bai2zd.html).\n' +
      '* Barrington et al. (2023) Barrington, S., Barua, R., Koorma, G., and Farid, H. Single and multi-speaker 복제 음성 검출: From perceptual to learned features. _ arXiv preprint arXiv:2307.07683_, 2023.\n' +
      '* Borrelli et al. (2021) Borrelli, C., Bestagini, P., Antonacci, F., Sarti, A., and Tubaro, S. 단기 및 장기 예측 추적을 통한 합성 음성 검출. _ EURASIP Journal on Information Security_, 2021(1):1-14, 2021.\n' +
      '* Borsoz et al. (2022) Borsoz, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Roblek, D., Teboul, O., Grangier, D., Tagliasacchi, M., and Zeghidour, N. Audiolm: 오디오 생성에 대한 언어 모델링 접근법. _ IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 31:2523-2533, 2022.\n' +
      '* Borsoz et al. (2023) Borsoz, Z., Sharifi, M., Vincent, D., Kharitonov, E., Zeghidour, N., and Tagliasacchi, M. 사운드스톰: 효율적인 병렬 오디오 생성. _ arXiv preprint arXiv:2305.09636_, 2023.\n' +
      '* Casanova et al. (2022) Casanova, E., Weber, J., Shulby, C. D., Junior, A. C., Golge, E., and Ponti, M. A. Yourttts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone. In _International Conference on Machine Learning_, pp. 2709-2720. PMLR, 2022.\n' +
      '* Chen et al. (2023) Chen, G., Wu, Y., Liu, S., Liu, T., Du, X., and Wei, F. Wavmark: Watermarking for audio generation. _ arXiv preprint arXiv:2308.12770_, 2023.\n' +
      '* Copet et al. (2023) Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y., and Defossez, A. Simple and controllable music generation. _ arXiv preprint arXiv:2306.05284_, 2023.\n' +
      '* Defossez et al. (2020) Defossez, A., Synnaeve, G., and Adi, Y. 파형 도메인에서의 실시간 음성 향상, 2020.\n' +
      '* Defossez et al. (2022) Defossez, A., Copet, J., Synnaeve, G., and Adi, Y. 고충실도 신경 오디오 압축 ARXiv 프리프린트 arXiv:2210.13438_, 2022.\n' +
      '* Fernandez et al. (2023a) Fernandez, P., Chaffin, A., Tit, K., Chappelier, V., and Furon, T. 대형 언어 모델의 워터마크를 통합하기 위한 세 개의 벽돌 2023 IEEE International Workshop on Information Forensics and Security (WIFS)_, 2023a.\n' +
      '* Fernandez et al. (2023b) Fernandez, P., Couairon, G., Jegou, H., Douze, M., and Furon, T. 안정 시그니처: 잠재 확산 모델에서 워터마크를 푸는 것 ICCV_, 2023b.\n' +
      '* Furon(2007) Furon, T. 제로 비트 워터마킹을 위한 구성적이고 통일적인 프레임워크. _ IEEE Transactions on Information Forensics and Security_, 2(2):149-163, 2007.\n' +
      '* Gritsenko et al. (2020) Gritsenko, A., Salimans, T., van den Berg, R., Snoek, J., and Kalchbrenner, N. 병렬 음성 합성을 위한 스펙트럼 에너지 거리 _ 신경 정보 처리 시스템_, 33:13062-13072, 2020에서의 발전.\n' +
      '* Hines et al. (2012) Hines, A., Skoglund, J., Kokaram, A., and Harte, N. Visqol: 가상 음성 품질 객관적 청취자. _IWAENC 2012; International workshop on acoustic signal enhancement_, pp. 1-4. VDE, 2012.\n' +
      '\n' +
      '* Hsu et al. (2023) Hsu, W. -N., Akinyemi, A., Rakotoarison, A., Tjandra, A., Vyas, A., Guo, B., Akula, B., Shi, B., Ellis, B., Cruz, I., Wang, J., Zhang, J., Williamson, M., Le, M., Moritz, R., Adkins, R., Ngan, W., Zhang, X., Yungster, Y., and Wu, Y. - C. Audiobox: 자연어 프롬프트가 있는 통합 오디오 생성. _ arXiv 프리프린트 arXiv:..._ 2023년\n' +
      '* Janicki(2015) Janicki, A. Spoofing countermeasure based on analysis of linear prediction error. 2015년 국제 음성 통신 협회의 제16차 연례 회의에서.\n' +
      '* Juvela & Wang(2023) Juvela, L. 그리고 Wang, X. 적대적 음성 합성을 위한 협력 워터마킹 arXiv preprint arXiv:2309.15224_, 2023.\n' +
      '* Kalantari et al. (2009) Kalantari, N. K., Akhaee, M. A., Ahadi, S. M., and Amindavar, H. Robust multiplicative patchwork method for audio watermarking. _ IEEE Trans. Speech Audio Process.__ , 17(6):1133-1141, 2009. doi: 10.1109/TASL.2009.2019259. URL[https://doi.org/10.1109/TASL.2009.2019259](https://doi.org/10.1109/TASL.2009.2019259).\n' +
      '* Kharitonov et al. (2023) Kharitonov, E., Vincent, D., Boros, Z., Marinier, R., Girgin, S., Pietquin, O., Sharifi, M., Tagliasacchi, M., and Zeghidour, N. 말하기, 읽기 및 프롬프트: 최소한의 감독으로 고충실도 텍스트 투 스피치 _ ArXiv_, abs/2302.03540, 2023.\n' +
      '*Kim et al. (2023) Kim, C., Min, K., Patel, M., Cheng, S., and Yang, Y. Wouaf: 텍스트-이미지 확산 모델에서 사용자 속성 및 핑거프린팅을 위한 가중치 변조. _ arXiv preprint arXiv:2306.04744_, 2023.\n' +
      '* Kim et al. (2019) Kim, C. D., Kim, B., Lee, H., and Kim, G. Audiocaps: Generating caption for audioios in the wild. 2019년 _NAACL-HLT_에서.\n' +
      '* Kim et al. (2021) Kim, J., Kong, J., and Son, J. Conditional Variational Autoencoder with adversarial learning for end-to-end text-to-speech. In _International Conference on Machine Learning_, pp. 5530-5540. PMLR, 2021.\n' +
      '* Kirchenbauer et al. (2023) Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., and Goldstein, T. 대형 언어 모델을 위한 워터마크. _ arXiv preprint arXiv:2301.10226_, 2023.\n' +
      '* Kirovski & Attias (2003) Kirovski, D. and Attias, H. Audio watermark robustness to desynchronization via beat detection. In Petitcolas, F. A. P. (ed.), _Information Hiding_, pp. 160-176, Berlin, Heidelberg, 2003. Springer Berlin Heidelberg. ISBN 978-3-540-36415-3\n' +
      '* Kirovski & Malvar (2003) Kirovski, D. and Malvar, H. S. Spread-spectrum watermarking of audio signals. _ IEEE Trans. Signal Process.__ , 51(4):1020-1033, 2003. doi:10.1109/TSP.2003.809384. URL[https://doi.org/10.1109/TSP.2003.809384](https://doi.org/10.1109/TSP.2003.809384).\n' +
      '* Kong et al. (2020) Kong, J., Kim, J., and Bae, J. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 17022-17033. Curran Associates, Inc., 2020.\n' +
      '* Kreuk et al. (2023) Kreuk, F., Synnaeve, G., Polyak, A., Singer, U., Defossez, A., Copet, J., Parikh, D., Taigman, Y., and Adi, Y. Audiogen: Textually guided audio generation. _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Kumar et al. (2019) Kumar, K., Kumar, R., de Boissiere, T., Gestin, L., Teoh, W. Z., Sotelo, J. M. R., de Brebisson, A., Bengio, Y., and Courville, A. C. Melgan: Generative adversarial networks for conditional waveform synthesis. _Neural Information Processing Systems_, 2019.\n' +
      '* Kumar et al. (2023) Kumar, R., Seetharaman, P., Luebs, A., Kumar, I., and Kumar, K. 향상된 rvqgan을 갖는 고 충실도 오디오 압축. _ ArXiv_, abs/2306.06546, 2023.\n' +
      '* Le et al. (2023) Le, M., Vyas, A., Shi, B., Karrer, B., Sari, L., Moritz, R., Williamson, M., Manohar, V., Adi, Y., Mahadeokar, J., et al. Voicebox: Text-guided multilingual universal speech generation at scale. _ arXiv preprint arXiv:2306.15687_, 2023.\n' +
      '* Lie & Chang (2006) Lie, W. 장룡 저주파 진폭 변형 기반의 강인하고 고품질의 시간 영역 오디오 워터마킹 IEEE Trans. Multim._ , 8(1):46-59, 2006. doi: 10.1109/TMM.2005.861292. URL[https://doi.org/10.1109/TMM.2005.861292](https://doi.org/10.1109/TMM.2005.861292).\n' +
      '* Liu et al. (2023) Liu, C., Zhang, J., Fang, H., Ma, Z., Zhang, W., and Yu, N. Dear: Deep-learning 기반 오디오 재기록 탄성 워터마킹. Williams, B., Chen, Y., and Neville, J. (eds.), _Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Th13enth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023_, pp. 13201-13209. AAAI Press, 2023. doi: 10.1609/aaai.v37i11.26550.\n' +
      '* Liu et al. (2019) Liu, Z., Huang, Y., and Huang, J. Patchwork-based audio watermarking robust against de-synchronization and recapturing attacks _ IEEE Trans. Inf. 과학수사대 Secur__ , 14(5):1171-1180, 2019. doi: 10.1109/TIFS.2018.2871748. URL[https://doi.org/10.1109/TIFS.2018.2871748](https://doi.org/10.1109/TIFS.2018.2871748).\n' +
      '* Luo & Mesgarani (2019) Luo, Y. 및 메스가라니, N. Conv-tasnet: Surpasspass ideal time-frequency magnitude masking for speech separation. _ IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 27(8):1256-1266, 2019. doi: 10.1109/TASLP.2019.2915167.\n' +
      '* Muller et al. (2022) Muller, N. M., Czempin, P., Dieckmann, F., Froghyar, A., and Bottinger, K. 오디오 딥페이크 탐지가 일반화되는가? _ arXiv preprint arXiv:2203.16263_, 2022.\n' +
      '* Murphy et al. (2024) Murphy, M., Metz, R., Bergen, M., and Bloomberg. Biden audio deepfake spurs a start 11labs --valued in 11억 달러 -- to ban account: \'We\'ll see the much more of this\'. _ Fortune_, January 2024. URL[https://fortune.com/2024/01/27/ai-fortune.com/elevenlabs-anount-for-biden-audio-deepfake/](https://fortune.com/2024/01/27/ai-firm-elevenlabs-anount-for-biden-audio-deepfake/]\n' +
      '* Natgunanathan et al. (2012) Natgunanathan, I., Xiang, Y., Rong, Y., Zhou, W., and Guo, S. 디지털 오디오 워터마킹을 위한 강인한 패치워크 기반 임베딩 및 디코딩 기법. _ IEEE Trans. Speech Audio Process.__ , 20(8):2232-2239, 2012. doi: 10.1109/TASL.2012.2199111. URL[https://doi.org/10.1109/TASL.2012.2199111](https://doi.org/10.1109/TASL.2012.2199111).\n' +
      '* Nguyen et al. (2023) Nguyen, T. A., Hsu, W. N., d\'Avirro, A., Shi, B., Gat, I., Fazel-Zarani, M., Remez, T., Copet, J., Synnaeve, G., Hassid, M., et al. Expressso: A benchmark and analysis of discrete expressive speech resynthesis. _ arXiv preprint arXiv:2308.05725_, 2023.\n' +
      '* Pavlovic et al. (2022) Pavlovic, K., Kovacevic, S., Djurovic, I., and Wojciechowski, A. Robust speech watermarking by jointly trained embeddedder and detector using a dnn. _ Digital Signal Processing_, 122:103381, 2022.\n' +
      '*Qu et al.(2023) Qu, X., Yin, X., Wei, P., Lu, L., and Ma, Z. Audioqr: qr 코드에 대한 심층 신경 오디오 워터마크. _ IJCAI_, 2023.\n' +
      '* Ren et al. (2023) Ren, Y., Zhu, H., Zhai, L., Sun, Z., Shen, R., and Wang, L. 실제로 누가 말하고 있나요? 강력하고 다재다능한 음성 변환을 위한 화자 추적 가능성 _ arXiv preprint arXiv:2305.05152_, 2023.\n' +
      '* Rix et al. (2001) Rix, A. W., Beerends, J. G., Hollier, M. P., and Hekstra, A. P. Perceptual Evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs. _2001 IEEE 국제학술대회에서는 음향, 음성 및 신호처리에 관한 내용을 다루고 있다. 회보(Cat. No. 01CH37221) _, volume 2, pp. 749-752. IEEE, 2001.\n' +
      '* Sahidullah et al. (2015) Sahidullah, M., Kinnunen, T., and Hanilci, C. The comparison of features for synthetic speech detection. _ ISCA (the International Speech Communication Association)_, 2015.\n' +
      '* Schnupp et al. (2011) Schnupp, J., Nelken, I., and King, A. _Auditory neuroscience: Making sense of sound_. 2011년 MIT 기자\n' +
      '* Seamless Communication et al. (2023) Seamless Communication, Barrault, L., Chung, Y. - A., Meglioli, M. C., Dale, D., Dong, N., Dupenthaler, M., Duquenne, P.-A., Ellis, B., Elsahar, H., Haaheim, J., Hoffman, J., Hwang, M. J., Inaguma, H., Klaiber, C., Kulikov, I., Li, P., Mavlyutov, R., Rakotoarison, A., Ramakrishnan, A., Tran, T., Wenzek, G., Yang, Y., Ye, E., Evtimov, I., Fernandez, P., Gao, C., Hansanti, P., Kalbassi, A., Kozhevnikov, A., Mejia, G., Roman, R. S., Wood, C., Yu, B., Andrews, P., Balioglu, C., C., P.-J., Costa-jussa, M. 심리스: 다국어 표현 및 스트리밍 음성 번역. 2023년\n' +
      '* Series(2014) Series, B. Method for subjective assessment of intermediate quality level of audio systems. _ International Telecommunication Union Radiocommunication Assembly_, 2014.\n' +
      '* Shen et al. (2023) Shen, K., Ju, Z., Tan, X., Liu, Y., Leng, Y., He, L., Qin, T., Zhao, S., and Bian, J. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers. _ CoRR_, abs/2304.09116, 2023. doi: 10.48550/ARXIV.2304.09116. URL[https://doi.org/10.48550/arXiv.2304.09116](https://doi.org/10.48550/arXiv.2304.09116)\n' +
      '* Su et al. (2018) Su, Z., Zhang, G., Yue, F., Chang, L., Jiang, J., and Yao, X. 강인한 오디오 워터마킹의 스케일링 파라미터를 최적화하기 위한 Snr-constrained heuristics. _ IEEE Trans. Multim._ , 20(10):2631-2644, 2018. doi: 10.1109/TMM.2018.2812599. URL[https://doi.org/10.1109/TMM.2018.2812599](https://doi.org/10.1109/TMM.2018.2812599).\n' +
      '* Taal et al. (2010) Taal, C. H., Hendriks, R. C., Heusdens, R., and Jensen, J. 시간-주파수 가중 잡음 음성에 대한 단시간 객관적 명료도 측정치. In _2010 IEEE international conference on Acoustics, speech and signal processing_, pp. 4214-4217. IEEE, 2010.\n' +
      '* Tai & Mansour (2019) Tai, Y. -Y. 및 Mansour, M. F. Audio watermarking with modulated self-correlation. In _ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pp. 2452-2456. IEEE, 2019.\n' +
      '* telecommunication Union(2011) telecommunication Union, I. Algorithm to measure audio program loudness and true-peak audio level. _ 시리즈, BS_, 2011.\n' +
      '* 백악관(2023) 백악관. 안전, 보안 및 신뢰할 수 있는 ai. [https://www.whitehouse.gov/wp-content/uploads/2023/07/Ensuring] (https://www.whitehouse.gov/wp-content/uploads/2023/07/Ensuring)Safe-Secure-and-Trustworthy-AI.pdf, July 2023. Accessed:[july 2023].\n' +
      '* van den Oord et al. (2016) van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. Wavenet: 원시 오디오에 대한 생성 모델입니다. 2016년 _Arxiv_에서\n' +
      '* Wang et al. (2021) Wang, C., Riviere, M., Lee, A., Wu, A., Talnikar, C., Haziza, D., Williamson, M., Pino, J. M., and Dupoux, E. Voxpopuli: representation learning, semi-supervised learning and interpretation. Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021_, pp. 993-1003. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.ACL-LONG.80. URL[https://doi.org/10.18653/v1/2021.acl-long.80](https://doi.org/10.18653/v1/2021.acl-long.80](https://doi.org/10.18653/v1/2021.acl-long.80).\n' +
      '* Wang et al. (2023) Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., et al. 신경 코덱 언어 모델은 음성 합성기에 대한 제로 샷 텍스트이다. _ arXiv preprint arXiv:2301.02111_, 2023.\n' +
      '* Wen et al. (2023) Wen, Y., Kirchenbauer, J., Geiping, J., and Goldstein, T. 트리-링 워터마크: 비가시적이고 강건한 확산 이미지에 대한 핑거프린트. _ arXiv preprint arXiv:2305.20030_, 2023.\n' +
      '* Wu et al. (2023) Wu, S., Liu, J., Huang, Y., Guan, H., and Zhang, S. 적대적 오디오 워터마킹: 깊은 특징에 워터마크를 삽입한다. In _2023 IEEE International Conference on Multimedia and Expo (ICME)_, pp. 61-66. IEEE, 2023.\n' +
      '* Xiang et al. (2014) Xiang, Y., Natgunanathan, I., Guo, S., Zhou, W., and Nahavandi, S. 비동기화 공격에 강인한 패치워크 기반의 오디오 워터마킹 방법. _ IEEE ACM Trans. Audio Speech Lang. 처리._ , 22(9):1413-1423, 2014. doi: 10.1109/TASLP.2014.2328175. URL[https://doi.org/10.1109/TASLP.2014.2328175](https://doi.org/10.1109/TASLP.2014.2328175).\n' +
      '* Xiang et al. (2018) Xiang, Y., Natgunanathan, I., Peng, D., Hua, G., and Liu, B. Spread spectrum audio watermarking using multiple orthogonal PN sequences and variable embedding strength and polarities. _ IEEE ACM Trans. Audio Speech Lang. 처리._ , 26(3):529-539, 2018. doi: 10.1109/TASLP.2017.2782487. URL[https://doi.org/10.1109/TASLP.2017.2782487](https://doi.org/10.1109/TASLP.2017.2782487).\n' +
      '* Yang et al. (2021) Yang, Y. -Y., Hira, M., Ni, Z., Chourdia, A., Astafurov, A., Chen, C., Yeh, C.-F., Puhrsch, C., Pollack, D., Genzel, D., Greenberg, D., Yang, E. Z., Lian, J., Mahadeokar, J., Hwang, J., Chen, J., Goldsborough, P., Roy, P., Narethiran, S., Watanabe, S., Chintala, S., Quenneville-Belair, V., and Shi, Y. 토르차우디오: 오디오 및 음성 처리를 위한 빌딩 블록들 _ arXiv preprint arXiv:2110.15018_, 2021.\n' +
      '* Yin et al. (2019) Yin, P., Lyu, J., Zhang, S., Osher, S., Qi, Y., and Xin, J. understanding straight-through estimator in training activation quantized neural net. _ ArXiv preprint arXiv:1903.05662_, 2019.\n' +
      '* Yu et al. (2021a) Yu, N., Skripniuk, V., Abdelnabi, S., and Fritz, M. 생성 모델을 위한 인공 핑거프린팅: 훈련 데이터에서 딥페이크 속성 제거 In _Proceedings of the IEEE/CVF International conference on computer vision_, pp. 14448-14457, 2021a.\n' +
      '* Yu et al. (2021b) Yu, N., Skripniuk, V., Chen, D., Davis, L. S., and Fritz, M. 확장 가능한 핑거프린팅을 사용하여 생성 모델의 책임 있는 공개. _International Conference on Learning Representations_, 2021b.\n' +
      '* Zeghidour et al. (2022) Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M. 사운드스트림: End-to-End 신경 오디오 코덱. _ IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 30:495-507, 2022. doi: 10.1109/TASLP.2021.3129994.\n' +
      '* Zhang et al. (2017) Zhang, C., Yu, C., and Hansen, J. H. investigation of deep-learning frameworks for speaker verification antispoofing. _ IEEE Journal of Selected Topics in Signal Processing_, 11(4):684-694, 2017.\n' +
      '\n' +
      '확장된 관련 작업\n' +
      '\n' +
      '제로샷 TTS와 보컬 스타일 보존.소량의 데이터만을 이용하여 보컬 스타일을 모방하거나 보존하는 모델이 등장하였다. 한 가지 주요 예는 제로 샷 텍스트 투 스피치(TTS) 모델이다. 이 모델들은 특별히 훈련되지 않은 발성 스타일로 연설을 만듭니다. 예를 들어, VALL-E(Wang et al., 2023), YourTTS(Casanova et al., 2022), Natural-Speech2(Shen et al., 2023)와 같은 모델은 3초 기록만으로 고품질의 개인화된 음성을 합성한다. 또한, 음성박스(Le et al., 2023), A\\({}^{3}\\)T(Bai et al., 2022) 및 Audiobox(Hsu et al., 2023)와 같은 제로샷 TTS 모델들은 비자동 회귀 추론과 함께 텍스트 유도 음성 주입과 같은 태스크들을 수행하며, 여기서 목표는 주변 오디오 및 텍스트 전사물을 주어진 마스킹된 음성을 생성하는 것이다. 그것은 그들을 언어 조작을 위한 강력한 도구로 만든다. 음성 기계 번역의 맥락에서, SeamlessExpressive(Seamless Communication et al., 2023)는 음성을 번역할 뿐만 아니라, 화자의 고유한 발성 스타일 및 감정 변덕을 유지하여, 그러한 시스템의 능력을 넓히는 모델이다.\n' +
      '\n' +
      '오디오 생성 및 압축.초기 모델들은 웨이브넷(van den Oord et al., 2016)과 같이 자기회귀적이며, 확장된 컨볼루션 및 파형 재구성을 목표로 한다. 후속 접근법들은 스케일-불변 신호 대 잡음비(SINR)(Luo and Mesgarani, 2019) 또는 Mel 스펙트로그램 거리(Defossez et al., 2020)와 같은 상이한 오디오 손실들을 탐색한다. 이러한 목적들 중 어느 것도 오디오 품질에 이상적인 것으로 간주되지 않으며, HiFi-GAN(Kong et al., 2020) 또는 MelGAN(Kumar et al., 2019)에서 적대적 모델들의 채택으로 이어진다. 우리의 훈련 목표 및 아키텍처는 고품질 파형 생성에 초점을 맞추고 그들의 훈련 프로세스에서 이러한 다양한 목표의 조합을 통합하는 보다 최근의 신경 오디오 압축 모델(Defossez et al., 2022; Kumar et al., 2023; Zeghidour et al., 2022)에 의해 영감을 받는다.\n' +
      '\n' +
      '워터마크를 정확하게 추출하기 위해서는 인코더와 디코더 사이의 동기화가 필수적이다. 그러나 이는 시간 및 피치 스케일링과 같은 비동기화 공격에 의해 중단될 수 있다. 이 문제를 해결하기 위해 다양한 기술이 개발되었다. 한 가지 접근법은 시간 및 주파수 도메인 둘 다를 따라 워터마크 신호를 반복하는 블록 반복이다(Kirovski and Malvar, 2003; Kirovski and Attias, 2003). 다른 방법은 워터마크된 신호에 동기화 비트를 이식하는 것을 포함한다(Xiang et al., 2014). 디코딩 동안, 이러한 동기화 비트는 동기화를 개선하고 동기화 해제 공격의 영향을 완화시키는 역할을 한다. 워터마크 검출을 위한 동기화 비트들의 검출은 일반적으로 브루트 힘 알고리즘들을 사용한 철저한 검색을 수반하며, 이는 디코딩 시간을 상당히 늦춘다.\n' +
      '\n' +
      '## 부록 B 허위양성률 - 이론과 실천\n' +
      '\n' +
      '이론적 FPR. multi-bit 워터마킹을 수행할 때, 이전 작업들(Yu et al., 2021; Kim et al., 2023; Fernandez et al., 2023; Chen et al., 2023)은 보통 내용 \\(x\\)에서 메시지 \\(m^{\\prime}\\)을 추출하고 이를 음성 샘플에 내장된 원래의 이진 서명 \\(m\\in\\{0,1\\}^{k}\\)과 비교한다. 검출 테스트는 매칭 비트 수\\(M(m,m^{\\prime})\\):\n' +
      '\n' +
      '\\[\\text{if}\\;M\\left(m,m^{\\prime}\\right)\\geq\\tau\\;\\;\\text{where}\\;\\;\\tau\\in\\{0,\\dots,k\\},\\tag{4}\\]\n' +
      '\n' +
      '그러면 오디오가 플래그됩니다. 이는 허위 양성률에 대한 이론적 보장을 제공한다.\n' +
      '\n' +
      '형식적으로, 통계적 가설은 \\(H_{1}\\): "오디오 신호 \\(x\\)는 워터마킹됨", 귀무가설 \\(H_{0}\\): "오디오 신호 \\(x\\)는 정품임"이다. (H_{0}\\)하에서 (m_{1}^{\\prime},\\dots,m_{k}^{\\prime}\\)의 비트가 독립적이고 동일하게 분포하면 (i.i.d.) Bernoulli 확률변수는 매개변수 \\(0.5\\), 그리고 \\(M(m,m^{\\prime})\\)은 매개변수 (\\(k\\), \\(0.5\\))를 갖는 이항분포를 따른다. False Positive Rate (FPR)은 주어진 임계값 \\(\\tau\\)을 초과하는 확률로 정의된다. 정규화된 불완전 베타 함수 \\(I_{x}(a;b)\\)(이항 분포의 CDF에 연결됨)을 사용하여 닫힌 형태의 식을 제공할 수 있다:\n' +
      '\n' +
      '\\[\\text{FPR}(\\tau)=\\mathbb{P}\\left(M>\\tau|H_{0}\\right)=I_{1/2}(\\tau+1,k-\\tau). \\tag{5}\\]\n' +
      '\n' +
      '경험적 연구.우리는 검증 데이터셋에 대한 WavMark 기반 탐지의 FPR을 실증적으로 연구한다. 원본 논문과 동일한 파라미터를 사용하며, 1s 음성 샘플로부터 _i.e._\\(k=32\\)-비트를 추출한다. 먼저 10k 정품 샘플에서 소프트 비트(임계화 전에)를 추출하고 그림 1의 점수 히스토그램을 표시한다. 8(좌측). 우리는 평균 \\(0.5\\)의 가우시안 분포를 관찰해야 하며, 경험적으로 점수는 \\(0.38\\)을 중심으로 한다. 이것은 정품 샘플에 대한 비트 0에 심하게 편향된 결정을 만든다. 따라서 이것이 실제 PR을 크게 과소평가할 것이기 때문에 이론적으로 FPR을 설정하는 것은 불가능하다.\n' +
      '\n' +
      '그림 8: (왼쪽) 10k 정품 샘플에 대한 WavMark의 추출기에 의해 출력된 점수의 히스토그램. (오른쪽) 선택된 숨겨진 메시지가 모두 0일 때 경험적 및 이론적 FPR.\n' +
      '\n' +
      '예를 들어, 그림 8(오른쪽)은 선택된 숨겨진 메시지가 가득 차 있을 때 \\(\\tau\\)의 다른 값에 대한 이론적 및 경험적 FPR을 보여준다. 다르게 말하면, 숨겨진 비트가 탐지율에 대한 이론적 보장을 허용한다는 주장은 실제로 유효하지 않다.\n' +
      '\n' +
      '## 부록 C 실험 상세\n' +
      '\n' +
      '### Loudness\n' +
      '\n' +
      '우리의 라우드니스 함수는 토치우디오(Yang et al., 2021) 라이브러리에서의 구현의 단순화에 기초한다. 다단계 프로세스를 통해 계산됩니다. 처음에, 오디오 신호는 인간의 귀의 반응을 모방하기 위해 특정 주파수를 강조하는 필터링 프로세스인 K-가중치를 거친다. 이는 고역 필터와 고역 필터를 적용하여 달성된다. 이에 이어서, 오디오 신호의 에너지가 신호의 각 블록에 대해 계산된다. 이것은 신호를 제곱하고 각 블록에 대해 평균함으로써 수행된다. 그 후, 에너지는 오디오 신호 내의 채널들의 수에 따라 가중되며, 인지된 라우드니스에 대한 그들의 다양한 기여들을 설명하기 위해 상이한 채널들에 상이한 가중치들이 적용된다. 마지막으로, 라우드니스는 가중된 에너지의 합에 로그를 취하여 일정한 오프셋을 더함으로써 계산된다.\n' +
      '\n' +
      '### Robustness Augmentations\n' +
      '\n' +
      '다음은 열차 시간(T) 및 평가 시간(E)에서 사용되는 오디오 편집 증강의 세부사항이다:\n' +
      '\n' +
      '* **Bandpass Filter:** 하이패스 필터링과 로우패스 필터링을 결합하여 특정 주파수 대역이 통과할 수 있도록 한다. (T) 300Hz와 8000Hz 사이에 고정; (E) 500Hz와 5000Hz 사이에 고정.\n' +
      '**하이패스 필터:** 입력 오디오에 하이패스 필터를 사용하여 특정 임계값 이하의 주파수를 차단한다. (T) 500Hz로 고정; (E) 1500Hz로 고정.\n' +
      '**저역 통과 필터:** 입력 오디오에 저역 통과 필터를 적용하여 차단 주파수 이상의 주파수를 차단합니다. (T) 5000Hz로 고정; (E) 500Hz로 고정.\n' +
      '**속도:** 오디오의 속도를 1에 가까운 팩터만큼 변경합니다. (T) 0.9에서 1.1 사이의 랜덤; (E) 1.25로 고정됩니다.\n' +
      '**리샘플:**중간 샘플 레이트로 업샘플링한 후, 형태를 변경하지 않고 오디오를 원래 레이트로 다시 다운샘플링한다. (T) 및 (E) 32kHz.\n' +
      '***부스트 오디오:**인수를 곱하여 오디오를 증폭합니다. (T) 팩터가 1.2로 고정되고 (E) 팩터가 10으로 고정됩니다.\n' +
      '**덕 오디오:** 오디오의 볼륨을 곱함수로 줄입니다. (T) 팩터가 0.8로 고정됨; (E) 팩터가 0.1로 고정됨.\n' +
      '* **에코:** 오디오에 에코 효과를 적용하여 원본의 지연 및 덜 큰 복사본을 추가합니다. (T) 0.1 내지 0.5 초 사이의 랜덤 지연, 0.1 내지 0.5 사이의 랜덤 볼륨; (E) 0.5 초의 고정 지연, 0.5의 고정 볼륨.\n' +
      '* **핑크 노이즈:** 배경 노이즈 효과를 위해 핑크 노이즈를 추가합니다. (T) 표준편차는 0.01로 고정; (E)는 0.1로 고정.\n' +
      '* ** 화이트 노이즈:** 파형에 가우시안 노이즈를 추가합니다. (T) 표준편차는 0.001로 고정; (E)는 0.05로 고정.\n' +
      '**Smooth:** 가변 윈도우 크기를 갖는 이동 평균 필터를 이용하여 오디오 신호를 스무딩한다. (T) 2와 10 사이의 랜덤 윈도우 크기; (E) 40으로 고정.\n' +
      '**AAC:** 오디오를 AAC 형식으로 인코딩합니다. (T) 128kbps의 비트레이트; (E) 64kbps의 비트레이트.\n' +
      '**MP3:** MP3 형식으로 오디오를 인코딩합니다. (T) 128kbps의 비트레이트; (E) 32kbps의 비트레이트.\n' +
      '***EnCodec:** 24kHz에서 샘플하고, \\(nq=16\\)(토큰의 16개 스트림)으로 EnCodec으로 오디오를 인코딩하고, 다시 16kHz로 예제한다.\n' +
      '\n' +
      '증강의 구현은 가능한 경우 줄리우스 파이썬 라이브러리로 수행된다.\n' +
      '\n' +
      '우리는 수정 강도에 대한 탐지 정확도를 여러 번 증가시키기 위해 그림 9에 표시한다. 대부분의 증강에서 볼 수 있듯이 우리의 방법은 모든 매개변수에 대해 Wavmark보다 우수하다. 우리는 훈련 범위(500Hz)보다 훨씬 높은 하이패스 필터의 경우 Wavmark가 훨씬 더 나은 탐지 정확도를 가지고 있다고 언급합니다.\n' +
      '\n' +
      '### 네트워크 아키텍처(도 4)\n' +
      '\n' +
      '워터마크 생성기는 인코더 및 디코더로 구성되며, 둘 다 인코더로부터의 엘리먼트들을 통합한다(Defossez et al., 2022). 인코더는 32개의 채널과 7의 커널 크기를 갖는 1D 컨벌루션을 적용하고, 이어서 4개의 컨벌루션 블록을 적용한다. 이 블록들은 각각 잔차 단위와 다운 샘플링 레이어를 포함하며, 스트라이드(S\\)와 커널 크기(K=2S\\)와의 컨벌루션을 사용한다. 잔차 단위는 다운 샘플링 동안 채널이 두 배인 스킵-연결을 갖는 2개의 커널-3 컨볼루션을 갖는다. 인코더는 2-레이어 LSTM 및 7 및 128 채널의 커널 크기를 갖는 최종 1D 컨볼루션으로 마무리한다. Strides \\(S\\) 값은 (2, 4, 5, 8)이고 잔차 단위의 비선형 활성화는 지수 선형 단위(ELU)이다. 디코더는 인코더를 미러링하지만 대신 역순으로 보폭과 함께 전치된 컨볼루션을 사용한다.\n' +
      '\n' +
      '검출기는 인코더, 전치된 컨볼루션 및 선형 층을 포함한다. 인코더는 제너레이터의 아키텍처를 공유하지만, 다른 가중치를 갖는다. 전치된 컨볼루션은 \\(h\\)개의 출력 채널들을 가지며, 활성화 맵을 원래의 오디오 해상도로 업샘플링한다(결과적으로 형상 \\((t,h)\\)). 선형 층은 \\(h\\) 차원을 2로 줄인 다음 샘플별 확률 점수를 제공하는 소프트맥스 함수가 뒤따른다.\n' +
      '\n' +
      '#MUSHRA 프로토콜 세부사항\n' +
      '\n' +
      '무쉬라 프로토콜은 참가자가 일부 샘플의 품질을 100개 중 평가해야 하는 크라우드 소싱 테스트로 구성된다. 근본적인 진실이 주어졌다. 우리는 각각 10초의 100개의 음성 샘플을 사용했다. 모든 샘플은 최소 20명의 참가자가 평가한다. 우리는 엔코덱을 사용하여 1.5kbps에서 매우 낮은 앵커 손실 압축을 연구에 포함했다. 최소 할당의 80%에 대해 낮은 앵커에 대해 최저 점수를 평가하지 않는 참가자는 연구에서 제거된다. 비교로서 Ground truth 시료는 \\(80.49\\)와 low anchor의 \\(53.21\\)을 얻었다.\n' +
      '\n' +
      '### Out of domain(ood) 평가\n' +
      '\n' +
      '이전에 설명한 대로 모델이 모든 유형의 음성 복제 방법에서 잘 작동하는지 확인하기 위해 여러 음성 복제 모델 및 기타 오디오 양식의 출력에 대해 테스트했다. 각 우로드 데이터 세트를 구성한 10k 샘플에서 사전 탐지 방법으로 잘못 분류된 샘플은 없다. 게다가 우리는 동일한 증강 세트를 수행하고 매우 유사한 수를 관찰했다. 표 5에 결과를 보여준다. AI 생성 음성을 훈련하지 않더라도 테스트 데이터에 비해 성능이 증가하는 것을 관찰한다.\n' +
      '\n' +
      '워터마크에 대한 공격\n' +
      '\n' +
      '검출기에 대한 적대적 공격은 샘플\\(x\\)과 검출기\\(D\\)이 주어졌을 때, 우리는 \\(D(x^{\\prime}\\sim x\\)가 1-D(x)\\이 되도록 \\(x^{\\prime}\\sim x\\을 찾고자 한다. 이를 위해 그라디언트 기반 공격을 사용한다. 랜덤 가우시안 잡음으로 왜곡 \\(\\delta_{adv}\\)을 초기화하는 것으로 시작한다. 알고리즘은 여러 단계 \\(n\\) 동안 반복적으로 왜곡을 업데이트한다. 각 단계마다, 왜곡은 \\(x=x+\\alpha.\\)를 통해 원래의 오디오에 추가된다. mathrm{tanh}(\\delta_{adv})\\), 모델을 통과하여 예측을 얻었다. 교차 엔트로피 손실은 라벨 0(제거용) 또는 1(단조용)로 계산되며, Adam 최적화기를 사용하여 왜곡을 업데이트하기 위해 검출기를 통해 역전파된다. 프로세스가 끝나면 적대적 오디오는 \\(x++\\alpha.\\ mathrm{tanh}(\\delta_{adv})\\. 공격에서는 스케일링 팩터 \\(\\alpha=10^{-3}\\), 다수의 단계 \\(n=100\\), 학습률 \\(10^{-1}\\)을 사용한다. 왜곡이 작게 유지되도록 하기 위해 \\(\\mathrm{tanh}\\) 함수를 사용하며, 상대 오디오의 SNR에 상한을 부여한다.\n' +
      '\n' +
      '악성 탐지기의 훈련.여기서, 우리는 두 유형의 많은 샘플에 액세스할 수 있는 경우 워터마크된 샘플과 비-워터마크된 샘플을 구별할 수 있는 분류기를 훈련시키는 데 관심이 있다. 분류기를 훈련시키기 위해, 제안된 방법을 사용하여 워터마킹된 Voicebox(Le et al., 2023)로부터 8초 스피치의 80k개 이상의 샘플로 만들어진 데이터세트와 유사한 양의 정품(비-워터마킹된) 스피치 샘플들을 사용한다. 분류기는 AudioSeal의 검출기와 동일한 아키텍처를 공유한다. 분류기는 64개의 1초 샘플들의 배치들로 200k 업데이트들에 대해 트레이닝된다. 샘플의 완벽한 분류를 달성합니다. 이것은 Voicebox의 발견과 일치한다(Le et al., 2023).\n' +
      '\n' +
      '그림 9: 증강 강도에 대한 증강 샘플에 대한 검출기의 정확도.\n' +
      '\n' +
      '## 부록 D 연산 효율\n' +
      '\n' +
      '우리는 그림 10에서 오디오 지속 시간에 따른 탐지 및 생성의 평균 런타임을 보여준다. 해당 번호는 표 6에 나와 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline Model & Watermarked & **Detection ms (speedup)** & **Generation ms (speedup)** \\\\ \\hline Wavmark & No & 1710.70 \\(\\pm\\) 1314.02 & – \\\\ AudioSeal (ours) & No & **3.25 \\(\\pm\\) 1.99** (**485\\(\\times\\)**) & – \\\\ \\hline Wavmark & Yes & 106.21 \\(\\pm\\) 66.95 & 104.58 \\(\\pm\\) 65.66 \\\\ AudioSeal (ours) & Yes & **3.30 \\(\\pm\\) 2.03** (**35\\(\\times\\)**) & **7.41 \\(\\pm\\) 4.52** (**14 \\(\\times\\)**) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 최신 Wavmark(Chen et al., 2023) 방법에 대해 제안된 AudioSeal 모델의 샘플당 평균 런타임(ms). 우리의 실험은 단일 Nvidia Quadro GP100 GPU를 사용하여 1초에서 10초 사이의 오디오 세그먼트 데이터 세트에 대해 수행되었다. 표에 표시된 결과는 워터마크의 존재 여부에 관계없이 워터마크 생성 및 탐지 모두에 대해 상당한 속도 향상을 보여준다. 특히 워터마크 검출을 위해 AudioSeal은 워터마크가 없는 동안 Wavmark보다 **485\\(\\times\\) 더 빠릅니다. 섹션 5.5에서 더 자세히 설명합니다.\n' +
      '\n' +
      '도 10: **Mean runtime**(\\(\\downarrow\\) is better) of AudioSeal versus WavMark. 오디오실(AudioSeal)은 워터마크 생성에 있어서 1배, 동일한 오디오 입력에 대해 2배 더 빠른 워터마크 검출을 위해 2배 더 빠른 크기로서, 실시간 오디오 워터마킹 효율의 상당한 향상을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c|c c} \\hline \\hline Aug & Seamless (Cmn) & Seamless (Spa) & Seamless (Fra) & Seamless(Ita) & Seamless (Deu) & Voicebox (Eng) & AudioGen & MusicGen \\\\ \\hline None & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\\\ \\hline Bandpass & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\\\ Highpass & 0.71 & 0.68 & 0.70 & 0.70 & 0.70 & 0.64 & 0.52 & 0.52 \\\\ Lowpass & 1.00 & 0.99 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\\\ Boost & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\\\ Duck & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\\\ Echo & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\\\ Pink & 0.99 & 1.00 & 0.99 & 1.00 & 0.99 & 1.00 & 1.00 & 1.00 \\\\ White & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\\\ Fast (x1.25) & 0.97 & 0.98 & 0.99 & 0.98 & 0.99 & 0.98 & 0.87 & 0.87 \\\\ Smooth & 0.96 & 0.99 & 0.99 & 0.99 & 0.99 & 0.98 & 0.98 & 0.98 \\\\ Resample & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\\\ AAC & 0.99 & 0.99 & 0.99 & 0.99 & 0.97 & 0.99 & 0.98 \\\\ MP3 & 0.99 & 0.99 & 0.99 & 0.99 & 0.97 & 0.99 & 1.00 \\\\ Encodec & 0.97 & 0.98 & 0.99 & 0.99 & 0.98 & 0.96 & 0.95 & 0.95 \\\\ \\hline Average & 0.97 & 0.97 & 0.98 & 0.98 & 0.98 & 0.97 & 0.95 & 0.95 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 도메인 및 언어 전반에 걸친 오디오실 일반화 평가. 즉, Expresso 데이터세트(Nguyen et al., 2023)로부터의 음성 샘플의 번역은 SeamlessExpressive 모델(Seamless Communication et al., 2023)을 사용하여 중국어(CMN), 프랑스어(FR), 이탈리아어(IT) 및 스페인어(SP)의 네 가지 타겟 언어로 번역된다. Music from MusicGen (Copet et al., 2023) and environmental sound from AudioGen (Kreuk et al., 2023).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>