<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MobileVLM V2: 비젼 언어 모델을 위한 더 빠르고 강한 기준선\n' +
      '\n' +
      '상상추원, 임맹차오원, 신유장원, 샹쑤원, 페이웨이원, 양양1,3\n' +
      '\n' +
      '샤오페이 선1, 이밍후,1 신양린1, 보장1, 춘화선2\n' +
      '\n' +
      '(주)원메이투안 2절강대학교, 중국3다롄공과대학, 중국\n' +
      '\n' +
      '동일한 기여도, 성의 알파벳 순서에 의해서만 정렬됨\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '우리는 MobileVLM에 따라 상당히 개선된 비전 언어 모델의 패밀리인 MobileVLM V2를 소개하며, 이는 새로운 아키텍처 설계의 섬세한 조정, 모바일 VLM에 맞춘 개선된 훈련 방식 및 풍부한 고품질 데이터 세트 큐레이션이 VLM의 성능에 실질적으로 도움이 될 수 있음을 입증한다. 구체적으로, MobileVLM V2 1.7B는 3B 규모에서 훨씬 더 큰 VLM에 비해 표준 VLM 벤치마크에서 더 나은 또는 온-파 성능을 달성한다. 특히, 우리의 3B 모델은 7B+ 규모에서 다양한 VLM을 능가한다. 모델은 [https://github.com/Meituan-AutoML/MobileVLM](https://github.com/Meituan-AutoML/MobileVLM)에서 출시됩니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '현재까지 비전 언어 모델(VLM)[46, 58, 75, 4]은 인공 지능 커뮤니티[71]의 중요한 연구 초점이 되어 왔다. 대형 언어 모델(LLM)과 멀티 모달리티 기능을 통합하는 것은 제미니[26] 및 GPT-4V[53]와 같은 독점 모델에 의해 입증된 전례 없는 다재다능한 능력을 갖는 것으로 검증되며, 이는 다양한 다운스트림 작업 [1, 26]에서 놀라운 성능을 발휘했다. 그러나, 능력 있는 인텔리전스 어시스턴트로서의 비전 언어 모델이 모바일 장치, 자율 주행 자동차 및 체화된 AI 시스템, _etc_와 같은 실제 시나리오에 배치될 수 있도록 하기 위한 과제가 남아 있다.\n' +
      '\n' +
      '가장 최근에 MobileVLM[15]은 혁신적인 하드웨어 중심의 아키텍처로 모바일 규모에서 VLM의 용량을 탐색하는 데 우선시된다. MoE-LLVA[41]은 VLM에 대해 혼합 전문가 방법[22, 30]을 적용하는데, 이는 훨씬 더 큰 모델에 비해 더 작은 모델의 한계를 압도적으로 밀어내어 능가 성능을 산출했다. 최근 조사[71]에서 알 수 있듯이 VLM의 진행은 확장된 양식, 정제된 훈련 파이프라인, 효율적인 아키텍처 및 고품질 훈련 데이터 세트의 추세를 나타낸다.\n' +
      '\n' +
      '본 논문에서는 MobileVLM[15]을 기반으로 보다 빠르고 강력한 베이스라인을 구축한다. 주요 개선 사항은 크게 세 가지 측면, 즉 소형 VLM의 기여 훈련 데이터를 활용하고 효과적인 훈련 전략을 탐색하며 고성능 경량 프로젝터를 개조하는 것이다. 구체적으로 ShareGPT4V[10]에서 수행한 고품질 이미지 텍스트 쌍을 사용하여 시각 언어 기능을 효과적으로 정렬하고, ScienceQA[49], TextVQA[60], SBU[54], _etc_와 같은 데이터 다양성과 명령어 추적 능력을 높이기 위해 더 많은 학문적 작업을 통합한다. 훈련 패러다임의 경우, 사전 훈련 및 명령어 튜닝 단계에서 프로젝터와 언어 모델의 모든 매개변수에 대한 철저한 훈련을 수행하는데, 이는 우수한 품질의 데이터의 완전한 잠재력을 활용하는 데 유리함을 입증한다. 또한 시각과 언어 모델을 연결하는 보다 능률적이면서도 강력한 투영 메커니즘을 소개합니다. 위치 정보가 향상된 이미지 토큰의 표현을 개선하여 성능 저하 없이 이미지 토큰의 수를 크게 압축할 수 있다.\n' +
      '\n' +
      '우리의 주요 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '1. 우리는 증가하는 성능을 탐색하고 평가한다\n' +
      '\n' +
      '도 1: 여러 표준 벤치마크 및 속도(llama.cpp와 함께 NVIDIA Jetson Orin에서 테스트됨)에 걸친 평균 성능 측면에서 SOTA VLM의 비교. MobileVLM V2는 훨씬 빠른 추론 속도로 새로운 최첨단 결과를 달성한다.\n' +
      '\n' +
      '모바일VLM[15]과 같은 소형 VLM들과 대형 VLM들 사이의 갭을 상당히 메우는, 소형 비전 언어 모델들에 대한 트레이닝 데이터.\n' +
      '2. 모바일 시나리오를 위한 더 나은 훈련 전략에 뛰어들고 보다 고품질의 멀티모달 데이터의 잠재력을 완전히 활용하는 방법에 대한 새로운 훈련 계획을 설계한다. 우리는 약간의 성능 저하와 함께 시각적 토큰을 크게 줄이기 위해 매우 가벼운 프로젝터를 제안한다.\n' +
      '3. 본 논문에서 제안하는 방법은 여러 비젼 언어 벤치마크에서 성능과 추론 속도의 새로운 최첨단의 절충점을 달성한다. 우리의 모델을 7B 매개변수로 확장함으로써, 우리의 방법은 명확한 마진을 가진 이전의 SOTA 모델보다 우수하다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '대규모 언어 모델.지난 몇 년 동안 LLM의 출현은 GPT-3[6], PaLM[13], OPT[72], BLOOM[59]을 포함하여 자연어 처리에서 상당한 발전을 가져왔다. 민주적 LLM의 대표적인 작업은 LLM 효과 개선에 대한 연구에 대한 열의를 더욱 높이는 LLaMA이다. 대표적인 작품인 InstructGPT[55]와 ChatGPT[51]에 이어 알파카와 비쿠나는 언어 상호 작용 능력을 향상시키기 위해 인간 유도 주석으로 LLaMA를 조정했다. 최근 많은 기관에서 대규모 언어 모델[3, 21, 63, 69, 2, 52]을 제안했는데, 이는 여러 벤치마크에서 평가되어 우수한 성능을 달성했다. 휴대폰, 자율 주행 자동차, 체화된 AI 시스템과 같은 에지 장치의 제한된 계산 자원으로 인해 몇 개의 라이프-LLM[56, 15, 39, 5, 62]이 상당한 관심을 끌었다. TinyLLaMA[56]이 그들의 1.1B 기반 및 채팅 모델에 대해 훌륭한 오픈 소스 작업을 수행한다는 점에 주목할 필요가 있다. MobileLLaMA[15]는 LLaMA의 아키텍처를 축소하고 공개 데이터 세트로 처음부터 훈련된 1B 및 3B 모델을 릴리스한다. 한편, 대용량 언어 모델에 대한 양자화 및 프루닝을 포함하는 모델 압축 기술도 활발히 연구되고 있다[23, 24, 68]. 최근 LLM의 정밀도는 최소 정확도 저하로 더 빠른 추론을 위해 W4A8[38] 또는 이진[37]로 감소한다.\n' +
      '\n' +
      '멀티모달 대형 언어 모델.몇몇 작품 [61, 58, 36]은 주로 비전 인코더와 언어 모델로 구성된 일련의 멀티모달 모델 아키텍처를 제안했다. [66, 12] 통일된 트랜스포머 아키텍처 하에서 시각적 언어 추론 작업을 탐구합니다. 비전 언어 모델의 패러다임은 특정 작업을 위해 설계된 전통적인 비전 모델보다 더 나은 일반화 가능성을 보여준다. LLM의 급속한 발전으로 많은 작품[4, 10, 75, 46]이 LLM에 시각적 지식을 주입하는 데 집중되었다. FROMAGe[32] 및 LLaVA[46]은 시각적 토큰을 LLM에 직접 공급함으로써, LLM이 이미지의 의미 정보를 이해하고 시각적 콘텐츠에 대한 질의에 정확하게 응답할 수 있게 한다. InternLM-XComposer2[20]은 사전 훈련된 언어 지식의 무결성을 보존하기 위한 부분적 LoRA(Partial LoRA) 접근법을 제안하며, 이는 문학적 재능과 함께 정밀한 비전 이해와 텍스트 구성 사이의 균형을 이룬다. 경쟁력이 높은 성과는 복합적 이해의 영역에서 괄목할 만한 숙련도를 보여준다. 엣지 장치에 MLLM을 배치하는 긴급한 요구를 해결하기 위해 여러 작업[15, 26, 41, 67, 76]이 제안되었다. 이 분야의 선두주자인 제미니[26]는 스마트폰용으로 특별히 설계된 1.8B/3.25B 파라미터가 적용된 경량 비전 언어 모델 제미니 나노를 출시했다. 그러나 그들의 모델과 데이터는 공개 소스되지 않습니다. MobileVLM[15]은 자원 제약 시나리오 하에서 오픈 소스 1B/3B 비전 언어 모델을 제공하는 첫 번째 접근법일 수 있다. MoE-LLaVA[41]는 시각적 이해 능력을 향상시키기 위해 MoE 기반 희소 모델 아키텍처를 제안한다. LLaVA-Phi[76]은 Phi-2.7B를 언어 기반 모델로 활용하여 비전 언어 추론 작업에서 우수한 성능을 달성한다. 바이어리[67]는 개선된 시각 어휘를 도입하고 이미지 특징을 향상시켜 더 나은 일반성으로 이어진다. 특히 Vary[67]은 세립 지각 작업에서 우수한 잠재력을 보여준다.\n' +
      '\n' +
      '멀티모달 명령어-튜닝.시각과 언어 사이의 모달리티 갭을 해결하기 위해 [58]과 같은 작업은 시각적 및 텍스트적 특징 표현을 정렬하기 위해 대조적 학습을 사용한다. 또한 [33, 57]의 방법은 강력한 제로 샷 기능으로 범용 탐지 또는 분할을 달성하기 위해 대규모 언어 모델의 기능을 활용했다. 자연어 처리의 성공에 영감을 받아, Liu et al. [46]은 언어를 태스크 명령어로서 갖는 범용 멀티모달 모델을 생성하는 것을 목표로 하는 시각적 명령어 튜닝을 도입하였다. SVIT[73]은 이미지의 풍부한 수동 주석으로 GPT-4를 프롬프트하여 생성된 고품질 및 풍부한 다양성으로 특징지어지는 420만 개의 시각적 명령 튜닝 데이터 세트를 구성하여 시각적 명령 튜닝을 확장한다. ShareGPT4V[10]은 GPT4-Vision에서 100K 고품질 캡션을 구성하며, 이는 사전 교육, 세계 지식, 객체 속성, 공간 관계 및 미적 평가를 위한 120만 개의 상세하고 유익한 캡션으로 확장되었다. LVIS-instruct4v[65]는 LVIS의 이미지로 강력한 GPT-4V를 프롬프트함으로써 생성된 220K 시각적 정렬 및 상황 인식 명령을 구축한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '우리의 방법은 MobileVLM[15]과 유사한 프레임워크를 따른다. 도 2에 도시된 바와 같이, MobileVLM V2의 전체 아키텍처는 이미지 특징을 추출하기 위한 사전 훈련된 비전 인코더, 멀티모달 토큰을 처리하고 최종 답변을 생성하기 위한 사전 훈련된 대형 언어 모델 MobileLLaMA 및 이미지 특징을 언어 모델에 정렬하기 위한 모바일 친화적인 프로젝터, _i.e_, 경량 다운샘플 프로젝터(LDPv2로 표시됨)로 구성된다. 우리는 다음에서 각 구성 요소의 세부 사항을 소개한다.\n' +
      '\n' +
      '### Vision Encoder\n' +
      '\n' +
      '모바일VLM에 이어, 우리는 CLIP ViT-L/14[58]을 비전 인코더\\(\\mathbf{F}_{enc}\\)로 사용하며, 이는 수백만 개의 이미지 언어 쌍에 대해 대비적으로 사전 훈련되고 VLM[15]에 대해 효과적인 것으로 입증되었다. 특히, 이미지\\(\\mathbf{X}_{v}\\in\\mathbb{R}^{H}\\times W\\times C}\\)를 먼저 336\\(\\times\\)의 해상도로 리사이징하고, 스트라이드\\(P\\)이 14인 패치로 평탄화한다. 그리고, 이미지의 의미 정보를 표현하기 위해 높은 수준의 시각적 임베딩(f_{v}\\in\\mathbb{R}^{N}{v}\\times D_{v}\\)을 추출하며, 여기서 \\(N_{v}=HW/P^{2}\\)과 \\(D_{v}\\)은 시퀀스 길이와 시각적 임베딩의 숨겨진 크기를 나타낸다. 공식적으로\n' +
      '\n' +
      '\\[f_{v}=\\mathbf{F}_{enc}(\\mathbf{X}_{v}). \\tag{1}\\]\n' +
      '\n' +
      '### Language Model\n' +
      '\n' +
      '우리는 MobileLLaMA[15] 시리즈를 기본 대형 언어 모델(LLM)로 사용한다. 이 선택에는 세 가지 장점이 있습니다. 첫째, MobileLLaMA는 기성품 배치를 용이하게 하기 위해 설계되었으며, 인상적인 성능으로 자원이 제한된 장치에서 실시간 속도를 입증했다. 언어 모델을 변경하지 않고 유지하는 것은 데이터 코퍼스의 확장, 훈련 전략 개선, 프로젝터의 새로운 디자인 리노베이션 등과 같은 다른 많은 측면을 탐구하기 위해 통제된 실험을 수행하는 데 도움이 된다. 둘째, MobileLLaMA는 동일한 토큰타이저를 LLaMA2와 공유하여 통증 없이 증류를 수행하는 데 도움이 된다. 마지막으로, 오픈 데이터 세트에 대해 훈련되며 데이터 누출로 인한 평가 오염의 위험이 없습니다. 이것은 또한 이 모델이 다른 독점적 대응물을 능가할 수 있는지 여부와 방법을 확인하는 데 도움이 됩니다.\n' +
      '\n' +
      '구체적으로 MobileLLaMA-1.4B-Chat과 MobileLLaMA-2.7B-Chat을 채택한다. 텍스트 입력 \\(\\mathbf{X}_{q}\\)은 먼저 토큰화되어 텍스트 토큰 \\(\\mathbf{H}_{q}\\in\\mathbb{R}^{N}{t}\\times D_{t}\\)으로 처리되며, 여기서 \\(N_{t}\\)은 텍스트 토큰의 시퀀스 길이를 나타내고 \\(D_{t}\\)은 단어 임베딩 공간의 숨겨진 크기이다. 텍스트 토큰\\(\\mathbf{H}_{q}\\)과 비주얼 토큰\\(\\mathbf{H}_{v}\\)은 프로젝터에 의해 변환되어 언어 모델의 입력으로 연결된다. 길이\\(L\\)을 갖는 최종 응답 \\(\\mathbf{Y}_{a}\\)는 다음과 같이 자기회귀 방식으로 생성되고,\n' +
      '\n' +
      '\\[p(\\mathbf{Y}_{a}|\\mathbf{H}_{v},\\mathbf{H}_{q})=\\prod_{i=1}^{L}p(y_{i}|\\mathbf{H}_{v},\\mathbf{H}_{q},y_{<i}). \\tag{2}\\\n' +
      '\n' +
      '경량 다운샘플 프로젝터\n' +
      '\n' +
      'MobileVLM[15]의 LDP 설계에서 영감을 얻은 새로운 프로젝터를 도입하여 더 적은 파라미터로 더 나은 비전 언어 특징 정렬을 수행합니다. 그것은 _i.e_, _feature transformation_, _token reduction_, _positional information enhancement_의 세 가지 구성요소를 포함한다. 먼저, LLM의 특징 차원과 일치하도록 이미지 토큰에 두 개의 점별 컨볼루션 레이어를 사용한다. 그런 다음 이미지 토큰의 수를 극도로 압축하기 위해 평균 풀링 계층을 도입한다. 마지막으로, 위치 정보를 향상시키기 위해 스킵 연결을 갖는 매우 간단하지만 효과적인 모듈 PEG[16]를 적용한다. LDP[15]와 비교하여, 이 위치 부분은 더 효율적이고 파라미터들의 **99.8%** 수를 감소시키며, 달리기 속도에서 약간 더 빠르다.\n' +
      '\n' +
      '제형에 있어서, 경량 다운샘플 프로젝터 LDPv2(\\(\\mathbf{P}\\)는 위치 향상과 함께 시각 임베딩 \\(f_{v}\\in\\mathbb{R}^{N}{v}\\times D_{v}\\)을 모달리티-정렬 시각 토큰 \\(\\mathbf{H}_{v}\\)으로 변환한다. 주류 추론 프레임워크에 의해 잘 지원되는 운영자로 구성되기 때문에 이 설계도 배치 친화적이라는 점에 주목할 필요가 있다. 평균 커널(k\\)이 주어지면 나머지 토큰의 수는 입력 피쳐의 \\(1/k^{2}\\)에 불과하다. 구체적으로, 제안하는 LDPv2를 다음과 같이 공식화하고,\n' +
      '\n' +
      '\\begin{cases}f_{0}&=PW(GELU(PW(f_{v})) \\\\f_{1}&=AvgPool_{2\\times 2}(f_{0})\\\\mathbf{H}_{v}&=DW(f_{1})+f_{1}.\\end{cases}\\tag{3}\\times\n' +
      '\n' +
      '여기서 \\(PW\\) 및 \\(DW\\)은 각각 점방향 및 깊이방향 컨볼루션이고, \\(GELU\\)은 GELU[28] 활성화 층이고, \\(AvgPool_{2\\times 2}\\)은 \\(2\\times 2\\) 평균 풀링 층이다.\n' +
      '\n' +
      '### Training Strategy\n' +
      '\n' +
      '우리의 훈련 과정은 사전 훈련과 다중 작업 훈련의 두 단계로 나뉜다. 표 1에 도시된 바와 같이,\n' +
      '\n' +
      '그림 2: **MobileVLM V2의 아키텍처.**\\(\\mathbf{X}_{v}\\) 및 \\(\\mathbf{X}_{q}\\)는 각각 이미지와 언어 지시를 나타내며, \\(\\mathbf{Y}_{a}\\)는 언어 모델 MobileLLaMA로부터의 텍스트 응답을 나타낸다. 오른쪽 아래 모서리의 다이어그램은 LDPv2, _i.e_, 경량 다운샘플 프로젝터 v2에 대한 상세한 설명이다.\n' +
      '\n' +
      'LLaVA-1.5[44] 및 MobileVLM[15]의 이전 훈련 패러다임, MobileVLM V2는 시각적 인코더가 동결된 상태에서 두 단계 모두에서 프로젝터와 대형 언어 모델을 일관되게 개방한다.\n' +
      '\n' +
      '#### 3.4.1 Pre-training\n' +
      '\n' +
      '대부분의 비젼-언어 모델들(VLMs) [7, 44]은 일반적으로 최적화 어려움을 피하기 위해 사전 트레이닝 동안 비주얼 인코더 및 언어 모델을 동결시킨다. ShareGPT-4V[10]은 시각적 인코더를 부분적으로 동결시키고 언어 모델을 트레이닝 가능하게 한다. 우리의 경우, CLIP ViT-L/14[58]와 MobileLLaMA[15]로부터 시각적 인코더와 언어 모델의 가중치를 각각 초기화한다. 이러한 초기화는 후속 통합 트레이닝 프로세스를 위한 강건한 기초로서 작용한다. 비전 인코더를 고정하는 동안 프로젝터와 LLM을 완전히 훈련할 수 있습니다. ViT를 동결하면 훈련 비용도 절감됩니다. 그런 다음 모델의 학습 목표는 자기 회귀 손실 함수를 사용하여 다음 토큰의 예측에 집중된다. 이 특정 작업을 연마함으로써 모델은 시각적 정보의 맥락에서 언어 생성의 복잡성을 더 잘 학습하여 멀티모달 작업에 대한 성능 향상으로 이어진다.\n' +
      '\n' +
      '표 2에 보고된 바와 같이 사전 훈련 단계 동안 우리의 모델은 120만 개의 이미지-텍스트 쌍을 포함하는 ShareGPT4V-PT 데이터 세트[10]를 사용하여 훈련되었다. 이 데이터 세트는 멀티모달 표현 학습의 중요한 측면인 모델의 이미지-텍스트 정렬 능력을 향상시키는 데 도움이 된다.\n' +
      '\n' +
      '3.4.2 다중작업 훈련\n' +
      '\n' +
      '이미지-텍스트 정렬 학습의 사전 학습 단계 이후 MobileVLM V2는 이미지 내용을 이해할 수 있는 초보적인 능력을 획득하였다. 그러나, 일련의 다운스트림 작업에서 분석 및 대화를 위한 시각적 정보를 활용하는 숙련도가 부족하다. 결과적으로, 다중 작업 훈련 단계에서는 다중 작업 분석 및 이미지 텍스트 변환을 위한 모델을 제공하기 위해 훈련 과정에 여러 개의 비전 언어 작업을 도입한다.\n' +
      '\n' +
      '다중 작업 훈련 단계에서는 모델의 다양한 기술을 더욱 발전시키기 위해 다양한 작업을 특징으로 하는 많은 데이터 세트를 사용한다. 표 2에 요약된 바와 같이, 이들 데이터 세트는 비주얼 다이얼로그 데이터세트[19], TextVQA 데이터세트[60]를 통한 OCR 기술, COCO 캡션[11] 및 SBU[54] 데이터세트를 통한 장면 이해 능력, 및 VSR 데이터세트[43]에 의한 위치 이해 능력을 향상시키기 위해 세심하게 선택된다. 데이터 볼륨이 공식적으로 출시된 데이터와 정확히 일치하지 않을 수 있는 SBU 데이터 세트를 청소하고 정제했다. 이 단계에 대한 집계된 데이터는 총 240만 개의 샘플로 구성되어 다양한 양식과 작업에 걸친 포괄적인 학습 경험을 보장한다. 다양한 데이터 세트에 걸친 대화 포맷에 관한 예는 부록 A에 제공된다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      '**사전 훈련** 표 3에 열거된 바와 같이, LDPv2는 이 단계에서 랜덤하게 초기화되며, 언어 모델 및 비전 인코더는 각각 MobileLLaMA 및 CLIP ViT-L/14로부터 사전 훈련된 가중치로 초기화된다. 최적화를 위해 무게 감쇠가 없는 AdamW 최적화기[48]를 사용한다. LDPv2와 다른 구성 요소에 대한 최대 학습률은 코사인 학습률 스케줄에 따라 각각 \\(1e^{-3}\\)과 \\(2e^{-5}\\)으로 구성된다. 사전 훈련 단계는 8에 걸쳐 256의 전체 배치 크기를 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{Pre-training} & \\multicolumn{3}{c}{Multi-task training} \\\\  & \\(\\mathcal{V}\\) & \\(\\mathcal{P}\\) & \\(\\mathcal{L}\\) & \\(\\mathcal{V}\\) & \\(\\mathcal{P}\\) & \\(\\mathcal{L}\\) \\\\ \\hline LLaVA-1.5-7B [44] & ✗ & ✓ & ✗ & ✗ & ✓ & ✓ \\\\ ShareGPT4V-7B [10] & ✗ & ✓ & ✓ & ✗ & ✓ & ✓ \\\\ MobileVLM [15] & ✗ & ✓ & ✗ & ✗ & ✓ & ✓ \\\\ MobileVLM V2 & ✗ & ✓ & ✓ & ✗ & ✓ & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **훈련전략 비교. \\ (\\mathcal{V}\\), \\(\\mathcal{P}\\) 및 \\(\\mathcal{L}\\)은 각각 비전 인코더, 프로젝터 및 언어 모델을 나타낸다. ✓ 대응하는 모델 파라미터들이 트레이닝에서 최적화됨을 나타내고, ✗는 파라미터들이 동결됨을 나타내고, ✓는 모델 파라미터들의 서브세트가 동결됨을 나타낸다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Datasets & Type & Samples \\\\ \\hline _Pretraining_ & & \\\\ ShareGPT4V-PT [10] & Caption & 1.2M \\\\ _Multi-task training_ & & \\\\ Visual Dialog [19] & Conversation & 123K \\\\ Text-VQA [60] & VQA(Open) & 35K \\\\ VSR [43] & VQA(Open) & 13K \\\\ VIGC [64] & VQA(Open) & 37K \\\\ IConQA [50] & VQA(MC) & 107K \\\\ SQA [49] & VQA(MC) & 13K \\\\ COCO [11] & Caption & 592K \\\\ SBU [54] & Caption & 844K \\\\ ShareGPT4V [10] & Mixed & 665K \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: MobileVLM V2 트레이닝에 사용되는 **데이터세트. 열 "유형"은 데이터세트의 작업이며, 여기서 "혼합"은 데이터세트에 다양한 작업에 속하는 샘플이 포함되어 있음을 나타낸다. 열 "샘플"은 각 데이터 세트의 이미지-텍스트 쌍의 수이다.**NVIDIA A100 GPU는 약 5시간 동안이다.\n' +
      '\n' +
      '**멀티 태스크 트레이닝** 이 트레이닝 단계 동안, MobileVLM V2의 가중치는 첫 번째 단계부터 초기화된다. 대부분의 훈련 하이퍼 파라미터는 훈련 전 단계와 유사하다. 또한 학습률은 \\(4e^{-5}\\)으로 설정하였다. 이 단계의 훈련은 전체 배치 크기가 128인 약 9시간 동안 8개의 NVIDIA A100 GPU를 필요로 한다. 자세한 훈련 설정은 표 3에 나와 있다.\n' +
      '\n' +
      '### 최신 방법과의 비교\n' +
      '\n' +
      'MobileVLM에 이어 이미지 질의 응답 시리즈 GQA[29], SQA[49], TextVQA[60], 종합 벤치마크 MME[25], MMBench[47], 객체 환각 벤치마크 POPE[40] 등의 벤치마크 목록을 채택한다.\n' +
      '\n' +
      '**SOTA 방법과의 비교.** MobileVLM V2의 성능을 평가하고 표 4의 정확도 결과를 보여준다. 이전 모델들은 실행 시간 지연에 관계없이 주로 정확도 향상에 초점을 맞추고 있음에 유의한다. 우리의 모델은 두 가지 측면에 관한 실제 응용 프로그램을 대상으로 하지만 명확한 마진을 가진 대부분의 이전 모델을 능가한다. 75% 빠른 속도의 장점으로 MobileVLM V2 3B는 평균 점수에서 MoE-LLaVA-2.7B\\(\\times\\)4[41]보다 1.4점 더 우수하다. MoE-LLaVA-2.7B\\(\\times\\)4 [41]은 이미 많은 7B+ VLM에 비해 비슷하거나 더 나은 정확도를 달성한다. 그들 중 다수는 많은 양의 교육비를 도입한다. 이와는 대조적으로, 우리의 방법은 새로운 최신 결과를 달성하는 반면, 우리의 훈련 비용은 계산 친화적인 LLaVA-1.5[45]에 필적한다.\n' +
      '\n' +
      '**Latency Comparison.** 많은 모델이 진보된 모바일 추론 프레임워크에 의해 지원되지 않았기 때문에 그림 3과 같이 NVIDIA A100 GPU에서 PyTorch 프레임워크를 사용하여 최근 모델의 더 큰 스펙트럼의 레이턴시를 비교한다. MobileVLM V2 모델은 일반적으로 토큰 생성 및 테스트된 벤치마크에서 평균 점수 측면에서 더 빠르고 강력하다. 특히 MoileVLM V2 1B/3B는 각각 37.37tokens/s와 28.97tokens/s로 MoE-LLaVA에 비해 1.65\\(\\times\\) 빠르지만 평균 성능은 더 높다.\n' +
      '\n' +
      '**MoE-LLaVA와의 비교**MoE-LLaVA는 여러 전문가를 활용하여 좋은 성능을 발휘하며, 각 전문가는 추론 속도를 향상시키기 위한 작은 모델이다. 비록 일부 파라미터만이 활성화되지만, 여전히 전체 파라미터를 저장해야 하며, 이는 모바일 시나리오에서 필연적으로 IO 오버헤드를 발생시킨다. 또한 이러한 모델에 모델 압축 트릭[14, 24, 37, 74]을 적용하는 것은 간단하지 않습니다. 대조적으로, 우리의 방법은 배포에 잘 지원되고 최적화될 수 있다. 한마디로, 테슬라 A100 GPU에서 평가된 추론 속도 이점은 자원이 제한된 환경에서 테스트되면 더욱 확대될 것이다. 원칙적으로, MobileVLM V2는 또한 그것의 MoE 설계와 결합될 수 있다. 그러나 MobileVLM V2의 메모리 및 대기 시간 이점을 희생하지 않고 결합하는 방법은 향후 작업으로 남아 있다.\n' +
      '\n' +
      '**MobileVLM과의 비교.** 표 4는 MobileVLM V2가 MobileVLM의 정확도 성능을 상당히 향상시킴을 보여준다. _ 평균 정확도는 5.3 포인트_만큼 부스팅된다. 이 두 방법은 시각과 언어 양식에 대해 동일한 인코더를 공유하기 때문에 향상된 데이터, 훈련 전략 및 프로젝터의 새로운 설계에 기인한다. MobileVLM V2의 이러한 좋은 성능은 [15]의 MobileLLaMA가 재생 가능하고 평가 데이터 누출 위험이 낮은 오픈 리소스 데이터세트 Rebrajama[17]에 구축되기 때문에 소규모 언어 모델의 좋은 기준선임을 나타낸다. 개방적이고 더 강력한 작은 언어 모델을 탐구하는 것은 우리의 미래 작업으로 남아 있습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Configuration & Pre-training & Multi-task training \\\\ \\hline Vision encoder init & CLIP ViT-L & CLIP ViT-L \\\\ LLM init & MobileLLaMA & MobileVLM V2 PT \\\\ Projector init & Random & MobileVLM V2 PT \\\\ Image resolution & \\(336^{2}\\) & \\(336^{2}\\) \\\\ Image token num & 144 & 144 \\\\ Global batch size & 256 & 128 \\\\ Training steps & 5K & 19K \\\\ Optimizer & AdamW & AdamW \\\\ LR schedule & Cosine decay & Cosine decay \\\\ Projector LR & \\(1e^{-3}\\) & \\(4e^{-5}\\) \\\\ Base LR & \\(2e^{-5}\\) & \\(4e^{-5}\\) \\\\ Weight decay & 0 & 0 \\\\ Warm-up ratio & 0.03 & 0.03 \\\\ DeepSpeed Stage & 2 & 3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: **MobileVLM V2.** MobileVLM V2 PT의 훈련 하이퍼파라미터는 사전 훈련 단계 이후에 저장된 파라미터를 나타냄에 유의한다. LR은 학습률을 나타낸다.\n' +
      '\n' +
      '그림 3: NVIDIA A100 GPU에서 SOTA VLM의 속도 비교. 정확도는 6개의 VLM 벤치마크에서 평균화된다(표 4 참조). (1의 배치 크기로 테스트됨, 256개의 토큰을 생성함).\n' +
      '\n' +
      '우리의 목표는 자원 제약 시나리오에 대한 강력한 멀티모달 모델을 설계하는 것이지만, 우리는 성능 상한을 확인하기 위해 모델을 추가로 확장한다. 이것은 또한 많은 현존하는 VLM과 사과 대 사과 비교를 형성한다. 구체적으로, 우리는 Vicuna-7B를 LLM 모델로 활용하고 종합적인 멀티모달 성능 향상이 나타나는 그림 4의 결과를 보여준다. 우리는 우리의 MobileVLM V2 7B와 LLaVA-1.5 7B[44] 및 ShareGPT4V 7B와 같은 주류 대규모 VLM을 비교한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c c} \\hline \\hline Method & LLM & Res. & GQA & SQA\\({}^{1}\\) & VQA\\({}^{\\text{T}}\\) & POPE & MME\\({}^{\\text{P}}\\) & MMB\\({}^{\\text{dev}}\\) & Avg. \\\\ \\hline IDEFICS-80B [34] & LLaMA-65B & 224 & 45.2 & – & 30.9 & – & – & 54.5 & – \\\\ BLIP-2 [35] & Vicuna-13B & 224 & 41.0 & 61.0 & 42.5 & 85.3 & 1293.8 & – & – \\\\ InstructBLIP [18] & Vicuna-13B & 224 & 49.5 & 63.1 & 50.7 & 78.9 & 1212.8 & – & – \\\\ Shikra [9] & Vicuna-13B & 224 & – & – & – & – & – & 58.8 & – \\\\ Openflamingo [2] & MPT-7B & 336 & – & – & 33.6 & – & – & 4.6 & – \\\\ Qwen-VL [4] & Qwen-7B & 448 & 59.3 & 67.1 & 63.8 & – & 1487.6 & 38.2 & – \\\\ mPLUG-Owl [70] & LLaMA-7B & 224 & – & – & – & – & 967.3 & 49.4 & – \\\\ IDEFICS-9B [34] & LLaMA-7B & 224 & 38.4 & – & 25.9 & – & – & 48.2 & – \\\\ MiniGPT-v2 [8] & LLaMA-7B & 448 & 60.3 & – & – & – & – & 12.2 & – \\\\ MiniGPT-4 [75] & Vicuna-7B & 224 & 32.2 & – & – & – & 581.7 & 23.0 & – \\\\ InstructBLIP [18] & Vicuna-7B & 224 & 49.2 & 60.5 & 50.1 & – & – & 36.0 & – \\\\ LLaVA-1.5 [44] & Vicuna-7B & 336 & 62.0 & 66.8 & 58.2 & 85.9 & 1510.7 & 64.3 & 68.8 \\\\ ShareGPT4V [10] & Vicuna-7B & 336 & 63.3 & 68.4 & 60.4 & 85.7 & 1567.4 & 68.8 & 70.8 \\\\ MoE-LLaVA-1.6B\\(\\times\\)4 [41] & StableLM-1.6B & 336 & 60.4 & 62.6 & 47.8 & 84.3 & 1300.8\\({}^{\\text{s}}\\) & 59.4 & 63.3 \\\\ MoE-LLaVA-2.7B\\(\\times\\)4 [41] & Phi-2.7B & 336 & 61.1 & 68.7 & 50.2 & 85.0 & 1396.4\\({}^{\\text{t}}\\) & 65.5 & 66.7 \\\\ \\hline MobileVLM 1.7B [15] & MobileLLaMA 1.4B & 336 & 56.1 & 57.3 & 41.5 & 84.5 & 1196.2 & 53.2 & 58.7 \\\\ MobileVLM V2 1.7B & MobileLLaMA 1.4B & 336 & 59.3 & 66.7 & 52.1 & 84.3 & 1302.8 & 57.7 & 64.2 \\\\ \\hline MobileVLM 3B [15] & MobileLLaMA 2.7B & 336 & 59.0 & 61.2 & 47.5 & 84.9 & 1288.9 & 59.6 & 62.8 \\\\ MobileVLM V2 3B & MobileLLaMA 2.7B & 336 & 61.1 & 70.0 & 57.5 & 84.7 & 1440.5 & 63.2 & 68.1 \\\\ \\hline MobileVLM V2 7B & Vicuna-7B & 336 & 62.6 & 74.8 & 62.3 & 85.3 & 1560.7 & 69.2 & 72.1 \\\\ MobileVLM V2 7B w/o AvgPool & Vicuna-7B & 336 & 64.6 & 74.8 & 66.8 & 86.1 & 1558.7 & 70.8 & 73.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 6개의 VLM 벤치마크에 대한 SOTA 방법과의 **비교.** GQA[29]; SQA\\({}^{1}\\): ScienceQA-IMG[49]; VQA\\({}^{\\text{T}}\\): TextVQA[60]; POPE[40]; MME\\({}^{\\text{P}}\\): MME 지각[25]; MMB\\({}^{\\text{dev}}\\): MMBench-dev[47]; Column _Res._L 는 비전 모델의 이미지 해상도이다. 컬럼 _Avg._ 6개의 평가 벤치마크에 대한 평균 정확도를 나타냅니다. MME({}^{\\text{P}}\\) 열의 값은 평균 정확도 계산에 포함되기 전에 2000으로 나누어야 한다. *는 논문[41]에 제공되지 않은 결과가 그들의 최신 repo(commit 5ba14e8)를 사용하여 평가되었음을 나타낸다. SQA\\({}^{1}\\)에 대한 MobileVLM 값은 [15]에서 오류가 있으며 이 버전에서는 수정된다.\n' +
      '\n' +
      '그림 4: MobileVLM V2의 모델을 여러 표준 작업에 걸쳐 스케일링할 때 6개의 VLM 벤치마크에 대한 평균 성능 개선(표 4 참조)\n' +
      '\n' +
      '그림 5: 6개의 표준 벤치마크에서 모바일VLM V2의 성능을 피어와 비교한 레이더 플롯.\n' +
      '\n' +
      '[10]의 정확도와 추론 속도는 모두 그림 5와 표 5와 같다. 본 논문에서 제안하는 MobileVLM V2 7B는 여러 벤치마크에 걸쳐 상당한 성능 에지를 확보할 뿐만 아니라 대규모 대표자에 비해 추론 속도의 명확한 리드를 확립함을 알 수 있다. 모바일VLM V2 7B가 ShareGPT4V를 평균 성능에서 1.3점 앞섰다. 이것은 데이터 스케일링 전략, 훈련 전략 및 새로운 프로젝터 설계의 효과를 추가로 보여준다.\n' +
      '\n' +
      '7B 모델에서 대기시간 간격이 좁아지는 것을 관찰하여, 토큰 감소 성분인 _i.e._, 커널이 2인 평균 풀링(pooling)을 제거한다. 이 설정 하에서, 7B 모델은 ShareGPT4V와 동일한 대기시간을 갖는다. 표 4의 마지막 행에서 결과를 보여준다. MobileVLM V2 7B(w/o AvgPool)는 평균 73.5점을 달성하여 LLaVA-1.5보다 4.7점 크게 능가한다. MobileVLM V2 7B(AvgPool 포함)와 비교했을 때, 성능 향상은 주로 OCR 작업인 TextVQA 작업에 대한 점수 증가에서 비롯된다. 이 작업에는 토큰 축소가 해로울 수 있는 작은 개체가 많이 포함되어 있습니다. 향후 작업에서 고해상도 입력을 효과적으로 사용하는 방법을 유지합니다.\n' +
      '\n' +
      '### 모바일 기기의 대기시간 측정\n' +
      '\n' +
      '일관성을 유지하기 위해, 우리는 모바일VLM과 동일한 구성을 갖는 NVIDIA AGX Jetson Orin 플랫폼에서 모바일VLM V2의 추론 지연을 측정한다[15]. 추론 프레임워크는 l1ama.cpp[27] 프레임워크를 사용한다. 구체적으로, Jetson Orin 플랫폼의 경우, LDPv2(섹션 3.3)를 위한 보다 효율적인 CUDA 구현을 개발하여 하드웨어를 최대한 활용하여 최상의 성능을 발휘한다. 표 5는 추론 지연 시간의 비교 결과를 보고한다. 추론 성능을 보다 객관적으로 평가하기 위해 총 시간을 256개의 출력 토큰 수로 나누어 계산한 출력 토큰의 실제 생성 속도를 나타내는 \\(Eval_{avg}\\)을 소개한다.\n' +
      '\n' +
      '**NVIDIA Jetson Orin.** Jetson Orin 플랫폼에서 MobileVLM V2가 동일한 매개변수 척도에서 상대보다 낮은 추론 지연 시간을 보여준다고 결론지었다. 표 5에 나타난 바와 같이, MobileVLM V2는 추론 속도에서 최고의 성능을 달성하는데, 이는 프로젝터의 보다 경량화된 설계에 기인할 수 있다: 우리는 시각적 프롬프트의 원래 576 토큰을 144개로 최적화하는 반면, 평균 정확도에 대한 성능은 그대로 유지되거나 훨씬 더 좋아진다.\n' +
      '\n' +
      '## 5 Abablation Study\n' +
      '\n' +
      '### 데이터 스케일링의 효과\n' +
      '\n' +
      '본 절에서는 데이터 향상 전략의 효과를 탐색한다. 표 6에 나타낸 바와 같이, 첫 번째 행은 MobileVLM 1.7B의 베이스라인을 나타낸다. 우리는 사전 훈련 데이터 세트를 ShareGPT4V 데이터 세트[10]로 교체하고 섹션 3.4.2에 설명된 대로 명령어 튜닝 데이터 세트를 2.4M으로 확장한다. 훈련 전략 및 모델 아키텍처는 베이스라인과 동일하게 유지된다는 점에 유의한다. GQA, SQA 및 TextVQA에 대한 성능 향상은 데이터 향상 후에 인지 및 대화에서 모델의 능력이 향상되었음을 보여준다. 그러나, 보다 높은 품질의 데이터 구성에서, 원래의 트레이닝 전략을 유지하는 것은 데이터 이점을 완전히 이용할 수 없고, MME 및 MMBench에서의 성능 저하를 초래한다는 것을 또한 관찰한다.\n' +
      '\n' +
      '### 훈련전략의 효과\n' +
      '\n' +
      '섹션 5.1의 관찰을 기반으로 보다 합리적인 훈련 전략을 추가로 탐색한다. 향상된 데이터 및 소규모 VLM의 설정 하에서, 전체 트레이닝 단계에서 언어 모델을 잠금해제하는 것은 보다 효과적인 학습을 가능하게 한다. 따라서, 우리는 언어 모델뿐만 아니라 프로젝터의 동시 미세 조정을 선택한다. 비교 결과는 표 6의 두 번째 행과 세 번째 행에 나와 있다. 이 훈련 설정은 평균 정확도에서 2점 이득으로 대부분의 VLM 벤치마크에서 모델 성능의 포괄적인 향상을 가능하게 한다는 것을 알 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c c c c c c} \\hline \\hline \\multicolumn{2}{c|}{D T} & \\multicolumn{1}{c}{GQA} & \\multicolumn{1}{c}{SQA\\({}^{\\text{1}}\\)} & \\multicolumn{1}{c}{VQA\\({}^{\\text{T}}\\)} & \\multicolumn{1}{c}{POPE} & \\multicolumn{1}{c}{MME\\({}^{\\text{P}}\\)} & \\multicolumn{1}{c}{MMB\\({}^{\\text{dev}}\\)} & \\multicolumn{1}{c}{Avg.} \\\\ \\hline (a) & ✗ & ✗ & 56.1 & 57.3 & 41.5 & 84.5 & 1196.2 & 53.2 & 58.7 \\\\ (b) & ✓ & ✗ & 57.5 & 63.9 & 49.8 & 83.9 & 1157.5 & 51.6 & 60.8 \\\\ \\hline (c) & ✓ & ✓ & 58.5 & 65.4 & 50.8 & 83.4 & 1262.6 & 55.4 & 62.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: MobileVLM 1.7B에 대한 데이터 전략 및 훈련 전략의 절제 결과. \'D\'와 \'T\'는 각각 우리의 데이터 전략과 훈련 전략을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c} \\hline \\hline Model & \\begin{tabular}{c} Avg. \\\\ accuracy (tokens/s) \\\\ \\end{tabular} & \\begin{tabular}{c} \\(Eval_{avg}\\) \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} \\(Total\\) (s) \\\\ \\end{tabular} \\\\ \\hline ShareGPT4V-7B & 70.8 & 13.00 & 19.69 \\\\ LLaVA-1.5 7B & 68.8 & 12.96 & 19.75 \\\\ MobileVLM V2 7B & **72.1** & 15.49 & 16.53 \\\\ LLaVA-1.5 3.3B & 62.7 & 20.45 & 12.52 \\\\ MobileVLM 3B & 62.8 & 30.80 & 8.31 \\\\ MobileVLM V2 3B & 68.1 & 30.80 & 8.38 \\\\ LLaVA-1.5 1.4B & 55.7 & 43.39 & 5.90 \\\\ MobileVLM 1.7B & 58.7 & 49.80 & 5.14 \\\\ MobileVLM V2 1.7B & 64.2 & **51.63** & **4.96** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: NVIDIA Jetson Orin에 대한 모바일 규모 VLM의 잠복기 비교. VLM의 언어 모델은 l1ama.cpp를 사용하여 4비트로 양자화된다. 평균 정확도는 표 4와 동일한 6개의 벤치마크에서 평가된다.\n' +
      '\n' +
      '프로젝터의###효과\n' +
      '\n' +
      'MobileVLM을 돌아보면, 제안된 LDPv1은 거의 동등한 성능을 유지하면서 토큰의 수를 75%(576 \\(\\rightarrow\\) 144) 줄이며, 표 7의 첫 번째 행과 두 번째 행을 참조하라. 새롭게 제안된 데이터 구성 및 훈련 전략을 기반으로 위의 해당 아키텍처는 각각 \\(5.4\\) 및 \\(4.5\\) 평균 개선(세 번째 행 및 네 번째 행)을 달성할 수 있다. 그러나 시각 및 언어 특징의 더 나은 정렬 방법을 계속 탐색하려고 할 때 학습 가능한 매개변수의 수가 증가하면 최적화 문제가 쉽게 발생하여 전체 정렬 효과가 감소할 수 있음을 관찰한다. 이를 바탕으로 먼저 LDPv1의 \\([DW^{\\kappa}PW]\\) 블록을 당혹스러울 정도로 간단한 파라미터가 없는 동작인 _i.e_., \\(2\\times 2\\) 평균 풀링으로 교체하고, 평균 0.4 포인트(표 7의 5행 참조)의 성능 향상을 얻을 수 있음을 발견했다. 둘째, 보다 강한 위치 정보를 갖는 정렬된 시각적 특징을 만들기 위해, 가장 간단한 아이디어는 6번째 행에서 \\(0.5\\)의 성능 향상을 가져오는 학습 가능한 위치 인코딩을 사용하는 것이다. 또한, 위치 인코딩이 입력 토큰들의 로컬 이웃에 동적으로 생성되고 컨디셔닝되는 PEG [16]의 청구항들에 의해 영감을 받아, 우리는 더 향상된 특징들을 얻기 위해 학습가능한 PE를 PEG 층으로 교체한다. 표 7의 일곱 번째 행은 0.5점의 개선을 나타낸다. MobileVLM V2 1.7B에 적용된 PEG 층은 \\(0.02M\\)(\\(2048\\times 3\\times 3\\)) 매개변수만 포함한다는 점은 언급할 가치가 있다. 모바일VLM(MobileVLM)의 \\([DW^{\\kappa}PW]\\) 블록과 비교하여 학습 가능한 파라미터 수는 거의 \\(630\\times\\)(\\(12.64M\\to 0.02M\\)) 감소했지만, 전체 성능은 1.4점 향상되어 본 설계의 유효성을 입증한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '간단히 말해서, 우리는 MobileVLM 기반의 MobileVLM V2라는 일련의 효율적인 비전 언어 모델을 제안한다[15]. 우리는 작은 VLM 모델의 설정 하에서 전체 성능을 향상시키기 위해 데이터 스케일링 방식, 개선된 훈련 전략 및 효율적인 모달리티 정렬 설계를 탐구한다. [44]와 같은 유사한 훈련 비용으로, 본 방법은 실제 제품 환경을 목표로 하는 정확도와 지연 시간 측면에서 새로운 최첨단 파레토 전선을 달성한다. 우리의 모델은 자원이 제한된 시나리오에서 고급 AI를 즐길 수 있는 유망한 방법을 제공하는 상당한 추론 이점을 가진 더 큰 모델보다 우수하다.\n' +
      '\n' +
      '인정: 이 작업은 부분적으로 중국 국가 핵심 R&D 프로그램(No. 2022ZD0118-700)에 의해 지원되었다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, Angel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, and Graham Neubig. An in-depth look at gemini\'s language abilities. _arXiv preprint arXiv:2312.11444_, 2023.\n' +
      '* [2] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, Mar. 2023.\n' +
      '* [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Owen technical report. _arXiv preprint arXiv:2309.16609_, 2023.\n' +
      '* [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. _arXiv preprint arXiv:2308.12966_, 2023.\n' +
      '* [5] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\'Brien, Eric Hallahan, Mohammad Afiah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyz\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c c c c|c} \\multicolumn{12}{l}{VL Projector Architecture Design} & w/o D\\&T & Tokens & Params. & GQA & SQA\\({}^{1}\\) & VQA\\({}^{\\rm T}\\) & POPE & MME & MMBd\\({}^{\\rm adv}\\) & Avg. \\\\ \\hline \\([PW]_{\\times 2}\\) & ✗ & 576 & 6.30M & 56.9 & 57.1 & 43.7 & 85.7 & 1137.7 & 52.8 & 58.8 \\\\ \\([PW]_{\\times 2}[DW^{\\kappa=1}PW]_{\\times 1}[DW^{\\kappa=2}PW]_{\\times 1}\\) & ✗ & 144 & 18.94M & 56.1 & 57.3 & 41.5 & 84.5 & 1196.2 & 53.2 & 58.7 \\\\ \\hline \\([PW]_{\\times 2}\\) & ✓ & 576 & 6.30M & 59.9 & 63.7 & 53.9 & 85.0 & 1271.3 & 56.0 & 63.7 \\\\ \\([PW]_{\\times 2}[DW^{\\kappa=1}PW]_{\\times 1}[DW^{\\kappa=2}PW]_{\\times 1}\\) & ✓ & 144 & 18.94M & 58.5 & 65.4 & 50.8 & 83.4 & 1262.6 & 55.4 & 62.8 \\\\ \\([PW]_{\\times 2}[AVgPool^{\\rho=2}]_{\\times 1}\\) & ✓ & 144 & 6.30M & 59.3 & 65.0 & 53.1 & 84.0 & 1292.2 & 54.5 & 63.2 \\\\ \\([PW]_{\\times 2}[AVgPool^{\\rho=2}]_{\\times 1}[LearnablePE]_{\\times 1}\\) & ✓ & 144 & 6.59M & 59.1 & 67.1 & 52.3 & 84.3 & 1286.7 & 55.5 & 63.7 \\\\ \\([\\overline{PW}]_{\\times 2}[AVgPool^{\\rho=2}]_{\\times 1}[DW^{\\kappa=1}]_{\\times 1}\\) & ✓ & 144 & 6.32M & 59.3 & 66.7 & 52.1 & 84.3 & 1302.8 & 57.7 & 64.2 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: MobileLLaMA 1.4B를 기반으로 한 프로젝터 설계의 탐사. 상기 \\(PW\\)는 점방향-\\(conv\\)을 나타내고, 상기 \\(DW\\)는 깊이방향-\\(conv\\)을 나타낸다. 첨자\\(\\times\\)는 해당 모듈이 반복적으로 적층되는 횟수를 나타낸다. 위 첨자 \\(\\kappa\\)는 \\(3\\times 3\\)\\(conv\\) 보폭을 나타내고 \\(\\rho\\)은 풀링 커널 크기를 나타낸다. D 는 MobileVLM V2에서 제안된 데이터 전략과 훈련 전략을 위한 T를 나타낸다. 녹색 행은 MobileVLM V2에서 제안된 LDPv2이다.\n' +
      '\n' +
      '교육 및 스케일링을 통해 대규모 언어 모델을 구축하는 중입니다. _International Conference on Machine Learning_, pages 2397-2430. PMLR, 2023.\n' +
      '* [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [7] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Locality-enhanced projector for multimodal llm. _arXiv preprint arXiv:2312.06742_, 2023.\n' +
      '* [8] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. _arXiv preprint arXiv:2310.09478_, 2023.\n' +
      '* [9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm\'s referential dialogue magic. _arXiv preprint arXiv:2306.15195_, 2023.\n' +
      '* [10] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.\n' +
      '* [11] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server, 2015.\n' +
      '* [12] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In _International Conference on Machine Learning_, pages 1931-1942. PMLR, 2021.\n' +
      '* [13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n' +
      '* [14] Xiangxiang Chu, Liang Li, and Bo Zhang. Make repvgg greater again: A quantization-aware approach. In _AAAI_, 2024.\n' +
      '* [15] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices. _arXiv preprint arXiv:2312.16886_, 2023.\n' +
      '* [16] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and Chunhua Shen. Conditional positional encodings for vision transformers. In _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* [17] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\n' +
      '* [18] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructtblip: Towards general-purpose vision-language models with instruction tuning. _arXiv preprint arXiv:2305.06500_, 2023.\n' +
      '* [19] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 326-335, 2017.\n' +
      '* [20] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. _arXiv preprint arXiv:2401.16420_, 2024.\n' +
      '* [21] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335, 2022.\n' +
      '* [22] David Eigen, Marc\'Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of experts. 2013.\n' +
      '* [23] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023.\n' +
      '* [24] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. _arXiv preprint arXiv:2210.17323_, 2022.\n' +
      '* [25] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023.\n' +
      '* [26] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang, Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang Shen, Zhang Mengdan, Peixian Chen, Sirui Zhao, Shaohui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hongsheng Li, and Xing Sun. A challenger to gpt-4v? early explorations of gemini in visual expertise. _arXiv preprint arXiv:2312.12436_, 2023.\n' +
      '* [27] Georgi Gerganov. Ilama.cpp. [https://github.com/gerganov/llama.cpp](https://github.com/gerganov/llama.cpp). [Accessed: 2023-11-07].\n' +
      '* [28] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.\n' +
      '* [29] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6700-6709, 2019.\n' +
      '* [30] Robert A Jacobs, Michael I Jordan, Stuart J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. _Neural Computation_, 3(1):79-87, 1991.\n' +
      '* [31] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. Accessed: 2023-03-01.\n' +
      '\n' +
      '* [32] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal generation. _arXiv preprint arXiv:2301.13823_, 2023.\n' +
      '* [33] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. _arXiv preprint arXiv:2308.00692_, 2023.\n' +
      '* [34] Hugo Laurereno, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelisc: An open web-scale filtered dataset of interleaved image-text documents. _arXiv preprint arXiv:2306.16527_, 2023.\n' +
      '* [35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [36] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. _Advances in neural information processing systems_, 34:9694-9705, 2021.\n' +
      '* [37] Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm tweaking: High-performance low-bit quantization of large language models. In _AAAI_, 2024.\n' +
      '* [38] Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yifan Lu, Xiangxiang Chu, Yerui Sun, and Yuchen Xie. A speed odyssey for deployable quantization of llms. _arXiv preprint arXiv:2311.09550_, 2023.\n' +
      '* [39] Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Sunya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report, 2023.\n' +
      '* [40] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. _arXiv preprint arXiv:2305.10355_, 2023.\n' +
      '* [41] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. Moe-llava: Mixture of experts for large vision-language models, 2024.\n' +
      '* [42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In _Eur. Conf. Comput. Vis._, pages 740-755. Springer, 2014.\n' +
      '* [43] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. _Transactions of the Association for Computational Linguistics_, 2023.\n' +
      '* [44] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.\n' +
      '* [45] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv:2310.03744_, 2023.\n' +
      '* [46] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '* [47] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281_, 2023.\n' +
      '* [48] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* [49] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In _Advances in Neural Information Processing Systems_, pages 27730-27744, 2022.\n' +
      '* [50] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. _arXiv preprint arXiv:2110.13214_, 2021.\n' +
      '* [51] OpenAI. ChatGPT. [https://openai.com/blog/ChatGPT/](https://openai.com/blog/ChatGPT/), 2023. Online; accessed 2023-01-01.\n' +
      '* [52] OpenAI. Gpt-4 technical report. 2023. Technical Report.\n' +
      '* [53] OpenAI. Gpt-4v(ision) system card. 2023.\n' +
      '* [54] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned photographs. In _Neural Information Processing Systems (NIPS)_, 2011.\n' +
      '* [55] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* [56] Tianduo Wang Peiyuan Zhang, Guangtao Zeng and Wei Lu. Tinyllama, Sep 2023.\n' +
      '* [57] Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, and Lingpeng Kong Tong Zhang. Detgpt: Detect what you need via reasoning. _arXiv preprint arXiv:2305.14167_, 2023.\n' +
      '* [58] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [59] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.\n' +
      '* [60] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8317-8326, 2019.\n' +
      '* [61] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. _arXiv preprint arXiv:1908.07490_, 2019.\n' +
      '**[62] 로스 테일러, 마르신 카다스, 길렘 쿠쿠럴, 토마스 시알롬, 앤서니 하트쇼른, 엘비스 사라비아, 앤드류 폴튼, 빅토르 커케즈, 로버트 스토즈닉. 갈락티카: 과학을 위한 큰 언어 모델. 2022년\n' +
      '* [63] IntermLM Team. Intermlm: A multilingual language model with progressively enhanced capabilities. [https://github.com/IntermlM/InternmLM](https://github.com/IntermlM/InternmLM), 2023.\n' +
      '* [64] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, et al. Vigc: Visual instruction generation and correction. _arXiv preprint arXiv:2308.12714_, 2023.\n' +
      '* [65] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. _arXiv preprint arXiv:2311.07574_, 2023.\n' +
      '* [66] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. _arXiv preprint arXiv:2208.10442_, 2022.\n' +
      '* [67] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large vision-language models. _arXiv preprint arXiv:2312.06109_, 2023.\n' +
      '* [68] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In _International Conference on Machine Learning_, pages 38087-38099. PMLR, 2023.\n' +
      '* [69] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. _arXiv preprint arXiv:2309.10305_, 2023.\n' +
      '* [70] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. _arXiv preprint arXiv:2304.14178_, 2023.\n' +
      '* [71] Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. Mm-llms: Recent advances in multimodal large language models. _arXiv preprint arXiv:2401.13601_, 2024.\n' +
      '* [72] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. 2022.\n' +
      '* [73] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up visual instruction tuning. _arXiv preprint arXiv:2307.04087_, 2023.\n' +
      '* [74] Sifan Zhou, Liang Li, Xinyu Zhang, Bo Zhang, Shipeng Bai, Miao Sun, Ziyu Zhao, Xiaobo Lu, and Xiangxiang Chu. Lidar-pq:post-training quantization for point cloud 3d object detection. _International Conference on Learning Representations (ICLR 2024)_, 2024.\n' +
      '* [75] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '* [76] Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, and Jian Tang. Llava-\\(\\phi\\): Efficient multi-modal assistant with small language model. _arXiv preprint arXiv:2401.02330_, 2024.\n' +
      '\n' +
      '다양한 데이터 세트의 대화 형식입니다.\n' +
      '\n' +
      '사전 훈련 단계에서 COCO[42], SAM[31], LLaVA-1.5 사전 훈련 데이터[44]를 주로 포함하는 ShareGPT4V의 사전 훈련 단계에서 120만 개의 이미지 텍스트 쌍을 활용했다. 그 후, 다중 과제 훈련 단계에서 240만 개의 고품질 수업 데이터를 수집했다. 이들 데이터의 상세한 포맷은 표 8 및 표 9에 제공된다.\n' +
      '\n' +
      '## MobileVLM V2의 부록 B 예.\n' +
      '\n' +
      '그림 6은 세밀한 속성 이해, 이미지 이해 및 관계 추론, 속성 및 공간 관계 이해, OCR 및 세계 지식, _etc_를 포함한 다양한 시나리오의 멀티모달 대화에서 MobileVLM V2 1.7B의 질적 결과를 보여준다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '도 6: 다양한 장면에서의 MobileVLM V2 1.7B의 예.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>