<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# CyberDemo: Augmenting Simulated Human Dem demonstration\n' +
      '\n' +
      '현실 세계의 익스플로러 조작을 위해\n' +
      '\n' +
      'Jun Wang\\({}^{1}\\)1\n' +
      '\n' +
      'Yuzhe Qin\\({}^{1}\\)1\n' +
      '\n' +
      'Kaiming Kuang\\({}^{1}\\)\n' +
      '\n' +
      'Yigit Korkmaz\\({}^{2}\\)\n' +
      '\n' +
      'Akhilan Gurumoorthy\\({}^{1}\\)\n' +
      '\n' +
      'Hao Su\\({}^{1}\\)\n' +
      '\n' +
      'Xiaolong Wang\\({}^{1}\\)\n' +
      '\n' +
      'UC 샌디에이고\n' +
      '\n' +
      '남부 캘리포니아 주립대학교\n' +
      '\n' +
      '각주 1: 균등 기여.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '우리는 실제 작업에 대해 시뮬레이션된 인간 시연을 활용하는 로봇 모방 학습에 대한 새로운 접근법인 **CyberDemo**를 소개한다. 시뮬레이션 환경에서 광범위한 데이터 증강을 통합함으로써 사이버데모는 다양한 물리적 및 시각적 조건을 처리하면서 실제 세계로 이전될 때 전통적인 도메인 내 실제 세계 시연보다 우수하다. 사이버데모는 데이터 수집의 경제성 및 편의성에 관계없이 다양한 작업에서 성공률 측면에서 기준선 방법을 능가하고 이전에 보이지 않은 객체와 일반화할 수 있다. 예를 들어, 3개 밸브만 포함하는 인간 시연에도 불구하고 새로운 4개 밸브와 5개 밸브를 회전할 수 있다. 우리의 연구는 실제 손재주 있는 조작 작업에 대한 시뮬레이션된 인간 시연의 상당한 잠재력을 보여준다. 보다 자세한 내용은 [https://cyber-demo.github.io/](https://cyber-demo.github.io/]에서 확인할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '모방 학습은 로봇 조작에서 유망한 접근법으로 인간 시연으로부터 복잡한 기술을 습득할 수 있게 했다. 그러나 이 접근법의 효과는 고품질 시연 데이터의 가용성에 크게 의존하며, 이는 종종 데이터 수집을 위한 상당한 인적 노력이 필요하다[6, 9, 48]. 이 도전은 작업의 복잡성과 복잡성이 매우 상세하고 정확한 시연을 필요로 하는 다중 손가락 손재주가 있는 조작의 맥락에서 더욱 증폭된다.\n' +
      '\n' +
      '모방 학습에서는 배치 환경에서 직접 수집한 데이터를 의미하는 도메인 내 시연이 로봇 조작 작업에 일반적으로 사용된다[43]. 일반적으로 특정 작업을 해결하는 가장 효과적인 방법은 해당 작업에 대한 실제 로봇으로부터 직접 시연을 수집하는 것이라고 여겨진다. 이 믿음은 금본위제로 유지되었지만, 우리는 그것에 도전하고 싶다. 모의실험에서 인간의 시연을 수집하면 실제 하드웨어가 필요하지 않고 원격 및 병렬로 실행될 수 있을 뿐만 아니라 시뮬레이터 전용 데이터 증강을 사용하여 최종 작업 성능을 향상시킬 수 있기 때문에 실제 작업에 대해 우수한 결과를 얻을 수 있다고 주장한다[34, 37, 42, 44, 53, 58]. 이를 통해 초기 시연 세트보다 수백 배 더 큰 데이터셋을 생성할 수 있다. 그러나, 기존의 연구들은 생성된 데이터 세트를 사용하여 시뮬레이션 내에서 도메인 내 정책을 훈련시키는 반면, 정책을 실제 세계로 이전하는 sim2real 과제는 해결되지 않은 문제로 남아 있다.\n' +
      '\n' +
      '본 논문에서는 시뮬레이션된 인간 데모를 실제 로봇 조작 작업에 어떻게 활용할 것인가에 대한 문제를 연구한다. 우리는 시뮬레이션된 인간 데모를 활용하여 시각적 관찰로부터 로봇 모방 학습을 위해 설계된 새로운 프레임워크인 **CyberDemo**를 소개한다. 우리는 먼저 시뮬레이션 환경에서 저비용 장치를 사용하여 원격 조작을 통해 적당한 양의 인간 시연 데이터를 수집한다. 그런 다음 사이버데모는 광범위한 데이터 증강을 원래 인간 시연에 통합한다. 증강 세트는 데이터 수집 동안 발생하지 않는 광범위한 시각적 및 물리적 조건을 커버하여 이러한 변화에 대한 훈련된 정책의 견고성을 향상시킨다. 이러한 증강 기술은 또한 다운스트림 sim2real 전송을 염두에 두고 설계되었다. 우리는 증강된 데이터 세트에서 정책을 훈련하기 위해 고유한 커리큘럼 학습 전략을 채택하고 몇 가지 실제 데모(3분 궤적)를 사용하여 세부 조정함으로써 실제 상황으로 효과적으로 이전할 수 있다. 실세계 시연에만 훈련된 정책은 조명 조건, 객체 기하학 및 객체 초기 포즈의 변화로 인해 어려움을 겪을 수 있지만, 우리의 정책은 추가적인 인간의 노력 없이도 이를 처리할 수 있다.\n' +
      '\n' +
      '본 시스템은 원격조작(RealSense 카메라)을 위해 저가의 모션캡처 장치를 사용하고, 최소한의 인간 노력(즉, 30분 데모 궤적)을 요구하는 강력한 모방 학습 정책을 학습할 수 있다. 비용과 최소한의 인력 요구에도 불구하고 사이버 데모는 실제 로봇에서 더 나은 성능을 달성할 수 있습니다. 실제 시연을 위한 R3M[46]과 같은 사전 훈련된 정책들과 비교하였을 때, CyberDemo는 준정적_pick과 place_ 태스크의 경우 \\(35\\%\\), 비준정적_rotate_ 태스크의 경우 \\(20\\%\\) 더 높은 성공률을 보였다. 일반화 테스트에서, 기준선 방법은 테스트 중에 보이지 않는 물체를 다루는데 어려움을 겪는 반면, 본 방법은 인간의 시연이 3개의 밸브(그림 1의 두 번째 행)만을 커버하더라도 \\(42.5\\%\\)의 성공률로 새로운 테트라 밸브와 펜탈베를 회전시킬 수 있다. 우리의 방법은 또한 상당한 광 교란(도 1의 마지막 열)을 관리할 수 있다. 절제 연구에서 시뮬레이터에서 증가된 시연 횟수와 함께 데이터 증강을 사용하면 실제 시연의 동등한 증가에 비해 우수한 성능을 얻을 수 있음을 관찰한다. 추가 연구를 촉진하기 위해 코드 및 인간 시연 데이터 세트를 공개적으로 사용할 수 있도록 할 것이다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**로봇 조작 학습을 위한 데이터** 모방 학습은 로봇 조작에 대한 효과적인 접근법으로 입증되어 시연 모음으로 정책 교육을 가능하게 한다. 많은 연구는 사전 프로그래밍된 정책[17, 23, 31, 33, 84], 언어[30, 66, 67, 69] 및 인간 비디오[5, 47, 54, 63, 64] 또는 광범위한 실제 로봇 원격 조작[2, 3, 6, 9, 20, 32, 39, 43, 48]과 같은 대체 데이터 소스를 사용하여 대규모 데이터 세트를 구축하는 데 중점을 두었다. 그러나 이러한 작업은 주로 표적 병렬 그리퍼로 작동한다. 높은 DoF 능숙한 손에 대한 대규모 시연 데이터 세트를 수집하는 것은 여전히 중요한 과제이다. 한편, 데이터 증강은 데이터 분포의 다양성을 증가시켜 정책 일반화를 개선하는 실행 가능한 전략을 제시한다. 기존 연구들은 컬러 지터, 블러링, 크로핑과 같은 낮은 수준의 시각적 공간[16, 28, 56, 65]에서 증강을 적용한 반면, 최근에는 생성 모델[7, 14, 15, 40, 78, 86]을 사용한 의미 인식 데이터 증강을 제안한다. 그러나 이러한 증강은 이미지 수준에서 작동하며 물리적 현실에 기반을 두지 않는다. CyberDemo는 시각적 및 물리적 변이를 모두 고려하여 물리적 시뮬레이터를 사용하여 데이터 증강을 궤적 수준으로 확장한다. 우리의 작업과 동시에, MimicGen[44]은 다수의 인간 궤적을 통합하여 긴 지평선 작업에 대한 시연을 합성하는 시스템을 제안한다. 그러나, 실제 로봇으로 이전하지 않고 시뮬레이션 데모만으로 시뮬레이션 정책을 훈련하는 것은 도메인 내 학습에 국한된다. 대조적으로, 우리의 작업은 실제 문제 해결을 위한 시뮬레이션을 활용하는 것을 목표로 한다. 로봇 시연을 수집하기 위해 시뮬레이터의 편리성을 이용하고, 이러한 데모를 다중 손가락 휴머노이드 핸드가 장착된 능숙한 로봇으로 전달하기 위해 sim2real 접근법을 사용한다. 우리의 연구는 실제 로봇 조작을 위해 시뮬레이션된 시연을 활용하는 일반적인 프레임워크를 강조한다.\n' +
      '\n' +
      '**로봇공학을 위한 사전 훈련된 시각적 표현** 대규모 자기 지도 학습[10, 26, 27]의 최근 진전은 다운스트림 로봇 작업[62, 80, 83]에 유리한 시각적 표현의 개발을 가능하게 했다. 여러 연구에서 ImageNet[18] 및 Ego4D[22]와 같은 비로봇 데이터 세트에 대한 사전 훈련과 다운스트림 로봇 제어[46, 49, 75]에 정적 표현을 활용하는 데 중점을 두었다. 다른 연구는 로봇 데이터세트에 대한 시각적 표현을 사전 훈련하거나, 액션에 의존하는 액션 감독 자가 학습 목표[60, 64]를 사용하거나, 비디오의 시간적 일관성을 학습 목표[59, 61, 70, 76]로 활용하는 데 중점을 두었다. 이러한 조사는 주로 시각 기반 로봇 조작의 효과적인 훈련을 위한 특징을 학습하는 것을 목표로 했다. 일부 연구자들은 오프라인 데이터 세트에 대한 시각적 표현을 훈련하는 것 외에도 강화 학습에 사용될 보상 함수를 학습하는 것을 탐구했다[4, 36, 41, 45, 82]. 선행 연구와 달리 재현 학습을 위해 자기 지도 학습을 사용하기보다는 시뮬레이션 데이터를 사전 훈련에 활용함으로써 우리의 작업이 분기된다. 이는 이미지 표현들의 학습을 향상시킬 뿐만 아니라, 액션 정보의 사용을 통해 태스크 사전들을 신경망에 통합한다. 시뮬레이션된 환경에서 사전 훈련함으로써, 조작 정책은 새로운 기하학적 구조 및 접촉 패턴을 갖는 새로운 객체에 더 잘 일반화될 수 있다.\n' +
      '\n' +
      '**Sim2Real Transfer**Sim2real transfer로 알려진 시뮬레이션에서 실제 시나리오로 기술을 옮기는 도전은 로봇 학습의 핵심 초점이 되어 왔다. 일부 접근법은 실제 시스템의 수학적 모델을 구축하고 물리적 매개변수를 식별하기 위해 시스템 식별을 사용했다[12, 29, 35, 38, 52, 73]. 실제 동역학을 교정하는 대신, 도메인 랜덤화[50, 71]는 랜덤화된 특성을 갖는 시뮬레이션된 환경을 생성하고 이들 모두에 걸쳐 모델 함수를 훈련시킨다. 후속 연구에서는 무작위화 매개변수의 선택이 자동화될 수 있음을 보여주었다[1, 11, 24, 81]. 그러나 강력한 정책을 학습하기 위한 광범위한 샘플 요구 사항으로 인해 도메인 무작위화는 일반적으로 수백만 개의 상호 작용 샘플을 포함하는 RL과 함께 사용된다. 도메인 적응(DA)은 sim와 실제 사이의 데이터 분포를 정렬하기 위해 개발된 전이 학습 전략 세트를 의미한다. 일반적인 기술에는 도메인 적대적 훈련[21, 72] 및 시뮬레이션된 이미지를 실제 이미지와 유사하게 만들기 위한 생성 모델의 사용이 포함된다[8, 28]. 이러한 DA 접근법의 대부분은 시각적 격차를 줄이는 데 중점을 둔다. 그러나 역학 격차를 해결해야 하는 과제는 여전히 중요하다. 심2리얼 갭은 높은 DoF 작동 및 복잡한 상호 작용 패턴[24, 51, 77, 79]을 갖는 능숙한 로봇 손에 대해 훨씬 더 두드러진다. 본 연구에서는 도메인 랜덤화의 개념을 시뮬레이터에서 수집된 인간 시연으로 확장하고 시뮬레이션을 실제 로봇에 효과적으로 활용할 수 있는 데이터 증강 기술에 초점을 맞춘다. 우리는 실제 데이터에만 의존하는 대신 sim2real 격차에도 불구하고 시뮬레이터에서 인간 시연을 수집하는 데 상당한 이점이 있을 수 있음을 보여준다.\n' +
      '\n' +
      '## 3 CyberDemo\n' +
      '\n' +
      '사이버 데모에서 우리는 처음에 원격 조작을 통해 시뮬레이터에서 동일한 작업에 대한 인간 시연을 수집한다(섹션 3.1). 시뮬레이터의 샘플링 능력과 오라클 상태 정보를 활용하여 다양한 방법으로 시뮬레이션 시연을 향상시켜 vi를 증가시킨다.\n' +
      '\n' +
      '그림 2: **CyberDemo Pipeline.** 먼저 비전 기반 원격조작을 통해 시뮬레이션 및 실제 시연을 모두 수집합니다. 이 후, 제안된 데이터 증강 기술을 통합하여 시뮬레이션 데이터에 대한 정책을 훈련한다. 교육 과정에서 과업 수행에 따라 무작위성 척도를 점진적으로 향상시키는 자동 커리큘럼 학습을 적용한다. 마지막으로, 정책은 실제 세계에 배치되기 전에 몇 가지 실제 데모로 미세 조정됩니다.\n' +
      '\n' +
      '수동, 운동학적 및 기하학적 다양성을 통해 시뮬레이션된 데이터 세트를 풍부하게 한다(섹션 3.2). 이 증강 데이터 세트를 사용하여 자동 커리큘럼 학습 및 액션 집계(섹션 3.3)를 사용하여 조작 정책을 훈련한다.\n' +
      '\n' +
      '### 인간 원격 조작 데이터 수집\n' +
      '\n' +
      '이 작업의 각 능숙한 조작 작업에 대해 시뮬레이션된 환경과 실제 환경 모두에서 원격 조작을 사용하여 인간 시연을 수집한다. 실제 데이터의 경우 [55]에서 참조한 저비용 원격 운영 시스템을 활용한다. 이 비전 기반 원격 조작 시스템은 인간의 손 동작을 입력으로 캡처하기 위해 카메라만 필요하며, 이는 로봇 팔과 손재주가 있는 손에 대한 실시간 모터 명령으로 변환된다. 각 프레임에 대해 30Hz의 속도로 관찰(RGB 영상, 로봇 고유 감각)과 동작(로봇 엔드 이펙터의 6D 직교 속도, 손가락 관절 위치 제어 대상)을 기록한다. 이 작업을 위해 실제 로봇의 각 작업에 대해 3분의 로봇 궤적만 수집한다.\n' +
      '\n' +
      '시뮬레이션의 데이터는 실제 시나리오에서 사용되는 테이블과 객체를 복제하기 위해 SAPIEN[74] 시뮬레이터 내에서 실제 작업 환경을 구축한다. 원격조작을 위해서는 강화학습 설정과 같이 보상설계와 관찰공간의 필요성이 없어 시뮬레이터에서 새로운 작업을 설정하는 과정이 비교적 간단하다는 점에 주목할 필요가 있다. 우리는 시뮬레이터에서 인간 시연을 수집하기 위해 동일한 원격 조작 시스템[55]을 사용한다.\n' +
      '\n' +
      '시뮬레이터에서의 인간 데모 증강\n' +
      '\n' +
      '카메라 RGB 이미지, 로봇 고유 감각과 같은 물리적 센서의 관찰을 기록하는 데 국한된 실제 데이터 수집과 달리 시뮬레이션 시스템은 가상 환경 내에서 지상-진실 상태 및 연락처 정보를 기록할 수 있게 한다. 시뮬레이션의 이러한 독특한 이점은 실제 사례에 비해 시뮬레이션된 데모를 위한 보다 포괄적인 데이터 형식을 제공한다. 따라서, 실제 데이터로 실현 가능하지 않은 이러한 시뮬레이션된 데모에 대한 데모 리플레이 기술을 활용할 수 있다.\n' +
      '\n' +
      '시뮬레이터에서 데이터 증강 기술을 개발할 때, 훈련된 정책을 실제 로봇에 배치하는 것이 궁극적인 목표라는 것을 명심하는 것이 필수적이다. 따라서 증강은 실제 세계에서 마주칠 가능성이 있는 시각적 및 동적 변화에 초점을 맞춰야 한다. 또한, 데이터 수집 과정에서 발생하지 않는 새로운 객체들로 일반화하기 위한 조작 정책을 목표로 한다. 예를 들어, 그림 3의 트리플 밸브에서만 데이터를 수집할 때 테트라 밸브를 조작하는 것이다. 구체적으로, 우리는 시각적 변화에 대한 정책의 견고성을 향상시키기 위해 조명 조건, 카메라 뷰 및 객체 텍스처를 강화하기로 결정했다. 또한 동적 변화에 대한 정책의 견고성을 향상시키기 위해 객체의 기하학적 모양과 로봇 및 객체의 초기 자세를 다음과 같이 수정했다.\n' +
      '\n' +
      '카메라 뷰를 랜덤화합니다. 시뮬레이션과 현실 사이에는 말할 것도 없고 데모 컬렉션과 최종 평가 사이에 카메라 뷰를 정확하게 정렬하는 것은 중요한 문제를 제기합니다. 이 문제를 해결하기 위해 훈련 중 카메라 포즈를 랜덤화하고 시뮬레이터의 내부 상태를 다시 재생하여 새로운 카메라 뷰에서 이미지 시퀀스를 렌더링한다. 자르기 및 이동과 같은 표준 이미지 증강 기술과 달리, 본 방법은 물리적으로 사실적인 방식으로 원근 투영을 존중한다.\n' +
      '\n' +
      '랜덤 라이트 및 텍스쳐.Sim2real 전송을 용이하게 하고 시각적 변화에 대한 정책의 견고성을 개선하기 위해 조명과 객체 모두의 시각적 속성을 랜덤화한다(그림 3, 오른쪽 하단). 빛 특성에는 방향, 색상, 그림자 특성 및 주변 조명이 포함됩니다. 물체 특성은 경면성, 거칠기, 금속성 및 질감을 포함한다. 카메라 뷰 랜덤화와 유사하게 시뮬레이션 상태를 다시 재생하여 새로운 이미지 시퀀스를 렌더링할 수 있다.\n' +
      '\n' +
      '다양한 객체를 추가합니다.이 접근법에서 우리는 원래 데모에서 조작된 객체를 새로운 객체로 대체합니다(오른쪽 상단 그림 3). 그러나, 동일한 궤적을 직접 재생하는 것은 물체 모양이 다르기 때문에 작동하지 않을 것이다. 대신, 가우시안 잡음으로 원래의 데모로부터 동작 시퀀스를 섭동하여 새로운 궤적을 생성한다. 이러한 궤적은 합리적인 조작 전략을 제공하지만 원래 궤적과는 약간 다르다. 시뮬레이터에서 매우 비용 효율적인 샘플링을 통해 섭동이 성공할 때까지 섭동을 열거할 수 있다. 이 기술은 실제 시연으로 실현 가능하다는 점에 유의하는 것이 중요하다.\n' +
      '\n' +
      '도 3: **Data Augmentation**. 데이터 집합 증강은 (a) 랜덤 카메라 뷰, (b) 다양한 객체, (c) 랜덤 객체 포즈, (d) 랜덤 조명 및 텍스처의 네 가지 차원을 포함한다.\n' +
      '\n' +
      '**무작위 객체 포즈.** 일반화 가능성을 강화하기 위한 강화 학습의 일반적인 접근법은 재설정 동안 객체 포즈를 무작위화하는 것을 포함한다. 그러나 유사한 결과를 얻기 위해 모방 학습 데이터를 증강하는 것은 직관적이지 않다. 프레임(A\\)에 대한 프레임(B\\)의 포즈로서 디노트(T_{A}^{B}\\in SE(3)\\)를 사용한다. 오리지널 오브젝트 포즈는 \\(T_{W}^{O_{gld}}\\), 새롭게 랜덤화된 오브젝트 포즈는 \\(T_{W}^{O_{new}}\\), 오리지널 엔드 이펙터 포즈는 \\(T_{W}^{R_{gld}}\\)이다. 목적은 물체 자세 변화(T_{W}^{O_{new}}(T_{W}^{O_{gld}})^{-1}\\)를 처리하는 것이다. 간단한 전략은 먼저 로봇 엔드 이펙터를 새로운 초기 포즈인 \\(T_{W}^{R_{new}}=T_{W}^{O_{new}}(T_{W}^{O_{gld}})^{-1}T_{W}^{R_{gld}}\\으로 이동시키는 것이다. 그런 다음 로봇과 물체 사이의 상대적 포즈가 원래 시연과 일치하여 작업을 수행하기 위해 동일한 액션 시퀀스를 다시 재생할 수 있다. 이 방법은 새로운 궤적을 생성하는 데 성공하지만 다운스트림 모방 학습을 위한 최소한의 지원을 제공한다. 새로운 궤적은 항상 새로운 엔드 이펙터 포즈에 대한 계산된 도달 궤적(T_{W}^{R_{new}}\\)과 원래 궤적의 두 부분으로 구성된다. 상이한 증강 궤적이 종종 중복성의 상당한 부분을 공유한다는 점을 감안할 때, 이들은 학습 알고리즘에 실질적인 새로운 정보를 제공하지 못한다.\n' +
      '\n' +
      '이를 해결하기 위해, 우리는 인간 시연에 대한 객체 포즈를 무작위화하는 **감도 인식 운동학 증강**을 제안한다. 이 방법은 원래 궤적보다 먼저 새로운 궤적을 추가하는 대신, 원래 데모에서 각 단계에 대한 동작을 수정하여 객체 포즈의 변화를 수용한다. 방법은 두 단계를 포함한다: (i) 전체 궤적을 여러 세그먼트로 나누고 각 세그먼트의 감도를 계산함; (ii) 새로운 동작을 계산하기 위해 감도에 기초하여 엔드 이펙터 포즈 궤적을 수정함.\n' +
      '\n' +
      '(i) **Trajectory Segments에 대한 민감도 분석.** 민감도는 액션 노이즈에 대한 견고성에 관한 것이다. 예를 들어, 파지 전 상태는 손이 물체에 가까이 있는 경우 손이 멀리 있는 상태에 비해 감도가 높다. 중요한 통찰은 물체 자세 변화(\\(\\Delta T=T_{W}^{O_{new}}(T_{W}^{O_{gld}})^{-1}\\)를 처리하기 위해 민감도가 낮은 상태들의 동작을 수정하는 것이 더 간단하다는 것이다. 궤적 세그먼트 \\(\\psi\\)의 강건성(감도의 곱셈 역)은 다음과 같이 수학적으로 정의될 수 있다:\n' +
      '\n' +
      '\\exp(\\max\\delta_{seg}&=\\exp(\\begin{split}\\psi_{seg}&)\\quad\\textbf{eval}(\\tau^{\\prime})=1\\\\tau^{\\prime}&=\\{a_{1},a_{2},...,a_{n}^{\\prime},...,a_{n+K-1}^{\\prime}=a_{i}+\\delta_{i}\\sim\\mathcal{N}(0,1}\\delta_{i}},...,a_{n+K-1}^{\\prime}=a_{i}+\\delta_{i},...,a_{n+K-1}^{\\prime}=a_{i}+\\delta_{i}\\sim\\mathcal{N}(0,1}\\delta_{i}},...,a_{n+K-1}^{\\prime}=a_{i}+\\\n' +
      '\n' +
      '이 방정식에서 우리는 길이가 \\(N\\)인 원래의 행동궤적 \\(\\tau\\)을 \\(M\\)의 세그먼트로 나누고, 각 세그먼트는 크기가 \\(K=N/M\\)이다. 그 다음, 스케일 \\(\\delta_{a}\\)의 가우시안 잡음을 원래의 동작 \\(\\{a_{m},a_{n+1},...,a_{n+k-1}\\}\\)에 더하여 세그먼트 \\(seg\\)내의 동작을 섭동시키고, 이 세그먼트 밖의 모든 동작을 그대로 유지하여 섭동된 궤적 \\(\\tau^{\\prime}\\)을 생성한다. 우리는 액션 공간이 이미 \\([-1,1]\\)으로 정규화되어 있다고 가정하고, **eval**는 액션 궤적이 작업을 성공적으로 해결할 수 있는지 여부를 나타내는 이진 함수이다. 직관적으로, 더 작은 섭동이 실패하게 할 수 있다면, 시연 세그먼트는 더 민감하다. 이 감도는 새로운 객체 포즈를 처리하기 위해 동작을 조정하는 방법에 대해 안내합니다. 실제로, 태스크가 \\(\\max\\delta_{a}\\)을 결정하지 못할 때까지 원래의 동작 궤적에 적용되는 잡음 스케일 \\(\\delta_{a}\\)을 점진적으로 증가시킨다.\n' +
      '\n' +
      '(ii) **New End Effector Pose Trajectory.** 새로운 객체 포즈를 수용하기 위해, 엔드 이펙터의 총 포즈 변화는 객체 포즈 \\(\\Delta T\\)의 변화와 동일해야 한다. 각 작업은 이 변화에 작은 부분을 기여합니다. 감도에 따라 각 단계에 이 "작업"을 배포합니다.\n' +
      '\n' +
      '{split}\\overline{\\psi}_{seg_{j}}&=\\frac{\\psi_{seg_{j}}{\\sum_{j}\\psi_{m}\\psi_{j}},\\quad\\forall seg_{j}\\overline{\\psi}_{seg_{j}}\\log(\\Delta T)/K\\a_{i}^{new}&=a_{i}\\tag{2}\\\n' +
      '\n' +
      '이 방정식에서 \\(\\overline{\\psi}_{seg_{j}}\\)는 정규화된 강건성이고, \\(\\Delta T_{j}\\)은 각 단계에 대한 포즈 수정을 나타내며, 동일한 세그먼트 내의 모든 상태는 새로운 액션 \\(a_{i}^{new}\\)을 계산하기 위한 동일한 양의 수정 "작업"을 담당한다. \\(\\delta T_{j}\\) (f_{i}\\)는 세계 프레임에서 현재 엔드 이펙터 프레임으로 움직임을 변환하는 \\(SE(3)\\) 공간에서의 유사성 변환이다. 직관적으로, 더 높은 견고성을 갖는 세그먼트들은 더 큰 변화들로 태스크된다.\n' +
      '\n' +
      '위에서 논의한 모든 동작은 엔드 이펙터의 6D 델타 포즈에만 해당하며 손재주 있는 손의 손가락 동작을 포함하지 않는다는 점에 유의하십시오. 또한 타겟 포즈(예를 들어, 픽앤플레이스에서의 플레이트 포즈 또는 붓기에서의 보울포즈)를 수반하는 픽앤플레이스 또는 붓기와 같은 작업의 경우, 타겟 포즈에 동일한 증강 전략을 적용할 수 있다(도 2의 레벨 3에 예시된 바와 같이).\n' +
      '\n' +
      '## 3.3 학습 Sim2Real 정책\n' +
      '\n' +
      '증강 시뮬레이션 데이터 세트가 주어지면, 우리는 로봇의 동작을 예측하기 위해 이미지 및 로봇 고유 감각을 입력으로 하는 시각적 조작 정책을 훈련한다. 인간 원격 조작 시연에서 로봇 움직임은 모라비안이나 시간적으로 상관 관계가 없다. 이 문제를 해결하기 위해, 우리의 정책은 액션 춘킹과 트랜스포머(ACT)를 사용하여 단계별 액션이 아닌 액션 청크를 예측하도록 훈련된다[85]. 이 접근법은 더 부드러운 궤적을 생성하고 합성 오류를 줄인다.\n' +
      '\n' +
      '다양한 시각적 및 동적 조건을 수용할 수 있는 데이터 증강 능력에도 불구하고 로봇 컨트롤러에는 심2real 격차가 남아 있다. 이 간격은 엔드 이펙터가 높은 DoF 다중 손가락 손재주가 있는 작업에서 더 어려워집니다. 이 제어기 갭은 그림 1의 두 번째 행과 같이 밸브를 회전시키는 것과 같은 비준정적 작업에 상당한 영향을 미칠 수 있다. 이 갭을 닫기 위해, 우리는 작은 세트의 실세계 시연(3분 궤적)을 사용하여 네트워크를 미세 조정한다. 그러나 시뮬레이션과 현실 사이의 인간 데모 데이터 수집 패턴의 불일치로 인해 실제 데이터를 직접 미세 조정하면 과적합 위험이 있다. 더 원활한 심2real 전송을 보장하기 위해 후속적으로 논의되는 몇 가지 기술을 사용한다.\n' +
      '\n' +
      '**Automatic Curriculum Learning.** Curriculum Learning과 Data Augmentation 기법을 함께 사용하여 보다 원활한 교육 과정을 제공하는 경우가 많다. 선행 강화 학습 작업 [1, 24]의 교육과정 설계 정신을 따라 우리의 모방 학습 맥락에 적용할 수 있는 교육과정 학습 전략을 고안한다. 훈련 전에 3.2절의 증강을 그림 2와 같이 증가하는 복잡성의 4단계로 그룹화한다. 알고리즘 1에 따라 가장 단순한 수준인 \\(L=0\\)에서 훈련을 시작하고 증강이 없음을 의미한 다음 몇 단계의 훈련 후 작업 성공률을 평가한다. 평가 난이도는 현재 수준인 \\(L\\)에 부합한다. 성공률이 미리 정의된 임계값을 초과할 때, 우리는 더 큰 확대와 더 어려운 평가를 가져오는 다음 단계로 나아간다. 성공률이 임계값에 도달하지 못할 경우, 추가 증강 훈련 데이터를 생성하고 현재 수준에 머무른다. 우리는 모든 단계가 완료될 때까지 이 반복적인 과정을 계속한다. 무궁무진한 훈련을 방지하기 위해 페일세이프 \\(N_{max}\\)을 도입한다. 이 커리큘럼 학습 접근법은 적절한 수준의 무작위화를 가진 훈련 데이터를 동적으로 생성하기 위해 데이터 증강 기술에 크게 의존한다. 이 개념은 데이터가 훈련 전에 미리 설정된 일반적인 지도 학습 시나리오와 대조된다. 이러한 주문형 데이터 생성 및 사용자 지정은 실제 시연보다 시뮬레이션 데이터의 이점을 강조한다.\n' +
      '\n' +
      '**작은 움직임을 위한 액션 집합** 인간 시연은 종종, 특히 손재주가 있는 조작 중에 소음을 포함한다. 예를 들어, 경미한 흔들림 및 의도하지 않은 중단이 시연 궤적 내에서 발생하여 잠재적으로 훈련 프로세스를 손상시킬 수 있다. 이를 해결하기 위해, 우리는 작은 움직임으로 특징지어지는 단계를 집계하고, 이러한 동작을 하나의 동작으로 병합한다. 실제로, 주어진 모션이 작은 것으로 적합한지 여부를 식별하기 위해 엔드 이펙터와 손가락 모션 모두에 대한 임계값을 설정한다. 집계 과정을 통해 인간의 행동에서 작은 동작 잡음을 제거할 수 있어, 모방 학습 정책이 상태-행동 궤적에서 의미 있는 정보를 추출할 수 있다.\n' +
      '\n' +
      '##4 실험 설정\n' +
      '\n' +
      '우리의 실험 설계는 다음과 같은 주요 질의를 해결하는 것을 목표로 한다:\n' +
      '\n' +
      '(i) 시뮬레이션 기반 데이터 증강은 강건성 및 일반화 가능성 모두의 관점에서 실제 시연으로부터의 학습과 어떻게 비교되는가?\n' +
      '\n' +
      '(ii) 우리의 자동 교육과정 학습이 어떻게 개선된 정책 학습에 기여하는가?\n' +
      '\n' +
      '(iii) 실세계 로봇에 대한 효과적인 정책을 훈련시키기 위해 시뮬레이션된 데이터와 실제 데이터 사이의 이상적인 비율은 얼마인가?\n' +
      '\n' +
      '#exterous Manipulation Task\n' +
      '\n' +
      '두 개의 준정적 작업(픽 앤 플레이스, 푸어)과 하나의 비준정적 작업(회전)을 포함하여 실제 환경과 시뮬레이션 환경에서 세 가지 유형의 조작 작업을 설계했다. 실험을 위해 XArm6에 부착된 Allegro hand를 사용하였으며, 동작공간은 로봇팔의 6딤 델타 엔드 이펙터 자세와 손손의 16딤 손가락 관절 위치로 구성되어 있으며, 팔과 손 모두에 PD 제어를 사용하였다.\n' +
      '\n' +
      'Pick and Place.__Pick and Place. 이 작업은 로봇이 테이블에서 물체를 들어 올려 플레이트(도 1의 첫 번째 행)에 배치해야 합니다. 대상물이 빨간색 플레이트 위에 올바르게 배치될 때 성공이 이루어집니다. 우리는 데이터를 수집하고 여러 다른 객체에 대해 테스트하는 동안 두 개의 객체를 선택한다.\n' +
      '\n' +
      '_Rotate.__ 이 작업을 수행하려면 로봇이 테이블(도 1의 두 번째 행)에서 밸브를 회전해야 합니다. 밸브는 고정 베이스와 회전 조인트(revololute joint)를 통해 연결된 이동 밸브 형상으로 구성된다. 로봇이 밸브를 720도까지 회전시키면 작업이 성공합니다. 데이터 수집에 트리 밸브를 사용하고 테트라 밸브와 펜타 밸브에 대해 테스트한다.\n' +
      '\n' +
      '부어.__Pour.__ 이 작업은 로봇이 병에서 작은 상자를 그릇에 부어야 한다(도 1의 세 번째 줄). 그것은 세 단계를 수반한다: (i) 병을 들어올리고; (ii) 그릇 가까이 이동시키고; (iii) 작은 상자를 그릇 안으로 분배하기 위해 병을 회전시킨다. 4개의 상자를 모두 그릇에 부었을 때 성공이 이루어집니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '데이터 증강이 강화된 현실 세계 설정과 4가지 수준 모두에 대해 훈련된 정책은 모든 메트릭에서 탁월합니다. 흥미롭게도, 이 정책은 훈련 데이터에 더 많은 무작위성이 도입될 때 시뮬레이션에서도 더 간단한 설정을 더 효과적으로 해결하도록 관리한다. 이러한 실험은 시뮬레이터 기반 데이터 증강의 중요성을 강조한다.\n' +
      '\n' +
      '자동교과 학습에서의### 어블레이션\n' +
      '\n' +
      '본 실험에서는 시뮬레이션에서 200회 테스트하고 20회의 실제 테스트를 수행하여 정책의 효과를 평가한다. 표 3에서 보는 바와 같이 데이터 생성률만을 기반으로 자동 영역 무작위화를 적용한 교육과정 학습을 사용하면 모델 성과에 기반한 접근법에 비해 낮은 결과를 얻을 수 있다.\n' +
      '\n' +
      '심데모스와 실데모스의 비율에 대한###절제\n' +
      '\n' +
      '시뮬레이션과 실제 데모 간의 최적의 비율을 결정하기 위해 표 4와 같이 시뮬레이션과 실제 데모를 서로 다른 조합을 사용하여 테스트를 수행했으며, 50개의 실제 데모에서만 훈련하면 이미지의 시각적 정보를 활용하기보다는 공동 위치에 대한 정책 적합성이 떨어지는 것으로 관찰된다. 시뮬레이션 시연 15개와 실제 시연 35개를 조합하여 가장 좋은 결과를 얻었다. 이러한 결과는 시뮬레이션 데이터를 수집하는 것이 훨씬 더 낮은 데이터 수집 비용을 고려할 때 예외적으로 가치가 있을 수 있음을 강조한다.\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      '우리는 시뮬레이션에서 수집된 시연을 활용하여 로봇 조작에서 모방 학습을 위한 새로운 파이프라인인 사이버데모를 제안한다. 일반적인 믿음은 실제 시연이 실제 문제를 해결하는 최적의 방법임을 시사하지만, 광범위한 데이터 증강이 실제 시연보다 시뮬레이션 데이터를 훨씬 더 가치 있게 만들 수 있음을 입증함으로써 이 개념에 도전한다. 한 가지 한계는 각 실제 작업에 대해 시뮬레이션된 환경을 설계하여 관련된 인간의 노력을 증가시켜야 한다는 것이다. 그러나 우리의 방법은 강화 학습 과제처럼 특정한 보상의 설계를 요구하지 않기 때문에, 종종 가장 도전적인 측면인 전체적인 노력이 그렇게 크지 않다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c|c c} \\hline \\hline  & \\multicolumn{4}{c|}{Test in Sim} & \\multicolumn{4}{c}{Test in Real} \\\\ Dataset & Level 1 & Level 2 & Level 3 & Level 4 & In Domain & Out of Position \\\\ \\hline\n' +
      '50 sim & 73.5\\% & **80\\%** & **63.5\\%** & 36\\% & 0\\% & 0\\%\n' +
      '35 sim + 15 real & **77\\%** & 70.5\\% & 61\\% & **45.5\\%** & 2\\% & 38\\%\n' +
      '15 sim + 35 real & 63\\% & 74\\% & 55\\% & 33\\% & 33\\% & **50\\%** & 15\\%\\\\%\n' +
      '50 real & 0\\% & 0\\% & 0\\% & 0\\% & 10\\% & 0\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **Sim 및 Real Demos의 비율에 대한 절제.** 시뮬레이션 및 실제 시연의 다양한 양으로 인한 성능을 비교하여 총 시연의 수를 일정하게 유지한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c|c c c} \\hline \\hline  & \\multicolumn{4}{c|}{Test in Sim} & \\multicolumn{4}{c}{Test in Real} \\\\ Method & Level 1 & Level 2 & Level 3 & Level 4 & Out of Position \\\\  & & & & & & + Random Light \\\\ \\hline ACL (Task) & **80\\%** & **61\\%** & 43.5\\% & 57\\% & **35\\%** \\\\ ACL (Data) & 19.5\\% & 30\\% & **75\\%** & **66\\%** & 20\\% \\\\ ACL wo CL(Data) & 20\\% & 22\\% & 32.5\\% & 15\\% & 5\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: ** Auto-Curriculum Learning**에 대한 절제. 이를 위해 세 가지 설정((1) 성공률을 기반으로 한 자동교육과정 학습)을 비교한다. (2) 데이터 생성률(전체 시도 횟수 대비 성공적으로 생성된 궤적의 비율)을 기반으로 한 자동 교육과정 학습. (3) 데이터 생성률만을 기반으로 하는 자동 도메인 랜덤화.\n' +
      '\n' +
      '도 5: **회전을 위한 새로운 객체로의 일반화.** 이 태스크에 대한 실험 설정은 "Pick and Place를 위한 새로운 객체로의 일반화" 실험의 것을 반영한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c|c c c} \\hline \\hline  & \\multicolumn{4}{c|}{Test in Sim} & \\multicolumn{4}{c}{Test in Real} \\\\ Dataset & Level 1 & Level 2 & Level 3 & Level 4 & In Domain & Out of Position \\\\ \\hline\n' +
      '50 sim & 73.5\\% & **80\\%** & **63.5\\%** & 36\\% & 0\\% & 0\\%\n' +
      '35 sim + 15 real & **77\\%** & 70.5\\% & 61\\% & **45.5\\%** & 2\\% & 38\\%\n' +
      '15 sim + 35 real & 63\\% & 74\\% & 55\\% & 33\\% & 33\\% & **50\\%** & 15\\%\\\\%\n' +
      '50 real & 0\\% & 0\\% & 0\\% & 0\\% & 10\\% & 0\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **Sim 및 Real Demos의 비율에 대한 절제.** 시뮬레이션 및 실제 시연의 다양한 양으로 인한 성능을 비교하여 총 시연의 수를 일정하게 유지한다.\n' +
      '\n' +
      '## 7 Acknowledgement\n' +
      '\n' +
      '알레그로 핸드 하드웨어 수리와 관련하여 통찰력 있는 대화를 나눈 하이촨 체와 청체 지아에 감사를 표합니다. 우리는 또한 시각적 사전 훈련에 대한 귀중한 입력에 대해 니클라스 한센과 지아루이 쉬에게 감사한다. 트랜스포머와의 액션 춘킹에 대한 오픈 소스 프로젝트를 통해 커뮤니티에 아낌없는 기여를 한 토니 자오 덕분이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik\'s cube with a robot hand. _arXiv preprint arXiv:1910.07113_, 2019.\n' +
      '* [2] Sridhar Pandian Arunachalam, Irmak Guzey, Soumith Chintala, and Lerrel Pinto. Holo-dex: Teaching dexterity with immersive mixed reality. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 5962-5969. IEEE, 2023.\n' +
      '* [3] Sridhar Pandian Arunachalam, Sneha Silwal, Ben Evans, and Lerrel Pinto. Dexterous imitation made easy: A learning-based framework for efficient dexterous manipulation. In _2023 ieee international conference on robotics and automation (icra)_, pages 5954-5961. IEEE, 2023.\n' +
      '* [4] Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu Wang, and Nando De Freitas. Playing hard exploration games by watching youtube. _Advances in neural information processing systems_, 31, 2018.\n' +
      '* [5] Homanga Bharadhwaj, Abhinav Gupta, and Shubham Tullsani. Visual affordance prediction for guiding robot exploration. _arXiv preprint arXiv:2305.17783_, 2023.\n' +
      '* [6] Homanga Bharadhwaj, Jay Vakil, Mohit Sharma, Abhinav Gupta, Shubham Tulsiani, and Vikash Kumar. Roboagent: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking. _arXiv preprint arXiv:2309.01918_, 2023.\n' +
      '* [7] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-shot robotic manipulation with pretrained image-editing diffusion models. _arXiv preprint arXiv:2310.10639_, 2023.\n' +
      '* [8] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3722-3731, 2017.\n' +
      '* [9] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. _arXiv preprint arXiv:2212.06817_, 2022.\n' +
      '* [10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660, 2021.\n' +
      '* [11] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 8973-8979. IEEE, 2019.\n' +
      '* [12] Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, Edward Adelson, and Pulkit Agrawal. Visual dexterity: In-hand reorientation of novel and complex object shapes. _Science Robotics_, 8(84):eadc9244, 2023.\n' +
      '* [13] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. _arXiv preprint arXiv:2003.04297_, 2020.\n' +
      '* [14] Zoey Chen, Karl Van Wyk, Yu-Wei Chao, Wei Yang, Arsalan Mousavian, Abhishek Gupta, and Dieter Fox. Learning robust real-world dexterous grasping policies via implicit shape augmentation. _arXiv preprint arXiv:2210.13638_, 2022.\n' +
      '* [15] Zoey Chen, Sho Kiami, Abhishek Gupta, and Vikash Kumar. Genaug: Retargeting behaviors to unseen situations via generative augmentation. _arXiv preprint arXiv:2302.06671_, 2023.\n' +
      '* [16] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arxiv 2018. _arXiv preprint arXiv:1805.09501_, 1805.\n' +
      '* [17] Murtaza Dalal, Ajay Mandlekar, Caelan Garrett, Ankur Handa, Ruslan Salakhutdinov, and Dieter Fox. Imitating task and motion planning with visuomotor transformers. _arXiv preprint arXiv:2305.16309_, 2023.\n' +
      '* [18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.\n' +
      '* [19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* [20] Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic skills with cross-domain datasets. _arXiv preprint arXiv:2109.13396_, 2021.\n' +
      '* [21] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. _The journal of machine learning research_, 17(1):2096-2030, 2016.\n' +
      '* [22] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18995-19012, 2022.\n' +
      '\n' +
      '* [23] Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, et al. Maniskill2: A unified benchmark for generalizable manipulation skills. _arXiv preprint arXiv:2302.04659_, 20NAS 2:40659, 2023.\n' +
      '* [24] Ankur Handa, Arthur Allshire, Viktor Makovivchuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, et al. Dextreme: Transfer of agile in-hand manipulation from simulation to reality. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 5977-5984. IEEE, 2023.\n' +
      '* [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.\n' +
      '* [26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9729-9738, 2020.\n' +
      '* [27] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16000-16009, 2022.\n' +
      '* [28] Daniel Ho, Kanishka Rao, Zhuo Xu, Eric Jang, Mohi Khansari, and Yunfei Bai. Retinagan: An object-aware approach to sim-to-real transfer. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 10920-10926. IEEE, 2021.\n' +
      '* [29] Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe Qin, Yaodong Yang, Nikolay Atanasov, and Xiaolong Wang. Dynamic handover: Throw and catch with bimanual hands. _arXiv preprint arXiv:2309.05655_, 2023.\n' +
      '* [30] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. _arXiv preprint arXiv:2307.05973_, 2023.\n' +
      '* [31] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot learning benchmark & learning environment. _IEEE Robotics and Automation Letters_, 5(2):3019-3026, 2020.\n' +
      '* [32] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In _Conference on Robot Learning_, pages 991-1002. PMLR, 2022.\n' +
      '* [33] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. _arXiv_, 2022.\n' +
      '* [34] Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba, and Sanja Fidler. Meta-sim: Learning to generate synthetic datasets. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4551-4560, 2019.\n' +
      '* [35] Manuel Kaspar, Juan D Munoz Osorio, and Jurgen Bock. Sim2real transfer for reinforcement learning without dynamics randomization. In _2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 4383-4388. IEEE, 2020.\n' +
      '* [36] Sateesh Kumar, Jonathan Zamora, Nicklas Hansen, Rishabh Jangir, and Xiaolong Wang. Graph inverse reinforcement learning from diverse videos. In _Conference on Robot Learning_, pages 55-66. PMLR, 2023.\n' +
      '* [37] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martin-Martin, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In _Conference on Robot Learning_, pages 80-93. PMLR, 2023.\n' +
      '* [38] Jacky Liang, Saumya Saxena, and Oliver Kroemer. Learning active task-oriented exploration policies for bridging the sim-to-real gap. _arXiv preprint arXiv:2006.01952_, 2020.\n' +
      '* [39] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. _IEEE Robotics and Automation Letters_, 2023.\n' +
      '* [40] Zhao Mandi, Homanga Bharadhwaj, Vincent Moens, Shuran Song, Aravind Rajeswaran, and Vikash Kumar. Cacti: A framework for scalable multi-task multi-scene visual imitation learning. _arXiv preprint arXiv:2212.05711_, 2022.\n' +
      '* [41] Priyanka Mandikal and Kristen Grauman. Dexvp: Learning dexterous grasping with human hand pose priors from video. In _Conference on Robot Learning_, pages 651-661. PMLR, 2022.\n' +
      '* [42] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Max Spero, Albert Tung, Julian Gao, John Emmons, Anchit Gupta, Emre Orbay, et al. Roboturk: A crowdsourcing platform for robotic skill learning through imitation. In _Conference on Robot Learning_, pages 879-893. PMLR, 2018.\n' +
      '* [43] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Martin-Martin. What matters in learning from offline human demonstrations for robot manipulation. _arXiv preprint arXiv:2108.03298_, 2021.\n' +
      '* [44] Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiaiyo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and Dieter Fox. Mimicgen: A data generation system for scalable robot learning using human demonstrations. _arXiv preprint arXiv:2310.17596_, 2023.\n' +
      '* [45] Oier Mees, Markus Merklinger, Gabriel Kalweit, and Wolfram Burgard. Adversarial skill networks: Unsupervised robot skill learning from video. In _2020 IEEE International Conference on Robotics and Automation (ICRA)_, pages 4188-4194. IEEE, 2020.\n' +
      '* [46] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot manipulation. _arXiv preprint arXiv:2203.12601_, 2022.\n' +
      '* [47] Anh Nguyen, Dimitrios Kanoulas, Luca Muratore, Darwin G Caldwell, and Nikos G Tsagarakis. Translating videos to commands for robotic manipulation with deep recurrent neural networks. In _2018 IEEE International Conference on Robotics and Automation (ICRA)_, pages 3782-3788. IEEE, 2018.\n' +
      '* [48] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x- embodiment: Robotic learning datasets and rt-x models. _arXiv preprint arXiv:2310.08864_, 2023.\n' +
      '* [49] Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The unsurprising effectiveness of pre-trained vision models for control. In _International Conference on Machine Learning_, pages 17359-17371. PMLR, 2022.\n' +
      '* [50] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In _2018 IEEE international conference on robotics and automation (ICRA)_, pages 3803-3810. IEEE, 2018.\n' +
      '* [51] Haozhi Qi, Ashish Kumar, Roberto Calandra, Yi Ma, and Jitendra Malik. In-hand object rotation via rapid motor adaptation. In _Conference on Robot Learning_, pages 1722-1732. PMLR, 2023.\n' +
      '* [52] Haozhi Qi, Brent Yi, Sudharshan Suresh, Mike Lambeta, Yi Ma, Roberto Calandra, and Jitendra Malik. General in-hand object rotation with vision and touch. In _Conference on Robot Learning_, pages 2549-2564. PMLR, 2023.\n' +
      '* [53] Yuzhe Qin, Hao Su, and Xiaolong Wang. From one hand to multiple hands: Imitation learning for dexterous manipulation from single-camera teleoperation. _RA-L_, 7(4):10873-10881, 2022.\n' +
      '* [54] Yuzhe Qin, Yueh-Hua Wu, Shaowei Liu, Hanwen Jiang, Ruihan Yang, Yang Fu, and Xiaolong Wang. Dexmv: Imitation learning for dexterous manipulation from human videos. In _European Conference on Computer Vision_, pages 570-587. Springer, 2022.\n' +
      '* [55] Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk, Hao Su, Xiaolong Wang, Yu-Wei Chao, and Dietor Fox. Anyteleop: A general vision-based dexterous robot arm-hand teleoperation system. _arXiv preprint arXiv:2307.04577_, 2023.\n' +
      '* [56] Kanishka Rao, Chris Harris, Alex Irpan, Sergey Levine, Julian Ibarz, and Mohi Khansari. Rl-cyclegan: Reinforcement learning aware simulation-to-real. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11157-11166, 2020.\n' +
      '* [57] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115:211-252, 2015.\n' +
      '* [58] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9339-9347, 2019.\n' +
      '* [59] Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine, and Chelsea Finn. Reinforcement learning with videos: Combining offline observations with interaction. _arXiv preprint arXiv:2011.06507_, 2020.\n' +
      '* [60] Max Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, R Devon Hjelm, Philip Bachman, and Aaron C Courville. Pretraining representations for data-efficient reinforcement learning. _Advances in Neural Information Processing Systems_, 34:12686-12699, 2021.\n' +
      '* [61] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In _2018 IEEE international conference on robotics and automation (ICRA)_, pages 1134-1141. IEEE, 2018.\n' +
      '* [62] Rutav Shah and Vikash Kumar. Rrl: Resnet as representation for reinforcement learning. _arXiv preprint arXiv:2107.03380_, 2021.\n' +
      '* [63] Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette Bohg. Concept2robot: Learning manipulation concepts from instructions and human demonstrations. _The International Journal of Robotics Research_, 40(12-14):1419-1434, 2021.\n' +
      '* [64] Kenneth Shaw, Shikhar Bahl, and Deepak Pathak. Videodex: Learning dexterity from internet videos. In _Conference on Robot Learning_, pages 654-665. PMLR, 2023.\n' +
      '* [65] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. _Journal of big data_, 6(1):1-48, 2019.\n' +
      '* [66] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In _Conference on Robot Learning_, pages 894-906. PMLR, 2022.\n' +
      '* [67] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 11523-11530. IEEE, 2023.\n' +
      '* [68] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. _Advances in neural information processing systems_, 28, 2015.\n' +
      '* [69] Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, and Heni Ben Amor. Language-conditioned imitation learning for robot manipulation tasks. _Advances in Neural Information Processing Systems_, 33:13139-13150, 2020.\n' +
      '* [70] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning from reinforcement learning. In _International Conference on Machine Learning_, pages 9870-9879. PMLR, 2021.\n' +
      '* [71] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In _2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)_, pages 23-30. IEEE, 2017.\n' +
      '* [72] Joanne Truong, Sonia Chernova, and Dhruv Batra. Bi-directional domain adaptation for sim2real transfer of embodied navigation agents. _IEEE Robotics and Automation Letters_, 6(2):2634-2641, 2021.\n' +
      '* [73] Luobin Wang, Runlin Guo, Quan Vuong, Yuzhe Qin, Hao Su, and Henrik Christensen. A real2sim2real method for robust object grasping with neural surface reconstruction. In _2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)_, pages 1-8. IEEE, 2023.\n' +
      '* [74] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11097-11107, 2020.\n' +
      '* [75] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. _arXiv preprint arXiv:2203.06173_, 2022.\n' +
      '* [76] Lin Yen-Chen, Andy Zeng, Shuran Song, Phillip Isola, and Tsung-Yi Lin. Learning to see before learning to act: Visual pre-training for manipulation. In _2020 IEEE International Conference on Robotics and Automation (ICRA)_, pages 7286-7293. IEEE, 2020.\n' +
      '* [77] Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng Chen, and Xiaolong Wang. Rotating without seeing: Towards in-hand dexterity through touch. _arXiv preprint arXiv:2303.10880_, 2023.\n' +
      '* [78] Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Jodilyn Peralta, Brian Ichter, et al. Scaling robot learning with semantically imagined experience. _arXiv preprint arXiv:2302.11550_, 2023.\n' +
      '* [79] Ying Yuan, Haichuan Che, Yuzhe Qin, Binghao Huang, Zhao-Heng Yin, Kang-Won Lee, Yi Wu, Soo-Chul Lim, and Xiaolong Wang. Robot synesthesia: In-hand manipulation with visuotactile sensing. _arXiv preprint arXiv:2312.01853_, 2023.\n' +
      '* [80] Zhecheng Yuan, Zhengrong Xue, Bo Yuan, Xueqian Wang, Yi Wu, Yang Gao, and Huazhe Xu. Pre-trained image encoder for generalizable visual reinforcement learning. _Advances in Neural Information Processing Systems_, 35:13022-13037, 2022.\n' +
      '* [81] Sergey Zakharov, Wadim Kehl, and Slobodan Ilic. Deceptionnet: Network-driven domain randomization. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 532-541, 2019.\n' +
      '* [82] Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, and Debidatta Dwibedi. Xirl: Cross-embodiment inverse reinforcement learning. In _Conference on Robot Learning_, pages 537-546. PMLR, 2022.\n' +
      '* [83] Yanjie Ze, Nicklas Hansen, Yinbo Chen, Mohit Jain, and Xiaolong Wang. Visual reinforcement learning with self-supervised 3d representations. _IEEE Robotics and Automation Letters_, 8(5):2890-2897, 2023.\n' +
      '* [84] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, et al. Transporter networks: Rearranging the visual world for robotic manipulation. In _Conference on Robot Learning_, pages 726-747. PMLR, 2021.\n' +
      '* [85] Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. _arXiv preprint arXiv:2304.13705_, 2023.\n' +
      '* [86] Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang, Yong Yu, and Weinan Zhang. Diffusion models for reinforcement learning: A survey. _arXiv preprint arXiv:2311.01223_, 2023.\n' +
      '\n' +
      '# CyberDemo: Augmenting Simulated Human Dem demonstration\n' +
      '\n' +
      '현실 세계의 익스플로러 조작을 위해\n' +
      '\n' +
      'Supplementary Material\n' +
      '\n' +
      '## 부록 개요\n' +
      '\n' +
      '이 보충 문서는 기본 논문을 보완하기 위해 추가 정보, 결과 및 시각화를 제공합니다. 구체적으로, 우리는 다음을 포함한다:\n' +
      '\n' +
      '* 데이터 수집에 대한 상세;\n' +
      '* 훈련 및 테스트 절차에 대한 상세;\n' +
      '* 평가 레벨의 설계에 관한 상세;\n' +
      '* 다른 데이터 생성 방법과의 비교;\n' +
      '* 더 많은 절제 연구;\n' +
      '* 객체 포즈들을 랜덤화하기 위한 데이터 증강의 도출에 대한 추가적인 세부사항들.\n' +
      '\n' +
      '## 부록 B 구현 세부사항\n' +
      '\n' +
      '이 섹션에서는 데이터 수집, 교육 및 테스트 프로세스에 대한 개요를 제공합니다.\n' +
      '\n' +
      '♬인간시연집♬\n' +
      '\n' +
      '인간 놀이 데이터는 원격 조작 설정을 통해 수집되며, 여기서 인간 조작자는 시뮬레이션된 환경과 실제 환경 모두에서 단일 실시간 카메라를 사용하여 시스템을 제어한다. 전체 궤적은 초당 30 프레임의 속도로 기록되며, 각 궤적은 대략 20-30초에 걸쳐 있다.\n' +
      '\n' +
      '실세계 설정에서, 추가 실감 카메라가 RGB 이미지를 캡처하는 데 사용되며, 이는 데이터 세트에서 관찰의 역할을 한다. 시뮬레이션된 환경과 실제 환경 사이의 정렬을 보장하기 위해 실제 세계에서 손 눈 보정을 수행한다. 이 보정 과정을 통해 카메라와 로봇 암 사이의 상대 위치를 결정할 수 있어 시뮬레이션에 이러한 변환을 적용할 수 있다.\n' +
      '\n' +
      '리얼 월드 셋팅\n' +
      '\n' +
      '데이터 수집을 위한 시스템 설계는 그림 6과 같다. 도면에서 나타낸 바와 같이, 인간 놀이 데이터의 수집은 인간 연산자와 카메라를 통합한다. 카메라는 초당 30프레임의 빈도로 비디오 영상을 캡처합니다. 데이터 수집 프로세스 전반에 걸쳐, 인간 운영자는 어떠한 정의된 작업 목표 없이 장면과 상호작용한다. 대신, 그들은 호기심과 흥미로운 행동을 관찰하려는 의도에 동기 부여되어 환경과 자유롭게 상호 작용한다.\n' +
      '\n' +
      '실험에서 인간 놀이 데이터는 각 시연에서 30초의 중단 없는 상호 작용을 기록하여 수집된다. 이 기간은 충분한 데이터를 수집할 수 있도록 하여 조사 및 연구를 위한 풍부하고 다양한 행동 수집을 산출한다.\n' +
      '\n' +
      '### Policy Training\n' +
      '\n' +
      '시뮬레이션 시연을 위한 훈련은 조건부 VAE[68]를 사용한다. 액션 덩어리 크기는 [85]에서 채택된 방법론에 따라 \\(50\\)으로 고정되었다. 시뮬레이션 데이터 훈련 후, 실제 데이터에 대해 더 작은 학습률과 뚜렷한 배치 규범을 사용하여 모델을 실제 시연으로 미세 조정했다.\n' +
      '\n' +
      '정책 학습과 관련된 하이퍼파라미터는 표 5에 표시되어 있는 반면, 표 6은 오토 커리큘럼 학습과 관련된 하이퍼파라미터를 나열하고 있다.\n' +
      '\n' +
      '증대 과정에 다양성을 주입하기 위해 각 증대에 대해 \\(0\\)에서 \\(10\\)까지의 무작위 척도를 통합했다. 오토 커리큘럼 학습의 맥락에서 이 무작위성 척도는 각 테스트 주기 전반에 걸쳐 일정한 분산으로 점진적으로 상승한다.\n' +
      '\n' +
      '오토 커리큘럼 학습 과정에서는 4가지 시뮬레이션 수준 모두에 걸쳐 정책의 성과를 평가하고 성공률을 평균화한다. 성공률이 성공률 임계값 이하로 떨어지면 랜덤성 척도의 증가가 중단된다. 이 전략은 무작위성을 도입하는 것과 정책이 지속적으로 과제를 달성하도록 하는 것 사이의 균형을 유지하는 데 도움이 된다.\n' +
      '\n' +
      '요약하면, 이러한 하이퍼파라미터와 오토 커리큘럼 학습에서의 평가 절차는 시간이 지남에 따라 정책이 진화하고 향상되도록 하여 만족스러운 성공률을 유지하면서 무작위성 척도를 점진적으로 확대한다.\n' +
      '\n' +
      '도 6: **실세계에서의 시스템 설정 세부사항**\n' +
      '\n' +
      '### Policy Testing\n' +
      '\n' +
      '실제 테스트 단계에서 모델의 성능을 평가하기 위해 도메인 내 테스트와 도메인 외 테스트를 모두 수행한다. 도메인 외 테스트의 경우 원래 데이터에 포함되지 않은 위치를 의식적으로 선택하여 객체의 위치를 상당히 무작위화한다. 이 단계는 모델이 익숙하지 않은 상황에서 검사되어 새로운 객체 배열을 일반화하고 조정할 수 있는 능력을 평가함을 보장한다.\n' +
      '\n' +
      '또한 시각적인 장애를 도입하고 모델의 견고성을 테스트하기 위해 디스코 조명을 통합한다. 디스코 광은 시각적 교란들을 생성하고, 테스트 환경에 복잡성의 여분의 레이어를 추가한다. 이러한 접근 방식을 통해 예상치 못한 시각적 입력을 처리할 때 모델의 복원력과 이러한 중단 속에서 성능을 유지할 수 있는 능력을 평가할 수 있다.\n' +
      '\n' +
      '결론 단계에서는 그림 7과 같이 다양한 대상에 대한 정책 일반화 능력을 평가한다. 이 일반화 가능성 테스트를 수행하기 위해 각 대상에 대해 \\(10\\)의 고유한 대상을 추가하여 초기 \\(100\\) 시뮬레이션 시범을 강화한 후 파이프라인을 다시 실행한다. 실제 환경에서 픽 앤 플레이스 작업을 위해 3개의 다른 객체를 포함하는 15개의 시연(빨간색 프레임 내에서 각 객체에 대해 5개의 시연)을 수집했다. 회전 작업의 경우 실제 데이터 세트는 원래 테스트 경우와 동일한 하나의 객체만 포함한다.\n' +
      '\n' +
      '이러한 평가를 수행하고 다양한 대상을 통합함으로써 다양한 상황에서 정책의 적응성과 성과를 평가하여 견고성과 유연성을 보장하는 것을 목표로 한다. 데모 선택은 그림 8에 나와 있습니다.\n' +
      '\n' +
      '### 시뮬레이션 평가 수준 설계 세부사항\n' +
      '\n' +
      '픽 & 플레이스 및 푸어 작업에서 우리는 다양한 정도의 무작위성을 도입하기 위해 다양한 레벨을 정의했다:\n' +
      '\n' +
      '* 레벨 1은 원래 도메인을 나타내고, 엔드-이펙터 포즈 및 배향을 포함하는 조작된 오브젝트들의 포즈의 약간의 랜덤화를 포함한다.\n' +
      '* 레벨 2는 조명 및 텍스처의 랜덤화를 포함한다.\n' +
      '* 레벨 3은 대상 객체(픽 플레이스에서의 플레이트, 붓기에서의 그릇)의 최소 랜덤화를 통합한다.\n' +
      '* 레벨 4는 조작된 객체 및 대상 객체 모두의 랜덤성 스케일을 증가시킨다.\n' +
      '\n' +
      '객체 하나가 있기 때문에 회전 태스크의 경우, 회전 태스크에 대해 사물이 다르다:\n' +
      '\n' +
      '* 레벨 1은 원래 도메인이기도 한 객체의 방향을 랜덤화하는 것으로 설정된다.\n' +
      '* 레벨 2는 픽 플레이스 및 붓기 작업과 동일하다.\n' +
      '* 레벨 3은 조작된 객체들의 엔드-이펙터 포즈의 랜덤화를 추가하는 것이다.\n' +
      '* 레벨 4는 조작된 객체의 랜덤성 스케일을 증가시킨다.\n' +
      '\n' +
      '다음은 각 작업에 대해 정의된 무작위성 매개변수이며, 모든 숫자는 성명서가 없는 경우 국제 단위로 나열된다. 우리의 설정에서 위치(0,0)는 테이블의 중심을 나타낸다.\n' +
      '\n' +
      '**픽 앤 플레이스의 레벨 디자인**\n' +
      '\n' +
      '* 작은 스케일로 랜덤 조작된 객체 포즈:\n' +
      '* 조작된 객체의 x 좌표는 -0.1 내지 0.1의 범위이다.\n' +
      '* 조작된 객체의 y 좌표는 0.2 내지 0.3의 범위이다.\n' +
      '* 조작된 물체의 z축 오일러 정도의 범위는 80 내지 90이다.\n' +
      '* Random Light and Texture(여기서 Randomness scale은 2로 고정한다):\n' +
      '* 빛의 방향이 원형의 범위 내에서 구속된다. 이 원의 반지름은 0.5에서 임의 척도까지 확장됩니다.\n' +
      '* 광들에 대한 각 채널의 컬러를 결정하기 위해, 균일한 샘플링 접근법이 채용된다. 이것은 범위 내의 값[해당 채널의 기본 색상 - 랜덤성 척도]을 선택하는 것을 포함한다.\n' +
      '*0.1, 해당 채널의 기본 색상 + 랜덤성 척도\n' +
      '* 환경지도의 지면색 및 하늘색은 조명과 동일하게 무작위 배정한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline Hyperparameters & Default \\\\ \\hline \\hline Batch Size & 128 \\\\ Num of Epochs & None \\\\ Finetuning Epochs & 3000 \\\\ Optimizer & AdamW \\\\ Learning Rate (LR) & 1e-5 \\\\ Finetuning LR & 1e-6 \\\\ Weight Decay & 1e-2 \\\\ Evaluation Frequency & 100 epochs \\\\ Encoder Layers & 4 \\\\ Decoder Layers & 7 \\\\ Heads & 8 \\\\ Feedforward Dimension & 3200 \\\\ Hidden Dimension & 256 \\\\ Chunk Size & 50 \\\\ Dropout & 0.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: **Policy Network*** Random Target Object Pose의 hyperparameters with small scale:\n' +
      '* 대상 객체의 x 좌표는 -0.1 내지 0.1의 범위이다.\n' +
      '* 대상 객체의 y 좌표는 -0.3 내지 -0.1의 범위이다.\n' +
      '* Random Manipulated and Target Object Pose with Large Scale:\n' +
      '* 조작된 객체의 x 좌표는 -0.2 내지 0.2의 범위이다.\n' +
      '* 조작된 객체의 y 좌표는 0.1 내지 0.3의 범위이다.\n' +
      '* 조작된 물체의 z축 오일러 정도의 범위는 70 내지 90이다.\n' +
      '* 조작된 물체의 z축 오일러 정도의 범위는 80 내지 90이다.\n' +
      '* 대상 객체의 x 좌표는 -0.2 내지 0.2의 범위이다.\n' +
      '* 대상 객체의 y 좌표는 -0.3 내지 0의 범위이다.\n' +
      '\n' +
      '**부음 레벨 설계**\n' +
      '\n' +
      '* 작은 스케일로 랜덤 조작된 객체 포즈:\n' +
      '* 조작된 객체의 x 좌표는 -0.1 내지 0.1의 범위이다.\n' +
      '* 조작된 객체의 y 좌표는 -0.2 내지 -0.1의 범위이다.\n' +
      '* 조작된 물체의 z축 오일러 정도의 범위는 0 내지 179이다.\n' +
      '* Random Light and Texture(여기서 Randomness scale은 2로 고정) : pick and place와 동일하다.\n' +
      '* 작은 스케일의 랜덤 타겟 오브젝트 포즈:\n' +
      '* 대상 객체의 x 좌표는 -0.1 내지 0.1의 범위이다.\n' +
      '* 대상 객체의 y 좌표는 0.2 내지 0.3의 범위이다.\n' +
      '* Random Manipulated and Target Object Pose with Large Scale:\n' +
      '* 조작된 객체의 x 좌표는 -0.1 내지 0.15의 범위이다.\n' +
      '* 조작된 객체의 y 좌표는 -0.3 내지 0의 범위이다.\n' +
      '* 대상 객체의 x 좌표는 -0.2 내지 0.2의 범위이다.\n' +
      '* 조작된 객체의 y 좌표는 -0.3 내지 0.3의 범위이다.\n' +
      '* 조작된 물체의 z축 오일러 정도의 범위는 0 내지 60이다.\n' +
      '\n' +
      '## 부록 C 더 실험 결과\n' +
      '\n' +
      '### 데이터 생성 방법의 비교\n' +
      '\n' +
      '데이터 증강 접근법의 효율성을 테스트하기 위해 MimicGen[44]에 의해 증강된 시뮬레이션 데이터를 사용하여 사전 훈련한 다음 원래 MimicGen 프레임워크에 포함되지 않은 sim2real 전송인 실제 원격 조작 데이터로 미세 조정했다. 도 7에 도시된 바와 같이, MimicGen은 새로운 객체 포즈들에 (보라색으로) 보간된 궤적을 추가함으로써, 잠재적으로 급격한 전환을 야기한다. 그러나, 우리의 방법은 전체 시퀀스를 매끄럽게 통합함으로써 더 많은 유체 운동을 초래한다. 따라서 우리의 데이터로 훈련된 모방 학습 정책은 표에 표시된 시뮬레이션 및 현실의 개선된 결과에 의해 입증된 바와 같이 다른 것보다 우수하다.\n' +
      '\n' +
      '### Action Aggregation에 대한 Ablation\n' +
      '\n' +
      '그림 9에 예시된 바와 같이, Small Motion과 함께 액션 집성의 사용은 단일 도메인 내에서의 모방 학습에서의 장점을 넘어 확장된다. 또한 시뮬레이션된 환경과 실제 환경 사이의 격차를 줄이는 데 효과적인 도구로 기능합니다. 도 10에 예시된 바와 같이, 성공\n' +
      '\n' +
      '도 7: **Object Set in Real World.** 적색 프레임 내에 위치한 객체들은 훈련을 위해 할당되는 반면, 나머지 객체들은 이전에 보이지 않는 객체들에 대한 테스트를 위해 따로 설정된다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '접근하고 있어요 그림 12는 이러한 뚜렷한 증강 전략을 통해 생성된 데이터 세트를 사용하여 훈련하는 동안 성공률을 보여준다. 우리의 연구 결과는 제안된 기술의 적용을 통해 모델이 세 가지 조작 작업 모두에 대해 일관된 개선을 입증함을 보여준다. 이러한 결과는 민감도 분석을 포즈 데이터 증강에 통합하는 효과를 강조한다.\n' +
      '\n' +
      '## 데이터 증강의 부록 D 도출\n' +
      '\n' +
      '이 섹션에서는 무작위 객체 포즈의 데이터 증강에 적용된 공식의 도출을 더 깊이 파헤친다. 우리는 본 논문에서 식 2를 재현하고 상세한 설명을 제공한다:\n' +
      '\n' +
      '\\frac{ \\psi_{seg_{j}}=\\frac{ \\psi_{seg_{j}}{\\sum_{j}\\psi_{j}},\\quad\\forall seg_{j}\\f_{i}(\\Delta T_{j}}\\log(\\Delta T)/K)\\a_{i}^{new}&=a_{i}\\tag{3}\\\n' +
      '\n' +
      '방정식의 첫 번째 행은 본 논문에서 식 1로부터 계산된 강건성 점수를 정규화하여 모든 점수의 합이 \\(1\\)이 되도록 한다. 이 매개변수는 로봇에게 새로운 포즈를 안내하기 위해 각 청크가 수행해야 하는 수정 비율을 나타내는 각 액션 청크에 대한 가중치로 해석될 수 있다.\n' +
      '\n' +
      '두 번째 선은 청크\\(j\\)에서 각 단계에 대한 상대적 포즈 수정을 계산한다. 청크에는 \\(K\\) 단계가 있으며, 각 단계는 동일한 청크 내에서 동일한 수정량을 할당받는다. 여기서, \\(log()\\)은 \\(SE(3)\\)Lie 그룹을 \\(se(3)\\)Lie 대수에 매핑하며, 여기서 \\(exp\\)은 역수이고, \\(se(3)\\)을 다시 \\(SE(3)\\)으로 매핑한다.\n' +
      '\n' +
      '이 방정식의 세 번째 줄은 포즈 수정을 기반으로 새로운 동작을 계산한다. 여기서, \\(f_{i}\\)는 월드 프레임에서 현재 엔드 이펙터 프레임으로 움직임을 천이시키는 \\(SE(3)\\) 공간에서의 유사성 변환이다. 우리는 이제 \\(f_{i}\\)의 상세한 유도를 제공한다. \\(\\Delta T=T_{W}^{O_{new}}(T_{W}^{O_{old}})^{-1}=T_{O_{old}}^{O_{new}}\\)이므로, 이러한 상대적인 포즈 변화는 오래된 객체 포즈 프레임에서의 표현이다. 이 포즈 변환을 사용하여 액션을 수정하기 위해, 우리는 이 상대적인 포즈를 현재 엔드-이펙터 포즈의 프레임인 액션에 대응하는 프레임으로 변환할 필요가 있다. 프레임\\(B\\)에서 표현되는 움직임\\(X\\)을 프레임\\(A\\)으로 변환하는 유사도 변환\\(T_{B}^{A}X(T_{B}^{A})^{-1}\\)을 고려할 때, \\(f_{i}\\(f_{i}(\\Delta T)=T_{R_{i}}^{O_{old}}\\Delta T(T_{R_{i}}}^{O_{old}})^{-1}\\)로 유도할 수 있으며, 여기서 \\(T_{R_{i}}\\)은 프레임에서의 로봇 엔드 이펙터 포즈이다.\n' +
      '\n' +
      '그림 12: **증강 비교** 우리는 선택된 하나의 오리지널 데모에 운동학적 증강을 적용하여 MimicGen 방법과 우리의 방법을 모두 사용하여 새로운 위치에 적응시킨다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>