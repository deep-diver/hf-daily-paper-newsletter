<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'underexplored tasks intrinsic to binary-native operations, including data conversion and CPU state modelling, which represent algorithm and hardware simulation, respectively. The study of byte models not only marks a significant step towards holistic and unified deep learning, but also offers a new perspective on modelling the digital world.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '### Language Models\n' +
      '\n' +
      'LMs, from earlier LSTM-based (Hochreiter and Schmidhuber, 1997) to recent Transformer-based models (Vaswani et al., 2017), are crucial for both understanding and generating human language while simulating human intelligence. Text tokenization plays a fundamental role in these models, as it involves breaking down text into smaller units, such as words or subwords (Sennrich et al., 2016; Wu et al., 2016; Kudo, 2018; Kudo and Richardson, 2018), which serve as the input for the model. The introduction of Generative Pre-trained Transformer (GPT) models (Radford et al., 2018; 2019; Brown et al., 2020) represents a significant advancement in this field. GPT models are pre-trained through self-supervised learning, particularly via next token prediction on extensive text data. This training technique, next token prediction, teaches the model to predict the most likely next token in a sequence, enabling it to capture the structure and semantics behind languages.\n' +
      '\n' +
      'Next token prediction has extended its influence to various data types. In audio processing, models like AudioPaLM (Rubenstein et al., 2023) merge text and speech, enabling speech-to-speech translation and advanced speech recognition. MusicGen (Copet et al., 2023) excels in conditional music generation by modelling multiple parallel streams of acoustic tokens extracted by EnCodec (Defossez et al., 2022). In image processing, iGPT (Chen et al., 2020) applies Transformer models to predict the next pixel in an image, while several vision-language models (Liu et al., 2023; Zhu et al., 2023; Li et al., 2023) have emerged to bridge the gap between textual and visual data. In biochemical sequences, Tranception (Notin et al., 2022) leverages autoregressive transformers and retrieval to predict protein fitness, while ProtGPT2 (Ferruz et al., 2022) generates protein sequences with natural amino acid propensities. HyenaDNA (Nguyen et al., 2023) extends context lengths in genomic modelling, enabling long-range sequence understanding.\n' +
      '\n' +
      'Next token prediction has empowered LMs to grasp the intricacies of human intelligence and the world. Expanding these techniques to binary data via next byte prediction could further enhance their versatility in handling digital information and simulating the digital world.\n' +
      '\n' +
      '### Byte Models\n' +
      '\n' +
      'While binary data lacks the inherent structure and semantics of human-interpretable data like text, recent research efforts are exploring its modelling and information extraction, opening up new possibilities for byte models.\n' +
      '\n' +
      'By modelling native binary data, systems like MalConv (Raff et al., 2018) and DeepVSA (Guo et al., 2019) have emerged as potent tools for malware detection and program analysis. MalConv employs Convolutional Neural Networks (CNNs) (LeCun et al., 1989) to analyze raw byte sequences in executable files, while DeepVSA enhances memory alias analysis within the context of value set analysis for postmortem program analysis. Additionally, the concept of language models compressing byte sequences (Deletang et al., 2023) introduces a novel perspective on how large pre-trained models (Hoffmann et al., 2022) can be utilized.\n' +
      '\n' +
      'Several studies have validated the utility of byte-level encoding for language tasks. For instance, Byte-level Byte Pair Encoding (BBPE) has been used to enhance multilingual model pre-training (Wei et al., 2021) and has also shown promise in machine translation (Wang et al., 2020), striking a balance between processing efficiency and linguistic breadth. ByT5 (Xue et al., 2022) builds on this by using standard Transformer models for byte sequences, promoting a token-free encoding method that improves noise robustness and spelling sensitivity in multilingual scenarios.\n' +
      '\n' +
      'Figure 1: The bGPT framework simulates digital systems through native binary data, and integrates diverse data types into a single model, treating everything as a byte sequence. This approach simplifies integration and expands application possibilities in the digital world.\n' +
      '\n' +
      'Byte encoding has also been applied to other human-interpretable data, allowing models to work with binary representations of text, images, and diverse data types in a universal framework. For example, ByteFormer (Horton et al., 2023) directly handles raw byte sequences converted from images and audio while maintaining versatility and privacy. MegaByte (Yu et al., 2023), on the other hand, has been tested and proven to excel in modelling long byte sequences across various modalities. Inspired by MegaByte (Yu et al., 2023), MambaByte (Wang et al., 2024) leverages the Mamba network structure (Gu & Dao, 2023) to excel in byte-level language modelling and even outperforms LMs based on subword tokenization.\n' +
      '\n' +
      'Despite progress, current research often neglects native binary data, focusing on narrow tasks and overlooking the broader potential of byte models in digital world simulation. To address these issues, we employed bGPT to model native binary data and conducted comprehensive evaluations across various tasks. This approach provides a holistic assessment of byte models in various applications, offering insights into the potential of digital world modelling.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      'In this section, we introduce bGPT, a model optimized for modelling digital data at the byte level. We start by presenting its hierarchical Transformer framework, which segments byte sequences into patches to manage computational efficiency. Subsequently, we present the training objectives of bGPT, including generative modelling and classification.\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      'Working with digital data at the byte level not only allows models to learn digital system patterns but also offers a unified approach for incorporating various data types into a single framework. However, the high granularity of bytes results in long sequences, substantially raising computational costs. This issue is more pronounced in Transformer-based models due to quadratic self-attention scaling, restricting their efficiency and scalability for handling binary data.\n' +
      '\n' +
      'To address computational limitations in Transformer-based models with long byte sequences, inspired by previous work (Wu et al., 2023; Yu et al., 2023; Wu et al., 2023; Yang et al., 2023), bGPT adapts a hierarchical Transformer architecture and segments a sequence of bytes \\(B=\\{b_{1},b_{2},...,b_{T}\\}\\) of length \\(T\\) into a sequence of patches \\(\\mathcal{P}\\), where each patch contains exactly \\(S\\) bytes:\n' +
      '\n' +
      '\\[\\mathcal{P}=[P_{1},P_{2},\\ldots,P_{N}], \\tag{1}\\]\n' +
      '\n' +
      'where \\(N=\\lceil\\frac{T}{S}\\rceil\\) is the number of patches, and \\(P_{i}=[b_{(i-1)S+1},\\ldots,b_{iS}]\\) for \\(1\\leq i\\leq N\\). If \\(T\\mod S\\neq 0\\), the last patch \\(P_{N}\\) is defined as:\n' +
      '\n' +
      '\\[P_{N}=[b_{(N-1)S+1},\\ldots,b_{T},\\underbrace{e,\\ldots,e}_{S-(T\\mod S)}], \\tag{2}\\]\n' +
      '\n' +
      'where \\(e\\) represents the <eop> (end-of-patch) token used to pad the last patch \\(P_{N}\\) to the size \\(S\\). By segmenting byte sequences into more manageable patches, bGPT balances the need for byte-level sequence modelling with computational efficiency.\n' +
      '\n' +
      'As illustrated in Fig. 2, bGPT consists of three main components: a linear projection layer, a patch-level decoder, and a byte-level decoder. bGPT processes the patch sequence \\(\\mathcal{P}\\) through its components as follows:\n' +
      '\n' +
      '**Linear Projection Layer**: Each patch \\(P_{i}\\) from \\(\\mathcal{P}\\) is viewed as a matrix of size \\(S\\times 257\\), where \\(S\\) is the patch size, and each byte is one-hot encoded into a 257D vector, including all 256 byte values and an <eop> token. These patches are then flattened into one-dimensional vectors, where the rows in the matrix are concatenated sequentially. The linear projection layer then maps each flattened vector into a dense vector \\(E_{i}\\) of a hidden size \\(H\\). For each patch, the operation can be formulated as:\n' +
      '\n' +
      '\\[E_{i}=\\text{flatten}(P_{i})\\cdot W_{\\text{linear}},\\quad 1\\leq i\\leq N, \\tag{3}\\]\n' +
      '\n' +
      'where \\(W_{\\text{linear}}\\) is the weight matrix of the linear projection layer with a shape of \\((S\\times 257,H)\\). This dense embedding enables more efficient processing of the byte sequence by reducing the dimensionality while preserving the essential information contained in each patch.\n' +
      '\n' +
      '**Patch-Level Decoder**: This decoder takes the sequence of embedded patches \\(\\mathcal{E}=\\{E_{1},E_{2},\\ldots,E_{N}\\}\\) and processes it to autoregressively predict the features of the subsequent patch, effectively learning the structure of the data:\n' +
      '\n' +
      '\\[\\hat{E}_{i}=\\text{Decoder}_{\\text{patch}}(\\mathcal{E}_{<i}\\oplus\\mathcal{X }_{<i}), \\tag{4}\\]\n' +
      '\n' +
      'where \\(\\mathcal{E}_{<i}\\) denotes the sequence of patch embeddings before the \\(i\\)-th patch, and \\(\\mathcal{X}_{<i}\\) represents the corresponding\n' +
      '\n' +
      'Figure 2: bGPT segments byte sequences into patches, predicts next patch features with a patch-level decoder, and reconstructs bytes within patches using these features with a byte-level decoder.\n' +
      '\n' +
      'positional embeddings. The \\(\\oplus\\) symbol denotes the element-wise addition of these two sequences. The output, \\(\\hat{E}_{i}\\), is the predicted feature for the \\(i\\)-th patch.\n' +
      '\n' +
      '**Byte-Level Decoder**: It takes the predicted feature \\(\\hat{E}_{i}\\) of an individual patch and autoregressively reconstructs the sequence of bytes within that patch. The process is independent for each patch and operates by conditioning on the feature representation \\(\\hat{E}_{i}\\) of the current patch:\n' +
      '\n' +
      '\\[\\hat{b}_{i,j}=\\text{Decoder}_{\\text{byte}}(\\hat{E}_{i},b_{i,<j}),\\quad 1\\leq j \\leq S, \\tag{5}\\]\n' +
      '\n' +
      'where \\(\\hat{b}_{i,j}\\) is the predicted byte at position \\(j\\) in the \\(i\\)-th patch, and \\(b_{i,<j}\\) represents all preceding bytes in the current patch.\n' +
      '\n' +
      '### Training Objectives\n' +
      '\n' +
      'The training for bGPT primarily revolves around generative modelling through next byte prediction as its core focus, playing a pivotal role in predicting and generating bytes.\n' +
      '\n' +
      '**Generative Modelling**: It aims to predict the next byte \\(b_{i+1}\\) in a sequence based on preceding bytes \\(\\{b_{1},b_{2},...,b_{i}\\}\\) without explicit guidance. For a byte sequence \\(B=\\{b_{1},b_{2},...,b_{T}\\}\\) of length \\(T\\), the objective is minimizing the negative log-likelihood of the next byte prediction across the sequence, defined as:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{GEN}}(\\theta)=-\\sum_{i=1}^{T-1}\\log p(b_{i+1}|b_{1},b_{2},...,b_{i};\\theta), \\tag{6}\\]\n' +
      '\n' +
      'where \\(\\theta\\) represents the model parameters, and \\(p(\\cdot)\\) denotes the predicted probability distribution over possible next bytes. The loss function \\(\\mathcal{L}_{\\text{GEN}}(\\cdot)\\) in generative modelling encourages the model to understand the sequential dependencies in data at the byte level.\n' +
      '\n' +
      'After being initially trained on generative modelling via next byte prediction, bGPT is further adapted to classification tasks with a new training objective.\n' +
      '\n' +
      '**Classification**: Upon the foundation of generative modelling through next byte prediction, bGPT is further trained on labelled datasets, where it predicts categories from byte sequences. This involves extracting a global feature from the byte sequence, which is then processed by a classification head. The goal is to minimize the classification loss \\(\\mathcal{L}_{\\text{CLF}}\\), formulated as:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{CLF}}(\\theta)=-\\sum_{k=1}^{K}y_{k}\\log p(y_{k}|B;\\theta), \\tag{7}\\]\n' +
      '\n' +
      'where \\(y_{k}\\) is the boolean label for the \\(k\\)-th category, indicating whether the byte sequence belongs (true) or does not belong (false) to that category. \\(K\\) is the total number of categories, and \\(p(y_{k}|B;\\theta)\\) is the predicted probability of category \\(k\\) given the byte sequence \\(B\\).\n' +
      '\n' +
      '## 4 Applications\n' +
      '\n' +
      'Byte models such as bGPT excel at understanding binary data, and have proficiency in digital media file processing (e.g., text, audio, and images) and simulating algorithms and hardware operations for in-depth software and digital system analysis. This section introduces the selected applications and corresponding experimental settings for evaluating the proposed bGPT.\n' +
      '\n' +
      '### Digital Media Processing\n' +
      '\n' +
      'The field of deep learning is steadily advancing its proficiency in both the generation and classification of diverse forms of media, including text, audio, and images (Devlin et al., 2018; Ao et al., 2022; Ramesh et al., 2022), which are essential for human communication and information exchange. These media files, typically stored and transmitted as byte sequences on electronic devices, enable bGPT to process such digital content for generative modelling and classification.\n' +
      '\n' +
      'bGPT trains in generative modelling for representation learning via next token prediction. It then uses features from the final patch-level decoder layer, employing average pooling to derive global features for classification.\n' +
      '\n' +
      'To streamline the training process, we standardized audio and image datasets. Audio files were converted to WAV format, with specifications including an 8000 Hz sampling rate, mono channel, and 8-bit depth, each trimmed to one-second lengths. Image data were set to BMP format with a resolution of 32\\(\\times\\)32, RGB colour, and 24-bit depth.\n' +
      '\n' +
      '### Algorithm and Hardware Simulation\n' +
      '\n' +
      'To demonstrate the capabilities of byte models in predicting and modelling digital processes, we select two examples--data conversion and CPU state modelling.\n' +
      '\n' +
      '**Data Conversion**: The process involves converting data from one format to another, with symbolic music formats such as ABC notation and MIDI files serving as our main examples. For background information on ABC notation and MIDI, please refer to Appendix A. In this task, bGPT employs the generative modelling approach on concatenated byte sequences of paired ABC and MIDI files, separated by a special patch. The bGPT model learns to convert text-based ABC notation music scores into binary MIDI performance signals and, reversely, convert MIDI back into ABC notation. This necessitates the ability to simulate and reverse-engineer the conversion algorithm1, which indicates an essential capability for modelling the digital world.\n' +
      '\n' +
      'Footnote 1: [https://github.com/xlvector/abcmidi](https://github.com/xlvector/abcmidi)\n' +
      '\n' +
      '**CPU State Modelling**: Similarly, the model is fed with concatenated sequences of low-level machine instructions followed by a series of CPU register states. The objective is to accurately predict how the state updates with each instruction until the program halts. This task demonstrates the capacity of bGPT to interpret operational data and replicate digital activities within hardware.\n' +
      '\n' +
      'For CPU state modelling, we introduce the CPU States dataset (with 2.1 million instances), offering a simplified representation of CPU behaviour for ease of data collection and evaluation. Each dataset instance contains a 1KB memory block with varying numbers of machine instructions, followed by a sequence of 16-byte CPU register states. These states include various instructions, totaling 21 unique types with 43 variants, such as data movement, logical operations, and arithmetic operations. Within each state, 1 byte each is allocated for the Program Counter (PC) and Accumulator (ACC), 4 bytes are allocated for the Instruction Register (IR), with an additional 10 bytes reserved for general-purpose registers. Instances are generated by executing random sequences of 1 to 256 instructions and capturing the state after each execution. Despite simplifications, this dataset effectively simulates typical CPU behaviour. See Appendix B for more details.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Settings\n' +
      '\n' +
      'Our experiments utilized open-source datasets across various domains, including images, speech, text, two types of symbolic music, and CPU states, as detailed in Table 1. The training settings were consistent across all bGPT models and datasets as per Table 2, supporting file sizes up to 8KB using a patch size of 16 and a sequence length of 512. The 110M-parameter bGPT model matches the standard Transformer-based model scale. Notably, we avoided hyperparameter tuning and data augmentation in all evaluations. We employed accuracy (Acc) as the metric to assess classification performance and Bits-Per-Byte (BPB) for other generative modelling tasks, unless otherwise specified.\n' +
      '\n' +
      '### Digital Media Processing\n' +
      '\n' +
      'The study aimed to evaluate the effectiveness of bGPT in processing digital media files at the byte level compared to specialized models. We followed the standard pre-training and fine-tuning approach in deep learning, pre-trained bGPT on diverse datasets like ImageNet (\\(\\mathrm{bGPT}_{\\mathrm{image}}\\)), Wikipedia (\\(\\mathrm{bGPT}_{\\mathrm{wiki}}\\)), and LibriSpeech (\\(\\mathrm{bGPT}_{\\mathrm{libef}}\\)). We also explored the impact of mixed pre-training: \\(\\mathrm{bGPT}_{\\mathrm{signal}}\\) combined the ImageNet and LibriSpeech datasets, while \\(\\mathrm{bGPT}_{\\mathrm{mix}}\\) integrated all three datasets. A random-initialized variant \\(\\mathrm{bGPT}_{\\mathrm{random}}\\) was used as a baseline. These models were first fine-tuned using next byte prediction on AG News, CIFAR-10, and Speech Commands v2, and then further fine-tuned for classification.\n' +
      '\n' +
      '#### 5.2.1 Baselines\n' +
      '\n' +
      'For baseline comparisons, we selected Transformer-based models of similar scale that excel in their respective domains: GPT2-small (Radford et al., 2019) for text generation/classification, and ViT-B/16 (Dosovitskiy et al., 2021) and AST (Gong et al., 2021) for image and audio classification, respectively. GPT2-small was pre-trained on English Wikipedia under the same settings as bGPT. ViT and AST were pre-trained on ImageNet (Deng et al., 2009), and their results were taken from the original studies. In the case of CIFAR-10 (Krizhevsky et al., 2009) and Speech Commands v2 (Warden, 2018), where generative modelling benchmarks were lacking, we did not report BPB metrics.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Dataset** & **Total Files (Million)** & **Total Bytes (GB)** & **Mean File Size (Byte)** & **GPU hours per Epoch** & **Modality** \\\\ \\hline Wikipedia (Wikimedia, 2022) & 6.28 & 13.67 & 2237 & 107 & \\\\ AG News (Zhang et al., 2015) & 0.13 & 0.03 & 236.4 & 2.1 & \\\\ ImageNet (Deng et al., 2009) & 1.33 & 3.88 & 3126 & 18 & \\\\ CIFAR-10 (Krizhevsky et al., 2009) & 0.06 & 0.17 & 3126 & 1.1 & \\\\ LibriSpeech (Panayotov et al., 2015) & 3.68 & 27.57 & 8044 & 84 & \\\\ Speech Commands v2 (Warden, 2018) & 0.09 & 0.68 & 8044 & 2.4 & \\\\ IrishMAN-ABC (Wu et al., 2023a) & 0.21 & 0.06 & 313.9 & 2.4 & \\\\ IrishMAN-MIDI (Wu et al., 2023a) & 0.21 & 0.39 & 1916 & 3.2 & \\\\ CPU States (proposed) & 2.1 & 6.09 & 3103 & 36 & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Overview of datasets for bGPT evaluation, with computational costs benchmarked in NVIDIA V100 GPU hours. Details include file counts in millions, dataset sizes in gigabytes, average file sizes in bytes, and the GPU hours required per epoch for training.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Hyperparameter** & **Pre-training** & **Fine-tuning** \\\\ \\hline Patch Size & 16 & 16 \\\\ Patch Length & 512 & 512 \\\\ Patch-Level Layers & 12 & 12 \\\\ Byte-Level Layers & 3 & 3 \\\\ Hidden Size & 768 & 768 \\\\ Epochs & 32 & 32 \\\\ Learning Rate & 1e-04 & 1e-05 \\\\ Batch Size & 16 & 1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Hyperparameter settings for pre-training and fine-tuning.\n' +
      '\n' +
      '#### 5.2.2 Results\n' +
      '\n' +
      'Table 3 presents the results of various bGPT and baseline models on different benchmarks. The primary insight from this comparison is the significant influence of pre-training on model performance across all tasks. For example, \\(\\mathrm{bGPT}_{\\mathrm{wiki}}\\) pre-trained on Wikipedia performs well in text-related tasks, while \\(\\mathrm{bGPT}_{\\mathrm{libri}}\\), pre-trained on LibriSpeech, excels in spoken content tasks compared to other variants. This indicates that bGPT models can achieve better performance in downstream tasks when there is a close match between pre-training and fine-tuning datasets, aligning with standard pre-training principles (Liu et al., 2022).\n' +
      '\n' +
      'Despite not having modality-specific prior knowledge, bGPT models still manage to achieve performances that are on par with the baseline models. For instance, \\(\\mathrm{bGPT}_{\\mathrm{wiki}}\\) achieved a score of 1.0639 BPB and an accuracy of 92.49% on AG News, which, while not quite at the level of GPT2-small, scored 0.9237 BPB and 94.50% accuracy, still demonstrates competitive performance. Likewise, \\(\\mathrm{bGPT}_{\\mathrm{libri}}\\) reached an accuracy of 96.03% on Speech Commands v2, coming close to the accuracy of AST at 98.11%. However, in the analysis of CIFAR-10, a noticeable difference is seen with \\(\\mathrm{bGPT}_{\\mathrm{image}}\\) lagging ViT, with an accuracy of 88.69% against 98.13%. This discrepancy in image tasks is likely due to the sequential processing nature of byte models, which struggle to capture the essential two-dimensional spatial relationships within images (Yu et al., 2023). Despite this, simply scaling the model size while retaining this sequential processing approach could still hold promise for achieving state-of-the-art results (Chen et al., 2020).\n' +
      '\n' +
      'bGPT models pre-trained on mixed modalities, namely \\(\\mathrm{bGPT}_{\\mathrm{signal}}\\) and \\(\\mathrm{bGPT}_{\\mathrm{mix}}\\), yield performances that generally align with the average performance of models pre-trained on individual modalities. For example, \\(\\mathrm{bGPT}_{\\mathrm{mix}}\\) on the AG News dataset, with a BPB of 1.0935 and accuracy of 91.75%, outperforms other variants but falls short of the performance demonstrated by \\(\\mathrm{bGPT}_{\\mathrm{wiki}}\\), which is specifically tuned for text data. Similar trends can be observed in the case of \\(\\mathrm{bGPT}_{\\mathrm{signal}}\\), which typically shows performance levels that lie somewhere between \\(\\mathrm{bGPT}_{\\mathrm{image}}\\) and \\(\\mathrm{bGPT}_{\\mathrm{libri}}\\). This illustrates a trade-off in byte models--while mixed-modality pre-training fosters versatility, it may dilute the depth of domain-specific understanding.\n' +
      '\n' +
      'Another noteworthy observation from Table 3 is the mixed results in cross-modal fine-tuning on bGPT models, with both positive and negative transfer effects. Positive transfer occurs when models pre-trained on one data type (e.g., \\(\\mathrm{bGPT}_{\\mathrm{libri}}\\) on LibriSpeech for audio or \\(\\mathrm{bGPT}_{\\mathrm{image}}\\) on ImageNet for images) are fine-tuned on tasks of another modality, showing non-trivial improvements over random initialization. This suggests shared byte patterns between modalities like audio and images. However, negative transfer is observed when transitioning between text and other modalities, indicating that the benefits of structured pattern learning in pre-training do not universally apply. Text, as a human-created abstraction, exhibits distinct byte-level organizational patterns that differ significantly from audio and visual data, which may explain the negative transfer effects observed in text-related cross-modal fine-tuning.\n' +
      '\n' +
      'To further investigate cross-modal knowledge transfer in byte models, we evaluated their performance on the Speech Commands v2 dataset, transformed into 32\\(\\times\\)32 BMP spectrograms. This process, converting 8KB audio files to 3KB images, inevitably leads to information loss. However, our focus shifts towards exploring the nuances of knowledge transfer dynamics rather than mere performance gains. We selected two pre-trained models for evaluation: \\(\\mathrm{bGPT}_{\\mathrm{image}}\\) for its data format consistency with spectrograms and \\(\\mathrm{bGPT}_{\\mathrm{libri}}\\) for its information similarity with spectrograms (both involving speech). These models were employed for conditional generative modelling and classification, following a procedure akin to that in Table 3.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{**AG News (4 classes)**} & \\multicolumn{2}{c}{**CIFAR-10 (10 classes)**} & \\multicolumn{2}{c}{**Speech Commands v2 (36 classes)**} \\\\ \\cline{2-7}\n' +
      '**Model** & **BPB** & **Acc (\\%)** & **BPB** & **Acc (\\%)** & **BPB** & **Acc (\\%)** \\\\ \\hline \\(\\mathrm{bGPT}_{\\mathrm{random}}\\) & 1.3496 & 84.74 & 3.4928 & 76.73 & 1.5414 & 92.43 \\\\ \\(\\mathrm{bGPT}_{\\mathrm{wiki}}\\) & **1.0639** & **92.49** & 3.6663 & 77.02 & 1.5719 & 93.56 \\\\ \\(\\mathrm{bGPT}_{\\mathrm{image}}\\) & 1.4179 & 83.16 & **3.1234** & **88.69** & 1.5326 & 93.91 \\\\ \\(\\mathrm{bGPT}_{\\mathrm{libri}}\\) & 1.3993 & 83.59 & 3.3345 & 83.51 & **1.4818** & **96.03** \\\\ \\(\\mathrm{bGPT}_{\\mathrm{signal}}\\) & 1.4058 & 83.80 & 3.1554 & 87.65 & 1.4898 & 95.66 \\\\ \\(\\mathrm{bGPT}_{\\mathrm{mix}}\\) & 1.0935 & 91.75 & 3.2279 & 84.32 & 1.5086 & 95.09 \\\\ \\hline \\hline \\multicolumn{7}{l}{Baselines} & 0.9237 & 94.50 & — & 98.13 & — & 98.11 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Performance comparison of bGPT models pre-trained on different datasets and baseline models in their respective modalities: GPT2-small on AG News, ViT-B/16 on CIFAR-10, and AST on Speech Commands v2. \\(\\mathrm{bGPT}_{\\mathrm{signal}}\\) is pre-trained on a merged dataset of ImageNet and LibriSpeech, while \\(\\mathrm{bGPT}_{\\mathrm{mix}}\\) is pre-trained on a combination of ImageNet, LibriSpeech, and Wikipedia datasets.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      'to MIDI conversion, \\(\\mathrm{BPB}_{\\mathrm{abc}}\\) assesses generative modelling, as the model generates content from scratch. While \\(\\mathrm{BPB}_{\\mathrm{midi}}\\) evaluates data conversion, considering the complete ABC byte sequence is given.\n' +
      '\n' +
      'We observe a lower starting loss and a more rapid convergence in Fig. 2(a) as the data scale grows, indicating that increased data volume directly enhances model performance in simulating the data conversion process. From Table 5, we see that as the scale of data increases, BPB for both ABC to MIDI and MIDI to ABC conversions decrease significantly. The \\(\\mathrm{bGPT}^{5}\\) model achieves an impressively low \\(\\mathrm{BPB}_{\\mathrm{midi}}\\) of 0.0011 in ABC to MIDI conversion, which is extremely close to perfect performance (where BPB reaches 0), surpassing the performance of the \\(\\mathrm{bGPT}^{3}\\) model by orders of magnitude.\n' +
      '\n' +
      'Table 5 suggests consistently higher BPB for ABC in both directions, which is likely due to two factors: 1) The forward conversion from ABC to MIDI focuses on simulating an existing algorithm with necessary information, while the reverse process from MIDI to ABC requires inferring and reconstructing missing information in MIDI files like score structures, musical ornaments, and expressions. 2) As MIDI is a lower-level binary format and ABC notation is a human-readable text format, byte models may find it easier to learn patterns within MIDI files.\n' +
      '\n' +
      '#### 5.3.2 CPU State Modelling\n' +
      '\n' +
      'CPU state modelling aims to replicate CPU functionality by predicting updates to internal states from machine instructions. We employed bGPT for predicting these states, selecting the highest probability byte at each step based on complete instructions and initial states. The accuracy was assessed through byte-wise comparisons with actual states.\n' +
      '\n' +
      'We discovered that data volume significantly influences modelling performance. Table 6 shows significant performance variations, with a notable BPB drop from \\(\\mathrm{bGPT}^{4}\\) to \\(\\mathrm{bGPT}^{5}\\), but diminishing returns beyond \\(\\mathrm{bGPT}^{5}\\). Both \\(\\mathrm{bGPT}^{5}\\) and \\(\\mathrm{bGPT}^{6}\\) achieved near-perfect accuracies (99.97% and 99.99%), suggesting an efficiency beyond simple memorization, given that each test case contained an average of 128 random instructions, and the vast potential combinations of instruction scenarios (over 516 million).\n' +
      '\n' +
      'A significant improvement in the performance of \\(\\mathrm{bGPT}^{5}\\) occurred around epoch 11, as shown in Fig. 2(b), indicating an emergent ability in CPU state modelling. This leap, especially in BPB and accuracy when comparing \\(\\mathrm{bGPT}^{4}\\) and \\(\\mathrm{bGPT}^{5}\\), suggests a deeper understanding of CPU states may stem from a qualitative enhancement in capability. This aligns with the concept of emergent abilities in large LMs (Wei et al., 2022), where capabilities seem to spontaneously arise with scale and complexity.\n' +
      '\n' +
      'However, scepticism exists regarding whether these improvements reflect genuine learning (Schaeffer et al., 2023). Critics argue the performance boosts might be due to nonlinear metrics or overfitting. Nonetheless, the linear and smooth nature of BPB counters this, indicating that the improvements likely stem from a real comprehension of CPU operations, suggesting consistent learning rather than metric anomalies.\n' +
      '\n' +
      'In summary, bGPT demonstrates strong scalability on native binary data with emergent abilities in data conversion and CPU state modelling, which illuminate its potent capabilities in algorithm and hardware simulation tasks. While the tasks utilized for demonstration in this study were not excessively complicated, the near-perfect performance observed in these contexts points to the broader potential of byte models for simulating and reverse-engineering a wide range of algorithms and hardware.\n' +
      '\n' +
      '## 6 Conclusions\n' +
      '\n' +
      'In this paper, we present bGPT as a versatile simulator for the digital world, extending deep learning to binary data processing via next byte prediction. Our experiments demonstrate the effectiveness of bGPT in modelling digital media data, which showcases modality-agnostic knowledge transfer. We observe a strong scalability of bGPT in modelling native binary data and even signs of emergent abilities. bGPT performs comparably to specialized models across diverse datasets without modality-specific designs, and excels in data conversion and CPU state modelling, demonstrating its potential for simulating various algorithms and hardware.\n' +
      '\n' +
      'Nonetheless, our experiments illuminate opportunities for improvement. In this study, we confine the modelling to short audio segments and low-resolution images, a consequence of the resource-intensive nature intrinsic to byte models. Due to limited computational resources, we only investigated data conversion between ABC notation and MIDI, without broader assessments across alternate formats. Furthermore, to simplify data collection and evaluation, our CPU state modelling experiments focused solely on simplified CPUs, omitting the use of real modern CPUs, which are considerably more complex.\n' +
      '\n' +
      'Future research directions for byte models include: 1) reducing computational cost to make training byte models more feasible; 2) scaling models and dataset sizes to accommodate a broader range of native binary data, as well as handling larger digital media files such as high-resolution images and videos; and 3) improving model performance, particularly for underexplored tasks involving native binary data across diverse application domains.\n' +
      '\n' +
      '## 7 Impact Statements\n' +
      '\n' +
      'In this paper, we introduce bGPT, a model designed for binary data processing and digital world modelling, pushing the boundaries of deep learning into the realm of native binary data. This innovation enables bGPT to directly interpret and manipulate binary data, offering profound insights into digital systems. While bGPT presents advancements in understanding and modelling the digital world, it also necessitates a careful examination of its ethical implications and potential impact on societal norms and legal frameworks.\n' +
      '\n' +
      'Its ability to simulate or reverse-engineer algorithms and hardware has two major implications: 1) it can significantly boost technological innovation, aiding in the development of cybersecurity, software, and hardware by understanding and improving on existing technologies; 2) it poses a risk to intellectual property, as training bGPT on extensive datasets of paired source code and executable software, might enable the reverse-engineering of proprietary software. This capability, while showcasing its potential, could facilitate unauthorized access to or modification of software, raising security and legal issues.\n' +
      '\n' +
      'In conclusion, while bGPT and similar byte models offer exciting opportunities for advancing our understanding and capabilities within the digital world, their deployment requires thoughtful consideration of the ethical, societal, and legal implications. This is especially crucial for safeguarding intellectual property and ensuring security against potential malicious use.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Ao et al. (2022) Ao, J., Wang, R., Zhou, L., Wang, C., Ren, S., Wu, Y., Liu, S., Ko, T., Li, Q., Zhang, Y., Wei, Z., Qian, Y., Li, J., and Wei, F. Speech5: Unified-modal encoder-decoder pre-training for spoken language processing. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pp. 5723-5738. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.393. URL [https://doi.org/10.18653/v1/2022.acl-long.393](https://doi.org/10.18653/v1/2022.acl-long.393).\n' +
      '* Boros et al. (2023) Boros, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Roblek, D., Teboul, O., Granigier, D., Tagliasacchi, M., and Zeghidour, N. Audiolm: A language modeling approach to audio generation. _IEEE ACM Trans. Audio Speech Lang. Process._, 31:2523-2533, 2023. doi: 10.1109/TASLP.2023.3288409. URL [https://doi.org/10.1109/TASLP.2023.32888409](https://doi.org/10.1109/TASLP.2023.32888409).\n' +
      '* Brown et al. (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).\n' +
      '* Bubeck et al. (2023) Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S. M., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang, Y. Sparks of artificial general intelligence: Early experiments with GPT-4. _CoRR_, abs/2303.12712, 2023. doi: 10.48550/ARXIV.2303.12712. URL [https://doi.org/10.48550/arXiv.2303.12712](https://doi.org/10.48550/arXiv.2303.12712).\n' +
      '* Chen et al. (2020) Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. Generative pretraining from pixels. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pp. 1691-1703. PMLR, 2020. URL [http://proceedings.mlr.press/v119/chen20s.html](http://proceedings.mlr.press/v119/chen20s.html).\n' +
      '* Copet et al. (2023) Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y., and Defossez, A. Simple and controllable music generation. _CoRR_, abs/2306.05284, 2023. doi: 10.48550/ARXIV.2306.05284. URL [https://doi.org/10.48550/arXiv.2306.05284](https://doi.org/10.48550/arXiv.2306.05284).\n' +
      '* Defossez et al. (2022) Defossez, A., Copet, J., Synnaeve, G., and Adi, Y. High fidelity neural audio compression. _CoRR_, abs/2210.13438, 2022. doi: 10.48550/ARXIV.2210.13438. URL [https://doi.org/10.48550/arXiv.2210.13438](https://doi.org/10.48550/arXiv.2210.13438).\n' +
      '* Deletang et al. (2009) Deletang, G., Ruoss, A., Duquenne, P., Catt, E., Genewein, T., Mattern, C., Grau-Moya, J., Wenliang, L. K., Aitchison, M., Orseau, L., Hutter, M., and Veness, J. Language modeling is compression. _CoRR_, abs/2309.10668, 2023. doi: 10.48550/ARXIV.2309.10668. URL [https://doi.org/10.48550/arXiv.2309.10668](https://doi.org/10.48550/arXiv.2309.10668).\n' +
      '* Deng et al. (2009) Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In _2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA_, pp. 248-255. IEEE Computer Society, 2009. doi: 10.1109/CVPR.2009.5206848.\n' +
      '\n' +
      'URL [https://doi.org/10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848).\n' +
      '* Devlin et al. (2018) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* Dosovitskiy et al. (2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL [https://openreview.net/forum?id=YicbFdNTTY](https://openreview.net/forum?id=YicbFdNTTY).\n' +
      '* Ferruz et al. (2022) Ferruz, N., Schmidt, S., and Hocker, B. Protgpt2 is a deep unsupervised language model for protein design. _Nature Communications_, 13:4348, 2022. doi: 10.1038/s41467-022-32007-7. URL [https://doi.org/10.1038/s41467-022-32007-7](https://doi.org/10.1038/s41467-022-32007-7).\n' +
      '* 3 September 2021_, pp. 571-575. ISCA, 2021. doi: 10.21437/INTERSPEECH.2021-698. URL [https://doi.org/10.21437/Interspeech.2021-698](https://doi.org/10.21437/Interspeech.2021-698).\n' +
      '* Gu & Dao (2023) Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. _CoRR_, abs/2312.00752, 2023. doi: 10.48550/ARXIV.2312.00752. URL [https://doi.org/10.48550/arXiv.2312.00752](https://doi.org/10.48550/arXiv.2312.00752).\n' +
      '* Guo et al. (2019) Guo, W., Mu, D., Xing, X., Du, M., and Song, D. DEEPVSA: facilitating value-set analysis with deep learning for postmortem program analysis. In Heninger, N. and Traynor, P. (eds.), _28th USENIX Security Symposium, USENIX Security 2019, Santa Clara, CA, USA, August 14-16, 2019_, pp. 1787-1804. USENIX Association, 2019. URL [https://www.usenix.org/conference/usenixsecurity19/presentation/guo](https://www.usenix.org/conference/usenixsecurity19/presentation/guo).\n' +
      '* He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 770-778, 2016.\n' +
      '* Hochreiter & Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. Long short-term memory. _Neural Comput._, 9(8):1735-1780, 1997. doi: 10.1162/NECO.1997.9.8.1735. URL [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735).\n' +
      '* Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training compute-optimal large language models. _CoRR_, abs/2203.15556, 2022. doi: 10.48550/ARXIV.2203.15556. URL [https://doi.org/10.48550/arXiv.2203.15556](https://doi.org/10.48550/arXiv.2203.15556).\n' +
      '* Horton et al. (2023) Horton, M., Mehta, S., Farhadi, A., and Rastegari, M. Bytes are all you need: Transformers operating directly on file bytes. _CoRR_, abs/2306.00238, 2023. doi: 10.48550/ARXIV.2306.00238. URL [https://doi.org/10.48550/arXiv.2306.00238](https://doi.org/10.48550/arXiv.2306.00238).\n' +
      '* Hsiao et al. (2021) Hsiao, W., Liu, J., Yeh, Y., and Yang, Y. Compound word transformer: Learning to compose full-song music over dynamic directed hypergraphs. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pp. 178-186. AAAI Press, 2021. doi: 10.1609/AAAI.V3511.16091. URL [https://doi.org/10.1609/aaai.v35i1.16091](https://doi.org/10.1609/aaai.v35i1.16091).\n' +
      '* Huang et al. (2019) Huang, C. A., Vaswani, A., Uszkoreit, J., Simon, I., Hawthorne, C., Shazeer, N., Dai, A. M., Hoffman, M. D., Dinculescu, M., and Eck, D. Music transformer: Generating music with long-term structure. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL [https://openreview.net/forum?id=rJe4ShAcF7](https://openreview.net/forum?id=rJe4ShAcF7).\n' +
      '* Krizhevsky et al. (2009) Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.\n' +
      '* Kudo (2018) Kudo, T. Subword regularization: Improving neural network translation models with multiple subword candidates. In Gurevych, I. and Miyao, Y. (eds.), _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers_, pp. 66-75. Association for Computational Linguistics, 2018. doi: 10.18653/V1/P18-1007. URL [https://aclanthology.org/P18-1007/](https://aclanthology.org/P18-1007/).\n' +
      '* Kudo & Richardson (2018) Kudo, T. and Richardson, J. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Blanco, E. and Lu, W. (eds.), _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, Brussels,Belgium, October 31 - November 4, 2018_, pp. 66-71. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-2012. URL [https://doi.org/10.18653/v1/d18-2012](https://doi.org/10.18653/v1/d18-2012).\n' +
      '* LeCun et al. (1989) LeCun, Y., Boser, B. E., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. E., and Jackel, L. D. Handwritten digit recognition with a back-propagation network. In Touretzky, D. S. (ed.), _Advances in Neural Information Processing Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989]_, pp. 396-404. Morgan Kaufmann, 1989. URL [http://papers.nips.cc/paper/293-handwritten-digit-recognition-with-a-back-propagation-network](http://papers.nips.cc/paper/293-handwritten-digit-recognition-with-a-back-propagation-network).\n' +
      '* Li et al. (2023) Li, J., Li, D., Savarese, S., and Hoi, S. C. H. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pp. 19730-19742. PMLR, 2023. URL [https://proceedings.mlr.press/v202/li23q.html](https://proceedings.mlr.press/v202/li23q.html).\n' +
      '* Liu et al. (2023) Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. _CoRR_, abs/2304.08485, 2023. doi: 10.48550/ARXIV.2304.08485. URL [https://doi.org/10.48550/arXiv.2304.08485](https://doi.org/10.48550/arXiv.2304.08485).\n' +
      '* December 9, 2022_, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/dlc88f9790765146ec8fb5d02e5653a0-Abstract_html](http://papers.nips.cc/paper_files/paper/2022/hash/dlc88f9790765146ec8fb5d02e5653a0-Abstract_html).\n' +
      '* Nguyen et al. (2023) Nguyen, E., Poli, M., Faizi, M., Thomas, A. W., Birch-Sykes, C., Wornow, M., Patel, A., Rabideau, C. M., Massaroli, S., Bengio, Y., Ermon, S., Baccus, S. A., and Re, C. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. _CoRR_, abs/2306.15794, 2023. doi: 10.48550/ARXIV.2306.15794. URL [https://doi.org/10.48550/arXiv.2306.15794](https://doi.org/10.48550/arXiv.2306.15794).\n' +
      '* Notin et al. (2018) Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A. N., Marks, D. S., and Gal, Y. Tranception: Protein fitness prediction with autoregressive transformers and inference-time retrieval. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pp. 16990-17017. PMLR, 2022. URL [https://proceedings.mlr.press/v162/notin22a.html](https://proceedings.mlr.press/v162/notin22a.html).\n' +
      '* Oord et al. (2016) Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. Wavenet: A generative model for raw audio. _arXiv preprint arXiv:1609.03499_, 2016.\n' +
      '* Panayotov et al. (2015) Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeco-an ASR corpus based on public domain audio books. In _2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015_, pp. 5206-5210. IEEE, 2015. doi: 10.1109/ICASSP.2015.7178964. URL [https://doi.org/10.1109/ICASSP.2015.7178964](https://doi.org/10.1109/ICASSP.2015.7178964).\n' +
      '* Peters et al. (2018) Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. In Walker, M. A., Ji, H., and Stent, A. (eds.), _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)_, pp. 2227-2237. Association for Computational Linguistics, 2018. doi: 10.18653/V1/N18-1202. URL [https://doi.org/10.18653/v1/n18-1202](https://doi.org/10.18653/v1/n18-1202).\n' +
      '* Radford et al. (2018) Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding by generative pre-training. 2018.\n' +
      '* Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n' +
      '* Raff et al. (2018) Raff, E., Barker, J., Sylvester, J., Brandon, R., Catanzaro, B., and Nicholas, C. K. Malware detection by eating a whole EXE. In _The Workshops of the The Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018_, volume WS-18 of _AAAI Technical Report_, pp. 268-276. AAAI Press, 2018. URL [https://aaai.org/ocs/index.php/WS/AAAIWI8/paper/view/16422](https://aaai.org/ocs/index.php/WS/AAAIWI8/paper/view/16422).\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with CLIP latents. _CoRR_, abs/2204.06125, 2022. doi: 10.48550/ARXIV.2204.06125. URL [https://doi.org/10.48550/arXiv.2204.06125](https://doi.org/10.48550/arXiv.2204.06125).\n' +
      '\n' +
      'Rubenstein, P. K., Asawaorengchai, C., Nguyen, D. D., Bapna, A., Borosos, Z., de Chaumont Quitry, F., Chen, P., Badawy, D. E., Han, W., Kharitonov, E., Muckenhirn, H., Padfield, D., Qin, J., Rozenberg, D., Sainath, T. N., Schalkwyk, J., Sharifi, M., Ramanovich, M. T., Tagliascachi, M., Tudor, A., Veilimirovic, M., Vincent, D., Yu, J., Wang, Y., Zayats, V., Zeghidour, N., Zhang, Y., Zhang, Z., Zilka, L., and Frank, C. H. Audiopalm: A large language model that can speak and listen. _CoRR_, abs/2306.12925, 2023. doi: 10.48550/ARXIV.2306.12925. URL [https://doi.org/10.48550/arXiv.2306.12925](https://doi.org/10.48550/arXiv.2306.12925).\n' +
      '* Schaeffer et al. (2023) Schaeffer, R., Miranda, B., and Koyejo, S. Are emergent abilities of large language models a mirage? In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL [https://openreview.net/forum?id=ITw9edRD1D](https://openreview.net/forum?id=ITw9edRD1D).\n' +
      '* Sennrich et al. (2016) Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers_. The Association for Computer Linguistics, 2016. doi: 10.18653/V1/P16-1162. URL [https://doi.org/10.18653/v1/p16-1162](https://doi.org/10.18653/v1/p16-1162).\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models. _CoRR_, abs/2302.13971, 2023a. doi: 10.48550/ARXIV.2302.13971. URL [https://doi.org/10.48550/arXiv.2302.13971](https://doi.org/10.48550/arXiv.2302.13971).\n' +
      '* Touvron et al. (2022) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Canton-Ferrer, C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models. _CoRR_, abs/2307.09288, 2023b. doi: 10.48550/ARXIV.2307.09288. URL [https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288).\n' +
      '* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* Wang et al. (2020) Wang, C., Cho, K., and Gu, J. Neural machine translation with byte-level subwords. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pp. 9154-9160. AAAI Press, 2020. doi: 10.1609/AAAI.V34105.6451. URL [https://doi.org/10.1609/aaai.v34i05.6451](https://doi.org/10.1609/aaai.v34i05.6451).\n' +
      '* Wang et al. (2023) Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F. Neural codec language models are zero-shot text to speech synthesizers. _CoRR_, abs/2301.02111, 2023. doi: 10.48550/ARXIV.2301.02111. URL [https://doi.org/10.48550/arXiv.2301.02111](https://doi.org/10.48550/arXiv.2301.02111).\n' +
      '* Wang et al. (2024) Wang, J., Gangavarapu, T., Yan, J. N., and Rush, A. M. Mambabyte: Token-free selective state space model, 2024.\n' +
      '* Warden (2018) Warden, P. Speech commands: A dataset for limited-vocabulary speech recognition. _CoRR_, abs/1804.03209, 2018. URL [http://arxiv.org/abs/1804.03209](http://arxiv.org/abs/1804.03209).\n' +
      '* Wei et al. (2022) Wei, J., Liu, Q., Guo, Y., and Jiang, X. Training multilingual pre-trained language model with byte-level subwords. _arXiv preprint arXiv:2101.09469_, 2021.\n' +
      '* Wei et al. (2022) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. Emergent abilities of large language models. _Trans. Mach. Learn. Res._, 2022, 2022. URL [https://openreview.net/forum?id=yzkSU5zdwD](https://openreview.net/forum?id=yzkSU5zdwD).\n' +
      '* Wikimedia downloads (2022) Wikimedia. Wikimedia downloads, 2022. URL [https://dumps.wikimedia.org](https://dumps.wikimedia.org).\n' +
      '* Wu et al. (2023) Wu, S., Li, X., Yu, F., and Sun, M. Tunesformer: Forming irish tunes with control codes by bar patching. In Porcaro, L., Battle-Roca, R., and Gomez, E. (eds.), _Proceedings of the 2nd Workshop on Human-Centric Music Information Retrieval 2023 co-located with the 24th International Society for Music Information Retrieval Conference (ISMIR 2023), Milan, Italy, November 10, 2023_, volume 3528 of _CEUR Workshop Proceedings_. CEUR-WS.org, 2023a. URL [https://ceur-ws.org/Vol-3528/paper1.pdf](https://ceur-ws.org/Vol-3528/paper1.pdf).\n' +
      '\n' +
      'Wu, S., Yu, D., Tan, X., and Sun, M. Clamp: Contrastive language-music pre-training for cross-modal symbolic music information retrieval. In Sarti, A., Antonacci, F., Sandler, M., Bestagini, P., Dixon, S., Liang, B., Richard, G., and Pauwels, J. (eds.), _Proceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR 2023, Milan, Italy, November 5-9, 2023_, pp. 157-165, 2023b. doi: 10.5281/ZENODO.10265247. URL [https://doi.org/10.5281/zenodo.10265247](https://doi.org/10.5281/zenodo.10265247).\n' +
      '* Wu et al. (2016) Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., and Dean, J. Google\'s neural machine translation system: Bridging the gap between human and machine translation. _CoRR_, abs/1609.08144, 2016. URL [http://arxiv.org/abs/1609.08144](http://arxiv.org/abs/1609.08144).\n' +
      '* Xue et al. (2022) Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., and Raffel, C. Byt5: Towards a token-free future with pre-trained byte-to-byte models. _Trans. Assoc. Comput. Linguistics_, 10:291-306, 2022. doi: 10.1162/TACL\\_A\\_00461. URL [https://doi.org/10.1162/tacl_a_00461](https://doi.org/10.1162/tacl_a_00461).\n' +
      '* Yang et al. (2023) Yang, D., Tian, J., Tan, X., Huang, R., Liu, S., Chang, X., Shi, J., Zhao, S., Bian, J., Wu, X., et al. Uniaudio: An audio foundation model toward universal audio generation. _arXiv preprint arXiv:2310.00704_, 2023.\n' +
      '* Yu et al. (2023) Yu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer, L., and Lewis, M. MEGABYTE: Predicting million-byte sequences with multiscale transformers. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL [https://openreview.net/forum?id=JTmO2V9Xpz](https://openreview.net/forum?id=JTmO2V9Xpz).\n' +
      '* Zeng et al. (2021) Zeng, M., Tan, X., Wang, R., Ju, Z., Qin, T., and Liu, T. Musicbert: Symbolic music understanding with large-scale pre-training. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), _Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021_, volume ACL/IJCNLP 2021 of _Findings of ACL_, pp. 791-800. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021. FINDINGS-ACL.70. URL [https://doi.org/10.18653/v1/2021.findings-acl.70](https://doi.org/10.18653/v1/2021.findings-acl.70).\n' +
      '* Zhang et al. (2015) Zhang, X., Zhao, J. J., and LeCun, Y. Character-level convolutional networks for text classification. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pp. 649-657, 2015. URL [https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html).\n' +
      '* Zhu et al. (2023) Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _CoRR_, abs/2304.10592, 2023. doi: 10.48550/ARXIV.2304.10592. URL [https://doi.org/10.48550/arXiv.2304.10592](https://doi.org/10.48550/arXiv.2304.10592).\n' +
      '\n' +
      '## Appendix A Symbolic Music Formats\n' +
      '\n' +
      '### ABC Notation\n' +
      '\n' +
      'ABC notation is a shorthand form of music notation that uses ASCII characters to represent musical notes and symbols. It is a compact, human-readable, and easily typed format that has gained popularity for its ability to facilitate the swift digital transcription, sharing, and editing of music. Originating within folk and traditional music circles, ABC notation has become a universal method for musicians and enthusiasts to exchange music without the need for proprietary software or extensive musical training.\n' +
      '\n' +
      'To illustrate the fundamentals of ABC notation, consider the example below, which depicts a folk tune:\n' +
      '\n' +
      '```\n' +
      'X:1 L:1/8 M:3/4 K:D de |"D" f3 g a(/g)f | "A" e4 AB |"C" =c3 d"G/B" B(/A)G |"A" A4 fg | "D" a3 g fd | "A" e4 AB |"C" =c3 d e(/f)g | "D" f4 :i d(/ed)cA) | "Bm" B2 A2 F2 | "F#m" A3 B d(/edc)A | "C" B2 A2 F2 | "A" (E4 E)A |"Bm" B3 A FD | "F#m" C3 D (F/G/A) | "G" B3 e"A" dc | "D" d4 :i\n' +
      '```\n' +
      '\n' +
      'The corresponding sheet music rendered from this ABC sequence is shown in Fig. 4. In this example, the structure of ABC notation starts with a header that includes:\n' +
      '\n' +
      '* **X (Tune Number)**: This is a unique number assigned to the tune, serving as a reference in collections or databases.\n' +
      '* **L (Unit Note Length)**: This defines the default duration of notes. In this context, 1/8 indicates that each note is an eighth note, which is a standard choice for folk tunes.\n' +
      '* **M (Meter)**: This denotes the time signature of the piece, which determines the rhythmic pulse. Here, 3/4 suggests a waltz-like rhythm.\n' +
      '* **K (Key)**: This sets the key signature for the tune. D signifies that the piece is in the key of D major.\n' +
      '\n' +
      'After the header, the body of the ABC notation consists of the musical notes themselves. Each note is represented by a letter (A to G), which corresponds to the pitch. The octaves are indicated by lower-cased letters or apostrophes for higher octaves and upper-cased letters or commas for lower or higher octaves, respectively. The duration of notes is modified by numbers; for instance, a 2 following a note would double its value, while a /2 would halve it. Chords can be notated by enclosing the\n' +
      '\n' +
      'Figure 4: Sheet music example corresponding to the ABC notation of a folk tune in D major.\n' +
      '\n' +
      'chord name in quotation marks to suggest harmony, such as "F#m" representing the F sharp minor. Additional symbols are used for sharps \\(\\hat{\\ }\\), flats \\(\\_\\), and naturals \\(\\_\\) to alter pitch, and various other characters represent articulations and expressions.\n' +
      '\n' +
      'This notation describes the tune with sufficient information, from its rhythmic structure down to the details of its melody and harmony. As such, ABC notation serves as a bridge between the traditional form of musical scores and the modern digital world, offering a text-based alternative that is both efficient and widely accessible. For a detailed documentation of ABC notation, please refer to the ABC notation standard2.\n' +
      '\n' +
      'Footnote 2: [https://abcnotation.com/wiki/abc:standard:v2.1](https://abcnotation.com/wiki/abc:standard:v2.1)\n' +
      '\n' +
      '### Midi\n' +
      '\n' +
      'MIDI, short for Musical Instrument Digital Interface, is a technical standard that describes a protocol, digital interface, and connectors and allows a wide variety of electronic musical instruments, computers, and other related devices to connect and communicate with one another. Unlike ABC notation, which is designed for human readability and the transcription of musical scores, MIDI is focused on capturing and conveying information about music performance and production in a format that machines can interpret and process.\n' +
      '\n' +
      'The core of MIDI communication involves MIDI messages, which are digital signals that represent various musical elements such as notes, pitch, velocity (how hard a note is played), control changes (modifications to sound parameters), and program changes (instrument changes). These messages do not transmit audio; instead, they send instructions about how music should be played or produced. This approach allows MIDI to be incredibly versatile and lightweight compared to raw audio, suitable for real-time control during live performances and detailed editing in studio environments.\n' +
      '\n' +
      'MIDI files, commonly with the extension.mid or.mid, store sequences of MIDI messages that can be played back by computer software or electronic instruments. These files make it possible to share and distribute musical compositions in a form that can be easily modified, transposed, or played back with different instrument sounds, offering a level of flexibility and creative freedom that is unique to the digital music domain.\n' +
      '\n' +
      'Figure 5: Sheet music example corresponding to the MIDI of the same folk tune presented in Fig. 4.\n' +
      '\n' +
      'There are several ways to create and edit MIDI, each catering to different workflows and preferences:\n' +
      '\n' +
      '* **MIDI Keyboards and Controllers**: Physical devices that allow musicians to input notes and control data into a computer. They emulate traditional instruments and can capture the nuances of a performance in MIDI format.\n' +
      '* **Piano Roll**: A graphical interface in digital audio workstations (as shown in Fig. 1) where notes are displayed and edited on a grid representing pitch and time. It is intuitive for composing melodies, chords, and rhythms.\n' +
      '* **Score Editor**: Allows users to input and edit music in traditional notation, converting written notes into MIDI data. Ideal for those familiar with musical notation.\n' +
      '* **Programming and Scripting**: Involves writing code to generate and manipulate MIDI data, offering complex and generative musical possibilities beyond manual input methods. For instance, complex MIDI sequences and automation can be generated using software such as Max/MSP, Pure Data, or programming languages (e.g., SuperCollider).\n' +
      '\n' +
      'Although MIDI offers immense benefits for musicians and producers, when converting MIDI data back into score-oriented symbolic music formats like ABC notation, certain challenges arise due to the intrinsic differences between these formats. MIDI excels at capturing how a piece of music is played, including the timing, velocity, and expressiveness of each note. In contrast, ABC notation focuses on the structural elements of music, such as melody, harmony, and rhythm, using a textual format that is more accessible for human reading and writing but less detailed in terms of performance nuances.\n' +
      '\n' +
      'To illustrate the conversion between MIDI and ABC notation, consider the following example:\n' +
      '\n' +
      '```\n' +
      'X:l L:1/8 M:3/4 K:D def3 g |ag/<f/ e4 | AB =c3 d |BA/<G/ A4 | fga a3 g |fd e4 AB =c3 d |ef/<g/ f4 | def3 g |ag/<f/ e4 | AB =c3 d |BA/<G/ A4 | fga a3 g |fd e4 | AB =c3 d |ef/<g/ f4 | de/4d/c/4/4/4 2B A2 | F2 A3 B | def/4d/c/4/A2 B2 A | F2 E4 | EA B3 A | PD C3 D | F/G/A B3 | dc d4 | def/4d/c4/A/4 B2 A2 | F2 A3 B | def/4d/4c/4A/4 B2 A2 | F2 A3 B | def/4d/4c/4A/4 B2 A2 | F2 E4 | EA B3 A | FD C3 D | F/G/A B3 e | dcd d4 |\n' +
      '```\n' +
      '\n' +
      'This is the same tune we have demonstrated in Fig, 4, but it has now been converted from ABC notation to MIDI and subsequently back to ABC notation. The corresponding sheet music, rendered from this ABC sequence, is shown in Fig. 5. One can observe that the ABC sequence and the musical score appear significantly longer than the original due to the elimination of repeat signs in MIDI, requiring repeated sections to be explicitly written out. Additionally, the bar divisions and the ornaments in the MIDI-to-ABC conversion might not align perfectly with the original ABC notation. These ornaments are often represented as a series of rapid notes in MIDI, which can complicate their translation back into ABC notation.\n' +
      '\n' +
      'While MIDI-rendered sheet music and its ABC notation counterpart may sound identical when performed, the visual complexity of the MIDI transcription can make it more challenging to read than the ABC version. This discrepancy underscores the fact that MIDI and ABC are formats tailored for different contexts. MIDI is adept at capturing nuanced performance data, such as dynamics and subtle timing variations, which are essential for playback fidelity but are not as visually straightforward as ABC notation. On the other hand, ABC notation, with its simplified symbolic representation, is not ideal for conveying detailed performance information.\n' +
      '\n' +
      'Both formats have overlapping capabilities in representing musical information, yet each also possesses unique characteristics that do not translate perfectly when converting from one to the other. This results in a loss of information: nuances specific to each format can be diminished or omitted entirely during the conversion process. Such losses present a challenge in preserving the full intent and detail of a musical piece when moving between ABC and MIDI formats.\n' +
      '\n' +
      '## Appendix B CPU States Dataset\n' +
      '\n' +
      'The CPU States Dataset has been crafted using a Python script to simulate the operations of a CPU, offering a controlled environment to assess the capabilities of bGPT in modelling the hardware behaviour. This appendix offers an overview of the dataset, introducing the structure, instruction set, and details of the dataset curation.\n' +
      '\n' +
      'The dataset comprises entries of 1KB memory blocks, each paired with a subsequent 16-byte sequence representing the CPU register states. The memory blocks encode varying numbers of machine instructions, while the register state sequence includes the CPU status post-execution of each instruction. The CPU registers in each state consist of:\n' +
      '\n' +
      '* **Program Counter (PC)**: 1 byte in size, points to the next instruction in the memory.\n' +
      '* **Accumulator (ACC)**: A 1-byte register involved in arithmetic and logic operations.\n' +
      '* **Instruction Register (IR)**: Occupies 4 bytes, storing the currently executed instruction.\n' +
      '* **General-Purpose Registers**: An additional 10 bytes are allocated, emulating a range of registers (labelled A-J) used for various computational purposes.\n' +
      '\n' +
      'Transitions between states mimic the execution cycle of a CPU, including fetching instructions from memory, decoding them to determine the required operation, and executing the operation to modify the state. The execution process includes instructions for modifying register values, performing arithmetic calculations, and controlling the program flow.\n' +
      '\n' +
      'Each instruction in the dataset is encoded in a four-byte format OP ADDR1 ADDR2 ADDR3, with each byte serving a specific purpose:\n' +
      '\n' +
      '* **OP (Operation Code)**: 1 byte representing the opcode of the instruction to be executed.\n' +
      '* **ADDR1, ADDR2, ADDR3**: 1 byte each, representing up to three addresses that the instruction may interact with.\n' +
      '\n' +
      'The opcode and addresses together describe the action to be taken by the CPU. The instructions vary in terms of the number of addresses they use, from zero-address to three-address instructions. This variability allows the simulation of a wide range of operations, from simple data movement to complex arithmetic and logical functions.\n' +
      '\n' +
      'Table 7 (located on page 19) details the instruction set and the variants each instruction can have, depending on the values in the address fields. The variant to be executed is determined by the non-zero value in the address fields. For zero-address instructions, all address fields are zero. For single-address instructions, ADDR1 holds the address of the register involved, and the other fields are zero. For two- and three-address instructions, the respective fields hold the addresses of the registers involved in the operation.\n' +
      '\n' +
      'In designing the instruction set, complex instructions like jump commands were deliberately excluded to prevent the occurrence of infinite loops or other complexities that could hinder straightforward simulation.\n' +
      '\n' +
      'The Python script generates the CPU States Dataset by randomly initializing the ACC and general-purpose registers for each simulation, thereby ensuring a wide range of starting conditions. It then executes a random sequence of instructions, varying in length from 1 to 256, with the effect of each instruction on the CPU state being recorded. This results in a sequence of states that reflects the cumulative impact of the executed instructions. The script guarantees that each sequence culminates with a halt instruction HLT, marking the end of a program. Furthermore, when considering only the register changes related to instructions, there are over 516 million potential scenarios resulting from combinations of various instruction types, registers, and their values.\n' +
      '\n' +
      'The dataset thus produced comprises 2.1 million entries for the training set and 21,000 entries for the evaluation set. This process, with its random initial states and varied instruction lengths, produces a diverse dataset that is essential for training and evaluating the effectiveness of bGPT in modelling CPU states.\n' +
      '\n' +
      'To aid in a more intuitive comprehension of the operational logic of the CPU, a randomly generated program for executing 10 instructions along with their corresponding states in human-readable form is provided below:\n' +
      '\n' +
      'Program: [\'MUL J A\', \'DIV I\', \'MUL E D\', \'ADD C\', \'LOAD1 66\', \'MOV A H\', \'AND D E\', \'Pop H\', \'CLR\', \'HLT\']\n' +
      '\n' +
      'State at step 0:\n' +
      '\n' +
      'PC: 0\n' +
      '\n' +
      'ACC: 146\n' +
      '\n' +
      'IR: HLT\n' +
      '\n' +
      'Registers: [\'A\':= 19, \'B\':= 55, \'C\':= 245, \'D\':= 35, \'E\':= 174, \'F\':= 185, \'G\': 9, \'H\': 20, \'I\':= 140, \'J\':= 2]\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      'The dataset is intended for use in machine learning research, particularly for developing models capable of predicting CPU behaviour. It can also serve as an educational tool, helping students understand CPU operation principles.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|p{56.9pt}|p{56.9pt}|p{284.5pt}|} \\hline\n' +
      '**Instruction** & **Address Variants** & **Meaning** \\\\ \\hline HLT & 0 & Halts the CPU with “HLT” \\\\ \\hline CLR & 0, 1 & Zeroes the accumulator with “CLR” or a specific register with “CLR A” \\\\ \\hline INC & 0, 1 & Increments the accumulator with “INC” or a specific register with “INC A” \\\\ \\hline DEC & 0, 1 & Decrements the accumulator with “DEC” or a specific register with “DEC A” \\\\ \\hline SHL & 0, 1 & Shifts the accumulator left by one bit with “SHL” or a specific register with “SHL A” \\\\ \\hline SHR & 0, 1 & Shifts the accumulator right by one bit with “SHR” or a specific register with “SHR A” \\\\ \\hline ROL & 0, 1 & Rotates the accumulator left by one bit with “ROL” or a specific register with “ROL A” \\\\ \\hline ROR & 0, 1 & Rotates the accumulator right by one bit with “ROR” or a specific register with “ROR A” \\\\ \\hline NOT & 0, 1 & Performs bitwise NOT on the accumulator with “NOT” or a specific register with “NOT A” \\\\ \\hline PUSH & 1 & Pushes the value of a specific register onto the stack with “PUSH A” \\\\ \\hline POP & 1 & Pops the top of the stack into a specific register with “POP A” \\\\ \\hline LOADI & 1 & Loads an immediate value into the accumulator with “LOADI value” \\\\ \\hline SWAP & 1, 2 & Swaps values between the accumulator and a register with “SWAP A” or between two registers with “SWAP A B” \\\\ \\hline ADD & 1, 2, 3 & Adds using one, two, or three addresses with examples “ADD A”, “ADD A B”, “ADD A B C” \\\\ \\hline SUB & 1, 2, 3 & Subtracts using one, two, or three addresses with examples “SUB A”, “SUB A B”, “SUB A B C” \\\\ \\hline MUL & 1, 2, 3 & Multiplies using one, two, or three addresses with examples “MUL A”, “MUL A B”, “MUL A B C” \\\\ \\hline DIV & 1, 2, 3 & Divides using one, two, or three addresses with examples “DIV A”, “DIV A B”, “DIV A B C” \\\\ \\hline AND & 1, 2, 3 & Performs bitwise AND using one, two, or three addresses with examples “AND A”, “AND A B”, “AND A B C” \\\\ \\hline OR & 1, 2, 3 & Performs bitwise OR using one, two, or three addresses with examples “OR A”, “OR A B”, “OR A B C” \\\\ \\hline XOR & 1, 2, 3 & Performs bitwise XOR using one, two, or three addresses with examples “XOR A”, “XOR A B”, “XOR A B C” \\\\ \\hline MOV & 2 & Moves a value from one register to another with “MOV A B” \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Defined instruction set and the address variants of different instructions.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>