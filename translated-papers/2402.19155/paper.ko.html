<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '알고리즘과 하드웨어 시뮬레이션을 각각 나타내는 데이터 변환 및 CPU 상태 모델링을 포함한 이진 고유 작업에 내재된 미해결 작업. 바이트 모델에 대한 연구는 전체적이고 통합된 딥 러닝을 향한 중요한 단계를 나타낼 뿐만 아니라 디지털 세계를 모델링하는 새로운 관점을 제공한다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '### Language Models\n' +
      '\n' +
      'LMs는 이전 LSTM 기반 모델(Hochreiter and Schmidhuber, 1997)에서 최근 트랜스포머 기반 모델(Vaswani et al., 2017)에 이르기까지 인간 지능을 시뮬레이션하면서 인간 언어를 이해하고 생성하는 데 중요하다. 텍스트 토큰화는 텍스트를 단어 또는 서브워드(Sennrich et al., 2016; Wu et al., 2016; Kudo, 2018; Kudo and Richardson, 2018)와 같은 더 작은 단위로 분해하는 것을 포함하기 때문에 이러한 모델에서 근본적인 역할을 한다. GPT(Generative Pre-trained Transformer) 모델(Radford et al., 2018; 2019; Brown et al., 2020)의 도입은 이 분야에서 상당한 발전을 나타낸다. GPT 모델은 특히 광범위한 텍스트 데이터에 대한 다음 토큰 예측을 통해 자체 지도 학습을 통해 사전 훈련된다. 이 훈련 기법인 다음 토큰 예측은 모델이 시퀀스에서 가장 가능성이 높은 다음 토큰을 예측하도록 가르쳐 언어 이면의 구조와 의미를 캡처할 수 있게 한다.\n' +
      '\n' +
      '다음 토큰 예측은 다양한 데이터 유형으로 그 영향력을 확장했다. 오디오 처리에서 AudioPaLM(Rubenstein et al., 2023)과 같은 모델은 텍스트와 음성을 병합하여 음성 대 음성 변환 및 고급 음성 인식을 가능하게 한다. MusicGen(Copet et al., 2023)은 EnCodec(Defossez et al., 2022)에 의해 추출된 음향 토큰의 다중 병렬 스트림을 모델링하여 조건부 음악 생성에 탁월하다. 이미지 처리에서, iGPT(Chen et al., 2020)는 이미지 내의 다음 픽셀을 예측하기 위해 Transformer 모델들을 적용하는 반면, 몇몇 비전-언어 모델들(Liu et al., 2023; Zhu et al., 2023; Li et al., 2023)은 텍스트 데이터와 시각적 데이터 사이의 갭을 메우기 위해 등장하였다. 생화학적 서열에서 Tranception(Notin et al., 2022)은 자가회귀 변압기와 검색을 활용하여 단백질 적합성을 예측하는 반면 ProtGPT2(Ferruz et al., 2022)는 천연 아미노산 특성을 가진 단백질 서열을 생성한다. HyenaDNA(Nguyen et al., 2023)는 게놈 모델링에서 컨텍스트 길이를 확장하여 장거리 서열 이해를 가능하게 한다.\n' +
      '\n' +
      '다음 토큰 예측은 LMs가 인간 지능과 세계의 복잡성을 파악할 수 있는 권한을 부여했다. 이러한 기술을 다음 바이트 예측을 통해 이진 데이터로 확장하면 디지털 정보를 처리하고 디지털 세계를 시뮬레이션하는 데 있어 범용성을 더욱 향상시킬 수 있다.\n' +
      '\n' +
      '### Byte Models\n' +
      '\n' +
      '이진 데이터는 텍스트와 같이 인간이 해석할 수 있는 데이터의 고유한 구조와 의미가 부족하지만, 최근 연구 노력은 그 모델링과 정보 추출을 탐색하여 바이트 모델에 대한 새로운 가능성을 열어주고 있다.\n' +
      '\n' +
      '네이티브 바이너리 데이터를 모델링하여 MalConv(Raff et al., 2018) 및 DeepVSA(Guo et al., 2019)와 같은 시스템이 악성코드 탐지 및 프로그램 분석을 위한 강력한 도구로 등장했다. MalConv는 실행 파일의 원시 바이트 시퀀스를 분석하기 위해 CNN(Convolutional Neural Networks)(LeCun et al., 1989)을 사용하는 반면, DeepVSA는 사후 프로그램 분석을 위해 값 세트 분석의 컨텍스트 내에서 메모리 별칭 분석을 향상시킨다. 추가적으로, 바이트 시퀀스들을 압축하는 언어 모델들의 개념(Deletang et al., 2023)은 얼마나 큰 사전 훈련된 모델들(Hoffmann et al., 2022)이 활용될 수 있는지에 대한 새로운 관점을 도입한다.\n' +
      '\n' +
      '여러 연구에서 언어 작업에 대한 바이트 수준 인코딩의 유용성을 검증했다. 예를 들어, 바이트 레벨 바이트 페어 인코딩(BBPE: Byte-level Byte Pair Encoding)은 다국어 모델 사전 트레이닝(Wei et al., 2021)을 향상시키기 위해 사용되어 왔으며 기계 번역(Wang et al., 2020)에서도 가능성을 보여 처리 효율과 언어적 폭 사이의 균형을 이룬다. ByT5(Xue et al., 2022)는 바이트 시퀀스에 대한 표준 트랜스포머 모델을 사용하여 이를 기반으로 하여, 다국어 시나리오에서 잡음 강건성 및 철자 감도를 향상시키는 토큰-프리 인코딩 방법을 촉진한다.\n' +
      '\n' +
      '그림 1: bGPT 프레임워크는 네이티브 바이너리 데이터를 통해 디지털 시스템을 시뮬레이션하고, 다양한 데이터 유형을 단일 모델로 통합하여 모든 것을 바이트 시퀀스로 처리한다. 이 접근법은 통합을 단순화하고 디지털 세계의 응용 가능성을 확장한다.\n' +
      '\n' +
      '바이트 인코딩은 인간이 해석할 수 있는 다른 데이터에도 적용되어 모델이 보편적인 프레임워크에서 텍스트, 이미지 및 다양한 데이터 유형의 이진 표현과 함께 작동할 수 있다. 예를 들어, ByteFormer(Horton et al., 2023)는 다용도와 프라이버시를 유지하면서 이미지 및 오디오로부터 변환된 원시 바이트 시퀀스를 직접 처리한다. 반면에 메가바이트(Yu et al., 2023)는 다양한 양식에 걸쳐 긴 바이트 시퀀스를 모델링하는 데 탁월하다는 것이 테스트 및 입증되었다. MegaByte(Yu et al., 2023), MambaByte(Wang et al., 2024)에 의해 영감을 받은, Mamba Byte(Wang et al., 2024)는 Mamba 네트워크 구조(Gu & Dao, 2023)를 활용하여 바이트 레벨 언어 모델링에서 탁월하고 심지어 서브워드 토큰화에 기초한 LMs보다 우수하다.\n' +
      '\n' +
      '진보에도 불구하고, 현재 연구는 종종 좁은 작업에 초점을 맞추고 디지털 세계 시뮬레이션에서 바이트 모델의 광범위한 잠재력을 간과하면서 기본 이진 데이터를 무시한다. 이러한 문제를 해결하기 위해 bGPT를 사용하여 기본 이진 데이터를 모델링하고 다양한 작업에 걸쳐 포괄적인 평가를 수행했다. 이 접근법은 다양한 응용 프로그램에서 바이트 모델에 대한 전체론적 평가를 제공하여 디지털 세계 모델링의 잠재력에 대한 통찰력을 제공한다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '본 절에서는 바이트 수준의 디지털 데이터 모델링에 최적화된 모델인 bGPT를 소개한다. 먼저 연산 효율을 관리하기 위해 바이트 시퀀스를 패치로 분할하는 계층적 트랜스포머 프레임워크를 제시한다. 그 후, 우리는 생성 모델링 및 분류를 포함한 bGPT의 훈련 목표를 제시한다.\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      '바이트 수준에서 디지털 데이터로 작업하면 모델이 디지털 시스템 패턴을 학습할 수 있을 뿐만 아니라 다양한 데이터 유형을 단일 프레임워크에 통합할 수 있는 통합 접근 방식을 제공한다. 그러나 바이트의 높은 입도는 긴 시퀀스를 초래하여 계산 비용을 실질적으로 증가시킨다. 이 문제는 2차 자기 주의 축척으로 인해 트랜스포머 기반 모델에서 더 두드러져 이진 데이터 처리에 대한 효율성과 확장성을 제한한다.\n' +
      '\n' +
      '이전 작업(Wu et al., 2023; Yu et al., 2023; Wu et al., 2023; Yang et al., 2023), bGPT는 계층적 트랜스포머 아키텍처를 채택하고 길이\\(T\\)의 바이트\\(B=\\{b_{1},b_{2},...,b_{T}\\}\\)의 시퀀스를 패치\\(\\mathcal{P}\\)의 시퀀스로 분할하며, 여기서 각 패치는 정확히 \\(S\\) 바이트를 포함한다:\n' +
      '\n' +
      '\\[\\mathcal{P}=[P_{1},P_{2},\\ldots,P_{N}], \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(N=\\lceil\\frac{T}{S}\\rceil\\)은 패치의 수이고 \\(P_{i}=[b_{(i-1)S+1},\\ldots,b_{iS}]\\)는 \\(1\\leq i\\leq N\\)에 대한 것이다. 만약 \\(T\\mod S\\neq 0\\)이면, 마지막 패치 \\(P_{N}\\)은 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[P_{N}=[b_{(N-1)S+1},\\ldots,b_{T},\\underbrace{e,\\ldots,e}_{S-(T\\mod S}], \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(e\\)은 마지막 패치 \\(P_{N}\\)을 크기 \\(S\\)으로 패딩하기 위해 사용되는 <eop>(end-of-patch) 토큰을 나타낸다. 바이트 시퀀스를 보다 관리 가능한 패치로 분할함으로써, bGPT는 바이트 레벨 시퀀스 모델링의 필요성과 계산 효율성의 균형을 맞춘다.\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 2를 참조하면, bGPT는 선형 프로젝션 레이어, 패치 레벨 디코더, 바이트 레벨 디코더의 세 가지 주요 구성 요소로 구성된다. bGPT는 그 구성요소를 통해 패치 시퀀스 \\(\\mathcal{P}\\)를 다음과 같이 처리한다:\n' +
      '\n' +
      '**Linear Projection Layer**: \\(\\mathcal{P}\\)에서 각 패치 \\(P_{i}\\)는 크기 \\(S\\times 257\\)의 행렬로 간주되며, 여기서 \\(S\\)은 패치 크기이며, 각 바이트는 256 바이트 값과 <eop> 토큰을 포함하여 257D 벡터로 원-핫 인코딩된다. 그런 다음 이러한 패치는 1차원 벡터로 평평해지며, 여기서 행렬의 행은 순차적으로 연결된다. 그런 다음 선형 투영층은 각 평평해진 벡터를 숨겨진 크기\\(H\\)의 조밀한 벡터\\(E_{i}\\)로 매핑한다. 각각의 패치에 대해, 동작은 다음과 같이 공식화될 수 있다:\n' +
      '\n' +
      '\\[E_{i}=\\text{flatten}(P_{i})\\cdot W_{\\text{linear}},\\quad 1\\leq i\\leq N,\\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(W_{\\text{linear}}\\)는 \\((S\\times 257,H)\\의 형상을 갖는 선형 투영층의 가중치 매트릭스이다. 이러한 밀집 임베딩은 각 패치에 포함된 필수 정보를 보존하면서 차원을 줄임으로써 바이트 시퀀스의 보다 효율적인 처리를 가능하게 한다.\n' +
      '\n' +
      '**Patch-Level Decoder**: 이 디코더는 임베딩된 패치들의 시퀀스 \\(\\mathcal{E}=\\{E_{1},E_{2},\\ldots,E_{N}\\}\\)을 취하여 후속 패치의 특징들을 자동으로 예측하도록 처리함으로써, 데이터의 구조를 효과적으로 학습한다:\n' +
      '\n' +
      '\\[\\hat{E}_{i}=\\text{Decoder}_{\\text{patch}(\\mathcal{E}_{<i}\\oplus\\mathcal{X}_{<i}), \\tag{4}\\)\n' +
      '\n' +
      '여기서 \\(\\mathcal{E}_{<i}\\)는 \\(i\\)번째 패치 이전의 패치 임베딩들의 시퀀스를 나타내고, \\(\\mathcal{X}_{<i}\\)은 대응하는 대응하는\n' +
      '\n' +
      '그림 2: bGPT는 바이트 시퀀스를 패치로 세그먼트화하고, 패치 레벨 디코더로 다음 패치 피쳐를 예측하며, 바이트 레벨 디코더로 이러한 피쳐를 사용하여 패치 내의 바이트를 재구성한다.\n' +
      '\n' +
      '위치 삽입. \\(\\oplus\\) 기호는 이 두 서열의 요소별 추가를 나타낸다. 출력인 \\(\\hat{E}_{i}\\)는 \\(i\\)번째 패치에 대한 예측 특성이다.\n' +
      '\n' +
      '**Byte-Level Decoder**: 개별 패치의 예측 특징 \\(\\hat{E}_{i}\\)을 취하여 해당 패치 내의 바이트 시퀀스를 자동으로 복원한다. 프로세스는 각 패치에 대해 독립적이며 현재 패치의 특징 표현 \\(\\hat{E}_{i}\\)에 대한 컨디셔닝에 의해 동작한다:\n' +
      '\n' +
      '\\[\\hat{b}_{i,j}=\\text{Decoder}_{\\text{byte}(\\hat{E}_{i},b_{i,<j}),\\quad 1\\leq j\\leq S,\\tag{5}\\)\n' +
      '\n' +
      '여기서 \\(\\hat{b}_{i,j}\\)는 \\(i\\)번째 패치에서 위치 \\(j\\)에서 예측된 바이트이고, \\(b_{i,<j}\\)은 현재 패치에서 모든 선행 바이트를 나타낸다.\n' +
      '\n' +
      '### Training Objectives\n' +
      '\n' +
      'bGPT에 대한 교육은 주로 다음 바이트 예측을 통한 생성 모델링을 핵심 초점으로 하며 바이트를 예측하고 생성하는 데 중추적인 역할을 한다.\n' +
      '\n' +
      '**생성 모델링**: 명시적인 안내 없이 선행 바이트 \\(\\{b_{1},b_{2},...,b_{i}\\}\\)에 기초하여 시퀀스에서 다음 바이트 \\(b_{i+1}\\)을 예측하는 것을 목표로 한다. 길이\\(T\\)의 바이트 시퀀스\\(B=\\{b_{1},b_{2},...,b_{T}\\}\\)에 대해, 목표는 다음과 같이 정의된 시퀀스에 걸친 다음 바이트 예측의 음의 로그 우도를 최소화하는 것이다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{GEN}}(\\theta)=-\\sum_{i=1}^{T-1}\\log p(b_{i+1}|b_{1},b_{2},...,b_{i};\\theta), \\tag{6}\\)\n' +
      '\n' +
      '여기서 \\(\\theta\\)는 모델 파라미터를 나타내고, \\(p(\\cdot)\\)는 가능한 다음 바이트에 대한 예측된 확률 분포를 나타낸다. 생성적 모델링에서 손실함수\\(\\mathcal{L}_{\\text{GEN}}(\\cdot)\\)는 바이트 수준에서 데이터의 순차적 종속성을 이해하도록 유도한다.\n' +
      '\n' +
      '다음 바이트 예측을 통해 생성 모델링에 대해 초기에 훈련된 후, bGPT는 새로운 훈련 목표를 갖는 분류 작업에 추가로 적응된다.\n' +
      '\n' +
      '**분류**: 다음 바이트 예측을 통한 생성 모델링의 기초에 따라, bGPT는 라벨링된 데이터 세트에 대해 추가로 훈련되며, 여기서 바이트 시퀀스로부터 카테고리를 예측한다. 이것은 바이트 시퀀스로부터 전역 특징을 추출하는 것을 포함하며, 이는 분류 헤드에 의해 처리된다. 목표는 다음과 같이 공식화된 분류 손실\\(\\mathcal{L}_{\\text{CLF}}\\)을 최소화하는 것이다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{CLF}}(\\theta)=-\\sum_{k=1}^{K}y_{k}\\log p(y_{k}|B;\\theta), \\tag{7}\\]\n' +
      '\n' +
      '여기서 \\(y_{k}\\)는 \\(k\\)번째 카테고리에 대한 부울 레이블로서, 바이트 시퀀스가 해당 카테고리에 속하는지(참) 또는 속하지 않는지(거짓)를 나타낸다. \\(y_{k}\\) (K\\)는 총 카테고리 수이고, \\(p(y_{k}|B;\\theta)\\)는 바이트 시퀀스 \\(B\\)이 주어졌을 때 카테고리 \\(k\\)의 예측 확률이다.\n' +
      '\n' +
      '## 4 Applications\n' +
      '\n' +
      'bGPT와 같은 바이트 모델은 이진 데이터를 이해하는 데 탁월하며, 디지털 미디어 파일 처리(예: 텍스트, 오디오 및 이미지) 및 심층 소프트웨어 및 디지털 시스템 분석을 위한 알고리즘 및 하드웨어 동작을 시뮬레이션하는 데 능숙하다. 이 섹션에서는 제안된 bGPT를 평가하기 위한 선택된 응용 프로그램과 해당 실험 설정을 소개한다.\n' +
      '\n' +
      '##### 디지털 미디어 처리\n' +
      '\n' +
      '딥러닝 분야는 인간의 소통과 정보교류에 필수적인 텍스트, 오디오, 이미지 등 다양한 형태의 미디어(Devlin et al., 2018; Ao et al., 2022; Ramesh et al., 2022)의 생성과 분류 모두에서 꾸준히 숙련도를 높이고 있다. 일반적으로 전자 장치에 바이트 시퀀스로 저장되고 전송되는 이러한 미디어 파일은 bGPT가 생성 모델링 및 분류를 위해 이러한 디지털 콘텐츠를 처리할 수 있게 한다.\n' +
      '\n' +
      'bGPT는 다음 토큰 예측을 통해 표현 학습을 위한 생성 모델링에서 훈련한다. 그런 다음 최종 패치 레벨 디코더 계층의 특징을 사용하여 분류를 위한 전역 특징을 도출하기 위해 평균 풀링을 사용한다.\n' +
      '\n' +
      '트레이닝 프로세스를 간소화하기 위해 오디오 및 이미지 데이터 세트를 표준화했습니다. 오디오 파일은 8000Hz 샘플링 속도, 모노 채널 및 8비트 깊이를 포함하는 사양으로 WAV 형식으로 변환되었으며 각각 1초 길이로 트리밍되었다. 영상 데이터는 32\\(\\times\\)32의 해상도와 RGB 색상, 24비트 깊이의 BMP 형식으로 설정되었다.\n' +
      '\n' +
      '### 알고리즘 및 하드웨어 시뮬레이션\n' +
      '\n' +
      '디지털 프로세스 예측 및 모델링에서 바이트 모델의 기능을 입증하기 위해 데이터 변환과 CPU 상태 모델링의 두 가지 예를 선택한다.\n' +
      '\n' +
      '**데이터 변환**: 프로세스는 ABC 표기법 및 MIDI 파일과 같은 기호 음악 형식을 주요 예로 사용하여 데이터를 한 형식에서 다른 형식으로 변환하는 것을 포함한다. ABC 표기법과 MIDI에 대한 배경 정보는 부록 A를 참조하세요. 이 작업에서 bGPT는 특수 패치로 분리된 쌍을 이루는 ABC와 MIDI 파일의 연결된 바이트 시퀀스에 생성 모델링 접근법을 사용합니다. bGPT 모델은 텍스트 기반의 ABC 표기 악보를 이진 MIDI 연주 신호로 변환하고 역으로 MIDI를 다시 ABC 표기법으로 변환하는 학습을 한다. 이것은 디지털 세계를 모델링하기 위한 필수적인 능력을 나타내는 변환 알고리즘1을 시뮬레이션하고 역공학하는 능력을 필요로 한다.\n' +
      '\n' +
      '각주 1: [https://github.com/xlvector/abcmidi](https://github.com/xlvector/abcmidi)\n' +
      '\n' +
      '**CPU 상태 모델링**: 유사하게, 모델은 일련의 CPU 레지스터 상태가 뒤따르는 저-레벨 기계 명령어의 연결된 시퀀스와 함께 공급된다. 목표는 프로그램이 중단될 때까지 상태가 각각의 명령으로 어떻게 업데이트되는지 정확하게 예측하는 것이다. 이 작업은 bGPT가 운영 데이터를 해석하고 하드웨어 내에서 디지털 활동을 복제할 수 있는 능력을 보여준다.\n' +
      '\n' +
      'CPU 상태 모델링을 위해 데이터 수집 및 평가의 용이성을 위해 CPU 동작의 단순화된 표현을 제공하는 210만 인스턴스와 함께 CPU 상태 데이터 세트를 소개한다. 각각의 데이터세트 인스턴스는 다양한 수의 머신 명령들을 갖는 1KB 메모리 블록을 포함하고, 이어서 16바이트 CPU 레지스터 상태들의 시퀀스가 뒤따른다. 이러한 상태는 데이터 이동, 논리 연산 및 산술 연산과 같은 43개의 변형을 가진 총 21개의 고유한 유형을 포함하는 다양한 명령을 포함한다. 각 상태 내에서, 프로그램 카운터(PC) 및 어큐뮬레이터(ACC)에 대해 각각 1 바이트가 할당되고, 명령어 레지스터(IR)에 대해 4 바이트가 할당되며, 범용 레지스터에 대해 추가로 10 바이트가 예약된다. 인스턴스는 1 내지 256개의 명령어의 랜덤 시퀀스를 실행하고, 각각의 실행 후의 상태를 캡처함으로써 생성된다. 단순화에도 불구하고 이 데이터 세트는 일반적인 CPU 동작을 효과적으로 시뮬레이션합니다. 자세한 내용은 부록 B를 참조하십시오.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Settings\n' +
      '\n' +
      '실험은 표 1에 자세히 설명된 대로 이미지, 음성, 텍스트, 두 가지 유형의 상징 음악 및 CPU 상태를 포함한 다양한 도메인에 걸쳐 오픈 소스 데이터 세트를 활용했으며, 훈련 설정은 표 2에 따라 모든 bGPT 모델과 데이터 세트에서 일관되었으며 패치 크기 16과 시퀀스 길이 512를 사용하여 최대 8KB까지 파일 크기를 지원했으며 110M 매개변수 bGPT 모델은 표준 트랜스포머 기반 모델 척도와 일치했다. 특히, 모든 평가에서 하이퍼파라미터 튜닝 및 데이터 증가를 피했다. 달리 명시되지 않는 한 다른 생성 모델링 작업에 대해 분류 성능 및 Bits-Per-Byte(BPB)를 평가하기 위해 정확도(Acc)를 메트릭으로 사용했다.\n' +
      '\n' +
      '##### 디지털 미디어 처리\n' +
      '\n' +
      '이 연구는 전문 모델과 비교하여 바이트 수준에서 디지털 미디어 파일을 처리하는 데 있어 bGPT의 효과를 평가하는 것을 목표로 했다. 딥러닝에서 표준 사전 훈련 및 미세 조정 접근법을 따랐고, 이미지넷(\\(\\mathrm{bGPT}_{\\mathrm{image}}\\)), 위키피디아(\\(\\mathrm{bGPT}_{\\mathrm{wiki}}\\)), LibriSpeech(\\(\\mathrm{bGPT}_{\\mathrm{libef}}\\))와 같은 다양한 데이터 세트에서 사전 훈련된 bGPT를 따랐다. 또한 이미지넷과 LibriSpeech 데이터셋을 결합한 \\(\\mathrm{bGPT}_{\\mathrm{signal}\\)과 세 데이터셋을 모두 통합한 \\(\\mathrm{bGPT}_{\\mathrm{mix}\\)의 혼합 사전 훈련의 영향을 탐색하였다. Random-initialized variant \\(\\mathrm{bGPT}_{\\mathrm{random}\\)을 기준선으로 사용하였다. 이러한 모델은 AG 뉴스, CIFAR-10 및 음성 명령 v2에서 다음 바이트 예측을 사용하여 먼저 미세 조정되었으며 분류를 위해 추가로 미세 조정되었다.\n' +
      '\n' +
      '#### 5.2.1 Baselines\n' +
      '\n' +
      '베이스라인 비교를 위해 텍스트 생성/분류를 위한 GPT2-small(Radford et al., 2019), 이미지 및 오디오 분류를 위한 ViT-B/16(Dosovitskiy et al., 2021) 및 AST(Gong et al., 2021) 등 각각의 도메인에서 우수한 유사한 스케일의 트랜스포머 기반 모델을 각각 선택하였다. GPT2-small는 bGPT와 동일한 설정 하에 영어 위키피디아에서 사전 훈련되었다. ViT와 AST는 ImageNet(Deng et al., 2009)에서 사전 훈련되었으며, 그들의 결과는 원래 연구에서 가져왔다. 생성적 모델링 벤치마크가 부족한 CIFAR-10(Krizhevsky et al., 2009)과 Speech Commands v2(Warden, 2018)의 경우 BPB 메트릭을 보고하지 않았다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Dataset** & **Total Files (Million)** & **Total Bytes (GB)** & **Mean File Size (Byte)** & **GPU hours per Epoch** & **Modality** \\\\ \\hline Wikipedia (Wikimedia, 2022) & 6.28 & 13.67 & 2237 & 107 & \\\\ AG News (Zhang et al., 2015) & 0.13 & 0.03 & 236.4 & 2.1 & \\\\ ImageNet (Deng et al., 2009) & 1.33 & 3.88 & 3126 & 18 & \\\\ CIFAR-10 (Krizhevsky et al., 2009) & 0.06 & 0.17 & 3126 & 1.1 & \\\\ LibriSpeech (Panayotov et al., 2015) & 3.68 & 27.57 & 8044 & 84 & \\\\ Speech Commands v2 (Warden, 2018) & 0.09 & 0.68 & 8044 & 2.4 & \\\\ IrishMAN-ABC (Wu et al., 2023a) & 0.21 & 0.06 & 313.9 & 2.4 & \\\\ IrishMAN-MIDI (Wu et al., 2023a) & 0.21 & 0.39 & 1916 & 3.2 & \\\\ CPU States (proposed) & 2.1 & 6.09 & 3103 & 36 & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: NVIDIA V100 GPU 시간에서 벤치마킹된 계산 비용과 함께 bGPT 평가를 위한 데이터 세트의 개요. 세부 사항에는 수백만 개의 파일 카운트, 기가바이트 단위의 데이터 세트 크기, 바이트 단위의 평균 파일 크기 및 교육을 위해 에포크당 필요한 GPU 시간이 포함된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Hyperparameter** & **Pre-training** & **Fine-tuning** \\\\ \\hline Patch Size & 16 & 16 \\\\ Patch Length & 512 & 512 \\\\ Patch-Level Layers & 12 & 12 \\\\ Byte-Level Layers & 3 & 3 \\\\ Hidden Size & 768 & 768 \\\\ Epochs & 32 & 32 \\\\ Learning Rate & 1e-04 & 1e-05 \\\\ Batch Size & 16 & 1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 사전 훈련 및 미세 조정을 위한 하이퍼파라미터 설정.\n' +
      '\n' +
      '#### 5.2.2 Results\n' +
      '\n' +
      '표 3은 다양한 벤치마크에 대한 다양한 bGPT 및 기준선 모델의 결과를 보여준다. 이 비교의 주요 통찰력은 사전 훈련이 모든 작업에 걸쳐 모델 성능에 미치는 중요한 영향이다. 예를 들어, 위키피디아에서 사전 훈련된 \\(\\mathrm{bGPT}_{\\mathrm{wiki}}\\)은 텍스트 관련 작업에서 잘 수행하는 반면, LibriSpeech에서 사전 훈련된 \\(\\mathrm{bGPT}_{\\mathrm{libri}}\\)은 다른 변종에 비해 음성 콘텐츠 작업에서 탁월하다. 이는 bGPT 모델이 사전 훈련과 미세 조정 데이터 세트 간에 밀접한 일치가 있을 때 다운스트림 작업에서 더 나은 성능을 달성할 수 있음을 나타내며, 표준 사전 훈련 원칙에 부합한다(Liu et al., 2022).\n' +
      '\n' +
      '양식별 사전 지식이 없음에도 불구하고 bGPT 모델은 여전히 기준 모델과 동등한 성능을 달성하도록 관리한다. 예를 들어, AG 뉴스에서 \\(\\mathrm{bGPT}_{\\mathrm{wiki}\\)는 1.0639 BPB 및 92.49%의 정확도를 달성했지만, GPT2-small 수준에서는 0.9237 BPB 및 94.50%의 정확도는 여전히 경쟁적 성능을 보여준다. 마찬가지로, \\(\\mathrm{bGPT}_{\\mathrm{libri}}\\)는 음성 명령 v2에서 96.03%의 정확도에 도달하여 98.11%의 AST 정확도에 근접했다. 그러나 CIFAR-10의 분석 결과, \\(\\mathrm{bGPT}_{\\mathrm{image}}\\) 지연 ViT에서는 98.13%에 대해 88.69%의 정확도로 눈에 띄는 차이가 나타났다. 이미지 작업에서 이러한 불일치는 이미지 내에서 필수적인 2차원 공간 관계를 포착하는 데 어려움을 겪는 바이트 모델의 순차적 처리 특성 때문일 수 있다(Yu et al., 2023). 그럼에도 불구하고, 이러한 순차적 처리 접근법을 유지하면서 모델 크기를 단순히 스케일링하는 것은 여전히 최신 결과를 달성하기 위한 가능성을 가질 수 있다(Chen et al., 2020).\n' +
      '\n' +
      'bGPT 모델은 혼합 모달리티, 즉 \\(\\mathrm{bGPT}_{\\mathrm{signal}\\) 및 \\(\\mathrm{bGPT}_{\\mathrm{mix}\\)에 대해 사전 훈련된 모델이며, 일반적으로 개별 모달리티에 대해 사전 훈련된 모델의 평균 성능과 일치하는 성능을 보인다. 예를 들어, AG 뉴스 데이터세트에서 BPB가 1.0935이고 정확도가 91.75%인 \\(\\mathrm{bGPT}_{\\mathrm{mix}\\)는 다른 변형보다 우수하지만 텍스트 데이터에 대해 특별히 조정되는 \\(\\mathrm{bGPT}_{\\mathrm{wiki}\\)에 의해 입증된 성능에는 미치지 못한다. 일반적으로 \\(\\mathrm{bGPT}_{\\mathrm{signal}\\)과 \\(\\mathrm{bGPT}_{\\mathrm{image}\\) 사이에 있는 성능 수준을 나타내는 \\(\\mathrm{bGPT}_{\\mathrm{libri}\\)의 경우에도 유사한 경향이 관찰될 수 있다. 이것은 바이트 모델의 절충안을 보여주는데, 혼합 모달리티 사전 훈련은 다양성을 촉진하지만 도메인별 이해의 깊이를 희석시킬 수 있다.\n' +
      '\n' +
      '표 3의 또 다른 주목할 만한 관찰은 bGPT 모델에 대한 교차 모드 미세 조정에서 긍정적인 전달 효과와 부정적인 전달 효과가 모두 있는 혼합 결과이다. 하나의 데이터 유형(예: 오디오용 LibriSpeech의 \\(\\mathrm{bGPT}_{\\mathrm{libri}\\) 또는 이미지용 ImageNet의 \\(\\mathrm{bGPT}_{\\mathrm{image}\\)에 대해 미리 훈련된 모델이 다른 모달리티의 작업에 대해 미세 조정될 때 긍정적인 전달이 발생하며, 이는 무작위 초기화에 비해 자명하지 않은 개선을 보여준다. 이것은 오디오와 이미지와 같은 모달리티 간의 공유 바이트 패턴을 제안합니다. 그러나 텍스트와 다른 양식 사이에서 전이할 때 부정적인 전이가 관찰되어 사전 훈련에서 구조화된 패턴 학습의 이점이 보편적으로 적용되지 않음을 나타낸다. 텍스트는 인간이 만든 추상화로서 오디오 및 시각적 데이터와 크게 다른 뚜렷한 바이트 수준의 조직 패턴을 나타내며, 이는 텍스트 관련 교차 모달 미세 조정에서 관찰되는 부정적인 전달 효과를 설명할 수 있다.\n' +
      '\n' +
      'BMP 스펙트로그램인 32\\(\\times\\)32 BMP 스펙트로그램으로 변환된 음성 명령 v2 데이터 셋에서 바이트 모델의 상호모달 지식 전달에 대한 성능을 평가하였다. 8KB 오디오 파일을 3KB 이미지로 변환하는 이 과정은 필연적으로 정보 손실로 이어진다. 그러나 우리의 초점은 단순한 성능 향상보다는 지식 전달 역학의 뉘앙스를 탐구하는 방향으로 이동한다. 이를 위해 스펙트로그램과의 데이터 형식 일관성을 위해 사전 학습된 모델인 \\(\\mathrm{bGPT}_{\\mathrm{image}\\)과 스펙트로그램과의 정보 유사성을 위해 \\(\\mathrm{bGPT}_{\\mathrm{libri}\\)을 선정하였다. 이 모델은 표 3과 유사한 절차에 따라 조건부 생성 모델링 및 분류에 사용되었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{**AG News (4 classes)**} & \\multicolumn{2}{c}{**CIFAR-10 (10 classes)**} & \\multicolumn{2}{c}{**Speech Commands v2 (36 classes)**} \\\\ \\cline{2-7}\n' +
      '**Model** & **BPB** & **Acc (\\%)** & **BPB** & **Acc (\\%)** & **BPB** & **Acc (\\%)** \\\\ \\hline \\(\\mathrm{bGPT}_{\\mathrm{random}}\\) & 1.3496 & 84.74 & 3.4928 & 76.73 & 1.5414 & 92.43 \\\\ \\(\\mathrm{bGPT}_{\\mathrm{wiki}}\\) & **1.0639** & **92.49** & 3.6663 & 77.02 & 1.5719 & 93.56 \\\\ \\(\\mathrm{bGPT}_{\\mathrm{image}}\\) & 1.4179 & 83.16 & **3.1234** & **88.69** & 1.5326 & 93.91 \\\\ \\(\\mathrm{bGPT}_{\\mathrm{libri}}\\) & 1.3993 & 83.59 & 3.3345 & 83.51 & **1.4818** & **96.03** \\\\ \\(\\mathrm{bGPT}_{\\mathrm{signal}}\\) & 1.4058 & 83.80 & 3.1554 & 87.65 & 1.4898 & 95.66 \\\\ \\(\\mathrm{bGPT}_{\\mathrm{mix}}\\) & 1.0935 & 91.75 & 3.2279 & 84.32 & 1.5086 & 95.09 \\\\ \\hline \\hline \\multicolumn{7}{l}{Baselines} & 0.9237 & 94.50 & — & 98.13 & — & 98.11 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: AG 뉴스에서 GPT2-small, CIFAR-10에서 ViT-B/16, 음성 명령에서 AST v2에서 서로 다른 데이터 세트와 기준 모델에 대해 사전 훈련된 bGPT 모델의 성능 비교. \\(\\mathrm{bGPT}_{\\mathrm{signal}}\\)은 ImageNet과 LibriSpeech의 병합 데이터 세트에서 사전 훈련되고, \\(\\mathrm{bGPT}_{\\mathrm{mix}}\\)은 ImageNet, LibriSpeech 및 위키피디아 데이터 세트의 조합에서 사전 훈련된다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      'MIDI 변환으로, \\(\\mathrm{BPB}_{\\mathrm{abc}}\\) 모델은 처음부터 콘텐츠를 생성함에 따라 생성 모델링을 평가한다. [\\(\\mathrm{BPB}_{\\mathrm{midi}}\\)은 데이터 변환을 평가하는 반면, 완전한 ABC 바이트 시퀀스를 고려한다.\n' +
      '\n' +
      '그림 1에서 더 낮은 시작 손실과 더 빠른 수렴을 관찰한다. 2(a) 데이터 스케일이 커짐에 따라 증가된 데이터 볼륨은 데이터 변환 프로세스를 시뮬레이션하는 데 있어 모델 성능을 직접적으로 향상시킨다는 것을 나타낸다. 표 5에서 데이터의 규모가 증가함에 따라 ABC에서 MIDI로, MIDI에서 ABC로의 변환 모두에 대한 BPB가 크게 감소한다는 것을 알 수 있다. ABC-MIDI 변환에서 \\(\\mathrm{bGPT}^{5}\\) 모델은 0.0011의 인상적으로 낮은 \\(\\mathrm{BPB}_{\\mathrm{midi}\\)을 달성하며, 이는 BPB가 0에 도달하는 완벽한 성능(\\mathrm{bGPT}^{3}\\)에 매우 근접하여 \\(\\mathrm{bGPT}^{5}\\) 모델의 성능을 훨씬 능가한다.\n' +
      '\n' +
      '표 5는 ABC에 대해 양방향으로 일관되게 더 높은 BPB를 제안하는데, 이는 1) ABC에서 MIDI로의 순방향 변환은 필요한 정보가 있는 기존 알고리즘을 시뮬레이션하는 데 중점을 두는 반면, MIDI에서 ABC로의 역방향 과정은 점수 구조, 음악 장식품 및 표현과 같은 MIDI 파일에서 누락된 정보를 추론하고 재구성해야 한다. 2) MIDI는 하위 레벨의 이진 포맷이고 ABC 표기법은 인간이 읽을 수 있는 텍스트 포맷이므로 바이트 모델은 MIDI 파일 내에서 패턴을 학습하는 것이 더 쉽다는 것을 발견할 수 있다.\n' +
      '\n' +
      '###### 5.3.2 CPU 상태 모델링\n' +
      '\n' +
      'CPU 상태 모델링은 기계 명령으로부터 내부 상태에 대한 업데이트를 예측함으로써 CPU 기능을 복제하는 것을 목표로 한다. 우리는 완전한 명령어와 초기 상태를 기반으로 각 단계에서 가장 높은 확률 바이트를 선택하여 이러한 상태를 예측하기 위해 bGPT를 사용했다. 정확도는 실제 상태와의 바이트별 비교를 통해 평가되었다.\n' +
      '\n' +
      '우리는 데이터 양이 모델링 성능에 상당한 영향을 미친다는 것을 발견했다. 표 6은 BPB가 \\(\\mathrm{bGPT}^{4}\\)에서 \\(\\mathrm{bGPT}^{5}\\)으로 떨어지지만 \\(\\mathrm{bGPT}^{5}\\)을 초과하는 수익률을 감소시키는 현저한 성능 변화를 보여준다. (\\mathrm{bGPT}^{5}\\)과 \\(\\mathrm{bGPT}^{6}\\)은 모두 99.97%와 99.99%의 정확도로, 각 테스트 케이스마다 평균 128개의 무작위 명령어와 5억 1,600만 개 이상의 방대한 잠재적 명령어 조합이라는 점에서 단순한 암기 이상의 효율성을 시사한다.\n' +
      '\n' +
      '그림 11에서 볼 수 있듯이 \\(\\mathrm{bGPT}^{5}\\)의 성능이 크게 향상되었다. 2(b)는 CPU 상태 모델링에서 출현 능력을 나타낸다. 특히 BPB와 정확도에서 \\(\\mathrm{bGPT}^{4}\\)과 \\(\\mathrm{bGPT}^{5}\\)을 비교할 때 CPU 상태에 대한 더 깊은 이해는 능력의 질적 향상에서 기인할 수 있음을 시사한다. 이는 대규모 LMs(Wei et al., 2022)에서 능력이 규모와 복잡성으로 자발적으로 발생하는 것으로 보이는 긴급 능력의 개념과 일치한다.\n' +
      '\n' +
      '그러나 이러한 개선이 진정한 학습을 반영하는지에 대해서는 회의론이 존재한다(Schaeffer et al., 2023). 비평가들은 성능 향상이 비선형 메트릭이나 과적합 때문일 수 있다고 주장한다. 그럼에도 불구하고 BPB의 선형적이고 부드러운 특성은 이에 대응하며, 이는 개선이 CPU 작업에 대한 실제 이해에서 기인할 가능성이 있음을 나타내며, 이는 메트릭 이상보다는 일관된 학습을 시사한다.\n' +
      '\n' +
      '요약하면, bGPT는 데이터 변환 및 CPU 상태 모델링에서 긴급한 능력을 가진 네이티브 바이너리 데이터에 대해 강력한 확장성을 보여주며, 이는 알고리즘과 하드웨어 시뮬레이션 작업에서 강력한 능력을 조명한다. 이 연구에서 실증에 사용된 작업은 과도하게 복잡하지 않았지만 이러한 맥락에서 관찰된 거의 완벽한 성능은 광범위한 알고리즘과 하드웨어를 시뮬레이션하고 역공학하기 위한 바이트 모델의 광범위한 잠재력을 나타낸다.\n' +
      '\n' +
      '## 6 Conclusions\n' +
      '\n' +
      '본 논문에서는 다음 바이트 예측을 통해 딥 러닝을 이진 데이터 처리로 확장하여 디지털 세계를 위한 다목적 시뮬레이터로서 bGPT를 제시한다. 우리의 실험은 양식 진단 지식 전달을 보여주는 디지털 미디어 데이터 모델링에서 bGPT의 효과를 보여준다. 우리는 기본 이진 데이터를 모델링할 때 bGPT의 강력한 확장성과 심지어 새로운 능력의 징후를 관찰한다. bGPT는 모달리티별 설계 없이 다양한 데이터셋에 걸쳐 특화된 모델과 비교 가능하게 수행하며, 데이터 변환 및 CPU 상태 모델링에 탁월하여 다양한 알고리즘과 하드웨어를 시뮬레이션할 수 있는 가능성을 보여준다.\n' +
      '\n' +
      '그럼에도 불구하고, 우리의 실험은 개선의 기회를 조명한다. 본 연구에서는 바이트 모델에 내재된 자원 집약적 특성의 결과인 짧은 오디오 세그먼트와 저해상도 이미지에 모델링하는 것을 제한한다. 제한된 계산 자원으로 인해 대체 형식에 걸쳐 광범위한 평가 없이 ABC 표기법과 MIDI 사이의 데이터 변환만 조사했다. 또한 데이터 수집 및 평가를 단순화하기 위해 CPU 상태 모델링 실험은 훨씬 더 복잡한 실제 현대 CPU의 사용을 생략하면서 단순화된 CPU에만 초점을 맞췄다.\n' +
      '\n' +
      '바이트 모델에 대한 향후 연구 방향은 1) 바이트 모델을 보다 실현 가능하게 만들기 위해 계산 비용을 줄이는 것, 2) 더 넓은 범위의 네이티브 바이너리 데이터를 수용하기 위해 모델 및 데이터 세트 크기를 스케일링하는 것, 고해상도 이미지 및 비디오와 같은 더 큰 디지털 미디어 파일을 처리하는 것, 3) 특히 다양한 애플리케이션 도메인에 걸쳐 네이티브 바이너리 데이터를 포함하는 미개척 작업에 대해 모델 성능을 개선하는 것이다.\n' +
      '\n' +
      '## 7 영향 진술\n' +
      '\n' +
      '본 논문에서는 이진 데이터 처리와 디지털 월드 모델링을 위해 설계된 모델인 bGPT를 소개하고 딥 러닝의 경계를 네이티브 이진 데이터의 영역으로 밀어넣는다. 이러한 혁신은 bGPT가 바이너리 데이터를 직접 해석하고 조작할 수 있게 하여 디지털 시스템에 대한 심오한 통찰력을 제공한다. bGPT는 디지털 세계에 대한 이해와 모델링의 발전을 제시하지만 윤리적 의미와 사회적 규범 및 법적 프레임워크에 대한 잠재적 영향에 대한 신중한 검토가 필요하다.\n' +
      '\n' +
      '알고리즘과 하드웨어를 시뮬레이션하거나 역공학하는 능력은 1) 기존 기술에 대한 이해와 개선을 통해 사이버 보안, 소프트웨어 및 하드웨어 개발에 도움이 되는 기술 혁신을 크게 향상시킬 수 있으며, 2) 페어링된 소스 코드와 실행 가능한 소프트웨어의 광범위한 데이터 세트에 대한 bGPT 교육을 통해 독점 소프트웨어의 역공학을 가능하게 할 수 있으므로 지적 재산에 위험을 초래할 수 있다. 이 기능은 잠재력을 보여주면서도 소프트웨어의 무단 액세스 또는 수정을 용이하게 하여 보안 및 법적 문제를 제기할 수 있다.\n' +
      '\n' +
      '결론적으로, bGPT 및 유사한 바이트 모델은 디지털 세계 내에서 우리의 이해와 능력을 발전시킬 수 있는 흥미로운 기회를 제공하지만, 그들의 배치에는 윤리적, 사회적, 법적 의미에 대한 신중한 고려가 필요하다. 이것은 지적 재산을 보호하고 잠재적인 악의적인 사용에 대한 보안을 보장하는 데 특히 중요합니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Ao et al. (2022) Ao, J., Wang, R., Zhou, L., Wang, C., Ren, S., Wu, Y., Liu, S., Ko, T., Li, Q., Zhang, Y., Wei, Z., Qian, Y., Li, J., and Wei, F. Speech5: Unified-modal encoder-decoder pre-training for spoken language processing. Muresan, S., Nakov, P., and Villavicencio, A. (eds.), _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pp. 5723-5738. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.393. URL[https://doi.org/10.18653/v1/2022.acl-long.393](https://doi.org/10.18653/v1/2022.acl-long.393](https://doi.org/10.18653/v1/2022.acl-long.393).\n' +
      '* Boros et al. (2023) Boros, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Roblek, D., Teboul, O., Granigier, D., Tagliasacchi, M., and Zeghidour, N. Audiolm: 오디오 생성에 대한 언어 모델링 접근법. _ IEEE ACM Trans. Audio Speech Lang. 처리._ , 31:2523-2533, 2023. doi: 10.1109/TASLP.2023.3288409. URL[https://doi.org/10.1109/TASLP.2023.32888409](https://doi.org/10.1109/TASLP.2023.32888409)\n' +
      '* Brown et al. (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Grey, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. 언어 모델은 소수의 학습자이다. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL[https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)에서,\n' +
      '* Bubeck et al. (2023) Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S. M., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang, Y. 인공 지능의 불꽃: GPT-4와의 초기 실험. _CoRR_, abs/2303.12712, 2023. doi: 10.48550/ARXIV.2303.12712. URL[https://doi.org/10.48550/arXiv.2303.12712](https://doi.org/10.48550/arXiv.2303.12712)\n' +
      '* Chen et al. (2020) Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. Generative pretraining from pixels. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pp. 1691-1703. PMLR, 2020. URL[http://proceedings.mlr.press/v119/chen20s.html](http://proceedings.mlr.press/v119/chen20s.html).\n' +
      '* Copet et al. (2023) Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y., and Defossez, A. Simple and controllable music generation. _ CoRR_, abs/2306.05284, 2023. doi: 10.48550/ARXIV.2306.05284. URL[https://doi.org/10.48550/arXiv.2306.05284](https://doi.org/10.48550/arXiv.2306.05284).\n' +
      '* Defossez et al. (2022) Defossez, A., Copet, J., Synnaeve, G., and Adi, Y. 고충실도 신경 오디오 압축 CoRR_, abs/2210.13438, 2022. doi: 10.48550/ARXIV.2210.13438. URL[https://doi.org/10.48550/arXiv.2210.13438](https://doi.org/10.48550/arXiv.2210.13438).\n' +
      '* Deletang et al. (2009) Deletang, G., Ruoss, A., Duquenne, P., Catt, E., Genewein, T., Mattern, C., Grau-Moya, J., Wenliang, L. K., Aitchison, M., Orseau, L., Hutter, M., and Veness, J. Language modeling is compression. _ CoRR_, abs/2309.10668, 2023. doi: 10.48550/ARXIV.2309.10668. URL[https://doi.org/10.48550/arXiv.2309.10668](https://doi.org/10.48550/arXiv.2309.10668)\n' +
      '* Deng et al. (2009) Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei-Fei, L. Imagenet: 대규모 계층 이미지 데이터베이스. 2009년 _2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA_, pp. 248-255. IEEE Computer Society, 2009. doi: 10.1109/CVPR.2009.5206848.\n' +
      '\n' +
      'URL [https://doi.org/10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848).\n' +
      '* Devlin et al. (2018) Devlin, J., Chang, M. - W., Lee, K., and Toutanova, K. Bert: 언어 이해를 위한 깊은 양방향 변압기의 사전 훈련. _ arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* Dosovitskiy et al.(2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. 이미지는 16x16 단어의 가치가 있습니다: 스케일에서 이미지 인식을 위한 트랜스포머입니다. _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL[https://openreview.net/forum?id=YicbFdNTTY](https://openreview.net/forum?id=YicbFdNTTY)이다.\n' +
      '* Ferruz et al. (2022) Ferruz, N., Schmidt, S., and Hocker, B. Protgpt2는 단백질 설계를 위한 심층 비감독 언어 모델이다. _ Nature Communications_, 13:4348, 2022. doi: 10.1038/s41467-022-32007-7. URL[https://doi.org/10.1038/s41467-022-32007-7](https://doi.org/10.1038/s41467-022-32007-7].\n' +
      '*3 September 2021_, pp. 571-575. ISCA, 2021. doi: 10.21437/INTERSPEECH.2021-698. URL[https://doi.org/10.21437/Interspeech.2021-698](https://doi.org/10.21437/Interspeech.2021-698).\n' +
      '* Gu & Dao(2023) Gu, A. and Dao, T. Mamba: 선택적 상태 공간을 갖는 선형-시간 시퀀스 모델링 _ CoRR_, abs/2312.00752, 2023. doi: 10.48550/ARXIV.2312.00752. URL[https://doi.org/10.48550/arXiv.2312.00752](https://doi.org/10.48550/arXiv.2312.00752).\n' +
      '* Guo et al. (2019) Guo, W., Mu, D., Xing, X., Du, M., and Song, D. DEEPVSA: facilitating value-set analysis with deep learning for 사후 프로그램 분석을 위한. Heninger, N. and Traynor, P. (eds.), _28th USENIX Security Symposium, USENIX Security 2019, Santa Clara, CA, USA, August 14-16, 2019_, pp. 1787-1804. USENIX Association, 2019. URL[https://www.usenix.org/conference/usenixsecurity19/presentation/guo](https://www.usenix.org/conference/usenixsecurity19/presentation/guo).\n' +
      '* He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 770-778, 2016.\n' +
      '* Hochreiter & Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. Long shortterm memory. _ Neural Comput._ , 9(8):1735-1780, 1997. doi: 10.1162/NECO.1997.9.8.1735. URL[https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)\n' +
      '* Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. 컴퓨터 최적 대형 언어 모델 훈련 CoRR_, abs/2203.15556, 2022. doi: 10.48550/ARXIV.2203.15556. URL[https://doi.org/10.48550/arXiv.2203.15556](https://doi.org/10.48550/arXiv.2203.15556).\n' +
      '* Horton et al. (2023) Horton, M., Mehta, S., Farhadi, A., and Rastegari, M. 바이트만 있으면 됩니다. 파일 바이트에서 직접 작동하는 트랜스포머입니다. _ CoRR_, abs/2306.00238, 2023. doi: 10.48550/ARXIV.2306.00238. URL[https://doi.org/10.48550/arXiv.2306.00238](https://doi.org/10.48550/arXiv.2306.00238).\n' +
      '* Hsiao et al. (2021) Hsiao, W., Liu, J., Yeh, Y., and Yang, Y. 복합어 변환기: 동적 방향 하이퍼그래프 위에 풀송 음악을 구성하는 학습. In _30-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pp. 178-186. AAAI Press, 2021. doi: 10.1609/AAAI.V3511.16091. URL[https://doi.org/10.1609/aaai.v35i1.16091](https://doi.org/10.1609/aaai.v35i1.16091)\n' +
      '* Huang et al. (2019) Huang, C. A., Vaswani, A., Uszkoreit, J., Simon, I., Hawthorne, C., Shazeer, N., Dai, A. M., Hoffman, M. D., Dinculescu, M., and Eck, D. Music transformer: Long-term 구조의 음악을 생성한다. _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL[https://openreview.net/forum?id=rJe4ShAcF7](https://openreview.net/forum?id=rJe4ShAcF7)이다.\n' +
      '* Krizhevsky et al. (2009) Krizhevsky, A., Hinton, G., et al. Learning multiple layer of features from tiny images. 2009년\n' +
      '* 쿠도(2018) 쿠도, T. 서브워드 정규화: 다수의 서브워드 후보들로 신경망 번역 모델들을 개선한다. 구레비치, I.와 미야오, Y. (eds.), _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers_, pp. 66-75. Association for Computational Linguistics, 2018. doi: 10.18653/V1/P18-1007. URL[https://aclanthology.org/P18-1007/](https://aclanthology.org/P18-1007/).\n' +
      '* Kudo & Richardson (2018) Kudo, T. and Richardson, J. Sentencepiece: a simple and language independent subword tokenizer and detokenizer for neural text processing. Blanco, E. and Lu, W. (eds.), _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, Brussels,Belgium, October 31 - November 4, 2018_, pp. 66-71. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-2012. URL[https://doi.org/10.18653/v1/d18-2012](https://doi.org/10.18653/v1/d18-2012)을 포함할 수 있다.\n' +
      '* LeCun et al. (1989) LeCun, Y., Boser, B. E., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. E., and Jackel, L. D. Handwritten digit recognition with the back-propagation network. Touretzky, D. S. (ed.), _Advances in Neural Information Processing Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989]_, pp. 396-404. Morgan Kaufmann, 1989. URL[http://papers.nips.cc/paper/293-handwritten-digit-recognition-with-a-back-propagation-network](http://papers.nips.cc/paper/293-handwritten-digit-recognition-with-a-back-propagation-network)\n' +
      '* Li 등(2023) Li, J., Li, D., Savarese, S., and Hoi, S. C. H. BLIP-2: bootstrapping language-image pre-training with frozen image encoder and large language models. Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pp. 19730-19742. PMLR, 2023. URL[https://proceedings.mlr.press/v202/li23q.html](https://proceedings.mlr.press/v202/li23q.html).\n' +
      '* Liu et al. (2023) Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. _ CoRR_, abs/2304.08485, 2023. doi: 10.48550/ARXIV.2304.08485. URL[https://doi.org/10.48550/arXiv.2304.08485](https://doi.org/10.48550/arXiv.2304.08485).\n' +
      '* December 9, 2022_, 2022. URL[http://papers.nips.cc/paper_files/paper/2022/hash/dlc88f9790765146ec8fb5d02e5653a0-Abstract_html](http://papers.nips.cc/paper_files/paper/2022/hash/dlc88f9790765146ec8fb5d02e5653a0-Abstract_html)\n' +
      '* Nguyen et al. (2023) Nguyen, E., Poli, M., Faizi, M., Thomas, A. W., Birch-Sykes, C., Wornow, M., Patel, A., Rabideau, C. M., Massaroli, S., Bengio, Y., Ermon, S., Baccus, S. A., and Re, C. Hyenadna: Long range genomic sequence modeling at single nucleotide resolution. _ CoRR_, abs/2306.15794, 2023. doi: 10.48550/ARXIV.2306.15794. URL[https://doi.org/10.48550/arXiv.2306.15794](https://doi.org/10.48550/arXiv.2306.15794).\n' +
      '* Notin et al. (2018) Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A. N., Marks, D. S., and Gal, Y. Tranception: 자동 회귀 변압기를 사용한 단백질 적합성 예측 및 추론-시간 검색. Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pp. 16990-17017. PMLR, 2022. URL[https://proceedings.mlr.press/v162/notin22a.html](https://proceedings.mlr.press/v162/notin22a.html).\n' +
      '* Oord et al. (2016) Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. Wavenet: raw audio를 위한 생성 모델. _ ArXiv:1609.03499_, 2016.\n' +
      '* Panayotov et al. (2015) Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. 공개 도메인 오디오 북을 기반으로 하는 리브리스페코-ASR 코퍼스. In _2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015_, pp. 5206-5210. IEEE, 2015. doi: 10.1109/ICASSP.2015.7178964. URL[https://doi.org/10.1109/ICASSP.2015.7178964](https://doi.org/10.1109/ICASSP.2015.7178964)\n' +
      '* Peters et al. (2018) Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 심층 문맥화된 단어 표현 워커, M. A., Ji, H., and Stent, A. (eds.), _Proceedings of the 2018 Conference of the North American chapter of the Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1(Long Papers)_, pp. 2227-2237. Association for Computational Linguistics, 2018. doi: 10.18653/V1/N18-1202. URL[https://doi.org/10.18653/v1/n18-1202](https://doi.org/10.18653/v1/n18-1202)을 포함할 수 있다.\n' +
      '* Radford et al. (2018) Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Generative Pre-training에 의한 언어 이해력 향상. 2018년\n' +
      '* Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. _ OpenAI blog_, 1(8):9, 2019.\n' +
      '* Raff et al. (2018) Raff, E., Barker, J., Sylvester, J., Brandon, R., Catanzaro, B., and Nicholas, C. K. Malware detection by eating a whole EXE. The Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018_, volume WS-18 of _AAAI Technical Report_, pp. 268-276. AAAI Press, 2018. URL[https://aaai.org/ocs/index.php/WS/AAAIWI8/paper/view/16422](https://aaai.org/ocs/index.php/WS/AAAIWI8/paper/view/16422)\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. CLIP Latents를 이용한 계층적 텍스트 조건 이미지 생성 CoRR_, abs/2204.06125, 2022. doi: 10.48550/ARXIV.2204.06125. URL[https://doi.org/10.48550/arXiv.2204.06125](https://doi.org/10.48550/arXiv.2204.06125).\n' +
      '\n' +
      'Rubenstein, P. K., Asawaorengchai, C., Nguyen, D. D., Bapna, A., Borosos, Z., de Chaumont Quitry, F., Chen, P., Badawy, D. E., Han, W., Kharitonov, E., Muckenhirn, H., Padfield, D., Qin, J., Rozenberg, D., Sainath, T. N., Schalkwyk, J., Sharifi, M., Ramanovich, M. T., Tagliascachi, M., Tudor, A., Yu, J., Wang, Y., Zayats, V., Zeghidour, N., Zhang, Z., Zilka, L., and Frank, C. H. Audiopalm. CoRR_, abs/2306.12925, 2023. doi: 10.48550/ARXIV.2306.12925. URL[https://doi.org/10.48550/arXiv.2306.12925](https://doi.org/10.48550/arXiv.2306.12925)\n' +
      '* Schaeffer et al. (2023) Schaeffer, R., Miranda, B., and Koyejo, S. 대형 언어 모델의 출현 능력은 신기루인가? _30-7th Conference on Neural Information Processing Systems_, 2023. URL[https://openreview.net/forum?id=ITw9edRD1D](https://openreview.net/forum?id=ITw9edRD1D)에서,\n' +
      '* Sennrich et al. (2016) Sennrich, R., Haddow, B., and Birch, A. neural machine translation of rare word with subword units. _Proceedings of the 54th Annual Meeting of the Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers_. 컴퓨터 언어학 협회, 2016. Doi: 10.18653/V1/P16-1162. URL[https://doi.org/10.18653/v1/p16-1162](https://doi.org/10.18653/v1/p16-1162)\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models. _ CoRR_, abs/2302.13971, 2023a. doi: 10.48550/ARXIV.2302.13971. URL[https://doi.org/10.48550/arXiv.2302.13971](https://doi.org/10.48550/arXiv.2302.13971)\n' +
      '* Touvron et al.(2022) Touvron, H., Martin, L., Almahairi, A., Babaei, Y., Bhosale, S., Bikel, D., Blecher, L., Canton-Ferrer, C., Chen, M., Cucurull, G., Fernandes, J., Fu, J., Fu, W., Fuller, B., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, V., Karkez, M., Kloumann, I., Kounev, A., Koux, P. S., Lavril, T., Mishra, P., M. 라마 2: 오픈 파운데이션 및 미세 조정 채팅 모델들_ CoRR_, abs/2307.09288, 2023b. doi: 10.48550/ARXIV.2307.09288. URL[https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288)\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention all you need. _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Wang et al. (2020) Wang, C., Cho, K., and Gu, J. Neural machine translation with byte-level subwords. In _The Thirty- fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pp. 9154-9160. AAAI Press, 2020. doi: 10.1609/AAAI.V34105.6451. URL[https://doi.org/10.1609/aaai.v34i05.6451](https://doi.org/10.1609/aaai.v34i05.6451](https://doi.org/10.1609/aaai.v34i05.6451)\n' +
      '* Wang et al. (2023) Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F. Neural codec language models are zero-shot text to speech synthesizers. _ CoRR_, abs/2301.02111, 2023. doi: 10.48550/ARXIV.2301.02111. URL[https://doi.org/10.48550/arXiv.2301.02111](https://doi.org/10.48550/arXiv.2301.02111).\n' +
      '* Wang et al. (2024) Wang, J., Gangavarapu, T., Yan, J. N., and Rush, A. M. Mambabyte: Token-free selective state space model, 2024.\n' +
      '* 워든(2018) 워든, P. 스피치 커맨드: 한정 어휘 음성 인식을 위한 데이터셋. _ CoRR_, abs/1804.03209, 2018. URL[http://arxiv.org/abs/1804.03209](http://arxiv.org/abs/1804.03209).\n' +
      '* Wei et al. (2022) Wei, J., Liu, Q., Guo, Y., and Jiang, X. 바이트 수준 하위 단어로 다국어 사전 학습 언어 모델 학습. _ arXiv preprint arXiv:2101.09469_, 2021.\n' +
      '* Wei et al. (2022) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. 대형 언어 모델의 새로운 능력. _ 트랜스 마흐 배워 Res._ , 2022, 2022. URL[https://openreview.net/forum?id=yzkSU5zdwD](https://openreview.net/forum?id=yzkSU5zdwD).\n' +
      '* 위키미디어 다운로드(2022) 위키미디어. Wikimedia downloads, 2022. URL[https://dumps.wikimedia.org](https://dumps.wikimedia.org).\n' +
      '* Wu et al. (2023) Wu, S., Li, X., Yu, F., and Sun, M. 튜너스포머: 막대 패칭으로 제어 코드로 아이리쉬 선율을 형성합니다. Porcaro, L., Battle-Roca, R., and Gomez, E. (eds.), _Proceedings of the 2nd Workshop on Human-Centric Music Information Retrieval 2023 with the 24th International Society for Music Information Retrieval Conference(ISMIR 2023), Milan, Italy, November 10, 2023_, volume 3528 of _CEUR Workshop Proceedings_. CEUR-WS.org, 2023a. URL[https://ceur-ws.org/Vol-3528/paper1.pdf](https://ceur-ws.org/Vol-3528/paper1.pdf)\n' +
      '\n' +
      'Wu, S., Yu, D., Tan, X., and Sun, M. 클램프: 크로스모달 심볼릭 음악 정보 검색을 위한 대비 언어-음악 사전 훈련. Sarti, A., Antonacci, F., Sandler, M., Bestagini, P., Dixon, S., Liang, B., Richard, G., and Pauwels, J. (eds.), _Proceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR 2023, Milan, Italy, November 5-9, 2023_, pp. 157-165, 2023b. doi: 10.5281/ZENODO.10265247. URL[https://doi.org/10.5281/zenodo.10265247](https://doi.org/10.5281/zenodo.10265247).\n' +
      '* Wu et al. (2016) Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., and Dean, J. 구글의 신경 기계 번역 시스템: 인간 및 기계 번역 사이의 갭을 브리징한다. _ CoRR_, abs/1609.08144, 2016. URL[http://arxiv.org/abs/1609.08144](http://arxiv.org/abs/1609.08144).\n' +
      '* Xue et al. (2022) Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., and Raffel, C. Byt5: Towards a token-free future with the pre-trained byte-to-byte models. _ 트랜스 Assoc. 컴퓨터 Linguistics_, 10:291-306, 2022. doi: 10.1162/TACL\\_A\\_00461. URL[https://doi.org/10.1162/tacl_a_00461](https://doi.org/10.1162/tacl_a_00461).\n' +
      '* Yang et al. (2023) Yang, D., Tian, J., Tan, X., Huang, R., Liu, S., Chang, X., Shi, J., Zhao, S., Bian, J., Wu, X., et al. Uniaudio: universal audio generation을 향한 audio foundation model. _ arXiv preprint arXiv:2310.00704_, 2023.\n' +
      '* Yu et al. (2023) Yu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer, L., and Lewis, M. MEGABYTE: 멀티스케일 변압기로 백만 바이트 시퀀스를 예측하는 것. _30-7th Conference on Neural Information Processing Systems_, 2023. URL[https://openreview.net/forum?id=JTmO2V9Xpz](https://openreview.net/forum?id=JTmO2V9Xpz).\n' +
      '* Zeng et al. (2021) Zeng, M., Tan, X., Wang, R., Ju, Z., Qin, T., and Liu, T. 뮤직버트: 대규모 사전 훈련으로 상징적인 음악 이해. Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), _Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021_, volume ACL/IJCNLP 2021 of _Findings of ACL_, pp. 791-800. Association for Computational Linguistics, 2021. Doi: 10.18653/V1/2021. FINDINGS-ACL.70. URL[https://doi.org/10.18653/v1/2021.findings-acl.70](https://doi.org/10.18653/v1/2021.findings-acl.70)을 포함할 수 있다.\n' +
      '* Zhang et al.(2015) Zhang, X., Zhao, J. J., and LeCun, Y. 텍스트 분류를 위한 문자 레벨 컨볼루션 네트워크. Cortes, C., Lawrence, N. D., Lee, D., Sugiyama, M., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pp. 649-657, 2015. URL[https://proceedings.neurips.cc/paper/2015/hash/250cf8b51cf8b8b4be867a9a02-Abstract.html](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c8b4be867a9a02-Abstract.html)\n' +
      '* Zhu et al. (2023) Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: 고급 대형 언어 모델로 비전 언어 이해력 향상. _ CoRR_, abs/2304.10592, 2023. doi: 10.48550/ARXIV.2304.10592. URL[https://doi.org/10.48550/arXiv.2304.10592](https://doi.org/10.48550/arXiv.2304.10592).\n' +
      '\n' +
      '## 부록 기호음악형식\n' +
      '\n' +
      '### ABC Notation\n' +
      '\n' +
      'ABC 표기법은 ASCII 문자를 사용하여 음표와 기호를 나타내는 음악 표기법의 약칭 형태이다. 그것은 음악의 신속한 디지털 전사, 공유 및 편집을 용이하게 하는 능력으로 인기를 얻은 작고 사람이 읽을 수 있으며 쉽게 입력할 수 있는 형식이다. 민속 및 전통 음악계 내에서 시작된 ABC 표기법은 음악가와 애호가들이 독점적인 소프트웨어나 광범위한 음악 교육이 필요 없이 음악을 교환하는 보편적인 방법이 되었다.\n' +
      '\n' +
      'ABC 기보의 기본을 설명하기 위해 민속 가락을 묘사한 아래의 예를 고려한다:\n' +
      '\n' +
      '```\n' +
      'X:1 L:1/8 M:3/4 K:D de|"D" f3 g a(/g)f|"A" e4 AB|"C" = c3 d"G/B" G|"A" A4 fg|"D" a3 g fd|"A" e4 AB|"C" = c3 d e(/f)g|"D" f4:i d(/ed)cA|"Bm" B2 A2 F2|"F#m" A3 B2 A2 F2|"A" (E4 E)A|"Bm" B3 A2 F2|"A" (E4 E)A|"B3 B3 e"A" dc|"D" d4:i"C"\n' +
      '```\n' +
      '\n' +
      '이 ABC 시퀀스로부터 렌더링된 대응하는 악보가 도 4에 도시된다. 이 예에서, ABC 표기의 구조는 다음을 포함하는 헤더로 시작한다:\n' +
      '\n' +
      '***X(Tune Number)**: 이는 곡조에 할당된 고유 번호로서, 모음 또는 데이터베이스에서 참조 역할을 한다.\n' +
      '***L(단위 노트 길이)**: 이것은 노트들의 디폴트 지속기간을 정의한다. 이러한 맥락에서 1/8은 각 음이 민요의 표준 선택인 8분음임을 나타낸다.\n' +
      '* **M(미터)**: 이것은 리듬 맥박을 결정하는 조각의 타임 시그니처를 나타낸다. 여기서 3/4은 왈츠 같은 리듬을 시사한다.\n' +
      '**K(Key)**: 이것은 곡조에 대한 키 서명을 설정한다. D는 그 작품이 D장조의 핵심에 있다는 것을 의미한다.\n' +
      '\n' +
      '헤더 뒤에 ABC 표기법의 본문은 악보 자체로 구성되어 있다. 각 음표는 문자(A 내지 G)로 표현되며, 이는 음정에 해당한다. 옥타브는 각각 상위 옥타브의 경우 하위 케이스 문자 또는 아포스트로피, 하위 또는 상위 옥타브의 경우 상위 케이스 문자 또는 콤마로 표시된다. 음표의 지속 시간은 숫자에 의해 수정되는데, 예를 들어 음표의 다음 2는 그 값의 두 배가 되는 반면 a/2는 그 값을 반으로 줄인다. 코드를 봉인하여 표기할 수 있습니다.\n' +
      '\n' +
      '그림 4: D장조 민요의 ABC 표기에 해당하는 악보 예.\n' +
      '\n' +
      'F 샤프 단조를 나타내는 "F#m"과 같이 화음을 암시하기 위해 따옴표의 화음 이름. 피치를 변경하기 위해 샤프(\\hat{\\}\\), 플랫(\\\\) 및 자연(\\\\\\)에 추가 기호가 사용되며 기타 다양한 문자가 조음과 표현을 나타낸다.\n' +
      '\n' +
      '이 표기는 리듬 구조부터 선율과 화성의 세부 사항까지 충분한 정보를 가진 곡조를 묘사한다. 이처럼 ABC 표기법은 전통적인 형태의 악보와 현대 디지털 세계의 가교 역할을 하여 효율적이고 광범위하게 접근할 수 있는 텍스트 기반 대안을 제공한다. ABC 표기법에 대한 자세한 설명서는 ABC 표기 표준 2를 참조하십시오.\n' +
      '\n' +
      '각주 2: [https://abcnotation.com/wiki/abc:standard:v2.1](https://abcnotation.com/wiki/abc:standard:v2.1)\n' +
      '\n' +
      '### Midi\n' +
      '\n' +
      '뮤지컬 악기 디지털 인터페이스의 줄임말인 MIDI는 프로토콜, 디지털 인터페이스 및 커넥터를 기술하고 다양한 전자 악기, 컴퓨터 및 기타 관련 장치가 서로 연결하고 통신할 수 있도록 하는 기술 표준이다. MIDI는 인간의 가독성과 악보 전사를 위해 설계된 ABC 표기법과 달리 기계가 해석하고 가공할 수 있는 형식으로 음악 연주와 제작에 대한 정보를 포착하고 전달하는 데 초점을 맞추고 있다.\n' +
      '\n' +
      'MIDI 통신의 핵심은 음표, 음정, 속도(음표가 얼마나 세게 연주되는지), 제어 변경(소리 매개변수에 대한 수정), 프로그램 변경(악기 변경)과 같은 다양한 음악 요소를 나타내는 디지털 신호인 MIDI 메시지를 포함한다. 이러한 메시지는 오디오를 전송하지 않습니다. 대신 음악을 재생하거나 제작하는 방법에 대한 지침을 보냅니다. 이 접근법을 통해 MIDI는 원시 오디오에 비해 믿을 수 없을 정도로 다재다능하고 경량화되어 스튜디오 환경에서 라이브 공연 및 상세한 편집 중 실시간 제어에 적합하다.\n' +
      '\n' +
      '일반적으로 확장자. 중간 또는 중간과 함께 MIDI 파일은 컴퓨터 소프트웨어 또는 전자 악기에 의해 재생될 수 있는 MIDI 메시지의 시퀀스를 저장한다. 이러한 파일들은 다양한 악기 사운드로 쉽게 수정, 전치 또는 재생될 수 있는 형태로 음악 작곡을 공유 및 배포할 수 있게 하여 디지털 음악 영역 고유의 유연성 및 창작의 자유도를 제공한다.\n' +
      '\n' +
      '그림 5: 그림 4에 제시된 같은 민속 가락의 미디에 해당하는 악보 예.\n' +
      '\n' +
      'MIDI를 생성하고 편집할 수 있는 여러 가지 방법이 있으며, 각 방법은 서로 다른 워크플로우와 환경설정을 제공합니다.\n' +
      '\n' +
      '**미디 키보드 및 컨트롤러**: 음악가들이 컴퓨터에 노트를 입력하고 데이터를 제어할 수 있는 물리적 장치. 그들은 전통 악기를 모방하고 미디 형식으로 공연의 뉘앙스를 포착할 수 있다.\n' +
      '***피아노 롤**: 디지털 오디오 워크스테이션에서의 그래픽 인터페이스(도 1에 도시된 바와 같이) 음표는 음정과 시간을 나타내는 그리드에 표시되고 편집됩니다. 선율, 화음, 리듬을 구성하는 데 직관적입니다.\n' +
      '**점수 편집기**: 사용자가 전통적인 표기법으로 음악을 입력하고 편집할 수 있도록 하며, 작성된 음을 미디 데이터로 변환한다. 음악적 표기법에 익숙한 사람들에게 이상적입니다.\n' +
      '**프로그래밍 및 스크립팅**: MIDI 데이터를 생성하고 조작하기 위해 코드를 작성하는 것과 관련되어 수동 입력 방법을 넘어 복잡하고 생성 가능한 음악적 가능성을 제공한다. 예를 들어, 복잡한 MIDI 시퀀스 및 자동화는 Max/MSP, Pure Data, 또는 프로그래밍 언어(예를 들어, SuperCollider)와 같은 소프트웨어를 사용하여 생성될 수 있다.\n' +
      '\n' +
      'MIDI는 음악가와 제작자에게 엄청난 이점을 제공하지만, ABC 표기법과 같이 MIDI 데이터를 다시 악보 중심의 상징 음악 형식으로 변환할 때 이러한 형식 간의 본질적인 차이로 인해 특정 문제가 발생한다. MIDI는 각 음표의 타이밍, 속도, 표현력을 포함하여 음악이 연주되는 방식을 포착하는 데 탁월합니다. 대조적으로 ABC 표기법은 인간의 읽기와 쓰기에 더 접근가능하지만 연주 뉘앙스 측면에서 덜 상세한 텍스트 형식을 사용하여 멜로디, 화성, 리듬과 같은 음악의 구조적 요소에 초점을 맞춘다.\n' +
      '\n' +
      'MIDI와 ABC 표기법 사이의 변환을 설명하기 위해, 다음의 예를 고려한다:\n' +
      '\n' +
      '```\n' +
      'X:l L:1/8 M:3/4 K:D def3 g|ag/<G/A4|fga a3 g|fd e4 AB=c3 d|ef/<g/f4|def3 g|ag/<g/e4|AB/<G/A4|fga a3|fga a3 g|fd d4|F3 A4|F3 D|f/G/4c/4A/4 B2 A2|F3 D|f/4d/4c/4A/4 B2 A2|F3 D|f/4d/4d/4A/4 B2 A2|F3 A3 D|f/4d/4d/4A/4 B2 A2|F3 D|f/4d/4d/4A/4 B2 A2|F3 D|f/4D/4d/4A/4 B2 A2|F3 D|f/4D/4d/4A/4 B2 A2|F3 A4|F3 D|f/4d/4\n' +
      '```\n' +
      '\n' +
      '이것은 그림 4에서 설명한 것과 동일한 곡조이지만 이제 ABC 표기법에서 MIDI로 변환된 후 다시 ABC 표기법으로 변환되었다. 이 ABC 시퀀스에서 렌더링된 해당 악보는 그림 5에 나와 있다. 미디에서 반복 부호가 제거되어 ABC 시퀀스와 악보가 원본보다 훨씬 더 길게 나타나는 것을 관찰할 수 있으며, 반복 섹션이 명시적으로 작성되어야 한다. 또한 MIDI-to-ABC 변환의 막대 분할과 장식품은 원래 ABC 표기법과 완벽하게 일치하지 않을 수 있다. 이러한 장신구는 종종 미디에서 일련의 빠른 음표로 표현되며, 이는 ABC 표기법으로 다시 번역을 복잡하게 할 수 있다.\n' +
      '\n' +
      'MIDI-렌더링된 악보와 그것의 ABC 표기 대응물은 수행될 때 동일하게 들릴 수 있지만, MIDI 전사의 시각적 복잡성은 ABC 버전보다 판독을 더 어렵게 할 수 있다. 이러한 불일치는 MIDI와 ABC가 서로 다른 컨텍스트에 맞게 조정된 형식이라는 사실을 강조한다. MIDI는 재생 충실도에 필수적이지만 ABC 표기법만큼 시각적으로 간단하지 않은 역학 및 미묘한 타이밍 변화와 같은 미묘한 성능 데이터를 캡처하는 데 능숙하다. 반면, ABC 표기법은 단순화된 기호 표상을 가지고 있어 상세한 성능 정보를 전달하는 데 이상적이지 않다.\n' +
      '\n' +
      '두 형식 모두 음악적 정보를 표현하는 능력이 중복되지만, 각각 다른 형식으로 변환할 때 완벽하게 번역되지 않는 독특한 특성을 가지고 있다. 이것은 정보의 손실을 초래한다: 각 포맷에 특정한 뉘앙스는 변환 프로세스 동안 완전히 감소되거나 생략될 수 있다. 이러한 손실은 ABC와 MIDI 형식 사이를 이동할 때 음악 작품의 전체 의도와 세부 사항을 보존하는 데 어려움을 나타낸다.\n' +
      '\n' +
      '## 부록 B CPU 상태 데이터세트\n' +
      '\n' +
      'CPU 상태 데이터셋은 CPU의 동작을 시뮬레이션하기 위해 파이썬 스크립트를 사용하여 제작되어 하드웨어 동작을 모델링할 때 bGPT의 기능을 평가할 수 있는 제어된 환경을 제공한다. 이 부록은 데이터 세트의 개요를 제공하여 데이터 세트 큐레이션의 구조, 명령어 세트 및 세부 정보를 소개한다.\n' +
      '\n' +
      '데이터세트는 CPU 레지스터 상태들을 나타내는 후속 16바이트 시퀀스와 각각 쌍을 이루는 1KB 메모리 블록들의 엔트리들을 포함한다. 메모리 블록들은 다양한 수의 머신 명령어들을 인코딩하는 반면, 레지스터 상태 시퀀스는 각 명령어들의 실행 후 CPU 상태를 포함한다. 상기 각 상태의 CPU 레지스터들은,\n' +
      '\n' +
      '**PC(Program Counter)**: 1바이트 크기, 메모리 내의 다음 명령어를 가리킨다.\n' +
      '**Accumulator(ACC)**: 산술 및 논리 연산에 관여하는 1바이트 레지스터.\n' +
      '**IR(Instruction Register)**: 4바이트를 점유하며, 현재 실행된 명령어를 저장한다.\n' +
      '***일반-목적 레지스터**: 추가의 10 바이트가 할당되어, 다양한 계산 목적으로 사용되는 레지스터(라벨링된 A-J)의 범위를 에뮬레이트한다.\n' +
      '\n' +
      '상태들 사이의 전환들은 CPU의 실행 사이클을 모방하고, 메모리로부터의 명령들을 페치하고, 필요한 동작을 결정하기 위해 그것들을 디코딩하고, 상태를 수정하기 위해 동작을 실행하는 것을 포함한다. 실행 프로세스는 레지스터 값들을 수정하고, 산술 계산을 수행하고, 프로그램 흐름을 제어하기 위한 명령어들을 포함한다.\n' +
      '\n' +
      '데이터세트 내의 각각의 명령어는 4바이트 포맷 OP ADDR1 ADDR2 ADDR3으로 인코딩되고, 각각의 바이트는 특정 목적을 서빙한다:\n' +
      '\n' +
      '**OP(Operation Code)**: 실행될 명령의 opcode를 나타내는 1 바이트.\n' +
      '**ADDR1, ADDR2, ADDR3**: 각각 1바이트로, 명령어가 상호 작용할 수 있는 최대 3개의 주소를 나타낸다.\n' +
      '\n' +
      '상기 opcode와 어드레스들은 함께 CPU가 취할 액션을 기술한다. 지침은 제로 주소에서 3 주소 지침까지 사용하는 주소 수에 따라 다릅니다. 이러한 가변성은 단순한 데이터 이동에서 복잡한 산술 및 논리 함수에 이르기까지 광범위한 연산의 시뮬레이션을 가능하게 한다.\n' +
      '\n' +
      '표 7(페이지 19에 위치함)은 어드레스 필드들 내의 값들에 따라, 명령 세트 및 각 명령들이 가질 수 있는 변형들을 상세히 설명한다. 실행될 변형은 어드레스 필드들 내의 비제로 값에 의해 결정된다. 제로-어드레스 명령어의 경우, 모든 어드레스 필드는 제로이다. 단일 주소 명령의 경우 ADDR1은 관련된 레지스터의 주소를 유지하고 다른 필드는 0이다. 2- 및 3-어드레스 명령어들에 대해, 각각의 필드들은 동작에 관련된 레지스터들의 어드레스들을 보유한다.\n' +
      '\n' +
      '명령 세트를 설계함에 있어서, 간단한 시뮬레이션을 방해할 수 있는 무한 루프 또는 다른 복잡성의 발생을 방지하기 위해 점프 명령과 같은 복잡한 명령은 의도적으로 제외되었다.\n' +
      '\n' +
      '파이썬 스크립트는 시뮬레이션마다 ACC와 범용 레지스터를 랜덤하게 초기화하여 CPU State Dataset을 생성함으로써 넓은 범위의 시작 조건을 보장한다. 그런 다음 CPU 상태에 대한 각 명령의 효과가 기록되면서 길이가 1에서 256까지 변하는 명령들의 무작위 시퀀스를 실행한다. 이는 실행된 명령어의 누적 영향을 반영하는 일련의 상태를 초래한다. 스크립트는 각 시퀀스가 프로그램의 끝을 표시하는 중지 명령 HLT로 절정에 이르도록 보장한다. 또한, 명령과 관련된 레지스터 변경만을 고려할 때, 다양한 명령 유형, 레지스터 및 그 값의 조합으로 인해 5억 1,600만 개 이상의 잠재적인 시나리오가 있다.\n' +
      '\n' +
      '이렇게 생성된 데이터 세트는 트레이닝 세트에 대한 210만 개의 엔트리 및 평가 세트에 대한 21,000 개의 엔트리로 구성된다. 이 과정은 무작위 초기 상태와 다양한 명령어 길이를 사용하여 CPU 상태 모델링에서 bGPT의 훈련 및 효과 평가에 필수적인 다양한 데이터 세트를 생성한다.\n' +
      '\n' +
      'CPU의 동작 로직의 보다 직관적인 이해를 돕기 위해, 인간-판독가능한 형태로 그들의 대응하는 상태들과 함께 10개의 명령들을 실행하기 위한 랜덤하게 생성된 프로그램이 아래에 제공된다:\n' +
      '\n' +
      '프로그램 : [\'MUL J A\', \'DIV I\', \'MUL E D\', \'ADD C\', \'LOAD1 66\', \'MOV A H\', \'AND D E\', \'Pop H\', \'CLR\', \'HLT]\n' +
      '\n' +
      '단계 0에서의 상태는:\n' +
      '\n' +
      'PC: 0\n' +
      '\n' +
      'ACC: 146\n' +
      '\n' +
      'IR: HLT\n' +
      '\n' +
      '레지스터: [\'A\':=19, \'B\':=55, \'C\':=245, \'D\':=35, \'E\':=174, \'F\':=185, \'G\':9, \'H\':20, \'I\':=140, \'J\':=2]\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '데이터 세트는 머신 러닝 연구, 특히 CPU 동작을 예측할 수 있는 모델 개발에 사용하기 위한 것이다. 그것은 또한 학생들이 CPU 작동 원리를 이해하는 데 도움이 되는 교육 도구 역할을 할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|p{56.9pt}|p{56.9pt}|p{284.5pt}|} \\hline\n' +
      '**Instruction** & **Address Variants** & **Meaning** \\\\ \\hline HLT & 0 & Halts the CPU with “HLT” \\\\ \\hline CLR & 0, 1 & Zeroes the accumulator with “CLR” or a specific register with “CLR A” \\\\ \\hline INC & 0, 1 & Increments the accumulator with “INC” or a specific register with “INC A” \\\\ \\hline DEC & 0, 1 & Decrements the accumulator with “DEC” or a specific register with “DEC A” \\\\ \\hline SHL & 0, 1 & Shifts the accumulator left by one bit with “SHL” or a specific register with “SHL A” \\\\ \\hline SHR & 0, 1 & Shifts the accumulator right by one bit with “SHR” or a specific register with “SHR A” \\\\ \\hline ROL & 0, 1 & Rotates the accumulator left by one bit with “ROL” or a specific register with “ROL A” \\\\ \\hline ROR & 0, 1 & Rotates the accumulator right by one bit with “ROR” or a specific register with “ROR A” \\\\ \\hline NOT & 0, 1 & Performs bitwise NOT on the accumulator with “NOT” or a specific register with “NOT A” \\\\ \\hline PUSH & 1 & Pushes the value of a specific register onto the stack with “PUSH A” \\\\ \\hline POP & 1 & Pops the top of the stack into a specific register with “POP A” \\\\ \\hline LOADI & 1 & Loads an immediate value into the accumulator with “LOADI value” \\\\ \\hline SWAP & 1, 2 & Swaps values between the accumulator and a register with “SWAP A” or between two registers with “SWAP A B” \\\\ \\hline ADD & 1, 2, 3 & Adds using one, two, or three addresses with examples “ADD A”, “ADD A B”, “ADD A B C” \\\\ \\hline SUB & 1, 2, 3 & Subtracts using one, two, or three addresses with examples “SUB A”, “SUB A B”, “SUB A B C” \\\\ \\hline MUL & 1, 2, 3 & Multiplies using one, two, or three addresses with examples “MUL A”, “MUL A B”, “MUL A B C” \\\\ \\hline DIV & 1, 2, 3 & Divides using one, two, or three addresses with examples “DIV A”, “DIV A B”, “DIV A B C” \\\\ \\hline AND & 1, 2, 3 & Performs bitwise AND using one, two, or three addresses with examples “AND A”, “AND A B”, “AND A B C” \\\\ \\hline OR & 1, 2, 3 & Performs bitwise OR using one, two, or three addresses with examples “OR A”, “OR A B”, “OR A B C” \\\\ \\hline XOR & 1, 2, 3 & Performs bitwise XOR using one, two, or three addresses with examples “XOR A”, “XOR A B”, “XOR A B C” \\\\ \\hline MOV & 2 & Moves a value from one register to another with “MOV A B” \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 정의된 명령어 세트 및 상이한 명령어들의 어드레스 변형들.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>