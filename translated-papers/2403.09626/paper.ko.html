<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 비디오 맘바 스위트: 비디오 이해를 위한 다용도 대안으로서의 상태 공간 모델\n' +
      '\n' +
      'Guo Chen\n' +
      '\n' +
      '1남경대학교 2OpenGV Lab, Shanghai AI Laboratory\n' +
      '\n' +
      'Yifei Huang\n' +
      '\n' +
      '상하이 AI 연구소 2OpenGV Lab\n' +
      '\n' +
      'Jilan Xu\n' +
      '\n' +
      '4Zhejiang University\n' +
      '\n' +
      'Baoqi Pei\n' +
      '\n' +
      '1남경대학교 2OpenGV Lab, Shanghai AI Laboratory\n' +
      '\n' +
      'Zhe Chen\n' +
      '\n' +
      '1남경대학교 2OpenGV Lab, Shanghai AI Laboratory\n' +
      '\n' +
      'Zhiqi Li\n' +
      '\n' +
      '1남경대학교 2OpenGV Lab, Shanghai AI Laboratory\n' +
      '\n' +
      'Jiahao Wang\n' +
      '\n' +
      '1남경대학교 2OpenGV Lab, Shanghai AI Laboratory\n' +
      '\n' +
      'Kunchang Li\n' +
      '\n' +
      '상하이 AI 연구소 2OpenGV Lab\n' +
      '\n' +
      'Tong Lu\n' +
      '\n' +
      '1남경대학교 2OpenGV Lab, Shanghai AI Laboratory\n' +
      '\n' +
      'Limin Wang\n' +
      '\n' +
      '1남경대학교 2OpenGV Lab, Shanghai AI Laboratory\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '비디오를 이해하는 것은 RNN, 3D CNN, 트랜스포머와 같은 다양한 아키텍처를 탐구하는 데 전념하는 광범위한 노력으로 컴퓨터 비전 연구의 기본 방향 중 하나이다. 새롭게 제안된 상태 공간 모델인 Mamba 구조는 긴 시퀀스 모델링의 성공을 비디오 모델링으로 확장할 수 있는 유망한 특성을 보여준다. Mamba가 비디오 이해 영역에서 트랜스포머에 대한 실행 가능한 대안이 될 수 있는지 평가하기 위해, 본 연구에서는 Mamba가 비디오 모델링에서 수행할 수 있는 다양한 역할을 조사하는 동시에 Mamba가 우월성을 나타낼 수 있는 다양한 작업을 조사하는 포괄적인 연구 세트를 수행합니다. 동영상 모델링을 위해 Mamba를 4개의 역할로 분류하고, 14개의 모델/모듈로 구성된 Video Mamba Suite(1)를 도출하여 12개의 동영상 이해 작업에 대해 평가한다. 우리의 광범위한 실험은 유망한 효율성-성능 절충점을 보여주면서 비디오 전용 및 비디오 언어 작업 모두에서 맘바의 강력한 잠재력을 보여준다. 이 작업이 비디오 이해에 대한 향후 연구에 귀중한 데이터 포인트와 통찰력을 제공할 수 있기를 바랍니다. 코드는 공개된다: [https://github.com/OpenGVLab/video-mamba-suite](https://github.com/OpenGVLab/video-mamba-suite)\n' +
      '\n' +
      '키워드: 비디오 이해 상태 공간 모델 Mamba\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '비디오 이해는 컴퓨터 비전 연구에서 근본적인 문제로, 활동을 현지화하거나 진화를 추론하기 위해 비디오에서 시공간적 역학을 포착해야 한다. 비디오 이해를 위한 아키텍처에 대한 현재 탐색은 세 가지 범주로 나눌 수 있다. 첫 번째 작업 라인은 프레임 기반 특징 인코딩과 GRU 및 LSTM과 같은 순환 네트워크를 통한 시간적 종속성 모델링을 사용한다. 이러한 유형의 분할된 시공간 모델링은 조인트 시공간 정보를 캡처할 수 없기 때문에, 작업의 또 다른 스트라이프는 컨볼루션 신경망에서 3D 커널을 사용하여 공간적 및 시간적 상관 관계를 동시에 고려한다[25, 73].\n' +
      '\n' +
      '언어 [62, 63, 70, 76] 및 이미지 [61, 69, 5, 18, 34] 트랜스포머의 승리에 이어 비디오 트랜스포머 [2, 80, 68]도 상당한 보폭 invideo 이해도를 만들어 RNN 및 3D-CNN에 비해 더 강력한 기능을 나타낸다. 비디오 트랜스포머는 토큰의 시퀀스 내에서 비디오를 캡슐화하고, 어텐션 메커니즘은 글로벌 컨텍스트 상호 작용 및 데이터 의존적 동적 계산을 가능하게 할 수 있다. 결과적으로, 모델은 통합된 방식으로 비디오 내에서 시간 [10, 78, 89, 90] 또는 시공간 [6, 68] 정보를 처리하는 데 능숙하다. 긴 비디오에서 비디오 변압기의 제한된 계산 효율로 인해 여러 변형 [2, 6, 20, 55]가 등장하여 속도-성능 트레이드오프의 균형을 이룬다.\n' +
      '\n' +
      '최근, 상태 공간 모델(State Space Model, SSM)은 자연어 처리(Natural Language Processing, NLP)에서 장점을 증명하고 있다. 현대의 SSM[32]은 선형-시간 복잡성을 유지하면서 특히 긴 시퀀스 모델링에서 NLP에서 강한 표현 능력을 나타낸다. 이는 이들의 선택 메커니즘이 완전한 컨텍스트를 저장할 필요성을 제거할 수 있기 때문이다. 특히, Mamba[30]은 시변 파라미터를 SSM에 통합하고, 매우 효율적인 훈련 및 추론을 가능하게 하는 하드웨어 인식 알고리즘을 제안한다. 맘바의 인상적인 스케일링 성능은 변압기의 유망한 대안임을 나타냅니다. 한편, 맘바의 강력한 성능과 효율성은 영상 이해 작업에 매우 적합합니다. 그러나 Mamba가 이미지 모델링[53, 96]에서 어떻게 적용될 수 있는지 탐구하려는 일부 초기 시도에도 불구하고 비디오 이해에 대한 효과는 여전히 불분명하다. 맘바의 영상 이해 가능성에 대한 포괄적인 연구의 부재는 다양한 범위의 영상 관련 과제에서 그 역량에 대한 추가 탐구를 방해한다.\n' +
      '\n' +
      '본 논문에서는 새로운 방법을 제안하지 않는다. 대신, 우리는 비디오 이해의 맥락에서 맘바가 예시하는 SSM의 잠재력에 대한 광범위한 조사를 수행한다. 우리의 목표는 맘바가 이 도메인의 변압기에 대한 실행 가능한 대안이 될 수 있는지 여부를 평가하는 것이다. 이를 위해, 우리는 Mamba가 비디오를 이해하는 데 잠재적으로 수행할 수 있는 다양한 역할을 탐색하고 Mamba가 우위를 나타낼 수 있는 다양한 작업을 조사합니다. 우리는 분류한다\n' +
      '\n' +
      '그림 1: 우리는 Mamba로 예시된 SSM을 비디오 이해에 대해 조사한다. 저희 비디오 맘바 스위트는 12개의 비디오 이해 작업을 위한 14개의 SSM 모델/모듈로 구성되어 있습니다. 우리는 비디오 모델링에서 SSM의 4가지 역할을 탐색하고 13개의 주요 데이터 세트에 대한 광범위한 실험을 수행한다.\n' +
      '\n' +
      'Mamba를 1) 시간적 모델, 2) 시간적 모듈, 3) 다중 모달 상호작용 네트워크, 4) 공간적-시간적 모델의 4가지 역할로 모델링한다. 각 역할에 대해 다양한 비디오 이해 작업에 대한 비디오 모델링 기능을 조사합니다. 변압기와의 공정한 비교를 위해, 우리는 표준 또는 개선된 변압기 아키텍처를 기반으로 상대 모델을 신중하게 선택한다. 우리의 탐색은 12개의 비디오 이해 작업을 위한 14개의 모델/모듈로 구성된 비디오 맘바 스위트를 도출합니다. 비디오 맘바 스위트가 비디오 이해 분야에서 SSM 기반 모델의 향후 탐구를 위한 귀중한 자원이 되기를 바랍니다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### Video Modeling\n' +
      '\n' +
      '비디오 모델링은 깊은 비디오 이해도를 달성하기 위한 초석이 되며 시간이 지남에 따라 상당한 발전을 거쳤다. 초기에, TSN[82]은 균일한 비디오 샘플링을 채용하고, 비디오 표현들을 생성하기 위한 멀티-프레임 컨센서스를 확립하기 위해 2D 네트워크[35, 40]를 활용하였다. 그 후, 비디오 컨볼루션 네트워크의 실질적인 발전이 이루어졌다. [48, 75, 81]과 같은 접근법은 시공간 모델링을 용이하게 하기 위해 2D 네트워크에 시간 모듈을 통합하는 데 중점을 두었다. 반대로, [9, 25, 60, 73, 74, 87]과 같은 방법들은 2D 컨볼루션 네트워크들[35, 40, 86]의 커널들을 증강시키거나 3D 컨볼루션 네트워크들을 처음부터 훈련시켜 공간적-시간적 특징들을 추출한다.\n' +
      '\n' +
      '그럼에도 불구하고, 컨볼루션 네트워크는 정적 로컬 연산자에 의존하는 것에 의해 제약되어, 제한된 표현 용량을 초래한다. 언어 트랜스포머[8, 17, 62, 63, 70]와 이미지 트랜스포머[11, 18, 20, 34, 54, 69, 84]의 효과에서 영감을 받아, 비디오 트랜스포머의 다양한 구조가 연구자들[2, 6, 55, 45]에 의해 조사되었다. 특히, 공간-시간 공동 주의를 통합한 비디오 트랜스포머는 이러한 구조들 중에서 우수한 능력을 보여준다. 결과적으로 후속 연구 노력[24, 46, 68, 80, 85]은 구조에 따라 다른 사전 훈련 방법론을 탐구한다. 그러나 공동 주의와 관련된 2차 계산 복잡성은 심각한 장애물을 제기하여 더 긴 컨텍스트를 처리하기 위한 비디오 변압기의 확장성을 방해하며, 이는 LLM[71, 72]이 직면한 문제와 유사하다. 여러 연구에서 128개 이상의 프레임으로 구성된 긴 형식의 비디오를 모델링하기 위해 맞춤형 변압기 기반 변형 [2, 6, 55]를 개발했다.\n' +
      '\n' +
      '대안적으로, 다른 접근법은 선형 복잡성을 갖는 모델 아키텍처를 설계하는 것이다. 예를 들어, RetNet[67] 및 RWKV[59]는 전역 정보를 캡처하기 위해 지수 감쇠 기술을 활용했다. 상태 공간 모델[31, 32, 33]은 또한 선형 복잡성을 제공하며, Mamba[30]은 데이터 의존적 추론을 용이하게 하기 위해 효율적인 구현을 사용한다. 비전 분야에서, 일부 작품 [1, 19, 96]은 선형 복잡성을 갖는 비전 인코더를 탐색했다. XCiT [1]은 입력 토큰 간의 교차 분산을 계산하여 선형 복잡도로 전역 상호 작용을 달성했다. 최근 [53, 96]은 Mamba 기반 비전 애플리케이션에 대한 초기 탐사를 수행하였다. 본 연구에서는 영상 이해를 위한 Mamba 기반 영상 모델 생성을 연구한다.\n' +
      '\n' +
      '### State-Space Models (SSMs)\n' +
      '\n' +
      'State-Space Models(SSM) 시대의 선두주자로서 [32]는 구조화된 State-Space Sequence(S4)라는 새로운 모델을 도입했으며, 이는 장거리 의존성을 포착하기 위한 CNN 및 변압기에 대한 대안을 제공한다. S4 모델은 시퀀스 길이에 따라 선형적으로 스케일링되는 유망한 특성을 나타낸다. 이를 바탕으로 [65]는 MIMO SSM과 효율적인 병렬 스캐닝을 S4 아키텍처에 통합하는 S5로 더빙된 고급 레이어를 제안했다. 이 개발은 SSM의 제약을 극복하고 효능을 향상시키려고 한다. 또한, [26]은 새로운 SSM 계층인 H3에 기여하여 언어 모델링에서 SSM과 변압기 기반 주의력 사이의 성능 격차를 크게 줄였다. [57] 게이티드 스테이트 스페이스 레이어에 게이팅 유닛을 추가로 도입하여 S4 모델을 확장하여 표현성을 높였다. 보다 최근에는 [30]에서 데이터 종속 SSM 계층을 도입하여 Mamba라는 다재다능한 언어 모델을 개발하였다. Mamba는 다양한 크기의 대규모 실제 데이터에 걸쳐 성능 면에서 변압기를 능가하며 시퀀스 길이에 따른 선형 스케일링을 보여준다. 본 연구에서는 언어 영역에서 맘바의 성공을 비디오 이해로 확장하는 것을 목표로 한다. 구체적으로, 비디오 이해를 위한 Mamba의 성능을 개발, 검증 및 분석하기 위해 Video Mamba Suite라고 하는 다용도 프레임워크를 구축한다.\n' +
      '\n' +
      '동영상 이해를 위해 SSM을 사용하는 작업은 제한적일 뿐, 장기 동영상 분류[41, 79]. [41]에 주안점을 두고 있다. S4 아키텍처를 확장하고 멀티-스케일 시간적 S4 디코더를 비디오 시퀀스 분류에 통합한다. [79] 상태 공간 모델을 증강하기 위해 마스킹된 대조 학습 프레임워크를 도입했다. 우리의 연구는 SSM의 적용 범위를 훨씬 더 광범위한 범위의 비디오 이해 작업으로 확장한다. 우리는 Mamba [30]을 예로 사용하여 SSM의 여러 사용 및 더 많은 비디오 관련 작업을 다루며 긴 비디오의 분류에 대한 유일한 초점을 넘어 이동합니다.\n' +
      '\n' +
      '## 3 Preliminaries\n' +
      '\n' +
      '국가 우주 모델\n' +
      '\n' +
      '본 절에서는 구조 상태 공간(Structured State-Space, SSM)을 기반으로 한 모델, 구체적으로 S4[32]와 Mamba[30] 모델을 소개한다. 이러한 모델은 시퀀스 또는 기능을 처리하는 연속 시스템에서 영감을 끌어낸다. 입력의 순서를 시간에 따라 \\(x(t)\\)로 하고, 입력들을 숨은 상태 \\(h(t)\\)를 통해 변환하면서 출력의 순서를 \\(y(t)\\)으로 생성하는 시스템을 상상해 보자.\n' +
      '\n' +
      '이 시스템은 은닉 상태의 진화를 정의하기 위해 행렬 \\(\\mathbf{A}^{\\mathrm{N}\\times\\mathrm{N}\\)을 사용하고, 입력 상태와 은닉 상태를 출력으로 각각 투영하기 위해 \\(\\mathbf{B}^{\\mathrm{N}\\times1}\\)과 \\(\\mathbf{C}^{1\\times\\mathrm{N}\\)을 사용한다. 이 과정은 \\(h^{\\prime}(t)=\\mathbf{A}h(t)+\\mathbf{B}x(t),y(t)=\\mathbf{C}h(t)\\으로 요약될 수 있다.\n' +
      '\n' +
      'S4와 Mamba는 이러한 연속 시스템의 이산 등가물로 설계되었다. 그들은 연속 매개변수(\\(\\mathbf{A}\\),\\(\\mathbf{B}\\))를 이산 대응 변수(\\(\\overline{\\mathbf{A}},\\overline{\\mathbf{B}}\\)로 변환하기 위해 시간 척도 매개변수 \\(\\Delta\\)를 통합한다. 이 변환은 0차 홀드(zero-order Hold) 기법을 사용하여 이산 수식 \\(\\overline{\\mathbf{A}}=\\exp(\\Delta\\mathbf{A}),\\overline{\\mathbf{B}}=(\\Delta\\mathbf{A})^{-1}(\\exp(\\Delta\\mathbf{A})-\\mathbf{I})\\cdot\\Delta\\mathbf{B}\\을 생성한다. 매개변수가 이산화되면, 각 이산시간단계에서의 시스템의 거동은 \\(h_{t}=\\overline{\\mathbf{A}}h_{t-1}+\\overline{\\mathbf{B}}x_{t},y_{t}=\\mathbf{C}h_{t}\\)에 의해 주어지며, 숨겨진 상태가 어떻게 업데이트되고 출력을 생성하는지를 보여준다.\n' +
      '\n' +
      '마지막으로, 구조화된 컨볼루션 커널을 포함하는 전역 컨볼루션 과정을 통해 출력을 계산한다. \\(\\overline{\\mathbf{K}=(\\mathbf{C}\\overline{\\mathbf{B}},\\mathbf{C}\\overline{\\mathbf{A}}\\overline{\\mathbf{B}},...,\\mathbf{C}\\overline{\\mathbf{A}}^{\\text{-1}}\\overline{\\mathbf{B}}}). 이 커널은 변환된 투영 파라미터로부터 구성되며 최종 출력 시퀀스 \\(\\mathbf{y}\\), _i.e. \\\\ (\\mathbf{y}=\\mathbf{x}*\\overline{\\mathbf{K}\\). 여기서, \\(\\mathtt{M}\\)은 입력 시퀀스 \\(\\mathbf{x}\\)의 길이이다. 연속 역학을 이산 단계로 변환함으로써 S4 및 맘바 모델은 복잡한 종속성을 갖는 서열의 처리를 허용한다.\n' +
      '\n' +
      '### Mamba Block\n' +
      '\n' +
      '맘바 블록[30]은 도 2의 (a)에 예시된 바와 같이, 게이트 어텐션 유닛(GAU)[38]에서 영감을 받아 선형 어텐션 연산자와 MLP 블록을 조합한다. 이 구조는 제어 가능한 확장 인자\\(\\mathtt{E}\\)에 의해 모델 차원\\(\\mathtt{D}\\)을 확장하는 것을 포함한다. 각 블록에 대해 대부분의 매개변수(\\(\\mathtt{3ED}^{2}\\))는 선형 투영(\\(\\mathtt{2ED}^{2}\\)의 입력 투영,\\(\\mathtt{ED}^{2}\\)의 출력 투영)에 있는 반면 내부 SSM은 덜 기여한다. SM 매개변수(\\(\\Delta\\), \\(\\mathbf{B}\\), \\(\\mathbf{C}\\) 및 행렬 \\(\\mathbf{A}\\)의 수는 비교에서 훨씬 적다.\n' +
      '\n' +
      '### ViM Block\n' +
      '\n' +
      'ViM 블록[96]은 도 2의 (b)에 도시된 바와 같이, 추가적인 파라미터를 갖는 역방향 선택적 스캐닝 분기를 맘바 블록에 추가한다. 이에 기초하여, 두 스캐닝 방향에 대한 특징들은 동일한 선형 투영 및 게이티드 층들을 공유한다. 입력\\(\\mathbf{x}\\)이 주어지면, 각 방향에 대해 ViM 블록은 먼저 입력\\(\\mathbf{x}\\)에 1D 컨벌루션을 적용하고 \\(\\mathbf{x}_{o}^{\\prime}\\)을 얻는다. 그런 다음 블록은 각각 \\(\\mathbf{x}_{o}^{\\prime}\\)을 \\(\\mathbf{B}_{o},\\mathbf{C}_{o},\\Delta_{o}\\)으로 선형 투영한다. \\\\ 그리고 나서 \\(\\overline{\\mathbf{A}}_{o}\\)와 \\(\\overline{\\mathbf{B}}_{o}\\)을 각각 변환하기 위해 (\\delta_{o}\\)를 사용한다. 다음으로, SSM은 순방향 스캔 특징\\(\\mathbf{y}_{f}\\)과 역방향 스캔 특징\\(\\mathbf{y}_{b}\\)을 계산한다. 마지막으로, 두 피처들은 게이티드 레이어에 의해 게이팅되고 출력 토큰 시퀀스를 얻기 위해 평균화된다.\n' +
      '\n' +
      '그림 2: 세 개의 SSM 블록의 그림. (a)는 바닐라 맘바 블록[30]이다. (b)는 ViM 블록[96]이다. (c)는 입력 프로젝터를 분리하고 두 스캐닝 방향에서 SSM의 매개변수를 공유하는 제안된 DBM 블록이다.\n' +
      '\n' +
      '### DBM Block\n' +
      '\n' +
      '우리는 그림 2(c)에 표시된 대로 DBM으로 표시된 분해된 양방향 맘바 블록을 추가로 조사한다. ViM 블록에 비해 역설계가 적용되었습니다. 입력 시퀀스\\(\\mathbf{x}\\)이 주어졌을 때, DBM 블록은 초기에 뚜렷한 선형 레이어를 사용하여 순방향 특징\\(\\mathbf{x}_{f}\\)과 역방향 특징\\(\\mathbf{x}_{b}\\)을 분리한다. 이러한 특징들은 양방향 스캐닝을 위해 공유된 파라미터들과 함께 SSM 모듈에 전달되어 \\(\\mathbf{x}_{f}^{{}^{\\prime}\\) 및 \\(\\mathbf{x}_{b}^{{}^{\\prime}\\)이 생성된다. 이어서, 두 피처들은 출력 토큰 시퀀스를 생성하기 위해 연접되기 전에 두 개의 별개의 레이어들에 의해 게이팅을 겪는다. 이 블록은 방향성 바이어스를 도입하고 특정 역학을 약화시킨다. 섹션 4.1에 자세히 설명된 실험에서 DBM 블록이 소규모 데이터 세트에서 향상된 성능을 보여주는 것을 관찰한다.\n' +
      '\n' +
      '##4 비디오 맘바 스위트룸\n' +
      '\n' +
      '이 섹션에서는 다양한 비디오 이해 작업에 대한 다양한 맘바 기반 모델을 포함하는 비디오 맘바 스위트를 제시한다. 이 제품군은 시간 모델, 시간 모듈, 다중 모드 상호 작용 모델 및 시공간 시퀀스 모델의 네 가지 별개의 역할로 맘바를 활용한다. 각 역할은 별도의 하위 섹션에서 논의되며, 영상 이해에서 맘바의 성과를 점진적으로 드러내고 주요 특성을 강조할 것이다. 공간적인 제약으로 인해 더 많은 구현 세부 사항과 실험 결과가 보충 자료에 제공된다.\n' +
      '\n' +
      '비디오 시간 모델링을 위한### Mamba\n' +
      '\n' +
      '**작업 및 데이터세트.** 우리는 5개의 비디오 시간적 태스크에 걸친 Mamba의 성능을 평가한다: 시간적 액션 로컬리제이션(HACS Segment[91]), 시간적 액션\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Method & Block & mAP@0.5 & mAP@0.75 & mAP@0.95 & mAP@Avg \\\\ \\hline ActionFormer [90] & Window Attn [12] & 62.62 & 44.61 & 12.73 & 43.34 \\\\ ActionMamba [96] & ViM [96] & 63.78 & 45.45 & 13.01 & 44.26 \\\\ ActionMamba & DBM(ours) & **64.02** & **45.71** & **13.34** & **44.56** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: HACS 세그먼트에 대한 시간적 액션 국소화의 결과[91]. 미터법은 다중 tIoU 임계값 {0.5, 0.75, 0.95} 하에서 평균 평균 정밀도(mAP)이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline Method & Block & Acc & Edit & F1@10 & F1@25 & F1@50 \\\\ \\hline MS-TCN [21] & Dilated Conv, Enc & 76.3 & 79.0 & 85.8 & 83.4 & 69.8 \\\\ \\hline ASFmer [89] & Window Attn [12], Enc-Dec & **79.7** & 84.6 & 90.1 & 88.8 & 79.2 \\\\ ASFmer\\({}^{\\dagger}\\)[89] & Window Attn [12], Enc-Dec & 77.1 & 81.0 & 86.2 & 84.8 & 77.0 \\\\ ASFmer\\({}^{\\dagger}\\)[89] & Window Attn [12], Enc & 75.4 & 78.1 & 82.7 & 80.5 & 68.4 \\\\ ASMamba [96] & ViM [96], Enc & 79.3 & 87.0 & 90.3 & 89.0 & 77.9 \\\\ ASMamba & DBM (ours), Enc & 78.4 & **87.5** & **91.1** & **89.8** & **79.7** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: GTEA [22] 데이터세트에 대한 시간적 액션 분할의 결과. 상기 메트릭들은 정확도, 편집 거리[7], 및 다수의 tIoU 임계치{0.1, 0.25, 0.5} 하에서 인스턴스-방향 F1이다. \\ (\\dagger\\)는 공식 코드로 재현된 결과를 나타낸다.\n' +
      '\n' +
      '세그먼트화(GTEA[22]), 밀집 비디오 캡션화(ActivityNet[36], YouCook[95]), 비디오 단락 캡션화(ActivityNet[36], YouCook[95]), 및 액션 예상화(Epic-Kitchen-100[13])를 포함할 수 있다.\n' +
      '\n' +
      '기준선 및 경쟁사.우리는 각 작업의 기준선으로 변압기 기반 상대방을 선택한다. 구체적으로, 변압기 베이스라인은 ActionFormer[90], ASFormer[89], Testra[92], PDVC[83]이다. Mamba 챌린저를 구축하기 위해 기준 모델의 변압기 블록을 바닐라 Mamba[30], ViM[96], DBM을 포함한 Mamba 기반 블록으로 교체한다. 인과적 추론을 포함하는 행동 기대의 맥락에서, 우리는 기준선의 성능을 바닐라 맘바 블록과 비교한다[30].\n' +
      '\n' +
      '결과 및 분석.표 1과 표 5의 4가지 작업에 대한 서로 다른 모델의 비교 결과를 제시한다. 전반적으로 말하면, 일부 변압기 기반 모델은 성능 향상을 위해 주의 변형을 통합한 반면, 표는 변압기 시리즈의 기존 방법에 비해 Mamba 시리즈의 우수한 성능을 보여준다.\n' +
      '\n' +
      '_Temporal action localization.___Temporal action localization. 표 1에 묘사된 바와 같이, ActionFormer[90]의 트랜스포머 블록을 ViM 블록으로 변경하는 것을 기반으로 하는 제안된 ActionMamba는 HACS 세그먼트 데이터 세트에서 평균 44.26의 mAP를 달성한다. ActionMamba는 DBM 블록을 사용하여 평균 mAP를 44.56으로 더 향상시킬 수 있으며, 이는 1.22(44.56 _vs._43.34)만큼 변압기 대응물보다 훨씬 우수하다.\n' +
      '\n' +
      '_Temporal action segmentation.____Temporal action segmentation. 표 2는 시간적 행위 분할의 결과를 보고한다. 제안된 ASMamba는 재생된 AS이전 [89]에 비해 우수한 성능을 보여준다. ASMamba는 순수 인코더인 반면 ASFormer는 인코더-디코더 구조를 채택하기 때문에 성능 저하가 관찰되는 ASFormer의 인코더 버전을 실험한다. 이 결과는 트랜스포머에 비해 맘바의 큰 잠재력을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{Block} & \\multicolumn{4}{c}{ActivityNet} & \\multicolumn{4}{c}{YouCook2} \\\\ \\cline{3-10}  & & B-4 & M & R & C & B-4 & M & R & C \\\\ \\hline PDVC [83] & DeformAttn [97] & 9.07 & 12.52 & 29.02 & 13.12 & 6.44 & 12.94 & 28.74 & 12.48 \\\\ PDVC [83] & ViM [96] & **9.33** & 13.52 & 29.83 & 14.25 & 6.50 & 12.96 & 28.59 & **13.08** \\\\ PDVC & DBM (ours) & 9.05 & **14.05** & **29.86** & **14.43** & **7.24** & **13.47** & **29.09** & 13.03 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: ActivityNet [36] 및 YouCook2 [95]에 대한 비디오 단락 캡셔닝의 결과. 메트릭은 BLEU-4[58], METEOR[4], ROUGE-L[47], CIDEr[77]을 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{Block} & \\multicolumn{4}{c}{ActivityNet} & \\multicolumn{4}{c}{YouCook2} \\\\ \\cline{3-10}  & & B-4 & M & C & SODA & B-4 & M & C & SODA \\\\ \\hline PDVC [83] & DeformAttn [97] & 1.75 & 6.73 & 26.07 & **5.47** & 0.73 & 4.25 & 20.48 & 4.02 \\\\ PDVC [83] & ViM [96] & 1.68 & 6.92 & 26.26 & 5.33 & 0.71 & 4.32 & 20.59 & 4.09 \\\\ PDVC & DBM (ours) & **1.76** & **7.16** & **26.77** & 5.27 & **0.86** & **4.44** & **22.11** & **4.32** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: ActivityNet [36] 및 YouCook2 [95]에 대한 조밀한 비디오 캡션의 결과. 메트릭은 BLEU-4[58], METEOR[4], CIDEr[77] 및 SODA_c[27]을 포함한다.\n' +
      '\n' +
      'Dense video captioning.__Dense video captioning.__ 표 3은 조밀한 비디오 캡셔닝에 대한 결과를 보여준다. 베이스라인 PDVC 모델[83]은 시각적 정보를 인코딩하기 위해 변형 가능한 트랜스포머를 채택한다. 비교 결과, DBM 블록을 사용한 양방향 Mamba는 시간 이벤트 지역화와 자막 생성 모두에서 더 강한 성능을 보였다.\n' +
      '\n' +
      'captioning.__Video paragraph captioning.__ 표 4는 비디오 단락 캡셔닝에 대한 결과 비교를 보여준다. 또한 캡션 손실만으로 모델을 훈련함으로써 PDVC를 기본 모델[83]로 채택한다. 캡셔닝과 로컬리제이션이 모두 중요한 조밀한 비디오 캡셔닝 작업과는 달리, 비디오 단락 캡셔닝은 단지 캡션을 생성하기 위한 세밀한 시각적 정보를 추출하는 것에 초점을 맞추고 있다. 표 4의 결과는 우리의 양방향 맘바가 시간적 변형 인코더에 비해 캡셔닝을 위한 더 강한 특징 표현을 제공한다는 것을 보여준다.\n' +
      '\n' +
      '_Action anticipation. 우리는 행동 예측 작업을 통해 인과 모델링에서 맘바의 능력을 추가로 평가한다. 입력으로 5초 시간적 특징을 고려하여 테스트라[92]와 인과적 자기 주의 블록 및 인과적 맘바[30] 블록을 비교한다. 표 5에 제시된 바와 같이 결과는 표 4의 텍스트 생성에서의 결론과 일치하는 맘바의 우월한 인과 추론 능력을 보여준다.\n' +
      '\n' +
      'Cross-Modal Interaction을 위한 Mamba\n' +
      '\n' +
      '#### 4.2.1 작업 및 데이터 세트.\n' +
      '\n' +
      '단일 모달 작업 외에도 교차 모달 상호 작용에 대한 맘바의 성능을 평가한다. 먼저 비디오 시간 접지(VTG) 작업을 사용하여 평가한다. 관련 데이터 세트에는 Qv하이라이트[44] 및 샤레이드-STA[28]가 포함되어 있다.\n' +
      '\n' +
      '####4.2.2 기준선과 경쟁자.\n' +
      '\n' +
      '이 작업에서 UniVTG[50]을 사용하여 맘바 기반 VTG 모델을 생성한다. UniVTG는 다중 모드 상호 작용 네트워크로 변압기를 사용한다. 주어진 비디오 특징 \\(\\mathbff{V}=\\{\\mathbf{v}_{i}\\}_{i=1}^{L_{v}\\in\\mathbbb{R}^{L_{v}\\times D}\\)과 텍스트 특징 \\(\\mathbfff{Q}=\\{\\mathbff{q}_{j}\\}_{i=1}^{L_{q}\\in\\mathbbb{R}^{L_{pos}\\)과 학습 가능한 위치 임베딩 \\(\\mathbf{E}^{type}\\)을 각 모달리티에 추가하여 위치 및 모달리티 정보를 모두 유지한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{Block} & \\multicolumn{3}{c}{Overall} & \\multicolumn{3}{c}{Unseen} & \\multicolumn{3}{c}{Tail} \\\\ \\cline{3-11}  & & Ver & Nou & Act & Ver & Nou & Act & Ver & Nou & Act \\\\ \\hline Testra\\({}^{+}\\)[92] & long short Attn [76] & 30.8 & 35.8 & 17.6 & 29.6 & 26.0 & 12.8 & 23.2 & 29.2 & 14.2 \\\\ MAT\\({}^{+}\\)[78] & long short Attn [76] & 35.0 & 38.8 & 19.5 & 32.5 & 30.3 & 13.8 & 28.7 & 33.1 & 16.9 \\\\ \\hline Testra [92] & short Attn [76] & 25.1 & 30.8 & 14.1 & 24.3 & **24.5** & 10.7 & 17.4 & 23.0 & 10.9 \\\\ Testra & short Mamba [30] & **27.9** & **34.1** & **15.2** & **28.1** & 24.2 & **12.0** & **20.5** & **27.8** & **12.3** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: EK-100 [13]에 대한 행동 기대 결과. 표준 프로토콜에 따른 class-mean recall@5(%)로 측정한 정확도. \\ ({}^{+}\\)는 데이터 증강으로 학습된 모델을 나타낸다. "긴"과 "짧은"은 장단기 기억을 사용하는 것을 의미한다.\n' +
      '\n' +
      '\\mathbf{E}_{V}^{pos}+\\mathbf{E}_{V}^{type}, \\tag{1}\\]\\[\\tilde{\\mathbf{Q}} =\\mathbf{Q}+\\mathbf{E}_{Q}^{pos}+\\mathbf{E}_{Q}^{type}.\\\n' +
      '\n' +
      '그런 다음 텍스트와 비디오 토큰을 연결하여 조인트 입력\\(\\tilde{\\mathbf{Z}=[\\tilde{\\mathbf{V};\\tilde{\\mathbf{Q}}]\\in\\mathbb{R}^{L\\times D}\\), 여기서 \\(L=L_{v}+L_{q}\\)을 얻는다. 또한, 멀티모달 트랜스포머 인코더에 \\(\\mathbf{Z}\\)를 입력한다. 마지막으로, 텍스트가 강화된 비디오 특징들\\(\\tilde{\\mathbf{V}}^{e}\\)을 꺼내어 예측 헤드로 공급한다. 교차 모달 맘바 경쟁자를 생성하기 위해 양방향 맘바 블록을 적층하여 변압기 기준선을 대체하는 다중 모달 맘다 인코더를 구성한다.\n' +
      '\n' +
      '**결과 및 분석.** 표 6의 Qyhighlight [44]에 대한 다중 모델의 성능을 제시한다. Mamba는 평균 mAP 44.74를 달성하여 변압기(44.74 _vs._38.48)와 비교하여 상당한 개선을 나타낸다. 샤레이드-STA[28]의 경우, 맘바 기반 방법은 또한 유사한 성능을 달성한다. 이것은 맘바가 여러 양식을 효과적으로 통합할 수 있는 잠재력을 가지고 있음을 시사한다. Mamba[30]은 선형 스캐닝을 기반으로 하는 모델인 반면, 변환기는 전역 토큰 상호작용을 기반으로 한다는 점을 고려할 때, 직관적으로, 우리는 토큰 시퀀스에서 텍스트의 위치가 다중 모드 집합의 효율성에 영향을 미칠 수 있다고 믿는다. 이를 조사하기 위해 표 7에 서로 다른 텍스트-시각 융합 방법을 포함하는 반면 그림 3은 토큰의 네 가지 다른 배열을 보여준다. 우리는 시각적 특징의 왼쪽에서 텍스트 조건이 융합될 때 최상의 결과가 얻어짐을 관찰한다. Qy하이라이트[44]는 이 융합의 영향을 덜 받는 반면 샤레이드-STA[28]은 데이터 세트의 특성에 기인할 수 있는 텍스트 위치에 대한 특정 감도를 보여준다.\n' +
      '\n' +
      '**결과 및 분석.** TimeMamba와 TimeSformer [6]에 대한 성능 비교는 표 8, 표 9, 표 10 및 도 5에 나타내었다.\n' +
      '\n' +
      '비디오 시간 어댑터로서의 Mamba\n' +
      '\n' +
      '**작업 및 데이터 세트.** 시간 후 모델링에서 맘바의 성능을 평가하는 것 외에도 비디오-시간 어댑터로서의 효과도 평가한다. 자아 중심 데이터[29, 49]에 대해 비디오 텍스트 대조 학습을 수행하여 이중 타워 모델을 사전 훈련하는데, 이 모델에는 세밀한 내레이션이 포함된 4M 비디오 클립이 포함되어 있다. 평가를 위해 Epic-Kitchens-100 데이터셋 [13]에서 zero-shot/fine-tuned multi-instance 검색과 fine-tuned action 인식을, EgoSchema 데이터셋 [56]에서 zero-shot long-form Question Answering을 고려한다.\n' +
      '\n' +
      '**기준선 및 경쟁자.** TimeSformer[6]은 분할된 시공간 주의 블록을 채택하여 비디오 내의 공간적 및 시간적 관계를 별도로 모델링한다. TimeSformer에 이어, 우리는 개선된 분할된 시공간 상호작용을 위해 바닐라 시간 자기 주의를 대체하기 위한 시간 어댑터로 양방향 맘바 블록을 소개한다. TimeSformer의 공간 주의 레이어는 공정한 비교를 위해 변경되지 않고 그대로 유지됩니다.\n' +
      '\n' +
      '여기서는 ViM[96] 블록을 시간 모듈로 사용하고 결과 모델을 TimeMamba라고 한다. 일관성을 위해 변압기 기준선을 다시 구현하고 초기 값이 0인 탠 게이팅 메커니즘[37]을 추가하는 일관된 적응 방법을 사용한다. 이렇게 하면 새 모델의 출력이 원래 모델의 출력과 일치합니다. 특히, 표준 ViM 블록은 자기 주의보다 더 많은 매개변수(약 6.25C^{2}\\)를 갖는다.\n' +
      '\n' +
      '그림 4: 탐색된 구조의 그림입니다. (a) 및 (b)는 타임스포머 [6]에 대한 바닐라 스타일 [6] 및 냉동 스타일 [3] 잔류 연결 형태를 나타낸다. (c)와 (d)는 두 스타일 모두에서 ViM 블록을 시간 모듈로 사용하는 생성된 TimeMamba를 제시한다. (e) 시간적 ViM 블록을 시공간 ViM 블록으로 대체하는 것을 제공한다.\n' +
      '\n' +
      '블록(\\(4C^{2}\\)), 여기서 \\(C\\)은 특징 차원이다. 따라서 적절한 비교를 위해 ViM 블록의 확장비 E를 1로 설정하여 매개변수량을 \\(3.25C^{2}\\)로 줄였다. TimeSformer [6]에서 사용하는 바닐라 잔류 연결 형태 외에도 겨울왕국식 [3] 적응 방식을 추가적으로 탐색하였다. 우리는 그림 4에서 서로 다른 시공간 상호작용을 가진 블록을 나열한다. 우리는 AVION[93] 코드베이스를 사용하여 4프레임 입력으로 모델을 훈련하고 나머지 설정은 [94]와 [93]과 같이 변경되지 않는다. 모델은 이미지 텍스트 대비 학습을 통해 미리 학습된 CLIP-B16[61]로 초기화된다.\n' +
      '\n' +
      '다중 인스턴스 검색.__0-Zero-shot 다중 인스턴스 검색.___ 먼저 표 8의 시공간 상호작용 연산으로 서로 다른 모델을 평가하였으며, 재현된 Frozen 스타일 잔차 연결은 LaviLa[94]와 일관된 결과를 얻었다. 바닐라와 겨울왕국[3] 스타일을 비교할 때, 우리는 겨울왕국 스타일이 일관되게 더 나은 결과를 산출한다는 것을 관찰한다(ID4 vs ID5, ID6 vs ID7). 나아가, 동일한 적응 방식 하에서, ViM 기반 시간 모듈은 어텐션 기반 시간 모듈(ID4 _vs_ ID6, ID5 _vs_ ID7)을 일관되게 능가한다. 특히, 우리가 사용한 ViM 시간적 블록은 시간적 자기 주의 블록에 비해 더 적은 파라미터를 가지며, 이는 Mamba의 선택적 스캐닝의 예외적인 파라미터 이용 및 정보 추출 능력을 강조한다[30]. 또한, ViM의 시간적 모델링 능력을 넘어 시공간 ViM 블록을 검증한다. 시공간 ViM 블록은 전체 비디오 시퀀스에 걸쳐 시간적 ViM 블록을 조인트 시공간 모델링으로 대체한다. 놀랍게도 시공간 수준에서 전역 모델링을 도입했음에도 불구하고 시공간 ViM 블록이 실제로 성능 저하(ID7 _vs_\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c c c} \\hline \\hline \\multirow{2}{*}{ID} & \\multirow{2}{*}{Model} & \\multirow{2}{*}{Adaptation} & \\multicolumn{2}{c}{mAP} & \\multicolumn{3}{c}{nDCG} \\\\ \\cline{3-10}  & & & V2T & T2V & Avg & V2T & T2V & Avg \\\\ \\hline\n' +
      '1 & EgoVLP [49 & Frozen [3], Attn in Time & 19.4 & 13.9 & 16.6 & 24.1 & 22.0 & 23.1 \\\\\n' +
      '2 & EgoVLP[93 & Frozen[3], Attn in Time & 26.0 & 20.6 & 23.3 & 28.8 & 27.0 & 27.9 \\\\\n' +
      '3 & LaViLa [94] & Frozen [3], Attn in Time & - & - & 26.0 & - & - & 28.8 \\\\ \\hline\n' +
      '4 & TimeSformer & Vanilla[6], Attn in Time & 29.2 & 21.8 & 25.5 & 30.1 & 27.1 & 28.6\\\\\n' +
      '5 & TimeSformer & Frozen [3] Attn in Time & 29.8 & 22.2 & 26.0 & 30.6 & 27.5 & 29.0\\\\\n' +
      '6 & TimeMamba (ours) & Vanilla [6], Mamba in Time & 30.3 & 22.1 & 26.2 & 30.9 & 27.5 & 29.2 \\\\\n' +
      '7 & TimeMamba (ours) & Frozen [3], Mamba in Time & **30.7** & **22.8** & **26.8** & *31.3** & *27.8** & *29.5**\n' +
      '8 & TimeMamba (ours) & Frozen [3], Mamba in Space-Time & 30.1 & 21.9 & 26.0 & 30.7 & 27.1 & 28.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: EK100 [13]에 대한 제로샷 멀티 인스턴스 검색 결과. 우리는 서로 다른 모델을 분할된 시공간 상호작용과 비교한다. 추가 구성 "시공간에서의 맘바"는 추가 비교를 위해 사용된다. 우리는 훈련과 추론을 위해 4개의 프레임을 균일하게 샘플링한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multicolumn{4}{c}{Multi-instance Retrieval} & \\multicolumn{4}{c}{Action Recognition} \\\\ \\cline{2-10}  & & mAP & & nDCG & & Verb & Noun & Action \\\\ \\cline{2-10}  & V2T & T2V & Avg & V2T & T2V & Avg & Top1 & Top1 & Top5 \\\\ \\hline EgoVLP [49] & 49.9 & **40.5** & 45.0 & 60.9 & 57.9 & 59.4 & - & - & - & - \\\\ TimeSformer [6] & 49.1 & 39.3 & 44.2 & 60.0 & 57.6 & 58.8 & 63.8 & 52.4 & 41.3 & 60.4 \\\\ TimeMamba (ours) & **50.3** & 40.3 & **45.3** & **62.4** & **59.2** & **60.9** & **66.6** & **53.3** & **42.8** & **63.2** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: EK100 [13]에 대한 미세 조정된 다중 인스턴스 검색 및 액션 인식 결과. 우리는 훈련과 추론을 위해 16개의 프레임을 균일하게 샘플링한다.\n' +
      '\n' +
      'ID8). 우리는 스캐닝 기반 공간-시간이 미리 훈련된 공간 주의 블록에 의해 생성된 공간 특징 분포를 손상시킬 수 있다고 가정한다.\n' +
      '\n' +
      '_Fine-tuned multiinstance retrieval and action recognition.__Fine-tuned multiinstance retrieval and action recognition._ 우리는 다중 인스턴스 검색 및 동작 인식을 위해 Epic-Kitchen-100 [13] 데이터 세트에서 16개의 프레임으로 사전 훈련된 모델을 계속 미세 조정한다. 표 9에서 우리는 타임맘바가 타임폼러를 상당한 차이로 능가한다는 것을 관찰한다. 특히 TimeMamba는 동사 인식 맥락에서 TimeSformer를 2.8점 능가하여 시간적 모델링에 있어 그 효과를 입증하고 있다.\n' +
      '\n' +
      '영구 숏 롱폼 비디오 QA.__ 우리는 EgoSchema에 대한 모델의 긴 형태의 비디오 질문 응답 성능에 대한 추가 평가를 수행한다[56]. 표 10에 나타난 바와 같이, TimeSformer와 TimeMamba 모두 Ego4D에서 사전 훈련되었을 때[29], 대규모 사전 훈련된 모델들의 성능을 능가한다[85, 88]. 또한 ViM 블록의 긴 시간 모델링 능력의 효과를 탐색하기 위해 테스트 프레임의 수를 늘린다. 그림 5와 같이 두 모델 모두 4개의 프레임에서 사전 학습되었음에도 불구하고 타임맘바와 타임스포머의 성능은 프레임 증가에 따라 꾸준히 개선된다. 한편, 8192 프레임 사용 시 상당한 개선을 관찰할 수 있다. 입력 프레임이 32를 초과하는 경우, TimeMamba는 일반적으로 TimeSformer에 비해 더 많은 프레임들로부터 이득을 얻으며, 이는 시간적 자기-어텐션에 대한 시간적 ViM 블록의 우월성을 나타낸다.\n' +
      '\n' +
      '시공간 모델링을 위한### Mamba\n' +
      '\n' +
      '**작업 및 데이터 세트.** 마지막으로, 맘바의 공간적-시간적 모델링 능력을 평가한다. 이전 하위 섹션과 유사하게, 우리는 Epic-Kitchens-100 데이터 세트에서 제로 샷 다중 인스턴스 검색에서 모델의 성능을 평가한다[13].\n' +
      '\n' +
      '**기준 및 경쟁자.** ViViT[2] 및 TimeSformer[6]은 공간적 주의가 있는 ViT를 공간적-시간적 공동 주의가 있는 모델로 변환하는 것을 조사했다. 이러한 작업에 따라 ViM 모델[96]의 공간 선택적 스캐닝을 확장하여 시공간 선택적 스캐닝을 통합한다. 우리는 이 확장된 모델을 ViViM이라고 부른다. 초기화를 위해 ImageNet-1K[16]에서 사전 학습된 ViM 모델을 활용한다. ViM 모델은 편평화된 토큰 시퀀스의 중간에 삽입된 cls 토큰을 통합한다. ViM 모델을 ViViM으로 변환하기 위해 그림 6에 예시된 간단한 접근법을 채택한다. \\(M\\) 프레임으로 구성된 주어진 입력에 대해, 우리는 각 프레임에 해당하는 토큰 시퀀스의 중간에 cls 토큰을 삽입한다. 또한 각 프레임에 대해 0으로 초기화된 시간적 위치 임베딩을 추가한다. 평탄화된 비디오 시퀀스는 그 후 ViViM 모델에 입력된다. 모델의 출력은 각 프레임에서 cls 토큰의 평균을 계산하여 취한다.\n' +
      '\n' +
      '**결과 및 분석** 우리는 Zero-shot 다중 인스턴스 검색에 대한 ViViM의 결과를 추가로 분석한다. 표 11은 제로 샷 다중 인스턴스 검색에 대한 다양한 시공간 모델의 성능을 나타낸다. ImageNet-1K[16]에서 미리 훈련된 ViT와 ViViM을 비교할 때, 우리는 ViViM이 ViT보다 우수하다는 것을 관찰한다. 흥미롭게도, ImageNet-1K 상의 ViT-S[69]와 ViM-S[96] 사이의 성능 갭은 미미하지만(79.8 _vs._ 80.5),\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multirow{2}{*}{Param} & \\multirow{2}{*}{Pretrain} & \\multirow{2}{*}{\\#F} & \\multicolumn{3}{c}{mAP} & \\multicolumn{3}{c}{nDCG} \\\\ \\cline{5-10}  & & & & & V2T & & Avg & V2T & T2V & Avg \\\\ \\hline ViT-B & 86M & CLIP [61], WIT & 4 & 30.95 & 23.15 & 27.05 & 30.95 & 27.65 & 29.30 \\\\ \\hline ViT-T & 6M & DeiT [69], IN1K [16] & 4 & 15.50 & 11.10 & 13.30 & 22.48 & 19.66 & 21.07 \\\\ ViT-B & 86M & DeiT [69], IN1K [16] & 4 & 25.08 & 18.49 & 21.79 & 27.80 & 24.87 & 26.34 \\\\ ViT-T & 6M & DeiT [69], IN1K [16] & 16 & 20.47 & 15.29 & 17.88 & 25.74 & 22.89 & 24.31 \\\\ ViT-S & 22M & DeiT [69], IN1K [16] & 16 & 23.80 & 17.60 & 20.70 & 27.40 & 24.40 & 25.90 \\\\ \\hline ViViM-T (ours) & 7M & DeiT [69], IN1K [16] & 16 & 23.31 & 17.21 & 20.26 & 27.40 & 24.30 & 25.80 \\\\ ViViM-S (ours) & 26M & DeiT [69], IN1K [16] & 16 & **26.00** & **19.60** & **22.80** & **28.20** & **25.30** & **26.70** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: EK100 [13]에 대한 제로샷 멀티 인스턴스 검색 결과. 우리는 ViT와 시공간 관절 주의력을 비교하고 ViViM을 시공간 관절 선택 스캐닝과 비교한다. F는 훈련 및 추론을 위한 프레임 번호를 나타낸다.\n' +
      '\n' +
      '그림 6: 당사의 ViViM의 모델링 과정에 대한 삽화입니다. 시공간 선택적 스캐닝의 과정을 결정하는 점선 화살표를 통해 비디오 토큰을 평평하게 만드는 방향을 강조한다.\n' +
      '\n' +
      'ViViM-S는 제로 샷 다중 인스턴스 검색에서 ViT-S에 비해 상당한 개선(+2.1 mAP@Avg)을 보여준다. 이 발견은 ViViM이 긴 서열을 모델링하는 데 매우 효과적이며 성능이 향상됨을 시사한다.\n' +
      '\n' +
      '## 5 효율성 분석\n' +
      '\n' +
      '우리는 서로 다른 시공간 모델의 추론 속도를 비교한다. 이 테스트는 196개의 토큰을 공간 차원에서 고정하고 프레임 수를 지속적으로 증가시킨다. 모든 테스트는 단일 A100 GPU에서 반 정밀도로 수행된다. 공정한 비교를 위해, 모든 주의 블록은 플래시-어텐션[14, 15]을 장착한다.\n' +
      '\n' +
      '본 논문에서는 4 프레임부터 8192 프레임까지의 추론 속도를 실험하고, 실험 결과를 그림 8과 그림 8에 나열하였다. 두 표 모두 Mambo가 특히 프레임 수가 많을 때 트랜스포머 시리즈 모델에 비해 속도 이점을 제공할 수 있음을 보여준다. 그림 8에서 공정하고 포괄적인 비교를 위해 ViViM-T와 Flash-attention을 사용하거나 사용하지 않는 ViT를 비교한다[14, 15]. ViViM-T와 ViT+Flash-attention의 비교는 하드웨어 I/O 속도를 고려하여 최적화되었기 때문에 공정하다. 우리의 ViViM-T는 입력 프레임 번호가 256보다 클 때 플래시-어텐션이 있는 ViT-T보다 더 효율적이 된다. 플래시-어텐션이 없는 ViViM-T는 프레임 번호가 64보다 클 때 ViT를 능가하여 상대적으로 더 효율적이다. 그림 8의 TimeMambo-B의 경우, 입력이 8192 프레임 이상일 때, 효율은 타임폼러-B의 효율을 초과하기 시작한다. 토큰 상호 작용의 형태는 시간 상호 작용만 다르기 때문에 효율성 차이는 ViViM과 ViT의 비교만큼 크지 않다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '비디오 이해 도메인 내에서 맘보에 대한 포괄적인 평가는 전통적인 변압기에 대한 실행 가능한 대안으로서 가능성을 보여준다. 12개의 비디오 이해 작업에 걸쳐 14개의 모델/모듈로 구성된 비디오 맘보 스위트를 통해 복잡한 시공간 역학을 효율적으로 처리할 수 있는 맘바의 능력을 보여주며 우수한 성능과 유망한 효율성-성능 상충 관계를 모두 보여준다. 이러한 연구 결과는 Mamba의 비디오 분석 작업에 대한 적합성을 강조할 뿐만 아니라 컴퓨터 비전에서의 적용을 위한 새로운 길을 열어준다. 향후 작업은 맘바의 적응성을 더 탐색하고 보다 복잡하고 다중 모드 비디오 이해 문제로 그 유용성을 확장할 수 있다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      'ViM 블록.** 입력 시퀀스 \\(\\mathbf{x}\\in\\mathbb{R}^{N}times d}\\)이 주어지면, \\(N\\)은 시퀀스 길이를 나타내고 \\(d\\)은 특징 차원을 나타내며, ViM 블록은 처음에 차원을 \\(d\\cdot\\mathtt{E}\\)으로 확장시켜 두 개의 숨겨진 상태 시퀀스, 즉 \\(\\mathbf{x}_{s}\\)과 \\(\\mathbf{x}_{g}\\)을 생성한다. 첫 번째 수열인 \\(\\mathbf{x}_{s}\\)은 스캐닝을 위한 것이고, 두 번째 수열인 \\(\\mathbf{x}_{g}\\)은 게이팅에 사용된다. 선택적 주사층은 두 개의 뚜렷한 매개변수를 사용하여 양방향으로 \\(\\mathbf{x}_{s}\\)을 주사한다. 마지막으로, \\(\\mathbf{x}_{g}\\)을 사용하여 양방향 스캔된 피쳐를 게이트하고, 두 개의 게이트 피쳐의 평균을 통해 출력 피쳐를 얻는다.\n' +
      '\n' +
      'DBM 블록.** DBM 블록에서 입력 시퀀스 \\(\\mathbf{x}\\in\\mathbb{R}^{N\\times d}\\)이 주어지면, \\(N\\)은 시퀀스 길이를 나타내고 \\(d\\)은 특징 차원을 나타내며, 프로세스는 ViM 블록과 유사하다. 처음에 DBM 블록은 차원을 \\(d\\cdot\\mathtt{E}\\)으로 확장하여 \\(\\mathbf{x}_{s}\\)과 \\(\\mathbf{x}_{g}\\)의 두 가지 숨은 상태 시퀀스를 생성한다. 그러나, DBM 블록에서는 순방향 및 역방향 특징이 채널 차원을 따라 분리되어 4개의 숨겨진 상태 시퀀스(\\(\\mathbf{x}_{s}^{forward},\\mathbf{x}_{g}^{forward},\\mathbf{x}_{s}^{backward},\\mathbf{x}_{g}^{backward},\\mathbbb{R}^{N\\times d\\cdot\\frac{\\mathtt{E}}{2}})가 생성된다. 선택적 주사층은 공유 파라미터를 이용하여 \\(\\mathbf{x}_{s}^{forward}\\)와 \\(\\mathbf{x}_{g}^{forward}\\)을 주사한다. 마지막으로, 양방향으로 스캔된 두 특징을 각각 게이트하기 위해 \\(\\mathbf{x}_{g}^{forward}\\)와 \\(\\mathbf{x}_{g}^{backward}\\)을 사용한다. 출력 피쳐는 두 개의 게이티드 피쳐를 연결하고 투영함으로써 얻어진다.\n' +
      '\n' +
      '**Analysis.** \\(\\mathtt{E}=2\\)을 고려할 때, ViM 블록과 DBM 블록은 정적 및 동적 모델링을 위해 사용되는 파라미터의 수 및 스캐닝 컨텍스트 측면에서 차이를 보인다. 맘보[30]이 기준선 역할을 하는 경우, 표 A20은 ViM[96]과 DBM 사이의 매개변수, 컨텍스트 및 시간 비용의 불일치를 제시한다. ViM 블록에 비해 DBM 블록은 정적 방향 분리를 제공하고 동적 모델링을 위한 용량을 감소시켜 시간적 액션 로컬라이제이션과 같은 특정 다운스트림 데이터세트 및 작업과 더 호환된다. 추가적으로, 스캐닝 컨텍스트를 절반으로 줄이면 코어가 생성됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Method & Block & Acc & Edit & F1@10 & F1@25 & F1@50 \\\\ \\hline MS-TCN [21] & Dilated Conv, Enc & 80.7 & 67.9 & 76.3 & 74.0 & 64.5 \\\\ \\hline ASFormer [89] & Window Attn [12], Enc-Enc & 77.0 & 64.4 & 73.0 & 70.5 & 60.3 \\\\ ASFormer [89] & Window Attn [12], Enc-Dec & **85.6** & **79.6** & **85.1** & 73.4 & **76.0** \\\\ \\hline ASMamba & ViM [96], Enc & 85.4 & 77.7 & 84.5 & **83.4** & 74.7 \\\\ ASMamba & DBM (ours), Enc & 83.7 & 75.2 & 82.5 & 80.5 & 72.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 16: 50salads [66] 데이터세트에 대한 시간적 액션 분할의 결과. 상기 메트릭들은 정확도, 편집 거리[7], 및 다수의 tIoU 임계치{0.1, 0.25, 0.5} 하에서 인스턴스-방향 F1이다. \\ (\\dagger\\)는 공식 코드로 재현된 결과를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Method & Block & B-4 & METEOR & ROUGE-L & CIDER & SODA & Recall & Precision \\\\ \\hline PDVC [83] & DeformAttn [97] & 1.75 & 6.73 & 14.65 & 26.07 & **5.47** & 51.7 & 56.1 \\\\ PDVC [83] & ViM [96] & 1.68 & 6.92 & 14.72 & 26.26 & 5.33 & **53.1** & **56.3** \\\\ PDVC & DBM (ours) & **1.76** & **7.16** & **14.83** & **26.77** & 5.27 & 52.4 & **56.3** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 17: ActivityNet 상에서 조밀한 비디오 캡셔닝의 결과[36]. 메트릭은 BLEU-4[58], METEOR[4], CIDEr[77] SODA_c[27], Recall and Precision을 포함한다.\n' +
      '\n' +
      '바닐라 맘바[30] 블록과 일치하는 (훈련 및 추론) 시간 비용의 상응하는 절반이다.\n' +
      '\n' +
      '## 부록 0. 하이퍼파라미터 민감도\n' +
      '\n' +
      '또한, 우리는 맘바 시리즈 모델의 하이퍼파라미터 민감도에 대한 분석을 수행했다. 대부분의 실험에서 훈련 하이퍼파라미터는 둔감한 것으로 나타났다. 대부분의 작업을 위해 변압기 블록을 맘바 기반 블록으로 교체했습니다. 그러나, 비디오 시간 접지의 경우, 더 큰 학습률이 더 나은 최적화 결과를 산출하는 것을 관찰했다. 또한 비디오-텍스트 정렬 손실에 대한 손실 가중치를 증가시키면 모델의 수렴이 용이해졌다. 이러한 조정이 특히 mutli-modal 집계에 대한 스캐닝 메커니즘과 글로벌 상호 작용의 구별과 관련이 있다고 가정한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Ali, A., Touvron, H., Caron, M., Bojanowski, P., Douze, M., Joulin, A., Laptev, I., Neverova, N., Synnaeve, G., Verbeek, J., Jegou, H.: Xcit: Cross-covariance image transformers. pp. 20014-20027 (2021)\n' +
      '* [2] Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lucic, M., Schmid, C.: Vivit: A video vision transformer. In: ICCV. pp. 6816-6826 (2021)\n' +
      '* [3] Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video and image encoder for end-to-end retrieval. In: ICCV. pp. 1708-1718 (2021)\n' +
      '* [4] Banerjee, S., Lavie, A.: Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In: Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. pp. 65-72 (2005)\n' +
      '* [5] Bao, H., Dong, L., Piao, S., Wei, F.: Beit: BERT pre-training of image transformers. In: ICLR (2022)\n' +
      '* [6] Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for video understanding? In: ICML. vol. 139, pp. 813-824 (2021)\n' +
      '* [7] Brill, E., Moore, R.C.: An improved error model for noisy channel spelling correction. In: 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong, China, October 1-8, 2000. pp. 286-293. ACL (2000)\n' +
      '* [8] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners (2020)\n' +
      '* [9] Carreira, J., Zisserman, A.: Quo vadis, action recognition? A new model and the kinetics dataset. In: CVPR. pp. 4724-4733 (2017)\n' +
      '* [10] Chen, G., Zheng, Y.D., Wang, J., Xu, J., Huang, Y., Pan, J., Wang, Y., Wang, Y., Qiao, Y., Lu, T., et al.: Videollm: Modeling video sequence with large language models. arXiv preprint arXiv:2305.13292 (2023)\n' +
      '* [11] Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Muyan, Z., Zhang, Q., Zhu, X., Lu, L., et al.: Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238 (2023)\n' +
      '* [12] Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al.: Rethinking attention with performers. arXiv preprint arXiv:2009.14794 (2020)\n' +
      '* [13] Damen, D., Doughty, H., Farinella, G.M., Furnari, A., Kazakos, E., Ma, J., Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: Rescaling egocentric vision: Collection, pipeline and challenges for EPIC-KITCHENS-100. IJCV **130**(1), 33-55 (2022)\n' +
      '* [14] Dao, T.: FlashAttention-2: Faster attention with better parallelism and work partitioning. In: ICLR (2023)\n' +
      '* [15] Dao, T., Fu, D.Y., Ermon, S., Rudra, A., Re, C.: FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In: NeurIPS (2022)\n' +
      '* [16] Deng, J., Dong, W., Socher, R., Li, L., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: CVPR. pp. 248-255 (2009)\n' +
      '* [17] Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirectional transformers for language understanding. In: NAACL. pp. 4171-4186 (2019)* [18] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021) [1]\n' +
      '* [19] Duan, Y., Wang, W., Chen, Z., Zhu, X., Lu, L., Lu, T., Qiao, Y., Li, H., Dai, J., Wang, W.: Vision-rwkv: Efficient and scalable visual perception with rwkv-like architectures. arXiv preprint arXiv:TODO (2024)\n' +
      '* [20] Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., Feichtenhofer, C.: Multiscale vision transformers. In: ICCV. pp. 6804-6815 (2021)\n' +
      '* [21] Farha, Y.A., Gall, J.: Ms-tcn: Multi-stage temporal convolutional network for action segmentation. In: CVPR. pp. 3575-3584 (2019)\n' +
      '* [22] Fathi, A., Ren, X., Rehg, J.M.: Learning to recognize objects in egocentric activities. In: CVPR. pp. 3281-3288 (2011)\n' +
      '* [23] Feichtenhofer, C.: X3d: Expanding architectures for efficient video recognition. In: CVPR. pp. 203-213 (2020)\n' +
      '* [24] Feichtenhofer, C., Fan, H., Li, Y., He, K.: Masked autoencoders as spatiotemporal learners (2022)\n' +
      '* [25] Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recognition. In: ICCV. pp. 6201-6210 (2019)\n' +
      '* [26] Fu, D.Y., Dao, T., Saab, K.K., Thomas, A.W., Rudra, A., Re, C.: Hungry hungry hungry hungry hungry: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052 (2022)\n' +
      '* [27] Fujita, S., Hirao, T., Kamigaito, H., Okumura, M., Nagata, M.: Soda: Story oriented dense video captioning evaluation framework. In: ECCV. pp. 517-531 (2020)\n' +
      '* [28] Gao, J., Sun, C., Yang, Z., Nevatia, R.: Tall: Temporal activity localization via language query. In: ICCV. pp. 5267-5275 (2017)\n' +
      '* [29] Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., et al.: Ego4d: Around the world in 3,000 hours of egocentric video. In: CVPR. pp. 18995-19012 (2022)\n' +
      '* [30] Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023)\n' +
      '* [31] Gu, A., Goel, K., Gupta, A., Re, C.: On the parameterization and initialization of diagonal state space models. NeurIPS **35**, 35971-35983 (2022)\n' +
      '* [32] Gu, A., Goel, K., Re, C.: Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396 (2021)\n' +
      '* [33] Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., Re, C.: Combining recurrent, convolutional, and continuous-time models with linear state space layers. NeurIPS **34**, 572-585 (2021)\n' +
      '* [34] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., Girshick, R.B.: Masked autoencoders are scalable vision learners. In: CVPR. pp. 15979-15988 (2022)\n' +
      '* [35] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. pp. 770-778 (2016)\n' +
      '* [36] Heilbron, F.C., Escorcia, V., Ghanem, B., Niebles, J.C.: Activitynet: A large-scale video benchmark for human activity understanding. In: CVPR. pp. 961-970 (2015)\n' +
      '* [37] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. **9**(8), 1735-1780 (1997)\n' +
      '* [38] Hua, W., Dai, Z., Liu, H., Le, Q.V.: Transformer quality in linear time. vol. 162, pp. 9099-9117 (2022)* [39] Idrees, H., Zamir, A.R., Jiang, Y., Gorban, A., Laptev, I., Sukthankar, R., Shah, M.: The THUMOS challenge on action recognition for videos "in the wild" **155**, 1-23 (2017) 16\n' +
      '* [40] Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. vol. 37, pp. 448-456 (2015) 3\n' +
      '* [41] Islam, M.M., Bertasius, G.: Long movie clip classification with state-space video models. In: ECCV. pp. 87-104 (2022) 4\n' +
      '* [42] Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., Zisserman, A.: The kinetics human action video dataset. CoRR **abs/1705.06950** (2017) 17\n' +
      '* [43] Kuehne, H., Arslan, A., Serre, T.: The language of actions: Recovering the syntax and semantics of goal-directed human activities. In: CVPR. pp. 780-787 (2014) 17\n' +
      '* [44] Lei, J., Berg, T.L., Bansal, M.: Detecting moments and highlights in videos via natural language queries. pp. 11846-11858 (2021) 8\n' +
      '* [45] Li, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning. IEEE TPAMI (2023) 3\n' +
      '* [46] Li, K., Wang, Y., Li, Y., Wang, Y., He, Y., Wang, L., Qiao, Y.: Unmasked teacher: Towards training-efficient video foundation models. In: ICCV. pp. 19948-19960 (2023) 3\n' +
      '* [47] Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. In: Text summarization branches out. pp. 74-81 (2004) 7\n' +
      '* [48] Lin, J., Gan, C., Han, S.: TSM: temporal shift module for efficient video understanding. In: ICCV. pp. 7082-7092 (2019) 3\n' +
      '* [49] Lin, K.Q., Wang, J., Soldan, M., Wray, M., Yan, R., Xu, E.Z., Gao, D., Tu, R., Zhao, W., Kong, W., Cai, C., Wang, H., Damen, D., Ghanem, B., Liu, W., Shou, M.Z.: Egocentric video-language pretraining (2022) 10\n' +
      '* [50] Lin, K.Q., Zhang, P., Chen, J., Pramanick, S., Gao, D., Wang, A.J., Yan, R., Shou, M.Z.: Univtg: Towards unified video-language temporal grounding. In: ICCV. pp. 2794-2804 (2023) 8\n' +
      '* [51] Liu, Y., Li, S., Wu, Y., Chen, C.W., Shan, Y., Qie, X.: Umt: Unified multi-modal transformers for joint video moment retrieval and highlight detection. In: CVPR. pp. 3042-3051 (2022) 9\n' +
      '* [52] Liu, Y., Wang, L., Wang, Y., Ma, X., Qiao, Y.: Fineaction: A fine-grained video dataset for temporal action localization. IEEE TIP **31**, 6937-6950 (2022) 16\n' +
      '* [53] Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 (2024) 2\n' +
      '* [54] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV. pp. 9992-10002 (2021) 3\n' +
      '* [55] Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H.: Video swin transformer. In: CVPR. pp. 3192-3201 (2022) 2\n' +
      '* [56] Mangalam, K., Akshulakov, R., Malik, J.: Egoschema: A diagnostic benchmark for very long-form video language understanding. NeurIPS **36** (2024) 10\n' +
      '* [57] Mehta, H., Gupta, A., Cutkosky, A., Neyshabur, B.: Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947 (2022) 4\n' +
      '* [58] Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine translation. In: ACL. pp. 311-318 (2002) 7\n' +
      '* [59] Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K.K., et al.: Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048 (2023)* [60] Qiu, Z., Yao, T., Mei, T.: Learning spatio-temporal representation with pseudo-3d residual networks. In: ICCV. pp. 5534-5542 (2017)\n' +
      '* [61] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. vol. 139, pp. 8748-8763 (2021)\n' +
      '* [62] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog **1**(8), 9 (2019)\n' +
      '* [63] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. **21**, 140:1-140:67 (2020)\n' +
      '* [64] Sharir, G., Noy, A., Zelnik-Manor, L.: An image is worth 16x16 words, what is a video worth? arXiv preprint arXiv:2103.13915 (2021)\n' +
      '* [65] Smith, J.T., Warrington, A., Linderman, S.W.: Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933 (2022)\n' +
      '* [66] Stein, S., McKenna, S.J.: Combining embedded accelerometers with computer vision for recognizing food preparation activities. pp. 729-738 (2013)\n' +
      '* [67] Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., Wei, F.: Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621 (2023)\n' +
      '* [68] Tong, Z., Song, Y., Wang, J., Wang, L.: Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training (2022)\n' +
      '* [69] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training data-efficient image transformers & distillation through attention. vol. 139, pp. 10347-10357 (2021)\n' +
      '* [70] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)\n' +
      '* [71] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)\n' +
      '* [72] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)\n' +
      '* [73] Tran, D., Ray, J., Shou, Z., Chang, S., Paluri, M.: Convnet architecture search for spatiotemporal feature learning. CoRR **abs/1708.05038** (2017)\n' +
      '* [74] Tran, D., Wang, H., Feiszli, M., Torresani, L.: Video classification with channel-separated convolutional networks. In: ICCV. pp. 5551-5560 (2019)\n' +
      '* [75] Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y., Paluri, M.: A closer look at spatiotemporal convolutions for action recognition. In: CVPR. pp. 6450-6459 (2018)\n' +
      '* [76] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: NeurIPS. pp. 5998-6008 (2017)\n' +
      '* [77] Vedantam, R., Lawrence Zitnick, C., Parikh, D.: Cider: Consensus-based image description evaluation. In: CVPR. pp. 4566-4575 (2015)\n' +
      '* [78] Wang, J., Chen, G., Huang, Y., Wang, L., Lu, T.: Memory-and-anticipation transformer for online action understanding. In: ICCV. pp. 13824-13835 (2023)\n' +
      '* [79] Wang, J., Zhu, W., Wang, P., Yu, X., Liu, L., Omar, M., Hamid, R.: Selective structured state-spaces for long-form video understanding. In: CVPR. pp. 6387-6397 (2023)* [80] Wang, L., Huang, B., Zhao, Z., Tong, Z., He, Y., Wang, Y., Wang, Y., Qiao, Y.: VideoMAE V2: scaling video masked autoencoders with dual masking. In: CVPR. pp. 14549-14560 (2023)\n' +
      '* [81] Wang, L., Tong, Z., Ji, B., Wu, G.: TDN: temporal difference networks for efficient action recognition. In: CVPR. pp. 1895-1904 (2021)\n' +
      '* [82] Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Gool, L.V.: Temporal segment networks for action recognition in videos. IEEE TPAMI **41**(11), 2740-2755 (2019)\n' +
      '* [83] Wang, T., Zhang, R., Lu, Z., Zheng, F., Cheng, R., Luo, P.: End-to-end dense video captioning with parallel decoding. In: ICCV. pp. 6847-6857 (2021)\n' +
      '* [84] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: ICCV. pp. 568-578 (2021)\n' +
      '* [85] Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu, Y., Wang, Z., et al.: Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191 (2022)\n' +
      '* [86] Xie, S., Girshick, R.B., Dollar, P., Tu, Z., He, K.: Aggregated residual transformations for deep neural networks. In: CVPR. pp. 5987-5995 (2017)\n' +
      '* [87] Xie, S., Sun, C., Huang, J., Tu, Z., Murphy, K.: Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In: ECCV. vol. 11219, pp. 318-335 (2018)\n' +
      '* [88] Yang, A., Miech, A., Sivic, J., Laptev, I., Schmid, C.: Zero-shot video question answering via frozen bidirectional language models. NeurIPS **35**, 124-141 (2022)\n' +
      '* [89] Yi, F., Wen, H., Jiang, T.: Asformer: Transformer for action segmentation. In: BMVC. p. 236 (2021)\n' +
      '* [90] Zhang, C., Wu, J., Li, Y.: Actionformer: Localizing moments of actions with transformers. In: Avidan, S., Brostow, G.J., Cisse, M., Farinella, G.M., Hassner, T. (eds.) ECCV. vol. 13664, pp. 492-510 (2022)\n' +
      '* [91] Zhao, H., Torralba, A., Torresani, L., Yan, Z.: HACS: human action clips and segments dataset for recognition and temporal localization. In: ICCV. pp. 8667-8677 (2019)\n' +
      '* [92] Zhao, Y., Krahenbuhl, P.: Real-time online video detection with temporal smoothing transformers. In: ECCV. pp. 485-502 (2022)\n' +
      '* [93] Zhao, Y., Krahenbuhl, P.: Training a large video model on a single machine in a day. arXiv preprint arXiv:2309.16669 (2023)\n' +
      '* [94] Zhao, Y., Misra, I., Krahenbuhl, P., Girdhar, R.: Learning video representations from large language models. In: CVPR. pp. 6586-6597 (2023)\n' +
      '* [95] Zhou, L., Xu, C., Corso, J.: Towards automatic learning of procedures from web instructional videos. In: AAAI. vol. 32 (2018)\n' +
      '* [96] Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417 (2024)\n' +
      '* [97] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable DETR: deformable transformers for end-to-end object detection. In: ICLR (2021)\n' +
      '* [\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>