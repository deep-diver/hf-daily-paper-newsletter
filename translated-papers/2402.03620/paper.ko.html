<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '이는 self-consistency(Wang et al., 2022)와 같은 추론-헤비 앙상블 접근법들보다 더 수행적이면서도, _task-level_ 상에서 3개의 추론 단계들만을 더 필요로 하기 때문에 계산된다. 마지막으로, 발견된 추론 구조는 태스크에 내재적이며, 최적화된 프롬프트(Zhou et al., 2022; Yang et al., 2023)보다 더 _interpretable_ 방식으로 태스크에 대한 LLMs의 통찰력을 전달한다.\n' +
      '\n' +
      '우리는 Big Bench-Hard (BBH) (Suzgun et al., 2022), Thinking for Doing (T4D) (Zhou et al., 2023), MATH (Hendrycks et al., 2021) 등 25개의 도전적 추론 과제들에 대해 Self-Discover를 테스트한다. 자기 발견은 최대 42%의 성능 향상으로 21/25 태스크에서 CoT를 능가하며(그림 1), 하나의 선험적 CoT 모듈에 대해 원자 추론 모듈에서 구성된 자기 발견 추론 구조의 이점을 강조한다. 또한, Self-Discover는 10-40배 적은 추론 계산량을 요구하면서도 CoT + Self-Consistency 및 모든 모듈의 다수결 투표와 같은 추론-헤비 방식에 비해 우수한 성능을 달성함을 보인다(그림 5). 마지막으로, 훈련 세트(Yang et al., 2023)(도 9)를 사용하여 Self-Discover와 프롬프트 최적화(OPRO)를 비교한다. 우리는 자기 발견 추론 구조가 훨씬 더 해석 가능한 반면, 자기 발견이 여전히 OPRO보다 동등하거나 더 잘 수행된다는 것을 발견했다.\n' +
      '\n' +
      '자기 발견의 효과를 이해하기 위해 일련의 분석을 수행한다. BBH 태스크를 4개의 다른 카테고리로 분해함으로써, 우리는 Self-Discover가 세계 지식이 필요한 태스크에서 가장 잘 수행하고 CoT(그림 4)에 비해 알고리즘 태스크에서 중간 정도의 성능 향상을 갖는다는 것을 발견한다. 이것은 74.7%의 모델 고장이 계산 오류(예: 수학)에서 비롯되는 MATH에 대한 오류 분석을 통해 추가로 확인된다. 또한 자기 발견 추론 구조를 자세히 살펴보고 PaLM 2-L에서 GPT-4로, GPT-4에서 Llama-2-70B로 전이성 연구를 통해 이들의 보편성을 보여준다. 우리는 LLM을 사용하여 어려운 문제를 해결하기 위한 구조화된 추론에 대한 더 많은 미래의 작업을 장려하기를 바란다.\n' +
      '\n' +
      '##2 문제 해결을 위한 자기 발견 추론 구조\n' +
      '\n' +
      '우리는 인간이 문제를 해결하기 위한 추론 프로그램을 고안하기 위해 사전 지식과 기술을 어떻게 사용하는지에 영감을 얻는다(Newell et al., 1958; Rasmussen, 1983). 우리가 새로운 문제에 직면했을 때, 우리는 종종 내부적으로 사전 경험에서 어떤 지식과 기술이 그것을 해결하는 데 도움이 될 수 있는지 검색합니다. 그런 다음 관련 지식과 기술을 이 작업에 적용하려고 시도합니다. 그리고 마지막으로 우리는 문제를 해결하기 위해 여러 개인의 기술과 지식을 연결할 것입니다. 우리는 이러한 단계를 그림 2와 같이 두 단계로 제정하기 위해 Self-Discover를 설계한다.\n' +
      '\n' +
      '"_Use critical thinking_", "_Let\'s step by step_"와 같은 높은 수준의 문제 해결 휴리스틱을 나타내는 과제 및 추론 모듈 기술 세트가 주어지면, Self-Discover의 Stage 1은 메타추론을 통해 이 과제를 해결하기 위한 내재적 추론 구조를 밝히는 것을 목표로 한다. 구체적으로, 우리는 3개의 메타 프롬프트를 사용하여 LLM이 레이블이나 훈련이 필요하지 않은 실행 가능한 추론 구조를 선택, 적응 및 구현하도록 안내한다. JSON의 추론 및 생성 품질(Zhou et al., 2023; OpenAI, 2023a)에 대한 해석 가능성 및 발견으로 인해 JSON과 유사한 키-값 쌍으로 구조를 포맷한다. 의 구조\n' +
      '\n' +
      '그림 1: Self-Discover는 LLM들이 도전적인 과제를 해결하기 위해 원자 추론 모듈을 스스로 발견하고 추론 구조로 구성하도록 안내한다. 본 논문에서는 빅벤치하드(Big Bench-Hard, BBH), 에이전트 추론(Agent reasoning, T4D), MATH를 포함한 도전적 추론 벤치마크에 대한 테스트를 통해, PaLM 2-L을 이용한 제로샷 설정에서 Self-Discover가 23/25에서 Direct Answering, 21/25 작업에서 CoT보다 우수함을 알 수 있었다. 전체 BBH 결과는 부록 C 표 3에 나와 있다.\n' +
      '\n' +
      '메타 프롬프트와 전체 프롬프트는 부록에 나와 있습니다. 1단계는 _task-level_로 작동하는데, 이는 각 작업에 대해 Self-Discover를 한 번만 실행하면 된다는 것을 의미한다. 그런 다음, 단계 2에서는 발견된 추론 구조를 이용하여 주어진 태스크의 모든 _instance_를 모델들에게 각 키를 채워서 주어진 구조를 따르도록 지시하고 최종 해답에 도달하도록 함으로써 간단히 해결할 수 있다.\n' +
      '\n' +
      '### Stage 1: Self-Discover Task-Specific Structure\n' +
      '\n' +
      '첫 번째 단계는 1) 과제 해결을 위한 관련 추론 모듈이 추론 모듈 설명 집합에서 선택되는 SELECT, 2) 선택된 추론 모듈에 대한 설명이 당면한 과제에 보다 구체적이도록 재표현되는 ADAPT, 3) 적응된 추론 설명이 구조를 따라 과제를 해결할 수 있도록 구조화된 실행 가능한 계획으로 구현되는 IMPLEMENT의 세 가지 단계로 구성된다.\n' +
      '\n' +
      'SelectFirst, 모든 추론 모듈이 모든 작업에 도움이 되는 것은 아니므로 Self-Discover의 첫 번째 단계는 작업 예제를 기반으로 유용한 모듈을 선택하기 위해 모델을 안내한다. 예를 들어, "_반사적 사고_"는 과학 문제에 대한 첫 번째 원리 이론을 검색하는 데 도움이 될 수 있는 반면, "_창의적 사고_"는 이야기에 대한 새로운 연속을 생성하는 데 도움이 된다. "_critical thinking_", "_break the problem to sub-problems_"(부록 A에서 전체 집합)과 레이블이 없는 몇 가지 과제 예시가 주어졌을 때, Self-Discover는 먼저 모델 \\(\\mathcal{M}\\)과 메타프롬프트 \\(p_{S}\\)을 사용하여 과제를 해결하는 데 유용한 추론 모듈 \\(D_{S}\\)의 하위 집합을 선택한다:\n' +
      '\n' +
      '\\[D_{S}=\\mathcal{M}(p_{S}\\parallel D\\parallel t_{i}). \\tag{1}\\\n' +
      '\n' +
      '각 추론 모듈은 문제를 해결하는 방법에 대한 일반적인 설명을 제공하기 때문에, Self-Discover의 다음 단계는 _tailoring_ 각 선택된 모듈을 당면한 태스크에 맞추는 것을 목표로 한다. 예를 들어, "_break the problem to sub-problems_"에서 "_calculate each arithmetic operation in order_"로 연산 문제들에 대해 나타낼 수 있다. 이전 단계로부터 선택된 추론 모듈 서브세트 \\(D_{S}\\)이 주어지면, ADAPT는 태스크에 더 구체적이 되도록 선택된 모듈의 각 _erase를 재지정한다. SELECT와 유사하게, 이 단계는 메타-프롬프트\\(p_{A}\\) 및 생성 모델\\(\\mathcal{M}\\)을 사용하여 적응된 추론 모듈 설명\\(D_{A}\\)을 생성한다:\n' +
      '\n' +
      '\\[D_{A}=\\mathcal{M}(p_{A}\\parallel D_{S}\\parallel t_{i}). \\tag{2}\\\n' +
      '\n' +
      '마지막으로, 적응된 추론 모듈 설명 \\(D_{A}\\)이 주어졌을 때, Self-Discover는 각 단계에 대해 무엇을 생성할지에 대한 지정된 명령으로 추론 모듈을 구현된 _reasoning structure_\\(D_{I}\\)으로 연산한다. 또한, IMPLEMENT는 메타 프롬프트\\(p_{I}\\) 이외에, 자연 언어 기술들을 추론 구조로 더 잘 변환하기 위해 다른 태스크에 대한 인간-기입 추론 구조\\(S_{Human}\\)의 실증을 제공한다:\n' +
      '\n' +
      '[D_{I}=\\mathcal{M}(p_{A}\\parallel S_{human}\\parallel D_{A}\\parallel t_{i}. \\tag{3}\\\\mathcal{M}(p_{A}\\parallel S_{human}\\parallel D_{A}\\parallel t_{i})\n' +
      '\n' +
      '### 2단계: 탐색된 구조를 이용한 태클 작업\n' +
      '\n' +
      '세 단계 후에, 우리는 우리가 풀어야 할 작업에 고유하게 적응된 구현 추론 구조 \\(D_{I}\\)를 갖는다. 그런 다음 태스크의 모든 인스턴스에 추론 구조를 추가하고 추론 구조를 따르도록 프롬프트 모델을 추가하여 답변을 생성할 수 있다.\\(A\\):\n' +
      '\n' +
      '\\[A=\\mathcal{M}(D_{S}\\parallel t),\\forall t\\in T.\\tag{4}\\]\n' +
      '\n' +
      '프롬프트에 대한 자세한 내용은 부록 A에 포함되어 있다.\n' +
      '\n' +
      '## 3 실험 설정\n' +
      '\n' +
      '### Tasks\n' +
      '\n' +
      '우리는 여전히 LLM에 도전하는 다양한 추론 벤치마크에 초점을 맞추고 있다: BIG-Bench Hard (BBH) (Suzgun)\n' +
      '\n' +
      '그림 2: **문제 해결을 위한 Self-Discover 사용설명**. 생성 LM, 태스크 및 시드 추론 모듈 설명이 주어지면, 우리는 태스크를 해결하기 위해 LMs가 _key-value_ 형식으로 추론 구조를 생성하도록 안내한다. 마지막으로, 모델들은 JSON의 값을 단계적으로 기입함으로써 태스크로부터 모든 인스턴스를 해결하기 위해 자기 발견 구조를 따를 수 있다.\n' +
      '\n' +
      'BBH 태스크는 BIG-Bench(Srivastava et al., 2023)에서 엄선된 23개의 도전 과제를 포함하고 있다. BBH 태스크는 저자에 따라 다음과 같은 4가지 범주에 걸쳐 다양한 추론 문제를 포함한다: 1) 알고리즘 및 다단계 산술 추론, 2) 자연 언어 이해, 3) 세계 지식의 사용, 4) 다국어 지식과 추론 또한 모델들이 수행할 행동을 결정하기 위해 정신 상태 추론을 활용해야 하는 근거 소셜 에이전트 추론 태스크인 Thinking for Doing(T4D)에 대해 테스트한다(Zhou et al., 2023). 여기서 CoT를 사용한 GPT-4는 약 50%에 불과하다. 마지막으로 MATH(Hendrycks et al., 2021) 테스트 세트로부터 200개의 예를 서브샘플링하고, MATH 태스크의 복잡성에 적응하기 위해 원샷 데모를 통해 인스턴스 레벨 추론 구조를 생성한다. 평가를 위해 정확도를 사용하여 BBH, T4D 및 MATH에 대한 모델 성능을 측정한다(자세한 내용은 부록 B에서 찾을 수 있다).\n' +
      '\n' +
      '### Models\n' +
      '\n' +
      '우리는 GPT-4(gpt-4-turbo-preview)(OpenAI, 2023b), GPT-3.5-turbo(ChatGPT)(OpenAI, 2022)1, 명령어 조정된 PaLM 2-L(Anil et al., 2023)2, 및 오픈 소스 LLM Llama2-70B(Touvron et al., 2023)와 같은 여러 최신 LLM을 사용한다.\n' +
      '\n' +
      '각주 1: 2023년 10월-12월 접속\n' +
      '\n' +
      '각주 2: MATH의 경우, 우리는 더 복잡한 추론 구조의 더 나은 지시를 따를 수 있도록 더 강한 지시 튜닝을 갖는 PaLM 2-L 모델을 사용한다.\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '우리는 LLM 추론을 위한 다른 제로샷 프롬프트 방법과 Self-Discover를 비교한다.\n' +
      '\n' +
      '* **Direct Prompting**, 여기서 모델은 중간 추론 단계 없이 직접 답변을 생성한다.\n' +
      '**CoT**(Wei et al., 2022; Kojima et al., 2022) 여기서 모델들은 최종 답변으로 이어지는 추론 프로세스를 생성하도록 프롬프트된다.\n' +
      '* **Plan-and-Solve**(Wang et al., 2023)에서, 모델들은 먼저 계획을 생성한 다음 문제를 해결하도록 프롬프트된다. 자기 발견은 원자 추론 모듈에서 추론 구조를 접지하고, 명시적인 키-값 추론 구조를 따르도록 디코딩을 촉구함으로써 다르다.\n' +
      '\n' +
      '다음으로, Self-Discover에 전달하는 원시 씨앗 추론 모듈(RM)을 사용하는 다른 기준도 고려한다. 본 논문에서는 태스크의 하위 집합에 대한 추론 호출 효율과 다음과 같은 방법의 성능을 비교한다.\n' +
      '\n' +
      '* **CoT-Self-Consistency**(Wang et al., 2022), 우리는 LLM에서 CoT로 여러 출력을 샘플링하고 최종 답변을 얻기 위해 답변을 집계한다. 이 방법은 반복적인 질의의 비용으로 인해 태스크의 하위 집합에 대해 비교된다.\n' +
      '각 RM의 ** 다수결 투표**: 각 RM을 추가하여 작업을 해결하고 모든 답변의 다수결 투표를 사용하여 최종 답변을 얻도록 모델을 프롬프트한다. 본 논문에서는 논리적인 추론 구조에 _multiple_ RM들을 통합하는 것이 태스크를 해결하기 위해 각 RM들을 적용하고 사후적으로 다수결 투표를 사용하여 앙상블하는 데 유리하며, 추론 계산 비용이 훨씬 더 많이 드는지를 살펴본다.\n' +
      '* **각 RM**의 최선: 이 방법은 오라클 레이블에 액세스할 수 있다고 가정하고 가장 높은 정확도를 사용합니다.\n' +
      '\n' +
      '그림 3: **Self-Discover의 세 가지 행동에 대한 그림. 우리는 LMs를 사용하여 관련 모듈을 선택하고 작업별 설명에 적응하여 일관된 추론 구조를 구성하고 JSON에서 추론 구조를 구현한다.**\n' +
      '\n' +
      '각 RM을 적용하는 중입니다. 이를 비교하여 자기 발견이 새로운 작업에 어떤 RM을 사용할지에 대한 완벽한 사전 지식에 의존하는 방법과 경쟁하는지 여부를 조사한다.\n' +
      '\n' +
      '또한, 추론 구조의 보편성에 대한 분석을 위해, 프롬프트: LLMs를 최적화기(**OPRO**)로 개선하기 위해 _training_set을 요구하는 프롬프트 최적화 방법과 비교한다(Yang et al., 2023). 우리는 한 모델에서 최적화된 구조나 프롬프트를 적용할 때 추론 구조가 프롬프트의 표현보다 더 많은 성능 향상을 유지할 수 있음을 보여 주는 것을 목표로 한다.\n' +
      '\n' +
      '## 4 Results\n' +
      '\n' +
      '우리는 실험 결과를 통해 다음과 같은 질문에 답한다: 1)_추론 구조를 발견하면 LLM 추론 능력이 향상됩니까?_ (4.1) 2)_Self-Discover가 어떤 종류의 문제를 가장 잘 수행하는가?_ (4.2) 및 3) _Can Self-Discover boost LLM performance efficiently?_ (4.3) 마지막으로 자기발견된 구조, 구조에 따른 LLM 출력의 정성적인 예를 보여주고, 추론하기 위한 다른 프롬프트 방법에 따른 LLM 출력과 비교할 것이다(4.4).\n' +
      '\n' +
      '자기 발견이 LLM 추론을 향상시키는가?\n' +
      '\n' +
      '**전반적으로 Self-Discover는 다양한 추론 태스크 집합에 걸쳐 PaLM 2-L과 GPT-4의 추론을 향상시킨다.** 표 1은 PaLM 2-L과 GPT-4를 사용하여 BBH, T4D 및 MATH의 복잡한 추론 태스크에 대한 전반적인 결과를 보여준다. Self-Discover는 직접 프롬프트, CoT 및 Plan-and-Solve(PS)를 포함한 기준선과 비교된다.\n' +
      '\n' +
      'BBH의 총 23개 과제에서 Self-Discover는 PaLM 2-L에서 Chain-of-Thought와 Plan-and-Solve에 비해 각각 7%와 6%의 절대 개선을 달성하였다. GPT-4에 Self-Discover를 적용했을 때 유사한 이득(6%와 8%)이 관찰된다. 각 과제의 직접 응답과 PaLM 2-L의 CoT에 대한 개선 결과는 그림 1에 나와 있으며, 여기서 Self-Discover가 20/24 이상의 과제에 비해 우수함을 알 수 있다. 23개의 BBH 태스크 모두에 대한 태스크별 수행은 부록 C를 참조하시기 바랍니다.\n' +
      '\n' +
      '근거된 소셜 에이전트 작업 T4D에서 Self-Discover는 PaLM 2-L(GPT-4)의 모든 기준선에서 \\(\\geq 27\\%\\)(\\(32\\%\\)) 절대 개선에 도달한다. Self-Discover는 PaLM 2-L과 GPT-4에서 69%와 85%의 정확도를 얻었으며, 전문가 설계 추론 구조를 사용하는 Foreee and Reflect(FaR)와 같은 기존의 SoTA 프롬프트 방법을 크게 능가했다. 대조적으로, Self-Discover는 인간의 개입 없이 원자 추론 모듈 세트로부터 추론 구조를 자동으로 생성한다.\n' +
      '\n' +
      'MATH의 경우, 우리는 기준선에 비해 Self-Discover로부터 PaLM 2-L(GPT-4)에서 1%-7%(2%-3%)의 적당한 이득을 관찰한다. 오류 분석(자세한 내용은 부록 D 참조) 시, Self-Discover로부터 PaLM 2-L에 의해 생성된 추론 구조가 87.5% 정확하다는 것을 발견한다: 인간 전문가가 추론 구조를 따라 작업을 완벽하게 해결할 수 있다. 대부분의 고장(74.7%)은 계산 수행의 오류에서 비롯되며, 이는 이전 연구 결과와 일치한다(Zheng et al., 2023).\n' +
      '\n' +
      '자기 발견이 가장 도움이 되는 문제 유형은 무엇인가?\n' +
      '\n' +
      '**셀프 디스커버는 _diverse world knowledge_**을 필요로 하는 태스크에서 가장 잘 수행한다. 그림 4는 우리가 테스트하는 4가지 범주의 추론 과제에 대해 직접 답과 CoT에 대한 자기 발견 정확도의 델타 측면에서 평균 개선을 보여준다. 우리는 Suzgun et al.(2022)의 범주화를 채택한다. 우리는 모든 범주에서 이 두 가지 기준선에 비해 셀프 디스커버가 개선되지만 특히 _스포츠 이해_, _영화 추천_ 및 _루인 이름_와 같은 세계 지식이 필요한 작업에 대해 개선된다.\n' +
      '\n' +
      '이러한 작업은 사실과 일반을 사용하여 추론 모델을 요구합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c} \\hline \\hline Method & BBH & T4D & MATH \\\\ \\hline PaLM 2-L & 56\\% & 30\\% & 45\\% \\\\ PaLM 2-L + CoT & 60\\% & 40\\% & 42\\% \\\\ PaLM 2-L + PS & 61\\% & 42\\% & 49\\% \\\\ PaLM 2-L + Self-Discover & **67\\%** & **69\\%** & **50.5\\%** \\\\ \\hline GPT-4 & 58\\% & 51\\% & 70.5\\% \\\\ GPT-4 + CoT & 75\\% & 52\\% & 71\\% \\\\ GPT-4 + PS & 73\\% & 53\\% & 70\\% \\\\ GPT-4 + Self-Discover & **81\\%** & **85\\%** & **73\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: Self-Discover는 BBH, T4D 및 MATH의 25가지 복잡한 태스크의 다양한 세트에 걸쳐 LLM 추론을 상당히 향상시킨다. CoT: Zero-shot Chain of Thought(Kojima et al., 2022). PS: plan-and-solve prompting (Wang et al., 2023).\n' +
      '\n' +
      '그림 4: PaLM 2-L 상의 4개 카테고리에 대한 Self-Discover 성능 개선 **Breakdown. 셀프 디스커버는 세계지식이 필요한 작업에서 최고의 성능을 발휘합니다.**\n' +
      '\n' +
      '상식적인 지식. 우리는 이러한 과제에 대한 Self-Discover의 장점을 다양한 관점에서 여러 추론 모듈을 통합함으로써 CoT를 적용하는 것만이 추론 과정의 핵심 지식을 놓칠 수 있는 강점으로 해석한다. 알고리즘 범주에 대한 이득은 MATH에 대한 Sec. 4.1의 결과와 일치하는 중간 정도임을 관찰한다.\n' +
      '\n' +
      '자기 발견이 얼마나 효율적인가?\n' +
      '\n' +
      'Self-Discover는 self-consistency 또는 majority voting에 비해 10-40배 적은 추론 컴퓨터를 요구하면서도 더 나은 성능을 달성한다. 여기서 우리는 BBH로부터 2개의 태스크의 서브세트를 조사하고 24개의 태스크 모두에서 실행하기에 너무 많은 비용이 드는 많은 추론 호출을 요구하는 방법을 포함하여 보다 철저한 방법을 비교한다. 그림 5는 GPT-4를 사용하여 각 방법에 대해 인스턴스당 필요한 추론 호출의 평균 정확도와 횟수를 보여준다. **정확도 wise(y축)**, Self-Discover가 CoT-self-consistency 및 각 RM 적용의 다수결 투표와 같은 반복 추론 호출이 필요한 경우에도 다른 기준선보다 우수함을 알 수 있다. **Efficiency wise (x-axis)**, Self-Discover는 인스턴스당 1번의 호출과 태스크 레벨에서 3번의 추론 호출만 필요하며 CoT-self-consistency는 인스턴스당 10번의 샘플링을 해야 하기 때문에 10배가 더 필요하며, 각 RM을 사용하는 방법은 40개의 RM을 사용함에 따라 40배가 더 필요하다. 요약하면, Self-Discover는 대규모에 배치하기에 효율적인 강력한 추론 부스팅 방법을 제시한다.\n' +
      '\n' +
      '### Qualitative Examples\n' +
      '\n' +
      '우리는 그림 6에서 PaLM \\(2\\)-L로부터 다양한 추론 작업에 대한 모델 발견 구조의 예를 보여준다. 우리는 각 구조가 작업에 고유하게 적응하고 여러 추론 모듈을 통합하며 작업을 해결하는 방법에 대한 통찰력을 제공한다는 것을 관찰한다. 또한, CoT, Plan-and-Solve, Self-Discover의 추론 과정을 비교한 예는 그림 7과 같다. CoT와 Plan-and-Solve는 잘못된 주장을 조기에 하고 오답에 도달하는 반면, Self-Discover의 구조를 따라가면 모델이 논리적인 결론을 생성하게 된다("_path는 시작 좌표와 끝 좌표가 같으므로 닫힙니다") 그리고 정답에 도달한다.\n' +
      '\n' +
      '##5 Self-Discovered Reasoning Structure로 딥다이빙\n' +
      '\n' +
      '다양한 추론에 대한 자기 발견의 효과성과 효율성을 보여주는 실험 결과\n' +
      '\n' +
      '그림 5: ** 인스턴스당 필요한 추론 호출 수와 정확도의 비교**. CoT-Self-Consistency를 위해 10번 표본을 추출합니다. 각 RM 방법의 최고에는 금 라벨(*)이 필요합니다. 셀프 디스커버는 GPT-4에서 40배 더 많은 호출 요구 방법(각 RM의 다수 투표)에 비해 더 나은 성능에 도달하면서 직접 및 CoT와 동일한 인스턴스당 1개의 추론 호출(태스크 레벨에서 3개의 메타 프롬프트 추가)만을 필요로 하며, 셀프 디스커버 입력과 출력이 CoT 및 Direct 프롬프트보다 더 길어져 비용이 증가한다는 것을 인정한다. 그러나 인스턴스의 수가 증가함에 따라 인스턴스당 추론 측면에서 Self-Discover의 효율성이 매우 바람직하다.\n' +
      '\n' +
      '그림 6: PaLM 2-L을 사용하여 BBH 작업에서 자체 발견된 구조의 **예. 우리는 "_step-by-step thinking_", "_reflect on task nature_", 모델들이 괄호 파싱 작업을 해결하기 위해 _stack_을 사용하는 알고리즘을 고안하는 흥미로운 창의적 사고 사례와 같은 원자 추론 모듈의 특성을 관찰한다.**\n' +
      '\n' +
      '과제, 이 섹션에서는 **이 필요한 Self-Discover의 모든 행위이며 **자가 발견한 구조가 가져올 수 있는 다른 이점은 무엇인가?** 5.1절에서 SELECT, ADAPT 및 IMPLEMENT의 세 단계를 통해 발견된 추론 구조를 사용하는 것이 모델의 성능에 중요하다는 것을 보여준다. 5.2절에서는 (1) PaLM 2-L이 발견한 구조를 GPT-4에 적용하고, (2) GPT-4가 발견한 구조를 Llama-2-70B에 적용하여 자기 발견 추론 구조의 **일반성**을 입증한다. 우리는 부록 E에서 추론 구조와 인간 추론 패턴 사이의 공통점을 추가로 보여준다.\n' +
      '\n' +
      'Self-Discover Action의 중요도\n' +
      '\n' +
      '셀프 디스커버 액션의 효과를 분석하기 위해 SELECT, ADAPT, IMPLEMENT의 세 가지 액션에 대한 절제 연구를 수행한다. 그림 8은 SELECT(-S)를 적용하거나 SELECT와 ADAPT(-SA)를 적용하거나 세 가지 동작을 모두 적용할 때 4가지 추론 태스크에 GPT-4를 사용한 결과를 보여준다. 각 단계에서 모델의 제로 샷 추론 능력은 작업 전반에 걸쳐 일관되게 향상되어 세 가지 행동 모두가 유익함을 나타낸다. 특히, 세 가지 동작들(SAI) 모두 후에, 추론 구조들은 태스크 특정으로 적응되고, 추론 태스크들을 해결하는 데 가장 큰 이득을 가져온다.\n' +
      '\n' +
      '발견된 추론구조의 보편성을 향한###\n' +
      '\n' +
      '** PaLM 2-L 발견 구조를 GPT-4**에 적용하는 단계 먼저 PaLM 2-L 모델을 사용하여 4가지 추론 태스크의 추론 구조를 발견한다. 그런 다음, GPT-4를 접지로서 디코딩하는데 그 결과 추론 구조를 적용한다. 최적화를 통해 제로샷 프롬프트를 발견한 OPRO(Yang et al., 2023)와 우리의 접근법을 비교한다. 각 태스크에 PaLM 2-L을 사용하여 최적화된 OPRO 프롬프트를 동일한 추론 태스크에 GPT-4에 적용한다. 그림 9는 OPRO가 20% 데이터를 사용하여 최적화를 수행했음에도 불구하고 4개 작업 중 3개 작업에서 Self-Discover가 OPRO보다 우수하다는 것을 보여준다.\n' +
      '\n' +
      '그림 8: **4개의 추론 과제에 대한 세 가지 자기 발견 행동에 대한 절제 연구**: 세 가지 행동 모두 과제 해결에 유익하다.\n' +
      '\n' +
      '그림 7: **BBH-기하학적 형상 작업에 대한 CoT, Plan-and-Solve, Self-Discover로부터 생성된 추론 과정의 비교. CoT와 Plan-and-Solve 모두 경로가 닫힌 경로(빨간색으로 강조됨)가 아니므로 규칙적인 모양을 형성하지 않고 오답에 도달한다고 잘못 주장한다. Self-Discover의 추론 구조(블루 쿠리에 폰트)는 먼저 각 선분을 분해하여 좌표를 주의 깊게 분석한 후 논리적 추론을 활용하여 경로가 같은 좌표(보라색과 주황색으로 강조됨)에서 끝나면서 닫힌 형태를 이룬다고 결론을 내리고 최종 추론을 통해 정답을 선택한다.**\n' +
      '\n' +
      'GPT-4 탐색된 구조를 Llama2와 ChatGPT**에 적용하여 LLMs에 걸친 전이성 성능으로 동기화된 LLMs로부터 구조 자체 3을 도출하기 어려운 작은 LMs_에 대해 스스로 탐색된 추론 구조를 탐색할 수 있다. GPT-4를 사용하여 태스크 내재적 추론 구조를 탐색한 후 BBH의 두 하위 집합에서 GPT-3.5-터보(ChatGPT)뿐만 아니라 오픈소싱 Llama2-70B의 디코딩에 적용한다. 연구 결과, Llama2(52%)가 CoT(42%)보다 명확성 QA 제로샷에서, GPT-3.5-터보(56%)가 기하학에서 CoT(51%)보다 우수한 성능을 보였다.\n' +
      '\n' +
      '각주 3: 우리는 Llama2를 프롬프트하는 제로 샷 메타를 시도했지만 저품질 구조 출력을 관찰했다.\n' +
      '\n' +
      '##6 관련 업무\n' +
      '\n' +
      '### Prompting Methods\n' +
      '\n' +
      '최근 LLMs 분야의 발전은 많은 수의 샷(Brown et al., 2020) 및 명령어(Mishra et al., 2022; Wei et al., 2021; Ouyang et al., 2022) 프롬프팅 기술, _Chain-of-Thought_ 프롬프팅(CoT)(Nye et al., 2021; Wei et al., 2022), Least-to-most 프롬프팅(Zhou et al., 2022; Drozdov et al., 2022), Decomposed 프롬프팅(Khot et al., 2022), Reframing(Mishra et al., 2022), Help Me Think Prompting(Mishra and Nouri, 2023), Stepback Prompting(Zheng et al., 2023) 및 _Tree-of-Thought(ToT)_(Yao et al., 2023), Graph-of-Thought(Besta et al., 2023; Yao et al., 2023) 및 RAP(Hao et al., 2023)를 포함한다. 각 프롬프트 방법은 성공적인 응용 도메인 측면에서 몇 가지 강점과 약점을 가지고 있다. 본 논문에서 제안하는 Self-Discover는 제안된 Self-Discovery 메커니즘을 통해 다양한 prompting 방법에 대해 Self-Composition을 할 수 있는 방법을 제공하기 때문에, Self-Discover는 prompting 문헌에서 누락된 조각을 제시한다. Self-Discover에서 프롬프트 메소드들을 구성하는 것은 루프, if/else 조건 등과 같은 다양한 기본 빌딩 블록들을 사용하여 프로그램이 작성되는 프로그래밍 문헌과 유사하다.\n' +
      '\n' +
      '### 추론과 계획\n' +
      '\n' +
      'GSM8K(Cobbe et al., 2021), Math(Hendrycks et al.), BigBench(Srivastava et al., 2023) 등과 같은 다양한 추론 및 계획 벤치마크의 개발에 따라 모델 성능을 향상시키기 위한 다양한 방법들이 제안되고 있다. 종종 이러한 방법들은 데이터세트와 연관된 기본 태스크의 추론 구조를 모방하는 특정 추론 구조를 유도한다. 예를 들어, 사고 체인(Wei et al., 2022) 및 스크래치패드(Nye et al., 2021)는 추론 질문과 연관된 설명들의 생성을 유도한다. 유사하게 다른 방법들은 질문 요약(Kuznia et al., 2022), 질문 분해(Patel et al., 2022), 프로그램 생성(Mishra et al., 2022; Chen et al., 2022; Gao et al., 2023) 등과 같은 특정 추론 구조를 유도한다. 그러나, 실제 사용자 트래픽에서 질의는 다양한 추론 구조를 포괄하여 다양할 수 있다. 우리의 작업 Self-Discover는 모델이 작업 레이블에 액세스할 필요 없이 구조로 자체 구성함으로써 여러 추론 접근 방식을 결합할 수 있게 한다. SkiC(Chen et al., 2023), 전략 구상(Gao et al., 2023), 반복 쿼링(Liu et al., 2023)과 같은 맥락에서 기술을 결합한 LLM을 탐구하는 관련 작업이 있다. 그러나, 이들은 인간의 주석 기술과 추론 계획을 필요로 하는 반면, Self-Discover는 LLM의 메타 태스크 추론 능력의 도움으로 확장 가능한 솔루션을 활용한다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '우리는 일반적인 문제 해결 기술의 씨앗 세트로부터 모든 과제에 대한 추론 구조를 스스로 발견하기 위한 모델의 효율적이고 수행 가능한 프레임워크인 Self-Discover를 소개한다. 우리는 최대 30%까지 여러 LLM에서 도전적인 추론 벤치마크에 대한 급격한 개선을 관찰한다. Self-Discover에 대한 제거 연구는 구성된 추론 구조가 LLMs 간에 보편적으로 전이 가능하다는 것을 보여준다. 앞으로 우리는 문제 해결의 경계를 넘어 휴먼-AI 협업의 잠재력을 발견하기 위해 LLM 구조 추론에 대해 더 많이 탐구하게 되어 기쁩니다.\n' +
      '\n' +
      '도 9: 최적화된 프롬프트(OPRO) 및 구성된 구조(Self-Discover)의 **전달성 테스트. 표시된 결과는 PaLM 2-L을 사용하여 최적화되거나 구성된 프롬프트 및 구조를 사용하는 GPT-4에서 가져온 것이다. 우리는 자기 발견 추론 구조가 최적화된 프롬프트보다 더 강건하게 전달됨을 발견한다.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      '우리는 이 논문에 대한 통찰력 있는 피드백에 대해 구글 딥마인드의 앤드류 다이와 애덤스 유에게 감사한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*R. 안일아목대 피라트 존슨, D. 레피킨, A. 파소스, S. 샤케리, E. 타로파, P. 베일리, Z. Chen, et al. Palm 2 기술 보고서. ArXiv:2305.10403, 2023\n' +
      '* Besta et al. (2023) Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. ArXiv:2308.09687, 2023\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models is few-shot learners. 2020년, 신경 정보 처리 시스템 33:1877-1901의 발전.\n' +
      '* Chen et al. (2023) Chen, J., Pan, X., Yu, D., Song, K., Wang, X., Yu, D., and Chen, J. Skills-in-context prompting: Unlocking compositionality in large language models. ArXiv:2308.00304, 2023.\n' +
      '* Chen et al. (2022) Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. ArXiv:2211.12588, 2022년\n' +
      '* Chowdhery et al. (2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. ArXiv:2204.02311, 2022년\n' +
      '* Chung et al. (2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. ArXiv:2210.11416, 2022년\n' +
      '*Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. ArXiv:2110.14168, 2021년\n' +
      '* Drozdov et al. (2022) Drozdov, A., Scharli, N., Akyurek, E., Scales, N., Song, X., Chen, X., Bousquet, O., and Zhou, D. Large language models과의 Compositional semantic parsing. ArXiv:2209.15003, 2022년\n' +
      '* Fernando et al. (2023) Fernando, C., Banarse, D., Michalewski, H., Osindero, S., and Rocktaschel, T. 프롬브라이더: 신속한 진화를 통한 자기 참조적 자기 향상 ArXiv:2309.16797, 2023.\n' +
      '* Gao et al. (2023a) Gao, C., Jiang, H., Cai, D., Shi, S., and Lam, W. 전략: 문제 해결을 위한 전략 생성기, 실행기, 최적화기 및 평가기로서의 대규모 언어 모델. ArXiv:2311.08803, 2023a.\n' +
      '* Gao et al. (2023b) Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764-10799. PMLR, 2023b.\n' +
      '*Hao et al. (2023) Hao, S., Gu, Y., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z., and Hu, Z. 언어 모델을 이용한 추론은 세계 모델을 이용한 계획이다. ArXiv:2305.14992, 2023\n' +
      '* Hendrycks et al. (2021) Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. 정렬, 2(4):0-6.\n' +
      '* Hendrycks et al. (2021) Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset, 2021.\n' +
      '* Khot et al. (2022) Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., and Sabharwal, A. Decomposed prompting: A modular approach for solve complex tasks. 2022년 제11회 학습 표상 국제 회의에서\n' +
      '* Kojima et al. (2022) Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. 대형 언어 모델은 제로 샷 추론기입니다. 2022년, 신경 정보 처리 시스템 35:22199-22213의 발전.\n' +
      '* Kuznia et al. (2022) Kuznia, K., Mishra, S., Parmar, M., and Baral, C. Less is more: Summary of long instructions is better for program synthesis. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 4532-4552, 2022.\n' +
      '*Liu et al. (2023) Liu, T., Guo, Q., Yang, Y., Hu, X., Zhang, Y., Qiu, X., and Zhang, Z. 계획, 검증 및 전환: 다양한 생각을 가진 통합 추론입니다. ArXiv:2310.14628, 2023\n' +
      '* Mishra & Nouri (2023) Mishra, S. 그리고 누리, E. HELP METhink: 비전문가들이 모델로 맞춤형 콘텐츠를 만들기 위한 간단한 프롬프트 전략. 로저스, A., Boyd-Graber, J., and Okazaki, N. (eds.), _Findings of the Association for Computational Linguistics: ACL 2023_, pp. 11834-11890, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl. 751. URL[https://aclanthology.org/2023.findings-acl.751](https://aclanthology.org/2023.findings-acl.751).\n' +
      '\n' +
      '* Mishra et al. (2022) Mishra, S., Finlayson, M., Lu, P., Tang, L., Welleck, S., Baral, C., Rajpurohit, T., Tafjord, O., Sabharwal, A., Clark, P., et al. Lila: A unified benchmark for mathematical reasoning. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pp. 5807-5832, 2022a.\n' +
      '* Mishra et al. (2022b) Mishra, S., Khashabi, D., Baral, C., Choi, Y., and Hajishirzi, H. Reframing instruction prompts to gptk\'s language. In _Findings of the Association for Computational Linguistics: ACL 2022_, pp. 589-612, 2022b.\n' +
      '* Mishra et al. (2022c) Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Cross-task generalization via natural language crowdsourcing instructions. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 3470-3487, 2022c.\n' +
      '* Newell et al. (1958) Newell, A., Shaw, J. C., and Simon, H. A. Elements of the theory of human problem solving. _ Psychological review_, 65(3):151, 1958.\n' +
      '* Nye et al. (2021) Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. show your work: Scratchpads for intermediate computation with language models. _ arXiv preprint arXiv:2112.00114_, 2021.\n' +
      '* OpenAI(2022) OpenAI. Chatgpt: Optimizing language models for dialogue, 2022. URL[https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/).\n' +
      '* OpenAI(2023a) OpenAI. Json generation mode, 2023a. URL[https://platform.openai.com/docs/guides/text-generation/json-mode](https://platform.openai.com/docs/guides/text-generation/json-mode)\n' +
      '* OpenAI(2023b) OpenAI, R. Gpt-4 기술 보고서입니다 arXiv_, pp. 2303-08774, 2023b.\n' +
      '* Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. training language models to follow instructions with human feedback. _ 신경 정보 처리 시스템_, 35:27730-27744, 2022에서의 발전.\n' +
      '* Patel et al. (2022) Patel, P., Mishra, S., Parmar, M., and Baral, C. Is a question decomposition unit all we need? In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pp. 4553-4569, 2022.\n' +
      '* Polya(2004) Polya, G. _How to solve it: A new aspect of mathematical method_, volume 85. Princeton University Press, 2004.\n' +
      '* Rasmussen et al. (1983) Rasmussen, J. Skills, rules, and knowledge; signal, sign, and symbols, and other distctions in human performance models. _ IEEE transaction on systems, man, and cybernetics_, (3):257-266, 1983.\n' +
      '* Saha et al. (2023) Saha, S., Levy, O., Celikyilmaz, A., Bansal, M., Weston, J., and Li, X. Branch-solve-merge는 대용량 언어 모델 평가 및 생성을 향상시킨다. _ arXiv preprint arXiv:2310.15123_, 2023.\n' +
      '* Srivastava et al. (2023) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapating the capabilities of language models. _ 기계 학습 연구 관련 거래, 2023년\n' +
      '* Suzgun et al. (2022) Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challengeing big-bench tasks and whether the chain-of-thought can solve them. _ arXiv preprint arXiv:2210.09261_, 2022.\n' +
      '* Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention만 있으면 된다. In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL [https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf]).\n' +
      '* Wang et al. (2023) Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., and Lim, E.-P. 계획 및 해결 프롬프트: 대규모 언어 모델에 의한 제로 샷 사고 연쇄 추론 개선 arXiv preprint arXiv:2305.04091_, 2023.\n' +
      '* Wang et al. (2022) Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves the chain of thought reasoning in language models. _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* Wei et al. (2021) Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned 언어 모델은 제로 샷 학습자이다. _International Conference on Learning Representations_, 2021.\n' +
      '* Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. _ 신경 정보 처리 시스템_, 35:24824-24837, 2022에서의 발전.\n' +
      '* Yang et al. (2023) Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., and Chen, X. 최적화 역할을 하는 대규모 언어 모델 _ arXiv preprint arXiv:2309.03409_, 2023.\n' +
      '\n' +
      '* Yao et al. (2023a) Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. 생각의 나무: 큰 언어 모델을 사용하여 문제를 해결합니다. _ arXiv preprint arXiv:2305.10601_, 2023a.\n' +
      '* Yao et al. (2023b) Yao, Y., Li, Z., and Zhao, H. Beyond chain-of-thought, effective graph-of-thought reasoning in large language models. _ arXiv preprint arXiv:2305.16582_, 2023b.\n' +
      '* Yasunaga et al. (2023) Yasunaga, M., Chen, X., Li, Y., Pasupat, P., Leskovec, J., Liang, P., Chi, E. H., and Zhou, D. Large language models as analogical reasoners. _ arXiv preprint arXiv:2310.01714_, 2023.\n' +
      '* Zheng et al. (2023) Zheng, H. S., Mishra, S., Chen, X., Cheng, H.-T., Chi, E. H., Le, Q. V., and Zhou, D. Take a step back: Evoking reasoning via Zhou, D. arXiv preprint arXiv:2310.06117_, 2023.\n' +
      '* Zhong et al. (2021) Zhong, R., Lee, K., Zhang, Z., and Klein, D. Adaptation language models for zero-shot learning by meta-tuning on dataset and prompt collections. _ ArXiv:2104.04670_, 2021.\n' +
      '* Zhou et al. (2022a) Zhou, D., Scharli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q. V., et al. Least-to-most prompting은 큰 언어 모델에서 복잡한 추론을 가능하게 한다. _The Eleventh International Conference on Learning Representations_, 2022a.\n' +
      '* Zhou et al. (2023) Zhou, P., Madaan, A., Potharaju, S. P., Gupta, A., McKee, K. R., Holtzman, A., Pujara, J., Ren, X., Mishra, S., Nematzadeh, A., et al. arXiv preprint arXiv:2310.03051_, 2023.\n' +
      '* Zhou et al. (2022b) Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large 언어 모델은 인간 수준의 프롬프트 엔지니어이다. _The Eleventh International Conference on Learning Representations_, 2022b.\n' +
      '\n' +
      '## 부록 자기 발견 프롬프트 세부사항\n' +
      '\n' +
      '표 2는 문제 해결의 인지적 휴리스틱을 포함하는 Fernando et al.(2023)로부터 채택된 Self-Discover를 위해 우리가 사용하는 39개의 추론 모듈 모두를 보여준다.\n' +
      '\n' +
      '<그림 10>은 과제 수준에서의 내재적 추론 구조를 발견하는 1단계 동안 자기 발견의 세 가지 행위의 구조를 담고 있다.\n' +
      '\n' +
      '자체발견된 구조를 사용하여 태스크 인스턴스를 해결하는 단계 2의 경우, "_JSON에서 단계별 추론 계획을 따라 태스크를 올바르게 해결한다. 주어진 태스크에 대해 구체적으로 추론하여 키를 따르는 값을 채워넣는다. 단순히 키를 다시 주문하지 않는다._"라는 프롬프트로 시작한다. 추론 구조, 그리고 마지막으로 과제 인스턴스가 뒤따른다.\n' +
      '\n' +
      '## 부록 B 평가 세부사항\n' +
      '\n' +
      '우리는 BBH, T4D 및 MATH에서 테스트한 다른 방법과 마찬가지로 정확도와 정확한 매칭을 사용한다. LLM들로부터 생성된 답변들을 적절하게 평가하기 위해, 모델들이 "_Thus, 최종 답변은 [X]_"로 답변을 종료하도록 프롬프트하며, 여기서 X는 "_A_"와 같은 하나의 답변 옵션 또는 "_valid_"와 같은 스트링이다. 평가하는 동안 LLM에서 각 태스크의 출력을 수동으로 조사하고 최종 답변을 추출하기 위해 휴리스틱을 설계한다. MATH 데이터 세트의 경우 정답을 정확하게 추출하는 것이 어렵다는 것을 발견했다. 그 결과, MATH에서 200개의 테스트 예제를 서브샘플링하고, 본 논문에서 테스트된 모든 방법에 대해 수동으로 정상 상태를 확인하고 추출된 답변에 주석을 달았다.\n' +
      '\n' +
      '## 부록 CBBH Per Task Performance\n' +
      '\n' +
      'BBH에 대한 작업별 성능(총 23개 작업)은 표 3과 같다.\n' +
      '\n' +
      '## 부록 D 오류 분석\n' +
      '\n' +
      '200개 샘플의 MATH 데이터 세트에 대해 고장 모드를 이해하기 위해 Self-Discover의 오류 분석을 수행한다. 생성된 추론 구조가 정확한지 여부와 Self-Discover를 이용하여 모델 예측의 정확성 여부를 함께 수동으로 주석을 달았다. 인간의 전문가가 단순히 추론 구조를 따라 과제를 해결할 수 있다면 추론 구조는 올바른 것으로 정의된다.\n' +
      '\n' +
      '그림 10: **Self-Discover의 세 가지 행동에 대한 메타 프롬프트. 각 메타 프롬프트는 시작과 끝의 명령어, 추론 모듈 설명, 레이블이 없는 작업 예제로 구성된다. IMPLEMENT를 위해 추론 구조(plan)의 예를 모델링하기 위해 다른 작업을 위해 JSON에서 사람이 작성한 구조를 제시한다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline\n' +
      '**Reasoning Modules** \\\\ \\hline\n' +
      '1 그 문제를 해결하기 위한 실험을 어떻게 고안할 수 있을까? \\\\\n' +
      '2 이 문제를 해결하기 위한 아이디어 목록을 만들고, 이를 문제에 하나씩 적용하여 진전이 있는지 알아본다. \\\\\n' +
      '3 이 문제에 대한 진행도를 어떻게 측정할 수 있을까? \\\\\n' +
      '4 문제를 더 쉽게 풀 수 있도록 어떻게 단순화할 수 있을까? \\\\\n' +
      '5 이 문제의 기초가 되는 핵심 가정은 무엇인가? \\\\\n' +
      '6 각 솔루션의 잠재적인 위험과 단점은 무엇입니까? \\\\\n' +
      '7 이 문제에 대한 대안적 관점이나 관점은 무엇인가? \\\\\n' +
      '8 이 문제와 그 해결책의 장기적인 의미는 무엇인가? \\\\\n' +
      '9 이 문제를 어떻게 더 작고 다루기 쉬운 부분으로 분해할 수 있을까? \\\\\n' +
      '10 비판적 사고: 이 스타일은 문제를 다양한 관점에서 분석하고 가정을 질문하며 사용 가능한 증거나 정보를 평가하는 것을 포함한다. 논리적 추론, 증거 기반 의사 결정, 사고의 잠재적 편향이나 결함을 식별하는 데 중점을 둔다. \\\\\n' +
      '11 창의적인 사고를 시도하고, 혁신적이고 즉각적인 아이디어를 생성하여 문제를 해결합니다. 전통적인 경계를 넘어선 사고, 상상력과 독창성을 장려하는 파격적인 해결책을 탐색합니다. \\\\\n' +
      '12 문제를 해결하기 위해 다른 사람의 입력과 협업을 찾아보세요. 팀워크, 열린 의사소통, 그룹의 다양한 관점과 전문성을 활용하여 효과적인 솔루션을 제시합니다. \\\\\\\n' +
      '13 시스템 사고 사용: 문제를 더 큰 시스템의 일부로 간주하고 다양한 요소의 상호 연결성을 이해한다. 문제에 영향을 미치는 근본적인 원인, 피드백 루프 및 상호 의존성을 식별하고 시스템 전체를 다루는 전체론적 솔루션을 개발하는 데 중점을 둡니다. \\\\\n' +
      '14 위험 분석 사용: 문제에 대한 다양한 솔루션 또는 접근법과 관련된 잠재적 위험, 불확실성 및 절충안을 평가합니다. 성공 또는 실패의 잠재적 결과와 가능성을 평가하고 위험과 이익에 대한 균형 잡힌 분석을 기반으로 정보에 입각한 결정을 내리는 것을 강조합니다. \\\\\n' +
      '15 성찰적 사고 활용: 문제에서 물러나고 성찰과 자기 성찰의 시간을 갖는다. 문제 해결에 영향을 미칠 수 있는 개인적 편견, 가정 및 정신 모델을 검토하고 미래 접근 방식을 개선하기 위해 과거 경험에서 학습에 열려 있습니다. \\\\\n' +
      '16 해결해야 할 핵심 문제나 문제는 무엇인가? \\\\\n' +
      '17 문제에 기여하는 근본적인 원인이나 요인은 무엇인가? \\\\\n' +
      '18 이전에 시도된 잠재적인 해결책이나 전략이 있습니까? 만약 그렇다면, 결과와 교훈은 무엇이었을까? \\\\\n' +
      '19 이 문제를 해결하는 데 발생할 수 있는 잠재적인 장애물이나 문제는 무엇입니까? \\\\\n' +
      '20 문제에 대한 통찰력을 제공할 수 있는 관련 데이터나 정보가 있습니까? 그렇다면 어떤 데이터 소스를 사용할 수 있으며 어떻게 분석할 수 있습니까?\\\\\n' +
      '21 이 문제에 직접적으로 영향을 받는 이해관계자나 개인이 있는가? 그들의 관점과 요구는 무엇인가? \\\\\n' +
      '22 문제를 효과적으로 해결하기 위해 어떤 자원(재정, 인적, 기술적 등)이 필요한가?\\\\\n' +
      '23 과제 해결의 진행이나 성공은 어떻게 측정 또는 평가될 수 있는가? \\\\\\\n' +
      '24 어떤 지표들 또는 메트릭들이 사용될 수 있는가? \\\\\n' +
      '25 문제는 특정 전문 지식이나 기술을 필요로 하는 기술적 또는 실용적 문제인가? 아니면 개념적이거나 이론적인 문제인가? \\\\\\\n' +
      '26 문제는 제한된 자원, 인프라 또는 공간과 같은 물리적 제약을 수반합니까?\\\\\n' +
      '도 27은 사회적, 문화적 또는 심리적 이슈와 같은 인간의 행동과 관련된 문제인가? \\\\\n' +
      '28 문제는 의사 결정이나 계획과 관련이 있으며, 불확실하거나 경쟁적인 \\\\ 목표를 가지고 선택을 해야 하는가?\\\\\\\n' +
      '도 29는 데이터 분석, 모델링 또는 최적화 기술이 필요한 분석적 문제입니까?\\\\\n' +
      '30은 창의적 해결과 혁신이 필요한 설계 과제인가?\\\\\\\n' +
      '31 문제는 개별 사례보다 시스템 또는 구조적 문제를 해결해야 합니까? \\\\\\\n' +
      '32 문제는 시간에 민감하거나 긴급하여 즉각적인 주의와 조치가 필요한가?\\\\\\\n' +
      '33 이러한 종류의 문제 사양을 위해 전형적으로 어떤 종류의 솔루션이 생산됩니까? \\\\\n' +
      '34 문제 명세와 현재의 최상의 해결책이 주어졌을 때, 다른 가능한 해결책들에 대한 추측이 있다. \\\\\\\n' +
      '35 현재의 최선의 해결책이 완전히 틀렸다고 상상해 보자. 문제 명세에 대해 생각할 다른 방법이 뭐가 있을까? \\\\\n' +
      '36 이러한 문제 사양에 대해 알고 있는 것을 감안할 때 현재 최상의 솔루션을 수정하는 가장 좋은 방법은 무엇입니까? \\\\\n' +
      '37 현재 최상의 솔루션을 무시하면 문제에 완전히 새로운 솔루션을 만들 수 있습니다. \\\\\n' +
      '38 차근차근 생각해 보자. \\\\\n' +
      '39 Let’s make a step by step plan and implement it with good notion and explanation. \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 문제 해결을 위한 높은 수준의 인지 휴리스틱으로 구성된 39개의 추론 모듈 모두. 우리는 Fernando et al.(2023)로부터 그것들을 채택한다.\n' +
      '\n' +
      '200개의 예제 중 87.5%(175개)의 예제가 올바른 추론 구조를 가지고 있음을 알 수 있다. 12.5%(25)의 예는 예측 오류로 이어지는 잘못된 추론 구조를 가지고 있다. 표 4는 LLM이 과제를 오해하거나, 단계 중 하나에 오류를 범하거나 추론 구조에서 불필요한 단계를 추가하는 그러한 4가지 예를 보여준다.\n' +
      '\n' +
      '다음으로 Self-Discover에 나타난 모델의 오류를 분석한다. 모델 예측이 잘못된 99개의 예제 중 잘못된 추론 구조가 25.3%의 오류만을 차지한다. 나머지 74.7%의 오류는 수학 계산과 같은 중간 계산의 오류로 인한 것이다. 표 5는 이러한 에러의 3가지 예를 나타낸다. 이러한 통찰력은 향후 개선이 도구 또는 코드 생성과 같은 LLM의 단계적 계산 정확도를 개선하는 것을 목표로 해야 함을 나타낸다.\n' +
      '\n' +
      '## 부록 E 추가 분석\n' +
      '\n' +
      '모델이 발견한 추론 구조 대 추론 구조 인간 추론 패턴 LLM이 발견한 추론 구조가 인간 추론 패턴과 몇 가지 공통점을 공유하는지 조사한다. 인간에게 레이블이 없는 3개의 태스크 인스턴스와 예제 추론 구조(Self-Discover meta-reasoning stage와 동일)를 부여하고 이를 해결하기 전에 태스크에 대한 추론 구조를 작성하도록 요청한다. 그림 11은 BBH-navigation 과제에 대한 인간과 LLM으로 구성된 추론 구조를 비교한 것이다. 우리는 각 동작 후에 멘탈 노팅과 같은 유사한 구조를 관찰한다. LLM 자체 발견 구조의 유망한 발견에서 인간 메타 추론의 특성을 높이고 공유함으로써 복잡한 문제 해결을 위한 인간-AI 협력을 연구하기 위한 더 많은 미래 작업을 장려하기를 바란다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline\n' +
      '**Big Bench-Hard Task** & **Human (Avg.)** & **Human (Max)** & **GPT-4** & **GPT-4** & **GPT-4** & **PaLM 2-L** & **PaLM 2-L** & **PaLM 2-L** \\\\  & & **Direct** & **+ CoT** & **+ Self-Discover** & **Direct** & **+ CoT** & **+ Self-Discover** \\\\ \\hline boolean\\_expressions & 79 & 100 & 73 & 83 & 85 & 71 & 84 & 84 \\\\ causal\\_judgement & 70 & 100 & 67 & 75 & 80 & 46 & 59 & 61 \\\\ date\\_understanding & 77 & 100 & 74 & 80 & 81 & 73 & 78 & 78 \\\\ disambiguation\\_qa & 67 & 93 & 60 & 70 & 80 & 54 & 50 & 57 \\\\ dyck\\_languages & 48 & 100 & 69 & 73 & 77 & 94 & 95 & 98 \\\\ formal\\_fallacies & 91 & 100 & 60 & 60 & 80 & 60 & 63 & 69 \\\\ geometric\\_shapes & 54 & 100 & 30 & 56 & 60 & 33 & 34 & 39 \\\\ hyperbendation & 75 & 100 & 68 & 69 & 76 & 80 & 75 & 82 \\\\ logical\\_deduction\\_seven\\_objects & 40 & 89 & 60 & 70 & 70 & 45 & 39 & 50 \\\\ movie\\_recommendation & 61 & 90 & 70 & 70 & 86 & 83 & 54 & 66 \\\\ multistep\\_arithmetic\\_two & 10 & 25 & 10 & 92 & 70 & 4 & 50 & 47 \\\\ navigate & 82 & 100 & 70 & 90 & 90 & 38 & 63 & 67 \\\\ object\\_counting & 86 & 100 & 90 & 100 & 100 & 27 & 44 & 70 \\\\ penguins\\_in\\_a\\_table & 78 & 100 & 80 & 100 & 90 & 70 & 67 & 75 \\\\ reasoning\\_about\\_colored\\_objects & 75 & 100 & 77 & 80 & 79 & 36 & 79 & 75 \\\\ ruin\\_names & 78 & 100 & 90 & 80 & 97 & 79 & 58 & 90 \\\\ salient\\_translation\\_error\\_detection & 37 & 80 & 40 & 50 & 70 & 56 & 48 & 60 \\\\ snarks & 77 & 100 & 73 & 89 & 97 & 58 & 62 & 86 \\\\ sports\\_understanding & 71 & 100 & 54 & 61 & 90 & 44 & 47 & 89 \\\\ temporal\\_sequences & 91 & 100 & 96 & 99 & 100 & 99 & 97 & 99 \\\\ tracking\\_shuffled\\_objects\\_seven\\_objects & 65 & 100 & 24 & 80 & 68 & 22 & 58 & 36 \\\\ web\\_of\\_lies & 81 & 100 & 15 & 80 & 71 & 54 & 42 & 67 \\\\ word\\_sorting & 63 & 100 & 65 & 90 & 85 & 12 & 4 & 15 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: Big Bench-Hard(Suzgun et al., 2022) per-task performance of GPT-4 and PaLM 2-L with Self-Discover.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Prompt & Reasoning Structure & Error \\\\ \\hline How many numbers between 1 and 2005 are integer multiples of 3 or 4 but not 12? & 1. Find the number of multiples of 3 between 1 and 2005. & Need to subtract the number of multiples of 4 between 1 \\\\  & 2. Find the number of multiples of 4 between 1 & ber of multiples of 12 \\\\  & 3. Find the number of multiples of 12 between 1 & twice instead \\\\  & 2005. & of once. \\\\  & 4. Add the number of multiples of 3 and 4. & \\\\  & 5. Subtract the number of multiples of 12 (to avoid \\\\  & double counting). & \\\\  & 6. The result is the number of integers between 1 & \\\\  & and 2005 that are multiples of 3 or 4 but not 12. & \\\\ \\hline How many numbers are in & 1. Find the number of pairs in the list. & LLM misunderstands the \\\\  & the list 6,7,10,11,14,15,..., & 2. Find the number of numbers in each pair. & \\\\\n' +
      '94,95,98? & 3. Multiply the number of pairs by the number of numbers in each pair to find the total number of numbers in the list. & \\\\ \\hline A restaurant offers three & 1. Define the variables: & There are 365 \\\\  & and exactly twice & - M: number of main courses \\\\  & as many appetizers as main & - A: number of appetizers (twice the number of \\\\  & courses) & - M: number of desserts (fixed at 3) \\\\  & and a dessert. What is the least number of main courses & 2. Calculate the total number of possible dinner \\\\  & that the restaurant should offer so that a customer could & - T = M * A * D \\\\  & have a different dinner each & 3. Set the total number of possible dinner combinations equal to the number of nights in the year 2003: \\\\  & - T = 2003 & 2003: \\\\  & 4. Substitute the expression for A in terms of M: & \\\\  & - T = M * (2 * M) * 3 & \\\\  & 5. Solve the equation for M: & \\\\  & - \\(6M^{2}=2003\\) & \\\\  & 6. Find the smallest integer value of M that satisfies the equation. & \\\\ \\hline Determine the number of & 1. Count the total number of letters in the word & Step 5 is un- \\\\  & ways to arrange the letters of & NINE. & \\\\  & the word NINE. & 2. Identify the number of times each letter appears in the word. & \\\\  & 3. Calculate the factorial of the total number of & \\\\  & letters. & \\\\  & 4. Divide the factorial by the factorial of the number of times each letter appears. & \\\\  & 5. Multiply the results from step 4 for each unique \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: MATH에 대한 잘못된 추론 구조의 예. 추론 구조의 첫 번째 오류는 빨간색으로 강조 표시됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Prompt & Self-Discover Prediction & Error \\\\ \\hline How many ways are there & "1. Calculate the total number of ways to arrange & There are 7 rooms \\\\ to arrange 6 people around a circular table with 7 seats? & 6 people in 7 seats without considering rotations & There are 7 rooms instead \\\\ (Two seatings are considered & - There are 7 choices for the first person. & of 6 that are considered the \\\\ the same if one is a rotation & - There are 6 choices for the second person. & same. \\\\ of the other.) & - There are 5 choices for the third person. & \\\\  & - There are 4 choices for the fourth person. & \\\\  & - There are 3 choices for the fifth person. & \\\\  & - There are 2 choices for the sixth person. & \\\\  & "2. Divide the result from step 1 by the number of & \\\\  & rotations that are considered the same.": 5040 / 6 & = 840 \\\\  & - There are 6 rotations that are considered the same. & \\\\ \\hline A customer ordered 15 pieces of gourmet chocolate. & "1. Find the number of ways to package the chocolates using only boxes of 1 piece.": 1, & There is 0 way to package 15 \\\\ The order can be packaged in small boxes that contain 1, 2 or 4 pieces of chocolate. & "2. Find the number of ways to package the chocolates using only boxes of 2 pieces.": 1, & \\\\ Any box that is used must be full. How many different combinations of boxes can be used for the customer’s 15 & "4. Find the number of ways to package the chocolate pieces.": 7, & \\\\ chocolate pieces? One such combination to be included & "5. Find the number of ways to package the chocolates using a combination of boxes of 1 and 4 pieces.": 3, & \\\\  & "6. Find the number of ways to package the chocolates using a combination of boxes of 2 and 4 pieces.": 2, & \\\\  & "7. Find the number of ways to package the chocolates using a combination of boxes of 1, 2, and 4 pieces.": 4, & \\\\  & "8. Sum up the total number of ways from the & \\\\  & seven cases above.": \\(1+1+0+7+3+2+4=18\\) & \\\\ \\hline How many primes are in the & "1. Identify the row of Pascal’s Triangle that starts with a 1 followed by a 6.": "1, 6, 15, 20, 15, 6, 1”, & 6 is not a prime. \\\\ row of Pascal’s Triangle that starts with a 1 followed by a 6? & "2. List all the numbers in that row.": [1, 6, 15, 20, 6? & 6? \\\\  & 15, 6, 1], & \\\\  & "3. Check each number in the list for primality.": [False, True, False, False, False, True, False], & \\\\  & "4. Count the number of primes found in the list.": 2 & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: MATH에 대한 잘못된 계산의 예. 중간 계산의 첫 번째 오류는 빨간색으로 강조 표시됩니다.\n' +
      '\n' +
      '그림 11: **인간 작성 구조의 사례 연구는 LLM이 발견한 추론 구조와 공통점을 공유한다.** 유사한 추론 패턴을 관찰한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>