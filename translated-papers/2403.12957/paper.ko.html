<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#GVGEN: Text-to-3D Generation with\n' +
      '\n' +
      'Volumetric Representation\n' +
      '\n' +
      'Xianglong He\n' +
      '\n' +
      '1상해 AI Lab\n' +
      '\n' +
      'Junyi Chen\n' +
      '\n' +
      '1상해 AI Lab\n' +
      '\n' +
      'Sida Peng\n' +
      '\n' +
      '이화선전 국제대학원\n' +
      '\n' +
      'Di Huang\n' +
      '\n' +
      '1상해 AI Lab\n' +
      '\n' +
      'Yangguang Li\n' +
      '\n' +
      '1상해 AI Lab\n' +
      '\n' +
      'Xiaoshui Huang\n' +
      '\n' +
      '1상해 AI Lab\n' +
      '\n' +
      'Chun Yuan\n' +
      '\n' +
      '이화선전 국제대학원\n' +
      '\n' +
      'Wanli Ouyang\n' +
      '\n' +
      '1상해 AI Lab\n' +
      '\n' +
      'Tong He\n' +
      '\n' +
      '1상해 AI Lab\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근, 3D 가우시안 스플래팅은 빠르고 고품질의 렌더링 능력으로 알려진 3D 복원 및 생성을 위한 강력한 기술로 부상하고 있다. 그럼에도 불구하고 이러한 방법은 종종 다양한 샘플을 생성하는 능력이 부족하거나 장기간의 추론 시간이 필요한 제한 사항이 있다. 이러한 단점을 해결하기 위해 본 논문에서는 텍스트 입력으로부터 3차원 가우시안 표현을 효율적으로 생성할 수 있도록 설계된 새로운 확산 기반 프레임워크인 GVGEN을 소개한다. 우리는 (1) _Structured Volumetric Representation_이라는 두 가지 혁신적인 기법을 제안한다. 먼저 3차원 가우시안 점들을 구조화된 형태의 가우시안 볼륨으로 배열한다. 이 변환은 고정된 수의 가우시안들로 구성된 볼륨 내에서 복잡한 텍스처 디테일을 캡처할 수 있게 한다. 이러한 세부 사항의 표현을 더 잘 최적화하기 위해 선택적 최적화를 통해 세부 사항 충실도를 향상시키는 후보 풀 전략이라는 독특한 가지치기 및 고밀도화 방법을 제안한다. (2) _Coarse-to-fine Generation Pipeline_. 가우시안 볼륨의 생성을 단순화하고 모델이 상세한 3D 지오메트리를 갖는 인스턴스들을 생성하도록 권한을 부여하기 위해, 우리는 거친-대-파인 파이프라인을 제안한다. 그것은 처음에 기본적인 기하학적 구조를 구성하고, 이어서 완전한 가우시안 속성들의 예측이 뒤따른다. 우리의 프레임워크인 GVGEN은 기존의 3D 생성 방법에 비해 질적 및 양적 평가에서 우수한 성능을 보여준다. 동시에 빠른 생성 속도(\\(\\sim\\)7초)를 유지하여 품질과 효율의 균형을 효과적으로 유지합니다.\n' +
      '\n' +
      '키워드:Text-to-3D Generation Feed-forward Generation 3D Gaussians +\n' +
      '각주 †: 대응하는 작가들.\n' +
      '\n' +
      '+\n' +
      '각주 †: 대응하는 작가들.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '3D 모델의 개발은 컴퓨터 그래픽의 중추적인 작업으로 비디오 게임 디자인, 필름 제작 및 AR/VR 기술을 포함한 다양한 산업 전반에 걸쳐 증가된 관심을 받고 있다. 3D 모델링의 다양한 측면 중 텍스트 기술에서 3D 모델을 생성하는 것은 접근성과 사용 용이성으로 인해 특히 흥미로운 연구 영역으로 부상했다. 작업을 처리하기 위해 다양한 방법[16, 31, 33]이 제안되었다. 여전히 텍스트의 모호성과 텍스트 설명과 해당 3D 자산 사이의 내재적 도메인 격차로 인해 어려움을 계속 제시하고 있다.\n' +
      '\n' +
      '기존의 텍스트 대 3D 접근법은 크게 최적화 기반 생성[8, 33, 41, 48]과 피드 포워드 생성[31, 32, 19, 32]의 두 가지 범주로 분류할 수 있다. 최근 텍스트-이미지 확산 모델의 급속한 발전으로 인해 최적화 기반 방법이 다소 인기를 얻고 있다[36, 12]. 이러한 방법들은 일반적으로 스코어 증류 샘플링(Score Distillation Sampling; SDS)[33]을 통해 텍스트 또는 이미지에 조건화된 3D 객체를 최적화하며, 2D 이미지 생성 모델로부터 풍부한 지식을 증류한다. 인상적인 결과를 얻었음에도 불구하고 최적화 기반 방법은 야누스 문제[33]에 직면하여 다중 면 또는 과포화 문제로 나타난다. 또한, 단일 객체의 최적화는 엄청난 시간 소모로 인해 광범위한 계산 노력이 필요할 수 있다. 대조적으로, 피드포워드 접근법은 텍스트 설명으로부터 직접 3D 자산을 생성하기 위해 노력하므로 야누스 문제를 피하고 생성 프로세스를 상당히 서두른다. 우리의 작업은 피드포워드 기반 방법과 밀접한 관련이 있다. 그러나 다시점 생성 모델을 이용하는 피드포워드 방법은 다시점 영상 대비 저해상도 3차원 자산을 생성하는 경우가 많다. 더욱이, 텍스트들로부터 3D 객체들을 직접 생성하는 모델들은 복잡한 프롬프트들이 사용될 때 종종 의미론적인 어려움들에 직면한다.\n' +
      '\n' +
      '텍스트-2D-3D 프레임워크를 따르는 [19]와 같은 이전의 피드포워드 기반 방법과 달리, 본 방법은 3D 표현을 직접 학습하여 3D 자산을 생성하는 것을 제안한다. 본 연구에서는 텍스트 기술로부터 직접 3D 가우시안 생성을 위한 혁신적이고 유선형 거친-미세 생성 파이프라인인 GVGEN을 소개한다. 3D Gaussians의 높은 표현력과 빠른 렌더링 능력을 활용하여, 우리의 방법은 유망한 결과를 얻을 뿐만 아니라 빠른 텍스트 대 3D 생성 및 렌더링을 유지한다. 도 1에 도시된 바와 같다. 본 논문에서 제안하는 방법은 가우시안 볼륨 피팅과 텍스트-투-3D 생성의 두 단계로 구성된다. 첫 번째 단계에서는 3차원 가우시안(Gaussians)으로 구성된 구조화된 체적 형태인 가우시안 볼륨(Gaussian Volume)을 소개한다. 이를 달성하는 것은 원래의 3D 가우시안들을 최적화하는 희박하고 비구조화된 특성 때문에 어렵다. 이를 해결하기 위해 가지치기와 치밀화를 위한 새로운 후보 풀 전략을 소개한다. 이 방법은 다음 단계에서 사용되는 확산 기반 프레임워크에 더 유용한 생성 프로세스를 만들기 위해 순서화되지 않은 가우시안 포인트보다 가우시안들의 고품질 볼륨 표현을 피팅할 수 있게 한다.\n' +
      '\n' +
      '가우스볼륨이 기존의 확산 파이프라인과 원활하게 통합되는 구조화된 체적 프레임워크를 구축했음에도 불구하고, 3D 가우시안들의 풍부한 특징들의 본질적인 복잡성은 중요한 과제를 제시한다. 구체적으로 방대한 양의 학습 데이터의 분포를 효과적으로 포착하는 것이 어려워져 확산 모델에 대한 하드 컨버전스가 발생하게 된다. 이러한 문제를 해결하기 위해 텍스트-3D 생성을 거친 기하학 생성과 가우시안 속성 예측의 두 단계로 나눈다. 보다 구체적으로, 첫 번째 단계에서는 확산 모델을 사용하여 물체의 거친 기하학을 생성하는데, 이를 가우시안 거리 필드(GDF)라고 한다 - 각 그리드 포인트가 가장 가까운 가우시안 포인트의 중심에 근접함을 설명하는 등방성 표현이다. 이에 따라 생성된 GDF는 텍스트 입력과 함께 3D U-Net 기반 모델을 통해 처리되어 가우시안 볼륨의 속성을 예측하여 향상된 제어 및 모델 수렴을 보장한다.\n' +
      '\n' +
      '우리가 아는 한, 이것은 텍스트로부터 3D 가우시안들을 직접 피드포워드 생성하는 첫 번째 연구이며, 빠른 3D 콘텐츠 생성 및 응용을 위한 새로운 길을 탐색한다. 우리의 주요 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '* 3D 가우시안들로 구성된 구조화된 체적 형태인 가우시안 볼륨을 소개한다. 프루닝 및 복제를 위한 혁신적인 후보 풀 전략을 통해 고정된 볼륨 해상도 내에서 고품질 가우스 볼륨 피팅을 수용한다. 이 프레임워크는 기존의 생성 네트워크와 원활하게 통합되어 명시적이고 효율적인 표현을 위해 3D 가우시안 고유의 이점을 활용한다.\n' +
      '* 먼저 지오메트리 볼륨을 생성한 후 세밀한 3D 가우시안 속성을 예측하여 생성된 자산의 다양한 지오메트리 및 외형을 보다 잘 제어하는 효율적인 텍스트-3D coarse-to-fine 생성 파이프라인인 GVGEN을 제안한다. GVGEN은 기본 방법에 비해 빠른 생성 속도(\\(\\sim\\)7초)를 달성하여 품질과 효율성 사이의 균형을 효과적으로 달성한다.\n' +
      '* 기존 기준선과 비교하여 GVGEN은 양적 측면과 질적 측면 모두에서 경쟁력 있는 역량을 보여준다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '### Text-to-3D Generation\n' +
      '\n' +
      '텍스트를 기반으로 하는 3D 객체를 생성하는 것은 최근 몇 년 동안 도전적이면서도 두드러진 연구 영역이 되었다. 이전 접근법[16, 29]은 3D 자산 최적화를 위한 이전으로 CLIP[34]를 활용하지만 사실성과 충실도가 부족했다. 텍스트-이미지 생성 모델[12, 36], 드림퓨전[33]은 다양한 3D 객체를 생성하기 위해 스코어 증류 샘플링(SDS)을 활용하여 이러한 모델에 내장된 풍부한 사전 지식을 활용한다. 그 후, 일부 작업 [24, 25, 6, 21]은 일관성을 향상시키기 위해 객체의 다중 모달 속성(예: 색상, 알베도, 정규 및 깊이)을 모델링하고 학습하는 데 중점을 둔다. ProlificDreamer[41]와 LucidDreamer[22]는 품질을 향상시키기 위해 새로운 손실을 도입한다. 일부 연구[15, 37, 38, 23]는 객체에 대한 멀티뷰를 예측하는 것을 탐색하고, 이어서 피드-포워드 방법 또는 SDS 기반 최적화를 사용하여 3D 객체를 생성한다. 또한, 최근에 제안된 3D Gaussian Splatting[18], 작업 [39, 46, 8]을 통합하면 텍스트 대 3D 객체 생성을 위한 수렴 속도가 향상된다. 그러나, 이러한 방법들은 여전히 객체당 상당한 생성 시간(\\(\\sim\\) 시간)을 요구하거나 생성된 객체의 3차원 불일치를 직면한다.\n' +
      '\n' +
      '대규모 3D 데이터셋[10, 11]의 등장으로 Feed-forward 모델은 생성 시간을 줄이기 위해 학습된다. 수백만 개의 3D 자산에 대해 훈련된 포인트-E[31] 및 Shap-E[17]은 각각 포인트 클라우드 및 신경 복사 필드를 생성한다. 3D VADER[32]는 잠재 볼륨에 맞도록 오토 디코더를 훈련시키고 확산 모델을 훈련시키는 데 사용한다. 유사하게, VolumeDiffusion[40]은 확산 모델에 대한 트레이닝 데이터를 생성하기 위해 효율적인 체적 인코더를 트레이닝한다. 이러한 발전에도 불구하고 텍스트에서 3D 가우스 생성은 주로 조직화되지 않은 가우시안들을 조직화하는 복잡성으로 인해 대부분 미개척 상태로 남아 있다. 본 논문에서는 텍스트로부터 3차원 가우시안들을 생성하기 위한 coarse-to-fine 파이프라인(peed-forward pipeline)을 소개한다. 먼저 객체의 coarse geometry를 생성한 후, 그것의 명시적인 속성을 예측함으로써, 텍스트 기술로부터 3차원 가우시안들을 직접 생성하는 것을 개척한다.\n' +
      '\n' +
      '### 차별화된 3D 표현\n' +
      '\n' +
      'NeRF(Neural Radiance Field)[28]의 제안 이후 다양한 미분 가능한 신경 렌더링 방법[2, 3, 5, 30]이 등장하여 장면 재구성 및 새로운 뷰 합성에서 현저한 능력을 보여준다. 이러한 방법들은 3D 생성 작업에서도 널리 사용되고 있다. 인스턴트-NGP[30]은 대응하는 공간 포지션들에서만 피처들을 질의함으로써 가속들에 대한 피처 볼륨들을 채용한다. [3, 5]는 기능을 하위 차원으로 분해하여 더 빠른 훈련과 더 적은 메모리 저장 공간을 제공합니다. 그러나, 이들은 렌더링 속도 및 명시적 조작성의 측면에서 점 기반 렌더링 방법[4, 45]에 비해 여전히 몇 가지 단점을 가지고 있다.\n' +
      '\n' +
      '최근 연구에서는 3D Gaussian Splatting[18]이 널리 주목받고 있다. 장면을 표현하기 위해 이방성 가우시안(Gaussians)을 채택하여 실시간성을 달성합니다.\n' +
      '\n' +
      '도 1: **GVGEN의 개요. 우리의 프레임워크는 두 단계로 구성된다. 데이터 전처리 단계에서는 가우시안 볼륨(Sec. 3.1)을 피팅하고 가우시안 거리 필드(GDF)를 학습 데이터로 추출한다. 생성 단계(Sec. 3.2)는 먼저 확산 모델을 통해 GDF를 생성한 후 3D U-Net으로 전송하여 가우시안 볼륨의 속성을 예측한다.**\n' +
      '\n' +
      '3D 생성[35, 44, 47], 장면 편집[7] 및 동적 장면 렌더링[42]과 같은 다운스트림 작업을 렌더링하고 용이하게 한다. 본 논문에서는 가우시안 볼륨(Gaussian Volume)이라는 구조화된 볼륨 형태로 3D 가우시안(Gaussian Gaussians)을 적합시키기 위한 전략을 소개한다. 가우시안 볼륨은 기존의 생성 파이프라인과의 좋은 통합을 가능하게 하며, 3D 가우시안(Gaussian Gaussians)의 장점을 활용하고 빠른 생성 및 렌더링 속도를 달성한다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '우리의 텍스트-투-3D 생성 프레임워크(Fig. GVGEN은 가우시안 볼륨 피팅(Sec. 3.1)과 텍스트-투-3D 생성(Sec. 3.2)의 두 가지 중추 단계로 설명된다. 처음에, 가우시안 볼륨 피팅 단계에서, 우리는 가우시안 볼륨이라고 불리는 3D 가우시안들의 구조화된 체적 형태를 제안한다. 생성 단계에 대한 학습 데이터로 가우시안 볼륨을 피팅한다. 이 단계는 조직화되지 않은 점-클라우드 유사 가우시안 점을 신경망 처리에 더 적합한 형식으로 배열하는 데 중요하다. 이를 해결하기 위해 3D 가우시안들의 고정된 개수(즉, 볼륨에 대한 고정된 볼륨 해상도)를 사용하여 가우시안 볼륨(Sec. 3.1)을 형성함으로써 처리의 용이성을 용이하게 한다. 또한, 적합 자산의 충실도를 향상시키기 위해 가우시안 포인트의 동적 가지치기, 복제 및 분할을 위한 후보 풀 전략(Candidate Pool Strategy, CPS)을 소개한다. 우리의 가우시안 볼륨은 적은 수의 가우시안(32,768점)만으로 높은 렌더링 품질을 유지한다.\n' +
      '\n' +
      '생성 단계 3.2에서는 먼저 입력 텍스트에 조건화된 확산 모델을 사용하여 생성된 객체의 기하학을 나타내는 가우시안 거리 필드(GDF)라고 하는 거친 기하학 볼륨을 생성한다. 이어서, 3D U-Net 기반 재구성 모델은 GDF 및 텍스트 입력을 활용하여 최종 가우시안 볼륨의 속성을 예측함으로써 텍스트 설명으로부터 상세한 3D 객체의 생성을 달성한다.\n' +
      '\n' +
      '### Stage1 : 가우시안 볼륨 피팅\n' +
      '\n' +
      '오리지널 3D 가우시안 스플래팅 기법은 빠른 최적화 및 실시간 렌더링을 제공한다. 그러나 구조가 부족하여 3D 신경에 어려움이 있다.\n' +
      '\n' +
      '도 2: **GaussianVolume Fitting의 도면. 우리는 고정된 수의 3D 가우시안들을 가우시안 볼륨이라고 하는 부피 형태로 구성한다. 위치 오프셋을 사용하여 그리드 포인트에서 가우시안 센터로의 약간의 움직임을 표현함으로써 객체의 세부 사항을 캡처할 수 있다. 제안된 후보 풀 전략(CPS)(Sec. 3.1)은 프루닝된 포인트를 저장하는 풀로 효과적인 프루닝 및 치밀화를 가능하게 한다.**\n' +
      '\n' +
      '네트워크를 효과적으로 처리합니다. 네트워크를 통해 직접 3D 가우시안들을 생성하는 것은 기존의 포인트 클라우드 네트워크들을 통합하는 것을 포함하며, 가우시안들을 의미적으로 풍부한 특징들의 캐리어로 취급한다. 그러나 선행 연구[43]에서는 3차원 색상 특징만으로 포인트 클라우드를 직접 생성하는 것이 어렵다는 점을 지적하였다. 특히 고차원 가우시안에게 이러한 방법을 확장하는 것은 학습 과정을 매우 어렵게 만들 수 있다. 우리는 또한 3D 가우시안 생성을 위해 점 구름 확산을 사용하여 이전 작업 [27]을 확장하려고 시도했지만 결과는 실망스러웠다.\n' +
      '\n' +
      '최근 연구[44, 49]에서는 3차원 가우시안과 트리플레인-NeRF를 결합한 하이브리드 표현을 제안하며, 기하학에는 포인트 클라우드를, 텍스쳐 질의에는 트리플레인 특징을 활용하여 최종적으로 3차원 가우시안들을 디코딩한다. 이러한 방법은 효과적인 결과를 보여주지만, 재구성된 결과는 예측된 점 구름의 정확도에 크게 제약을 받고 3D 가우시안들의 직접적인 조작성 이점을 상실한다. 이러한 문제에 비추어, 우리는 고정된 수의 3D 가우시안들로 구성된 볼륨인 가우시안 볼륨을 소개한다. 이 혁신은 기존 세대 및 재구성 모델에 의한 3D 가우시안 처리를 용이하게 하여 가우시안들의 효율성 이점을 유지한다. 피팅 공정의 예시는 그림 2에서 확인할 수 있다.\n' +
      '\n' +
      'GaussianVolume\n' +
      '\n' +
      '3D Gaussian Splatting [18]에서 3D Gaussians의 특징은 중심 위치\\(\\mu\\in\\mathbb{R}^{3}\\), 공분산 행렬\\(\\Sigma\\), 색상 정보\\(c\\in\\mathbb{R}^{3}\\) (SH 차수=0일 때), 불투명도\\(\\alpha\\in\\mathbb{R}\\) 등의 속성을 포함한다. 공분산 행렬을 더 잘 최적화하기 위해, \\(\\Sigma\\)은 스케일링 행렬 \\(S\\in\\mathbb{R}^{3}\\)과 회전 행렬 \\(R\\in\\mathbb{R}^{3\\times 3}\\)을 통해 타원체의 구성을 설명하는 것과 유사하며, 만족한다:\n' +
      '\n' +
      '\\[\\Sigma=RSS^{T}R^{T}. \\tag{1}\\]\n' +
      '\n' +
      '구현에서 \\(S,R\\)은 각각 3D 벡터 \\(s\\in\\mathbb{R}^{3}\\)와 쿼터니언 \\(q\\in\\mathbb{R}^{4}\\)으로 저장될 수 있었다. 이러한 미분 가능한 특징들로 3D 가우시안들은 2D 스플랫에 쉽게 투영될 수 있고, 2D-픽셀 컬러들을 렌더링하기 위해 신경점 기반 \\(\\alpha\\) 블렌딩 기술을 채용한다. 효율적인 타일 기반 래스터라이저는 빠른 렌더링과 역전파를 위해 사용된다.\n' +
      '\n' +
      '본 논문에서는 3차원 가우시안(Gaussians)으로 구성된 볼륨(V\\in\\mathbb{R}^{C\\times N\\times N\\times N}\\)을 이용하여 물체의 다시점 영상을 학습하여 각각의 물체를 표현한다. 여기서, \\(C,N\\)은 각각 특징 채널의 수와 볼륨 해상도를 나타내며, 3D 가우시안 수는 \\(N^{3}\\)이다. 우리는 원작에서 관례와 렌더링 방법을 따르지만, 볼륨에서 각 격자점의 위치 \\(p\\)와 가우스 점의 중심 \\(\\mu\\) 사이의 약간의 움직임을 표현하기 위해 위치 오프셋 \\(\\Delta\\mu\\)을 사용한다:\n' +
      '\n' +
      '\\[\\mu=p+\\Delta\\mu. \\tag{2}\\]\n' +
      '\n' +
      '피팅 단계에서는 오프셋 \\(\\Delta\\mu\\)에만 역연산을 적용하여 보다 세립화된 3차원 자산을 표현할 수 있도록 하고, 더 나은 구조를 형성할 수 있도록 제한한다. 고정된 가우시안 수로 인해 우리는 치밀화 제어를 위한 원래 전략을 직접 적용할 수 없다. 대신, 우리는 효과적으로 가지치기와 복제를 위한 후보 풀 전략을 제안한다.\n' +
      '\n' +
      '2.2 후보 풀 전략\n' +
      '\n' +
      '우리는 고정된 수의 가우시안들에 적합하지 않은 가우시안들의 수의 동역학적 변화로 인해, 3차원 가우시안들을 이동시키기 위해 원래의 인터리브 최적화/밀도 전략을 직접적으로 적용할 수 없다. 가우시안들은 격자점이 할당된 양형사이기 때문에 가우시안들을 자유롭게 치밀화하거나 가지치기하는 것은 허용되지 않는다. 이 문제를 회피하기 위한 순진한 방법은 기울기 역전파에 의존하는 오프셋 \\(\\Delta\\mu\\)만을 조정하는 것이다. 불행히도, 우리는 가우스 중심들의 이동 범위가 가지치기 및 치밀화 전략 없이 크게 제한된다는 것을 실험적으로 발견했으며, 이는 렌더링 품질을 낮추고 기하학을 부정확하게 만든다(그림 참조). 7(a)). 이러한 이유로, 우리는 고정된 수의 가우시안들을 치밀화하고 다듬기 위한 새로운 전략(알고리즘 1)을 제안한다. 우리의 전략의 핵심은 나중에 치밀화를 위해 가지치기된 점을 후보 풀(P\\)에 저장하는 것이다.\n' +
      '\n' +
      '우리는 처음에 가우스 중심, \\(\\mu\\)을 할당된 위치, \\(p\\) 및 세트 오프셋 \\(\\Delta\\mu=0\\)(알고리즘 1의 라인 2)과 정렬한다. 후보 풀(P\\)은 빈 집합(Line 3)으로 초기화된다. 이 풀에는 최적화 중에 가지치기된 점을 나타내는 "비활성화" 점이 저장됩니다. 이 점은 전후진 과정에 포함되지 않습니다. 우리는 \\(T\\) 반복을 위해 각 자산을 최적화한다. 원래의 3차원 가우시안 스플래팅[18]에 이어서, 뷰-공간 위치 구배에 대한 임계치 \\(\\tau_{p},\\tau_{d}\\)는 모든 고정 반복(_IsRefinementIteration(t)_ line 8)에서 프루닝 및 치밀화를 위해 사용된다.\n' +
      '\n' +
      '정련 과정에서 가우시안 점(G_{p}\\)이 미리 정의된 임계값(\\tau_{p}\\)으로 프루닝되면 후보 풀(P\\)에 추가되어 렌더링 및 최적화를 위해 "비활성화"된다(라인 9-10). 치밀화를 위해 먼저 "활성화된" 가우시안 집합에서 치밀화되어야 하는 점\\(G_{d}\\)을 결정한다. 그리고 새롭게 추가된 점들 \\(G_{new}\\)은 몇 가지 기준을 통해 후보 풀 \\(P\\)에서 선택된다. (예를 들어, 우리는 밀집된 점들 \\(G_{d}\\)에 일정 거리 이내의 가장 가까운 점을 사용한다.) 추가된 점에 대한 해당 좌표 간격띄우기가 계산됩니다. 그들은 전진 및 후진 프로세스 모두에 대해 "활성화"되고 후속적으로 후보 풀(P\\)(라인 11-13)에서 제거된다. 최적화/밀도의 끝에서, 후보 풀의 모든 점들은 추가 정제(라인 15-17)를 위한 최적화에 재도입된다.\n' +
      '\n' +
      '이 전략은 가우시안 포인트들이 최적화 프로세스 동안 적응적으로 이동할 수 있게 하여, 보다 복잡한 객체 형상들을 표현할 수 있게 한다. 동시에, 결과적인 구조화된 체적 형태는 물리적 의미를 유지하여 적응성과 잘 정의된 구조 사이의 균형을 보여준다.\n' +
      '\n' +
      '###### 3.1.3 훈련손실\n' +
      '\n' +
      '감독을 위한 최종 손실은 정규화 손실을 추가하는 3D 가우시안 스플래팅에서 사용된 원래의 손실이다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{offsets}=Mean(\\text{ReLU}(|\\Delta\\mu-\\epsilon_{offsets}|)), \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(\\epsilon_{offsets}\\)은 하이퍼-파라미터이고, 3D 가우시안들의 중심을 그들의 대응하는 그리드 포인트들로부터 너무 멀지 않게 제한하기 위해,\n' +
      '\n' +
      '\\[\\mathcal{L}_{fitting}=\\lambda_{1}\\mathcal{L}_{1}+\\lambda_{2}\\mathcal{L}_{SSIM}+\\lambda_{3}\\mathcal{L}_{offsets}\\tag{4}\\text\n' +
      '\n' +
      '트레이닝 후, 포인트들은 생성 단계에 대한 트레이닝 데이터로서 볼륨 좌표에 따라 공간 순서로 정렬된다. 훈련이 끝나면, 각각의 가우시안 볼륨은 다소 가볍기 때문에 타겟 객체들의 2D 이미지들은 초고속으로 렌더링될 수 있다. 더 많은 구현 세부 사항은 첨부 자료에서 찾을 수 있다.\n' +
      '\n' +
      '### Stage2 : Text-to-3D Generation\n' +
      '\n' +
      '작업[40]에서 논의된 바와 같이, 3D 볼륨 공간은 고차원이며, 이는 확산 모델을 훈련하는 데 잠재적으로 문제를 제기한다. 경험적으로 볼 때, 우리는 3D 가우시안들을 체적 구조로 형성하지만, 많은 양의 데이터로 훈련할 때 모델 수렴에 여전히 너무 오래 걸린다는 것을 발견했다. 또한, 단일 확산 모델을 통해 다양한 범주에서 가우시안 볼륨의 데이터 분포를 학습하면 단일 단계 재구성으로 붕괴되어 생성 다양성이 저하된다. 이러한 문제를 해결하기 위해 먼저 가우시안 거리 필드(Gaussian Distance Field, GDF)를 생성하고 가우시안 볼륨의 속성을 예측하는 가우시안-대-파인 파이프라인을 소개한다. 첫 번째 단계에서는 GDF를 생성하기 위해 입력 텍스트에 대한 확산 모델을 채택한다. 두 번째 단계에서는 텍스트 조건과 함께 GDF를 입력하여 가우시안들의 속성을 예측하기 위해 3D U-Net 기반 모델을 사용하여 최종 가우시안 볼륨으로 이어진다.\n' +
      '\n' +
      '######4.0.1 가우스 거리장 생성\n' +
      '\n' +
      '가우시안 거리장(GDF) \\(F\\in\\mathbb{R}_{0}^{+1\\times N\\times N}\\)은 가우시안 볼륨의 기본 기하학을 나타내는 등방성 특징을 저장한다. 각 그리드 좌표와 가장 가까운 가우시안 점 사이의 거리를 측정하는데, 이는 부호 없는 거리 필드의 정의와 유사하다. 이 속성은 정렬 알고리즘을 통해 피팅된 가우시안 볼륨에서 쉽게 추출할 수 있다. 지상진리 GDF를 구한 후, 텍스트 기반의 확산 모델을 학습하여 GDF를 생성하고 객체의 거친 기하학을 생성한다. 모델은 Ground truth GDF와 생성된 GDF 사이의 평균제곱오차(Mean Square Error, MSE) 손실\\(\\mathcal{L}_{3D}\\)을 최소화함으로써 감독되며, 이는 확산 과정에서 추가된 잡음을 예측하는 것과 동일하다. 확산 모델을 이용하여 물체 형상에 따른 생성 다양성을 소개한다.\n' +
      '\n' +
      '###### 4.0.2 가우스 볼륨 예측\n' +
      '\n' +
      'GDF를 생성한 후, 가우시안 볼륨의 모든 속성을 예측하기 위해 텍스트 설명과 함께 SDFusion [9]에서 수정된 U-Net 기반 모델로 전송한다. 재구성 모델이 채택된 이유는 단일 단계 모델이 확산 모델과 유사한 가우시안 속성 예측을 생성한다는 것을 경험적으로 발견했기 때문이다.\n' +
      '\n' +
      '이 단계에서는 지상진리 가우시안 볼륨과 예측된 가우시안 볼륨 사이의 MSE 손실\\(\\mathcal{L}_{3D}\\)과 렌더링 손실\\(\\mathcal{L}_{2D}\\)의 두 가지 손실을 사용한다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{2D}=\\lambda\\mathcal{L}_{1}+(1-\\lambda)\\mathcal{L}_{SSIM} \\tag{5}\\]\n' +
      '\n' +
      'L1 및 SSIM 손실로 구성된 렌더링된 이미지에 대해. 상기 총 손실은,\n' +
      '\n' +
      '\\[\\mathcal{L}=\\lambda_{3D}\\mathcal{L}_{3D}+\\lambda_{2D}\\mathcal{L}_{2D} \\tag{6}\\]\n' +
      '\n' +
      '멀티모달 로스를 사용하면 글로벌 시맨틱과 로컬 디테일의 균형을 맞추고, 트레이닝 프로세스를 보다 안정적으로 유지할 수 있다. 모델 아키텍처 및 데이터 처리에 대한 더 많은 구현 세부 정보는 첨부 자료에서 찾을 수 있다.\n' +
      '\n' +
      '도 3: **GaussianVolume Fitting의 시각화. 렌더링 결과는 가우시안 볼륨의 우수한 복원 성능을 보여준다.**\n' +
      '\n' +
      '상추, 토마토, 그리고 다양한 토플을 곁들인 햄버거.\n' +
      '\n' +
      '나무 뚜껑이 있는 투명한 유리 병.\n' +
      '\n' +
      '손잡이가 달린 녹슨 청동 발톱\n' +
      '\n' +
      '작은 금속 램프\n' +
      '\n' +
      '노란색과 검은색 지게차.\n' +
      '\n' +
      '노란색과 검은색 지게차.\n' +
      '\n' +
      '노란색과 검은색 지게차.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '###기준방법 및 데이터세트\n' +
      '\n' +
      '######4.1.1 기준방법\n' +
      '\n' +
      '우리는 피드포워드 기반 방법과 최적화 기반 방법 모두와 비교한다. 피드포워드 범주에서 우리는 Shap-E[17]와 VolumeDiffusion[40]을 평가하며, 둘 다 3D 자산을 직접 생성한다. 최적화 기반 방법의 경우, 우리는 드림가우스[39]를 기준선으로 간주하며, 여기서 거친 3D 가우시안들은 SDS 손실[33]과 함께 최적화를 거친 다음 추가 개선을 위해 메쉬로 변환된다. 볼륨 확산의 경우 사전 훈련된 텍스트-이미지 모델을 통해 SDS를 사용한 사후 최적화 없이 피드-포워드 프로세스에서 생성된 결과를 보고하여 다른 방법과의 공정한 비교를 보장한다.\n' +
      '\n' +
      '#### 4.1.2 Dataset\n' +
      '\n' +
      '우리의 훈련 데이터 세트는 1,156개의 범주에 \\(\\sim\\) 46,000개의 3D 모델을 포함하는 Objaverse-LVIS 데이터 세트[11]로 구성된다. 교육을 위한 텍스트 프롬프트의 경우,\n' +
      '\n' +
      '그림 4: **최첨단 텍스트 대 3D 방법과의 비교.** 본 방법은 텍스트 조건과 더 나은 정렬로 경쟁력 있는 시각적 결과를 달성한다.\n' +
      '\n' +
      '우리는 캡3D[26]의 캡션을 사용하는데, 캡션은 BLIP-2[20]을 사용하여 객체의 멀티뷰 이미지를 캡션하고 GPT-4[1]을 통해 단일 캡션으로 통합한다. 평가를 위해 100개의 자산을 생성하고 각 자산에 대해 8개의 뷰를 렌더링합니다. 이러한 뷰들에 대한 카메라 포즈들은 객체 주위에 균일하게 샘플링된다. 우리의 방법은 시각화와 의미 정렬 모두에서 탁월하다.\n' +
      '\n' +
      '### 정성적, 정량적 결과\n' +
      '\n' +
      '######4.2.1 가우스 볼륨 피팅\n' +
      '\n' +
      '그림 3에서 가우시안 볼륨 피팅의 복원 결과를 제시한다. 이러한 결과는 제안된 가우시안 볼륨이 볼륨 분해능 \\(N=32\\) 하에서 적은 수의 가우시안 포인트로 고품질의 3D 자산을 표현할 수 있음을 보여준다. 더 높은 해상도는 더 나은 렌더링 효과를 산출하지만 생성 단계를 위해 더 많은 계산 자원을 필요로 한다. 가우시안 볼륨 피팅에 대한 추가 절제 연구는 Sec. 4.3 및 첨부 자료에서 찾을 수 있다.\n' +
      '\n' +
      '그림 5: GVGEN에 의한 Text-to-3D 생성 결과.\n' +
      '\n' +
      '######4.2.3 문자-3D 생성\n' +
      '\n' +
      '그림 1의 방법과 기존 기준선에 대한 시각적 비교 및 정량적 분석을 제공한다. 4 및 탭. 각각 1. 그림에 묘사된 것처럼, 우리의 모델은 합리적인 기하학과 그럴듯한 질감을 생성한다. Shap-E[17]은 거친 형상을 생성할 수 있지만, 때때로 입력 텍스트들에 대해 오정렬된 외관들을 초래한다. 볼륨 확산[40]은 비현실적인 질감을 생성하는 경향이 있다. 그리고 최적화 기반 접근법을 채택하는 DreamGaussian[39]는 항상 과포화 결과를 생성한다. 정량적 결과를 위해 렌더링된 이미지와 해당 텍스트 간의 CLIP 점수 및 이러한 방법의 생성 시간을 비교한다. 객체의 수가 가장 많은 40개의 LVIS 범주에서 평가를 위해 300개의 텍스트 프롬프트를 무작위로 선택한다. 각 객체에 대해 균일하게 샘플링된 카메라 포즈에서 8개의 뷰를 렌더링한다. 생성 시간을 측정하기 위해 확산 기반 방법의 샘플링 단계를 100으로 설정했다. DreamGaussian[39]의 구성은 기본 설정을 따른다. Fig.를 참조한다. 5 더 많은 생성 결과를 제공합니다.\n' +
      '\n' +
      '4.2.4 세대 다이버시티\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 6의 (a)를 참조하면, GVGEN은 동일한 프롬프트를 조건으로 다양한 자산을 생성할 수 있다. 우리의 방법의 생성적 다양성은 재구성 접근법과 다를 뿐만 아니라 사용자의 상상력을 높인다. 또한 CLIP 이미지 임베딩을 조건으로 한 이미지 대 3D 모델을 개발하고 그 결과를 최근 인기 있는 단일 이미지 재구성 모델 TGS[49] 및 LRM[14]와 비교한다(그림 참조). 6(b)). 근접-소싱된 LRM에 대한 액세스가 없기 때문에, 우리는 그것의 재구현, 즉 OpenLRM을 사용한다[13]. 단일 뷰 재구성 모델은 평균 패턴의 문제로 인해 보이지 않는 지역에서 믿을 수 없는 모양과 질감으로 이어지는 반면 GVGEN은 합리적인 외관을 생성한다.\n' +
      '\n' +
      '도 6: **생성 다양성의 표시.**(a)는 GVGEN을 통해 동일한 텍스트 프롬프트에 따라 다양한 생성 자산을 표시한다. (b)는 이미지 조건 GVGEN, OpenLRM [13] 및 TGS [49] 간의 비교를 보여준다. 단일 뷰 재구성 모델은 평균 패턴의 문제로 인해 보이지 않는 영역에서 믿을 수 없는 모양과 질감이 발생하는 반면 GVGEN은 합리적인 모양과 기하학을 생성한다.\n' +
      '\n' +
      '및 지오메트리를 포함할 수 있다. 이러한 비교는 GVGEN과 재구성 방법 사이의 중요한 차이를 강조한다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '######4.3.1 가우스 볼륨 피팅\n' +
      '\n' +
      '먼저 시각적 비교를 통해 가우시안 볼륨에 맞는 전략의 효과를 연구한다. 이러한 절제 실험에서, 3D 자산은 72개의 이미지로 훈련되며, 카메라 포즈는 세계 중심으로부터 \\(2.4\\)의 거리에 균일하게 분포되고, 24개의 이미지는 \\(1.6\\)의 거리에 균일하게 포즈된다. 부피 분해능(N=32\\), 즉 가우시안 포인트 수(N^{3}=32,768\\)를 설정하였다. 데이터 준비 단계에서 사용되는 풀 방법은 가우시안(32,768\\)만을 사용하더라도 렌더링 결과 면에서 가장 우수한 성능을 보인다. "w/o CPS"는 역전파를 통해서만 좌표 오프셋 \\(\\Delta\\mu\\)을 최적화하는 것을 의미하고, "w/o 오프셋"은 점 위치를 최적화하지 않고 가우시안 좌표 \\(\\mu=p\\)을 고정하는 것을 의미한다. 도. 도 7의 (a)는 풀 메소드를 사용하는 것이 최상의 렌더링 결과를 생성하는 것을 도시한다. 탭 2는 평균 PSNR, SSIM 및 LPIPS 점수를 보고하여 지원\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c} \\hline \\hline Metrics & CLIP score \\(\\uparrow\\) Time \\(\\downarrow\\) \\\\ \\hline Ours & **28.53** & **7 sec** \\\\ Shap-E & 28.48 & 11 sec \\\\ VolumeDiffusion & 25.09 & 7 sec \\\\ DreamGaussian & 23.60 & \\(\\sim\\)3 min \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **CLIP 점수 및 추론 속도에 대한 기준 방법에 대한 정량적 비교.** 메트릭은 우리 방법의 우수한 성능을 보여준다.\n' +
      '\n' +
      '도 7: **절제 연구에 대한 정성적 결과.**(a)는 상이한 가우시안 볼륨 피팅 방법의 시각적 비교를 나타낸다. (b)는 가우시안 볼륨 예측 모델을 훈련시키기 위해 상이한 손실을 사용하는 결과를 나타낸다.\n' +
      '\n' +
      '양적으로 제안된 전략의 효과 보충 자료에서 더 많은 결과를 찾을 수 있다.\n' +
      '\n' +
      '######4.3.3 문자-3D 생성\n' +
      '\n' +
      '필요한 계산 자원으로 인해 절제 연구를 위해 전체 Objaverse-LVIS 데이터 세트에 대해 훈련된 모델로 설계의 각 부분을 평가하는 것은 비실용적이다. 따라서, 우리는 가우시안 볼륨 속성을 예측하는 단계에서 손실의 유효성을 검증하기 위해 Objaverse-LVIS의 작은 하위 집합에 대한 모델을 훈련한다. 구체적으로, 우리는 훈련을 위해 24개의 LVIS 범주에서 1,336개의 자산 테마 동물을 선택한다. 학습 데이터에서 데이터의 20%를 무작위로 선택하고 각 자산의 새로운 뷰를 사용하여 정량적 메트릭을 평가한다. 도. 도 7의 (b)는 제안된 다중모달 손실이 세밀한 세부사항과 함께 그럴듯한 결과를 생성한다는 것을 보여주고, 정량적 결과는 탭 2에 나열되어 있다. 두 가지 유형의 손실의 조합은 전체 형상을 매끄럽게 유지하면서 보다 상세한 텍스처로 이어진다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'GVGEN은 3D 객체를 생성하는데 고무적인 결과를 보여주었다. 그러나 그림 1과 같이 입력 텍스트가 학습 데이터 도메인에서 크게 다른 경우 그 성능이 제한된다. 12를 보충으로 한다. 훈련 데이터를 준비하기 위해 객체당 가우시안 볼륨을 맞추어야 하기 때문에, 더 나은 다양성을 위해 수백만 개의 객체 데이터를 확장하는 것은 시간이 많이 걸린다. 또한 볼륨 분해능 \\(N\\)을 32(3D Gaussians의 경우 점 수는 \\(N^{3}\\)=32,768)로 설정하여 계산 자원을 절약하여 매우 복잡한 텍스처를 가진 3D 자산의 렌더링 효과를 제한한다. 앞으로 더 도전적인 시나리오에서 더 높은 품질의 3D 자산을 생성하는 방법을 더 탐구할 것입니다.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      '결론적으로 본 논문에서는 텍스트를 기반으로 한 명시적 3D 가우시안(Gaussians)의 피드포워드 생성을 탐구한다. 우리는 복잡한 3차원 가우시안 점들을 가우시안 볼륨이라고 하는 구조화된 볼륨 형태로 혁신적으로 조직하여 거친-미세 생성 파이프라인을 통해 3차원 가우시안들의 피드-포워드 생성을 가능하게 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c} \\hline \\hline Metrics & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) \\\\ \\hline Full & **30.122** & **0.963** & **0.038** \\\\ w/o CPS & 29.677 & 0.958 & 0.049 \\\\ w/o offsets & 27.140 & 0.936 & 0.084 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\begin{tabular}{c|c c c} \\hline \\hline Metrics & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) \\\\ \\hline Full & 35.03 & **0.9872** & **0.0236** \\\\ w/o \\(\\mathcal{L}_{3D}\\) & **35.21** & 0.9846 & 0.0268 \\\\ w/o \\(\\mathcal{L}_{2D}\\) & 29.55 & 0.9654 & 0.0444 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **Ablation Studies를 위한 정량적 메트릭.** 좌측 테이블은 가우시안 볼륨 피팅 전략을 분석하는 반면, 우측 테이블은 가우시안 볼륨 속성 예측 모델을 트레이닝하는 상이한 손실을 비교한다. 정성적 비교는 그림 7을 참조하십시오.\n' +
      '\n' +
      '본 논문에서는 고품질 가우시안 볼륨의 트레이닝을 용이하게 하기 위해 새로운 가지치기 및 치밀화 전략, 즉 후보 풀 전략을 제안한다. 제안된 프레임워크인 GVGEN은 텍스트로부터 3D 가우시안들을 생성하는 데 놀라운 효율성을 보여준다. 실험 결과는 정성적 및 정량적 측면에서 GVGEN의 경쟁 능력을 보여준다. 이 진전은 현장 내에서 광범위한 범위의 문제를 해결하는 데 있어 접근법의 잠재적인 확장을 시사한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]Achiam, J., Adler, S., Agarwal, L., Ahmad, I., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 기술 보고서. arXiv preprint arXiv:2303.08774 (2023)\n' +
      '* [2] Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srinivasan, P.P.: Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5855-5864 (2021)\n' +
      '* [3] Cao, A., Johnson, J.: Hexplane: A fast representation for dynamic scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 130-141 (2023)\n' +
      '* [4] Chang, J.H.R., Chen, W.Y., Ranjan, A., Yi, K.M., Tuzel, O.: Pointersect: Neural rendering with cloud-ray intersection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8359-8369 (2023)\n' +
      '* [5] Chen, A., Xu, Z., Geiger, A., Yu, J., Su, H.: Tensorf: Tensorial radiance fields. In: European Conference on Computer Vision. pp. 333-350. Springer (2022)\n' +
      '* [6] Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873 (2023)\n' +
      '* [7] Chen, Y., Chen, Z., Zhang, C., Wang, F., Yang, X., Wang, Y., Cai, Z., Yang, L., Liu, H., Lin, G.: Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. arXiv preprint arXiv:2311.14521 (2023)\n' +
      '* [8] Chen, Z., Wang, F., Liu, H.: Text-to-3d using gaussian splatting. arXiv preprint arXiv:2309.16585 (2023)\n' +
      '* [9] Cheng, Y.C., Lee, H.Y., Tulyakov, S., Schwing, A.G., Gui, L.Y.: Sdfusion: Multimodal 3d shape completion, reconstruction, and generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4456-4465 (2023)\n' +
      '* [10] Deitke, M., Liu, R., Wallingford, M., Ngo, H., Michel, O., Kusupati, A., Fan, A., Laforte, C., Voleti, V., Gadre, S.Y., et al.: Objaverse-xl: A universe of 10m+ 3d objects. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [11] Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3d objects. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13142-13153 (2023)\n' +
      '* [12] Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. Advances in neural information processing systems **34**, 8780-8794 (2021)\n' +
      '* [13] He, Z., Wang, T.: Openlrm: Open-source large reconstruction models. [https://github.com/3DTopia/OpenLRM](https://github.com/3DTopia/OpenLRM) (2023)* [14] Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400 (2023)\n' +
      '* [15] Huang, Z., Wen, H., Dong, J., Wang, Y., Li, Y., Chen, X., Cao, Y.P., Liang, D., Qiao, Y., Dai, B., et al.: Epidiff: Enhancing multi-view synthesis via localized epipolar-constrained diffusion. arXiv preprint arXiv:2312.06725 (2023)\n' +
      '* [16] Jain, A., Mildenhall, B., Barron, J.T., Abbeel, P., Poole, B.: Zero-shot text-guided object generation with dream fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 867-876 (2022)\n' +
      '* [17] Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463 (2023)\n' +
      '* [18] Kerbl, B., Kopanas, G., Leimkuhler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics **42**(4) (2023)\n' +
      '* [19] Li, J., Tan, H., Zhang, K., Xu, Z., Luan, F., Xu, Y., Hong, Y., Sunkavalli, K., Shakhnarovich, G., Bi, S.: Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214 (2023)\n' +
      '* [20] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)\n' +
      '* [21] Li, W., Chen, R., Chen, X., Tan, P.: Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. arXiv preprint arXiv:2310.02596 (2023)\n' +
      '* [22] Liang, Y., Yang, X., Lin, J., Li, H., Xu, X., Chen, Y.: Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching (2023)\n' +
      '* [23] Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: Syncdreamer: Generating multiview-consistent images from a single-view image. arXiv preprint arXiv:2309.03453 (2023)\n' +
      '* [24] Liu, Z., Li, Y., Lin, Y., Yu, X., Peng, S., Cao, Y.P., Qi, X., Huang, X., Liang, D., Ouyang, W.: Unidream: Unifying diffusion priors for relightable text-to-3d generation. arXiv preprint arXiv:2312.08754 (2023)\n' +
      '* [25] Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H., Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008 (2023)\n' +
      '* [26] Luo, T., Rockwell, C., Lee, H., Johnson, J.: Scalable 3d captioning with pretrained models. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [27] Melas-Kyriazi, L., Rupprecht, C., Vedaldi, A.: Pc2: Projection-conditioned point cloud diffusion for single-image 3d reconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12923-12932 (2023)\n' +
      '* [28] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM **65**(1), 99-106 (2021)\n' +
      '* [29] Mohammad Khalid, N., Xie, T., Belilovsky, E., Popa, T.: Clip-mesh: Generating textured meshes from text using pretrained image-text models. In: SIGGRAPH Asia 2022 conference papers. pp. 1-8 (2022)\n' +
      '* [30] Muller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG) **41**(4), 1-15 (2022)\n' +
      '* [31] Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751 (2022)* [32] Ntavelis, E., Siarohin, A., Olszewski, K., Wang, C., Gool, L.V., Tulyakov, S.: Autodecoding latent 3d diffusion models. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [33] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022)\n' +
      '* [34] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [35] Ren, J., Pan, L., Tang, J., Zhang, C., Cao, A., Zeng, G., Liu, Z.: Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142 (2023)\n' +
      '* [36] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [37] Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.: Zero123++: a single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110 (2023)\n' +
      '* [38] Shi, Y., Wang, P., Ye, J., Long, M., Li, K., Yang, X.: Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512 (2023)\n' +
      '* [39] Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)\n' +
      '* [40] Tang, Z., Gu, S., Wang, C., Zhang, T., Bao, J., Chen, D., Guo, B.: Volumediffusion: Flexible text-to-3d generation with efficient volumetric encoder. arXiv preprint arXiv:2312.11459 (2023)\n' +
      '* [41] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [42] Wu, G., Yi, T., Fang, J., Xie, L., Zhang, X., Wei, W., Liu, W., Tian, Q., Wang, X.: 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528 (2023)\n' +
      '* [43] Wu, Z., Wang, Y., Feng, M., Xie, H., Mian, A.: Sketch and text guided diffusion model for colored point cloud generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 8929-8939 (2023)\n' +
      '* [44] Xu, D., Yuan, Y., Mardani, M., Liu, S., Song, J., Wang, Z., Vahdat, A.: Agg: Amortized generative 3d gaussians for single image to 3d. arXiv preprint arXiv:2401.04099 (2024)\n' +
      '* [45] Xu, Q., Xu, Z., Philip, J., Bi, S., Shu, Z., Sunkavalli, K., Neumann, U.: Point-nerf: Point-based neural radiance fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5438-5448 (2022)\n' +
      '* [46] Yi, T., Fang, J., Wu, G., Xie, L., Zhang, X., Liu, W., Tian, Q., Wang, X.: Gausiandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. arXiv preprint arXiv:2310.08529 (2023)\n' +
      '* [47] Yin, Y., Xu, D., Wang, Z., Zhao, Y., Wei, Y.: 4dgen: Grounded 4d content generation with spatial-temporal consistency. arXiv preprint arXiv:2312.17225 (2023)\n' +
      '* [48] Yu, X., Guo, Y.C., Li, Y., Liang, D., Zhang, S.H., Qi, X.: Text-to-3d with classifier score distillation. arXiv preprint arXiv:2310.19415 (2023)\n' +
      '* [49] Zou, Z.X., Yu, Z., Guo, Y.C., Li, Y., Liang, D., Cao, Y.P., Zhang, S.H.: Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers. arXiv preprint arXiv:2312.09147 (2023)\n' +
      '\n' +
      '##6 구현 세부사항\n' +
      '\n' +
      '### Stage 1 : 가우시안 볼륨 피팅\n' +
      '\n' +
      '이 단계에서는 96개의 균일하게 렌더링된 이미지를 사용하여 각 객체를 정확하게 피팅하여 다양한 포즈를 캡처한다. 초기 72개의 이미지는 2.4 유닛의 카메라 대 세계 중심 거리 주위에 균일하게 분포된 카메라 포즈로 렌더링된다. 나머지 24개의 이미지는 보다 상세한 뷰를 제공하기 위해 1.6 유닛의 더 가까운 거리에서 렌더링된다. 부피 분해능은 \\(N=32\\)으로, 구면 고조파(SH) 차수는 0으로 설정하여 특징 채널 번호 \\(C=14\\)을 갖는 각 가우시안 점을 얻었다.\n' +
      '\n' +
      '피팅 볼륨은 전역 좌표계 내에서 중심을 이루는 측면 길이가 1 세계 단위인 정육면체로 가정한다. 이 큐브 내부에는 가우스 점들이 균일하게 분포하여 볼륨의 포괄적인 커버리지를 보장한다. 처음에, 각 가우시안 점 \\(\\Delta\\mu\\)에 대한 오프셋은 0으로 설정된다. 각 인스턴스를 최적화하기 위해 20,000번의 반복에 대해 흰색 배경에 대한 훈련을 수행한다. 반복 500에서 15,000까지 고밀도화 전략이 사용되며, 후속 작업은 모델의 밀도를 점진적으로 향상시키기 위해 100회 반복마다 실행된다. 이 밀도화 단계 이후, 우리는 일관성을 유지하고 이상치를 방지하기 위해 100회 반복마다 피쳐를 미리 정의된 범위로 주기적으로 클리핑한다. 훈련 과정에서 각 가우시안 포인트의 불투명도를 재설정하는 것을 삼간다. 이러한 결정은 모델의 학습 및 적응 단계에 불안정성을 도입하는 것을 피하기 위해 이루어진다.\n' +
      '\n' +
      '\\(\\mathcal{L}_{fitting}\\)에서 가중치 선택을 위해 \\(\\lambda_{1}=0.8,\\lambda_{2}=0.2,\\lambda_{3}=20.0\\)을 설정하고 오프셋 임계값을 1.5 복셀 거리로 설정한다. 수학적으로, 이것은 \\(\\epsilon_{offsets}=\\frac{1.5}{32-1}\\)으로 표현되며, 정의된 체적 공간에서 인접한 격자점 사이의 계산된 거리를 나타낸다. 다른 파라미터들의 경우, 우리는 3D 가우시안 스플래팅(3D Gaussian Splatting)에서의 디폴트 구성들을 고수한다[18].\n' +
      '\n' +
      '### Stage 2 : Text-to-3D Generation\n' +
      '\n' +
      '1 데이터 전처리\n' +
      '\n' +
      '생성 단계에서는 먼저 가우시안 볼륨 데이터를 전처리하여 학습 과정의 안정성과 수렴성을 향상시킨다.\n' +
      '\n' +
      '먼저, 각 가우시안들의 스케일링 벡터 \\(s\\in\\mathbb{R}^{3}\\)에 순서 제약을 가하여 공분산 행렬의 분해를 표준화하여 오름차순으로 정렬되도록 한다. 이 조직은 고유 벡터의 직교성을 유지하면서 결과 공분산 행렬을 수정하지 않습니다. 또한 회전벡터 \\(q\\in\\mathbb{R}^{4}\\)는 스케일링에 의해 도입된 변환의 순서와 일치하도록 조정된다. 두 번째로 구면 좌표를 이용하여 4원수를 표현한다. 이 변환은 회전 벡터\\(q\\)를 3차원 표현으로 줄여 각 가우시안 매개변수의 고유성과 일관성을 향상시킨다. 이러한 방식으로 4원수를 정규화하여 동일한 모양이 동일한 매개변수로 표시되도록 한다. 마지막으로 각 특징 채널의 평균과 분산을 기반으로 각 특징 채널을 개별적으로 정규화한다. 이 정규화 프로세스는 다른 채널에 걸쳐 스케일을 평형을 유지합니다.\n' +
      '\n' +
      '######6.2.3 모델 구조 및 훈련\n' +
      '\n' +
      'GDF(Gaussian Density Fields)의 생성 단계에서는 [9]에서 수정된 3D U-Net을 이용한다. 텍스트 조건은 CLIP ViT-L/14 [34] 텍스트 인코더로 추출된 77\\(\\times\\) 768개의 임베딩이며, 교차 주의 메커니즘을 사용하여 3D U-Net에 주입된다. 추가적인 MLP 층이 인터-모달 갭을 브릿지하기 위해 사용된다. 예측 모델의 구조는 입력 채널 번호의 수정과 타임스텝 블록의 생략을 포함하는 조정과 함께 3D U-Net을 반영한다.\n' +
      '\n' +
      'GDF를 예측하는 생성 모델을 훈련할 때, 우리는 감독으로 MSE 손실\\(\\mathcal{L}_{3D}\\)만을 사용한다. 예측 모델을 학습하기 위해 전체 과정을 통해 렌더링 손실(\\mathcal{L}_{2D}\\)에 대해 \\(\\lambda=0.2\\)을 설정하였다. 그 다음, 전체 손실 \\(\\mathcal{L}=\\lambda_{3D}\\mathcal{L}_{3D}+\\lambda_{2D}\\mathcal{L}_{2D}\\)에 대해, 첫 100시대에 \\(\\lambda_{3D}=1.0,\\lambda_{2D}=0\\)을 설정하여 3차원 일관성에만 초점을 맞춘 강건한 초기화를 제공한다. 그 후, 가중치를 \\(\\lambda_{3D}=0.2\\) 및 \\(\\lambda_{2D}=0.8\\)으로 조정하여 초점을 2D 렌더링의 최적화 및 추가 정제 쪽으로 이동시킨다.\n' +
      '\n' +
      '##7 탐사 실험\n' +
      '\n' +
      '초기 탐색에서, 우리는 다양한 생성 모델의 효율성을 조사하고, 처음에 포인트 클라우드 확산 모델 및 원시 3D U-Net 기반 확산 모델의 세 가지 무조건 모델을 훈련시키고, 이를 전체 모델과 비교했다. 우리는 특히 의자 LVIS 범주에 속하는 8개의 적합 가우스 볼륨을 훈련 데이터로 활용했으며 4,000개의 에폭에 대해 각 모델을 훈련했다. 정성적 결과는 그림 8에 나와 있다.\n' +
      '\n' +
      '우리의 초기 시도는 3D 가우시안 생성을 용이하게 하기 위해 이전 작업[27]에서 영감을 얻은 포인트 클라우드 확산 모델을 적용하는 것을 포함했다. 불행히도, 이 접근법은 수렴 문제로 고전하여 만족스럽지 못한 결과를 낳았다. 3D 가우시안들의 미묘한 구조를 포착하고 생성하는 데 포인트 클라우드 확산 모델의 고유한 한계가 분명해져 대안 전략을 모색하게 되었다. 원시 1단계 확산 모델을 사용하면 기본 3D 구조를 생성할 수 있다. 그러나 이 모델은 주로 특징적인 거친 출력을 생성했다.\n' +
      '\n' +
      '그림 8: 탐색 실험에서 생성된 자산의 시각적 결과.\n' +
      '\n' +
      '그럴듯한 3D 기하학을 달성하기 위한 더 미묘한 생성 기술의 필요성 조대-미세 생성 파이프라인을 통합한 전체 모델은 3D 가우시안 생성에서 상당한 개선을 보여주었다. 이 접근법은 생성 프로세스를 단순화할 뿐만 아니라 모델이 보다 정확하고 사실적인 3D 기하학을 가진 인스턴스를 생성할 수 있는 권한을 부여했다. 생성된 출력을 순차적으로 정제함으로써, coarse-to-fine 파이프라인은 초기 모델에서 관찰된 한계를 효과적으로 해결하여 복잡한 3D 구조를 생성하는 데 있어 우수성을 보여준다.\n' +
      '\n' +
      '## 8 Application\n' +
      '\n' +
      '우리의 방법의 우수성을 입증하기 위해, 우리는 추가 개선을 위해 GSGEN[8]과 같은 최적화 기반 방법과 통합하는 GVGEN의 능력을 보여준다(도 9 참조). 상기 생성된 가우시안Volume6을 초기화로서 사용하는 단계는\n' +
      '\n' +
      '도 9: **GSGEN[8] Results Initialized with Our Method and Point-E[31].** 왼쪽 열은 서로 다른 방법으로 초기화된 렌더링 결과를 나타내고, 오른쪽 열은 GSGEN으로 최적화한 후의 렌더링 결과를 나타낸다.\n' +
      '\n' +
      '점-E의 점 구름을 랜덤 색상으로 대체합니다. 이러한 이동은 GSGEN이 3D 가우시안들을 추가로 최적화할 수 있게 하여 텍스처 및 기하학 모두에서 텍스트 설명들과 더 나은 정렬을 달성한다. 이러한 향상은 GVGEN에 의해 생성된 특징이 최적화 프로세스에 더 호환되고 유익하기 때문에 포인트-E의 색상 속성이 GSGEN의 활용에 미치는 부정적인 영향을 피하는 것에서 비롯된다.\n' +
      '\n' +
      '##9 가우스 볼륨 피팅 단계에 대한 추가 절제 연구\n' +
      '\n' +
      '### 오프셋 임계값 \\(\\epsilon_{offsets}\\)의 효과\n' +
      '\n' +
      '탭에서 도 3 및 도 3을 참조하여 설명한다. 도 10을 참조하면, 가우시안 볼륨 피팅 시 오프셋 임계값 \\(\\epsilon_{offsets}\\)의 변화에 따른 효과를 제시한다. "0-dist"라는 용어는 오프셋 용어의 부재를 나타내는 반면, "\\(\\infty\\)-dist"는 오프셋 정규화의 생략을 나타낸다. 우리의 관찰은 최소의 정규화가 렌더링 성능을 향상시킨다는 것을 보여준다. 그러나, 이것은 볼륨 내의 가우시안 포인트들이 더 비구조화되고 그리드 포인트들로부터 이탈하는 대가를 치르게 된다. 이러한 정규화가 없으면 오프셋 항이 지나치게 유연해져 어려움을 겪는다.\n' +
      '\n' +
      '도 10: **가우시안 센터의 렌더링된 이미지 및 위치의 시각적 결과. 적합 자산은 서로 다른 오프셋 임계값 \\(\\epsilon_{offets}\\)으로 최적화된다.**\n' +
      '\n' +
      '네트워크가 효과적으로 학습하도록 합니다. 유연성과 잘 정의된 구조를 유지하기 위해 1.5 복셀 거리에 해당하는 \\(\\epsilon_{offsets}=\\frac{1.5}{32-1}\\)을 최적의 임계값으로 선택했다.\n' +
      '\n' +
      '가우시안 볼륨 해상도의### 효과\n' +
      '\n' +
      '우리는 탭에서 렌더링 결과와 정량적 메트릭의 비교를 제시한다. 도 4 및 도 4를 참조하여 설명한다. 11, Objaverse 데이터셋의 "Chunky 기사" 자산을 활용한다. 가우시안 볼륨의 해상도\\(N\\)-res의 증가는 더 많은 수의 가우시안 포인트인 \\(N^{3}\\)과 상관관계가 있다. 다양한 볼륨 해상도를 수용하기 위해 초기화 및 오프셋 임계값, 특히 \\(\\epsilon_{offets}=\\frac{1.5}{N-1}\\)을 조정한다. "3DGS"라는 용어는 원래의 3D 가우시안 스플랫팅[18]을 사용하여 객체를 피팅하는 프로세스를 나타내며, 반복 설정은 우리에 매칭되고 다른 매개변수는 기본값으로 설정된다. 분석은 더 많은 수의 가우시안 포인트가 메모리 사용 증가를 희생하더라도 동일한 설정에서 피팅 품질을 향상시킨다는 것을 보여준다. 특히, 더 적은 가우시안 포인트들을 사용할 때에도, 본 방법은 원래의 3DGS 접근법에 필적하는 시각적 품질을 제공하고, 구조화된 표현에서 더 높은 PSNR 값을 달성한다. 이것은 고품질의 메모리 효율적인 표현을 재구성하는 데 있어 우리의 방법의 효율성과 효율성을 강조한다.\n' +
      '\n' +
      '## 10 실패 사례\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 12를 참조하면, 모델에 의해 생성된 객체들 중 일부는 흐릿한 텍스처와 부정확한 지오메트리를 겪는다. 이 현상은 대체로 약 46,000개로 구성된 훈련 데이터 세트의 상대적으로 작은 크기에서 비롯된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c} \\hline \\hline Metrics & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) & Num of Gaussians & Time(s) \\\\ \\hline\n' +
      '16-res & 23.079 & 0.887 & 0.122 & 4,096 & 169\\\\\n' +
      '32-res & **30.437** & 0.966 & 0.046 & 32,768 & 179\\\\\n' +
      '64-res & 30.395 & **0.972** & 0.032 & 262,144 & 195\\\\\n' +
      '3DGS & 29.789 & 0.969 & **0.030** & 175,532 & 191 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **가우시안 볼륨 해상도의 다른 설정에 대한 정량적 결과**.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Metrics & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) \\\\ \\hline\n' +
      '0-dist & 18.189 & 0.859 & 0.176 \\\\\n' +
      '1.5-dist & **30.437** & 0.966 & 0.046\\\\\n' +
      '3-dist & 30.395 & 0.969 & 0.038 \\\\ \\(\\infty\\)-dist & 30.007 & **0.969** & **0.034** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 상이한 오프셋 임계값 선택에 대한 **정량적 결과**.\n' +
      '\n' +
      '인스턴스 이러한 데이터 세트 크기는 광범위한 텍스트 입력에 대한 응답으로 다양한 출력을 생성하는 모델의 용량을 제한한다. 향후, 우리의 작업은 모델 아키텍처 개선과 데이터 품질 향상이라는 두 가지 주요 방법에 초점을 맞출 것이다. 이러한 측면을 해결함으로써 대규모 시나리오에서 적용하기 위한 모델을 확장하는 것을 목표로 하며, 이는 세대 다양성을 개선하고 더 나은 렌더링 결과를 가져올 것으로 예상된다.\n' +
      '\n' +
      '##11 추가 질적 결과\n' +
      '\n' +
      '그림 1에서 더 많은 시각적 결과를 제공한다. 도 13 및 도 13을 참조하여 설명한다. 14.\n' +
      '\n' +
      '그림 11: 다른 가우시안 볼륨 해상도 설정과 원본 3DGS 간의 **정성적 비교**.\n' +
      '\n' +
      '도 12: **실패 사례들**\n' +
      '\n' +
      '그림 13: 보다 질적인 결과.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:25]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>