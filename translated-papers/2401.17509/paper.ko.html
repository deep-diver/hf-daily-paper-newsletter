<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Any Scene에서의 Anything: Photorealistic Video Object 삽입\n' +
      '\n' +
      '천바이, 제만 샤오, 구옥상 장, 디 량, 제 양, 주루이 장, 유젠 궈\n' +
      '\n' +
      '청장중, 이차오치우, 전동왕, 이첸관, 샤오인정, 타오왕, 청루\n' +
      '\n' +
      'XPeng Motors\n' +
      '\n' +
      '{chenbai, zemans, guoxiangz, liangd2, yangj23, zhangzr, guoyj4, chengzhangz, yiqiaoq\n' +
      '\n' +
      'zhendongw, guanyc, xiaoyinz, taow, luc}@xiaopeng.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '사실적인 비디오 시뮬레이션은 가상 현실부터 영화 제작까지 다양한 응용 분야에서 상당한 잠재력을 보여주었다. 이것은 실제 환경에서 비디오를 캡처하는 것이 비실용적이거나 비용이 많이 드는 시나리오에 특히 해당된다. 비디오 시뮬레이션에서의 기존의 접근법들은 종종 조명 환경을 정확하게 모델링하거나, 물체 기하학을 표현하거나, 높은 수준의 실사성을 달성하지 못한다. 본 논문에서는 물리적 사실성에 중점을 둔 기존의 동적 비디오에 임의의 객체를 끊김없이 삽입할 수 있는 실감형 비디오 시뮬레이션을 위한 새롭고 일반적인 프레임워크인 Anything in Any Scene을 제안한다. 제안된 일반 프레임워크는 3가지 핵심 프로세스를 포함한다: 1) 기하학적 사실성을 보장하기 위해 적절한 배치와 함께 주어진 장면 비디오에 사실적인 객체를 통합하는 단계; 2) 하늘과 환경 조명 분포를 추정하고 조명 사실성을 향상시키기 위해 사실적인 그림자를 시뮬레이션하는 단계; 3) 실사성을 최대화하기 위해 최종 비디오 출력을 개선하는 스타일 전달 네트워크를 사용하는 단계. 애니 씬 프레임워크의 Anything이 위대한 기하학적 사실주의, 조명 사실주의, 그리고 사진 현실주의의 시뮬레이션된 비디오를 생성한다는 것을 실험적으로 증명한다. 비디오 데이터 생성과 관련된 문제를 상당히 완화함으로써, 본 프레임워크는 고품질 비디오를 획득하기 위한 효율적이고 비용 효율적인 솔루션을 제공합니다. 또한, 그 애플리케이션은 비디오 데이터 증강을 훨씬 넘어 확장되어, 가상 현실, 비디오 편집 및 다양한 다른 비디오 중심 애플리케이션에서 유망한 잠재력을 보여준다. 프로젝트 코드 및 고해상도 비디오 결과에 대한 액세스는 프로젝트 웹사이트 [https://anythinginanyscene.github.io](https://anythinginanyscene.github.io)를 확인하십시오.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '이미지와 비디오 시뮬레이션은 가상 현실부터 영화 제작까지 다양한 응용 분야에서 성공을 거두었다. 사실적인 이미지와 비디오 시뮬레이션을 통해 다양하고 고품질의 시각적 콘텐츠를 생성할 수 있는 능력은 이러한 분야를 발전시켜 새로운 가능성과 응용 분야를 도입할 수 있는 잠재력을 가지고 있다. 실제 환경에서 캡처된 이미지와 비디오는 진정성에 매우 중요하지만 롱테일 배포의 한계로 인해 어려움을 겪는 경우가 많다. 이는 공통 시나리오가 과도하게 표현되는 반면 드물지만 중요한 상황은 과소 표현되어 유통 외 문제로 알려져 있는 문제를 제시한다. 비디오 수집 및 편집을 통해 이러한 한계를 해결하는 전통적인 방법은 모든 가능한 상황을 포괄하는 고유한 어려움으로 인해 비실용적이거나 과도하게 비용이 많이 드는 것으로 판명된다. 비디오 시뮬레이션의 중요성은 특히 기존 비디오와 새로 삽입된 객체의 통합을 통해 이러한 문제를 극복하는 데 가장 중요하다. 비디오 시뮬레이션은 대규모, 다양하고 사실적인 시각적 콘텐츠를 생성함으로써 가상 현실, 비디오 편집 및 비디오 데이터 증강에서 애플리케이션의 향상에 기여한다.\n' +
      '\n' +
      '그러나, 물리적 사실성을 고려하여 사실적인 시뮬레이션 비디오를 생성하는 것은 여전히 어려운 개방형 문제이다. 기존의 방법들은 종종 특정 설정들, 특히 실내 환경들[9, 26, 45, 46, 57]에 집중함으로써 한계를 나타낸다. 이러한 방법은 다양한 조명 조건과 빠르게 움직이는 물체를 포함하여 실외 장면의 복잡성을 적절하게 해결하지 못할 수 있다. 3D 모델 등록에 의존하는 방법들은 제한된 클래스들의 객체들[12, 32, 40, 42]만을 통합하는데 제약된다. 많은 접근법들은 조명 환경 모델링, 적절한 물체 배치, 그리고 실사 성취와 같은 필수적인 요소들을 무시한다[12, 36]. 결과적으로, 이러한 제한들은 자율 주행 및 로봇 공학과 같이 확장성이 높고 기하학적으로 일관적이며 사실적인 장면 비디오 시뮬레이션이 필요한 분야에서 그들의 응용을 상당히 제한한다.\n' +
      '\n' +
      '본 논문에서는 이러한 문제점을 해결하기 위한 실사 영상 객체 삽입을 위한 Anything in Any Scene의 종합적인 프레임워크를 제안한다. 프레임워크는 보편적인 적용 가능성을 갖도록 설계되었으며 실내 및 실외 장면 모두에 적응 가능하여 기하학적 사실성, 조명 사실성 및 사실성 측면에서 물리적 정확성을 보장한다. 우리의 목표는 기계 학습에서 시각적 데이터 증강에 유리할 뿐만 아니라 가상 현실 및 비디오 편집과 같은 다양한 비디오 애플리케이션에 적응할 수 있는 비디오 시뮬레이션을 만드는 것이다.\n' +
      '\n' +
      '애니 씬 프레임워크의 Anything 개요는 그림 2에 나와 있으며, 섹션 3에서 장면 비디오와 객체 메쉬의 다양한 자산 은행을 구축하기 위한 새롭고 확장 가능한 파이프라인을 자세히 설명하고 설명 키워드를 사용하여 시각적 쿼리로부터 관련 비디오 클립을 효율적으로 검색할 수 있도록 설계된 시각적 데이터 쿼리 엔진을 소개한다. 다음으로, 기존의 3D 자산과 다시점 영상 재구성을 활용하여 3D 메쉬를 생성하는 두 가지 방법을 제시한다. 이는 매우 불규칙적이거나 의미적으로 약하더라도, 임의의 원하는 물체의 삽입을 제한 없이 허용한다. 섹션 4에서는 물리적 사실성을 유지하는 데 중점을 두고 객체를 동적 장면 비디오에 통합하는 접근 방식을 자세히 설명한다. 섹션 4.1에 설명된 객체 배치 및 안정화 방법을 설계하여 삽입된 객체가 연속 비디오 프레임에 걸쳐 안정적으로 고정되도록 한다. 실감적인 조명과 그림자 효과를 생성하는 문제를 해결하기 위해, 우리는 섹션 4.2에 설명된 바와 같이 렌더링 과정에서 하늘과 환경 조명을 추정하고 사실적인 그림자를 생성한다. 결과로 생성된 시뮬레이션 비디오 프레임은 노이즈 레벨, 색상 충실도 및 선명도의 이미징 품질 불일치와 같은 실제 캡처 비디오와 다른 비현실적인 아티팩트를 필연적으로 포함한다. 우리는 4.3절에서 실사를 향상시키기 위해 스타일 전달 네트워크를 채택한다.\n' +
      '\n' +
      '제안된 프레임워크에서 제작된 시뮬레이션 비디오는 5.3절과 같이 조명 사실성, 기하학적 사실성, 실사성이 높은 수준에 도달하여 다른 비디오보다 질적으로나 양적으로 우수한 성능을 보였으며, 5.4절에서는 실제적 가치를 검증하기 위해 시뮬레이션 비디오를 훈련 인지 알고리즘에 적용하였다. 애니 씬(Anything in Any Scene) 프레임워크는 시간 효율성과 사실적인 시각적 품질로 데이터 증강을 위한 대규모 저비용 비디오 데이터 세트를 생성할 수 있으며, 이는 비디오 데이터 생성의 부담을 완화하고 잠재적으로 롱테일 배포 및 배포 이탈 문제를 개선한다. 일반적인 프레임워크 설계로 Anything in Any Scene 프레임워크는 개선된 3D 메쉬 재구성 방법과 같은 개선된 모델과 새로운 모듈을 쉽게 통합할 수 있어 비디오 시뮬레이션 성능을 더욱 향상시킬 수 있다.\n' +
      '\n' +
      '우리의 주요 기여는 다음과 같이 요약할 수 있다.\n' +
      '\n' +
      '1. 비디오 시뮬레이션을 위한 임의의 장면 프레임워크에서 임의의 객체를 임의의 동적 장면 비디오에 통합할 수 있는 새롭고 확장 가능한 임의의 것을 소개한다.\n' +
      '2. 이 프레임워크는 비디오 시뮬레이션에서 기하학적 사실성, 조명 사실성 및 실사성을 보존하는 데 독특하게 중점을 두어 고품질 및 사실적인 출력을 보장한다.\n' +
      '3. 사실적인 비디오 시뮬레이션을 생성하는 프레임워크의 능력을 입증하는 광범위한 검증을 수행하여 이 분야의 범위와 잠재적 적용을 크게 확장했다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**이미지 합성 및 편집**: 이미지 인페인팅에서 스타일 이전까지의 작업을 포함하는 것은 학술 및 산업 커뮤니티 모두에서 상당한 관심을 끌었다. 전통적인 방법은 대부분 픽셀, 패치 및 낮은 수준의 이미지 특징을 기반으로 하며 종종 높은 수준의 의미 정보가 부족하다. 구체적으로, 이미지 인페인팅 방법은 이미지 복구를 위해 픽셀 또는 패치를 복제한다[2, 3, 10, 19, 27]. 비모수 기반 텍스처 합성 방법들은 주어진 소스 텍스처의 픽셀들을 재샘플링하여 실사 텍스처들을 생성한다[13, 28]. 이미지 유추[21]와 같은 스타일 전달 방법은 패치를 사용하여 예제 기반 스타일화를 수행합니다.\n' +
      '\n' +
      '딥 러닝 네트워크, 특히 생성적 적대적 네트워크(GAN) [17]은 컴퓨터 비전 및 이미지 처리 작업에서 상당한 능력을 입증하여 이미지 생성에서 인상적인 성공을 달성했다. MGAN[30], SGAN[25], PSGAN[4]과 같은 다양한 GAN은 작업에 현저한 숙련도를 보였다.\n' +
      '\n' +
      '도 1: 이미지가 물리적 사실성이 결여되도록 하는 잘못된 조명 환경 추정, 잘못된 객체 배치 위치 및 비현실적인 텍스처 스타일을 갖는 시뮬레이션된 비디오 프레임의 예\n' +
      '\n' +
      '를 포함하는 것을 특징으로 하는 텍스쳐 합성 방법. 또한, GAN은 문맥 이미지 인페인팅(contextual image inpainting) 및 다중 스케일 이미지 완성(multi-scale image completion; 59)에 성공적으로 적용되었다. pix2pix[24] 및 cycleGAN[62]는 GAN 아키텍처를 활용하여 스타일 전송을 위한 생성 모델을 훈련한다. GAN에 의해 생성된 이미지는 흐릿하지 않고 더 높은 사실성을 나타내는 경향이 있어 훈련 이미지 데이터의 분포와 밀접하게 정렬된다.\n' +
      '\n' +
      '**비디오 합성 및 편집**: 이미지에서 비디오 합성으로의 전환은 특히 시간적 일관성을 유지하는 추가적인 과제를 해결하는 것을 필요로 한다.\n' +
      '\n' +
      '[44], [52], [53]과 같은 무조건적인 비디오 합성 방법은 랜덤 노이즈를 입력으로 하고 공간적 및 시간적 상관 관계를 모두 모델링하여 비디오를 생성한다. 그러나, 이들은 종종 출력 비디오 시퀀스들에서 제약된 모션 패턴들을 초래한다. 대조적으로, 조건부 비디오 합성 방법들은 입력 콘텐츠에 기초하여 비디오 생성을 위한 생성 모델을 트레이닝하기 위해 조건부 GAN[37]을 채용한다. [55] 및 그 다음의 작업 [56]에서, 생성 네트워크는 각각의 후속 프레임 생성에 대해 소스 비디오의 이전 프레임에 컨디셔닝된다. [34] 이전에 생성된 모든 프레임을 고려하여 이 접근법을 추가로 수행하여 비디오 합성에서 향상된 장기 시간 일관성을 달성한다.\n' +
      '\n' +
      '추가적으로, [29] 및 [23]에서 제안된 비디오 자동 합성 방법들은 공간 및 시간 정보를 이용하여 객체의 비디오를 다른 비디오에 삽입한다. 최근, [7]에서 제안된 GeoSim 프레임워크는 덜 일반적인 객체 및 다양한 유형의 장면 비디오에 대한 적용이 제한적이지만 주어진 실제 주행 장면 비디오에 자동차 삽입에서 인상적인 결과를 달성했다. 우리의 작업은 이 격차를 해소하여 모든 장면 비디오에서 모든 객체 삽입 가능성을 확장하고자 한다.\n' +
      '\n' +
      '##3 장면 비디오 및 객체 메쉬 자산 은행\n' +
      '\n' +
      '애니 씬 프레임워크의 목표는 동적 장면 비디오와 관심 객체의 구성을 통해 대규모 및 고품질 시뮬레이션 비디오를 생성하는 것이다. 이를 달성하기 위해, 장면 비디오들 및 객체 메시들 둘 모두의 자산 뱅크가 시뮬레이트 비디오 합성을 위해 요구된다.\n' +
      '\n' +
      '대규모 비디오 자산은행으로부터 구도를 위한 대상 비디오를 효율적으로 찾기 위해, 주어진 시각 단서 기술자를 기반으로 시뮬레이션된 비디오 구도를 위한 관련 장면 비디오 클립을 검색하는 시각 데이터 쿼리 엔진을 제안하였다. 대상 객체의 메쉬 모델은 기존 비디오 클립에 삽입되기 전에 필요하다. 기존 3D 에셋의 Houdini Engine과 다시점 영상으로부터 NeRF 기반의 3D 재구성을 이용하여 대상 객체의 3D 메쉬 생성을 도입하였으며, 이를 통해 기존의 장면 영상에 무한한 클래스의 객체를 삽입할 수 있다.\n' +
      '\n' +
      '메쉬 자산 은행에 대한 자세한 설명은 보충 자료에서 확인할 수 있습니다.\n' +
      '\n' +
      '##4 사실적인 비디오 시뮬레이션\n' +
      '\n' +
      '기하적 사실주의, 조명 사실주의, 실사주의로 비디오 시뮬레이션을 달성하기 위해 제안된 프레임워크는 다음과 같은 세 가지 주요 구성 요소로 구성된다.\n' +
      '\n' +
      '1. 객체 배치 및 안정화(제4.1절)\n' +
      '2. 조명 및 그림자 생성(제4.2절)\n' +
      '3. 사실적 스타일 전달(4.3절)\n' +
      '\n' +
      '### 객체 배치 및 안정화\n' +
      '\n' +
      '비디오 합성을 위해 배경 비디오에 객체를 삽입하는 것은 비디오 시퀀스 내의 각각의 프레임에 대해 결정된 객체 배치 위치를 필요로 한다. 본 논문에서는 4.1.1절에 기술된 장면 내 다른 기존 객체와의 폐색을 고려한 새로운 객체 배치 방법을 설계 및 제안하였다.\n' +
      '\n' +
      '그러나, 각각의 단일 프레임으로부터 독립적으로 추정되는 배치 위치들은 비디오 시간 정보가 고려되지 않았기 때문에 비현실적인 이동 트랙들을 산출할 수 있다. 이 문제를 해결하기 위해 섹션 4.1.2에서 각 프레임에서의 배치 위치를 보정하는 객체 배치 안정화 방법을 제안한다. 삽입된 객체가 연속 비디오 프레임에 걸쳐 사실적으로 동작하도록 하기 위해 연속 프레임 사이의 광학 흐름 추적을 사용한다.\n' +
      '\n' +
      '그림 2: 실사 영상 객체 삽입을 위한 Any Scene 프레임워크에서 제안된 Anything 개요\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '실외 장면 및 실내 장면에 대한 환경에 대해, 렌더링 프로세스 동안 삽입된 객체의 시각적 외관에 영향을 미친다.\n' +
      '\n' +
      '렌더링 과정에서 정확한 조명과 음영 효과를 시뮬레이션하기 위해 먼저 4.2.1절에서 HDR(High Dynamic Range) 파노라마 영상 재구성 방법을 도입하였고, 마지막으로 4.2.2절에서 주 조명원의 위치를 추정하여 삽입된 물체의 그림자를 렌더링하였다.\n' +
      '\n' +
      '######4.2.1 HDR 파노라마 영상 재구성\n' +
      '\n' +
      '소비자 카메라에 의해 캡처된 저 다이나믹 레인지(Low Dynamic Range; LDR) 이미지는 일반적으로 주변 환경 조명에 비해 주 조명의 극도로 높은 밝기로 인해 과포화 상태이며, 이는 주 조명의 위치 및 휘도 분포를 추정하는 것을 훨씬 더 어렵게 한다. 이 문제를 해결하기 위해 먼저 이미지 인페인팅 네트워크를 사용하여 렌더링을 위한 조명 분포의 서라운드 뷰를 추론한다. 그런 다음 스카이 HDR 재구성 네트워크를 적용하여 조명 소스 위치를 식별하고 HDR 파노라마 이미지를 생성한다.\n' +
      '\n' +
      '**파노라마 이미지 인페인팅**: 소비자 카메라에 의해 캡처된 이미지는 제한된 FOV(Field of View)를 가지며, 이는 렌더링 프로세스에서 누락된 조명으로 이어진다. 우리는 이 작업을 제한된 FOV 이미지에서 파노라마 이미지를 추론하는 인페인팅 작업으로 변환함으로써 해결한다. 또한, 확산 모델[22, 50]을 이용하여 어라운드 뷰 영상을 추론하는 것을 목표로 한다. 본 논문에서는 입력 조건에서의 반복적인 잡음 제거 과정을 통해 표준 가우시안 분포의 샘플을 데이터 분포의 샘플로 변환하는 조건부 확산 모델인 이미지 대 이미지 확산 모델을 사용할 것을 제안하였다. 본 논문에서는 기존의 모델[43]을 적용하여 입력 영상에 조건을 두어 파노라마 영상을 생성한다.\n' +
      '\n' +
      '**휘도 분포 추정**: [47]에서 제안한 HDR 영상 재구성 방법은 생성적 적대 네트워크(GAN: Generative Adversarial Network)를 활용하여 태양 및 하늘 휘도 분포를 모델링하는 인코더-디코더 네트워크를 학습시킨다. 입력은 단일 실외 LDR 파노라마 영상과 ResNet[20]을 백본으로 하는 U-Net[41] 아키텍처 네트워크를 사용하여 하늘 영역 휘도 분포\\(L_{sky}\\)를 추정한다.\n' +
      '\n' +
      '다른 수정된 VGG16 네트워크[49]는 태양을 포함하는 입력 LDR 파노라마 영상에서 픽셀에서의 확률을 나타내는 태양 위치 확률 맵 \\(x_{i,j}\\)을 추정하기 위해 사용된다. CNN 블록으로부터의 출력 특징 맵들은 VGG가 표현된 디랙 델타 함수인 태양 복사도 맵을 인코딩하기 위한 컨볼루션 레이어들에 공급되는 입력으로서 함께 연결된다:\n' +
      '\n' +
      '\\[\\delta(x_{i,j},\\tau,\\beta)=\\frac{\\tau}{\\beta\\sqrt{\\pi}exp(-\\frac{(1-x_{i,j})^{2}{\\beta})\\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(\\tau\\) 및 \\(\\beta\\)는 하늘의 투과율과 선명도 값이다. 그런 다음 태양 복사 맵을 태양 영역과 병합하여 태양 영역 휘도 분포\\(L_{sun}\\)를 생성한다. 역 톤 매핑 연산에 \\(L_{sun}\\)과 \\(L_{sky}\\)을 적용하여 최종 출력 HDR 맵(L\\)을 생성한다.\n' +
      '\n' +
      '이 방법을 조명 추정 모듈에 적용하고 GAN을 사용하여 HDR 맵(L\\) 생성을 위한 네트워크를 재훈련하는 [47]과 동일한 프로세스를 따른다. 그리고 삽입된 비디오 프레임에 \\(L\\)을 적용하였다.\n' +
      '\n' +
      '**환경 HDR 이미지 재구성:** 실외 시나리오의 경우, 주요 조명인 태양만이 삽입된 물체의 시각적 외관에 영향을 줄 수 있는 것은 아니며, 보다 사실적인 렌더링 결과를 얻기 위해 난반사로 인한 환경 조명도 고려해야 한다. 환경적인 HDR 영상을 재구성하기 위해 장면의 여러 측면 LDR 영상을 수집하고 기존 모델을 사용하여 HDR 영상으로 복원하여 연속 노출값 표현[6]을 학습한다. 각 사이드뷰 영상에 대한 카메라 외재 파라미터를 추정하여 하나의 HDR 파노라마 영상으로 스티치하기 위해 동일한 과정을 따랐다(그림 5와 같은 환경 HDR 영상의 예). 따라서, 다중 측면 영상으로부터 추정된 환경광 분포를 구하고, 이를 삽입된 객체 렌더링 과정에 적용할 수 있다.\n' +
      '\n' +
      '도 4: 원래 하늘 이미지, 재구성된 HDR 이미지 및 그와 관련된 태양 조명 분포도의 예\n' +
      '\n' +
      '도 5: 원본 및 재구성된 HDR 환경 파노라마 영상의 예\n' +
      '\n' +
      '######4.2.2 객체 그림자 생성\n' +
      '\n' +
      '본 논문에서는 실외 장면의 경우 태양, 실내 장면의 경우 빛 등 주요 조명원의 위치와 분포를 추정하였기 때문에 보다 높은 성능과 효율적인 컴퓨팅 자원 사용을 제공하는 3D 그래픽 응용 Vulkan[54]을 이용하여 삽입된 물체의 그림자를 렌더링하였다. 또한, 실제 렌더링의 성능을 향상시키기 위해 Vulkan 응용 프로그램에 광선 추적을 통합하였다[16]. 삽입된 물체에 대해 생성된 그림자의 예는 그림 6에 나와 있다.\n' +
      '\n' +
      'Photorealalistic Style Transfer\n' +
      '\n' +
      '시뮬레이션된 비디오는 실세계 시나리오에서 캡처된 비디오에 포함되지 않는 일관성 없는 조명 및 컬러 밸런싱과 같은 비현실적인 아티팩트를 필연적으로 포함한다. 이 문제를 해결하기 위해 우리는 시뮬레이션된 비디오 시퀀스의 실사성을 향상시키기 위해 스타일을 충실하게 전달하는 이미지 인페인팅 네트워크를 사용할 것을 제안했다.\n' +
      '\n' +
      '구체적으로, 우리는 원래 이미지에서 누락된 영역을 인페인팅하도록 지정된 [61]에서 제안된 거친-대-미세 메커니즘을 적용한다. [61]에서 coarse network와 refinement network를 사용하였으며, 두 가지 모두 dilated convolution layer로 구성되어 입력 영상을 기반으로 정제된 영상을 생성하였다. 두 네트워크에 대한 입력 구성을 수정했습니다. 코어스 네트워크는 전경 영역에 채워진 블랙 픽셀을 갖는 이미지, 전경 영역을 나타내는 바이너리 마스크, 및 배경 영역에 채워진 블랙 픽셀을 갖는 삽입된 객체의 전경 이미지를 촬영한다. 정제된 네트워크는 거친 네트워크로부터의 출력과 함께 거친 네트워크와 동일한 입력을 취하고, 최종 정제된 이미지 결과를 생성한다.\n' +
      '\n' +
      '생성 모델을 훈련하기 위해 WGAN[1] 손실을 사용하는 [61]에서 제안한 것과 동일한 훈련 전략을 채택하며, 그 목적 함수는 다음과 같이 나타낼 수 있다.\n' +
      '\n' +
      '\\mathbb{E}_{x\\sim\\\\mathccal{D}\\mathbb{P}_{x}}[D(x)] - \\mathbb{E}_{\\hat{x}\\sim\\mathbb{P}_{g}[D(\\tilde{x})] \\tag{4}\\\\mathbb{P}_{x\\sim\\mathccal{D}\\mathbb{P}_{x\\sim\\mathccal{D}\\mathbb{P}_{x\\sim\\mathccal{D}\\mathbb{P}_{x\\sim\\mathccal{D}\\mathbb{P}_{x\\sim\\mathccal{D}\\mathbb{P}_{x\\sim\\mathbb{P}_{x}}[D(\\min_{G}\\max_{D}}\\mathbb{P}_{x}}[D(x)]}\\mathbb{P}_{g}[D(\\tilde{\n' +
      '\n' +
      '여기서 \\(\\mathcal{D}\\)은 1-Lipschitz 함수의 집합이고, \\(\\mathbb{P}_{r}\\)은 데이터 분포이고 \\(\\mathbb{P}_{g}\\)은 \\(\\tilde{x}=G(z)\\)에 의해 암시적으로 정의된 모델 분포이고, \\(z\\)은 생성기에 대한 입력이다.\n' +
      '\n' +
      'WGAN을 개선하기 위해 [18]에서 제안한 그래디언트 페널티항을 추가하여 전경 영역의 픽셀에 적용하였다. 따라서 패널티 함수는 다음과 같이 표현될 수 있다:\n' +
      '\n' +
      'b{E}_{\\hat{x}\\sim\\\\mathbb{P}_{x}}(||\\triangledown_{\\hat{x}}D( \\hat{x})\\odot(1-m)||_{2}-1)^{2} \\tag{5}\\}\n' +
      '\n' +
      '여기서, 분포\\(\\mathbb{P}_{r}\\)와 \\(\\mathbb{P}_{g}\\)에서 샘플링된 점 사이의 직선으로부터 샘플링된 \\(\\hat{x}\\)은 전경 영역의 입력 이진 마스이다.\n' +
      '\n' +
      '## 5 실험 평가\n' +
      '\n' +
      '이 섹션에서는 비디오 시뮬레이션을 위해 제안된 방법의 평가 세부 사항을 설명한다. 우리는 성능을 정량화하기 위해 섹션 5.1에 평가 메트릭을 도입한다. 검증용 비디오 데이터 세트는 5.2절에 나열되어 있으며, 5.3절에서 본 프레임워크의 각 모듈의 효율성을 평가하기 위해 절제 분석을 수행하고, 마지막으로 5.4절에서 다운스트림 인식 작업에 프레임워크를 적용하는 것을 보여준다.\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      '제안된 프레임워크에 의해 생성된 시뮬레이션된 비디오의 품질을 평가하기 위해 [7]에서 사용된 다음 두 가지 평가 메트릭을 채택한다. 데이터 세트의 모든 비디오 프레임에 걸쳐 각 메트릭에 대한 평균값을 보고한다.\n' +
      '\n' +
      '**인간 점수:** 이 메트릭은 인간 A/B 테스트에서 기준선 방법의 결과보다 하나의 방법의 결과를 선호하는 참가자의 백분율을 측정한다. 인간 연구에 대한 자세한 설명은 보충 자료에서 찾을 수 있다. 또한, 본 연구에 사용된 비디오 쌍 및 GUI 애플리케이션의 전체 세트는 [https://anythinginanyscene.github.io](https://anythinginanyscene.github.io)에서 당사 웹사이트에서 사용할 수 있다. 우리는 동료 연구원들이 이러한 비디오 비교를 다운로드하고 검토하거나 결과를 확인하기 위해 자체 인간 연구를 수행할 것을 권장한다.\n' +
      '\n' +
      '**FID(Frechet Inception Distance)**: 이 메트릭은 생성된 이미지의 분포와 지상진실 이미지의 분포를 비교하여 생성된 이미지의 사실성과 다양성을 정량화한다. 낮은 점수는 더 큰 유사성을 나타내며, 0 점수는 동일한 이미지 세트를 의미한다.\n' +
      '\n' +
      '### Evaluation Data\n' +
      '\n' +
      '다양한 장면 비디오와 객체의 사실적인 비디오 구성을 위한 방법의 성능을 입증하기 위해, 실외 및 실내 장면 비디오 데이터 세트와 다양한 삽입 객체 항목을 사용하여 본 방법의 유효성을 검증한다.\n' +
      '\n' +
      '**야외 장면 비디오**: 팬더셋[58]은 낮의 다른 시간 및 날씨를 포함하는 다양한 조건에서 자율 주행 장면을 캡처하는 멀티모달 데이터셋이다. 이 데이터 세트의 모든 103개의 비디오 클립 중 95개를 활용했으며, 각각은 10Hz에서 샘플링된 8초의 프레임을 포함했다.\n' +
      '\n' +
      '**실내 장면 비디오**: ScanNet++[60]은 3D 스캐닝 실제 환경에 의해 생성된 실내 장면의 대규모 데이터세트이며, 상기 데이터세트는 DSLR 이미지, RGB-D 시퀀스, 및 시맨틱 및 인스턴스 주석을 포함하며,\n' +
      '\n' +
      '그림 6: 우리의 방법을 평가하기 위해 삽입된 객체 종합 리소스에 대한 생성된 그림자의 예. 보충 자료에서 실내 장면 비디오 데이터 세트에 대한 실험 결과를 제공한다.\n' +
      '\n' +
      '**Object Mesh Assets**: 섹션 3에 소개된 방법을 사용하여 서로 다른 유형의 차량 및 보행자 모델을 포함한 다양한 객체를 중심으로 3D 객체 메쉬를 생성했다.\n' +
      '\n' +
      '### Experimental Results\n' +
      '\n' +
      '다양한 스타일 전달 네트워크의 성능을 평가하기 위해 CNN 기반 방법 DoveNet[8], 트랜스포머 기반 방법 StyTR2[11], 확산 모델 기반 방법 PHDiffusion[33], 섹션 4.3에 소개된 방법 등 다양한 방법을 비교하였으며, 인간 연구를 위해 스타일 전달 모듈이 없는 프레임워크를 비교 기준으로 사용한다. 그 결과를 표 1에 요약하면, 우리의 전송 네트워크는 3.730으로 가장 낮은 FID와 61.11%로 가장 높은 인간 점수를 달성하여 대체 방법을 능가한다.\n' +
      '\n' +
      '**절제 연구**: 각 핵심 모듈의 효과를 조사하기 위해 절제 연구를 수행하고 성능을 평가했다. 우리는 배치(w/o 배치), HDR 이미지 재구성(w/o HDR), 그림자 생성(w/o 그림자) 및 스타일 전달(w/o 스타일 전달)의 한 가지 모듈을 한 번에 프레임워크에서 제거했다. 이 인간 연구에서 w/o 스타일 전달 방법이 기준선 역할을 했으며 다른 모든 절제 방법과 비교되었다. 결과는 표 2에 요약되어 있으며 배치, HDR 및 스타일 전달 모듈이 없는 경우 FID가 더 높았다. 특히 그림자를 추가하면 FID 점수에 비례적으로 반영되지 않았지만 인간 관찰자에 대한 인지된 사실성이 크게 향상되었다. 이러한 불일치는 이전 연구 [7]에서도 언급했듯이 지각 품질에 대한 계산 평가와 인간 판단 사이의 잠재적인 격차를 시사한다. 제안된 방법은 50% 이상의 인간 점수를 달성했으며 다른 방법은 50% 미만의 점수를 획득하여 제안된 프레임워크에서 각 모듈의 기여도를 강조했다.\n' +
      '\n' +
      '**정성적 비교**: 도 7에서, 우리는 실외 장면 데이터세트 팬더셋에 적용된 상이한 스타일 전송 네트워크를 사용하여 샘플 비디오 프레임들의 정성적 비교를 제공한다. 그림 6(a), 6(b) 및 6(c)는 각각 DoveNet, StyTR2 및 PHDiffusion에 의해 정제된 이미지를 보여준다. 이 이미지들에서 삽입된 객체는 장면의 조명 및 기상 조건과 일치하지 않는 색조를 나타낸다. 반대로 그림 6(d)와 같이 제안된 방법으로 정제된 이미지는 4가지 중 가장 좋은 시각적 품질을 보여주며, 우리의 방법이 FID 및 인간 연구 점수 모두에서 다른 방법을 능가함을 보여주는 표 1에 보고된 결과와 일치한다. 이는 개선된 스타일 전달 네트워크가 Any Scene 프레임워크에서 우리의 Anything 내에서 사실성을 크게 향상시킬 수 있음을 나타낸다.\n' +
      '\n' +
      '또한 Anything에 의해 생성된 비디오의 시각적 품질을 Any Scene 프레임워크에 적용하여 평가한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Method & Human Score(\\%) & FID \\\\ \\hline \\hline\n' +
      '**Proposed method** & **61.11** & **3.730** \\\\ \\hline StyTR2 style transfer & 58.89 & 4.091 \\\\ \\hline PHDiffusion style transfer & 47.22 & 4.554 \\\\ \\hline DoveNet style transfer & 47.78 & 3.999 \\\\ \\hline w/o style transfer & N/A & 4.499 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: Any Scene 프레임워크의 Anything에 플러그인된 다양한 스타일 전송 네트워크에 대한 실험 결과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Method & Human Score(\\%) & FID \\\\ \\hline \\hline\n' +
      '**Proposed method** & **61.11** & **3.730** \\\\ \\hline StyTR2 style transfer & 58.89 & 4.091 \\\\ \\hline PHDiffusion style transfer & 47.22 & 4.554 \\\\ \\hline DoveNet style transfer & 47.78 & 3.999 \\\\ \\hline w/o style transfer & N/A & 4.499 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: Any Scene 프레임워크의 Anything에 플러그인된 다양한 스타일 전송 네트워크에 대한 실험 결과.\n' +
      '\n' +
      '그림 7: 서로 다른 스타일 전송 네트워크를 사용하여 팬더셋 데이터세트로부터 시뮬레이션된 비디오 프레임의 정성적 비교.\n' +
      '\n' +
      '실외 장면 데이터세트 팬더셋을 참조로 사용하여 한 번에 하나의 모듈을 제거합니다. 이 평가는 그림 8의 두 개의 비교 샘플로 시각적으로 예시된다. 그림 (c)c 및 그림 (d)d에서, 삽입된 객체가 주변 환경 및 장면 내의 다른 객체들과 불일치하는 컬러 텍스처들을 나타내는 것을 관찰한다. 또한, 그림 (b)b는 삽입된 객체에 생성된 그림자가 없는 인스턴스를 강조한다. 이러한 부재는 물체가 공기 중에 있는 것처럼 나타나는 시각적 효과를 생성하여 사실적인 시뮬레이션을 위한 그림자 렌더링의 중요성을 강조한다. 대조적으로, 그림 (e)e는 우리의 프레임워크에 의해 생성된 비디오의 시각적 품질을 보여주는데, 삽입된 객체는 기하학, 조명 및 전반적인 광-실감 측면에서 장면과 높은 일관성을 나타낸다. 이는 다양한 장면 설정에 객체를 사실적으로 통합할 수 있는 Anything in Any Scene 프레임워크의 기능을 보여준다.\n' +
      '\n' +
      '다운스트림 인식 평가\n' +
      '\n' +
      '실제 데이터 세트는 종종 긴 꼬리 클래스 분포를 나타내며, 여기서 소수의 공통 클래스는 과대 대표되는 반면 대부분의 클래스는 과소 대표된다. 이러한 불균형은 딥러닝 모델에 상당한 문제를 제기하여 훈련 중 공통 클래스에 대한 편향을 초래하고 추론 중 희귀 클래스에 대한 성능을 악화시킨다.\n' +
      '\n' +
      '이 문제를 해결하기 위해, 우리는 데이터 증강을 위해 희귀한 경우를 포함하는 합성 이미지를 생성하기 위해 Anything in Any Scene 프레임워크의 사용을 조사한다. 우리는 1,500개의 실제 주행 장면과 30개 이상의 객체 범주를 포함하여 KITTI[15], nuScenes[5], ONCE[35] 데이터 세트의 이미지 데이터 집합인 CODA 데이터 집합 [31]에 대한 평가를 수행한다.\n' +
      '\n' +
      '이 작업의 목표는 CODA2022 검증 데이터 세트의 이미지에 9개의 다른 희귀 객체 범주를 삽입하는 것이며, 각 범주는 전체 경계 상자의 0.4% 미만을 포함한다. 데이터 세트에서 2930개의 이미지 하위 집합에 대해 YOLOX-S, YOLOX-L 및 YOLOX-X[14]의 세 가지 모델을 훈련하여 테스트를 위해 다른 977개의 이미지를 예약했다. 그런 다음 Any Scene 프레임워크에서 Anything을 사용하여 다양한 객체를 삽입하여 이러한 훈련 이미지를 증강했다. 이 프로세스는 훈련 데이터 세트에서 원래 이미지를 대체하는 증강된 훈련 이미지 세트를 생성했다. 동일한 훈련 전략을 적용하고 증강된 훈련 데이터 세트에서 모델을 다시 훈련했다.\n' +
      '\n' +
      '세 가지 모델의 성능을 평가하기 위해 원본 및 증강 데이터 세트 모두에서 훈련한 다음 동일한 테스트 데이터 세트에서 테스트한다. 표 3에 자세히 설명된 결과는 세 가지 모델 모두에 대한 평균 평균 정밀도(mAP)의 개선을 나타낸다. 구체적으로, YOLOX-S의 경우 mAP에서 3.7%, YOLOX-L의 경우 1.1%, YOLOX-X의 경우 2.6%의 향상이 있다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 연구에서는 현실감 있는 비디오 시뮬레이션을 위해 설계된 혁신적이고 확장 가능한 프레임워크인 Anything in Any Scene을 제안하였다. 제안된 프레임워크는 다양한 동적 비디오에 광범위한 객체를 원활하게 통합함으로써 기하학적 사실성, 조명 사실성 및 사실성의 보존을 보장한다. 광범위한 시연을 통해 비디오 데이터 수집 및 생성과 관련된 문제를 완화하여 다양한 시나리오에 적응할 수 있는 비용 효율적이고 시간 효율적인 솔루션을 제공하는 효과를 보여주었다. 아플리카\n' +
      '\n' +
      '그림 8: 다양한 렌더링 조건에서 팬더셋 데이터세트로부터 시뮬레이션된 비디오 프레임의 정성적 비교.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline Method & Data & mAP & \\\\ \\hline \\hline \\multirow{2}{*}{YOLOX-S} & Original & 0.186 & \\multirow{2}{*}{\\(0.037\\uparrow\\)} \\\\  & Original + Ours & **0.223** & \\\\ \\hline \\multirow{2}{*}{YOLOX-L} & Original & 0.260 & \\multirow{2}{*}{\\(0.011\\uparrow\\)} \\\\  & Original + Ours & **0.271** & \\\\ \\hline \\multirow{2}{*}{YOLOX-X} & Original & 0.249 & \\multirow{2}{*}{\\(0.026\\uparrow\\)} \\\\  & Original + Ours & **0.275** & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: Any Scene 프레임워크에서 우리의 Anything을 사용하여 원본 이미지와 증강 이미지의 조합에 대해 훈련될 때의 성능과 비교하여 CODA 데이터 세트에서 원본 이미지에 대해 훈련된 YOLOX 모델의 성능. 우리는 9개의 모든 객체 범주에 대한 평균을 나타내는 mAP를 보고한다.\n' +
      '\n' +
      '본 프레임워크의 채택은 특히 객체 검출에서 긴 꼬리 분포 문제를 해결하는 데 있어 다운스트림 인식 작업에서 눈에 띄는 개선을 보여주었다. 본 프레임워크의 유연성은 각 모듈에 대해 개선된 모델을 쉽게 통합할 수 있도록 하며, 본 프레임워크는 현실적 비디오 시뮬레이션 분야에서 향후 탐색 및 혁신을 위한 강력한 토대가 된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan, 2017.\n' +
      '* [2] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. Patchmatch: A randomized correspondence algorithm for structural image editing. _ACM Trans. Graph._, 28(3):24, 2009.\n' +
      '* [3] Connelly Barnes, Fang-Lue Zhang, Liming Lou, Xian Wu, and Shi-Min Hu. Patchtable: Efficient patch queries for large datasets and applications. _ACM Transactions on Graphics (ToG)_, 34(4):1-10, 2015.\n' +
      '* [4] Urs Bergmann, Nikolay Jetchev, and Roland Vollgraf. Learning texture manifolds with the periodic spatial gan. _arXiv preprint arXiv:1705.06566_, 2017.\n' +
      '* [5] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11621-11631, 2020.\n' +
      '* [6] Su-Kai Chen, Hung-Lin Yen, Yu-Lun Liu, Min-Hung Chen, Hou-Ning Hu, Wen-Hsiao Peng, and Yen-Yu Lin. Learning continuous exposure value representations for single-image hdr reconstruction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 12990-13000, 2023.\n' +
      '* [7] Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, and Raquel Urtasun. Geosim: Realistic video simulation via geometry-aware composition for self-driving. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 7230-7240, 2021.\n' +
      '* [8] Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li, and Liding Zhang. Dovenet: Deep image harmonization via domain verification. In _CVPR_, 2020.\n' +
      '* [9] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. 2016.\n' +
      '* [10] Antonio Criminisi, Patrick Perez, and Kentaro Toyama. Object removal by exemplar-based inpainting. In _2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings._, pages II-II. IEEE, 2003.\n' +
      '* [11] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and Changsheng Xu. Stytr2: Image style transfer with transformers. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11326-11336, 2022.\n' +
      '* [12] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In _Conference on robot learning_, pages 1-16. PMLR, 2017.\n' +
      '* [13] Alexei A Efros and Thomas K Leung. Texture synthesis by non-parametric sampling. In _Proceedings of the seventh IEEE international conference on computer vision_, pages 1033-1038. IEEE, 1999.\n' +
      '* [14] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in 2021. _arXiv preprint arXiv:2107.08430_, 2021.\n' +
      '* [15] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2012.\n' +
      '* [16] GitHub. Ray tracing examples and tutorials, 2023. [https://github.com/nvpro-samples/vk_raytracing_tutorial_KHR/tree/master](https://github.com/nvpro-samples/vk_raytracing_tutorial_KHR/tree/master) [Accessed: (October 16, 2023)].\n' +
      '* [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.\n' +
      '* [18] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [19] James Hays and Alexei A Efros. Scene completion using millions of photographs. _ACM Transactions on Graphics (ToG)_, 26(3):4-es, 2007.\n' +
      '* [20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.\n' +
      '* [21] Aaron Hertzmann, Charles E Jacobs, Nuria Oliver, Brian Curless, and David H Salesin. Image analogies. In _Seminal Graphics Papers: Pushing the Boundaries, Volume 2_, pages 557-570. 2023.\n' +
      '* [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [23] Hao-Zhi Huang, Sen-Zhe Xu, Jun-Xiong Cai, Wei Liu, and Shi-Min Hu. Temporally coherent video harmonization using adversarial networks. _IEEE Transactions on Image Processing_, 29:214-224, 2019.\n' +
      '* [24] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. _CVPR_, 2017.\n' +
      '* [25] Nikolay Jetchev, Urs Bergmann, and Roland Vollgraf. Texture synthesis with spatial generative adversarial networks. _arXiv preprint arXiv:1611.08207_, 2016.\n' +
      '* [26] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. _arXiv preprint arXiv:1712.05474_, 2017.\n' +
      '**[27] 니코 코모다키스와 조지오스 치리타스. 우선순위 스케줄링과 동적 프루닝을 통한 효율적인 믿음 전파를 이용한 영상 완성 IEEE Transactions on Image Processing_, 16(11):2649-2661, 2007.\n' +
      '* [28] Vivek Kwatra, Arno Schodl, Irfan Essa, Greg Turk, and Aaron Bobick. Graphcut textures: Image and video synthesis using graph cuts. _Acm transactions on graphics (tog)_, 22(3):277-286, 2003.\n' +
      '* [29] Donghoon Lee, Tomas Pfister, and Ming-Hsuan Yang. Inserting videos into videos. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10061-10070, 2019.\n' +
      '* [30] Chuan Li and Michael Wand. Precomputed real-time texture synthesis with markovian generative adversarial networks. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14_, pages 702-716. Springer, 2016.\n' +
      '* [31] Kaican Li, Kai Chen, Haoyu Wang, Lanqing Hong, Chaoqiang Ye, Jianhua Han, Yukai Chen, Wei Zhang, Chunjing Xu, Dit-Yan Yeung, et al. Coda: A real-world road corner case dataset for object detection in autonomous driving. _arXiv preprint arXiv:2203.07724_, 2022.\n' +
      '* [32] Wei Li, CW Pan, Rong Zhang, JP Ren, YX Ma, Jin Fang, FL Yan, QC Geng, XY Huang, HJ Gong, et al. Aads: Augmented autonomous driving simulation using data-driven algorithms. _Science robotics_, 4(28):eaaw0863, 2019.\n' +
      '* [33] Lingxiao Lu, Jiangtong Li, Junyan Cao, Li Niu, and Liqing Zhang. Painterly image harmonization using diffusion model. In _Proceedings of the 31st ACM International Conference on Multimedia_, pages 233-241, 2023.\n' +
      '* [34] Arun Mallya, Ting-Chun Wang, Karan Sapra, and Ming-Yu Liu. World-consistent video-to-video synthesis. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VIII 16_, pages 359-378. Springer, 2020.\n' +
      '* [35] Jiageng Mao, Minzhe Niu, Chenhan Jiang, Xiaodan Liang, Yamin Li, Chaoqiang Ye, Wei Zhang, Zhenguo Li, Jie Yu, Chunjing Xu, et al. One million scenes for autonomous driving: Once dataset. 2021.\n' +
      '* [36] Mark Martinez, Chawin Sitawarin, Kevin Finch, Lennart Meincke, Alex Yablonski, and Alain Kornhauser. Beyond grand theft auto v for training, testing and enhancing deep learning in self driving cars. _arXiv preprint arXiv:1712.01397_, 2017.\n' +
      '* [37] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. _arXiv preprint arXiv:1411.1784_, 2014.\n' +
      '* [38] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2536-2544, 2016.\n' +
      '* [39] QIU023. Guivideodisplayselector: A simple tkinter-based gui application for video comparison and selection., 2023. [https://github.com/QIU023/GUIVideoDisplaySelector](https://github.com/QIU023/GUIVideoDisplaySelector) [Accessed: (November 8, 2023)].\n' +
      '* [40] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14_, pages 102-118. Springer, 2016.\n' +
      '* [41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.\n' +
      '* [42] German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3234-3243, 2016.\n' +
      '* [43] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In _ACM SIGGRAPH 2022 Conference Proceedings_, pages 1-10, 2022.\n' +
      '* [44] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In _Proceedings of the IEEE international conference on computer vision_, pages 2830-2839, 2017.\n' +
      '* [45] Manolis Savva, Angel X Chang, Alexey Dosovitskiy, Thomas Funkhouser, and Vladlen Koltun. Minos: Multimodal indoor simulator for navigation in complex environments. _arXiv preprint arXiv:1712.03931_, 2017.\n' +
      '* [46] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9339-9347, 2019.\n' +
      '* [47] Gyeongik Shin, Kyeongmin Yu, Mpabulungi Mark, and Hyunki Hong. Hdr map reconstruction from a single ldr sky panoramic image for outdoor illumination estimation. _IEEE Access_, 2023.\n' +
      '* [48] SideFX. Unreal plug-in, 2023. [https://www.sidefx.com/products/houdini-engine/plug-ins/unreal-plug-in/](https://www.sidefx.com/products/houdini-engine/plug-ins/unreal-plug-in/) [Accessed: (October 24, 2023)].\n' +
      '* [49] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.\n' +
      '* [50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pages 2256-2265. PMLR, 2015.\n' +
      '* [51] Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Errui Ding, Jingdong Wang, and Gang Zeng. Delicate textured mesh recovery from nerf via adaptive surface refinement. _arXiv preprint arXiv:2303.02091_, 2023.\n' +
      '* [52] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1526-1535, 2018.\n' +
      '* [53] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. _Advances in neural information processing systems_, 29, 2016.\n' +
      '* [54] Vulkan. Vulkan, cross platform 3d graphics, 2023. [https://www.vulkan.org/](https://www.vulkan.org/) [Accessed: (October 16, 2023)].\n' +
      '* [55] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-video synthesis. _arXiv preprint arXiv:1808.06601_, 2018.\n' +
      '* [56] Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. Few-shot video-to-video synthesis. _arXiv preprint arXiv:1910.12713_, 2019.\n' +
      '* [57] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 9068-9079, 2018.\n' +
      '* [58] Pengchuan Xiao, Zhenlei Shao, Steven Hao, Zishuo Zhang, Xiaolin Chai, Judy Jiao, Zesong Li, Jian Wu, Kai Sun, Kun Jiang, et al. Pandaset: Advanced sensor suite dataset for autonomous driving. In _2021 IEEE International Intelligent Transportation Systems Conference (ITSC)_, pages 3095-3101. IEEE, 2021.\n' +
      '* [59] Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang, and Hao Li. High-resolution image inpainting using multiscale neural patch synthesis. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6721-6729, 2017.\n' +
      '* [60] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Niessner, and Angela Dai. Scannet++: A high-fidelity dataset of 3d indoor scenes. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [61] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with contextual attention. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5505-5514, 2018.\n' +
      '* [62] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _Computer Vision (ICCV), 2017 IEEE International Conference on_, 2017.\n' +
      '\n' +
      '## 임의의 장면에서 임의의 것: 사실적 비디오 객체 삽입\n' +
      '\n' +
      'Supplementary Material\n' +
      '\n' +
      '이 보충 자료에는 추가 기술 세부 사항과 제안된 방법의 광범위한 양적 및 질적 결과가 포함된다. 먼저 섹션 7의 자산 은행, 섹션 8의 객체 배치, 섹션 9의 조명 추정 및 그림자 생성, 섹션 10의 실제 스타일 전송에 대한 추가 세부 사항을 설명하고 섹션 11의 다양한 시뮬레이션 비디오를 비교하기 위해 인간 연구를 수행한 세부 사항을 소개한다.\n' +
      '\n' +
      '또한, 12절에서 실내 데이터세트 ScanNet++를 이용한 정량적 검증 결과를 제시하고, 13절에서 수행한 다운스트림 태스크에 대한 추가 세부 정보를 제공하고, 13절에서 수행한 다운스트림 태스크의 결과에 대한 추가 세부 정보를 시각적으로 강조하기 위해 14절에서 비교 분석을 위해 프레임워크에서 생성된 시뮬레이션 비디오 갤러리를 포함한다.\n' +
      '\n' +
      '마지막으로, 리뷰어는 이러한 대표적인 예를 통해 시뮬레이션 방법의 성능을 더 잘 평가하기 위해 우리의 보충 비디오 파일(실외 장면의 경우_sample_video_outdoor.mp4_ 및 실내 장면의 경우_sample_video_indoor.mp4_)을 볼 것을 제안합니다.\n' +
      '\n' +
      '##7 자산은행 상세내역\n' +
      '\n' +
      '우리의 Anything in Any Scene 프레임워크는 동적 장면 비디오를 관심 객체와 통합하여 대규모 시뮬레이션 비디오를 생성하는 것을 목표로 한다. 이를 위해서는 장면 비디오 및 객체 메쉬의 자산 은행이 필요하다. 이를 위해 시각 기술자를 기반으로 장면 비디오를 효율적으로 선택하기 위한 시각 데이터 쿼리 엔진을 개발하였다. 또한, 3차원 메쉬 생성을 위해 Houdini 엔진과 NeRF(Neural Radiance Fields) 기반 재구성을 사용하여 다양한 객체를 비디오에 통합할 수 있다.\n' +
      '\n' +
      '### 시각 데이터 질의 엔진\n' +
      '\n' +
      '본 논문에서 제안하는 방법은 대규모 비디오 자산은행으로부터 구도를 위한 대상 비디오를 효율적으로 찾기 위해 비주얼 데이터 질의 엔진을 활용한다. 이 엔진은 제공된 설명 단어와 시각적으로 일치하는 비디오 클립의 클러스터를 검색하도록 설계되었다. 상세한 시각적 특징을 가진 대규모 이미지 및 비디오 데이터를 처리하기 위해, 우리는 Bag of Visual Words (BoVW) 접근법을 사용한다.\n' +
      '\n' +
      '우리는 먼저 장면 비디오 자산 은행의 각 프레임에 대한 시맨틱 세분화 마스크를 추정한다. 이 세그먼테이션은 각각의 비디오 프레임을 라벨링된 관심 영역들로 분해한다. 이를 위해 SIFT(Scale Invariant Feature Transform) 알고리즘을 이용하여 분할된 영상에서 시각적 특징을 추출한다. 각 프레임에서 특징점들을 검출하고, 이들 특징점들을 포함하는 영역들에 대한 특징 벡터로 표현된 디스크립터들을 계산한다. 그런 다음 이러한 기술자는 BoVW에서 \'시각적 단어\'를 나타내는 각 클러스터의 중심과 함께 클러스터링된다. 비디오 데이터세트에 걸친 이러한 시각적 단어의 빈도는 각 비디오에 대한 빈도 히스토그램을 구축하는 데 사용된다. 결과적으로 BoVW 표현은 주어진 시각적 단어들의 발생과 빈도에 기초하여 매칭 비디오들을 효과적으로 검색할 수 있게 하여, Any Scene 프레임워크에서 우리의 Anything에 적합한 비디오들을 선택하는 과정을 개선한다.\n' +
      '\n' +
      '### 객체 메쉬 생성\n' +
      '\n' +
      '대상 객체의 메쉬 모델은 기존 비디오 클립에 삽입되기 전에 필요하다. 객체 메쉬 모델을 생성하기 위해 다음과 같은 두 가지 방법을 사용한다.\n' +
      '\n' +
      '**객체 메쉬 생성을 위한 후디니 엔진** 시각적으로 매력적이고 물리적으로 정확한 객체 메쉬를 생성하기 위해 물리 기반 렌더링 기능을 활용하여 실제 물리적 효과를 가진 기존의 객체 메쉬 모델을 향상시키는 후디니 엔진[48]을 활용한다.\n' +
      '\n' +
      '그림 9: 강력한 3D 애니메이션 절차 도구로 알려진 비디오 시뮬레이션 후디니 엔진을 위한 생성된 객체 메쉬의 예는 변형, 애니메이션, 반사 및 입자 시각 효과와 같은 광범위한 물리적 효과를 생성할 수 있다. 예시가 그림 9a에 나타나 있듯이, 후디니 엔진은 변형 효과를 적용하여 트럭 모델을 충돌 모델로 변환할 수 있다. 또한, 입자 시각 시스템을 사용하여 충돌한 자동차에서 발생하는 연기와 같은 다양한 현실적인 물리적 효과를 시뮬레이션할 수 있다. 이 접근법은 실제 시나리오에서 캡처하기 어렵거나 비용이 많이 드는 객체 메쉬를 생성하는 데 특히 중요하다.\n' +
      '\n' +
      '**NeRF 기반 객체 메쉬 재구성** 후디니 엔진에서 제작하기 어려운 객체를 커버하고 임의의 객체를 포함하도록 자산 뱅크를 일반화하기 위해, 우리는 보완적인 NeRF 기반 객체 메쉬 재구성을 제안한다. 다시점 영상으로부터 3차원 재구성에서 신경 복사 필드(NeRF)의 인상적인 성능은 광범위한 3차원 자산 뱅크를 구축할 수 있는 잠재력을 제공한다. 본 논문에서는 NeRF와 메쉬 표현의 장점을 결합한 Off-the-shelf 방법[51]을 제안한다. 이 방법은 다시점 RGB 영상으로부터 객체 메쉬 모델을 재구성한다. 재구성된 사람 객체 메시의 예가 도 9b에 도시되어 있으며, 이는 렌더링 프로세스들을 따르기에 적합한 풍부한 텍스처들 및 상세한 기하구조들을 특징으로 한다.\n' +
      '\n' +
      '## 8 객체 배치 세부사항\n' +
      '\n' +
      '삽입된 객체를 장면 비디오 내에 정확하게 위치시키기 위해, 제1 단계는 캡처된 환경의 3D 포인트 클라우드 표현을 재구성하는 것을 포함한다. 그 후, 객체 배치 포인트는 세그먼테이션 마스크에 의해 안내되는, 3D 공간에서 결정된다. 2D-to-3D 프로젝션 과정에서 삽입된 물체에 대한 적절한 배치 평면을 추정하는 데 중점을 둔다. 이 평면은 방정식으로 표시되는 가장 적합한 평면으로 개념화된다:\n' +
      '\n' +
      '\\[Ax+By+Cz+D=0 \\tag{6}\\]\n' +
      '\n' +
      '는 선택된 점 \\((x,y,z)\\)을 기준으로 한다.\n' +
      '\n' +
      '보다 정확한 추정을 위해, 우리는 최적의 피팅 평면을 결정하기 위해 다수의 3D 포인트들을 이용한다. 도 11에 예시된 바와 같이, 우리는 물체가 현실적으로 삽입될 수 있는 지면 평면을 추정하기 위해 도로 영역 내의 여러 지점들(도 11의 황색)을 선택한다.\n' +
      '\n' +
      '**PandaSet 및 ScanNet++ Datasets에 대한 설정**: 우리가 사용한 두 데이터 세트인 실외 장면에 대한 PandaSet[58], 실내 장면에 대한 ScanNet++[60]은 RGB 카메라와 깊이 센서에 의해 캡처된 장면으로 구성된다. 이 센서는 팬더셋의 주행 장면과 ScanNet++의 객실 장면을 녹화합니다. 이러한 데이터 세트에서 비디오 클립에 대한 우리의 선택 프로세스는 전방 대면 카메라에 의해 캡처된 것, 특히 카메라가 움직임을 나타내는 클립에 초점을 맞추는 것을 선택하여 구성을 위한 동적 및 다양한 프레임을 보장하는 것을 포함한다. ScanNet++ 데이터세트와 관련하여 각 원본 비디오 클립에서 5초 세그먼트를 선택하여 원본 60Hz에서 20Hz로 다운 샘플링했다. 각 클립에서 최소 20개의 프레임을 사용할 수 있도록 하여 효과적인 비디오 합성을 위한 적절한 수의 프레임을 제공했다. 우리는 카메라가 교통 신호에서 정지 차량에 부착되는 그림 10과 같은 시나리오와 같이 낮은 프레임 속도 문제로 고통받거나 카메라가 녹화 중에 정지 상태로 유지되는 비디오 클립을 제외한다.\n' +
      '\n' +
      '두 데이터 세트에서 사용할 수 있는 깊이 정보와 분할 데이터를 사용하여 장면에 대한 3D 포인트 클라우드를 재구성한다. 이를 통해 3차원 공간에서 지정된 배치 가능 영역 내에서 객체 배치 지점을 정밀하게 선택할 수 있다. 예를 들어, 판다 세트로부터의 운전 장면에서, 그림 11에 예시된 바와 같이, 노란색으로 강조된 도로 영역은 장면에 자동차를 삽입하기 위한 적절한 위치로 식별된다.\n' +
      '\n' +
      '도 11: 객체 삽입을 위한 투영된 3D 장면. 노란색 영역은 삽입된 객체를 배치할 수 있는 기준면입니다.\n' +
      '\n' +
      '도 10: 팬더 세트에서 제외된 비디오 클립의 일례. 카메라는 신호등을 기다리는 차량에 있기 때문에 전체 비디오 클립 동안 안정적입니다.\n' +
      '\n' +
      '##9 조명 추정 상세\n' +
      '\n' +
      '그림 12에서는 조명 추정 및 그림자 생성 과정에 대한 포괄적인 개요를 제공한다. 렌더링 시 삽입된 객체에 대한 사실적인 음영 효과를 보장하기 위해 하늘과 주변 환경의 높은 동적 범위(High Dynamic Range, HDR) 이미지를 추정한다.\n' +
      '\n' +
      'HDR 하늘 이미지 추정을 위해, 이미지 인페인팅 네트워크는 처음에 파노라마 하늘 이미지를 추론한다. 이후 스카이 HDR 재구성 네트워크를 사용하여 이 파노라마 스카이 이미지를 HDR 이미지로 변환한다. 동시에, HDR 환경 이미지의 추정은 LDR에서 HDR 네트워크를 사용하여 장면의 LDR(Low Dynamic Range) 측면 뷰 이미지로부터 HDR 이미지를 재구성하는 것을 포함한다. 그런 다음 이러한 이미지는 HDR 파노라마 환경 이미지를 형성하기 위해 매끄럽게 함께 스티칭된다.\n' +
      '\n' +
      'HDR 하늘과 환경 이미지는 모두 렌더링 과정에서 삽입된 객체에 대한 사실적인 조명 효과를 달성하기 위해 함께 통합된다. 또한, 추정된 HDR 스카이 이미지를 활용하여 삽입된 객체에 대한 그림자를 렌더링한다. 이를 위해 3D 그래픽스 애플리케이션인 Vulkan을 활용한다.\n' +
      '\n' +
      '##10 포토리얼리즘 스타일 전달 세부사항\n' +
      '\n' +
      '우리는 사실적 스타일 전달을 위해 coarse-to-fine 메커니즘을 활용하며, 네트워크의 개요는 그림 13에 나와 있으며, 여기서 coarse 네트워크와 정제 네트워크는 모두 확장된 컨볼루션 레이어로 구성된다. 전경 영역에 채워진 검은색 화소, 전경 영역을 나타내는 이진 마스크, 배경 영역에 채워진 검은색 화소로 이미지를 초기 거친 예측을 출력하는 거친 네트워크에 입력으로 연결한다. 정제된 네트워크는 거친 네트워크의 입력과 출력의 구도를 취하고, 최종 정제된 완성된 이미지를 생성한다.\n' +
      '\n' +
      '[61]에서 설명한 것과 동일한 훈련 전략을 따랐으며, 거친 네트워크는 재구성 손실로 훈련되고, 정제 네트워크는 재구성 및 GAN 손실로 훈련된다. 우리는 야외 시나리오에 대한 팬더셋 데이터 세트의 네트워크를 훈련하고 미세 조정했다. 모든 입력은 함께 연결되고 입력 이미지 해상도로 \\(256\\times 256\\) 크기 조정된다.\n' +
      '\n' +
      '또한 제안된 프레임워크에서 실제적 스타일 개선의 과제에 대한 다양한 스타일 전달 방법의 성능에 관심이 있다. 구체적으로 본 논문에서 소개한 방법과 비교하여 CNN 기반 방법 DoveNet[8], 변압기 기반 방법 StyTR2[11], 확산 모델 기반 방법 PHDiffusion[33]의 사용법을 조사하였다.\n' +
      '\n' +
      '**도브넷**: U-Net 기반 네트워크는 삽입된 전경 영역의 시각 영역을 배경과 일치하도록 번역하기 위한 생성기로 사용되며, 두 개의 서로 다른 판별기를 갖는 GAN 프레임워크는 보다 사실적이고 조화로운 이미지 출력을 위해 생성기를 훈련시키기 위해 레버리지된다. 우리는 [8]에서 설명한 것과 동일한 과정을 따르며, 훈련 및 테스트 단계에서 입력 이미지의 크기를 \\(256\\times 256\\)으로 조정한다.\n' +
      '\n' +
      '**StyTR2**: 트랜스포머는 스타일 전송을 위해 이미지 특징의 장거리 의존성을 캡처하기 위해 인코더로서 레버리지된다. StyTR2 모델에 대해 스타일 가중치를 \\(10.0\\), 내용 가중치를 \\(7.0\\)으로 설정하고, 학습 단계에서 입력 영상으로 이미지를 \\(512\\times 512\\)으로 축소한 후 무작위로 이미지를 \\(256\\)으로 크롭한다.\n' +
      '\n' +
      '**PHDiffusion**: 전경 특징을 스타일화하기 위해 두 개의 인코더를 사용하기 위해 안정적인 확산 모델이 제안된다. 두 인코더의 기능을 결합하여 스타일 전송 프로세스를 완료합니다. 트레이닝과 테스트 모두에서 입력 해상도로 미리 훈련된 안정 확산 모델 가중치를 로드하고 모든 이미지의 크기를 \\(512\\times 512\\)으로 조정한다.\n' +
      '\n' +
      '그림 12: 조명 추정 및 그림자 생성 개요.\n' +
      '\n' +
      '##11 인간 연구 세부사항\n' +
      '\n' +
      '제안된 방법에 의해 생성된 시뮬레이션 비디오는 인간 A/B 테스트를 통해 검증되었다. 사용자가 직접 설계하고 개발한 GUI 애플리케이션[39]을 활용하여 두 동영상을 나란히 비교하고, 그 중 선호하는 것을 선택할 수 있도록 하였다. 각 인간 판사가 연구를 수행하기 전에 다음과 같은 지침을 제공한다.\n' +
      '\n' +
      '_당신은 컴퓨터에 의해 만들어진 비디오의 사실성을 평가하기 위한 연구에 참여하고 있다. 각 비디오는 객체가 삽입된 장면을 특징으로 합니다. 당신의 임무는 두 개의 비디오를 나란히 비교하고 당신에게 더 사실적으로 보이는 비디오를 선택하는 것입니다. 디스플레이 화면 앞에 적절한 거리에 착석하여 영상을 재생하고 이전 또는 다음 작업으로 가는 등 컨트롤에 익숙해지도록 하십시오. 본 연구를 위해 현실성은 객체가 장면 비디오에 얼마나 설득력 있게 통합되는지에 의해 정의된다. 평가에는 다음과 같은 요소를 고려할 수 있다: 1. 장면에 묘사된 물리적 규칙과 물체의 일관성. 2. 조명, 그림자, 복제의 자연성. 3. 물체의 환경과 상호작용의 신뢰성. 결정을 내리기 전에 두 비디오를 한 번 이상 전체로 시청하고, 장면에 삽입된 객체를 중심으로 필요에 따라 얼마든지 볼 수 있다. 해당 "select" 버튼을 눌러 더 나은 리얼리즘이 있다고 믿는 영상을 선택해주세요._\n' +
      '\n' +
      '인간 판사들은 모든 비디오 쌍을 통한 내비게이션, 비디오 재생, 비디오 선택, 비디오 서스펜드, 및 선택 뷰 패널과 같은 다수의 제어들을 제공하는, 도 14에 도시된 바와 같은 애플리케이션을 사용한다.\n' +
      '\n' +
      '각 데이터 세트의 검증을 위해 두 개의 개별 인간 연구를 수행했다. 실외 데이터셋에 대한 연구는 24명의 인간 판사가 참여했으며 실내 데이터셋에 대한 연구는 16명의 참가자가 있었다. 판다세트 데이터 세트를 검증할 때 100개의 비디오 풀이 있었고 그 중 38개가 인간 연구를 위해 무작위로 선택되었다. ScanNet++ 데이터 세트의 경우 52개의 사용 가능한 비디오 중 30개가 인간 연구를 수행하기 위해 무작위로 선택되었다. 모든 비디오는 FID 점수 계산에 사용되었습니다. 각 연구에서 모든 판사는 총 105쌍의 비디오를 평가하고 라벨링하는 임무를 받았다. 첫 번째 연구에서, 우리는 DoveNet, StyTR2, PHDiffusion 및 우리의 프레임워크를 포함하는 제안된 프레임워크에 연결된 다양한 스타일 전송 네트워크의 성능을 비교한다. 또 다른 인간 연구의 경우, 주요 구성 요소 중 하나를 제거하면 제안된 방법의 성능을 분석한다. 두 연구 설정 모두에서 스타일 전달 과정이 없는 제안된 방법을 기준으로 설정했다.\n' +
      '\n' +
      '방법 A의 인간 점수는 다음과 같이 계산될 수 있다.\n' +
      '\n' +
      '\\[\\begin{split}\\text{times of results by method A selected}\\\\text{total times of results by method A and B selected}\\end{split}\\tag{7}\\\\\n' +
      '\n' +
      '여기서, 방법 B는 기준선이고, 방법 A는 비교를 위한 방법이다. 방법 A가 또한 기준이라고 가정하면, 이론적으로 기준 방법은 50%의 인간 점수를 갖는다. 본 논문에서 실외 데이터셋 팬더셋에 대한 정량적 및 정성적 결과를 제공하며, 실내 데이터셋 ScanNet+에 대한 결과는 섹션 14에 자세히 설명되어 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline Method & Human Score(\\%) & FID \\\\ \\hline \\hline\n' +
      '**Proposed method** & **57.92** & **10.537** \\\\ \\hline StyTR2 style transfer & 53.33 & 11.145 \\\\ \\hline PHDiffusion style transfer & 36.25 & 12.004 \\\\ \\hline DoveNet style transfer & 44.58 & 10.832 \\\\ \\hline w/o style transfer & N/A & 11.901 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: Any Scene 프레임워크에서 Anything에 플러그인된 서로 다른 스타일 전송 네트워크를 갖는 실내 장면 데이터세트 ScanNet+의 실험 결과.\n' +
      '\n' +
      '그림 13: 사실적 스타일 전달을 위한 거친-대-미세 메커니즘의 개요. 입력은 블랙 전경 영역을 갖는 배경 RGB 이미지, 삽입된 객체 전경 RGB 이미지, 및 전경 분할 마스크이다. 출력은 정제된 RGB 이미지입니다.\n' +
      '\n' +
      '##12 실내 장면의 실험 결과\n' +
      '\n' +
      '본 논문에서 자세히 설명된 것과 동일한 실험 설정을 따랐고 실내 장면에서 제안된 방법의 검증을 수행했다. 실외 장면에 대한 검증과 유사하게 CNN 기반 방법 DoveNet, 변압기 기반 방법 StyTR2, 확산 모델 기반 방법 PHD 확산 및 우리의 방법을 비교하여 다양한 스타일 전달 네트워크의 성능을 평가했다. 인간 연구에서 14에 설명된 대로 스타일 전달 모듈이 없는 프레임워크를 비교를 위한 기준선으로 지정했다. 표 4에 요약된 비교 결과는 우리의 스타일 전달 네트워크가 10.537의 가장 낮은 프레쳇 인셉션 거리(FID) 점수와 57.92%의 가장 높은 인간 점수를 달성하여 다른 방법의 성능을 능가했음을 보여준다.\n' +
      '\n' +
      '**절제 연구**: 또한 실내 데이터세트 ScanNet+를 사용하여 절제 연구를 수행하여 개별 모듈이 전체 성능에 미치는 영향을 평가했다. 마찬가지로, 배치(w/o 배치), HDR 이미지 재구성(w/o HDR) 및 스타일 전송(w/o 스타일 전송)이라는 하나의 모듈을 프레임워크에서 제거했다. 결과는 표 5에 자세히 설명되어 있으며 실외 데이터 세트 팬더셋의 결과와 유사하게 HDR의 부재 및 스타일 전달 모듈이 더 높은 FID를 초래했다. 비현실적인 위치에 객체를 배치하면 인간 관찰자 사이에서 인지된 사실성이 크게 감소했다. 그러나 이러한 사실성의 감소는 FID 점수에 정확하게 반영되지 않았다. 한 가지 주요 이유는 종종 제한된 공간을 갖는 실내 장면의 특성이다. 이는 삽입된 객체가 카메라의 시야에서 부분적으로 또는 완전히 벗어나 평가 메트릭에 영향을 미치는 결과를 초래할 수 있다. 우리의 방법은 일관되게 50% 이상의 인간 점수를 받은 반면, 다른 방법은 50% 미만으로 떨어져 시스템의 효능에 대한 각 모듈의 기여를 강조했다.\n' +
      '\n' +
      '## 13 다운스트림 작업 세부사항\n' +
      '\n' +
      'CODA2022 검증 데이터 세트의 이미지에 삽입하기 위한 25개의 객체 범주를 포함하도록 범위를 확장했다. 유사하게, 우리는 YOLOX-S, YOLOX-L 및 YOLOX-X [14]의 세 가지 모델을 훈련시켰다. 우리는 "Any Scene에서 Any" 프레임워크를 활용하여 다양한 객체를 삽입하여 원본 학습 이미지를 증강하여 새로운 학습 데이터 세트를 생성했다. 그런 다음 이 향상된 데이터 세트를 사용하여 모델을 재훈련하여 원래 훈련 전략과의 일관성을 보장했다.\n' +
      '\n' +
      '모델의 성능은 원본 및 증강 데이터 세트 모두에 대해 훈련한 다음 테스트하여 평가되었다.\n' +
      '\n' +
      '도 14: 두 비디오 품질을 비교하기 위한 휴먼 스터디 인터페이스의 예. 인간 판사는 보다 사실적인 시각적 효과 때문에 올바른 비디오를 선택한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Method & Human Score(\\%) & FID \\\\ \\hline \\hline Proposed method & 57.92 & 10.537 \\\\ \\hline w/o placement & 9.58 & 9.709 \\\\ \\hline w/o HDR & 32.92 & 10.824 \\\\ \\hline w/o shadow & 36.25 & 10.464 \\\\ \\hline w/o style transfer & N/A & 11.901 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: Anything in Any scene 프레임워크에서 모듈의 절제 분석을 위한 실험 결과. 기준 w/o 스타일 이전 방법은 이론적으로 50%의 인간 점수를 갖는다는 점에 유의한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '도 15: 상이한 렌더링 옵션들을 갖는 실외 장면 데이터세트 팬더셋을 사용하여 시뮬레이션된 비디오 프레임의 정성적 비교. (a) 객체 배치가 없는 방법에 의해 생성된 것; (b) HDR 이미지 재구성이 없는 방법에 의해 생성된 것; (c) 그림자 생성이 없는 방법에 의해 생성된 것; (d) 스타일 전달이 없는 방법에 의해 생성된 것; (e) 모든 렌더링 옵션을 포함하는 제안된 방법에 의해 생성된 것이다.\n' +
      '\n' +
      '도 16: 상이한 스타일 전송 네트워크를 갖는 실외 장면 데이터세트 팬더세트를 사용하여 시뮬레이션된 비디오 프레임의 정성적 비교. (a) DoveNet에 의해 생성된; (b) StyTR2에 의해 생성된; (c) 제안된 스타일 전송 네트워크에 의해 생성된 PHDiffusion(d)에 의해 생성된\n' +
      '\n' +
      '도 17: 상이한 렌더링 옵션들을 갖는 실내 장면 데이터세트 ScanNet++를 사용하여 시뮬레이션된 비디오 프레임의 정성적 비교. (a) 객체 배치가 없는 방법에 의해 생성된 것; (b) HDR 이미지 재구성이 없는 방법에 의해 생성된 것; (c) 그림자 생성이 없는 방법에 의해 생성된 것; (d) 스타일 전달이 없는 방법에 의해 생성된 것; (e) 모든 렌더링 옵션을 포함하는 제안된 방법에 의해 생성된 것이다.\n' +
      '\n' +
      '도 18: 상이한 스타일 전송 네트워크를 갖는 실내 장면 데이터세트 ScanNet++를 사용하여 시뮬레이션된 비디오 프레임의 정성적 비교. (a) DoveNet에 의해 생성된; (b) StyTR2에 의해 생성된; (c) 제안된 스타일 전송 네트워크에 의해 생성된 PHDiffusion(d)에 의해 생성된\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>