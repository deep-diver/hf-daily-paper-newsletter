<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#1비트 LLMs의 시대:\n' +
      '\n' +
      '모든 대형 언어 모델은 1.58비트에 있습니다.\n' +
      '\n' +
      '마홍유 왕링샤오 마레이 왕웬후이 왕샤오한 황설리 왕질롱 후루웨이\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '비트넷 [2]와 같은 최근의 연구는 1비트 대용량 언어 모델(LLM)의 새로운 시대를 여는 길을 열어주고 있다. 이 작업에서 우리는 LLM의 모든 단일 매개변수(또는 가중치)가 삼원 {-1, 0, 1}인 1비트 LLM 변형, 즉 **BitNet b1.58**를 소개한다. 이는 풀-정밀도(즉, FP16 또는 BF16) 트랜스포머 LLM과 동일한 모델 크기 및 트레이닝 토큰을 사용하여 퍼플렉시티 및 엔드-태스크 성능 측면에서 일치하면서도 레이턴시, 메모리, 스루풋 및 에너지 소비 측면에서 훨씬 더 비용 효율적이다. 보다 심층적으로, 1.58비트 LLM은 고성능이면서 비용 효율적인 LLM의 새로운 세대를 훈련시키기 위한 새로운 스케일링 법칙과 레시피를 정의한다. 또한, 새로운 계산 패러다임을 가능하게 하고 1비트 LLM에 최적화된 특정 하드웨어를 설계할 수 있는 문을 열 수 있다.\n' +
      '\n' +
      '마홍유 왕링샤오 마레이 왕웬후이 왕샤오한 황설리 왕질롱 후루웨이\n' +
      '\n' +
      '[https://aka.ms/GeneralAI](https://aka.ms/GeneralAI)\n' +
      '\n' +
      '[FIGURE\n' +
      '\n' +
      '도 1: 1-비트 LLMs(예를 들어, BitNet b1.58)는 모델 성능을 유지하면서 LLMs의 추론 비용(지연, 처리량 및 에너지)을 감소시키기 위해 파레토 솔루션을 제공한다. 비트넷 b1.58의 새로운 계산 패러다임은 1비트 LLM에 최적화된 새로운 하드웨어를 설계하기 위한 동작을 요구한다.\n' +
      '\n' +
      '1비트 LLM의 시대\n' +
      '\n' +
      '최근 몇 년 동안 AI 분야는 대규모 언어 모델(LLM)의 규모와 역량이 빠르게 성장하고 있다. 이러한 모델은 광범위한 자연어 처리 작업에서 놀라운 성능을 보여주었지만, 그 크기가 증가함에 따라 배치의 어려움이 제기되었으며 높은 에너지 소비로 인한 환경 및 경제적 영향에 대한 우려가 제기되었다. 이러한 과제를 해결하기 위한 한 가지 접근법은 추론용 저비트 모델을 생성하기 위해 훈련 후 양자화를 사용하는 것이다[23, 1, 13, 14]. 이 기술은 가중치 및 활성화의 정밀도를 감소시켜 LLM의 메모리 및 계산 요구 사항을 크게 감소시킨다. 4비트 변형[1, 15]과 같이 16비트에서 하위 비트로 이동하는 경향이 있다. 그러나, 훈련 후 양자화는 산업 LLM에서 널리 사용되고 있음에도 불구하고 차선책이다.\n' +
      '\n' +
      '비트넷(BitNet, 23)과 같은 1비트 모델 아키텍처에 대한 최근 연구는 LLM의 성능을 유지하면서 비용 절감을 위한 유망한 방향을 제시한다. 바닐라 LLM은 16비트 부동 값(즉, FP16 또는 BF16)에 있으며, 임의의 LLM의 대부분은 행렬 곱셈이다. 따라서 부동소수점 덧셈 연산과 곱셈 연산에서 계산 비용이 크게 발생한다. 대조적으로, 비트넷의 행렬 곱셈은 LLM에 대한 에너지 비용의 순서를 절약하는 정수 덧셈만을 포함한다. 많은 칩에서 성능을 계산하는 근본적인 한계가 전력이기 때문에 에너지 절약도 더 빠른 계산으로 변환할 수 있다.\n' +
      '\n' +
      '계산 외에도, 모델 파라미터들을 DRAM으로부터 온-칩 가속기(예를 들어, SRAM)의 메모리로 전달하는 프로세스는 추론 동안 비용이 많이 들 수 있다. 스루풋을 향상시키기 위해 SRAM을 확대하려는 시도가 있었지만, 이것은 DRAM보다 상당히 높은 비용을 도입한다. 풀-정밀 모델에 비해, 1-비트 LLM은 용량 및 대역폭 관점에서 훨씬 더 낮은 메모리 풋프린트를 갖는다. 이는 DRAM으로부터의 로딩 가중치들의 비용 및 시간을 상당히 감소시킬 수 있고, 더 빠르고 더 효율적인 추론으로 이어진다.\n' +
      '\n' +
      '이 작업에서 우리는 **BitNet b1.58**라고 하는 중요한 1비트 LLM 변종을 소개하며, 여기서 모든 매개변수는 {-1, 0, 1}의 값을 취한다. 우리는 원래의 1비트 비트넷에 0의 추가 값을 추가하여 이진 시스템에서 1.58비트를 얻었다. 비트넷 b1.58은 매트릭스 곱셈을 위한 곱셈 연산이 거의 필요하지 않고 고도로 최적화될 수 있는 새로운 계산 패러다임을 포함하여 원래 1비트 비트넷의 모든 이점을 유지한다. 또한, 원래의 1비트 비트넷과 동일한 에너지 소비를 가지며 FP16 LLM 베이스라인에 비해 메모리 소비, 처리량 및 지연 시간 측면에서 훨씬 더 효율적이다. 또한 비트넷 b1.58은 두 가지 추가 이점을 제공한다. 첫째, 1비트 LLM의 성능을 크게 향상시킬 수 있는 모델 가중치에 0을 포함시킴으로써 가능한 특징 필터링에 대한 명시적인 지원으로 인해 모델링 능력이 더 강해진다. 두 번째로, 우리의 실험은 비트넷 b1.58이 동일한 구성(예를 들어, 모델 크기, 트레이닝 토큰 등)을 사용할 때 3B 크기에서 시작하여 복잡성과 엔드-태스크 성능 모두에서 완전한 정밀도(즉, FP16) 기준선을 일치시킬 수 있음을 보여준다.\n' +
      '\n' +
      '## 2 BitNet b1.58\n' +
      '\n' +
      'BitNet b1.58은 BitNet 아키텍처를 기반으로 하며, _nn.Linear_을 _BitLinear_로 대체하는 Transformer이다. 1.58비트 무게와 8비트 활성화로 처음부터 훈련됩니다. 오리지널 비트넷과 비교하여 아래에서 요약하는 몇 가지 수정 사항을 소개한다.\n' +
      '\n' +
      '양자화 함수. 가중치를 -1, 0 또는 +1로 제한하기 위해 _absmean_ 양자화 함수를 채택한다. 먼저 가중치 행렬을 평균 절대값으로 스케일링한 후 각 값을 {-1, 0, +1} 중 가장 가까운 정수로 라운드한다:\n' +
      '\n' +
      '\\[\\widetilde{W}=\\mathrm{RoundClip}(\\frac{W}{\\gamma+\\epsilon},-1,1), \\tag{1}\\] \\[\\mathrm{RoundClip}(x,a,b)=\\max(a,\\min(b,\\mathrm{round}(x))),\\] (2) \\[\\gamma=\\frac{1}{nm}\\sum_{ij}|W_{ij}|. \\tag{3}\\\\tag{3}\\\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\\m\n' +
      '\n' +
      '활성화에 대한 양자화 함수는 비선형 함수들 이전의 활성화를 범위 \\([0,Q_{b}]\\)으로 확장하지 않는다는 점을 제외하고는 BitNet에서 동일한 구현을 따른다. 대신에, 활성화는 제로-포인트 양자화를 제거하기 위해 토큰당 \\([-Q_{b},Q_{b}]\\)으로 모두 스케일링된다. 이것은 구현 및 시스템 수준 최적화 모두에 더 편리하고 간단하지만 실험에서 성능에 무시할 수 있는 영향을 도입한다.\n' +
      '\n' +
      'LLaMA 유사 컴포넌트.LLaMA [111, 123]의 아키텍처는 오픈 소스 LLM의 실제 백본이었다. 오픈 소스 커뮤니티를 수용하기 위해 비트넷 b1.58의 설계는 LLaMA 유사 구성 요소를 채택한다. 구체적으로, RMSNorm[10], SwiGLU[14], 회전 임베딩[13]을 사용하며, 모든 바이어스를 제거한다. 이러한 방식으로, 비트넷 b1.58은 최소한의 노력으로 인기 있는 오픈 소스 소프트웨어(예를 들어, 허깅페이스, vLLM[11], 및 llama.cpp2)에 통합될 수 있다.\n' +
      '\n' +
      '각주 2: [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)\n' +
      '\n' +
      '## 3 Results\n' +
      '\n' +
      '우리는 BitNet b1.58을 다양한 크기로 복제된 FP16 LLaMA LLM과 비교했다. 공정한 비교를 보장하기 위해 우리는 1,000억 토큰에 대해 레드파자마 데이터 세트 [15]에서 모델을 사전 훈련했다. 우리는 ARC-Easy[10], ARC-Challenge[10], Hellaswag[11], Winogrande[2], PIQA[10], OpenbookQA[14], BoolQ[12] 등 다양한 언어 과제에서 제로샷 성능을 평가하였다. 우리는 또한 위키텍스트2 [13] 및 C4 [15] 데이터 세트에 대한 검증 복잡성을 보고했다.\n' +
      '\n' +
      'LLaMA LLM과 BitNet b1.58의 런타임 GPU 메모리와 레이턴시를 비교하였으며, 그 결과는 GPU 장치에서 LLM 추론 레이턴시에 잘 최적화된 FasterTransformer3 코드베이스를 이용하여 측정하였다. Ladder [23]의 2비트 커널은 비트넷 b1.58에 대해서도 통합되어 있으며, 추론을 위한 주요 비용으로서 출력 토큰당 시간을 보고하였다.\n' +
      '\n' +
      '각주 3: [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)\n' +
      '\n' +
      '표 1은 BitNet b1.58과 LLaMA LLM에 대한 당혹성과 비용을 요약한 것이다. 비트넷 b1.58은 GPU 메모리를 2.71배 더 빠르고 3.55배 덜 사용하는 반면 복잡성 측면에서 3B 모델 크기에서 완전 정밀 LLaMA LLM과 일치하기 시작함을 보여준다. 특히, 3.9B 모델 크기를 갖는 BitNet b1.58은 2.4배 빠르고, 3.32배 적은 메모리를 소모하지만, LLaMA LLM 3B보다 훨씬 우수한 성능을 보인다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Models** & **Size** & **Memory (GB)\\(\\downarrow\\)** & **Latency (ms)\\(\\downarrow\\)** & **PPL\\(\\downarrow\\)** \\\\ \\hline LLaMA LLM & 700M & 2.08 (1.00x) & 1.18 (1.00x) & 12.33 \\\\\n' +
      '**BitNet b1.58** & 700M & 0.80 (2.60x) & 0.96 (1.23x) & 12.87 \\\\ \\hline LLaMA LLM & 1.3B & 3.34 (1.00x) & 1.62 (1.00x) & 11.25 \\\\\n' +
      '**BitNet b1.58** & 1.3B & 1.14 (2.93x) & 0.97 (1.67x) & 11.29 \\\\ \\hline LLaMA LLM & 3B & 7.89 (1.00x) & 5.07 (1.00x) & 10.04 \\\\\n' +
      '**BitNet b1.58** & 3B & **2.22(3.55x)** & **1.87(2.71x)** & **9.91**\n' +
      '**BitNet b1.58** & 3.9B & **2.38 (3.32x)** & **2.11 (2.40x)** & **9.62** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: BitNet b1.58 및 LLaMA LLM의 비용뿐만 아니라 복잡도.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline\n' +
      '**Models** & **Size** & **ARCe** & **ARCc** & **HS** & **BQ** & **OQ** & **PQ** & **WGe** & **Avg.** \\\\ \\hline LLaMA LLM & 700M & 54.7 & 23.0 & 37.0 & 60.0 & 20.2 & 68.9 & 54.8 & 45.5 \\\\\n' +
      '**BitNet b1.58** & 700M & 51.8 & 21.4 & 35.1 & 58.2 & 20.0 & 68.1 & 55.2 & 44.3 \\\\ \\hline LLaMA LLM & 1.3B & 56.9 & 23.5 & 38.5 & 59.1 & 21.6 & 70.0 & 53.9 & 46.2 \\\\\n' +
      '**BitNet b1.58** & 1.3B & 54.9 & 24.2 & 37.7 & 56.7 & 19.6 & 68.8 & 55.8 & 45.4 \\\\ \\hline LLaMA LLM & 3B & 62.1 & 25.6 & 43.3 & 61.8 & 24.6 & 72.1 & 58.2 & 49.7 \\\\\n' +
      '**BitNet b1.58** & 3B & **61.4** & **28.3** & *42.9** & **61.5** & **26.6** & *71.5** & *59.3** & *50.2**\n' +
      '**BitNet b1.58** & 3.9B & **64.2** & **28.7** & **44.2** & **63.5** & **24.2** & **73.2** & **60.5** & **51.2** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 엔드 태스크에 대한 BitNet b1.58 및 LLaMA LLM의 제로샷 정확도.\n' +
      '\n' +
      '<표 2>는 엔드 태스크에 대한 제로샷 정확도의 세부 결과를 보고한다. 우리는 평가를 수행하기 위해 _lm-평가-harness4_에서 파이프라인을 따랐다. 결과는 모델 크기가 증가함에 따라 BitNet b1.58과 LLaMA LLM 간의 성능 격차가 줄어든다는 것을 보여준다. 더 중요한 것은, 비트넷 b1.58은 3B 크기로부터 시작하는 완전 정밀 기준선의 성능과 일치할 수 있다는 것이다. 퍼플렉시티의 관찰과 유사하게, 엔드-태스크 결과는 비트넷 b1.58 3.9B가 더 낮은 메모리 및 레이턴시 비용으로 LLaMA LLM 3B보다 우수함을 드러낸다. 이는 BitNet b1.58이 최신 LLM 모델에 비해 파레토 개선임을 보여준다.\n' +
      '\n' +
      '각주 4: [https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)\n' +
      '\n' +
      '메모리 및 Latency 우리는 모델 크기를 7B, 13B 및 70B로 추가로 확장하고 비용을 평가했다. 그림 2는 지연 시간과 메모리의 추세를 보여주며 모델 크기가 스케일링될수록 속도가 증가한다는 것을 보여준다. 특히 BitNet b1.58 70B는 LLaMA LLM 기준선보다 4.1배 빠르다. 이는 모델 크기에 따라 _nn.Linear_에 대한 시간 비용이 커지기 때문이다. 임베딩이 완전한 정밀도를 유지하고 더 큰 모델에 대해 메모리 비율이 더 작기 때문에 메모리 소비는 유사한 경향을 따른다. 레이턴시와 메모리는 모두 2비트 커널로 측정되었기 때문에 비용을 더 줄이기 위한 최적화의 여지가 여전히 있다.\n' +
      '\n' +
      '에너지 우리는 또한 비트넷 b1.58과 LLaMA LLM 모두의 산술 연산 에너지 소비를 추정한다. 우리는 행렬 곱셈의 계산이 LLMs의 비용에 가장 많이 기여하기 때문에 주로 행렬 곱셈에 대한 계산에 중점을 둔다. 그림 3은 에너지 비용의 구성을 보여준다. BitNet b1.58의 대부분은 INT8 덧셈 계산인 반면 LLaMA LLM은 FP16 덧셈과 FP16 곱셈으로 구성된다. [12, 2]의 에너지 모델에 따르면, 비트넷 b1.58은 7nm 칩에서 행렬 곱셈을 위해 71.4배의 산술 연산 에너지 소비를 절약한다. 512개의 토큰을 가진 모델에 대한 종단 간 에너지 비용을 추가로 보고했다. 우리의 결과는 모델 크기가 스케일링됨에 따라 비트넷 b1.58이 FP16 LLaMA LLM 기준선에 비해 에너지 소비 측면에서 점점 더 효율적이 된다는 것을 보여준다. 이는 모델 크기에 따라 _nn.Linear_의 비율이 증가하는 반면, 다른 구성 요소로부터의 비용은 더 큰 모델에 대해 더 작기 때문이다.\n' +
      '\n' +
      'throughputWe compare the throughput of BitNet b1.58 and LLaMA LLM with 70B parameters on two 80GB A100 card, using pipeline parallelism [1] to be run on the devices. GPU 메모리 제한에 도달할 때까지 배치 크기를 증가시켰으며 시퀀스 길이는 512이다. 표 3은 BitNet b1.58 70B가 LLaMA LLM의 배치 크기의 최대 11배까지 지원할 수 있어 처리량이 8.9배 더 높다는 것을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Models** & **Size** & **Max Batch Size** & **Throughput (tokens/s)** \\\\ \\hline LLaMA LLM & 70B & 16 (1.0x) & 333 (1.0x) \\\\\n' +
      '**BitNet b1.58** & 70B & **176 (11.0x)** & **2977 (8.9x)** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: BitNet b1.58 70B와 LLaMA LLM 70B의 처리량 비교.\n' +
      '\n' +
      '도 2: 모델 크기를 가변하는 BitNet b1.58의 디코딩 레이턴시(Left) 및 메모리 소모량(Right)\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n' +
      '### LLMs에서 긴 시퀀스의 고유 지원\n' +
      '\n' +
      'LLM 시대에 긴 시퀀스를 처리할 수 있는 능력은 중요한 수요가 되었다. 긴 시퀀스 추론을 위한 한 가지 주요 과제는 KV 캐시에 의해 도입된 메모리 소비이다. 비트넷 b1.58은 16비트에서 8비트로 활성화를 줄여 동일한 리소스가 주어지면 컨텍스트 길이가 두 배로 증가할 수 있기 때문에 긴 시퀀스에 대한 네이티브 지원을 향한 중요한 단계를 나타낸다. 이것은 1.58비트 LLM에 대해 4비트 또는 심지어 더 낮은 무손실 압축될 수 있으며, 이는 우리가 향후 작업으로 남긴다.\n' +
      '\n' +
      'Edge and Mobile에서의### LLMs.\n' +
      '\n' +
      '1.58 비트 LLM의 사용은 에지 및 모바일 장치에서 언어 모델의 성능을 크게 향상시킬 수 있는 잠재력을 가지고 있다. 이러한 장치는 종종 메모리와 계산 능력에 의해 제한되며, 이는 LLM의 성능과 규모를 제한할 수 있다. 그러나, 1.58-비트 LLM들의 감소된 메모리 및 에너지 소비는 이들이 이들 디바이스들에 배치될 수 있게 하여, 이전에 가능하지 않았던 광범위한 애플리케이션들을 가능하게 한다. 이것은 에지 및 모바일 장치의 능력을 크게 향상시킬 수 있고 LLM의 새롭고 흥미로운 애플리케이션을 가능하게 할 수 있다. 또한, 1.58 비트 LLM은 에지 및 모바일 장치에서 사용되는 메인 프로세서인 CPU 장치에 더 친숙하다. 이것은 비트넷 b1.58이 이들 디바이스 상에서 효율적으로 실행될 수 있어서, 이들의 성능 및 능력을 더욱 향상시킬 수 있다는 것을 의미한다.\n' +
      '\n' +
      '1비트 LLM을 위한 새로운 하드웨어\n' +
      '\n' +
      'Groq5와 같은 최근 작업은 LLM을 위한 특정 하드웨어(예: LPU)를 구축할 수 있는 유망한 결과와 큰 잠재력을 보여주었다. 한 단계 더 나아가, 우리는 비트넷에서 활성화된 새로운 계산 패러다임을 고려할 때 1비트 LLM에 특별히 최적화된 새로운 하드웨어 및 시스템을 설계하기 위한 동작을 구상하고 요청한다[23].\n' +
      '\n' +
      '각주 5: [https://groq.com/](https://groq.com/)\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[BZB\\({}^{+}\\)19] 요나탄 비스크, 로완 젤러스, 로난 르 브라스, 지안펑 가오, 최예진. PIQA: 자연 언어에서 물리적 상식에 대한 추론 _ CoRR_, abs/1911.11641, 2019.\n' +
      '* [CCKS23] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 보장성을 갖는 대형 언어 모델의 2비트 양자화. _ CoRR_, abs/2307.13304, 2023.\n' +
      '* [CLC\\({}^{+}\\)19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova. 불크: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구합니다. _ CoRR_, abs/1905.10044, 2019.\n' +
      '* [Com23] Together Computer. 레드파자마: 2023년 대규모 언어 모델을 훈련하기 위한 공개 데이터세트.\n' +
      '* [FAHA23] 엘리아스 프란타, 살레 애쉬크보스, 토스텐 호플러, 댄 알리스타. OPTQ: 생성 사전 훈련된 변압기에 대한 정확한 양자화. _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '*[HCB\\({}^{+}\\)19] Yanping Huang, Yulong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, Hyouk Middleong Lee, Jiquan Ngiam, Quoc V. 르, 용희우, 지펑첸 Gpipe: 파이프라인 병렬성을 이용한 거대 신경망의 효율적인 훈련. _Advances in Neural Information Processing Systems_, pages 103-112, 2019.\n' +
      '* [Hor14] Mark Horowitz. 1.1 컴퓨팅의 에너지 문제(그리고 우리가 그것에 대해 할 수 있는 것) In _2014 IEEE International Conference on Solid-State Circuits Conference, ISSCC 2014, Digest of Technical Papers, San Francisco, CA, USA, February 9-13, 2014_, pages 10-14, 2014.\n' +
      '*[KLZ\\({}^{+}\\)23] 우석권, 주한리, 시위안장, 영성, 리안민정, 코디 하오유, 조셉 E. 곤잘레스, 하오장, 이온스토이카. 페이지어텐션으로 서비스를 제공하는 대용량 언어 모델을 위한 효율적인 메모리 관리 In _Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles_, 2023.\n' +
      '\n' +
      '*[LTT\\({}^{+}\\)23] 지린, 지밍탕, 하오톈탕, 상양, 신규당, 송한. AWQ: LLM 압축 및 가속을 위한 활성화 인식 가중치 양자화_ CoRR_, abs/2306.00978, 2023.\n' +
      '* [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot 및 Ashish Sabharwal. 갑옷이 전기를 통할 수 있나요? 열린 책 질문 응답을 위한 새 데이터 집합입니다. _ CoRR_, abs/1809.02789, 2018.\n' +
      '* [MXBS16] Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher. 포인터 센티넬 혼합 모델 2016년\n' +
      '*[PKL\\({}^{+}\\)16] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, 그리고 Raquel Fernandez. LAMBADA 데이터세트: 광범위한 담화 컨텍스트를 요구하는 단어 예측. _Proceedings of the 54th Annual Meeting of the Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers_. 2016년 컴퓨터 언어학 협회\n' +
      '*[RSR\\({}^{+}\\)19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 단일 텍스트-텍스트 변환기를 이용한 전이학습의 한계점 탐색 CoRR_, abs/1910.10683, 2019.\n' +
      '*[SAL\\({}^{+}\\)24] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 로포머: 회전 위치 매립을 갖는 향상된 트랜스포머. _ Neurocomputing_, 568:127063, 2024.\n' +
      '* [SBBC20] 사카구치 게이스케, 로난 르 브라스, 찬드라 바가바툴라, 최예진. 위노 그란데: 적의 윈그라드 스키마가 스케일에서 도전한다. 인공지능에 관한 제34차 AAAI 회의에서, 2020년 페이지 8732-8740.\n' +
      '*[Sha20] Noam Shazeer. GLU 변형은 변압기를 개선한다. _ CoRR_, abs/2002.05202, 2020.\n' +
      '[TBMR] 조나단 토우, 마르코 벨라젠테, 다코타 마한, 카를로스 리켈메. 안정화 3b 4e1t.\n' +
      '* [TCS\\({}^{+}\\)24] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#:Hadamard 비간섭성 및 격자 코드북을 사용한 LLM 양자화가 더욱 우수하다. _ CoRR_, abs/2402.04396, 2024.\n' +
      '*[TLI\\({}^{+}\\)23] Hugo Touvron, Thibaut Lavril, Gautier Izzard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: 개방적이고 효율적인 기초 언어 모델들_ CoRR_, abs/2302.13971, 2023.\n' +
      '*[TMS\\({}^{+}\\)23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, and et al. 2: open foundation and finetuned chat models. _ CoRR_, abs/2307.09288, 2023.\n' +
      '[WLG17] 요하네스 웰블, 넬슨 F. 류, 맷 가드너. 다중 선택 과학 질문을 크라우드소싱합니다. Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, Editors, _Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017_, pages 94-106. Association for Computational Linguistics, 2017.\n' +
      '*[WMC\\({}^{+}\\)23] Lei Wang, Lingxiao Ma, Shijie Cao, Ningxin Zheng, Quanlu Zhang, Jilong Xue, Ziming Miao, Ting Cao, and Yuqing Yang. 래더: 사용자 정의된 데이터 형식에 대한 효율적인 텐서 컴파일 _OSDI_, 2023.\n' +
      '*[WMD\\({}^{+}\\)23] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu Wei. 비트넷: 대규모 언어 모델을 위한 1비트 변압기의 스케일링 _ CoRR_, abs/2310.11453, 2023.\n' +
      '\n' +
      '*[XLS\\({}^{+}\\)23] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: 대형 언어 모델에 대한 정확하고 효율적인 훈련 후 양자화. _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, 2023.\n' +
      '[YBS19] 비카스 야다브, 스티븐 베서드, 미하이 수르데아누. 빠르고 더럽다: 멀티홉 질의응답을 위한 정당화 문장의 감독되지 않은 선택. 이누이 켄타로, 징장, 빈센트 응, 완샤오준에서는 편집자, _EMNLP-IJCNLP_, 2019.\n' +
      '*[ZHB\\({}^{+}\\)19] Rowan Zellers, Ari Holtzman, 요나탄 Bisk, Ali Farhadi, Yejin Choi. 헬라 스웨그: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? _Proceedings of the 57th Conference of the Computational Linguistics_, pages 4791-4800, 2019.\n' +
      '* [ZS19] 바이오 장과 리코 센리히. 루트 평균 제곱 계층 정규화 한나 M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\'Alche-Buc, Emily B. Fox, and Roman Garnett, Editors, _Advances in Neural Information Processing Systems_, pages 12360-12371, 2019.\n' +
      '*[ZZL22] Yichi Zhang, Zhiru Zhang, and Lukasz Lew. PokeBNN: 경량 정확도의 바이너리 추구. _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12465-12475. IEEE, 2022.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>