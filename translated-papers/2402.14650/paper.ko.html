<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#GaussianPro: 3D Gaussian Splatting with Proressive Propagation\n' +
      '\n' +
      ' 카이청\n' +
      '\n' +
      '동등한 기여도 \\({}^{1}\\)중국과학기술대학 \\({}^{2}\\)홍콩대학 \\({}^{3}\\)남경대학 \\({}^{4}\\)애들레이드대학 \\({}^{5}\\)상하이테크대학 \\({}^{6}\\)텍사스 A&M대학 대응: Xuejin Chen \\(<\\)xjchen99@ustc.edu.cn\\(>\\)\n' +
      '\n' +
      ' 샤오샤오롱\n' +
      '\n' +
      '동등한 기여도 \\({}^{1}\\)중국과학기술대학 \\({}^{2}\\)홍콩대학 \\({}^{3}\\)남경대학 \\({}^{4}\\)애들레이드대학 \\({}^{5}\\)상하이테크대학 \\({}^{6}\\)텍사스 A&M대학 대응: Xuejin Chen \\(<\\)xjchen99@ustc.edu.cn\\(>\\)\n' +
      '\n' +
      ' 양광기\n' +
      '\n' +
      '동등한 기여도 \\({}^{1}\\)중국과학기술대학 \\({}^{2}\\)홍콩대학 \\({}^{3}\\)남경대학 \\({}^{4}\\)애들레이드대학 \\({}^{5}\\)상하이테크대학 \\({}^{6}\\)텍사스 A&M대학 대응: Xuejin Chen \\(<\\)xjchen99@ustc.edu.cn\\(>\\)\n' +
      '\n' +
      ' 요야오\n' +
      '\n' +
      '동등한 기여도 \\({}^{1}\\)중국과학기술대학 \\({}^{2}\\)홍콩대학 \\({}^{3}\\)남경대학 \\({}^{4}\\)애들레이드대학 \\({}^{5}\\)상하이테크대학 \\({}^{6}\\)텍사스 A&M대학 대응: Xuejin Chen \\(<\\)xjchen99@ustc.edu.cn\\(>\\)\n' +
      '\n' +
      ' 위인\n' +
      '\n' +
      'Yuexin Ma\n' +
      '\n' +
      '동등한 기여도 \\({}^{1}\\)중국과학기술대학 \\({}^{2}\\)홍콩대학 \\({}^{3}\\)남경대학 \\({}^{4}\\)애들레이드대학 \\({}^{5}\\)상하이테크대학 \\({}^{6}\\)텍사스 A&M대학 대응: Xuejin Chen \\(<\\)xjchen99@ustc.edu.cn\\(>\\)\n' +
      '\n' +
      ' 왕웬핑\n' +
      '\n' +
      '텍사스 A&M 대학교 대응: Xuejin Chen \\(<\\)xjchen99@ustc.edu.cn\\(>\\)\n' +
      '\n' +
      'Xuejin Chen\n' +
      '\n' +
      '동등한 기여도 \\({}^{1}\\)중국과학기술대학 \\({}^{2}\\)홍콩대학 \\({}^{3}\\)남경대학 \\({}^{4}\\)애들레이드대학 \\({}^{5}\\)상하이테크대학 \\({}^{6}\\)텍사스 A&M대학 대응: Xuejin Chen \\(<\\)xjchen99@ustc.edu.cn\\(>\\)\n' +
      '\n' +
      'Xuejin Chen\n' +
      '\n' +
      '동등한 기여도 \\({}^{1}\\)중국과학기술대학 \\({}^{2}\\)홍콩대학 \\({}^{3}\\)남경대학 \\({}^{4}\\)애들레이드대학 \\({}^{5}\\)상하이테크대학 \\({}^{6}\\)텍사스 A&M대학 대응: Xuejin Chen \\(<\\)xjchen99@ustc.edu.cn\\(>\\)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '3DGaussian Splatting(3DGS)의 출현은 최근 신경 렌더링 분야에서 혁명을 일으켜 실시간 속도로 고품질 렌더링을 가능하게 했다. 그러나 3DGS는 SfM(Structural-from-Motion) 기법에 의해 생성된 초기화된 포인트 클라우드에 크게 의존한다. 어쩔 수 없이 질감이 없는 표면을 포함하는 대규모 장면으로 태클할 때 SfM 기술은 항상 이러한 표면에서 충분한 포인트를 생성하지 못하고 3DGS에 대한 좋은 초기화를 제공할 수 없다. 결과적으로 3DGS는 어려운 최적화 및 낮은 품질의 렌더링을 겪는다. 본 논문에서는 고전적인 다시점 스테레오(multi-view stereo, MVS) 기법에서 착안하여 3차원 가우시안(Gaussians)의 치밀화를 유도하기 위해 점진적 전파 전략을 적용하는 새로운 방법인 가우시안 프로(GaussianPro)를 제안한다. 제안된 방법은 3DGS에 사용되는 단순 분할 및 복제 전략과 비교하여 기존의 재구성된 장면의 기하학적 구조와 패치 매칭 기법을 활용하여 정확한 위치와 방향을 가진 새로운 가우시안들을 생성한다. 본 논문에서 제안한 방법은 웨이모 데이터 셋에서 3DGS를 크게 능가하여 PSNR 측면에서 1.15dB의 향상된 성능을 보였다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '새로운 뷰 합성은 캡처된 장면에서 새로운 시점의 이미지를 생성하는 것을 목표로 하는 컴퓨터 비전 및 컴퓨터 그래픽에서 중요하지만 도전적인 작업이다. 가상현실(Deng et al., 2022), 자율주행(Yang et al., 2023; Cheng et al., 2023), 3D 콘텐츠 생성(Poole et al., 2022; Tang et al., 2023) 등 다양한 영역에서 광범위하게 응용되고 있다. 최근, Neural Radiance Fields (NeRF)(Mildenhall et al., 2020)는 3D 장면들, 텍스처 및 조명을 명시적으로 모델링하지 않고 고-충실도 렌더링을 달성하면서, 이 작업을 상당히 부스팅하였다. 그러나, 볼륨 렌더링의 무거운 방식으로 인해, NeRFs는 다양한 노력들(Muller et al., 2022; Barron et al., 2022; Chen et al., 2022; Xu et al., 2022)이 이루어졌지만 여전히 느린 렌더링 속도에 시달리고 있다.\n' +
      '\n' +
      '실시간 신경렌더링을 위해 3DGaussian Splatting (3DGS) (Kerbl et al., 2023)이 개발되었다. 스플랫팅 기법은 시간 소모적인 광선 샘플링을 피하고 병렬 계산을 허용하여 높은 효율과 빠른 렌더링을 가능하게 한다. 3DGS는 SfM(structure-from-Motion) 기법에 의해 생성된 희소점 구름에 크게 의존하여 Gaussians을 초기화하는 기법, 예를 들어, 그들의 위치, 색상, 그리고 모양을 초기화하는 기법, 그리고 3DGaussians의 완전한 커버리지를 얻기 위해 더 많은 새로운 Gaussians을 생성하기 위한 복제 및 분할 전략, 그리고 3DGaussians의 완전한 커버리지를 얻기 위해 더 많은 새로운 Gaussians을 생성하기 위한 두 가지 주요 제한으로 이어진다. **기존의 재구성된 기하학의 이전을 무시하라.** 새로운 가우시안들은 이전의 가우시안들과 동일하게 복제되거나 임의의 위치 및 배향으로 초기화된다. 제한이 적은 고밀도화는 3D 가우시안(예를 들어, 잡음이 많은 기하학적 구조)의 최적화에 어려움을 초래하고 텍스쳐가 없는 영역에서는 가우시안(Gaussians)이 거의 없어 최종적으로 렌더링 품질을 저하시킨다. 그림 1과 같이 3DGS의 결과에는 잡음이 많은 가우시안들이 많이 포함되어 있고 일부 영역은 충분한 가우시안들로 가려지지 않는다.\n' +
      '\n' +
      '본 논문에서는 3DGS를 용이하게 하기 위한 새로운 점진적 전파 전략을 제안하며, 이는 보다 컴팩트하고 정확한 3D 가우시안(Gaussians)을 생성할 수 있고, 따라서 특히 텍스쳐가 없는 표면에서 렌더링 품질을 향상시킬 수 있다. 이 방법의 핵심 아이디어는 재구성된 장면 기하학을 사전 및 고전적 패치 매칭 기술로 완전히 활용하여 정확한 위치와 방향을 가진 새로운 가우시안들을 점진적으로 생성하는 것이다.\n' +
      '\n' +
      '특히, 3차원 세계 공간과 2차원 영상 공간 모두에서 가우시안 치밀화를 고려한다. 각 입력 영상에 대해 알파 블렌딩을 통해 3D 가우시안들의 위치와 방향을 누적하여 깊이와 정규 지도를 렌더링한다. 이웃하는 픽셀들이 유사한 깊이 및 정규 값들을 공유할 가능성이 있다는 관찰에 기초하여, 픽셀에 대해, 우리는 후보들의 세트를 공식화하기 위해 이웃 픽셀들의 깊이 및 정규 값들을 이 픽셀에 반복적으로 전파한다. 이 후보들을 이용하여 기존의 패치 매칭 기법을 이용하여 다시점 측광 일관성 제약 조건을 만족하는 최적의 후보들을 선택하여 각 픽셀에 대해 새로운 깊이 및 정규(전파 깊이/정규)을 얻는다. 기존의 3차원 가우시안(Gaussians)들이 실제 기하학을 정확하게 포착하지 못한다는 것을 의미하기 때문에, 전파된 깊이가 렌더링된 깊이와 크게 다른 픽셀들을 선택한다. 그 결과, 전파된 깊이를 이용하여 선택된 픽셀을 3차원 공간으로 명시적으로 역투영하여 새로운 가우시안(Gaussian)으로 초기화한다. 또한, 전파된 정규식을 이용하여 3차원 가우시안들의 방향을 정규화하여 재구성된 3차원 기하학과 렌더링 품질을 더욱 향상시킨다.\n' +
      '\n' +
      '제안된 점진적 전파 전략은 잘 모델링된 영역에서 모델링되지 않은 영역으로 정확한 기하학적 정보를 전달함으로써 보다 작고 정확한 3D 가우시안들을 생성할 수 있다. 도면에 도시된 바와 같이. 본 논문에서 제안한 방법은 3DGS와 비교하여 더 정확하고 컴팩트한 가우시안(Gaussians)을 생성하므로 3D 장면의 커버리지를 향상시킨다. Waymo 및 MipNeRF360과 같은 공개 데이터 세트에 대한 실험은 제안된 전략이 3DGS의 성능을 크게 향상시킨다는 것을 검증한다. 전반적으로, 우리의 방법의 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '* 우리는 특히 질감이 낮은 영역에서 더 작고 정확한 가우시안들을 생성하기 위해 치밀화를 안내하는 새로운 가우시안 전파 전략을 제안한다.\n' +
      '* 우리는 가우시안 최적화에서 추가적인 제약을 제공하는 평면 손실을 추가적으로 활용한다.\n' +
      '* 우리의 방법은 Waymo 및 MipNeRF360 데이터 세트에서 새로운 최첨단 렌더링 성능을 달성한다. 또한 다양한 입력 영상에 대한 강인성을 보인다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### Multi-view Stereo\n' +
      '\n' +
      '멀티뷰 스테레오(MVS)는 포즈된 이미지들의 집합으로부터 3D 모델을 재구성하는 것을 목표로 하며, 이는 새로운 뷰들을 생성하기 위해 전통적인 렌더링 알고리즘들과 더 결합될 수 있다. 전통적인 방법들(Campbell et al., 2008; Furukawa and Ponce, 2009; Bleyer et al., 2011; Furukawa et al., 2015; Schonberger et al., 2016; Xu and Tao, 2019)은 손으로 조작된 이미지 특징들에 기초하여 이미지들 사이의 픽셀 대응들을 명시적으로 확립한 다음, 이미지들 중에서 최상의 픽셀 대응들을 달성하기 위해 3D 구조를 최적화한다. 학습 기반 MVS 방법(Yao et al., 2018; Vakalopoulou et al., 2018; Long et al., 2020; Chen et al., 2019; Long et al., 2021; Ma et al., 2022; Long et al., 2022; Feng et al., 2023)은 종단간 프레임워크에서 특징들에 기초하여 학습 가능한 특징들과 퇴보 깊이 또는 3D 볼륨을 갖는 멀티뷰 대응들을 암시적으로 구축한다. 본 논문에서는 가우시안 기하학을 개선하기 위해 MVS의 깊이 최적화로부터 영감을 얻어 더 나은 렌더링 결과를 얻는다.\n' +
      '\n' +
      '신경 복사 필드\n' +
      '\n' +
      'NeRF는 3D 볼륨 표현과 딥러닝 기술을 결합하여 3D 장면을 학습 가능한 연속 밀도장으로 변환한다. 볼륨 렌더링에서 광선 정합을 이용하여, NeRF는 3D 장면과 조명의 명시적인 모델링 없이 고품질의 새로운 뷰 합성을 달성할 수 있다. 렌더링 품질을 더욱 향상시키기 위해, 일부 접근법들(Barron et al., 2021; Xu et al., 2022; Barron et al., 2023)은 볼륨 렌더링 프로세스의 보다 정확한 모델링을 위해 광선 행칭에서의 포인트 샘플링 전략을 직접적으로 개선한다. 다른 것들(Barron et al., 2022; Wang et al., 2023)은 더 컴팩트한 장면 표현 및 더 쉬운 학습 프로세스를 생성하기 위해 장면을 재매개화함으로써 렌더링을 개선한다. 또한, 정칙화 용어들(Deng et al., 2022; Yu et al., 2022)은 실제 기하학의 더 가까운 근사치를 향해 장면 표현을 제약하기 위해 도입될 수 있다. 이러한 발전에도 불구하고, NeRF는 렌더링 동안 여전히 높은 계산 비용을 발생시킨다. NeRF는 장면을 나타내기 위해 MLP를 채용하기 때문에, 장면 내의 임의의 포인트의 계산 및 최적화는 전체 MLP에 의존한다. 많은 연구들은 렌더링을 가속화하기 위해 새로운 장면 표현을 제안한다. 그들은 MLP들을 희소 복셀들(Liu et al., 2020; Fridovich-Keil et al., 2022), 해시 테이블들(Muller et al., 2022), 또는 트라이플레인(Chen et al., 2022)로 대체함으로써, 각 포인트의 계산 및 최적화가 장면의 대응하는 로컬 영역에 국부화될 수 있게 한다. 이러한 방법은 렌더링 속도를 크게 향상시키지만 볼륨 렌더링의 고유한 광선 행칭 전략으로 인해 실시간 렌더링은 여전히 어렵다.\n' +
      '\n' +
      '###3D Gaussian Splatat\n' +
      '\n' +
      '3DGS는 2D 스크린 상에 이방성 3D 가우시안들을 투영하기 위해 스패팅-기반 래스터화(Zwicker et al., 2002) 접근법을 채용한다. 투영된 2차원 가우시안들에 대해 깊이 정렬과 \\(\\alpha\\)-블렌딩을 수행하여 픽셀의 색상을 계산하며, 이는 광선 행칭의 정교한 샘플링 전략을 피하고 실시간 렌더링을 달성한다. 일부 동시 작업은 3DGS를 개선했다. 첫째, 3DGS는 샘플링 주파수에 민감하며, 즉 카메라의 초점 거리나 카메라 거리를 변경하면 렌더링 아티팩트가 발생할 수 있다. 이러한 아티팩트는 저역 통과 필터링(Yu et al., 2023) 또는 다중 스케일 가우시안 표현(Yan et al., 2023)을 도입함으로써 해결된다. 또한, 3DGS는 장면의 실제 기하학적 구조를 명시적으로 제약하지 않고 가우시안들을 과도하게 성장시켜 수많은 중복 가우시안들과 상당한 기억 소비를 초래한다. 일부 방법들은 스케일에 의해 렌더링에 대한 가우시안들의 기여도를 평가하거나(Lee et al., 2023), 뷰들에서 가시성을 계산함으로써(Fan et al., 2023), 작은 기여도로 가우시안들을 제거하도록 강제한다. 다른 것들은 양자화 기법에 의해 가우시안 속성들의 저장을 압축한다(Navaneet et al., 2023). 또는 구조화된 그리드 피처들로부터 가우시안 속성들을 보간한다(Morgenstern et al., 2023; Lu et al., 2023).\n' +
      '\n' +
      '이러한 방법들은 가우시안들의 저장 오버헤드를 상당히 감소시키지만, 가우시안들의 기하학적 구조를 명시적으로 제한하지는 않는다. 3DGS는 실제 표면에서 멀리 떨어진 위치에서 다른 훈련 뷰에 맞게 성장할 수 있으며, 이로 인해 중복성이 발생하고 새로운 뷰에 대한 렌더링 품질이 저하될 수 있다. 이 논문은 실제 표면에 가까운 가우시안들의 성장을 명시적으로 제한하는 장면에서의 평면 이전을 고려한다. 이 접근법은 가우시안들이 장면의 실제 기하학에 더 잘 맞도록 하여 향상된 렌더링 및 보다 컴팩트한 표현을 달성할 수 있게 한다.\n' +
      '\n' +
      '## 3 Preliminaries\n' +
      '\n' +
      '3DGS(Kerbl et al., 2023)는 3D 장면을 이방성 3D 가우시안들의 세트로서 모델링하고, 이는 스플래팅-기반 래스터화 기법을 사용하여 이미지들에 추가로 렌더링된다(Zwicker et al., 2002). 각각의 3D 가우시안 \\(G\\)에 대해, 다음과 같이 정의된다:\n' +
      '\n' +
      '[G(\\mathbf{x})=e^{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\boldsymbol{ \\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}), \\tag{1}\\tag{1}}\n' +
      '\n' +
      '여기서 \\(\\boldsymbol{\\mu}\\in\\mathbb{R}^{3\\times 1}\\)은 평균 벡터를 의미하고 \\(\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{3\\times 3}\\)은 공분산 행렬을 의미한다. 최적화 과정에서 공분산 행렬의 정의 반정의 성질을 보장하기 위해, 회전 행렬 \\(\\mathbf{R}\\in\\mathbbb{R}^{3\\times 3}\\)이 직교하고, 스케일 행렬 \\(\\mathbf{S}\\in\\mathbb{R}^{3\\times 3}\\)이 대각선인 \\(\\boldsymbol{\\Sigma}=\\mathbf{RSS}^{T}\\mathbf{R}^{T}\\)으로 표현된다.\n' +
      '\n' +
      '주어진 시점에서 영상을 렌더링하기 위해, 각 픽셀의 색상은 \\(N\\) 정렬된 가우시안 \\(\\{G_{i}\\mid i=1,\\cdots,N\\}\\)과 겹치는 \\(\\mathbf{p}\\)을 블렌딩하여 계산된다.\n' +
      '\n' +
      '\\mathbf{c}(\\mathbf{p})=\\sum_{i=1}^{N}\\mathbf{c}_{i}\\alpha_{i}\\prod_{j=1}^{i-1} \\left(1-\\alpha_{j}\\right), \\tag{2}\\tag{c}\\sum_{i=1}^{N}\\mathbf{c}_{i}\\alpha_{i}\\prod_{j=1}^{i-1}\\left(1-\\alpha_{j}\\right)\n' +
      '\n' +
      '여기서 \\(\\alpha_{i}\\)는 \\(\\mathbf{p}\\)에 학습된 불투명도를 곱한 \\(G_{i}\\)의 \\(G_{i}\\)에서 투영된 2D 가우시안(Zwicker et al., 2002)을 평가하여 구하며, \\(\\mathbf{c}_{i}\\)은 \\(G_{i}\\)의 학습 가능한 색상이다. 현재 시점에서 \\(\\mathbf{p}\\)를 커버하는 가우시안들은 깊이의 오름차순으로 정렬된다. 미분 가능한 렌더링 기법을 통해 가우시안들의 모든 속성을 학습 뷰 재구성을 통해 종단 간 최적화할 수 있다.\n' +
      '\n' +
      '장면 기하학을 정확하게 표현하기 위해, 3DGS는 또한 새로운 가우시안들을 생성하기 위해 치밀화 전략을 사용한다. 각각의 트레이닝 반복에 대해, 렌더링 손실로부터 현재 가우시안으로의 역전파된 그래디언트가 특정 임계치를 초과하면, 3DGS는 그것이 대응하는 3D 영역을 충분히 나타내지 않는다고 고려한다. 가우시안 공분산이 크면 두 개의 가우시안들로 분할된다. 반대로 공분산이 작으면 복제된다. 이 전략은 3DGS가 캡처된 장면을 커버하기 위해 가우시안 수를 늘리도록 장려한다.\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      '본 논문에서는 정확한 위치 및 방향을 갖는 3차원 가우시안(Gaussians)을 명시적으로 생성하여 렌더링 품질과 압축성을 향상시키는 새로운 점진적 전파 전략을 제안한다. 첫째, 3차원 공간과의 결합 대신에 3차원 공간과 2차원 영상 공간 모두에서 이 문제를 해결할 것을 제안한다. 3차원 가우시안(Gaussians)을 2차원 공간에 투영하여 깊이와 정규 지도를 생성하는데, 이는 가우시안(Sec. 4.2)의 성장을 유도하는 데 사용된다. 그런 다음, 이웃한 픽셀들로부터 전파된 픽셀을 기반으로 각 픽셀의 깊이와 법선을 반복적으로 업데이트한다. 새로운 깊이가 초기 깊이와 크게 다른 픽셀은 3D 포인트로 3D 공간으로 다시 투영되고 이러한 포인트는 새로운 가우시안(Sec.4.3)으로 추가로 초기화된다. 또한, 평면 손실은 가우시안 기하학을 더욱 정규화하기 위해 통합되어 보다 정확한 기하학(Sec. 4.4)을 산출한다. 전반적인 훈련 전략은 4.5절에 도입되었다.\n' +
      '\n' +
      '### 하이브리드 기하 표현\n' +
      '\n' +
      '이 절에서는 3차원 가우시안과 2차원 뷰에 의존하는 깊이 및 정규 지도를 결합한 하이브리드 기하학적 표현을 제안하며, 여기서 2차원 표현은 가우시안들의 치밀화를 돕기 위해 사용된다.\n' +
      '\n' +
      '3차원 가우시안들의 이산적이고 불규칙한 토폴로지로 인해, 국부적인 표면에서 이웃하는 가우시안들을 탐색하는 것과 같이 기하학의 연결성을 지각하는 것은 불편하다. 그 결과 가우시안 치밀화를 유도하기 위해 기존의 기하학을 인지하기가 어렵다. 고전적인 MVS 방법에 영감을 받아 3D 가우시안들을 구조화된 2D 이미지 공간으로 매핑하여 이 문제를 해결할 것을 제안한다. 이 매핑을 통해 가우시안 이웃을 효율적으로 결정하고 그 사이에 기하학적 정보를 전파할 수 있다. 구체적으로, 가우시안들이 3차원 공간에서 동일한 국부 평면에 위치할 때, 그들의 2차원 투영들은 또한 인접한 영역들에 있어야 하며, 유사한 기하학적 특성들, 즉 깊이 및 법선을 나타내야 한다.\n' +
      '\n' +
      '가우시안(Gaussian)의 깊이 값은 카메라 extrinsics \\([\\mathbf{W},\\mathbf{t}]\\in\\mathbb{R}^{3\\times 4}\\)을 갖는 각 시점에 대해 가우시안 \\(G_{i}\\)의 중심 \\(\\boldsymbol{\\mu}_{i}\\)을 카메라 좌표계에 \\(\\boldsymbol{\\mu}_{i}^{\\prime}\\)으로 투영할 수 있다:\n' +
      '\n' +
      '\\boldsymbol{\\mu}_{i}^{\\prime}=\\left[\\begin{array}{c}x_{i}\\\\y_{i}\\\\z_{i}\\end{array}\\right]=\\mathbf{W}\\boldsymbol{\\mu}_{i}+\\mathbf{t}, \\tag{3}\\t}\n' +
      '\n' +
      '여기서 \\(z_{i}\\)는 현재 시점에서의 가우시안 깊이를 의미한다.\n' +
      '\n' +
      '가우시안(G_{i}\\)에서 공분산 행렬은 \\(\\boldsymbol{\\Sigma}_{i}=\\mathbf{R}_{i}\\mathbf{S}_{i}\\mathbf{S}_{i}^{T}\\mathbf{R}_{i}^{T}\\mathbf{R}_{i}^{T}\\mathbf{R}_{i}\\mathbf{R}_{i}\\mathbf{R}_{i}}^{T}\\mathbf{R}_{i}\\mathbf{R}_{i}\\mathbf{R}_{i}\\mathbf{R}_{i}\\mathbf{R}_{i}\\mathbf{R}_{i}\\mathbf{R}_{i}\\mathbf{R}_{i}\\mathbf{R}_{i}\\mathbf{R}_{i}\\mathbf{R} 회전 행렬 \\(\\mathbf{R}_{i}\\)은 세 개의 직교 고유벡터를 결정하고, 스케일링 행렬 \\(\\mathbf{S}_{i}\\in\\mathbb{R}^{3\\times 3}\\)은 고유벡터 방향을 따라 스케일을 결정한다. 3차원 가우시안 공분산 행렬 \\(\\boldsymbol{\\Sigma}_{i}\\)은 타원체의 형상을 나타내는 것과 비교될 수 있는데, 고유벡터는 타원체의 축에 해당하고 스케일은 축의 길이를 나타낸다. Gaussian-Shader(Jiang et al., 2023)에 따르면, 가우시안 구는 최적화 과정에서 점차 평평해지고 평면에 접근하게 된다. 따라서, 가장 짧은 축의 방향은 가우시안(Gaussian)의 법선 방향 \\(\\mathbf{n}_{i}\\)에 근사할 수 있다.\n' +
      '\n' +
      '\\[\\mathbf{n}_{i}=\\mathbf{R}_{i}[r,:],r=\\operatorname{argmin}\\left([s_{1},s_{2},s_{3}]\\right), \\tag{4}\\right)\n' +
      '\n' +
      '여기서 \\(diag(s_{1},s_{2},s_{3})=\\mathbf{S}_{i}\\), \\(\\operatorname{argmin}(\\cdot)\\)은 최소값의 인덱스를 구하는 연산이다.\n' +
      '\n' +
      '마지막으로, 현재 시점에서의 2차원 깊이와 정규 맵은 식에 정의된 \\(\\alpha\\)-블렌딩에 기초하여 렌더링된다. 2에서 속성색 \\(\\mathbf{c}_{i}\\)은 가우시안 깊이 \\(z_{i}\\)과 정규 \\(\\mathbf{n}_{i}\\)으로 대체된다.\n' +
      '\n' +
      '도 2: 가우시안의 점진적 전파. 먼저 3차원 가우시안으로부터 깊이와 정규 지도를 렌더링한다. 그런 다음 패치 매칭 기법을 통해 렌더링된 깊이 및 법선에 대한 전파 연산을 반복적으로 수행하여 새로운 깊이 및 법선 값(전파 깊이 및 전파 법선으로 표시됨)을 생성한다. 우리는 기하학적 일관성을 이용하여 신뢰할 수 없는 전파 깊이와 법선을 필터링하여 필터링된 깊이와 필터링된 법선을 산출한다. 마지막으로, 렌더링된 깊이와 정규선이 필터링된 영역에서 크게 벗어나는 영역을 식별하여 기존 가우시안들이 기하학을 부정확하게 캡처하지 않을 수 있으므로 더 많은 가우시안들이 필요함을 나타낸다. 이러한 영역의 픽셀은 필터링된 깊이와 법선을 사용하여 새로운 가우시안들을 초기화하기 위해 3D 공간에 투영된다.\n' +
      '\n' +
      '### 점진적 가우시안 전파\n' +
      '\n' +
      '이 섹션에서는 잘 모델링된 영역에서 모델링되지 않은 영역으로 정확한 기하학을 전파할 수 있는 점진적 가우시안 전파 전략을 도입하여 새로운 가우시안 생성을 가능하게 한다. 도 2에 도시된 바와 같이, 렌더링된 깊이 맵들 및 정규 맵들과 함께, 패치 매칭(Barnes et al., 2009)을 채용하여 이웃 픽셀들로부터 현재 픽셀로 깊이 및 정규 정보를 전파하고, 이는 새로운 깊이 및 정규(전파된 깊이/정규로 명명됨)들을 생성한다. 우리는 더 많은 가우시안들이 필요한 픽셀들을 선택하고 그들의 전파된 깊이와 정규식을 이용하여 새로운 가우시안들을 초기화하기 위해 기하학적 필터링 및 선택 연산을 추가로 수행한다.\n' +
      '\n' +
      '**평면 정의.** 전파를 달성하기 위해, 각 픽셀의 깊이 및 법선은 먼저 3D 로컬 평면으로 변환될 필요가 있다. 좌표 \\(\\mathbf{p}\\)를 갖는 각 픽셀에 대해, 대응하는 3D 로컬 평면은 \\((d,\\mathbf{n})\\)으로 파라미터화되며, 여기서 \\(\\mathbf{n}\\)은 픽셀의 정규 렌더링이고, \\(d\\)은 카메라 좌표의 원점에서 로컬 평면까지의 거리로서 계산된다:\n' +
      '\n' +
      '\\[d=z\\mathbf{n}^{\\top}\\mathbf{K}^{-1}\\widetilde{\\mathbf{p}}, \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(\\widetilde{\\mathbf{p}\\)는 \\(\\mathbf{p}\\)의 균질 좌표이고, \\(z\\)은 픽셀의 렌더링 깊이이며, \\(\\mathbf{K}\\)은 카메라 진성을 나타낸다.\n' +
      '\n' +
      '**후보 선택.** 3D 로컬 평면을 정의한 후, 각 픽셀의 이웃들이 전파를 위해 선택될 필요가 있다. 우리는 ACMH(Xu & Tao, 2019)에 정의된 바둑판 패턴을 따라 이웃 픽셀을 선택한다. 명확성을 위해 우리는 4개의 가장 가까운 픽셀을 가진 픽셀의 전파를 설명한다. 각 픽셀에 대해, 평면 후보들\\(\\{(d_{k_{l}},\\mathbf{n}_{k_{l}})\\mid l\\in\\{0,1,2,3,4\\},\\,\\text{는 전파}\\을 통해 얻어진다. (\\(k_{l}\\)은 픽셀\\(p\\)과 그 네 개의 이웃 픽셀들의 인덱스를 의미한다.\n' +
      '\n' +
      '**Patch Matching.** 평면 후보들을 구한 후, 패치 매칭을 통해 각 픽셀에 대한 최적의 평면을 결정한다. 좌표 \\(\\mathbf{p}\\)을 갖는 픽셀 \\(p\\)에 대해, 이웃 프레임에서 \\(\\mathbf{p}\\)에서 \\(\\mathbf{p}^{\\prime}\\)으로 뒤틀리는 각 평면 후보 \\((d_{k_{t}},\\mathbf{n}_{k_{t}})\\을 기반으로 호모그래피 변환 \\(\\mathbf{H}\\)을 수행한다.\n' +
      '\n' +
      '\\[\\widetilde{\\mathbf{p}^{\\prime}}\\simeq\\mathbf{H}\\widetilde{\\mathbf{p}}, \\tag{6}\\]\n' +
      '\n' +
      '여기서 \\(\\widetilde{\\mathbf{p}^{\\prime}\\)는 \\(\\mathbf{p}^{\\prime}\\)의 균질 좌표이고, \\(\\mathbf{H}\\)은 다음과 같이 유도될 수 있다:\n' +
      '\n' +
      '\\mathbf{H}=\\mathbf{K}\\left(\\mathbf{W}_{\\text{rel}}-\\frac{\\mathbf{t}_{\\text{rel}}\\mathbf{n}_{k_{l}}^{\\top}{d_{k_{l}}}\\right}\\mathbf{K}^{-1}, \\tag{7}\\trac}\n' +
      '\n' +
      '여기서 \\([\\mathbf{W}_{\\text{rel}},\\mathbf{t}_{\\text{rel}}]\\)는 기준 뷰에서 이웃 뷰로의 상대적 변환이다. 마지막으로 NCC(Normalized Cross Correlation)를 이용하여 \\(p\\)과 \\(p^{\\prime}\\)의 색상 일관성을 평가하였다 (Yoo & Han, 2009). 색상 일관성이 가장 좋은 평면후보로 \\(p\\)의 국부평면이 갱신될 것이다. 도. 또한, 도 3은 이러한 프로세스의 직관적인 시각화를 제공한다. 평면 후보에 대한 전파는 큰 영역에 걸쳐 효과적인 기하학적 정보를 전송하기 위해 \\(u\\)회 반복된다. 그런 다음 픽셀의 깊이와 법선이 전파된 평면에서 업데이트되어 궁극적으로 그림 2의 전파된 깊이와 법선 맵이 생성된다.\n' +
      '\n' +
      '**Geometric Filtering and Selection.** 전파된 결과의 필연적인 오류로 인해 Multi-view geometric consistency check(Schonberger et al., 2016)를 통해 부정확한 깊이 및 정규도를 필터링하고, 필터링된 깊이 및 정규도를 획득한다. 마지막으로, 필터링된 깊이와 렌더링된 깊이 사이의 절대 상대 차이를 계산한다. 임계값\\(\\sigma\\)보다 큰 절대상대차를 갖는 영역의 경우, 기존의 가우시안들은 이러한 영역을 정확하게 모델링하지 못하는 것으로 간주한다. 따라서 이러한 영역의 픽셀을 다시 3차원 공간으로 투영하고 동일한 초기화를 사용하여 3차원 가우시안(Gaussians)으로 초기화한다. 그런 다음 이러한 가우시안들은 추가 최적화를 위해 기존 가우시안들에 추가된다.\n' +
      '\n' +
      '### 평면 제약조건 최적화\n' +
      '\n' +
      '원래의 3DGS에서, 최적화는 기하학적 제약들을 통합하지 않고 단지 이미지 재구성 손실에만 의존한다. 결과적으로, 최적화된 가우시안 형상들은 실제 표면 기하학으로부터 상당히 벗어날 수 있다. 이러한 편차는 특히 제한된 뷰를 갖는 대규모 장면의 경우 새로운 관점에서 볼 때 렌더링 품질의 저하를 초래한다. 도 1에 도시된 바와 같다. 도 4에 도시된 바와 같이, 3DGS에서 가우시안들의 형상은 도로의 기하학과 크게 다르며, 이는 새로운 관점에서 볼 때 심각한 렌더링 아티팩트를 초래한다. 이 절에서는 가우시안 모양이 실제 표면과 매우 유사하도록 유도하는 평면적 제약을 제안한다.\n' +
      '\n' +
      '그림 3: 패치 매칭. 전파 과정에서 픽셀\\(p\\)에 대한 최적의 평면 후보를 선택하기 위해 각 평면 후보와 \\(p\\) 사이의 호모그래피 변환을 수행하여 이웃 뷰의 가능한 해당 픽셀을 생성한다. 평면 후보는 \\(p\\)과 가능한 쌍을 이루는 픽셀 사이에서 가장 높은 색 일관성을 보이는 것을 해결책으로 선택한다. 선택된 평면 후보는 픽셀의 깊이와 법선을 갱신하는데 사용된다.\n' +
      '\n' +
      '구체적으로, 섹션 4.3에서 전파된 2D 노말 맵은 장면 내의 평면들의 배향을 나타낸다. 가우시안 렌더링 법선과 전파 법선 사이의 일관성을 \\(\\mathcal{L}_{1}\\)과 각도 손실을 \\(\\mathcal{L}_{\\text{normal}\\)으로 명시적으로 강제한다:\n' +
      '\n' +
      '\\sum_{\\mathcal{L}_{\\text{normal}=\\sum_{\\mathbf{p}\\in\\mathcal{Q}}\\left\\|\\hat{N}(\\mathbf{p})-\\bar{N}(\\mathbf{p})\\right\\|_{1}+\\left\\|1-\\hat{N}(\\mathbf{p})^{ \\top}\\bar{N}(\\mathbf{p})\\right\\|_{1}, \\tag{8}\\tag{N}(\\mathbf{p})\\tag{n}(\\mathbf{p})\\tag{n}(\\mathbf{p})\\tag{n}(\\mathbf{p})\\tag{n}(\\mathbf{p})\\tag{n}(\\mathbf{p})\\tag{n}(\\mathbf{p})\\tag{n}(\\mathbf{p})\\tag\n' +
      '\n' +
      '여기서 \\(\\hat{N}\\)은 렌더링된 정규 맵이고 \\(\\bar{N}\\)은 전파된 정규 맵이며 \\(\\mathcal{Q}\\)은 섹션 4.3에서 기하학적 필터링 후 유효한 픽셀의 집합을 나타낸다.\n' +
      '\n' +
      '또한 Gaussian의 가장 짧은 축이 법선 방향을 나타낼 수 있도록 NeuSG(Chen et al., 2023)에서 scale regularization loss\\(\\mathcal{L}_{\\text{scale}}\\)을 통합하였다. 이 손실은 가우시안에서 최소 스케일을 0에 가깝게 제한하여 가우시안들을 평면 모양으로 효과적으로 평평하게 만든다. 마지막으로, 평면 제약은 두 손실의 가중 합으로 표현될 수 있다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{planar}}=\\beta\\mathcal{L}_{\\text{normal}}+\\gamma\\mathcal{L}_{\\text{scale}. \\tag{9}\\\n' +
      '\n' +
      '### Training Strategy\n' +
      '\n' +
      '요약하면, 우리는 점진적 가우시안 전파 전략을 3DGS에 통합하고, 최적화에서 매 반복마다 활성화하며, 여기서 \\(m=50\\)을 설정한다. 전파된 정규 맵은 평면 제약 손실을 계산하기 위해 저장된다. 최종 학습 손실\\(\\mathcal{L}\\)은 3DGS의 영상 복원 손실\\(\\mathcal{L}_{1}\\)과 제안된 평면 제약 손실을 갖는 \\(\\mathcal{L}_{D-SSIM}\\)으로 구성된다. 10.\n' +
      '\n' +
      '\\[\\mathcal{L}=(1-\\lambda\\mathcal{L}_{1}+\\lambda\\mathcal{L}_{\\text{D-SSIM}+\\mathcal{L}_{\\text{planar}, \\tag{10}\\]\n' +
      '\n' +
      '여기서 중량\\(\\lambda\\)은 3DGS와 동일하게 \\(0.2\\)으로 설정된다.\n' +
      '\n' +
      '## 5 Experiment\n' +
      '\n' +
      '### 데이터 세트 및 구현 세부사항\n' +
      '\n' +
      '**Datasets.** 대규모 도시 데이터세트 Waymo(Sun et al., 2020) 및 일반적인 NeRF 벤치마크 Mip-NeRF360 데이터세트에서 실험을 수행한다. (Caesar et al., 2020). 웨이모 데이터 세트에서 평가를 위해 9개의 장면을 무작위로 선택한다. 새로운 뷰 합성의 성능을 평가하기 위해 일반적인 설정에 따라 8개의 이미지 중 하나를 테스트 이미지로 선택하고 나머지 이미지를 학습 데이터로 선택한다. 평가에는 널리 사용되는 세 가지 메트릭, 즉 \\(즉, \\), PSNR(Peak Signal-to-Noise Ratio), SSIM(Structural similarity index measure), 그리고 학습된 지각 이미지 패치 유사도(LPIPS)를 적용한다(Zhang et al., 2018).\n' +
      '\n' +
      '**Implementation Details.** Our method is built on the popular open-source 3DGS code base (Kerbl et al., 2023). 3DGS에 설명된 접근법과 정렬하여, 우리의 모델은 3DGS의 훈련 일정 및 하이퍼파라미터에 따라 모든 장면에서 30,000번의 반복에 대해 훈련된다. 3DGS에 사용되는 원본 클론 및 분할 가우시안 밀도화 전략 외에도, 3회 전파가 수행되는 훈련 반복마다 제안된 점진적 전파 전략을 추가로 수행한다. 절대 상대 차이의 임계값 \\(\\sigma\\)을 \\(0.8\\)으로 설정한다. 평면손실은 \\(\\beta=0.001\\)과 \\(\\gamma=100\\)으로 설정하였다. 모든 실험은 RTX 3090 GPU에서 수행된다.\n' +
      '\n' +
      '### 양적, 질적 결과\n' +
      '\n' +
      '탭에 표시된 대로입니다. 도 1을 참조하면, 본 방법은 Instant-NGP(Muller et al., 2022), Mip-NeRF360(Barron et al., 2022), ZipNetRF(Barron et al., 2023) 및 3DGS(Kerbl et al., 2023)를 포함하는 SOTA(state-of-the-art) 방법과 비교된다.\n' +
      '\n' +
      'Waymo.**에 대한 결과.** 대규모 도시 데이터 세트 Waymo에서 본 방법은 모든 평가 메트릭에서 다른 방법보다 훨씬 우수합니다. 스트리트 뷰에 질감이 없는 영역이 존재하기 때문에 이러한 영역에서 포인트 클라우드를 초기화하는 것은 SfM의 과제가 된다. 결과적으로 3DGS는 이러한 영역에서 장면의 기하학을 정확하게 나타내는 가우시안들을 치밀화하기 어렵다. 그렇지 않으면, 우리의 전파 전략은 장면에서 누락된 기하학을 정확하게 보완한다. 추가적으로, 우리의 평면 제약은 장면의 평면들을 더 잘 모델링할 수 있게 한다. 따라서 기준선 3DGS에 비해 PSNR이 1.15 dB 향상됨을 확인하였다. 시각적 결과는 그림 1에 나와 있다. 도 5는 본 방법이 풍부한 텍스처 및 텍스처가 없는 영역 모두에서 선명한 디테일과 더 나은 렌더링을 달성함을 보여준다.\n' +
      '\n' +
      'MipNeRF360.**의 결과 MipNeRF360에서 공식 코드 베이스에서 사용되는 SfM 포인트를 관찰했기 때문에 생성된 SfM 포인트 클라우드를 사용하여 3DGS를 재훈련한다. We report the quantitative results of\n' +
      '\n' +
      '그림 4: 새로운 뷰 합성에 대한 3DGS와의 시각적 비교. 3DGS의 렌더링된 이미지는 가우시안 구가 순서를 벗어나 실제 형상을 정확하게 모델링하지 못하기 때문에 심각한 아티팩트를 포함한다. 반대로, 우리의 방법은 도로의 세부 사항을 충실하게 포착하고 가우스 구는 더 작고 질서정연하다.\n' +
      '\n' +
      '기존의 3DGS(3DGS*로 표기)와 탭에서 재학습된 3DGS. 1. 본 논문에서 제안한 방법은 3DGS와 유사한 결과를 얻었으며, 약간의 개선 효과를 얻었다. MipNeRF360 데이터 세트는 질감이 풍부한 매우 작은 규모의 자연 및 실내 장면을 포함하고 있으므로 SfM 기법은 초기화를 위해 일반적으로 고품질 포인트 클라우드를 제공하고 단순 복제 및 분할 밀도화 전략은 소규모 장면에서 병목 현상을 나타내지 않는다. 몇 가지 질감이 약한 표면을 가진 실내 장면의 경우, 우리의 방법은 여전히 개선되는 것을 보여준다. 우리는 결론을 더 뒷받침하기 위해 부록의 MipNeRF360 아래 각 장면에 대한 결과를 보고한다. 그림 5와 같이 3DGS와 비교하여 우리의 방법은 더 정확한 렌더링과 명확한 세부 사항을 달성한다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**전파 전략과 평면 제약의 효과.** Waymo 데이터셋에서 제안된 전파 전략과 평면 제약의 유효성을 검증한다. 탭에 표시된 대로입니다. 2, 점진적 전파 전략(세 번째 행)은 기준선에 비해 상당한 개선을 가져온다. 이러한 개선은 특히 초기 3DGS가 상당한 오류를 나타내는 영역(도 7의 첫 번째 및 두 번째 행에 표시됨)에서 장면의 기하학적 표현을 정제하는 능력에 기인할 수 있다. 평면 제약은 도 7의 세 번째 행에 도시된 바와 같이 평면들의 법선을 정확하게 모델링함으로써 렌더링 품질을 더욱 향상시킬 수 있다.\n' +
      '\n' +
      '**희소 트레이닝 이미지에 대한 강인성.** 트레이닝 이미지의 수가 감소함에 따라, 3DGS를 포함한 뉴럴 렌더링 방법의 렌더링 품질은 감소하는 경향이 있다. 표 3에서, 우리는 MipNeRF360의 한 장면으로부터 훈련 영상들의 \\(30\\%\\), \\(50\\%\\), \\(70\\%\\), \\(100\\%\\)으로 구성된 랜덤하게 선택된 부분 집합들을 사용하여 3DGS를 훈련한 결과와 우리의 방법을 제시한다. 특히, 우리의 방법은 훈련 영상들의 다른 퍼센트에 걸쳐 3DGS에 비해 우수한 렌더링 결과를 일관되게 달성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c|c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{FPS \\(\\uparrow\\)} & \\multicolumn{3}{c|}{Waymo} & \\multicolumn{3}{c}{MipNeRF 360} \\\\  & & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) \\\\ \\hline Instant-NGP (Muller et al., 2022) & 3 & 30.98 & 0.886 & 0.281 & 25.59 & 0.699 & 0.331 \\\\ Mip-NeRF 360 (Barron et al., 2022) & 0.02 & 30.09 & 0.909 & 0.262 & 27.69 & 0.792 & 0.237 \\\\ Zip-NeRF (Barron et al., 2023) & 0.09 & 34.22 & 0.939 & 0.205 & **28.54** & **0.828** & **0.189** \\\\ \\hline\n' +
      '3DGS* (Kerbl et al., 2023) & 103 & 33.53 & 0.938 & 0.226 & 27.21 & 0.815 & 0.214 \\\\\\\n' +
      '3DGS (Retrained) & 102 & - & - & - & 27.88 & 0.824 & 0.209 \\\\ GaussianPro (Ours) & **108** & **34.68** & **0.949** & **0.191** & 27.92 & 0.825 & 0.208 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: Waymo 및 MipNeRF-360 데이터 세트에 대한 정량적 비교. 우리는 각각 굵은 선과 밑줄을 긋고 가장 좋은 것과 두 번째로 좋은 것을 나타낸다. 3DGS*는 3DGS가 더 나은 SfM 포인트 클라우드로 재훈련하여 얻은 결과를 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c} \\hline \\hline Propagation & Planar & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) \\\\ \\hline \\(\\bigstar\\) & \\(\\bigstar\\) & 33.53 & 0.938 & 0.226 \\\\ \\(\\bigstar\\) & \\(\\bigstar\\) & 34.02 & 0.942 & 0.218 \\\\ \\(\\bigstar\\) & \\(\\bigstar\\) & 34.48 & 0.946 & 0.203 \\\\ \\(\\bigstar\\) & \\(\\bigstar\\) & **34.68** & **0.949** & **0.191** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 제안된 전파 전략과 평면 제약에 대한 절제 연구.\n' +
      '\n' +
      '그림 5: Waymo(왼쪽) 및 MipNeRF360(오른쪽) 데이터 세트에 대한 렌더링 결과. 3DGS와 비교하여 질감이 없는 표면과 날카로운 디테일 모두에서 눈에 띄는 개선을 달성했습니다.\n' +
      '\n' +
      '도 6: MipNeRF360 데이터세트의 _Room_ 장면에서 가우시안들의 시각화. 우리의 방법은 더 적은 잡음 가우시안들을 포함하고 더 컴팩트한 표현을 달성한다.\n' +
      '\n' +
      '**Efficiency Analysis.**Tab.4에 도시된 바와 같이 3DGS와 본 방법의 효율성을 비교하기 위해 두 가지 전형적인 실외 및 실내 장면을 선택하였으며, 훈련 시간이 약간 증가함에 따라 렌더링 품질이 눈에 띄게 향상되었다. 거리 장면의 경우 3DGS는 그림의 파란색 원과 같이 땅을 나타내기 위해 큰 잘못된 가우시안(Gaussians)을 사용한다. 1, 결과적으로 우리의 방법에 비해 가우시안 수가 줄어들었다. 그러나, 실내 장면에 대해, 우리의 방법은 더 적은 잡음(도 6에 또한 도시됨)을 갖는 더 컴팩트한 가우시안들을 초래한다. 또한, 본 논문에서 제안하는 방법은 3DGS와 유사한 실시간 렌더링 프레임 레이트를 갖는다.\n' +
      '\n' +
      '**MVS Inputs와 비교.** 우리의 방법은 가우시안 기하학을 개선하여 더 나은 렌더링 품질을 달성하므로, 3DGS에 더 조밀하고 정확한 MVS 포인트 클라우드를 직접 입력하여 유사한 효과를 얻을 수 있는지에 대한 질문을 제기한다. 이를 조사하기 위해 MVS 방법으로 생성된 밀집 포인트 클라우드와 3DGS를 최적화한 결과를 비교한다(Schonberger et al., 2016). 탭 도 4는 MVS 포인트 클라우드를 직접 입력하는 것이 추가적인 MVS 프로세스 및 많은 수의 초기 가우시안들로 인해 트레이닝 시간(대략 4배)을 크게 증가시키는 것을 나타낸다. 더욱이, 가우시안들의 수는 상당히 증가하고 렌더링 속도는 렌더링 품질의 약간의 향상에도 불구하고 눈에 띄게 감소한다. 이와는 대조적으로, 본 방법은 렌더링 품질과 효율성 사이에서 유리한 균형을 달성한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 장면의 표면 구조에 따라 가우시안 치밀화를 유도하는 새로운 점진적 전파 전략인 가우시안 프로를 제안한다. 또한 전파 과정을 기반으로 최적화 과정에서 평면 제약 조건을 도입하여 가우시안들이 평면 표면을 더 잘 모델링하도록 유도한다. 제안된 방법은 웨이모 및 MipNeRF360 데이터 세트에서 3DGS와 비교하여 우수한 렌더링 결과를 보여주며, 압축된 가우시안 표현을 유지한다. 제안된 방법은 구조화된 장면들에서 상당한 개선들을 보여주며, 트레이닝 이미지들의 수의 변화들에 강건하게 유지된다. 그러나 이 방법은 동적 객체들을 특별히 모델링하지 않으며, 모든 정적 가우시안 방법들과 같이 이러한 영역들에 인공물들을 제시할 것이다. 향후, 최근의 동적 가우시안 기법들은 동적 객체들을 다루기 위한 상보적인 컴포넌트로서 우리의 방법에 통합될 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c|c c c} \\hline \\hline Scene & \\multicolumn{2}{c|}{Strategy} & \\multicolumn{2}{c|}{PSNR} & \\multicolumn{2}{c|}{Gaussians} & \\multicolumn{2}{c}{Training} & \\multicolumn{1}{c}{FPS} \\\\ \\hline \\multirow{2}{*}{Street} & SM points+3DGS & 35.05 & **665k** & **40min** & **119** \\\\  & MVS points+3DGS & **36.13** & 1705k & 250min & 75 \\\\  & SfM points+GaussianPro & 36.08 & 991k & 560min & 108 \\\\ \\hline \\multirow{3}{*}{Room} & SIM points+3DGS & 31.71 & 1537k & **59min** & 105 \\\\  & MVS points+3DGS & **32.05** & 1832k & 270min & 90 \\\\ \\cline{1-1}  & SfM points+GaussianPro & 31.98 & **1461k** & 70min & **113** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 효율 분석. SfM 포인트 또는 MVS 포인트를 이용하여 초기화의 효과를 분석한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c|c c|c c|c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c|}{30\\%} & \\multicolumn{3}{c|}{50\\%} & \\multicolumn{3}{c|}{70\\%} & \\multicolumn{3}{c}{100\\%} \\\\  & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS \\\\ \\hline\n' +
      '3DGS & 28.45 & 0.896 & 0.216 & 29.97 & 0.912 & 0.203 & 30.87 & 0.921 & 0.194 & 31.71 & 0.919 & **0.192** \\\\ GaussianPro(Ours) & **28.64** & **0.900** & **0.210** & **30.27** & **0.914** & **0.199** & **30.93** & **0.924** & **0.189** & **31.98** & **0.927** & **0.192** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: MipNeRF360 데이터 세트의 _room_ 장면에서 서로 다른 훈련 뷰 비율을 갖는 3DGS와 우리의 비교.\n' +
      '\n' +
      '도 7: 점진적 전파 전략은 장면의 기하학을 효과적으로 향상시키고, 결과적으로 렌더링 품질을 향상시킨다. 평면 구속조건은 평면의 지오메트리 및 렌더링을 더욱 향상시킨다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Barnes et al. (2009) Barnes, C., Shechtman, E., Finkelstein, A., and Goldman, D. B. Patchmatch: structural image editing을 위한 randomized correspondence algorithm. _ ACM Trans. Graph.__ , 28(3):24, 2009.\n' +
      '* Barron et al. (2021) Barron, J. T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., and Srinivasan, P. P. Mip-nerf: a multiscale representation for anti-aliasing neural radiance fields. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 5855-5864, 2021.\n' +
      '* Barron et al. (2022) Barron, J. T., Mildenhall, B., Verbin, D., Srinivasan, P. P., and Hedman, P. Mip-nerf 360: Unbounded anti-aliased neural radiance field. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 5470-5479, 2022.\n' +
      '* Barron et al. (2023) Barron, J. T., Mildenhall, B., Verbin, D., Srinivasan, P. P., and Hedman, P. Zip-nerf: Anti-aliased grid-based neural radiance fields. _ Proceedings of the IEEE International Conference on Computer Vision_, 2023.\n' +
      '* Bleyer et al. (2011) Bleyer, M., Rhemann, C., and Rother, C. Patchmatch stereo-stereo matching with slanted support windows. In _Bmvc_, volume 11, pp. 1-11, 2011.\n' +
      '* Caesar et al. (2020) Caesar, H., Bankiti, V., Lang, A. H., Vora, S., Liong, V. E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G., and Beijbom, O. nuscenes: 자율 주행을 위한 멀티모달 데이터세트. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 11621-11631, 2020.\n' +
      '* Campbell et al. (2008) Campbell, N. D., Vogiatzis, G., Hernandez, C., and Cipolla, R. 다중 가설을 사용하여 다중 뷰 스테레오에 대한 깊이 맵을 개선합니다. In _Computer Vision-ECCV 2008: 10th European Conference on Computer Vision, Marseille, France, October 12-18, 2008, Proceedings, Part I 10_, pp. 766-779. Springer, 2008.\n' +
      '* Chen et al. (2022) Chen, A., Xu, Z., Geiger, A., Yu, J., and Su, H. Tensorf: Tensorial radiance field. In _European Conference on Computer Vision_, pp. 333-350. Springer, 2022.\n' +
      '* Chen et al. (2023) Chen, H., Li, C., and Lee, G. H. Neusg: Neural implicit surface reconstruction with 3d gaussian splatting guidance. _ arXiv preprint arXiv:2312.00846_, 2023.\n' +
      '* Chen et al. (2019) Chen, R., Han, S., Xu, J., and Su, H. Point-based multi-view stereo network. In _Proceedings of the IEEE/CVF international conference on computer vision_, pp. 1538-1547, 2019.\n' +
      '*Cheng et al. (2023) Cheng, K., Long, X., Yin, W., Wang, J., Wu, Z., Ma, Y., Wang, K., Chen, X., and Chen, X. Uc-nerf: 과소 보정된 다시점 카메라를 위한 신경 복사 필드. _The Twelfth International Conference on Learning Representations_, 2023.\n' +
      '* Deng et al. (2022a) Deng, K., Liu, A., Zhu, J.-Y., and Ramanan, D. Depth-supervised nerf: 더 적은 뷰들 및 더 빠른 트레이닝 무상으로. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 12882-12891, 2022a.\n' +
      '* Deng et al. (2022b) Deng, N., He, Z., Ye, J., Duinkharjav, B., Chakravarthula, P., Yang, X., and Sun, Q. Fov-nerf: 가상 현실을 위한 포브레이티드 신경 복사 필드들 _ IEEE Transactions on Visualization and Computer Graphics_, 28(11):3854-3864, 2022b.\n' +
      '* Fan et al. (2023) Fan, Z., Wang, K., Wen, K., Zhu, Z., Xu, D., and Wang, Z. Lightgaussian: 15x reduction 및 200+fps를 갖는 비한계 3d 가우스 압축. _ arXiv preprint arXiv:2311.17245_, 2023.\n' +
      '* Feng et al. (2023) Feng, Z., Yang, L., Guo, P., and Li, B. Cvrecon: Rethinking 3d geometric feature learning for neural reconstruction. _ arXiv preprint arXiv:2304.14633_, 2023.\n' +
      '* Fridovich-Keil et al. (2022) Fridovich-Keil, S., Yu, A., Tancik, M., Chen, Q., Recht, B., and Kanazawa, A. Plenoxels: Radiance fields without neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 5501-5510, 2022.\n' +
      '* Furukawa & Ponce (2009) Furukawa, Y. and Ponce, J. Accurate, dense, and robust multiview stereopsis. _ IEEE transaction on pattern analysis and machine intelligence_, 32(8):1362-1376, 2009.\n' +
      '* Furukawa et al. (2015) Furukawa, Y., Hernandez, C., et al. Multi-view stereo: A tutorial. _ Foundations and Trends(r) in Computer Graphics and Vision_, 9(1-2):1-148, 2015.\n' +
      '* Jiang et al. (2023) Jiang, Y., Tu, J., Liu, Y., Gao, X., Long, X., Wang, W., and Ma, Y. Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces. _ arXiv preprint arXiv:2311.17977_, 2023.\n' +
      '* Kerbl et al. (2023) Kerbl, B., Kopanas, G., Leimkuhler, T., and Drettakis, G. 3d gaussian splatting for real time radiance field rendering. _ ACM Transactions on Graphics_, 42(4), 2023.\n' +
      '* Lee et al. (2023) Lee, J. C., Rho, D., Sun, X., Ko, J. H., and Park, E. Compact 3d gaussian representation for radiance field. _ arXiv preprint arXiv:2311.13681_, 2023.\n' +
      '* Liu et al. (2020) Liu, L., Gu, J., Zaw Lin, K., Chua, T. - S., and Theobalt, C. Neural sparse voxel fields. _ 신경 정보 처리 시스템_, 33:15651-15663, 2020에서의 발전.\n' +
      '* Long et al. (2020) Long, X., Liu, L., Theobalt, C., and Wang, W. 적응적 정규 제약 조건을 갖는 폐색 인식 깊이 추정. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IX 16_, pp. 640-657. Springer, 2020.\n' +
      '*Liu et al. (2020)Long, X., Liu, L., Li, W., Theobalt, C., and Wang, W. 에피폴라 시공간 네트워크를 이용한 다시점 깊이 추정 In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 8258-8267, 2021.\n' +
      '* Long et al. (2022) Long, X., Lin, C., Wang, P., Komura, T., and Wang, W. 스파르세누스: 희소 뷰에서 일반화할 수 있는 빠른 신경 표면 재구성. In _European Conference on Computer Vision_, pp. 210-227. Springer, 2022.\n' +
      '* Lu et al. (2023) Lu, T., Yu, M., Xu, L., Xiangli, Y., Wang, L., Lin, D., and Dai, B. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. _ arXiv preprint arXiv:2312.00109_, 2023.\n' +
      '* Ma et al. (2022) Ma, Z., Teed, Z., and Deng, J. Multiview stereo with cascaded epipolar raft. In _European Conference on Computer Vision_, pp. 734-750. Springer, 2022.\n' +
      '* Mildenhall et al. (2020) Mildenhall, B., Srinivasan, P., Tancik, M., Barron, J., Ramamoorthi, R., and Ng, R. Nerf: 장면들을 뷰 합성을 위한 신경 래디언스 필드들로서 표현하는 것. 2020년 컴퓨터 비전에 관한 유럽 회의에서.\n' +
      '* Morgenstern et al. (2023) Morgenstern, W., Barthel, F., Hilsmann, A., and Eisert, P. Compact 3d scene representation via self-organizing gaussian grid. _ arXiv preprint arXiv:2312.13299_, 2023.\n' +
      '* Muller et al. (2022) Muller, T., Evans, A., Schied, C., and Keller, A. Instant neural graphics primitives with multiresolution hash encoding. _ ACM Transactions on Graphics(ToG)_, 41(4):1-15, 2022.\n' +
      '* Navaneet et al. (2023) Navaneet, K., Meibodi, K. P., Koohpayegani, S. A., and Pirsiavash, H. Compact3d: Compressing gaussian splat radiance field models with vector quantization. _ arXiv preprint arXiv:2311.18159_, 2023.\n' +
      '* Poole et al. (2022) Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dreamfusion: Text-to-3d using 2d diffusion. _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* Schonberger et al. (2016) Schonberger, J. L., Zheng, E., Frahm, J.-M., and Pollefeys, M. 비구조화된 멀티 뷰 스테레오를 위한 픽셀별 뷰 선택. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14_, pp. 501-518. Springer, 2016.\n' +
      '* Sun et al. (2020) Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., et al. Scalability in perception for autonomous driving: Waymo open dataset. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 2446-2454, 2020.\n' +
      '* Tang et al. (2023) Tang, J., Ren, J., Zhou, H., Liu, Z., and Zeng, G. Dream-gaussian: Generative gaussian splatting for efficient 3d content creation. _ arXiv preprint arXiv:2309.16653_, 2023.\n' +
      '* Vakalopoulou et al. (2018) Vakalopoulou, M., Chassagnon, G., Bus, N., Marini, R., Zacharaki, E. I., Revel, M. - P., and Paragios, N. Atlasnet: 의료 영상 분할을 위한 다중-아틀라스 비선형 딥 네트워크. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part IV 11_, pp. 658-666. Springer, 2018.\n' +
      '* Wang et al. (2023) Wang, P., Liu, Y., Chen, Z., Liu, L., Liu, Z., Komura, T., Theobalt, C., and Wang, W. F2-nerf: 자유 카메라 궤적을 갖는 고속 신경 복사 필드 트레이닝. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 4150-4159, 2023.\n' +
      '* Xu & Tao(2019) Xu, Q. 및 Tao, W. 다축척 기하학적 일관성 유도 멀티뷰 스테레오. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 5483-5492, 2019.\n' +
      '*Xu et al. (2022) Xu, Q., Xu, Z., Philip, J., Bi, S., Shu, Z., Sunkavalli, K., and Neumann, U. 점-nerf: 점 기반 신경 복사 필드. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 5438-5448, 2022.\n' +
      '* Yan et al. (2023) Yan, Z., Low, W. F., Chen, Y., and Lee, G. H. Multi-scale 3d gaussian splatting for anti-aliased rendering. _ arXiv preprint arXiv:2311.17089_, 2023.\n' +
      '* Yang et al. (2023) Yang, Z., Chen, Y., Wang, J., Manivasagam, S., Ma, W. - C., Yang, A. J., and Urtasun, R. Unisim: neural closed-loop sensor simulator. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 1389-1399, 2023.\n' +
      '* Yao et al. (2018) Yao, Y., Luo, Z., Li, S., Fang, T., and Quan, L. Mvsnet: 비구조화된 다시점 스테레오에 대한 깊이 추론. In _Proceedings of the European conference on computer vision (ECCV)_, pp. 767-783, 2018.\n' +
      '*유앤한(2009) 유제이-씨. and Han, T. H. Fast normalized cross-correlation. _ 회로, 시스템 및 신호 처리_, 28:819-843, 2009.\n' +
      '* Yu et al. (2022) Yu, Z., Peng, S., Niemeyer, M., Sattler, T., and Geiger, A. Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. _ 신경 정보 처리 시스템_, 35:25018-25032, 2022에서의 발전.\n' +
      '* Yu et al. (2023) Yu, Z., Chen, A., Huang, B., Sattler, T., and Geiger, A. Hip-splatting: Alias-free 3d gaussian splatting. _ arXiv preprint arXiv:2311.16493_, 2023.\n' +
      '\n' +
      '* Zhang et al. (2018) Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. 지각적 척도로서 깊은 특징의 불합리한 효과. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 586-595, 2018.\n' +
      '* Zwicker et al. (2002) Zwicker, M., Pfister, H., Van Baar, J., and Gross, M. 에바 스플래팅 IEEE Transactions on Visualization and Computer Graphics_, 8(3):223-238, 2002.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>