<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 단순 선형 주의 언어 모델은 리콜-처리량 트레이드오프의 균형을 유지한다.\n' +
      '\n' +
      'Simran Arora\n' +
      '\n' +
      '해당 저자; SA, SE, MZ(SSM)에 대한 동일한 기여 및 무작위 순서.\n' +
      '\n' +
      'Sabri Eyuboglu\n' +
      '\n' +
      '해당 저자; SA, SE, MZ(SSM)에 대한 동일한 기여 및 무작위 순서.\n' +
      '\n' +
      'Michael Zhang\n' +
      '\n' +
      '해당 저자; SA, SE, MZ(SSM)에 대한 동일한 기여 및 무작위 순서.\n' +
      '\n' +
      'Aman Timalsina\n' +
      '\n' +
      '버팔로대학교\n' +
      '\n' +
      'Silas Alberti\n' +
      '\n' +
      'Pardue University\n' +
      '\n' +
      '{simran,eyuboglu,mzhang,alberti,jamesz,chrismre}@cs.stanford.edu\n' +
      '\n' +
      'Dylan Zinsley\n' +
      '\n' +
      'Stanford University\n' +
      '\n' +
      'James Zou\n' +
      '\n' +
      'Pardue University\n' +
      '\n' +
      '{simran,eyuboglu,mzhang,alberti,jamesz,chrismre}@cs.stanford.edu\n' +
      '\n' +
      'Atri Rudra\n' +
      '\n' +
      'Pardue University\n' +
      '\n' +
      'Christopher Re\n' +
      '\n' +
      '{qylanxin,atri}@buffalo.edu\n' +
      '\n' +
      'Christopher Re\n' +
      '\n' +
      '{atimalsi}@purdue.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근 연구에 따르면 주의 기반 언어 모델은 이전에 문맥에서 볼 수 있었던 토큰에서 세대를 접지하는 능력인 _recall_에서 탁월하다. 그러나 KV 캐시의 공격적인 메모리 소모로 인해 추론 과정에서 주의력 기반 모델의 효율성이 떨어진다. 이 연구에서는 리콜을 손상시키지 않으면서 언어 모델 효율성(예: 메모리 소비를 줄임으로써)을 향상시킬 수 있는지 탐색한다. 실험과 이론을 광범위한 아키텍처에 적용하여 모델의 _상태 크기_와 회상 능력 사이의 핵심 트레이드오프를 식별한다. 우리는 주의(예: H3, Mamba, RWKV)에 대한 효율적인 대안이 고정된 크기의 반복 상태를 유지하지만 회상 시 어려움을 겪는다는 것을 보여준다. 본 논문에서는 선형 윈도우 어텐션과 슬라이딩 윈도우 어텐션을 결합한 간단한 구조를 제안한다. 기반 윈도우 크기와 선형 주의 특징 차원을 변화시킴으로써 상태 크기를 다이얼링하고 리콜-메모리 트레이드오프 곡선의 파레토 프론티어를 횡단하여 한쪽 끝에서는 주의의 완전한 품질을 회복하고 다른 쪽 끝에서는 주의-대안의 작은 상태 크기를 회복할 수 있다. 우리는 언어 모델을 최대 1.3b 매개변수까지 훈련하고 Based가 가장 강력한 하위 2차 모델(예: Mamba)과 당혹스러움에서 일치하고 실제 리콜 집약 작업에서 6.22 정확도 포인트만큼 능가함을 보여준다. 선형 주의의 구현은 종종 최적화된 표준 주의 구현보다 덜 효율적이다. 이를 위해 1.3b 파라미터 모델을 사용하여 1024개의 토큰을 생성할 때 FlashAttention-2보다 언어 생성에서 24\\(\\times\\) 더 높은 처리량을 가능하게 하는 IO 인식 알고리즘을 개발한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '언어 모델에서 시퀀스 믹서(_e.g._ attention, convolution)의 선택은 그 품질과 효율 모두에 영향을 미친다[1, 2]. 이전 작업은 주의력이 이전에 본 토큰 [1, 3]에서 지상 세대에 대한 능력인 _recall_에서 탁월하다는 것을 보여준다. 반면에 주의력 기반 모델의 처리량은 2차 계산 복잡도에 의한 훈련과 공격적인 메모리 소비에 의한 추론 동안 병목 현상이 발생한다. 자연스러운 질문은 다음과 같습니다. _우리는 품질?_를 포함하지 않고 언어 모델의 실제 속도 및 메모리 사용을 개선할 수 있습니까?_\n' +
      '\n' +
      '최근에, 복잡성 [4, 5, 6, 7, 8]에서 주의를 일치시키면서 실질적으로 더 높은 처리량을 가능하게 하는 다수의 아키텍처가 제안되었다. 그러나 전반적인 복잡성과 같은 거친 메트릭은 모델 품질의 중요한 차이를 모호하게 할 수 있다. 예를 들어, 최근의 작업은 특정 부류의 아키텍처들, _gated-convolutions_가, 시퀀스 길이에서 서브-쿼드적으로 스케일링하는 복잡성에도 불구하고, 리콜을 수행하는 데 있어서 주의력보다 덜 효율적임을 보여준다[1]. 이 분석을 기반으로 실제 리콜 집약적인 작업에 걸쳐 더 광범위한 종류의 아키텍처를 평가하고 가장 주의력이 없는 대안인 Mampa보다 46.7개의 정확도 포인트만큼 주의력이 향상됨을 보여줍니다(표 1). 1\n' +
      '\n' +
      '각주 1: 회상 집약적 업무의 예로는 정보 추출, 독해, 커스텀 변수와 함수명을 이용한 코드 생성 등이 있다. 이는 생성 중에 컨텍스트(암기된 정보를 대조)로 사용해야 합니다.\n' +
      '\n' +
      '이러한 관찰에 동기 부여되어 고레콜 모델과 고처리량 모델 간의 트레이드오프의 파레토 프론티어를 탐구한다. 우리는 인기 있는 합성 _associative recall_ task [1, 3, 9]에 대한 다양한 아키텍처들(_e.g. attention, SSMs, convolutions)을 평가한다. 생성 스루풋은 메모리 소비에 의해 병넥킹되기 때문에, 우리는 생성 동안 _recurrent_ 상태의 크기에 영향을 미치는 하이퍼 파라미터들(예를 들어, 모델 차원)을 가변하고 아키텍처 클래스들에 걸쳐 유지되는 기본적인 리콜-메모리 트레이드오프를 입증한다(도 2). 어텐션은 연관 회상을 완벽하게 수행하지만, 반복 상태(_i.e._KV-캐시)는 시퀀스 길이에 따라 선형적으로 성장한다. 슬라이딩 윈도우 어텐션은 더 나쁜 장거리 리콜의 비용으로 재발성 상태의 크기를 제한할 수 있다[10]. 그러나 최근 제안된 SSM 아키텍처인 Mampa는 슬라이딩 윈도우를 넘어 파레토 프론티어를 확장한다. 이것은 질문을 제기합니다: 파레토 프론티어를 확장할 수 있는 다른, 아마도 더 간단한 모델이 있습니까?_\n' +
      '\n' +
      '메모리 소모를 줄이기 위해 슬라이딩 윈도우 어텐션과 소프트맥스 근사 선형 어텐션의 두 가지 간단한 기법을 사용하는 것을 고려한다. 언어 모델링(표 1) 및 합성 회상 실험(그림 1, 중심)에 대한 우리의 결과는 원시만으로는 파레토 프론티어를 탐색하기에 충분하지 않음을 시사한다.\n' +
      '\n' +
      '1. 우리는 _linear attention_ 단독이 연관 회상을 해결하기 위해 고군분투한다는 것을 발견한다(도 1, 중심). 우리는 선형 주의가 국소 토큰 이동 및 비교를 수행할 정밀도가 부족하기 때문이라고 가정한다[1, 9].\n' +
      '2. _sliding window attention_에서, 연관 리콜 범위는 윈도우들의 폭에 의해 제한된다(도 1, 중심). 윈도우 크기를 증가시킴에 따라 반복 상태는 선형적으로 성장하고 병렬 훈련 및 추론 동안 속도에 비선형적인 영향을 미친다(그림 1, 왼쪽).\n' +
      '\n' +
      '우리는 이 두 가지 기술을 하나의 아키텍처로 결합하는데, 우리는 이것을 기반(그림 1, 오른쪽)이라고 부른다. 슬라이딩 윈도우 어텐션과 선형 어텐션이 서로 보완되어 리콜-메모리 트레이드오프의 파레토 프론티어를 확장할 수 있다는 것을 발견했다(그림 2). 우리는 (1) 선형 주의의 큰 순환 기억이 시퀀스에서 장거리 토큰 상호작용을 모델링하는 데 도움이 될 수 있고 (2) 슬라이딩 윈도우 주의는 연관 회상을 수행하는 데 필요한 정확한 국소 이동을 처리할 수 있다고 의심한다.\n' +
      '\n' +
      'SoTA 주의[11] 및 반복 [5] 모델과 벽-시계 및 처리량 메트릭에서 Based를 경쟁적으로 만들기 위해 몇 가지 IO 인식 최적화를 소개한다.\n' +
      '\n' +
      '1. 이론적으로 개선된 복잡도에도 불구하고, _linear attention_ 구현들은 종종 잘 최적화된 attention implementations[12]보다 _slower_이다. 본 논문에서는 소프트맥스의 2차 테일러 근사화를 선형 주의 특징 맵으로 사용하여 시퀀스 길이\\(N\\)와 머리 치수\\(d\\)을 사용한다.\n' +
      '\n' +
      '도 1: **Based overview. 선형 어텐션과 _tiny_ 슬라이딩 윈도우 소프트맥스 어텐션(예를 들어, 폭이 64 또는 128 토큰)을 결합하면 제한된 효율 오버헤드와 비교하여 향상된 리콜 정확도를 가능하게 한다. 타일 크기가 더 작아요 (_Left_) Cutlass GEMMs(\\(y\\)) vs. 슬라이딩 윈도우 어텐션 크기(\\(x\\)), 텐서 코어에 배치 크기 512가 있다. (_Center_) Model recall accuracy((\\(y\\)) vs. 슬라이딩 윈도우 어텐션 크기((\\(x)) 선형 주의력 단독(짙은 파란색), 슬라이딩 윈도우 주의력 단독(밝은 파란색) 및 이들의 조합(기본, 주황색)을 비교한다. (_Right_) 두 구성 요소가 상호 보완하는 방법을 설명하는 Based의 도식도.**\n' +
      '\n' +
      '순간적으로 \\(\\mathcal{O}(Nd^{3})\\) 시간 및 공간 복잡도를 요구한다[13, 14]. 실세계 벽시계 시간과 메모리 사용에서 우리의 관심을 경쟁적으로 만들기 위해 하드웨어 효율적인 알고리즘과 맞춤형 CUDA 구현을 제공한다. 기준선에 비해 본 논문에서 제안한 알고리즘은 HBM(slower-to-access memory)에서 SRAM(faster-to-access memory)으로의 데이터 이동을 \\(\\mathcal{O}(Nd^{2})바이트만큼 감소시키고, SRAM에서 레지스터(fast memory)를 \\(O(Nd^{3})바이트만큼 감소시킨다(섹션 5).\n' +
      '2. _Sliding window attention_ exploits tensor core, specialized units on modern GPUs for performing matrix multiplications (GEMMs). 인기 있는 아키텍처는 긴 창 크기(예: 미스트랄-7B[10]의 경우 4096)를 사용하지만 하드웨어 특성에 따라 고정된 크기 64 창을 선택합니다. 특히 텐서 코어 커널 런치 레이턴시를 숨기기 위해 충분한 점유량을 사용한다. 텐서 코어는 \\(16\\times 16\\) 타일에서 작동하지만 그림 1(왼쪽)에서 \\(16\\times 16\\) 대 \\\\을 수행하기 위한 레이턴시를 볼 수 있다. NVIDIA H100 텐서 코어에서 (64\\times 64\\)(또는 심지어 \\(128\\times 128\\)) 차원 행렬 곱셈은 유사하여 윈도우 크기를 알 수 있다.\n' +
      '\n' +
      '실험에서 기반은 파일 언어, DNA 모델링 및 LM Eval Harness[16]에 대한 언어 모델링에서 최대 1.3Bn 매개변수까지 모델에서 강력한 트랜스포머++[15] 및 SoTA 하위 2차 기준선과 품질 경쟁한다는 것을 보여준다. 이 외에도 기반은 파일의 연상 회상 슬라이스와 다운스트림 회상 집약 태스크에서 각각 0.14 퍼플렉시티 포인트 및 6.22 정확도 포인트만큼 이전 하위 2차 아키텍처를 능가한다. 효율성에 있어서, 기반은 생성 시 강력한 FlashAttention-2 구현보다 최대 24\\(\\times\\)의 높은 처리량을 가능하게 한다. 이 작업에 대한 코드는 [https://github.com/HazyResearch/based](https://github.com/HazyResearch/based)에서 제공된다.\n' +
      '\n' +
      '##2 예비 작업 및 관련 작업\n' +
      '\n' +
      '우리는 이 섹션의 주요 관련 작업에 대해 논의하고 부록 A에서 확장된 토론을 제공한다.\n' +
      '\n' +
      '트랜스포머 [2]에 의해 인기있는 어텐션은 _de facto_ 언어 모델링 프리미티브로서, 소프트맥스 어텐션은 길이\\(N\\) 및 머리 차원\\(d\\)의 입력\\(\\mathbf{x}\\in\\mathbbb{R}^{N\\times d}\\)을 취하고, 소프트맥스 오버 프로젝션\\(\\mathbf{q},\\mathbf{k},\\mathbf{x}\\mathbf{W}_{q},\\mathbf{v}=\\mathbf{x}\\mathbf{W}_{q},\\mathbf{x}\\mathbf{W}_{v},\\mathbbbb{R}^{N\\times d}\\)을 통해 출력\\(\\mathbf{y}\\in\\mathbbbb{R}^{N\\times d}\\)을 계산하고, _i.e.e.e.e.e.e.e.e.e.e.e.e.e.e.e.e.e.e\n' +
      '\n' +
      '\\frac{\\exp(\\mathbf{y}_{i}=\\sum_{j=1}^{i}\\frac{\\exp(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}/\\sqrt{d})\\mathbf{v}_{j}{\\sum_{m=1}^{i}\\exp(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{m}/\\sqrt{d}}} \\tag{1}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\m}\\\n' +
      '\n' +
      '\\(\\mathbf{W}_{q},\\mathbf{W}_{k},\\mathbf{W}_{v}\\in\\mathbb{R}^{d\\times d}\\)는 학습 가능한 행렬이다. [1]의 회상에 효과적이고 훈련하기에 효율적이지만(식 1은 GPU에서 병렬화가능하고, [12]의 최근 발전과 함께 기억에서 \\(\\mathcal{O}(N)\\), 세대에 대한 관심은 여전히 비싸다. 모든 새로운 출력\\(\\mathbf{y}_{n}\\)에 대해, 우리는 이전\\(\\{\\mathbf{k}_{i},\\mathbf{v}_{i}_{i=1}^{n-1}\\)의 증가하는 _\\(K\\)V-cache_에 걸쳐 \\(nd\\) 연산을 필요로 한다. 이는 더 긴 시퀀스에 대해 더 큰 메모리 소비 및 더 낮은 처리량을 초래한다.\n' +
      '\n' +
      '효율적인 주의를 기울이는 다양한 작업은 품질을 희생시키지 않으면서 주의의 효율성을 향상시키기 위해 노력한다. _ Sparse attentions_는 특정 스트라이드 패턴 또는 로컬 _슬라이딩 윈도우_[17, 18, 19]에 대해서만 참석함으로써 주의의 시간 및 메모리 요구 사항을 감소시킨다. 대형 언어 모델(Mistral, Jiang et al. [10])에서 더욱 대중화되었지만, 이전 작업은 조밀한 상호 작용을 포착하지 못하는 희박한 패턴으로 완전한 주의를 저평가하거나, 여전히 대형 KV-캐시와 후속의 비효율성을 허용하는 대형 윈도우 크기를 사용한다.\n' +
      '\n' +
      '한편, _linear attentions_는 표준 주의에서 소프트맥스를 대체 커널 함수[14, 20, 21, 22, 23]로 대체한다. Feature Map dot-products \\(\\phi(\\mathbf{q}^{\\top}\\mathbf{k})\\(\\phi(\\mathbf{q})^{\\top}\\phi(\\mathbf{k})\\)에 유리한 \\(\\exp(\\mathbf{q}^{\\top}\\mathbf{k})\\)를 제거함으로써, 이러한 방법들은 \\(\\mathcal{O}(Nd^{2})\\) 시간 및 공간에서 주의력을 계산하기 위해 행렬 곱 연관성을 사용한다[24]. 또한, 상수 메모리에 대한 _recurrent view_와 토큰당 \\(\\mathcal{O}(1)\\) 시간 [25, 26]을 허용한다. 그러나, 현재의 선형 주의 특징 맵은 리콜에 대한 표준 주의와 일치하지 않거나 계산하는데 비용이 많이 든다[13]. 선형 주의력은 또한 실제 [12]에서 더 빠른 벽 시계 시간 또는 더 낮은 기억력 대 현대 표준 주의력을 달성하지 못한다.\n' +
      '\n' +
      '마지막으로, 다양한 모델들은 스테이트-스페이스 모델들(SSMs)[27, 28], 게이티드 컨볼루션들(7, 9], 입력-의존적 재발들(5, 8)과 같은 어텐션-프리 시퀀스 믹서들을 사용하여 그 효율성을 향상시키면서 라이벌 어텐션 성능을 향상시킨다. 그러나, 최근의 이러한 모델들은 전반적인 복잡성에서 주의와 일치할 수 있지만, 추가 연구는 그들이 리콜 및 상황 내 학습과 같은 작업에서 트랜스포머를 저성능화할 수 있음을 시사한다[1, 29].\n' +
      '\n' +
      '무료 점심식사: 메모리-리콜 트레이드오프\n' +
      '\n' +
      '이 섹션에서는 추론 동안 모델의 메모리 소비(_즉, 반복 상태의 크기)와 리콜을 수행하는 능력 사이의 근본적인 트레이드오프를 보여준다. 합성 데이터에 대한 실험과 이론적 분석의 조합을 사용한다.\n' +
      '\n' +
      '***메모리-리콜 트레이드오프에 대한 경험적 연구:** 섹션 3.1에서, 우리는 모델의 반복 상태 크기에 영향을 미치는 다양한 하이퍼파라미터인 합성 연관 리콜 태스크에 대한 다수의 인기 있는 아키텍처 클래스(_예:_맘바, 하이에나)를 평가한다(도 2). 각 아키텍처 클래스 내에서 우리는 명확한 트레이드오프(반복 상태 크기가 클수록 더 나은 리콜)를 관찰한다. 그러나 고정된 반복 상태 크기에 대해 아키텍쳐 간에 성능이 일관되지 않습니다. 우리는 일부 시퀀스 믹서가 파레토 프론티어 아래에 잘 떨어지는 것을 관찰한다. 이는 파레토 프론티어를 확장할 수 있는 시퀀스 믹서의 설계에 동기를 부여한다.\n' +
      '리콜에 필요한 메모리의 하한: 섹션 3.2에서, 우리는 _any_ recurrent 모델 정리 F.1로 정확한 리콜을 수행하는 데 필요한 recurrent 상태 크기를 하한화한다. 이 분석은 처리량-리콜 트레이드오프에 대한 우리의 경험적 관찰을 강화한다.\n' +
      '\n' +
      '### 기억-회상 절충점에 대한 실증적 연구\n' +
      '\n' +
      '**Setup.** 우리는 MQAR(Multi-Query Associative Recall) [1]이라는 합성 AR 태스크를 사용하여 트레이드오프를 시연한다. 이 작업에서 입력 시퀀스는 질의가 뒤따르는 다수의 키-값 쌍으로 구성된다. 주어진 쿼리에 대해, 모델은 다음 토큰을 예측하기 위해 시퀀스의 이전으로부터 대응하는 키-값 쌍을 회상해야 한다. 예를 들어, 아래의 입력에 대한 정확한 출력은 4, 6, 1, 2, 3일 것이다:\n' +
      '\n' +
      '\\[\\text{A4B3C6\\begin{subarray}{c}\\text{F1E2}\\rightarrow\\text{A?C?F2E?B?\\\\text{Key-Value}\\end{subarray}\\text{A4B3C6\\begin{subarray}{c}\\text{F1E2}\\rightarrow\\text{A?C?F2E?B?\\\\text{Key-Value}\\end{subarray}\\text{F1E2}\\rightarrow\\text{A?C?F2E?B?\\\\text{Key-Value}\\end{subarray}\\text{A4B3C6\\begin{subarray}\n' +
      '\n' +
      '우리는 4에서 64개의 키-값 쌍을 포함하는 길이 256개의 토큰의 시퀀스에 대해 훈련한다. 평가 동안, 우리는 4개 내지 256개의 키-값 쌍들을 포함하는 길이 1,024개의 토큰들의 시퀀스들에 대한 정확도를 측정한다.\n' +
      '\n' +
      '우리는 주의[2], 슬라이딩 윈도우 주의[19], 맘바[5], H3[9], 하이에나[7] 및 기반의 6가지 시퀀스 믹서를 훈련하고 평가한다. 각각에 대해 추론 동안 메모리 소비에 영향을 미치는 하이퍼파라미터를 변화시킨다. 예를 들어, 슬라이딩 윈도우 주의에서 우리는 윈도우 폭을 변화시킨다. 우리는 MQAR 정확도가 반복 상태의 크기에 따라 어떻게 달라지는지 비교한다. 부록 E.1에는 상태 크기가 각 아키텍처에 대해 계산되는 방법에 대한 세부 정보가 포함되어 있습니다.\n' +
      '\n' +
      '그림 2와 그림 3은 [https://github.com/HazyResearch/zoology](https://github.com/HazyResearch/zoology)에서 제공되는 스크립트를 사용하여 새로운 아키텍처로 재현되거나 확장될 수 있다.\n' +
      '\n' +
      '그림 2에서 우리는 아키텍처 클래스 내부 및 전체에 걸쳐 유지되는 MQAR에서 반복 상태 크기와 정확도 사이의 근본적인 트레이드오프를 보여준다. 각 아키텍처 클래스(_e.g._H3 모델) 내에서, 리커런트 상태 크기를 증가시키는 것은 거의 항상 정확도의 향상으로 이어진다. 건축학 수업 전반에 걸쳐, 우리는 또한 절충안을 본다 주의력은 완벽한 회상 정확도를 달성하지만, 그것의 반복 상태 크기는 시퀀스의 길이에 따라 성장한다. Mamba 및 H3와 같은 다른 아키텍처 클래스는 훨씬 더 작은 반복 상태를 가진 모델을 인정하지만 이러한 모델은 리콜 용량이 제한적이다.\n' +
      '\n' +
      '도 2: **쓰루풋(메모리)-리콜 트레이드오프.**\\(x\\)-축은 생성 동안의 상태 크기(바이트)를 나타내고; \\(y\\)-축은 MQAR 리콜 태스크 [1]에 대한 정확도를 나타낸다. 각각의 아키텍처에 대해, 우리는 반복 상태 크기(_e.g._ 모델 차원)에 영향을 미치는 다양한 하이퍼파라미터들을 트레이닝한다. 그림은 광범위한 모델 [1, 5, 9]에 적용되는 반복 상태 크기와 회상 능력 사이의 근본적인 균형을 보여준다.\n' +
      '\n' +
      '고정된 반복 상태가 주어지면 모든 아키텍처가 동일한 리콜 용량을 갖는 것은 아니다. 이전 작업에서 제안된 아키텍처 중 맘보는 제한된 메모리 예산을 가장 잘 사용한다. 특히, 컨볼루션 뷰(_e.g._ Hyena 및 H3)를 갖는 아키텍처는 파레토 프론티어보다 훨씬 아래로 떨어진다. 제안된 아키텍처는 (섹션 4에 소개됨) 맘보를 넘어 파레토 프론티어를 확장한다. 상태 크기(_예를 들어, 특징 차원 및 모델 차원)를 결정하는 하이퍼-파라미터를 변화시킴으로써, 높은 리콜 용량을 갖는 효율적인 모델들과 메모리-배고픈 모델들 사이의 트레이드오프를 부드럽게 탐색할 수 있다.\n' +
      '\n' +
      '### Theoretical Analysis\n' +
      '\n' +
      '우리의 이론적 분석은 위에서 설명한 경험적 관찰에 대한 추가 통찰력을 제공한다. 먼저, 통신 복잡도 이론의 결과를 이용하여, _any_ causal model (_e.g._ Mambo, Attention)의 recall capacity가 그것의 recurrent state의 크기(theorem F.3 in Appendix F)에 의해 제한됨을 보인다.\n' +
      '\n' +
      '입력\\(\\mathbf{u}\\in\\{0,1\\}^{N\\times d}\\)에 인과관계에 따른 임의의 순환모델2는 MQAR.__\n' +
      '\n' +
      '각주 2: 특히 맘보 [5]의 경우 Corollary F.1을 참조하십시오.\n' +
      '\n' +
      '각주 3: 여기서, 우리는 주의 엔트리들이 경계되어야 한다.\n' +
      '\n' +
      '이 결과는 그림 2에서 관찰된 트레이드오프가 구조적 변덕의 인공물이 아니라 기본임을 시사한다.\n' +
      '\n' +
      '다음으로, 게이팅 및 컨볼루션(_e.g._H3, Hyena, RWKV v4)으로부터 구축된 광범위한 부류의 아키텍처인 _gated-convolutions_에 초점을 맞춘다. 광범위한 게이팅 컨볼루션 제안 세트를 이론적으로 분석하는 데 진전을 이루기 위해, 선행 연구는 게이팅 및 컨볼루션 프리미티브로부터 구축된 _any_ 아키텍처를 입증 가능하게 시뮬레이션할 수 있는 BaseConv라고 하는 _canonical_gated-convolution을 개발한다.\n' +
      '\n' +
      '이 작업을 바탕으로 BaseConv가 상수-다층(부록 F의 정리 F.5와 정리 F.6)에서 MQAR을 풀 수 없음을 보인다.\n' +
      '\n' +
      '**정리 3.2**: _입력 시퀀스\\(\\mathbf{u}\\in\\{0,1\\}^{3N\\times d}\\)이 주어졌을 때, 여기서 \\(N\\)과 \\(d\\)은 각각 시퀀스 길이 및 머리 차원을 나타내며, 데이터 독립적인 BaseConv 모델은 \\(d=\\log_{2}(c)\\)에 대한 MQAR을 풀기 위해 \\(\\log(2d)\\)-레이어가 필요하며, 여기서 \\(c\\)은 어휘 크기 4._\n' +
      '\n' +
      '각주 4: 즉, 어휘로부터의 각 토큰은 \\(\\{0,1\\}^{\\log_{2}(c)}\\)의 자연 이진 인코딩을 갖는다.\n' +
      '\n' +
      '대조적으로, Arora et al. [1]은 attention이 constant-many layer에서 MQAR을 해결한다는 것을 보여준다. 이 결과는 그림 2의 게이티드 콘볼루션 아키텍처(H3 및 하이에나)가 왜 새로운 아키텍처에 의해 확립된 파레토 프론티어 아래에 있는지 설명하는 데 도움이 된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Architecture} & \\multirow{2}{*}{Params} & \\multicolumn{2}{c}{**Efficiency**} & \\multicolumn{2}{c}{**Language Modeling (Pile)**} & \\multicolumn{2}{c}{**Info. Extraction**} & \\multicolumn{2}{c}{**Question Answering**} \\\\  & & Prefill & Generate & All & AR & Other & SWDE & FDA & SQUAD & Common \\\\  & & Tok./ms \\(\\uparrow\\) & Tok./ms \\(\\uparrow\\) & Ppl. \\(\\downarrow\\) & Ppl. \\(\\downarrow\\) & Ppl. \\(\\downarrow\\) & Acc \\(\\uparrow\\) & P\\(\\uparrow\\) & Avg. Acc. \\(\\uparrow\\) & P\\(\\uparrow\\) & Avg. Acc. \\(\\uparrow\\) \\\\ \\hline \\hline Transformer++ & 1.33b & 103.50 & 0.99 & **7.26** & **1.74** & **8.10** & **41.97** & **73.23** & **36.19** & **47.64** \\\\ Based & 1.35b & **161.71** & 24.28 & 7.43 & 1.87 & 8.26 & 30.83 & 24.41 & 30.46 & 46.68 \\\\ Mambo & 1.32b & 112.22 & **25.69** & 7.48 & 1.96 & 8.29 & 25.93 & 12.89 & 28.20 & 46.84 \\\\ \\hline Transformer++ & 360m & 207.77 & 23.82 & **8.39** & **1.87** & **9.42** & **37.62** & **58.00** & **27.18** & **44.08** \\\\ Based & 363m & **514.57** & **47.23** & 8.65 & 2.07 & 9.64 & 22.81 & 11.71 & 25.07 & 43.03 \\\\ Mambo & 358m & 267.09 & 39.95 & 8.64 & 2.21 & 9.59 & 25.61 & 6.53 & 24.06 & 43.51 \\\\ GLA & 362m & — & — & 9.12 & 2.36 & 10.68 & — & — & — & — \\\\ RWKV v5 & 362m & — & — & 9.79 & 2.40 & 10.90 & — & — & — & — \\\\ H3 & 362m & — & — & 10.60 & 4.88 & 11.23 & 17.59 & 0.64 & 7.87 & 39.35 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **미리 훈련된 언어 모델의 평가. 모든 모델은 파일[30]에서 추출한 100억 토큰의 동일한 세트에 대해 훈련되었다. 본 논문에서는 pre-fill의 \\(360\\)m param에 대한 \\(4,096\\) 토큰(\\(16,384\\)과 아키텍처의 하위 집합에 대한 순환 생성의 \\(2,048\\) 토큰에 대한 추론 처리량을 보고한다. 전체 파일 테스트 세트에 대한 언어 모델 복잡성과 테스트 세트의 두 슬라이스에 대한 복잡성을 보고한다(섹션 6.1, [1] 참조). 우리는 SWDE와 FDA에 대한 정보 검색과 SQUAD에 대한 질문 응답의 세 가지 _recall-intensive_ 태스크에 대한 제로 샷 성능을 보고한다. 마지막으로, 구와 도에서 사용되는 상식 추론 과제인 _LM Eval Harness_[16]의 집합에 대한 평균 성능을 부록 D에서 보고한다. 이러한 과제는 입력 텍스트가 일반적으로 매우 짧기 때문에 상당한 회상 능력을 필요로 하지 않는다. 섹션 6.1을 참조하십시오. 생성에 대한 반복 뷰를 구현하지 않는 일부 제안된 아키텍처는 다음과 같이 표시됩니다.\n' +
      '\n' +
      '**3.1**: \\(p\\)-hot encodings(Definition F.7)이라고 하는 원-hot encodings을 일반화하는 입력 인코딩 클래스에 대해, 입력 종속적인 BaseConv는 \\(d=p\\cdot\\sqrt[p]{c}\\)인 MQAR을 해결하기 위해 적어도 \\(\\lfloor\\log(2p)\\rfloor\\) 층이 필요하다.\n' +
      '\n' +
      '마지막으로, BaseConv[1]의 기반인 BaseConv[1]을 이용하여 계층 수(부록 F의 명제 F.1)에서 다중 로그 블로업을 수행하여 Gated-convolution 구조에 대한 선형 주의의 상대적 효율성을 나타내는 선형 주의[20]을 시뮬레이션할 수 있음을 보인다.\n' +
      '\n' +
      '##4 기반 건축\n' +
      '\n' +
      '이 절에서는 Based를 소개합니다. 이 아키텍처를 설계하는 우리의 목표는 잘 알려진 아키텍처 빌딩 블록을 사용하여 메모리-리콜 트레이드오프의 파레토 프론티어를 탐색하는 방법을 시연하는 것이다.\n' +
      '\n' +
      '소프트맥스 주의력은 리콜에서 탁월하지만 KV-캐시는 시퀀스의 길이에 따라 제한되지 않고 성장하기 때문에 그림 2의 오른쪽 상단 사분면에 고정된다. 우리는 주의력의 반복 상태 크기를 제한하기 위한 두 가지 간단한 접근 방법인 선형 주의력과 슬라이딩 윈도우 주의력을 연구한다. 선형 주의(_i.e._ attention without softmax)의 반복 상태 크기는 시퀀스 길이에 따라 성장하지 않으며 간단한 하이퍼파라미터를 변경함으로써 변조될 수 있다[20]. 슬라이딩 윈도우 주의를 기울이면 반복 상태 크기를 윈도우의 너비가 되도록 캡을 씌운다.\n' +
      '\n' +
      '그러나 실제 언어 모델링(표 4) 및 합성 연상 회상(그림 1 중간)에 대한 우리의 실험은 원시만으로는 파레토 프론티어를 탐색하기에 충분하지 않음을 시사한다. 선형 주의는 로컬 토큰 이동 및 비교[1, 31]를 수행하는 정밀도가 부족합니다. 슬라이딩 윈도우 주의에서 연관 회상 범위는 윈도우의 폭(도 2, 중심)에 의해 제한된다. 윈도우 크기를 증가시킴에 따라 반복 상태는 선형적으로 성장하며 병렬 훈련 및 추론 시 속도에 비선형적인 영향을 미친다(그림 2, 왼쪽).\n' +
      '\n' +
      '기반 모델은 단순히 (1) 전 세계적으로 적용되는 소프트맥스 근사 선형 주의력과 (2) 작은 슬라이딩 윈도우에서 국부적으로 적용되는 정확한 소프트맥스 주의력을 결합한다(그림 1, 오른쪽). 이 조합을 사용하면 1e-5\\(\\times\\)의 대기 시간에서 전체 소프트맥스 주의의 회상 정확도의 90.8%를 복구하는 놀랍도록 작은 슬라이딩 윈도우(예: 64 토큰)에서 소프트맥스 주의를 사용할 수 있다. Sliding window and linear attention _alone_ fail (도 1, 좌측).\n' +
      '\n' +
      '테일러 선형 주의\n' +
      '\n' +
      '선형 특징 맵을 사용하여 소프트맥스 어텐션을 근사화함으로써, 전역 토큰 상호 작용을 유지하면서 순환 상태의 크기를 제한할 수 있다(_i.e._각각의 토큰은 시퀀스에서 그 이전의 모든 토큰에 의존한다).\n' +
      '\n' +
      'Katharopoulos et al. [20], Choromanski et al. [21], Tsai et al. [32]는 \\(\\phi(\\mathbf{q}_{i})^{\\top}\\phi(\\mathbf{k}_{j})\\approx\\exp(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}/\\sqrt{d})\\(\\phi(\\mathbf{q}_{i}})\\approx\\exp(\\mathbf{q}_{j}/\\sqrt{d})\\(\\phi:\\mathbbb{R}^{d}\\rightarrow\\mathbb{R}^{\\tilde{d}\\)를 선택할 수 있음을 보여준다. 그런 다음 식 (1)에서 소프트맥스 주의에 대한 공식을 다음과 같이 다시 쓸 수 있다.\n' +
      '\n' +
      '\\frac{\\phi(\\mathbf{q}_{i})\\mathbf{v}_{j}}{\\top}\\phi(\\sum_{j=1}^{i}}{\\phi(\\mathbf{q}_{i})\\sum_{j=1}^{i}\\phi(\\mathbf{q}_{j})}=\\frac{\\phi(\\mathbf{q}_{i})\\sum_{j=1}^{i}\\left(\\phi(\\mathbf{k}_{j})\\sum_{j=1}^{i}\\phi(\\mathbf{q}_{j}}}\\ffi(\\mathbf{q}_{i})}\\frac{\\phi(\\mathbf{q}_{i}}}\\fff{v}_{i}}}{\\tag{2}\\ffi(\\mathbf{k}_{j})}\\frac{\\phi(\\\n' +
      '\n' +
      '모든 질의가 \\(\\mathcal{O}(Nd^{2})\\) 시간 및 공간 복잡도에서 모든 과거 키에 참석하는 경우. 또한, Katharopoulos et al. [24]는 선형 주의가 생성 동안 고정된 크기 반복 상태를 갖는다는 것을 보여준다. \\(\\mathbff{s}_{i}=\\sum_{j=1}^{i}\\phi(\\mathbff{k}_{j})^{\\top}\\mathbff{v}_{j}\\)와 \\(\\mathbff{z}_{i}=\\sum_{j=1}^{i}\\phi(\\mathbfff{k}_{j})^{\\top}\\)를 각각 "KV-상태"와 "K-상태"로 하여 식 (2)를 계산할 수 있다.\n' +
      '\n' +
      '\\mathbf{s}_{i}=\\mathbf{s}_{i-1}+\\phi(\\mathbf{s}_{i})^{\\top}\\mathbf{z}_{i},\\\\mathbf{z}_{i}=\\mathbf{z}_{i-1}+\\phi(\\mathbf{k}_{i})^{\\top},\\]\\[\\mathbf{y}_{i}=\\frac{\\phi(\\mathbf{q}_{i}}\\mathbf{z}_{i}}\\tag{3}\\mathbf{z}_{i}\n' +
      '\n' +
      '여기서 \\(\\mathbf{s}_{i}\\in\\mathbb{R}^{d\\times\\tilde{d}\\) 및 \\(\\mathbf{z}_{i}\\in\\mathbb{R}^{\\tilde{d}\\)\n' +
      '\n' +
      'Feature map. approximate \\(\\exp(\\mathbf{q}_{i}^{\\top}\\mathbff{k}_{j}/\\sqrt{d})\\)를 구하기 위해 \\(2^{\\text{nd}\\)-order Taylor series feature map, picking \\(\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d^{2}}이 되도록 \\(2^{\\text{nd}\\)-order Taylor series feature map을 사용한다.\n' +
      '\n' +
      '\\phi(\\mathbf{q}_{i})^{\\top}\\phi(\\mathbf{k}_{j})=1+\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}+\\frac{(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}}^{2}}{4}\\tag{4}\\frac{(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}}^{2}}\\tag{4}\\frac{(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}}^{2}}\\tag{4}\\frac{(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}}}1+\\mathbf{q}_{i}+\\frac{(\\mathbf{q}_{i\n' +
      '\n' +
      'Zhang et al. [13]은 \\(\\tilde{d}=d^{2}\\)의 특징맵을 선택하는 것은 \\(\\mathcal{O}(Nd^{3})\\)의 시간 및 공간 복잡성과 크기 \\(O(d^{3})\\의 큰 반복 상태를 갖는 선형 주의 집중을 초래한다는 점에 주목하지만, 질의와 키를 \\(\\mathbf{W}_{q},\\mathbf{W}_{k}\\in\\mathbbb{R}^{d\\times d^{prime}\\)의 작은 차원으로 투영함으로써 리콜 용량에 대한 트레이드오프 효율을 얻을 수 있다. 우리는 \\(d^{\\prime}\\)을 변화시킴으로써 반복상태의 크기를 조절한다.\n' +
      '\n' +
      'feature map의 선택이 memory-recall trade-off?__ 이전 작업은 결합 리콜에 대한 테일러 특징 맵의 강력한 성능을 보여준다[13]. 이 분석을 바탕으로 [23]에 정의된 특징맵(\\(\\phi_{\\text{ReLU}(x)=\\max(x,0)\\), \\(\\phi_{\\text{PosEUL}(x)=\\text{ELU}(x)+1\\), \\(\\phi_{\\text{Square}(x)=x^{2}\\), \\(\\phi_{\\text{Identity}(x)=x\\), \\(\\phi_{\\text{CosFormer}}\\), \\(\\phi_{\\text{Performer}}\\)의 광범위한 특징맵 집합(\\(\\phi_{\\text{ReLU}(x)=\\max(x,0)\\), \\(\\phi_{\\text{PosEUL}(x)=\\text{ELU}(x)+1\\), \\(\\phi_{\\text{Square}}(x)=x^{2}\\), \\(\\phi_{\\text{Identity}(x)=x\\), \\(\\phi_{\\text{Cos 테일러 계열 특징맵은 간단한 \\(\\phi_{\\text{PosEUL}}\\) 및 \\(\\phi_{\\text{ReLU}}\\) 특징맵과 함께 파레토 프론티어에 위치한다. 이러한 대안들에 비해 테일러 특징 맵의 한 가지 장점은 파라미터의 수를 변경하지 않고 반복 상태 크기(리콜 용량 개선)를 확장한다는 것이다. 도 3(하단)에 도시된 바와 같이, 테일러 시리즈 특징 맵은 높은 재현 용량을 달성하기 위해 대안보다 더 적은 파라미터를 필요로 한다. 이 분석과 \\(표\\)4의 어블레이션은 다른 간단한 특징 맵도 효과적일 수 있지만 테일러 근사치를 사용하기로 결정했음을 알렸다.\n' +
      '\n' +
      '### 텐서코어 슬라이딩 윈도우를 이용한 국부적 정밀 주의\n' +
      '\n' +
      '세립된 로컬 상호 작용을 효율적으로 모델링하기 위해, Based는 16(최대 64 토큰)의 작은 배수로 설정된 윈도우 크기를 갖는 슬라이딩 윈도우 어텐션을 사용한다. 과거(인과적) 구현[18, 19]과 유사하게, 윈도우 크기\\(w\\)에 대해 각 질의\\(\\mathbf{q}_{i}\\)는 과거 키\\(\\{\\mathbf{k}_{i-w+1},\\dots,\\mathbf{k}_{i}\\}\\)에만 적용된다. 이는 상수 메모리 생성을 위한 \\(w\\) 크기의 KV-캐시와 시퀀스 길이 \\(N\\)의 선형 스케일링을 위한 \\(\\mathcal{O}(Nw)\\) 시간 및 공간 복잡도를 가능하게 한다.\n' +
      '\n' +
      '그러나, 크기 256[17]에서 4096[10]까지 \\(w\\)을 유지하는 과거의 슬라이딩 윈도우 주의와 달리, Based는 현대 GPU를 가장 잘 활용하기 위해 \\(w=16\\), 32 또는 64만을 사용한다. 섹션 5에서, 우리는 이 "텐서 코어 인식" 윈도우(tcWindow)가 현대의 LLMs(_e.g.,_Mistral 7B[10])에서 \\(w=4096\\) 윈도우보다 1e-5\\(\\times\\)의 대기시간을 달성하는 방법에 대해 논의한다.\n' +
      '\n' +
      'tcWindow의 작은 \\(w\\)는 빠른 로컬 및 정확한 주의를 가능하게 하지만, 장거리 모델링에 대한 과제를 제시한다. 단지 \\(w=64\\)으로, \\(w=4096\\)의 미스트랄 슬라이딩 윈도우 주의의 모든 층에 대해 우리는 동일한 수용 필드를 달성하기 위해 64개의 기반 층이 필요할 것이다. 모델 깊이와 시퀀스 길이를 제어하는 그림 2는 실제로 연상 재현율 정확도에서 선형적으로 감소하는 더 작은 \\(w\\)을 보여준다. 위에서 설명한 global _linear attention_는 낮은 \\(w\\)으로 제시된 장거리 모델링의 부족을 극복한다.\n' +
      '\n' +
      '기반에 대한 추가 아키텍처 세부 사항은 부록 C에서 논의되고 실험에 사용된 레이어의 혼성화는 표 7에 제공된다. 우리는 표 4의 아키텍처 선택 삭제를 포함하고 섹션 6의 기반에 대한 전반적인 품질과 효율성을 평가한다.\n' +
      '\n' +
      '도 3: ** AR.**\\(x\\): 세대 또는 파라메터 동안의 상태 크기(바이트) 상의 선형 주의 특징 맵. count; \\(y\\): MQAR 정확도. 이 설정은 무화과보다 어렵다. 2(256 키-값 쌍)입니다. 현대 GPU를 개발합니다. 섹션 5에서, 우리는 이 "텐서 코어 인식" 윈도우(tcWindow)가 현대의 LLMs(_e.g.,_Mistral 7B[10])에서 \\(w=4096\\) 윈도우보다 1e-5\\(\\times\\)의 대기시간을 달성하는 방법에 대해 논의한다.\n' +
      '\n' +
      'Efficient Implementation\n' +
      '\n' +
      '이 절에서는 Based의 효율성에 중점을 둡니다. 순진한 구현은 많은 양의 높은 레이턴시 메모리 이동을 요구하기 때문에 가장 효율적인 표준 주의 구현(도 4에 도시됨)보다 _slower_이다. 먼저 GPU 실행 모델과 메모리 계층에 대한 예비 사항을 설명한다. 다음으로 섹션 5.1의 선형 주의와 섹션 5.2의 슬라이딩 윈도우 주의에 대한 기준선과 하드웨어 인식 알고리즘을 제시한다.\n' +
      '\n' +
      '예비 GPU 연산 또는 _kernels_는 수천 개의 병렬 스레드에 의해 실행된다. NVIDIA 용어에서 GPU 스트리밍 멀티프로세서는 소프트웨어 수준에서 _thread block_를 시작한다. 이들 블록은 하드웨어 레벨에서 코어에 할당되는 _warps_(_e.g._32 스레드)로 분할된다. 스레드는 계산을 수행하고 출력을 쓰기 위해 입력들을 그들의 _registers_로 판독할 필요가 있다. 읽고 쓰는 데 걸리는 시간을 IO 비용이라고 한다.\n' +
      '\n' +
      '데이터를 로드하는 시간 대 시간에 따라 작업이 메모리 또는 계산 바인딩일 수 있습니다. 로드된 데이터에 대한 계산을 수행합니다. IO 인식 알고리즘을 설계할 때 현대 GPU의 두 가지 핵심 속성을 활용하고자 한다. 첫째, 텐서 코어 유닛(고속 매트릭스 다중 유닛)은 비-매트릭스 다중 코어에 대해 19 TFLOP/s에 대해 312 TFLOP/s 속도를 달성한다. 둘째, GPU는 많은 양의 느린 액세스 메모리와 적은 양의 빠른 액세스 메모리를 가진 메모리 계층에 직면한다. 액세스 속도는 데이터 주소에 대한 프로세서의 근접성에 의해 지배된다. 예를 들어, 현대의 NVIDIA 80GB A100 GPU 상의 계층은: 2 TB/s 대역폭을 갖는 HBM 80GB, L2 캐시의 80MB, SM당 19 TB/s 대역폭을 갖는 L1 캐시/공유 메모리(SRAM을 통해 구현됨) 192KB, SM당 256KB 레지스터 파일[33]이다. 레지스터 메모리는 실행 스레드에 비공개이므로, 스레드는 블록 내의 다른 스레드에 데이터를 통신하기 위해 공유 메모리에 기록할 필요가 있다. 읽기 및 쓰기에 필요한 시간을 줄이기 위해, 핵심 원리는 느린 메모리에 다시 쓰기 전에 빠른 메모리에 있는 동안 동일한 데이터 슬라이스에서 _fuse_ 다중 연산을 수행하는 것이다.\n' +
      '\n' +
      '테일러 지수 선형 주의\n' +
      '\n' +
      '이론적으로 개선된 복잡성에도 불구하고, 이전 작업에서 입증된 선형 주의 방법은 실제 벽-시계 시간 및 메모리 사용에서 측정될 때 매우 최적화된 소프트맥스 주의 구현(플래시 주의[12])보다 종종 덜 효율적이다. 다음으로 테일러 선형 주의를 효율적으로 하기 위한 하드웨어 인식 알고리즘을 제시한다. 우리는 (1) 프리필, 생성 중 프롬프트 또는 트레이닝 중 순방향 패스를 처리하는 것과 (2) 생성 중 다음 토큰 예측의 두 가지 연산에 초점을 맞추며, 이는 또한 순환 은닉 상태 상태를 업데이트해야 한다.\n' +
      '\n' +
      '이 절에서는 4절에 따라 배치 크기, 머리 수, 머리 치수, 순서 길이, 특징 치수 등을 \\(B\\), \\(H\\), \\(d\\), \\(N\\), \\(d^{\\prime}\\)으로 표기한다. 표기의 용이성을 위해 이 절에서는 \\(D=1+d^{\\prime}+d^{\\prime2}\\으로 표기한다. IO-Aware 알고리즘에 대한 추가 세부 사항은 부록 B에 포함되어 있다.\n' +
      '\n' +
      '1.1 Forward Pass/ Generation Prefill 5.1.1\n' +
      '\n' +
      '기저선 구현 부록 B(1)에 자세히 설명된 순진한 구현은 특징 맵(Q,K\\)을 생성하고, (2) 큰 은닉 상태(KV\\in\\mathbb{R}^{H\\times d\\times D}\\)를 계산하고 구체화한 다음, (3) \\(Q\\)과 \\(KV\\) 사이의 인과 내적을 계산한다. 선행 작업은 인과 내적/단계 (3)을 효율적으로 수행하기 위해 선형 주의를 위한 대중적인 CUDA 커널을 출시하였다[34]. 커널은 헤드 및 배치에 걸쳐 계산을 병렬화하고, SRAM에 \\(V\\) 및 \\(K\\)의 타일을 로드하고, SRAM에서 실행 중인 \\(KV\\) 상태를 업데이트하고, SRAM에 \\(Q\\)의 타일을 로드하고, SRAM에서 최종 출력을 생성하고, 그 결과를 HBM에 기록한다.\n' +
      '\n' +
      '전체 IO 비용에서 선형 주의 계층에서의 입력 및 출력 투영을 무시하고, 이 절차는 HBM에 Faturized \\(Q,K\\)을 쓰기 위해 \\(2BHND\\) 바이트가 필요하다. 인과적 내적 동안, 이것은 결과를 쓰기 위해 \\(Q,K,V\\) 타일을 읽기 위해 \\(2BHND+BHNd\\) 바이트와 \\(BHNd\\) 바이트가 필요하다. 계산 전반에 걸쳐 \\(\\mathcal{O}(BHNDd)\\) 바이트(note this is shape \\(KV\\) state during the forward pass)는 스레드 레지스터에서 SRAM으로 읽기 및 읽기하여 19TB/s 대역폭에서 실행 출력과 \\(KV\\) 상태를 업데이트한다.\n' +
      '\n' +
      '효율성을 향상시키기 위해 본 논문에서 제안하는 알고리즘은 고속 메모리에서 특징 맵과 인과 내적을 모두 계산한다. 우리의 전체 알고리즘은 알고리즘 1에 자세히 설명되어 있으며 여기에서 전체 프로세스에 대해 논의한다.\n' +
      '\n' +
      '알고리즘은 먼저 각 배치의 각 헤드에 대해 계산이 독립적이기 때문에 배치 \\(B\\) 및 헤드 \\(H\\) 차원에 걸쳐 병렬화된다. 우리는 선형 주의 출력에서 세 항 \\(T_{0},T_{1},T_{2}\\in\\mathbb{R}^{N\\times d}\\)을 고려하며, 이 세 항은 지수함수 테일러 다항식에 해당한다. 또한 수열을 처리할 때 누적 \\(KV\\) 상태에서 해당 세 항을 고려한다. 주어진 _tile_(Q,K,V\\의 sub-matrix)에 대해, 16개의 토큰, 16개의 특징 차원 및 64개의 헤드 차원이 주어진 타일(예를 들어, \\(Q,K\\in\\mathbb{R}^{16\\times 16}\\)과 \\(V\\in\\mathbb{R}^{16\\times 64}\\)을 고속 메모리에 로드하고 세 항 각각에 대해 실행 중인 \\(KV\\) 상태와 출력을 업데이트한다. 레지스터 파일에 타일을 로드하고 퓨즈 동작을 통해 레지스터의 인과적 내적을 계산하며, 병렬 워프에 걸쳐 누적 \\(KV\\) 상태를 동기화하기 위해 SRAM에 기록만 한다. 우리는 레지스터 메모리가 워프-특정(하나의 워프에 있는 스레드는 다른 워프에 있는 스레드에 대한 레지스터에 액세스하지 않음)이기 때문에 이 동기화를 위해 SRAM에 명시적 쓰기를 사용한다.\n' +
      '\n' +
      'IO 비용에서 선형 주의층의 입력 및 출력 투영을 무시하고 HBM과 SRAM 사이에 \\(q,k\\)을 읽기 위한 \\(2BHNd^{\\prime}\\) 바이트와 \\(v\\)을 읽기 위한 \\(2BHNd\\) 바이트와 쓰기 출력 \\(y\\)을 요구한다. 전반적으로, 본 알고리즘은 HBM에서 SRAM 데이터 이동으로 HBM(\\mathcal{O}(2BHND)\\) 바이트를 회피한다. 또한 데이터 이동을 등록하기 위해 SRAM에서 \\(\\mathcal{O}(BHNDd)\\) 바이트를 피하기 위해 계산 인 레지스터를 수행함으로써 기준선을 개선한다.\n' +
      '\n' +
      '###### 5.1.2 다음 토큰 예측\n' +
      '\n' +
      '다음 토큰 예측시 중요한 고려사항은 타임스텝(t\\)에서 순환상태\\(KV_{t}\\in\\mathbb{R}^{BHDd}\\)를 효율적으로 갱신하는 방법이다. 다음 토큰 예측시 값비싼 연산은 투영된 은닉 상태\\(k_{t+1}\\in\\mathbb{R}^{BHD}\\)와 \\(v_{t+1}\\in\\mathbb{R}^{BHd}\\) 사이의 외부 곱을 계산하는 것이다. 외부곱은 \\(\\mathcal{O}(BHDd)\\) 계산과 공간을 필요로 하며, 그 결과를 \\(KV_{t}\\)으로 합하여 \\(KV_{t+1}\\)을 생성한다. 본 논문에서는 알고리즘 2에서 상태 업데이트를 위한 IO 인식 알고리즘을 제안한다. 이 알고리즘은 SRAM 데이터 이동(q,k,v) 투영을 로드하기 위해 HBM의 바이트 수를 증가시킨다.\n' +
      '\n' +
      '### tcWindow\n' +
      '\n' +
      '다음으로 우리는 tcWindow에 대한 윈도우 크기 선택에 동기를 부여한다. 일반적인 미스트랄 모델과 같은 슬라이딩 윈도우 스타일 모델은 큰 윈도우 크기\\(w=4096\\)[10]를 사용하는 것과 달리 하드웨어 사양에 따라 윈도우 크기를 선택한다. GPU 텐서 코어는 \\(16\\times 16\\) 타일에 동작한다. 대형 GEMM은 컴퓨팅 바인딩(예: 긴 컨텍스트 주의)입니다. 그러나 텐서 코어 유닛의 대기 시간을 숨기기 위해 충분한 점유량이 필요하다. 그림 1(오른쪽)은 \\(64\\times 64\\) 차원 행렬 곱셈이 \\(16\\times 16\\)과 거의 동일한 지연 시간을 보여준다. 기본 집합 \\(w\\)을 사용하여 \\(64\\ 곱하기 64\\) 타일을 사용합니다. 이전 슬라이딩 창과 구별하기 위해 이 접근법을 tcWindow라고 한다. 학습 중 플래시 어텐션 슬라이딩 윈도우 구현[11]을 사용하고, 부록 B 알고리즘 3에서는 다음 토큰 예측을 위한 tcWindow의 IO 인식 알고리즘을 제공한다.\n' +
      '\n' +
      '이러한 IO 인식 알고리즘으로 구현된 기반에 대한 엔드 투 엔드 벤치마크는 섹션 6에 제공되며, 베이스라인 구현에 대한 각 커널에 대한 마이크로 벤치마크는 부록 B에 제공된다.\n' +
      '\n' +
      '## 6 Results\n' +
      '\n' +
      '본 절에서는 다음의 청구항들에 대한 결과들을 제시한다:\n' +
      '\n' +
      '1. **언어 모델링 전체.** 파일 [30] 및 LM Eval Harness [16]의 표준 벤치마크에서 처음부터 사전 훈련의 아키텍처를 평가한다. 우리는 이러한 설정에서 기반 일치를 찾거나 가장 강력한 하위 2차 아키텍처(예: 맘바[5])를 능가합니다.\n' +
      '2. **언어 모델링 리콜.** 기반은 실제 세계 파일 언어 모델링 코퍼스(표 1 참조)의 도전적 연상 리콜 슬라이스에 대한 주목의 갭을 좁혀, 이전의 하위 2차 아키텍처를 능가한다. 이러한 사전 학습된 모델들을 리콜 집약 태스크(예: 정보 추출, QA)의 집합에 제로샷(zero-shot)을 적용하여, Based가 유사한 처리량을 갖는 다른 효율적인 아키텍처를 체계적으로 능가한다는 것을 보여준다.\n' +
      '\n' +
      '3. **Generation throughput.** Based에서 반복 생성의 IO-aware 구현은 배치 크기 128에서 1024개의 토큰을 생성하는데 있어서 FlashAttention-2와 Mamba에 비해 \\(40-60\\%\\)의 속도 향상을 가능하게 한다(도 4 참조).\n' +
      '4. **키 디자인 선택의 Ablations.** 우리는 (1) 특징 맵, (2) 특징 치수, (3) 로컬 슬라이딩-윈도우 소프트맥스의 사용, (4) 짧은 게이트-컨볼루션의 네 가지 키 디자인 선택 및 하이퍼파라미터를 ablate한다. 이들 절제의 결과를 표 4에 나타낸다.\n' +
      '\n' +
      '기준선 문헌의 주요 기준선과 비교합니다. 회전식 엔코딩(35)과 게이트 선형 유닛을 추가한 트랜스포머(GPT 아키텍처)와 트랜스포머++(Llama 아키텍처[15])와 비교한다. 본 논문에서는 하이에나[7], RWKV[8], H3[9]를 포함한 게이팅 프리미티브와 롱컨볼루션 프리미티브로 구성된 효율적인 아키텍처의 초기 클래스를 비교한다. 마지막으로 Mamba[5] 및 Gated Linear Attention[6]을 포함한 긴 컨볼루션 모델에서 품질을 향상시키기 위해 입력 종속 시퀀스 집계를 사용하는 최근의 최신 아키텍처와 비교한다.\n' +
      '\n' +
      '### 언어 모델링 평가\n' +
      '\n' +
      '언어 모델링 벤치마크는 파일 [30]에서 두 개의 매개변수 척도(355M 및 1.3Bn 매개변수)에서 처음부터 언어 모델을 사전 훈련한다. 각 모델은 동일한 100억 토큰의 사전 훈련 데이터를 동일한 순서로 본다. 파일 데이터는 GPT-2 BPE 토큰화기를 이용하여 토큰화된다[36]. 우리는 파일에 대한 당혹감을 측정하고 표 1의 결과를 보고하며 추가 실험 세부 정보는 부록 E.1에 제공된다.\n' +
      '\n' +
      '기준선 [5, 6]에 대한 표준 프로토콜과 마찬가지로 표준 LM Eval Harness [16]에서 사전 훈련된 모델을 추가로 평가한다. 작업 및 메트릭에 대한 자세한 내역은 부록 D에서 찾을 수 있다. 사전 훈련과 LM 평가 하니스 모두에서 기반은 가장 강력한 트랜스포머++ 및 맘바 베이스라인과 일관되게 경쟁한다. 이러한 전반적인 메트릭이 도움이 되지만, 다음으로 실제 데이터에 대한 리콜 및 상황 내 학습 능력에 대한 세밀한 분석으로 전환한다.\n' +
      '\n' +
      'Recall 및 In-Context Learning 평가 표 1에서 다운스트림 리콜 용량을 테스트하기 위해 선택된 일련의 컨텍스트 학습 태스크에 대해 사전 훈련된 모델을 평가한다. 이러한 태스크는 (1) **실세계 AR** 복잡성 점수를 넘어 다음 토큰 예측을 파일에서 슬라이스하여 각 아키텍처의 AR 품질을 이해한다(부록 E.1). (2) **정보 추출(IE)** SWDE 및 FDA는 각각 인기 있는 반구조 및 비구조 문서 IE 벤치마크이다[37, 38, 39]. SWDE는 8개의 영화 및 5개의 대학 웹사이트(예를 들어, IMDB, US News)에 대한 _raw HTML_ 및 웹사이트당 8-274개의 속성(예를 들어, 영화 런타임)에 대한 주석, 및 (3)**문맥내 패시지에서 질문 응답**을 갖는다. 본 논문에서 제안하는 기반 아키텍처는 이러한 평가에서 기준선 하위 2차 아키텍처를 능가하여 트랜스포머++에 대한 격차를 좁힌다. 이러한 경향은 섹션 3.1의 MQAR 합성 결과를 추적한다.\n' +
      '\n' +
      '도 4: (**Left**) 2의 고정된 배치 크기에서 다양한 프리필 시퀀스 길이에 대한 처리량 수. **Right** 1024 토큰의 고정된 생성 길이에서 다양한 배치 크기에서 생성 처리량. \\(y\\)-축은 인 레이턴시(ms)를 나타낸다. 모델 메모리가 부족할 때 선이 차단됩니다. 우리는 360M과 1.3Bn 모두에 대한 결과를 보여주며 모든 숫자는 단일 NVIDIA H100 GPU에서 계산된다.\n' +
      '\n' +
      '본 연구에서는 상황 내 학습 성능을 이해하기 위해 Table 2의 SuperGLUE 벤치마크 [40]에 대한 몇 개의 샷 평가를 수행하였고, 각 모델은 0 샷(_i.e._, 상황 내 예제 수), 1 샷 및 5 샷 프롬프트를 사용하여 모든 태스크에 대해 평가되었다. 트랜스포머++와 Based는 모두 샷 수를 늘리는 것에서 단조로운 개선을 보인다. 그러나 맘바의 경우 0-샷에서 1-샷으로 약간 개선되었지만 0-샷보다 5-샷에서 더 나쁜 성능을 보인다. 이 결과는 맘바에서 관찰된 제한된 회상 능력이 소수의 샷 능력에도 영향을 미칠 수 있음을 시사한다.\n' +
      '\n' +
      'DNA 모델링은 자연 영어를 넘어 Based의 능력을 이해하기 위해 다음에는 DNA 다음 토큰 예측을 수행하는 능력에 대해 각 아키텍처를 평가한다(표 3). 본 논문에서는 HG38(human genome) 벤치마크에 대한 아키텍쳐를 \\(1k\\), \\(4k\\), \\(8k\\)의 서열 길이에서 평가한다 [5, 41]. DNA 태스크는 바이트 레벨 토큰화기를 사용하며, 여기서 어휘는 뉴클레오티드 염기에 대응하는 문자로 구성된다. 우리는 Based가 평가된 시퀀스 길이에 걸쳐 최첨단 아키텍처와 경쟁적이라는 것을 발견합니다. 부록 D에서 다운스트림 DNA 분류 작업에 대한 이러한 체크포인트를 평가한다.\n' +
      '\n' +
      '### Efficiency Benchmarks\n' +
      '\n' +
      '우리는 제안된 IO-Aware 알고리즘이 있거나 없는 Based의 처리량을 벤치마킹한다(섹션 5). 우리는 **forward pass/ generation prefill** 및 **next token prediction** 단계를 모두 고려한다. 실험은 H100 NVIDIA GPU를 사용하여 실행되었으며 평균 20회 반복되었다. 결과는 그림 4에 나와 있다.\n' +
      '\n' +
      '우리의 효율적인 구현(섹션 5)을 사용하여, Based는 \\(4k\\) 시퀀스 길이에서 FlashAttention-2[11]보다 56% 더 빠른 프리필과 Mamba보다 44% 더 빠른 1.3Bn 매개변수(FlashAttention-2보다 28% 더 빠르고 360M 매개변수에서 Mamba보다 76% 더 빠른)를 달성한다. 본 논문에서는 플래시 어텐션-2 구현에서 가장 최적화된 플래시 어텐션-2 구현보다 다음 토큰 생성이 더 높은 처리율(tokens/second)을 제공하며, 배치 크기 128과 1.3Bn 파라미터에서 순환 Mamba 아키텍처의 처리율(98% 더 높은 처리율 vs. Flash 어텐션-2와 118% 더 높은 처리율 vs. Mamba)을 달성한다. 모든 벤치마크는 다음 토큰 예측 동안 CUDA 캐시 그래프를 사용하여 단일 NVIDIA H100 GPU에 있다[42].\n' +
      '\n' +
      '그림 4에서는 인기 있는 Fast를 사용하는 Based의 기본 구현에 대한 결과도 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Shots** & **BoolQ** & **CB** & **COPA** & **MultiRC** & **ReCoRD** & **RTE** & **WiC** & **WSC** & **Avg** \\\\  & & Acc. \\(\\uparrow\\) & Acc. \\(\\uparrow\\) & F1 \\(\\uparrow\\) & Acc. \\(\\uparrow\\) & Acc. \\(\\uparrow\\) & F1 \\(\\uparrow\\) & F1 \\(\\uparrow\\) & EM \\(\\uparrow\\) & Acc. \\(\\uparrow\\) & Acc. \\(\\uparrow\\) & Acc. \\(\\uparrow\\) \\\\ \\hline \\hline \\multirow{2}{*}{Based} & 0 & 59.0 & 41.1 & 19.4 & 69.0 & 54.9 & 14.5 & 14.0 & 52.0 & 50.0 & 36.5 & 45.7 \\\\  & 1 & 57.5 & 37.5 & 26.8 & 68.0 & 52.5 & 19.9 & 19.2 & 47.7 & 50.9 & 49.0 & 47.2 \\\\  & 5 & 56.6 & 44.6 & 28.9 & 73.0 & 53.6 & 24.9 & 24.1 & 48.7 & 51.1 & 39.4 & 48.0 \\\\ \\hline \\multirow{2}{*}{Transformer++} & 0 & 57.3 & 41.1 & 21.3 & 67.0 & 57.0 & 16.6 & 16.1 & 53.8 & 50.0 & 37.5 & 46.3 \\\\  & 1 & 54.2 & 39.3 & 25.3 & 69.0 & 51.5 & 22.2 & 21.6 & 50.9 & 47.0 & 55.8 & 47.8 \\\\  & 5 & 50.7 & 58.9 & 49.9 & 64.0 & 46.9 & 24.2 & 23.6 & 47.3 & 52.2 & 51.9 & 48.9 \\\\ \\hline Mamba & 0 & 57.5 & 35.7 & 24.4 & 71.0 & 57.2 & 18.8 & 18.3 & 52.4 & 50.0 & 36.5 & 46.6 \\\\  & 1 & 51.1 & 39.3 & 27.4 & 71.0 & 52.9 & 21.6 & 21.0 & 46.6 & 46.2 & 52.9 & 46.9 \\\\  & 5 & 41.1 & 37.5 & 23.6 & 69.0 & 49.2 & 20.4 & 19.9 & 48.4 & 51.7 & 51.9 & 45.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 미리 훈련된 언어 모델의 SuperGLUE에 대한 **Few-shot 다운스트림 평가.** 표 1과 동일한 모델 세트는 모두 파일 [30]에서 추출한 동일한 100억 토큰에 대해 훈련되었으며, EleutherAI [16]의 LM 평가 하니스를 사용하여 SuperGLUE 벤치마크 [40]에서 평가되었다. 평균을 계산할 때 먼저 작업별로 메트릭을 평균화한 다음 작업 전반에 걸쳐 평균을 낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & & \\multicolumn{4}{c}{**HG38 PPL \\(\\downarrow\\)**} \\\\ Model & Params & \\(N=1024\\) & \\(N=4096\\) & \\(N=8192\\) \\\\ \\hline \\hline Transformer++ & 46.2 & 2.52 & 2.50 & 2.51 \\\\ Mamba & 46.1 & **2.51** & **2.49** & **2.49** \\\\ Based & 48.8 & **2.51** & 2.50 & **2.49** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: HG38 데이터세트에 대한 **DNA 모델링 성능.** 모든 모델은 각각 \\(N=1\\)k, 4k 및 8k 서열 길이에서 10Bn 토큰에 대해 처음부터 사전 훈련된다. 각 아키텍처별 학습률을 하이퍼파라미터 스윕한 후 결과를 보고한다.\n' +
      '\n' +
      '원인 내적 [34]를 계산하기 위한 선형 주의 문헌의 변환자 CUDA 커널(섹션 5에서 논의됨)이다. 우리 작업에 도입된 _custom kernel_은 Based의 효율성을 해제한다.\n' +
      '\n' +
      '마이크로 벤치마크는 엔드 투 엔드 기반 아키텍처가 하이브리드 아키텍처이므로 부록 B의 주요 베이스라인 구현과 비교하여 개별 IO 인식 커널의 마이크로 벤치마크를 제공한다.\n' +
      '\n' +
      '### Quality Ablations\n' +
      '\n' +
      '우리의 목표는 강력한 성능을 달성하는 가장 간단한 선형 주의 모델의 처리량과 회상을 측정하는 것이다. 따라서 본 논문에서는 주요 설계 결정인 피쳐 맵의 선택, 테일러 맵의 피쳐 치수, 슬라이딩 윈도우 및 컨볼루션을 축소하여 기반 품질에 대한 기여도를 파악한다. 우리는 이전 실험과 동일한 수의 토큰과 데이터 순서를 사용하여 파일 코퍼스[30]에서 이러한 삭제를 수행한다.\n' +
      '\n' +
      '*feature map ablations**에서 우리는 이전 작업 [13]에서 강한 선형 주의력으로 입증된 CosFormer [23] 및 Performers [21] 특징 맵을 고려한다. 우리는 또한 학습된 투영을 사용하여 상태 크기를 확장하고 테일러 맵의 더 큰 상태 크기와 비교하기 위해 CosFormer를 적용하는 기준선을 포함한다. 이러한 기준선의 경우, 나머지 기반 아키텍처는 동일하게 유지한다(선형 주의 계층 수 및 슬라이딩 윈도우 및 게이티드 컨볼루션 계층과의 혼성화). 우리는 주 크기가 클수록 Cos이전의 품질이 테일러 맵과 점점 더 경쟁력이 있다는 것을 관찰한다. 우리는 상태 크기를 확장하려면 테일러 맵과 대조적으로 Cos이전 기준선에서 모델의 전체 매개변수 카운트(학습된 투영으로 인해)를 증가시켜야 한다는 점에 주목한다.\n' +
      '\n' +
      '다음으로, 우리는 테일러 맵에 고정된 특징 맵을 유지하면서 **특징 차원**을 삭제한다. 우리는 더 큰 특징 차원이 24차원에서 32차원으로 감소하면서 품질을 향상시킨다는 것을 발견한다. 특징 차원 \\(\\sqrt{1024}=32\\)에 주목하고, 여기서 1024는 우리의 실험에서 360 파라미터 스케일에서의 주의 모델 차원이다.\n' +
      '\n' +
      '마지막으로, 삭제는 **컨볼루션** 및/또는 **슬라이딩 윈도우 어텐션**을 제거하는 것이 품질을 저하시킨다는 것을 보여준다. 우리는 _either_convolutions 또는 슬라이딩 윈도우를 추가하는 것이 _neither_에 대한 연관 리콜 슬라이스에 도움이 된다는 것을 관찰한다(예를 들어, 2.29 AR Ppl. on the Pile with _neither_ vs. 2.09 or 2.11 with sliding window _or_convolutions). 또한, 윈도우 크기를 0에서 64로 증가시키는 단계를 더 포함할 수 있다. 64 내지 128(또한 도 1의 효율적인 설계 포인트, 좌측)은 품질을 계속 돕지만, 한계 개선이 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline  & \\multicolumn{3}{c}{**Hyperparameters**} & \\multicolumn{3}{c}{**Language Modeling (Pile)**} & \\multicolumn{1}{c}{**Info. Extraction**} & \\multicolumn{1}{c}{**QA**} \\\\  & Feat. Map & Feat. Dim. & Sliding & Convs. & Decay & All & AR & Other & FDA & SQUAD \\\\ \\hline Taylor Exp. (2nd) & 16 (153) & ✓(64) & ✓ & ✓ & **8.65** & **2.07** & **9.64** & **11.71** & 25.07 \\\\ Performer & 16 (16) & ✓(64) & ✓ & ✓ & 9.08 & 8.53 & 11.62 & 0.36 & 7.47 \\\\ CosFormer & 16 (32) & ✓(64) & ✓ & ✓ & 9.03 & 2.42 & 9.98 & 7.71 & 24.63 \\\\ CosFormer & 64 (128) & ✓(64) & ✓ & ✓ & 8.82 & 2.18 & 9.80 & 9.07 & **27.85** \\\\ \\hline Taylor Exp. (2nd) & 32 (561) & ✓(64) & ✓ & ✓ & **8.56** & **2.00** & **9.57** & 12.89 & **26.74** \\\\ Taylor Exp. (2nd) & 24 (325) & ✓(64) & ✓ & ✓ & 8.58 & 2.02 & 9.58 & **20.87** & 24.77 \\\\ Taylor Exp. (2nd) & 16 (153) & ✓(64) & ✓ & ✓ & 8.65 & 2.07 & 9.64 & 11.71 & 25.07 \\\\ Taylor Exp. (2nd) & 8 (45) & ✓(64) & ✓ & ✓ & 8.77 & 2.18 & 9.75 & 12.79 & 22.35 \\\\ \\hline Taylor Exp. (2nd) & 16 (153) & ✓(64) & ✓ & ✓ & **8.65** & 2.07 & **9.64** & **11.71** & **25.07** \\\\ Taylor Exp. (2nd) & 16 (153) & ✓(64) & ✓ & � & **8.65** & **2.04** & 9.66 & 1.72 & 10.80 \\\\ Taylor Exp. (2nd) & 16 (153) & ✗ & ✓ & ✓ & 8.91 & 2.11 & 9.94 & 10.16 & 24.5 \\\\ Taylor Exp. (2nd) & 16 (153) & ✓(64) & ✗ & ✓ & 8.74 & 2.09 & 9.74 & 2.36 & 18.87 \\\\ Taylor Exp. (2nd) & 24 (325) & ✗ & ✓ & ✓ & 9.49 & 2.29 & 10.58 & 8.71 & 11.33 \\\\ \\hline Taylor Exp. (2nd) & 16 (153) & ✓(128) & ✓ & ✓ & **8.61** & **2.06** & **9.60** & **14.39** & **31.84** \\\\ Taylor Exp. (2nd) & 16 (153) & ✓(64) & ✓ & ✓ & 8.65 & 2.07 & 9.64 & 11.71 & 25.07 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: Based.**에서 설계 선택 및 하이퍼파라미터의 **Ablations.** 모든 모델은 섹션 4에 설명된 Based 아키텍처의 362M param 변형이며 파일에서 100억 토큰으로 훈련되었다. (1) 특징맵의 선택(4.1절 참조), (2) 특징 딤(d^{\\prime}\\)의 크기(4.1절 참조), (3) 국부 시퀀스 믹서의 사용(슬라이딩 윈도우 주의 및 짧은 컨볼루션) 및 (4) 섹션 4에 정의된 데이터 의존적 감쇠를 보여준다.\n' +
      '\n' +
      'Conclusion\n' +
      '\n' +
      '이 작업은 고품질 및 효율적인 서열 믹서의 특성을 연구한다. 우리는 이론과 실험을 통해 리콜, 상황 내 학습의 핵심, 처리량 사이의 근본적인 균형을 식별한다. 주의력은 리콜을 완벽하게 수행하지만 시퀀스 길이에 따라 증가하는 KV 캐시를 유지해야 합니다. 대안으로서, Based는 훈련 동안 하위 2차적이며 효율적인 반복 추론 뷰를 허용하는 소프트맥스 지수 함수의 테일러 근사치를 통해 국소 세립 주의와 장거리 선형 주의의 두 가지 간단한 기술을 결합한다. 본 논문에서는 벽시계 효율을 높이기 위해 1.3Bn 파라미터 스케일에서 FlashAttention-2보다 최대 24\\times\\의 빠른 생성을 유도하는 Taylor 선형 어텐션 계산의 IO 인식 알고리즘을 도입하여 1024개의 토큰을 생성한다. 전체적인 복잡성에서 경쟁하는 것을 넘어, Based는 언어 모델링 아키텍처로서의 Based의 약속을 지적하면서 최대 6.2개의 정확도 포인트만큼 리콜 품질에서 이전의 하위 2차 아키텍처를 능가한다.\n' +
      '\n' +
      '#### Acknowledgments\n' +
      '\n' +
      '트리 다오, 다니엘 푸, 송린 양, 제시카 그로건, 알버트 구, 에릭 응우옌, 마이클 워노우, 앨리사 우넬, 고탐 마치라주 등 이 작품에서 도움이 되는 피드백과 토론에 감사드린다. 이 작업을 지원해 주신 게으른 연구실과 함께 AI에 감사드립니다. 우리는 No. U54EB020405(모빌리즈)에 따른 NIH의 지원, Nos에 따른 NSF를 기쁘게 인정한다. CCF2247015(Hardware-Aware), CCF1763315(Beyond Sparsity), CCF1563078(Volume to Velocity), 1937301(RTML); US DEVCOM ARL under No. W911NF-23-2-0184 (Long-context), W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000142312633(Deep Signal Processing), N000141712266(Unifying Weak Supervision), N000142012480(Non-Euclidean Geometry), N000142012275(NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, HAI-GCP Cloud Credits for Research Program, The Stanford Data Science Initiative(SDSI), and members of Stanford DAWN Project: Facebook, Google, VMWare. 미국 정부는 그에 대한 저작권 표기에도 불구하고 정부 목적을 위해 재인쇄물을 복제하고 배포할 권한이 있다. 이 자료에 표현된 모든 의견, 결과 및 결론 또는 권장 사항은 저자의 의견이며 NIH, ONR 또는 미국 정부의 표현 또는 암시된 견해, 정책 또는 승인을 반드시 반영하는 것은 아니다. AR의 연구는 NSF 보조금 CCF#2247014에 의해 지원된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Re. Zoology: Measuring and improving recall in efficient language models. _International Conference on Learning Representations_, 2023.\n' +
      '* [2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. volume 30, 2017.\n' +
      '* [3] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_, 2022.\n' +
      '* [4] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. _arXiv preprint arXiv:2212.10544_, 2022.\n' +
      '* [5] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_, 2023.\n' +
      '* [6] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. _arXiv preprint arXiv:2312.06635_, 2023.\n' +
      '* [7] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena hierarchy: Towards larger convolutional language models. _arXiv preprint arXiv:2302.10866_, 2023.\n' +
      '\n' +
      '* [8] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, and Jiaming et al. Kong. Rwkv: Reinventing rnns for the transformer era. _arXiv:2305.13048_, 2023.\n' +
      '* [9] Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Re. Hungry Hungry Hippos: Towards language modeling with state space models. In _International Conference on Learning Representations_, 2023.\n' +
      '* [10] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* [11] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\n' +
      '* [12] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* [13] Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Re. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=4g0212N2Nx](https://openreview.net/forum?id=4g0212N2Nx).\n' +
      '* [14] Feyza Duman Keles, Pruthuvi Maheskaya Wijewardena, and Chinmay Hegde. On the computational complexity of self-attention. In _34th International Conference on Algorithmic Learning Theory_, volume 201, page 1-23, 2023.\n' +
      '* [15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, and Shruti Bhosale. Llama 2: Open foundation and fine-tuned chat models. _arXiv:2307.09288_, 2023.\n' +
      '* [16] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).\n' +
      '* [17] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In _International conference on machine learning_, pages 4055-4064. PMLR, 2018.\n' +
      '* [18] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_, 2019.\n' +
      '* [19] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_, 2020.\n' +
      '* [20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In _International conference on machine learning_, pages 5156-5165. PMLR, 2020.\n' +
      '* [21] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. _arXiv preprint arXiv:2009.14794_, 2020.\n' +
      '* [22] Krzysztof Choromanski, Haoxian Chen, Han Lin, Yuanzhe Ma, Arijit Sehanobish, Deepali Jain, Michael S Ryoo, Jake Varley, Andy Zeng, Valerii Likhosherstov, et al. Hybrid random features. _arXiv preprint arXiv:2110.04367_, 2021.\n' +
      '\n' +
      '* [23] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. _arXiv preprint arXiv:2202.08791_, 2022.\n' +
      '* [24] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2020. URL [https://arxiv.org/abs/2006.16236](https://arxiv.org/abs/2006.16236).\n' +
      '* [25] Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. Finetuning pretrained transformers into RNNs. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 10630-10643, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.830. URL [https://aclanthology.org/2021.emnlp-main.830](https://aclanthology.org/2021.emnlp-main.830).\n' +
      '* [26] Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. Linear transformers are secretly fast weight programmers. In _International Conference on Machine Learning_, pages 9355-9366. PMLR, 2021.\n' +
      '* [27] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. _arXiv preprint arXiv:2111.00396_, 2021.\n' +
      '* [28] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 2023.\n' +
      '* [29] Ekin Akyurek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning: Architectures and algorithms. 2024. URL [https://arxiv.org/abs/2401.12973](https://arxiv.org/abs/2401.12973).\n' +
      '* [30] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. _arXiv preprint arXiv:2101.00027_, 2020.\n' +
      '* [31] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher Re. Simple hardware-efficient long convolutions for sequence modeling. _arXiv preprint arXiv:2302.06646_, 2023.\n' +
      '* [32] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: a unified understanding of transformer\'s attention via the lens of kernel. _arXiv preprint arXiv:1908.11775_, 2019.\n' +
      '* [33] NVIDIA. Nvidia H100 tensor core GPU architecture, 2022.\n' +
      '* [34] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. 2020.\n' +
      '* [35] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023.\n' +
      '* [36] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n' +
      '* [37] Eric Wu, Kevin Wu, Roxana Daneshjou, David Ouyang, Daniel Ho, and James Zou. How medical ai devices are evaluated: limitations and recommendations from an analysis of fda approvals. _Nature Medicine_, 27:1-3, 04 2021.\n' +
      '* [38] Xiang Deng, Prashant Shirakkar, Colin Lockard, Binxuan Huang, and Huan Sun. Dom-lm: Learning generalizable representations for html documents. 2022.\n' +
      '* [39] Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, and Christopher Re. Language models enable simple systems for generating structured views of heterogeneous data lakes. _arXiv:2304.09433_, 2023.\n' +
      '* [40] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. _SuperGLUE: a stickier benchmark for general-purpose language understanding systems_. Curran Associates Inc., Red Hook, NY, USA, 2019.\n' +
      '\n' +
      '* Nguyen et al. [2023] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, and Chris Re. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution, 2023.\n' +
      '* NVIDIA[2019] NVIDIA. cuda 그래프로 시작하는 것은 2019. URL[https://developer.nvidia.com/blog/cuda-graphs/](https://developer.nvidia.com/blog/cuda-graphs/)이다.\n' +
      '* Kitaev et al. [2020] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020.\n' +
      '* Zaheer et al. [2020] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and et al. Big bird: Transformers for longer sequences. _Proceedings of NeurIPS_, 2020.\n' +
      '* Wang et al. [2020] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.\n' +
      '* Zhu et al. [2021] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. _Advances in neural information processing systems_, 34:17723-17736, 2021.\n' +
      '* Alberti et al. [2023] Silas Alberti, Niclas Dern, Laura Thesing, and Gitta Kutyniok. Summermer: Universal approximation for efficient transformers. _arXiv preprint arXiv:2307.02301_, 2023.\n' +
      '* Tay et al. [2022] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. _ACM Computing Surveys_, 55(6):1-28, 2022.\n' +
      '* Xiong et al. [2021] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystromformer: A nystrom-based algorithm for approximating self-attention. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 14138-14148, 2021.\n' +
      '* Chen et al. [2021] Yifan Chen, Qi Zeng, Heng Ji, and Yun Yang. Skyformer: Remodel self-attention with gaussian kernel and nystr\\(\\backslash\\)"om method. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL [https://openreview.net/forum?id=p2CYG7gjkKz](https://openreview.net/forum?id=p2CYG7gjkKz).\n' +
      '* De Brebisson and Vincent[2015] Alexandre De Brebisson and Pascal Vincent. 구면 손실 계열에 속하는 소프트맥스 대안의 탐색. _ arXiv preprint arXiv:1511.05042_, 2015.\n' +
      '* Chen et al. [2021] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Re. Scatterbrain: Unifying sparse and low-rank attention approximation. _arXiv preprint arXiv:2110.15343_, 2021.\n' +
      '* Qin et al. [2022] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 7025-7041, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.473. URL [https://aclanthology.org/2022.emnlp-main.473](https://aclanthology.org/2022.emnlp-main.473).\n' +
      '* Ba et al. [2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.\n' +
      '* 장과 센리치[2019] 비아오 장과 리코 센리치. Root mean square layer normalization. _ Neural Information Processing Systems_, 32, 2019에서의 발전\n' +
      '* 쿨리와 투키[1965] 제임스 W 쿨리와 존 W 투키. 복소 푸리에 시리즈의 기계 계산을 위한 알고리즘. _ Computics of computation_, 19(90):297-301, 1965.\n' +
      '* Romero et al. [2022] David W. Romero, Anna Kuzina, Erik J. Bekkers, Jakub M. Tomczak, and Mark Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data. 2022.\n' +
      '\n' +
      '* [58] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022.\n' +
      '* [59] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Re. On the parameterization and initialization of diagonal state space models, 2022.\n' +
      '* [60] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces, 2022.\n' +
      '* [61] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Zettlemoyer Luke. Mega: Moving average equipped gated attention. _arXiv preprint arXiv:2209.10655_, 2022.\n' +
      '* [62] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 1, 2021.\n' +
      '* [63] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In _International conference on machine learning_, pages 933-941. PMLR, 2017.\n' +
      '* [64] Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and ChengXiang Zhai. Sparse modular activation for efficient sequence modeling. _arXiv preprint arXiv:2306.11197_, 2023.\n' +
      '* [65] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. _arXiv preprint arXiv:2310.01889_, 2023.\n' +
      '* [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In _Proceedings of the 29th Symposium on Operating Systems Principles_, pages 611-626, 2023.\n' +
      '* [67] Daniel Y Fu, Hermann Kumbong, Eric Nguyen, and Christopher Re. Flashfftconv: Efficient convolutions for long sequences with tensor cores. _arXiv preprint arXiv:2311.05908_, 2023.\n' +
      '* [68] Markus N Rabe and Charles Staats. Self-attention does not need \\(o(n^{2})\\) memory. _arXiv preprint arXiv:2112.05682_, 2021.\n' +
      '* [69] Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. Mnnfast: A fast and scalable system architecture for memory-augmented neural networks. In _2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA)_, pages 250-263, 2019.\n' +
      '* [70] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. _arXiv preprint arXiv:2305.19370_, 2023.\n' +
      '* [71] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In _International Conference on Machine Learning_, pages 9099-9117. PMLR, 2022.\n' +
      '* [72] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 12 2023. doi: 10.57967/hf/1595. URL [https://github.com/togethercomputer/stripedhyena](https://github.com/togethercomputer/stripedhyena).\n' +
      '* [73] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023.\n' +
      '* [74] Katarina Gresova, Vlastimil Martinek, David Cechak, Petr Simecek, and Panagiotis Alexiou. Genomic benchmarks: A collection of datasets for genomic sequence classification. _bioRxiv_, 2022. doi: 10.1101/2022.06.08.495248. URL [https://www.biorxiv.org/content/early/2022/06/10/2022.06.08.495248](https://www.biorxiv.org/content/early/2022/06/10/2022.06.08.495248).\n' +
      '* [75] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambda dataset: Word prediction requiring a broad discourse context, 2016.\n' +
      '\n' +
      '* [76] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019.\n' +
      '* [77] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019.\n' +
      '* [78] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.\n' +
      '* [79] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.\n' +
      '* [80] Colin Lockard, Prashant Shirakkar, and Xin Luna Dong. OpenCeres: When open information extraction meets the semi-structured web. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 3047-3056, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1309. URL [https://aclanthology.org/N19-1309](https://aclanthology.org/N19-1309).\n' +
      '* [81] Peter Burgisser, Michael Clausen, and Mohammad A Shokrollahi. _Algebraic complexity theory_, volume 315. Springer Science & Business Media, 2013.\n' +
      '* [82] Thathachar S Jayram, Ravi Kumar, and Dandapani Sivakumar. The one-way communication complexity of hamming distance. _Theory of Computing_, 4(1):129-135, 2008.\n' +
      '* [83] Swastik Kopparty. Topics in algorithms and complexity theory: Spring 2020. 2020.\n' +
      '\n' +
      '부록은 다음과 같이 구성된다:\n' +
      '\n' +
      '1. 부록 A는 확장된 관련 작업 논의를 포함한다.\n' +
      '2. 부록 B는 IO-aware 구현 및 Based를 위한 벤치마킹에 대한 세부사항을 포함한다.\n' +
      '3. 부록 C는 Based architectural details에 대한 추가적인 논의를 포함한다.\n' +
      '4. 부록 D는 추가적인 실험 결과를 제공한다.\n' +
      '5. 부록 E는 실험 세부사항을 제공한다.\n' +
      '6. 부록 F는 이론적 결과와 증명을 포함한다.\n' +
      '\n' +
      '## 부록 확장 관련 업무\n' +
      '\n' +
      '우리의 작업은 효율적인 시퀀스 모델링의 다양한 개발과 광범위하게 관련이 있다. 이 절에서는 이러한 관련 작업을 (1) 모델 기반 또는 알고리즘 기여(부록 A.1) 및 (2) 구현 또는 시스템 기반 기여(부록 A.2)로 구성한다.\n' +
      '\n' +
      '### 효율적인 언어 모델링 아키텍처\n' +
      '\n' +
      '트랜스포머는 종종 최첨단 언어 모델링 품질을 달성하는 반면, 이들의 설계는 입력 시퀀스를 처리하고 출력을 생성할 때 다양한 효율성 향상에 동기를 부여한다. 특히, 다양한 작업들은 입력 처리 시 입력 시퀀스 길이\\(N\\)에서 2차 스케일링(\\(\\mathcal{O}(N^{2})\\)을 유지하면서 모델링 품질을 유지하기 위해 노력하며, 길이\\(M\\)의 출력에 대한 출력 디코딩 시 시간 및 공간(\\(\\mathcal{O}(NM)\\)을 개선한다(어텐션 메커니즘에서 이전 키와 값을 캐싱할 때).\n' +
      '\n' +
      '우리는 대부분의 관련 작업 라인이 두 가지 프리미티브 중 하나인 _attention approximations_(_e.g._, linear attentions, sparse attentions, sparse and low-rank attentions) 또는 _state-space models_(SSMs)("long" convolutional model 또는 recurrent neueral networks"로 대체 파라미터화를 갖는)에 기반한다는 점에 주목한다. 두 모델 클래스 모두 입력 처리 시 부차적인 시간 및 공간 복잡도를 달성하는 반면, 선형 주의 사항과 SSM은 순환 신경망(RNN)처럼 반복적으로 입력을 처리하는 능력을 통해 \\(\\mathcal{O}(NM)\\) 디코딩보다 더 나은 디코딩을 가능하게 한다.\n' +
      '\n' +
      '다음에는 이러한 각 모델 클래스에 대해 설명합니다.\n' +
      '\n' +
      '1.1 효율적인 주목\n' +
      '\n' +
      '본 논문에서는 이러한 문제를 효율적으로 해결하기 위한 두 가지 패러다임인 _structured sparse attentions_와 _linear attentions_에 주목한다. 우리는 지역성에 민감한 해싱[43], 무작위 희소 주의력[44], 시퀀스 압축[45, 46, 47]과 같이 주의를 더 효율적으로 계산하기 위한 많은 이전 작업을 인정한다. 종합적인 조사는 [48]을 참조하십시오.\n' +
      '\n' +
      'Structured sparse attentionsStructured parse attentions는 특정 스트라이드 패턴 또는 로컬_슬라이딩 윈도우_[17, 18, 19]에 대해서만 참석함으로써 주의의 시간 및 메모리 요구 사항을 감소시킨다. 예를 들어, [17]은 과거 \\(w\\) 토큰의 로컬 윈도우에서만 컴퓨팅 어텐션을 제안하여, 시퀀스 \\(N\\) 토큰의 처리 시간이 \\(\\mathcal{O}Nw\\) 시간과 공간만 소요되도록 한다. [18] 이 창만으로는 (장기적인 상호작용과 같은) 모든 원하는 종속성을 모두 포착할 수 없다는 점에 주목하고, 질의와 키 사이의 내적을 더 멀리 계산하기 위해 두 개의 스트라이드 패턴을 제안한다. [19] 특정 토큰이 조밀한 방식으로 다른 모든 토큰에 참석할 수 있도록 추가로 제안합니다.\n' +
      '\n' +
      '최근의 대형 언어 모델들(Mistral, Jiang et al. [10])에서 더욱 대중화되었지만, 우리는 이러한 구현들이 여전히 효율성을 향상시킬 여지를 남겨두는 대형 윈도우 크기들을 사용한다는 점에 주목한다. 본 논문에서는 하드웨어 유도형 설계(소형 윈도우 사용)와 슬라이딩 윈도우 구현(슬라이딩 윈도우 구현)을 소개한다.\n' +
      '\n' +
      '선형 주의 집중 선형 주의 집중은 표준 주의 집중과 동일한 "시퀀스 혼합" 작업을 보존하고 쿼리 및 키 간의 내적을 계산하여 해당 값에 가중치를 부여합니다. 그러나, 그들의 핵심 통찰력은 표준 주의에서 소프트맥스를 대체 커널 함수로 대체하는 것이다[20]. 기계적으로, \\(\\exp(\\mathbf{q}^{\\top}\\mathbf{k})\\)를 특징맵 닷-곱 \\(\\phi(\\mathbf{q})^{\\top}\\phi(\\mathbf{k})\\)에 유리하게 제거함으로써, 이러한 방법들은 \\(\\mathcal{O}(Nd^{2})\\) 시간 및 공간 [24] (식 (2)]에서 주의력을 계산하기 위해 행렬 곱 연관성을 사용한다. 또한, 상수 메모리에 대한 _recurrent view_와 토큰당 \\(\\mathcal{O}(1)\\) 시간 [25, 26] (식 (3))을 허용한다.\n' +
      '\n' +
      '선행 연구들은 선형 주의 모델링 품질을 향상시키기 위해 서로 다른 특징 맵 \\(\\phi\\)을 제안한다. [20] 원래 _positive elu_ function \\(1+\\text{elu}\\)을 사용하여 \\(\\phi(\\mathbf{q})^{\\top}\\phi(\\mathbf{k})\\)가 양으로 유지되고 주의력 가중치가 아핀으로 유지된다. [23] 대신 코사인 기반 재가중 함수와 결합된 ReLU 함수를 사용하여 지역성 편향을 추가합니다. 다른 접근법들은 랜덤 푸리에 피처들[21, 22] Nystrom 방법[49, 50] 또는 결정론적 저차 다항식 근사들[13, 14, 51]과 같은 소프트맥스를 근사하는 것을 목표로 하는 피처 맵들을 제안한다. 마지막으로, 최근의 작업들은 특징 맵을 학습 가능한 함수로 취급하고[25], 선택적으로 소프트맥스 커널을 복구하도록 특징 맵을 명시적으로 트레이닝한다[13].\n' +
      '\n' +
      '마지막으로, 기반과 유사한 또 다른 작업 라인은 희소 및 선형 주의를 결합한다. 산란뇌[52]는 과거 희소 및 저순위 근사치를 통일하기 위해 희소 및 저순위 분해를 사용한다. 그들은 이론적으로 그리고 경험적으로 주의의 \\(\\exp(QK^{T})\\)의 낮은 순위 근사가 우리의 관찰에서 반향된 희소 + 낮은 순위 근사보다 훨씬 더 큰 근사 오차를 가질 것이라는 것을 보여준다. 트랜스노머[53]은 레이어노름[54] 또는 RMS노름[55]과 같은 정규화를 특정 계층에서의 선형 어텐션 출력에 적용하고, 다른 계층에서의 로컬 청크에서 소프트맥스 어텐션을 적용한다.\n' +
      '\n' +
      '####a.1.2 주의 대안\n' +
      '\n' +
      '우리는 이제 주의의 2차 스케일링을 개선하는 데 중점을 둔 다른 주의 대안을 검토한다. 이 정맥의 초기 작업은 선형 시간 불변 상태 공간 모델(SSM) 또는 긴 컨볼루션을 사용하며, 이는 FFT-컨볼루션 정리를 호출하는 \\(O(N\\log N)\\) 시간에서 길이 \\(N\\)의 시퀀스를 시퀀스 믹서[4, 27, 31, 57, 58, 59, 60, 61]로 효율적으로 처리할 수 있다. SSM은 또한 빠른 \\(O(1)\\) 추론을 허용하기 위해 재발로 재기입될 수 있다.\n' +
      '\n' +
      '후속 작업은 긴 컨볼루션만으로는 언어 모델링에서 특정 하위 작업을 수행할 수 있을 만큼 충분히 표현적이지 않다는 것을 식별했다. 이전 작업은 순수한 선형 SSM이 모델의 맥락 내 학습 능력[3, 62]과 상관되는 기술인 연관 회상을 수행할 수 없음을 보여주고, 모델이 시퀀스 [7, 8, 9]에서 토큰을 비교할 수 있도록 토큰 사이에 곱셈 상호 작용(게이팅 또는 하다마드 곱[63])을 도입한다. 그러나, Arora et al. [1]은 경험적이고 이론적으로 게이팅된 컨볼루션 아키텍처의 클래스, 두 게이팅 및 컨볼루션 프리미티브로부터 구축된 임의의 아키텍처는 관심만큼 효율적으로 연관 리콜(합성 및 실제 언어 데이터 상에서)을 학습하기 위해 고군분투한다. 그들은 주의가 AR을 일정한 많은 계층에서/시퀀스 길이와 무관한 모델 차원으로 해결하지만, 임의의 게이티드 컨볼루션 아키텍처는 시퀀스 길이에 따라 스케일하는 차원을 사용한다 -- 섹션 3.2의 하한 논증을 사용하여 그들의 상한 이론 결과를 구축한다. 또한 이 작업에서 게이티드 컨볼루션을 넘어 더 광범위한 아키텍처 세트를 연구한다.\n' +
      '\n' +
      'Arora et al. [1], Gu and Dao[5], Yang et al. [6]은, _input-dependent_ sequence mixers의 사용이 AR을 주의만큼 효율적으로 수행하기 위한 아키텍처에 중요하다는 것을 식별한다. AR은 다음 토큰[9]을 예측하기 위해, 시퀀스에서 현재(마지막) 토큰들과 상호작용하기 위해 시퀀스에서 이전에 나타나는 쉬프팅 정보를 필요로 한다. 게이팅이 데이터-의존성[7]을 도입하는 하나의 방식인 반면, 두 개의 시퀀스(예를 들어, 시프트된 시퀀스 및 시프트되지 않은 시퀀스)에서 토큰을 비교할 수 있지만, 게이팅만을 사용하여, 시퀀스의 프리픽스로부터 어떤 정보_가 처음에 포워드로 시프트될지를 _선택하는 것은 어렵다. 직관적으로, 시프트할 정보는 입력의 속성_에 의존한다. 따라서 여러 하위 2차 아키텍처는 입력 의존성을 도입하기 위한 대체 전략을 고려한다[5, 61, 64, 24, 6]. 우리는 작업에서 효율적인 입력 의존적 서열 혼합을 위한 또 다른 전략을 제시한다.\n' +
      '\n' +
      '### Efficient Implementations\n' +
      '\n' +
      '다양한 작업은 새로운 모델 아키텍처를 설계하는 것을 넘어 훈련 및 추론 효율성을 향상시키기 위해 시스템 수준의 혁신을 도입한다. 여기에는 주의[11, 65, 66], 긴 컨볼루션[31, 67], 선형 주의[6, 20]과 같은 아키텍처 프리미티브의 대체 구현이 포함된다. 그들은 종종 매트릭스 곱셈과 같은 연산을 단일 CUDA 커널에 "융합"하고, 다양한 읽기 및 쓰기 연산의 결과를 서로 다른 수준의 GPU 메모리 간에 분배하고 계산하는 "IO 인식" 방법을 설계함으로써 현대 GPU에서 감소된 메모리와 증가된 계산 속도를 모두 달성한다.\n' +
      '\n' +
      '###### a.2.1 효율적인 주의사항 구현\n' +
      '\n' +
      '[12] 플래시 어텐션을 소개한다. 플래시 어텐션은 어텐션 연산을 단일 CUDA 커널에 융합하고 어텐션 연산을 분산하여 높은 대역폭 메모리(HBM)와 정적 랜덤 액세스 메모리(SRAM)를 더 잘 활용함으로써 메모리와 속도를 향상시키는 소프트맥스 어텐션의 대안적이지만 정확한 구현이다. 그들은 먼저 관심의 쿼리-키-값 도트-생성물, 마스킹 및 소프트맥스를 단일 커널로서 함께 계산한다. SRAM에 한 번 로드한 후 출력을 HRAM으로 다시 이동시킴으로써 SRAM의 빠른 계산량을 활용하고 전체 읽기-쓰기 연산 횟수를 줄인다. SRAM의 작은 메모리 크기를 피하고 입력 시퀀스 길이에 대한 주의의 2차 메모리 크기를 피하기 위해, 그들은 질의, 키 및 값 입력을 더 작은 "블록"으로 분할하기 위해 _tiling_을 사용하고, 각 블록에 대한 주의 동작을 계산하고, 소프트맥스를 적절하게 정규화하기 위해 모든 블록을 컴퓨팅한 후 출력을 조정한다[68, 69]. SRAM에서 빠르게 역전파를 수행하기 위해, SRAM의 제한된 저장소를 저장하기 보다는 구배를 재계산함으로써 SRAM의 제한된 저장소를 우회한다. 추가 작업에도 불구하고, 이러한 IO 인식 구현은 훈련 중 벽-시계 시간을 여전히 상당히 향상시킨다.\n' +
      '\n' +
      '유사하게 블록-와이즈 계산을 사용하여, [65]는 대신에 링어텐션에서 상이한 _devices_에 걸쳐 어텐션 블록을 계산함으로써, 디바이스 카운트로 스케일링되는 훨씬 더 큰 컨텍스트 길이에 걸쳐 트레이닝 및 추론을 가능하게 한다. 그들은 각 블록의 주의 동작을 여러 호스트에 병렬로 분배하고 계산하며, 마찬가지로 요약 통계를 추적하여 결과를 정확한 주의에 정확하게 수집한다. 그러나, 그들은 오버헤드를 줄이기 위해 블록들의 통신을 조정하기 위해 "중첩" 메커니즘을 도입한다. 그들은 또한 메모리를 줄이기 위해 Blockwise Parallel Transformers[70]를 사용하는데, FlashAttention과 유사하게 Adention 연산을 블록별 정규화 통계와 함께 조정된 소프트맥스 출력을 다시 모으기 전에 별도의 블록으로 나누어 Adention의 2차 메모리 스케일링을 제거한다.\n' +
      '\n' +
      '주의력 훈련 및 추론에 대한 보완으로서 [66]은 PagedAttention으로 주의력 생성을 개선한다. PagedAttention은 생성 중 메모리 활용 문제를 해결하기 위해 블록별 계산을 유사하게 사용하며, 여기서 KV 캐시는 결정되지 않은 양을 증가시킬 수 있다. 기존의 시스템은 대량의 연속 메모리를 사전 할당함으로써 순진하게 이를 처리할 수 있다. 그러나 이로 인해 낮은 활용률과 계산상의 병목 현상이 발생할 수 있다. 이에 따라 PagedAttention은 관심의 성장하는 KV 캐시를 물리적 메모리에 별도로 저장할 수 있는 _KV block_로 나눈다. 이것은 메모리 기반 병목 현상을 줄이기 위해 필요할 때 더 작은 청크들이 상이한 위치들에서 할당될 수 있는, 보다 유연한 메모리 관리를 가능하게 한다.\n' +
      '\n' +
      '기반에서 우리는 2차 테일러 시리즈 선형 주의와 슬라이딩 윈도우 소프트맥스 주의 모두를 더 효율적으로 계산하고 훈련과 추론 모두를 위해 유사한 차단 전략을 사용한다.\n' +
      '\n' +
      '####a.2.2 효율적인 주의-대체 구현\n' +
      '\n' +
      '주목을 위한 최적화를 넘어 다양한 작업에서도 유사한 "IO 인식" 구현을 도입하여 컨볼루션 및 반복 작업에 대한 메모리 사용 및 속도를 개선한다. 본 논문에서는 SRAM의 결과를 계산하기 위해 퓨징 연산과 블록킹(tiling)과 같은 유사한 기법을 사용하는 Based와 가장 관련성이 높은 연구들을 소개한다.\n' +
      '\n' +
      '긴 컨볼루션[67]은 현대 GPU에서 긴 컨볼루션의 효율성을 향상시킵니다. 그들은 고속 푸리에 변환(FFT)을 사용하여 구축하는데, 이 변환은 입력 시퀀스 길이와 동일한 필터 크기를 갖는 컨볼루션들을 \\(\\mathcal{O}(N^{2})\\)에서 \\(\\mathcal{O}(N\\log N)\\)으로 계산할 수 있다. 그러나, GPU에서 이 알고리즘을 효율적으로 계산하기 위해, 그들은 FFT의 _Monarch_ 분해를 통해 콘볼루션을 별도의 행렬 다중 연산으로 분해하는데, 이는 (1) (감소된 읽기-쓰기 연산을 위해) FFT의 여러 단계를 함께 융합하고 (2) SRAM에서 더 작은 SRAM 메모리 제약 하에 남아 있는 동안 빠른 계산을 위해 이러한 연산을 스케줄링하는 것을 허용한다.\n' +
      '\n' +
      'Recurrence[5]는 플래시 어텐션과 유사한 몇 가지 기술을 사용하여 최근 신경 상태 공간 모델(SSMs) [27]의 효율성을 개선한다. 그들은 결과를 HBM에 다시 저장하기 전에 계산을 위해 SSM 파라미터를 SRAM에 로드하고, 또한 HBM에서 SRAM으로 입력이 로드될 때 중간 상태가 저장되지 않고 오히려 재계산되는 _recomputation_를 사용한다. 그들은 마침내 SSM의 반복 뷰를 병렬 스캔으로 병렬화함으로써 벽-시계 시간을 개선한다.\n' +
      '\n' +
      '마지막으로, 실제 벽-시계 시간과 선형 주의의 기억-사용을 개선하기 위한 기법들을 제안한다. [20] [6] 선형 주의의 인과곱에 여러 연산을 혼합하다. 블록킹을 사용하여 플래시리니어 어텐션에서 선형 어텐션 행렬을 SRAM-컴퓨팅 청크로 나눈다. 선형 주의의 느리지만 기억 효율적인 RNN 뷰와 더 빠르지만 기억 집약적인 병렬 "표준 주의" 뷰 사이의 트레이드오프로서, 그들은 선형 주의의 "청크-와이즈" 구현을 더욱 최적화한다[71]. 입력 시퀀스들을 처리할 때, 입력은 먼저 여러 개의 비중첩 청크들로 분할되는데, 여기서 우리는 각 청크의 끝에서 "kv 상태들"을 계산함으로써 메모리를 절약하고, 주어진 청크에서 토큰들을 병렬로 계산함으로써 시간을 절약한다.\n' +
      '\n' +
      'IO 인식 구현\n' +
      '\n' +
      '이 섹션에서는 벤치마킹 실험과 관련된 추가 세부 정보를 제공하고 섹션 6의 종단간 벤치마킹 결과를 보완하기 위해 우리가 기여하는 개별 커널에 대한 마이크로 벤치마킹 결과를 제공한다.\n' +
      '\n' +
      '### Forward/ Generation Prefill\n' +
      '\n' +
      '도 4에서, 우리는 IO-aware Taylor 선형 주의 알고리즘 1을 사용하여 Based를 구현한다. 우리의 커널 이전에 [13]에서 제시된 베이스라인 접근법은 인과 내적을 계산하기 위해 Fast Transformers로부터 인기 있는 선형 주의 CUDA 커널을 사용한다[20, 34]. 5. 아래 목록은 참조를 위한 베이스라인 구현을 보여준다(여기서 라인 76-77은 순수 PyTorch 또는 Fast Transformers 커널을 사용하여 계산될 수 있다).\n' +
      '\n' +
      '각주 5: [https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/causal_linear_attention.py](https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/causal_linear_attention.py)\n' +
      '\n' +
      '마이크로 벤치마크 섹션 6의 종단 간 아키텍처 벤치마크를 보완하기 위해 그림 5의 선형 주의 전진 통과에 대해서만 마이크로 벤치마크 결과를 제공한다.\n' +
      '\n' +
      '```\n' +
      '1fromeinopsimportrearrange\n' +
      '2importtorch\n' +
      '3fromtorchimportnn\n' +
      '4\n' +
      '5classTaylorExp(nn.Module):\n' +
      '6"""\n' +
      '7Featuremaptocompute2nd-orderTaylorapprox.ofexp(q^Tk/sqrt(d))\n' +
      '8"""\n' +
      '9\n' +
      '10def__init__(self,input_dim,head_dim_idx,temp=None,eps=1e-12):\n' +
      '11super()__init__()\n' +
      '12\n' +
      '13self.input_dim=input_dim\n' +
      '14self.head_dim_idx=head_dim_idx\n' +
      '15self.temp=1.0iftempisNoneelsetemp\n' +
      '```\n' +
      '\n' +
      '도 5: 테일러 선형 주의 전진 통과를 컴퓨팅하는 상이한 방식들에 대한 시간(ms) - Pure PyTorch(목록에 도시되고 [13]에 도입됨), Fast Transformers 커널(목록에 표시된 바와 같이) [24, 34] 또는 우리의 기반 커널(알고리즘 1)을 사용한다. **(왼쪽)** 고정된 시퀀스 길이 1024에서 배치 크기를 가변한다. **(오른쪽)** 고정된 배치 크기 4에서 시퀀스 길이를 가변한다. **(모두)** 벤치마킹은 16개의 특징 차원, 16개의 헤드, 64개의 헤드 차원을 사용하며 선형 주의의 _numerator_에 초점을 맞춘다. 각 점은 단일 NVIDIA H100 GPU에서 측정되는 10번의 반복에 걸친 중앙값을 나타낸다. 라인은 메모리 외 오류로 종료됩니다.\n' +
      '\n' +
      'self.eps=epsself.r2=math.sqrt(2)self.rd=math.sqrt(self.input_dim)self.rrd=math.sqrt(self.rd)\n' +
      '2\n' +
      '3defforward(self,x:torch.Tensor):\n' +
      '#Get2nd-orderterms(rearrange(x*x),\'....m->...(mn)\') x2=(x.unsqueezeeze(-1)*x.unsqueezeeze(-2))flatten(start_dim=-2)/self.r2 term1=torch.ones(x[...,:1].shape.to(x.device) term2=x/self.rrd term3=x2/self.rd\n' +
      '4\n' +
      '5terms=[term1,term2,term3] returntorch.cat(tfortinerms),dim=self.head_dim_idx)\n' +
      '6\n' +
      '7classTaylorLinAttn(nn.Module): def__init__(self): super()__init__() self.d_model=d_model self.feature_dim=16 self.num_heads=16 self.num_key_value_heads=16 self.head_dim=self.d_model//self.num_key_value_heads self.eps=1e-12\n' +
      '8feature_map_kwargs={\n' +
      '9"input_dim":self.feature_dim",head_dim_idx":-1,"eps":1e-12,"self.feature_map=TaylorExp(**feature_map_kwargs)self.proj_q=nn.Linear(self.d_model,self.feature_dim*self.num_heads,bias=False\n' +
      '10 self.proj_k=nn.Linear(self.d_model,self.feature_dim*self.num_heads,bias=False\n' +
      '11) self.proj_v=nn.Linear(self.d_model,self.num_key_value_heads*self.head_dim,bias=False\n' +
      '12) self.proj_o=nn.Linear(self.num_heads*self.head_dim,self.d_model,bias=False\n' +
      '13)\n' +
      '14\n' +
      '15defforward(self,hidden_states:torch.Tensor,*args,**kwargs):\n' +
      '16b,l,_=hidden_states.size() q=self.proj_q(hidden_states)\n' +
      '17x=self.proj_k(hidden_states) v=self.proj_y(hidden_states)\n' +
      '18q=q.view(b,l,self.num_heads,self.feature_dim).transpose(1,2)\n' +
      '19k=k.view(b,l,self.num_key_value_heads,self.feature_dim).transpose(1,2)\n' +
      '20v=v.view(b,l,self.num_key_value_heads,self.head_dim).transpose(1,2)\n' +
      '21\n' +
      '22\n' +
      '23#Linearattention q,k=self.feature_map(q),self.feature_map(k) q,k,v=q.unsqueezeeze(-2),k.unsqueezeeze(-2),v.unsqueezeeze(-1)\n' +
      '23\n' +
      '24#Computeattentioncausal(alAlternativelyusetheFastTransformerskernel) num=(q*(k*v.cumsum(dim=2)).sum(dim=-1) denom=(q*k.cumsum(dim=2)).sum(dim=-1)+self.eps y=(num/denom) y=(num/denom) y=rearrange(y,"bhld->b1(hd)) y=self.proj_o(y) returny``\n' +
      '\n' +
      '목록 1: 테일러 선형 주의의 PyTorch 구현.\n' +
      '\n' +
      '"Input:Input projected hidden state \\(q,k,v\\in\\mathbb{R}^{N\\times d}\\). 출력: 출력\\(y=T0+T1+T2\\in\\mathbb{R}^{N\\times d}\\) 블록당 n\\({}_{text{warps}=8\\) 워프를 갖는 배치\\(\\times\\) 헤드의 병렬 연산으로 병렬화한다. 한 블록 내에서: Based Define n\\({}_{\\text{warps}}=\\frac{N}{T}\\(\\triangleright\\)\\(T=16\\)의 임시 저장을 위한 Define n\\({}_{\\text{blocks}=\\text{n}_{\\text{tiles}}/\\text{n}_{\\text{warps}\\times T\\times 4T\\)의 SRAM 버퍼 A0, A1, A2(Size \\(2\\times\\text{n}_{\\text{warps}}\\times T\\times 4T\\))의 SRAM 버퍼들을 생성한다. 결과들은 warps로서 시퀀스 Create SRAM buffer \\(total_{A0}\\)와 \\(total_{A1}\\)을 처리하여 누적("KV") 상태를 유지한다. 최종 출력 Create 레지스터 fragment \\(\\text{q}_{\\text{a},\\text{q}_{\\text{b},\\text{k}_{\\text{a},\\text{q}_{\\text{frag},\\text{k}_{\\text{facum}\\(16\\times 16\\)을 저장하기 위한 (Size n\\({}_{text{warps}}\\times T\\times 4T\\))의 SRAM buffer \\(y\\)을 생성한다. 레지스터 조각은 \\(\\text{v}_{\\text{frag}},\\text{a}0_{\\text{frag}}\\), \\(\\text{a}1_{\\text{accum}\\), \\(A2_{0}\\), \\(A2_{1}\\), \\(\\text{q}2A_{\\text{accum}\\), \\(\\text{o}_{\\text{accum}\\), \\(16\\times 64\\). 이 조각은 등록 내 계산 중에 데이터를 유지하기 위한 것입니다. 단편을 \\(0\\)으로 초기화합니다. 각 워프는 초기 타일 \\(B_{q}[\\text{tic}][\\text{warpid}]\\gets Q_{t},B_{k}[\\text{warpid}]][\\text{warpid}]\\gets K_{t}\\과 \\(B_{v}[\\text{tic}][\\text{warpid}]\\gets V_{t}\\triangleright\\gets V_{t}\\triangleright\\gets HBM을 SRAM에 로딩하여 \\(\\text{cur}\\text{block}\\in[0.n_{\\text{block}-1]\\text{toc}\\text{block}\\in[0.n_{\\text{block}-1]\\text{tic}=0\\oplus=1\\), \\(\\text{toc}\\oplus 1\\)do\\(\\triangleright\\) XORs tic andtoc to 토글링한다. \\(\\text{frag}}\\to\\text{a}0_{\\text{frag}}\\(\\text{warpid}]]\\(\\triangleright\\)\\(\\text{frag}}\\to\\text{a}0_{\\text{frag}}\\(\\text{warpid}]]\\(\\triangleright\\)\\(\\text{frag}}\\(\\text{frag}}]\\(\\text{warpid}]]\\(\\triangleright\\)\\(\\text{frag}}\\(\\text{frag}}]\\(\\text{frag}}]\\(\\text{warpid}]]\\)\\(\\triangleright\\)\\(\\text{frag}}\\(\\text{frag}}\\text{frag}}\\(\\text{frag}}]\\(\\text{warpid}]]\\)\\(\\triangleright\\)\\(\\text{frag}}\\(\\text{frag}}\\text{frag} (\\\\ttriangleright\\) T0 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 연산 스레드는 함께 특징 맵의 2차 외부 제품에서 발생하는 256개의 요소를 계산합니다. 각 warp는 warp 레지스터에 걸쳐 \\(A2)\\(A2)\\과 \\(A2)\\(\\triangleright\\)을 분할하여 \\(k_{a}\\)와 \\(q_{b}A2_{1}to\\text{frag}\\)의 두 개의 warp 부하 \\(k_{a}\\), \\(k_{b}\\)와 \\(k_{b}\\)의 KV 상태 갱신 정사각형 \\(k_{a}\\)과 \\(k_{b}\\)의 kV 상태 갱신 정사각형 \\(k_{b}\\)과 \\(k_{b}\\)의 kV 상태 갱신 정사각형 \\(k_{a}\\)과 \\(k_{b}\\)의 kV 상태 갱신 정사각형 \\(k_{b}\\)과 \\(k_{b}\\)의 kV 상태 갱신 정사각형 \\(k_{b}\\)과 \\(k_{b}\\)의 kV 상태 갱신 정 (y)를 저장한다. 선택적으로 생성을 위해 \\(A0\\), \\(A1\\), \\(A2\\)("KV 상태를 포함함)을 저장한다. \\(A0\\), \\(A1\\), \\(A2\\) (\\triangleright\\) SRAM을 HBM 알고리즘으로 다시 살펴보고 알고리즘 1을 상세히 설명한다.\n' +
      '\n' +
      '_Objective_ Section 4로부터의 첫 번째 리콜:\n' +
      '\n' +
      '\\frac{\\phi(\\mathbf{o}_{i}=\\sum_{j=1}^{i}\\frac{\\phi(\\mathbf{q}_{i})\\mathbf{v}_{j}}{ \\phi(\\mathbf{q}_{i})\\sum_{j=1}^{i}\\phi(\\mathbf{k}_{j}}=\\frac{\\phi(\\mathbf{q}_{i})\\sum_{j=1}^{i}\\left(\\phi(\\mathbf{k}_{i})\\sum_{j=1}^{i}\\phi(\\mathbf{q}_{j}}}}\\tag{5}\\fi(\\mathbf{q}_{i})\\sum_{j=1}^{i}\\frac{\\phi(\\mathbf{q}_{i})}\\fff{j=1}^{i}\\\n' +
      '\n' +
      '여기서 \\(q_{i}\\)는 수열에서 \\(n\\) 총 토큰의 \\(i^{th}\\)을 반영하고 모든 쿼리는 \\(\\mathcal{O}(Nd^{2})\\)의 시간 및 공간 복잡도에서 모든 과거 키에 적용된다.\n' +
      '\n' +
      '\\(\\exp(\\mathbf{q}_{i}^{\\top}\\mathbff{k}_{j}/\\sqrt{d})\\)를 근사하기 위해 \\(2^{\\text{nd}\\)-order Taylor series feature map을 이용하여 \\(\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d^{2}}\\)을 선택함으로써\n' +
      '\n' +
      '\\phi(\\mathbf{q}_{i})^{\\top}\\phi(\\mathbf{k}_{j})=1+\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}+\\frac{(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}}^{2}}{6}\\tag{6}\\frac{(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}}^{2}}\\tag{6}\\frac{(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}}^{2}}\\tag{6}\\frac{(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}}}1+\\mathbf{q}_{i}+\\frac{(\\mathbf{q}_{i\n' +
      '\n' +
      '본 절에서는 하드웨어가 병렬적으로 데이터의 청크들에 대해 동작하기 때문에 단일 토큰 대신에 데이터의 **tile**(예: 16개의 토큰들)로서 \\(q_{i}\\)을 언급할 것이다.\n' +
      '\n' +
      '알고리즘 1에서 각 스레드 블록은 특정 \\((\\text{batch},\\text{head})\\) 입력에 대한 결과를 계산할 수 있도록 한다. 스레드 블록 내에서, 우리는 8개의 워프/작업자를 사용하여 결과를 생성한다. SRAM에서 데이터 구조(\\(B_{q},B_{k},B_{v})와 \\(\\text{q}_{\\text{a},\\text{q}_{\\text{b},\\text{k}_{\\text{a},\\text{k}_{\\text{b},\\text{q}_{\\text{frag},\\text{k}_{\\text{frag},\\text{v}_{\\text{frag}})를 레지스터에서 초기화하여 \\(q,k,v\\) 입력의 청크 또는 _tiles_를 유지한다. SRAM의 데이터 구조 \\(A0,A1,A2\\)와 \\(\\text{a}0_{\\text{frag}}\\), \\(\\text{a}1_{\\text{accum}}\\), \\(\\text{q}\\text{A}2_{\\text{accum}\\)을 레지스터에 초기화하여 \\(0^{th},1^{st},2^{nd}\\)차 테일러 다항식 항에 대한 실행 \\(KV\\) 상태에 대한 계산을 유지한다.\n' +
      '\n' +
      '시퀀스 차원에 따른 연산을 \\(\\text{n}_{\\text{block}\\)으로 분할하고, 각 루프에서 1부터 \\(\\text{n}_{\\text{block}\\)까지 다음 8개의 청크를 고속 메모리에 로딩한다. 2048개의 시퀀스 길이와 8개의 워프, 16개의 타일 크기에 대해, 우리는 \\(\\text{n}_{\\text{tiles}}=128\\) 및 \\(\\text{n}_{\\text{block}}=16\\)으로 끝난다. 각 반복에서 \\(q,k\\)와 \\(v\\)의 \\(16\\times 64\\) 타일의 각 휨 하중은 \\(16\\times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\Times 16\\ 일단 타일이 스트리밍되면, 우리는 타일을 재사용할 필요가 없으며, 이는 선형 주의의 효율성의 핵심이다.\n' +
      '\n' +
      '**Zeroeth order Taylor terms:** 계산 중, Taylor 다항식의 \\(0^{th}\\) 항에 대해, \\(q,k\\)은 특징 맵(식 (6))을 적용한 후 1이다. 따라서, \\(q(k^{T}v)\\)에 대한 누적 합을 계산하는 것은 우리가 수열에 걸쳐 반복할 때 \\(v\\)의 누적 합을 유지하는 것으로 감소한다.\n' +
      '\n' +
      '**첫 번째 순서 테일러 항:** 다음으로 \\(1^{st}\\) 순서 항을 고려합니다. **On-diagonal:** 먼저 타일 \\(q_{i},k_{i},v_{i}\\)과 관련하여 온-diagonal 블록을 고려한다. 이를 위해 표준 주의의 연산 순서에 따라 \\(q^{T}k\\), 마스킹(원인화) 및 \\(v\\)을 곱한다. 이를 통해 마스킹(0 out non-causal element)을 쉽게 적용할 수 있다. 이제 각 날개는 \\(q_{i},k_{i},v_{i}\\)의 대각선상의 타일 집합에 대한 국부적인 결과를 포함한다. **off-diagonal:** 그러나 \\((q_{i}^{T}k_{j})v_{j}\\)이 모든 \\(j\\in[1..i]\\)(식 (5))에 의존하는 _global_누적합을 구해야 한다. 따라서 각 휨은 타일 \\(j\\in[1..i-1]\\)에 대한 결측값이다. 이 계산을 통합하기 위해, 우리는 \\(i-1\\)까지의 워프에 대한 누적 \\(KV\\) 은닉 상태를 계산하고 이것을 \\(q\\)의 로컬 타일(즉, \\(\\text{q}_{\\text{frag}}\\)과 곱할 것이다. 이를 위해 알고리즘 1에서는 스레드 레지스터에서 은닉 상태의 국부 타일들을 계산하기 위해 \\(\\text{k}_{\\text{frag}}}^{T}\\)과 \\(\\text{v}_{\\text{frag}}\\)을 곱한다. 8 워프의 로컬 결과에 대한 전역 누적 합을 수행하기 위해 SRAM의 레지스터(스레드 특정)에서 \\(A1\\)까지 기록한다. 공유 메모리에서 전역 누적 합을 계산한 후, 각 워프는 타일([1..i-1]\\)에 대한 모든 선행(KV\\)(history)을 포함하도록 \\(KV\\) 상태(A1\\)를 레지스터에 다시 로드한다. 그리고 레지스터의 국부적인 \\(\\text{q}_{\\text{frag}\\)와 이 \\(KV\\) 상태를 곱하여 현재의 \\(\\text{n}_{\\text{block}\\)까지 \\(1^{st}\\)의 최종 출력을 갱신한다. \\(\\text{n}_{\\text{block}\\)을 따라 다음 반복을 위해 \\(A1\\) 공유 메모리에서 \\(1^{st}\\) 차수항에 해당하는 실행 \\(KV\\) 상태를 유지한다는 점에 유의한다.\n' +
      '\n' +
      '**2차 테일러 항:** 우리는 마침내 \\(2^{nd}\\) 차수 항을 계산해야 한다. 우리는 \\(1^{st}\\) 차수 항과 유사하게, **On-diagonal:** 위에서 계산을 활용할 수 있다. 우리는 위로부터 인과적 \\((qk^{T})^{2}\\)을 제곱하고 \\(\\text{v}_{\\text{frag}\\)과 곱하여 대각선상의 타일 \\(q_{i},k_{i},v_{i}\\)에 해당하는 \\(2^{nd}\\) 차수항의 _portion_를 구할 것이다. **off-diagonal:** 다시, 우리는 또한 타일 \\([1..i-1]\\)에 대한 결과를 계산해야 한다.\n' +
      '\n' +
      '2차항에 대한 KV 은닉상태(\\(\\mathcal{O}(d^{2}D)\\)는 특징치수\\(d\\)와 머리치수\\(D\\))가 크고, 워프는 제한된 수의 레지스터를 가지므로, 8 워프의 레지스터를 가로질러 저장공간을 슬라이스한다. 알고리즘 1에서 (16^{2}\\times 64\\)((d^{2}\\times D\\)hidden 상태)를 고려하여 시퀀스 차원을 따라 16개의 슬라이스로 나누고, 알고리즘 1의 스레드 레지스터에서 (16\\times 64\\) 슬라이스((A2_{0},A2_{1}\\) 조각에 저장) 8개의 워프들이 각각 2개씩 처리하도록 한다. Wwarp \\(i\\)는 스레드당 두 개의 레지스터에서 슬라이스 \\(2i\\)과 \\(2i+1\\)을 유지할 것이다.\n' +
      '\\(2^{nd}\\) 순서에 대한 ** 계산 출력** \\(q_{i}\\)의 하나의 타일에서 각각 2개의 레지스터로 휨 \\(i\\) 부하를 계산한다. Taylor \\(2^{nd}\\) 차수 항(특징 치수 16)에 의해 계산된 각 토큰에 대해 256개의 외부 곱 항을 계산하기 위해 워프에 32개의 스레드를 사용할 것이다. 다음으로, 스레드는 256개의 항과 실행 \\(A2_{0}\\) 및 \\(A2_{1}\\) 슬라이스를 곱한다. 두 슬라이스에 대한 결과를 레지스터에 합산한 후 SRAM(\\(A2\\)[warpid])에 저장한다. \\(o_{i}\\)는 궁극적으로 \\(q_{i}\\) 항과 \\(A2\\)의 _all_ 슬라이스를 곱한 (식 (5))의 합이기 때문에, 우리는 모든 워프의 결과를 함께 합하고 (\\(A2\\)의 나머지 슬라이스를 보유함) 결과를 \\(y\\)[블록]에 저장한다. 우리는 그 결과를 토큰의 \\((8\\times\\mathrm{cur}_{\\mathrm{block}+\\mathrm{block}\\) 타일까지 유지하는 것으로 생각할 수 있다(노트 8은 \\(\\mathrm{cur}_{\\mathrm{block}\\)의 각 증분에서 8개의 워프가 8개의 다른 타일들을 처리하기 때문이다).\n' +
      '**KV\\(kV\\) 상태 갱신:** 블록에 대해 각각 \\(k[i],v[i]\\(16\\times 16\\) 및 \\(16\\times 64\\) 크기의 \\(k[i],v[i]\\) 타일에 로드하여 \\(k_{a},k_{b},\\mathrm{v}_{\\mathrm{frag}}}를 등록한다. 32개의 스레드를 사용하여 256개의 외부곱 항을 \\(k[i]\\)에 계산하고 \\(\\mathrm{v}_{\\mathrm{frag}}\\)과 곱하고 그 결과를 \\(A2_{0},A2_{1}\\) 실행 상태에 저장한다.\n' +
      '\n' +
      '최종 결과인 \\(y\\)을 출력으로 합하여 \\(2^{nd}\\) 차수 계산을 완료한다.\n' +
      '\n' +
      '# 다음 토큰 예측\n' +
      '\n' +
      '차세대 토큰 예측 시, 테일러 선형 주의에서 값비싼 \\(KV\\) 상태 갱신과 슬라이딩 윈도우 주의 계산에 IO 인식 알고리즘을 사용한다.\n' +
      '\n' +
      '2.1 테일러 선형 주의집중\n' +
      '\n' +
      'PyTorch에서의 KV 업데이트는 다음의 목록에 제공된다. 그림 6에서 우리는 커널에 대한 PyTorch 구현 속도를 벤치마킹한다.\n' +
      '\n' +
      '```\n' +
      '1fromeinopsimportrearrange\n' +
      '2importtorch\n' +
      '3fromtorchimportnn\n' +
      '4\n' +
      '5defstep(self,kv_state:torch.Tensor,k_state:torch.Tensor,q:torch.Tensor,k:torch.Tensor,v:torch.Tensor);\n' +
      '6"""\n' +
      '7Computelinearattentionwithrecurrentview\n' +
      '8->Assumeq.shapeis(b,h,1,D);kandv.shapeare(b,h,1,d),whereDisthedimensionafterapplyingthefeaturemapanddistheheaddimension.\n' +
      '9"""\n' +
      '10b,h,1,d=q.shape\n' +
      '11assert1==1,f\'q.shapeis(q.shape)butshouldbe((b),{h},1,{d})\'\n' +
      '12#Expanddimsforbroadcastingtopputelinearettention\n' +
      '13q,k,v=q.unsqueezeeze(-2),k.unsqueezeeze(-2),v.unsqueezeeze(-1)\n' +
      '14\n' +
      '15kv_state+=k[:,:,-1:]*v[:,:,-1:]\n' +
      '16k_state+=k[:,:,-1:]\n' +
      '17\n' +
      '18#Computelinearattention\n' +
      '19num=(q*kv_state).sum(dim=-1)\n' +
      '20y=num/((q*k_state).sum(dim=-1)+self.eps)\n' +
      '21\n' +
      '22y=rearrange(y,\'bh1d->b1(hd)\').to(q.dtype)\n' +
      '23returnself.dropout(self.out_proj(y))\n' +
      '```\n' +
      '\n' +
      '목록 2: 테일러 선형 주의 KV 업데이트의 PyTorch 구현\n' +
      '\n' +
      '도 6: Pure PyTorch(목록에 도시되고 [13]에 소개됨) 대를 사용하여 테일러 선형 주의 반복 업데이트를 계산하기 위한 시간(ms). 우리의 기반 커널(알고리즘 2). 벤치마킹은 16개의 특징 차원, 16개의 헤드, 64개의 헤드 차원을 사용하며, 선형 어텐션의 _numerator_에 초점을 맞춘다. 각 점은 단일 NVIDIA H100 GPU에서 측정되는 10번의 반복에 걸친 중앙값을 나타낸다.\n' +
      '\n' +
      '```\n' +
      '0:\\(KV_{t-1}\\) 상태\\(\\in\\mathbb{R}^{Hd^{2}d}\\), 시간\\(t\\) Featurized \\(q,k\\in\\mathbb{R}^{B\\times H\\times 1\\times D}\\) and \\(V\\in\\mathbb{R}^{B\\times H\\times 1\\times d}\\) for \\(d\\) for the head dimension (예: 64) and \\(D\\) for the expanded feature map dimension (예: 273=1+16+16^{2}\\) for feature dim 16. 하드웨어에 친숙하기 위해 패딩을 통해 \\(D=320\\)(s.t. \\(320\\mod 64=0\\))을 허용하였다.\n' +
      '0: 갱신 \\(KV_{t}\\) 상태. \\(\\mathrm{batch}\\times\\mathrm{heads}\\) 병렬 연산으로 병렬화하고, 블록당 \\(\\mathrm{n_{warps}}=8\\)의 워프를 갖는다. \\(\\mathrm{n_{threads}=\\mathrm{n_{warps}}\\times 32\\(\\triangleright\\) per warp\\(\\mathrm{buffer}}=\\mathrm{n_{warps}}\\times 8\\times d\\) \\(k\\) \\(B_{v}\\) (Size \\(d\\)) for \\(q\\) \\(\\mathrm{tic}=0,\\mathrm{toc}=1) \\(k\\)에 대한 Create SRAM buffer \\(B_{v}\\) (Size \\(d\\)) for \\(k\\)에 대한 Create SRAM buffer \\(\\mathrm{total_{batch}}=5\\) (Size \\(2\\times\\mathrm{320}}=64\\) 중간 계산을 위한 SRAM 버퍼 A(Size\\(\\mathrm{n_warps}}}times d\\)의 Create 레지스터 \\(B_{q}\\gets q\\)의 Size2; Threads \\(i\\in[0..\\mathrm{total_{batch}})에 대한 모든 \\(j\\mod 32\\(B_{v}\\gets v[m]\\\\tangleright\\)의 부하 \\(j<d\\); \\(j+=32,m+=1\\mathrm{toc}\\oplus 1\\)의 부하 \\(1\\times 64)\\times 64)를 각각 보유한다.\n' +
      '```\n' +
      '\n' +
      '**알고리즘 2** 컴퓨팅\\(KV\\) 상태 업데이트\n' +
      '\n' +
      '2.2 슬라이딩 윈도우 주의\n' +
      '\n' +
      '트레이닝/프리필 동안, 우리는 플래시 어텐션 슬라이딩 윈도우 구현[11]을 사용한다.\n' +
      '\n' +
      '우리의 IO 인식 구현은 다음 토큰 예측에 중점을 둔다. 아래 목록에는 토치 참조가 포함되어 있습니다. 본 논문에서 제안한 IO-aware 슬라이딩 윈도우 어텐션 알고리즘은 3에 제시되어 있으며, 스레드 레지스터에서 연산을 융합하여 데이터 이동을 레지스터하기 위한 느린 SRAM을 최소화하는 것이 핵심이다.\n' +
      '\n' +
      'Micro BenchmarkWe 벤치마크 키 베이스라인(Torch, Flash Attention-2[11], and the Based kernel on a NVIDIA H100 GPU in Figure 7. The benchmark are used window size 64, head dimension 64, and number of head 16. We vary of batch size on \\(x\\) axis and repeat across \\(y\\) axis. 이러한 타이밍은 \\(KV\\)-캐시 상태를 업데이트하는 시간이 아니라 주의 계산만을 포함한다는 점에 유의한다. 이러한 타이밍은 또한 (아래에 도시된 바와 같이) 로터리 인코딩에 대한 어떠한 처리도 포함하지 않는다.\n' +
      '\n' +
      '```\n' +
      '1importtorch\n' +
      '2fromtorchimportnn\n' +
      '3\n' +
      '4"""\n' +
      '5b:batchsize\n' +
      '6h:numberofheads\n' +
      '7n:sequencelength\n' +
      '8d:headimension\n' +
      '9\n' +
      '10w:windowsize\n' +
      '11\n' +
      '12qw:bxhx1xd\n' +
      '13kw:bxhxwxd\n' +
      '14vw:bxhxwxd\n' +
      '15"""\n' +
      '16w=torch.einsum("bhod,bhdnd->bhn",qw,kw)\n' +
      '17a=torch.nn.functional.softmax(w,dim=-1)\n' +
      '18result=torch.einsum("bhn,bhd->bhd",a,vw)\n' +
      '```\n' +
      '\n' +
      '목록 3: 슬라이딩 윈도우의 파이토치 구현\n' +
      '\n' +
      '도 7: 슬라이딩 윈도우 어텐션을 컴퓨팅하는 상이한 방식들에 대한 시간(ms) 다음 토큰 예측 — PyTorch, 플래시 어텐션(슬라이딩 윈도우 기능을 지원하는) 또는 우리의 추론 커널을 사용한다. 각 점은 생성에서 서로 다른 토큰 위치에 있는 쿼리 토큰에 대한 중위수를 나타냅니다.\n' +
      '\n' +
      '```\n' +
      '0:\\(KV_{t-1}\\) state \\(\\in\\mathbb{R}^{Hwd}\\), time \\(t\\) 및 projected hidden state \\(q,k,v\\in\\mathbb{R}^{B\\times H\\times 1\\times d}\\), \\(H\\) head, head dimension \\(d\\), sliding window size \\(w\\) 및 batch size \\(B\\)을 나타낸다.\n' +
      '0: 갱신 \\(KV_{t}\\) 상태. 블록당 n({}{text{warps}=4\\)의 와프를 갖는 배치 \\(\\times\\) 헤드의 병렬 연산으로 병렬화한다. 블록 내에서 정의 타일 크기\\(T\\)\\(\\triangleright\\)\\(T=16\\) based Define n\\({}_{\\text{threads}=\\)n\\({}_{\\text{warps}\\times 32\\)\\(\\triangleright\\) 워프당 32개의 스레드를 가정하면 SRAM 버퍼\\(B_{k}\\)과 \\(B_{v}\\)(각각 크기\\(4T\\times 4T\\))를 유지하여 SRAM 버퍼\\(B_{k}\\)와 \\(B_{v}\\)을 생성한다. (\\triangleright\\) Assumption \\(4T=64\\) is the \\(w\\), \\(d\\) Create SRAM vector \\(B_{q}\\)(Size \\(1\\times 4T\\)) to hold \\(q\\) during the kernel execution. \\(w\\), \\(d\\) (\\triangleright\\) Single query, assume \\(d=64\\) Create SRAM vector \\(B_{w}\\)(Size \\(1\\times 4T\\)) of type float for intermediate attention computation. SRAM 벡터 \\(B_{o}\\)(Size \\(1\\times 4T\\))를 생성하여 출력을 유지한다. \\(B_{o}\\)(Size \\(1\\times 4T\\)) (\\triangleright\\) Single output, assume \\(d=64\\) SRAM buffer max and sum (각 작업자는 float size를 기준으로)을 생성한다. 융합 연산 시 데이터를 유지하기 위해 레지스터 조각 q\\({}_{\\text{reg}}\\), k\\({}_{\\text{reg}}\\), v\\({}_{\\text{reg}}\\)을 생성한다. 중간 계산 인 레지스터를 저장하기 위해 레지스터 조각 w\\({}_{\\text{reg}}\\)(size \\(1\\times 4T\\))과 w\\({}_{text{reg}}\\(size \\(4T\\times 1\\))을 생성한다. 출력 인 레지스터를 저장하기 위해 레지스터 프래그먼트 o\\({}_{\\text{reg}}\\)(size \\(4T\\times 1\\))를 생성한다. n\\({}_{text{threads}}\\)을 사용한 로드\\(B_{k}\\gets k\\); n\\({}_{text{threads}}\\)을 사용한 로드\\(B_{v}\\gets v\\); 하나의 워프를 사용한 로드\\(B_{q}\\gets q\\). (\\triangleright\\) HBM to SRAM Loads q\\({}_{\\text{reg}}\\gets B_{q}\\). \\\\ (q\\)는 모든 warps에 방송된다. \\ (\\triangleright\\) SRAM to Register Loads k\\({}_{\\text{reg}\\gets B_{k}[\\text{warpid}]\\). 각 warp는 \\(B_{k}\\)(_i.e. a column)에서 \\(T\\times 4T\\)의 \\(T\\times 4T\\)을 얻는다. loads v\\({}_{\\text{reg}\\gets B_{v}[\\text{warpid}]\\. 각 warp는 \\(B_{v}\\) (_i.e. a column)에서 \\(T\\times 4T\\)의 \\(T\\times 4T\\)을 얻는다. W\\(\\\\text{reg}\\\\text{reg}\\\\text{reg}\\\\text{reg}\\\\text{reg}\\\\text{leright)의 전역적 합을 계산하기 위해 모든 워프에 대해 W\\(\\\\text{w}\\\\text{reg}\\\\text{w}\\\\text{reg}\\\\text{w}\\\\text{reg}\\\\text{w}\\\\text{reg}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{w}\\\\text{ 휨 하중은 \\(B_{w}\\); 모든 슬라이스는 o\\({}_{text{reg}\\)를 0으로 초기화한다. \\(B_{w}\\); 모든 슬라이스는 o\\({}_{text{reg}\\)을 0으로 초기화한다. (\\triangleright\\) Matrix-vector (GEMV) multiplication \\(\\text{o}_{\\text{reg}\\leftarrow\\) w\\({}_{\\text{reg}\\)v\\({}_{\\text{reg}\\) Write o\\({}_{\\text{reg}\\) to global memory\\(\\triangleright\\) Register to SRAM, SRAM to HBMExtended Architecture Details\n' +
      '\n' +
      '이 섹션에서는 언어 모델 복잡성의 **작은** 개선을 가능하게 할 수 있는 두 가지 추가 아키텍처 세부 사항을 설명한다. 그러나 테일러 선형 주의와 tcWindow 레이어의 조합만으로도 이러한 추가 구성요소를 사용하여 최상의 모델의 0.1 퍼플렉시티 포인트 내에 도달하기에 충분하다는 것을 강조한다(표 4).\n' +
      '\n' +
      '컨볼루션.우리는 선형 주의 및 tcWindow 계층 중 일부를 게이티드 컨볼루션[1, 9, 72]으로 교체하는 것이 언어 모델링 성능에서 작은 개선을 가능하게 한다는 것을 발견했다. 우리는 짧은 회선과 SiIU 비선형성을 갖는 BaseConv 층[1]을 사용한다[73]. 컨볼루션을 짧게 유지(예: 폭 3)함으로써, 리커런트 상태 크기를 낮게 유지하고 처리량을 향상시킨다. 사영은 차원을 인자\\(c=4\\)만큼 확장한다. 참고로, 이 계층은 Mampa 블록 _without_ the SSM[5]과 거의 동일하다.\n' +
      '\n' +
      '\\mathbf{y}\\coloneqq(\\underbrace{(\\mathbf{u}\\cdot\\mathbf{W}_{1}+\\mathbf{b}_{1}}}_{\\text{ Linear Projection}}\\odot\\sigma\\underbrace{(\\mathbf{h}*\\mathbf{u}\\cdot\\mathbf{W}_{2}+\\mathbf{b}_{2}}}_{\\text{ Convolution}}}\\cdot\\mathbff{W}_{3}+\\mathbf{b}_{3}\\tag{7}\\mathbf{b}_{1}}}_{\\text{1}}\\text{선형 프로젝션}}\\odot\\sigma\\underbrace{(\\mathbf{h}*\\mathbf{u}\\cdot\\mathbf{w}_{2}+\\mathbf{b}_{2}}}\\cdot\\mathbf{w}_{3}+\n' +
      '\n' +
      '여기서 \\(\\mathbf{u}\\in\\mathbbb{R}^{N\\times d}\\)는 투영된 입력이고, \\(h\\in\\mathbb{R}^{N\\times cd}\\)은 학습된 필터이고, \\(\\odot\\)은 하다마드 곱이고, \\(\\mathbf{W}_{1},\\mathbff{W}_{2}\\in\\mathbbb{R}^{d\\times cd}\\), \\(\\mathbff{W}_{3}\\in\\mathbbb{R}^{cd\\times d}\\), \\(\\mathbf{b}{b}{2}\\in\\mathbbb{R}^{d}\\), \\(\\mathbf{b}{b}{b}{b}{2}\\in\\mathbb}{R}^{d}\\), \\(\\mathbb}{b}{b}{b}{b}{b}{\n' +
      '\n' +
      'Decay.최근 반복 아키텍처는 다양한 방식으로 구현되는 _decay_ 용어의 사용을 포함한다[5, 6, 27, 28]. 직관으로서 붕괴 항은 토큰이 "최근" 토큰 대 "최근" 토큰에 얼마나 참석해야 하는지 제어한다. 순서에서 "초기" 토큰입니다. 이전 작업은 입력 독립 [27, 28, 특히 [5, 6] 또는 입력 종속 [5, 6] 붕괴율을 사용하는 두 가지 범주로 나뉜다. 후자는 개선된 품질을 제공하지만, 시퀀스 프로세싱 동안 병렬 스캔의 사용을 요구한다[5]. 대신, 병렬 스캔을 피하면서 선형 주의층에 대해 더 거친 입력 종속 감쇠 기법을 소개한다. 우리는 이전 작업에서와 같이 _fixed_ decay rate를 사용하는 decay 행렬로 시작하고 선형 주의에서 각 \\(h\\) 헤드가 인과 내적에 고유한 decay schedule을 적용하도록 한다. 그런 다음, 출력 표현을 생성할 때 각 입력이 헤드의 조합(감소율)을 효과적으로 측정할 수 있는 학습된 프로젝션 \\(\\mathbf{W}\\in\\mathbb{R}^{d\\times h}\\)을 소개한다.\n' +
      '\n' +
      '## 부록 D 확장 결과\n' +
      '\n' +
      '다운스트림 언어 결과 언어 모델링에서 Based의 성능을 추가로 평가하기 위해 여러 다운스트림 작업에 대해 PILE 사전 훈련된 모델을 평가한다. 우리는 EleutherAI[16]의 LM 평가 하네스를 사용하여 [5]와 동일한 프로토콜을 사용한다. 특히, 우리는 다음의 메트릭들 및 태스크들의 세트를 사용한다:\n' +
      '\n' +
      '* LAMBADA(perplexity and accuracy) [75]\n' +
      '* HellaSwag (normalized accuracy) [76]\n' +
      '* PIQA(정확도) [77]\n' +
      '* ARC-챌린지(정규화된 정확도) 및, 별도로, 쉬운 서브세트 ARC-쉬운(정확도) [78]\n' +
      '* 위노그랑데(정확도) [79]\n' +
      '\n' +
      '정규화된 정확도는 시퀀스 길이에 의해 정규화된 정확도를 말하며 [5]로 동등한 설정을 유지하는 데 사용된다. 표 5의 결과를 보고한다. 3억 6천만 개의 파라미터 모델과 13억 개의 파라미터 모델 모두에 대해 Based는 Mamba 및 Modern Transformers (Transformer++ (LLaMa))를 포함한 최신 및 최신 아키텍처와 경쟁적으로 수행한다.\n' +
      '\n' +
      '다운스트림 DNA 분류 우리는 DNA 모델링을 위해 서로 다른 아키텍처가 어떻게 비교되는지 추가로 평가한다. 섹션 6.1에 설명된 사전 훈련된 모델을 사용하고 최근 DNA 언어 모델을 평가하는 데 사용되는 인기 벤치마크(게놈 벤치마크) [74]에서 DNA 서열 분류로 얼마나 잘 전달되는지 테스트한다[41].\n' +
      '\n' +
      '우리는 표 6에서 HG38-사전 훈련된 Transformer++ 모델과 기반 모델을 비교했으며, 작업 간에 유사한 성능을 발견하여 사전 훈련 중 품질이 다운스트림 분류로 이전되었음을 나타낸다. 참고로, [41]의 결과도 포함한다. 토큰화의 상당한 차이에 직접적으로 필적하지는 않지만, 평가는 기반으로 서로 다른 양식에 대해 강력하게 수행할 수 있으며 최근 서열 모델링 아키텍처도 평가된 DNA 작업에서 이전 최신 기술을 능가하거나 경쟁할 수 있음을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Dataset & Enhancer Cohn & Enhancer Ens & Human Reg. & Non-TATA Promoters & Human OCR Ens. \\\\ \\hline CNN & 69.5 & 68.9 & 93.3 & 84.6 & 68.0 \\\\ DNABERT & 74.0 & 85.7 & 88.1 & 85.6 & 75.1 \\\\ GPT & 70.5 & 83.5 & 91.5 & 87.7 & 73.0 \\\\ HumanDNA & 74.2 & 89.2 & **93.8** & 96.6 & **80.9** \\\\ \\hline Transformer++ & 73.4 & **89.5** & 89.9 & 94.4 & 79.5 \\\\ Mamba & 73.0 & - & - & 96.6 & - \\\\ Based & **74.6** & **89.5** & 89.5 & **96.8** & 79.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: GenomicsBenchmarks에 대한 사전 훈련된 DNA 모델의 **다운스트림 평가[74]. 우리는 [41]에서 이전에 보고된 결과와 함께 사전 훈련된 모델(Transformer++, Mamba, Based)을 사용하여 상위 1 분류 정확도(%)를 보고한다. 우리는 다운스트림 작업으로의 사전 훈련에서 유사한 품질 매칭을 찾는다. 최신 아키텍처는 또한 분류 작업에 대한 최신 결과를 얻을 수 있습니다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline Architecture & Params & LAMBADA & HellaSwag & PIQA & Arc-E & Arc-C & WinoGrande & Average \\\\  & & Ppl. \\(\\downarrow\\) & Acc. \\(\\uparrow\\) & Acc. Norm. \\(\\uparrow\\) & Acc \\(\\uparrow\\) & Acc \\(\\uparrow\\) & Acc. Norm. \\(\\uparrow\\) & Acc. \\(\\uparrow\\) & Acc. \\(\\uparrow\\) \\\\ \\hline \\hline Transformer++ (LLaMa) & 360m & 18.39 & 42.52 & 33.48 & 63.98 & 46.04 & 24.49 & 53.99 & 44.08 \\\\ Transformer (Pythia) & 356m & 25.17 & 37.16 & 31.32 & 63.76 & 44.82 & 23.8 & 51.54 & 42.08 \\\\ Baspl & 363m & 21.80 & 38.66 & 33.43 & 64.42 & 45.79 & 24.66 & 51.22 & 43.03 \\\\ Mamba & 358m & 20.23 & 39.65 & 33.63 & 65.02 & 47.01 & 25.00 & 50.75 & 43.51 \\\\ H3 & 362m & 57.59 & 23.58 & 30.62 & 63.11 & 45.20 & 23.29 & 50.28 & 39.35 \\\\ \\hline Transformer++ (LLaMa) & 1.33b & 11.12 & 49.10 & 39.29 & 66.16 & 51.68 & 26.19 & 53.43 & 47.64 \\\\ Baspl & 1.35b & 12.35 & 46.96 & 39.11 & 66.32 & 50.72 & 26.54 & 50.43 & 46.68 \\\\ Mamba & 1.32b & 13.11 & 46.13 & 39.41 & 66.38 & 52.36 & 25.94 & 50.83 & 46.84 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 미리 훈련된 언어 모델의 **다운스트림 평가. 표 1과 동일한 모델 세트는 모두 파일 [30]에서 추출한 동일한 100억 토큰에 대해 훈련되었으며 엘루터AI [16]**실험 세부 정보에 의한 LM 평가 하니스를 사용하여 평가되었다.\n' +
      '\n' +
      '### 언어모델 사전학습\n' +
      '\n' +
      '우리는 모든 실험을 실행하기 위해 A100 80GB Nvidia GPU를 사용한다. 우리는 모든 사전 훈련 실행을 위해 플래시 주의 코드 기반[https://github.com/Dao-AILab/flash-attention/tree/main](https://github.com/Dao-AILab/flash-attention/tree/main]에서 밀접하게 적응된 훈련 인프라를 사용한다[11]. 파일 데이터는 GPT2BPETokenizer를 사용하여 토큰화되며 모든 모델은 동일한 순서로 데이터를 본다. 여기서는 각 아키텍처를 훈련하는 데 사용되는 하이퍼파라미터 및 구성에 대한 세부 정보를 제공합니다. 또한 FLOP 계산에 대한 세부 정보를 제공합니다.\n' +
      '\n' +
      '**Based** 표 7의 사양을 사용하여 훈련한다. 우리의 구현은 여기에 제공된다: [https://github.com/HazyResearch/based](https://github.com/HazyResearch/based) 초기 모델은 부록 B [24, 34]에서 논의된 Fast Transformer CUDA 커널을 사용하여 훈련되고 평가되었다.\n' +
      '**트랜스포머++[15]** 이전 작업 [5, 6]에 이어 로터리 인코딩, RMSNorm 및 SwiGLU를 트랜스포머++로 사용하는 현대 라마 아키텍처를 참조합니다. 여기에 제공된 플래시 어텐션 훈련 코드[https://github.com/Dao-AILab/flash-attention/tree/main](https://github.com/Dao-AILab/flash-attention/tree/main][11]을 사용하여 표 8의 사양을 사용하여 훈련한다.\n' +
      '**맘바[5]** 표 9의 사양을 사용하여 훈련하며, 여기서 파라미터는 [5]의 부록에서 조달된다. 구현은 [https://github.com/state-space/mamba](https://github.com/state-space/mamba)에서 제공된 참조로부터 조달된다.\n' +
      '* **Hyena[7]** 표 10의 사양을 사용하여 훈련하며, 여기서 파라미터는 [7]의 부록에서 조달된다. 구현은 [https://github.com/HazyResearch/safari](https://github.com/HazyResearch/safari)에서 제공된 참조에서 제공된다.\n' +
      '***H3[9]** 표 11의 사양을 사용하여 훈련한다. 구현은 [https://github.com/HazyResearch/safari](https://github.com/HazyResearch/safari)에서 제공된 참조로부터 조달된다.\n' +
      '**RWKV[8]** 표 12의 사양을 사용하여 훈련하고 [https://github.com/BlinkDL/RNKV-LM](https://github.com/BlinkDL/RNKV-LM)에서 참조 구현을 사용한다. 우리는 RWKV-V5를 구체적으로 평가한다.\n' +
      '***Gated Linear Attention(GLA)** 표 13의 사양을 사용하여 훈련한다. [https://github.com/berlino/gated_linear_attention](https://github.com/berlino/gated_linear_attention)에서 기준 구현에 따라 훈련한다.\n' +
      '\n' +
      '우리는 모든 모델들에게 개선된 트랜스포머++ 레시피(예를 들어, SwiGLU)를 관련있게 제공한다.\n' +
      '\n' +
      '### 컴퓨팅 재귀 상태 크기\n' +
      '\n' +
      '이 섹션에서는 섹션 3.1에 설명된 결과에 대해 반복 숨겨진 상태의 크기를 계산하는 방법에 대한 세부 정보를 제공한다. 우리는 합성 연관 회상 태스크인 주의[2], 슬라이딩 윈도우 주의[19], 맘바[5], H3[9], 하이에나[7]에서 6개의 시퀀스 믹서를 훈련하고 평가한다. 각각에 대해 추론 동안 메모리 소비에 영향을 미치는 하이퍼파라미터를 변화시킨다. 우리는 MQAR 정확도가 반복되는 숨겨진 상태의 크기에 따라 어떻게 달라지는지 비교한다.\n' +
      '\n' +
      '기반에서 반복상태 크기는 특징맵 \\(\\tilde{d}\\)을 적용한 후 모델 차원 \\(d\\)과 은닉차원의 크기에 의해 결정된다. \\(+1\\)은 분모를 계산하는 데 필요한 K-상태를 설명한다. 기준의 반복 보기에 대한 자세한 내용은 4를 참조하십시오.\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=(d+1)\\times\\tilde{d} \\tag{8}\\]\n' +
      '\n' +
      '본 논문에서는 테일러 지수 특징맵을 이용하여 보다 작은 차원(d^{\\prime}\\)으로 축소 투영하였다. 이러한 접근법으로, 반복 상태 크기는 다음과 같이 주어진다:\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=(d+1)\\times(1+\\frac{3d^{\\prime}}{2}+\\frac{d^{\\prime2}}{2}) \\tag{9}\\times(1+\\frac{3d^{\\prime}}{2})\n' +
      '\n' +
      '합성실험에서 우리는 \\(d\\in\\{48,64,128\\}\\)과 \\(d^{\\prime}\\in\\{8,16,24\\}\\)으로 Based를 실행하였다.\n' +
      '\n' +
      'Attention.Attention에서 recurrent state size (_i.e._KV-cache size)는 모델 차원 \\(d\\)과 시퀀스 길이 \\(N\\)의 두 파라미터에 의존한다. 아래 식에서 2는 KV-캐시의 키 및 값에 대한 별도의 저장소를 설명한다.\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=2\\times d\\times N\\tag{10}\\)\n' +
      '\n' +
      '합성 실험에서 우리는 \\(d\\in\\{64,128\\}\\)으로 주의를 기울인다. 시퀀스 길이\\(N\\)는 모델 아키텍처가 아닌 태스크에 의해 결정된다.\n' +
      '\n' +
      '슬라이딩 윈도우 어텐션. 슬라이딩 윈도우 어텐션의 반복 상태 크기는 모델 차원\\(d\\)과 슬라이딩 윈도우의 폭\\(k_{\\text{sliding}}\\)에 의해 주어진다. 아래 식에서 2는 KV-캐시의 키 및 값에 대한 별도의 저장소를 설명한다.\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=2\\times d\\times\\min(N,k_{\\text{sliding}}) \\tag{11}\\]\n' +
      '\n' +
      '합성 실험에서는 \\(d\\in\\{128\\}\\)와 \\(k\\text{sliding}}\\in\\{8,16,32,64,128,256,512,1024\\}\\)의 슬라이딩 윈도우 어텐션을 수행하였다.\n' +
      '\n' +
      'Mamba.Mamba의 반복 상태 크기는 모델 차원 \\(d\\)과 머리 수 \\(h\\)에 의해 결정된다. 아래 식에서 2는 맘바 블록에서의 확장을 설명한다.\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=2\\times d\\times d_{\\text{state}}\\tag{12}\\]\n' +
      '\n' +
      '합성실험에서는 \\(d\\in\\{64,128,256\\}\\)과 \\(d\\text{state}}\\in\\{8,16,24\\}\\)으로 Mamba를 실행하였다.\n' +
      '\n' +
      'H3.H3의 반복 상태 크기는 모델 차원\\(d\\)과 헤드 수\\(d_{\\text{state}}\\)에 의해 결정된다.\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=d\\times d_{\\text{state}}\\tag{13}\\]\n' +
      '\n' +
      '합성실험에서 H3는 \\(d\\in\\{64,128,256\\}\\)과 \\(d\\text{state}=\\frac{d}{4}\\)으로 실행하였다.\n' +
      '\n' +
      '하이에나. 하이에나의 반복 상태 크기는 모델 차원 \\(d\\)과 머리 수 \\(h\\)에 의해 결정된다. 아래 식에서 2는 KV-캐시의 키 및 값에 대한 별도의 저장소를 설명한다.\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=d\\times N\\tag{14}\\)\n' +
      '\n' +
      '합성 실험에서 우리는 하이에나를 \\(d\\in\\{64,128,256\\}\\)으로 실행했다.\n' +
      '\n' +
      '##### 언어 모델 평가\n' +
      '\n' +
      '이 섹션에서는 표 1과 4에 보고된 각 평가(열)에 대한 세부 정보를 제공한다.\n' +
      '\n' +
      'Pile_(Language Modeling)_ 먼저, 우리는 파일 테스트 세트에 대한 전반적인 당혹감을 보고한다[30]. 그런 다음, 반복 용량으로 인해 복잡성 갭의 양이 얼마나 많은지를 이해하기 위해, 우리는 또한 테스트 세트의 두 슬라이스(_i.e._ 서브세트)에 대해 복잡성을 평가한다:\n' +
      '\n' +
      '1. _Associative recall(AR) tokens._ 이전에 컨텍스트에서 발생한 바이그램의 최종 위치는 토큰이지만 훈련 데이터에서는 \\(\\leq 1250\\)회 발생한다.\n' +
      '2. _기타 토큰._ 다른 모든 토큰들\n' +
      '\n' +
      '이러한 슬라이스를 구성하기 위해 우리는 Arora 등의 [1]의 프로토콜을 정확히 따르고 더 자세한 내용을 위해 판독기를 해당 작업에 참조한다. 우리는 테스트 세트의 처음 1,600만 토큰에서 이러한 조각을 계산한다.\n' +
      '\n' +
      'SWDE 벤치마크에서 태스크는 원시 HTML 웹 사이트에서 반구조화된 관계를 추출하는 것이다. 예를 들어, 영화에 대한 IMBD 페이지(_e.g. Harry Potter and the Sorcerer\'s Stone_) 및 관계 키(_e.g._ release date)가 주어지면, 모델은 정확한 관계 값(_e.g._ 2001)을 추출해야 한다. SWDE 벤치마크는 원래 반구조화된 웹으로부터 개방 정보 추출의 작업을 위해 Lockard 등에 의해 큐레이션되었다[80]. 비교적 작은 언어 모델의 제로샷 기능을 평가하고 있기 때문에 작업을 약간 더 쉽게 하기 위해 조정합니다. 우리의 작업 설정은 Arora et al. [39]에서 사용된 것과 그 후에 유사하다.\n' +
      '\n' +
      'Fda(Information Extraction). 태스크는 FDA 웹사이트에서 긁어낸 PDF 세트에서 키-값 쌍을 추출하는 것이다. [39]에서 수집된 데이터 집합과 레이블을 사용합니다. 우리는 문서를 1,920개의 토큰 덩어리로 나눕니다. 청크에 나타나는 모든 키-값 쌍에 대해 간단한 프롬프트 템플릿을 사용하여 제로 샷 프롬프트를 만듭니다.\n' +
      '\n' +
      '{청크}\\n{key:\n' +
      '\n' +
      '우리는 모델이 프롬프트 후에 고정된 수의 토큰을 생성하고 그 값이 생성 내에 포함되어 있는지 확인(사례 불감증)할 수 있도록 한다. 세대에 값이 포함된 프롬프트의 비율인 **정확도**를 보고합니다.\n' +
      '\n' +
      '아래에는 키 값 쌍 "_Type of Test: Quantitative, colorimetric, pyranose oxidase(PROD)_"에 대한 제로 샷 프롬프트의 한 예가 포함된다. 실제 청크는 데이터 세트에서 실질적으로 더 길다(줄임말에 주의).\n' +
      '\n' +
      '510(k) SUBSTENTIAL EQUIVALENCE DETERMINATION DECISION ASSAY ONLY TEMPLATE A. 510(k) Number: k180209 B. Purpose for Submission: New Device C. Measurand: 1,5-Anhydroglucitol (1,5-AG) D. **Type of Test: Quantitative, colorimetric, pyranose oxidase (PROD)** E. Applicant: Diazyme Laboratories Inc. F. Proprietary and Established Names: Diazyme 1,5-AG Assay G. Regulatory Information: 1. Regulation section: 21 CFR 864.7470; Glycosylated hemoglobin assay 2. Classification: Class II… [1,920 토큰 of context from PDF]_... 디아자임의 1,5-AG 분석법은 효소 피라노스 산화효소(PROD)를 사용하여 1,5-AG의 2번째 위치 히드록실기를 산화시키고 생성된 과산화수소를 과산화효소(POD)를 사용하여 비색법으로 검출한다. **테스트의 종류:**\n' +
      '\n' +
      'Squad(Question Answering). Stanford Question Answering Dataset(SQUAD)는 언어 모델의 읽기 이해도를 평가하는 데 사용될 수 있다. 모델은 텍스트의 구절과 구절에 답이 포함된 질문을 부여받는다.\n' +
      '\n' +
      '이 작업에서 훈련된 모델은 상대적으로 소규모(100억 토큰에 훈련된 최대 13억 매개 변수)이며 미세 조정을 지시하지 않기 때문에 직접 질문할 때 질문에 답하는 데 어려움을 겪는다. 작업은 이러한 원시 언어 모델에 더 적합하도록 하기 위해 먼저 GPT-4를 사용하여 모델이 훈련된 다음 토큰 예측 작업과 더 밀접하게 유사하도록 질문을 재구성한다.\n' +
      '\n' +
      '당신은 이 질문을 다시 쓰고 진술로 대답할 수 있습니까? 정답이 진술의 마지막 부분인지 확인하십시오. \\ n\\n 질문: {question}\\n\\n Answer: {answer}\\n\\n Rewrite:\n' +
      '\n' +
      '예를 들어, "_Question: 어떤 NFL 팀이 슈퍼볼 50에서 AFC를 대표했는가? Answer: Denver Broncos_"라는 질문과 답변은 GPT-4에 의해 "_The NFL 팀이 슈퍼볼 50에서 AFC를 대표했는가?"라는 질문과 답변은 "_The NFL 팀은 Denver Broncos_"가 아닌 모든 문장(질문의 40%)을 폐기하고, 재작성된 문장이 실제로 답변으로 끝나는지 검증한다.\n' +
      '\n' +
      '우리는 검증 세트의 5,000개 스쿼드 질문에 대해 재포맷을 실행하여 다음 토큰 예측으로 포맷된 **2,984개 질문의 최종 데이터 세트를 생성한다.\n' +
      '\n' +
      '아래에는 제로 샷 프롬프트의 한 예가 포함됩니다. 재포맷된 질문은 굵은 글씨로 되어 있다.\n' +
      '\n' +
      '세 번째 시즌 동안, 두 회의의 1위 씨앗은 슈퍼볼에서 만났습니다. 캐롤라이나 팬서스는 단 1패로 정규시즌을 마친 10개 팀 중 한 팀이 됐고, 6개 팀 중 한 팀이 15-1 기록을 획득한 반면 덴버 브롱코스는 슈퍼볼에 8차례 출전한 4개 팀 중 한 팀이 됐다. 브롱코스는 3년 만에 슈퍼볼 XLVIII에 오르며 두 번째 슈퍼볼에 올랐고, 팬서스는 프랜차이즈 역사상 두 번째 슈퍼볼에 올랐으며, 다른 출연은 슈퍼볼 XXXVIII이다. 공교롭게도 두 팀 모두 슈퍼볼 50에 앞서 마지막 슈퍼볼 출전에서 존 폭스의 지도를 받았다. **15-1 기록을 가진 슈퍼볼 50의 팀은**이론적인 결과였다.\n' +
      '\n' +
      '### Introduction\n' +
      '\n' +
      '이 섹션에서 우리의 초점은 논문의 이론적 결과에 있을 것이다. 구체적으로, 최소 게이트 컨볼루션 연산자 [1, Definition 4.1]인 BaseConv와 모델 Based와 Mamba[5]의 동등성을 보이고, 다양한 설정에서 MQAR 문제 [1, Section H.7.1]에 대한 하한을 증명한다. 우리는 표기법을 설정하고 모델의 이론적 공식을 소개하는 것으로 시작한다.\n' +
      '\n' +
      '기호. 우리는 각각 \\(\\begin{bmatrix}1&1&\\ldots&1&1\\end{bmatrix}\\)과 \\(\\begin{bmatrix}0&0&0&0\\end{bmatrix}\\)의 크기 \\(k\\)과 \\(\\mathbf{1}^{k}\\)과 \\(\\mathbf{0}^{k}\\)의 크기 \\(k\\)과 \\(\\begin{bmatrix}0&0&0\\end{bmatrix}\\)의 크기 \\(k\\)의 모든 행 벡터를 \\(\\mathbf{1}^{k}\\)과 \\(\\mathbf{0}^{k}\\)으로 나타낼 것이다. 또한 이 노트에서 표준 기저 벡터 \\(\\mathbf{e}_{i}\\)를 열 벡터로 해석하고 다음 행렬 인덱싱 규칙을 준수한다. \\(\\mathbf{M}[i,j]\\)는 \\(i\\)번째 행과 \\(j\\)번째 열, \\(\\mathbbbb{F}^{1\\times n}\\)는 \\(i\\)번째 행, 그리고 \\(\\mathbbbbb{F}^{m\\times n}\\)는 \\(\\mathbbb{M}\\in\\mathbbb{F}^{m\\times n}\\)의 필드이고, 여기서 \\(\\mathbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb 행렬\\(\\mathbf{M}\\in\\mathbb{R}^{n\\times m}\\)에 대하여, 우리는 \\(\\mathbff{M}\\circ\\mathbff{M}\\in\\mathbb{R}^{n\\times m^{2}\\)의 열의 쌍별 하다마드 곱을 \\(\\mathbff{M}\\circ\\mathbb{R}^{n\\times m^{2}\\)으로 정의한다.\n' +
      '\n' +
      '(\\mathbf{M}\\circ\\mathbf{M})[:,i]:=\\mathbf{M}[:,j]\\odot\\mathbf{M}[:,k]\\quad\\text{for}\\quad i\\in[m^{2}],\\\\mathbf{M}[:,i]:=\\mathbf{M}[:,j]\\odot\\mathbf{M}[:,k]\\quad\\text{for}\\quad i\\in[m^{2}],\\\n' +
      '\n' +
      '\\[j=\\left\\lfloor\\frac{i-1}{m}\\right\\rfloor+1,\\quad k=(i-1)\\mod m+1.\\tag{15}\\]\n' +
      '\n' +
      '또한, 행렬 \\(\\mathbf{M}\\)의 요소별 지수를 \\(\\exp[\\mathbf{M}]]\\으로 정의하며, 여기서 \\(\\exp[\\mathbf{M}]_{ij}=\\exp(\\mathbf{M}_{ij})\\이다. 다음으로, 우리는 벡터 \\(\\mathbf{u},\\mathbf{v}\\in\\mathbbb{F}^{n}\\)의 _Hadamard 곱을 \\(\\mathbf{u}\\odot\\mathbf{v}\\)으로 나타내고, 연산은 그에 따라 행렬로 확장될 수 있으며, 벡터 \\(\\mathbf{u},\\mathbf{v}\\in\\mathbbb{F}^{n}\\)의 _linear(또는 비순환) convolution_를 \\(\\mathbf{u}*\\mathbf{v}\\)으로 나타낸다.\n' +
      '\n' +
      '연산 회로 표기.우리는 연산 회로의 표기[81]를 간단히 소개한다. 필드 \\(\\mathbb{F}\\) 위의 변수 \\(X\\triangleq\\{x_{1},x_{2},\\ldots,x_{n}\\}\\)을 갖는 _arithmetic circuit_\\(\\mathcal{C}\\)은 방향성 비순환 그래프로 해석되며, 입력 노드는 \\(X\\)의 변수 또는 \\(\\mathbb{F}\\)의 상수에 의해 레이블링되고 내부 노드는 출력 노드에서 계산된 다항식으로 \\(+\\) 또는 \\(\\times\\)으로 레이블링된다.\n' +
      '\n' +
      '우리는 또한 회로의 _size_를 노드의 수로, 회로의 _depth_를 입력 노드와 출력 노드 사이의 가장 긴 경로의 길이로, 회로의 _width_를 회로 내의 병렬 연산의 수로, 또는 회로를 통해 수평 \'컷\'에 의해 교차될 \'와이어\'를 지칭할 것이다. 더욱이, 회로의 _degree_는 회로에 의해 계산된 다항식의 차수로 정의된다. 이를 다음과 같은 정의로 요약한다.\n' +
      '\n' +
      '**Definition F.1**.: arithmetic circuit \\(\\mathcal{C}\\)은 \\(n,s,\\Delta,w)\\-circuit_, 만약 \\(\\mathcal{C}\\)이 크기 \\(s\\) 및 깊이 \\(\\Delta\\) 및 너비 \\(w\\)의 \\(n\\)-ariate arithmetic circuit이다.\n' +
      '\n' +
      '### The Models\n' +
      '\n' +
      '이제 우리는 독자의 편의를 위해 모델 기반과 맘바의 정의를 소개한다. 우리는 위에 제시된 표기법과 일관성을 보장하기 위해 이러한 모델을 재정의했다.\n' +
      '\n' +
      '#### f.2.1 Based\n' +
      '\n' +
      '기반 모델은 아래에 정의된 BaseConv 및 LinearAttention의 두 가지 계층 유형을 결합합니다.\n' +
      '\n' +
      '*Definition F.2**(BaseConv[1]): 입력 시퀀스 \\(\\mathbf{u}\\in\\mathbbb{R}^{N\\times d}\\)이 주어지면, 여기서 \\(N\\)은 시퀀스 길이이고 \\(d\\)은 모델 차원, 학습된 가중치 행렬 \\(\\mathbffW}\\in\\mathbbb{R}^{d\\times d}\\) 및 바이어스 \\(\\mathbfB}\\in\\mathbbb{R}^{N\\times d}\\) 및 컨볼루션 필터 \\(\\mathbfK}\\in\\mathbbb{R}^{N\\times d}\\)의 행렬은 다음과 같다.\n' +
      '\n' +
      '\\mathbf{z}^{\\texttt{BaseConv}}:=(\\mathbf{u}\\mathbf{W}^{B}+\\mathbf{B}^{B})\\odot\\left(\\mathbf{K}*\\bm{u}+\\mathbf{B}^{K}\\right)\\in\\mathbbb{R}^{N\\times d}, \\tag{16}\\times\n' +
      '\n' +
      '여기서 컨볼루션은 입력 길이 \\(N\\)에 걸쳐 적용된다.\n' +
      '\n' +
      '정의 F.3**(LinearAttention[24]): 입력 시퀀스 \\(\\mathbf{u}\\in\\mathbb{R}^{N\\times d}\\)이 주어지면, 여기서 \\(N\\)은 시퀀스 길이이고 \\(d\\)은 모델 차원, 선형 프로젝션 6\\(\\texttt{Projection}_{q},\\texttt{Projection}_{k}\\in\\mathbb{R}^{d\\times d^{\\prime}, \\texttt{Projection}_{v}\\in\\mathbb{R}^{d\\times d}\\)은 특징 차원, 여기서 \\(d^{\\prime}\\)은 다음을 계산한다.\n' +
      '\n' +
      '각주 6: 행렬 \\(\\mathbf{u}\\in\\mathbb{R}^{m\\times n}\\)의 선형 투영에 의해, 우리는 일부 가중치 행렬 \\(\\mathbf{w}\\in\\mathbbb{R}^{n\\times n}\\)과 바이어스 \\(\\mathbfbb{R}^{m\\times n}\\)에 대한 \\(\\mathbf{u}\\mathbf{w}+\\mathbf{B}\\)을 의미한다.\n' +
      '\n' +
      '\\left(\\overline{\\mathbf{z}^{\\texttt{LinearAttention}}):=\\left(\\overline{\\mathbf{Q}\\overline{\\mathbf{K}^{\\top}\\right)\\mathbf{V}\\in\\mathbb{R}^{N\\times d}, \\tag{17}\\)\n' +
      '\n' +
      '여기서 \\(\\mathbf{Q}:=\\texttt{Projection}_{q}(\\mathbf{u}),\\mathbf{K}:=\\texttt{Projection}_{k}(\\mathbf{u}),\\mathbf{V}:=\\texttt{Projection}_{v}(\\mathbf{u})를 갖고,\n' +
      '\n' +
      '\\\\overline{\\mathbf{Q}} =[\\mathbf{1},\\mathbf{Q},\\mathbf{Q}\\circ\\mathbbb{R}^{N\\times(1+d^{ \\prime}+d^{\\prime 2}),\\]\\[\\overline{\\mathbf{K}} =[\\mathbf{1},\\mathbf{Q},\\mathbf{K}\\circ\\mathbbb{R}^{N\\times(1+d^{ \\prime}+d^{\\prime 2}}}}},\\\\overline{\\mathbf{K}} =[\\mathbf{1},\\mathbf{Q},\\mathbf{K}\\circ\\mathbbb{R}^{N\\times(1+d^{ \\prime}+d^{\\prime 2}}}}}}.\\\\overline{\\mathbf{K}} =[\\mathbf{1},\\mathb\n' +
      '\n' +
      '#### f.2.2 Mamba\n' +
      '\n' +
      '이제 [5]의 맘바 모델을 소개합니다.\n' +
      '\n' +
      '**Definition F.4** (Mamba[5]): 입력 시퀀스 \\(\\mathbf{u}\\in\\mathbb{R}^{N\\times d}\\)이 주어지면, 여기서 \\(N\\)은 시퀀스 길이이고 \\(d\\)은 모델 차원이며, Mamba 레이어는 다음을 계산한다:\n' +
      '\n' +
      '\\[\\mathbf{z}^{\\texttt{Mamba}:=\\texttt{SSM}(\\overline{\\mathbf{A}},\\overline{\\mathbf{B}},\\mathbf{C})(\\mathbf{u})\\in\\mathbbb{R}^{N\\times d},\\tag{18}\\).\n' +
      '\n' +
      '(\\overline{\\mathbf{A}\\in\\mathbb{R}^{\\overline{d}\\times\\overline{d},\\overline{\\bm{B}\\in\\mathbbb{R}^{\\overline{d}\\in\\mathbb{R}^{\\overline{d}\\으로 정의됨)\n' +
      '\n' +
      '\\exp\\left(\\Delta\\mathbf{A}}:=\\exp\\left(\\Delta\\mathbf{A}\\right), \\tag{19}\\tag{B}:=\\left(\\Delta\\mathbf{A}\\right)^{-1}\\left(\\exp\\left(\\Delta\\mathbf{A}\\right)-\\mathbf{I}\\right)\\cdot\\Delta\\mathbf{B}(\\exp\\left(\\Delta\\mathbf{A})-\\mathbf{I}\\right)\\cdot\\mathbf{B},\\tag{19}\\tag{19}}:=\\exp\\left(\\Delta\\mathbf{A}\\right)^{-1}\\left(\\Delta\\mathbf{A}\\right)\\cdot\\mathbf{B},\\tag{19}\\tag{B}:=\\exp\\left(\\Delta\\mathbf{A}\\right)-\\mathbf{I}\\right)\\\n' +
      '\n' +
      '여기서 \\(\\overline{d}\\), 상태 차원 및 \\(\\mathbfbbb{R}^{\\overline{d}\\times\\overline{d}\\)은 모델의 파라미터이며, 다음의 _input-dependent_ 파라미터 \\(\\mathbf{B},\\mathbf{C}\\in\\mathbbb{R}^{N\\times\\overline{d}},\\Delta\\in\\mathbbb{R}^{N\\times d}\\)과 함께 입력 \\(\\mathbf{u}\\)에 의존하지 않는다.\n' +
      '\n' +
      '\\texttt{Linear}_{N\\times\\overline{d}}(\\mathbf{u}):=\\texttt{Linear}_{N\\times\\overline{d}}(\\mathbf{u}):=\\texttt{Linear}_{N\\times\\overline{d}}(\\mathbf{u})\\in\\mathbbb{R}}\\in\\texttt{Linear}_{N\\times\\overline{d}}(\\mathbf{u})\\in\\mathbb{R}\\in\\texttt{Linear}_{N\\times\\overline{d}}(\\mathbf{u})}\n' +
      '\n' +
      'for \\(i\\in[N]\\) 여기서 유의할 점은 매개변수 \\(\\overline{\\mathbf{B},\\mathbf{C},\\Delta\\)는 인과관계 7이고, 우리는 \\(i\\in[N]\\)에 대한 입력 \\(i\\in[N]\\)의 \\(i\\)번째 행에 대한 의존성을 나타낸다. 여기서 \\(\\overline{\\mathbf{A}}_{i}\\in\\mathbbbb{R}^{\\overline{d}\\times\\overline{d}\\)에 대한 의존성은 방정식 19의 \\(\\delta_{i}\\에서 상속되며, \\(\\overline{\\mathbf{B}}[i,:]=:\\mathbf{C}}[i,:]=:\\mathbf{C}}[i,:]=:\\mathbf{C}_{i}\\\\(i\\in[N]\\)에 대한 첨자 \\(i\\)을 추가하여 \\(i\\in[N]\\)에 대한 입력 \\(i\\)번째 행\n' +
      '\n' +
      '각주 7: 즉 \\(\\mathbf{B}[i,:],C[i,:]\\)와 \\(\\Delta[i,:]\\)는 \\(\\mathbf{u}[0\\cdots i-1]\\에만 의존한다.\n' +
      '\n' +
      '마지막으로, 식 18의 SSM은 선형 재발로 실현된다. 즉, 모든 \\((i,j)\\in[N]\\times[d]\\에 대해, 우리는\n' +
      '\n' +
      '\\overline{\\mathbf{h}[i,j] =\\overline{\\mathbf{a}}_{i}\\mathbf{h}[i-1,j]+\\overline{\\mathbf{B}_{i}\\mathbf{u}[i,j]\\tag{21}\\[\\mathbf{z}[i,j] =\\mathbf{C}_{i}^{\\top}\\mathbf{h}[i,j]\\mathbf{h}[i,j]\n' +
      '\n' +
      '여기서 \\(\\mathbf{h}[i,j]\\in\\mathbb{R}^{\\overline{d},\\mathbf{z}[i,j]\\in\\mathbb{R}\\)는 eq에서 SSM의 잠재상태와 출력을 나타낸다. (18).\n' +
      '\n' +
      'BaseConv의 동등성\n' +
      '\n' +
      '필드\\(\\mathbb{F}\\) 위에 변수\\(X\\)가 있는 다항식의 경우, 방향 비순환 그래프로 해석할 때 그 끝 노드에서 다항식의 출력을 계산하는 연산 회로\\(\\mathcal{C}\\)이 있다. [1, 정리 4.2]는 크기\\(s\\)와 깊이\\(\\Delta\\)의 산술회로\\(\\mathcal{C}\\)에 대하여 \\(\\tilde{\\mathcal{O}}(s\\Delta)\\) 파라미터와 \\(\\tilde{\\mathcal{O}}(\\Delta)\\) 레이어를 사용하는 등가 BaseConv 연산자의 존재를 보였다. 후속편에서는 식 17과 식 18에서 계산된 모델 출력을 다항식으로 \\(\\mathbf{u}\\)과 \\(\\exp\\left(\\mathbf{u}\\right)\\)으로 표현하여 이 결과를 사용하여 이질적인 모델 간의 동등성을 보여준다. 이제 [1, 정리 4.2]를 회상하고자 합니다. 그렇게 하기 전에 먼저 [1]에서 다음과 같은 정의를 설정한다.\n' +
      '\n' +
      '**Definition F.5**.: An\\(\\left(N,L,d,\\tilde{N},\\tilde{d}\\right)-\\text{Gated Convolution Model}\\)은 \\(L\\) 층들을 갖는 적층된 시퀀스 대 시퀀스 모델이다:\n' +
      '\n' +
      '1. 입력 및 출력은 \\(N\\times d\\) 행렬이고,\n' +
      '2. 각 레이어의 연산은 요소별 게이팅, 컨벌루션, 선형 프로젝션, 및\n' +
      '3. 모든 개별 게이트 컨볼루션 레이어는 \\(\\tilde{N}\\times\\tilde{d}\\) 행렬과 \\(\\tilde{N}\\times\\tilde{d}\\) 행렬로 구성된다. 우리는 tuple \\((\\tilde{N},\\tilde{d})\\)을 모델의 내부 차원_으로 지칭한다.\n' +
      '\n' +
      '또한 입력\\(\\mathbf{u}\\in\\mathbb{R}^{N\\times d}\\)이 \\(\\mathbf{u}^{\\prime}\\in\\mathbbb{R}^{\\tilde{N}\\times\\tilde{d}\\)에 임베딩되어 있다고 가정한다.\n' +
      '\n' +
      '\\[\\mathbf{u}^{\\prime}[n,t]=\\begin{cases}\\mathbf{u}[n,t]&\\text{if }n<N,\\t<d\\\\0&\\text{otherwise.}\\end{cases}\\text{if }n<N,\\t<d\\\\0&\\text{otherwise.}\\mathbf{u}[n,t]=\\begin{cases}\\mathbf{u}[n,t]&\\text{if }n<N,\\t<d\\\\0&\\text{otherwise.}\\end{cases}\\text{if\n' +
      '\n' +
      '마지막 층(\\mathbf{z}\\in\\mathbbb{R}^{\\tilde{N}\\times\\tilde{d}\\)의 출력은 \\(\\mathbf{z}\\)의 왼쪽 상단(N\\times d\\) 엔트리를 추출하여 출력 \\(\\mathbf{y}\\in R^{N\\times d}\\)으로 변환된다.\n' +
      '\n' +
      'Theorem F.1**([1], Theorem 4.2).: _For any \\((nd,s,\\Delta,w)\\)-arithmetic circuit \\(\\mathcal{C}\\)에 대해 \\(\\left(N,\\Delta^{\\prime},d,\\tilde{N},\\tilde{d}\\right)-\\texttt{BaseConv}\\(N=n,\\Delta^{\\prime}=\\mathcal{O}(\\Delta\\log w)\\)를 모사하는 \\(\\tilde{N}=\\mathcal{O}(w),\\tilde{d}=d\\)이 존재한다.\n' +
      '\n' +
      '**Remark F.1**.: Notational simplicity을 위해 \\(\\mathbf{u}_{i,j}\\)을 엔트리 \\(\\mathbf{u}[i,j]\\)을 나타내는 \\(\\mathbf{u}\\)에서 다항식의 변수에 대한 기호로 사용할 것이다.\n' +
      '\n' +
      '우리는 이제 정리 F.1을 사용하여 부록 F.2의 모델과 방정식 16의 BaseConv 층 사이의 동등성을 보여주는 결과를 제시한다.\n' +
      '\n' +
      '*Proposition F.1**: _Given a input \\(\\mathbf{u}\\in\\mathbb{R}^{N\\times d}\\), equivalent \\(\\left(N,O(\\log^{2}(Nd)), d,O(N(d+d^{\\prime 2}), O(\\max(d,d^{\\prime 2})) \\texttt{BaseConv}\\)이 존재한다. eq. (17)._\n' +
      '\n' +
      '\\(\\mathbf{Q},\\mathbf{K}\\in\\mathbbb{R}^{N}times d^{\\prime},\\mathbf{V}\\in\\mathbb{R}^{N}times d}\\), 모든 \\(1\\) 행렬. 및 \\(\\mathbf{B}^{ell}\\equiv\\mathbbb{R}^{d\\times d}\\), 그리고 \\(\\mathbff{B}^{k}\\in\\mathbbb{R}^{d\\times d}\\,\\mathbbb{R}^{V}\\in\\mathbbb{R}^{N}times d}\\), 모든 \\(1\\) 행렬. 원시 [1, 명제 H.10]을 이용하여 이들 각각을 차례로 계산할 수 있으며, 다른 것들은 \\(O(1)\\) 층들과 \\(Nd\\) 파라미터들을 이용하여 기억할 수 있다.\n' +
      '\n' +
      '다음으로, \\(\\mathbf{Q}\\circ\\mathbf{Q},\\mathbf{K}\\circ\\mathbf{K}\\in\\mathbb{R}^{N\\times d^{\\prime 2}\\)의 각 엔트리 \\((i,j)\\in[N]\\times[d^{\\prime 2}]\\에 대한 식을 유도한다. 식 15로부터 \\(\\mathbf{M}\\circ\\mathbf{M}\\)의 각 엔트리는 \\(\\mathbf{M}\\)의 엔트리의 곱으로 쓰일 수 있음을 관찰한다. 그래서 우리는\n' +
      '\n' +
      '(\\mathbf{Q}\\circ\\mathbf{Q})[i,j]\\equiv\\mathbf{Q}[i,k]\\cdot\\mathbf{Q}[i,\\ell]\\][(\\mathbf{K}\\circ\\mathbf{K}[i,j]\\equiv\\mathbf{K}[i,k]\\cdot\\mathbf{K}[i,k]\\cdot\\mathbf{K}[i,k]\\cdot\\mathbf{Q}[i,k]\\cdot\\mathbf{Q}[i,k]\\cdot\\mathbf{Q}[i,k]\\cdot\\mathbf{Q}[i,k]\\cdot\\mathbf{Q}[i,k]\\cdot\\mathbf{Q}[i,k]\\cdot\\mathbf{Q}[i,k]\\cdot\\mathbf{Q}[i,k]\\cdot\\mathbf{Q}[i\n' +
      '\n' +
      '그러나, \\(k=\\left\\lfloor\\frac{i-1}{d^{\\prime}\\right\\rfloor+1,\\quad\\ell=(j-1)\\mathbf{Q}{1},\\mathbf{Q}{2}\\in\\mathbbb{R}{N}\\times d}\\)을 \\(k=\\left\\lfloor\\frac{j-1}\\right\\rfloor+1,\\quad\\ell=(j-1)\\mathbffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff \\(\\mathbf{P}\\)에 의한 곱셈과는 별도로 \\(O(1)\\) 레이어만 사용하면 되며, 또한 \\(\\mathbf{P}\\mathbf{u}\\)를 계산하는 회로는 단순히 입력을 재배열하기 때문에 \\(\\mathbf{P}\\mathbf{u}\\)[1, Corollary H.20]을 계산하는 단일 BaseConv 레이어가 존재한다. 적층레마[1, Lemma H.11]에 의해, 우리는 \\(\\left(N,O(1),d,O(N(d+d^{\\prime 2}),O(\\max(d,d^{\\prime 2}))\\texttt{BaseConv}\\) 모델을 얻기 위해 지금까지 출력의 구성을 얻기 위해 이 층들을 적층할 수 있다. 또한, 연접행렬 \\(\\overline{\\mathbf{Q}},\\overline{\\mathbf{K}}\\)\\(\\in\\mathbb{R}^{N\\times(1+d^{\\prime}+d^{\\prime 2})})은 지금까지 계산된 성분들의 덧셈을 취하며, 이는 BaseConv의 \\(O(1)\\)층을 취한다.\n' +
      '\n' +
      '마지막으로 LinearAttention의 출력의 각 엔트리 \\((i,j)\\in[N]\\times[d]\\)를 다항식으로 표현하면 다음과 같다.\n' +
      '\n' +
      '\\[\\mathbf{z}_{i,j}(\\mathbf{u})\\equiv\\sum_{m\\in[1+d^{\\prime}+d^{\\prime 2}],n\\in[N}\\overline{\\mathbf{Q}[i,m]\\cdot\\overline{\\mathbf{K}[n,m]\\cdot\\mathbf{V}[n,j]. \\tag{23}\\w} 따라서, 입력까지 BaseConv 층의 출력을 취하여 \\(\\mathbf{z}_{i,j}(\\mathbf{u})\\)를 계산하는 연산 회로를 유도하고, 세 가지 모두의 출력을 곱하여 합 내부의 각 항을 계산하고 추가 \\(\\log\\left\\lceil Nd\\right\\rceil\\) 깊이를 사용하여 합을 계산할 수 있다. 합 안의 각 항은 깊이가 \\(2\\)인 두 개의 곱셈 게이트를 필요로 하며, 각각의 곱셈 게이트는 합을 계산하는 크기 \\(Nd\\)의 회로에 입력 역할을 한다. 또한, 출력 게이트는 각각 병렬로 계산되어 크기\\(O(N\\cdot d)\\), 깊이\\(O(\\log(Nd))\\) 및 폭\\(O(Nd))의 회로가 있다. O 전체적으로 정리 F.1을 적용하면 \\(\\mathbf{z}\\)을 계산하는 등가가 된다.\n' +
      '\n' +
      '아래쪽 경계\n' +
      '\n' +
      '속편에서는 [1, Section H.7.1]에서 정의한 바와 같은 _multiple-query associative recall_ problem (MQAR)을 고려한다. 우리는 여기서 그 정의를 간단히 회상한다.\n' +
      '\n' +
      '입력수열 \\(\\mathbf{u}[0\\cdots 3N-1]\\triangleq\\left\\{\\left(\\mathbf{k}_{0},\\mathbf{v}_{0},\\mathbf{q}_{0}\\right),\\ldots,\\left(\\mathbf{k}_{N-1},\\mathbf{v}_{N-1},\\mathbf{q}_{N-1}\\right)\\right\\\\(c=|C|\\)이 주어졌다고 가정하자. 그 다음, 각 \\(1\\leq i\\leq N-1\\)에 대해 \\(0\\leq j<i\\)이 있는지 확인하고, \\(\\mathbf{q}_{i}\\equiv\\mathbf{k}_{j}\\)이 있으면 출력 \\(\\mathbf{v}_{j}\\)을 확인하는 것이 우리의 목표이다.\n' +
      '\n' +
      '4.1 AR의 공간복잡도\n' +
      '\n' +
      '우리는 표준 연관 회상(AR) 문제 해결의 공간 복잡성에 대한 하한을 제공하는 것으로 시작할 것이다. AR은 MQAR의 하위 클래스이기 때문에 이것은 자연스럽게 MQAR의 공간 복잡성에 대한 하한을 제공한다. 여기서, 우리는 공식적으로 연상 소환 문제를 회상한다.\n' +
      '\n' +
      'AR 문제는 키-값 쌍 \\(\\left\\{\\mathbf{k}_{i},\\mathbf{v}_{i}\\right\\}_{i=0}^{n-1}\\)과 마지막에 추가된 질의 \\(\\mathbf{q}\\)을 입력으로 하고, 어떤 \\(i\\in[0,N-1]\\에 대해 \\(\\mathbf{q}=\\mathbf{k}_{i}\\)이면 \\(\\mathbf{v}_{i}\\)을 출력하는 것을 목표로 한다.\n' +
      '\n' +
      '우리는 이제 _index 문제에 대한 무작위 통신 복잡도 하한 결과를 요구한다:\n' +
      '\n' +
      '인덱스 문제는 앨리스와 밥의 두 에이전트를 가지고 있는데, 앨리스는 스트링\\(\\mathbf{x}\\in\\{0,1\\}^{n}\\)을 가지고 있고 밥은 인덱스\\(i\\in[n]\\)을 가지고 있으며, 플레이어의 목표는 \\(i\\)번째 엔트리\\(\\mathbf{x}_{i}\\)을 출력하는 것이다. 더욱이, 우리는 또한 통신이 _one-way_가 되도록 요구한다: Alice만이 Bob에게 단일 메시지를 전송하는 것이 허용되고 Bob은 답변을 출력할 필요가 있다.\n' +
      '\n' +
      '우리는 다음과 같은 하한 결과를 사용할 것이다.\n' +
      '\n' +
      '**정리 F.2** ([82]): _\\(n\\) 길이의 비트열을 보내기 위한 인덱스 문제의 단방향 랜덤화 통신 복잡도 8은 \\(\\Omega(n)\\)._\n' +
      '\n' +
      '각주 8: 함수 \\(f\\)의 무작위 통신 복잡도는 \\(\\min_{\\pi}\\|\\pi\\|\\)으로 정의되며, 여기서 \\(\\pi\\)은 적어도 \\(2/3\\)의 성공 확률로 \\(f\\)을 해결할 수 있는 모든 무작위 프로토콜에 걸쳐 범위이다.\n' +
      '\n' +
      '반복 모델에 대한 하위바운드\n' +
      '\n' +
      '우리는 이제 정리 F.2를 사용하여 먼저 AR을 풀기 위해 다음 모델 클래스에서 필요한 비트 수에 대한 하한을 제공한다.\n' +
      '\n' +
      'F.6**(Recurrent Models): 입력\\(\\mathbf{u}\\in\\mathbbb{R}^{N}times d}\\)을 취하는 모델\\(\\mathcal{M}\\)은 입력 길이이고 \\(d\\)은 모델 차원이며, 위치에서의 출력을 나타내는 \\(i\\), \\(\\mathbf{Z}_{\\mathcal{M}}^{i}\\in\\mathbbb{R}^{tilde{d}\\)의 경우 _recurrent model_로 명명되며, 상태 크기를 나타내는 \\(\\tilde{d}\\)는 입력\\(\\mathbf{u}[0\\ldots i-1]\\의 선행 요소에 의해 배타적으로 결정된다. 상태\\(\\mathbf{Z}_{\\mathcal{M}}^{i}\\)는 \\(i\\)번째 요소까지의 입력에 따른 모델의 누적 정보를 나타내며, 입력 시퀀스에 대해 정적인 학습된 파라미터와 구별된다.\n' +
      '\n' +
      '구체적으로, \\(\\mathbf{Z}_{\\mathcal{M}}^{i}(\\mathbf{u})=\\phi(\\mathbf{u}[0\\ldots i-1])\\)는 입력 히스토리의 함수이지만 전체 입력 시퀀스의 함수가 아님을 나타낸다. 더욱이, 우리는 이것을 다음과 같이 표현할 수 있다.\n' +
      '\n' +
      '\\[\\mathbff{Z}_{\\mathcal{M}}^{i}(\\mathbf{u})=f_{\\mathcal{M}}^{i}(\\mathbf{Z}_{\\mathcal{M}}^{i-1},\\mathbf{u}[i]), \\tag{24}\\}\n' +
      '\n' +
      '함수들의 순서를 \\(\\{f_{\\mathcal{M}}^{i}\\}}_{i\\in[N]}\\)으로 정의하며, 이때 각 함수는 즉각적인 과거 상태와 현재 입력에 기초하여 상태를 진화하도록 조정된다.\n' +
      '\n' +
      '**리마크 F.2**.: 정의 F.6은 전체 입력에 대한 비인과 컨볼루션 연산에 기초하는 것과 같은 임의의 상태에서의 계산을 위해 본질적으로 전체 입력 시퀀스를 요구하는 모델을 제외한다는 점에 유의한다.\n' +
      '\n' +
      '*정리 F.3**.: AR을 해결하는 임의의 순환 모델\\(\\mathcal{M}\\)(정의 F.6)은 \\(\\max_{i}\\left|\\mathbf{Z}_{\\mathcal{M}}^{i}\\right|\\)이 적어도 \\(\\Omega(N)-bits._\n' +
      '\n' +
      '증명: \\(\\mathbf{x}\\in\\{0,1\\}^{N}\\)의 인덱스 문제를 인스턴스 \\((\\mathbf{x},i)\\)로 고려한다. 이제 AR 문제의 해당 인스턴스를 설명한다:\n' +
      '\n' +
      '\\[\\{j,\\mathbf{x}_{j}\\}_{j=0}^{N-1},i. \\tag{25}\\]\n' +
      '\n' +
      '다음으로 회귀모형 \\(\\mathcal{M}\\)을 이용하여 지수문제를 해결하기 위한 다음과 같은 단방향 프로토콜을 고려한다. Alice는 \\(\\mathbf{x}\\in\\{0,1\\}^{N}\\)에 접근하여 식 25와 같이 AR(쿼리 없이)에 대한 입력을 생성한다. Alice는 \\(\\mathcal{M}\\)을 \\(\\{i,\\mathbff{x}_{j}\\}_{j=0}^{N-1}\\)에 대해 모델 \\(\\mathcal{M}\\)을 실행하고 모델 \\(\\mathcal{M}\\)에 대한 메모리 내용을 Bob에게 전송한다. 이것은 크기\\(\\tilde{d}\\)의 상태\\(\\mathbf{Z}_{\\mathcal{M}}^{N-1}\\)를 포함해야 한다. 이 상태\\(\\f_{\\mathcal{M}}^{j}\\}_j\\in[N]}\\)는 함수 집합에 접근할 수 있다고 합리적으로 가정할 수 있기 때문이다. 이 모델이 AR을 해결한다고 가정하기 때문에, 출력 \\(\\texttt{Out}[N,:]=\\mathbf{x}_{i}\\)는 \\(i\\)의 연관된 값을 포함해야 한다. 여기서 Bob은 Alice가 보낸 메모리 컨텐츠를 이용하여 함수 \\(f^{N}\\)을 적용하여 \\(\\texttt{Out}[N,:]\\)을 다음과 같이 계산할 수 있다.\n' +
      '\n' +
      '\\[\\mathbf{x}_{i}=\\texttt{Out}[N,:]=f^{N}(\\mathbf{Z}^{N-1},\\mathbf{u}[N]).\\]\n' +
      '\n' +
      '즉, 이 프로토콜에서 통신되는 총 비트 수는 \\(\\left|\\mathbf{Z}_{\\mathcal{M}}^{N-1}\\right|\\)이다. 이제 \\(\\max_{j}\\left|\\mathbf{Z}_{\\mathcal{M}}^{j}\\right|\\)이 \\(o(N)\\)비트일 경우, \\(o(N)\\)의 통신 복잡도를 사용하는 인덱스 문제를 해결하기 위한 단방향 통신 프로토콜이 존재함을 보였다. 이는 이론 F.2와 모순되며, 따라서 AR을 푸는 모델\\(\\mathcal{M}\\)에도 \\(\\Omega(N)\\) 비트가 필요하다는 결론을 내린다.\n' +
      '\n' +
      'Corollary F.1**: _Given a input \\(\\mathbf{u}\\in\\mathbb{R}^{N\\times d}\\) to the AR problem, the causal Mamba model with all entries in its computation taking \\(O(1)\\) bits need \\(d+\\overline{d}\\geq\\Omega(N)\\) to solve AR.__\n' +
      '\n' +
      '증거: 우리는 먼저 인과적 맘바가 반복적 모델이라는 것을 보여줄 것이다. 이를 위해 먼저 방정식 21을 관찰하고 정의 F.4에서 언급한 바와 같이 입력 종속 변수\\(\\overline{\\mathbf{A},\\overline{\\mathbf{B},\\mathbf{C},\\Delta\\)가 인과적이라는 사실을 주목한다.\n' +
      '\n' +
      '다음으로 식 21에 의해 \\(\\mathbf{z}_{N,:}\\in\\mathbb{R}^{d}\\)을 계산하기 위해서는 \\(\\mathbff{C}_{N}\\in\\mathbbb{R}^{\\overline{d},\\overline{\\mathbf{B}}_{N}\\in\\mathbbb{R}^{R}}}\\(\\mathbf{h}[N-1,:]\\in\\mathbbb{R}^{d}\\)과 \\(\\Delta_{N}\\in\\mathbb{R}^{d}\\)이 필요하다. 여기서, 우리는 주어진 \\((N-1)\\)-st 상태 \\(\\mathbf{Z}_{\\texttt{Mamba}}^{N-1}\\in\\mathbb{R}^{overline{3d}+d}\\)을 갖는다.\n' +
      '\n' +
      '\\[\\mathbf{Z}_{\\texttt{Mamba}}^{N-1}:=\\{\\mathbf{h}[i-1,:],\\Delta_{N}^{1},\\overline{\\mathbf{B }}_{N}^{1},\\mathbf{C}_{N}^{1}\\},\\]\n' +
      '\n' +
      '\\(\\delta_{N}^{1},\\overline{\\mathbf{B}}_{N}^{1},\\mathbf{C}_{N}^{1}\\)은 모두 \\(\\delta_{N}^{2},\\overline{\\mathbf{B}}_{N}^{2},\\mathbf{C}_{N}}^{2}\\(\\delta_{N}=\\delta_{N}^{1}+\\delta_{N}^{1},\\overline{\\mathbf{B}}_{N}=\\mathbf{C}^{N}^{2}\\(\\delta_{N}}+\\delta_{N}^{2},\\overline{\\mathbf{B}}_{N}=\\mathbf{C}^{N}^{2}\\(\\delta_{N}^{2},\\overline{\\mathbf{B}}}}_{N}}} 그런 다음 우리는 함수 \\(f^{N}\\)을 다음과 같이 정의할 수 있다.\n' +
      '\n' +
      '\\mathbf{Z}_{\\texttt{Mamba}^{N}[j]\\mathbf{B}}_{N}\\bm{u}[N,j]\\]\\overline[=\\overline{\\mathbf{A}}_{N}\\mathbf{h}[N-1,j]+\\overline{\\mathbf{B}}_{N}\\mathbf{u}[N,j],\\]\\[texttt{Out}[N,j]=f^{N}(\\mathbf{Z}_{Texttt{Mamba}}^{N-1}}[j]=\\mathbf{C}_{N}^{top}\\mathbf{Z}_{Mamba}}^{N}[j]\\overline[=\\overline{\\mathbf{A}}_{N}\\mathbf{h}[N-1,j]+\\overline[mathbf{B}}_{N}\\mathbf{u}[N,j],\\\n' +
      '\n' +
      '따라서 F.3의 정리로 인해 AR을 풀기 위해서는 \\(\\left|\\mathbf{Z}_{\\texttt{Mamba}^{N-1}\\right|\\)\\(\\Omega(N)\\)비트가 필요하다는 결론을 내릴 수 있다. 마지막으로, \\(\\mathbf{Z}_{\\texttt{Mamba}^{N-1}\\)의 각 엔트리를 표현하려면 \\(O(1)\\)의 비트가 필요하다고 가정하면, 전체 상태 \\(\\mathbff{Z}_{\\texttt{Mamba}^{N-1}\\)을 표현하려면 \\(O(d+\\overline{d})\\)이 필요하며, 이는 클레임의 증명을 완료한다.\n' +
      '\n' +
      'AR을 위한 층수 하위 경계\n' +
      '\n' +
      '다음으로, 우리는 다시 정리 F.2를 사용하여 AR을 풀기 위해 필요한 층의 수에 더 나은 경계를 제공할 것이다.\n' +
      '\n' +
      '문장 F.4**: \\(\\mathbf{u}\\in\\{0,1\\}^{N\\times d}\\)이 주어졌을 때, \\(\\log c\\leq d\\leq 2^{(\\log N)^{1-\\epsilon}}\\)에 대해, 그리고 \\(c\\leq N\\)이 주어졌을 때, \\(O(\\log N)) 비트를 갖는 데이터 독립적인 BaseConv 모델인 \\(\\Omega(\\epsilon\\log N) 레이어가 필요하다. 증명: \\(L\\) 레이어를 사용하여 AR을 해결하는 BaseConv 모델의 경우, 정의에 따르면, 임의의 \\(\\mathbf{u}\\in\\{0,1\\}^{N\\times d9}\\)에 대해 AR을 해결하는 최대 차수 \\(2^{L}\\)의 다항식 \\(P(\\mathbf{u})\\)이 존재한다. 이것은 \\(\\mathbf{Z}^{i}_{\\texttt{BaseConv}}\\)으로 주어지는 BaseConv의 \\(i\\)번째 층의 출력에 대해, 우리는\n' +
      '\n' +
      '\\[\\mathbf{Z}^{i}_{\\texttt{BaseConv}}(\\mathbf{Y}^{i-1}_{\\mathcal{M}})\\equiv P^{i}(\\mathbf{Z}^{i-1}_{\\texttt{BaseConv}}),\\quad\\deg(P^{i})\\leq 2,\\\n' +
      '\n' +
      '차수 2의 다항식 \\(P^{i}\\)은 단순히 내부 곱을 취하여 모델이 AR을 풀 수 있도록 하는데, 여기서 \\(\\mathbf{Z}^{0}_{\\texttt{BaseConv}}:=\\mathbf{u}\\)이다. 또한, \\(L\\) 레이어가 있는 모델의 경우, 조성에 따라 \\(i\\in[L]\\)에 대한 \\(i\\)번째 레이어의 출력도 입력 \\(\\mathbf{u}\\)에 대한 다항식이며 최대 \\(2^{i}\\)의 차수를 갖는다. 결국, 우리는 \\(\\mathbf{u}\\in\\{0,1\\}^{N\\times d}\\)에 대한 차수 \\(\\leq 2^{L}\\)의 다항식 \\(P(\\mathbf{u})\\)을 갖는다. 정리 F.3의 증명에서와 같이, \\(\\mathbf{x}\\in\\{0,1\\}^{N}\\)과 AR 문제의 해당 인스턴스가 있는 인덱스 문제에 대한 인스턴스 인스턴스 \\((\\mathbf{x},i)\\)를 이전과 같이 다시 취한다.\n' +
      '\n' +
      '\\[\\mathbf{u}:=\\{j,\\mathbf{x}_{j}\\}_{j=0}^{N-1},i. \\tag{26}\\]\n' +
      '\n' +
      '다음으로 AR을 해결한다는 가설로부터 BaseConv 모델을 이용하여 인덱스 문제를 해결하기 위한 다음과 같은 단방향 프로토콜을 구축한다. Alice가 \\(\\mathbf{x}\\in\\{0,1\\}^{N}\\)에 접근하면 식 26과 같이 AR(쿼리 없이)에 대한 입력 \\(\\mathbf{u}\\)이 다시 생성된다.\n' +
      '\n' +
      'Alice는 먼저 \\(\\mathbf{a}:=\\mathbf{u}[0:N-2,:]\\in\\{0,1\\}^{(N-1)\\times d}\\) 값을 취하고, 이들 알려진 \\((N-1)d\\) 값을 대체함으로써 다음의 다항식을 정의한다:\n' +
      '\n' +
      '\\[Q(\\mathbf{u}_{N-1,0},\\ldots,\\mathbf{u}_{N-1,d-1})=P(\\mathbf{a},\\mathbf{u}_{N-1,0},\\ldots,\\mathbf{u}_{N-1,d-1}) \\tag{27}\\tag{27}}}\n' +
      '\n' +
      '여기서, \\(Q\\)은 밥이 가지고 있고 사소하게 차수 \\(D\\leq 2^{L}\\)을 갖는 값 \\(\\mathbf{u}[N-1,:]\\)에 해당하는 \\(d\\) 변수에서 다항식임을 주목하라. 이제, Alice는 모델 \\(\\mathcal{M}\\)을 실행하고 \\(Q\\)의 계수를 검색하여 Bob에게 전송할 수 있다. 우리는 \\(P\\)이 AR을 해결한다고 가정하기 때문에 Bob은 \\(i\\)의 연관값인 \\(P(\\mathbf{u}[N-1,:]\\)을 \\(Q\\)으로 대체하고 \\(P(\\mathbf{u})\\)을 계산할 수 있다.\n' +
      '\n' +
      '여기서, Alice가 보내는 다항식 \\(Q\\)은 \\(d^{2^{L}}}\\)의 각 항으로 최대 \\(2^{L}\\)의 차수를 가질 수 있다. 각 계수가 \\(B\\)비트를 갖는 경우, 정리 F.2를 이용하면, 통신되는 총 비트 수는 \\(B\\cdot d^{2^{L}}\\geq\\Omega(N)\\)를 만족해야 한다. 이것은 만약 \\(B\\cdot d^{2^{L}}\\leq o(N)\\)이라면, 방정식 26의 \\(i\\)의 연관 값이 인덱싱 문제에 대한 해답이기 때문에, 우리는 인덱스 문제를 해결하기 위한 단방향 통신 프로토콜이 \\(o(N)\\)의 통신 복잡도를 사용한다는 것을 보여주었고, 이는 정리 F.2와 모순된다. 따라서, 우리는 반드시 \\(o(N)\\)의 통신 복잡도를 사용해야 한다.\n' +
      '\n' +
      '2^{L}\\log(d)\\geq\\log\\left(\\frac{N}{B}\\right)-O(1)\\geq\\log\\left(\\frac{N}{B}\\right)-O(1)\\geq\\Omega(N)\\implies 2^{L}\\log(d)\\geq\\log\\left(\\frac{N}{B}\\right)-O(1)\\.\n' +
      '\n' +
      '양쪽에 로그를 취한 다음 수율\n' +
      '\n' +
      '\\frac{\\log\\left(\\frac{N}{B}\\right)}{\\log\\left(d\\right)}{\\log\\left(d\\right)}{\\log\\left(\\frac{\\log\\log B}{\\log\\left(d\\right)}O(1)\\geq\\log\\left(\\frac{\\log N-\\log B}{\\log\\left(d\\right)}{\\log\\left(d\\right)}O(1)\\]\\[\\geq\\log\\left(\\frac{\\log N-\\log B}{(\\log N-\\log B}{(\\log N^{1-\\epsilon}}\\right), \\tag{28}\\w}\\w}\n' +
      '\n' +
      '여기서 우리는 식 28에서 임의의 \\(\\epsilon>0\\)에 대한 \\(d\\leq 2^{(\\log N)^{1-\\epsilon}}\\)의 사실을 사용한다.\n' +
      '\n' +
      '또한, 모델 파라미터가 \\(O(\\log N)\\) 비트로 가정됨에 따라, \\(Q\\)의 모든 계수는 최대 \\(Nd\\) 변수의 곱이 될 수 있으므로 최대 \\(\\left(2^{O(\\log N)}\\cdot Nd\\right)^{2^{L}}\\)에서 절대값을 가져야 한다. 즉, 어떤 \\(\\alpha>0\\)에 대해, 우리는 각 계수에 다음과 같은 경계를 갖는다:\n' +
      '\n' +
      '\\[2^{B}\\leq\\left(N^{\\alpha+1}d\\right)^{2^{L}}\\leq\\left(N^{(\\alpha+2)2^{L}}\\right)\\]\n' +
      '\n' +
      '여기서 마지막 등식은 \\(d\\leq N\\)이라는 사실을 사용한다. 따라서 우리는\n' +
      '\n' +
      '\\[\\log(B)\\leq\\log(\\alpha+2)+L+\\log\\log N. \\tag{29}\\\\] 식 29를 식 28에 대입하면\n' +
      '\n' +
      '[L\\geq\\log\\left(\\frac{\\log N-\\log(\\alpha+2)-L-\\log\\log N}{(\\log N^{1-\\epsilon}}\\right) \\tag{30}\\]]\n' +
      '\n' +
      '이제, \\(L>\\log\\log N\\)이면, 우리는 끝이다. 그렇지 않고 \\(L\\leq\\log\\log N\\)이면 이를 식 30으로 대체하여 얻을 수 있다.\n' +
      '\n' +
      '\\frac{\\log N-\\log(\\alpha+2)-2\\log\\log N}{(\\log N^{ 1-\\epsilon}}\\right)\\] \\[=\\log\\left(\\log N-\\log(\\alpha+2)-2\\log\\log N\\right)-(1-\\epsilon}\\log\\log N\\tag{31}\\\\]\n' +
      '\n' +
      '우리는 이제 방정식 31의 첫 번째 항이 다음을 만족한다고 주장한다:\n' +
      '\n' +
      '[\\log\\left(\\log N-\\log(\\alpha+2)-2\\log\\log N\\right)\\geq(1-\\frac{ \\epsilon}{2})\\log\\log N. \\tag{32}\\]\n' +
      '\n' +
      '이것을 보기 위해, 충분히 큰 \\(N\\)에 대해, 다음이 성립한다는 것을 주목하라:\n' +
      '\n' +
      '\\[\\frac{\\log N}{2}\\geq\\log(\\alpha+2)+2\\log\\log N,\\\\\n' +
      '\n' +
      'hence, we get\n' +
      '\n' +
      '\\log\\log\\left(\\log N-\\log(\\alpha+2)-2\\log\\log N\\right)\\geq\\log\\left(\\frac{\\log N}{2}\\right)\\geq\\log N-1\\geq(1-\\frac{\\epsilon}{2}\\log\\log N.\\)\\log\\log\n' +
      '\n' +
      '이것은 수학식 32의 주장을 증명한다. 마지막으로, 수학식 32를 사용하여, 수학식 31은 다음과 같이 이어진다:\n' +
      '\n' +
      '\\[L\\geq(1-\\frac{\\epsilon}{2})\\log\\log N-(1-\\epsilon)\\log\\log N=\\frac{\\epsilon}{2}\\log\\log N,\\\\\n' +
      '\n' +
      '여전히 원하는 하한 \\(L=\\Omega(\\epsilon\\log\\log N)\\)을 제공한다.\n' +
      '\n' +
      'F.3**를 언급한다.: 우리는 정리 F.4를 각 층으로부터의 출력이 어느 정도의 다항식(\\Delta\\geq 2\\)인 모든 모델로 확장하여 \\(\\Omega(\\epsilon\\log\\log N/\\log\\Delta)\\의 하한을 구하는 것이 가능함을 언급한다.\n' +
      '\n' +
      '#### f.4.4 \\(d=\\log_{2}c\\)를 갖는 MQAR의 층수에 대한 하한\n' +
      '\n' +
      '설정. \\(d=\\log_{2}c\\)을 사용하여 \\(C\\)에서 가능한 모든 \\(c\\) 토큰을 인코딩합니다. 즉, 모든 \\(2^{d}\\) 가능한 \\(d\\) 비트 벡터는 MQAR에 대한 입력에서 토큰으로 나타날 수 있다. 이러한 MQAR의 설정을 해결하기 위해 데이터 독립적인 BaseConv는 \\(\\Omega(\\log d)\\) = \\(\\Omega(\\log\\log c)\\)-레이어를 필요로 하는 반면, Attention(+ReLU)는 \\(O(1)\\) 레이어에서 이를 해결할 수 있음을 보인다.\n' +
      '\n' +
      '먼저 Attention(+ReLU)을 이용하여 하찮은 해를 제공한다.\n' +
      '\n' +
      '**Proposition F.2**.: _Attention (linear bias and ReLU) followed with two layer of MLPs can solve MQAR for input sequence \\(\\mathbf{u}\\in\\{0,1\\}^{3N\\times d}\\) 그렇게 \\(d=\\log_{2}(c)\\) in \\(O(1)\\) layer._\n' +
      '\n' +
      'Proof.: a row \\(\\mathbf{u}[i,:]\\in\\{0,1\\}^{d}\\)을 주어 각 row을 \\(\\mathbf{w}[i,:]\\in\\{-1,1\\}^{d}\\)으로 표현하며, 여기서 \\(\\mathbf{w}+\\mathbf{B}\\), \\(\\mathbf{W}:=\\operatorname{diag}(2,\\ldots,2)\\in\\mathbbb{R}^{d\\times d}\\) 및 bias 행렬 \\(\\mathbf{B}\\)이 모든 \\(-1\\)\'의 행렬이 되도록 \\(\\mathbf{w}[i,j]=2\\mathbf{u}[i,j]=2\\mathbf{u}[i,j]-1\\)로 표현한다. 그리고 질의와 주요 투영행렬 \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{3N\\times d}\\)을 다음과 같이 지정할 수 있다.\n' +
      '\n' +
      '\\mathbf{k}_{\\lfloor i/3\\rfloor}&\\text{ if }i\\equiv 0\\mathbf{0}&\\text{otherwise}\\mathbf{w}[i,:]=\\mathbf{q}_{\\lfloor i/3\\rfloor}&\\text{otherwise}\\mathbf{0}&\\text{otherwise}\\mathbf{0}&\\text{cases}\\mathbf{w}[i+1,:]=\\mathbf{v}_{\\lfloor i/3\\rfloor}&\\text{if}i\\equiv 0\\mod 3\\mathbf{0}&\\text{otherwise}\\mathbf{cases}\\mathbf{w}[i,:] 상기 쌍방향 내부 생성물들을 계산하는 단계는,\n' +
      '\n' +
      'bf{K}^{\\top}[i,j}\\mathbf{Q}\\equiv\\begin{cases}\\langle\\mathbf{q}_{\\lfloor i/3\\rfloor},\\mathbf{k}_{\\lfloor j/3\\rfloor}\\rangle&\\text{if $i\\equiv 2\\mod 3$ 및 $j\\equiv 0\\mod 3$\\\\mathbf{0}\\text{otherwise}\\end{cases}\\mathbf{q}\\langle\\mathbf{q}\\lfloor i/3\\rfloor}\\rangle&\\text{if $i\\equiv 2\\mod 3$ 및 $j\\equiv 0\\mod 3$\\\\mathbf{0}\\text{otherwise}\\end{cases}\\mathbf{q}\\floor i/3\\rfloor}\\text{if $i\\equiv 2\\mod 3$ 및 $j\\equiv 0\\mod 3$\\\\mathbf{0}\\text\n' +
      '\n' +
      '그러나 \\(\\mathbf{q}_{\\lfloor i/3\\rfloor},\\mathbf{k}_{\\lfloor j/3\\rfloor}\\in\\{-1,1\\\\rfloor}d\\\\(\\langle\\mathbff{q}_{\\lfloor i/3\\rfloor},\\mathbf{k}_{\\lfloor j/3\\rfloor}\\lfloor j/3\\rfloor}\\\\(\\langle\\mathbff{q}_{\\lfloor i/3\\rfloor},\\mathbf{k}_{\\lfloor j/3\\rfloor}\\\\(\\langle\\mathbff{q}_{\\lfloor j/3\\rfloor}\\\\(\\langle\\mathbff{q}_{\\lfloor j/3\\rfloor}\\\\(\\langle\\mathbff{q}_{\\lfloor j/3\\rfloor}\\\\(\\langle\\ 그리고 각 엔트리 \\(-d+1\\)의 행렬로 바이어스 \\(\\mathbfB}\\in\\mathbb{R}^{3N\\times 3N}\\)을 취하여 \\(d-1\\)을 각 엔트리에서 뺀다. \\(\\mathbf{Z}:=\\textsc{ReLU}(\\mathbf{Q}\\mathbf{K}^{\\top}+\\mathbf{B})\n' +
      '\n' +
      '\\[\\mathbf{Z}[i,j]=\\mathbb{1}\\,\\{\\mathbf{q}_{\\lfloor i/3\\rfloor}\\equiv\\mathbf{k}_{\\lfloor j/3\\rfloor}\\}.\\\\\n' +
      '\n' +
      '다음, 우리는 여러 개의 정합을 가질 수 있고 단지 \\(1\\)을 반환하기만 하면 되므로, 다음과 같이 정의된 \\(\\mathbf{W}_{1},\\mathbf{W}_{2}\\in\\mathbb{R}^{d\\times d}\\) 행렬을 곱하여 \\(\\mathbf{Z}\\)을 수정하고 bias \\(\\mathbf{B}\\in\\mathbb{R}^{d\\times d}\\)을 추가한다.\n' +
      '\n' +
      '[\\mathbf{W}_{1}[k,j:=\\begin{cases}1&\\text{if $k\\geq j$}\\\\0&\\text{otherwise}\\end{cases},\\quad\\mathbf{W}_{2}[\\ell,k:=\\begin{cases}-1&\\text{if $k=\\ell,\\ell\\neq 0$}\\\\text{otherwise}\\end{cases},\\quad\\mathbf{B}[i,j=1.\\text{if $k=0$}\\ell,\\ell\\neq 0$}\\\\text{otherwise}\\end{cases},\\quad\\mathbf{B}[i,j=1.\\text{if $k=0$}\\ell,\\ell\\neq 0$}\\text{otherwise}\\end{cases},\\quad\\mathbf{W}_{2}[\\ell,k:=\\begin{cases}-1&\n' +
      '\n' +
      '\\(\\mathbf{Z}_{1}:=\\mathbf{Z}\\mathbf{W}_{1}\\) 및 \\(\\mathbf{Z}_{2}:=\\mathbff{Z}\\mathbf{W}_{1}\\mathbf{W}_{2}\\)에 대해, 우리는:\n' +
      '\n' +
      'ff{W}_{1}[i,k]\\mathbff{W}_{1}[i,k]=\\sum_{k}\\mathbf{Z}{1}[i,j]=\\sum_{k\\geq j}\\mathbf{Z}[i,k],\\]\\[\\mathbff{Z}_{2}[i,j]=\\sum_{k}\\mathbfff{Z}_{1}[i,j]=\\mathbff{Z}_{1}[i,j]=\\mathbff{Z}_{1}[i,j]=\\mathbff{Z}_{1}[i,j]=\\mathbff{Z}_{1}[i,0]\\mathbfff{Z}_{1}[i,j]=\\mathbfff{Z}_{1}[i,j]=\\mathbfff{Z}_{1}[i,j]=\\mathbff\n' +
      '\n' +
      '즉, \\(\\mathbf{Z}_{1}\\)의 각 엔트리는 동일한 또는 더 높은 열 인덱스에 있는 행의 엔트리들을 합하고, \\(\\mathbf{Z}_{2}\\)의 각 열은 행 내의 각 엔트리로부터 행 내의 모든 엔트리들의 합인 첫 번째 엔트리를 감산한다. 의미적으로, \\(\\mathbf{Z}_{1}\\)의 각 행에 대해, 첫 번째 매치의 인덱스까지의 \\(0\\)의 엔트리는 동일한 값을 가져야 하며, 따라서 \\(\\mathbf{Z}_{2}\\)의 유일한 비음수 엔트리이다. 다음으로, 우리는 \\(\\mathbf{Z}^{\\prime}\\in\\mathbb{R}^{3N\\times d}\\)을 얻기 위해 ReLU 하에서 바이어스를 추가하고 활성화한다.\n' +
      '\n' +
      '\\textsc{ReLU}(\\mathbf{Z}_{2}+\\mathbf{B})[i,k]:=\\textsc{ReLU}(\\mathbf{Z}_{2}+\\mathbf{B}[i,k]=\\begin{cases}1&\\text{if $k\\leq\\min\\{j|\\\\mathbf{q}_{\\lfloor i/3\\rfloor}\\equiv\\mathbf{k}_{\\lfloor j/3\\rfloor}\\}$}\\\\0&\\text{otherwise}.\\end{cases}\\text{if $k\\leq\\min\\\\text{if $k\\leq\\min\\m\\mathbf{q}_{\\lfloor j/3\\rfloor}\\equiv\\mathbf{k}_{\\lfloor j/3\\rfloor}\\\\text{otherwise}.\\end{cases}\\text{cases}1&\\text{if $k\\le\n' +
      '\n' +
      '이제, 우리는 다음과 같이 정의된 가중치 행렬 \\(\\mathbf{W}_{3}\\in\\mathbb{R}^{3N\\times d}\\)을 곱한다.\n' +
      '\n' +
      '\\[\\mathbf{W}_{3}[k,j]:=\\begin{cases}-1&\\text{if $k=j+1$}\\\\1&\\text{if $k=j$}\\\\0&\\text{otherwise}\\end{cases}\\begin{cases}-1&\\text{if $k=j$}\\\\0&\\text{if\n' +
      '\n' +
      '이것은 주어진 리트리버\\(\\overline{\\mathbf{Z}=\\mathbff{Z}^{\\prime}\\mathbff{W}_{3}\\in\\mathbb{R}^{3N\\times d}\\)를 산출한다.\n' +
      '\n' +
      '\\sum_{\\ell}\\mathbff{Z}^{\\prime}[i,k]:=\\sum_{\\ell}\\mathbf{W}_{3}[\\ell,k] =\\mathbf{Z}^{\\prime}[i,k]-\\mathbf{Z}^{\\prime}[i,k+1]=\\mathbbb{1}\\mathbb{q}_{\\min\\{j|\\\\mathbf{q}_{\\lfloor i/3\\rfloor}\\equiv\\mathbf{k}_{\\lfloor j/3\\rfloor}\\}\\\\mathbf{W}_{3}[i,k] =\\mathbf{Z}^{\\prime}[i,k]-\\mathbf{Z}^{\\prime}[i,k]-\\mathbb{Z}^{\\prime}[i,k+1]=\\mathbb{1}\\mathbb{q}_{\\lfloor i/3\\rfloor}\\equ\n' +
      '\n' +
      '마지막으로, 우리는 \\(\\mathbf{V}\\) 값을 곱하여 얻는다.\n' +
      '\n' +
      '\\mathbf{V}[i,:]\\mathbf{V}\\mathbf{Z}[i,j^{*}]\\cdot\\mathbf{V}[j^{*},:\\equiv\\begin{cases}\\mathbf{v}_{j^{*}&\\text{if $\\mathbf{q}_{\\lfloor i/3\\rfloor}\\equiv\\mathbf{k}_{\\lfloor j/3\\rfloor}\\equiv\\mathbf{k}_{\\lfloor j/3\\rfloor}\\equiv\\mathbf{0}&\\text{if no such $j^{*}}.\\end{cases}\\text{if\n' +
      '\n' +
      '즉, 질의에 대응하는 행은 제1 매칭 키에 연관된 값을 반환한다. 따라서, Attention(Computing \\(\\mathbf{Z}\\)) 다음에 두 개의 MLP(\\mathbf{Z}^{\\prime}\\)와 \\(\\overline{\\mathbf{Z}\\)을 각각 갖는 모델이 MQAR 문제를 해결한다.\n' +
      '\n' +
      '다음으로, BaseConv의 \\(L\\) 레이어의 출력을 계산하는 다항식의 차수와 연관시킨다.\n' +
      '\n' +
      '**Lemma F.1**.: _임의의 입력 시퀀스 \\(\\mathbf{u}\\)에 대해, 최대 차수 \\(2^{L}\\)의 BaseConv의 \\(L\\) 층들에 의해 계산된 다항식에 대한 다중선형 다항식 등가물(부울 입력들 위에)이 존재한다._ Proof.: \\(P(\\mathbf{u})\\)는 BaseConv의 \\(L\\) 레이어에 의해 계산된 다항식이라고 하자. BaseConv의 단일 레이어의 출력은 최대 2의 차수를 갖는 입력 변수에 대한 다항식과 같기 때문에, 이러한 레이어들을 구성하는 것은 최대 2의 차수를 갖는 다항식을 산출한다. 그러나 \\(P(\\mathbf{u})\\)는 다중 선형일 필요는 없지만, 다음과 같이 정의되는 다항식\n' +
      '\n' +
      '\\[Q(\\mathbf{u}):=(\\cdots((P(\\mathbf{u})\\mod(u_{1}^{2}-u_{1}))\\mod(u_{2}^{2}-u_{2}))\\cdots) \\mod(u_{3Nd}^{2}-u_{3Nd})\\]\n' +
      '\n' +
      '는 \\((u_{i}^{2}-u_{i})\\)로서 \\(P(\\mathbf{u})\\)에 상당하고, \\(u_{i}^{2}-u_{i})\\)는 각 입력 var\\(u_{i}\\in\\{0,1\\}\\)에 대해 0으로 평가된다. 그러나 \\(\\deg(Q(\\mathbf{u}))\\leq\\deg(P(\\mathbf{u}))\\)이므로 청구가 성립한다.\n' +
      '\n' +
      '우리는 이제 MQAR(위의 설정에서)을 계산하는 다항식의 정도와 연관시킨다.\n' +
      '\n' +
      'Lemma F.2**.: _The MQAR problem with \\(d=\\log_{2}(c)\\)는 차수\\(2d+1\\)의 다중선형 다항식으로 표현된다._\n' +
      '\n' +
      '증거: MQAR을 해결하는 명백한 부울 회로를 지정하는 것으로 시작할 것이다. 먼저 키와 쿼리의 XNOR을 비트 단위로 취하면 다음과 같다.\n' +
      '\n' +
      '\\mathbf{x}^{ij}=\\mathbf{q}_{i}\\texttt{ xnor }\\mathbf{k}_{j}:=(\\mathbf{q}_{i}\\wedge\\mathbf{k}_{j}) \\vee(\\neg\\mathbf{q}_{i}\\wedge\\neg\\mathbf{k}_{j}\\,\\text{ for }i>j, \\tag{33}\\text{\n' +
      '\n' +
      '여기서, \\(\\mathbf{x},\\mathbf{y}\\in\\{0,1\\}^{d}\\)에 대해, 우리는\n' +
      '\n' +
      '[\\mathbf{x}\\texttt{ xnor }\\mathbf{y}][k]:=\\begin{cases}1&\\text{if}\\mathbf{x}[k]=\\mathbf{y}[k] \\\\0&\\text{otherwise}\\end{cases}\\begin{cases}1&\\text{if}\\mathbf{x}[k]=\\mathbf{y}[k]\\text{otherwise}\\end{cases}\\begin{cases}1&\\text{if}\\mathbf{x}[k]\n' +
      '\n' +
      '즉, \\(\\mathbf{x}^{ij}\\)의 각 비트는 \\(\\mathbf{q}_{i}\\) 및 \\(\\mathbf{k}_{j}\\)의 해당 비트가 일치하면 1 iff로 설정된다. 다음으로 \\(d\\)비트의 AND를 구하여\n' +
      '\n' +
      '\\[\\mathbf{y}^{ij}:=\\bigwedge_{k\\in[d]}\\mathbf{x}_{k}^{ij},i>j. \\tag{34}\\]\n' +
      '\n' +
      '따라서, \\(\\mathbf{y}^{ij}\\)는 질의 \\(\\mathbf{q}_{i}\\)가 키 \\(\\mathbf{k}_{j}\\)와 일치하면 1로 설정된다. 마지막으로, 주어진 \\(k\\in[d]\\)에 대한 \\(k\\)번째 비트와 출력 \\(\\mathbf{z}^{ij}\\)을 얻기 위해 각 비트의 값을 곱한다.\n' +
      '\n' +
      '\\[\\mathbf{z}_{k}^{ij}:=\\mathbf{y}_{ij}\\wedge[\\mathbf{v}_{j}]_{k}. \\tag{35}\\]\n' +
      '\n' +
      '따라서, 회로의 출력은 다음과 같이 나타낼 수 있다.\n' +
      '\n' +
      '\\begin{cases}\\mathbf{v}_{i}&\\text{if}\\mathbf{q}_{i}\\equiv\\mathbf{k}_{j},i>j\\\\mathbf{0}&\\text{otherwise}.\\end{cases}\\text{if}\\mathbf{q}_{i}\\equiv\\mathbf{k}_{j},i>j\\\\mathbf{0}&\\text{otherwise}\n' +
      '\n' +
      '우리는 이제 위의 회로를 다중 선형 다항식으로 직접 변환할 수 있다. 표기를 약간 남용했을 때, 우리는 방정식 34에 대해 다음과 같은 대응 관계를 가지고 있다. 여기서 \\(\\mathbf{u}_{i}\\equiv\\mathbff{q}_{i},\\mathbf{u}_{j}\\equiv\\mathbff{k}_{j},i>j\\) 그리고 우리는 \\(\\mathbf{u}_{ij}\\)을 사용하여 진입 \\(\\mathbf{u}[i,j]\\)에 해당하는 변수를 나타낸다.\n' +
      '\n' +
      '=\\mathbf{u}_{ik}\\mathbf{u}_{jk}+(1-\\mathbf{u}_{ik})(1-\\mathbf{u}_{jk}) \\quad\\text{for each }k\\in[d],i>j.\\\n' +
      '\n' +
      '다음으로, 우리는 방정식 34를 다음과 같이 번역한다.\n' +
      '\n' +
      '\\[\\mathbf{y}^{ij}(\\mathbf{u}):=\\prod_{k\\in[d]}\\left(\\mathbf{u}_{ik}\\mathbf{u}_{jk}+(1-\\mathbf{u}_{ ik})(1-\\mathbf{u}_{jk})\\right).\\]\n' +
      '\n' +
      '마지막으로, 우리는 MQAR을 계산하는 다항식을 다음과 같이 쓸 수 있다.\n' +
      '\n' +
      '\\left(\\prod_{k\\in[d]}\\mathbf{z}^{ij}(\\mathbf{z}}}\\mathbf{u}_{ik}\\mathbf{u}_{jk}+(1-\\mathbf{u}_{ ik})(1-\\mathbf{u}_{jk})\\right)\\mathbf{u}_{(i+1)k}\\quad\\text{for each }k\\in[d],i>j, \\tag{36}\\right)\n' +
      '\n' +
      '여기서 \\(\\mathbf{u}[i+1,:]\\equiv\\mathbf{v}_{j}\\). 그런 다음 방정식 36이 다중 선형이고 차수\\(2d+1\\)을 갖는다는 것을 쉽게 관찰할 수 있다.\n' +
      '\n' +
      '우리는 이제 하한선을 제공할 준비가 되었다.\n' +
      '\n' +
      '*theorem F.5**.: _A data-independent BaseConv model need \\(\\log(2d)\\)-layer for solve MQAR for the input sequence \\(\\mathbf{u}\\in\\{0,1\\}^{3N\\times d}\\) with \\(d=\\log_{2}(c)\\)._\n' +
      '\n' +
      '증명: Lemma F.2로 인해 우리는 MQAR을 해결하는 다중선형 다항식이 존재한다는 것을 알고 있으며, [83, Lecture 3, Proposition 4]로 인해 독특하다. 구체적으로 우리는 차수\\(\\leq 2d\\)의 다중선형 다항식으로 MQAR을 풀 수 없다. 이제 MQAR을 정확히 해결하는 \\(L\\) 레이어가 있는 BaseConv 모델이 있다고 가정하자. 그리고 Lemma F.1로 인해 최대 2^{L}\\의 다선형 다항식 \\(P(\\mathbf{u})\\)을 얻을 수 있다. 여기서, \\(L\\leq\\log(2d)\\), \\(L\\) 층으로 생성된 BaseConv는 차수 \\(\\leq 2d\\)의 다중선형 다항식을 생성한다. 이것은 우리가 정확히 MQAR을 나타내는 차수\\(<2d+1\\)의 다중선형다항식을 가질 수 없다는 위의 주장과 모순된다. 결과적으로, 데이터 독립적인 BaseConv 모델은 MQAR을 해결하기 위해 \\(\\geq\\log(2d)\\) 레이어가 필요하다.\n' +
      '\n' +
      '특정 부호화를 갖는 \\(d\\geq\\log_{2}c\\)의 층수에 대한### 하한\n' +
      '\n' +
      '1 Equality Problem\n' +
      '\n' +
      '입력 쌍 \\(\\mathbf{u}_{1},\\mathbf{u}_{2}\\)에 대해, 각 \\(\\mathbf{u}_{i}\\)은 크기 \\(c=|C|\\)의 어휘에서 추출한 토큰으로 \\(\\{0,1\\}^{d}\\)에 임베딩된 입력 쌍에 대해, 두 인코딩이 동일한지 확인하는 것으로 _equality 문제_(EQ)를 정의한다.\n' +
      '\n' +
      '우리는 먼저 MQAR을 해결하는 모든 모델이 다음 제안을 통해 EQ도 해결한다는 점에 주목한다.\n' +
      '\n' +
      '**Proposition F.3**. : MQAR을 해결하는 _Any model \\(M_{\\mathrm{MQAR}}\\)도 동일한 개수의 레이어를 사용하여 EQ를 해결한다._\n' +
      '\n' +
      '\\(L\\) 레이어를 사용하여 MQAR을 해결하는 모델\\(M_{\\mathrm{MQAR}\\)이 존재한다면,\\(\\mathbf{u}_{1},\\mathbf{u}_{2}\\in\\mathbbb{R}^{2\\times d}\\)에 의해 주어진 EQ에 대한 임의의 입력 인스턴스에 대해, 우리는 MQAR에 대한 다음의 입력 인스턴스를 생성할 수 있다.\\(\\mathbf{u}:=\\{(\\mathbf{u}_{1},\\mathbbm{1},\\mathbf{u}_{1}),(\\mathbbm{1},\\mathbm{1},\\mathbm{2})\n' +
      '\n' +
      '명제 F.3으로 인해 다음과 같은 결과를 얻을 수 있다.\n' +
      '\n' +
      '**Corollary F.2**.: _Any lower bound \\(\\overline{L}\\) on the number of layers \\(L\\) of BaseConv to solve EQ는 또한 MQAR._\n' +
      '\n' +
      '우리는 이제 \\(d\\geq\\log_{2}c\\)의 경우에 대한 하한을 증명하려고 한다. 먼저, F.5로부터의 하한이 성립하는 임베딩이 존재한다는 점에 주목한다: 첫 번째 \\(\\log_{2}c\\)이 이전과 같이 작은 이진 임베딩을 갖지만 마지막 \\(d-\\log_{2}c\\) 비트가 모든 토큰에 대해 동일한 임베딩을 고려한다. 우리는 대신 더 흥미로운 임베딩 집합에 대한 하한을 증명할 것이다.\n' +
      '\n' +
      '#### f.5.2 \\(p\\geq 1\\)에 대한 \\(p\\)-Hot Encoding\n' +
      '\n' +
      '*Definition F.7**((Almost) \\(p\\)-Hot Encoding): \\(p\\)_-hot Encoding_을 기본 \\(\\sqrt[p]{c}: (t_{0},..,t_{p-1})\\in[0,\\sqrt[p]{c})^{p}\\)에서 각각 \\(t_{i}\\)을 하나의 Hot Encoding으로 표현할 수 있도록 \\(\\mathbf{x}_{t}\\)에 대한 임베딩의 집합으로 정의한다. 즉, \\(d=p\\cdot\\sqrt[p]{c}\\)를 취한다.\n' +
      '\n' +
      '또한, 원-핫 인코딩의 마지막 비트를 \\(\\{0,1\\}^{\\sqrt[p]{c}}\\)에 드롭하여 얻은 \\(\\{0,1\\}^{\\sqrt[p]{c}-1}\\)에 각 \\(t_{i}\\)이 매핑되는 임베딩의 집합이라고 정의한다.\n' +
      '\n' +
      '두 인코딩 모두 원-핫 인코딩 각각에서 파생된 \\(p\\)-많은 블록을 가지고 있다는 점에 유의한다.\n' +
      '\n' +
      '*Definition F.8**(Block-Exclusive).: \\(\\mathbf{u}:=(\\mathbf{u}_{0},\\ldots,\\mathbf{u}_{p-1})\\)의 변수가 있는 다항식 \\(P\\)은 곱이 주어진 \\(P\\)의 0이 아닌 단일항이면 _block-exclusive_라고 한다.\n' +
      '\n' +
      '\\[\\prod_{i\\in[p],\\ j\\in\\{\\sqrt[p]{c}\\}}\\mathbf{u}_{i,j}\\]\n' +
      '\n' +
      '는 \\(i\\in[p],j,j^{\\prime}\\in[\\sqrt[p]{c]\\)에 대한 \\(\\mathbf{u}_{i,j}\\mathbf{u}_{i,j^{\\prime}\\) 형태의 곱을 포함하지 않는다.\n' +
      '\n' +
      '**Remark F.4**.: Definition F.8에 명시된 조건은 임의의 0이 아닌 단항에서 \\(j=j^{\\prime}\\)에 대한 \\(\\mathbf{u}_{i,j}\\mathbf{u}_{i,j^{\\prime}\\)이라는 용어를 허용하지 않기 때문에 블록 배타적 다항식이 반드시 다중선임을 보장한다.\n' +
      '\n' +
      'Lemma F.3**.:: 거의 \\(p\\)-hot encoding_ 또는 _the \\(p\\)-hot encoding setting으로부터의 입력을 갖는 임의의 부울 함수 \\(f:\\{0,1\\}\\to\\{0,1\\}\\)에 대해, \\(f\\).__ 입력이 거의 \\(p\\)-hot 인코딩 또는 \\(p\\)-hot 인코딩에서 \\(\\mathbf{u}:=(\\mathbf{u}_{0},\\ldots,\\mathbf{u}_{p-1})\\)으로 주어졌을 때, 우리는 먼저 \\(f(\\mathbf{u})\\)을 나타내는 다항식 \\(P(\\mathbf{u})\\)이 동일한 블럭의 변수들과 0이 아닌 단항식을 가질 수 없다는 것을 관찰한다. 구체적으로, \\(0\\leq j<p\\)의 경우, \\(P\\)의 0이 아닌 단항식은 \\(k\\neq k^{\\prime}\\)에 대한 \\(\\mathbf{u}_{j,k}\\mathbf{u}_{j,k^{\\prime}\\) 형태의 곱을 가질 수 없다. 이를 위해, 동일한 \\(\\mathbf{u}_{j,k}\\mathbf{u}_{j,k^{\\prime}\\)에서 두 항 이상의 논-제로 단항이 존재한다고 가정하면, 단항은 항상 \\(0\\)으로 평가되며, \\(0,1\\}^{\\sqrt[p]{c}\\)에서 원-핫 인코딩 또는 \\(\\{0,1\\}^{\\sqrt[p]{c}-1}\\)에서 거의 원-핫 인코딩으로 유도되므로 한 비트 이상의 비트를 가질 수 없다.\n' +
      '\n' +
      '다음으로, \\(P\\)에서 0이 아닌 단항이 \\(k,k^{\\prime}\\in[\\sqrt[p]{c}]\\에 대한 \\(\\mathbf{u}_{j,k}\\mathbf{u}_{j,k^{\\prime}\\) 형태의 곱을 포함한다면, 우리는 다항식을 정의할 수 있다.\n' +
      '\n' +
      '\\[Q(\\mathbf{u}):=(\\cdots((P(\\mathbf{u})\\mod(u_{0,0}^{2}-u_{0,0}))\\mod(u_{0,1}^{2}-u_{0,1 }))\\cdots)\\mod(u_{p-1,\\sqrt[p]{c}-1}^{2}-u_{p-1,\\sqrt[p]{c}-1}).\\]\n' +
      '\n' +
      '각 엔트리가 부울이므로 \\(Q\\)은 부울 입력보다 \\(P\\)과 같고, 따라서 \\(Q\\)은 \\(f\\)과 동등한 블록 배타 다항식이다.\n' +
      '\n' +
      '**Proposition F.4**.: _Any Boolean function \\(f:\\{0,1\\}\\to\\{0,1\\}\\) with inputs from almost \\(p\\)-hot encoding setting is a unique representation as a block-exclusive polynomial.__\n' +
      '\n' +
      '[83, 명제 4]로 인해 모든 부울함수 \\(f\\)는 다중선형 다항식으로 표현됨을 알 수 있다. 또한, Lemma F.3으로부터 \\(f(\\mathbf{u})\\)을 나타내는 다항식 \\(P(\\mathbf{u})\\)이 거의 \\(p\\)-핫 인코딩으로 \\(\\mathbf{u}\\)에 대해 블록 배타적이라는 것을 알 수 있다.\n' +
      '\n' +
      '독특성을 보이기 위해 [83, 강의 3, 명제 4]에서 논증을 재현한다. 두 개의 블록 배타적 다항식 \\(P\\)과 \\(f\\)에 해당하는 \\(P^{\\prime}\\)을 주어 거의 \\(p\\)-핫 인코딩의 입력으로 \\((P-P^{\\prime})(\\mathbf{u})\\equiv 0\\). 이제, 모순을 위해, \\(P-P^{\\prime}\\not\\equiv 0\\)이라고 가정하자. 여기서 유의할 점은 \\(P-P^{\\prime}\\)은 동일하지 않고 0이 아닌 단항식을 가지며, 입력들은 거의 \\(p\\)-hot 인코딩에서 나오기 때문에 이 단항식은 \\(\\mathbf{u}_{j,k}\\mathbf{u}_{j,k^{\\prime}\\) 형태의 곱을 포함할 수 없다는 것이다. [S\\subseteq[p]\\times[\\sqrt[p]{c}-1]\\(S\\prod_{(j,k)\\in S\\mathbf{u}_{j,k}\\)이 0이 아닌 계수인 \\(P-P^{\\prime}\\)에 나타나도록 최소 지수 집합으로 하자. \\(\\chi_{S}\\)은 \\(S\\)의 각 블록이 최대 하나의 영이 아닌 엔트리로 할당될 수 있기 때문에 \\(f\\)에 유효한 입력을 형성한다는 점에 유의한다. 그런 다음, 다른 모든 단항식으로 \\((P-P^{\\prime})(\\chi_{S})\\neq 0\\)이 \\(\\chi_{S}\\)에 대해 \\(0\\)에 할당되는 적어도 하나의 변수를 얻을 것이기 때문에, 우리는 모순을 달성하며, 따라서 \\(P-P^{\\prime}\\)은 거의 \\(p\\)-핫 인코딩의 입력에서 동일하게 0이어야 한다.\n' +
      '\n' +
      '**Lemma F.4**.:: 거의 \\(p\\)-핫 인코딩 설정에서의 EQ 문제는 차수 \\(2p\\)의 블록 배타 다항식으로 표현된다._\n' +
      '\n' +
      'Proof.: EQ 문제에 대한 각 입력 쌍 \\(\\mathbf{u}^{1},\\mathbf{u}^{2}\\)은 \\(\\mathbf{u}^{i}:=(\\mathbf{u}^{i}_{0},\\ldots,\\mathbf{u}^{i}_{p-1})\\)에 대한 \\(i\\in\\{1,2\\})으로 나타낼 수 있으며, 여기서 각 \\(0<j<p\\)에 대한 EQ 문제에 대한 각 입력 쌍 \\(\\mathbf{u}^{i}}^{1},\\mathbf{u}^{u}^{2}})은 \\(\\mathbf{u}^{i}_{i}:=(\\mathbf{u}^{i}_{0},\\ldots,\\mathbf{u}^{i}_{p-1})으로 나타낼 수 있다.\n' +
      '\n' +
      '\\[\\mathbf{u}^{i}_{j}:=(\\mathbf{u}^{i}_{j,0},\\ldots,\\mathbf{u}^{i}_{j,\\sqrt[p]{c}-2})\\in\\{0, 1\\}^{\\sqrt[p]{c}-1}.\\]\n' +
      '\n' +
      '다음 다항식은 이러한 원-핫 인코딩들 각각의 내적 곱을 취한다:\n' +
      '\n' +
      '(\\mathbf{u}):=\\sum_{k=0}^{\\sqrt[p]{c}-2}\\mathbf{u}^{1}_{j,k}\\cdot\\mathbf{u}^{2}_{j,k} +(1-\\sum_{k=0}^{\\sqrt[p]{c}-2}\\mathbf{u}^{1}_{j,k})(1-\\sum_{k=0}^{\\sqrt[p]{c}-2}\\mathbf{u}^{2}_{j,k}}\\mathbf{u}^{u}^{j,k}}\\mathbf{u}^{u}^{2}_{j,k}}+(1-\\sum_{k=0}^{\\sqrt[p]{c}-2}}\\mathbf{u}^{c}-2}}\\mathbf{u}^{u}^{j,\n' +
      '\n' +
      'for \\(0<j<p\\). 여기서, \\(\\mathbf{u}^{1}_{j}\\)와 \\(\\mathbf{u}^{2}_{j}\\)는 모두 기껏해야 \\(1\\)일 수 있으며, 따라서 \\(j\\)번째 블록이 일치하면 \\(P^{j}(\\mathbf{u})=1\\일 수 있다.\n' +
      '\n' +
      '다음, 다음 다항식은 EQ 문제를 해결하는 부울 함수와 같다:\n' +
      '\n' +
      '\\[P(\\mathbf{u}):=\\prod_{j=0}^{p-1}P^{j}(\\mathbf{u}),\\]\n' +
      '\n' +
      '그리고 우리는 \\(P(\\mathbf{u})=1\\{\\mathbf{u}^{1}\\equiv\\mathbf{u}^{2}\\}\\을 갖는다. 여기서 유의할 점은 \\(P\\)은 다중선형이며 각 \\(P^{j}\\)은 차수-\\(2\\) 다항식이므로 차수\\(2p\\)을 갖는다. 또한, 각 \\(P^{j}\\)이 블록 배타적이므로 \\(P\\)은 블록 배타적이며, 우리는 \\(P\\)에서 서로 다른 블록으로부터 단수를 곱할 뿐이다.\n' +
      '\n' +
      '**Proposition F.5**.: _\\(P\\)을 \\(p\\)-hot 인코딩에서 EQ 문제를 해결하는 블록 배타적 다항식이 되도록 하자. 그런 다음 \\(\\deg(P)\\geq 2p\\)._\n' +
      '\n' +
      '증명: 모순을 위해 차수\\(\\leq 2p-1\\)를 갖는 \\(p\\)-핫 인코딩 설정에서 EQ를 해결하는 블록 배타적 다항식 \\(P\\)이 존재한다고 가정한다. 그런 다음, 거의 \\(p\\)-핫 인코딩으로부터 입력 \\(\\mathbf{u}:=(\\mathbf{u}_{0},\\ldots,\\mathbf{u}_{p-1}))이 주어지면, 각 블록 \\(\\mathbf{u}_{i}\\)은 \\(0,1\\}^{\\sqrt[p]{c}-1}\\)에서 원-핫 인코딩으로부터 절단된 비트열에 해당하며, 이 입력을 \\(p\\)-핫 인코딩 \\(\\mathbf{v}:=(\\mathbf{v}_{0},\\ldots,\\mathbf{v}_{p-1})으로 변환할 수 있다.\n' +
      '\n' +
      '\\[\\mathbf{v}_{i}:=\\left(\\mathbf{u}_{i,0},\\ldots,\\mathbf{u}_{i,\\sqrt[p]{c}-2},1-\\sum_{j=0}^{ \\sqrt[p]{c}-2}\\mathbf{u}_{i,j}\\right)\\]\n' +
      '\n' +
      '그리고 블록별 다중선형 다항식 \\(Q(\\mathbf{u})=P(\\mathbf{v})\\)는 거의 하나의 핫 인코딩 설정에서 EQ 문제를 해결하며, 명제 F.4와 Lemma F.4의 조합에 모순되는 \\(\\deg(Q)\\leq\\deg(P)\\leq 2p-1\\을 갖는다.\n' +
      '\n' +
      'Theorem F.6**: _A data-independent BaseConv model need least \\(\\lfloor\\log(2p)\\rfloor\\)-layer for solve MQAR for the input sequence \\(\\mathbf{u}\\in\\{0,1\\}^{3N\\times d}\\) in \\(p\\-hot encoding setting, where \\(d=p\\cdot\\sqrt[p]{c}\\)._\n' +
      '\n' +
      '증거: 우리는 Corollary F.2로부터 그것이 EQ 문제에 대한 하한을 나타내기에 충분하다는 것을 안다. 또한, 제안 F.5를 통해 우리는 차수\\(\\leq 2p-1\\)의 블록 배타 다항식으로 \\(p\\)-핫 인코딩 설정에서 EQ 문제를 해결할 수 없음을 알 수 있다. 이제, \\(p\\)-핫 인코딩 설정에서 EQ를 정확하게 해결하는 \\(L\\) 레이어가 있는 BaseConv 모델이 있다고 가정하자. 그리고 Lemma F.1과 Proposition F.4로 인해 블록 배타적 다항식 \\(P(\\mathbf{u})\\)의 차수가 최대 2^{L}\\이 된다. 여기서, 층수가 \\(L<\\lfloor\\log(2p)\\rfloor\\)일 경우, 층수가 \\(L\\)인 BaseConv는 차수 \\(\\leq 2p-1\\)의 블록 배타적 다항식이 된다. 이것은 우리가 정확히 EQ를 나타내는 차수 \\(<2p\\)의 블록 배타적 다항식을 가질 수 없다는 위의 주장과 모순된다. 결과적으로, 데이터 독립적인 BaseConv 모델은 EQ를 해결하기 위해 \\(\\geq\\lfloor\\log(2p)\\rfloor\\) 층이 필요하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline \\hline  & 355M & 1.4B \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.95\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 27 & 36 \\\\ Hidden Size & 1024 & 1792 \\\\ MLP Activation & SwiGLU \\\\ MLP Width & 2 \\\\ \\hline Num. Linear Attn Layers & 5 & 7 \\\\ Num. Linear Attn Heads & 16 \\\\ Taylor Feature Dimension & 16 \\\\ Linear Attn Positional Encodings & None \\\\ \\hline Num. Sliding Window Layers & 5 & 7 \\\\ Sliding Window Size & 64 & 16 \\\\ Sliding Window Heads & 16 \\\\ Sliding Window Positional Encodings & Rotary \\\\ \\hline Num. BaseConv Layers & 17 & 22 \\\\ BaseConv Projection Expansion Factor & 4 \\\\ BaseConv Filter Size & 3 \\\\ BaseConv Activation & SiLU \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 기반 훈련 설정\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline  & 355M & 1.4B \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.95\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 24 & 36 \\\\ Hidden Size & 1024 & 1680 \\\\ Num Heads & 16 & 24 \\\\ RMSNorm & True \\\\ MLP Bias & False \\\\ Flash Attn & True \\\\ Rotary Emb. Fraction & 0.5 \\\\ MLP Activation & SwiGLU \\\\ MLP Width & 4 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 주의력 훈련 설정\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline  & 355M & 1.4B \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.95\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 46 \\\\ Hidden Size & 1024 & 2048 \\\\ RMSNorm & True \\\\ Norm Epsilon & \\(1e-5\\) \\\\ Dt State & 16 \\\\ Dt (Min, Max) & \\((0.001,0.1)\\) \\\\ Dt Init. Strategy & Random \\\\ Dt Init. Floor & \\(1e-4\\) \\\\ Dt Scale & 1.0 \\\\ Dt Softplus & True \\\\ Projection Expansion Factor & 2 \\\\ Short Conv Filter Size & 4 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 맘바 트레이닝 설정\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline  & 355M \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.95\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 29 \\\\ Hidden Size & 1024 \\\\ Num Heads & 1 \\\\ MLP Width & 2 \\\\ Short Conv. Filter Size & 3 \\\\ Exp. Mod. Decay (Fast, Slow) & 0.3, 1.2 \\\\ Filter Sine Freq. (w) & 14 \\\\ Filter Order & 64 \\\\ Filter Inner MLP & 2 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 하이에나 트레이닝 설정\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline  & 355M \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.99\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 24 (No Attention Layers) \\\\ Hidden Size & 1024 \\\\ Num Heads & 16 \\\\ MLP Width & 4 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: 하이에나 훈련 설정\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline \\hline  & 355M \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.95\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 24 \\\\ Hidden Size & 1024 \\\\ Num Heads & 4 \\\\ MLP Width & 2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: GLA(Gated Linear Attention) 트레이닝 설정\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline \\hline  & 355M \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.99\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 24 \\\\ Hidden Size & 1024 \\\\ Num Heads & 4 \\\\ MLP Width & 2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 12: 하이에나 트레이닝 설정\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>