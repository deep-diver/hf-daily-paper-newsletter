<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Simple linear attention language models balance the recall-throughput tradeoff\n' +
      '\n' +
      'Simran Arora\n' +
      '\n' +
      'Corresponding authors; equal contribution and random ordering for SA, SE, MZ (SSM).\n' +
      '\n' +
      'Sabri Eyuboglu\n' +
      '\n' +
      'Corresponding authors; equal contribution and random ordering for SA, SE, MZ (SSM).\n' +
      '\n' +
      'Michael Zhang\n' +
      '\n' +
      'Corresponding authors; equal contribution and random ordering for SA, SE, MZ (SSM).\n' +
      '\n' +
      'Aman Timalsina\n' +
      '\n' +
      'University at Buffalo\n' +
      '\n' +
      'Silas Alberti\n' +
      '\n' +
      'Pardue University\n' +
      '\n' +
      '{simran,eyuboglu,mzhang,alberti,jamesz,chrismre}@cs.stanford.edu\n' +
      '\n' +
      'Dylan Zinsley\n' +
      '\n' +
      'Stanford University\n' +
      '\n' +
      'James Zou\n' +
      '\n' +
      'Pardue University\n' +
      '\n' +
      '{simran,eyuboglu,mzhang,alberti,jamesz,chrismre}@cs.stanford.edu\n' +
      '\n' +
      'Atri Rudra\n' +
      '\n' +
      'Pardue University\n' +
      '\n' +
      'Christopher Re\n' +
      '\n' +
      '{qylanxin,atri}@buffalo.edu\n' +
      '\n' +
      'Christopher Re\n' +
      '\n' +
      '{atimalsi}@purdue.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Recent work has shown that attention-based language models excel at _recall_, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache\'s aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (_e.g._ by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model\'s _state size_ and recall ability. We show that efficient alternatives to attention (_e.g._ H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose Based a simple architecture combining linear and sliding window attention. By varying Based window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that Based matches the strongest sub-quadratic models (_e.g._ Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make Based competitive, we develop IO-aware algorithms that enable 24\\(\\times\\) higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'The choice of sequence mixer (_e.g._ attention, convolution) in a language model affects both its quality and efficiency [1, 2]. Prior work shows that attention excels at _recall_, the ability to ground generations in previously seen tokens [1, 3]. On the other hand, the throughput of attention-based models is bottle-necked during training by quadratic compute complexity and during inference by aggressive memory consumption. The natural question is: _can we improve the real-world speed and memory-use of language models without comprising on quality?_\n' +
      '\n' +
      'Recently, a number of architectures have been proposed that enable substantially higher throughput while matching attention in perplexity [4, 5, 6, 7, 8]. However, coarse metrics like overall perplexity can obscure important differences in model quality. For example, recent work shows that a specific class of architectures, _gated-convolutions_, despite complexity scaling sub-quadratically in sequence length, are less efficient thanattention at performing recall [1]. Building on this analysis, we evaluate a broader class of architectures across real-world recall-intensive tasks and show attention improves over the best attention-free alternative, Mampa, by 46.7 accuracy points (Table 1). 1\n' +
      '\n' +
      'Footnote 1: Examples of recall-intensive tasks include information extraction, reading comprehension, and code generation using custom variable and function names. These require using in context (contrasting memorized) information during generation.\n' +
      '\n' +
      'Motivated by these observations, we explore the pareto frontier of the tradeoff between high-recall and high-throughput models. We evaluate a range of architectures (_e.g._ attention, SSMs, and convolutions) on a popular synthetic _associative recall_ task [1, 3, 9]. Since generation throughput is bottle-necked by memory consumption, we vary hyperparameters (e.g. model dimension) that affect the size of the _recurrent_ state during generation and demonstrate a fundamental recall-memory tradeoff that holds across architecture classes (Figure 2). Attention performs associative recall perfectly, but the recurrent state (_i.e._ the KV-cache) grows linearly with the sequence length. Sliding window attention can cap the size of the recurrent state at the cost of worse long-range recall [10]. However, Mampa, a recently proposed SSM architecture expands the Pareto frontier beyond sliding window. This begs the question: _are there other, perhaps simpler, models that can also expand the pareto frontier?_\n' +
      '\n' +
      'To reduce the memory consumption, we consider using two simple techniques: sliding window attention and softmax-approximating linear attention. Our results on language modeling (Table 1) and synthetic recall experiments (Figure 1, center) suggest neither primitive alone suffices to navigate the Pareto frontier.\n' +
      '\n' +
      '1. We find that _linear attention_ alone struggles to solve associative recall (Figure 1, center). We hypothesize that this is because linear attention lacks the precision to perform local token shifts and comparisons [1, 9].\n' +
      '2. In _sliding window attention_, associative recall range is limited by the width of the windows (Figure 1, center). As we increase the window size, the recurrent state grows linearly and has a non-linear affect on speed during parallel training and inference (Figure 1, left).\n' +
      '\n' +
      'We combine these two techniques into a single architecture, which we call Based (Figure 1, right). We find that sliding window attention and linear attention complement each other, enabling Based to expand the pareto frontier of the recall-memory tradeoff (Figure 2). We suspect that (1) the large recurrent memory of linear attention could help model long-range token interactions in the sequence and (2) sliding window attention handles the precise local shifts needed to perform associative recall.\n' +
      '\n' +
      'To make Based competitive with SoTA attention [11] and recurrent [5] models under wall-clock and throughput metrics, we introduce several IO-aware optimizations.\n' +
      '\n' +
      '1. Despite the theoretically improved complexity, _linear attention_ implementations are often _slower_ than well-optimized attention implementations [12]. In Based, we use the 2nd-order Taylor approximation of softmax as the linear attention feature map With sequence length \\(N\\) and head dimension \\(d\\), this\n' +
      '\n' +
      'Figure 1: **Based overview. Combining linear attention with _tiny_ sliding window softmax attention (e.g., 64 or 128 tokens in width) enables improved recall accuracy with limited efficiency overhead vs. smaller tile sizes. (_Left_) Time to execute Cutlass GEMMs (\\(y\\)) vs. sliding window attention size (\\(x\\)), with batch size 512 on tensor cores. (_Center_) Model recall accuracy (\\(y\\)) vs. sliding window attention size (\\(x\\)). We compare linear attention alone (dark blue), sliding window attention alone (light blue), and their combination (Based, orange). (_Right_) Schematic diagram of Based illustrating how the two components complement each other.**\n' +
      '\n' +
      'naively requires \\(\\mathcal{O}(Nd^{3})\\) time and space complexity [13, 14]. To make our attention competitive in real-world wall-clock time and memory usage, we provide hardware-efficient algorithms and custom CUDA implementations. Relative to the baseline, our algorithm reduces data movement from HBM (slower-to-access memory) to SRAM (faster-to-access memory) by \\(\\mathcal{O}(Nd^{2})\\) bytes and from SRAM to register (fastest memory) by \\(O(Nd^{3})\\) bytes (Section 5).\n' +
      '2. _Sliding window attention_ exploits tensor cores, specialized units on modern GPUs for performing matrix multiplications (GEMMs). While popular architectures use long window sizes (e.g. 4096 for Mistral-7B [10]), we choose fixed size 64 windows, guided by hardware properties. In particular, we use just enough occupancy to hide the tensor core kernel launch latency. Although tensor cores operate on \\(16\\times 16\\) tiles, in Figure 1 (left), we see that the latencies for performing \\(16\\times 16\\) vs. \\(64\\times 64\\) (or even \\(128\\times 128\\)) dimension matrix multiplications on NVIDIA H100 tensor cores are similar, informing our window size.\n' +
      '\n' +
      'In experiments, we show that Based competes in quality with strong Transformer++ [15] and SoTA sub-quadratic baselines in models up to the 1.3Bn parameters across language modeling on the Pile language, DNA modeling, and the LM Eval Harness [16]. Beyond this, Based outperforms prior sub-quadratic architectures on the associative recall slice of the Pile and in downstream recall-intensive tasks by 0.14 perplexity points and 6.22 accuracy points, respectively. In efficiency, Based enables up to 24\\(\\times\\) higher throughput than the strong FlashAttention-2 implementation on generation. Code for this work is provided at: [https://github.com/HazyResearch/based](https://github.com/HazyResearch/based).\n' +
      '\n' +
      '## 2 Preliminaries and Related Work\n' +
      '\n' +
      'We discuss the key relevant work in this section and provide an extended discussion in Appendix A.\n' +
      '\n' +
      'AttentionPopularized by Transformers [2] as the _de facto_ language modeling primitive, softmax attention takes inputs \\(\\mathbf{x}\\in\\mathbb{R}^{N\\times d}\\) of length \\(N\\) and head dimension \\(d\\), and computes outputs \\(\\mathbf{y}\\in\\mathbb{R}^{N\\times d}\\) via the softmax over projections \\(\\mathbf{q},\\mathbf{k},\\mathbf{v}=\\mathbf{x}\\mathbf{W}_{q},\\mathbf{x}\\mathbf{W}_{k},\\mathbf{x}\\mathbf{W}_{v}\\), _i.e._,\n' +
      '\n' +
      '\\[\\mathbf{y}_{i}=\\sum_{j=1}^{i}\\frac{\\exp(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}/\\sqrt{d})\\mathbf{ v}_{j}}{\\sum_{m=1}^{i}\\exp(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{m}/\\sqrt{d})} \\tag{1}\\]\n' +
      '\n' +
      'in the causal case where \\(\\mathbf{W}_{q},\\mathbf{W}_{k},\\mathbf{W}_{v}\\in\\mathbb{R}^{d\\times d}\\) are learnable matrices. While effective at recall [1] and efficient to train (Eq 1 is parallelizable on GPUs and \\(\\mathcal{O}(N)\\) in memory with recent advances [12]), attention remains expensive for generation. For every new output \\(\\mathbf{y}_{n}\\), we require \\(nd\\) operations over a growing _\\(K\\)V-cache_ of prior \\(\\{\\mathbf{k}_{i},\\mathbf{v}_{i}\\}_{i=1}^{n-1}\\). This results in larger memory consumption and lower-throughput for longer sequences.\n' +
      '\n' +
      'Efficient attentionsVarious works thus try to improve on attention\'s efficiency without sacrificing quality. _Sparse attentions_ reduce attention\'s time and memory requirements by only attending over specific strided patterns or local _sliding windows_[17, 18, 19]. While further popularized in large language models (Mistral, Jiang et al. [10]), prior works either underperform full attention with sparse patterns that fail to capture dense interactions, or use large window sizes that still permit large KV-caches and subsequent inefficiency.\n' +
      '\n' +
      'Meanwhile, _linear attentions_ replace the softmax in standard attention with alternative kernel functions [14, 20, 21, 22, 23]. By removing the \\(\\exp(\\mathbf{q}^{\\top}\\mathbf{k})\\) in favor of feature map dot-products \\(\\phi(\\mathbf{q})^{\\top}\\phi(\\mathbf{k})\\), these methods use matrix product associativity to compute attention in \\(\\mathcal{O}(Nd^{2})\\) time and space [24]. Furthermore, they permit a _recurrent view_ for constant memory and \\(\\mathcal{O}(1)\\) time per-token generation [25, 26]. However, present linear attention feature maps either fail to match standard attention on recall or remain expensive to compute [13]. Linear attentions also do not achieve faster wall-clock time or lower memory versus modern standard attention in practice [12].\n' +
      '\n' +
      'Attention alternativesFinally, various models use attention-free sequence mixers such as state-space models (SSMs) [27, 28], gated convolutions [7, 9] and input-dependent recurrences [5, 8] to rival attention performance while improving its efficiency. However, while recent such models can match attention in overall perplexity, further study suggests they may underperform Transformers on tasks such as recall and in-context learning [1, 29].\n' +
      '\n' +
      'No Free Lunch: The Memory-Recall Tradeoff\n' +
      '\n' +
      'In this section, we demonstrate a fundamental tradeoff between a model\'s memory consumption during inference (_i.e.,_ the size of its recurrent state) and its capacity to perform recall. We use a combination of experiments on synthetic data and theoretical analysis.\n' +
      '\n' +
      '* **Empirical study of memory-recall tradeoff :** In Section 3.1, we evaluate a number of popular architecture classes (_e.g._ Mamba, Hyena) on a synthetic associative recall task, varying hyperparameters that affect the model\'s recurrent state size (Figure 2). Within each architecture class, we observe a clear tradeoff: the larger the recurrent state size, the better recall. However, for a fixed recurrent state size, performance is not consistent across architectures. We observe that some sequence mixers fall well-below the pareto-frontier. This motivates the design of sequence mixers that can expand the pareto frontier.\n' +
      '* **Lower bounds on memory required for recall**: In Section 3.2, we lower bound the recurrent state size required to perform exact recall with _any_ recurrent model Theorem F.1. This analysis reinforces our empirical observations on the throughput-recall tradeoff.\n' +
      '\n' +
      '### Empirical study of memory-recall tradeoff\n' +
      '\n' +
      '**Setup.** We use a synthetic AR task called Multi-Query Associative Recall (MQAR) [1] to demonstrate the tradeoff. In this task, input sequences consist of a number of key-value pairs followed by queries. For a given query, the model must recall the corresponding key-value pair from earlier in the sequence in order to predict the next token. For example, the correct output for input below would be 4, 6, 1, 2, 3:\n' +
      '\n' +
      '\\[\\text{A 4 B 3 C 6 \\begin{subarray}{c}\\text{F 1 E 2}\\rightarrow\\text{A? C? F 2 E? B? \\\\ \\text{Key-Value}\\end{subarray}}}\\]\n' +
      '\n' +
      'We train on sequences of length 256 tokens containing between 4 and 64 key-value pairs. During evaluation, we measure accuracy on sequences of length 1,024 tokens containing between 4 and 256 key-value pairs.\n' +
      '\n' +
      'We train and evaluate six sequence mixers: attention [2], sliding window attention [19], Mamba [5], H3 [9], Hyena [7], and Based. For each, we vary hyperparameters that affect the memory consumption during inference. For example, in sliding window attention we vary the window width. We compare how MQAR accuracy varies with the size of the recurrent state. Appendix E.1 contains details on how the state size is calculated for each architecture.\n' +
      '\n' +
      'Figures 2 and 3 can be reproduced or extended to new architectures using the scripts provided at [https://github.com/HazyResearch/zoology](https://github.com/HazyResearch/zoology).\n' +
      '\n' +
      'ResultsIn Figure 2, we demonstrate a fundamental tradeoff between recurrent state size and accuracy on MQAR that holds within and across architecture classes. Within each architecture class (_e.g._ H3 models), increasing the recurrent state size almost always leads to an improvement in accuracy. Across architecture classes, we see a tradeoff as well. Attention achieves perfect recall accuracy, but its recurrent state size grows with the length of the sequence. Other architecture classes like Mamba and H3 admit models with much smaller recurrent states, but these models have limited recall capacity.\n' +
      '\n' +
      'Figure 2: **Throughput (memory) - recall tradeoff.**\\(x\\)-axis shows state size (bytes) during generation; \\(y\\)-axis shows accuracy on the MQAR recall task [1]. For each architecture, we train several models varying hyperparameters that affect the recurrent state size (_e.g._ model dimension). The plot shows a fundamental tradeoff between the recurrent state size and recall capacity that applies to broad class of models [1, 5, 9].\n' +
      '\n' +
      'Given a fixed recurrent state, not all architectures have the same recall capacity. Among architectures proposed in prior work, Mambo makes the best use of a limited memory budget. Notably, architectures with a convolutional view (_e.g._ Hyena and H3) fall well below the pareto frontier. Our proposed architecture, Based (introduced in Section 4), expands the pareto-frontier beyond Mambo. By varying hyper-parameters that determine its state size (_e.g._ feature dimension and model dimension), we can smoothly navigate the tradeoff between efficient models and memory-hungry models with high recall capacity.\n' +
      '\n' +
      '### Theoretical Analysis\n' +
      '\n' +
      'Our theoretical analysis provides further insight into the empirical observations described above. First, using results from communication complexity theory, we show that the recall capacity of _any_ causal model (_e.g._ Mambo, Attention) is bounded by the size of its recurrent state (Theorem F.3 in Appendix F).\n' +
      '\n' +
      '**Theorem 3.1**.: _Any recurrent model2 depending causally on input \\(\\mathbf{u}\\in\\{0,1\\}^{N\\times d}\\) requires \\(\\Omega(N)\\)-bits3 in state size to solve MQAR._\n' +
      '\n' +
      'Footnote 2: In particular, for Mambo [5], see Corollary F.1.\n' +
      '\n' +
      'Footnote 3: Here, we need the entries of the state to be bounded.\n' +
      '\n' +
      'This result suggests that the tradeoff observed in Figure 2 is fundamental, not an artifact of architectural quirks.\n' +
      '\n' +
      'Next, we focus on _gated-convolutions_, a broad class of architectures built from gating and convolutions (_e.g._ H3, Hyena, RWKV v4). To make progress in theoretically analyzing the broad set of gated convolution proposals, prior work develops a _canonical_ gated-convolution, referred to as BaseConv which can provably simulate _any_ architecture built from gating and convolution primitives.\n' +
      '\n' +
      'Building on this work, we show that BaseConv cannot solve MQAR in constant-many layers (Theorem F.5 and Theorem F.6 in Appendix F).\n' +
      '\n' +
      '**Theorem 3.2**.: _Given an input sequence \\(\\mathbf{u}\\in\\{0,1\\}^{3N\\times d}\\), where \\(N\\) and \\(d\\) denote the sequence length and head dimension, respectively, a data-independent BaseConv model needs \\(\\log(2d)\\)-layers to solve MQAR for \\(d=\\log_{2}(c)\\), where \\(c\\) denotes the vocabulary size4._\n' +
      '\n' +
      'Footnote 4: That is, each token from the vocabulary has the natural binary encoding in \\(\\{0,1\\}^{\\log_{2}(c)}\\)\n' +
      '\n' +
      'In contrast, Arora et al. [1] show that attention solves MQAR in constant-many layers. This result helps to explain why the gated-convolution architectures (H3 and Hyena) in Figure 2 lie below the pareto frontier established by newer architectures.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Architecture} & \\multirow{2}{*}{Params} & \\multicolumn{2}{c}{**Efficiency**} & \\multicolumn{2}{c}{**Language Modeling (Pile)**} & \\multicolumn{2}{c}{**Info. Extraction**} & \\multicolumn{2}{c}{**Question Answering**} \\\\  & & Prefill & Generate & All & AR & Other & SWDE & FDA & SQUAD & Common \\\\  & & Tok./ms \\(\\uparrow\\) & Tok./ms \\(\\uparrow\\) & Ppl. \\(\\downarrow\\) & Ppl. \\(\\downarrow\\) & Ppl. \\(\\downarrow\\) & Acc \\(\\uparrow\\) & P\\(\\uparrow\\) & Avg. Acc. \\(\\uparrow\\) & P\\(\\uparrow\\) & Avg. Acc. \\(\\uparrow\\) \\\\ \\hline \\hline Transformer++ & 1.33b & 103.50 & 0.99 & **7.26** & **1.74** & **8.10** & **41.97** & **73.23** & **36.19** & **47.64** \\\\ Based & 1.35b & **161.71** & 24.28 & 7.43 & 1.87 & 8.26 & 30.83 & 24.41 & 30.46 & 46.68 \\\\ Mambo & 1.32b & 112.22 & **25.69** & 7.48 & 1.96 & 8.29 & 25.93 & 12.89 & 28.20 & 46.84 \\\\ \\hline Transformer++ & 360m & 207.77 & 23.82 & **8.39** & **1.87** & **9.42** & **37.62** & **58.00** & **27.18** & **44.08** \\\\ Based & 363m & **514.57** & **47.23** & 8.65 & 2.07 & 9.64 & 22.81 & 11.71 & 25.07 & 43.03 \\\\ Mambo & 358m & 267.09 & 39.95 & 8.64 & 2.21 & 9.59 & 25.61 & 6.53 & 24.06 & 43.51 \\\\ GLA & 362m & — & — & 9.12 & 2.36 & 10.68 & — & — & — & — \\\\ RWKV v5 & 362m & — & — & 9.79 & 2.40 & 10.90 & — & — & — & — \\\\ H3 & 362m & — & — & 10.60 & 4.88 & 11.23 & 17.59 & 0.64 & 7.87 & 39.35 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Evaluation of pre-trained language models. All models were trained on the same set of 10 billion tokens drawn from the Pile [30]. We report inference throughput on \\(4,096\\) tokens (\\(16,384\\) for \\(360\\)m param.) of pre-fill and \\(2,048\\) tokens of recurrent generation for a subset of architectures. We report language model perplexity on the overall Pile test set as well as perplexity on two slices of the test set: associative recall tokens and other tokens (see Section 6.1, [1]). We report zero-shot performance on three _recall-intensive_ tasks: information retrieval on SWDE and FDA as well as question answering on SQUAD. Finally, we report average performance on the set of _LM Eval Harness_[16] common sense reasoning tasks used in Gu and Dao [5], details in Appendix D. These tasks do not require significant recall capacity because the input text is typically very short. See Section 6.1. Some proposed architectures that do not implement recurrent views for generation are marked with a —.\n' +
      '\n' +
      '**Remark 3.1**.: For a class of input encodings that generalizes one-hot encodings termed as \\(p\\)-hot encodings (Definition F.7), input-dependent BaseConv needs at least \\(\\lfloor\\log(2p)\\rfloor\\)-layers to solve MQAR where \\(d=p\\cdot\\sqrt[p]{c}\\).\n' +
      '\n' +
      'Finally, we show that we can simulate linear attention [20], the foundation of Based, using BaseConv[1] with a poly-log blowup in the number of layers (Proposition F.1 in Appendix F), pointing to the relative efficiency of linear attention over gated-convolution architectures.\n' +
      '\n' +
      '## 4 The Based Architecture\n' +
      '\n' +
      'In this section, we introduce Based. Our objective in designing this architecture is to demonstrate how we can navigate the pareto-frontier of the memory-recall tradeoff using well-known architectural building blocks.\n' +
      '\n' +
      'Softmax attention excels at recall, but since its recurrent state, the KV-cache, grows unconstrained with the length of sequence, it is stuck in the upper right quadrant of Figure 2. We study two simple approaches for constraining the size of attention\'s recurrent state: linear attention and sliding window attention. The recurrent state size of linear attention (_i.e._ attention without softmax) does not grow with the sequence length and can be modulated by changing simple hyperparameters [20]. With sliding window attention, we cap the recurrent state size to be the width of the window.\n' +
      '\n' +
      'However, our experiments on real-world language modeling (Table 4) and synthetic associative recall (Figure 1 middle) suggest that neither primitive alone suffices to navigate the pareto frontier. Linear attention lacks the precision to perform local token shifts and comparisons [1, 31]. In sliding window attention, associative recall range is limited by the width of the windows (Figure 2, center). As we increase the window size, the recurrent state grows linearly and has a non-linear effect on speed during parallel training and inference (Figure 2, left).\n' +
      '\n' +
      'Based models simply combine (1) softmax-approximating linear attention applied globally and (2) exact softmax attention applied locally in small sliding windows (Figure 1, right). This combination allows us to use softmax attention in surprisingly small sliding windows (_e.g.,_ 64 tokens) that recover 90.8% of full softmax attention\'s recall accuracy at 1e-5\\(\\times\\) its latency. Sliding window and linear attention _alone_ fail (Figure 1, left).\n' +
      '\n' +
      '### Taylor Linear Attention\n' +
      '\n' +
      'By approximating softmax attention using linear feature maps, we can constrain the size of the recurrent state while maintaining global token interactions (_i.e._ each token depends on every token before it in the sequence).\n' +
      '\n' +
      'Katharopoulos et al. [20], Choromanski et al. [21], Tsai et al. [32] show that we can select a feature map \\(\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{\\tilde{d}}\\) such that \\(\\phi(\\mathbf{q}_{i})^{\\top}\\phi(\\mathbf{k}_{j})\\approx\\exp(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j }/\\sqrt{d})\\). We can then rewrite the formula for softmax attention in Equation (1) as\n' +
      '\n' +
      '\\[\\sum_{j=1}^{i}\\frac{\\phi(\\mathbf{q}_{i})^{\\top}\\phi(\\mathbf{k}_{j})\\mathbf{v}_{j}}{\\phi( \\mathbf{q}_{i})\\sum_{j=1}^{i}\\phi(\\mathbf{k}_{j})}=\\frac{\\phi(\\mathbf{q}_{i})\\sum_{j=1}^{ i}\\left(\\phi(\\mathbf{k}_{j})^{\\top}\\mathbf{v}_{j}\\right)}{\\phi(\\mathbf{q}_{i})\\sum_{j=1}^{i} \\phi(\\mathbf{k}_{j})} \\tag{2}\\]\n' +
      '\n' +
      'where every query attends to every past key in \\(\\mathcal{O}(Nd^{2})\\) time and space complexity. Furthermore, Katharopoulos et al. [24] show that linear attention has a fixed size recurrent state during generation. Letting \\(\\mathbf{s}_{i}=\\sum_{j=1}^{i}\\phi(\\mathbf{k}_{j})^{\\top}\\mathbf{v}_{j}\\) and \\(\\mathbf{z}_{i}=\\sum_{j=1}^{i}\\phi(\\mathbf{k}_{j})^{\\top}\\) be a "KV-state" and "K-state" respectively, we can compute Equation (2) as\n' +
      '\n' +
      '\\[\\mathbf{s}_{i}=\\mathbf{s}_{i-1}+\\phi(\\mathbf{k}_{i})^{\\top}\\mathbf{v}_{i},\\ \\ \\mathbf{z}_{i}=\\mathbf{z}_{i-1}+\\phi(\\mathbf{k}_{i})^{\\top},\\] \\[\\mathbf{y}_{i}=\\frac{\\phi(\\mathbf{q}_{i})\\mathbf{s}_{i}}{\\phi(\\mathbf{q}_{i})\\mathbf{ z}_{i}} \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\mathbf{s}_{i}\\in\\mathbb{R}^{d\\times\\tilde{d}}\\) and \\(\\mathbf{z}_{i}\\in\\mathbb{R}^{\\tilde{d}}\\).\n' +
      '\n' +
      'Feature map.To approximate \\(\\exp(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}/\\sqrt{d})\\), we use the \\(2^{\\text{nd}}\\)-order Taylor series feature map, picking \\(\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d^{2}}\\) such that\n' +
      '\n' +
      '\\[\\phi(\\mathbf{q}_{i})^{\\top}\\phi(\\mathbf{k}_{j})=1+\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}+\\frac{( \\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j})^{2}}{2} \\tag{4}\\]\n' +
      '\n' +
      'While Zhang et al. [13] note that picking a feature map with \\(\\tilde{d}=d^{2}\\) results in linear attention with \\(\\mathcal{O}(Nd^{3})\\) time and space complexity and large recurrent state of size \\(O(d^{3})\\), we can tradeoff efficiency for recall capacity by projecting queries and keys to smaller dimensions _i.e.,_\\(\\mathbf{W}_{q},\\mathbf{W}_{k}\\in\\mathbb{R}^{d\\times d^{\\prime}}\\) with \\(d^{\\prime}=16\\). By changing \\(d^{\\prime}\\) we modulate the size of the recurrent state.\n' +
      '\n' +
      '_How does the choice of feature map affect the memory-recall trade-off?_ Prior work demonstrates the strong performance of the Taylor feature map on associative recall [13]. Building on this analysis, we evaluate a broad set of feature maps (\\(\\phi_{\\text{ReLU}}(x)=\\max(x,0)\\), \\(\\phi_{\\text{PosEUL}}(x)=\\text{ELU}(x)+1\\), \\(\\phi_{\\text{Square}}(x)=x^{2}\\), \\(\\phi_{\\text{Identity}}(x)=x\\), \\(\\phi_{\\text{CosFormer}}\\) as defined in [23], and \\(\\phi_{\\text{Performer}}\\) as defined in [21]) using the experimental setup described in Section 3.1. In Figure 3 (top), we plot the memory-recall tradeoff curves for these feature maps. The Taylor series feature map, along with the simple \\(\\phi_{\\text{PosEUL}}\\) and \\(\\phi_{\\text{ReLU}}\\) feature maps, sits at the pareto frontier. One advantage of the Taylor feature map over these alternatives is that it expands the recurrent state size (improving recall capacity) without changing the number of parameters. As shown in Figure 3 (bottom), the Taylor series feature map requires fewer parameters than alternatives to achieve high recall capacity. This analysis and the ablations in \\(Table\\) 4 informed our decision to use the Taylor approximation, though other simple feature maps may be effective as well.\n' +
      '\n' +
      '### Local Exact Attention with Tensor Core Sliding Windows\n' +
      '\n' +
      'To efficiently model fine-grained local interactions, Based uses sliding window attention with window sizes set at small multiples of 16 (up to 64 tokens). Similar to past (causal) implementations [18, 19], for window size \\(w\\) each query \\(\\mathbf{q}_{i}\\) only attends to past keys \\(\\{\\mathbf{k}_{i-w+1},\\dots,\\mathbf{k}_{i}\\}\\). This enables \\(\\mathcal{O}(Nw)\\) time and space complexity for linear scaling in sequence length \\(N\\), with a \\(w\\)-sized KV-cache for constant-memory generation.\n' +
      '\n' +
      'However, unlike past sliding window attentions that keep \\(w\\) at sizes 256 [17] to 4096 [10], Based uses only \\(w=16\\), 32, or 64 to best exploit modern GPUs. In Section 5, we discuss how this "Tensor core-aware" window (tcWindow) achieves 1e-5\\(\\times\\) the latency than the \\(w=4096\\) windows in modern LLMs (_e.g.,_ Mistral 7B [10]).\n' +
      '\n' +
      'While the small \\(w\\) in tcWindow enable fast local and exact attention, it presents a challenge for long range modeling. With just \\(w=64\\), for every layer of \\(w=4096\\) Mistral sliding window attention we would require 64 layers of Based to achieve the same receptive field. Controlling for model depth and sequence length, Figure 2 indeed shows smaller \\(w\\) linearly decreasing in associative recall accuracy. Based\'s global _linear attention_ described above overcomes the lack of long-range modeling presented with low \\(w\\).\n' +
      '\n' +
      'Additional architectural details for Based are discussed in Appendix C and the hybridization of layers used in experiments are provided in Table 7. We include ablations of architectural choices in Table 4 and evaluate the overall quality and efficiency of Based in Section 6.\n' +
      '\n' +
      'Figure 3: **Linear attention feature maps on AR.**\\(x\\): state size (bytes) during generation or param. count; \\(y\\): MQAR accuracy. This setting is harder than fig. 2 (256 key-value pairs). exploit modern GPUs. In Section 5, we discuss how this "Tensor core-aware" window (tcWindow) achieves 1e-5\\(\\times\\) the latency than the \\(w=4096\\) windows in modern LLMs (_e.g.,_ Mistral 7B [10]).\n' +
      '\n' +
      'Efficient Implementation\n' +
      '\n' +
      'In this section we focus on the efficiency of Based. A naive implementation is _slower_ than the most efficient standard attention implementations (shown in Figure 4) as it requires large amounts of high latency memory movement. We first describe preliminaries of the GPU execution model and memory hierarchy. We next present the baseline and our hardware-aware algorithms for linear attention in Section 5.1 and for sliding window attention in Section 5.2.\n' +
      '\n' +
      'PreliminariesGPU operations, or _kernels_, are executed by thousands of parallel threads. In NVIDIA terms, GPU streaming multiprocessors launch _thread blocks_ at the software level. These blocks are divided into _warps_ (_e.g._ 32 threads) that are assigned to cores at the hardware level. Threads need to read inputs into their _registers_ to perform computations and write the outputs. The time taken to read and write is referred to as the IO cost.\n' +
      '\n' +
      'Operations could either be memory or compute bound, depending on the time to load data vs. perform computations on loaded data. In designing our IO-aware algorithms, we would like to exploit two key properties of modern GPUs. First, tensor core units (fast matrix multiply units) achieve 312 TFLOP/s speeds relative to 19 TFLOP/s for the non-matrix multiply cores. Second, GPUs face a memory hierarchy with large amounts of slow-to-access memory and smaller amounts of fast-to-access memory. The access speed is governed by the proximitiy of the processor to the data address. For instance, the hierarchy on a modern NVIDIA 80GB A100 GPU is: 80GB of HBM with 2 TB/s bandwidth, 80MB of L2 cache, 192KB of L1 cache / shared memory (implemented via SRAM) with 19 TB/s bandwidth per SM, and 256 KB of register file per SM [33]. Note that register memory is private to an executing thread, so threads need to write to shared memory to communicate data to other threads in the block. To reduce the time required for reads and writes, a key principle is to _fuse_ multiple operations on the same data slice while it\'s in fast memory before writing it back to slow memory.\n' +
      '\n' +
      '### Taylor Exponential Linear Attention\n' +
      '\n' +
      'Despite the theoretically improved complexity, the linear attention methods demonstrated in prior work are often less efficient than highly-optimized softmax attention implementations (Flash Attention [12]) when measured in real-world wall-clock time and memory usage. We next present hardware-aware algorithms to make Taylor linear attention efficient. We focus on two operations: (1) prefill, corresponding to processing the prompt during generation or the forward pass during training, and (2) next token prediction during generation, which also requires updating the recurrent hidden state state.\n' +
      '\n' +
      'In this section, we refer to the batch size as \\(B\\), number of heads as \\(H\\), head dimension as \\(d\\), sequence length as \\(N\\) and feature dimension as \\(d^{\\prime}\\), following Section 4. For ease of notation, let \\(D=1+d^{\\prime}+d^{\\prime 2}\\) in this section. Additional details for the IO-Aware algorithms are included in Appendix B.\n' +
      '\n' +
      '#### 5.1.1 Forward Pass / Generation Prefill\n' +
      '\n' +
      'Baseline ImplementationThe naive implementation detailed in Appendix B (1) produces the feature maps \\(Q,K\\), (2) computes and materializes the large hidden state \\(KV\\in\\mathbb{R}^{H\\times d\\times D}\\), then (3) computes the causal dot product between \\(Q\\) and \\(KV\\). Prior work has released popular CUDA kernels for linear attention to efficiently perform the causal dot product / step (3) [34]. The kernel parallelizes computation across the heads and batches, loads tiles of \\(V\\) and \\(K\\) to SRAM, updates the running \\(KV\\) state in SRAM, loads tiles of \\(Q\\) to SRAM, produces the final output in SRAM, and writes the result to HBM.\n' +
      '\n' +
      '_Analysis_ In overall IO cost, ignoring the input and output projections in the linear attention layer, this procedure requires \\(2BHND\\) bytes for writing featurized \\(Q,K\\) to HBM. During the causal dot product, this requires \\(2BHND+BHNd\\) bytes to read \\(Q,K,V\\) tiles and \\(BHNd\\) bytes to write the result. Throughout the computation, \\(\\mathcal{O}(BHNDd)\\) bytes (note this is the shape \\(KV\\) state during the forward pass) are read in and out of thread registers to SRAM to update the running output and \\(KV\\) state at 19TB/s bandwidth.\n' +
      '\n' +
      'AlgorithmTo improve efficiency, our algorithm computes both the feature map and the causal dot product in fast memory. Our overall algorithm is detailed in Algorithm 1 and we discuss the overall process here.\n' +
      '\n' +
      'The algorithm first parallelizes over the batch \\(B\\) and head \\(H\\) dimensions, since computation is independent for each head in each batch. We consider the three terms \\(T_{0},T_{1},T_{2}\\in\\mathbb{R}^{N\\times d}\\) in the linear attention output, corresponding to the three terms in the \\(2^{nd}\\)-order Taylor polynomial for the exponential. We also consider the corresponding three terms, in the cumulative \\(KV\\) state as we process a sequence. For a given _tile_ (i.e. sub-matrix) of \\(Q,K,V\\), we load the tiles (e.g., tile of \\(Q,K\\in\\mathbb{R}^{16\\times 16}\\) and \\(V\\in\\mathbb{R}^{16\\times 64}\\) given 16 tokens, 16 feature dimension, and 64 head dimension) into fast memory and update the running \\(KV\\) state and output for each of the three terms. We load the tiles into the register file and fuse-operations to compute the causal dot product in register, only writing to SRAM to synchronize the cumulative \\(KV\\) state across parallel warps. We use explicit writes to SRAM for this synchronization because register memory is warp-specific (threads in one warp do not access the registers for a thread in a different warp).\n' +
      '\n' +
      '_Analysis_ In IO cost, again ignoring the input and output projections in the linear attention layer, our procedure requires \\(2BHNd^{\\prime}\\) bytes for reading \\(q,k\\) and \\(2BHNd\\) bytes for reading \\(v\\) and writing output \\(y\\) between HBM and SRAM. Overall, our algorithm avoids in HBM \\(\\mathcal{O}(2BHND)\\) bytes in HBM to SRAM data movement. We additionally improve upon the baseline by performing computation in-register to avoid the \\(\\mathcal{O}(BHNDd)\\) bytes in SRAM to register data movement.\n' +
      '\n' +
      '#### 5.1.2 Next token prediction\n' +
      '\n' +
      'During next token prediction, an important consideration is how to efficiently update the recurrent state \\(KV_{t}\\in\\mathbb{R}^{BHDd}\\) at timestep \\(t\\). The expensive operation during next token prediction is computing the outer product between projected hidden states \\(k_{t+1}\\in\\mathbb{R}^{BHD}\\) and \\(v_{t+1}\\in\\mathbb{R}^{BHd}\\). The outer product requires \\(\\mathcal{O}(BHDd)\\) computation and space, and the result is summed with \\(KV_{t}\\) to produce \\(KV_{t+1}\\). We provide an IO-aware algorithm for the state updates in Algorithm 2. This algorithm incurs \\(\\mathcal{O}(BHD+BHd)\\) bytes of HBM to SRAM data movement (to load the \\(q,k,v\\) projections).\n' +
      '\n' +
      '### tcWindow\n' +
      '\n' +
      'Next we motivate the choice of window size for tcWindow. In contrast to sliding-window style models such as the popular Mistral models, which use large window sizes \\(w=4096\\)[10], Based chooses a window size based on hardware specifications. GPU tensor cores operate on \\(16\\times 16\\) tiles. Large GEMMs are compute bound (for e.g. in long-context attention). But, we need sufficient occupancy to hide the latency of the tensor core units. Figure 1 (Right) shows \\(64\\times 64\\) dimension matrix multiplications are approximately the same latency as \\(16\\times 16\\). Based sets \\(w\\) to use \\(64\\times 64\\) tiles (Figure 1). To distinguish from prior sliding windows, we refer to this approach as tcWindow. We use the Flash Attention sliding window implementation during training [11] and in Appendix B Algorithm 3, we provide an IO-aware algorithm of tcWindow for next token prediction.\n' +
      '\n' +
      'End-to-end benchmarks for Based implemented with these IO-aware algorithms are provided in Section 6. Micro-benchmarks for each kernel against the baseline implementations are provided in Appendix B.\n' +
      '\n' +
      '## 6 Results\n' +
      '\n' +
      'In this section, we present results for the following claims:\n' +
      '\n' +
      '1. **Language modeling overall.** We evaluate architectures in pretraining from scratch on the Pile [30] and on standard benchmarks from the LM Eval Harness [16]. We find Based matches or outperforms the strongest sub-quadratic architectures (e.g. Mamba [5]) across these settings.\n' +
      '2. **Language modeling recall.** Based closes the gap to attention on the challenging associative recall slice of the real-world Pile language modeling corpus (see Table 1), outperforming prior sub-quadratic architectures. We apply these pretrained models zero-shot to a suite of recall-intensive tasks (_e.g._ information extraction, QA), showing that Based systematically outperforms other efficient architectures with comparable throughput.\n' +
      '\n' +
      '3. **Generation throughput.** Our IO-aware implementation of recurrent generation in Based enables \\(40-60\\%\\) speedups relative to FlashAttention-2 and Mamba for prefill at \\(4k\\) sequence length and up to \\(24\\times\\) higher throughput over FlashAttention-2 in generating 1024 tokens at batch size 128 (see Figure 4).\n' +
      '4. **Ablations of key design choices.** We ablate four key design choices and hyperparameters: (1) the feature map, (2) the feature dimension, (3) use of local sliding-window softmax, and (4) the use of short gated-convolutions. The results of these ablations are shown in Table 4.\n' +
      '\n' +
      'BaselinesWe compare to key baselines in the literature. We compare to Transformer (GPT architecture) and Transformer++ (Llama architecture [15]), which adds rotary encodings [35] and gated linear units. We compare an early class of efficient architectures built from gating and long-convolution primitives including Hyena [7], RWKV [8], and H3 [9]. We finally compare to recent state-of-the-art architectures, that use input-dependent sequence aggregation to improve in quality upon the long-convolution models, including Mamba [5] and Gated Linear Attention [6].\n' +
      '\n' +
      '### Language Modeling Evaluations\n' +
      '\n' +
      'Language Modeling BenchmarksWe pretrain language models from scratch at two parameter scales (355M and 1.3Bn parameters) on the Pile [30]. Each model sees the same 10 billion tokens of pretraining data in the same order. The Pile data is tokenized using the GPT-2 BPE tokenizer [36]. We measure perplexity on the Pile and report results in Table 1 and further experimental details are provided in Appendix E.1.\n' +
      '\n' +
      'We additionally evaluate the pretrained models on the standard LM Eval Harness [16], as is standard protocol for our baselines [5, 6]. A detailed breakdown of tasks and metrics can be found in Appendix D. In both pretraining and on the LM Eval Harness, Based consistently competes with the strongest Transformer++ and Mamba baselines. While these overall metrics are helpful, we next turn to a fine-grained analysis of recall and in-context learning ability on real-world data.\n' +
      '\n' +
      'Recall and In-Context Learning EvaluationsWe evaluate our pretrained models on a suite of context learning tasks selected to test the downstream recall capacity in Table 1. These tasks fall into three categories: (1) **Real-world AR** Beyond perplexity scores, we slice the next token predictions on the Pile to understand each architecture\'s AR quality ( Appendix E.1). (2) **Information extraction (IE)** SWDE and FDA are popular semi-structured and unstructured document IE benchmarks respectively [37, 38, 39]. SWDE has _raw HTML_ for 8 Movie and 5 University websites (e.g. IMDB, US News) and annotations for 8-274 attributes per website (e.g., Movie runtime), and (3) **Question answering** from in-context passages. We find Based outperforms the baseline sub-quadratic architectures across these evaluations, closing the gap to Transformer++. These trends tracks the MQAR synthetic results from Section 3.1.\n' +
      '\n' +
      'Figure 4: (**Left**) Throughput numbers for the varied prefill sequence lengths at a fixed batch size of 2. **Right** generation throughput at varied batch sizes at a fixed generation length of 1024 tokens. The \\(y\\)-axis shows the in latency (ms). Lines are cutoff when the model runs out of memory. We show results for both 360M and 1.3Bn, and all numbers are computed on a single NVIDIA H100 GPU.\n' +
      '\n' +
      'In order to understand in-context-learning performance, we next perform few-shot evaluations on the SuperGLUE benchmark [40] for Based, Mamba and Transformer++ in Table 2. Each model was evaluated on all tasks using under 0 shot (_i.e._, number of in-context examples), 1 shot and 5 shot prompting, respectively. Transformer++ and Based both see monotonic improvement from increasing the number of shots. For Mamba, however, albeit getting a slight improvement from 0-shot to 1-shot, it performs worse on 5-shot than even on 0-shot. This result suggests that the limited recall ability observed in Mamba could also impact few-shot abilities.\n' +
      '\n' +
      'DNA modelingTowards understanding the capability of Based beyond natural English language, we next evaluate each architecture on its ability to perform DNA next token prediction (Table 3). We evaluate architectures on the HG38 (human genome) benchmark at \\(1k\\), \\(4k\\), and \\(8k\\) sequence lengths used in prior architecture evaluations [5, 41]. The DNA tasks uses a byte-level tokenizer wherein the vocabulary consists of characters corresponding to the nucleotide bases. We find Based is competitive with state-of-the-art architectures across evaluated sequence lengths. In Appendix D, we evaluate these checkpoints on downstream DNA classification tasks.\n' +
      '\n' +
      '### Efficiency Benchmarks\n' +
      '\n' +
      'We benchmark the throughput of Based, with and without our proposed IO-Aware algorithms (Section 5). We consider both the **forward pass / generation prefill** and **next token prediction** stages. Experiments were run using an H100 NVIDIA GPU and averaged over 20 repetitions. Results are shown in Figure 4.\n' +
      '\n' +
      'End-to-end benchmarksUsing our efficient implementation (Section 5), Based achieves 56% faster prefill than FlashAttention-2 [11] and 44% faster than Mamba at \\(4k\\) sequence length and 1.3Bn parameters (28% faster than FlashAttention-2 and 76% faster than Mamba at 360M parameters). We find that next token generation, with _no prefill_, provides \\(24\\times\\) higher throughput (tokens/second) over the highly optimized FlashAttention-2 implementation and achieves 95% and the throughput of the recurrent Mamba architecture at batch size 128 and 1.3Bn parameters (98% higher throughput vs. FlashAttention-2 and 118% higher throughput vs. Mamba at 360M parameters). All benchmarks is on a single NVIDIA H100 GPU, using CUDA cache graphs during next token prediction [42].\n' +
      '\n' +
      'In Figure 4, we also include results for the baseline implementation of Based that uses the popular Fast\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Shots** & **BoolQ** & **CB** & **COPA** & **MultiRC** & **ReCoRD** & **RTE** & **WiC** & **WSC** & **Avg** \\\\  & & Acc. \\(\\uparrow\\) & Acc. \\(\\uparrow\\) & F1 \\(\\uparrow\\) & Acc. \\(\\uparrow\\) & Acc. \\(\\uparrow\\) & F1 \\(\\uparrow\\) & F1 \\(\\uparrow\\) & EM \\(\\uparrow\\) & Acc. \\(\\uparrow\\) & Acc. \\(\\uparrow\\) & Acc. \\(\\uparrow\\) \\\\ \\hline \\hline \\multirow{2}{*}{Based} & 0 & 59.0 & 41.1 & 19.4 & 69.0 & 54.9 & 14.5 & 14.0 & 52.0 & 50.0 & 36.5 & 45.7 \\\\  & 1 & 57.5 & 37.5 & 26.8 & 68.0 & 52.5 & 19.9 & 19.2 & 47.7 & 50.9 & 49.0 & 47.2 \\\\  & 5 & 56.6 & 44.6 & 28.9 & 73.0 & 53.6 & 24.9 & 24.1 & 48.7 & 51.1 & 39.4 & 48.0 \\\\ \\hline \\multirow{2}{*}{Transformer++} & 0 & 57.3 & 41.1 & 21.3 & 67.0 & 57.0 & 16.6 & 16.1 & 53.8 & 50.0 & 37.5 & 46.3 \\\\  & 1 & 54.2 & 39.3 & 25.3 & 69.0 & 51.5 & 22.2 & 21.6 & 50.9 & 47.0 & 55.8 & 47.8 \\\\  & 5 & 50.7 & 58.9 & 49.9 & 64.0 & 46.9 & 24.2 & 23.6 & 47.3 & 52.2 & 51.9 & 48.9 \\\\ \\hline Mamba & 0 & 57.5 & 35.7 & 24.4 & 71.0 & 57.2 & 18.8 & 18.3 & 52.4 & 50.0 & 36.5 & 46.6 \\\\  & 1 & 51.1 & 39.3 & 27.4 & 71.0 & 52.9 & 21.6 & 21.0 & 46.6 & 46.2 & 52.9 & 46.9 \\\\  & 5 & 41.1 & 37.5 & 23.6 & 69.0 & 49.2 & 20.4 & 19.9 & 48.4 & 51.7 & 51.9 & 45.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Few-shot downstream evaluation on SuperGLUE of pre-trained language models.** The same set of models as in table 1, all were trained on the same 10 billion tokens drawn from the Pile [30], evaluated on the SuperGLUE benchmark [40] using the LM eval harness by EleutherAI [16]. When computing the average, we first average the metrics by task and then average across tasks.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & & \\multicolumn{4}{c}{**HG38 PPL \\(\\downarrow\\)**} \\\\ Model & Params & \\(N=1024\\) & \\(N=4096\\) & \\(N=8192\\) \\\\ \\hline \\hline Transformer++ & 46.2 & 2.52 & 2.50 & 2.51 \\\\ Mamba & 46.1 & **2.51** & **2.49** & **2.49** \\\\ Based & 48.8 & **2.51** & 2.50 & **2.49** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **DNA modeling performance on the HG38 dataset.** All models are pretrained from scratch for 10Bn tokens at \\(N=1\\)k, 4k, and 8k sequence lengths respectively. We report results after hyperparameter sweeping the learning rate for each architecture.\n' +
      '\n' +
      'Transformers CUDA kernel in the linear attention literature to compute the causal dot product [34] (discussed in Section 5). The _custom kernel_ introduced in our work unlocks the efficiency of Based.\n' +
      '\n' +
      'Micro benchmarksAs the end-to-end Based architecture is a hybrid architecture, we provide micro benchmarks of the individual IO-aware kernels against the key baseline implementations in Appendix B.\n' +
      '\n' +
      '### Quality Ablations\n' +
      '\n' +
      'Our objective with Based is to measure the throughput and recall of the simplest possible linear attention model that achieves strong performance. Therefore, we ablate the key design decisions -- choice of feature map, feature dimension for the Taylor map, use of sliding window and convolutions -- to understand their contributions to the quality of Based. We perform these ablations on the Pile corpus [30] using the same number of tokens and data ordering as the prior experiments.\n' +
      '\n' +
      'In **feature map ablations**, we consider the CosFormer [23] and Performers [21] feature maps, which have been demonstrated as strong linear attentions in prior work [13]. We also include a baseline that expands the state size using learned projections and applies CosFormer towards comparing to the larger state size of the Taylor map. For these baselines, we keep the rest of the Based architecture the same (_i.e._ in the number of linear attention layers and hybridization with sliding window and gated convolution layers). We observe that with the larger state size, CosFormer quality is increasingly competitive with the Taylor map. We note that expanding the state size requires increasing the model\'s overall parameter count (due to the learned projections) in the CosFormer baseline, in contrast to the Taylor map.\n' +
      '\n' +
      'Next, we ablate the **feature dimension**, holding the feature map fixed to the Taylor map. We find larger feature dimension improves quality, with diminishing returns going from 24 to 32 dimension. Note that feature dimension \\(\\sqrt{1024}=32\\), where 1024 is the attention model dimension at the 360 parameter scale in our experiments.\n' +
      '\n' +
      'Finally, the ablations show that eliminating the **convolutions** and/or the **sliding window attention** degrades quality. We observe that adding _either_ convolutions or sliding window helps on the associative recall slice relative to _neither_ (e.g. 2.29 AR Ppl. on the Pile with _neither_ vs. 2.09 or 2.11 with sliding window _or_ convolutions.). Further, increasing the window size from 0 to 64 vs. 64 to 128 (also an efficient design point in Figure 1, left) continues to help quality, but with marginal improvements.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline  & \\multicolumn{3}{c}{**Hyperparameters**} & \\multicolumn{3}{c}{**Language Modeling (Pile)**} & \\multicolumn{1}{c}{**Info. Extraction**} & \\multicolumn{1}{c}{**QA**} \\\\  & Feat. Map & Feat. Dim. & Sliding & Convs. & Decay & All & AR & Other & FDA & SQUAD \\\\ \\hline Taylor Exp. (2nd) & 16 (153) & ✓(64) & ✓ & ✓ & **8.65** & **2.07** & **9.64** & **11.71** & 25.07 \\\\ Performer & 16 (16) & ✓(64) & ✓ & ✓ & 9.08 & 8.53 & 11.62 & 0.36 & 7.47 \\\\ CosFormer & 16 (32) & ✓(64) & ✓ & ✓ & 9.03 & 2.42 & 9.98 & 7.71 & 24.63 \\\\ CosFormer & 64 (128) & ✓(64) & ✓ & ✓ & 8.82 & 2.18 & 9.80 & 9.07 & **27.85** \\\\ \\hline Taylor Exp. (2nd) & 32 (561) & ✓(64) & ✓ & ✓ & **8.56** & **2.00** & **9.57** & 12.89 & **26.74** \\\\ Taylor Exp. (2nd) & 24 (325) & ✓(64) & ✓ & ✓ & 8.58 & 2.02 & 9.58 & **20.87** & 24.77 \\\\ Taylor Exp. (2nd) & 16 (153) & ✓(64) & ✓ & ✓ & 8.65 & 2.07 & 9.64 & 11.71 & 25.07 \\\\ Taylor Exp. (2nd) & 8 (45) & ✓(64) & ✓ & ✓ & 8.77 & 2.18 & 9.75 & 12.79 & 22.35 \\\\ \\hline Taylor Exp. (2nd) & 16 (153) & ✓(64) & ✓ & ✓ & **8.65** & 2.07 & **9.64** & **11.71** & **25.07** \\\\ Taylor Exp. (2nd) & 16 (153) & ✓(64) & ✓ & � & **8.65** & **2.04** & 9.66 & 1.72 & 10.80 \\\\ Taylor Exp. (2nd) & 16 (153) & ✗ & ✓ & ✓ & 8.91 & 2.11 & 9.94 & 10.16 & 24.5 \\\\ Taylor Exp. (2nd) & 16 (153) & ✓(64) & ✗ & ✓ & 8.74 & 2.09 & 9.74 & 2.36 & 18.87 \\\\ Taylor Exp. (2nd) & 24 (325) & ✗ & ✓ & ✓ & 9.49 & 2.29 & 10.58 & 8.71 & 11.33 \\\\ \\hline Taylor Exp. (2nd) & 16 (153) & ✓(128) & ✓ & ✓ & **8.61** & **2.06** & **9.60** & **14.39** & **31.84** \\\\ Taylor Exp. (2nd) & 16 (153) & ✓(64) & ✓ & ✓ & 8.65 & 2.07 & 9.64 & 11.71 & 25.07 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: **Ablations of design choices and hyperparameters in Based.** All models are 362M param variants of the Based architecture described in Section 4, trained to 10 billion tokens on the Pile. We ablate the hyperparameters central to the design of Based: (1) the choice of feature map \\(\\phi\\) (see Section 4.1), (2) the size of the feature dim \\(d^{\\prime}\\) (we show the effective size of the feature after applying the feature map in parantheses, see Section 4.1), (3) the use of local sequence mixers (sliding window attention and short convolutions), and (4) the data-dependent decay defined in Section 4.\n' +
      '\n' +
      'Conclusion\n' +
      '\n' +
      'This work studies the properties of high-quality and efficient sequence mixers. We identify a fundamental tradeoff between recall, key to in-context learning, and throughput through theory and experiments. Attention performs recall perfectly, but requires retaining a KV cache that grows with the sequence length. As an alternative, Based combines two simple techniques -- local fine-grained attention and long-range linear attention via a Taylor approximation of the softmax exponential function - that are sub-quadratic during training and permit an efficient recurrent inference view. To enable wall clock efficiency, we introduce IO-aware algorithms of the Taylor linear attention computation that leads Based to perform generation up to \\(24\\times\\) faster than FlashAttention-2 at the 1.3Bn parameter scale, generating 1024 tokens. Beyond competing in overall perplexity, Based outperforms prior sub-quadratic architectures in recall quality by up to 6.2 accuracy points, pointing to Based\'s promise as a language modeling architecture.\n' +
      '\n' +
      '#### Acknowledgments\n' +
      '\n' +
      'We thank Tri Dao, Daniel Fu, Songlin Yang, Jessica Grogan, Albert Gu, Eric Nguyen, Michael Wornow, Alyssa Unell, and Gautam Machiraju for their helpful feedback and discussion during this work. We thank the Hazy Research lab and Together AI for supporting this work. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF2247015 (Hardware-Aware), CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM ARL under Nos. W911NF-23-2-0184 (Long-context) and W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under Nos. N000142312633 (Deep Signal Processing), N000141712266 (Unifying Weak Supervision), N000142012480 (Non-Euclidean Geometry), and N000142012275 (NEPTUNE); Stanford HAI under No. 247183; NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. AR\'s research is supported by NSF grant CCF#2247014.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Re. Zoology: Measuring and improving recall in efficient language models. _International Conference on Learning Representations_, 2023.\n' +
      '* [2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. volume 30, 2017.\n' +
      '* [3] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_, 2022.\n' +
      '* [4] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. _arXiv preprint arXiv:2212.10544_, 2022.\n' +
      '* [5] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_, 2023.\n' +
      '* [6] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. _arXiv preprint arXiv:2312.06635_, 2023.\n' +
      '* [7] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena hierarchy: Towards larger convolutional language models. _arXiv preprint arXiv:2302.10866_, 2023.\n' +
      '\n' +
      '* [8] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, and Jiaming et al. Kong. Rwkv: Reinventing rnns for the transformer era. _arXiv:2305.13048_, 2023.\n' +
      '* [9] Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Re. Hungry Hungry Hippos: Towards language modeling with state space models. In _International Conference on Learning Representations_, 2023.\n' +
      '* [10] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* [11] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\n' +
      '* [12] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* [13] Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Re. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=4g0212N2Nx](https://openreview.net/forum?id=4g0212N2Nx).\n' +
      '* [14] Feyza Duman Keles, Pruthuvi Maheskaya Wijewardena, and Chinmay Hegde. On the computational complexity of self-attention. In _34th International Conference on Algorithmic Learning Theory_, volume 201, page 1-23, 2023.\n' +
      '* [15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, and Shruti Bhosale. Llama 2: Open foundation and fine-tuned chat models. _arXiv:2307.09288_, 2023.\n' +
      '* [16] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).\n' +
      '* [17] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In _International conference on machine learning_, pages 4055-4064. PMLR, 2018.\n' +
      '* [18] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_, 2019.\n' +
      '* [19] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_, 2020.\n' +
      '* [20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In _International conference on machine learning_, pages 5156-5165. PMLR, 2020.\n' +
      '* [21] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. _arXiv preprint arXiv:2009.14794_, 2020.\n' +
      '* [22] Krzysztof Choromanski, Haoxian Chen, Han Lin, Yuanzhe Ma, Arijit Sehanobish, Deepali Jain, Michael S Ryoo, Jake Varley, Andy Zeng, Valerii Likhosherstov, et al. Hybrid random features. _arXiv preprint arXiv:2110.04367_, 2021.\n' +
      '\n' +
      '* [23] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. _arXiv preprint arXiv:2202.08791_, 2022.\n' +
      '* [24] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2020. URL [https://arxiv.org/abs/2006.16236](https://arxiv.org/abs/2006.16236).\n' +
      '* [25] Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. Finetuning pretrained transformers into RNNs. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 10630-10643, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.830. URL [https://aclanthology.org/2021.emnlp-main.830](https://aclanthology.org/2021.emnlp-main.830).\n' +
      '* [26] Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. Linear transformers are secretly fast weight programmers. In _International Conference on Machine Learning_, pages 9355-9366. PMLR, 2021.\n' +
      '* [27] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. _arXiv preprint arXiv:2111.00396_, 2021.\n' +
      '* [28] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 2023.\n' +
      '* [29] Ekin Akyurek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning: Architectures and algorithms. 2024. URL [https://arxiv.org/abs/2401.12973](https://arxiv.org/abs/2401.12973).\n' +
      '* [30] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. _arXiv preprint arXiv:2101.00027_, 2020.\n' +
      '* [31] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher Re. Simple hardware-efficient long convolutions for sequence modeling. _arXiv preprint arXiv:2302.06646_, 2023.\n' +
      '* [32] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: a unified understanding of transformer\'s attention via the lens of kernel. _arXiv preprint arXiv:1908.11775_, 2019.\n' +
      '* [33] NVIDIA. Nvidia H100 tensor core GPU architecture, 2022.\n' +
      '* [34] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. 2020.\n' +
      '* [35] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023.\n' +
      '* [36] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n' +
      '* [37] Eric Wu, Kevin Wu, Roxana Daneshjou, David Ouyang, Daniel Ho, and James Zou. How medical ai devices are evaluated: limitations and recommendations from an analysis of fda approvals. _Nature Medicine_, 27:1-3, 04 2021.\n' +
      '* [38] Xiang Deng, Prashant Shirakkar, Colin Lockard, Binxuan Huang, and Huan Sun. Dom-lm: Learning generalizable representations for html documents. 2022.\n' +
      '* [39] Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, and Christopher Re. Language models enable simple systems for generating structured views of heterogeneous data lakes. _arXiv:2304.09433_, 2023.\n' +
      '* [40] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. _SuperGLUE: a stickier benchmark for general-purpose language understanding systems_. Curran Associates Inc., Red Hook, NY, USA, 2019.\n' +
      '\n' +
      '* Nguyen et al. [2023] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, and Chris Re. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution, 2023.\n' +
      '* NVIDIA [2019] NVIDIA. Getting started with cuda graphs, 2019. URL [https://developer.nvidia.com/blog/cuda-graphs/](https://developer.nvidia.com/blog/cuda-graphs/).\n' +
      '* Kitaev et al. [2020] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020.\n' +
      '* Zaheer et al. [2020] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and et al. Big bird: Transformers for longer sequences. _Proceedings of NeurIPS_, 2020.\n' +
      '* Wang et al. [2020] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.\n' +
      '* Zhu et al. [2021] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. _Advances in neural information processing systems_, 34:17723-17736, 2021.\n' +
      '* Alberti et al. [2023] Silas Alberti, Niclas Dern, Laura Thesing, and Gitta Kutyniok. Summermer: Universal approximation for efficient transformers. _arXiv preprint arXiv:2307.02301_, 2023.\n' +
      '* Tay et al. [2022] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. _ACM Computing Surveys_, 55(6):1-28, 2022.\n' +
      '* Xiong et al. [2021] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystromformer: A nystrom-based algorithm for approximating self-attention. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 14138-14148, 2021.\n' +
      '* Chen et al. [2021] Yifan Chen, Qi Zeng, Heng Ji, and Yun Yang. Skyformer: Remodel self-attention with gaussian kernel and nystr\\(\\backslash\\)"om method. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL [https://openreview.net/forum?id=p2CYG7gjkKz](https://openreview.net/forum?id=p2CYG7gjkKz).\n' +
      '* De Brebisson and Vincent [2015] Alexandre De Brebisson and Pascal Vincent. An exploration of softmax alternatives belonging to the spherical loss family. _arXiv preprint arXiv:1511.05042_, 2015.\n' +
      '* Chen et al. [2021] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Re. Scatterbrain: Unifying sparse and low-rank attention approximation. _arXiv preprint arXiv:2110.15343_, 2021.\n' +
      '* Qin et al. [2022] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 7025-7041, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.473. URL [https://aclanthology.org/2022.emnlp-main.473](https://aclanthology.org/2022.emnlp-main.473).\n' +
      '* Ba et al. [2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.\n' +
      '* Zhang and Sennrich [2019] Biao Zhang and Rico Sennrich. Root mean square layer normalization. _Advances in Neural Information Processing Systems_, 32, 2019.\n' +
      '* Cooley and Tukey [1965] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series. _Mathematics of computation_, 19(90):297-301, 1965.\n' +
      '* Romero et al. [2022] David W. Romero, Anna Kuzina, Erik J. Bekkers, Jakub M. Tomczak, and Mark Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data. 2022.\n' +
      '\n' +
      '* [58] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022.\n' +
      '* [59] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Re. On the parameterization and initialization of diagonal state space models, 2022.\n' +
      '* [60] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces, 2022.\n' +
      '* [61] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Zettlemoyer Luke. Mega: Moving average equipped gated attention. _arXiv preprint arXiv:2209.10655_, 2022.\n' +
      '* [62] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 1, 2021.\n' +
      '* [63] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In _International conference on machine learning_, pages 933-941. PMLR, 2017.\n' +
      '* [64] Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and ChengXiang Zhai. Sparse modular activation for efficient sequence modeling. _arXiv preprint arXiv:2306.11197_, 2023.\n' +
      '* [65] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. _arXiv preprint arXiv:2310.01889_, 2023.\n' +
      '* [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In _Proceedings of the 29th Symposium on Operating Systems Principles_, pages 611-626, 2023.\n' +
      '* [67] Daniel Y Fu, Hermann Kumbong, Eric Nguyen, and Christopher Re. Flashfftconv: Efficient convolutions for long sequences with tensor cores. _arXiv preprint arXiv:2311.05908_, 2023.\n' +
      '* [68] Markus N Rabe and Charles Staats. Self-attention does not need \\(o(n^{2})\\) memory. _arXiv preprint arXiv:2112.05682_, 2021.\n' +
      '* [69] Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. Mnnfast: A fast and scalable system architecture for memory-augmented neural networks. In _2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA)_, pages 250-263, 2019.\n' +
      '* [70] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. _arXiv preprint arXiv:2305.19370_, 2023.\n' +
      '* [71] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In _International Conference on Machine Learning_, pages 9099-9117. PMLR, 2022.\n' +
      '* [72] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 12 2023. doi: 10.57967/hf/1595. URL [https://github.com/togethercomputer/stripedhyena](https://github.com/togethercomputer/stripedhyena).\n' +
      '* [73] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023.\n' +
      '* [74] Katarina Gresova, Vlastimil Martinek, David Cechak, Petr Simecek, and Panagiotis Alexiou. Genomic benchmarks: A collection of datasets for genomic sequence classification. _bioRxiv_, 2022. doi: 10.1101/2022.06.08.495248. URL [https://www.biorxiv.org/content/early/2022/06/10/2022.06.08.495248](https://www.biorxiv.org/content/early/2022/06/10/2022.06.08.495248).\n' +
      '* [75] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambda dataset: Word prediction requiring a broad discourse context, 2016.\n' +
      '\n' +
      '* [76] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019.\n' +
      '* [77] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019.\n' +
      '* [78] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.\n' +
      '* [79] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.\n' +
      '* [80] Colin Lockard, Prashant Shirakkar, and Xin Luna Dong. OpenCeres: When open information extraction meets the semi-structured web. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 3047-3056, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1309. URL [https://aclanthology.org/N19-1309](https://aclanthology.org/N19-1309).\n' +
      '* [81] Peter Burgisser, Michael Clausen, and Mohammad A Shokrollahi. _Algebraic complexity theory_, volume 315. Springer Science & Business Media, 2013.\n' +
      '* [82] Thathachar S Jayram, Ravi Kumar, and Dandapani Sivakumar. The one-way communication complexity of hamming distance. _Theory of Computing_, 4(1):129-135, 2008.\n' +
      '* [83] Swastik Kopparty. Topics in algorithms and complexity theory: Spring 2020. 2020.\n' +
      '\n' +
      'The appendix is organized as follows:\n' +
      '\n' +
      '1. Appendix A includes an extended related works discussion.\n' +
      '2. Appendix B includes details on the IO-aware implementation and benchmarking for Based.\n' +
      '3. Appendix C includes additional discussion of Based architectural details.\n' +
      '4. Appendix D provides additional experimental results.\n' +
      '5. Appendix E provides experimental details.\n' +
      '6. Appendix F includes theoretical results and proofs.\n' +
      '\n' +
      '## Appendix A Extended Related Work\n' +
      '\n' +
      'Our work relates broadly to various developments in efficient sequence modeling. In this section, we organize these related works into (1) model-based or algorithmic contributions (appendix A.1) and (2) implementation or systems-based contributions (appendix A.2).\n' +
      '\n' +
      '### Efficient Language Modeling Architectures\n' +
      '\n' +
      'While Transformers often achieve state-of-the-art language modeling quality, their design motivates various efficiency improvements when both processing input sequences and generating outputs. In particular, various works try to retain their modeling quality, while improving on their quadratic scaling (\\(\\mathcal{O}(N^{2})\\) in input sequence length \\(N\\)) when processing inputs and \\(\\mathcal{O}(NM)\\) time and space when decoding outputs for outputs of length \\(M\\) (when caching prior keys and values in the attention mechanism).\n' +
      '\n' +
      'We note that most related lines of work build on one of two primitives: _attention approximations_ (_e.g._, linear attentions, sparse attentions, sparse and low-rank attentions), or _state-space models_ (SSMs) (which have alternative parameterizations as either "long" convolutional models or recurrent neueral networks). Both model classes achieve subquadratic time and space complexity when processing inputs, while linear attentions and SSMs also enable better than \\(\\mathcal{O}(NM)\\) decoding via their ability to process inputs recurrently like a recurrent neural network (RNN).\n' +
      '\n' +
      'We describe each of these model classes next.\n' +
      '\n' +
      '#### a.1.1 Efficient Attentions\n' +
      '\n' +
      'We focus on two of the most related paradigms for efficiently computing attention here, _structured sparse attentions_ and _linear attentions_. We acknowledge a great deal of prior work to compute attention more efficiently, such as via locality-sensitive hashing [43], random sparse attentions [44], and sequence compression [45, 46, 47]. Please see [48] for a comprehensive survey.\n' +
      '\n' +
      'Structured sparse attentionsStructured parse attentions reduce attention\'s time and memory requirements by only attending over specific strided patterns or local _sliding windows_[17, 18, 19]. For example, [17] propose computing attention only over a local window of the past \\(w\\) tokens, such that processing sequences \\(N\\) tokens long only takes \\(\\mathcal{O}Nw\\) time and space. [18] note that this window alone may not all capture all desired dependencies (such as long-term interactions), and propose two strided patterns to compute dot products between queries and keys further away. [19] further propose allowing specific tokens to attend to all other tokens in a dense manner.\n' +
      '\n' +
      'While further popularized in recent large language models (Mistral, Jiang et al. [10]), we note that these implementations use large window sizes that still leave room for improving efficiency. In Based, we introduce a hardware-guided design (using small windows) and sliding window implementation that allows us to capitalize on sparse attention\'s efficiency.\n' +
      '\n' +
      'Linear attentionsLinear attentions preserve the same "sequence-mixing" operations as standard attention, computing dot products between queries and keys to weight corresponding values. However, their key insight is to replace the softmax in standard attention with alternative kernel functions [20]. Mechanically, by removing the \\(\\exp(\\mathbf{q}^{\\top}\\mathbf{k})\\) in favor of feature map dot-products \\(\\phi(\\mathbf{q})^{\\top}\\phi(\\mathbf{k})\\), these methods use matrix product associativity to compute attention in \\(\\mathcal{O}(Nd^{2})\\) time and space [24] (Equation (2)). Furthermore, they permit a _recurrent view_ for constant memory and \\(\\mathcal{O}(1)\\) time per-token generation [25, 26] (Equation (3)).\n' +
      '\n' +
      'Prior works propose different feature maps \\(\\phi\\) to improve linear attention modeling quality. [20] originally use the _positive elu_ function \\(1+\\text{elu}\\) such that \\(\\phi(\\mathbf{q})^{\\top}\\phi(\\mathbf{k})\\) remains positive and attention weights remain affine. [23] instead use the ReLU function combined with a cosine-based reweighting function to add a locality bias. Other approaches propose feature maps that aim to approximate the Softmax, such as Random Fourier Features [21, 22] the Nystrom method [49, 50], or deterministic low-degree polynomial approximations [13, 14, 51]. Finally, recent works treat the feature map as a learnable function [25], and optionally train the feature map explicitly to recover the softmax kernel [13].\n' +
      '\n' +
      'Combining sparse and linear attentionsFinally, another line of works similar in spirit to Based combine sparse and linear attentions. Scatterbrain [52] use a sparse and low-rank decomposition to unify past sparse and low-rank approximations. They theoretically and empirically show that any low rank approximation of attention\'s \\(\\exp(QK^{T})\\) will have a much larger approximation error than a sparse plus low rank approximation, echoed in our observations. TransNormer [53] apply normalizations such as LayerNorm [54] or RMSNorm [55] to linear attention outputs in certain layers, and apply softmax attention in local chunks in other layers.\n' +
      '\n' +
      '#### a.1.2 Attention Alternatives\n' +
      '\n' +
      'We now review other attention alternatives, which focus on improving upon the quadratic scaling of attention. Initial work in this vein uses linear time invariant state space models (SSMs) or long convolutions, which can efficiently process sequences of length \\(N\\) in \\(O(N\\log N)\\) time invoking the FFT-convolution theorem [56], as the sequence mixer [4, 27, 31, 57, 58, 59, 60, 61]. SSMs can also be rewritten as recurrences to permit fast \\(O(1)\\) inference.\n' +
      '\n' +
      'Subsequent work identified that the long convolution alone is not expressive enough to perform particular sub-tasks in language modeling. Prior work shows pure linear SSMs cannot perform associative recall, a skill that is correlated with a model\'s in-context learning capability [3, 62], and introduces multiplicative interactions (via gating or Hadamard product [63]) between tokens to allow the model to compare tokens in the sequence [7, 8, 9]. However, Arora et al. [1] show empirically and theoretically the class of gated convolution architectures, any architectures built from the two gating and convolution primitives, struggles to learn associative recall (on synthetic and real language data) as efficiently as attention. They show that while attention solves AR in constant many layers / with model dimension that is independent of sequence length, any gated convolution architecture uses dimensionality that scales with the sequence length -- we build upon their upper bound theoretical results with a lower bound argument in Section 3.2. We also study a broader set of architectures in this work beyond gated convolutions.\n' +
      '\n' +
      'Arora et al. [1], Gu and Dao [5], Yang et al. [6] identify that the use of _input-dependent_ sequence mixers is important for an architecture to perform AR as efficiently as attention. AR requires shifting information that appears prior in a sequence to interact with the current (last) tokens in the sequence, in order to predict the next token [9]. While gating is one way to introduce data-dependence [7], allowing comparing tokens in two (e.g. a shifted and unshifted) sequences, it is difficult to _select which information_ from the prefix of the sequence to shift forwards in the first place, using gating alone. Intuitively, the information to shift _depends on the input\'s properties_. Thus, several subquadratic architectures consider alternate strategies to introduce input-dependence [5, 61, 64, 24, 6]. We present another strategy for efficient input-dependent sequence mixing in our work.\n' +
      '\n' +
      '### Efficient Implementations\n' +
      '\n' +
      'Beyond designing new model architectures, various works introduce systems-level innovations to improve training and inference efficiency. These include alternative implementations of architecture primitives such as attention [11, 65, 66], long convolutions [31, 67], and linear attention [6, 20]. They frequently achieve both reduced memory and increased computational speed on modern GPUs by "fusing" operations such as matrix multiplications into a single CUDA kernel, and designing "IO-aware" ways to distribute and compute the results of various read and write operations between different levels of GPU memory.\n' +
      '\n' +
      '#### a.2.1 Efficient Attention Implementations\n' +
      '\n' +
      '[12] introduce FlashAttention, an alternative yet exact implementation of softmax attention that improves memory and speed by both fusing attention operations into a single CUDA kernel and distributing the attention operations to better exploit High Bandwidth Memory (HBM) and Static Random Access Memory (SRAM). They first compute attention\'s query-key-value dot-products, masking, and softmax, together as a single kernel. By doing so after a single load to SRAM before moving the output back to HRAM, they exploit SRAM\'s fast compute and reduce the total number of read-write operations. To get around SRAM\'s small memory size and avoid attention\'s quadratic memory size over input sequence length, they use _tiling_ to split up the query, key, and value inputs into smaller "blocks", compute the attention operations for each block, and adjust the outputs after computing all blocks to properly normalize the softmax [68, 69]. To perform backpropagation fast on SRAM, they get around SRAM\'s limited storage by _recomputing_ the gradients rather than storing them. Despite the extra operations, this IO-aware implementation still significantly improves wall-clock time during training.\n' +
      '\n' +
      'Similarly making use of block-wise computation, [65] instead compute attention blocks across different _devices_ in RingAttention, enabling training and inference over much larger context lengths that scale with device count. They distribute and compute the attention operations in each block across multiple hosts in parallel, likewise keeping track of summary statistics to gather results correctly into exact attention. However, they introduce an "overlapping" mechanism to coordinate communication of blocks to reduce overhead. They further make use of Blockwise Parallel Transformers [70] to reduce memory, which similar to FlashAttention removes the quadratic in memory scaling of attention by dividing the attention operation into separate blocks before gathering back the adjusted softmax output with block-wise normalization statistics.\n' +
      '\n' +
      'As a complement to attention training and inference, [66] improve attention generation with PagedAttention. PagedAttention similarly uses block-wise computation to address memory utilization issues during generation, where the KV cache can grow an undetermined amount. Existing systems may naively handle this by pre-allocating large amounts of contiguous memory. However, this can result in low utilization and computational bottlenecks. Accordingly, PagedAttention divides attention\'s growing KV cache into _KV blocks_ that can be stored separately on physical memory. This enables more flexible memory management, where smaller chunks can be allocated in different locations when needed to reduce memory-based bottlenecks.\n' +
      '\n' +
      'In Based, we use similar blocking strategies to more efficiently compute both the second-order Taylor series linear attention and the sliding window softmax attention, and for both training and inference.\n' +
      '\n' +
      '#### a.2.2 Efficient Attention-Alternative Implementations\n' +
      '\n' +
      'Beyond optimizations for attention, various works also introduce similar "IO-aware" implementations to improve memory usage and speed for convolutional and recurrent operations. We overview the most relevant works to Based, which make use of similar techniques such as fusing operations and blocking (tiling) to compute results in SRAM.\n' +
      '\n' +
      'Long convolutions[67] improve the efficiency of long convolutions on modern GPUs. They build on using the Fast Fourier Transform (FFT), which enables computing convolutions with filter sizes equal to input sequence length from \\(\\mathcal{O}(N^{2})\\) (if \\(N\\) is filter size and sequence length) to \\(\\mathcal{O}(N\\log N)\\). However, to compute this algorithm efficiently on GPUs, they break down the convolution into separate matrix multiply operations via a _Monarch_ decomposition of the FFT, which allows both (1) fusing multiple steps of the FFT together (for reduced read-write operations) and (2) scheduling these operations for fast computation in SRAM while remaining under the smaller SRAM memory constraints.\n' +
      '\n' +
      'Recurrence[5] improve the efficiency of recent neural state-space models (SSMs) [27] using several similar techniques to FlashAttention, specifically with regard the recurrent view. They load the SSM parameters into SRAM for computation before saving results back in HBM, and also use _recomputation_ where during backpropagation the intermediate states are not saved but rather recomputed when inputs are loaded from HBM to SRAM. They finally improve wall-clock time by parallelizing the recurrent view of the SSM as a parallel scan.\n' +
      '\n' +
      'Linear AttentionFinally, several works propose techniques to improve the real-world wall-clock time and memory-usage of linear attention. [20] fuse several operations in the causal dot product of linear attention. [6] use blocking to divide the linear attention matrices into SRAM-computable chunks in FlashLinearAttention. As a trade-off between the slow yet memory-efficient RNN view of linear attention and faster but memory-intensive parallel "standard attention" view, they further optimize a "chunk-wise" implementation of linear attention [71]. When processing input sequences, the input is first divided into several non-overlapping chunks, where we save memory by computing "kv states" at the end of each chunk, and save time by computing the tokens in a given chunk in parallel.\n' +
      '\n' +
      'IO Aware Implementations\n' +
      '\n' +
      'In this section, we provide additional details pertaining to the benchmarking experiments and we provide micro-benchmarking results for the individual kernels we contribute, to complement the end-to-end benchmarking results in the Section 6.\n' +
      '\n' +
      '### Forward / Generation Prefill\n' +
      '\n' +
      'BaselinesIn Figure 4, we implement Based using our IO-aware Taylor linear attention Algorithm 1. The baseline approach presented in [13], prior to our kernel, uses the popular linear attention CUDA kernel from Fast Transformers for computing the causal dot product [20, 34]. 5. The listing below shows the baseline implementation for reference (where line 76-77 can be computed using pure PyTorch or the Fast Transformers kernel).\n' +
      '\n' +
      'Footnote 5: [https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/causal_linear_attention.py](https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/causal_linear_attention.py)\n' +
      '\n' +
      'Micro BenchmarkTo complement the end-to-end architecture benchmarks in Section 6, we provide micro benchmark results for only the linear attention forward pass in Figure 5.\n' +
      '\n' +
      '```\n' +
      '1fromeinopsimportrearrange\n' +
      '2importtorch\n' +
      '3fromtorchimportnn\n' +
      '4\n' +
      '5classTaylorExp(nn.Module):\n' +
      '6"""\n' +
      '7Featuremaptocompute2nd-orderTaylorapprox.ofexp(q^Tk/sqrt(d))\n' +
      '8"""\n' +
      '9\n' +
      '10def__init__(self,input_dim,head_dim_idx,temp=None,eps=1e-12):\n' +
      '11super()__init__()\n' +
      '12\n' +
      '13self.input_dim=input_dim\n' +
      '14self.head_dim_idx=head_dim_idx\n' +
      '15self.temp=1.0iftempisNoneelsetemp\n' +
      '```\n' +
      '\n' +
      'Figure 5: Time (ms) for different ways of computing the Taylor linear attention forward pass — using Pure PyTorch (shown in the Listing and introduced in [13]), Fast Transformers kernel (as indicated in the listing) [24, 34], or our Based kernel (Algorithm 1). **(Left)** Varying the batch size at fixed sequence length 1024. **(Right)** Varying the sequence length at fixed batch size 4. **(All)** Benchmarking uses 16 feature dimension, 16 heads, 64 head dimension, and focuses on the _numerator_ of the linear attention. Each point represents the median across 10 iterations is measured on a single NVIDIA H100 GPU. Lines terminate on out-of-memory errors.\n' +
      '\n' +
      'self.eps=eps self.r2=math.sqrt(2) self.rd=math.sqrt(self.input_dim) self.rrd=math.sqrt(self.rd)\n' +
      '2\n' +
      '3defforward(self,x:torch.Tensor):\n' +
      '#Get2nd-orderterms(rearrange(x*x),\'....m->...(mn)\') x2=(x.unsqueezeeze(-1)*x.unsqueezeeze(-2)).flatten(start_dim=-2)/self.r2 term1=torch.ones(x[...,:1].shape).to(x.device) term2=x/self.rrd term3=x2/self.rd\n' +
      '4\n' +
      '5terms=[term1,term2,term3] returntorch.cat(tfortinerms),dim=self.head_dim_idx)\n' +
      '6\n' +
      '7classTaylorLinAttn(nn.Module): def__init__(self): super()__init__() self.d_model=d_model self.feature_dim=16 self.num_heads=16 self.num_key_value_heads=16 self.head_dim=self.d_model//self.num_key_value_heads self.eps=1e-12\n' +
      '8feature_map_kwargs={\n' +
      '9"input_dim":self.feature_dim,"head_dim_idx":-1,"eps":1e-12,"self.feature_map=TaylorExp(**feature_map_kwargs) self.proj_q=nn.Linear( self.d_model,self.feature_dim*self.num_heads,bias=False\n' +
      '10 self.proj_k=nn.Linear( self.d_model,self.feature_dim*self.num_heads,bias=False\n' +
      '11) self.proj_v=nn.Linear( self.d_model,self.num_key_value_heads*self.head_dim,bias=False\n' +
      '12) self.proj_o=nn.Linear( self.num_heads*self.head_dim,self.d_model,bias=False\n' +
      '13)\n' +
      '14\n' +
      '15defforward(self,hidden_states:torch.Tensor,*args,**kwargs):\n' +
      '16b,l,_=hidden_states.size() q=self.proj_q(hidden_states)\n' +
      '17x=self.proj_k(hidden_states) v=self.proj_y(hidden_states)\n' +
      '18q=q.view(b,l,self.num_heads,self.feature_dim).transpose(1,2)\n' +
      '19k=k.view(b,l,self.num_key_value_heads,self.feature_dim).transpose(1,2)\n' +
      '20v=v.view(b,l,self.num_key_value_heads,self.head_dim).transpose(1,2)\n' +
      '21\n' +
      '22\n' +
      '23#Linearattention q,k=self.feature_map(q),self.feature_map(k) q,k,v=q.unsqueezeeze(-2),k.unsqueezeeze(-2),v.unsqueezeeze(-1)\n' +
      '23\n' +
      '24#Computeattentioncausal(alternativelyusetheFastTransformerskernel) num=(q*(k*v).cumsum(dim=2)).sum(dim=-1) denom=(q*k.cumsum(dim=2)).sum(dim=-1)+self.eps y=(num/denom) y=rearrange(y,"bhld->b1(hd)") y=self.proj_o(y) returny ```\n' +
      '\n' +
      'Listing 1: PyTorch implementation of Taylor linear attention.\n' +
      '\n' +
      '``` Input: Input projected hidden states \\(q,k,v\\in\\mathbb{R}^{N\\times d}\\). Output: Output \\(y=T0+T1+T2\\in\\mathbb{R}^{N\\times d}\\). Parallelize into batch \\(\\times\\) heads parallel computations, with n\\({}_{\\text{warps}}=8\\) warps per block. Within a block: Define tile size \\(T\\) \\(\\triangleright\\)\\(T=16\\) in Based Define n\\({}_{\\text{tiles}}=\\frac{N}{T}\\) \\(\\triangleright\\) Block along the sequence dimension Define n\\({}_{\\text{blocks}}=\\text{n}_{\\text{tiles}}/\\text{n}_{\\text{warps}}\\)\\(\\triangleright\\) Block along the number of warps Define \\(\\text{tic}=0\\), \\(\\text{toc}=1\\)\\(\\triangleright\\) Flags for asynchronous data loading Create SRAM buffers \\(B_{q}\\), \\(B_{k}\\) (Size \\(2\\times\\text{n}_{\\text{warps}}\\times T\\times T\\)) and \\(B_{v}\\) (Size \\(2\\times\\text{n}_{\\text{warps}}\\times T\\times 4T\\)) Create SRAM buffers A0, A1, A2 (Size n\\({}_{\\text{warps}}\\times T\\times 4T\\)) for storing interim. results for \\(T0,T1,T2\\) as warps process the sequence Create SRAM buffers \\(total_{A0}\\) and \\(total_{A1}\\) to hold cumulative ("KV") state corresponding to \\(T0,T1\\) Create SRAM buffers \\(y\\) of (Size n\\({}_{\\text{warps}}\\times T\\times 4T\\)) for storing the final output Create register fragments \\(\\text{q}_{\\text{a}},\\text{q}_{\\text{b}},\\text{k}_{\\text{a}},\\text{k}_{\\text{ b}},\\text{q}_{\\text{frag}},\\text{k}_{\\text{facum}}\\) of size \\(16\\times 16\\). We create register fragments \\(\\text{v}_{\\text{frag}},\\text{a}0_{\\text{frag}}\\), \\(\\text{a}1_{\\text{accum}}\\), \\(A2_{0}\\), \\(A2_{1}\\), \\(\\text{q}2A_{\\text{accum}}\\), \\(\\text{o}_{\\text{accum}}\\) of size \\(16\\times 64\\). These fragments are for holding data during in-register computation. Initialize the fragments to \\(0\\). Each warp loads initial tiles \\(B_{q}[\\text{tic}][\\text{warpid}]\\gets Q_{t},B_{k}[\\text{tic}][\\text{warpid }]\\gets K_{t}\\) and \\(B_{v}[\\text{tic}][\\text{warpid}]\\gets V_{t}\\triangleright\\) HBM into SRAM for \\(\\text{cur}_{\\text{block}}\\in[0..n_{\\text{blocks}}-1]\\); \\(\\text{tic}=0\\oplus=1\\), \\(\\text{toc}\\oplus 1\\)do\\(\\triangleright\\) XORs tic andtoc to toggle.  Warp loads \\(B_{q}[\\text{toc}][\\text{warpid}]\\gets Q_{t}\\) for \\(\\text{cur}_{\\text{block}}+1\\)\\(\\triangleright\\) HBM to SRAM  Warp loads \\(B_{k}[\\text{toc}][\\text{warpid}]\\gets K_{t}\\) for \\(\\text{cur}_{\\text{block}}+1\\)\\(\\triangleright\\) Warp loads \\(B_{v}[\\text{toc}][\\text{warpid}]\\gets V_{t}\\) for \\(\\text{cur}_{\\text{block}}+1\\)\\(\\triangleright\\) Warp loads \\(\\text{q}_{\\text{frag}}\\gets q[\\text{tic}][\\text{warpid}]\\)\\(\\triangleright\\) SRAM into register  Warp loads k\\({}_{\\text{frag}}\\gets k[\\text{tic}][\\text{warpid}]\\)\\(\\triangleright\\) RSM into register  Warp loads v\\({}_{\\text{frag}}\\gets v[\\text{tic}][\\text{warpid}]\\)\\(\\triangleright\\) RSM toH  Compute the warp-local cumulative sum on v\\({}_{\\text{frag}}\\to\\text{a}0_{\\text{frag}}\\). \\(\\triangleright\\) T0 computation Add the running \\(A0\\) to the current a0\\({}_{\\text{frag}}\\) Compute q\\({}_{\\text{frag}}\\)\\({}_{T}\\) (attention) and make it causal and store in a q\\({}_{\\text{accum}}\\)\\(\\triangleright\\) T1 computation Compute q\\({}_{\\text{kaccum}}\\)v\\({}_{\\text{frag}}\\to\\text{o}_{\\text{accum}}\\)\\(\\triangleright\\) Store causal \\(qk^{T}v\\) Warps store k\\({}_{\\text{frag}}^{T}\\)v\\({}_{\\text{frag}}\\to\\text{a}1_{\\text{accum}}\\) and write a1\\({}_{\\text{accum}}\\to A1[\\text{warpid}]\\)\\(\\triangleright\\) Register to SRAM  Compute cumulative sum over \\(A1\\) in SRAM, updating \\(A1\\) entries Warps read \\(A1\\) tiles back to registers \\(\\triangleright\\) Each warp now contains its preceeding \\(A1\\) Warps multiply the values in register with q\\({}_{\\text{frag}}\\) to update \\(\\to\\text{o}_{\\text{accum}}\\)\\(\\triangleright\\) Add in T1 to the running result Update a0\\({}_{\\text{frag}}\\to\\text{o}_{\\text{accum}}\\)\\(\\triangleright\\) Add in T0 to the running result Square q\\({}_{\\text{kaccum}}\\), multiply with v\\({}_{\\text{frag}}\\) and add \\(\\to\\text{o}_{\\text{accum}}\\)\\(\\triangleright\\) Add in diagonal T2 to the running result Sum the values of o\\({}_{\\text{accum}}\\) into \\(y[\\text{warpid}]\\) for block in n\\({}_{\\text{warps}}\\) iterations do\\(\\triangleright\\) Remaining T2 computation; Assumes feature dimension 16  Each of 8 warps copies the _same_ slice of \\(q[\\text{tic}][\\text{warpid}]\\) to 2 registers \\(q_{a}\\), \\(q_{b}\\)  Each thread \\(j\\) in the warp computes \\(q_{a}[;.2j]q_{a}\\) for dimension \\(2j\\), and for \\(2j+1\\) (and for \\(q_{b}\\)). Together the threads compute the 256 elements resulting from the second order outer product in the feature map.  Each warp stores two slices of \\(A2\\): \\(A2_{0}\\) and \\(A2_{1}\\)\\(\\triangleright\\) Partitioning the large \\(A2\\) across warp registers Accumulate both \\(q_{a}A2_{0}\\) and \\(q_{b}A2_{1}\\to\\text{q}\\text{A}2_{\\text{accum}}\\)  Warp writes qA2\\({}_{\\text{accum}}\\to A2[\\text{warpid}]\\)\\(\\triangleright\\) Register to SRAM Sum results across all in \\(A2[\\text{warpid}]\\) and store the sum in \\(y[\\text{block}]\\)\\(\\triangleright\\) Add in T2 Each of 8 warps copies the _same_ slice of \\(k[\\text{tic}][\\text{block}]\\) to 2 registers \\(k_{a}\\), \\(k_{b}\\)\\(\\triangleright\\) KV state update Square \\(k_{a}\\) and \\(k_{b}\\)  Each of the 8 warps loads \\(v[\\text{tic}][\\text{block}]\\) to v\\({}_{\\text{frag}}\\) in register  Multiply \\(k_{a}\\) and v\\({}_{\\text{frag}}\\), \\(k_{b}\\) and v\\({}_{\\text{frag}}\\) and accumulate the results into \\(A2_{0}\\) and \\(A2_{1}\\), the two in-register slices of \\(A2\\) for the warp, respectively End. Store \\(y\\). Optionally store \\(A0\\), \\(A1\\), \\(A2\\) (comprising the "KV state") for generation. \\(\\triangleright\\) SRAM to HBMAlgorithmHere we revisit the key equations we aim to compute and then describe Algorithm 1 in detail.\n' +
      '\n' +
      '_Objective_ First recall from Section 4:\n' +
      '\n' +
      '\\[\\mathbf{o}_{i}=\\sum_{j=1}^{i}\\frac{\\phi(\\mathbf{q}_{i})^{\\top}\\phi(\\mathbf{k}_{j})\\mathbf{v}_{j}}{ \\phi(\\mathbf{q}_{i})\\sum_{j=1}^{i}\\phi(\\mathbf{k}_{j})}=\\frac{\\phi(\\mathbf{q}_{i})\\sum_{j=1} ^{i}\\left(\\phi(\\mathbf{k}_{j})^{\\top}\\mathbf{v}_{j}\\right)}{\\phi(\\mathbf{q}_{i})\\sum_{j=1} ^{i}\\phi(\\mathbf{k}_{j})} \\tag{5}\\]\n' +
      '\n' +
      'where \\(q_{i}\\) reflects the \\(i^{th}\\) of \\(N\\) total tokens in the sequence and every query attends to every past key in \\(\\mathcal{O}(Nd^{2})\\) time and space complexity for embedding dimension \\(d\\).\n' +
      '\n' +
      'To approximate \\(\\exp(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}/\\sqrt{d})\\), we use the \\(2^{\\text{nd}}\\)-order Taylor series feature map, picking \\(\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d^{2}}\\) such that\n' +
      '\n' +
      '\\[\\phi(\\mathbf{q}_{i})^{\\top}\\phi(\\mathbf{k}_{j})=1+\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}+\\frac{( \\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j})^{2}}{2} \\tag{6}\\]\n' +
      '\n' +
      'In this section, we will refer to \\(q_{i}\\) as a **tile** of data (e.g. of 16 tokens) instead of as a single token since the hardware operates on chunks of data in parallel.\n' +
      '\n' +
      '_Algorithm description_ In Algorithm 1, we allow each thread block to compute the result for a particular \\((\\text{batch},\\text{head})\\) input. Within the thread block, we use 8 warps / workers to produce the result. We initialize data structures \\(B_{q},B_{k},B_{v}\\) in SRAM and \\(\\text{q}_{\\text{a}},\\text{q}_{\\text{b}},\\text{k}_{\\text{a}},\\text{k}_{\\text{b} },\\text{q}_{\\text{frag}},\\text{k}_{\\text{frag}},\\text{v}_{\\text{frag}}\\) in register to hold chunks or _tiles_ of the \\(q,k,v\\) inputs. We initialize data structures \\(A0,A1,A2\\) in SRAM and \\(\\text{a}0_{\\text{frag}}\\), \\(\\text{a}1_{\\text{accum}}\\), \\(\\text{q}\\text{A}2_{\\text{accum}}\\) in register to hold computation for the running \\(KV\\) state for the \\(0^{th},1^{st},2^{nd}\\) order Taylor polynomial terms.\n' +
      '\n' +
      'We partition the computation along the sequence dimension into \\(\\text{n}_{\\text{blocks}}\\), where in each loop from 1 to \\(\\text{n}_{\\text{blocks}}\\), the warps load the next 8 chunks into fast memory. Note that for 2048 sequence length and 8 warps, 16 tile size, we end up with \\(\\text{n}_{\\text{tiles}}=128\\) and \\(\\text{n}_{\\text{blocks}}=16\\). In each iteration, each warp loads in \\(16\\times 16\\) tiles of \\(q,k\\) and \\(16\\times 64\\) tiles of \\(v\\), where 16 indicates a chunk of 16 tokens along the sequence dimension and \\(16,64\\) are the feature and head dimensions respectively. Once tiles are streamed in, we do not need to reuse them, which is key to the efficiency of linear attention.\n' +
      '\n' +
      '**Zeroeth order Taylor terms:** During the computation, for the \\(0^{th}\\) term in the Taylor polynomial, \\(q,k\\) are 1 after we apply the feature map (Equation (6)). Therefore, computing a cumulative sum over \\(q(k^{T}v)\\) reduces to maintaining a cumulative sum of \\(v\\) as we iterate across the sequence.\n' +
      '\n' +
      '**First order Taylor terms:** Next we consider the \\(1^{st}\\) order terms. **On-diagonal:** First consider the on-diagonal blocks, e.g. with respect to tiles \\(q_{i},k_{i},v_{i}\\). For these, we simply multiply \\(q^{T}k\\), masking (making it causal), and then multiplying with \\(v\\), following the order of operations in standard attention. This makes it easy to apply the masking (0 out non-causal elements). Now each warp contains a local result for its set of on-diagonal tiles of \\(q_{i},k_{i},v_{i}\\). **Off-diagonal:** However, we need to obtain a _global_ cumulative sum where \\((q_{i}^{T}k_{j})v_{j}\\) depends on all \\(j\\in[1..i]\\) (Equation (5)). Each warp is therefore missing values for tiles \\(j\\in[1..i-1]\\). To incorporate this computation, we will now compute the cumulative \\(KV\\) hidden state for the warp up until \\(i-1\\) and multiply this with the local tile of \\(q\\) (i.e. \\(\\text{q}_{\\text{frag}}\\)). To accomplish this, note in Algorithm 1, we multiply \\(\\text{k}_{\\text{frag}}{}^{T}\\) and \\(\\text{v}_{\\text{frag}}\\) to compute local tiles of the hidden state, local to each warp, in thread register. To perform the global cumulative sum across the 8 warps\' local results, we write from registers (thread specific) to \\(A1\\) in SRAM (shared across warp threads). After computing the global cumulative sum in shared memory, each warp loads back the \\(KV\\) state (in \\(A1\\)) into its registers such that it contains all the preceeding \\(KV\\) (history) for tiles \\([1..i-1]\\). We then multiply the local \\(\\text{q}_{\\text{frag}}\\) in register with this \\(KV\\) state to update the final output for the \\(1^{st}\\) up until the current \\(\\text{n}_{\\text{blocks}}\\). Note that we maintain the running \\(KV\\) state corresponding to the \\(1^{st}\\) order term in \\(A1\\) shared memory for the next iteration along \\(\\text{n}_{\\text{blocks}}\\).\n' +
      '\n' +
      '**Second order Taylor terms:** We finally need to compute the \\(2^{nd}\\) order term. Similar to the \\(1^{st}\\) order term, we\'ll consider **On-diagonal:** We can leverage the computation from above. We\'ll square the causal \\((qk^{T})^{2}\\) from above and multiply with \\(\\text{v}_{\\text{frag}}\\) to obtain the _portion_ of the \\(2^{nd}\\) order term corresponding to the on-diagonal tiles \\(q_{i},k_{i},v_{i}\\). **Off-diagonal:** Again, we also need to compute the result with respect to tiles \\([1..i-1]\\).\n' +
      '\n' +
      '* **Partitioning KV hidden state for \\(2^{nd}\\) order** Because the hidden state for the second order term is large (\\(\\mathcal{O}(d^{2}D)\\) in feature dimension \\(d\\) and head dimension \\(D\\)) and warps have a limited number of registers, we slice its storage across the registers of the 8 warps. Considering the the \\(16^{2}\\times 64\\) (\\(d^{2}\\times D\\)hidden state (stored in \\(A2\\) SRAM in Algorithm 1), we divide this into 16 slices along the sequence dimension and let each of the 8 warps handle 2 of the \\(16\\times 64\\) slices (stored in \\(A2_{0},A2_{1}\\) fragments in thread registers in Algorithm 1). Warp \\(i\\) will maintain slices \\(2i\\) and \\(2i+1\\) in two registers per thread.\n' +
      '* **Computing output for \\(2^{nd}\\) order** Each warp \\(i\\) loads in one tile of \\(q_{i}\\) into 2 registers. We will use the 32 threads in the warp to compute the 256 outer product terms for each token computed by the Taylor \\(2^{nd}\\) order term (for feature dimension 16). Next, the threads multiply these 256 terms with the running \\(A2_{0}\\) and \\(A2_{1}\\) slices. The results for the two slices are summed in register and then stored in SRAM (\\(A2\\)[warpid]). Since \\(o_{i}\\) is ultimately the sum of \\(q_{i}\\) terms multiplied with _all_ slices of \\(A2\\) (Equation (5)), we then sum the results from all the warps together (which hold the remaining slices of \\(A2\\)) and store the result in \\(y\\)[block]. We can think of \\(y\\)[block] as holding the result up until the \\((8\\times\\mathrm{cur}_{\\mathrm{block}}+\\mathrm{block})\\) tile of tokens (note 8 is because in each increment of \\(\\mathrm{cur}_{\\mathrm{block}}\\), the 8 warps handle 8 different tiles of the sequence).\n' +
      '* **Updating the \\(KV\\) state:** For block \\(=i\\), we load in \\(k[i],v[i]\\) tiles of size \\(16\\times 16\\) and \\(16\\times 64\\) respectively to registers \\(k_{a},k_{b},\\mathrm{v}_{\\mathrm{frag}}\\). We compute the 256 outer product terms on \\(k[i]\\) using the 32 threads, multiply with \\(\\mathrm{v}_{\\mathrm{frag}}\\), and store the result in the \\(A2_{0},A2_{1}\\) running state.\n' +
      '\n' +
      'The final result in \\(y\\) is summed into the output to complete the \\(2^{nd}\\) order computation.\n' +
      '\n' +
      '### Next Token Prediction\n' +
      '\n' +
      'During next token prediction in generation, we use IO-aware algorithms for the expensive \\(KV\\) state update in Taylor linear attention and for the sliding window attention computation.\n' +
      '\n' +
      '#### b.2.1 Taylor linear attention\n' +
      '\n' +
      'The KV update in PyTorch is provided in the following listing. In Figure 6 we benchmark the speed of the PyTorch implementation against our kernel.\n' +
      '\n' +
      '```\n' +
      '1fromeinopsimportrearrange\n' +
      '2importtorch\n' +
      '3fromtorchimportnn\n' +
      '4\n' +
      '5defstep(self,kv_state:torch.Tensor,k_state:torch.Tensor,q:torch.Tensor,k:torch.Tensor,v:torch.Tensor);\n' +
      '6"""\n' +
      '7Computelinearattentionwithrecurrentview\n' +
      '8->Assumeq.shapeis(b,h,1,D);kandv.shapeare(b,h,1,d),whereDisthedimensionafterapplyingthefeaturemapanddistheheaddimension.\n' +
      '9"""\n' +
      '10b,h,1,d=q.shape\n' +
      '11assert1==1,f\'q.shapeis(q.shape)butshouldbe((b),{h},1,{d})\'\n' +
      '12#Expanddimsforbroadcastingtopputelinearettention\n' +
      '13q,k,v=q.unsqueezeeze(-2),k.unsqueezeeze(-2),v.unsqueezeeze(-1)\n' +
      '14\n' +
      '15kv_state+=k[:,:,-1:]*v[:,:,-1:]\n' +
      '16k_state+=k[:,:,-1:]\n' +
      '17\n' +
      '18#Computelinearattention\n' +
      '19num=(q*kv_state).sum(dim=-1)\n' +
      '20y=num/((q*k_state).sum(dim=-1)+self.eps)\n' +
      '21\n' +
      '22y=rearrange(y,\'bh1d->b1(hd)\').to(q.dtype)\n' +
      '23returnself.dropout(self.out_proj(y))\n' +
      '```\n' +
      '\n' +
      'Listing 2: PyTorch implementation of Taylor linear attention KV update\n' +
      '\n' +
      'Figure 6: Time (ms) for computing the Taylor linear attention recurrent update using Pure PyTorch (shown in the Listing and introduced in [13]) vs. our Based kernel (Algorithm 2). Benchmarking uses 16 feature dimension, 16 heads, 64 head dimension, and focuses on the _numerator_ of the linear attention. Each point represents the median across 10 iterations is measured on a single NVIDIA H100 GPU.\n' +
      '\n' +
      '```\n' +
      '0:\\(KV_{t-1}\\) state \\(\\in\\mathbb{R}^{Hd^{2}d}\\), at time \\(t\\). Featurized \\(q,k\\in\\mathbb{R}^{B\\times H\\times 1\\times D}\\) and \\(V\\in\\mathbb{R}^{B\\times H\\times 1\\times d}\\), for \\(d\\) as the head dimension (e.g. 64) and \\(D\\) as the expanded feature map dimension (e.g. \\(273=1+16+16^{2}\\) for feature dim 16). To be hardware-friendly, we let \\(D=320\\) (s.t. \\(320\\mod 64=0\\)) via padding.\n' +
      '0: Updated \\(KV_{t}\\) state. Parallelize into \\(\\mathrm{batch}\\times\\mathrm{heads}\\) parallel computations, with \\(\\mathrm{n_{warps}}=8\\) warps per block. Within a block:  Define \\(\\mathrm{n_{threads}}=\\mathrm{n_{warps}}\\times 32\\) \\(\\triangleright\\) Assuming 32 threads per warp  Define \\(\\mathrm{buffer_{size}}=\\mathrm{n_{warps}}\\times 8\\times d\\) \\(\\triangleright\\) E.g. \\(\\mathrm{total_{batches}}=5\\) if \\(D=320\\); For \\(k\\), \\(\\frac{320}{5}=64\\) values per batch  Define \\(\\mathrm{tic}=0,\\mathrm{toc}=1\\)  Create SRAM buffer \\(B_{q}\\) (Size \\(D\\)) for \\(q\\)  Create SRAM buffer \\(B_{k}\\) (Size \\(D\\)) for \\(k\\)  Create SRAM buffer \\(B_{v}\\) (Size \\(d\\)) for \\(V\\)  Create SRAM buffer \\(B_{k_{vas}}\\) (Size \\(2\\times\\mathrm{buffer_{size}}\\)) for storing blocks of \\(\\mathrm{kv_{state}}\\)  Create SRAM buffer o (Size \\(d\\)) for output.  Create SRAM buffer A (Size \\(\\mathrm{n_{warps}}\\times d\\)) for intermediate computation  Create register buffer \\(\\mathrm{v_{reg}}\\) (Size 2) to store \\(V\\) data  Create register \\(\\mathrm{A_{reg}}\\) (Size 2) for intermediate computation  Warps load \\(B_{q}\\gets q\\) \\(\\triangleright\\) HBM to SRAM; Load all \\(D=320\\) elements of \\(q\\)  Warps load \\(B_{k}\\gets k\\)  Warps load \\(B_{v}\\gets V\\)  Warps load chunk \\(B_{kvs}[\\mathrm{tic}]\\leftarrow\\mathrm{kv_{state}}\\) \\(\\triangleright\\) Load \\((1\\times 64)\\times 64\\) of the \\((\\mathrm{total_{batches}}\\times 64)\\times 64\\) elements in \\(KV_{t-1}\\)  Initialize \\(m=0\\) for Threads \\(j\\in[0..31]\\); \\(j<d\\); \\(j+=32,m+=1\\)do\\(\\triangleright\\) Each thread holds 2 values (\\(d=64\\); 32 threads)  Load \\(\\mathrm{v_{reg}}[m]\\gets v[j]\\)\\(\\triangleright\\) SRAM to Register; Now \\(v[j]\\) is stored in thread \\(j\\mod 32\\) for\\(i\\in[0..\\mathrm{total_{batches}}]\\); \\(i=i+1\\), \\(\\mathrm{tic}\\oplus 1\\), \\(\\mathrm{toc}\\oplus 1\\)do  Loads \\(B_{kvs}[\\mathrm{toc}]\\leftarrow\\mathrm{next}\\) batch of \\(\\mathrm{kv_{state}}\\) \\(\\triangleright\\) Asynchronous loads of next batch for\\(j=\\) warpid; \\(j<d\\); \\(j+=\\mathrm{n_{warps}}\\)do\\(\\triangleright\\) Each of the 8 warps loads 8 of the 64 _rows_ of \\(k\\), \\(q\\) in the batch \\(\\mathrm{k_{val}}\\gets B_{k}[i*d+j]\\)\\(\\triangleright\\) Grab single rows \\(q[i]\\) and \\(k[i]\\), Broadcast to all threads \\(\\mathrm{q_{val}}\\gets B_{q}[i*d+j]\\) \\(p=B_{kvs}[\\mathrm{tic}]+j*d\\)\\(\\triangleright\\) Point to output rows of \\(KV_{t}\\); We write \\(d\\times\\frac{D}{\\mathrm{total_{batches}}}\\) sub-matrix for this batch  Initialize \\(m=0\\) for Thread \\(k\\in[0..31]\\); \\(k<d\\); \\(k+=32,m+=1\\)do\\(p[k]+=\\mathrm{k_{val}}*\\mathrm{v_{reg}}[m]\\)\\(\\triangleright\\) Update running state by multiplying broadcasted \\(\\mathrm{k_{val}}\\) with the full \\(\\mathrm{v_{reg}}\\) \\(\\triangleright\\) This updates a \\(1\\times d\\) strip of the \\(d\\times D\\) full \\(KV_{t}\\) outer product \\(\\mathrm{A_{reg}}[m]+=\\mathrm{q_{val}}*p[k]\\)\\(\\triangleright\\) Multiply \\(\\mathrm{q_{val}}\\) with the running state, updating all values in the \\(1\\times d\\) output  Write out new \\(KV_{t}\\) state for this batch: \\(B_{kvs}[\\mathrm{tic}][k]\\)\\(\\triangleright\\) SRAM to HBM  Initialize \\(m=0\\) for Threads \\(j\\in[0..31]\\); \\(j<d\\); \\(j+=32,m+=1\\)do\\(\\triangleright\\) Each thread holds info for 2 of the 64 output values  Store \\(A[\\mathrm{warpid}][j]\\leftarrow\\mathrm{A_{reg}}[m]\\)\\(\\triangleright\\) Register to SRAM for Thread \\(j\\); \\(j<d\\); \\(j+=\\mathrm{n_{threads}}\\)do\\(\\triangleright\\)\\(d=64\\) threads put values from first warp in \\(n_{j}\\) \\(n_{j}=A[0][j]\\)\\(\\triangleright\\) Each warp had only computed output values for a subset of (e.g. 8) rows of \\(k\\) and \\(q\\) for\\(w\\in[0..\\mathrm{n_{warps}}]\\)do\\(\\triangleright\\) Need to combine results across warps  Sum the \\(n_{j}+=A[w][j]\\) across  Store \\(o[j]\\gets n_{j}\\)  Write output o \\(\\triangleright\\) SRAM to HBM\n' +
      '```\n' +
      '\n' +
      '**Algorithm 2** Computing \\(KV\\) State Updates\n' +
      '\n' +
      '#### b.2.2 Sliding window attention\n' +
      '\n' +
      'BaselinesDuring training / prefill, we use the Flash Attention sliding window implementation [11].\n' +
      '\n' +
      'Our IO-aware implementation focuses on next token prediction. In the listing below, we include a Torch reference. Our IO-aware sliding window attention algorithm is provided in 3. The key insight is to fuse operations _in thread registers_ to minimize slower SRAM to register data movement.\n' +
      '\n' +
      'Micro BenchmarkWe benchmark key baselines (Torch, Flash Attention-2 [11], and the Based kernel on an NVIDIA H100 GPU in Figure 7. The benchmark uses window size 64, head dimension 64, and number of heads 16. We vary the batch size on the \\(x\\) axis and repeat the median timing across iterations on the \\(y\\) axis. Note that these timings include only the attention computation and not the time for updating the \\(KV\\)-cache state. These timings also do not include any processing for Rotary encodings (as shown below).\n' +
      '\n' +
      '```\n' +
      '1importtorch\n' +
      '2fromtorchimportnn\n' +
      '3\n' +
      '4"""\n' +
      '5b:batchsize\n' +
      '6h:numberofheads\n' +
      '7n:sequencelength\n' +
      '8d:headimension\n' +
      '9\n' +
      '10w:windowsize\n' +
      '11\n' +
      '12qw:bxhx1xd\n' +
      '13kw:bxhxwxd\n' +
      '14vw:bxhxwxd\n' +
      '15"""\n' +
      '16w=torch.einsum("bhod,bhdnd->bhn",qw,kw)\n' +
      '17a=torch.nn.functional.softmax(w,dim=-1)\n' +
      '18result=torch.einsum("bhn,bhd->bhd",a,vw)\n' +
      '```\n' +
      '\n' +
      'Listing 3: PyTorch implementation of Sliding Window\n' +
      '\n' +
      'Figure 7: Time (ms) for different ways of computing sliding window attention next token prediction — using PyTorch, Flash Attention (which supports a sliding window function), or our inference kernel. Each point represents the median across query tokens at different token positions in the generation \\(\\in\\{100,250,500,750\\}\\).\n' +
      '\n' +
      '```\n' +
      '0:\\(KV_{t-1}\\) state \\(\\in\\mathbb{R}^{Hwd}\\), at time \\(t\\) and projected hidden states \\(q,k,v\\in\\mathbb{R}^{B\\times H\\times 1\\times d}\\), for \\(H\\) heads, head dimension \\(d\\), sliding window size \\(w\\), and batch size \\(B\\).\n' +
      '0: Updated \\(KV_{t}\\) state. Parallelize into batch \\(\\times\\) heads parallel computations, with n\\({}_{\\text{warps}}=4\\) warps per block. Within a block: Define tile size \\(T\\) \\(\\triangleright\\)\\(T=16\\) in Based Define n\\({}_{\\text{threads}}=\\) n\\({}_{\\text{warps}}\\times 32\\)\\(\\triangleright\\) Assuming 32 threads per warp Create SRAM buffers \\(B_{k}\\) and \\(B_{v}\\) (Each of size \\(4T\\times 4T\\)) to hold \\(k,v\\). \\(\\triangleright\\) Assumes \\(4T=64\\) is the \\(w\\), \\(d\\) Create SRAM vector \\(B_{q}\\) (Size \\(1\\times 4T\\)) to hold \\(q\\) during the kernel execution. \\(\\triangleright\\) Single query, assume \\(d=64\\) Create SRAM vector \\(B_{w}\\) (Size \\(1\\times 4T\\)) of type float for intermediate attention computation. Create SRAM vector \\(B_{o}\\) (Size \\(1\\times 4T\\)) to hold the output. \\(\\triangleright\\) Single output, assume \\(d=64\\) Create SRAM buffers max and sum (Each of workers by float size). Create register fragments q\\({}_{\\text{reg}}\\), k\\({}_{\\text{reg}}\\), v\\({}_{\\text{reg}}\\) to hold data during fused computation in-register. Create register fragments w\\({}_{\\text{reg}}\\) (size \\(1\\times 4T\\)) and w\\({}_{\\text{reg}}\\) (size \\(4T\\times 1\\)) to store intermediate computation in-register. Create register fragment o\\({}_{\\text{reg}}\\) (size \\(4T\\times 1\\)) to store output in-register. Loads \\(B_{k}\\gets k\\) using n\\({}_{\\text{threads}}\\); \\(B_{v}\\gets v\\) using n\\({}_{\\text{threads}}\\); \\(B_{q}\\gets q\\) using one warp. \\(\\triangleright\\) HBM to SRAM Loads q\\({}_{\\text{reg}}\\gets B_{q}\\). \\(q\\) gets broadcasted to all warps. \\(\\triangleright\\) SRAM to Register Loads k\\({}_{\\text{reg}}\\gets B_{k}[\\text{warpid}]\\). Each warp gets \\(T\\times 4T\\) of the \\(4T\\times 4T\\) in \\(B_{k}\\) (_i.e._ a column). Loads v\\({}_{\\text{reg}}\\gets B_{v}[\\text{warpid}]\\). Each warp gets \\(T\\times 4T\\) of the \\(4T\\times 4T\\) in \\(B_{v}\\) (_i.e._ a column). Initialize w\\({}_{\\text{reg}}\\) to zero \\(\\triangleright\\) Matrix-vector (GEMV) multiplication \\(\\text{w}_{\\text{reg}}\\leftarrow\\) q\\({}_{\\text{reg}}\\)k\\({}_{\\text{reg}}\\) Initialize float \\(m=-\\infty\\) for the max \\(\\triangleright\\) Obtain the max across tiles for Softmax Update \\(m\\leftarrow\\) max(w\\({}_{\\text{reg}}\\)) with the max from the local data \\(\\text{max}[\\text{warpid}]\\gets m\\) for all warps to access Iterate over n\\({}_{\\text{warps}}\\) entries in max buffer to compute the global max of w\\({}_{\\text{reg}}\\) Put global max back into each warp\'s \\(m\\) float Initialize float \\(s=0\\) for the sum \\(\\triangleright\\) Obtain the sum across tiles for Softmax Update \\(s\\leftarrow\\) sum(w\\({}_{\\text{reg}}\\)) with the sum from the local data sum[warpid] \\(\\leftarrow\\)\\(s\\) for all warps to access Iterate over n\\({}_{\\text{warps}}\\) entries in sum buffer to compute the global sum of w\\({}_{\\text{reg}}\\) Put global sum back into each warp\'s \\(s\\) float \\(\\text{w}_{\\text{reg}}\\leftarrow\\) w\\({}_{\\text{reg}}-m\\)\\(\\triangleright\\) Start attention computation in register \\(\\text{w}_{\\text{reg}}\\leftarrow\\) exp(w\\({}_{\\text{reg}}\\)) \\(\\text{w}_{\\text{reg}}\\leftarrow\\frac{\\text{w}_{\\text{reg}}}{s}\\)\\(\\triangleright\\) Register to SRAM; storing for the slice of \\(k\\) w\\({}_{\\text{reg}}\\gets B_{w}\\)\\(\\triangleright\\) SRAM to Register. Warp loads entirety of \\(B_{w}\\); all slices Initialize o\\({}_{\\text{reg}}\\) to zero. \\(\\triangleright\\) Matrix-vector (GEMV) multiplication \\(\\text{o}_{\\text{reg}}\\leftarrow\\) w\\({}_{\\text{reg}}\\)v\\({}_{\\text{reg}}\\) Write o\\({}_{\\text{reg}}\\) to global memory\\(\\triangleright\\) Register to SRAM, SRAM to HBMExtended Architecture Details\n' +
      '\n' +
      'In this section, we describe two additional architectural details that can enable **small** improvements in language model perplexity. We emphasize, however, that the combination of Taylor linear attention and tcWindow layers alone is sufficient to come within 0.1 perplexity points of our best models using these additional components (Table 4).\n' +
      '\n' +
      'Convolution.We find that replacing some of the linear attention and tcWindow layers with gated convolutions [1, 9, 72] enables small improvements in language modeling performance. We use BaseConv layers [1] with short convolutions and a SiIU non-linearity [73]. By keeping the convolutions short (_e.g._ width 3), we keep the recurrent state size low and improve throughput. The projections expand the dimensionality by a factor \\(c=4\\). Note, this layer is nearly identical to a Mampa block _without_ the SSM [5].\n' +
      '\n' +
      '\\[\\mathbf{y}\\coloneqq(\\underbrace{(\\mathbf{u}\\cdot\\mathbf{W}_{1}+\\mathbf{b}_{1})}_{\\text{ Linear Projection}}\\odot\\sigma\\underbrace{(\\mathbf{h}*\\mathbf{u}\\cdot\\mathbf{W}_{2}+\\mathbf{b}_{2})}_{\\text{ Convolution}})\\cdot\\mathbf{W}_{3}+\\mathbf{b}_{3} \\tag{7}\\]\n' +
      '\n' +
      'where \\(\\mathbf{u}\\in\\mathbb{R}^{N\\times d}\\) is a projected input, \\(h\\in\\mathbb{R}^{N\\times cd}\\) is a learned filter, \\(\\odot\\) is the Hadamard product, and \\(\\mathbf{W}_{1},\\mathbf{W}_{2}\\in\\mathbb{R}^{d\\times cd}\\), \\(\\mathbf{W}_{3}\\in\\mathbb{R}^{cd\\times d}\\), \\(\\mathbf{b}_{1},\\mathbf{b}_{2}\\in\\mathbb{R}^{cd}\\), and \\(\\mathbf{b}_{3},\\in\\mathbb{R}^{d}\\) define weights and biases of three linear projections.\n' +
      '\n' +
      'Decay.Recent recurrent architectures include the use of _decay_ terms, implemented in a variety of ways [5, 6, 27, 28]. As intuition, decay terms control how much a token should attend to "recent" tokens vs. "early" tokens in the sequence. Prior work falls in two categories: using input-independent [27, 28, inter alia.] or input-dependent [5, 6] decay rates. The latter offers improved quality, but requires the use of a parallel scan during sequence processing [5]. Instead, we introduce a coarser input-dependent decay technique for the linear attention layer, avoiding the parallel scan. We start with decay matrices that use a _fixed_ decay rate as in prior work and let each of the \\(h\\) heads in the linear attention apply a unique decay schedule to the causal dot product. We then introduce a learned projection \\(\\mathbf{W}\\in\\mathbb{R}^{d\\times h}\\) that effectively allows each input to weigh the combination of heads (decay rates) when producing the output representation.\n' +
      '\n' +
      '## Appendix D Extended Results\n' +
      '\n' +
      'Downstream Language ResultsTo further evaluate Based\'s performance in language modeling, we evaluate the PILE-pretrained models on several downstream tasks. We use the same protocol as [5], utilizing the LM evaluation harness by EleutherAI [16]. In particular, we use the following set of metrics and tasks:\n' +
      '\n' +
      '* LAMBADA (perplexity and accuracy) [75]\n' +
      '* HellaSwag (normalized accuracy) [76]\n' +
      '* PIQA (accuracy) [77]\n' +
      '* ARC-challenge (normalized accuracy) and, separately, the easy subset ARC-easy (accuracy) [78]\n' +
      '* WinoGrande (accuracy) [79]\n' +
      '\n' +
      'Normalized accuracy refers to accuracy normalized by sequence length and is used to maintain the equivalent setting to [5]. We report results in Table 5. For both 360 million and 1.3 billion parameter models, Based performs competitively with recent and state-of-the art architectures, including Mamba and modern Transformers (Transformer++ (LLaMa)).\n' +
      '\n' +
      'Downstream DNA ClassificationWe further evaluate how different architectures compare for DNA modeling. We use the pretrained models described in Section 6.1, and test how well they transfer to DNA sequence classification on a popular benchmark (GenomicBenchmarks) [74] used to evaluate recent DNA language models [41].\n' +
      '\n' +
      'We compare our HG38-pretrained Transformer++ and Based models in Table 6. We find similar performance across tasks, indicating that prior matching in quality during pretraining transfers to downstream classification. For reference, we also include results from [41]. Although not directly comparable to due differences in tokenization, the evaluations suggest Based can perform strongly on different modalities, and that recent sequence modeling architectures are also able to outpeform or compete with prior state-of-the-art on evaluated DNA tasks.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Dataset & Enhancer Cohn & Enhancer Ens & Human Reg. & Non-TATA Promoters & Human OCR Ens. \\\\ \\hline CNN & 69.5 & 68.9 & 93.3 & 84.6 & 68.0 \\\\ DNABERT & 74.0 & 85.7 & 88.1 & 85.6 & 75.1 \\\\ GPT & 70.5 & 83.5 & 91.5 & 87.7 & 73.0 \\\\ HumanDNA & 74.2 & 89.2 & **93.8** & 96.6 & **80.9** \\\\ \\hline Transformer++ & 73.4 & **89.5** & 89.9 & 94.4 & 79.5 \\\\ Mamba & 73.0 & - & - & 96.6 & - \\\\ Based & **74.6** & **89.5** & 89.5 & **96.8** & 79.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: **Downstream evaluation of pre-trained DNA models on GenomicsBenchmarks [74]. We report top-1 classification accuracy (%) with pretrained models (Transformer++, Mamba, Based) along with prior reported results in [41]. We find the similar quality-matching in pretraining transfers to downstream tasks. Modern architectures are also able to achieve state-of-the-art results on the classification tasks.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline Architecture & Params & LAMBADA & HellaSwag & PIQA & Arc-E & Arc-C & WinoGrande & Average \\\\  & & Ppl. \\(\\downarrow\\) & Acc. \\(\\uparrow\\) & Acc. Norm. \\(\\uparrow\\) & Acc \\(\\uparrow\\) & Acc \\(\\uparrow\\) & Acc. Norm. \\(\\uparrow\\) & Acc. \\(\\uparrow\\) & Acc. \\(\\uparrow\\) \\\\ \\hline \\hline Transformer++ (LLaMa) & 360m & 18.39 & 42.52 & 33.48 & 63.98 & 46.04 & 24.49 & 53.99 & 44.08 \\\\ Transformer (Pythia) & 356m & 25.17 & 37.16 & 31.32 & 63.76 & 44.82 & 23.8 & 51.54 & 42.08 \\\\ Baspl & 363m & 21.80 & 38.66 & 33.43 & 64.42 & 45.79 & 24.66 & 51.22 & 43.03 \\\\ Mamba & 358m & 20.23 & 39.65 & 33.63 & 65.02 & 47.01 & 25.00 & 50.75 & 43.51 \\\\ H3 & 362m & 57.59 & 23.58 & 30.62 & 63.11 & 45.20 & 23.29 & 50.28 & 39.35 \\\\ \\hline Transformer++ (LLaMa) & 1.33b & 11.12 & 49.10 & 39.29 & 66.16 & 51.68 & 26.19 & 53.43 & 47.64 \\\\ Baspl & 1.35b & 12.35 & 46.96 & 39.11 & 66.32 & 50.72 & 26.54 & 50.43 & 46.68 \\\\ Mamba & 1.32b & 13.11 & 46.13 & 39.41 & 66.38 & 52.36 & 25.94 & 50.83 & 46.84 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: **Downstream evaluation of pre-trained language models. The same set of models as in table 1, all were trained on the same 10 billion tokens drawn from the Pile [30], evaluated using the LM eval harness by EleutherAI [16]**Experimental Details\n' +
      '\n' +
      '### Language Model Pretraining\n' +
      '\n' +
      'We use A100 80GB Nvidia GPUs to run all experiments. We use training infrastructure closely adapted from the FlashAttention code base: [https://github.com/Dao-AILab/flash-attention/tree/main](https://github.com/Dao-AILab/flash-attention/tree/main) for all pretraining runs [11]. The Pile data is tokenized using the GPT2BPETokenizer and all models see the data in the same order. Here we provide details on the hyperaparamters and configurations used for training each architecture. We also provide details on the FLOPs computation.\n' +
      '\n' +
      '* **Based** We train using the specifications in Table 7. Our implementation is provided here: [https://github.com/HazyResearch/based](https://github.com/HazyResearch/based). The initial models were trained and evaluated using the Fast Transformer CUDA kernels discussed in Appendix B [24, 34].\n' +
      '* **Transformer++ [15]** We refer to the modern Llama architecture with Rotary encodings, RMSNorm and SwiGLU as Transformer++, following prior work [5, 6]. We train using the the specifications in Table 8 using the Flash Attention training code provided here: [https://github.com/Dao-AILab/flash-attention/tree/main](https://github.com/Dao-AILab/flash-attention/tree/main) [11].\n' +
      '* **Mamba [5]** We train using the specifications in Table 9, where the parameters are sourced from the Appendix of [5]. The implementation is sourced from the provided reference at [https://github.com/state-spaces/mamba](https://github.com/state-spaces/mamba).\n' +
      '* **Hyena [7]** We train using the specifications in Table 10, where the parameters are sourced from the Appendix of [7]. The implementation is sourced from the provided reference at [https://github.com/HazyResearch/safari](https://github.com/HazyResearch/safari).\n' +
      '* **H3 [9]** We train using the specifications in Table 11. The implementation is sourced from the provided reference at [https://github.com/HazyResearch/safari](https://github.com/HazyResearch/safari).\n' +
      '* **RWKV [8]** We train using the specifications in Table 12 and use the reference implementation at [https://github.com/BlinkDL/RNKV-LM](https://github.com/BlinkDL/RNKV-LM). We specifically evaluate RWKV-V5.\n' +
      '* **Gated Linear Attention (GLA)** We train using the specifications in Table 13. We train following the reference implementation at [https://github.com/berlino/gated_linear_attention](https://github.com/berlino/gated_linear_attention).\n' +
      '\n' +
      'We give all models the improved Transformer++ recipe (e.g., SwiGLU) as relevant.\n' +
      '\n' +
      '### Computing Recurrent State Size\n' +
      '\n' +
      'In this section, we provide details on how we compute the size of the recurrent hidden state for the results described in Section 3.1. We train and evaluate six sequence mixers on a synthetic associative recall task: attention [2], sliding window attention [19], Mamba [5], H3 [9], Hyena [7], and Based. For each, we vary hyperparameters that affect the memory consumption during inference. We compare how MQAR accuracy varies with the size of the recurrent hidden state.\n' +
      '\n' +
      'Based.The recurrent state size in Based is determined by the model dimension \\(d\\) and the size of the hidden dimension after applying the feature map \\(\\tilde{d}\\). The \\(+1\\) accounts for the K-state required for computing the denominator. For more details on the recurrent view of Based, see 4.\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=(d+1)\\times\\tilde{d} \\tag{8}\\]\n' +
      '\n' +
      'In Based, we use the Taylor Exponential feature map after projecting \\(d\\) down to a smaller dimension \\(d^{\\prime}\\). With this approach, recurrent state size is given by:\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=(d+1)\\times(1+\\frac{3d^{\\prime}}{2}+\\frac{d^{ \\prime 2}}{2}) \\tag{9}\\]\n' +
      '\n' +
      'In our synthetic experiments, we run Based with \\(d\\in\\{48,64,128\\}\\) and \\(d^{\\prime}\\in\\{8,16,24\\}\\).\n' +
      '\n' +
      'Attention.The recurrent state size (_i.e._ KV-cache size) in attention depends on two parameters: the model dimension \\(d\\) and the sequence length \\(N\\). The 2 in the expression below accounts for the separate storage for keys and values in the KV-cache.\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=2\\times d\\times N \\tag{10}\\]\n' +
      '\n' +
      'In our synthetic experiments we run attention with \\(d\\in\\{64,128\\}\\). The sequence length \\(N\\) is determined by the task, not the model architecture.\n' +
      '\n' +
      'Sliding window attention.The recurrent state size in sliding window attention is given by the model dimension \\(d\\) and the width of the sliding window \\(k_{\\text{sliding}}\\). The 2 in the expression below accounts for the separate storage for keys and values in the KV-cache.\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=2\\times d\\times\\min(N,k_{\\text{sliding}}) \\tag{11}\\]\n' +
      '\n' +
      'In our synthetic experiment we run sliding window attention with \\(d\\in\\{128\\}\\) and \\(k_{\\text{sliding}}\\in\\{8,16,32,64,128,256,512,1024\\}\\).\n' +
      '\n' +
      'Mamba.The recurrent state size in Mamba is determined by the model dimension \\(d\\) and the number of heads \\(h\\). The 2 in the expression below accounts for the expansion in the Mamba block.\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=2\\times d\\times d_{\\text{state}} \\tag{12}\\]\n' +
      '\n' +
      'In our synthetic experiments, we run Mamba with \\(d\\in\\{64,128,256\\}\\) and \\(d_{\\text{state}}\\in\\{8,16,24\\}\\).\n' +
      '\n' +
      'H3.The recurrent state size in H3 is determined by the model dimension \\(d\\) and the number of heads \\(d_{\\text{state}}\\).\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=d\\times d_{\\text{state}} \\tag{13}\\]\n' +
      '\n' +
      'In our synthetic experiments, we run H3 with \\(d\\in\\{64,128,256\\}\\) and \\(d_{\\text{state}}=\\frac{d}{4}\\).\n' +
      '\n' +
      'Hyena.The recurrent state size in Hyena is determined by the model dimension \\(d\\) and the number of heads \\(h\\). The 2 in the expression below accounts for the separate storage for keys and values in the KV-cache.\n' +
      '\n' +
      '\\[\\text{sizeof}(\\mathbf{s}_{i})=d\\times N \\tag{14}\\]\n' +
      '\n' +
      'In our synthetic experiments, we run Hyena with \\(d\\in\\{64,128,256\\}\\).\n' +
      '\n' +
      '### Language Model Evaluation\n' +
      '\n' +
      'In this section, we provide details on each of the evaluations (columns) reported in tables 1 and 4.\n' +
      '\n' +
      'Pile_(Language Modeling)._ First, we report overall perplexity on the Pile test set [30]. Then, to understand how much of the perplexity gap is due to recall capacity, we also evaluate perplexity on two slices (_i.e._ subsets) of the test set:\n' +
      '\n' +
      '1. _Associative recall(AR) tokens._ Tokens in the final position of a bigram which previously occured in context, but \\(\\leq 1250\\) times in the training data.\n' +
      '2. _Other tokens._ All other tokens.\n' +
      '\n' +
      'To construct these slices, we exactly follow the protocol in Arora et al. [1] and refer the reader to that work for more details. We compute these slices on the first 16 million tokens in the test set.\n' +
      '\n' +
      'Swde(Information Extraction).The task in the SWDE benchmark is to extract semi-structured relations from raw HTML websites. For example, given an IMBD page for a movie (_e.g. Harry Potter and the Sorcerer\'s Stone_) and a relation key (_e.g._ release date), the model must extract the correct relation value (_e.g._ 2001). The SWDE benchmark was originally curated by Lockard et al. [80] for the task of open information extraction from the semi-structured web. Because we are evaluating the zero-shot capabilities of relatively small language models, we adapt the task to make it slightly easier. Our task setup is similar after to that used in Arora et al. [39].\n' +
      '\n' +
      'Fda(Information Extraction).The task is to extract key-value pairs from a set of PDFs scraped from the FDA website. We use the dataset and labels collected in [39]. We break apart the documents into chunks of 1,920 tokens. For every key-value pair that appears in the chunk, we create a zero-shot prompt using the simple prompt template:\n' +
      '\n' +
      '{chunk} \\n {key}:\n' +
      '\n' +
      'We allow the model to generate a fixed number of tokens after the prompt and check (with case insensitivity) if the value is contained within the generation. We report **accuracy**, the fraction of prompts for which the generation contains the value.\n' +
      '\n' +
      'Below we include one example of a zero-shot prompt for the key-value pair "_Type of Test: Quantitative, colorimetric, pyranose oxidase (PROD)_". The actual chunk is substantially longer in the dataset (note the ellipsis).\n' +
      '\n' +
      '510(k) SUBSTENTIAL EQUIVALENCE DETERMINATION DECISION SUMMARY ASSAY ONLY TEMPLATE A. 510(k) Number: k180209 B. Purpose for Submission: New Device C. Measurand: 1,5-Anhydroglucitol (1,5-AG) D. **Type of Test: Quantitative, colorimetric, pyranose oxidase (PROD)** E. Applicant: Diazyme Laboratories Inc. F. Proprietary and Established Names: Diazyme 1,5-AG Assay G. Regulatory Information: 1. Regulation section: 21 CFR 864.7470; Glycosylated hemoglobin assay 2. Classification: Class II... _[1,920 tokens of context from the PDF]_... Diazyme\'s 1,5-AG assay uses the enzyme pyranose oxidase (PROD) to oxidize the 2nd position hydroxyl group of 1,5-AG and to detect the generated hydrogen peroxide by colorimetry using peroxidase (POD). **Type of Test:**\n' +
      '\n' +
      'Squad(Question Answering).The Stanford Question Answering Dataset (SQUAD) can be used to evaluate the reading comprehension of language models. The model is given a passage of text and a question whose answer is contained in the passage.\n' +
      '\n' +
      'Because the models trained in this work are relatively small-scale (up to 1.3 billion parameters trained on 10 billion tokens) and not instruction fine-tuned, they struggle to answer questions when asked directly. To make the task more amenable to these raw language models, we first use GPT-4 to reformat the questions to more closely resemble the next-token-prediction task the models were trained on:\n' +
      '\n' +
      'Can you rewrite this question and answer as a statement. Ensure that the answer is the last part of the statement. \\n \\n Question: {question} \\n\\n Answer: {answer} \\n\\n Rewrite:\n' +
      '\n' +
      'For example, the question and answer "_Question: Which NFL team represented the AFC at Super Bowl 50? Answer: Denver Broncos_" was rewritten by GPT-4 as "_The NFL team that represented the AFC at Super Bowl 50 was the Denver Broncos_." We verify that the rewritten sentence does indeed end with the answer, discarding any sentences where it does not (40% of questions).\n' +
      '\n' +
      'We run the reformatting on 5,000 squad questions from the validation set, yielding a final dataset of **2,984 questions** formatted as next token predictions.\n' +
      '\n' +
      'Below we include one example of a zero-shot prompt. The reformatted question is in bold.\n' +
      '\n' +
      'For the third straight season, the number one seeds from both conferences met in the Super Bowl. The Carolina Panthers became one of only ten teams to have completed a regular season with only one loss, and one of only six teams to have acquired a 15-1 record, while the Denver Broncos became one of four teams to have made eight appearances in the Super Bowl. The Broncos made their second Super Bowl appearance in three years, having reached Super Bowl XLVIII, while the Panthers made their second Super Bowl appearance in franchise history, their other appearance being Super Bowl XXXVIII. Coincidentally, both teams were coached by John Fox in their last Super Bowl appearance prior to Super Bowl 50. **The team in Super Bowl 50 that had a 15-1 record was the**Theoretical Results\n' +
      '\n' +
      '### Introduction\n' +
      '\n' +
      'Our focus in this section will be on the theoretical results of the paper. Specifically, we will show the equivalence of models Based and Mamba[5] with BaseConv, a minimal gated-convolution operator [1, Definition 4.1], and prove lower bounds for the MQAR problem [1, Section H.7.1] in various settings. We begin by setting notation and introducing the theoretical formulations of the models.\n' +
      '\n' +
      'Notation.We will be denoting the all 1 row vector of size \\(k\\), given by \\(\\begin{bmatrix}1&1&\\ldots&1&1\\end{bmatrix}\\), and the all 0 row vector of size \\(k\\), given by \\(\\begin{bmatrix}0&0&\\ldots&0&0\\end{bmatrix}\\), as \\(\\mathbf{1}^{k}\\) and \\(\\mathbf{0}^{k}\\), respectively. We will also construe the standard basis vector \\(\\mathbf{e}_{i}\\) as a column vector in these notes, and adhere to the following matrix indexing convention: \\(\\mathbf{M}[i,j]\\) is the entry in the \\(i\\)th row and the \\(j\\)th column, \\(\\mathbf{M}[i,:]\\in\\mathbb{F}^{1\\times n}\\) denotes the \\(i\\)th row, and \\(\\mathbf{M}[:,j]\\in\\mathbb{F}^{m\\times 1}\\) denotes the \\(j\\)th column of \\(\\mathbf{M}\\in\\mathbb{F}^{m\\times n}\\), where \\(\\mathbb{F}\\) is a field and the reader can substitute \\(\\mathbb{F}\\) for \\(\\mathbb{R}\\) for convenience. For a matrix \\(\\mathbf{M}\\in\\mathbb{R}^{n\\times m}\\), we define the pair-wise Hadamard product of columns of \\(\\mathbf{M}\\) as \\(\\mathbf{M}\\circ\\mathbf{M}\\in\\mathbb{R}^{n\\times m^{2}}\\), where\n' +
      '\n' +
      '\\[(\\mathbf{M}\\circ\\mathbf{M})[:,i]:=\\mathbf{M}[:,j]\\odot\\mathbf{M}[:,k]\\quad\\text{for} \\quad i\\in[m^{2}],\\]\n' +
      '\n' +
      '\\[j=\\left\\lfloor\\frac{i-1}{m}\\right\\rfloor+1,\\quad k=(i-1)\\mod m+1. \\tag{15}\\]\n' +
      '\n' +
      'Moreover, we define the element-wise exponentiation of a matrix \\(\\mathbf{M}\\) as \\(\\exp[\\mathbf{M}]\\) where \\(\\exp[\\mathbf{M}]_{ij}=\\exp(\\mathbf{M}_{ij})\\). Next, we denote the _Hadamard product_ of vectors \\(\\mathbf{u},\\mathbf{v}\\in\\mathbb{F}^{n}\\) as \\(\\mathbf{u}\\odot\\mathbf{v}\\); the operation can be extended to matrices accordingly, and for vectors \\(\\mathbf{u},\\mathbf{v}\\in\\mathbb{F}^{n}\\), we denote their _linear (or acyclic) convolution_ as \\(\\mathbf{u}*\\mathbf{v}\\)\n' +
      '\n' +
      'Arithmetic Circuit Notation.We briefly introduce the notation of arithmetic circuits [81]. An _arithmetic circuit_\\(\\mathcal{C}\\) with variables \\(X\\triangleq\\{x_{1},x_{2},\\ldots,x_{n}\\}\\) over a field \\(\\mathbb{F}\\) is interpreted as a directed acyclic graph, where the input nodes are labelled by either the variables from \\(X\\) or constants from \\(\\mathbb{F}\\) and the internal nodes are labelled by \\(+\\) or \\(\\times\\) with the output being the polynomial computed at the output node.\n' +
      '\n' +
      'We shall also refer to the _size_ of the circuit as the number of nodes, the _depth_ of the circuit as the length of the longest path between an input node and the output node, and the _width_ of the circuit as the number of parallel operations in the circuit, or \'wires\' which will be intersected by a horizontal \'cut\' through the circuit. Moreover, the _degree_ of a circuit is defined as the degree of the polynomial computed by the circuit. We summarize this with the following definition:\n' +
      '\n' +
      '**Definition F.1**.: An arithmetic circuit \\(\\mathcal{C}\\) is an _\\((n,s,\\Delta,w)\\)-circuit_ if \\(\\mathcal{C}\\) is an \\(n\\)-variate arithmetic circuit of size \\(s\\) and of depth at most \\(\\Delta\\), and width \\(w\\).\n' +
      '\n' +
      '### The Models\n' +
      '\n' +
      'We now introduce the definitions of the models Based and Mamba for the reader\'s convenience. Note that we have redefined these models to ensure consistency with the notation presented above.\n' +
      '\n' +
      '#### f.2.1 Based\n' +
      '\n' +
      'The Based model combines two layer types: BaseConv and LinearAttention defined below.\n' +
      '\n' +
      '**Definition F.2** (BaseConv[1]).: Given an input sequence \\(\\mathbf{u}\\in\\mathbb{R}^{N\\times d}\\), where \\(N\\) is the sequence length and \\(d\\) is the model dimension, a learned weight matrix \\(\\mathbf{W}^{B}\\in\\mathbb{R}^{d\\times d}\\) and biases \\(\\mathbf{B}^{B},\\mathbf{B}^{K}\\in\\mathbb{R}^{N\\times d}\\) and a matrix of convolution filters \\(\\mathbf{K}\\in\\mathbb{R}^{N\\times d}\\), a BaseConvlayer computes the following:\n' +
      '\n' +
      '\\[\\mathbf{z}^{\\texttt{BaseConv}}:=(\\mathbf{u}\\mathbf{W}^{B}+\\mathbf{B}^{B})\\odot\\left(\\mathbf{K}*\\bm {u}+\\mathbf{B}^{K}\\right)\\in\\mathbb{R}^{N\\times d}, \\tag{16}\\]\n' +
      '\n' +
      'where the convolutions are applied across the input length \\(N\\).\n' +
      '\n' +
      '**Definition F.3** (LinearAttention [24]).: Given an input sequence \\(\\mathbf{u}\\in\\mathbb{R}^{N\\times d}\\), where \\(N\\) is the sequence length and \\(d\\) is the model dimension, a set of linear projections6\\(\\texttt{Projection}_{q},\\texttt{Projection}_{k}\\in\\mathbb{R}^{d\\times d^{\\prime}}, \\texttt{Projection}_{v}\\in\\mathbb{R}^{d\\times d}\\), where \\(d^{\\prime}\\) is the feature dimension, the LinearAttention layer computes the following:\n' +
      '\n' +
      'Footnote 6: By linear projections of a matrix \\(\\mathbf{u}\\in\\mathbb{R}^{m\\times n}\\), we mean \\(\\mathbf{u}\\mathbf{W}+\\mathbf{B}\\) for some weight matrix \\(\\mathbf{W}\\in\\mathbb{R}^{n\\times n}\\) and bias \\(\\mathbf{B}\\in\\mathbb{R}^{m\\times n}\\).\n' +
      '\n' +
      '\\[\\mathbf{z}^{\\texttt{LinearAttention}}:=\\left(\\overline{\\mathbf{Q}}\\ \\overline{\\mathbf{K}}^{\\top}\\right)\\mathbf{V}\\in\\mathbb{R}^{N\\times d}, \\tag{17}\\]\n' +
      '\n' +
      'where \\(\\mathbf{Q}:=\\texttt{Projection}_{q}(\\mathbf{u}),\\mathbf{K}:=\\texttt{Projection}_{k}(\\mathbf{u }),\\mathbf{V}:=\\texttt{Projection}_{v}(\\mathbf{u})\\), and we have\n' +
      '\n' +
      '\\[\\overline{\\mathbf{Q}} =[\\mathbf{1},\\mathbf{Q},\\mathbf{Q}\\circ\\mathbf{Q}]\\in\\mathbb{R}^{N\\times(1+d^{ \\prime}+d^{\\prime 2})},\\] \\[\\overline{\\mathbf{K}} =[\\mathbf{1},\\mathbf{Q},\\mathbf{K}\\circ\\mathbf{K}]\\in\\mathbb{R}^{N\\times(1+d^{ \\prime}+d^{\\prime 2})}.\\]\n' +
      '\n' +
      '#### f.2.2 Mamba\n' +
      '\n' +
      'We now introduce the Mamba model from [5].\n' +
      '\n' +
      '**Definition F.4** (Mamba [5]).: Given an input sequence \\(\\mathbf{u}\\in\\mathbb{R}^{N\\times d}\\), where \\(N\\) is the sequence length and \\(d\\) is the model dimension, the Mamba layer computes the following:\n' +
      '\n' +
      '\\[\\mathbf{z}^{\\texttt{Mamba}}:=\\texttt{SSM}(\\overline{\\mathbf{A}},\\overline{\\mathbf{B}}, \\mathbf{C})(\\mathbf{u})\\in\\mathbb{R}^{N\\times d}, \\tag{18}\\]\n' +
      '\n' +
      'with the parameters, \\(\\overline{\\mathbf{A}}\\in\\mathbb{R}^{\\overline{d}\\times\\overline{d}},\\overline{\\bm {B}}\\in\\mathbb{R}^{\\overline{d}}\\), defined as\n' +
      '\n' +
      '\\[\\overline{\\mathbf{A}} :=\\exp\\left(\\Delta\\mathbf{A}\\right), \\tag{19}\\] \\[\\overline{\\mathbf{B}} :=\\left(\\Delta\\mathbf{A}\\right)^{-1}\\left(\\exp\\left(\\Delta\\mathbf{A} \\right)-\\mathbf{I}\\right)\\cdot\\Delta\\mathbf{B},\\] \\[=\\mathbf{A}^{-1}(\\exp\\left(\\Delta\\mathbf{A})-\\mathbf{I}\\right)\\cdot\\mathbf{B},\\]\n' +
      '\n' +
      'where \\(\\overline{d}\\), the state dimension, and \\(\\mathbf{A}\\in\\mathbb{R}^{\\overline{d}\\times\\overline{d}}\\) are parameters of the model and do not depend on the input \\(\\mathbf{u}\\), along with the following _input-dependent_ parameters \\(\\mathbf{B},\\mathbf{C}\\in\\mathbb{R}^{N\\times\\overline{d}},\\Delta\\in\\mathbb{R}^{N\\times d}\\) defined as\n' +
      '\n' +
      '\\[\\mathbf{B} :=\\texttt{Linear}_{N\\times\\overline{d}}(\\mathbf{u})\\in\\mathbb{R}^{ \\overline{d}}, \\tag{20}\\] \\[\\mathbf{C} :=\\texttt{Linear}_{N\\times\\overline{d}}(\\mathbf{u})\\in\\mathbb{R}^{ \\overline{d}},\\] \\[\\Delta :=\\texttt{Linear}_{N\\times d}(\\mathbf{u})\\in\\mathbb{R}\\]\n' +
      '\n' +
      'for \\(i\\in[N]\\). It is important to note here that the parameters \\(\\overline{\\mathbf{B}},\\mathbf{C},\\Delta\\) are causal7 and we denote the dependence on upto the \\(i\\)th row of the input \\(\\mathbf{u}\\) for \\(i\\in[N]\\) by adding a subscript \\(i\\) where the dependence for \\(\\overline{\\mathbf{A}}_{i}\\in\\mathbb{R}^{\\overline{d}\\times\\overline{d}}\\) is inherited from \\(\\Delta_{i}\\) in equation 19 and we denote \\(\\overline{\\mathbf{B}}[i,:]=:\\mathbf{B}_{i},\\overline{\\mathbf{C}}[i,:]=:\\mathbf{C}_{i}\\).\n' +
      '\n' +
      'Footnote 7: That is, \\(\\mathbf{B}[i,:],C[i,:]\\) and \\(\\Delta[i,:]\\) depend only on \\(\\mathbf{u}[0\\cdots i-1]\\).\n' +
      '\n' +
      'Finally, the SSM in equation 18 is realized as a linear recurrence. That is, for every \\((i,j)\\in[N]\\times[d]\\), we have\n' +
      '\n' +
      '\\[\\mathbf{h}[i,j] =\\overline{\\mathbf{A}}_{i}\\mathbf{h}[i-1,j]+\\overline{\\mathbf{B}}_{i}\\mathbf{u}[ i,j] \\tag{21}\\] \\[\\mathbf{z}[i,j] =\\mathbf{C}_{i}^{\\top}\\mathbf{h}[i,j]\\]\n' +
      '\n' +
      'where \\(\\mathbf{h}[i,j]\\in\\mathbb{R}^{\\overline{d}},\\mathbf{z}[i,j]\\in\\mathbb{R}\\) denote the latent state and the output of the SSM in eq. (18), respectively.\n' +
      '\n' +
      '### Equivalency to BaseConv\n' +
      '\n' +
      'For a polynomial with variables \\(X\\) over a field \\(\\mathbb{F}\\), there exists a corresponding arithmetic circuit \\(\\mathcal{C}\\) over \\(X\\) that computes the output of the polynomial at its terminating node when interpreted as a directed acyclic graph. For any such arithmetic circuit \\(\\mathcal{C}\\) of size \\(s\\) and depth \\(\\Delta\\), [1, Theorem 4.2] showed the existence of an equivalent BaseConv operator that uses \\(\\tilde{\\mathcal{O}}(s\\Delta)\\) parameters and \\(\\tilde{\\mathcal{O}}(\\Delta)\\) layers. In the sequel, we use this result by expressing the model outputs computed in equation 17 and equation 18 as polynomials in \\(\\mathbf{u}\\) and \\(\\exp\\left(\\mathbf{u}\\right)\\) to show the equivalency between these disparate models. We would now like to recall [1, Theorem 4.2]. Before doing so, we first establish the following definitions from [1].\n' +
      '\n' +
      '**Definition F.5**.: An \\(\\left(N,L,d,\\tilde{N},\\tilde{d}\\right)-\\text{Gated Convolution Model}\\) is a stacked sequence to sequence model with \\(L\\) layers such that:\n' +
      '\n' +
      '1. input and output are \\(N\\times d\\) matrices,\n' +
      '2. each layer\'s operations consist of element-wise gating, convolution, linear projection, and\n' +
      '3. all the individual gated convolution layers take in \\(\\tilde{N}\\times\\tilde{d}\\) matrices and output \\(\\tilde{N}\\times\\tilde{d}\\) matrices. We refer to the tuple \\((\\tilde{N},\\tilde{d})\\) as the _inner dimension_ of the model.\n' +
      '\n' +
      'We also assume that the input \\(\\mathbf{u}\\in\\mathbb{R}^{N\\times d}\\) is embedded into \\(\\mathbf{u}^{\\prime}\\in\\mathbb{R}^{\\tilde{N}\\times\\tilde{d}}\\) such that\n' +
      '\n' +
      '\\[\\mathbf{u}^{\\prime}[n,t]=\\begin{cases}\\mathbf{u}[n,t]&\\text{if }n<N,\\ t<d\\\\ 0&\\text{otherwise.}\\end{cases}\\]\n' +
      '\n' +
      'The output from the last layer \\(\\mathbf{z}\\in\\mathbb{R}^{\\tilde{N}\\times\\tilde{d}}\\) is transformed into output \\(\\mathbf{y}\\in R^{N\\times d}\\) by extracting the top left \\(N\\times d\\) entries in \\(\\mathbf{z}\\).\n' +
      '\n' +
      '**Theorem F.1** ([1], Theorem 4.2).: _For any \\((nd,s,\\Delta,w)\\)-arithmetic circuit \\(\\mathcal{C}\\), there exists an equivalent \\(\\left(N,\\Delta^{\\prime},d,\\tilde{N},\\tilde{d}\\right)-\\texttt{BaseConv}\\) with \\(N=n,\\Delta^{\\prime}=\\mathcal{O}(\\Delta\\log w)\\), \\(\\tilde{N}=\\mathcal{O}(w),\\tilde{d}=d\\) that simulates \\(\\mathcal{C}\\)._\n' +
      '\n' +
      '**Remark F.1**.: For notational simplicity, we will use \\(\\mathbf{u}_{i,j}\\) as the symbol for the variable in the polynomial in \\(\\mathbf{u}\\) representing the entry \\(\\mathbf{u}[i,j]\\).\n' +
      '\n' +
      'We now present the results showing equivalency between the models in appendix F.2 and the BaseConv layer in equation 16 using theorem F.1.\n' +
      '\n' +
      '**Proposition F.1**.: _Given an input \\(\\mathbf{u}\\in\\mathbb{R}^{N\\times d}\\), there exists an equivalent \\(\\left(N,O(\\log^{2}(Nd)),d,O(N(d+d^{\\prime 2}),O(\\max(d,d^{\\prime 2})) \\right)-\\texttt{BaseConv}\\) that computes the output of the LinearAttention layer with feature dimension \\(d^{\\prime}\\), cf. eq. (17)._\n' +
      '\n' +
      'Proof.: For the matrices \\(\\mathbf{Q},\\mathbf{K}\\in\\mathbb{R}^{N\\times d^{\\prime}},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}\\) with the corresponding projection matrices \\(\\mathbf{W}^{Q},\\mathbf{W}^{k}\\in\\mathbb{R}^{d\\times d^{\\prime}},\\mathbf{W}^{V}\\in\\mathbb{R }^{d\\times d}\\), a single BaseConv layer that computes each of these matrices by simply taking identical projection and \\(\\mathbf{h}^{s},\\mathbf{h}^{l},\\mathbf{B}^{s}\\equiv 0\\) and \\(\\mathbf{B}^{\\ell}\\equiv\\mathbb{1}^{N\\times d}\\), the all \\(1\\) matrix. Using the remembering primitive [1, Proposition H.10], we can compute each of these in turn while remembering others using \\(O(1)\\) layers and \\(Nd\\) parameters.\n' +
      '\n' +
      'Next, we derive an expression for each entry \\((i,j)\\in[N]\\times[d^{\\prime 2}]\\) of \\(\\mathbf{Q}\\circ\\mathbf{Q},\\mathbf{K}\\circ\\mathbf{K}\\in\\mathbb{R}^{N\\times d^{\\prime 2}}\\). From equation 15, observe that each entry of \\(\\mathbf{M}\\circ\\mathbf{M}\\) can be written as the product of entries from \\(\\mathbf{M}\\). Hence we have\n' +
      '\n' +
      '\\[(\\mathbf{Q}\\circ\\mathbf{Q})[i,j] \\equiv\\mathbf{Q}[i,k]\\cdot\\mathbf{Q}[i,\\ell]\\] \\[(\\mathbf{K}\\circ\\mathbf{K})[i,j] \\equiv\\mathbf{K}[i,k]\\cdot\\mathbf{K}[i,\\ell] \\tag{22}\\]\n' +
      '\n' +
      'for \\(k=\\left\\lfloor\\frac{i-1}{d^{\\prime}}\\right\\rfloor+1,\\quad\\ell=(j-1)\\mod d^{ \\prime}+1.\\) Note, however, that we can simulate the above by first increasing the inner dimension and copying over columns of \\(\\mathbf{Q}\\) to get \\(\\mathbf{Q}_{1},\\mathbf{Q}_{2}\\in\\mathbb{R}^{N\\times d}\\) defined as \\(\\mathbf{Q}_{1}[i,j]:=\\mathbf{Q}[i,k]\\) and \\(\\mathbf{Q}_{2}[i,j]:=\\mathbf{Q}[i,\\ell]\\) for \\(k=\\left\\lfloor\\frac{j-1}{d^{\\prime}}\\right\\rfloor+1,\\quad\\ell=(j-1)\\mod d^{ \\prime}+1\\) so that \\((\\mathbf{Q}\\circ\\mathbf{Q})=\\mathbf{Q}_{1}\\odot\\mathbf{Q}_{2}\\), which, _mutatis mutandis_, also applies to \\((\\mathbf{K}\\circ\\mathbf{K})\\) We can achieve the copying of the columns by simply using the projection matrix \\(\\mathbf{W}^{B}\\) and another permutation matrix \\(\\mathbf{P}\\). Apart from the multiplication by \\(\\mathbf{P}\\), we only need to use \\(O(1)\\) layers, and moreover, since the circuit that computes \\(\\mathbf{P}\\mathbf{u}\\) simply rearranges the input, there exists a single BaseConv layer that computes \\(\\mathbf{P}\\mathbf{u}\\)[1, Corollary H.20]. By the stacking lemma [1, Lemma H.11], we can stack these layers to get a composition of the outputs so far to get a \\(\\left(N,O(1),d,O(N(d+d^{\\prime 2}),O(\\max(d,d^{\\prime 2}))\\right)-\\texttt{BaseConv}\\) model. Moreover, the concatenated matrices \\(\\overline{\\mathbf{Q}},\\overline{\\mathbf{K}}\\)\\(\\in\\mathbb{R}^{N\\times(1+d^{\\prime}+d^{\\prime 2})}\\) then take the addition of the computed components so far which again takes \\(O(1)\\) layers of BaseConv.\n' +
      '\n' +
      'Finally, we can express each entry \\((i,j)\\in[N]\\times[d]\\) of the output of LinearAttention as a polynomial as follows:\n' +
      '\n' +
      '\\[\\mathbf{z}_{i,j}(\\mathbf{u})\\equiv\\sum_{m\\in[1+d^{\\prime}+d^{\\prime 2}],n\\in[N]} \\overline{\\mathbf{Q}}[i,m]\\cdot\\overline{\\mathbf{K}}[n,m]\\cdot\\mathbf{V}[n,j]. \\tag{23}\\]Thus, we can derive the arithmetic circuit that computes \\(\\mathbf{z}_{i,j}(\\mathbf{u})\\) by taking in the outputs of the BaseConv layers so far as input and compute each of the terms inside the sum by multiplying the outputs from all three and compute the sum using additional \\(\\log\\left\\lceil Nd\\right\\rceil\\) depth. Each term inside the sum requires two multiplication gates with depth \\(2\\), each of which serve as inputs to the circuit with size \\(Nd\\) computing the sum. Moreover, there are \\(N\\cdot d\\) such output gates each of which is computed in parallel resulting in a circuit of size \\(O(N\\cdot d)\\), depth \\(O(\\log(Nd))\\) and width \\(O(Nd)\\). O Overall, applying theorem F.1 then results in an equivalent that computes \\(\\mathbf{z}\\). \n' +
      '\n' +
      '### The Lower Bounds\n' +
      '\n' +
      'In the sequel, we consider the _multiple-query associative recall_ problem (MQAR) as defined in [1, Section H.7.1]. We briefly recall the definition here.\n' +
      '\n' +
      'Suppose we are given an input sequence \\(\\mathbf{u}[0\\cdots 3N-1]\\triangleq\\left\\{\\left(\\mathbf{k}_{0},\\mathbf{v}_{0},\\mathbf{q}_{0} \\right),\\ldots,\\left(\\mathbf{k}_{N-1},\\mathbf{v}_{N-1},\\mathbf{q}_{N-1}\\right)\\right\\}\\) with each \\(\\mathbf{k}_{i},\\mathbf{v}_{i},\\mathbf{q}_{i}\\in C\\) is a token drawn from a vocabulary of size \\(c=|C|\\). Our goal is then to check, for each \\(1\\leq i\\leq N-1\\), whether there exists \\(0\\leq j<i\\) such that \\(\\mathbf{q}_{i}\\equiv\\mathbf{k}_{j}\\), and if so, output \\(\\mathbf{v}_{j}\\).\n' +
      '\n' +
      '#### f.4.1 The Space Complexity of AR\n' +
      '\n' +
      'We will start by providing a lower bound on the space complexity of solving the standard associative recall (AR) problem. As AR is a subclass of MQAR, this naturally provides a lower bound on the space complexity of MQAR as well. Here, we formally recall the associative recall problem.\n' +
      '\n' +
      'The AR problem takes key-value pairs \\(\\left\\{\\mathbf{k}_{i},\\mathbf{v}_{i}\\right\\}_{i=0}^{n-1}\\) along with a query \\(\\mathbf{q}\\) appended at the end as input and the goal is to output \\(\\mathbf{v}_{i}\\) if \\(\\mathbf{q}=\\mathbf{k}_{i}\\) for some \\(i\\in[0,N-1]\\).\n' +
      '\n' +
      'We now require a randomized communication complexity lower bound result for the _index problem_:\n' +
      '\n' +
      'The index problem has two agents, Alice and Bob, where Alice has a string \\(\\mathbf{x}\\in\\{0,1\\}^{n}\\) and Bob has an index \\(i\\in[n]\\), and the goal for the players is to output the \\(i\\)-th entry \\(\\mathbf{x}_{i}\\). Moreover, we also require the communication to be _one-way_: only Alice is allowed to send a single message to Bob and Bob needs to output the answer.\n' +
      '\n' +
      'We will make use of the following lower-bound result.\n' +
      '\n' +
      '**Theorem F.2** ([82]).: _The one-way randomized communication complexity8 of the index problem for sending an \\(n\\)-length bit string is \\(\\Omega(n)\\)._\n' +
      '\n' +
      'Footnote 8: The randomized communication complexity of function \\(f\\) is defined as \\(\\min_{\\pi}\\|\\pi\\|\\), where \\(\\pi\\) ranges over all randomized protocols that can solve \\(f\\) with probability of success at least \\(2/3\\).\n' +
      '\n' +
      '#### f.4.2 Lower Bound for Recurrent Models\n' +
      '\n' +
      'We now use theorem F.2 to first provide a lower bound on the number of bits required by the following class of models to solve AR.\n' +
      '\n' +
      '**Definition F.6** (Recurrent Models).: A model \\(\\mathcal{M}\\) taking an input \\(\\mathbf{u}\\in\\mathbb{R}^{N\\times d}\\), where \\(N\\) is the input length and \\(d\\) is the model dimension, is termed a _recurrent model_ if its \\(i\\)-th state, representing the output at location \\(i\\), \\(\\mathbf{Z}_{\\mathcal{M}}^{i}\\in\\mathbb{R}^{\\tilde{d}}\\), with \\(\\tilde{d}\\) denoting the state size, is determined exclusively by the preceding elements of the input \\(\\mathbf{u}[0\\ldots i-1]\\). The state \\(\\mathbf{Z}_{\\mathcal{M}}^{i}\\) represents the accumulated information of the model depending on the inputs up to the \\(i\\)-th element, and is distinct from learned parameters that are static with respect to the input sequence.\n' +
      '\n' +
      'Specifically, \\(\\mathbf{Z}_{\\mathcal{M}}^{i}(\\mathbf{u})=\\phi(\\mathbf{u}[0\\ldots i-1])\\), indicating that the state is a function of the input history but not of the entire input sequence simultaneously. Moreover, we can express this as:\n' +
      '\n' +
      '\\[\\mathbf{Z}_{\\mathcal{M}}^{i}(\\mathbf{u})=f_{\\mathcal{M}}^{i}(\\mathbf{Z}_{\\mathcal{M}}^{i-1 },\\mathbf{u}[i]), \\tag{24}\\]\n' +
      '\n' +
      'for a sequence of functions \\(\\{f_{\\mathcal{M}}^{i}\\}_{i\\in[N]}\\), where each function is tailored to evolve the state based on the immediate past state and the current input.\n' +
      '\n' +
      '**Remark F.2**.: Note that definition F.6 excludes models that inherently require the entire input sequence for computation at any state, such as those based on non-causal convolutional operations over the full input.\n' +
      '\n' +
      '**Theorem F.3**.: _Any recurrent model \\(\\mathcal{M}\\) (definition F.6) that solves AR requires \\(\\max_{i}\\left|\\mathbf{Z}_{\\mathcal{M}}^{i}\\right|\\) to be at least \\(\\Omega(N)\\)-bits._\n' +
      '\n' +
      'Proof.: Consider an instance \\((\\mathbf{x},i)\\) of the index problem with \\(\\mathbf{x}\\in\\{0,1\\}^{N}\\). We now describe the corresponding instance of the AR problem:\n' +
      '\n' +
      '\\[\\{j,\\mathbf{x}_{j}\\}_{j=0}^{N-1},i. \\tag{25}\\]\n' +
      '\n' +
      'Next, consider the following one-way protocol for solving the index problem using the regressive model \\(\\mathcal{M}\\). Alice with their access of \\(\\mathbf{x}\\in\\{0,1\\}^{N}\\) generate an input for AR (without the query) as in equation 25. Alice then runs the model \\(\\mathcal{M}\\) on \\(\\{i,\\mathbf{x}_{j}\\}_{j=0}^{N-1}\\) and sends the memory content of running the model \\(\\mathcal{M}\\) to Bob. This should include the state \\(\\mathbf{Z}_{\\mathcal{M}}^{N-1}\\) of size \\(\\tilde{d}\\) as we can reasonably assume that both have access to the set of functions \\(\\{f_{\\mathcal{M}}^{j}\\}_{j\\in[N]}\\). Since we assume that this model solves AR, the output \\(\\texttt{Out}[N,:]=\\mathbf{x}_{i}\\) should contain the associated value of \\(i\\). Here, Bob can compute \\(\\texttt{Out}[N,:]\\) by using the memory content sent by Alice and applying the function \\(f^{N}\\) as follows.\n' +
      '\n' +
      '\\[\\mathbf{x}_{i}=\\texttt{Out}[N,:]=f^{N}(\\mathbf{Z}^{N-1},\\mathbf{u}[N]).\\]\n' +
      '\n' +
      'That is, the total number of bits that are communicated in this protocol is \\(\\left|\\mathbf{Z}_{\\mathcal{M}}^{N-1}\\right|\\). Now, if \\(\\max_{j}\\left|\\mathbf{Z}_{\\mathcal{M}}^{j}\\right|\\) is \\(o(N)\\) bits, we have shown that a one-way communication protocol exists for solving the index problem exists that uses \\(o(N)\\) communication complexity. This contradicts theorem F.2 and hence, we conclude that the model \\(\\mathcal{M}\\) solving AR also needs \\(\\Omega(N)\\) bits. \n' +
      '\n' +
      '**Corollary F.1**.: _Given an input \\(\\mathbf{u}\\in\\mathbb{R}^{N\\times d}\\) to the AR problem, a causal Mamba model with all entries in its computation taking \\(O(1)\\) bits needs \\(d+\\overline{d}\\geq\\Omega(N)\\) to solve AR._\n' +
      '\n' +
      'Proof.: We will first show that causal Mamba is a recurrent model. To see this, first observe equation 21 and note the fact that the input-dependent parameters \\(\\overline{\\mathbf{A}},\\overline{\\mathbf{B}},\\mathbf{C},\\Delta\\) are causal as mentioned in definition F.4.\n' +
      '\n' +
      'Next, due to equation 21, in order to compute \\(\\mathbf{z}_{N,:}\\in\\mathbb{R}^{d}\\), we need \\(\\mathbf{C}_{N}\\in\\mathbb{R}^{\\overline{d}},\\overline{\\mathbf{B}}_{N}\\in\\mathbb{R}^{ \\overline{d}}\\) and \\(\\Delta_{N}\\in\\mathbb{R}^{d}\\) along with \\(\\mathbf{h}[N-1,:]\\in\\mathbb{R}^{\\overline{d}}\\). Here, we have the \\((N-1)\\)-st state \\(\\mathbf{Z}_{\\texttt{Mamba}}^{N-1}\\in\\mathbb{R}^{\\overline{3d}+d}\\) given by\n' +
      '\n' +
      '\\[\\mathbf{Z}_{\\texttt{Mamba}}^{N-1}:=\\{\\mathbf{h}[i-1,:],\\Delta_{N}^{1},\\overline{\\mathbf{B }}_{N}^{1},\\mathbf{C}_{N}^{1}\\},\\]\n' +
      '\n' +
      'where \\(\\Delta_{N}^{1},\\overline{\\mathbf{B}}_{N}^{1},\\mathbf{C}_{N}^{1}\\) are all linear functions of \\(\\mathbf{u}[0\\cdots N-1]\\) that we receive from the \\((N-1)\\)-st state and we compute \\(\\Delta_{N}^{2},\\overline{\\mathbf{B}}_{N}^{2},\\mathbf{C}_{N}^{2}\\) as linear functions of \\(\\mathbf{u}[N]\\) so that we have \\(\\Delta_{N}=\\Delta_{N}^{1}+\\Delta_{N}^{1},\\overline{\\mathbf{B}}_{N}=\\overline{\\bm {B}}_{N}^{1}+\\overline{\\mathbf{B}}_{N}^{2},\\mathbf{C}_{N}=\\mathbf{C}_{N}^{1}+\\mathbf{C}_{N}^{2}\\). We can then define the function \\(f^{N}\\) as follows:\n' +
      '\n' +
      '\\[\\mathbf{Z}_{\\texttt{Mamba}}^{N}[j] =\\exp(\\Delta_{N}[j]\\mathbf{A})\\mathbf{h}[N-1,j]+\\overline{\\mathbf{B}}_{N}\\bm {u}[N,j]\\] \\[=\\overline{\\mathbf{A}}_{N}\\mathbf{h}[N-1,j]+\\overline{\\mathbf{B}}_{N}\\mathbf{u}[N,j],\\] \\[\\texttt{Out}[N,j] =f^{N}(\\mathbf{Z}_{\\texttt{Mamba}}^{N-1})[j]=\\mathbf{C}_{N}^{\\top}\\mathbf{Z}_{ \\texttt{Mamba}}^{N}[j].\\]\n' +
      '\n' +
      'Thus, due to theorem F.3, we can conclude that \\(\\left|\\mathbf{Z}_{\\texttt{Mamba}}^{N-1}\\right|\\) does require \\(\\Omega(N)\\)-bits to solve AR. Finally, assuming each entry of \\(\\mathbf{Z}_{\\texttt{Mamba}}^{N-1}\\) needs \\(O(1)\\) bits to represent, the overall state \\(\\mathbf{Z}_{\\texttt{Mamba}}^{N-1}\\) needs \\(O(d+\\overline{d})\\) to represent, which completes the proof of the claim. \n' +
      '\n' +
      '#### f.4.3 Lower Bound on the Number of Layers for AR\n' +
      '\n' +
      'Next, we will again use theorem F.2 to provide a better bound on the number of layers required to solve AR.\n' +
      '\n' +
      '**Theorem F.4**.: _Given an input \\(\\mathbf{u}\\in\\{0,1\\}^{N\\times d}\\) to the AR problem with any encoding such that \\(\\log c\\leq d\\leq 2^{(\\log N)^{1-\\epsilon}}\\) for \\(\\epsilon>0\\), and \\(c\\) possible tokens from the vocabulary with \\(c\\leq N\\), a data-independent BaseConv model with model parameters taking \\(O(\\log N)\\) bits needs \\(\\Omega(\\epsilon\\log\\log N)\\) layers to solve AR._Proof.: For a BaseConv model that solves AR using \\(L\\) layers, by definition, there exists a polynomial \\(P(\\mathbf{u})\\) of degree at most \\(2^{L}\\) that solves AR for any \\(\\mathbf{u}\\in\\{0,1\\}^{N\\times d9}\\). This is because for the output of the \\(i\\)th layer of BaseConv, given by \\(\\mathbf{Z}^{i}_{\\texttt{BaseConv}}\\), we have\n' +
      '\n' +
      '\\[\\mathbf{Z}^{i}_{\\texttt{BaseConv}}(\\mathbf{Y}^{i-1}_{\\mathcal{M}})\\equiv P^{i}(\\mathbf{Z} ^{i-1}_{\\texttt{BaseConv}}),\\quad\\deg(P^{i})\\leq 2,\\]\n' +
      '\n' +
      'for some polynomial \\(P^{i}\\) of degree 2 which simply takes the inner products allowing the model to solve AR, where \\(\\mathbf{Z}^{0}_{\\texttt{BaseConv}}:=\\mathbf{u}\\). Further, for such a model with \\(L\\) layers, by composition, the output of the \\(i\\)-th layer for \\(i\\in[L]\\) is also a polynomial over the input \\(\\mathbf{u}\\) and has degree at most \\(2^{i}\\). At the end, we have a polynomial \\(P(\\mathbf{u})\\) of degree \\(\\leq 2^{L}\\) for \\(\\mathbf{u}\\in\\{0,1\\}^{N\\times d}\\). As in the proof of theorem F.3, again take the instance instance \\((\\mathbf{x},i)\\) of the index problem with \\(\\mathbf{x}\\in\\{0,1\\}^{N}\\) and the corresponding instance of the AR problem as before\n' +
      '\n' +
      '\\[\\mathbf{u}:=\\{j,\\mathbf{x}_{j}\\}_{j=0}^{N-1},i. \\tag{26}\\]\n' +
      '\n' +
      'Next, we build the following one-way protocol for solving the index problem using the BaseConv model from the hypothesis that it solves AR. Alice with their access of \\(\\mathbf{x}\\in\\{0,1\\}^{N}\\) will again generate an input \\(\\mathbf{u}\\) for AR (without the query) as in equation 26.\n' +
      '\n' +
      'Alice first takes the values \\(\\mathbf{a}:=\\mathbf{u}[0:N-2,:]\\in\\{0,1\\}^{(N-1)\\times d}\\) and substitutes these known \\((N-1)d\\) values to define the following polynomial:\n' +
      '\n' +
      '\\[Q(\\mathbf{u}_{N-1,0},\\ldots,\\mathbf{u}_{N-1,d-1})=P(\\mathbf{a},\\mathbf{u}_{N-1,0},\\ldots,\\mathbf{ u}_{N-1,d-1}). \\tag{27}\\]\n' +
      '\n' +
      'Here, note that \\(Q\\) is a polynomial in \\(d\\) variables that correspond to the values \\(\\mathbf{u}[N-1,:]\\) that Bob has and trivially has degree \\(D\\leq 2^{L}\\). Now, Alice can run the model \\(\\mathcal{M}\\), retrieve the coefficients of \\(Q\\), and send it to Bob. Since we assume that \\(P\\) solves AR, Bob can take the coefficients of \\(Q\\) and substitute \\(\\mathbf{u}[N-1,:]\\) to \\(Q\\) to compute \\(P(\\mathbf{u})\\) which is the associated value of \\(i\\).\n' +
      '\n' +
      'Here, the polynomial \\(Q\\) that Alice sends has at most \\(d^{2^{L}}\\) coefficients as each term in \\(Q\\) can have degree at most \\(2^{L}\\). If each such coefficient has \\(B\\) bits, then using theorem F.2, the total number of bits being communicated must satisfy \\(B\\cdot d^{2^{L}}\\geq\\Omega(N)\\). This follows from the fact that if \\(B\\cdot d^{2^{L}}\\leq o(N)\\), then since the associated value of \\(i\\) in equation 26 is the answer to the indexing problem, we have shown that a one-way communication protocol for solving the index problem uses \\(o(N)\\) communication complexity, which then contradicts theorem F.2. Thus, we must have\n' +
      '\n' +
      '\\[B\\cdot d^{2^{L}}\\geq\\Omega(N)\\implies 2^{L}\\log(d)\\geq\\log\\left(\\frac{N}{B} \\right)-O(1).\\]\n' +
      '\n' +
      'Taking logarithm of both sides then yields\n' +
      '\n' +
      '\\[L \\geq\\log\\left(\\frac{\\log\\left(\\frac{N}{B}\\right)}{\\log\\left(d \\right)}\\right)-O(1)\\geq\\log\\left(\\frac{\\log N-\\log B}{\\log\\left(d\\right)} \\right)-O(1)\\] \\[\\geq\\log\\left(\\frac{\\log N-\\log B}{(\\log N)^{1-\\epsilon}}\\right), \\tag{28}\\]\n' +
      '\n' +
      'where we use the fact that \\(d\\leq 2^{(\\log N)^{1-\\epsilon}}\\) for any \\(\\epsilon>0\\) in equation 28.\n' +
      '\n' +
      'Moreover, as the model parameters are assumed to be \\(O(\\log N)\\) bits, any coefficient in \\(Q\\) should have absolute value at most \\(\\left(2^{O(\\log N)}\\cdot Nd\\right)^{2^{L}}\\) as each coefficient can be a product of at most \\(Nd\\) variables. That is, for some \\(\\alpha>0\\), we have the following bound on each coefficient:\n' +
      '\n' +
      '\\[2^{B}\\leq\\left(N^{\\alpha+1}d\\right)^{2^{L}}\\leq\\left(N^{(\\alpha+2)2^{L}}\\right)\\]\n' +
      '\n' +
      'where the last equality uses the fact that \\(d\\leq N\\). We thus have\n' +
      '\n' +
      '\\[\\log(B)\\leq\\log(\\alpha+2)+L+\\log\\log N. \\tag{29}\\]Substituting equation 29 to equation 28, we get\n' +
      '\n' +
      '\\[L\\geq\\log\\left(\\frac{\\log N-\\log(\\alpha+2)-L-\\log\\log N}{(\\log N)^{1- \\epsilon}}\\right) \\tag{30}\\]\n' +
      '\n' +
      'Now, if \\(L>\\log\\log N\\), we are done. Otherwise, if \\(L\\leq\\log\\log N\\), then we can substitute this to equation 30 to get\n' +
      '\n' +
      '\\[L \\geq\\log\\left(\\frac{\\log N-\\log(\\alpha+2)-2\\log\\log N}{(\\log N)^{ 1-\\epsilon}}\\right)\\] \\[=\\log\\left(\\log N-\\log(\\alpha+2)-2\\log\\log N\\right)-(1-\\epsilon) \\log\\log N \\tag{31}\\]\n' +
      '\n' +
      'We now claim that first term in equation 31 satisfies the following:\n' +
      '\n' +
      '\\[\\log\\left(\\log N-\\log(\\alpha+2)-2\\log\\log N\\right)\\geq(1-\\frac{ \\epsilon}{2})\\log\\log N. \\tag{32}\\]\n' +
      '\n' +
      'To see this, note that, for sufficiently large enough \\(N\\), the following holds:\n' +
      '\n' +
      '\\[\\frac{\\log N}{2}\\geq\\log(\\alpha+2)+2\\log\\log N,\\]\n' +
      '\n' +
      'hence, we get\n' +
      '\n' +
      '\\[\\log\\left(\\log N-\\log(\\alpha+2)-2\\log\\log N\\right)\\geq\\log\\left( \\frac{\\log N}{2}\\right)\\geq\\log\\log N-1\\geq(1-\\frac{\\epsilon}{2})\\log\\log N.\\]\n' +
      '\n' +
      'This proves the claim in equation 32. Finally, using equation 32, equation 31 leads to the following:\n' +
      '\n' +
      '\\[L\\geq(1-\\frac{\\epsilon}{2})\\log\\log N-(1-\\epsilon)\\log\\log N= \\frac{\\epsilon}{2}\\log\\log N,\\]\n' +
      '\n' +
      'which still provides the lower bound \\(L=\\Omega(\\epsilon\\log\\log N)\\), as desired. \n' +
      '\n' +
      '**Remark F.3**.: We remark that it is possible to extend theorem F.4 to any model whose output from each layer is a polynomial of some degree \\(\\Delta\\geq 2\\) to get a lower bound of \\(\\Omega(\\epsilon\\log\\log N/\\log\\Delta)\\).\n' +
      '\n' +
      '#### f.4.4 Lower Bound on the Number of Layers for MQAR with \\(d=\\log_{2}c\\)\n' +
      '\n' +
      'Setup.We take \\(d=\\log_{2}c\\) to encode all \\(c\\) possible tokens from \\(C\\). That is, all the \\(2^{d}\\) possible \\(d\\)-bit vectors can appear as a token in the input for MQAR. We will show that data-independent BaseConv needs \\(\\Omega(\\log d)\\) = \\(\\Omega(\\log\\log c)\\)-layers to solve this setting of MQAR, while Attention (+ReLU) can solve this in \\(O(1)\\) layers.\n' +
      '\n' +
      'We first provide the trivial solution using Attention (+ReLU).\n' +
      '\n' +
      '**Proposition F.2**.: _Attention (with linear biases and ReLU) followed by two layers of MLPs can solve MQAR for an input sequence \\(\\mathbf{u}\\in\\{0,1\\}^{3N\\times d}\\) such that \\(d=\\log_{2}(c)\\) in \\(O(1)\\) layers._\n' +
      '\n' +
      'Proof.: Given a row \\(\\mathbf{u}[i,:]\\in\\{0,1\\}^{d}\\), we express each row as \\(\\mathbf{w}[i,:]\\in\\{-1,1\\}^{d}\\) by applying the projection \\(\\mathbf{u}\\mathbf{W}+\\mathbf{B}\\), where \\(\\mathbf{W}:=\\operatorname{diag}(2,\\ldots,2)\\in\\mathbb{R}^{d\\times d}\\) and the bias matrix \\(\\mathbf{B}\\) is the matrix of all \\(-1\\)\'s so that \\(\\mathbf{w}[i,j]=2\\mathbf{u}[i,j]-1\\). Then, we can specify the query and key projection matrices \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{3N\\times d}\\) as follows:\n' +
      '\n' +
      '\\[\\mathbf{K}[i,:] \\equiv\\begin{cases}\\mathbf{w}[i,:]=\\mathbf{k}_{\\lfloor i/3\\rfloor}&\\text{ if }i\\equiv 0\\mod 3\\\\ \\mathbf{0}&\\text{otherwise}\\end{cases}\\] \\[\\mathbf{Q}[i,:] \\equiv\\begin{cases}\\mathbf{w}[i,:]=\\mathbf{q}_{\\lfloor i/3\\rfloor}&\\text{ if }i\\equiv 2\\mod 3\\\\ \\mathbf{0}&\\text{otherwise}\\end{cases},\\] \\[\\mathbf{V}[i,:] \\equiv\\begin{cases}\\mathbf{w}[i+1,:]=\\mathbf{v}_{\\lfloor i/3\\rfloor}& \\text{if }i\\equiv 0\\mod 3\\\\ \\mathbf{0}&\\text{otherwise}\\end{cases}\\]where the values are shifted to the corresponding key index. Computing the pair-wise inner products then yields\n' +
      '\n' +
      '\\[\\mathbf{Q}\\mathbf{K}^{\\top}[i,j]\\equiv\\begin{cases}\\langle\\mathbf{q}_{\\lfloor i/3 \\rfloor},\\mathbf{k}_{\\lfloor j/3\\rfloor}\\rangle&\\text{if $i\\equiv 2\\mod 3$ and $j\\equiv 0\\mod 3$}\\\\ \\mathbf{0}&\\text{otherwise}\\end{cases}\\]\n' +
      '\n' +
      'However, since both \\(\\mathbf{q}_{\\lfloor i/3\\rfloor},\\mathbf{k}_{\\lfloor j/3\\rfloor}\\in\\{-1,1\\}^{d}\\), we have \\(\\langle\\mathbf{q}_{\\lfloor i/3\\rfloor},\\mathbf{k}_{\\lfloor j/3\\rfloor}\\rangle\\leq d\\) with equality iff \\(\\mathbf{q}_{\\lfloor i/3\\rfloor}\\equiv\\mathbf{k}_{\\lfloor j/3\\rfloor}\\). We then subtract off \\(d-1\\) from each of the \\(3N\\times 3N\\) entries by taking the bias \\(\\mathbf{B}\\in\\mathbb{R}^{3N\\times 3N}\\) as the matrix with each entry \\(-d+1\\). Let \\(\\mathbf{Z}:=\\textsc{ReLU}(\\mathbf{Q}\\mathbf{K}^{\\top}+\\mathbf{B})\\) so that we have\n' +
      '\n' +
      '\\[\\mathbf{Z}[i,j]=\\mathbb{1}\\,\\{\\mathbf{q}_{\\lfloor i/3\\rfloor}\\equiv\\mathbf{k}_{ \\lfloor j/3\\rfloor}\\}.\\]\n' +
      '\n' +
      'Next, as we may have multiple matches and we only need to return \\(1\\), we modify \\(\\mathbf{Z}\\) by multiplying with the matrices \\(\\mathbf{W}_{1},\\mathbf{W}_{2}\\in\\mathbb{R}^{d\\times d}\\) and adding the bias \\(\\mathbf{B}\\in\\mathbb{R}^{d\\times d}\\) defined as follows:\n' +
      '\n' +
      '\\[\\mathbf{W}_{1}[k,j]:=\\begin{cases}1&\\text{if $k\\geq j$}\\\\ 0&\\text{otherwise}\\end{cases},\\quad\\mathbf{W}_{2}[\\ell,k]:=\\begin{cases}-1&\\text{ if $k=0$}\\\\ 1&\\text{if $k=\\ell,\\ell\\neq 0$}\\\\ 0&\\text{otherwise}\\end{cases},\\quad\\mathbf{B}[i,j]=1.\\]\n' +
      '\n' +
      'For \\(\\mathbf{Z}_{1}:=\\mathbf{Z}\\mathbf{W}_{1}\\) and \\(\\mathbf{Z}_{2}:=\\mathbf{Z}\\mathbf{W}_{1}\\mathbf{W}_{2}\\), we have:\n' +
      '\n' +
      '\\[\\mathbf{Z}_{1}[i,j] =\\sum_{k}\\mathbf{Z}[i,k]\\mathbf{W}_{1}[k,j]=\\sum_{k\\geq j}\\mathbf{Z}[i,k],\\] \\[\\mathbf{Z}_{2}[i,j] =\\sum_{k}\\mathbf{Z}_{1}[i,k]\\mathbf{W}_{2}[k,j]=\\mathbf{Z}_{1}[i,j]-\\mathbf{Z}_{1 }[i,0].\\]\n' +
      '\n' +
      'That is, each entry in \\(\\mathbf{Z}_{1}\\) sums the entries in the row that are at the same or higher column index while each column in \\(\\mathbf{Z}_{2}\\) subtracts the first entry--the sum of all entries in the row--from each entry in the row. Semantically, for each row in \\(\\mathbf{Z}_{1}\\), the entries from \\(0\\) to the index of the first match must have the same value, and thus, are the only non-negative entries in \\(\\mathbf{Z}_{2}\\). Next, we add the bias and activate under ReLU to get \\(\\mathbf{Z}^{\\prime}\\in\\mathbb{R}^{3N\\times d}\\):\n' +
      '\n' +
      '\\[\\mathbf{Z}^{\\prime}[i,k]:=\\textsc{ReLU}(\\mathbf{Z}_{2}+\\mathbf{B})[i,k]=\\begin{cases}1& \\text{if $k\\leq\\min\\{j|\\ \\mathbf{q}_{\\lfloor i/3\\rfloor}\\equiv\\mathbf{k}_{\\lfloor j/3 \\rfloor}\\}$}\\\\ 0&\\text{otherwise}.\\end{cases}\\]\n' +
      '\n' +
      'Now, we multiply by the weight matrix \\(\\mathbf{W}_{3}\\in\\mathbb{R}^{3N\\times d}\\) defined as\n' +
      '\n' +
      '\\[\\mathbf{W}_{3}[k,j]:=\\begin{cases}-1&\\text{if $k=j+1$}\\\\ 1&\\text{if $k=j$}\\\\ 0&\\text{otherwise}\\end{cases}\\]\n' +
      '\n' +
      'This yields the retriever \\(\\overline{\\mathbf{Z}}=\\mathbf{Z}^{\\prime}\\mathbf{W}_{3}\\in\\mathbb{R}^{3N\\times d}\\) given by\n' +
      '\n' +
      '\\[\\overline{\\mathbf{Z}}[i,k]:=\\sum_{\\ell}\\mathbf{Z}^{\\prime}[i,\\ell]\\mathbf{W}_{3}[\\ell,k] =\\mathbf{Z}^{\\prime}[i,k]-\\mathbf{Z}^{\\prime}[i,k+1]=\\mathbb{1}\\{k=\\min\\{j|\\ \\mathbf{q}_{\\lfloor i/3\\rfloor}\\equiv\\mathbf{k}_{\\lfloor j/3 \\rfloor}\\}\\}.\\]\n' +
      '\n' +
      'Finally, we multiply with the values \\(\\mathbf{V}\\) to get\n' +
      '\n' +
      '\\[(\\overline{\\mathbf{Z}}\\mathbf{V})[i,:]\\equiv\\overline{\\mathbf{Z}}[i,:]\\mathbf{V}\\equiv \\overline{\\mathbf{Z}}[i,j^{*}]\\cdot\\mathbf{V}[j^{*},:]\\equiv\\begin{cases}\\mathbf{v}_{ j^{*}}&\\text{if $\\mathbf{q}_{\\lfloor i/3\\rfloor}\\equiv\\mathbf{k}_{\\lfloor j^{*}/3 \\rfloor},j^{*}=\\min\\{j|\\ \\mathbf{q}_{\\lfloor i/3\\rfloor}\\equiv\\mathbf{k}_{\\lfloor j/3 \\rfloor}\\}$}.\\\\ \\mathbf{0}&\\text{if no such $j^{*}$ exists}.\\end{cases}\\]\n' +
      '\n' +
      'That is, the row corresponding to the query returns the value associated to the first matching key. Thus, the model with Attention (computing \\(\\mathbf{Z}\\)) followed by two MLPs computing \\(\\mathbf{Z}^{\\prime}\\) and \\(\\overline{\\mathbf{Z}}\\), respectively, solves the MQAR problem. \n' +
      '\n' +
      'Next, we relate the output of \\(L\\) layers of BaseConv to the degree of the polynomial that it computes.\n' +
      '\n' +
      '**Lemma F.1**.: _For any input sequence \\(\\mathbf{u}\\), there exists a multilinear polynomial equivalent (over Boolean inputs) to the polynomial computed by \\(L\\) layers of BaseConv with degree at most \\(2^{L}\\)._Proof.: Let \\(P(\\mathbf{u})\\) be the polynomial computed by \\(L\\) layers of BaseConv. Since the output of a single layer of BaseConv is equivalent to a polynomial over the input variables with degree at most 2, composing \\(L\\) such layers yields a polynomial of degree at most \\(2^{L}\\). However, \\(P(\\mathbf{u})\\) need not be multi linear, but the polynomial defined as\n' +
      '\n' +
      '\\[Q(\\mathbf{u}):=(\\cdots((P(\\mathbf{u})\\mod(u_{1}^{2}-u_{1}))\\mod(u_{2}^{2}-u_{2}))\\cdots) \\mod(u_{3Nd}^{2}-u_{3Nd})\\]\n' +
      '\n' +
      'is equivalent to \\(P(\\mathbf{u})\\) as \\((u_{i}^{2}-u_{i})\\) evaluates to 0 for each input var \\(u_{i}\\in\\{0,1\\}\\). However, \\(\\deg(Q(\\mathbf{u}))\\leq\\deg(P(\\mathbf{u}))\\), and thus, the claim holds. \n' +
      '\n' +
      'We now relate the MQAR (in the above setting) to the degree of the polynomial that it computes.\n' +
      '\n' +
      '**Lemma F.2**.: _The MQAR problem with \\(d=\\log_{2}(c)\\) is represented by a multi-linear polynomial of degree \\(2d+1\\)._\n' +
      '\n' +
      'Proof.: We will start by specifying the obvious Boolean circuit that solves MQAR. First, we take the XNOR of keys and queries bitwise as follows.\n' +
      '\n' +
      '\\[\\mathbf{x}^{ij}=\\mathbf{q}_{i}\\texttt{ xnor }\\mathbf{k}_{j}:=(\\mathbf{q}_{i}\\wedge\\mathbf{k}_{j}) \\vee(\\neg\\mathbf{q}_{i}\\wedge\\neg\\mathbf{k}_{j})\\,\\text{ for }i>j, \\tag{33}\\]\n' +
      '\n' +
      'where, for \\(\\mathbf{x},\\mathbf{y}\\in\\{0,1\\}^{d}\\), we have\n' +
      '\n' +
      '\\[[\\mathbf{x}\\texttt{ xnor }\\mathbf{y}][k]:=\\begin{cases}1&\\text{if }\\mathbf{x}[k]=\\mathbf{y}[k] \\\\ 0&\\text{otherwise}\\end{cases}\\]\n' +
      '\n' +
      'That is, each bit from \\(\\mathbf{x}^{ij}\\) is set to 1 iff the corresponding bits from \\(\\mathbf{q}_{i}\\) and \\(\\mathbf{k}_{j}\\) match. Next, we take the AND of the \\(d\\)-bits to get\n' +
      '\n' +
      '\\[\\mathbf{y}^{ij}:=\\bigwedge_{k\\in[d]}\\mathbf{x}_{k}^{ij},i>j. \\tag{34}\\]\n' +
      '\n' +
      'Thus, \\(\\mathbf{y}^{ij}\\) is set to 1 iff the query \\(\\mathbf{q}_{i}\\) matches with the key \\(\\mathbf{k}_{j}\\). Finally, we AND with each bit of the values to get the output \\(\\mathbf{z}^{ij}\\) with the \\(k\\)th bit for \\(k\\in[d]\\) given by\n' +
      '\n' +
      '\\[\\mathbf{z}_{k}^{ij}:=\\mathbf{y}_{ij}\\wedge[\\mathbf{v}_{j}]_{k}. \\tag{35}\\]\n' +
      '\n' +
      'Thus, the output of the circuit can be represented as\n' +
      '\n' +
      '\\[\\mathbf{z}^{ij}=\\begin{cases}\\mathbf{v}_{i}&\\text{if }\\mathbf{q}_{i}\\equiv\\mathbf{k}_{j},i>j \\\\ \\mathbf{0}&\\text{otherwise}.\\end{cases}\\]\n' +
      '\n' +
      'We can now directly translate the above circuit into a multi-linear polynomial. With slight abuse of notation, we have the following correspondence for equation 34, where \\(\\mathbf{u}_{i}\\equiv\\mathbf{q}_{i},\\mathbf{u}_{j}\\equiv\\mathbf{k}_{j},i>j\\) and we use \\(\\mathbf{u}_{ij}\\) to represent the variable corresponding to the entry \\(\\mathbf{u}[i,j]\\).\n' +
      '\n' +
      '\\[\\mathbf{x}_{k}^{ij}(\\mathbf{u}):=\\mathbf{u}_{ik}\\mathbf{u}_{jk}+(1-\\mathbf{u}_{ik})(1-\\mathbf{u}_{jk}) \\quad\\text{for each }k\\in[d],i>j.\\]\n' +
      '\n' +
      'Next, we translate equation 34 as follows.\n' +
      '\n' +
      '\\[\\mathbf{y}^{ij}(\\mathbf{u}):=\\prod_{k\\in[d]}\\left(\\mathbf{u}_{ik}\\mathbf{u}_{jk}+(1-\\mathbf{u}_{ ik})(1-\\mathbf{u}_{jk})\\right).\\]\n' +
      '\n' +
      'Finally, we can write the polynomial that computes MQAR as follows.\n' +
      '\n' +
      '\\[\\mathbf{z}^{ij}(\\mathbf{u}):=\\left(\\prod_{k\\in[d]}\\mathbf{u}_{ik}\\mathbf{u}_{jk}+(1-\\mathbf{u}_{ ik})(1-\\mathbf{u}_{jk})\\right)\\mathbf{u}_{(i+1)k}\\quad\\text{for each }k\\in[d],i>j, \\tag{36}\\]\n' +
      '\n' +
      'where \\(\\mathbf{u}[i+1,:]\\equiv\\mathbf{v}_{j}\\). It is then easy to observe that equation 36 is multi-linear and has degree \\(2d+1\\).\n' +
      '\n' +
      'We are now ready to provide the lower bound.\n' +
      '\n' +
      '**Theorem F.5**.: _A data-independent BaseConv model needs \\(\\log(2d)\\)-layers to solve MQAR for an input sequence \\(\\mathbf{u}\\in\\{0,1\\}^{3N\\times d}\\) with \\(d=\\log_{2}(c)\\)._\n' +
      '\n' +
      'Proof.: Due to Lemma F.2, we know there exists a multi-linear polynomial that solves MQAR, and due to [83, Lecture 3, Proposition 4], it is unique. Specifically we cannot solve MQAR with a multi-linear polynomial of degree \\(\\leq 2d\\). Now, assume that there is a BaseConv model with \\(L\\) layers that exactly solves MQAR. Then, due to Lemma F.1, this yields a multilinear polynomial \\(P(\\mathbf{u})\\) of degree at most \\(2^{L}\\). Here, if \\(L\\leq\\log(2d)\\), then the resulting BaseConv with \\(L\\) layers results in a multilinear polynomial of degree \\(\\leq 2d\\). This contradicts the above claim that we cannot have a multi linear polynomial of degree \\(<2d+1\\) that exactly represents MQAR. Consequently, a data-independent BaseConv model needs \\(\\geq\\log(2d)\\)-layers to solve MQAR. \n' +
      '\n' +
      '### Lower Bound on the Number of Layers for \\(d\\geq\\log_{2}c\\) with Specific Encodings\n' +
      '\n' +
      '#### f.5.1 The Equality Problem\n' +
      '\n' +
      'For an input pair \\(\\mathbf{u}_{1},\\mathbf{u}_{2}\\) where each \\(\\mathbf{u}_{i}\\) is a token drawn from a vocabulary of size \\(c=|C|\\) and embedded in \\(\\{0,1\\}^{d}\\), we define the _equality problem_ (EQ) as checking whether the two encodings are equal: \\(\\mathbf{u}_{1}\\equiv\\mathbf{u}_{2}\\).\n' +
      '\n' +
      'We first note that any model that solves MQAR also solves EQ via the following proposition.\n' +
      '\n' +
      '**Proposition F.3**.: _Any model \\(M_{\\mathrm{MQAR}}\\) that solves MQAR also solves EQ using the same number of layers._\n' +
      '\n' +
      'Proof.: If there exists a model \\(M_{\\mathrm{MQAR}}\\) that solves MQAR using \\(L\\) layers, then for an arbitrary input instance for EQ given by \\(\\mathbf{u}_{1},\\mathbf{u}_{2}\\in\\mathbb{R}^{2\\times d}\\), we can produce the following input instance for MQAR: \\(\\mathbf{u}:=\\{(\\mathbf{u}_{1},\\mathbbm{1},\\mathbf{u}_{1}),(\\mathbf{u}_{2},\\mathbbm{1},\\mathbf{u}_ {2})\\}\\) and solve EQ using \\(L\\) layers with \\(M_{\\mathrm{MQAR}}\\) returning \\(\\mathbbm{1}\\) iff there is a match. \n' +
      '\n' +
      'Due to Proposition F.3, we obtain the following corollary.\n' +
      '\n' +
      '**Corollary F.2**.: _Any lower bound \\(\\overline{L}\\) on the number of layers \\(L\\) of BaseConv to solving EQ is also a lower bound on the number of layers required for solving MQAR._\n' +
      '\n' +
      'We now try to prove a lower bound for the case of \\(d\\geq\\log_{2}c\\). First, note that there are embeddings here where the lower bound from F.5 holds: consider the embedding where the first \\(\\log_{2}c\\) has the compact binary embedding as before but the last \\(d-\\log_{2}c\\) bits are the same for all the tokens. We will instead prove a lower bound for a more interesting set of embeddings.\n' +
      '\n' +
      '#### f.5.2 The \\(p\\)-Hot Encoding for \\(p\\geq 1\\)\n' +
      '\n' +
      '**Definition F.7** ((Almost) \\(p\\)-Hot Encoding).: We define the \\(p\\)_-hot encoding_ to be the collection of embeddings for a token \\(\\mathbf{x}_{t}\\) with \\(0\\leq t<c\\) such that we express \\(t\\) in base \\(\\sqrt[p]{c}:(t_{0},..,t_{p-1})\\in[0,\\sqrt[p]{c})^{p}\\) and represent each \\(t_{i}\\) as one hot encoding in \\(\\{0,1\\}^{\\sqrt[p]{c}}\\). That is, we take \\(d=p\\cdot\\sqrt[p]{c}\\).\n' +
      '\n' +
      'Moreover, we define the _almost \\(p\\)-hot encoding_ to be the collection of embeddings where each \\(t_{i}\\) is mapped in \\(\\{0,1\\}^{\\sqrt[p]{c}-1}\\) obtained by dropping the last bit of its one-hot encoding in \\(\\{0,1\\}^{\\sqrt[p]{c}}\\).\n' +
      '\n' +
      'Note that both of the encodings have \\(p\\)-many blocks derived from each of the one-hot encodings.\n' +
      '\n' +
      '**Definition F.8** (Block-Exclusive).: We say that a polynomial \\(P\\) with variables in \\(\\mathbf{u}:=(\\mathbf{u}_{0},\\ldots,\\mathbf{u}_{p-1})\\) is _block-exclusive_ if each non-zero monomial in \\(P\\) given by the product\n' +
      '\n' +
      '\\[\\prod_{i\\in[p],\\ j\\in\\{\\sqrt[p]{c}\\}}\\mathbf{u}_{i,j}\\]\n' +
      '\n' +
      'does not contain any product of the form \\(\\mathbf{u}_{i,j}\\mathbf{u}_{i,j^{\\prime}}\\) for \\(i\\in[p],j,j^{\\prime}\\in[\\sqrt[p]{c}]\\).\n' +
      '\n' +
      '**Remark F.4**.: The condition specified in Definition F.8 ensures that a block-exclusive polynomial is necessarily multilinear, as it disallows the term \\(\\mathbf{u}_{i,j}\\mathbf{u}_{i,j^{\\prime}}\\) for \\(j=j^{\\prime}\\) in any non-zero monomial.\n' +
      '\n' +
      '**Lemma F.3**.: _For any Boolean function \\(f:\\{0,1\\}\\to\\{0,1\\}\\) with inputs from the almost \\(p\\)-hot encoding_ or _the \\(p\\)-hot encoding setting, there exists a block-exclusive polynomial equivalent to \\(f\\)._Proof.: Given an input \\(\\mathbf{u}\\) to \\(f\\) from the almost \\(p\\)-hot encoding or the \\(p\\)-hot encoding such that \\(\\mathbf{u}:=(\\mathbf{u}_{0},\\ldots,\\mathbf{u}_{p-1})\\), we first observe that the polynomial \\(P(\\mathbf{u})\\) representing \\(f(\\mathbf{u})\\) cannot have a non-zero monomial with variables from the same block. Specifically, for \\(0\\leq j<p\\), any non-zero monomial in \\(P\\) cannot have a product of the form \\(\\mathbf{u}_{j,k}\\mathbf{u}_{j,k^{\\prime}}\\) for \\(k\\neq k^{\\prime}\\). To see this, assume that there exists a non-zero monomial in \\(P\\) with at least two terms \\(\\mathbf{u}_{j,k}\\mathbf{u}_{j,k^{\\prime}}\\) from the same \\(j\\)th block in \\(\\mathbf{u}\\), then monomial always evaluates to \\(0\\) as the \\(j\\)th block is derived from the one-hot encoding in \\(\\{0,1\\}^{\\sqrt[p]{c}}\\) or the almost one-hot encoding in \\(\\{0,1\\}^{\\sqrt[p]{c}-1}\\), and hence, cannot have more than one bit set to \\(1\\).\n' +
      '\n' +
      'Next, if a non-zero monomial in \\(P\\) does contain a product of the form \\(\\mathbf{u}_{j,k}\\mathbf{u}_{j,k^{\\prime}}\\) for \\(k,k^{\\prime}\\in[\\sqrt[p]{c}]\\), we can define the polynomial\n' +
      '\n' +
      '\\[Q(\\mathbf{u}):=(\\cdots((P(\\mathbf{u})\\mod(u_{0,0}^{2}-u_{0,0}))\\mod(u_{0,1}^{2}-u_{0,1 }))\\cdots)\\mod(u_{p-1,\\sqrt[p]{c}-1}^{2}-u_{p-1,\\sqrt[p]{c}-1}).\\]\n' +
      '\n' +
      'Since each entry is Boolean, \\(Q\\) is equivalent to \\(P\\) over Boolean inputs, and thus, \\(Q\\) is the block-exclusive polynomial equivalent to \\(f\\). \n' +
      '\n' +
      '**Proposition F.4**.: _Any Boolean function \\(f:\\{0,1\\}\\to\\{0,1\\}\\) with inputs from the almost \\(p\\)-hot encoding setting has a unique representation as a block-exclusive polynomial._\n' +
      '\n' +
      'Proof.: Due to [83, Proposition 4], we know that every Boolean function \\(f\\) is represented by a multilinear polynomial. Moreover, from Lemma F.3, we know that the polynomial \\(P(\\mathbf{u})\\) representing \\(f(\\mathbf{u})\\) is block-exclusive for \\(\\mathbf{u}\\) with the almost \\(p\\)-hot encoding.\n' +
      '\n' +
      'To show uniqueness, we replicate the argument from [83, Lecture 3, Proposition 4]: Given two block-exclusive polynomials \\(P\\) and \\(P^{\\prime}\\) equivalent to \\(f\\) with inputs from the almost \\(p\\)-hot encoding, we have \\((P-P^{\\prime})(\\mathbf{u})\\equiv 0\\). Now, assume, for the sake of contradiction, that \\(P-P^{\\prime}\\not\\equiv 0\\). Here, note that as \\(P-P^{\\prime}\\) is not identically zero and we have a non-zero monomial, and since the inputs are from the almost \\(p\\)-hot encoding, we know that this monomial cannot contain any product of the form \\(\\mathbf{u}_{j,k}\\mathbf{u}_{j,k^{\\prime}}\\). Let \\(S\\subseteq[p]\\times[\\sqrt[p]{c}-1]\\) be a minimal set of indices such that the monomial \\(\\prod_{(j,k)\\in S}\\mathbf{u}_{j,k}\\) appears in \\(P-P^{\\prime}\\) with non-zero coefficient. Note that \\(\\chi_{S}\\) forms a valid input to \\(f\\) as each block in \\(S\\) can be assigned at most one non-zero entry. Then, since \\((P-P^{\\prime})(\\chi_{S})\\neq 0\\) as every other monomial will get at least one variable that is assigned to \\(0\\) for \\(\\chi_{S}\\), we achieve a contradiction, and thus, \\(P-P^{\\prime}\\) must be identically zero on inputs from the almost \\(p\\)-hot encoding. \n' +
      '\n' +
      '**Lemma F.4**.: _The EQ problem in the almost \\(p\\)-hot encoding setting is represented by a block-exclusive polynomial of degree \\(2p\\)._\n' +
      '\n' +
      'Proof.: Each input pair \\(\\mathbf{u}^{1},\\mathbf{u}^{2}\\) to the EQ problem can be represented as \\(\\mathbf{u}^{i}:=(\\mathbf{u}^{i}_{0},\\ldots,\\mathbf{u}^{i}_{p-1})\\) for \\(i\\in\\{1,2\\}\\), where for each \\(0<j<p\\) such that we have\n' +
      '\n' +
      '\\[\\mathbf{u}^{i}_{j}:=(\\mathbf{u}^{i}_{j,0},\\ldots,\\mathbf{u}^{i}_{j,\\sqrt[p]{c}-2})\\in\\{0, 1\\}^{\\sqrt[p]{c}-1}.\\]\n' +
      '\n' +
      'The following polynomial takes the inner product of each of these one-hot encodings:\n' +
      '\n' +
      '\\[P^{j}(\\mathbf{u}):=\\sum_{k=0}^{\\sqrt[p]{c}-2}\\mathbf{u}^{1}_{j,k}\\cdot\\mathbf{u}^{2}_{j,k} +(1-\\sum_{k=0}^{\\sqrt[p]{c}-2}\\mathbf{u}^{1}_{j,k})(1-\\sum_{k=0}^{\\sqrt[p]{c}-2} \\mathbf{u}^{2}_{j,k})\\]\n' +
      '\n' +
      'for \\(0<j<p\\). Here, note that there can be only be at most \\(1\\) in both \\(\\mathbf{u}^{1}_{j}\\) and \\(\\mathbf{u}^{2}_{j}\\), and thus, \\(P^{j}(\\mathbf{u})=1\\) iff the \\(j\\)th block agree.\n' +
      '\n' +
      'Next, the following polynomial is equivalent to the Boolean function that solves the EQ problem:\n' +
      '\n' +
      '\\[P(\\mathbf{u}):=\\prod_{j=0}^{p-1}P^{j}(\\mathbf{u}),\\]\n' +
      '\n' +
      'and we have \\(P(\\mathbf{u})=1\\{\\mathbf{u}^{1}\\equiv\\mathbf{u}^{2}\\}\\). Here, note that \\(P\\) is multi-linear and has degree \\(2p\\) as each \\(P^{j}\\) is a degree-\\(2\\) polynomial. Moreover, \\(P\\) is block-exclusive as each \\(P^{j}\\) is block-exclusive and we only multiply monomials from different blocks in \\(P\\).\n' +
      '\n' +
      '**Proposition F.5**.: _Let \\(P\\) be the block-exclusive polynomial that solves the EQ problem in the \\(p\\)-hot encoding. Then, \\(\\deg(P)\\geq 2p\\)._\n' +
      '\n' +
      'Proof.: For the sake of contradiction, assume that there exists a block-exclusive polynomial \\(P\\) that solves EQ in the \\(p\\)-hot encoding setting with degree \\(\\leq 2p-1\\). Then, given an input \\(\\mathbf{u}:=(\\mathbf{u}_{0},\\ldots,\\mathbf{u}_{p-1})\\) from the almost \\(p\\)-hot encoding, where each block \\(\\mathbf{u}_{i}\\) corresponds to the truncated bit string from the one-hot encoding in \\(\\{0,1\\}^{\\sqrt[p]{c}-1}\\), we can convert this input to the \\(p\\)-hot encoding \\(\\mathbf{v}:=(\\mathbf{v}_{0},\\ldots,\\mathbf{v}_{p-1})\\) as follows:\n' +
      '\n' +
      '\\[\\mathbf{v}_{i}:=\\left(\\mathbf{u}_{i,0},\\ldots,\\mathbf{u}_{i,\\sqrt[p]{c}-2},1-\\sum_{j=0}^{ \\sqrt[p]{c}-2}\\mathbf{u}_{i,j}\\right)\\]\n' +
      '\n' +
      'Then, the block-wise multilinear polynomial \\(Q(\\mathbf{u})=P(\\mathbf{v})\\) solves the EQ problem in the almost one-hot encoding setting and has \\(\\deg(Q)\\leq\\deg(P)\\leq 2p-1\\) which contradicts the combination of Proposition F.4 and Lemma F.4. \n' +
      '\n' +
      '**Theorem F.6**.: _A data-independent BaseConv model needs at least \\(\\lfloor\\log(2p)\\rfloor\\)-layers to solve MQAR for an input sequence \\(\\mathbf{u}\\in\\{0,1\\}^{3N\\times d}\\) in the \\(p\\)-hot encoding setting, where \\(d=p\\cdot\\sqrt[p]{c}\\)._\n' +
      '\n' +
      'Proof.: We know from Corollary F.2 that it suffices to show a lower bound for the EQ problem. Moreover, we know from Proposition F.5 that we cannot solve the EQ problem in the \\(p\\)-hot encoding setting with a block-exclusive polynomial of degree \\(\\leq 2p-1\\). Now, assume that there is a BaseConv model with \\(L\\) layers that exactly solves EQ in the \\(p\\)-hot encoding setting. Then, due to Lemma F.1 and Proposition F.4, this yields a block-exclusive polynomial \\(P(\\mathbf{u})\\) of degree at most \\(2^{L}\\). Here, if \\(L<\\lfloor\\log(2p)\\rfloor\\) which, then the resulting BaseConv with \\(L\\) layers results in a block-exclusive polynomial of degree \\(\\leq 2p-1\\). This contradicts the above claim that we cannot have a block-exclusive polynomial of degree \\(<2p\\) that exactly represents EQ. Consequently, a data-independent BaseConv model needs \\(\\geq\\lfloor\\log(2p)\\rfloor\\)-layers to solve EQ.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline \\hline  & 355M & 1.4B \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.95\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 27 & 36 \\\\ Hidden Size & 1024 & 1792 \\\\ MLP Activation & SwiGLU \\\\ MLP Width & 2 \\\\ \\hline Num. Linear Attn Layers & 5 & 7 \\\\ Num. Linear Attn Heads & 16 \\\\ Taylor Feature Dimension & 16 \\\\ Linear Attn Positional Encodings & None \\\\ \\hline Num. Sliding Window Layers & 5 & 7 \\\\ Sliding Window Size & 64 & 16 \\\\ Sliding Window Heads & 16 \\\\ Sliding Window Positional Encodings & Rotary \\\\ \\hline Num. BaseConv Layers & 17 & 22 \\\\ BaseConv Projection Expansion Factor & 4 \\\\ BaseConv Filter Size & 3 \\\\ BaseConv Activation & SiLU \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Based Training Settings\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline  & 355M & 1.4B \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.95\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 24 & 36 \\\\ Hidden Size & 1024 & 1680 \\\\ Num Heads & 16 & 24 \\\\ RMSNorm & True \\\\ MLP Bias & False \\\\ Flash Attn & True \\\\ Rotary Emb. Fraction & 0.5 \\\\ MLP Activation & SwiGLU \\\\ MLP Width & 4 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Attention Training Settings\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline  & 355M & 1.4B \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.95\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 46 \\\\ Hidden Size & 1024 & 2048 \\\\ RMSNorm & True \\\\ Norm Epsilon & \\(1e-5\\) \\\\ Dt State & 16 \\\\ Dt (Min, Max) & \\((0.001,0.1)\\) \\\\ Dt Init. Strategy & Random \\\\ Dt Init. Floor & \\(1e-4\\) \\\\ Dt Scale & 1.0 \\\\ Dt Softplus & True \\\\ Projection Expansion Factor & 2 \\\\ Short Conv Filter Size & 4 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: Mamba Training Settings\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline  & 355M \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.95\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 29 \\\\ Hidden Size & 1024 \\\\ Num Heads & 1 \\\\ MLP Width & 2 \\\\ Short Conv. Filter Size & 3 \\\\ Exp. Mod. Decay (Fast, Slow) & 0.3, 1.2 \\\\ Filter Sine Freq. (w) & 14 \\\\ Filter Order & 64 \\\\ Filter Inner MLP & 2 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 10: Hyena Training Settings\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline  & 355M \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.99\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 24 (No Attention Layers) \\\\ Hidden Size & 1024 \\\\ Num Heads & 16 \\\\ MLP Width & 4 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 11: Hyena Training Settings\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline \\hline  & 355M \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.95\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 24 \\\\ Hidden Size & 1024 \\\\ Num Heads & 4 \\\\ MLP Width & 2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 13: Gated Linear Attention (GLA) Training Settings\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r} \\hline \\hline  & 355M \\\\ \\hline Optimizer & Adam \\\\ Optimizer momentum & \\(\\beta_{1},\\beta_{2}=0.9,0.99\\) \\\\ Optimizer eps & \\(1e-8\\) \\\\ Precision & BFloat16 \\\\ \\hline Warmup & 1\\% \\\\ Learning rate decay & Cosine \\\\ Learning rate (min, base) & 8e-5, 8e-4 \\\\ Global batch size & 256 \\\\ Weight decay & 0.1 \\\\ \\hline Num Layers & 24 \\\\ Hidden Size & 1024 \\\\ Num Heads & 4 \\\\ MLP Width & 2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 12: Hyena Training Settings\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>