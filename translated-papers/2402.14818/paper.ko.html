<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# PALO: 5B 사람들을 위한 Polyglot Large Multimodal Model\n' +
      '\n' +
      ' Muhammad Maaz\\({}^{1}\\)\n' +
      '\n' +
      'Hanoona Rasheed\\({}^{1}\\)\n' +
      '\n' +
      'Abdelrahman Shaker\\({}^{1}\\)\n' +
      '\n' +
      'Salman Khan\\({}^{1,2}\\)\n' +
      '\n' +
      'Hisham Cholakal\\({}^{1}\\)\n' +
      '\n' +
      '라오민 Anwer\\({}^{1,3}\\)\n' +
      '\n' +
      'Tim Baldwin\\({}^{1,4}\\)\n' +
      '\n' +
      'Michael Felsberg\\({}^{5}\\)\n' +
      '\n' +
      '파하드 S. 칸\\({}^{1,5}\\)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 연구에서는 보다 포괄적인 시각언어 모델(VLM)을 추구하기 위해 Palo라는 대규모 다국어 멀티모달 모델을 소개한다. 팔로는 총 5B명(세계 인구의 65%)에 이르는 영어, 중국어, 힌디어, 스페인어, 프랑스어, 아랍어, 벵골어, 러시아어, 우르두어, 일본어 등 10개 주요 언어로 시각적 추론 능력을 제공한다. 본 논문에서 제안한 방법은 반자동 번역 방법을 사용하여 다중 모드 명령어 데이터셋을 세밀하게 조정된 대규모 언어 모델을 사용하여 목표 언어에 맞게 수정하여 높은 언어 충실도를 보장하면서도 최소한의 수작업으로 인한 확장성을 보장한다. 다양한 수업 세트의 통합은 특히 힌디어, 아랍어, 벵골어 및 우르두어와 같이 과소 대표되는 여러 언어에 걸친 전반적인 성능을 향상시키는 데 도움이 됩니다. 결과 모델은 3개의 척도(1.7B, 7B 및 13B 매개변수)에 걸쳐 훈련되어 강력한 기준선에 비해 상당한 개선을 관찰할 수 있는 일반화 및 확장성을 보여준다. 또한 언어 간 비전 언어 추론 능력을 평가하기 위한 다가오는 접근법에 대한 첫 번째 다국어 멀티모달 벤치마크를 제안한다. 코드: [https://github.com/mbzual-oryx/PALO](https://github.com/mbzual-oryx/PALO)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '생성 AI의 발전으로 추진된 대형 멀티모달 모델(LMM: Large Multimodal Models)[14, 23, 24]은 시야와 언어 작업 사이의 격차를 원활하게 해소하는 분야에서 중추적인 발전으로 부상했다. LLaVA[14] 및 miniGPT4[25]와 같은 초기 노력은 시각적 입력을 기반으로 효과적인 텍스트 응답을 합성하는 데 흥미로운 성능을 보여주었지만, 주로 영어에 집중하여 비영어 언어에 대한 멀티모달 이해에 상당한 격차를 남겼다. 그 결과, 기존의 LMM은 일반적으로 세계 인구의 언어적 다양성, 특히 수십억 명의 원어민을 통틀어 차지하는 중국어, 힌디어, 스페인어, 프랑스어, 아랍어, 벵골어, 러시아어, 우르두어, 일본어 등 대규모 집단이 구사하는 언어를 간과하고 있다. 우리의 연구는 전 세계 인구의 65%를 차지하는 10개 주요 언어를 포함하는 팔로라는 최초의 완전 오픈 소스 다국어 LMM을 개발함으로써 이러한 격차를 해결한다.\n' +
      '\n' +
      '그림 1: **Palo vs. 영어-VLMs. 그림은 팔로를 10개의 다른 언어에 걸쳐 해당하는 비전 언어 모델(VLM)과 비교한다. 이러한 언어에는 영어, 중국어, 힌디어, 스페인어, 프랑스어, 아랍어, 벵골어, 러시아어, 우르두어, 일본어가 있으며 총체적으로 약 5B명과 전 세계 인구의 65%를 포괄한다. LLaVA 및 MobileVLM과 같은 영어 훈련 VLM은 훈련 단계에서 이러한 언어의 과소 표현으로 인해 힌디어, 아랍어, 벵골어 및 우르두어를 포함한 저자원 언어에서 열악한 성능을 나타낸다. 대조적으로 팔로는 10개 언어 모두에서 동시에 대화를 진행할 수 있는 통합 모델로, 현재 멀티모달 모델에서 과소 대표되는 일관된 성능을 보여준다.**guages.\n' +
      '\n' +
      '문제는 영어에 비해 고품질 다국어 멀티모달 데이터의 부족에 있다. 제한된 고품질 데이터의 과제, 특히 힌디어, 아랍어, 벵골어 및 우르두어와 같은 과소 대표되는 언어의 문제를 해결하기 위해, 우리의 접근법은 각 타겟 언어에 대해 최신 대용량 언어 모델(LLM) 브라운 등(2020)에 의해 생성된 번역의 신중한 분석 및 후속 개선을 포함한다. 인간의 개입을 통해 번역의 부정확성을 식별하고 수정함으로써 고품질 다국어 데이터 세트를 생성한다. 그런 다음 이 큐레이션된 데이터 세트는 목표 언어 주석을 정제하는 기초가 되어 훈련에서 목표 언어의 보다 정확하고 미묘한 표현을 보장한다.\n' +
      '\n' +
      '우리의 고품질 다국어 비전 언어 명령어 데이터 세트와 최근 대형 멀티모달 모델링의 발전을 활용하여 10개 언어로 된 질문에 동시에 답할 수 있는 _unified_ 모델로 Pao를 개발한다. 우리의 훈련 파이프라인은 높은 자원 언어에 대한 성능을 유지(또는 더 향상)하면서 낮은 자원 언어(LLM 훈련 데이터 세트에 과소 표현됨)에서 상당한 이득을 제공한다. 이 작업의 기여도는 다음과 같습니다.\n' +
      '\n' +
      '* Pao: 10개 주요 언어를 포괄하는 첫 번째 다국어 대규모 멀티모달 모델(LMM)을 개발하여 10개 언어 중 어느 하나의 언어로 응답을 생성할 수 있는 일반 모델을 통해 비전 언어 추론을 용이하게 한다.\n' +
      '* 우리는 최첨단 대용량 언어 모델의 목표 언어 번역에 대한 비판적 분석과 후속 개선을 통해 광범위한 다국어(10개 언어) 명령어 조정 데이터 세트를 조립한다. 이 데이터 세트는 여러 언어에 걸쳐 언어학적으로 정확한 콘텐츠를 처리하고 생성하는 숙련도를 향상시키는 데 중추적이다.\n' +
      '* 우리는 우리의 훈련 파이프라인의 확장성을 입증하기 위해 3개의 별개의 스케일, 즉 1.7B, 7B 및 13B 파라미터에 걸쳐 최신 LMMs Liu et al.(2023); Chu et al.(2023)의 다국어 성능을 향상시킨다. 결과적인 폴리글로트 LMM은 영어, 중국어, 프랑스어 및 스페인어와 같은 고자원 언어에 대한 고성능을 손상시키지 않으면서 힌디어, 아랍어, 벵골어 및 우르두어와 같은 저자원 언어에 대한 이해 및 콘텐츠 생성에서 상당한 향상으로 다양한 언어 작업에 대한 성능 향상을 보여준다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '대규모 언어 모델(LLM)의 도입은 자연어 처리 분야를 상당히 발전시켰다. 그러나, 다국어 LLM의 개발은 주로 언어 데이터 Costa-jussa 등(2022)의 왜곡된 분포로 인해 상당한 어려움에 직면해 있다. 영어 및 유럽 언어는 기존의 데이터 세트를 지배하여, Mandarin Chinese 및 Hindi 과소 대표되는 Eberhard 등(2015)과 같은 널리 사용되는 언어를 남긴다. 더욱이, 여러 언어를 LLM에 통합하는 것은 종종 영어 성능 Scao 등(2022)의 저하로 이어지며, 언어 간 성능을 유지하는 데 있어 주요 과제를 강조한다.\n' +
      '\n' +
      '최근 노력은 향상된 능력을 가진 다국어 LLM을 개발함으로써 이러한 문제를 해결하는 것을 목표로 하고 있다. Almazrouei et al.(2023); Touvron et al.(2023); Le Scao et al.; Wei et al.(2023). 46개 언어의 소스들을 포함하는 ROOTS 코퍼스 로렌콘 등(2022)에 대해 훈련된 BLOOM Le Scao 등(2014)은 더 적은 리소스들을 포함하는 광범위한 언어들에 걸쳐 LLM들이 액세스가능하도록 하는데 있어서 실질적인 진전을 표시한다. PaLM Chowdhery et al.(2023)은 정교한 훈련 기술과 새로운 경로 아키텍처를 통해 단일 언어 및 다국어 작업 모두에서 개선된 결과를 달성하는 스케일링의 이점을 보여준다.\n' +
      '\n' +
      'LMMs(Large Multimodal Models)의 발전은 Liu et al. (2023); Chu et al. (2023)은 영역-특정 분석 Rasheed et al. (2023) 및 시공간 대화 Maaz et al. (2023); Lin et al. (2023)은 이 도메인에서 상당한 진전을 강조하면서 유연성을 제공하는 것으로 발전했다. 그러나 다국어 능력에 대한 탐색은 제한적이었다. Qwen Bai et al. (2023) 및 mPLUG-Owl Ye et al. (2023)은 영어와 중국어 모두에서 시각적 입력을 처리하기 위해 LMM 기능을 확장하여 이중 언어 시각적 정보를 처리하는 데 적응성을 보여준다. Ziya-Visual Lu et al. (2023)은 영어 이미지 텍스트 데이터 세트를 중국어로 번역하는 것을 보여주며, 명령어-응답 생성을 위한 인-컨텍스트 학습을 사용한다. 그러나 이러한 LMM은 두 가지 언어로 제한되어 있다.\n' +
      '\n' +
      '우리는 다국어 LMM의 격차를 해결하기 위해 10개 주요 언어에 걸쳐 시각적 추론 기능을 제공하는 최초의 완전 오픈 소스 LMM인 Palo를 소개한다. 폐쇄형 소스이고 API를 통해서만 액세스 가능한 GPT-4(Achiam et al., 2023)와 대조적으로, 우리의 것은 LMM 능력을 다중 언어로 확장하기 위한 오픈 소스 도메인에서 가장 큰 노력이다.\n' +
      '\n' +
      '##3 Palo: Polyglot LMM\n' +
      '\n' +
      '전 세계적으로 접근 가능한 비전 언어 모델(VLM)을 위해 모델 팔로(**P**olyglot Large Multimodal Model)는 10개의 주요 언어로 콘텐츠를 이해하고 생성하여 전 세계 인구의 거의 2/3에 걸쳐 있는 청중에게 제공되도록 설계되었다. Palo의 아키텍처는 LLaVA (Large Language and Vision Assistant)(Liu et al., 2023, 2023) for our large-scale models (7/13B), MobileVLM for our mobile-efficient model (1.7B)로부터 유도되어 Palo가 상이한 계산 설정들에 걸쳐 다재다능하게 유지되는 것을 보장한다.\n' +
      '\n' +
      '아키텍처는 비젼 인코더를 언어 모델과 완벽하게 통합한다(도 2 참조). 입력 이미지 및 사용자 텍스트 쿼리가 주어지면, 모델은 정확한 자연어 응답을 생성한다.\n' +
      '\n' +
      'Palo는 비젼 인코더로서 CLIP ViT-L/14(Radford et al., 2021)를 사용하고 이어서 프로젝터를 사용하여 비젼 토큰을 언어 모델의 입력 임베딩 공간으로 변환한다. LLaVA (Liu et al., 2023)에 이어서, 우리는 우리의 7/13B 모델들을 위한 프로젝터로 GELU 활성화를 갖는 2-층 MLP를 사용한다. 그러나, 모바일팔로-1.7B 모델에는 경량 다운샘플 프로젝터(LDP)(Chu et al., 2023)가 사용된다. LDP는 깊이별 분리 가능한 컨볼루션을 사용하여 비전 토큰을 다운샘플링하여 입력 토큰을 언어 모델로 크게 줄이고 따라서 훈련 및 추론 시간을 크게 줄인다. 또한, LDP의 컨볼루션은 MLP에 비해 매개 변수가 적어 모바일 모델이 매개 변수와 계산 효율이 모두 향상된다. 다른 PALO 버전에 사용된 프로젝터는 그림 2에 나와 있다.\n' +
      '\n' +
      '투영된 비전 토큰들은 이후 토큰화된 사용자 텍스트 쿼리와 연결되고 응답을 생성하기 위해 언어 모델에 전달된다. 팔로가 광범위한 멀티모달 명령어 튜닝 데이터셋을 사용하여 10개 언어를 학습함에 따라 토큰라이저의 용량을 보다 효과적으로 활용할 수 있을 뿐만 아니라 검색 공간을 확장시켜 보다 풍부한 맥락과 훈련에 더 도전적인 예를 제공한다. 언어 모델. 이 접근법은 다양한 언어 세트에 걸쳐 반응을 이해하고 생성하는 모델의 능력을 크게 향상시킨다.\n' +
      '\n' +
      '우리는 7/13B 모델에서 큰 언어 모델(LLM)로 Vicuna(Zheng et al., 2023)를 사용하고 작은 란으로 MobileLLaMA(Chu et al., 2023)를 사용한다.\n' +
      '\n' +
      '그림 2: **팔로의 건축 개요. (_left_) 모델은 이미지를 인코딩하는 비전 인코더와, 언어 모델의 입력 임베딩 공간에 비전 특징을 투영하는 프로젝터로 구성된다. 사용자의 텍스트 쿼리는 토큰화되고, 토큰들은 응답을 생성하기 위해 인과 언어 모델에 입력되기 전에 비전 토큰들과 연결된다. Palo 7B 및 13B 변형의 경우, Vicuna가 Large Language Model로 사용되는 반면 MobileLLaMA(Chu et al., 2023)는 our MobilePalo-1.7B 변형의 Small Language Model로 사용된다. CLIP ViT-L/336px는 모든 변형에서 비전 인코더로 사용된다. 팔로의 상이한 변형들에서 사용되는 (_right_) 프로젝터들이 도시된다. Palo 7B 및 13B의 경우, 다음(Liu et al., 2023), 우리는 GELU 활성화를 갖는 2-층 MLP 프로젝터를 사용한다. 우리의 모바일 버전의 Palo(MobilePalo-1.7B)에 대해, 우리는 (Chu et al., 2023)로부터의 경량 다운샘플 프로젝터(Lightweight Downsample Projector; LDP)를 사용한다. 깊이별 분리 가능한 컨볼루션을 활용하여 이미지 토큰을 다운샘플링하여 표준 MLP 프로젝터보다 빠르다.**\n' +
      '\n' +
      'MobilePalo-1.7B 모델에서의 SLM (guage model) Vicuna fine-tunes LLaMA-2 on user-shared conversation collected from ShareGPT, while LLaMA-2 is pre-trained on 2T tokens collected from different public source[20]. 한편, MobileLLaMA는 RedPajama-v1 [3]의 1.3T 토큰에 대해 프리트레이닝을 수행한 후 공개 버전의 ShareGPT 데이터(Huggingface)에 대해 미세 조정을 수행한다.\n' +
      '\n' +
      '### Dataset\n' +
      '\n' +
      '우리 작업의 주요 기여는 포괄적인 다국어 비전 언어 명령어 조정 데이터 세트의 세심한 준비에 있다. 우리는 초점을 맞추기 위해 최첨단 LMM 모델[17]을 선택하는 것으로 시작한다. 확장 가능한 방법으로 여러 언어에 대해 명령어 조정 데이터 세트를 보다 효과적으로 조정하기 위해 LLM 모델 [1]을 활용하여 반자동 번역 파이프라인을 개발한다. 이 접근법은 영어 데이터세트를 타겟 언어들로 번역하는 것을 수반하여, 강건한 다국어 데이터세트를 생성하고, 이는 모델의 언어적 범위 및 적용 가능성을 상당히 넓힌다.\n' +
      '\n' +
      '**번역 과정 및 도전**: LLM 모델 [1]을 사용하여 영어에서 목표 언어로의 순진한 번역 접근법은 기본적인 의미를 효과적으로 전달하지만 각 언어에 특정한 몇 가지 언어적 과제를 도입한다. 구두, 문법 뉘앙스, 번역 일관성 및 성별 사용 오류와 같은 문제는 직접 LLM 기반 번역을 통해 관찰된다(그림 3 참조). 이러한 도전은 중국어의 성조 복잡성에서 힌디어의 스크립트 다양성에 이르기까지 관련된 언어의 언어적 다양성과 스페인어, 아랍어, 러시아어와 같은 언어의 성별에 따른 복잡성으로 인해 크게 다르다. 예를 들어, 아랍어의 경우 일반적인 구두점 오류는 쉼표와 마침표 주변의 잘못된 간격을 포함한다. 아랍어 문법에서 중요한 명성은 때때로 생략되거나 잘못 적용된다. 또한 특정 영어 단어는 번역된 텍스트에서 번역되지 않은 상태로 남아 있으며 일부 대상 언어에서 문법의 성별 특수성을 고려할 때 상당한 우려를 제기하는 번역에서 잘못된 성별 정렬과 함께 동사가 명사로 잘못 변환되는 경우가 있다.\n' +
      '\n' +
      '**챌린지**: 번역된 데이터 세트의 품질을 개선하기 위해 자동 및 수동 검증 단계의 조합을 사용한다. 이 반자동 파이프라인에서 각 언어에 대한 원어민 팀은 초기 번역에서 작은 하위 집합에 대한 자세한 검토 및 수정을 제공하여 언어별 문제, 성별 정확도 및 전반적인 언어 무결성을 해결한다. 자동화된 스크립트는 공통 구두점 오류를 수정하고 검증 프로세스를 최적화하기 위해 각 언어에 맞게 조정됩니다.\n' +
      '\n' +
      'LLM의 미세 조정**: 다국어 번역에 대한 LLM의 한계를 인정하여 LLM을 미세 조정하기 위한 고품질 데이터 세트로서 수동으로 검증 및 수정된 번역(언어당 1K 대화)을 활용한다. 이러한 미세 조정은 번역 정확도를 향상시키는 것뿐만 아니라 톤 및 오쏘그래피와 같은 각 언어의 특정 속성에 출력을 정렬하는 데 중점을 둔다. 그런 다음 강화 및 미세 조정된 LLM은 영어로부터 약 150K 명령어(즉, [17]로부터의 LLaVA-Instruct-150K)를 포함하는 광범위한 VLM 명령어 튜닝 데이터세트[17]를 각각의 언어로 번역하기 위해 사용된다. 우리는 GPT3.5-터보를 번역 모델로 사용하고 OpenAI 피네튜닝 플랫폼을 사용하여 피네튜닝한다.\n' +
      '\n' +
      '**정제된 데이터의 영향**: 이 프로세스는 팔로의 효과적인 미세 조정에 중요한 포괄적이고 고품질 다국어 데이터 세트를 생성한다. 개선된 데이터세트는 각 언어의 특정 측면을 다룰 뿐만 아니라, 포함된 모든 언어에서 문맥적으로 관련되고 문법적으로 정확한 콘텐츠를 처리하고 생성하는 모델의 능력을 현저하게 향상시킨다. 예를 들어, 그림 3은 아랍어 번역에 대한 영어의 두 가지 주요 개선을 강조하며, 첫 번째 예는 향상된 어휘 정밀도를 보여주고 두 번째 예는 향상된 어휘 정밀도를 보여준다.\n' +
      '\n' +
      '그림 3: **미세조정의 영향을 보여주는 정성적 결과**. LLM을 미세 조정하기 전과 후의 영어와 아랍어 번역의 비교 시각화. 그림은 정확한 어휘 사용, 성별 일치, 문법적 정확성 등 언어별 문제의 개선을 보여 미세 조정 모델의 향상된 성능을 강조한다.\n' +
      '\n' +
      '문법적 일치성이 향상되었음을 보여줍니다. 이 데이터 세트를 LMM의 훈련 프로세스에 통합하는 것은 영어와 9개의 다른 언어를 모두 효과적으로 포함하도록 기능을 확장하는 열쇠이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'LLaVA 및 MobileVLM 기준선과 유사하게 CC-595K [14]라고 하는 CC3M 데이터 세트의 하위 집합에서 모델을 사전 훈련한다. 사전 훈련 중에는 프로젝터만 학습하고 나머지 모델 구성 요소는 냉동 보관한다. 우리는 8개의 A-100 40GB GPU에서 GPU당 32개의 배치 크기로 전체 배치 크기가 256인 1 에폭에 대한 모델을 훈련한다. 모델은 2e-3의 학습률을 갖는 Adam Optimizer와 코사인 LR 스케줄러를 사용하여 최적화하였으며, 1.7B에서는 약 1.5시간, 7B에서는 5시간, 13B 모델에서는 거의 9시간이 소요되었다.\n' +
      '\n' +
      '우리는 10개 언어의 대화를 포함하는 다양한 명령어 데이터 세트에서 모델을 미세 조정한다. 구체적으로, LLaVA-Instruct-665K[14]로부터의 665K 명령어는 영어에 사용되고, LLaVA-Instruct-150K[14]로부터의 중국어, 프랑스어, 스페인어, 러시아어, 일본어, 아랍어, 힌디어, 벵골어 및 우르두어에 대한 대략 150K 대화는 총 약 2.1M 명령어를 합산한다. 미세 조정 동안 비전 인코더만 동결된 상태로 유지되고 나머지 모델은 트레이닝된다. 언어모델은 LORA[14]를 사용하여 \\(\\alpha=128\\)로 미세조정하는 동안 프로젝터는 완전히 학습된다. 우리는 8개의 A-100 GPU에서 GPU당 16개의 배치 크기로 전체 배치 크기가 128인 1 에폭에 대한 모델을 훈련한다. 1.7/7B 변종은 40GB A-100 GPU, 13B 변종은 80GB A-100 GPU를 사용합니다. 모델은 Adam Optimizer와 코사인 LR 스케줄러를 사용하여 최적화하였으며, 프로젝터의 경우 2e-5 베이스 학습률을, 언어 모델의 경우 2e-4 베이스 학습률을 사용하였다. 미세 조정은 1.7B의 경우 약 12시간, 7B의 경우 42시간, 13B 모델의 경우 거의 76시간이 소요된다.\n' +
      '\n' +
      '### 고자원 대 저자원 언어\n' +
      '\n' +
      '우리의 작업은 높은 자원 언어와 낮은 자원 언어의 두 그룹으로 나뉜 10개 언어를 훈련하고 평가한다. 영어, 중국어, 프랑스어, 스페인어, 러시아어 및 일본어는 언어 모델 훈련 데이터에 이러한 언어에서 합리적인 수의 샘플이 포함되어 있기 때문에 고자원 언어로 간주된다. 반면, 아랍어, 힌디어, 벵골어, 우르두어는 언어 모델 학습 데이터에서 과소 대표되어 저자원 언어로 분류된다.\n' +
      '\n' +
      '예를 들어, LLaMA-2[15] 사전 훈련 데이터는 거의 2조 개의 토큰을 포함하고 있으며, 이 중 89.7%는 영어이고 거의 1.92%는 중국어, 프랑스어, 스페인어, 러시아어, 일본어, 21개 더 유사한 언어이다. 아랍어, 힌디어, 벵골어, 우르두의 표현은 무시할 수 있다. 유사하게, MobileLLaMA [14]는 거의 1.3조 토큰, 주로 영어 토큰으로 구성된 RedPajama-v1 [1] 데이터 세트에서 사전 훈련한다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      'VLM의 다국어 능력을 평가할 때 고품질 평가 세트를 활용하여 다양한 언어에 걸쳐 종합적인 평가를 수행한다. 이 세트는 번역기에 의해 구성됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c|c} \\hline \\hline\n' +
      '**Model** & **Eng.** & **Chinese** & **French** & **Spanish** & **Russ.** & **Japan.** & **Arabic** & **Hindi** & **Bengali** & **Urdu** & **Avg.H** & **Avg.L** & **Avg.** \\\\ \\hline LLaVA-7B & **67.9** & **55.7** & **62.4** & **64.5** & 55.3 & **59.2** & 38.9 & 29.4 & 13.9 & 21.8 & **60.8** & 26.0 & 46.9 \\\\ PALo-7B & 64.2 & **55.7** & 58.3 & 61.0 & **57.4** & 57.5 & **57.8** & **57.6** & **51.7** & **55.3** & 59.0 & **55.6** & **57.7** \\\\ \\hline  & -3.7 & 0.0 & -4.1 & -3.5 & +2.1 & -1.7 & +18.9 & +28.2 & +37.8 & +33.5 & -1.8 & +29.6 & +10.8 \\\\ \\hline LLaVA-13B & **69.5** & **62.9** & **67.5** & 64.6 & 62.3 & **65.3** & 37.2 & 27.8 & 20.4 & 22.1 & **65.4** & 26.9 & 49.9 \\\\ PALo-13B & 65.5 & 62.1 & 66.4 & **65.9** & **62.4** & 60.6 & **56.9** & **66.8** & **53.5** & **59.6** & 63.8 & **59.2** & **61.9** \\\\ \\hline  & -4.0 & -0.8 & -1.1 & +1.3 & +0.1 & -4.7 & +19.7 & +39.0 & +33.1 & +37.5 & -1.5 & +32.3 & +12.0 \\\\ \\hline MobileVLM-1.7B & 46.6 & 23.2 & 28.1 & 29.1 & 28.1 & 26.4 & 12.4 & 13.7 & 15.6 & 15.6 & 30.3 & 14.3 & 23.9 \\\\ MobilePalo-1.7B & **48.2** & **34.0** & **42.6** & **40.1** & **38.2** & **32.5** & **32.8** & **26.8** & **19.9** & **24.1** & **39.3** & **25.9** & **33.9** \\\\ \\hline  & +1.6 & +10.8 & +14.5 & +11.0 & +10.1 & +6.1 & +20.4 & +13.1 & +4.3 & +8.5 & +9.0 & +11.6 & +10.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 다국어 멀티모달 평가에 대한 **표준 VLMs vs PALo. 이 표는 LLaVA-Bench(In-the-Wild)의 특수하게 적응된 다국어 버전 10개 언어에 대한 LLaVA 및 MobileVLM과 PALo의 비교를 보여준다. LLaVA 7/13B 및 MobileVLM-1.7B는 LLaVA-Instruct-665K에서 미세 조정되고 PALo는 LLaVA-Instruct-665K와 10개 언어로 번역된 LLaVA-Instruct-150K에서 미세 조정된다. 모든 모델은 CC-595K [14] 데이터 세트에서 사전 훈련된다. Avg.H와 Avg.L은 각각 고자원(영어, 중국어, 프랑스어, 스페인어, 러시아어, 일본어)과 저자원(아랍어, 힌디어, 벵골어, 우르두어) 언어에 대한 평균을 나타낸다. 에이브 LLaVA-Bench(In-the-Wild) Liu et al. (2023)을 GPT-4-Turbo Achiam et al. (2023)을 사용하여 모든 타겟 언어에 대한 평균을 나타내며, 특히 언어적 진정성을 보존하고 신중한 인간 수정을 통해 자동화된 번역의 공통 이슈를 완화하는 데 주의를 기울인다. 벤치마크는 실내 및 실외 장면, 밈, 아트웍과 같은 다양한 도메인의 24개의 다양하고 도전적인 이미지로 구성되며, 각각 상세한 설명과 모델의 이해 및 일반화 능력을 테스트하기 위해 설계된 60개 질문 세트가 있다.\n' +
      '\n' +
      '표 1의 결과는 팔로가 이들 언어에 걸쳐 각각 평균 59.0 및 63.8의 점수를 받는 7/13B 모델에 의해 나타난 바와 같이 고자원 언어에서 강력한 성능을 얻는다는 것을 보여준다. 이것은 우리의 다국어 확장이 모델의 원래 기능을 손상시키지 않고 효과적으로 통합되었음을 보여준다. 또한, 7B 및 13B 모델에 대해 평균 점수가 각각 26.0 및 26.9에서 55.6 및 59.2점으로 상승하여 저자원 언어에서 우수한 성능 개선을 보여준다.\n' +
      '\n' +
      '7B 모델이 평균 57.65점을 달성하고 13B 모델이 61.97점에 도달하는 등 10개 언어에 걸친 전반적인 성능도 개선되었다. 데이터는 우리의 접근법이 비전 언어 작업에서 글로벌 언어의 복잡한 풍경을 다룰 수 있는 보다 포괄적이고 다양하며 성능이 높은 VLM을 성공적으로 생성했음을 반영한다(정성적 결과는 그림 4 및 5 참조).\n' +
      '\n' +
      '우리의 모바일 모델은 모바일VLM 기준 23.9점에 비해 전체 평균 33.9점의 이득으로 높은 자원 언어와 낮은 자원 언어 모두에서 일관된 개선을 보여준다. 7/13B 모델에서 관찰된 추세와 달리, 우리의 모바일 버전은 또한 영어와 중국어와 같은 고자원 언어의 개선을 보여준다. 이러한 성능 차이는 언어 모델 사전 훈련 데이터에 기인한다. LLaMA-2는 1.3조 개의 영어 토큰에 주로 훈련되는 MobileLLaMA에 비해 높은 자원 언어의 표현이 더 나은 2조 개의 토큰에 대해 훈련된다.\n' +
      '\n' +
      '### Ablations\n' +
      '\n' +
      '표 2는 각 언어에서 150K 번역된 지침에 대해 7B 모델을 훈련하고 모든 언어에 걸쳐 모든 모델을 평가한 삭제를 보여준다. 결과는 베이스라인이 중국어, 프랑스어, 스페인어, 일본어를 포함한 고자원 언어에 대한 언어별 미세 조정 모델보다 더 잘 수행됨을 보여준다. 이는 이러한 언어들이 베이스라인에 비해 멀티모달 데이터가 적기 때문이다(즉, 영어 모델은 665K 명령어들에 대해 트레이닝되는 반면, 언어-특정 모델들은 150K 명령어들에 대해 트레이닝됨), 그리고 잡음이 있는 반자동 번역 프로세스로 인해. 반대로, 언어별 미세 조정 모델은 아랍어, 힌디어, 벵골어 및 우르두의 경우 LLM 사전 훈련 데이터에서 이러한 언어가 과소 표현되기 때문에 더 잘 수행된다. 마지막으로, 결합 훈련은 자원이 부족한 언어에 대한 성능을 더욱 향상시킵니다. 또한, 번역된 멀티모달 학습 데이터의 양을 증가시키면 성능이 향상됨을 알 수 있었다. 예를 들어, GQA 데이터셋 Hudson and Manning(2019)의 72K 명령어를 추가로 Bengali로 번역하고 총 222K 명령어를 사용하여 Bengali 결과를 34.8에서 38.3으로 개선한다. 본 연구는 리소스 제약으로 인해 각 언어에 대해 150K 명령어로 제한된다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리는 세계 인구의 거의 3분의 2를 차지하는 5B 사람들을 위한 폴리글로트 LLM인 팔로를 소개한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c|c|c} \\hline\n' +
      '**Data** & **English** & **Chinese** & **French** & **Spanish** & **Russian** & **Japanese** & **Arabic** & **Hindi** & **Bengali** & **Urdu** & **Avg.** \\\\ \\hline\n' +
      '665K & **67.9** & **55.7** & **62.4** & **64.5** & 55.3 & *59.2** & 38.9 & 29.4 & 13.9 & 21.8 & 46.9\\\\\n' +
      '150K-Chinese & 59.3 & 55.0 & 60.0 & 57.0 & 32.9 & 40.5 & 21.2 & 20.3 & 21.7 & 19.3 & 38.7\\\\\n' +
      '150K-French & 51.0 & 41.0 & 57.8 & 54.4 & 35.4 & 54.6 & 17.6 & 23.2 & 13.1 & 16.7 & 36.5\\\\\n' +
      '150K-Spanish & 61.1 & 52.2 & 54.8 & 61.6 & 50.1 & 51.7 & 27.8 & 24.4 & 15.4 & 18.5 & 41.8\\\\\n' +
      '150K & 러시아 & 55.2 & 51.1 & 62.2 & 60.6 & **57.8** & 50.9 & 25.3 & 28.2 & 13.6 & 16.7 & 42.2\\\\\n' +
      '150K-Japanese & 54.5 & 41.1 & 59.2 & 57.6 & 36.1 & 57.6 & 18.0 & 23.6 & 13.3 & 18.4 & 37.9\\\\\n' +
      '150K-Arabic & 67.8 & 42.9 & 56.4 & 54.7 & 38.4 & 44.7 & 56.0 & 25.7 & 19.4 & 33.4 & 43.9\\\\\n' +
      '150K-Hindi & 52.2 & 39.1 & 56.8 & 54.0 & 35.0 & 33.4 & 18.4 & 54.1 & 12.8 & 23.8 & 37.9\\\\\n' +
      '150K-Bengali & 26.4 & 40.2 & 56.0 & 54.5 & 37.3 & 26.0 & 12.8 & 16.3 & 34.8 & 14.0 & 31.8\\\\\n' +
      '150K-Urdu & 28.9 & 30.6 & 44.6 & 50.1 & 22.5 & 16.0 & 22.1 & 25.5 & 20.9 & 47.7 & 30.9 \\\\ Combined & 64.2 & **55.7** & 58.3 & 61.0 & 57.4 & 57.5 & **57.8** & **57.6** & **51.7** & **55.3** & **57.7** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 다국어 미세 조정 데이터 세트에 대한 **절제. 표는 서로 다른 언어의 미세 조정 데이터를 사용할 때 10개 언어에 대한 성능의 영향을 보여준다. 이 절제에 7B 매개변수가 있는 모델이 사용됩니다.**tion. 이미지 및 사용자 텍스트 쿼리를 입력으로 간주하고 영어, 중국어, 프랑스어, 스페인어, 러시아어 및 일본어와 같은 고자원 언어와 아랍어, 힌디어, 벵골어 및 우르두어와 같은 저자원 언어 모두에서 효과적으로 대화한다. 10개 언어로 모델을 교육하기 위해 맞춤형 LLM을 사용하여 150K 지침을 각 언어로 번역합니다. 언어 번역 작업에서 LLM을 미세 조정하기 위해 각 대상 언어에 대해 1K 인간 주석 대화를 사용한다. 우리의 최종 모델은 10개 언어로 동시에 역량을 제공하고 비전 언어 평가에 대한 전반적인 성능 향상을 제공한다. 팔로를 3개의 척도(1.7B, 7B, 13B)에 걸쳐 교육하여 10개 언어에 걸친 일반화와 확장성을 입증합니다. 코드, 모델 및 데이터 세트가 공개됩니다.\n' +
      '\n' +
      '## 6 Limitations\n' +
      '\n' +
      '반자동 번역 과정은 효율적이지만 각 언어에 내재된 깊은 맥락과 문화적 뉘앙스를 완전히 파악하지 못할 수 있다. 이는 모델이 필요한 문화적 깊이, 정확성 및 정밀도로 콘텐츠를 이해하고 생성하는 능력에 영향을 미칠 수 있다. 또한 10개 언어에 걸쳐 있지만 선택할 수 있습니다.\n' +
      '\n' +
      '그림 4: Palo**의 다국어 능력을 보여주는 **정성적 결과. 사용자 질의와 함께 제시될 때, 모델은 시각적 콘텐츠 및 관련 언어와 관련된 정확한 텍스트 응답을 생성한다. 이 그림은 다양한 언어에 걸쳐 시각과 언어 이해를 연결하는 능력을 강조한다. 이 그림에서 우리는 2개의 높은 자원 언어(스페인어와 중국어)와 2개의 낮은 자원 언어(힌디어와 아랍어)의 대화 상자를 탐색한다. 팔로는 현대 슈퍼마켓 환경에서 중세 복장을 한 두 사람이 등장하는 이미지의 특이한 측면을 정확하게 해석한다. 이 모델은 _Chinese_에서 창의적인 상상력을 보여주며, 이 캐릭터들이 동화책에서 왕과 여왕이 될 수 있는 배경 이야기를 제안합니다. _Hindi_에서 팔로는 중세 커플을 시간 여행자로 현재에 끌어들인 가능한 상황을 설명함으로써 시나리오 구축을 보여준다. 바닥에서 팔로는 아랍어에서 유머의 감각을 보여주며, 왕이 말할 수 있는 유희적인 대화를 떠올리며 맥락과 문화 특유의 유머에 대한 미묘한 이해를 보여준다. 이 이미지는 높은 언어 정밀도와 문화적 지능을 반영하여 다양한 언어로 콘텐츠를 처리하고 생성하는 고급 능력을 효과적으로 시각화한다.\n' +
      '\n' +
      '세계 인구의 3분의 2는 여전히 상당한 수의 세계 언어를 배제하고 있으며, 이는 VLM 내에서 언어적 다양성과 포괄성을 향상시키기 위해 추가 확장의 여지가 있음을 나타낸다.\n' +
      '\n' +
      '##6 잠재적인 위험\n' +
      '\n' +
      '반자동 번역의 사용은 특히 자원이 부족한 언어의 경우 LLM에 내재된 편향과 관련된 잠재적인 위험을 초래할 수 있다.\n' +
      '\n' +
      '그림 5: 팔로의 시각적 추론과 다국어의 능숙함을 보여주는 **정성적 결과. 팔로는 각 언어에 대해 문맥적으로 적절한 방식으로 시각적 콘텐츠에 정확하게 반응한다. 우리는 프랑스어, 러시아어 및 일본어와 _1 저자원 언어인 Urdu_의 3가지 고자원 언어로 대화를 설명한다. _French_ 세그먼트에서 모델은 냉장고에서 사용 가능한 재료를 사용하는 레시피를 제안하여 시각적 인식을 요리 제안에 연결함으로써 실용적인 추론을 보여준다. _Russian_에서 Palo는 비타민 C가 풍부한 품목을 식별하고 _Urdu_ 예제에서 모델은 냉장고 내용물을 식품군으로 구성하여 품목을 분류하고 영양 지식을 적용하는 능력을 보여준다. 이것은 대화의 맥락을 유지하면서 언어 사이를 전환하는 능력을 효과적으로 강조하며, 고자원 및 저자원 언어 모두에서 관련되고 문화적으로 인식되는 콘텐츠를 생성할 수 있는 능력을 반영한다.**\n' +
      '\n' +
      '모델은 문화적 상징이나 제스처의 해석과 같은 시각적 데이터의 뉘앙스를 설명해야 잘못된 표현을 방지할 수 있다. 이러한 편향의 영향을 받는 모델의 해석은 문화적으로 민감한 맥락에서 부정확성을 초래할 수 있다. 그러한 위험을 완화하기 위해 필요한 훈련을 평가하고 채택할 필요가 있다.\n' +
      '\n' +
      '##8 데이터 및 AI 어시스턴트의 활용\n' +
      '\n' +
      'LLaVA-Instruct Liu et al. (2023) dataset, licensed under Creative Commons Attribution (CCA) 4.0 International, available for research. 또한, GPT 모델의 사용은 (OpenAI)에 따른다. 소스 라이선스 정보를 존중하여 CCA 4.0 국제 라이선스에 따라 이 작업에서 생성된 모든 데이터 세트를 공개합니다.\n' +
      '\n' +
      '##9 인간 주석\n' +
      '\n' +
      '각 언어에 대한 LLaVA-Bench Liu et al.(2023) 평가는 성별과 인구 통계의 다양한 혼합을 나타내기 위해 선택된 주석자에 의해 검증되고 수정된다. 주석에는 번역된 버전과 함께 영어 버전이 제공됩니다. 그들은 교정 과정에서 톤과 편향을 중화시키기 위한 구체적인 지시를 받는다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 기술보고서; _ arXiv preprint arXiv:2303.08774_.\n' +
      '* Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidi, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. Falcon series of open language models. _ arXiv preprint arXiv:2311.16867_.\n' +
      '* Bai et al. (2023) Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: 다재다능한 능력을 가진 프론티어 대형 비전 언어 모델 _ arXiv preprint arXiv:2308.12966_.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 소수의 학습자를 의미한다. _ 신경 정보 처리 시스템들_, 33:1877-1901의 진보들.\n' +
      '* Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. _ Journal of Machine Learning Research_, 24(240):1-113.\n' +
      '* Chu et al. (2023) Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. 2023. Mobilevlm: A fast, reproducable and strong vision language assistant for mobile devices. _ arXiv preprint arXiv:2312.16886_.\n' +
      '* 컴퓨터(2023) Together Computer. 2023. Redpajama: 라마 트레이닝 데이터세트를 재현하기 위한 오픈 소스 레시피.\n' +
      '* Costa-jussa et al. (2022) Marta R Costa-jussa, James Cross, Onur Celebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left: Scaling human-centered machine translation. _ arXiv preprint arXiv:2207.04672_.\n' +
      '* Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. 인스트럭션 블립: 인스트럭션 튜닝이 있는 범용 비전-언어 모델에 대한_ arXiv:2305.06500_.\n' +
      '* Eberhard et al. (2015) David M Eberhard, Gary Francis Simons, and Charles D Fenning. 2015. Ethnologue: Languages of world.\n' +
      '* Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: 대형 언어 모델의 저순위 적응. _International Conference on Learning Representations_.\n' +
      '* Hudson and Manning (2019) Drew A Hudson and Christopher D Manning. 2019. Gqa: real-world visual reasoning and compositional question answering을 위한 새로운 데이터셋. IEEE/CVF Conference on computer vision and pattern recognition_의 _Proceedings, pages 6700-6709.\n' +
      '* 허깅페이스(2019) 허깅페이스. 허깅페이스 데이터세트. [https://huggingface.co/datasets/Aeala/Share6PT_Vicuna_unfiltered] (https://huggingface.co/datasets/Aeala/Share6PT_Vicuna_unfiltered).\n' +
      '* Laurencon et al. (2022) Hugo Laurencon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonzalez Ponferrada, Huu Nguyen et al. 2022. The Bigscience root corpus: A 1.6 to composite multilingual dataset. _ 신경 정보 처리 시스템_, 35:31809-31826에서의 발전.\n' +
      '* Le Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. corr, abs/2211.05100, 2022. doi: 10.48550. _arXiv preprint arXiv:2211.05100_.\n' +
      '* Lin et al. (2023) Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. 2023. Video-llava: Learning united visual representation by alignment before projection. _ arXiv preprint arXiv:2311.10122_.\n' +
      '\n' +
      '* Liu et al. (2023) Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. 시각적 지시 조정을 통해 개선된 기준선입니다. _ arXiv:2310.03744_.\n' +
      '* Liu et al. (2023b) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. 시각적 지시 조율 _NeurIPS_에서.\n' +
      '* Lu et al. (2023) Junyu Lu, Dixiang Zhang, Xiaojun Wu, Xinyu Gao, Ruyi Gan, Jiaxing Zhang, Yan Song, Pingjian Zhang. 2023. Ziya-visual: 다중 작업 명령어 튜닝을 통한 이중언어 대형 비전 언어 모델 _ arXiv e-prints_, pages arXiv-2310.\n' +
      '* Maaz et al. (2023) Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. 비디오-채팅: 대형 비전 및 언어 모델을 통한 상세한 비디오 이해에 대하여 _ arXiv:2306.05424_.\n' +
      '* OpenAI(2021) OpenAI. 개방형 사용 용어. [https://openai.com/정책/사용 기간] (https://openai.com/정책/이용기간).\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision.\n' +
      '* Rasheed et al. (2023) Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, 그리고 Fahad S. 칸 2023. 글램: 픽셀 접지 대형 멀티모달 모델 _ ArXiv 2311.03356_.\n' +
      '* Scao et al. (2022) Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Mueminighoff, Jason Phang et al. 2022. 100만 gpu hours를 가지고 있다면 어떤 언어 모델을 훈련시킬 것인가? _ arXiv preprint arXiv:2210.15424_.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_.\n' +
      '* Wei et al. (2023) Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, et al. 2023. PolyIm: An open source polyplot large language model _ arXiv preprint arXiv:2307.06018_.\n' +
      '* Ye et al. (2023) Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality. _ arXiv preprint arXiv:2304.14178_.\n' +
      '* Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Zhuang, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. llm-as-a-judgeing with mt-bench and chatbot arena. _ arXiv:2306.05685_.\n' +
      '* Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: 고급 대형 언어 모델로 비젼-언어 이해력 향상. _ arXiv:2304.10592_.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>