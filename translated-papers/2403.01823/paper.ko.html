<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# RT-H : 언어를 이용한 액션 계층\n' +
      '\n' +
      '수넬 벨케일, 톈리 딩, 테드 샤오, 피에르 서머넷, 꽝 부엉, 조나단 톰슨\n' +
      '\n' +
      '예브겐 체보타르\\({}^{*}\\), 데비다타 드위베디\\({}^{*}\\), 도르사 새디\\({}^{*}\\)\n' +
      '\n' +
      'Google DeepMind\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '언어는 복잡한 개념을 소화 가능한 조각으로 분해하는 방법을 제공한다. 로봇 모방학습의 최근 연구는 시각적 관찰과 언어로 명시된 높은 수준의 과제가 주어졌을 때 행동을 예측하는 언어 조건화 정책을 학습하는 것을 제안하고 있다. 이러한 방법은 자연어의 구조를 활용하여 다중 작업 데이터 세트에서 의미적으로 유사한 작업(예: "코카인 캔 따기" 및 "사과 따기") 간에 데이터를 공유한다. 그러나, 태스크들이 더 의미적으로 다양해짐에 따라(예를 들어, "코크 캔 선택" 및 "컵 부음") 태스크들 간에 데이터를 공유하는 것이 더 어려워지고 따라서 고-레벨 태스크들을 액션들에 매핑하는 것을 학습하는 것은 실질적으로 더 많은 실증 데이터를 필요로 한다. 이 작업을 작업과 작업으로 나누기 위해 우리의 통찰력은 "팔을 앞으로 이동" 또는 "가까운 그리퍼"와 같은 더 세밀한 문구로 낮은 수준의 동작을 설명하는 로봇에게 동작의 언어를 가르치는 것이다. 이러한_언어 모션_을 상위 레벨 태스크와 액션 사이의 중간 단계로 예측하는 것은 정책이 겉보기에 이질적인 태스크에 걸쳐 하위 레벨 모션의 공유 구조를 학습하도록 강제한다. 또한, 언어 모션에 대해 조건화된 정책은 사람이 지정한 언어 모션을 통해 실행 중에 쉽게 _corrected_될 수 있다. 이는 언어에 대한 인간의 개입으로부터 배울 수 있는 유연한 정책에 대한 새로운 패러다임을 가능하게 한다. 본 논문에서 제안하는 RT-H는 언어 동작을 이용하여 _action hierarchy_를 구축한다. 먼저 언어 동작을 예측하는 방법을 학습하고, 이를 상위 태스크와 함께 조건화하여 모든 단계에서 시각적 컨텍스트를 이용하여 동작을 예측한다. 실험적으로 우리는 RT-H가 이 언어 행동 계층을 활용하여 다중 작업 데이터 세트를 효과적으로 활용함으로써 더 강력하고 유연한 정책을 학습한다는 것을 보여준다. 우리는 이러한 정책이 언어 개입에 대응할 수 있을 뿐만 아니라 그러한 개입에서 배울 수 있고 원격 조작 개입에서 배우는 방법을 능가할 수 있음을 보여준다. 우리의 웹사이트와 비디오는 rt-hierarchy.github.io에서 찾을 수 있습니다.\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      '언어는 인간 추론의 엔진이며, 우리에게 복잡한 개념을 더 단순한 것으로 분해하고, 우리의 오해를 바로잡고, 새로운 환경에서 개념을 일반화할 수 있는 권한을 부여한다. 최근 몇 년 동안 로봇도 고급 개념을 분해하거나 [1] 언어 수정을 제공하거나 [2, 3] 새로운 설정으로 일반화를 가능하게 하기 위해 언어의 효율적이고 구성적인 구조를 활용하기 시작했다. 이러한 작업은 종종 공통 패러다임을 공유한다: "pick coke can"와 같은 언어로 기술된 상위 레벨 _task_가 주어지면, 그들은 대규모 다중 작업 데이터 세트에 걸쳐 언어에서의 관찰 및 작업 설명을 하위 레벨 로봇 _actions_에 매핑하는 정책을 학습한다. 이러한 설정에서 언어의 장점은 유사한 작업들(예를 들어, "코카인 캔 따기" 대 "사과 따기") 사이의 공유 구조를 인코딩하는 것이다. 작업에서 작업으로 매핑을 배우는 데 필요한 데이터를 줄입니다. 그러나, 태스크들이 더 다양해짐에 따라, 각각의 태스크를 설명하는 언어(예를 들어, "코카인 캔을 고른다" 대 "컵을 따라라")도 마찬가지이다. 높은 수준의 언어만으로는 서로 다른 과제들 간의 공유된 구조를 배우기 어렵게 만든다.\n' +
      '\n' +
      '다양한 작업을 배우기 위해 우리의 목표는 이러한 작업 간의 유사성을 더 잘 포착하는 것입니다. 우리는 언어가 단지 높은 수준의 작업보다 훨씬 더 많은 것을 표현할 수 있다는 것을 관찰한다: 우리는 또한 작업을 수행하기 위해 _how_를 표현할 수 있다 - 낮은 수준의 작업에 더 가까운 더 세밀한 표현이다. 예를 들어, 우리는 "pick coke can" 태스크를 일련의 세밀한 동작들의 시퀀스로 분해할 수 있는데, 이것은 _language motions_: "팔을 앞으로 이동", "캔을 잡고", 그리고 나서 "팔을 위로 이동"으로 나타낸다. 우리의 핵심 통찰력은 언어 모션을 높은 수준의 작업 설명과 낮은 수준의 작업 사이의 중간 예측 계층으로 활용하여 언어 모션을 통해 _action 계층 구조를 구축하는 것이다. 이러한 액션 계층을 생성하면 몇 가지 이점이 있다. (1) 언어 동작 수준에서 서로 다른 작업 간에 훨씬 더 나은 데이터 공유를 가능하게 하여 다양한 다중 작업 데이터 세트에서 더 나은 언어 동작 구성 및 일반화를 유도한다. 예를 들어, "컵을 따라라"와 "코카인 캔을 집어라"는 의미적으로 다르더라도, 오브젝트가 선택될 때까지 언어 동작 레벨에서 완전히 중첩된다. (2) 언어동작은 단순히 고정된 프리미티브가 아니라, 명령어 및 시각적 관찰을 이용하여 현재 태스크 및 장면의 _context_에서 학습된다. 예를 들어, "팔을 앞으로 이동"만으로는 얼마나 빨리 움직이거나 정확한 방향 벡터를 전달하지 못하며, 이는 작업과 관찰에 따라 달라진다. 학습된 언어 동작의 문맥성과 유연성은 새로운 능력 세트를 도입한다: 그것은 정책이 100% 성공적이지 않을 때 인간이 언어 동작에 그들 자신의 _corrections_를 제공할 수 있게 한다(도 1의 센터 오렌지 박스 참조). 또한, 로봇은 전적으로 언어 동작의 영역에서 이러한 인간 교정으로부터 학습할 수도 있다. 예를 들어, "픽 코크 캔"으로 로봇이 그리퍼를 일찍 닫으면 대신 "팔을 앞으로 이동"하라고 말할 수 있으며, RT-H는 현재 장면의 맥락에서 해석한다. 이러한 언어 동작의 약간의 변화는 인간이 제공하기 쉬울 뿐만 아니라, 개별 로봇 동작을 교정하는 것에 비해 훨씬 더 쉽게 배울 수 있다.\n' +
      '\n' +
      '언어 동작의 이점에 착안하여, 본 논문에서는 이러한 동작 계층을 학습하기 위한 종단간 프레임워크인 RT-H(Robot Transformer with Action Hierarchies)를 제안한다. 각 단계에서 RT-H 상태 관측과 높은 수준의 작업 설명으로 현재 언어 동작(언어 동작 쿼리)을 예측하여 모델이 세밀한 수준에서 작업을 수행하는 방법에 대해 추론할 수 있도록 한다. 그런 다음 RT-H는 관찰, 작업 및 추론된 언어 동작을 사용하여 해당 단계(동작 쿼리)에 대한 동작을 예측하며, 여기서 언어 동작은 정확한 동작의 예측을 개선하기 위해 추가 컨텍스트를 제공한다(도 1의 보라색 상자 참조). 데이터에서 언어 모션을 추출하기 위해 로봇 고유 감각에서 단순화된 언어 모션을 추출하는 자동화된 접근법을 개발하여 수동 주석 노력 없이 2500개 이상의 언어 모션을 풍부한 라이브러리로 생성한다. 우리는 정책 학습을 개선하기 위해 인터넷 규모의 비전과 언어 데이터를 공동 훈련하는 대규모 비전 언어 모델인 RT-2를 기반으로 모델 아키텍처를 구축한다[4]. RT-H는 언어 모션 및 액션 쿼리에 대해 단일 모델을 사용하여 액션 계층의 모든 수준에 대해 이 광범위한 인터넷 규모 지식을 활용합니다.\n' +
      '\n' +
      '실험적으로, 언어 동작 계층을 사용하면 다양한 다중 작업 데이터 세트를 섭취했을 때 상당한 개선을 얻을 수 있으며, 광범위한 작업에서 RT-2를 15% 능가한다. 또한 동일한 작업에서 언어 동작을 교정하는 것이 거의 완벽한 성공률에 도달한다는 것을 발견하여 학습된 언어 동작의 유연성과 맥락성을 보여준다. 또한, 언어 동작 개입으로 모델을 미세 조정하면 IWR [5]와 같은 최신 대화형 모방 학습 방법을 50% 능가한다. 마지막으로, RT-H에서의 언어동작이 RT-2보다 장면 및 객체의 변화에 더 잘 일반화됨을 보인다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '이 절에서는 정책 학습에서 언어의 역할, 모방 학습에서 위계가 어떻게 사용되었는지, 로봇 정책에 대한 인간 수정 제공 및 학습을 위한 이전 접근법에 대해 논의한다.\n' +
      '\n' +
      '**언어 조건 정책** 최근 몇 년 동안 언어는 로봇 작업에 대한 강력한 목표 표현으로 부상했다. 모방 학습(IL)에서, 많은 접근법들은 언어로 기술된 태스크들을 미리 트레이닝된 언어 모델들을 사용하여 임베딩들로 인코딩하고, 그 다음, 멀티-태스크 로봇 데이터세트들[6, 7, 8, 9, 10, 11]에 트레이닝되는 정책에 입력된다. 이러한 사전 훈련된 언어 임베딩은 시각적 이해가 부족하므로 다른 작업은 종종 대규모 인터넷 규모 데이터 세트를 사용하고 때로는 로봇 데이터를 포함하는 바닥에서 시각적 및 언어 표현을 공동으로 훈련한다[12, 13, 14, 15]. 그 후, 결과적인 목표 표현들은 시각적 및 의미적 컨텍스트 둘 다를 제공하기 위해 정책에 입력될 수 있다. 보다 최근에, 비전 언어 모델(VLM) 백본들에 기초하여 구축된 정책들은 사전 훈련된 임베딩들(4, 16)의 필요 없이 시각적 관찰들 및 언어로부터 직접 액션들을 학습할 수 있게 되었다. 이러한 모든 접근법은 언어를 활용하여 높은 수준의 작업을 나타내고 종종 낮은 수준의 작업을 직접 예측하지만 두 언어 작업 설명이 의미적으로 다양해짐에 따라 서로 다른 작업 간의 데이터 공유가 어려워지므로 훨씬 더 많은 데이터가 필요하다.\n' +
      '\n' +
      '**Hierarchical Action Representation in Imitation Learning.** 성능을 높이기 위한 대안적 접근은 _hierarchical Action Representation_의 사용을 통해 다중 작업 학습 문제에 구조를 부과하는 것이다. 여러 연구에서 다양한 다중 작업 데이터 세트[19, 10, 20, 21, 22, 23, 24, 25, 26, 27, 28]에서 동작 또는 객체와의 상호작용의 짧은 시퀀스를 설명하기 위해 매개변수화된 프리미티브[17, 18] 또는 임베딩으로서 일반적인 "기술" 표현을 학습하는 것을 탐구했다. 그들은 일반적으로 성능을 향상시키지만, 종종 상당히 계산적으로 복잡하고 하이퍼파라미터에 민감하다. 또 다른 작업 라인은 IL에서 거친 액션 추상화와 미세한 액션 추상화를 분리하지만 거친 액션 주석과 미세한 액션 주석을 모두 필요로 하는 이점을 보여준다[29, 30].\n' +
      '\n' +
      '언어는 또한 다중 작업 학습에서 계층 구조를 만드는 데 사용되었다. 롱라이즌 명령어들을 다룰 때, 많은 최근의 접근법들은 롱라이즌 명령어들을 언어로 지정된 태스크들의 시퀀스로 분해하기 위해 LLM들 또는 VLM들을 사용한다[16, 31, 32, 33, 34, 1]. 일반적으로 스크립팅되거나 개별적으로 훈련된 정책이 이러한 작업을 실행하는 데 사용되어 확장성을 제한한다. 긴 수평선 정책을 종단 간 학습하기 위해 Hu와 Clune은 모델을 훈련하여 먼저 언어 작업을 예측한 다음 해당 작업을 조건으로 하는 작업을 예측합니다[35]. 이 접근법은 RT-H와 유사하지만 액션 계층에서 한 단계 더 높게 존재하는데, 이는 세밀한 언어 모션에 레이블을 지정하거나 학습하지 않는다. 몇몇 작품들은 더 세밀한 언어의 사용을 탐구한다. Sharma 등은 LLM을 사용하여 태스크들을 모션 프리미티브들의 시퀀스로 분해한다[36]. 그러나 이러한 모션 프리미티브는 하드 코딩되고 더 복잡한 설정에서 요구되는 맥락성이 부족하다. RT-H는 작업과 장면 모두의 맥락에서 언어 동작을 학습하여 더 나은 정책과 더 많은 맥락적 수정을 가능하게 한다.\n' +
      '\n' +
      '**대화형 모방 학습 및 교정.**대화형 IL 방법은 로봇 실행 중 인간의 피드백으로부터 학습한다[37]. Ross 등은 DAgger를 제안했는데, DAgger는 온라인 롤아웃에 대해 전문가 주석이 달린 액션들을 반복적으로 집계한다[38]. 효과적이지만 이러한 전문가 주석을 제공하는 것은 비용이 많이 들기 때문에 이후 작업은 더 선택적인 개입을 사용했는데, 예를 들어 인간이 개입 시기를 결정하게 하거나[39, 5] 정책이 [40, 41, 42, 43]을 결정하도록 한다. 이러한 방법들은 모두 로봇의 동작 공간, 즉 로봇 원격 조작[5] 또는 운동감각 교육[44, 45]에 대한 개입을 필요로 하며, 이는 비전문가에게 어렵고 확장하기가 어려울 수 있다.\n' +
      '\n' +
      '중재를 보다 직관적이고 확장 가능하게 하기 위해, 몇몇 작품들은 중재 매체로서 언어, 예를 들어 인간의 지침으로 잘못된 작업 예측에 개입하는 것을 연구해왔다[32, 46]. 좀 더 세밀한 수준에서 언어를 수정하는 것은 어렵다. 몇몇 작품들은 세립형 언어 교정들의 고정된 세트를 정의한 다음, 자연 언어 발화를 이러한 교정 프리미티브들에 매핑한다[47]. 이후 작업은 이러한 부서지기 쉬운 프리미티브를 제거하고 구성 가능한 비용 함수로 대체하지만 특권적인 환경 지식을 가정한다[3]. 데이터 주도 접근법도 탐색되었으며, 언어 수정(48, 49, 50, 51)의 대규모 데이터 세트 또는 배포 중 공유 자율성을 필요로 한다[2]. RT-H는 대신 액션으로_predict_ 언어 모션을 끝에서 끝까지 학습하여 언어 모션의 공간에서의 수정뿐만 아니라 이러한 수정으로부터 효율적인 학습을 가능하게 한다.\n' +
      '\n' +
      '## III RT-H : 언어를 이용한 액션 계층\n' +
      '\n' +
      '높은 수준의 작업 설명으로 표현되지 않는 다중 작업 데이터 세트에 걸쳐 공유 구조를 효과적으로 캡처하기 위해 우리의 목표는 _action 계층_을 명시적으로 활용하는 정책을 학습하는 것이다. 구체적으로, 우리는 중간_언어 움직임_ 예측 계층을 정책 학습에 도입한다. 로봇의 세밀한 동작을 설명하는 언어 모션은 다중 작업 데이터 세트에서 유용한 정보를 캡처할 수 있으며 수행 정책으로 이어질 수 있다.\n' +
      '\n' +
      '학습된 정책들이 수행하기 위해 고군분투할 때, 언어 동작은 다시 구조될 수 있다: 그것들은 주어진 장면에 맥락적인 온라인 인간 교정을 위한 직관적인 인터페이스를 가능하게 한다. 언어 동작으로 훈련된 정책은 자연적으로 낮은 수준의 인간 교정을 따르고 교정 데이터가 주어진 작업을 성공적으로 달성할 수 있다. 추가적으로, 정책은 심지어 언어 교정 데이터에 대해 트레이닝될 수 있고 그 성능을 더욱 향상시킬 수 있다.\n' +
      '\n' +
      '**RT-H 모델 개요.**RT-H, 그림. 도 2를 참조하면, 두 가지 주요 단계를 갖는다 : 먼저 작업 설명 및 시각적 관찰로부터 언어 동작을 예측한다(_언어 동작 쿼리_, 도 2의 좌측 상단). 그런 다음 예측된 언어 동작, 작업 및 관찰에 대한 조건을 설정하여 정확한 동작을 추론한다(_action query_, 도 2의 왼쪽 하단). 우리는 VLM 백본을 사용하여 RT-H를 인스턴스화하고 RT-2 [4]의 훈련 절차를 따른다. RT-2와 유사하게, 우리는 공동 훈련을 통해 자연 언어에 대한 엄청난 사전 지식과 인터넷 규모 데이터에 대한 이미지 처리를 활용합니다. 이 사전 지식을 액션 계층의 모든 레벨에 통합하기 위해, 단일 모델은 언어 모션 및 액션 쿼리 모두를 학습한다.\n' +
      '\n' +
      '### _ 정형화 행동 계층_\n' +
      '\n' +
      '자연어로 된 과제 설명(\\(g\\in\\mathcal{G}\\))과 쌍을 이루는 \\(\\tau\\) 전문가 시연(\\(\\tau\\)의 데이터세트 \\(\\tau\\D}=\\{(\\tau_{1},g_{1}),\\ldots,(\\tau_{N},g_{N})\\}\\)이 주어지며, 여기서 각 \\(g\\)은 \\(m\\)의 높은 수준의 과제 집합 \\(\\{T_{i}\\}_{i=1}^{m}\\)에서 정확히 하나의 과제를 설명한다. 각 시연 \\(\\tau_{i}\\)은 일련의 관찰과 길이 \\(L_{i}\\)의 _action 계층으로 구성된다. 자연어로 표현된 중간 행동 표현(z\\in\\mathcal{Z}\\)과 낮은 수준의 행동 표현(a\\in\\mathcal{A}\\)으로 구성된 행동 계층을 정의한다. 여기서 중간 액션은 상위 레벨 작업보다 세밀하지만 하위 레벨 작업보다 거친 액션이다. 따라서 우리는 \\(\\tau_{i}=\\{(o_{1},z,a_{1}),\\ldots,(o_{L_{i}},z_{L_{i}},a_{L_{i}})\\}\\), 관찰 \\(o\\in\\mathcal{O}\\)을 쓴다. 우리의 목표는 관찰과 과제 설명을 중간 행동으로 매핑하는 상위 정책\\(\\pi_{h}:\\mathcal{G}\\times\\mathcal{G}\\times\\mathcal{G}\\times\\mathcal{G}\\times\\mathcal{G}\\times\\mathcal{G}\\times\\mathcal{A}\\)과 관찰과 과제 설명, 중간 행동을 하위 행동으로 매핑하는 하위 정책\\(\\pi_{h}:\\mathcal{G}\\times\\mathcal{G}\\times\\mathcal{A}\\)의 순서를 학습하는 것이다. 그런 다음 이 두 정책의 구성으로서 행동 계층 정책을 정의합니다. \\(\\pi(a,z|o,g)=\\pi_{h}(z|o,g)\\pi_{l}(a|o,g,z)\\.\n' +
      '\n' +
      '본 연구에서는 "팔을 앞으로 움직인다" 또는 "팔을 오른쪽으로 돌린다"와 같은 언어 동작을 사용하여 중간 동작 표현 \\(z\\)을 모델링한다. 작용 계층은 단지 하나의 수준(즉, \\(z^{1}\\ldots z^{K}\\) 이상으로 쉽게 확장될 수 있다는 점에 유의한다.\n' +
      '\n' +
      '### _RT-H : 모델 및 훈련 세부사항_\n' +
      '\n' +
      '이 액션 계층을 모델링하고 언어 동작의 이점을 얻기 위해 그림 1에 표시된 방법 RT-H를 사용한다. 2는 인터넷 스케일 데이터와 함께 학습된 단일 VLM으로 \\(\\pi_{h}\\)과 \\(\\pi_{l}\\)을 학습한다. RT-2[4] - RT-H와 동일한 PaLI-X 55B[52] 구조로 이 VLM을 인스턴스화하고 ViT 인코더 모델을 사용하여 이미지를 토큰으로 처리한 다음 인코더-디코더 트랜스포머를 사용하여 이미지와 자연어 토큰의 스트림을 액션 토큰으로 변환한다. 이러한 액션 토큰은 각 액션 차원을 256개의 빈으로 이산화하고 이러한 빈을 정수 값으로 인코딩함으로써 RT-2와 동일한 방식으로 생성된다. 각 액션 \\(a\\)은 엔드 이펙터의 델타 위치, 엔드 이펙터의 델타 축 각도 회전, 그리퍼를 닫거나 여는 액션, 종료 플래그로 구성된다. RT-H는 VLM에 대해 두 개의 쿼리를 구성한다. 먼저, **언어 동작 질의** 모델 \\(\\pi_{h}\\), 언어 \\(g\\) 및 이미지 관찰 \\(o\\)을 언어 동작 \\(z\\)에 매핑하는 작업 (엔코더는 \\(g\\) 및 \\(o\\)을 보고, 디코더는 \\(z\\)을 예측한다.) 이 첫 번째 단계는 RT-H가 낮은 수준의 로봇 동작보다 더 거칠고 압축된 동작 공간에서 올바른 행동(언어 동작)을 먼저 예측하도록 가르치며, 각 작업의 구조를 더 잘 모델링하고 따라서 다양한 작업에 걸쳐 하위 궤적의 더 나은 공유를 가능하게 한다. 둘째, **action query** 모델 \\(\\pi_{l}\\), 이미지 \\(o\\), 태스크 \\(g\\), 언어 모션 \\(z\\)을 액션 토큰에 매핑한 후 로봇 액션 \\(a\\)으로 디토큰화 한다 (Encoder see \\(g\\), \\(o\\), \\(z\\), Decoder predict \\(a\\)). 이 두 번째 단계는 RT-H가 언어 동작\\(z\\)을 실행할 정확한 동작으로 디코딩하는 방법(장면과 작업 설명 모두)에서 맥락적(상황적)이 되도록 가르친다. 이 추가 컨텍스트는 작업을 성공적으로 완료하는 데 종종 중요하며 섹션 IV-A에서 논의하는 바와 같이 수정에서 수행하고 학습하는 데에도 중요하다. 하나의 질의에서 언어 동작 및 동작을 자동으로 훈련시키는 것과 비교하여, 두 개의 질의를 사용하는 것은 (1) 각 질의에 대해 특화된 프롬프트를 가능하게 하고, (2) 언어 동작\\(z\\)은 동작을 예측할 때 디코더가 아닌 트랜스포머 인코더로 전달된다.\n' +
      '\n' +
      '그런 다음 RT-H는 미리 훈련된 체크포인트로부터 시작하여 RT-2에서 사용되는 동일한 PaLI-X[52] 훈련 혼합물을 사용하여 공동 훈련된다. 이 공동 교육을 위해 ViT 인코더가 동결되었습니다. RT-H는 RT-2의 액션 예측 쿼리를 동일한 샘플링 속도로 언어 모션 및 액션 쿼리로 대체한다. 단일 모델을 사용하는 것은 트레이닝 프로세스를 단순화하고, 언어 모션 및 액션 쿼리 둘 모두가 PaLI-X 트레이닝 혼합물에서 광범위한 사전 지식으로부터 이익을 얻을 수 있게 한다. 모델 및 교육 세부 정보는 부록A를 참조하십시오.\n' +
      '\n' +
      '###_Extracting Language Motions_\n' +
      '\n' +
      '원칙적으로 인간은 세립 언어 동작의 전체 스펙트럼에 라벨을 붙일 수 있지만, 우리는 인간이 오프라인에서 이러한 라벨을 제공하는 것이 데이터 세트에 걸쳐 언어 불일치와 라벨이 붙은 기술의 부정확성으로 이어진다는 것을 발견했다. 예를 들어, 인간은 종종 기술 간의 전환에 라벨을 잘못 붙이거나 카메라 각도로 인해 로봇의 움직임 방향을 잘못 판단합니다. 따라서 각 에피소드에서 각 시간 단계에서 신뢰할 수 있는 언어 동작\\(z\\)을 저렴하게 추출하기 위해 로봇 고유 감각 정보에 의존하는 자동화된 라벨링 기법을 개발한다. 먼저, 로봇 엔드 이펙터 포즈 변화의 각 차원을 공간 차원(예: 위치 변화 맵의 z축)에 연결한다. 9개의 액션 차원들(델타 위치를 위한 3 차원들, 델타 방향을 위한 3 차원들, 베이스 이동을 위한 2 차원들, 그리퍼를 위한 1 차원들) 모두에 대해 이를 수행하는 것은, 로봇의 현재 _dominant_공간 움직임들의 리스트, 예를 들어 "암을 위로 그리고 오른쪽으로 이동", "닫힌 그리퍼", "암을 시계 반대 방향으로 회전" 또는 "회전 베이스 왼쪽"을 결정한다. 그런 다음 선택한 "작은 액션" 임계값 아래에 있는 치수를 필터링한 다음 결과 액션을 액션 크기 순으로 구성할 수 있다. 예를 들어, 로봇이 주로 팔을 앞으로 움직이지만 그립퍼를 닫기 시작하는 경우 "팔을 앞으로 움직이며 그립퍼를 닫기 시작"을 추출합니다.\n' +
      '\n' +
      '도. 2: RT-H **개요. 왼쪽:** 우리의 방법은 언어를 활용하여 정책 학습을 위한 액션 계층을 만든다. 액션 예측 문제는 이미지 토큰과 작업 설명 토큰을 사용하여 "팔을 앞으로 움직"와 같은 세밀한 언어 동작을 예측하는 언어 동작 쿼리(\\(\\pi_{h}\\)), 작업과 장면의 컨텍스트를 사용하여 이 언어 동작을 유연하게 액션으로 디코딩하는 액션 쿼리(\\(\\pi_{l}\\))로 분리한다. 우리는 RT-2[4]를 기반으로 한 두 쿼리에 대해 단일 VLM을 활용하여 액션 계층의 각 수준에서 인터넷 규모 데이터에 광범위한 사전 지식을 캡슐화한다. **Right:** 사용자는 액션 쿼리에 직접 개입하여 로봇 행동에 대한 언어 모션 수정을 제공할 수 있으며, 예를 들어 여기(상단)에서 "팔을 앞으로 이동" 대신 "팔을 왼쪽으로 이동"할 수 있다. 보정을 통해 학습하기 위해, 우리는 언어 모션 쿼리를 새로 라벨링된 언어 모션 수정(하단)으로 업데이트할 수 있다. 그런 다음 업데이트된 모델을 작업 계층(주황색 블록)에 다시 배치합니다.\n' +
      '\n' +
      '이러한 방식으로, 언어의 조합적 성질은 2500개 이상의 언어 모션들이 알려진 모션들의 간단한 세트로부터 추출될 수 있게 한다. 또한, 이러한 언어 모션은 액션에서 직접 파생되기 때문에 RT-H에서 액션 쿼리를 실행할 때 액션 자체에 대한 높은 예측력을 유지한다. 중요한 것은 작업과 관계없이 모든 실험 및 데이터 세트에 대해 이 절차의 세부 사항을 수정하므로 이 절차를 설계하는 것은 개발자에게 일회성 고정 비용이다.\n' +
      '\n' +
      '물론 이 절차는 언어 동작을 정의하고 추출하는 하나의 간단한 방법을 나타내며 다른 많은 방법이 존재할 수 있다. 예를 들어, 더 높은 레벨의 객체-참조 언어 모션 공간, 예를 들어 "객체에 도달" 또는 "객체 핸들을 파지"를 개발하는 것을 상상할 수 있지만, 이는 인간 주석 또는 강건한 객체 검출 및 추적을 필요로 할 가능성이 있다. 예를 들어 "팔을 천천히 앞으로 이동"과 같은 움직임 속도를 설명하는 것과 같이 훨씬 더 세밀한 수준에서 레이블을 지정할 수 있다. 그러나 이러한 예는 언어 모션에 대해 선택된 추상화 수준에 존재하는 근본적인 절충점을 강조한다: 세밀한 수준일수록 언어 모션 쿼리에 대해 예측하기 어려울 것이지만 액션 쿼리에 대해 더 많은 지침을 제공하고 그 반대의 경우도 마찬가지이다. 섹션 V에서 볼 수 있듯이 언어 모션의 선택은 언어 모션과 액션 쿼리 정확도 모두에서 좋은 균형을 이루는 동시에 레이블이 저렴합니다.\n' +
      '\n' +
      '##IV RT-H : 추론 및 보정\n' +
      '\n' +
      '테스트 시간에서 RT-H는 먼저 언어 모션 쿼리를 실행하여 스킬을 추론한 다음, 액션 쿼리에서 이 추론된 스킬을 사용하여 액션을 계산한다. 그러나, 이 과정은 두 질의가 각 시간 단계에서 순차적으로 실행되어야 하기 때문에 추론 시간을 두 배로 증가시킨다. RT-H에서 사용되는 55B 모델과 같은 더 큰 모델 크기로 더 작은 모델에는 문제가 없지만 불가피한 쿼리 지연을 경험할 것이다. 이 문제를 해결하기 위해 언어 동작 추론의 두 가지 모드에 대해 논의한다. (1) ** 비동기 쿼리**: RT-H에서 언어 동작 쿼리만 훈련하여 미래의 기술을 한 단계 예측한다. 그런 다음 테스트 시간에 _previous_시간 단계의 추론된 언어 동작을 사용하여 액션을 쿼리하는 동시에 다음 시간 단계에 대한 언어 동작을 예측한다. 이를 통해 RT-2와 거의 동일한 질의지연을 얻을 수 있다. (2) **고정빈도**: \\(H\\)단계마다 한 번씩 스킬 질의를 평가할 수 있고, 또한 분할시차를 줄일 수 있다. 실험에서 스킬은 종종 고정된 주파수와 정렬되지 않을 수 있는 정확한 시간 단계에서 변경해야 하기 때문에 비동기 쿼리를 선택한다.\n' +
      '\n' +
      '###_언어 모션을 통한 수정_\n' +
      '\n' +
      'RT-H 정책이 새로운 조작 설정을 접하거나 주어진 작업에서 실패하는 경우에도 RT-H의 액션 계층은 정책을 _correct_ 하는 것을 가능하게 한다: 사용자는 학습된 언어 모션에 직접 개입할 수 있다. 정책은 높은 수준의 작업을 수행하는 데 어려움을 겪을 수 있지만 낮은 수준의 언어 움직임을 따르는 것이 훨씬 쉽다.\n' +
      '\n' +
      '모델에 대한 간섭은 RT-H에서 간단하다(그림 2의 오른쪽 상단 참조). 텍스트 기반이기 때문에 키보드나 마이크만 있으면 됩니다. IWR(Intervention Weighted Regression) [5]와 같은 이전의 대화형 모방 학습 접근법들과 유사하게, 우리는 예를 들어 키보드 상의 키를 누름으로써 인간 연산자가 모델에 개입할 시기를 결정하도록 한다. 일단 그들이 보정 모드에 진입하면, 그들은 키보드에 새로운 언어 모션 보정을 타이핑하거나 공통 언어 모션에 대해 허드키를 사용할 수 있다. 이 새로운 언어 모션은 RT-H의 액션 쿼리에 직접 전달될 것이다(도 2 참조). 사용자의 의도와 일치하는 컨텍스트 액션을 생성합니다.\n' +
      '\n' +
      '우리는 또한 투명성을 위해 현재 예측된 언어 동작을 표시하고 사용자에게 추가 컨텍스트를 제공할 수 있으므로 로봇이 무엇을 계획하고 있는지 알고 보정을 더 잘 선택할 수 있다. 그런 다음 고정된 주파수에서 사용자는 새로운 언어 동작 보정을 입력하거나 이전에 입력된 언어 동작 보정을 계속 실행하거나 보정 모드를 종료해야 한다. 고정 빈도를 요청하면 사용자가 수정을 업데이트하거나 모델이 다시 한 번 인수하도록 결정할 수 있는 시간이 주어집니다.\n' +
      '\n' +
      '**수정 데이터에서 학습.** 이러한 언어 동작 수정에서 _learn_를 위해 제공된 언어 동작 레이블과 성공적인 에피소드에서 수정을 위한 관련 이미지를 수집한다. 그런 다음, 모델은 이미 언어 모션 보정을 작업에서 성공한 액션에 매핑하는 방법을 알고 있기 때문에 RT-H에서 액션 쿼리를 다시 훈련할 필요가 없으며 대신 언어 모션 쿼리를 업데이트하여 올바른 상위 레벨 모션만 생성하면 된다(도 2 참조; 오른쪽 하단). 이는 큰 동작 공간보다는 더 작은 언어 동작 공간의 사소한 변화만을 학습하면 되기 때문에 교정으로부터 학습의 복잡성을 크게 감소시킨다. IWR [5]와 같이, 우리는 원래 데이터세트(액션 및 언어 모션 쿼리 둘 다)와 수정 데이터세트(단지 언어 모션 쿼리)의 혼합으로 공동 훈련한다. 자세한 혼합물 내용은 부록 B를 참조하십시오. 물론, 언어 모션 및 액션 쿼리 모두에 대해 단일 모델을 사용하기 때문에, 하나만을 업데이트하는 것은 여전히 _original dataset_에서 두 쿼리 모두에 대해 다른 - 공동 트레이닝 RT-H를 업데이트하는 것과 동시에 개입 데이터세트를 통해 언어 모션 공간의 사소한 변화를 여전히 학습하는 데 도움이 된다.\n' +
      '\n' +
      '## V Experiments\n' +
      '\n' +
      'RT-H의 성능을 종합적으로 평가하기 위해 네 가지 주요 실험 질문을 연구한다.\n' +
      '\n' +
      '***Q1(성능)**: 언어를 사용한 액션 계층은 다양한 다중 작업 데이터 세트에 대한 정책 성능을 향상시키나요?\n' +
      '**Q2(Contextuality)**: RT-H에서 학습된 언어 동작이 작업과 장면에 맥락적입니까?\n' +
      '****Q3(Corrections)**: 언어 동작 교정에 대한 훈련이 원격 조작 교정보다 더 나은가요?\n' +
      '**Q4(일반화)**: 액션 계층 구조가 유통 외 설정에 대한 견고성을 개선합니까?\n' +
      '\n' +
      '**Dataset**: 우리는 무작위화된 객체 포즈 및 배경을 가진 100K 시연으로 구성된 대규모 다중 작업 데이터 세트를 활용한다. 이 데이터 세트는 다음 데이터 세트를 결합한다:\n' +
      '\n' +
      '* _Kitchen_: RT-1[6] 및 RT-2[4]에서 사용된 데이터셋은 70K 시연에서 6개의 시맨틱 태스크 범주로 구성된다.\n' +
      '\n' +
      '*_Diverse_: 24개 이상의 시맨틱 태스크 카테고리를 갖지만, 단지 30K 시연(더 자세한 내용은 부록 C 참조)으로 더 복잡한 태스크 범위로 구성된 새로운 데이터세트.\n' +
      '\n' +
      '우리는 이 결합된 데이터 세트를 _Diverse+Kitchen_(_D+K_) 데이터 세트라고 부르며 섹션 III-C에 설명된 자동화된 절차를 사용하여 언어 동작으로 레이블이 지정된다. 우리는 가장 어려운 작업의 대표적인 샘플인 8개의 작업에 대해 전체 _Diverse+Kitchen_ 데이터 세트에 대해 훈련된 방법을 평가한다:\n' +
      '\n' +
      '1. "카운터에 그릇을 똑바로 뒤집기"\n' +
      '2. "open pistachio jar"\n' +
      '3. "Close pistachio jar"\n' +
      '4. "시리얼 디스펜서로부터 그릇을 멀리 이동"\n' +
      '5. "시리얼 디스펜서 아래에 그릇 넣기"\n' +
      '6. "오트밀 봉지를 그릇에 넣어라"\n' +
      '7. "grab scooper from basket."\n' +
      '8. "디스펜서로부터 냅킨 끌어당기기"\n' +
      '\n' +
      '이 8가지 작업은 그림 1에 나와 있다. 3은 복잡한 동작의 시퀀스와 높은 정밀도를 요구하기 때문에 선택되었다.\n' +
      '\n' +
      '** 방법들**: RT-H에서 다수의 선택들을 절제하는 것을 포함하는 다음의 방법들을 연구하고 비교한다:\n' +
      '\n' +
      '**RT-H**는 이 작업에서 제안된 방법이며, 이러한 실험을 위해 비동기 쿼리 변형을 사용한다(섹션 IV 참조).\n' +
      '**RT-H-Joint**도 우리의 방법이지만 각 쿼리에 대해 두 개의 다른 프롬프트로 VLM에 두 번 쿼리하는 것이 아니라 언어 동작과 동작을 모두 생성하기 위해 단일 자동 회귀 쿼리를 사용한다. RT-H-Joint는 먼저 언어 모션을 출력한 다음 액션(여기서 액션은 여전히 언어 모션에 컨디셔닝된다)을 출력한다. RT-H와 RT-H-Joint가 모두 언어 모션에 대해 자기회귀적이지만, RT-H는 액션 및 언어 모션에 대해 별개의 쿼리를 사용한다("What _motion..._" vs. "What _action..._, 주어진 모션..." 반면 RT-H-Joint는 하나의 쿼리("What _motion and action..._")만 있습니다. 보다 구체적으로, RT-H는 액션 질의에서 언어 모션을 Encoder로 전달하는 반면, RT-H-Joint는 액션을 예측할 때 언어 모션을 Decoder 입력으로 처리한다. 따라서 RT-H와 비교할 수 있는 성능을 기대할 수 있다.\n' +
      '**RT-H-클러스터**는 자동화된 언어 모션 라벨링 절차의 삭제로, 대신 K-means[53]을 사용하여 액션을 정수 레이블이 있는 클래스 집합으로 직접 클러스터링한다. 이러한 클래스 레이블은 RT-H에서 자동으로 레이블이 지정된 언어 동작 대신 사용됩니다.\n' +
      '***RT-H-OneHot**는 각각의 고유한 언어 모션을 정수 클래스 레이블로 대체함으로써 RT-H에서 모션을 나타내기 위한 언어의 사용을 제거한다.\n' +
      '**RT-2**는 어떠한 액션 계층구조도 사용하지 않는 플랫 모델이다[4].\n' +
      '**RT-H+인적 개입**은 실행 중에 언어 동작들만을 인간이 올바르게 하는 것을 수반하지만, 여전히 RT-H(도 2의 우측 상단)로부터의 액션 쿼리를 사용하는 것을 수반한다. RT-H + 인간 개입은 인간이 개입할 수 있도록 하는 우리의 방법의 변형이므로 다른 비 개입 기반 방법에 대한 상한이 될 것으로 기대한다. 우리는 이것을 V-C 섹션에서 논의한다.\n' +
      '***RT-H-Intervene**는 언어 움직임 보정을 이용하여 인간의 개입 데이터에 대해 추가적으로 훈련된 RT-H 방법의 확장이다. 우리는 이것을 V-C 섹션에서 논의한다.\n' +
      '***RT-H-InterveneAction**는 인간의 개입 데이터로부터 동작 교정 및 언어 동작 교정을 훈련하는 RT-H-Intervene 방법의 절제이다. 우리는 이것을 V-C 섹션에서 논의한다.\n' +
      '* 언어 동작 보정과 대비되는\n' +
      '- 및 V-C 섹션의 RT-H-인터벤트와 비교된다.\n' +
      '\n' +
      'RT-H-Joint, RT-H-Cluster 및 RT-H-OneHot는 여전히 액션 계층을 활용하는 RT-H의 변형이다. 정확한 쿼리와 각 RT-H 변형 구현에 대한 자세한 내용은 부록 A를 참조하십시오.\n' +
      '\n' +
      'V-A 절에서는 먼저 다양한 다중 작업 데이터 세트(**Q1**)에서 RT-H의 성능을 훈련하고 평가한다. V-B절에서는 언어 동작이 문맥에 어떻게 적응하는지 알아보기 위해 다양한 작업에 걸쳐 학습된 언어 동작을 정성적으로 분석한다(**Q2**). 섹션 V-C에서는 RT-H 위에 있는 언어 동작 보정을 수집하고 훈련하여 훈련이 있음을 보여준다.\n' +
      '\n' +
      '도. 3: 8개의 도전적 평가 과제로 구성된 _Diverse+Kitchen_ multi-task dataset에 대한 결과. RT-H는 평균 15%의 RT-2를 능가하여 작업의 6/8에서 더 높은 성능을 얻습니다. 클래스 레이블(RT-H-OneHot)로 언어를 대체하면 성능이 크게 떨어집니다. 자동화된 모션 라벨링 절차 대신 K-Means[53]를 통해 액션 클러스터를 사용하면 성능도 약간 떨어지며(RT-H-Cluster), 중간 액션 계층으로서 언어 모션의 유용성을 입증한다.\n' +
      '\n' +
      '언어의 움직임 보정은 정책 성능을 향상시킨다(**Q3**). 마지막으로 V-D 섹션에서는 장면, 객체 및 작업의 변화에 대한 RT-H의 견고성을 테스트한다(**Q4**).\n' +
      '\n' +
      '다양한 Multi-Task Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_Datasets_D\n' +
      '\n' +
      '여기서는 액션 계층 구조가 **Q1**을 다루는 정책 성능을 개선할 수 있는 방법에 대해 논의합니다. 우리는 먼저 _Diverse+Kitchen_ 데이터셋에서 훈련될 때 RT-H 및 그 변형의 온라인 성능에 대해 논의할 것이다. 그런 다음 오프라인 성능 메트릭을 제시하여 언어 동작의 역할을 추가로 분석한다.\n' +
      '\n' +
      '**On-Robot Performance**: Fig. 도 3은 _Diverse+Kitchen_ 데이터세트에 대해 트레이닝되고 앞서 논의된 이 데이터세트 내의 8개의 선택된 태스크들에 대해 평가될 때 각각의 방법의 성능을 예시한다. 검사점은 검증 작업 MSE를 사용하여 선택한 다음 각 작업에 대해 10개의 제어된 시도(방법당 총 시도 80개)를 실행합니다. RT-H는 대부분의 작업에서 RT-2를 능가하며, **RT-2를 평균 15% 능가하며, 이는 추가 인간 주석을 사용하지 않았음에도 불구하고 액션 계층(**Q1**)의 이점을 강력하게 지원한다. 각 태스크의 각 단계에 대한 성공률은 부록 D를 참조하십시오. 여기서 RT-H가 7/8 태스크에서 성공으로 더 많이 발전하는 것을 볼 수 있습니다. 또한, RT-2는 4/8 태스크에서만 0이 아닌 성능을 달성하는 반면, RT-H는 6/8 태스크에서 0이 아니며 RT-H-Joint는 모든 태스크에서 0이 아니므로 RT-H와 RT-H-Joint가 데이터 세트에서 작업의 다양성을 더 잘 관리함을 시사한다.\n' +
      '\n' +
      '**Ablations**: RT-H-Joint는 RT-H와 비교가능하며, RT-H가 정확한 질의 메커니즘에 강인함을 보여준다. RT-H-클러스터는 자동화된 라벨링 절차를 액션 클러스터링으로 대체하며, 언어 없이 평균적으로 RT-H보다 약간 더 나쁜 성능을 수행한다. 흥미롭게도 RT-H-클러스터는 평가 세트(개방 및 폐쇄 피스타치오 병)에서 가장 어려운 작업을 더 잘 수행한다. 우리는 RT-H-클러스터가 데이터 세트에서 파생된 클러스터를 사용하기 때문에, 그 클러스터는 라벨링 절차보다 훨씬 더 미세한 액션 컨텍스트를 제공하여 정확한 작업에서 RT-H를 능가할 수 있다고 가정한다. 그러나, 언어의 부족은 광범위한 데이터 세트를 사용할 때 언어 동작을 예측하는 것보다 클러스터를 예측하는 것을 어렵게 하여 광범위한 작업 세트에서 RT-H-클러스터의 성능을 악화시킨다. RT-H-OneHot는 언어 모션을 원핫 클래스 레이블로 대체하며, 동일한 기본 언어 모션에서 파생되었음에도 불구하고 RT-H보다 훨씬 더 나쁜 성능을 수행한다. 따라서, 액션 계층 구조 자체가 우리에게 그 방법의 일부를 제공하지만, 언어의 구조는 언어 움직임과 액션 예측을 크게 향상시킨다.\n' +
      '\n' +
      '**오프라인 성능**: RT-H와 그 조인트 변형 RT-H-Joint 대 RT-H-Joint에 걸친 종단간 액션 예측을 위한 오프라인 유효성 검사 평균 제곱 오차(MSE)를 비교함으로써 액션 예측을 위한 중간 계층으로서의 언어 모션이 눈에 띄는 효과가 있는지 조사한다. 평면 RT-2 모델(**Q1**)입니다. 엔드 투 엔드 MSE는 각 모델이 액션 예측을 얼마나 잘 학습하는지 반영한다. RT-H의 경우, 언어 동작 쿼리(\\(\\pi_{h}\\))로부터 추론된 언어 동작을 입력하기보다는 데이터에 라벨링된 _ground truth_(GT) 언어 동작을 동작 쿼리에 입력(\\(\\pi_{l}\\))으로 사용할 때 동작 검증 MSE를 연구한다. 이 그라운드 트루스 MSE는 진정한 언어 동작이 동작을 예측하는 데 얼마나 유익한지를 반영한다. 표 I에서 우리는 _Diverse+Kitchen_ 데이터세트 또는 _Kitchen_ 데이터세트에 대해 훈련될 때 RT-H, RT-H-Joint 및 RT-2에 대한 훈련 체크포인트에 걸쳐 최소 MSE를 보고한다. RT-H는 RT-2**보다 MSE가 약 **20% 낮고 RT-H-Joint는 RT-2**보다 MSE가 **5-10% 낮아 액션 계층이 대규모 다중 작업 데이터 세트에서 오프라인에서 액션 예측을 개선하는 데 도움이 됨을 보여준다. 또한 하나의 질의(RT-H-Joint) 대신에 두 질의(RT-H)를 사용하는 것은 동작 예측을 향상시키는 것으로 보이며, 이는 언어 모션이 (RT-H를 위한 인코더를 통해) 모델로 전달되는 방법과 RT-H-Joint를 위한 디코더를 통해 전달되는 방법에서 기인할 수 있다. RT-H(GT)는 Ground truth MSE 메트릭을 이용하며, MSE와 끝에서 끝까지의 간격이 40%임을 알 수 있으며, 이는 올바른 라벨링된 언어 모션이 행동을 예측하는 데 매우 유익함을 보여준다.\n' +
      '\n' +
      '###_Contextual & flexible Language Motions_###\n' +
      '\n' +
      '이 절에서는 (1) **맥락성**: 단일 배포 언어 모션에 대한 액션이 장면의 맥락과 작업 지시에 얼마나 잘 적응하는지, (2) **유연성**: RT-H가 배포 외 언어 모션에 얼마나 잘 반응하는지 분석한다.\n' +
      '\n' +
      '**Contextuality**: 우리는 도 4에서 RT-H의 온라인 평가로부터 취해진 문맥 모션의 여러 예를 예시한다. 우리는 동일한 언어 모션이 종종 더 높은 레벨의 언어 모션(**Q2**)을 존중하면서, 작업을 완료하기 위한 액션의 미묘한 변화를 초래한다는 것을 안다. 예를 들어, 도의 좌측 상단 예에서 "팔을 앞으로 이동"한다. 도 4에 도시된 바와 같이, 암은 일반적으로 전방으로 이동하지만 또한 관심 대상인 냅킨 디스펜서를 향해 이동한다. 또한 팔을 약간 기울여 냅킨을 더 쉽게 잡을 수 있습니다. 오른쪽 상단에서 동일한 명령이 곡물 디스펜서와 충돌을 피하기 위해 그리퍼의 약간의 하향 각도와 회전으로 이어지는 것을 볼 수 있다. 그림의 왼쪽 가운데에 있는 "암 왼쪽 이동"에 대해. 4, 우리는 유사하게 왼쪽이 오트밀 패킷을 그릇 위로 정확하게 이동하는 것을 의미하는 반면, 오른쪽 중간에서 왼쪽은 병에 걸리기 위해 뚜껑의 정확한 움직임을 의미한다. 이러한 맥락성을 포착하기 위해 하나의 "이동 팔 왼쪽" 원시적인 것을 설계하는 것은 엄청나게 어려울 것이다. 그림 1의 하단 행에서 "회전 암 오른쪽"에 대한 유사한 동작을 볼 수 있다. 4, 왼쪽 케이스에서 팔\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c|c} Train & Eval & RT-2 & RT-H-Joint & RT-H & RT-H (GT) \\\\ Dataset & Dataset & & & & \\\\ \\hline _Kitchen_ & _Kitchen_ & 30.2 & 28.22 & **24.9** & 17.9 \\\\ _D+K_ & _Diverse_ & 27.7 & 25.44 & **23.6** & 17.8 \\\\ \\end{tabular}\n' +
      '\\end{table} TABLE I: Best checkpoint Mean Squared Error (MSE) for end-to-end action prediction on the validation set for models (columns) trained on different multi-task datasets (rows). _Kitchen_ refers to the data used to train RT-1 [6] and RT-2 [4] (70K demonstrations), _Diverse+Kitchen_ (_D+K_) refers to a combination of _Kitchen_ and the more complex set of tasks (30K demonstrations). We also report the MSE of using the _ground truth_ language motion (the labeled language motion) for the action query in RT-H (GT) rather than the inferred language motion from the language motion query. RT-H and RT-H-Joint achieve lower MSE on both datasets compared to RT-2, illustrating the benefits of action hierarchies for ingesting multi-task datasets compared to flat models like RT-2. Also, RT-H has lower MSE than RT-H-Joint.\n' +
      '\n' +
      '닫힌 항아리 위에 앉기 위해 회전하고 위로 머무르는 동안 오른쪽 경우 암이 회전하고 아래로 이동하여 테이블의 뚜껑에 도달합니다. 언어 동작 맥락성에 대한 정량적 분석은 부록 D를 참조하십시오.\n' +
      '\n' +
      '**Flexibility**: In Fig. 도 5를 참조하면, 온라인 상에서 RT-H의 언어 동작에 개입하여, 대신 배포 내 작업에 대한 배포 외 언어 동작을 수행함으로써 RT-H의 유연성을 입증한다. 첫 번째 행 (a)에서 RT-H는 "풀 냅킨" 작업을 완료하는 두 가지 유효한 방법으로 테스트되며 두 가지 모두에 올바르게 응답함을 알 수 있다. 각 언어 동작에도 불구하고 그림 1에 나와 있다. 5는 작업에 대한 배포가 불가능하므로 RT-H는 이러한 새로운 언어 동작을 쉽게 따라할 수 있다(**Q2**). 하단의 두 행(b)에 있어서, 도 5를 참조하면, RT-H는 작업에만 국한되지 않고 훈련 데이터에서 볼 수 없는 보다 일반적인 언어 모션에도 유연하다는 것을 알 수 있다. 예를 들어, 중간 우측 예에서, 팔을 항아리에서 멀리 이동시키는 것은 "닫힌 피스타치오 항아리"에 대한 일반적인 언어 모션이 아니지만, RT-H는 여전히 이러한 언어 모션에 응답하여 올바르게 동작할 수 있다. 보다 일반적인 언어 모션에 유연하게 반응하는 것은 특히 작업이나 장면이 배포되지 않고 새로운 언어 모션 시퀀스를 필요로 할 때 매우 다양한 언어 모션 보정에 응답하는 데 중요하다.\n' +
      '\n' +
      '전반적으로 RT-H는 언어 동작(**Q2**)을 통해 각 과제의 상위 구조를 학습하면서 행동의 유연성과 맥락성을 유지할 수 있음을 알 수 있다. 문맥성에 대한 정량적 분석과 RT-H의 언어 모션 멀티모달리티에 대한 정성적 관찰은 부록 D를 참조하고 각 작업에 대한 각 방법에 대한 단계별 성공률을 참조하십시오. 다음으로 이러한 속성을 활용하여 RT-H를 개선하기 위해 언어 동작 보정을 수집하고 훈련한다.\n' +
      '\n' +
      '### _Online Corrections__\n' +
      '\n' +
      '이 섹션에서는 원격 조작 보정 데이터(**Q3**)를 사용하는 액션 계층 구조가 없는 방법과 비교하여 RT-H가 언어 모션 보정으로부터 얼마나 잘 학습할 수 있는지에 관심이 있다. V-A절에서 8개의 작업 각각에 대해 다중 작업 언어 모션 보정 데이터 세트와 원격 조작 보정 데이터 세트를 수집한다. 이전의 상호 작용 IL 방법 [5, 39]에서와 같이, 인간은 일반적으로 작업 실패를 예상하거나 응답하는 두 데이터 세트에서 언제 수정할지를 결정한다. 다음으로 각 방법에 대한 수집 및 훈련 파이프라인에 대해 설명한다.\n' +
      '\n' +
      '도. 4: 언어 모션들이 장면 및 태스크의 _context_에 어떻게 의존하는지를 보여주는 예들로서, _Diverse+Kitchen_ 데이터세트 상에서 트레이닝된 RT-H의 온라인 평가들로부터 취해진다. 각 행에 대해, 주어진 언어 동작("암 앞으로 이동", "암 왼쪽으로 이동", "암 오른쪽으로 회전")은 스피드의 미묘한 변화, "암 앞으로 이동"에 대한 회전, 및 심지어 그리퍼 위치와 같은 작업 및 관찰에 따라 상이한 변형(열)으로 나타난다.\n' +
      '\n' +
      '도. 5: 학습된 언어 동작의 유연성의 예. 맨 위 행 (a)에서 우리는 디스펜서에서 냅킨을 "오른쪽과 아래" 또는 "위쪽과 뒤쪽"으로 빼내기 위해 두 가지 다른 작업 완료 언어 동작을 사용하여 RT-H를 보정하며 RT-H가 둘 다 올바르게 수행함을 보여준다. 아래 두 행(b)의 경우 여전히 언어 동작을 수정하지만 RT-H에게 각 작업에 대해 보다 일반적인 언어 동작 세트를 수행하도록 요청하여 RT-H가 주어진 작업에 대해 완전히 배포되지 않은 언어 동작에도 종종 유연함을 보여준다.\n' +
      '\n' +
      '**RT-H-Intervene 및 RT-H-InterveneAction**: 섹션 IV-A에 기술된 교정 절차를 사용하여, 8개의 작업 각각에 대해 30개의 언어 동작 교정의 에피소드(필터링되지 않은 에피소드)를 수집한다. 수집에 사용되는 기본 정책은 _Diverse+Kitchen_ 데이터세트(섹션 V-A와 동일)에 대해 훈련된 RT-H이다. 그런 다음 섹션 IV-A에 설명된 방식으로 이러한 온 정책 수정에 대해 RT-H를 훈련하여 RT-H-인터벤트를 생성한다(개입 데이터에 대한 언어 모션 쿼리만 훈련). RT-H-InterveneAction을 훈련하기 위해, 우리는 중재 데이터에 대해 훈련할 때 동일한 샘플링 속도로 언어 모션 및 액션 쿼리를 모두 포함한다.\n' +
      '\n' +
      '**RT-2-IWR**: 언어 동작 교정 대신 VR 기반 원격 조작을 사용하여 동일한 8개의 작업에 대해 원격 조작 교정의 30개 에피소드(필터링되지 않은 에피소드)를 수집한다. 우리는 RT-2의 고장 모드를 수정하는 학습에만 신경을 쓰기 때문에, RT-H-Intervene와 공정한 비교를 보장하기 위해 수집의 기본 정책으로 _Diverse+Kitchen_ 데이터세트(RT-H-Intervene와 동일)에서 훈련된 RT-2를 사용해야 한다. 그런 다음 개입 가중 회귀(IWR) 방법[5]을 사용하여 이러한 온 정책 수정에 대해 RT-2를 훈련하여 RT-2-IWR을 생성한다.\n' +
      '\n' +
      '물론 RT-2에 대한 기본 정책은 RT-H보다 이러한 작업에 대해 _Diverse+Kitchen_보다 더 나쁜 성능을 수행하므로 공정한 비교를 보장하기 위해 각 방법에 대한 보정 데이터에 대한 훈련 전후의 성공률에서 _change_에 중점을 둔다.\n' +
      '\n' +
      '**결과**: 우리는 섹션 V-A에서와 동일한 8개의 평가 과제에서 두 방법을 모두 평가한다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 6에서 각 방법의 성능을 보정 전 모델인 RT-H 및 RT-2와 각각 비교한다(도 3에서 중복). 또한 필요한 경우 온라인 휴먼-인-더-루프 언어 움직임 보정을 사용하지만, 여전히 이러한 언어 움직임 보정을 조건으로 하는 RT-H에서 액션 쿼리를 사용하는 상위바운드 방법으로 RT-H + 휴먼 개입을 비교한다.\n' +
      '\n' +
      '먼저, 가장 정확한 작업에서도 매우 높은 성공률을 얻을 수 있는 RT-H + 인간 개입으로 언어 동작 교정이 RT-H에 얼마나 순응적인지를 알 수 있다. 이것은 RT-H가 테스트 시간에 언어 동작 교정을 통해 작업 관련 방식으로 실제로 행동을 변화시킨다는 것을 보여준다. 이는 V-B절에서 언어 모션이 유연하고 맥락적이라는 주장을 더욱 뒷받침한다. 또한 언어 움직임 예측은 종종 성능에 대한 병목 현상이라는 점을 강조하므로 개입을 통한 언어 움직임 예측의 정교화는 명확한 개선을 가져올 것으로 기대한다.\n' +
      '\n' +
      '최첨단 온라인 모방 학습 방법인 RT-2-IWR은 작업당 상대적으로 적은 양의 데이터가 결합되고 한 번의 수정만 사용되었기 때문에 평균 25%에서 13%로 성능이 저하된다고 본다. 또한, 원격 조작 기반 보정은 훈련 데이터(따라서 기본 정책)와 너무 다른 액션 분포를 도입할 가능성이 더 높다고 의심한다. 반면에 RT-H의 언어 움직임 보정은 기본 정책 자체(언어 움직임 공간의 약간의 변화 아래)에서 액션이 나오고 따라서 학습하기 쉽기 때문에 훈련 데이터와 훨씬 더 일치한다.\n' +
      '\n' +
      '반면에 RT-H-인터벤트는 동일한 양의 데이터를 사용함에도 불구하고 이 설정에서 RT-2-IWR을 실질적으로 능가하여 더 단단한 정밀 작업(개방 및 폐쇄 피스타치오 병)에서 60-70% 개선된다. RT-H-인터벤은 "오트밀 패킷을 그릇에 담아라"라는 하나의 작업에서 회귀하는데, 여기서 로봇이 패킷을 매우 자주 성공적으로 파악하지만 "가까운 그립퍼"를 예측하지 못하는 것을 관찰했다. 데이터 세트에서 언어 동작 보정에 대한 약간의 편향이 있는 것으로 의심되며, 후속 보정으로 "팔을 위로 이동"을 지정하거나 더 많은 보정을 실행하여 해결할 수 있다. 오트밀 예는 또한 언어 모션 보정이 정책의 동작을_해석 가능_으로 만들고 따라서 디버깅에 더 직관적이게 할 수 있는 방법을 강조한다 - 설계자가 실패 지점을 식별하거나 수정할 수 있도록 더 효과적으로 허용한다.\n' +
      '\n' +
      'RT-H-InterveneAction은 또한 RT-H에서 개선되어 평균 9%의 성능을 능가한다. 우리는 RT-H-Intervene에 비해, RT-H-InterveneAction은 정책 변성을 겪으며, 개입에서 새로운 액션이 실제 전문가 분포보다는 모델 생성 액션(RT-H에서 액션 쿼리를 사용하여 언어 움직임 보정 데이터를 수집하기 때문에)에 대한 액션 분포를 편향시키는 것으로 의심한다. 이러한 모델 생성 액션은 최적이 아닐 수 있으므로 성능에 영향을 미칠 수 있다. 예를 들어, 근접 피스타치오 항아리 작업은 RT-H-InterveneAction에서 RT-H-InterveneAction만큼 개선되지 않는다는 것을 알 수 있는데, 이는 정책이 개입 생성 액션이 적은 상태(예: 항아리 뚜껑을 파지한 후)에서 거의 0에 가까운 액션을 생성하기 시작하기 때문이다.\n' +
      '\n' +
      '도. 6: 도 3에서와 동일한 8개의 평가 태스크에 대해, _Diverse+Kitchen_ 멀티-태스크 데이터세트 상에서 트레이닝된 모델들에 대한 수정에 대한 결과들. RT-2-IWR은 롤링 아웃 RT-2로부터의 텔레오퍼레이션 보정들에 대해 트레이닝되는 반면, RT-H-인터벤트는 롤링 아웃 RT-H로부터의 언어 모션 보정들에 대해 트레이닝된다. RT-H-InterveneAction은 언어 모션 및 액션 보정 데이터 모두에 대해 트레이닝된다. 우리는 RT-H-Intervene가 RT-H에서 개선되고 RT-2-IWR보다 훨씬 더 나은 성능을 보여 언어 동작이 원격 조작 동작보다 수정을 배우기에 훨씬 더 효율적인 공간임을 시사한다. RT-H-InterveneAction은 RT-H보다 더 나은 성능을 보이지만, 개입 중 RT-H에 의해 생성되는 동작은 차선책이 될 수 있기 때문에 미세 조정 동작은 때때로 정책 퇴화로 이어진다.\n' +
      '\n' +
      '전반적으로 언어 동작 보정은 작업당 30회의 보정**로 RT-H **의 평균 성공률을 40%에서 63%로 가져온다는 것을 알 수 있다. 보다 응축된 언어 모션 공간으로 액션을 추상화함으로써, RT-H는 텔레오퍼레이션 보정(**Q3**)보다 언어 모션 보정으로부터의 피드백으로부터 자신을 개선하는 것을 더 빠르게 학습할 수 있다.\n' +
      '\n' +
      '### _Generalization_\n' +
      '\n' +
      '**Q4**를 평가하기 위해, 우리는 새로운 장면(비슷하지만 새로운 배경과 조명이 있는)에 대한 일반화, 새로운 대상에 대한 일반화, 새로운 작업에 대한 일반화의 세 가지 유형을 연구한다. 달리 언급되지 않는 한(즉, _Diverse_ 데이터를 포함하지 않는) _Kitchen_ 데이터세트 [6]에만 훈련된 RT-H를 사용하며, 이는 다양한 객체에 대한 다음과 같은 훈련 및 평가 작업으로 구성된다:\n' +
      '\n' +
      '1. "녹오버"\n' +
      '2. "서랍장"\n' +
      '3. "move"\n' +
      '4. "pick"\n' +
      '5. "열림/닫힘 서랍"\n' +
      '6. "직립 위치"\n' +
      '\n' +
      '**New Scenes로의 일반화**: 우리는 새로운 환경, 특히 다양한 조명, 다양한 배경 및 층으로 구성된 새로운 건물에서 _Kitchen_ 데이터 세트의 각 작업을 평가한다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 7을 참조하면, RT-H 및 RT-H-Joint는 "place upright" 태스크 및 "open/Close drawer" 태스크들(**Q4**) 중 가장 어려운 태스크들에 대해 특히 큰 델타들과 함께 장면들의 변화에 더 견고하다는 것을 알 수 있다.\n' +
      '\n' +
      '**새로운 물체로의 일반화**: 우리는 배, 코코넛 워터, 오레오와 같은 훈련 중에 보이지 않는 물체에 대한 50개의 평가를 사용하여 물체 일반화 하에서 "선택"과 "이동"을 평가한다. 표 II에서 RT-H는 이러한 작업에서 65%를 달성하는 반면 RT-2는 55%를 달성한다는 것을 발견했다. 부록 D에 도시된 바와 같이, RT-H는 또한 평균적으로 RT-2(**Q4**)에 비해 각 태스크(각 태스크의 단계 측면에서)에서 더 멀리 진행한다.\n' +
      '\n' +
      '**제한된 수정을 가진 새로운 작업으로의 일반화**: 배포 외 작업에 대한 제로 샷 성공은 그림 1에서 상당히 어렵다. 도 8을 참조하면, 우리는 보이지 않는 작업의 경우에도 RT-H가 작업에서 성공하기 위해 몇 번의 적절한 시간 보정만 필요하다는 것을 정성적으로 보여준다. 이러한 예를 위해 우리는 다양한 컨텍스트에서 입증된 언어 모션과 함께 RT-H를 제공하기 위해 _Diverse+Kitchen_ 데이터 세트에서 훈련된 RT-H 버전을 사용한다. 도. 도 8은 또한, 겉보기에 다양한 태스크들 사이의 공유 구조를 도시한다: 이러한 태스크들 각각은 태스크를 시작하기 위해 약간의 피킹 동작을 필요로 하고, 많은 다양한 태스크들에 걸쳐 언어 모션들의 공유 구조를 학습함으로써, RT-H는 어떠한 수정 없이 피킹 단계를 완료할 수 있다(**Q4**). RT-H가 더 이상 언어 움직임 예측을 일반화할 수 없는 경우에도 언어 움직임 보정이 일반화하는 경우가 많다는 것을 알 수 있어 몇 번의 보정만으로 작업을 성공적으로 완료할 수 있다(**Q2**, **Q4**). 이것은 새로운 작업에 대한 데이터 수집을 확장하기 위한 언어 모션의 잠재력을 보여준다.\n' +
      '\n' +
      '## VI Conclusion\n' +
      '\n' +
      '본 논문에서는 높은 수준의 작업과 낮은 수준의 작업 사이의 중간 예측 계층으로 "팔을 앞으로 움직인다"와 같은 언어 동작을 활용하는 RT-H를 소개한다. RT-H는 언어로 기술된 작업을 언어 동작으로 매핑하고, 추론된 언어 동작을 사용하여 동작을 예측하며, 여기서 두 단계 모두 시각적 입력과 작업에 조건화된다. 우리는 인간의 라벨링 비용 없이 다양한 작업에 확장되는 자동화된 절차를 사용하여 언어 모션에 라벨을 붙인다. 우리는 RT-2[4]와 같은 단일 트랜스포머 모델을 사용하여 RT-H를 인스턴스화하는데, 여기서 액션 쿼리와 언어 모션 쿼리는 모두 방대한 양의 인터넷 스케일 데이터와 함께 공동 학습된다. RT-H(1)는 겉보기에 이질적인 태스크들에 걸쳐 공유된 태스크 구조를 학습함으로써 상이한 태스크들 간의 더 많은 데이터 공유를 가능하게 하고, 따라서 더 유능하다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c}  & pick & move & **Average** \\\\ \\hline \\hline RT-2 & 60 & 50 & 55 \\\\ RT-H & 70 & 60 & 65 \\\\ \\end{tabular}\n' +
      '\\end{table} TABLE II: We evaluate RT-2 and RT-H trained on _Kitchen_ data [6] on the “pick” and “move” tasks but under novel objects for 50 scenarios total. RT-H outperforms RT-2, demonstrating that action hierarchy helps the policy generalize to novel objects.\n' +
      '\n' +
      '도. 7:_Kitchen_ 데이터에 대해 훈련된 모델 [6]이 동일한 작업에 배치되었을 때 결과이지만, 새로운 배경, 조명 및 바닥이 있는 새로운 건물에서 RT-H 및 RT-H-Joint는 각각 RT-2를 능가하여 액션 계층 구조의 사용이 정책이 새로운 장면으로 일반화되는 데 도움이 됨을 시사한다. RT-2는 특히 이러한 새로운 장면에서 직립하고 서랍을 열고 닫는 데 어려움을 겪는다.\n' +
      '\n' +
      '다중 작업 데이터 세트를 스케일로 섭취하고 (2) 장면과 작업의 컨텍스트 내에서 기본 동작을 변경하는 언어 동작 보정에 적합하다. 실험에서 RT-H가 다양한 다중 작업 데이터에서 RT-2 및 액션 계층 삭제를 능가한다는 것을 보여준다. 그리고 RT-2는 보이지 않는 언어 동작에서도 언어 동작 공간에서 높은 보정이 가능하며, 이러한 언어 동작 보정에 의한 학습이 원격 조작 기반 보정에 의한 학습보다 우수하다는 것을 보인다. 마지막으로, RT-H가 RT-2에 비해 장면 및 객체 변화에 더 강인하다는 것을 보여준다. 이러한 결과는 언어를 이용한 액션 계층 구조의 가능성을 보여주며, RT-H가 데이터 수집과 로봇 학습을 확장할 수 있는 강력한 기반을 제공한다고 믿는다.\n' +
      '\n' +
      '**제한과 미래 작업**: RT-H는 미래 작업을 위한 몇 가지 흥미로운 길을 열어줍니다. 먼저, 크고 다양한 데이터 세트에서 RT-H를 테스트하여 최첨단 성능을 달성하지만 절대 성공률은 교정 훈련 후에도 여전히 개선의 여지가 있다. RT-H의 보다 효율적인 언어 동작 보정 샘플로 입증된 바와 같이 향후 작업은 오프라인 데이터 세트와 수정 파이프라인을 모두 확장해야 하며 언어 동작을 사용할 수도 있다고 믿는다.\n' +
      '\n' +
      '그림 8: 우리는 최소한의 보정으로 완전히 보이지 않는 작업으로 RT-H의 일반화 능력을 보여준다. RT-H는 작업을 언어 동작으로 분해함으로써 겉보기에 다양한 작업 간의 공유 구조를 학습한다. 이를 통해 RT-H가 쉽게 피킹 단계를 수행하는 각 태스크의 첫 번째 부분에서 볼 수 있듯이 언어 동작을 새로운 태스크로 일반화할 수 있다. 또한 RT-H가 제로샷 일반화를 할 수 없을 때 언어 모션 보정이 종종 일반화되어 몇 번의 적절한 시간 보정만으로 이러한 작업을 완료할 수 있음을 보여준다.\n' +
      '\n' +
      'OXE[54]와 같은 많은 상이한 실시예들을 갖는 데이터세트들을 브릿지하거나, 심지어는 언어로만 기술된 액션들을 갖는 인간 비디오들로부터 학습하는 것을 돕는다.\n' +
      '\n' +
      '둘째, 섹션 V-A에서 서로 다른 액션 계층을 제거하지만, 향후 작업이 중간 계층(예: 객체 참조 언어 대 언어 모션 사용)에 대한 최상의 추상화 수준을 결정하는 데 필요하다. 또한, 우리는 주로 이 작업에서 하나의 중간 액션 레이어 또는 액션 추론의 한 단계만을 테스트하지만, 향후 작업은 사용자가 어떤 수준의 추상화에서도 개입할 수 있는 대신 액션 추론의 여러 단계를 정의할 수 있다. 예를 들어, 우리는 언어 동작 및 액션에 태스크를 매핑하기 전에 "방 청소"와 같은 긴 수평선 명령에서 "코카인 캔 선택"과 같은 개별 태스크로 가기 위해 _task_ 예측 레벨을 추가할 수 있다. 계층의 모든 수준에서 보정을 분해하려면 모델에 계층에서 보정을 자동으로 지정하도록 가르친 다음 로봇 작업 수준에 도달할 때까지 하위 수준 작업을 자동으로 예측합니다.\n' +
      '\n' +
      '셋째, 언어 동작은 행동을 예측할 수 있는 맥락적이고 압축된 공간을 나타낸다. RT-H에서 이러한 모션 컨텍스트성을 활용하여 강화 학습 방법과 정책 탐색을 위한 액션 공간을 크게 압축하여 언어 모션 예측을 사전으로 활용할 수 있다. 우리는 언어 운동이 인간에게 매우 해석 가능하고 교정 가능한 동시에 더 의미 있는 수준의 탐색과 더 많은 표본 효율적인 정책 학습을 제공할 것이라고 의심한다. 모방 학습에서도 여러 작품들이 시범자 간 행동 일관성의 중요성을 보여주었고[55, 56], 우리는 언어 동작을 압축된 행동 공간으로 사용하는 것이 더 일관된 행동으로 이어질 수 있고 따라서 더 표본 효율적인 정책 학습으로 이어질 수 있다고 가정한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do as i can, not as i say: Grounding language in robotic affordances. In _Conference on Robot Learning_, pages 287-318. PMLR, 2023.\n' +
      '* [2] Yuchen Cui, Siddharth Karamcheti, Raj Palleti, Nidhya Shivakumar, Percy Liang, and Dorsa Sadigh. No, to the right: Online language corrections for robotic manipulation via shared autonomy. In _Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction_, HRI \'23, page 93-101, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450399647. doi: 10.1145/3568162.3578623. URL [https://doi.org/10.1145/3568162.3578623](https://doi.org/10.1145/3568162.3578623).\n' +
      '* [3] Pratyusha Sharma, Balakumar Sundaralingam, Valts Blukis, Chris Paxton, Tucker Hermans, Antonio Torralba, Jacob Andreas, and Dieter Fox. Correcting robot plans with natural language feedback. _ArXiv_, abs/2204.05186, 2022. URL [https://api.semanticscholar.org/CorpusID:248085271](https://api.semanticscholar.org/CorpusID:248085271).\n' +
      '* [4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikat Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In _arXiv preprint arXiv:2307.15818_, 2023.\n' +
      '* [5] Ajay Mandlekar, Danfei Xu, Roberto Martin-Martin, Yuke Zhu, Li Fei-Fei, and Silvio Savarese. Human-in-the-loop imitation learning using remote teleoperation. _CoRR_, abs/2012.06733, 2020. URL [https://arxiv.org/abs/2012.06733](https://arxiv.org/abs/2012.06733).\n' +
      '* [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. _arXiv preprint arXiv:2212.06817_, 2022.\n' +
      '* [7] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. BC-z: Zero-shot task generalization with robotic imitation learning. In _5th Annual Conference on Robot Learning_, 2021. URL [https://openreview.net/forum?id=8kbp23tSGYv](https://openreview.net/forum?id=8kbp23tSGYv).\n' +
      '* [8] Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, and Heni Ben Amor. Language-conditioned imitation learning for robot manipulation tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 13139-13150. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/9909794d52985cbc5d95c26e31125d1a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/9909794d52985cbc5d95c26e31125d1a-Paper.pdf).\n' +
      '* [9] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In _Proceedings of the 5th Conference on Robot Learning (CoRL)_, 2021.\n' +
      '* [10] Oier Mees, Lukas Hermann, and Wolfram Burgard. What matters in language conditioned robotic imitation learning over unstructured data. _IEEE Robotics and Automation Letters_, 7(4):11205-11212, 2022.\n' +
      '* [11] Priya Sundaresan, Suneel Belkhale, Dorsa Sadigh, and Jeannette Bohg. KITE: Keypoint-conditioned policies for semantic manipulation. In _7th Annual Conference on Robot Learning_, 2023. URL [https://openreview.net/forum?id=veGdf4L4Xz](https://openreview.net/forum?id=veGdf4L4Xz).\n' +
      '* [12] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR, 18-24 Jul 2021. URL [https://proceedings.mlr.press/v139/radford21a.html](https://proceedings.mlr.press/v139/radford21a.html).\n' +
      '* Nair et al. [2023] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot manipulation. In _Conference on Robot Learning_, pages 892-909. PMLR, 2023.\n' +
      '* Karamcheti et al. [2022] Siddharth Karamcheti, Suraj Nair, Annie S. Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy Liang. Language-driven representation learning for robotics. In _Robotics: Science and Systems (RSS)_, 2023.\n' +
      '* Ma et al. [2022] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* Driess et al. [2022] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In _arXiv preprint arXiv:2303.03378_, 2023.\n' +
      '* Konidaris et al. [2012] George Konidaris, Scott Kuindersma, Roderic Grupen, and Andrew Barto. Robot learning from demonstration by constructing skill trees. _The International Journal of Robotics Research_, 31(3):360-375, 2012. doi: 10.1177/0278364911428653. URL [https://doi.org/10.1177/0278364911428653](https://doi.org/10.1177/0278364911428653).\n' +
      '* Niekum et al. [2012] Scott Niekum, Sarah Osentoski, George Konidaris, and Andrew G. Barto. Learning and generalization of complex tasks from unstructured demonstrations. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 5239-5246, 2012. doi: 10.1109/IROS.2012.6386006.\n' +
      '* Krishnan et al. [2017] Sanjay Krishnan, Roy Fox, Ion Stoica, and Ken Goldberg. Ddco: Discovery of deep continuous options for robot learning from demonstrations. In _Conference on robot learning_, pages 418-437. PMLR, 2017.\n' +
      '* Shankar and Gupta[2020] Tanmay Shankar and Abhinav Gupta. 시간 가변 추론을 통해 로봇 기술을 학습합니다. _International Conference on Machine Learning_, pages 8624-8633. PMLR, 2020.\n' +
      '* Kipf et al. [2019] Thomas Kipf, Yujia Li, Hanjun Dai, Vinicius Zambaldi, Alvaro Sanchez-Gonzalez, Edward Grefenstette, Pushmeet Kohli, and Peter Battaglia. Compile: Compositional imitation learning and execution. In _International Conference on Machine Learning_, pages 3418-3428. PMLR, 2019.\n' +
      '* Shankar et al. [2020] Tanmay Shankar, Shubham Tulsiani, Lerrel Pinto, and Abhinav Gupta. Discovering motor programs by re-composing demonstrations. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/forum?id=rkgHY0NYwr](https://openreview.net/forum?id=rkgHY0NYwr).\n' +
      '* Tameberg et al. [2021] Daniel Tameberg, Kai Ploeger, Elmar Rueckert, and Jan Peters. Skid raw: Skill discovery from raw trajectories. _IEEE robotics and automation letters_, 6(3):4696-4703, 2021.\n' +
      '* Zhu et al. [2022] Yifeng Zhu, Peter Stone, and Yuke Zhu. Bottom-up skill discovery from unsegmented demonstrations for long-horizon robot manipulation. _IEEE Robotics and Automation Letters_, 7(2):4126-4133, 2022.\n' +
      '* Hakhamaneshi et al. [2021] Kourosh Hakhamaneshi, Ruihan Zhao, Albert Zhan, Pieter Abbeel, and Michael Laskin. Hierarchical few-shot imitation with skill transition models. In _International Conference on Learning Representations_, 2021.\n' +
      '* Wang et al. [2017] Ziyu Wang, Josh S Merel, Scott E Reed, Nando de Freitas, Gregory Wayne, and Nicolas Heess. Robust imitation of diverse behaviors. _Advances in Neural Information Processing Systems_, 30, 2017.\n' +
      '* Lynch et al. [2020] Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet. Learning latent plans from play. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura, editors, _Proceedings of the Conference on Robot Learning_, volume 100 of _Proceedings of Machine Learning Research_, pages 1113-1132. PMLR, 30 Oct-01 Nov 2020. URL [https://proceedings.mlr.press/v100/lyncha0a.html](https://proceedings.mlr.press/v100/lyncha0a.html).\n' +
      '* 벨칼드와 새디[2022] 수닐 벨칼드와 도사 새디. PLATO: 객체 중심 놀이를 통해 잠재된 여력을 예측하는 것. _6th Annual Conference on Robot Learning_, 2022. URL[https://openreview.net/forum?id=UAASbNospA0](https://openreview.net/forum?id=UAASbNospA0).\n' +
      '* 존스[2021] 에드워드 존스. 거친 모방 학습: 한 번의 시연으로 로봇을 조작합니다. _2021 IEEE 국제 회의 on robotics and automation (ICRA)_, pages 4613-4619. IEEE, 2021.\n' +
      '* Belkhale et al. [2023] Suneel Belkhale, Yuchen Cui, and Dorsa Sadigh. Hydra: Hybrid robot actions for imitation learning. In _Conference on Robot Learning_, pages 2113-2133. PMLR, 2023.\n' +
      '* Huang et al. [2023] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. In _Conference on Robot Learning_, pages 1769-1782. PMLR, 2023.\n' +
      '* Sermanet et al. [2021] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil J Joshi, Pete Florence, Wei Han, Robert Baruch, Yao Lu, Suvir Mirchandani, Peng Xu, Pannag Sanketi, Karol Hausman, Izhak Shafran, Brian Ichter, and Yuan Cao. Robovqa: Multimodal long-horizon reasoning for robotics. In _arXiv preprint arXiv:2311.00899_, 2023.\n' +
      '* Mirchandani et al. [2021] Suvir Mirchandani, Siddharth Karamcheti, and Dorsa Sadigh. ELLA: Exploration through learned language abstraction. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL [https://doi.org/10.1007/s10054-021-0021-0213-0](https://doi.org/10.1007/s10054-021-0021-0213-0).\n' +
      '\n' +
      'openreview.net/forum?id=VVUldGZ3lrR.\n' +
      '* [34] Joey Hejna, Pieter Abbeel, and Lerrel Pinto. Improving long-horizon imitation through instruction prediction. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 7857-7865, 2023.\n' +
      '* [35] Shengran Hu and Jeff Clune. Thought Cloning: Learning to think while acting by imitating human thinking. _Advances in Neural Information Processing Systems_, 2023.\n' +
      '* [36] Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. Skill induction and planning with latent language. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1713-1726, 2022.\n' +
      '* [37] Xinjie Liu. Interactive imitation learning in robotics based on simulations, 2022.\n' +
      '* [38] Stephane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. No-regret reductions for imitation learning and structured prediction. _CoRR_, abs/1011.0686, 2010. URL [http://arxiv.org/abs/1011.0686](http://arxiv.org/abs/1011.0686).\n' +
      '* [39] Michael Kelly, Chelsea Sidrane, Katherine Driggs-Campbell, and Mykel J Kochenderfer. Hg-daggerger: Interactive imitation learning with human experts. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 8077-8083. IEEE, 2019.\n' +
      '* [40] Ryan Hoque, Ashwin Balakrishna, Ellen Novoseller, Albert Wilcox, Daniel S Brown, and Ken Goldberg. Thrifydagger: Budget-aware novelty and risk gating for interactive imitation learning. In _Conference on Robot Learning_, pages 598-608. PMLR, 2022.\n' +
      '* [41] Ryan Hoque, Ashwin Balakrishna, Carl Putterman, Michael Luo, Daniel S. Brown, Daniel Seita, Brijen Thananjeyan, Ellen R. Novoseller, and Ken Goldberg. Lazydagger: Reducing context switching in interactive imitation learning. In _CASE_, pages 502-509, 2021. URL [https://doi.org/10.1109/CASE49439.2021.9551469](https://doi.org/10.1109/CASE49439.2021.9551469).\n' +
      '* [42] Jiakai Zhang and Kyunghyun Cho. Query-efficient imitation learning for end-to-end simulated driving. In _Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence_, AAAI\'17, page 2891-2897. AAAI Press, 2017.\n' +
      '* [43] Kunal Menda, Katherine Driggs-Campbell, and Mykel J. Kochenderfer. Ensembledagger: A bayesian approach to safe imitation learning. In _2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 5041-5048, 2019. doi: 10.1109/IROS40897.2019.8968287.\n' +
      '* [44] Mengxi Li, Alper Canberk, Dylan P Losey, and Dorsa Sadigh. Learning human objectives from sequences of physical corrections. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 2877-2883. IEEE, 2021.\n' +
      '* [45] Dylan P Losey, Andrea Bajcsy, Marcia K O\'Malley, and Anca D Dragan. Physical interaction as communication: Learning robot objectives online from human corrections. _The International Journal of Robotics Research_, 41(1):20-44, 2022.\n' +
      '* [46] Lihan Zha, Yuchen Cui, Li-Heng Lin, Minae Kwon, Monserrat Gonzalez Arenas, Andy Zeng, Fei Xia, and Dorsa Sadigh. Distilling and retrieving generalizable knowledge for robot manipulation via language corrections. In _2nd Workshop on Language and Robot Learning: Language as Grounding_, 2023.\n' +
      '* [47] Alexander Broad, Jacob Arkin, Nathan Ratliff, Thomas Howard, and Brenna Argall. Real-time natural language corrections for assistive robotic manipulators. _The International Journal of Robotics Research_, 36(5-7):684-698, 2017.\n' +
      '* [48] Arthur Bucker, Luis Figueredo, Sami Haddadini, Ashish Kapoor, Shuang Ma, and Rogerio Bonatti. Reshaping robot trajectories using natural language commands: A study of multi-modal data alignment using transformers. In _2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 978-984. IEEE, 2022.\n' +
      '* [49] Arthur Bucker, Luis Figueredo, Sami Haddadin, Ashish Kapoor, Shuang Ma, Sai Vemprala, and Rogerio Bonatti. Latte: Language trajectory transformer, 2022.\n' +
      '* [50] John D Co-Reyes, Abhishek Gupta, Suvansh Sanjeev, Nick Altieri, Jacob Andreas, John DeNero, Pieter Abbeel, and Sergey Levine. Guiding policies with language via meta-learning. In _International Conference on Learning Representations_, 2018.\n' +
      '* [51] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. _IEEE Robotics and Automation Letters_, pages 1-8, 2023. doi: 10.1109/LRA.2023.3295255.\n' +
      '* [52] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual vision and language model. _arXiv preprint arXiv:2305.18565_, 2023.\n' +
      '* [53] S. Lloyd. Least squares quantization in pcm. _IEEE Transactions on Information Theory_, 28(2):129-137, 1982. doi: 10.1109/TIT.1982.1056489.\n' +
      '* [54] Open X-Emboliment Collaboration, Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, Antonin Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Scholkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Giulio Schiavi, Hao Su, Hao-Shu Fang, Haochen Shi, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walker, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jaehyung Kim, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Joao Silverio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Keyvan Majd, Krishan Rana, Krishnan Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag R Sanketi, Paul Wohlhart, Peng Xu, Pierre Sermanet, Priya Sundaresan, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martin-Martin, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shuran Song, Sichun Xu, Siddhant Haldar, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh Hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, and Zichen Jeff Cui. Open X-Embodiment: Robotic learning datasets and RT-X models. [https://arxiv.org/abs/2310.08864](https://arxiv.org/abs/2310.08864), 2023.\n' +
      '* [55] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Martin-Martin. What matters in learning from offline human demonstrations for robot manipulation. In _5th Annual Conference on Robot Learning_, 2021. URL [https://openreview.net/forum?id=JrsfBJtDFdI](https://openreview.net/forum?id=JrsfBJtDFdI).\n' +
      '* [56] Suneel Belkhale, Yuchen Cui, and Dorsa Sadigh. Data quality in imitation learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL [https://openreview.net/forum?id=FwmvbDiMk](https://openreview.net/forum?id=FwmvbDiMk).\n' +
      '\n' +
      '## Appendix A\n' +
      '\n' +
      '우리는 먼저 교육 레시피와 함께 부록 A의 RT-H 및 어블레이션의 구현에 대해 개요를 설명한다. 그리고 부록 B에서 수정된 방법을 이용한 방법들에 대한 구현 및 훈련 프로토콜에 대해 논의한다. 이후, 부록 C에서 사용된 데이터 세트 각각에 대해 상세히 설명한다. 마지막으로, 부록 D에서 각 방법에 대한 각 태스크의 상이한 단계에 대한 성공률, RT-H의 문맥성 정량적 분석, RT-H의 언어 모션 멀티모달리티의 정성적 분석을 포함하여 보다 상세한 결과를 보여준다.\n' +
      '\n' +
      '### _Method Implementations_\n' +
      '\n' +
      '섹션 III에 설명된 바와 같이, RT-H 및 RT-2 [4]는 Pali-X 55B 멀티모달 인코더 디코더 트랜스포머 아키텍처[52]를 사용하여 구현된다. 이미지는 Pali-X 사전 훈련 단계에서 학습되지만 로봇 시연 데이터 공동 훈련 동안 고정된 22B ViT 아키텍처를 사용하여 인코딩된다. 인코딩된 이미지 및 프롬프트는 인코더를 통과하고, 각 쿼리에 대한 출력은 디코더로 자동으로 디코딩된다. 다음으로 각 방법(오프라인 전용)에 대해 자세히 설명합니다.\n' +
      '\n' +
      '**RT-H**: RT-H는 먼저 다음의 **언어 동작 쿼리를 사용하여 작업으로부터 언어 동작을 예측한다**: _Q: 로봇이 [task]?A:_에 어떤 스킬을 해야 하는가? 여기서 작업은 언어로 지정되고, 그 결과 언어 동작(스킬)은 언어로 반환된다. 그런 다음 예측된 언어 모션을 사용하여 다음 ** 액션 쿼리를 사용하여 액션 예측을 더 잘 알려줍니다**: _Q: 현재 스킬: [모션]?A:_로 로봇이 [작업]에 무엇을 해야 하는가? 여기서 출력은 토큰화된 액션 문자열입니다. RT-H 열차는 RT-2[4]와 같은 방식으로 운행된다. 먼저, RT-2와 동일한 대형 비전 언어 데이터셋에서 RT-H를 사전 훈련한 후, 전체 훈련 샘플의 50%(각 25%)를 사용하여 언어 동작 및 동작 쿼리를 공동 훈련하고, 원래의 사전 훈련 혼합물을 사용하여 50%를 훈련한다. RT-2와 유사하게 1e-3의 학습 속도와 일정한 선형 워밍업 및 제곱근 정규화 붕괴, 배치 크기 1024를 사용한다.\n' +
      '\n' +
      '**RT-H-Joint**: RT-H와 달리 RT-H-Joint는 하나의 쿼리를 사용하여 언어 동작과 동작을 모두 예측한다. 두 방법 모두 언어 모션에 대해 자기 회귀적이지만 RT-H는 언어 모션 또는 액션 예측을 나타내기 위해 서로 다른 단어를 사용하는 두 개의 쿼리를 가지며 RT-H는 프롬프트 문자열의 일부이기 때문에 액션 쿼리를 위해 언어 모션에서 인코더로 전달한다. RT-H-Joint에 대한 프롬프트는 다음과 같다: _Q: 로봇이 [작업]?A:_에 대해 어떤 기술과 동작을 수행해야 하는가? 그러면 출력은 언어에서 제1 언어 모션(스킬)의 연결이고, 그 다음에 토큰화된 액션 스트링은, _skill:[skill], action:[action]_의 형태로 된다. RT-H-Joint는 RT-2 및 RT-H와 동일하게 훈련되지만 RT-2로부터 액션 예측 대신 조인트 언어 모션 및 액션 쿼리로 훈련된다.\n' +
      '\n' +
      '**RT-H-Cluster**: RT-H-Cluster는 RT-H와 동일한 두 질의 절차 및 훈련 구현을 따른다. 액션 클러스터를 결정하기 위해 먼저 데이터 세트 통계를 사용하여 데이터 세트에서 액션을 정규화한다. 그런 다음 256개의 클러스터 센터를 사용하여 K-means[53]을 사용하여 액션을 클러스터링한다. 우리는 자동화된 라벨링 절차에서 데이터 세트에서 활발하게 사용되는 언어 모션의 수와 동일하도록 이 수를 선택했다. 그런 다음 클러스터 중심을 0에서 255까지의 정수로 대체하고 액션 계층에서 언어 동작을 대신하여 사용한다. 이 삭제는 데이터 세트의 특정 액션에 조정된 임베딩과 비교하여 언어 모션의 유용성을 테스트한다.\n' +
      '\n' +
      '**RT-H-OneHot**: RT-H-OneHot 또한 RT-H와 동일한 두 질의 절차 및 훈련 구현을 따른다. 유일한 변화는 고유한 언어 동작을 정수로 대체하는 것이다. 우리는 먼저 스킬이 얼마나 흔한지 순서대로 열거한 다음 각각에 고유한 정수 값을 할당한다. 중요하게도, 이 공식화는 언어의 고유한 구조를 포착하지 못하는데, 예를 들어, "팔을 앞으로 이동" 및 "팔을 앞으로 이동 및 왼쪽으로 이동"은 여러 면에서 유사하고 이와 같이 취급되어야 하지만, 이들의 대체된 원-핫 라벨은 임의의 다른 두 랜덤 언어 동작만큼 등거리일 가능성이 높다. 따라서 RT-H-OneHot는 언어 동작을 예측할 때 언어의 구조의 중요성을 테스트한다.\n' +
      '\n' +
      '### _Corrections_\n' +
      '\n' +
      '섹션 IV-A에 설명된 바와 같이 RT-H는 인간이 새로운 언어 동작으로 개입할 수 있게 하고, 그 후 이러한 수정은 로봇에 전개될 수 있다. 보정에 대한 RT-H를 훈련하기 위해, 언어 동작 쿼리로부터 추론된 언어 동작 대신에 액션 쿼리에 직접 전달될 언어 동작 수정을 직접 입력하거나 말할 수 있다. 이는 액션 계층에서 수정에 대한 부담을 액션에서 언어 모션으로 한 단계 위로 이동시킨다. 언어 동작 보정 데이터 세트를 수집하고, 관찰, 작업 및 언어 동작 보정을 기록한 다음, 원래 사전 훈련 데이터 세트, 로봇 시연 데이터 세트 및 가중 언어 동작 보정과 함께 모델을 공동 훈련한다. 이러한 대규모 데모 데이터세트의 경우 각 보정 샘플이 해당 데모 데이터세트 예만큼 자주 50x로 표시되는 것을 목표로 한다. 따라서 _Diverse+Kitchen_ 데이터세트에 대한 트레이닝 동안 샘플링 가중치는 다음과 같다:\n' +
      '\n' +
      '* 사전 훈련 질의: 50%\n' +
      '* 시연 데이터 언어 동작 질의: 23%\n' +
      '* 실증 데이터 액션 질의: 23%\n' +
      '* 보정 데이터 언어 모션 질의: 4%\n' +
      '\n' +
      '언어 모션 보정 데이터세트 크기에 대한 시연 데이터세트 크기의 비율이 대략 300:1이라는 점을 감안할 때, 이는 각 언어 모션 보정 샘플을 50:1만큼 가중하는 것에 대응한다.\n' +
      '\n' +
      '우리는 RT-2-IWR로 원격 조작 수정에서 교육을 위해 동일한 레시피를 사용한다. 유일한 차이점은 언어 모션 트레이닝 쿼리가 IWR에서와 같은 액션 쿼리로 대체된다는 것이다.\n' +
      '\n' +
      '### _Datasets_\n' +
      '\n' +
      '본 연구에서는 RT-1[6]과 RT-2[4]의 _Kitchen_ dataset과 새로운 _Diverse+Kitchen_ dataset(확장 버전 _Kitchen_)의 두 가지 데이터 세트를 사용한다. 키친_은 70K 시연**에서 평가에 사용되는 **6개의 의미 태스크로 구성되며, 캔, 병 및 과일과 같은 여러 공통 객체 범주에 걸쳐 542개의 고유한 명령어를 형성한다. 상기 시맨틱 태스크 명령어들은 다음과 같다:\n' +
      '\n' +
      '**knock over**: Knock Object Over.\n' +
      '**서랍 장소**: 서랍에서 물건을 골라 카운터 위에 놓으세요.\n' +
      '**move**: 객체 가까이 이동\n' +
      '오브젝트 선택\n' +
      '**열림/닫힘 서랍** : [열림/닫힘] [상/중/하] 서랍.\n' +
      '직립 위치로: 물체를 위로 올려 놓습니다.\n' +
      '\n' +
      '_Diverse+Kitchen_는 _Kitchen_로부터의 모든 시연으로 구성되지만, 단지 30K 추가 시연**에서 **24개의 더 많은 시맨틱 태스크가 있으며, 165개의 고유 명령어가 있다. 데이터세트 내의 새로운 작업 명령어는 빈도별로(대부분에서 가장 적게) 정렬된 다음과 같다:\n' +
      '\n' +
      '* 냅킨을 디스펜서에서 꺼내어 냅킨을 카운터 위에 평평하게 놓음\n' +
      '** 냅킨을 디스펜서에서 꺼냅니다**\n' +
      '그릇을 고르고 그릇을 카운터 위에 똑바로 놓으세요\n' +
      '** 카운터에서 뚜껑을 사용하여 피스타치오가 들어 있는 큰 유리병을 닫음\n' +
      '* 컵을 따서 컵을 카운터 위에 세워 놓는다\n' +
      '피스타치오로 큰 유리병을 열어요\n' +
      '스쿠퍼를 잡아요\n' +
      '* 바스켓에서 스쿱을 줍기\n' +
      '* 피스타치오로 큰 유리병을 열고 뚜껑을 카운터 위에 놓는다\n' +
      '* 바구니 안에 스쿱을 놓습니다\n' +
      '시리얼 디스펜서 스파우트 밑에 그릇을 놓으세요\n' +
      '주둥이 밑으로 그릇을 치워\n' +
      '오트밀 봉지를 골라 그릇에 오트밀 봉지를 넣는다\n' +
      '* 숟가락을 집어서 시리얼과 함께 그릇에 숟가락을 놓다\n' +
      '* 그릇이 반쯤 채워질 때까지 시리얼 디스펜서를 돌려라\n' +
      '* 바구니에서 집게를 집는다\n' +
      '* 집게를 바구니에 넣음\n' +
      '* 상기 스쿱으로부터 상기 컵에 상기 스쿱을 붓는 단계\n' +
      '* 항아리에서 간식을 떠내다\n' +
      '*픽 오브젝트\n' +
      '* 객체 근처의 이동 객체\n' +
      '* 물건을 때려 눕히다\n' +
      '물체를 똑바로 세우다.\n' +
      '꿀을 그릇에 짜넣다\n' +
      '\n' +
      '이것은 종종 작업 간에 엄청난 데이터 불균형이 있는 로봇의 다양한 행동 범위를 나타낸다. 이 데이터 세트에는 전체 데이터의 작은 부분을 구성하지만 노크 오버, 이동, 선택 및 배치 작업에 대한 추가 시연이 있습니다. V-A절과 V-C절에서 평가에 사용된 작업은 볼드체로 되어 있다.\n' +
      '\n' +
      '### _Detailed Results_\n' +
      '\n' +
      '**다양한 평가**: 다음으로 섹션 V-A의 _Diverse+Kitchen_ 평가에서 각 작업의 여러 단계에 대한 누적 성공률을 보여준다. 도. 도 9는 "위치볼 업라이트" 태스크를 나타내며, RT-H와 RT-H-Joint는 50%의 시간(RT-2의 경우 20%에 비해)을 그릇을 픽업할 수 있지만 RT-H는 이후에 그릇을 회전시키기 위해 고군분투한다. 도. 도 10은 "Open Pistachio Jar" 태스크를 나타내며, 여기서 우리는 액션 계층을 갖는 메소드가 이 태스크 상에서 RT-2보다 실질적으로 더 멀리 도달하는 것을 본다. 도. 도 11은 "Close Pistachio Jar" 태스크를 나타내며, 여기서 다시 한번 RT-2는 액션 계층을 갖는 방법들에 비해 정확한 거동을 거의 나타내지 않는다. 따라서 모든 방법에 대한 성공률은 개방 및 폐쇄 항아리 작업에서 상당히 낮지만 RT-H 및 그 변형이 훨씬 더 멀리 진행될 수 있음을 알 수 있다. 도. 도 12는 "무브 볼 어웨이(Move Bowl Away)" 태스크를 나타낸 것으로, RT-2보다 액션 위계를 갖는 방법이 훨씬 더 멀어지는 것을 다시 한 번 볼 수 있다. 여기서, 파지 성공률이 높은 RT-H-Joint에 비해 RT-H가 그릇의 얇은 테두리를 파지하는 데 어려움을 겪는 것을 알 수 있다. 도. 도 13은 "풋볼 언더" 태스크를 보여주는데, 여기서 RT-H 및 다른 액션 계층 방식들은 태스크의 각 단계에서 더 잘 수행되고, RT-H가 가장 높은 최종 성공률을 얻는다. 도. 도 14는 "오트밀을 그릇에 담아라" 작업을 나타내며 RT-H 및 RT-H-Joint는 RT-2, RT-H-OneHot 및 Fig.에 비해 작업에서 훨씬 더 멀리 도달한다. 도 15는 "Grab Scooper" 태스크를 나타낸 것으로, 일부 액션 계층 방식(RT-H-Cluster 및 RT-H-OneHot)보다 RT-2가 더 우수한 몇 안 되는 태스크 중 하나이지만, RT-H는 태스크의 모든 단계에서 RT-2보다 우수하다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 16에서는 "냅킨 꺼내기" 과제를 보여주며, RT-H, RT-H-Joint, RT-2 모두 매우 높은 성공률을 보인다.\n' +
      '\n' +
      '전체적으로 볼 때, 수정이 필요한 작업은 한두 단계에 불과한 경우가 많다는 것을 알 수 있다. 이것은 종종 몇 가지 언어 수정만 필요로 하며, 이는 RT-H-Intervene가 왜 그렇게 적은 수의 새로운 데이터로 태스크 성능을 향상시킬 수 있는지에 대한 통찰력을 제공한다.\n' +
      '\n' +
      '**일반화**: 다음으로 섹션 V-D 및 표 II에 표시된 것처럼 RT-H를 새로운 객체로 일반화하기 위한 단계적 누적 성공률을 보여준다. 도. 도 17은 픽 태스크 및 도 1을 나타낸다. 도 18은 이동 태스크를 나타내며, 두 태스크 모두에서 RT-H가 최종 성공률뿐만 아니라 각 태스크의 각 개별 단계에서도 더 우수함을 알 수 있다.\n' +
      '\n' +
      '**맥락성**: RT-H의 맥락성을 정량적으로 강조하기 위해, 우리는 동일한 언어 동작 그룹에 속하는 동작에 대한 각 동작 차원의 평균(및 표준 편차)을 표 III에 계산한다. 우리는 이러한 통계를 계산하기 위해 _Diverse+Kitchen_ 데이터 세트의 검증 세트(섹션 III-C로부터의 자동화된 언어 모션 라벨링 절차를 사용)를 사용한다. 각 언어 동작에 대한 지배적인 동작 차원이 가장 큰 평균 및 동작 분산(표 III에서 볼드)을 가지더라도 다른 동작 차원도 자명하지 않은 분산을 가짐을 발견하여 각 언어 동작에 대한 해석이 장면과 작업에 따라 변경됨을 시사한다. 즉, 언어 모션을 액션(액션 쿼리)으로 번역하는 것은 문맥적인 과정이다. 때때로, 비-지배적 액션 축의 평균은 또한 자명하지 않다(예를 들어, 회전 암 우측의 경우, 암은 일부 암(x) 및 (y) 바이어스를 갖는데, 이는 데이터세트에서 선택된 태스크 세트로부터의 바이어스로 인한 것일 가능성이 높다).\n' +
      '\n' +
      '**멀티모달리티**: 다음으로 언어 동작 추상화를 통해 RT-H가 각 단계에서 올바른 언어 동작뿐만 아니라 동일한 작업을 수행하는 다양한 방법을 학습할 수 있는지 연구한다. 이를 정성적으로 분석하기 위해 오프라인 유효성 검사 데이터에 언어 모션 쿼리를 실행합니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:18]\n' +
      '\n' +
      '도 14: 보울에 오트밀 패킷을 배치: 각 방법에 대한 누적 성공률.\n' +
      '\n' +
      '그림 12: 시리얼 디스펜서에서 그릇을 멀리 이동: 각 방법에 대한 누적 성공률.\n' +
      '\n' +
      '그림 13: 그릇을 시리얼 디스펜서에 넣기: 각 방법에 대한 누적 성공률.\n' +
      '\n' +
      '그림 15: 스쿠퍼 잡기: 각 방법에 대한 누적 성공률.\n' +
      '\n' +
      '그림 16: **냅킨을 디스펜서에서 꺼내기: 각 방법에 대한 누적 성공률**\n' +
      '\n' +
      '도 17: **Pick(신규 객체): RT-2 및 RT-H에 대한 누적 성공률. RT-H는 RT-2에 비해 최종 성공률이 높을 뿐만 아니라 작업의 각 단계에서 성공한다.**\n' +
      '\n' +
      '도 18: **Move(신규 객체): RT-2 및 RT-H에 대한 누적 성공률. RT-H는 RT-2에 비해 최종 성공률이 높을 뿐만 아니라 작업의 각 단계에서 성공한다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c||c c c|c c c c|c} Language Motion & arm (x) & arm (y) & arm (z) & arm (rx) & arm (ry) & arm (rz) & gripper \\\\ \\hline _move arm forward_ & **7.7 (6.2)** & -0.5 (3.3) & -1.4 (3.2) & 3.4 (10.9) & -1.1 (8.9) & -4.0 (10.3) & 1.0 (3.3) \\\\ _move arm backward_ & **-10.9 (10.1)** & -2.4 (4.6) & -2.3 (5.2) & 0.5 (9.2) & 7.8 (11.6) & 0.0 (13.1) & 0.5 (6.6) \\\\ _move arm left_ & -0.4 (3.2) & **8.2 (6.3)** & -2.1 (3.4) & 6.1 (12.0) & 2.5 (7.7) & 7.9 (11.5) & 1.0 (2.1) \\\\ _move arm right_ & -1.4 (4.1) & **-6.9 (7.3)** & -1.5 (4.0) & -1.9 (11.7) & 2.2 (7.7) & -6.6 (11.6) & 1.1 (0.9) \\\\ _move arm up_ & -1.7 (4.6) & -2.1 (4.9) & **11.1 (8.6)** & 1.7 (11.7) & -7.4 (10.9) & 0.5 (10.5) & 0.9 (4.6) \\\\ _move arm down_ & -0.6 (4.0) & -0.3 (3.1) & **-8.1 (9.0)** & 1.4 (10.0) & 5.6 (10.5) & -1.0 (8.2) & 1.1 (3.5) \\\\ _rotate arm right_ & 2.1 (4.6) & 3.0 (5.5) & 0.3 (5.1) & **29.3 (24.5)** & -2.1 (12.2) & 0.0 (13.0) & 1.0 (1.5) \\\\ _rotate arm left_ & 0.4 (4.2) & -0.4 (4.2) & -0.5 (3.9) & **-24.3 (21.9)** & -4.1 (14.8) & 0.9 (10.6) & 1.2 (1.6) \\\\ _rotate arm up_ & 0.0 (4.5) & 0.3 (3.3) & -2.3 (4.4) & 2.5 (12.4) & **20.5 (18.2)** & 1.7 (8.8) & 1.0 (1.4) \\\\ _rotate arm down_ & 0.3 (4.5) & 0.8 (3.8) & 1.9 (5.0) & -7.4 (15.1) & **-28.7 (26.6)** & -0.1 (13.6) & 1.0 (1.6) \\\\ _rotate arm counterclockwise_ & 3.3 (4.3) & -1.1 (4.8) & -0.3 (4.5) & 4.2 (12.7) & -3.1 (12.0) & **-24.1 (21.5)** & 1.1 (3.4) \\\\ _rotate arm clockwise_ & -0.5 (5.4) & 3.1 (5.1) & 0.0 (4.8) & -0.9 (12.3) & -0.1 (11.8) & **24.0 (20.8)** & 0.9 (5.2) \\\\ _open gripper_ & 0.6 (0.9) & 0.7 (1.3) & 0.7 (1.3) & 0.7 (2.7) & 0.6 (2.0) & 0.8 (2.3) & **-67.6 (42.9)** \\\\ _close gripper_ & 0.7 (1.8) & 0.8 (1.3) & 0.8 (2.0) & 0.9 (4.3) & 0.5 (3.8) & 0.9 (3.6) & **65.0 (43.5)** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 검증 세트에 걸쳐 계산된 _Diverse+Kitchen_ 데이터세트의 각 액션 차원(암 델타 x,y,z; 회전 암 델타 x,y,z; 및 그리퍼)에 대한 기본 언어 모션(카디널 방향)에 대한 액션 수단(및 표준 편차)이다. 굵은 숫자는 언어 동작이 가리키는 지배적인 축에 해당한다. 위치와 회전은 서로 일치하도록 조정되지 않습니다. 우리는 지배적인 축이 각 기술(굵은 수)에 대해 가장 큰 평균과 분산을 갖는 반면, 다른 축도 사소한 변동(그러나 종종 0 평균에 가깝음)을 갖는다는 것을 발견했다. 이는 주어진 스킬이 단순히 고정된 프리미티브가 아니라, 실제 상태 및 태스크(즉, 컨텍스트)에 따라 매우 다양한 잠재적 액션들에 매핑된다는 것을 입증한다.\n' +
      '\n' +
      '도 19: 언어 모션 쿼리에 대한 빔 탐색을 사용하는 RT-H에서의 멀티모달 언어 모션 예측의 네 가지 예. 우리는 각 예제에 대한 상위 세 가지 언어 움직임 예측(P1, P2, P3)을 보여준다. (a)와 (b) (상단 행)에서 RT-H는 모두 매우 맥락적인 작업을 수행하는 여러 유효한 방법을 나타낼 수 있음을 알 수 있다. (a)와 (b)에서 출력되는 언어 동작은 서로 상당히 유사하지만, 미묘하지만 과제 관련 방식이 다르다. (c)와 (d)에서 RT-H는 태스크와 장면에 다시 한 번 맥락적으로 태스크를 수행하는 훨씬 더 다양한 방법을 예측한다는 것을 알 수 있다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>