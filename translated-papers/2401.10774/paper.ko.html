<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '멀티코딩 헤드.\n' +
      '\n' +
      '***({}^{5}\\), ***Hongwu Peng**\\({}^{5}\\), ***Hongwu Peng**\\({}^{5}\\), ***d.\n' +
      '\n' +
      '**Jason D.***\\({}^{1}\\), **Tri Dao**\\({}^{1}\\), **Tri Dao**\\({}^{3}\\), **Tri Dao**\\({}^{3,2}\\)입니다.\n' +
      '\n' +
      'IMS({}^{1}\\)는 AI와 함께 일리노이 어바-캠페인,\\({}^{1}\\) 대학교,\\({}^{2}\\)를 융합한다.\n' +
      '\n' +
      '카네기 멜론 대학({}^{4}\\)은 코네티컷 대학교({}^{4}\\,{}^{5}\\)의 대학교이다.\n' +
      '\n' +
      'Equal contribution.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대형 언어 모델(LLM)의 추론 과정은 종종 자동 억제 디코딩 과정에서 병렬주의가 없기 때문에 제한되며, 이로 인해 대부분의 동작이 가속기의 메모리 대역폭에 의해 제한된다. 이 문제를 해결하기 위해 투기 디코딩과 같은 방법이 제안되었지만 별도의 초안 모델을 획득하고 유지하는 것과 관련된 도전에 의해 구현이 방해된다. 본 논문에서는 복수의 후속 토큰을 병렬로 예측하기 위해 추가 디코딩 헤드를 추가하여 LLM 추론을 증가시키는 효율적인 방법인 메두사를 제시한다. 메두사는 _tree 기반 주의 메커니즘_을 사용하여 복수의 후보 연속체를 구성하고 각 디코딩 단계에서 동시에 검증한다. 병렬 처리를 활용함으로써 메두사는 단일 단계 잠복기 측면에서 최소한의 오버헤드만을 도입하고 필요한 디코딩 단계의 수를 실질적으로 감소시킨다.\n' +
      '\n' +
      '우리는 다양한 사용 사례의 요구를 충족시키기 위해 메두사에 대한 두 가지 수준의 미세 조정 절차를 제시한다.\n' +
      '\n' +
      '* 메두사-1: 메두사는 _fro 동결_ 백본 LLM 위에 직접 미세 조정되어 무손실 추론 가속도가 가능하다.\n' +
      '* 메두사-2: 메두사는 백본 LLM과 함께 미세 조정되어 메두사 머리의 더 나은 예측 정확도와 더 높은 스피드를 가능하게 하지만 백본 모델의 능력을 보존하는 특별한 훈련 레시피가 필요하다.\n' +
      '\n' +
      '더욱이, 우리는 발전 품질을 유지하면서 수용률을 높이기 위해 훈련 데이터가 없는 상황과 _typical 수용 방식_을 처리하기 위해 _ 셀프 증류_를 포함한 메두사의 유용성을 개선하거나 확장하는 여러 확장을 제안한다.\n' +
      '\n' +
      '다양한 크기와 훈련 절차의 모델에 대한 메두사를 평가한다. 우리의 실험은 메두사-1이 생성 품질을 손상시키지 않으면서 2.2\\ 이상의 속도업을 달성할 수 있는 반면 메두사-2는 2.3-3.6\\(분절)로 속도를 더욱 향상시킬 수 있음을 보여준다. 이 구현을 위한 코드는 [https://github.com/FasterDec코딩/Medusa] (https://github.com/FasterDec코딩/Medusa)에서 사용할 수 있다.\n' +
      '\n' +
      '###### Contents\n' +
      '\n' +
      '1명 소개\n' +
      '2번 근무.\n' +
      '	2.1LLM\n' +
      '	2.2샘플링 솥.\n' +
      '메날라 3.\n' +
      '	3.1 Key 구성 요소 3.1*\n' +
      '		3.1* 3.1.1.1* 3.1.1.1 메두사 헤드 3.1.1 메두사 헤드 3.1.1 메두사 헤드입니다.\n' +
      '		트리고의 3.1.2* 3.1.2* 3.1.2.2 트리의도 3.1.2* 3.1.2.2 트리의도 3.1.2 트리의도 3.1.2 트리의도 3.1.2 트리고의를 보였다.\n' +
      '	훈련 전략 3.2* 3.2 훈련 전략 3.2 훈련 전략 3.2 훈련 전략\n' +
      '		냉동 백골골 3.2.1 메두사-1: 냉동 백골 3.2.1 메두사-1: 냉동 백골 3.2.1 메두사 3.2.1 메두사-1: 냉동 백골골 3.2.1* 3.2.1 메두사-1: 냉동 백골 3.2.1 메두사-1: 냉동 화병.\n' +
      '		공동훈련 3.2.2 메두사-2: 합동훈련 3.2.2.2 메두사-2: 공동훈련 3.2.2* 3.2.2 메두사-2: 합동훈련 3.2*3.2.2 메두사-2: 공동훈련\n' +
      '	3.3 연장* 3.3 확장* 3.3 확장* 3.3 확장* 3.3 확장*.\n' +
      '		전형적인 수용도 3.3.1* 3.3.1.1 T 일반적인 수용도 3.3.1* 3.3.1이었다.\n' +
      '		자활-분진 3.3.2 셀프-분진 3.3.2* 3.3.2 셀프-분진 3.3.2 셀프-분진 3.3.2* 3.3.2 셀프-분진 3.3.2 셀프-분진 3.3.2 자성-분진 3.3.2* 3.3.2 셀프-분광* 3.3.3.2 셀프-분동* 3.3.3.2－3.3.2 셀프-분동* 3.3.3.3.3.2 셀프-분동* 3.3.3.3.3.2 셀프-분동* 3.3.3.3.3.2 셀프-분진* 3.3.3.3.3.3.2 셀프-분진* 3.3.3.3.2 셀프-분진* 3.3.3.3.2 셀프-분진* 3.3.3.3.3.2 셀프-분진* 3.3.3.3.2 셀프-분진* 3.3.3.2 셀프-분진* 3.\n' +
      '		최적화된 트리건설 검색 검색은 최적화된 트리건설 3.3.3.3* Optimimized 트리건설을 검색한다.\n' +
      '4개의 실험.\n' +
      '		공유 설정물 4.0.1 및 4.0.1 공유 설정물(4.0.1)은 공유 설정물(4.0.1)이다.\n' +
      '	비쿠나 7B 및 13B에서 메두사-2. 바이쿠나 7B 및 13B.1 사례 연구: 메두사-1 v. 메디사-2. 메디사-1 v.\n' +
      '		1.1.1 실험 설정 4.1* 4.1.1.1 실험 설정 4.1* 4.1.1.1 실험 설정 4.1.1 실험 설정 4.1.1 실험 설정 4.1* 4.1.1 실험 설정 1* 4.1.1 실험 설정 4.1 실험 설정 4.1 실험 설정 4.1* 4.1 실험 설정 4.1 실험 설정 4.1* 4.1.\n' +
      '		결과 4.1.2.2의 결과 4.1.2.2의 결과 4.1.2.2의 결과 4.1.2.2의 결과 4.1.2.2의 결과 4.1.2의 결과 4.1.2의 결과 4.1.2의 결과, 4.1.2의 결과, 4.1.2.2의 결과 4.1.2* 4.1.2.2의 결과 4.1.2.2의 결과 4.1.2.1.2.2.1.2.1.2.1.2의 결과 4.1.2* 4.1.2.1.2.2.1.2.1.2의 결과 4.1.2의 결과 4.1.2* 4.1.2.1.2의 결과 4.1.2.1.2의 결과 4.1.2의 결과 4.1.2의 결과 4.1.2의 결과 4.1.2 및 4.1.2의 결과 4.1.2의 결과 4.1.2의 결과 4.1.2의 결과 4.1.2\n' +
      '	Vicuna-33B 및 Zephyr-7B-7B-7B-7B-4.2 사례 연구: Vicuna-33B 및 Zephyr-33B에서 자체 분리 훈련: 4.2 사례 연구였다.\n' +
      '		2.2.1 실험 설정 4.2.1, 실험 설정 4.2.1* 4.2.1 실험 설정 4.2.1 실험 설정 4.2.1 실험 설정 4.2.1 실험 설정 4.2.1 실험 설정 4.2.1 실험 설정 4.2.1 실험 설정 4.2.1 실험 설정 4.2.1 실험 설정 4.2.\n' +
      '		2.2.2의 결과 4.2.2.2의 결과 4.2.2.2.2의 결과 4.2.2.2* 4.2.2.2의 결과 4.2.2* 4.2.2.2의 결과 4.2.2* 4.2.2.\n' +
      '	4.3 Ablation 연구* 4.3 Ablation 연구* 4.3 Ablation 연구* 4.3 Ablation 연구* 4.3 Ablation 연구* 4.3 Ablation 연구이다.\n' +
      '		트리고의 경과 4.3.1* 4.3.1.3.1 점, 트리표시 4.3.1 점\n' +
      '		일반적인 수용성의 임계치 4.3.2* 4.3.2.2.\n' +
      '		2단계 Fine-튜닝의 효과성 2단계 Fine-튜닝의 효과성 4.3.3.3* 4.3.3.3.\n' +
      '	토론 4.4, 토론 4.4, 토론 4.4, 토론 4.4, 토론 4.4, 토론 4.4, 토론 4.4다.\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      '최근 대규모 언어 모델(LLM)의 발전은 모델 크기가 증가함에 따라 언어 생성의 질이 크게 향상되어 수십억 개의 매개변수(브라운 et al., 2020; Chowdhery et al., 2022; Chowdhery et al., 2022; Hoffmann et al., 2022; OpenAI, 2023; 구글, Touvron et al., 2023)에 도달한다는 것을 보여주었다. 그러나 이러한 성장은 _inference 잠복기_의 증가로 이어졌으며 이는 실제 응용 분야에서 중요한 문제를 제기한다. 시스템 관점에서 LLM 추론은 주로 메모리 결합(Shazeer, 2019, 김 et al., 2023)이며, 산술 계산보다는 가속기의 메모리 대역폭에서 비롯된 주요 레이턴시 병목현상이 있다. 이 병목전은 자동 억제 디코딩의 순차적 특성에 내재되어 있으며, 여기서 각 정방향 패스에는 전체 모델 파라미터가 하이-베이드폭 메모리(HBM)에서 가속기의 캐시로 전달되어야 한다. 단일 토큰만을 생성하는 이 과정은 현대 가속기의 연산 가능성을 과소 활용함으로써 비효율성을 초래한다.\n' +
      '\n' +
      '이를 해결하기 위해, LLM 추론을 가속화하기 위한 하나의 접근법은 디코딩 프로세스의 산술 강도_(총 부동 소수점 연산 대 전체 데이터 이동의 비율)를 증가시키고, _ 복호화 단계_의 수를 감소시키는 것을 포함한다. 이 개념에 따르면 투기 디코딩은 제안(Leviathan et al., 2022; Chen et al., 2023; 샤, 2023; Miao et al., 2023)되었다. 이 방법은 더 작은 초안 모델을 사용하여 각 단계에서 토큰의 시퀀스를 생성한 다음 허용 가능한 지속을 위해 원래 더 큰 모델에 의해 정제된다. 그러나 적절한 초안 모델을 얻는 것은 여전히 어려운 상태로 남아 있으며, 초안 모델을 분산 시스템(Chen et al., 2023)으로 통합할 때 일이 더욱 어려워진다.\n' +
      '\n' +
      '후보 출력을 순차적으로 생성하기 위해 별도의 초안 모델을 사용하는 대신 본 논문에서는 추론(Stern et al, 2018)을 신속화하기 위해 백본 모델 상단에 다중 디코딩 헤드를 사용하는 개념을 재방문하고 정제한다. 우리는 효과적으로 적용될 때 이 기술이 투기 디코딩의 문제를 극복할 수 있어 기존의 LLM 시스템으로의 원활한 통합을 가능하게 한다는 것을 발견했다. 구체적으로, 우리는 다중 토큰을 동시에 예측할 수 있는 추가 디코딩 헤드를 통합하여 LLM 추론을 향상시키는 방법인 메두사를 소개한다. 이 헤드는 _파라미터 효율_ 방식으로 미세 조정되며 기존 모델에 추가할 수 있다. 새로운 모델에 대한 요구 사항이 없는 메두사는 분산 환경에 있는 것을 포함하여 현재 LLM 시스템에 쉽고 자동 통합되어 사용자 친화적인 경험을 보장합니다.\n' +
      '\n' +
      '우리는 두 가지 주요 통찰력으로 메두사를 더욱 향상시킵니다. 먼저, 각 디코딩 단계에서 단일 후보 지속을 생성하는 현재 접근법은 제한된 수용 길이와 계산 자원의 비효율적인 사용으로 이어진다. 이를 해결하기 위해 메두사 헤드를 사용하여 다수의 후보 연속체를 생성하고 주의 마스크에 대한 간단한 조정을 통해 동시에 검증해 줄 것을 제안한다. 둘째, 우리는 투기 디코딩에 사용된 것과 유사한 거부반응 샘플링 방식을 사용하여 원래 모델과 동일한 분포로 반응을 생성할 수 있지만 일반적으로 많은 LLM 응용 분야에서 불필요하다. 또는 메두사 헤드 출력에서 _reasonable_ 후보를 선택하는 _typical 수락_ 체계를 소개합니다. 우리는 온도를 임계값으로 사용하여 원래 모델의 예측에서 편차를 관리하여 거부 표본 추출 방법에 대한 효율적인 대안을 제공한다. 이 접근법은 더 높은 온도에서 속도 감소와 같은 한계를 효과적으로 다룬다.\n' +
      '\n' +
      'LLM을 예측 메두사 헤드로 탑재하기 위해 다양한 시나리오에 맞춘 두 가지 별개의 미세 조정 절차를 제안한다. 제한된 계산 자원을 가진 상황이나 수행에 영향을 미치지 않으면서 메두사를 기존 모델에 통합하는 것이 목적이 있는 경우 메두사-1을 추천한다. 이 방법은 최소한의 메모리를 필요로 하며 고정 백본 모델로 인한 생성 품질을 손상시키지 않으면서 QLoRA(디트리머 등 2023년)에 있는 것과 유사한 양자화 기술로 추가 최적화할 수 있다. 그러나 메두사-1에서는 백본 모델의 전위가 활용되지 않는다. 메두사 머리의 예측 정확도를 높이기 위해 더 미세 조정하여 직접 더 빠른 속도로 이어질 수 있습니다. 따라서 충분한 계산 자원을 가진 시나리오나 베이스 모델에서 직접 슈퍼엔드 파인튜닝(SFT)에 적합한 메두사-2를 소개합니다. 메두사-2의 핵심은 모델의 차세대 예측 능력과 출력 품질을 손상시키지 않으면서 메두사 머리와 백본 모델의 공동 훈련을 가능하게 하는 훈련 프로토콜이다. 모델의 훈련 레시피 및 데이터셋 가용성에 따라 학습 데이터셋을 얻기 위한 다양한 전략을 제안한다. 모델이 공개 데이터셋에 미세 조정되면 메두사에 직접 사용할 수 있습니다. 데이터셋을 사용할 수 없거나 모델이 인간 피드백(RLHF)(Ouyang et al., 2022) 과정으로 강화 학습을 받은 경우 메두사 헤드에 대한 학습 데이터셋을 생성하기 위한 자가 증류 접근법을 제안한다.\n' +
      '\n' +
      '우리의 실험은 LLM이 주로 비쿠나-7B, 13B(공공 데이터셋으로 훈련), 비쿠나-33B(추랑 등 2023), 제피르-7B(개인 데이터셋3으로 훈련)를 포함한 다양한 크기와 훈련 설정 모델에 대해 개인용 2위 테스트 메두사를 위해 현지적으로 주최하는 경우를 대표하며, 이는 감독된 미세 조정과 정렬으로 훈련된다. 메두사는 세대의 품질을 손상시키지 않으면서 다양한 프롬프트 유형에 걸쳐 2.3~3.6배의 속도를 달성할 수 있다.\n' +
      '\n' +
      '발주 2: 메두사가 배치된 추론 환경에서 원활하게 사용될 수 있지만 vLLM(권 등 2023)과 같은 서빙 엔진에 통합하기 위한 추가 공학 노력이 필요하다는 점에 유의하는 것이 중요하다. 저희는 이것을 연구하고 있으며 저희를 돕기 위한 커뮤니티 기여도 환영합니다.\n' +
      '\n' +
      '부츠 3: 저자와 접촉하는 이 버전은 실험이며 비쿠나 7B 및 13B와 약간의 다른 데이터를 사용한다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      'LLM.\n' +
      '\n' +
      '대규모 언어 모델(LLM) 추론의 비효율성은 주로 자가 억제 디코딩 과정의 메모리 결합 특성에 기인한다. 이 문제를 완화시켜 추론 지연과 처리량을 개선하기 위한 몇 가지 방법이 제안되었다. 전통적으로 배치 추론은 일관되었다.\n' +
      '\n' +
      '그림 1: ** 메두사 면담. 미디어는 LLM의 마지막 숨겨진 상태 위에 _multiple 헤드_를 도입하여 여러 후속 토큰을 병렬로 예측할 수 있다(섹션 3.1.1). 훈련 메두사 머리의 경우 원래 모델은 _fro Mult_(중사-1, 섹션 3.2.1)이거나 메두사 헤드와 함께 훈련(중사-2, 섹션 3.2.2)된다. 추론 중에 각 헤드는 지정된 위치에 대해 여러 개의 최상위 예측을 생성한다. 이러한 예측은 후보들로 조립되며, 이는 이후에 _tree 기반 주의_ 메커니즘(섹션 3.1.2)을 사용하여 병렬로 처리된다. 마지막 단계는 후보자를 확인하고 계속을 받아들이는 것이다. 표준 거부 반응 샘플링 방식 외에도 _typical 수락_ 방식(섹션 3.3.1)을 사용하여 합리적인 연속체를 선택할 수 있으며, _장기 수용 후보 프리픽스_는 다음 디코딩 단계에 사용될 것이다. 더 많은 토큰을 동시에 수용함으로써 디코딩 공정의 효율이 향상되어 필요한 디코딩 단계의 수를 감소시킨다.\n' +
      '\n' +
      '산술 강도를 높이고 메모리 결합 한계를 탈출하기 위한 간단한 방법으로 고용자를 고용한다. 그러나 LLM과 함께 모델 파라미터와 키-밸류(KV) 캐시는 모두 상당한 가속 메모리를 소비하여 큰 배치 크기의 활용을 방해한다. 이러한 문제를 해결하기 위한 기존의 방법은 개념적으로 (1) 기억 소모를 완화하고, 이에 의해 메모리 전송 오버헤드를 최소화하고, 더 큰 배치 크기를 가능하게 하고, (2) 디코딩 단계의 수를 미니징하여 지연 시간을 직접 줄일 수 있는 두 가지 주요 범주로 나눌 수 있다.\n' +
      '\n' +
      '멀티-쿼리 주의(Shazeer, 2019) 및 그룹-쿼리 주의(Ainslie et al, 2023)와 같은 KV 캐쉬 방법을 줄이는 것은 KV 캐시를 줄이기 위한 직접적인 접근법을 채택한다. 질의 헤드에 대한 주의 모듈에서 더 적은 키 및 값 헤드를 사용하여 이러한 전략은 KV의 메모리 소비를 실질적으로 절단하여 더 큰 배치 크기와 향상된 가속기 활용(Pope et al, 2022)을 용이하게 한다. 또한 장 등은 가장 중요한 KV 토큰을 선택적으로 유지하여 KV 캐시를 더욱 줄일 것을 제안한다. 시스템 관점에서 권씨(2023)는 KV 캐시의 단편화를 줄이기 위한 페이징 메모리 관리 방식을 소개한다.\n' +
      '\n' +
      '양자화 정량화 기술은 LLM의 기억 소비를 줄이는 데 광범위하게 사용된다. 샤오 등은 이상치를 제거하고 양자화 과정을 단순화하기 위해 액티베이션과 파라미터 간의 리칼링을 적용한다(2023). 검출기 등(2022)은 매트릭스 곱셈을 주로 8비트 및 소수의 16비트 작업으로 분해한다. 탄수화물 et al.(2022)은 3/4 비트로 반복적으로 둥근 무게 컬럼을 반복하는 반면, Lin et al.(2023)는 염기성 가중치를 보호하고 LLM을 3/4 비트로 압축하기 위해 활성화 인식 양자화 방식을 제시한다. 김 등(2023)은 다른 기술들 중에서 중요한 가중치의 작은 부분을 처리하기 위해 희소하고 낮은 고정 패턴을 소개한다.\n' +
      '\n' +
      '누적 탈코딩은 앞서 언급한 방법과 직교하는 접근법으로 투기 디코딩(Leviathan et al, 2022, Chen et al, 2023)은 여러 디코딩 단계를 병렬로 실행하여 필요한 총 단계 수를 줄이는 것을 목표로 한다. 이러한 병렬화는 LLM이 적절한 것으로 집합적으로 평가하고 받아들이는 여러 후속 단어를 추측하기 위해 더 작은 초안 모델을 사용하여 실현된다. 비자율적 생성 문헌(Xiao et al., 2023)과 공진하면서, 이 방법은 앞서 언급한 비효율성을 해결하기 위해 LLM에 특별히 맞춤화된다. 이전 작품과 달리, 우리는 추가 초안 모델을 도입하기보다는 예측하기 위해 원래 모델을 레버리징할 것을 제안한다. 이 접근법은 두 가지 모델을 관리하는 복잡성 없이 기존 시스템에 더 간단하고 원활하게 통합된다. 독립적으로 Miao et al.(2023), 스펙터 및 Re(2023)는 다수의 후보들을 병렬적으로 생성하기 위해 트리 구조화된 주의의 사용을 제안하며, Miao et al.(2023)은 후보자를 제안하기 위해 모델의 앙상블을 사용하는 것을 제안하며, 분광기와 Re(2023)는 모델 초안을 위한 다른 계층성을 추가하라고 옹호한다. 메두사 첫 발매 후 증류(Liu et al., 2023; Z404 et al., 2023)의 관점에서 투기적 복호화를 향상시키는 새로운 작품이 다수 발견되어 모델 훈련 없는 초안(H et al., 2023; Fu et al., 2023)을 만들었다.\n' +
      '\n' +
      '### Sampling Scheme\n' +
      '\n' +
      '대규모 언어 모델(LLM)에서 텍스트를 샘플링한 방식은 생성된 출력의 품질에 유의한 영향을 미칠 수 있다. 최근 연구에 따르면 언어 모델에서 직접 샘플링하면 불부착 또는 비감각적인 결과가 발생할 수 있다(Pillutla et al., 2021; Holtzman et al., 2020). 이 도전에 대한 응답으로 _분계 샘플링_ 제도는 도입(Fan et al., 2018; Basu et al., 2021; Meister et al., 2022; Hewitt et al., 2022; Meister et al., 2023)되었다. 이러한 접근법은 각 디코딩 단계에서 특정 _할러 설정된_에 걸쳐 절단된 분포에 대한 샘플링을 수행하여 고품질 및 다양한 샘플을 생성하는 것을 목표로 한다.\n' +
      '\n' +
      '다른 전략은 이러한 허용된 세트를 다양한 방식으로 정의합니다. 예를 들어, 상위\\(k\\) 샘플링(Fan et al., 2018)은 \\(k\\) 가능성이 가장 높은 단어를 유지하는 반면, 최상위\\(p\\) 샘플링(Holtzman et al., 2020)은 확률의 \\(p\\) 백분율을 차지하는 최소 단어 세트를 통합한다. 전형적인 디코딩(마이스터 등, 2023)으로 알려진 또 다른 방법은 포함을 위한 임계값을 설정하기 위해 예측된 분포의 엔트로피를 사용한다. Hewitt et al.(2022)은 절단 샘플링 기술을 포괄적으로 이해하기 위한 통일된 프레임워크를 제공한다.\n' +
      '\n' +
      '이러한 방법으로부터 영감을 얻으려면 우리의 전형적인 수용 방식은 샘플링 과정에서 즉흥적인 후보를 배제할 수 있는 허용 세트를 정의하는 개념과 일치한다. 그러나 우리는 출력과 언어 모델 분포 사이의 정확한 대응 관계를 주장하지 않기 때문에 분기한다. 이러한 편차를 통해 보다 다양하면서도 고품질의 출력을 용이하게 하여 생성된 텍스트의 무결성을 손상시키지 않으면서 더 큰 효율성을 달성할 수 있다.\n' +
      '\n' +
      '## 3 Medusa\n' +
      '\n' +
      '메두사는 투기 디코딩과 동일한 프레임워크를 따르는데, 각 디코딩 단계는 주로 (1) 후보 생성, (2) 처리 후보, (3) 후보 수용의 세 개의 변전소로 구성된다. 메두사에게 (1)은 메두사 헤드가, (2)는 나무 관심으로 실현되며, 메두사 헤드는 원래 모델 위에 있기 때문에 (2)에서 계산된 로테이션은 다음 디코딩 단계에 대해 변전소(1)에 사용될 수 있다. 최종 단계(3)는 거부 표본 추출(Leviathan et al., 2022; Chen et al., 2023) 또는 전형적인 수용(섹션 3.3.1)에 의해 실현될 수 있다. 전체 파이프라인은 그림 1에 나와 있다.\n' +
      '\n' +
      '본 절에서는 메두사 머리를 포함한 메두사의 핵심 구성 요소, 나무 주의 등을 먼저 소개한다. 그런 다음 다양한 사용 사례의 요구를 충족시키기 위해 메두사에 대한 두 가지 수준의 미세 조정 절차를 제시한다. 마지막으로, 우리는 메두사에 대한 훈련 데이터가 없는 상황을 처리하고 디코딩 과정의 효율성을 향상시키기 위해 자가 증류 및 전형적인 수용을 포함한 메두사에 두 가지 확장을 제안한다.\n' +
      '\n' +
      '### Key Components\n' +
      '\n' +
      '3.1.1.1 메두사 헤드들의############\n' +
      '\n' +
      '투기 디코딩에서 후속 토큰은 보조 초안 모델에 의해 예측된다. 이 초안 모델은 원래 모델이 받아들일 연속체를 생성할 만큼 작지만 효과적이어야 한다. 이러한 요구 사항을 포함하는 것은 어려운 작업이며 기존 접근법(스펙터 및 Re, 2023; Miao et al., 2023)은 종종 별도로 _pre-트레이닝_더 작은 모델에 중점을 둔다. 이러한 사전 훈련 과정은 상당한 추가 계산 자원을 요구한다. 예를 들어, (Miao et al., 2023)에서 보고된 275 NVIDIA A100 GPU 시간을 사용했다. 추가적으로 별도의 사전 훈련은 초안 모델과 원 모델 사이의 분배 이동을 잠재적으로 생성할 수 있으며, 이는 원래 모델이 유리하게 할 수 없는 연속체로 이어질 수 있다. Chen et al.(2023)은 또한 분산 환경에서 여러 모델을 제공하는 복잡성을 강조했다.\n' +
      '\n' +
      'LLM 추론의 가속도를 간소화하고 민주화하기 위해 스턴 등(2018)에서 영감을 얻고 메두사 머리를 소개한다. 이들은 원본 모델의 마지막 숨겨진 상태에 추가된 추가 디코딩 헤드이다. 구체적으로, 위치 \\(t\\)에서 원래 모델의 마지막 숨겨진 상태 \\(h_{t}\\)를 감안할 때 \\(K\\) 디코딩 헤드를 \\(h_{t}\\)에 추가한다. 제 \\(k\\)-제 헤드는 다음 토큰의 \\(t+k+1)\\ 위치에서의 토큰을 예측하는데 사용된다(원어 모델 헤드는 \\(t+1)\\ 위치 예측에 사용된다. (k\\)-th 헤드의 예측은 \\(p_{t}^{(k)}\\로 표시되고, 어휘에 대한 분포를 나타내는 반면, 원본 모델의 예측은 \\(p_{t}^{{(0)}\\로 표시된다. Stern et al.(2018)의 접근에 따라, 우리는 각 머리에 대한 잔류 연결과 함께 단일 층의 사료 포워드 네트워크를 사용한다. 이 간단한 디자인은 만족스러운 성능을 달성하기에 충분하다는 것을 알게 되었습니다. \\(k\\)번째 머리의 정의는 다음과 같다.\n' +
      '\n' +
      '(W_{1}{L}} W_{1}^{{{(k)}\\text{{{(k)}\\text{{{(k)}\\cot h_{t}}^{{{(k)+h_{t}\\cot h_{t}}\\cot h_{t}}\\cot h_{t}}\\cot h_{t}{{{{{{{(k){t}\\cot h_{t}}\\cot h_{t}{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{t}}\\cot h_{t} <{t}}{{{{{{{{{{{{{{{{{{{{{{{{{t}}{t}}{{{{{(k){t}.{t}}{{{{(k){t}<{t}.{t}.{t}.{{(k){t}.{t\n' +
      '\n' +
      '우리는 \\(W_{1}^{(k)}\\)를 원래의 언어 모델 헤드와 동일하게, \\(W_{2}^{(k)}\\)를 0으로 초기화한다. 이것은 메두사 머리의 초기 예측과 원래 모델의 초기 예측을 정렬한다. SiLU 활성화 기능(Elfwing et al, 2017)은 라마 모델(Touvron et al., 2023)에 따라 사용된다.\n' +
      '\n' +
      '메두사 헤드는 초안 모델과 달리 훈련(중사-1) 중 _fro 동결_이 유지되거나 함께 훈련될 수 있는 원래의 백본 모델과 연계하여 훈련된다(메디사-2). 이 방법은 강력한 염기 모델의 학습된 표현을 사용하여 단일 GPU에서도 큰 모델을 미세 조정할 수 있도록 한다. 나아가 메두사 헤드의 분포가 원래 모델의 분포와 정렬되어 분포 이동 문제를 완화시킬 수 있도록 한다. 또한 새로운 헤드는 원어 모델 머리와 유사한 단일 레이어로만 구성되어 있기 때문에 메두사는 서빙 시스템 설계에 복잡성을 추가하지 않으며 분산 설정에 친화적이다. 3.2절 메두사 수장 교육 레시피에 대해 논의하겠습니다.\n' +
      '\n' +
      '3.1.2트리고의####### 3.1.2.2트리고의##########\n' +
      '\n' +
      '메두사 헤드를 통해 후속 \\(K+1\\) 토큰에 대한 확률 예측을 얻는다. 이러한 예측을 통해 후보로서 길이\\(K+1\\) 연속체를 생성할 수 있다. 투기 디코딩 연구[10, 11]는 단일 연속을 후보로 샘플링하는 것을 제안하지만 디코딩 동안 여러 후보를 레버리지는 디코딩 단계 내에서 예상되는 수용 길이를 향상시킬 수 있다. 그럼에도 불구하고 더 많은 후보자들은 또한 계산 요구를 높일 수 있다. 잔액을 짓기 위해 여러 후보를 동시에 처리하기 위해 트리 구조화된 주의 메커니즘을 사용한다. 이러한 주의 메커니즘은 전통적인 인과적 주의 패러다임으로부터 분기된다.\n' +
      '\n' +
      '이 틀 내에서 동일한 지속의 토큰만 역사적 데이터로 간주된다. 그래프 신경망 도메인[23]에서 제안된 바와 같이 그래프 구조를 임베딩 개념으로부터 주목을 받는 영감을 얻었으며, 우리는 그림 2에서 시각화된 트리 구조를 주의 마스크에 통합했으며, 주어진 \\(k\\)-머리, 상위(s_{k}\\) 예측은 후보 형성의 기초가 되며, 여기서 \\(s_{k}\\)는 지정된 하이퍼파라미터이다. 이 후보들은 각 머리의 최상위(s_{k}\\) 예측의 카르티안 제품을 결정함으로써 설정된다. 예를 들어, 그림 2에서 \\(s_{1}=2\\) 및 \\(s_{2}=3\\)로 각 제1 헤드 예측은 제2 헤드의 임의의 예측에 의해 성공될 수 있다. 이는 \\(s_{k}\\) 가지가 \\(k\\)-th 레벨(0\\) 수준에서 존재하는 트리 구조로 이어지며, 실제로 이 \\(0\\) 수준은 독립적으로 샘플링될 수 있는 원본 모델의 언어 모델 헤드의 예측을 위한 것이다. 이 나무 내에서 토큰의 선대만이 역사적 맥락으로 간주되며, 우리의 주의 마스크는 토큰의 선대자에게만 주의를 가하도록 보장한다. 이 마스크를 사용하고 위치 인코딩을 위한 위치 지수를 적절하게 설정함으로써 배치 크기를 확장할 필요 없이 수많은 후보를 동시에 처리할 수 있다. 새로운 토큰의 누적 수는 \\(\\sum_{k=1}^{K}\\prod_{i =1}^{k}s_{i}\\)로 계산된다.\n' +
      '\n' +
      '이 절에서는 카트언 제품을 복용하여 나무 구조를 구성하는 가장 간단하고 규칙적인 방법을 보여준다. 다만, 나무 구조를 보다 정교한 방식으로 구성하고, 서로 다른 머리의 서로 다른 상단 예측의 불균형한 정확도를 활용할 수 있다. 이를 3.3.3절에서 논의하겠습니다.\n' +
      '\n' +
      '그림 2: **Tree 주의를 기울였다. 이 시각화는 여러 후보를 동시에 처리하기 위해 나무 주의를 기울이는 것을 보여준다. 예시된 바와 같이, 제1 메디사 머리의 상위 2차 예측과 제2차로부터의 상위 3차 예측은 총 \\(2차 3=6\\) 후보를 나타낸다. 이들 후보들 각각은 트리 구조 내의 별개의 가지들에 해당한다. 각 토큰이 이전 토큰에만 액세스한다는 것을 보장하기 위해 현재 토큰에서 선행 토큰으로 다시 주의 흐름을 독점적으로 허용하는 주의 마스크를 고안했다. 위치 인코딩에 대한 위치 지수는 이 구조에 따라 조정된다.***\n' +
      '\n' +
      '### Training Strategies\n' +
      '\n' +
      '가장 기본적인 수준에서 우리는 백본 모델을 동결하고 메두사 머리에만 초점을 맞춰 메두사 머리를 훈련시킬 수 있다. 이 접근법은 간단하며 최소한의 계산 자원을 필요로 한다. 그러나 메두사 머리와 함께 백본을 훈련하면 메두사 머리의 정확도를 크게 높일 수 있다. 계산 자원 및 사용 사례의 특정 요구 사항에 따라 메두사 헤드에 대한 두 가지 수준의 훈련 전략을 제안한다.\n' +
      '\n' +
      '본 절에서는 목표 모델의 출력 분포와 일치하는 학습 데이터셋의 가용성을 가정한다. 이것은 타겟 모델의 슈퍼엔드 파인-튜닝(SFT)에 사용되는 데이터세트일 수 있다. 3.3.2절에서 자기 증류 접근법을 사용하여 이러한 데이터 세트의 필요성을 제거하는 방법에 대해 논의할 것이다.\n' +
      '\n' +
      '3.2.1 메두사-1: 냉동 백골골 3.2.1 메두사-1.\n' +
      '\n' +
      '냉동 백본 모형으로 메두사 헤드를 훈련시키기 위해 메두사 헤드의 예측과 지상 진실 사이의 교차 역진 손실을 사용할 수 있다. 구체적으로,\\(k\\)-번 헤드의 손실은 위치 \\(t+k+1\\)에서 \\(y_{t+k+1\\)로서, 지반 진리 토큰(\\mathcal{L}_{k}=-\\log p_{t}}(k)}}(y_{t}^{{(k)은 \\(k\\)에 의해 예측된 토큰 \\(k\\)의 확률을 나타낸다. 우리는 또한 \\(카스칼{L}_{k}\\)가 더 클 때 \\(k\\)가 더 크며, 이는 \\(k\\) 헤드의 예측이 더 클 때 더 불확실하기 때문에 합리적임을 관찰했다. 따라서 다양한 헤드의 손실을 균형을 맞추기 위해 중량 \\(\\lambda_{k}\\)를 \\(\\mathcal{L}_{k}\\)에 추가할 수 있다. 그리고 총 메두사 손실은요.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{Medusa-1}=\\sum_{k=1}^{K}-\\{K}-\\lambda_{k}\\log p_{t}^{{(k)}(y_{t +k+1})\n' +
      '\n' +
      '실제로 우리는 \\(0.8\\)와 같은 상수의 \\(k\\) 전력으로 \\(\\lambda_{k}\\)를 설정하였다. 숨겨진 상태를 제공하기 위해 백본 모델만 사용하기 때문에 메모리 소비를 줄이기 위해 백본 모델의 양자화된 버전을 사용할 수 있습니다. 이는 양자화와 마찬가지로 메두사가 QLoRA(검출기 등, 2023)와 유사한 단일 소비자 GPU에서 큰 모델에 대해 학습될 수 있기 때문에 LLM 추론을 가속화하는 보다 민주화된 방법을 소개한다. 훈련은 60k ShareGPT 샘플을 훈련시키기 위해 단일 NVIDIA A100 PCIE GPU가 있는 비쿠나 7B 모델에서 메두사-1에 대해 몇 시간(예: 5시간)만 소요된다.\n' +
      '\n' +
      '공동연수원 3.2.2 메두사-2: 합동연수원 3.2.2##########\n' +
      '\n' +
      '메두사 머리의 정확도를 더욱 향상시키기 위해, 우리는 백본 모델과 함께 메두사 헤드를 훈련시킬 수 있다. 그러나 이것은 백본 모델의 차세대 예측 능력과 출력 품질을 보존하기 위한 특별한 훈련 레시피가 필요하다. 이를 달성하기 위해 우리는 세 가지 전략을 제안한다.\n' +
      '\n' +
      '*** 결합 손실**: 백본 모델의 차세대 예측 능력을 유지하기 위해 백본 모델 \\(\\mathcal{L}_{\\text{LM}}=-\\log p_{t}^{{(0)}(y_{t+1})\\의 교차 역진 손실을 메두사 손실에 추가해야 한다. 또한 중량 \\(\\lambda_{0}\\)를 추가하여 백본 모델과 메두사 헤드의 손실을 균형을 맞추었다. 따라서 총 손실은 \\[\\mathcal{L}_{\\text{Medusa-2}=\\mathcal{L}_{\\text{L}}_{\\text{LM}}+\\lambda_{0}\\mathcal{L }_{\\text{Medusa-1}]이다. (2)\n' +
      '*** 차분 학습률**: 백본 모델이 이미 잘 훈련되고 메두사 헤드가 더 많은 훈련이 필요하기 때문에 백본 모델의 능력을 보존하면서 메두사 헤드의 더 빠른 융합을 가능하게 하기 위해 별도의 학습률을 사용할 수 있다.\n' +
      '**** 헤드의 평가**: 훈련 초기에 메두사 머리는 손실이 커서 큰 구배로 이어져 백본 모델의 매개변수를 왜곡할 수 있다. 금마르 등의 아이디어(2022년)에 이어 2단계 훈련 과정을 채용할 수 있다. 1단계에서는 백본 모델만 메두사-1로 훈련하고, 2단계에서는 평가 전략과 함께 백본 모델과 메두사 머리를 양성한다. 구체적으로, 우리는 먼저 몇 개의 에포크를 위해 백본 모델을 훈련시킨 다음, 백본 모델과 함께 메두사 머리를 훈련시킨다. 이 간단한 전략 외에도 백본 모델의 손실의 무게 \\(\\lambda_{0}\\)를 점진적으로 증가시켜 보다 정교한 평가 전략을 사용할 수도 있다. 우리는 두 전략 모두 실무에서 잘 작동하고 있다.\n' +
      '\n' +
      '이러한 전략을 종합하면, 백본 모델의 능력을 해치지 않고 백본 모델과 함께 메두사 머리를 훈련시킬 수 있다. 더욱이, 이 레시피는 슈퍼엔드 파인튜닝(SFT)과 함께 적용될 수 있어 토종 메두사 지원으로 모델을 얻을 수 있습니다.\n' +
      '\n' +
      '### Extensions\n' +
      '\n' +
      '대표적인 수용력 3.3.1.1도###### 3.3.1도이다.\n' +
      '\n' +
      '투기 디코딩 논문(Leviathan et al., 2022; Chen et al., 2023)에서 저자는 거부반응 샘플링을 사용하여 원래 모델의 분포와 일치하는 다양한 출력을 생성한다. 그러나 후속 구현(조오잔테, 2023;스펙터 및 Re, 2023)은 이 샘플링 전략이 샘플링 온도가 증가함에 따라 효율을 감소시킨다는 것을 보여준다. 직관적으로 이것은 초안의 모델이 원래 모델과 동일한 극단적인 경우에 이해할 수 있다. 여기서 욕심 디코딩을 사용할 경우 초안의 모델의 모든 출력이 수용되어 효율을 극대화할 것이다. 반대로, 거부반응 샘플링은 초안 모델과 원래 모델을 독립적으로 샘플링하기 때문에 추가 오버헤드를 도입한다. 그들의 분포가 완벽하게 정렬되더라도, 드래프트 모델의 출력은 여전히 거부될 수 있다.\n' +
      '\n' +
      '그러나 실제 시나리오에서는 언어 모델에서 샘플링을 사용하여 다양한 반응을 생성하는 경우가 많고, 온도 매개변수는 응답의 "창의성"을 조절하는 데만 사용된다. 따라서 더 높은 온도는 원래 모델이 모델 초안 출력을 수용할 수 있는 더 많은 기회를 산출해야 한다. 우리는 원래 모델의 분포와 일치시키는 것이 일반적으로 불필요하다는 것을 확인한다. 따라서 거부 표본 추출보다는 그럴듯한 후보를 선택하기 위해 _typical 수락_ 체계를 사용할 것을 제안한다. 이 접근법은 절단 샘플링 연구(휴트 등 2022년)에서 영감을 얻는다(심층 설명을 위해 제2절 참조). 우리의 목표는 _typical_인 후보자를 선택하는 것인데, 이는 원래 모델에 의해 생산될 가능성이 높지 않다는 것을 의미한다. 우리는 _원본 모델_의 예측 확률을 이에 대한 자연 게이지로서 사용하고 예측 분포를 기반으로 임계값을 설정하여 수용을 결정한다. 구체적으로 \\(x_{1},x_{2},\\cdots,x_{n}\\)를 기초로 하여 후보 서열((x_{n+1},x_{n+2},x_{n+2},\\cdots,x_{n+T+1})을 평가할 때 조건을 맥락으로 간주한다.\n' +
      '\n' +
      '>비지니스}(x_{n+{2},\\ff)는\\(p_{dtext\\cex_{n+k-1})를 포함하고 있다(p_{\\text\\atx_{n+k-1}.\n' +
      '\n' +
      'H(\\cdot)\\는 엔트로피 함수를 나타내고, \\(\\epsilon,\\delta\\)는 하이퍼파라미터이다. 이 기준은 휴트 등(2022년)에서 적응되어 상대적으로 확률이 높은 (1) 토큰이 의미가 있으며, (2) 분포의 엔트로피가 높은 경우 다양한 연속체가 합리적이라고 볼 수 있다. 디코딩 동안, 모든 후보들은 이 기준을 사용하여 평가되며, 조건을 만족하면 후보들의 _prefix_가 수용된다. 각 단계에서 적어도 하나의 토큰의 생성을 보장하기 위해 첫 번째 토큰에 대해 _greedy 디코딩_을 적용하고 후속 토큰에 대한 전형적인 수용을 사용하면서 _unconditionally_를 받아들인다. 현재 단계에 대한 최종 예측은 모든 후보들 중 _장기 수용 프리픽스_에 의해 결정된다.\n' +
      '\n' +
      '이 계획을 살펴보는 것은 여러 통찰력으로 이어집니다. 첫째, 온도가 \\(0\\)로 설정되면 가장 가능성 있는 토큰만이 0이 아닌 확률을 가지고 있기 때문에 욕심 디코딩으로 되돌아간다. 온도가 \\(0\\)를 초과함에 따라 욕심 디코딩의 결과는 적절한 \\(\\epsilon,\\delta\\)로 일관되게 수용될 것이며, 그 토큰은 최대 확률을 가지며 최대 속도를 산출하기 때문이다. 마찬가지로, 일반적인 시나리오에서 온도가 증가하면 우리의 실험 결과에 의해 확증되는 바와 같이 더 오래 수용된 서열이 발생할 것이다.\n' +
      '\n' +
      '#### 3.3.2 Self-Distillation\n' +
      '\n' +
      '3.2절에서는 목표 모델의 출력 분포와 일치하는 학습 데이터셋의 존재를 가정한다. 그러나 이것은 항상 그런 것은 아닙니다. 예컨대, 모델 소유자는 학습 데이터 없이 모델만 해제하거나, 모델은 학습 데이터 세트와 다른 모델의 출력 분포를 만드는 인간 피드백(RLHF) 절차를 사용한 강화 학습을 거쳤을 수 있다. 이 문제를 해결하기 위해 모델의 출력 분포와 일치하는 메두사 헤드에 대한 학습 데이터 세트를 생성하기 위해 모델 자체를 사용하기 위해 자동화된 자가 증류 파이프라인을 제안한다.\n' +
      '\n' +
      '데이터세트 생성 과정은 간단합니다. 우리는 먼저 타겟 모델과 유사한 도메인, 예를 들어 채팅 모델에 대해 ShareGPT(ShareGPT, 2023) 데이터 세트를 사용하여 공공 종자 데이터 세트를 취한다. 그런 다음 데이터 세트의 프롬프트를 취하고 모델이 프롬프트에 회신하도록 요청합니다. 멀티턴 대화 샘플을 얻기 위해 종자 데이터세트부터 모델까지 프롬프트를 순차적으로 먹일 수 있습니다. 또는 대화의 두 역할을 모두 훈련하고 있는 제피르 7B(Tunstall et al., 2023)와 같은 모델에 대해, 그들은 자기 해결 능력을 가지고 있으며, 우리는 단순히 첫 번째 프롬프트를 먹여 모델을 여러 차례의 대화를 생성할 수 있다.\n' +
      '\n' +
      '메두사-1의 경우 이 데이터 세트는 메두사 헤드를 훈련하기에 충분하다. 그러나 메두사-2의 경우 백본 및 메두사 헤드를 훈련시키기 위해 이 데이터 세트를 단독으로 사용하는 것이 보통 더 낮은 생성 품질로 이어진다는 것을 관찰했다. 실제로 메두사 헤드를 훈련시키지 않더라도 이 데이터셋으로 백본 모델을 훈련하면 성능 저하가 발생할 것이다. 이는 고전적인 지식 증류 작품(김, 러시, 2016)과 유사하게 지상 진리 토큰을 백본 모델의 라벨로 사용하는 대신 원 모형의 확률 예측도 활용할 필요가 있음을 시사한다. 근본적으로 백본 모델의 손실은 그렇다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{LM-distill}}=KL(p^{(0)}_{\\text{original},t}||p^{(0)}_{t}),\\]\n' +
      '\n' +
      'Hf(p^{(0)}_{\\text{or 오리지널},t}\\)는 위치 \\(t\\)에서 원모형 예측의 확률 분포를 나타낸다.\n' +
      '\n' +
      '그러나 순전히 원본 모델의 확률 예측을 얻기 위해서는 훈련 중 두 가지 모델을 유지하여 기억 요구 사항을 높여야 한다. 이 문제를 더욱 완화하기 위해 자체 증류 설정을 활용하는 단순하면서도 효과적인 방법을 제안한다. 우리는 백본 모델을 미세 조정하기 위해 LoRA(Hu et al., 2021)와 같은 매개변수 효율적인 어댑터를 사용할 수 있다. 이와 같이 원래 모델은 단순히 어댑터가 꺼진 모델이다. 따라서 증류에는 추가 메모리 소비가 필요하지 않습니다. 함께 이 자가 증류 파이프라인을 사용하여 백본 모델의 능력을 해치지 않고 메두사-2를 양성하고 추가 메모리 소비를 거의 도입할 수 없다. 마지막으로, 자기분류를 사용하는 것에 대한 하나의 팁은 이 경우 양자화 없이 LoRA를 사용하는 것이 바람직하고, 그렇지 않으면 교사 모델이 양자화 모델이 되어 생성 품질이 낮아질 수 있다.\n' +
      '\n' +
      '최적화된 트리건설에 대한 검색 검색은 3.3.3.3이다.\n' +
      '\n' +
      '3.1.2절에서는 카트언 제품을 복용하여 나무 구조를 구성하는 가장 간단한 방법을 제시한다. 다만, 나무에 총 노드가 고정된 경우, 정규 트리 구조가 최상의 선택이 아닐 수 있다. 직관적으로, 상이한 헤드의 상위 예측으로 구성된 후보들은 서로 다른 정확도를 가질 수 있다. 따라서, 우리는 트리 구조를 구성하기 위해 정확도의 추정을 레버리지할 수 있다.\n' +
      '\n' +
      '구체적으로, 보정 데이터 세트를 사용하고 다른 헤드의 상위 예측의 정확도를 계산할 수 있다. (a^{(i)}_{k}\\)는 \\(k\\) 헤드의 \\(i\\)-번째 상위 예측의 정확도를 나타낸다. 정확도가 독립적인 것으로 가정하면, 우리는 상위 \\([i_{1},i_{2},\\cdots,i_{k}]\\)에 의해 구성된 후보 서열의 정확도를 \\(\\prod_{j=1}^{k}a^{{(i_{j})}_{j}\\)로 추정할 수 있다. I\\(I\\)는 \\([i_{1},i_{2},\\cdots,i_{k}]\\)의 가능한 모든 조합의 세트를 나타내고 \\(I\\)의 각 요소는 \\([i_{1},i_{2},\\cdots,i_{k}]\\)의 모든 가능한 조합의 노드에 매핑될 수 있다.\n' +
      '\n' +
      '그림 3: 메두사-2 비쿠나-7B에 대한 희소 트리 설정의 표준화를 보여준다. 나무에는 계산과 관련된 4개의 메두사 헤드를 나타내는 깊이 4가 있다. 각 노드는 메두사 머리의 최상위 예측에서 얻은 토큰을 나타내며, 가장자리는 이들 사이의 연결을 보여준다. 빨간 선은 미래 토큰을 올바르게 예측하는 경로를 강조한다.\n' +
      '\n' +
      '(잎 노드뿐만 아니라 모든 노드들이 포함)된다. 그런 다음 후보 시퀀스의 수용 길이에 대한 기대가 된다.\n' +
      '\n' +
      '\\[\\sum_{[i_{1},i_{2},\\cdots,i_{k}]\\in I}\\prod_{j=1}^{k}a_{j}^{(i_{j})}.\\]\n' +
      '\n' +
      '노드를 하나씩 추가하여 트리를 구축하는 것을 고려할 때, 기대에 대한 새로운 노드의 기여는 정확히 노드의 정확성이다. 따라서 현재 트리와 연결되어 있고 정확도가 가장 높은 노드를 선택하여 나무에 노드를 탐욕스럽게 추가할 수 있다. 이 과정은 전체 노드 수가 원하는 수에 도달할 때까지 반복될 수 있다. 이와 같이 수용 길이의 기대를 극대화하는 트리 구조를 구성할 수 있다.\n' +
      '\n' +
      '그림. 3은 메두사-2 비쿠나-7B 모델에 대해 희소하게 구성된 나무의 구조를 보여준다. 이 나무 구조는 4개의 깊이가 확장되어 계산에서 4개의 메두사 헤드의 관여를 나타낸다. 나무는 처음에 카트언 제품 접근법을 통해 형성되며 이후 알라카-이발 데이터세 두비아 등(2023)에서 측정된 각 메두사 머리의 톱크 예측에 대한 통계적 기대에 기초하여 프루닝함으로써 정제된다. 왼쪽으로 나무의 기울기는 각 머리에 더 높은 확률을 갖는 노드에 대한 알고리즘의 선호도를 시각적으로 나타낸다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '이 섹션에서는 다양한 환경에서 메두사의 효과를 입증하기 위한 두 가지 실험 세트를 제시한다. 먼저 Vicuna-33B 및 Zephyr-7B 모델에서 메두사를 평가하여 Vicuna-33B 모델과 Zephyr-7B(Tunstall et al., 2023) 모델에서 메두사를 평가하여 Vicuna-33B 모델, 훈련 데이터셋이 공개적으로 사용할 수 없기 때문에 자가 분동의 효과를 연구한다.\n' +
      '\n' +
      '우리는 일반적으로 사용되는 세 가지 용어() 감속률: 디코딩 단계당 디코딩된 토큰의 평균 수를 나타낸다. 표준 자동 억제 모델에서 이 비율은 1.0 b) 오버헤드로 고전적 디코딩에 비해 디코딩 단계 오버헤드를 특성화하는 데 사용되며 메두사 모델의 스텝 레이턴시당 평균을 바닐라 모델의 평균으로 나누어 계산된다. c) 스피드업: 이는 벽-시간 가속율을 의미한다. 이러한 정의에 따라 스피드업 = 감속률/오버헤드의 관계를 가지고 있다.\n' +
      '\n' +
      '그림 4.0.1는 4.0.1과 같이 설정된 설정물을 고정하였다.\n' +
      '\n' +
      '모든 실험에 대해 우리는 훈련을 위해 Axolotl(Axolotl, 2023) 프레임워크를 사용한다. 우리는 평가기가 있는 코사인 학습률 스케쥴러를 사용하고 8비트 AdamW(ddetmers et al, 2021) 최적화기를 사용합니다. 우리는 \\(1\\) 층이 있는 \\(5\\) 미디어 헤드를 훈련시키고 Eq에 \\(\\lambda_{k}\\)를 설정한다. (1)은 \\(0.8^{k}\\)이다. 메두사-2의 경우 미세조정을 위해 LoRA(Hu et al, 2021) 또는 QLoRA(ddetmers et al, 2023)를 사용하고 메두사 헤드의 학습률을 백본 모델보다 \\(4\\) 배 더 크게 설정했다. LoRA는 언어 모델 헤드를 포함하여 백본 모델의 모든 선형 레이어에 적용된다. LoRA 어댑터의 순위는 \\(32\\)로 설정되며, \\(\\alpha\\)는 \\(16\\)로 설정된다. LoRA 어댑터에 \\(0.05\\)의 탈락이 추가된다.\n' +
      '\n' +
      '비쿠나 7B 및 13B##### 사례 연구: 메두사-1 v. 비쿠나 7B 및 13B##### Case 연구: 메디사-1 v.의 메두사-2.\n' +
      '\n' +
      '그림 4.1.1.1.1실험용 세투업#############################\n' +
      '\n' +
      '우리는 라마 모델(Touvron et al., 2023)에서 미세 조정된 다양한 크기(7B, 13B, 33B)의 채팅 모델을 포함하는 비쿠나 모델 클래스(Chiang et al., 2023)를 사용한다. 이 중 7B 및 13B 모델은 ShareGPT(ShareGPT, 2023) 데이터셋에서 학습되고 33B 모델은 실험 모델이며 개인 데이터셋에서 훈련된다. 이 섹션에서는 ShareGPT 데이터 세트를 사용하여 7B 및 13B 모델에서 \\(2\\) epochs에 대한 메두사 헤드를 훈련시킨다. 우리는 서열 길이 4096을 갖는 라마-2 모델에서 미세 조정되는 Vicuna 모델의 v1.5 버전을 사용한다. 우리는 백본에 대해 \\(64\\)의 글로벌 배치 크기와 \\(5e^{-4}\\)의 피크 학습률을 사용하고 메두사 헤드에 대해 \\(2e^{-3}\\) 및 \\(40\\) 단계를 위한 평가(2e^{-3}\\)를 사용한다. 우리는 두 모델 모두에 \\(4\\)-비트 양자화된 백본 모델을 사용한다. 먼저 메두사-1로 모델을 훈련하고 이러한 훈련된 모델을 초기화로 사용하여 메두사-2를 훈련시켰다. 우리는 Eq에서 메두사-2에 대한 QLoRA와 \\(\\lambda_{0}\\)를 사용한다. (2)는 \\(0.2\\)로 설정됩니다.\n' +
      '\n' +
      '#### 4.1.2 Results\n' +
      '\n' +
      '우리는 결과를 수집하여 그림 4에 보여주며 기준선은 바닐라 허깅 표면 구현이다. 그림에서. (a)a는 7B 모델의 경우 메두사-1 및 메두사-2 구성이 초당 처리된 토큰에서 측정되는 속도가 크게 증가함을 알 수 있다. 메두사-1은 2.18\\(분절) 속도를 나타내는 반면 메두사-2는 이를 2.83\\(분절)로 더욱 개선한다. 더 큰 13B 모델에 적용했을 때 메두사-1은 2.33\\(분절) 속도를 증가시키는 반면 메두사-2는 기준선에서 2.83\\(분절)의 유사한 성능 이득을 유지한다. 메두사-2 비쿠나-7B 모델의 범주당 속도도 도표팅합니다. 우리는 "코딩" 범주가 3.29\\(표본) 속도로 혜택을 받는다는 것을 관찰하며, 이는 메두사가 이 영역의 작업에 특히 효과적임을 시사한다. 이는 소프트웨어 개발 및 기타 프로그래밍 관련 작업에 널리 사용되는 코딩 LLM을 최적화할 수 있는 중요한 잠재력을 지적한다. "추출" 범주는 3.62\\(표본)에서 가장 높은 속도를 나타내며, 이는 이 과제가 메두사에 의해 고도로 최적화되었음을 나타낸다. 전반적으로, 결과는 메두사가 서로 다른 모델 크기와 작업에 걸쳐 추론 속도를 크게 향상시킨다는 것을 시사한다.\n' +
      '\n' +
      '## 사례 연구는 Vicuna-33B 및 Zephyr-7B-7B에서 자가 변천 훈련이다.\n' +
      '\n' +
      '4.2.1 실험 설정 설정 4.2.1.1 실험 설정####### 4.2.1 실험 설정\n' +
      '\n' +
      '본 사례 연구에서는 자가 분산이 필요한 경우에 초점을 맞추고 있다. 우리는 Vicuna-33B 모델(Chiang et al., 2023)과 Zephyr-7B 모델(Tunstall et al, 2023)을 예로 사용한다. 3.3.2 구간에 설명된 절차에 따라 먼저 일부 종자 프롬프트가 있는 데이터 세트를 생성한다. 우리는 ShareGPT(ShareGPT, 2023), UltraChat(Ding et al., 2023)를 종자 데이터 세트로 사용하고 두 경우 모두에 대해 약 \\(100k\\) 샘플에서 데이터 세트를 수집한다. 흥미롭게도, 우리는 제피르 모델이 단일 프롬프트와 여러 라운드의 대화를 계속 생성할 수 있다는 것을 발견하여 큰 데이터 세트를 쉽게 수집할 수 있다. 비쿠나-33B의 경우 각 멀티턴 종자 대화에서 프롬프트를 반복적으로 공급하여 멀티턴 대화를 생성한다. 두 모델 모두 서열 길이 \\(2048\\) 및 배치 크기 \\(128\\)로 훈련된다. 우리는 두 모델에 메두사-2를 사용하고 2단계 훈련 절차를 사용하는 대신 \\(\\theta_{0}\\)의 사인 일정을 사용하여 훈련 종료 시 값이 최고치로 점진적으로 증가하기 때문에 이 접근법이 동등하게 효과적이라는 것을 발견했다. 우리는 백본 LoRA 어댑터의 피크 학습률을 \\(1e^{-4}\\)로 설정하고 평가 단계를 \\(20\\)로 설정하였다. 자가 증류 손실이 상대적으로 작기 때문에 Eq에서 \\(\\lambda_{0}\\)를 설정했다. (2)는 \\(0.01\\)이다.\n' +
      '\n' +
      '#### 4.2.2 Results\n' +
      '\n' +
      '표 1은 MT-벤치에 대한 가속도율, 오버헤드 및 품질 측면에서 다양한 메두사-2 모델을 비교하여 이러한 결과를 보완한다. 특히, 메두사-2 비쿠나-33B-33B.\n' +
      '\n' +
      '그림 4: Left: Vicuna-7B/13B에 대한 기준선, 메디사-1 및 메두사-2의 스피드 비교는 그림 4:였다. 메두사-1은 기준 구현에 비해 2\\ 이상의 벽 시간 속도를 달성하고 메두사-2는 상당한 마진으로 속도를 더욱 향상시킨다. 맞아요: MT-벤치에서 8개 범주에서 비쿠나-7B의 소매 속도 성능.\n' +
      '\n' +
      '모델은 더 낮은 가속도 속도를 나타내며, 비슷한 품질을 유지합니다. 이것은 숨겨진 훈련 데이터셋과 자가 분동에 사용된 데이터세트 사이의 불일치로 인한 것이라고 가정한다.\n' +
      '\n' +
      '이러한 결과는 모델 크기를 스케일링하고 자가 증류 기술을 적용할 때 속도와 성능 사이의 복잡한 상호 작용을 강조한다. 이 연구 결과는 또한 메두사-2 구성의 가능성을 강조하여 모델의 출력의 품질을 조심스럽게 보존하면서 가공 효율을 높이는 동시에 메두사 헤드와 LLM을 공동 최적화 할 수 있는 유망한 방향을 제시한다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '트리고의 검토 4.3.1.3.1의##################\n' +
      '\n' +
      '트리 주의 절제 연구는 메두사-2 비쿠나-7B를 사용하여 MT-벤치 데이터셋(정 et al., 2023)의 쓰기 및 롤플레이 카테고리에 대해 수행된다. 우리는 나무 주의의 동기와 그 성능을 묘사하기 위해 목표로 합니다.\n' +
      '\n' +
      '그림. 5(a)는 최적화된 희소 트리 설정(섹션 3.3.3.3)에 대해 무작위로 샘플링된 조밀한 트리 구성(청색 점으로 묘사된 섹션 3.1.2)의 가속율을 비교한 것이다. 64개의 노드를 갖는 희소 트리 구성은 256개의 노드를 갖는 조밀한 트리 설정보다 더 나은 가속율을 보여준다. 그림. 5(b)는 조밀하고 희소 트리 설정 모두에 대한 속도를 제시한다. 여기에서 관찰된 경향은 추가 길이가 증가함에 따라 현저한 속도 감소를 나타낸다. 이것은 희소 나무 주의가 꾸준한 가속율을 유지하는 데 유용하지만 특히 더 높은 추가 길이에서 감소된 속도의 상충과 함께 있음을 시사한다.\n' +
      '\n' +
      '관찰된 속도 감소는 하드웨어 아키텍처에 의해 도입된 오버헤드 증가에 기인한다. 더 복잡한 트리는 가속율을 향상시킬 수 있지만 하드웨어가 중첩된 오버헤드로 인한 속도 비용으로 그렇게 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l} \\hline \\hline Model Name & Vicuna-7B & Zephyr-7B & Vicuna-13B & Vicuna-33B \\\\ \\hline Acc. rate & 3.47 & 3.14 & 3.51 & 3.01 \\\\ Overhead & 1.22 & 1.18 & 1.23 & 1.27 \\\\ Quality & 6.18 (+0.01) & 7.25 (-0.07) & 6.43 (-0.14) & 7.18 (+0.05) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 다양한 메두사-2 모델의 비교. 품질은 MT-벤치 벤치마크(정 등, 2023)의 평균 점수를 나타낸다. 메디사-2는 가벼운 오버헤드로 유망한 가속율을 달성하고 모델 품질을 보존합니다.\n' +
      '\n' +
      '그림 5: 메두사-2 메두사-2를 사용한 다양한 모델의 스피드업은 모든 모델에 걸쳐 상당한 속도 향상을 보이는 반면, 자가 분류로 훈련된 모델은 모델 품질 보존과 모델 속도 증강 사이의 트레이드 오프로 인해 속도 향상이 약하다.\n' +
      '\n' +
      '일반적인 수용성의 크기 4.3.2.3.2########\n' +
      '\n' +
      '일반적인 수용의 임계값은 메두사-2 비쿠나 7B를 사용하여 MT-벤치 데이터셋(정 et al., 2023)의 "쓰기" 및 "역할" 범주에 대해 연구된다. 비쿠나 7B 모델을 활용하여 우리는 \\(\\alpha=\\sqrt{\\epsilon}\\)를 설정하는 히위트 등(2022)에 의해 묘사된 접근법과 방법론을 정렬했다. 그림. 7은 다양한 샘플링 설정에 걸쳐 모델의 성능에 대한 비교 분석을 제시한다. 이러한 설정은 0.01에서 시작하여 0.01 단계에서 0.25로 증가하는 임계값 \\(몰프실론\\)에서 다양하다. 또한 창의성을 요구하는 작업의 경우 기본 무작위 샘플링이 성능에서 욕심 샘플링을 능가하고 제안된 전형적인 샘플링은 \\(\\epsilon\\)가 증가할 때 무작위 샘플링과 비슷하다는 점에 주목한다.\n' +
      '\n' +
      '그림 6: 나무에 의해 도입된 추가 길이 평가. 좌표: 무작위로 샘플링된 조밀한 트리 설정(블루 점) 및 최적화된 희소 트리 설정(레드 별)에 대한 가속율. 맞아요, 두 설정 모두 속도(토켄/s)입니다. 트렌드 라인은 수용률이 희박 나무에 대해 상대적으로 안정적이지만 추가 길이가 증가함에 따라 속도의 현저한 감소가 있음을 나타낸다.\n' +
      '\n' +
      '그림 7: 제안된 전형적인 샘플링을 사용한 메두사의 성능 비교이다. 도표는 가속 속도(동작)를 보여준다. 레이트(Rate)와 평균 점수는 3가지 설정의 경우 고정 온도가 0.7인 "쓰기" 및 "역할"(MT 벤치마크)에 대해 메두사를 사용한 욕심 샘플링, 무작위 샘플링 및 다른 임계값 하에서 전형적인 샘플링에 대한 평균 점수를 매겼다. 모델은 완전히 미세한 비쿠나-7B입니다.\n' +
      '\n' +
      '2단계 정품 조정의 효과성 4.3.3.3######## 4.3.\n' +
      '\n' +
      '표 2의 비쿠나-7B 모델에 대한 두 가지 미세 조정 전략 간의 성능 차이를 조사한다. 우리는 모델을 메디사 헤드와 대 모델을 직접 미세 조정하는 비교를 제공했다. 3.2.2절에서 설명한 2단계 미세 조정과 관련된 미디어-2. 미세 조정을 위해 메두사-2를 구현하는 것은 모델의 품질을 유지하고 메두사-1 대 속도를 동시에 향상시킨다는 것을 나타낸다.\n' +
      '\n' +
      '### Discussion\n' +
      '\n' +
      '결론적으로, 우리는 모델을 추가 예측 디코딩 헤드와 탑재하여 큰 언어 모델 추론을 가속화하는 새로운 방법인 메두사를 제안했다. 메두사는 모델이 단계당 여러 토큰을 생성할 수 있게 하여 순차적인 자동 억제 디코딩의 병목 현상을 극복한다. 모델 성능을 보존하면서 이러한 추가 머리를 효율적으로 훈련시키기 위해 메두사-1 및 메두사-2의 두 가지 절차를 입증했다. 다양한 크기와 훈련 방법의 모델에 대한 실험은 다양한 프롬프트 유형 및 모델에 걸쳐 단일 프롬프트 추론에서 2.3-3.6\\(표본)의 일관된 속도를 보여준다.\n' +
      '\n' +
      '메두사의 주요 장점에는 단순성, 매개변수 효율성, 기존 시스템으로의 통합 용이성이 포함된다. 메두사는 투기적 디코딩 개념 위에 구축함으로써 전문화된 모델 초안을 필요로 하지 않는다. 전형적인 수용 방식은 또한 합리적인 산출물을 제공하면서 거부반응 샘플링의 합병증을 제거한다. 마지막으로, 미세 조정 절차는 원본 모델의 성능에 영향을 미치지 않으면서 고품질 세대를 보장합니다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '이 프로젝트에 대한 공헌이 매우 어려운 여러 개인에 대한 진심 어린 감사의 마음을 확장합니다.\n' +
      '\n' +
      '* 주한 리는 LLM 서빙에 대한 귀중한 통찰력을 가지고 있다. 이미 안하셨다면 주한의 vLLM 프로젝트를 확인해보세요, 인상적인 것은 없습니다.\n' +
      '* Shaojie Bai는 이 작품의 초기 단계를 형성하는 데 도움이 되는 중요한 논의를 수행합니다.\n' +
      '* 데니 주씨는 절삭 샘플링 계획을 톈레에 소개하고 톈레에게 LLM 서빙 영역을 탐색하도록 장려했다.\n' +
      '* 옌핑황은 톈레에 봉사하는 LLM과 관련된 메모리 결합 과제를 지적했다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* 아인슬리 등은 (2023) 조슈아인슬리, 제임스 이투로프, 미첼 데종, 예리 제멜리만스키, 페데리코 레브론, 수미티 상하이 등이 있다. Gqa: 훈련 일반화된 멀티-쿼리 변압기 모델을 다중-헤드 체크포인트에서 훈련시키는 __다-헤드 체크포인트에서 모델링한다. arXiv 프리프린트 arXiv:2305.13245_, 2023.\n' +
      '* Axolotl(2023) Axolotl. Axolotl[https://github.com/개방 액세스-AI-집락/악솔로톨로틀]]. 2023년 (https://github.com/오픈 액세스-AI-집단/axolotl)이다.\n' +
      '* 바두 등은 (2021) 실리야 바두, 고바다나 사치타난담 라마차란란, 니티시 시리시 케스카르, 라브 라브 등이 있다. 바셜니. {MIROSTAT}: A {neural} {em코딩} {dec코딩} {g 알고리즘} {} 직접} { 대조군} {perplexity}입니다. 2021년 국제 학습 발표_, 2021년 국제 토론회에서 URL[https://openopenreview.net/forum?\'WI6IJEIy5_](https://openopenreview.net/forum?id=WI6IJZEIy5_)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & Baseline & Direct Fine-tuning & Medusa-1 & Medusa-2 \\\\ \\hline Quality & 6.17 & 5.925 & 6.23 & 6.18 \\\\ Speed Up & N/A & N/A & 2.18 & 2.83 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 다른 설정의 비쿠나-7B 비교. MT-벤치에 대한 모델을 평가하여 품질을 얻는다.\n' +
      '\n' +
      '톰 브라운, 벤자민 맨, 니크 리더, 멜라노이 Subbiah, 자르드 디 카플란, 프라풀라 다하리왈, 아르빈드 네나칸탄, 프랜나브 시흐만탄, 기러시 스스티, 아미다 아셀 등 언어 모델은 몇 가지 샷 학습자들이다. 신경 정보 처리 시스템_, 2020년 33:1877-1901의 발전이다.\n' +
      '* 첸 등은 (2023) 찰리 첸, 세바스티안 보게우드, 거프리 어빙, 장-베티스트 레스피아우, 로랑 시프레, 존 주커퍼 등이다. 사행 샘플링으로 큰 언어 모델 디코딩을 가속화할 수 있습니다. 1961년 2월. 도이: 10.48550/ARXIV.2302.01318.\n' +
      '*치앙 등은 (2023) 위린치앙, 주한리, 지린, 빙선고, 장하오우, 하오장, 리안민정, 시유안주강, 용하오 주앙, 조셉 에르니제일즈, 아이온 스투카, 에릭 퍼시링 등이다. 비쿠나: 안 오픈 소스 챗봇은 90%* 챗봇 품질, 2023년 3월 URL[https://lmsys.org/blog/2023-03-30-vicna/] (https://lmsys.org/blog/2023-03-30-vicuna/]로 gpt-4를 손상시킨다.\n' +
      '* 차우드헤이 등(2022) 아악크하 차우더리, 샤란 나랑, 샤란 나랑, 자콥 데블린, 마르트엔 보스마, 가우라브 미슈라, 아담 로버츠, 폴 바햄, 형원정, 찰스 수턴, 세바스티안 게르만 등의 언어 모델링은 경로이다. arXiv 프리프린트 arXiv:2204.02311_, 2022.\n' +
      '(2021) 팀 디트로머, M.(2021) 팀 디트로머 등. 루이스, 삼슬리퍼, 루크 제트렘러. 블록별 양자화를 통한 _8비트 최적화기 _8비트 최적화기. 2021년 학습 발표_국제회의.\n' +
      '* 디트리머 등은 (2022) 팀 디트머, 마이크 루이스, 예제 벨카다, 루크 제트렘거 등이 있다. Llm. 블렌트8(): 스케일에서의 변압기에 대한 8비트 행렬 곱셈. __. arXiv 프리프린트 arXiv:2208.07339_, 2022.\n' +
      '* 디트리머 등은 (2023) 팀 디트머, 아르티도 파피니, 아리 홀츠만, 루크 제트렘거 등이 있다. 플로라: 정량화된 llms의 효율적인 핀셋링. __ 정량화된 llms. arXiv 프리프린트 arXiv:2305.14314_, 2023.\n' +
      '* 딩 등은 (2023) 닝딩딩, 이눌린 첸, 복이Xu, 유자아 진, 지히정, 선딩후, 지히안 류, 마송선, 보웬 주 등이 있다. 고품질 수업 대화를 2023년 스케일링하여 채팅 언어 모델을 강화합니다.\n' +
      '* 두두아 등은 (2023) 예나두두아, 주에첸 리, 로한 태리, 톈이 장, 이하안 굴라자니, 지미 Ba, 카를로스 게스트린, 페시 리앙, 타쓰노리 바슈모토. 아파팜: 2023년 인간 피드백에서 배우는 방법에 대한 시뮬레이션 프레임워크입니다.\n' +
      '* Elfwing et al.(2017) 스테판 엘파이팅, E. 우치베 및 K. 도야. 강화 학습에서 신경망 기능 근사화를 위한 __신경망 함수 근사화를 위한 Sigmoid 강조 선형 단위. __ 뉴럴네트웍스_ 2017. 도이: 10.1016/j.neunet.2017.12.012.뉴네트.\n' +
      '* 판 등은 (2018) 안젤라 판, 마이크 루이스, 야만 다푸신 등이 있다. 계층적 신경 스토리 생성. 제56차 컴퓨터통계학회 연차회의 _검토(1: 롱 파이어스)_. 컴퓨팅 언어학 조합 2018. 도이: 10.18653/v1/p18-1082.\n' +
      '* 프랑타르 등 (2022) 에리오스 프란타르, 세일링 애슈보스, 토스트렌 회플러, 단 알리스타르 등이다. 창의적 사전 훈련된 변압기들에 대한 정확한 사후 학습 양자화 __Gptq: 생성적 사전 훈련된 변압기에 대한 정확한 사후 학습 양자화이다. arXiv 프리프린트 arXiv:2210.17323_ 2022년입니다.\n' +
      '* Fu 등은 (2023) 요차 후, 피터 베일리스, 이온 스투카 및 하오 장이다. 톡시헤드 디코딩, 2023년 11월 URL[https://lmsys.org/blog/2023-11-21-lookahead-dec코딩/] (https://lmsys.org/blog/2023-11-21-lookahead-dec코딩/)을 사용하여 llm 추론의 순차적 의존성을 파괴한다.\n' +
      '* 구글(2023) 구글입니다. 팔름 2 기술 보고서 2023. URL[https://ai.google/ 정수/문서/palm2tech 보고:pdf] (https://ai.google/ 양방향/문서/서류/palm2tech 보고.pdf)\n' +
      '* He et al.(2023) Zhenyu He, Zexuan Z홍, 톈레 Cai, Jason D Lee, Di He. 재평가 기반 투기 디코딩 __ 회복 : 재평가 기반 투기 디코딩. arXiv 프리프린트 arXiv: 2311.08252_, 2023.\n' +
      '* 휴이트 등(2022) 존 휴이트, 크리스토퍼 디 마닝, 퍼시 리앙 등이 있다. 대화 샘플링은 언어 모델 탈착으로 설정됩니다. 2022년 10월 10일 도이: 10.48550/ARXIV.2210.15191.\n' +
      '* 호프만, 알(2022) 요르단 호프만, 세바스티안 보게우드, 아르투르 엠센치, 엘레나 부차츠카야, 트레보르 카다, 엘리자 루터포드, 디에고 데 라스 카사, 리사 안네 헨드릭스, 요하네스 웰블, 에단 클라크 등 훈련은 최적 대형 언어 모델을 구성합니다. arXiv 프리프린트 arXiv:2203.15556_ 2022.\n' +
      '* 하프너 등은 (2020) 아리 홀츠만, 얀 버이스, 리두, 맥스웰 포브스, 예진 최씨 등이다. 신경 텍스트 변성의 궁금증이 있는 경우. 학습 발표__국제회의에서 2020년 URL[https://openreview.net/forum?�rygGQyrFvH](https://openopenreview.net/forum?id=rygGQyrFvH).\n' +
      '* Hu et al.(2021) 에드워드 J Hu, 예롱 심, 필립 월리스, 지유안 알렌 주, 원히 리, 시안 왕, 위즈후 첸 등이다. 로라: 대형 언어 모델의 낮은 순위 적응. __Lora: 대형 언어 모델의 낮은 순위 적응. ICLR_ 2021.\n' +
      '* 간테(2023) 조오 가테. 보조 세대: 2023년 저역성 텍스트 생성, 2023년 URL[https://huggingface.co/blog/보조세대] (https://huggingface.co/blog/assard-세대)에 대한 새로운 방향.\n' +
      '* 김씨(2023) 세훈킴, 코맨호퍼, 아미르 가홀라미, 줌동, 시우유 리, 선가선, 마이클 와마니, 커트 게이저 등이 있다. Squeezellm: Dense-and-sparse 양자화. __Squeezellm: Dense-and-sparse 양자화. arXiv 프리프린트 arXiv:2306.07629_, 2023.\n' +
      '* 김씨와 러시(2016) 윤킴과 알렉산더 M. 루쉬. 서열 수준 지식 증류 __ 서열 수준 지식 증류. __ 서열 수준 지식 증류. 2016년EMNLP_.\n' +
      '* 쿠마르 등은 (2022) 안야야 쿠마르, 아티 라구나탄, 로비 존스, 벵유 마, 퍼시 리앙 등이 있다. 제거 처리하면 전처리된 특징과 과소 분포를 왜곡할 수 있으며 _ 고정 조정은 사전 배포된 특징을 왜곡할 수 있다. 국제 학습 발표회의_ 2022년입니다.\n' +
      '* 권씨는 (2023)우석권, 주한리, 시유안주강, 예잉선고, 리안민정, 체디 하오유, 조셉 에르니제일즈, 하오장, 이온 스투카 등이 있다. 퍼지션으로 제공되는 대형 언어 모델에 대한 효율적인 메모리 관리입니다. ACM SIGOPS 29차 심포지엄의 _검토에서 운영 시스템 원칙_ 2023.\n' +
      '* 레비아탄 등 (2022) 옌비 레비아탄, 마탄 칼만, 요시 마티아스 등이 있다. 변압기의 마지막 추론은 투기적 디코딩을 통해 발생한다. 2022년 11월 도이: 10.48550/ARXIV.2211.17192.\n' +
      '* Lin et al.(2023) 지린, 지밍 당, 해티안 당, 샹양, 쉬규당, 송한. lm 압축 및 가속도를 위한 활성 인식 가중치 양자화는 __lm 압축 및 가속도를 위한 활성화 인식 가중치 양자화이다. arXiv 프리프린트 arXiv:2306.00978_, 2023.\n' +
      '* 류 등은 알(2023) 샤오산안 류, 란시앙 후, 피터 베일리스, 이온 스투카, 지히 덩, 알빈 처웅, 하오 장 등이 있다. 온라인 투기 디코딩 __ 온라인 투기 디코딩. _ 온라인 투기 디코딩. _ 온라인 투기 디코딩. _ 온라인 투기 디코딩. arXiv 프리프린트 arXiv: 2310.07177_, 2023.\n' +
      '* 마이스터 등(2022) 클라 마이스터, 지안 위셔, 티고 피멘텔, 라이언 쿠테렐 등이 있다. 언어 생성의 확률-품질 역설에 대해. 2022년 3월. 도이: 10.48550/ARXIV.2203.17217.\n' +
      '* 마이스터 등 (2023) 클라 마이스터, 티고 피멘텔, 지안 위셔, 라이언 코테렐 등이 있다. 종래의 전형적인 샘플링. _로 전형적인 샘플링. __. 컴퓨터 로직_, 11:102-121, 2023 협회의 거래.\n' +
      '* 미오 등은 (2023) 잿펑미오, 가벨 올레아로, 지하오 장, 신하오 청, 지유 왕, 리잉 예이 웡, 주밍 크런, 다야야 아페엔, 레이나 압하이카르, 지하오 지아 등을 들 수 있다. 시표퍼: 투기 추론 및 토큰 트리 검증과 함께 제공되는 생성 llm 서빙. _가속화 생성 llm 검증. arXiv 프리프린트 arXiv:2305.09781_, 2023.\n' +
      '* 오픈AI(2023) 오픈AI입니다. ept-4 기술 보고서 2023.\n' +
      '*오우양(2022) 롱오양(2022) 롱오양, 제프우, Xu 장, 디아고 알메다, 카롤 L Wainwright, 파멜라 미쉬킨, 총 장, 샌히니 아가왈, 카타리나 슬라왈, 알렉스 라이와 같은 언어 모델을 학습시켜 인간의 피드백으로 지침을 따르도록 한다. arXiv 프리프린트 arXiv:2203.02155_, 2022.\n' +
      '* 필루틀라 등은 (2021) 케리슈나 피루틀라, 스와바하 스웨이마딩사, 로완 제셀러, 존 티케스트룬, 세안 웰레크, 최예진, 지이드 하카우이 등이 있다. MAUVE: 게실 프론티어를 사용하여 신경 텍스트와 인간 텍스트 사이의 격차를 측정하는 것이다. A. 비이겔지머에서 Y. 다푸신, P. 리앙 및 J. 위트만 비노안, 편집자, 2021년 신경정보 처리 시스템__Advances. URL[https://openopennet/forum?Tqx7nJp7PR] (https://openopenreview.net/forum?Tqx7nJp7PR)\n' +
      '* 교황 등은 알(2022) 라이너 교황, 샤토 두글러스, 아악카 차와헤이, 제이콥 데브린, 제임스 브래드버리, 안셀름 레브스카야, 조나단 희크, 케파 샤오, 시바니 아크로갈, 제프 딘 등이 있다. 효율적으로 스케일링 변환기 추론. 2022년 11월 도이: 10.48550/ARXIV.2211.05102.\n' +
      '* Pillutla et al.(2021)ShareGPT. 셰어GPT[대항신경화면.co/datasets/Aeala/ShareGPT_Vicuna_unf여과]] (https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unf여과) 2023.\n' +
      '* 셰저(2019) 노암 셰저. 패스트 변압기 디코딩: 원 쓰기 헤드가 필요한 전부입니다. _ast 변압기 디코딩. arXiv 프리프린트 arXiv:1911.02150_ 2019.\n' +
      '*스펙터 & Re(2023) 벤자민스펙터 및 크리스 리입니다. 램핑된 투기 디코딩으로 llm 추론을 가속화하는 __가속화 llm 추론을 가속화한다. arXiv 프리프린트 arXiv:2308.04623_, 2023.\n' +
      '* 스턴 등 (2018) 미첼 스턴 노암 M. 샤저, 작노 우즈코레이트. 심층 자기회귀 모델에 대한 __ 깊은 자기회귀 모델에 대한 차단 평행 디코딩. _ 차단 병렬 디코딩. 신경정보처리시스템_2018.\n' +
      '* 타우브론 등은 (2023) 허고 투브론, 루이 마틴, 케빈 스톤, 피터 알베르트, 아미자드 알마헤이, 야스민 바흐에이, 니콜레이 바시코프, 소우미아 바트라, 프라자왈 바하바바, 슈투티 바하바바, 샤르티 보세일, 오픈 파운데이션 및 미세 색채 모델. arXiv 프리프린트 arXiv:2307.09288_, 2023.\n' +
      '* 튀스트란 등은 (2023) 루이스 튀스트레알, 에드워드 비칭, 나스탱 램버트, 나스턴 라바니, 나시프 라자니, 카시프 라술, 예운스 벨카다, 선치 황, 리난드로 폰 베르트라, 코틴 포리에, 나탄 하비브, 나탄 사르그라빈, 오마르 산세비에로, 알렉산더 마에로, 알렉산더 마에른 등이 있다. 러쉬와 토마스 울프. 제피르: 2023년 lm 정렬의 다이렉트 증류.\n' +
      '*샤 등은 (2023) 하밍샤, 도거, 시킹첸, 푸루위, 지팡수이 등이 있다. 누적 디코딩: 2023년 자기회귀 번역의 무손실 속도 조절, 2023년 URL[https://openopenreview.net/forum?=H-VlwsYvVi](https://openopennet/forum?id=H-VlwsYvVi)\n' +
      '* 샤오 등은 (2023a) 광수안 샤오, 지린, 미카엘 세즈앙, 하오우, 진리엔 데머스, 송한 등이 있다. 스모쿼트: 대규모 언어 모델에 대한 정확하고 효율적인 사후 학습 양자화입니다. 기계학습_국제회의에서는 38087-38099 페이지, 2023a. PMLR.\n' +
      '* 샤오 등은 (2023b) 정샤오, 리준우, 준량구오, 준타오 리, 민장, 도진, 티옌 류 등이 있다. 신경 기계 번역 및 그 이상의 비자기회귀 생성에 대한 조사. __ 신경 기계 번역을 위한 비자기회귀 생성에 대한 조사. IEEE 전환은 패턴 분석과 기계 지능_, 2023b에 관한 것이다.\n' +
      '*잉 등은 (2021) 청수안잉, 톈글캐, 선지루오, 샤신정, 구올린커, 디헤, 옌밍선, 티야 류 등이 있다. 변압기는 그래프 표현에 대해 정말 나쁜 성능을 발휘합니까? __? 2021년 신경 정보 처리 시스템_, 34:28877-28888의 발전입니다.\n' +
      '*장 등은 24일(2022) 수산장, 스티븐 롤러, 남안 고열, 미켈 아르테크스, 모야 첸, 샤오의 첸, 크리스토퍼 듀란, 몬가 디바, 시안 리, 시 빅토리아 린 등의 사전 훈련된 변압기 언어 모델. Opt: 오픈 사전 훈련된 변압기 모델. arXiv 프리프린트 arXiv:2205.01068_, 2022.\n' +
      '* 장 등은 큰 언어 모델의 효율적인 생성 추론을 위해 중히터 리, 클락 바르렛, H__2: 중하터 오라클. arXiv 프리프린트 arXiv:2306.14048_, 2023.\n' +
      '*정 등은 (2023) 리안민정, 위린치앙, 예잉 선가, 시유안 주앙, 장하오 우, 용하오 주앙, 지린, 주한 리, 다정 리, 에릭. PXing, 하오장, 조셉 에르위니아 곤졸레스, 이온 스투카. mt-bench와 챗봇 선두로 llm-as-a- 판단될 때 2023년입니다.\n' +
      '*주 등은 (2023) 용차오 주, 가이펑 루우, 안키트 싱 라트, 아디아 키슈나 메논, 아프리카신 로스타미자데, 산지비 쿠마르, 장프란코리스 카그, 리샤바 아가왈 등이 있다. 분포: 지식 증류를 통한 투기 디코딩 개선: 지식 증류를 통한 투기 디코딩을 향상한다. arXiv 프리프린트 arXiv: 2310.08461_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>