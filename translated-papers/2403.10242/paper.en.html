<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model\n' +
      '\n' +
      'Qijun Feng\n' +
      '\n' +
      '1Fudan University, Shanghai, China1\n' +
      '\n' +
      ' Zhen Xing\n' +
      '\n' +
      '1Fudan University, Shanghai, China1\n' +
      '\n' +
      ' Zuxuan Wu\n' +
      '\n' +
      '1Fudan University, Shanghai, China1\n' +
      '\n' +
      ' Yu-Gang Jiang\n' +
      '\n' +
      '1Fudan University, Shanghai, China1\n' +
      '\n' +
      'Fudan University, Shanghai, China1\n' +
      '\n' +
      'Fudan University, Shanghai, China1\n' +
      '\n' +
      'Fudan University, Shanghai, China1\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Reconstructing detailed 3D objects from single-view images remains a challenging task due to the limited information available. In this paper, we introduce FDGaussian, a novel two-stage framework for single-image 3D reconstruction. Recent methods typically utilize pre-trained 2D diffusion models to generate plausible novel views from the input image, yet they encounter issues with either multi-view inconsistency or lack of geometric fidelity. To overcome these challenges, we propose an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, enabling the generation of consistent multi-view images. Moreover, we further accelerate the state-of-the-art Gaussian Splatting incorporating epipolar attention to fuse images from different viewpoints. We demonstrate that FDGaussian generates images with high consistency across different views and reconstructs high-quality 3D objects, both qualitatively and quantitatively. More examples can be found at our website [https://qjfeng.net/FDGaussian/](https://qjfeng.net/FDGaussian/).\n' +
      '\n' +
      'Keywords:3D Reconstruction Gaussian Splatting Diffusion Model\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Single-view 3D reconstruction aims to recover 3D geometry and appearance of an object from a single RGB image. This task holds immense importance as it allows machines to understand and interact with the real 3D world, enabling various applications in virtual reality (VR), augmented reality (AR) [22, 24] and robotics [53].\n' +
      '\n' +
      'A major challenge in 3D reconstruction involves securing a representation that is both high-quality and efficient. Explicit representations like point clouds [1, 13, 38, 39], voxels [10, 61, 65, 33], and meshes [52, 15] are commonly used due to their intuitive and deformation friendly property, but struggle to represent realistic appearance. In recent years, implicit representations (_e.g._, Neural Radiance Field (NeRF) [27, 36, 69]) have witnessed great success since the continuous nature of these methods helps optimization. However, the stochastic sampling required for rendering is time-consuming and may result in noise.\n' +
      '\n' +
      'To leverage the strength of both implicit and explicit radiance fields while overcoming their drawbacks, 3D Gaussian Splatting [21] is proposed. It combines the benefits of neural network-based optimization and explicit, structured data storage, allowing for high-quality rendering with competitive training and inference time. Current methods [49, 50, 70] often feed a single image for Gaussian Splatting, ignoring the spatial correspondence of multiple views. Additionally, we observe that the original implementation of Gaussian Splatting [21] neglects the distance between 3D Gaussians, causing many unnecessary split and clone operations.\n' +
      '\n' +
      'Several works [51, 32] have shown that fine-turning a 2D generator to understand the correlation between different views of the object significantly facilitates 3D reconstruction. Yet these methods either suffer from multi-view inconsistency or struggle to handle objects with complicated geometric structures.\n' +
      '\n' +
      'Considering these, we propose **FDGGaussian**, a novel two-stage framework for single-image 3D reconstruction composed of a geometric-aware multi-view generation stage and the following accelerated 3D Gaussian reconstruction stage. The generation stage aims to synthesize 3D-aware and multi-view consistent high-fidelity images. To achieve the goal, 3D features are extracted as geometric condition by decoupling the orthogonal planes while semantic condition is obtained with the CLIP [40] encoder. Together with the input image, both conditions are fed into the diffusion model [44]. At the reconstruction stage, we introduce epipolar attention to fuse the generated consistent views, which fully exploits the underlying geometric correlation between views, allowing for competitive visual quality enhancement. Moreover, to further accelerate the optimization process, we propose a novel metric termed Gaussian Divergent Significance (GDS) to avoid unnecessary operations.\n' +
      '\n' +
      'Extensive experiments and ablations on Objaverse [11] and GSO [12] dataset demonstrate that our method is able to generate high-quality 3D objects with multi-view consistency and detailed geometric. Furthermore, we show that FDGaussian can seamlessly integrate with text-to-image models in downstream text-to-3D applications. Our main contributions can be summarized as following:* We incorporate an orthogonal plane decomposition mechanism with a diffusion model to synthesize multi-view consistent and geometric-aware novel view images.\n' +
      '* In order to take full advantage of the consistent multi-view images, we introduce epipolar attention into the rendering process, allowing for efficient and effective communication between images.\n' +
      '* We derive a novel metric named Gaussian Divergent Significance (GDS) to prune unnecessary split and clone operations during optimization, achieving significant time reduction.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Representations for 3D Reconstruction\n' +
      '\n' +
      '#### 2.1.1 Explicit representations\n' +
      '\n' +
      'Explicit representations have been predominant in industries and academic research for a long time. Classic representations, including point clouds [13, 35, 38], voxels [10, 59, 60, 61, 65, 33, 65], meshes [52, 55, 58, 66], have been revisited for 3D reconstruction. While these explicit representations offer detailed descriptions of geometry and appearance, they lack the flexibility of underlying topology and often struggle to capture realistic appearances effectively.\n' +
      '\n' +
      '#### 2.1.2 Implicit representations\n' +
      '\n' +
      'Different from explicit representations, implicit representations, including signed distance fields (SDF) [8, 35, 6] and unsigned distance fields (UDF) [16, 9, 31], offer the advantage of accurately modeling arbitrary geometry and topology. Thanks to the continuous nature of implicit representations, they can leverage deep neural networks to support data-driven geometry learning. In recent years, Neural Radiance Field (NeRF) [36] has demonstrated encouraging progress, allowing for 3D optimization with only 2D supervision via volumetric rendering. Nevertheless, implicit approaches suffer from extensive sampling to fit the implicit functions of 3D scenes. This leads to significant computational costs, particularly in high-resolution or interactive rendering scenarios, even with accelerated NeRF versions [2, 5, 14, 45]. It is difficult to achieve real-time rendering and high-quality view synthesis at the same time.\n' +
      '\n' +
      '#### 2.1.3 Gaussian splatting\n' +
      '\n' +
      'In order to tackle the aforementioned obstacles, Gaussian Splatting [21] has emerged as an alternative representation and has shown remarkable advancements in terms of both quality and speed, offering a promising avenue. Some methods [50, 70] leverage Gaussians to generate coarse 3D representation and then refine it with reference information, which often requires extra depth input or mesh extraction process. Other methods [49] mitigate this problem by directly predicting 3D representation from the monocular input image, yet suffering from artifacts in unseen regions. In contrast to these works, our method combines the creativity of diffusion models and the efficiency of Gaussian Splatting and can render high-quality Gaussian representation in a relatively short time without explicit depth or normal hint.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '### 3D Reconstruction Guided by 2D Diffusion Models\n' +
      '\n' +
      'The recent success of denoising diffusion probabilistic models (DDPM) [46, 47, 48, 62, 48, 63, 64] has caught a surge of interest. In visual content creation, language-guided image diffusion models such as DALL-E2 [43], Imagen [42], and Stable Diffusion [44] have shown huge potential in generating photorealistic images with strong semantic correlation to the given text-prompt inputs. They possess powerful priors about our 3D world and have inspired a growing body of research to employ 2D prior models for assisting 3D generative tasks. Many methods follow the paradigm of per-shape optimization [3, 37, 19, 23, 25, 34, 7]. They typically optimize a 3D representation and utilize 2D diffusion models for gradient guidance. While they have yielded impressive results, these methods tend to suffer from prolonged optimization times, the "multi-face" problem, over-saturated colors, and a lack of diversity in results.\n' +
      '\n' +
      'A new wave of studies, highlighted by works like Zero-1-to-3 [30], has show-cased the promise of using pre-trained 2D diffusion models for synthesizing novel views from singular images or text, opening new doors for 3D generation. Nevertheless, the multi-view images produced by previous methods [29, 30, 68] lack consistency and geometric details. Our research, along with several concurrent studies [57, 56, 26, 4, 28, 25], is dedicated to improving multi-view consistency while capturing the intricate structure of the reference image.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Geometry-aware Multi-view Image Generation\n' +
      '\n' +
      'Finetuning pre-trained diffusion models [44] to synthesize novel images under a given camera transformation has demonstrated promising results [30, 57, 68, 32]. One stream of methods [57, 68] address the multi-view inconsistency problem by conditioning on previously generated images, which tends to be susceptible to cumulative errors and reduced processing speeds. Another stream of methods [28, 30] solely use the reference image and semantic guidance to generate novel views, but suffer from collapsed geometry and limited fidelity.\n' +
      '\n' +
      'We argue that the pivot lies in fully utilizing the geometric information provided by the reference image. However, directly extracting 3D information from a single 2D image is not feasible. Thus, it is imperative to effectively disentangle 3D features from the image plane (_i.e_. \\(xy\\)-plane) by decoupling orthogonal planes. We first employ a vision transformer to encode the input image and capture overall correlations in the image, generating high-dimensional latent \\(\\mathbf{h}\\). Then we leverage two decoders, an image-plane decoder and an orthogonal-plane decoder, to generate geometric-aware features from the latent. The image-plane decoder reverses the encoding operation, leveraging a self-attention mechanism on the encoder output and converting it into \\(F_{xy}\\). In order to generate orthogonal-plane features while maintaining structural alignment with the image plane, a cross-attention mechanism is employed to decode \\(yz\\) and \\(xz\\) plane features \\(F_{yz}\\) and \\(F_{xz}\\). To facilitate the decoding process across different planes, we introducea learnable embedding \\(\\mathbf{u}\\) that supplies additional information for decoupling new planes. The learnable embedding \\(\\mathbf{u}\\) is first processed through self-attention encoding and then used as a query in a cross-attention mechanism with the encoded image latent \\(\\mathbf{h}\\). The image features are converted into keys and values for the cross-attention mechanism as following:\n' +
      '\n' +
      '\\[\\texttt{CrossAttn}(\\mathbf{u},\\mathbf{h})=\\texttt{SoftMax}\\bigg{(}\\frac{(W^{Q}\\texttt{ SelfAttn}(\\mathbf{u}))(W^{K}\\mathbf{h})^{T}}{\\sqrt{d}}\\bigg{)}(W^{V}\\mathbf{h}), \\tag{1}\\]\n' +
      '\n' +
      'where \\(W^{Q}\\), \\(W^{K}\\), and \\(W^{V}\\) are learnable parameters and \\(d\\) is the scaling coefficient. Finally, the features are combined as geometric condition:\n' +
      '\n' +
      '\\[F=F_{xy}(\\texttt{\\textcircled{C}})(F_{yz}+F_{xz}), \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\texttt{\\textcircled{C}}\\) and \\(+\\) are concatenation and summation operations, respectively.\n' +
      '\n' +
      '**Backbone design.** Similar to previous works [18, 44], we use a latent diffusion architecture with an encoder \\(\\mathcal{E}\\), a denoiser UNet \\(\\epsilon_{\\theta}\\), and a decoder \\(\\mathcal{D}\\). The network is initialized from the pre-trained weights of Zero-1-to-3 [30] given its massive scale of training data. Following [30] and [32], the input view is channel-concatenated with the noisy target view as the input to UNet. We employ the\n' +
      '\n' +
      'Figure 2: **Overview of our method.** In _generation stage_, we extract 3D features from the single input image by decoupling the orthogonal planes, and feed them into the UNet to generate high-quality multi-view images. In _reconstruction stage_, we leverage the epipolar attention to fuse images with different viewpoints. We further leverage Gaussian Divergent Significance (GDS) to accelerate the adaptive density control during optimization, allowing competitive training and inference time.\n' +
      '\n' +
      'CLIP image encoder [40] for encoding \\(\\mathcal{I}_{ref}\\), while the CLIP text encoder [40] is utilized to encode \\(\\Delta\\pi\\). The concatenation of their embeddings, denoted as \\(c(\\mathcal{I}_{ref},\\Delta\\pi)\\), forms the semantic condition in the framework. We can learn the network by optimizing the following objective:\n' +
      '\n' +
      '\\[\\min_{\\theta}\\mathbb{E}_{z\\sim\\mathcal{E}(\\mathcal{I}),t,e\\sim\\mathcal{N}(0,1 )}\\|\\epsilon-\\epsilon_{\\theta}(z_{t},t,c(\\mathcal{I}_{ref},\\Delta\\pi))\\|_{2}^{2} \\tag{3}\\]\n' +
      '\n' +
      '### Preliminary of Gaussian Splitting\n' +
      '\n' +
      '3D Gaussian Splatting is a learning-based rasterization technique for 3D scene reconstruction and novel view synthesis [21]. Each Gaussian element is defined with a position (mean) \\(\\boldsymbol{\\mu}\\), a full 3D covariance matrix \\(\\boldsymbol{\\Sigma}\\), color \\(c\\), and opacity \\(\\sigma\\). The Gaussian function \\(G(x)\\) can be formulated as:\n' +
      '\n' +
      '\\[G(x)=exp(-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^{T}\\boldsymbol{\\Sigma} ^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})). \\tag{4}\\]\n' +
      '\n' +
      'To ensure the positive semi-definiteness of \\(\\boldsymbol{\\Sigma}\\), the covariance matrix \\(\\boldsymbol{\\Sigma}\\) can be factorized into a scaling matrix \\(S\\) represented by a 3D-vector \\(s\\in\\mathbb{R}^{3}\\) and a rotation matrix \\(R\\) expressed as a quaternion \\(q\\in\\mathbb{R}^{4}\\) for the differentiable optimization: \\(\\boldsymbol{\\Sigma}=RSS^{T}R^{T}\\).\n' +
      '\n' +
      'The rendering technique of splatting, as initially introduced in [21], is to project the Gaussians onto the camera image planes, which are employed to generate novel view images. Given a viewing transformation \\(W\\), the covariance matrix \\(\\boldsymbol{\\Sigma}^{\\prime}\\) in camera coordinates is given as: \\(\\boldsymbol{\\Sigma}^{\\prime}=JW\\boldsymbol{\\Sigma}W^{T}J^{T}\\), where \\(J\\) is the Jacobian matrix of the affine approximation of the projective transformation. After mapping 3D Gaussians to a 2D image space, we count 2D Gaussians that overlap with each pixel and calculate their color \\(c_{i}\\) and opacity \\(\\sigma_{i}\\) contribution. Specifically, the color of each Gaussian is assigned to every pixel based on the Gaussian representation described in Eq. (4). And the opacity controls the influence of each Gaussian. The per-pixel color \\(\\hat{C}\\) can be obtained by blending N ordered Gaussians: \\(\\hat{C}=\\sum_{i\\in N}c_{i}\\sigma_{i}\\prod_{j=1}^{i-1}(1-\\sigma_{i})\\).\n' +
      '\n' +
      '### Accelerating the Optimization\n' +
      '\n' +
      'The optimization of Gaussian Splatting is based on successive iterations of rendering and comparing the resulting image to the training views. 3D Gaussians are first initialized from either Structure-from-Motion (SfM) or random sampling. Inevitably, geometry may be incorrectly placed due to the ambiguities of 3D to 2D projection. The optimization process thus needs to be able to adaptively create geometry and also remove geometry (termed as _split_ and _clone_) if it is incorrectly positioned.\n' +
      '\n' +
      'However, the split and clone operations proposed by the original work [21] overlook the distance between 3D Gaussians, during the optimization process which significantly slows down the process. We observe that if two Gaussians are close to each other, even if the positional gradients are larger than a threshold, they should not be split or cloned since these Gaussians are updating their \n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      'to simplify the calculation of Eq. (5). Details of GDS will be discussed in the Supplementary Materials.\n' +
      '\n' +
      '### Epipolar Attention for Multi-view Rendering\n' +
      '\n' +
      'Previous methods [50, 70] usually use a single input image for coarse Gaussian Splatting, which requires further refinement or repainting in unseen regions. The intuitive idea is to exploit the generated consistent multi-view images to reconstruct high-quality 3D objects. However, relying solely on cross-attention to communicate between images of multiple viewpoints is insufficient. Therefore, given a sequence of generated views, we propose epipolar attention to allow association between the features of different views. The epipolar line for a given feature point in one view is the line on which the corresponding feature point in the other view must lie, based on the known geometric relationship between two views. It acts as a constraint to reduce the number of potential pixels in one view that can attend to another view. We present the illustration of epipolar line and epipolar attention in Fig. 4. By enforcing this constraint, we can limit the search space for corresponding features in different views, making the association process more efficient and accurate.\n' +
      '\n' +
      'Consider the intermediate UNet feature \\(f_{s}\\), we can compute its corresponding epipolar lines \\(\\{l_{t}\\}_{t\\neq s}\\) on the feature map of all other views \\(\\{f_{t}\\}_{t\\neq s}\\) (please refer to Supplementary Materials for the details). Each point \\(p\\) on \\(f_{s}\\) will only access the features that lie along the camera ray (in other views) as all points in its own views during rendering. We then estimate the weight maps for all positions in \\(f_{s}\\), stack these maps, and get the epipolar weight matrix \\(M_{st}\\). Finally, the output of the epipolar attention layer \\(\\hat{f}_{s}\\) can be formulated as:\n' +
      '\n' +
      '\\[\\hat{f}_{s}=\\texttt{SoftMax}\\bigg{(}\\frac{f_{s}M_{st}^{T}}{\\sqrt{d}}\\bigg{)}M _{st}. \\tag{6}\\]\n' +
      '\n' +
      'In this way, our proposed epipolar attention mechanism facilitates the efficient and accurate association of features across multiple views. By constraining the search space to the epipolar lines, we effectively reduce the computation cost as well as eliminate potential artifacts.\n' +
      '\n' +
      'Figure 4: **Illustration of epipolar line and epipolar attention** The epipolar line for a given feature point in one view is the line on which the corresponding feature point in the other view must lie, based on the known geometric transformation.\n' +
      '\n' +
      '### Loss Function\n' +
      '\n' +
      'During the training of the reconstruction stage, we suppose each reference image \\(\\mathcal{I}_{ref}\\) has \\(N\\) corresponding views \\(\\mathcal{I}\\) with the relative camera pose change \\(\\Delta\\pi\\). Then we feed the reference image \\(\\mathcal{I}_{ref}\\) into the network, and minimize the average reconstruction loss of target view \\(\\mathcal{I}^{(s)}\\):\n' +
      '\n' +
      '\\[\\mathcal{L}_{rec}=\\frac{1}{N}\\sum_{s=1}^{N}\\|\\mathcal{I}^{(s)}-g(f(\\mathcal{I} _{ref}),\\Delta\\pi^{(s)})\\|^{2}, \\tag{7}\\]\n' +
      '\n' +
      'where \\(g\\) is the renderer that maps the set of Gaussians to an image and \\(f\\) is an inverse function that reconstructs the mixture of Gaussians from an image.\n' +
      '\n' +
      'The efficiency of our method stems from the idea that it renders the entire image at each training iteration. Therefore, instead of decomposing the results into pixels, we can leverage image-level losses as a whole. In practice, we employ SSIM loss to ensure the structural similarity between ground truth and synthesized images, and LPIPS loss for image quality, _i.e_.\n' +
      '\n' +
      '\\[\\mathcal{L}=\\mathcal{L}_{rec}+\\lambda_{1}\\mathcal{L}_{SSIM}+\\lambda_{2} \\mathcal{L}_{LPIPS}, \\tag{8}\\]\n' +
      '\n' +
      'where \\(\\lambda_{1}\\) and \\(\\lambda_{2}\\) are the hyper-parameters of loss weights. Empirically, we set \\(\\lambda_{1}=0.02\\) and \\(\\lambda_{2}=0.01\\) as default.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '#### 4.1.1 Implementation Details\n' +
      '\n' +
      'The two stages are trained separately. For the generation stage, we use the similar network structure of [30, 32] and initialize the weight with Zero-1-to-3 pre-trained weights for training efficiency. We utilize a Vision Transformer (ViT) model of depth 6 as the reference image encoder and generate an output of size \\(1024\\times 256\\). The decoding process involves two decoders, _i.e_. image plane decoder and orthogonal plane decoder, each with a depth of three and outputs a feature map \\(F\\in\\mathbb{R}^{128\\times 128\\times 64}\\). After the multi-view generation, we directly adopt the implementation of [67] to select 16 views with the highest perceptual quality score. For the reconstruction stage, the network that maps the input images to the mixtures of Gaussians is architecturally identical to the UNet [46]. The last layer is replaced with a \\(1\\times 1\\) convolutional layer with 15 output channels. As mentioned in Sec. 3.4, in order to allow the network to coordinate and exchange information between views, we add epipolar attention blocks after residual blocks followed by the cross-attention layers. We use the AdamW optimizer with \\(\\beta_{1}=0.9\\) and \\(\\beta_{2}=0.999\\) with a learning rate of \\(10^{-4}\\). All experiments are performed and measured under NVIDIA V100 (16GB) GPU.\n' +
      '\n' +
      '#### 4.2.2 Datasets\n' +
      '\n' +
      'We train our diffusion model on the recently released Objaverse [11] dataset, which is a large-scale CAD dataset containing 800K high-quality objects. We directly employ the processed rendering data from Zero-1-to-3, which provides 12 random views of each object. For evaluation, we use the test split of Objaverse provided by Zero-1-to-3. In addition, to test the performance of our model on the out-of-distribution data, we also evaluate the Google Scanned Object dataset [12], which contains high-quality scanned household items. During the training stage, images are resized to \\(256\\times 256\\) resolution.\n' +
      '\n' +
      '#### 4.2.3 Baselines\n' +
      '\n' +
      'We mainly evaluate our approach against methods that can generalize to open-set categories and accept single-view RGB images as inputs. In particular, we adopt Zero-1-to-3 [30], Realfusion [34], Consistent-123 [68], Shap-E [20], and DreamGaussian [50] as baseline methods. Zero-1-to-3 is able to synthesize novel views conditioned on viewpoints without training data. Realfusion is based on Stable Diffusion and utilizes SDS loss for single-view reconstruction. Shap-E converts the single input image into a point cloud encoded in MLP and is trained on the OpenAI 3D dataset. DreamGaussain leverages 3D Gaussian Splatting and diffusion priors during reconstruction, which greatly improves the speed. We adopt the implementation of ThreeStudio [17] for reconstruction with Zero-1-to-3, which achieves better performance than the original implementation. For other works, we use their officially released code for quantitative and qualitative evaluation.\n' +
      '\n' +
      '#### 4.2.4 Evaluation metrics\n' +
      '\n' +
      'We mainly focus on two tasks, _i.e_. novel view synthesis (NVS) and single image 3D reconstruction (1-to-3). On the NVS task, we use\n' +
      '\n' +
      'Figure 5: Qualitative comparison of 3D reconstruction results with baselines.\n' +
      '\n' +
      'Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM) [54], and Learned Perceptual Image Patch Similarity (LPIPS) [71] to measure the similarity between rendered images and ground truth images. On the 1-to-3 task, we report the commonly used Chamfer Distance (CD) and CLIP similarity [41].\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '#### 4.2.1 Comparison on novel view synthesis\n' +
      '\n' +
      'As shown in Tab. 1, FDGaussian surpasses all baseline methods regarding PSNR, LPIPS, and SSIM, indicating it provides a sharper and more accurate reconstruction. The qualitative result of our method is demonstrated in Fig. 3. The nearby views synthesized by FDGaussian are geometrically and semantically similar to the reference view, while the views with large viewpoint change showcase reasonable diversity. Furthermore, the orthogonal-plane decomposition mechanism enables our model to capture the details of the input image (discussed in Sec. 4.3).\n' +
      '\n' +
      '#### 4.2.2 Comparison on 3D reconstruction\n' +
      '\n' +
      'For the single-image 3D reconstruction task, we show the results in Tab. 2. FDGaussian outperforms competing approaches by a substantial margin. By leveraging the pruning techniques, we\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c c} \\hline \\multirow{2}{*}{Methods} & \\multicolumn{4}{c}{Objaverse} & \\multicolumn{4}{c}{Google Scanned Objects} \\\\ \\cline{2-7}  & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline Zero-1-to-3 [30] & 18.68 & 0.883 & 0.189 & 18.37 & 0.877 & 0.212 \\\\ Realfusion [34] & 18.95 & 0.882 & 0.167 & 15.26 & 0.722 & 0.283 \\\\ Consistent-123 [68] & 20.72 & 0.877 & 0.122 & 19.46 & 0.858 & 0.146 \\\\ DreamGaussian [50] & 21.53 & 0.915 & 0.122 & 19.93 & 0.895 & 0.177 \\\\ FDGaussian(**Ours**) & 23.97 & 0.921 & 0.113 & 22.98 & 0.899 & 0.146 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **The quantitative comparison for novel-view synthesis.** We color each cell as best and second best. We report PSNR, SSIM, and LPIPS on Objaverse [11] and GSO [12] datasets. The proposed FDGaussian significantly improves the view consistency compared with baselines by a large margin.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c} \\hline \\multicolumn{5}{c}{CLIP Sim.\\(\\uparrow\\)} & CD\\(\\downarrow\\) & Avg. Time\\(\\downarrow\\) \\\\ \\hline Shap-E [20] & \\multicolumn{4}{c}{68.4} & 0.0636 & **1min** \\\\ Zero-1-to-3 [30] & NeRF-based & 79.1 & 0.0339 & 30min \\\\ Realfusion [34] & & 71.5 & 0.0819 & 20min \\\\ \\hline DreamGaussian [50] & \\multirow{2}{*}{GS-based} & 75.8 & 0.0246 & 2min \\\\ FDGaussian(**Ours**) & & 80.0 & 0.0232 & 70s \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **The quantitative comparison for single-view 3D reconstruction.** We report CLIP similarity, Chamfer Distance and the (overall) reconstruction time on GSO [12] dataset.\n' +
      '\n' +
      'further reduce the overall reconstruction time to about 70 seconds. Fig. 5 displays the qualitative comparison results between our method and the baselines. From the visual comparison, we discover that Shap-E is susceptible to collapsing during the generation process, resulting in an unpredictable and blurring outcome. Zero-1-to-3 suffers from multi-view inconsistency. Although Dream-Gaussian keeps the generated results consistent and semantically faithful to the reference image, it fails to synthesize intricate details and often produces over-smooth textures. This demonstrates the superiority of FDGassian over the current state-of-the-art methods and its capacity to generate high-quality 3D objects even with complex structures.\n' +
      '\n' +
      '### Ablations and Analyses\n' +
      '\n' +
      '#### 4.3.1 Overall ablation study\n' +
      '\n' +
      'FDGassian is composed of two stages: geometric-aware multi-view generation and the following Gaussian Splatting reconstruction. We present the qualitative ablation results in Fig. 6. The orthogonal-plane decomposition mechanism plays a crucial role in generating geometrically consistent novel views. CLIP embedding also helps to eliminate the artifacts of the synthesized views while preserving the semantic fidelity to the reference image. The epipolar attention demonstrates great importance in fusing images from various viewpoints. Without it, even highly consistent multi-view images can lead to inconsistent 3D results.\n' +
      '\n' +
      '#### 4.3.2 Ablations of multi-view generation\n' +
      '\n' +
      'Our multi-view generation stage mainly consists of geometric and semantic guidance. Removing them respectively or simultaneously gives us four different combinations. As shown in Tab. 3 and Fig. 6, the orthogonal-plane decomposition mechanism contributes the most to\n' +
      '\n' +
      'Figure 6: Qualitative ablation study of different components.\n' +
      '\n' +
      'the geometric accuracy and consistency, bringing about visual enhancement to a great extent. The semantic guidance further increases the metric score and slightly improves visual consistency.\n' +
      '\n' +
      '#### 4.3.2 Number of synthesized views\n' +
      '\n' +
      'We adopt the Chamfer distance (CD) to evaluate the quality of reconstruction. As shown in Tab. 4, we find that as the number of synthesized views increased, the quality of 3D reconstruction is enhanced accordingly but the time required does not increase much. This is expected since there is more overlapping and interaction across views. However, when the number of views reaches 32, the total time increases dramatically while the quality improvement is insignificant. This might be because over-sufficient views could become the bottleneck of computation.\n' +
      '\n' +
      '#### 4.3.3 Acceleration of the optimization\n' +
      '\n' +
      'As mentioned in Sec. 3.3, we propose to use the Gaussian Divergent Significance (GDS) measure to further regularize the split and clone process. As demonstrated in Tab. 5, this strategy has significantly reduced the optimization time while not sacrificing the reconstruction quality, leading to at most \\(15\\times\\) faster convergence speed when compared with the original split and clone operation proposed in [21].\n' +
      '\n' +
      '### Compatibility with Text-to-Image Models\n' +
      '\n' +
      'FDGaussian seamlessly integrates with the off-the-shelf text-to-image models [43, 44]. These models convert textual descriptions into 2D images, which our\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\# of views & CD & gene. time recon. time \\\\ \\hline\n' +
      '4 & 0.0552 & 9s & 52s \\\\\n' +
      '8 & 0.0327 & 10s & 53s \\\\\n' +
      '16 & 0.0233 & 15s & 55s \\\\\n' +
      '32 & 0.0232 & 21s & 68s \\\\ \\hline \\end{tabular} \n' +
      '\\begin{tabular}{c c c} \\hline Threshold & CD & recon. time \\\\ \\hline w/o GDS & 0.0234 & 15min \\\\\n' +
      '0.01 & 0.0232 & 93s \\\\\n' +
      '0.1 & 0.0233 & 55s \\\\\n' +
      '0.5 & 0.0235 & 78s \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: The quantitative comparison of different numbers of synthesized views. Here the number of views includes the reference view. The generation time refers to the time of multi-view generation.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline id geometric cond. CLIP embedding & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline a & ✓ & ✓ & **22.98** & **0.899** & **0.146** \\\\ b & ✓ & ✗ & 20.79 & 0.878 & 0.175 \\\\ c & ✗ & ✓ & 18.37 & 0.877 & 0.212 \\\\ d & ✗ & ✗ & 17.05 & 0.801 & 0.203 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Ablation studies of multi-view generation. Evaluated on the GSO dataset [12].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline Threshold & CD & recon. time \\\\ \\hline w/o GDS & 0.0234 & 15min \\\\\n' +
      '0.01 & 0.0232 & 93s \\\\\n' +
      '0.1 & 0.0233 & 55s \\\\\n' +
      '0.5 & 0.0235 & 78s \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Quantitative results of ablating GDS metric. A proper choice of GDS threshold leads to significant efficiency improvement.\n' +
      '\n' +
      'model further transforms into high-quality multi-view images and Gaussian representations. Visual examples are shown in Fig. 7. Notably, our model excels in reconstructing the essence of the given 2D image, even managing to capture details for occluded parts.\n' +
      '\n' +
      '### Limitations and Future Works\n' +
      '\n' +
      'While FDGaussian shows promising results in reconstructing 3D objects from single-view images, there are still some limitations that the current framework does not entirely address. First, the number of generated views is fixed in our method. Adaptively generating different numbers of views for objects with different topological symmetries might further reduce the total reconstruction time. Additionally, our current method is restricted to single-object 3D reconstruction. It remains to be extended to complex scenes or multi-object reconstruction in the future.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      'In this work, we proposed a two-stage model named FDGaussian to reconstruct 3D objects from single-view images. This method first synthesizes consistent yet 3D-aware multi-view images via a diffusion model under the guidance of an orthogonal-plane decomposition mechanism. Then epipolar attention is leveraged to render with these images during Gaussian Splatting. The novel metric, _i.e_. Gaussian Divergent Significance (GDS), is proposed to accelerate optimization. Qualitative and quantitative results show that the proposed method reconstructs 3D Gaussian representations that 1) are consistent in different viewpoints, 2) are high fidelity to the reference image, and 3) display plausible creativity in the unseen areas.\n' +
      '\n' +
      'Figure 7: **Text-to-3D.** FDGaussian, when combined with text-to-image models [43, 44], enables text-to-3D.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Achlioptas, P., Diamanti, O., Mitliagkas, I., Guibas, L.: Learning representations and generative models for 3d point clouds. In: International conference on machine learning. pp. 40-49. PMLR (2018)\n' +
      '* [2] Chen, A., Xu, Z., Geiger, A., Yu, J., Su, H.: Tensorf: Tensorial radiance fields. In: European Conference on Computer Vision. pp. 333-350. Springer (2022)\n' +
      '* [3] Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873 (2023)\n' +
      '* [4] Chen, Y., Fang, J., Huang, Y., Yi, T., Zhang, X., Xie, L., Wang, X., Dai, W., Xiong, H., Tian, Q.: Cascade-zero123: One image to highly consistent 3d with self-prompted nearby views. arXiv preprint arXiv:2312.04424 (2023)\n' +
      '* [5] Chen, Z., Funkhouser, T., Hedman, P., Tagliasacchi, A.: Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16569-16578 (2023)\n' +
      '* [6] Chen, Z., Zhang, H.: Learning implicit fields for generative shape modeling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5939-5948 (2019)\n' +
      '* [7] Chen, Z., Wang, F., Liu, H.: Text-to-3d using gaussian splatting. arXiv preprint arXiv:2309.16585 (2023)\n' +
      '* [8] Chibane, J., Alldieck, T., Pons-Moll, G.: Implicit functions in feature space for 3d shape reconstruction and completion. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 6970-6981 (2020)\n' +
      '* [9] Chibane, J., Pons-Moll, G., et al.: Neural unsigned distance fields for implicit function learning. Advances in Neural Information Processing Systems **33**, 21638-21652 (2020)\n' +
      '* [10] Choy, C.B., Xu, D., Gwak, J., Chen, K., Savarese, S.: 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In: Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14. pp. 628-644. Springer (2016)\n' +
      '* [11] Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3d objects. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13142-13153 (2023)\n' +
      '* [12] Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K., McHugh, T.B., Vanhoucke, V.: Google scanned objects: A high-quality dataset of 3d scanned household items. In: 2022 International Conference on Robotics and Automation (ICRA). pp. 2553-2560. IEEE (2022)\n' +
      '* [13] Fan, H., Su, H., Guibas, L.J.: A point set generation network for 3d object reconstruction from a single image. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 605-613 (2017)\n' +
      '* [14] Fridovich-Keil, S., Yu, A., Tancik, M., Chen, Q., Recht, B., Kanazawa, A.: Plenoxels: Radiance fields without neural networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5501-5510 (2022)\n' +
      '* [15] Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M.: Atlasnet: A papiermache approach to learning 3d surface generation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 216-224 (2018)* [16] Guillard, B., Stella, F., Fua, P.: Meshudf: Fast and differentiable meshing of unsigned distance field networks. In: European Conference on Computer Vision. pp. 576-592. Springer (2022)\n' +
      '* [17] Guo, Y.C., Liu, Y.T., Shao, R., Laforte, C., Voleti, V., Luo, G., Chen, C.H., Zou, Z.X., Wang, C., Cao, Y.P., Zhang, S.H.: threestudio: A unified framework for 3d content generation. [https://github.com/threestudio-project/threestudio](https://github.com/threestudio-project/threestudio) (2023)\n' +
      '* [18] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in neural information processing systems **33**, 6840-6851 (2020)\n' +
      '* [19] Jain, A., Mildenhall, B., Barron, J.T., Abbeel, P., Poole, B.: Zero-shot text-guided object generation with dream fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 867-876 (2022)\n' +
      '* [20] Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463 (2023)\n' +
      '* [21] Kerbl, B., Kopanas, G., Leimkuhler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics **42**(4) (2023)\n' +
      '* [22] Kopf, J., Matzen, K., Alsisan, S., Quigley, O., Ge, F., Chong, Y., Patterson, J., Frahm, J.M., Wu, S., Yu, M., et al.: One shot 3d photography. ACM Transactions on Graphics (TOG) **39**(4), 76-1 (2020)\n' +
      '* [23] Lee, H.H., Chang, A.X.: Understanding pure clip guidance for voxel grid nerf models. arXiv preprint arXiv:2209.15172 (2022)\n' +
      '* [24] Li, S., Li, C., Zhu, W., Yu, B., Zhao, Y., Wan, C., You, H., Shi, H., Lin, Y.: Instant-3d: Instant neural radiance field training towards on-device ar/vr 3d reconstruction. In: Proceedings of the 50th Annual International Symposium on Computer Architecture. pp. 1-13 (2023)\n' +
      '* [25] Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 300-309 (2023)\n' +
      '* [26] Lin, Y., Han, H., Gong, C., Xu, Z., Zhang, Y., Li, X.: Consistent123: One image to highly consistent 3d asset using case-aware diffusion priors. arXiv preprint arXiv:2309.17261 (2023)\n' +
      '* [27] Liu, H., Zheng, Y., Chen, G., Cui, S., Han, X.: Towards high-fidelity single-view holistic reconstruction of indoor scenes. In: European Conference on Computer Vision. pp. 429-446. Springer (2022)\n' +
      '* [28] Liu, M., Shi, R., Chen, L., Zhang, Z., Xu, C., Wei, X., Chen, H., Zeng, C., Gu, J., Su, H.: One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885 (2023)\n' +
      '* [29] Liu, M., Xu, C., Jin, H., Chen, L., Xu, Z., Su, H., et al.: One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. arXiv preprint arXiv:2306.16928 (2023)\n' +
      '* [30] Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9298-9309 (2023)\n' +
      '* [31] Liu, Y.T., Wang, L., Yang, J., Chen, W., Meng, X., Yang, B., Gao, L.: Neudf: Leaning neural unsigned distance fields with volume rendering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 237-247 (2023)* [32] Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: Syncdreamer: Generating multiview-consistent images from a single-view image. arXiv preprint arXiv:2309.03453 (2023)\n' +
      '* [33] Maturana, D., Scherer, S.: Voxnet: A 3d convolutional neural network for real-time object recognition. In: 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS). pp. 922-928. IEEE (2015)\n' +
      '* [34] Melas-Kyriazi, L., Laina, I., Rupprecht, C., Vedaldi, A.: Realfusion: 360deg reconstruction of any object from a single image. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8446-8455 (2023)\n' +
      '* [35] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks: Learning 3d reconstruction in function space. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4460-4470 (2019)\n' +
      '* [36] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM **65**(1), 99-106 (2021)\n' +
      '* [37] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022)\n' +
      '* [38] Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for 3d classification and segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 652-660 (2017)\n' +
      '* [39] Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems **30** (2017)\n' +
      '* [40] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [41] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [42] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog **1**(8), 9 (2019)\n' +
      '* [43] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 **1**(2), 3 (2022)\n' +
      '* [44] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [45] SCHIED, C.: Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph **1145**, 3528223-3530127 (2022)\n' +
      '* [46] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020)\n' +
      '* [47] Song, Y., Ermon, S.: Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems **32** (2019)\n' +
      '* [48] Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.: Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 (2020)* [49] Szymanowicz, S., Rupprecht, C., Vedaldi, A.: Splatter image: Ultra-fast single-view 3d reconstruction. arXiv preprint arXiv:2312.13150 (2023)\n' +
      '* [50] Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)\n' +
      '* [51] Tseng, H.Y., Li, Q., Kim, C., Alsisan, S., Huang, J.B., Kopf, J.: Consistent view synthesis with pose-guided diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16773-16783 (2023)\n' +
      '* [52] Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., Jiang, Y.G.: Pixel2mesh: Generating 3d mesh models from single rgb images. In: Proceedings of the European conference on computer vision (ECCV). pp. 52-67 (2018)\n' +
      '* [53] Wang, Y., Long, Y., Fan, S.H., Dou, Q.: Neural rendering for stereo 3d reconstruction of deformable tissues in robotic surgery. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 431-441. Springer (2022)\n' +
      '* [54] Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing **13**(4), 600-612 (2004)\n' +
      '* [55] Wen, C., Zhang, Y., Li, Z., Fu, Y.: Pixel2mesh++: Multi-view 3d mesh generation via deformation. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 1042-1051 (2019)\n' +
      '* [56] Weng, H., Yang, T., Wang, J., Li, Y., Zhang, T., Chen, C., Zhang, L.: Consistent123: Improve consistency for one image to 3d object synthesis. arXiv preprint arXiv:2310.08092 (2023)\n' +
      '* [57] Woo, S., Park, B., Go, H., Kim, J.Y., Kim, C.: Harmonyview: Harmonizing consistency and diversity in one-image-to-3d. arXiv preprint arXiv:2312.15980 (2023)\n' +
      '* [58] Worchel, M., Diaz, R., Hu, W., Schreer, O., Feldmann, I., Eisert, P.: Multi-view mesh reconstruction with neural deferred shading. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6187-6197 (2022)\n' +
      '* [59] Wu, J., Wang, Y., Xue, T., Sun, X., Freeman, B., Tenenbaum, J.: Marrnet: 3d shape reconstruction via 2.5 d sketches. Advances in neural information processing systems **30** (2017)\n' +
      '* [60] Xie, H., Yao, H., Sun, X., Zhou, S., Zhang, S.: Pix2vox: Context-aware 3d reconstruction from single and multi-view images. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 2690-2698 (2019)\n' +
      '* [61] Xing, Z., Chen, Y., Ling, Z., Zhou, X., Xiang, Y.: Few-shot single-view 3d reconstruction with memory prior contrastive network. In: European Conference on Computer Vision. pp. 55-70. Springer (2022)\n' +
      '* [62] Xing, Z., Dai, Q., Hu, H., Wu, Z., Jiang, Y.G.: Simda: Simple diffusion adapter for efficient video generation. arXiv preprint arXiv:2308.09710 (2023)\n' +
      '* [63] Xing, Z., Dai, Q., Zhang, Z., Zhang, H., Hu, H., Wu, Z., Jiang, Y.G.: Vidiff: Translating videos via multi-modal instructions with diffusion models. arXiv preprint arXiv:2311.18837 (2023)\n' +
      '* [64] Xing, Z., Feng, Q., Chen, H., Dai, Q., Hu, H., Xu, H., Wu, Z., Jiang, Y.G.: A survey on video diffusion models. arXiv preprint arXiv:2310.10647 (2023)\n' +
      '* [65] Xing, Z., Li, H., Wu, Z., Jiang, Y.G.: Semi-supervised single-view 3d reconstruction via prototype shape priors. In: European Conference on Computer Vision. pp. 535-551. Springer (2022)* [66] Xu, Q., Wang, W., Ceylan, D., Mech, R., Neumann, U.: Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. Advances in neural information processing systems **32** (2019)\n' +
      '* [67] Yang, S., Wu, T., Shi, S., Lao, S., Gong, Y., Cao, M., Wang, J., Yang, Y.: Maniqa: Multi-dimension attention network for no-reference image quality assessment. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1191-1200 (2022)\n' +
      '* [68] Ye, J., Wang, P., Li, K., Shi, Y., Wang, H.: Consistent-1-to-3: Consistent image to 3d view synthesis via geometry-aware diffusion models. arXiv preprint arXiv:2310.03020 (2023)\n' +
      '* [69] Zhang, C., Cui, Z., Zhang, Y., Zeng, B., Pollefeys, M., Liu, S.: Holistic 3d scene understanding from a single image with implicit representation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8833-8842 (2021)\n' +
      '* [70] Zhang, J., Tang, Z., Pang, Y., Cheng, X., Jin, P., Wei, Y., Yu, W., Ning, M., Yuan, L.: Repair123: Fast and high-quality one image to 3d generation with progressive controllable 2d repainting. arXiv preprint arXiv:2312.13271 (2023)\n' +
      '* [71] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 586-595 (2018)\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:20]\n' +
      '\n' +
      'Finally, the distance between a point \\(p^{s}\\) on the source view image plane and the epipolar line can be computed as\n' +
      '\n' +
      '\\[d(p_{epi},p^{s})=\\frac{\\|(p^{s}-o^{t\\to s})\\times(p^{t\\to s}-o^{t\\to s})\\|}{\\|p^{t \\to s}-o^{t\\to s}\\|}, \\tag{15}\\]\n' +
      '\n' +
      'where \\(\\times\\) and \\(\\|\\cdot\\|\\) indicate vector cross-product and vector norm, respectively. According to the epipolar line, we compute the weight map, where higher pixel values indicate a closer distance to the line\n' +
      '\n' +
      '\\[m_{st}(p^{s})=1-\\texttt{sigmod}(60(d(p_{epi},p^{s})-0.06))\\quad\\forall p^{s} \\in\\mathcal{I}_{s}. \\tag{16}\\]\n' +
      '\n' +
      'We use the constant 60 to make the sigmoid function steep and use the constant 0.06 to include the pixels that are close to the epipolar line. After estimating the weight maps for all positions in \\(f_{s}\\), we stack these maps and reshape them to get the epipolar weight matrix \\(M_{st}\\), which is used to compute the epipolar attention described in the paper.\n' +
      '\n' +
      '### Potential Social Impact\n' +
      '\n' +
      'Our method mainly focuses on object reconstruction and does not involve the use of human data, ensuring that it does not violate human privacy or raise concerns regarding personal data misuse. However, despite this safeguard, it\'s imperative to acknowledge potential negative social impacts. We are committed to ensuring that our technology is not misused to generate fake data or facilitate deceptive practices, such as counterfeiting or cheating. Ethical considerations and responsible deployment are paramount in our research and development efforts.\n' +
      '\n' +
      'Figure 8: More qualitative results on wild images.\n' +
      '\n' +
      'Figure 9: More qualitative results on the GSO dataset.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>