<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# FDGaussian: 기하학적 인식 확산 모델을 통한 단일 영상으로부터 빠른 가우시안 스플래팅\n' +
      '\n' +
      'Qijun Feng\n' +
      '\n' +
      '중국 상하이 1푸단대학교\n' +
      '\n' +
      ' 전흥\n' +
      '\n' +
      '중국 상하이 1푸단대학교\n' +
      '\n' +
      ' 주원우\n' +
      '\n' +
      '중국 상하이 1푸단대학교\n' +
      '\n' +
      ' 유강강\n' +
      '\n' +
      '중국 상하이 1푸단대학교\n' +
      '\n' +
      '중국 상하이 푸단대학교\n' +
      '\n' +
      '중국 상하이 푸단대학교\n' +
      '\n' +
      '중국 상하이 푸단대학교\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '단일 뷰 이미지로부터 상세한 3D 객체를 재구성하는 것은 이용 가능한 제한된 정보로 인해 여전히 어려운 작업으로 남아 있다. 본 논문에서는 단일 영상 3차원 복원을 위한 새로운 2단계 프레임워크인 FDGaussian을 소개한다. 최근의 방법들은 일반적으로 입력 영상으로부터 그럴듯한 새로운 뷰들을 생성하기 위해 미리 훈련된 2D 확산 모델들을 이용하지만, 이들은 다중 뷰 불일치 또는 기하학적 충실도의 결여와 같은 문제들을 직면한다. 이러한 문제점을 극복하기 위해 본 논문에서는 2차원 입력으로부터 3차원 기하학적 특징을 추출하여 일관된 다시점 영상을 생성할 수 있는 직교 평면 분해 기법을 제안한다. 또한, 다양한 시점의 영상을 융합하기 위해 에피폴라 주의를 통합하는 최첨단 가우스 스플래팅을 더욱 가속화한다. 우리는 FDGaussian이 서로 다른 뷰에 걸쳐 높은 일관성을 갖는 이미지를 생성하고 질적으로나 양적으로 고품질의 3D 객체를 재구성한다는 것을 증명한다. 더 많은 예는 우리의 웹사이트 [https://qjfeng.net/FDGaussian/](https://qjfeng.net/FDGaussian/)에서 찾을 수 있다.\n' +
      '\n' +
      '키워드:3D 재구성 가우시안 분할 확산 모델\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '단일 시점 3D 복원은 단일 RGB 이미지로부터 물체의 3D 기하학 및 외형을 복원하는 것을 목표로 한다. 이 작업은 기계가 실제 3D 세계를 이해하고 상호 작용할 수 있도록 하여 가상 현실(VR), 증강 현실(AR)[22, 24] 및 로봇 공학[53]에서 다양한 응용 프로그램을 가능하게 하므로 매우 중요하다.\n' +
      '\n' +
      '3D 재구성의 주요 과제는 고품질 및 효율적인 표현을 확보하는 것이다. 점 구름[1, 13, 38, 39], 복셀[10, 61, 65, 33], 메쉬[52, 15]와 같은 명시적 표현은 직관적이고 변형 친화적인 특성으로 인해 일반적으로 사용되지만 사실적인 외관을 표현하기 위해 고군분투한다. 최근 몇 년 동안, 암시적 표현들(_e.g._, 신경 복사 필드(NeRF) [27, 36, 69])은 이러한 방법들의 연속적 특성이 최적화를 돕기 때문에 큰 성공을 목격했다. 그러나 렌더링에 필요한 확률적 샘플링은 시간이 많이 걸리고 노이즈가 발생할 수 있다.\n' +
      '\n' +
      '이러한 단점을 극복하기 위해 3차원 가우시안 스플래팅(Gaussian Splatting, 21)을 제안하였다. 신경망 기반 최적화와 명시적이고 구조화된 데이터 저장의 이점을 결합하여 경쟁력 있는 훈련 및 추론 시간과 고품질 렌더링을 가능하게 한다. 현재의 방법들[49, 50, 70]은 종종 다수의 뷰들의 공간적 대응성을 무시하고, 가우시안 스플래팅을 위한 단일 이미지를 공급한다. 또한, 가우시안 스플래팅[21]의 원래 구현은 3D 가우시안 사이의 거리를 무시하여 불필요한 분할 및 복제 연산을 많이 야기한다는 것을 관찰한다.\n' +
      '\n' +
      '여러 작업[51, 32]은 물체의 서로 다른 뷰들 사이의 상관 관계를 이해하기 위해 2D 생성기를 미세 회전시키는 것이 3D 재구성을 상당히 용이하게 한다는 것을 보여주었다. 그러나 이러한 방법은 다중 뷰 불일치로 인해 어려움을 겪거나 복잡한 기하학적 구조를 가진 객체를 처리하는데 어려움을 겪는다.\n' +
      '\n' +
      '이를 고려하여, 우리는 기하학적 인식 다시점 생성 단계와 가속화된 3차원 가우시안 재구성 단계로 구성된 단일 영상 3차원 재구성을 위한 새로운 2단계 프레임워크인 **FDGGaussian**를 제안한다. 생성 단계는 3D 인식 및 다시점 일관성 높은 충실도 영상을 합성하는 것을 목표로 한다. 목표를 달성하기 위해, CLIP[40] 인코더로 의미 조건을 얻는 동안 직교 평면들을 디커플링함으로써 3D 특징들이 기하학적 조건으로서 추출된다. 입력 이미지와 함께, 두 조건들이 확산 모델에 공급된다[44]. 재구성 단계에서는 생성된 일관된 뷰를 융합하기 위해 에피폴라 주의를 도입하며, 이는 뷰 간의 근본적인 기하학적 상관 관계를 완전히 활용하여 경쟁적인 시각적 품질 향상을 허용한다. 또한, 최적화 과정을 더욱 가속화하기 위해 불필요한 연산을 피하기 위해 GDS(Gaussian Divergent Significance)라는 새로운 메트릭을 제안한다.\n' +
      '\n' +
      'Objaverse [11] 및 GSO [12] 데이터 세트에 대한 광범위한 실험 및 삭제는 본 방법이 멀티 뷰 일관성과 상세한 기하학을 가진 고품질 3D 객체를 생성할 수 있음을 보여준다. 또한, FDGaussian이 다운스트림 텍스트-투-3D 응용에서 텍스트-투-이미지 모델과 원활하게 통합될 수 있음을 보인다. 본 논문의 주요 기여도는 다음과 같이 요약할 수 있다. * 우리는 다중 뷰 일관되고 기하학적으로 인식 가능한 새로운 뷰 이미지를 합성하기 위해 확산 모델과 직교 평면 분해 메커니즘을 통합한다.\n' +
      '* 일관된 다시점 영상을 최대한 활용하기 위해 렌더링 과정에 에피폴라 주의력을 도입하여 영상 간 효율적이고 효과적인 커뮤니케이션이 가능하도록 한다.\n' +
      '* 우리는 최적화 동안 불필요한 분할 및 클론 연산을 프루닝하기 위해 GDS(Gaussian Divergent Significance)라는 새로운 메트릭을 도출하여 상당한 시간 감소를 달성한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '3차원 복원을 위한### 표현\n' +
      '\n' +
      '###### 2.1.1 명시적 표현\n' +
      '\n' +
      '명시적 표현은 오랫동안 산업 및 학술 연구에서 우세했다. 점 구름[13, 35, 38], 복셀[10, 59, 60, 61, 65, 33, 65], 메시[52, 55, 58, 66]을 포함하는 고전 표현들이 3D 복원을 위해 재조명되었다. 이러한 명시적 표현은 기하학과 외형에 대한 상세한 설명을 제공하지만 기본 위상의 유연성이 부족하고 종종 현실적인 외관을 효과적으로 포착하는 데 어려움을 겪는다.\n' +
      '\n' +
      '###### 2.1.2 암시적 표현\n' +
      '\n' +
      '명시적 표현과는 달리, 부호화된 거리 필드들(SDF) [8, 35, 6] 및 부호화되지 않은 거리 필드들(UDF) [16, 9, 31]을 포함하는 암시적 표현들은 임의의 지오메트리 및 토폴로지를 정확하게 모델링하는 이점을 제공한다. 암시적 표현의 지속적인 특성 덕분에 심층 신경망을 활용하여 데이터 기반 기하학 학습을 지원할 수 있다. 최근, 신경 복사 필드(NeRF) [36]은 볼륨 렌더링을 통한 2D 감독만으로 3D 최적화를 가능하게 하는 고무적인 진전을 입증했다. 그럼에도 불구하고, 암시적 접근은 3D 장면의 암시적 기능에 맞게 광범위한 샘플링으로 인해 어려움을 겪는다. 이것은 특히 고속화된 NeRF 버전[2, 5, 14, 45]을 사용하더라도 고해상도 또는 대화형 렌더링 시나리오에서 상당한 계산 비용을 초래한다. 실시간 렌더링과 고품질 뷰 합성을 동시에 달성하기 어렵다.\n' +
      '\n' +
      '1.3 Gaussian splatting####2.1.3 Gaussian splatting\n' +
      '\n' +
      '앞서 언급한 장애물을 해결하기 위해 가우시안 스플래팅[21]이 대체 표현으로 등장했으며 품질과 속도 측면에서 놀라운 발전을 보여 유망한 길을 제공했다. 일부 방법[50, 70]은 가우시안들을 활용하여 거친 3D 표현을 생성한 다음 참조 정보로 정제하는데, 이는 종종 여분의 깊이 입력 또는 메쉬 추출 과정을 필요로 한다. 다른 방법들[49]은 단안 입력 이미지로부터 3D 표현을 직접 예측함으로써, 그러나 보이지 않는 영역들에서 아티팩트들로 고통받음으로써 이 문제를 완화한다. 이와는 달리 확산 모델의 창의성과 가우시안 스플래팅의 효율성을 결합하여 명시적인 깊이나 정상적인 힌트 없이 비교적 짧은 시간에 고품질의 가우시안 표현을 렌더링할 수 있다.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '2차원 확산 모델을 이용한### 3차원 재구성\n' +
      '\n' +
      '최근 디노이징 확산 확률 모델(DDPM)[46, 47, 48, 62, 48, 63, 64]의 성공으로 관심이 급증했다. 시각 콘텐츠 생성에서 DALL-E2[43], Imagen[42], Stable Diffusion[44]와 같은 언어 유도 이미지 확산 모델은 주어진 텍스트 프롬프트 입력에 강한 의미적 상관 관계를 갖는 사실적 이미지를 생성하는 데 큰 잠재력을 보여주었다. 그들은 우리의 3D 세계에 대한 강력한 전적을 가지고 있으며 3D 생성 작업을 지원하기 위해 2D 이전 모델을 사용하기 위해 증가하는 연구에 영감을 주었다. 많은 방법들은 형상당 최적화(3, 37, 19, 23, 25, 34, 7)의 패러다임을 따른다. 이들은 일반적으로 3D 표현을 최적화하고 기울기 안내를 위해 2D 확산 모델을 활용한다. 인상적인 결과를 얻었지만 이러한 방법은 장기간의 최적화 시간, "다중 표면" 문제, 과포화 색상 및 결과의 다양성 부족으로 고통받는 경향이 있다.\n' +
      '\n' +
      'Zero-1-to-3[30]과 같은 연구에서 강조된 새로운 연구의 물결은 미리 훈련된 2D 확산 모델을 사용하여 단일 이미지 또는 텍스트에서 새로운 뷰를 합성하고 3D 생성을 위한 새로운 문을 열겠다는 약속을 보여주었다. 그럼에도 불구하고, 이전의 방법들[29, 30, 68]에 의해 생성된 멀티뷰 이미지들은 일관성과 기하학적 세부사항들이 부족하다. 우리의 연구는 여러 동시 연구[57, 56, 26, 4, 28, 25]와 함께 참조 이미지의 복잡한 구조를 캡처하면서 다중 뷰 일관성을 개선하는 데 전념한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### 기하인식 다시점 영상 생성\n' +
      '\n' +
      '주어진 카메라 변환 하에서 새로운 이미지들을 합성하기 위해 미리 훈련된 확산 모델들을 재조정하는 것[44]은 유망한 결과들을 입증하였다[30, 57, 68, 32]. 방법[57, 68]의 한 스트림은 이전에 생성된 이미지에 대한 컨디셔닝에 의해 멀티뷰 불일치 문제를 해결하며, 이는 누적 에러 및 감소된 처리 속도에 취약해지는 경향이 있다. 방법 [28, 30]의 또 다른 스트림은 참조 이미지 및 의미 안내만을 사용하여 새로운 뷰를 생성하지만, 붕괴된 기하학 및 제한된 충실도를 겪는다.\n' +
      '\n' +
      '우리는 피벗이 참조 이미지가 제공하는 기하학적 정보를 완전히 활용하는 데 있다고 주장한다. 그러나 단일 2차원 영상에서 직접 3차원 정보를 추출하는 것은 불가능하다. 따라서, 이미지 평면(_i.e_. \\ (xy\\)-plane)인 것을 특징으로 하는 직교 평면들을 디커플링하는 방법. 먼저 비전 트랜스포머를 이용하여 입력 영상을 부호화하고 영상 내의 전체 상관관계를 포착하여 고차원 잠재력\\(\\mathbf{h}\\)을 생성한다. 그런 다음 이미지-평면 디코더와 직교-평면 디코더의 두 디코더를 활용하여 잠재된 특징들로부터 기하학적 인식 특징을 생성한다. 영상-플레인 디코더는 엔코더 출력에서 자기 주목 메커니즘을 이용하여 엔코딩 동작을 반전시켜 \\(F_{xy}\\)으로 변환한다. 이미지 평면과의 구조적 정렬을 유지하면서 직교 평면 피쳐를 생성하기 위해 교차 어텐션 메커니즘을 사용하여 \\(yz\\) 및 \\(xz\\) 평면 피쳐 \\(F_{yz}\\) 및 \\(F_{xz}\\)을 디코딩한다. 다양한 평면에서 디코딩 과정을 용이하게 하기 위해, 새로운 평면들을 디커플링하기 위한 추가적인 정보를 제공하는 학습 가능한 임베딩 \\(\\mathbf{u}\\)을 소개한다. 학습 가능한 임베딩 \\(\\mathbf{u}\\)은 먼저 자기 주의 인코딩을 통해 처리되고, 인코딩된 이미지 잠재 \\(\\mathbf{h}\\)과 교차 주의 메커니즘에서 쿼리로 사용된다. 이미지 특징들은 다음과 같이 교차-어텐션 메커니즘에 대한 키들 및 값들로 변환된다:\n' +
      '\n' +
      '\\texttt{CrossAttn}(\\mathbf{u},\\mathbf{h})=\\texttt{SoftMax}\\bigg{(}\\frac{(W^{Q}\\texttt{ SelfAttn}(\\mathbf{u}))(W^{K}\\mathbf{h})^{T}{\\sqrt{d}\\bigg{}(W^{V}\\mathbf{h}), \\tag{1}\\texttt{\n' +
      '\n' +
      '여기서 \\(W^{Q}\\), \\(W^{K}\\), \\(W^{V}\\)은 학습 가능한 매개변수이고 \\(d\\)은 스케일링 계수이다. 마지막으로, 특징들은 기하학적 조건으로서 결합된다:\n' +
      '\n' +
      '\\[F=F_{xy}(\\texttt{\\textcircled{C}})(F_{yz}+F_{xz}), \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(\\texttt{\\textcircled{C}}\\) 및 \\(+\\)은 각각 연결 및 합산 연산이다.\n' +
      '\n' +
      '백본 설계.** 이전 연구 [18, 44]와 유사하게 엔코더\\(\\mathcal{E}\\), 디노이저 UNet\\(\\epsilon_{\\theta}\\), 디코더\\(\\mathcal{D}\\)을 갖는 잠재 확산 아키텍처를 사용한다. 네트워크는 사전 훈련된 0-1-3[30]의 가중치로부터 초기화된다. [30] 및 [32]에 이어서, 입력 뷰는 UNet에 대한 입력으로서 노이즈 타겟 뷰와 채널-연결된다. 우린 고용했어\n' +
      '\n' +
      '그림 2: **우리의 방법의 개요.** _generation stage_에서 우리는 직교 평면을 디커플링하여 단일 입력 이미지에서 3D 특징을 추출하고 UNet에 공급하여 고품질 멀티뷰 이미지를 생성한다. _재구성 단계_에서 우리는 에피폴라 주의를 사용하여 서로 다른 시점을 가진 이미지를 융합한다. 또한 GDS(Gaussian Divergent Significance)를 활용하여 최적화 과정에서 적응 밀도 제어를 가속화하여 경쟁 훈련 및 추론 시간을 가능하게 한다.\n' +
      '\n' +
      'CLIP 이미지 인코더[40]는 \\(\\mathcal{I}_{ref}\\)를 인코딩하는 반면, CLIP 텍스트 인코더[40]는 \\(\\Delta\\pi\\)를 인코딩하는 데 사용된다. 이 임베딩의 연결은 \\(c(\\mathcal{I}_{ref},\\Delta\\pi)\\)로 표현되며, 프레임워크에서 의미적 조건을 형성한다. 우리는 다음 목적을 최적화함으로써 네트워크를 학습할 수 있다:\n' +
      '\n' +
      '\\mathcal{E}[\\min_{\\theta}\\mathbb{E}_{z\\sim\\mathcal{E}(\\mathcal{I}),t,e\\sim\\mathcal{N}(0,1}\\|\\epsilon-\\epsilon_{\\theta}(z_{t},t,c(\\mathcal{I}_{ref},\\Delta\\pi))\\|_{2}^{2} \\tag{3}\\\\tag{3}\\\\epsilon-\\epsilon_{\\theta}(z_{t},t,c(\\mathcal{I}_{ref},\\Delta\\pi))\\|_{2}^{2}}\\tag{3}\\t,t,e\\sim\\mathcal{N}(0,1}\\|\\epsilon-\\epsilon_{\\theta}(z_{t},t,c(\\mathcal{I}_{ref},\\Delta\\pi))\\|_{2}^{2}\\tag\n' +
      '\n' +
      '### 가우스 분할의 예비\n' +
      '\n' +
      '3차원 가우시안 스플래팅은 3차원 장면 재구성 및 새로운 뷰 합성을 위한 학습 기반 래스터화 기법이다[21]. 각 가우시안 요소는 위치(평균)\\(\\boldsymbol{\\mu}\\), 전체 3차원 공분산 행렬\\(\\boldsymbol{\\sigma}\\), 색상\\(c\\), 불투명도\\(\\sigma\\)으로 정의된다. 가우시안 함수 \\(G(x)\\)는 다음과 같이 공식화될 수 있다:\n' +
      '\n' +
      '[G(x)=exp(-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}}^{T}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})) \\tag{4}\\tag{4}}\n' +
      '\n' +
      '공분산 행렬(\\boldsymbol{\\Sigma}\\)을 3D 벡터(s\\in\\mathbb{R}^{3}\\)로 표현되는 스케일링 행렬(S\\)과 쿼터니온(q\\in\\mathbb{R}^{4}\\)으로 표현되는 회전 행렬(R\\)로 인수분해하여 미분최적화(\\boldsymbol{\\Sigma}=RSS^{T}R^{T}\\)할 수 있다.\n' +
      '\n' +
      '스플래팅의 렌더링 기법은 [21]에서 처음 소개된 바와 같이 가우시안들을 카메라 이미지 평면에 투영하는 것으로 새로운 뷰 이미지를 생성하기 위해 사용된다. 카메라 좌표의 공분산 행렬(\\boldsymbol{\\Sigma}^{\\prime}\\)은 뷰잉 변환이 주어지면 다음과 같다. \\(\\boldsymbol{\\Sigma}^{\\prime}=JW\\boldsymbol{\\Sigma}W^{T}J^{T}\\), 여기서 \\(J\\)는 사영 변환의 아핀 근사치의 자코비안 행렬이다. 3차원 가우시안들을 2차원 영상 공간에 매핑한 후, 각 픽셀과 겹치는 2차원 가우시안들을 계수하여 색상\\(c_{i}\\)과 불투명도\\(\\sigma_{i}\\) 기여도를 계산한다. 구체적으로, 각 가우시안들의 색상은 Eq에 기술된 가우시안 표현에 기초하여 모든 픽셀에 할당된다. (4). 상기 불투명도는 각 가우시안들의 영향을 제어하는 것을 특징으로 하는 플라즈마 디스플레이 패널의 구동 방법. 픽셀당 색상\\(\\hat{C}\\)은 N 순서 가우시안(\\hat{C}=\\sum_{i\\in N}c_{i}\\sigma_{i}\\prod_{j=1}^{i-1}(1-\\sigma_{i})\\)을 블렌딩하여 얻을 수 있다.\n' +
      '\n' +
      '최적화 가속화\n' +
      '\n' +
      '가우스 스플래팅의 최적화는 렌더링의 연속적인 반복 및 결과 이미지를 트레이닝 뷰들과 비교하는 것에 기초한다. 3D 가우시안들은 먼저 SfM(Structural-from-Motion) 또는 랜덤 샘플링으로부터 초기화된다. 불가피하게, 기하학은 3D 내지 2D 투영의 모호성으로 인해 부정확하게 배치될 수 있다. 따라서 최적화 프로세스는 지오메트리를 적응적으로 생성할 수 있어야 하며, 지오메트리가 잘못 위치되면 지오메트리(_split_ 및 _clone_로 명명됨)도 제거할 수 있어야 한다.\n' +
      '\n' +
      '그러나, 원작 [21]이 제안한 분할 및 복제 연산은 최적화 과정에서 3D 가우시안 사이의 거리를 간과하여 공정을 크게 늦춘다. 우리는 두 가우스인이 서로 가까이 있으면 위치 기울기가 임계값보다 크더라도 분할하거나 복제해서는 안 된다는 것을 관찰한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      'Eq의 계산을 단순화합니다. (5). GDS에 대한 자세한 내용은 보충 자료에서 논의될 것이다.\n' +
      '\n' +
      '다시점 렌더링을 위한### 에피폴라 어텐션\n' +
      '\n' +
      '이전 방법들[50, 70]은 일반적으로 거친 가우시안 스플래팅을 위해 단일 입력 이미지를 사용하는데, 이는 보이지 않는 영역들에서 추가적인 정제 또는 재도색을 필요로 한다. 직관적인 아이디어는 생성된 일관된 다시점 이미지를 활용하여 고품질 3D 객체를 재구성하는 것이다. 그러나, 여러 시점의 영상들 간의 의사소통을 위해 교차 주의에만 의존하는 것은 불충분하다. 따라서 생성된 뷰의 시퀀스가 주어지면 서로 다른 뷰의 특징 간의 연관성을 허용하는 에피폴라 주의를 제안한다. 한 뷰에서 주어진 특징점에 대한 에피폴라 선은 두 뷰 사이의 알려진 기하학적 관계에 기초하여, 다른 뷰에서 대응하는 특징점이 놓여야 하는 선이다. 다른 뷰에 참석할 수 있는 한 뷰에서 잠재적인 픽셀의 수를 줄이기 위한 제약으로 작용한다. 그림 4에서 에피폴라 선과 에피폴라 주의의 그림을 제시하며, 이러한 제약을 적용함으로써 서로 다른 뷰에서 해당 피쳐에 대한 검색 공간을 제한하여 연관 프로세스를 보다 효율적이고 정확하게 만들 수 있다.\n' +
      '\n' +
      '중간 UNet 특징\\(f_{s}\\)을 고려한다면, 우리는 다른 모든 뷰의 특징 맵에서 에피폴라 선\\(\\{l_{t}\\}_{t\\neq s}\\)을 계산할 수 있다. (자세한 내용은 보충 자료를 참조) \\(f_{s}\\)의 각 점\\(p\\)은 렌더링 중에 카메라 광선을 따라 있는 피쳐(다른 뷰에서)에만 액세스합니다. 그런 다음, \\(f_{s}\\)의 모든 위치에 대한 가중치 맵을 추정하고, 이 맵들을 스택하고 에피폴라 가중치 매트릭스 \\(M_{st}\\)을 얻는다. 마지막으로, 에피폴라 주의층 \\(\\hat{f}_{s}\\)의 출력은 다음과 같이 공식화될 수 있다:\n' +
      '\n' +
      '\\[\\hat{f}_{s}=\\texttt{SoftMax}\\bigg{(}\\frac{f_{s}M_{st}^{T}{\\sqrt{d}\\bigg{}M_{st}. \\tag{6}\\cigg{\n' +
      '\n' +
      '이러한 방식으로, 제안된 에피폴라 주의 메커니즘은 여러 뷰에 걸쳐 특징의 효율적이고 정확한 연관성을 용이하게 한다. 탐색 공간을 에피폴라 선으로 제한함으로써 계산 비용을 효과적으로 줄일 수 있고 잠재적인 아티팩트를 제거할 수 있다.\n' +
      '\n' +
      '도 4: ** 에피폴라 라인 및 에피폴라 주의** 한 뷰에서의 주어진 특징점에 대한 에피폴라 라인은 알려진 기하학적 변환에 기초하여, 다른 뷰에서의 대응하는 특징점이 놓여야 하는 라인이다.\n' +
      '\n' +
      '### Loss Function\n' +
      '\n' +
      '재구성 단계에서 각 기준 영상(\\(\\mathcal{I}_{ref}\\)은 상대적인 카메라 포즈 변화에 대한 대응 뷰(\\(\\mathcal{I}\\)를 갖는다고 가정한다. 그런 다음 기준 이미지\\(\\mathcal{I}_{ref}\\)를 네트워크에 공급하고 목표 뷰\\(\\mathcal{I}^{(s)}\\)의 평균 재구성 손실을 최소화한다:\n' +
      '\n' +
      '\\frac{1}{N}\\sum_{s=1}^{N}\\|\\mathcal{I}^{(s)}-g(f(\\mathcal{I} _{ref}),\\Delta\\pi^{(s)})\\|^{2}, \\tag{7}\\]\n' +
      '\n' +
      '여기서 \\(g\\)은 가우시안들의 집합을 이미지에 매핑하는 렌더러이고 \\(f\\)은 이미지로부터 가우시안들의 혼합물을 재구성하는 역함수이다.\n' +
      '\n' +
      '이 방법의 효율성은 각 학습 반복에서 전체 이미지를 렌더링한다는 아이디어에서 비롯된다. 따라서 결과를 픽셀로 분해하는 대신 이미지 수준의 손실을 전체적으로 활용할 수 있다. 실제로 지상진실과 합성영상 사이의 구조적 유사성을 보장하기 위해 SSIM 손실과 화질(_i.e_)을 위한 LPIPS 손실을 사용한다.\n' +
      '\n' +
      '\\[\\mathcal{L}=\\mathcal{L}_{rec}+\\lambda_{1}\\mathcal{L}_{SSIM}+\\lambda_{2}\\mathcal{L}_{LPIPS}, \\tag{8}\\]\n' +
      '\n' +
      '여기서 \\(\\lambda_{1}\\) 및 \\(\\lambda_{2}\\)는 손실 가중치의 하이퍼 파라미터이다. 경험적으로 \\(\\lambda_{1}=0.02\\)과 \\(\\lambda_{2}=0.01\\)을 기본값으로 설정하였다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '######4.1.1 구현 상세\n' +
      '\n' +
      '두 단계는 별도로 훈련됩니다. 생성 단계에서는 [30, 32]와 유사한 네트워크 구조를 사용하고 훈련 효율을 위해 Zero-1-to-3 사전 훈련 가중치로 가중치를 초기화한다. 깊이 6의 ViT(Vision Transformer) 모델을 참조 영상 부호화기로 사용하고, 크기 \\(1024\\times 256\\)의 출력을 생성한다. 디코딩 과정은 _i.e_라는 두 개의 디코더를 포함한다. 영상 평면 디코더와 직교 평면 디코더는 각각 깊이가 3이고 특징 맵(F\\in\\mathbb{R}^{128\\times 128\\times 64}\\)을 출력한다. 멀티뷰 생성 후, 우리는 지각 품질 점수가 가장 높은 16개의 뷰를 선택하기 위해 [67]의 구현을 직접 채택한다. 재구성 단계를 위해, 입력 이미지들을 가우시안들의 혼합물들에 매핑하는 네트워크는 UNet[46]과 아키텍처적으로 동일하다. 마지막 레이어는 15개의 출력 채널을 갖는 \\(1\\times 1\\) 컨볼루션 레이어로 대체된다. Sec. 3.4에서 언급한 바와 같이, 네트워크가 뷰들 사이에서 정보를 조정하고 교환할 수 있도록 하기 위해, 우리는 잔여 블록들 다음에 교차-어텐션 계층들 다음에 에피폴라 어텐션 블록들을 추가한다. 학습률은 \\(\\beta_{1}=0.9\\)과 \\(\\beta_{2}=0.999\\)인 AdamW 최적화기를 사용하였다. 모든 실험은 NVIDIA V100(16GB) GPU에서 수행되고 측정된다.\n' +
      '\n' +
      '#### 4.2.2 Datasets\n' +
      '\n' +
      '우리는 800K 고품질 객체를 포함하는 대규모 CAD 데이터세트인 최근 출시된 Objaverse [11] 데이터세트에 대한 확산 모델을 훈련한다. 각 객체의 12개의 랜덤 뷰를 제공하는 Zero-1-to-3의 렌더링 데이터를 직접 사용한다. 이를 위해 Zero-1-to-3에서 제공하는 Objaverse의 테스트 스플릿을 이용하였으며, 배포되지 않은 데이터에 대한 모델의 성능을 테스트하기 위해 고품질 스캔된 가정용품을 포함하는 Google Scanned Object 데이터셋[12]을 평가하였다. 트레이닝 단계에서 영상의 크기는 \\(256\\times 256\\)으로 조정된다.\n' +
      '\n' +
      '#### 4.2.3 Baselines\n' +
      '\n' +
      '우리는 주로 개방형 범주로 일반화하고 단일 뷰 RGB 이미지를 입력으로 받아들일 수 있는 방법에 대해 접근 방식을 평가한다. 특히 기준선 방법으로 Zero-1-to-3[30], Realfusion[34], Consistent-123[68], Shap-E[20], DreamGaussian[50]을 채택하였다. Zero-1-to-3은 학습 데이터 없이 시점에 맞춰진 새로운 뷰를 합성할 수 있다. Realfusion은 Stable Diffusion을 기반으로 하며 단일 뷰 재구성을 위해 SDS 손실을 활용한다. Shap-E는 단일 입력 이미지를 MLP로 인코딩된 포인트 클라우드로 변환하고 OpenAI 3D 데이터 세트에 대해 트레이닝된다. DreamGaussain은 재구성 동안 3D 가우시안 스플래팅 및 확산 전개를 활용하여 속도를 크게 향상시킨다. 본 논문에서는 Zero-1-to-3으로 재구성하기 위한 ThreeStudio[17]의 구현을 채택하여 기존의 구현보다 우수한 성능을 보인다. 다른 작업은 정식으로 공개된 코드를 사용하여 정량적 및 정성적 평가를 수행한다.\n' +
      '\n' +
      '###### 4.2.4 평가지표\n' +
      '\n' +
      '우리는 주로 두 가지 작업인 _i.e_에 중점을 둡니다. 새로운 뷰 합성(NVS) 및 단일 이미지 3D 재구성(1-to-3). NVS 작업에서 우리는\n' +
      '\n' +
      '그림 5: 기준선과 3D 재구성 결과의 질적 비교.\n' +
      '\n' +
      'PSNR(Peak Signal-to-Noise Ratio), SSIM(Structural Similarity Index)[54],LPIPS(Learned Perceptual Image Patch Similarity)[71]를 이용하여 렌더링된 영상과 지상진실 영상의 유사도를 측정한다. 1-to-3 과제에서는 일반적으로 사용되는 Chamfer Distance (CD)와 CLIP 유사도를 보고한다[41].\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '새로운 시각 합성 비교 4.2.1\n' +
      '\n' +
      '탭에 표시된 대로입니다. 1, FDGaussian은 PSNR, LPIPS 및 SSIM에 관한 모든 베이스라인 방법을 능가하여 더 선명하고 정확한 재구성을 제공한다. 이 방법의 정성적 결과는 그림 3에서 입증되었으며, FDGaussian에 의해 합성된 주변 뷰들은 기준 뷰와 기하학 및 의미학적으로 유사하지만, 시점 변화가 큰 뷰들은 합리적인 다양성을 보여준다. 또한, 직교 평면 분해 메커니즘은 본 모델이 입력 이미지의 세부 사항을 캡처할 수 있게 한다(Sec. 4.3에서 논의됨).\n' +
      '\n' +
      '3D 복원의 비교\n' +
      '\n' +
      '단일 영상 3차원 재구성 작업에 대해, Tab. 2. FDGaussian이 경쟁 접근법보다 상당한 여백으로 우수한 결과를 보인다. 프루닝 기술을 활용하여\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c c} \\hline \\multirow{2}{*}{Methods} & \\multicolumn{4}{c}{Objaverse} & \\multicolumn{4}{c}{Google Scanned Objects} \\\\ \\cline{2-7}  & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline Zero-1-to-3 [30] & 18.68 & 0.883 & 0.189 & 18.37 & 0.877 & 0.212 \\\\ Realfusion [34] & 18.95 & 0.882 & 0.167 & 15.26 & 0.722 & 0.283 \\\\ Consistent-123 [68] & 20.72 & 0.877 & 0.122 & 19.46 & 0.858 & 0.146 \\\\ DreamGaussian [50] & 21.53 & 0.915 & 0.122 & 19.93 & 0.895 & 0.177 \\\\ FDGaussian(**Ours**) & 23.97 & 0.921 & 0.113 & 22.98 & 0.899 & 0.146 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **신규-뷰 합성에 대한 정량적 비교.** 각 셀을 최고 및 두 번째로 착색한다. 우리는 Objaverse [11] 및 GSO [12] 데이터 세트에 대한 PSNR, SSIM 및 LPIPS를 보고한다. 제안된 FDGaussian은 기준선에 비해 큰 마진만큼 뷰 일관성을 크게 개선한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c} \\hline \\multicolumn{5}{c}{CLIP Sim.\\(\\uparrow\\)} & CD\\(\\downarrow\\) & Avg. Time\\(\\downarrow\\) \\\\ \\hline Shap-E [20] & \\multicolumn{4}{c}{68.4} & 0.0636 & **1min** \\\\ Zero-1-to-3 [30] & NeRF-based & 79.1 & 0.0339 & 30min \\\\ Realfusion [34] & & 71.5 & 0.0819 & 20min \\\\ \\hline DreamGaussian [50] & \\multirow{2}{*}{GS-based} & 75.8 & 0.0246 & 2min \\\\ FDGaussian(**Ours**) & & 80.0 & 0.0232 & 70s \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **단일 뷰 3D 재구성에 대한 정량적 비교.** GSO [12] 데이터 세트에서 CLIP 유사성, 챔퍼 거리 및 (전체) 재구성 시간을 보고한다.\n' +
      '\n' +
      '전체 재구성 시간을 약 70초로 더 감소시킨다. 도. 도 5는 우리의 방법과 기준선 사이의 질적 비교 결과를 보여준다. 시각적 비교에서 우리는 Shap-E가 생성 과정에서 붕괴되기 쉬우며 예측 불가능하고 흐릿한 결과를 초래한다는 것을 발견했다. Zero-1-to-3은 멀티뷰 불일치로 고통받는다. Dream-Gaussian은 생성된 결과를 참조 이미지에 일관되고 의미적으로 충실하게 유지하지만 복잡한 세부 정보를 합성하지 못하고 종종 과도하게 매끄러운 질감을 생성한다. 이는 복잡한 구조를 가지고도 고품질의 3D 객체를 생성할 수 있는 능력과 현재 최첨단 방법에 비해 FDGassian의 우수성을 보여준다.\n' +
      '\n' +
      '### 분석 및 분석\n' +
      '\n' +
      '4.3.1 전체 절제 연구\n' +
      '\n' +
      'FDGassian은 기하학적 인식 다시점 생성과 가우시안 스플래팅 재구성의 두 단계로 구성된다. 우리는 그림 6의 정성적 절제 결과를 제시하며, 직교 평면 분해 메커니즘은 기하학적으로 일관된 새로운 뷰를 생성하는 데 중요한 역할을 한다. CLIP 임베딩은 또한 참조 이미지에 대한 의미적 충실도를 보존하면서 합성된 뷰들의 아티팩트들을 제거하는 것을 돕는다. 에피폴라 주의는 다양한 관점에서 이미지를 융합하는 데 매우 중요함을 보여준다. 그것이 없다면, 매우 일관된 다시점 이미지조차도 일관성 없는 3D 결과를 초래할 수 있다.\n' +
      '\n' +
      'Multi-view 세대의 설명\n' +
      '\n' +
      '우리의 멀티뷰 생성 단계는 주로 기하학적 및 의미적 안내로 구성된다. 각각 제거하거나 동시에 네 가지 다른 조합을 제공합니다. 탭에 표시된 대로입니다. 도 3 및 도 3을 참조하여 설명한다. 도 6에 있어서, 직교면 분해 기구는,\n' +
      '\n' +
      '그림 6: 다양한 구성 요소에 대한 질적 절제 연구.\n' +
      '\n' +
      '기하학적 정확성과 일관성을 통해 시각적 향상을 크게 가져올 수 있습니다. 의미적 지침은 메트릭 점수를 더 증가시키고 시각적 일관성을 약간 향상시킨다.\n' +
      '\n' +
      '4.3.2 합성 뷰 수\n' +
      '\n' +
      '우리는 재구성 품질을 평가하기 위해 챔퍼 거리(CD)를 채택한다. 탭에 표시된 대로입니다. 도 4를 참조하면, 합성 뷰 수가 증가함에 따라 3차원 재구성의 품질은 향상되지만 소요 시간은 크게 증가하지 않음을 알 수 있다. 이는 뷰 간에 더 많은 중첩 및 상호 작용이 있기 때문에 예상된다. 그러나 조회수가 32회에 도달하면 총 시간이 급격히 증가하는 반면 품질 개선은 미미하다. 너무 많은 뷰가 계산의 병목 현상이 될 수 있기 때문일 수 있습니다.\n' +
      '\n' +
      '최적화의 가속화\n' +
      '\n' +
      'Sec. 3.3에서 언급한 바와 같이, 분할 및 클론 프로세스를 추가로 정규화하기 위해 GDS(Gaussian Divergent Significance) 측정을 사용할 것을 제안한다. 탭에서 설명한 대로입니다. 도 5를 참조하면, 이 방법은 재구성 품질을 희생시키지 않으면서 최적화 시간을 크게 줄였으며, [21]에서 제안한 원래의 분할 및 클론 연산에 비해 최대 15\\(15\\times\\) 더 빠른 수렴 속도를 보였다.\n' +
      '\n' +
      '텍스트-이미지 모델과의### 호환성\n' +
      '\n' +
      'FDGaussian은 기성 텍스트-이미지 모델들과 매끄럽게 통합된다[43, 44]. 이 모델은 텍스트 설명을 2D 이미지로 변환하며, 이는 우리의 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\# of views & CD & gene. time recon. time \\\\ \\hline\n' +
      '4 & 0.0552 & 9s & 52s \\\\\n' +
      '8&0327&10s&53s\\\\\n' +
      '16 & 0233 & 15s & 55s\\\\\n' +
      '32 & 0.0232 & 21s & 68s \\\\ \\hline \\end{tabular}\n' +
      '\\begin{tabular}{c c c} \\hline Threshold & CD & recon. time \\\\ \\hline w/o GDS & 0.0234 & 15min \\\\\n' +
      '0.01 & 0.0232 & 93s\\\\\n' +
      '0.1 & 0.0233 & 55s\\\\\n' +
      '0.5 & 0.0235 & 78s \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 합성 뷰의 상이한 수의 정량적 비교. 여기서 뷰의 개수는 참조 뷰를 포함한다. 생성 시간은 다시점 생성 시간을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline id geometric cond. CLIP embedding & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline a & ✓ & ✓ & **22.98** & **0.899** & **0.146** \\\\ b & ✓ & ✗ & 20.79 & 0.878 & 0.175 \\\\ c & ✗ & ✓ & 18.37 & 0.877 & 0.212 \\\\ d & ✗ & ✗ & 17.05 & 0.801 & 0.203 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 다시점 생성의 절제 연구. GSO 데이터 세트 [12]에서 평가되었습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline Threshold & CD & recon. time \\\\ \\hline w/o GDS & 0.0234 & 15min \\\\\n' +
      '0.01 & 0.0232 & 93s\\\\\n' +
      '0.1 & 0.0233 & 55s\\\\\n' +
      '0.5 & 0.0235 & 78s \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: GDS 메트릭을 삭감한 정량적 결과. GDS 임계값의 적절한 선택은 상당한 효율성 향상으로 이어진다.\n' +
      '\n' +
      '모델은 더 나아가 고품질 멀티뷰 이미지 및 가우시안 표현으로 변환한다. 시각적 예는 그림 7에 나와 있다. 특히, 우리의 모델은 주어진 2D 이미지의 본질을 재구성하는 데 탁월하며, 심지어 폐색된 부분에 대한 세부 사항을 캡처하는 데에도 탁월하다.\n' +
      '\n' +
      '### 한계와 미래 작품\n' +
      '\n' +
      'FDGaussian은 단일 시점 영상으로부터 3차원 객체를 재구성하는 데 유망한 결과를 보여주지만, 현재 프레임워크가 완전히 다루지 못하는 몇 가지 한계가 있다. 먼저, 생성된 뷰의 수는 우리의 방법에서 고정된다. 상이한 위상 대칭을 갖는 객체들에 대해 상이한 수의 뷰들을 적응적으로 생성하는 것은 총 재구성 시간을 더 감소시킬 수 있다. 또한, 본 논문에서 제안하는 방법은 단일 객체 3차원 복원에 국한되어 있다. 향후 복잡한 장면이나 다중 객체 재구성으로 확장해야 한다.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      '본 연구에서는 단일 시점 영상으로부터 3차원 객체를 재구성하기 위해 FDGaussian이라는 2단계 모델을 제안하였다. 이 방법은 먼저 직교 평면 분해 메커니즘의 안내 하에 확산 모델을 통해 일관적이지만 3D 인식 다중 뷰 이미지를 합성한다. 그런 다음 에피폴라 주의를 활용하여 가우시안 스플래팅 동안 이러한 이미지로 렌더링한다. 새로운 메트릭, _i.e_. Gaussian Divergent Significance (GDS)는 최적화를 가속화하기 위해 제안된다. 정성적 및 정량적 결과는 제안된 방법이 1) 서로 다른 시점에서 일관성이 있고, 2) 참조 영상에 대한 충실도가 높으며, 3) 3D 가우시안 표현을 재구성한다는 것을 보여준다. 보이지 않는 영역에 그럴듯한 창의력을 발휘합니다.\n' +
      '\n' +
      '도 7: **Text-to-3D.** FDGaussian, text-to-image 모델[43, 44]과 결합될 때, text-to-3D를 가능하게 한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Achlioptas, P., Diamanti, O., Mitliagkas, I., Guibas, L.: Learning representations and generative models for 3d point clouds. In: International conference on machine learning. pp. 40-49. PMLR (2018)\n' +
      '* [2] Chen, A., Xu, Z., Geiger, A., Yu, J., Su, H.: Tensorf: Tensorial radiance fields. In: European Conference on Computer Vision. pp. 333-350. Springer (2022)\n' +
      '* [3] Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873 (2023)\n' +
      '* [4] Chen, Y., Fang, J., Huang, Y., Yi, T., Zhang, X., Xie, L., Wang, X., Dai, W., Xiong, H., Tian, Q.: Cascade-zero123: One image to highly consistent 3d with self-prompted nearby views. arXiv preprint arXiv:2312.04424 (2023)\n' +
      '* [5] Chen, Z., Funkhouser, T., Hedman, P., Tagliasacchi, A.: Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16569-16578 (2023)\n' +
      '* [6] Chen, Z., Zhang, H.: Learning implicit fields for generative shape modeling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5939-5948 (2019)\n' +
      '* [7] Chen, Z., Wang, F., Liu, H.: Text-to-3d using gaussian splatting. arXiv preprint arXiv:2309.16585 (2023)\n' +
      '* [8] Chibane, J., Alldieck, T., Pons-Moll, G.: Implicit functions in feature space for 3d shape reconstruction and completion. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 6970-6981 (2020)\n' +
      '* [9] Chibane, J., Pons-Moll, G., et al.: Neural unsigned distance fields for implicit function learning. Advances in Neural Information Processing Systems **33**, 21638-21652 (2020)\n' +
      '* [10] Choy, C.B., Xu, D., Gwak, J., Chen, K., Savarese, S.: 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In: Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14. pp. 628-644. Springer (2016)\n' +
      '* [11] Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3d objects. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13142-13153 (2023)\n' +
      '* [12] Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K., McHugh, T.B., Vanhoucke, V.: Google scanned objects: A high-quality dataset of 3d scanned household items. In: 2022 International Conference on Robotics and Automation (ICRA). pp. 2553-2560. IEEE (2022)\n' +
      '* [13] Fan, H., Su, H., Guibas, L.J.: A point set generation network for 3d object reconstruction from a single image. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 605-613 (2017)\n' +
      '* [14] Fridovich-Keil, S., Yu, A., Tancik, M., Chen, Q., Recht, B., Kanazawa, A.: Plenoxels: Radiance fields without neural networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5501-5510 (2022)\n' +
      '* [15] Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M.: Atlasnet: A papiermache approach to learning 3d surface generation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 216-224 (2018)* [16] Guillard, B., Stella, F., Fua, P.: Meshudf: Fast and differentiable meshing of unsigned distance field networks. In: European Conference on Computer Vision. pp. 576-592. Springer (2022)\n' +
      '* [17] Guo, Y.C., Liu, Y.T., Shao, R., Laforte, C., Voleti, V., Luo, G., Chen, C.H., Zou, Z.X., Wang, C., Cao, Y.P., Zhang, S.H.: threestudio: A unified framework for 3d content generation. [https://github.com/threestudio-project/threestudio](https://github.com/threestudio-project/threestudio) (2023)\n' +
      '* [18] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in neural information processing systems **33**, 6840-6851 (2020)\n' +
      '* [19] Jain, A., Mildenhall, B., Barron, J.T., Abbeel, P., Poole, B.: Zero-shot text-guided object generation with dream fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 867-876 (2022)\n' +
      '* [20] Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463 (2023)\n' +
      '* [21] Kerbl, B., Kopanas, G., Leimkuhler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics **42**(4) (2023)\n' +
      '* [22] Kopf, J., Matzen, K., Alsisan, S., Quigley, O., Ge, F., Chong, Y., Patterson, J., Frahm, J.M., Wu, S., Yu, M., et al.: One shot 3d photography. ACM Transactions on Graphics (TOG) **39**(4), 76-1 (2020)\n' +
      '* [23] Lee, H.H., Chang, A.X.: Understanding pure clip guidance for voxel grid nerf models. arXiv preprint arXiv:2209.15172 (2022)\n' +
      '* [24] Li, S., Li, C., Zhu, W., Yu, B., Zhao, Y., Wan, C., You, H., Shi, H., Lin, Y.: Instant-3d: Instant neural radiance field training towards on-device ar/vr 3d reconstruction. In: Proceedings of the 50th Annual International Symposium on Computer Architecture. pp. 1-13 (2023)\n' +
      '* [25] Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 300-309 (2023)\n' +
      '* [26] Lin, Y., Han, H., Gong, C., Xu, Z., Zhang, Y., Li, X.: Consistent123: One image to highly consistent 3d asset using case-aware diffusion priors. arXiv preprint arXiv:2309.17261 (2023)\n' +
      '* [27] Liu, H., Zheng, Y., Chen, G., Cui, S., Han, X.: Towards high-fidelity single-view holistic reconstruction of indoor scenes. In: European Conference on Computer Vision. pp. 429-446. Springer (2022)\n' +
      '* [28] Liu, M., Shi, R., Chen, L., Zhang, Z., Xu, C., Wei, X., Chen, H., Zeng, C., Gu, J., Su, H.: One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885 (2023)\n' +
      '* [29] Liu, M., Xu, C., Jin, H., Chen, L., Xu, Z., Su, H., et al.: One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. arXiv preprint arXiv:2306.16928 (2023)\n' +
      '* [30] Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9298-9309 (2023)\n' +
      '* [31] Liu, Y.T., Wang, L., Yang, J., Chen, W., Meng, X., Yang, B., Gao, L.: Neudf: Leaning neural unsigned distance fields with volume rendering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 237-247 (2023)* [32] Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: Syncdreamer: Generating multiview-consistent images from a single-view image. arXiv preprint arXiv:2309.03453 (2023)\n' +
      '* [33] Maturana, D., Scherer, S.: Voxnet: A 3d convolutional neural network for real-time object recognition. In: 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS). pp. 922-928. IEEE (2015)\n' +
      '* [34] Melas-Kyriazi, L., Laina, I., Rupprecht, C., Vedaldi, A.: Realfusion: 360deg reconstruction of any object from a single image. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8446-8455 (2023)\n' +
      '* [35] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks: Learning 3d reconstruction in function space. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4460-4470 (2019)\n' +
      '* [36] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM **65**(1), 99-106 (2021)\n' +
      '* [37] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022)\n' +
      '* [38] Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for 3d classification and segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 652-660 (2017)\n' +
      '* [39] Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems **30** (2017)\n' +
      '* [40] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [41] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [42] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog **1**(8), 9 (2019)\n' +
      '* [43] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 **1**(2), 3 (2022)\n' +
      '* [44] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [45] SCHIED, C.: Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph **1145**, 3528223-3530127 (2022)\n' +
      '* [46] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020)\n' +
      '* [47] Song, Y., Ermon, S.: Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems **32** (2019)\n' +
      '* [48] Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.: Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 (2020)* [49] Szymanowicz, S., Rupprecht, C., Vedaldi, A.: Splatter image: Ultra-fast single-view 3d reconstruction. arXiv preprint arXiv:2312.13150 (2023)\n' +
      '* [50] Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)\n' +
      '* [51] Tseng, H.Y., Li, Q., Kim, C., Alsisan, S., Huang, J.B., Kopf, J.: Consistent view synthesis with pose-guided diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16773-16783 (2023)\n' +
      '* [52] Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., Jiang, Y.G.: Pixel2mesh: Generating 3d mesh models from single rgb images. In: Proceedings of the European conference on computer vision (ECCV). pp. 52-67 (2018)\n' +
      '* [53] Wang, Y., Long, Y., Fan, S.H., Dou, Q.: Neural rendering for stereo 3d reconstruction of deformable tissues in robotic surgery. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 431-441. Springer (2022)\n' +
      '* [54] Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing **13**(4), 600-612 (2004)\n' +
      '* [55] Wen, C., Zhang, Y., Li, Z., Fu, Y.: Pixel2mesh++: Multi-view 3d mesh generation via deformation. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 1042-1051 (2019)\n' +
      '* [56] Weng, H., Yang, T., Wang, J., Li, Y., Zhang, T., Chen, C., Zhang, L.: Consistent123: Improve consistency for one image to 3d object synthesis. arXiv preprint arXiv:2310.08092 (2023)\n' +
      '* [57] Woo, S., Park, B., Go, H., Kim, J.Y., Kim, C.: Harmonyview: Harmonizing consistency and diversity in one-image-to-3d. arXiv preprint arXiv:2312.15980 (2023)\n' +
      '* [58] Worchel, M., Diaz, R., Hu, W., Schreer, O., Feldmann, I., Eisert, P.: Multi-view mesh reconstruction with neural deferred shading. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6187-6197 (2022)\n' +
      '* [59] Wu, J., Wang, Y., Xue, T., Sun, X., Freeman, B., Tenenbaum, J.: Marrnet: 3d shape reconstruction via 2.5 d sketches. Advances in neural information processing systems **30** (2017)\n' +
      '* [60] Xie, H., Yao, H., Sun, X., Zhou, S., Zhang, S.: Pix2vox: Context-aware 3d reconstruction from single and multi-view images. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 2690-2698 (2019)\n' +
      '* [61] Xing, Z., Chen, Y., Ling, Z., Zhou, X., Xiang, Y.: Few-shot single-view 3d reconstruction with memory prior contrastive network. In: European Conference on Computer Vision. pp. 55-70. Springer (2022)\n' +
      '* [62] Xing, Z., Dai, Q., Hu, H., Wu, Z., Jiang, Y.G.: Simda: Simple diffusion adapter for efficient video generation. arXiv preprint arXiv:2308.09710 (2023)\n' +
      '* [63] Xing, Z., Dai, Q., Zhang, Z., Zhang, H., Hu, H., Wu, Z., Jiang, Y.G.: Vidiff: Translating videos via multi-modal instructions with diffusion models. arXiv preprint arXiv:2311.18837 (2023)\n' +
      '* [64] Xing, Z., Feng, Q., Chen, H., Dai, Q., Hu, H., Xu, H., Wu, Z., Jiang, Y.G.: A survey on video diffusion models. arXiv preprint arXiv:2310.10647 (2023)\n' +
      '* [65] Xing, Z., Li, H., Wu, Z., Jiang, Y.G.: Semi-supervised single-view 3d reconstruction via prototype shape priors. In: European Conference on Computer Vision. pp. 535-551. Springer (2022)* [66] Xu, Q., Wang, W., Ceylan, D., Mech, R., Neumann, U.: Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. Advances in neural information processing systems **32** (2019)\n' +
      '* [67] Yang, S., Wu, T., Shi, S., Lao, S., Gong, Y., Cao, M., Wang, J., Yang, Y.: Maniqa: Multi-dimension attention network for no-reference image quality assessment. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1191-1200 (2022)\n' +
      '* [68] Ye, J., Wang, P., Li, K., Shi, Y., Wang, H.: Consistent-1-to-3: Consistent image to 3d view synthesis via geometry-aware diffusion models. arXiv preprint arXiv:2310.03020 (2023)\n' +
      '* [69] Zhang, C., Cui, Z., Zhang, Y., Zeng, B., Pollefeys, M., Liu, S.: Holistic 3d scene understanding from a single image with implicit representation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8833-8842 (2021)\n' +
      '* [70] Zhang, J., Tang, Z., Pang, Y., Cheng, X., Jin, P., Wei, Y., Yu, W., Ning, M., Yuan, L.: Repair123: Fast and high-quality one image to 3d generation with progressive controllable 2d repainting. arXiv preprint arXiv:2312.13271 (2023)\n' +
      '* [71] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 586-595 (2018)\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:20]\n' +
      '\n' +
      '마지막으로, 소스 뷰 이미지 평면 상의 한 점\\(p^{s}\\)과 에피폴라 라인 사이의 거리는 다음과 같이 계산될 수 있다.\n' +
      '\n' +
      '\\[d(p_{epi},p^{s})=\\frac{\\|(p^{s}-o^{t\\to s})\\times(p^{t\\to s}-o^{t\\to s})\\|}{\\|p^{t\\to s}-o^{t\\to s}\\|}, \\tag{15}\\]\n' +
      '\n' +
      '여기서 \\(\\times\\) 및 \\(\\|\\cdot\\|\\)은 각각 벡터 교차 곱 및 벡터 규범을 나타낸다. 에피폴라 선에 따라 가중치 맵을 계산하며, 여기서 더 높은 픽셀 값은 선에 더 가까운 거리를 나타낸다.\n' +
      '\n' +
      '(d(p_{epi},p^{s})-0.06))\\quad\\forall p^{s}\\in\\mathcal{I}_{s}. \\tag{16}\\\n' +
      '\n' +
      '우리는 상수 60을 사용하여 시그모이드 함수를 가파르게 만들고 상수 0.06을 사용하여 에피폴라 선에 가까운 픽셀을 포함한다. 모든 위치에 대해 \\(f_{s}\\)의 가중치 맵을 추정한 후, 이 맵들을 스택하여 에피폴라 가중치 행렬 \\(M_{st}\\)을 얻기 위해 재구성하며, 이는 논문에 기술된 에피폴라 주의력을 계산하는 데 사용된다.\n' +
      '\n' +
      '잠재적인 사회적 영향\n' +
      '\n' +
      '우리의 방법은 주로 객체 재구성에 초점을 맞추고 인간 데이터의 사용을 포함하지 않으므로 인간의 프라이버시를 침해하거나 개인 데이터 오남용에 대한 우려를 제기하지 않는다. 그러나 이러한 보호 조치에도 불구하고 잠재적인 부정적인 사회적 영향을 인정하는 것이 필수적이다. 우리는 우리의 기술이 가짜 데이터를 생성하거나 위조 또는 부정행위와 같은 기만적인 관행을 촉진하기 위해 잘못 사용되지 않도록 하기 위해 최선을 다하고 있다. 윤리적 고려와 책임 있는 배치가 우리의 연구 개발 노력에 있어 가장 중요하다.\n' +
      '\n' +
      '그림 8: 야생 이미지에 대한 보다 질적인 결과.\n' +
      '\n' +
      '그림 9: GSO 데이터 세트에 대한 보다 정성적 결과.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>