<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# FreeSCo: Zero-Shot Video Translation을 위한 시공간 대응\n' +
      '\n' +
      'Shuai Yang\\({}^{1}\\) Yifan Zhou\\({}^{2}\\) Ziwei Liu\\({}^{2}\\) Chen Change Loy\\({}^{2}\\)\\({}^{\\boxtimes}\\)\n' +
      '\n' +
      '북경대학 컴퓨터공학연구소, 남양기술대학\n' +
      '\n' +
      'williamyang@pku.edu.cn{yifan006, ziwei.liu, ccloy}@ntu.edu.sg\n' +
      '\n' +
      '양수아이가 NTU S-Lab에서 RAP를 했을 때 일을 했어요\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '텍스트 대 이미지 확산 모델의 놀라운 효능은 비디오 도메인에서 잠재적인 응용 프로그램에 대한 광범위한 탐구를 동기화했다. 제로샷 방법은 모델 교육을 필요로 하지 않고 이미지 확산 모델을 비디오로 확장한다. 최근의 방법들은 주로 프레임 간 대응을 주의 메커니즘에 통합하는 것에 초점을 맞추고 있다. 그러나, 유효한 특징들에 대해 어디에 참석할지를 결정하는 것에 부과되는 소프트 제약은 때때로 불충분하여, 시간적 불일치를 초래할 수 있다. 본 논문에서는 보다 강건한 시공간 제약을 확립하기 위해 프레임 간 대응과 함께 프레임 내 대응인 **FRESCO**를 소개한다. 이러한 향상은 프레임들에 걸쳐 의미적으로 유사한 콘텐츠의 보다 일관된 변환을 보장한다. 단순한 주의 지침을 넘어, 우리의 접근법은 입력 비디오와 높은 시공간 일관성을 달성하기 위해 특징의 명시적인 업데이트를 포함하며, 결과 번역 비디오의 시각적 일관성을 크게 향상시킨다. 광범위한 실험은 고품질 일관성 있는 비디오를 생성하는 데 제안된 프레임워크의 효과를 입증하여 기존 제로 샷 방법에 비해 눈에 띄는 개선을 나타낸다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '오늘날 디지털 시대에 짧은 동영상은 엔터테인먼트의 지배적인 형태로 등장했다. 이러한 비디오의 편집 및 예술적 렌더링은 상당한 실질적인 중요성을 가지고 있다. 최근 확산 모델[33, 34, 36]의 발전은 사용자가 자연어 프롬프트를 통해 편리하게 이미지를 조작할 수 있게 함으로써 이미지 편집에 혁명을 일으켰다. 이미지 도메인에서의 이러한 보폭에도 불구하고, 비디오 조작은 특히 시간적 일관성을 갖는 자연적 움직임을 보장하는 데 있어 독특한 과제를 계속 제기한다.\n' +
      '\n' +
      '시간-간섭성 모션은 광범위한 비디오 데이터세트 [6, 18, 38]에서 비디오 모델을 훈련시키거나 단일 비디오 [25, 37, 44]에서 리팩토링된 이미지 모델을 미세 조정함으로써 학습될 수 있지만, 이는 비용 효율적이지도 않고 일반 사용자에게 편리하지도 않다. 대안적으로, 제로-샷 방법들[4, 5, 11, 23, 31, 41, 47]은 여분의 시간적 일관성 제약들을 갖는 이미지 모델들의 추론 프로세스를 변경함으로써 비디오 조작을 위한 효율적인 방법을 제공한다. 효율성 외에도 제로샷 방식은 이미지 모델을 위해 설계된 다양한 보조 기법, 예를 들어, ControlNet[49] 및 LoRA[19]와 호환성이 높아 보다 유연한 조작이 가능하다는 장점이 있다.\n' +
      '\n' +
      '기존의 제로 샷 방법은 주로 주의 메커니즘을 정제하는 데 집중한다. 이러한 기술들은 종종 자기-관심들을 크로스-프레임 관심들[23, 44]로 대체하며, 다수의 프레임들에 걸쳐 특징들을 집계한다. 그러나 이 접근 방식은 거친 수준의 글로벌 스타일 일관성만을 보장합니다. 더 정제된 시간적 일관성을 달성하기 위해, Rerender-A-Video[47] 및 FLATTEN[5]와 같은 접근법들은 생성된 비디오가 원본과 동일한 프레임간 대응관계를 유지한다고 가정한다. 그들은 특징 융합 프로세스를 안내하기 위해 원래 비디오의 광학 흐름을 통합한다. 이 전략은 가능성을 보여주지만, 세 가지 문제는 해결되지 않은 채로 남아 있다. **1) 비일관성.** 조작 중 광학 흐름의 변화는 일관되지 않은 안내를 초래하여, 적절한 전경 움직임 없이 정지된 배경 영역들에서 나타나는 전경의 부분들과 같은 이슈들로 이어질 수 있다(도들). 2(a)(f)). **2) 언더커버리지.** 폐색 또는 빠른 모션이 정확한 광학 흐름 추정을 방해하는 영역에서, 결과적인 제약들이 불충분하여, 도에 예시된 바와 같은 왜곡을 초래한다. 2(c)~(e). **3) 불정확도.** 순차적인 프레임별 생성은 국부적 최적화로 제한되며, 시간 경과에 따른 에러의 누적으로 이어진다(도에서 누락된 손가락). 2(b) 이전 프레임들에서 참조 핑거들이 없기 때문이다.\n' +
      '\n' +
      '위의 중요한 문제를 해결하기 위해 FRamE 시공간 대응(**FRESCO**)을 제시한다. 이전의 방법들은 주로 _inter-frame temporal correspondence_를 제한하는 것에 초점을 맞추고 있지만, 우리는 _intra-frame spatial correspondence_를 보존하는 것이 동등하게 중요하다고 믿는다. 우리의 접근법은 의미적으로 유사한 콘텐츠가 번역 후 유사성을 유지하면서 응집적으로 조작되도록 한다. 이 전략은 처음 두 가지 과제를 효과적으로 해결하는데, 전경이 배경으로 잘못 번역되는 것을 방지하고 광학 흐름의 일관성을 향상시킨다. 광학 흐름을 사용할 수 없는 영역의 경우 원래 프레임 내의 공간적 대응은 그림 2와 같이 규제 메커니즘 역할을 할 수 있다.\n' +
      '\n' +
      '우리의 접근법에서 FreSCo는 주의와 기능의 두 가지 단계로 도입된다. 주의력 수준에서 FreSCo 유도 주의력을 소개합니다. [5]로부터 옵티컬 플로우 안내를 기반으로 하고 입력 프레임의 자기 유사성을 통합하여 주의 메커니즘을 풍부하게 한다. 이는 입력 비디오로부터의 프레임간 및 프레임내 큐들 둘 모두의 효과적인 사용을 가능하게 하며, 보다 제약적인 방식으로 유효 피처들에 포커스를 전략적으로 지향시킨다. 특징 레벨에서 FreSCo-aware 특징 최적화를 제시한다. 이는 단순히 피쳐 주의에만 영향을 미치는 것을 넘어 U-Net 디코더 계층에서 의미적으로 의미 있는 피쳐의 명시적인 업데이트를 포함한다. 이것은 입력 비디오의 높은 공간적-시간적 일관성과 밀접하게 정렬하기 위해 경사 하강을 통해 달성된다. 이 두 가지 개선의 시너지는 그림 1과 같이 성능이 눈에 띄게 향상되며, 최종 과제를 극복하기 위해 다중 프레임 처리 전략을 사용한다. 배치 내의 프레임은 일괄적으로 처리되어 서로 가이드할 수 있는 반면, 앵커 프레임은 배치 간 일관성을 보장하기 위해 배치 간에 공유된다. 긴 비디오 변환을 위해, 우리는 키프레임 선택을 위해 휴리스틱 접근법을 사용하고, 비-키프레임 프레임들을 위해 보간법을 사용한다. 우리의 주요 기여는 다음과 같습니다.\n' +
      '\n' +
      '* 일관성 있고 유연한 비디오 변환을 위한 프레임 시공간 대응성에 의해 안내되는 새로운 제로-샷 확산 프레임워크.\n' +
      '* FreSCo-유도 특징 주의 및 최적화를 광학 흐름 단독보다 더 우수한 일관성 및 커버리지를 갖는 강건한 프레임 내 및 프레임 간 제약으로서 결합한다.\n' +
      '* 배치간 일관성을 갖는 배치 프레임을 공동으로 처리하여 긴 비디오 번역.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**이미지 확산 모델.** 최근 몇 년 동안 텍스트 유도 이미지 생성 및 편집을 위한 이미지 확산 모델의 폭발적인 성장을 목격했다. 확산 모델은 반복적인 잡음 제거 과정을 통해 이미지를 합성한다[17]. DALLE-2[33]은 CLIP[32]를 활용하여 텍스트와 이미지를 텍스트 대 이미지 생성을 위해 정렬한다. Imagen[36]은 텍스트 컨디셔닝을 개선하기 위해 클래스가 없는 안내[29]가 사용되는 고해상도 생성을 위한 확산 모델을 캐스케이드한다. 안정적 확산은 잠재 확산 모델[34]을 기반으로 복잡성을 더욱 줄이기 위해 컴팩트한 잠재 공간에서 잡음을 제거한다.\n' +
      '\n' +
      '텍스트 대 이미지 모델이 일련의 이미지를 생성했습니다.\n' +
      '\n' +
      '그림 2: 실제 비디오에서 CG 비디오 번역. 광학 흐름에만 의존하는 방법 [47]은 (a)(f) 일관성이 없거나 (c)(d)(e) 광학 흐름 안내 누락 및 (b) 오류 누적을 겪는다. FreSCo를 도입함으로써, 우리의 방법은 이러한 문제들을 잘 해결한다.\n' +
      '\n' +
      '조작 모델[2, 16]. 프롬프트2프롬프트[16]은 이미지 레이아웃을 유지하기 위해 교차 주의 제어를 도입한다. 실제 영상을 편집하기 위해 DIM 반전[39]과 Null-Text 반전[28]을 제안하여 주의 제어[3, 30, 40]로 편집을 위한 잡음 잠재 특징에 실제 영상을 삽입한다.\n' +
      '\n' +
      '텍스트 컨디셔닝 외에도 다양한 유연한 조건이 도입됩니다. SDEdit[27]은 생성 중에 이미지 안내를 도입한다. 객체 외형 및 스타일은 텍스트 임베딩[8], 모델 가중치[14, 19, 24, 35] 또는 인코더[9, 12, 43, 46, 48]를 미세 조정함으로써 사용자화될 수 있다. ControlNet[49]은 세밀한 생성을 위한 구조 또는 레이아웃 정보를 제공하기 위한 제어 경로를 도입한다. 우리의 제로 샷 프레임워크는 사전 훈련된 모델을 변경하지 않으므로 그림 1과 같이 유연한 제어 및 사용자 정의를 위한 이러한 조건과 호환된다.\n' +
      '\n' +
      '** 제로 샷 텍스트 유도 비디오 편집** 비디오에 대해 훈련되거나 미세 조정된 대형 비디오 확산 모델이 연구되었지만 [1, 6, 7, 10, 13, 15, 18, 26, 37, 38, 42, 44, 51], 본 논문은 가볍고 호환성이 높은 제로 샷 방법에 중점을 둔다. 제로샷 방식은 인버젼 기반 방식과 인버젼 프리 방식으로 나눌 수 있다.\n' +
      '\n' +
      '반전 기반 방법[22, 31]은 DDIM 반전을 비디오에 적용하고 편집 중에 주의력 제어를 위해 주의력 특징을 기록한다. FateZero[31]은 편집되지 않은 영역을 탐지하고 보존하며 글로벌 외관 일관성을 강화하기 위해 크로스 프레임 주의를 사용한다. 프레임 간 대응을 명시적으로 활용하기 위해, Pix2Video[4] 및 TokenFlow[11]은 이전 편집된 프레임으로부터의 특징들을 일치시키거나 혼합한다. FLATTEN [5]는 세립 시간 일관성을 위해 주의 메커니즘에 광학 흐름을 도입한다.\n' +
      '\n' +
      '역전이 없는 방법은 주로 ControlNet을 번역에 사용한다. Text2Video-Zero[23]은 움직이는 잡음에 의해 움직임을 시뮬레이션한다. ControlVideo[50]는 Cross-frame attention과 inter-frame smoothing이 있는 비디오로 ControlNet을 확장한다. VideoControlNet[20]과 Rerender-A-Video[47]은 이전 편집된 프레임을 옵티컬 플로우와 워핑 및 융합하여 시간적 일관성을 향상시킨다. 역전 기반 방법에 비해 역전이 없는 방법은 보다 유연한 컨디셔닝과 맞춤형 모델과의 호환성이 높아 사용자가 편리하게 출력 외형을 제어할 수 있다. 그러나, DDIM 역산 특징들의 안내 없이, 역산이 없는 프레임워크는 깜박거리기 쉽다. 우리의 프레임워크는 또한 반전되지 않지만 프레임 내 대응을 추가로 통합하여 높은 제어성을 유지하면서 시간적 일관성을 크게 향상시킨다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      '우리는 SDEdit[27]과 ControlNet[49]을 기반으로 한 Stable Diffusion의 반전 없는 영상 번역 파이프라인을 따라 비디오 번역에 적응한다. 입력 프레임 \\(I\\)은 Encoder \\(\\mathcal{E}\\)을 사용하여 잠재 특징 \\(x_{0}=\\mathcal{E}(I)\\)에 매핑된다. 그런 다음 SDEdit은 DDPM 순방향 과정[17]을 적용하여 \\(x_{0}\\)에 가우시안 잡음을 추가한다.\n' +
      '\n' +
      'q(x_{t}|x_{0})=\\mathcal{N}(x_{t};\\sqrt{\\bar{\\alpha}_{t}}x_{0},(1-\\bar{\\alpha}_{t})\\mathbf{I}), \\tag{1}\\\n' +
      '\n' +
      '여기서 \\(\\bar{\\alpha}_{t}\\)는 DDPM 단계에서 미리 정의된 하이퍼파라미터이다. 그 다음, DDPM 역방향 과정[17]에서, 안정 확산 U-Net(\\epsilon_{\\theta}\\)은 잠재 특징의 잡음을 예측하여 프롬프트 \\(c\\)에 의해 안내된 \\(x^{\\prime}_{T}=x_{T}\\)을 \\(x^{\\prime}_{0}\\)으로 반복적으로 변환한다:\n' +
      '\n' +
      '[x^{\\prime}_{t-1}=\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_{t}{1-\\bar{\\alpha}_{t}}\\hat{x^{\\prime}_{0}+\\frac{(1-\\bar{\\alpha}_{t-1})(\\sqrt{\\alpha}_{t}}x^{\\prime}_{t}+\\beta_{t}}{1-\\bar{\\alpha}_{t}}, \\tag{2}\\frac{(1-\\bar{\\alpha}_{t-1}}\\frac{(1-\\bar{\\alpha}_{t-1})(\\sqrt{\\alpha}_{t}}x^{\\prime}_{t}+\\beta_{t}_{t}}{1-\\bar{\\alpha}_{t}},\\tag{2}\\frac{(1-\\bar{\\alpha}_\n' +
      '\n' +
      '여기서 \\(\\alpha_{t}\\) 및 \\(\\beta_{t}=1-\\alpha_{t}\\)은 미리 정의된 하이퍼파라미터이고, \\(z_{t}\\)은 무작위로 샘플링된 표준 구아시안 잡음이고, \\(\\hat{x}^{\\prime}_{0}\\)은 잡음 제거 단계 \\(t\\)에서 예측된 \\(x^{\\prime}_{0}\\)이고,\n' +
      '\n' +
      '\\[\\hat{x}^{\\prime}_{0}=(x^{\\prime}_{t}-\\sqrt{1-\\bar{\\alpha}_{t}\\epsilon_{\\theta}(x^{\\prime}_{t},t,c,e))/\\sqrt{\\bar{\\alpha}_{t}, \\tag{3}\\t}\n' +
      '\n' +
      '그리고 \\(\\epsilon_{\\theta}(x_{t},t^{\\prime},c,e)\\)는 \\(t\\), 텍스트 프롬프트 \\(c\\) 및 ControlNet 조건 \\(e\\)에 기초하여 \\(x^{\\prime}_{t}\\)의 예측된 잡음이다. \\(e\\)는 여분의 구조 또는 레이아웃 정보를 제공하기 위해 \\(I\\)으로부터 추출된 에지, 포즈 또는 깊이 맵일 수 있다. 마지막으로 Decoder를 이용하여 I^{\\prime}=\\mathcal{D}(x^{\\prime}_{0})\\(I^{\\prime}=\\mathcal{D})을 구한다. SDEdit은 사용자가 \\(I^{\\prime}\\)와 \\(I\\) 사이의 더 큰 외형 변동을 위해 \\(T\\), _i.e._, \\(T\\)으로 초기 잡음 레벨을 다르게 설정하여 변환 정도를 조정할 수 있도록 한다. 간단히 하기 위해, 우리는 다음에서 잡음 제거 단계 \\(t\\)를 생략할 것이다.\n' +
      '\n' +
      '### Overall Framework\n' +
      '\n' +
      '제안된 제로-샷 비디오 변환 파이프라인은 그림 3에 예시되어 있다. 비디오 프레임 세트\\(\\mathbf{I}=\\{I_{i}\\}_{i=1}^{N}\\)이 주어지면, 우리는 Sec. 3.1을 따라 DDPM 전후 과정을 수행하여 변환된 \\(\\mathbf{I}=\\{I^{\\prime}_{i}\\}_{i=1}^{N}\\)을 얻는다. 우리의 적응은 U-Net에 \\(\\mathbf{I}\\)의 공간적 및 시간적 대응 관계를 통합하는 데 중점을 둔다. 보다 구체적으로 \\(\\mathbf{I}\\)의 시간적, 공간적 대응관계를 다음과 같이 정의한다.\n' +
      '\n' +
      '* ** 시간 통신** 이러한 프레임 간 대응은 시간적 일관성을 유지하는 중추 요소인 인접 프레임 간의 광학 흐름에 의해 측정된다. 광류 및 폐색 마스크를 각각 \\(I^{i}\\)에서 \\(w^{j}_{i}\\) 및 \\(M^{j}_{i}\\)으로 표현하여, 비폐색 영역에서 \\(I^{prime}_{i}\\) 및 \\(I^{prime}_{i+1}\\)이 \\(w^{i+1}_{i}\\을 공유하도록 하는 것을 목적으로 한다.\n' +
      '**공간 통신*** 이러한 프레임 내 대응은 단일 프레임 내의 픽셀들 간의 자기 유사성에 의해 측정된다. 목적은 \\(I^{\\prime}_{i}\\)과 \\(I_{i}\\)으로 자기유사성을 공유하는 것이며, \\(I_{i}\\)으로 의미적으로 유사한 내용이 유사한 외관으로 변환되고, 그 반대의 경우도 마찬가지이다. 이러한 의미론과 공간 레이아웃의 보존은 번역 중 시간적 일관성을 향상시키는 데 암묵적으로 기여한다.\n' +
      '\n' +
      '우리의 적응은 U-Net 내에서 디코더 계층의 _input feature_와 _attention module_에 초점을 맞추는데, 이는 디코더 계층이 인코더 계층보다 잡음이 적고 \\(x_{t}\\) 잠재 공간:*[leftmargin=*]보다 의미적으로 의미적이기 때문이다.\n' +
      '**피쳐 각색** 우리는 그림 3과 같은 새로운 FreSCo 인식 특징 최적화 방법을 제안한다. 입력 프레임과의 시간적, 공간적 정합성을 강화하기 위해 디코더 레이어 특징\\(\\mathbffff=\\{f_{i}\\}_{i=1}^{N}\\)을 직접 최적화하기 위해 공간적 정합성 손실\\(\\mathcal{L}_{spat}\\)과 시간적 정합성 손실\\(\\mathcal{L}_{temp}\\)을 설계한다.\n' +
      '주목해 주세요 우리는 그림 3과 같이 세 가지 구성 요소로 구성된 FreSCo 유도 주의로 자기 주의를 대체한다. 공간 유도 주의는 먼저 입력 프레임의 자기 유사성을 기반으로 특징을 집계한다. 그런 다음 교차 프레임 주의를 사용하여 모든 프레임에 걸쳐 피쳐를 집계합니다. 마지막으로, 시간적 유도 주의는 시간적 일관성을 더욱 강화하기 위해 동일한 광학 흐름을 따라 특징을 응집시킨다.\n' +
      '\n' +
      '제안된 특징 적응은 \\(\\mathbf{I}\\)으로 높은 공간 및 시간 일관성을 위해 특징을 직접 최적화한다. 한편, 우리의 주의 적응은 유효한 기능을 처리하는 방법과 장소에 부드러운 제약을 부과함으로써 일관성을 간접적으로 향상시킨다. 우리는 이 두 가지 형태의 적응을 결합하면 최고의 성능을 달성한다는 것을 발견했다.\n' +
      '\n' +
      '### FreSCo-ware Feature Optimization\n' +
      '\n' +
      'U-Net의 각 디코더 계층의 입력 특징 \\(\\mathbf{f}=\\{f_{i}\\}_{i=1}^{N}\\)은 최적화 과정을 통해 경사 하강에 의해 업데이트된다.\n' +
      '\n' +
      '\\[\\hat{\\mathbf{f}}=\\arg\\min_{\\hat{\\mathbf{f}}\\mathcal{L}_{temp}(\\mathbf{f})+\\mathcal{L}_{spat}(\\mathbf{f}). \\tag{4}\\\n' +
      '\n' +
      '갱신된 \\(\\hat{\\mathbf{f}\\)은 후속 처리를 위해 \\(\\mathbf{f}\\)을 대체한다.\n' +
      '\n' +
      '시간적 일관성 손실\\(\\mathcal{L}_{temp}\\)에 대해, 우리는 인접한 두 프레임 사이의 대응 위치의 특징값이 일치하기를 원한다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{temp}(\\mathbf{f})=\\sum_{i}\\|M_{i}^{i+1}(f_{i+1}-w_{i}^{i+1}(f_{i}))\\|_{1}\\tag{5}\\|M_{i}^{i+1}(f_{i})\n' +
      '\n' +
      '공간 일관성 손실\\(\\mathcal{L}_{spat}\\)을 위해 특징 공간에서의 코사인 유사도를 이용하여 \\(I_{i}\\)의 공간 대응성을 측정한다. 구체적으로, 1단계 DDPM 순방향과 역방향 과정을 \\(I_{i}\\)에 걸쳐 수행하고, \\(f_{i}^{\\tau}\\)으로 표시된 U-Net 디코더 특징을 추출한다. 단일 단계 전진 과정은 무시할 수 있는 잡음을 추가하기 때문에, \\(f_{i}^{\\tau}\\)는 의미적 유사성을 계산하기 위해 \\(I_{i}\\)의 의미적 의미 표현 역할을 할 수 있다. 그런 다음, 모든 요소 쌍 간의 코사인 유사도는 정규화된 특징의 그램 매트릭스로 간단히 계산될 수 있다. \\(\\tilde{f}\\)는 \\(\\tilde{f}\\)의 각 원소가 단위 벡터가 되도록 정규화된 \\(f\\)을 나타낸다. 우리는 \\(\\tilde{f}_{i}\\)의 그램 행렬이 \\(\\tilde{f}_{i}^{\\tau}\\)의 그램 행렬에 접근하기를 원한다.\n' +
      '\n' +
      '\\lambda_{spat}(\\mathbf{f})=\\lambda_{\\text{spat}\\sum_{i}\\|\\tilde{f}_{i}\\tilde{f}_{i}^{\\top}-\\tilde{f}_{i}\\tilde{f}_{i}^{\\tau\\top}\\|_{2}^{2}. \\tag{6}\\tilde{f}_{i}^{\\top}-\\tilde{f}_{i}^{\\tau\\top}\\|_{2}^{2}.\n' +
      '\n' +
      '### FreSCo-Guided Attention\n' +
      '\n' +
      'FreSCo 유도 주의 레이어는 그림 3과 같이 공간 유도 주의, 효율적인 프레임 교차 주의 및 시간 유도 주의의 세 가지 연속 모듈을 포함한다.\n' +
      '\n' +
      '**공간 유도 주의.**자기 주의와 대조적으로 공간 유도 주의의 패치는 자신의 유사성보다는 번역 전의 패치의 유사성에 기초하여 서로 집합한다. 구체적으로 Sec. 3.3에서 \\(\\mathcal{L}_{spat}\\)을 계산하는 것과 일치하게, \\(I_{i}\\)에 걸쳐 단일 단계 DDPM 전진 및 후진 과정을 수행하고, 자기 주의 질의 벡터 \\(Q_{i}^{r}\\) 및 키 벡터 \\(K_{i}^{r}\\)을 추출한다. 그런 다음 공간 유도 어텐션 집합체\\(Q_{i}\\)\n' +
      '\n' +
      '\\textit{Softmax}(\\frac{Q_{i}^{r}K_{i}^{\\tau\\top}{\\lambda_{s}\\sqrt{d}})\\cdot Q_{i}, \\tag{7}\\\n' +
      '\n' +
      '여기서 \\(\\lambda_{s}\\)는 스케일 팩터이고 \\(d\\)는 쿼리 벡터 차원이다. 도 1에 도시된 바와 같다. 도 4를 참조하면, 전경 패치는 주로\n' +
      '\n' +
      '도 3: FRamE 시공간 대응(FreSCo)에 의해 안내된 제로-샷 비디오 번역의 프레임워크. FreSCo-aware 최적화는 입력 프레임과의 시간적, 공간적 정합성을 강화하기 위해 U-Net 특징에 적용된다. FreSCo를 자기 주의 계층으로 통합하여 입력 프레임과의 공간 대응성을 유지하기 위한 공간 유도 주의, 입력 프레임과의 거칠고 미세한 시간 대응성을 유지하기 위한 효율적인 교차 프레임 주의 및 시간 유도 주의를 각각 생성한다.\n' +
      '\n' +
      'C자형 전경 영역에서 피쳐를 집계하고 배경 영역에 덜 참석합니다. 그 결과 \\(Q^{\\prime}\\)이 \\(Q\\)보다 입력 프레임과의 공간 일관성이 더 우수하였다.\n' +
      '\n' +
      '**효율적인 크로스 프레임 어텐션** 글로벌 스타일 일관성을 정규화하기 위해 셀프 어텐션을 크로스 프레임 어텐션으로 대체합니다. 첫 번째 프레임 또는 이전 프레임을 기준 [4, 23]으로 사용하기보다는 (V1, 도 4) 새로 나타난 물체(도 2의 손가락)를 다룰 수 없는 것을 특징으로 하는 방법. 2(b)), 또는 사용 가능한 모든 프레임을 기준(V2, 도 4)으로 사용하는 단계 계산적으로 비효율적인 모든 프레임을 동시에 고려하되 가능한 한 적은 중복성으로 고려하는 것을 목표로 한다. 따라서, 우리는 효율적인 교차 프레임 주의를 제안한다: 첫 번째 프레임을 제외하고, 이전 프레임(폐색 영역)에서 볼 수 없었던 각 프레임의 영역에 대해서만 참조한다. 따라서, 우리는 위의 영역 내의 모든 패치의 교차 프레임 인덱스 \\(p_{u}\\)를 구성할 수 있다. 이 패치들의 키와 값은 \\(K[p_{u}]\\), \\(V[p_{u}]\\)로 샘플링될 수 있다. 그러면, 크로스 프레임 어텐션이 적용된다.\n' +
      '\n' +
      '\\[V^{\\prime}_{i}=\\textit{Softmax}(\\frac{Q^{\\prime}_{i}(K[p_{u}])^{\\top}{\\sqrt{ d}})\\cdot V[p_{u}]. \\tag{8}\\\n' +
      '\n' +
      '**Temporal-guided attention.** FLATTEN[5]에서 영감을 받은 우리는 플로우 기반 attention을 사용하여 미세한 수준의 크로스 프레임 일관성을 규칙화한다. 그림 4와 같이 서로 다른 프레임에서 동일한 패치를 추적한다. 각 광학 흐름에 대해 이 흐름에서 모든 패치의 교차 프레임 인덱스 \\(p_{f}\\)를 구축한다. FLATTEN에서 각 패치는 다른 프레임의 패치만 처리할 수 있으며 흐름이 패치를 거의 포함하지 않을 때 불안정하다. 이와는 달리, 시간적 유도 주의는 그러한 제한을 갖지 않으며,\n' +
      '\n' +
      '[H[p_{f}]=\\textit{Softmax}(\\frac{Q[p_{f}](K[p_{f}])^{\\top}{\\lambda_{t}\\sqrt{d}})\\cdot V^{\\prime}[p_{f}], \\tag{9}\\]\n' +
      '\n' +
      '여기서 \\(\\lambda_{t}\\)는 스케일 팩터이다. 그리고 \\(H\\)는 FreSCo 유도 주의층의 최종 출력이다.\n' +
      '\n' +
      '장시간 비디오 번역\n' +
      '\n' +
      '한 번에 처리할 수 있는 프레임 수\\(N\\)는 GPU 메모리에 의해 제한된다. 긴 비디오 번역을 위해 Rerender-A-Video[47]을 따라 키프레임에 대해서만 제로샷 비디오 번역을 수행하고 Ebsynth[21]을 사용하여 번역된 키프레임을 기반으로 비키프레임을 보간한다.\n' +
      '\n' +
      '**키프레임 선택.**렌더-A-비디오[47]는 차선의 키프레임을 균일하게 샘플링한다. 알고리즘 1에 요약된 휴리스틱 키프레임 선택 알고리즘을 제안한다. 고정된 샘플링 단계는 움직임이 클 때 간격([s_{\\text{min}}, s_{\\text{max}]\\)으로 완화하고, 조밀한 샘플 키프레임은 프레임 간의 거리(L_{2}\\)로 측정한다.\n' +
      '\n' +
      '**키프레임 변환** \\(N\\) 이상의 키프레임으로 몇 개의 \\(N\\) 프레임 배치로 나눕니다. 각 배치에는 배치 간 일관성을 부여하기 위해 이전 배치의 첫 번째 프레임과 마지막 프레임이 포함되며, \\(k\\) 번째 배치의 키프레임 인덱스는 \\(\\{1, (k-1)(N-2)+2, (k-1)(N-2)+3,...,k(N-2)+2\\}\\이다. 또한 전체 잡음 제거 단계에서 잠재 특징(x^{\\prime}_{t}\\)을 기록한다. (2))를 이용하여, 각 배치의 첫 번째 프레임 및 마지막 프레임의 다음 배치에서의 대응하는 잠재 특징들을 대체한다.\n' +
      '\n' +
      '```\n' +
      '0: Video \\(\\mathbf{I}=\\{I_{i}\\}_{i=1}^{M}\\), 샘플 파라미터 \\(s_{\\text{min}\\), \\(s_{\\text{max}\\)\n' +
      '0 : 오름차순으로 키프레임 인덱스 리스트 \\(\\Omega\\)\n' +
      '1: \\(\\Omega=[1,M]\\) 및 \\(d_{i}=0,\\forall i\\in[1,M]\\) 초기화\n' +
      '2: set \\(d_{i}=L_{2}(I_{i},I_{i-1}),\\forall i\\in[s_{\\text{min}}+1,N-s_{\\text{min}]]\n' +
      '3: \\(i\\)이 존재하여 \\(\\Omega[i+1]-\\Omega[i]>s_{\\text{max}}\\)\n' +
      '4:\\(\\오메가\\ texttt{insert}(i)\\ texttt{sort}()\\) with \\(i=\\arg\\max_{i}(d_{i})\\)\n' +
      '5: set \\(d_{j}=0\\), \\(\\forall\\;j\\in(i-s_{\\text{min}},i+s_{\\text{min}})\\)\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 키프레임 선택\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '**구현 세부사항** 실험은 하나의 NVIDIA Tesla V100 GPU에서 수행된다. 기본적으로 입력 비디오 해상도, 손실 가중치\\(\\lambda_{\\text{spat}}=50\\), 스케일 팩터\\(\\lambda_{s}=\\lambda_{t}=5\\)을 기반으로 배치 크기\\(N\\in[6,8]\\)를 설정한다. 특징 최적화를 위해 Adam optimizer와 학습률 \\(0.4\\)을 사용하여 \\(K=20\\) 반복에 대한 \\(\\mathbf{f}\\)을 갱신한다. 우리는 \\(K=20\\)과 더 큰 \\(K\\)이 명백한 이득을 가져오지 않을 때 최적화가 대부분 수렴함을 발견한다. GMFlow[45]는 광학 흐름 및 폐색 마스크를 추정하는 데 사용된다. 배경 영역의 시간적 일관성을 향상시키기 위해 배경 평활화[23]를 적용한다.\n' +
      '\n' +
      '도 4: 주의 메커니즘의 일러스트레이션. 빨간색 십자가로 표시된 패치는 색상이 있는 패치를 처리하고 기능을 집계합니다. 프리스코 유도 주의는 이전의 주의와 비교하여 입력의 프레임 내 및 프레임 간 대응 관계를 추가로 고려한다. 공간-유도 어텐션은 입력 프레임의 자기-유사성에 기초하여 프레임 내 특징들을 집계한다(어두운 것은 더 높은 가중치들을 나타낸다). 효율적인 크로스 프레임 주의는 중복 패치를 제거하고 고유한 패치를 유지합니다. 시간 안내 주의는 동일한 흐름에서 프레임 간 피쳐를 집계합니다.\n' +
      '\n' +
      '최신 방법과의 비교\n' +
      '\n' +
      '우리는 Text2Video-Zero[23], ControlVideo[50], Reender-A-Video[47]의 세 가지 최근 반전 없는 제로 샷 방법과 비교한다. 공정한 비교를 보장하기 위해 모든 방법은 ControlNet, SDEdit 및 LoRA의 동일한 설정을 사용한다. 도 1에 도시된 바와 같다. 도 5를 참조하면, 모든 방법은 제공된 텍스트 프롬프트에 따라 비디오를 성공적으로 번역한다. 그러나 ControlNet 조건에 의존하는 반전 없는 방법은 디포커스 또는 모션 블러와 같은 문제로 인해 품질이 낮은 경우 비디오 편집 품질의 저하를 경험할 수 있다. 예를 들어, ControlVideo는 개와 권투 선수의 그럴듯한 모습을 생성하지 못한다. Text2Video-Zero와 Reender-A-Video는 고양이의 자세와 복서 장갑의 구조를 유지하기 위해 고군분투한다. 이와는 대조적으로, 본 방법은 제안된 강건한 FreSCo 안내에 기초하여 일관된 비디오를 생성할 수 있다.\n' +
      '\n' +
      '정량적 평가를 위해 표준 관행[4, 31, 47]을 준수하여 FrameAcc(CLIP 기반 프레임별 편집 정확도), Tmp-Con(연속 프레임 간의 CLIP 기반 코사인 유사성) 및 Pixel-MSE(정렬된 연속 프레임 간의 평균 평균 제곱 픽셀 오류)의 평가 메트릭을 사용한다. 또한 공간 일관성에 대한 SpatioCon (\\(L_{spat}\\)을 보고한다. 23개의 비디오에 대한 평균 결과는 표 1에 보고되었으며, 특히 우리의 방법은 최고의 편집 정확도와 시간적 일관성을 달성한다. 57명의 참가자와 함께 사용자 연구를 추가로 수행합니다. 참가자들은 네 가지 방법 중 가장 바람직한 결과를 선택하는 임무를 받는다. 표 1은 11개의 테스트 비디오에 대한 평균 선호율을 제시하며, 우리의 방법이 가장 선호되는 선택으로 나타난다는 것을 보여준다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '전체 성능에 대한 다양한 모듈의 기여도를 검증하기 위해 프레임워크에서 특정 모듈을 체계적으로 비활성화한다. 그림 6은 공간적, 시간적 대응 관계를 통합한 효과를 보여준다. 베이스라인 방법은 시간 일관성을 위해 프레임 간 주의를 단독으로 사용한다. 시간적 적응을 도입하여 질감의 정렬과 두 프레임에 걸친 태양의 위치 안정화와 같은 일관성의 개선을 관찰한다. 한편, 공간 관련 적응은 번역 중에 포즈를 보존하는 데 도움이 된다.\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 7, 주의 집중 적응과 특징 적응의 효과를 연구한다. 분명히, 각각의 향상은 개별적으로\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c} \\hline \\hline Metric & Frame-Acc \\(\\uparrow\\) & Tem-Con \\(\\uparrow\\) & Pixel-MSE \\(\\downarrow\\) & Sepat-Con \\(\\downarrow\\) & User \\(\\uparrow\\) \\\\ \\hline T2V-Zero & 0.918 & 0.965 & 0.038 & 0.0845 & 9.1\\% \\\\ ControlVideo & 0.932 & 0.951 & 0.066 & 0.0957 & 2.6\\% \\\\ Reender & 0.955 & 0.969 & 0.016 & 0.0836 & 23.3\\% \\\\ Ours & **0.978** & **0.975** & **0.012** & **0.0805** & **65.0\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 정량적 비교 및 사용자 선호도 비율.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c} \\hline \\hline Metric & baseline & w/ temp & w/ spat & w/ attn & w/ opt & full \\\\ \\hline Frame-Acc \\(\\uparrow\\) & **1.000** & **1.000** & **1.000** & **1.000** & **1.000** & **1.000** \\\\ Tem-Con \\(\\uparrow\\) & 0.974 & 0.979 & 0.976 & 0.976 & 0.977 & **0.980** \\\\ Pixel-MSE \\(\\downarrow\\) & 0.032 & 0.015 & 0.020 & 0.016 & 0.019 & **0.012** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 정량적 절제 연구.\n' +
      '\n' +
      '그림 5: 반전 없는 제로 샷 비디오 번역 방법과의 시각적 비교.\n' +
      '\n' +
      '시간적 일관성을 어느 정도 향상시키지만 둘 다 완벽을 이루지는 못한다. 두 개의 조합만이 모발 가닥에서 관찰된 불일치를 완전히 제거하며, 이는 그림 1의 픽셀-MSE 점수 0.037, 0.021, 0.018, 0.015로 정량적으로 검증된다. 도 7의 (b) 내지 (e)를 각각 나타낸다. 주의 적응과 관련하여 우리는 시간 안내 주의와 공간 안내 주의에 대해 더 자세히 조사한다. 그들이 부과하는 제약조건의 강도는 각각 \\(\\lambda_{t}\\)와 \\(\\lambda_{s}\\)에 의해 결정된다. 도 1에 도시된 바와 같다. 도 8-9에 도시된 바와 같이, \\(\\lambda_{t}\\)의 증가는 배경 영역에서 두 변환 프레임 사이의 일관성을 효과적으로 향상시키는 반면, \\(\\lambda_{s}\\)의 증가는 변환 고양이와 원래 고양이 사이의 포즈 일관성을 향상시킨다. 공간 유도 주의 외에도 우리의 공간 일관성 손실은 그림 1에서 검증된 바와 같이 중요한 역할을 한다. 10. 이 예에서, 빠른 모션 및 블러는 광학 흐름을 예측하기 어렵게 하여 큰 폐색 영역으로 이어진다. 공간 대응 지침은 이 영역의 렌더링을 제한하는데 특히 중요하다. 분명히, 각각의 적응은 원치 않는 스키 폴과 일관되지 않은 눈 질감을 제거하는 것과 같은 뚜렷한 기여를 한다. 두 가지를 결합하면 그림 1에 대해 0.031, 0.028, 0.025, 0.024의 픽셀-MSE 점수로 정량적으로 확인된 가장 일관된 결과를 얻을 수 있다. 도 10의 (b) 내지 (e)를 각각 나타낸다.\n' +
      '\n' +
      '표 2는 각 모듈의 영향에 대한 정량적 평가를 제공한다. 시각적 결과와의 정렬에서, 각 모듈은 시간적 일관성의 전반적인 향상에 기여한다는 것이 명백하다. 특히, 모든 적응의 조합은 최고의 성능을 산출합니다.\n' +
      '\n' +
      '그림 11은 제안된 효율적인 크로스 프레임 주의를 제거한다. Fig.의 Rerender-A-Video와 마찬가지로. 도 2의 (b)에 도시된 바와 같이, 순차적인 프레임별 변환은 새로운 출현 객체들에 취약하다. 교차 프레임 주의는 배치 프레임 내의 모든 고유한 객체에 주의를 기울일 수 있으며, 이는 그림 1에서 설명한 대로 효율적일 뿐만 아니라 더 강력하다. 12.\n' +
      '\n' +
      '도 8: \\(\\lambda_{t}\\)의 효과. 정량적으로, 픽셀-MSE 점수는 (a) 0.016, (b) 0.014, (c) 0.013, (d) 0.012이다. 황색 화살표는 두 프레임 사이의 불일치를 나타낸다.\n' +
      '\n' +
      '그림 10: 공간 대응 통합의 효과. (a) 적색 폐색 마스크로 덮인 입력. (b)-(d) 우리의 공간 유도 주의와 공간 일관성 손실은 각각 스키 폴(노란색 화살표)과 눈 텍스처(빨간색 화살표)의 불일치를 줄이는 데 도움이 된다. 프롬프트: 만화 스파이더맨이 스키를 타고 있습니다.\n' +
      '\n' +
      '그림 6: 공간적, 시간적 대응관계를 통합한 효과. 파란색 화살표는 입력 프레임과의 공간적 불일치를 나타낸다. 빨간색 화살표는 두 출력 프레임 간의 시간적 불일치를 나타낸다.\n' +
      '\n' +
      '그림 7: 주의 적응과 특징 적응의 효과. 맨 위 행: (a) 입력. 다른 행: (b) 교차-프레임 주의력, (c) 주의력 적응, (d) 특징 적응, (e) 주의력 및 특징 적응 둘 다로 각각 얻어진 결과. 파란색 영역은 오른쪽에서 대비가 강화되어 더 나은 비교를 위해 확대된다. 프롬프트: CG 스타일의 아름다운 여성.\n' +
      '\n' +
      '도 9: \\(\\lambda_{s}\\)의 효과. 빨간색 상자의 영역은 더 나은 비교를 위해 오른쪽 상단에 확대되어 표시된다. 프롬프트: 분홍색 배경의 만화 흰 고양이.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:8]\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In _Proc. IEEE Int\'l Conf. Computer Vision and Pattern Recognition_, pages 22563-22575, 2023.\n' +
      '* [2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Intructpix2pix: Learning to follow image editing instructions. In _Proc. IEEE Int\'l Conf. Computer Vision and Pattern Recognition_, pages 18392-18402, 2023.\n' +
      '* [3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. MasaCtrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In _Proc. Int\'l Conf. Computer Vision_, 2023.\n' +
      '* [4] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra. Pix2video: Video editing using image diffusion. In _Proc. Int\'l Conf. Computer Vision_, pages 23206-23217, 2023.\n' +
      '* [5] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. FLATTEN: optical flow-guided attention for consistent text-to-video editing. _arXiv preprint arXiv:2310.05922_, 2023.\n' +
      '* [6] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Grasskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In _Proc. Int\'l Conf. Computer Vision_, pages 7346-7356, 2023.\n' +
      '* [7] Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo Chen, and Baining Guo. Cecedit: Creative and controllable video editing via diffusion models. _arXiv preprint arXiv:2309.16496_, 2023.\n' +
      '* [8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In _Proc. Int\'l Conf. Learning Representations_, 2022.\n' +
      '* [9] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. _ACM Transactions on Graphics_, 42(4):1-13, 2023.\n' +
      '* [10] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior for video diffusion models. In _Proc. Int\'l Conf. Computer Vision_, 2023.\n' +
      '* [11] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. In _Proc. Int\'l Conf. Learning Representations_, 2024.\n' +
      '* [12] Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, and Yujiu Yang. TaleCrafter: Interactive story visualization with multiple characters. In _ACM SIGGRAPH Asia Conference Proceedings_, 2023.\n' +
      '* [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. AnimateDiff: Animate your personalized text-to-image diffusion models without specific tuning. _arXiv preprint arXiv:2307.04725_, 2023.\n' +
      '* [14] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. SVDiff: Compact parameter space for diffusion fine-tuning. In _Proc. IEEE Int\'l Conf. Computer Vision and Pattern Recognition_, 2023.\n' +
      '* [15] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. _arXiv preprint arXiv:2211.13221_, 2022.\n' +
      '* [16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aherman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In _Proc. Int\'l Conf. Learning Representations_, 2022.\n' +
      '* [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _Advances in Neural Information Processing Systems_, pages 6840-6851, 2020.\n' +
      '* [18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '* [19] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-rank adaptation of large language models. In _Proc. Int\'l Conf. Learning Representations_, 2021.\n' +
      '* [20] Zhihao Hu and Dong Xu. VideocontoIntet: A motion-guided video-to-video translation framework by using diffusion model with controlnet. _arXiv preprint arXiv:2307.14073_, 2023.\n' +
      '* [21] Ondrej Jamriska, Sarka Sochorova, Ondrej Texler, Michal Lukac, Jakub Fiser, Jingwan Lu, Eli Shechtman, and Daniel Sykora. Stylizing video by example. _ACM Transactions on Graphics_, 38(4):1-11, 2019.\n' +
      '* [22] Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zero-shot grounded video editing using text-to-image diffusion models. _arXiv preprint arXiv:2310.01107_, 2023.\n' +
      '* [23] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2Video-Zero: Text-to-image diffusion models are zero-shot video generators. In _Proc. Int\'l Conf. Computer Vision_, 2023.\n' +
      '* [24] Nupur Kumar, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In _Proc. IEEE Int\'l Conf. Computer Vision and Pattern Recognition_, 2023.\n' +
      '* [25] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-P2P: Video editing with cross-attention control. _arXiv preprint arXiv:2303.04761_, 2023.\n' +
      '* [26] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. VideoFusion: Decomposed diffusion models for high-quality video generation. In _Proc. IEEE Int\'l Conf. Computer Vision and Pattern Recognition_, pages 10209-10218, 2023.\n' +
      '\n' +
      '* [27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In _Proc. Int\'l Conf. Learning Representations_, 2021.\n' +
      '* [28] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In _Proc. IEEE Int\'l Conf. Computer Vision and Pattern Recognition_, pages 6038-6047, 2023.\n' +
      '* [29] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In _Proc. IEEE Int\'l Conf. Machine Learning_, pages 16784-16804, 2022.\n' +
      '* [30] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In _ACM SIGGRAPH Conference Proceedings_, pages 1-11, 2023.\n' +
      '* [31] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. FateZero: Fusing attentions for zero-shot text-based video editing. In _Proc. Int\'l Conf. Computer Vision_, 2023.\n' +
      '* [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _Proc. IEEE Int\'l Conf. Machine Learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.\n' +
      '* [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proc. IEEE Int\'l Conf. Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.\n' +
      '* [35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proc. IEEE Int\'l Conf. Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.\n' +
      '* [36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In _Advances in Neural Information Processing Systems_, pages 36479-36494, 2022.\n' +
      '* [37] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, and Sungroh Yoon. Edit-A-Video: Single video editing with object-aware consistency. _arXiv preprint arXiv:2303.07945_, 2023.\n' +
      '* [38] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-A-Video: Text-to-video generation without text-video data. In _Proc. Int\'l Conf. Learning Representations_, 2023.\n' +
      '* [39] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _Proc. Int\'l Conf. Learning Representations_, 2021.\n' +
      '* [40] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In _Proc. IEEE Int\'l Conf. Computer Vision and Pattern Recognition_, pages 1921-1930, 2023.\n' +
      '* [41] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. _arXiv preprint arXiv:2303.17599_, 2023.\n' +
      '* [42] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. _arXiv preprint arXiv:2309.15103_, 2023.\n' +
      '* [43] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: Encoding visual concepts into textual embeddings for customized text-to-image generation. In _Proc. Int\'l Conf. Computer Vision_, 2023.\n' +
      '* [44] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-A-Video: One-shot tuning of image diffusion models for text-to-video generation. In _Proc. Int\'l Conf. Computer Vision_, pages 7623-7633, 2023.\n' +
      '* [45] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. GMFlow: Learning optical flow via global matching. In _Proc. IEEE Int\'l Conf. Computer Vision and Pattern Recognition_, pages 8121-8130, 2022.\n' +
      '* [46] Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, and Humphrey Shi. Prompt-free diffusion: Taking "text" out of text-to-image diffusion models. _arXiv preprint arXiv:2305.16223_, 2023.\n' +
      '* [47] Shuai Yang, Yifan Zhou, Ziwei Liu,, and Chen Change Loy. Rerender a video: Zero-shot text-guided video-to-video translation. In _ACM SIGGRAPH Asia Conference Proceedings_, 2023.\n' +
      '* [48] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. _arXiv preprint arXiv:2308.06721_, 2023.\n' +
      '* [49] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proc. Int\'l Conf. Computer Vision_, pages 3836-3847, 2023.\n' +
      '* [50] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. ControlVideo: Training-free controllable text-to-video generation. In _Proc. Int\'l Conf. Learning Representations_, 2024.\n' +
      '* [51] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>