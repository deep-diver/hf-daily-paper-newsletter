<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Simple and Scalable Strategies to Continually Pre-train Large Language Models\n' +
      '\n' +
      'Adam Ibrahim\\({}^{*\\dagger}\\)\n' +
      '\n' +
      'Benjamin Therien\n' +
      '\n' +
      'kshitij.gupta@mila.quebec\n' +
      '\n' +
      'Mats L. Richter\n' +
      '\n' +
      'mats.richter@mila.quebec\n' +
      '\n' +
      'Quentin Anthony\n' +
      '\n' +
      'qubitentin@gmail.com\n' +
      '\n' +
      'Timothee Lesort\n' +
      '\n' +
      't.lesort@gmail.com\n' +
      '\n' +
      'Eugene Belilovsky\n' +
      '\n' +
      'eugene.belilovsky@concordia.ca\n' +
      '\n' +
      'Irina Rish\n' +
      '\n' +
      'irina.rish@umontreal.ca\n' +
      '\n' +
      'Department of Computer Science and Operation Research,\n' +
      '\n' +
      'Universite de Montreal, Montreal, Canada\n' +
      '\n' +
      'Department of Computer Science and Software Engineering,\n' +
      '\n' +
      'Concordia University, Montreal, Canada\n' +
      '\n' +
      'Mila, Montreal, Canada\n' +
      '\n' +
      'EleutherAI\n' +
      '\n' +
      'Equal contribution; authorship order within equal contributors was randomized.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models - saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English\\(\\rightarrow\\)English) and a stronger distribution shift (English\\(\\rightarrow\\)German) at the 405M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Over the past few years, large pre-trained models have enabled massive performance improvements in language modeling (Brown et al., 2020; Zhao et al., 2023), visual understanding (Radford et al., 2021; Alayrac et al., 2022; Kirillov et al., 2023), text-to-image generation (Rombach et al., 2022; Pernias et al., 2024), and text-to-video generation (Brooks et al., 2024)--to name a few. Large language models (LLMs) are at the center of all these improvements, providing an intuitive means for humans to interface with machine learning algorithms through language.\n' +
      '\n' +
      'While LLMs are the cornerstone of current generative AI technology, they are prohibitively expensive to train and keep up to date. However, as new and higher-quality datasets continue to become available (Gao et al., 2020; Soboleva et al., 2023; Computer, 2023; Soldaini et al., 2024), organizations will need to update their models to stay abreast of the competition. Currently, LLMs are re-trained on a combination of old and newly collected data. Existing works aim to reduce these training costs by enabling low-cost hyperparameter optimization (Yang et al., 2022) or providing guidelines for maximizing performance under a given compute budget (Hoffmann et al., 2022). However, these works assume that models will be _trained from random initialization_, raising the following question: Should practitioners always combine existing datasets and _train from random initialization_ to obtain the best performance? Doing so for every update of the models quickly becomes prohibitively expensive.\n' +
      '\n' +
      'To circumvent the need for complete re-training, we explore simple and scalable continual learning strategies for continuing to pre-train LLMs (up to 10B parameters) on large amounts of new data (200B+ tokens). We refer to our setting as "continual pre-training" and highlight that it is _distinct_ from existing settings in the literature (Gururangan et al., 2020; Ke et al., 2022; Scialom et al., 2022; Xie et al., 2023) due to the large amount of incoming data we consider. In this work, we do not intend to improve on the performance of models trained from a random initialization on all of the available data. Instead, we consider models trained on the union of existing datasets as baselines whose performance we seek to match using a combination of continual learning strategies at scale.\n' +
      '\n' +
      'Naively continuing to train the model on new data, however, tends to lead to performance far below re-training on all available data, often due to 1) poor adaptation (failure to optimize the new dataset) or 2) catastrophic forgetting (significant capability loss on the previous dataset). Firstly, the question of adaptation is central to our setting as training on large datasets is costly. One would presumably not choose to spend considerable computational resources training on a new dataset only to minimally adapt to it. However, most performant open-source LLMs (Touvron et al., 2023; Jiang et al., 2023; Gemma Team et al., 2024) decay their learning rate to a small value by the end of training. We hypothesize, therefore, that the learning rate must be re-increased and re-decayed to improve adaptation per compute spent when training on a new dataset. We note that this has not been thoroughly studied in the continual learning literature. Secondly, catastrophic forgetting is a key difficulty to overcome if one is to realize the full potential of continual pre-training. Adapting to hundreds of billions of new tokens is important, but it must not come at the cost of erasing most existing knowledge in the LLM. Recent work (Scialom et al., 2022) shows, in an LLM fine-tuning setting, that replaying previous data (as little as 1%) is sufficient to mitigate forgetting to a large extent. While continually pre-training on large amounts of new data will almost surely lead to more forgetting than fine-tuning, we hypothesize that an appropriate amount of replay could mitigate forgetting--even in our setting. Moreover, recent works show that pre-training (Cossu et al., 2022; Ramasesh et al., 2022; Mehta et al., 2023) and increasing model size (Mirzadeh et al., 2022) both help to reduce the effects of forgetting. We, therefore, expect the trend of increasing language model capacity and pre-training dataset size in tandem (Kaplan et al., 2020; Hoffmann et al., 2022; Touvron et al., 2023) will yield models increasingly capable of continual learning (Scialom et al., 2022), suggesting that our experimental results should only improve with models scale.\n' +
      '\n' +
      'Given the great potential for continual learning to considerably reduce costs associated with re-training models and the potential for LLMs to be strong continual learners, we ask ourselves the following question: _when simple and scalable continual learning techniques are applied, what is the performance difference between continually pre-trained LLMs relative to LLMs pre-trained from random initialization on the union of all data?_ To answer this question, we conduct a large-scale empirical study of continual learning techniques for LLM pre-training. Our empirical evaluation spans large (10B parameters) and small (405M parameters) models as well as weak (English \\(\\rightarrow\\) English) and stronger (English \\(\\rightarrow\\) German) distribution shifts. Our main contributions can be summarized as follows:\n' +
      '\n' +
      '1. We establish the effect of learning rate re-warming and re-decaying for models pre-trained using a cosine schedule, showing that re-warming and re-decaying is necessary for adaptation during continual pre-training.\n' +
      '\n' +
      ' 2. We establish the effect of replaying previous data while keeping compute constant across two distribution shifts and many replay percentages. We find that, even when updating models on hundreds of billions of new tokens, it is possible to significantly mitigate forgetting with an appropriate amount of replay.\n' +
      '3. We demonstrate, across two model sizes and distribution shifts, that a simple and scalable combination of LR re-warming, LR re-decaying, and compute-equivalent replay allows continually pre-trained models to attain similar performance to models re-trained on the union of all data while using significantly less compute.\n' +
      '4. We propose infinite learning rate schedules (schedules allowing smooth transition across datasets) for the continual pre-training of LLMs as a promising way to circumvent optimization difficulties associated with learning rate re-warming.\n' +
      '\n' +
      'Upon publication, we will make our code and final model checkpoints publicly available.\n' +
      '\n' +
      '## 2 Main findings and Takeaways\n' +
      '\n' +
      'Our experimental results assume that continually pre-trained LLMs undergo two or more pre-training phases sequentially. That is, our results apply to situations where a continually pre-trained LLM is randomly initialized and pre-trained on datasets \\(\\mathcal{D}_{0},\\mathcal{D}_{1},\\dots,\\mathcal{D}_{N-1}\\) in sequence where \\(N\\geq 2\\) and \\(\\textit{tokens}(\\mathcal{D}_{i})\\geq 100\\)B. We note that this includes situations where the LLM in question is an open-source model (Touvron et al., 2023; 20; Jiang et al., 2023; Gemma Team et al., 2024) which has already been pre-trained on \\(\\mathcal{D}_{0}\\) and situations where organizations may wish to train an initial LLM with the intention of continually pre-training it on new data. The new data may be similar to the previous data, corresponding to a weak distribution shift (e.g., the latest web-scrape of different domains), or quite different from previous data, corresponding to a strong distribution shift (e.g., data from a completely new language). Our experimental evaluation accounts for these difficulties, finding that appropriately applying LR re-warming, LR re-decaying, and replay is sufficient to match the performance of re-training across weak and strong distribution shifts and two model sizes (see\n' +
      '\n' +
      'Figure 1: **Continual pre-training decreases computational costs of updating the model while maintaining similar final validation and evaluation performance. We report results for Pile \\(\\cup\\) SlimPajama(SP)/German(Ger.) Baseline model trained on the union of both datasets which we consider to be an upper bound on performance. We also report performance for two continually pre-trained models. “PT on Pile” starts from a pre-trained Pile checkpoint and only uses learning rate re-warming and re-decaying, while “Replay (PT on Pile)” re-warms the learning rate, re-decays it, and uses 5% replay for SlimPajama and 25% replay for German. We observe that the combination of LR re-warming, re-decaying, and replay allows our continually pre-trained model to attain similar performance to the baseline model while requiring substantially less compute. We note that this setting assumes that a pre-trained model is available (e.g., via Huggingface hub or an in-house model designed to be continually pre-trained).**\n' +
      '\n' +
      'Fig. 1). To make our findings as accessible to the community as possible, we now provide _Rules of thumb_ for applying our findings:\n' +
      '\n' +
      '```\n' +
      'Rules of thumb for continual pre-training\n' +
      '```\n' +
      '\n' +
      '**Caveat**--The following guidelines are written to the best of our _current knowledge_.\n' +
      '\n' +
      '**Learning rate schedule:**\n' +
      '\n' +
      '* If the learning rate was cosine-decayed from a large value \\(\\eta_{max}\\) to a small value \\(\\eta_{min}\\) during pre-training on the initial dataset, the following guidelines can help to continually pre-train your model:\n' +
      '* Re-warming and re-decaying the learning rate from \\(\\mathcal{O}(\\eta_{max})\\) to \\(\\mathcal{O}(\\eta_{min})\\) improves adaptation to a new dataset, e.g. compared to continuing from small learning rates \\(\\mathcal{O}(\\eta_{min})\\).\n' +
      '* Decreasing the schedule\'s maximum learning rate can help reduce forgetting, whereas increasing it can improve adaptation.\n' +
      '* Infinite LR schedules are promising alternatives to cosine decay schedules. They transition into a high constant learning rate across tasks, helping prevent optimization-related forgetting by avoiding re-warming the LR between tasks. They also avoid committing to a specific budget of tokens as a final exponential decay can be used to train the model to convergence at any point during training.\n' +
      '**Replay:**\n' +
      '\n' +
      '* As a default value, we recommend 5% replay. More replay should be used with stronger distribution shifts, while one can get away with as little as 1% for weak distribution shifts.\n' +
      '\n' +
      '## 3 Related Work\n' +
      '\n' +
      '### Continual learning\n' +
      '\n' +
      'Continual learning (CL) approaches aim to learn from an evolving data distribution, adapting to novel data while retaining knowledge gathered through prior training (French, 1999; Rolnick et al., 2019; Caccia et al., 2020; Lesort et al., 2021). The key challenge of continual learning is to avoid forgetting past information, while also adapting to novel information. This trade-off is known as the rigidity-plasticity dilemma (Mermillod et al., 2013; Ostapenko et al., 2019; Riemer et al., 2019).\n' +
      '\n' +
      'CL approaches are convenient even in small-scale settings to avoid re-training from scratch or to bridge the data availability issue (Smith et al., 2021). However, at scale, CL is more than a convenience; it may be necessary to process huge amounts of continually gathered data. The recent increase in training scale, most notably for LLMs (Scao et al., 2022; Brown et al., 2020; Zhao et al., 2023), offers new opportunities for CL to reduce the cost of re-training and increase efficiency for memory, computing, and storage (Prabhu et al., 2023; Aljundi et al., 2019; Harun et al., 2023; Veniat et al., 2021; Harun et al., 2023). Just as federated learning can enable the sharing of compute and data between different agents co-located in space (McMahan et al., 2017; Reddi et al., 2021; Douillard et al., 2023; Ryabinin et al., 2021), continual learning allows the sharing of compute and data progressively through time and could be a useful tool for large-scale training.\n' +
      '\n' +
      'Recent work shows that optimizers such as SGD and Adam have interesting knowledge retention properties in DNNs that could be beneficial at scale for CL (Lesort et al., 2023) and that just a small amount of replay could be sufficient to boost knowledge accumulation (Scialom et al., 2022). In this work, we want to benefit from the efficiency of those approaches in the context of large language models pretraining and boost them with the right learning rate scheduling and replay policy.\n' +
      '\n' +
      '### Pre-training, Model Scale, and Continual-learning\n' +
      '\n' +
      'Several existing works evaluate the impact of pre-training and model scale on continual learning. Cossu et al. (2022) investigate pre-training scenarios for language and vision. They find that unsupervised and self-supervised pre-training plays a fundamental role in mitigating forgetting, while supervision hurts performance. Similarly, Mehta et al. (2023) find that pre-trained models forget less than randomly initialized models, due to their weights lying in flatter regions of the loss landscape. They also find that larger models forget less which is connected to the findings of Ramasesh et al. (2022); Mirzadeh et al. (2022). The former finds that pre-trained models forget less as they are scaled up, suggesting that it may be due to the hidden representations growing more orthogonal with scale. The latter finds that wider neural networks forget less compared to their parameter-equivalent deeper counterparts. Hernandez et al. (2021) establish scaling laws for transfer: equations that can predict the performance of a neural network on a new task as a function of its parameter count and pre-training dataset size. The authors find that this positive transfer consistently improves as the parameter count increases. Finally, Scialom et al. (2022) show that autoregressive LLMs have a strong ability to learn continually which they hypothesize is related to their pre-training objective.\n' +
      '\n' +
      '### Domain Adaptive Continual Pre-training (DACPT)\n' +
      '\n' +
      'Existing work considers Domain Adaptive Continual Pre-training (DACPT), a setting where a series of unlabelled domains become available to the LM sequentially and practitioners wish to train on each domain in a self-supervised fashion while retaining performance across each of them. While the objective is similar to our own, we consider general-purpose pre-training datasets that mix many domains as opposed to domain-specific datasets. Ke et al. (2022) assume data from previous domains is not available when training on new domains and develop a new technique for this setting which involves an importance mask of parameters for all previous tasks to prevent forgetting when pre-training with a masked language modeling (MLM) objective. Gururangan et al. (2020) investigated domain and task adaptive pre-training of RoBERTa (also MLM) and contributed a sample selection strategy for efficient continual pre-training. Similarly, Xie et al. (2023) also propose a data selection strategy that reduces the computational cost of continual pre-training (shown for autoregressive LMs). Qin et al. (2023) investigate re-cycling fine-tuned adapter layers of previous base LMs as the initialization of new adapters for adapting continually updated versions of the base LM to specific tasks. Recently, Wu et al. (2024) proposed LLaMA Pro, a method for the continual pre-training of LLMs that enables learning new tasks without forgetting previous knowledge. However, unlike our work which considers adapting all existing weights, LLaMA Pro requires growing the size of the model for each new update and only adjusting the new weights.\n' +
      '\n' +
      '### Continual Learning for LMs Applied to Specific Domains\n' +
      '\n' +
      'Several related works apply continual pre-training to specific tasks and domains (Sun et al., 2020; Jang et al., 2022; 2022; Gong et al., 2022; Zan et al., 2022; Yadav et al., 2023; Ma et al., 2023; Yang et al., 2024). While these works also utilize continual pre-training techniques, they differ from our work by focusing on particular domains instead of general pre-training techniques, on smaller-scale datasets \\(<\\) 10B tokens with smaller models. The only existing work that approaches our dataset scale is (Gogoulou et al., 2023), which explores continual autoregressive language modeling across English, Danish, Icelandic, and Norwegian datasets (73B each). While they do not use replay they do re-warm and re-decay the learning rate. The only existing work that approaches our model scale is (Yang et al., 2024). They continually pre-train and instruction tune LLaMA2 on small-scale academic plant science data. This concurrent work uses a very similar continual learning setup to the one we propose: replay, LR re-warming, and LR re-decaying. While, unlike our work, they _do not_ build a controlled experimental framework to systematically evaluate the validity of these approaches for continual pre-training, it is nice to see further experimental evidence validating our approach.\n' +
      '\n' +
      '### Learning Rate Schedules\n' +
      '\n' +
      'Several studies have examined the impact of different learning rate (LR) schedules on the training stability and final performance of neural networks. Goyal et al. (2018) found that a gradual warm-up of LR early onin training can help overcome optimization challenges, particularly with large mini-batch sizes. Additionally, Popel and Bojar (2018) emphasized the importance of a warm-up stage when training Post-LN Transformers. On the other hand, Xiong et al. (2020) discovered that Pre-LN Transformers are more stable and may not require a warm-up stage. You et al. (2019) explored the role of the LR decay and found that a large initial LR prevents the network from memorizing noisy data, whereas a small LR helps learn complex patterns. Kaplan et al. (2020) explored LR schedules for pre-training Large Language Models (LLMs) and found that schedule choice did not significantly impact performance. Correcting this erroneous finding, Hoffmann et al. (2022) found that the LR schedule does play an important role. Hoffmann et al. (2022) and Rae et al. (2021) established best practices for using a cosine schedule when pre-training LLMs, which have become widely adopted. In contrast, Raffel et al. (2023) and Zhai et al. (2022) explore LR schedules that follow the inverse square root decay for large-scale pre-training. Raffel et al. (2023) utilized an inverse square root decay for training LLMs, allowing flexibility in adjusting the number of training steps. In Zhai et al. (2022), authors use these schedules referred to as \'infinite learning rate schedules\' to train vision transformers. These schedules enable indefinite training and the evaluation of multiple training durations in a single run. We note that our proposed infinite learning rate schedules for LLMs (Sec. 7.4) are inspired by this idea.\n' +
      '\n' +
      '## 4 Background & Methodology\n' +
      '\n' +
      'In this section, we provide appropriate background and methodology as it relates to continual pre-training in the context of LLMs.\n' +
      '\n' +
      '### Linear Warmup and Cosine Decay Schedule\n' +
      '\n' +
      'Hoffmann et al. (2022) and Rae et al. (2021) established best practices for using a cosine schedule when pre-training LLMs. Specifically, they recommend starting with a linear warmup phase and decaying the learning rate to \\(10\\times\\) its maximum value such that the end of the cosine cycle is set to match the number of tokens. While the linear warmup duration differs, most notable works have a duration between 0.1% and 0.5% for training steps (Zhao et al., 2023). Given that many popular open-source models (Touvron et al., 2023; 20; Almazrouei et al., 2023) follow this learning rate schedule recipe, it is critical to understand its nuances for continually pre-training such models. The schedule first linearly increases the learning rate over \\(T_{warmup}\\) timesteps, or equivalently until some timestep \\(t_{ann}=T_{warmup}\\):\n' +
      '\n' +
      '\\[\\eta_{t}=\\eta_{max}\\cdot\\frac{t}{T_{warmup}} \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\eta_{t}\\) is the value of the learning rate at iteration \\(t\\), and \\(\\eta_{max}\\) is the maximum learning rate. The schedule then transitions into a cosine annealing phase over \\(T_{ann}\\) timesteps, equivalently until some timestep \\(t_{end}=T_{ann}+t_{ann}\\):\n' +
      '\n' +
      '\\[\\eta_{t}=\\eta_{min}+\\frac{(\\eta_{max}-\\eta_{min})}{2}\\cdot\\left(\\cos\\left( \\pi\\cdot\\frac{t-t_{ann}}{t_{end}-t_{ann}}\\right)+1\\right) \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\eta_{max}\\) is the maximum learning rate and \\(\\eta_{min}\\) is the minimum learning rate.\n' +
      '\n' +
      '### Compute-equivalent Replay\n' +
      '\n' +
      'In many of our experiments, we compare models trained with replay to models trained without it. When making such comparisons, we keep the amount of compute constant for training both models. That is, we correspondingly reduce the number of tokens seen from the new dataset to accommodate the additional tokens seen from the replay buffer. We refer to this use of replay as _compute-equivalent replay_. For instance, suppose datasets \\(\\mathcal{D}_{0}\\) and \\(\\mathcal{D}_{1}\\) each contain 100B tokens. We wish to compare model (a) trained sequentially on \\(\\mathcal{D}_{0}\\) and \\(\\mathcal{D}_{1}\\) to model (b) trained sequentially on \\(\\mathcal{D}_{0}\\) and \\(\\mathcal{D}_{1}\\) with 5% compute equivalent replay. Model (a) will see all tokens from both datasets for a total of 200B unique tokens. Model (b) will see 100B unique tokens of \\(\\mathcal{D}_{0}\\) and 95B unique tokens of \\(\\mathcal{D}_{1}\\) plus 5B replayed tokens from \\(\\mathcal{D}_{0}\\) for a total of 200B tokens. In this way, both compared models expend the same amount of compute.\n' +
      '\n' +
      'Replay in the two dataset settingIn our settings that span only two datasets \\((\\mathcal{D}_{0},\\mathcal{D}_{1})\\), we use replay of data from \\(\\mathcal{D}_{0}\\) when training on \\(\\mathcal{D}_{1}\\). We refer to these models as "\\(\\mathcal{D}_{1}\\)\\(x\\)% Replay", where \\(x\\) is the percentage of data in each training batch that comes from \\(\\mathcal{D}_{0}\\). Conversely, \\((100\\%-x)\\)% of the samples in each training batch will be sampled from \\(\\mathcal{D}_{1}\\). When comparing models trained with replay to other configurations, we ensure that the compute is _equivalent_ by reducing the number of \\(\\mathcal{D}_{1}\\) tokens to accommodate replay tokens from \\(\\mathcal{D}_{0}\\).\n' +
      '\n' +
      '## 5 Experimental Setup\n' +
      '\n' +
      'To empirically evaluate the effectiveness of continually pre-training LLMs in comparison to training LLMs from a random initialization, we select recent pre-training datasets from the literature, outline practical continual pre-training settings for investigation, and select several baselines to compare with our proposed techniques. Our goal is to fairly compare our continual pre-training techniques to baselines in a controlled setting. We _do not_ seek to obtain state-of-the-art performance or compare with models out of the scope of this paper.\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      'We use three datasets for training and validation: SlimPajama (Soboleva et al., 2023), German CommonCrawl (Laippala et al., 2022), and Pile (Gao et al., 2020). For all datasets, use the same tokenizer as Black et al. (2022) trained specifically on the Pile. To create our training set for SlimPajama, we randomly sub-sample the dataset (606B Total Tokens) to form a \\(\\sim\\)299B token subset (see Table 1) that is of comparable size to Pile. We also further sub-sample this SlimPajama subset to create three \\(\\sim\\) 100B token splits of the dataset (see Sec. 7.4 for details). To create the SlimPajama validation set we simply tokenize the default validation set that has been extensively deduplicated (Soboleva et al., 2023). To create the German training and validaiton sets, we split and tokenized the German Common Crawl scrape, available as part of the Oscar Dataset (Laippala et al., 2022), into a 195.43B token training set and a 982.6M token validation set. The pile dataset comes pre-shuffled and mixed, we simply used the default training and validation sets. The training set is \\(\\sim 330\\)B tokens total, though in our experiments we only train on a 300B token subset.\n' +
      '\n' +
      '### Continual Learning Settings\n' +
      '\n' +
      'We consider three realistic continual pre-training settings in the main body and provide results for a third which we believe is less warranted in the appendix. Each setting was carefully selected to expose different challenges and strengths of continual pre-training. Our setups assume that continually pre-trained LLMs undergo two or more pre-training phases sequentially. That is, our results apply to situations where a continually pre-trained LLM is randomly initialized and pre-trained on datasets \\(\\mathcal{D}_{0},\\mathcal{D}_{1},\\dots,\\mathcal{D}_{N-1}\\) in sequence\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r} \\hline \\hline Dataset & Size (Tokens) & Sampling (\\%) \\\\ \\hline Wikipedia & 11.96B & 4.00 \\\\ Book & 12.58B & 4.20 \\\\ C4 & 79.87B & 26.69 \\\\ Stack Exchange & 10.09B & 3.37 \\\\ GitHub & 15.63B & 5.22 \\\\ Common Crawl & 155.89B & 52.09 \\\\ Arxiv & 13.25B & 4.43 \\\\ \\hline Total & 299.28B & 100.00 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Domain sizes of the 300B token training set of SlimPajama.** We sub-sampled the SlimPajama dataset (606B total tokens) into a 300B token split to make it of comparable size to Pile. We report the size of the subsampled domains that make up SlimPajama and the sampling percentage used at training time (e.g., the percentage of samples in each batch that come from a certain domain).\n' +
      '\n' +
      'where \\(N\\geq 2\\). For the realistic settings we consider \\(\\textit{tokens}(\\mathcal{D}_{i})\\geq 100\\)B. In each case, we consider the following natural baselines:\n' +
      '\n' +
      '* A model trained from random initialization on the union of all datasets i.e. \\(\\bigcup_{i=0}^{N-1}\\mathcal{D}_{i}\\), and\n' +
      '* A model trained from random initialization on individual dataset \\(\\mathcal{D}_{i}\\), \\(0\\leq i\\leq N\\).\n' +
      '\n' +
      '\\(N=2\\) **settings -** Here we assume a model is available (e.g. via hugging face or pre-trained in-house) that has been pre-trained for autoregressive language modeling on a dataset (\\(\\mathcal{D}_{0}\\)) using a linear warmup and cosine decay LR schedule. We also assume that the schedule follows existing conventions in the literature (e.g. decaying to the token budget; see Sec. 4 for details) as it is the case for most performant pre-trained LLMs (Rae et al., 2021; Hoffmann et al., 2022; Touvron et al., 2023a;b). Given a model pre-trained on \\(\\mathcal{D}_{0}\\), we now assume that a practitioner wants to update this model on a new dataset \\(\\mathcal{D}_{1}\\) using the same self-supervised objective. We consider the following concrete variations of the **two-dataset setting**:\n' +
      '\n' +
      '* **Two datasets, weak shift**: In this variation, we consider \\(\\mathcal{D}_{0}\\) to be the Pile (Gao et al., 2020) and \\(\\mathcal{D}_{1}\\) to be pre-training on SlimPajama (Soboleva et al., 2023). SlimPajama is an extensively deduplicated version of RedPajama (Computer, 2023) which is built based on the LLaMA dataset (Touvron et al., 2023a). We consider this to be a weak but realistic distribution shift as both datasets are English-language, contain common domains(CommonCrawl, GitHub, Arxiv, Wikipedia, and StackExchange) as well as other non-overlapping domains. Additionally, SlimPajama (2023) is a newer dataset than Pile (2020) and is therefore likely to have newer data within the overlapping domains. Therefore, despite the potential overlap, we believe this transition is realistic and is likely to be of interest to many practitioners wishing to update an LLM on a similar distribution to pre-training (e.g., newly collected data of the same sources with higher quality filtering).\n' +
      '* **Two datasets, stronger shift**: In this variation, we consider \\(\\mathcal{D}_{0}\\) to be pre-training on the Pile (Gao et al., 2020) and \\(\\mathcal{D}_{1}\\) to be pre-training on German Common Crawl. German Common Crawl is a \\(\\sim 200\\)B token dataset taken from the Oscar dataset (Laippala et al., 2022). We note that this constitutes a stronger shift given the change of language. This setting is of particular interest for practitioners wishing to augment an LLM with a new natural language, programming language, or specific domain that is notably different in vocabulary from pre-training. We note, however, that as the domain strays farther and farther away from the tokenizer\'s training corpus, the tokenizer may become a key bottleneck to performance. We leave the treatment of the tokenizer to future work.\n' +
      '\n' +
      '\\(N>2\\) **settings -** We also consider the following settings with more dataset transitions to investigate how well the methods considered scale with more datasets:\n' +
      '\n' +
      '* **Three datasets, no shift** : We consider an \\(N=3\\) setting, where \\(\\mathcal{D}_{0},\\mathcal{D}_{1},\\mathcal{D}_{2}\\) are each district 100B token splits of SlimPajama. This setting is primarily used to evaluate the ability of our techniques to scale to many future updates and to assess the performance of our proposed infinite learning rate schedules.\n' +
      '* **Domain incremental continual pre-training**: This setting considers consuming the tokens of SlimPajama sequentially ordered by domain. That is, we train on a sequence of \\(N\\) future datasets \\(\\{\\mathcal{D}_{0},\\mathcal{D}_{1},\\dots,\\mathcal{D}_{N-1}\\}\\) each of is a distinct domain of SlimPajama 300B. We note that this is similar to DACPT (Ke et al., 2022), however, we consider much larger datasets for each domain. This setting is particularly challenging due to the distribution shift experience at the transition between each domain. While it is certainly interesting, we believe it is unnecessarily difficult compare to mixing the SlimPajama data before training on it. The poor results in this setting (Sec. A.1 of the appendix) suggest that general-purpose LLMs should be continually pre-trained on a mixture of domains if possible, not updated per domain.\n' +
      '\n' +
      '### Training Setup\n' +
      '\n' +
      'Using GPT-NeoX (Andonian et al., 2021) based on Megatron-DeepSpeed (Shoeybi et al., 2019; Microsoft, 2020), we train autoregressive decoder-only transformers with a causal language modeling objective. The models use Pre-LN. Each model is trained using the same tokenizer as Black et al. (2022), which was trained exclusively on the Pile via the BPE algorithm (Sennrich et al., 2016). For all models, we train with the adamW optimizer (Loshchilov and Hutter, 2019) using a batch size of 1104 and a sequence length of 2048. Training on 300B tokens, thus, takes \\(132,366\\) total steps of gradient descent. We consider two model sizes 405M and 9.6B parameters (referred to as 10B in this work) including embeddings. We train the smaller models using data parallelism across 46 6 GPU nodes using a micro-batch size of 4. The larger model is trained using tensor parallelism(Shoeybi et al., 2020) spanning six GPUs within a node and pipeline parallelism(Huang et al., 2019) spanning four nodes; that is, each model replica spans 24 GPUs across four nodes. We train this model on 276 nodes using gradient accumulation of 4 steps. Each model uses optimizer sharding via ZeRO-1 (Rajbhandari et al., 2020), activation checkpointingChen et al. (2016), activation partitioning across tensor parallel ranks, and mixed precision FP16/FP32 to reduce GPU memory consumption and fully utilize NVIDIA tensor cores during training. We provided an extended description of all hyperparameters in the appendix (Table. 13).\n' +
      '\n' +
      '### German and English LM Evaluation Benchmark\n' +
      '\n' +
      'We measure performance on a wide variety of downstream tasks, which can be broadly categorized as follows.\n' +
      '\n' +
      'English Benchmarks\n' +
      '\n' +
      '* **Commonsense Reasoning (0-shot):** HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2019), PIQA (Bisk et al., 2019), OpenBookQA (Mihaylov et al., 2018), ARC-Easy, ARC-Challenge (Clark et al., 2018)\n' +
      '* **World Knowledge (5-shot):** NaturalQuestions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017)\n' +
      '* **Reading Comprehension (0-shot):** BoolQ (Clark et al., 2019)\n' +
      '* **Math:** MathQA (Amini et al., 2019)\n' +
      '* **Popular Aggregated Results:** MMLU (5-shot) (Hendrycks et al., 2021)\n' +
      '\n' +
      'German Benchmarks from (Pluster, 2023), which translated their English counterparts using GPT 3.5 API\n' +
      '\n' +
      '* **Commonsense Reasoning (0-shot):** HellaSwag-DE (Zellers et al., 2019), ARC-Challenge-DE (Clark et al., 2018)\n' +
      '* **World Knowledge (5-shot):** TriviaQA-DE (Joshi et al., 2017)\n' +
      '* **Popular Aggregated Results:** MMLU-DE (5-shot) (Hendrycks et al., 2021)\n' +
      '\n' +
      '## 6 Results\n' +
      '\n' +
      'We focus on continual pre-training when incoming datasets are large (200B tokens+). In such settings, training is expensive, thus, it is critical to efficiently adapt to the large amount of incoming data. However, most performant LLMs (Rae et al., 2021; Hoffmann et al., 2022; Zhao et al., 2023; Touvron et al., 2023b;a) are trained with a linear warmup and cosine decay schedule with a relatively low minimum learning rate. We hypothesize that **re-warming** this learning rate to a relatively high value and subsequently re-decaying it is needed to efficiently adapt to the new dataset. To this end, in section 6.1 we study the effect of linear warmup duration, re-warming the LR, re-decaying the LR, and maximum learning rate magnitude on adaptation and forgetting. Finding that re-warming and re-decaying increases both adaptation and forgetting, in section 6.2 we investigate whether replay can help mitigate forgetting when the learning rate is re-warmed and re-decayed. Subsections 6.3 and 6.4 combine the strategies studied in the previous two sections and report their performance relative to baselines for weak and strong distribution shifts and at large model scale.\n' +
      '\n' +
      'Finally, in section 7, we illustrate LR re-warming can cause unwanted forgetting, introduce infinite learning rate schedules as a promising way to circumvent it, and compare these schedules to baselines.\n' +
      '\n' +
      '### Learning Rate Schedule\n' +
      '\n' +
      'Given the influence that the learning rate can have on adaptation and the low final LR values of prominent LLMs (Rae et al., 2021; Hoffmann et al., 2022; Zhao et al., 2023; Touvron et al., 2023b;a), we hypothesize that the LR should be re-warmed and re-decayed to promote adaptation during continual pre-training. In this section, we investigate the effect of linear warmup duration, re-warming the LR, re-decaying the LR, and the magnitude of the \\(\\eta_{max}\\) when continuing to pre-train. Specifically, we evaluate their respective effects in the **two-dataset weak shift** setting (300B Pile \\(\\rightarrow\\) 300B SlimPajama) and the **two-dataset stronger shift** setting (300B Pile \\(\\rightarrow\\) 300B SlimPajama). Notably, the model trained on \\(\\mathcal{D}_{0}\\) (300B tokens of Pile) follow a linear warmup and cosine decay schedule1, simulating many common open-source pre-trained LLMs.\n' +
      '\n' +
      'Footnote 1: For all cosine decays in this paper, unless otherwise specified, we fit the cosine annealing phase to the token budget, set the linear warmup duration (\\(T_{warmup}\\)) to 1% of training iterations, and set \\(\\eta_{min}=0.1\\cdot\\eta_{max}\\)\n' +
      '\n' +
      '#### 6.1.1 The Effect of Linear Warmup for Weak and Strong Distribution Shifts.\n' +
      '\n' +
      'We first investigate the effect of linear warm-up duration on forgetting and adaptation in the **two datasets, weak shift** and **two dataset, stronger shift** settings (see Sec. 5.2 for details). The models are pre-trained on 300B tokens of Pile (Gao et al., 2020) (\\(\\mathcal{D}_{0}\\)). We continue to pre-train the models on SlimPajama (weak shift) and German Common Crawl (strong shift) for the first 50B tokens of training. We re-warm and re-decay\n' +
      '\n' +
      'Figure 2: **The effect of linear warmup for weak and strong distribution shifts.** (a),(b) and (c),(d) have the same legends respectively, shown in the right figures. We train 405M parameters models following a linear warmup and cosine decay schedule with varying linear warmup durations: 0%,0.5%,1%, and 2% of training iterations. Each learning rate schedule decays to \\(0.1\\eta_{max}\\) by the end of training based on the size of the dataset. We report results for the first 50B tokens of training. In the settings explored, we observe that the duration of the warm-up phase does not appear to be impactful when continuing to pre-train.\n' +
      '\n' +
      'the learning rate using a cosine learning rate schedule set to reach its minimal value (\\(\\eta_{min}=0.1\\cdot\\eta_{max}\\)) at 300B and 200B tokens, respectively. We consider warming up the learning rate for 0.5%, 1%, and 2% of \\(\\mathcal{D}_{1}\\)\'s total training iterations (132366 and 86000 iterations, respectively). Additionally, we train a model with no linear warm-up (0%) that immediately decays the LR from \\(\\eta_{max}\\). All experiments are conducted on a 405M parameter model.\n' +
      '\n' +
      'Figure 2 reports the validation losses for \\(\\mathcal{D}_{0}\\) and \\(\\mathcal{D}_{1}\\) for all models throughout the first 50B tokens of continued pre-training on \\(\\mathcal{D}_{1}\\). The top row reports results for the weak distribution shift, while the bottom row reports results for the strong distribution shift. Across both distribution shifts, we observe initially that models using shorter linear warmup initially forget and adapt faster than their longer warmup counterparts. This happens because they increase the LR faster which leads to faster forgetting and adaptation. In all scenarios, however, these initial differences diminish throughout training, leaving all models with relatively similar forgetting and adaptation after 50B tokens. Thus, in the settings explored, **the duration of linear warm-up phase does not appear to be impactful on forgetting or adaptation when continuing to pre-train.** With this in mind, we set a linear warmup duration of 1% of training iterations for all subsequent experiments.\n' +
      '\n' +
      '1.2 The effect of re-warming, re-decaying, and varying \\(\\eta_{max}\\) for Weak and Strong Distribution Shifts.\n' +
      '\n' +
      'We now investigate the benefits of re-warming and re-decaying the learning rate (e.g., following a cosine schedule) for different values of \\(\\eta_{max}\\). Specifically, we compare these models to two natural baselines: a model that does not re-warm, staying constant at \\(\\eta_{min}\\) (\\(3\\cdot 10^{-5}\\)), and a model that re-warms to the pre-training\n' +
      '\n' +
      'Figure 3: **The effect of re-warming and re-decaying the learning rate on adaptation and forgetting. We consider two constant baselines and three models that re-warm and re-decay. One baseline continues training from \\(\\eta_{min}\\) of pre-training (\\(3\\cdot 10^{-4}\\)) while the other warms up to \\(\\eta_{max}\\) from pre-training (\\(3\\cdot 10^{-4}\\)). For the models that re-warm and re-decay we vary \\(\\eta_{max}\\in\\{1.5\\cdot 10^{-4},3\\cdot 10^{-4},6\\cdot 10^{-4}\\}\\). All models except the \\(\\eta_{min}\\) baseline use linear warmup for 1% training iteration. The non-baseline models cosine decay the learning to reach 0.1 - \\(\\eta_{max}\\) by the end of training. We observe that re-warming and re-decaying the learning rate is needed to best adapt to the new dataset. Small increases or decreases in \\(\\eta_{max}\\) allow to trade-off between more or less adaptation. A stronger distribution shift seems to be a catalyst for both forgetting and adaptation.**\n' +
      '\n' +
      '\\((3\\cdot 10^{-4})\\) but does not re-decay. We use the same two two-dataset settings: we first pre-train on the Pile (\\(\\mathcal{D}_{0}\\)) for 300B tokens and continually pre-train our model on SlimPjama (weak shift) or German Common Crawl (strong shift) as our \\(\\mathcal{D}_{1}\\) datasets. The continual pre-training is conducted for the full size (300B and 200B tokens, respectively) of the datasets. The models that re-warm and re-decay the LR consider three strategies: re-warming to half the pre-training\'s \\(\\eta_{max}\\) (\\(1.5\\cdot 10^{-4}\\)), re-warming to the same \\(\\eta_{max}\\) as pre-training (\\(3\\cdot 10^{-4}\\)), and re-warming to twice the \\(\\eta_{max}\\) of pre-training (\\(6\\cdot 10^{-4}\\)). In all cases, the learning rate is cosine-decayed after linear warmup to reach \\(\\eta_{min}=0.1\\cdot\\eta_{max}\\) by the end of training. Finally, we consider models trained on \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\) as a third baseline to provide an upper bound on performance.\n' +
      '\n' +
      'Figure 3 reports validation losses for the \\(\\mathcal{D}_{0}\\) and \\(\\mathcal{D}_{1}\\) datasets throughout the continual pre-training of all models. The top row of plots reports results for the weak distribution shift (300B Pile\\(\\rightarrow\\)300B SP), while the bottom row reports results for the strong distribution shift (300B Pile\\(\\rightarrow\\)200B Ger.). For both shifts, the \\(\\eta_{min}\\) constant learning rate model achieves the least forgetting on \\(\\mathcal{D}_{0}\\). It also adapts the least on \\(\\mathcal{D}_{1}\\) for the strong shift, however, for the weak shift it adapts more than the \\(\\eta_{max}\\) constant baseline. When comparing these baselines to the models that re-warm and re-decay on both datasets, we observe that the latter models adapt better to the new dataset by a significant margin for both distribution shifts. This shows that re-warming and re-decaying are necessary to maximize adaptation to the new dataset when continually pre-training LLMs. Among the models that re-warm and re-decay the LR, we observe that varying the learning rate causes small differences in adaptation and forgetting: higher values of \\(\\eta_{max}\\) lead to more forgetting and more adaptation while the opposite is true for lower values. When comparing the baselines to the union-trained baseline, we observe that the final validation loss for \\(\\mathcal{D}_{1}\\) is significantly higher than the union-trained model on both distribution shifts. This is also the case for \\(\\mathcal{D}_{1}\\) on the weak distribution shift, but interestingly for the strong distribution shift, the constant baselines achieve lower \\(\\mathcal{D}_{1}\\) validation loss than the union-trained model. We hypothesize that this is due to the stronger distribution shift generally enhancing adaptation and exacerbating forgetting in the context of LLMs. When comparing models continually pre-trained with re-warming and re-decaying to the union baseline, we note that these models adapt better (lower final validation loss) to \\(\\mathcal{D}_{1}\\) than the union baseline. However, these models experience significant forgetting on \\(\\mathcal{D}_{0}\\), showing the need for replay to make these models competitive with the union baseline.\n' +
      '\n' +
      '_In summary, continually pre-training LLMs, both re-warming and re-decaying are necessary to maximize adaptation to the new dataset; small increases or decreases in \\(\\eta_{max}\\) allow to trade-off between more or less adaptation; a stronger distribution shift between \\(\\mathcal{D}_{0}\\) and \\(\\mathcal{D}_{1}\\) exacerbates forgetting and enhances adaptation; and the duration of linear warm-up phase does not appear to be impactful on forgetting or adaptation._\n' +
      '\n' +
      '### The Effect of Replay\n' +
      '\n' +
      'In this subsection, we explore the effect of compute-equivalent replay when continually pre-training models that re-warm and re-decay the learning rate.\n' +
      '\n' +
      'Given the need to mitigate forgetting when re-warming and re-decaying, we move on to investigate the effects of replay in our weak and strong-shift continued pre-training scenarios. Specifically, we use compete equivalent replay (see Sec. 4.2 for details) where replay tokens from \\(\\mathcal{D}_{0}\\) are added at the cost of removing the equivalent number of \\(\\mathcal{D}_{1}\\) tokens from the budget. Following the same two dataset settings, the model is pre-trained on \\(\\mathcal{D}_{0}\\) (Pile) for 300B tokens. This is followed by continual pre-training on a SlimPajama (weak shift) or German Common Crawl (strong shift). For more details regarding the setup, please see Section 5.2. Our continued pre-training is conducted for the full size of the respective datasets, which is 300B tokens for SlimPajama (weak shift) and 200B tokens for German Common Crawl (strong shift). We consider 1%, 5%, 10%, and 50% replay for both shifts and add 0.5% and 25% replay runs for the weak and strong distribution shifts respectively. We consider two baselines to put these results into a broader context. The first baseline is a model trained on \\(\\mathcal{D}_{1}\\) without replay. The second baseline model is trained from random initialization on a union of \\(\\mathcal{D}_{0}\\) and \\(\\mathcal{D}_{1}\\) for 600B tokens (SlimPajama) and 500B tokens (German Common Crawl). The latter baseline reflects the practice of fully re-training the model to update it instead of continually pre-training the existing model. All models re-warm and re-decay the learning rate using a cosine decay schedule fit to their token budget with the same \\(\\eta_{max}\\) (\\(3\\cdot 10^{-4}\\)) and \\(\\eta_{min}\\) (\\(3\\cdot 10^{-5}\\)) values as during pre-training on \\(\\mathcal{D}_{0}\\).\n' +
      '\n' +
      'Validation Loss ComparisonThe results in Fig. 4 (top and bottom) show the evolution of the validation loss during continual pre-training on the respective \\(\\mathcal{D}_{1}\\) datasets. Table 2 reports the average final validation loss for each of these models. The final loss is averaged over the last 100 iterations of training sampled at intervals of 10 iterations. We consistently observe across both distribution shifts that even the lowest tested replay of 1% significantly reduces forgetting on Pile compared to the no-replay baselines. This effect is more pronounced in the strong-shift scenario due to the larger amount of forgetting in this setting. We observe little impact on downstream performance for 1%,5%, and 10% replay when compared to the 0% baseline, showing that the forgetting benefits of replay come at little cost in our setting. However, when using an extreme amount of replay (50%), we observe that the model adapts less to \\(\\mathcal{D}_{0}\\). Interestingly, for both datasets, the 50% replay models attain or surpass the final average validation performance of the baseline training on \\(\\mathcal{D}_{1}\\cup\\mathcal{D}_{0}\\). This is curious as these model have seen 150B and 100B fewer tokens of \\(\\mathcal{D}_{1}\\) than their respective baselines.\n' +
      '\n' +
      'In summary, we find that, when re-warming and re-decaying the LR in a continual pre-training context, replay is a useful tool for reducing forgetting. For both distribution shifts, using an appropriate amount of replay yields similar final validation loss to the \\(\\mathcal{D}_{1}\\cup\\mathcal{D}_{0}\\) baseline. Moreover, for both shifts, the use of replay seems to negligibly affect adaptation to the downstream dataset, showing that reducing forgetting via replay comes at very little cost when continually pre-training LLMs.\n' +
      '\n' +
      'Figure 4: **The effect of replay at 405M scale for weak and strong distribution shifts.** We report Pile validation loss (left) and SlimPajama/German validation (right top/bottom) during training. Each model is trained from a checkpoint pre-trained on 300B tokens of Pile. The blue dotted line reports the final validation loss for models trained on Pile\\(\\cup\\)SlimPajama or Pile\\(\\cup\\)German data, totaling 600B and 500B tokens datasets respectively. We observe that replay significantly reduces forgetting across both shifts, however, the stronger shift requires more replay to mitigate forgetting to the same extent.\n' +
      '\n' +
      '### Continual Pre-training Final Performance for Weak and Strong Distribution Shifts.\n' +
      '\n' +
      'In this subsection, we compare two continually pre-trained 405M parameter models to several baselines in the _two dataset weak shift_ (Pile \\(\\rightarrow\\) SlimPajama) and _two dataset strong shift_ (Pile \\(\\rightarrow\\) German) settings. Our main goal is to determine how the differences in distribution shift affect final performance.\n' +
      '\n' +
      'Continually Pre-trained ModelsTo ablate the performance of combining LR re-warming and re-decaying with replay, we opt to train one model that exclusively re-warms and re-decays the learning rate and another that combines both techniques. Given results from the previous section for the weak distribution shift, we select 5% replay for the weak shift setting and 25% replay for the stronger shift setting. For both models, we re-warm to the \\(\\eta_{\\textit{max}}\\) of pre-training (\\(3\\cdot 10^{-4}\\)) and re-decay it using a cosine decay schedule set to reach \\(\\eta_{\\textit{min}}\\) by the end of continual pre-training. More hyperparameters are reported in Table 13 of the appendix.\n' +
      '\n' +
      'BaselinesWe also train several baselines. Two baselines are trained on \\(\\mathcal{D}_{0}\\) and \\(\\mathcal{D}_{1}\\) respectively while the third is trained on the union of each dataset \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\). We consider the model trained on \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\) to be an upper bound on performance as it represents an expensive full re-training. The baselines trained on individual datasets can be seen as compute-equivalent alternatives to continual pre-training (e.g., one could opt to train a model from random initialization on \\(\\mathcal{D}_{1}\\) instead of continually pre-training it).\n' +
      '\n' +
      '#### 6.3.1 Final Performance Evaluated by Loss\n' +
      '\n' +
      'Figure 5 reports the validation loss during continual pre-training of 405M parameter models for weak (top) and strong (bottom) shifts. Table 3 reports the average (over the last 100 iterations) final loss value for these models. Since the transition from English to German represents a starker distribution shift than Pile to SlimPajama, training on German leads to significantly more forgetting on Pile (\\(\\mathcal{D}_{0}\\)) for the continually pre-trained model without replay (0.27 vs 1.39 for weak and strong shifts respectively). However, choosing 25% replay to handle the starker shift significantly reduces the amount of forgetting on Pile, a reduction of 1.23 in terms of final loss. When comparing continually pre-trained models to baselines trained exclusively on \\(\\mathcal{D}_{1}\\), we observe that the continually pre-trained models always have lower validation loss across both distribution shifts. When comparing the continually pre-trained models with the \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\) baselines we find that both models achieve nearly identical (weak shift) or identical (strong shift) average final validation losses. This shows that for strong and weak distribution shifts, a simple and scalable combination of LR re-warming, LR re-decaying, and replay can achieve similar performance to the \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\) baseline.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline \\multicolumn{1}{c|}{**Training Tokens**} & \\multicolumn{3}{c}{**Validation Loss**} \\\\  & \\(\\mathcal{D}_{0}\\) Pile & \\(\\mathcal{D}_{1}\\) SlimPajama/German & AVG \\\\ \\hline\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP & 2.44 & 2.50 & 2.47 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP (0.5\\% Replay) & 2.27 & 2.50 & 2.39 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP (15 Replay) & 2.26 & 2.50 & 2.38 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP (55\\% Replay) & 2.23 & 2.51 & 2.37 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP (50\\% Replay) & 2.21 & 2.51 & 2.36 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP (50\\% Replay) & 2.16 & 2.54 & **2.35** \\\\\n' +
      '600B Pile \\(\\cup\\) SP & 2.17 & 2.53 & **2.35** \\\\ \\hline\n' +
      '300B Pile \\(\\rightarrow\\) 200B Ger. & 3.56 & 1.11 & 2.34 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 200B Ger. (1\\% Replay) & 2.83 & 1.12 & 1.97 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 200B Ger. (55 Replay) & 2.57 & 1.12 & 1.85 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 200B Ger. (10\\% Replay) & 2.46 & 1.13 & 1.80 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 200B Ger. (25\\% Replay) & 2.33 & 1.16 & 1.75 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 200B Ger. (50\\% Replay) & 2.24 & 1.22 & **1.73** \\\\\n' +
      '500B Pile \\(\\cup\\) Ger. & 2.26 & 1.25 & 1.75 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Final loss of English-only 405M parameter models trained with varying amounts of replay.** The loss is averaged over the last 100 iterations of training sampled at intervals of 10 iterations. The standard error for these measurements was computed but is not reported as it was \\(<0.001\\) for all models. We observe that models using more replay achieve a better adaptation-forgetting trade-off (AVG Loss). Interestingly, the model using 50% replay archives nearly identical loss values while seeing 150B fewer tokens on SlimPajama.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'Figure 5: **Final loss of 405M parameter models trained on two distribution shifts. Figures (a) and (b) are duplicated from Fig. 6 for convenient comparison. we provided three baselines and two continually pre-trained models. The baselines (light blue, dark blue, and maroon) are trained from random initialization on 300B tokens of SlimPajama, 300B tokens of Pile, and the union of both datasets (600B tokens). The continually pre-trained models (black and violet) start from a checkpoint pre-trained on 300B tokens of Pile (dark blue curve) and use 0% and 5% replay, respectively. We observe that for both distribution shifts, the combination of re-warming the learning rate and using a small percentage of replay helps to strike a balance between forgetting and adaptation. Importantly, we note that the use of replay minimally affects downstream performance compared to the models using 0% replay.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c} \\hline \\hline \\multicolumn{1}{c|}{**Training Tokens**} & \\multicolumn{3}{c|}{**Validation Loss**} & \\multicolumn{2}{c}{**LM Eval. Acc.**} \\\\  & \\(\\mathcal{D}_{\\text{b}}\\)**Pile** & \\(\\mathcal{D}_{\\text{l}}\\)**German/SP** & **AVG** & **English** & **HellaSwag-DE** \\\\ \\hline\n' +
      '300B Pile & 2.17 & 2.70 & 2.44 & 33.95 & 27.09 \\\\\n' +
      '300B SP & 2.51 & 2.53 & 2.52 & 34.11 & 27.03 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP & 2.44 & 2.50 & 2.47 & 34.93 & 27.43 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP (5\\% Replay) & 2.23 & 2.51 & **2.37** & 35.14 & 27.09 \\\\\n' +
      '600B Pile \\(\\cup\\) SP & 2.17 & 2.53 & **2.35** & 34.30 & 27.36 \\\\ \\hline\n' +
      '300B Pile & 2.17 & 2.70 & 2.44 & 33.95 & 27.09 \\\\\n' +
      '200B German & 3.97 & 1.17 & 2.57 & 27.74 & 29.53 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B German & 3.56 & 1.11 & 2.34 & 29.20 & 31.23 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 200B German (25\\% Replay) & 2.33 & 1.16 & **1.75** & 32.48 & 31.04 \\\\\n' +
      '500B Pile \\(\\cup\\) German & 2.26 & 1.25 & **1.75** & 32.43 & 30.45 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **Final loss of continually pre-trained English-only & English-German models. All models have 405M parameters. The loss is averaged over the last 100 iterations of training sampled at intervals of 10 iterations. The standard error for these measurements was computed but is not reported as it was \\(<0.001\\) for all models. We observe that even for starker distribution shifts, the combination of LR warmup and 5% replay helps to approach the average performance of the Pile \\(\\cup\\) German model.**\n' +
      '\n' +
      '#### 6.3.2 Final Performance Evaluated by Zero-shot and Few-shot Results on Popular LM Benchmarks\n' +
      '\n' +
      'While final accuracy provides a good measure of performance on the pre-training objective, LLMs\' abilities are typically judged by their performance on evaluation tasks. With the caveat that we use base models, that is our models have not been instruction-tuned, fine-tuned, or adapted to human preferences in any way, we present their evaluation on popular benchmarks in this section. Furthermore, we also provide a qualitative evaluation of German-trained models. We refer the reader to Sec. 5.4 of the main manuscript and Sec. A.5 of the appendix for a more detailed description of the chosen evaluation tasks.\n' +
      '\n' +
      'Table 3 reports the average accuracy of each model for our English evaluation tasks and the normalized accuracy for the German Hella Swag evaluation task. We do not report the average German evaluation score as it is not informative due to evaluations having near-random chance accuracy (see Table 11). We observe that English models consistently outperform German models on the English evaluations. However, the strong replay used with the 25% replay German model helps to reduce this gap. English models\' English evaluation performance is very similar with a range of 1.19 between the highest and lowest values. We suspect that there is significant noise in the evaluation process for base models of this size and believe that the differences are likely not significant. That being said, the continually pre-trained model with LR re-warming, LR re-decaying, and replay does improve on the \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\) model. When evaluating German-trained models on English evaluation tasks, we see consistent improvements for models using more replay. We note that once again the model trained with LR re-warming, LR re-decaying, and replay does improve on the \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\) model. Turning to the German Hella Swag results we observe that German models consistently outperform their English counterparts. Among German-trained models, the continually trained models outperform the union-trained model and the model trained exclusively on German.\n' +
      '\n' +
      'Given the poor performance of German models on all German evaluation tasks except HellaSwag (the same as English models on average), we further investigated their understanding of German by conducting a short qualitative study of model generations. In section A.4 of the appendix, we select five German prompts that contain various peculiarities of the German language (see Tab. 8 of the appendix). We then generate a fixed token-length response for each of the models trained German Common Crawl. As a baseline, we also evaluate the model trained only on the Pile. Despite the poor quality of generations at small model scale, we find that there is an observable improvement in the generative quality of German-language outputs from the models trained on German Common Crawl when compared to the Pile baseline, which tends to be systematically off-topic. This suggests that while our German-trained models have learned about the language, the evaluation tasks are too difficult to pick it up at the 405M parameter scale. Another reason is that the German dataset is smaller than the English datasets considered, and contains only web-scraped data, as opposed to the more sophisticated English datasets used in this work.\n' +
      '\n' +
      '_In summary, for weak and stronger distribution shifts alike, it is possible to achieve competitive performance to a model trained on \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\) by utilizing a simple and scalable combination of LR re-warming, LR re-decaying, and replay. This is true for final validation loss and language model evaluation scores, showing that this powerful combination of simple techniques can equip language models with new knowledge with little compromise to existing knowledge._\n' +
      '\n' +
      '### Continual Pre-training Final Performance at Different Model Scales\n' +
      '\n' +
      'In this subsection, we establish the effect of increasing parameter count by an order of magnitude on the final performance of continual pre-training. To accomplish this we compare two continually pre-trained models to several baselines at 405M and 10B parameter model sizes in the _two dataset weak shift_ (Pile \\(\\rightarrow\\) SlimPajama) and _two dataset strong shift_ (Pile \\(\\rightarrow\\) German) settings.\n' +
      '\n' +
      'Continuually Pre-trained ModelsTo ablate the performance of combining LR re-warming and re-decaying with replay, we opt to train one model that exclusively re-warms and re-decays the learning rate and another that combines both techniques. Given results from (Sec. 6.2) for the weak distribution shift, we select 5% replay for both model scales. For both models, we re-warm to the \\(\\eta_{max}\\) of pre-training (\\(3\\cdot 10^{-4}\\)) and re-decay using cosine annealing set to reach \\(\\eta_{min}\\) by the end of continual pre-training. More hyperparameters are reported in Table 13 of the appendix.\n' +
      '\n' +
      'BaselinesWe also train several baselines. Two baselines are trained on \\(\\mathcal{D}_{0}\\) and \\(\\mathcal{D}_{1}\\) respectively while the third is trained on \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\). We consider the model trained on \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\) to be an upper bound on performance as it represents an expensive full re-training. The baselines trained on individual datasets can be seen as compute-equivalent alternatives to continual pre-training (e.g., one could opt to train a model from random initialization on \\(\\mathcal{D}_{1}\\) instead of continually pre-training it).\n' +
      '\n' +
      '#### 6.4.1 Final Performance Evaluated by Loss\n' +
      '\n' +
      'Figure 6 reports the validation loss during continual pre-training for 405M and 10B models, while Table 4 reports the average (over the last 100 iterations) final loss value for each model. As expected, we observe that all baselines and continually pre-trained models consistently improve in perplexity on both datasets from increasing parameter count. For the 405M models, we observe that \\(\\text{Pile}\\cup\\text{SP}\\) achieves identical validation loss on each dataset to the baselines trained individually on them. In contrast, the 10B parameter model trained on \\(\\text{Pile}\\cup\\text{SP}\\) outperforms the models trained individually on each. We hypothesize that this happens due to larger models having more capacity, thus being capable of learning at a higher rate for longer. We observe that replaying 5% pile data when continuing to pre-train on SlimPajama reduces forgetting on Pile validation by 0.19 and 0.21 for 10B and 405M parameter models respectively. The negligible difference in forgetting-reduction from replay despite the order of magnitude difference in parameters between both models suggests that model scale has a limited negative influence on forgetting-re\n' +
      '\n' +
      'Figure 6: **Validation loss during continual pre-training of 10B (top) and 405M (bottom) parameter models.** At each model scale we provided three baselines and two continually pre-trained models. The baselines (light blue, dark blue, and maroon) are trained from random initialization on 300B tokens of SlimPajama, 300B tokens of Pile, and the union of both datasets (600B tokens). The continually pre-trained models (black and violet) start from a checkpoint pre-trained on 300B tokens of Pile (dark blue curve) and use 0% and 5% replay, respectively. We observe that for both model sizes, the combination of LR re-warming, LR re-decaying, and using a small percentage of replay helps to strike a balance between forgetting and adaptation. Importantly, we note that the use of replay minimally affects downstream performance compared to the models using 0% replay (black and violet curves overlap in figures (b) and (d)).\n' +
      '\n' +
      'believe this is because larger models forget less by default. Indeed, the models trained without replay from a pre-trained Pile checkpoint forget 0.23 and 0.27 nats of Pile perplexity for 10B and 405M respectively. While the difference is small, this suggests that larger models forget less, confirming our hypothesis. When comparing the average final validation loss of the models with 5% replay and baselines trained on the union of both datasets, we notice that there is only a difference of 0.02 for both model sizes. This shows that for weak but realistic distribution shifts at two model scales, continual pre-training can achieve similar performance to the expensive re-training baseline.\n' +
      '\n' +
      '#### 6.4.2 Final Performance Evaluated by Zero-shot and Few-shot Results on Popular LM Benchmarks\n' +
      '\n' +
      'While final accuracy provides a good measure of performance on the pre-training objective, LLMs abilities are typically judged by their performance on evaluation tasks. With the caveat that we use base models, that is our models have not been instruction-tuned, fine-tuned, or adapted to human preferences in any way, we present their evaluation on popular benchmarks in this section. We refer the reader to Sec. 5.4 of the main manuscript and Sec. A.5 of the appendix for a more detailed description of the chosen evaluation tasks.\n' +
      '\n' +
      'Table. 5 reports English-language LM evaluation results for our english-only continually pre-trained LLMs. Normalized accuracy is reported for HellaSwag and exact match (EM) is reported for NaturalQuestions and TriviaQA. All other tasks report unnormalized accuracy. As expected, we observe that the larger (10B) models achieve stronger performance than their smaller counterparts and that models trained on more tokens always achieve better performance than models trained on fewer tokens. For both model scales, we observe that the models pre-trained continually using a combination of learning rate re-warming and 5% replay approach (10B) or surpass (405M) the performance of the models trained on the union of both datasets in terms of average accuracy. When comparing union-trained models to continually pre-trained models for\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|c c c} \\hline \\hline \\multirow{2}{*}{**Model Size**} & \\multicolumn{3}{c|}{**Validation Loss**} \\\\  & \\multicolumn{1}{c}{**Training Tokens**} & \\(\\mathcal{D}_{\\text{t}}\\)**Pile** & \\(\\mathcal{D}_{\\text{t}}\\)**SimPajama** & **AVG** \\\\ \\hline \\multirow{6}{*}{10B} & 300B Pile & 1.75 & 2.24 & 1.99 \\\\  & 300B SP & 2.08 & 2.05 & 2.07 \\\\  & 300B Pile + 300B SP & 1.98 & 2.00 & 1.99 \\\\  & 300B Pile + 300B SP (5\\% Replay) & 1.79 & 2.00 & **1.89** \\\\  & 600B Pile \\(\\cup\\) SP & 1.72 & 2.02 & **1.87** \\\\ \\hline \\multirow{6}{*}{405M} & 300B Pile & 2.17 & 2.70 & 2.44 \\\\  & 300B SP & 2.51 & 2.53 & 2.52 \\\\ \\cline{1-1}  & 300B Pile + 300B SP & 2.44 & 2.50 & 2.47 \\\\ \\cline{1-1}  & 300B Pile + 300B SP (5\\% Replay) & 2.23 & 2.51 & **2.37** \\\\ \\cline{1-1}  & 600B Pile \\(\\cup\\) SP & 2.17 & 2.53 & **2.35** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: **Final loss of 10B and 405M parameter models.** The loss is averaged over the last 100 iterations of training sampled at intervals of 10 iterations. The standard error for these measurements was computed but is not reported as it was \\(<0.001\\) for all models. We observe that at both model scales, learning rate re-warming combined with 5% replay approaches the average loss value of joint training.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|c c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Model Size} & Training Tokens & HellaSwag & ARC-C & ARC-C & BookQ & MolQA & MMLU & O0B4 & PIGA & WC & TQA1 & TQA2 & NO & TQA & AVG \\\\ \\hline \\multirow{6}{*}{10B} & 300B Pile & 68.46 & 38.81 & 02.09 & 68.83 & 27.34 & 27.38 & 27.20 & 78.62 & 62.51 & 20.44 & 33.86 & 66.53 & 49.92 & 43.45 \\\\  & 300B SP & 70.38 & 36.77 & 71.93 & 68.01 & 24.76 & 27.42 & 28.20 & 79.69 & 60.04different tasks, we observe for the 10B parameter models that the 5% replay model and union-trained model exchange best performance on different tasks with notable differences being OpenBookQA in favor of the replay model and MMLU in favor of the union model. For the 405M parameter models, the 5% replay model and union-trained model exchange best performance on different tasks with no notable differences. At both model scales, the replay model improves over the model only using re-warming though differences are small and may be attributable to noise.\n' +
      '\n' +
      '_In summary, we find that models continually pre-trained with a combination of LR re-warming, LR re-decaying, and replay exceed the performance (e.g., w.r.t. final validation loss and evaluation accuracy) of baselines trained from random initialization on individual datasets and achieve comparable evaluation performance to the expensive re-training baseline (trained on the union of both datasets). These results show that the benefits of continual pre-training hold at the \\(10\\)B parameter scale, suggesting that this may also be the case for models with an order of magnitude more parameters (e.g. for \\(100\\)B+ parameters)._\n' +
      '\n' +
      '## 7 Understanding and Circumventing the Pathologies of Re-warming\n' +
      '\n' +
      'In this section, find that LR re-warming causes unwanted forgetting, introduce infinite learning rate schedules as a promising way to circumvent it, and compare these schedules to baselines from the literature.\n' +
      '\n' +
      '### Re-warming on the Same Data\n' +
      '\n' +
      'In section 6.1, we have seen that continuing to pre-train on new data initially leads to a quick increase of the loss on past data, which motivated the use of replay. The increase of the loss was, in particular, more pronounced for greater \\(\\eta_{max}\\) values. One hypothesis for the increase in loss is that it is mostly due to a distribution shift between the pre-training datasets and associated negative transfer. To assess this hypothesis, we re-warm and re-decay over 300B tokens in a setting with no distribution shift. That is, we follow a similar methodology as in our experiments from Fig. 3 but continue to pre-train on Pile as \\(\\mathcal{D}_{1}\\).\n' +
      '\n' +
      'As seen in Fig. 7, independently of the distribution shift, rewarming the learning rate appears to be a significant cause of the increase in loss seen previously in Fig. 3 when starting to continue to pre-train, as evidenced by the increase in perplexity when re-warming the learning rate while training on the same distribution. For example, the re-warming leads to a peak increase of the Pile validation loss of 0.1 relative to its initial value with a \\(\\eta_{max}=3\\cdot 10^{-4}\\) as we continue pre-training on Pile, which might be contrasted with the Pile validation loss increase of 0.35 with the same learning rate schedule when continuing to pre-train on SlimPajama as in Fig. 3. It is noteworthy that the higher the re-warming, the more pronounced this effect is,\n' +
      '\n' +
      'Figure 7: **Pile validation loss when continuing to pre-train on Pile (a) and SlimPajama (b). Each curve starts from the same checkpoint pre-trained on 300B tokens of Pile but is trained with a different maximum learning rate. We observe that every model that re-increases its learning rate from the minimum learning rate of pre-training (e.g., all models except constant) sees an increase in loss.**\n' +
      '\n' +
      'as seen with the \\(\\eta_{max}=6\\cdot 10^{-4}\\) curve when continuing to pre-train on Pile (with a peak loss increase of 0.2) vs continuing to pre-train on SlimPajama (peak loss increase of 0.45).\n' +
      '\n' +
      'In particular, after re-warming, models fail to recover quickly from the performance hit due to rewarming the learning rate even when training on the same dataset. This motivates finding alternatives to learning rate schedules requiring re-warming in order to improve the efficiency of continual pre-training.\n' +
      '\n' +
      '### Infinite Learning Rate Schededues\n' +
      '\n' +
      'In this subsection, we investigate the use of learning rate schedules that intrinsically may not require re-warming. The motivations are twofold. On the one hand, a cosine decay schedule requires us to know the total number of tokens we want to pre-train on in advance. This limits the ability to continue to pre-train a converged checkpoint. On the other hand, we saw in the previous section that when continuing to pre-train a model that was initially pre-trained with a cosine decay schedule ending with a small learning rate, re-warming the learning rate from its minimum value is needed to best adapt to the new dataset. However, as seen in the previous subsection, we observe that re-warming the learning rate can exacerbate forgetting.\n' +
      '\n' +
      'Thus, we explore "Infinite Learning rate schedules" (Zhai et al., 2022) which keep the learning rate at a constant value across all new tasks. This can help prevent forgetting by avoiding re-warming the learning on new tasks. Additionally, this schedule is independent of the total number of tokens making it more suitable for continual learning setups compared to repeating the cosine decay schedule cyclically for each new dataset. As we saw, since a high constant learning rate is also suboptimal, we opt to perform a fast annealing of the learning rate at the end of pre-training, over a limited amount of tokens. We hope that this will recover the performance advantage of re-decaying the learning rate, while allowing the use of a pre-annealing checkpoint when continuing to pre-train.\n' +
      '\n' +
      'The infinite learning rate schedules considered have 4 phases:\n' +
      '\n' +
      '1. **Linear warm-up phase** - As before, the learning rate is initially increased to some maximum value \\(\\eta_{max}\\) over \\(T_{warmup}\\) timesteps, or equivalently until timestep \\(t_{cd}=T_{warmup}\\). The learning rate undergoes a warm-up only once (during the first task) and does not require re-warming for future tasks.\n' +
      '2. **Cooldown phase** - During this stage the learning rate undergoes a cooldown phase where the learning rate is gradually decayed to constant value \\(\\eta_{const}\\) according to some decay function \\(f_{cd}\\) over \\(T_{cd}\\) timesteps from timestep \\(t_{cd}\\) to \\(t_{const}=t_{cd}+T_{cd}\\). This stage also occurs only once during the first task.\n' +
      '3. **Constant phase** - The learning rate then remains constant for all future tasks over \\(T_{const}\\) timesteps from timestep \\(t_{const}\\) to \\(t_{ann}=t_{const}+T_{const}\\). The checkpoint obtained at the end of this phase is the one one should resume from when continuing to pretrain on a new dataset.\n' +
      '4. **Annealing phase** - The learning rate is annealed to a small value \\(\\eta_{min}\\) over \\(T_{ann}\\) timesteps from timestep \\(t_{ann}\\) to \\(t_{end}=t_{ann}+T_{ann}\\), helping train the model to convergence before being deployed.\n' +
      '\n' +
      'Thus, the infinite learning rate schedules considered here can be written as:\n' +
      '\n' +
      '\\[\\eta_{t}=\\left\\{\\begin{array}{llll}&\\eta_{max}\\cdot\\frac{t}{T_{warmup}}&t \\in[0,t_{cd}]&(\\textit{warm-up})\\\\ &f_{cd}(t)&t\\in(t_{cd},t_{const}]&(\\textit{cooldown})\\\\ &\\eta_{const}&t\\in(t_{const},t_{ann}]&(\\textit{constant})\\\\ &\\eta_{const}\\cdot\\left(\\frac{\\eta_{min}}{\\eta_{const}}\\right)^{\\frac{t-t_{ conn}}{t_{end-t_{ann}}}}&t\\in(t_{ann},t_{end}]&(\\textit{annealing})\\end{array}\\right.\\]\n' +
      '\n' +
      'In this work, we consider the two following functions for the cooldown phase\'s decay \\(f_{cd}\\):1. Cosine decay \\[f_{cd}(t)=\\eta_{const}+\\frac{\\eta_{max}-\\eta_{const}}{2}\\cdot\\left(1+\\cos\\left(\\pi \\left(\\frac{t-t_{cd}}{t_{const}-t_{cd}}\\right)\\right)\\right)\\] (3)\n' +
      '2. Inverse Square Root decay \\[f_{cd}(t)=\\eta_{max}+\\frac{\\eta_{const}-\\eta_{max}}{h(1)}\\cdot h\\left(\\frac{t- t_{cd}}{t_{const}-t_{cd}}\\right)\\] (4) where \\[h(x)=\\frac{1}{\\sqrt{1+\\alpha x}}-1\\]\n' +
      '\n' +
      'with \\(\\alpha\\) controlling the steepness of the inverse square root decay. We shift and stretch the Inverse Square root decay to adapt to the interval \\((t_{cd},t_{const}]\\).\n' +
      '\n' +
      'The three different schedules are seen in Fig. 8 (b).\n' +
      '\n' +
      'We now compare infinite learning rate schedules to a cosine decay schedule. We first explore a simple single-dataset pre-training setup to evaluate the feasibility of the schedule for LLM pre-training. Subsequently, we explore its benefits in our _three datasets, no shift_ setting.\n' +
      '\n' +
      '### Comparing Cosine Decay to Variants of our Infinite Schedules\n' +
      '\n' +
      'Here we compare a cosine decay schedule with infinite learning rate schedules in the common single-dataset pre-training setting. The aim of these experiments is to test if the infinite learning rate schedules can result in models that perform as well as models trained with a conventional cosine decay schedule.\n' +
      '\n' +
      'The models are pre-trained on 300B tokens of SlimPajama from random initialization. Figure 8 shows the training curves of 3 405M parameter models trained on SlimPajama with different learning rate schedules. We observe that all methods reach similar final validation loss showing that infinite learning rate schedules can be used for the common case of pre-training as well. These schedules additionally have the advantage that one can start annealing at any time in the constant phase to efficiently improves the loss when deciding to finalize pre-training, and a pre-annealing checkpoint can be loaded to continue pre-training.\n' +
      '\n' +
      'Figure 8: **Infinite learning rate schedules v.s. Cosine decay. We train a 405M parameter model on 300B tokens of SlimPajama from random initialization with two new schedules, _Cosine Inf_ and _InvSgrt Inf_, and compare them to the cosine decay baseline. _Cosine Inf_ and _InvSgrt Inf_ first decay to a fixed constant LR value and stay constant thereafter until an abrupt final decay. These schedules, therefore, have the advantage that they can smoothly transition between one pre-training phase and the next without re-warming. We find that all methods reach similar final validation loss showing that Cosine decay is not a prerequisite for strong performance.**\n' +
      '\n' +
      '### Infinite Learning Rate Schedules: Scaling to Infinite Future Updates\n' +
      '\n' +
      'We now explore the role of the infinite learning rate schedules when multiple new datasets are seen in a continual learning setup. The models are trained from random initialization with different learning rate schedules on 3 IID 100B subsets of SlimPajama (e.g., our _three datasets no shift_ setting; see Sec 5.2). We focus on the no shift setting in these preliminary experiments and leave the weak and strong shift cases to future work. This task simulates a setting where large amounts of data from the same distribution are received at time increments and we wish to continue pre-training our models on them (e.g., continuing to pre-train the model on the latest web-scrape). To make our results applicable to situations where previous optimizer states are not available, we do not keep optimizer states across dataset boundaries. Fig. 9 reports training curves for 405M parameter models.\n' +
      '\n' +
      'We observe that all schedules perform relatively similarly, however, the two infinite schedules have the advantage that we can start annealing at any time during the constant learning rate phase on each split, while the repeated cosine decays require knowing the number of tokens in advance. Additionally, we see negligible forgetting across dataset boundaries for the infinite LR schedules. While the losses initially increase sharply due to re-initializing the optimizer states, the infinite schedules models immediately recover from this.\n' +
      '\n' +
      'In future works, it would be interesting to study the impact of infinite learning rate schedules in continual learning setups with distribution shifts, and investigate the stability of training over large amounts of tokens with a long constant phase of the learning rate.\n' +
      '\n' +
      '_In summary, we saw that re-warming impacts performance, but that alternatives to cosine decay schedules might circumvent these issues. The alternatives explored in this work are promising in that they provide an intuitive way to control ending or continuing to pre-train._\n' +
      '\n' +
      '## 8 Limitations\n' +
      '\n' +
      'While we have conducted a thorough empirical evaluation of continual pre-training for LLMs, there are some limitations to our work. In no particular order: 1) we only studied two model sizes (405M and 10B); 2) we did not run deduplication between the German training and validation datasets created from the German Common Crawl scrape (Laippala et al., 2022); 3) we primarily study the transition between two subsequent tasks; 4) we did not run our experiments over multiple seeds; and 5) our experiments on infinite learning rate schedules are limited to 405M scale with no distribution shift. More explicitly, the first limitation is the number of model scales we consider. While we do consider a 405M and a 10B parameter model (much larger than most works), we could not extend the study to another order of magnitude due to computational\n' +
      '\n' +
      'Figure 9: **Infinite learning rate schedules evaluated on 3 IID 100B token subsets of SP. The experiment simulates a setting where new data from the same distribution arrives over time and the practitioner wishes to update their model on the new data. The models are trained from random initialization. We note that in figure (b) the black and violet schedules overlap after \\(\\sim 80\\)B tokens.**limitations (e.g., 100B parameter scale). The second limitation of our work is that the German validation set was not deduplicated from the German training data. While we were careful to take distinct shards for training and validation, there may be some contamination between the two. Given that all baselines have access to the same dataset, however, we believe our results are still valid. The third limitation is that we did not run experiments updating models on more than two subsequent tasks. While we believe that studying this is important, our goal was to focus our compute on different distribution shifts and studying transitions between large datasets, rather than using a large number of datasets. The fourth limitation is that we did not run experiments over multiple seeds due to high computational cost, meaning that there is likely a stochastic element to some results. That being said, our LLMs are trained with a large batch size (2M+ tokens) and, thus, there is little variance in the gradient estimates. Coupled with the fact that the samples from each dataset are processed in the same order in all cases, we believe that our results should be relatively stable to changes in random initialization dictated by the seed. The fifth limitation is that it is very possible that over enough tokens, the infinite schedules may end up being suboptimal due to only having a single phase of warmup and cooldown, as the learning on all subsequent datasets may just be equivalent to using a constant learning rate, which proved to be suboptimal (see Fig. 3). While Fig. 9 showed that the annealing phase helps recover from this suboptimality in the case of IID splits of the same dataset, it is unclear if this would hold over more tokens, or in the case where the different datasets have distribution shifts. Hence, experiments involving distribution shifts, and a larger scale of models and datasets would be important to further test these infinite schedules. Finally, another important consideration to explore at a larger scale is the stability of pre-training with such schedules (in particular, during the constant learning rate phase without \\(\\mu P\\)(Yang et al., 2022)).\n' +
      '\n' +
      '## 9 Conclusion\n' +
      '\n' +
      'In the context of continual pre-training of LLMs, we have seen that learning rate re-warming and re-decaying is important for adaptation and found that the forgetting is easily mitigated with replay in this setting--at seemingly little cost to adaptation. Given their powerful ability to enhance adaptation and mitigate forgetting simultaneously, we proposed the simple and scalable combination of LR re-warming, LR re-decaying, and replay for continually pre-training LLMs at scale. We showed that these strategies enable continual pre-training to achieve performance on par with expensively re-training from scratch on all data, across two distribution shifts (weak & strong) and two model scales (405M & 10B). Upon further analysis, we identified a pathology of LR re-warming and, inspired by previous work, proposed infinite learning rate schedules for continually pre-training LLMs. In initial experiments, our schedules achieve performance on par with cosine decay while circumventing the need for LR re-warming.\n' +
      '\n' +
      'Our findings show that continual pre-training is an efficient alternative to re-training when updating LLMs on new data. Equipped with our strategies, practitioners can efficiently update their existing models (Rae et al., 2021; Hoffmann et al., 2022; Touvron et al., 2023; Jiang et al., 2023; Gemma Team et al., 2024) on newly created higher-quality datasets. These strategies might also be relevant for pre-training curricula such as the ones used by Gemma Team et al. (2024). With the strong incentive for our community to continue creating datasets of increasing quality, we only expect the need for continual pre-training to increase.\n' +
      '\n' +
      'In follow-up work, it will be important to further investigate infinite learning rate schedules, continual pre-training in the context of vision language models and other text-based generative models, growing models during continual pre-training (e.g., mixture-of-experts or block expansion), and adapting the tokenizer to handle drastic changes to the data distribution.\n' +
      '\n' +
      '#### Broader Impact Statement\n' +
      '\n' +
      'Large language models have seen widespread adoption across a wide range of industry sectors due to their ability to perform very well after being trained on relevant datasets. Moreover, improvements in datasets (better filtering, updating knowledge, etc.) have been crucial to increasing the quality of the output of LLMs. As such, it is reasonable to expect that organizations will spend a significant amount of computing power and, thus, energy to create more powerful models. It is likely that some of this energy will come from non-renewable sources. While the experiments presented in our paper are environmentally costly, as arguedin the paper, continuing to pre-train is a promising method to significantly reduce the compute associated with updating a model and, hence, the energy required to maintain foundation models.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'We acknowledge support from NSERC Discovery Grant RGPIN- 2021-04104 [E.B.], the Canada CIFAR AI Chair Program [I.R.], and the Canada Excellence Research Chairs Program [I.R.]. We would also like to acknowledge funding from the FRQNT Doctoral (B2X) scholarship [B.T.], the scholarship for Artificial Intelligence of Universite de Montreal\'s Etudes Superieures et Postdoctables[A.I.], and a fellowship of the IFI program of the German Academic Exchange Service (DAAD)[M.R.]. This research was made possible thanks to the computing resources on the Summit supercomputer, provided as a part of the INCITE 2023 program award "Scalable Foundation Models for Transferable Generalist AI". These resources were provided by the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725. In particular, we thank Jens Glaser for his help with the Summit supercomputer.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Aljundi et al. (2019) LuCA Caccia, Eugene Bellovsky, Massimo Caccia, Min Lin, Laurent Charlin, and Tinne Tuytelaars. Online continual learning with maximal interfered retrieval. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\'Alche-Buc, E. Fox, and R. Garnett (Eds.), _Advances in Neural Information Processing Systems 32_, pp. 11849-11860. Curran Associates, Inc., 2019. URL [http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-](http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-) interfered-retrieval.pdf.\n' +
      '* Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alboebidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The Falcon series of open language models. _CoRR_, abs/2311.16867, 2023. URL [https://doi.org/10.48550/arXiv.2311.16867](https://doi.org/10.48550/arXiv.2311.16867).\n' +
      '* Amini et al. (2019) Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms, 2019.\n' +
      '* Andonian et al. (2021) Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Wang Phil, and Samuel Weinbach. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch, 8 2021. URL [https://www.github.com/eleutherai/gpt-neoX](https://www.github.com/eleutherai/gpt-neoX).\n' +
      '* Bisk et al. (2019) Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019.\n' +
      '* Black et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b: An open-source autoregressive language model, 2022.\n' +
      '* Black et al. (2021)Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Wing Yin Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators).\n' +
      '* Brown et al. (2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, pp. 1877-1901, 2020. URL [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165).\n' +
      '* Buzzega et al. (2020) Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. In Hugo Larochelle, Marc\'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/b704ea2c39778707c617f6b7ce480e9e-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/b704ea2c39778707c617f6b7ce480e9e-Abstract.html).\n' +
      '* Caccia et al. (2020) Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Caccia, Issam Laradji, Irina Rish, Alexandre Lacoste, David Vazquez, and Laurent Charlin. Online fast adaptation and knowledge accumulation: a new approach to continual learning. _NeurIPS_, 2020. URL [https://arxiv.org/abs/2003.05856](https://arxiv.org/abs/2003.05856).\n' +
      '* Chen et al. (2016) Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. _CoRR_, abs/1604.06174, 2016. URL [http://arxiv.org/abs/1604.06174](http://arxiv.org/abs/1604.06174).\n' +
      '* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019.\n' +
      '* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.\n' +
      '* Computer (2023) Together Computer. Redpajama: an open dataset for training large language models, 2023. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).\n' +
      '* Cossu et al. (2022) Andrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia Passaro, Vincenzo Lomonaco, and Davide Bacciu. Continual pre-training mitigates forgetting in language and vision, 2022. URL [https://arxiv.org/abs/2205.09357](https://arxiv.org/abs/2205.09357).\n' +
      '* Douillard et al. (2023) Arthur Douillard, Qixuan Feng, Andrei A Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc\'Aurelio Ranzato, Arthur Szlam, and Jiajun Shen. Diloco: Distributed low-communication training of language models. _arXiv preprint arXiv:2311.08105_, 2023.\n' +
      '* French (1999) Robert M. French. Catastrophic forgetting in connectionist networks. _Trends in Cognitive Sciences_, 3(4):128-135, 1999. ISSN 13646613. doi: 10.1016/S1364-6613(99)01294-2. URL [https://www.sciencedirect.com/science/article/abs/pii/S1364661399012942](https://www.sciencedirect.com/science/article/abs/pii/S1364661399012942).\n' +
      '* Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. _arXiv preprint arXiv:2101.00027_, 2020.\n' +
      '* Team et al. (2024) Thomas Mesnard Gemma Team, Cassidy Hardin, Robert Dadashi, Surya Bhuptiraju, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Leonard Hussenot, and et al. Gemma: Open models based on gemini research and technology. 2024. URL [https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf).\n' +
      '* Gogoulou et al. (2023) Evangelia Gogoulou, Timothee Lesort, Magnus Boman, and Joakim Nivre. A study of continual learning under language shift. _CoRR_, abs/2311.01200, 2023. URL [https://doi.org/10.48550/arXiv.2311.01200](https://doi.org/10.48550/arXiv.2311.01200).\n' +
      '\n' +
      'Zheng Gong, Kun Zhou, Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. Continual pre-training of language models for math problem understanding with syntax-aware memory network, 2022. URL [https://aclanthology.org/2022.acl-long.408/](https://aclanthology.org/2022.acl-long.408/).\n' +
      '* Goyal et al. (2018) Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour, 2018.\n' +
      '* Gururangan et al. (2020) Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don\'t stop pretraining: Adapt language models to domains and tasks. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, pp. 8342-8360. Association for Computational Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.acl-main.740](https://doi.org/10.18653/v1/2020.acl-main.740).\n' +
      '* Harun et al. (2023a) Md Yousuf Harun, Jhair Gallardo, Tyler L Hayes, and Christopher Kanan. How efficient are today\'s continual learning algorithms? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 2430-2435, 2023a.\n' +
      '* Harun et al. (2023b) Md Yousuf Harun, Jhair Gallardo, Tyler L. Hayes, Ronald Kemker, and Christopher Kanan. Siesta: Efficient online continual learning with sleep, 2023b.\n' +
      '* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021.\n' +
      '* Hernandez et al. (2021) Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. _CoRR_, abs/2102.01293, 2021. URL [https://arxiv.org/abs/2102.01293](https://arxiv.org/abs/2102.01293).\n' +
      '* Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022. URL [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556).\n' +
      '* Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism, 2019.\n' +
      '* Jang et al. (2022) Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, and Minjoon Seo. Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pp. 6237-6250. Association for Computational Linguistics, 2022a. URL [https://doi.org/10.18653/v1/2022.emnlp-main.418](https://doi.org/10.18653/v1/2022.emnlp-main.418).\n' +
      '* Jang et al. (2022) Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, and Minjoon Seo. Towards continual knowledge learning of language models. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022b. URL [https://openreview.net/forum?id=vfsRB5Mimo9](https://openreview.net/forum?id=vfsRB5Mimo9).\n' +
      '* Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b. _CoRR_, abs/2310.06825, 2023. URL [https://doi.org/10.48550/arXiv.2310.06825](https://doi.org/10.48550/arXiv.2310.06825).\n' +
      '* Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan (eds.), _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147).\n' +
      '* Liu et al. (2018)Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _CoRR_, abs/2001.08361, 2020. URL [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361).\n' +
      '* Ke et al. (2022) Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. Continual pre-training of language models, 2022. URL [https://openreview.net/forum?id=m_GDIItaI3o](https://openreview.net/forum?id=m_GDIItaI3o).\n' +
      '* Kirillov et al. (2023) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. _arXiv:2304.02643_, 2023.\n' +
      '* Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 7:452-466, 2019. doi: 10.1162/tacl_a_00276. URL [https://aclanthology.org/Q19-1026](https://aclanthology.org/Q19-1026).\n' +
      '* 17, 2022_, pp. 215-221. Association for Computational Linguistics, 2022. URL [https://aclanthology.org/2022.wnut-1.23](https://aclanthology.org/2022.wnut-1.23).\n' +
      '* Lesort et al. (2023) Timothee Lesort, Oleksiy Ostapenko, Pau Rodriguez, Diganta Misra, Md Rifat Arefin, Laurent Charlin, and Irina Rish. Challenging common assumptions about catastrophic forgetting and knowledge accumulation. In Sarath Chandar, Razvan Pascanu, Hanie Sedghi, and Doina Precup (eds.), _Conference on Lifelong Learning Agents, 22-25 August 2023, McGill University, Montreal, Quebec, Canada_, volume 232 of _Proceedings of Machine Learning Research_, pp. 43-65. PMLR, 2023. URL [https://proceedings.mlr.press/v232/lesort23a.html](https://proceedings.mlr.press/v232/lesort23a.html).\n' +
      '* Lesort et al. (2021) Timothee Lesort, Massimo Caccia, and Irina Rish. Understanding continual learning settings with data distribution drift analysis. _arXiv preprint arXiv:2104.01678_, 2021.\n' +
      '* Lin et al. (2022) Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2022.\n' +
      '* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7).\n' +
      '* Ma et al. (2023) Shirong Ma, Shen Huang, Shulin Huang, Xiaobin Wang, Yangning Li, Hai-Tao Zheng, Pengjun Xie, Fei Huang, and Yong Jiang. Ecomppt-ct: Continual pre-training of e-commerce large language models with semi-structured data. _CoRR_, abs/2312.15696, 2023. URL [https://doi.org/10.48550/arXiv.2312.15696](https://doi.org/10.48550/arXiv.2312.15696).\n' +
      '* McMahan et al. (2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pp. 1273-1282. PMLR, 2017.\n' +
      '* Mehta et al. (2023) Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. An empirical investigation of the role of pre-training in lifelong learning. _J. Mach. Learn. Res._, 24:214:1-214:50, 2023. URL [http://jmlr.org/papers/v24/22-0496.html](http://jmlr.org/papers/v24/22-0496.html).\n' +
      '* Mermillod et al. (2013) Martial Mermillod, Aurelia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: investigating the continuum from catastrophic forgetting to age-limited learning effects. _Frontiers in psychology_, 4(August):504, 2013. ISSN 1664-1078. doi: 10.3389/fpsyg.2013.00504. URL [http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3732997](http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3732997){&}tool=pmcentrez{&}rendertype=abstract.\n' +
      '* Mermillod et al. (2017)Microsoft. Megatron-DeepSpeed. [https://github.com/microsoft/Megatron-DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed), 2020. Accessed: February 28, 2024.\n' +
      '* Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018.\n' +
      '* Mirzadeh et al. (2022) Seyed-Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Huiyi Hu, Razvan Pascanu, Dilan Gorir, and Mehrdad Farajtabar. Wide neural networks forget less catastrophically. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pp. 15699-15717. PMLR, 2022. URL [https://proceedings.mlr.press/v162/mirzadeh22a.html](https://proceedings.mlr.press/v162/mirzadeh22a.html).\n' +
      '* Ostapenko et al. (2019) Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin Nabi. Learning to remember: A synaptic plasticity driven framework for continual learning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 11321-11329, 2019. URL [https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html](https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html)\n' +
      '* Pernias et al. (2024) Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville. Wurstchen: An efficient architecture for large-scale text-to-image diffusion models. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=gU58d5QeGv](https://openreview.net/forum?id=gU58d5QeGv).\n' +
      '* Pluster (2023) Bjorn Pluster. German Benchmark Datasets, 8 2023. URL [https://github.com/bjoernpl/GermanBenchmark](https://github.com/bjoernpl/GermanBenchmark).\n' +
      '* Popel and Bojar (2018) Martin Popel and Ondrej Bojar. Training tips for the transformer model. _The Prague Bulletin of Mathematical Linguistics_, 110(1):43-70, April 2018. ISSN 1804-0462. doi: 10.2478/pralin-2018-0002. URL [http://dx.doi.org/10.2478/pralin-2018-0002](http://dx.doi.org/10.2478/pralin-2018-0002).\n' +
      '* Prabhu et al. (2023) Ameya Prabhu, Zhipeng Cai, Puneet Dokania, Philip Torr, Vladlen Koltun, and Ozan Sener. Online continual learning without the storage constraint. _arXiv preprint arXiv:2305.09253_, 2023.\n' +
      '* Qin et al. (2023) Yujia Qin, Cheng Qian, Xu Han, Yankai Lin, Huadong Wang, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Recyclable tuning for continual pre-training, 2023. URL [https://arxiv.org/abs/2305.08702](https://arxiv.org/abs/2305.08702).\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pp. 8748-8763. PMLR, 2021. URL [http://proceedings.mlr.press/v139/radford21a.html](http://proceedings.mlr.press/v139/radford21a.html).\n' +
      '* Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. _arXiv preprint arXiv:2112.11446_, 2021. URL [https://arxiv.org/abs/2112.11446](https://arxiv.org/abs/2112.11446).\n' +
      '* Raffel et al. (2023) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023.\n' +
      '* Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Christine Cuicchi, Irene Qualters, and William T. Kramer (eds.), _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020_, pp. 20. IEEE/ACM, 2020.\n' +
      '* Raffel et al. (2020)Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. Effect of scale on catastrophic forgetting in neural networks. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. URL [https://openreview.net/forum?id=GHvSS_yPeEa](https://openreview.net/forum?id=GHvSS_yPeEa).\n' +
      '* Reddi et al. (2021) Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=LkFG31B13U5](https://openreview.net/forum?id=LkFG31B13U5).\n' +
      '* Riemer et al. (2019) Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu,, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In _International Conference on Learning Representations_, 2019. URL [https://openreview.net/forum?id=B1gTShAct7](https://openreview.net/forum?id=B1gTShAct7).\n' +
      '* Rolnick et al. (2019) David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. In _Advances in Neural Information Processing Systems_, pp. 348-358, 2019. URL [https://arxiv.org/abs/1811.11682](https://arxiv.org/abs/1811.11682).\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pp. 10674-10685. IEEE, 2022. URL [https://doi.org/10.1109/CVPR52688.2022.01042](https://doi.org/10.1109/CVPR52688.2022.01042).\n' +
      '* Ryabinin et al. (2021) Max Ryabinin, Eduard Gorbunov, Vesvold Polkhotnyuk, and Gennady Pekhimenko. Moshpit sgd: Communication-efficient decentralized training on heterogeneous unreliable devices. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems_, volume 34, pp. 18195-18211. Curran Associates, Inc., 2021. URL [https://proceedings.neurips.cc/paper_files/paper/2021/file/97275a23ca44226c9964043c8462be96-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/97275a23ca44226c9964043c8462be96-Paper.pdf).\n' +
      '* Sakaguchi et al. (2019) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.\n' +
      '* Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022. URL [https://arxiv.org/abs/2211.05100](https://arxiv.org/abs/2211.05100).\n' +
      '* Scialom et al. (2022) Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. Fine-tuned language models are continual learners. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pp. 6107-6122, 2022.\n' +
      '* Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Katrin Erk and Noah A. Smith (eds.), _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1715-1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL [https://aclanthology.org/P16-1162](https://aclanthology.org/P16-1162).\n' +
      '* Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. _arXiv preprint arXiv:1909.08053_, 2019.\n' +
      '* Shoeybi et al. (2020) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.\n' +
      '* Smith et al. (2021) James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen, Hongxia Jin, and Zsolt Kira. Always be dreaming: A new approach for data-free class-incremental learning. pp. 9374-9384, October 2021.\n' +
      '* Soboleva et al. (2023) Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. [https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama), June 2023. URL [https://huggingface.co/datasets/cerebras/SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B).\n' +
      '* Sohn et al. (2019)* Soldaini et al. (2024) Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilsha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research. _CoRR_, abs/2402.00159, 2024. URL [https://doi.org/10.48550/arXiv.2402.00159](https://doi.org/10.48550/arXiv.2402.00159).\n' +
      '* Sun et al. (2020) Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE 2.0: A continual pre-training framework for language understanding. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pp. 8968-8975. AAAI Press, 2020. URL [https://doi.org/10.1609/aaai.v34i05.6428](https://doi.org/10.1609/aaai.v34i05.6428).\n' +
      '* Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023a. URL [https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971).\n' +
      '* Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. _CoRR_, abs/2307.09288, 2023b. URL [https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288).\n' +
      '* Veniat et al. (2021) Tom Veniat, Ludovic Denoyer, and MarcAurelio Ranzato. Efficient continual learning with modular networks and task-driven priors. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=EKV158tSfwv](https://openreview.net/forum?id=EKV158tSfwv).\n' +
      '* Wu et al. (2024) Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, and Ying Shan. Llama pro: Progressive llama with block expansion. _CoRR_, abs/2401.02415, 2024. URL [https://doi.org/10.48550/arXiv.2401.02415](https://doi.org/10.48550/arXiv.2401.02415).\n' +
      '* Xie et al. (2023) Yong Xie, Karan Aggarwal, and Aitzaz Ahmad. Efficient continual pre-training for building domain specific large language models, 2023. URL [https://arxiv.org/abs/2311.08545](https://arxiv.org/abs/2311.08545).\n' +
      '* Xiong et al. (2020) Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture, 2020.\n' +
      '* Yadav et al. (2023) Prateek Yadav, Qing Sun, Hantian Ding, Xiaopeng Li, Dejiao Zhang, Ming Tan, Parminder Bhatia, Xiaofei Ma, Ramesh Nallapati, Murali Krishna Ramanathan, Mohit Bansal, and Bing Xiang. Exploring continual learning for code generation models. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023_, pp. 782-792. Association for Computational Linguistics, 2023a. URL [https://doi.org/10.18653/v1/2023.acl-short.68](https://doi.org/10.18653/v1/2023.acl-short.68).\n' +
      '* Yadav et al. (2023) Prateek Yadav, Derek Tam, Leshem Choshen, Colin A. Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), _Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,USA, December 10 - 16, 2023_, 2023b. URL [http://papers.nips.cc/paper_files/paper/2023/hash/1644c9af28ab7916874f6fd6228a9bcf-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/1644c9af28ab7916874f6fd6228a9bcf-Abstract-Conference.html).\n' +
      '* Yang et al. (2022) Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, David Farhi, Jakub Pachocki, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyper-parameter transfer. In _NeurIPS 2021_, March 2022. URL [https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/](https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/).\n' +
      '* Yang et al. (2024) Xianjun Yang, Junfeng Gao, Wenxin Xue, and Erik Alexandersson. PLMA: An open-source large language model for plant science. _CoRR_, abs/2401.01600, 2024. URL [https://doi.org/10.48550/arXiv.2401.01600](https://doi.org/10.48550/arXiv.2401.01600).\n' +
      '* You et al. (2019) Kaichao You, Mingsheng Long, Jianmin Wang, and Michael I. Jordan. How does learning rate decay help modern neural networks?, 2019.\n' +
      '* Zan et al. (2022) Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-Guang Lou. CERT: continual pre-training on sketches for library-oriented code generation. In Luc De Raedt (ed.), _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022_, pp. 2369-2375. ijcai.org, 2022. URL [https://doi.org/10.24963/ijcai.2022/329](https://doi.org/10.24963/ijcai.2022/329).\n' +
      '* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019.\n' +
      '* Zhai et al. (2022) Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pp. 1204-1213. IEEE, 2022. URL [https://doi.org/10.1109/CVPR52688.2022.01179](https://doi.org/10.1109/CVPR52688.2022.01179).\n' +
      '* Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _arXiv preprint arXiv:2303.18223_, 2023. URL [https://arxiv.org/abs/2303.18223](https://arxiv.org/abs/2303.18223).\n' +
      '\n' +
      '###### Contents\n' +
      '\n' +
      '* 1 Introduction\n' +
      '* 2 Main findings and Takeaways\n' +
      '* 3 Related Work\n' +
      '	* 3.1 Continual learning\n' +
      '	* 3.2 Pre-training, Model Scale, and Continual-learning\n' +
      '	* 3.3 Domain Adaptive Continual Pre-training (DACPT)\n' +
      '	* 3.4 Continual Learning for LMs Applied to Specific Domains\n' +
      '	* 3.5 Learning Rate Schedules\n' +
      '* 4 Background & Methodology\n' +
      '	* 4.1 Linear Warmup and Cosine Decay Schedule\n' +
      '	* 4.2 Compute-equivalent Replay\n' +
      '* 5 Experimental Setup\n' +
      '	* 5.1 Datasets\n' +
      '	* 5.2 Continual Learning Settings\n' +
      '	* 5.3 Training Setup\n' +
      '	* 5.4 German and English LM Evaluation Benchmark\n' +
      '* 6 Results\n' +
      '	* 6.1 Learning Rate Schedule\n' +
      '		* 6.1.1 The Effect of Linear Warmup for Weak and Strong Distribution Shifts.\n' +
      '		* 6.1.2 The effect of re-warming, re-decaying, and varying \\(\\eta_{max}\\) for Weak and Strong Distribution Shifts.\n' +
      '	* 6.2 The Effect of Replay\n' +
      '	* 6.3 Continual Pre-training Final Performance for Weak and Strong Distribution Shifts.\n' +
      '		* 6.3.1 Final Performance Evaluated by Loss\n' +
      '		* 6.3.2 Final Performance Evaluated by Zero-shot and Few-shot Results on Popular LM Benchmarks\n' +
      '	* 6.4 Continual Pre-training Final Performance at Different Model Scales\n' +
      '		* 6.4.1 Final Performance Evaluated by Loss\n' +
      '		* 6.4.2 Final Performance Evaluated by Zero-shot and Few-shot Results on Popular LM Benchmarks\n' +
      '* 7 Understanding and Circumventing the Pathologies of Re-warming\n' +
      '	* 7.1 Re-warming on the Same Data\n' +
      '* 8	* 7.2 Infinite Learning Rate Schedules\n' +
      '	* 7.3 Comparing Cosine Decay to Variants of our Infinite Schedules\n' +
      '	* 7.4 Infinite Learning Rate Schedules: Scaling to Infinite Future Updates\n' +
      '* 8 Limitations\n' +
      '* 9 Conclusion\n' +
      '* A Extended results\n' +
      '* A.1 Domain Incremental continual pre-training\n' +
      '* A.2 Model Merging v.s. Continual Pre-training\n' +
      '* A.3 Replay for Different Dataset Sizes\n' +
      '* A.4 Qualitative evaluation of German models\n' +
      '* A.5 Aggregated LM Evaluation Results\n' +
      '* A.6 Aggregated average final accuracy\n' +
      '* B Model hyperparameters\n' +
      '\n' +
      '## Appendix A Extended results\n' +
      '\n' +
      'In the following subsections, we first present some new results in the _Domain incremental continual pre-training_ setting, comparing replay for different dataset sizes, and providing a qualitative analysis of our German language models. We also provide aggregated evaluation and final loss tables for all models in the paper.\n' +
      '\n' +
      '### Domain Incremental continual pre-training\n' +
      '\n' +
      'We consider a **domain incremental learning** setting, where we train on a sequence of \\(N\\) future datasets \\(\\{\\mathcal{D}_{0},\\mathcal{D}_{1},\\dots,\\mathcal{D}_{N-1}\\}\\) each of which comes from a distinct domain. This setting is particularly challenging due to the distribution shift experience at the transition between each domain. Concretely, we consider dataset \\(\\mathcal{D}_{0}\\) to be pre-training on the Pile (Gao et al., 2020) and \\(\\mathcal{D}_{1}\\) to be pre-training on SlimPajama. However, instead of consuming the data in \\(\\mathcal{D}_{1}\\) using the sampling percentages from table 1, we process the dataset one domain at a time starting from the largest domain to the smallest. This simulates a situation where new data is received from different domains at different times and we wish to update our models on the sequence while being robust to all distribution shifts.\n' +
      '\n' +
      'Adapting replay to Domain Incremental continual pre-trainingIn settings that span more than two tasks, we use a form of reservoir sampling (Buzzega et al., 2020), where the replay buffer is updated at discrete intervals. We refer to this technique as _discrete reservoir sampling_. Specifically, given a sequence of \\(N\\) datasets \\(\\mathcal{D}_{0},\\mathcal{D}_{1},\\dots,\\mathcal{D}_{N-1}\\) of sizes \\(s_{0},s_{1},\\dots,s_{N-1}\\) that are trained on in sequence, we update the replay buffer \\(\\mathcal{R}\\) at each dataset transition. Let \\(\\mathcal{R}_{i}\\) correspond to the state of the replay buffer before the ith task. For replay ratio \\(0\\leq\\alpha\\leq 1\\), at any given \\(i>0\\), \\(\\mathcal{R}_{i}\\) will contain data from all \\(\\mathcal{D}_{j}\\) for \\(j<i\\) in proportions \\(p_{i,j}:=\\frac{s_{j}\\cdot(1-\\alpha)+\\sum_{j=i+1}^{i-1}p_{i,j}\\cdot s_{k}\\cdot\\alpha }{\\sum_{k=0}^{i-1}s_{k}}\\), where \\(i\\) is the time index and \\(j\\) is the dataset index.\n' +
      '\n' +
      'Figure 10 reports results for a domain incremental approach to consuming our 300B token split of SlimPajama, where each domain is processed in decreasing order of size. We use learning rate re-warming, LR re-decaying, and replay with discrete reservoir sampling. The total data processed is the same as the model trained with LR re-warming and re-decaying on the 300B tokens of SlimPajama (blue dotted line). However, the order in which the data is processed and the learning rate schedule differs from the baseline.\n' +
      '\n' +
      'Figure 10: **Ingesting data sequentially—by domain. We explore an alternative approach to consuming the available data. Similar to task-incremental or class-incremental learning, we train on one domain at a time in a sequential fashion. We use LR re-warming and LR re-decaying for each domain as well as _discrete reservoir sampling_.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:35]\n' +
      '\n' +
      '### Replay for Different Dataset Sizes\n' +
      '\n' +
      'To ablate the insights gathered from the experiments in Sec. 6.2 we vary the size of the continued pre-training on SlimPajama with 100B, 200B, and 300B tokens. The respective linear warmup schedules and cosine decays are fitted to the altered length of training. As expected, we observe a consistent decrease in loss at the end of training for SlimPajama with an increased number of training tokens (see Fig. 11). We further observe similar trends concerning the effect of replay on forgetting and adapting to novel data. In conclusion, the simple combination of LR re-warming, LR re-decaying, and replay is effective at different dataset sizes.\n' +
      '\n' +
      '### Qualitative evaluation of German models\n' +
      '\n' +
      'In this section, we provide a brief qualitative evaluation of the models trained on German Common crawl (Sec. 6.3). We select five German prompts that contain various peculiarities of the German language (see Tab. 8). We then generate a fixed token-length response for each of the models trained or continually pre-trained on German Common Crawl. As a baseline, we also evaluate the same model trained only on the Pile.\n' +
      '\n' +
      'We provide the responses and a manual translation of the response in Table 9. While the relatively small 405M parameter models generally do not produce meaningful sentences, the model trained on German Common Crawl generally produces grammatically correct sentences. This includes correct upper and lower casing, the generation of compound words, and the use of Unlaut characters. In all cases, responses tend to be repetitive after a few words. The incompleteness and fractional words of some outputs can be attributed to the number of tokens generated being very short. Longer sequences tend to provide syntactically correct sentences but also tend to repeat themselves. The limited number of samples and the generally low quality of the generated text does not allow us to make robust statements of the qualitative differences between the individual models trained on German Common Crawl. However, the aforementioned problems of repetitiveness as well as grammatical errors seem to be significantly stronger on the Pile baseline, which generally fails to even respect the context given in the prompt. This is expected as the amount of German text is significantly smaller on the Pile.\n' +
      '\n' +
      'Thus we conclude that there is an observable effect on the generative quality of the German language from the models that used the data of German Common Crawl during training.\n' +
      '\n' +
      'Figure 11: **Replay v.s. no replay when re-warming the learning rate and continually pre-training on different amounts of data. We train 405M parameter models with and without 5% replay on 100B, 200B, and 300B tokens of SlimPajama data. Each model starts from a checkpoint pre-trained on 300B tokens of Pile and re-warms the learning rate using a linear warmup and cosine decay schedule fitted to its dataset size.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:37]\n' +
      '\n' +
      '### Aggregated LM Evaluation Results\n' +
      '\n' +
      'We evaluate our models on the following English and German LM evaluation tasks:\n' +
      '\n' +
      '1. **HellaSwag**(Zellers et al., 2019) and **HellaSwag DE**: An English commonsense reasoning benchmark composed of multiple-choice questions that are deliberately designed to confuse language models. HellaSwag DE is a German translation of the HellaSwag benchmark.\n' +
      '2. **AI2 Reasoning Challenge (ARC)**(Clark et al., 2018): An English commonsense reasoning benchmark composed of science examination questions in multiple-choice format. The \\(7,787\\) total questions have been divided into an easy subset with \\(5,197\\) questions and a hard subset with \\(2,590\\) questions. ARC-c DE is a German translation of the challenge subset of questions.\n' +
      '3. **BoolQ**(Clark et al., 2019): An English reading comprehension benchmark composed of \\(15,942\\) yes/no question-answering samples. Each example is split into a question, relevant paragraph, and the solution.\n' +
      '4. **MathQA**(Amini et al., 2019): An English math word problem benchmark composed of multiple-choice questions across various areas of mathematics.\n' +
      '5. **MMLU**(Hendrycks et al., 2021) and **MMLU-DE**: An English benchmark designed to evaluate both zero-shot and few-shot scenarios, in order to evaluate both the general knowledge and on-the-fly problem solving of the model under test. MMLU covers a broad range of subjects. MMLU-DE is a German translation of the MMLU question set, which was translated by the OpenAI GPT 3.5 API.\n' +
      '6. **OpenBookQA (OBQA)**(Mihaylov et al., 2018): An English question-answering benchmark modeled after real-world open-book exams for assessing human understanding of a particular subject. Questions about elementary science are paired with scientific facts and common knowledge, which the model is intended to use in multi-hop reasoning.\n' +
      '7. **PIQA**(Bisk et al., 2019): An English question-answering benchmark designed to test the physical commonsense reasoning abilities of the model. Most questions focus on applying uncommon solutions to everyday situations, which requires understanding of the physical world.\n' +
      '8. **WinoGrande**(Sakaguchi et al., 2019): An English natural language understanding benchmark that involves determining when two or more expressions in a text refer to the same entity. The benchmark includes a diverse set of sentences and a new evaluation metric that rewards models for making human-like predictions.\n' +
      '9. **TruthfulQA** and **TruthfulQA DE**(Lin et al., 2022): An English question-answering benchmark designed to evaluate the truthfulness of generated answers to questions. The questions are designed to contain common human misunderstandings that lead to incorrect answers. TruthfulQA DE is a German translation of the TruthfulQA benchmark.\n' +
      '10. **Natural Questions**(Kwiatkowski et al., 2019): Is an English question-answering benchmark consisting of search queries submitted to the Google search engine.\n' +
      '11. **TriviaQA**(Joshi et al., 2017): Is an English question-answering benchmark comprised of question-answer pairs provided by trivia enthusiasts. Its main focus is to determine a model\'s general world knowledge.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:39]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:40]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:41]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Description & Value \\\\ \\hline\n' +
      '**10B Model- Cosine Schedule** & \\\\ Max learning rate (\\(\\eta_{max}\\)) & \\(1.2\\cdot 10^{-4}\\) \\\\ Min learning rate (\\(\\eta_{min}\\)) & \\(1.2\\cdot 10^{-5}\\) \\\\ Warmup percent (\\(T_{warmup}\\)) & 1 \\\\ \\hline\n' +
      '**405M Model - Cosine Schedule** & \\\\ Max learning rate (\\(\\eta_{max}\\)) & \\(3\\cdot 10^{-4}\\) \\\\ Min learning rate (\\(\\eta_{min}\\)) & \\(3\\cdot 10^{-5}\\) \\\\ Warmup percent (\\(T_{warmup}\\)) & 1 \\\\ \\hline\n' +
      '**405M Model - Infinite LR Schedule Common** & \\\\ Max learning rate (\\(\\eta_{max}\\)) & \\(3\\cdot 10^{-4}\\) \\\\ Min learning rate (\\(\\eta_{min}\\)) & \\(3\\cdot 10^{-5}\\) \\\\ Constant learning rate (\\(\\eta_{const}\\)) & \\(1.65\\cdot 10^{-4}\\) \\\\ Warmup percent (\\(T_{warmup}\\)) & 1 \\\\ Cooldown iters percent (\\(T_{cd}\\)) & 60 \\\\ Constant iters percent (\\(T_{ann}\\)) & 25 \\\\ \\hline\n' +
      '**Inverse Square root cooldown schedule** & \\\\ Timescale (\\(\\alpha\\)) & 10 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 14: **Hyperparameters of LR schedules.** Unless otherwise specified in the text, we use these values.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>