<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '연속적으로 사전 훈련된 대용량 언어 모델을 위한 간단하고 확장 가능한 전략\n' +
      '\n' +
      'Adam Ibrahim\\({}^{*\\dagger}\\)\n' +
      '\n' +
      'Benjamin Therien\n' +
      '\n' +
      'kshitij.gupta@mila.quebec\n' +
      '\n' +
      'Mats L. 리치\n' +
      '\n' +
      'mats.richter@mila.quebec\n' +
      '\n' +
      'Quentin Anthony\n' +
      '\n' +
      'qubitentin@gmail.com\n' +
      '\n' +
      'Timothee Lesort\n' +
      '\n' +
      't.lesort@gmail.com\n' +
      '\n' +
      'Eugene Belilovsky\n' +
      '\n' +
      'eugene.belilovsky@concordia.ca\n' +
      '\n' +
      'Irina Rish\n' +
      '\n' +
      'irina.rish@umontreal.ca\n' +
      '\n' +
      '고려대학교 컴퓨터학과\n' +
      '\n' +
      '캐나다 몬트리올 대학교\n' +
      '\n' +
      '고려대학교 컴퓨터소프트웨어공학과\n' +
      '\n' +
      '캐나다 몬트리올 컨코디아 대학교\n' +
      '\n' +
      '캐나다 몬트리올 밀라\n' +
      '\n' +
      'EleutherAI\n' +
      '\n' +
      '동일한 기여; 동일한 기여자 내의 저자 순서는 무작위화되었다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)은 일상적으로 수십억 개의 토큰에 대해 사전 훈련되며, 새로운 데이터를 사용할 수 있게 되면 프로세스를 다시 시작합니다. 훨씬 더 효율적인 해결책은 이러한 모델을 지속적으로 사전 훈련하는 것이다 - 재 훈련에 비해 상당한 계산량을 절약한다. 그러나, 새로운 데이터에 의해 유도된 분포 이동은 전형적으로 이전 데이터에 대한 성능 저하 또는 새로운 데이터에 대한 열악한 적응을 초래한다. 이 연구에서, 우리는 최종 손실 및 언어 모델(LM) 평가 벤치마크에 의해 측정된 모든 가용 데이터에 대해 처음부터 완전히 재교육하는 성능을 일치시키기에 이전 데이터의 단순하고 확장 가능한 학습률(LR) 재온화, LR 재감소 및 재생의 조합이 충분하다는 것을 보여준다. 구체적으로, 일반적으로 사용되는 두 LLM 사전 학습 데이터 세트(영어\\(\\rightarrow\\)English) 사이의 약하지만 현실적인 분포 이동과 큰 데이터 세트 크기(수천억 토큰)를 갖는 405M 파라미터 모델 스케일에서 더 강한 분포 이동(영어\\(\\rightarrow\\)German)에 대해 이를 보여준다. 대규모 실험을 위한 약하지만 현실적인 이동을 선택하면, 우리는 또한 우리의 지속적인 학습 전략이 10B 매개변수 LLM에 대한 재훈련 기준선과 일치한다는 것을 발견했다. 우리의 결과는 LLM이 단순하고 확장 가능한 연속 학습 전략을 통해 성공적으로 업데이트될 수 있음을 보여주며, 계산의 일부만 사용하여 재교육 기준선과 일치합니다. 마지막으로, 이전 작업에서 영감을 받아 LR 재온화에 의해 유발된 잊힘을 우회하고 고정된 토큰 예산에 구속되지 않는 코사인 학습 속도 일정에 대한 대안을 제안한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '지난 몇 년 동안, 대규모 사전 훈련된 모델들은 언어 모델링(Brown et al., 2020; Zhao et al., 2023), 시각적 이해(Radford et al., 2021; Alayrac et al., 2022; Kirillov et al., 2023), 텍스트-투-이미지 생성(Rombach et al., 2022; Pernias et al., 2024) 및 텍스트-투-비디오 생성(Brooks et al., 2024)에서 상당한 성능 향상을 가능하게 했다. 대규모 언어 모델(LLM)은 이러한 모든 개선의 중심에 있으며, 인간이 언어를 통해 기계 학습 알고리즘과 인터페이스할 수 있는 직관적인 수단을 제공한다.\n' +
      '\n' +
      'LLM은 현재의 생성 AI 기술의 초석이지만, 그들은 훈련하고 최신 정보를 유지하는 데 엄청나게 비싸다. 그러나, 새롭고 더 높은 품질의 데이터 세트가 계속 이용 가능하게 됨에 따라(Gao et al., 2020; Soboleva et al., 2023; Computer, 2023; Soldaini et al., 2024), 조직은 경쟁에 보조를 맞추기 위해 그들의 모델을 업데이트해야 할 것이다. 현재 LLM은 오래된 데이터와 새로 수집된 데이터의 조합으로 재교육된다. 기존 연구들은 저가의 하이퍼파라미터 최적화를 가능하게 하거나(Yang et al., 2022), 주어진 계산 예산 하에서 성능을 최대화하기 위한 가이드라인을 제공함으로써 이러한 훈련 비용을 줄이는 것을 목표로 한다(Hoffmann et al., 2022). 그러나, 이러한 작업들은 모델들이 랜덤 초기화_로부터 _train될 것이라고 가정하며, 다음과 같은 질문을 제기한다: 실무자들은 항상 기존의 데이터세트와 랜덤 초기화_로부터 _train을 결합하여 최상의 성능을 얻어야 하는가? 모든 모델의 업데이트에 대해 그렇게 하는 것은 엄청나게 비쌉니다.\n' +
      '\n' +
      '완전한 재교육의 필요성을 피하기 위해 대량의 새로운 데이터(200B+ 토큰)에 대해 LLMs(최대 10B 매개변수)를 사전 훈련하기 위한 간단하고 확장 가능한 연속 학습 전략을 탐구한다. 우리는 우리의 설정을 "연속적인 사전 훈련"이라고 지칭하며, 우리가 고려하는 많은 양의 들어오는 데이터로 인해 문헌에 존재하는 설정으로부터 _distinct_임을 강조한다(Gururangan et al., 2020; Ke et al., 2022; Scialom et al., 2022; Xie et al., 2023). 이 연구에서는 사용 가능한 모든 데이터에 대해 무작위 초기화를 통해 학습된 모델의 성능을 향상시키려고 하지 않는다. 대신, 우리는 기존 데이터 세트의 조합에 대해 훈련된 모델을 규모에서 연속 학습 전략의 조합을 사용하여 일치하는 성능을 추구하는 기준선으로 간주한다.\n' +
      '\n' +
      '그러나 새로운 데이터에 대한 모델을 체계적으로 계속 훈련하는 것은 종종 1) 적응 불량(새로운 데이터 세트를 최적화하지 못함) 또는 2) 치명적인 망각(이전 데이터 세트에서 상당한 능력 손실)으로 인해 사용 가능한 모든 데이터에 대한 재훈련보다 훨씬 낮은 성능으로 이어지는 경향이 있다. 첫째, 대규모 데이터 세트에 대한 훈련은 비용이 많이 들기 때문에 적응 문제는 우리 설정의 핵심이다. 아마도 새로운 데이터 세트에 최소한으로 적응하기 위해 상당한 계산 리소스 교육을 사용하기로 선택하지 않을 것이다. 그러나, 대부분의 수행 오픈 소스 LLM들(Touvron et al., 2023; Jiang et al., 2023; Gemma Team et al., 2024)은 트레이닝이 끝날 때까지 그들의 학습 속도를 작은 값으로 붕괴시킨다. 따라서 우리는 새로운 데이터 세트에 대해 훈련할 때 사용된 계산당 적응을 개선하기 위해 학습률을 다시 증가시키고 다시 감소시켜야 한다고 가정한다. 우리는 이것이 지속적인 학습 문헌에서 철저히 연구되지 않았다는 점에 주목한다. 둘째, 재난적 망각은 지속적인 사전 훈련의 잠재력을 완전히 실현하는 것이라면 극복해야 할 핵심 어려움이다. 수천억 개의 새로운 토큰에 적응하는 것은 중요하지만, LLM에서 현존하는 대부분의 지식을 지우는 대가를 치러서는 안 된다. 최근 작업(Scialom et al., 2022)은 LLM 미세 조정 설정에서 이전 데이터(1%만큼 적게)를 다시 재생하면 망각을 크게 완화하기에 충분하다는 것을 보여준다. 많은 양의 새로운 데이터에 대한 지속적인 사전 교육이 미세 조정보다 거의 확실히 더 많은 망각을 초래할 것이지만 적절한 양의 재생이 우리의 설정에서도 망각을 완화할 수 있다고 가정한다. 또한, 최근 연구는 사전 훈련(Cossu et al., 2022; Ramasesh et al., 2022; Mehta et al., 2023)과 모델 크기 증가(Mirzadeh et al., 2022)가 망각의 영향을 줄이는 데 도움이 된다는 것을 보여준다. 따라서, 우리는 언어 모델 용량과 사전 훈련 데이터 세트 크기가 탠덤으로 증가하는 추세(Kaplan et al., 2020; Hoffmann et al., 2022; Touvron et al., 2023)가 점점 더 지속적인 학습이 가능한 모델을 산출할 것으로 예상하며(Scialom et al., 2022), 이는 우리의 실험 결과가 모델 규모로만 개선되어야 함을 시사한다.\n' +
      '\n' +
      '연속 학습이 재학습 모델과 관련된 비용을 상당히 줄일 수 있는 큰 가능성과 LLM이 강력한 연속 학습자가 될 수 있는 가능성을 감안할 때, 우리는 다음과 같은 질문을 자문한다. _단순하고 확장 가능한 연속 학습 기법이 적용될 때 모든 데이터의 조합에 대한 무작위 초기화에서 사전 학습된 LLM에 비해 연속 학습된 LLM 간의 성능 차이는 무엇입니까?_ 이 질문에 답하기 위해 LLM 사전교육을 위한 지속학습 기법에 대한 대규모 실증연구를 실시한다. 본 연구의 실증분석은 큰 모형(10B)과 작은 모형(405M)에 걸쳐 있으며, 약한 모형(영문\\(\\rightarrow\\) 영어)과 강한 모형(영문\\(\\rightarrow\\) 독일어)의 분포 변화를 보인다. 우리의 주요 기여는 다음과 같이 요약할 수 있다.\n' +
      '\n' +
      '1. 코사인 스케쥴을 이용하여 사전 훈련된 모델에 대하여 학습률 재보온 및 재보온 효과를 설정하여, 지속적인 사전 훈련 중 적응을 위해서는 재보온 및 재보온이 필요함을 보여준다.\n' +
      '\n' +
      ' 2. 두 가지 분포 이동과 많은 재생 백분율에 걸쳐 계산량을 일정하게 유지하면서 이전 데이터를 재생하는 효과를 설정한다. 우리는 수천억 개의 새로운 토큰에 모델을 업데이트할 때에도 적절한 양의 재생으로 망각을 크게 완화할 수 있음을 발견했다.\n' +
      '3. 두 가지 모델 크기와 분포 이동에 걸쳐 LR 재-온화, LR 재-감소 및 계산-동등성 재생의 단순하고 확장 가능한 조합이 훨씬 적은 계산량을 사용하면서 모든 데이터의 조합에서 재-훈련된 모델과 유사한 성능을 지속적으로 얻을 수 있음을 보여준다.\n' +
      '4. 학습률 재온화와 관련된 최적화 어려움을 피할 수 있는 유망한 방법으로 LLM의 지속적인 사전 훈련을 위한 무한 학습률 스케줄(데이터 세트 간의 원활한 전환을 허용하는 스케줄)을 제안한다.\n' +
      '\n' +
      '출판되면 코드와 최종 모델 체크포인트를 공개적으로 사용할 수 있도록 하겠습니다.\n' +
      '\n' +
      '##2 주요결과 및 포장\n' +
      '\n' +
      '실험 결과는 연속적으로 사전 훈련된 LLM이 순차적으로 두 개 이상의 사전 훈련 단계를 거친다고 가정한다. 즉, 연속적으로 미리 학습된 LLM이 랜덤하게 초기화되고 데이터세트 \\(\\mathcal{D}_{0},\\mathcal{D}_{1},\\dots,\\mathcal{D}_{N-1}\\)에서 차례로 \\(N\\geq 2\\)과 \\(\\textit{tokens}(\\mathcal{D}_{i})\\geq 100\\)B로 사전 학습된 상황에 본 연구의 결과를 적용한다. 우리는 여기에 문제의 LLM이 오픈 소스 모델(Touvron et al., 2023; 20; Jiang et al., 2023; Gemma Team et al., 2024)이 이미 \\(\\mathcal{D}_{0}\\)에 대해 사전 훈련된 상황과 조직들이 새로운 데이터에 대해 지속적으로 사전 훈련할 의도로 초기 LLM을 훈련하기를 원할 수 있는 상황을 포함한다는 점에 주목한다. 새로운 데이터는 약한 분포 이동(예를 들어, 상이한 도메인들의 최신 웹-스크래프)에 대응하는 이전 데이터와 유사하거나, 강한 분포 이동(예를 들어, 완전히 새로운 언어로부터의 데이터)에 대응하는 이전 데이터와 상당히 상이할 수 있다. 우리의 실험 평가는 이러한 어려움을 설명하며 LR 재온난화, LR 재감소 및 재재생을 적절하게 적용하면 약하고 강한 분포 이동과 두 모델 크기에 걸쳐 재훈련의 성능과 일치하기에 충분하다는 것을 발견했다(참고).\n' +
      '\n' +
      '그림 1: **연속 사전 훈련은 유사한 최종 검증 및 평가 성능을 유지하면서 모델을 업데이트하는 데 드는 계산 비용을 감소시킨다. 본 논문에서는 Pile \\(\\cup\\) SlimPajama(SP)/German(Ger.) Baseline 모델에 대한 결과를 보고한다. 또한 사전 훈련된 두 가지 모델에 대한 성능을 보고합니다. "PT on Pile"은 미리 훈련된 Pile 체크포인트에서 시작하여 학습률 재워밍 및 재디케이팅만을 사용하는 반면, "Replay(PT on Pile)"은 학습률을 재워밍하여 재디케이팅하고 SlimPajama는 5%, German은 25%의 리플레이를 사용한다. LR 재온난화, 재부패 및 재생의 조합이 지속적으로 사전 훈련된 모델이 기준 모델과 유사한 성능을 달성하면서도 실질적으로 덜 계산이 필요하다는 것을 관찰한다. 이 설정은 사전 훈련된 모델이 사용 가능하다고 가정한다는 점에 유의한다(예: 허깅페이스 허브 또는 지속적으로 사전 훈련되도록 설계된 사내 모델을 통해).**\n' +
      '\n' +
      '도. (p<0.05). 우리의 연구 결과를 가능한 한 커뮤니티에 접근할 수 있도록 이제 연구 결과를 적용하기 위한 _Rules of thumb_를 제공한다.\n' +
      '\n' +
      '```\n' +
      '지속적인 사전 교육을 위한 경험 규칙\n' +
      '```\n' +
      '\n' +
      '**Caveat**--다음 지침은 현재 지식의 최선으로 작성됩니다.\n' +
      '\n' +
      '**Learning rate schedule:**\n' +
      '\n' +
      '* 초기 데이터셋에 대한 사전 훈련 시 학습률이 큰 값\\(\\eta_{max}\\)에서 작은 값\\(\\eta_{min}\\)으로 코사인 감쇄되었다면, 다음과 같은 지침은 모델을 지속적으로 사전 훈련하는 데 도움이 될 수 있다:\n' +
      '* \\(\\mathcal{O}(\\eta_{max})\\)에서 \\(\\mathcal{O}(\\eta_{min})\\)으로 학습률을 재온화 및 재감쇠하는 것은 작은 학습률 \\(\\mathcal{O}(\\eta_{min})\\)에서 계속되는 것에 비해 새로운 데이터 세트에 대한 적응을 향상시킨다.\n' +
      '* 일정의 최대 학습률을 낮추는 것은 망각을 줄이는 데 도움이 될 수 있는 반면, 이를 높이는 것은 적응을 향상시킬 수 있다.\n' +
      '* 무한 LR 스케줄은 코사인 붕괴 스케줄에 대한 유망한 대안이다. 그들은 태스크 간에 높은 일정한 학습 속도로 전환되어, 태스크 간의 LR을 다시 따뜻하게 하는 것을 피함으로써 최적화 관련 망각을 방지하는 데 도움이 된다. 그들은 또한 최종 지수 붕괴가 훈련 중 어느 시점에서 모델을 수렴으로 훈련시키는 데 사용될 수 있기 때문에 토큰의 특정 예산에 전념하는 것을 피한다.\n' +
      '**Replay:**\n' +
      '\n' +
      '* 기본값으로 5% 리플레이를 권장합니다. 더 많은 재생은 더 강한 분포 이동과 함께 사용되어야 하는 반면 약한 분포 이동의 경우 1%만큼 적게 벗어날 수 있다.\n' +
      '\n' +
      '##3 관련 업무\n' +
      '\n' +
      '### Continual learning\n' +
      '\n' +
      '연속 학습(CL) 접근법은 사전 훈련을 통해 수집된 지식을 유지하면서 새로운 데이터에 적응하면서 진화하는 데이터 분포로부터 학습하는 것을 목표로 한다(French, 1999; Rolnick et al., 2019; Caccia et al., 2020; Lesort et al., 2021). 지속적인 학습의 핵심 과제는 과거의 정보를 잊는 것을 피하는 동시에 새로운 정보에 적응하는 것이다. 이러한 절충은 강성-소성 딜레마(Mermillod et al., 2013; Ostapenko et al., 2019; Riemer et al., 2019)로 알려져 있다.\n' +
      '\n' +
      'CL 접근법은 처음부터 재교육을 피하거나 데이터 가용성 문제를 해결하기 위해 소규모 설정에서도 편리하다(Smith et al., 2021). 그러나 규모 면에서 CL은 편리함 이상이며, 지속적으로 수집된 방대한 양의 데이터를 처리해야 할 수도 있다. 최근 훈련 규모의 증가는, 특히 LLMs(Scao et al., 2022; Brown et al., 2020; Zhao et al., 2023)에 대해, CL이 재훈련 비용을 감소시키고 메모리, 컴퓨팅, 및 저장에 대한 효율성을 증가시키기 위한 새로운 기회들을 제공한다(Prabhu et al., 2023; Aljundi et al., 2019; Harun et al., 2023; Veniat et al., 2021; Harun et al., 2023). 연합 학습은 공간에 공동 배치된 서로 다른 에이전트들(McMahan et al., 2017; Reddi et al., 2021; Douillard et al., 2023; Ryabinin et al., 2021) 간의 컴퓨팅 및 데이터의 공유를 가능하게 할 수 있듯이, 지속적인 학습은 시간을 통해 점진적으로 컴퓨팅 및 데이터의 공유를 가능하게 하며 대규모 훈련에 유용한 도구가 될 수 있다.\n' +
      '\n' +
      '최근의 연구는 SGD 및 Adam과 같은 최적화기들이 CL에 대해 스케일에서 유익할 수 있는 DNN들에서 흥미로운 지식 보유 특성들을 가지며(Lesort 외, 2023), 단지 소량의 리플레이가 지식 축적을 증가시키기에 충분할 수 있다는 것을 보여준다(Scialom 외, 2022). 본 연구에서는 대규모 언어 모델의 사전 훈련 상황에서 이러한 접근법의 효율성으로부터 이익을 얻고 적절한 학습률 스케줄링 및 재생 정책으로 이를 개선하고자 한다.\n' +
      '\n' +
      '### 사전학습, 모형척도, 연속학습\n' +
      '\n' +
      '기존의 여러 연구는 사전 훈련과 모델 척도가 지속적인 학습에 미치는 영향을 평가한다. Cossu et al.(2022)은 언어 및 시각에 대한 사전 훈련 시나리오를 조사한다. 그들은 감독되지 않고 자체 감독된 사전 훈련이 망각을 완화하는 데 근본적인 역할을 하는 반면 감독은 성과를 해친다는 것을 발견한다. 유사하게, Mehta et al. (2023)은 미리 훈련된 모델들이 손실 경관의 더 평평한 영역들에 놓여 있는 가중치들로 인해, 랜덤하게 초기화된 모델들보다 적게 잊어버린다는 것을 발견한다. 그들은 또한 라마세시 외(2022); Mirzadeh 외(2022)의 발견과 연결된 더 큰 모델들은 더 적게 잊는다는 것을 발견한다. 전자는 미리 훈련된 모델이 확장됨에 따라 덜 잊어버린다는 것을 발견했으며, 이는 축척과 더 직교적으로 성장하는 숨겨진 표현 때문일 수 있음을 시사한다. 후자는 더 넓은 신경망이 매개변수 등가 더 깊은 대응물에 비해 덜 잊어버린다는 것을 발견한다. Hernandez et al. (2021)은 전송을 위한 스케일링 법칙들: 새로운 태스크에 대한 신경망의 성능을 그의 파라미터 카운트 및 사전-트레이닝 데이터세트 크기의 함수로서 예측할 수 있는 방정식들을 확립한다. 저자들은 이 긍정적인 전달이 매개변수 수가 증가함에 따라 일관되게 개선된다는 것을 발견했다. 마지막으로, Scialom et al. (2022)은 자기회귀 LLM이 사전 훈련 목표와 관련된 가설을 지속적으로 학습할 수 있는 강력한 능력을 가지고 있음을 보여준다.\n' +
      '\n' +
      '### 도메인 적응 연속 사전 훈련(DACPT)\n' +
      '\n' +
      '기존 작업은 도메인 적응 연속 사전 훈련(DACPT, Domain Adaptive Continual Pre-training)을 고려하는데, 도메인 적응 사전 훈련은 레이블이 지정되지 않은 일련의 도메인을 LM이 순차적으로 사용할 수 있게 하고 실무자는 각 도메인에 걸쳐 성능을 유지하면서 자체 감독 방식으로 각 도메인에 대해 훈련하기를 원하는 설정이다. 목적은 우리의 것과 유사하지만, 우리는 도메인 특정 데이터 세트와는 대조적으로 많은 도메인을 혼합하는 범용 사전 훈련 데이터 세트를 고려한다. Ke et al. (2022)는 새로운 도메인에 대한 트레이닝 시 이전 도메인으로부터의 데이터를 이용할 수 없다고 가정하고, 마스크된 언어 모델링(MLM) 목적으로 사전 트레이닝할 때 잊혀지는 것을 방지하기 위해 모든 이전 태스크에 대한 파라미터의 중요도 마스크를 포함하는 이 설정을 위한 새로운 기술을 개발한다. Gururangan et al. (2020)은 RoBERTa (also MLM)의 도메인 및 태스크 적응형 사전 트레이닝을 조사하고 효율적인 연속 사전 트레이닝을 위한 샘플 선택 전략에 기여했다. 유사하게, Xie et al.(2023)은 또한 연속적인 사전-트레이닝(자기회귀 LMs에 대해 도시됨)의 계산 비용을 감소시키는 데이터 선택 전략을 제안한다. Qin et al. (2023)은 베이스 LM의 지속적으로 업데이트된 버전들을 특정 태스크들에 적응시키기 위한 새로운 어댑터의 초기화로서 이전 베이스 LMs들의 재사이클링 미세 조정 어댑터 층들을 조사한다. 최근 Wu et al. (2024)는 이전 지식을 잊지 않고 새로운 작업을 학습할 수 있도록 하는 LLM의 지속적인 사전 훈련을 위한 방법인 LLaMA Pro를 제안하였다. 그러나 기존의 모든 가중치를 적용하는 것을 고려하는 작업과 달리 LLaMA Pro는 새로운 업데이트마다 모델의 크기를 늘리고 새로운 가중치만 조정해야 한다.\n' +
      '\n' +
      '특정 도메인에 적용된 LM의 연속 학습###\n' +
      '\n' +
      '여러 관련 작업은 특정 작업 및 도메인에 대한 지속적인 사전 훈련을 적용한다(Sun et al., 2020; Jang et al., 2022; 2022; Gong et al., 2022; Zan et al., 2022; Yadav et al., 2023; Ma et al., 2023; Yang et al., 2024). 이러한 작업은 또한 지속적인 사전 훈련 기술을 활용하지만, 더 작은 모델을 가진 더 작은 규모의 데이터 세트 \\(<\\) 10B 토큰에서 일반적인 사전 훈련 기술 대신 특정 도메인에 초점을 맞추어 우리의 작업과 다르다. 우리의 데이터세트 척도에 접근하는 유일한 기존 작업은 영어, 덴마크어, 아이슬란드어 및 노르웨이어 데이터세트(각각 73B)에 걸쳐 지속적인 자기회귀 언어 모델링을 탐구하는 고굴루 외, 2023이다. 그들은 리플레이를 사용하지 않는 동안 학습률을 다시 따뜻하게 하고 다시 약화시킨다. 우리의 모델 척도에 접근하는 현존하는 유일한 작업은 (Yang et al., 2024)이다. 그들은 소규모 학술 식물 과학 데이터에서 LLaMA2를 지속적으로 사전 훈련하고 가르친다. 이 동시 작업은 우리가 제안하는 것과 매우 유사한 연속 학습 설정(replay, LR re-warming, LR re-decaying)을 사용한다. 우리의 작업과 달리, 그들은 지속적인 사전 훈련을 위해 이러한 접근법의 유효성을 체계적으로 평가하기 위해 통제된 실험 프레임워크를 구축하지만, 우리의 접근법을 검증하는 추가 실험 증거를 보는 것은 좋다.\n' +
      '\n' +
      '### 학습률 스케줄\n' +
      '\n' +
      '여러 연구에서 다양한 학습률(LR) 일정이 신경망의 훈련 안정성과 최종 성능에 미치는 영향을 조사했다. Goyal et al.(2018)은 LR 얼리 오닌 트레이닝의 점진적인 워밍업이 특히 큰 미니 배치 사이즈로 최적화 도전을 극복하는 데 도움이 될 수 있다는 것을 발견했다. 또 포펠과 보야르(2018)는 포스트LN 트랜스포머 훈련 시 워밍업 무대의 중요성을 강조했다. 반면, Xiong et al.(2020)은 Pre-LN Transformers가 더 안정적이며 워밍업 단계가 필요하지 않을 수 있음을 발견하였다. You et al.(2019)은 LR 붕괴의 역할을 탐색한 결과, 큰 초기 LR은 네트워크가 잡음 데이터를 암기하는 것을 방지하는 반면 작은 LR은 복잡한 패턴을 학습하는 데 도움이 된다는 것을 발견했다. Kaplan et al. (2020)은 LLM(Pre-training Large Language Models)을 위한 LR 스케줄을 탐색한 결과, 스케줄 선택이 성능에 큰 영향을 미치지 않는 것으로 나타났다. 이러한 잘못된 발견을 정정하는, Hoffmann et al.(2022)은 LR 스케줄이 중요한 역할을 한다는 것을 발견했다. Hoffmann et al. (2022) 및 Rae et al. (2021)은 널리 채택된 LLMs를 사전 트레이닝할 때 코사인 스케줄을 사용하기 위한 모범 사례들을 확립하였다. 대조적으로, Raffel et al. (2023) 및 Zhai et al. (2022)는 대규모 사전 훈련을 위해 역 제곱근 붕괴를 따르는 LR 스케줄을 탐색한다. Raffel et al.(2023)은 훈련 LLM들을 위해 역 제곱근 감쇠를 활용함으로써, 훈련 단계들의 수를 조정하는데 유연성을 허용한다. Zhai et al.(2022)에서, 저자들은 비전 트랜스포머를 훈련시키기 위해 \'무한 학습률 스케쥴\'로 지칭되는 이러한 스케쥴들을 이용한다. 이러한 스케줄은 무기한 훈련 및 단일 실행에서 여러 훈련 기간의 평가를 가능하게 한다. 우리는 LLMs(Sec. 7.4)에 대해 제안된 무한 학습 속도 스케줄이 이 아이디어에서 영감을 받았다는 점에 주목한다.\n' +
      '\n' +
      '##4 배경 및 방법론\n' +
      '\n' +
      '이 섹션에서는 LLM의 맥락에서 지속적인 사전 훈련과 관련하여 적절한 배경과 방법론을 제공한다.\n' +
      '\n' +
      '### 선형 웜업 및 코사인 감쇠 스케줄\n' +
      '\n' +
      'Hoffmann et al. (2022) 및 Rae et al. (2021)은 LLMs를 사전 트레이닝할 때 코사인 스케줄을 사용하기 위한 모범 사례들을 확립하였다. 구체적으로, 선형 웜업 단계로 시작하여 코사인 사이클의 종료가 토큰의 수와 일치하도록 설정되도록 학습률을 최대값(10\\times\\)으로 낮추는 것을 추천한다. 선형 준비 기간은 다르지만, 가장 주목할만한 작업은 훈련 단계에 대해 0.1% 내지 0.5% 사이의 기간을 갖는다(Zhao et al., 2023). 많은 인기 있는 오픈 소스 모델(Touvron et al., 2023; 20; Almazrouei et al., 2023)이 이 학습 속도 스케줄 레시피를 따른다는 점을 감안할 때, 이러한 모델을 지속적으로 사전 훈련하기 위한 그 뉘앙스를 이해하는 것이 중요하다. 일정은 먼저 \\(T_{warmup}\\) 타임스템에 걸쳐 학습률을 선형적으로 증가시키거나, 또는 등가적으로 일부 타임스템 \\(t_{ann}=T_{warmup}\\)까지 증가한다:\n' +
      '\n' +
      '\\[\\eta_{t}=\\eta_{max}\\cdot\\frac{t}{T_{warmup}} \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(\\eta_{t}\\)는 반복 \\(t\\)에서의 학습률의 값이고, \\(\\eta_{max}\\)는 최대 학습률이다. 그런 다음 일정은 \\(T_{end}=T_{ann}+t_{ann}\\)까지의 등가적으로, \\(T_{ann}\\) 타임스템에 걸쳐 코사인 어닐링 단계로 전환된다:\n' +
      '\n' +
      '\\[\\eta_{t}=\\eta_{min}+\\frac{(\\eta_{max}-\\eta_{min}}{2}\\cdot\\left(\\cos\\left(\\pi\\cdot\\frac{t-t_{ann}}{t_{end}-t_{ann}}\\right)+1\\right}\\tag{2}\\cos\\left(\\pi\\cdot\\frac{t-t_{ann}}}\n' +
      '\n' +
      '여기서 \\(\\eta_{max}\\)는 최대 학습률이고 \\(\\eta_{min}\\)는 최소 학습률이다.\n' +
      '\n' +
      '### Compute-equivalent Replay\n' +
      '\n' +
      '많은 실험에서 우리는 재생으로 훈련된 모델과 재생 없이 훈련된 모델을 비교한다. 이러한 비교를 할 때, 우리는 두 모델을 훈련시키기 위해 계산량을 일정하게 유지한다. 즉, 리플레이 버퍼에서 볼 수 있는 추가 토큰을 수용하기 위해 새로운 데이터 세트에서 볼 수 있는 토큰의 수를 대응적으로 줄인다. 우리는 이러한 리플레이의 사용을 _compute-equivalent replay_라고 한다. 예를 들어, 데이터 세트 \\(\\mathcal{D}_{0}\\)와 \\(\\mathcal{D}_{1}\\)이 각각 100B 토큰을 포함한다고 가정하자. 우리는 \\(\\mathcal{D}_{0}\\) 및 \\(\\mathcal{D}_{1}\\)에 순차적으로 훈련된 모델 (a)와 \\(\\mathcal{D}_{0}\\) 및 \\(\\mathcal{D}_{1}\\)에 순차적으로 훈련된 모델 (b)를 5%의 계산 등가 리플레이와 비교하고자 한다. 모델 (a)는 총 200B 고유 토큰에 대해 두 데이터 세트에서 모든 토큰을 볼 수 있습니다. 모델 (b)는 총 200B 토큰에 대해 \\(\\mathcal{D}_{0}\\)의 100B 고유 토큰과 \\(\\mathcal{D}_{1}\\)의 95B 고유 토큰과 \\(\\mathcal{D}_{0}\\)의 5B 재생 토큰을 볼 것이다. 이러한 방식으로, 비교된 두 모델 모두 동일한 양의 계산량을 소모한다.\n' +
      '\n' +
      '두 데이터세트에서의 재생은 단지 두 데이터세트\\((\\mathcal{D}_{0},\\mathcal{D}_{1})\\)에 걸쳐 있는 우리의 설정에서, 우리는 \\(\\mathcal{D}_{1}\\)에서 훈련할 때 \\(\\mathcal{D}_{0}\\)의 데이터를 다시 재생한다. 우리는 이러한 모델을 "\\(\\mathcal{D}_{1}\\)\\(x\\)% Replay"라고 하며, 여기서 \\(x\\)은 \\(\\mathcal{D}_{0}\\)에서 오는 각 훈련 배치에서 데이터의 백분율이다. 반대로, 각 훈련 배치에서 샘플의 \\((100\\%-x)\\)%는 \\(\\mathcal{D}_{1}\\)에서 샘플링될 것이다. 리플레이로 훈련된 모델을 다른 구성과 비교할 때, 우리는 \\(\\mathcal{D}_{0}\\)에서 리플레이 토큰을 수용하기 위해 \\(\\mathcal{D}_{1}\\) 토큰의 수를 줄임으로써 계산이 _equivalent_임을 보장한다.\n' +
      '\n' +
      '## 5 실험 설정\n' +
      '\n' +
      '무작위 초기화의 학습 LLM과 비교하여 연속 사전 학습 LLM의 효과를 경험적으로 평가하기 위해 문헌에서 최근 사전 학습 데이터 세트를 선택하고 조사를 위한 실제 연속 사전 학습 설정의 개요를 설명하고 제안된 기술과 비교할 몇 가지 기준선을 선택한다. 우리의 목표는 통제된 환경에서 기준선과 지속적인 사전 훈련 기술을 공정하게 비교하는 것입니다. 우리는 본 논문의 범위를 벗어나 최신 성능을 얻거나 모델과 비교하려고 하지 않는다.\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '훈련 및 검증을 위해 SlimPajama (Soboleva et al., 2023), German CommonCrawl (Laippala et al., 2022), Pile (Gao et al., 2020)의 세 가지 데이터 세트를 사용한다. 모든 데이터 세트에 대해, 파일에 대해 구체적으로 트레이닝된 Black et al.(2022)과 동일한 토큰화기를 사용한다. 슬림파자마에 대한 훈련 세트를 만들기 위해 데이터 세트(606B 총 토큰)를 무작위로 하위 샘플링하여 파일과 비슷한 크기의 \\(\\sim\\)299B 토큰 하위 집합(표 1 참조)을 형성한다. 우리는 또한 이 슬림파자마 서브세트를 추가로 서브샘플링하여 데이터세트의 3\\(\\sim\\)100B 토큰 분할을 생성한다(자세한 내용은 Sec. 7.4 참조). SlimPajama 유효성 검사 세트를 생성하기 위해 우리는 광범위하게 중복 제거된 기본 유효성 검사 세트를 간단히 토큰화한다(Soboleva et al., 2023). 독일어 훈련 및 유효아이톤 세트를 생성하기 위해, 우리는 오스카 다타세트(라이팔라 등, 2022)의 일부로 사용 가능한 독일어 커먼 크롤 스크래프를 195.43B 토큰 훈련 세트와 982.6M 토큰 검증 세트로 분할하고 토큰화했다. 파일 데이터 세트는 미리 셔플되고 혼합되며 기본 훈련 및 유효성 검사 세트를 사용했다. 학습 집합은 총 ${\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim\n' +
      '\n' +
      '### 연속 학습 설정\n' +
      '\n' +
      '우리는 본문에서 세 가지 현실적인 연속 사전 훈련 설정을 고려하고 부록에서 덜 보장된다고 믿는 3분의 1에 대한 결과를 제공한다. 지속적인 사전 훈련의 다양한 도전과 장점을 노출시키기 위해 각 설정을 신중하게 선택했다. 우리의 설정은 지속적으로 사전 훈련된 LLM이 순차적으로 두 개 이상의 사전 훈련 단계를 거친다고 가정한다. 즉, 연속적으로 사전 학습된 LLM이 랜덤하게 초기화되어 데이터셋에 사전 학습된 경우(\\mathcal{D}_{0},\\mathcal{D}_{1},\\dots,\\mathcal{D}_{N-1}\\)에 순차적으로 적용된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r} \\hline \\hline Dataset & Size (Tokens) & Sampling (\\%) \\\\ \\hline Wikipedia & 11.96B & 4.00 \\\\ Book & 12.58B & 4.20 \\\\ C4 & 79.87B & 26.69 \\\\ Stack Exchange & 10.09B & 3.37 \\\\ GitHub & 15.63B & 5.22 \\\\ Common Crawl & 155.89B & 52.09 \\\\ Arxiv & 13.25B & 4.43 \\\\ \\hline Total & 299.28B & 100.00 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **SlimPajama.**의 300B 토큰 트레이닝 세트의 도메인 크기. 우리는 SlimPajama 데이터세트(606B 총 토큰)를 300B 토큰 분할로 서브 샘플링하여 파일과 비슷한 크기로 만들었다. 슬림파자마를 구성하는 서브샘플링된 도메인의 크기와 훈련 시간에 사용된 샘플링 백분율(예: 특정 도메인에서 오는 각 배치의 샘플 백분율)을 보고한다.\n' +
      '\n' +
      '여기서 \\(N\\geq 2\\). 현실적인 설정을 위해 \\(\\textit{tokens}(\\mathcal{D}_{i})\\geq 100\\)B를 고려한다. 각각의 경우에, 우리는 다음의 자연 기준선을 고려한다:\n' +
      '\n' +
      '* 모든 데이터셋의 합집합 즉 \\(\\bigcup_{i=0}^{N-1}\\mathcal{D}_{i}\\)에 대한 랜덤 초기화로부터 학습된 모델, 및\n' +
      '* 개별 데이터세트 \\(\\mathcal{D}_{i}\\), \\(0\\leq i\\leq N\\)에 대한 랜덤 초기화로부터 학습된 모델.\n' +
      '\n' +
      '본 논문에서는 선형 웜업(linear warmup)과 코사인 감쇠 LR 스케줄(cosine decay LR schedule)을 사용하여 데이터세트(\\(\\mathcal{D}_{0}\\))에서 자기회귀 언어 모델링을 위해 사전 훈련된 (예: hugging face 또는 사전 훈련된 in-house) 모델을 사용할 수 있다고 가정한다. 우리는 또한 일정이 문헌의 기존 관습(예: 토큰 예산으로 쇠퇴함; Sec 참조)을 따른다고 가정한다. 자세한 내용은 도 4에 도시된 바와 같다(Rae et al., 2021; Hoffmann et al., 2022; Touvron et al., 2023a;b). 본 논문에서 제안한 모델은 기존에 학습한 모델(\\(\\mathcal{D}_{0}\\)이 주어졌을 때, 본 논문에서 제안한 모델을 새로운 데이터세트(\\(\\mathcal{D}_{1}\\)에서 갱신하고자 하는 것으로 가정하였다. 우리는 **2-dataset 설정**의 구체적인 변형을 고려한다:\n' +
      '\n' +
      '두 개의 데이터 세트, 약한 이동**: 이 변형에서, 우리는 \\(\\mathcal{D}_{0}\\)을 파일(Gao et al., 2020)로 간주하고, \\(\\mathcal{D}_{1}\\)을 SlimPajama(Soboleva et al., 2023)에 대한 사전 훈련으로 간주한다. SlimPajama는 LLaMA 데이터세트(Touvron et al., 2023a)를 기반으로 구축된 RedPajama(Computer, 2023)의 광범위하게 중복 제거된 버전이다. 우리는 두 데이터 세트가 모두 영어이고 공통 도메인(CommonCrawl, GitHub, Arxiv, Wikipedia, StackExchange)과 다른 비중첩 도메인을 포함하고 있기 때문에 약하지만 현실적인 배포 이동이라고 생각한다. 추가적으로, SlimPajama(2023)는 파일(2020)보다 더 새로운 데이터세트이며, 따라서 중복되는 도메인 내에서 더 새로운 데이터를 가질 가능성이 있다. 따라서 잠재적인 중복에도 불구하고, 우리는 이러한 전환이 현실적이며 사전 훈련(예: 더 높은 품질 필터링으로 동일한 소스의 새로 수집된 데이터)과 유사한 분포에서 LLM을 업데이트하기를 원하는 많은 실무자에게 흥미로울 가능성이 있다고 믿는다.\n' +
      '두 개의 데이터 세트, 더 강한 이동**: 이 변화에서, 우리는 \\(\\mathcal{D}_{0}\\)을 파일에서 사전 훈련으로 간주하고(Gao et al., 2020), \\(\\mathcal{D}_{1}\\)을 독일 커먼 크롤에서 사전 훈련으로 간주한다. 독일 커먼 크롤은 오스카 데이터세트(Laippala et al., 2022)에서 가져온 \\(\\sim 200\\)B 토큰 데이터세트이다. 우리는 이것이 언어의 변화를 고려할 때 더 강력한 변화를 구성한다는 점에 주목한다. 이 설정은 사전 훈련과 어휘가 현저하게 다른 새로운 자연 언어, 프로그래밍 언어 또는 특정 영역으로 LLM을 보강하고자 하는 실무자에게 특히 중요하다. 그러나 우리는 도메인이 토큰라이저의 학습 코퍼스에서 점점 멀어질수록 토큰라이저는 성능에 중요한 병목 현상이 될 수 있다는 점에 주목한다. 토키나이저의 치료는 추후 작업에 맡깁니다.\n' +
      '\n' +
      '\\(N>2\\) **settings -** 우리는 또한 더 많은 데이터 세트를 갖는 스케일들이 얼마나 잘 고려되었는지를 조사하기 위해 더 많은 데이터 세트 전이들을 갖는 다음의 설정들을 고려한다:\n' +
      '\n' +
      'SlimPajama의 각 구역 100B 토큰 분할을 \\(\\mathcal{D}_{0},\\mathcal{D}_{1},\\mathcal{D}_{2}\\)으로 하는 \\(N=3\\) 설정을 고려한다. 이 설정은 주로 미래의 많은 업데이트로 확장하는 기술의 능력을 평가하고 제안된 무한 학습 속도 일정의 성능을 평가하는 데 사용된다.\n' +
      '***도메인 증분 연속 사전-트레이닝**: 이 설정은 도메인별로 순차적으로 순서화된 슬림파자마의 토큰을 소비하는 것을 고려한다. 즉, 우리는 SlimPajama 300B의 별개의 영역인 \\(\\{\\mathcal{D}_{0},\\mathcal{D}_{1},\\dots,\\mathcal{D}_{N-1}\\})의 미래 데이터 세트들의 시퀀스를 학습한다. 이는 DACPT(Ke et al., 2022)와 유사하지만 각 도메인에 대해 훨씬 더 큰 데이터 세트를 고려한다는 점에 주목한다. 이러한 설정은 각 도메인 간의 전환 시 분포 이동 경험으로 인해 특히 어렵다. 확실히 흥미롭지만, 우리는 슬림파자마 데이터를 훈련하기 전에 혼합하는 것과 비교할 때 불필요하게 어렵다고 믿는다. 이 설정의 열악한 결과(부록의 Sec. A.1)는 범용 LLM이 도메인당 업데이트되지 않고 가능한 한 도메인의 혼합물에서 지속적으로 사전 훈련되어야 함을 시사한다.\n' +
      '\n' +
      '### Training Setup\n' +
      '\n' +
      'Megatron-DeepSpeed(Shoeybi et al., 2019; Microsoft, 2020)에 기초한 GPT-NeoX(Andonian et al., 2021)를 사용하여, 인과 언어 모델링 목적을 갖는 자기회귀 디코더 전용 변압기를 트레이닝한다. 모델은 Pre-LN을 사용합니다. 각 모델은 Black et al.(2022)과 동일한 tokenizer를 이용하여 학습되며, BPE 알고리즘을 통해 Pile 상에서 배타적으로 학습되었다(Sennrich et al., 2016). 모든 모델에 대해 배치 크기 1104와 시퀀스 길이 2048을 사용하여 adamW 최적화기(Loshchilov and Hutter, 2019)를 사용하여 훈련하고, 300B 토큰에서 훈련하여 총 132,366\\의 경사 하강 단계를 거치게 된다. 우리는 임베딩을 포함하는 두 개의 모델 크기 405M 및 9.6B 매개변수(이 작업에서 10B로 지칭됨)를 고려한다. 4개의 마이크로 배치 크기를 사용하여 46개의 6개의 GPU 노드에 걸쳐 데이터 병렬성을 사용하여 더 작은 모델을 학습한다. 더 큰 모델은 노드 내에서 6개의 GPU에 걸쳐 있는 텐서 병렬성(Shoeybi et al., 2020)과 4개의 노드에 걸쳐 있는 파이프라인 병렬성(Huang et al., 2019)을 사용하여 학습한다. 즉, 각 모델 복제본은 4개의 노드에 걸쳐 24개의 GPU에 걸쳐 있다. 우리는 4단계의 기울기 축적을 사용하여 276개의 노드에서 이 모델을 훈련한다. 각 모델은 ZeRO-1을 통한 최적화 샤딩(Rajbhandari et al., 2020), 활성화 체크포인팅Chen et al.(2016), 텐서 병렬 랭크에 걸친 활성화 파티셔닝, 및 혼합 정밀도 FP16/FP32를 사용하여 GPU 메모리 소비를 감소시키고 트레이닝 동안 NVIDIA 텐서 코어를 완전히 활용한다. 우리는 부록의 모든 하이퍼파라미터에 대한 확장된 설명을 제공했다(표 13).\n' +
      '\n' +
      '### 독일어 및 영어 LM 평가 벤치마크\n' +
      '\n' +
      '우리는 매우 다양한 다운스트림 작업에 대한 성능을 측정하는데, 이를 크게 다음과 같이 분류할 수 있다.\n' +
      '\n' +
      'English Benchmarks\n' +
      '\n' +
      '**상식 추론(0-shot):**HellaSwag(Zellers et al., 2019), Winogrande(Sakaguchi et al., 2019), PIQA(Bisk et al., 2019), OpenBookQA(Mihaylov et al., 2018), ARC-Easy, ARC-Challenge(Clark et al., 2018)\n' +
      '**World Knowledge(5-shot):** NaturalQuestions(Kwiatkowski et al., 2019), TriviaQA(Joshi et al., 2017)\n' +
      '**Reading Comprehension(0-shot):**BoolQ(Clark et al., 2019)\n' +
      '* **Math:** MathQA (Amini et al., 2019)\n' +
      '* **Popular Aggregated Results:** MMLU(5-shot)(Hendrycks et al., 2021)\n' +
      '\n' +
      'GPT 3.5 API를 사용하여 영어 상대방을 번역한 (Pluster, 2023)의 독일어 벤치마크\n' +
      '\n' +
      '* **Commonsense Reasoning (0-shot):** HellaSwag-DE (Zellers et al., 2019), ARC-Challenge-DE (Clark et al., 2018)\n' +
      '* **World Knowledge (5-shot):** TriviaQA-DE (Joshi et al., 2017)\n' +
      '* **Popular Aggregated Results:** MMLU-DE(5-shot)(Hendrycks et al., 2021)\n' +
      '\n' +
      '## 6 Results\n' +
      '\n' +
      '우리는 들어오는 데이터 세트가 큰 경우(200B 토큰+)에 대한 지속적인 사전 교육에 중점을 둡니다. 이러한 설정에서, 트레이닝은 비용이 많이 들기 때문에, 많은 양의 들어오는 데이터에 효율적으로 적응하는 것이 중요하다. 그러나, 대부분의 수행자 LLMs(Rae et al., 2021; Hoffmann et al., 2022; Zhao et al., 2023; Touvron et al., 2023b;a)는 비교적 낮은 최소 학습률을 갖는 선형 웜업 및 코사인 감쇠 스케줄로 트레이닝된다. 우리는 **re-warming** 이 학습률이 상대적으로 높은 값으로 증가하고 후속적으로 새로운 데이터 세트에 효율적으로 적응하기 위해 재발생하는 것이 필요하다고 가정한다. 이를 위해 섹션 6.1에서 선형 워밍업 기간, LR 재워밍, LR 재디케이팅 및 최대 학습 속도 크기가 적응 및 망각에 미치는 영향을 연구한다. 재워밍과 재디케이팅이 적응과 망각을 모두 증가시킨다는 것을 발견하면, 섹션 6.2에서 우리는 학습률이 재워지고 재디케이팅될 때 재플레이가 망각을 완화하는 데 도움이 될 수 있는지 조사한다. 하위 섹션 6.3과 6.4는 이전 두 섹션에서 연구된 전략을 결합하고 약하고 강한 분포 이동에 대한 기준선과 대규모 모델 규모에 대한 성능을 보고한다.\n' +
      '\n' +
      '마지막으로 섹션 7에서 LR 재온난화가 원치 않는 망각을 유발할 수 있음을 설명하고 이를 우회하는 유망한 방법으로 무한 학습률 일정을 소개하고 이러한 일정을 기준선과 비교한다.\n' +
      '\n' +
      '### 학습률 스케줄\n' +
      '\n' +
      '학습률이 적응에 미칠 수 있는 영향과 두드러진 LLM의 낮은 최종 LR 값(Rae et al., 2021; Hoffmann et al., 2022; Zhao et al., 2023; Touvron et al., 2023b;a)을 감안할 때, 우리는 LR이 지속적인 사전 훈련 동안 적응을 촉진하기 위해 다시 데워지고 다시 감쇠되어야 한다고 가정한다. 이 절에서는 사전 훈련을 계속할 때 선형 예열 기간, LR의 재가열, LR의 재감쇠 및 \\(\\eta_{max}\\)의 크기에 대한 영향을 조사한다. 구체적으로 **2-dataset weak shift** 설정(300B Pile \\(\\rightarrow\\) 300B SlimPajama)과 **2-dataset stronger shift** 설정(300B Pile \\(\\rightarrow\\) 300B SlimPajama)에서 각각의 효과를 평가한다. 특히, \\(\\mathcal{D}_{0}\\)(파일의 300B 토큰)으로 훈련된 모델은 선형 웜업 및 코사인 감쇠 스케줄1을 따르며, 많은 일반적인 오픈 소스 사전 훈련된 LLM을 시뮬레이션한다.\n' +
      '\n' +
      '각주 1: 본 논문의 모든 코사인 소멸에 대해, 달리 명시되지 않는 한, 코사인 어닐링 단계를 토큰 버짓에 맞추고, 선형 워밍업 지속 시간(\\(T_{warmup}\\))을 훈련 반복의 1%로 설정하고, \\(\\eta_{min}=0.1\\cdot\\eta_{max}\\)을 설정한다.\n' +
      '\n' +
      '선형 웜업이 약분포 및 강분포 변화에 미치는 영향\n' +
      '\n' +
      '먼저 **2 데이터 세트, 약한 시프트** 및 **2 데이터 세트, 더 강한 시프트** 설정에서 망각 및 적응에 대한 선형 워밍업 기간의 영향을 조사한다(자세한 내용은 Sec. 5.2 참조). 모델들은 Pile(Gao et al., 2020)의 300B 토큰들(\\(\\mathcal{D}_{0}\\))에 대해 미리 트레이닝된다. 우리는 훈련의 첫 50B 토큰에 대해 슬림파자마(약 교대)와 독일 커먼 크롤(강 교대)에 대한 모델을 계속 사전 훈련한다. 우리는 다시 따뜻해지고 다시 쇠퇴해\n' +
      '\n' +
      '그림 2: ** 약한 분포 이동과 강한 분포 이동에 대한 선형 준비의 효과.**(a), (b) 및 (c), (d)는 오른쪽 그림에 표시된 범례가 각각 같다. 우리는 훈련 반복의 0%, 0.5%, 1% 및 2%의 선형 예열 기간을 가변하는 선형 예열 및 코사인 감쇠 일정에 따라 405M 매개변수 모델을 훈련한다. 각 학습률 스케줄은 데이터세트의 크기에 따라 학습이 종료되면 \\(0.1\\eta_{max}\\)로 감소한다. 우리는 훈련의 처음 50B 토큰에 대한 결과를 보고한다. 탐색된 설정에서 예열 단계의 기간이 사전 훈련을 계속할 때 영향을 미치지 않는 것으로 보인다는 것을 관찰한다.\n' +
      '\n' +
      '300B 및 200B 토큰에서 각각 최소값(\\(\\eta_{min}=0.1\\cdot\\eta_{max}\\))에 도달하도록 설정된 코사인 학습률 스케줄을 이용한 학습률. 0.5%, 1%, 2%의 \\(\\mathcal{D}_{1}\\) 총 훈련 반복(각각 132366회, 86000회 반복)에 대해 학습률을 예열하는 것을 고려한다. 또한 LR을 \\(\\eta_{max}\\)에서 즉시 붕괴시키는 선형 웜업(0%)이 없는 모델을 훈련한다. 모든 실험은 405M 매개변수 모델에서 수행된다.\n' +
      '\n' +
      '그림 2는 모든 모델에 대한 \\(\\mathcal{D}_{0}\\) 및 \\(\\mathcal{D}_{1}\\)에 대한 검증 손실을 \\(\\mathcal{D}_{1}\\)에 대한 계속된 사전 훈련의 처음 50B 토큰 전체에 걸쳐 보고한다. 상단 행은 약한 분포 이동의 결과를 보고하고 하단 행은 강한 분포 이동의 결과를 보고합니다. 두 분포 이동 모두에서, 우리는 초기에 더 짧은 선형 웜업을 사용하는 모델이 더 긴 웜업 대응물보다 초기에 잊고 더 빨리 적응한다는 것을 관찰한다. 이것은 LR을 더 빠르게 증가시켜 더 빠른 잊기와 적응으로 이어지기 때문에 발생한다. 그러나 모든 시나리오에서 이러한 초기 차이는 훈련 전반에 걸쳐 감소하여 50B 토큰 후 망각 및 적응이 비교적 유사한 모든 모델을 남긴다. 따라서 탐색된 설정에서 선형 웜업 단계의 지속 시간은 사전 훈련을 계속할 때 잊거나 적응하는 데 영향을 미치지 않는 것으로 판단된다.** 이 점을 염두에 두고 모든 후속 실험에 대해 훈련 반복의 1%의 선형 웜업 기간을 설정한다.\n' +
      '\n' +
      '1.2 재온, 재감쇠, \\(\\eta_{max}\\)의 영향이 약분포 및 강분포의 변화에 미치는 영향.\n' +
      '\n' +
      '우리는 이제 \\(\\eta_{max}\\)의 다른 값에 대해 (예를 들어, 코사인 스케줄에 따라) 학습률을 다시 따뜻하게 하고 다시 감쇠시키는 것의 이점을 조사한다. 구체적으로, 이 모델들을 두 개의 자연기준선(\\(\\eta_{min}\\)(\\(3\\cdot 10^{-5}\\))과 재온화하지 않는 모델(\\(3\\cdot 10^{-5}\\))과 사전 훈련에 재온화시키는 모델)과 비교한다.\n' +
      '\n' +
      '그림 3: **학습률을 다시 따뜻하게 하고 다시 쇠퇴시키는 것이 적응과 망각에 미치는 영향. 우리는 2개의 일정한 기준선과 재따뜻하고 재감쇠하는 3개의 모델을 고려한다. 한 기준선은 사전 훈련(\\(3\\cdot 10^{-4}\\))의 \\(\\eta_{min}\\)에서 훈련을 계속하고, 다른 기준선은 사전 훈련(\\(3\\cdot 10^{-4}\\)에서 \\(\\eta_{max}\\)까지 예열된다. 재온 및 재감쇠 모델의 경우 \\(\\eta_{max}\\in\\{1.5\\cdot 10^{-4},3\\cdot 10^{-4},6\\cdot 10^{-4}\\})을 달리한다. \\(\\eta_{min}\\) 기준선을 제외한 모든 모델은 1% 훈련 반복에 선형 워밍업을 사용한다. 비기준 모델인 코사인 모델은 학습이 끝날 때까지 0.1 - \\(\\eta_{max}\\)에 도달하도록 학습을 붕괴시킨다. 우리는 새로운 데이터 세트에 가장 잘 적응하기 위해 학습률을 다시 따뜻하게 하고 다시 감쇠시키는 것이 필요하다는 것을 관찰한다. \\(\\eta_{max}\\)의 작은 증가 또는 감소는 다소간의 적응 사이의 절충을 허용한다. 더 강한 분배 이동은 망각과 적응 모두에 촉매가 되는 것 같다.\n' +
      '\n' +
      '\\((3\\cdot 10^{-4})\\))))는 재감쇠되지 않는다. 우리는 300B 토큰에 대해 파일(\\(\\mathcal{D}_{0}\\))에서 먼저 사전 훈련하고, \\(\\mathcal{D}_{1}\\) 데이터 세트로 슬림프자마(약 시프트) 또는 독일 커먼 크롤(강 시프트)에서 모델을 지속적으로 사전 훈련한다. 연속적인 사전 훈련은 데이터 세트의 전체 크기(각각 300B 및 200B 토큰)에 대해 수행된다. LR을 재온화 및 재감쇠시키는 모델은 사전 훈련의 절반(\\eta_{max}\\)으로 재온화(\\(1.5\\cdot 10^{-4}\\)), 사전 훈련과 동일한 \\(\\eta_{max}\\)으로 재온화(\\(3\\cdot 10^{-4}\\)), 사전 훈련의 두 배(\\(6\\cdot 10^{-4}\\))의 세 가지 전략을 고려한다. 모든 경우에, 학습률은 선형 예열 후 코사인 감쇠되어 학습이 끝날 때까지 \\(\\eta_{min}=0.1\\cdot\\eta_{max}\\)에 도달한다. 마지막으로 성능 상한을 제공하기 위해 \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\)에 훈련된 모델을 세 번째 기준선으로 간주한다.\n' +
      '\n' +
      '그림 3은 모든 모델의 지속적인 사전 훈련에 걸쳐 \\(\\mathcal{D}_{0}\\) 및 \\(\\mathcal{D}_{1}\\) 데이터 세트에 대한 검증 손실을 보고한다. 도표의 상단 행은 약한 분포 이동(300B Pile\\(\\rightarrow\\)300B SP)에 대한 결과를 보고하고, 하단 행은 강한 분포 이동(300B Pile\\(\\rightarrow\\)200B Ger)에 대한 결과를 보고하며, 두 이동 모두에 대해 \\(\\eta_{min}\\) 상수 학습률 모델은 \\(\\mathcal{D}_{0}\\)에서 가장 적은 망각을 달성한다. 또한 강한 이동의 경우 \\(\\mathcal{D}_{1}\\)에 가장 적게 적응하지만, 약한 이동의 경우 \\(\\eta_{max}\\) 상수 기준선보다 더 많이 적응한다. 이러한 기준선을 두 데이터 세트에서 다시 따뜻하고 다시 감쇠하는 모델과 비교할 때 후자의 모델이 두 분포 이동 모두에 대해 상당한 마진만큼 새로운 데이터 세트에 더 잘 적응한다는 것을 관찰한다. 이는 LLM을 지속적으로 사전 훈련할 때 새로운 데이터 세트에 대한 적응을 최대화하기 위해 재온화 및 재붕괴가 필요함을 보여준다. LR을 다시 따뜻하게 하고 다시 감쇠시키는 모델들 중에서, 우리는 학습률의 변화가 적응과 망각의 작은 차이를 야기한다는 것을 관찰한다: \\(\\eta_{max}\\)의 높은 값은 더 많은 망각과 더 많은 적응으로 이어지는 반면, 낮은 값은 그 반대이다. 기준선을 조합 훈련된 기준선과 비교할 때, 두 분포 이동에서 \\(\\mathcal{D}_{1}\\)에 대한 최종 검증 손실이 조합 훈련된 모델보다 훨씬 더 높다는 것을 관찰한다. 이 역시 약한 분포이동에 대한 \\(\\mathcal{D}_{1}\\)의 경우이지만, 흥미롭게도 강한 분포이동에 대해 상수기준선들은 합집합-훈련된 모델보다 더 낮은 \\(\\mathcal{D}_{1}\\) 검증손실을 달성한다. 우리는 이것이 LLM의 맥락에서 일반적으로 적응을 강화하고 망각을 악화시키는 더 강한 분포 이동 때문이라고 가정한다. 이 모델들은 조합 기준선에 재온화 및 재감쇠와 함께 지속적으로 사전 훈련된 모델들을 비교할 때, 조합 기준선보다 \\(\\mathcal{D}_{1}\\)에 더 잘 적응(최종 검증 손실 감소)한다는 점에 주목한다. 그러나 이 모델들은 \\(\\mathcal{D}_{0}\\)에서 상당한 망각을 경험하며, 이러한 모델들이 조합 기준선과 경쟁하도록 하기 위해 재생의 필요성을 보여준다.\n' +
      '\n' +
      '요약하면, 새로운 데이터 세트에 대한 적응을 최대화하기 위해서는 지속적으로 사전 훈련 LLM의 재-온화 및 재-감소 모두 필요하다; \\(\\eta_{max}\\)의 작은 증가 또는 감소는 적응 사이의 절충을 허용한다; \\(\\mathcal{D}_{0}\\)과 \\(\\mathcal{D}_{1}\\) 사이의 더 강한 분포 이동은 망각을 악화시키고 적응을 향상시킨다; 그리고 선형 예열 단계의 지속 시간은 망각 또는 적응에 영향을 미치지 않는 것으로 보인다.\n' +
      '\n' +
      '### Replay의 효과\n' +
      '\n' +
      '이 하위 섹션에서는 학습률을 다시 따뜻하게 하고 다시 감쇠시키는 모델을 지속적으로 사전 훈련할 때 계산 등가 재생의 효과를 탐구한다.\n' +
      '\n' +
      '재온난화 및 재부패 시 망각을 완화할 필요성을 감안할 때, 우리는 약하고 강한 이동의 지속적인 사전 훈련 시나리오에서 재플레이의 영향을 조사하기 위해 이동한다. 구체적으로, 예산에서 \\(\\mathcal{D}_{1}\\) 토큰의 등가 수를 제거하는 비용으로 \\(\\mathcal{D}_{0}\\) 토큰이 추가되는 경쟁 등가 리플레이(자세한 내용은 Sec. 4.2 참조)를 사용한다. 동일한 두 데이터 세트 설정에 따라 모델은 300B 토큰에 대해 \\(\\mathcal{D}_{0}\\)(파일)에서 사전 훈련된다. 이어 슬림파자마(약한 교대)나 독일 커먼 크롤(강한 교대)에 대한 지속적인 사전 훈련이 뒤따른다. 설치에 대한 자세한 내용은 섹션 5.2를 참조하십시오. 각 데이터 세트의 전체 크기에 대해 지속적인 사전 교육이 수행되며, 이는 슬림파자마(약한 시프트)의 경우 300B 토큰, 독일 커먼 크롤(강한 시프트)의 경우 200B 토큰입니다. 교대 모두에서 1%, 5%, 10% 및 50%의 리플레이를 고려하고 약한 분포 이동과 강한 분포 이동에서 각각 0.5% 및 25%의 리플레이 실행을 추가한다. 우리는 이러한 결과를 더 넓은 맥락에 넣기 위해 두 가지 기준을 고려한다. 첫 번째 베이스라인은 재생 없이 \\(\\mathcal{D}_{1}\\)으로 훈련된 모델이다. 두 번째 베이스라인 모델은 600B 토큰(SlimPajama)과 500B 토큰(German Common Crawl)에 대한 \\(\\mathcal{D}_{0}\\)과 \\(\\mathcal{D}_{1}\\)의 조합에 대한 랜덤 초기화로부터 학습된다. 후자의 기준선은 기존 모델을 지속적으로 사전 훈련하는 대신 모델을 업데이트하기 위해 완전히 재 훈련하는 관행을 반영한다. 모든 모델들은 \\(\\eta_{max}\\) (\\(3\\cdot 10^{-4}\\))과 \\(\\eta_{min}\\)(\\(3\\cdot 10^{-5}\\)) 값으로 토큰 버짓에 맞는 코사인 감쇠 스케줄을 사용하여 학습률을 재온화 및 재감쇠시킨다.\n' +
      '\n' +
      '검증 손실 비교 그림의 결과. 도 4(상하)는 각 \\(\\mathcal{D}_{1}\\) 데이터 세트에 대한 지속적인 사전 훈련 동안 검증 손실의 진화를 보여준다. 표 2는 이러한 모델 각각에 대한 평균 최종 검증 손실을 보고한다. 최종 손실은 10회 반복 간격으로 샘플링된 훈련의 마지막 100회 반복에 걸쳐 평균화된다. 우리는 두 분포 이동 모두에서 가장 낮은 테스트된 1%의 재생도 재생되지 않는 기준선에 비해 파일에서의 망각을 크게 감소시킨다는 것을 일관되게 관찰한다. 이 효과는 이 설정에서 더 많은 양의 망각으로 인해 강한 이동 시나리오에서 더 두드러진다. 우리는 0% 기준선과 비교할 때 1%, 5% 및 10% 리플레이에 대한 다운스트림 성능에 거의 영향을 미치지 않으며, 이는 리플레이의 잊는 이점이 설정에서 거의 비용이 들지 않는다는 것을 보여준다. 그러나 극도의 재생량(50%)을 사용할 경우, 모델이 \\(\\mathcal{D}_{0}\\)에 덜 적응하는 것을 관찰할 수 있다. 흥미롭게도 두 데이터 세트 모두 50%의 리플레이 모델이 \\(\\mathcal{D}_{1}\\cup\\mathcal{D}_{0}\\)에서 기초선 훈련의 최종 평균 검증 성능을 달성하거나 능가한다. 이 모델은 각 기준선보다 \\(\\mathcal{D}_{1}\\)의 토큰이 150B 및 100B 더 적었기 때문에 흥미롭다.\n' +
      '\n' +
      '요약하면, 우리는 LR을 지속적인 사전 훈련 맥락에서 다시 따뜻하게 하고 다시 쇠퇴시킬 때, 다시 재생이 망각을 줄이는 데 유용한 도구라는 것을 발견한다. 두 분포 이동에서 적절한 양의 리플레이를 사용하면 \\(\\mathcal{D}_{1}\\cup\\mathcal{D}_{0}\\) 기준선과 유사한 최종 검증 손실이 발생한다. 더욱이, 두 이동 모두에 대해 리플레이의 사용은 다운스트림 데이터 세트에 대한 적응에 무시할 수 있는 영향을 미치는 것으로 보이며, 이는 LLM을 지속적으로 사전 훈련할 때 리플레이를 통한 잊기를 줄이는 데 비용이 거의 들지 않는다는 것을 보여준다.\n' +
      '\n' +
      '도 4: **약하고 강한 분포 이동에 대한 405M 스케일에서의 재생 효과.** 훈련 중 파일 유효성 검사 손실(왼쪽) 및 슬림파자마/독일 유효성 검사(오른쪽 상단/하단)를 보고한다. 각 모델은 파일의 300B 토큰에서 미리 훈련된 체크포인트로부터 훈련된다. 파란색 점선은 Pile\\(\\cup\\)SlimPajama 또는 Pile\\(\\cup\\)German 데이터에 대해 각각 600B 및 500B 토큰 데이터 집합으로 훈련된 모델에 대한 최종 검증 손실을 보고한다. 우리는 리플레이가 두 교대 모두에서 망각을 상당히 감소시킨다는 것을 관찰하지만, 더 강한 이동은 동일한 정도로 망각을 완화하기 위해 더 많은 리플레이를 필요로 한다.\n' +
      '\n' +
      '약점 및 강점 분산 변화에 대한 연속 사전 훈련 최종 성능\n' +
      '\n' +
      '이 하위 섹션에서, 우리는 _2 데이터세트 약한 shift_(Pile \\(\\rightarrow\\) SlimPajama)와 _2 데이터세트 강한 shift_(Pile \\(\\rightarrow\\) German) 설정에서 두 개의 연속 사전 훈련된 405M 매개변수 모델을 여러 기준선과 비교한다. 우리의 주요 목표는 분배 이동의 차이가 최종 성과에 어떤 영향을 미치는지 결정하는 것이다.\n' +
      '\n' +
      'LR 재-워밍 및 재-디케이팅과 리플레이를 결합하는 성능을 줄이기 위해, 우리는 독점적으로 재-워밍 및 재-디케이팅하는 하나의 모델과 두 기술을 모두 결합하는 다른 모델을 훈련하기로 선택한다. 위크 분포 시프트에 대한 이전 섹션의 결과를 감안할 때, 우리는 위크 시프트 설정에 대해 5%의 리플레이와 더 강한 시프트 설정에 대해 25%의 리플레이를 선택한다. 두 모델 모두 사전 훈련(\\(3\\cdot 10^{-4}\\))의 \\(\\eta_{\\textit{max}\\)으로 재가열하고, 연속 사전 훈련이 끝날 때까지 \\(\\eta_{\\textit{min}\\)에 도달하도록 설정된 코사인 감쇠 스케줄을 사용하여 재감쇠한다. 더 많은 하이퍼파라미터가 부록의 표 13에 보고되어 있다.\n' +
      '\n' +
      '기준선 또한 여러 기준선을 훈련합니다. 두 개의 기준선은 각각 \\(\\mathcal{D}_{0}\\)과 \\(\\mathcal{D}_{1}\\)으로 훈련되고, 세 번째 기준선은 각 데이터세트의 합집합 \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\)으로 훈련된다. 고가의 완전 재학습을 나타내기 때문에, 본 논문에서는 \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\)으로 훈련된 모델을 성능 상한으로 간주한다. 개별 데이터 세트에 대해 훈련된 기준선은 연속 사전 훈련에 대한 계산적 등가 대안(예: 연속 사전 훈련 대신 \\(\\mathcal{D}_{1}\\)에 대한 무작위 초기화로부터 모델을 훈련하도록 선택할 수 있음)으로 볼 수 있다.\n' +
      '\n' +
      '손실에 의해 평가된 최종 성능\n' +
      '\n' +
      '그림 5는 약한(상단) 및 강한(하단) 이동에 대한 405M 매개변수 모델의 지속적인 사전 훈련 동안 검증 손실을 보고한다. 표 3은 이러한 모델에 대한 평균(마지막 100회 반복) 최종 손실 값을 보고한다. 영어에서 독일어로의 전환은 파일에서 슬림파자마로의 전환보다 더 극명한 분포 이동을 나타내므로, 독일어 훈련은 재생 없이 지속적으로 사전 훈련된 모델의 경우 파일(\\(\\mathcal{D}_{0}\\))에 대한 망각(각각 약 및 강 이동의 경우 0.27 대 1.39)으로 이어진다. 그러나 스타커 시프트를 처리하기 위해 25%의 리플레이를 선택하면 파일에서 잊는 양이 크게 줄어 최종 손실 측면에서 1.23이 감소한다. 연속적으로 사전 훈련된 모델을 \\(\\mathcal{D}_{1}\\)에서 독점적으로 훈련된 기준선과 비교할 때, 연속적으로 사전 훈련된 모델은 두 분포 이동 모두에서 항상 더 낮은 검증 손실을 갖는다는 것을 관찰한다. 연속적으로 사전 훈련된 모델과 \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\) 기준선을 비교할 때, 두 모델 모두 거의 동일한 (약 시프트) 또는 동일한 (강 시프트) 평균 검증 손실을 달성함을 알 수 있다. 이는 강한 분포 이동과 약한 분포 이동에 대해 LR 재-온화, LR 재-감소 및 재생의 단순하고 확장 가능한 조합이 \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\) 기준선과 유사한 성능을 달성할 수 있음을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline \\multicolumn{1}{c|}{**Training Tokens**} & \\multicolumn{3}{c}{**Validation Loss**} \\\\  & \\(\\mathcal{D}_{0}\\) Pile & \\(\\mathcal{D}_{1}\\) SlimPajama/German & AVG \\\\ \\hline\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP & 2.44 & 2.50 & 2.47 \\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP (0.5\\% Replay) & 2.27 & 2.50 & 2.39 \\\\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP (15 Replay) & 2.26 & 2.50 & 2.38 \\\\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP (55\\% Replay) & 2.23 & 2.51 & 2.37 \\\\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP (50\\% Replay) & 2.21 & 2.51 & 2.36 \\\\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP (50\\% Replay) & 2.16 & 2.54 & **2.35** \\\\\\\n' +
      '600B Pile \\(\\cup\\) SP & 2.17 & 2.53 & **2.35** \\\\ \\hline\n' +
      '300B Pile \\(\\rightarrow\\) 200B Ger. & 3.56 & 1.11 & 2.34 \\\\\n' +
      '300B 파일 \\(\\rightarrow\\) 200B Ger. (1\\% Replay & 2.83 & 1.12 & 1.97 \\\\\\\n' +
      '300B 파일 \\(\\rightarrow\\) 200B Ger. (55 Replay) & 2.57 & 1.12 & 1.85 \\\\\\\n' +
      '300B 파일 \\(\\rightarrow\\) 200B Ger. (10\\% Replay) & 2.46 & 1.13 & 1.80 \\\\\\\n' +
      '300B 파일 \\(\\rightarrow\\) 200B Ger. (25\\% Replay) & 2.33 & 1.16 & 1.75 \\\\\\\n' +
      '300B 파일 \\(\\rightarrow\\) 200B Ger. (50\\% Replay) & 2.24 & 1.22 & **1.73** \\\\\\\n' +
      '500B Pile \\(\\cup\\) Ger. & 2.26 & 1.25 & 1.75 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 다양한 양의 리플레이로 훈련된 영어 전용 405M 파라미터 모델의 **최종 손실.** 손실은 10번의 반복 간격으로 샘플링된 훈련의 마지막 100번의 반복에 걸쳐 평균화된다. 이러한 측정에 대한 표준 오차는 계산되었지만 모든 모델에 대해 \\(<0.001\\)인 것으로 보고되지 않았다. 우리는 더 많은 리플레이를 사용하는 모델이 더 나은 적응-잊는 트레이드-오프(AVG Loss)를 달성한다는 것을 관찰한다. 흥미롭게도 50% 리플레이를 사용하는 모델은 슬림파자마에서 150B 더 적은 토큰을 보면서 거의 동일한 손실 값을 보관한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '그림 5: 두 가지 분포 이동에 대해 훈련된 405M 매개변수 모델의 **최종 손실. 도 (a) 및 (b)는 도로부터 중복된다. 편리한 비교를 위해 6. 우리는 3개의 기준선과 2개의 지속적으로 사전 훈련된 모델을 제공했다. 기준선(밝은 파란색, 짙은 파란색 및 적갈색)은 슬림파자마의 300B 토큰, 파일의 300B 토큰 및 두 데이터 세트의 결합(600B 토큰)에 대한 무작위 초기화로부터 훈련된다. 연속적으로 미리 훈련된 모델(검은색과 보라색)은 파일(짙은 파란색 곡선)의 300B 토큰에서 미리 훈련된 체크포인트에서 시작하여 각각 0%와 5%의 리플레이를 사용한다. 우리는 두 분포 이동 모두에 대해 학습률을 다시 따뜻하게 하고 적은 비율의 리플레이를 사용하는 조합이 망각과 적응 사이의 균형을 맞추는 데 도움이 된다는 것을 관찰한다. 중요하게도, 우리는 리플레이의 사용이 0% 리플레이를 사용하는 모델과 비교하여 다운스트림 성능에 최소한으로 영향을 미친다는 점에 주목한다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c} \\hline \\hline \\multicolumn{1}{c|}{**Training Tokens**} & \\multicolumn{3}{c|}{**Validation Loss**} & \\multicolumn{2}{c}{**LM Eval. Acc.**} \\\\  & \\(\\mathcal{D}_{\\text{b}}\\)**Pile** & \\(\\mathcal{D}_{\\text{l}}\\)**German/SP** & **AVG** & **English** & **HellaSwag-DE** \\\\ \\hline\n' +
      '300B Pile & 2.17 & 2.70 & 2.44 & 33.95 & 27.09\\\\\n' +
      '300B SP & 2.51 & 2.53 & 2.52 & 34.11 & 27.03\\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP & 2.44 & 2.50 & 2.47 & 34.93 & 27.43 \\\\\\row\n' +
      '300B Pile \\(\\rightarrow\\) 300B SP (5\\% Replay) & 2.23 & 2.51 & **2.37** & 35.14 & 27.09 \\\\\\row\n' +
      '600B Pile \\(\\cup\\) SP & 2.17 & 2.53 & **2.35** & 34.30 & 27.36 \\\\ \\hline\n' +
      '300B Pile & 2.17 & 2.70 & 2.44 & 33.95 & 27.09\\\\\n' +
      '200B German & 3.97 & 1.17 & 2.57 & 27.74 & 29.53\\\\\n' +
      '300B Pile \\(\\rightarrow\\) 300B German & 3.56 & 1.11 & 2.34 & 29.20 & 31.23 \\\\\\row\n' +
      '300B Pile \\(\\rightarrow\\) 200B German (25\\% Replay) & 2.33 & 1.16 & **1.75** & 32.48 & 31.04 \\\\\\arrow\\\n' +
      '500B Pile \\(\\cup\\) German & 2.26 & 1.25 & **1.75** & 32.43 & 30.45 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: **사전 훈련된 영어 전용 및 영어-독일 모델의 최종 손실. 모든 모델에는 405M 매개변수가 있습니다. 손실은 10회 반복 간격으로 샘플링된 훈련의 마지막 100회 반복에 걸쳐 평균화된다. 이러한 측정에 대한 표준 오차는 계산되었지만 모든 모델에 대해 \\(<0.001\\)인 것으로 보고되지 않았다. 우리는 스타커 분포 이동에도 LR 워밍업과 5% 리플레이의 조합이 파일\\(\\cup\\) 독일 모델의 평균 성능에 접근하는 데 도움이 된다는 것을 관찰한다.**\n' +
      '\n' +
      'Zero-shot과 Few-shot 결과를 이용한 인기 LM 벤치마크에 대한 최종 성능 평가\n' +
      '\n' +
      '최종 정확도는 사전 훈련 목표에 대한 좋은 성능을 제공하지만 LLM의 능력은 일반적으로 평가 과제에 대한 성능으로 판단된다. 우리가 기본 모델을 사용한다는 경고와 함께, 그것은 우리의 모델이 어떤 식으로든 인간의 선호에 대해 지시 조정, 미세 조정 또는 적응되지 않았다는 점에서, 우리는 이 섹션에서 인기 있는 벤치마크에 대한 평가를 제시한다. 또한, 독일어 훈련 모델에 대한 정성 평가도 제공합니다. 선택된 평가 과제에 대한 보다 상세한 설명은 독자에게 본 원고 5.4절과 부록 A.5절을 참조한다.\n' +
      '\n' +
      '표 3은 우리의 영어 평가 과제에 대한 각 모델의 평균 정확도와 독일 헬라 스웨그 평가 과제에 대한 정규화된 정확도를 보고한다. 우리는 독일 평균 평가 점수가 거의 무작위 확률 정확도를 가진 평가로 인해 유익하지 않기 때문에 보고하지 않는다(표 11 참조). 우리는 영어 모델이 영어 평가에서 독일 모델을 지속적으로 능가하는 것을 관찰한다. 그러나 25%의 리플레이 독일 모델과 함께 사용된 강력한 리플레이는 이러한 격차를 줄이는 데 도움이 된다. 영어 모델들의 영어 평가 성능은 가장 높은 값과 가장 낮은 값 사이에서 1.19의 범위로 매우 유사하다. 우리는 이 크기의 기본 모델에 대한 평가 과정에서 상당한 노이즈가 있다고 의심하고 그 차이가 크지 않을 가능성이 있다고 믿는다. 즉, LR 재-온화, LR 재-감소 및 재생이 연속적으로 사전 훈련된 모델은 \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\) 모델에서 개선된다. 영어 평가 과제에 대해 독일어 훈련 모델을 평가할 때, 우리는 더 많은 리플레이를 사용하는 모델에 대해 일관된 개선을 본다. 우리는 LR 재-온화, LR 재-감쇠 및 재생으로 훈련된 모델이 \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\) 모델에서 개선됨을 다시 한 번 주목한다. 독일 헬라 스웨그 결과로 돌아가서 우리는 독일 모델이 일관되게 영국 모델을 능가한다는 것을 관찰한다. 독일어 훈련 모델 중 지속적으로 훈련된 모델은 연합 훈련된 모델과 독일어 전용으로 훈련된 모델을 능가한다.\n' +
      '\n' +
      'HellaSwag(평균적으로 영어 모델과 동일)를 제외한 모든 독일 평가 과제에 대한 독일 모델의 저조한 성과를 감안할 때 모델 세대에 대한 짧은 질적 연구를 수행하여 독일어에 대한 이해도를 추가로 조사했다. 부록의 A.4 절에서 우리는 독일어의 다양한 특수성을 담고 있는 5개의 독일어 프롬프트를 선택한다(부록의 탭 8 참조). 그런 다음 독일 커먼 크롤을 훈련시킨 모델 각각에 대해 고정된 토큰 길이 응답을 생성한다. 기준선으로서 우리는 또한 파일에서만 훈련된 모델을 평가한다. 작은 모델 규모에서 세대의 품질이 좋지 않음에도 불구하고, 체계적으로 주제를 벗어난 경향이 있는 파일 기준선과 비교할 때 독일 커먼 크롤로 훈련된 모델에서 독일어 출력의 생성 품질이 관찰 가능한 개선이 있음을 발견했다. 이는 독일어 훈련 모델이 언어에 대해 학습한 반면, 평가 과제는 405M 매개변수 척도에서 선택하기 너무 어렵다는 것을 시사한다. 또 다른 이유는 독일 데이터 세트가 고려된 영어 데이터 세트보다 작고 이 작업에 사용되는 보다 정교한 영어 데이터 세트와 달리 웹 스크래핑된 데이터만 포함하고 있기 때문이다.\n' +
      '\n' +
      '요약하면, 약한 분포 이동과 강한 분포 이동 모두에 대해 LR 재-온화, LR 재-감소 및 재생의 단순하고 확장 가능한 조합을 활용하여 \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\)으로 훈련된 모델에 대한 경쟁 성능을 달성할 수 있다. 이는 최종 유효성 검사 손실 및 언어 모델 평가 점수에 해당하며, 간단한 기술의 강력한 조합이 기존 지식과 거의 타협하지 않고 새로운 지식을 언어 모델에 장착할 수 있음을 보여준다._\n' +
      '\n' +
      '### 모델 척도에 따른 연속 사전 훈련 최종 성능\n' +
      '\n' +
      '이 하위 섹션에서는 지속적인 사전 훈련의 최종 성능에 대한 매개변수 수를 크기만큼 증가시키는 효과를 설정한다. 이를 위해 약 shift_(Pile \\(\\rightarrow\\) SlimPajama)와 강한 shift_(Pile \\(\\rightarrow\\) German) 설정에서 405M과 10B 매개변수 모델 크기에서 두 가지 사전 훈련된 모델을 여러 기준선과 비교한다.\n' +
      '\n' +
      'LR 재-워밍 및 재-디케이팅과 리플레이를 결합하는 성능을 줄이기 위해, 우리는 독점적으로 재-워밍 및 재-디케이팅하는 하나의 모델과 두 기술을 모두 결합하는 다른 모델을 훈련하기로 선택한다. 약 분포 이동에 대한 (Sec. 6.2)의 결과가 주어지면 두 모델 척도에 대해 5% 리플레이를 선택한다. 두 모델 모두 사전훈련(\\(3\\cdot 10^{-4}\\))의 \\(\\eta_{max}\\)으로 재가열하고, 연속적 사전훈련이 끝날 때까지 \\(\\eta_{min}\\)에 도달하도록 코사인 어닐링 세트를 사용하여 재감쇠한다. 더 많은 하이퍼파라미터가 부록의 표 13에 보고되어 있다.\n' +
      '\n' +
      '기준선 또한 여러 기준선을 훈련합니다. 두 개의 기준선은 각각 \\(\\mathcal{D}_{0}\\)과 \\(\\mathcal{D}_{1}\\)으로 훈련되고 세 번째 기준선은 \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\)으로 훈련된다. 고가의 완전 재학습을 나타내기 때문에, 본 논문에서는 \\(\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}\\)으로 훈련된 모델을 성능 상한으로 간주한다. 개별 데이터 세트에 대해 훈련된 기준선은 연속 사전 훈련에 대한 계산적 등가 대안(예: 연속 사전 훈련 대신 \\(\\mathcal{D}_{1}\\)에 대한 무작위 초기화로부터 모델을 훈련하도록 선택할 수 있음)으로 볼 수 있다.\n' +
      '\n' +
      '손실 평가를 통한 최종 성능 평가\n' +
      '\n' +
      '그림 6은 405M 및 10B 모델에 대한 지속적인 사전 훈련 동안 검증 손실을 보고하는 반면, 표 4는 각 모델에 대한 평균(마지막 100회 반복 동안) 최종 손실 값을 보고한다. 예상대로 모든 기준선과 지속적으로 사전 훈련된 모델이 매개변수 수가 증가함에 따라 두 데이터 세트에서 당혹감을 지속적으로 개선한다는 것을 관찰한다. 405M 모델의 경우, 각 데이터 세트에서 개별적으로 훈련된 기준선에 대해 동일한 검증 손실을 달성한다는 것을 알 수 있다. 대조적으로, \\(\\text{Pile}\\cup\\text{SP}\\)으로 훈련된 10B 파라미터 모델은 각각 개별적으로 훈련된 모델보다 우수하다. 우리는 이것이 더 많은 용량을 가진 더 큰 모델로 인해 발생하므로 더 긴 시간 동안 더 높은 속도로 학습할 수 있다고 가정한다. 우리는 슬림파자마에서 사전 훈련을 계속할 때 5% 파일 데이터를 다시 재생하면 10B 및 405M 매개변수 모델에 대해 파일 검증에 대한 망각이 각각 0.19 및 0.21 감소한다는 것을 관찰한다. 두 모델 간의 매개변수 크기 차에도 불구하고 재생에서 망각 감소의 무시할 수 있는 차이는 모델 척도가 망각-재생에 제한된 부정적인 영향을 미친다는 것을 시사한다.\n' +
      '\n' +
      '그림 6: 10B(상단) 및 405M(하단) 매개변수 모델의 연속 사전 훈련 중 **검증 손실.** 각 모델 척도에서 3개의 기준선과 2개의 연속 사전 훈련 모델을 제공했다. 기준선(밝은 파란색, 짙은 파란색 및 적갈색)은 슬림파자마의 300B 토큰, 파일의 300B 토큰 및 두 데이터 세트의 결합(600B 토큰)에 대한 무작위 초기화로부터 훈련된다. 연속적으로 미리 훈련된 모델(검은색과 보라색)은 파일(짙은 파란색 곡선)의 300B 토큰에서 미리 훈련된 체크포인트에서 시작하여 각각 0%와 5%의 리플레이를 사용한다. 우리는 두 모델 크기 모두에서 LR 재온화, LR 재붕괴 및 소량의 재생의 조합이 망각과 적응 사이의 균형을 맞추는 데 도움이 된다는 것을 관찰한다. 중요하게도, 우리는 리플레이의 사용이 0% 리플레이를 사용하는 모델에 비해 다운스트림 성능에 최소한으로 영향을 미친다는 점에 주목한다(그림 (b)와 (d)에서 검은색과 보라색 곡선이 겹친다).\n' +
      '\n' +
      '이것은 더 큰 모델들이 기본적으로 더 적게 잊어버리기 때문이라고 믿으세요. 실제로, 미리 훈련된 파일 체크포인트에서 다시 재생하지 않고 훈련된 모델은 10B 및 405M에 대해 각각 0.23 및 0.27 Nats의 파일 당혹감을 잊는다. 차이는 작지만 이는 더 큰 모델이 더 적게 잊어버린다는 것을 시사하여 우리의 가설을 확인시켜준다. 모델의 평균 최종 검증 손실과 두 데이터 세트의 조합에 대해 훈련된 5% 리플레이 및 기준선을 비교할 때 두 모델 크기에 대해 0.02의 차이만 있음을 알 수 있다. 이는 두 모델 척도에서 약하지만 현실적인 분포 이동에 대해 지속적인 사전 훈련이 값비싼 재훈련 기준선과 유사한 성능을 달성할 수 있음을 보여준다.\n' +
      '\n' +
      'Zero-shot과 Few-shot 결과를 이용한 인기 LM 벤치마크에 대한 최종 성능 평가\n' +
      '\n' +
      '최종 정확도는 사전 훈련 목표에 대한 좋은 성능을 제공하지만 LLMs 능력은 일반적으로 평가 과제에 대한 성능으로 판단된다. 우리가 기본 모델을 사용한다는 경고와 함께, 그것은 우리의 모델이 어떤 식으로든 인간의 선호에 대해 지시 조정, 미세 조정 또는 적응되지 않았다는 점에서, 우리는 이 섹션에서 인기 있는 벤치마크에 대한 평가를 제시한다. 선택된 평가 과제에 대한 보다 상세한 설명은 독자에게 본 원고 5.4절과 부록 A.5절을 참조한다.\n' +
      '\n' +
      '테이블 5는 영어 전용 연속 사전 훈련된 LLM에 대한 영어 LM 평가 결과를 보고한다. HellaSwag에 대해 정규화된 정확도가 보고되며 NaturalQuestions 및 TriviaQA에 대해 정확한 일치(EM)가 보고된다. 다른 모든 작업은 정상화되지 않은 정확도를 보고합니다. 예상대로, 우리는 더 큰(10B) 모델이 더 작은 모델보다 더 강한 성능을 달성하고 더 많은 토큰으로 훈련된 모델이 더 적은 토큰으로 훈련된 모델보다 항상 더 나은 성능을 달성한다는 것을 관찰한다. 두 모델 척도에 대해, 우리는 모델이 평균 정확도 측면에서 두 데이터 세트의 조합에 대해 훈련된 모델의 성능을 학습률 재온화 및 5% 재생 접근법(10B) 또는 초과(405M)의 조합을 사용하여 지속적으로 사전 훈련되는 것을 관찰한다. 조합 훈련된 모델과 계속 사전 훈련된 모델을 비교할 때\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|c c c} \\hline \\hline \\multirow{2}{*}{**Model Size**} & \\multicolumn{3}{c|}{**Validation Loss**} \\\\  & \\multicolumn{1}{c}{**Training Tokens**} & \\(\\mathcal{D}_{\\text{t}}\\)**Pile** & \\(\\mathcal{D}_{\\text{t}}\\)**SimPajama** & **AVG** \\\\ \\hline \\multirow{6}{*}{10B} & 300B Pile & 1.75 & 2.24 & 1.99 \\\\  & 300B SP & 2.08 & 2.05 & 2.07 \\\\  & 300B Pile + 300B SP & 1.98 & 2.00 & 1.99 \\\\  & 300B Pile + 300B SP (5\\% Replay) & 1.79 & 2.00 & **1.89** \\\\  & 600B Pile \\(\\cup\\) SP & 1.72 & 2.02 & **1.87** \\\\ \\hline \\multirow{6}{*}{405M} & 300B Pile & 2.17 & 2.70 & 2.44 \\\\  & 300B SP & 2.51 & 2.53 & 2.52 \\\\ \\cline{1-1}  & 300B Pile + 300B SP & 2.44 & 2.50 & 2.47 \\\\ \\cline{1-1}  & 300B Pile + 300B SP (5\\% Replay) & 2.23 & 2.51 & **2.37** \\\\ \\cline{1-1}  & 600B Pile \\(\\cup\\) SP & 2.17 & 2.53 & **2.35** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **10B 및 405M 파라미터 모델의 최종 손실.** 손실은 10회 반복의 간격으로 샘플링된 트레이닝의 마지막 100회 반복에 걸쳐 평균화된다. 이러한 측정에 대한 표준 오차는 계산되었지만 모든 모델에 대해 \\(<0.001\\)인 것으로 보고되지 않았다. 우리는 두 모델 척도에서 5% 리플레이와 결합된 학습률 재온화가 관절 훈련의 평균 손실 값에 접근한다는 것을 관찰한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|c c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Model Size} & Training Tokens & HellaSwag & ARC-C & ARC-C & BookQ & MolQA & MMLU & O0B4 & PIGA & WC & TQA1 & TQA2 & NO & TQA & AVG \\\\ \\hline \\multirow{6}{*}{10B} & 300B Pile & 68.46 & 38.81 & 02.09 & 68.83 & 27.34 & 27.38 & 27.20 & 78.62 & 62.51 & 20.44 & 33.86 & 66.53 & 49.92 & 43.45 \\\\  & 300B SP & 70.38 & 36.77 & 71.93 & 68.01 & 24.76 & 27.42 & 28.20 & 79.69 & 60.04different tasks, we observe for the 10B parameter models that the 5% replay model and union-trained model exchange best performance on different tasks with notable differences being OpenBookQA in favor of the replay model and MMLU in favor of the union model. For the 405M parameter models, the 5% replay model and union-trained model exchange best performance on different tasks with no notable differences. At both model scales, the replay model improves over the model only using re-warming though differences are small and may be attributable to noise.\n' +
      '\n' +
      '요약하면, LR 재배열, LR 재배열 및 재생의 조합으로 지속적으로 사전 훈련된 모델이 개별 데이터 세트의 무작위 초기화에서 훈련된 기준선의 성능(예: w.r.t. 최종 검증 손실 및 평가 정확도)을 초과하고 값비싼 재 훈련 기준선(두 데이터 세트의 조합에서 훈련됨)과 유사한 평가 성능을 달성한다는 것을 발견했다. 이러한 결과는 \\(10\\)B 매개변수 척도에서 지속적인 사전 훈련 유지의 이점이 있음을 보여주며, 이는 더 많은 매개변수(예: \\(100\\)B+ 매개변수의 경우)를 갖는 모델의 경우에도 마찬가지일 수 있음을 시사한다._\n' +
      '\n' +
      '##7 재온화 병리의 이해와 순환\n' +
      '\n' +
      '이 섹션에서는 LR 재온난화가 원치 않는 망각을 유발한다는 것을 발견하고 이를 우회하기 위한 유망한 방법으로 무한 학습률 일정을 소개하고 이러한 일정을 문헌의 기준선과 비교한다.\n' +
      '\n' +
      '같은 데이터에 대한### 재온난화\n' +
      '\n' +
      '섹션 6.1에서 우리는 새로운 데이터에 대한 사전 훈련을 계속하면 초기에 과거 데이터에 대한 손실이 빠르게 증가하여 재생의 사용을 동기화하는 것을 보았다. 손실의 증가는 특히 큰 \\(\\eta_{max}\\) 값에서 더 두드러졌다. 손실 증가에 대한 한 가지 가설은 대부분 사전 훈련 데이터 세트 간의 분포 이동과 관련 부정적인 전달에 기인한다는 것이다. 이 가설을 평가하기 위해 분포 이동이 없는 설정에서 300B 토큰에 대해 다시 따뜻하고 다시 감쇠한다. 즉, 그림 1의 실험에서와 유사한 방법론을 따른다. 3) 그러나 파일에서 \\(\\mathcal{D}_{1}\\)으로 사전 훈련을 계속한다.\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 7은 분포 이동과 무관하게 학습률을 다시 따뜻하게 하는 것이 이전에 그림 1에서 볼 수 있는 손실 증가의 중요한 원인으로 보인다. 3번은 사전 훈련을 계속하기 시작할 때, 동일한 분포로 훈련하면서 학습률을 다시 따뜻하게 할 때 당혹감이 증가하는 것을 알 수 있다. 예를 들어, 재보온은 파일 사전 훈련을 계속할 때 초기 값에 비해 0.1의 파일 검증 손실의 피크 증가로 이어지며, 이는 그림 3과 같이 슬림파자마에서 사전 훈련을 계속할 때 동일한 학습률 스케줄로 0.35의 파일 검증 손실 증가와 대조될 수 있다. 재보온이 높을수록 이러한 효과가 더 두드러진다는 점은 주목할 만하다.\n' +
      '\n' +
      '도 7: 파일(a) 및 슬림파자마(b)에서 사전 훈련을 계속할 때 **파일 유효성 검사 손실. 각 곡선은 파일의 300B 토큰에서 미리 훈련된 동일한 체크포인트에서 시작되지만 다른 최대 학습 속도로 훈련된다. 사전 훈련의 최소 학습률에서 학습률을 다시 증가시키는 모든 모델(예: 상수를 제외한 모든 모델)이 손실의 증가를 보는 것을 관찰한다.**\n' +
      '\n' +
      '[\\(\\eta_{max}=6\\cdot 10^{-4}\\] 곡선에서 볼 수 있듯이, 파일에서 사전 훈련을 계속할 때(0.2의 피크 손실 증가)와 슬림파자마에서 사전 훈련을 계속할 때(0.45의 피크 손실 증가)가 있다.\n' +
      '\n' +
      '특히, 재워밍 후 모델은 동일한 데이터셋에 대한 학습 시에도 학습률 재워밍으로 인한 성능 타격에서 빠르게 회복되지 못한다. 이는 지속적인 사전 훈련의 효율성을 향상시키기 위해 재보온이 필요한 학습률 일정에 대한 대안을 찾는 동기를 부여한다.\n' +
      '\n' +
      '### 무한 학습률 스케쥴링\n' +
      '\n' +
      '이 하위 섹션에서는 본질적으로 재온난화가 필요하지 않을 수 있는 학습 속도 일정의 사용을 조사한다. 동기는 두 가지입니다. 한편으로 코사인 붕괴 일정은 우리가 사전 훈련하고 싶은 토큰의 총 수를 미리 알아야 한다. 이것은 수렴된 체크포인트를 계속 사전 훈련하는 능력을 제한한다. 반면에, 우리는 이전 섹션에서 초기에 작은 학습 속도로 끝나는 코사인 감쇠 일정으로 사전 훈련된 모델을 계속 사전 훈련할 때 새로운 데이터 세트에 가장 잘 적응하기 위해 최소 값에서 학습 속도를 다시 따뜻하게 하는 것이 필요하다는 것을 보았다. 그러나 이전 하위 섹션에서 볼 수 있듯이 학습률을 다시 따뜻하게 하는 것이 망각을 악화시킬 수 있음을 관찰한다.\n' +
      '\n' +
      '따라서, 우리는 모든 새로운 작업에 걸쳐 학습률을 일정한 값으로 유지하는 "무한 학습률 스케줄"(Zhai et al., 2022)을 탐구한다. 이것은 새로운 과제에 대한 학습을 다시 따뜻하게 하는 것을 피함으로써 잊는 것을 방지하는 데 도움이 될 수 있습니다. 추가적으로, 이 스케줄은 토큰들의 총 수와 무관하여, 각각의 새로운 데이터세트에 대해 순환적으로 코사인 감쇠 스케줄을 반복하는 것에 비해 지속적인 학습 셋업들에 더 적합하다. 우리가 본 바와 같이, 높은 일정한 학습 속도 또한 차선책이기 때문에, 우리는 제한된 양의 토큰에 대해 사전 트레이닝이 끝날 때 학습 속도의 빠른 어닐링을 수행하도록 선택한다. 이를 통해 학습률을 재감쇠하는 성능 우위를 회복하는 한편, 사전 훈련을 계속할 때 사전 어닐링 체크포인트를 사용할 수 있기를 기대한다.\n' +
      '\n' +
      '고려된 무한 학습 속도 스케줄은 4개의 단계를 갖는다:\n' +
      '\n' +
      '1. **Linear warm-up phase** - 이전과 같이, 학습률은 초기에 \\(\\eta_{max}\\)의 최대값 \\(T_{warmup}\\)에 걸쳐 증가하거나, 또는 동등하게 timestep \\(t_{cd}=T_{warmup}\\)까지 증가한다. 학습률은 1회(1차 과제 수행 중)만 워밍업을 거치며, 추후 과제는 다시 워밍업을 하지 않아도 된다.\n' +
      '2. **Cooldown phase** - 이 단계 동안 학습률은 timestep \\(t_{cd}\\)에서 \\(t_{const}=t_{cd}+T_{cd}\\)까지의 시간에 걸쳐 일부 decay 함수 \\(f_{cd}\\)에 따라 일정한 값 \\(\\eta_{const}\\)으로 점진적으로 decay되는 cooldown 단계를 거치게 된다. 이 단계 또한 첫 번째 작업 동안 한 번만 발생합니다.\n' +
      '3. **Constant phase** - 학습률은 timestep \\(t_{const}\\)에서 \\(t_{ann}=t_{const}+T_{const}\\)까지의 \\(T_{const}\\) 시간 동안 모든 미래 태스크에 대해 일정하게 유지된다. 이 단계가 끝날 때 얻은 체크포인트는 새로운 데이터 세트에서 사전 훈련을 계속할 때부터 다시 시작해야 한다.\n' +
      '4. **Annealing phase** - 학습률을 timestep \\(t_{ann}\\)에서 \\(t_{end}=t_{ann}+T_{ann}\\)까지 \\(T_{ann}\\)에 걸쳐 \\(\\eta_{min}\\)의 작은 값으로 어닐링하여 모델을 전개하기 전에 수렴으로 훈련시키는 것을 돕는다.\n' +
      '\n' +
      '따라서, 여기서 고려되는 무한 학습 속도 스케줄들은 다음과 같이 작성될 수 있다:\n' +
      '\n' +
      '[\\eta_{t}=\\left\\{\\begin{array}{llll}&\\eta_{max}\\cdot\\frac{t}{T_{warmup}}&t\\in[0,t_{cd}]&(\\textit{warm-up})\\\\&f_{cd}(t)&t\\in(t_{cd},t_{const}]&(\\textit{cooldown})\\\\\\&\\eta_{const}&t\\in(t_{const},t_{ann}]&(\\textit{const}\\cdot\\left(\\frac{\\eta_{min}}{\\eta_{const}}}\\frac{t-t_{ conn}}}{t_{end}}}}}\\frac{t-t_{cn}}}{t_{cn}}}{t_{cn}}}{t_{cn}}}{t_{cn}}}{t_{cn}}\n' +
      '\n' +
      '본 연구에서는 cooldown phase의 decay \\(f_{cd}\\):1. Cosine decay \\[f_{cd}(t)=\\eta_{const}+\\frac{\\eta_{max}-\\eta_{const}}{2}\\cdot\\left(1+\\cos\\left(\\pi\\left(\\frac{t-t_{cd}}{t_{const}-t_{cd}}\\right)\\right)\\right)에 대한 두 가지 함수를 고려한다. (3)\n' +
      '2. 역제곱근 감쇠 \\[f_{cd}(t)=\\eta_{max}+\\frac{\\eta_{const}-\\eta_{max}{h(1)}\\cdot h\\left(\\frac{t-t_{cd}}{t_{const}-t_{cd}}\\right)\\] (4) 여기서 \\[h(x)=\\frac{1}{\\sqrt{1+\\alpha x}}-1\\은\n' +
      '\n' +
      '(\\(\\alpha\\)는 역제곱근 붕괴의 경사를 조절하는 역할을 한다. 우리는 구간 \\((t_{cd},t_{const}]\\)에 적응하기 위해 역제곱근 감쇠를 이동 및 신장한다.\n' +
      '\n' +
      '세 가지 다른 일정이 그림 1에 나와 있다. 8(b).\n' +
      '\n' +
      '우리는 이제 무한 학습 속도 스케줄을 코사인 감쇠 스케줄과 비교한다. 먼저 LLM 사전 훈련 일정의 타당성을 평가하기 위해 간단한 단일 데이터 세트 사전 훈련 설정을 탐색한다. 그 후, 우리는 시프트 설정이 없는 _3 데이터 세트에서 그것의 이점을 탐구한다.\n' +
      '\n' +
      '코사인 감쇠와 무한 스케줄의 변화 비교\n' +
      '\n' +
      '여기에서 우리는 공통 단일 데이터 세트 사전 훈련 설정에서 코사인 감쇠 일정과 무한 학습 속도 일정을 비교한다. 이 실험의 목적은 무한 학습 속도 스케줄이 기존의 코사인 감쇠 스케줄로 훈련된 모델뿐만 아니라 수행하는 모델을 초래할 수 있는지 테스트하는 것이다.\n' +
      '\n' +
      '모델들은 랜덤 초기화로부터 슬림파자마의 300B 토큰들에 대해 미리 트레이닝된다. 그림 8은 학습률 스케줄이 다른 슬림파자마에서 학습된 405M 파라미터 모델 3개의 학습 곡선을 보여준다. 우리는 모든 방법이 유사한 최종 검증 손실에 도달한다는 것을 관찰하며, 무한 학습 속도 스케줄이 사전 훈련의 일반적인 경우에도 사용될 수 있음을 보여준다. 이러한 스케줄들은 또한 사전-훈련을 마무리하기로 결정할 때 손실을 효율적으로 개선하기 위해 일정한 단계에서 언제든지 어닐링을 시작할 수 있고, 사전-어닐링 체크포인트가 로딩되어 사전-훈련을 계속할 수 있다는 이점을 갖는다.\n' +
      '\n' +
      '그림 8: **무한 학습 속도는 v.s. 코사인 붕괴를 스케줄링한다. 슬림파자마의 300B 토큰에 대해 405M 파라미터 모델을 학습하고, 랜덤 초기화로부터 2개의 새로운 스케줄인 _Cosine Inf_와 _InvSgrt Inf_를 사용하여 이들을 코사인 붕괴 베이스라인과 비교한다. _Cosine Inf_와 _InvSgrt Inf__ Cosine Inf_ 및 _InvSgrt Inf_는 고정된 상수 LR 값으로 먼저 감쇠하고 그 후 급격한 최종 감쇠까지 일정하게 유지된다. 따라서 이러한 일정은 다시 따뜻해지지 않고 한 사전 훈련 단계와 다음 단계 사이에서 원활하게 전환될 수 있다는 장점이 있다. 모든 방법이 유사한 최종 검증 손실에 도달한다는 것을 발견하여 코사인 붕괴가 강력한 성능을 위한 전제 조건이 아님을 보여준다.**\n' +
      '\n' +
      '### 무한 학습 속도 스케쥴: 무한 미래 업데이트로의 스케일링\n' +
      '\n' +
      '우리는 이제 연속 학습 설정에서 여러 개의 새로운 데이터 세트가 보일 때 무한 학습 속도 스케줄의 역할을 탐구한다. 모델들은 SlimPajama의 3개의 IID 100B 서브세트들(예를 들어, 우리의 _3 데이터세트 no shift_ setting; Sec 5.2 참조) 상에서 상이한 학습 레이트 스케줄들을 갖는 랜덤 초기화로부터 트레이닝된다. 우리는 이러한 예비 실험에서 무 교대 설정에 초점을 맞추고 약하고 강한 교대 사례를 향후 작업에 맡긴다. 이 작업은 동일한 분포로부터 많은 양의 데이터가 시간 증분으로 수신되는 설정을 시뮬레이션하고, 그들에 대해 모델을 사전 트레이닝(예를 들어, 최신 웹-스크래프에서 모델을 사전 트레이닝하는 것을 계속)하기를 원한다. 이전의 최적화 상태들을 이용할 수 없는 상황들에 우리의 결과들을 적용가능하도록 하기 위해, 우리는 데이터세트 경계들에 걸쳐 최적화 상태들을 유지하지 않는다. 도. 9개는 405M 매개변수 모델에 대한 훈련 곡선을 보고한다.\n' +
      '\n' +
      '모든 스케줄이 비교적 유사하게 수행되는 것을 관찰하지만, 두 무한 스케줄은 각 분할에 대해 일정한 학습 속도 단계 동안 언제든지 어닐링을 시작할 수 있는 장점이 있는 반면, 반복되는 코사인 디케이는 토큰의 수를 미리 알아야 한다. 또한 무한 LR 일람표에 대해 데이터 세트 경계를 무시할 수 있는 망각을 볼 수 있다. 최적화 상태들의 재초기화로 인해 손실이 초기에 급격히 증가하는 반면, 무한 스케줄 모델들은 이로부터 즉시 복구된다.\n' +
      '\n' +
      '향후 연구에서는 유통 이동이 있는 지속적인 학습 설정에서 무한 학습률 스케줄의 영향을 연구하고, 학습률의 일정한 위상이 긴 대량의 토큰에 대한 훈련의 안정성을 조사하는 것이 흥미로울 것이다.\n' +
      '\n' +
      '요약하면, 우리는 재온난화가 성능에 영향을 미치지만 코사인 붕괴 일정에 대한 대안이 이러한 문제를 우회할 수 있음을 보았다. 이 작업에서 탐색된 대안은 종료를 제어하거나 사전 훈련을 계속하는 직관적인 방법을 제공한다는 점에서 유망하다._\n' +
      '\n' +
      '## 8 Limitations\n' +
      '\n' +
      'LLM에 대한 지속적인 사전 훈련에 대한 철저한 경험적 평가를 수행했지만 작업에는 몇 가지 한계가 있다. 특별한 순서는 1) 두 가지 모델 크기(405M 및 10B)만 연구했고, 2) 독일 커먼 크롤 스크랩(라이팔라 등, 2022)에서 생성된 독일 훈련 및 검증 데이터 세트 간에 중복제거를 실행하지 않았으며, 3) 주로 두 후속 작업 간의 전환을 연구했으며, 4) 여러 종자에 대한 실험을 실행하지 않았으며, 5) 무한 학습 속도 스케줄에 대한 실험은 분포 이동 없이 405M 척도로 제한되었다. 보다 명시적으로 첫 번째 한계는 우리가 고려하는 모델 척도의 수이다. 405M 및 10B 매개변수 모델(대부분의 작업보다 훨씬 큰)을 고려하지만 계산으로 인해 연구를 다른 규모까지 확장할 수 없었다.\n' +
      '\n' +
      '그림 9: **무한 학습 속도 스케줄은 SP의 3개의 IID 100B 토큰 서브세트에 대해 평가된다. 실험은 동일한 분포의 새로운 데이터가 시간이 지남에 따라 도착하고 실무자가 새로운 데이터에 대한 모델을 업데이트하기를 원하는 설정을 시뮬레이션한다. 모델은 무작위 초기화로부터 학습된다. 그림 (b)에서 검은색과 보라색 일정은 \\(\\sim 80\\)B 토큰.**제한(예: 100B 매개변수 척도) 후에 겹친다는 점에 주목한다. 우리 작업의 두 번째 한계는 독일 검증 세트가 독일 훈련 데이터에서 중복 제거되지 않았다는 것이다. 교육 및 검증을 위해 뚜렷한 파편을 조심했지만 둘 사이에 약간의 오염이 있을 수 있다. 그러나 모든 기준선이 동일한 데이터 세트에 액세스할 수 있다는 점을 감안할 때 결과는 여전히 유효하다고 생각한다. 세 번째 한계는 두 가지 이상의 후속 작업에 대해 모델을 업데이트하는 실험을 실행하지 않았다는 것이다. 이를 연구하는 것이 중요하다고 생각하지만, 우리의 목표는 많은 수의 데이터 세트를 사용하기보다는 다양한 배포 이동과 대규모 데이터 세트 간의 전환을 연구하는 데 초점을 맞추는 것이었다. 네 번째 한계는 높은 계산 비용으로 인해 여러 종자에 대한 실험을 실행하지 않았다는 것인데, 이는 일부 결과에 확률적 요소가 있을 가능성이 있음을 의미한다. 즉, LLM은 큰 배치 크기(2M+ 토큰)로 훈련되므로 기울기 추정치의 분산이 거의 없다. 각 데이터 세트의 샘플이 모든 경우에 동일한 순서로 처리된다는 사실과 함께, 우리는 우리의 결과가 시드에 의해 지시되는 무작위 초기화의 변화에 상대적으로 안정적이어야 한다고 믿는다. 다섯 번째 한계는 충분한 토큰에 걸쳐, 모든 후속 데이터 세트에 대한 학습이 차선으로 판명된 일정한 학습 속도를 사용하는 것과 동등할 수 있기 때문에, 무한 스케줄이 워밍업 및 쿨다운의 단일 단계만을 갖는 것으로 인해 차선이 될 수 있다는 것이다(도 3 참조). 그러는 동안 도 9는 어닐링 단계가 동일한 데이터세트의 IID 분할의 경우, 이것이 더 많은 토큰을 보유할 것인지, 또는 상이한 데이터세트가 분포 이동을 갖는 경우에 이 아최적성으로부터 회복하는 것을 돕는다는 것을 보여주었다. 따라서 이러한 무한한 일정을 추가로 테스트하려면 분포 이동과 더 큰 규모의 모델 및 데이터 세트를 포함하는 실험이 중요할 것이다. 마지막으로, 더 큰 규모에서 탐색하기 위한 또 다른 중요한 고려 사항은 이러한 스케줄을 갖는 사전 훈련의 안정성이다(특히, \\(\\mu P\\)(Yang et al., 2022).\n' +
      '\n' +
      '## 9 Conclusion\n' +
      '\n' +
      'LLM의 지속적인 사전 훈련의 맥락에서, 우리는 학습 속도 재온화 및 재붕괴가 적응에 중요하다는 것을 보았고, 적응에 거의 비용이 들지 않는 것처럼 보이는 이 환경에서 다시 재생하면 망각이 쉽게 완화된다는 것을 발견했다. 적응을 강화하고 망각을 동시에 완화할 수 있는 강력한 능력을 감안할 때, 우리는 LLM을 규모에서 지속적으로 사전 훈련하기 위해 LR 재온화, LR 재붕괴 및 재생의 간단하고 확장 가능한 조합을 제안했다. 우리는 이러한 전략이 두 가지 분포 이동(약점 및 강점)과 두 가지 모델 척도(405M & 10B)에 걸쳐 모든 데이터에서 처음부터 비용적으로 재교육하는 것과 동등하게 성능을 달성할 수 있음을 보여주었다. 추가 분석을 통해 LR 재온화의 병리를 식별하고 이전 작업에서 영감을 받아 LLM을 지속적으로 사전 훈련하기 위한 무한 학습 속도 일정을 제안했다. 초기 실험에서 우리의 일정은 LR 재온화의 필요성을 우회하면서 코사인 붕괴와 동등한 성능을 달성한다.\n' +
      '\n' +
      '우리의 연구 결과는 새로운 데이터에 LLM을 업데이트할 때 지속적인 사전 훈련이 재교육에 대한 효율적인 대안임을 보여준다. 우리의 전략을 갖춘 실무자는 새로 생성된 고품질 데이터 세트에 대해 기존 모델(Rae et al., 2021; Hoffmann et al., 2022; Touvron et al., 2023; Jiang et al., 2023; Gemma Team et al., 2024)을 효율적으로 업데이트할 수 있다. 이러한 전략은 또한 Gemma Team 등이 사용하는 것과 같은 사전 훈련 커리큘럼과 관련이 있을 수 있다(2024). 우리 커뮤니티가 계속해서 품질을 높이는 데이터 세트를 만들려는 강력한 인센티브로 인해 지속적인 사전 훈련의 필요성만 증가할 것으로 예상한다.\n' +
      '\n' +
      '후속 작업에서는 무한 학습 속도 일정, 비전 언어 모델 및 기타 텍스트 기반 생성 모델의 맥락에서 지속적인 사전 훈련, 지속적인 사전 훈련(예: 전문가 혼합 또는 블록 확장) 동안 성장하는 모델, 데이터 분포에 대한 급격한 변화를 처리하기 위해 토큰화기를 적용하는 것이 더 중요할 것이다.\n' +
      '\n' +
      '브로드웨이 광고성명\n' +
      '\n' +
      '대규모 언어 모델은 관련 데이터 세트에 대해 교육을 받은 후 매우 잘 수행할 수 있는 능력으로 인해 광범위한 산업 부문에 광범위하게 채택되었다. 또한 데이터 세트의 개선(더 나은 필터링, 지식 업데이트 등)은 LLM의 출력 품질을 높이는 데 중요했다. 따라서 조직은 상당한 양의 컴퓨팅 능력과 이에 따라 더 강력한 모델을 생성하기 위해 에너지를 소비할 것으로 예상하는 것이 합리적이다. 이 에너지의 일부는 재생 불가능한 소스에서 나올 가능성이 있다. 우리 논문에서 제시된 실험은 환경적으로 비용이 많이 들지만, 논문에서 주장한 바와 같이 사전 훈련을 계속하는 것은 모델 업데이트와 관련된 계산과 기초 모델을 유지하는 데 필요한 에너지를 크게 줄이는 유망한 방법이다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'NSERC Discovery Grant RGPIN-2021-04104 [E.B.], 캐나다 CIFAR AI Chair Program [I.R.], 캐나다 Excellence Research Chairs Program [I.R.], FRQNT Doctoral (B2X) 장학금 [B.T.], Universite de Montreal\'s Etudes Superieures et Postdoctables (A.I.), 독일 학술교류서비스(DAAD)의 IFI Program의 장학금 [M.R.]의 지원을 인정한다. 본 연구는 INCITE 2023 Program Award "Scalable Foundation Models for Transferable Generalist AI"의 일환으로 제공된 서밋 슈퍼컴퓨터의 컴퓨팅 자원 덕분에 가능했다. 이 자원들은 미국 에너지부의 과학부가 계약번호 DE-AC05-00OR22725에 따라 지원하는 오크리지 국립 연구소의 오크리지 리더십 컴퓨팅 시설에서 제공되었으며, 특히 젠스 글레이저가 서밋 슈퍼컴퓨터를 도와준 것에 대해 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Aljundi et al. (2019) LuCA Caccia, Eugene Bellovsky, Massimo Caccia, Min Lin, Laurent Charlin, and Tinne Tuytelaars. 검색에 최대 방해가 되는 온라인 연속 학습. H. Wallach, H. Larochelle, A. Beygelzimer, F. d\'Alche-Buc, E. Fox, R. Garnett(Eds.), _Advances in Neural Information Processing Systems 32_, pp. 11849-11860. Curran Associates, Inc., 2019. URL[http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-](http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-) interfered-retrieval.pdf.\n' +
      '* Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alboebidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 열린 언어 모델의 팔콘 시리즈야 CoRR_, abs/2311.16867, 2023. URL[https://doi.org/10.48550/arXiv.2311.16867](https://doi.org/10.48550/arXiv.2311.16867).\n' +
      '* Amini et al. (2019) Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: 해석 가능한 수학 단어 문제 해결을 위해 연산 기반 형식론으로 2019년.\n' +
      '* Andonian et al. (2021) Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Wang Phil, and Samuel Weinbach. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch, 8 2021. URL[https://www.github.com/eleutherai/gpt-neoX](https://www.github.com/eleutherai/gpt-neoX).\n' +
      '* Bisk 등(2019) 요나탄 비스크, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: 자연어로 물리적 상식에 대한 추론, 2019.\n' +
      '* Black et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b: An open-source autoregressive language model, 2022.\n' +
      '* Black et al. (2021)Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Wing Yin Ng, Ricky Wang, and Aditya Ramesh. 비디오 생성 모델은 월드 시뮬레이터입니다. 2024. URL[https://openai.com/research/video-generation-models-as-world-simulator](https://openai.com/research/video-generation-models-as-world-simulator)\n' +
      '* Brown et al. (2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 소수의 학습자이다. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, pp. 1877-1901, 2020. URL[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165).\n' +
      '* Buzzega et al. (2020) Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. 일반적인 지속적인 학습을 위한 어두운 경험: 강하고 간단한 기준선. Hugo Larochelle, Marc\'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin(eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL[https://proceedings.neurips.cc/paper/2020/hash/b704ea2c39778707c617f6b7ce480e9e-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/b704ea2c39778707c617f6b7ce480e9e-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/b704ea2c39778707c617f6b7ce480e9e-Abstract.html)\n' +
      '* Caccia et al. (2020) Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Caccia, Issam Laradji, Irina Rish, Alexandre Lacoste, David Vazquez, and Laurent Charlin. 온라인 빠른 적응과 지식 축적: 지속적인 학습에 대한 새로운 접근 방법. _ NeurIPS_, 2020. URL[https://arxiv.org/abs/2003.05856](https://arxiv.org/abs/2003.05856).\n' +
      '* Chen et al. (2016) Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 하위 선형 메모리 비용으로 딥 네트를 훈련합니다. _ CoRR_, abs/1604.06174, 2016. URL[http://arxiv.org/abs/1604.06174](http://arxiv.org/abs/1604.06174).\n' +
      '* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: 2019년, 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구합니다.\n' +
      '* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 질문에 답하는 걸 해결했다고 생각해? try arc, the ai2 reasoning challenge, 2018.\n' +
      '* 컴퓨터(2023) Together Computer. Redpajama: Open dataset for training large language models, 2023. URL[https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).\n' +
      '* Cossu et al. (2022) Andrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia Passaro, Vincenzo Lomonaco, and Davide Bacciu. 지속적인 사전 훈련은 언어 및 시각에서의 망각을 완화한다. 2022. URL[https://arxiv.org/abs/2205.09357](https://arxiv.org/abs/2205.09357)\n' +
      '* Douillard et al. (2023) Arthur Douillard, Qixuan Feng, Andrei A Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc\'Aurelio Ranzato, Arthur Szlam, and Jiajun Shen. Diloco: 언어 모델의 분산 저통신 훈련. _ arXiv preprint arXiv:2311.08105_, 2023.\n' +
      '*프랑스어(1999) Robert M. 프랑스어 연결주의 네트워크에서의 재앙적 망각 Cognitive Sciences_, 3(4):128-135, 1999. ISSN 13646613. doi: 10.1016/S1364-6613(99)01294-2. URL[https://www.sciencedirect.com/science/article/abs/pii/S1364661399012942](https://www.sciencedirect.com/science/article/abs/pii/S1364661399012942)\n' +
      '* Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. pile: 언어 모델링을 위한 다양한 텍스트의 800gb 데이터세트 _ ArXiv:2101.00027_, 2020.\n' +
      '* Team et al. (2024) Thomas Mesnard Gemma Team, Cassidy Hardin, Robert Dadashi, Surya Bhuptiraju, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Leonard Hussenot, and et al. Gemma: Gemini 연구 및 기술을 기반으로 한 오픈 모델. 2024. URL[https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)\n' +
      '* Gogoulou et al. (2023) Evangelia Gogoulou, Timothee Lesort, Magnus Boman, and Joakim Nivre. 언어교체에 따른 지속학습에 관한 연구 CoRR_, abs/2311.01200, 2023. URL[https://doi.org/10.48550/arXiv.2311.01200](https://doi.org/10.48550/arXiv.2311.01200)이다.\n' +
      '\n' +
      '정공, 건주, 신자오, 징사, 왕시진, 지롱원 등이다. 구문 인식 메모리 네트워크를 이용한 수학 문제 이해를 위한 언어 모델의 지속적인 사전 훈련, 2022. URL[https://aclanthology.org/2022.acl-long.408/](https://aclanthology.org/2022.acl-long.408/]).\n' +
      '* Goyal et al. (2018) Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 정확하고 큰 미니 배치 sgd: 2018년 1시간 후 훈련 이미제넷.\n' +
      '* Gururangan et al. (2020) Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 사전 교육을 멈추지 마십시오: 언어 모델을 도메인 및 작업에 적용합니다. 댄 주라프스키, 조이스 차이, 나탈리 슐루터, 조엘 R. Tetreault(eds.), _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, pp. 8342-8360. Association for Computational Linguistics, 2020. URL[https://doi.org/10.18653/v1/2020.acl-main.740](https://doi.org/10.18653/v1/2020.acl-main.740).\n' +
      '* Harun et al. (2023a) Md Yousuf Harun, Jhair Gallardo, Tyler L Hayes, and Christopher Kanan. 오늘날의 지속적인 학습 알고리즘은 얼마나 효율적입니까? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 2430-2435, 2023a.\n' +
      '* Harun et al. (2023b) Md Yousuf Harun, Jhair Gallardo, Tyler L. 헤이즈, 로널드 켐커 크리스토퍼 캐넌 Siesta: 수면을 이용한 효율적인 온라인 연속 학습, 2023b.\n' +
      '* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 대규모 멀티태스킹 언어 이해도 측정, 2021년.\n' +
      '* Hernandez et al. (2021) Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 전송을 위한 법률 확대. _ CoRR_, abs/2102.01293, 2021. URL[https://arxiv.org/abs/2102.01293](https://arxiv.org/abs/2102.01293).\n' +
      '* Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al training compute-optimal large language models. _ arXiv preprint arXiv:2203.15556_, 2022. URL[https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556).\n' +
      '* Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, Hyouk Middleong Lee, Jiquan Ngiam, Quoc V. 르, 용희우, 지펑첸 Gpipe: 파이프라인 병렬성을 이용한 거대 신경망의 효율적인 훈련, 2019.\n' +
      '* 장 등(2022) 조엘 장, 성현 예, 창호 이, 소희 양, 중보 신, 장훈 한, 경훈 김, 민준 서. Temporalwiki: 끊임없이 진화하는 언어 모델을 훈련하고 평가하기 위한 평생의 벤치마크. 요브 골드버그, Zornitsa Kozareva, and Yue Zhang(eds.), _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pp. 6237-6250. Association for Computational Linguistics, 2022a. URL[https://doi.org/10.18653/v1/2022.emnlp-main.418](https://doi.org/10.18653/v1/2022.emnlp-main.418)\n' +
      '* 장 등(2022) 조엘 장, 성현 예, 소희 양, 중보 신, 장훈 한, 경훈 김, 스탠리정규 최, 민준 서. 언어 모델의 지속적인 지식 학습에 관한 것입니다. 10차 국제학술대회에서는 ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022b. URL[https://openreview.net/forum?id=vfsRB5Mimo9](https://openreview.net/forum?id=vfsRB5Mimo9)\n' +
      '* Jiang et al. (2023) Albert Q. 장, 알렉산드르 사블레이롤, 아서 멘쉬, 크리스 뱀포드, 데벤드라 싱 채플롯, 디에고 데 라스 카사스, 플로리안 브레산드, 지아나 령겔, 기욤 옴플, 루실 사울니에, 릴리오 레나르 라바우, 마리안 라초, 피에르 스톡, 테븐 르 스카오, 티보트 라브릴, 토마스 왕, 티모티 라크루아, 윌리엄 엘 사예드. 미스트랄 7b. _ CoRR_, abs/2310.06825, 2023. URL[https://doi.org/10.48550/arXiv.2310.06825](https://doi.org/10.48550/arXiv.2310.06825)이다.\n' +
      '* Joshi et al.(2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 트리비아QA: 읽기 이해를 위해 멀리 감독된 대규모 챌린지 데이터 세트. Regina Barzilay and Min-Yen Kan(eds.), _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL[https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147).\n' +
      '* Liu et al. (2018)Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 신경 언어 모델의 법칙을 확장합니다. _ CoRR_, abs/2001.08361, 2020. URL[https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361).\n' +
      '* Ke et al. (2022) Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. 언어 모델의 지속적인 사전 훈련, 2022. URL[https://openreview.net/forum?id=m_GDIItaI3o](https://openreview.net/forum?id=m_GDIItaI3o)\n' +
      '* Kirillov et al. (2023) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. 뭐든 구분해 봐 arXiv:2304.02643_, 2023.\n' +
      '* Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov. 자연 질문: 질문 답변 연구의 벤치마크. _ The Association for Computational Linguistics_, 7:452-466, 2019. doi: 10.1162/tacl_a_00276. URL[https://aclanthology.org/Q19-1026](https://aclanthology.org/Q19-1026).\n' +
      '*17, 2022_, pp. 215-221. Association for Computational Linguistics, 2022. URL[https://aclanthology.org/2022.wnut-1.23](https://aclanthology.org/2022.wnut-1.23).\n' +
      '* Lesort et al. (2023) Timothee Lesort, Oleksiy Ostapenko, Pau Rodriguez, Diganta Misra, Md Rifat Arefin, Laurent Charlin, and Irina Rish. 재난적 망각과 지식 축적에 대한 일반적인 가정에 도전합니다. Sarath Chandar, Razvan Pascanu, Hanie Sedghi, and Doina Precup(eds.), _Conference on Lifelong Learning Agents, 22-25 August 2023, McGill University, Montreal, Quebec, Canada_, volume 232 of _Proceedings of Machine Learning Research_, pp. 43-65. PMLR, 2023. URL[https://proceedings.mlr.press/v232/lesort23a.html](https://proceedings.mlr.press/v232/lesort23a.html).\n' +
      '* Lesort et al. (2021) Timothee Lesort, Massimo Caccia, and Irina Rish. 데이터 분포 드리프트 분석으로 연속 학습 설정 이해. _ arXiv preprint arXiv:2104.01678_, 2021.\n' +
      '* Lin et al.(2022) Stephanie Lin, Jacob Hilton, and Owain Evans. 진실: 모델이 인간의 거짓을 어떻게 모방하는지 측정하는 것, 2022년.\n' +
      '* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. 분리된 중량 감쇠 규칙화. _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL[https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)이다.\n' +
      '* Ma et al. (2023) Shirong Ma, Shen Huang, Shulin Huang, Xiaobin Wang, Yangning Li, Hai-Tao Zheng, Pengjun Xie, Fei Huang, and Yong Jiang. Ecomppt-ct: 반정형 데이터를 가진 전자 상거래 대형 언어 모델의 지속적인 사전 훈련. _ CoRR_, abs/2312.15696, 2023. URL[https://doi.org/10.48550/arXiv.2312.15696](https://doi.org/10.48550/arXiv.2312.15696).\n' +
      '* McMahan et al.(2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 탈중앙화된 데이터로부터 심층 네트워크의 통신 효율적인 학습. In _Artificial intelligence and statistics_, pp. 1273-1282. PMLR, 2017.\n' +
      '* Mehta et al. (2023) Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. 평생학습에서 사전훈련의 역할에 대한 실증적 조사. _ J 마흐 배워 Res._ , 24:214:1-214:50, 2023. URL[http://jmlr.org/papers/v24/22-0496.html](http://jmlr.org/papers/v24/22-0496.html).\n' +
      '* Mermillod et al. (2013) Martial Mermillod, Aurelia Bugaiska, and Patrick Bonin. 안정성-가소성 딜레마: 치명적인 망각에서 연령 제한 학습 효과에 이르는 연속체를 조사하는 것 Frontiers in psychology_, 4(August):504, 2013. ISSN 1664-1078. doi: 10.3389/fpsyg.2013.00504. URL[http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3732997](http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3732997]{&}tool=pmcentrez{&}rendertype=추상적이다.\n' +
      '* Mermillod et al. (2017)Microsoft. 메가트론-딥스피드. [https://github.com/마이크로소프트/메가트론-딥스피드] (https://github.com/microsoft/Megatron-DeepSpeed), 2020. Accessed: 2024년 2월 28일.\n' +
      '* Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 갑옷이 전기를 통할 수 있나요? 2018년 오픈북 질의응답을 위한 새로운 데이터셋.\n' +
      '* Mirzadeh et al. (2022) Seyed-Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Huiyi Hu, Razvan Pascanu, Dilan Gorir, and Mehrdad Farajtabar. 광범위한 신경망은 덜 재앙적으로 잊는다. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato(eds.), _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pp. 15699-15717. PMLR, 2022. URL[https://proceedings.mlr.press/v162/mirzadeh22a.html](https://proceedings.mlr.press/v162/mirzadeh22a.html).\n' +
      '* Ostapenko et al. (2019) Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin Nabi. 기억하기 위한 학습: 지속적인 학습을 위한 시냅스 가소성 기반 프레임워크. IEEE Conference on Computer Vision and Pattern Recognition_의 _Proceedings, pp. 11321-11329, 2019. URL[https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html](https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Remember_Framework_for_Continual_CVPR_2019_paper.html](https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Remember_Framework_for_Continual_CVPR_2019_paper.html)\n' +
      '* Pernias et al. (2024) Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville. Wurstchen: 대규모 텍스트-이미지 확산 모델을 위한 효율적인 아키텍처. _The Twelfth International Conference on Learning Representations_, 2024. URL[https://openreview.net/forum?id=gU58d5QeGv](https://openreview.net/forum?id=gU58d5QeGv).\n' +
      '* Pluster(2023) Bjorn Pluster. German Benchmark Datasets, 8 2023. URL[https://github.com/bjoernpl/GermanBenchmark](https://github.com/bjoernpl/GermanBenchmark)\n' +
      '* Popel and Bojar(2018) Martin Popel and Ondrej Bojar. 변압기 모델의 훈련 팁입니다. _ Prague Bulletin of Mathematical Linguistics_, 110(1):43-70, April 2018. ISSN 1804-0462. doi: 10.2478/pralin-2018-0002. URL[http://dx.doi.org/10.2478/pralin-2018-0002](http://dx.doi.org/10.2478/pralin-2018-0002).\n' +
      '* Prabhu et al. (2023) Ameya Prabhu, Zhipeng Cai, Puneet Dokania, Philip Torr, Vladlen Koltun, and Ozan Sener. 스토리지 제약 없이 온라인 연속 학습. _ arXiv preprint arXiv:2305.09253_, 2023.\n' +
      '* Qin et al. (2023) Yujia Qin, Cheng Qian, Xu Han, Yankai Lin, Huadong Wang, Ruobing Xie, Zhiyuan Liu, Maosong Sun, 및 Jie Zhou. 지속적인 사전 훈련을 위한 재활용 가능한 튜닝, 2023. URL[https://arxiv.org/abs/2305.08702](https://arxiv.org/abs/2305.08702).\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 자연어 감독에서 전이 가능한 시각적 모델을 학습합니다. Marina Meila and Tong Zhang(eds.), _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pp. 8748-8763. PMLR, 2021. URL[http://proceedings.mlr.press/v139/radford21a.html](http://proceedings.mlr.press/v139/radford21a.html).\n' +
      '* Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis and insights from training gopher. _ arXiv preprint arXiv:2112.11446_, 2021. URL[https://arxiv.org/abs/2112.11446](https://arxiv.org/abs/2112.11446).\n' +
      '* Raffel et al. (2023) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 통합 텍스트-텍스트 변환기(2023)로 전이 학습의 한계를 탐구한다.\n' +
      '* Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 제로: 조 단위 매개변수 모델을 훈련하기 위한 메모리 최적화입니다. 크리스틴 쿠치, 아이린 퀄터, 윌리엄 T. Kramer(eds.), _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event/ Atlanta, Georgia, USA, November 9-19, 2020_, pp. 20. IEEE/ACM, 2020.\n' +
      '* Raffel et al. (2020) Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. 신경망에서 재앙적인 망각에 대한 규모의 영향. 10차 국제학술대회에서는 ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. URL[https://openreview.net/forum?id=GHvSS_yPeEa](https://openreview.net/forum?id=GHvSS_yPeEa).\n' +
      '* Reddi et al. (2021) Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and Hugh Brendan McMahan. 적응형 연합 최적화 _International Conference on Learning Representations_, 2021. URL[https://openreview.net/forum?id=LkFG31B13U5](https://openreview.net/forum?id=LkFG31B13U5)이다.\n' +
      '* Riemer et al. (2019) Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. 전이를 극대화하고 간섭을 최소화하여 잊지 않고 학습하는 학습. _International Conference on Learning Representations_, 2019. URL[https://openreview.net/forum?id=B1gTShAct7](https://openreview.net/forum?id=B1gTShAct7)이다.\n' +
      '* Rolnick et al. (2019) David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. 지속적인 학습을 위한 재생을 경험하세요. In _Advances in Neural Information Processing Systems_, pp. 348-358, 2019. URL[https://arxiv.org/abs/1811.11682](https://arxiv.org/abs/1811.11682).\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성 In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pp. 10674-10685. IEEE, 2022. URL[https://doi.org/10.1109/CVPR52688.2022.01042](https://doi.org/10.1109/CVPR52688.2022.01042)\n' +
      '* Ryabinin et al. (2021) Max Ryabinin, Eduard Gorbunov, Vesvold Polkhotnyuk, and Gennady Pekhimenko. Moshpit sgd: 이질적인 신뢰할 수 없는 장치에 대한 통신 효율적인 분산형 교육. 인민 란자토, A. 베이겔지머, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan(eds.), _Advances in Neural Information Processing Systems_, Volume 34, pp. 18195-18211. Curran Associates, Inc., 2021. URL[https://proceedings.neurips.cc/paper_files/paper/2021/file/97275a23ca44226c9964043c8462be96-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/97275a23ca44226c9964043c8462be96-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/97275a23ca44226c9964043c8462be96-Paper.pdf]).\n' +
      '* Sakaguchi et al. (2019) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: Anversarial winograd schema challenge at scale, 2019.\n' +
      '* Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: 176b-parameter open-access multilingual language model. _ arXiv preprint arXiv:2211.05100_, 2022. URL[https://arxiv.org/abs/2211.05100](https://arxiv.org/abs/2211.05100).\n' +
      '* Scialom et al. (2022) Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. 미세 조정 언어 모델은 지속적인 학습자입니다. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pp. 6107-6122, 2022.\n' +
      '* Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. 하위 단어 단위가 있는 희귀 단어의 신경 기계 번역. In Katrin Erk and Noah A. Smith (eds.), _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1715-1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL[https://aclanthology.org/P16-1162](https://aclanthology.org/P16-1162).\n' +
      '* Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 메가트론-1m: 모델 병렬성을 사용하여 수십억 매개 변수 언어 모델을 훈련합니다. _ ArXiv preprint arXiv:1909.08053_, 2019.\n' +
      '* Shoeybi et al. (2020) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 메가트론-1m: 2020년 모델 병렬성을 사용하여 수십억 매개 변수 언어 모델을 훈련한다.\n' +
      '* Smith et al. (2021) James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen, Hongxia Jin, and Zsolt Kira. 항상 꿈을 꾸세요: 데이터 없는 수업 증대 학습을 위한 새로운 접근 방식입니다. pp. 9374-9384, 2021년 10월.\n' +
      '* Soboleva et al. (2023) Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: RedPajama의 627B 토큰 세정 및 중복제거 버전. [https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-anddeduplicated-version-of-redpajama] (https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-anddeduplicated-version-of-redpajama), 6월 2023. URL[https://huggingface.co/datasets/cerebras/SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B).\n' +
      '손 등(2019) * 솔다이니 등(2024) 루카 솔다이니, 로드니 키니, 악시타 바기아, 더스틴 슈벤크, 데이비드 아틴슨, 러셀 아투르, 벤 보긴, 키아티 찬두, 제니퍼 두마스, 야나이 엘라자르, 발렌틴 호프만, 아나냐 하르쉬 자하, 사친 쿠마르, 리 루시, 신시 류, 나탄 램버트, 이안 마그누슨, 제이콥 모리슨, 니클라스 뮌너슨, 제이콥 모리슨, 니클라스 뮌너오프, 아칸샤 나이크, 크리스탈 남, 매튜 E. 피터, 아빌샤 라비찬더, 카일 리차드슨, 제장 셸, 에마 스트루벨, 니샨 서브라마니, 오이빈드 타프조르, 피트 월시, 루크 제틀모이어, 노아 A. 스미스, 한나네 하지시르지, 이즈 도지, 카일 로. Dolma: 언어 모델 사전 훈련 연구를 위한 3조 토큰의 개방형 말뭉치. _ CoRR_, abs/2402.00159, 2024. URL[https://doi.org/10.48550/arXiv.2402.00159](https://doi.org/10.48550/arXiv.2402.00159).\n' +
      '* Sun et al. (2020) Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE 2.0: 언어 이해를 위한 지속적인 사전 훈련 프레임워크. In _The Thirty- fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pp. 8968-8975. AAAI Press, 2020. URL[https://doi.org/10.1609/aaai.v34i05.6428](https://doi.org/10.1609/aaai.v34i05.6428)\n' +
      '* Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023a. URL[https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971).\n' +
      '* Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Likel, Likas Blecher, Cristian Canton-Ferrer, Moya Bhosale, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Cynthia Gao, Vedan Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Vindan Kardas, Viktor Kerkez, Miedan Lavril, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Lavril, Yixin Nie, Likan Leean Larov, Yiaia Williams, Liaia Subramanian Tan, Binh Tang, Ross Taylor, Adina Williams, Liaia Zarov, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Serge 라마 2: 오픈 파운데이션 및 미세 조정 채팅 모델들_ CoRR_, abs/2307.09288, 2023b. URL[https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288).\n' +
      '* Veniat et al. (2021) Tom Veniat, Ludovic Denoyer, and MarcAurelio Ranzato. 모듈식 네트워크 및 작업 주도 전과를 통한 효율적인 지속적인 학습 _International Conference on Learning Representations_, 2021. URL[https://openreview.net/forum?id=EKV158tSfwv](https://openreview.net/forum?id=EKV158tSfwv)이다.\n' +
      '* Wu et al. (2024) Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, and Ying Shan. 라마 프로: 블록 확장을 갖는 점진적 라마. _ CoRR_, abs/2401.02415, 2024. URL[https://doi.org/10.48550/arXiv.2401.02415](https://doi.org/10.48550/arXiv.2401.02415)이다.\n' +
      '* Xie et al. (2023) Yong Xie, Karan Aggarwal, and Aitzaz Ahmad. 도메인 특정 대형 언어 모델을 구축하기 위한 효율적인 연속 사전 훈련, 2023. URL[https://arxiv.org/abs/2311.08545](https://arxiv.org/abs/2311.08545)\n' +
      '* Xiiong et al. (2020) Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 트랜스포머 아키텍처에서 온 레이어 정규화, 2020.\n' +
      '* Yadav et al. (2023) Prateek Yadav, Qing Sun, Hantian Ding, Xiaopeng Li, Dejiao Zhang, Ming Tan, Parminder Bhatia, Xiaofei Ma, Ramesh Nallapati, Murali Krishna Ramanathan, Mohit Bansal, and Bing Xiang. 코드 생성 모델에 대한 지속적인 학습 탐색 조던 L. 안나 로저스 Boyd-Graber, and Naoaki Okazaki(eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics(Volume 2: Short Papers), ACL 2023, Toronto, July 9-14, 2023_, pp. 782-792. Association for Computational Linguistics, 2023a. URL[https://doi.org/10.18653/v1/2023.acl-short.68](https://doi.org/10.18653/v1/2023.acl-short.68).\n' +
      '* Yadav et al. (2023) Prateek Yadav, Derek Tam, Leshem Choshen, Colin A. Raffel, and Mohit Bansal. 타이 병합: 모델을 병합할 때 간섭을 해결합니다. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine(eds.), _Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,USA, December 10 - 16, 2023_, 2023b. URL[http://papers.nips.cc/paper_files/paper/2023/hash/1644c9af28ab7916874f6fd6228a9bcf-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/1644c9af28ab7916874f6fd6228a9bcf-Abstract-Conference.html)\n' +
      '* Yang et al. (2022) Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, David Farhi, Jakub Pachocki, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. 텐서 프로그램 v: 제로-샷 하이퍼-파라미터 전달을 통해 대형 신경망들을 튜닝한다. 2022년 3월 _NeurIPS 2021_에서 URL[https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/](https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/)\n' +
      '* Yang et al. (2024) Xianjun Yang, Junfeng Gao, Wenxin Xue, and Erik Alexandersson. PLMA: 식물 과학을 위한 오픈소스 대형 언어 모델 _ CoRR_, abs/2401.01600, 2024. URL[https://doi.org/10.48550/arXiv.2401.01600](https://doi.org/10.48550/arXiv.2401.01600).\n' +
      '* You et al. (2019) Kaichao You, Mingsheng Long, Jianmin Wang, and Michael I. Jordan. 학습 속도 붕괴가 현대 신경망에 어떤 도움이 됩니까? 2019년입니다.\n' +
      '* Zan et al. (2022) Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, 및 Jian-Guang Lou. CERT: 라이브러리 지향 코드 생성을 위한 스케치에 대한 지속적인 사전 교육. Luc De Raedt(ed.), _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022_, pp. 2369-2375. ijcai.org, 2022. URL[https://doi.org/10.24963/ijcai.2022/329](https://doi.org/10.24963/ijcai.2022/329).\n' +
      '* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, 요나탄 Bisk, Ali Farhadi, and Yejin Choi. 헬라스바그: 기계가 정말로 당신의 문장을 끝낼 수 있나요? 2019년.\n' +
      '* Zhai et al. (2022) Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 비전 트랜스포머를 확장합니다. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pp. 1204-1213. IEEE, 2022. URL[https://doi.org/10.1109/CVPR52688.2022.01179](https://doi.org/10.1109/CVPR52688.2022.01179)\n' +
      '* Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _ arXiv preprint arXiv:2303.18223_, 2023. URL[https://arxiv.org/abs/2303.18223](https://arxiv.org/abs/2303.18223).\n' +
      '\n' +
      '###### Contents\n' +
      '\n' +
      '*1 소개\n' +
      '*2 주요 소견 및 다케아웨이\n' +
      '* 3 관련 작업\n' +
      '	* 3.1 연속 학습\n' +
      '	* 3.2 사전교육, 모형척도, 연속학습\n' +
      '	* 3.3 도메인 적응 연속 사전 훈련(DACPT)\n' +
      '	* 3.4 특정 도메인에 적용된 LMs에 대한 연속 학습\n' +
      '	* 3.5 학습률 스케쥴\n' +
      '* 4 배경 및 방법\n' +
      '	* 4.1 Linear Warmup and Cosine Decay Schedule\n' +
      '	*4.2 Compute-equivalent Replay\n' +
      '* 5 실험 설정\n' +
      '	* 5.1 데이터세트\n' +
      '	* 5.2 연속 학습 설정\n' +
      '	* 5.3 훈련 설정\n' +
      '	* 5.4 독일어 및 영어 LM 평가 벤치마크\n' +
      '* 6 결과\n' +
      '	* 6.1 학습률 스케쥴\n' +
      '		* 6.1.1 약하고 강한 분포 이동에 대한 선형 웜업의 효과.\n' +
      '		* 6.1.2 Weak and Strong Distribution Shift에 대한 재-온화, 재-감쇠 및 다양한 \\(\\eta_{max}\\)의 효과.\n' +
      '	*6.2 Replay의 효과\n' +
      '	* 6.3 약한 분산 및 강한 분산 이동에 대한 연속 사전 훈련 최종 성능.\n' +
      '		* 6.3.1 손실에 의해 평가된 최종 성능\n' +
      '		* 6.3.2 Zero-shot과 Few-shot으로 평가한 인기 LM 벤치마크에 대한 최종 성능\n' +
      '	* 6.4 모델 척도에 따른 연속 사전 훈련 최종 성능\n' +
      '		* 6.4.1 손실에 의해 평가된 최종 성능\n' +
      '		* 6.4.2 Zero-shot과 Few-shot으로 평가한 인기 LM 벤치마크에 대한 최종 성능\n' +
      '*7 재온화 병리의 이해와 순환\n' +
      '	* 7.1 동일 데이터에 대한 재온화\n' +
      '* 8 라디에이터* 7.2 무한학습률 스케쥴\n' +
      '	* 7.3 코사인 감쇠와 무한 스케줄의 변이 비교\n' +
      '	* 7.4 무한 학습 속도 스케쥴: 무한 미래 업데이트로의 스케일링\n' +
      '*8 한계\n' +
      '* 9 결론\n' +
      '* 확장된 결과\n' +
      '* A.1 도메인 증분 연속 사전 훈련\n' +
      '* A.2 Model Merging v.s. Continual Pre- Training\n' +
      '* 다른 데이터 세트 크기에 대한 A.3 리플레이\n' +
      '* A.4 독일 모델의 정성적 평가\n' +
      '* A.5 종합 LM 평가 결과\n' +
      '* A.6 집계 평균 최종 정확도\n' +
      '* B 모델 하이퍼파라미터\n' +
      '\n' +
      '## 부록 A 확장된 결과\n' +
      '\n' +
      '다음 하위 섹션에서는 먼저 _도메인 점진적 연속 사전 훈련_ 설정에서 몇 가지 새로운 결과를 제시하고, 다른 데이터 세트 크기에 대한 리플레이를 비교하고, 독일어 모델에 대한 정성적 분석을 제공한다. 또한 논문의 모든 모델에 대한 집계된 평가 및 최종 손실 테이블을 제공한다.\n' +
      '\n' +
      '###도메인 증분 연속 사전 훈련\n' +
      '\n' +
      '우리는 **도메인 증분 학습** 설정을 고려한다. **도메인 증분 학습** 설정은 각각 별개의 도메인에서 오는 \\(\\{\\mathcal{D}_{0},\\mathcal{D}_{1},\\dots,\\mathcal{D}_{N-1}\\})의 미래 데이터 세트를 훈련한다. 이러한 설정은 각 도메인 간의 전환 시 분포 이동 경험으로 인해 특히 어렵다. 구체적으로, 데이터세트 \\(\\mathcal{D}_{0}\\)을 파일(Gao et al., 2020)에서 사전 훈련으로 간주하고, \\(\\mathcal{D}_{1}\\)을 슬림파자마에서 사전 훈련으로 간주한다. 그러나 표 1의 샘플링 백분율을 사용하여 \\(\\mathcal{D}_{1}\\)의 데이터를 소비하는 대신, 가장 큰 도메인에서 가장 작은 도메인까지 한 번에 한 도메인씩 데이터 세트를 처리한다. 이것은 다른 시간에 다른 도메인에서 새로운 데이터가 수신되는 상황을 시뮬레이션하고 모든 배포 이동에 강인하면서 시퀀스에 대한 모델을 업데이트하고자 한다.\n' +
      '\n' +
      '도메인 증분 연속 사전 학습에 리플레이를 적응하는 두 가지 이상의 작업에 걸쳐 있는 설정에서는 리플레이 버퍼가 이산 간격으로 업데이트되는 저장소 샘플링(Buzzega et al., 2020)의 형태를 사용한다. 우리는 이 기술을 _discrete reservoir sampling_라고 한다. 구체적으로, 수열로 훈련된 \\(N\\) 데이터세트 \\(\\mathcal{D}_{0},\\mathcal{D}_{1},\\dots,\\mathcal{D}_{N-1}\\)의 크기 \\(s_{0},s_{1},\\dots,s_{N-1}\\)이 주어지면, 각 데이터세트 전이시 리플레이 버퍼 \\(\\mathcal{R}\\)을 갱신한다. \\(\\mathcal{R}_{i}\\)는 i번째 태스크 이전의 리플레이 버퍼의 상태에 대응된다. 리플레이 비율 \\(0\\leq\\alpha\\leq 1\\)에 대해, 임의의 주어진 \\(i>0\\)에서, \\(\\mathcal{R}_{i}\\)은 \\(p_{i,j}:=\\frac{s_{j}\\cdot(1-\\alpha)+\\sum_{j=i+1}p_{i,j}\\cdot s_{k}\\cdot\\alpha\\cdot\\alpha\\cdot\\alpha\\cdot\\i,j}\\cdot s_{k}}\\에 대한 모든 \\(\\mathcal{D}_{j}\\)의 데이터를 포함할 것이며, 여기서 \\(i\\)은 시간 인덱스이고 \\(j\\)은 데이터세트 인덱스이다.\n' +
      '\n' +
      '그림 10은 슬림파자마의 300B 토큰 분할을 소비하기 위한 도메인 증분 접근법에 대한 결과를 보고하며, 여기서 각 도메인은 크기 순으로 처리된다. 학습률 재배열, LR 재배열 및 이산 저장소 샘플링으로 재생한다. 처리된 총 데이터는 슬림파자마(파란색 점선)의 300B 토큰 상에서 LR 재-온화 및 재-디케이팅으로 트레이닝된 모델과 동일하다. 그러나 데이터가 처리되는 순서와 학습률 일정은 기준선과 다르다.\n' +
      '\n' +
      '그림 10: **도메인별로 데이터를 순차적으로 섭취합니다. 우리는 이용 가능한 데이터를 소비하기 위한 대안적인 접근 방식을 탐구한다. 과제 증가 또는 수업 증가 학습과 유사하게, 우리는 한 번에 하나의 영역을 순차적인 방식으로 훈련한다. 우리는 _discrete reservoir sampling_**뿐만 아니라 각 도메인에 대해 LR 재온 및 LR 재붕괴를 사용한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:35]\n' +
      '\n' +
      '다양한 데이터 세트 크기를 위한### 재생\n' +
      '\n' +
      'Sec. 6.2의 실험에서 수집된 통찰력을 줄이기 위해 100B, 200B 및 300B 토큰으로 슬림파자마에 대한 지속적인 사전 훈련의 크기를 변경한다. 각각의 선형 워밍업 스케줄 및 코사인 디케이는 변경된 훈련 길이에 맞춰진다. 예상대로, 우리는 훈련 토큰의 수가 증가한 슬림파자마에 대한 훈련 종료 시 손실의 일관된 감소를 관찰한다(도 11 참조). 우리는 새로운 데이터에 대한 잊기와 적응에 대한 재생의 영향과 관련하여 유사한 경향을 추가로 관찰한다. 결론적으로 LR 재배열, LR 재배열 및 재생의 간단한 조합은 다양한 데이터 세트 크기에서 효과적이다.\n' +
      '\n' +
      '### 독일 모델의 정성적 평가\n' +
      '\n' +
      '이 섹션에서는 독일 커먼 크롤(Sec. 6.3)에서 훈련된 모델에 대한 간략한 정성적 평가를 제공한다. 우리는 독일어의 다양한 특수성을 담고 있는 5개의 독일어 프롬프트를 선택한다(탭 8 참조). 그런 다음 독일 커먼 크롤에서 훈련되거나 지속적으로 사전 훈련된 모델 각각에 대해 고정된 토큰 길이 응답을 생성한다. 기준선으로서, 우리는 또한 파일에서만 훈련된 동일한 모델을 평가한다.\n' +
      '\n' +
      '우리는 표 9에서 응답과 응답의 수동 번역을 제공한다. 비교적 작은 405M 매개변수 모델은 일반적으로 의미 있는 문장을 생성하지 않지만, 독일 커먼 크롤에 대해 훈련된 모델은 일반적으로 문법적으로 올바른 문장을 생성한다. 여기에는 올바른 상부 및 하부 케이싱, 복합어 생성, 언라우트 문자 사용 등이 포함된다. 모든 경우에, 응답은 몇 마디 후에 반복되는 경향이 있다. 일부 산출물의 불완전성과 분수어는 생성된 토큰의 수가 매우 짧기 때문이라고 볼 수 있다. 더 긴 시퀀스는 구문적으로 올바른 문장을 제공하는 경향이 있지만 스스로 반복하는 경향이 있다. 제한된 샘플 수와 일반적으로 생성된 텍스트의 품질이 낮기 때문에 독일 커먼 크롤에 대해 훈련된 개별 모델 간의 질적 차이에 대한 강력한 진술을 할 수 없다. 그러나 앞서 언급한 반복성 문제와 문법 오류 문제는 파일 기준선에서 상당히 더 강한 것으로 보이며, 이는 일반적으로 프롬프트에서 주어진 맥락조차 존중하지 못한다. 이것은 독일어 텍스트의 양이 파일에서 상당히 작기 때문에 예상됩니다.\n' +
      '\n' +
      '따라서 우리는 훈련 중 독일 커먼 크롤의 데이터를 사용한 모델에서 독일어의 생성 품질에 관찰 가능한 효과가 있다고 결론지었다.\n' +
      '\n' +
      '그림 11: **Replay v.s. 학습률을 다시 데우고 다른 양의 데이터에 대해 지속적으로 사전 훈련할 때 다시 플레이하지 않음. 슬림파자마 데이터의 100B, 200B 및 300B 토큰에서 5% 리플레이가 있거나 없는 405M 매개변수 모델을 훈련한다. 각 모델은 파일의 300B 토큰에서 미리 훈련된 체크포인트에서 시작하여 데이터 세트 크기에 맞는 선형 워밍업 및 코사인 감쇠 일정을 사용하여 학습 속도를 다시 워밍한다.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:37]\n' +
      '\n' +
      '### 집계 LM 평가 결과\n' +
      '\n' +
      '우리는 다음과 같은 영어 및 독일어 LM 평가 과제에 대한 모델을 평가한다:\n' +
      '\n' +
      '1. **HellaSwag**(Zellers et al., 2019) 및 **HellaSwag DE**: 언어 모델을 혼동하도록 의도적으로 설계된 객관식 질문으로 구성된 영어 상식 추론 벤치마크. HellaSwag DE는 HellaSwag 벤치마크를 독일어로 번역한 것이다.\n' +
      '2. **AI2 Reasoning Challenge(ARC)**(Clark et al., 2018): 객관식 형식의 과학 시험 문항으로 구성된 영어 상식 추론 벤치마크. 전체 문항은 \\(5,197\\)의 쉬운 부분집합과 \\(2,590\\)의 어려운 부분집합으로 나누어졌다. ARC-c DE는 질문의 도전 하위 집합을 독일어로 번역한 것이다.\n' +
      '3. **BoolQ**(Clark et al., 2019): \\(15,942\\) yes/no question-answering 샘플로 구성된 영어 독해 벤치마크. 각 예는 질문, 관련 단락 및 해결책으로 나뉜다.\n' +
      '4. **MathQA**(Amini et al., 2019): 수학의 다양한 영역에 걸친 객관식 질문으로 구성된 영어 수학 단어 문제 벤치마크.\n' +
      '5. **MMLU**(Hendrycks et al., 2021) and **MMLU-DE**: An English benchmark designed to evaluate both zero-shot and few-shot scenarios, to evaluate both general knowledge and on-the-fly problem solving of the model under test. MMLU는 광범위한 주제를 다룬다. MMLU-DE는 OpenAI GPT 3.5 API에 의해 번역된 MMLU 질문 세트의 독일어 번역이다.\n' +
      '6. **OpenBookQA(OBQA)**(Mihaylov et al., 2018): 특정 과목에 대한 인간의 이해를 평가하기 위한 실제-세계 오픈-북 시험을 모델로 한 영어 질문-답변 벤치마크. 초등 과학에 대한 질문은 과학적 사실 및 공통 지식과 쌍을 이루며, 이 모델은 다중 홉 추론에서 사용하고자 한다.\n' +
      '7. **PIQA**(Bisk et al., 2019): 모델의 물리적 상식 추론 능력을 테스트하기 위해 설계된 영어 질문-답변 벤치마크. 대부분의 질문은 일상적인 상황에 흔치 않은 해결책을 적용하는 데 중점을 두고 있으며, 이는 물리적 세계에 대한 이해가 필요하다.\n' +
      '8. **WinoGrande**(Sakaguchi et al., 2019): 텍스트 내의 두 개 이상의 표현이 동일한 엔티티를 지칭할 때를 결정하는 것을 포함하는 영어 자연 언어 이해 벤치마크. 벤치마크는 다양한 문장 세트와 인간과 유사한 예측을 하기 위해 모델을 보상하는 새로운 평가 메트릭을 포함한다.\n' +
      '9. **TruthfulQA** 및 **TruthfulQA DE**(Lin et al., 2022): 질문에 대한 생성된 답변의 진실성을 평가하기 위해 설계된 영어 질문-답변 벤치마크. 그 질문들은 오답으로 이어지는 일반적인 인간의 오해를 포함하도록 고안되었다. TruthfulQA DE는 TruthfulQA 벤치마크를 독일어로 번역한 것이다.\n' +
      '10. **Natural Questions**(Kwiatkowski et al., 2019): 구글 검색 엔진에 제출된 검색 질의로 구성된 영어 질의 응답 벤치마크이다.\n' +
      '11. **TriviaQA**(Joshi et al., 2017): 영어 질문-답변 벤치마크는 퀴즈 애호가들에 의해 제공되는 질문-답변 쌍들로 구성된다. 그 주된 초점은 모델의 일반적인 세계 지식을 결정하는 것이다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:39]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:40]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:41]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Description & Value \\\\ \\hline\n' +
      '**10B Model- Cosine Schedule** & \\\\ Max learning rate (\\(\\eta_{max}\\)) & \\(1.2\\cdot 10^{-4}\\) \\\\ Min learning rate (\\(\\eta_{min}\\)) & \\(1.2\\cdot 10^{-5}\\) \\\\ Warmup percent (\\(T_{warmup}\\)) & 1 \\\\ \\hline\n' +
      '**405M Model - Cosine Schedule** & \\\\ Max learning rate (\\(\\eta_{max}\\)) & \\(3\\cdot 10^{-4}\\) \\\\ Min learning rate (\\(\\eta_{min}\\)) & \\(3\\cdot 10^{-5}\\) \\\\ Warmup percent (\\(T_{warmup}\\)) & 1 \\\\ \\hline\n' +
      '**405M Model - Infinite LR Schedule Common** & \\\\ Max learning rate (\\(\\eta_{max}\\)) & \\(3\\cdot 10^{-4}\\) \\\\ Min learning rate (\\(\\eta_{min}\\)) & \\(3\\cdot 10^{-5}\\) \\\\ Constant learning rate (\\(\\eta_{const}\\)) & \\(1.65\\cdot 10^{-4}\\) \\\\ Warmup percent (\\(T_{warmup}\\)) & 1 \\\\ Cooldown iters percent (\\(T_{cd}\\)) & 60 \\\\ Constant iters percent (\\(T_{ann}\\)) & 25 \\\\ \\hline\n' +
      '**Inverse Square root cooldown schedule** & \\\\ Timescale (\\(\\alpha\\)) & 10 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 14: **LR 스케줄의 하이퍼파라미터.** 본문에 달리 명시되지 않는 한 이러한 값을 사용한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>