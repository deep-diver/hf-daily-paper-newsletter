<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# StreamMultDiffusion: Region-Based Semantic Control을 이용한 실시간 대화형 생성\n' +
      '\n' +
      'Jaerin Lee\n' +
      '\n' +
      '1ASRI, ECE 부서\n' +
      '\n' +
      '1\n' +
      '\n' +
      '다니엘성호정\n' +
      '\n' +
      '2인공지능 학제간 프로그램\n' +
      '\n' +
      '3\n' +
      '\n' +
      'Kanggeon Lee\n' +
      '\n' +
      '1ASRI, ECE 부서\n' +
      '\n' +
      '1\n' +
      '\n' +
      '이경무\n' +
      '\n' +
      '1ASRI, ECE 부서\n' +
      '\n' +
      '1\n' +
      '\n' +
      '각주 1: 이메일: {ironjr,dqj5182,dlrkdrjs97,kyoungmu}@snu.ac.kr\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '텍스트-이미지 합성에서 확산 모델의 엄청난 성공으로 인해 이미지 생성 및 편집을 위한 차세대 최종 사용자 애플리케이션의 유망한 후보가 되었다. 기존 연구들은 영역 기반 텍스트 프롬프트와 같은 새로운 세밀한 제어를 허용함으로써 추론 시간을 줄이거나 사용자 상호 작용성을 증가시켜 확산 모델의 사용성을 향상시키는 데 중점을 두었다. 그러나 우리는 두 가지 작업을 통합하는 것이 사소하지 않아 확산 모델의 잠재력을 제한한다는 것을 경험적으로 발견했다. 이러한 비호환성을 해결하기 위해, 우리는 최초의 실시간 영역 기반 텍스트-이미지 생성 프레임워크인 StreamMultiDiffusion을 제시한다. 빠른 추론 기법을 안정화하고 모델을 새롭게 제안된 _multi-prompt stream batch_ 구조로 재구성함으로써, 단일 RTX 2080 Ti GPU 상에서 영역 기반 텍스트-이미지 합성에서 \\(\\times 10\\)의 빠른 파노라마 생성과 \\(1.57\\) FPS의 생성 속도를 달성한다. 본 논문의 솔루션은 정의된 의미 의미(예:_eagle, girl)를 인코딩하여 주어진 다수의 핸드 드로잉 영역으로부터 실시간으로 고품질의 이미지를 생성하는 대화형 이미지 생성을 위한 새로운 패러다임인 _semantic palette_를 제공한다. 우리의 코드 및 데모 애플리케이션은 [https://github.com/ironjr/StreamMultiDiffusion](https://github.com/ironjr/StreamMultiDiffusion)에서 사용할 수 있다.\n' +
      '\n' +
      '키워드: 확산 모델 스트리밍 확산 모델 텍스트-이미지 생성 영역 기반 파노라마 생성\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '생성적 모델들이 언론의 주목을 받고 있다. 이 분야의 이러한 대중의 관심 수준은 이미지 생성기의 대규모 상업화로 이어지는 생성 모델의 현재 지배적인 패러다임인 확산 모델[1, 28, 29, 30, 31, 39]에 대한 언급이 없는 상태에서 설명될 수 없다. 확산 모델의 성공의 큰 크기는 텍스트 대 이미지 합성을 위한 직관적인 사용자 인터페이스뿐만 아니라 이미지의 사실적이고 고해상도 생산에 크게 기인한다[2, 8]. 좋은 전망에도 불구하고, 우리는 현재 확산 모델에서 전문가 등급의 도구를 내놓기 전에 여전히 큰 어려움을 겪고 있다. 여기에서 우리는 이러한 문제를 두 그룹으로 분류할 수 있다. (1) 대규모 확산 모델로부터의 더 빠른 추론의 필요성[23; 24; 32]과 (2) 이러한 모델에 대한 더 지능적인 제어의 필요성[37; 4; 39]. 두 목표 모두 실제 응용 프로그램에 대해 동시에 충족되어야 한다는 점에 유의하십시오.\n' +
      '\n' +
      '몇 가지 돌파구가 생겼다. 기존 연구들에서 DDIM[32]과 LCM[23; 24; 33]은 필요한 추론 단계의 수를 수천에서 수십으로 줄였다가 4로 줄였다. 또한, StreamDiffusion[15]는 최근 파이프라인 구조의 대규모 확산 모델[30]으로부터 2초 이하 추론을 시연하여 실시간 응용이 가능하게 되었다. 컨디셔닝을 다루는 후자의 작업 라인에서 ControlNet[39]과 IP-Adapter[37]은 이미지 유형 조건으로부터 생성 프로세스에 걸쳐 다양한 세밀한 제어를 시연했으며, MultiDiffusion[4]는 이미지 생성을 위한 영역 기반 텍스트 프롬프트를 가능하게 했다. 이 두 작품은 거의 직교적으로 전개되어 있다. 이는 두 가지 분야의 아이디어가 더 빠른 제어 가능한 세대를 위해 공동으로 적용될 수 있는 기회를 열어준다. 예를 들어, 이미지 형태의 제어 입력을 수신하기 위한 모델에 ControlNet[39]을 부착할 수 있는 경우, 기존의 네트워크 가중치를 조정하는 LoRA(Low-Rank Adaptation) 기법[12; 24]을 통해 LCM 스케줄링[23]으로 보다 빠른 샘플링을 실현할 수 있다.\n' +
      '\n' +
      '그러나 다른 많은 경우에는 이러한 상호 운용성이 더 좋을 수 있습니다. 예를 들어, 도 3에 도시된 바와 같이, 가속화에 MultiDiffusion[4]를 순진하게 적용하는 단계\n' +
      '\n' +
      '도 1: 개요. 우리의 StreamMultiDiffusion은 임의의 형태의 영역 기반 텍스트-이미지 생성을 위한 실시간 솔루션이다. 이 스트리밍 아키텍처는 손으로 그린 시맨틱 마스크의 온라인 사용자 명령에 기초하여 실시간으로 이미지가 생성되는 대화형 애플리케이션 프레임워크인 _semantic palette_를 가능하게 한다.\n' +
      '\n' +
      ' 감소된 추론 단계들 [24]을 갖는 확산 모델은 프롬프트 마스크들 사이의 급격한 경계들을 갖는 불일치 생성으로 이어진다. 최악의 경우, 예를 들어 그림 2의 파노라마 생성에서, 멀티 확산은 작동하지 않고 오히려 가속 기술과 함께 사용될 때 흐릿한 비-이미지들을 산출한다. 이는 다중 영역 기반 프롬프트로 이미지 생성 프로세스를 조정하기 위한 MultiDiffusion의 차선책 절차 때문이다. 컨디셔닝 프로세스는 각 지역 프롬프트에서 나타난 U-Net 잠복기를 조화시키기 위해 긴 중간 추론 단계 시퀀스에 크게 의존하기 때문에 몇 가지 추론 단계가 충분하지 않다. 본 논문의 주요 목표는 다중확산(MultiDiffusion)에 비해 제어 가능한 텍스트-이미지 합성(text-to-image synthesis)을 더 빠르게 할 수 있는 큰 개선점을 제안하는 것이다. 이를 위해 먼저 빠른 영역 기반 텍스트-이미지 합성 및 파노라마 생성을 위한 LCM 호환 프레임워크를 구축하기 위해 (1) _latent pre-averaging_, (2) _mask-centering bootstrapping_, (3) _quantized mask_의 세 가지 기법으로 MultiDiffusion을 안정화한다.\n' +
      '\n' +
      '우리 조상들이 라스코 동굴 벽에 최초의 동물 그림을 그린 이래로, 인간의 손은 항상 우리의 창조성을 탐구하는 최고의 도구였습니다. 우리는 전문 이미지 도구의 가장 중요한 요구 사항은 _handy_라고 믿는다. 즉, 확산 모델을 이상적인 도구로 만들기 위해서는 실시간 응답을 갖는 브러시 형태의 편집 인터페이스가 필요하다. streamDiffusion[15]에서 영감을 얻어, 우리는 마침내 알고리즘을 _multi-prompt로 재구성한다.\n' +
      '\n' +
      '도 2: LCM LoRA[24]를 이용한 MD[4]에 의한 파노라마 생성의 가속은 작동하지 않는다. 이 예는 현재 영역 기반 텍스트 대 이미지 합성 알고리즘과 고속 샘플링 기술 간의 비호환성을 보여준다. 크기\\(512\\times 3072\\)의 이미지는 MD의 경우 50단계, MD+LCM 및 Ours의 경우 4단계로 샘플링된다.\n' +
      '\n' +
      '스트림 배치 아키텍처_로, 영역 기반 텍스트-이미지 합성 프레임워크를 위한 스트리밍 인터페이스를 허용한다. 결과 알고리즘인 StreamMultiDiffusion은 실시간 반응형 영상 생성 및 편집 도구이다. 강력한 확산 모델의 고품질 이미지 합성 능력, 일관성 모델의 빠른 샘플링 기술, 스트리밍 가능한 파이프라인 아키텍처, 영역 기반 프롬프트 알고리즘의 높은 제어 가능성을 수용함으로써, 본 스트림멀티디퓨전은 차세대 이미지 생성 패러다임으로 새로운 의미적 드로잉 프레임워크인 _semantic palette_를 제안한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '확산모델의 가속추론\n' +
      '\n' +
      '[30, 32]에 의해 입증된 바와 같이 잠재 공간에서 작동하는 것과 같은 확산 모델은 고해상도 이미지 합성 작업에서 인상적인 성능을 보여주었다. 이러한 모델은 샘플을 점진적으로 잡음제거하기 위해 반복적인 역과정을 사용한다. 그러나 확산 모델의 두드러진 단점은 역방향 프로세스에서 상당한 수의 단계들로 인해 느린 생성 속도이며, 이는 실시간 적용성을 제한한다. [32]와 같은 초기 방법은 순방향 확산 과정을 DDPM[11]에서 비마코비안으로 확장하여 샘플링 효율을 크게 향상시키고 DDPM[11]에 비해 우수한 일관성을 나타낸다. 다른 연구에서는 추가적인 훈련 없이 ODE 해결기 [20, 21]를 향상시키기 위한 다양한 방법을 제안하여 이미지 생성을 위한 \\(10-20\\) 샘플링 단계 내에서 더 빠른 샘플링 속도를 허용한다. 후속 접근법은 주로 높은 이미지 충실도를 유지하면서 하나 또는 소수의 단계를 달성하기 위해 대형 확산 모델을 증류하는 데 중점을 둔다. 예를 들어, 일관성 정규화를 갖는 잠재 확산 모델로부터 증류된 LCM[24]은 LoRA[12]를 사용하여 효율적으로 훈련될 수 있다. Lin 등이 제안한 적대적 증류와 같은 방법은 높은 충실도와 실시간 이미지 생성기에 큰 확산 모델을 가져온다. 또한, StreamDiffusion[15]은 실시간 상호작용 이미지 생성을 위해 맞춤화된 새로운 아키텍처를 도입하여, 배치 잡음 제거 프로세스를 통해 모델 처리량을 서브-초 레벨로 증가시킨다. 이러한 발전은 스트리밍 생성 모델과 같은 이미지 생성 및 응용 분야에서 새로운 패러다임을 도입했다. 그러나, 그들은 통제 가능성의 어려움에 직면할 수 있다.\n' +
      '\n' +
      '확산 모델 제어\n' +
      '\n' +
      '이미지 변환, 페인트-투-이미지, 또는 낙서로 편집과 같은 이미지 생성 프로세스를 제어하기 위한 초기 방법은 역 프로세스[7]에서 잠재 변수를 반복적으로 정제하려고 시도했다. 이러한 방법에는 마스크[22] 주변의 픽셀 값을 샘플링하거나 조건부 참조 이미지[25]를 교란하여 사용자가 제어할 수 있는 결과를 달성하는 것이 포함되었다. 안정적 확산[30]의 도입 이후 WebUI[2]는 상당한 대중의 관심을 받았다. 또한, ControlNet[39]은 원본 이미지 생성 모델의 표현 능력을 손상시키지 않으면서 이미지의 모양, 구성, 의미 등의 측면에 대한 정밀한 제어가 가능하도록 개발되어 다양한 확장[9]과 추가 연구로 이어졌다. 최근 연구는 확산 모델을 여러 프롬프트 및 불규칙한 크기의 캔버스로 확산 모델을 확장하거나[4], 확산 모델에 대한 보다 강력하고 정밀한 제어를 보장하기 위해 텍스트 또는 이미지 프롬프트의 임베딩 능력을 향상시키는 등 다양한 제어 영역으로 확산 모델의 적용을 넓히는 방법을 도입하였다[37].\n' +
      '\n' +
      '이미지 편집 도구로서의### 확산 모델\n' +
      '\n' +
      '확산 모델을 사용한 이미지 편집에 대한 기존 방법 [10, 13, 14, 19, 25, 34, 36]은 이미지 안내 및 특정 조건을 입력으로 하여 완전히 새로운 이미지를 생성한다. 이미지 안내 및 조건부 입력과 적당히 정렬된 고품질 이미지를 생성하는 능력에도 불구하고, 생성된 이미지는 종종 원본 이미지와 매우 구별된다. 특히 텍스트 조건 중심의 이미지 편집[10, 13, 14, 19, 26]은 다양하고 사실적인 이미지 생성의 핵심 동기로 활발히 연구되고 있다. 그들의 결과들 중 일부는 특정 공간 위치들 상의 이미지 편집(예를 들어, 특정 객체를 대체하고, 인간 또는 동물의 포즈를 변경하는 것)을 보여주지만, 사용자들은 텍스트 조건들로 이미지 출력만을 편집할 수 있기 때문에, 이미지 내의 특정 공간 영역들 상의 상세한 이미지 편집에는 여전히 고유한 제한들이 존재한다. 일부 비텍스트 조건 이미지 편집 방법[34, 36]도 조건을 다른 이미지로 가지고 존재한다. 그러나, 이러한 방법들은 또한 국부화된 이미지 편집을 지원하지 않는다. 우리의 StreamMultiDiffusion과 가장 관련이 있는 작업은 SDEdit[25]이며, 이는 사용자의 스트로크 페인팅을 기반으로 이미지를 합성하고 편집한다. 그러나 이 방법은 사용자의 스트로크 그림과 원본 이미지 사이의 경계 영역에 대한 조화를 보장하기 위한 명시적인 메커니즘을 포함하지 않는다. 지역화된 이미지 편집을 보장하지 않는 기존 방법과 달리 텍스트만 이미지 편집 조건으로 안내하거나 경계 지역에 대한 조화를 지원합니다.\n' +
      '\n' +
      '그림 3: 우리의 스트림 다중 확산은 다중 확산의 안정적인 가속에 의해 빠른 영역 기반 텍스트 대 이미지 생성을 가능하게 한다[4]. PreAvg, Bstrap 및 QMask는 _latent pre-averaging_, _mask-centering bootstrapping_ 및 _quantized mask_를 나타낸다. (d), (e), (f)에서 사용된 각각의 방법은 이전 영상에서 사용된 방법을 포함한다. 이미지들은 768\\times 512\\ 크기의 단일 타일들로 샘플링된다.\n' +
      '\n' +
      '멀티디퓨전은 사용자의 스트로크 페인팅과 원본 이미지 사이의 경계 영역에 조화가 있는 국부적인 이미지 편집을 명시적으로 허용한다.\n' +
      '\n' +
      '대부분의 선구적인 작품[3, 22, 27, 35]은 확산 모델을 사용한 이미지 인페인팅에 대해 단순히 이미지 내에서 마스킹된 영역에 대해 가장 합리적인 콘텐츠를 생성한다. 작업 "인페인팅"의 관점 측면에서 이러한 방법은 이미지의 마스킹된 영역을 재생성하기 위한 강력하고 강력한 도구 역할을 한다. 이미지 인페인팅에 대한 최근 연구들[3, 27, 35]은 "인페인팅"이라는 작업이 주는 공간적 제약과 확산의 생성력을 결합하여 강력한 이미지 편집 파이프라인 역할을 한다. GLIDE[27]은 텍스트 조건 확산 모델에서 주어진 텍스트 프롬프트로 마스킹된 이미지 영역을 채우는 텍스트 유도 이미지 인페인팅 프레임워크를 제안한다. SmartBrush[35]는 텍스트와 마스크 사이의 미스얼라인 문제를 해결하기 위해 멀티모달 이미지 인페인팅을 수행한다. 혼합 확산[3]은 이미지 인페인팅 및 편집을 위한 사용자 제공 텍스트 프롬프트에 초점을 맞추기 위해 사전 훈련된 언어 이미지 모델을 사용하는 이미지 인페인팅 파이프라인으로 이미지 편집을 개발하고 시연하는 데 중점을 둔다. 우리의 StreamMultiDiffusion은 의미적 드로잉과 편집의 지원으로 사용자가 제공하는 드로잉 영역과 원본 입력 영상 간에 명시적으로 조화를 이루어 기존의 이미지 인페인팅 방법과 차이가 있다.\n' +
      '\n' +
      '## 3 StreamMultiDiffusion\n' +
      '\n' +
      '### Preliminary: MultiDiffusion\n' +
      '\n' +
      '전형적인 텍스트-조건부 이미지 확산 모델[30]\\(\\mathbf{\\epsilon}_{\\theta}:\\mathbb{R}^{H\\times W\\times D\\times\\mathbb{R}^{K}\\times[0,T]\\rightarrow\\mathbb{R}^{H\\times W\\times D\\\\)은 이미지 또는 잠재 공간[\\(\\mathbb{R}^{H\\times W\\times D\\,\\)에 정의된 가우스 잡음 추정기이다. 영상\\(\\mathbf{x}\\in\\mathbbb{R}^{H\\times W\\times D\\,\\)에 더하여, 모델은 텍스트 임베딩\\(\\mathbf{y}\\in\\mathbb{R}^{K}\\)과 샘플링 반복\\(t\\in[0,T]\\)을 조건부 입력으로서 수신하며, 여기서 \\(T\\in\\mathbbb{N}\\)은 기존의 \\(T=\\)1,000으로 설정된 유한 최대 타임스템이다. 타겟 도메인 이미지\\(\\mathbf{x}\\in\\mathbbb{R}^{H\\times W\\times D\\,\\)이 주어지면, 타임스템을 갖는 순방향 확산 과정은 잡음 영상을 생성한다.\n' +
      '\n' +
      '\\[\\mathbf{x}_{t}=\\sqrt{\\alpha(t)}\\mathbf{x}_{0}+\\sqrt{1-\\alpha(t)}\\mathbf{\\epsilon}\\,, \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(\\alpha:[0,T]\\rightarrow[0,1]\\)은 \\(\\alpha(0)=1\\) 및 \\(\\alpha(T)=0\\,\\)을 갖는 잡음 스케줄의 단조 감소 함수이다. 우리는 순방향 확산 과정을 영상\\(\\mathbf{x}_{0}\\)과 단위 백색 가우시안 잡음 텐서\\(\\mathbf{\\epsilon}\\sim\\mathcal{N}(0,1)^{H\\times W\\times D}\\,\\) 사이의 (스케일링된) 선형 보간으로 간주할 수 있다. 확산 모델\\(\\mathbf{\\epsilon}_{\\theta}\\)은 주어진 잡음 영상\\(\\mathbf{x}_{t}\\,\\)으로부터 잡음 성분\\(\\mathbf{\\epsilon}\\)을 추정하도록 학습되며, 여기서 텍스트 조건\\(\\mathbf{y}\\)과 시간 단계\\(t\\)은 추측, _i.e._,\\(\\mathbf{\\epsilon}_{\\theta}(\\mathbf{\\epsilon}_{t},t,\\mathbf{y})\\approx\\mathbf{\\epsilon}\\,\\). 생성 시간에 미리 학습된 확산 모델\\(\\mathbf{\\epsilon}_{\\theta}\\)은 조건 입력\\(\\mathbf{y}\\,\\)에 기술된 정보와 상관관계가 있는 순수 잡음\\(\\mathbff{x}_{T}\\sim\\mathcal{N}(0,1)^{H\\times W\\times D}\\)으로부터 이미지\\(\\hat{\\mathbf{x}_{0}\\approx\\mathbf{x}_{0}\\)을 추정하기 위해 여러 번 상담된다. 재성형 방정식 (1)은 다음과 같이 원본 이미지의 간단한 추정량 \\(\\mathbf{x}_{0}\\)을 제공한다.\n' +
      '\n' +
      '[38] \\(t\\)이 가우스 잡음 제거 문제의 비포즈성 때문에 \\(\\mathbf{x}_{x}_{0|t}=\\frac{1}{\\sqrt{\\alpha(t)}\\bm{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{y})\\approx\\mathbff{x}_{0}\\,\\tag{2}\\]에 가까운 경우, 이 직접 1단계 추론은 일반적으로 \\(\\mathbf{x}_{0}\\)의 매우 흐릿한 추정치를 생성한다.\n' +
      '\n' +
      '\\textsc{MultiStepSampler}(\\hat{\\mathbf{x}}_{0}=\\textsc{MultiStepSampler}(\\mathbf{x}_{T},\\mathbf{y};\\mathbf{\\epsilon}_{\\theta},\\alpha,\\mathbf{t})\\approx\\mathbf{x}_{0}\\,\\tag{3}\\textsc{MultiStepSampler}(\\hat{\\mathbf{x}_{T},\\mathbf{y};\\mathbf{\\epsilon}_{\\theta},\\alpha,\\mathbf{t})\\approx\\mathbf{x}_{0}\\,\\tag{3}\\textsc{MultiStepSampler}(\\mathbf{x}_{t})\n' +
      '\n' +
      '여기서 \\(\\mathbf{t}=\\{t_{1},\\ldots,t_{n}\\}\\subset\\{0,1,\\ldots T\\\\subset\\mathbb{N}\\)은 일반적으로 \\(T-1\\)에서 시작하는 단조적으로 감소하는 타임스텝들의 시퀀스를 정의하는 중간 타임스텝들의 집합이다. 이들 알고리즘들 각각은 단일-단계 역확산에 대한 재귀 호출로서 표현될 수 있다.\n' +
      '\n' +
      '\\textsc{Step}(\\mathbf{x}_{t_{i-1}}=\\textsc{Step}(\\mathbf{x}_{t_{i}},\\mathbf{y},i,\\mathbf{\\epsilon};\\mathbf{\\epsilon}_{\\theta},\\alpha,\\mathbf{t}})\\,, \\tag{4}\\\n' +
      '\n' +
      '이는 식 (2)와 식 (1)의 조합이다. 여기서, 우리는 현재 시간 단계 \\(t_{i}\\)의 인덱스로 \\(i\\)을 나타낸다. 선택적으로 추가된 잡음\\(\\mathbf{\\epsilon}\\)은 명시적으로 명시되어 있다.\n' +
      '\n' +
      '식 (3)과 식 (4)에 제시된 고수준 알고리즘은 조건부 확산 모델[11, 20, 32]에 대한 거의 모든 생성 방법을 포함하고 있지만, 실제 수요가 많은 경우를 고려하지 않는다. 이미지의 원하는 모양(\\hat{\\mathbf{x}}_{0}^{\\prime}\\in\\mathbbb{R}^{H^{\\prime}\\times W^{\\prime}\\times D}\\)이 학습 집합(\\(H\\times W\\))과 다른 텍스트 프롬프트(\\(\\mathbf{y}_{1},\\ldots,\\mathbf{y}_{p}\\)가 생성된 이미지의 다른 영역과 상관관계가 있는 경우. 이러한 문제는 MultiDiffusion 프레임워크[4]에서 다루어지는 문제이며, 주요 아이디어는 샘플링 단계 \\(t_{i}\\)마다 특징을 평균하여 중간 래턴의 겹칠 수 있는 여러 타일들을 통합하는 것이다.\n' +
      '\n' +
      '\\textsc{x}_{t_{i-1}}^{\\prime}(\\mathbf{x}_{t_{i}}^{\\prime},\\mathbf{y},i,\\mathcal{W};\\textsc{Step}\\\\frac{1}{\\sum_{\\mathbf{w}\\mathbf{w}\\mathcal{W}\\textsc{Step}(\\texttt{crop}(\\mathbf{w}\\odot\\mathbf{x}_{t_{i}}^{\\prime},i;\\mathbf{y}_{\\theta},\\tag{6}\\mathbf{t}}}^{\\prime},\\frac{1}{\\sum_{\\mathbf{w}\\mathbf{w}\\mathcal{W}\\textsc{Step}(\\texttt{crop}(\\mathbf{w}\\odot\\mathbf{x}_{i}}^{\\prime},\\mathbf{y}\n' +
      '\n' +
      '여기서 \\(\\odot\\)은 요소별 곱셈이고, \\(\\mathbf{w}\\subset\\{0,1\\}^{H^{\\prime}\\times W^{\\prime}\\)은 각 잠재 타일에 대한 이진 마스크이며, \\(\\mathbf{y}_{\\mathbf{w}\\in\\mathbbb{R}^{K}\\)는 학습 영상과 동일한 크기의 타일에 가능한 \\(\\mathbf{x}_{t_{i}}^{\\prime}\\times W\\times D\\times\\mathbb{R}^{H^{\\prime}\\times W\\times D\\times\\mathcrop}:\\mathbbb{R}^{H^{\\prime}\\times W\\times\\times 우리의 알고리즘과 기준선 MultiDiffusion [4] 사이의 자세한 알고리즘 차이는 부록 0.A에서 논의된다.\n' +
      '\n' +
      '### Stabilizing MultiDiffusion\n' +
      '\n' +
      '불행하게도, 단순히 SD(Stable Diffusion) 모델[30]을 LCM(Latent Consistency Model)[23]으로 교체하고, 디폴트 DDIM 샘플러[32]를 LCM 샘플러[23]로 교체하는 것은 더 빠른 MultiDiffusion으로 이어지지 않는다. 이러한 비호환성은 _both_LCM 및 MultiDiffusion의 잠재적 적용을 크게 제한한다. 우리는 각각의 원인에 대해 논의하고 더 빠르고 더 강력한 대안을 모색한다.\n' +
      '\n' +
      '**단계 1:잠재적 사전-평균화와의 LCM 호환성.** 도 2의 흐릿한 파노라마 이미지에 대한 일차적인 이유는 원래의 MultiDiffusion 알고리즘이 기저 역확산 단계 함수들의 상이한 타입들을 고려하지 않기 때문이다. 기존의 역확산 알고리즘은 (1) 모든 단계에서 새롭게 샘플링된 잡음을 레이턴트에 추가하는 알고리즘과 (2) 잡음을 추가하지 않고 레이턴트만을 잡음제거하는 두 가지로 분류할 수 있다. 예를 들어, LCM을 위한 디폴트 샘플러 [23]인 DPM-Solver [20]은 전자의 클래스의 인스턴스이고, MultiDiffusion을 위한 디폴트 샘플러 [4]인 DDIM [32]는 후자의 예이다. MultiDiffusion 알고리즘은 중간 잠복기를 평균화하는 것에 의존하기 때문에, MultiDiffusion을 적용하는 것은 단계에서 즉시 추가된 잡음을 효과적으로 상쇄하고, 따라서 품질을 저하시킨다. 간단한 해결 방법으로 이 문제를 피할 수 있습니다. 먼저 단계 함수를 결정론적 잡음 제거 부분과 선택적 잡음 추가 부분으로 나눈다.\n' +
      '\n' +
      '\\textsc{StepExceptNoise}(\\mathbf{x}_{t_{i-1}}+\\eta_{t_{i-1}}\\mathbf{\\epsilon}\\tag{7}\\textsc{StepExceptNoise}(\\mathbf{x}_{t_{i}},\\mathbf{y},i;\\mathbf{\\epsilon}_{\\theta},\\alpha,\\mathbf{t})+\\eta_{t_{i-1}\\mathbf{\\epsilon}\\textsc{StepExceptNoise}(\\mathbf{x}_{t_{i-1}}+\\eta_{t_{i-1}}\\textsc{StepExceptNoise}(\\mathbf{x}_{t_{i}},\\mathbf{y},i;\\mathbf{\\epsilon}_{\\theta},\\alpha,\\mathbf{t})+\\eta_{t_{i-1}\\mathb\n' +
      '\n' +
      '여기서 \\(\\eta_{t}\\)는 알고리즘 종속 파라미터이다. 그 후 식 (6)의 평균 연산은 풀 스텝의 출력 대신 잡음 제거 부분 \\(\\tilde{\\mathbf{x}}_{t_{i-1}}\\)의 출력에 적용된다. 다중 확산 단계 후에 노이즈가 추가됩니다.\n' +
      '\n' +
      '\\textsc{MultiDiffusionStep}(\\mathbf{x}^{\\prime}_{t_{i-1}}=\\textsc{MultiDiffusionStep}(\\mathbf{x}^{\\prime}_{t_{i}},\\mathbf{y},i,\\mathcal{W};\\textsc{StepExceptNoise})+\\eta_{t_{i-1}}\\mathbf{ \\epsilon}\\,.\\tag{9}\\textsc{y}}\n' +
      '\n' +
      '그림 2에서 볼 수 있듯이 이 알고리즘의 작은 변화는 파노라마와 같은 불규칙한 크기의 이미지를 생성하는데 있어서 호환성 문제를 해결한다.\n' +
      '\n' +
      '그림 4: 부트스트래핑 전략 개요. 우리는 영역 기반 텍스트 대 이미지 생성의 정확하고 빠른 샘플링을 구축하기 위해 다중 확산[4]을 안정화한다. 부트스트래핑 및 마스크 양자화 전략의 효과는 그림 3과 같다. 부트스트래핑, 센터링 및 미센터링은 생성 프로세스에서 처음 몇 단계(1-3)에만 적용되는 반면, 다중 확산 집계는 처음부터 끝까지 적용된다. 독자는 또한 부록 A에서 알고리즘 2의 엄격한 표기법을 참조할 수 있다.\n' +
      '\n' +
      '**단계 2: 마스크-밀착 합성을 위한 부트스트래핑 재사고.** 비호환성의 두 번째 원인은 원래 마스크-밀착 영역 기반 텍스트-이미지 합성을 위해 제안된 MultiDiffusion [4]의 부트스트래핑 단계에 있다. MultiDiffusion의 추론 단계 중 첫 번째 (40\\%\\)에서는 각 마스킹된 잠재 타일을 임의의 일정한 색상을 갖는 배경과 혼합한다. 즉, 식 (6)에서 \\(\\texttt{crop}(\\mathbf{w}\\odot\\mathbf{x}_{t_{i}}^{\\prime})\\)을 \\(\\texttt{crop}(\\mathbf{w}\\odot\\mathbf{x}_{t_{i}}^{\\prime}+(\\mathbf{1}-\\mathbf{w})\\odot\\mathbf{c})\\(i<0.4n\\,\\)에 대한 \\(\\texttt{crop}(\\mathbf{w}\\odot\\mathbf{x}_{t_{i}}^{\\prime}+)으로 대체한다. 여기서, \\(\\mathbf{c}=\\texttt{enc}(c\\mathbf{1})\\)은 확산 모델의 잠재 인코더인 \\(\\texttt{enc}(\\cdot)\\), \\(c\\sim\\mathcal{U}(0,1)^{3}\\), 그리고 \\(\\mathbbb{R}^{H\\times W\\times 3}\\,\\)을 갖는 임의의 일정한 색상 배경의 잠재 임베딩이다. 부트스트래핑 배경색 \\(c\\)은 생성 결과에 대한 \\(c\\)의 침입을 무효화하기 위해 각 마스크와 각 단계에 대해 무작위로 선택된다. 그러나 타임스텝의 수를 \\(n=50\\) 스텝에서 \\(n=4\\) 또는 \\(5\\) 스텝으로 10배 감소시킴에 따라 부트스트래핑의 수 또한 감소하여 \\(c\\,\\) 스텝에서 원하지 않는 정보의 상쇄 효과를 제한한다. 그 결과 그림 4와 같이 랜덤 컬러가 최종 이미지에 부과되며, 랜덤 컬러\\(c\\,\\)의 대안으로 백색 배경\\(c=1\\)을 사용하여 적은 수의 샘플링 단계에서 출력 이미지의 품질을 효과적으로 보존한다는 것을 발견했다. 우리는 그림 4와 같이 흰색 배경과 실제 배경을 점진적으로 혼합하는 것이 모델이 다른 프롬프트에 의해 생성된 외부 영역에 적응하는 데 도움이 된다는 것을 경험적으로 발견했다.\n' +
      '\n' +
      '더욱이, 적은 수의 샘플링 단계를 갖는 이 경우, 처음 몇 개의 단계는 생성된 이미지의 전체 구조의 행렬식이다. 단일 단계 후에도 네트워크는 그림 4에서 _fg_로 표시된 이미지 패치에서 볼 수 있듯이 생성 중인 객체의 전체 형태를 결정한다. 이러한 빠른 수렴은 이점뿐만 아니라 다중 확산의 마스킹된 집합과 결합될 때 새로운 문제를 수반한다[4]. 즉, 가속 방법을 적용할 경우 다중 확산의 초기 단계에서 오프 중심 객체가 마스킹되는 경우가 많다. 결과적으로 최종 결과는 종종 작고 중심적이지 않은 지역 프롬프트를 무시한다. 따라서 우리는 화이트 마스크 부트스트래핑 외에 _centering_ 단계를 제안한다.\n' +
      '\n' +
      '세대의 처음 몇 단계(1-3)에 대해, 각각의 프롬프트로부터의 중간 세대는 U-Net에 의해 취급되기 전에 프레임의 중심으로 이동된다. 확산 U-Net의 중심 선호도를 이용하여, 최종적으로 다수의 영역 기반 프롬프트로부터 빠른 이미지 샘플링을 위한 안정적인 부트스트래핑 알고리즘을 얻는다.\n' +
      '\n' +
      '단계 3: Seamless 생성을 위한 Quantized Mask.MultiDiffusion [4]의 감소된 추론 단계들의 수로부터 나타난 또 다른 문제는 각각의 개별적으로 라벨링된 영역들이 단일 이미지로 조화되지 않는다는 것이다. 그림 3에서 볼 수 있듯이 생성된 영역 사이에 급격한 경계가 보인다. 원래의 MultiDiffusion에서는 (50) 역확산 단계가 연속적으로 잡음을 추가하고 이를 흐리게 함으로써 마스크 경계를 효과적으로 매끄럽게 하는 문제가 존재하지 않는다. 그러나, 본 논문에서 제안하는 빠른 추론 기법에서는 경계선을 흐리게 할 수 있는 충분한 수의 단계를 제공하지 못한다. 따라서, 우리는 서로 다른 텍스트 프롬프트 및 마스크의 생성 영역을 매끄럽게 병합하는 대안적인 방법을 개발한다: _양자화된 마스크_를 도입함으로써. 이진 마스크가 주어지면 가우시안 블러(Gaussian blur)를 적용하여 평활화된 버전을 얻는다. 그리고 확산 샘플러의 잡음 레벨에 의해 실수 마스크를 양자화한다. 그림 4에서 알 수 있듯이 각 잡음 제거 단계는 해당 잡음 수준을 가진 마스크를 사용한다. 알고리즘이 반복될수록 잡음 레벨이 단조롭게 감소하기 때문에, 마스크의 커버리지는 점진적으로 증가한다. 시맨틱 마스크의 이러한 이완은 또한 전문 그래픽 편집 소프트웨어에서 가장 널리 사용되는 도구 중 하나인 _brushes_에 대한 직관적인 해석을 제공한다. 우리는 5절에서 이 해석을 재검토할 것이다.\n' +
      '\n' +
      '### Streaming Pipeline\n' +
      '\n' +
      '섹션 1에서 언급한 바와 같이, 우리는 실시간 응답을 달성하는 것이 최종 사용자 적용에 중요하다고 믿는다. streamDiffusion에서 영감을 받아 [15], 우리는 지역 기반 텍스트-이미지 합성 프레임워크를 파이프라인 구조로 수정한다.\n' +
      '\n' +
      '도 7: StreamMultiDiffusion 파이프라인 아키텍처. 단일 배치에서 서로 다른 타임스텝에서 레이턴트를 집계함으로써 대기 시간을 숨김으로써 처리량을 최대화할 수 있다.\n' +
      '\n' +
      '이미지 생성의 처리량을 최대화합니다. 코드 및 데모 애플리케이션은 [https://github.com/ironjr/StreamMultiDiffusion](https://github.com/ironjr/StreamMultiDiffusion)에서 사용할 수 있다.\n' +
      '\n' +
      '**아키텍처.** 도 7은 우리의 파이프라인의 아키텍처 및 인터페이스를 예시한다. _ 스트림 batch_는 모델의 처리량을 최대화하기 위해 Kodaira _et al._[15]에 의해 처음 제안된 새로운 형태의 확산 모델 아키텍처이다. 동기화된 타임스텝을 갖는 확산 모델의 전형적인 미니빗치 사용 대신에, 잡음 추정 U-Net은 이미지의 마지막 처리된 배치와 함께 매 타임스텝마다 새로운 입력 이미지를 공급받는다. 따라서 미니 배치의 각 이미지는 서로 다른 타임스텝을 갖는다. 이 아키텍처 수정은 식 (3)에서 역확산의 다단계 알고리즘으로 인한 대기 시간을 숨긴다. 이 기능을 영역 기반 텍스트 대 이미지 합성 프레임워크에 적용하기 위해 단일 이미지 대신 그림 7과 같이 모든 타임스텝에서 노이즈 추정 U-Net에 서로 다른 프롬프트와 마스크의 이미지를 미니 배치한다.\n' +
      '\n' +
      '**처리량 최적화** 텍스트 프롬프트 및 선택적 배경 이미지 전처리는 노이즈 제거 프로세스에 비해 상당한 지연이 발생한다는 것을 발견한다. 따라서 이러한 처리 단계를 생성 파이프라인에서 멀리하는 것이 좋습니다. Tiny AutoEncoder[5]와 같은 압축된 오토인코더를 사용함으로써 처리량의 추가적인 증가가 달성될 수 있다. 처리량 최적화의 효과에 대한 자세한 분석은 표 2에 나와 있다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '이 섹션은 스트림멀티디퓨전으로부터 추가적인 양적 및 질적 결과를 제공한다. 본 논문에서는 영역 기반 텍스트-이미지 생성과 임의의 형태 생성(예: 파노라마)의 속도를 가속화하기 위한 접근법의 타당성을 입증하는 데 중점을 둔다. 우리의 방법은 안정적인 확산 v1.5[30]에서 구현된 LCM LoRA[24]의 사용에 의존하기 때문에 우리의 실험은 이 특정 버전의 공개 체크포인트를 기반으로 한다. 그러나 우리는 우리의 방법이 커뮤니티에서 생성된 모든 체크포인트에 적용될 수 있음을 주목한다. B절에서 더 많은 결과를 찾을 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline Method & \\multicolumn{2}{c}{Throughput (FPS) Relative Speedup} \\\\ \\hline MultiDiffusion & 0.0189 & \\(\\times 1.0\\) \\\\ \\hline Ours without Stream Batch & 0.183 & \\(\\times 9.7\\) \\\\ + Multi-Prompt Stream Batch & 1.38 & \\(\\times 73.0\\) \\\\ + Tiny AutoEncoder [5] & **1.57** & \\(\\times\\)**83.1** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 단일 RTX 2080 Ti로 측정된, 처리량 최적화 기술에 대한 정제.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline Method & \\multicolumn{2}{c}{Throughput (FPS) Relative Speedup} \\\\ \\hline MultiDiffusion & 0.0189 & \\(\\times 1.0\\) \\\\ \\hline Ours without Stream Batch & 0.183 & \\(\\times 9.7\\) \\\\ + Multi-Prompt Stream Batch & 1.38 & \\(\\times 73.0\\) \\\\ + Tiny AutoEncoder [5] & **1.57** & \\(\\times\\)**83.1** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 영역 기반 생성의 충실도 및 속도.\n' +
      '\n' +
      '### 안정화 성능 평가\n' +
      '\n' +
      '파노라마 세대 4.1.1 파노라마 세대\n' +
      '\n' +
      '그림 2와 8은 불규칙한 크기의 이미지, 즉 파노라마 생성에 대한 정성적 결과를 보여준다. 텍스트-대-파노라마 생성의 예시들을 생성하기 위해, 우리는 \\(9:1\\,\\)의 종횡비를 갖는 \\(512\\times 4608\\) 파노라마 세트를 샘플링한다. 우리는 LCM LoRA[24]를 사용하여 다중확산(MD) [4] 기준선과 순가속도를 비교한다. 안정적인 확산 버전 1.5에 대한 LCM LoRA는 출력 이미지의 색상 변화를 이동시키는 스타일 정보를 포함하는 커뮤니티 생성 체크포인트로부터 생성된다는 점에 유의한다. 그림 2와 8에서 알 수 있듯이, 우리의 안정화 기법은 너무 많은 품질을 저하시키지 않고 추론 속도의 \\(\\배 10\\) 이상을 가속한다.\n' +
      '\n' +
      '######4.1.2 지역기반 프롬프트\n' +
      '\n' +
      '다음으로, 다중확산(MultiDiffusion, MD)의 기준선에서 시작하여 영역 기반 텍스트 대 이미지 생성을 평가하고 안정화 기법의 각 구성 요소를 추가한다. 그림 3과 9는 정성적 절제 연구를 보여주고 표 1은 정량적 결과를 보여준다. COCO 데이터셋[18]을 사용하여 미리 훈련된 분할 모델[6]에 의해 추론된 GT 마스크와 IoU(교차 교차 결합) 점수로 측정된 영역 기반 텍스트 대 이미지 합성의 마스크 충실도를 비교한다. 이것은 MultiDiffusion[4]에서 행해진 것과 같은 조달이다. 결과는 우리의 세 가지 안정화 기술이 LCM[24]과 다중확산[4] 사이의 비호환성 문제를 완화하는 데 도움이 된다는 것을 보여준다.\n' +
      '\n' +
      '그림 8: 텍스트 대 파노라마 세대의 비교. 우리의 StreamMultiDiffusion은 몇 초 만에 고해상도 이미지를 합성할 수 있습니다. 보충 자료에 더 많은 예가 나와 있다.\n' +
      '\n' +
      '### Speed Comparison\n' +
      '\n' +
      '마지막으로, 표 2에서 처리량 최적화의 효과를 측정하며, 빠른 추론 영역에서 다중 확산을 안정화함으로써 달성한 속도 향상은 _multi-prompt stream batch_ 구조를 통해 더욱 가속화된다. 저메모리 오토인코더[5]를 도입하여 실시간 다중 텍스트-이미지 생성의 기반인 1.57 FPS를 달성할 수 있었다. 대용량 영상 생성과 다중 영역 기반 텍스트-이미지 생성의 속도는 부록 0.B에서 예를 보인 바와 같이 상호 보완적이다. 특히, 그림 11에서는 원본 다중확산 [4]와 가속 알고리즘 모두에 대해 _nine_영역 할당 텍스트 프롬프트로부터 \\(768\\times 1920\\) 이미지를 생성함으로써 스트레스 테스트를 수행한다. 제안된 알고리즘은 기준선에 비해 현저한 \\(\\times 52.5\\)의 속도 향상을 보인다. 흥미롭게도, 본 알고리즘은 MultiDiffusion[4]보다 빠를 뿐만 아니라 그림 11과 그림 12에서 명확하게 볼 수 있는 것처럼 더 높은 마스크 충실도를 달성한다. 본 알고리즘의 속도와 제어 충실도는 모든 이미지 확산 모델에서 다양한 이미지 생성 도구를 지원할 수 있음을 의미한다. 이는 다음 섹션으로 이어지며, 스트리밍 알고리즘을 기반으로 한 사용자 애플리케이션을 제안합니다.\n' +
      '\n' +
      '## 5 Discussion\n' +
      '\n' +
      '### Semantic Palette\n' +
      '\n' +
      '스트림멀티디퓨전(streamMultiDiffusion)의 실시간 인터페이스는 이미지 생성을 위한 사용자-대화형 애플리케이션의 새로운 패러다임을 열어주는데, 이를 _semantic palette_라고 한다. 우리는 이 프레임워크의 주요 특징과 가능한 응용 프로그램에 대해 논의한다.\n' +
      '\n' +
      '#### 5.1.1 Concept.\n' +
      '\n' +
      '스트리밍 파이프라인에 의해 활성화된 반응 영역 기반 텍스트 대 이미지 합성은 사용자가 도면과 유사하게 프롬프트 마스크를 편집할 수 있게 한다. 섹션 3.3에서 논의된 바와 같이 무거운 전처리를 필요로 하는 마스크가 아닌 프롬프트이기 때문에, 마스크 수정 피드백은 생성된 이미지에 따라 그들의 명령을 반복적으로 변경하도록 사용자에게 즉시 주어질 수 있다. 다시 말해, 프롬프트가 애플리케이션에 의해 전처리되면, 사용자는 채색된 브러시로 도면을 그릴 수 있는 것과 마찬가지로 _text 프롬프트로 _paint_를 할 수 있으므로, 이름: _semantic palette_이다.\n' +
      '\n' +
      '#### 5.1.2 샘플 응용 디자인.\n' +
      '\n' +
      '이것은 _semantic palette_를 구현하는 샘플 응용 프로그램에 대한 간략한 설명이다. 스크린샷 및 설계 도식은 그림 10에 나와 있다. 애플리케이션은 프론트엔드 사용자 인터페이스와 StreamMultiDiffusion을 실행하는 백엔드 서버로 구성된다. 각각의 사용자 입력은 배경 이미지, 텍스트 프롬프트들, 마스크들, 및 텍스트 프롬프트들 및 혼합 비율들 및 블러 강도들과 같은 마스크들에 대한 조정가능한 옵션들의 수정이다. 프롬프트 또는 배경의 변경과 같은 전처리 단계를 필요로 하는 주요 변경을 명령할 때, 백-엔드 파이프라인은 새롭게 주어진 컨텍스트로 플러싱되고 재초기화된다. 그렇지 않으면 파이프라인은 그림 9: 영역 기반 텍스트 대 이미지 합성 결과입니다. 안정화 방법은 품질을 유지하면서 다중확산[4]을 최대 10배까지 가속한다.\n' +
      '\n' +
      '생성된 이미지 스트림을 얻기 위해 반복적으로 호출된다. 사용자는 먼저 배경 이미지를 선택하고 긍정적인 텍스트 프롬프트와 부정적인 텍스트 프롬프트의 쌍을 입력하여 _semantic palette_를 생성한다. 그런 다음 사용자는 익숙한 브러시 도구, 형상 도구 또는 페인트 도구로 생성된 팔레트에 대응하는 마스크를 그릴 수 있다. 애플리케이션은 사용자 입력에 따라 합성 이미지 스트림을 자동으로 생성한다. 사용자는 또한 스트림을 일시 중지하거나 재개하거나, 이미지를 하나씩 또는 일괄적으로 생성할 수 있다. 핵심 개념을 보여주는 당사의 기술 데모 애플리케이션이 코드와 함께 출시됩니다.\n' +
      '\n' +
      '### Limitation\n' +
      '\n' +
      '우리의 솔루션은 잠재 일관성 모델[23, 24, 33], StreamDiffusion[15], MultiDiffusion[4]에 의해 만들어진 돌파구를 효과적으로 통합하지만 몇 가지 알려져 있는 한계가 있다. 첫째, 우리의 솔루션은 여전히 역확산의 몇 단계(4~6)를 필요로 한다. 둘째, 백색 배경 영상을 이용한 1단계 부트스트래핑으로 영역 기반 텍스트-이미지 생성의 마스크 충실도를 향상시켰지만, 완벽한 피팅은 불가능하다. 그럼에도 불구하고, 우리의 주요 목표는 불규칙한 크기의 캔버스에서 영역 기반 텍스트 대 이미지 합성을 위한 첫 번째 스트리밍 애플리케이션을 시연하는 것이었기 때문에 향후 작업을 위해 이러한 개선을 남겨두고자 한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '우리는 이미지 생성과 편집을 위한 빠르고 강력한 대화형 도구인 StreamMultiDiffusion을 제안했다. 잠재적 일관성 모델과 MultiDiffusion 간의 호환성 문제를 해결하고, StreamDiffusion 파이프라인에서 아키텍처적 돌파구를 병합하여 전문적인 사용을 위한 실시간 상호작용성이 높은 이미지 생성 시스템을 제안하였다. 본 논문에서 제안하는 StreamMultiDiffusion은 보다 빠른 파노라마 영상을 생성할 수 있을 뿐만 아니라, 영상 확산 모델의 2차 실시간 영상 합성 패러다임에 가장 적합한 새로운 형태의 사용자 인터페이스인 _semantic palette_를 제공한다.\n' +
      '\n' +
      '도 10: 우리의 StreamMultiDiffusion 알고리즘에 의해 인에이블되는 _semantic palette_를 입증하는 샘플 애플리케이션. 사용자는 프롬프트와 선택적 배경 이미지를 등록한 후 텍스트 프롬프트로 그림을 그려 실시간으로 이미지를 만들 수 있습니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: GPT-4 기술 보고서. arXiv preprint arXiv:2303.08774 (2023)\n' +
      '* [2] AUTOMATIC1111: Stable diffusion WebUI. [https://github.com/AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui) (2022)\n' +
      '* [3] Avrahami, O., Lischinski, D., Fried, O.: Blended diffusion for text-driven editing of natural images. In: CVPR (2022)\n' +
      '* [4] Bar-Tal, O., Yariv, L., Lipman, Y., Dekel, T.: MultiDiffusion: Fusing diffusion paths for controlled image generation. arXiv preprint arXiv:2302.08113 (2023)\n' +
      '* [5] Bohan, O.B.: Tiny autoencoder for stable diffusion. [https://github.com/madebyollin/taesd](https://github.com/madebyollin/taesd) (2023)\n' +
      '* [6] Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention mask transformer for universal image segmentation. In: CVPR (2022)\n' +
      '* [7] Choi, J., Kim, S., Jeong, Y., Gwon, Y., Yoon, S.: ILVR: Conditioning method for denoising diffusion probabilistic models. In: ICCV (2021)\n' +
      '* [8] comfyanonymous: ComfyUI. [https://github.com/comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI) (2023)\n' +
      '* [9] Guo, Y., Yang, C., Rao, A., Liang, Z., Wang, Y., Qiao, Y., Agrawala, M., Lin, D., Dai, B.: AnimateDiff: Animate your personalized text-to-image diffusion models without specific tuning. In: ICLR (2023)\n' +
      '* [10] Hertz, A., Mokady, R., Tenenbaum, J., Aherman, K., Pritch, Y., Cohen-or, D.: Prompt-to-prompt image editing with cross-attention control. In: ICLR (2022)\n' +
      '* [11] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS (2020)\n' +
      '* [12] Hu, E.J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.: LoRA: Low-rank adaptation of large language models. In: ICLR (2021)\n' +
      '* [13] Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani, M.: Imagic: Text-based real image editing with diffusion models. In: CVPR (2023)\n' +
      '* [14] Kim, G., Kwon, T., Ye, J.C.: DiffusionCLIP: Text-guided diffusion models for robust image manipulation. In: CVPR (2022)\n' +
      '* [15] Kodaira, A., Xu, C., Hazama, T., Yoshimoto, T., Ohno, K., Mitsuhori, S., Sugano, S., Cho, H., Liu, Z., Keutzer, K.: StreamDiffusion: A pipeline-level solution for real-time interactive generation. arXiv preprint arXiv:2312.12491 (2023)\n' +
      '* [16] Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In: ICML (2023)\n' +
      '* [17] Lin, S., Wang, A., Yang, X.: SDXL-Lightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929 (2024)\n' +
      '* [18] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV (2014)\n' +
      '* [19] Liu, X., Park, D.H., Azadi, S., Zhang, G., Chopikyan, A., Hu, Y., Shi, H., Rohrbach, A., Darrell, T.: More control for free! Image synthesis with semantic diffusion guidance. In: WACV (2023)\n' +
      '* [20] Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., Zhu, J.: DPM-Solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In: NeurIPS (2022)\n' +
      '*[*[21] Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., Zhu, J.: DPM-Solver++: 확산 확률 모델의 유도 샘플링을 위한 고속 솔버. arXiv preprint arXiv:2211.01095 (2022)\n' +
      '* [22] Lugmayr, A., Danelljan, M., Romero, A., Yu, F., Timofte, R., Van Gool, L.: Re-Paint: Inpainting using denoising diffusion probabilistic models. In: CVPR (2022)\n' +
      '* [23] Luo, S., Tan, Y., Huang, L., Li, J., Zhao, H.: Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378 (2023)\n' +
      '* [24] Luo, S., Tan, Y., Patil, S., Gu, D., von Platen, P., Passos, A., Huang, L., Li, J., Zhao, H.: LCM-LoRA: A universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556 (2023)\n' +
      '* [25] Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.Y., Ermon, S.: SDEdit: Guided image synthesis and editing with stochastic differential equations. In: ICLR (2022)\n' +
      '* [26] Mokady, R., Hertz, A., Aherman, K., Pritch, Y., Cohen-Or, D.: Null-text inversion for editing real images using guided diffusion models. In: CVPR (2023)\n' +
      '* [27] Nichol, A.Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., Mcgrew, B., Sutskever, I., Chen, M.: GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. In: ICML (2022)\n' +
      '* [28] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 (2022)\n' +
      '* [29] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: Zero-shot text-to-image generation. In: ICML (2021)\n' +
      '* [30] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR (2022)\n' +
      '* [31] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. In: NeurIPS (2022)\n' +
      '* [32] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: ICLR (2020)\n' +
      '* [33] Song, Y., Dhariwal, P., Chen, M., Sutskever, I.: Consistency models. In: ICML (2023)\n' +
      '* [34] Su, X., Song, J., Meng, C., Ermon, S.: Dual diffusion implicit bridges for image-to-image translation. In: ICLR (2022)\n' +
      '* [35] Xie, S., Zhang, Z., Lin, Z., Hinz, T., Zhang, K.: SmartBrush: Text and shape guided object inpainting with diffusion model. In: CVPR (2023)\n' +
      '* [36] Yang, B., Gu, S., Zhang, B., Zhang, T., Chen, X., Sun, X., Chen, D., Wen, F.: Paint by example: Exemplar-based image editing with diffusion models. In: CVPR (2023)\n' +
      '* [37] Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: IP-Adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023)\n' +
      '* [38] Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising. IEEE TIP (2017)\n' +
      '* [39] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: ICCV (2023)\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:18]\n' +
      '\n' +
      '우리의 담론의 완전성을 위해, 우리는 3.2절에서 원래의 다중확산[4]과 안정화된 버전을 알고리즘적으로 비교한다. 알고리즘 1은 우리의 기본 모델인 원래의 다중확산의 상세한 알고리즘을 보여준다. 다중 확산의 주요 기여는 17행의 평균 집계 단계이다. 섹션 3에서 논의한 바와 같이 집계 단계의 부적절한 배치와 부트스트래핑 전략의 강한 간섭은 일관성 모델과 같은 보다 최근의 빠른 추론 알고리즘과 다중 확산 알고리즘의 호환성을 제한한다[23, 24, 33]. 따라서 이 두 작업 간의 호환성을 얻기 위해 9-12번 라인의 부트스트래핑 단계와 13번 라인의 확산 업데이트 단계를 변경하는 데 중점을 둔다.\n' +
      '\n' +
      '알고리즘 2는 섹션 3.2에서 개발된 보다 안정적인 버전의 다중확산 대안이며, 기존 다중확산과의 접근 방식 간의 차이점은 파란색으로 표시된다. 먼저, 10행에서 부트스트랩 \'``을 변경한다.\n' +
      '입력: 확산 모델\\(\\mathbf{\\epsilon}_{\\theta}), 잠재 오토인코더\\(\\texttt{enc},\\texttt{dec}\\right),\\) 프롬프트 임베딩\\(\\mathbf{y}_{1:p}\\), 양자화된 마스크\\(\\mathbf{w}_{1:p}^{\\left(t_{1:n}\\right), timesteps\\(\\mathbf{t}=t_{1:n}\\), 출력 크기\\(\\left(H^{\\prime},W^{\\prime}\\right),\\) 잡음 스케줄\\(\\alpha\\) 및\\(\\eta\\), 타일 크기\\(\\left(H,W\\right),\\) 추론 알고리즘 Noise, 부트스트랩핑 스텝 수\\(n_{\\mathrm{bstrap}}\\). 출력: 다중 텍스트-마스크 쌍으로부터 생성된 지정 크기\\(\\left(8H^{\\prime},8W^{\\prime}\\right)\\의 이미지\\(\\mathbf{I}\\)\n' +
      '1\\(\\mathbf{x}_{t_{n}}^{\\prime}\\sim\\mathcal{N}(0,1)^{H^{\\prime}\\times W^{\\prime}\\times D}\\)\n' +
      '2\\(\\{\\mathcal{T}_{1},\\ldots,\\mathcal{T}_{m}\\subset\\{\\left(h_{\\mathrm{t},h_{\\mathrm{b},w_{\\mathrm{l},w_{\\mathrm{r}\\right):0\\leq h_{\\mathrm{t}<h_{\\mathrm{b}}\\leq H^{\\prime},0\\leq w_{\\mathrm{l}<w_{\\mathrm{r}}\\leq W^{\\prime}\\\\leq\n' +
      '3for\\(i\\gets n\\)to\\(1\\)do\n' +
      '4\\(\\bar{\\mathbf{x}\\leftarrow\\mathbf{0}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times D}\\)\n' +
      '5\\(\\bar{\\mathbf{w}}\\leftarrow\\mathbf{0}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}}\\)\n' +
      '6for\\(j\\gets 1\\)to\\(m\\)do\n' +
      '7\\(\\bar{\\mathbf{x}}_{1:p}\\leftarrow\\texttt{repeat}(\\texttt{crop}(\\mathbf{x}_{t_{i}}, \\mathcal{T}_{j}),p)\\)\n' +
      '8\\(\\bar{\\mathbf{w}}_{1:p}^{\\left(t_{i}\\right)}\\leftarrow\\texttt{crop}(\\mathbf{w}_{1:p}^{ \\left(t_{i}\\right)},\\mathcal{T}_{j}\\)//타임스텝마다 다른 양자화 마스크를 사용한다.\n' +
      '9if\\(i\\leq n_{bstrap}\\)then\n' +
      '10\\(\\mathbf{x}_{\\mathrm{bg}}\\leftarrow\\texttt{enc}(1)\\)//흰색 배경\n' +
      '11\\(\\mathbf{x}_{\\mathrm{bg}\\leftarrow\\sqrt{\\alpha(t_{i})}\\mathbf{x}_{\\mathrm{bg}\\sqrt{1-\\alpha(t_{i})}\\mathbf{\\epsilon}\\, 여기서 \\(\\mathbf{\\epsilon}\\sim\\mathcal{N}(0,1)^{H\\times W\\times D}\\)\n' +
      '12\\(\\bar{\\mathbf{x}}_{1:p}\\leftarrow\\bar{\\mathbf{w}}_{1:p}\\odot\\bar{\\mathbf{x}}_{1:p}+\\left( \\mathbf{1}-\\bar{\\mathbf{w}}_{1:p}\\right)\\odot\\mathbf{x}_{\\mathrm{bg}}\\)\n' +
      '13\\(\\mathbf{u}_{1:p}\\leftarrow\\texttt{get\\_bounding\\_box\\_centers}(\\bar{\\mathbf{w}_{1:p}) \\in\\mathbbb{R}^{p\\times 2}\\)//마스크 중심은 현재 시점 타일의 각 마스크의 바운딩 박스 중심이다.\n' +
      '14\\(\\bar{\\mathbf{x}}_{1:p}\\leftarrow\\texttt{roll\\_by\\_coordinates}(\\bar{\\mathbf{x}}_{1:p}, \\mathbf{u}_{1:p}))///중심 전경들을 마스크 중심들에 적용하여 확산 모델의 중심 선호도를 이용한다.\n' +
      '15\n' +
      '16\n' +
      '17\\(\\bar{\\mathbf{x}_{1:p}\\leftarrow\\texttt{StepExceptNoise}(\\bar{\\mathbf{x}_{1:p}, \\mathbf{y}_{1:p},i;\\mathbf{\\epsilon}_{\\theta},\\alpha,\\mathbf{t})//사전 평균화\n' +
      '18if\\(i\\leq n_{bstrap}\\)then\n' +
      '19\\(\\bar{\\mathbf{x}}_{1:p}\\leftarrow\\texttt{roll\\_by\\_coordinates}(\\bar{\\mathbf{x}}_{1:p}, -\\mathbf{u}_{1:p}) // 센터링으로부터의 복원\n' +
      '20\n' +
      '21\n' +
      '22\\(\\bar{\\mathbf{x}}[\\mathcal{T}_{j}]\\leftarrow\\bar{\\mathbf{x}}[\\mathcal{T}_{j}]+\\sum_{k=1}^ {p}\\bar{\\mathbf{w}}_{k}\\odot\\bar{\\mathbf{x}}_{k}\\)\n' +
      '23\\(\\bar{\\mathbf{w}}[\\mathcal{T}_{j}]\\leftarrow\\bar{\\mathbf{w}}[\\mathcal{T}_{j}]+\\sum_{k=1}^ {p}\\bar{\\mathbf{w}}_{k}\\)\n' +
      '24\n' +
      '25\n' +
      '26\\(\\mathbf{x}_{t_{i-1}}\\leftarrow\\bar{\\mathbf{x}}\\odot\\bar{\\mathbf{w}}^{-1}\\)\n' +
      '27\\(\\mathbf{x}_{t_{i-1}}\\leftarrow\\mathbf{x}_{t_{i-1}}+\\eta_{t_{i-1}}\\mathbf{\\epsilon}\\), 여기서 \\(\\mathbf{\\epsilon}\\sim\\mathcal{N}(0,1)^{H\\times W\\times D}\\)//소음의 후가산\n' +
      '28\n' +
      '29는,\n' +
      '30\\(\\mathbf{I}\\leftarrow\\texttt{dec}(\\mathbf{x}_{t_{1}})\\)\n' +
      '```\n' +
      '\n' +
      '**알고리즘 2** 섹션 3.2의 안정화된 다중확산 버전입니다.\n' +
      '\n' +
      '배경색을 흰색으로 바꿉니다. 매우 적은 수의 샘플링 단계들(4-5)로, 이 부트스트래핑 배경은 그림 3에서 볼 수 있는 바와 같이 최종 이미지를 통해 쉽게 누출된다. 그러나, 확산 모델들이 트레이닝되는 이미지 데이터세트들에서 백색 배경들이 공통적이기 때문에, 백색 배경들을 사용하여 이러한 문제를 완화시킨다. 라인 13-14는 부트스트래핑을 위한 우리의 _mask-centering_stage이다. 확산 모델은 생성된 객체를 프레임의 중앙에 배치하는 경향이 있다. 생성 첫 몇 단계에서 주어진 마스크마다 프레임들을 객체 바운딩 박스의 중심으로 이동시킴으로써, 다음의 마스킹 스테이지들이 생성된 객체들의 핵심 부분들을 포기하는 것을 방지한다. 17-19행은 13-14행에서의 센터링 동작을 취소한다. 마지막으로, 알고리즘 1의 14행의 단일 역확산 단계는 알고리즘 2의 16행의 잡음 제거 부분과 알고리즘 2의 24행의 잡음 추가 부분으로 분할된다. 그림 2에서 시각적 예제와 논의한 바와 같이, 원래의 다중확산[4]의 간단한 증강은 LCM-LoRA[23, 24]와 같은 빠른 추론 기술로 알고리즘을 안정화시킨다. 다중확산 안정화를 위한 우리의 전략은 그림 4에 설명되어 있으며 독자는 제출된 코드를 참조하여 설명을 할 수 있다.\n' +
      '\n' +
      '## 부록 0.B 추가 결과\n' +
      '\n' +
      '이 섹션에서는 잠재 일관성 모델 [23, 24]에 직접 적용된 다중 확산의 변형인 다중 확산 [4]와 안정화된 알고리즘 2 간의 추가 시각적 비교 결과를 제공한다.\n' +
      '\n' +
      '도 12: 도 11에서 생성 결과의 마스크 오버레이 이미지. 우리의 StreamMultiDiffusion에 의한 생성은 높은 수렴 속도를 달성할 뿐만 아니라, 베이스라인 MultiDiffusion[4]에 비해, 큰 크기의 영역 기반 텍스트-이미지 합성에서 높은 마스크 충실도를 달성한다. 각 셀은 아래 라벨에 설명된 바와 같이 각 마스크(배경 영역을 포함)가 이미지의 각 생성된 영역에 매핑되는 방법을 보여줍니다. 우리는 _not_가 단순히 텍스트 프롬프트와 이진 마스크의 쌍인 _semantic palette_ 이외의 추가 색상 또는 구조 제어를 제공했다는 점에 유의하십시오.\n' +
      '\n' +
      '단일 상용 기성 그래픽 카드인 RTX 2080 Ti GPU를 사용하여 여러 지역 프롬프트에서 대규모 이미지를 생성할 수 있습니다.\n' +
      '\n' +
      '### Panorama Generation\n' +
      '\n' +
      '그림 14는 우리의 방법을 이용한 대규모 파노라마 영상 생성의 추가 결과를 보여준다. 4.1절과 같이 단일 텍스트 프롬프트로부터 512×4608×512×4608×512×4608×512×4608×512×512×4608×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512× 이는 안정적인 확산 모델 [30]이 훈련된 \\(512\\times 512\\) 또는 \\(768\\times 768\\)의 해상도보다 훨씬 큰 크기의 \\(13\\times 13\\) 더 빠른 이미지 생성을 가능하게 한다.\n' +
      '\n' +
      '### 영역 기반 텍스트-이미지 생성\n' +
      '\n' +
      '그림 13에서 추가적인 영역 기반 텍스트-이미지 생성 결과를 보여준다. 그림 9와 함께 생성된 샘플들은 생성 품질을 유지하면서 영역 기반 텍스트-이미지 생성을 최대 \\(\\times 10\\)까지 가속화할 수 있음을 보여준다. 또한 그림 11은 섹션 B.1의 대형 생성과 섹션 B.2의 영역 기반 제어 생성에 대한 가속 방법의 이점이 실제로 동시에 즐겁다는 것을 보여준다. 여기서는 한국 궁중 병풍의 유명한 작품인 _Irworobongdo_ ("Painting of the Sun, Moon and the Five Peaks")1에서 영감을 받아 그림 11에 쓰여진 배경을 포함하여 지역적으로 할당된 9개의 텍스트 프롬프트로부터 크기 \\(768\\times 1920\\)의 이미지를 생성하고, 원작의 의미 구조를 대략 따르는 8개의 이진 마스크를 손으로 그린 스케치를 그린다. 우리가 적용한 유일한 전처리는 앞의 마스크에 의해 가려진 각 마스크의 영역에서 알파 채널 값을 0.3으로 줄이는 것이다. 본 논문의 가속 방법은 안정 확산 모델 [30]이 59초 만에 장면을 생성할 수 있게 한다. 이는 수렴하는 데 51분 이상 걸리는 기준선 MultiDiffusion[4]보다 \\(배 52.5\\) 빠르다. 확산 모델을 이용한 전문적인 영상 생성 과정은 일반적으로 서로 다른 씨앗을 갖는 다수의 재샘플링 시험을 수반한다는 점에서, 멀티디퓨전(MultiDiffusion)의 시간당 한 영상의 수렴 속도는 알고리즘의 적용성을 심각하게 제한한다. 대조적으로, 우리의 가속 방법은 동일한 큰 크기의 영역 기반 텍스트 대 이미지 합성을 1분 이내에 수행할 수 있게 한다. 이는 본 논문에서 제안하는 방법이 전문 크리에이터를 위한 확산 모델의 활용성을 상당히 넓힌다는 것을 의미한다.\n' +
      '\n' +
      '각주 1: [https://en.wikipedia.org/wiki/Irworobongdo](https://en.wikipedia.org/wiki/Irworobongdo)\n' +
      '\n' +
      '그림 11은 우리 방법의 한 가지 추가 이점을 보여준다. 대규모 영역 기반 제어 생성에서 우리는 더 나은 마스크 충실도를 분명히 달성한다. 그림 12는 예를 들어 마스크 충실도에 대해 더 자세히 설명합니다. 해와 달의 위치와 크기는 제공된 마스크와 거의 완벽하게 일치하지만, 산과 폭포는 지역 경계를 위반하지 않고 전체 이미지 내에서 조화를 이룬다. 이는 의 유연성과 속도를 보여준다.\n' +
      '\n' +
      '그림 13: 추가 영역 기반 텍스트 대 이미지 합성 결과. 이 방법은 마스크 충실도를 보존하거나 심지어 부스팅하면서 다중확산[4]을 최대 \\(\\배 10\\)까지 가속한다.\n' +
      '\n' +
      '도 14: 파노라마 생성 추가 결과. 512×4608×512×4608×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512×512× 우리는 추론 지연 시간에서 \\(\\times 13\\)의 개선을 달성한다.\n' +
      '\n' +
      '의미 팔레트_로 그리는 우리의 세대 패러다임도 전문적인 사용이 가능합니다. 이것은 제출된 데모 신청서에 대한 설명인 이 보충 자료의 마지막 섹션으로 이어진다.\n' +
      '\n' +
      '## 부록 0.C 데모 어플리케이션을 위한 명령어\n' +
      '\n' +
      '마지막으로 5절에서 소개한 StreamMultiDiffusion을 위한 데모 애플리케이션의 구조와 사용에 대해 자세히 설명한다. 0.C.1절에서 사용자 인터페이스에 대한 기본 설명부터 0.C.2절에서 앱 사용에 대한 예상 절차에 대해 논의하고, 제안된 _semantic palette_ 개념이 첫 번째 실시간 대화형 영역 기반 텍스트-이미지 생성 알고리즘인 StreamMultiDiffusion을 기반으로 어떻게 구현될 수 있는지 논의한다.\n' +
      '\n' +
      '### User Interface\n' +
      '\n' +
      '도 9의 (b)에 예시된 바와 같이, 사용자 상호작용들은 모델로부터의 응답의 레이턴시에 기초하여, _i.e._, 느린 프로세스들 및 빠른 프로세스들의 두 그룹으로 분류된다. 텍스트 인코더와 이미지 인코더의 높은 오버헤드로 인해, 이러한 모듈들을 포함하는 프로세스들은 느린 프로세스들로 분류된다. 그러나 마스크 텐서의 전처리 또는 저장, 샘플링 등의 연산\n' +
      '\n' +
      '도 15: 보조 데모 애플리케이션의 스크린샷. 번호가 매겨진 구성 요소에 대한 자세한 내용은 표 3에 자세히 설명되어 있다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:26]\n' +
      '\n' +
      '브러시. 마스크가 스케줄러의 미리 정의된 노이즈 레벨로 양자화되기 전에, 섹션 3.2에서 정교화된 바와 같이, 마스크는 먼저 마스크 알파에 곱해지고 지정된 표준 편차를 갖는 등방성 가우시안 블러(isotropic Gaussian blur)를 거친다. 즉, 마스크\\(\\boldsymbol{w}\\), 마스크 알파\\(a\\) 및 노이즈 레벨 스케줄링 함수\\(\\beta(t)=\\sqrt{1-\\alpha(t)}\\)가 주어지면, 결과적으로 양자화된 마스크\\(\\boldsymbol{w}_{1:p}^{(t_{i}})는 다음과 같다.\n' +
      '\n' +
      '\\[\\boldsymbol{w}_{1:p}^{(t_{i})}=\\mathbb{1}[a\\boldsymbol{w}>\\beta(t_{i})]\\,, \\tag{10}\\]\n' +
      '\n' +
      '여기서 \\(\\mathbb{1}[\\cdot]\\)은 시간 \\(t_{i}\\)에서 부울 마스크 텐서 \\(\\boldsymbol{w}_{1:p}^{(t_{i})}\\)을 만들기 위해 부등식을 이진 연산자로 사용하는 지표 함수이다. 잠재적 일관성 모델 [23]의 기본 잡음 수준\\(\\beta(t)\\)은 그림 5와 같이 1에 가깝다. 이는 마스크 알파를 극도로 민감하게 만든다. 값을 0.98로 약간만 변경하면 해당 프롬프트는 이미 처음 두 샘플링 단계를 건너뛴다. 이것은 프롬프트의 내용을 빠르게 퇴화시키고, 따라서 마스크 알파(no.9) 치료에 사용되어야 한다. 마스크 블러 std(no.10)의 효과 그림 6에 나와 있으며 이 섹션에서는 더 이상 자세히 설명하지 않는다. 시스템의 시드는 시드 제어(no.11)에 의해 조정될 수 있다. 그럼에도 불구하고, 애플리케이션이 무한 스트림에서 이미지를 생성하기 때문에 의사 랜덤 생성기를 제어하는 것은 거의 필요하지 않을 것이다. 프롬프트 편집(no. 12) 는 시멘틱 브러시의 주요 제어이다. 사용자는 생성이 스트림에 있는 경우에도 텍스트 프롬프트를 변경할 수 있습니다. 프롬프트의 변경이 적용되기 위해서는 추론 단계의 총 수인 _i.e._, 5 단계가 정확히 필요하다. 마지막으로, 신속한 강도(no.13)를 제공한다. 고감도 마스크 알파(no.9)의 대안으로서, 대상 프롬프트의 현저성을 제어합니다. 알파 채널을 수정하는 것은 이미 알파 블렌딩에 익숙한 그래픽 디자이너에게 좋은 직관을 제공하지만, 일관성 모델 [23, 33]의 노이즈 수준은 마스크 알파 값이 알파 블렌딩에서 우리의 직관과 잘 정렬되지 않도록 한다. 프롬프트 강도는 주어진 시맨틱 브러시의 전경 텍스트 프롬프트의 임베딩과 배경 텍스트 프롬프트 사이의 혼합 비율이다. 즉시 강도를 변경하면 마스크 알파보다 전경-배경 블렌딩 강도를 더 부드럽게 제어할 수 있음을 경험적으로 발견했다. 그러나 마스크 알파는 국부적으로 적용될 수 있는 반면, 신속한 강도는 전 세계적으로만 적용된다. 따라서 우리는 두 통제가 서로 보완적이라고 믿는다.\n' +
      '\n' +
      '### Basic Usage\n' +
      '\n' +
      '우리는 스트림멀티디퓨전 파이프라인에서 사용할 수 있는 _semantic palette_로 이미지를 만드는 가장 간단한 절차를 제공한다. 그림 16의 스크린샷은 4단계 프로세스를 보여준다.\n' +
      '\n' +
      '####0.c.2.1 응용프로그램을 시작합니다.\n' +
      '\n' +
      '필요한 패키지를 설치한 후, 사용자는 다음과 같은 명령 프롬프트로 애플리케이션을 열 수 있다:\n' +
      '\n' +
      '"pythonapp.py-model"KBlueLeaf/kohaku-v2.1"-height512-width512 the application front-end는 웹 기반이며 Localhost:8000을 통해 임의의 웹 브라우저로 열 수 있다. 현재 잠재 일관성 모델[23, 24]의 가용성으로 인해 --model 옵션을 위한 Stable Diffusion version 1.5 체크포인트[30]만을 지원한다. 캔버스의 높이 및 폭은 애플리케이션의 시작 시에 미리 정의된다.\n' +
      '\n' +
      '####2.2.1 배경 이미지 업로드\n' +
      '\n' +
      '도 15(a)를 참조한다. 애플리케이션과의 첫 번째 상호작용은 배경 이미지 업로드를 클릭함으로써 임의의 이미지를 배경으로 업로드하는 것이다(no.5). 패널. 업로드된 배경 이미지는 캔버스와 일치하도록 크기가 조정됩니다. 이미지를 업로드한 후, 미리 학습된 BLIP-2 모델에 의해 업로드된 이미지의 배경 프롬프트가 사용자에 대해 자동으로 생성된다[16]. 배경 프롬프트는 섹션 C.1에서 정교화된 바와 같이 전체적으로 프롬프트 레벨에서 전경과 배경 사이의 혼합을 위해 사용된다. 보간은 전경 텍스트 프롬프트 임베딩을 프롬프트 강도가 1보다 작은 것으로 할당할 때 발생한다. 사용자는 항상 _semantic palette_의 다른 프롬프트와 마찬가지로 배경 프롬프트를 변경할 수 있습니다.\n' +
      '\n' +
      '문자 프롬프트의####3.2.2 유형\n' +
      '\n' +
      '도 15(b)를 참조한다. 다음 단계는 _semantic palette_(no. 1-2)와 상호 작용하여 시맨틱 브러시를 생성하고 관리하는 단계이다. 최소 필수 수정은 프롬프트를 통한 텍스트 프롬프트 할당입니다.\n' +
      '\n' +
      '도 16: StreamMultiDiffusion의 데모 애플리케이션의 삽화 사용 가이드.\n' +
      '\n' +
      '편집부(제12호) 패널. 사용자는 그림 15(b)에서 노란색으로 표시된 제어판에서 다른 옵션을 추가로 수정할 수 있다.\n' +
      '\n' +
      '## 4 Draw\n' +
      '\n' +
      '도 15(c)를 참조한다. 사용자는 레이어 선택에서 레이어를 선택함으로써 드로잉을 시작할 수 있다(no.4). 이전 단계에서 사용자가 지정한 텍스트 프롬프트와 일치하는 탭입니다. 그리기 도구(No. 6)에서 브러시를 잡습니다. 뽑은 마스크를 단추, 그리기, 제출합니다. 재생 버튼을 누른다(아니오). 도 15(d)에서 노란색으로 표시된 7)은 디스플레이 상에 나타날 생성된 이미지들의 스트림을 개시할 것이다(no. 8).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>