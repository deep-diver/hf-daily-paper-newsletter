<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Models\n' +
      '\n' +
      ' Shyam Marjit\n' +
      '\n' +
      'Harshit Singh\n' +
      '\n' +
      'Nityanand Mathur\n' +
      '\n' +
      'Sayak Paul\n' +
      '\n' +
      'Chia-Mu Yu\n' +
      '\n' +
      'Pin-Yu Chen\n' +
      '\n' +
      'Project Page: [https://diffusekrona.github.io/](https://diffusekrona.github.io/)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'In the realm of subject-driven text-to-image (T2I) generative models, recent developments like DreamBooth and BLIP-Diffusion have led to impressive results yet encounter limitations due to their intensive fine-tuning demands and substantial parameter requirements. While the low-rank adaptation (LoRA) module within DreamBooth offers a reduction in trainable parameters, it introduces a pronounced sensitivity to hyperparameters, leading to a compromise between parameter efficiency and the quality of T2I personalized image synthesis. Addressing these constraints, we introduce _DiffuseKronA_, a novel Kronecker product-based adaptation module that not only significantly reduces the parameter count by 35% and 99.947% compared to LoRA-DreamBooth and the original DreamBooth, respectively, but also enhances the quality of image synthesis. Crucially, _DiffuseKronA_ mitigates the issue of hyperparameter sensitivity, delivering consistent high-quality generations across a wide range of hyperparameters, thereby diminishing the necessity for extensive fine-tuning. Furthermore, a more controllable decomposition makes _DiffuseKronA_ more interpretable and even can achieve up to a 50% reduction with results comparable to LoRA-Dreambooth. Evaluated against diverse and complex input images and text prompts, _DiffuseKronA_consistently outperforms existing models, producing diverse images of higher quality with improved fidelity and a more accurate color distribution of objects, all the while upholding exceptional parameter efficiency, thus presenting a substantial advancement in the field of T2I generative modeling.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'In recent years, text-to-image (T2I) generation models (Gu et al., 2022; Chang et al., 2023; Rombach et al., 2022; Podell et al., 2023; Yu et al., 2022) have rapidly evolved, generating intricate and highly detailed images that often defy discernment from real-world photographs. The current state-of-the-art has marked significant progress and demonstrated substantial improvement, which hints at a future where the boundary between human imagination and computational representation becomes increasingly blurred. In this context, subject-driven T2I generative models (Ruiz et al., 2023a; Li et al., 2023a) unlock creative potential such as image editing, subject-specific property modifications, art renditions, etc. Works like DreamBooth (Ruiz et al., 2023a), BLIP-Diffusion (Li et al., 2023a) seamlessly introduce new subjects into the pre-trained models while preserving the priors learned by the original model without impacting its generation capabilities. These approaches excel at retaining the essence and subject-specific details across various styles when fine-tuned with few-shot examples, leveraging foundational pre-trained latent diffusion models (LDMs) (Rombach et al., 2022).\n' +
      '\n' +
      'However, DreamBooth with Stable Diffusion (Rombach et al., 2022) suffers from some primary issues, such as incorrect prompt context synthesis, context appearance entanglement, and hyperparameter sensitivity. Additionally, DreamBooth finetunes all parameters of latent diffusion model\'s (Rombach et al., 2022) UNet and text encoder (Radford et al., 2021), which significantly increases the trainable parameter count, making the finetuning process expensive. Here, the widely used low-rank adaptation module (Hu et al., 2021) (LoRA) within DreamBooth attempts to significantly trim the parameter counts but it magnifies the aforementioned DreamBooth-reported issues, which makes a complete tradeoff between parameter efficiency and satisfactory subject-driven image synthesis. Moreover, it suffers from high sensitivity to hyperparameters, necessitating extensive fine-tuning to achieve desired outputs. This motivates us to design a more robust and effective parameter-efficient fine-tuning (PEFT) method for adapting T2I generative models to subject-driven personalized generation.\n' +
      '\n' +
      'In this paper, we introduce _DiffuseKronA_, a novel parameter-efficient module that leverages the Kronecker product-based adaptation module for fine-tuning T2I diffusion models, focusing on few-shot adaptations. LoRA adheres to a vanilla encoder-decoder type architecture, which learns similar representations within decomposed matrices due to constrained flexibility and similar-sized matrix decomposition (Tahaei et al., 2022). In contrast, Kronecker\'s decomposition exploits patch-specific redundancies, offering a much higher-rank approximation of the original weight matrix with less parameter count and greater flexibility in representation by allowing different-sized decomposed matrices. This fundamental difference is attributed to several improvements including parameter reduction, enhanced stability, and greater flexibility. Moreover, it effectively captures crucial subject-specific spatial features while producing images that closely adhere to the provided prompts. This results in higher quality, improved fidelity, and more accurate color distribution in objects during personalized image generation, achieving comparable results to state-of-the-art techniques.\n' +
      '\n' +
      'Our key contributions are as follows:\n' +
      '\n' +
      '**Parameter Efficiency:**_DiffuseKronA_ significantly reduces trainable parameters by 35% and 99.947% as compared to LoRA-DreamBooth and vanilla DreamBooth using SDXL (Podell et al., 2023) as detailed in Table 2. By changing Kronecker factors, we can even achieve up to a 50% reduction with results comparable to state-of-the-art as demonstrated in Figure 26 in the Appendix.\n' +
      '\n' +
      '**Enhanced Stability:**_DiffuseKronA_ offers a much more stable image-generation process formed within a fixed spectrum of hyperparameters when fine-tuning, even when working with complicated input images and diverse prompts. In Figure 4, we demonstrate the trends associated with hyperparameter changes in both methods and highlight our superior stability over LoRA-DreamBooth.\n' +
      '\n' +
      '**Text Alignment and Fidelity:** On average, _DiffusekronA_ captures better subject semantics and large contextual prompts. We refer the readers to Figure 7 and Figure 8 for qualitative and quantitative comparisons, respectively.\n' +
      '\n' +
      '**Interpretability:** Notably, we conduct extensive analysis to explore the advantages of the Kronecker product-based\n' +
      '\n' +
      'Figure 2: Schematic illustration: LoRA is limited to one controllable parameter, the rank \\(\\mathbf{r}\\); while the Kronecker product showcases enhanced interpretability by introducing two controllable parameters \\(\\mathbf{a_{1}}\\) and \\(\\mathbf{a_{2}}\\) (or equivalently \\(\\mathbf{b_{1}}\\) and \\(\\mathbf{b_{2}}\\)).\n' +
      '\n' +
      'adaptation module within personalized diffusion models. More controllable decomposition makes _DiffusekronA_ more interpretable as demonstrated in Figure 2.\n' +
      '\n' +
      'Extensive experiments on 42 datasets under the few-shot setting demonstrate the aforementioned effectiveness of _DiffusKronA_, achieving the best trade-off between parameter efficiency and satisfactory image synthesis.\n' +
      '\n' +
      '## 2 Related Works\n' +
      '\n' +
      '**Text-to-Image Diffusion Models.** Recent advancements in T2I diffusion models such as Stable Diffusion (SD) (Rombach et al., 2022; Podell et al., 2023), Imagen (Saharia et al., 2022), DALL-E2 (Ramesh et al., 2022) & E3 (Betker et al., 2023), PixArt-\\(\\alpha\\)(Chen et al., 2023), Kandinsky (Lui et al., 2019), and eDiff-I (Balaji et al., 2022) have showcased remarkable efficacy in modeling data distributions, yielding impressive results in image synthesis and opening the door for various creative applications across domains. Compared to the previous iterations of the SD model, Stable Diffusion XL (SDXL) (Podell et al., 2023) represents a significant advancement in T2I synthesis owing to a larger backbone and an improved training procedure. In this work, we mainly incorporate SDXL due to its impressive capability to generate high-resolution images, prompt adherence, as well as better composition and semantics.\n' +
      '\n' +
      '**Subject-driven T2I Personalization.** Given only a few images (typically 3 to 5) of a specific subject, T2I personalization techniques aim to synthesize diverse contextual images of the subject based on textual input. In particular, Textual Inversion (Gal et al., 2022) and DreamBooth (Ruiz et al., 2023a) were the first lines of work. Textual Inversion fine-tunes text embedding, while DreamBooth fine-tunes the entire network using an additional preservation loss as regularization, resulting in visual quality improvements that show promising outcomes. More recently, BLIP-Diffusion (Li et al., 2023a) enables zero-shot subject-driven generation capabilities by performing a two-stage pre-training process leveraging the multimodal BLIP-2 (Li et al., 2023b) model. These studies focus on single-subject generation, with later works (Kumari et al., 2023; Han et al., 2023; Ma et al., 2023; Tewel et al., 2023) delving into multi-subject generation.\n' +
      '\n' +
      '**PEFT Methods within T2I Personalization.** In contrast to foundational models (Ruiz et al., 2023a; Li et al., 2023a) that fine-tune large pre-trained models at full scale, several seminal works (Kumari et al., 2023; Han et al., 2023; Ruiz et al., 2023; Ye et al., 2023) in parameter-efficient fine-tuning (PEFT) have emerged as a transformative approach. Within the realm of PEFT techniques, low-rank adaptation methods (Hu et al., 2021; von Platen et al., 2023) has become a de-facto way of reducing the parameter count by introducing learnable truncated Singular Value Decomposition (SVD) modules into the original model weights on essential layers. For instance, Custom Diffusion (Kumari et al., 2023) focuses on fine-tuning the \\(K\\) and \\(V\\) matrices of the cross-attention, introducing multiple concept generation for the first time, and employing LoRA for efficient parameter compression. SVDiff (Han et al., 2023) achieves parameter efficiency by fine-tuning the singular values of the weight matrices with a Cut-Mix-Unmix data augmentation technique to enhance the quality of multi-subject image generation. Hyper-Dreambooth (Ruiz et al., 2023b) proposed a hypernetwork to make DreamBooth rapid and memory-efficient for personalized fidelity-controlled face generation. T2I-Adapters (Mou et al., 2023), a conceptually similar approach to ControlNets (Zhang et al., 2023), makes use of an auxiliary network to compute the representations of the additional inputs and mixes that with the activations of the UNet. Mix-of-Show (Gu et al., 2023), on the other hand, involves training distinct LoRA models for each subject and subsequently performing fusion.\n' +
      '\n' +
      'In context, the LoRA-Dreambooth (Ryu, 2023) technique has encountered difficulties due to its poor representational capacity and low interpretability, and to address these constraints we introduce _DiffusKronA_. Our method is inspired by the KronA technique initially proposed by (Edalati et al., 2022b). However, there are key distinctions: (**1**) The original paper was centered around language models, whereas our work extends this exploration to LDMs, particularly in the context of T2I generation. (**2**) Our focus lies on the efficient fine-tuning of various modules within LDMs. (**3**) More importantly, we investigate the impact of altering Kronecker factors on subject-specific generation, considering interpretability, parameter efficiency, and subject fidelity. It is also noteworthy to mention that LoKr (Yeh et al., 2023) is a concurrent work, and we discuss the key differences in Appendix F.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '**Problem Formulation.** Given a pre-trained T2I latent diffusion model \\(\\mathcal{D}_{\\phi}\\) with size \\(|\\mathcal{D}_{\\phi}|\\) and weights denoted by \\(\\phi\\), we aim to develop a parameter-efficient adaptation technique with trainable parameters \\(\\theta\\) of size \\(m\\) such that \\(m\\ll|\\mathcal{D}_{\\phi}|\\) holds (_i.e._ efficiency) while attaining satisfactory and comparable performance with a full fine-tuned model. At inference, newly trained parameters will be integrated with their corresponding original weight matrix, and diverse images can be synthesized from the new personalized model, \\(\\mathcal{D}_{\\phi+\\theta}\\).\n' +
      '\n' +
      '**Method Overview.** Figure 3 shows an overview of our proposed _DiffusKronA_ for PEFT of T2I diffusion models in subject-driven generation. _DiffusKronA_ only updates parameters in the attention layers of the UNet model while keeping text encoder weights frozen within the SDXL backbone. Here, we first outline a preliminary section in Section 3.1 followed by a detailed explanation of _DiffusKronA_ in Section 3.2. Particularly, in Section 3.2, we provide insights and mathematical explanations of _"Why Di_fuseKronA is a more parameter-efficient and interpretable way of fine-tuning Diffusion models compared to vanilla LoRA?"_\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      'T2I Diffusion Models.LDMs (Rombach et al., 2022), a prominent variant of probabilistic generative Diffusion models denoted as \\(\\mathcal{D}_{\\phi}\\), aim to produce an image \\(\\mathbf{x}_{gen}=\\mathcal{D}_{\\phi}\\left(\\mathbf{\\epsilon},\\mathbf{c}\\right)\\) by incorporating a noise map \\(\\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\) and a conditioning embedding \\(\\mathbf{c}=\\mathcal{T}\\left(\\mathbf{P}\\right)\\) derived from a text prompt \\(\\mathbf{P}\\) using a text encoder, \\(\\mathcal{T}\\). LDMs transform the input image \\(\\mathbf{x}\\in\\mathbb{R}^{H\\times W\\times 3}\\) into a latent representation \\(\\mathbf{z}\\in\\mathbb{R}^{h\\times w\\times v}\\) through an encoder \\(\\mathcal{E}\\), where \\(\\mathbf{z}=\\mathcal{E}\\left(\\mathbf{x}\\right)\\) and \\(v\\) is the latent feature dimension. In this context, the denoising diffusion process occurs in the latent space, \\(\\mathcal{Z}\\), utilizing a conditional UNet (Ronneberger et al., 2015) denoiser \\(\\mathcal{D}_{\\phi}\\) to predict noise \\(\\mathbf{\\epsilon}\\) at the current timestep \\(t\\) given the noisy latent \\(\\mathbf{z}_{t}\\) and generation condition \\(c\\). In brief, the denoising training objective of an LDM \\(\\mathcal{D}_{\\phi}\\) can be simplified to:\n' +
      '\n' +
      '\\[\\mathbb{E}_{\\mathcal{E}\\left(\\mathbf{x}\\right),\\mathbf{c},\\mathbf{\\epsilon}\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}),t\\sim\\mathcal{U}(0,1)}\\left[w_{t}\\left\\| \\mathcal{D}_{\\phi}\\left(\\mathbf{z}_{t}|\\mathbf{c},t\\right)-\\mathbf{\\epsilon}\\right\\| _{2}^{2}\\right], \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\mathcal{U}\\) denotes uniform distribution and \\(w_{t}\\) is a time-dependent weight on the loss.\n' +
      '\n' +
      'Low Rank Adaptation (LoRA).Pre-trained large models exhibit a low "intrinsic dimension" for task adaptation (Hu et al., 2021; Han et al., 2023), implying efficient learning after subspace projection. Based on this, LoRA (Hu et al., 2021) hypothesizes that weight updates also possess low "intrinsic rank" during adaptation and inject trainable rank decomposition matrices into essential layers of the model for task adaptations, significantly reducing the number of trainable parameters.\n' +
      '\n' +
      'In the context of a pre-trained weight matrix \\(W_{0}\\in\\mathbb{R}^{d\\times k}\\), the update of \\(W_{0}\\) is subject to constraints imposed through the representation of the matrix as a low-rank decomposition \\(W_{0}+\\Delta W:=W_{0}+AB\\), where \\(A\\in\\mathbb{R}^{d\\times r}\\), \\(B\\in\\mathbb{R}^{r\\times h}\\), and the rank \\(r\\ll\\min(d,h)\\). As a result, the sizes of \\(A\\) and \\(B\\) are significantly smaller than \\(W_{0}\\), reducing the number of trainable parameters. Throughout the training process, \\(W_{0}\\) remains fixed, impervious to gradient updates, while the trainable parameters are contained within \\(A\\) and \\(B\\). For \\(h=W_{0}x\\), the modified forward pass is formulated as follows:\n' +
      '\n' +
      '\\[f(x)=W_{0}x+\\Delta Wx+b_{0}:=W_{\\text{LoRA}}\\,x+b_{0}, \\tag{2}\\]\n' +
      '\n' +
      'where \\(b_{0}\\) is the bias term of the pre-trained model.\n' +
      '\n' +
      'LoRA-DreamBooth.LoRA (Hu et al., 2021) is strategically employed to fine-tune DreamBooth with the primary purpose of reducing the number of trainable parameters. LoRA injects trainable modules with low-rank decomposed matrices in \\(W_{Q}\\), \\(W_{K}\\), \\(W_{V}\\), and \\(W_{O}\\) weight matrices of attention modules within the UNet and text encoder. During training, the weights of the pre-trained UNet and text encoder are frozen and only LoRA modules are tuned. However, during inference, the weights of fine-tuned LoRA modules are annexed to the corresponding pre-trained weights. Moreover, this task does not increase the inference time.\n' +
      '\n' +
      '### DiffuseKronA\n' +
      '\n' +
      'LoRA demonstrates effectiveness in the realm of diffusion models but is hindered by its limited representation power. In contrast, the Kronecker product offers a more nuanced representation by explicitly capturing pairwise interactions between elements of two matrices. This ability to capture intricate relationships enables the model to learn and represent complex patterns in the data with greater detail.\n' +
      '\n' +
      'Kronecker Product (\\(\\otimes\\)) is a matrix multiplication method that allows multiplication between matrices of different shapes. For two matrices \\(A\\in\\mathbb{R}^{a_{1}\\times a_{2}}\\) and \\(B\\in\\mathbb{R}^{b_{1}\\times b_{2}}\\), each block of their Kronecker product \\(A\\otimes B\\in\\mathbb{R}^{a_{1}b_{1}\\times a_{2}b_{2}}\\) is defined by multiplying the entry \\(A_{i,j}\\) with \\(B\\) such that\n' +
      '\n' +
      'Figure 3: Overview of _DiffuseKronA_: (a) Fine-tuning process involves optimizing the multi-head attention parameters (a.1) using Kronecker Adapter, elaborated in the subsequent block, a.2, (b) During inference, newly trained parameters, denoted as \\(\\theta\\), are integrated with the original weights \\(\\mathcal{D}_{\\phi}\\) and images are synthesized using the updated personalized model \\(\\mathcal{D}_{\\phi+\\theta}\\).\n' +
      '\n' +
      '\\[A\\otimes B=\\left[\\begin{array}{cccc}a_{1,1}B&\\cdots&a_{1,a_{2}}B\\\\ \\vdots&\\ddots&\\vdots\\\\ a_{a_{1},1}B&\\cdots&a_{a_{1},a_{2}}B\\end{array}\\right]. \\tag{3}\\]\n' +
      '\n' +
      'The Kronecker product can be used to create matrices that represent the relationships between different sets of model parameters. These matrices encode how changes in one set of parameters affect or interact with another set. In Figure 9, we showcase _how Kronecker product works_. Interestingly, it does not suffer from rank deficiency as low-rank down-projection does, as in the case of techniques such as LoRA and Adapter. The Kronecker product has several advantageous properties that make it a good option for handling complex data (Greenewald & Hero, 2015).\n' +
      '\n' +
      '**Kronecker Adapter (KronA).** Firstly introduced in studying PEFT of language models (Edalati et al., 2022), The Kronecker product takes advantage of the structured relationships encoded in the matrices. Instead of explicitly performing all the multiplications required to compute the product \\(A\\otimes B\\), the following equivalent matrix-vector multiplication can be applied, reducing the overall computational cost. This is particularly beneficial when working with large matrices or when computational resources are constrained:\n' +
      '\n' +
      '\\[(A\\otimes B)x=\\gamma\\left(B\\eta_{b_{2}\\times a_{2}}(x)A^{\\top}\\right) \\tag{4}\\]\n' +
      '\n' +
      'where \\(A^{\\top}\\) is transposed to \\(A\\). The rationale is that a vector \\(y\\in\\mathbb{R}^{m\\cdot n}\\) can be reshaped into a matrix \\(Y\\) of size \\(m\\times n\\) using the mathematical operation \\(\\eta_{m\\times n}(\\mathbf{y})\\). Similarly, \\(Y\\in\\mathbb{R}^{m\\times n}\\) can also be transformed back into a vector by stacking its columns using the \\(\\gamma(Y)\\) operation. This approach achieves \\(\\mathcal{O}(b\\log b)\\) computational complexity and \\(\\mathcal{O}(\\log b)\\) space complexity for a \\(b\\)-dimensional vector, a drastic improvement over the standard unstructured Kronecker multiplication (Zhang et al., 2015).\n' +
      '\n' +
      '**Fine-Tuning Diffusion Models with KronA.** In essence, KronA can be applied to any subset of weight matrices in a neural network for parameter-efficient adaptation as specified in the equation below, where \\(U\\) denotes different modules in diffusion models, including Key (\\(K\\)), Query (\\(Q\\)), Value (\\(V\\)), and Linear (\\(O\\)) layers. During fine-tuning, KronA modules are applied in parallel to the pre-trained weight matrices. The Kronecker factors are multiplied, scaled, and merged into the original weight matrix after they have been adjusted. Hence, like LoRA, KronA maintains the same inference time.\n' +
      '\n' +
      '\\[\\begin{split}\\Delta W^{U}=&\\ A^{U}\\otimes B^{U},U\\in \\{K,Q,V,O\\};\\\\ &\\ W_{\\text{fine-tuned}}=W_{\\text{pre-trained}}+\\Delta W.\\end{split} \\tag{5}\\]\n' +
      '\n' +
      'Previous studies (Kumari et al., 2023; von Platen et al., 2023; Tewel et al., 2023) have conducted extensive experiments to identify the most influential modules in the fine-tuning process. In (Kumari et al., 2023; Li et al., 2020), authors explored the rate of changes in each module during fine-tuning on different datasets, denoted as \\(\\delta_{l}=\\left\\|\\theta_{l}^{\\prime}-\\theta_{l}\\right\\|/\\left\\|\\theta_{l}\\right\\|\\), where \\(\\theta_{l}^{\\prime}\\) and \\(\\theta_{l}\\) represent the updated and pre-trained model parameters of layer \\(l\\). Their findings indicated that the cross-attention module exhibited a relatively higher \\(\\delta\\), signifying its pivotal role in the fine-tuning process. In light of these studies, we conducted fine-tuning on the attention layers and observed their high effectiveness. Additional details on this topic are available in Appendix D.\n' +
      '\n' +
      '**A closer look at LoRA v.s. _DiffuseKronA._** Higher-rank matrices are decomposable to a higher number of singular vectors, capturing better expressibility and allowing for a richer capacity for PEFT. In LoRA, the rank of the resultant update matrix \\(\\Delta W_{\\text{lora}}\\) is bounded by the minimum rank between matrices \\(A\\) and \\(B\\), _i.e._\\(rank(\\Delta W_{\\text{lora}})=\\min(rank(A),rank(B))\\). Conversely, in _DiffuseKronA_, the matrix rank \\(\\Delta W_{\\text{KronA}}=A\\otimes B\\) is the product of the ranks of matrices \\(A\\) and \\(B\\), _i.e._\\(rank(\\Delta W_{\\text{KronA}})=rank(A)\\cdot rank(B)\\), which can be properly configured to produce a higher-rank matrix than LoRA while maintaining lower-rank decomposed matrices than LoRA. Hence, for personalized T2I diffusion models, _DiffuseKronA_ is expected to carry more subject-specific information in lesser parameters, as compared in Table 2 and Table 3. More details are provided in Appendix E.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'In this section, we assess the various components of personalization using _DiffuseKronA_ through a comprehensive ablation study to confirm their effectiveness, using SDXL (von Platen et al., 2023) and SD (CompVis, 2021) models as backbones. Furthermore, we have conducted an insightful comparison between _DiffuseKronA_ and LoRA-DreamBooth in six aspects in Section 4.3 and also compare _DiffuseKronA_ with other related prior works in Section 4.4, highlighting our superiority.\n' +
      '\n' +
      '### Datasets and Evaluation\n' +
      '\n' +
      '**Datasets.** We have performed extensive experimentation on four types of subject-specific datasets: (i) 12 datasets (9 are from (Ruiz et al., 2023) and 3 are from (Kumari et al., 2023)) of living subjects/pets such as stuffed animals, dogs, and cats; (ii) dataset of 21 unique objects including sunglasses, backpacks, etc.; (iii) our 5 collected datasets on cartoon characters including Super-Saiyan, Akimi, Kiriko, Shoko Komi, and Hatake Kakashi; (iv) our 4 collected datasets on facial images. More details are given in Ap\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c} \\hline \\hline\n' +
      '**Decomposed Matrix** & **Notation** & **Module** & **Factorization** \\\\\n' +
      '**Factor Name** & & **Parameters** & **Constraint** \\\\ \\hline Kronecker down factor & \\(A\\in\\mathbb{R}^{n\\times a_{1}}\\) & \\(a_{1}a_{2}+b_{1}b_{2}\\) & \\(a_{1}b_{1}=d\\) \\\\ Kronecker up factor & \\(B\\in\\mathbb{R}^{n\\times b_{1}}\\) & & \\(a_{2}b_{2}=h\\) \\\\ \\hline LoRA down projection & \\(A\\in\\mathbb{R}^{d\\times r}\\) & & \\(r(d+h)\\) & \\(r\\ll\\min(d,h)\\) \\\\ LoRA up projection & \\(B\\in\\mathbb{R}^{r\\times h}\\) & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Comparing Kronecker factors and LoRA projections.\n' +
      '\n' +
      'pendix B.\n' +
      '\n' +
      '**Implementation Details.** We observe that \\(\\sim\\) 1000 iterations, employing a learning rate of \\(5\\times 10^{-4}\\), and utilizing an average of 3 training images prove sufficient for generating desirable results. The training process takes \\(\\sim\\) 5 minutes for SD (CompVis, 2021) and \\(\\sim\\) 40 minutes for SDXL (von Platen et al., 2023) on a 24GB NVIDIA RTX-3090 GPU.\n' +
      '\n' +
      '**Evaluation metrics.** We evaluate _DiffuseKronA_ on (1) _Image-alignment_: we compute the CLIP (Radford et al., 2021) visual similarity (CLIP-I) and DINO (Caron et al., 2021) similarity scores of generated images with the reference concept images, and (2) _Text-alignment_: we quantify the CLIP text-image similarity (CLIP-T) between the generated images and the provided textual prompts. A detailed mathematical explanations are available in Appendix C.\n' +
      '\n' +
      '### Unlocking the Optimal Configurations of _DiffuseKronA_\n' +
      '\n' +
      'Throughout our experimentation, we observed the following trends and found the optimal configuration of hyperparameters for better image synthesis using _DiffuseKronA_.\n' +
      '\n' +
      '**How to perform Kronecker decomposition?** Unlike LoRA, _DiffuseKronA_ features two controllable Kronecker factors, as illustrated in Table 1, providing greater flexibility in decomposition. Our findings reveal that the dimensions of the downward Kronecker matrix \\(\\mathbf{A}\\) must be smaller than those of the upward Kronecker matrix \\(\\mathbf{B}\\). Specifically, we determined the optimal value of \\(a_{2}\\) to be precisely 64, while \\(a_{1}\\) falls within the set \\(\\{2,4,8\\}\\). Remarkably, among all pairs of \\((a_{1},a_{2})\\) values, \\((4,64)\\) yields images with the highest fidelity. Additionally, it has been observed that images exhibit minimal variation with learning rates when \\(a_{2}=64\\), as depicted in Figure 4 and Figure 15. Detailed ablation about Kronecker factors, their initializations, and their impact on fine-tuning is provided in Appendix D.2.\n' +
      '\n' +
      '**Effect of learning rate.**_DiffuseKronA_ produces consistent results across a wide range of learning rates. Here, we observed that the images generated for a learning rate closer to the optimal learning rate value \\(5\\times 10^{-4}\\) generate similar images. However, learning rates exceeding \\(1\\times 10^{-3}\\) contribute to model overfitting, resulting in high-fidelity images but with diminished emphasis on input text prompts. Conversely, learning rates below \\(1\\times 10^{-4}\\) lead to lower fidelity in generated images, prioritizing input text prompts to a greater extent. This pattern is evident in Figure 4, where our approach produces exceptional images that faithfully capture both the input image and the input text prompt. Additional results are provided in Appendix D.3 to justify the same.\n' +
      '\n' +
      'Additionally, we conducted investigations into model ablations, examining (a) choice of modules to fine-tune the model in Appendix D.1 (b) effects of no training images in Appendix D.5 and steps in Appendix D.4, (c) one-shot model performance in Appendix D.5.1, and (d) effect of inference hyperparameters such as the number of inference steps and the guidance score in Appendix D.6.\n' +
      '\n' +
      '### Exploring Model Performance: LoRA-Dreambooth vs _DiffuseKronA_\n' +
      '\n' +
      'We use SDXL and employ our _DiffuseKronA_ to generate\n' +
      '\n' +
      'Figure 4: Comparison between _DiffuseKronA_ and LoRA-DreamBooth across varying learning rates on SDXL. In our approach, we set the value of \\(a_{2}\\) to 64. _DiffuseKronA_ produces favorable results across a wider range of learning rates, specifically from \\(1\\times 10^{-4}\\) to \\(1\\times 10^{-3}\\). In contrast, no discernible patterns are observed in LoRA. The right part of the figure shows plots of Text & Image Alignment for LoRA-DreamBooth and _DiffuseKronA_, where points belonging to _DiffuseKronA_ seem to be dense and those of LoRA-DreamBooth seems to be sparse, signifying that _DiffuseKronA_ tends to be more _stable_ than LoRA-DreamBooth while changing learning rates.\n' +
      '\n' +
      'images from various subjects and text prompts and show its effectiveness in generating images with high fidelity, more accurate color distribution of objects, text alignment, and stability as compared to LoRA-DreamBooth.\n' +
      '\n' +
      '**Fidelity & Color Distribution.** Our approach consistently produces images of superior fidelity compared to LoRA-DreamBooth, as illustrated in Figure 5. Notably, the _clock_ generated by _DiffuseKronA_ faithfully reproduces the intricate details, such as the exact depiction of the _numeral_**3**, mirroring the original image. In contrast, the output from LoRA-DreamBooth exhibits difficulties in achieving such high fidelity. Additionally, _DiffuseKronA_ demonstrates improved color distribution in the generated images, a feature clearly evident in the _RC Car_ images in Figure 5. Moreover, it struggles to maintain fidelity to the numeral _numeral_**1** on the chest of the sitting toy. Additional examples are shown in Figure 23 in the Appendix.\n' +
      '\n' +
      '**Text Alignment.**_DiffuseKronA_ comprehends the intricacies and complexities of text prompts provided as input, producing images that align with the given text prompts, as depicted in Figure 6. The generated image of the _anime character_ in response to the prompt exemplifies the meticulous attention _DiffuseKronA_ pays to detail. It elegantly captures the _presence of a shop in the background_ and _accompanying soup bowls_. In contrast, LoRA-DreamBooth struggles to generate an image that aligns seamlessly with the complex input prompt. _DiffuseKronA_ not only generates images that align with text but is also proficient in producing a diverse range of images for a given input. More supportive examples are shown in Figure 24 in the Appendix.\n' +
      '\n' +
      '**Superior Stability.**_DiffuseKronA_ produces images that closely align with the input images across a wide range of learning rates, which are specifically optimized for our approach. In contrast, LoRA-DreamBooth neglects the significance of input images even within its optimal range1 which is evident in Figure 4. The generated _dog_ images by _DiffuseKronA_ maintain a high degree of similarity to the input images throughout its optimal range, while LoRA-DreamBooth struggles to perform at a comparable level. Additional examples are shown in Figure 16 in Appendix.\n' +
      '\n' +
      'Footnote 1: Optimal learning rates are determined through extensive experimentation. Additionally, we have considered observations from (von Platen et al., 2023; Ruiz et al., 2023) while fine-tuning LoRA-DreamBooth.\n' +
      '\n' +
      '**Complex Input images and Prompts.**_DiffuseKronA_ consistently performs well, demonstrating robust performance even when presented with intricate inputs. This success is attributed to the enhanced representational power of Kronecker Adapters. As depicted in Figure 1, _DiffuseKronA_ adeptly captures the features of the _human face_ and _anime characters_, yielding high-quality images. Additionally, from the last row of Figure 1, it is evident that _DiffuseKronA_ elegantly captures the semantic nuances of the text. For instance, considering the context of, _"without blazer"_ and _"upset sitting under the umbrella"_, _DiffuseKronA_ generates exceptional images which demonstrate that even when the input text prompt is huge, _DiffuseKronA_ adeptly captures various concepts mentioned as nouns in the text. It generates images that encompass all the specified concepts while maintaining a coherent and meaningful overall relationship. Furthermore, we refer the readers to Figure 10 and Figure 11 in the Appendix.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c|c|c} \\hline \\hline \\multicolumn{2}{c|}{Model} & Train. Time (\\(\\downarrow\\)) & \\# Param (\\(\\downarrow\\)) & Model size (\\(\\downarrow\\)) \\\\ \\hline \\multirow{3}{*}{**LoRA-DreamBooth**} & _LoRA-DreamBooth_ & \\(\\sim\\)**38 min.** & 5.8 M & 22.32 MB \\\\  & _DiffuseKronA_ & \\(\\sim\\)**40 min.** & **38.8** M & **14.95 MB** \\\\ \\hline \\multirow{3}{*}{**LoRA-DreamBooth**} & \\multirow{3}{*}{\\(\\sim\\)**5.3 min.**} & **1.09 M** & 4.3 MB \\\\  & & & **0.52 M** & **1.2 MB** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Exploring model efficiency metrics (_DiffuseKronA_ variant used (\\(a_{1}=4\\) and \\(a_{2}=64\\)).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline \\hline Model & CLIP-1 (\\(\\uparrow\\)) & CLIP-T (\\(\\uparrow\\)) & DINO (\\(\\uparrow\\)) \\\\ \\hline \\multirow{2}{*}{**LoRA-DreamBooth**} & 0.785 & 0.301 & 0.661 \\\\  & \\(\\pm\\) 0.062 & \\(\\pm\\) 0.027 & \\(\\pm\\) 0.127 \\\\ \\hline \\multirow{2}{*}{**DiffuseKronA**} & **0.809** & **0.322** & **0.677** \\\\  & \\(\\pm\\) **0.052** & \\(\\pm\\) 0.021 & \\(\\pm\\)**0.100** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **Quantitative comparison** of CLIP-1, CLIP-T, and DINO scores between _DiffuseKronA_ and LoRA-Dreambooth. The obtained values are average across 42 datasets, with a learning rate of \\(5\\times 10^{-4}\\) for _DiffuseKronA_ and \\(1\\times 10^{-4}\\) for LoRA-DreamBooth.\n' +
      '\n' +
      'Figure 5: _DiffuseKronA_ preserving superior fidelity.\n' +
      '\n' +
      'Figure 6: _DiffuseKronA_ illustrating enhanced text alignment.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      'adaption. The images generated by LoRA-DreamBooth often require extensive fine-tuning to achieve the desired results. Methods like custom diffusion take more parameters to fine-tune the model. As compared to SVDiff our proposed approach excels in both (a) achieving superior image-text alignment, as depicted in Figure 8, and (b) maintaining parameter efficiency. For each method, we showcase text and image alignment scores in Figure 8 and _DiffuseKronA_ obtains the best alignment qualitatively and quantitatively. Additional results across a variety of datasets and prompts are presented in Figure 31 and Figure 32. Moreover, we present the average scores of all baseline models across 12 datasets, each evaluated with 10 prompts in Table 4.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'We proposed a new parameter-efficient adaption module, _DiffuseKronA_, to enhance text-to-image personalized diffusion models, aiming to achieve high-quality image generation with improved parameter efficiency. Leveraging the Kronecker product\'s capacity to capture structured relationships in weight matrices, _DiffuseKronA_ produces images closely aligned with input text prompts and training images, outperforming LoRA-DreamBooth in visual quality, text alignment, fidelity, parameter efficiency, and stability. _DiffuseKronA_ thus provides a new and efficient tool for advancing text-to-image personalized image generation tasks.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]J. Balaji, S. Nah, X. Huang, A. Vahdat, J. Song, K. Kreis, M. Aittala, T. Aila, S. Laine, B. Catanzaro, et al. (2022) FedMI: text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324. Cited by: SS1.\n' +
      '* [2]J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. (2023) Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf2, pp. 3. Cited by: SS1.\n' +
      '* [3]M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin (2021) Emerging properties in self-supervised vision transformers. External Links: 2102.02748 Cited by: SS1.\n' +
      '* [4]H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. Jiang, M. Yang, K. Murphy, W. T. Freeman, M. Rubinstein, et al. (2023) Muse: text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704. Cited by: SS1.\n' +
      '* [5]J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Y. Wu, Z. Wang, J. Kwok, P. Luo, H. Lu, and Z. Li (2023) Pixart-\\(\\alpha\\): fast training of diffusion transformer for photorealistic text-to-image synthesis. External Links: 2301.00704 Cited by: SS1.\n' +
      '* [6]J. Devlin, M. Chang, K. Lee, and K. Toutanova (2018) Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Cited by: SS1.\n' +
      '* [7]A. Edalati, M. S. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Computer-efficient generative adversarial networks for biomedical image generation. arXiv preprint arXiv:2106.04647. Cited by: SS1.\n' +
      '* [8]A. Edalati, M. Sahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Efficient generative adversarial networks for biomedical image generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [9]A. Edalati, M. Sahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Efficient generative adversarial networks for biomedical image generation. arXiv preprint arXiv:2106.04647. Cited by: SS1.\n' +
      '* [10]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [11]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [12]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [13]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [14]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [15]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [16]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [17]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [18]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [19]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [20]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [21]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [22]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [23]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [24]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [25]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [26]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [27]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [28]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [29]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [30]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '* [31]A. Edalati, M. Tahaei, A. Rashid, V. P. Nia, J. Clark, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '\n' +
      'Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022. URL [https://arxiv.org/abs/2208.01618](https://arxiv.org/abs/2208.01618).\n' +
      '* Greenewald & Hero (2015) Greenewald, K. and Hero, A. O. Robust kronecker product pca for spatio-temporal covariance estimation. _IEEE Transactions on Signal Processing_, 63(23):6368-6378, 2015.\n' +
      '* Gu et al. (2022) Gu, S., Chen, D., Bao, J., Wen, F., Zhang, B., Chen, D., Yuan, L., and Guo, B. Vector quantized diffusion model for text-to-image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 10696-10706, 2022.\n' +
      '* Gu et al. (2023) Gu, Y., Wang, X., Wu, J. Z., Shi, Y., Chen, Y., Fan, Z., Xiao, W., Zhao, R., Chang, S., Wu, W., et al. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. _arXiv preprint arXiv:2305.18292_, 2023.\n' +
      '* Hameed et al. (2023) Hameed, M. G. A., Tahaei, M. S., Mosleh, A., Nia, V. P., Chen, H., Deng, L., Yan, T., and Li, G. Convolutional neural network compression through generalized kronecker product decomposition. _IEEE Transactions on Neural Networks and Learning Systems_, 34(5):2205-2219, 2023.\n' +
      '* Han et al. (2023) Han, L., Li, Y., Zhang, H., Milanfar, P., Metaxas, D., and Yang, F. Svdiff: Compact parameter space for diffusion fine-tuning. _arXiv preprint arXiv:2303.11305_, 2023.\n' +
      '* He et al. (2015) He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, 2015.\n' +
      '* He et al. (2022) He, X., Li, C., Zhang, P., Yang, J., and Wang, X. E. Parameter-efficient model adaptation for vision transformers. _arXiv preprint arXiv:2203.16329_, 2022.\n' +
      '* Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In _Int. Conf. Mach. Learn._, pp. 2790-2799. PMLR, 2019.\n' +
      '* Hu et al. (2021) Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. In _ICLR_, 2021.\n' +
      '* Kumari et al. (2023) Kumari, N., Zhang, B., Zhang, R., Shechtman, E., and Zhu, J.-Y. Multi-concept customization of text-to-image diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 1931-1941, 2023.\n' +
      '* Li et al. (2018) Li, C., Farkhoor, H., Liu, R., and Yosinski, J. Measuring the intrinsic dimension of objective landscapes. In _International Conference on Learning Representations_, 2018.\n' +
      '* Li et al. (2023a) Li, D., Li, J., and Hoi, S. C. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. _arXiv preprint arXiv:2305.14720_, 2023a.\n' +
      '* Li et al. (2023b) Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023b.\n' +
      '* Li et al. (2020) Li, Y., Zhang, R., Lu, J., and Shechtman, E. Few-shot image generation with elastic weight consolidation. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS\'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\n' +
      '* Lui et al. (2019) Lui, C., Bhowmick, S. S., and Jatowt, A. Kandinsky: Abstract art-inspired visualization of social discussions. In _Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR\'19, pp. 1345-1348, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361729. doi: 10.1145/3331184.3331411. URL [https://doi.org/10.1145/3331184.3331411](https://doi.org/10.1145/3331184.3331411).\n' +
      '* Ma et al. (2023) Ma, J., Liang, J., Chen, C., and Lu, H. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. _arXiv preprint arXiv:2307.11410_, 2023.\n' +
      '* Mou et al. (2023) Mou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., and Qie, X. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_, 2023.\n' +
      '* Nagy & Perrone (2003) Nagy, J. G. and Perrone, L. Kronecker products in image restoration. In _Advanced Signal Processing Algorithms, Architectures, and Implementations XIII_, volume 5205, pp. 155-163. International Society for Optics and Photonics, 2003.\n' +
      '* Podell et al. (2023) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '\n' +
      'Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.\n' +
      '* Ronneberger et al. (2015) Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pp. 234-241. Springer, 2015.\n' +
      '* Ruiz et al. (2023) Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aherman, K. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _CVPR_, pp. 22500-22510, June 2023a.\n' +
      '* Ruiz et al. (2023b) Ruiz, N., Li, Y., Jampani, V., Wei, W., Hou, T., Pritch, Y., Wadhwa, N., Rubinstein, M., and Aherman, K. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models, 2023b.\n' +
      '* Ryu (2023) Ryu, S. Low-rank adaptation for fast text-to-image diffusion fine-tuning, 2023. URL [https://github.com/cloneofsimo/lora](https://github.com/cloneofsimo/lora).\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* Tahaei et al. (2022a) Tahaei, M., Charlaix, E., Nia, V., Ghodsi, A., and Rezagholizadeh, M. KroneckerBERT: Significant compression of pre-trained language models through kronecker decomposition and knowledge distillation. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 2116-2127, Seattle, United States, 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main. 154. URL [https://aclanthology.org/2022.naacl-main.154](https://aclanthology.org/2022.naacl-main.154).\n' +
      '* Tahaei et al. (2022b) Tahaei, M., Charlaix, E., Nia, V., Ghodsi, A., and Rezagholizadeh, M. Kroneckerbert: Significant compression of pre-trained language models through kronecker decomposition and knowledge distillation. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 2116-2127, 2022b.\n' +
      '* Tewel et al. (2023) Tewel, Y., Gal, R., Chechik, G., and Atzmon, Y. Key-locked rank one editing for text-to-image personalization. In _ACM SIGGRAPH 2023 Conference Proceedings_, pp. 1-11, 2023.\n' +
      '* Thakker et al. (2019) Thakker, U., Beu, J., Gope, D., Zhou, C., Fedorov, I., Dasika, G., and Mattina, M. Compressing rnns for iot devices by 15-38x using kronecker products. _arXiv preprint arXiv:1906.02876_, 2019.\n' +
      '* von Platen et al. (2023) von Platen, P., Patil, S., Lozhkov, A., Cuenca, P., Lambert, N., Rasul, K., Davaadorj, M., and Wolf, T. Diffusers: State-of-the-art diffusion models, 2023. URL [https://github.com/huggingface/diffusers](https://github.com/huggingface/diffusers).\n' +
      '* Wang et al. (2023) Wang, D., Wu, B., Zhao, G., Yao, M., Chen, H., Deng, L., Yan, T., and Li, G. Kronecker cp decomposition with fast multiplication for compressing rnns. _IEEE Transactions on Neural Networks and Learning Systems_, 34(5):2205-2219, 2023.\n' +
      '* Ye et al. (2023) Ye, H., Zhang, J., Liu, S., Han, X., and Yang, W. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. _arXiv preprint arXiv:2308.06721_, 2023.\n' +
      '* Yeh et al. (2023) Yeh, S.-Y., Hsieh, Y.-G., Gao, Z., Yang, B. B., Oh, G., and Gong, Y. Navigating text-to-image customization: From lycoris fine-tuning to model evaluation. _arXiv preprint arXiv:2309.14859_, 2023.\n' +
      '* Yu et al. (2022) Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scaling autoregressive models for content-rich text-to-image generation. _Transactions on Machine Learning Research_, 2022.\n' +
      '* Zhang et al. (2020) Zhang, A., Tay, Y., Zhang, S., Chan, A., Luu, A. T., Hui, S., and Fu, J. Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with \\(1/n\\) parameters. In _International Conference on Learning Representations_, 2020.\n' +
      '* Zhang et al. (2023) Zhang, L., Rao, A., and Agrawala, M. Adding conditional control to text-to-image diffusion models, 2023.\n' +
      '* Zhang et al. (2015) Zhang, X., Yu, F. X., Guo, R., Kumar, S., Wang, S., and Chang, S.-F. Fast orthogonal projection based on kronecker product. In _2015 IEEE International Conference on Computer Vision (ICCV)_, pp. 2929-2937, 2015. doi: 10.1109/ICCV.2015.335.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:12]\n' +
      '\n' +
      'Figure 11: Results for car modifications and showcasing our method’s potential application in the Automobile industry.\n' +
      '\n' +
      'Figure 10: The results for the human face and anime characters generation highlight our method’s endless application in creating portraits, animes, and avatars.\n' +
      '\n' +
      'adaptation as a subspace training problem via Kronecker Adaptation (KAdaptation) and performs a comprehensive benchmarking over different efficient adaptation methods.\n' +
      '\n' +
      'On the other hand, authors of (Thakker et al., 2019) compressed RNNs for resource-constrained environments (e.g. IOT devices) using Kronecker product (KP) by 15-38x with minimal accuracy loss and by quantizing the resulting models to 8 bits, the compression factor is further pushed to 50x. In (Wang et al., 2023), RNNs are compressed based on a novel Kronecker CANDECOMP/PARAFAC (KCP) decomposition, derived from Kronecker tensor (KT) decomposition, by proposing two fast algorithms of multiplication between the input and the tensor-decomposed weight.\n' +
      '\n' +
      'Besides all of the above, Kronecker decomposition is also being applied for GPT compression (Edalati et al., 2022c) which attempts to compress the linear mappings within the GPT-2 model. The proposed model, Kronecker GPT-2 (KnGPT2) is initialized based on the Kronecker decomposed version of the GPT-2 model. Subsequently, it undergoes a very light pre-training on only a small portion of the training data with intermediate layer knowledge distillation (ILKD).\n' +
      '\n' +
      'From the aforementioned literature study, we have witnessed the efficacy of Kronecker products for the task of model compression within various domains including NLP, RNN, CNN, ViT, and GPT space. Consequently, it has sparked considerable interest in exploring its impact on Generative models.\n' +
      '\n' +
      'Figure 12: A collection of sample images representing all individual subjects involved in this study. Our collected subjects are highlighted in green.\n' +
      '\n' +
      '## Appendix B Datasets Descriptions\n' +
      '\n' +
      'We have incorporated a total of 25 datasets from DreamBooth (Ruiz et al., 2023a), encompassing images of backpacks, dogs, cats, and stuffed animals. Additionally, we integrated 7 datasets from custom diffusion (Kumari et al., 2023) to introduce variety in our experimentation. To assess our model\'s ability to capture spatial features on faces, we curated a dataset consisting of 4 to 7 images each of 4 humans, captured from different angles. To further challenge our model against complex input images and text prompts, we compiled a dataset featuring 6 anime images from various sources. All datasets are categorized into four groups: _living animals_, _non-living objects_, _anime_, and _human faces_. Furthermore, the keywords utilized for fine-tuning the model remain consistent with those specified in the original papers. In Figure 12, we present a sample image for all the considered subjects used in this study.\n' +
      '\n' +
      '**Image Attribution.** Our collected datasets are taken from the following resources:\n' +
      '\n' +
      '* **Rolls Royce:*\n' +
      '* **Hugging Face:** [https://huggingface.co/brand](https://huggingface.co/brand)\n' +
      '\n' +
      '* **Shoko Komi:*\n' +
      '* **Kakashi Hatake:*\n' +
      '\n' +
      '## Appendix C Evaluation Metrics\n' +
      '\n' +
      'We utilize metrics introduced in DreamBooth (Ruiz et al., 2023a) for evaluation: DINO and CLIP-I scores measure subject fidelity, while CLIP-T assesses image-text alignment. The DINO score is the normalized pairwise cosine similarity between the ViT-S/16 DINO embeddings of the generated and input (real) images. Similarly, the CLIP-I score is the normalized pairwise CLIP ViT-B/32 image embeddings of the generated and input images. Meanwhile, the CLIP-T score computes the normalized cosine similarity between the given text prompt and generated image CLIP embeddings.\n' +
      '\n' +
      'Let\'s denote the pre-trained CLIP Image encoder as \\(\\mathcal{I}\\), the CLIP text encoder as \\(\\mathcal{T}\\), and the DINO model as \\(\\mathcal{V}\\). We measure cosine similarity between two embeddings \\(x\\) and \\(y\\) as \\(sim(x,\\,y)=\\frac{x.y}{\\left\\|x\\right\\|\\cdot\\left\\|y\\right\\|}\\). Given two sets of images, we represent the input image set as \\(\\mathcal{R}=\\left\\{R_{i}\\right\\}_{i=1}^{n}\\) and generated image set as \\(\\mathcal{G}=\\left\\{G_{i}\\right\\}_{i=1}^{m}\\) corresponding to the prompt set \\(\\mathcal{P}=\\left\\{P_{i}\\right\\}_{i=1}^{m}\\), where \\(m\\) and \\(n\\) represents the number of generated and input images, respectively and \\(R,G\\in\\mathbb{R}^{3\\times H\\times W}\\) (\\(H\\) and \\(W\\) is the height and width of the image). Then, CLIP-I image-to-image and CLIP-T text-to-image similarity scores would be computed as \\(S_{CLIP}^{I}\\) and \\(S_{CLIP}^{T}\\), respectively.\n' +
      '\n' +
      '\\[S_{CLIP}^{I}=\\frac{1}{mn}\\sum_{i=1}^{n}\\sum_{j=1}^{m}sim(\\mathcal{I}\\left(R_{i} \\right),\\,\\mathcal{I}\\left(G_{j}\\right)) \\tag{6}\\]\n' +
      '\n' +
      '\\[S_{CLIP}^{T}=\\frac{1}{m}\\sum_{i=1}^{m}sim(\\mathcal{I}\\left(G_{i}\\right),\\, \\mathcal{T}\\left(P_{i}\\right)) \\tag{7}\\]Similarly, the DINO image-to-image similarity score would be computed as\n' +
      '\n' +
      '\\[S_{DINO}=\\frac{1}{mn}\\sum_{i=1}^{n}\\sum_{j=1}^{m}sim(\\mathcal{V}\\left(R_{i}\\right), \\;\\mathcal{V}\\left(G_{j}\\right)). \\tag{8}\\]\n' +
      '\n' +
      'Notably, the DINO score is preferred to assess subject fidelity owing to its sensitivity to differentiate between subjects within a given class. In personalized T2I generations, all three metrics should be considered jointly for evaluation to avoid a biased conclusion. For instance, models that copy training set images will have high DINO and CLIP-I scores but low CLIP-T scores, while a vanilla T2I generative model like SD and SDXL without subject knowledge will produce high CLIP-T scores with poor subject alignment. As a result, both models are not considered desirable for the subject-driven T2I generation. In Table-5, we showcase mean subject-specific CLIP-I, CLIP-T, and DINO scores along with standard deviations computed across 36 datasets, with a total of around 1600 generated images and prompts.\n' +
      '\n' +
      '## Appendix D _DiffuseKronA_ Ablations Study\n' +
      '\n' +
      'As outlined in 4.2 of the main paper, we explore various trends and observations derived from extensive experimentation on the datasets specified in Figure 12.\n' +
      '\n' +
      '### Choice of modules to fine-tune the model\n' +
      '\n' +
      'Within the UNet network\'s transformer block, the linear layers consist of two components: a) attention matrices and b) a feed-forward network (FFN). Our investigation focuses on discerning the weight matrices with the highest importance for fine-tuning, aiming for efficiency in parameter utilization.\n' +
      '\n' +
      'Our findings reveal that fine-tuning only the attention weight matrices, namely \\((W_{K},W_{Q},W_{V},W_{O})\\), proves to be the most impactful and parameter-efficient strategy. Conversely, fine-tuning the FFN layers does not significantly enhance image synthesis quality but substantially increases the parameter count, approximately doubling the computational load. Refer to Figure 13 for a visual representation comparing synthesis image quality with and without fine-tuning FFN layers on top of attention matrices. This graph unequivocally demonstrates that incorporating MLP layers does not enhance fidelity in the results. On the contrary, it diminishes the quality of generated images in certain instances, such as _"A [V] backpack in sunflower field"_, while concurrently escalating the number of trainable parameters substantially, approximately 2x times.\n' +
      '\n' +
      'This approach of exclusively fine-tuning attention layers not only maximizes efficiency but also helps maintain a lower overall parameter count. This is particularly advanta\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c} \\hline \\hline\n' +
      '**Subject** & Cat & Cat2 & Dog2 & Dog & Dog3 & Dog6 \\\\ \\hline\n' +
      '**CLIP-I** & 0.858 \\(\\pm\\) 0.017 & 0.826 \\(\\pm\\) 0.030 & 0.833 \\(\\pm\\) 0.023 & 0.854 \\(\\pm\\) 0.015 & 0.789 \\(\\pm\\) 0.027 & 0.845 \\(\\pm\\) 0.031 \\\\\n' +
      '**CLIP-T** & 0.348 \\(\\pm\\) 0.033 & 0.343 \\(\\pm\\) 0.030 & 0.331 \\(\\pm\\) 0.028 & 0.349 \\(\\pm\\) 0.029 & 0.338 \\(\\pm\\) 0.025 & 0.323 \\(\\pm\\) 0.032 \\\\\n' +
      '**DINO** & 0.814 \\(\\pm\\) 0.025 & 0.752 \\(\\pm\\) 0.021 & 0.750 \\(\\pm\\) 0.049 & 0.856 \\(\\pm\\) 0.008 & 0.549 \\(\\pm\\) 0.060 & 0.788 \\(\\pm\\) 0.017 \\\\ \\hline \\hline\n' +
      '**Subject** & Dog5 & Dog7 & Dog8 & Doggy & Cat3 & Cat4 \\\\ \\hline\n' +
      '**CLIP-I** & 0.824 \\(\\pm\\) 0.024 & 0.853 \\(\\pm\\) 0.015 & 0.829 \\(\\pm\\) 0.021 & 0.734 \\(\\pm\\) 0.031 & 0.834 \\(\\pm\\) 0.034 & 0.861 \\(\\pm\\) 0.016 \\\\\n' +
      '**CLIP-T** & 0.337 \\(\\pm\\) 0.026 & 0.334 \\(\\pm\\) 0.025 & 0.343 \\(\\pm\\) 0.026 & 0.329 \\(\\pm\\) 0.030 & 0.348 \\(\\pm\\) 0.029 & 0.349 \\(\\pm\\) 0.032 \\\\\n' +
      '**DINO** & 0.761 \\(\\pm\\) 0.001 & 0.730 \\(\\pm\\) 0.049 & 0.717 \\(\\pm\\) 0.050 & 0.686 \\(\\pm\\) 0.039 & 0.744 \\(\\pm\\) 0.031 & 0.863 \\(\\pm\\) 0.030 \\\\ \\hline \\hline\n' +
      '**Subject** & Nami (Anime) & Kiriko (Anime) & Kakshi (Anime) & Shoko Komi (Anime) & Harshit (Human) & Nityanand (Human) \\\\ \\hline\n' +
      '**CLIP-I** & 0.781 \\(\\pm\\) 0.035 & 0.738 \\(\\pm\\) 0.039 & 0.834 \\(\\pm\\) 0.028 & 0.761 \\(\\pm\\) 0.029 & 0.724 \\(\\pm\\) 0.018 & 0.665 \\(\\pm\\) 0.031 \\\\\n' +
      '**CLIP-T** & 0.337 \\(\\pm\\) 0.029 & 0.320 \\(\\pm\\) 0.032 & 0.318 \\(\\pm\\) 0.031 & 0.356 \\(\\pm\\) 0.028 & 0.297 \\(\\pm\\) 0.036 & 0.307 \\(\\pm\\) 0.030 \\\\\n' +
      '**DINO** & 0.655 \\(\\pm\\) 0.023 & 0.483 \\(\\pm\\) 0.041 & 0.617 \\(\\pm\\) 0.061 & 0.596 \\(\\pm\\) 0.024 & 0.555 \\(\\pm\\) 0.025 & 0.447 \\(\\pm\\) 0.068 \\\\ \\hline \\hline\n' +
      '**Subject** & Shyam (Human) & Teapot & Robot Toy & Backpack & Dog Backpack & Rc Car \\\\ \\hline\n' +
      '**CLIP-I** & 0.731 \\(\\pm\\) 0.015 & 0.836 \\(\\pm\\) 0.051 & 0.828 \\(\\pm\\) 0.026 & 0.907 \\(\\pm\\) 0.026 & 0.774 \\(\\pm\\) 0.037 & 0.797 \\(\\pm\\) 0.020 \\\\\n' +
      '**CLIP-T** & 0.297 \\(\\pm\\) 0.026 & 0.347 \\(\\pm\\) 0.025 & 0.285 \\(\\pm\\) 0.032 & 0.347 \\(\\pm\\) 0.021 & 0.333 \\(\\pm\\) 0.027 & 0.321 \\(\\pm\\) 0.027 \\\\\n' +
      '**DINO** & 0.531 \\(\\pm\\) 0.030 & 0.528 \\(\\pm\\) 0.132 & 0.642 \\(\\pm\\) 0.023 & 0.660 \\(\\pm\\) 0.088 & 0.649 \\(\\pm\\) 0.037 & 0.651 \\(\\pm\\) 0.065 \\\\ \\hline \\hline\n' +
      '**Subject** & Shiny Shoes & Duck & Clock & Vase & Plushiel & Monster Toy \\\\ \\hline\n' +
      '**CLIP-I** & 0.806 \\(\\pm\\) 0.025 & 0.845 \\(\\pm\\) 0.023 & 0.825 \\(\\pm\\) 0.062 & 0.827 \\(\\pm\\) 0.013 & 0.897 \\(\\pm\\) 0.014 & 0.782 \\(\\pm\\) 0.041 \\\\\n' +
      '**CLIP-T** & 0.308 \\(\\pm\\) 0.023 & 0.303 \\(\\pm\\) 0.016 & 0.308 \\(\\pm\\) 0.035 & 0.332 \\(\\pm\\) 0.026 & 0.308 \\(\\pm\\) 0.030 & 0.308 \\(\\pm\\) 0.029 \\\\\n' +
      '**DINO** & 0.735 \\(\\pm\\) 0.090 & 0.682 \\(\\pm\\) 0.049 & 0.590 \\(\\pm\\) 0.158 & 0.705 \\(\\pm\\) 0.025 & 0.813 \\(\\pm\\) 0.027 & 0.573 \\(\\pm\\) 0.060 \\\\ \\hline \\hline\n' +
      '**Subject** & Plushiel2 & Plushiel3 & Building & Book & Car & HuggingFace \\\\ \\hline\n' +
      '**CLIP-I** & 0.803 \\(\\pm\\) 0.022 & 0.792 \\(\\pm\\) 0.015 & 0.852 \\(\\pm\\) 0.013 & 0.695 \\(\\pm\\) 0.023 & 0.830 \\(\\pm\\) 0.024 & 0.810 \\(\\pm\\) 0.002 \\\\\n' +
      '**CLIP-T** & 0.324 \\(\\pm\\) 0.024 & 0.337 \\(\\pm\\) 0.031 & 0.268 \\(\\pm\\) 0.023 & 0.301 \\(\\pm\\) 0.022 & 0.299 \\(\\pm\\) 0.032 & 0.288 \\(\\pm\\) 0.042 \\\\\n' +
      '**DINO** & 0.728 \\(\\pm\\) 0.020 & 0.766 \\(\\pm\\) 0.033 & 0.742 \\(\\pm\\) 0.019 & 0.579 \\(\\pm\\) 0.040 & 0.684 \\(\\pm\\) 0.036 & 0.692 \\(\\pm\\) 0.001 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Average metrics (CLIP-I, CLIP-T, and DINO scores) from various prompt runs for each subject using our proposed method.\n' +
      '\n' +
      'geous when computational resources are limited, ensuring computational efficiency in the fine-tuning process.\n' +
      '\n' +
      '### Effect of Kronecker Factors\n' +
      '\n' +
      '**How to initialize the Kronecker factors?** Initialization plays a crucial role in the fine-tuning process. Networks that are poorly initialized can prove challenging to train. Therefore, having a well-crafted initialization strategy is crucial for achieving effective fine-tuning. In our experiments, we explored three initialization methods: Normal initialization, Kaiming Uniform initialization (He et al., 2015), and Xavier initialization. These methods were applied to initialize the Kronecker factors \\(A_{k}\\) and \\(B_{k}\\). We observed that initializing both factors with the same type of initialization failed to preserve fidelity. Surprisingly, initializing \\(B_{k}\\) with zero yielded the best results in the fine-tuning process.\n' +
      '\n' +
      'As illustrated in Figure 14, images initialized with (\\(A_{k}=\\text{Normal}^{s}\\), \\(B_{k}=0\\)) and (\\(A_{k}=\\text{KU}\\), \\(B_{k}=0\\)) produce the most favorable results, while images initialized with (\\(A_{k}=\\text{Normal}^{s}\\), \\(B_{k}=\\text{Normal}^{s}\\)) and (\\(A_{k}=\\text{XU}\\), \\(B_{k}=\\text{XU}\\)) result in the least satisfactory generations. Here, \\(s\\in{1,2}\\) denotes two different normal distributions - \\(\\mathcal{N}\\left(0,1/a_{2}\\right)\\) and \\(\\mathcal{N}\\left(0,\\sqrt{min(d,h)}\\right)\\) respectively, where \\(d\\) and \\(h\\) represents in features and out features dimension.\n' +
      '\n' +
      'Figure 13: Qualitative and Quantitative comparison between fine-tuning with MLP and w/o MLP. Fine-tuning MLP layers introduces more parameters and doesn’t enhance image generation compared with fine-tuning solely attention-weight matrices. So, the best outcomes and efficient use of parameters occur when only attention weight (without MLP) matrices are fine-tuned.\n' +
      '\n' +
      '**Effect of size of Kronecker Factors.** The size of the Kronecker factors significantly influences the images generated by _DiffuseKronA_. Larger Kronecker factors tend to produce images with higher resolution and more detailing, while smaller Kronecker factors result in lower-resolution images with less detailing. Images generated with larger Kronecker factors tend to look more realistic, while those generated with smaller Kronecker factors appear more abstract. Varying the Kronecker factors can result in a wide range of images, from highly detailed and realistic to more abstract and lower resolution.\n' +
      '\n' +
      'In Figure 15 when both \\(a_{1}\\) and \\(a_{2}\\) are set to relatively high values (8 and 64 respectively), the generated images are of very high fidelity and detail. The features of the dog and the house in the background are more defined and realistic with the house having a blue colour as mentioned in the prompt. When \\(a_{1}\\) is halved (4) while maintaining the same (64) results in images where the dog and the house are still quite detailed due to the high value of \\(a_{2}\\), but perhaps less so than in the previous case due to the smaller value of \\(a_{1}\\). However, when the factors are small \\(\\leq\\) 8, not only the generated images do not adhere to the prompt, but the number of trainable parameters increases drastically.\n' +
      '\n' +
      'In Table 6, we present the count of trainable parameters corresponding to different Kronecker factors.\n' +
      '\n' +
      'Figure 14: Impact of different initialization strategies: optimal outcomes are achieved when initializing \\(B_{k}\\) to zero while initializing \\(A_{k}\\) with either a Normal or Kaiming uniform distribution.\n' +
      '\n' +
      'Figure 15: Effect of Kronecker factors _i.e._, \\(a_{1}\\) and \\(a_{2}\\) in image generations. Optimal selection of \\(a_{1}\\) and \\(a_{2}\\) considers **image fidelity** and **parameter count**. Following this, we choose \\(a_{1}\\) and \\(a_{2}\\) as 4 and 64, respectively, interpreting that the lower Kronecker factor (\\(A\\)) should have a lower dimension compared to the upper Kronecker factor (\\(B\\)).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:21]\n' +
      '\n' +
      'iterations as depicted in Figure 17 and Figure 18.\n' +
      '\n' +
      'The images generated by _DiffuseKronA_ show a clear progression in quality with respect to different steps. As the steps increase, the model seems to refine the details and improve the quality of the images. This iterative process allows the model to gradually improve the image, adding more details and making it more accurate to the prompt.\n' +
      '\n' +
      'In Figure 17 for instance, _"A cat floating on water in a swimming pool"_, in the initial iterations, the model generates a basic image of a cat floating on water. As the iterations progress and reach 500, the model refines the image, adding more details such as the color and texture of the cat, the ripples in the water, and the details of the swimming pool. At 1000 steps the image is a detailed and realistic representation of the prompt.\n' +
      '\n' +
      'In Figure 17, _"A backpack on top of a white rug"_, the early iterations produce a simple image of a backpack on a white surface. However, as the iterations increase, the model adds\n' +
      '\n' +
      'Figure 17: Effect of training steps in image generation on SDXL. In the case of simple prompts (row 1), _DiffuseKronA_ consistently delivers favorable results between steps 500 and 1000. Conversely, for more complex prompts (row 2), reaching the desired outcome might necessitate waiting until after step 1000.\n' +
      '\n' +
      'Figure 19: One-shot image generation results showcase the remarkable effectiveness of _DiffuseKronA_ while preserving high fidelity and better text alignment.\n' +
      '\n' +
      'Figure 18: Plots depicting image alignment, text alignment, and DINO scores against training iterations. The scores are computed from the same set of images and prompts as depicted in Figure 17.\n' +
      '\n' +
      'more details to the backpack, such as the zippers, pockets, and straps. It also starts to add texture to the white rug, making it look more realistic. By the final iteration, the white rug gets smoother in texture producing a fine image.\n' +
      '\n' +
      '### Effect of the number of training images\n' +
      '\n' +
      '#### d.5.1 One shot image generation\n' +
      '\n' +
      'The images are high-quality and accurately represent the text prompts. They are clear and well-drawn, and the content of each image matches the corresponding text prompt perfectly. For instance, in Figure 19, the image of the _"A [V] logo"_ is a yellow smiley face with hands. The _"made as a coin"_ prompt resulted in a grey ghost with a white border, demonstrating the model\'s ability to incorporate abstract concepts. The _"futuristic neon glow"_ and _"made with watercolours"_ prompts resulted in a pink and a yellow octopus respectively, showcasing the model\'s versatility in applying different artistic styles. The model\'s ability to generate an image of a guitar-playing octopus on a grey notebook from the prompt _"sticker on a notebook"_ is a testament to its advanced capabilities.\n' +
      '\n' +
      'The images are diverse in terms of style and content which is impressive, especially considering that these images were generated in a one-shot setting which makes it suitable for image editing tasks. While our model demonstrates remarkable proficiency in generating compelling results with a single input image, it encounters challenges when attempting to generate diverse poses or angles. However, when supplied with multiple images (2, 3, or 4), our model adeptly captures additional spatial features from the input images, facilitating the generation of images with a broader range of poses and angles. Our model can effectively use the information from multiple input images to generate more accurate and detailed output images as depicted in Figure 20.\n' +
      '\n' +
      '### Effect of Inference Hyperparameters\n' +
      '\n' +
      '**Guidance Score (\\(\\alpha\\)).** The guidance score, denoted as \\(\\alpha\\), regulates the variation and distribution of colors in the generated images. A lower guidance score produces a more subdued version of colors in the images, aligning with the description provided in the input text prompt. In contrast, a higher guidance score results in images with more vibrant and pronounced colors. Guidance scores ranging from 7 to 10 generally yield images with an appropriate and well-distributed color palette.\n' +
      '\n' +
      'In the example of _"A [V] toy"_ in Figure 21, when the prompt is _"made of purple color"_, it is evident that a reddish lavender hue is generated for a guidance score of 1 or 3. Conversely, with a guidance score exceeding 15, a mulberry shade is produced. For guidance scores close to 8, images with a pure purple color are formed.\n' +
      '\n' +
      '**Number of inference Steps.** The number of steps plays a crucial role in defining the granularity of the generated images. As illustrated in Figure 22, during the initial steps, the model creates a subject that aligns with the text prompt and begins incorporating features from the input image. With the progression of generation, finer details emerge in the images. Optimal results, depending on the complexity of prompts, are observed within the range of 30 to 70 steps, with an average of 50 steps proving to be the most effective. However, exceeding 100 steps results in the introduction of noise and a decline in the quality of the generated images.\n' +
      '\n' +
      'The quality of the generated images appears to improve with an increase in the number of inference steps. For instance, the images for the prompt _"a toy"_ and _"wearing sunglasses"_ appear to be of higher quality at 50 and 75 inference steps respectively, compared to at 10 inference steps.\n' +
      '\n' +
      '## Appendix E Detailed study on LoRA-DreamBooth vs _DiffuseKronA_\n' +
      '\n' +
      'In this section, we expand our analysis of model performance (from Section 4.3), comparing LoRA-DreamBooth and _DiffuseKronA_ across various aspects, including fidelity, color distribution, text alignment, stability, and complexity.\n' +
      '\n' +
      '### Multiplicative Rank Property and Gradient Updates\n' +
      '\n' +
      'Let \\(A\\) and \\(B\\) be \\(m\\times n\\) and \\(p\\times q\\) matrices respectively. Suppose that \\(A\\) has rank \\(r\\) and \\(B\\) has rank \\(s\\).\n' +
      '\n' +
      '**Theorem E.1**.: _Ranks for dot product are bound by the rank of multiplicand and multiplier, i.e. \\(rank(A\\cdot B)=\\min(rank(A),\\ rank(B))=\\min(r,\\ s)\\)._\n' +
      '\n' +
      '**Theorem E.2**.: _Ranks for Kronecker products are multiplicative i.e. \\(rank(A\\otimes B)=rank(A)\\times rank(B)=r\\times s\\)._\n' +
      '\n' +
      'Since the Kronecker Product has the advantage of multiplicative rank, it has a better representation of the underlying distribution of images as compared to the dot product.\n' +
      '\n' +
      'Another notable difference between the Low-rank decomposition (LoRA) and the Kronecker product is when computing the derivatives, denoted by \\(d\\left(\\cdot\\right)\\). In the case of LoRA, \\(d\\left(A\\cdot B\\right)=d\\left(A\\right)\\cdot B+A\\cdot d\\left(B\\right)\\). But in the case of the Kronecker product, \\(d\\left(A\\otimes B\\right)=d\\left(A\\right)\\otimes d\\left(B\\right)\\). The gradient updates in LoRA are direct without a structured relationship, whereas the Kronecker product preserves the structure during an update. While a dot product is simpler and LoRA updates each parameter independently, a Kronecker product introduces structured updates that can be beneficial when preserving relationships between parameters stored in \\(A\\) and \\(B\\).\n' +
      '\n' +
      '### Fidelity & Color Distribution\n' +
      '\n' +
      '_DiffuseKronA_ generates images of superior fidelity as compared to LoRA-DreamBooth in lieu of the higher representational power of Kronecker Products along with its ability to capture spatial features.\n' +
      '\n' +
      'In the example of _"A [V] backpack"_ in Figure 23, the following observations can be made:\n' +
      '\n' +
      '(1) _"with the Eiffel Tower in the background"_: The backpack generated by _DiffuseKronA_ is pictured with the Eiffel Tower in the background, creating a striking contrast between the red of the backpack and the muted colors of the cityscape, which LoRA-DreamBooth fails to do.\n' +
      '\n' +
      '(2) _"city in background"_: The backpack generated by _DiffuseKronA_ is set against a city backdrop, where the red color stands out against the neutral tones of the buildings, whereas, LoRA-DreamBooth does not generate high contrast between images.\n' +
      '\n' +
      '(3) _"on the beach"_: The image generated by _DiffuseKronA_ shows the backpack on a beach, where the red contrasts with the blue of the water and the beige of the sand.\n' +
      '\n' +
      'Figure 20: The influence of training images on fine-tuning. Even though _DiffuseKronA_ produces impressive results with a single image, the generation of images with a broader range of perspectives is enhanced when more training images are provided with variations.\n' +
      '\n' +
      'Figure 21: Images produced by adjusting the guidance score (\\(\\alpha\\)) reveal that a score of \\(7\\) produces the most realistic results. Increasing the score beyond \\(7\\) significantly amplifies the contrast of the images.\n' +
      '\n' +
      'Figure 22: The influence of inference steps on image generation. Optimal results are achieved in the range of 50-70 steps, striking a balance between textual input and subject fidelity. Here, we opted for 50 inference steps to minimize inference time.\n' +
      '\n' +
      'Figure 23: Comparison of fidelity and color preservation in _DiffuseKronA_ and LoRA-DreamBooth.\n' +
      '\n' +
      '### Text Alignment\n' +
      '\n' +
      '_DiffuseKronA_ is more accurate in aligning text with images compared to the Lora-DreamBooth. For instance, in the first row, _DiffuseKronA_ correctly aligns the text with _"sunflowers inside"_ with the image of a vase with sunflowers, whereas LoRA-DreamBooth fails to align the sunflower in the vase of the same color as of input images.\n' +
      '\n' +
      'In more complex input examples like in Figure 24, such as the one involving anime in _"A [V] character"_, the generated images by LoRA-DreamBooth lack the sense of cooking a meal and a karaoke bar, whereas _DiffuseKronA_ consistently produces images that closely align with the provided text prompts.\n' +
      '\n' +
      '### Complex Input images and Prompts\n' +
      '\n' +
      '_DiffuseKronA_ demonstrates a notable emphasis on capturing nuances within text prompts and excels in preserving intricate details from input images to the highest degree. In contrast, LoRA-DreamBooth lacks these properties. This distinction is evident in Figure 25, where, for the prompt _"A [V] face"_, _DiffuseKronA_ successfully generates an ivorywhite blazer and a smiling face, while LoRA-DreamBooth struggles to maintain both the color and the smile on the face.\n' +
      '\n' +
      'Similarly, for the prompt _"A [V] clock"_ in Figure 25, _DiffuseKronA_ accurately reproduces detailed numbers, particularly 3, from the input images. Although it encounters challenges in preserving the structure of numbers while creating a clock of cubical shape, it still maintains a strong focus on text details-- a characteristic lacking in LoRA-DreamBooth.\n' +
      '\n' +
      '### Qualitative and Quantitative comparison\n' +
      '\n' +
      'We have assessed the image generation capabilities of _DiffuseKronA_ and LoRA-DreamBooth on SDXL (Podell et al., 2023). Our findings reveal that _DiffuseKronA_ excels in generating images with high fidelity, more accurate color distribution, and greater stability compared to LoRA-DreamBooth.\n' +
      '\n' +
      'Figure 24: Comparison of text alignment in generated images by our proposed _DiffuseKronA_ and LoRA-DreamBooth.\n' +
      '\n' +
      '## Appendix F Comparison with other Low-Rank Decomposition methods\n' +
      '\n' +
      'In this section, we compare our _DiffuseKronA_ with low-rank methods other than LoRA, specifically with LoKr (Yeh et al., 2023) and LoHA (Yeh et al., 2023). We also note that our implementation is independent of the LyCORIS project (Yeh et al., 2023), and we did not use LoKr nor LoHA in _DiffuseKronA1_. We summarize the key differences between _DiffuseKronA_ and these methods as follows:\n' +
      '\n' +
      'Footnote 1: To ensure a fair comparison, we have incorporated LoKr and LoHA into the SDXL backbone.\n' +
      '\n' +
      '_DiffuseKronA_ has 2 controllable parameters (\\(a_{1}\\) and \\(a_{2}\\)), which are chosen manually through extensive experiments (refer to Figure 15 and Table 6), whereas LoKr (Yeh et al., 2023) follows the procedure mentioned in the factorization function (see right) which depends on input dimension and another hyper-parameter called _factor_. Following the descriptions on the implementation of Figure 2 in (Yeh et al., 2023), and we quote "we set the factor to 8 and do not perform further decomposition of the second block", the default implementation makes \\(A\\) a square matrix of dimension (\\(\\textit{factor}\\times\\textit{factor}\\)). Notably, for any factor, \\(f>0\\), \\(A\\) would always be a square matrix of shape \\((f\\times f)\\) which is a special case (a subset) of _DiffusKronA_ (diagonal entry in Figure 15) but for \\(f=-1\\), \\(A\\) matrix size would be completely dependent upon dimension, and it would not be a square matrix always.\n' +
      '\n' +
      '```\n' +
      '1deffactorization(dim:int,factor:int=-1)->tuple(int,int]:\n' +
      '2\n' +
      '3iffactor>0and(dim%factor)==0:\n' +
      '4m=factor\n' +
      '5n=dim//factor\n' +
      '6ifm>n:\n' +
      '7n,m=m,n\n' +
      '8returnm,n\n' +
      '9iffactor<0:\n' +
      '10factor=dim\n' +
      '11m,n=1,dim\n' +
      '12length=m+n\n' +
      '13whilem<n:\n' +
      '14new_m=m+1\n' +
      '15whiledim%new_m!=0:\n' +
      '16new_m+=1\n' +
      '17new_n=dim//new_m\n' +
      '18ifnew_m+new_m>lengthornew_m>factor:\n' +
      '19break\n' +
      '20else:\n' +
      '21m,n=new_m,new_m\n' +
      '22ifm>n:\n' +
      '23n,m=m,n\n' +
      '24returnm,n\n' +
      '```\n' +
      '\n' +
      'Listing 1: This code snippet is extracted from the official LyCORIS codebase (Link).\n' +
      '\n' +
      'Figure 25: Comparison of image generation on complex prompts and input images by _DiffuseKronA_ and LoRA-DreamBooth.\n' +
      '\n' +
      'These attributes make our way of performing Kronecker decomposition a superset of LoKr, offering greater control and flexibility compared to LoKr. On the other hand, LoHA has only one controllable parameter, _i.e._, rank, similar to LoRA.\n' +
      '\n' +
      'LoKr takes the generic form of \\(\\Delta W=A\\otimes(B\\cdot C)\\), and LoHA adopts \\(\\Delta W=(A\\cdot B)\\odot(C\\cdot D)\\), where \\(\\odot\\) denotes the Hadamard product. For more details, we refer the readers to Figure 1 in (Yeh et al., 2023). Based on the definition, LoHA does not explore the benefits of using Kronecker decomposition.\n' +
      '\n' +
      'Yeh et al. (2023) provided the first use of Kronecker decomposition in Diffusion model fine-tuning but limited analysis in the few-shot T2I personalization setting. In our study, we conducted detailed analysis and exploration to demonstrate the benefits of using Kronecker decomposition. Our new insights include large-scale analysis of parameter efficiency, enhanced stability to hyperparameters, and improved text alignment and fidelity, among others.\n' +
      '\n' +
      'We further compare our _DiffuseKronA_ with LoKr and LoHA using the default implementations from (Yeh et al., 2023) in Figure 26 and Figure 27, respectively. However, the default settings were used in the SD variant, and it is also evident that personalized T2I generations are very sensitive to model settings and hyper-parameter choices. Bearing these facts, we also explored the hyperparameters in both adapters. In Figure 28, we have presented the ablation study examining the factors and ranks for LoKr utilizing SDXL, while in Figure 29, we showcase an ablation study on the learning rate. Moreover, Figure 30 features an ablation study on the learning rate and rank for LoHA using SDXL. These analyses reveal that for LoKr, the optimal factor is -1 and the optimal rank is 8, with a learning rate of \\(1\\times 10^{-3}\\); while for LoHA, the optimal rank is 4, with a learning rate of \\(1\\times 10^{-4}\\).\n' +
      '\n' +
      'Additionally, quantitative comparisons are conducted, encompassing parameter count alongside image-to-image and image-to-text alignment scores, as detailed in Table 7 and Table 8. The results in Table 7 indicate that although LoKr marginally possesses fewer parameters still _DiffuseKronA_ with \\(a_{1}=16\\) achieves superior CLIP-I, CLIP-T, and DINO scores. This contrast is readily noticeable in the visual examples depicted in Figure 26. For the prompt _"A [V] toy with the Effel Tower in the background"_, LoKr fails to construct the _Eiffel Tower_ in the background, unlike _DiffuseKronA_ (\\(a_{1}=16\\)). Similarly, in the case of _"A [V] teapot floating on top of water"_ LoKr distorts the teapot\'s spout, whereas _DiffuseKronA_ maintains fidelity. In the case of _"A [V] toy"_ (last row), the results of _DiffuseKronA_ are much more aligned as compared to LoKr for both prompts. Conversely, for _dog_ and _cat_ examples, all the methods demonstrate similar visual appearance in terms of fidelity as well as textual alignment. Consequently, it\'s evident that while LoKr reduces parameter count, it struggles with complex input images or text prompts with multiple contexts. Hence, _DiffusekronA_ achieves efficiency in parameters while upholding average scores across CLIP-I, CLIP-T, and DINO metrics. Hence, achieving a better trade-off between parameter efficiency and personalized image generation.\n' +
      '\n' +
      'Figure 26: **Qualitative comparison** of four variants of _DiffusekronA_ with other low-rank methods including LoRA, LoKr, and LoHA. Learning rates: _DiffusekronA_ (\\(5\\times 10^{-4}\\)), LoRA (\\(1\\times 10^{-4}\\)), LoKr (\\(1\\times 10^{-3}\\)) & LoHA (\\(1\\times 10^{-4}\\)).\n' +
      '\n' +
      'Figure 27: **Qualitative comparison.** Results are shown for the default factors given by the LoKr implementation, with the varying factors being 2, 4, 8, and 16.\n' +
      '\n' +
      'Figure 28: **Ablation study on factor and rank for LoKr using SDXL, with a learning rate of \\(1\\times 10^{-3}\\). We found that the optimal factor and rank are -1 and 8, respectively. We also experimented with db=True, which indicates further low-rank decomposition of both matrices \\(A\\) and \\(B\\), whereas db=False means only matrix \\(B\\) is decomposed further. (continued.)**\n' +
      '\n' +
      'Figure 28: **Ablation study on factor and rank** for LoKr using SDXL, with a learning rate of \\(1\\times 10^{-3}\\). We found that the optimal factor and rank are -1 and 8, respectively. We also experimented with db=True, which indicates further low-rank decomposition of both matrices \\(A\\) and \\(B\\), whereas db=False means only matrix \\(B\\) is decomposed further. (**continued.**)\n' +
      '\n' +
      'Figure 28: **Ablation study on factor and rank** for LoKr using SDXL, with a learning rate of \\(1\\times 10^{-3}\\). We found that the optimal factor and rank are -1 and 8, respectively. We also experimented with db=True, which indicates further low-rank decomposition of both matrices \\(A\\) and \\(B\\), whereas db=False means only matrix \\(B\\) is decomposed further. **(end)**\n' +
      '\n' +
      'Figure 29: **Ablation study on factor and learning rate for LoKr using SDXL, with a fixed factor of -1. We found that the optimal learning rate and rank are \\(1\\times 10^{-3}\\) and 8, respectively. We also experimented with db=True, which indicates further low-rank decomposition of both matrices \\(A\\) and \\(B\\), whereas db=False means only matrix \\(B\\) is decomposed further. (continued..)**\n' +
      '\n' +
      'Figure 29: **Ablation study on factor and learning rate for LoKr using SDXL, with a fixed factor of -1. We found that the optimal learning rate and rank are \\(1\\times 10^{-3}\\) and 8, respectively. We also experimented with db=True, which indicates further low-rank decomposition of both matrices \\(A\\) and \\(B\\), whereas db=False means only matrix \\(B\\) is decomposed further. (continued..)**\n' +
      '\n' +
      'Figure 29: **Ablation study on factor and learning rate for LoKr using SDXL, with a fixed factor of -1. We found that the optimal learning rate and rank are \\(1\\times 10^{-3}\\) and 8, respectively. We also experimented with db=True, which indicates further low-rank decomposition of both matrices \\(A\\) and \\(B\\), whereas db=False means only matrix \\(B\\) is decomposed further. (end)**\n' +
      '\n' +
      'Figure 30: Ablation study on learning rate and rank for LoHA using SDXL. We found the optimal learning rate and rank to be \\(1\\times 10^{-4}\\) and 4, respectively.\n' +
      '\n' +
      '## Appendix G Comparison with state-of-the-arts\n' +
      '\n' +
      '**Qualitative Comparison.** In this section, we extend upon 4.4 of the main paper, comparing _DiffuseKronA_ with State-of-the-art text-to-image personalization models including DreamBooth, LoRA-DreamBooth, SVDiff, Textual Invention, and Custom Diffusion.\n' +
      '\n' +
      '**(1)** Textual Inversion (Gal et al., 2022) is a fine-tuning method that optimizes a placeholder embedding to reconstruct the training set of subject images. Learning a new concept requires 3,000 steps, which takes around 30 minutes on an A100 GPU (Li et al., 2023).\n' +
      '\n' +
      '**(2)** DreamBooth (Ruiz et al., 2023) refines the entire network through additional preservation loss as a form of regularization, leading to enhancements in visual quality that exhibit promising results. Updating DreamBooth for a new concept typically requires about 6 minutes on an A100 GPU (Li et al., 2023).\n' +
      '\n' +
      '**(3)** LoRA-DreamBooth (Ryu, 2023) explores low-rank adaptation for parameter-efficient fine-tuning attention-weight matrices of the text-to-image diffusion model. Fine-tuning LoRA-DreamBooth for a new concept typically takes about 5 minutes on a single 24GB NVIDIA RTX-3090 GPU.\n' +
      '\n' +
      '**(4)** SVDiff (Han et al., 2023) involves fine-tuning the singular values of the weight matrices, leading to a compact and efficient parameter space that reduces the risk of overfitting and language drifting. It took around 15 minutes on a single 24GB NVIDIA RTX-3090 GPU1.\n' +
      '\n' +
      'Footnote 1: SVDiff did not release official codebase, we used open-source code for SVDiff results in Figure 31.\n' +
      '\n' +
      '**(5)** Custom diffusion (Kumari et al., 2023) involves selective fine-tuning of weight matrices through a conditioning mechanism, enabling parameter-efficient refinement of diffusion models. This approach is further extended to encompass multi-concept fine-tuning. The fine-tuning time of Custom diffusion is around 6 minutes on 2 A100 GPUs.\n' +
      '\n' +
      '**Qualitative Comparison.**_DiffuseKronA_ consistently produces images closely aligned with the input images and consistently integrates features specified in the input text prompt. The enhanced fidelity and comprehensive comprehension of the input text prompts can be attributed to the structure-preserving capability and improved expressiveness facilitated by Kronecker product-based adaptation. The images generated by LoRA-DreamBooth are not of high quality and demand extensive experimentation for improvement, as depicted in Figure 31. As depicted in the figure, _DiffuseKronA_ not only generates well-defined images but also has a better color distribution as compared to Custom Diffusion.\n' +
      '\n' +
      '## Appendix H Practical Implications\n' +
      '\n' +
      '* Content Creation: It can be used to generate photorealistic content from text prompts.\n' +
      '* Image Editing and In-painting: The model can be used to edit images or fill in missing parts of an image.\n' +
      '* Super-Resolution: It can be used to enhance the resolution of images.\n' +
      '* Video Synthesis: The model can be used to generate videos from text prompts.\n' +
      '* 3D Assets Production: It can be used to create 3D assets from text prompts.\n' +
      '* Personalized Generation: The model can be used in personalized generation with DreamBooth fine-tuning.\n' +
      '* Resource Efficiency: The model is resource-efficient and can be trained with limited resources.\n' +
      '* Model Compression: The model allows for architectural compression, reducing the number of parameters, MACs per sampling step, and latency.\n' +
      '\n' +
      'Figure 31: **Qualitative comparison** between generated images by _DiffuseKronA_, LoRA-DreamBooth, Textual Inversion, DreamBooth, and Custom Diffusion. Notably, our methods’ results are generated considering \\(a_{2}=8\\). We maintained the original settings of all these methods and used the SD CompVis-1.4 (CompVis, 2021) variant to ensure a fair comparison.\n' +
      '\n' +
      'Figure 32: **Quantitative comparison** of _DiffuseKronA_ with SOTA on Text-Image Alignment. The scores are computed from the same set of images and prompts as depicted in Figure 31.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>