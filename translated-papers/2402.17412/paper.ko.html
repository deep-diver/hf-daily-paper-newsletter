<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# DiffuseKronA: 개인화된 확산 모델을 위한 파라미터 효율적인 미세 조정 방법\n' +
      '\n' +
      ' 샤얌 마지트\n' +
      '\n' +
      'Harshit Singh\n' +
      '\n' +
      'Nityanand Mathur\n' +
      '\n' +
      'Sayak Paul\n' +
      '\n' +
      'Chia-Mu Yu\n' +
      '\n' +
      'Pin-Yu Chen\n' +
      '\n' +
      '프로젝트 페이지: [https://diffusekrona.github.io/](https://diffusekrona.github.io/]\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'T2I(Subject-driven text-to-image) 생성 모델의 영역에서 최근 드림부스 및 BLIP 확산과 같은 개발로 인해 인상적인 결과가 도출되었지만 집중적인 미세 조정 요구와 상당한 매개변수 요구로 인해 한계에 직면했다. 드림부스 내의 LoRA(Low-Rank Adaptation) 모듈은 훈련 가능한 파라미터의 감소를 제공하지만, 하이퍼파라미터에 대한 현저한 민감도를 도입함으로써 파라미터 효율과 T2I 개인화된 이미지 합성의 품질 사이의 절충을 초래한다. 이러한 제약 조건을 해결하기 위해 LoRA-DreamBooth와 오리지널 DreamBooth에 비해 파라미터 카운트가 각각 35%와 99.947% 크게 감소할 뿐만 아니라 이미지 합성의 품질을 향상시키는 새로운 Kronecker 제품 기반 적응 모듈인 _DiffuseKronA_를 소개한다. 결정적으로 _DiffuseKronA_는 하이퍼파라미터 민감도 문제를 완화하여 광범위한 하이퍼파라미터에 걸쳐 일관된 고품질 세대를 제공하여 광범위한 미세 조정의 필요성을 감소시킨다. 또한, 더 제어 가능한 분해는 _DiffuseKronA_를 더 해석 가능하게 만들고 LoRA-Dreambooth에 필적하는 결과로 최대 50% 감소를 달성할 수 있다. 다양하고 복잡한 입력 이미지 및 텍스트 프롬프트에 대해 평가된 _DiffuseKronA_는 기존 모델보다 일관되게 우수하며, 향상된 충실도와 객체의 보다 정확한 색상 분포로 더 높은 품질의 다양한 이미지를 생성하며, 예외적인 매개변수 효율성을 유지하므로 T2I 생성 모델링 분야에서 상당한 발전을 나타낸다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근, 텍스트-투-이미지(Text-to-Image; T2I) 생성 모델들(Gu et al., 2022; Chang et al., 2023; Rombach et al., 2022; Podell et al., 2023; Yu et al., 2022)은 종종 실제-세계 사진들로부터 식별을 거부하는 복잡하고 매우 상세한 이미지들을 생성하면서 빠르게 진화해왔다. 현재의 최첨단 기술은 상당한 진전을 보이고 상당한 개선을 보여주었으며, 이는 인간의 상상력과 계산 표현 사이의 경계가 점점 더 모호해지는 미래를 암시한다. 이러한 맥락에서, 주제-구동 T2I 생성 모델(Ruiz et al., 2023a; Li et al., 2023a)은 이미지 편집, 주제-특정 속성 수정, 아트 렌디션 등과 같은 창조적 잠재력을 잠금 해제한다. DreamBooth(Ruiz et al., 2023a), BLIP-Diffusion(Li et al., 2023a)과 같은 작업은 생성 능력에 영향을 미치지 않고 원래 모델에 의해 학습된 이전을 보존하면서 미리 훈련된 모델에 새로운 주제를 원활하게 도입한다. 이러한 접근법은 몇 가지 샷 예제로 미세 조정될 때 다양한 스타일에 걸쳐 본질 및 주제별 세부 사항을 유지하는 데 탁월하며, 기초 사전 훈련 잠재 확산 모델(LDM)을 활용한다(Rombach 외, 2022).\n' +
      '\n' +
      '그러나, 안정확산을 갖는 드림부스(Rombach et al., 2022)는 부정확한 프롬프트 컨텍스트 합성, 컨텍스트 외관 얽힘 및 하이퍼파라미터 민감도와 같은 몇 가지 주요 문제를 겪는다. 또한, DreamBooth는 잠재 확산 모델의 모든 파라미터(Rombach et al., 2022) UNet 및 텍스트 인코더(Radford et al., 2021)를 미세 조정하는데, 이는 훈련 가능한 파라미터 카운트를 상당히 증가시켜 미세 조정 프로세스를 비싸게 만든다. 여기서, 드림부스 내에서 널리 사용되는 저순위 적응 모듈(Hu et al., 2021)(LoRA)은 파라미터 카운트를 상당히 트리밍하려고 시도하지만, 이는 전술한 드림부스-보고된 이슈를 확대함으로써 파라미터 효율과 만족스러운 피사체-구동 이미지 합성 사이의 완전한 트레이드오프를 만든다. 또한, 하이퍼파라미터에 대한 높은 민감도로 인해 원하는 출력을 얻기 위해 광범위한 미세 조정이 필요하다. 이는 T2I 생성 모델을 주제 중심의 개인화된 생성에 적용하기 위해 보다 강력하고 효과적인 매개변수 효율적인 미세 조정(PEFT) 방법을 설계하도록 동기를 부여한다.\n' +
      '\n' +
      '본 논문에서는 소수 샷 적응을 중심으로 미세 조정 T2I 확산 모델에 크로네커 제품 기반 적응 모듈을 활용하는 새로운 파라미터 효율 모듈인 _DiffuseKronA_를 소개한다. LoRA는 제한된 유연성 및 유사한 크기의 행렬 분해로 인해 분해된 행렬 내에서 유사한 표현을 학습하는 바닐라 인코더-디코더 타입 아키텍처를 고수한다(Tahaei et al., 2022). 대조적으로, 크로네커의 분해는 패치별 중복성을 활용하여 서로 다른 크기의 분해된 매트릭스를 허용함으로써 매개변수 수가 적고 표현의 유연성이 더 큰 원래 가중치 매트릭스의 훨씬 더 높은 순위 근사치를 제공한다. 이러한 근본적인 차이는 매개변수 감소, 향상된 안정성 및 더 큰 유연성을 포함한 몇 가지 개선에 기인한다. 또한, 제공된 프롬프트를 밀접하게 준수하는 이미지를 생성하면서 중요한 주제별 공간 특징을 효과적으로 캡처합니다. 이는 개인화된 이미지 생성 동안 더 높은 품질, 향상된 충실도 및 객체에서의 더 정확한 색상 분포를 초래하여, 최첨단 기술과 비교 가능한 결과를 달성한다.\n' +
      '\n' +
      '우리의 주요 기여는 다음과 같다:\n' +
      '\n' +
      '**Parameter Efficiency:**_DiffuseKronA_는 표 2에 자세히 설명된 바와 같이 SDXL(Podell et al., 2023)을 사용하여 LoRA-DreamBooth 및 바닐라 DreamBooth와 비교하여 훈련 가능한 파라미터를 35% 및 99.947% 유의하게 감소시킨다. Kronecker 인자를 변경함으로써, 부록의 도 26에서 입증된 바와 같은 최신 기술에 필적하는 결과로 최대 50% 감소를 달성할 수 있다.\n' +
      '\n' +
      '**Enhanced Stability:**_DiffuseKronA_는 복잡한 입력 이미지 및 다양한 프롬프트로 작업하는 경우에도 미세 조정 시 하이퍼파라미터의 고정된 스펙트럼 내에서 형성되는 훨씬 더 안정적인 이미지 생성 프로세스를 제공한다. 그림 4에서 두 방법 모두에서 하이퍼파라미터 변화와 관련된 경향을 보여주고 LoRA-DreamBooth보다 우수한 안정성을 강조한다.\n' +
      '\n' +
      '**텍스트 정렬 및 충실도:** 평균적으로 _DiffusekronA_는 더 나은 주제 의미론 및 큰 문맥 프롬프트를 캡처한다. 우리는 각각 정성적 및 정량적 비교를 위해 독자를 그림 7과 그림 8을 참조한다.\n' +
      '\n' +
      '**해석 가능성:** 특히 크로네커 제품 기반 이점을 탐색하기 위해 광범위한 분석을 수행합니다.\n' +
      '\n' +
      '도 2: 도식적 예시: LoRA는 하나의 제어 가능한 파라미터, 랭크\\(\\mathbf{r}\\)로 제한되고, 크로네커 제품은 두 개의 제어 가능한 파라미터 \\(\\mathbf{a_{1}}\\) 및 \\(\\mathbf{a_{2}}\\)(또는 등가적으로 \\(\\mathbf{b_{1}}\\) 및 \\(\\mathbf{b_{2}}\\)을 도입함으로써 향상된 해석성을 보여준다.\n' +
      '\n' +
      '개인화된 확산 모델 내의 적응 모듈. 보다 제어 가능한 분해는 그림 2에서 설명한 대로 _DiffusekronA_를 더 해석 가능하게 만든다.\n' +
      '\n' +
      '소샷 설정 하에서 42개의 데이터 세트에 대한 광범위한 실험은 앞서 언급한 _DiffusKronA_의 효과를 입증하여 매개변수 효율성과 만족스러운 이미지 합성 사이의 최상의 균형을 달성한다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '**Text-to-Image Diffusion Models.** Stable Diffusion (SD) (Rombach et al., 2022; Podell et al., 2023), Imagen (Saharia et al., 2022), DALL-E2 (Ramesh et al., 2022) & E3 (Betker et al., 2023), PixArt-\\(\\alpha\\)(Chen et al., 2023), Kandinsky (Lui et al., 2019) 및 eDiff-I (Balaji et al., 2022)와 같은 T2I 확산 모델에서의 최근 발전은 데이터 분포를 모델링하는 데 현저한 효과를 보여주었고, 이미지 합성에서 인상적인 결과를 가져왔고 도메인 전반에 걸친 다양한 창의적인 응용을 위한 문을 열었다. SD 모델의 이전 반복에 비해, 안정 확산 XL(Stable Diffusion XL)(Podell et al., 2023)은 더 큰 백본 및 개선된 훈련 절차로 인해 T2I 합성의 상당한 발전을 나타낸다. 이 작업에서 우리는 고해상도 이미지를 생성하는 인상적인 능력, 신속한 준수, 더 나은 구성 및 의미론 때문에 주로 SDXL을 통합한다.\n' +
      '\n' +
      '**Subject-driven T2I Personalization.** 특정 피사체의 몇 개의 이미지(일반적으로 3~5개)만 주어졌을 때, T2I Personalization 기법은 텍스트 입력을 기반으로 피사체의 다양한 상황별 이미지를 합성하는 것을 목표로 한다. 특히 Textual Inversion(Gal et al., 2022)과 DreamBooth(Ruiz et al., 2023a)가 첫 번째 작업 라인이었다. 텍스트 역산은 텍스트 임베딩을 미세 조정하는 반면, 드림부스는 추가 보존 손실을 정규화로 사용하여 전체 네트워크를 미세 조정하여 유망한 결과를 보여주는 시각적 품질 개선을 초래한다. 보다 최근에, BLIP-확산(Li et al., 2023a)은 멀티모달 BLIP-2(Li et al., 2023b) 모델을 레버리지하는 2단계 사전 트레이닝 프로세스를 수행함으로써 제로-샷 주체-구동 생성 능력을 가능하게 한다. 이들 연구는 단일 대상 세대에 초점을 맞추고 있으며, 이후 작업(Kumari et al., 2023; Han et al., 2023; Ma et al., 2023; Tewel et al., 2023)은 다중 대상 세대로 파고든다.\n' +
      '\n' +
      '**PEFT Methods within T2I Personalization.** 대형 사전-훈련된 모델들을 완전 스케일에서 미세-조정하는 기초 모델들(Ruiz et al., 2023a; Li et al., 2023a)과 대조적으로, 파라미터-효율 미세-조정(PEFT)에서의 몇몇 정성적 작업들(Kumari et al., 2023; Han et al., 2023; Ruiz et al., 2023; Ye et al., 2023)이 변환적 접근으로서 등장하였다. PEFT 기법의 영역 내에서 저순위 적응 방법(Hu et al., 2021; von Platen et al., 2023)은 학습 가능한 절단 특이값 분해(SVD) 모듈을 필수 계층 상의 원래 모델 가중치에 도입함으로써 파라미터 카운트를 감소시키는 사실상 방법이 되었다. 예를 들어, Custom Diffusion (Kumari et al., 2023)은 교차 주의의 \\(K\\) 및 \\(V\\) 행렬을 미세 조정하고, 다중 개념 생성을 처음으로 도입하며, 효율적인 파라미터 압축을 위해 LoRA를 채용하는 것에 초점을 맞추고 있다. SVDiff(Han et al., 2023)는 다중 피사체 영상 생성의 품질을 향상시키기 위해 Cut-Mix-Unmix 데이터 증강 기법으로 가중치 행렬의 특이값을 미세 조정함으로써 파라미터 효율을 달성한다. Hyper-Dreambooth(Ruiz et al., 2023b)는 개인화된 충실도 제어 얼굴 생성을 위해 DreamBooth를 빠르고 효율적으로 만들기 위해 하이퍼 네트워크를 제안했다. ControlNets(Zhang et al., 2023)에 개념적으로 유사한 접근법인 T2I-Adapters(Mou et al., 2023)는 보조 네트워크를 사용하여 추가 입력의 표현을 계산하고 UNet의 활성화와 혼합한다. 한편, Mix-of-Show(Gu et al., 2023)는 각 피험자에 대해 별개의 LoRA 모델을 훈련시키고 후속적으로 융합을 수행하는 것을 포함한다.\n' +
      '\n' +
      '이와 관련하여 LoRA-Dreambooth (Ryu, 2023) 기법은 표현 능력이 떨어지고 해석성이 낮아 어려움을 겪고 있으며, 이러한 제약을 해결하기 위해 _DiffusKronA_를 도입한다. 우리의 방법은 처음에 제안된 KronA 기법(Edalati et al., 2022b)에 의해 영감을 받는다. 그러나 주요 차이점이 있다: (**1**) 원래 논문은 언어 모델을 중심으로 한 반면, 우리의 연구는 특히 T2I 세대의 맥락에서 LDM으로 이 탐구를 확장한다. (**2**) 우리의 초점은 LDM들 내의 다양한 모듈들의 효율적인 미세 조정에 있다. (**3**) 더 중요한 것은 해석 가능성, 매개변수 효율성 및 주제 충실도를 고려하여 주제별 생성에 대한 크로네커 요인의 변경이 미치는 영향을 조사한다는 것이다. LoKr(Yeh et al., 2023)이 동시 작업임을 언급하는 것도 주목할 만하며, 부록 F의 주요 차이점을 논의한다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      'training된 T2I 잠재확산모델(T2I 잠재확산모델)의 크기\\(|\\mathcal{D}_{\\phi}|\\)과 가중치\\(\\phi\\)을 갖는 학습가능 파라미터\\(\\theta\\)와 크기\\(m\\ll|\\mathcal{D}_{\\phi}|\\)을 갖는 파라미터 효율적인 적응기법을 개발하고자 한다. 추론에서, 새롭게 훈련된 파라미터들은 원래의 가중치 매트릭스와 통합될 것이고, 새로운 개인화된 모델인 \\(\\mathcal{D}_{\\phi+\\theta}\\)로부터 다양한 이미지들이 합성될 수 있다.\n' +
      '\n' +
      '**방법 개요.** 도 3은 주제 주도형 세대에서의 T2I 확산 모델의 PEFT에 대해 제안된 _DiffusKronA_의 개요를 나타낸다. _ DiffusKronA_는 SDXL 백본 내에서 텍스트 인코더 가중치를 동결 상태로 유지하면서 UNet 모델의 주의 계층에서 파라미터만 업데이트한다. 여기서는 먼저 섹션 3.1의 예비 섹션에 대한 개요를 설명하고 섹션 3.2의 _DiffusKronA_에 대한 자세한 설명을 제공한다. 특히 섹션 3.2에서는 "Di_fuseKronA가 바닐라 LoRA에 비해 더 매개변수 효율적이고 해석 가능한 미세 조정 확산 모델의 방법"에 대한 통찰력과 수학적 설명을 제공한다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      'T2I Diffusion Models.LDMs(Rombach et al., 2022)는 확률적 생성확산 모델의 두드러진 변형인 \\(\\mathcal{D}_{\\phi}\\)으로 표기된 이미지 \\(\\mathbf{x}_{gen}=\\mathcal{D}_{\\phi}\\left(\\mathbf{\\epsilon},\\mathbf{c}\\right)\\)를 텍스트 인코더를 사용하여 텍스트 프롬프트 \\(\\mathbf{P}\\(\\mathbf{0},\\mathbf{I})에서 파생된 잡음 맵 \\(\\mathbf{x}_{gen}=\\mathcal{D}_{\\phi}\\left(\\mathbf{P}\\right)\\)과 조건 임베딩 \\(\\mathbf{c}=\\mathcal{T}\\left(\\mathbf{P}\\right)\\)을 통합함으로써 이미지를 생성하는 것을 목표로 한다. LDM은 입력 영상 \\(\\mathbf{x}\\in\\mathbb{R}^{H\\times W\\times 3}\\)을 인코더 \\(\\mathcal{E}\\)을 통해 잠재 표현 \\(\\mathbf{z}\\in\\mathbbb{R}^{h\\times w\\times v}\\)으로 변환하며, 여기서 \\(\\mathbf{z}=\\mathcal{E}\\left(\\mathbf{x}\\right) 및 \\(v\\)은 잠재 특징 차원이다. 이러한 맥락에서, 잡음 확산 과정은 잠재 공간인 \\(\\mathcal{Z}\\)에서 발생하는데, 이는 조건부 UNet(Ronneberger et al., 2015)을 이용하여 잡음 잠재\\(\\mathcal{D}_{\\phi}\\)과 잡음 잠재\\(\\mathbf{z}_{t}\\)과 생성 조건 \\(c\\\\)이 주어졌을 때, 현재 timestep \\(t\\)에서 잡음(\\mathbf{\\epsilon}\\)을 예측한다. 간단히 말해서, LDM(\\mathcal{D}_{\\phi}\\)의 잡음 제거 훈련 목표는 다음과 같이 단순화될 수 있다.\n' +
      '\n' +
      '\\mathbbb{E}_{\\mathcal{E}\\left(\\mathbf{x}\\right),\\mathbf{c},\\mathbf{\\epsilon}\\simmathcal{N}(\\mathbf{0},\\mathbf{I}),t\\sim\\mathcal{U}(0,1}\\left[w_{t}\\left\\mathcal{D}_{\\phi}\\left(\\mathbf{z}_{t}|\\mathbf{c},t\\right)-\\mathbf\\epsilon\\right\\|_{2}^{2}\\right], \\tag{1}\\t}\\mathcal{U}(0,1}\\left[w_{t}\\left\\mathcal{D}_{\\phi}\\left(\\mathbf{z}_{t}|\\mathbf{c},t\\right)-\\mathbf\\epsilon\\right\\|_{2}\\right],\\tag{1}\\t}\\mathcal{U}(\n' +
      '\n' +
      '여기서 \\(\\mathcal{U}\\)는 균일한 분포를 나타내고 \\(w_{t}\\)는 손실에 대한 시간 의존적 가중치이다.\n' +
      '\n' +
      'Low Rank Adaptation (LoRA). Pre-trained large model은 태스크 적응을 위한 낮은 "내재적 차원"을 나타낸다(Hu et al., 2021; Han et al., 2023). 이를 바탕으로 LoRA(Hu et al., 2021)는 가중치 업데이트도 적응 동안 낮은 "내재 순위"를 보유하며 작업 적응을 위한 모델의 필수 계층에 훈련 가능한 순위 분해 매트릭스를 주입하여 훈련 가능한 매개변수의 수를 상당히 감소시킨다고 가정한다.\n' +
      '\n' +
      '사전 훈련된 가중치 행렬 \\(W_{0}\\in\\mathbb{R}^{d\\times k}\\)의 맥락에서, \\(W_{0}\\)의 갱신은 행렬을 낮은 순위 분해 \\(W_{0}+\\Delta W:=W_{0}+AB\\)으로 표현함으로써 부과되는 제약조건들을 받는다. 여기서 \\(A\\in\\mathbb{R}^{d\\times r}\\), \\(B\\in\\mathbb{R}^{r\\times h}\\), 및 \\(r\\ll\\min(d,h)\\). 그 결과, \\(A\\)와 \\(B\\)의 크기가 \\(W_{0}\\)보다 현저히 작아 훈련 가능한 매개변수의 수를 줄일 수 있었다. 훈련 과정 내내 \\(W_{0}\\)은 고정된 상태로 유지되며 구배 업데이트에 불침투하며 훈련 가능한 매개변수는 \\(A\\) 및 \\(B\\) 내에 포함된다. \\(h=W_{0}x\\)에 대해, 수정된 순방향 패스는 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\[f(x)=W_{0}x+\\Delta Wx+b_{0}:=W_{\\text{LoRA}}\\,x+b_{0}, \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(b_{0}\\)는 미리 학습된 모델의 바이어스 항이다.\n' +
      '\n' +
      'LoRA-DreamBooth.LoRA(Hu et al., 2021)는 훈련 가능한 파라미터의 수를 줄이는 일차적인 목적으로 DreamBooth를 미세 조정하기 위해 전략적으로 사용된다. LoRA는 UNet과 텍스트 인코더 내에서 주의 모듈의 \\(W_{Q}\\), \\(W_{K}\\), \\(W_{V}\\), \\(W_{O}\\) 가중치 행렬에 저순위 분해 행렬로 훈련 가능한 모듈을 주입한다. 트레이닝 동안, 미리 트레이닝된 UNet 및 텍스트 인코더의 가중치는 동결되고 LoRA 모듈들만이 튜닝된다. 그러나, 추론 동안, 미세 조정된 LoRA 모듈들의 가중치들은 대응하는 미리 트레이닝된 가중치들에 병합된다. 더욱이, 이 작업은 추론 시간을 증가시키지 않는다.\n' +
      '\n' +
      '### DiffuseKronA\n' +
      '\n' +
      'LoRA는 확산 모델의 영역에서 효율성을 입증하지만 제한된 표현력으로 인해 방해를 받는다. 대조적으로, 크로네커 제품은 두 행렬의 요소 간의 쌍별 상호 작용을 명시적으로 포착함으로써 더 미묘한 표현을 제공한다. 복잡한 관계를 포착하는 이러한 능력은 모델이 데이터에서 복잡한 패턴을 더 자세히 학습하고 나타낼 수 있게 한다.\n' +
      '\n' +
      '크로네커 곱(\\(\\otimes\\))은 서로 다른 모양의 행렬들 간의 곱셈을 가능하게 하는 행렬 곱셈 방법이다. 두 행렬 \\(A\\in\\mathbb{R}^{a_{1}\\times a_{2}}\\)과 \\(B\\in\\mathbb{R}^{b_{1}\\times b_{2}}\\)에 대해, Kronecker 곱 \\(A\\otimes B\\in\\mathbb{R}^{a_{1}b_{1}}\\times a_{2}}\\)의 각 블록은 엔트리 \\(A_{i,j}\\)에 \\(B\\)을 곱하여 정의된다.\n' +
      '\n' +
      '도 3: _DiffuseKronA_: (a) Fine-tuning 프로세스의 개요는 Kronecker Adapter를 사용하여 다중 헤드 어텐션 파라미터(a.1)를 최적화하는 것을 포함하고, 후속 블록에서 정교화된, a.2, (b) 추론 동안, \\(\\theta\\)로 표시된 새로 트레이닝된 파라미터들은 원래의 가중치 \\(\\mathcal{D}_{\\phi}\\)와 통합되고, 이미지들은 업데이트된 개인화된 모델 \\(\\mathcal{D}_{\\phi+\\theta}\\)을 사용하여 합성된다.\n' +
      '\n' +
      '[A\\otimes B=\\left[\\begin{array}{cccc}a_{1,1}B&\\cdots&a_{1,a_{2}}B\\\\\\vdots&\\ddots&\\vdots\\\\a_{a_{1},1}B&\\cdots&a_{a_{1},a_{2}}B\\end{array}\\right]. \\tag{3}\\dots&a\\vdots&\\vdots\\a_{1},1}B&\\cdots&a_{a_{1},a_{2}}B\\end{array}\\right].\n' +
      '\n' +
      '크로네커 곱은 모델 파라미터들의 상이한 세트들 사이의 관계들을 나타내는 매트릭스들을 생성하는 데 사용될 수 있다. 이러한 행렬들은 한 세트의 파라미터들의 변화들이 다른 세트에 어떻게 영향을 주거나 상호 작용하는지를 인코딩한다. 그림 9에서 우리는 크로네커 제품이 작동하는 방법을 보여준다. 흥미롭게도 LoRA 및 어댑터와 같은 기술의 경우와 같이 낮은 순위 하향 투영처럼 순위 결핍을 겪지 않는다. 크로네커 제품은 여러 가지 유리한 특성을 가지고 있어 복잡한 데이터를 처리할 수 있는 좋은 옵션이다(Greenewald & Hero, 2015).\n' +
      '\n' +
      '**Kronecker Adapter(KronA).** 언어 모델의 PEFT를 연구하는데 처음 소개된 바 있다(Edalati et al., 2022), The Kronecker Product는 행렬에 인코딩된 구조화된 관계를 이용한다. 곱 \\(A\\otimes B\\)을 계산하는 데 필요한 모든 곱셈을 명시적으로 수행하는 대신 다음과 같은 등가 행렬-벡터 곱셈을 적용하여 전체 계산 비용을 줄일 수 있다. 이것은 큰 매트릭스들로 작업할 때 또는 계산 자원들이 제약될 때 특히 유익하다:\n' +
      '\n' +
      '(A\\otimes B)x=\\gamma\\left(B\\eta_{b_{2}\\times a_{2}}(x)A^{\\top}\\right}\\tag{4}\\)\n' +
      '\n' +
      '여기서 \\(A^{\\top}\\)는 \\(A\\)으로 전치된다. 그 이론적 근거는 수학적 연산 \\(\\eta_{m\\times n}(\\mathbf{y})\\)을 이용하여 벡터 \\(y\\in\\mathbb{R}^{m\\cdot n}\\)을 크기 \\(m\\times n\\)의 행렬 \\(Y\\)으로 재구성할 수 있다는 것이다. 마찬가지로 \\(Y\\in\\mathbb{R}^{m\\times n}\\)도 \\(\\gamma(Y)\\) 연산을 이용하여 열을 쌓아 벡터로 다시 변환할 수 있다. 이 방법은 \\(\\mathcal{O}(b\\log b)\\) 계산 복잡도와 \\(\\mathcal{O}(\\log b)\\) 공간 복잡도를 \\(b\\) 차원 벡터에 대해 달성한다. 표준 비구조화된 Kronecker 곱셈에 비해 급격한 개선이다 (Zhang et al., 2015).\n' +
      '\n' +
      'KronA.**를 갖는 미조정 확산 모델 본질에서, KronA는 아래 식에 명시된 바와 같이 파라미터-효율적 적응을 위해 신경망 내의 가중치 매트릭스의 임의의 서브세트에 적용될 수 있으며, 여기서 \\(U\\)은 확산 모델에서의 상이한 모듈들을 나타내며, Key(\\(K\\)), Query(\\(Q\\)), Value(\\(V\\)), 및 Linear(\\(O\\)) 층들을 포함한다. 미세 조정 동안 KronA 모듈은 미리 훈련된 가중치 매트릭스에 병렬로 적용된다. 크로네커 인자는 조정된 후 원래 가중치 매트릭스에 곱해지고, 스케일링되고, 병합된다. 따라서 LoRA와 마찬가지로 KronA는 동일한 추론 시간을 유지한다.\n' +
      '\n' +
      '\\[\\begin{split}\\Delta W^{U}=&\\A^{U}\\otimes B^{U},U\\in\\{K,Q,V,O\\;\\\\&\\W_{\\text{fine-tuned}=W_{\\text{pre-trained}}+\\Delta W.\\end{split}\\tag{5}\\\\\n' +
      '\n' +
      '이전 연구들(Kumari et al., 2023; von Platen et al., 2023; Tewel et al., 2023)은 미세 조정 프로세스에서 가장 영향력 있는 모듈들을 식별하기 위해 광범위한 실험들을 수행하였다. (Kumari et al., 2023; Li et al., 2020)에서, 저자들은 \\(\\delta_{l}=\\left\\|\\theta_{l}^{\\prime}-\\theta_{l}\\right\\|/\\left\\|\\theta_{l}\\right\\\\\\)으로 표시된 다른 데이터 세트에서 미세 조정 동안 각 모듈의 변화 속도를 탐구했으며, 여기서 \\(\\theta_{l}^{\\prime}\\) 및 \\(\\theta_{l}\\)은 레이어 \\(l\\)의 업데이트되고 사전 훈련된 모델 매개변수를 나타낸다. 이들의 결과는 교차 주의 모듈이 상대적으로 더 높은 \\(\\delta\\)을 나타내어 미세 조정 과정에서 중추적인 역할을 한다는 것을 나타낸다. 이러한 연구에 비추어, 우리는 주의층에 대한 미세 조정을 수행하고 높은 효과를 관찰했다. 이 주제에 대한 자세한 내용은 부록 D에서 확인할 수 있습니다.\n' +
      '\n' +
      'LoRA v.s. _DiffuseKronA.* * 더 높은 순위 행렬은 더 많은 수의 특이 벡터로 분해 가능하므로 더 나은 표현성을 포착하고 PEFT에 대한 더 풍부한 용량을 허용한다. LoRA에서 생성된 갱신 행렬 \\(\\Delta W_{\\text{lora}}\\)의 랭크는 행렬 \\(A\\)과 \\(B\\) 사이의 최소 랭크, _i.e._\\(rank(\\Delta W_{\\text{lora}})=\\min(rank(A),rank(B))에 의해 제한된다. 반대로, _DiffuseKronA_에서 행렬순위 \\(\\Delta W_{\\text{KronA}}=A\\otimes B\\)는 행렬순위 \\(A\\)와 \\(B\\), _i.e._\\(rank(\\Delta W_{\\text{KronA}})=rank(A)\\cdot 순위(B)\\)의 곱으로, 이는 LoRA보다 낮은 순위의 분해된 행렬을 유지하면서 LoRA보다 높은 순위의 행렬을 생성하도록 적절하게 구성될 수 있다. 따라서 개인화된 T2I 확산 모델의 경우, _DiffuseKronA_는 표 2 및 표 3에 비해 더 적은 매개변수에서 더 많은 주제별 정보를 전달할 것으로 예상되며, 더 자세한 내용은 부록 E에 제공된다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '본 절에서는 SDXL(von Platen et al., 2023)과 SD(CompVis, 2021) 모델을 백본으로 사용하여 그 효과를 확인하기 위해 포괄적인 절제 연구를 통해 _DiffuseKronA_를 사용하여 개인화의 다양한 구성요소를 평가한다. 또한, 섹션 4.3에서 6가지 측면에서 _DiffuseKronA_와 LoRA-DreamBooth를 통찰력 있게 비교하고 섹션 4.4에서 _DiffuseKronA_를 다른 관련 선행 연구와 비교하여 우리의 우월성을 강조한다.\n' +
      '\n' +
      '### 데이터 세트 및 평가\n' +
      '\n' +
      '**Datasets.** 우리는 4가지 유형의 주제별 데이터세트에 대해 광범위한 실험을 수행했다: (i) 12개의 데이터세트(9개는 (Ruiz et al., 2023), 3개는 (Kumari et al., 2023))의 살아있는 주제/반려동물, 인형, 고양이, (ii) 선글라스, 배낭 등을 포함한 21개의 고유한 객체의 데이터세트. (iii) Super-Saiyan, Akimi, Kiriko, Shoko Komi, Hatake Kakashi 등 만화 캐릭터에 대한 5개의 데이터셋을 수집하였고, (iv) 얼굴 이미지에 대한 4개의 데이터셋을 수집하였다. 자세한 내용은 Ap에 나와 있습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c} \\hline \\hline\n' +
      '** 분해된 Matrix** & **Notation** & **Module** & **Factorization**\n' +
      '**Factor Name** & & **Parameters** & **Constraint** \\\\ \\hline Kronecker down factor & \\(A\\in\\mathbb{R}^{n\\times a_{1}}\\) & \\(a_{1}a_{2}+b_{1}b_{2}\\) & \\(a_{1}b_{1}=d\\) \\\\ Kronecker up factor & \\(B\\in\\mathbb{R}^{n\\times b_{1}}\\) & & \\(a_{2}b_{2}=h\\) \\\\ \\hline LoRA down projection & \\(A\\in\\mathbb{R}^{d\\times r}\\) & & \\(r(d+h)\\) & \\(r\\ll\\min(d,h)\\) \\\\ LoRA up projection & \\(B\\in\\mathbb{R}^{r\\times h}\\) & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 크로네커 인자와 LoRA 투영을 비교하는 것.\n' +
      '\n' +
      'pendix B.\n' +
      '\n' +
      '구현 상세.** 우리는 \\(\\sim\\) 1000번의 반복을 관찰하며, \\(5\\times 10^{-4}\\)의 학습률을 사용하고, 평균 3개의 학습 이미지를 활용하면 바람직한 결과를 생성하기에 충분하다는 것을 알 수 있다. 훈련 과정은 24GB NVIDIA RTX-3090 GPU에서 SD(CompVis, 2021)의 경우 \\(\\sim\\) 5분, SDXL(von Platen et al., 2023)의 경우 \\(\\sim\\) 40분이 소요된다.\n' +
      '\n' +
      '**평가 메트릭.** 우리는 (1) _Image-alignment_에 대한 _DiffuseKronA_를 평가한다: 참조 개념 이미지들과 생성된 이미지들의 CLIP(Radford et al., 2021) 시각적 유사성(CLIP-I) 및 DINO(Caron et al., 2021) 유사성 스코어들을 계산하고, (2) _Text-alignment_: 생성된 이미지들과 제공된 텍스트 프롬프트들 사이의 CLIP 텍스트-이미지 유사성(CLIP-T)을 정량화한다. 자세한 수학적 설명은 부록 C에 나와 있다.\n' +
      '\n' +
      '###_DiffuseKronA_의 최적 구성 해제\n' +
      '\n' +
      '실험을 통해 다음과 같은 경향을 관찰하고 _DiffuseKronA_를 사용하여 더 나은 이미지 합성을 위한 하이퍼파라미터의 최적 구성을 발견했다.\n' +
      '\n' +
      '**크로네커 분해를 수행하는 방법?** LoRA와 달리 _DiffuseKronA_는 표 1에 예시된 바와 같이 두 개의 제어 가능한 크로네커 인자를 특징으로 하여 분해에 더 큰 유연성을 제공한다. 우리의 연구 결과는 아래쪽의 Kronecker 행렬 \\(\\mathbf{A}\\)의 치수가 위쪽의 Kronecker 행렬 \\(\\mathbf{B}\\)의 치수보다 작아야 함을 보여준다. 구체적으로, \\(a_{2}\\)의 최적값은 정확히 64인 반면 \\(a_{1}\\)은 설정된 \\(\\{2,4,8}\\)에 속함을 확인하였다. 놀랍게도, 모든 쌍의 \\((a_{1},a_{2})\\) 값 중에서 \\((4,64)\\)이 가장 높은 충실도를 갖는 이미지를 산출한다. 또한 그림 4와 그림 15에 묘사된 바와 같이 \\(a_{2}=64\\)의 학습률에 따라 영상이 최소의 변화를 보이는 것을 관찰할 수 있었다. 크로네커 인자에 대한 상세한 절제, 초기화 및 미세 조정에 대한 영향은 부록 D.2에 제시되어 있다.\n' +
      '\n' +
      '**학습률의 효과**_DiffuseKronA_는 광범위한 학습률에 걸쳐 일관된 결과를 생성한다. 여기서, 최적의 학습률 값 \\(5\\times 10^{-4}\\)에 더 가까운 학습률을 위해 생성된 이미지가 유사한 이미지를 생성하는 것을 관찰하였다. 그러나, 학습률이 \\(1\\times 10^{-3}\\)을 초과하는 것은 모델 과적합에 기여하여, 높은 충실도의 이미지를 생성하지만 입력 텍스트 프롬프트에 대한 강조는 감소한다. 반대로, \\(1\\times 10^{-4}\\) 이하의 학습률은 생성된 이미지에서 더 낮은 충실도로 이어져 입력 텍스트 프롬프트를 더 크게 우선시한다. 이 패턴은 그림 4에서 분명하며, 여기서 우리의 접근법은 입력 이미지와 입력 텍스트 프롬프트를 모두 충실하게 캡처하는 예외적인 이미지를 생성한다. 추가 결과는 동일한 것을 정당화하기 위해 부록 D.3에 제공된다.\n' +
      '\n' +
      '또한, 부록 D.1에서 모델을 미세 조정할 수 있는 모듈의 선택(a)을 조사하여 부록 D.5 및 부록 D.4에서 학습 이미지가 없는 효과, 부록 D.5.1에서 원샷 모델 성능, 부록 D.6에서 추론 단계 수 및 지침 점수와 같은 추론 하이퍼파라미터의 효과를 조사했다.\n' +
      '\n' +
      '### 모델 성능 탐색: LoRA-Dreambooth vs _DiffuseKronA_\n' +
      '\n' +
      '우리는 SDXL을 사용하고 우리의 _DiffuseKronA_를 사용하여 생성\n' +
      '\n' +
      '그림 4: SDXL에서 다양한 학습 속도에 걸친 _DiffuseKronA_와 LoRA-DreamBooth의 비교. 본 논문에서 제안한 방법은 \\(a_{2}\\)의 값을 64로 설정하였다. _DiffuseKronA_는 \\(1\\times 10^{-4}\\)부터 \\(1\\times 10^{-3}\\)까지 다양한 학습률에서 좋은 결과를 얻을 수 있다. 대조적으로, LoRA에서는 식별 가능한 패턴이 관찰되지 않는다. 그림의 오른쪽 부분은 LoRA-DreamBooth와 _DiffuseKronA_에 대한 Text & Image Alignment의 플롯을 보여주는데, 여기서 _DiffuseKronA_에 속하는 점들은 조밀해 보이고 LoRA-DreamBooth의 점들은 희소해 보여 학습률을 변화시키면서 _DiffuseKronA_가 LoRA-DreamBooth보다 _stable_인 경향이 있음을 나타낸다.\n' +
      '\n' +
      '다양한 피사체와 텍스트로부터의 이미지는 LoRA-DreamBooth에 비해 높은 충실도, 객체의 더 정확한 색상 분포, 텍스트 정렬 및 안정성을 갖는 이미지를 생성하는데 그 효과를 보여준다.\n' +
      '\n' +
      '**Fidelity & Color Distribution.** 우리의 접근법은 도 5에 예시된 바와 같이 LoRA-DreamBooth에 비해 우수한 충실도의 이미지를 일관되게 생성한다. 특히, _DiffuseKronA_에 의해 생성된 _clock_는 원본 이미지를 미러링하는 _numeral_**3**의 정확한 묘사와 같은 복잡한 세부사항을 충실하게 재현한다. 대조적으로, LoRA-DreamBooth로부터의 출력은 그러한 높은 충실도를 달성하는 데 어려움을 나타낸다. 추가적으로, _DiffuseKronA_는 생성된 이미지들에서 개선된 색상 분포를 보여주는데, 이는 도 5의 _RC Car_ 이미지들에서 분명히 분명한 특징이다. 더욱이, 앉은 장난감의 가슴에 있는 숫자 _numeral_**1**에 대한 충실도를 유지하는데 어려움을 겪는다. 추가적인 예는 부록의 그림 23에 나와 있다.\n' +
      '\n' +
      '**Text Alignment.**_DiffuseKronA_는 입력으로 제공되는 텍스트 프롬프트의 복잡성과 복잡성을 이해하여, 주어진 텍스트 프롬프트와 일치하는 이미지를 생성하는데, 이는 도 6에 도시된 바와 같다. 프롬프트에 대한 응답으로 생성된 _anime 문자_의 이미지는 세심한 주의력 _DiffuseKronA_가 세부 사항에 지불함을 예시한다. 배경_ 및 _동반 수프 그릇_에서 상점의 _프레즌스를 우아하게 포착합니다. 대조적으로, LoRA-DreamBooth는 복잡한 입력 프롬프트와 매끄럽게 정렬되는 이미지를 생성하는데 어려움을 겪는다. _ DiffuseKronA_는 텍스트와 일치하는 이미지를 생성할 뿐만 아니라 주어진 입력에 대해 다양한 범위의 이미지를 생성하는 데 능숙하다. 보다 지지적인 예는 부록의 그림 24에 나와 있다.\n' +
      '\n' +
      '**우수한 안정성.**_DiffuseKronA_는 광범위한 학습 속도에 걸쳐 입력 이미지와 밀접하게 정렬되는 이미지를 생성하며, 이는 우리의 접근법에 특별히 최적화된다. 대조적으로, LoRA-DreamBooth는 그림 4에서 명백한 최적 범위 1 내에서도 입력 이미지의 중요성을 무시한다. _DiffuseKronA_에 의해 생성된 _dog_ 이미지는 최적 범위 전체에 걸쳐 입력 이미지와 높은 유사도를 유지하는 반면, LoRA-DreamBooth는 비교 가능한 수준에서 수행하기 위해 고군분투한다. 추가적인 예는 부록의 그림 16에 나와 있다.\n' +
      '\n' +
      '각주 1: 광범위한 실험을 통해 최적의 학습률을 결정한다. 추가로, 우리는 LoRA-DreamBooth를 미세 조정하는 동안 (von Platen et al., 2023; Ruiz et al., 2023)으로부터의 관찰을 고려하였다.\n' +
      '\n' +
      '**Complex Input 이미지와 Prompts.**_DiffuseKronA_는 지속적으로 좋은 성능을 보여 복잡한 입력으로 제시된 경우에도 강인한 성능을 보여준다. 이러한 성공은 크로네커 어댑터의 향상된 대표력에 기인한다. 도 1에 도시된 바와 같이, _DiffuseKronA_는 _human face_ 및 _anime characters_의 특징을 능숙하게 캡처하여 고품질 이미지를 산출한다. 추가적으로, 도 1의 마지막 행으로부터, _DiffuseKronA_가 텍스트의 의미론적 뉘앙스를 우아하게 포착한다는 것이 명백하다. 예를 들어, 우산 아래에 앉아 있는 _"without blazer"_ 및 _"upset, _DiffuseKronA_의 문맥을 고려하면, _DiffuseKronA_는 입력 텍스트 프롬프트가 거대한 경우에도, _DiffuseKronA_가 텍스트에서 명사로 언급된 다양한 개념을 능숙하게 캡처한다는 것을 입증하는 예외적인 이미지를 생성한다. 일관성 있고 의미 있는 전체적인 관계를 유지하면서 지정된 모든 개념을 포괄하는 이미지를 생성한다. 더 나아가 부록의 <그림 10>과 <그림 11>을 독자들에게 참고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c|c|c} \\hline \\hline \\multicolumn{2}{c|}{Model} & Train. Time (\\(\\downarrow\\)) & \\# Param (\\(\\downarrow\\)) & Model size (\\(\\downarrow\\)) \\\\ \\hline \\multirow{3}{*}{**LoRA-DreamBooth**} & _LoRA-DreamBooth_ & \\(\\sim\\)**38 min.** & 5.8 M & 22.32 MB \\\\  & _DiffuseKronA_ & \\(\\sim\\)**40 min.** & **38.8** M & **14.95 MB** \\\\ \\hline \\multirow{3}{*}{**LoRA-DreamBooth**} & \\multirow{3}{*}{\\(\\sim\\)**5.3 min.**} & **1.09 M** & 4.3 MB \\\\  & & & **0.52 M** & **1.2 MB** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 모델 효율성 메트릭(_DiffuseKronA_ variant used (\\(a_{1}=4\\) 및 \\(a_{2}=64\\))을 탐색한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline \\hline Model & CLIP-1 (\\(\\uparrow\\)) & CLIP-T (\\(\\uparrow\\)) & DINO (\\(\\uparrow\\)) \\\\ \\hline \\multirow{2}{*}{**LoRA-DreamBooth**} & 0.785 & 0.301 & 0.661 \\\\  & \\(\\pm\\) 0.062 & \\(\\pm\\) 0.027 & \\(\\pm\\) 0.127 \\\\ \\hline \\multirow{2}{*}{**DiffuseKronA**} & **0.809** & **0.322** & **0.677** \\\\  & \\(\\pm\\) **0.052** & \\(\\pm\\) 0.021 & \\(\\pm\\)**0.100** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: _DiffuseKronA_와 LoRA-Dreambooth 사이의 CLIP-1, CLIP-T 및 DINO 점수의 **정량적 비교**. 구해진 값은 42개의 데이터 세트에 걸쳐 평균이며, _DiffuseKronA_의 경우 \\(5\\times 10^{-4}\\), LoRA-DreamBooth의 경우 \\(1\\times 10^{-4}\\)의 학습률을 보인다.\n' +
      '\n' +
      '그림 5: _DiffuseKronA_ 우수한 충실도를 보존한다.\n' +
      '\n' +
      '도 6: 향상된 텍스트 정렬을 예시하는 _DiffuseKronA_.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      '적응. LoRA-DreamBooth에 의해 생성된 이미지는 종종 원하는 결과를 얻기 위해 광범위한 미세 조정이 필요하다. 맞춤 확산과 같은 방법은 모델을 미세 조정하기 위해 더 많은 매개변수를 필요로 한다. SVDiff와 비교하여 제안된 접근법은 (a) 그림 8과 같이 우수한 이미지-텍스트 정렬을 달성하고 (b) 매개변수 효율성을 유지하는 데 탁월하다. 각 방법에 대해 그림 8의 텍스트 및 이미지 정렬 점수를 보여주며 _DiffuseKronA_는 정성적 및 정량적으로 최상의 정렬을 얻는다. 다양한 데이터 세트와 프롬프트에 대한 추가 결과는 그림 31과 그림 32에 나와 있다. 또한, 표 4의 10개의 프롬프트로 평가된 12개의 데이터 세트에 대한 모든 기본 모델의 평균 점수를 제시한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 개선된 파라미터 효율로 고품질 이미지 생성을 목표로 텍스트-이미지 개인화 확산 모델을 향상시키기 위한 새로운 파라미터 효율 적응 모듈인 _DiffuseKronA_를 제안하였다. 크로네커 제품의 가중치 매트릭스에서 구조화된 관계를 캡처하는 능력을 활용하여 _DiffuseKronA_는 입력 텍스트 프롬프트 및 트레이닝 이미지와 밀접하게 정렬된 이미지를 생성하며, 시각적 품질, 텍스트 정렬, 충실도, 매개변수 효율성 및 안정성에서 LoRA-DreamBooth를 능가한다. _DiffuseKronA_는 텍스트 프롬프트 및 트레이닝 이미지와 밀접하게 정렬된 이미지를 생성한다. 따라서 DiffuseKronA_는 텍스트에서 이미지로의 개인화된 이미지 생성 작업을 발전시키기 위한 새롭고 효율적인 도구를 제공한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]J. 발라지 아니, X 황아바닷 J.송 크레이스 아티탈라 아일라 Laine, B. Catanzaro, et al.(2022) FedMI: text-to-image diffusion models with ensemble of expert denoisers. ArXiv:2211.01324. 인용: SS1.\n' +
      '*[2]J. 베커고 징태 브룩스 J. Wang, L. 이림 오양, 장정, 이영 Guo, et al.(2023) 더 나은 캡션을 갖는 이미지 생성 개선. 컴퓨터 과학 https://cdn. openai. com/papers/dall-e-3. pdf2, pp. 3. Cited by: SS1.\n' +
      '*[3]M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin (2021) Emerging properties in self-supervised vision transformer. 외부 링크: 2102.02748 인용: SS1.\n' +
      '*[4]H. 장홍장, J. Barber, A. Maschinot, J. Lezama, L. 장민 양광 머피, W. T. 프리먼, M. Rubinstein, et al. (2023) Muse: 마스킹된 생성 트랜스포머를 통한 텍스트-이미지 생성. ArXiv:2301.00704. 인용: SS1.\n' +
      '*[5]J. 천진유 야오은시 우진 왕재욱, P. Luo, H. Lu, Z. Li(2023) Pixart-\\(\\alpha\\): 실시간 텍스트-이미지 합성을 위한 확산 트랜스포머의 빠른 훈련. 외부 링크: 2301.00704 인용: SS1.\n' +
      '*[6]J. 데블린 장경 이경호 Toutanova (2018) Bert: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. ArXiv:1810.04805. 인용: SS1.\n' +
      '*[7]A. 에달라티, M. S. 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Computer-efficient Generative Adversarial Networks for biomedical image generation. ArXiv:2106.04647. 인용: SS1.\n' +
      '*[8]A. 에달라티 사해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021)의 효율적인 생성적 적대적 네트워크는 생물의학적 이미지 생성을 위한 것이다. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[9]A. 에달라티 사해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021)의 효율적인 생성적 적대적 네트워크는 생물의학적 이미지 생성을 위한 것이다. ArXiv:2106.04647. 인용: SS1.\n' +
      '*[10]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[11]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[12]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[13]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[14]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[15]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[16]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[17]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[18]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[19]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[20]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[21]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[22]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[23]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[24]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[25]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[26]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[27]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[28]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[29]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[30]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '*[31]A. 에달라티 타해이, A. 라시드, V. P. 니아, J. 클라크, M. Rezagholizadeh, and A. Torran (2021) Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219-226. Cited by: SS1.\n' +
      '\n' +
      'Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022. URL[https://arxiv.org/abs/2208.01618](https://arxiv.org/abs/2208.01618).\n' +
      '* Greenewald & Hero (2015) Greenewald, K. and Hero, A. O. Robust kronecker product pca for spatio-time covariance estimation. _ IEEE Transactions on Signal Processing_, 63(23):6368-6378, 2015.\n' +
      '* Gu et al. (2022) Gu, S., Chen, D., Bao, J., Wen, F., Zhang, B., Chen, D., Yuan, L., and Guo, B. Vector Quantized diffusion model for text-to-image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 10696-10706, 2022.\n' +
      '* Gu et al. (2023) Gu, Y., Wang, X., Wu, J. Z., Shi, Y., Chen, Y., Fan, Z., Xiao, W., Zhao, R., Chang, S., Wu, W., et al. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. _ arXiv preprint arXiv:2305.18292_, 2023.\n' +
      '* Hameed et al. (2023) Hameed, M. G. A., Tahaei, M. S., Mosleh, A., Nia, V. P., Chen, H., Deng, L., Yan, T., and Li, G. Convolutional neural network compression through generalized kronecker product decomposition. _ IEEE Transactions on Neural Networks and Learning Systems_, 34(5):2205-2219, 2023.\n' +
      '* Han et al. (2023) Han, L., Li, Y., Zhang, H., Milanfar, P., Metaxas, D., and Yang, F. Svdiff: Compact parameter space for diffusion fine-tuning. _ arXiv preprint arXiv:2303.11305_, 2023.\n' +
      '* He et al. (2015) He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, 2015.\n' +
      '* He et al. (2022) He, X., Li, C., Zhang, P., Yang, J., and Wang, X. E. Parameter-efficient model adaptation for vision transformer. _ arXiv preprint arXiv:2203.16329_, 2022.\n' +
      '* Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. nlp를 위한 파라미터 효율적인 전이 학습. 인트로. Conf. 마흐 배워._ , pp. 2790-2799. PMLR, 2019.\n' +
      '* Hu et al. (2021) Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. 2021년 _ICLR_에서.\n' +
      '* Kumari et al. (2023) Kumari, N., Zhang, B., Zhang, R., Shechtman, E., and Zhu, J.-Y. 텍스트-이미지 확산의 다중 개념 맞춤화. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 1931-1941, 2023.\n' +
      '* Li 등 (2018) Li, C., Farkhoor, H., Liu, R., and Yosinski, J. Measuring the intrinsic dimension of objective landscape. _International Conference on Learning Representations_, 2018.\n' +
      '* Li et al. (2023a) Li, D., Li, J., and Hoi, S. C. Blip-diffusion: controllable text-to-image generation and editing. _ arXiv preprint arXiv:2305.14720_, 2023a.\n' +
      '* Li 등(2023b) Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: 냉동 이미지 인코더 및 대형 언어 모델을 사용한 부트스트래핑 언어-이미지 사전 트레이닝_ arXiv preprint arXiv:2301.12597_, 2023b.\n' +
      '* Li 등(2020) Li, Y., Zhang, R., Lu, J., and Shechtman, E. Few-shot image generation with elastic weight consolidation. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS\'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546\n' +
      '* Lui et al. (2019) Lui, C., Bhowmick, S. S., and Jatowt, A. Kandinsky: Abstract art- inspired visualization of social discussions. In _Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR\'19, pp. 1345-1348, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361729. doi: 10.1145/3331184.3331411. URL[https://doi.org/10.1145/3331184.3331411](https://doi.org/10.1145/3331184.3331411)\n' +
      '* Ma et al. (2023) Ma, J., Liang, J., Chen, C., and Lu, H. Subject-diffusion: Open domain personalized text-to image generation without test-time fine-tuning. _ arXiv preprint arXiv:2307.11410_, 2023.\n' +
      '* Mou et al. (2023) Mou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., and Qie, X. T2i-어댑터: 텍스트 대 이미지 확산 모델에 대한 보다 제어 가능한 능력을 발굴하기 위한 학습 어댑터; _ arXiv preprint arXiv:2302.08453_, 2023.\n' +
      '* Nagy & Perrone (2003) Nagy, J. G. and Perrone, L. 크로네커 제품은 이미지 복원에 사용됩니다. In _Advanced Signal Processing Algorithms, Architectures, and Implementations XIII_, volume 5205, pp. 155-163. International Society for Optics and Photonics, 2003.\n' +
      '* Podell et al. (2023) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: 고해상도 이미지 합성을 위한 잠재 확산 모델 개선, 2023.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '\n' +
      'Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J., Exploring the limit of transfer learning with unified text-to-text transformer. _ The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. 클립 래턴트를 사용한 계층적 텍스트 조건 이미지 생성. _ arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.\n' +
      '* Ronneberger et al. (2015) Ronneberger, O., Fischer, P., and Brox, T. U-net: 생체 의학 영상 분할을 위한 컨볼루션 네트워크. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pp. 234-241. Springer, 2015.\n' +
      '* Ruiz et al. (2023) Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aherman, K. 드림부스: 피사체 중심 생성을 위한 텍스트 대 이미지 확산 모델을 미세 조정합니다. _CVPR_, pp. 22500-22510, June 2023a.\n' +
      '* Ruiz et al. (2023b) Ruiz, N., Li, Y., Jampani, V., Wei, W., Hou, T., Pritch, Y., Wadhwa, N., Rubinstein, M., and Aherman, K. Hyperdreambooth: 텍스트-이미지 모델들의 빠른 개인화를 위한 하이퍼네트워크들, 2023b.\n' +
      '*류(2023)류, S. 빠른 텍스트-이미지 확산 미세 조정을 위한 낮은 순위 적응, 2023. URL[https://github.com/cloneofsimo/lora](https://github.com/cloneofsimo/lora)\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. _ 신경 정보 처리 시스템_, 35:36479-36494, 2022에서의 발전.\n' +
      '* Tahaei et al. (2022a) Tahaei, M., Charlaix, E., Nia, V., Ghodsi, A., and Rezagholizadeh, M. KroneckerBERT: Kronecker 분해 및 지식 증류를 통해 사전 훈련된 언어 모델의 상당한 압축. In _Proceedings of the 2022 Conference of the North American chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 2116-2127, Seattle, United States, 2022a. 컴퓨터 언어학과의 연관성 doi: 10.18653/v1/2022.naacl-main. 154. URL[https://aclanthology.org/2022.naacl-main.154](https://aclanthology.org/2022.naacl-main.154)\n' +
      '* Tahaei et al. (2022b) Tahaei, M., Charlaix, E., Nia, V., Ghodsi, A., and Rezagholizadeh, M. 크로네커버트: 크로네커 분해 및 지식 증류를 통해 사전 훈련된 언어 모델의 상당한 압축. In _Proceedings of the 2022 Conference of the North American chapter of the Computational Linguistics: Human Language Technologies_, pp. 2116-2127, 2022b.\n' +
      '* Tewel et al. (2023) Tewel, Y., Gal, R., Chechik, G., and Atzmon, Y. 텍스트 대 이미지 개인화를 위한 키 잠금 순위 1 편집 In _ACM SIGGRAPH 2023 Conference Proceedings_, pp. 1-11, 2023.\n' +
      '* Thakker et al. (2019) Thakker, U., Beu, J., Gope, D., Zhou, C., Fedorov, I., Dasika, G., and Mattina, M. 크로나커 제품을 사용하여 iot 디바이스용 rnns를 15-38x만큼 압축하는 단계; _ ArXiv preprint arXiv:1906.02876_, 2019.\n' +
      '* von Platen et al. (2023) von Platen, P., Patil, S., Lozhkov, A., Cuenca, P., Lambert, N., Rasul, K., Davaadorj, M., and Wolf, T. 디퓨저: 최첨단 확산 모델, 2023. URL[https://github.com/huggingface/diffusers](https://github.com/huggingface/diffusers).\n' +
      '* Wang et al. (2023) Wang, D., Wu, B., Zhao, G., Yao, M., Chen, H., Deng, L., Yan, T., and Li, G. Kronecker cp decomposition with fast multiplication for compression rnns. _ IEEE Transactions on Neural Networks and Learning Systems_, 34(5):2205-2219, 2023.\n' +
      '* Ye et al. (2023) Ye, H., Zhang, J., Liu, S., Han, X., and Yang, W. Ip-adapter: 텍스트 대 이미지 확산 모델에 대한 텍스트 호환 이미지 프롬프트 어댑터. _ arXiv preprint arXiv:2308.06721_, 2023.\n' +
      '* Yeh et al.(2023) Yeh, S. - Y., Hsieh, Y. - G., Gao, Z., Yang, B. B., Oh, G., and Gong, Y. 텍스트-이미지 사용자 정의 탐색: 라이코리스 미세 조정에서 모델 평가에 이르기까지 arXiv preprint arXiv:2309.14859_, 2023.\n' +
      '* Yu et al. (2022) Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scaling autoregressive models for content-rich text-to-image generation. _ 2022년 기계 학습 연구 관련 거래\n' +
      '* Zhang et al. (2020) Zhang, A., Tay, Y., Zhang, S., Chan, A., Luu, A. T., Hui, S., and Fu, J. Beyond fully-connected layer with quadaternions: Parameterization of hypercomplex multiplications with \\(1/n\\) parameters. _International Conference on Learning Representations_, 2020.\n' +
      '* Zhang et al. (2023) Zhang, L., Rao, A., and Agrawala, M. 텍스트 대 이미지 확산 모델에 조건부 제어를 추가, 2023년\n' +
      '* Zhang et al. (2015) Zhang, X., Yu, F. X., Guo, R., Kumar, S., Wang, S., and Chang, S. - F. 크로네커 제품을 기반으로 한 고속 직교 투영입니다. In _2015 IEEE International Conference on Computer Vision (ICCV)_, pp. 2929-2937, 2015. doi: 10.1109/ICCV.2015.335.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:12]\n' +
      '\n' +
      '그림 11: 자동차 수정에 대한 결과 및 자동차 산업에서 우리의 방법의 잠재적 적용을 보여준다.\n' +
      '\n' +
      '그림 10: 인간 얼굴 및 애니메이션 캐릭터 생성에 대한 결과는 초상화, 애니메이션 및 아바타를 만드는 데 우리의 방법의 끝없는 적용을 강조한다.\n' +
      '\n' +
      'KAdaptation은 Kronecker Adaptation(KAdaptation)을 통한 부분 공간 훈련 문제로서 다양한 효율적인 적응 방법에 대해 포괄적인 벤치마킹을 수행한다.\n' +
      '\n' +
      '한편, (Thakker et al., 2019)의 저자들은 Kronecker product (KP)를 사용하여 자원 제약 환경(예를 들어, IOT 디바이스들)을 위한 RNN들을 최소 정확도 손실로 15-38x만큼 압축하고, 결과 모델들을 8비트로 양자화함으로써, 압축 인자는 50x로 추가로 푸시된다. (Wang et al., 2023)에서, RNN들은 입력과 텐서-분해된 가중치 사이의 곱셈의 두 개의 빠른 알고리즘을 제안함으로써, 크로네커 텐서(KT) 분해로부터 유도된 새로운 크로네커 CANDECOMP/PARAFAC(KCP) 분해에 기초하여 압축된다.\n' +
      '\n' +
      '상기 모든 것 외에도, 크로네커 분해는 GPT-2 모델 내에서 선형 매핑을 압축하려고 시도하는 GPT 압축(Edalati et al., 2022c)에도 적용되고 있다. 제안된 모델인 Kronecker GPT-2(KnGPT2)는 Kronecker 분해된 GPT-2 모델을 기반으로 초기화된다. 이어서, 중간층 지식 증류(ILKD)로 훈련 데이터의 극히 일부에 대해서만 매우 가벼운 사전 훈련을 거친다.\n' +
      '\n' +
      '앞서 언급한 문헌 연구에서 우리는 NLP, RNN, CNN, ViT 및 GPT 공간을 포함한 다양한 도메인 내에서 모델 압축 작업에 대한 크로네커 제품의 효능을 목격했다. 결과적으로 생성 모델에 미치는 영향을 탐구하는 데 상당한 관심을 불러일으켰다.\n' +
      '\n' +
      '그림 12: 본 연구에 관련된 모든 개별 피험자를 나타내는 샘플 이미지 모음. 수집된 주제는 녹색으로 강조 표시됩니다.\n' +
      '\n' +
      '## 부록 B 데이터세트 설명\n' +
      '\n' +
      '우리는 드림부스(Ruiz et al., 2023a)의 총 25개의 데이터 세트를 통합했으며, 배낭, 개, 고양이 및 박제 동물의 이미지를 포함했다. 또한, 맞춤형 확산(Kumari et al., 2023)에서 7개의 데이터 세트를 통합하여 실험에 다양성을 도입했다. 얼굴의 공간 특징을 캡처하는 모델의 능력을 평가하기 위해 서로 다른 각도에서 캡처된 4명의 인간 각각 4~7개의 이미지로 구성된 데이터 세트를 큐레이션했다. 복잡한 입력 이미지와 텍스트 프롬프트에 대해 모델을 추가로 도전하기 위해 다양한 소스에서 6개의 애니메이션 이미지를 특징으로 하는 데이터 세트를 컴파일했다. 모든 데이터 세트는 _living animals_, _non-living objects_, _anime_ 및 _human face_의 네 그룹으로 분류된다. 또한 모델을 미세 조정하기 위해 사용된 키워드는 원래 논문에 명시된 키워드와 일치한다. 그림 12에서는 본 연구에서 사용된 고려된 모든 피험자에 대한 샘플 이미지를 제시한다.\n' +
      '\n' +
      '**Image Attribution.** 우리의 수집된 데이터 세트는 다음의 리소스들로부터 취해진다:\n' +
      '\n' +
      '롤스 로이스\n' +
      '**Hugging Face:**[https://huggingface.co/brand](https://huggingface.co/brand)\n' +
      '\n' +
      '* ** 쇼코 코미 **\n' +
      '**카카시 하타케 **\n' +
      '\n' +
      '## 부록 C 평가 메트릭\n' +
      '\n' +
      'DINO 및 CLIP-I 점수는 주제 충실도를 측정하는 반면 CLIP-T는 이미지-텍스트 정렬을 평가하기 위해 DreamBooth(Ruiz et al., 2023a)에 소개된 메트릭을 활용한다. DINO 점수는 생성 및 입력(실제) 이미지의 ViT-S/16 DINO 임베딩 간의 정규화된 쌍별 코사인 유사성이다. 유사하게, CLIP-I 점수는 생성된 및 입력 이미지의 정규화된 쌍별 CLIP ViT-B/32 이미지 임베딩이다. 한편, CLIP-T 점수는 주어진 텍스트 프롬프트와 생성된 이미지 CLIP 임베딩 사이의 정규화된 코사인 유사도를 계산한다.\n' +
      '\n' +
      '사전 학습된 CLIP 이미지 인코더를 \\(\\mathcal{I}\\), CLIP 텍스트 인코더를 \\(\\mathcal{T}\\), DINO 모델을 \\(\\mathcal{V}\\)으로 표시하자. 두 임베딩 사이의 코사인 유사도를 \\(x\\)과 \\(y\\)으로 \\(sim(x,\\,y)=\\frac{x.y}{\\left\\|x\\right\\|\\cdot\\left\\|y\\right\\|}으로 측정한다. 두 개의 이미지 집합이 주어졌을 때, 입력 이미지 집합은 \\(\\mathcal{R}=\\left\\{R_{i}\\right\\}_{i=1}^{n}\\)이고 생성된 이미지 집합은 prompt 집합 \\(\\mathcal{P}=\\left\\{P_{i}\\right\\}_{i=1}^{m}\\)에 해당하는 \\(\\mathcal{G}=\\left\\{G_{i}\\right\\}_{i=1}^{m}\\)으로 표현되며, 여기서 \\(m\\)과 \\(n\\)은 각각 생성 이미지와 입력 이미지의 수를 나타내며, \\(R,G\\in\\mathbb{R}^{3\\times H\\times W}\\) (\\(H\\)과 \\(W\\)은 이미지의 높이와 너비이다. 그런 다음 CLIP-I 이미지 대 이미지 및 CLIP-T 텍스트 대 이미지 유사성 점수는 각각 \\(S_{CLIP}^{I}\\) 및 \\(S_{CLIP}^{T}\\)으로 계산된다.\n' +
      '\n' +
      '[S_{CLIP}^{I}=\\frac{1}{mn}\\sum_{i=1}^{n}\\sum_{j=1}^{m}sim(\\mathcal{I}\\left(R_{i}\\right),\\,\\mathcal{I}\\left(G_{j}\\right))\\tag{6}\\sim(\\mathcal{I}\\left(R_{i}\\right))\n' +
      '\n' +
      '유사하게, DINO 이미지 간 유사성 점수는 (\\mathcal{I}\\left(G_{i}\\right),\\, \\mathcal{T}=\\frac{1}{m}\\sum_{i=1}^{m}sim(\\mathcal{I}\\left(G_{i}\\right),\\, \\mathcal{T}\\left(P_{i}\\right))\\tag{7}\\으로 계산될 것이다.\n' +
      '\n' +
      '[S_{DINO}=\\frac{1}{mn}\\sum_{i=1}^{n}\\sum_{j=1}^{m}sim(\\mathcal{V}\\left(R_{i}\\right), \\;\\mathcal{V}\\left(G_{j}\\right)) \\tag{8}\\sim(\\mathcal{V}\\left(R_{i}\\right))\n' +
      '\n' +
      '특히, DINO 점수는 주어진 클래스 내에서 피험자를 구별하기 위한 민감성 때문에 피험자 충실도를 평가하는 데 선호된다. 개인화된 T2I 세대에서는 편향된 결론을 피하기 위해 평가를 위해 세 가지 메트릭을 모두 공동으로 고려해야 한다. 예를 들어, 훈련 세트 이미지를 복사하는 모델은 높은 DINO 및 CLIP-I 점수를 갖지만 낮은 CLIP-T 점수를 갖는 반면, 주제 지식이 없는 SD 및 SDXL과 같은 바닐라 T2I 생성 모델은 주제 정렬이 좋지 않은 높은 CLIP-T 점수를 생성한다. 결과적으로 두 모델 모두 주제 중심 T2I 생성에는 바람직하지 않은 것으로 간주된다. 표-5에서는 총 1600개의 생성된 이미지와 프롬프트가 있는 36개의 데이터 세트에 걸쳐 계산된 표준 편차와 함께 평균 주제별 CLIP-I, CLIP-T 및 DINO 점수를 보여준다.\n' +
      '\n' +
      '## 부록 D _DiffuseKronA_ Abluse Study\n' +
      '\n' +
      '주요 논문의 4.2에 요약된 바와 같이 그림 12에 명시된 데이터 세트에 대한 광범위한 실험에서 파생된 다양한 경향과 관찰을 탐구한다.\n' +
      '\n' +
      '### 모델을 미세 조정하기 위한 모듈 선택\n' +
      '\n' +
      'UNet 네트워크의 트랜스포머 블록 내에서 선형 계층은 a) 주의 행렬과 b) 피드 포워드 네트워크(FFN)의 두 가지 구성 요소로 구성된다. 우리의 조사는 매개변수 활용의 효율성을 목표로 미세 조정에 가장 중요한 가중치 매트릭스를 식별하는 데 중점을 둔다.\n' +
      '\n' +
      '본 연구 결과는 주의력 가중치 행렬, 즉 \\((W_{K},W_{Q},W_{V},W_{O})\\)만을 미세 조정하는 것이 가장 효과적이고 매개변수 효율적인 전략임을 입증한다. 반대로, FFN 층들을 미세 조정하는 것은 이미지 합성 품질을 크게 향상시키지 않지만, 파라미터 카운트를 실질적으로 증가시켜, 계산 부하를 대략 두 배로 증가시킨다. 어텐션 매트릭스 위에 미세 조정 FFN 레이어가 있거나 없는 합성 이미지 품질을 비교하는 시각적 표현은 그림 13을 참조한다. 이 그래프는 MLP 레이어를 통합하는 것이 결과에서 충실도를 향상시키지 않는다는 것을 분명히 보여준다. 반대로, 이는 해바라기 필드 "_"에서 _"A[V] 배낭과 같은 특정 경우에서 생성된 이미지의 품질을 감소시키면서 동시에 훈련 가능한 매개변수의 수를 대략 2배 증가시킨다.\n' +
      '\n' +
      '독점적으로 미세 조정 주의 계층의 이러한 접근법은 효율성을 극대화할 뿐만 아니라 더 낮은 전체 매개변수 수를 유지하는 데 도움이 된다. 이것은 특히 어드밴타이다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c} \\hline \\hline\n' +
      '**Subject** & Cat & Cat2 & Dog2 & Dog & Dog3 & Dog6 \\\\ \\hline\n' +
      'CLIP-I** & 0.858 \\(\\pm\\) 0.017 & 0.826 \\(\\pm\\) 0.030 & 0.833 \\(\\pm\\) 0.023 & 0.854 \\(\\pm\\) 0.015 & 0.789 \\(\\pm\\) 0.027 & 0.845 \\(\\pm\\) 0.031 \\\\\\\\\n' +
      'CLIP-T** & 0.348 \\(\\pm\\) 0.033 & 0.343 \\(\\pm\\) 0.030 & 0.331 \\(\\pm\\) 0.028 & 0.349 \\(\\pm\\) 0.029 & 0.338 \\(\\pm\\) 0.025 & 0.323 \\(\\pm\\) 0.032 \\\\\\\\\n' +
      '**DINO** & 0.814 \\(\\pm\\) 0.025 & 0.752 \\(\\pm\\) 0.021 & 0.750 \\(\\pm\\) 0.049 & 0.856 \\(\\pm\\) 0.008 & 0.549 \\(\\pm\\) 0.060 & 0.788 \\(\\pm\\) 0.017 \\\\ \\hline \\hline\n' +
      '**Subject** & Dog5 & Dog7 & Dog8 & Doggy & Cat3 & Cat4 \\\\ \\hline\n' +
      'CLIP-I** & 0.824 \\(\\pm\\) 0.024 & 0.853 \\(\\pm\\) 0.015 & 0.829 \\(\\pm\\) 0.021 & 0.734 \\(\\pm\\) 0.031 & 0.834 \\(\\pm\\) 0.034 & 0.861 \\(\\pm\\) 0.016 \\\\\\\\\n' +
      'CLIP-T** & 0.337 \\(\\pm\\) 0.026 & 0.334 \\(\\pm\\) 0.025 & 0.343 \\(\\pm\\) 0.026 & 0.329 \\(\\pm\\) 0.030 & 0.348 \\(\\pm\\) 0.029 & 0.349 \\(\\pm\\) 0.032 \\\\\\(\\pm\\) 0.032\n' +
      '**DINO** & 0.761 \\(\\pm\\) 0.001 & 0.730 \\(\\pm\\) 0.049 & 0.717 \\(\\pm\\) 0.050 & 0.686 \\(\\pm\\) 0.039 & 0.744 \\(\\pm\\) 0.031 & 0.863 \\(\\pm\\) 0.030 \\\\ \\hline \\hline\n' +
      '**Subject** & Nami (Anime) & Kiriko (Anime) & Kakshi (Anime) & Shoko Komi (Anime) & Harshit (Human) & Nityanand (Human) \\\\ \\hline\n' +
      'CLIP-I** & 0.781 \\(\\pm\\) 0.035 & 0.738 \\(\\pm\\) 0.039 & 0.834 \\(\\pm\\) 0.028 & 0.761 \\(\\pm\\) 0.029 & 0.724 \\(\\pm\\) 0.018 & 0.665 \\(\\pm\\) 0.031 \\\\\\\\\n' +
      'CLIP-T** & 0.337 \\(\\pm\\) 0.029 & 0.320 \\(\\pm\\) 0.032 & 0.318 \\(\\pm\\) 0.031 & 0.356 \\(\\pm\\) 0.028 & 0.297 \\(\\pm\\) 0.036 & 0.307 \\(\\pm\\) 0.030 \\\\\\\\\n' +
      '**DINO** & 0.655 \\(\\pm\\) 0.023 & 0.483 \\(\\pm\\) 0.041 & 0.617 \\(\\pm\\) 0.061 & 0.596 \\(\\pm\\) 0.024 & 0.555 \\(\\pm\\) 0.025 & 0.447 \\(\\pm\\) 0.068 \\\\ \\hline \\hline\n' +
      '**Subject** & Shyam (Human) & Teapot & Robot Toy & Backpack & Dog Backpack & Rc Car \\\\ \\hline\n' +
      'CLIP-I** & 0.731 \\(\\pm\\) 0.015 & 0.836 \\(\\pm\\) 0.051 & 0.828 \\(\\pm\\) 0.026 & 0.907 \\(\\pm\\) 0.026 & 0.774 \\(\\pm\\) 0.037 & 0.797 \\(\\pm\\) 0.020 \\\\\\\\\n' +
      'CLIP-T** & 0.297 \\(\\pm\\) 0.026 & 0.347 \\(\\pm\\) 0.025 & 0.285 \\(\\pm\\) 0.032 & 0.347 \\(\\pm\\) 0.021 & 0.333 \\(\\pm\\) 0.027 & 0.321 \\(\\pm\\) 0.027 \\\\(\\pm\\)\n' +
      '**DINO** & 0.531 \\(\\pm\\) 0.030 & 0.528 \\(\\pm\\) 0.132 & 0.642 \\(\\pm\\) 0.023 & 0.660 \\(\\pm\\) 0.088 & 0.649 \\(\\pm\\) 0.037 & 0.651 \\(\\pm\\) 0.065 \\\\ \\hline \\hline\n' +
      '**Subject** & Shiny Shoes & Duck & Clock & Vase & Plushiel & Monster Toy \\\\ \\hline\n' +
      'CLIP-I** & 0.806 \\(\\pm\\) 0.025 & 0.845 \\(\\pm\\) 0.023 & 0.825 \\(\\pm\\) 0.062 & 0.827 \\(\\pm\\) 0.013 & 0.897 \\(\\pm\\) 0.014 & 0.782 \\(\\pm\\) 0.041 \\\\\\\\\n' +
      'CLIP-T** & 0.308 \\(\\pm\\) 0.023 & 0.303 \\(\\pm\\) 0.016 & 0.308 \\(\\pm\\) 0.035 & 0.332 \\(\\pm\\) 0.026 & 0.308 \\(\\pm\\) 0.030 & 0.308 \\(\\pm\\) 0.029 \\\\\\\\\n' +
      '**DINO** & 0.735 \\(\\pm\\) 0.090 & 0.682 \\(\\pm\\) 0.049 & 0.590 \\(\\pm\\) 0.158 & 0.705 \\(\\pm\\) 0.025 & 0.813 \\(\\pm\\) 0.027 & 0.573 \\(\\pm\\) 0.060 \\\\ \\hline \\hline\n' +
      '**Subject** & Plushiel2 & Plushiel3 & Building & Book & Car & HuggingFace \\\\ \\hline\n' +
      'CLIP-I** & 0.803 \\(\\pm\\) 0.022 & 0.792 \\(\\pm\\) 0.015 & 0.852 \\(\\pm\\) 0.013 & 0.695 \\(\\pm\\) 0.023 & 0.830 \\(\\pm\\) 0.024 & 0.810 \\(\\pm\\) 0.002 \\\\\\\n' +
      'CLIP-T** & 0.324 \\(\\pm\\) 0.024 & 0.337 \\(\\pm\\) 0.031 & 0.268 \\(\\pm\\) 0.023 & 0.301 \\(\\pm\\) 0.022 & 0.299 \\(\\pm\\) 0.032 & 0.288 \\(\\pm\\) 0.042 \\\\\\\\\n' +
      '**DINO** & 0.728 \\(\\pm\\) 0.020 & 0.766 \\(\\pm\\) 0.033 & 0.742 \\(\\pm\\) 0.019 & 0.579 \\(\\pm\\) 0.040 & 0.684 \\(\\pm\\) 0.036 & 0.692 \\(\\pm\\) 0.001 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 제안된 방법을 사용하여 각 피험자에 대한 다양한 프롬프트 실행으로부터의 평균 메트릭(CLIP-I, CLIP-T 및 DINO 점수)이다.\n' +
      '\n' +
      '계산 자원이 제한적일 때, 미세 조정 과정에서 계산 효율을 보장한다.\n' +
      '\n' +
      '크로네커 인자의### 효과\n' +
      '\n' +
      '**크로네커 요인을 초기화하는 방법?** 미세 조정 과정에서 초기화가 중요한 역할을 합니다. 초기화가 잘 되지 않은 네트워크는 훈련하기 어려운 것으로 판명될 수 있다. 따라서, 잘 만들어진 초기화 전략을 갖는 것은 효과적인 미세 조정을 달성하는 데 중요하다. 실험에서는 Normal initialization, Kaiming Uniform initialization (He et al., 2015), Xavier initialization의 세 가지 초기화 방법을 탐색하였다. 이 방법들은 Kronecker factor \\(A_{k}\\)와 \\(B_{k}\\)을 초기화하기 위해 적용되었다. 동일한 유형의 초기화로 두 요인을 초기화하는 것이 충실도를 유지하는 데 실패했음을 관찰했다. 놀랍게도, 0으로 \\(B_{k}\\)을 초기화하는 것이 미세 조정 과정에서 가장 좋은 결과를 얻었다.\n' +
      '\n' +
      '도 14에 예시된 바와 같이, (\\(A_{k}=\\text{Normal}^{s}\\), (B_{k}=0\\)) 및 (\\(A_{k}=\\text{KU}\\), (B_{k}=0\\))으로 초기화된 이미지가 가장 양호한 결과를 생성하는 반면, (\\(A_{k}=\\text{Normal}^{s}\\), (B_{k}=\\text{Normal}^{s}\\), (\\(A_{k}=\\text{XU}\\), (B_{k}=\\text{XU}\\))으로 초기화된 이미지는 가장 만족스럽지 못한 세대들을 초래한다. 여기서 \\(s\\in{1,2}\\)은 서로 다른 두 정규 분포 - \\(\\mathcal{N}\\left(0,1/a_{2}\\right)\\)와 \\(\\mathcal{N}\\left(0,\\sqrt{min(d,h)}\\right)\\)를 각각 나타내며, \\(d\\)과 \\(h\\)은 각각 특징 및 아웃 특징 차원을 나타낸다.\n' +
      '\n' +
      '그림 13: MLP와 w/o MLP와의 미세 조정 간의 질적 및 양적 비교. 미세 조정 MLP 레이어는 더 많은 파라미터를 도입하며, 주의 집중 가중치 매트릭스만을 미세 조정하는 것에 비해 이미지 생성을 향상시키지 않는다. 따라서 MLP 행렬이 없는 주의 가중치만 미세 조정될 때 가장 좋은 결과와 매개변수의 효율적인 사용이 발생한다.\n' +
      '\n' +
      '**크론커 인자의 크기의 영향.**크론커 인자의 크기는 _DiffuseKronA_에 의해 생성된 이미지에 상당한 영향을 미친다. 크로나커 인자가 클수록 해상도가 높고 디테일이 많은 이미지를 생성하는 경향이 있는 반면, 크로나커 인자가 작을수록 디테일이 적은 해상도가 낮은 이미지를 생성한다. 더 큰 크로네커 요인으로 생성된 이미지는 더 사실적으로 보이는 경향이 있는 반면, 더 작은 크로네커 요인으로 생성된 이미지는 더 추상적으로 보인다. 크로네커 요소를 변경하면 매우 상세하고 사실적인 이미지에서 보다 추상적이고 낮은 해상도에 이르기까지 광범위한 이미지가 생성될 수 있다.\n' +
      '\n' +
      '그림 15에서 \\(a_{1}\\)과 \\(a_{2}\\)이 모두 상대적으로 높은 값(각각 8과 64)으로 설정되었을 때, 생성된 이미지는 매우 높은 충실도와 상세도를 갖는다. 개 및 배경의 집의 특징은 프롬프트에서 언급한 바와 같이 파란색 색상을 갖는 집으로 더 정의되고 사실적이다. (64)를 유지하면서 \\(a_{1}\\)을 절반으로 줄이면, \\(a_{2}\\)의 높은 값으로 인해 강아지와 집이 여전히 상당히 세밀한 이미지가 생성되지만, \\(a_{1}\\)의 작은 값으로 인해 이전 경우보다 작을 수 있다. 그러나, 인자들이 작은 \\(\\leq\\)8일 때, 생성된 이미지들은 프롬프트에 부착되지 않을 뿐만 아니라 트레이닝 가능한 파라미터들의 수가 급격히 증가한다.\n' +
      '\n' +
      '표 6에서 우리는 서로 다른 크로네커 인자에 해당하는 훈련 가능한 매개변수의 수를 제시한다.\n' +
      '\n' +
      '그림 14: 다양한 초기화 전략의 영향: 정규 분포 또는 Kaiming 균일 분포로 \\(A_{k}\\)을 초기화하는 동안 \\(B_{k}\\)을 0으로 초기화할 때 최적의 결과가 달성된다.\n' +
      '\n' +
      '그림 15: 이미지 생성에서 Kronecker 인자 _i.e._, \\(a_{1}\\) 및 \\(a_{2}\\)의 영향. *(a_{1}\\) 및 \\(a_{2}\\)의 최적 선택은 ** 이미지 충실도** 및 ** 파라미터 카운트**를 고려한다. 이 후, 우리는 각각 4와 64로 \\(a_{1}\\)와 \\(a_{2}\\)을 선택하는데, 이는 낮은 Kronecker 인자(\\(A\\))가 높은 Kronecker 인자(\\(B\\))에 비해 더 낮은 차원을 가져야 한다고 해석한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:21]\n' +
      '\n' +
      '도 17 및 도 18에 도시된 바와 같은 반복들이다.\n' +
      '\n' +
      '_DiffuseKronA_에 의해 생성된 이미지들은 상이한 단계들에 대해 품질의 명확한 진행을 보여준다. 단계가 증가함에 따라 모델은 세부 사항을 개선하고 이미지의 품질을 향상시키는 것으로 판단된다. 이러한 반복 프로세스를 통해 모델은 이미지를 점진적으로 개선하여 더 많은 세부 사항을 추가하고 프롬프트에 더 정확하게 만들 수 있다.\n' +
      '\n' +
      '예를 들어, 도 17에서, _"수영장에서 물 위에 떠 있는 고양이"_에서, 초기 반복들에서, 모델은 물 위에 떠 있는 고양이의 기본 이미지를 생성한다. 반복이 진행되어 500에 도달함에 따라 모델은 이미지를 정제하여 고양이의 색상과 질감, 물 속의 물결, 수영장의 디테일과 같은 더 많은 디테일을 추가한다. 1000단계에서 이미지는 프롬프트의 상세하고 사실적인 표현입니다.\n' +
      '\n' +
      '도 17에서, _"A backpack on a white rug"_에서, 초기 반복들은 백색 표면 상의 백팩의 간단한 이미지를 생성한다. 그러나 반복이 증가함에 따라 모델은 추가\n' +
      '\n' +
      '그림 17: 이미지 생성에서의 훈련 단계가 SDXL에 미치는 영향. 단순 프롬프트(행 1)의 경우, _DiffuseKronA_는 단계 500과 1000 사이에서 일관되게 유리한 결과를 제공한다. 반대로, 더 복잡한 프롬프트(행 2)의 경우, 원하는 결과에 도달하려면 단계 1000 이후까지 기다려야 할 수 있다.\n' +
      '\n' +
      '그림 19: 원샷 이미지 생성 결과는 높은 충실도와 더 나은 텍스트 정렬을 유지하면서 _DiffuseKronA_의 현저한 효과를 보여준다.\n' +
      '\n' +
      '도 18: 트레이닝 반복들에 대한 이미지 정렬, 텍스트 정렬, 및 DINO 스코어들을 묘사하는 플롯들. 점수는 그림 17에 표시된 것과 동일한 이미지 및 프롬프트 세트로부터 계산된다.\n' +
      '\n' +
      '지퍼, 포켓 및 스트랩과 같은 백팩에 대한 자세한 내용입니다. 또한 화이트 러그에 질감을 더하기 시작하여 더욱 사실적으로 보입니다. 마지막 반복에 의해 흰색 러그는 질감이 더 부드러워져 미세한 이미지를 생성합니다.\n' +
      '\n' +
      '### 트레이닝 이미지 수의 영향\n' +
      '\n' +
      '1 원샷 이미지 생성\n' +
      '\n' +
      '이미지는 고품질이며 텍스트 프롬프트를 정확하게 나타냅니다. 명확하고 잘 그려져 있으며 각 이미지의 내용이 해당 텍스트 프롬프트와 완벽하게 일치합니다. 예를 들어, 도 19에서, _"A[V] 로고"_의 이미지는 손을 가진 노란색 스마일 얼굴이다. 동전으로 만들어진 __ 프롬프트는 흰색 테두리를 가진 회색 귀신을 낳아 추상적인 개념을 통합하는 모델의 능력을 보여주었다. 수채화로 만든 _"미래형 네온 글로우"_와 _"_ 프롬프트는 각각 분홍색 문어와 노란색 문어를 생성하여 다양한 예술적 스타일을 적용하는 모델의 다양성을 보여준다. 모형이 프롬프트 _"노트북의 스티커"_에서 회색 노트에서 기타를 연주하는 문어의 이미지를 생성하는 능력은 고급 능력의 증거이다.\n' +
      '\n' +
      '이미지는 스타일 및 내용 면에서 다양하며, 특히 이러한 이미지가 원샷 설정으로 생성되어 이미지 편집 작업에 적합하다는 점을 고려하면 인상적이다. 우리의 모델은 단일 입력 이미지로 강력한 결과를 생성하는 데 놀라운 숙련도를 보여주지만 다양한 포즈나 각도를 생성하려고 할 때 도전에 직면한다. 그러나, 다수의 이미지들(2, 3, 또는 4)이 공급될 때, 우리의 모델은 입력 이미지들로부터 추가적인 공간 특징들을 능숙하게 캡처하여, 더 넓은 범위의 포즈들 및 각도들을 갖는 이미지들의 생성을 용이하게 한다. 우리의 모델은 그림 20과 같이 더 정확하고 상세한 출력 이미지를 생성하기 위해 여러 입력 이미지의 정보를 효과적으로 사용할 수 있다.\n' +
      '\n' +
      '추론 하이퍼파라미터의### 효과\n' +
      '\n' +
      '**Guidance Score (\\(\\alpha\\))** \\(\\alpha\\)로 표시된 안내 점수는 생성된 이미지에서 색상의 변동 및 분포를 조절한다. 낮은 안내 점수는 입력 텍스트 프롬프트에 제공된 설명과 정렬하여 이미지에서 더 차분한 버전의 색상을 생성한다. 대조적으로, 더 높은 안내 점수는 더 활기차고 뚜렷한 색상을 갖는 이미지를 초래한다. 7에서 10까지의 지침 점수는 일반적으로 적절하고 잘 분포된 색상 팔레트를 사용하여 이미지를 산출한다.\n' +
      '\n' +
      '도 21의 _"A[V] 완구"_의 예에서, 프롬프트가 _"보라색"_로 만들어진 경우, 1 또는 3의 안내 점수에 대해 적색의 라벤더 색조가 생성되는 것이 명백하다. 반대로, 15를 초과하는 안내 점수로, 뽕나무 음영이 생성된다. 8에 가까운 지도 점수의 경우 순수한 보라색을 띠는 이미지가 형성된다.\n' +
      '\n' +
      '**추론 단계 수.** 단계 수는 생성된 이미지의 세분성을 정의하는 데 중요한 역할을 한다. 도 22에 예시된 바와 같이, 초기 단계들 동안, 모델은 텍스트 프롬프트에 정렬되는 주제를 생성하고 입력 이미지로부터 특징들을 통합하기 시작한다. 세대가 진행됨에 따라 이미지에는 더 미세한 세부 사항이 등장합니다. 프롬프트의 복잡성에 따라 최적의 결과가 30~70단계 범위에서 관찰되며 평균 50단계가 가장 효과적인 것으로 입증되었다. 그러나, 100 단계를 초과하는 것은 노이즈의 도입 및 생성된 이미지의 품질 저하를 초래한다.\n' +
      '\n' +
      '생성된 이미지의 품질은 추론 단계의 수가 증가함에 따라 향상되는 것으로 나타난다. 예를 들어, 프롬프트 _"장난감"_ 및 _"착용 선글라스"_에 대한 이미지는 10개의 추론 단계에 비해 각각 50 및 75개의 추론 단계에서 더 높은 품질을 갖는 것으로 보인다.\n' +
      '\n' +
      'LoRA-DreamBooth vs _DiffuseKronA__DiffuseKronA_\n' +
      '\n' +
      '이 섹션에서는 충실도, 색상 분포, 텍스트 정렬, 안정성 및 복잡성을 포함한 다양한 측면에 걸쳐 LoRA-DreamBooth 및 _DiffuseKronA_를 비교하여 모델 성능 분석을 확장한다(섹션 4.3).\n' +
      '\n' +
      '### 곱셈 순위 특성 및 기울기 업데이트\n' +
      '\n' +
      '\\(A\\)과 \\(B\\)을 각각 \\(m\\times n\\)과 \\(p\\times q\\) 행렬로 하자. 만약 \\(A\\)이 \\(r\\)의 순위를 가지고 \\(B\\)이 \\(s\\)의 순위를 가지고 있다고 가정하자.\n' +
      '\n' +
      '**정리 E.1**.: 내적에 대한 _Ranks는 곱셈과 곱셈의 순위로 묶인다. 즉 \\(rank(A\\cdot B)=\\min(rank(A),\\ rank(B))=\\min(r,\\s)\\)._\n' +
      '\n' +
      '**theorem E.2**.: _Ranks for Kronecker products is multiplicative, 즉 \\(rank(A\\otimes B)=rank(A\\times rank(B)=r\\times s\\)._\n' +
      '\n' +
      '크로네커 곱은 곱셈 순위의 장점이 있기 때문에 내적에 비해 이미지의 기본 분포를 더 잘 표현할 수 있다.\n' +
      '\n' +
      'Low-rank decomposition (LoRA)와 Kronecker 곱의 또 다른 차이점은 \\(d\\left(\\cdot\\right)\\)으로 표시된 도함수를 계산할 때이다. LoRA의 경우 \\(d\\left(A\\cdot B\\right)=d\\left(A\\right)\\cdot B+A\\cdot d\\left(B\\right)\\이다. 그러나 Kronecker 제품의 경우 \\(d\\left(A\\otimes B\\right)=d\\left(A\\right)\\otimes d\\left(B\\right)\\. LoRA의 그래디언트 업데이트는 구조화된 관계 없이 직접적인 반면 크로네커 제품은 업데이트 동안 구조를 보존한다. 내적이 더 간단하고 LoRA가 각각의 파라미터를 독립적으로 업데이트하는 반면, Kronecker 제품은 \\(A\\)와 \\(B\\)에 저장된 파라미터 사이의 관계를 보존할 때 유익할 수 있는 구조화된 업데이트를 도입한다.\n' +
      '\n' +
      '### 충실도 및 색상 분포\n' +
      '\n' +
      'DiffuseKronA_는 공간 특징을 캡처하는 능력과 함께 크로네커 제품의 더 높은 표현력 대신 LoRA-DreamBooth에 비해 우수한 충실도의 이미지를 생성한다.\n' +
      '\n' +
      '도 23의 _"A[V] backpack"_의 예에서, 다음과 같은 관찰이 이루어질 수 있다:\n' +
      '\n' +
      '(1)_"with the Eiffel Tower in the background"_: _DiffuseKronA_에 의해 생성된 배낭은 Eiffel Tower in the background로 그림화되어, LoRA-DreamBooth가 하지 못하는 배낭의 빨간색과 도시 경관의 뮤트한 색들 사이의 현저한 대조를 생성한다.\n' +
      '\n' +
      '(2) 배경에서 _"city"_: _DiffuseKronA_에 의해 생성된 백팩은 도시 배경에 대해 설정되며, 여기서 빨간색은 건물의 중성 톤에 대해 두드러지는 반면, LoRA-DreamBooth는 이미지 간에 높은 콘트라스트를 생성하지 않는다.\n' +
      '\n' +
      '(3) 해변에서 _"_: _DiffuseKronA_에 의해 생성된 이미지는 해변 위의 배낭을 보여주는데, 여기서 빨간색은 물의 파란색과, 모래는 베이지색으로 대조된다.\n' +
      '\n' +
      '그림 20: 훈련 이미지가 미세 조정에 미치는 영향. _DiffuseKronA_는 단일 이미지로 인상적인 결과를 생성하지만, 더 많은 트레이닝 이미지가 변형으로 제공될 때 더 넓은 범위의 관점을 갖는 이미지의 생성이 향상된다.\n' +
      '\n' +
      '그림 21: 안내 점수(\\(\\alpha\\))를 조정하여 제작한 이미지는 \\(7\\)의 점수가 가장 현실적인 결과를 산출함을 알 수 있다. 점수가 \\(7\\) 이상으로 증가하면 이미지의 대비가 크게 증폭됩니다.\n' +
      '\n' +
      '그림 22: 추론 단계가 이미지 생성에 미치는 영향. 50-70 단계의 범위에서 최적의 결과가 달성되어 텍스트 입력과 주제 충실도 사이의 균형을 이룬다. 여기서, 우리는 추론 시간을 최소화하기 위해 50개의 추론 단계를 선택했다.\n' +
      '\n' +
      '도 23: _DiffuseKronA_ 및 LoRA-DreamBooth에서의 충실도 및 색상 보존의 비교.\n' +
      '\n' +
      '### Text Alignment\n' +
      '\n' +
      '_DiffuseKronA_는 Lora-DreamBooth에 비해 이미지와 텍스트를 정렬하는 데 더 정확하다. 예를 들어, 첫 번째 행에서 _DiffuseKronA_는 텍스트를 "_" 내부의 _"해바라기와 해바라기가 있는 꽃병의 이미지와 올바르게 정렬하는 반면 LoRA-DreamBooth는 입력 이미지와 동일한 색상의 꽃병에 해바라기 정렬에 실패한다.\n' +
      '\n' +
      '_"A[V] 문자"_에서 애니메이션을 포함하는 것과 같은 도 24에서와 같은 보다 복잡한 입력 예들에서, LoRA-DreamBooth에 의해 생성된 이미지들은 식사를 요리하는 감각 및 노래방 바가 부족한 반면, _DiffuseKronA_는 제공된 텍스트 프롬프트들과 밀접하게 정렬되는 이미지들을 일관되게 생성한다.\n' +
      '\n' +
      '### 복잡한 입력 이미지와 프롬프트\n' +
      '\n' +
      '_DiffuseKronA_는 텍스트 프롬프트 내에서 뉘앙스를 캡처하는 데 주목할 만한 중점을 보여주며 입력 이미지에서 가장 높은 정도로 복잡한 세부 사항을 보존하는 데 탁월한다. 대조적으로, LoRA-DreamBooth는 이러한 속성이 부족하다. 이러한 구별은 도 25에서 명백하며, 프롬프트 _"A[V] 얼굴"_에 대해 _DiffuseKronA_는 아이보리 화이트 블레이저와 웃는 얼굴을 성공적으로 생성하는 반면, LoRA-DreamBooth는 얼굴의 색상과 미소를 모두 유지하기 위해 고군분투한다.\n' +
      '\n' +
      '유사하게, 도 25의 프롬프트 _"A[V] 클록"_에 대해, _DiffuseKronA_는 입력 이미지들로부터 상세한 번호들, 특히 3을 정확하게 재생한다. 입방체 모양의 시계를 만들면서 숫자의 구조를 보존하는 데 어려움을 겪지만 여전히 텍스트 세부 사항에 대한 강력한 초점을 유지하고 있습니다. 이는 LoRA-DreamBooth에서 결여된 특성입니다.\n' +
      '\n' +
      '### 질적·정량적 비교\n' +
      '\n' +
      'SDXL 상에서 _DiffuseKronA_ 및 LoRA-DreamBooth의 이미지 생성 능력을 평가하였다 (Podell et al., 2023). 우리의 연구 결과는 LoRA-DreamBooth에 비해 높은 충실도, 더 정확한 색상 분포 및 더 큰 안정성을 가진 이미지를 생성하는 데 _DiffuseKronA_가 탁월하다는 것을 보여준다.\n' +
      '\n' +
      '그림 24: 제안된 _DiffuseKronA_와 LoRA-DreamBooth에 의해 생성된 이미지에서 텍스트 정렬의 비교.\n' +
      '\n' +
      '## 부록 F 비교 다른 저순위 분해 방법과의 비교\n' +
      '\n' +
      '이 섹션에서, 우리는 우리의 _DiffuseKronA_를 LoRA 이외의 저-랭크 방법, 구체적으로 LoKr(Yeh et al., 2023) 및 LoHA(Yeh et al., 2023)와 비교한다. 우리는 또한 우리의 구현이 LyCORIS 프로젝트(Yeh et al., 2023)와 무관하며, _DiffuseKronA1_에서 LoKr 또는 LoHA를 사용하지 않았다는 점에 주목한다. 우리는 _DiffuseKronA_와 이러한 방법 사이의 주요 차이점을 다음과 같이 요약한다:\n' +
      '\n' +
      '각주 1: 공정한 비교를 보장하기 위해 LoKr과 LoHA를 SDXL 백본에 통합했다.\n' +
      '\n' +
      '_DiffuseKronA_는 2개의 제어 가능한 매개변수(\\(a_{1}\\) 및 \\(a_{2}\\))를 가지며, 이는 광범위한 실험을 통해 수동으로 선택되는 반면(도 15 및 표 6 참조), LoKr(Yeh et al., 2023)은 입력 차원과 _factor_라고 하는 또 다른 하이퍼-파라미터에 의존하는 인수분해 함수(오른쪽 참조)에서 언급된 절차를 따른다. (Yeh et al., 2023)에서 도 2의 구현에 대한 설명에 이어, "우리는 인자를 8로 설정하고 두 번째 블록의 추가 분해를 수행하지 않는다"를 인용하면, 기본 구현은 \\(A\\)을 차원의 제곱 행렬(\\(\\textit{factor}\\times\\textit{factor}\\)로 만든다. 특히, 어떤 요인에서 \\(f>0\\)에 대해 \\(A\\)은 항상 형상 \\(f\\times f)\\의 정사각형 행렬일 것이다. 이는 _DiffusKronA_(그림 15의 대각진입)의 특수한 경우(부분집합)이지만 \\(f=-1\\)에 대해 \\(A\\) 행렬 크기는 치수에 완전히 의존하며 항상 정사각형 행렬은 아니다.\n' +
      '\n' +
      '```\n' +
      '1deffactorization(dim:int,factor:int=-1)->tuple(int,int]:\n' +
      '2\n' +
      '3iffactor>0and(dim%factor)==0:\n' +
      '4m=factor\n' +
      '5n=dim//factor\n' +
      '6ifm>n:\n' +
      '7n,m=m,n\n' +
      '8returnm,n\n' +
      '9iffactor<0:\n' +
      '10factor=dim\n' +
      '11m,n=1,dim\n' +
      '12length=m+n\n' +
      '13whilem<n:\n' +
      '14new_m=m+1\n' +
      '15whiledim%new_m!=0:\n' +
      '16new_m+=1\n' +
      '17new_n=dim//new_m\n' +
      '18ifnew_m+new_m>lengthornew_m>factor:\n' +
      '19break\n' +
      '20else:\n' +
      '21m,n=new_m,new_m\n' +
      '22ifm>n:\n' +
      '23n,m=m,n\n' +
      '24returnm,n\n' +
      '```\n' +
      '\n' +
      '목록 1: 이 코드 조각은 공식 LyCORIS 코드베이스(링크)로부터 추출된다.\n' +
      '\n' +
      '그림 25: _DiffuseKronA_와 LoRA-DreamBooth에 의한 복합 프롬프트 및 입력 영상에 대한 영상 생성 비교.\n' +
      '\n' +
      '이러한 속성은 크로네커 분해를 수행하는 방법을 LoKr의 슈퍼세트로 만들어 LoKr에 비해 더 큰 제어와 유연성을 제공한다. 반면에 LoHA는 LoRA와 유사하게 제어 가능한 파라미터 _i.e._, rank를 하나만 가지고 있다.\n' +
      '\n' +
      'LoKr은 \\(\\Delta W=A\\otimes(B\\cdot C)\\)의 일반적인 형태를 취하고, LoHA는 \\(\\Delta W=(A\\cdot B)\\odot(C\\cdot D)\\)을 채택하며, 여기서 \\(\\odot\\)은 하다마드 생성물을 나타낸다. 더 자세한 내용은 독자들을 도 1(Yeh et al., 2023)을 참조한다. 정의에 기초하여 LoHA는 크로네커 분해를 사용하는 것의 이점을 탐구하지 않는다.\n' +
      '\n' +
      'Yeh et al. (2023)은 확산 모델 미세 조정에서 Kronecker 분해의 첫 번째 사용을 제공했지만 소수 샷 T2I 개인화 설정에서는 제한된 분석을 제공했다. 본 연구에서는 크로네커 분해를 사용하는 것의 이점을 입증하기 위해 상세한 분석과 탐색을 수행했다. 우리의 새로운 통찰력에는 매개변수 효율성에 대한 대규모 분석, 하이퍼파라미터에 대한 안정성 향상, 텍스트 정렬 및 충실도 향상 등이 포함된다.\n' +
      '\n' +
      '우리는 도 26 및 도 27의 (Yeh et al., 2023)로부터의 디폴트 구현들을 사용하여 우리의 _DiffuseKronA_를 LoKr 및 LoHA와 추가로 비교한다. 그러나 기본 설정은 SD 변형에서 사용되었으며 개인화된 T2I 세대가 모델 설정 및 하이퍼 매개변수 선택에 매우 민감하다는 것도 분명하다. 이러한 사실을 고려하여 두 어댑터의 하이퍼파라미터도 조사했다. 그림 28에서는 SDXL을 활용하여 LoKr에 대한 요인과 순위를 조사한 절제 연구를 제시하였고, 그림 29에서는 학습률에 대한 절제 연구를 제시한다. 또한, 그림 30은 SDXL을 사용한 LoHA의 학습률과 순위에 대한 절제 연구를 특징으로 한다. 분석 결과, LoKr의 경우 학습률이 \\(1\\times 10^{-3}\\)인 경우 최적 계수는 -1이고 최적 순위는 8이며, LoHA의 경우 학습률이 \\(1\\times 10^{-4}\\)인 경우 최적 순위는 4이다.\n' +
      '\n' +
      '또한, 표 7 및 표 8에 자세히 설명된 바와 같이 이미지 대 이미지 및 이미지 대 텍스트 정렬 점수와 함께 매개변수 수를 포함하는 정량적 비교가 수행된다. 표 7의 결과는 LoKr이 여전히 더 적은 매개변수 _DiffuseKronA_(a_{1}=16\\)를 보유하지만 우수한 CLIP-I, CLIP-T 및 DINO 점수를 달성함을 나타낸다. 이러한 대비는 도 26에 묘사된 시각적 예들에서 쉽게 눈에 띈다. 배경에서 Effel Tower를 갖는 프롬프트 _"A[V] 장난감에 대해, LoKr은 _DiffuseKronA_(\\(a_{1}=16\\))와 달리 배경에서 _Eiffel Tower_를 구성하지 못한다. 유사하게, 물 위에 떠 있는 _"A[V] 찻주전자의 경우 "_ LoKr는 찻주전자의 주둥이를 왜곡하는 반면, _DiffuseKronA_는 충실도를 유지한다. _"A[V] toy"_(마지막 행)의 경우, _DiffuseKronA_의 결과는 두 프롬프트에 대해 LoKr에 비해 훨씬 더 정렬된다. 반대로, _dog_ 및 _cat_ 예제의 경우, 모든 방법은 텍스트 정렬뿐만 아니라 충실도 측면에서 유사한 시각적 외관을 보여준다. 결과적으로 LoKr은 매개변수 수를 줄이면서도 복잡한 입력 이미지나 여러 컨텍스트가 있는 텍스트 프롬프트에 어려움을 겪는다는 것을 알 수 있다. 따라서 _DiffusekronA_는 CLIP-I, CLIP-T 및 DINO 메트릭에 걸쳐 평균 점수를 유지하면서 매개변수에서 효율성을 달성한다. 따라서 매개변수 효율성과 개인화된 이미지 생성 간의 더 나은 균형을 달성한다.\n' +
      '\n' +
      '그림 26: LoRA, LoKr 및 LoHA를 포함한 다른 저순위 방법과 _DiffusekronA_의 4가지 변이체의 **정성적 비교**. 학습률: _DiffusekronA_ (\\(5\\times 10^{-4}\\)), LoRA (\\(1\\times 10^{-4}\\)), LoKr (\\(1\\times 10^{-3}\\)) & LoHA (\\(1\\times 10^{-4}\\))\n' +
      '\n' +
      '그림 27: **정성적 비교.** 결과는 LoKr 구현에 의해 주어진 기본 요인에 대해 표시되며 다양한 요인은 2, 4, 8 및 16이다.\n' +
      '\n' +
      'Figure 28: **Ablation study on factor and rank for LoKr using SDXL, the learning rate \\(1\\times 10^{-3}\\) 우리는 최적 요인과 순위가 각각 -1과 8임을 발견했다. 또한 행렬 \\(A\\)과 \\(B\\)의 저순위 분해를 나타내는 db=True를 실험하였고, db=False는 행렬 \\(B\\)만을 더 분해하였다. (계속)**\n' +
      '\n' +
      'Figure 28: **Ablation study on factor and rank** for LoKr using SDXL, and learning rate \\(1\\times 10^{-3}\\) 우리는 최적 요인과 순위가 각각 -1과 8임을 발견했다. 또한 행렬 \\(A\\)과 \\(B\\)의 저순위 분해를 나타내는 db=True를 실험하였고, db=False는 행렬 \\(B\\)만을 더 분해하였다. (**Continued.**)\n' +
      '\n' +
      'Figure 28: **Ablation study on factor and rank** for LoKr using SDXL, and learning rate \\(1\\times 10^{-3}\\) 우리는 최적 요인과 순위가 각각 -1과 8임을 발견했다. 또한 행렬 \\(A\\)과 \\(B\\)의 저순위 분해를 나타내는 db=True를 실험하였고, db=False는 행렬 \\(B\\)만을 더 분해하였다. **(end)**\n' +
      '\n' +
      'Figure 29: **Ablation study on factor and learning rate for LoKr using SDXL, with fixed factor of -1. We found the optimal learning rate and rank is \\(1\\times 10^{-3}\\) and 8. 또한 행렬 \\(A\\)과 \\(B\\)의 저순위 분해를 나타내는 db=True를 실험하였고, db=False는 행렬 \\(B\\)만을 더 분해하였다. (계속..)**\n' +
      '\n' +
      'Figure 29: **Ablation study on factor and learning rate for LoKr using SDXL, with fixed factor of -1. We found the optimal learning rate and rank is \\(1\\times 10^{-3}\\) and 8. 또한 행렬 \\(A\\)과 \\(B\\)의 저순위 분해를 나타내는 db=True를 실험하였고, db=False는 행렬 \\(B\\)만을 더 분해하였다. (계속..)**\n' +
      '\n' +
      'Figure 29: **Ablation study on factor and learning rate for LoKr using SDXL, with fixed factor of -1. We found the optimal learning rate and rank is \\(1\\times 10^{-3}\\) and 8. 또한 행렬 \\(A\\)과 \\(B\\)의 저순위 분해를 나타내는 db=True를 실험하였고, db=False는 행렬 \\(B\\)만을 더 분해하였다. (끝)**\n' +
      '\n' +
      '도 30: SDXL을 이용한 LoHA에 대한 학습률 및 랭크에 대한 절제 연구. 최적 학습률과 순위는 각각 \\(1\\times 10^{-4}\\)과 \\(4)으로 나타났다.\n' +
      '\n' +
      '## 부록 G 최신기술과의 비교\n' +
      '\n' +
      '**Qualitative Comparison.** 이 절에서는 본 논문의 4.4를 확장하여 _DiffuseKronA_와 DreamBooth, LoRA-DreamBooth, SVDiff, Textual Invention, Custom Diffusion을 포함한 최첨단 텍스트-이미지 개인화 모델을 비교한다.\n' +
      '\n' +
      '**(1)** Textual Inversion(Gal et al., 2022)은 피사체 이미지의 트레이닝 세트를 재구성하기 위해 플레이스홀더 임베딩을 최적화하는 미세 조정 방법이다. 새로운 개념을 학습하기 위해서는 3,000 단계가 필요한데, A100 GPU에서 30분 정도 걸린다(Li et al., 2023).\n' +
      '\n' +
      '**(2)** DreamBooth(Ruiz et al., 2023)는 정규화의 한 형태로서 추가적인 보존 손실을 통해 전체 네트워크를 정제하여, 유망한 결과를 나타내는 시각적 품질의 향상으로 이어진다. 새로운 개념에 대해 DreamBooth를 업데이트하는 것은 통상적으로 A100 GPU 상에서 약 6분을 필요로 한다(Li et al., 2023).\n' +
      '\n' +
      '**(3)** LoRA-DreamBooth(Ryu, 2023)는 텍스트-이미지 확산 모델의 파라미터 효율적인 미세 조정 어텐션-가중치 행렬에 대한 저순위 적응을 탐구한다. 새로운 개념의 미세 조정 LoRA-DreamBooth는 일반적으로 단일 24GB NVIDIA RTX-3090 GPU에서 약 5분이 소요됩니다.\n' +
      '\n' +
      '**(4)** SVDiff(Han et al., 2023)는 가중치 매트릭스들의 특이값들을 미세 조정하는 것을 수반하여, 과적합 및 언어 표류의 위험을 감소시키는 작고 효율적인 파라미터 공간으로 이어진다. 단일 24GB 엔비디아 RTX-3090 GPU1에서 약 15분이 소요되었습니다.\n' +
      '\n' +
      '각주 1: SVDiff는 공식 코드베이스를 공개하지 않았으며, 그림 31의 SVDiff 결과에 대해 오픈 소스 코드를 사용했다.\n' +
      '\n' +
      '**(5)** 맞춤형 확산(Kumari et al., 2023)은 컨디셔닝 메커니즘을 통한 가중치 매트릭스들의 선택적 미세 조정을 수반하여, 확산 모델들의 파라미터-효율적인 정제화를 가능하게 한다. 이 접근법은 다중 개념 미세 조정을 포함하도록 추가로 확장된다. 사용자 정의 확산의 미세 조정 시간은 2개의 A100 GPU에서 약 6분이다.\n' +
      '\n' +
      '**Qualitative Comparison.**_DiffuseKronA_는 입력 이미지와 밀접하게 정렬된 이미지를 일관되게 생성하고 입력 텍스트 프롬프트에 지정된 기능을 일관되게 통합한다. 입력 텍스트 프롬프트에 대한 향상된 충실도와 포괄적인 이해는 크로네커 제품 기반 적응에 의해 촉진되는 구조 보존 능력과 향상된 표현력에 기인할 수 있다. LoRA-DreamBooth에 의해 생성된 이미지들은 도 31에 도시된 바와 같이, 고품질은 아니며 개선을 위한 광범위한 실험을 요구한다. 도면에서 도시된 바와 같이, _DiffuseKronA_는 잘 정의된 이미지들을 생성할 뿐만 아니라 Custom Diffusion에 비해 더 나은 색상 분포를 갖는다.\n' +
      '\n' +
      '## 부록 H 실용적 시사점\n' +
      '\n' +
      '* 콘텐츠 생성: 텍스트 프롬프트로부터 사실적 콘텐츠를 생성하는 데 사용될 수 있다.\n' +
      '* 이미지 편집 및 인페인팅: 모델을 사용하여 이미지를 편집하거나 이미지의 누락된 부분을 채울 수 있습니다.\n' +
      '* Super-Resolution: 영상의 해상도를 높이는데 사용될 수 있다.\n' +
      '* 비디오 합성: 모델은 텍스트 프롬프트로부터 비디오를 생성하는 데 사용될 수 있다.\n' +
      '* 3D 자산 생산: 텍스트 프롬프트로부터 3D 자산을 생성하는 데 사용될 수 있다.\n' +
      '* 개인화 세대: 모델은 드림부스 미세 조정으로 개인화 세대에 활용할 수 있다.\n' +
      '* 자원 효율성: 모델은 자원 효율적이며 제한된 자원으로 훈련될 수 있다.\n' +
      '* 모델 압축: 모델은 아키텍처 압축을 가능하게 하여, 파라미터 수, 샘플링 단계당 MAC, 및 레이턴시를 감소시킨다.\n' +
      '\n' +
      '도 31: _DiffuseKronA_, LoRA-DreamBooth, Textual Inversion, DreamBooth, Custom Diffusion에 의해 생성된 이미지들 간의 **정성적 비교** 특히, 제안된 방법의 결과는 \\(a_{2}=8\\)을 고려하여 생성된다. 이러한 모든 방법의 원래 설정을 유지하고 공정한 비교를 보장하기 위해 SD CompVis-1.4(CompVis, 2021) 변형을 사용했다.\n' +
      '\n' +
      '도 32: **Text-Image Alignment 상의 SOTA와 _DiffuseKronA_의 정량적 비교**. 점수는 그림 31에 표시된 것과 동일한 이미지 및 프롬프트 세트로부터 계산된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>