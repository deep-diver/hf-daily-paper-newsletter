<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MAGID: 합성 다중 모달 데이터 셋 생성을 위한 자동 파이프라인\n' +
      '\n' +
      'Hossein Aboutalebi\n' +
      '\n' +
      'AWS AI 연구소에서 인턴을 하면서 진행한 업무입니다.\n' +
      '\n' +
      'Hwanjun Song\n' +
      '\n' +
      'AWS AI 연구소에서 인턴을 하면서 진행한 업무입니다.\n' +
      '\n' +
      'Yusheng Xie\n' +
      '\n' +
      'AWS AI 연구소에서 인턴을 하면서 진행한 업무입니다.\n' +
      '\n' +
      'Arshit Gupta\n' +
      '\n' +
      'AWS AI 연구소에서 인턴을 하면서 진행한 업무입니다.\n' +
      '\n' +
      'Justin Sun\n' +
      '\n' +
      'AWS AI 연구소에서 인턴을 하면서 진행한 업무입니다.\n' +
      '\n' +
      'Hang Su\n' +
      '\n' +
      'AWS AI 연구소에서 인턴을 하면서 진행한 업무입니다.\n' +
      '\n' +
      'Igor Shalyminov\n' +
      '\n' +
      'AWS AI 연구소에서 인턴을 하면서 진행한 업무입니다.\n' +
      '\n' +
      'Nikolaos Pappas\n' +
      '\n' +
      'AWS AI 연구소에서 인턴을 하면서 진행한 업무입니다.\n' +
      '\n' +
      'Siffi Singh\n' +
      '\n' +
      'AWS AI 연구소에서 인턴을 하면서 진행한 업무입니다.\n' +
      '\n' +
      'Saab Mansour\n' +
      '\n' +
      'AWS AI 연구소에서 인턴을 하면서 진행한 업무입니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '멀티모달 대화형 시스템의 개발은 LLM을 위해 대량으로 필요한 풍부한 멀티모달(텍스트, 이미지) 대화 데이터의 부족으로 인해 방해를 받는다. 이전 접근 방식은 검색된 이미지로 텍스트 대화를 보완하여 개인 정보 보호, 다양성 및 품질 제약을 제기한다. 본 연구에서는 다양하고 고품질의 이미지를 가진 텍스트 전용 대화문을 증강하기 위한 프레임워크인 \'M**ultimodal **A**ugmented **G**enerative **I**mages **D**ialogues(MAGID)\'를 소개하고, 확산 모델을 적용하여 식별된 텍스트와의 정렬을 보장한다. 마지막으로, MAGID는 이미지 기술 생성 모듈(텍스트 LLM)과 이미지 품질 모듈(어드레싱 미학, 이미지-텍스트 매칭 및 안전성) 사이의 혁신적인 피드백 루프를 통합하여 고품질 및 다중 모드 대화를 생성한다. 자동화 및 인간 평가를 사용하여 3개의 대화 데이터 세트에 대한 MAGID를 다른 SOTA 기준선과 비교한다. 우리의 결과는 MAGID가 기준선과 비슷하거나 우수하다는 것을 보여주며, 특히 이미지 데이터베이스가 작은 검색 기준선에 대해 인간 평가가 크게 개선되었다.\n' +
      '\n' +
      '각주 1: 코드로의 링크: [http://anon_for_review.com](http://anon_for_review.com)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근, 대규모 언어 모델(LLM)의 발전은 AI에서 가능성과 연구 방향을 확장했으며, 연구는 대화 데이터 세트 Liu et al.(2023); Penedo et al.(2023)을 처리하는 데 광범위한 능력을 강조한다. 특히, 이미지들_ 공유가 인간-인간 대화 Alayrac 등(2022); OpenAI 등(2023); Liu 등(2023)의 필수적인 측면이라는 점을 감안할 때, 멀티모달 대화 데이터 세트에 대한 그들의 적용에 대한 관심이 증가하고 있다.\n' +
      '\n' +
      'MMDialog Feng et al. (2022), DialogCC Lee et al. (2022)2 및 PhotoChat Zang et al. (2021)과 같은 여러 다중 모드 대화 데이터 세트가 다중 모드 LLM을 훈련하기 위해 도입되었다. 이러한 데이터 세트는 검색 기반 접근법을 사용하거나 MS-COCO Lin et al.(2014)과 같은 세트 이미지 뱅크에서 이미지를 끌어오거나 실제 인간-인간 채팅을 수반하더라도 대화를 대화당 하나의 이미지만으로 제한한다. 또한 소셜 미디어와 같은 플랫폼에서 실제 데이터 세트를 활용할 때 개인 정보 보호 문제 및 이미지 품질과 관련된 문제는 교육에 중요한 과제가 된다.\n' +
      '\n' +
      '각주 2: 최근 공개된 버전의 DialogCC는 LLM Lee 등을 활용한다(2023). 이 논문을 쓸 당시, 우리는 새로운 버전에 접근할 수 없었다.\n' +
      '\n' +
      '결과적으로, 이러한 방법들은 작은 이미지 데이터베이스가 리 등(2021, 2022)의 실제 인간-인간 대화의 넓은 범위를 적절하게 포착할 수 없기 때문에 이미지의 다양성을 제한한다. 또한, 유해하고 사적인 콘텐츠를 포함하는 저품질 이미지 Feng et al.(2022) 및 액세스 가능한 데이터 Lee et al.(2022)의 부족, 특히 소셜 미디어 소스로부터의 실제 인간-인간 대화를 활용할 때 발생하는 어려움에 직면한다.\n' +
      '\n' +
      '이러한 과제를 해결하기 위해, 우리는 _generative_ 기반 멀티모달 대화 생성 프레임워크인 **MAGID**를 제안한다. 그림 1에서 알 수 있듯이 MAGID는 (i) 이미지를 추가하여 강화할 수 있는 가장 적합한 발화를 찾는 방법과 (ii) 유해하고 사적인 내용이 없는 사실적이고 다양한 이미지를 생성하는 방법이라는 두 가지 연구 과제를 해결함으로써 기존의 텍스트 전용 데이터를 컨텍스트가 풍부한 멀티모달 데이터로 변환하는 것을 목표로 한다.\n' +
      '\n' +
      '전자의 경우, 우리는 이미지를 필요로 하는 발화를 정확히 찾아내고 후속적으로 대응하는 이미지 설명을 생성하도록 설계된 _LLM 기반 스캐너_를 도입하여 사고 연쇄 프롬프트를 활용한다. 후자의 경우, 우리는 생성된 이미지 설명을 입력으로 사용하여 눈에 띄는 다양성을 가진 이미지를 만드는 데 적합한 _확산 기반 이미지 생성기_를 사용한다. 또한, 생성된 이미지의 일치성과 품질을 보장하기 위해 _quality assurance_ 모듈이 프레임워크에 통합되어 멀티모달 대화 내에서 일관성과 충실성을 보존한다. 생성된 이미지가 이 모듈의 기준을 만족하지 못하는 경우, MAGID는 피드백 루프를 시작하여 프롬프트 및 이미지 생성 프로세스를 다시 검토한다.\n' +
      '\n' +
      '다중 모달 데이터 세트를 선별하기 위한 이미지 검색 기법(Lee et al., 2021, 2022) -**a 방법은 제한된 이미지 다양성과 대화 기존 발화와의 잠재적인 불일치를 초래할 수 있는 방법 - 생성 모델 Stable Diffusion XL (Podell et al., 2023)을 사용한다. 수십억 개의 이미지(Schuhmann et al., 2022)에 대해 트레이닝함으로써, 이 접근법은 풍부하고 다양한 출력을 보장한다. 이러한 출력은 LLM 피드백에 의해 제공되는 대화 컨텍스트와 잘 정렬되어 다중 모달 데이터 세트의 품질과 다양성을 높인다.\n' +
      '\n' +
      '이 프레임워크는 텍스트 전용 데이터셋(Lee et al., 2021, 2022)을 사용한 선행 연구와 일치하지만, 생성 기반 데이터 생성 방법을 사용하여 검색 기반 전략과 관련된 한계를 해결한다. Liu et al. (2023); Lee et al. (2021)과 달리, 우리는 대화당 하나의 이미지만을 포함하는 것을 제한하지 않는다. 결과적으로, MAGID는 합성적이지만 더 현실적인 다중 모달 대화 데이터 세트를 생성하여 데이터 접근성 문제를 완화하고 고급 다중 모달 모델의 개발을 촉진한다.\n' +
      '\n' +
      '요약하자면, 우리의 주요 기여는 다음과 같습니다.\n' +
      '\n' +
      '* 검색 기반 접근법의 한계를 해결하는 생성 기반 다중 모달 대화 데이터 생성 프레임워크인 MAGID를 제시한다.\n' +
      '* LLM 기반 스캐너와 확산 기반 이미지 생성기 간의 상호 작용을 최적화하기 위해 다양한 프롬프트 엔지니어링 전략을 사용하여 실험을 수행한다.\n' +
      '* 생성 모델의 성능을 효과적으로 제어하기 위한 새로운 품질 보증 설계를 제안한다.\n' +
      '* MAGID 파이프라인의 효과를 보여주기 위한 개념의 증명으로서 중간 크기의 데이터셋을 제공한다(섹션 5).\n' +
      '* 데이터셋에 대한 광범위한 인간 평가를 수행하고 견고성과 신뢰성을 보장하기 위해 다중 LLM 모델을 테스트한다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '### Generative Models\n' +
      '\n' +
      '최근 Generative AI의 발전은 기존 딥러닝 모델의 역량 확장에 새로운 트렌드를 시작했다. NLP에서 (Radford et al., 2019; Ouyang et al., 2022)와 같은 작업은 더 나은 LLM 모델을 구축하기 위한 훈련 데이터의 중요성을 보여주었다. 이와 관련하여, 최근 LLM 모델들은 Falcon-40b-Instruct (Penedo et al., 2023), Koala 13b (Geng et al., 2023), LLaMA 13b (Touvron et al., 2023),\n' +
      '\n' +
      '도 1: MAGID 프레임워크의 개요. MAGID는 (1) 이미지를 증강하기 위한 적절한 발화를 식별하기 위한 LLM 기반 스캐너, (2) 사실적인 이미지를 생성하기 위한 확산 기반 이미지 생성기, (3) 이미지 품질, 심미성 및 안전 점수를 향상시키기 위한 품질 보증 모듈의 세 가지 구성 요소로 구성된다. 텍스트 전용 대화는 MAGID를 이용하여 멀티모달 대화로 자동 변환된다.\n' +
      '\n' +
      'OpenLLaMA(Touvron et al., 2023), 및 Vicuna 13b(Chiang et al., 2023)는 더 높은 성능을 달성하기 위해 더 나은 큐레이팅된 트레이닝 데이터 세트를 사용한다. 이와 관련하여, 크리스티아누 등(2017)과 같은 논문은 더 빠른 훈련에서 (인간 피드백으로부터) 더 높은 품질의 데이터를 사용하는 것의 극적인 영향을 보여주었다. 그러나 인간의 피드백과 크라우드 소싱을 사용하는 것이 항상 저렴한 것은 아닙니다. 이를 해결하기 위해 (Veselovsky et al., 2023; Kamalloo et al., 2023)과 같은 새로운 작업은 LLM이 인간 생성 데이터세트의 작업을 수행하는 능력을 가지고 있음을 시사한다. 또한, 컴퓨터 비전에서의 확산 모델은 실제와 구별할 수 없는 이미지를 생성하는데 유망한 결과를 보여주었다(Podell et al., 2023; Ho et al., 2020). 마지막으로, 최근 연구는 GPT-4(OpenAI, 2023), LLaVA(Liu et al., 2023b), AnyMAL(Moon et al., 2023) 등 모든 모달리티를 지원하는 멀티모달 LLM 모델을 구축하는 것에 초점을 맞추고 있다. 구체적으로, LLaVA는 이미지 및 텍스트 임베딩을 결합하여 텍스트 전용 출력을 생성하는 멀티모달 입력을 수용한다.\n' +
      '\n' +
      '### 멀티모달 데이터세트 생성\n' +
      '\n' +
      '또한 멀티 모달리티 데이터 세트를 생성하는 데 중점을 둔 작업도 있습니다. 특히, MMDD(Lee et al., 2021) 및 DialogCC(Lee et al., 2022)는 텍스트 전용 데이터 세트를 다중 모달 데이터 세트에 증강하기 위해 이미지 검색 접근법을 사용한다. PhotoChat(Zang et al., 2021)은 데이터 세트를 구축하기 위해 특정 이미지를 논의하기 위해 작업자를 고용한다. MMDialog(Feng et al., 2022)는 인터넷으로부터 다중 모달 대화를 수집하여 훈련 세트로 사용하기 위해 잠재적으로 프라이버시 문제를 제기할 수 있는 데이터 세트를 구축한다. 영상과 음성을 포함한 텍스트와 이미지를 넘어 모달리티에 초점을 맞춘 작품(Wang et al., 2023; Corona et al., 2021, 2020; Ciliberto et al., 2021; Abdrakhmanova et al., 2021)도 있다. 예를 들어, 코로나 등(2021)은 활동 검출을 위한 비디오들을 포함하는 데이터세트를 제공한다. IntenVid(Wang et al., 2023)는 텍스트 외에 비디오를 포함하는 또 다른 예이다.\n' +
      '\n' +
      '## 3 MAGID 파이프라인\n' +
      '\n' +
      '텍스트 전용 대화에서 멀티모달 대화로의 전환에는 두 가지 핵심 과제가 존재한다. 첫 번째는 이미지에 의해 강화될 수 있는 대화 내에서 가장 적합한 발화의 식별이다. 두 번째는 선택된 발화와 정렬되는 대응하는 정확한 이미지의 생성이다. 이와 관련하여, 우리는 이미지와 텍스트 사이의 조화롭고 일관된 일치를 보장하여 수용 가능한 이미지-텍스트 정렬을 달성할 필요가 있다.\n' +
      '\n' +
      '우리는 그림 1의 다음 세 가지 핵심 모듈, 즉 LLM 기반 스캐너, 확산 기반 이미지 생성기 및 품질 보증 모듈의 구현을 통해 이러한 문제를 해결했으며, 이는 후속 섹션에 자세히 설명되어 있다.\n' +
      '\n' +
      '### MAGID Scanner\n' +
      '\n' +
      '이 모듈의 주요 목적은 이미지에 의해 시각적으로 표현될 수 있는 적합한 발화를 식별하는 것이다. 최상의 성능을 달성하려면 LLM 모델의 동작에 대한 정밀한 제어가 필요하다. LLM의 출력을 제어하기 위해 신속한 엔지니어링과 특수 형식을 사용합니다.\n' +
      '\n' +
      'LLM의 시스템 프롬프트를 미세 조정하기 위한 세 가지 프롬프트 엔지니어링 전략을 실험했다.\n' +
      '\n' +
      '**** 제로샷 프롬프트:** LLM은 일반적인 문제 설명과 함께 입력 및 예상 출력의 형식만 제공된다. 도 2는 제로-샷 프롬프트의 예를 도시한다.\n' +
      '\n' +
      '그림 2: 이미지로 증강하기 위해 대화에서 회전을 선택하고 해당 이미지에 대한 설명을 생성하는 스캐너 모듈(섹션 3.1)의 제로샷 프롬프트. 추가적인 소수의 샷 및 생각 사슬 프롬프트가 보충 재료(섹션 A)에 제공된다.\n' +
      '\n' +
      '** **Few-shot 예제 프롬프트:** 제로-shot 프롬프트에서 제공된 정보 외에, LLM은 또한 LLM 모델 Brown 등(2020)으로부터 예상되는 응답을 입증하기 위해 여러 입력-출력 예제를 공급받는다. 우리는 이러한 유형의 프롬프트를 보충 자료에 포함시켰다(항 A).\n' +
      '* **Thought prompting:** Wei et al. (2022)에 따라, 이러한 prompting 전략은 각각의 예에 대한 일련의 중간 추론 단계를 부여하는 것을 포함하며, 이는 보다 진보된 추론을 위한 LLM 모델의 용량을 용이하게 한다. 이 프롬프트(항 A)의 예를 들어 보충 자료를 참조하십시오.\n' +
      '\n' +
      '섹션 4.3.1에서 이러한 프롬프트 전략을 평가했다. 연구 결과를 바탕으로 MAGID 프레임워크의 최적 선택으로 사고 사슬을 선택했다.\n' +
      '\n' +
      'LLM 출력 포맷 제어\n' +
      '\n' +
      '본 논문에서는 HTML과 같은 태그를 사용하여 LLMs 출력의 구조화를 간소화하고, 보다 쉬운 구문 분석을 용이하게 하고 의사 결정 과정을 조명하는 방법을 소개한다. <\\(<\\text{result}>\\) 및 <\\(<\\text{reason}>\\) 태그의 활용은 각각 답안 및 논리를 포락하기 위한 것으로, 잠재적으로 후처리가 보다 간단해지고 모델의 추론에 투명성을 제공하여 디버깅 목적에 도움이 될 수 있다.\n' +
      '\n' +
      '그림 3은 제안된 HTML 형식을 사고 프롬프트의 체인 내부에서 사용하는 것의 영향을 보여주며, 반응에 대한 세심한 분석이 코너 사례를 식별하고 생성된 이미지에서 맥락적 일치를 보장하는 방법을 보여준다. 첫 번째 이미지가 이전 텍스트와 정렬되는 반면, 두 번째 이미지는 컨텍스트가 부족합니다. <\\(\\text{reason}>\\) 태그는 "Look it a look"과 같은 문구가 이미지 생성에 영향을 미친다는 것을 보여준다. 상황적 관련성 및 모델 신뢰성을 향상시키기 위해, 시스템 프롬프트는 LLM에 상세한 설명과 페어링될 때만 이미지를 생성하도록 지시하여 상황적 불일치를 방지하도록 개선되었다.\n' +
      '\n' +
      '#### MAGID 이미지 생성기\n' +
      '\n' +
      '그림 1과 같이 LLM 모델의 이미지 프롬프트는 확산 모델에 의해 해당 이미지를 생성하는 데 사용된다. 이와 관련하여, 우수한 이미지 생성 Rombach et al.(2022); Ho et al.(2020)이 GANs Goodfellow et al.(2014)보다 선택되었다. 테스트된 모델에는 SDXl 1.0, SDXL 0.9 및 Stable Diffusion version from Stability AI Podell et al.(2023)이 포함되었으며, 보충 재료(섹션 C)에서 상세한 비교가 이루어졌다.\n' +
      '\n' +
      '궁극적으로 SDXl 1.0은 MAGID 데이터 세트의 생성된 이미지의 품질과 신뢰성을 강화하면서 최첨단 기능을 위해 선택되었다. 그럼에도 불구하고, 향후 모델 개발은 MAGID 데이터 세트 생성을 개선하기 위해 통합될 수 있다.\n' +
      '\n' +
      '그림 3: MAGID의 사고 유발 사슬은 SDXL 1.0 확산 모델과 GPT-4 OpenAI(2023)를 활용하여 코너 사례의 디버깅 및 식별을 용이하게 한다. 묘사된 대화는 MMDialog 데이터세트 Feng 등(2022)에서 실제 인간-인간 상호작용으로부터 조달된다.\n' +
      '\n' +
      'MAGID 품질보증\n' +
      '\n' +
      'QA(Quality Assurance) 모듈은 MAGID 파이프라인의 효율성을 향상시키기 위해 필수적이다. 생성된 이미지는 **Image-Text Matching**, **Image Quality** 및 **Image Safety**의 세 가지 도메인에서 사용자 설정 표준을 만족하도록 보장한다.\n' +
      '\n' +
      '**1 - 이미지-텍스트 매칭**: 우리는 CLIP 스코어(Radford et al., 2021)를 사용하여 이미지와 LLM 모델의 발화 사이의 매칭을 검증한다. 낮은 CLIP 점수는 카운트 값을 하이퍼파라미터로 결정하여 이미지 재생을 트리거한다. 이 작업에서 재생 횟수를 2로 설정했습니다.\n' +
      '\n' +
      '**2- 이미지 품질**: 이미지는 CLIP 임베딩에 이어 MLP를 사용하는 (Schuhmann et al., 2022; Schuhmann, 2023)로부터의 미적 점수에 기초하여 평가된다. 이 모델은 확산 모델 출력의 아티팩트를 식별합니다. 0.51의 임계값은 대부분의 아티팩트를 효율적으로 감지하여 이 미만의 점수에 대한 이미지 재생성을 촉발한다.\n' +
      '\n' +
      '**3 - 이미지 안전**: 특히 NSFW 콘텐츠에 대한 이미지 안전이 중요합니다. 많은 모델이 이를 평가하지만 데이터 세트에서 안전하지 않은 이미지가 거의 발견되지 않아 프로세스의 신뢰성을 나타낸다.\n' +
      '\n' +
      '이 강력한 QA는 MAGID가 관련되고 고품질이며 안전한 이미지를 출력할 수 있음을 보장합니다.\n' +
      '\n' +
      '###### 3.4.1 피드백 루프\n' +
      '\n' +
      '확산 모델이 품질 보증 모듈의 조건에 맞지 않는 이미지를 생성하면 문제는 LLM 모델의 프롬프트에서 비롯될 수 있다. 잘못된 프롬프트는 낮은 이미지 텍스트 일치 또는 안전하지 않은 이미지를 생성할 수 있습니다. 이를 완화하기 위해 그림 1에 제시된 우리의 설계는 피드백 루프를 포함하여 LLM 모델에 이전 이미지 설명과 함께 재생성된 이미지가 제공되는 더 나은 이미지 설명을 생성하도록 지시하여 품질 보증 표준에 지속적으로 미치지 못한다.\n' +
      '\n' +
      '도 4는 MMDD(Lee et al., 2021) 및 PhotoChat(Zang et al., 2021)의 두 개의 다른 데이터 세트와 MAGID 샘플의 비교를 나타낸다. 정성적 분석은 MAGID가 PhotoChat과 같은 실제 데이터 세트에 필적하는 품질을 산출하고 고품질 멀티모달 데이터 세트를 생성하는 데 있어 MMDD와 같은 합성 데이터 세트를 능가한다는 것을 보여준다. 더 많은 예들이 보충(섹션 H)에 포함된다.\n' +
      '\n' +
      '## 4 Evaluation\n' +
      '\n' +
      '우리는 MAGID에 의해 생성된 다중 모드 데이터 세트의 효능과 적용 가능성을 면밀히 조사한다. 여기 우리가 평가에서 다루었던 세 가지 중요한 질문들이 있다\n' +
      '\n' +
      '1. MAGID가 실제 멀티모달 데이터세트와 정량적으로 어떻게 비교하는가? \\ (\\rhd\\) Section 4.1\n' +
      '2. MAGID가 실제와 같이 사람의 눈을 인지할 수 있는 품질을 가진 멀티모달 데이터세트를 생성할 수 있는가?\\ (\\rhd\\) Section 4.2\n' +
      '3. 스캐너 프롬프트 튜닝 및 품질 보증 모듈이 MAGID에 미치는 영향은? \\ (\\rhd\\) Section 4.3\n' +
      '\n' +
      '첫 번째와 세 번째 질문은 MAGID에 의해 생성된 데이터의 정확성과 품질을 조사하는 정량적 분석을 조사한다. 또한, 두 번째 질문은 MAGID가 인간 평가 표준을 충족하지 못하면 긍정적인 인간 중심 평가를 얻을 수 없는 낮은 품질의 훈련 데이터 세트가 발생하기 때문에 중요하다.\n' +
      '\n' +
      '또한, 보충(항 E)에서는 멀티모달 모델 훈련에 대해 연구했다.\n' +
      '\n' +
      '그림 4: 이미지 검색 기반 합성 MMDD 및 실제 인간 이미지 기반 포토챗 데이터세트와 MAGID의 질적 비교.\n' +
      '\n' +
      'MAGID를 사용하여 실제 이미지를 사용하여 훈련하는 것과 비교했다.\n' +
      '\n' +
      '### Quantitative Evaluation\n' +
      '\n' +
      'Setup.Addressing the first question, multi-dimensional evaluation was evaluated image quality and accuracy of MAGID in selection right utterance. MAGID의 일반 사용 적용 가능성을 공정하게 비교하기 위해 LLM 모델을 유도하여 올바른 발화를 선택하는 프롬프트 엔지니어링만 활용했다. 이와 관련하여, 그라운드 트루스로서 인간-인간 상호작용 데이터 세트 MMDialog 및 PhotoChat을 선택하고 테스트 세트에서 이미지를 제거하고 MAGID를 사용하여 텍스트 전용 데이터를 다중 모달 데이터 세트로 변환했다.\n' +
      '\n' +
      'LLM 기반 모델의 경우 GPT-4(OpenAI, 2023), GPT-3.5(OpenAI, 2023), Falcon-40b-Instruct(Penedo et al., 2023), Koala 13b(Geng et al., 2023), LLaMA 13b(Touvron et al., 2023), OpenLLaMA(Touvron et al., 2023), Vicuna 13b(Chiang et al., 2023) 등 다양한 모델을 채택하였다. 이미지 생성을 위해 SDXL 1.0은 모든 모델에서 일관되게 활용되었다. 여기에서 MMDialog 데이터 세트의 결과를 제시하고 PhotoChat 결과는 보충(섹션 B)에 포함된다. 이 실험에서 CLIP 모델의 임계값은 0.21, 심미적 점수 임계값은 0.51로 설정하였으며, 이러한 하이퍼 파라미터를 찾기 위해 그리드 검색을 사용하였다. 계산 비용에 대한 더 자세한 내용은 보충(섹션 F)에 제공된다.\n' +
      '\n' +
      '결과.표 1은 MMDialog 데이터셋에 대한 다양한 LLM 모델의 성능을 제시한다. 표는 MMDialog 데이터세트와 비교하여 서로 다른 LLM 모델을 사용하여 MAGID의 응답 생성을 정량화한다. 첫 번째 열은 사용된 LLM 모델을 나열하는 반면, 후속 4개의 열은 이미지로 증강될 올바른 발화를 선택하는 데 정확성, 정밀도, 재현율 및 F1 점수를 측정한다. CLIP score gauges image-text matching, 그리고 (Feng et al., 2022)에 소개된 MM-Relevance는 응답들 간의 유사성을 나타낸다. 우리의 맥락에서, 그것은 생산된 이미지와 MMDialog의 원본 이미지의 유사성을 결정한다. 다음 열인 미적 점수는 (슈만, 2023)에서 논의된 바와 같은 이미지 품질을 나타낸다. 마지막 행은 CLIP 점수, 이미지 카운트 및 이미지의 미적 품질을 강조하는 지상 진실 데이터 세트를 제시한다.\n' +
      '\n' +
      '표에서 GPT-4 및 GPT-3.5가 모든 메트릭에서 다른 모델보다 성능이 우수함을 알 수 있다. 특히, GPT-4와 GPT-3.5를 이용한 MAGID의 CLIP와 심미성 점수는 지상진리값도 능가한다. 다음 섹션에서는 또한 MAGI가 정량적 결과와 일치하는지 테스트하기 위해 다른 데이터 세트에 대해 MAGI에 대한 인간 평가에서 이미지-텍스트 매칭 및 이미지 품질을 검사한다.\n' +
      '\n' +
      '### Human Evaluation\n' +
      '\n' +
      '설립.설문지와 함께 웹 사이트를 사용하여 인간 평가를 수행했다. 참가자들은 MAGID의 이미지가 있는 대화와 데이터 세트 MMDD(Lee et al., 2021), PhotoChat(Zang et al., 2021) 또는 MMDialog(Feng et al., 2022)의 두 가지 대화를 보았다. MAGID는 GPT-4를 언어 모델로, SDXL 1.0을 이미지 생성에 사용하였다. 언급된 데이터 세트에서 각각 20개의 대화 상자를 선택하고 총 60개의 대화 상자를 선택하여 이미지를 MAGID로 대체했으며 평가 동안 참가자들은 MAGID의 다중 모달 대화 상자를 정보 없이 원본과 비교했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c} \\hline \\hline Model & Accuracy & Precision & Recall & F1 score & CLIP score & MM-Relevance & Aesthetic & \\#images \\\\ \\hline GPT 4 & **67.24\\%** & **70.49\\%** & **46.87\\%** & **0.56** & **0.27** & **294.52** & 0.57 & 1359 \\\\ GPT 3.5 & 63.54\\% & 69.43\\% & 33.97\\% & 0.46 & 0.26 & 293.51 & **0.58** & 1001 \\\\ Falcon-40b-Ins. & 58.93\\% & 61.26\\% & 24.13\\% & 0.35 & 0.25 & 254.50 & **0.58** & 794 \\\\ Koala 13b & 56.28\\% & 62.33\\% & 6.91\\% & 0.12 & 0.25 & 243.31 & 0.57 & 223 \\\\ Llama 13b & 57.10\\% & 60.00\\% & 13.64\\% & 0.22 & 0.25 & 247.99 & 0.57 & 460 \\\\ OpenLLaMA & 57.94\\% & 64.36\\% & 12.69\\% & 0.21 & 0.25 & 250.96 & **0.58** & 390 \\\\ Vicuna 13b & 58.77\\% & 66.60\\% & 14.38\\% & 0.24 & 0.26 & 255.18 & 0.57 & 506 \\\\ MMDialogue3 & N/A & N/A & N/A & N/A & 0.262 & N/A & 0.47 & 2717 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 이미지 증강(정확도, 정밀도, 재현율, F1)을 위한 턴 선택에 의해 측정된 바와 같은 스캐너 모듈 성능 및 MMDialog 데이터세트 상의 생성된 설명(CLIP, MM-관련성, 심미성)으로부터의 결과 이미지들은 지상-진실로서 MMDialog 데이터세트 상에 존재한다. 품질 보증 모듈은 **enabled**입니다. 우리는 일련의 사고 프롬프트를 사용하여 스캐너 모듈에 전원을 공급하는 다양한 LLM을 비교한다.\n' +
      '\n' +
      '대화의 기원에 대한 설명.\n' +
      '\n' +
      '각 대화 쌍(MAGID에서 하나, 벤치마크 데이터 세트에서 하나)에 대해 참가자는 다음 프롬프트에 응답했다.\n' +
      '\n' +
      '1. [label=Q0]\n' +
      '2. 어떤 대화가 더 사실적으로 보이는가?\n' +
      '3. 어떤 대화의 이미지가 더 큰 지식을 전달하는가?\n' +
      '4. 어떤 대화에서 이미지와 바로 앞의 텍스트 간에 더 잘 일치합니까?\n' +
      '5. 어떤 대화에서 이미지가 전체 대화 맥락과 더 밀접하게 일치합니까?\n' +
      '6. 어떤 대화가 더 매력적이죠?\n' +
      '7. 어떤 대화가 더 높은 품질의 이미지를 특징으로 하는가?\n' +
      '\n' +
      '각 프롬프트에 대한 이진 선택(대화 A 또는 대화 B)에서 선택한 응답자.\n' +
      '\n' +
      '이 평가를 위해 15명의 인간 주석자가 답변을 제공했다. 웹사이트 인터페이스의 스키마는 보충 자료(섹션 D)에서 사용할 수 있습니다.\n' +
      '\n' +
      '결과.표 2는 MMDD, MMDialog 및 PhotoChat 데이터 세트에 대한 MAGID의 결과를 표시한다. \'Mean MAGID\' 열은 MAGID를 선호하는 주석자의 백분율을 나타내는 반면 \'Mean Other\'는 대체 데이터 세트를 선호하는 주석자를 나타낸다. 마지막 열에서 발견된 Gwet의 AC1 측정은 주석자 간 신뢰도를 평가하는 데 사용되었다. 그것은 코헨의 카파[20]에 대한 안정성을 제공하며 이상치에 더 탄력적이다(자세한 설명은 보충 자료 섹션 G 참조).\n' +
      '\n' +
      '표 2(a)에서 주석자는 모든 질문 범주에서 합성적으로 생성된 MMDD 데이터 세트보다 MAGID를 선호한다는 것이 분명하다. 또한, 그웨트의 AC1 값이 높다는 것은 MMDD보다 MAGID를 선택하는 데 있어 주석자 간의 강력한 합의를 나타낸다. 대조적으로, 표 2(b)를 검토할 때 주석자는 사실성 측면에서 진정한 MMDialog 데이터 세트에 대해 약간의 선호도를 나타냈다. 특히, 그웨트의 AC1 값은 MMDD 결과보다 여기에서 상당히 낮아 주석자 간의 합의가 감소했음을 시사한다. 그럼에도 불구하고, MAGID는 이미지 품질 및 이미지-텍스트 매칭 측면에서 MMDialog를 능가했다. **이러한 발견은 우리의 정량적 평가를 긍정하고 교육을 위한 우수한 데이터 소스를 생산하는 데 있어 생성 AI의 잠재력을 보여준다.** 포토챗 데이터 세트의 경우(표 2(c))는 진정한 인간 상호 작용으로 구성되지만 인간 참가자는 실제 대화를 모의하도록 지시받았다. 흥미롭게도 우리의 주석자는 포토챗보다 MAGID 쪽으로 약간 기울었다. 이 결과는 다중 모달 데이터 세트 개발에서 MAGID가 기계 투르크에 대한 대안으로 작용할 수 있는 유망한 능력을 시사한다.\n' +
      '\n' +
      'MAGID의### 절제에 관한 연구\n' +
      '\n' +
      '발화 식별을 위한 다른 프롬프트를 사용하여 (1)에 대한 절제 연구를 수행했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c c|c c c} \\hline \\hline \\multicolumn{6}{c}{(a) MAGID vs. MMDD} & \\multicolumn{3}{c}{(b) MAGID vs. MMDialogue} & \\multicolumn{3}{c}{(c) MAGID vs. PhotoChat} \\\\ \\hline \\multirow{2}{*}{\\#} & Mean & Mean & Gwet’s & Mean & Mean & Gwet’s & Mean & Mean & Gwet’s \\\\  & MAGID & MMDD & AC1 & MAGID & MMDial. & AC1 & MAGID & Photo. & AC1 \\\\ \\hline Q1 & **96.29**\\% & 3.71\\% & 0.74 & 48.17\\% & **51.83**\\% & 0.63 & **58.11**\\% & 41.89\\% & 0.47 \\\\ Q2 & **96.29**\\% & 3.71\\% & 0.89 & 49.33\\% & **50.67**\\% & 0.65 & **68.24**\\% & 31.76\\% & 0.71 \\\\ Q3 & **89.11**\\% & 10.89\\% & 0.75 & **52.72**\\% & 47.28\\% & 0.54 & **64.90**\\% & 35.10\\% & 0.53 \\\\ Q4 & **91.11**\\% & 8.89\\% & 0.83 & 46.31\\% & **53.69**\\% & 0.65 & **61.98**\\% & 38.02\\% & 0.54 \\\\ Q5 & **95.57**\\% & 4.43\\% & 0.89 & **51.94**\\% & 48.06\\% & 0.63 & **64.02**\\% & 35.98\\% & 0.61 \\\\ Q6 & **80.92**\\% & 19.08\\% & 0.65 & **63.90**\\% & 36.10\\% & 0.55 & **69.99**\\% & 30.01\\% & 0.64 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: MAGID 생성 데이터세트 대 검색 기반 합성 데이터세트인 MMDD 및 두 개의 실제 데이터세트인 MMDialouge 및 PhotoChat의 인간 평가 결과이며, 여기서 평균은 참가자 중 한 데이터세트에서 대화 상대가 선호된 시간의 백분율을 보여준다. (Q1: 보다 사실적인 대화? Q2: 어떤 대화가 더 많은 지식을 제공하는 이미지?, Q3: 더 나은 텍스트-이미지 매치?, Q4: 더 나은 컨텍스트-이미지 매치?, Q5: 더 매력적인?, Q6: 더 높은 이미지 품질?)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline Prompt & Accuracy & Precision & Recall & F1 \\\\  & & & score \\\\ \\hline ZS & 65.53\\% & 73.12\\% & 36.16\\% & 0.48 \\\\ FS & 63.89\\% & 69.67\\% & 34.45\\% & 0.46 \\\\ CoT & **68.51**\\% & **73.37**\\% & **47.32**\\% & **0.57** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: MMDialogue(ground-truth)에 대한 세 가지 다른 프롬프트를 사용한 외래 선택 정확도이며, 여기서 ZS, FS 및 CoT는 각각 제로 샷, 소수 샷 및 사상 사슬을 나타낸다.\n' +
      '\n' +
      '그리고 (2) 품질 보증(QA) 모듈의 영향을 조사합니다.\n' +
      '\n' +
      '######4.3.1 스캐너용 프롬프트\n' +
      '\n' +
      '표 3은 MAGID에 대한 GPT-3.5 모델에 적용된 Zero-shot(ZS) 프롬프트, Few-shot 프롬프트(FS) 및 Chain of Thought(CoT) 프롬프트의 세 가지 프롬프트 전략의 결과를 보여준다. 이러한 결과는 품질 보증이 비활성화된 MMDialog 데이터 세트에 대해 보고되어 LLM 모델의 정확도를 단독으로 측정한다. 특히, 사상 사슬 전략은 평가된 모든 메트릭에서 다른 두 가지 전략보다 우수하다.\n' +
      '\n' +
      'QA 모듈의 영향 4.3.2\n' +
      '\n' +
      '표 4는 QA 모듈이 활성화되거나 비활성화되었을 때와 대조적으로 MAGID에서 4개의 LLM 모델의 성능을 보여준다. 표 4를 살펴보면 QA가 비활성화될 때 모든 모델에서 미적 점수, MM-관련성 및 CLIP 점수가 감소하는 것으로 나타났다. 또한, 대부분의 모델의 정밀도에서 눈에 띄는 감소를 관찰할 수 있으며, QA 모듈이 이미지 생성을 위한 최적의 발화를 정확하게 찾아냄으로써 정밀도를 향상시킴으로써 MAGID를 강화한다는 것을 검증한다. 대조적으로, QA를 비활성화하는 것은 MAGID가 이미지 생성을 위해 더 광범위한 발화의 배열을 선택하여 위음의 비율을 감소시키기 때문에 리콜의 상승으로 이어진다. 향후 연구에서는 전체 파이프라인에 대한 회수율을 높일 수 있는 정제된 QA 모듈의 개발을 탐색할 수 있다.\n' +
      '\n' +
      '## 5 MAGID 데이터세트\n' +
      '\n' +
      '개념의 증명으로서, (Lee et al., 2021)과 같은 연구와 일치하게, DailyDialog(Li et al., 2017), Persona-Chat(Zhang et al., 2018), PhotoChat(Zang et al., 2021)과 같은 텍스트 전용 데이터 세트를 사용하여 53,620 대화의 다중 모달 데이터 세트 4를 생성했다. 실험 결과를 바탕으로 GPT-3.5를 사용하여 47,868개의 입력 대화 상자를 변환하고 나머지는 GPT-4를 확장했다. 표 5는 MAGID로 생성된 데이터셋의 통계를 보여준다. 데이터 및 코드는 수락 시 일반인이 사용할 수 있도록 합니다.\n' +
      '\n' +
      '각주 4: 데이터세트에 대한 링크: [http://anon_for_review.com](http://anon_for_review.com)\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '우리는 신속한 엔지니어링을 통해 LLM의 힘을 활용하여 텍스트 전용 데이터 세트를 다중 모드 변형으로 변환하도록 설계된 생성적이고 완전 자동화된 파이프라인을 제시했다. 이 솔루션은 특히 데이터 프라이버시, 접근성, 제약된 이미지 배포 및 부적합하거나 비동의적인 콘텐츠의 발생 측면에서 선행 방법이 직면한 한계를 해결한다. 결정적으로, 우리의 파이프라인은 실제 잠재적으로 프라이버시 타협 이미지를 합성 대응물로 대체할 수 있게 한다. 우리는 인간 평가, 다양한 LLM을 사용한 정량적 분석 및 심층 절제 연구를 사용하여 다중 모드 데이터 생성 방법을 철저히 평가했다. 유망한 결과는 기계식 터크와 같은 전통적인 데이터 생성 방법의 대안으로 설 수 있는 생성 AI의 능력을 강조한다.\n' +
      '\n' +
      '앞을 내다보면, 우리의 데이터 세트는 텍스트와 비주얼을 통해 사용자와 참여할 수 있는 대규모 멀티모달 언어 모델을 개발하는 길을 열어준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c} \\hline \\hline Model & Accuracy & Precision & Recall & F1 score & CLIP score & MM-Relevance & Aesthetic & \\#images \\\\ \\hline GPT 4 & **67.24\\%** & **70.49\\%** & **46.87\\%** & **0.56** & **0.27** & **294.52** & 0.57 & 1359 \\\\ GPT 3.5 & 63.54\\% & 69.43\\% & 33.97\\% & 0.46 & 0.26 & 293.51 & **0.58** & 1001 \\\\ Falcon-40b-Ins. & 58.93\\% & 61.26\\% & 24.13\\% & 0.35 & 0.25 & 254.50 & 0.58 & 794 \\\\ OpenLLaMA & 57.94\\% & 64.36\\% & 12.69\\% & 0.21 & 0.25 & 250.96 & **0.58** & 390 \\\\ \\hline \\hline GPT 4 & 67.86\\% & 69.70\\% & **50.64\\%** & **0.59** & **0.27** & **282.25** & 0.55 & 1485 \\\\ GPT 3.5 & **68.51\\%** & **73.37\\%** & 47.32\\% & 0.57 & 0.26 & 278.16 & 0.55 & 1109 \\\\ Falcon-40b-Ins. & 56.77\\% & 53.58\\% & 28.80\\% & 0.37 & 0.23 & 224.59 & 0.55 & 1075 \\\\ OpenLLaMA & 58.92\\% & 62.50\\% & 21.51\\% & 0.32 & 0.21 & 213.56 & **0.56** & 696 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 품질 보증(QA) 모듈이 있거나 없는 MAGID 프레임워크의 절제 결과. MMDialog(ground-truth)에서 4개의 LLM에 대한 턴 선택 및 화질 성능에 대한 결과를 보여준다. 처음 네 행은 QA 모듈이 있는 결과이고 마지막 네 행은 없는 결과입니다. 시스템 프롬프트는 일련의 생각입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c} \\hline \\hline Category & Train & Test \\\\ \\hline Total dialogues & 47643 & 5977 \\\\ Avg length of dialogues & 11.76 & 11.36 \\\\ Avg length of sentences & 9.77 & 9.60 \\\\ Total images & 67951 & 10229 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: MAGID 데이터셋의 통계량.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '본 논문은 LLM과 확산 모델을 사용하여 다중 모달 데이터 세트 생성의 프라이버시, 다양성 및 품질을 높이는 데 주로 초점을 맞추고 있다. 생성 확산 모델을 사용하면 프라이버시 침해와 관련된 문제를 완화할 수 있지만, 이러한 모델은 또한 광범위한 웹 이미지에 대해 훈련되므로 저작권 침해에 취약하다(Aboutalebi et al., 2023). 이 문제를 해결하는 것은 이 논문의 야심을 초과하고 향후 작업을 위한 강력한 방법을 제시한다.\n' +
      '\n' +
      '더욱이 현재 작업은 이미지와 텍스트 양식을 독점적으로 강조한다. 후속 연구 노력에는 비디오 공유, 음성 공유 등과 같은 추가 양식으로 고려 사항을 확장하는 것이 권장된다. 또한, 이미지 생성을 위한 대규모 언어 모델의 미세 조정은 향후 작업에 맡겨진다.\n' +
      '\n' +
      '대화에서 생성된 이미지 일관성을 향상시키는 것은 MAGID에 의해 생성된 멀티모달 데이터세트의 품질을 더욱 향상시킬 수 있는 또 다른 중요한 측면이다. DALL-E 3(Betker et al., 2023)과 같은 보다 최근의 확산 모델들을 채용하는 것은 이들이 보다 일관된 이미지 생성을 할 수 있기 때문에 이러한 문제를 해결할 수 있다. 이와 관련하여 보충 자료의 J 섹션에서는 제안된 MAGID 파이프라인의 한계를 보여주는 추가 예를 포함했다.\n' +
      '\n' +
      '결론적으로, 우리의 품질 보증 모듈의 향상은 텍스트 전용 입력으로부터 보다 현실적인 다중 모달 데이터 세트를 개발하는 데 중추적이다. 이와 관련하여 (Tian et al., 2023)과 같은 작업은 이미 합성 이미지를 사용하는 것이 효과적임을 보여주었다. 이 작품은 미적 점수, 클립 점수, 안전성과 같은 측면을 우선시합니다. 향후 연구에서는 추가 요소를 탐색하여 멀티모달 산출물로의 변환에 현실성을 더욱 정교화하고 추가할 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Abdrakhmanova et al. (2021) Madina Abdrakhmanova, Askat Kuzdeuov, Sheikh Jarju, Yerbolat Khassanov, Michael Lewis, and Huseyin Atakan Varol. 2021. 스피킹페이스: 시각적 및 열적 비디오 스트림을 갖는 음성 명령들의 대규모 멀티모달 데이터세트. _ Sensors_, 21(10):3465.\n' +
      '* Aboutalebi 등(2023) Hossein Aboutalebi, Daniel Mao, Carol Xu, and Alexander Wong. 2023. Deepfakeart challenge: Generative ai art 위변조 및 데이터 중독 검출을 위한 벤치마크 데이터셋 _ arXiv preprint arXiv:2306.01272_.\n' +
      '* Alayrac et al. (2022) Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lene, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: few-shot learning을 위한 시각적 언어 모델. _ 신경 정보 처리 시스템_, 35:23716-23736에서의 발전.\n' +
      '* Betker et al. (2023) James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. 2023. 개선된 캡션을 갖는 이미지 생성. _ 컴퓨터 과학 https://cdn. openai. com/papers/dall-e-3. pdf_, 2(3):8.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 소수의 학습자를 의미한다. _ 신경 정보 처리 시스템들_, 33:1877-1901의 진보들.\n' +
      '* Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Liaminin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: 90%* chatgpt 품질의 gpt-4를 인상하는 오픈소스 챗봇.\n' +
      '* Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. _ 신경 정보 처리 시스템_, 30의 발전.\n' +
      '* Ciliberto et al. (2021) Mathias Ciliberto, Vitor Fortes Rey, Alberto Calatroni, Paul Lukowicz, and Daniel Roggen. 2021. Opportunity++: 비디오 및 웨어러블, 객체 및 주변 센서 기반 인간 활동 인식을 위한 멀티모달 데이터세트. _ Frontiers in Computer Science_, 3:792065.\n' +
      '* 코로나 등(2020) 켈리 코로나, 케이티 오스터달, 로데릭 콜린스, 앤서니 후그. 2020. Meva: A large-scale multi-view _ 액티비티 검출을 위한 멀티모달 비디오 데이터세트.\n' +
      '* 코로나 등(2021) 켈리 코로나, 케이티 오스터달, 로데릭 콜린스, 앤서니 후그. 2021. Meva: 액티비티 검출을 위한 대규모 멀티뷰, 멀티모달 비디오 데이터세트. In _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pages 1060-1068.\n' +
      '* Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. 이미지는 16x16 단어의 가치가 있다: 이미지 인식을 위한 Transformers for scale. _ arXiv preprint arXiv:2010.11929_.\n' +
      '* Feng et al. (2022) Jiazhan Feng, Qingfeng Sun, Can Xu, Pu Zhao, Yaming Yang, Chongyang Tao, Dongyan Zhao, and Qingwei Lin. 2022. Mmdialog: 멀티모달 오픈 도메인 대화를 향한 대규모 멀티-턴 대화 데이터세트. _ arXiv preprint arXiv:2211.05719_.\n' +
      '* Huang et al. (2020)Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. 코알라: 학술 연구를 위한 대화 모델. _ Blog post, April_, 1.\n' +
      '* Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial net. _ 신경 정보 처리 시스템들_, 27에서의 진보들.\n' +
      '* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. 디노이징 확산 확률 모델. _ 신경 정보 처리 시스템들_, 33:6840-6851에서의 진보들.\n' +
      '* Kamalloo et al. (2023) Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur, and Jimmy Lin. 2023. Hagrid: 속성을 가진 생성적 정보추구를 위한 Human-llm 협업 데이터셋. _ arXiv preprint arXiv:2307.16883_.\n' +
      '* 이민영(2023) 2023. 빌딩 멀티모달 아이 챗봇 arXiv preprint arXiv:2305.03512_.\n' +
      '* Lee et al. (2021) Nyoungwoo Lee, Suwon Shin, Jaegul Choo, Ho-Jin Choi, and Sung-Hyun Myaeng. 2021. 텍스트를 의미적으로 관련된 이미지로 대체하여 멀티모달 대화 데이터셋을 구축하는 단계. _ arXiv preprint arXiv:2107.08685_.\n' +
      '* 이등(2022) 이영준, 고병수, 김한규, 최호진 2022. 다이얼로그cc: 대규모 멀티모달 대화 데이터셋. _ arXiv preprint arXiv:2212.04119_.\n' +
      '* 이등(2023) 이영준, 고병수, 김한규, 종환현, 최호진 2023. 다이얼로그: 고품질 멀티모달 대화 데이터 세트를 생성하기 위한 자동화된 파이프라인. In _NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following_.\n' +
      '* Li 등(2017) Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, Shuzi Niu. 2017. Dailydialog: manually labeled multi-turn dialogue dataset. _ arXiv preprint arXiv:1710.03957_.\n' +
      '* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. 2014. Microsoft coco: context in common objects. _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, Proceedings, Part V 13_, pages 740-755. Springer.\n' +
      '* Liu et al. (2023a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. 시각적 지시 조율 arXiv preprint arXiv:2304.08485_.\n' +
      '* Liu et al. (2023b) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. 시각적 지시 조율\n' +
      '* Liu et al. (2022) Siyang Liu, Sahand Sabour, Yinhe Zheng, Pei Ke, Xiaoyan Zhu, and Minlie Huang. 2022. 별개의 메트릭을 재생각하고 정제하는 단계; _ arXiv preprint arXiv:2202.13587_.\n' +
      '* Liu et al. (2023c) Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, et al. 2023c. Ch chatgpt/gpt-4 연구의 요약과 대형 언어 모델의 미래를 향한 관점. _ arXiv preprint arXiv:2304.01852_.\n' +
      '*Moon et al. (2023) 승완문, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murgesan, Peyman Heidari, Yue Liu, et al. 2023. Angrilent and scalable any-modality augmented language model. _ arXiv preprint arXiv:2309.16058_.\n' +
      '* OpenAI(2023) R OpenAI. 2023. Gpt-4 기술보고서 _ arXiv_, 페이지 2303-08774.\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022, training language models to follow instructions with human feedback. _ 신경 정보 처리 시스템_, 35:27730-27744의 발전.\n' +
      '* Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: 기계 번역의 자동 평가 방법. _Proceedings of the 40th annual meeting for Computational Linguistics_, pages 311-318.\n' +
      '* Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. 매를 위한 정제된 웹 데이터세트 llm: 웹 데이터와 함께 큐레이션된 말뭉치를 능가하고 웹 데이터만을 제공한다. _ arXiv preprint arXiv:2306.01116_.\n' +
      '* Podell et al. (2023) Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. 2023. Sdxl: 고해상도 영상 합성을 위한 잠재 확산 모델 개선 arXiv preprint arXiv:2307.01952_.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. IMT-2000 3GPP-기계학습에 관한 국제학술대회 - 페이지 8748-8763. PMLR.\n' +
      '* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비감독 멀티태스크 학습자들이다. _ OpenAI blog_, 1(8):9.\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. 잠재 확산 모델을 이용한 고해상도 영상 합성. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 회의의 _Proceedings에서, 페이지 10684-10695.\n' +
      '* Schuhmann (2023) Christoph Schuhmann. 2023. improved-aesthetic-predictor. [https://github.com/christophschuhmann/ improved-aesthetic-predictor] (https://github.com/christophschuhmann/ improved-aesthetic-predictor). 깃허브 저장소입니다.\n' +
      '* Schuhmann et al. (2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: 차세대 이미지-텍스트 모델을 훈련시키기 위한 개방형 대규모 데이터세트. _ 신경 정보 처리 시스템_, 35:25278-25294에서의 발전.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '- 질의: > Utterance: 0: 오늘 집에서 아티와 함께 일하기! 오늘 내 인생에는 많은 줌이 있어! 1: 줌 호출을 위해 켜고 끄는 폴로를 편리하게 보관합니다. 매우 화려한 대답이 되는 방법: > <결과> Utterance: 0: atie</결과>로 재택근무\n' +
      '\n' +
      '그림 5: 수-샷 예제 프롬프트는 문제 설명과 함께 입력 및 예상 출력 모두에 대한 포맷을 제공할 뿐만 아니라 LLM으로부터 원하는 응답을 설명하기 위한 다수의 예시들을 포함한다. 여기에는 예제만 포함됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c} \\hline \\hline Model & Accuracy & Precision & Recall & F1 score & CLIP score & MM-Relevance & \\#images & Aesthetic \\\\ \\hline GPT 3.5 & 86.11\\% & **28.62\\%** & 25.91\\% & 0.27 & 0.25 & 313.64 & 87 & 0.57 \\\\ Falcon-40b-Ins. & 88.10\\% & 28.04\\% & 11.83\\% & 0.17 & 0.24 & 303.68 & 403 & **0.58** \\\\ Koala 13b & 89.61\\% & 30.43\\% & 2.94\\% & 0.05 & 0.24 & 283.44 & 92 & 0.61 \\\\ Llama 13b & 87.32\\% & 20.79\\% & 9.54\\% & 0.13 & 0.23 & 244.36 & 433 & 0.59 \\\\ OpenLLaMA & 88.75\\% & 27.31\\% & 8.03\\% & 0.12 & 0.23 & 270.36 & 696 & 0.59 \\\\ Vicuna 13b & 88.40\\% & 25.48\\% & 8.35\\% & 0.13 & 0.24 & 244.97 & 602 & 0.55 \\\\ PhotoChat & N/A & N/A & N/A & N/A & 0.213 & N/A & 961 & 0.49 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: PhotoChat(ground-truth)에 대한 상이한 LLM 모델 테스트. 품질 보증 모듈을 사용할 수 있습니다. 시스템 프롬프트는 일련의 생각입니다.\n' +
      '\n' +
      '그림 6: 소수의 예시 프롬프트에 제공된 시스템 프롬프트를 기반으로 하는 생각의 연쇄 프롬프트는 또한 발화 선택에 대한 상세한 추론을 통합한다.\n' +
      '\n' +
      '텍스트 및 이미지 및 이미지-텍스트 매칭에만 집중합니다. 15명의 주석자가 작업을 완료했으며 각각 20개의 비교를 수행했다.\n' +
      '\n' +
      '## 부록 E 다운스트림 트레이닝\n' +
      '\n' +
      '여기서는 MAGID에 의해 생성된 합성 이미지로 원본 이미지를 변경할 때 MAGID가 멀티모달 모델 학습에 얼마나 영향을 미칠 수 있는지 연구한다. 또한, 훈련에 이미지가 존재하지 않을 때 벤치마크 사례와 MMDD Lee et al.(2021) 접근법과 비교하여 이미지를 대화체에 포함시켰다. 이를 위해 Huggingface Wolf et al. (2019)의 VisionTextDualEncoder인 Lee (2023)에서 제안한 아키텍쳐와 동일한 아키텍쳐를 사용하여 이미지의 인코딩을 텍스트 임베딩과 함께 공유된 공통 공간에 투영하였다. 영상의 인코딩을 위해 ViT Dosovitskiy et al.(2020), 텍스트를 처리하기 위해 사전 훈련된 DialoGPT Zhang et al.(2019)을 사용하였다. 입력은 다중 모드이지만 출력은 텍스트뿐입니다. 이 작업에서 우리는 마지막 텍스트 발화를 생략하며 모델은 이전 이미지와 텍스트가 주어지면 이를 예측해야 한다.\n' +
      '\n' +
      'MMDialog 데이터셋에 대한 모델을 미세 조정하고 그 결과를 표 8에 보고하였으며, 이 실험을 위해 Adam Optimizer와 0.00005의 학습률을 사용하였다. 표 8에서는 학습 세트 이미지가 MMDialogue, MAGID, MMDD에서 나오는 경우 및 이미지가 누락된 경우에 대한 테스트 세트에 대한 결과를 보여준다. MDD의 경우, 비교를 가능하게 하기 위해 텍스트 전용 대사에 이미지를 주입하는 데 사용된 것과 동일한 코드를 사용했다. 이 실험을 위해 훈련 세트는 5156개의 대화로 구성되고 테스트 세트는 MMDialogue 데이터 세트에서 샘플링된 633개의 대화로 구성된다.\n' +
      '\n' +
      '알 수 있는 바와 같이, 소스 이미지를 트레이닝 세트(MMDialog)로 사용할 때, 우리는 가장 높은 BLEU 점수 Papineni 등(2002)을 달성한다. MAGID를 이용한 모형의 당혹감은 가장 낮은 것으로 나타나 모형이 예측하는데 더 자신감이 있음을 알 수 있다. 또한, 응답의 다양성을 나타내는 뚜렷한 점수 Liu 등(2022)은 MAGID 이미지와 함께 제공되는 더 높은 이미지-텍스트 매칭에 기인할 수 있는 MAGID로 가장 높다. MMDialog 데이터셋은 실제 데이터셋이기 때문에 공유되는 이미지의 품질이 반드시 텍스트와 일치하는 것은 아니며 이는 모델을 덜 확신하게 만들고 더 높은 당혹감을 초래할 수 있다는 점에 유의하는 것이 중요하다. 한편, MAGID에 의해 생성된 이미지는 보다 제어된다.\n' +
      '\n' +
      '이 실험을 위해 24개의 GiB 메모리가 있는 4개의 NVIDIA RTX GPU를 사용했으며 훈련은 하루 종일 걸렸다.\n' +
      '\n' +
      '## 부록 F 실험 계산 비용\n' +
      '\n' +
      'MAGID 파이프라인 실행을 위해 24 GiB 메모리가 있는 NVIDIA RTX가 있는 GPU 하나로 실행할 수 있다.\n' +
      '\n' +
      '## 평가자간 신뢰도 측정치 선택에 관한 부록 G 논의\n' +
      '\n' +
      '섹션 4.2에서 우리는 리뷰어 간의 일관성을 평가하기 위해 그웨트 AC1을 사용했으며, 이상치에 대한 민감성과 모든 참가자에 걸쳐 높은 평균 점수에도 불구하고 일관되지 않은 결과를 보일 가능성이 있기 때문에 코헨의 카파를 사용하지 않기로 선택했다. Wongpakaran et al.(2013)의 연구에서 자세히 설명한 바와 같이, Gwet의 AC1은 Cohen의 Kappa와 비교할 때 평가자 간 신뢰도 평가에서 더 높은 일관성을 인정받으며 이상치에 대한 향상된 복원력과 함께 Wongpakaran et al.(2013)의 분석에 더 신뢰할 수 있는 척도를 제공한다. 이 접근법은 검토자 일관성에 대한 보다 안정적이고 정확한 평가를 보장하여 이상 현상이 신뢰도 점수에 미치는 영향을 완화한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c} \\hline \\hline Model & \\begin{tabular}{c} Aesthetic \\\\ Score \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} CLIP \\\\ Score \\\\ \\end{tabular} \\\\ \\hline SDXL 1.0 & 0.56 & 0.26 \\\\ SDXL 0.9 & **0.57** & 0.26 \\\\ Stable Diffusion 2.0 & 0.53 & 0.26 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: MAGID 파이프라인으로 상이한 안정 확산 모델을 테스트하는 단계\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\hline \\hline Dataset & PPL & \\begin{tabular}{c} BLEU- \\\\ 1 \\\\ \\end{tabular} & \\begin{tabular}{c} BLEU- \\\\ 2 \\\\ \\end{tabular} & \\begin{tabular}{c} distinct- \\\\ 1 \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} distinct- \\\\ 2 \\\\ \\end{tabular} \\\\ \\hline MMDialogue & 73.09 & **8.3** & **3.9** & 0.94 & 0.965 \\\\ MAGID & **70.99** & 7.9 & **3.9** & 0.94 & **0.971** \\\\ MMDD & 78.86 & 7.5 & 3.0 & 0.93 & 0.963 \\\\ No image & 78.88 & 7.9 & 3.0 & 0.92 & 0.952 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 다운스트림 트레이닝. 사용된 모델은 DialoGPT + ViT이다. 블루 점수는 백분율입니다.\n' +
      '\n' +
      'MAGID의 추가 예\n' +
      '\n' +
      '그림 8, 9 및 10은 MAGID와 MMDialog, PhotoChat 및 MMD를 비교하는 더 많은 예를 제공한다.\n' +
      '\n' +
      '### Experiment Setting\n' +
      '\n' +
      '이미지-텍스트 매칭과 미적 점수의 임계값을 결정하기 위해 검증 세트에 교차 검증을 사용했다. 이와 관련하여 CLIP 점수에 대한 임계값은 0.21로 설정되었고, 미적 점수에 대한 임계값은 0.51로 설정되었으며, 관찰에 기초하여 생성된 이미지가 폐기되고 피드백 루프를 트리거하기 전에 최대 2회 실패할 수 있는 프로토콜을 확립하였다. 이 접근법은 고품질 이미지를 생성하는 것과 효율적인 처리를 유지하는 것 사이의 균형을 보장했다. 모든 실험에서 이미지 생성을 위해 SDXL 1.0 모델을 사용했다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '그림 11, 12 및 13에서 가장 일반적인 시나리오는 MAGID가 선행 발화를 적절하게 지원하는 이미지를 생성하지 못할 수 있다는 것이다. 구체적으로, 도 11은 일반적인 예를 도시하는데, 여기서 생성된 이미지는 일반적으로 생성된 이미지에 적절한 텍스트 부호를 넣지 못한다. 도 12 및 도 13에서는 생성된 이미지가 숫자 객체 측면에서 정확한 설명을 따르지 않는 예들이 이미지 내에 존재해야 하는 예를 보여준다. 우리는 DALL-E 3와 같은 보다 진보된 확산 모델을 사용하는 것이 이 문제를 완화해야 한다고 믿는다.\n' +
      '\n' +
      '도 7: 인간 평가를 수행하기 위해 사용된 웹사이트의 스키마.\n' +
      '\n' +
      '도 8: MAGID(좌측) 대 MMDialog(우측)\n' +
      '\n' +
      '도 9: MAGID(좌측) 대 PhotoChat(우측)\n' +
      '\n' +
      '도 10: MAGID(좌측) 대 MMDD(우측)\n' +
      '\n' +
      '도 11: MAGID에 의한 생성 이미지가 표지판을 제대로 보여주지 못함 도 13: MAGID에 의한 생성 이미지가 5가 아닌 6개의 물고기를 제대로 보여주지 못함\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>