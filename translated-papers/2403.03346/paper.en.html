<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'Notably, the use of massive amount of image-caption pairs, which are automatically generated using images and their associated Hypertext Markup Language (HTML) alt-text has enabled the development of some important VLMs such as CLIP models [46] and diffusion models [49]. Similarly, the use of screenshots and simplified HTML text pairs powered the Pix2Struct models [32]. However, methods capable of producing automatically annotated data beyond basic image-text pairs are currently under explored. Consequently, the effects of employing explicit, automatically generated, fine-grained supervision for pre-training have been understudied.\n' +
      '\n' +
      'Therefore, in this work, we extend the use of web crawl corpuses and propose a novel pre-training framework that utilizes rich and diverse supervisions generated from web rendering. Modern websites are built using a combination of technologies such as HTML, CSS, JavaScript, that enable website creators to design dynamic content and interactive elements with diverse layouts.\n' +
      '\n' +
      'To leverage such information, our solution renders crawled web-pages into screenshot images. We also have access to textual content, position, attribute and relationship of HTML elements - all of which can be obtained cheaply and utilized in pre-training. Building on this extracted data, we propose a set of pre-training tasks (see details in 3.2) that are highly synergistic to downstream tasks. Our results demonstrate significant performance improvements compared to the image-to-text pre-training baseline. On average, we observed an improvement of **+2.7%** points across 5 datasets (ChartQA, RefExp, Widget Captioning, Screen Summarization and WebSRC) with language outputs, and a notable average increase of **+25.3%** points on 4 datasets (PubLayNet, PubTables1M, RefExp candidate free and ICDAR 2019 modern) with localization outputs. See more in Tables 1 and 2. Our key contributions:\n' +
      '\n' +
      '* We develop an automatic data annotation pipeline that is able to render web crawls and create rich labels. Coupled with our carefully designed data cleaning process, we create a high-quality and large-scale vision language pre-training dataset.\n' +
      '* S4, composed of ten carefully designed tasks on large scale web-screenshots showing the effectiveness on a wide range of benchmarks.\n' +
      '* up to 76.1% improvements on Table Detection, and at least 1% on Widget Captioning.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Next, we discuss in detail the difference to previous pre-training approaches.\n' +
      '\n' +
      '**Masked Signal Modeling.** Pre-training through self-supervision has revolutionized the field of natural language processing (NLP). Pioneering models like BERT [48] and GPTs [7, 44] demonstrated the profound impact of self-supervised learning in enhancing generalization across a variety of language tasks. The success in NLP spurred analogous research in computer vision, leading to innovations in Masked Image Modeling (MIM) with approaches such as BEiT [5], SimMIM [59], and MAE [21] that recover masked pixels or patches, and improvements on classic vision tasks such as classification, semantic segmentation, etc are observed. In the domain of Vision Language(VL), MLIM [3] and MaskVLM [30] propose to integrate MLM and MIM and conduct VL pretraining in a joint manner.\n' +
      '\n' +
      '**Supervised Pre-training** In supervised pre-training, image-caption pair annotations are generated on a large scale automatically from web crawls. This enables the training of models that generalize well in tasks like classification, retrieval, and captioning, as seen in works like CLIP, OFA, PaLi [10, 46, 56]. Donut [27] proposes a OCR-free model that relies on text reading pre-training of documents. SPOTLIGHT uses [33] region to text pre-training task on website and UI datasets. Pix2Struct [32] leverages screenshot and image pairs with a screen parsing pre-training task that converts webpage screenshots to HTML text. Our work proposes a pre-training paradigm that goes beyond image-text pairing type of tasks. We develop a suite of diverse, heterogeneous tasks specifically crafted to mirror the nature of downstream applications.\n' +
      '\n' +
      '## 3 S4 Pre-training\n' +
      '\n' +
      'In this section, we propose a novel pre-training paradigm for Vision-Language Models -- Strongly Supervised pre-training with ScreenShots (S4) from large scale website rendering. We will first describe the creation procedure of our dataset for S4 pretraining, which we will call S4 Data, and then go through our proposed pre-training tasks enabled by our novel preprocessing method.\n' +
      '\n' +
      '### Dataset Description\n' +
      '\n' +
      'CommonCrawl2 provides access to a large-scale web page corpus spanning over a decade. We download the web crawls from the Registry of Open Data on AWS3 and we filter content with an explicit copyright notice. We execute our rendering and extraction pipeline (described in 3.1.2) and data pre-processing and cleaning procedure (described in 3.1.3) to obtain 15M screenshots enriched with supervisions. We applied deduplication based on urls to make sure our screenshots are unique. Each page is rendered at a resolution of 1280x1280 and is paired with matching annotations that enable the proposed pre-training tasks described in 3.2.\n' +
      '\n' +
      '#### 3.1.2 Efficient Rendering and Supervision Extraction\n' +
      '\n' +
      'We use Playwright +, which provides a programmatic interface to headless browsers that we use to render raw HTML files into screenshots. For each web page, we retrieve and cache the associated CSS, JavaScript fonts and images needed to render the page accurately. Caching those assets avoids making unnecessary requests to the originating website and quicker rendering, allowing us to create 5M parsed screenshot per day with 500 CPUs.\n' +
      '\n' +
      'Footnote †: [https://github.com/microsoft/playwright](https://github.com/microsoft/playwright)\n' +
      '\n' +
      'We build the annotations by traversing through the document object model (DOM) tree and collecting annotations for every leaf node of type Text, Image, Table or Input. More information about the dataset and example annotation can be found in the supplementary material.\n' +
      '\n' +
      '#### 3.1.3 Pre-processing and Cleaning\n' +
      '\n' +
      'During data rendering, we found that directly traversing through the DOM tree and collecting information on each node would lead to the inclusion of elements that were not visible in the page. We solve this issue by inspecting their CSS property for visibility and verifying the alignment to their corresponding bounding box. Specifically, if the element elem_b returned by clicking on the center elem_a\'s bounding box is not a descendent of elem_a, then elem_a is pruned. This simple heuristics helps us get rid of most of the annotation that contains invisible elements. Also, we implemented recursive pre-order traversal to filter out overflow words in a textnode where the texts are overflowing outside of it\'s ancestor\'s bounding box. Without such a filter, words that are occluded by other elements would be included in the final annotation. Finally, we get rid of all <iframe> tags since the Same Origin Policy prohibits direct access of the nodes in <iframe>.\n' +
      '\n' +
      '### Pre-training Task construction\n' +
      '\n' +
      'Using the rich information provided by the HTML document structure, we design ten diverse supervised objec\n' +
      '\n' +
      'Figure 2: Compared to traditional pre-training paradigms, our rich supervised pre-training leverages much more information that is also cheap to acquire (i.e via browser). We can then utilize the rich semantic and structural annotations to construct novel pre-training tasks that are naturally and directly aligned with downstream tasks. We use green words to refer to the words contained (visible) in the screenshot. We use red words to refer to the words that are not visible in the screenshot. For instance, “price” is not shown on the screenshot, but is the id of an element (refer to picture). We use brown words in the format of <x><y><x><y> to denote the bounding box.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '#### 3.2.8 Table Parsing\n' +
      '\n' +
      'The original Screen Parsing objective, although encouraging structure-level understanding, does not emphasize the semantics of those structures, as the pre-processing replaces tags with empty brackets. We argue that the information contained in the tags is also useful signal for pre-training, especially for well-structured elements like <table>. Therefore, we design a table parsing objective which contains the original tag name as well as the text contents for tables inside a page, as shown in Figure 2.\n' +
      '\n' +
      '#### 3.2.9 Screenshot Titling\n' +
      '\n' +
      'To encourage the model to summarize the content in the screenshot and improve its ability on image captioning, we propose a screen tilting task. Specifically, the main title in the screenshot is masked and the model is asked to generate the title text by only looking at the rest of the web page. The ground truth title is obtained from the \\(<title>\\) node of the HTML DOM tree. The Screenshot Titling task closely resembles the screen summarization task for UI understanding,\n' +
      '\n' +
      '#### 3.2.10 Layout Analysis\n' +
      '\n' +
      'Obtaining layout from a screenshot is realized by grouping elements under the same sub-tree in the HTML. Specifically, for each element we obtain its _cleaned Xpath_ by only keeping tags in [<p>,<table>,<form>,<dl>,<button>,<ol>, <ul>,<nav>,<img>,<object>] as they represent the semantic abstraction of the element. Then, we group each elements according to the value of their _cleaned Xpath_ to form layout of the screenshot. A visualization of the layout from a screenshot is shown in Figure 3.\n' +
      '\n' +
      '### Architecture\n' +
      '\n' +
      'We adopt a simple architecture with an image encoder followed by a text decoder, same as Pix2Struct [32] and similar to Donut [27]. The Image encoder is ViT [14] and text decoder is transformer decoder, where the vocabulary is extended with 1000 coordinate tokens (representing discrete positions in images, normalized between 0-1000) to support localization tasks such as object detection and visual grounding. Such image-encoder-text-decoder models don\'t need text input and have the advantage of being OCR-free, which leads to reduced latency [27]. On the other hand, in order to read textual content that are typically small, input image resolution has to be high for good performance, which leads to increased memory usage. Our proposed S4 pre-training paradigm is not limited to this architecture and can be applied to other approaches as well.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'We validate the effectiveness of our ten proposed pre-training tasks by fine-tuning the model on nine downstream tasks and compare its performance to a Pix2Struct baseline model that was only pre-trained with screen parsing. Based on the output format, we also divide the downstream tasks into two groups.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '**Pre-training schema.** We propose 2 pre-training schemes, S4\\({}_{NL}\\) for natural language generation and S4\\({}_{Loc}\\) for localization, targeting on different downstream tasks. Specifically, S4\\({}_{NL}\\) includes the baseline screen parsing task and all the tasks on natural language generation, including Attribute Prediction, Table Parsing, Title Generation, and Node Relation Prediction. S4\\({}_{Loc}\\) comprises of tasks with bounding box generations, including OCR, Image Grounding, Element Grounding, Table Detection and Layout Analysis, in addition to the screen parsing task. During pre-training, we randomly sample one task for each image with uniform distribution.\n' +
      '\n' +
      '#### 4.1.1 Pretraining Settings\n' +
      '\n' +
      'We conducted pretraining on both 2 million and 15 million subsets of our S4 dataset, during which we set the screenshot\'s viewport to 1280*1280. We initialize our model with weights from Pix2struct-base and set our batch size to 32 for each node and use 4 A100 nodes during pretraining. The maximum sequence length for the pretraining targets is 128 and the patch size for the input image is 2048. Our optimizer is AdamW with learning rate set to 1e-4 with cosine decay, and for both 2M and 15M subsets we pretrain with\n' +
      '\n' +
      'Figure 3: Visualization of layout parsed from a screenshot. Corresponding HTML tags like <h1> are visualize on top-left corner of the bounding box.\n' +
      '\n' +
      '1 epoch per pretraining task. For instance, for S4\\({}_{NL}\\) pre-training with the 2M subset, there are 5 tasks so the total training sample is 5*2M = 10M. Note that since for each screenshots we can obtain multiple tasks, the models sees the same subset of screenshots regardless of the number of pretraining tasks.\n' +
      '\n' +
      '### Chart, Web, and UI Understanding\n' +
      '\n' +
      'In this section we evaluate on tasks that require generating natural language responses from image inputs. We focus on Chart & Web VQA, UI summarization and UI widget captioning.\n' +
      '\n' +
      '#### 4.2.1 Datasets\n' +
      '\n' +
      '**ChartQA**: ChartQA[42] is a VQA dataset for different types of charts (bar charts, line graphs, etc.). It includes both extractive and reasoning questions, which requires analyzing the visual data in charts to extract the relevant information. We follow the convention and report the Relaxed Match metric on ChartQA.\n' +
      '\n' +
      '**WebSRC**: WebSRC[9] is a web-based VQA dataset. It contains both cleaned HTML and the screenshot of web pages, and the task is to answer the question about the content in the web page. Prior arts mostly tackle this problem by taking the ground truth cleaned HTML code as inputs, which is an unrealistic setting as real-word applications often have much more complex HTML codes than the cleaned data. Instead, our model only takes the screenshot as inputs and predicts the answer from pure-vision information. On WebSRC the Exact Match metric is reported.\n' +
      '\n' +
      '**Screen2words**: Screen2words[54] is a dataset for extracting summarization from screenshots of mobile app screens. We use Bleu and Cider scores as the evaluation metrics.\n' +
      '\n' +
      '**Widget Captioning**: Widget Captioning[38] is a dataset for generating descriptive captions for UI widgets. The task is to generate captions that accurate describe the purpose or function of a widget in a bounding box, such as a button, slider, or a checkbox. In the input screenshot, the target widget is specified through the rendered bounding box. Bleu and Cider scores are used as evaluation metrics.\n' +
      '\n' +
      '**UI RefExp\\({}_{\\text{cls}}\\)** UI Referential Expression (RefExp) [4] is a dataset specifically designed for grounding referring expressions for UI elements in screenshots. Current SOTA usually approach this problem in a simplified classification formulation: given a question and a candidate widget from annotation, the model is asked to predict whether the widget and question are related. This setting requires little localization ability from the model as the candidates widget bounding boxes are provided as inputs. We call this classification setting RefExp\\({}_{\\text{cls}}\\)and report the classification accuracy as the metric.\n' +
      '\n' +
      '#### 4.2.2 Settings\n' +
      '\n' +
      'Following the same training and evaluation protocol [32], our model was pre-trained with S4\\({}_{NL}\\) objectives and fine-tuned on the Chart, Web, and UI Understanding tasks. Since there\'s no access to Google\'s 80M private data, we compare to two PixStruct variations. The first one is with the weights pre-trained on its private data using screen parsing released by the original author. The second one is initialized with the former\'s weights, and is further pre-trained on our 2M and 15M S4 data with only screen parsing objective. To have a fair comparison, our model is initialized with the same weights, and pre-trained on the same amount of data (2M and 15M S4 data) but with extra tasks. We also compare to Donut[27], which is another model uses pure-vision inputs and produces text predictions.\n' +
      '\n' +
      '#### 4.2.3 Results\n' +
      '\n' +
      'We tabulate the results in Tab. 1. Our method consistently outperforms Pix2Struct on all downstream tasks with significant margins when pre-trained with the same\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c c c c} \\hline \\hline\n' +
      '**Methods** & \\begin{tabular}{c} **Pre-training** \\\\ **Dataset** \\\\ \\end{tabular} & \\begin{tabular}{c} **Pre-training** \\\\ **Objectives** \\\\ \\end{tabular} & \\begin{tabular}{c} **Finetune** \\\\ **Batchsize** \\\\ \\end{tabular} & \\begin{tabular}{c} **ChartQA** \\(\\uparrow\\) \\\\ **RefExp\\({}_{\\text{cls}}\\)\\(\\uparrow\\)** \\\\ **Cap.** \\\\ \\end{tabular} & \\begin{tabular}{c} **Widget \\(\\uparrow\\) Screen \\(\\uparrow\\)** \\\\ **Sum.** \\\\ \\end{tabular} & \n' +
      '\\begin{tabular}{c} **WebSRC**\\(\\uparrow\\) \\\\ **WebSRC**\\(\\uparrow\\) \\\\ \\end{tabular} \\\\ \\hline Pix2Struct [32] & Google Priv. Data - 80M & Screen Parsing & 32 to 256 & 56.0 & 92.2 & 133.1 & 107.0 & - \\\\ Pix2Struct\\({}^{\\dagger}\\) & Google Priv. Data - 80M & Screen Parsing & 8 & 54.3 & 91.7 & 131.1 & 105.5 & 60.4 \\\\ \\hline Donut [27] & SynthDoG - 37M & OCR & 64 & 41.8 & - & 127.4 & 56.4 & - \\\\ Pix2Struct\\({}^{*}\\) & S4 Data - 2M & Screen Parsing & 8 & 47.4 & 87.9 & 129.5 & 101.3 & 58.7 \\\\ Pix2Struct\\({}^{*}\\) & S4 Data - 15M & Screen Parsing & 8 & 52.1 & 88.1 & 129.2 & 104.5 & 60.1 \\\\ S4\\({}^{*}\\)**(Ours)** & S4 Data - 2M & S4\\({}_{NL}\\) & 8 & 50.5 & 92.4 & 130.5 & 103.2 & 60.5 \\\\ S4\\({}^{*}\\)**(Ours)** & S4 Data - 15M & S4\\({}_{NL}\\) & 8 & **55.0** & **94.9** & **130.6** & **105.7** & **61.1** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Results for Chart, Web, and UI Understanding datasets. \\({}^{*}\\) denotes that we load Pix2Struct’s pre-trained weight and further pre-train on our S4 dataset with corresponding objectives. \\({}^{\\dagger}\\) denotes the reproduced results on downstream tasks with Pix2Struct-base’s pre-trained weight and smaller batch size. Results from gray rows are not directly comparable to our S4 model since we don’t have access to their non-released pre-training datasets. The results from last 4 rows show that in addition to the Pix2Struct’s pre-training objective, our supervised pre-training extracted from HTML DOM tree brings consistent improvement on various of downstream tasks. Note that the OCR objective for donut doesn’t include bounding box prediction.\n' +
      '\n' +
      'data. Specifically, when pre-trained with 15M image-text pairs, our method achieves 2.9, 6.8, 1.4, 1.2, and 1.0 improvement over Pix2Struct on ChartQA, RefExp\\({}_{\\text{cls}}\\), Widget Captioning, Screen Summarization, and WebSRC, respectively. Notice that the largest improvement is obtained on RefExp\\({}_{\\text{cls}}\\), because the pre-training tasks we proposed, such the attribute prediction and node relation prediction, help the model build robust connections between the UI elements and their referring expressions. In addition, when more data (15M) is available, our pre-training scheme S4\\({}_{NL}\\) gets improved accuracy compared to less training data (2M). The outstanding results comparing to the baseline and Donut demonstrate the efficacy of the proposed S4 pre-training scheme for downstream tasks that involve chart, web, and UI understanding.\n' +
      '\n' +
      'We also noticed that, comparing to the original Pix2struct, further pre-training it on our 2M data with screen parsing objective harms the performance across different datasets. This is expected as our data collected from Common Crawl has different distribution against the original Pix2struct data due to the data size (2M vs. 80M) and potentially different website filtering strategies. Specifically, we filter out website whose CSS and JS files failed to download, while the filtering process remain unclear for Pix2struct. In addition, we also used a much smaller batch size (128 vs. 2048) due to computational constraints. Therefore, the pre-train on 2M data might drive the model weight to a less optimal state. With 15M pre-training data, we observed performance improvements over 2M data, implying that more data might compensate this distribution shift.\n' +
      '\n' +
      '### Detection and Grounding\n' +
      '\n' +
      'We further investigate the effect of S4 pre-training on tasks that require spatial information understanding, such as image grounding and localization. While current SOTA models adopt specific architectures for detection, we show that with sufficient pre-training tasks on localization, an auto-gressive model can close the gap towards detection-specific architectures.\n' +
      '\n' +
      '#### 4.3.1 Datasets\n' +
      '\n' +
      '**PubMedNet**: PubLayNet [71] is a large-scale dataset for document layout analysis, containing more than 360,000 pages of scientific articles. We evaluate on the bounding box prediction task and report AP 50 as the metric.\n' +
      '\n' +
      '**ICDAR2019**: ICDAR2019 [17] is a table detection dataset that contains 1200 modern and archived documents. We only evaluate on the modern document split as the archived documents do not have bounding boxes. We use AP 50 as our evaluation metric.\n' +
      '\n' +
      '**PubMedTables-1M** PubTables-1M [53] has around 400k images with 947,642 tables from PMCOA scientific articles and we use it for table detection experiments. AP 50 is used as the evaluation metric.\n' +
      '\n' +
      '**UI RefExp\\({}_{\\text{cand,free}}\\)** As mentioned earlier, current works mostly treat the UI RefExp task as a binary classification problem using the ground truth bounding boxes as candidates, making it less challenging as it does not measure whether the model can localize the UI elements. In this work, we propose a new task, **UI RefExp\\({}_{\\text{cand,free}}\\)**, to include the element grounding into the challenges. In this task, the input is only the screenshot with the text description, and the model is asked to predict the bounding box of the related UI element directly, thus "candidate free". For evaluation, the predicted bounding box will be matched to the closest ground truth box to compute the accuracy.\n' +
      '\n' +
      '#### 4.3.2 Settings\n' +
      '\n' +
      'Our model was pre-trained with S4\\({}_{Loc}\\) for the benefits on localization related tasks. The model is then fine-tuned and evaluated on each downstream task dataset. We compare to\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c c c} \\hline \\hline \\multirow{2}{*}{**Methods**} & \\multirow{2}{*}{**Pre-training**} & \\multirow{2}{*}{**Pre-training**} & \\multirow{2}{*}{**RefExp\\({}_{\\text{cand,free}}\\)**} & \\multirow{2}{*}{**PublayNet \\(\\uparrow\\)**} & \\multirow{2}{*}{**PubMedTables1M \\(\\uparrow\\)**} & \\multirow{2}{*}{**ICDAR 2019 \\(\\uparrow\\)**} \\\\  & & & & & & (**Table Det.**) & (**Modern Subset)** \\\\  & \\multicolumn{1}{c}{**Dataset**} & & & & **30k samples** & 1M samples** & **400k samples** & **600 samples** \\\\ \\hline DETR & - & - & - & - & - & 99.5 & - \\\\ DiT-B (Cascade RCNN) & \\multicolumn{1}{c}{IIT-CDIP - 42M} & MIM & - & 95.4 & - & 97.2 \\\\ \\hline Pix2Struct [32] & Google Priv. Data - 80M & Screen Parsing & 55.1 & 91.1 & 97.0 & 3.6 \\\\ Pix2Struct\\({}^{\\text{t}}\\) & S4 Data - 2M & Screen Parsing & 52.7 & 91.0 & 97.1 & 3.3 \\\\ S4\\({}^{\\text{*}}\\)**(Ours)** & S4 Data - 2M & S4\\({}_{Loc}\\) & 83.6 & 92.5 & 98.4 & 70.7 \\\\ S4\\({}^{\\text{*}}\\)**(Ours)** & S4 Data - 15M & S4\\({}_{Loc}\\) & **84.3** & **93.1** & **99.0** & **79.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Results on Detection and Grounding datasets. We train all of the models with batch size = 32 and patch size = 2048.\\({}^{\\text{*}}\\) denotes that we load Pix2Struct-base’s pre-trained weight and further pre-train on our S4 dataset with corresponding objectives. Models denoted gray are specialist detection models that cannot parse language input (i.e cannot do grounding tasks). With our pre-training objectives, autoregressive models can get significant boosts on various detection & grounding tasks. MIM refers to Masked Image Modeling.\n' +
      '\n' +
      'two PixStruct variations. The first one is the weights pre-trained on its private data using screen parsing released by the original author. The second one is initialized with the former weights and is further pre-trained on our S4 data 2M with only screen parsing, to be compared to our model pre-trained on the same amount of data.\n' +
      '\n' +
      '#### 4.3.3 Results\n' +
      '\n' +
      'The evaluation results on all detection and grounding related tasks are tabulated in Tab. 2. Our method shows clear advantages over the baseline Pix2Struct model that was only pre-trained with the screen parsing task. Specifically, on \\(\\text{RefExp}_{\\text{cand\\_free}}\\), when pre-trained with 2M data, our method outperforms Pix2Struct by a significant margin of 30.9 (83.6 vs. 52.7). This is because our pre-training tasks like OCR prediction and element grounding, help the model learn to localize items in the image given their semantic descriptions. Similarly, on ICDAR, which only has 600 training samples, our method achieves 70.7 AP with 2M pre-training data, while the baseline Pix2Struct only obtains 3.3, due to its lack of localization ability. When there are more fine-tuning data for the downstream tasks (PublayNet 1M & PubTables 400K), Pix2Struct can learn to localize the objects and obtain decent performance, but our training scheme S4\\({}_{Loc}\\) still benefits the model and improves the performance by 1.5 on PublayNet and 1.3 on PubTables.\n' +
      '\n' +
      'The benefits of S4\\({}_{Loc}\\) pre-training becomes more prominent when more pre-train data is available. Our model pre-trained with 15M data consistently improves the accuracy on all four downstream tasks compared to 2M pre-train data. In particular, on ICDAR the 15M pre-train data improves the accuracy from 70.7 to 79.4, showing that the pre-training task benefits more when the downstream task has less data. It is worth noting that, as a generic auto-regressive text generation model, our method with 15M pre-training data achieves comparable performance on PublayNet and PubTables to detection specific models like DeTR and Dit-B, showing that sufficient pre-training with proper tasks helps close the gap between auto-regressive models and detection-specific architectures.\n' +
      '\n' +
      '### Contribution of each task\n' +
      '\n' +
      'We conducted ablative studies to show the effectiveness of each individual pre-training tasks besides screen parsing. For S4\\({}_{NL}\\), we evaluate on ChartQA, Widget Captioning, Screen Summarization, and \\(\\text{RefExp}_{\\text{cls}}\\), by adding the natural language related tasks gradually. For S4\\({}_{Loc}\\), we also add the localization related tasks incrementally and evaluate on \\(\\text{RefExp}_{\\text{cand\\_free}}\\) and ICDAR. The results are shown in Tab. 3. Observe that the downstream task usually benefits from the addition of the most related pre-training task. For example, Screen Summarization gets 2.1 performance improvement when the screen tilting pre-training task is added, while the other tasks have little effect on the performance. The attribute prediction task encourages the model to associate website elements to their text description. Therefore, adding it to the pre-training scheme significantly improves the performance on both Widget Captioning and \\(\\text{RefExp}_{\\text{cls}}\\), which requires the model to associate UI elements to texts. Similarly, adding all the localization related pre-train task substantially improves the model\'s ability on grounding elements, resulting in higher accuracy on both ICDAR and \\(\\text{RefExp}_{\\text{cand\\_free}}\\).\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      'We introduced a novel pre-training framework for vision-language models, with which models are exposed with a variety of supervised tasks on diverse and massive amount website screenshots. This innovation is enabled by our proposed data pipeline, in which the web pages are rendered, extracted, and cleaned automatically to generate screenshots and corresponding annotations. The tasks in our pre-training scheme are designed to maximize the utilization of annotations in our data as well as the similarities between the downstream tasks. Through extensive experiments, we demonstrated the efficacy of our method on boosting the downstream tasks performance on 9 different datasets.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c|c|c|c|c|c} \\hline \\hline Titing & \\multicolumn{1}{c|}{Attibute Pred.} & \\multicolumn{1}{c|}{NRP} & \\multicolumn{1}{c|}{Table Parsing.} & \\multicolumn{1}{c|}{Screen Parsing} & \\multicolumn{1}{c|}{ChartQA} & Widget Cap. & Screen Sum. & \\(\\text{RefExp}_{\\text{cls}}\\) \\\\ \\hline  & & & & ✓ & 47.4 & 129.5 & 101.3 & 87.9 \\\\  & & ✓ & ✓ & 48.7 & 128.3 & 101.4 & 87.8 \\\\  & & ✓ & ✓ & ✓ & **50.7** & 128.3 & 101.3 & 89.4 \\\\  & ✓ & ✓ & ✓ & ✓ & 50.1 & **130.7** & 101.1 & **92.7** \\\\ ✓ & ✓ & ✓ & ✓ & ✓ & 50.5 & 130.5 & **103.2** & 92.4 \\\\ \\hline \\hline \\multicolumn{1}{c|}{Table Detection} & \\multicolumn{1}{c|}{Layout Analysis} & \\multicolumn{1}{c|}{Image \\& Element Grounding} & OCR & Screen Parsing & ICDAR & \\(\\text{RefExp}_{\\text{cand\\_free}}\\) \\\\ \\hline  & & & & & ✓ & 3.3 & 52.7 \\\\  & & & & ✓ & ✓ & 50.1 & 68.6 \\\\  & & ✓ & ✓ & ✓ & ✓ & 52.9 & 66.2 \\\\ ✓ & ✓ & ✓ & ✓ & ✓ & **70.7** & **83.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Ablation study on adding different pre-training objectives using 2M S4 data.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '- modulated detection for end-to-end multi-modal understanding. _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 1760-1770, 2021.\n' +
      '* [27] Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yin, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Oc-free document understanding transformer, 2022.\n' +
      '* [28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* [29] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _International Journal of Computer Vision_, 128(7):1956-1981, 2020.\n' +
      '* [30] Gukyeong Kwon, Zhaowei Cai, Avinash Ravichandran, Erhan Bas, Rahul Bhotika, and Stefano Soatto. Masked vision and language modeling for multi-modal representation learning. _arXiv preprint arXiv:2208.02131_, 2022.\n' +
      '* [31] Justin Lazarow, Kwonjoon Lee, Kunyu Shi, and Zhuowen Tu. Learning instance occlusion for panoptic segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10720-10729, 2020.\n' +
      '* [32] Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding, 2023.\n' +
      '* [33] Gang Li and Yang Li. Spotlight: Mobile ui understanding using vision-language models with a focus. 2023.\n' +
      '* [34] Junlong Li, Yiheng Xu, Lei Cui, and Furu Wei. Markuplm: Pre-training of text and markup language for visually-rich document understanding. 2021.\n' +
      '* [35] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, 2022.\n' +
      '* [36] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jeng-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10955-10965, 2021.\n' +
      '* [37] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. _ECCV 2020_, 2020.\n' +
      '* [38] Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements, 2020.\n' +
      '* [39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.\n' +
      '* [40] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic vision-guistic representations for vision-and-language tasks. In _Neural Information Processing Systems_, 2019.\n' +
      '* [41] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. _ArXiv_, abs/2206.08916, 2022.\n' +
      '* [42] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning, 2022.\n' +
      '* [43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* [44] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.\n' +
      '* [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, 2021.\n' +
      '* [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [47] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _arXiv e-prints_, 2019.\n' +
      '* [48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.\n' +
      '* [49] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '* [50] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Lainon-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.\n' +
      '* [51] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 8430-8439, 2019.\n' +
      '* [52] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Testing prompt tuning for zero-shot generalization in vision-language models. _ArXiv_, abs/2209.07511, 2022.\n' +
      '* [53] Brandon Smock, Rohith Pesala, and Robin Abraham. PubTables-1M: Towards comprehensive table extraction from unstructured documents. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4634-4642, 2022.\n' +
      '* [54] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words: Automatic mobile ui summarization with multimodal learning, 2021.\n' +
      '* [55] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In _International Conference on Machine Learning_, 2022.\n' +
      '* [56] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In _International Conference on Machine Learning_, pages 23318-23340. PMLR, 2022.\n' +
      '* [57] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. _ArXiv_, abs/2111.02358, 2021.\n' +
      '* [58] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. _ArXiv_, abs/2208.10442, 2022.\n' +
      '* [59]Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9653-9663, 2022.\n' +
      '* [60] Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, and Fei Huang. E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning. _ArXiv_, abs/2106.01804, 2021.\n' +
      '* [61] Jinyu Yang, Jiali Duan, S. Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul M. Chilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive learning. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 15650-15659, 2022.\n' +
      '* [62] Xu Yang, Hanwang Zhang, Guojun Qi, and Jianfei Cai. Causal attention for vision-language tasks. _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9842-9852, 2021.\n' +
      '* [63] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat sense Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language models. _ArXiv_, abs/2109.11797, 2021.\n' +
      '* [64] Andy Zeng, Adrian S. Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael S. Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Peter R. Florence. Socratic models: Composing zero-shot multimodal reasoning with language. _ArXiv_, abs/2204.00598, 2022.\n' +
      '* [65] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts with visual concepts. _ArXiv_, abs/2111.08276, 2021.\n' +
      '* [66] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun-Juan Zhu, Lionel Ming shuan Ni, and Heung yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. _ArXiv_, abs/2203.03605, 2022.\n' +
      '* [67] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. _ArXiv_, abs/2206.05836, 2022.\n' +
      '* [68] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Making visual representations matter in vision-language models. _ArXiv_, abs/2101.00529, 2021.\n' +
      '* [69] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Making visual representations matter in vision-language models. _CVPR 2021_, 2021.\n' +
      '* [70] Zhaoyang Zhang, Yantao Shen, Kunyu Shi, Zhaowei Cai, Jun Fang, Siqi Deng, Hao Yang, Davide Modolo, Zhuowen Tu, and Stefano Soatto. Musketeer (all for one, and one for all): A generalist vision-language model with task explanation prompts, 2023.\n' +
      '* [71] Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. Publaynet: largest dataset ever for document layout analysis. In _2019 International Conference on Document Analysis and Recognition (ICDAR)_, pages 1015-1022. IEEE, 2019.\n' +
      '* 2348, 2021.\n' +
      '* [73] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 16795-16804, 2022.\n' +
      '* [74] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and vqa. _ArXiv_, abs/1909.11059, 2019.\n' +
      '* [75] Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin, Ben Chen, Hao Zhou, Minghui Qiu, and Ling Shao. Kaleidobert: Vision-language pre-training on fashion domain. _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 12642-12652, 2021.\n' +
      '\n' +
      '## Enhancing Vision-Language Pre-training with Rich Supervisions\n' +
      '\n' +
      'Supplementary Material\n' +
      '\n' +
      '## Appendix A Quantitative Results on Joint Training\n' +
      '\n' +
      'We provide some additional discussions for the impact of joint training language and location tasks. Directly combining S4\\({}_{NL}\\) objectives and S4\\({}_{Loc}\\) objectives, which we denote as S4\\({}_{Joint}\\), harms the performance on most downstream tasks. For S4\\({}_{Joint}\\), we use the following weighted loss as it gives the best average performance across all tasks:\n' +
      '\n' +
      'loss_weights = { \'screen2html\': 1.0,  \'attribute_prediction\': 0.5,  \'title_generation\': 0.5,  \'node_relation_prediction\':0.1,  \'table_parsing\':0.1,  \'ocr\':0.1,  \'table_detection\':0.1,  \'layout_analysis\':0.1,  \'image_grounding\':0.1,  \'element_grounding\':0.1 } ```\n' +
      '\n' +
      '\\begin{tabular}{l|c c c|c c c c} \\hline \\hline\n' +
      '**Methods** & \\begin{tabular}{c} **Pre-training** \\\\ **Dataset** \\\\ \\end{tabular} & \\begin{tabular}{c} **Pre-training** \\\\ **Objectives** \\\\ \\end{tabular} & \\begin{tabular}{c} **Finetune** \\\\ **Batchsize** \\\\ \\end{tabular} & \\begin{tabular}{c} **ChartQA** \\(\\uparrow\\) \\\\ **RefExp\\({}_{cls}\\)\\(\\uparrow\\) \\\\ **Cap.** \\\\ \\end{tabular} & \\begin{tabular}{c} **Widget** \\(\\uparrow\\) \\\\ **Sum.** \\\\ \\end{tabular} & \n' +
      '\\begin{tabular}{c} **Screen** \\(\\uparrow\\) \\\\ **WebSRC** \\(\\uparrow\\) \\\\ \\end{tabular} \\\\ \\hline S4\\({}^{*}\\) **(Ours)** & S4 Data - 15M & S4\\({}_{NL}\\) & 8 & **55.0** & **94.9** & **130.6** & **105.7** & **61.1** \\\\ S4\\({}^{*}\\) **(Ours)** & S4 Data - 15M & S4\\({}_{Joint}\\) & 8 & 52.1 & **94.9** & 128.4 & 101.5 & 60.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{l|c c c|c c c c} \\hline \\hline\n' +
      '**Methods** & \\begin{tabular}{c} **Pre-training** \\\\ **Dataset** \\\\ \\end{tabular} & \\begin{tabular}{c} **Pre-training** \\\\ **Objectives** \\\\ \\end{tabular} & \\begin{tabular}{c} **RefExp** \\(\\uparrow\\) \\\\ **cand\\({}_{\\text{,free}}\\) \\\\ **30k samples** \\\\ \\end{tabular} & \\begin{tabular}{c} **PubJNet** \\(\\uparrow\\) \\\\ **(Table Det.)** \\\\ **400k samples** \\\\ \\end{tabular} & \n' +
      '\\begin{tabular}{c} **PubMedTables** \\(\\uparrow\\) \\\\ **(Modern Subset)** \\\\ **600 samples** \\\\ \\end{tabular} \\\\ \\hline S4\\({}^{*}\\) **(Ours)** & S4 Data - 15M & S4\\({}_{Loc}\\) & **84.3** & **93.1** & **99.0** & **79.4** \\\\ S4\\({}^{*}\\) **(Ours)** & S4 Data - 15M & S4\\({}_{Joint}\\) & 79.2 & 91.6 & 97.7 & 76.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '## Appendix B Qualitative Results during Pre-training\n' +
      '\n' +
      'We visualize qualitative pre-training results in below figures. For table detection, image grounding, and element grounding, red boxes denotes ground truth boxes and blue boxes denotes predicted boxes. We present four images for each of these three tasks, where the bottom right is a failure case and the others are good cases. For layout analysis, we present a pair of prediction (blue) and ground truth layout (red).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Figure 4: Attribute Prediction\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:14]\n' +
      '\n' +
      'where is the image that describes ARC Logo7\n' +
      '\n' +
      'where is the image that describes Aknymy blog - 5 actionable tips increase conversions landing page thumbnail?\n' +
      '\n' +
      'Figure 6: Image Grounding\n' +
      '\n' +
      'Figure 7: Layout Analysis\n' +
      '\n' +
      'Figure 8: Node Relation\n' +
      '\n' +
      'Figure 9: OCR\n' +
      '\n' +
      'Figure 10: Screen2html\n' +
      '\n' +
      'Figure 11: Screen Tiltling\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:21]\n' +
      '\n' +
      'Figure 13: Table Parsing\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>