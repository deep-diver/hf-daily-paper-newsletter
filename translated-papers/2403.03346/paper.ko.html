<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '특히, 이미지 및 관련 하이퍼텍스트 마크업 언어(HTML) 알트텍스트를 사용하여 자동으로 생성되는 방대한 양의 이미지-캡션 쌍을 사용함으로써 CLIP 모델[46] 및 확산 모델[49]과 같은 일부 중요한 VLM의 개발이 가능하게 되었다. 유사하게, 스크린샷들 및 단순화된 HTML 텍스트 쌍들의 사용은 Pix2Struct 모델들에 전력을 공급했다[32]. 그러나 기본 이미지-텍스트 쌍을 넘어 자동으로 주석이 달린 데이터를 생성할 수 있는 방법이 현재 연구되고 있다. 결과적으로 사전 교육을 위해 명시적이고 자동으로 생성되며 세밀한 감독을 사용하는 효과는 제대로 연구되지 않았다.\n' +
      '\n' +
      '따라서 본 연구에서는 웹 크롤링 코퍼스의 사용을 확장하고, 웹 렌더링에서 생성되는 풍부하고 다양한 감독들을 활용하는 새로운 사전 훈련 프레임워크를 제안한다. 현대 웹 사이트는 HTML, CSS, JavaScript와 같은 기술의 조합을 사용하여 구축되며, 웹 사이트 제작자는 다양한 레이아웃으로 동적 콘텐츠 및 대화형 요소를 설계할 수 있다.\n' +
      '\n' +
      '이러한 정보를 활용하기 위해 솔루션은 웹 페이지를 스크린샷 이미지로 크롤링합니다. 우리는 또한 HTML 요소의 텍스트 내용, 위치, 속성 및 관계에 액세스할 수 있으며, 이 모든 것은 저렴하게 얻을 수 있으며 사전 훈련에 활용할 수 있다. 이 추출된 데이터를 기반으로 다운스트림 작업과 높은 시너지 효과를 내는 사전 훈련 작업 세트(3.2의 세부 정보 참조)를 제안한다. 우리의 결과는 이미지 대 텍스트 사전 훈련 기준선에 비해 상당한 성능 향상을 보여준다. 평균적으로, 언어 출력으로 5개의 데이터셋(ChartQA, RefExp, Widget Captioning, Screen Summarization and WebSRC)에 걸쳐 **+2.7%** 포인트의 개선이 관찰되었으며, 국지화 출력으로 4개의 데이터셋(PubLayNet, PubTables1M, RefExp 후보 프리 및 ICDAR 2019 modern)에서 **+25.3%** 포인트의 현저한 평균 증가가 관찰되었다. 표 1 및 표 2에서 더 많은 것을 참조하시오. 우리의 주요 기여는 다음과 같다.\n' +
      '\n' +
      '* 웹 크롤을 렌더링하고 풍부한 레이블을 생성할 수 있는 자동 데이터 주석 파이프라인을 개발합니다. 세심하게 설계된 데이터 클리닝 프로세스와 함께 고품질 및 대규모 비전 언어 사전 훈련 데이터 세트를 만듭니다.\n' +
      '* S4는 광범위한 벤치마크에 대한 효과를 보여주는 대규모 웹 스크린샷에서 신중하게 설계된 10개의 작업으로 구성된다.\n' +
      '* 표 검출에서 최대 76.1% 개선, 위젯 캡션에서 최소 1% 개선.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '다음으로, 이전의 사전 훈련 접근법과의 차이에 대해 자세히 논의한다.\n' +
      '\n' +
      '**Masked Signal Modeling.** 자가 감독을 통한 사전 훈련은 자연어 처리(NLP) 분야에 혁명을 일으켰다. BERT[48] 및 GPT[7, 44]와 같은 선구적인 모델은 다양한 언어 작업에 걸쳐 일반화를 향상시키는 데 자가 지도 학습의 심각한 영향을 입증했다. NLP의 성공은 컴퓨터 비전에 대한 유사한 연구에 박차를 가하면서 마스킹된 픽셀 또는 패치를 복구하는 BEiT[5], SimMIM[59], MAE[21] 등의 접근법으로 MIM(Masked Image Modeling)의 혁신으로 이어졌으며 분류, 시맨틱 세분화 등과 같은 고전적인 비전 작업에 대한 개선이 관찰된다. VL(Vision Language) 영역에서 MLIM[3]과 MaskVLM[30]은 MLM과 MIM을 통합하고, VL 프리트레이닝을 관절 방식으로 수행할 것을 제안한다.\n' +
      '\n' +
      '**감독된 사전-훈련** 감독된 사전-훈련에서 이미지-캡션 쌍 주석은 웹 크롤로부터 자동으로 대규모로 생성된다. 이를 통해 CLIP, OFA, PaLi [10, 46, 56]과 같은 작업에서 볼 수 있듯이 분류, 검색 및 캡션과 같은 작업에서 잘 일반화되는 모델을 교육할 수 있다. 도넛[27]은 문서의 텍스트 읽기 사전 훈련에 의존하는 OCR이 없는 모델을 제안한다. SPOTLIGHT는 [33] 영역을 사용하여 웹 사이트 및 UI 데이터 세트에서 사전 교육 작업을 텍스트화한다. Pix2Struct[32]는 웹페이지 스크린샷을 HTML 텍스트로 변환하는 스크린 파싱 사전-트레이닝 태스크와 함께 스크린샷 및 이미지 쌍을 활용한다. 본 논문에서는 이미지-텍스트 페어링 형태의 태스크를 넘어서는 사전 학습 패러다임을 제안한다. 다운스트림 애플리케이션의 특성을 반영하도록 특별히 제작된 다양하고 이질적인 작업 세트를 개발합니다.\n' +
      '\n' +
      '## 3 S4 사전교육\n' +
      '\n' +
      '이 섹션에서는 대규모 웹 사이트 렌더링에서 스크린샷을 사용한 강력한 감독 사전 교육(S4)인 비전 언어 모델에 대한 새로운 사전 교육 패러다임을 제안한다. 우리는 먼저 S4 데이터를 호출할 S4 사전 훈련을 위한 데이터 세트의 생성 절차를 설명하고 새로운 전처리 방법으로 활성화된 제안된 사전 훈련 작업을 검토한다.\n' +
      '\n' +
      '### Dataset Description\n' +
      '\n' +
      'CommonCrawl2는 10년에 걸쳐 걸쳐 있는 대규모 웹 페이지 코퍼스에 대한 액세스를 제공한다. 우리는 AWS3의 오픈 데이터 레지스트리에서 웹 크롤을 다운로드하고 명시적인 저작권 공지로 콘텐츠를 필터링한다. 이를 위해 렌더링 및 추출 파이프라인(3.1.2에 설명됨)과 데이터 전처리 및 클리닝 절차(3.1.3에 설명됨)를 실행하여 감독 기능이 풍부한 15M 스크린샷을 얻는다. 우리는 스크린샷이 고유한지 확인하기 위해 URL 기반 중복제거를 적용했습니다. 각 페이지는 1280x1280의 해상도로 렌더링되며 3.2에 설명된 제안된 사전 훈련 작업을 가능하게 하는 매칭 주석과 쌍을 이룬다.\n' +
      '\n' +
      '######3.1.2 효율적인 렌더링 및 감독 추출\n' +
      '\n' +
      '우리는 원시 HTML 파일을 스크린샷으로 렌더링하기 위해 사용하는 헤드리스 브라우저에 프로그램 인터페이스를 제공하는 플레이라이트 +를 사용합니다. 각 웹 페이지에 대해, 우리는 페이지를 정확하게 렌더링하기 위해 필요한 연관된 CSS, 자바스크립트 폰트 및 이미지를 검색하고 캐시한다. 이러한 자산을 캐싱하면 원래 웹 사이트에 불필요한 요청을 하고 렌더링이 빨라지는 것을 방지하여 500개의 CPU로 하루에 5M 파싱된 스크린샷을 생성할 수 있습니다.\n' +
      '\n' +
      '각주 †: [https://github.com/microsoft/playwright](https://github.com/microsoft/playwright)\n' +
      '\n' +
      '문서 객체 모델 트리를 통해 주석을 탐색하고 텍스트, 이미지, 테이블 또는 입력의 모든 리프 노드에 대한 주석을 수집하여 주석을 구축한다. 데이터세트 및 예제 주석에 대한 더 많은 정보는 보충 자료에서 찾을 수 있다.\n' +
      '\n' +
      '1.3 전처리 및 세척\n' +
      '\n' +
      '데이터 렌더링 과정에서 DOM 트리를 통해 직접 탐색하고 각 노드에 대한 정보를 수집하면 페이지에 보이지 않는 요소가 포함될 수 있음을 발견했으며, 가시성을 위해 CSS 속성을 검사하고 해당 경계 상자에 정렬을 확인함으로써 이 문제를 해결한다. 구체적으로, 중심 elem_a의 바운딩 박스를 클릭함으로써 리턴되는 엘리먼트 elem_b가 elem_a의 하강이 아니라면, elem_a는 프루닝된다. 이 간단한 휴리스틱은 보이지 않는 요소를 포함하는 대부분의 주석을 제거하는 데 도움이 된다. 또한, 텍스트가 조상 바운딩 박스 밖에서 넘쳐나는 텍스트 노드에서 오버플로우 단어를 걸러내기 위해 재귀적 사전 순서 탐색을 구현하였다. 이러한 필터가 없으면 다른 요소에 의해 가려지는 단어가 최종 주석에 포함될 것이다. 마지막으로, 동일 원산지 정책은 <iframe>에서 노드의 직접 접근을 금지하기 때문에 모든 <iframe> 태그를 제거한다.\n' +
      '\n' +
      '### 사전 훈련 작업 구축\n' +
      '\n' +
      'HTML 문서 구조에 의해 제공되는 풍부한 정보를 사용하여 10개의 다양한 감독 오벡을 설계한다.\n' +
      '\n' +
      '도 2: 전통적인 사전 훈련 패러다임에 비해, 우리의 풍부한 감독 사전 훈련은 (즉, 브라우저를 통해) 얻기에 값싼 훨씬 더 많은 정보를 활용한다. 그런 다음 풍부한 의미 및 구조적 주석을 활용하여 다운스트림 작업과 자연스럽고 직접 정렬된 새로운 사전 훈련 작업을 구성할 수 있다. 우리는 그린 워드를 사용하여 스크린샷에 포함된(보이는) 워드를 참조합니다. 우리는 스크린샷에 보이지 않는 단어를 언급하기 위해 빨간색 단어를 사용한다. 예를 들어, "가격"은 스크린샷에 표시되지 않지만 요소(그림 참조)의 ID입니다. 우리는 경계 상자를 나타내기 위해 <x><y><x><y>의 형식으로 갈색 단어를 사용한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '테이블 파싱 3.2.8 테이블 파싱\n' +
      '\n' +
      '기존의 Screen Parsing 목표는 구조 수준의 이해를 장려하지만 사전 처리가 태그를 빈 대괄호로 대체하기 때문에 이러한 구조의 의미를 강조하지 않는다. 우리는 태그에 포함된 정보가 사전 훈련, 특히 <표>와 같은 잘 구조화된 요소에 대해서도 유용한 신호라고 주장한다. 따라서 본 논문에서는 그림 2와 같이 페이지 내 테이블에 대한 텍스트 내용뿐만 아니라 원래 태그 이름을 포함하는 테이블 파싱 목적을 설계한다.\n' +
      '\n' +
      '###### 3.2.9 스크린샷 타이틀링\n' +
      '\n' +
      '모델이 스크린샷에 있는 내용을 요약하고 이미지 캡셔닝에 대한 능력을 향상시키기 위해 스크린 틸팅 태스크를 제안한다. 구체적으로, 스크린샷 내의 메인 타이틀을 마스킹하고, 나머지 웹 페이지만을 보고 타이틀 텍스트를 생성하도록 모델을 요청하며, HTML DOM 트리의 \\(<title>\\) 노드로부터 그라운드 트리의 타이틀을 획득한다. 상기 스크린샷 타이틀링 태스크는 UI 이해를 위한 화면 요약 태스크와 매우 유사하고,\n' +
      '\n' +
      'Layout Analysis 3.2.10\n' +
      '\n' +
      '스크린샷으로부터 레이아웃을 획득하는 것은 HTML에서 동일한 서브-트리 아래의 엘리먼트들을 그룹화함으로써 실현된다. 구체적으로, 각 엘리먼트에 대해, 그들이 엘리먼트의 의미적 추상화를 나타내는 바와 같이 [<p>, <table>, <form>, <dl>, <button>, <ol>, <ul>, <nav>, <img>, <object>]에 태그만을 유지함으로써 그 _cleaned Xpath_를 얻는다. 그런 다음 각 요소를 _cleaned Xpath_ 값에 따라 그룹화하여 스크린샷의 레이아웃을 형성한다. 스크린샷에서 레이아웃의 시각화는 그림 3에 나와 있다.\n' +
      '\n' +
      '### Architecture\n' +
      '\n' +
      '우리는 Pix2Struct[32]와 동일하고 도넛[27]과 유사한 텍스트 디코더가 뒤따르는 이미지 인코더를 사용하여 간단한 아키텍처를 채택한다. 이미지 인코더는 ViT[14]이고 텍스트 디코더는 트랜스포머 디코더이며, 여기서 어휘는 객체 검출 및 시각적 접지와 같은 로컬화 작업을 지원하기 위해 1000개의 좌표 토큰(이미지 내의 이산 위치를 나타내고 0-1000 사이에서 정규화된)으로 확장된다. 그러한 이미지-인코더-텍스트-디코더 모델들은 텍스트 입력이 필요하지 않고 OCR-프리라는 이점을 가지며, 이는 감소된 레이턴시로 이어진다[27]. 한편, 통상적으로 작은 텍스트 콘텐츠를 읽기 위해서는 좋은 성능을 위해 입력 이미지 해상도가 높아야 하며, 이는 메모리 사용 증가로 이어진다. 제안된 S4 사전 훈련 패러다임은 이 아키텍처에 국한되지 않으며 다른 접근 방식에도 적용할 수 있다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '우리는 9개의 다운스트림 태스크에서 모델을 미세 조정함으로써 제안된 10개의 사전 훈련 태스크의 유효성을 검증하고, 그 성능을 스크린 파싱으로만 사전 훈련된 Pix2Struct 기준 모델과 비교한다. 출력 형식을 기반으로 다운스트림 작업도 두 그룹으로 나눕니다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '**사전 학습 스키마.** 자연어 생성을 위한 S4\\({}_{NL}\\)와 로컬리제이션을 위한 S4\\({}_{Loc}\\)의 두 가지 사전 학습 기법을 제안한다. 구체적으로, S4\\({}_{NL}\\)은 베이스라인 스크린 파싱 태스크와 속성 예측, 테이블 파싱, 타이틀 생성, 노드 관계 예측을 포함한 자연어 생성에 대한 모든 태스크를 포함한다. S4\\({}_{Loc}\\)는 화면 파싱 작업 외에 OCR, Image Grounding, Element Grounding, Table Detection 및 Layout Analysis 등 바운딩 박스 생성 작업을 포함한다. 사전 훈련 동안, 우리는 균일한 분포를 갖는 각각의 이미지에 대해 하나의 태스크를 랜덤하게 샘플링한다.\n' +
      '\n' +
      '######4.1.1 사전 훈련 설정\n' +
      '\n' +
      'S4 데이터셋의 2백만 개와 1,500만 개의 하위 집합에 대해 사전 훈련을 수행하였고, 그 동안 스크린샷의 뷰포트를 1280*1280으로 설정하고, Pix2struct-base의 가중치로 모델을 초기화하고 각 노드에 대해 배치 크기를 32로 설정하고 사전 훈련 동안 4개의 A100 노드를 사용한다. 사전 훈련 대상에 대한 최대 시퀀스 길이는 128이고 입력 영상에 대한 패치 크기는 2048이다. 최적화기는 학습률이 1e-4로 설정된 AdamW와 코사인 감쇠를 가지며, 2M 및 15M 부분 집합 모두에 대해 사전 훈련한다.\n' +
      '\n' +
      '도 3: 스크린샷으로부터 파싱된 레이아웃의 시각화. <h1>과 같은 대응하는 HTML 태그는 바운딩 박스의 상단-좌측 코너에 시각화된다.\n' +
      '\n' +
      '사전 훈련 작업당 1시간입니다. 예를 들어, 2M 하위 집합으로 S4\\({}_{NL}\\) 사전 훈련의 경우, 5개의 작업이 있으므로 총 훈련 샘플은 5*2M = 10M이다. 각 스크린샷에 대해 여러 작업을 얻을 수 있으므로 모델은 사전 훈련 작업의 수에 관계없이 동일한 스크린샷 하위 집합을 봅니다.\n' +
      '\n' +
      '### 차트, 웹 및 UI 이해\n' +
      '\n' +
      '이 섹션에서는 이미지 입력에서 자연어 응답을 생성해야 하는 작업에 대해 평가한다. 우리는 차트 & 웹 VQA, UI 요약 및 UI 위젯 캡셔닝에 중점을 둡니다.\n' +
      '\n' +
      '#### 4.2.1 Datasets\n' +
      '\n' +
      '**ChartQA**: ChartQA[42]는 상이한 유형의 차트(막대 차트, 선 그래프 등)에 대한 VQA 데이터세트이다. 추출 및 추론 질문을 모두 포함하며, 이는 관련 정보를 추출하기 위해 차트 내의 시각적 데이터를 분석해야 한다. 우리는 규칙을 따르고 ChartQA에 릴렉스된 매치 메트릭을 보고한다.\n' +
      '\n' +
      '**WebSRC**: WebSRC[9]는 웹 기반 VQA 데이터셋이다. 웹 페이지의 클린닝된 HTML과 스크린샷을 모두 포함하고 있으며, 태스크는 웹 페이지 내의 콘텐츠에 대한 질문에 답하는 것이다. 선행 기술들은 대부분 클린닝된 HTML 코드를 입력으로 취함으로써 이 문제를 해결하는데, 이는 실제 워드 애플리케이션들이 클린닝된 데이터보다 훨씬 더 복잡한 HTML 코드를 갖는 경우가 많기 때문에 비현실적인 설정이다. 대신, 우리의 모델은 스크린샷을 입력으로 삼고 순수 비전 정보로부터 답을 예측한다. 웹SRC에서 정확한 일치 메트릭이 보고됩니다.\n' +
      '\n' +
      '**Screen2words**: Screen2words[54]는 모바일 앱 화면의 스크린샷에서 요약을 추출하기 위한 데이터셋이다. 우리는 Bleu와 Cider 점수를 평가 지표로 사용한다.\n' +
      '\n' +
      '**Widget Captioning**: Widget Captioning[38]은 UI 위젯에 대한 설명 캡션을 생성하기 위한 데이터셋이다. 작업은 버튼, 슬라이더 또는 확인란과 같은 경계 상자에 위젯의 목적 또는 기능을 정확하게 설명하는 캡션을 생성하는 것이다. 입력 스크린샷에서, 대상 위젯은 렌더링된 바운딩 박스를 통해 지정된다. Bleu 및 Cider 점수는 평가 메트릭으로 사용된다.\n' +
      '\n' +
      '**UI RefExp\\({}_{\\text{cls}}\\)**UI 참조 표현식(RefExp) [4]는 스크린샷에서 UI 요소에 대한 참조 표현을 접지하기 위해 특별히 설계된 데이터셋이다. 현재의 SOTA는 보통 단순화된 분류 공식에서 이 문제에 접근한다: 질문과 주석으로부터 후보 위젯이 주어지면, 모델은 위젯과 질문이 관련이 있는지 여부를 예측하도록 요청된다. 이 설정은 후보 위젯 바운딩 박스들이 입력들로서 제공되기 때문에 모델로부터의 로컬화 능력을 거의 필요로 하지 않는다. 이 분류 설정을 RefExp\\({}_{\\text{cls}\\)라고 하고 분류 정확도를 메트릭으로 보고한다.\n' +
      '\n' +
      '#### 4.2.2 Settings\n' +
      '\n' +
      '동일한 훈련 및 평가 프로토콜[32]에 따라, 본 모델은 S4\\({}_{NL}\\) 목표를 가지고 사전 훈련되었고 차트, 웹 및 UI 이해 작업에서 미세 조정되었다. 구글의 80M 개인 데이터에 대한 액세스가 없기 때문에, 우리는 두 개의 PixStruct 변종과 비교한다. 첫 번째는 원저자가 공개한 화면파싱을 이용하여 개인 데이터에 미리 학습된 가중치이다. 두 번째는 전자의 가중치로 초기화되고, 스크린 파싱 목적만으로 2M 및 15M S4 데이터에 대해 사전 학습된다. 공정한 비교를 위해, 본 모델은 동일한 가중치로 초기화되고, 동일한 양의 데이터(2M 및 15M S4 데이터)에 대해 사전 훈련되지만 추가 작업이 있다. 우리는 또한 순수 비전 입력을 사용하고 텍스트 예측을 생성하는 또 다른 모델인 도넛[27]과 비교한다.\n' +
      '\n' +
      '#### 4.2.3 Results\n' +
      '\n' +
      '우리는 탭에서 결과를 표로 작성한다. 1. 우리의 방법은 동일한 방법으로 사전 훈련되었을 때 상당한 여유를 가진 모든 다운스트림 작업에서 일관되게 Pix2Struct보다 우수하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c c c c} \\hline \\hline\n' +
      '**Methods** & \\begin{tabular}{c} **Pre-training** \\\\ **Dataset** \\\\ \\end{tabular} & \\begin{tabular}{c} **Pre-training** \\\\ **Objectives** \\\\ \\end{tabular} & \\begin{tabular}{c} **Finetune** \\\\ **Batchsize** \\\\ \\end{tabular} & \\begin{tabular}{c} **ChartQA** \\(\\uparrow\\) \\\\ **RefExp\\({}_{\\text{cls}}\\)\\(\\uparrow\\)** \\\\ **Cap.** \\\\ \\end{tabular} & \\begin{tabular}{c} **Widget \\(\\uparrow\\) Screen \\(\\uparrow\\)** \\\\ **Sum.** \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} **WebSRC**\\(\\uparrow\\) \\\\ **WebSRC**\\(\\uparrow\\) \\\\ \\end{tabular} \\\\ \\hline Pix2Struct [32] & Google Priv. Data - 80M & Screen Parsing & 32 to 256 & 56.0 & 92.2 & 133.1 & 107.0 & - \\\\ Pix2Struct\\({}^{\\dagger}\\) & Google Priv. Data - 80M & Screen Parsing & 8 & 54.3 & 91.7 & 131.1 & 105.5 & 60.4 \\\\ \\hline Donut [27] & SynthDoG - 37M & OCR & 64 & 41.8 & - & 127.4 & 56.4 & - \\\\ Pix2Struct\\({}^{*}\\) & S4 Data - 2M & Screen Parsing & 8 & 47.4 & 87.9 & 129.5 & 101.3 & 58.7 \\\\ Pix2Struct\\({}^{*}\\) & S4 Data - 15M & Screen Parsing & 8 & 52.1 & 88.1 & 129.2 & 104.5 & 60.1 \\\\ S4\\({}^{*}\\)**(Ours)** & S4 Data - 2M & S4\\({}_{NL}\\) & 8 & 50.5 & 92.4 & 130.5 & 103.2 & 60.5 \\\\ S4\\({}^{*}\\)**(Ours)** & S4 Data - 15M & S4\\({}_{NL}\\) & 8 & **55.0** & **94.9** & **130.6** & **105.7** & **61.1** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 차트, 웹 및 UI 이해 데이터셋에 대한 결과. \\ ({}^{*}\\)는 Pix2Struct의 사전 훈련된 가중치를 로드하고 해당 목표를 가진 S4 데이터 세트에 추가 사전 훈련을 함을 나타낸다. \\ ({}^{\\dagger}\\)는 Pix2Struct-base의 사전 훈련된 무게와 더 작은 배치 크기로 다운스트림 작업에서 재현된 결과를 나타낸다. 회색 행의 결과는 공개되지 않은 사전 훈련 데이터 세트에 액세스할 수 없기 때문에 S4 모델과 직접 비교할 수 없다. 마지막 4개 행의 결과는 Pix2Struct의 사전 훈련 목표 외에도 HTML DOM 트리에서 추출한 감독 사전 훈련이 다양한 다운스트림 작업에 일관된 개선을 가져온다는 것을 보여준다. 도넛에 대한 OCR 목표는 바운딩 박스 예측을 포함하지 않는다는 점에 유의한다.\n' +
      '\n' +
      '데이터. 구체적으로 15M 이미지-텍스트 쌍으로 사전 학습하였을 때 ChartQA, RefExp\\({}_{\\text{cls}}\\), Widget Captioning, Screen Summarization, WebSRC에서 각각 Pix2Struct에 비해 2.9, 6.8, 1.4, 1.2, 1.0의 향상을 보였다. RefExp\\({}_{\\text{cls}}\\)에서 가장 큰 개선 효과를 얻을 수 있음을 주목하라. 왜냐하면 우리가 제안한 사전 훈련 작업인 속성 예측 및 노드 관계 예측은 UI 요소와 참조 표현 간의 견고한 연결을 구축하는 데 도움이 되기 때문이다. 또한, 더 많은 데이터(15M)를 사용할 수 있을 때, 사전 학습 기법 S4\\({}_{NL}\\)은 적은 학습 데이터(2M)에 비해 향상된 정확도를 얻는다. 베이스라인 및 도넛과 비교하여 우수한 결과는 차트, 웹 및 UI 이해도를 포함하는 다운스트림 작업에 대해 제안된 S4 사전 훈련 스킴의 유효성을 입증한다.\n' +
      '\n' +
      '또한 기존 Pix2struct와 비교하여 스크린 파싱 목표를 사용하여 2M 데이터에서 사전 훈련하는 것이 다른 데이터 세트에서 성능에 해를 끼친다는 것을 발견했다. 이것은 Common Crawl에서 수집된 데이터가 데이터 크기(2M 대 80M)로 인해 원래 Pix2struct 데이터와 다른 분포를 가지므로 예상된다. 및 잠재적으로 다른 웹 사이트 필터링 전략. 구체적으로, CSS 및 JS 파일이 다운로드되지 않은 웹 사이트를 필터링하는 반면, Pix2struct에 대한 필터링 프로세스는 불분명하다. 또한 훨씬 더 작은 배치 크기(128 대 2048)를 사용했습니다. 계산 제약으로 인해. 따라서 2M 데이터에 대한 사전 훈련은 모델 가중치를 덜 최적의 상태로 유도할 수 있다. 15M 사전 훈련 데이터를 사용하여 2M 데이터에 비해 성능 향상을 관찰했으며, 이는 더 많은 데이터가 이러한 분포 변화를 보상할 수 있음을 의미한다.\n' +
      '\n' +
      '### 검출 및 접지\n' +
      '\n' +
      'S4 사전 훈련이 영상접지, 국산화 등 공간정보 이해가 필요한 업무에 미치는 영향을 추가로 조사한다. 현재 SOTA 모델은 검출을 위해 특정 아키텍처를 채택하지만, 로컬화에 대한 충분한 사전 훈련 작업을 통해 자동 회귀 모델이 검출 특정 아키텍처로의 격차를 좁힐 수 있음을 보여준다.\n' +
      '\n' +
      '#### 4.3.1 Datasets\n' +
      '\n' +
      '**PubMedNet**: PubLayNet[71]은 36만 페이지 이상의 과학 기사를 포함하는 문서 레이아웃 분석을 위한 대규모 데이터세트이다. 경계 상자 예측 작업에 대해 평가하고 AP 50을 메트릭으로 보고한다.\n' +
      '\n' +
      '**ICDAR2019**: ICDAR2019 [17]은 1200개의 현대적이고 보관된 문서들을 포함하는 테이블 검출 데이터세트이다. 보관된 문서에 경계 상자가 없기 때문에 현대 문서 분할에 대해서만 평가합니다. 평가 메트릭으로 AP 50을 사용합니다.\n' +
      '\n' +
      '**PubMedTables-1M** PubTables-1M[53]은 PMCOA 과학 논문의 947,642개의 표로 약 400k 개의 이미지를 가지고 있으며 표 탐지 실험에 사용한다. AP(50)는 평가 메트릭으로서 사용된다.\n' +
      '\n' +
      '**UI RefExp\\({}_{\\text{cand,free}}})** 앞에서 언급한 바와 같이, 현재 작업들은 대부분 UI RefExp 태스크를 Ground truth 바운딩 박스를 후보로 사용하여 이진 분류 문제로 처리하므로, 모델이 UI 엘리먼트를 로컬화할 수 있는지 여부를 측정하지 못하여 덜 어렵다. 본 논문에서는 요소 접지를 과제에 포함시키기 위한 새로운 작업인 **UI RefExp\\({}_text{cand,free}}}**을 제안한다. 이 작업에서 입력은 텍스트 설명이 있는 스크린샷일 뿐이며, 모델은 관련 UI 요소의 바운딩 박스를 직접 예측하도록 요청되어 "후보 자유"된다. 평가를 위해, 예측된 바운딩 박스는 정확도를 계산하기 위해 가장 가까운 그라운드 진리 박스에 매칭될 것이다.\n' +
      '\n' +
      '#### 4.3.2 Settings\n' +
      '\n' +
      '우리의 모델은 현지화 관련 작업에 대한 이점을 위해 S4\\({}_{Loc}\\)으로 사전 훈련되었다. 그런 다음 모델을 미세 조정하고 각 다운스트림 작업 데이터 세트에 대해 평가한다. 우리는 비교합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c c c} \\hline \\hline \\multirow{2}{*}{**Methods**} & \\multirow{2}{*}{**Pre-training**} & \\multirow{2}{*}{**Pre-training**} & \\multirow{2}{*}{**RefExp\\({}_{\\text{cand,free}}\\)**} & \\multirow{2}{*}{**PublayNet \\(\\uparrow\\)**} & \\multirow{2}{*}{**PubMedTables1M \\(\\uparrow\\)**} & \\multirow{2}{*}{**ICDAR 2019 \\(\\uparrow\\)**} \\\\  & & & & & & (**Table Det.**) & (**Modern Subset)** \\\\  & \\multicolumn{1}{c}{**Dataset**} & & & & **30k samples** & 1M samples** & **400k samples** & **600 samples** \\\\ \\hline DETR & - & - & - & - & - & 99.5 & - \\\\ DiT-B (Cascade RCNN) & \\multicolumn{1}{c}{IIT-CDIP - 42M} & MIM & - & 95.4 & - & 97.2 \\\\ \\hline Pix2Struct [32] & Google Priv. Data - 80M & Screen Parsing & 55.1 & 91.1 & 97.0 & 3.6 \\\\ Pix2Struct\\({}^{\\text{t}}\\) & S4 Data - 2M & Screen Parsing & 52.7 & 91.0 & 97.1 & 3.3 \\\\ S4\\({}^{\\text{*}}\\)**(Ours)** & S4 Data - 2M & S4\\({}_{Loc}\\) & 83.6 & 92.5 & 98.4 & 70.7 \\\\ S4\\({}^{\\text{*}}\\)**(Ours)** & S4 Data - 15M & S4\\({}_{Loc}\\) & **84.3** & **93.1** & **99.0** & **79.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 탐지 및 접지 데이터 세트에 대한 결과. 배치 크기 = 32, 패치 크기 = 2048로 모든 모델을 학습한다. ({}^{\\text{*}}\\)는 Pix2Struct-base의 사전 훈련된 가중치를 적재하고 해당 목표를 가진 S4 데이터 세트에 추가 사전 훈련을 수행함을 나타낸다. 회색으로 표시된 모델은 언어 입력을 구문 분석할 수 없는 전문가 탐지 모델(즉, 접지 작업을 수행할 수 없음)이다. 우리의 사전 훈련 목표를 통해, 자기 회귀 모델은 다양한 탐지 및 접지 작업에서 상당한 증가를 얻을 수 있다. MIM은 마스킹된 이미지 모델링을 의미한다.\n' +
      '\n' +
      '두 개의 PixStruct 변형입니다. 첫 번째는 원저자가 공개한 화면파싱을 이용하여 개인 데이터에 미리 학습된 가중치이다. 두 번째는 이전 가중치로 초기화되고 스크린 파싱만으로 S4 데이터 2M에서 추가로 사전 훈련되어 동일한 양의 데이터로 사전 훈련된 모델과 비교된다.\n' +
      '\n' +
      '#### 4.3.3 Results\n' +
      '\n' +
      '모든 탐지 및 접지 관련 작업에 대한 평가 결과는 탭 2에서 표로 작성되었으며, 본 방법은 스크린 파싱 작업만으로 사전 훈련된 기본 Pix2Struct 모델에 비해 분명한 이점을 보여준다. 구체적으로, 2M 데이터로 사전 학습된 경우, 본 논문에서 제안한 방법이 Pix2Struct에 비해 30.9(83.6 vs. 52.7)의 상당한 성능 향상을 보였다. 이는 OCR 예측 및 요소 접지와 같은 사전 훈련 작업이 모델이 의미 설명을 통해 이미지의 항목을 로컬화하는 데 도움이 되기 때문이다. 유사하게, 600개의 훈련 샘플만 있는 ICDAR에서, 우리의 방법은 2M 사전 훈련 데이터로 70.7 AP를 달성하는 반면, 베이스라인 Pix2Struct는 국소화 능력의 부족으로 인해 3.3만을 얻는다. Pix2Struct는 다운스트림 태스크(PublayNet 1M & PubTables 400K)에 대해 더 많은 미세 조정 데이터가 있을 때, 객체들의 로컬화를 학습하고 양호한 성능을 얻을 수 있지만, 우리의 트레이닝 스킴 S4\\({}_{Loc}\\)은 여전히 모델에 도움이 되며, PublayNet에서는 1.5, PubTables에서는 1.3의 성능을 향상시킨다.\n' +
      '\n' +
      'S4({}_{Loc}\\) 사전 훈련의 이점은 더 많은 사전 훈련 데이터를 사용할 수 있을 때 더욱 두드러진다. 15M 데이터로 사전 훈련된 모델은 2M 사전 훈련 데이터에 비해 4개의 다운스트림 작업 모두에서 일관되게 정확도를 향상시킨다. 특히, ICDAR에서 15M 사전 훈련 데이터는 70.7에서 79.4로 정확도를 향상시키며, 이는 다운스트림 작업이 더 적은 데이터를 가질 때 사전 훈련 작업이 더 많은 이점을 갖는다는 것을 보여준다. 일반적인 자동 회귀 텍스트 생성 모델로서 15M 사전 훈련 데이터를 사용한 우리의 방법은 PublayNet 및 PubTables에서 DeTR 및 Dit-B와 같은 탐지 특정 모델과 유사한 성능을 달성하여 적절한 작업을 사용한 충분한 사전 훈련이 자동 회귀 모델과 탐지 특정 아키텍처 간의 격차를 줄이는 데 도움이 된다는 것을 보여준다.\n' +
      '\n' +
      '### 각 과제의 기여도\n' +
      '\n' +
      '우리는 스크린 파싱 외에 각 개별 사전 훈련 작업의 효과를 보여주기 위해 절제 연구를 수행했다. S4\\({}_{NL}\\)에 대해 자연어 관련 작업을 점진적으로 추가하여 ChartQA, Widget Captioning, Screen Summarization, \\(\\text{RefExp}_{\\text{cls}\\)에 대해 평가한다. S4\\({}_{Loc}\\)의 경우, 국부화 관련 작업을 점진적으로 추가하고 \\(\\text{RefExp}_{\\text{cand\\_free}\\) 및 ICDAR에 대해 평가한다. 그 결과를 Tab. 3에 나타내었다. 다운스트림 태스크는 일반적으로 가장 관련된 사전 훈련 태스크의 추가로부터 이익을 얻는다는 것을 관찰하라. 예를 들어, 화면 요약은 화면 기울임 사전 훈련 태스크가 추가될 때 2.1 성능 향상을 얻는 반면, 다른 태스크는 성능에 거의 영향을 미치지 않는다. 속성 예측 태스크는 모델이 웹사이트 요소를 텍스트 설명에 연관시키도록 유도한다. 따라서 이를 사전 학습 기법에 추가하면 Widget Captioning과 \\(\\text{RefExp}_{\\text{cls}\\)에 대한 성능이 크게 향상되며, 이는 모델이 UI 요소를 텍스트에 연결해야 한다. 이와 유사하게, 모든 국소화 관련 사전 훈련 작업을 추가하면 접지 요소에 대한 모델의 능력이 실질적으로 향상되어 ICDAR 및 \\(\\text{RefExp}_{\\text{cand\\_free}}\\) 모두에서 정확도가 향상된다.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      '우리는 다양하고 방대한 양의 웹 사이트 스크린샷에서 다양한 감독 작업을 통해 모델이 노출되는 비전 언어 모델을 위한 새로운 사전 훈련 프레임워크를 도입했다. 이 혁신은 웹 페이지가 자동으로 렌더링, 추출 및 세척되어 스크린샷 및 해당 주석을 생성하는 제안된 데이터 파이프라인에 의해 활성화된다. 사전 학습 기법의 태스크는 데이터의 주석 활용과 다운스트림 태스크 간의 유사성을 최대화하도록 설계되었다. 광범위한 실험을 통해 9개의 다른 데이터 세트에서 다운스트림 작업 성능을 높이는 방법의 유효성을 입증했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c|c|c|c|c|c} \\hline \\hline Titing & \\multicolumn{1}{c|}{Attibute Pred.} & \\multicolumn{1}{c|}{NRP} & \\multicolumn{1}{c|}{Table Parsing.} & \\multicolumn{1}{c|}{Screen Parsing} & \\multicolumn{1}{c|}{ChartQA} & Widget Cap. & Screen Sum. & \\(\\text{RefExp}_{\\text{cls}}\\) \\\\ \\hline  & & & & ✓ & 47.4 & 129.5 & 101.3 & 87.9 \\\\  & & ✓ & ✓ & 48.7 & 128.3 & 101.4 & 87.8 \\\\  & & ✓ & ✓ & ✓ & **50.7** & 128.3 & 101.3 & 89.4 \\\\  & ✓ & ✓ & ✓ & ✓ & 50.1 & **130.7** & 101.1 & **92.7** \\\\ ✓ & ✓ & ✓ & ✓ & ✓ & 50.5 & 130.5 & **103.2** & 92.4 \\\\ \\hline \\hline \\multicolumn{1}{c|}{Table Detection} & \\multicolumn{1}{c|}{Layout Analysis} & \\multicolumn{1}{c|}{Image \\& Element Grounding} & OCR & Screen Parsing & ICDAR & \\(\\text{RefExp}_{\\text{cand\\_free}}\\) \\\\ \\hline  & & & & & ✓ & 3.3 & 52.7 \\\\  & & & & ✓ & ✓ & 50.1 & 68.6 \\\\  & & ✓ & ✓ & ✓ & ✓ & 52.9 & 66.2 \\\\ ✓ & ✓ & ✓ & ✓ & ✓ & **70.7** & **83.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 2M S4 데이터를 사용하여 다른 사전 훈련 목표를 추가하는 것에 대한 절제 연구.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '- end-to-end multi-modal understanding를 위한 modulated detection. _ 2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 1760-1770, 2021.\n' +
      '* [27] Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yin, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Oc-free document understanding transformer, 2022.\n' +
      '* [28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* [29] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _International Journal of Computer Vision_, 128(7):1956-1981, 2020.\n' +
      '* [30] Gukyeong Kwon, Zhaowei Cai, Avinash Ravichandran, Erhan Bas, Rahul Bhotika, and Stefano Soatto. Masked vision and language modeling for multi-modal representation learning. _arXiv preprint arXiv:2208.02131_, 2022.\n' +
      '* [31] Justin Lazarow, Kwonjoon Lee, Kunyu Shi, and Zhuowen Tu. Learning instance occlusion for panoptic segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10720-10729, 2020.\n' +
      '* [32] Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding, 2023.\n' +
      '* [33] Gang Li and Yang Li. Spotlight: Mobile ui understanding using vision-language models with a focus. 2023.\n' +
      '* [34] Junlong Li, Yiheng Xu, Lei Cui, and Furu Wei. Markuplm: Pre-training of text and markup language for visually-rich document understanding. 2021.\n' +
      '* [35] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, 2022.\n' +
      '* [36] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jeng-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10955-10965, 2021.\n' +
      '* [37] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. _ECCV 2020_, 2020.\n' +
      '* [38] Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements, 2020.\n' +
      '* [39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.\n' +
      '* [40] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic vision-guistic representations for vision-and-language tasks. In _Neural Information Processing Systems_, 2019.\n' +
      '* [41] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. _ArXiv_, abs/2206.08916, 2022.\n' +
      '* [42] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning, 2022.\n' +
      '* [43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* [44] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.\n' +
      '* [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, 2021.\n' +
      '* [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [47] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _arXiv e-prints_, 2019.\n' +
      '* [48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.\n' +
      '* [49] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '* [50] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Lainon-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.\n' +
      '* [51] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 8430-8439, 2019.\n' +
      '* [52] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Testing prompt tuning for zero-shot generalization in vision-language models. _ArXiv_, abs/2209.07511, 2022.\n' +
      '* [53] Brandon Smock, Rohith Pesala, and Robin Abraham. PubTables-1M: Towards comprehensive table extraction from unstructured documents. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4634-4642, 2022.\n' +
      '* [54] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words: Automatic mobile ui summarization with multimodal learning, 2021.\n' +
      '* [55] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In _International Conference on Machine Learning_, 2022.\n' +
      '* [56] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In _International Conference on Machine Learning_, pages 23318-23340. PMLR, 2022.\n' +
      '* [57] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. _ArXiv_, abs/2111.02358, 2021.\n' +
      '* [58] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. _ArXiv_, abs/2208.10442, 2022.\n' +
      '* [59] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. 심심: 마스킹된 이미지 모델링을 위한 간단한 프레임워크입니다. IEEE/CVF Conference on Computer Vision and Pattern Recognition_의 _Proceedings, pages 9653-9663, 2022.\n' +
      '* [60] Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, and Fei Huang. E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning. _ArXiv_, abs/2106.01804, 2021.\n' +
      '* [61] Jinyu Yang, Jiali Duan, S. Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul M. Chilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive learning. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 15650-15659, 2022.\n' +
      '* [62] Xu Yang, Hanwang Zhang, Guojun Qi, and Jianfei Cai. Causal attention for vision-language tasks. _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9842-9852, 2021.\n' +
      '* [63] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat sense Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language models. _ArXiv_, abs/2109.11797, 2021.\n' +
      '* [64] Andy Zeng, Adrian S. Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael S. Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Peter R. Florence. Socratic models: Composing zero-shot multimodal reasoning with language. _ArXiv_, abs/2204.00598, 2022.\n' +
      '* [65] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts with visual concepts. _ArXiv_, abs/2111.08276, 2021.\n' +
      '* [66] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun-Juan Zhu, Lionel Ming shuan Ni, and Heung yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. _ArXiv_, abs/2203.03605, 2022.\n' +
      '* [67] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. _ArXiv_, abs/2206.05836, 2022.\n' +
      '* [68] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Making visual representations matter in vision-language models. _ArXiv_, abs/2101.00529, 2021.\n' +
      '* [69] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Making visual representations matter in vision-language models. _CVPR 2021_, 2021.\n' +
      '* [70] Zhaoyang Zhang, Yantao Shen, Kunyu Shi, Zhaowei Cai, Jun Fang, Siqi Deng, Hao Yang, Davide Modolo, Zhuowen Tu, and Stefano Soatto. Musketeer (all for one, and one for all): A generalist vision-language model with task explanation prompts, 2023.\n' +
      '* [71] Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. Publaynet: largest dataset ever for document layout analysis. In _2019 International Conference on Document Analysis and Recognition (ICDAR)_, pages 1015-1022. IEEE, 2019.\n' +
      '*2348, 2021.\n' +
      '* [73] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 16795-16804, 2022.\n' +
      '* [74] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and vqa. _ArXiv_, abs/1909.11059, 2019.\n' +
      '* [75] Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin, Ben Chen, Hao Zhou, Minghui Qiu, and Ling Shao. Kaleidobert: Vision-language pre-training on fashion domain. _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 12642-12652, 2021.\n' +
      '\n' +
      '풍부한 수퍼비전을 통한 비전 언어 사전 훈련 향상\n' +
      '\n' +
      'Supplementary Material\n' +
      '\n' +
      '## 부록 합동훈련의 정량적 결과\n' +
      '\n' +
      '공동 훈련 언어 및 위치 작업의 영향에 대한 몇 가지 추가 논의를 제공합니다. S4({}_{NL}\\) 목표와 S4({}_{Loc}\\) 목표를 직접 결합하는 것은 대부분의 다운스트림 작업에서 성능에 해를 끼친다. S4\\({}_{Joint}\\)의 경우, 모든 태스크에 걸쳐 최상의 평균 성능을 제공하기 때문에 다음과 같은 가중 손실을 사용한다:\n' +
      '\n' +
      'loss_weights = {\'screen2html\': 1.0, \'attribute_prediction\': 0.5, \'title_generation\': 0.5, \'node_relation_prediction\':0.1, \'table_parsing\':0.1, \'ocr\':0.1, \'table_detection\':0.1, \'layout_analysis\':0.1, \'image_grounding\':0.1, \'element_grounding\':0.1}``\n' +
      '\n' +
      '\\begin{tabular}{l|c c c|c c c c} \\hline \\hline\n' +
      '**Methods** & \\begin{tabular}{c} **Pre-training** \\\\ **Dataset** \\\\ \\end{tabular} & \\begin{tabular}{c} **Pre-training** \\\\ **Objectives** \\\\ \\end{tabular} & \\begin{tabular}{c} **Finetune** \\\\ **Batchsize** \\\\ \\end{tabular} & \\begin{tabular}{c} **ChartQA** \\(\\uparrow\\) \\\\ **RefExp\\({}_{cls}\\)\\(\\uparrow\\) \\\\ **Cap.** \\\\ \\end{tabular} & \\begin{tabular}{c} **Widget** \\(\\uparrow\\) \\\\ **Sum.** \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} **Screen** \\(\\uparrow\\) \\\\ **WebSRC** \\(\\uparrow\\) \\\\ \\end{tabular} \\\\ \\hline S4\\({}^{*}\\) **(Ours)** & S4 Data - 15M & S4\\({}_{NL}\\) & 8 & **55.0** & **94.9** & **130.6** & **105.7** & **61.1** \\\\ S4\\({}^{*}\\) **(Ours)** & S4 Data - 15M & S4\\({}_{Joint}\\) & 8 & 52.1 & **94.9** & 128.4 & 101.5 & 60.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{l|c c c|c c c c} \\hline \\hline\n' +
      '**Methods** & \\begin{tabular}{c} **Pre-training** \\\\ **Dataset** \\\\ \\end{tabular} & \\begin{tabular}{c} **Pre-training** \\\\ **Objectives** \\\\ \\end{tabular} & \\begin{tabular}{c} **RefExp** \\(\\uparrow\\) \\\\ **cand\\({}_{\\text{,free}}\\) \\\\ **30k samples** \\\\ \\end{tabular} & \\begin{tabular}{c} **PubJNet** \\(\\uparrow\\) \\\\ **(Table Det.)** \\\\ **400k samples** \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} **PubMedTables** \\(\\uparrow\\) \\\\ **(Modern Subset)** \\\\ **600 samples** \\\\ \\end{tabular} \\\\ \\hline S4\\({}^{*}\\) **(Ours)** & S4 Data - 15M & S4\\({}_{Loc}\\) & **84.3** & **93.1** & **99.0** & **79.4** \\\\ S4\\({}^{*}\\) **(Ours)** & S4 Data - 15M & S4\\({}_{Joint}\\) & 79.2 & 91.6 & 97.7 & 76.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '## 부록 B 사전교육 중 질적 결과\n' +
      '\n' +
      '우리는 질적인 사전 훈련 결과를 아래 그림으로 시각화합니다. 테이블 검출, 이미지 접지 및 요소 접지의 경우 빨간색 상자는 접지 진리 상자를 나타내고 파란색 상자는 예측 상자를 나타낸다. 이 세 가지 작업 각각에 대해 4개의 이미지를 제시하며, 오른쪽 하단은 실패 사례이고 나머지는 좋은 사례이다. 레이아웃 분석을 위해 예측(파란색)과 그라운드 트루스 레이아웃(빨간색)의 쌍을 제시한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '도 4 : 속성 예측\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:14]\n' +
      '\n' +
      '여기서 ARC Logo7을 설명하는 이미지\n' +
      '\n' +
      'Aknymy 블로그를 설명하는 이미지는 어디에 있는가? - 5가지 실행 가능한 팁은 전환 랜딩 페이지 섬네일을 증가시킵니다.\n' +
      '\n' +
      '도 6 : 이미지 접지부\n' +
      '\n' +
      '도 7 : 레이아웃 분석\n' +
      '\n' +
      '도 8 : 노드 관계\n' +
      '\n' +
      '도 9 : OCR\n' +
      '\n' +
      '도 10 : Screen2html\n' +
      '\n' +
      '도 11 : 스크린 틸트\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:21]\n' +
      '\n' +
      '도 13 : 테이블 파싱\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>