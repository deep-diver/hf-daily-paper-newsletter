<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:2]\n' +
      '\n' +
      'More information along with robot videos are available on our project website: [https://ok-robot.github.io](https://ok-robot.github.io).\n' +
      '\n' +
      '## II Technical Components and Method\n' +
      '\n' +
      'Our method, on a high level, solves the problem described by the query: "Pick up **A** (from **B**) and drop it on/in **C**", where A is an object and B and C are places in a real-world environment such as homes. The system we introduce is a combination of three primary subsystems combined on a Hello Robot: Stretch. Namely, these are the open-vocabulary object navigation module, the open-vocabulary RGB-D grasping module, and the dropping heuristic. In this section, we describe each of these components in more details.\n' +
      '\n' +
      '### _Open-home, open-vocabulary object navigation_\n' +
      '\n' +
      'The first component of our method is an open-home, open-vocabulary object navigation model that we use to map a home and subsequently navigate to any object of interest designated by a natural language query.\n' +
      '\n' +
      '**Scanning the home**: For open vocabulary object navigation, we follow the approach from CLIP-Fields [27] and assume a pre-mapping phase where the home is "scanned" manually using an iPhone. This manual scan simply consists of taking a video of the home using the Record3D app on the iPhone, which results in a sequence of posed RGB-D images.\n' +
      '\n' +
      'Alternatively, this could be done automatically using frontier-based exploration [15, 25, 26], but for speed and simplicity we prefer the manual approach [26, 27]. We take this approach since the frontier-based approaches tend to be slow and numbering, especially for a novel space, while our "scan" take less than one minute for each room. Once collected, the RGB-D images, along with the camera pose and positions, are exported to our library for map-building.\n' +
      '\n' +
      'To ensure our semantic memory contains both the objects of interest as well as the navigable surface and any obstacles, the recording must capture the floor surface alongside the objects and receptacles in the environment.\n' +
      '\n' +
      '**Detecting objects**: On each frame of the scan, we run an open-vocabulary object detector. Unlike previous works which used Detic [7], we chose OWL-ViT [8] as the object detector since we found it to perform better in preliminary queries. We apply the detector on every frame, and extract each of the object bounding box, CLIP-embedding, detector confidence, and pass them onto the object memory module of our navigation module.\n' +
      '\n' +
      'Building on top of previous work [27], we further refine the bounding boxes into object masks with Segment Anything (SAM) [28]. Note that, in many cases, open-vocabulary object detectors still require a set of natural language object queries that they try to detect. We supply a large set of such object queries, derived from the original Scannet200 labels [29], such that the detector captures most common objects in the scene.\n' +
      '\n' +
      '**Object-centric semantic memory**: We use an object-centric memory similar to Clip-Fields [27] and OVMM [25] that we call the VoxelMap. The object masks are back-projected in real-world coordinates using the depth image and the pose collected by the camera, giving us a point cloud where each point has an associated semantic vector coming from CLIP. Then, we voxelize the point cloud to a 5 cm resolution and for each voxel, calculate the detector-confidence weighted average for the CLIP embeddings that belong to that voxel. This voxel map builds the base of our object memory module. Note that the representation created this way remains static after the first scan, and cannot be adapted during the robot\'s operation. This inability to dynamically create a map is discussed in our limitations section (Section V).\n' +
      '\n' +
      '**Querying the memory module**: Our semantic object memory gives us a static world representation represented as possibly non-empty voxels in the world, and a semantic CLIP vector associated with each voxel. Given a language query, we convert it to a semantic vector using the CLIP language encoder. Then, we find the top voxel where the dot product between the encoded vector and the voxel\'s semantic representation is maximized. Since each voxel is associated with a real location in the home, this lets us find the location where a queried object is most likely to be found, similar to Figure 2(a).\n' +
      '\n' +
      'When necessary, we implement "A on B" as "A near B". We do so by selecting top-10 points for query A and top-50 points for query B. Then, we calculate the \\(10\\times 50\\) pairwise euclidean distances and pick the A-point associated with the shortest (A, B) distance. Note that, during the object navigation phase, we use this query only to navigate to the object approximately, and not for manipulation.\n' +
      '\n' +
      'This approach gives us two advantages: our map can be as lower resolution than those in prior work [26, 27, 30], and we can deal with small movements in object\'s location after building the map.\n' +
      '\n' +
      '**Navigating to objects in the real world**: Once our navigation model gives us a 3D location coordinate in the real world, we use that as a navigation target for our robot to initialize our manipulation phase. In previous works [15, 27, 31], the navigation objective was to go and look at an object, which can be done while staying at a safe distance from the object itself. In contrast, our navigation module must place the robot at an arms length so that the robot can manipulate the target object afterwards. Thus, our navigation method has to balance the following objectives:\n' +
      '\n' +
      '1. the robot needs to be close enough to the object to manipulate it,\n' +
      '2. the robot needs some space to move its gripper, so there needs to be a small but non-negligible space between the robot and the objectm and\n' +
      '3. the robot needs to avoid collision during manipulation, and thus needs to keep its distance from all obstacles.\n' +
      '\n' +
      'We use three different navigation score functions, each associated with one of our previous concerns, and evaluate them on each point of the space to find the best position to place the robot.\n' +
      '\n' +
      'Let a random point be \\(\\vec{x}\\), the closest obstacle point as \\(\\vec{x}_{obs}\\), and the target object as \\(\\vec{x}_{o}\\). Then, we can define the following three functions \\(s_{1},s_{2},s_{3}\\) to capture our three criterion. Then, their weighted sum \\(s\\) to find the ideal navigation point \\(\\vec{x^{*}}\\) in our space that minimizes \\(s(\\vec{x})\\), and the direction is towards the vector from \\(\\vec{x^{*}}\\) to \\(\\vec{x_{o}}\\).\n' +
      '\n' +
      '\\[s_{1}(\\vec{x}) =||\\vec{x}-\\vec{x}_{o}||\\] \\[s_{2}(\\vec{x}) =40-\\min(||\\vec{x}-\\vec{x}_{o}||,40)\\] \\[s_{3}(\\vec{x}) =\\begin{cases}1/||\\vec{x}-\\vec{x}_{obs}||,&\\text{if }||\\vec{x}-\\vec{x}_{obs}||_{0}\\leq 30 \\\\ 0,&\\text{otherwise}\\end{cases}\\] \\[s(\\vec{x}) =s_{1}(\\vec{x})+8s_{2}(\\vec{x})+8s_{3}(\\vec{x})\\]\n' +
      '\n' +
      'To navigate to this target point safely from any other point in space, we follow a similar approach to [26, 32] by building an obstacle map from our previously captured posed RGB-D images. We build a 2D, 10cm\\(\\times\\)10cm grid of obstacles over which we navigate using the A* algorithm. To convert our voxel map to an obstacle map, we first set a floor and ceiling height. Presence of occupied voxels in between them implies the grid cell is occupied, while presence of neither ceiling nor floor voxels mean that the grid cell is unexplored. We mark both occupied or unexplored cells as not navigable. Around each occupied point, we mark any point within a 20 cm radius as also non-navigable to account for the robot\'s radius and a turn radius. In our A* algorithm, we use the \\(s_{3}\\) function as a heuristic on the node costs to navigate further away from any obstacles, which makes our generated paths similar to ideal Voronoi paths [33] in our experiments.\n' +
      '\n' +
      '### _Open-vocabulary grasping in the real world_\n' +
      '\n' +
      'Unlike open-vocabulary navigation, for grasping, our method needs to physically interact with arbitrary objects in the real world, which makes this part significantly more difficult. As a result, we opt for using a pre-trained grasping model to generate grasp poses in the real world, and filter that with language-conditioning using a modern VLM.\n' +
      '\n' +
      '**Grasp perception**: Once the robot reaches the object using the navigation method outlined in Section II-A, we use a pre-trained grasping model or heuristic to generate a grasp for the robot. We point the robot\'s RGB-D head camera towards the object\'s location in space, as given to us by our semantic memory module, and capture an RGB-D image from it (Figure 3, column 1). We backproject and convert the depth image to a pointcloud as necessary. Then, we pass this information to our grasp generation module. The grasp generation module that we use in our work is AnyGrasp [19], which generates collision free grasps with a parallel jaw gripper in a scene given a single RGB image and a pointcloud.\n' +
      '\n' +
      'AnyGrasp provides us with possible grasps in the scene (Figure 3 column 2) with grasp point, width, height, depth, and a "graspness score", which indicates uncalibrated model confidence in each grasp. However, such modules generally generate all possible grasps in a scene, which we need to filter using the language query.\n' +
      '\n' +
      '**Filtering grasps using language queries**: Once we get all proposed grasps from AnyGrasp, we filter the grasps using LangSam [24]. We use LangSam [24] to segment the captured image and get the desired object\'s mask with the language query (Figure 3 column 3). Then, we project all the proposed grasp points onto the image and find the grasps that fall into the object mask (Figure 3 column 4).\n' +
      '\n' +
      'We pick the best grasp using a heuristic, where if the grasp score is \\(\\mathcal{S}\\) and the angle between the grasp normal and floor normal is \\(\\theta\\), then the new heuristic score is \\(\\mathcal{S}-(\\nicefrac{{\\theta}}{{10}})\\). This heuristic prioritizes grasps with the highest graspness score but also a horizontally flat proposed grasp. We prefer horizontal grasps because they are robust to small calibration errors on the robot, while vertical grasps need to be quite point-accurate to be successful. Being robust to hand-eye calibration errors is a desired property as we transport the robot to different homes\n' +
      '\n' +
      'Fig. 2: Open-vocabulary, open knowledge object localization and navigation in the real-world. We use the VoxelMap [25] for localizing objects with natural language queries, and use an A* algorithm similar to USANet [26] for path planning.\n' +
      '\n' +
      'over the course of our experiments.\n' +
      '\n' +
      '**Grasp execution**: Once we identify the best grasp (Figure 3 column 5), we use a simple pre-grasp approach [34] to grasp our intended object. Let\'s assume that \\(\\overrightarrow{p}\\) is the grasp point and \\(\\overrightarrow{a}\\) is the approach vector given by our grasping model. Then, our robot gripper follows the following trajectory:\n' +
      '\n' +
      '\\[\\langle\\overrightarrow{p}-0.2\\overrightarrow{a},\\overrightarrow{p}-0.08 \\overrightarrow{a},\\overrightarrow{p}-0.04\\overrightarrow{a},\\overrightarrow {p}\\rangle\\]\n' +
      '\n' +
      'Put simply, our method approaches the object from a pre-grasp position in a line with progressively smaller motions. Moving slower as we approach the object is important since the robot can knock over light objects otherwise. Once we reach the predicted grasp point, we close the gripper in a close loop fashion to make sure we can get a solid grip on the object without crushing it. Finally, after grasping the object, we lift up the robot arm, retract it fully, and rotate the wrist to have the object tucked over the body. This behavior maintains the robot footprint while ensuring the object is held securely by the robot and doesn\'t fall while navigating to the drop location.\n' +
      '\n' +
      '### _Dropping heuristic_\n' +
      '\n' +
      'After picking up an object, we find and navigate to the location to drop it using the same methods as described in Section II-A. Unlike in HomeRobot\'s baseline implementation [25], which assumes that the drop-off location is a flat surface, we extend our heuristic to also cover concave objects such as sink, bins, boxes, and bags. First, we segment the point cloud \\(P\\) captured by the robot\'s head camera using LangSam [24] similar to Section II-B using the drop language query. Then, we align that segmented point cloud such that X-axis is aligned with the way the robot is facing, Y-axis is to its left and right, and the Z-axis of the point cloud is aligned with the floor normal. We call this aligned pointcloud \\(P_{a}\\). Finally, we normalize the point cloud so that the robot\'s X- and Y- coordinate is \\((0,0)\\), and the floor plane is at \\(z=0\\). On the aligned, segmented point cloud, we consider the X- and Y-coordinates for each point, and find the respective medians on each axis that we call \\(x_{m}\\) and \\(y_{m}\\). Finally, we find a drop height using \\(z_{\\max}=0.2+\\max\\{z\\mid(x,y,z)\\in P_{a};0\\leq x\\leq x_{m};|y-y_{m}|<0.1\\}\\) on the segmented, aligned pointcloud. We add a small buffer of \\(0.2\\) to the height to avoid collisions between the robot and the drop location. Finally, we move the robot gripper above the drop point, and open the gripper to drop the object. While this heuristic sometimes fails to place an object on a cluttered surface, in our experiments it performs well on average.\n' +
      '\n' +
      '### _Deployment in homes_\n' +
      '\n' +
      'Once we have our navigation, pick, and drop primitive in place, we combine them directly to create our robot method that can be applied in any novel home directly. For a new home environment, we can "scan" the room in under a minute. Then, it takes less than five minutes to process that into our VoxelMap. For our ablations, it takes about 50 minutes to train the necessary implicit semantic fields/SDF models such as CLIP-Fields or USA-Net if we are using them. Once that is\n' +
      '\n' +
      'Fig. 3: Open-vocabulary grasping in the real world. From left to right, we show the (a) robot POV image, (b) all suggested grasps from AnyGrasp [19], (c) object mask given label from LangSam [24], (d) grasp points filtered by the mask, and (e) grasp chosen for execution.\n' +
      '\n' +
      'done, the robot can be immediately placed at the base and start operating. From arriving into a completely novel environment to start operating autonomously in it, our system takes under 10 minutes on average to complete the first pick-and-drop task.\n' +
      '\n' +
      '**State machine model**: The transition between different modules happens automatically, in a predefined fashion, once a user specifies the object to pick and where to drop it. Since we do not implement error detection or correction, our state machine model is a simple linear chain of steps leading from navigating to object, to grasping, to navigating to goal, and to dropping the object at the goal to finish the task.\n' +
      '\n' +
      '**Protocol for home experiments**: To run our experiment in a novel home, we first move the robot to a previously unobserved room. There, we record the scene and create our VoxelMap. Concurrently, we arbitrarily pick between 10-20 objects in each scene that can fit in the robot gripper. These are objects "found" in the scene, and are not ones selected beforehand. We come up with a language query for each chosen object using GPT-4V [35] to keep the queries consistent and free of experimenter bias. The effect of different queries for the same object on OK-Robot is discussed in Section III-D. Then, we query our navigation module to filter out all the navigation failures; i.e. objects whose location could not be found by our semantic memory module. Then, we execute pick-and-drop on remaining objects sequentially, without resets between trials.\n' +
      '\n' +
      '## III Experiments\n' +
      '\n' +
      'We evaluate our method in two set of experiments. On the first set of experiments, we evaluate between multiple alternatives for each of our navigation and manipulation modules. These experiments give us insights about which modules to use and evaluate in a home environment as a part of our method. On the next set of experiments, we took our robots to 10 homes and ran 171 pick-and-drop experiments to empirically evaluate how our method performs in completely novel homes, and to understand the failure modes of our system.\n' +
      '\n' +
      'Through these experiments, we look to answer a series of questions regarding the capabilities and limits of current Open Knowledge robotic systems, as embodied by OK-Robot. Namely, we ask the following:\n' +
      '\n' +
      '1. How well can such a system tackle the challenge of pick and drop in arbitrary homes?\n' +
      '2. How well do alternate primitives for navigation and grasping compare to the recipe presented here for building an Open Knowledge robotic system?\n' +
      '3. How well can our current systems handle unique challenges that make homes particularly difficult, such as clutter, ambiguity, and affordance challenges?\n' +
      '4. What are the failure modes of such a system and its individual components in real home environments?\n' +
      '\n' +
      '### _List of home experiments_\n' +
      '\n' +
      'Over the 10 home environment, OK-Robot achieved a 58.5% success rates in completing full pick-and-drops. Notably, this is a zero-shot algorithm, and the success rate is over novel objects sourced from each home. As a result, each of the success and the failure of the robot tells us something interesting about applying open-knowledge models in robotics, which is what we analyze over the next sections.\n' +
      '\n' +
      'In Appendix C, we provide the details of all our home experiments and results from the same, and in Appendix B we show a subset of the objects OK-Robot operated on. Snippets of our experiments are in Figure 1, and full videos can be seen on our project website.\n' +
      '\n' +
      '### _Ablations over system components_\n' +
      '\n' +
      'Apart from the navigation and manipulation strategies that we used in the home experiments, we also evaluated a number of alternative semantic memory module and open vocabulary navigation modules. We compared them by evaluating them in three different environment setups in our lab.\n' +
      '\n' +
      '**Alternate semantic navigation strategies**: We evaluate the following semantic memory modules:\n' +
      '\n' +
      '* **VoxelMap [25]:** VoxelMap converts every detected object to a semantic vector and stores such info into an associated voxel. Occupied voxels serve as an obstacle map.\n' +
      '* **CLIP-Fields [27]:** CLIP-Fields converts a sequence of posed RGB-D images to a semantic vector field by using open-label object detectors and semantic language embedding models. The result associates each point in the space with two semantic vectors, one generated via a VLM [9], and another generated via a language model [36], which is then embedded into a neural field [37].\n' +
      '* **USA-Net [26]:** USA-Net generates multi-scale CLIP features and embeds them in a neural field that also doubles as a signed distance field. As a result, a single model can support both object retrieval and navigation.\n' +
      '\n' +
      'We compare them in the same three environments with a fixed set of queries, the results of which are shown in Figure 5.\n' +
      '\n' +
      '**Alternate grasping strategies**: Similarly, we compare multiple grasping strategies to find out the best grasping strategy for our method.\n' +
      '\n' +
      '* **AnyGrasp [19]:** AnyGrasp is a single view RGB-D based grasping model. It is trained on the GraspNet dataset which contains 1B grasp labels.\n' +
      '* **Open Graspness [19]:** Since the AnyGrasp model is free but not open source, we use an open licensed baseline trained on the same dataset.\n' +
      '* **Contact-GraspNet [16]:** We use Contact-GraspNet as a prior work baseline, which is trained on the Acronym [38] dataset. One limitation of Contact-GraspNet is that it was trained on a fixed camera view for a tabletop setting. As a result, in our application with a moving camera and arbitrary locations, it failed to give us meaningful grasps.\n' +
      '* **Top-down grasp [25]:** As a heuristic based baseline, we compare with the top-down heuristic grasp provided in the HomeRobot project.\n' +
      '\n' +
      'In Figure 5, we see their comparative performance in three lab environments. For semantic memory modules, we see that VoxelMap, used in OK-Robot and described in Sec. II-A,outperforms other semantic memory modules by a small margin. It also has much lower variance compared to the alternatives, meaning it is more reliable. As for grasping modules, AnyGrap clearly outperforms other grasping methods, performing almost 50% better in a relative scale over the next best candidate, top-down grasp. However, the fact that a heuristic-based algorithm, top-down grasp from HomeRobot [25] beats the open-source AnyGrasp baseline and Contact-GraspNet shows that building a truly general-purpose grasping model remains difficult.\n' +
      '\n' +
      '### _Impact of clutter, object ambiguity, and affordance_\n' +
      '\n' +
      'What makes home environments especially difficult compared to lab experiments is the presence of physical clutter, language-to-object mapping ambiguity, and hard-to-reach positions. To gain a clear understanding of how such factors play into our experiments, we go through two "clean-up" processes in each environment. During the clean-up, we pick a subset of objects that are free from ambiguity from the previous rounds, clean the clutter around objects, and generally relocated them in an accessible locations. We go through two of such clean-up rounds at each environment, which gives us insights about the performance gap caused by the natural difficulties of a home-like environment.\n' +
      '\n' +
      'Fig. 4: All the success and failure cases in our home experiments, aggregated over all three cleaning phases, and broken down by mode of failure. From left to right, we show the application of the three components of OK-Robot, and show a breakdown of the long-tail failure modes of each of the components.\n' +
      '\n' +
      'Fig. 5: Ablation experiment using different semantic memory and grasping modules, with the bars showing average performance and the error bars showing standard deviation over the environments.\n' +
      '\n' +
      'Fig. 6: Failure modes of our method in novel homes, broken down by the failures of the three modules and the cleanup levels.\n' +
      '\n' +
      'We show a complete analysis of the tasks listed section III-A which failed in various stages in Figure 6. As we can see from this breakdown, as we clean up the environment and remove the ambiguous objects, the navigation accuracy goes up, and the total error rate goes down from 15% to 12% and finally all the way down to 4%. Similarly, as we clean up clutters from the environment, we find that the manipulation accuracy also improves and the error rates decrease from 25% to 16% and finally 13%. Finally, since the drop-module is agnostic of the label ambiguity or manipulation difficulty arising from clutter, the failure rate of the dropping primitive stays roughly constant through the three phases of cleanup.\n' +
      '\n' +
      '### _Understanding the performance of OK-Robot_\n' +
      '\n' +
      'While our method can show zero-shot generalization in completely new environments, we probe OK-Robot to better understand its failure modes. Primarily, we elaborate on how our model performed in novel homes, what were the biggest challenges, and discuss potential solutions to them.\n' +
      '\n' +
      'We first show a coarse-level breakdown of the failures, only considering the three high level modules of our method in Figure 6. We see that generally, the leading cause of failure is our manipulation failure, which intuitively is the most difficult as well. However, at a closer look, we notice a long tail of failure causes, which is presented in figure 4.\n' +
      '\n' +
      'We see that the leading three cause of failures are failing to retrieve the right object to navigate to from the semantic memory (9.3%), getting a difficult pose from the manipulation module (8.0%), and hardware difficulties (7.5%). In this section, we go over the analysis of the failure modes presented in Figure 4 and discuss the most frequent cases.\n' +
      '\n' +
      '**Natural language queries for objects**: One of the primary reasons our OK-Robot can fail is when a natural language query given by the user doesn\'t retrieve the intended object from the semantic memory. In Figure 7 we show how some queries may fail while semantically very similar but slightly modified wording of the same query might succeed.\n' +
      '\n' +
      'Generally, this has been the case for scenes where there are multiple visually or semantically similar objects, as shown in the figure. There are other cases where some queries may pass while other very similar queries may fail. An interactive system that gets confirmation from the user as it retrieves an object from memory would avoid such issues.\n' +
      '\n' +
      '**Grasping module limitations**: One potential failure mode of our system is that our manipulation is performed by executing the outputs of a pre-trained model-generated grasps that are predicted based on a single RGB-D image, with a model that wasn\'t designed for the Hello Robot: Stretch gripper.\n' +
      '\n' +
      'As a result, sometimes such grasps are unreliable or unrealistic, as shown in Figure 8. There are cases where the proposed grasp is infeasible given the robot joint limits, or is simply too far from the robot body. Development of better heuristics will let us sample better grasps for a given object. In some other cases, the model generates a good grasp pose, but as the robot is executing the grasping primitive, it collides with some minor environment obstacle. Since we do not plan the grasp trajectory, and instead try to apply the same grasp trajectory in every case, some such failures are inevitable. Better grasping models that generates a grasp trajectory as well as a pose may solve such issues. Finally, our grasping module struggles with flat objects categorically, like chocolate bars and books, since it\'s difficult to grasp them off a surface with a two-fingered gripper.\n' +
      '\n' +
      '**Robot hardware limitations**: While our robot of choice, a\n' +
      '\n' +
      'Fig. 8: Samples of failures of our manipulation module. Most failures stem from using only a single RGB-D view to generate the grasp and the limiting form-factor of a large two-fingered parallel jaw gripper.\n' +
      '\n' +
      'Fig. 7: Samples of failed or ambiguous language queries into our semantic memory module. Since the memory module depends on pretrained large vision language model, its performance shows susceptibility to particular “incentations” similar to current LLMs.\n' +
      '\n' +
      'Hello Robot: Stretch, is able to pick-and-drop a number of objects, there are certain hardware limitations that determines what the robot can and cannot manipulate. For example, the robot has a 1 kg (2 lbs) payload limit when the arm is fully extended, and as such our method is unable to move objects like a full dish soap container. Similarly, objects that are far from navigable floor space, such as in the middle of a bed, or on high places, is difficult for the robot to reach because of the reach limits of the arm. Finally, in some situations, the robot hardware or the RealSense camera can become miscalibrated over time, especially during continuous testing in homes. This miscalibration can lead to error since the manipulation module requires hand-eye coordination in the robot.\n' +
      '\n' +
      '## IV Related Works\n' +
      '\n' +
      '### _Vision-Language models for robotic navigation_\n' +
      '\n' +
      'Early applications of pre-trained open-knowledge models has been in open-vocabulary navigation. Navigating to various objects is an important task which has been looked at in a wide range of previous works [25, 31, 39], as well as in the context of longer pick-and-place tasks [40, 41]. However, these methods have generally been applied to relatively small numbers of objects [42]. Recently, Objavares [43] has shown navigation to thousands of object types, for example, but much of this work has been restricted to simulated or highly controlled environments.\n' +
      '\n' +
      'The early work addressing this problem builds upon representations derived from pre-trained vision language models, such as SemAbs [44], CLIP-Fields [27], VLMaps [45], NLMap-SayCan [46], and later, ConceptFusion [47] and LERF [30]. Most of these models show object localization in pre-mapped scenes, while CLIP-Fields, VLMaps, and NLMap-SayCan show integration with real robots for indoor navigation tasks. USA-Nets [26] extends this task to include an affordance model, navigating with open-vocabulary queries while doing object avoidance. ViNT [48] proposes a foundation model for robotic navigation which can be applied to vision-language navigation problems. More recently, GOAT [31] was proposed as a modular system for "going to anything" and navigating to any object in any environment. ConceptGraphs [49] proposed an open scene representation capable of handling complex queries using LLMs and creating a scene graph.\n' +
      '\n' +
      '### _Pretrained robot manipulation models_\n' +
      '\n' +
      'While humans can frequently look at objects and immediately know how to grasp it, such grasping knowledge is not easily accessible to robots. Over the years, there has been many works that has focused on creating such a general robot grasp generation model [50, 51, 52, 53, 54, 55] for arbitrary objects and potentially cluttered scenes via learning methods. Our work focuses on more recent iterations of such methods [16, 19] that are trained on large grasping datasets [18, 38]. While these models only perform one task, namely grasping, they predict grasps across a large object surface and thus enable downstream complex, long-horizon manipulation tasks [20, 21, 56].\n' +
      '\n' +
      'More recently, there is a set of general-purpose manipulation models moving beyond just grasping [57, 58, 59, 60, 61]. Some of these works perform general language-conditioned manipulation tasks, but are largely limited to a small set of scenes and objects. HACMan [62] demonstrates a larger range of object manipulation capabilities, focused on pushing and prodding. In the future, such models could expand the reach of our system.\n' +
      '\n' +
      '### _Open vocabulary robot systems_\n' +
      '\n' +
      'Many recent works have worked on language-enabled tasks for complex robot systems. Some examples include language conditioned policy learning [57, 63, 64, 65], learning goal-conditioned value functions [3, 66], and using large language models to generate code [67, 68, 69]. However, a fundamental difference remains between systems which aim to operate on arbitrary objects in an open-vocab manner, and systems where one can specify one among a limited number of goals or options using language. Consequently, Open-Vocabulary Mobile Manipulation has been proposed as a key challenge for robotic manipulation [25]. There has previously been efforts to build such a system [70, 71]. However, unlike such previous work, we try to build everything on an open platform and ensure our method can work without having to re-train anything for a novel home. Recently, UniTeam [23] won the 2023 HomeRobot OVMM Challenge [22] with a modular system doing pick-and-place to arbitrary objects, with a zero-shot generalization requirement similar to ours.\n' +
      '\n' +
      'In parallel, recently, there have been a number of papers doing open-vocabulary manipulation using GPT or especially GPT4 [35]. GPT4V can be included in robot task planning frameworks and used to execute long-horizon robot tasks, including ones from human demonstrations [72]. ConceptGraphs [49] is a good recent example, showing complex object search, planning, and pick-and-place capabilities to open-vocabulary objects. SayPlan [73] also shows how these can use used together with a scene graph to handle very large, complex environments, and multi-step tasks; this work is complementary to ours, as it doesn\'t handle how to implement pick and place.\n' +
      '\n' +
      '## V Limitations, Open Problems and Request for Research\n' +
      '\n' +
      'While our method shows significant success in completely novel home environments, it also shows many places where such methods can improve. In this section, we discuss a few of such potential improvement in the future.\n' +
      '\n' +
      '### _Live semantic memory and obstacle maps_\n' +
      '\n' +
      'All the current semantic memory modules and obstacle map builders build a static representation of the world, without a good way of keeping it up-to-date as the world changes. However, homes are dynamic environments, with many small changes over the day every day. Future research that can build a dynamic semantic memory and obstacle map would unlock potential for continuous application of such pick-and-drop methods in a novel home out of the box.\n' +
      '\n' +
      '### _Grasp plans instead of proposals_\n' +
      '\n' +
      'Currently, the grasping module proposes generic grasps without taking the robot\'s body and dynamics into account. Similarly, given a grasp pose, often the open loop grasping trajectory collides with environmental obstacles, which can be easily improved by using a module to generate grasp plans rather than grasp poses only.\n' +
      '\n' +
      '### _Improving interactivity between robot and user_\n' +
      '\n' +
      'One of the major causes of failure in our method is in navigation: where the semantic query is ambiguous and the intended object is not retrieved from the semantic memory. In such ambiguous cases, interaction with the user would go a long way to disambiguate the query and help the robot succeed more often.\n' +
      '\n' +
      '### _Detecting and recovering from failure_\n' +
      '\n' +
      'Currently, we observe a multiplicative error accumulation between our modules: if any of our independent components fail, the entire process fails. As a result, even if our modules each perform independently at or above 80% success rate, our final success rate can still be below 60%. However, with better error detection and retrying algorithms, we can recover from much more single-stage errors, and similarly improve our overall success rate [23].\n' +
      '\n' +
      '### _Robustifying robot hardware_\n' +
      '\n' +
      'While Hello Robot - Stretch [74] is an affordable and portable platform on which we can implement such an open-home system for arbitrary homes, we also acknowledge that with robust hardware such methods may have vastly enhanced capacity. Such robust hardware may enable us to reach high and low places, and pick up heavier objects. Finally, improved robot odometry will enable us to execute much more finer grasps than is possible today.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'NYU authors are supported by grants from Amazon, Honda, and ONR award numbers N00014-21-1-2404 and N00014-21-1-2758. NMS is supported by the Apple Scholar in AI/ML Fellowship. LP is supported by the Packard Fellowship. Our utmost gratitude goes to our friends and colleagues who helped us by hosting our experiments in their homes. Finally, we thank Jay Vakil, Siddhant Haldar, Paula Pascual and Ulyana Piterbarg for valuable feedback and conversations.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Lerrel Pinto and Abhinav Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. _ICRA_, 2016.\n' +
      '* [2] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. _The International journal of robotics research_, 37(4-5):421-436, 2018.\n' +
      '* [3] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as I can, not as I say: Grounding language in robotic affordances. _Conference on Robot Learning (CoRL)_, 2022.\n' +
      '* [4] Nur Muhammad Mahi Shafullah, Anant Rai, Haritheja Etukuru, Yiqian Liu, Ishan Misra, Soumith Chintala, and Lerrel Pinto. On bringing robots home, 2023.\n' +
      '* [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. _arXiv preprint arXiv:2212.06817_, 2022.\n' +
      '* [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. _arXiv preprint arXiv:2307.15818_, 2023.\n' +
      '* [7] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krahenbuhl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In _European Conference on Computer Vision_, pages 350-368. Springer, 2022.\n' +
      '* [8] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. Simple open-vocabulary object detection with vision transformers. In _European Conference on Computer Vision_, pages 728-755. Springer, 2022.\n' +
      '* [9] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning (ICML)_, volume 139, pages 8748-8763, 2021.\n' +
      '* [10] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In _Proceedings of the IEEE/cvf conference on computer vision and pattern recognition_, pages 3195-3204, 2019.\n' +
      '* [11] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Mieech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022.\n' +
      '* [12] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding DINO: Marrying DINO with grounded pre-training for open-set object detection. _arXiv preprint arXiv:2303.05499_, 2023.\n' +
      '\n' +
      '* [13] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n' +
      '* [14] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. _OpenAI Blog_, 2019.\n' +
      '* [15] Theophile Gervet, Soumith Chintala, Dhruv Batra, Jitendra Malik, and Devendra Singh Chaplot. Navigating to objects in the real world. _Science Robotics_, 8(79):eadf6991, 2023.\n' +
      '* [16] Martin Sundermeyer, Arsalan Mousavian, Rudolph Triebel, and Dieter Fox. Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 13438-13444. IEEE, 2021.\n' +
      '* [17] Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, and Ken Goldberg. Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. _arXiv preprint arXiv:1703.09312_, 2017.\n' +
      '* [18] Hao-Shu Fang, Chenxi Wang, Minghao Gou, and Cewu Lu. Graspnet-1billion: a large-scale benchmark for general object grasping. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11444-11453, 2020.\n' +
      '* [19] Hao-Shu Fang, Chenxi Wang, Hongjie Fang, Minghao Gou, Jirong Liu, Hengxu Yan, Wenhai Liu, Yichen Xie, and Cewu Lu. Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. _IEEE Transactions on Robotics_, 2023.\n' +
      '* [20] Ankit Goyal, Arsalan Mousavian, Chris Paxton, Yu-Wei Chao, Brian Okorn, Jia Deng, and Dieter Fox. Ifor: Iterative flow minimization for robotic object rearrangement. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14787-14797, 2022.\n' +
      '* [21] Weiyu Liu, Tucker Hermans, Sonia Chernova, and Chris Paxton. Structdiffusion: Object-centric diffusion for semantic rearrangement of novel objects. _arXiv preprint arXiv:2211.04604_, 2022.\n' +
      '* [22] Sriram Yenamandra, Arun Ramachandran, Mukul Khanna, Karmesh Yadav, Devendra Singh Chaplot, Gunjan Chhablani, Alexander Clegg, Theophile Gervet, Vidhi Jain, Ruslan Partsey, Ram Ramrakhya, Andrew Szot, Tsung-Yen Yang, Aaron Edsinger, Charlie Kemp, Binit Shah, Zsolt Kira, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, and Chris Paxton. The homerobot open vocab mobile manipulation challenge. In _Thirty-seventh Conference on Neural Information Processing Systems: Competition Track_, 2023.\n' +
      '* [23] Andrew Melnik, Michael Buttner, Leon Harz, Lyon Brown, Gora Chand Nandi, Arjun PS, Gaurav Kumar Yadav, Rahul Kala, and Robert Haschke. Uniteam: Open vocabulary mobile manipulation challenge. _arXiv preprint arXiv:2312.08611_, 2023.\n' +
      '* [24] Luca Medeiros. Lang segment anything. [https://github.com/luca-medeiros/lang-segment-anything](https://github.com/luca-medeiros/lang-segment-anything), 2023.\n' +
      '* [25] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, et al. Homerobot: Open-vocabulary mobile manipulation. _arXiv preprint arXiv:2306.11565_, 2023.\n' +
      '* [26] Benjamin Bolte, Austin Wang, Jimmy Yang, Mustafa Mukadam, Mrinal Kalakrishnan, and Chris Paxton. Usanet: Unified semantic and affordance representations for robot memory. _arXiv preprint arXiv:2304.12164_, 2023.\n' +
      '* [27] Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel Pinto, Soumith Chintala, and Arthur Szlam. Clip-fields: Weakly supervised semantic fields for robotic memory. _arXiv preprint arXiv:2210.05663_, 2022.\n' +
      '* [28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In _ICCV_, pages 4015-4026, October 2023.\n' +
      '* [29] David Rozenbergszki, Or Litany, and Angela Dai. Language-grounded indoor 3d semantic segmentation in the wild, 2022.\n' +
      '* [30] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 19729-19739, 2023.\n' +
      '* [31] Matthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min, Kavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra, et al. Goat: Go to any thing. _arXiv preprint arXiv:2311.06430_, 2023.\n' +
      '* [32] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Audio visual language maps for robot navigation. _arXiv preprint arXiv:2303.07522_, 2023.\n' +
      '* [33] Santiago Garrido, Luis Moreno, Mohamed Abderrahim, and Fernando Martin. Path planning for mobile robot navigation using voronoi diagram and fast marching. In _2006 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 2376-2381. IEEE, 2006.\n' +
      '* [34] Sudeep Dasari, Abhinav Gupta, and Vikash Kumar. Learning dexterous manipulation from exemplar object trajectories and pre-grasps, 2023.\n' +
      '* [35] OpenAI. GPT-4 technical report, 2023.\n' +
      '* [36] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks, 2019.\n' +
      '* [37] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _European Conference on Computer Vision (ECCV)_, 65(1):99-106, 2020.\n' +
      '* [38] Clemens Eppner, Arsalan Mousavian, and Dieter Fox. Acronym: A large-scale grasp dataset based on simulation. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 6222-6227. IEEE, 2021.\n' +
      '* [39] Arsalan Mousavian, Alexander Toshev, Marek Fiser, Jana Kosecka, Ayzaan Wahid, and James Davidson. Visual representations for semantic target driven navigation. In2019 International Conference on Robotics and Automation (ICRA)_, pages 8846-8852. IEEE, 5 2019.\n' +
      '* [40] Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and Yoav Artzi. A persistent spatial semantic representation for high-level natural language instruction execution. In _Conference on Robot Learning_, pages 706-717. PMLR, 2022.\n' +
      '* [41] So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan Bisk, and Ruslan Salakhutdinov. Film: Following instructions in language with modular methods. _arXiv preprint arXiv:2110.07342_, 2021.\n' +
      '* [42] Matt Deitke, Dhruv Batra, Yonatan Bisk, Tommaso Campari, Angel X Chang, Devendra Singh Chaplot, Changan Chen, Claudia Perez D\'Arpino, Kiana Ehsani, Ali Farhadi, et al. Retrospectives on the embodied ai workshop. _arXiv preprint arXiv:2210.06849_, 2022.\n' +
      '* [43] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13142-13153, 2023.\n' +
      '* [44] Huy Ha and Shuran Song. Semantic abstraction: Open-world 3d scene understanding from 2d vision-language models, 2022.\n' +
      '* [45] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps for robot navigation. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 10608-10615. IEEE, 2023.\n' +
      '* [46] Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana Gopalakrishnan, Michael S. Ryoo, Austin Stone, and Daniel Kappler. Open-vocabulary queryable scene representations for real world planning. In _arXiv preprint arXiv:2209.09874_, 2022.\n' +
      '* [47] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer, Soroush Sayazadi, Nikhil Keetha, Ayush Tewari, et al. Conceptfusion: Open-set multimodal 3d mapping. _arXiv preprint arXiv:2302.07241_, 2023.\n' +
      '* [48] Dhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Stachowicz, Kevin Black, Noriaki Hirose, and Sergey Levine. ViNT: A Foundation Model for Visual Navigation. In _7th Annual Conference on Robot Learning (CoRL)_, 2023.\n' +
      '* [49] Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, et al. Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. _arXiv preprint arXiv:2309.16650_, 2023.\n' +
      '* [50] Abhinav Gupta, Adithyavairavan Murali, Dhiraj Prakashchand Gandhi, and Lerrel Pinto. Robot learning in homes: Improving generalization and reducing dataset bias. _Advances in Neural Information Processing Systems_, 31:9094-9104, 2018.\n' +
      '* [51] Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, and Ken Goldberg. Dex-Net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. In _Robotics: Science and Systems (RSS)_, 2017.\n' +
      '* [52] Jeffrey Mahler, Matthew Matl, Xinyu Liu, Albert Li, David Gealy, and Ken Goldberg. Dex-net 3.0: Computing robust robot vacuum suction grasp targets in point clouds using a new analytic model and deep learning, 2018.\n' +
      '* [53] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. QT-Opt: Scalable deep reinforcement learning for vision-based robotic manipulation. _arXiv preprint arXiv:1806.10293_, 2018.\n' +
      '* [54] Yuzhe Qin, Rui Chen, Hao Zhu, Meng Song, Jing Xu, and Hao Su. S4g: Amodal single-view single-shot se(3) grasp detection in cluttered scenes, 2019.\n' +
      '* [55] Arsalan Mousavian, Clemens Eppner, and Dieter Fox. 6-dof graspnet: Variational grasp generation for object manipulation, 2019.\n' +
      '* [56] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, and D. Fox. Progr prompt: Generating situated robot task plans using large language models. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, page 11523, 2023.\n' +
      '* [57] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-Actor: A multi-task transformer for robotic manipulation. In _CoRL_, pages 785-799. PMLR, 2023.\n' +
      '* [58] Priyam Parashar, Jay Vakil, Sam Powers, and Chris Paxton. Spatial-language attention policies for efficient robot learning. _arXiv preprint arXiv:2304.11235_, 2023.\n' +
      '* [59] Nur Muhammad Shafiullah, Zichen Cui, Arintuya Arty Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning \\(k\\) modes with one stone. _Advances in neural information processing systems_, 35:22955-22968, 2022.\n' +
      '* [60] Zichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. From play to policy: Conditional behavior generation from uncurated robot data, 2022.\n' +
      '* [61] Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Katerina Fragkiadaki. Act3d: 3d feature field transformers for multi-task robotic manipulation. In _Conference on Robot Learning_, pages 3949-3965. PMLR, 2023.\n' +
      '* [62] Wenxuan Zhou, Bowen Jiang, Fan Yang, Chris Paxton, and David Held. Learning hybrid actor-critic maps for 6d non-prehensile manipulation. _arXiv preprint arXiv:2305.03942_, 2023.\n' +
      '* [63] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. CLIPort: What and where pathways for robotic manipulation. In _CoRL_, pages 894-906. PMLR, 2022.\n' +
      '* [64] Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet. Learning latent plans from play. In _CoRL_, pages 1113-1132. PMLR, 2020.\n' +
      '* [65] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data. _RoboticsScience and Systems_, 2021.\n' +
      '* [66] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. VoxPoser: Composable 3D value maps for robotic manipulation with language models. In _CoRL_, 2023.\n' +
      '* [67] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as Policies: Language model programs for embodied control. In _icra_, pages 9493-9500. IEEE, 2023.\n' +
      '* [68] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. _arXiv preprint arXiv: Arxiv-2305.16291_, 2023.\n' +
      '* [69] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. ProgPrompt: Generating situated robot task plans using large language models. In _ICRA_, pages 11523-11530. IEEE, 2023.\n' +
      '* [70] Naoki Yokoyama, Alex Clegg, Joanne Truong, Eric Undersander, Tsung-Yen Yang, Sergio Arnaud, Sehoon Ha, Dhruv Batra, and Akshara Rai. ASC: Adaptive skill coordination for robotic mobile manipulation. _arXiv preprint arXiv:2304.00410_, 2023.\n' +
      '* [71] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, Chelsea Finn, and Karol Hausman. Open-world object manipulation using pre-trained vision-language model. In _arXiv preprint_, 2023.\n' +
      '* [72] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi. Gpt-4v(ision) for robotics: Multimodal task planning from human demonstration. _arXiv preprint arXiv:2311.12015_, 2023.\n' +
      '* [73] Krishan Rana, Jesse Haviland, Sourav Garg, Jad Aboub-Chakra, Ian Reid, and Niko Suenderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. _arXiv preprint arXiv:2307.06135_, 2023.\n' +
      '* [74] Charles C Kemp, Aaron Edsinger, Henry M Clever, and Blaine Matulevich. The design of stretch: A compact, lightweight mobile manipulator for indoor human environments. In _2022 International Conference on Robotics and Automation (ICRA)_, pages 3150-3157. IEEE, 2022.\n' +
      '\n' +
      '## Appendix A Scannet200 text queries\n' +
      '\n' +
      'To detect objects in a given home environment using OWL-ViT, we use the Scannet200 labels. The full label set is here: [\'shower head\',\'spray\', \'inhaler\', \'guitar case\', \'plunger\', \'range hood\', \'toilet paper disperser\', \'adapter\',\'soy sauce\', \'pipe\', \'bottle\', \'door\',\'scale\', \'paper towel\', \'paper towel\', \'paper towel roll\',\'stove\',\'mailbox\',\'scissors\', \'tape\', \'bathroom stall\', \'chopsticks\', \'case of water bottles\', \'hand sanitizer\', \'laptop\', \'alcohol disinfection\', \'keyboard\', \'coffee maker\', \'light\', \'toaster\',\'stuffed anima1\', \'divider\', \'clothes dryer\', \'toilet seat cover dispenser\', \'file cabinet\', \'curtain\', \'ironing board\', \'fire extinguisher\', \'fruit\', \'object\', \'blinds\', \'container\', \'bag\', \'oven\', \'body wash\', \'bucket\', \'cd case\', \'tv\', \'tray\', \'bowl\', \'cabinet\',\'speaker\', \'crate\', \'projector\', \'book\',\'school bag\', \'laundry detergent\',\'mattress\', \'bathtub\', \'clothes\', \'candle\', \'basket\', \'glass\', \'face wash\', \'notebook\', \'purse\',\'shower\', \'power outlet\', \'trash bin\', \'paper bag\', \'water dispenser\', \'package\', \'bulletin board\', \'printer\', \'windowsill\', \'disinfecting wipes\', \'bookshelf\',\'recycling bin\', \'headphones\', \'dresser\',\'mouse\',\'shower gel\', \'dustpan\', \'cup\',\'storage organizer\', \'vacuum cleaner\', \'fireplace\', \'dish rack\', \'coffee kettle\', \'fire alarm\', \'plants\', \'rag\', \'can\', \'piano\', \'bathroom cabinet\',\'shelf\', \'cushion\',\'monitor\', \'fan\', \'tube\', \'box\', \'blackboard\', \'ball\', \'bicycle\', \'guitar\', \'trash can\', \'hand sanitizers\', \'paper towel dispenser\', \'whiteboard\', \'bin\', \'potted plant\', \'tennis\',\'soap dish\',\'structure\', \'calendar\', \'dumbbell\', \'fish oil\', \'paper cutter\', \'ottom\',\'stool\', \'hand wash\', \'lamp\', \'toaster owner\',\'music stand\', \'water bottle\', \'clock\', \'charger\', \'picture\', \'basketball\',\'sink\',\'microwave\',\'screwdriver\', \'kitchen counter\', \'rack\', \'apple\', \'washing machine\',\'suitcase\', \'ladder\', \'ping pong ball\', \'window\', \'dishwasher\',\'storage container\', \'toilet paper holder\', \'coat rack\',\'soap dispenser\',\'refrigerator\', \'banana\', \'counter\', \'toilet paper\',\'mug\',\'marker pen\', \'hat\', \'aerosol\', \'luggage\', \'poster\', \'bed\', \'cart\', \'light switch\', \'backpack\', \'power strip\', \'baseball\',\'mustard\', \'bathroom vanity\', \'water pitcher\', \'closet\', \'couch\', \'beverage\', \'toy\',\'salt\', \'plant\', \'pillow\', \'broom\', \'pepper\',\'muffins\',\'multivitamin\', \'towel\',\'storage bin\', \'nightstand\', \'radiator\', \'telephone\', \'pillar\', \'tissue box\',\'vent\', \'hair dryer\', \'ledge\',\'mirror\',\'sign\', \'plate\', \'tripod\', \'chair\', \'kitchen cabinet\', \'column\', \'water cooler\', \'plastic bag\', \'umbrella\', \'doorframe\', \'paper\', \'laundry hamper\', \'food\', \'jacket\', \'closet door\', \'computer tower\',\'stairs\', \'keyboard piano\', \'person\', \'table\',\'machine\', \'projector screen\',\'shoe\'].\n' +
      '\n' +
      '## Appendix B Sample objects from our trials\n' +
      '\n' +
      'During our experiments, we tried to sample objects that can plausibly be manipulated by the Hello Robot: Stretch gripper from the home environments. As a result, OK-Robot encountered a large variety of objects with different shapes and visual features. A subsample of such objects are presented in the Figures 9, 10.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n' +
      '## Appendix A\n' +
      '\n' +
      'Fig. 10: Sample objects on our home experiments, sampled from each home environment, which OK-Robot **failed** to pick up successfully.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Fit object & Price location & Result \\\\ \\hline Home 1 & \\\\ \\hline \\hline \\multicolumn{3}{c}{Cleanup level: none} \\\\ silver cup & white table & Success \\\\ blue eye glass case & chair & Success \\\\ printed paper cup, coffee cup [white table] & Manipulation failure \\\\ small red and white medication & Chair & Success \\\\ power adapter & Grey Bed & Success \\\\ wrapped paper & Navigation failure \\\\ blue body wash & study table & Success \\\\ blue air spray & white table & Success \\\\ black face wash & Manipulation failure \\\\ yellow face wash & chair & Success \\\\ body spray & Navigation failure \\\\ small hand sanitizer & & Manipulation failure \\\\ blue inhaler device(window) & white table & Success \\\\ inhaler box(window) & dust bin & Success \\\\ multivitamin container & & Navigation failure \\\\ red towel & white cloth bin (air conditioner) & Success \\\\ white shirt & white cloth bin (air conditioner) & Success \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: low} \\\\ silver cup & white table & Success \\\\ blue eye glass case & Navigation failure \\\\ printed paper cup, coffee cup [white table] & dust bin & Success \\\\ small red and white medication & Chair & Success \\\\ power adapter & Navigation failure \\\\ blue body wash & white table & Success \\\\ blue air spray & white table & Success \\\\ yellow face wash & white table & Success \\\\ small hand sanitizer & study table & Success \\\\ blue inhaler device(window) & & Manipulation failure \\\\ inhaler box(window) & dust bin & Success \\\\ red towel & white cloth bin(air conditioner) & Success \\\\ white shirt & white cloth bin(air conditioner) & Success \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: high} \\\\ silver cup & white table & Success \\\\ printed paper cup, coffee cup [white table] & dust bin & Success \\\\ blue body wash & white table & Success \\\\ blue air spray & white table & Success \\\\ yellow face wash & & Manipulation failure \\\\ small hand sanitizer & & Manipulation failure \\\\ inhaler box(window) & dust bin & Success \\\\ white shirt & white cloth bin(air conditioner) & Success \\\\ \\hline \\multicolumn{3}{c}{Home 2} \\\\ \\multicolumn{3}{c}{Cleanup level: None} \\\\ \\multicolumn{3}{c}{fanta can} & dust bin & Success \\\\ tennis ball & small red shopping bag & Success \\\\ black head band [bed] & Manipulation failure \\\\ purple shampoo bottle & white rack & Success \\\\ toothpaste & small red shopping bag & Success \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: A list of all tasks in the home environments, along with their categories and success rates out of 10 trials.\n' +
      '\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{1}{c}{Price Isolation} & \\multicolumn{1}{c}{Result} \\\\ \\hline orange packaging & dust bin & Success \\\\ green hair cream jar [white rack] & \\multicolumn{1}{c}{} & Navigation failure \\\\ green detergent pack [white rack] & white table & Success \\\\ blue moisturizer [white rack] & \\multicolumn{1}{c}{} & Navigation failure \\\\ green plastic cover & \\multicolumn{1}{c}{} & Navigation failure \\\\ storage container & \\multicolumn{1}{c}{} & Manipulation failure \\\\ blue hair oil bottle & \\multicolumn{1}{c}{} & White rack & Success \\\\ blue pretzels pack & \\multicolumn{1}{c}{} & White rack & Success \\\\ blue hair gel tube & \\multicolumn{1}{c}{} & Manipulation failure \\\\ red bottle [white rack] & \\multicolumn{1}{c}{} & brown desk & Success \\\\ blue bottle [air conditioner] & white cloth bin(air conditioner) & Success \\\\ wallet & \\multicolumn{1}{c}{} & Manipulation failure \\\\ \\hline  & \\multicolumn{1}{c}{} & \\\\ \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{1}{c}{} & \\\\ \\hline fanta can & black trash can & Success \\\\ tennis ball & red target bag & Success \\\\ black head band [bed] & red target bag & Success \\\\ purple shampoo bottle & red target bag & Success \\\\ toothpaste & red target bag & Success \\\\ orange packaging & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ green detergent pack [white rack] & \\multicolumn{1}{c}{} & Manipulation failure \\\\ blue moisturizer [white rack] & \\multicolumn{1}{c}{} & Navigation failure \\\\ blue hair oil bottle & white rack & Success \\\\ blue pretzels pack & \\multicolumn{1}{c}{} & White rack & Success \\\\ wallet & \\multicolumn{1}{c}{} & Manipulation failure \\\\ \\hline  & \\multicolumn{1}{c}{} & \\\\ \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{1}{c}{} & \\\\ \\hline fanta can & black trash can & Success \\\\ purple shampoo bottle & small red shopping bag & Success \\\\ orange packaging & black trash can & Success \\\\ blue moisturizer [white rack] & white rack & Success \\\\ blue hair oil bottle & \\multicolumn{1}{c}{} & Manipulation failure \\\\ blue hair gel tube & \\multicolumn{1}{c}{} & \\\\ red bottle [white rack] & \\multicolumn{1}{c}{} & blue bottle [air conditioner] & Success \\\\ \\hline  & \\multicolumn{1}{c}{} & \\\\ \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{1}{c}{} & \\\\ \\hline \\hline  & \\multicolumn{1}{c}\n' +
      '\n' +
      '\\begin{tabular}{l c c} \\hline \\multicolumn{3}{c}{Cleanup level: low} \\\\ \\hline apple & white plate & Success \\\\ ice cream & red basket & Success \\\\ green lime juice bottle & red basket & Success \\\\ yellow packet & green bag & Success \\\\ red packet & & Manipulation failure \\\\ orange can & card board box & Success \\\\ cooking oil bottle & marble surface [red basket] & Success \\\\ green bowl & & Manipulation failure \\\\ washing gloves & sink & Success \\\\ small oregano bottle & red basket & Success \\\\ yellow noodles packet [stove] & & Manipulation failure \\\\ blue dish wash bottle & card board box & Success \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: high} \\\\ apple & white plate & Success \\\\ ice cream & red basket & Success \\\\ green lime juice bottle & red basket & Success \\\\ orange can & card board box & Success \\\\ cooking oil bottle & & Manipulation failure \\\\ washing gloves & sink & Success \\\\ small oregano bottle & red basket & Success \\\\ yellow noodles packet [stove] & red basket & Success \\\\ blue dish wash bottle & card board box & Success \\\\ \\hline \\multicolumn{3}{c}{House 4} \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: none} \\\\ \\hline pepsi & black chair & Success \\\\ birdie & cloth bin & Success \\\\ black hat & & Navigation failure \\\\ owl like wood carving & bed & Success \\\\ red inhaler & & Manipulation failure \\\\ black sesame seeds & & Manipulation failure \\\\ banana & & Manipulation failure \\\\ loose-leaf herbal tea jar & black chair & Success \\\\ red pencil sharpener & & Navigation failure \\\\ fast-food French fries container & blue shopping bag [metal drying rack] & Placing failure \\\\ milk & plastic storage drawer unit & Success \\\\ socks[bed] & & Navigation failure \\\\ purple gloves & & Manipulation failure \\\\ target bag & cloth bin & Success \\\\ muffin & grey bed & Success \\\\ tissue paper & table & Success \\\\ grey eyeglass box & & Manipulation failure \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: low} \\\\ \\hline pepsi & basket & Success \\\\ birdie & white drawer & Success \\\\ owl like wood carving & & Navigation failure \\\\ red inhaler & plastic storage drawer unit & Success \\\\ black sesame seeds & bed & Success \\\\ loose-leaf herbal tea jar & table & Success \\\\ fast-food French fries container & chair & Success \\\\ \\hline \\multicolumn{3}{c}{Continued on the next page} \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Pie subject & Piex location & Result \\\\ \\hline milk & chair & Success \\\\ purple gloves & basket & Success \\\\ target bag & basket & Placing failure \\\\ muffin & table & Success \\\\ tissue paper & & Manipulation failure \\\\ grey eyeglass box & & Navigation failure \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: high} \\\\ \\hline pepsi & basket & Success \\\\ birdie & bed & Success \\\\ red inhaler & plastic storage drawer unit & Success \\\\ black sesame seeds & desk & Success \\\\ banana & & Manipulation failure \\\\ loose-leaf herbal tea jar & & Manipulation failure \\\\ milk & chair & Success \\\\ purple gloves & basket & Success \\\\ target bag & basket & Success \\\\ muffin & bed & Success \\\\ \\hline \\multicolumn{3}{c}{Home.S} \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: none} \\\\ \\hline tiger balm topical ointment & & Navigation failure \\\\ pink shampoo & trader joes shaping bag & Success \\\\ aveeno sunscreen protective lotion & trader joes shaping bag & Success \\\\ small yellow nozzle spray & & Manipulation failure \\\\ black hair care spray & & Manipulation failure \\\\ green hand sanitizer & & Manipulation failure \\\\ white hand sanitizer & & Navigation failure \\\\ white bowl [ketch] & black sofa chair & Success \\\\ blue bowl & & Manipulation failure \\\\ blue sponge & trader joes shaping bag & Success \\\\ ketchup & & Manipulation failure \\\\ white salt & & Manipulation failure \\\\ black pepper & black drawer & Success \\\\ blue bottle & & Navigation failure \\\\ purple light bulb box & trader joes shopping bag & Success \\\\ white plastic bag & bed & Success \\\\ rag & white rack & Success \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: low} \\\\ pink shampoo & & Navigation failure \\\\ aveeno sunscreen protective lotion & & Manipulation failure \\\\ small yellow nozzle spray & & Manipulation failure \\\\ white bowl [ketch] & black sofa chair & Success \\\\ blue sponge & bed & Success \\\\ ketchup & trader joes shopping bag & Success \\\\ white salt & trader joes shopping bag & Success \\\\ black pepper & & Navigation failure \\\\ blue bottle & black sofa chair & Success \\\\ purple light bulb box & & Manipulation failure \\\\ rag & white rack & Success \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: high} \\\\ pink shampoo & trader joes shopping bag & Success \\\\ \\hline \\multicolumn{3}{c}{Continued on the next page} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '[FIGURE\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{2}{c}{Cleanup level: none} \\\\ \\cline{2-3} blue plastic bag roll & Navigation failure \\\\ green bag & basket[window] & Success \\\\ toy cactus & desk & Success \\\\ toy van & chair & Success \\\\ brown medical bandage & chair & Success \\\\ power adapter & Navigation failure \\\\ red herbal tea & brown cardboard box & Success \\\\ apple juice box & brown cardboard box & Success \\\\ paper tow & blue cardboard box & Success \\\\ toy bear & bed blanket & Success \\\\ yellow ball & bed blanket & Success \\\\ black pants & basket[window] & Success \\\\ purple water bottle & desk & Success \\\\ blue eyeglass case & & Manipulation failure \\\\ brown toy monkey & & Navigation failure \\\\ blue hardware box [table] & blue cardboard box & Success \\\\ green zandu balm container & & blue cardboard box & Success \\\\ \\hline  & \\multicolumn{2}{c}{Cleanup level: low} \\\\ green bag & basket & Success \\\\ toy cactus & basket & Success \\\\ toy van & chair & Success \\\\ brown medical bandage & Manipulation failure \\\\ red herbal tea & brown box & Success \\\\ apple juice box & brown box & Success \\\\ paper tow & basket & Success \\\\ toy bear & desk & Success \\\\ purple water bottle & desk & Success \\\\ blue eyeglass case & & Manipulation failure \\\\ green zandu balm container & blue cardboard box & Success \\\\ \\hline  & \\multicolumn{2}{c}{Cleanup level: high} \\\\ green bag & stool [window] & Success \\\\ toy cactus & table & Success \\\\ toy van & white basket & Success \\\\ red herbal tea & brown cardboard box & Success \\\\ apple juice box & brown cardboard box & Success \\\\ paper tow & blue cardboard box & Success \\\\ toy bear & white basket & Success \\\\ yellow ball & bed & Success \\\\ purple water bottle & black tote bag & Success \\\\ green zandu balm container & blue cardboard box & Success \\\\ \\hline  & \\multicolumn{2}{c}{House S} \\\\ \\cline{2-3}  & \\multicolumn{2}{c}{Cleanup level: none} \\\\ cyan air spray & brown shelf [sink] & Success \\\\ blue gloves & kitchen sink & Success \\\\ blue peanut butter & black stove & Success \\\\ nutella & table & Success \\\\ green bag & brown shelf [sink] & Success \\\\ green bandage box & trash can & Success \\\\ green detergent & kitchen sink & Success \\\\ black ‘red pepper sauce’ & & Manipulation failure \\\\ \\hline  & \\multicolumn{2}{c}{Continued on the next page} \\\\ \\cline{2-3}  & \\multicolumn{2}{c}{Cleanup level: none} \\\\ cyan air spray & brown shelf [sink] & Success \\\\ blue gloves & kitchen sink & Success \\\\ blue peanut butter & black stove & Success \\\\ nutella & table & Success \\\\ green bag & brown shelf [sink] & Success \\\\ green bandage box & trash can & Success \\\\ green detergent & kitchen sink & Success \\\\ black ‘red pepper sauce’ & & Manipulation failure \\\\ \\hline  & \\multicolumn{2}{c}{Continued on the next page} \\\\ \\cline{2-3}  & \\multicolumn{2}{c}{Cleanup level: none} \\\\ cyan air spray & brown shelf [sink] & Success \\\\ blue gloves & kitchen sink & Success \\\\ blue peanut butter & black stove & Success \\\\ nutella & table & Success \\\\ green bag & brown shelf [sink] & Success \\\\ green bandage box & trash can & Success \\\\ green detergent & kitchen sink & Success \\\\ black ‘red pepper sauce’ & & Manipulation failure \\\\ \\hline  & \\multicolumn{2}{c}{Continued on the next page} \\\\ \\cline{2-3}  & \\multicolumn{2}{c}{Cleanup level: none} \\\\ cyan air spray & brown shelf [sink] & Success \\\\ blue gloves & kitchen sink & Success \\\\ blue peanut butter & black stove & Success \\\\ nutella & table & Success \\\\ green bag & brown shelf [sink] & Success \\\\ green bandage box & trash can & Success \\\\ green detergent & kitchen sink & Success \\\\ black ‘red pepper sauce’ & & Manipulation failure \\\\ \\hline  & \\multicolumn{2}{c}{Continued on the next page} \\\\ \\cl\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:23]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:24]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>