<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:2]\n' +
      '\n' +
      '프로젝트 웹사이트에서 로봇 영상과 함께 더 많은 정보가 제공되며, [https://ok-robot.githubio] (https://ok-robot.githubio)\n' +
      '\n' +
      'Ⅱ 기술 협력 및 방법.\n' +
      '\n' +
      '우리의 방법은 높은 수준에서 "픽 업 **A**( **B**에서)에 의해 기술된 문제를 해결하고, a는 물체이고 B와 C는 가정과 같은 실제 환경의 장소이다. 저희가 소개하는 시스템은 헬로로봇에 결합된 3개의 1차 서브시스템인 스레치입니다. 즉, 이들은 오픈보크 객체 내비게이션 모듈, 오픈보크 RGB-디싱 모듈 및 드롭 휴리스틱이다. 이 섹션에서는 이러한 각 구성 요소를 더 자세히 설명한다.\n' +
      '\n' +
      '열거한 집, 노보형 객체 항법__열거, 노천식 객체 항법__열집.\n' +
      '\n' +
      '우리 방법의 첫 번째 구성 요소는 집을 매핑하기 위해 사용하는 개방형 집, 개방형 동물 객체 내비게이션 모델이며, 이어서 자연어 쿼리에 의해 지정된 임의의 관심 대상으로 탐색한다.\n' +
      '\n' +
      '가정***Scanning: 오픈 어휘 객체 내비게이션의 경우 CLIP-필드[27]에서 접근 방식을 따르고 아이폰을 사용하여 집을 수동으로 "스캔"하는 사전 매핑 단계를 가정한다. 이 수동 스캔은 단순히 아이폰에 레코드3D 앱을 사용하여 홈 영상을 촬영한 것으로 구성되며, 이는 제기된 RGB-D 이미지의 순서를 초래한다.\n' +
      '\n' +
      '또는 프런티어 기반 탐색[15, 25, 26]을 사용하여 자동으로 수행할 수 있지만 속도와 단순성은 수동 접근[26, 27]을 선호한다. 프런티어 기반 접근법은 특히 새로운 공간에 대해 느리고 숫자화하는 경향이 있는 반면 "스캔"은 방마다 1분도 채 걸리지 않기 때문에 이 접근법을 취한다. 수집되면, RGB-D 이미지는 카메라 포즈와 위치와 함께 지도 구축을 위해 당사 라이브러리에 수출된다.\n' +
      '\n' +
      '우리의 의미 기억은 관심 대상과 항해 가능한 표면 및 장애물을 모두 포함하기 위해 녹음은 환경의 객체 및 수용체와 함께 바닥 표면을 포착해야 한다.\n' +
      '\n' +
      '** 검출 오브젝트**: 스캔의 각 프레임에서 오픈 배어 오브젝트 검출기를 실행한다. 두 가지 [7]을 사용한 이전 작품과 달리 예비 질의에서 더 나은 성능을 발휘한다는 것을 발견했기 때문에 OWL-ViT[8]를 객체 검출기로 선택했다. 우리는 모든 프레임에 검출기를 적용하고, 각각의 오브젝트 바운딩 박스, CLIP-블럭딩, 검출기 신뢰 등을 추출하고, 네비게이션 모듈의 객체 메모리 모듈에 전달한다.\n' +
      '\n' +
      '이전 작업 [27] 위에 건설되는데, 바운딩 박스를 세그먼트 애니션(SAM)[28]으로 객체 마스크로 더 정제한다. 많은 경우에 개방형 동물 객체 검출기는 여전히 그들이 감지하려는 자연어 객체 질의 세트를 필요로 한다는 점에 유의한다. 우리는 원본 스칸넷200 레이블[29]에서 파생된 이러한 객체 질의들의 많은 세트를 공급하여 검출기가 현장에서 가장 일반적인 객체들을 캡처한다.\n' +
      '\n' +
      '***객체 중심 의미 기억**: 우리는 VoxelMap이라고 부르는 Clip-Forks[27] 및 OVMM[25]와 유사한 객체 중심 메모리를 사용한다. 객체 마스크는 깊이 이미지와 카메라에 의해 수집된 포즈를 사용하여 실제 좌표로 역분석되어 각 지점이 CLIP에서 나오는 연관된 의미 벡터를 갖는 포인트 클라우드를 제공한다. 그런 다음 포인트 클라우드를 5cm 해상도와 각 복셀에 대해 복셀을 복셀하여 그 복셀에 속하는 CLIP 임베딩에 대한 검출기 과신 가중 평균을 계산한다. 이 복셀 맵은 객체 메모리 모듈의 베이스를 구축합니다. 이러한 방식으로 생성된 표현은 첫 스캔 후에 정적 상태로 유지되며 로봇의 작동 중에 적응될 수 없다는 점에 유의한다. 이러한 동적으로 지도를 만들 수 없는 것은 우리의 한계 구간(섹션 V)에서 논의된다.\n' +
      '\n' +
      '메모리 모듈****를 참조하는데, 우리의 의미론적 객체 메모리는 우리에게 세계에서 아마도 비-빈 복셀로 표시되는 정적 세계 표현과 각 복셀과 관련된 의미론적 CLIP 벡터를 제공한다. 언어 질의를 감안할 때 CLIP 언어 인코더를 사용하여 의미론적 벡터로 변환한다. 그런 다음 인코딩된 벡터와 복셀의 의미 표현 사이의 도트 제품이 최대화된 상단 복셀을 찾습니다. 각 복셀은 가정 내 실제 위치와 연관되어 있기 때문에 그림 2(a)와 유사하게 쿼드 오브젝트가 발견될 가능성이 가장 높은 위치를 찾을 수 있다.\n' +
      '\n' +
      '필요한 경우 "A 온 B"를 "B 근처에 있는 A"로 구현합니다. 질의 A에 대한 상위 10점과 질의 B에 대한 상위 50점을 선택하여, 우리는 \\(10\\t 50\\) 쌍별 유클리드 거리를 계산하고 최단(A, B) 거리의 A 포인트를 선택한다. 객체 내비게이션 단계 동안, 우리는 이 쿼리를 사용하여 조작을 위한 것이 아니라 대략적으로 오브젝트로 탐색한다.\n' +
      '\n' +
      '이 접근법은 우리 지도가 이전 작업(26, 27, 30)보다 더 낮은 해상도로 작용할 수 있으며 지도를 구축한 후 물체의 위치에서 작은 움직임을 처리할 수 있다.\n' +
      '\n' +
      '실제 세계***의 객체에 대한***: 네비게이션 모델은 실제 세계에서 3D 위치 좌표를 제공하므로 로봇이 조작 단계를 초기화하기 위한 항법 표적으로 사용한다. 이전 작품[15, 27, 31]에서는 항행 목적이 목적물을 가서 보는 것이었으며, 이는 대상 자체로부터 안전한 거리에 머물면서 할 수 있다. 대조적으로, 우리의 내비게이션 모듈은 로봇이 이후 타겟 객체를 조작할 수 있도록 로봇을 팔 길이에 배치해야 한다. 따라서, 우리의 내비게이션 방법은 다음과 같은 목적을 균형을 맞춰야 합니다.\n' +
      '\n' +
      '로봇은 1.1을 조작할 대상, 1.1을 조작할 목적물에 충분히 가까이 있을 필요가 있다.\n' +
      '로봇과 오브젝트m과 오브젝트m 사이, 로봇은 그리퍼를 이동시키기 위해서는 일부 공간이 필요하므로 로봇이 파지퍼를 이동시킬 수 있는 작지만 비협박적인 공간이 필요하다.\n' +
      '3. 로봇은 조작 중 충돌을 피해야 하므로 모든 장애물과 거리를 유지해야 합니다.\n' +
      '\n' +
      '우리는 각각의 이전 우려 사항 중 하나와 관련된 세 가지 다른 내비게이션 점수 함수를 사용하고, 이를 공간의 각 지점에 평가하여 로봇을 배치할 수 있는 최상의 위치를 찾습니다.\n' +
      '\n' +
      '무작위 포인트는 \\(\\vec{x}\\), 가장 가까운 장애물 포인트는 \\(\\vec{x}_{obs}\\)이고 대상 객체는 \\(\\vec{x}_{o}\\)로 한다. 그런 다음 세 가지 기준을 캡처하기 위해 다음 세 가지 기능 \\(s_{1},s_{2},s_{3}\\)을 정의할 수 있다. 그런 다음, 그들의 가중합 \\(s\\)은 \\(s(\\vec{x^{*}})를 최소화하는 우리 공간에서 이상적인 항법 포인트 \\(\\vec{x^{*}}\\)를 찾는데, 방향은 \\(\\c{x^{*}}}})에서 \\(\\c{x_{o}}\\)까지의 벡터로 향하고 있다.\n' +
      '\n' +
      '}.\n' +
      '\n' +
      '공간에서 다른 어떤 지점에서도 이 타겟 포인트로 안전하게 탐색하기 위해 이전에 캡처된 RGB-D 이미지로부터 장애물 맵을 구축하여 [26, 32]와 유사한 접근법을 따른다. 우리는 A* 알고리즘을 사용하여 탐색하는 장애물의 2D, 10cm\\(\\tot\\)10cm 그리드를 구축한다. 복셀 지도를 장애물 맵으로 변환하기 위해 먼저 바닥과 천장 높이를 설정했습니다. 이 사이에 점유된 복셀의 존재는 그리드 셀이 점유되는 반면 천장이나 바닥 복셀의 존재는 그리드 셀이 설명되지 않는다는 것을 의미한다. 우리는 점유되거나 설명되지 않은 세포를 모두 항해할 수 없는 것으로 표시한다. 각 점유 포인트 주위에 로봇의 반지름과 턴 반경을 설명하기 위해 20cm 반경 내의 임의의 지점을 사용할 수 없는 것으로 표시한다. A* 알고리즘에서 우리는 \\(s_{3}\\) 기능을 노드 비용에 대한 휴리스틱으로 사용하여 장애물과 더 멀리 탐색하여 실험에서 이상적인 보로노이 경로[33]와 유사한 생성된 경로를 만든다.\n' +
      '\n' +
      '실세계_실천에서 파악하는 _열생애요.\n' +
      '\n' +
      '개방형 항법과는 달리 파악을 위해서는 우리의 방법이 현실 세계의 임의의 객체와 물리적으로 상호 작용할 필요가 있어 이 부분을 훨씬 더 어렵게 만든다. 그 결과, 실제 세계에서 파악된 포즈들을 생성하기 위해 미리 학습된 파악 모델을 사용하고, 현대 VLM을 사용하여 언어 조건을 갖는 필터를 선택하게 된다.\n' +
      '\n' +
      '**Grasp 지각***: 로봇은 II-A에 요약된 내비게이션 방법을 사용하여 객체에 도달하고, 우리는 로봇에 대한 파악을 생성하기 위해 미리 훈련된 식별 모델 또는 휴리스틱을 사용한다. 우리는 로봇의 RGB-D 헤드 카메라를 의미론적 메모리 모듈에 의해 저희에게 주어진 공간에 있는 물체의 위치에 포인트하고, 그로부터 RGB-D 이미지를 캡처한다(그림 3, 열 1). 우리는 필요에 따라 깊이 이미지를 포인트 클라우드로 백업하고 변환합니다. 그런 다음 이 정보를 파악 생성 모듈에 전달합니다. 우리가 작품에서 사용하는 파악 생성 모듈은 단일 RGB 이미지와 포인트 클라우드를 제공하는 장면에서 병렬 턱 그리퍼와 충돌 자유 그릴을 생성하는 AnyGrasp[19]이다.\n' +
      '\n' +
      '모든 그리스는 현장(그림 3 컬럼 2)에서 가능한 그릴을 제공할 수 있으며, 이는 각 파악에 대한 비열화된 모델 확신을 나타내는 점, 폭, 높이, 깊이 및 "그래스니스 점수"를 식별한다. 그러나 이러한 모듈은 일반적으로 현장에서 가능한 모든 그릴을 생성하며, 이는 언어 쿼리를 사용하여 필터링해야 한다.\n' +
      '\n' +
      '언어 질의***를 사용하여 그래프**Fil터링 그래프: 아나그라즈에서 제안된 모든 그래프들을 얻으면 Lang삼[24]를 사용하여 그래프들을 필터링한다. 우리는 랑삼[24]을 이용하여 촬영된 이미지를 분할하여 원하는 물체의 마스크를 언어 질의로 구한다(그림 3 컬럼 3). 그런 다음 제안된 모든 파악점을 이미지에 투영하고 객체 마스크에 속하는 그릴을 찾습니다(그림 3 열 4).\n' +
      '\n' +
      '우리는 파악 점수가 \\(\\mathcal{S}\\)이고 파악 정상과 바닥 정상 사이의 각도가 \\(\\theta\\)이면 새로운 휴리스틱 점수가 \\(\\mathcal{S}-(\\nicefrac{{\\theta}}{{10}})\\인 휴리스틱을 사용하여 가장 잘 파악한다. 이 휴리스틱은 가장 높은 식별 점수를 가진 정렬을 우선시하지만 수평적으로 제안된 파악도 우선한다. 로봇의 작은 캘리브레이션 오차에 강건한 반면 수직 그릴은 성공하려면 상당히 점 검사해야 하기 때문에 수평 그릴을 선호합니다. 핸드아이 캘리브레이션 오류는 로봇을 다른 집으로 운반하기 때문에 원하는 속성입니다.\n' +
      '\n' +
      '그림. 2: 오픈패스, 오픈 지식 객체 위치화 및 실제 세계 항법입니다. 우리는 자연 언어 질의로 물체를 국소화하는 데 VoxelMap[25]를 사용하고, 경로 계획을 위해 USANet[26]과 유사한 A* 알고리즘을 사용한다.\n' +
      '\n' +
      '실험 과정을 밟습니다.\n' +
      '\n' +
      '**Grasp 실행**: 최고의 파악(그림 3 컬럼 5)을 식별하기 위해 간단한 프리그래스 접근법[34]을 사용하여 의도한 객체를 식별한다. 우리는 \\(터버우라로우{p}\\)가 파악점이고 \\(터버우라로우{a}\\)는 우리의 파악 모델에 의해 주어진 접근 벡터라고 가정하자. 그런 다음, 우리의 로봇 그리퍼는 다음 궤적을 따른다.\n' +
      '\n' +
      '다른 우뚝 솟은{a}, 살벌한 우뚝 솟은{a}, 평범하게 우뚝 솟은{p}-0.08, 직각 우뚝 솟은{p}-0.2, 별반 우뚝 솟은{p}-0.2, 평평하고 우뚝 솟은{p}-0.2, 직각 이불{p}-0.\n' +
      '\n' +
      '간단히 말해서, 우리의 방법은 점진적으로 더 작은 움직임을 가진 일렬로 프리 프롭스 위치에서 오브젝트에 접근한다. 로봇이 다른 곳에서 가벼운 물체를 노크할 수 있기 때문에 물체에 접근하는 것이 중요하므로 더 느립니다. 예측된 파악 지점에 도달하면, 우리는 그리퍼를 가까운 루프 방식으로 닫아 물체에 파쇄하지 않고 단단한 그립감을 얻을 수 있도록 한다. 마지막으로 객체를 파악한 후 로봇팔을 들어 올려서 완전히 후퇴시키고 손목을 회전시켜 물체를 몸 위에 두드려 놓는다. 이 행동은 물체가 로봇에 의해 안전하게 유지되며 드롭 위치로 탐색하면서 떨어지지 않도록 하는 동안 로봇 발자국을 유지한다.\n' +
      '\n' +
      '### _Dropping heuristic_\n' +
      '\n' +
      '물체를 선택한 후, 우리는 제Ⅱ-A절에서 설명한 것과 동일한 방법을 사용하여 위치를 찾고 탐색한다. 드롭오프 위치가 평평한 표면이라고 가정하는 홈로봇의 기준선 구현[25]에서와 달리, 우리는 휴리스틱을 싱크, 빈, 박스, 가방과 같은 오목한 물체를 덮도록 확장한다. 먼저 드롭 언어 질의를 사용하여 제Ⅱ-B와 유사한 섹션 II-B를 사용하여 로봇의 헤드 카메라에 의해 캡처된 포인트 클라우드 \\(P\\)를 분할한다. 그런 다음, 우리는 X축이 로봇이 마주보는 방식과 정렬되고 Y축은 좌우로, 포인트 클라우드의 Z축은 바닥 정상과 정렬되도록 분할 포인트 클라우드를 정렬한다. 우리는 이 정렬된 포인트 클라우드 \\(P_{a}\\)라고 부른다. 마지막으로 로봇의 X- 및 Y- 좌표가 \\(0,0)\\이고, 바닥면은 \\(z=0\\)로 되도록 포인트 클라우드를 정규화한다. 정렬되고 세분화된 포인트 클라우드에 대해 각 지점에 대해 X- 및 Y-좌표를 고려하고, 우리는 \\(x_{m}\\) 및 \\(y_{m}\\)로 부르는 각 축에서 각각의 중앙값을 찾는다. 마지막으로, 분할 정렬된 포인트 클라우드에서 \\(z_{\\max}=0.2+\\max\\{z\\mid(x,y,z)\\in P_{a};0\\leq x\\leq x_{m};|y-y_{m}|<0.1\\}\\)를 사용하여 드롭 높이를 찾습니다. 로봇과 드롭 위치 사이의 충돌을 피하기 위해 높이에 \\(0.2\\)의 작은 버퍼를 추가한다. 마지막으로 로봇 그리퍼를 드롭 포인트 위로 옮기고 그리퍼를 열어 물체를 떨어뜨린다. 이 휴리스틱은 때때로 덩어리된 표면에 물체를 배치하지 않지만, 우리의 실험에서 평균적으로 잘 수행된다.\n' +
      '\n' +
      '집_의 고용.\n' +
      '\n' +
      '내비게이션, 픽업 및 드롭 프리미티브가 있으면 직접 결합하여 모든 새로운 집에서 직접 적용할 수 있는 로봇 방법을 만듭니다. 새로운 가정 환경을 위해 1분 이내에 방을 "스캔"할 수 있습니다. 그런 다음 VoxelMap로 처리하는 데 5분도 채 걸리지 않습니다. 배려를 위해 CLIP-필드 또는 USA-Net과 같은 필요한 암묵적 의미 분야/SDF 모델을 사용하여 교육하는 데 약 50분이 소요된다. 일단.\n' +
      '\n' +
      '그림. 실제 세계에서 3:오픈 욕설을 파악합니다. 좌측에서 우측으로 (a) 로봇 POV 이미지, (b)는 모두 AnyGrasp[19], Lang삼[24]의 라벨이 제공된 (c) 오브젝트 마스크에서 제안된 그래프들을 보여주며, (d) 마스크로 필터링된 스코어들을 식별하며, (e) 실행을 위해 선택된 것을 파악한다.\n' +
      '\n' +
      '로봇은 바로 베이스에 배치하여 작동을 시작할 수 있습니다. 완전히 새로운 환경에 도착해서 자율적으로 운영하기 시작하는 것부터, 저희 시스템은 평균 10분 이상 소요되어 첫 픽 앤 드롭 작업을 완료합니다.\n' +
      '\n' +
      '** 스테이트 머신 모델**: 미리 정의된 방식으로 상이한 모듈들 간의 전환이 자동으로 발생하는데, 한 번 사용자가 선택하여 내려갈 물체를 특정하면 된다. 오류 탐지나 교정을 시행하지 않기 때문에, 우리의 상태 기계 모델은 탐색에서 대상체로 이어지는 단계들의 단순한 선형 사슬이며, 목표를 파악, 목표로 탐색하고, 작업을 완료하기 위해 목표에서 객체를 떨어뜨리는 것이다.\n' +
      '\n' +
      '홈 실험을 위한**프로토콜: 새로운 집에서 실험을 실행하려면 먼저 로봇이 이전에 관측되지 않은 방으로 이동합니다. 그곳에서 우리는 현장을 기록하고 VoxelMap을 만듭니다. 현재 로봇 그리퍼에 들어갈 수 있는 각 장면에서 임의로 10-20개의 물체를 선택합니다. 이들은 현장에서 "소리"하는 객체이며, 미리 선택된 것은 아니다. 우리는 쿼리들을 일관되고 실험자의 편향 없이 유지하기 위해 GPT-4V [35]를 사용하여 선택된 객체 각각에 대한 언어 질의를 제시했다. OK-로봇에 대한 동일한 객체에 대한 상이한 질의의 효과는 III-D 섹션에서 논의된다. 그런 다음 네비게이션 모듈을 쿼리하여 모든 내비게이션 실패를 걸러내고, 즉 위치가 의미론적 메모리 모듈에 의해 발견될 수 없는 물체를 검색합니다. 그런 다음, 시험 간에 리셋 없이 나머지 객체에 대해 순차적으로 픽 앤 드롭을 실행합니다.\n' +
      '\n' +
      '## III Experiments\n' +
      '\n' +
      '우리는 두 세트의 실험에서 우리의 방법을 평가한다. 첫 번째 실험 세트에 대해 네비게이션과 조작 모듈 각각에 대한 다중 대안을 평가한다. 이러한 실험은 우리 방법의 일부로 가정 환경에서 어떤 모듈을 사용하고 평가할지에 대한 통찰력을 제공한다. 다음 실험 세트에서 우리는 로봇을 10개의 집으로 가져갔고 171개의 픽 앤 드롭 실험을 실행하여 완전히 새로운 집에서 우리의 방법이 어떻게 수행되는지 실증적으로 평가하고 시스템의 실패 모드를 이해했다.\n' +
      '\n' +
      '이러한 실험을 통해 OK-Robot에 의해 구현된 현재 오픈 지식 로봇 시스템의 능력과 한계에 관한 일련의 질문에 답해 보고자 한다. 네, 다음을 요청드립니다.\n' +
      '\n' +
      '1. 이러한 시스템은 임의의 집에서 골라 떨어뜨리는 도전에 어떻게 대처할 수 있습니까?\n' +
      '2. 오픈 지식 로봇 시스템 구축을 위해 여기에 제시된 레시피와 비교하여 내비게이션 및 파악을 위한 대체 프리미티브는 어떻게 잘 되나요?\n' +
      '3. 현재 시스템은 집들이 특히 어포던스, 모호성, 어포던스 도전과 같은 어렵게 만드는 독특한 과제를 어떻게 처리할 수 있습니까?\n' +
      '4. 그러한 시스템의 실패 모드와 실제 가정 환경에서 개별 구성요소는 무엇인가요?\n' +
      '\n' +
      '홈 실험_홈 실험_##.\n' +
      '\n' +
      '10개 가정 환경에서 OK-로봇은 전체 픽 앤 드롭을 완료하는 데 58.5%의 성공률을 달성했다. 특히 이것은 제로샷 알고리즘이며 성공률은 각 가정에서 조달한 새로운 객체보다 높다. 결과적으로 로봇의 성공과 실패는 각각 우리가 다음 섹션에 대해 분석하는 로봇 공학에서 개방형 지식 모델을 적용하는 것에 흥미로운 것을 말해준다.\n' +
      '\n' +
      '부록 C에서 우리는 모든 가정 실험과 결과의 세부 사항을 제공하고 부록 B에서는 작동되는 객체 OK-로봇의 하위 집합을 보여준다. 우리의 실험의 에피펫은 그림 1에 있으며 전체 영상은 프로젝트 웹사이트에서 볼 수 있습니다.\n' +
      '\n' +
      '시스템 컴포넌트들(__)에 대한 수정.\n' +
      '\n' +
      '가정 실험에서 사용한 내비게이션 및 조작 전략과 별개로, 우리는 또한 다수의 대체 의미 메모리 모듈과 개방형 어휘 내비게이션 모듈을 평가했다. 우리는 연구실의 세 가지 다른 환경 설정에서 평가함으로써 그것들을 비교했다.\n' +
      '\n' +
      '** 대체 시맨틱 내비게이션 전략**: 다음 의미 메모리 모듈을 평가한다.\n' +
      '\n' +
      '* ** VoxelMap[25]:** VoxelMap:** VoxelMap은 검출된 모든 객체를 의미론적 벡터로 변환하여 이와 같은 정보를 연관된 복셀으로 저장한다. 직업 복셀은 장애물 맵 역할을 합니다.\n' +
      '****CLIP-필드[27]:** CLIP-필드들은 오픈 라벨 객체 검출기 및 시맨틱 언어 임베딩 모델을 사용하여 제기된 RGB-D 이미지의 시퀀스를 의미 벡터 필드로 변환한다. 그 결과는 공간의 각 지점을 VLM[9]을 통해 생성된 의미 벡터 2개와 연관시키고 언어 모델[36]을 통해 생성된 다른 점은 신경 필드[37]에 내장된다.\n' +
      '****USA-Net[26]:** USA-Net은 다중 규모의 CLIP 특징을 생성하여 서명된 거리 분야로도 두 배로 증가하는 신경 분야에 임베딩한다. 그 결과, 단일 모델은 객체 검색과 내비게이션을 모두 지원할 수 있다.\n' +
      '\n' +
      '우리는 동일한 세 가지 환경에서 고정된 쿼리 세트와 비교하고 그 결과는 그림 5에 나와 있다.\n' +
      '\n' +
      '** 대안 파악 전략**: 유사하게, 우리는 여러 파악 전략을 비교하여 우리의 방법에 대한 최상의 파악 전략을 알아보았다.\n' +
      '\n' +
      '* *** 모든 그라즈[19]:** AnyGrasp는 단일 뷰 RGB-D 기반 파악 모델이다. 1B 파악 라벨이 포함된 GraspNet 데이터셋에서 학습됩니다.\n' +
      '* ** 개방 그라팍스 [19]:**는 AnyGrasp 모델이 자유지만 오픈 소스가 아니기 때문에 동일한 데이터셋에서 훈련된 오픈 허가된 기준선을 사용한다.\n' +
      '* ** 연락처-그라즈넷[16]:** 우리는 아크로닉[38] 데이터셋에서 훈련되는 사전 작업 기준선으로 콘택트-그라즈넷을 사용한다. 콘택트-그라즈넷의 한 가지 한계는 테이블탑 설정을 위해 고정된 카메라 뷰로 훈련되었다는 것이다. 그 결과, 이동 카메라와 임의의 위치를 가진 응용 프로그램에서 의미 있는 그릴을 제공하지 못했습니다.\n' +
      '* ** 최고다운 파악[25]:** 휴리스틱 기반 기준선에서 홈 로봇 프로젝트에 제공된 하향다운 휴리스틱 파악과 비교한다.\n' +
      '\n' +
      '그림 5에서 우리는 세 가지 실험실 환경에서 그들의 비교 성능을 보여준다. 의미 메모리 모듈의 경우, 우리는 VoxelMap이 OK-Robot에 사용되며 Sec에 기술되었음을 보여준다. II-A는 작은 마진만큼 다른 의미론적 메모리 모듈을 형성한다. 또한 대안에 비해 분산이 훨씬 낮아 더 신뢰할 수 있다는 것을 의미한다. 모듈을 파악하는 데 있어 AnyGrap은 다른 파악 방법을 분명히 능가하여 다음 최고의 후보인 하향식 파악에 비해 상대적 규모로 거의 50% 더 나은 성능을 발휘한다. 그러나 홈로봇[25]에서 휴리스틱 기반 알고리즘, 하향식 파악이 오픈소스 AnyGrasp 기준선 및 콘택트-GraspNet을 꺾었다는 사실은 진정한 범용 파악 모델을 구축하는 것은 여전히 어렵다는 것을 보여준다.\n' +
      '\n' +
      '###는 어수터, 객체 모호성 및 어포즈__Impact의 영향이다.\n' +
      '\n' +
      '가정 환경을 실험실 실험에 비해 특히 어렵게 만드는 것은 물리적 클러터, 언어 대 대상 매핑 모호성 및 회복하기 어려운 위치의 존재이다. 이러한 요인이 실험에 어떻게 작용하는지에 대한 명확한 이해를 얻기 위해 각 환경에서 두 가지 "세척" 과정을 거친다. 정화하는 동안 이전 라운드에서 모호성이 없는 물체의 하위 집합을 선택하고, 사물 주변의 클러터를 청소하며, 일반적으로 접근할 수 있는 위치에서 이동시킨다. 우리는 각 환경에서 이러한 정화 라운드 중 두 개를 거치게 되는데, 이는 가정 같은 환경의 자연스러운 어려움으로 인한 성능 격차에 대한 통찰력을 제공한다.\n' +
      '\n' +
      '그림. 4: 우리 가정 실험에서 모든 성공 및 실패 사례, 세 가지 청소 단계 모두에 걸쳐 응집되고 실패 모드로 분해된다. 좌측에서 오른쪽으로 OK-로봇의 세 가지 구성 요소를 적용하고 각 구성 요소의 긴 꼬리 고장 모드를 분해하는 것을 보여준다.\n' +
      '\n' +
      '그림. 5: Ablation 실험은 상이한 의미 기억과 모듈을 사용하여 모듈을 식별하며, 막대는 평균 성능을 나타내고 오차 막대는 환경에 대한 표준 편차를 보여준다.\n' +
      '\n' +
      '그림. 6: 새로운 집에서 우리의 방법의 실패 모드는 3개의 모듈과 정화 수준의 실패로 인해 분해되었다.\n' +
      '\n' +
      '그림 6에서 다양한 단계에서 실패한 섹션 III-A에 대한 완전한 분석을 보여주는데, 우리는 환경을 청소하고 모호한 물체를 제거하므로 이 분해에서 알 수 있으며 내비게이션 정확도가 상승하며 전체 오류율은 15%에서 12%로 떨어지고 최종적으로 4%로 내려간다. 유사하게, 우리는 환경에서 클러터를 청소하기 때문에 조작 정확도도 향상되고 오류율도 25%에서 16%로 감소하고 최종적으로 13%로 감소한다는 것을 발견했다. 마지막으로, 드롭-모듈은 라벨 모호성 또는 클러터로부터 발생하는 조작 난이도의 농경이므로, 드롭 프리미티브의 고장 속도는 정화 3상을 통해 대략 일정하게 유지된다.\n' +
      '\n' +
      '### _ OK-Robot__의 성능을 이해한다.\n' +
      '\n' +
      '우리의 방법은 완전히 새로운 환경에서 제로 샷 일반화를 보여줄 수 있지만 실패 모드를 더 잘 이해하기 위해 OK-로봇을 조사한다. 주로 우리는 새로운 집에서 우리의 모델이 어떻게 수행되는지, 가장 큰 과제인 것을 자세히 설명하고 이에 대한 잠재적인 해결책을 논의한다.\n' +
      '\n' +
      '우리는 먼저 그림 6에서 우리의 방법의 3가지 높은 수준의 모듈만을 고려할 때 실패의 거친 수준 고장을 보여준다. 일반적으로 실패의 주요 원인이 직관적으로 가장 어려운 조작 실패임을 알 수 있다. 그러나 좀 더 자세히 살펴보면, 우리는 도 4에 제시된 긴 고장 원인 꼬리를 알 수 있다.\n' +
      '\n' +
      '우리는 실패의 주요 세 가지 원인이 의미 기억으로부터 탐색할 권리 대상을 회수하지 못하고(9.3%), 조작 모듈(8.0%), 하드웨어 어려움(7.5%)에서 어려운 포즈를 취하고 있음을 알 수 있다. 본 절에서는 그림 4에 제시된 고장 모드 분석을 살펴보고 가장 빈번한 사례를 논의하고자 한다.\n' +
      '\n' +
      '오브젝트(***)에 대한**천연 언어 질의: OK-로봇이 실패할 수 있는 주요 이유 중 하나는 사용자가 제공하는 자연어 쿼리가 의미론적 메모리로부터 의도된 객체를 검색하지 않는 경우이다. 그림 7에서 동일한 쿼리의 의미적으로 매우 유사하지만 약간 수정된 문자가 성공할 수 있는 동안 몇 가지 질문이 실패할 수 있는 방법을 보여준다.\n' +
      '\n' +
      '일반적으로 그림과 같이 시각적으로나 의미적으로 유사한 대상이 여러 개 있는 장면들에 대한 사례였다. 다른 매우 유사한 질문이 실패할 수 있는 동안 몇 가지 질문이 통과될 수 있는 다른 경우가 있다. 메모리로부터 객체를 검색함에 따라 사용자로부터 확인을 받는 상호 작용 시스템이 그러한 문제를 피할 것이다.\n' +
      '\n' +
      '** 그리싱 모듈 한계**: 시스템의 잠재적인 실패 모드는 헬로 로봇: 스레치 그리퍼에 대해 설계되지 않은 모델과 단일 RGB-D 이미지를 기반으로 예측된 미리 학습된 모델 생성 그래프들의 출력을 실행함으로써 우리의 조작이 수행된다는 것이다.\n' +
      '\n' +
      '그 결과 그림 8과 같이 간혹 그러한 그물이 신뢰할 수 없거나 비현실적인 경우가 있는데, 로봇 관절 한계를 고려할 때 제안된 파악이 불가피해지거나 로봇 몸체와 너무 거리가 먼 경우가 있다. 더 나은 휴리스틱의 개발은 주어진 객체에 대해 더 잘 발견할 수 있도록 할 것이다. 다른 경우에, 모델은 좋은 파악 포즈를 생성하지만 로봇이 식별된 프리미티브를 실행함에 따라 약간의 환경 장애물과 충돌한다. 우리는 파악 궤적을 계획하지 않고 대신 모든 경우에 동일한 파악 궤적을 적용하려고 하기 때문에 그러한 실패는 불가피하다. 포즈뿐만 아니라 파악 궤적을 생성하는 모델을 더 잘 파악하는 것은 그러한 문제를 해결할 수 있다. 마지막으로, 우리의 파악 모듈은 초콜릿 바, 책과 같이 평평한 물체로 분류적으로 투쟁하는데, 이는 2인용 그리퍼로 표면을 파악하기가 어렵기 때문이다.\n' +
      '\n' +
      '***로봇 하드웨어 제한***\n' +
      '\n' +
      '그림. 8: 조작 모듈의 고장 샘플입니다. 대부분의 실패는 하나의 RGB-D 뷰만을 사용하여 큰 2단 평행 턱 그리퍼의 파악 및 제한 형태 요소를 생성하는 것에서 비롯된다.\n' +
      '\n' +
      '그림. 7:3의 시맨틱 메모리 모듈에 실패하거나 모호한 언어 질의를 제공합니다. 메모리 모듈은 전처리된 대형 비전 언어 모델에 의존하기 때문에, 그것의 성능은 현재 LLM과 유사한 특정 \'인텐션\'에 대한 민감도를 보여준다.\n' +
      '\n' +
      '안녕하세요 로봇: 스레치는 여러 물체를 골라낼 수 있으며 로봇이 조작할 수 있는 것을 결정하고 조작할 수 없는 특정 하드웨어 제한이 있다. 예를 들어, 로봇은 팔이 완전히 연장될 때 1kg(2lbs) 페이로드 제한이 있으며, 이러한 방법은 전체 요리 비누 용기처럼 물체를 이동할 수 없기 때문이다. 마찬가지로 침대의 중간에 있거나 높은 곳에 있는 등 항해 가능한 바닥 공간과는 거리가 먼 객체는 팔의 도달 한계 때문에 로봇이 도달하기 어렵다. 마지막으로, 일부 상황에서 로봇 하드웨어 또는 리얼센스 카메라는 특히 가정에서 지속적인 테스트 동안 시간이 지남에 따라 오열화될 수 있다. 이러한 오열화는 조작 모듈이 로봇에서 손안 조율이 필요하기 때문에 에러로 이어질 수 있다.\n' +
      '\n' +
      '워크숍은\n' +
      '\n' +
      '로봇 네비게이션에 대한 _Vision-Language 모델.\n' +
      '\n' +
      '사전 훈련된 개방형 지식 모델의 초기 적용은 개방형 네비게이션에 있었다. 다양한 대상에 대한 탐색은 더 긴 픽업 작업[40, 41]의 맥락에서뿐만 아니라 광범위한 이전 작품[25, 31, 39]에서 살펴본 중요한 과제이다. 그러나 이러한 방법은 일반적으로 상대적으로 적은 수의 객체[42]에 적용되었다. 최근 오바마자바레[43]는 예를 들어 수천 개의 객체 유형에 대한 항법을 보여주었지만 이 작업의 대부분은 시뮬레이션 또는 고도로 통제된 환경으로 제한되었다.\n' +
      '\n' +
      '이 문제를 다루는 초기 작업은 세프트[44], CLIP-필드[27], VLMaps[45], NLMap-SayCan[46] 및 이후 ConceptFusion[47] 및 LERF [30]와 같은 사전 훈련된 비전 언어 모델에서 파생된 표현을 기반으로 한다. 이러한 모델의 대부분은 미리 덮인 장면에서 객체 위치를 나타내는 반면 CLIP-필드, VLMaps 및 NLMap-Say는 실내 내비게이션 작업을 위해 실제 로봇과 통합을 나타낼 수 있다. USA-Nets[26]은 어포던스 모델을 포함하도록 이 작업을 확장하여 객체 회피를 수행하면서 개방형 동물 질의로 탐색한다. ViNT[48]는 시력-언어 항법 문제에 적용될 수 있는 로봇 항법 기반 모델을 제안한다. 보다 최근에 GOAT[31]는 "모든 것을 시작한다"는 모듈식 시스템으로 제안되었고 모든 환경에서 어떤 물체로도 탐색되었다. 콘셉트그래프[49]는 LLM을 이용하여 복잡한 질의를 처리하고 장면 그래프를 생성할 수 있는 개방형 장면 표현을 제안하였다.\n' +
      '\n' +
      '모범 로봇 조작 모델__2.\n' +
      '\n' +
      '인간이 물건을 자주 보고 어떻게 파악할지 즉시 알 수 있지만, 그러한 지식을 파악하는 것은 로봇에 쉽게 접근할 수 없다. 수년 동안 이러한 일반 로봇 파악 생성 모델[50, 51, 52, 53, 54, 55]을 임의의 물체에 대해 만들고 잠재적으로 학습 방법을 통해 충돌하는 장면들을 만드는 데 중점을 둔 작품이 많았다. 우리의 연구는 큰 식별 데이터 세트[18, 38]에 대해 훈련되는 이러한 방법[16, 19]의 보다 최근의 반복에 중점을 둔다. 이러한 모델은 하나의 작업만을 수행하지만, 즉 큰 물체 표면에 걸쳐 그물을 예측하여 하류 복합체, 긴 수평 조작 작업[20, 21, 56]을 가능하게 한다.\n' +
      '\n' +
      '보다 최근에 단지 [57, 58, 59, 60, 61]을 파악하는 것을 넘어 움직이는 범용 조작 모델 세트가 있다. 이 작품들 중 일부는 일반적인 언어 조건 조작 작업을 수행하지만, 크게 작은 장면들과 사물들의 집합으로 제한된다. HACMan[62]는 밀고 양보하는 데 중점을 둔 더 많은 범위의 객체 조작 능력을 보여준다. 향후 이러한 모델은 시스템의 범위를 확장할 수 있습니다.\n' +
      '\n' +
      '오픈 어휘 로봇 시스템__오픈 어휘 로봇 시스템.\n' +
      '\n' +
      '최근 많은 작품이 복잡한 로봇 시스템에 대한 언어 지원 작업을 수행했습니다. 언어 조건화된 정책 학습[57, 63, 64, 65], 학습 목표 조건 가치 함수[3, 66], 대형 언어 모델을 사용하여 코드[67, 68, 69]를 생성하는 경우도 있다. 그러나, 임의의 객체들에 대해 개방형 방식으로 동작하려는 시스템들, 그리고 언어를 사용하여 제한된 수의 목표들 또는 옵션들 중 하나를 특정할 수 있는 시스템들 사이에 근본적인 차이가 남아 있다. 결과적으로, Open-Vocabulary 모바일 매니지먼트는 로봇 조작(25])의 핵심 과제로 제안되었다. 앞서 이러한 시스템[70, 71] 구축을 위한 노력이 있었다. 그러나 이러한 이전 작업과는 달리 오픈 플랫폼에서 모든 것을 구축하고 새로운 집에 대해 아무것도 재학습하지 않고도 우리의 방법이 작동할 수 있도록 노력합니다. 최근 유니팀[23]이 2023 홈로봇 OVMM 챌린지[22]에서 우리와 유사한 제로샷 일반화 요구사항으로 임의의 물체에 픽업 및 위치를 하는 모듈식 시스템으로 우승했다.\n' +
      '\n' +
      '동시에, 최근에는 GPT 또는 특히 GPT4[35]를 이용한 개방형 굴착 조작을 하는 논문이 다수 있었다. GPT4V는 로봇 작업 계획 프레임워크에 포함될 수 있으며 인간 시연[72]를 포함하여 장거리 로봇 작업을 실행하는 데 사용할 수 있다. 콘셉트그래프[49]는 개방형 사물에 대한 복잡한 객체 검색, 기획 및 픽업-장소 역량을 보여주는 좋은 최근 예이다. SayPlan[73]은 또한 이들들이 매우 크고 복잡한 환경 및 다단계 작업을 처리하기 위해 장면 그래프와 함께 사용할 수 있는 방법을 보여주는데, 이 작업은 선택 및 장소를 구현하는 방법을 처리하지 않기 때문에 우리와 보완적이다.\n' +
      '\n' +
      '연구용 연구용 문제 및 사항 제출.\n' +
      '\n' +
      '우리의 방법은 완전히 새로운 가정 환경에서 상당한 성공을 나타내지만 그러한 방법이 개선될 수 있는 많은 장소도 보여준다. 이 섹션에서는 향후 이러한 잠재적인 개선 사항 중 몇 가지를 논의한다.\n' +
      '\n' +
      'M_Live 의미 기억과 장애물 맵___Live 의미 기억과.\n' +
      '\n' +
      '현재 모든 시맨틱 메모리 모듈과 장애물 맵 구축자들은 세계가 바뀌면서 최신 상태로 유지하는 좋은 방법 없이 세계의 정적 표현을 구축한다. 그러나 주택은 동적 환경이며 매일 하루 동안 많은 작은 변화가 있다. 동적 의미 기억과 장애물 지도를 구축할 수 있는 향후 연구는 그러한 픽 앤 드롭 방법을 상자 밖으로 새로운 집에서 지속적으로 적용할 수 있는 가능성을 해제할 것이다.\n' +
      '\n' +
      '제안서 대신 제안서(_###, _Grasp) 계획을 수립한 것이다.\n' +
      '\n' +
      '현재, 파악 모듈은 로봇의 신체 및 역학을 고려하지 않고 일반적인 그릴을 제안한다. 유사하게, 파악 포즈를 감안할 때, 개방된 루프 파악 궤적은 종종 환경 장애물과 충돌하며, 이는 포즈만을 파악하기보다는 파악 계획을 생성하기 위해 모듈을 사용하여 쉽게 개선할 수 있다.\n' +
      '\n' +
      '로봇과 사용자_의 상호 작용성을 향상한다.\n' +
      '\n' +
      '우리의 방법에서 실패의 주요 원인 중 하나는 내비게이션에 있으며, 여기서 의미 질의는 모호하고 의도된 객체가 의미 기억으로부터 검색되지 않는다. 이러한 모호한 경우, 사용자와의 상호 작용은 질의를 무시하고 로봇이 더 자주 성공할 수 있도록 하기 위해 먼 길을 갈 것이다.\n' +
      '\n' +
      '실패의 감지 및 회복.\n' +
      '\n' +
      '현재 우리는 모듈 간의 곱셈 오류 축적을 관찰하는데, 독립적인 구성 요소 중 어느 것이든 실패하면 전체 과정이 실패한다. 결과적으로 우리 모듈이 각각 80% 이상의 성공률로 독립적으로 수행하더라도 최종 성공률은 여전히 60% 미만일 수 있다. 그러나 더 나은 오류 검출 및 지연 알고리즘으로 훨씬 더 많은 단일 단계 오류로부터 회복될 수 있으며 유사하게 전체 성공률[23]을 개선할 수 있다.\n' +
      '\n' +
      '로봇 하드웨어__이용 로봇 하드웨어.\n' +
      '\n' +
      '헬로로봇 - 스레치[74]는 임의 주택에 대한 이러한 개방형 홈 시스템을 구현할 수 있는 저렴하고 휴대용 플랫폼이지만, 또한 강력한 하드웨어로 이러한 방법이 방대한 용량을 향상시킬 수 있음을 인정한다. 이러한 강력한 하드웨어는 우리가 높고 낮은 장소에 도달하고 더 무거운 물체를 픽업할 수 있게 할 수 있다. 마지막으로 개선된 로봇 냄새 측정으로 오늘 가능한 것보다 훨씬 더 미세한 그릴을 실행할 수 있습니다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'NYU 저자는 아마존, 혼다 및 ONR 수상 번호 N00014-21-1-2404 및 N00014-21-1-2758의 교부금으로 지원되며 NMS는 AI/ML 펠로우십에서 애플 장학사가 지원한다. LP는 패커드 펠로우십에 의해 지원됩니다. 우리의 가장 감사한 것은 집에서 실험을 진행함으로써 우리를 도운 친구들과 동료들에게 간다. 마지막으로, 우리는 귀중한 피드백과 대화를 위해 제이 바칠, 시단 할다르, 파룰라 파시날, 우라이나 피터바그에게 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Lerrel Pinto and Abhinav Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. _ICRA_, 2016.\n' +
      '* [2] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. _The International journal of robotics research_, 37(4-5):421-436, 2018.\n' +
      '* [3] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as I can, not as I say: Grounding language in robotic affordances. _Conference on Robot Learning (CoRL)_, 2022.\n' +
      '* [4] Nur Muhammad Mahi Shafullah, Anant Rai, Haritheja Etukuru, Yiqian Liu, Ishan Misra, Soumith Chintala, and Lerrel Pinto. On bringing robots home, 2023.\n' +
      '* [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. _arXiv preprint arXiv:2212.06817_, 2022.\n' +
      '* [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. _arXiv preprint arXiv:2307.15818_, 2023.\n' +
      '* [7] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krahenbuhl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In _European Conference on Computer Vision_, pages 350-368. Springer, 2022.\n' +
      '* [8] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. Simple open-vocabulary object detection with vision transformers. In _European Conference on Computer Vision_, pages 728-755. Springer, 2022.\n' +
      '* [9] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning (ICML)_, volume 139, pages 8748-8763, 2021.\n' +
      '* [10] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In _Proceedings of the IEEE/cvf conference on computer vision and pattern recognition_, pages 3195-3204, 2019.\n' +
      '* [11] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Mieech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022.\n' +
      '* [12] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding DINO: Marrying DINO with grounded pre-training for open-set object detection. _arXiv preprint arXiv:2303.05499_, 2023.\n' +
      '\n' +
      '* [13] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n' +
      '* [14] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. _OpenAI Blog_, 2019.\n' +
      '* [15] Theophile Gervet, Soumith Chintala, Dhruv Batra, Jitendra Malik, and Devendra Singh Chaplot. Navigating to objects in the real world. _Science Robotics_, 8(79):eadf6991, 2023.\n' +
      '* [16] Martin Sundermeyer, Arsalan Mousavian, Rudolph Triebel, and Dieter Fox. Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 13438-13444. IEEE, 2021.\n' +
      '* [17] Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, and Ken Goldberg. Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. _arXiv preprint arXiv:1703.09312_, 2017.\n' +
      '* [18] Hao-Shu Fang, Chenxi Wang, Minghao Gou, and Cewu Lu. Graspnet-1billion: a large-scale benchmark for general object grasping. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11444-11453, 2020.\n' +
      '* [19] Hao-Shu Fang, Chenxi Wang, Hongjie Fang, Minghao Gou, Jirong Liu, Hengxu Yan, Wenhai Liu, Yichen Xie, and Cewu Lu. Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. _IEEE Transactions on Robotics_, 2023.\n' +
      '* [20] Ankit Goyal, Arsalan Mousavian, Chris Paxton, Yu-Wei Chao, Brian Okorn, Jia Deng, and Dieter Fox. Ifor: Iterative flow minimization for robotic object rearrangement. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14787-14797, 2022.\n' +
      '* [21] Weiyu Liu, Tucker Hermans, Sonia Chernova, and Chris Paxton. Structdiffusion: Object-centric diffusion for semantic rearrangement of novel objects. _arXiv preprint arXiv:2211.04604_, 2022.\n' +
      '* [22] Sriram Yenamandra, Arun Ramachandran, Mukul Khanna, Karmesh Yadav, Devendra Singh Chaplot, Gunjan Chhablani, Alexander Clegg, Theophile Gervet, Vidhi Jain, Ruslan Partsey, Ram Ramrakhya, Andrew Szot, Tsung-Yen Yang, Aaron Edsinger, Charlie Kemp, Binit Shah, Zsolt Kira, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, and Chris Paxton. The homerobot open vocab mobile manipulation challenge. In _Thirty-seventh Conference on Neural Information Processing Systems: Competition Track_, 2023.\n' +
      '* [23] Andrew Melnik, Michael Buttner, Leon Harz, Lyon Brown, Gora Chand Nandi, Arjun PS, Gaurav Kumar Yadav, Rahul Kala, and Robert Haschke. Uniteam: Open vocabulary mobile manipulation challenge. _arXiv preprint arXiv:2312.08611_, 2023.\n' +
      '* [24] Luca Medeiros. Lang segment anything. [https://github.com/luca-medeiros/lang-segment-anything](https://github.com/luca-medeiros/lang-segment-anything), 2023.\n' +
      '* [25] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, et al. Homerobot: Open-vocabulary mobile manipulation. _arXiv preprint arXiv:2306.11565_, 2023.\n' +
      '* [26] Benjamin Bolte, Austin Wang, Jimmy Yang, Mustafa Mukadam, Mrinal Kalakrishnan, and Chris Paxton. Usanet: Unified semantic and affordance representations for robot memory. _arXiv preprint arXiv:2304.12164_, 2023.\n' +
      '* [27] Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel Pinto, Soumith Chintala, and Arthur Szlam. Clip-fields: Weakly supervised semantic fields for robotic memory. _arXiv preprint arXiv:2210.05663_, 2022.\n' +
      '* [28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In _ICCV_, pages 4015-4026, October 2023.\n' +
      '* [29] David Rozenbergszki, Or Litany, and Angela Dai. Language-grounded indoor 3d semantic segmentation in the wild, 2022.\n' +
      '* [30] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 19729-19739, 2023.\n' +
      '* [31] Matthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min, Kavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra, et al. Goat: Go to any thing. _arXiv preprint arXiv:2311.06430_, 2023.\n' +
      '* [32] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Audio visual language maps for robot navigation. _arXiv preprint arXiv:2303.07522_, 2023.\n' +
      '* [33] Santiago Garrido, Luis Moreno, Mohamed Abderrahim, and Fernando Martin. Path planning for mobile robot navigation using voronoi diagram and fast marching. In _2006 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 2376-2381. IEEE, 2006.\n' +
      '* [34] Sudeep Dasari, Abhinav Gupta, and Vikash Kumar. Learning dexterous manipulation from exemplar object trajectories and pre-grasps, 2023.\n' +
      '* [35] OpenAI. GPT-4 technical report, 2023.\n' +
      '* [36] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks, 2019.\n' +
      '* [37] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _European Conference on Computer Vision (ECCV)_, 65(1):99-106, 2020.\n' +
      '* [38] Clemens Eppner, Arsalan Mousavian, and Dieter Fox. Acronym: A large-scale grasp dataset based on simulation. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 6222-6227. IEEE, 2021.\n' +
      '* [39] Arsalan Mousavian, Alexander Toshev, Marek Fiser, Jana Kosecka, Ayzaan Wahid, and James Davidson. Visual representations for semantic target driven navigation. In2019 International Conference on Robotics and Automation (ICRA)_, pages 8846-8852. IEEE, 5 2019.\n' +
      '* [40] Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and Yoav Artzi. A persistent spatial semantic representation for high-level natural language instruction execution. In _Conference on Robot Learning_, pages 706-717. PMLR, 2022.\n' +
      '* [41] So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan Bisk, and Ruslan Salakhutdinov. Film: Following instructions in language with modular methods. _arXiv preprint arXiv:2110.07342_, 2021.\n' +
      '* [42] Matt Deitke, Dhruv Batra, Yonatan Bisk, Tommaso Campari, Angel X Chang, Devendra Singh Chaplot, Changan Chen, Claudia Perez D\'Arpino, Kiana Ehsani, Ali Farhadi, et al. Retrospectives on the embodied ai workshop. _arXiv preprint arXiv:2210.06849_, 2022.\n' +
      '* [43] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13142-13153, 2023.\n' +
      '* [44] Huy Ha and Shuran Song. Semantic abstraction: Open-world 3d scene understanding from 2d vision-language models, 2022.\n' +
      '* [45] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps for robot navigation. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 10608-10615. IEEE, 2023.\n' +
      '* [46] Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana Gopalakrishnan, Michael S. Ryoo, Austin Stone, and Daniel Kappler. Open-vocabulary queryable scene representations for real world planning. In _arXiv preprint arXiv:2209.09874_, 2022.\n' +
      '* [47] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer, Soroush Sayazadi, Nikhil Keetha, Ayush Tewari, et al. Conceptfusion: Open-set multimodal 3d mapping. _arXiv preprint arXiv:2302.07241_, 2023.\n' +
      '* [48] Dhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Stachowicz, Kevin Black, Noriaki Hirose, and Sergey Levine. ViNT: A Foundation Model for Visual Navigation. In _7th Annual Conference on Robot Learning (CoRL)_, 2023.\n' +
      '* [49] Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, et al. Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. _arXiv preprint arXiv:2309.16650_, 2023.\n' +
      '* [50] Abhinav Gupta, Adithyavairavan Murali, Dhiraj Prakashchand Gandhi, and Lerrel Pinto. Robot learning in homes: Improving generalization and reducing dataset bias. _Advances in Neural Information Processing Systems_, 31:9094-9104, 2018.\n' +
      '* [51] Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, and Ken Goldberg. Dex-Net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. In _Robotics: Science and Systems (RSS)_, 2017.\n' +
      '* [52] Jeffrey Mahler, Matthew Matl, Xinyu Liu, Albert Li, David Gealy, and Ken Goldberg. Dex-net 3.0: Computing robust robot vacuum suction grasp targets in point clouds using a new analytic model and deep learning, 2018.\n' +
      '* [53] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. QT-Opt: Scalable deep reinforcement learning for vision-based robotic manipulation. _arXiv preprint arXiv:1806.10293_, 2018.\n' +
      '* [54] Yuzhe Qin, Rui Chen, Hao Zhu, Meng Song, Jing Xu, and Hao Su. S4g: Amodal single-view single-shot se(3) grasp detection in cluttered scenes, 2019.\n' +
      '* [55] Arsalan Mousavian, Clemens Eppner, and Dieter Fox. 6-dof graspnet: Variational grasp generation for object manipulation, 2019.\n' +
      '* [56] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, and D. Fox. Progr prompt: Generating situated robot task plans using large language models. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, page 11523, 2023.\n' +
      '* [57] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-Actor: A multi-task transformer for robotic manipulation. In _CoRL_, pages 785-799. PMLR, 2023.\n' +
      '* [58] Priyam Parashar, Jay Vakil, Sam Powers, and Chris Paxton. Spatial-language attention policies for efficient robot learning. _arXiv preprint arXiv:2304.11235_, 2023.\n' +
      '* [59] Nur Muhammad Shafiullah, Zichen Cui, Arintuya Arty Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning \\(k\\) modes with one stone. _Advances in neural information processing systems_, 35:22955-22968, 2022.\n' +
      '* [60] Zichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. From play to policy: Conditional behavior generation from uncurated robot data, 2022.\n' +
      '* [61] Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Katerina Fragkiadaki. Act3d: 3d feature field transformers for multi-task robotic manipulation. In _Conference on Robot Learning_, pages 3949-3965. PMLR, 2023.\n' +
      '* [62] Wenxuan Zhou, Bowen Jiang, Fan Yang, Chris Paxton, and David Held. Learning hybrid actor-critic maps for 6d non-prehensile manipulation. _arXiv preprint arXiv:2305.03942_, 2023.\n' +
      '* [63] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. CLIPort: What and where pathways for robotic manipulation. In _CoRL_, pages 894-906. PMLR, 2022.\n' +
      '* [64] Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet. Learning latent plans from play. In _CoRL_, pages 1113-1132. PMLR, 2020.\n' +
      '* [65] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data. _RoboticsScience and Systems_, 2021.\n' +
      '* [66] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. VoxPoser: Composable 3D value maps for robotic manipulation with language models. In _CoRL_, 2023.\n' +
      '* [67] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as Policies: Language model programs for embodied control. In _icra_, pages 9493-9500. IEEE, 2023.\n' +
      '* [68] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. _arXiv preprint arXiv: Arxiv-2305.16291_, 2023.\n' +
      '* [69] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. ProgPrompt: Generating situated robot task plans using large language models. In _ICRA_, pages 11523-11530. IEEE, 2023.\n' +
      '* [70] Naoki Yokoyama, Alex Clegg, Joanne Truong, Eric Undersander, Tsung-Yen Yang, Sergio Arnaud, Sehoon Ha, Dhruv Batra, and Akshara Rai. ASC: Adaptive skill coordination for robotic mobile manipulation. _arXiv preprint arXiv:2304.00410_, 2023.\n' +
      '* [71] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, Chelsea Finn, and Karol Hausman. Open-world object manipulation using pre-trained vision-language model. In _arXiv preprint_, 2023.\n' +
      '* [72] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi. Gpt-4v(ision) for robotics: Multimodal task planning from human demonstration. _arXiv preprint arXiv:2311.12015_, 2023.\n' +
      '* [73] Krishan Rana, Jesse Haviland, Sourav Garg, Jad Aboub-Chakra, Ian Reid, and Niko Suenderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. _arXiv preprint arXiv:2307.06135_, 2023.\n' +
      '* [74] Charles C Kemp, Aaron Edsinger, Henry M Clever, and Blaine Matulevich. The design of stretch: A compact, lightweight mobile manipulator for indoor human environments. In _2022 International Conference on Robotics and Automation (ICRA)_, pages 3150-3157. IEEE, 2022.\n' +
      '\n' +
      '응답자 A Scannet200 텍스트 쿼리\n' +
      '\n' +
      'OWL-ViT를 사용하여 주어진 가정 환경에서 물체를 검출하기 위해 Scannet200 라벨을 사용한다. \'반죽\', \'탄수화물\', \'탄수화물\', \'탄수화물\', \'탄수화물\', \'탄수화물\', \'채식\', \'포장품\', \'방목\', \'조품\', \'여행\', \'여행\', \'여행\', \'비행기\', \'여행\', \'여행\', \'여행\', \'여행\', \'보행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'통\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'조품\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'여행\', \'\n' +
      '\n' +
      '우리 재판에서 B 샘플 객체를 검색합니다.\n' +
      '\n' +
      '실험 동안 헬로 로봇: 홈 환경에서 스레치 그리퍼에 의해 그럴듯하게 조작될 수 있는 물체를 샘플링하려고 노력했다. 그 결과 OK-로봇은 서로 다른 모양과 시각적 특징을 가진 다양한 물체를 접했다. 이러한 물체의 하위 샘플은 그림 9, 10에 나와 있다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n' +
      '## Appendix A\n' +
      '\n' +
      '그림. 10: 가정 실험에 있는 샘플 객체는 OK-로봇 **failed***를 사용하여 성공적으로 픽업하는 각 가정 환경에서 샘플링했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Fit object & Price location & Result \\\\ \\hline Home 1 & \\\\ \\hline \\hline \\multicolumn{3}{c}{Cleanup level: none} \\\\ silver cup & white table & Success \\\\ blue eye glass case & chair & Success \\\\ printed paper cup, coffee cup [white table] & Manipulation failure \\\\ small red and white medication & Chair & Success \\\\ power adapter & Grey Bed & Success \\\\ wrapped paper & Navigation failure \\\\ blue body wash & study table & Success \\\\ blue air spray & white table & Success \\\\ black face wash & Manipulation failure \\\\ yellow face wash & chair & Success \\\\ body spray & Navigation failure \\\\ small hand sanitizer & & Manipulation failure \\\\ blue inhaler device(window) & white table & Success \\\\ inhaler box(window) & dust bin & Success \\\\ multivitamin container & & Navigation failure \\\\ red towel & white cloth bin (air conditioner) & Success \\\\ white shirt & white cloth bin (air conditioner) & Success \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: low} \\\\ silver cup & white table & Success \\\\ blue eye glass case & Navigation failure \\\\ printed paper cup, coffee cup [white table] & dust bin & Success \\\\ small red and white medication & Chair & Success \\\\ power adapter & Navigation failure \\\\ blue body wash & white table & Success \\\\ blue air spray & white table & Success \\\\ yellow face wash & white table & Success \\\\ small hand sanitizer & study table & Success \\\\ blue inhaler device(window) & & Manipulation failure \\\\ inhaler box(window) & dust bin & Success \\\\ red towel & white cloth bin(air conditioner) & Success \\\\ white shirt & white cloth bin(air conditioner) & Success \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: high} \\\\ silver cup & white table & Success \\\\ printed paper cup, coffee cup [white table] & dust bin & Success \\\\ blue body wash & white table & Success \\\\ blue air spray & white table & Success \\\\ yellow face wash & & Manipulation failure \\\\ small hand sanitizer & & Manipulation failure \\\\ inhaler box(window) & dust bin & Success \\\\ white shirt & white cloth bin(air conditioner) & Success \\\\ \\hline \\multicolumn{3}{c}{Home 2} \\\\ \\multicolumn{3}{c}{Cleanup level: None} \\\\ \\multicolumn{3}{c}{fanta can} & dust bin & Success \\\\ tennis ball & small red shopping bag & Success \\\\ black head band [bed] & Manipulation failure \\\\ purple shampoo bottle & white rack & Success \\\\ toothpaste & small red shopping bag & Success \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: A list of all tasks in the home environments, along with their categories and success rates out of 10 trials.\n' +
      '\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{1}{c}{Price Isolation} & \\multicolumn{1}{c}{Result} \\\\ \\hline orange packaging & dust bin & Success \\\\ green hair cream jar [white rack] & \\multicolumn{1}{c}{} & Navigation failure \\\\ green detergent pack [white rack] & white table & Success \\\\ blue moisturizer [white rack] & \\multicolumn{1}{c}{} & Navigation failure \\\\ green plastic cover & \\multicolumn{1}{c}{} & Navigation failure \\\\ storage container & \\multicolumn{1}{c}{} & Manipulation failure \\\\ blue hair oil bottle & \\multicolumn{1}{c}{} & White rack & Success \\\\ blue pretzels pack & \\multicolumn{1}{c}{} & White rack & Success \\\\ blue hair gel tube & \\multicolumn{1}{c}{} & Manipulation failure \\\\ red bottle [white rack] & \\multicolumn{1}{c}{} & brown desk & Success \\\\ blue bottle [air conditioner] & white cloth bin(air conditioner) & Success \\\\ wallet & \\multicolumn{1}{c}{} & Manipulation failure \\\\ \\hline  & \\multicolumn{1}{c}{} & \\\\ \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{1}{c}{} & \\\\ \\hline fanta can & black trash can & Success \\\\ tennis ball & red target bag & Success \\\\ black head band [bed] & red target bag & Success \\\\ purple shampoo bottle & red target bag & Success \\\\ toothpaste & red target bag & Success \\\\ orange packaging & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ green detergent pack [white rack] & \\multicolumn{1}{c}{} & Manipulation failure \\\\ blue moisturizer [white rack] & \\multicolumn{1}{c}{} & Navigation failure \\\\ blue hair oil bottle & white rack & Success \\\\ blue pretzels pack & \\multicolumn{1}{c}{} & White rack & Success \\\\ wallet & \\multicolumn{1}{c}{} & Manipulation failure \\\\ \\hline  & \\multicolumn{1}{c}{} & \\\\ \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{1}{c}{} & \\\\ \\hline fanta can & black trash can & Success \\\\ purple shampoo bottle & small red shopping bag & Success \\\\ orange packaging & black trash can & Success \\\\ blue moisturizer [white rack] & white rack & Success \\\\ blue hair oil bottle & \\multicolumn{1}{c}{} & Manipulation failure \\\\ blue hair gel tube & \\multicolumn{1}{c}{} & \\\\ red bottle [white rack] & \\multicolumn{1}{c}{} & blue bottle [air conditioner] & Success \\\\ \\hline  & \\multicolumn{1}{c}{} & \\\\ \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{1}{c}{} & \\\\ \\hline \\hline  & \\multicolumn{1}{c}\n' +
      '\n' +
      '\\begin{tabular}{l c c} \\hline \\multicolumn{3}{c}{Cleanup level: low} \\\\ \\hline apple & white plate & Success \\\\ ice cream & red basket & Success \\\\ green lime juice bottle & red basket & Success \\\\ yellow packet & green bag & Success \\\\ red packet & & Manipulation failure \\\\ orange can & card board box & Success \\\\ cooking oil bottle & marble surface [red basket] & Success \\\\ green bowl & & Manipulation failure \\\\ washing gloves & sink & Success \\\\ small oregano bottle & red basket & Success \\\\ yellow noodles packet [stove] & & Manipulation failure \\\\ blue dish wash bottle & card board box & Success \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: high} \\\\ apple & white plate & Success \\\\ ice cream & red basket & Success \\\\ green lime juice bottle & red basket & Success \\\\ orange can & card board box & Success \\\\ cooking oil bottle & & Manipulation failure \\\\ washing gloves & sink & Success \\\\ small oregano bottle & red basket & Success \\\\ yellow noodles packet [stove] & red basket & Success \\\\ blue dish wash bottle & card board box & Success \\\\ \\hline \\multicolumn{3}{c}{House 4} \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: none} \\\\ \\hline pepsi & black chair & Success \\\\ birdie & cloth bin & Success \\\\ black hat & & Navigation failure \\\\ owl like wood carving & bed & Success \\\\ red inhaler & & Manipulation failure \\\\ black sesame seeds & & Manipulation failure \\\\ banana & & Manipulation failure \\\\ loose-leaf herbal tea jar & black chair & Success \\\\ red pencil sharpener & & Navigation failure \\\\ fast-food French fries container & blue shopping bag [metal drying rack] & Placing failure \\\\ milk & plastic storage drawer unit & Success \\\\ socks[bed] & & Navigation failure \\\\ purple gloves & & Manipulation failure \\\\ target bag & cloth bin & Success \\\\ muffin & grey bed & Success \\\\ tissue paper & table & Success \\\\ grey eyeglass box & & Manipulation failure \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: low} \\\\ \\hline pepsi & basket & Success \\\\ birdie & white drawer & Success \\\\ owl like wood carving & & Navigation failure \\\\ red inhaler & plastic storage drawer unit & Success \\\\ black sesame seeds & bed & Success \\\\ loose-leaf herbal tea jar & table & Success \\\\ fast-food French fries container & chair & Success \\\\ \\hline \\multicolumn{3}{c}{Continued on the next page} \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Pie subject & Piex location & Result \\\\ \\hline milk & chair & Success \\\\ purple gloves & basket & Success \\\\ target bag & basket & Placing failure \\\\ muffin & table & Success \\\\ tissue paper & & Manipulation failure \\\\ grey eyeglass box & & Navigation failure \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: high} \\\\ \\hline pepsi & basket & Success \\\\ birdie & bed & Success \\\\ red inhaler & plastic storage drawer unit & Success \\\\ black sesame seeds & desk & Success \\\\ banana & & Manipulation failure \\\\ loose-leaf herbal tea jar & & Manipulation failure \\\\ milk & chair & Success \\\\ purple gloves & basket & Success \\\\ target bag & basket & Success \\\\ muffin & bed & Success \\\\ \\hline \\multicolumn{3}{c}{Home.S} \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: none} \\\\ \\hline tiger balm topical ointment & & Navigation failure \\\\ pink shampoo & trader joes shaping bag & Success \\\\ aveeno sunscreen protective lotion & trader joes shaping bag & Success \\\\ small yellow nozzle spray & & Manipulation failure \\\\ black hair care spray & & Manipulation failure \\\\ green hand sanitizer & & Manipulation failure \\\\ white hand sanitizer & & Navigation failure \\\\ white bowl [ketch] & black sofa chair & Success \\\\ blue bowl & & Manipulation failure \\\\ blue sponge & trader joes shaping bag & Success \\\\ ketchup & & Manipulation failure \\\\ white salt & & Manipulation failure \\\\ black pepper & black drawer & Success \\\\ blue bottle & & Navigation failure \\\\ purple light bulb box & trader joes shopping bag & Success \\\\ white plastic bag & bed & Success \\\\ rag & white rack & Success \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: low} \\\\ pink shampoo & & Navigation failure \\\\ aveeno sunscreen protective lotion & & Manipulation failure \\\\ small yellow nozzle spray & & Manipulation failure \\\\ white bowl [ketch] & black sofa chair & Success \\\\ blue sponge & bed & Success \\\\ ketchup & trader joes shopping bag & Success \\\\ white salt & trader joes shopping bag & Success \\\\ black pepper & & Navigation failure \\\\ blue bottle & black sofa chair & Success \\\\ purple light bulb box & & Manipulation failure \\\\ rag & white rack & Success \\\\ \\hline \\multicolumn{3}{c}{Cleanup level: high} \\\\ pink shampoo & trader joes shopping bag & Success \\\\ \\hline \\multicolumn{3}{c}{Continued on the next page} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '[FIGURE\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{2}{c}{Cleanup level: none} \\\\ \\cline{2-3} blue plastic bag roll & Navigation failure \\\\ green bag & basket[window] & Success \\\\ toy cactus & desk & Success \\\\ toy van & chair & Success \\\\ brown medical bandage & chair & Success \\\\ power adapter & Navigation failure \\\\ red herbal tea & brown cardboard box & Success \\\\ apple juice box & brown cardboard box & Success \\\\ paper tow & blue cardboard box & Success \\\\ toy bear & bed blanket & Success \\\\ yellow ball & bed blanket & Success \\\\ black pants & basket[window] & Success \\\\ purple water bottle & desk & Success \\\\ blue eyeglass case & & Manipulation failure \\\\ brown toy monkey & & Navigation failure \\\\ blue hardware box [table] & blue cardboard box & Success \\\\ green zandu balm container & & blue cardboard box & Success \\\\ \\hline  & \\multicolumn{2}{c}{Cleanup level: low} \\\\ green bag & basket & Success \\\\ toy cactus & basket & Success \\\\ toy van & chair & Success \\\\ brown medical bandage & Manipulation failure \\\\ red herbal tea & brown box & Success \\\\ apple juice box & brown box & Success \\\\ paper tow & basket & Success \\\\ toy bear & desk & Success \\\\ purple water bottle & desk & Success \\\\ blue eyeglass case & & Manipulation failure \\\\ green zandu balm container & blue cardboard box & Success \\\\ \\hline  & \\multicolumn{2}{c}{Cleanup level: high} \\\\ green bag & stool [window] & Success \\\\ toy cactus & table & Success \\\\ toy van & white basket & Success \\\\ red herbal tea & brown cardboard box & Success \\\\ apple juice box & brown cardboard box & Success \\\\ paper tow & blue cardboard box & Success \\\\ toy bear & white basket & Success \\\\ yellow ball & bed & Success \\\\ purple water bottle & black tote bag & Success \\\\ green zandu balm container & blue cardboard box & Success \\\\ \\hline  & \\multicolumn{2}{c}{House S} \\\\ \\cline{2-3}  & \\multicolumn{2}{c}{Cleanup level: none} \\\\ cyan air spray & brown shelf [sink] & Success \\\\ blue gloves & kitchen sink & Success \\\\ blue peanut butter & black stove & Success \\\\ nutella & table & Success \\\\ green bag & brown shelf [sink] & Success \\\\ green bandage box & trash can & Success \\\\ green detergent & kitchen sink & Success \\\\ black ‘red pepper sauce’ & & Manipulation failure \\\\ \\hline  & \\multicolumn{2}{c}{Continued on the next page} \\\\ \\cline{2-3}  & \\multicolumn{2}{c}{Cleanup level: none} \\\\ cyan air spray & brown shelf [sink] & Success \\\\ blue gloves & kitchen sink & Success \\\\ blue peanut butter & black stove & Success \\\\ nutella & table & Success \\\\ green bag & brown shelf [sink] & Success \\\\ green bandage box & trash can & Success \\\\ green detergent & kitchen sink & Success \\\\ black ‘red pepper sauce’ & & Manipulation failure \\\\ \\hline  & \\multicolumn{2}{c}{Continued on the next page} \\\\ \\cline{2-3}  & \\multicolumn{2}{c}{Cleanup level: none} \\\\ cyan air spray & brown shelf [sink] & Success \\\\ blue gloves & kitchen sink & Success \\\\ blue peanut butter & black stove & Success \\\\ nutella & table & Success \\\\ green bag & brown shelf [sink] & Success \\\\ green bandage box & trash can & Success \\\\ green detergent & kitchen sink & Success \\\\ black ‘red pepper sauce’ & & Manipulation failure \\\\ \\hline  & \\multicolumn{2}{c}{Continued on the next page} \\\\ \\cline{2-3}  & \\multicolumn{2}{c}{Cleanup level: none} \\\\ cyan air spray & brown shelf [sink] & Success \\\\ blue gloves & kitchen sink & Success \\\\ blue peanut butter & black stove & Success \\\\ nutella & table & Success \\\\ green bag & brown shelf [sink] & Success \\\\ green bandage box & trash can & Success \\\\ green detergent & kitchen sink & Success \\\\ black ‘red pepper sauce’ & & Manipulation failure \\\\ \\hline  & \\multicolumn{2}{c}{Continued on the next page} \\\\ \\cl\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:23]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:24]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>