<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '확산 모델들# EmerDiff에서 픽셀 수준의 지식.\n' +
      '\n' +
      ' 코니치 나메카타\\({}^{1,2}\\), Amirmojtaba Sabour\\({}^{1,2}\\), 산자 피들러\\({}^{1,2,3}\\), 승욱 김\\({}^{1,2,3}\\)\n' +
      '\n' +
      '\\({}^{2}\\) 제조업체 연구소,\\({}^{2}\\) 및\\({}^{3}\\)\n' +
      '\n' +
      '토론토.ca, {amabour, fidler, seung}@cs.toronto}@cs.toronto.edkata@mail.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '확산 모델은 최근 의미 세분화 작업에서 놀라운 전달 능력에 대한 연구 관심이 증가하고 있다. 그러나 확산 모델을 가진 미세 제조 분할 마스크를 생성하는 것은 종종 주석이 달린 데이터셋에 대한 추가 훈련을 필요로 하며, 사전 훈련된 확산 모델만으로는 생성된 이미지의 의미 관계를 어느 정도 이해하는지 불분명하다. 이 질문을 해결하기 위해, 우리는 스테이블 디퓨전(SD)에서 추출된 의미 지식을 레버리지하고, 추가적인 훈련 없이 미세 구성 분할 맵을 생성할 수 있는 이미지 세그먼터를 개발하는 것을 목표로 한다. 1차 어려움은 의미적으로 의미 있는 특징 맵이 전형적으로 공간적으로 낮은 차원 계층에만 존재한다는 사실에서 비롯되며, 이는 이러한 특징 맵으로부터 픽셀 수준 의미 관계를 직접 추출하는 데 어려움을 초래한다. 이 문제를 극복하기 위해 우리의 프레임워크는 SD의 생성 과정을 활용하여 이미지 픽셀과 저차원 특징 맵의 공간 위치 사이의 의미적 대응성을 식별하고 이미지 해상도 분할 맵 구축을 위해 활용한다. 광범위한 실험에서 생성된 분할 맵은 이미지의 세부 부분을 잘 묘사하고 캡처하는 것으로 입증되었으며, 이는 확산 모델에서 매우 정확한 픽셀 수준 의미 지식의 존재를 나타낸다. 프로젝트 페이지:_[프로젝트/프로젝트/EmerDiff/] (https://kmcodel.githubio/프로젝트/프로젝트/EmerDiff/)] (https://kmcodel.github.\n' +
      '\n' +
      '그림 1: EmerDiff은 미리 학습된 확산 모델에서 추출된 의미 지식에만 구축된 비지도 이미지 세그먼트이다. 얻어진 미세 상세 분할 맵은 확산 모델에서 매우 정확한 픽셀 수준 의미 지식의 존재를 시사한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 몇 년 동안 확산 모델(호 et al, 2020, Dhariwal and Nichol, 2021)이 고품질의 이미지를 합성하기 위한 최첨단 생성 모델로 부상했다. 특히, 사전 훈련된 확산 모델의 내부 표현은 의미적으로 풍부하여 의미론적 세분화 작업(바반륵 등 알, 2022; 카르자자 등, 2023; Li et al., 2023; Xu et al., 2023a)에서 인상적인 전달 능력을 보여준다. 그러나 사전 훈련된 확산 모델만으로는 생성된 이미지의 의미 관계를 어느 정도 이해하지는 못하는 것은 물론 마스크 주석(바만묵 등, 2022년, Xu et al., 2023a), 핸드메이드화된 제사(카라지자 등, 2023년, 마 등은 2023년)와 같은 추가 지식의 통합에 의존하는 경우가 많다. 이 질문을 해결하기 위해 미리 훈련된 확산 모델에서 추출한 의미 지식만을 활용함으로써 미세 개질된 분할 맵을 생성할 수 있는 비지도 이미지 세그먼터를 제시한다. 구체적으로, 우리의 작업은 다양한 고해상도 이미지를 생성할 수 있는 대규모 텍스트 조건 확산 모델인 스테이블 디퓨전(Rombach et al., 2022)에 구축된다.\n' +
      '\n' +
      '확산 모델에서 의미론적 관계를 포착하기 위한 일반적인 비지도 접근법은 의미적으로 의미 있는 특징 맵에 k-means를 적용하는 것이다. 이 기술은 의미적으로 정렬된 군집 맵(바만묵 등, 2022; 파슈니크 등, 2023; Xu et al., 2023a)을 생성하는 것으로 입증되었다. 이 방법은 확산 모델에 대한 의미론적 인식을 직관적으로 시각화하는 반면, 얻은 의미론적 관계는 의미적으로 의미 있는 특징 맵들이 전형적으로 공간적으로 저차원 계층에만 상주한다는 사실(콜린 등, 2020년, 바만묵 등, 2022년, 루고 등) 때문에 종종 거친 것이다. 그럼에도 불구하고 확산 모델은 저해상도 특징 맵에 내장된 의미적 지식을 바탕으로 고해상도 이미지를 구성하는 능력을 가지고 있다. 이는 의미적으로 의미 있는 저해상도 특징 맵이 생성 과정을 통해 출력 이미지에 어떻게 영향을 미치는지 분석하기 위해 동기를 부여하며, 이는 확산 모델에서 픽셀 수준 의미 지식을 추출하는 중요한 단계라고 가정한다.\n' +
      '\n' +
      '이를 위해, 우리는 생성된 이미지의 픽셀 값에 대한 저해상도 특징 맵의 값의 국소 변화의 영향을 조사한다. 우리의 핵심 발견은 저해상도 특징 맵의 하위 영역의 값을 교란할 때 생성된 이미지는 해당 하위 영역과 의미 있게 관련된 픽셀만 현저하게 변경되는 방식으로 변경된다는 것이다. 결과적으로, 우리는 픽셀 값의 변화를 단순히 측정함으로써 이미지 픽셀과 저차원 특징 맵의 서브 영역 사이의 의미 대응성을 자동으로 식별할 수 있다.\n' +
      '\n' +
      '이 통찰을 기반으로 제안된 이미지 세그먼터는 추가 지식이 필요 없이 미세 구성 분할 맵을 생성할 수 있다. 먼저 저차원 특징 맵에 k-means를 적용하여 _낮은 해상도 분할 맵_(예: \\(16\\)를 생성한다. 그런 다음, 우리는 각 이미지 픽셀을 가장 의미 있게 대응하는 저해상도 마스크에 매핑하여 하향다운 방식으로 _ 이미지-해상도 분할 맵_(예: \\(512\\·512\\)를 구축한다. 이러한 의미적 대응은 앞서 언급한 발견을 기반으로 하는 확산 모델로부터 추출된다.\n' +
      '\n' +
      '우리의 프레임워크의 효과는 COCO-Stuff(Caesar et al., 2018), PASCAL-Context(Mottaghi et al., 2014), ADE20K(Z2023 et al., 2019), 시티 스코프(Cordts et al., 2016)와 같은 여러 장면 중심 데이터셋에서 질적으로나 양적으로 광범위하게 평가된다. 기저 확산 모델은 주석이 달린 데이터셋에 대해 훈련되지 않지만, 우리의 프레임워크는 이미지의 세부 부분과 놀라울 정도로 잘 정렬되는 분할 맵을 생성하며, 이는 확산 모델에서 매우 정확한 픽셀 수준 의미 지식의 존재를 나타낸다.\n' +
      '\n' +
      '2개의 관련 작업이요.\n' +
      '\n' +
      '시맨틱 세분화를 위한*** 생성 모델. 의미 세분화를 위한 생성 모델의 사용은 시맨틱 분할을 위한 생성 모델의 사용을 다시 GAN의 시대로 거슬러 올라가는데, 여기서 콜린 등(2020)은 스타일GAN의 (카라스 등) 중간 특징 맵에 k-means를 적용한 것을 발견했으며 중간 특징 맵은 시맨틱 객체와 잘 정렬되는 군집을 생성한다. 그 후 선행 작품(Li et al, 2021; Trittrong et al, 2021; Xu 및 Zss, 2021; Zhang et al, 2021; Li et al., 2022)은 이러한 의미 있는 특징 지도를 활용하여 최소한의 감독을 통해 의미 있는 세분화를 학습했다. 특히, 확산 모델은 중간 특징들의 클러스터가 일관되게 그룹 시맨틱 객체들을 매핑하는 유사한 특성을 나타낸다(바반륵 등 알, 2022; Xu et al., 2023a). 이러한 특징 맵들은 키포인트 매칭(Hedlin et al., 2023; 루오 et al., 2023; 탕 et al.,2023a; Zhang et al.,2023) 및 의미 세분화(바반륵 et al., 2022; Li et al., 2023; Xu et al., 2023a)를 포함한 다양한 다운스트림 작업들에서 활용되며 GAN 기반 대응물을 능가한다. 또한 텍스트 조건 확산 모델(Balaji et al., 2022; Rombach et al., 2022; Saharia et al., 2022)의 교차 의도 계층은 객체 레이아웃(Hertz et al., 2022; 패타슈니크 et al., 2023; 탕 et al., 2023b)을 결정하기 위해 사용된다. 그러나 그러한 의미 있는 특징 맵들은 일반적으로 이미지 해상도보다 공간 해상도가 현저히 낮은 저차원 레이어에 존재한다. 업샘플링되고 정제된 분할 지도를 얻기 위해 이전 문헌은 경계 정제 기술(Krahenbuhl 및 Koltun, 2011; Barron and Poole, 2016; 아르슬란노프 및 Roth, 2020; 왕 et al., 2023)과 같은 처리 후 도구를 통합했지만 수작업 사제에 의존한다. 대조적으로, 우리의 프레임워크는 추가적인 지식이 필요하지 않은 채 저해상도 특징 맵(예: \\(16\\·16\\)에서 미세 재배열된 분할 마스크(예: \\(512\\)를 성공적으로 생성한다.\n' +
      '\n' +
      '*** 비지도 시맨틱 분할*** 비지도 시맨틱 분할은 주석이 달린 데이터 세트를 보지 않고 표지되지 않은 이미지의 픽셀을 의미 있게 의미 있는 개념으로 그룹화하는 작업이다. 이전 연구는 염기성 객체(반계비케 등 2021, 2021; 멜라스-키리아지 등, 2022; 멜라스-키니 et al., 2022; Simeoni et al., 2022; Simeoni et al.,2022; Zadaianchuk et al., 2023) 또는 전체 장면(지 et al., 2019; Cho et al, 2022; Wen et al, 2022; Wen et al. Wen et al)을 분할하거나 전체 장면(지 et al. 일반적으로 이러한 프레임워크는 자기 지도 학습을 통해 픽셀 임베딩을 생성하는 이미지 인코더를 학습시키는 이미지 인코더 1은 픽셀들을 미리 정의된 수의 의미 개념들로 그룹화하는 데 사용되는 개념 임베딩을 학습한다. 추론하는 동안, 픽셀들은 그들의 픽셀 임베딩들을 가장 가까운 개념 임베딩들과 매칭시켜 이들 개념들 중 하나로 분류된다. 현재 장면의 세분화를 위한 최첨단 접근법(함일턴 등 알, 2022, 세이처 등, 2022, 웨인 등)은 DINO(카온 등 2021)와 같은 사전 훈련된 자기 지도 ViT에 구축된다. 또한 전경 객체(Voynov et al., 2020; Abdal et al., 2021; Abdal et al., al., al., 2022; 멜라스-키리아지 et al., Feng et al., 2023; 구필드 et al., 2023)를 분할하기 위해 GAN의 잠재 공간을 활용하는 여러 연구가 존재하지만 이러한 접근법은 좁은 시각적 도메인에서만 작동하고 장면 중심 이미지에 적용할 수 없다.\n' +
      '\n' +
      '** 개방-보크 시맨틱 세분화** Open-vocabulary 의미 세분화는 추론 동안 임의의 사용자 정의 어휘에 따라 이미지를 분할하는 것을 목표로 한다. 모델들은 전형적으로 텍스트 이미지 쌍(Xu et al., 2022; 저우 et al., 2022; 차 et al., 2023; 묵호지 et al., 2023; 루싱허 et al., 2023; Xu et al., Xu et al., 2023b), 또는 표지되지 않은/표지된 주석 및 텍스트 이미지 감독의 조합(Ghiasi et al, 2022; 리시마 et al., 2022; 2022; Xu et al., 2022; Xu et al. 이들 모델의 대부분은 CLIP(라드포드 등 2021)와 같은 사전 훈련된 비전 언어 모델의 이미지 인코더에 구축되지만 텍스트 임베딩과의 더 나은 픽셀 수준 정렬을 나타내는 특징 표현을 생성하기 위해 학습한다. 그러나 주석이 달린 데이터셋 없이 훈련된 기존 주석 없는 모델은 시끄러운 분할 마스크를 생산하는 경향이 있다. 이 문제를 극복하기 위해 프레임워크를 이러한 모델에 통합합니다. 구체적으로, 프레임워크에서 생성된 각 분할 마스크에 대해, 우리는 그들의 이미지 인코더를 통해 마스크 임베딩을 계산하고 텍스트 임베딩으로 분류한다. 우리의 프레임워크의 분할 능력과 분류 능력을 결합하여 훨씬 더 나은 mIoU를 달성합니다.\n' +
      '\n' +
      '## 3 Methods\n' +
      '\n' +
      '그림 2에 도시된 바와 같이, 우리의 목표는 사전 훈련된 확산 모델에서 추출한 의미 지식만을 활용함으로써 미세 편성된 분할 지도를 생성하는 것이다. 이를 달성하기 위해 의미적으로 의미 있는 저차원 특징 맵(섹션 3.2)에 k-means를 적용하여 _낮은 해상도 분할 맵_을 생성함으로써 시작한다. 다음으로, 각 이미지 픽셀을 가장 의미 있게 대응하는 저해상도 마스크에 매핑하여 이미지 해상도 분할 맵을 구성한다. 이미지 픽셀과 마스크 사이의 의미적 대응성을 찾기 위해, 우리는 저해상도 특징 맵으로부터 고해상도 이미지를 생성하는 확산 모델의 메커니즘을 이용한다(섹션 3.3). 다음 섹션에서 먼저 확산 모델(섹션 3.1)의 특성에 대한 개요를 제공한 다음 접근법에 대한 추가 세부 정보를 설명한다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '확산 모델은 순수 가우시안 노이즈에서 연속적인 변성 단계를 취하여 이미지를 생성하기 위해 훈련되며, 여기서 각 변성 단계는 하향 및 상향 경로를 포함하는 U-Net 백본(론네버거 등, 2015)으로 일반적으로 수행된다. 구체적으로, 우리의 프레임워크는 공간적으로 하위차원 잠재 공간에서 변성 단계가 수행되는 스테이블 디퓨전(SD)(Rombach et al., 2022)에 구축되며, 다른 신경망으로 변성된 래치들을 디코딩하여 최종 생성된 이미지를 얻는다. SD의 U-Net 아키텍처는 시끄러운 래더와 텍스트 캡션(우리 실험에서 빈 스트링)을 모두 입력으로 하여 _모듈러 블록_ 스택을 통해 처리하는데, 여기서 각 블록은 잔차 블록(He et al, 2016), 자기 의도 계층 및 교차 의도 계층(Vaswani et al, 2017)으로 구성된다. 이들 블록은 4가지 공간 해상도 수준(명, 8\\), \\(16\\, \\ 16개), \\(32개 점수 32\\), \\(64개\\) 중 하나에 속하며, 특히 의미적으로 풍부한 표현(Luo et al., 2023; 파타시니크 등 2023)에 사용되는 상향 경로에 있는 해상도 \\(16\\, 16\\)에서 모듈식 블록이다. 구체적으로, 상향 경로에는 해상도 \\(16\\ 16\\)에서 3개의 연속적인 모듈화 블록이 있으며, 보이노브(2023)는 첫 번째 교차 의도 계층이 주로 콘텐츠 조작의 책임이 있는 반면 마지막 교차 의도 계층은 외모에 대한 더 많은 제어를 발휘한다는 것을 관찰했다. 후속 섹션에서는 이러한 의미적으로 의미 있는 층을 사용하여 미세 재배열된 분할 지도를 생성한다.\n' +
      '\n' +
      '저해상도 분할맵을 구성하세요.\n' +
      '\n' +
      '실제 이미지를 처리하기 위해 먼저 실제 이미지를 DDPM 기반 반전(Huberman-Spiegelglas et al., 2023)을 통해 더 큰 타임스팟이 노시어 이미지에 해당하는 특정 수의 데노징 단계(t=1\\cdots T\\)로 반전시켜 예정된 노이즈로 완벽한 재구성을 보장한다. 다음으로, 우리는 저차원 특징 맵이라고 할 타임스톤 \\(t_{f}\\)에서 상향(16\\ 16\\) 모듈러 블록의 첫 번째 교차 의도 계층에서 쿼리 벡터를 추출한다. 직관적으로, 질의 벡터는 텍스트 토큰과 직접 상호작용하도록 훈련되며, 따라서 그들의 표현은 의미 있게 인식되어야 한다. 마지막으로, 추출된 특징 맵에 k-means를 적용하여 _낮은 해상도 분할 마스크_ 역할을 하는 \\(K\\) 군집을 얻었다.\n' +
      '\n' +
      '이미지-해상도 분할 맵들.\n' +
      '\n' +
      '지금까지 우리는 원래의 이미지(예: \\(512\\·512\\)보다 해상도가 32배 낮은 저해상도 분할 맵(예: \\(16\\, 16\\)을 구성했다. 우리의 다음 목표는 이미지 픽셀과 저해상도 마스크 사이의 의미적 대응성을 식별함으로써 저해상도 분할 맵으로부터 이미지 해상도 분할 맵을 구축하는 것이다. 이를 위해 SD의 저차원 층이 생성된 이미지의 픽셀들에 어떻게 영향을 미치는지 관찰함으로써 시작된다. 구체적으로, 우리는 \\(16\\t 16\\) 교차 의도 계층에서 특징 맵의 하위 영역의 값을 _모듈링하고 이 지역 변화가 생성된 이미지의 픽셀 값에 어떻게 영향을 미치는지 관찰한다.\n' +
      '\n' +
      '공식 SD의 구현(\\sigma{QK^{T}}{\\srt{d}\\right)은 질의, 키 및 값 벡터(\\sets{QK^{T}\\cot V\\right)\\in\\mathb{R}^{R}^{R}\\\\t\\d\\i\\\\)에 입력들을 구성하며, 여기서 교차 요소 프로젝트들은 질의 벡터들(Q,K,V\\)\\in\\b}\\cartb}\\cartb}\\cartb}\\b}\\cartb}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\d\\in\\b}\\b}\\b}\\d\\in\\b}\\b}\\d\\i\\b}\\b}\\d\\i\\b}\\i\\i\\b}\\d\\i\\i\\i\\iI}\\d\\i\\i\\i\\i\\i\\i\\i\\i 우리는 횡단적 계층을 조절하기 위해 이 계산을 횡단적 계층으로 대체한다.\n' +
      '\n' +
      '그림 2: **_녹색_ 프레임워크를 살펴보고, 우리는 먼저 의미적으로 의미 있는 저차원 특징 맵에 k-means를 적용하여 저해상도 분할 지도를 구성한다. 오렌지_: Next은 변조된 데모화 공정에 의해 의미적 대응성이 식별되는 가장 의미적으로 대응하는 저해상도 마스크에 각 픽셀을 매핑하여 이미지 해상도 분할 맵을 생성한다.\n' +
      '\n' +
      '종종(\\frac{QK^{T}}{\\sqrt{d}}\\crt V\\right)+cM\\in\\mathbb{R}^{tot d}^{{R}\\) \\(c\\in\\mathb{R}\\)은 스칼라 값이고 \\(M\\in\\mathb{R}\\)는 공간 위치를 지정하는 이진 마스크이다. Verbally, 우리는 특징 맵의 하위 영역( \\(M\\)에 일정한 오프셋(c\\)을 균일하게 추가한다. i_변형된 데노징 프로세스_ 동안, 우리는 이 변조를 타임스메프 \\(t_{m}\\)에서 특정 교차 지향층에 적용한다.\n' +
      '\n' +
      '픽셀 값의 변화를 관찰하기 위해 먼저 두 개의 오프셋 \\(c=-\\lambda\\), \\(+\\lambda\\)로 조정된 데노징 프로세스를 별도로 실행하여 두 개의 변경된 이미지 \\(I^{-}, I^{-}\\in\\mathbb{R}^{H\\times W\\tcer 3}\\)를 얻는다. 그런 다음 RGB 차원 \\(d=||I^{-}-I^{+}||_{2}\\in\\mathbb{R}^{H\\tcer W}\\)보다 유클리드 거리를 취하여 차이 맵 \\(d\\)을 계산합니다. 그림 3과 같이 변조된 서브 영역과 의미적으로 관련된 픽셀들은 두드러지게 변화한 반면, 다른 픽셀들은 대략 동일하게 유지되었다. 따라서 얻어진 차이 맵은 이미지 픽셀과 저해상도 특징 맵의 서브 영역 사이의 의미 대응의 _ 강도_로 해석될 수 있다.\n' +
      '\n' +
      '이러한 관찰에 기초하여, 우리는 모든 쌍의 이미지 픽셀과 저해상도 분할 마스크 사이의 의미 대응의 강도를 계산할 수 있다. 세부적으로, 각 저해상도 마스크 \\(M^{i}\\{i}^{i}^{i}^{i}\\in\\mathbb{R}\\)에 대해 차이 맵 \\(d^{i}\\in\\mathbb{R}^{i}\\{R}^{i}^{bb{H}^{H}^{i}^{i}^{bb{R}^{i}\\ W}^{i}\\,^{i}\\)를 생성하며, 여기서 우리는 차이 맵 \\(d^{i}\\in\\mathbbb{R}\\i}\\in\\bbb{R}\\ W}^{bb{R}^{b{R}^{b{H}^{b{H}^{R}^{R}^{H}^{H}^{H}^{H}^{H}^{H}^{H\\ W}^{H}^{H}^{H\\ W}^{H\\ W}^{H\\ W}^{H 이미지 해상도 분할 지도를 구축하기 위해 각 픽셀 \\((x,y)을 낮은 고해상도 마스크 \\(k\\)로 레이블(즉, \\(k=\\operator이름*{argmax}_{i}_{x}_{x,y}\\)를 가장 강력한 의미 대응(k\\)으로 표지한다.\n' +
      '\n' +
      '추가 개선을 위해 변조된 변성 과정에서 모든 자기/교차 주의 계층의 주의도 \\(QK^{T}\\)를 고정한다. 주의 맵은 픽셀 친화도를 나타내며 객체 레이아웃에 강한 영향을 미치기 때문에, 주의 주입은 이미지 편집(Tumanyan et al, 2023)의 구조를 보존하기 위해 이미지 편집에서 일반적으로 사용되는 기술이다. 마지막으로, 차이 맵을 연산한 후, 우리는 픽셀화된 유물을 억제하기 위해 가우시안 필터링을 적용한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '본 절에서는 생산된 분할 마스크를 질적으로 양적으로 평가한다. 우리는 또한 부록 D에서 하이퍼파라미터 분석을 수행한다.\n' +
      '\n' +
      '### Implementation details\n' +
      '\n' +
      '실험 전반에 걸쳐 우리는 \\(50\\) 단계의 DDPM 샘플링 방식(명확한 목적을 위해 \\(T=1000\\)에서 타임스팟을 나타내는 공식 스테이블 디확산 v1.4 체크포인트를 사용한다. 저해상도 분할 맵을 생성하기 위해 타임스톤 \\(t_{f}=1\\)에서 특징 맵을 추출한다(최소 노이즈). 우리는 시간표 \\(t_{m}=281\\) 및 \\(\\lambda=10\\)에서 \\(16\\t 16\\)의 세 번째 교차 표시층에 변조를 적용한다. 앞서 살펴본 바와 같이, 이 계층은 외모를 제어하는 역할을 한다. 다양한 하이퍼모수들의 효과는 부록 D에서 논의된다.\n' +
      '\n' +
      '***런타임 분석*** 우리 방법의 가장 계산적으로 비싼 부분은 마스크마다 독립적으로 실행되는 변조된 데노징 프로세스입니다. 그러나 변조(즉, 변조 타임스톤 \\(t_{m}\\), 변조 타임스탬(i_{m}\\)을 적용하기 위해 타임스팟으로부터 변조된 데스토밍 프로세스를 실행해야만 한다.\n' +
      '\n' +
      '그림 3: ** 변조된 변성 과정의 시각화** 1열: 원본 이미지이다. 2열: 저해상도 변조 마스크 \\(M\\in\\{0,1\\}^{hw\\i w}\\) 세 번째 행은\\(H/h=W/w=32\\)에서 얻은 차이맵 \\(d\\in\\mathbb{R}^{H\\times W}\\)이다.\n' +
      '\n' +
      '우리의 하이퍼파라미터 설정에서 \\(15\\) 변성 단계를 필요로 한다. 나아가, 우리의 방법은 역전파를 포함하지 않으며, 전체 변성 공정은 잠재 공간(마지막 디코딩부를 제외)에서 작동된다. 따라서, 병렬화를 효과적으로 수행할 수 있다.\n' +
      '\n' +
      '### Qualitative analysis\n' +
      '\n' +
      '그림 1은 생성된 분할 맵의 예(부록 E에서 더 많은 결과)를 보여준다. 우리의 분할 맵은 잘 묘사되어 있으며 현장에서 적당히 작은 객체(예: 사람, 건물)를 성공적으로 구별한다. 관로의 효과를 추가로 입증하기 위해 세분화 맵과 순진한 업샘플링(비아 빌리네르 보간) 저해상도 분할 지도를 비교한다. 그림 4에서 시각화한 바와 같이 순삼화된 분할 맵은 조대하고 해석하기 어렵다. 대조적으로, 우리의 분할 맵은 동일한 저해상도 지도를 공유했음에도 불구하고 훨씬 더 명확하다. 마지막으로, 우리는 이미지당 생성된 마스크의 수를 달리합니다. 그림 5와 같이 마스크 수가 \\(2\\)만큼 적은 경우에도 의미적으로 해석할 수 있는 분할 지도를 얻는다.\n' +
      '\n' +
      '### Quantitative analysis\n' +
      '\n' +
      '분할 마스크를 정량적으로 평가하기 위해 비지도 시맨틱 분할과 주석 없는 오픈 어휘 분할의 두 가지 다운스트림 작업에 프레임워크를 적용한다. 이 평가에서 우리의 프레임워크는 이미지당 \\(30\\) 분할 마스크를 생성한다.\n' +
      '\n' +
      '** 비지도 시맨틱 세분화*** 표준 분할 데이터 세트에 대한 프레임워크를 평가한다. 전통적인 평가 프로토콜은 픽셀들이 데이터세트 클래스와 동일한 수의 의미개념으로 분류되어야 헝가리 매칭을 통해 후속적으로 데이터세트 클래스와 매칭될 수 있다. 사전 작업(함일턴 등 2022년)에서와 같이 픽셀/개념 임베딩을 생성하여 프레임워크를 확장하며, 여기서 각 픽셀은 가장 가까운 임베딩으로 개념으로 분류될 것이다. 픽셀 임베딩을 생성하기 위해 먼저 마스크를 만듭니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c} \\hline \\hline  & mIoU (\\(\\uparrow\\)) & \\multicolumn{2}{c}{mIoU (\\(\\uparrow\\))} \\\\ \\hline IIC (Ji et al., 2019) & 2.4 & TransFGU (Yin et al., 2022) & 16.2 \\\\ PiCIE (Cho et al., 2021) & 11.9 & SlotCon (Wen et al., 2022) & 18.3 \\\\ DINO ViT-B/8 (Caron et al., 2021) & 13.0 & STEGO (Hamilton et al., 2022) & **26.8** \\\\ SegDiscover (Huang et al., 2022) & 14.3 & DINOSAUR (Seitzer et al., 2022) & 24.0 \\\\ ACSeg (Li et al., 2023b) & 16.4 & **Ours** & 26.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ** 전통 평가 전략하에서 비지도 시맨틱 세분화 결과는 전체 COCO-Stuff-27(Caesar et al, 2018)에 대해 평가된***이다. ACSeg는 원래 종이에서 가져옵니다. IC, PiCIE 및 TransFGU는 Yin et al.(2022). 고에니그 등의DINO.(2023) 다른 결과는 세이처 등(2022)의 결과입니다. 일부 작품은 큐레이션 데이터세트(지 등 2019)에서 평가되며, 일반적으로 전체 데이터세트(Yin et al, 2022)에서 평가되는 것보다 더 높은 mIoU를 제공한다.\n' +
      '\n' +
      '그림 4: 순삼화된 저해상도 분할 지도와의 정성적 비교는 그림 4이다.\n' +
      '\n' +
      '그림 5: **Vary 세그먼테이션 마스크의 수를*** 우리 프레임워크는 의미적으로 의미 있는 방식으로 사물을 일관되게 그룹화한다.\n' +
      '\n' +
      'SD의 저차원 특징 맵(내부들은 부록 B에 있다)을 활용하여 각 분할 마스크의 조립을 수행한다. 그런 다음 각 픽셀은 해당 마스크 임베딩을 자체 픽셀 임베딩으로 채택한다. 개념 임베딩을 위해 전체 데이터셋에 걸쳐 픽셀 임베딩에서 k-means를 실행하고 원하는 수의 클러스터 센트로이드를 추출한다.\n' +
      '\n' +
      '그러나 표 1에서 입증된 바와 같이, 우리는 우리의 모델이 최근 DINO 기반 바젤린: STEGO(함밀턴 등 알, 2022) 및 DINOSAUR(선택자 등 2022)과 일치하는지 관찰한다. 이는 mIoU가 미리 정의된 데이터세트 클래스와 개념 임베딩이 얼마나 잘 일치하는지 민감하다는 전통적인 평가 프로토콜의 한계에서 기인하며, 이는 의미론적으로 그룹 객체를 고정된 수의 그룹으로 정렬하는 다양한 방법이 있을 때 문제가 된다(콘크리트 예들에 대한 부록에서 그림 9 참조).\n' +
      '\n' +
      '우리의 틀의 강도를 입증하기 위해 개념 임베딩을 구축하면서 주석에 대한 접근을 허용함으로써 전통적인 평가 프로토콜의 제한을 완화한다. 구체적으로, 우리는 동일한 근거 진실 라벨에 속하는 픽셀 임베딩의 평균을 취하고 얻어진 픽셀 임베딩을 설정한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\hline  & Backbone & AD150 & PC59 & PC459 & CS171 & CS27 & City19 \\\\ \\hline MDC & - & - & - & - & - & 8.6 & 14.0 \\\\ PiCIE & - & - & - & - & - & 11.7 & 15.1 \\\\ DINO & DINO ViT-B/8 & 19.1 & 30.0 & 10.1 & 19.0 & 32.2 & 34.6 \\\\ STEGO & DINO ViT-B/8 & - & - & - & 13.8 & 36.6 & 34.6 \\\\ CLIP & CLIP ViT-B/16 & 22.0 & 36.4 & 14.0 & 23.9 & 34.1 & 33.7 \\\\ TCL & CLIP ViT-B/16 & 20.8 & 35.5 & 11.3 & 23.5 & 31.9 & 32.6 \\\\ CLIPpy & T5 + DINO ViT-B/16 & 24.0 & 38.0 & 15.6 & 25.9 & 34.1 & 28.3 \\\\ \\hline SD & SD v1.4 & 29.1 & 41.5 & 20.6 & 27.6 & 42.1 & 32.3 \\\\ \\hline\n' +
      '**Ours** & SD v1.4 & **33.1** & **45.7** & **25.1** & **30.5** & **45.8** & **37.1** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2:**는 ADE20K(AD150)(Zhou et al., 2019), PASCAL-Context(PC59, PC459), Mottaghi et al., 2014), COCO-Stuff(CS171, CS27)(Caesar et al., 2018), 시티스코프(Cordts et al.Cordts et al., 2016)에 대해 평가 전략(Cordts et al.,Cordts)에 대해 평가하지 않은 의미 세분화(City19)에 대해 평가 전략에 대해 평가하지 않은 의미 세분화 결과를 평가했습니다. MDC(Cho et al, 2021), PiCIE(Cho et al, 2021), DINO 및 STEGO는 이미지에만 훈련되며 CLIP(라드포드 et al, 2021), TCL(Cha et al., 2023), CLIPpy(Ranasinghe et al., 2023)는 텍스트 이미지 쌍에 대해 훈련된다. CLIP의 경우, 우리는 이미지 인코더를 수정하여 픽셀별 임베딩을 출력하도록 저우 등(2022)을 따른다. SD의 경우, 우리는 순전히 샘플링된 저해상도 분할 맵(비아 빌리네르 보간, 그림 4)을 순전히 상향 조정하고 우리와 동일한 절차에 따라 픽셀 임베딩을 생성한다.\n' +
      '\n' +
      '그림 6: ** 수정 평가 전략에서 비지도 시맨틱 세분화의 시각화는 <그림 11>의 <부록>의** 모예이다.\n' +
      '\n' +
      '평균 임베딩은 개념 임베딩으로 임베딩됩니다. 그림 6에 예시된 바와 같이, 이 수정은 개념 임베딩이 각 데이터세트 클래스를 나타내고 mIoU 차이는 주로 픽셀 임베딩의 정확성과 불원성 능력에서 비롯됨을 보장한다. 우리의 수정된 평가 프로토콜은 픽셀별 임베딩을 출력하는 모델만 필요하기 때문에 STEGO(함밀턴 et al, 2022)와 자체 수정 이미지 인코더를 포함한 다른 기저부에 적용될 수 있다.\n' +
      '\n' +
      '표 2에 제시된 바와 같이, 수정된 평가 프로토콜은 이전 방법과 프레임워크 사이의 명확한 차이를 보여준다. 우리는 DINO 기반 모델(DINO 및 STEGO)이 일반적으로 데이터세트 내의 클래스 수가 많을 때 잘 수행되지 않는 반면, 확산 기반 모델(SD 및 우리, SD는 그림 4와 같이 순진한 업샘플링된 저해상도 마스크)은 이를 비교적 잘 처리한다는 것을 관찰했다. DINO는 원래 이미지넷(Russakovsky et al., 2015)과 같은 큐레이션된 객체 중심 데이터셋에 대해 훈련되는 반면 SD는 다양한 이미지에서 훈련되어 다양한 도메인에 더 강력하기 때문일 수 있다. 또한 STEGO가 DINO와 함께 수행한다는 것을 관찰하며, 이는 STEGO가 DINO와 유사하거나 심지어 열등한 선형 조사 성능을 갖는 후속 결과(Koenig et al., 2023)와 일치한다. 그럼에도 불구하고, 우리의 방법은 일관되게 모든 기저부를 능가한다. 또한 텍스트 정렬 이미지 인코더(CLIP, TCL, CLIPpy)와 비교를 수행하고 여전히 성능 격차를 관찰하여 수행 이익을 입증하면 사전 훈련 동안 텍스트 이미지 쌍의 가용성에만 기인한 것은 아니다. 마지막으로 동일한 마스크 임베딩을 공유하고 마스크 모양만 다른 프레임워크와 SD 사이의 명확한 개선을 봅니다. 이는 저희 파이프라인의 효과를 검증합니다.\n' +
      '\n' +
      '*** 무노테이션 오픈보크 시맨틱 세분화*** 본 연구에서는 두 접근 방식이 주석이 달린 데이터셋에 액세스하지 않고 이미지 텍스트 쌍에서만 훈련되는 기존 주석 없는 오픈보크 분할 모델과 프레임워크를 결합합니다. 그림 7에서 시각화된 바와 같이, 기존 기저부는 텍스트 정렬되지만 거친 픽셀 임베딩을 생성하는 반면, 우리의 프레임워크는 잘 묘사되었지만 계급 진단 마스크를 생성한다. 이것은 우리의 분할 마스크를 픽셀 임베딩과 결합하도록 동기를 부여하며, 이는 클래스 인식 미세 제조 분할 마스크를 생산하기 위한 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline  & \\multicolumn{4}{c}{mIoU (\\(\\uparrow\\))} \\\\  & AD150 & PC59 & PC459 & CS171 & City19 \\\\ \\hline OVSegmentor (Xu et al., 2023b) & 5.6 & 20.4 & - & - & - \\\\ GroupViT (Xu et al., 2022a) & 9.2 & 23.4 & - & 15.3 & 11.1 \\\\ \\hline \\hline MaskCLIP (Zhou et al., 2022) & 11.5 & 25.4 & 4.25 & 15.3 & 20.9 \\\\ MaskCLIP + **Ours** & **15.9** & **33.2** & **6.54** & **20.7** & **26.5** \\\\ \\hline \\hline TCL (Cha et al., 2023) & 14.6 & 30.9 & 5.22 & 18.7 & 20.9 \\\\ TCL + **Ours** & **17.4** & **35.4** & **6.49** & **21.8** & **23.4** \\\\ \\hline \\hline CLIPpy (Ranasinghe et al., 2023) & 12.6 & 28.0 & 4.64 & 16.6 & **9.76** \\\\ CLIPpy + **Ours** & **12.9** & **29.0** & **4.88** & **17.2** & 9.24 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: ** 주석 없는 오픈 어휘 의미 세분화에서 기저부와 기저부 + 우리 사이의 비교는 표 3: **이다. ADE20K(AD150)(AD150), PASCAL-콘텍스트(PC59, PC459)(모타그기 등 2014), COCO-Stuff(CS171)(Caesar et al., 2018), 시티스코프(Cordts et al.Cordts et al. 2016)에 대해 평가했다. 공정한 비교를 위해 동일한 신속한 엔지니어링으로 TCL, MaskCLIP 및 CLIPpy를 재평가한다. 다른 작품들의 결과들도 참고용으로 올려져 있는데, 여기서 OV세게이터가 원지에서 가져간다는 것(2023)***와 차 등의 그룹비T도 참조한다.\n' +
      '\n' +
      '그림 7: ** 무노테이션 오픈보크 시맨틱 분할입니다. 우리는 미세곡물 수업 진단 분할 마스크(녹색)와 기준선의 거친 텍스트 정렬 픽셀 임베딩(블루)을 결합하여 텍스트 정렬 미세화 분할 맵(오렌지)을 생성한다.\n' +
      '\n' +
      '세부적으로 우리는 이미지 및 텍스트 인코더가 포함된 공개적으로 이용 가능한 세 가지 주석 없는 기저부로 프레임워크를 통합한다: MaskCLIP(Z404 et al, 2022), TCL(Cha et al, 2023), CLIPpy(Ranasinghe et al., 2023). 마스크CLIP(휴리스틱 정제 없이)는 가장 표준 기준선으로, 사전 훈련된 CLIP 이미지 인코더(라드포드 등 2021)를 수정하여 추가적인 훈련 없이 픽셀별 임베딩을 출력한다. TCL 및 CLIPpy는 MaskCLIP와 유사하게 구조되지만 더 나은 픽셀 수준 표현을 생성하기 위해 훈련되었다. 우리의 프레임워크와 이러한 바젤을 결합하기 위해 먼저 기준선의 이미지 인코더로부터 생성된 픽셀 임베딩의 평균을 계산함으로써 각 마스크의 마스크 임베딩을 생성한다. 그런 다음 마스크 임베딩에 가장 가까운 텍스트 임베딩을 찾아 각 마스크를 분류합니다. 컨벤션 래드포드 등(2021)에 이어, 텍스트 임베딩을 생성하기 위해 프롬프트 템플릿(예:_{}__의 사진)이 사용된다. 부록 C에서 템플릿 목록을 제공합니다.\n' +
      '\n' +
      '표 3에 제시된 바와 같이, 우리는 대부분 우리의 틀과 결합된 후 성능 이득을 관찰하여 분할 마스크의 품질을 확보합니다. 특히, MaskCLIP의 성과 이득은 상당하며, 최근 기저부와 경쟁 성과를 보이고 있다(우리 방법과 결합되기 전에). 반면 CLIPpy의 성과 이득은 한계이다. 우리는 이것을 과부팅된 픽셀 임베딩(그림 7 참조)에 귀속시킨다.\n' +
      '\n' +
      '5건의 임문과 공동제척 및 남용.\n' +
      '\n' +
      '본 논문에서는 사전 훈련된 확산 모델에서 추출한 의미 지식만을 활용함으로써 미세 개질된 분할 맵을 생성할 수 있는 비지도 이미지 세그먼터를 개발하였다. 광범위한 실험은 효과를 검증했으며, 이는 확산 모델에서 매우 정확한 픽셀 수준 의미 지식의 존재를 시사한다.\n' +
      '\n' +
      '제한점으로서 우리의 프레임워크는 간혹 그림 8과 같이 극히 작은 물체(예: 동물 다리, 인간 얼굴)를 구별하기 위해 투쟁하는데, 이는 세부 부품이 저차원 층에서 함께 압축되기 때문이며, 저해상도 분할 지도를 생성할 때 우리의 프레임워크는 분리하지 못하기 때문일 수 있다. 추가적으로, 기본 특징 표현들은 객체 의미뿐만 아니라 공간 위치 및 색상 등과 같은 다른 속성들을 포함할 수 있으며, 이는 하늘 및 땅과 같은 일부 객체들을 과다 분할하도록 이끈다. 실용적인 사용을 위해 생산된 마스크를 의사 업무로 처리하고 약하게 구성된 프레임워크에 통합하는 것이 유망한 방향이 될 수 있습니다.\n' +
      '\n' +
      '마지막으로, 우리의 연구는 다양한 이미지를 생성할 수 있는 가장 일반적으로 연구된 확산 모델이기 때문에 Stable Diffusion에 구축된다. 그러나 의미 있는 특징 맵을 조절하는 우리의 근본적인 개념은 향후 작업을 위해 떠나는 다양한 생성 모델에 잠재적으로 적용될 수 있다. 우리의 연구 결과가 확산 모델의 내부 작업을 더 이해하고 판별 작업을 위해 생성 모델을 활용하는 연구 방향을 장려하는 데 도움이 되기를 바랍니다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '저자들은 NSERC와 Vector 연구소의 지원을 인정한다. SF는 캐나다 CIFAR AI 수리상을 인정하고 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* 압달 등은 (2021) 라메넨 압달, 페하오 주, 닐이 조미트라, 피터 원카 등이 있다. 라벨4가 없습니다: 스타일간을 사용한 비지도 분할입니다. i_에서 IEEE/CVF 국제 회의에 대한 IEEE/CVF 국제 회의는 컴퓨터에서 진행됩니다.\n' +
      '\n' +
      '그림 8: ** 실패 사례. 발효 마스크는 때때로 매우 작은 물체(예: 작은 책상, 동물 다리, 인간의 얼굴 부분)를 구별하지 못했다.\n' +
      '\n' +
      'Vision_, pp. 13970-13979 2021.\n' +
      '* 아라슬란노프 & 로트(2020) 니키타 아르슬란노프와 스티븐 로드가 있다. 이미지 라벨에서 단일 단계 시맨틱 분할입니다. 컴퓨터 비전 및 패턴 인식(CVPR)_, pp 4253-4262에 관한 _IEEE/CVF 콘퍼런스에서 2020년 6월 4253-4262.\n' +
      '* 발리지 등 알(2022) 요지 발지, 승준 나, 잔황, 아슈 바하트, 지밍 송, 카스텐 크리스, 미카 아무카, 티모 아필라, 사리 라인, 브라이언 카탄자로, 에티피: 전문가 데루아 앙상블이 있는 텍스트 대 이미지 확산 모델. arXiv 프리프린트 arXiv:2211.01324_, 2022.\n' +
      '* 바만묵 등은 (2022) 드미트리 바만륵, 안드리 보노브, 이반 루바초프, 발렌틴 케룰코프, 아르테엠 바벤코 등이 있다. 확산 모델을 사용한 라벨 효율적인 시맨틱 분할입니다. 학습 발표__국제회의에서는 2022년 URL[https://openopenreview.net/forum?id=SlxSY2UZQT](https://openopenreview.net/forum?id=SlxSY2UZQT)를 작성한다.\n' +
      '* 바론 & 포올(2016) 조나단 티 바르론과 벤 포올. 빠른 양측 솔버입니다. 컴퓨터 비전_에 관한 _유럽 회의에서는 2016년 pp. 617-632. 스프링거.\n' +
      '* 카사르 등 (2018) 홀거 카사르, 자스퍼 의자르, 비토리오 페라리 등이 있다. 코코스타프: 트싱과 아이템 클래스는 맥락입니다. i_컴퓨터 비전 및 패턴 인식(CVPR)에서 2018 IEEE 컨퍼런스_. IEEE, 2018.\n' +
      '* 카온 등은 알(2021) 마실데 카온, 허고 타우브론, 이산 미스라, 히베 제구, 진리엔 모랄, 피엇 보잔프스키, 아르만드 자울린 등이다. 자기 지도 비전 변압기의 출현 특성입니다. IEEE/CVF 국제 컨퍼런스의 _검토에서 2021년 컴퓨터 비전_, pp. 9650-9660.\n' +
      '*차 등은 (2023) 차준범, 종환문, 노병석. 이미지-텍스트 쌍에서만 오픈-월드 시맨틱 분할을 위한 텍스트-그라운드 마스크 생성을 학습하는 것이다. 컴퓨터 비전 및 패턴 인식_ pp. 11165-11174, 2023에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '* 조씨 등은 알(2021) 장현조, 유트카터몰, 카비타 발라, 바하트 하라란 등이 있다. 피시: 군집링에서 불변과 등변성을 사용한 무지도 시맨틱 세분화. 컴퓨터 비전 및 패턴 인식_, pp. 16794-16804에 대한 IEEE/CVF 회의의 _검토에서 2021년이다.\n' +
      '*콜린 등은 (2020) 에도콜린, 라자 발라, 밥 가격, 사빈 스스트렁크 등이다. 스타일에 맞는 것: 간들의 현지 의미들을 찾아보세요. 컴퓨터 비전 및 패턴 인식_, pp 5771-5780에 대한 IEEE/CVF 회의의 _검토에서 2020년이다.\n' +
      '*코드츠 등은 (2016) 마르니우스 코르트, 모하메드 오믈란, 세바스티안 라모스, 티모 레이펠트, 마르쿠스 엔즈웨일러, 로드리고 벤센슨, 우웨 프랑케, 스테판 로트, 베르렌 슈에레 등이 있다. 시멘틱 도시 장면 이해를 위한 데이터셋입니다. E_Proc에서. 컴퓨터 비전 및 패턴 인식(CVPR)_ 2016년 IEEE 회의의 경우.\n' +
      '* 다라리왈 & 니콜(2021) 프라풀라 다라리왈과 알렉산더 니콜이 있다. 확산 모델은 이미지 합성에서 게이스를 이겼다. __확산 모델이 이미지 합성에서 게이스를 꺾었다. 2021년 신경망 정보 처리 시스템_, 34:8780-8794의 발전이다.\n' +
      '*펑 등은 (2023) Qianli Feng, Raghudeep Gadde, Wentong Liao, 에듀바르 라몬, Aleix 마르티네즈이다. 네트워크가 없는 비지도 시맨틱 분할은 합성 이미지와 함께 제공됩니다. 컴퓨터 비전 및 패턴 인식_ pp. 23602-23610에 대한 IEEE/CVF 회의의 _검토에서 2023.\n' +
      '*가히시 등은 (2022) 골나즈 가히시, 시우예구, 진쿠이, 도성이린 등이다. 이미지 레벨 라벨로 오픈보크 이미지 분할을 계산합니다. 컴퓨터 비전_에 관한 _유럽 회의에서 pp. 540-557. 스프링거 2022.\n' +
      '* 해밀턴 등 (2022) 마크 해밀턴, 자퐁 장, 바하트 하라란, 노아 스틸리, 윌리엄 T. 레맨. 팽창되지 않은 의미 세분화는 특징을 분산시켜 대응한다. 학습 발표__국제회의에서는 2022년 URL[https://openopenreview.net/forum?id=SaKO6z6H10c](https://openopenreview.net/forum?id=SaKO6z6H10c)를 소개한다.\n' +
      '* He et al. (2016) 키밍 하이, 샹유 장, 샤오킹 르, 지안 선. 이미지 인식을 위한 심잔차학습. 컴퓨터 비전 및 패턴 인식_, pp. 770-778에 대한 IEEE 회의의 _발표 2016.\n' +
      '* 히들린 등 (2023) 에릭 하이들린, 고팔 샤마, 샤타 마하잔, 호삼 이락, 압스헤크 카, 안드레아 타글리아세치, 광무이 등이 있다. 안정적인 확산을 이용한 무지도 의미 대응이다. 2023.\n' +
      '\n' +
      '* 허츠 등은 (2022) 아미르 헤르츠, Ron 목다디, 제이 테네바움, Kfir Aherman, Yael Pritch, 다니엘 Cohen-Or. 교차 주의 제어와 함께 __ 절호의 촉진 대 촉진 이미지 편집. _-촉진 이미지 편집. arXiv 프리프린트 arXiv:2208.01626_, 2022.\n' +
      '*호 등은 (2020) 조나단호, 아약 자인, 피에테르 압벨 등이 있다. 덴노징 확산 확률적 모델 __데노징 확산 확률적 모델. _<덴도징 확산 확률적 모델. 신경 정보 처리 시스템_, 2020:6840-6851의 발전.\n' +
      '*황 등은 (2022) 하이양황, 지셸, 시니시아 루딘 등이 있다. 세그 디스커버: 비지도된 의미 세분화를 통한 비전개념 발견. __. arXiv 프리프린트 arXiv:2204.10926_, 2022.\n' +
      '* Huberman-Spieeglas 등 (2023) Inbar Huberman-Spieeglas, 블라디미르 킬리코프, Tomer Michaeli 등이 있다. 정교한 ddpm 소음 공간: 반전 및 조작. __ 우호적인 ddpm 소음 공간을 편집합니다. arXiv 프리프린트 arXiv:2304.06140_, 2023.\n' +
      '*지 등은 (2019) 주지, 조오 푸 헨리쿼스, 안드레아 베달디 등이 있다. 비지도 이미지 분류 및 분할을 위한 가변 정보 군집링이다. 컴퓨터 비전_, pp. 9865-9874에 대한 IEEE/CVF 국제 회의의 _검토에서 2019.\n' +
      '* 카르자자 등은 알(2023) 로리나스 카리아자, 이로 레나, 안드레아 베달디, 크리스티안 로브레흐트 등이다. 제로-샷 오픈-Vocabulary Se 세그먼트화를 위한 확산 모델. __제로-Shot Open-Vocabulary Se 세그먼트화를 위한 확산 모델. arXiv 프리프린트_, 2023.\n' +
      '* 카라스 등은 (2019) 테로 카라스, 삼리 루인, 티모 아라라 등이다. 생성적 적대 네트워크용 스타일 기반 발전기 아키텍처입니다. 컴퓨터 비전 및 패턴 인식_, pp. 4401-4410에 대한 IEEE/CVF 회의의 _검토에서 2019.\n' +
      '* 코에니그 등 (2023) 알렉산더 코에니히, 맥시밀리아 슈바흐, 요하네스 오터바흐 등이 있다. 안전한 비지도 의미 세분화를 위해 스티고의 내부 작업을 밝혀냅니다. 컴퓨터 비전 및 패턴 인식_ pp 3788-3797, 2023에 대한 IEEE/CVF 회의의 _검토에서.\n' +
      '* 크라헨부힐&콜튼(2011) 필리필 크라헨부블과 블라디미르콜튼이 있다. 가우시안 에지 전위를 갖는 완전히 연결된 crf와 가우시안 에지 전위로 완전히 연결된 crf에서 효율적인 추론. __. 신경 정보 처리 시스템_, 24, 2011의 발전이다.\n' +
      '* 리 등은 알(2022a) 보니 리, 킬리안 Q위버거, 세르게 벨롱기, 블라디미르콜튼, 로엔 러프트릴 등이 있다. 언어 중심 시맨틱 분할입니다. 학습 발표_국제회의에서는 2022a. URL[https://openreview.net/forum?id=RriDjddCLN](https://openopenreview.net/forum?id=RriDjddCLN)\n' +
      '* 리는 (2021) Daiqing Li, 준린 양, 카르스텐 크리스, 안토니오 토랄바, 산자 피들러 등이다. 생성 모델과의 의미 세분화: 반지도 학습과 강력한 영역 외 일반화이다. 컴퓨터 비전 및 패턴 인식_, pp 8300-8311에 대한 IEEE/CVF 회의의 _검토에서 2021년이다.\n' +
      '* 리 등은 알(2022b) 다이킹 리, 화난링, 승욱킴, 카스텐 크리스, 산자 피들러, 안토니오 토랄바 등이 있다. 빅다타세간: 픽셀별 주석으로 상상력을 합성합니다. 컴퓨터 비전 및 패턴 인식_, pp 21330-21340, 2022b에 대한 IEEE/CVF 회의의 _검토에서.\n' +
      '* 리는 (2023a) 두칭리, 화난링, 아칸 카, 데이비드 아쿠나, 승욱 김, 카스텐 크리스, 안토니오 토랄바, 산자 피들러 등이다. 드림터: 2023a 깊은 생성 모델이 있는 복원 이미지 백본입니다.\n' +
      '* 리는 (2023b) 케한 리, 제난 왕, 제센 청, 루니 유, 요안 자오, 구올리 송, 창 류, 리위안, 지셸 등이 있다. 학설: 비지도 시맨틱 세분화를 위한 적응 개념화. 컴퓨터 비전 및 패턴 인식_ pp 7162-7172, 2023b에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '* 리앙 등은 (2023) 펑 류, 비첸 우, 샤오리앙 다이, 쿤펑 리, 진안 자오, 항 장, 페자오 장, 피터 바자, 다나 마쿨라쿠 등이 있다. 마스크 적응 클립으로 오픈패스 시맨틱 세분화를 제공합니다. 컴퓨터 비전 및 패턴 인식_ pp 7061-7070에 대한 IEEE/CVF 회의의 _검토에서 2023.\n' +
      '* 루오 등은 (2023) 그레이스 루오, 리사 던랩, 동 홋파크, 알록산더 성실키, 트레보 다렐 등이 있다. 확산 하이퍼페인트: 의미 대응의 시간과 공간을 통한 검색: 의미 대응의 시간 및 공간을 검색한다. arXiv_, 2023.\n' +
      '* 마 등은 (2023) 차오판 마, 유호안 양, 첸 주, 페이 장, 진시강 류, 유왕, 야장, 옌펑 왕이다. 확산세그: 비지도 대상 발견에 대한 적응 확산. arXiv 프리프린트 arXiv:2303.09813_, 2023.\n' +
      '\n' +
      '* 멜라스-키리아지 등은 알(2022) 루크 멜라스-키리아지, 크리스티안 로브레흐트, 이로 레나, 안드레아 베달디 등이다. 심층 스펙트럼 방법: 비지도 시맨틱 세분화 및 국소화를 위한 놀랍게도 강한 기준선이다. 컴퓨터 비전 및 패턴 인식_, pp 8364-8375, 2022a에 대한 IEEE/CVF 회의의 _검토에서.\n' +
      '* 멜라스-키리아지 등은 알(2022b) 루크 멜라스-키리아지, 크리스티안 로브레흐트, 이로 레나, 안드레아 베달디 등이다. 깊이 생성 모델 각각에서 부적합한 이미지 세그먼터를 찾으세요. "국제 학습 발표회의"에서 2022b. URL[https://openreview.net/forum?id=Ug-bgjgSIKV](https://openopenreview.net/forum?id=Ug-bgjgSIKV)\n' +
      '* 모타그기 등은 (2014) 로얄히 모타게이, 시안지 첸, 샤오바이 류, 남규 조, 성휘 리, 산자 피들러, 라젤 우르타순, 알란 유유유이다. 야생에서 객체 검출 및 의미 세분화를 위한 맥락의 역할. 컴퓨터 비전 및 패턴 인식(CVPR)_ 2014년 _IEEE 콘퍼런스에서.\n' +
      '* 묵호키 등은 (2023) 지슈누 묵호키, 토성유린, 오미 펠레에드, 루이 왕, 아쉬쉬 샤, 필립 HS 토르, 세르남 임 등이 있다. 패치 정렬된 대비 학습으로 열린 어휘 시맨틱 분할입니다. 컴퓨터 비전 및 패턴 인식_, pp. 19413-19423, 2023에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '* 구필드 등은 (2023) 제임스 옛필드, 크리스토스 제글피스, 야니스 파나가키스, 미할리스 니콜라, 이오만니스 파라스 등이 있다. 판다: GAN의 특징 맵에서 부품과 외모에 대한 지도 학습. Eleventh 국제 학습 발표_, 2023년 URL[https://openopenreview.net/forum? aiUdSB2kK9GY] (https://openopenreview.net/forum?iUdSB2kK9GY)에서.\n' +
      '* 파타슈니크 등 (2023) 오리 파타시니크, 다니엘 가미비, 이단 아지리, 하다르 애버부치 에일러, 다니엘 코헨 등이 있다. 텍스트 대 이미지 확산 모델과 함께 객체 수준 형상 변화를 국소화한다. IEEE/CVF 국제 회의의 _검토에서 컴퓨터 비전(ICCV)_, 2023.\n' +
      '* Radford et al.(2021) 알레크 라드포드, 종욱 김, 크리스 홀리스, 아디아 레즈, 가브리엘 고, 샌히니 아가왈, 기리시 사스트리, 아미다 아셀, 파멜라 미슈킨, 잭 클라크 등 자연 언어 감독으로부터 시각적 모델을 전수할 수 있다. 머신러닝_, pp. 8748-8763에 관한 _국제회의에서 2021년 PMLR.\n' +
      '* 라누싱허 등은 (2023) 칸찬아 라네싱허, 브랜든 맥킨지, 사치인 라비, 유페이 양, 알렉산더 토스허프, 조온톤 슈렌스 등이다. 비교 비전-언어 모델에서 인식 그룹화를 수행한다. ICCV, 2023.\n' +
      '* 람바흐 등(2022) 로빈 라이바흐, 안드레아스 블라트만, 도미니크 로렌츠, 패트릭 에저, 비콘 오머 등이 있다. 잠재 확산 모델을 사용한 고해상도 이미지 합성입니다. 컴퓨터 비전 및 패턴 인식_ pp 10684-10695에 대한 IEEE/CVF 회의의 _발표에서 2022년 pp. 10684-10695.\n' +
      '* Ronneberger et al. (2015) Olaf Ronneberger, 필리필 피셔 및 토머스 Brox. U-net: 생물의학적 이미지 분할을 위한 콘볼루션 네트워크. E_ 의료 이미지 컴퓨팅 및 컴퓨터 보조 개입-MICCAI 2015: 제18차 국제 회의, 뮌헨, 독일, 2015년 10월 5-9일, 합의, 부분 III 18_ pp. 234-241. 스프링거.\n' +
      '* 러사코프스키 등은 (2015) 올가 루사코프스키, 지아 덩, 하오수, 조나탄 크라우즈, 산지프 토헤시, 세안 마, 지청 황, 안드레이자 카공감, 아디샤 카슬라, 미하엘 베르나슈타인, 에다. 국제 컴퓨터 비전 저널_, 115:211-252 2015.\n' +
      '* 사하라리아 등은 윌리엄 샤아리아, 윌리엄 찬, 소라바 시세나, 라라 리, 자 휘앙, 에밀릴 리, 카마르 게르미포, 라파엘 게티조 로프, 버쿠 카가골 아얀, 팀 살림산 등 언어 이해가 깊은 포토어리스틱 텍스트 대 이미지 확산 모델. 신경정보처리시스템_, 2022년 35:36479-36494의 효과.\n' +
      '* 세이처 등(2022) 맥시밀리아 세리처 알(2022) 맥시밀리안 세이처, 맥시 호르, 안드리 자다안추크, 도미니크 지엣우, 톈준 샤오, 카를-조한 시몬-가브리엘, 통허, 정장, 베나하 크스크프, 토마스 브릭스 등 실세계 대상 중심 학습과의 격차를 쏟아냈다. arXiv 프리프린트 arXiv:2209.14860_, 2022.\n' +
      '*신 등은 (2022)경인신, 사마엘 알바니, 위디 시이다. 스펙트럼 클러스터 투표로 초임되지 않은 두드러진 물체 검출이다. 컴퓨터 비전 및 패턴 인식_ pp. 3971-3980에 대한 IEEE/CVF 회의의 _검토에서 2022년이다.\n' +
      '\n' +
      '* Simeoni et al.(2023) 오리에네 시메로니, Chloe Sekkat, Gilles Puy, 안토닌 보베키, Eloi Zablocki, Patrick Perez. 지도되지 않은 객체 로컬화: 객체를 발견하기 위해 배경을 관찰한다. 컴퓨터 비전 및 패턴 인식_, pp 3176-3186, 2023에 대한 IEEE/CVF 회의의 _검토에서.\n' +
      '* 탕 등 (2023a) 탕, 멍린 자아, Qianqian 왕, 청퍼두, 바하트 하라란. 이미지 확산의 출현 대응. __ 영상 확산의 출현 대응. arXiv 프리프린트 arXiv:2306.03881_, 2023a.\n' +
      '* 탕 등은 (2023b) 라파엘 당, 린킹 류, 아크하트 판디이, 제히잉 장, 제페이 양, 카룬 쿠마르, 폰투스 스테네토르프, 짐미린, 페란 투어 등이다. DAAM: 교차 주의를 사용하여 안정적인 확산을 완화합니다. 2012년 7월 캐나다 토론토 토론토(토론토 5644-5659년 7월) 컴퓨터 통계 협회의 제61차 연차 회의(1: 롱 파이어스)_, pp. 5644-5659)가 개최되었다. 컴퓨팅 로직에 대한 연관입니다. 10.18653/v1/2023. URL[https://aclanthology.org/2023.acl-long.310](https://aclanthology.org/2023.acl- petri.310]).\n' +
      '* 타이트로프 등은 (2021) 노타와트 트릿롭, 피치콘 루이스보르콘, 수아스콘 수왓자나콘 등이다. 원샷 시맨틱 부품 분할을 위해 가즈를 교체하세요. 컴퓨터 비전 및 패턴 인식_ pp. 4475-4485에 대한 IEEE/CVF 회의의 _발표에서 2021년 pp. 4475-4485.\n' +
      '* 투만안 등은 (2023) 나레크 투마니안, 미칼 거가, 샤이 바곤, 탈리 드켈 등이 있다. 텍스트 구동 이미지 대 이미지 번역을 위한 플래그 앤 플레이 확산 기능이 있습니다. 컴퓨터 비전 및 패턴 인식(CVPR)_, pp 1921-1930, 2023년 6월 IEEE/CVF 회의의 _검토에서.\n' +
      '* 반 간시베크 등은 (2021) 위터 반 간사케, 시몬 밴데논, 스타마티오스 게오울리스, 루카 반 고올 등이 있다. 물체 마스크 제안과 대조하여 무지도 시맨틱 분할입니다. 2021년 컴퓨터 비전_, pp. 10052-10062에 대한 IEEE/CVF 국제 회의의 _검토에서.\n' +
      '* 바스완이 등은 (2017) 아샤시 바소와이, 노암 샤제리, 니키 파마르, 작노 우즈코레이트, 리온 존스, 디단 노 고메즈, 루카즈 카이저, 일리아 폴로숙신 등이 있다. 필요한 것이 전부입니다. __ 주의가 필요합니다. __ 주목된다. 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* 보노브 등 (2020) 안드리 보노프, 스탠리슬라브 모로조프, 아르템 바벤코 등이 있다. 대규모 생성 모델이 있는 라벨이 없는 객체 분할입니다. 기계 학습_, 2020년 국제 회의에서 URL[https://apisemanticscholar.org/CorpusID:235417600](https://apisemanticscholar.semanticscholar.org/CorpusID:235417600).\n' +
      '* 보이노브(2023) 안드리 보노프, 칭하오 추, 다니엘 코헨오, 키르 아헤만 등이 있다. P+: 텍스트 대 이미지 생성에서 확장 텍스트 컨디셔닝입니다. 2023.\n' +
      '* 왕 등은 (2023) 자동왕, 노릿 버다하, 스텔라 X유, 이산 미스라가 있다. 비지도 대상 검출 및 인스턴스 분할을 위해 자르고 학습한다. 컴퓨터 비전 및 패턴 인식_, pp 3124-3134, 2023에 대한 IEEE/CVF 회의의 _검토에서.\n' +
      '* 왕 등 (2022) Y. 왕, X. S, S. 후, Y. 위안, J. L. 크로우리, D. 바우프리다즈. 정규화된 컷을 사용하여 비지도 대상 발견을 위한 자가 지도 변압기를 사용한다. 컴퓨터 비전 및 패턴 인식(CVPR)_, pp 14523-14533, 로스앤젤레스 알라미토스, 미국 CA, 11월 IEEE 컴퓨터 학회에 대한 _2022 IEEE/CVF 콘퍼런스에서. 10.1109/CVPR52688: 10.1109/CVPR52688. URL[국경://doi.1109/CVPR526882022.01414](https://doi.ieeecomputers cont.org/10.1109/CVPR52688.2022.014]) (https://doi.ieeecomputersociety.g/10.1109/CVPR52688/CVPR52688)\n' +
      '* Wen et al.(2022) 신장 원, 빙첸 자오, 안린 정, 샹유 장, XIAOJUAN QI. 시맨틱 그룹화를 사용한 자기 지도 시각 표현 학습이 있다. 알리스 H. 오, 알레크 아가왈, 다니엘 벨그레브, 교윤 조(종)에서 신경정보처리시스템_, 2022년 URL의 _Advances.net/forum? aH3JObxJdBS] (https://openopenreview.net/forum?=H3JObxJdBS)의 _Advances.\n' +
      '* 우 등은 (2023) 위자아 우, 유중 자오, 마이크 정슈, 홍주, 춘화 선이 있다. 디플룸 마스크: 2023년 확산 모델을 사용하여 의미 세분화를 위한 픽셀 수준 주석으로 이미지를 합성한다.\n' +
      '*Xu & Zian진Xu, 창시정입니다. 생성적 적대 네트워크에서 선형 정수는 있다. 컴퓨터 비전 및 패턴 인식_, pp. 9351-9360에 대한 IEEE/CVF 회의의 _검토에서 2021년이다.\n' +
      '*Xu 등 (2022a) 지아루, 샤리니 데 모로, 사피 류, 원민 변, 토마스 브레엘, 얀 커츠, 샤오룽 왕이다. 그룹빗: 독보적인 세분화는 텍스트 감독으로부터 나타난다. 컴퓨터 비전 및 패턴 인식_ pp 18134-18144, 2022a에 대한 IEEE/CVF 회의의 _검토에서.\n' +
      '\n' +
      '*Xu 등은 (2023a) 지아루, 사미 류, 아라시 바하다트, 원민 변, 샤오룽 왕, 샤리니 데 메로 등이 있다. 텍스트 대 이미지 확산 모델을 사용한 오픈-보크 범프틱 분할이다. 컴퓨터 비전 및 패턴 인식_ pp. 2955-2966, 2023a에 대한 IEEE/CVF 회의의 _ 개시.\n' +
      '*Xu 등은 (2023b) 진안 주, 준린 호우, 유지 장, 로이펑, 이왕, 유샤오, 위디 제이 등이 있다. 자연어 감독으로부터 개방형 시맨틱 세분화 모델을 학습한다. 컴퓨터 비전 및 패턴 인식_ pp. 2935-2944, 2023b에 대한 IEEE/CVF 회의의 _검토에서.\n' +
      '*Xu et al. (2022b) 멍데 Xu, 정장, 장윤웨이, 유롱린, 유에코오, 한후, 샤앙바이. 사전 훈련된 비전-언어 모델로 개방형 시맨틱 분할을 위한 간단한 기준선이 있다. 컴퓨터 비전_에 관한 _유럽 회의에서 pp 736-753.스프링거 2022b.\n' +
      '*Xu 등 (2023c) 멍데 주, 정장, 장윤위, 한후, 샹바이. 개방형 시맨틱 세분화를 위한 사이드 어댑터 네트워크입니다. 컴퓨터 비전 및 패턴 인식_ pp. 2945-2954, 2023c에 대한 IEEE/CVF 회의의 _검토에서.\n' +
      '*유인(2022) 자오안 진, 피차오 왕, 판왕, 시안허 주, 한링장, 하오리, 루진 등이 있다. 트랜스포푸: 미세 편성된 비지도 의미 세분화에 대한 하향식 접근이다. 컴퓨터 비전_, pp. 73-89. 스프링거 2022년 _유럽 콘퍼런스에서.\n' +
      '* 자다니아추크 등은 (2023) 안드리 자다안추크, 마트리우스 클린다네너, 이주, 프란체스코 로카텔로, 토머스 브록스 등이다. 자기 지도 대상 중심적 표현으로 무지도 시맨틱 분할을 한다. Eleventh 국제 학습 발표_, 2023년 URL[https://openreview.net/forum?id=1_jFneF07YC] (https://openopenreview.net/forum? a=1_jFneF07YC)에서 학습 발표_, 2023. URL[https://openreview.net/forum?\n' +
      '*장(2023) 주이 장, 찰스 헤르만, 준화 후르, 루이스아 폴라니아 카브레라, 바룬 진파니, 데킹 선, 명함안 양. 스테인리스 확산이라는 두 가지 특징에 대한 이야기는 제로샷 의미 대응에 대해 디노를 보완한다. 2023.\n' +
      '*장 등은 (2021) 유수안 장, 화안 로잉, 준가오, 강수유인, 장-프란코리스 라플체, 아델라 바리소, 안토니오 토랄바, 산자 피들러 등이다. 다타세간: 최소한의 인간 노력으로 효율적인 라벨링된 데이터 공장을 사용합니다. 컴퓨터 비전 및 패턴 인식_ pp. 10145-10155, 2021년 IEEE/CVF 회의의 _검토에서.\n' +
      '* 저우 등은 (2019)볼리 주, 항 자오, 자비에 푸이그, 테테 샤오, 산자 피들러, 아델라 바르리아소, 안토니오 토랄바 등을 들 수 있다. ae20k 데이터셋을 통해 장면들에 대한 의미 있는 이해는 __ ade20k 데이터셋을 통해 장면들에 대한 의미 있는 이해이다. 컴퓨터 비전_, 127:302-321, 2019 국제 저널.\n' +
      '* 저우 등 (2022) 총주, 첸 체인지 로이, 보다이. 클립에서 자유 조밀한 라벨을 추출합니다. 컴퓨터 비전_에 관한 _유럽 회의에서 pp 696-712.스프링거 2022.\n' +
      '\n' +
      '조건부\n' +
      '\n' +
      '실제 이미지를 SD로 처리하기 위해서는 이미지의 각 차원이 64의 배수로 요구되며, 더 나아가 SD로 처리하기에는 너무 크거나 너무 작지 않도록 이미지의 크기를 조정해야 한다. 이러한 요구 사항을 충족하기 위해 우리는 원래 높이 대 폭 비율을 유지하면서 픽셀 카운트를 대략 \\(512^{2}\\) 유지하기 위해 이미지를 재구성한다. 그런 다음 각 차원을 \\(64\\)에 의해 분할되는 가장 가까운 정수까지 라운드팅한다. mIoU를 컴퓨팅할 때, 획득된 분할 맵들은 가장 가까운 이웃을 통해 원래 크기로 다시 재구성된다.\n' +
      '\n' +
      '실수된 사람\n' +
      '\n' +
      '** 구현 세부 사항** 전통적인 평가 프로토콜(지 등 2019)은 모델은 픽셀들을 데이터셋 클래스와 동일한 수의 의미개념으로 분류하여 헝가리의 매칭을 통해 사전 정의된 데이터세트 클래스와 후속적으로 의미개념을 매칭할 수 있도록 하여야 한다(여기에서 mIoU 극대화). 이 요건을 충족하기 위해, 이전 작품은 픽셀 임베딩과 컨셉 임베딩을 모두 학습하여 각 픽셀을 픽셀 임베딩으로부터 가장 가까운 개념 임베딩을 갖는 개념으로 분류할 수 있도록 한다. 특히, 현재 최첨단 모델인 STEGO(함밀턴 et al, 2022)는 먼저 픽셀 임베딩을 생성하는 이미지 인코더를 훈련시킨 다음 전체 데이터세트 상에서 픽셀 임베딩을 군집화하여 개념 임베딩 역할을 하는 클러스터 센트로이드를 획득한다.\n' +
      '\n' +
      '이 작업에 프레임워크를 적응시키기 위해 STEGO의 절차를 따르며 픽셀/개념 임베딩을 생성하여 프레임워크를 확장합니다. 픽셀 임베딩을 생성하기 위해 SD의 특징 맵을 사용하여 생성된 각 마스크에 대한 마스크 임베딩을 생성한다. 차면 영역(F\\in\\bb{R,1\\{R,^{h\\i}\\i}\\) 내부에 있는 마스크(F\\in\\b{0,1\\{H\\i}\\i}\\)는 마스크 \\(M\\in{0,1\\{h\\ w}\\)의 평균 값을 취함으로써 마스크(M\\in\\{0,1\\{h\\i}^{h\\i}\\)의 특징 맵(M\\in\\{0,1\\{H}^{h\\i}^{h\\i}^{h\\i}^{h\\i}^{h\\i}^{h\\i}^{h\\i}^{h\\i}^{h\\)의 특징 맵(M\\{h\\i}^{h\\i}^{h\\i}^{h\\i}^{h\\i}\\i}^{h\\i}\\i}\\i}\\i}\\i}\\i}\\i}\\i}\\i}\\i}\\)을 생성했으며,\\)은 마스크 \\)의 이러한 디자인은 분류 목적으로 SD의 특징 지도를 사용하여 마스크 임베딩을 생성하는 이전 작업(카라지자 등, 2023)에서 크게 차용된다. 마스크 임베딩이 생성되면, 각각의 픽셀 임베딩은 그것이 속한 마스크의 임베딩으로 정의된다. 전통적인 평가 프로토콜 하에서 개념 임베딩을 생성하기 위해 전체 데이터 세트를 가로질러 픽셀 임베딩에서 k-means를 실행하고 군집 중심을 추출한다.\n' +
      '\n' +
      '이 평가 프로토콜에서 높은 mIoU를 얻기 위해 전통적인 평가 프로토콜의****는 미리 정의된 데이터세트 클래스와 잘 일치하는 개념 임베딩을 생성하는 것이 매우 중요해진다. 우리는 구축 개념 임베딩을 위한 근거 진실 라벨에 접근할 수 없기 때문에 이 요구 사항은 부당하게 엄격해지고 특히 의미적으로 분할할 수 있는 여러 가지 유효한 방법이 있을 때 신뢰할 수 없게 된다. 예를 들어, COCO-Stuff 27(Caesar et al, 2018)은 비지도 시맨틱 세분화에 사용되는 인기 데이터세트로서 _person_를 지칭하는 하나의 클래스로만 구성되지만, 그림 9에서 시각화한 바와 같이 우리의 프레임워크는 머리, 신체, 팔의 사람에게는 적어도 세 가지 의미 개념을 생성한다. 한 가지 개념만 사람 수업과 매칭될 수 있기 때문에 다른 부분은 무관한 라벨로 불일치할 수밖에 없어 mIoU가 감소한다.\n' +
      '\n' +
      '전통적인 평가 프로토콜을 수정하는*****는 데이터세트 클래스를 추정하는 것이 아니라 생산된 분할 마스크의 품질을 적절하게 평가하는 데 중점을 두고 있기 때문에 각 임베딩이 각 데이터세트 클래스에 대응하도록 개념 임베딩을 사전 계산합니다. 이를 달성하기 위해 개념 임베딩을 구성하면서 근거 진실 주석의 접근을 허용함으로써 전통적인 평가 프로토콜의 제한을 완화한다. 구체적으로 각 데이터세트 클래스에 대해 모든 것을 통합합니다.\n' +
      '\n' +
      '그림 10: ** 수정되지 않은 의미 세분화 평가 프로토콜.**는 COCO-Stuff-27에 평가되었으며 그림 9와 대조적으로, 우리의 프레임워크는 데이터세트(예: 한 그룹에 대한 그룹)에서 사전 정의된 클래스에 따른 물체를 분할하여 세게이션 마스크의 품질을 보다 적절하게 평가했다.\n' +
      '\n' +
      '그림 9: **. COCO-Stuff-27에 평가되지 않은 의미 세분화의 전통적인 평가 프로토콜.**. 이 프레임워크는 사람을 머리, 팔, 몸으로 분할하지만 데이터세트에는 _person_에 대해 미리 정의된 클래스가 하나만 있어 다른 부분이 관련 없는 클래스와 일치하지 않을 수 없게 한다.\n' +
      '\n' +
      '전체 데이터세트로부터 클래스에 속하는 픽셀 임베딩들을 계산하고 그 평균을 계산한다. 얻어진 평균 임베딩은 이 클래스의 개념 임베딩 역할을 한다. 그림 10에서 시각화된 바와 같이, 이 수정된 평가 프로토콜은 데이터세트 내의 사전 정의된 클래스 카테고리에 따라 객체를 분할할 수 있게 하고, 성능 차이는 주로 개념 임베딩이 아닌 기본 픽셀 임베딩의 품질에서 비롯된다. 추가적인 질적 결과는 그림 11에서 확인할 수 있다.\n' +
      '\n' +
      '메이크랙 시맨틱 세분화 세부사항## 부록 C\n' +
      '\n' +
      '** 구현 세부 정보**. 차 등에서 논의된 바와 같이(2023년) 개방형 시맨틱 세분화에 대한 통일된 평가 프로토콜이 존재하지 않으며, 다양한 연구가 종종 다양한 프롬프트 엔지니어링 전략을 사용한다. 공정한 비교를 위해 동일한 그룹 라벨과 신속한 템플릿으로 바젤을 재평가했다. "특히 묵샷_{}"에 이어 (7\\) 프롬프트 템플릿으로 대체되며, 여기서 우리는 \\(7\\) 프롬프트 템플릿, 즉 __\' 종이접기_ {} __ {}의 나쁜 사진, __{}, __{}.\n' +
      '\n' +
      '그림 11: COCO171 및 ADE150K에 대한 비지도 시맨틱 분할의 실시예들은 수정된 평가 프로토콜에서 평가되었다.\n' +
      '\n' +
      '야. 이 평가의 목적은 개방론적 의미 세분화의 최첨단 결과를 달성하는 것이 아니라 기저부에 의해 생성된 시끄러운 특징 지도와 결합하여 세분화 지도의 전치성을 정량적으로 평가하는 것이다.\n' +
      '\n' +
      '부록 D\n' +
      '\n' +
      '본 절에서는 우리 틀에서 하이퍼파라미터 선택이 세분화 품질에 어떠한 영향을 미치는지 분석한다. 이 분석에서 비지도 시맨틱 분할은 수정된 평가 프로토콜에서 평가된다.\n' +
      '\n' +
      '이미지당 마스크****. 우리는 먼저 우리의 정량적 결과가 합리적인 범위 내에서 이미지당 생성된 마스크 수(여기에서 \\(10\\)에서 \\(40\\)에 민감하지 않도록 한다. 표 4에 제시된 바와 같이, 우리는 비지도 시맨틱 세분화와 오픈보크 시맨틱 분할 모두에 대한 mIoU의 한계 차이만을 관찰하여 실험 설정의 공정성과 견고성을 확인했다.\n' +
      '\n' +
      '특성 지도를 추출하기 위한 교차 의도 계층의*** 선택 3.2절에서는 교차 의도 계층 중 하나의 특징 맵을 추출하여 저해상도 분할 지도를 구축한다. 우리의 선택은 이전의 관찰(Luo et al., 2023; 카르자자 등, 2023)에 기초하며, 이는 \\(16\\ 16\\) 블록의 층이 가장 의미 있게 의미가 있다. 상향 경로에 \\(16\\t 16\\) 해상도에 3개의 교차 의도층이 있다는 점에 유의한다. 그러나 표 5에 보고된 바와 같이, 우리는 \\(16\\시 16\\) 교차 주의층이 선택되었는지에 관계없이 유사한 성능을 관찰했다.\n' +
      '\n' +
      '특징지도를 추출하기 위한**Timesteps*** 피처맵 추출 시간표를 달리하여 실험하였다. 추출된 특징 맵들이 저해상도 분할 맵들을 생성하기 위해 사용된다는 것을 상기시키며, 특징 맵들은 이미지들에 제시된 모든 내용을 명확하게 캡처하기를 원한다. 직관적으로 큰 소음(즉, 큰 타임스팟)을 추가하여 객체 레이아웃을 모호하게 추가하고 피해야 한다. 실제로 그림 12에서 시각화된 바와 같이 타임스팟이 증가할 때, 우리의 프레임워크는 이미지 내의 이웃 객체를 구별하기 위해 투쟁한다. 표 6은 또한 더 큰 소음을 추가할 때 성능 저하를 강조한다.\n' +
      '\n' +
      '** 수정 시간표 및 강도** 생성된 이미지에 대한 텍스트 포획의 영향은 데노징 과정(발지 등, 2022, 허츠 등, 2022, 파슈니크 등) 전반에 걸쳐 달라지는 것으로 알려져 있다. 우리의 조절이 U-Net의 특징 맵과 상호 작용하는 교차 의도 계층의 특징 맵을 변경한다는 점을 감안할 때, 우리의 변조의 영향도 타임스팟 전체에 걸쳐 다를 것으로 예상된다. 그림 13에서 시각화한 바와 같이 타임스팟이 너무 작으면 우리의 변조는 해당 시맨틱 객체의 픽셀 값에 효과적으로 영향을 미치지 않고 거친 분할 마스크를 생성했다. 반대로, 타임스팟이 너무 클 때는 일부 세구메가.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c|c c c}  & \\multicolumn{6}{c|}{mIoU (\\(\\uparrow\\))} \\\\  & \\multicolumn{6}{c|}{Open-vocabulary seg.} & \\multicolumn{3}{c}{Unsupervised seg.} \\\\ \\hline  & \\multicolumn{3}{c|}{MaskCLIP + Ours} & \\multicolumn{3}{c|}{TCL + Ours} & \\multicolumn{3}{c}{Ours} \\\\ \\# masks & AD150 & CS171 & City19 & AD150 & CS171 & City19 & AD150 & CS171 & City19 \\\\ \\hline\n' +
      '&30.0&30.0 & 34.0 \\\\ & 34.0 \\\\ & 17.2 & 15.7 & **21.5** & 24.2 & **21.5** & 24.2 & **22.5** & 24.2 및 **22.5** & 24.2.5** 및 24.3*22.5** 및 24.2 및 24.2.5*2.5*2.5*2.5*2.5*2.5*2.5*21.5*2.5*2.5*2.5*2.5*2.5*22.5*22.5*.5* 및 24.2 및 24.5*22.3*22.5* 및 24.2 및 24.5*22.5* 및 24.3*22.5* 및 24.2 및 24.3*22.5* 및 24.3*22.5* 및 24.2 및 24.3*22.5* 및 24.2 및 24.3*22.5* 및 24.3*.3\n' +
      '4&30.4&36.3\\\\ & 32.3 & 32.3 & 32.9 & 36.3－20 & **16.0** & 21.9 & **16.0** & 21.9**17.0** & 21.0** & 21.9**17.0** & 21.6** & 22.1 & 21.9*17.0** 및 21.9*17.0**.0*.0*.0*.0*.0*.0*-21.0*.0*.0*.0*.0*.0*.0*.0*.0*.0*.0*.0*.0* 및 21.0*.0*.0*.0*.0* 및 21.0*.0*.0* 및 21.0*.0*.0*.0* 및 21.0*.0*.0*.0* 및 21.0*.0*.0*.0*.0* 및 21.0*.0*.0* 및 21.0*\n' +
      '4&33.1&33.1 & **30.5* & 37.1, **30.5* & 15.9&20*26.5** & 15.9 & **26.5** & 17.7 & **26.5** & 17.4 & 21.1 및 **30.5** & 17.9 및 **26.5** & 17.9*.5** 및 15.9* 및 15.9*.5** 및 15.9*.5** 및 17.4 & 15.9*.5** 및 17.4 & 15.9*.5** 및 17.4 & 15.9*.5** 및 17.4 & 15.5**17.5*/17.5** 및 17.4 & 15.5** 및 17.4 & 15.9*-17.5** 및 17.4 & 15.5**17.5** 및 17.4 & 17.4 & 15.9*.5** 및 17.4 & 17.4 & 15.5** 및 17.\n' +
      '40 & 15.6 & 20.4 & 26.4 & 17.3 & 21.6 & **23.5** & **33.4** & 30.3 & **37.3** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 개방형 구조적 세분화 및 비지도 의미 세분화 작업에 대한 마스크 수를 변경하는 효과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline  & \\multicolumn{3}{c}{mIoU (\\(\\uparrow\\))} \\\\  & layer 1 & layer 2 & layer 3 \\\\ \\hline Unsupervised seg. & **33.1** & **33.1** & 32.8 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: ** \\(16\\t 16\\) 상향 블록에서 각 교차 의도 계층에서 특징 맵을 추출하는 효과이다. ADE150K에서 평가되었습니다. 성능의 유의한 차이는 없었다.****** 실적의 유의한 차이는 없었다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:18]\n' +
      '\n' +
      '중심 변조된 계산 및 교차 의도 계층입니다. 구체적으로, 변조된 계산을 위해 우리는 완전히 연결된 층 \\(f\\) 전에 오프셋 \\(c\\)을 추가하는 \\(fets\\left(\\sigma{QK^{T}}{\\sqrt{d}}\\right)\\(f\\)\\(f\\)를 고려한다. 그림 14에서 시각화된 바와 같이, 두 계산 모두 시각적으로 그럴듯한 분할 맵을 생성하며, 이는 우리의 틀이 오프셋 주입 장소에 민감하지 않음을 나타낸다. 서로 다른 교차 주의 레이어의 선택을 위해, 우리는 제1 레이어와 제3 레이어 사이의 명확한 차이를 관찰하는데, 이는 이들 레이어가 생성된 이미지 및 이미지 픽셀의 상이한 속성에 대한 책임이 다르기 때문일 수 있다. 전반적으로, 제3 교차 표시층은 변조된 계산과 \\(\\lambda\\)에 관계없이 상대적으로 괜찮은 분할 맵을 생성한다. 우리는 세 번째 층을 조절하는 것이 자연스럽게 의미론적 물체의 픽셀들의 값을 변경하게 된다는 것을 추측하며, 이는 세 번째 층이 외관을 제어하고 있다는 사실에 기인할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline  & \\multicolumn{5}{c}{mIoU (\\(\\uparrow\\))} \\\\ \\(t_{m}\\) & 1 & 81 & 281 & 481 & 681 & 881 \\\\ \\hline Unsupervised seg. & 29.0 & 31.8 & **33.1** & **33.1** & 31.5 & 27.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '**는 비지도 시맨틱 분할 w/ ADE20K/ ADE20K를 평가했으며, 표 7: **Varying 변조 타임스메프 \\(t_{m}\\)를 평가했습니다.\n' +
      '\n' +
      '그림 14: 서로 다른 교차 의도 계층을 변조하는 효과(\\(sigma\\cdot V\\right)+cM\\),\\(f\\left,\\sigma\\cdot V+cM\\right) 및 서로 다른 \\(\\lambda\\)를 나타낸다. 교차적 계층의 경우, 우리는 \\(16\\t 16\\) 상향 모듈식 블록에서 세 개의 다른 층을 실험한다. 우리는 편의를 위해 \\(\\sigma\\left(\\frac{QK^{T}{\\sqrt{d}\\right)\\에서 \\(\\sigma\\)로 약칭한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline  & \\multicolumn{5}{c}{mIoU (\\(\\uparrow\\))} \\\\ \\(\\lambda\\) & 1 & 10 & 100 & 1000 \\\\ \\hline Unsupervised seg. & 32.7 & **33.1** & 32.8 & 32.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '**는 비지도 시맨틱 분할 w/ ADE20K/ ADE20K를 평가했으며, 표 8: **Vary 변조 강도 \\(\\lambda\\)를 평가했습니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:20]\n' +
      '\n' +
      '그림 16: ADE20K(제우 et al., 2019)에서 제작한 분할 맵의 예.\n' +
      '\n' +
      '그림 17: 생성된 PASCAL-컨텍스트(Mottaghi et al., 2014)에 대한 분할 맵의 예.\n' +
      '\n' +
      '그림 18: 생산된 COCO-Stuff(Caesar et al., 2018)에 대한 분할 맵의 예.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>