<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# ViewDiff: 텍스트-이미지 모델을 이용한 3D-일관성 이미지 생성\n' +
      '\n' +
      'Lukas Hollein\\({}^{1,2}\\) Aljaz Bozic\\({}^{2}\\) Norman Muller\\({}^{2}\\) David Novotny\\({}^{2}\\) Hung-Yu Tseng\\({}^{2}\\)\n' +
      '\n' +
      'Christian Richardt\\({}^{2}\\) Michael Zollhofer\\({}^{2}\\) Matthias Niessner\\({}^{1}\\)\n' +
      '\n' +
      '뮌헨大學 \\({}^{2}\\)Meta\\({}^{1}\\)Technical University\n' +
      '\n' +
      '[https://lukashoel.github.io/ViewDiff/](https://lukashoel.github.io/ViewDiff/)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '3D 자산 생성은 최근 텍스트 유도 2D 콘텐츠 생성의 성공에 영감을 받아 엄청난 관심을 받고 있다. 기존의 텍스트-투-3D 방법은 사전 학습된 텍스트-투-이미지 확산 모델을 최적화 문제에 사용하거나 합성 데이터에 미세 조정함으로써 배경 없이 비사실적 3D 객체를 생성하는 경우가 많다. 본 논문에서는 사전 훈련된 텍스트-이미지 모델을 사전 모델로 활용하는 방법을 제시하고, 실제 데이터로부터 단일 잡음 제거 과정에서 다시점 이미지를 생성하는 방법을 학습한다. 구체적으로, 텍스트-투-이미지 모델의 기존 U-Net 네트워크의 각 블록에 3D 볼륨 렌더링 및 크로스-프레임-어텐션 레이어를 통합할 것을 제안한다. 또한, 임의의 시점에서 더 많은 3D-일관성 이미지를 렌더링하는 자기회귀 생성기를 설계한다. 우리는 실제 객체 데이터 세트를 기반으로 모델을 훈련하고 실제 환경에서 다양한 고품질 모양과 질감을 가진 인스턴스를 생성할 수 있는 기능을 보여준다. 제안된 방법은 기존의 방법들과 비교하였을 때, 일관된 결과를 보였으며, 좋은 시각적 품질(\\(-30\\%\\) FID, \\(-37\\%\\) KID를 보였다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근, 텍스트-투-이미지(T2I) 확산 모델[31, 35]이 텍스트 설명에 의해 안내되는 고품질 및 상상력 있는 2D 콘텐츠 생성에 혁명을 일으키며 첨단 기술로 부상하고 있다. 이러한 프레임워크는 ControlNet[59] 및 DreamBooth[34]와 같은 확장을 포함하여 광범위한 응용 프로그램을 발견하여 범용성과 잠재력을 보여준다. 이 영역에서 흥미로운 방향은 3차원(3D) 자산을 생성하기 위한 강력한 2D 사전으로 T2I 모델을 사용하는 것이다. 이러한 모델을 어떻게 효과적으로 사용하여 사진 현실적이고 다양한 3D 자산을 만들 수 있을까요?\n' +
      '\n' +
      '드림퓨전[29], 판타지아3D[5], ProlificDreamer[52]와 같은 기존 방법은 미리 훈련된 T2I 확산 모델로부터 점수 증류 샘플링[29]을 통해 3D 표현을 최적화하여 흥미로운 결과를 보여주었다. 이러한 방법에 의해 생성된 3D 자산은 강력한 다양성을 나타낸다. 그러나 이들의 시각적 품질은 T2I 모델에 의해 생성된 이미지만큼 일관되게 높지 않다. 3D 자산을 얻기 위한 핵심 단계는 원하는 객체와 주변 환경의 일관된 다시점 이미지를 생성할 수 있는 능력이다. 그런 다음 이러한 이미지는 NeRF[26] 또는 NeuS[50]와 같은 3D 표현에 적합할 수 있다. HoloDiffusion[19] 및 ViewsetDiffusion[42]는 다시점 영상을 이용하여 스크래치로부터 확산 모델 _from scratch_을 학습하고 3D-일관성 영상을 출력한다. GeNVS[4]와 DFM[46]은 추가적으로 객체 주변을 생성하여 세대의 현실감을 높인다. 이러한 방법은 실제 3D 데이터 세트[33, 62]에 대한 훈련을 통해 (사진) 사실적인 결과를 보장한다. 그러나 이러한 데이터 세트는 T2I 확산 모델을 훈련하는 데 사용되는 2D 데이터 세트보다 수십 배 작다. 결과적으로, 이러한 접근법들은 현실적이지만 덜 다양한 3D 자산들을 생성한다. 또는 Zero-1-to-3 [24] 및 One-2-3-45 [23]과 같은 최근 작업은 미리 훈련된 T2I 모델을 활용하고 3D 일관성을 위해 미세 조정한다. 이러한 방법은 대규모 합성 3D 데이터 세트[7]에서 훈련함으로써 생성된 결과의 다양성을 성공적으로 보존한다. 그럼에도 불구하고, 제작된 물체들은 덜 사실적일 수 있고 주변 환경이 없다.\n' +
      '\n' +
      '본 논문에서는 미리 훈련된 T2I 확산 모델의 2D 전개를 활용하여 _photo-realistic_와 _3D-consistent_ 3D 애셋 렌더링을 생성하는 방법을 제안한다. 그림 1의 첫 번째 두 행에 표시된 것처럼. 도 1에서, 입력은 원하는 렌더링된 이미지들의 카메라 포즈들과 함께, 텍스트의 설명 또는 객체의 이미지이다. 제안된 방법은 단일 순방향 패스에서 동일한 객체의 다중 이미지를 생성한다. 또한, 새로운 시점(그림 1, 세 번째 행)에서 더 많은 이미지를 렌더링할 수 있는 자기회귀 생성 기법을 설계한다. 구체적으로, 생성된 객체에 대한 명시적인 3D 지식을 인코딩하기 위해 기존의 U-Net 아키텍처에 전략적으로 배치된 프로젝션 및 크로스 프레임-어텐션 레이어를 소개한다(도 2 참조). 이렇게 함으로써, 우리의 접근법은 미리 훈련된 가중치로 인코딩된 큰 2D 사전으로부터 이익을 얻으면서 CO3D[33]와 같은 실제 3D 데이터 세트에서 T2I 모델을 미세 조정하는 방법을 제공한다. 생성된 이미지는 일관되고 다양하며 사실적인 객체 렌더링이다.\n' +
      '\n' +
      '요약하자면, 우리의 기여는 다음과 같습니다.\n' +
      '\n' +
      '* 텍스트-이미지 모델 이전에 미리 훈련된 2D를 활용하여 3D-일관성 있는 이미지 생성기로 바꾸는 방법. 우리는 실세계 멀티뷰 데이터셋에 대한 접근 방식을 훈련하여 사물과 주변 환경에 대한 사실적이고 고품질의 이미지를 생성할 수 있다(Sec. 3.1).\n' +
      '* 일반적으로 사용되는 2D 레이어와 3D 인식 레이어를 결합하는 새로운 U-Net 아키텍처. 우리의 프로젝션 및 크로스-프레임-어텐션 레이어는 명시적인 3D 지식을 U-Net 아키텍처의 각 블록에 인코딩한다(Sec. 3.2).\n' +
      '*_any_ desired viewpoint _directly_로부터 3D 객체의 이미지들을 3D-consistent 방식으로 우리의 모델과 렌더링하는 자기회귀 생성 방식(Sec. 3.3).\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '문자-2D 디노이징 확산 확률 모델(DDPM) [14]는 딥 네트워크로 가우시안 잡음 처리를 반전시키는 학습을 통해 데이터 분포를 모델링한다. 최근 DDPM은 생성적 적대 네트워크[8]보다 우수한 것으로 나타나 이미지 생성을 위한 최첨단 프레임워크가 되었다. 곧이어 Imagen[35] 또는 Dall-E 2[31]에서 10억 규모의 데이터에 대해 훈련된 대규모 텍스트 조건 모델이 제안되었다. [8]은 분류기로 안내를 통해 조건부 생성을 달성했지만 [13]은 분류기가 없는 안내를 제안했다. ControlNet[59]은 이미지 분할 또는 정규 맵과 같은 다양한 모달리티에서 컨디셔닝을 통해 확산 출력을 튜닝하는 방법을 제안했다. ControlNet과 유사하게, 우리의 방법은 사전 훈련된 텍스트-이미지(T2I) 모델 이전에 강력한 2D를 기반으로 한다. 우리는 물체의 3D-일관성 이미지를 생성하기 전에 이것을 조정하는 방법을 추가로 시연한다.\n' +
      '\n' +
      '텍스트-To-3D.2D DDPM은 텍스트 설명으로부터 3D 형상 [30, 37, 44, 47, 49, 55, 64] 또는 장면 [10, 15, 45]의 생성에 적용되었다. DreamFusion[29]은 DDPM의 믿음과 일치하는 3D 모양을 최적화하는 점수 증류 샘플링(SDS)을 제안했다. 개선된 샘플 품질은 2단계 메쉬 최적화[5, 22] 및 더 부드러운 SDS 수렴[38, 52]에 의해 달성되었다. 여러 방법들은 3D 데이터를 사용하여, 멀티-뷰 샘플들이 나중에 3D로 변환될 수 있는 신규-뷰 합성 모델을 트레이닝하는데, 예를 들어, 이미지 상의 2D DDPM 및 신규 뷰들을 생성하기 위한 상대적인 카메라 모션을 컨디셔닝한다[24, 53]. 그러나 지오메트리의 명시적인 모델링이 없기 때문에 출력이 뷰와 일치하지 않습니다. 일관성은 에피폴라 주의[48, 63]로 개선되거나, 또는 멀티뷰 제안들로부터 3D 형상을 최적화할 수 있다[23]. 본 논문에서는 2차원 T2I 모델을 미세조정하여 3차원 객체의 렌더링을 생성하되, 뷰 일관성을 향상시키기 위해 명시적인 3차원 비투영 연산자와 렌더링 연산자를 제안한다. 동시에 SyncDreamer[25]는 또한 그들의 2D DDPM에 3D 레이어를 추가한다. 우리는 배경이 있는 실제 데이터에 대한 훈련과 자기회귀 생성이 일관된 이미지를 생성하기에 충분하여 두 번째 3D 재구성 단계를 소모할 수 있음을 보여줌으로써 다르다.\n' +
      '\n' +
      'Diffusion on 3D Representations.몇몇 작품들은 3D Representations의 분포를 모델링한다. DiffRF [28]은 지상-진실 3D 모양을 활용하는 반면 홀로확산 [19]는 2D 이미지로만 감독된다. HoloFusion[18]은 2D 확산 렌더 후 프로세서로 이 작업을 확장합니다. 이미지는 또한 재구성 3D 형상[1, 42]을 렌더링함으로써 잡음제거될 수 있다. 불행히도, 기존의 3D 데이터 세트의 제한된 규모는 이러한 3D 확산 모델이 훈련 분포를 넘어 외삽하는 것을 방지한다. 대신에, 우리는 큰 2D 사전 훈련된 DDPM을 이용하고 더 작은 스케일 멀티뷰 데이터에 튜닝되는 3D 컴포넌트들을 추가한다. 이는 10억 규모의 이미지 데이터에 대한 사전 훈련으로 인한 표현성을 유지하면서 다시점 일관성을 향상시킨다.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '주어진 텍스트 또는 포즈된 이미지 입력으로부터 3D-일관성 이미지를 생성하는 방법을 제안한다(도 1의 상단/중간 참조). 구체적으로, 원하는 출력 포즈가 주어지면 조건에 해당하는 모든 이미지를 공동으로 생성한다. 우리는 미리 훈련된 텍스트 투 이미지(T2I) 모델[31, 35]을 활용하여 멀티 뷰 데이터로 미세 조정한다[33]. 각 블록에 새로운 레이어를 추가하여 기존의 U-Net 아키텍처를 증강할 것을 제안한다(도 2 참조). 테스트 시간에 우리는 여러 이미지에서 방법을 조정할 수 있다(그림 1 참조). 1 bottom), 이는 확산 모델과 함께 _any_ viewpoint _directly_로부터 동일한 객체를 자동으로 렌더링할 수 있게 한다(Sec. 3.3 참조).\n' +
      '\n' +
      '### 3D-Consistent Diffusion\n' +
      '\n' +
      '확산 모델 [14, 39]는 데이터 \\(x_{0}{\\sim}q(x_{0})\\)와 잠재 변수 \\(x_{1:T}{=}x_{1},\\dots,x_{T}\\)에 대한 확률 분포 \\(p_{\\theta}(x_{0}){=}\\int p_{\\theta}(x_{0:T})dx_{1:T}\\)을 학습하는 생성 모델의 클래스이다. 제안된 방법은 사전 학습된 텍스트-이미지 모델(diffusion model \\(p_{\\theta}(x_{0}\\mid c)\\)을 기반으로 추가 텍스트 조건 \\(c\\)을 갖는다. 명확성을 위해 이 섹션의 나머지 부분에 대한 조건 \\(c\\)을 삭제한다.\n' +
      '\n' +
      '서로 3차원적으로 일치하는 다중 영상을 한 번에 생성하기 위해, 이들의 결합 확률 분포(p_{\\theta}(x_{0}^{0:N}){=}\\int p_{\\theta}(x_{0:T}^{0:N})dx_{1:T}^{0:N}\\)를 모델링하고자 한다. Liu et al. [25]에 의한 동시 작업과 유사하게, 우리는 DDPMs [14]의 _reverse process_를 모든 이미지들에 대해 공동으로 마르코프 체인으로서 적응시킴으로써 하나의 이미지 세트 \\(p_{\\theta}(x_{0}^{0:N})\\)를 생성한다:\n' +
      '\n' +
      '\\[p_{\\theta}(x_{0:T}^{0:N}):=p(x_{T}^{0:N})\\prod_{t=1}^{T}\\prod_{n=0}^{N}p_{ \\theta}(x_{t-1}^{n}\\mid x_{t}^{0:N}), \\tag{1}\\\n' +
      '\n' +
      '여기서 우리는 이미지당 개별적으로 샘플링된 가우시안 잡음으로부터 생성을 시작한다. \\(p(x_{T}^{n})=\\mathcal{N}(x_{T}^{n};\\mathbf{0},\\mathbf{I})\\), \\(\\forall n\\in[0,N]\\). 모든 이미지 간에 공유되는 신경망 \\(\\mu_{\\theta}^{n}(x_{t-1}^{n}^{0:N});\\mu_{\\theta}^{n}(x_{t-1};\\mu_{\\theta}^{n}(x_{t}^{0:N},t),\\sigma_{t}^{2}\\mathbf{I})\\)을 예측하여 샘플 \\(p_{\\theta}(x_{t-1}^{n}(x_{t}^{0:N},t))=\\mathcal{N}(x_{t-1};\\mu_{\\theta}^{n}(x_{t}^{0:N},t)\\)을 점진적으로 잡음 제거한다. 중요한 것은, 각 단계에서 모델은 모든 이미지의 이전 상태\\(x_{t}^{0:N}\\) 즉, 모델 예측 동안 이미지 간의 통신이 존재한다는 것이다. 우리는 이것이 어떻게 구현되는지에 대한 자세한 내용은 Sec. 3.2를 참조한다. \\(\\mu_{\\theta}\\)를 훈련하기 위해, 우리는 _forward process_를 마르코프 체인으로 정의한다:\n' +
      '\n' +
      '\\[q(x_{1:T}^{0:N}\\mid x_{0}^{0:N})=\\prod_{t=1}^{T}\\prod_{n=0}^{N}q(x_{t}^{n}\\mid x_{t-1}^{n}), \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(q(x_{t}^{n}\\mid x_{t-1}^{n})=\\mathcal{N}(x_{t}^{n};\\sqrt{1-\\beta_{t}}x_{t-1}^{n},\\beta_{t}\\mathbf{I})\\) 및 \\(\\beta_{1},\\dots,\\beta_{T}\\)은 일정한 분산 스케줄을 정의하며, 즉, 훈련 샘플을 생성하기 위해 이미지당 별도의 노이즈를 적용한다.\n' +
      '\n' +
      '우리는 \\(\\mu_{\\theta}\\) 대신 _noise predictor_\\(\\epsilon_{\\theta}\\)을 학습하여 Ho et al. [14]를 따른다. 이를 통해 \\(L2\\) 손실로 \\(\\epsilon_{\\theta}\\)을 훈련시킬 수 있다.\n' +
      '\n' +
      '[\\mathbb{E}_{x_{0}^{0:N},c^{0:N}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}),n}\\left[\\left\\|\\epsilon^{n}-\\epsilon_{\\theta}^{n}(x_{t}^{0:N},t}\\right\\|^{2}\\right]. \\tag{3}\\\\t.\n' +
      '\n' +
      'U-Net 아키텍처의### 증강\n' +
      '\n' +
      '모든 영상에 대해 3차원 일관성 있는 잡음제거 과정을 모델링하기 위해 신경망(\\epsilon_{\\theta}\\)을 통해 영상당 잡음(\\epsilon_{\\theta}^{n}(x_{t}^{0:N},t)\\)을 예측한다. 이 신경망은 기존의 텍스트 대 이미지 모델의 사전 훈련된 가중치로부터 초기화되며, 일반적으로 U-Net 아키텍처[31, 35]로 정의된다. 우리는 모든 영상의 이전 상태\\(x_{t}^{0:N}\\)를 이용하여 3차원 일관성 있는 잡음 제거 단계에 도달하고자 한다. 이를 위해, 우리는 U-Net 아키텍처, 즉 크로스 프레임-어텐션 레이어와 프로젝션 레이어에 두 개의 레이어를 추가할 것을 제안한다. 예측된 영상당 잡음은 영상 특이적일 필요가 있다는 점에 주목한다.\n' +
      '\n' +
      '도 2: **Method Overview.** 모든 U-Net 블록에 새로운 레이어를 갖는 사전 훈련된 텍스트-이미지 모델의 U-Net 아키텍처를 증강한다. 이러한 레이어들은 일괄적으로 멀티뷰 이미지들 간의 통신을 용이하게 하여, 3D-일관성 이미지들을 공동으로 생성하는 잡음 제거 프로세스를 초래한다. 먼저, 모든 뷰의 공간 특징을 비교하는 cross-frame-attention(노란색)으로 self-attention을 대체한다. 우리는 각 이미지의 포즈(\\(RT\\)), 내재성(\\(K\\)), 강도(\\(I\\))에 대한 모든 주의 레이어를 컨디셔닝한다. 둘째, U-Net의 내부 블록에 프로젝션 레이어(그린)를 추가한다. 그것은 멀티뷰 피처들로부터 3D 표현을 생성하고, 그것들을 3D-일관된 피처들로 렌더링한다. 우리는 확산 잡음 제거 목적(Eq)을 사용하여 U-Net을 미세 조정한다. 3) timestep \\(t\\)에서, 캡션된 멀티뷰 이미지들로부터 감독된다.\n' +
      '\n' +
      '모든 이미지는 별도의 가우시안 잡음으로부터 시작하여 생성된다. 따라서 기존 ResNet[11] 및 ViT[9] 블록을 미세 조정함으로써 달성되는 각 이미지에 별도로 작용하는 2D 레이어를 유지하는 것이 중요하다. 우리는 그림 2에서 우리의 아키텍처를 요약한다. 이하에서는 제안된 두 가지 계층에 대해 더 자세히 논의한다.\n' +
      '\n' +
      'Cross-Frame Attention.Video Diffusion[54, 56]에서 영감을 받아 U-Net 아키텍처에 Cross-Frame Attention 레이어를 추가한다. 구체적으로, 기존의 자기 주의 계층을 수정하여 \\(\\textit{CFA}\\textit{t}\\textit{n}(Q,K,V){=}\\textit{softmax}\\big{(}\\frac{QK^{T}{\\sqrt{d}\\big{)}V\\)와 \\(\\textit{CFA}\\textit{t}\\textit{n}(Q,K,V){=}\\textit{softmax}\\big{(}\\frac{QK^{T}{\\sqrt{d}\\big{)}V\\)를 계산한다.\n' +
      '\n' +
      '\\[Q=W^{Q}h_{i},\\K=W^{K}[h_{j}]_{j\\neqi},\\V=W^{V}[h_{j}]_{j\\neqi},\\tag{4}\\]\n' +
      '\n' +
      '여기서 \\(W^{Q},W^{K},W^{V}\\)는 특징 투영을 위한 사전 훈련된 가중치이고, \\(h_{i}{\\in}\\mathbb{R}^{C\\times H\\times W}\\)는 각 이미지의 입력 공간 특징이다. 직관적으로 모든 프레임에 걸쳐 피쳐와 일치하므로 동일한 글로벌 스타일을 생성할 수 있습니다.\n' +
      '\n' +
      '또한, 모든 크로스-프레임 및 크로스-어텐션 레이어에 컨디셔닝 벡터를 추가하여 각 이미지의 시점을 네트워크에 알린다. 먼저, 각 영상의 카메라 행렬 \\(p\\in\\mathbb{R}^{4\\times 4}\\)을 Zero-1-to-3 [24]와 유사한 임베딩 \\(z_{1}\\in\\mathbb{R}^{4}\\)에 인코딩하여 포즈 정보를 추가한다. 또한 각 카메라의 초점거리와 주점을 임베딩 \\(z_{2}\\in\\mathbb{R}^{4}\\)으로 연결한다. 마지막으로, 이미지 RGB 값의 평균과 분산을 저장하는 강도 인코딩 \\(z_{3}\\in\\mathbb{R}^{2}\\)을 제공한다. 학습 시간에는 각 입력 영상의 참값으로 \\(z_{3}\\)을 설정하고, 테스트 시간에는 모든 영상에 대해 \\(z_{3}{=}[0.5,0]\\)을 설정한다. 이는 (예를 들어, 상이한 카메라 노출로 인해) 데이터세트에 포함된 뷰-종속 조명 차이를 감소시키는 것을 돕는다. 컨디셔닝 벡터를 \\(z{=}[z_{1},z_{2},z_{3}]\\으로 구성하고, 이를 LoRA-linear-layer[16]\\(W^{Q}\\)을 통해 특징 투영 행렬 \\(Q\\)에 추가한다. 구체적으로, 우리는 투영된 특징들을 다음과 같이 계산한다:\n' +
      '\n' +
      '\\[Q=W^{Q}h_{i}+s\\cdot W^{\\prime Q}[h_{i};z], \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(s{=}1\\)을 설정한다. 마찬가지로, 우리는 \\(W^{\\prime K}\\)을 \\(K\\)에, \\(W^{\\prime V}\\)을 \\(V\\)에 추가한다.\n' +
      '\n' +
      '프로젝션 레이어.크로스 프레임 주의 레이어는 전 세계적으로 3D 일관된 이미지를 생성하는 데 도움이 됩니다. 그러나, 객체들은 특정된 포즈들을 정확하게 따르지 않으며, 이는 뷰-불일관성으로 이어진다(도 5 및 탭 3 참조). 이를 위해 U-Net 아키텍처에 프로젝션 레이어를 추가한다(도 3). 이 계층의 아이디어는 다음 U-Net 계층(예를 들어, ResNet 블록)에 의해 추가로 처리되는 3D-일관성 특징을 생성하는 것이다. U-Net의 모든 단계에서 이 레이어를 반복함으로써 이미지당 기능이 3D 일관된 공간에 있는지 확인합니다. 우리는 첫 번째와 마지막 U-Net 블록에 프로젝션 레이어를 추가하지 않는데, 우리는 이러한 위치에서 그것들의 혜택을 보지 못했기 때문이다. 본 논문에서는 이러한 단계에서 이미지 특정 정보를 처리하므로 3D-일관성 있는 특징 공간이 필요하지 않다고 추론한다.\n' +
      '\n' +
      '다시점 스테레오 문헌[3, 17, 41]에서 영감을 얻어, 각 복셀을 각 이미지 평면에 투영하여 모든 입력 공간 특징(h_{\\text{in}^{0:N}\\in\\mathbb{R}^{C\\times H\\times W}\\)으로부터 3차원 특징 복셀 격자를 생성한다. 먼저, 축소된 특징 차원(C^{\\prime}{=}16\\)으로 \\(1{\\times}1\\)의 컨볼루션으로 \\(h_{\\text{in}^{0:N}\\)을 압축한다. 그런 다음 이미지 평면 위치에서 쌍선형 보간된 피쳐를 취하여 복셀에 배치한다. 이러한 방식으로 뷰당 별도의 복셀 그리드를 생성하고 집계기 MLP를 통해 단일 그리드로 병합한다. IBRNet[51]에서 영감을 얻은 MLP는 시점별 가중치에 이어 가중 특징 평균을 예측합니다. 그런 다음 복셀 그리드에서 작은 3D CNN을 실행하여 3D 피쳐 공간을 정제한다. 그 후, Voxel 격자를 NeRF[26]와 유사한 볼륨 렌더링을 사용하여 출력 특성\\(h_text{out}^{0:N}in\\mathbb{R}^{C}^{prime}\\times H\\times W}\\)으로 렌더링한다. 복셀 격자의 절반은 전경에, 절반은 배경에 할당하고, MERF[32]의 배경 모델을 레이 마킹 동안 적용한다.\n' +
      '\n' +
      '볼륨 렌더링 출력 후 스케일 함수를 추가할 필요가 있음을 발견하였다. 볼륨 렌더러는 일반적으로 레이-마칭 동안 최종 레이어로서 _sigmoid_활성화 함수를 사용한다[26]. 그러나 입력 피쳐는 임의의 부동 소수점 범위에서 정의됩니다. (h_{\\text{out}^{0:N}\\)을 다시 같은 범위로 변환하기 위해, 우리는 \\(1{\\times}1\\) 컨볼루션과 _ReLU_ 활성화로 특징을 비선형적으로 스케일링한다. 마지막으로 입력 특징 차원(C\\)으로 \\(h\\text{out}^{0:N}\\)을 확장한다. 각 구성 요소의 아키텍처에 대한 자세한 내용은 첨부 자료를 참조합니다.\n' +
      '\n' +
      '### Autoregressive Generation\n' +
      '\n' +
      '제안된 방법은 여러 개의 샘플(x_{t}^{0:N}\\)을 한 번에 입력 받아 3D-일관되게 잡음제거한다. 학습하는 동안, 우리는 \\(N{=}5\\)을 설정하지만, 추론 시간에 메모리 제약 조건(예: \\(N{=}30\\)까지 증가시킬 수 있다. 그러나 우리는 렌더링하고 싶습니다.\n' +
      '\n' +
      '그림 3: **프로젝션 레이어의 아키텍처.** 포즈된 입력 피처로부터 3D-일관된 출력 피처를 생성한다. 먼저, 압축된 이미지 특징을 3D로 투영하고 이를 MLP가 있는 조인트 복셀 그리드로 통합한다. 그런 다음 3D CNN으로 복셀 그리드를 정제한다. NeRF[26]와 유사한 볼륨 렌더러는 그리드로부터 3D-일관된 특징들을 렌더링한다. 마지막으로 학습된 축척 함수를 적용하여 특징 차원을 확장한다.\n' +
      '\n' +
      'object from _any_ possible viewpoint directly with our network. 이를 위해, 본 논문에서는 이전에 생성된 영상에 대해 다음 시점의 생성을 조건으로 하는 자기회귀 영상 생성 기법을 제안한다. 우리는 U-Net의 입력으로 각 이미지의 타임스텝 \\(t^{0:N}\\)을 제공한다. 우리는 \\(t^{0:N}\\)을 변화시킴으로써 다양한 유형의 컨디셔닝을 달성할 수 있다.\n' +
      '\n' +
      '무조건 생성.모든 샘플은 가우시안 잡음으로 초기화되고 공동으로 잡음 제거된다. Timesteps \\(t^{0:N}\\)은 _reverse process_ 전체에 걸쳐 모든 샘플에 대해 동일하게 유지된다. 이미지당 다양한 카메라와 단일 텍스트 프롬프트를 제공합니다. 생성된 영상들은 3D-일관성이 있으며, 원하는 시점으로부터 객체를 보여준다(도 4 및 도 5).\n' +
      '\n' +
      '영상조건생성(Image-Conditional Generation.N{=}n_{\\text{c}}{+}n_{\\text{g}\\)을 조건부(n_{\\text{c}}\\)와 생성부(n_{\\text{g}\\)로 나눈다. 첫 번째 표본은 입력으로 제공되는 이미지와 카메라에 해당한다. 다른 (ntext{g}}) 샘플들은 컨디셔닝 이미지들과 유사한 새로운 뷰들을 생성해야 한다. 본 논문에서는 n(ntext{g}}) 샘플들에 대해 가우시안 잡음으로부터 생성을 시작하고, 다른 샘플들에 대해서는 잡음이 없는 영상을 제공한다. 마찬가지로 모든 잡음 제거 단계에 대해 \\(t^{0:n_{\\text{c}}}}=}0\\)을 설정하고, 점차적으로 \\(t^{n_{\\text{g}}:N}\\)을 감소시킨다.\n' +
      '\n' +
      '(n_{\\text{c}}{=}1\\)일 때, 본 방법은 단일 이미지 재구성을 수행한다(도 6). 설정 \\(n_{\\text{c}}{>}1\\)은 이전 이미지들로부터 새로운 뷰들을 자동으로 점진적으로 생성할 수 있게 한다(도 1 하단). 실제로 우리는 먼저 한 배치의 이미지를 무조건 생성한 다음 이전 이미지의 하위 집합에서 다음 배치를 조정한다. 이를 통해 3D 객체 주변의 부드러운 궤적을 렌더링할 수 있다(첨부 자료 참조).\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'Dataset.우리는 실제 물체의 포즈된 다시점 영상으로 구성된 대규모 CO3Dv2[33] 데이터셋에서 방법을 훈련한다. 구체적으로 테디베어, 하이드런트, 애플, 도넛 카테고리를 선택합니다. 카테고리당 500-1000개의 객체를 학습하고, 각 200개의 이미지를 256(\\times\\)256의 해상도로 학습한 후 BLIP-2 모델[21]로 텍스트 캡션을 생성하고, 객체당 5개의 제안 중 하나를 샘플링한다.\n' +
      '\n' +
      '훈련.우리는 미리 훈련된 잠재 확산 텍스트 대 이미지 모델을 기반으로 한다. U-Net을 미세 조정하고 VAE 인코더와 디코더를 냉동 상태로 유지합니다. 각 반복에서, 우리는 \\(N{=}5\\) 이미지와 그들의 포즈를 선택한다. 1개의 잡음제거 타임스텝(t{\\sim}[0,1000]\\)을 샘플링하고, Eq.에 따라 영상에 잡음을 추가한다. 2, 그리고 Eq. 3에 따라 손실을 계산한다. 프로젝션 레이어에서, 우리는 새로운 뷰들로부터 렌더링될 수 있는 3D 표현을 학습하도록 강제하는 복셀 그리드를 구축할 때 마지막 이미지를 건너뛴다. 우리는 무조건 생성과 이미지 조건 생성(Sec. 3.3)을 달리하여 방법을 훈련한다. 구체적으로 확률 \\(p_{1}{=}0.25\\)과 \\(p_{2}{=}0.25\\)을 갖는 첫 번째 영상과 두 번째 영상을 입력으로 하여 각각의 타임스텝을 0으로 설정한다. Ruiz et al. [34]와 유사하게, 우리는 사전 훈련된 텍스트-투-이미지 모델로 _prior dataset_를 생성하고 2D 이전을 유지하기 위해 훈련 동안 그것을 사용한다(세부사항에 대한 보충 자료 참조).\n' +
      '\n' +
      '총 배치 크기가 64인 60K 반복(7일) 동안 2\\(\\times\\) A100 GPU에서 모델을 미세 조정한다. 볼륨 렌더러에 대한 학습률은 0.005, 다른 모든 레이어에 대한 학습률은 \\(5{\\times}10^{-5}\\)으로 설정하고 AdamW 최적화기를 사용한다[35]. 추론 과정에서 우리는 RTX 3090 GPU에서 \\(N\\)을 증가시키고 최대 30개의 이미지/배치를 생성할 수 있다. 우리는 15초가 걸리는 10개의 잡음 제거 단계가 있는 UniPC [61] 샘플러를 사용한다.\n' +
      '\n' +
      '## 4 Results\n' +
      '\n' +
      '기준.3D 생성 모델링을 위한 최신 작업과 비교합니다. 우리의 목표는 진정한 환경을 가진 현실 세계의 사실적인 객체로부터 멀티 뷰 일관된 이미지를 만드는 것입니다. 따라서 본 논문에서는 실세계 데이터셋을 학습한 방법을 고려하여 홀로퓨전(HF) [18], ViewsetDiffusion (VD) [42], DFM [46]을 선택한다. 무조건 생성(Sec. 4.1)과 단일 영상 재구성(Sec. 4.2)의 두 가지 작업에 대한 결과를 보여준다.\n' +
      '\n' +
      '메트릭스.2D/3D 생성을 위한 공통 메트릭으로 FID[12]와 KID[2]를 보고하고, 최대 신호 대 잡음비(PSNR), 구조적 유사성 지수(SSIM), LPIPS[60]를 사용하여 생성된 이미지의 다중 뷰 일관성을 측정한다. 비교 가능성을 보장하기 위해 모든 기준선 모델이 아닌 배경이 없는 이미지의 모든 메트릭을 평가한다.\n' +
      '\n' +
      '인공물(도 5 참조). VD[42]는 일관적이지만 흐릿한 이미지를 생성한다. 이와는 대조적으로, 본 방법은 배경 및 고해상도 객체 상세를 갖는 이미지를 생성한다. 부탁을 들어주세요. 보다 많은 예제 및 애니메이션 결과를 위한 재료입니다.\n' +
      '\n' +
      '### Single-Image Reconstruction\n' +
      '\n' +
      '본 논문에서 제안하는 방법은 임의의 새로운 뷰를 자기회귀적 방식으로 렌더링하기 위해 여러 개의 이미지에 대해 조건화 할 수 있다. 생성된 영상의 3차원 정합성을 측정하기 위해 단일 영상 재구성과 ViewsetDiffusion (VD) [42] 및 DFM [46]을 비교한다. 구체적으로, 데이터 세트에서 하나의 이미지를 샘플링하고 데이터 세트에서 샘플링된 새로운 뷰에서 20개의 이미지를 생성한다. 우리는 Szymanowicz et al. [42]를 따르고 모든 방법에 대해 다수의 객체 및 시점에 걸쳐 per-view 최대 PSNR/SSIM 및 평균 LPIPS를 보고한다. 우리는 정량적 결과를 보고한다.\n' +
      '\n' +
      '도 4: ** 우리의 방법과 기준선의 무조건적인 이미지 생성.** 여러 객체와 카테고리에 대해 서로 다른 시점의 렌더링을 보여준다. 제안하는 방법은 일관된 객체와 배경을 생성한다. 우리의 질감은 기준선에 비해 선명합니다. 자세한 예제 및 애니메이션은 첨부 자료를 참조하십시오.\n' +
      '\n' +
      '탭 도 2 및 도 6에서 정성적 결과를 도시한다. VD[42]는 배경 없이 그럴듯한 결과를 생성한다. DFM[46]은 낮은 영상 해상도(128\\(\\times\\)128)에서 배경과 일관된 결과를 생성한다. 제안된 방법은 DFM[46]과 유사한 복원 결과와 배경을 가진 고해상도 영상을 생성한다. 자세한 예제 및 애니메이션 결과는 첨부 자료를 참조하십시오.\n' +
      '\n' +
      '### Ablations\n' +
      '\n' +
      '우리 방법의 핵심 요소는 U-Net(Sec. 3.2)에 추가하는 교차 프레임 주의 및 투영 계층이다. 우리는 탭에서 그들의 중요성을 강조한다. 도 3 및 도 5에 도시된 바와 같다.\n' +
      '\n' +
      '프로젝션 레이어들은 얼마나 중요한가?그들은 이미지 뷰포인트들에 대한 정밀한 제어를 허용하는데 필요하다(예를 들어, 도 5 행 3은 지정된 회전을 따르지 않는다). 우리의 목표는 모델(Sec. 3.3)과 함께 _any_ viewpoint _directly_로부터 일관된 이미지 세트를 생성하는 것이다. 따라서 객체의 포즈를 제어할 수 있는 것은 우리의 기여에서 필수적인 부분이다. 프로젝션 레이어들은 볼륨 렌더링을 통해 3D-일관된 피처들로 명시적으로 렌더링되는 객체의 3D 표현을 구축한다. 이를 통해 단일 이미지 재구성(탭. 3)을 통해서도 입증된 바와 같이 시점 일관성을 달성할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{Teddybear} & \\multicolumn{3}{c}{Hydrant} & \\multicolumn{3}{c}{Donut} & \\multicolumn{3}{c}{Apple} \\\\ \\cline{2-13}  & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline VD [42] & 19.68 & 0.70 & 0.30 & 22.36 & 0.80 & 0.19 & 18.27 & 0.68 & 0.14 & 19.54 & 0.64 & 0.31 \\\\ DFM [46] & 21.81 & 0.82 & 0.16 & **22.67** & 0.83 & 0.12 & **23.91** & **0.86** & **0.10** & 25.79 & **0.91** & **0.07** \\\\ Ours & **21.98** & **0.84** & **0.13** & 22.49 & **0.85** & **0.11** & 21.50 & 0.85 & 0.18 & **25.94** & **0.91** & 0.11 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **단일 이미지 재구성의 정량적 비교.** 단일 이미지가 입력으로 주어지면 카테고리당 평균 PSNR, SSIM 및 LPIPS[60]을 통해 신규 뷰의 품질을 측정한다. 우리는 모든 방법에 걸쳐 비교 가능성을 보장하기 위해 생성된 배경을 마스킹한다. 우리는 DFM[46]과 동등하면서 VD[42] 이상으로 개선한다.\n' +
      '\n' +
      '도 5: **무조건 이미지 생성의 멀티-뷰 일관성.** 홀로퓨전(HF) [18]은 뷰-의존성 부동 아티팩트들(첫 번째 행의 베이스)을 갖는다. ViewsetDiffusion(VD) [42]에는 블러리어 렌더링(두 번째 행)이 있습니다. 투영 레이어가 없으면, 우리의 방법은 시점(세 번째 행)에 대한 정확한 제어가 없다. 프레임 간 주의 집중 없이, 본 방법은 객체(4번째 행)의 동일성 변화를 겪는다. 우리의 전체 방법은 3D 일치(5행)의 상세한 이미지를 생성한다.\n' +
      '\n' +
      '크로스 프레임-어텐션 레이어는 얼마나 중요한가요? 동일한 객체의 이미지를 만드는 데 필요합니다. 그것들이 없으면, Fig.의 테디베어입니다. 도 5(행 4)는 동일한 일반적인 컬러 스킴을 가지며 지정된 포즈들을 따른다. 그러나 모양과 질감의 차이는 일관되지 않은 이미지 집합으로 이어진다. 우리는 일관된 _object identity_를 정의하기 위해 cross-frame-attention layer가 필수적이라고 추론한다.\n' +
      '\n' +
      '2D 사전이 도움이 되나요? 우리는 3D 일관된 방식으로 미세 조정하는 사전 훈련된 텍스트-이미지 모델의 형태로 2D 사전을 활용합니다(Sec. 3.1). 이를 통해 본 논문에서 제안하는 방법은 서로 다른 관점에서 사물의 선명하고 세밀한 이미지를 생성할 수 있다. 또한 텍스트 기술(Sec. 3.4)을 통해 제어 가능한 생성을 유지하기 위해 자막 이미지에 대한 방법을 학습한다. 우리는 그림 1에서 우리 세대의 다양성과 통제 가능성을 보여준다. 7은 손으로 만든 텍스트 프롬프트가 있습니다. 이것은 미세 조정 후 우리 모델이 여전히 텍스트 입력에 충실하고 새로운 방식으로 속성을 결합할 수 있음을 강조한다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '제안하는 방법은 텍스트 기술이나 입력 영상에 따라 다양한 객체의 3D 일관성이 높은 고품질의 영상을 생성한다. 그럼에도 불구하고 몇 가지 제한 사항이 있다. 첫째, 본 논문에서 제안하는 방법은 보충과 같이 약간의 불일치가 있는 이미지를 생성하는 경우가 있다. 모델은 시점 의존적 효과(예: 노출 변화)로 구성된 실제 데이터 세트에서 미세 조정되기 때문에 프레임워크는 다양한 시점에 걸쳐 이러한 변화를 생성하도록 학습한다. 잠재적인 해결책은 ControlNet[59]을 통해 조명 조건을 추가하는 것이다. 둘째, 본 연구는 객체에 초점을 맞추지만, 유사한 대규모 데이터 세트 [6, 57]에 대한 장면 규모 생성을 탐색할 수 있다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 뷰디프(ViewDiff: ViewDiff)를 제안한다. 뷰디프는 텍스트 또는 이미지 입력을 통해 실제 환경에 배치된 물체의 3D-일관성 있는 이미지를 생성하는 방법이다. 본 논문에서 제안하는 방법은 대규모 2D 텍스트-이미지 모델의 표현성을 활용하고, 실제 3D 데이터 세트에서 이 2D를 미세 조정함으로써 관절 노이즈 제거 과정에서 다양한 다시점 이미지를 생성한다. 우리 작품의 핵심 통찰력은 두 개의 새로운 계층, 즉 크로스 프레임-어텐션과 프로젝션 계층(Sec. 3.2)이다. 자체 회귀 생성 기법(Sec. 3.3)은 생성된 3D 객체의 고품질 및 사실적인 새로운 뷰를 직접 렌더링할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Method & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) & FID\\(\\downarrow\\) & KID\\(\\downarrow\\) \\\\ \\hline Ours (no proj) & 16.55 & 0.71 & 0.29 & 47.95 & **0.034** \\\\ Ours (no cfa) & 18.15 & 0.76 & 0.25 & 47.93 & **0.034** \\\\ Ours & **22.24** & **0.84** & **0.11** & **47.92** & **0.034** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: ** 방법과 삭제의 정량적 비교.** 테디베어 및 하이드란트 범주에 대한 평균 PSNR, SSIM, LPIPS[60], FID[12], KID[2]를 보고한다. 우리는 U-Net에서 프로젝션 레이어("Ours no proj")와 크로스 프레임-어텐션 레이어("Ours no cfa")를 드롭하는 것과 비교한다(Sec. 3.2 참조). 여전히 유사한 FID/KID 점수를 가진 고품질 이미지를 생성하지만, 이는 제안된 레이어가 3D 일관된 이미지를 얻기 위해 필요하다는 것을 보여준다.\n' +
      '\n' +
      '그림 6: ** 우리의 방법과 기준선의 단일 이미지 재구성.** 하나의 이미지/포즈를 입력으로 주어지면 우리의 방법은 실제 모양과 질감과 일치하는 그럴듯한 새로운 뷰를 생성한다. 또한 입력과 일치하는 자세한 배경을 생성할 수 있습니다.\n' +
      '\n' +
      '그림 7: **생성된 결과의 다양성.** 원하는 스타일로 객체를 생성할 수 있는 텍스트 입력에 대해 방법을 조정한다. 우리는 새로운 방식으로 속성(예: 색상, 모양, 배경)을 결합하는 손으로 만든 텍스트 설명에 대한 샘플을 보여준다. 각 행은 우리의 방법과 다른 세대 제안을 나타내며 우리는 객체 범주(Teddybear, Hydrant)를 [C]로 표시한다. 이것은 생성된 결과의 다양성을 보여주는데, 즉, 동일한 설명에 대해 다수의 상이한 객체들이 생성된다.\n' +
      '\n' +
      '## 6 Acknowledgements\n' +
      '\n' +
      '이 작업은 메타 후원 연구 협정의 지원을 받은 TU 뮌헨뿐만 아니라 메타 리얼리티 랩스 취리히에서 루카스가 인턴을 하는 동안 수행되었다. 마티아스 니스너는 또한 ERC Starting Grant Scan2CAD(804724)에 의해 지원되었다. 우리는 또한 비디오 보이스오버에 대해 안젤라 다이에게 감사한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Titas Anciukevicius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Renderdiffusion: Image diffusion for 3D reconstruction, inpainting and generation. In _CVPR_, 2023.\n' +
      '* [2] Mikolaj Binkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In _ICLR_, 2018.\n' +
      '* [3] Aljaz Bozic, Pablo Palafox, Justus Thies, Angela Dai, and Matthias Niessner. TransformerFusion: Monocular RGB scene reconstruction using transformers. In _NeurIPS_, 2021.\n' +
      '* [4] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. Generative novel view synthesis with 3D-aware diffusion models. arXiv:2304.02602, 2023.\n' +
      '* [5] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3D: Disentangling geometry and appearance for high-quality text-to-3D content creation. In _ICCV_, 2023.\n' +
      '* [6] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. ScanNet: Richly-annotated 3D reconstructions of indoor scenes. In _CVPR_, 2017.\n' +
      '* [7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Obyarese: A universe of annotated 3D objects. In _CVPR_, 2023.\n' +
      '* [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In _NeurIPS_, 2021.\n' +
      '* [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16\\(\\times\\)16 words: Transformers for image recognition at scale. In _ICLR_, 2021.\n' +
      '* [10] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-NeRF2NeRF: Editing 3D scenes with instructions. In _ICCV_, 2023.\n' +
      '* [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, 2016.\n' +
      '* [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In _NeurIPS_, 2017.\n' +
      '* [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In _NeurIPS Workshops_, 2021.\n' +
      '* [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _NeurIPS_, 2020.\n' +
      '* [15] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Niessner. Text2Room: Extracting textured 3D meshes from 2D text-to-image models. In _ICCV_, 2023.\n' +
      '* [16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _ICLR_, 2022.\n' +
      '* [17] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. _JMLR_, 24(251):1-43, 2023.\n' +
      '* [18] Animesh Karnewar, Niloy J. Mitra, Andrea Vedaldi, and David Novotny. HoloFusion: Towards photo-realistic 3D generative modeling. In _ICCV_, 2023.\n' +
      '* [19] Animesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy J. Mitra. HoloDiffusion: Training a 3D diffusion model using 2D images. In _CVPR_, 2023.\n' +
      '* [20] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and Ming-Hsuan Yang. Learning blind video temporal consistency. In _European Conference on Computer Vision_, 2018.\n' +
      '* [21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrap language-image pre-training with frozen image encoders and large language models. arXiv:2301.12597, 2023.\n' +
      '* [22] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-3D content creation. In _CVPR_, 2023.\n' +
      '* [23] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang Xu, Hao Su, et al. One-2-3-45: Any single image to 3D mesh in 45 seconds without per-shape optimization. arXiv:2306.16928, 2023.\n' +
      '* [24] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9298-9309, 2023.\n' +
      '* [25] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. SyncHeamer: Generating multiview-consistent images from a single-view image. In _The Twelfth International Conference on Learning Representations_, 2024.\n' +
      '* [26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.\n' +
      '* [27] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 41(4), 2022.\n' +
      '* [28] Norman Muller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulo, Peter Kontschieder, and Matthias Niessner. DiffRF:Rendering-guided 3D radiance field diffusion. In _CVPR_, 2023.\n' +
      '* [29] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. In _ICLR_, 2023.\n' +
      '* [30] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksand Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3D object generation using both 2D and 3D diffusion priors. arXiv:2306.17843, 2023.\n' +
      '* [31] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. arXiv:2204.06125, 2022.\n' +
      '* [32] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Peter Hedman. MERF: Memory-efficient radiance fields for real-time view synthesis in unbounded scenes. _ACM Transactions on Graphics (TOG)_, 42(4):1-12, 2023.\n' +
      '* [33] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3D: Large-scale learning and evaluation of real-life 3D category reconstruction. In _ICCV_, 2021.\n' +
      '* [34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _CVPR_, 2023.\n' +
      '* [35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In _NeurIPS_, 2022.\n' +
      '* [36] Nikita Selin. CarveKit. github.com/OPHoperHPO/image-background-remove-tool, 2023.\n' +
      '* [37] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, and Seungryong Kim. Let 2D diffusion model know 3D-consistency for robust text-to-3D generation. arXiv:2303.07937, 2023.\n' +
      '* [38] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. MVDream: Multi-view diffusion for 3D generation. arXiv:2308.16512, 2023.\n' +
      '* [39] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, 2015.\n' +
      '* [40] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In _CVPR_, 2022.\n' +
      '* [41] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In _CVPR_, 2021.\n' +
      '* [42] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. ViewSet diffusion: (0-)image-conditioned 3D generative models from 2D data. In _ICCV_, 2023.\n' +
      '* [43] Matthew Tancik, Ethan Weber, Evonne Ng, Ruitlong Li, Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, and Angjoo Kanazawa. Nerrfstudio: A modular framework for neural radiance field development. In _SIGGRAPH_, 2023.\n' +
      '* [44] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3D: High-fidelity 3D creation from a single image with diffusion prior. In _ICCV_, 2023.\n' +
      '* [45] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa. MVDiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion. In _NeurIPS_, 2023.\n' +
      '* [46] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Joshua B. Tenenbaum, Fredo Durand, William T. Freeman, and Vincent Sitzmann. Diffusion with forward models: Solving stochastic inverse problems without direct supervision. In _NeurIPS_, 2023.\n' +
      '* [47] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari. TextMesh: Generation of realistic 3D meshes from text prompts. In _3DV_, 2024.\n' +
      '* [48] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsian, JianBin Huang, and Johannes Kopf. Consistent view synthesis with pose-guided diffusion models. In _CVPR_, 2023.\n' +
      '* [49] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score Jacobian chaining: Lifting pretrained 2D diffusion models for 3D generation. In _CVPR_, 2023.\n' +
      '* [50] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In _NeurIPS_, 2021.\n' +
      '* [51] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. IBRNet: Learning multi-view image-based rendering. In _CVPR_, 2021.\n' +
      '* [52] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelity and diverse text-to-3D generation with variational score distillation. arXiv:2305.16213, 2023.\n' +
      '* [53] Daniel Watson, William Chan, Ricardo Martin Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. In _ICLR_, 2023.\n' +
      '* [54] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In _ICCV_, 2023.\n' +
      '* [55] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin Tong. 3D-aware image generation using 2D diffusion models. In _ICCV_, 2023.\n' +
      '* [56] Shuai Yang, Yifan Zhou, Ziwei Liu,, and Chen Change Loy. Rerender a video: Zero-shot text-guided video-to-video translation. In _SIGGRAPH Asia_, 2023.\n' +
      '\n' +
      '* [57] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Niessner, and Angela Dai. ScanNet++: A high-fidelity dataset of 3D indoor scenes. In _ICCV_, 2023.\n' +
      '* [58] Zehao Yu, Anpei Chen, Bozidar Antic, Songyou Peng, Apratim Bhattacharyya, Michael Niemeyer, Siyu Tang, Torsten Sattler, and Andreas Geiger. SDFStudio: A unified framework for surface reconstruction, 2023.\n' +
      '* [59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _ICCV_, 2023.\n' +
      '* [60] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_, 2018.\n' +
      '* [61] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. UniPC: A unified predictor-corrector framework for fast sampling of diffusion models. In _NeurIPS_, 2023.\n' +
      '* [62] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. _ACM Transactions on Graphics_, 37(4):65:1-12, 2018.\n' +
      '* [63] Zhizhuo Zhou and Shubham Tulsiani. SparseFusion: Distilling view-conditioned diffusion for 3D reconstruction. In _CVPR_, 2023.\n' +
      '* [64] Joseph Zhu and Peijve Zhuang. HiFA: High-fidelity text-to-3D with advanced diffusion guidance. arXiv:2305.18766, 2023.\n' +
      '\n' +
      '**ViewDiff: Text-to-Image Models을 이용한 3D-Consistent Image Generation**\n' +
      '\n' +
      'Supplementary Material\n' +
      '\n' +
      '## 부록 추가 영상\n' +
      '\n' +
      '제안된 방법에 대한 종합적인 평가를 위해 첨부된 영상 1을 시청하시기 바랍니다. 우리는 서로 다른 카메라 고도에서 새로운 궤적에서 여러 개의 생성된 객체의 렌더링된 비디오를 포함한다(도 4에서와 같이 무조건적인 생성을 보여준다). 우리는 또한 단일 이미지 재구성에 대한 애니메이션 결과를 보여준다(도 6). 및 샘플 다이버시티(도 7).\n' +
      '\n' +
      '각주 1: [https://youtu.be/SdjoCqHzmMk](https://youtu.be/SdjoCqHzmMk)\n' +
      '\n' +
      '## 부록 B 훈련 세부사항\n' +
      '\n' +
      '### Data Preprocessing\n' +
      '\n' +
      '본 논문에서는 실제 물체의 다시점 영상으로 구성된 대규모 CO3Dv2[33] 데이터셋을 이용하여 학습한다. 구체적으로 테디베어, 하이드런트, 애플, 도넛 카테고리를 선택합니다. 카테고리별로 500-1000개의 객체를 학습하고, 각 객체에 대해 200개의 영상을 256×256×256×256×256의 해상도로 학습한다. BLIP-2 모델[21]을 사용하여 텍스트 캡션을 생성하고 각 학습 반복 동안 객체당 5개의 제안 중 하나를 샘플링한다. 확률\\(p_{1}{=}0.5\\)은 객체마다 랜덤하게 학습 영상을 선택하고 확률\\(p_{2}{=}0.5\\)은 객체 주위를 이동하는 촬영 궤적에서 연속된 영상을 선택한다. 우리는 256({\\times}\\)256의 해상도로 이미지를 랜덤하게 크롭하고 카메라 포즈를 정규화하여 캡처된 객체가 축 정렬된 단위 큐브에 놓이도록 한다. 구체적으로, 우리는 Szymanowicz et al. [42]를 따르고, 모든 카메라들이 축-정렬된 평면 상에 정렬되도록 회전 변환을 계산한다. 그런 다음 카메라 위치를 번역하고 축척하여 경계 상자가 단위 큐브에 포함되도록 한다.\n' +
      '\n' +
      '사전 보존 손실\n' +
      '\n' +
      'Ruiz et al. [34]에 의해 영감을 받아, 우리는 사전 훈련된 텍스트-대-이미지 모델로 카테고리당 300개의 이미지 및 랜덤 포즈의 _prior 보존_ 데이터세트를 생성한다. 사전 이미지 생성을 유지하기 위해 훈련 중에 사용합니다. 이것은 더 작은 규모의 데이터에 대해 큰 2D 확산 모델을 미세 조정할 때 성공적인 것으로 나타났다[34]. 300개의 이미지 각각에 대해 우리는 CO3Dv2의 훈련 세트에서 텍스트 설명을 무작위로 샘플링한다[33]. 그런 다음 텍스트 설명을 입력으로 하여 미리 훈련된 텍스트 대 이미지 모델로 이미지를 생성한다. 각 훈련 반복 동안 우리는 먼저 확산 목표(Eq)를 계산한다. 3) 데이터세트에서 샘플링된 \\(N{=}5\\) 다시점 영상에 대해 \\(L_{d}\\)을 구한다. 그런 다음, 우리는 _prior preservation_ dataset의 하나의 이미지를 샘플링하고 그것에 노이즈를 적용한다(식 2). 또한, 각 객체 범주에 대한 카메라의 분포 내에 있는 카메라(포즈 및 내부)를 샘플링한다. 그런 다음 마찬가지로 손실(Eq)을 계산합니다. 3) 모형의 예측결과 \\(L_{p}\\)을 얻었다. 우리는 다중이 아닌 단일 이미지만을 샘플링하기 때문에, 이것은 3D-일관성에 대한 확산 모델을 훈련시키지 않는다. 대신 이전에 이미지 생성을 유지하기 위해 모델을 훈련합니다. 구체적으로, 크로스-프레임-어텐션 레이어들은 다시 자기-어텐션 레이어들로 처리되고, 프로젝션 레이어들은 비투영 및 렌더링을 정상적으로 수행하지만, 입력으로서 단일 이미지로부터만 수행된다. 실제로, 우리는 사전 보존 손실을 인자\\(0.1\\)로 스케일링하고 데이터세트 손실에 추가하여 최종 손실인 \\(L{=}L_{d}+0.1L_{p}\\)을 얻는다.\n' +
      '\n' +
      '## 부록 C 평가 상세\n' +
      '\n' +
      '### Autoregressive Generation\n' +
      '\n' +
      '본 논문에서는 Sec. 4.1에서 제안한 방법의 무조건적인 생성을 보여주고, 이러한 결과를 얻기 위해 자기회귀 생성 기법(Sec. 3.3)을 사용한다. 구체적으로, 첫 번째 배치에 대한 테스트 세트로부터 (관측되지 않은) 이미지 캡션을 샘플링하고 \\(\\lambda_{\\text{cfg}{=}7.5\\)의 유도 척도[13]로 \\(N{=}10\\) 이미지를 생성한다. 그런 다음 후속 배치에 대해 \\(\\lambda_{\\text{cfg}}{=}0\\)을 설정하고 개체당 총 100개의 이미지를 생성한다. 첫 번째 배치가 물체를 중심으로 360도 회전으로 \\(N\\) 이미지를 생성한다면 그 결과가 가장 일치함을 알 수 있었다. 이러한 방식으로, 우리는 단일 잡음 제거 전진 패스에서 객체 모양과 텍스처를 전역적으로 정의한다. 모든 후속 배치들은 첫 번째 배치의 모든 \\(N\\) 이미지들에 컨디셔닝된다. 원활한 궤적을 렌더링하기 위해 우리는 카메라 포즈를 순서대로 다른 배치로 샘플링한다. 즉, 다음 \\(N\\) 영상은 그들 사이의 작은 회전만으로 서로 근접하게 된다. 우리는 보충 비디오에서 이 원리를 시각화한다.\n' +
      '\n' +
      '### Metric Computation\n' +
      '\n' +
      '탭에 표시된 대로 메트릭을 계산하는 방법에 대한 추가 세부 정보를 제공합니다. 1~3. 비교 가능성을 보장하기 위해 배경이 없는 이미지에 대한 모든 메트릭을 모든 기준 모델이 아닌 것으로 평가한다.\n' +
      '\n' +
      'FID/Kid.2D/3D 생성을 위한 공통 메트릭으로 FID[12]와 KID[2]를 보고한다. 우리는 이러한 메트릭을 계산하여 _unconditional_ 이미지 생성과 홀로퓨전 [18] 및 ViewsetDiffusion [42]를 비교한다. 이것은 생성된 이미지들의 데이터세트에 대한 유사성을 정량화하고, 그에 의해 그들의 품질(예를 들어, 텍스처 세부사항들 및 선명도) 및 다양성(예를 들어, 상이한 형상들 및 색상들)에 대한 통찰력을 제공한다. 기준선 [18, 19]에 따라 각 객체 범주에 대해 CO3Dv2 [33] 데이터 세트에서 20,000개의 이미지를 샘플링한다. 데이터 집합에 포함된 전경 마스크 확률을 이용하여 각 객체에서 배경을 제거한다. 마찬가지로 각 방법으로 20,000개의 이미지를 생성하고 제거한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '그런 다음 다음 U-Net 계층(예: ResNet 블록)에 의해 추가로 처리된다. 구체적으로, 축 정렬된 단위 큐브 내부에 정의된 복셀 그리드 형태의 모든 입력 피처로부터 3D 표현을 생성한다. 3차원 특징 차원을 \\(C^{\\prime}=16\\)으로 설정하고 복셀 격자의 기본 해상도를 128\\(\\times\\)128\\(\\times\\)128로 정의한다. U-Net을 통해 병목층에서의 2차원 특징과 동일한 업/다운 샘플링을 적용하여 해상도가 8\\(\\times\\)8\\(\\times\\)8로 감소한다. 프로젝션 레이어는 다수의 네트워크 컴포넌트들로 구성된다. 우리는 그림에서 이러한 구성 요소의 상세한 네트워크 아키텍처를 보여준다. 9 대 11.\n' +
      '\n' +
      '### 압축넷과 익스팬드넷\n' +
      '\n' +
      '우리는 3D 레이어를 \\(C^{\\prime}{=}16\\)의 단일차원으로 정의된 피쳐에 적용한다. 3D 레이어는 조밀한 복셀 그리드에서 작동하기 때문에 메모리 요구 사항을 낮추는 데 도움이 됩니다. 이 압축된 특징 공간으로 변환하기 위해 그림 9와 같이 작은 CNN을 사용한다. 이러한 도식에서 우리는 배치의 이미지 수로 \\(N\\), 압축되지 않은 특징 차원으로 \\(C\\), 특징의 공간 차원으로 \\(I\\)을 정의한다.\n' +
      '\n' +
      '### Aggregator MLP\n' +
      '\n' +
      '레이마킹을 통해 per-view 복셀 격자를 생성한 후(Sec. 3.2 참조), 모든 뷰에 대한 특징을 나타내는 하나의 복셀 격자로 \\(N\\) 복셀 격자를 결합한다. 이를 위해 그림 1에 표시된 대로 일련의 네트워크를 사용한다. 11. 이 도식에서 우리는 배치의 이미지 수로 \\(N\\), 압축된 피쳐 차원으로 \\(C^{\\prime}\\), 타임스텝 임베딩 차원으로 \\(T\\), 3D 복셀 그리드 해상도로 \\(G\\), 피쳐의 공간 차원으로 \\(I\\)을 정의한다. MLP는 _ELU_ 활성화를 사이에 두고 지정된 입력 및 출력 차원의 선형 레이어의 시퀀스로 정의된다.\n' +
      '\n' +
      '먼저, 각 복셀에 이미지 특징을 투영하기 위해 사용된 광선 방향 및 깊이의 인코딩과 복셀 특징을 연결한다. 또한 각 복셀에 타임스텝 임베딩을 연결합니다. 이를 통해 서로 다른 타임스테프의 per-view 복셀 그리드(예: Sec. 3.3의 이미지 조건부 생성에서 제안된 바와 같이)를 결합할 수 있다. 또한, 디노이징 프로세스 전반에 걸쳐 집계를 다르게 수행할 수 있게 하는 디노이징 타임스테프에 대해 후속 네트워크들에게 알리는 것이 유용하다. IBRNet[51]에서 영감을 얻은 MLP 세트는 뷰별 가중치에 이어 가중 피쳐 평균을 예측합니다. 우리는 이 평균화 연산을 요소별로 수행한다: 모든 복셀 그리드가 정의되어 있기 때문에\n' +
      '\n' +
      '도 11: **프로젝션 레이어 컴포넌트들의 아키텍처.**프로젝션 레이어는 컴포넌트 _Aggregator MLP_를 포함한다. 먼저, 시점별 복셀 그리드와 광선 방향/깊이 부호화(파란색) 및 시간 임베딩(녹색)을 결합한다. IBRNet[51]에서 영감을 얻은 MLP(핑크)는 뷰별 가중치에 이어 가중 특징 평균을 예측합니다. 마지막으로 복셀당 가중치를 평균 및 분산 그리드(노란색)와 결합하여 집계된 피쳐 그리드를 얻는다.\n' +
      '\n' +
      '동일한 단위 큐브, 모든 뷰에서 동일한 복셀을 결합할 수 있습니다. 마지막으로 복셀별 가중치를 평균 및 분산 격자와 결합하여 최종 집계된 특징 격자를 얻는다.\n' +
      '\n' +
      '### 3D Cnn\n' +
      '\n' +
      '뷰별 복셀 그리드를 공동 그리드로 집계한 후 해당 그리드를 더욱 세분화합니다. 이 네트워크의 목표는 형상의 글로벌 배향과 같은 특징 표현에 추가적인 세부사항들을 추가하는 것이다. 이를 달성하기 위해 그림 1과 같이 타임스텝 임베딩을 갖는 일련의 5개의 3D ResNet[11] 블록을 사용한다. 10. 이 도식에서는 압축 특징 차원으로서 \\(C^{\\prime}\\), 타임스텝 임베딩 차원으로서 \\(T\\), 3차원 복셀 그리드 해상도로 \\(G\\)을 정의한다.\n' +
      '\n' +
      '### 볼륨 렌더러 및 스케일넷\n' +
      '\n' +
      '복셀 그리드 형태의 정제된 3D 특징 표현을 얻은 후, 우리는 그 그리드를 다시 시점별 이미지 특징으로 렌더링한다(도 3 참조). 구체적으로는 NeRF[26]와 유사한 볼륨 렌더러를 채용하여 DVGO[40]와 유사한 그리드 기반 렌더러로 구현한다. 이를 통해 네트워크의 순방향 패스에 대한 병목 현상이 아닌 효율적인 방식으로 피쳐를 렌더링할 수 있다. NeRF와 달리, 우리는 _rgb_ 색상 대신 _features_를 렌더링한다. 구체적으로, 광선을 따라 128개의 점을 샘플링하고 각 점에 대해 복셀 격자 특징을 삼선형 보간하여 특징 벡터 \\(f\\in\\mathbb{R}^{C^{\\prime}\\)을 구한다. 그리고 밀도\\(d\\in\\mathbb{R}\\)와 샘플링된 특징\\(s\\in\\mathbb{R}^{C^{\\prime}\\)으로 변환하는 작은 3층 MLP를 사용한다. 알파 합성을 이용하여 모든 쌍((d_{0},s_{0}),...,(d_{127},s_{127})\\)을 광선을 따라 최종 렌더링된 특징(r\\in\\mathbb{R}^{C^{\\prime}}\\)으로 누적한다. 복셀 격자의 절반은 전경에, 절반은 배경에 할당하고, MERF[32]의 배경 모델을 레이 마킹 동안 적용한다.\n' +
      '\n' +
      '볼륨 렌더링 출력 후 스케일 함수를 추가할 필요가 있음을 발견하였다. 볼륨 렌더러는 일반적으로 레이-마칭 동안 최종 레이어로서 _sigmoid_활성화 함수를 사용한다[26]. 그러나 입력 피쳐는 임의의 부동 소수점 범위에서 정의됩니다. 다시 같은 범위로 변환하기 위해, 우리는 1\\(\\times\\)1 컨볼루션과 _ReLU_ 활성화로 특징을 비선형적으로 스케일링한다. 그림 9에서 이 _ScaleNet_의 아키텍처를 녹색 레이어로 묘사한다.\n' +
      '\n' +
      '## 부록 F 추가 결과\n' +
      '\n' +
      '### 추가기준과의 비교\n' +
      '\n' +
      '그림 1에서 사전 훈련된 텍스트 대 이미지 모델을 사용하는 추가 텍스트 대 3D 기준선과 비교한다. 12. 스코어 증류[29] 방법의 대표적인 방법으로 ProlificDreamer[52]를 선택한다. 최적화는 잡음이 많은 주변 환경과 과포화 텍스처를 생성할 수 있기 때문에 렌더링된 이미지는 덜 사실적이다. 우리와 유사하게 Zero123-XL[24]과 SyncDreamer[25]는 3D 일치 영상을 직접 생성함으로써 이 문제를 회피한다. 그러나 실제 이미지 대신 대규모 합성 데이터세트 [7]에서 미세 조정합니다. 결과적으로, 그들의 이미지는 합성 질감과 조명 효과를 가지며 배경은 없다. 우리는 이것을 탭에서 정량화한다. (입력 뷰 상에서 조건화된) 생성된 이미지들의 세트들 및 (배경 없이) 동일한 객체의 실제 이미지들 사이의 FID를 갖는 도 4. 이 방법은 생성된 영상이 더 사실적이기 때문에 더 좋은 점수를 갖는다.\n' +
      '\n' +
      '[20]에 이어서 광학 흐름 워핑을 갖는 비디오 렌더링의 시간적 안정성(\\(E_{\\text{warp}}\\))을 계산한다. 또한, 3D 표현을 직접 생성하지 않는 방법에 대해 생성된 이미지의 일관성을 측정한다. 구체적으로, NeRF [26] 재 렌더링과 입력 영상 사이의 PSNR과 [25]에 따른 점 대응의 개수를 보고한다. 표 4는 우리의 방법이 3D 일관성 측면에서 기준선과 동일하지만 더 높은 품질의 이미지를 생성한다는 것을 보여준다.\n' +
      '\n' +
      '### Unconditional Generation\n' +
      '\n' +
      '우리는 Sec. 4.1에서와 유사한 방식으로 이미지를 생성한다. 구체적으로, 첫 번째 배치에 대한 테스트 세트에서 (관측되지 않은) 이미지 캡션을 샘플링하고 \\(\\lambda_{cfg}{=}7.5\\)의 유도 척도[13]로 \\(N{=}10\\) 이미지를 생성한다. 그런 다음 후속 배치에 대해 \\(\\lambda_{cfg}{=}0\\)을 설정하고 개체당 총 100개의 이미지를 생성한다. 우리는 그림에서 추가적인 결과를 보여준다. 13 대 16.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c} \\hline \\hline Method & \\(E_{\\text{warp}}\\downarrow\\) & \\#Points\\(\\uparrow\\) & PSNR\\(\\uparrow\\) & FID\\(\\downarrow\\) \\\\ \\hline DFM [46] & 0.0034 & 17,470 & 32.32 & — \\\\ VD [42] & 0.0021 & — & — & — \\\\ HF [18] & 0.0031 & — & — & — \\\\ SyncDreamer [25] & 0.0042 & 4,126 & 33.81 & 135.78 \\\\ Zero123-XL (SDS) [24] & 0.0039 & — & — & 126.83 \\\\\n' +
      '**Ours** & 0.0036 & 18,358 & 33.65 & 85.08 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **일관성(중간)과 사실성(FID)의 비교.** 우리의 방법은 베이스라인으로서 유사한 3D-일관성을 보여주면서, 더 많은 사실성 이미지를 생성한다.\n' +
      '\n' +
      '그림 12: **이미지(위) 및 텍스트 입력(아래)의 다른 텍스트 대 3D 기준선과 비교.** 우리의 방법은 더 높은 사실성과 실제 환경을 가진 이미지를 생성한다.\n' +
      '\n' +
      '도 13: **우리의 방법의 추가적인 예들.** 텍스트 프롬프트가 입력으로서 주어지면, 우리는 우리의 자기회귀 생성 스킴(Sec. 3.3)으로 물체 주위의 부드러운 궤적을 생성한다. 생성된 샘플의 애니메이션에 대한 보충 비디오를 참조하십시오.\n' +
      '\n' +
      '도 14: **우리의 방법의 추가적인 예들.** 텍스트 프롬프트가 입력으로서 주어지면, 우리는 우리의 자기회귀 생성 스킴(Sec. 3.3)으로 물체 주위의 부드러운 궤적을 생성한다. 생성된 샘플의 애니메이션에 대한 보충 비디오를 참조하십시오.\n' +
      '\n' +
      '도 15: **우리의 방법의 추가적인 예들.** 텍스트 프롬프트가 입력으로서 주어지면, 우리는 우리의 자기회귀 생성 스킴(Sec. 3.3)으로 물체 주위의 부드러운 궤적을 생성한다. 생성된 샘플의 애니메이션에 대한 보충 비디오를 참조하십시오.\n' +
      '\n' +
      '도 16: **우리의 방법의 추가적인 예들.** 텍스트 프롬프트가 입력으로서 주어지면, 우리는 우리의 자기회귀 생성 스킴(Sec. 3.3)으로 물체 주위의 부드러운 궤적을 생성한다. 생성된 샘플의 애니메이션에 대한 보충 비디오를 참조하십시오.\n' +
      '\n' +
      '## NeRF/NeuS 최적화 부록 G\n' +
      '\n' +
      '본 논문에서 제안하는 방법은 새로운 카메라 위치의 영상을 자기회귀 생성 방식으로 직접 렌더링할 수 있다(Sec. 3.3 참조). 이를 통해 임의의 카메라 위치에서 동일한 3D 객체 주변의 부드러운 궤적을 렌더링할 수 있다. 사용 사례에 따라 생성된 3D 객체의 명시적인 3D 표현을 얻는 것이 바람직할 수 있다(우리의 방법을 사용하여 새로운 이미지를 자동으로 렌더링하는 대신). 우리는 생성된 이미지가 NeRF[26] 또는 NeuS[50]를 최적화하는 데 직접 사용될 수 있음을 보여준다. 구체적으로, 10K 반복(2분) 동안 생성된 영상에서 Instant-NGP[27] 구현으로 NeRF를 최적화한다. 또한, 20K 반복(15분) 동안 SDFSudio [43, 58]에서 _newsfacto_ 구현으로 NeuS를 최적화하여 메쉬를 추출한다. 먼저, 생성된 이미지의 배경을 카베킷[36]을 적용하여 제거한 후, 이 이미지들로 최적화를 시작한다. 우리는 그림에서 결과를 보여준다. 17 대 19\n' +
      '\n' +
      '도 17: **NeRF[26] 생성된 이미지로부터 최적화.** 왼쪽: 텍스트 프롬프트를 입력으로 주어지면, 우리는 자기회귀 생성 스킴(Sec. 3.3)으로 객체 주변의 부드러운 궤적을 생성한다. 총 100개의 이미지를 서로 다른 카메라 위치에 생성합니다. 오른쪽: 생성된 이미지에서 Instant-NGP[27]를 사용하여 NeRF를 생성한다. 최적화된 복사율 필드 위에 생성된 영상의 카메라 위치를 보여준다.\n' +
      '\n' +
      '도 19: 생성된 이미지로부터 **메쉬 추출.** 텍스트 프롬프트가 입력으로서 주어지면, 우리는 자기회귀 생성 스킴(Sec. 3.3)으로 객체 주위의 부드러운 궤적을 생성한다. 총 100개의 이미지를 서로 다른 카메라 위치에서 생성하고 카베킷으로 배경을 마스킹한다[36]. 그런 다음 NeuS[50]를 최적화하고 그로부터 메쉬를 추출한다(행당 마지막 4개의 이미지).\n' +
      '\n' +
      '도 18: **NeRF[26] 생성된 이미지로부터 최적화.** 왼쪽: 텍스트 프롬프트가 입력으로서 주어지면, 우리는 자기회귀 생성 스킴(Sec. 3.3)으로 물체 주위의 부드러운 궤적을 생성한다. 총 100개의 이미지를 서로 다른 카메라 위치에 생성합니다. 오른쪽: 생성된 이미지에서 Instant-NGP[27]를 사용하여 NeRF를 생성한다. 최적화된 복사율 필드 위에 생성된 영상의 카메라 위치를 보여준다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>