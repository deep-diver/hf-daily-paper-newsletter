<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '법인-소프트웨어 비디오 배경 회사법\n' +
      '\n' +
      '보시아오 판\\({}^{1,2}\\)는 반다 바울 황\\({}^{2}\\)가 쿠리샤나 쿠마르 싱\\({}^{2}\\)를 탄압한다.\n' +
      '\n' +
      '양주성({}^{1}\\)은 레오니다스 J. 게나스티({}^{2}\\)가 지메이 양\\({}^{2}\\)를 망라한다.\n' +
      '\n' +
      '고베 리서치({}^{1}\\,{}^{2}\\)는 스트란포드 대학({}^{2}\\)이다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '포그라운드 주제 운동에 대한 테일러가 움직이는 비디오 배경을 생성하는 것은 영화 산업과 시각적 효과 커뮤니티에 중요한 문제이다. 이 작업은 전경 주체의 운동과 외모에 부합하는 배경을 합성하는 동시에 작가의 창작 의도를 준수하는 것이다. 우리는 전통적으로 지루한 수동 노력을 필요로 하는 이 과정을 자동화하는 생성 모델인 법을 소개합니다. 우리의 모델은 대규모 비디오 확산 모델의 전력을 강조하며, 이 작업에 특별히 맞춰져 있습니다. 모든 경우에 전경 피사체 분할의 순서를 입력으로 하고 원하는 장면을 조건으로 설명하는 이미지와 조건 프레임에 부착하면서 현실적인 전경-배경 상호작용과 일관된 비디오를 생성한다. 우리는 인간-스크렌 상호작용 비디오의 대규모 데이터 세트에서 모델을 훈련시킨다. 광범위한 평가는 우리 모델의 우수한 성능을 보여주며, 기저부를 상당히 능가한다. 더욱이, 우리는 법률이 비인간 피험자를 포함한 다양한 분포 외 샘플로 일반화되는 경우를 보여준다. 프로젝트 웹페이지는 [기투비오의https://actany](기투비오의https://actany.githubio)에 방문하십시오.\n' +
      '\n' +
      '+\n' +
      '부츠 \'�: * 일\'은 아도베 인턴십 기간 동안 이루어졌다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '피험자의 연기 영상을 새로운 배경에 작곡하는 것은 영화 제작과 VFX에서 창의적인 스토리-텔링의 중심이다. 핵심 요건은 카메라 동작, 상호 작용, 조명 및 그림자 측면에서 전경 주체와 배경을 원활하게 통합함으로써 구성이 마치 현장에서 물리적으로 행동하는 것처럼 현실적이고 생생한 것으로 보인다. 영화 산업에서 이 과정은 예술인들이 먼저 3D 장면을 만든 다음 LED 벽의 스튜디오에서 연기 영상을 촬영하거나 3D 엔진에서 비디오를 렌더링하도록 하는 가상 제작[1]에 의해 수행되는 경우가 많다. 이 과정은 지루하고 비쌀 뿐만 아니라 가장 중요한 것은 예술가들이 아이디어를 빠르게 반복하는 것을 방지한다.\n' +
      '\n' +
      '이러한 예술적 워크플로우에 의해 영감을 받아 자동화 주제 인식 영상 배경 세대의 새로운 문제를 연구합니다. 그림과 같이. 1은 피사체 운동을 제공하는 전경 분할 시퀀스와 새로운 장면을 설명하는 조건 프레임을 고려할 때, 현실적으로 합성된 전경-배경 상호작용을 가진 소설 장면에 사람을 적응시키는 비디오를 생성하는 것을 목표로 한다. 이 조건 프레임은 배경 전용 이미지 또는 아도비 포토샵[3]과 같은 사진 편집 도구를 사용하거나 댈-E[32]와 같은 자동화된 이미지 아웃포팅 방법을 통해 수동으로 생성될 수 있는 배경 및 전경으로 구성된 복합 프레임일 수 있다.\n' +
      '\n' +
      '이 문제는 인간-점수 상호작용이 두 입력 신호만을 감안할 때 확장된 시공간 부피로 올바르게 추론되고 외삽될 필요가 있기 때문에 상당한 문제를 제기한다. 모델은 또한 본질적으로 모호한 문제인 전경 분할의 시퀀스로부터 카메라 운동에 대한 암묵적으로 이유할 필요가 있다. 예를 들어, 그림 1의 첫 번째 경우. 1, 모델은 여성이 지향하는 방향에 따라 움직이는 배경을 생성할 필요가 있다. 마지막으로, 다양한 응용 프로그램을 지원하기 위해 일반화 능력이 강한 모델을 가지고 있어 다양한 교과목의 현실적이고 창의적인 통합을 다양한 배경 장면으로 허용하는 것을 목표로 한다.\n' +
      '\n' +
      '현존하는 영상 생성 및 편집 작업들은 인상적인 진척에도 불구하고 이 과제를 해결할 수 없다. 최근의 접근 방식은 일반적으로 무조건적인 비디오 생성[15, 19, 45], 텍스트 조건 비디오 생성[6, 15, 17, 19, 39], 간단한 도장 마스킹 영역[44, 47]에 초점을 맞추고 있다. 한편, 비디오 편집 방법은 소스 비디오를 입력으로 가정하여 일부 조건 신호, 가장 일반적으로 자연어 [5, 8, 13, 16, 22, 24, 41]를 기반으로 편집한다. 그러나 이러한 방법을 만드는 편집은 주로 양식화에 한정되는데, 이는 소스 비디오에서 공간 구조를 보존하고 스타일화 변화만을 수행하는 것을 의미한다. 한편, 단순히 이미지를 전파하는 결과 [9, 17]는 반드시 전경 피사체 운동의 지침을 존중하지 않으므로 (나중에 Sec 4.2에 표시된) 과소 제한된다. 본 논문에서는 영상 배경의 구조와 질감을 완전히 생성하는 동시에 전경 피사체 운동과 일치하게 유지하는 것을 목표로 한다.\n' +
      '\n' +
      '이를 위해 시간적 추론을 위해 교차 프레임 관심을 이루는 확산 기반 모델을 제안한다. 구체적으로, 우리의 모델은 복각된 비디오 배경을 갖는 합성 비디오를 생성하기 위해 세그먼트화된 전경 피사체, 대응하는 마스크 및 배경의 단일 조건 프레임의 시퀀스를 입력함에 따라 이루어진다. 시간적 관심은 현재 확산 기반 영상 생성 기준[10, 13, 16, 17, 24, 41]이므로 시간적으로 일관된 동영상을 생성하는 능력으로 인해 프레임별 특징에 대한 시간적 자기 의사를 수행하는 동시에 배경 프레임의 특징에 대한 확산 과정을 컨디셔닝한다.\n' +
      '\n' +
      '자사고 방식으로 인간-스코어 상호 작용의 2.4M 영상으로 구성된 대규모 데이터세트[26]에서 모델을 훈련하고, DAVIS[30]의 비디오뿐만 아니라 홀드 아웃 세트 모두에서 평가한다. 어떤 곳에서는 조건 프레임을 따르는 매우 현실적인 비디오를 생성할 수 있으며 동시에 전경 동작에 부합하는 비디오 배경을 합성한다. 특히, 인간의 비디오에만 훈련되었음에도 불구하고, 어떤 곳에서도 비인간 교과를 제로샷 방식으로 일반화하는가.\n' +
      '\n' +
      '요약하면, 우리의 기여는 다음과 같습니다.\n' +
      '\n' +
      '1. 자동화 주제 인식 영상 배경 세대의 새로운 문제를 소개합니다.\n' +
      '2. 우리는 이 과제를 해결하기 위한 비디오 확산 기반 모델인 자가에서의 대규모 인간-scene 상호작용 비디오 데이터셋에서 이를 학습하기 위한 법을 제안합니다.\n' +
      '\n' +
      '그림 1: 전경 분할의 순서를 입력으로, 배경을 조건으로 설명하는 하나의 프레임, 어떤 식으로든 대상 운동에 적응하는 일관된 비디오 배경을 생성한다. 우리는 여기에서 두 개의 피험자를 보여주는데, 각각 두 개의 생성된 샘플을 가지고 있다. 물 튀기, 연기, 불꽃, 그림자, 오리발 등과 같은 매우 현실적인 디테일로 조건 프레임과 일치하는 비디오를 생성할 수 있는 경우 비인간 교과를 포함하여 교과와 배경의 다양한 분포로 일반화한다. 우리의 방법은 합성 프레임과 배경 전용 이미지를 모두 조건으로 작동합니다.\n' +
      '\n' +
      ' 어두운 방식으로.\n' +
      '3. 집중 평가는 우리의 모델이 사실적인 주제-점수 상호작용, 카메라 동작, 조명 및 그림자와 일관된 비디오를 생성하고 동물 및 인공물과 같은 비인간 피험자를 포함한 분포 외 데이터로 일반화한다는 것을 보여준다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '** 비디오 생성****입니다. 비디오 생성에는 긴 실의 작품이 있었습니다. 핵심 아키텍처는 GAN[11, 38, 40]에서 더 최근의 변압기[15, 39, 44, 47]로 진화하고 확산 모델[6, 9, 17, 19, 21, 24, 45]로 발전했다. 아래에서는 대부분의 관련 확산 기반 작품을 검토한다. 이러한 작품들 대부분은 시간적 인식을 얻기 위해 데노징 U-Net 내부의 시간적 자기 의도 블록들을 레버리지한다. 여기에 Text2V 비디오-Zero[24]가 영상 속 래치들을 연관시키기 위해 추가 시끄러운 스케줄링을 소개한다. LVDM[19]과 알바너 라텐트[6]는 둘 다 더 긴 기간 영상을 생성하기 위한 계층적 접근법을 설계한다. 고해상도 비디오 생성을 위한 공간 초해상도 모델을 추가로 정렬합니다. 기후D-iff [17]은 대규모 비디오 데이터셋에서 시간적 주의 블록을 훈련시킬 것을 제안하며, 이는 어떤 텍스트 대 이미지 확산 모델( 아키텍처가 맞는 것을 식별함)에 삽입되어 텍스트 대 비디오 모델로 전환할 수 있다. 비디오 워머1[9]은 공동 텍스트와 이미지 조건 생성을 가능하게 하기 위해 이중적 관심을 더 사용한다. 이 작품은 무조건적인 세대나 텍스트나 이미지 컨디셔닝에 초점을 맞추고 있지만, 추가적인 전경 운동의 지침을 따를 수는 없다.\n' +
      '\n' +
      '** 비디오 편집****입니다. 또 다른 스레드는 비디오 편집의 문제를 연구하는데, 여기서 소스 동영상이 입력으로 주어지고, 일부 조건 신호에 따라 편집이 수행된다. 텍스트2Live [5]는 입력 영상의 미리 학습된 영상 아틀레이스를 사용하고, 전경이나 배경에 텍스트 유도 편집을 수행한다. 유전자1[13]은 구조 일관성을 향상시키기 위한 추가적인 조건으로 미리 학습된 네트워크[33]에 의해 추정된 깊이 지도를 인용한다. Tune-A-V 비디오 [41]은 단일 입력 비디오 상의 공간-의도 블록들 및 모든 시간적-의도 블록들 중 일부만을 재정렬할 것을 제안한다. 토큰플로우[16]는 입력 비디오에서 계산된 잠재 가장 가까운 이웃 필드를 사용하여 모든 프레임에 걸쳐 편집된 특징을 전파한다. 비디오 컨트롤넷[22]과 컨트롤-A-비디오[10] 모두 입력 영상으로부터 추출된 깊이 맵 또는 캐니 에지들과 같은 추가적인 신호로 비디오 확산 과정을 조건화하기 위해 제어넷 유사 접근[46]을 채택한다. 이들 작품의 한 가지 단점은 생성된 동영상이 소스 영상으로부터 공간구조를 유지하는 경향이 있어 모델이 수행할 수 있는 편집을 크게 제한하고 있다는 점이다. 우리 작품에서 우리는 운동을 위한 전경 분할을 조건화하는 동시에 배경 정보를 하나의 조건 프레임에서만 추출할 것을 제안한다. 특히, 마스킹된 전경을 입력으로 사용하여 보존할 것과 무엇을 생성할 것인지와 같이 좋은 분리를 제공한다.\n' +
      '\n' +
      '**이미지와 비디오 그림***입니다. 이미지/영상 인포팅은 마스크로 표현되는 누락된 지역을 채우는 것을 목표로 한다. 이러한 방법은 자연어 및 이미지[34, 42, 35]와 같은 조건 신호를 취하거나 마스킹된 영역(14, 36, 44, 47]) 외부의 컨텍스트에만 의존한다. 최근의 확산 기반 이미지 인포팅 방법은 마스크 이미지와 마스크 자체의 조합을 사용하고, 자연 언어 [34, 42] 또는 조건 객체 [43]의 이미지 상에서 확산 과정을 조건하거나 무조건 확산 [36]을 수행한다. 비디오 인포팅의 경우 MAGVIT[44]는 마스킹 토큰 예측을 통해 훈련된 생성 비디오 변압기를 제안하고 이후 작은 마스킹 영역을 돌릴 수 있다. 프로페터[47]는 완성된 흐름을 통해 픽셀과 특징을 전파하여 유동 기반 방법을 설계한다. M3DDM[14]은 비디오 확산 모델을 기록하며, 비디오 인코더에 의해 추출된 글로벌 비디오 특징에 대한 확산 과정을 조건화한다. 이러한 작품과는 다른 조건 틀을 엄격하게 따르는 큰 배경 영역을 생성하는 것을 목표로 한다. 더욱이, 생성된 배경은 일관된 방식으로 전경 피사체 운동에 적응해야 한다. 이것은 이전의 실화 방법이 다룰 수 없다는 중요한 문제를 제기한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '먼저 Sec 3.1의 잠재 확산에 대한 필수 예비 배경을 제공한다. 우리는 Sec 3.2에서 우리의 문제를 공식적으로 정의하고 Sec 3.3에서 모델 설계에 설명되어 있으며, 따라서 트레이닝 세부 사항을 Sec 3.4에 명시한다.\n' +
      '\n' +
      '피난즈 모델.\n' +
      '\n' +
      'DDPM[20]과 같은 확산 모델은 노이즈를 추가하는 전진 공정과 탈의 역공정을 캡슐화한다. 확산 시간 단계 \\(\\tau\\)를 감안할 때, 순방향 공정은 \\(\\beta\\)로 표시된 미리 정의된 분산 일정에 따라 마르코프 사슬을 통해 데이터 분포 \\(x_{0}\\sim q(x_{0}})에 가우시안 노이즈를 증가적으로 도입한다.\n' +
      '\n' +
      '(\\mathbf{x}_{\\tau}_\\mathbf{tau}_\\mathcal{I})\\math{N}.\n' +
      '\n' +
      '백워드 과정을 위해 U-Net [35]\\(스메프실론_{\\theta}\\)를 훈련하여 \\(\\mathbf{x}_{\\tau}\\)를 변성시키고 원본 데이터 분포를 복구한다.\n' +
      '\n' +
      '타}(\\mathbf{tau),\\mathbf{worth}_{\\tau),\\mathbf{x}(\\math{f}_{\\tau)\\math{f}.\n' +
      '\n' +
      '\\(\\mathbf{\\mu}_{\\theta}\\) 및 \\(\\mathbf{\\Sigma}_{\\theta}\\)는 \\(\\epsilon_{\\theta}\\)에 의해 모수된다. 예측된 소음과 지상-진실 노이즈의 불일치는 훈련 목표로 최소화된다.\n' +
      '\n' +
      '안정적인 확산 [34]은 VAE[25]의 잠재 공간에서 확산 모델을 학습시킬 것을 추가로 제안한다. 구체적으로, 인코더 \\(\\mathcal{E}\\)는 입력 이미지 \\(x\\)를 잠재 표현(z=\\mathcal{E}(x)\\)로 압축하도록 학습하고 디코더 \\(\\mathcal{D}(x=\\mathcal{E})는 래치들을 픽셀 공간으로 재구성하도록 학습하여 \\(x=\\mathcal{D}(x))가 픽셀 공간으로 재구성한다. 이와 같이 VAE의 잠재공간에서 확산이 수행된다.\n' +
      '\n' +
      '### Problem Formulation\n' +
      '\n' +
      '전경 주체를 특징으로 하는 플라스크 R-CNN[18], 피험자 분할 서열, \\(\\mathcal{S}\\in\\mathbb{R}\\in\\mathbb{R}\\in\\mathb{R}^{T\\i 3)를 얻기 위해 먼저 입력 비디오, \\(\\mathcal{S}\\in\\mathbb{R}\\in\\mathbb{R}\\in\\mathbb{R}\\in\\mathb{S}\\in\\mathbb{R}\\in\\mathbb{R}\\in\\mathb{R}\\in\\mathb{R}\\mathb{R}\\in\\mathb{R}\\mathb{R}\\mathb{R}\\mathb{R}\\mathb{R}\\mathb{R}\\mathb{R}\\mathb{R}\\mathb{R}\\b{R}\\b{R}\\b{R}\\b{R}\\b{R \\(\\mathcal{S}\\) 및 \\(\\mathcal{M}\\) 모두 모델에 대한 입력 역할을 한다. (\\mathcal{S}\\)은 배경 픽셀이 127(grey)로 설정된 전경 피사체의 분할을 포함한다. (\\mathcal{M}\\)은 모든 실험, \\(H=W=256\\) 및 \\(T=16\\)에서 0과 1로 설정된 전경 픽셀을 가지고 있다.\n' +
      '\n' +
      '또한, 우리는 또한 우리가 생성하고자 하는 배경을 설명하는 단일 조건 프레임 \\(\\mathbf{c}\\in\\mathbb{R}^{H\\RA 3}\\)를 통합한다. 그림과 같이. 2, \\(\\mathbf{c}\\)는 훈련 시간에 \\(\\mathcal{X}\\)에서 무작위로 샘플링된 프레임인 반면 추론 시간에 전경-배경 구성 또는 배경 전용 이미지를 나타내는 프레임일 수 있다. 따라서, 목표는 합성된 배경과 동적으로 상호작용하는 피사체와 출력 비디오(\\mathcal{V}\\)를 생성하는 것이다. 이미지가 아닌 이미지를 조건으로 사용하는 동기는 이미지가 의도된 배경의 상세하고 구체적인 정보를 운반하기 위한 보다 단순한 매체이며, 특히 사용자가 이미 미리 정의된 타겟 장면 이미지를 갖는 경우이다.\n' +
      '\n' +
      '헬드-소프트웨어 비디오 디확산.\n' +
      '\n' +
      '잠재 비디오 확산 모델[17]을 기반으로 모델을 구축합니다. 건축 설계에서 우리는 1) 전경 피사체 시퀀스를 네트워크에 제공하여 적절한 움직임 안내를 가능하게 하고, 2) 배경 프레임으로부터 조건 신호를 주입하여 생성된 비디오를 조건에 부착하도록 한다.\n' +
      '\n' +
      '우리는 그림 2와 같이 전경 분할 서열 \\(\\mathcal{S}\\)을 사용하여 미리 훈련된 VAE[34] 인코더 \\(\\mathcal{E}\\)를 사용하여 잠재 특징(\\hat{\\mathcal{S}}\\hat{\\mathcal{S}}\\in\\mathbb{R}\\in\\mathbb{R}\\in\\mathb{R} 32\\시간 32\\)으로 전경 분할을 암호화한다. 우리는 전경 마스크 서열 \\(\\mathcal{M}\\)을 8회 다운샘플링하여 재구성된 마스크 서열 \\(\\hat{\\mathcal{M}}\\in\\mathbb{R}^{16\\tep 32\\t time 1}\\)을 얻어 잠재 피처 \\(\\hat{\\mathcal{S}\\)와 정렬한다. 디오징 네트워크 \\(\\epsilon_{\\theta}\\)를 훈련시키기 위해 동일한 VAE 인코더를 갖는 원래 프레임 \\(\\mathcal{X}\\mathcal{X}\\mathbb{R}\\in\\mathbb{R}^{16\\시간 32\\tgh 32\\t time 32\\)을 잠재 표현으로 인코딩하고 Eq에서 표시된 순방향 확산 과정(\\tau\\)으로 확산 시간 단계 \\(\\mathcal{X}\\)에 노이즈를 추가한다. (1) 시끄러운 잠재 기능 \\(\\mathcal{Z}_{\\tau}\\)를 얻기 위한 것이다. 우리는 이후 특징 차원을 따라 \\(\\hat{\\mathcal{M}}\\), \\(\\mathcal{Z}_{\\tau}}\\) 및 \\(\\mathcal{Z}_{\\tau}_{\\tau}^{i}\\in\\mathb{R}^{{R}^{{S}\\)를 U-Net으로 연결한다. 추론하는 동안 \\(\\mathcal{Z}_{0}\\)는 가우시안 노이즈로 초기화되고, Eq에서 설명한 후방 확산 과정에 따라 최종 결과를 샘플링하기 위해 여러 시간 단계에 대해 자동 조절 변성된다. >2. 그런 다음 변성된 래치들은 VAE 디코더 \\(\\mathcal{D}\\)를 통해 비디오로 디코딩된다.\n' +
      '\n' +
      '우리는 AnimateD-iff [17]을 기반으로 3D 데노징 U-Net을 구축합니다. 선학습된 T2I 확산 모델의 변성 U-Net에 공간 주의 레이어 사이에 일련의 모션 모듈을 삽입하여 미기후디프 작업을 수행한다. 이러한 모션 모듈은 몇 개의 특징 투영 계층에 이어 1D 시간적 자기 의도 블록으로 구성된다.\n' +
      '\n' +
      '조건 이미지 \\(\\mathbf{c}\\)의 경우 사전 작업 [26]을 따른다.\n' +
      '\n' +
      '그림 2: **건축 개요***. 훈련 중 훈련 영상에서 무작위로 샘플링된 프레임을 취하여 데모화 과정을 조건화한다. 시험 시간에 상태는 새로운 배경을 가진 주체의 합성 프레임 또는 배경 전용 이미지일 수 있다.\n' +
      '\n' +
      'CLIP 이미지 인코더[31]로 인코딩하기 위해 마지막 히든 레이어로부터의 특징을 자신의 인코딩 \\(\\mathbf{F}^{c}\\)로서 취한다. 그런 다음 이러한 특징을 [26, 34]와 유사한 교차 의도 계층을 통해 UNet \\(\\epsilon_{\\theta}\\)에 주입한다. 우리는 이 방법이 교차 의도 또는 다른 입력 특징과의 연결에 VAE 특징을 사용하는 것과 같은 다른 대안들에 비해 더 나은 시간적 일관성을 달성한다는 것을 경험적으로 발견했다.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      '모델 교육은 단순화된 확산 목표에 의해 감독되며, 즉 추가된 노이즈(20)를 예측한다.\n' +
      '\n' +
      'cal{L} (\\mathbf{F}{\\tau}^{2}\\tag{3}^mathbf{c})\n' +
      '\n' +
      '그 가운데 \\(\\epsilon\\)가 지반진소음이라고 덧붙였다.\n' +
      '\n' +
      '**Dataset****입니다. 우리는 HiC+라고 하는 [26]에 의해 수집 및 처리되는 대규모 데이터 세트를 훈련한다. 생성된 데이터 세트는 인간-스크렌 상호작용의 2.4M 비디오를 포함한다. 또한 전경 분할과 마스크를 제공합니다. 자세한 내용은 독자를 원본 종이에 참조합니다.\n' +
      '\n' +
      '** 사전 훈련된 가중치***입니다. 텍스트 조건 이미지 인포팅 작업에서 원래 스테이블 디퓨전 위에 미세 조정되는 스테이블 디퓨전 이미지 인포팅 모델[34]에서 미리 학습된 가중치로 데노징 네트워크 \\(\\epsilon_{\\theta}\\)의 가중치를 초기화한다. 삽입된 모션 모듈의 가중치를 아나펜디프 v2\\({}^{*}\\)로 초기화한다.\n' +
      '\n' +
      'CLIP 이미지 인코더의 경우, 마지막 히든 레이어의 특징이 1024의 차원을 갖는 OpenAI가 제공하는 "클립-빗-대형-패치14" 변이체+를 사용하고, 미리 학습된 U-Net은 텍스트 특징 공간에도 있는 조건으로서 치수 768의 특징을 취한다. 이를 설명하기 위해, 우리는 기능을 원하는 공간에 투영하기 위해 추가 2층 MLP를 훈련시킨다.\n' +
      '\n' +
      '부시 †: [https://github.com/guoywu/animatediff/] (https://github.com/guoywu/animatediff/) (https://github.com/guoywu/animatediff/)\n' +
      '\n' +
      '훈련 중 공유 VAE와 CLIP 인코더를 동결하고 모션 모듈로 U-Net을 미세 조정합니다.\n' +
      '\n' +
      '** 데이터 처리 및 증강***입니다. 영상에서 완벽한 분할 마스크를 얻는 것은 어려운 일입니다. 마스크는 불완전하거나 전경의 일부 부분을 누락하거나 경계 근처의 유출된 배경을 포함하도록 과다할 수 있다. 불완전 분할을 처리하기 위해 훈련 중 임의 직사각형 컷아웃을 전경 분할 및 마스크에 적용한다. 과도한 세분화의 정보 유출을 줄이기 위해 훈련과 추론 시 모두 크기 5 \\(\\touthern\\) 5의 균일한 커널로 이미지 침식을 분할과 마스크로 수행한다.\n' +
      '\n' +
      '** 무작위 상태는*** 감소한다. 시험 시간에 분류기가 없는 지침을 가능하게 하기 위해 훈련 중 각각 10% 확률로 세분화와 마스크, 조건 프레임 또는 전부를 무작위로 떨어뜨린다. 이 경우 우리는 각각의 인코더에 들어가기 전에 그것을 제로 설정했다.\n' +
      '\n' +
      '** 기타 세부 정보***입니다. 우리는 3e-5의 일정한 학습률로 AdamW[27] 최적화기를 사용하는데, 4의 배치 크기로 8 NVIDIA A100-80GB GPU에서 모델을 훈련시켜 완전히 수렴하는 데 약 일주일 정도 걸린다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '우리는 평가에 사용된 데이터를 설명하는 것으로 시작한다. 그런 다음 우리는 고정 프레임과 배경 전용 프레임을 컨디셔닝으로 사용하여 Sec 4.1에서 방법에서 생성된 다양한 샘플을 보여준다. Sec. 4.2에서 우리는 다양한 기저부와 비교된다. 우리는 Sec. 4.3에서 추가 결과와 분석을 제공하고 있으며, 특히 훈련된 한 번 모델에서 특정 일반적인 비디오 실화 능력이 나타남을 보여준다. 우리는 또한 우리의 모델이 테스트 시간에 부정확한 전경 분할에 강하다는 것을 보여준다. 마지막으로 모델 런타임에 대해 분석합니다.\n' +
      '\n' +
      '이전 작품[5, 10, 13, 16, 41]에 이어 DAVIS [30] 데이터세트로부터 샘플링된 비디오에 대한 이전 작품과 비교한다. 우리는 인간과 비인간적인 과목으로 영상을 선택합니다. 또한 HiC+ 데이터 세트의 홀드 아웃 샘플에 대해 평가한다. 우리의 방법을 사용한 샘플은 50개의 변성 단계로 생성되며 유도 척도[34]는 5이다.\n' +
      '\n' +
      '법.\n' +
      '\n' +
      '그림에서. 3, 우리는 침입 프레임 또는 배경 전용 프레임을 조건으로 사용하여 HiC+ 데이터세트로부터의 유지-아웃 분할 서열에 대한 결과를 보여준다. 어떻게든 거친 수준과 미세한 수준 모두에서 매우 현실적인 전경-배경 상호작용을 생성한다. 우리의 모델은 거친 수준에서 도로 구조, 호박장, 도시 견해, 파도 등을 합성한다. 피험자의 동작과 일치합니다. 미세한 수준에서 우리의 방법은 또한 버킷, 침대 시트, 말 및 사구 벌레와 같은 주제와 밀접한 상호 작용에 있는 작은 이동 객체를 생성한다. 더욱이, 이러한 세대는 프레임 전체에 걸쳐 일관되게 유지되며 조건 프레임의 지침을 단단히 따른다. 합성된 배경은 또한 일관된 척도, 번개 및 그림자(그림 1)를 나타낸다.\n' +
      '\n' +
      '바젤린과의 비교.\n' +
      '\n' +
      '** 기본*****입니다. 우리는 먼저 새로운 문제를 연구하기 때문에 지식 중 최고로 동일한 설정하에 작동하는 사전 작업이 없음을 명확히 한다. 따라서 우리는 가장 가까운 작품과 비교하고 _i.e_를 적응한다. 필요한 경우 αimateDiff[17] 그럼에도 불구하고, 우리는 제형과 파이프라인이 이 작업의 핵심 기여라는 것을 강조한다.\n' +
      '\n' +
      '우리는 여러 기저부에 해당하는 법을 비교하는데, 우리는 그들이 하는지 여부를 기준으로 분류한다(그림 4 탑). 혹은 그렇지 않다(그림 4 아래). 비디오를 입력으로 하세요. 비디오를 입력으로 하는 방법에 대해, Gen1[13]은 추가적인 이미지를 조건으로 취하고, 또한 미리 학습된 깊이-검증 네트워크[33]를 인용한다. 사전 훈련된 신경 아타아제[23]를 감안할 때, Text2LIVE[5]는 편집된 비디오를 합성하는 조건으로 텍스트 프롬프트를 가정한다. TokenFlow [16]도 그림 3: **의 방법**의 추가 결과를 보여준다. 상단부는 인파인팅된 프레임을 조건으로 사용하는 예를 나타내는 반면, 하단에는 배경 전용 컨디셔닝이 있는 예들이 포함되어 있다. 산림 서열은 HiC+의 유지-아웃 세트의 서열이다.\n' +
      '\n' +
      '텍스트 컨디셔닝을 제공합니다. 제어-A-비디오[10]는 먼저 입력 영상으로부터 캐니 에지를 추출한 후, 에지 및 텍스트에서 공동으로 조건화된 출력 비디오를 합성한다.\n' +
      '\n' +
      '영상을 입력하지 않은 바젤의 경우, 우리는 공개 풀 요청(2)에 의해 공헌된 전략을 사용하여 크리스티-디프[17]가 추가 이미지 컨디셔닝을 취한다. 구체적으로,\n' +
      '\n' +
      '그림 4: ** 기저부와 비교** DAVIS [30] 데이터세트로부터 샘플링된 두 개의 비디오에 대한 결과를 제공한다. 각 예에 대해 3개의 대표적인 프레임(톱)과 해당 조건 신호(좌표)를 보여준다. 다른 방법은 Sec 4.2에 명시된 바와 같이 상이한 입력, 컨디셔닝 또는 미리 학습된 모델을 가정한다.\n' +
      '\n' +
      '테스트 시간, 잠복 특징은 먼저 미리 학습된 SD VAE 인코더[34]로 조건 이미지에서 추출한 다음 선형 블렌딩을 통해 프레임당 가우시안 노이즈와 병합된다. 확산 과정도 나중에 텍스트 프롬프트에서 조건화된다. 비디오 오디오1[9]은 텍스트 대 비디오와 이미지 대 비디오 모델을 모두 제공합니다. 우리는 더 가까운 비교 설정을 위해 후자를 사용한다.\n' +
      '\n' +
      'DAVIS [30] 데이터 세트의 두 가지 예에 대한 정성 비교는 그림 4에 나와 있으며, 이 방법은 매우 현실적인 세부 사항인 _e.g_로 전경 운동을 따르는 시간적으로 일관된 비디오를 생성한다. 자동차 윈드실드에 눈과 눈이 내리는 한편, 상태 프레임이 주는 안내와 제약을 엄격하게 따르고 있다. 제1 카테고리의 기준 방법은 일반적으로 입력 비디오, _e.g_에 존재하는 구조를 상속한다. 길거리 방향, 말 등이 있고, 따라서 미세한 편집이 필요할 때 완전히 실패한다. 두 번째 경우 오토바이에 손색이 없습니다. 제2 카테고리의 방법은 안내 부족(제2 예시의 비디오 큐래더1)으로 인해 미제약된 움직임을 생성하며, 이는 보충 비디오에서 더 뚜렷하게 나타난다.\n' +
      '\n' +
      '조건부 결과 및 분석\n' +
      '\n' +
      '** 일반 비디오 그림***입니다. 흥미로운 사실은 한 번 훈련된 후, 우리의 모델에서 특정 일반적인 비디오 인포팅 능력이 나타난다. 우리는 수동으로 마스크 시퀀스를 생성하여 예비 실험을 수행하고 전경 순서를 가진 것을 모델에 입력으로 통과시키며 조건 신호를 0으로 설정하여 분해하여 그림 2에 나타내었다. 우리의 모델은 명시적으로 훈련되지 않았음에도 불구하고 누락된 지역을 우회할 수 있는 5. 이는 우리의 모델이 학습 동안 떨어뜨리는 랜덤 조건으로부터 혜택을 받을 수 있는 기본 데이터 분포를 일정 정도로 근사하도록 학습한다는 것을 시사할 수 있다(Sec. 3.4). 우리는 보충제를 보여 주는 일반적인 영상 그림과 유사한 결과를 찾습니다.\n' +
      '\n' +
      '*** 부정확한 마스크***에 능숙합니다. Sec. 3.4에서 밝힌 바와 같이 실제 생성되거나 추출된 마스크는 불완전하거나 과다한 상태로 불완전한 경우가 많다. 여기서는 디자인된 절차에 훈련된 모델이 불완전한 마스크에 강하다는 것을 보여준다. 그림에서. 6, 이 두 가지 예를 소개합니다. 기타(톱)와 양발(바닥)의 넓은 영역이 누락되었음에도 불구하고, 우리의 모델은 글로벌 맥락을 고려하여 합리적인 방식으로 그들을 환각할 수 있다.\n' +
      '\n' +
      '**런타임***입니다. NVIDIA A100 GPU에서 비디오 하나를 생성하는 것은 약 8.5초가 소요되므로 전통적인 워크플로우에 비해 훨씬 빠른 아이디어 반복이 가능하다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리는 입력 전경 분할 시퀀스 및 배경을 설명하는 조건 프레임을 감안할 때 일관성을 갖고 생생한 전경-배경 상호작용으로 비디오를 생성하는 비디오 확산 기반 모델을 어떻게든 제시한다. 우리의 모델은 이동 또는 상호 작용하는 물체 및 그림자와 같은 매우 현실적인 세부 사항을 합성한다. 생성된 비디오는 또한 일관된 카메라 스케일과 조명 효과를 나타낸다. 우리는 우리의 작업이 영화와 시각적 효과 커뮤니티에 유용한 도구이며 일반 대중은 이전에 가능하지 않은 간단하고 효율적인 방식으로 다양한 장면에서 연기 주제를 현장화하는 새로운 아이디어를 실현하기 위해 기여한다고 믿는다.\n' +
      '\n' +
      '그림 5: **Zero 샷 비디오는 우리의 모델과 함께 그림입니다. 우리는 각각 4개의 샘플링된 프레임과 함께 DAVIS의 두 가지 사례를 보여준다. 노란색 영역은 마스킹된 부위에 가려져 있는 것을 나타낸다.***.\n' +
      '\n' +
      '그림 6: 우리의 방법은 부정확한 마스크에 강합니다. 우리는 HiC+의 두 가지 예를 각각 전경 분할과 다른 조건 프레임으로 생성된 두 개의 출력을 보여준다. 우리는 하나의 프레임만 보여주고 공간 제한으로 인한 조건 프레임을 보여주지 않는다. 영상에서 전체 예에 대한 보충제를 참조하십시오.\n' +
      '\n' +
      '## 6 Acknowledgment\n' +
      '\n' +
      '데이터셋 HiC+, 특히 데이터에 액세스하는 코드 및 지침에 대해 스미스 쿠르트를 작성하여 처리하는 [26]의 저자에 감사드립니다. 또한 애도비 파빌리 젠필 API 설정을 돕기 위해 애도비 리서치 자아휘(가브리엘) 황에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '위키피디아 또는 위키/위키/온셋_가상 생산][1]_가상 생산][1]_가상 생산]][https://en.wikipedia][1]_Virtual Production_]. (https://enwikipedia.wikipedia.org/wiki/On-set_virtual_생산)\n' +
      '* [2] Adobe. _Firefly_. 2023. [https://www.adobe.com/sensei/generative-ai/firefly.html](https://www.adobe.com/sensei/generative-ai/firefly.html).\n' +
      '* [3] Adobe. _Photoshop, version_. 2023. [https://www.adobe.com/products/photoshop.html](https://www.adobe.com/products/photoshop.html).\n' +
      '* [4] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In _ICCV_, 2021.\n' +
      '* [5] Omer Bar-Tal, Dolev Orfi-Amar, Rafiali Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In _ECCV_, 2022.\n' +
      '* [6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In _CVPR_, 2023.\n' +
      '* [7] Tim Brooks and Alexei A Efros. Hallucinating pose-compatible scenes. In _ECCV_, 2022.\n' +
      '* [8] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra. Pix2video: Video editing using image diffusion. In _ICCV_, 2023.\n' +
      '* [9] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. _arXiv preprint arXiv:2310.19512_, 2023.\n' +
      '* [10] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xueteng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. _arXiv preprint arXiv:2305.13840_, 2023.\n' +
      '* [11] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets. _arXiv preprint arXiv:1907.06571_, 2019.\n' +
      '* [12] Ali Diba, Mohsen Fayyaz, Vivek Sharma, Manohar Paluri, Jurgen Gall, Rainer Stiefelhagen, and Luc Van Gool. Large scale holistic video understanding. In _ECCV_, 2020.\n' +
      '* [13] Patrick Esser, Johnathan Chiu, Parmida Atighechchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In _ICCV_, 2023.\n' +
      '* [14] Fanda Fan, Chaoxu Guo, Litong Gong, Biao Wang, Tiezheng Ge, Yuning Jiang, Chunjie Luo, and Jianfeng Zhan. Hierarchical masked 3d diffusion model for video outpainting. In _ACM MM_, 2023.\n' +
      '* [15] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer. In _ECCV_, 2022.\n' +
      '* [16] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. _arXiv preprint arXiv:2307.10373_, 2023.\n' +
      '* [17] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. _arXiv preprint arXiv:2307.04725_, 2023.\n' +
      '* [18] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _ICCV_, 2017.\n' +
      '* [19] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. _arXiv preprint arXiv:2211.13221_, 2023.\n' +
      '* [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _NeurIPS_, 2020.\n' +
      '* [21] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '* [22] Zhihao Hu and Dong Xu. Videocontrolnet: A motion-guided video-to-video translation framework by using diffusion model with controlnet. _arXiv preprint arXiv:2307.14073_, 2023.\n' +
      '* [23] Yoni Kasten, Dolev Orfi, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. _ACM TOG_, 40(6), 2021.\n' +
      '* [24] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. _arXiv preprint arXiv:2303.13439_, 2023.\n' +
      '* [25] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.\n' +
      '* [26] Sumith Kulast, Tim Brooks, Alex Aiken, Jiajun Wu, Jimei Yang, Jingwan Lu, Alexei A Efros, and Krishna Kumar Singh. Putting people in their place: Affordance-aware human insertion into scenes. In _CVPR_, 2023.\n' +
      '* [27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* [28] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. _IEEE TPAMI_, 42(2), 2019.\n' +
      '* [29] OpenAI. _ChatGPT_. 2023. [https://chat.openai.com/](https://chat.openai.com/).\n' +
      '* [30] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. _arXiv preprint arXiv:1704.00675_, 2017.\n' +
      '* [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '\n' +
      '* [32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.\n' +
      '* [33] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. _IEEE TPAMI_, 44(3), 2020.\n' +
      '* [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.\n' +
      '* [35] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _MICCAI_, 2015.\n' +
      '* [36] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In _ACM SIGGRAPH_, 2022.\n' +
      '* [37] Gunnar A Sigurdsson, Gul Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In _ECCV_, 2016.\n' +
      '* [38] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In _CVPR_, 2018.\n' +
      '* [39] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. _arXiv preprint arXiv:2210.02399_, 2022.\n' +
      '* [40] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In _NeurIPS_, 2016.\n' +
      '* [41] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In _ICCV_, 2023.\n' +
      '* [42] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape guided object inpainting with diffusion model. In _CVPR_, 2023.\n' +
      '* [43] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In _CVPR_, 2023.\n' +
      '* [44] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In _CVPR_, 2023.\n' +
      '* [45] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In _CVPR_, 2023.\n' +
      '* [46] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _CVPR_, 2023.\n' +
      '* [47] Shangchen Zhou, Chongyi Li, Kelvin CK Chan, and Chen Change Loy. Propainter: Improving propagation and transformer for video inpainting. In _ICCV_, 2023.\n' +
      '\n' +
      '이 보충제에서 우리는 먼저 일반 비디오 도장 적용의 더 많은 예를 제공하고 본고의 Sec. 4.3에 따라 부록 A의 부정확한 마스크에 우리의 모델이 강력하다는 것을 보여준다. 그런 다음 부록 B에서 훈련 및 평가 데이터에 대한 필수 처리 단계를 설명하고 실패 사례를 보여주고 부록 C. 마지막에서 모델의 한계를 논의하고 부록 D에서 이 작업의 윤리적 영향을 논의함으로써 결론을 내렸다.\n' +
      '\n' +
      '다양한 생성 콘텐츠와 카메라 동작을 가진 비디오 배경 생성 및 다양한 컨디셔닝 시나리오에서 광범위한 비디오 결과를 보여주는 프로젝트 웹페이지를 판독기가 확인하도록 강력하게 권장합니다. 또한 기저부와 비교한 비디오 버전이 포함되어 있습니다.\n' +
      '\n' +
      '본 논문의 Sec. 4.3에 대한 본지 중 Sec. 4.3에 대한 본지## 부록 A 풀예 중 Sec. 4.3에 대한### 부록 A 풀예.\n' +
      '\n' +
      '** 일반 비디오 아웃그림***입니다. 본고의 Sec. 4.3에 명시된 바와 같이, 한 번 훈련된 우리 모델에서 일반적인 비디오 인포팅/외화에 대한 예비 능력이 나타난다. 그림에서. 주요 논문의 5개, 우리는 일반적인 비디오 그림에 대한 결과를 보여준다. 여기에서 그림 7의 일반적인 비디오 실화에 동일한 모델을 적용하기 위한 결과를 제시한다. 우리는 프레임의 원래 서열을 0.75로 재변환하고 회색 경계로 패드를 한다. 누락된 지역을 나타내기 위해 관련 마스크도 생성됩니다. 그런 다음 무작위로 하나의 프레임을 선택하고 아도베 파빌리[2]를 사용하여 이를 능가하고 전체 서열을 능가하는 조건으로 사용한다. 일반적인 비디오 그림과 함께 그림 1에 나와 있다. 주요 논문의 5개, 이는 우리의 모델이 기초 데이터 분포를 일정 정도로 근사화하는 것을 학습한다는 것을 시사한다.\n' +
      '\n' +
      '*** 부정확한 마스크***에 능숙합니다. 우리는 그림 1의 전체 버전을 보여준다. 그림 8의 본고본 6편의 우리 모델은 마스크가 부정확하여 분할에서 누락된 기타와 남성의 손에 대한 사실적인 세부 사항을 채운다.\n' +
      '\n' +
      '자료 처리\n' +
      '\n' +
      '** 훈련****입니다. 주요 원고 중 Sec. 3.4에서 데이터 처리 및 훈련 데이터에 대한 증강 전략을 설명했다. 구체적으로 불완전 분할을 처리하기 위해 세분화와 마스크에 무작위 직사각형 컷아웃을 적용한다. 우리는 그림 9에서 두 가지 예를 보여준다.\n' +
      '\n' +
      '*** 평가***입니다. Sec에서 언급한 바와 같이. 본고의 1.1을 검정시점에서는 사진 편집 도구(_e.g_. Adobe Photoshop[3])와 같은 다양한 방법으로 조건으로 사용되는 합성 전경-배경 프레임을 생성할 수 있다. 자동화된 이미지 아웃그림 방법(_e.g_ Dall-E[32])이다. 우리의 실험에서 우리는 시험 시간에 사용하기 위해 이러한 합성 프레임을 자동으로 합성하기 위해 ChatGPT 4 [29]와 아도베 파빌리[2]를 채택한다. 구체적으로, 이 세 개의 이미지에 기초하여, 먼저 입력 비디오에서 0번째, 8번째, 15번째 프레임으로 설명해주세요."라는 질문에 ChatGPT에게 "먼저 0번째, 8번째, 15번째 프레임으로 설명해주세요.\n' +
      '\n' +
      '그림 7: **Zero 샷 비디오는 우리의 모델과 함께 그림입니다. 우리는 DAVIS 데이터세트***의 두 가지 예를 보여준다.\n' +
      '\n' +
      '그런 다음 포그라운드 분할과 텍스트 프롬프트를 고려할 때 10단어 미만의 파충류를 합성하기 위해 파이로이를 사용하는데, 우리는 이 것을 전경 분할과 텍스트 프롬프트의 \'gpt4-vision-preview\' 버전인 ChatGPT 4 4와 공식 파이어플라이 GenFill 5를 사용할 수 있는가?\n' +
      '\n' +
      '부츠 4: [https://plnote 4][https://platform.openai.com/docs/overview] (https://plasmai.com/docs/overview) (https://plResearcher.openai.com/docs/overview)\n' +
      '\n' +
      '부타주 5: [https://firefly.com/generate/inpaint] (https://firefire264adobe.com/genob/inpaint/inpaint)\n' +
      '\n' +
      '부록.\n' +
      '\n' +
      '우리는 그림 1에서 우리의 방법의 두 가지 실패 사례를 보여준다. 10. 첫 번째 예에서 드레스의 잔디 같은 질감은 분할에서 제외되므로 모델은 드레스를 벗어나 자라는 독립적인 객체로 잘못 인식한다. 두 번째 예에서, 반딧불이 난 프레임은 빗자루를 잘못된 방향으로 향하고 있다. 모델이 합리적인 것을 생성하려고 하지만, 이러한 실수를 수정하지 못해 일관된 비디오를 생산하지 못한다. 우리의 모델에서 특정 고장 내성에도 불구하고 적절한 입력 분할 및 조건 신호를 제공하는 것은 고품질 생성 결과를 보장하는 데 도움이 됩니다.\n' +
      '\n' +
      '부록\n' +
      '\n' +
      'HiC+ 데이터셋[26]에는 공공 데이터셋 HVU[12], 찰데스[37], 모더[28], WebVid10M[4]가 포함되어 있다. 독자는 독자에게 숙달의 A.1을 참조하도록 권장된다.\n' +
      '\n' +
      '그림 8: 우리 모델은 부정확한 마스크에 강합니다. 여기에서 그림 1의 전체 버전을 보여준다. 본고의 6.\n' +
      '\n' +
      '그림 10: ** 실패 사례**. DAVIS의 산림 서열.\n' +
      '\n' +
      '그림 9: ** 데이터 증강***입니다. 우리는 훈련 중 사람 분할에 무작위 컷아웃을 적용한다. 여기에서 우리는 해당 원래 프레임과 함께 컷아웃의 두 가지 예를 보여준다.\n' +
      '\n' +
      '이러한 데이터 세트의 라이선스에 대한 [7]의 톤 재료입니다. 더욱이 [26]과 마찬가지로 훈련 및 평가 시 더 많은 내부 영상이 통합된다. 필요한 법률 검토 절차를 수행했으며 요청 시 이 데이터에 대한 자세한 정보를 제공할 수 있습니다.\n' +
      '\n' +
      '인간과 비인간 과목을 다양한 배경 장면으로 배치하는 매우 현실적인 영상을 합성할 수 있는 방법을 제시한다. 우리의 작업은 광범위한 분야에 대한 시사점과 가치를 가지고 있지만, 우리의 모델은 악성 콘텐츠를 생성하기 위해 오용될 수 있다는 점에 주목한다. 파이어플라이 GenFill[2] 및 Gen1[13]과 유사하게 생성된 함량을 워터마크를 하여 오용을 방지할 수 있다. 더욱이, 우리의 모델은 학습 데이터에 제시된 인구 통계학적 편향을 계승한다. 우리는 표시된 모든 결과에서 공평성을 입증하기 위해 최선을 다합니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>