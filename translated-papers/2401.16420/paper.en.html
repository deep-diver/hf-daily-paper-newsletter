<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '_XComposer2 model series with 7B parameters are publicly available at_[https://github.com/InternLM/InternLM-XComposer](https://github.com/InternLM/InternLM-XComposer).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'In recent years, there has been a remarkable evolution in the field of large language models (LLMs) [8, 16, 17, 57, 58, 68, 79]. Foremost among these, models like Chat-GPT [57] have completely altered human interaction with technology. Concurrently, a variety of open-source LLMs, such as Llama [78], Mistra [37], InternLM [77], QWen [65], GLM [25], and Baichuan [7], have empowered the customization of LLMs. Building on these open-source foundations, the community has seen substantial progress in multimodal large language models (MLLMs) [6, 21, 29, 48, 49, 82, 95, 100]. These MLLMs are adept at interpreting images and engaging in text-image dialogues, showcasing impressive multimodal understanding. Unlike traditional MLLMs, a recent innovation, _i.e_., InternLM-XComposer [95], has focused on using MLLMs for text-image composition and comprehension, marking a novel direction in MLLM research. However, this pioneering work is currently limited to generating text-image articles based on titles alone, lacking the sophistication to meet more complex composition requirements. Furthermore, while achieving leading performance at its inception, this model still possesses significant potential for enhancement in detailed perception and complex reasoning capabilities to advance its vision-language comprehension performance.\n' +
      '\n' +
      'This observation motivates the development of more advanced vision-language models capable of practical and potent text-image composition and comprehension. In this paper, we introduce InternLM-XComposer2, a cutting-edge model excelling in free-form text-image composition and comprehension, built based on InternLM2 [77]. InternLM-XComposer2 represents a significant advancement over its predecessor, InternLM-XComposer [95], in both text-image composition and comprehension. InternLM-XComposer2 is adept at producing high-quality, integrated text-image articles from a variety of free-form inputs, such as detailed specifications, structured outlines, and reference images, serving to a wide range of application contexts. In the realm of multimodal understanding, it demonstrates exceptional capabilities in detailed perception, logical reasoning, and extensive knowledge integration. Its performance significantly surpasses that of existing open-source MLLMs, and it stands on par with, or even exceeds, advanced models like GPT-4V [58] and Gemini Pro [76] in various benchmarks.\n' +
      '\n' +
      'The appealing capabilities of InternLM-XComposer2 are primarily due to two critical design elements. (1) **Partial LoRA**: The Partial LoRA (P-LoRA) design harmonizes its abilities in composition and comprehension. This involves feeding forward image tokens with additional LoRA [33] (Low-Rank Adaptation) parameters, while language tokens retain the original architecture. This selective enhancement ensures robust performance in both visual and textual domains. (2) **High-quality and Diverse Data Foundataion**: The quality and diversity of the training data are pivotal. Our dataset for free-form text-image composition excels in: adhering to complex instructions, customization with text and image for tailored content, high-quality and stylistically diverse writing, and versatile text editing including condensing, expanding, and revising. For exceptional vision-language comprehension capabilities, we gather a wide range of high-quality pretraining and supervised fine-tuning multimodal data. This collection spans various aspects and types, such as captions, general QA, scientific QA, chat-based QA, mathematical QA, concept knowledge, conversation, and text-image composition.\n' +
      '\n' +
      'InternLM-XComposer2 surpasses existing benchmarks in both composition and comprehension. In the creation benchmark of OpenCompass [18] for evaluating the creativity of LLMs, InternLM-XComposer2 showcases outstanding performance. To demostrate our multimodal comprehension capility, we compare our InternLM-XComposer2 on a list of benchmarks with both open-source MLLMs and closed-source APIs, _e.g_., GPT4V [58], Gemini Pro [76], and Qwen-VL Plus [19]. We report results in Math-Vista [52], MMMU [91], AI2D [40], MME [27], MMBench [51], MMBench-Chinese [51], SEED-Bench (Image) [41], LLaVA-Bench (In-the-Wild) [49], QBench [85], MM-Vet [90], HallusionBench [31], ChartQA [56], and POPE [45]. InternLM-XComposer2 based on InternLM2-7B significantly exceeds the performance of existing open-source models by an impressive margin. Remarkably, it demonstrates superior performance to GPT4V [58], Gemini Pro [76] across six benchmarks.\n' +
      '\n' +
      '## 2 Related Works\n' +
      '\n' +
      '**Large Language Models (LLMs).** Recent LLM architectures have marked a transition from encoder-decoder frameworks (_e.g_., BERT [22], T5 [68]) to an emphasis on decoder-only models employed with autoregressive training techniques for next-token prediction (_e.g_., GPT [67]). The following works (_e.g_., GPT3 [8], InstructGPT [60], ChatGPT [57], PaLM [17]) have seen the integration of advanced techniques such as instruction-tuning and Reinforcement Learning from Human Feedback (RLHF). Coupled with expansive parameter sizes and extensive training data, these LLM models have achieved substantial performance enhancements across a diverse range of Natural Language Processing (NLP) tasks. Other notable LLMs encompass a range of developments, such as the OPT [96], LLaMA series [78, 79], _e.g_., Mistral [37, 38], InternLM [77], GLM series [25, 93], Qwen series [6, 65], Baichuan [7], Skywork [84] and Falcon [61] have made significant contributions to the field.\n' +
      '\n' +
      '**Multimodal Large Language Models (MLLMs).** Vision-language models (VLMs), exemplified by CLIP [66] and its subsequent works [26, 36, 43, 44, 50, 75, 94], align image and text features in a unified embedding space. This alignment is achieved through contrastive learning objectives applied to extensive image-text pair datasets. VLMs achieve strong zero-shot and few-shot performance, showcasing significant generalization abilities across a range of downstream tasks.\n' +
      '\n' +
      'Benefiting from existing large language models and VLMs as the visual encoder, recent Multimodal Large Language Models (MLLMs) [12, 14, 15, 24, 28, 58] achieve visual perception, understanding and reasoning abilities, show superb performance in diverse vision-language tasks. A series of studies [2, 5, 9, 10, 20, 21, 42, 46, 49, 62, 64, 80, 86, 92, 97, 98, 100] have explored further improve the MLLM in different dimensions, such as instruction tuning [11, 49, 98], efficient fine-tuning [33], high-resolution image inputs [6, 82, 83], hallucination mitigation [34, 87, 99], image generation [23, 30, 74, 89], 3D understanding [63] and image-text comprehension and composition [95].\n' +
      '\n' +
      'To enable highly customizable content creation, our model is designed for free-form text-image composition and comprehension based on MLLMs. We use Intern-LM2 as the LLM and CLIP ViT-Large as the visual encoder and propose a new partial LoRA to align the text-image modalities. Given flexible and multi-modal user inputs such as specifications, outlines, and reference images, our model is capable of generating high-quality interleaved text-image written content.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      'Our proposed model, InternLM-XComposer2, incorporates a vision encoder and a Language Learning Model (LLM). These two components are interconnected via an innovative Partial LoRA module. Given a set of images and text, the LLM utilizes the output from the vision encoder as visual tokens and the tokenized text as language tokens. These tokens are then concatenated to form the input sequence.\n' +
      '\n' +
      '**Vision Encoder.** The vision encoder in our model is designed to extract high-level visual features from raw images. It is pretrained in an image-language contrastive manner(CLIP). Our findings indicate that, when used in conjunction with our Partial LoRA module, a lightweight vision model performs effectively. For the sake of efficiency, we have opted to use the OpenAI ViT-Large model.\n' +
      '\n' +
      '**Large Language Model.** We employ the recently introduced InternLM-2 as our Large Language Model (LLM). This model boasts exceptional multi-lingual capabilities and has demonstrated impressive results in benchmarks. In practical applications, we utilize the InternLM2-7B-Chat-SFT variant as our LLM.\n' +
      '\n' +
      '**Partial Low-Rank Adaptation.** In the realm of multi-modal Language Learning Models (LLMs), one insufficiently explored area is the effective alignment of different modalities. A desired alignment should potentially enrich the LLM with new modality-specific knowledge, while simultaneously preserving its inherent capabilities. Current methods predominantly adopt one of two approaches: they either treat the visual token and language token equally or as entirely distinct entities. We contend that the first approach overlooks the inherent property distinctions between modalities, while the second approach results in a substantial alignment cost.\n' +
      '\n' +
      'In our pursuit of effective modality alignment, we introduce Partial LoRA, a versatile plug-in module designed to align knowledge from a new modality to the LLM. As illustrated in Figure X, Partial LoRA draws inspiration from the original LoRA and incorporates a low-rank adaptation that is exclusively applied to the new modality portion of the input tokens. In our specific configuration, Partial LoRA is applied to all visual tokens.\n' +
      '\n' +
      'Formally, for each linear layer \\(L_{0}\\) in the LLM blocks, we denote its weight matrix \\(W_{0}\\in\\mathbb{R}^{(C_{out}\\times C_{in})}\\) and bias \\(B_{0}\\in\\mathbb{R}^{C_{out}}\\), where \\(C_{in}\\) and \\(C_{out}\\) are the input and output dimension. Its corresponding Partial LoRA contains two low-rank matrix \\(W_{A}\\in\\mathbb{R}^{C_{r}\\times C_{in}}\\) and \\(W_{B}\\in\\mathbb{R}^{C_{out}\\times C_{r}}\\). With a given input \\(x=[x_{v},x_{t}]\\), we have the output feature \\(\\hat{x}\\) by:\n' +
      '\n' +
      '\\[\\hat{x}_{t} =W_{0}x_{t}+B_{0}\\] \\[\\hat{x}_{v} =W_{0}x_{v}+W_{B}W_{A}x_{v}+B_{0}\\] \\[\\hat{x}=[\\hat{x}_{v},\\hat{x}_{t}]\\]\n' +
      '\n' +
      'Figure 2: **The illustration of the Partial-LoRA.** The blue tokens represent the visual tokens and the gray tokens are the language tokens. Our Partial-LoRA is only applied to the visual tokens.\n' +
      '\n' +
      'where \\(x_{v}\\) and \\(x_{t}\\) are the visual tokens and language tokens of the input sequence respectively.\n' +
      '\n' +
      '### Pre-Training\n' +
      '\n' +
      'During the pre-training phase, the LLM remains constant while both the vision encoder and Partial LoRA are fine-tuned to align the visual tokens with the LLM. The pre-training data is meticulously curated with **three objectives** in mind: 1) general semantic alignment, 2) world knowledge alignment, 3) vision capability enhancement.\n' +
      '\n' +
      '**General Semantic Alignment.** The objective of general semantic alignment is to equip the MLLM with the fundamental ability to comprehend image content. For instance, the MLLM should be able to recognize that a picture of Einstein represents \'a human\'. We utilize image caption data from a variety of sources for this purpose, including high-quality, detailed captions from ShareGPT4V-PT, as well as concise and precise captions from COCO, NoCaps, TextCaps, _etc_. During the pre-training phase, we employ a simple instruction: _Describe this image briefly/in detail_.\n' +
      '\n' +
      '**World Knowledge Alignment.** World knowledge represents an advanced capability of the MLLM. For instance, the MLLM should be able to identify the man in the figure mentioned above as \'Albert Einstein\' and further talk something about him. To align the world knowledge depicted in the image with the knowledge already acquired by the LLM, we have constructed a concept dataset. This dataset is carefully filtered from the concept data utilized in IntermLMComposer [95]. Given that the text in the concept data only partially describes the content in the image and their relationship is complex to model, we employ a more broad instruction: _Tell me something about this image_.\n' +
      '\n' +
      '**Vision Capability Enhancement.** Finally, an advanced MLLM necessitates certain vision-specific capabilities, such as Optical Character Recognition (OCR), object localization (grounding), and the understanding of structured images (_e.g_., charts, tables). To achieve this, we have compiled relevant datasets, as outlined in Table.1, and have implemented corresponding instructions for training.\n' +
      '\n' +
      'Thanks to the design of Partial LoRA, the LLM is able to adapt to visual tokens while maintaining its original language processing capabilities. The fixed LLM also enables us to directly use in-context learning performance as a measure of pre-training quality.\n' +
      '\n' +
      'In our implementation, we employ the OpenAI CLIP ViT-L-14-336 as the vision encoder. We increase its resolution to \\(490\\times 490\\) for improved performance. For the Partial LoRA, we set a rank of \\(256\\) for all the linear layers in the LLM decoder block. Our training process involves a batch size of 4906 and spans across \\(2\\) epochs. The learning rate is initially set to increase to \\(2\\times 10^{-4}\\) within the first \\(1\\%\\) of the training steps. Following this, it decreases to \\(0\\) according to a cosine decay strategy. To preserve the pre-existing knowledge of the vision encoder, we apply a layer-wise learning rate (LLDR) decay strategy and the decay factor is set to \\(0.90\\).\n' +
      '\n' +
      '### Supervised Fine-tuning\n' +
      '\n' +
      'The pre-training phase aligns the visual feature with the language, enabling the Language Learning Model (LLM) to comprehend the content of the images. However, it still lacks the ability to effectively utilize the image information. To overcome this limitation, we introduce a range of vision-language tasks that the model engages in during the subsequent Supervised Fine-Tuning Stage. This stage comprises two sequential steps: Multi-task Training and Free-form Text-Image Composition. During this stage, we jointly fine-tune the vision encoder, LLM, and Partial LoRA.\n' +
      '\n' +
      '**Multi-task Training**. As delineated in Table 2, the multi-task training dataset is assembled from various sources, aiming to equip the model with a broad spectrum of capabilities. Each task is structured as a conversational interaction, and the instructions are augmented with GPT-4 to enhance diversity. Concurrently, to maintain the original language capability, we also incorporate the supervised fine-tuning\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline Task & Dataset \\\\ \\hline General Semantic Alignment & ShareGPT4V-PT [11], COCO [13], Necaps [1], TextCaps [73], LAION400M [69], SBU [59], CC 3M [72] \\\\ World Knowledge Alignment & Concept Data [95] \\\\ Vision Capability Enhancement & WanJuan [32], Flicker[88], MMC-Instruction[47] \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Datasets used for Pre-Training. The data are collected from diverse sources for the three objectives.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Task & Dataset \\\\ \\hline _Multi-task training_ & \\\\ Caption & ShareGPT4V [11], COCO [13],Nocaps [1] \\\\ General QA & VQAv2 [4], GQA [35], OK-VQA [55] \\\\ Science QA & AI2D [40], SQA [54] \\\\ Chart QA & DVQA [39], ChartQA [56] \\\\ Math QA & MathQA [3], Geometry3K[53] \\\\ World Knowledge QA & A-OKVQA [70], kVQA [71] \\\\ Conversation & LLaVA-150k [49], LVIS-Instruct4V [81] \\\\ \\hline _Instruction tuning_ & \\\\ Free-from Compositio & In-house data (Refer to Sec.3.4) \\\\ Conversation & LLaVA-150k [49], LVIS-Instruct4V [81] \\\\  & ShareGPT-en\\&zh [16], InternLM-Chat[77] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Datasets used for Supervised Fine-Tuning. We collect data from diverse sources to empower the model with different capabilities.\n' +
      '\n' +
      'data from InternLM2, which constitutes a fixed 10% of the total Supervised Fine-Tuning (SFT) data.\n' +
      '\n' +
      '**Free-form Text-Image Composition**. To further enhance the model\'s ability to follow instructions and compose free-form image-text content, we employ data from both pure-text conversation corpora and vision-language conversations, as outlined in Table 2. The dataset for free-form image-text composition is constructed following the methodology detailed in Section 3.4.\n' +
      '\n' +
      'In our approach, we jointly train all the components with a batch size of 2048 over 3000 steps. Data from multiple sources are sampled in a weighted manner, with the weights based on the number of data from each source. The maximum learning rate is set to \\(5\\times 10^{-5}\\), and each component has its own unique learning strategy. For the vision encoder, we set the Layer-wise Learning Rate Decay (LLDR) to \\(0.9\\), which aligns with the pretraining strategy. For the LLM, we employ a fixed learning rate scale factor of \\(0.2\\). This slows down the update of the LLM, achieving a balance between preserving its original capabilities and aligning it with vision knowledge.\n' +
      '\n' +
      '### Free-form Text-Image Composition\n' +
      '\n' +
      'Free-form text-image composition refers to the combination of textual content and visual elements in a flexible and unrestrictive manner. Our model generates interleaved text and images, specifically customized to align with the text requirements provided by users, which may include elements such as a title, outline, and writing material, and optionally, any visual requirements like image resources.\n' +
      '\n' +
      'To facilitate free-form text-image composition, we collect a wide range of high-quality and diverse in-house data across four key dimensions. These dimensions encompass: **Varied Writing Styles.** Our data spans a multitude of writing styles, from academic papers to social media posts and poems, ensuring a rich and diverse collection of text and image contents.\n' +
      '\n' +
      '**Flexible Text Editing.** Our dataset includes extensive examples of text editing, encompassing a wide spectrum of modifications such as shortening, expanding, and rewriting.\n' +
      '\n' +
      '**Complex Instruction Adherence.** We also capture instances of adhering to complex instructions to create content that caters to diverse demands like titles and outlines, encompassing both text and image-based compositions.\n' +
      '\n' +
      '**Customization with Materials.** Our collection extends to materials used for personalized content creation, covering both text and images, enabling customizable and unique content creation experiences.\n' +
      '\n' +
      'The distribution of data across the four dimensions is approximately equal, with a ratio of approximately 1:1:1:1. Our method follows previous work [95] to identify suitable positions for image insertion after generating the text content. A notable distinction in our approach is that when users provide their own image materials, these image materials are used for insertion instead of relying on retrieved images [95]. We also observe that having a high-resolution image input is not essential for text-image composition. Therefore, following the pre-training phase, we opt to down-sample the image input resolution to 224x224 during the SFT stage of free-form text-image composition.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'In this section, we validate the benchmark performance of our InternLM-XComposer2 after the supervised fine-tuning.\n' +
      '\n' +
      '### MLLM Benchmark results.\n' +
      '\n' +
      'In Table.3 and Table.4, we compare our InternLM-XComposer2 on a list of benchmarks with both SOTA open-source MLLMs and closed-source APIs. Here we report results in MathVista[52], MMMU[91], AI2D[40], MME Perception (MME\\({}^{P}\\)) [27], MME Cognition (MME\\({}^{C}\\))[27], MMBench (MMB) [51], MMBenchChinese (MMB\\({}^{CN}\\)) [51], SEED-Bench Image Part (SEED\\({}^{I}\\))[41], LLaVA-Bench In-the-Wild (LLaVA\\({}^{W}\\)) [49], QBench-Testset (QBench\\({}^{T}\\))[85], MM-Vet [90], HallusionBench (HallB)[31], ChartQA[56], POPE[45].\n' +
      '\n' +
      '**Comparison with Closed-Source APIs.** As shown in\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c c c c} \\hline Method & MathVista & A2D & MMMU & MME & MMB & MMB\\({}^{CN}\\) & SEED\\({}^{I}\\) & LLaVA\\({}^{W}\\) & QBench\\({}^{T}\\) & MM-Vet & HallB & ChartVQA \\\\ \\hline Open-Source & _SPH-MOE_ & _Monkey_ & _Yi-VL_ & _WeMM_ & _L-Int2_ & _L-Int2_ & _SPH-2_ & _CogVLM_ & _Int-XC_ & _CogVLM_ & _Monkey_ & _CogAgent_ \\\\ Previous SOTA & 8x7B & 10B & 34B & 6B & 20B & 20B & 17B & 17B & 8B & 30B & 10B & 18B \\\\  & 42.3 & 72.6 & 45.9 & 2066.6 & 75.1 & 73.7 & 74.8 & 73.9 & 64.4 & 56.8 & 58.4 & 68.4 \\\\ \\hline \\multicolumn{13}{l}{_Closed-source API_} \\\\ GPT-4V & 49.9 & 78.2 & **56.8** & 1926.5 & 77.0 & 74.4 & 69.1 & **93.1** & **74.1** & **67.7** & **65.8** & **78.5** \\\\ Gemini-Pro & 45.2 & 73.9 & 47.9 & 1933.3 & 73.6 & 74.3 & 70.7 & 79.9 & 70.6 & 64.3 & 63.9 & 74.1 \\\\ QwenVL-Plus & 43.3 & 75.9 & 46.5 & 2183.3 & 67.0 & 70.7 & 72.7 & 73.7 & 68.9 & 55.7 & 56.4 & 78.1 \\\\ \\hline Ours & **57.6** & **78.7** & 42.0 & **2242.7** & **79.6** & **77.6** & **75.9** & 81.8 & 72.5 & 51.2 & 60.3 & 72.6 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **Comparison with closed-source APIs and previous open-source SOTAs.** Our InternLM-XComposer2 gets SOTA results in 6 of the 12 benchmarks with only 7B parameters, showing competitive results with current closed-source APIs and previous open-source SOTA MLLMs. The best results are **bold** and the second-best results are underlined.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'In this paper, we present InternLM-XComposer2, which demonstrates its exceptional capabilities in the field of vision-language understanding and free-form text-image composition. Our proposed innovative Partial LoRA (PLoRA) approach, which applies additional LoRA parameters exclusively to image tokens, has proven effective in preserving the integrity of pre-trained language knowledge while striking a balance between precise vision understanding and text composition with literary talent. Our model\'s performance across various benchmarks not only significantly outperforms existing multimodal models but also matches or even surpasses GPT-4V and Gemini Pro in certain assessments, underscoring its remarkable proficiency in the realm of multimodal understanding. This research opens up new possibilities for highly customizable content creation and paves the way for future advancements in the MLLM field. The potential applications of InternLM-XComposer2 are vast and exciting, promising a future where AI can understand and generate high-quality long-text multi-modal content with ease and precision.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 8948-8957, 2019.\n' +
      '* [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022.\n' +
      '* [3] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. _arXiv preprint arXiv:1905.13319_, 2019.\n' +
      '* [4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In _International Conference on Computer Vision (ICCV)_, 2015.\n' +
      '* [5] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models. _arXiv.org_, 2023.\n' +
      '* [6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv.org_, 2023.\n' +
      '* [7] Baichuan. Baichuan 2: Open large-scale language models. _arXiv.org_, 2023.\n' +
      '* [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in Neural Information Processing Systems (NeurIPS)_, 33:1877-1901, 2020.\n' +
      '* [9] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. _arXiv preprint arXiv:2310.09478_, 2023.\n' +
      '* [10] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal lm\'s referential dialogue magic. _arXiv.org_, 2023.\n' +
      '* [11] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.\n' +
      '\n' +
      '* [12] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Mongomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali-x: On scaling up a multilingual vision and language model, 2023.\n' +
      '* [13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server, 2025.\n' +
      '* [14] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut. Pali-3 vision language models: Smaller, faster, stronger, 2023.\n' +
      '* [15] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Pugicerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language model, 2023.\n' +
      '* [16] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Linamin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impression gpt-4 with 90% + chatgrp quality, March 2023.\n' +
      '* [17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv.org_, 2022.\n' +
      '* [18] OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation models. [https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass), 2023.\n' +
      '* [19] QWen Contributors. Qwen-vl-plus. [https://huggingface.co/spaces/Qwen/Qwen-VL-Plus](https://huggingface.co/spaces/Qwen/Qwen-VL-Plus), year=2023.\n' +
      '* [20] XTuner Contributors. Xtuner: A toolkit for efficiently fine-tuning llm. [https://github.com/InternLM/xtuner](https://github.com/InternLM/xtuner), 2023.\n' +
      '* [21] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.\n' +
      '* [22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv.org_, 2018.\n' +
      '* [23] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. Dreamllm: Synergistic multimodal comprehension and creation. _arXiv preprint arXiv:2309.11499_, 2023.\n' +
      '* [24] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In _arXiv preprint arXiv:2303.03378_, 2023.\n' +
      '* [25] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335, 2022.\n' +
      '* [26] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 19358-19369, 2023.\n' +
      '* [27] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jirrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023.\n' +
      '* [28] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang, Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang Shen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Shaohui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hongsheng Li, and Xing Sun. A challenger to gpt-4v? early explorations of gemini in visual expertise. _arXiv preprint arXiv:2312.12436_, 2023.\n' +
      '* [29] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, W. Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Jiao Qiao. Llamada-adapter v2: Parameter-efficient visual instruction model. _ArXiv_, abs/2304.15010, 2023.\n' +
      '* [30] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large language model.\n' +
      '* [31] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models, 2023.\n' +
      '* [32] Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Da Lin. Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models. _ArXiv_, abs/2308.10755, 2023.\n' +
      '* [33] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022.\n' +
      '* [34] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bingu Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multimodal large language models via over-trust penalty and retrospection-allocation. _arXiv preprint arXiv:2311.17911_, 2023.\n' +
      '* [35] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [36] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _Proceedings of the International Conference on Machine learning (ICML)_, pages 4904-4916. PMLR, 2021.\n' +
      '* [37] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023.\n' +
      '* [38] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mistral of experts. _arXiv preprint arXiv:2401.04088_, 2024.\n' +
      '* [39] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5648-5656, 2018.\n' +
      '* [40] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 235-251. Springer, 2016.\n' +
      '* [41] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023.\n' +
      '* [42] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. _arXiv.org_, 2023.\n' +
      '* [43] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _Proceedings of the International Conference on Machine learning (ICML)_, pages 12888-12900. PMLR, 2022.\n' +
      '* [44] Liunian Harold Li\\({}^{*}\\), Pengchuan Zhang\\({}^{*}\\), Haotian Zhang\\({}^{*}\\), Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [45] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models, 2023.\n' +
      '* [46] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. _arXiv preprint arXiv:2311.06607_, 2023.\n' +
      '* [47] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. _arXiv preprint arXiv:2311.10774_, 2023.\n' +
      '* [48] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.\n' +
      '* [49] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv.org_, 2023.\n' +
      '* [50] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding onto: Marrying dino with grounded pre-training for open-set object detection. _arXiv.org_, 2023.\n' +
      '* [51] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? _arXiv:2307.06281_, 2023.\n' +
      '* [52] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In _International Conference on Learning Representations (ICLR)_, 2024.\n' +
      '* [53] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In _The 59th Annual Meeting of the Association for Computational Linguistics (ACL)_, 2021.\n' +
      '* [54] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. _Advances in Neural Information Processing Systems_, 35:2507-2521, 2022.\n' +
      '* [55] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In _Proceedings of the IEEE/crf conference on computer vision and pattern recognition_, pages 3195-3204, 2019.\n' +
      '* [56] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. _arXiv preprint arXiv:2203.10244_, 2022.\n' +
      '** [57] OpenAI. Chatgpt. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt), 2022.\n' +
      '* [58] OpenAI. Gpt-4 technical report, 2023.\n' +
      '* [59] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned photographs. In _Neural Information Processing Systems (NIPS)_, 2011.\n' +
      '* [60] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems (NeurIPS)_, 35:27730-27744, 2022.\n' +
      '* [61] Guilherme Penedo, Quentin Malatric, Daniel Hesslow, Ruanarda Cojocaru, Alessandro Cappelli, Hamza Alobeioli, Baptiste Panir, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.\n' +
      '* [62] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. _arXiv.org_, 2023.\n' +
      '* [63] Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, and Hengshuang Zhao. Gpt4point: A unified framework for point-language understanding and generation, 2023.\n' +
      '* [64] Zhangyang Qi, Ye Fang, Mengchen Zhang, Zeyi Sun, Tong Wu, Ziwei Liu, Dahua Lin, Jiaqi Wang, and Hengshuang Zhao. Gemini vs gpt-4v: A preliminary comparison and combination of vision-language models through qualitative cases, 2023.\n' +
      '* [65] Qwen. Introducing qwen-7b: Open foundation and human-aligned models (of the state-of-the-arts), 2023.\n' +
      '* [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _Proceedings of the International Conference on Machine learning (ICML)_, pages 8748-8763. PMLR, 2021.\n' +
      '* [67] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.\n' +
      '* [68] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research (JMLR)_, 21(1):5485-5551, 2020.\n' +
      '* [69] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Liaon-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [70] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In _European Conference on Computer Vision_, pages 146-162. Springer, 2022.\n' +
      '* [71] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. Kvqa: Knowledge-aware visual question answering. In _Proceedings of the AAAI conference on artificial intelligence_, 2019.\n' +
      '* [72] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2556-2565, 2018.\n' +
      '* [73] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pages 742-758. Springer, 2020.\n' +
      '* [74] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yuzee Wang, Hongcheng Gao, Jingjing Liu, Trejun Huang, and Xinlong Wang. Generative pretraining in multimodality. Jul 2023.\n' +
      '* [75] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-CLIP: A clip model focusing on wherever you want. _arXiv preprint arXiv:2312.03818_, 2023.\n' +
      '* [76] Gemini Team. Gemini: A family of highly capable multimodal models, 2023.\n' +
      '* [77] InternetLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. [https://github.com/InternLM/InternLM](https://github.com/InternLM/InternLM), 2023.\n' +
      '* [78] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv.org_, 2023.\n' +
      '* [79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almhairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models, 2023.\n' +
      '* [80] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, et al. Vigc: Visual instruction generation and correction. _arXiv.org_, 2023.\n' +
      '* [81] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. _arXiv preprint arXiv:2311.07574_, 2023.\n' +
      '* [82] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xinuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2023.\n' +
      '* [83] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large vision-language models. _arXiv preprint arXiv:2312.06109_, 2023.\n' +
      '* [84] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu, Rui Hu, et al. Skywork: A more open bilingual foundation model. _arXiv preprint arXiv:2310.19341_, 2023.\n' +
      '* [85] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. "Q-bench: A benchmark for general-purpose foundation models on low-level vision. _arXiv preprint arXiv:2309.14181_, 2023.\n' +
      '* [86] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. "mplug-owl: Modularization empowers large language models with multimodality. _arXiv.org_, 2023.\n' +
      '* [87] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. "Woodpecker: Hallucination correction for multimodal large language models. _arXiv preprint arXiv:2310.16045_, 2023.\n' +
      '* [88] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_, 2:67-78, 2014.\n' +
      '* [89] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Shevrin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashaul, Uriel Singer, Shang-Wen Li, Susan Zhang, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, and Armen Aghajanyan. Scaling autoregressive multi-modal models: Pretraining and instruction tuning.\n' +
      '* [90] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. _arXiv preprint arXiv:2308.02490_, 2023.\n' +
      '* [91] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. _arXiv preprint arXiv:2311.16502_, 2023.\n' +
      '* [92] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models. _arXiv preprint arXiv:2305.18279_, 2023.\n' +
      '* [93] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: An open bilingual pretrained model. In _The Eleventh International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* [94] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. _Advances in Neural Information Processing Systems (NeurIPS)_, 35:36067-36080, 2022.\n' +
      '* [95] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. "Intermlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. _arXiv preprint arXiv:2309.15112_, 2023.\n' +
      '* [96] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. "OPT: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.\n' +
      '* [97] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal in-context learning. _arXiv.org_, 2023.\n' +
      '* [98] Zhiyuan Zhao, Linke Ouyang, Bin Wang, Siyuan Huang, Pan Zhang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. "Mllm-datengine: An iterative refinement approach for mllm. _arXiv.org_, 2023.\n' +
      '* [99] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. _arXiv preprint arXiv:2311.16839_, 2023.\n' +
      '* [100] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv.org_, 2023.\n' +
      '\n' +
      '### French Pastries: A Sweet Indulgence\n' +
      '\n' +
      'French pastries are something that everyone should indulge in at least once in their lifetime. The delicate flavors and beautiful presentation of these pastries make them the perfect treat for any occasion. From croissants to macarons, eclairs to tars, French pastries come in a variety of shapes and sizes. These sweet delights are not only delicious but also incredibly addictive. It\'s hard to resist just one pastry when there are so many to choose from! Whether youre looking for a quick breakfast on the go or a decadent dessert after dinner, French pastries are sure to satisfy your carvings. So why not treat yourself to some French pastries today? You won\'t regret it!\n' +
      '\n' +
      '### Croissants\n' +
      '\n' +
      'The croissant is a crescent-shaped French pastry that has become one of the most well-known French pastries in the world. This buttery, flaky pastry is created by laminating dough with layers of butter; this process creates beautiful layers and flakiness when the croissant bakes. In France, croissants are typically served for breakfast or as a snack. Croissants can be eaten plain or topped with sugar, jam, Nutella, or other spreads. While croissants may seem like an indulgent treat, they\'re actually relatively easy to make at home. It\'s important to note that there are some variations on how to make croissants, including sweeter versions such as chocolate croissants and almond croissants.\n' +
      '\n' +
      '### Macarons\n' +
      '\n' +
      'Macarons are small, colorful cookies that are made with almond flour and filled with a variety of fillings such as ganache, buttercream, or jam. The delicate texture of these pastries make them perfect for pairing with coffee or tea. French Macarons are traditionally associated with Parisian cafes, where they can be found in an array of flavors ranging from classic to experimental.\n' +
      '\n' +
      '### Elclairs\n' +
      '\n' +
      'What are Eclairs? Eclairs (French pronunciation: (eklex)) are a type of pastry that originated in France and is still widely consumed there. They are long, thin pastries that are filled with cream or pastry cream. The exterior has a crispy coating while the interior is soft. There are many ways to make eclairs, but one thing remains constant--they have an unforgettable taste!\n' +
      '\n' +
      '### Tarts\n' +
      '\n' +
      'Tarts are open-faced pies that are typically filled with fruit or custard. They can be either sweet or savory, but the sweet versions are definitely more popular in France. There are many types of tarts in French cuisine: apple tart (tarte Tatin), almond tart (tarte amandine), chocolate tart (tarte au chocolat), cheese tart (tarte aux fromages) and many others.\n' +
      '\n' +
      '### Pain au Chocolat\n' +
      '\n' +
      'Pain au chocolat is a French pastry that consists of a crescent-shaped roll filled with chocolate. It\'s similar to a croissant but has a rich chocolate filling that adds an extra layer of decadence. The texture of the chocolate can be anything from soft and creamy to hard and crunchy depending on personal preference. The buttery, flaky crust makes it a deliciously indulgent treat for any time of day.\n' +
      '\n' +
      'In conclusion, French pastries are a sweet indulgence that should not be missed. Whether you are looking for something light like a croissant or something more substantial like a tart, there is sure to be a French pastry that will satisfy your carvings. So why not head to your local bakery and indulge in some French pastry goodness?\n' +
      '\n' +
      'Figure 3: Interleaved composition generated by InterLM-XComposer2.\n' +
      '\n' +
      'Figure 4: Interleaved composition generated by InterLM-XComposer2.\n' +
      '\n' +
      'Please write a English poetry based on Rabbindranath Tagore\'s classic poem: "If you shed tears when you miss the sun, you also miss the stars.", no less than 10 sentences.\n' +
      '\n' +
      'When the sun sets, the sky turns to dark And the tears that you shed, leave a mark\n' +
      '\n' +
      'But if you look up, you\'ll see the stars Twinkling in the night, like precious pearls The tears that you shed, can blind your sight And you may miss the beauty that\'s in sight But if you look up, you\'ll see the stars\n' +
      '\n' +
      'A symphony of nature\'s worth. The forests stand in solemn grandeur, Their leaves rustling in the breeze, A living testament to time, A place of peace and tranquility.\n' +
      '\n' +
      'Shing bright, like a beacon from afar So don\'t be sad when the sun is gone And don\'t shed tears for what\'s done Instead, look up and see the stars That light up the night, like shining stars The tears that you shed, can be replaced By the beauty of the stars, that you can embrace So don\'t miss the sun, but look up to the stars And find the beauty that\'s waiting, near and far.\n' +
      '\n' +
      'So let us not weep for what is lost, But cherish what remains, For life is fleeting, but the world, Is a place of endless wonders and charms.\n' +
      '\n' +
      'Figure 5: Interleaved composition generated by InterLM-XComposer2.\n' +
      '\n' +
      'Figure 6: Interleaved composition generated by InterLM-XComposer2.\n' +
      '\n' +
      'Figure 7: Interleaved composition generated by InterLM-XComposer2.\n' +
      '\n' +
      'Figure 8: Interleaved composition generated by InterLM-XComposer2.\n' +
      '\n' +
      'Please write an blog based on the title: how to learn swim.\n' +
      '\n' +
      'The first part should cover the swimming equipments.\n' +
      '\n' +
      'The second part should discuss warm-up exercises before swimming.\n' +
      '\n' +
      'The last part tells us how to swim, including different swimming techniques.\n' +
      '\n' +
      'The correct swimming posture is: keeping your head slightly above the water surface, keeping your chin down, extending your arms forward and placing them behind your ears, rotating your shoulders backwards, keeping your chest high, extending your legs straightly, bending your knees slightly, keeping your feet pointed, and putting your toes together.\n' +
      '\n' +
      'At the beginning of swimming, we should adopt floating and kicking method, that is, lying flat on the water surface with both hands extended forward, alternating between upper and lower kicks, allowing water to enter the mouth and nose, taking deep breaths when breathing. With repeated practice, the frequency of alternating between upper and lower kicks will become faster and more accurate.\n' +
      '\n' +
      'Swimming techniques include front crawl, back crawl, breaststroke, side stroke, butterfly kick, and frog kick. Each technique has its own advantages and disadvantages, and mastering each technique requires different movements. The swimming skills that we usually encounter in life include front crawl and back crawl, which are commonly known as "freeslyte". We just need to learn these two skills in order to enjoy swimming.\n' +
      '\n' +
      'Front crawl:\n' +
      '\n' +
      '1. Head position\n' +
      '\n' +
      'Head movement determines the whole stroke rhythm and direction of freestyle swimming. Freestyle swimming consists of two strokes per arm. The head and neck should always be in line with the center of gravity, turning sideways to breathe evenly, and turning to the opposite side at the end of each stroke. The head should move upwards naturally when turning.\n' +
      '\n' +
      'Figure 9: Interleaved composition generated by InternLM-XComposer2.\n' +
      '\n' +
      '2. Arm movement\n' +
      '\n' +
      'Arm movement is divided into two stages, namely the push-forward stage and the pull-back stage. The push-forward stage refers to the time when the forearm enters the water, moves downward, and extends forward until the fingertips pass the hips. It is important to extend the forearm fully during this stage. At the same time, the elbows should be lowered, and the hand should stretch out as far as possible.\n' +
      '\n' +
      '3. Leg movement\n' +
      '\n' +
      'Leg movement mainly includes the kick and the foot movement. First, flex the knee and lift the leg to 45 degrees, then stretch the legs slowly, and finally use the heel to start the kick, and make sure that the whole body is completely horizontal before finishing the kick. In addition, the calf muscle is also very important for propulsion, so keep flexing your calf muscles throughout the swimming process.\n' +
      '\n' +
      '4. Breathing method\n' +
      '\n' +
      'When swimming, turn your head sideways to breathe in natural rhythm, and look at the side about 60cm away from your face. You can also exhale while swimming in shallow water to control the air flow rate of the mouth and nose.\n' +
      '\n' +
      'Back crawl:\n' +
      '\n' +
      'The main difference between back crawl and front crawl is that the legs are kicked together instead of alternately. The back crawl is basically similar to the front crawl, except that the arms move backward instead of forward. Therefore, only one person uses his hands to carry the whole weight of the body, and the other person is supported by the palms and the back of the hands to move forward. As for the breathing method, you can follow the example of others to breathe naturally.\n' +
      '\n' +
      'Generate a travel diary focusing on a trip to Yellowstone National Park and recommend Yellowstone National Park in English.\n' +
      '\n' +
      'Yellowstone National Park is a must-see destination for any traveler. The park\'s natural beauty and geothermal wonders are truly awe-inspiring. From the stunning Old Faithful geyser to the breathtaking Grand Canyon of Yellowstone, there is no shortage of amazing sights to see in this national treasure.\n' +
      '\n' +
      'One of the most popular attractions in Yellowstone is the Old Faithful geyser. This iconic geyser erupts every 90 minutes, shooting water up to 180 feet into the air. It\'s a truly spectacular sight to behold, and visitors can watch from boardwalks that circle the geyser basin.\n' +
      '\n' +
      'Another highlight of Yellowstone is the Grand Canyon of Yellowstone. This massive canyon is over 20 miles long and up to 2,000 feet deep. The canyon is home to several waterfalls, including the impressive Lower Falls, which drops 308 feet into the canyon below.\n' +
      '\n' +
      'In addition to these two main attractions, Yellowstone is also home to numerous other geothermal features, including hot springs, mud pots, and fumaroles. These unique landscapes are unlike anything you\'ll find anywhere else in the world.\n' +
      '\n' +
      'Overall, Yellowstone National Park is a must-visit destination for anyone looking to experience the natural beauty and geothermal wonders of the American West. Whether you\'re interested in hiking, wildlife watching, or simply taking in the stunning scenery, Yellowstone has something for everyone.\n' +
      '\n' +
      'Figure 10: Interleaved composition generated by InternLM-XComposer2.\n' +
      '\n' +
      'Figure 11: Chat with InternLM-XComposer2.\n' +
      '\n' +
      'Figure 12: Chat with InternLM-XComposer2.\n' +
      '\n' +
      'First, we need to find the radius of the larger circle. Since the diameter is 10 cm, the radius is half of the diameter, which is: radius = diameter / 2 = 10 cm / 2 = 5 cm Next, we need to find the areas of the two smaller circles. The radius of each smaller circle is half of the radius of the large circle, which is: smaller radius = radius / 2 = 5 cm / 2 = 2.5 cm So the area of each smaller circle is: area of smaller circle = \\(\\pi\\) * (smaller radius)\\({}^{2}\\) area of smaller circle = \\(\\pi\\) * (2.5 cm)\\({}^{2}\\) = 19.63 cm\\({}^{2}\\) The sum of the areas of the two smaller circles is: 2 * area of smaller circle \\(\\approx\\) 39.26 cm\\({}^{2}\\) Finally, we can find the area of the shaded region by subtracting the sum of the areas of the two smaller circles from the area of the large circle: shaded region area = area of large circle - sum of smaller circle areas shaded region area = \\(\\pi\\) * (5 cm)\\({}^{2}\\) - 39.26 cm\\({}^{2}\\)\\(\\approx\\) 78.54 cm\\({}^{2}\\) - 39.26 cm\\({}^{2}\\) shaded region area \\(\\approx\\) 39.28 cm\\({}^{2}\\) The first column of the image contains two diamond-shaped objects. The first diamond shape is completely filled in with a solid color, while the second one also has an additional dot within it. Analyzing this pattern, it seems that the top row and bottom row contain the same type of figure but with different features - the top one is completely filled (possibly with a different color or texture), and the bottom one includes a dot inside it. From this information, we can infer that the missing image in the second column should be similar to the leftmost image in the second row--a star-shaped object with a solid fill. Since the other three images are already present and the pattern suggests repetition of figures with varying properties, the image in the second column will likely consist of a star with a solid fill.\n' +
      '\n' +
      'Figure 13: Chat with InternLM-XComposer2.\n' +
      '\n' +
      'The image is a black and white floor plan of a house, providing a detailed layout of the rooms and their dimensions. The plan is oriented with the entrance on the bottom left and the terrace on the top left. Starting from the entrance, there is a living room to the right, which is 146" x 205" in size. Adjacent to the living room is the dining area, which is 910" x 130" in size. The dining area is connected to the kitchen, which is 72" x 92" in size. Next to the kitchen is the second bedroom, which is 1010" x 147" in size. The master bedroom is located to the right of the second bedroom and is 1210" x 179" in size. The plan also includes a foyer, which is 60" x 53" in size, and a bathroom, which is 72" x 92" in size. The terrace, which is 216" x 80" in size, is located on the top left side of the plan. The floor plan provides a comprehensive view of the house\'s layout, allowing one to understand the spatial arrangement and dimensions of each room.\n' +
      '\n' +
      'Figure 14: Chat with InternLM-XComposer2.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>