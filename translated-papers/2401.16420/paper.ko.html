<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '7B 파라미터를 갖는_XComposer2 모델 시리즈는 공개적으로 at_[https://github.com/InternLM/InternLM-XComposer](https://github.com/InternLM/InternLM-XComposer)에서 이용 가능하다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근, 대형 언어 모델(LLM) 분야에서 현저한 진화가 있었다[8, 16, 17, 57, 58, 68, 79]. 무엇보다도, Chat-GPT[57]와 같은 모델들은 기술과의 인간 상호 작용을 완전히 변화시켰다. 동시에 Llama[78], Mistra[37], InternLM[77], QWen[65], GLM[25], Baichuan[7]과 같은 다양한 오픈 소스 LLM이 LLM의 사용자 정의에 권한을 부여했다. 이러한 오픈 소스 기반을 기반으로 커뮤니티는 멀티모달 대형 언어 모델(MLLM)[6, 21, 29, 48, 49, 82, 95, 100]에서 상당한 진전을 보았다. 이러한 MLLM은 이미지를 해석하고 텍스트 이미지 대화 상자에 참여하는 데 능숙하여 인상적인 멀티모달 이해도를 보여준다. 전통적인 MLLM과 달리 최근의 혁신인 _i.e_., InternLM-XComposer[95]는 텍스트 이미지 구성과 이해에 MLLM을 사용하는 데 중점을 두어 MLLM 연구에서 새로운 방향을 표시했다. 그러나, 이 선구적인 작업은 현재 제목만을 기반으로 한 텍스트 이미지 기사를 생성하는 것으로 제한되며, 보다 복잡한 구성 요건을 충족하기 위한 정교함이 부족하다. 또한, 이 모델은 시작 단계에서 선도적인 성과를 달성하지만, 여전히 비전 언어 이해 성능을 향상시키기 위한 세부 인식 및 복잡한 추론 능력의 향상에 상당한 잠재력을 가지고 있다.\n' +
      '\n' +
      '이러한 관찰은 실용적이고 강력한 텍스트 이미지 구성과 이해가 가능한 보다 진보된 비전 언어 모델의 개발에 동기를 부여한다. 본 논문에서는 InternLM2[77]를 기반으로 구축된 자유 형식의 텍스트 이미지 구성과 이해력이 뛰어난 첨단 모델인 InternLM-XComposer2를 소개한다. InternLM-XComposer2는 텍스트 이미지 구성과 이해 모두에서 전임자인 InternLM-XComposer[95]에 비해 상당한 발전을 나타낸다. InternLM-XComposer2는 상세한 사양, 구조화된 개요 및 참조 이미지와 같은 다양한 자유 형식 입력에서 고품질 통합 텍스트 이미지 기사를 생산하는 데 능숙하며 광범위한 응용 컨텍스트에 기여한다. 복합적 이해의 영역에서는 세밀한 인식, 논리적 추론, 광범위한 지식 통합에서 예외적인 역량을 발휘한다. 성능은 기존 오픈 소스 MLLM을 크게 능가하며 다양한 벤치마크에서 GPT-4V[58] 및 제미니 프로[76]와 동등하거나 심지어 능가합니다.\n' +
      '\n' +
      'InternLM-XComposer2의 매력적인 기능은 주로 두 가지 중요한 설계 요소 때문이다. (1) **Partial LoRA**: Partial LoRA(P-LoRA) 디자인은 구성과 이해에서 그 능력이 조화를 이룬다. 이것은 추가적인 LoRA[33] (Low-Rank Adaptation) 파라미터들을 갖는 이미지 토큰들을 포워딩하는 것을 포함하는 반면, 언어 토큰들은 원래의 아키텍처를 유지한다. 이러한 선택적 향상은 시각적 및 텍스트 도메인 모두에서 강력한 성능을 보장한다. (2) **High-quality and Diverse Data Foundataion**: 훈련 데이터의 품질 및 다양성이 중추적이다. 자유 형식의 텍스트 이미지 구성을 위한 데이터 세트는 복잡한 지침 준수, 맞춤형 콘텐츠를 위한 텍스트 및 이미지 맞춤화, 고품질 및 양식적으로 다양한 쓰기, 응축, 확장 및 수정을 포함한 다용도 텍스트 편집에 탁월합니다. 예외적인 시각 언어 이해 기능을 위해 다양한 고품질 사전 훈련 및 감독 미세 조정 멀티모달 데이터를 수집합니다. 이 컬렉션은 캡션, 일반 QA, 과학 QA, 채팅 기반 QA, 수학적 QA, 개념 지식, 대화, 텍스트 이미지 구성 등 다양한 측면과 유형에 걸쳐 있다.\n' +
      '\n' +
      'InternLM-XComposer2는 구성과 이해 모두에서 기존 벤치마크를 능가한다. LLM의 창의성을 평가하기 위한 OpenCompass[18]의 생성 벤치마크에서 InternLM-XComposer2는 뛰어난 성능을 보여준다. 다중 모드 이해 능력을 입증하기 위해 벤치마크 목록의 InternLM-XComposer2를 오픈 소스 MLLM 및 폐쇄 소스 API, 예를 들어 GPT4V[58], 제미니 프로[76], Qwen-VL Plus[19]와 비교한다. Math-Vista[52], MMMU[91], AI2D[40], MME[27], MMBench[51], MMBench-Chinese[51], SEED-Bench[41], LLaVA-Bench[49], QBench[85], MM-Vet[90], HallusionBench[31], ChartQA[56], POPE[45]의 결과를 보고한다. InternLM2-7B를 기반으로 한 InternLM-XComposer2는 기존 오픈소스 모델의 성능을 인상적인 차이로 크게 상회한다. 놀랍게도, 그것은 6개의 벤치마크에서 GPT4V[58], 제미니 프로[76]보다 우수한 성능을 보여준다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '**LLM(Large Language Models).** 최근 LLM 아키텍처들은 인코더-디코더 프레임워크들(_e.g_., BERT[22], T5[68])로부터 다음-토큰 예측을 위한 자기회귀 트레이닝 기법들과 함께 채용된 디코더 전용 모델들에 대한 강조로 전환을 표시했다(_e.g_., GPT[67]) 다음 작업들(_e.g_., GPT3[8], InstructGPT[60], ChatGPT[57], PaLM[17])은 인간 피드백으로부터의 명령어-튜닝 및 강화 학습(RLHF)과 같은 진보된 기술들의 통합을 보았다. 확장 파라미터 크기 및 광범위한 학습 데이터와 함께 이러한 LLM 모델은 다양한 자연어 처리(NLP) 작업에 걸쳐 상당한 성능 향상을 달성했다. 다른 주목할만한 LLM은 OPT[96], LLaMA 시리즈[78, 79], _e.g_., 미스트랄[37, 38], InternLM[77], GLM 시리즈[25, 93], Qwen 시리즈[6, 65], Baichuan[7], Skywork[84] 및 Falcon[61]과 같은 다양한 개발을 포함한다.\n' +
      '\n' +
      '**멀티모달 대형 언어 모델(MLLM).** CLIP[66] 및 그 후속 작업[26, 36, 43, 44, 50, 75, 94]으로 예시된 비전 언어 모델(VLM)은 단일화된 임베딩 공간에서 이미지 및 텍스트 특징을 정렬한다. 이 정렬은 광범위한 이미지-텍스트 쌍 데이터 세트에 적용된 대조적 학습 목표를 통해 달성된다. VLM은 강력한 제로 샷 및 소수 샷 성능을 달성하여 다양한 다운스트림 작업에 걸쳐 상당한 일반화 능력을 보여준다.\n' +
      '\n' +
      '기존의 대형 언어 모델과 VLM을 시각적 인코더로 사용하여 최근 멀티모달 대형 언어 모델(Multimodal Large Language Model, MLLM)[12, 14, 15, 24, 28, 58]은 시각 지각, 이해 및 추론 능력을 달성하며 다양한 시각 언어 작업에서 우수한 성능을 보여준다. 일련의 연구들[2, 5, 9, 10, 20, 21, 42, 46, 49, 62, 64, 80, 86, 92, 97, 98, 100]은 명령어 튜닝[11, 49, 98], 효율적인 미세 조정[33], 고해상도 이미지 입력[6, 82, 83], 환각 완화[34, 87, 99], 이미지 생성[23, 30, 74, 89], 3D 이해[63] 및 이미지-텍스트 이해 및 구성[95]과 같은 상이한 차원들에서 MLLM을 더욱 개선시키는 것을 탐구하였다.\n' +
      '\n' +
      '사용자 정의가 가능한 콘텐츠 제작이 가능하도록 MLLM을 기반으로 자유로운 형태의 텍스트 이미지 구성과 이해를 위한 모델을 설계하였다. 우리는 LLM으로 Intern-LM2를 사용하고 시각적 인코더로 CLIP ViT-Large를 사용하며 텍스트 이미지 양식을 정렬하기 위한 새로운 부분 LoRA를 제안한다. 사양, 윤곽선 및 참조 이미지와 같은 유연하고 다중 모달 사용자 입력이 주어지면, 본 모델은 고품질 인터리빙된 텍스트 이미지 작성 콘텐츠를 생성할 수 있다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      '제안된 모델인 InternLM-XComposer2는 비전 인코더와 언어 학습 모델(LLM)을 통합한다. 이 두 구성 요소는 혁신적인 부분 LoRA 모듈을 통해 상호 연결됩니다. 이미지들 및 텍스트들의 세트가 주어지면, LLM은 비전 인코더로부터의 출력을 시각적 토큰들로서 그리고 토큰화된 텍스트를 언어 토큰들로서 활용한다. 그런 다음 이러한 토큰이 연결되어 입력 시퀀스를 형성한다.\n' +
      '\n' +
      '**비전 인코더.** 저희 모델의 비전 인코더는 원시 이미지에서 높은 수준의 시각적 특징을 추출하도록 설계되었습니다. 이는 이미지 언어 대비 방식(CLIP)으로 사전 훈련된다. 우리의 연구 결과는 부분 LoRA 모듈과 함께 사용될 때 경량 비전 모델이 효과적으로 수행됨을 나타낸다. 효율성을 위해 오픈AI ViT-Large 모델을 사용하기로 결정했습니다.\n' +
      '\n' +
      '**대형 언어 모델.** 최근에 소개된 InternLM-2를 우리의 대형 언어 모델(LLM)로 사용한다. 이 모델은 탁월한 다국어 기능을 자랑하며 벤치마크에서 인상적인 결과를 보여주었습니다. 실제 응용 분야에서 우리는 InternLM2-7B-Chat-SFT 변형을 LLM으로 활용한다.\n' +
      '\n' +
      '**부분 저순위 적응** 다중 모달 언어 학습 모델(LLM)의 영역에서 불충분하게 탐색된 한 영역은 서로 다른 모달리티의 효과적인 정렬이다. 원하는 정렬은 잠재적으로 새로운 양식별 지식으로 LLM을 풍부하게 하는 동시에 고유한 기능을 보존해야 한다. 현재 방법은 시각적 토큰과 언어 토큰을 동등하게 취급하거나 완전히 구별되는 엔티티로 취급하는 두 가지 접근법 중 하나를 주로 채택한다. 첫 번째 접근법은 양식 간의 고유한 특성 차이를 간과하는 반면 두 번째 접근법은 상당한 정렬 비용을 초래한다고 주장한다.\n' +
      '\n' +
      '효과적인 모달리티 정렬을 추구하기 위해, 우리는 새로운 모달리티에서 LLM으로 지식을 정렬하도록 설계된 다용도 플러그인 모듈인 Partial LoRA를 소개한다. 그림 X에서 알 수 있듯이 부분 LoRA는 원래 LoRA에서 영감을 얻고 입력 토큰의 새로운 모달리티 부분에 독점적으로 적용되는 낮은 순위 적응을 통합한다. 우리의 구체적인 구성에서 부분 LoRA는 모든 시각적 토큰에 적용된다.\n' +
      '\n' +
      '형식적으로 LLM 블록의 각 선형 레이어 \\(L_{0}\\)에 대해 가중치 매트릭스 \\(W_{0}\\in\\mathbb{R}^{(C_{out}\\times C_{in})}\\)과 바이어스 \\(B_{0}\\in\\mathbb{R}^{C_{out}}\\)을 나타내며, 여기서 \\(C_{in}\\)과 \\(C_{out}\\)은 입출력 차원이다. 해당 부분 LoRA는 두 개의 저순위 행렬 \\(W_{A}\\in\\mathbb{R}^{C_{r}\\times C_{in}\\)과 \\(W_{B}\\in\\mathbb{R}^{C_{out}\\times C_{r}\\)을 포함한다. 주어진 입력 \\(x=[x_{v},x_{t}]\\)으로, 우리는 다음과 같은 출력 특징 \\(\\hat{x}\\)을 갖는다.\n' +
      '\n' +
      '\\[\\hat{x}_{t} = W_{0}x_{t}+B_{0}\\] \\[\\hat{x}_{v} = W_{0}x_{v}+W_{B}W_{A}x_{v}+B_{0}\\] \\[\\hat{x}=[\\hat{x}_{v},\\hat{x}_{t}]\\]\n' +
      '\n' +
      '도 2: **Partial-LoRA.**의 예시.** 파란색 토큰은 시각적 토큰을 나타내고 회색 토큰은 언어 토큰이다. 우리의 Partial-LoRA는 비주얼 토큰에만 적용됩니다.\n' +
      '\n' +
      '여기서 \\(x_{v}\\) 및 \\(x_{t}\\)는 각각 입력 시퀀스의 시각적 토큰 및 언어 토큰이다.\n' +
      '\n' +
      '### Pre-Training\n' +
      '\n' +
      '사전 훈련 단계 동안, LLM은 일정하게 유지되는 반면, 비전 인코더와 부분 LoRA는 모두 시각적 토큰을 LLM과 정렬하기 위해 미세 조정된다. 사전 훈련 데이터는 **3 목표를 염두에 두고 꼼꼼하게 큐레이션된다 : 1) 일반적인 의미 정렬, 2) 세계 지식 정렬, 3) 시력 능력 향상.\n' +
      '\n' +
      '**일반 의미 정렬.**일반 의미 정렬의 목적은 MLLM에 이미지 콘텐츠를 이해하는 기본적인 능력을 갖추는 것이다. 예를 들어, MLLM은 아인슈타인의 그림이 \'인간\'을 나타낸다는 것을 인식할 수 있어야 한다. 이를 위해 ShareGPT4V-PT의 고품질 상세 캡션과 COCO, NoCaps, TextCaps, _etc_의 간결하고 정확한 캡션을 포함한 다양한 소스의 이미지 캡션 데이터를 활용한다. 사전 훈련 단계에서는 간단한 지시를 사용한다. _Describe this image/in detail_.\n' +
      '\n' +
      '**세계 지식 정렬**세계 지식은 MLLM의 진보된 능력을 나타낸다. 예를 들어, MLLM은 위에서 언급한 그림 속의 남자를 \'알버트 아인슈타인\'으로 식별하고 그에 대해 더 이야기 할 수 있어야 한다. 이미지에 묘사된 세계 지식을 LLM이 이미 획득한 지식과 정렬하기 위해 개념 데이터 세트를 구성했다. 이 데이터 세트는 IntermLMComposer[95]에서 사용된 개념 데이터로부터 신중하게 필터링된다. 개념 데이터의 텍스트가 이미지의 내용을 부분적으로만 설명하고 그 관계가 모델링하기 복잡하다는 점을 감안할 때, 우리는 이 이미지에 대해 좀 더 광범위한 지침을 사용한다.\n' +
      '\n' +
      '**비전 능력 향상.** 마지막으로, 진보된 MLLM은 광학 문자 인식(OCR), 객체 로컬화(접지), 구조화된 이미지의 이해(_e.g_., 차트, 테이블)와 같은 특정 비전-특정 능력을 필요로 한다. 이를 달성하기 위해 표.1에 설명된 대로 관련 데이터 세트를 컴파일하고 교육을 위한 해당 지침을 구현했다.\n' +
      '\n' +
      'Partial LoRA의 설계 덕분에 LLM은 원래의 언어 처리 능력을 유지하면서 시각적 토큰에 적응할 수 있다. 고정 LLM은 또한 사전 훈련 품질의 척도로 맥락 내 학습 성능을 직접 사용할 수 있게 한다.\n' +
      '\n' +
      '구현에서는 OpenAI CLIP ViT-L-14-336을 비전 인코더로 사용한다. 성능 향상을 위해 해상도를 \\(490\\times 490\\)으로 높인다. 부분 LoRA의 경우, LLM 디코더 블록 내의 모든 선형 레이어에 대해 \\(256\\)의 랭크를 설정한다. 우리의 훈련 과정은 4906의 배치 크기와 \\(2\\) 에폭에 걸쳐 있다. 학습률은 첫 번째 학습 단계(1\\%\\) 내에서 \\(2\\×10^{-4}\\)으로 증가하도록 초기 설정되었다. 이 후 코사인 감쇠 전략에 따라 \\(0\\)으로 감소한다. 비젼 인코더의 기존 지식을 보존하기 위해 LDR(layer-wise learning rate) decay 전략을 적용하고 decay factor를 \\(0.90\\)으로 설정하였다.\n' +
      '\n' +
      '### Supervised Fine-tuning\n' +
      '\n' +
      '사전 훈련 단계는 시각적 특징을 언어와 정렬하여 언어 학습 모델(LLM)이 이미지의 내용을 이해할 수 있도록 한다. 그러나, 여전히 영상 정보를 효과적으로 활용할 수 있는 능력이 부족하다. 이러한 한계를 극복하기 위해, 우리는 모델이 후속 감독 미세 조정 단계에서 수행하는 다양한 비전 언어 작업을 소개한다. 이 단계는 다중 작업 훈련과 자유 형식의 텍스트 이미지 합성이라는 두 가지 순차적 단계로 구성된다. 이 단계에서 우리는 비전 인코더, LLM 및 부분 LoRA를 공동으로 미세 조정한다.\n' +
      '\n' +
      '**멀티태스크 트레이닝** 표 2에 설명된 바와 같이 다중 작업 훈련 데이터 세트는 다양한 소스에서 조립되어 모델에 광범위한 능력을 갖추는 것을 목표로 한다. 각 작업은 대화 상호 작용으로 구성되며, 지침은 다양성을 향상시키기 위해 GPT-4로 증강된다. 동시에, 원래의 언어 능력을 유지하기 위해 감독된 미세 조정도 통합한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline Task & Dataset \\\\ \\hline General Semantic Alignment & ShareGPT4V-PT [11], COCO [13], Necaps [1], TextCaps [73], LAION400M [69], SBU [59], CC 3M [72] \\\\ World Knowledge Alignment & Concept Data [95] \\\\ Vision Capability Enhancement & WanJuan [32], Flicker[88], MMC-Instruction[47] \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 사전 훈련에 사용되는 데이터 세트. 데이터는 세 가지 목적을 위해 다양한 출처에서 수집된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Task & Dataset \\\\ \\hline _Multi-task training_ & \\\\ Caption & ShareGPT4V [11], COCO [13],Nocaps [1] \\\\ General QA & VQAv2 [4], GQA [35], OK-VQA [55] \\\\ Science QA & AI2D [40], SQA [54] \\\\ Chart QA & DVQA [39], ChartQA [56] \\\\ Math QA & MathQA [3], Geometry3K[53] \\\\ World Knowledge QA & A-OKVQA [70], kVQA [71] \\\\ Conversation & LLaVA-150k [49], LVIS-Instruct4V [81] \\\\ \\hline _Instruction tuning_ & \\\\ Free-from Compositio & In-house data (Refer to Sec.3.4) \\\\ Conversation & LLaVA-150k [49], LVIS-Instruct4V [81] \\\\  & ShareGPT-en\\&zh [16], InternLM-Chat[77] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 감독 미세 조정에 사용되는 데이터 세트. 다양한 소스에서 데이터를 수집하여 다양한 기능을 가진 모델에 권한을 부여합니다.\n' +
      '\n' +
      '전체 SFT(Supervised Fine-Tuning) 데이터의 고정된 10%를 구성하는 InternLM2의 데이터.\n' +
      '\n' +
      '**자유 형식의 텍스트 이미지 합성** 자유형 이미지 텍스트 콘텐츠를 구성하고 지침을 따르는 모델의 능력을 더욱 향상시키기 위해 표 2에 요약된 바와 같이 순수 텍스트 대화 코퍼스와 비전 언어 대화의 데이터를 사용한다. 자유형 이미지 텍스트 구성을 위한 데이터 세트는 섹션 3.4에 설명된 방법론에 따라 구성된다.\n' +
      '\n' +
      '우리의 접근법에서, 우리는 3000단계에 걸쳐 2048의 배치 크기로 모든 구성 요소를 공동으로 훈련한다. 다수의 소스로부터의 데이터는 가중된 방식으로 샘플링되며, 가중치는 각 소스로부터의 데이터의 수에 기초한다. 최대 학습률은 \\(5\\times 10^{-5}\\)으로 설정되었으며, 각 구성 요소는 고유한 학습 전략을 가지고 있다. 비전 인코더의 경우, 사전 훈련 전략과 일치하는 계층별 학습 속도 감소(LLDR)를 \\(0.9\\)으로 설정했다. LLM은 \\(0.2\\)의 고정 학습률 척도 계수를 사용한다. 이는 LLM의 업데이트를 늦춰 원래의 능력을 보존하는 것과 비전 지식에 맞추는 것 사이의 균형을 이루게 한다.\n' +
      '\n' +
      '### 자유로운 형태의 텍스트-이미지 합성\n' +
      '\n' +
      '자유형 텍스트-이미지 구도는 유연하고 비제한적인 방식으로 텍스트 콘텐츠와 시각적 요소의 결합을 의미한다. 우리의 모델은 제목, 윤곽 및 필기 재료와 같은 요소, 선택적으로 이미지 리소스와 같은 임의의 시각적 요구 사항을 포함할 수 있는 사용자가 제공하는 텍스트 요구 사항과 정렬하도록 특별히 맞춤화된 인터리빙된 텍스트 및 이미지를 생성한다.\n' +
      '\n' +
      '자유 형식의 텍스트 이미지 구성을 용이하게 하기 위해 4가지 주요 차원에 걸쳐 다양하고 고품질의 사내 데이터를 수집한다. 이러한 차원은 **다양한 글쓰기 스타일.** 우리의 데이터는 학술 논문부터 소셜 미디어 게시물 및 시까지 다양한 글쓰기 스타일에 걸쳐 있으며, 텍스트 및 이미지 콘텐츠의 풍부하고 다양한 수집을 보장한다.\n' +
      '\n' +
      '**Flexible Text Editing.** 우리 데이터 세트는 단축, 확장 및 재기입과 같은 광범위한 수정 범위를 포함하는 텍스트 편집의 광범위한 예를 포함한다.\n' +
      '\n' +
      '**Complex Instruction Adherence.** 텍스트 및 이미지 기반 구성을 모두 포함하는 제목 및 개요와 같은 다양한 요구에 부합하는 콘텐츠를 생성하기 위해 복잡한 지침을 준수하는 사례도 캡처합니다.\n' +
      '\n' +
      '**재료와의 맞춤.** 우리 컬렉션은 텍스트와 이미지를 모두 포함하는 개인화된 콘텐츠 생성에 사용되는 재료로 확장되어 사용자 정의 가능하고 고유한 콘텐츠 생성 경험을 가능하게 합니다.\n' +
      '\n' +
      '4차원의 데이터 분포는 대략 1:1:1:1의 비율로 거의 동일하며, 본 방법은 텍스트 콘텐츠를 생성한 후 이미지 삽입에 적합한 위치를 식별하기 위해 이전 작업[95]을 따른다. 우리의 접근법에서 주목할 만한 차이점은 사용자가 자신의 이미지 자료를 제공할 때, 이러한 이미지 자료는 검색된 이미지에 의존하지 않고 삽입에 사용된다는 것이다[95]. 또한 고해상도 이미지 입력이 텍스트 이미지 구성에 필수적인 것은 아니라는 것을 관찰한다. 따라서 사전 훈련 단계에 따라 자유 형식의 텍스트 이미지 구도의 SFT 단계에서 이미지 입력 해상도를 224x224로 하향 샘플링한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '이 섹션에서는 감독 미세 조정 후 InternLM-XComposer2의 벤치마크 성능을 검증한다.\n' +
      '\n' +
      '### MLLM 벤치마크 결과입니다.\n' +
      '\n' +
      '표.3 및 표.4에서 우리는 벤치마크 목록에 있는 InternLM-XComposer2를 SOTA 오픈 소스 MLLM 및 폐쇄 소스 API와 비교한다. MathVista[52], MMMU[91], AI2D[40], MME Perception(MME\\({}^{P}\\))[27], MME 인지(MME\\({}^{C}\\))[27], MMBench(MMB)[51], MMBenchChinese(MMB\\({}^{CN}\\))[51], SEED-Bench Image Part(SEED\\({}^{I}\\))[41], LLaVA-Bench In-the-Wild(LLaVA\\({}^{W}\\))[49], QBench-Testset(QBench\\({}^{T}\\))[85], MM-Vet[90], HallusionBench(HallB)[31], ChartQA[56], POPE[45].\n' +
      '\n' +
      '**Closed-Source API와의 비교.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c c c c} \\hline Method & MathVista & A2D & MMMU & MME & MMB & MMB\\({}^{CN}\\) & SEED\\({}^{I}\\) & LLaVA\\({}^{W}\\) & QBench\\({}^{T}\\) & MM-Vet & HallB & ChartVQA \\\\ \\hline Open-Source & _SPH-MOE_ & _Monkey_ & _Yi-VL_ & _WeMM_ & _L-Int2_ & _L-Int2_ & _SPH-2_ & _CogVLM_ & _Int-XC_ & _CogVLM_ & _Monkey_ & _CogAgent_ \\\\ Previous SOTA & 8x7B & 10B & 34B & 6B & 20B & 20B & 17B & 17B & 8B & 30B & 10B & 18B \\\\  & 42.3 & 72.6 & 45.9 & 2066.6 & 75.1 & 73.7 & 74.8 & 73.9 & 64.4 & 56.8 & 58.4 & 68.4 \\\\ \\hline \\multicolumn{13}{l}{_Closed-source API_} \\\\ GPT-4V & 49.9 & 78.2 & **56.8** & 1926.5 & 77.0 & 74.4 & 69.1 & **93.1** & **74.1** & **67.7** & **65.8** & **78.5** \\\\ Gemini-Pro & 45.2 & 73.9 & 47.9 & 1933.3 & 73.6 & 74.3 & 70.7 & 79.9 & 70.6 & 64.3 & 63.9 & 74.1 \\\\ QwenVL-Plus & 43.3 & 75.9 & 46.5 & 2183.3 & 67.0 & 70.7 & 72.7 & 73.7 & 68.9 & 55.7 & 56.4 & 78.1 \\\\ \\hline Ours & **57.6** & **78.7** & 42.0 & **2242.7** & **79.6** & **77.6** & **75.9** & 81.8 & 72.5 & 51.2 & 60.3 & 72.6 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: **폐쇄 소스 API 및 이전 오픈 소스 SOTA와의 비교.** 우리의 InternLM-XComposer2는 7B 매개 변수만 있는 12개의 벤치마크 중 6개에서 SOTA 결과를 얻으며, 현재 폐쇄 소스 API 및 이전 오픈 소스 SOTA MLLM과의 경쟁 결과를 보여준다. 가장 좋은 결과는 **굵은**이며 두 번째로 좋은 결과는 밑줄이 그어져 있습니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 비전-언어 이해 및 자유 형태 텍스트-이미지 구성 분야에서 탁월한 역량을 발휘하는 InternLM-XComposer2를 제시한다. 본 논문에서 제안한 PLoRA(Partial LoRA) 기법은 이미지 토큰에만 LoRA 파라미터를 추가 적용하여 사전 학습된 언어 지식의 무결성을 유지하면서, 문학적 인재와 함께 정확한 시각 이해와 텍스트 구성 사이의 균형을 유지하는 데 효과적인 것으로 입증되었다. 다양한 벤치마크에 걸친 우리 모델의 성능은 기존 멀티모달 모델을 크게 능가할 뿐만 아니라 특정 평가에서 GPT-4V 및 제미니 프로와 일치하거나 심지어 능가하여 멀티모달 이해 영역에서 놀라운 숙련도를 강조한다. 본 연구는 맞춤형 콘텐츠 제작을 위한 새로운 가능성을 열어주고, 향후 MLLM 분야의 발전을 위한 길을 열어준다. InternLM-XComposer2의 잠재적인 응용 프로그램은 방대하고 흥미진진하며 AI가 쉽고 정확하게 고품질의 긴 텍스트 멀티모달 콘텐츠를 이해하고 생성할 수 있는 미래를 약속한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 8948-8957, 2019.\n' +
      '* [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022.\n' +
      '* [3] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. _arXiv preprint arXiv:1905.13319_, 2019.\n' +
      '* [4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In _International Conference on Computer Vision (ICCV)_, 2015.\n' +
      '* [5] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models. _arXiv.org_, 2023.\n' +
      '* [6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv.org_, 2023.\n' +
      '* [7] Baichuan. Baichuan 2: Open large-scale language models. _arXiv.org_, 2023.\n' +
      '* [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in Neural Information Processing Systems (NeurIPS)_, 33:1877-1901, 2020.\n' +
      '* [9] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. _arXiv preprint arXiv:2310.09478_, 2023.\n' +
      '* [10] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal lm\'s referential dialogue magic. _arXiv.org_, 2023.\n' +
      '* [11] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.\n' +
      '\n' +
      '* [12] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Mongomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali-x: On scaling up a multilingual vision and language model, 2023.\n' +
      '* [13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server, 2025.\n' +
      '* [14] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut. Pali-3 vision language models: Smaller, faster, stronger, 2023.\n' +
      '* [15] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Pugicerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language model, 2023.\n' +
      '* [16] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Linamin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impression gpt-4 with 90% + chatgrp quality, March 2023.\n' +
      '* [17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv.org_, 2022.\n' +
      '* [18] OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation models. [https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass), 2023.\n' +
      '* [19] QWen Contributors. Qwen-vl-plus. [https://huggingface.co/spaces/Qwen/Qwen-VL-Plus](https://huggingface.co/spaces/Qwen/Qwen-VL-Plus), year=2023.\n' +
      '* [20] XTuner Contributors. Xtuner: A toolkit for efficiently fine-tuning llm. [https://github.com/InternLM/xtuner](https://github.com/InternLM/xtuner), 2023.\n' +
      '* [21] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.\n' +
      '* [22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv.org_, 2018.\n' +
      '* [23] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. Dreamllm: Synergistic multimodal comprehension and creation. _arXiv preprint arXiv:2309.11499_, 2023.\n' +
      '* [24] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In _arXiv preprint arXiv:2303.03378_, 2023.\n' +
      '* [25] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335, 2022.\n' +
      '* [26] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 19358-19369, 2023.\n' +
      '* [27] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jirrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023.\n' +
      '* [28] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang, Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang Shen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Shaohui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hongsheng Li, and Xing Sun. A challenger to gpt-4v? early explorations of gemini in visual expertise. _arXiv preprint arXiv:2312.12436_, 2023.\n' +
      '* [29] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, W. Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Jiao Qiao. Llamada-adapter v2: Parameter-efficient visual instruction model. _ArXiv_, abs/2304.15010, 2023.\n' +
      '* [30] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large language model.\n' +
      '* [31] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models, 2023.\n' +
      '* [32] Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Da Lin. Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models. _ArXiv_, abs/2308.10755, 2023.\n' +
      '* [33] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022.\n' +
      '* [34] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bingu Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multimodal large language models via over-trust penalty and retrospection-allocation. _arXiv preprint arXiv:2311.17911_, 2023.\n' +
      '* [35] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [36] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _Proceedings of the International Conference on Machine learning (ICML)_, pages 4904-4916. PMLR, 2021.\n' +
      '* [37] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023.\n' +
      '* [38] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mistral of experts. _arXiv preprint arXiv:2401.04088_, 2024.\n' +
      '* [39] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5648-5656, 2018.\n' +
      '* [40] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 235-251. Springer, 2016.\n' +
      '* [41] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023.\n' +
      '* [42] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. _arXiv.org_, 2023.\n' +
      '* [43] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _Proceedings of the International Conference on Machine learning (ICML)_, pages 12888-12900. PMLR, 2022.\n' +
      '* [44] Liunian Harold Li\\({}^{*}\\), Pengchuan Zhang\\({}^{*}\\), Haotian Zhang\\({}^{*}\\), Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [45] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models, 2023.\n' +
      '* [46] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. _arXiv preprint arXiv:2311.06607_, 2023.\n' +
      '* [47] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. _arXiv preprint arXiv:2311.10774_, 2023.\n' +
      '* [48] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.\n' +
      '* [49] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv.org_, 2023.\n' +
      '* [50] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding onto: Marrying dino with grounded pre-training for open-set object detection. _arXiv.org_, 2023.\n' +
      '* [51] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? _arXiv:2307.06281_, 2023.\n' +
      '* [52] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In _International Conference on Learning Representations (ICLR)_, 2024.\n' +
      '* [53] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In _The 59th Annual Meeting of the Association for Computational Linguistics (ACL)_, 2021.\n' +
      '* [54] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. _Advances in Neural Information Processing Systems_, 35:2507-2521, 2022.\n' +
      '* [55] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In _Proceedings of the IEEE/crf conference on computer vision and pattern recognition_, pages 3195-3204, 2019.\n' +
      '* [56] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. _arXiv preprint arXiv:2203.10244_, 2022.\n' +
      '**[57] OpenAI. Chatgpt. [https://openai.com/blog/chatgpt] (https://openai.com/blog/chatgpt), 2022.\n' +
      '* [58] OpenAI. Gpt-4 technical report, 2023.\n' +
      '* [59] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned photographs. In _Neural Information Processing Systems (NIPS)_, 2011.\n' +
      '* [60] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems (NeurIPS)_, 35:27730-27744, 2022.\n' +
      '* [61] Guilherme Penedo, Quentin Malatric, Daniel Hesslow, Ruanarda Cojocaru, Alessandro Cappelli, Hamza Alobeioli, Baptiste Panir, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.\n' +
      '* [62] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. _arXiv.org_, 2023.\n' +
      '* [63] Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, and Hengshuang Zhao. Gpt4point: A unified framework for point-language understanding and generation, 2023.\n' +
      '* [64] Zhangyang Qi, Ye Fang, Mengchen Zhang, Zeyi Sun, Tong Wu, Ziwei Liu, Dahua Lin, Jiaqi Wang, and Hengshuang Zhao. Gemini vs gpt-4v: A preliminary comparison and combination of vision-language models through qualitative cases, 2023.\n' +
      '* [65] Qwen. Introducing qwen-7b: Open foundation and human-aligned models (of the state-of-the-arts), 2023.\n' +
      '* [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _Proceedings of the International Conference on Machine learning (ICML)_, pages 8748-8763. PMLR, 2021.\n' +
      '* [67] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.\n' +
      '* [68] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research (JMLR)_, 21(1):5485-5551, 2020.\n' +
      '* [69] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Liaon-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [70] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In _European Conference on Computer Vision_, pages 146-162. Springer, 2022.\n' +
      '* [71] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. Kvqa: Knowledge-aware visual question answering. In _Proceedings of the AAAI conference on artificial intelligence_, 2019.\n' +
      '* [72] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2556-2565, 2018.\n' +
      '* [73] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pages 742-758. Springer, 2020.\n' +
      '* [74] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yuzee Wang, Hongcheng Gao, Jingjing Liu, Trejun Huang, and Xinlong Wang. Generative pretraining in multimodality. Jul 2023.\n' +
      '* [75] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-CLIP: A clip model focusing on wherever you want. _arXiv preprint arXiv:2312.03818_, 2023.\n' +
      '* [76] Gemini Team. Gemini: A family of highly capable multimodal models, 2023.\n' +
      '* [77] InternetLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. [https://github.com/InternLM/InternLM](https://github.com/InternLM/InternLM), 2023.\n' +
      '* [78] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv.org_, 2023.\n' +
      '* [79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almhairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models, 2023.\n' +
      '* [80] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, et al. Vigc: Visual instruction generation and correction. _arXiv.org_, 2023.\n' +
      '* [81] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. _arXiv preprint arXiv:2311.07574_, 2023.\n' +
      '* [82] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xinuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2023.\n' +
      '* [83] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large vision-language models. _arXiv preprint arXiv:2312.06109_, 2023.\n' +
      '* [84] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu, Rui Hu, et al. Skywork: A more open bilingual foundation model. _arXiv preprint arXiv:2310.19341_, 2023.\n' +
      '* [85] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. "Q-bench: A benchmark for general-purpose foundation models on low-level vision. _arXiv preprint arXiv:2309.14181_, 2023.\n' +
      '* [86] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. "mplug-owl: Modularization empowers large language models with multimodality. _arXiv.org_, 2023.\n' +
      '* [87] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. "Woodpecker: Hallucination correction for multimodal large language models. _arXiv preprint arXiv:2310.16045_, 2023.\n' +
      '* [88] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_, 2:67-78, 2014.\n' +
      '* [89] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Shevrin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashaul, Uriel Singer, Shang-Wen Li, Susan Zhang, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, and Armen Aghajanyan. Scaling autoregressive multi-modal models: Pretraining and instruction tuning.\n' +
      '* [90] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. _arXiv preprint arXiv:2308.02490_, 2023.\n' +
      '* [91] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. _arXiv preprint arXiv:2311.16502_, 2023.\n' +
      '* [92] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models. _arXiv preprint arXiv:2305.18279_, 2023.\n' +
      '* [93] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: An open bilingual pretrained model. In _The Eleventh International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* [94] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. _Advances in Neural Information Processing Systems (NeurIPS)_, 35:36067-36080, 2022.\n' +
      '* [95] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. "Intermlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. _arXiv preprint arXiv:2309.15112_, 2023.\n' +
      '* [96] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. "OPT: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.\n' +
      '* [97] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal in-context learning. _arXiv.org_, 2023.\n' +
      '* [98] Zhiyuan Zhao, Linke Ouyang, Bin Wang, Siyuan Huang, Pan Zhang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. "Mllm-datengine: An iterative refinement approach for mllm. _arXiv.org_, 2023.\n' +
      '* [99] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. _arXiv preprint arXiv:2311.16839_, 2023.\n' +
      '* [100] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv.org_, 2023.\n' +
      '\n' +
      '### 프렌치 페이스트리: 달콤한 탐닉\n' +
      '\n' +
      '프랑스 페이스트리는 모든 사람들이 일생에 한 번쯤은 즐겨야 하는 것이다. 이 페이스트리의 섬세한 맛과 아름다운 프레젠테이션은 어떤 경우에도 완벽한 간식입니다. 크루아상부터 마카롱, 에클레어, 타르, 프렌치 페이스트리는 다양한 모양과 크기로 제공됩니다. 이 달콤한 기쁨은 맛있을 뿐만 아니라 믿을 수 없을 정도로 중독성이 있습니다. 선택할 수 있는 게 너무 많은데 페이스트리를 한 개만 거부하기는 어려워요! 이동 중 빠른 아침 식사를 찾든 저녁 식사 후 퇴폐적인 디저트를 찾든, 프렌치 페이스트리는 확실히 조각품을 만족시킬 것입니다. 그래서 오늘은 프렌치 페이스트리를 먹어보는 게 어때? 후회하지 않을 거야!\n' +
      '\n' +
      '### Croissants\n' +
      '\n' +
      '크루아상은 세계에서 가장 잘 알려진 프랑스 페이스트리 중 하나가 된 초승달 모양의 프랑스 페이스트리다. 이 버터처럼 벗겨진 페이스트리는 반죽에 버터를 겹쳐서 만들어집니다; 이 과정은 크루아상이 구워질 때 아름다운 층과 벗겨짐을 만들어냅니다. 프랑스에서는 일반적으로 크루아상을 아침 식사로 제공하거나 간식으로 제공합니다. 크루아상은 평평하게 먹거나 설탕, 잼, 누텔라 또는 기타 스프레드를 얹어 먹을 수 있습니다. 크루아상은 관대한 대접처럼 보일 수 있지만 실제로 집에서 만들기 비교적 쉽습니다. 초콜릿 크루아상과 아몬드 크루아상과 같은 더 달콤한 버전을 포함하여 크루아상을 만드는 방법에 대한 몇 가지 변형이 있다는 점에 유의하는 것이 중요합니다.\n' +
      '\n' +
      '### Macarons\n' +
      '\n' +
      '마카롱은 아몬드 가루로 만들고 가나슈, 버터크림, 잼 등 다양한 속을 채운 작고 다채로운 쿠키입니다. 이 페이스트리들의 섬세한 질감은 커피나 차와 페어링하기에 완벽합니다. 프랑스 마카롱은 전통적으로 파리 카페와 관련이 있으며, 고전에서 실험까지 다양한 맛으로 볼 수 있다.\n' +
      '\n' +
      '### Elclairs\n' +
      '\n' +
      '에클레어스가 뭐죠? 에클레어(프랑스 발음: (엑렉스))는 프랑스에서 유래된 페이스트리의 한 종류이며, 여전히 그곳에서 널리 소비되고 있다. 그것들은 크림이나 페이스트리 크림으로 가득 찬 길고 얇은 페이스트리다. 외관은 바삭한 코팅이 되어 있고 내부는 부드럽습니다. 에클레어를 만드는 방법은 여러 가지가 있지만, 한 가지는 변함없습니다. 그들은 잊을 수 없는 맛을 가지고 있습니다!\n' +
      '\n' +
      '### Tarts\n' +
      '\n' +
      '타르트는 일반적으로 과일이나 커스터드로 채워진 개방형 파이입니다. 달거나 고소할 수 있지만, 달콤한 버전은 확실히 프랑스에서 더 인기가 있습니다. 프랑스 요리에는 많은 종류의 타르트가 있다: 애플 타르트(타르테 타틴), 아몬드 타르트(타르테 아만딘), 초콜릿 타르트(타르테 오초콜릿), 치즈 타르트(타르테 옥사제) 등이 있다.\n' +
      '\n' +
      '# Pain au Chocolat\n' +
      '\n' +
      '파인오 초콜렛은 초콜렛으로 채워진 초승달 모양의 롤로 구성된 프렌치 페이스트리입니다. 크루아상과 비슷하지만 디카던스를 더해주는 풍부한 초콜릿 필링이 있습니다. 초콜릿의 질감은 개인 취향에 따라 부드럽고 크림 같은 것부터 단단하고 바삭한 것까지 무엇이든 될 수 있습니다. 버터처럼 벗겨진 크러스트는 하루 중 언제라도 맛있게 탐닉할 수 있는 간식입니다.\n' +
      '\n' +
      '결론적으로, 프랑스 페이스트리는 놓쳐서는 안 되는 달콤한 면죄부이다. 크루아상과 같은 가벼운 것을 찾든 타르트와 같은 더 실질적인 것을 찾든, 당신의 조각들을 만족시킬 프랑스 페이스트리가 분명히 있을 것입니다. 그러니 당신의 지역 빵집으로 가서 프랑스 제과점에 빠져보는 건 어때요?\n' +
      '\n' +
      '도 3: InterLM-XComposer2에 의해 생성된 인터리빙된 조성물.\n' +
      '\n' +
      '도 4: InterLM-XComposer2에 의해 생성된 인터리빙된 조성물.\n' +
      '\n' +
      '라빈드라나스 타고르의 고전시 "태양을 그리워할 때 눈물을 흘리면 별도 그리워한다"를 바탕으로 영시를 써주세요, 10문장 이상입니다.\n' +
      '\n' +
      '해가 지면 하늘은 어두워지고 눈물은 자국을 남기지\n' +
      '\n' +
      '하늘을 보면 반짝이는 별을 볼 수 있어 소중한 진주처럼\n' +
      '\n' +
      '자연의 가치가 있는 교향곡. 숲은 엄숙한 웅장함 속에 서 있고, 산들바람에 바스락거리는 나뭇잎, 시간의 살아있는 증거, 평화와 평온의 장소.\n' +
      '\n' +
      '저 멀리서 날아온 등불처럼 밝게 울지 마\n' +
      '\n' +
      '잃어버린 것에 대해 울지 말고 남은 것을 소중히 여기자 인생은 덧없지만 세상은 끝없는 경이로움과 매력의 장소이다\n' +
      '\n' +
      '도 5: InterLM-XComposer2에 의해 생성된 인터리빙된 조성물.\n' +
      '\n' +
      '도 6: InterLM-XComposer2에 의해 생성된 인터리빙된 조성물.\n' +
      '\n' +
      '도 7: InterLM-XComposer2에 의해 생성된 인터리빙된 조성물.\n' +
      '\n' +
      '도 8: InterLM-XComposer2에 의해 생성된 인터리빙된 조성물.\n' +
      '\n' +
      '제목을 바탕으로 블로그를 작성해주세요: 수영을 배우는 방법.\n' +
      '\n' +
      '첫 번째 부분은 수영 장비를 다루어야 합니다.\n' +
      '\n' +
      '두 번째 파트에서는 수영 전에 준비 운동에 대해 논의해야 합니다.\n' +
      '\n' +
      '마지막 부분은 다양한 수영 기술을 포함하여 수영하는 방법을 알려줍니다.\n' +
      '\n' +
      '올바른 수영 자세는 머리를 수면 위로 살짝 굽히고 턱을 아래로 내리고 팔을 앞으로 뻗은 뒤 귀 뒤로 눕히고 어깨를 뒤로 돌리고 가슴을 높게 펴고 다리를 곧게 펴고 무릎을 살짝 굽히고 발을 뾰족하게 펴고 발가락을 합치는 것이다.\n' +
      '\n' +
      '수영 초기에는 양손을 앞으로 뻗은 채 수면 위에 평평하게 눕혀 상·하 발차기를 번갈아 가며 입과 코로 물이 들어가 호흡할 때 심호흡을 하는 부상·발차기 방식을 채택해야 한다. 반복적인 연습으로 상, 하 킥의 교대 빈도가 빨라지고 정확해질 것이다.\n' +
      '\n' +
      '수영 기술로는 전방 크롤, 후방 크롤, 평영, 측면 스트로크, 나비킥, 개구리킥 등이 있다. 각 기법마다 장단점이 있고, 각 기법을 숙달하려면 서로 다른 움직임이 필요하다. 우리가 삶에서 흔히 접하는 수영 기술에는 흔히 \'프리슬리트\'로 알려진 앞쪽 크롤과 뒤쪽 크롤이 있다. 우리는 수영을 즐기기 위해 이 두 가지 기술을 배우기만 하면 됩니다.\n' +
      '\n' +
      'Front crawl:\n' +
      '\n' +
      '1. 머리 위치\n' +
      '\n' +
      '머리 운동은 전체 스트로크 리듬과 자유형 수영의 방향을 결정한다. 자유형 수영은 한 팔당 두 타로 구성되어 있다. 머리와 목은 항상 무게 중심과 일직선상에 있어야 하며, 옆으로 돌아서 고르게 숨을 쉴 수 있도록 하고, 각 획이 끝날 때마다 반대쪽으로 돌아야 한다. 머리를 돌릴 때 자연스럽게 위로 움직여야 합니다.\n' +
      '\n' +
      '도 9: InternLM-XComposer2에 의해 생성된 인터리브드 조성물.\n' +
      '\n' +
      '2. 팔 움직임\n' +
      '\n' +
      '팔의 움직임은 푸시-포워드 스테이지와 풀-백 스테이지의 두 단계로 나뉜다. 푸시포워드 스테이지는 팔뚝이 물에 들어가 아래로 움직이며 손끝이 엉덩이를 지날 때까지 앞으로 뻗는 시간을 말한다. 이 단계에서 팔뚝을 완전히 확장하는 것이 중요하다. 동시에 팔꿈치를 내리고 손은 최대한 뻗어야 한다.\n' +
      '\n' +
      '3. 다리 움직임\n' +
      '\n' +
      '다리 움직임은 주로 발차기와 발 움직임을 포함한다. 먼저 무릎을 구부리고 다리를 45도까지 들어올린 후 천천히 다리를 펴고 마지막으로 뒤꿈치를 이용해 킥을 시작하고, 킥을 끝내기 전에 온몸이 완전히 수평이 되도록 한다. 또한 종아리 근육은 추진에도 매우 중요하므로 수영 과정 내내 종아리 근육을 계속 구부립니다.\n' +
      '\n' +
      '4. 호흡방법\n' +
      '\n' +
      '수영할 때는 고개를 옆으로 돌려 자연스러운 리듬으로 숨을 쉬고, 얼굴에서 60cm 정도 떨어진 쪽을 바라본다. 또한 얕은 물에서 수영하면서 숨을 내쉬어 입과 코의 공기 유량을 조절할 수 있습니다.\n' +
      '\n' +
      'Back crawl:\n' +
      '\n' +
      '뒤쪽 크롤과 앞쪽 크롤의 주요 차이점은 다리가 교대로 차이지 않고 함께 찬다는 것이다. 후방 크롤은 팔이 전방이 아닌 후방으로 움직이는 것을 제외하면 기본적으로 전방 크롤과 유사하다. 따라서, 한 사람만이 자신의 손을 이용하여 신체의 전체 무게를 운반하고, 다른 사람은 손바닥과 손등에 지지되어 앞으로 나아간다. 호흡법은 다른 사람의 예를 따라 자연스럽게 호흡할 수 있다.\n' +
      '\n' +
      '옐로스톤 국립공원 여행을 중심으로 여행 일기를 만들어 영어로 옐로스톤 국립공원을 추천한다.\n' +
      '\n' +
      '옐로스톤 국립공원은 모든 여행객에게 꼭 가봐야 할 여행지입니다. 그 공원의 자연적 아름다움과 지열 경이로움은 진정으로 경외심을 불러일으킨다. 놀라운 올드 페이스풀 간헐천부터 숨막히는 옐로스톤 그랜드 캐니언까지, 이 국보에서 볼 수 있는 놀라운 광경들이 부족하지 않다.\n' +
      '\n' +
      '옐로스톤에서 가장 인기 있는 명소 중 하나는 올드 페이스풀 간헐천이다. 이 상징적인 간헐천은 90분마다 분출하여 180피트까지 공중으로 물을 뿜어낸다. 그것은 보기에 정말 장관이며, 방문객들은 간헐천 대야를 도는 산책로에서 볼 수 있습니다.\n' +
      '\n' +
      '옐로스톤의 또 다른 하이라이트는 옐로스톤의 그랜드 캐년입니다. 이 거대한 협곡은 길이가 20마일이 넘고 깊이가 2,000피트에 이른다. 협곡에는 아래 협곡으로 308피트 떨어진 인상적인 하부 폭포를 포함한 여러 폭포가 있습니다.\n' +
      '\n' +
      '옐로스톤은 이 두 가지 주요 명소 외에도 온천, 진흙 냄비 및 푸마롤을 포함한 수많은 다른 지열 기능의 본거지이다. 이 독특한 풍경은 세계 어디에서나 볼 수 있는 것과는 다릅니다.\n' +
      '\n' +
      '전반적으로 옐로스톤 국립공원은 미국 서부의 자연미와 지열 불가사의를 경험하고자 하는 모든 사람들에게 꼭 가봐야 할 방문지입니다. 등산, 야생 동물 관찰 또는 멋진 풍경을 감상하는 데 관심이 있든 상관없이 옐로스톤은 모두를 위한 무언가를 가지고 있습니다.\n' +
      '\n' +
      '도 10: InternLM-XComposer2에 의해 생성된 인터리브드 조성물.\n' +
      '\n' +
      '도 11: InternLM-XComposer2와의 채팅.\n' +
      '\n' +
      '도 12: InternLM-XComposer2와의 채팅.\n' +
      '\n' +
      '먼저, 우리는 더 큰 원의 반지름을 찾아야 한다. 직경이 10cm이므로 반지름은 지름의 절반이며, 반지름 = 지름/2 = 10cm/2 = 5cm 다음, 우리는 두 개의 더 작은 원의 넓이를 찾아야 한다. 작은 원의 반지름은 큰 원의 반지름의 절반으로 작은 원의 반지름 = 반지름/2 = 5 cm/2 = 2.5 cm이므로 작은 원의 반지름 = \\(\\pi\\)*(작은 반지름)\\({}^{2}\\)의 넓이 = \\(2.5 cm)\\({}^{2}\\)의 그림자 영역 영역 \\(\\approx\\) 78.54 cm\\({}^{2}\\) - 39.26 cm\\({}^{2}\\)의 그림자 영역 영역 합은 2 * 작은 원의 넓이 = \\(\\approx\\) 39.26 cm\\({}^{2}\\)이다. 첫 번째 다이아몬드 모양은 완전히 단색으로 채워져 있고, 두 번째 다이아몬드 모양도 그 안에 추가 점이 있습니다. 이 패턴을 분석해보면, 상단 행과 하단 행은 같은 유형의 도형을 포함하지만 다른 특징을 갖는 것으로 보인다 - 상단 행은 완전히 채워지고(아마도 다른 색상이나 질감으로), 하단 행은 그 안에 점을 포함한다. 이 정보를 통해 두 번째 열의 결측 이미지는 두 번째 행의 가장 왼쪽 이미지인 고체 채우기를 가진 별 모양의 객체와 유사해야 한다고 추론할 수 있다. 다른 세 개의 이미지가 이미 존재하고 패턴은 다양한 특성을 가진 도형의 반복을 시사하기 때문에 두 번째 열의 이미지는 솔리드 필을 가진 별로 구성될 가능성이 높다.\n' +
      '\n' +
      '도 13: InternLM-XComposer2와의 채팅.\n' +
      '\n' +
      '이미지는 집의 흑백 평면도이며, 객실과 치수의 상세한 레이아웃을 제공합니다. 계획은 왼쪽 하단에 입구가 있고 왼쪽 상단에 테라스가 있습니다. 입구에서 시작하여 오른쪽으로 거실이 있는데, 크기는 146"×205"이다. 거실과 인접해 있는 식당은 910인치 x 130인치 크기입니다. 식사 공간은 주방과 연결되어 있으며 크기는 72" x 92"입니다. 부엌 옆에는 1010" x 147" 크기의 두 번째 침실이 있습니다. 마스터 침실은 제2 침실의 오른쪽에 위치하며 크기가 1210" x 179"입니다. 이 계획에는 60" x 53" 크기의 로비와 72" x 92" 크기의 욕실이 포함됩니다. 평면 왼쪽 상단에 216인치 x 80인치 크기의 테라스가 있습니다. 평면도는 주택의 배치를 종합적으로 볼 수 있어 각 방의 공간 배치와 치수를 파악할 수 있다.\n' +
      '\n' +
      '도 14: InternLM-XComposer2와의 채팅.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>