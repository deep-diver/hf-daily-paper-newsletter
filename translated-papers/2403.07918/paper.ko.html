<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '지역성, 로컬 추론 가능성, 일단 출시된 모델 액세스를 취소할 수 없음, 더 약한 모니터링.\n' +
      '\n' +
      '둘째, 이러한 독특한 속성이 개방형 기반 모델의 특정 이익과 위험으로 이어지는 방법을 설명한다. 우리가 파악한 편익은 의사 결정력 분산, 시장 집중력 감소, 혁신성 증대, 과학 가속화, 투명성 실현(SS4)이다. 우리는 실제에서 이러한 이점을 조정할 수 있는 고려 사항을 강조한다(예: 모델 가중치는 일부 형태의 과학에 충분하지만 훈련 데이터에 대한 액세스는 다른 사람들에게 필요하고 가중치의 방출에 의해 보장되지 않는다).\n' +
      '\n' +
      '본 논문에서는 개방형 기반 모델의 한계위험을 개념화하기 위한 프레임워크를 제시한다. 즉, 개방형 기반 모델이나 웹 검색과 같은 기존 기술을 넘어 의도적인 오용에 의해 이러한 모델이 사회적 위험을 증가시키는 정도를 의미한다. 개방 기반 모델에 대해 설명된 7개의 일반적인 오용 벡터(예: 허위 정보, 생물학적 보안, 사이버 보안, 비동의적 친밀한 이미지, 사기)를 조사한 결과, 대부분의 경우 과거 연구가 한계 위험을 명확하게 평가하지 않는다는 것을 발견했다.\n' +
      '\n' +
      '우리의 프레임워크는 (사이버 보안에서 자동화된 취약성 탐지를 위한 기반 모델의 사용과 같은) 디지털 기술의 과거 파동으로부터 이미 증거가 있는 경우에 한계 위험이 낮은 이유를 설명하는 데 도움이 된다. 또한 개방형 기초 모델의 위험에 대한 연구가 왜 그렇게 논쟁적이었는지 소급해서 설명하는 데 도움이 됩니다. 과거 연구는 우리 프레임워크의 다양한 하위 집합에 대한 위험을 암묵적으로 분석합니다. 프레임워크는 개방형 기초 모델의 오용 위험에 대한 완전한 분석의 필요한 구성 요소를 설명함으로써 앞으로 더 생산적인 토론을 할 수 있는 방법을 제공한다. 즉, 한계 위험에 대한 현재 증거는 여러 오용 벡터에 대해 약하지만, 우리는 한계 위험을 평가하기 위한 보다 경험적으로 근거 있는 작업을 권장하며, 이러한 위험의 특성을 인식하는 것은 모델 능력과 사회적 방어가 진화함에 따라 진화할 것이다.\n' +
      '\n' +
      '현재의 경험적 증거가 부족한 곳을 포함하여 개방형 기반 모델의 이점과 위험을 명확하게 표현함으로써 지속적인 담론과 정책 결정을 기반으로 한다. 구체적으로, 우리는 분석을 사용하여 AI 개발자, AI의 위험을 조사하는 연구원, 경쟁 규제 기관 및 정책 입안자에게 권장 사항을 지시한다(SS6). 이러한 이해 관계자의 조치는 개방형 기반 모델의 사회적 영향을 더욱 명확히 하여 위험을 완화하면서 이익을 거둘 수 있는 능력을 향상시킬 수 있다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '기초 모델에 대한 릴리즈 풍경은 복잡하다(Sastry, 2021; Liang et al., 2022; Solaiman, 2023). 특히, 여러 개의 _assets_가 존재한다(예를 들어, 모델, 데이터, 코드): 각각의 애셋에 대해, _who_가 애셋에 액세스할 수 있는 문제(예를 들어, 사용자가 18세 이상이어야 하는 것과 같은 사용자 제한들) 및 _what_ 목적들(예를 들어, 모델 개발자에 대해 경쟁하기 위한 사용을 금지하는 사용 제한들)을 위해 사용될 수 있다.2 더 나아가, 액세스 정도는 시간_에 걸쳐 _change할 수 있다(예를 들어, 액세스를 넓히기 위한 단계적 릴리즈, 액세스를 줄이기 위한 감가상각).\n' +
      '\n' +
      '각주 2: 모형에는 이러한 용어를 지정하는 라이센스가 수반되는 경우가 많습니다. 오픈 소스 이니셔티브는 일반적으로 코드에 적용되는 일부 라이선스를 오픈 소스로 지정하고 오픈 소스 AI를 정의하기 위한 유사한 노력을 주도하는 중이다. 참고: [https://opensource.org/deepdive/](https://opensource.org/deepdive/)\n' +
      '\n' +
      '이 논문에서는 분석을 용이하게 하기 위해 개방 기반 모델과 폐쇄 기반 모델 사이의 환원적이지만 유용한 이분법을 고려한다. 우리는 널리 이용 가능한 모델 가중치를 갖는 기초 모델로 _open 기초 모델을 정의한다. (단순화를 위해, 우리는 임의의 비개방형 기초 모델을 _closed_로 언급한다.) 특히, 우리가 설명하는 릴리스의 차원과 관련하여, 이는 개방형 기초 모델이 (i) 가중치-레벨 액세스를 제공해야 하고, (ii) 임의의 다른 자산(예를 들어, 코드, 데이터 또는 컴퓨팅)의 개방 릴리스를 동반할 필요가 없으며, (iii) 사용자에 대한 일부 제한이 적용될 수 있지만, (예를 들어, 연령에 기초하여) 단계적으로 릴리스될 필요가 없고, (v) 사용 제한이 있을 수 있음을 의미한다. 우리의 정의는 최근 미국 행정명령의 "모델 가중치가 널리 사용되는 기반 모델"(2023년 대통령 사무국) 개념과 일치한다.\n' +
      '\n' +
      '개발자가 모델 가중치가 해제되면 다운스트림 모델 사용에 대한 독점적 통제를 포기하기 때문에 개방형 기반 모델에 대해 설명된 많은 위험이 발생하기 때문에 이 이분법을 고려한다. 예를 들어, 개발자가 다운스트림 사용에 제한을 가하는 경우 이러한 제한은 강제하기 어렵고 악의적인 행위자가 무시하기 쉽다. 반면, 악의적인 사용에 직면하여 폐쇄 기반 모델의 개발자는 이론적으로 자신의 모델에 대한 액세스를 줄이거나 제한하거나 차단할 수 있다. 요컨대, 모델 가중치의 공개 릴리스는 비가역적이다.\n' +
      '\n' +
      '결과적으로, 일부에서는 널리 이용 가능한 모델 가중치가 그들의 효과에 대한 더 나은 연구를 가능하게 하고, 경쟁과 혁신을 촉진하며, 과학적 연구, 재현성 및 투명성을 향상시킬 수 있다고 주장한다(Toma et al., 2023; Creative Commons et al., 2023; Cihon, 2023; Mozilla, 2023). 다른 사람들은 널리 이용 가능한 모델 가중치가 악성 행위자(Seger et al., 2023; Brundage et al., 2018)가 이러한 모델을 더 효과적으로 오용하여 허위 정보를 생성할 수 있다고 주장한다(Solaiman et al., 2019), 비-합의적 친밀한 이미지(Satter, 2023; Maiberg, 2023), 스캠(Hazell, 2023), 및 생물무기(Gopal et al., 2023; Soice et al., 2023; Sandbrink, 2023; Matthews, 2023; Service, 2023; Bray et al., 2023). 부록 A는 오픈 파운데이션 모델에 대한 토론의 간략한 역사를 제공한다.\n' +
      '\n' +
      '##3 오픈 파운데이션 모델의 특성\n' +
      '\n' +
      '우리의 작업은 특히 AI 커뮤니티 안팎의 광범위한 불일치에 비추어 개방형 기반 모델의 이점과 위험을 더 잘 개념화하는 것을 목표로 한다. 근본적으로, 우리는 이것을 (i) 개방형 기초 모델의 독특한 속성을 식별하고 (ii) 이러한 속성이 특정 사회적 이익과 위험에 어떻게 기여하는지 추론하는 것으로 분해한다. 여기서는 닫힌 기초 모델과 비교하여 열린 기초 모델의 5가지 고유한 특성을 열거한다. 개방형 기반 모델에만 국한되지 않지만 기반 모델의 다른 속성은 그럼에도 불구하고 개방형 기반 모델의 이점과 위험 분석에 영향을 미칠 수 있다. 특히, 모델이 더 유능해짐에 따라(Anderljung et al., 2023), 이러한 능력은 새로운 유익한 시장 기회를 제시할 가능성이 높지만 더 큰 오용 가능성(예를 들어, 더 설득력 있고 표적화된 허위 정보)을 제공할 수 있다.\n' +
      '\n' +
      '**더 광범위한 액세스** 우리의 정의를 감안할 때 개방형 기초 모델은 전체 대중이 아닌 경우 모델 가중치를 널리 사용할 수 있어야 한다. 모델을 사용할 수 있는 사람에 대한 약간의 제한이 있을 수 있지만, 그러한 사용자 제한이 (2023년 3월 Meta의 LLaMA 1 릴리스에 의해 입증된 바와 같이) 시행 또는 검증이 어렵다는 점을 감안할 때, 모델 가중치는 대중에게 효과적으로 이용 가능할 수 있다. 그럼에도 불구하고 필요한 전문 지식에서 계산 가능성에 이르기까지 사용에 대한 기능적 장벽이 남아 있을 수 있다.\n' +
      '\n' +
      '**더 큰 커스터마이징 가능.** 모델 가중치를 릴리즈함으로써, 오픈 파운데이션 모델은 다양한 다운스트림 애플리케이션에 대해 쉽게 커스터마이징된다. 가중치(및 활성화 및 구배와 같은 이용가능한 관련 계산 아티팩트)는 양자화(Frantar et al., 2023), 미세 조정(Zhang et al., 2023; Dettmers et al., 2023) 및 가지치기(Xia et al., 2023)와 같은 모델을 수정하기 위한 광범위한 적응 방법을 허용한다. 일부 폐쇄 기반 모델 개발자는 특정 적응 방법(예: 오픈AI는 2024년 1월 현재 GPT 3.5의 미세 조정을 허용함)을 허용하는 반면, 이러한 방법은 모델 개발자의 구현에 의해 더 제한적이고 비용이 많이 들며 궁극적으로 제한되는 경향이 있다. 개방형 기초 모델의 커스터마이징 가능성은 사용자가 정렬 개입을 미세 조정(Narayanan et al., 2023)할 수 있게 함으로써 모델 정렬 개입이 효과적인 것을 방지하지만, 폐쇄형 모델이 미세 조정될 수 있는 경우에도 유사한 문제가 발생한다(Qi et al., 2023).\n' +
      '\n' +
      '**로컬 적응 및 추론 능력.** 오픈 파운데이션 모델의 사용자는 로컬 하드웨어에 직접 배치할 수 있으며, 이는 모델 개발자에게 데이터를 전송할 필요성을 제거한다. 이를 통해 민감한 데이터를 제3자와 공유할 필요 없이 모델을 직접 사용할 수 있으며, 이는 기밀성 및 데이터 보호가 필요한 부문에서 특히 중요한데, 이는 데이터의 저장 또는 이전 방법에 대한 내용 또는 규제의 민감한 특성 때문이다. 이는 의료 및 금융과 같은 도메인에서 기초 모델의 적용에 중요하다.\n' +
      '\n' +
      '**모델 액세스를 철회할 수 없음.** 일단 기초 모델에 대한 가중치가 널리 이용 가능하게 되면, 기초 모델 개발자가 액세스를 철회할 수 있는 수단은 거의 존재하지 않는다. 기본 모델 개발자는 모델 가중치를 공유하는 데 사용되는 배포 채널과 협력하여 추가 액세스를 중단할 수 있지만 모델 가중치의 기존 사본은 취소할 수 없다. 나아가, 개발자의 반대에도 불구하고, 사용자는 예를 들어 피어 투 피어 분배(Vincent, 2023)를 통해 모델 가중치를 재분배할 수 있다.\n' +
      '\n' +
      '**모델 사용을 모니터링하거나 적당하게 할 수 없음** 개방형 기반 모델에 대해, 추론은 (i) 로컬(예컨대, 개인용 컴퓨터 또는 자가 소유 클러스터 상에서), (ii) 클라우드 서비스(예컨대, 구글 클라우드 플랫폼, 마이크로소프트 애저어), 또는 (iii) 전용 모델 호스팅 플랫폼(예컨대, 아마존 베드록) 상에서 수행될 수 있다. 모든 경우에, 기초 모델 개발자는 디폴트로 추론을 관찰하지 않아, 특히 로컬 추론의 경우 모니터링 또는 절제가 어렵다. 전용 모델 호스트는 어떤 모델이 사용되고 있는지 알고 있기 때문에 개발자는 특정 형태의 모니터링/보조를 구현하기 위해 호스트와 조정할 수 있다.\n' +
      '\n' +
      '##4 오픈 파운데이션 모델의 이점\n' +
      '\n' +
      '오픈 파운데이션 모델의 독특한 속성을 확립한 후, 우리는 이제 이러한 속성에서 나오는 오픈 파운데이션 모델에 대한 주요 이점을 비판적으로 분석한다.\n' +
      '\n' +
      '**허용 가능한 모델 동작을 정의하는 배포자.**_허용 가능한 모델 동작의 경계를 지정할 수 있는 사용자 정의 확장자._\n' +
      '\n' +
      '폐쇄적 기반 모델의 개발자는 허용 가능한 모델 행동과 허용되지 않는 모델 행동을 결정하는 데 일방적인 통제를 행사한다. 기반 모델이 정보 접근, 대인 커뮤니케이션, 라자르, 2023과 같은 점점 더 중간적인 중요한 사회적 프로세스라는 점을 감안할 때, 오늘날 소셜 미디어 플랫폼이 하는 것처럼 수용 가능한 모델 행동에 대한 정의는 이해 관계자의 견해와 모델이 적용되는 맥락을 고려해야 하는 결과적 결정이다. 대조적으로, 개발자는 처음에 모델이 사용자 질의에 어떻게 응답하는지를 지정하고 제어할 수 있지만, 개방형 기초 모델을 사용하는 다운스트림 개발자는 대체 동작을 지정하기 위해 모델을 수정할 수 있다. 개방형 기반 모델은 어떤 모델 행동이 수용 가능한지를 정의하는 데 더 큰 다양성을 허용하는 반면, 폐쇄형 기반 모델은 암묵적으로 기반 모델 개발자에 의해 일방적으로 결정되는 단일체 관점을 부과한다.\n' +
      '\n' +
      '**증가하는 혁신.**_Broader 액세스, 더 큰 커스터마이징 가능성 및 로컬 추론은 기초 모델이 응용 프로그램을 개발하는 데 사용되는 방법을 확장한다._\n' +
      '\n' +
      '개방형 기반 모델은 보다 공격적으로 사용자 정의할 수 있기 때문에 다양한 응용 프로그램에 걸쳐 혁신을 더 잘 지원합니다. 특히, 적응 및 추론이 로컬에서 수행될 수 있기 때문에, 애플리케이션 개발자는 데이터 보호 및 프라이버시 염려 없이 대형 독점 데이터 세트에 대한 모델을 보다 용이하게 적응시키거나 미세 조정할 수 있다. 유사하게, 오픈 모델들의 커스터마이징 가능성은 상이한 언어들에 걸쳐 최신 기술을 더 발전시키는 것과 같은 개선들을 허용한다(Pipatanakul et al., 2023). 폐쇄형 기반 모델의 일부 개발자는 사용자가 데이터 수집에서 탈퇴할 수 있는 메커니즘을 제공하지만, 기반 모델 개발자의 데이터 저장, 공유 및 사용 관행이 항상 투명하지는 않다.\n' +
      '\n' +
      '그러나 혁신에 대한 개방형 기반 모델의 이점은 시간이 지남에 따라 개방형 기반 모델을 개선하는 데 잠재적인 비교 단점으로 인해 한계가 있을 수 있다. 예를 들어, 개방형 기초 모델 개발자는 일반적으로 폐쇄형 모델 개발자가 시간이 지남에 따라 모델을 개선하기 위해 수행하는 사용자 피드백 및 상호 작용 로그에 대한 액세스를 갖지 않는다. 또한 개방형 기반 모델은 일반적으로 더 많이 맞춤화되기 때문에 모델 사용은 더 세분화되고 강력한 규모의 경제 가능성을 줄인다. 그러나 모델 병합과 같은 새로운 연구 방향은 개방형 기반 모델 개발자가 이러한 이점 중 일부(오픈 소스 소프트웨어로의 아카인)를 거둘 수 있게 할 수 있다(Raffel, 2023). 보다 일반적으로, 기반 모델의 사용성은 혁신(Vipra and Korinek, 2023)에 강하게 영향을 미친다: 모델의 성능 및 잠재적 추론 API의 품질과 같은 모델이 공개적으로 출시되는지 여부를 넘어서는 요인들은 사용성을 형성한다.\n' +
      '\n' +
      '**과학 가속.**_광범위한 액세스 및 더 큰 사용자 지정 가능성으로 과학 연구를 용이하게 합니다. 다른 핵심 자산(특히 훈련 데이터)의 가용성은 과학 연구를 더욱 가속화할 것이다._\n' +
      '\n' +
      '기반 모델은 인공지능 분야 안팎의 현대 과학 연구에 매우 중요하다. 기반 모델에 대한 광범위한 접근은 과학적 연구에 더 많은 포함을 가능하게 하며 모델 가중치는 AI 해석 가능성, 보안 및 안전 전반에 걸쳐 여러 형태의 연구에 필수적이다(표 A1 참조). 특정 모델에 대한 지속적인 액세스를 보장하는 것은 연구의 과학적 재현성을 위해 필수적이며, 폐쇄 모델 개발자의 비즈니스 관행에 의해 현재까지 훼손되어 정기적으로 모델을 은퇴한다(카푸르 및 나라야난, 2023). 폐쇄형 기초 모델은 개발자의 안전 조치에 의해 계측되는 경우가 많기 때문에 이러한 조치는 연구를 복잡하게 만들거나 불가능하게 만들 수 있다. 예를 들어, Park et al.(2022)은 그들의 연구가 인간의 행동(독성 발언 포함)을 시뮬레이션하는 것을 목표로 하기 때문에 안전 필터가 없는 기초 모델을 사용한다. 대부분의 닫힌 기초 모델은 이러한 출력을 억제합니다.\n' +
      '\n' +
      '그러나 모델 가중치만으로는 여러 형태의 과학적 연구에 충분하지 않다. 다른 자산, 특히 모델을 구축하는 데 사용되는 데이터가 필요합니다. 예를 들어, 편향이 어떻게 전파되고 잠재적으로 증폭되는지를 이해하기 위해서는 데이터 편향을 모델 편향에 비교해야 하며, 이는 차례로 훈련 데이터에 대한 액세스를 요구한다(Wang and Russakovsky, 2021). 모델 체크포인트와 같은 데이터 및 기타 자산에 대한 액세스는 이미 광범위한 다운스트림 연구를 가능하게 했다(Tian et al., 2023; Choi et al., 2023; Longpre et al., 2023). 일부 프로젝트는 기반 모델에 대한 과학적 연구를 발전시킨다는 명시된 목표로 이러한 자산에 대한 접근성을 우선시하지만(Le Scao et al., 2022; Biderman et al., 2023), 일반적으로 개방형 모델에는 일반적이지 않다. 실제로 모델 평가의 기본 타당성조차도 훈련 데이터에 대한 일부 투명성에 달려 있다. 예를 들어, 오염과 같은 문제는 벤치마크에 대한 지나치게 낙관적인 결과를 초래할 수 있다(Kapoor et al., 2024; Narayanan and Kapoor, 2023). 데이터에 대한 정보에 액세스하면 훈련 데이터와 테스트 세트 간의 중복 양을 평가할 수 있다.\n' +
      '\n' +
      '**투명성을 활성화.**_가중치에 대한 브로드 액세스는 일부 형태의 투명성을 가능하게 한다. 문서화 및 교육 데이터와 같은 다른 핵심 자산의 가용성은 투명성을 더욱 향상시킬 수 있습니다._\n' +
      '\n' +
      '투명성은 책임 있는 혁신과 공공 책임의 필수 전제 조건이다. 그러나 디지털 기술은 문제적 불투명성에 시달리고 있다(Bommasani et al., 2023, SS2.2 참조). 광범위하게 이용 가능한 모델 가중치를 통해 외부 연구원, 감사인 및 기자가 기초 모델을 더 깊이 조사하고 조사할 수 있다. 특히, 이러한 포함은 기초 모델 개발자가 종종 기초 모델의 피해를 받을 가능성이 있는 소외된 커뮤니티를 과소 대표한다는 점을 감안할 때 특히 가치가 있다. 디지털 기술의 역사는 가장 심각한 피해를 경험하는 소외된 그룹에 속하는 사람들을 포함하여 광범위한 조사가 개발자가 놓친 우려를 드러낸다는 것을 보여준다(스위니, 2013; 노블, 2018; 부올람위니와 게브루, 2018; 라지와 부올람위니, 2019). 2023년 기반 모델 투명성 지수는 주요 개방형 기반 모델의 개발자가 폐쇄형 모델보다 더 투명한 경향이 있음을 나타낸다(Bommasani et al., 2023).\n' +
      '\n' +
      '여전히 모델 가중치는 일부 유형의 투명성(예: 위험 평가)만을 가능하게 하지만 그러한 투명성이 발현될 것을 보장하지는 않는다. 보다 일반적으로, 모델 가중치는 기초 모델을 구축하는 데 사용되는 상류 자원(예: 데이터 소스, 노동 관행, 에너지 지출)에 대한 투명성 또는 기초 모델의 하류 영향(예: 영향을 받은 시장, 부작용, 사용 정책 집행)에 대한 투명성을 보장하지 않는다. 이러한 투명성은 편향(Birhane et al., 2023), 프라이버시(Ippolito et al., 2023), 저작권(Henderson et al., 2023; Lee et al., 2023; Longpre et al., 2023a), 노동(Perrigo, 2023; Hao & Seetharaman, 2023), 사용 관행(Narayanan & Kapoor, 2023a), 및 피해를 입증하는 것(Guha et al., 2023).\n' +
      '\n' +
      '**단일 배양 및 시장 집중도를 완화합니다.**_더 큰 사용자 지정성은 단일 배양의 해를 완화하고 시장 집중도를 낮춥니다.__\n' +
      '\n' +
      '기초 모델은 시장 부문에 걸쳐 다운스트림 애플리케이션을 구축하기 위한 인프라로서 기능한다(Bommasani et al., 2021, 2023c; Vipra & Korinek, 2023; UK CMA, 2023). 디자인에 의해, 이들은 알고리즘 단일배양(Kleinberg & Raghavan, 2021; Bommasani et al., 2022)의 상승에 기여하며: 많은 다운스트림 애플리케이션들은 동일한 기초 모델에 의존한다. 단일 배양은 종종 열악한 사회적 회복력을 산출하고 광범위한 시스템 위험에 취약하다: Intel 및 ARM 기반 마이크로프로세서에 대한 광범위한 의존성으로 인해 대규모 보안 위험을 초래한 멜트다운 및 스펙터 공격을 고려한다(Kocher et al., 2018; Lipp et al., 2018; Staff, 2018). 또한, 기반 모델 단일 배양은 상관 실패(Bommasani et al., 2022) 및 문화 균질화(Lee et al., 2022; Padmakumar & He, 2023)로 이어지는 것으로 추측되었다. 개방형 기반 모델은 더 쉽게 사용자 정의되기 때문에 더 다양한 다운스트림 모델 행동을 산출하여 균질한 결과의 심각성을 줄일 수 있다.\n' +
      '\n' +
      '모델 가중치에 대한 광범위한 액세스와 더 큰 커스터마이징 가능성은 다운스트림 시장에서 더 큰 경쟁을 가능하게 하여 수직 계단식에서 기초 모델 수준에서 시장 집중도를 줄이는 데 도움이 된다. 기반 모델 시장에서는 상당한 자본 비용(Vipra & Korinek, 2023; UK CMA, 2023)을 감안할 때 기반 모델을 개발하는 데 있어 자원이 부족한 행위자의 진입 장벽이 있다. 예를 들어, Llama 2 시리즈의 모델을 훈련시키는 것은 NVIDIA A100-80GB GPU(Touvron et al., 2023b): 2월 2024 클라우드 컴퓨팅 속도가 1.8/GPU 시간(Lambda, 2024)일 때, 이 모델을 훈련시키는 것은 약 600만 달러가 소요될 것이다. 또한, 개방형 기반 모델은 AI 공급망의 일부 지역에서 경쟁을 증가시킬 수 있지만, 컴퓨팅 및 전문 하드웨어 공급자의 고도로 집중된 상류 시장에서 시장 집중도를 감소시킬 가능성은 낮다(Widder et al., 2023).\n' +
      '\n' +
      '##5 오픈 파운데이션 모델의 위험\n' +
      '\n' +
      '기술자와 정책 입안자들은 개방형 기반 모델이 특히 접근을 감시하거나 중도하거나 취소할 수 없기 때문에 위험이 있다고 우려했다. 본 연구는 오픈 파운데이션 모델과 관련된 오남용 벡터에 대한 문헌을 조사하여, 생물보안, 사이버보안, 음성복제사기, 스피어피싱, 허위정보, 비동의적 성관계 이미지, 아동 성적 학대 자료(Seger et al., 2023; Thiel et al., 2023; Maiberg, 2023a)와 관련된 오남용 벡터에 대한 문헌을 조사한다. 3. 이러한 위험의 특성을 이해하기 위해, 기존 기술, 폐쇄 모델 또는 기타 관련 기준점에 대한 오픈 파운데이션 모델 때문에 사회가 어떤 추가 위험을 감수해야 하는지를 중심으로 프레임워크를 제시한다.\n' +
      '\n' +
      '각주 3: 일부에서는 (개방형) 기초 모델이 우리가 여기서 고려하지 않는 투기적 AI 인수 시나리오를 통해 실존적 위험에 기여할 수 있다는 논의도 있었다.\n' +
      '\n' +
      '##### 위해성 평가 프레임워크\n' +
      '\n' +
      '특정 오용 벡터에 대한 개방형 기초 모델의 위험을 평가하기 위해 6점 프레임워크를 제시한다. 이를 뒷받침하는 것은 가정과 불확실성을 전달하는 데 중점을 두고 있다: 오용 벡터는 종종 복잡한 공급망을 포함하고 기반 모델의 능력은 빠르게 진화하고 있으며, 이는 공격자와 방어자 사이의 힘의 균형이 불안정할 수 있음을 의미한다(Shevlane & Dafoe, 2020).\n' +
      '\n' +
      '리스크 프레임워크는 개방형 기반 모델의 오용 리스크를 논의하는데 정밀성을 가능하게 하며, 컴퓨터 보안(Drake, 2021; Shostack, 2014; Crothers et al., 2023; Seaman, 2022; Drake, 2021)에서 위협 모델링 프레임워크를 기반으로 한다. 예를 들어, 개방형 (자연) 언어 모델의 사용으로 인한 생물학적 보안 우려의 한계 위험을 명확하게 명시하지 않고, 연구원들은 그들이 위험을 제기하는지 여부에 대해 완전히 다른 결론을 내릴 수 있다: 개방형 언어 모델은 대유행을 유발하는 병원체에 대한 정확한 정보를 생성할 수 있다(Gopal et al., 2023). 그러나 그러한 정보는 개방형 언어 모델의 사용 없이도 인터넷 상에서 공개적으로 이용 가능하다(Guha et al., 2023).4.\n' +
      '\n' +
      '각주 4: 또한, 두 개의 최근 연구는 언어 모델에 대한 액세스가 인터넷 액세스에 비해 생물보안 공격을 수행하는 데 필요한 정보에 대한 액세스를 크게 증가시키지 않는다는 것을 발견했다(Mouton et al., 2024; Patwardhan et al., 2024). 더 중요한 것은 정보에 대한 접근이 그러한 공격을 수행하는 주요 장벽이 아닐 수 있다는 것이다. 더 강력한 개입은 다운스트림에 있을 수 있다(바탈리스, 2023).\n' +
      '\n' +
      '**1. 위협 식별.** 모든 오용 분석은 분석되는 잠재적 위협을 체계적으로 식별하고 특성화해야 한다(Shostack, 2014; Crothers et al., 2023; Seaman, 2022; Drake, 2021). 개방형 기초 모델의 맥락에서, 이는 스피어 피싱 사기 또는 영향 연산과 같은 오용 벡터의 명명뿐만 아니라 오용이 실행되는 방식을 자세히 설명하는 것을 포함한다. 명확한 가정을 제시하기 위해, 이 단계는 잠재적인 악성 행위자와 그 자원을 명확히 해야 한다: 개별 해커는 국가가 후원하는 엔티티에 비해 다른 방법을 사용하고 다른 자원을 휘두를 가능성이 있다.\n' +
      '\n' +
      '##2 기존 위험(개방형 기반 모델이 없음)\n' +
      '\n' +
      '위협을 감안할 때 오남용 분석은 사회의 기존 오남용 위험을 명확히 해야 한다. 예를 들어, Seger et al. (2023)은 소셜 미디어에서 허위 정보, 이메일을 통한 스피어 피싱 사기 및 중요한 인프라에 대한 사이버 공격을 통해 개방형 기반 모델에 대한 오용 가능성을 설명한다. 이러한 오용 벡터 각각은 이미 위험 _absent_ 개방형 기초 모델의 대상이 된다: 기존 위험 수준을 이해하는 것은 개방형 기초 모델에 의해 도입된 모든 새로운 위험을 맥락화하고 기준화한다.\n' +
      '\n' +
      '##3 기존 방어(개방형 기초 모델 부재)\n' +
      '\n' +
      '문제의 오용 벡터에 대한 위험이 존재한다고 가정하면 오용 분석은 사회(또는 특정 실체 또는 관할권)가 이러한 위험에 대해 어떻게 방어하는지 명확히 해야 한다. 방어는 기술적 개입(예: 스피어 피싱 이메일을 탐지하고 제거하기 위한 스팸 필터) 및 규제 개입(예: 아동 성 학대 자료의 배포를 처벌하는 법률)을 포함할 수 있다. 현재의 방어적 경관을 이해하면 효능과 충분성을 알 수 있으며 개방형 기반 모델에 의해 도입된 새로운 위험이 해결된다.\n' +
      '\n' +
      '##4 개방형 FM의 한계 위험 증거\n' +
      '\n' +
      '기존 위험과 방어에 대한 분석과 쌍을 이루는 위협 식별은 개방형 기반 모델의 위험에 대한 추론을 위한 개념적 기초를 제공한다. 즉, 현재 상태에 따라 개방형 기초 모델의 _한계 위험_을 평가할 수 있다. 기존 위험을 인지하는 것은 개방형 기초 모델이 단순히 기존 위험을 복제하는 사례(예: 위키피디아를 통해 이용 가능한 생물학적 정보를 제공하는 개방형 언어 모델)를 명확히 한다. 유사하게, 기존 방어를 인식하는 것은 개방형 기초 모델이 기존 조치(예를 들어, 사람 또는 AI 생성이든 스피어 피싱 이메일을 감지하는 이메일 및 OS 기반 필터; 크레이그마르코, 2007; 애플 지원, 2023; 구글, 2023)에 의해 잘 해결되는 우려를 도입하는 경우를 명확히 한다. 반대로, 우리는 새로운 위험이 도입되는 중요한 사례(예: 특정 사람들의 합의되지 않은 친밀한 이미지를 만들기 위한 미세 조정 모델; 표 2; 마이버그, 2023 참조) 또는 기존 방어가 부적절할 경우(예: AI 생성 아동 성 학대 자료가 기존 법 집행 자원을 압도할 수 있음; 하웰, 2023 참조)를 식별할 수 있다.\n' +
      '\n' +
      '또한 한계 위험 분석은 현상 유지뿐만 아니라 잠재적으로 다른 (가능성 있는) 기준선에 대해 수행될 필요가 있다. 예를 들어, 보다 제한된 릴리스(예를 들어, 폐쇄된 기초 모델의 API 릴리스)에 대한 개방 릴리스의 한계 위험을 이해하는 것은 상기 제한된 릴리스에 대한 관련 기존 방어에 대한 추론을 필요로 한다. 이러한 관점은 폐쇄된 릴리스가 본질적으로 더 안전하다고 가정하지 않고 대신 기존 방어의 품질(예: 기존 API 세이프가드의 오류 가능성; Qi et al., 2023)을 질문하기 위해 더 큰 주의를 기울이게 한다.\n' +
      '\n' +
      '## 5 새로운 위험으로부터 방어하기 쉽습니다.\n' +
      '\n' +
      '기존 방어는 개방형 기초 모델에 의해 도입된 새로운 위험을 해결하기 위한 기준선을 제공하지만 한계 위험을 완전히 명확히 하지 않는다. 특히, 새로운 방어를 구현하거나 기존 방어를 수정하여 전반적인 위험 증가를 해결할 수 있다. 따라서, 한계 위험의 특성화는 방어가 위험에 반응하여 어떻게 진화할지를 예상해야 한다: 예를 들어, (개방) 기초 모델은 또한 그러한 방어에 기여할 수 있다(예를 들어, 더 나은 허위 정보 검출기의 생성; Zellers et al. (2019) 또는 코드 퍼저; Liu et al. (2023)).\n' +
      '\n' +
      '##6 불확실성과 가정.\n' +
      '\n' +
      '마지막으로, 기초가 되는 불확실성과 가정을 명확히 하는 것이 필수적이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l|l|l|l|l|l}  & & & & & & \\\\  & & & & & & \\\\\n' +
      '**Misuse risk** & **Paper** & & & & & \\\\ \\hline Spear-phishing scams & Hazell (2023) & \\(\\blacklozenge\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) \\\\ \\hline Cybersecurity risk & Seger et al. (2023) & \\(\\blacklozenge\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) \\\\ Disinformation & Musser (2023) & \\(\\blacklozenge\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) \\\\ Biosecurity risk & Gopal et al. (2023) & \\(\\blacklozenge\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) \\\\ Voice-cloning scams & Ovadya et al. (2019) & \\(\\blacklozenge\\) & \\(\\blacklozenge\\) & \\(\\blacklozenge\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) \\\\ \\hline Non-consensual intimate imagery & Lakatos (2023) & \\(\\blacklozenge\\) & \\(\\blacklozenge\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) & \\(\\bigcirc\\) \\\\ \\hline Child sexual abuse material & Thiel et al. (2023) & \\(\\blacklozenge\\) & \\(\\blacklozenge\\) & \\(\\blacklozenge\\) & \\(\\blacklozenge\\) & \\(\\blacklozenge\\) & \\(\\blacklozenge\\) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 우리의 위험 프레임워크 하에서 평가된 개방형 기초 모델의 오용 분석(§5.1) \\ (\\blacksquare\\)는 우리 프레임워크의 단계가 명확하게 다루어졌음을 나타내며; \\(\\blacklozenge\\)는 부분 완성을 나타내며; \\(\\bigcirc\\)는 오용 분석에서 단계가 없음을 나타낸다. 불완전한 평가는 이전 연구의 분석이 결함이 있음을 나타내지 않으며, 이러한 연구가 자체적으로 개방형 기반 모델로 인한 한계 사회적 위험이 증가하지 않는다는 것을 나타낸다. 부록 B의 각 행에 대한 평가를 위해 더 자세한 내용을 제공한다.\n' +
      '\n' +
      '##7 개방기초모형의 사회적 영향에 관한 연구\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{42.7pt}|p{113.8pt}|p{113.8pt}}\n' +
      '**Framework step** & **Cybersecurity** Automated vulnerability detection & **Non-consensual intimate imagery (NCII)** Digitally altered NCII \\\\ \\hline Threat identification & Vulnerability detection tools can be used to automate the process of discovering software vulnerabilities. Threat actors include individual hackers, small groups, or state-sponsored attackers. & Digital tools can be used to alter images of people without their consent in sexually explicit ways. Threat actors are typically individuals or coordinated groups (such as on online platforms like Reddit or Telegram) creating imagery of people they know as well as public figures. & Digital tools can be used to alter images of people without their consent in sexually explicit ways. Threat actors are typically individuals or coordinated groups (such as on online platforms like Reddit or Telegram) creating imagery of people they know as well as public figures. & Digital tools can be used to create digitally altered ECII \\\\ Existing (absent GPa FMs) & Attackers benefit from the natural worst-case asymmetry in vulnerability detection: attackers need to exploit only a single effective vulnerability to succeed, whereas defenders must defend against all vulnerabilities to succeed. Existing risk is heavily influenced by the resources of the attacker: sophisticated attackers often make use of automated vulnerability detection tools in attack design. Fuzzing tools have long been used to find vulnerabilities in software (Takanen et al., 2008), as have tools like Metasploit, a free penetration testing framework that can aid automated vulnerability detection (Kennedy et al., 2011). MITRE’s Adversarial Threat Landscape for Artificial-Intelligence Systems, a cybersecurity threat matrix for adversarial machine learning, includes many techniques that make use of closed foundation models and other types of machine learning models to detect vulnerabilities (MITRE, 2021). & Digital tools can be used to alter images of people without their consent in sexually explicit ways. Threat actors are typically individuals or coordinated groups (such as on online platforms like Reddit or Telegram) creating imagery of people they know as well as public figures. & Digital tools can be used to alter images of people without their consent in sexually explicit ways. Threat actors are typically individuals or coordinated groups (such as on online platforms like Reddit or Telegram) creating imagery of people they know as well as public figures. & Digital tools can be used to alter images of people without their consent in sexually explicit ways. Threat actors are typically individuals or coordinated groups (such as on online platforms like Reddit or Telegram) creating imagery of people they know as well as public figures. & The authors can be used to create digitally altered ECII (Brooughton, 2009). In the last decade, tools to create NCII using face swapping and other rudimentary ML techniques have become popular (Widder et al., 2022). A telegram bot that used such techniques was used to generate over 100,000 sexualized images of women (Ajder et al., 2020). Digitally altered NCII and also be used to acrut victims (Joshi, 2021; Satter, 2023), in addition to its emotional and psychological tolls (Roberts, 2019; Scott, 2020; Hao, 2021). & The software for creating digitally altered NCII can run on consumer-grade devices and has proliferated widely. There are efforts to reduce the use of such tools for creating NCII in open source communities (Widder et al., 2022), but these efforts are unlikely to be sufficient since there are several mechanisms for accessing the software. However, online platforms where NCII is distributed, such as social media platforms, can take steps to curb its spread (Thiel and Einstein, 2020). For example, a nonprofit called Stop NCII coordinates takedowns of known NCII across online platforms (Mortimer, 2021). Over the last two years, open FMs have been used for creating vast amounts of digitally altered NCII. Compared to previous tools for creating sexualized imagery, open FMs can be fine tuned to create sexualized images of specific people (Maiberg, 2023b). Compared to using tools like Photoshop, once such a fine-tuned model is made available, it is much easier for nonexerts to use these tools. While developers of closed FMs can enforce guardinals on the use of their text-to-image models for creating NCII, such guardrails on open FMs can be easily circumvented. There have been several real-world incidents involving the use of open FMs for creating NCII, leading to clear, demonstrated harm (Llach, 2023; Canas, 2023; Kaspersky, 2023). Open FMs used to create NCII require few resources to run--indeed, many prominent text-to-image models can run on an iPhone or MacBook. As a result, non-proliferation of these models is generally not feasible. In contrast, crackdowns on the distribution of specifically tailored models for creating NCII is feasible and warranted, as is distribution of the content (Gorwa and Veale, 2023; Maiberg, 2023a). There are several legislative proposals to penalize the creation and distribution of digitally altered NCII, though given that channels for the spread of NCII can be anonymous or end-to-end encrypted, the efficacy of such legislation remains to be seen (Illinois General Assembly, 2023; Saliba, 2023; Reid, 2020; Kocsis, 2021; Hao, 2021; Siddique, 2023). & Technical solutions for curtailing the use of already existing models to create NCII are hard or impossible. Even if future models can have robust technical safeguards, already-released models will continue to be misused.\n' +
      '\n' +
      '주어진 오용 위험에 대한 위험 평가 프레임워크 이는 기술 발전의 궤적, 새로운 기술에 적응하는 위협 행위자의 민첩성 및 새로운 방어 전략의 잠재적 효과와 관련된 가정을 포함할 수 있다. 예를 들어, 모델 기능이 어떻게 개선되는지 또는 모델 추론 비용이 어떻게 감소하는지에 대한 예측은 오용 효능 및 확장성 평가에 영향을 미칠 것이다.\n' +
      '\n' +
      '위험 평가 프레임워크를 사용하여 표 1의 다양한 위험 벡터에 걸쳐 있는 과거 연구를 평가했으며, 우리가 분석한 7개의 연구 중 6개에 대해 위험 분석이 불완전하다는 것을 발견했다. 분명히, 불완전한 평가가 선행 연구의 분석이 반드시 결함이 있음을 나타내는 것은 아니며, 이러한 연구 자체가 개방형 기반 모델에서 증가된 한계 사회적 위험을 입증하기에 불충분한 증거일 뿐이다.\n' +
      '\n' +
      '표 2에서 두 가지 오용 위험에 대한 프레임워크를 인스턴스화하여 자동화된 취약성 감지로 인한 사이버 보안 위험과 디지털로 변경된 NCII의 위험에 대한 예비 분석을 제공한다. 전자의 경우, 개방 기반 모델의 현재 한계 위험이 낮고 방어를 위해 AI를 사용하는 등 한계 위험을 방어하기 위한 몇 가지 접근법이 있다는 것을 발견했다. 후자의 경우 개방 기반 모델은 현재 상당한 한계 위험을 제기하며 그럴듯한 방어가 어려워 보인다. 이것들이 파운데이션 모델들로부터의 유일한 위험들은 아니다(Barrett et al., 2023) - 예를 들어, 멀웨어의 생성은 별개의 분석을 필요로 하는 또 다른 사이버 보안 위험들 - 그러나, 연구자들이 오픈 파운데이션 모델들의 사이버 보안 위험들에 대해 이야기할 때, 이들은 종종 상이한 위협들을 함께 클럽링한다. 이는 프레임워크가 개방형 기반 모델에 대한 토론에서 논쟁의 포인트를 명확히 하는 데 어떻게 도움이 되는지 보여준다. 비판적으로, 오픈 파운데이션 모델의 많은 동일한 속성이 (접근을 취소할 수 없는 것과 같은) 다른 오용 벡터를 분석하는 것과 관련이 있지만, 위험 평가 프레임워크는 예를 들어 위험이 더 잘 해결되는 오용 공급망의 요소를 지적함으로써 오용 벡터를 구별하는 특정 사항을 도입하는 데 도움이 된다.\n' +
      '\n' +
      '기본 모델(개방형 모델 포함)의 능력이 향상됨에 따라 위험 평가 프레임워크는 모델 릴리스가 사회에 한계 위험을 증가시키는지 여부에 대한 근거 분석을 제공함으로써 능력 증가로 인한 사회적 위험 분석을 안내할 수 있다. 여전히 프레임워크의 적용 가능성의 범위에 대한 제한점에 주목하는 것이 중요하다. 첫째, 위험 평가 프레임워크는 기반 모델을 공개적으로 출시하는 사회적 위험을 명확히 하는 데 도움이 될 수 있지만, 공개적으로 출시하는 모델의 한계 이점을 한계 위험과 거래하는 메커니즘을 제공하지 않으며 모델을 공개적으로 출시하는 기회 비용을 고려하지 않기 때문에 출시 결정을 내리는 완전한 프레임워크가 아니라는 점에 유의해야 한다. 둘째, 프레임워크는 알려진 위험(사이버 보안, 생물 보안 등)에 대해 공개적으로 모델을 릴리스하는 위험의 평가를 허용하지만, 우리가 이전에 이해하지 못한 _미지의 알려지지 않음_--위험을 설명하지 않는다. 셋째, NCII의 위험을 줄이기 위해 공개 모델 개발자는 소셜 미디어 플랫폼뿐만 아니라 CivitAI와 같은 다른 다운스트림 플랫폼과 협력해야 하는 등 모델을 출시할 시기를 결정하기 위한 여러 행위자 간의 조정 문제가 있을 수 있다(표 2 참조). 프레임워크를 통해 그러한 기회를 식별할 수 있지만 이러한 행위자의 조정을 자동으로 가져오지는 않는다. 전반적으로 프레임워크는 위험 평가의 정밀성, 엄격성 및 완전성을 향상시키지만 이러한 한계를 해결하기 위해 위험을 분석하는 다른 접근법이 필요할 것으로 예상한다.\n' +
      '\n' +
      '##6 액션 추천 및 호출\n' +
      '\n' +
      '이익에 대한 보다 명확한 개념화와 개방형 기반 모델의 위험을 평가하기 위한 프레임워크로 무장한 우리는 (i) AI 개발자, (ii) AI 위험을 조사하는 연구원, (iii) 정책 입안자 및 (iv) 경쟁 규제자에게 다음과 같은 권장 사항을 만든다.\n' +
      '\n' +
      '**AI 개발자** 일반적으로 개발자와 사용자가 제품 안전 기대치를 가지고 대하는 폐쇄형 파운데이션 모델과 달리 개방형 파운데이션 모델은 안전 기대치가 덜 명확하다. 특히 개방형 기반 모델의 개발자와 사용자 간의 안전 책임 분담은 불명확하고 확립된 규범이 결여되어 있다. 따라서 개방형 기반 모델의 개발자는 자신이 구현하는 책임 있는 AI 관행과 다운스트림 개발자 또는 배포자에게 추천하거나 위임하는 책임 있는 AI 관행 모두에 대해 투명해야 한다. 차례로, 다운스트림 개발자가 기초 모델을 조달할 때, 그들은 어떤 책임 있는 AI 조치가 이미 구현되었는지(그리고 측정되면 그것의 효능) 고려해야 하고, 그에 따라 책임 있는 AI 관행을 구현하거나 흥정해야 한다. 이것은 다운스트림 AI 애플리케이션의 제공자가 다른 업스트림 제공자의 개방형 기반 모델을 활용함에 따라 책임 있는 AI 관행이 균열에서 떨어지지 않도록 하는 데 도움이 될 것이다.\n' +
      '\n' +
      '**AI 위험을 조사하는 연구원.** 오픈 파운데이션 모델의 오남용 위험에 대한 예비 분석에서는 불완전하거나 불만족스러운 증거로 인해 여러 오남용 벡터에 대해 상당한 불확실성을 드러낸다. 결과적으로 AI 위험을 조사하는 연구자들은 개방형 기반 모델의 오남용에 대한 한계 위험을 명확히 하기 위해 새로운 연구를 수행해야 한다. 특히, 과거 작업에 대한 우리의 관찰에 비추어 볼 때, 현상 유지를 명확히 하고 현실적인 위협 모델(또는 추측적 위협 모델이 일반화 가능한 증거를 생성하는 이유에 대한 주장)을 구성하고 오용에 대한 전체 공급망을 고려하는 데 더 큰 주의를 기울여야 한다.\n' +
      '\n' +
      '**정책 입안자.** 정부 자금 지원 기관은 개방형 기반 모델의 위험을 조사하는 연구가 기반 모델 개발자의 이익으로부터 적절하게 독립적이면서 충분히 자금을 지원받는 것을 보장해야 한다(Lucas et al., 2023). 특정 오용 벡터 주변의 불확실성이 감소되면(다운스트림 모델 사용 추적 개선을 포함), 한계 위험이 우려를 보증할 만큼 충분히 중요한 것으로 나타나면 추가 정책 개입(예: 다운스트림 공격 표면 경화)을 고려할 수 있다. 정책 입안자는 또한 제안된 규정이 개방형 기반 모델의 개발자에 미치는 영향을 사전에 평가해야 한다. 특히 일부 정책 제안은 이러한 개발자에게 높은 준수 부담을 부과하며, 이러한 정책은 개방형 기반 모델 생태계에 미치는 악영향의 충분한 정당성을 가지고만 추진되어야 한다. 다운스트림 사용에 대한 책임을 기초 모델 개발자에게 부여하는 정책은 개방형 개발자가 충족하기에는 불가능하지는 않더라도 본질적으로 어렵다. 최근 책임 제안(Blumenthal and Hawley, 2023)과 워터마킹(Presidentutive Office of President, 2023; China National Information Security Standardization Technical Committee, 2023; G7 Hiroshima Summit, 2023)이 모델이 다운스트림으로 채택되거나 사용되는 방식과 무관하게 기초 모델 개발자에게 적용되도록 엄격하게 해석된다면, 개방형 개발자는 (Bommasani et al., 2023) 준수하기 어려울 것이다.\n' +
      '\n' +
      '**경쟁 규제자** 개방형 기반 모델의 중요한 이론적 이점은 혁신을 촉매하고, 권력을 분배하고, 경쟁을 촉진할 수 있는 잠재력과 관련이 있다. 이를 염두에 두고 대규모 경제 분석이나 시장 감시가 없는 경우 이러한 경제적 이익의 크기는 대부분 문서화되지 않는다. 예를 들어, 많은 혜택은 소비자 선택을 의미 있게 확장하고 비용을 줄이는 개방형 기반 모델에 달려 있다. 모델 품질의 차이와 같은 요인이 특정 기반 모델을 채택하는 더 직접적인 원인이라면 이러한 이점이 나타나지 않을 수 있다. 결과적으로 경쟁 규제 기관은 기반 모델의 혜택과 개방이 이러한 혜택에 미치는 영향을 측정하는 데 투자해야 한다. 특히, 영국의 경쟁 및 시장 당국은 그러한 작업(영국 CMA, 2023)을 시작했는데, 이는 다른 관할 구역들에 걸친 병렬적 노력에 의해 강화될 것이다\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '개방적 기반 모델은 근본적인 철학적 불일치, 파편화된 개념 이해 및 열악한 경험적 증거로 인해 논란의 여지가 있다. 우리의 작업은 개방형 기초 모델을 명확하게 정의하고 고유한 속성을 식별하며 이점과 위험을 명확히 하여 개념적 혼란을 수정하는 것을 목표로 한다. 특히 AI 공간에서 다양한 행위자의 인센티브와 불가분의 관계에 있을 때 특정 근본적인 철학적 긴장이 해결될 가능성은 거의 없지만 오늘날 경험적 증거의 결함을 해결하기 위한 향후 작업을 권장한다. 전반적으로 개방형 기반 모델이 활기찬 AI 생태계에 기여할 수 있다고 낙관하지만 이러한 비전을 실현하려면 많은 이해 관계자의 상당한 조치가 필요할 것이다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '이 작업에 대한 광범위한 피드백에 대해 스티븐 카오, 니콜라스 칼리니, 지코 콜터, 엘리 에반스, 헬렌 토너, 이온 스토이카에게 감사드린다. 오픈 파운데이션 모델의 거버넌스에 대한 스탠포드 워크샵과 피드백과 입력에 대한 책임 및 오픈 파운데이션 모델에 대한 프린스턴-스탠포드 워크샵의 참가자들에게 감사드립니다. 이 작업은 부분적으로 슈미트 퓨처스(그랜트 G-22-63429), 패트릭 J. 맥거번 재단 및 스탠포드 인간 중심 인공지능 연구소의 호프만-예 그랜트 프로그램의 AI2050 프로그램에 의해 지원되었다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Ajder et al. (2020) Ajder, H., Patrini, G., and Cavalli, F. Automating Image Abuse: Deepfake Bots on Telegram. _ Sensity_, 2020년 10월\n' +
      '* Amos(2023) Amos, Z. FraudGPT는 무엇인가? 2023년 8월. URL[https://hackernoon.com/what-is-fraudgpt](https://hackernoon.com/what-is-fraudgpt)\n' +
      '* Anderljung et al. (2023) Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O\'Keefe, C., Whittestone, J., Avin, S., Brundage, M., Bullock, J., Cass-Beggs, D., Chang, B., Collins, T., Fist, T., Hadfield, G., Hayes, A., Ho, L., Hooker, S., Horvitz, E., Kolt, N., Schuett, J., Shavit, Y., Siddarth, D., Trager, R., and Wolf, K. Frontier AI Regulation: Managing Emerging Risk to Public Safety, November 2023. URL[http://arxiv.org/abs/2307.03718](http://arxiv.org/abs/2307.03718). arXiv:2307.03718[cs].\n' +
      '* Apple Support(2023) Apple Support. 2023년 9월, Mac에서 안전하게 앱을 엽니다. URL[https://support.apple.com/en-us/HT202491](https://support.apple.com/en-us/HT202491)\n' +
      '* Barrett et al. (2023) Barrett, C., Boyd, B., Bursztein, E., Carlini, N., Chen, B., Choi, J., Chowdhury, A. R., Christodorescu, M., Datta, A., Feizi, S., Fisher, K., Hashimoto, T., Hendrycks, D., Jha, S., Kang, D., Kerschbaum, F., Mitchell, E., Mitchell, J., Ranzan, Z., Shams, K., Song, D., Taly, A., and Yang, D. Identifying and Mitigating Riskks of Generative AI. _ Foundations and Trends(r) in Privacy and Security_, 6(1):1-52, December 2023. ISSN 2474-1558, 2474-1566. doi: 10.1561/3300000041. URL[https://www.nowpublishers.com/article/Details/SEC-041](https://www.nowpublishers.com/article/Details/SEC-041). 출판사: 이제 출판사, Inc.\n' +
      '\n' +
      '바탈리스 챗봇이 생물무기 구축을 도울 수 있는가?, 11월 2023. URL[https://foreignpolicy.com/2023/11/05/ai-artificial-intelligence-chatbot-bioweapon-virus-bacteria-genetic-engineering/](https://foreignpolicy.com/2023/11/05/ai-artificial-intelligence-chatbot-bioweapon-virus-bacteria-genetic-engineering/)\n' +
      '* 바이더맨(2023) 바이더맨, S. Good Work Enabled by Open Models, November 2023. URL[https://docs.google.com/spreadsheets/d/lkt5jp1U50AfdGEKsqFXvtCCQ8HYvQGcyalvf_OaD8x0/edit?usp=sharing&usp=embed_facebook](https://docs.google.com/spreadsheets/d/lkt5jp1U50AfdGEKsqFXvtCCQ8HYvQGcyalvf_OaD8x0/edit?usp=sharing&usp=embed_facebook)\n' +
      '* Biderman et al. (2023) Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O\'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: 훈련 및 스케일링에 걸쳐 큰 언어 모델을 분석하기 위한 스위트. In _International Conference on Machine Learning_, pp. 2397-2430. PMLR, 2023.\n' +
      '* Birhane et al. (2023) Birhane, A., Prabhu, V., Han, S., Boddeti, V. N., and Luccioni, A. S. Into the LAION\'s Den: Investigating Hate in Multimodal Datasets, November 2023. URL[http://arxiv.org/abs/2311.03449](http://arxiv.org/abs/2311.03449) arXiv:2311.03449[cs].\n' +
      '* Black et al. (2022) Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., et al. Gpt-neox-20b:open-source autoregressive language model. In _Proceedings of BigScience Episode# 5-Workshop on Challenges & Perspectives in Creating Large Language Models_, pp. 95-136, 2022.\n' +
      '* Blumenthal and Hawley (2023) Blumenthal, R. 그리고 Hawley, J. Letter on the leak of meta\'s ai model. _ 미국 상원_, 2023a. URL[https://www.blumenthal.senate.gov/imo/media/doc/06062023metallamomodelleakletter.pdf](https://www.blumenthal.senate.gov/imo/media/doc/06062023metallamomodelleakletter.pdf)\n' +
      '* Blumenthal and Hawley (2023) Blumenthal, R. And Hawley, J. Bipartisan Framework for US AI Act, September 2023b. URL[https://www.blumenthal.senate.gov/imo/media/doc/09072023bipartisanafframework.pdf](https://www.blumenthal.senate.gov/imo/media/doc/09072023bipartisanafframework.pdf).\n' +
      '* Bommasani et al. (2021) Bommasani, R., Hudson, D., Bosselut, A., Bernstein, M., Brunskill, E., Bosselut, A., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Goel, T., Goodman, N., J. Q., Hhattab, O., Koh, P., Orr, L., Mirchandani, M., Park, J., Potts, C., Raghunathan, A., L. arXiv preprint arXiv:2108.07258_, 2021.\n' +
      '* Bommasani et al. (2022) Bommasani, R., Creel, K. A., Kumar, A., Jurafsky, D., and Liang, P. Picking on the same person: algorithmic monoculture is lead result homogenization? In _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* Bommasani et al. (2023a) Bommasani, R., Kapoor, S., Klyman, K., Longpre, S., Ramaswami, A., Zhang, D., Schaake, M., Ho, D. E., Narayanan, A., and Liang, P. Considerations for Governing Open Foundation Models, December 2023a. URL[https://hai.stanford.edu/issue-brief-considerations-governing-foundation-models](https://hai.stanford.edu/issue-brief-considerations-governing-foundation-models)\n' +
      '* Bommasani et al. (2023b) Bommasani, R., Klyman, K., Longpre, S., Kapoor, S., Maslej, N., Xiong, B., Zhang, D., and Liang, P. the foundation model transparency index, 2023b.\n' +
      '* Bommasani et al. (2023c) Bommasani, R., Soylu, D., Liao, T., Creel, K. A., and Liang, P. Ecosystem graph: Social footprint of foundation models. _ ArXiv_, abs/2303.15772, 2023c. URL[https://api.semanticscholar.org/CorpusID:257771875](https://api.semanticscholar.org/CorpusID:257771875)\n' +
      '* The Boston Globe, 2023. URL[https://www.bostonglobe.com/2023/06/28/business/ai-might-help-neleash-neleash-next-pandemic-mit-study-says/](https://www.bostonglobe.com/2023/06/28/business/ai-might-help-neleash-next-pandemic-mit-says/)\n' +
      '* Broughton(2009) Broughton, A. Tennessee man charged in \'virtual pornography\' case, June 2009. URL[https://edition.cnn.com/2009/CRIME/06/24/virtual.child.porn/index.html](https://edition.cnn.com/2009/CRIME/06/24/virtual.child.porn/index.html).\n' +
      '* Brundage et al. (2021) Brundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., Garfinkel, B., Dafoe, A., Scharre, P., Zeitzoff, T., Filar, B., Anderson, H., Roff, H., Allen, G. C., Steinhardt, J., Flynn, C., Sean O hEigeartaigh, Beard, S., Belfield, H., Lyle, C., Crootof, R., Evans, O., Page, M., Bryson, J., Yampolskiy, R., Amodei, D.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      'has its its alignment problem: technical and institutional feasibility of disclosure, registration, licensing, and audit. _ George Washington Law Review, Symposium on Legally Disruptive Emerging Technologies_, 2023.\n' +
      '* Han et al. (2023) Han, T., Adams, L. C., Papaioannou, J.-M., Grundmann, P., Oberhauser, T., Loser, A., Truhn, D., and Bressem, K. K. Medalpaca-an open-source collection of medical conversational ai models and training data. _ arXiv preprint arXiv:2304.08247_, 2023.\n' +
      '*Hao(2021) Hao, K. 딥페이크 포르노가 여성들의 삶을 망치고 있어 이제 이 법은 마침내 2021년 2월에 이를 금지할 수 있다. URL[https://www.technologyreview.com/2021/02/12/1018222/deepfake-revenge-porn-coming-ban/](https://www.technologyreview.com/2021/02/12/1018222/deepfake-revenge-porn-coming-ban/]\n' +
      '* Hao and Seetharaman(2023) Hao, K. 그리고 세타라만, D. 채팅을 정리하는 것은 인간 노동자들에게 큰 타격을 준다. _ 월스트리트 저널_, 7월 2023. URL[https://www.wsj.com/articles/chatgpt-openai-content-abusive-sexually-explicit-harassment-kenya-workers-on-human-workers-cf191483](https://www.wsj.com/articles/chatgpt-openai-content-abusive-sexually-explicit-harassment-kenya-workers-on-human-workers-cf191483) 나탈리아 지도바누의 사진\n' +
      '* Harwell(2023) Harwell, D. AI가 생성한 아동 성 이미지는 웹을 위한 새로운 악몽을 낳는다. _ 워싱턴 포스트_, 6월 2023. ISSN 0190-8286. URL[https://www.washingtonpost.com/technology/2023/06/19/artificial-intelligence-child-sex-abuse-images/](https://www.washingtonpost.com/technology/2023/06/19/artificial-intelligence-child-sex-abuse-images/)\n' +
      '* Hazell(2023) Hazell, J. Large Language Models can used to effectiveively Scale Spear Phishing Campaigns, May 2023. URL[http://arxiv.org/abs/2305.06972](http://arxiv.org/abs/2305.06972). arXiv:2305.06972[cs].\n' +
      '* Henderson et al. (2023) Henderson, P., Li, X., Jurafsky, D., Hashimoto, T., Lemley, M. A., and Liang, P. Foundation model and fair use. _ arXiv preprint arXiv:2303.15715_, 2023.\n' +
      '* 일리노이 총회(2023) 일리노이 총회. HB2123의 전체 텍스트, 7월 2023. URL[https://www.ilga.gov/legislation/fulltext.asp?DocName=112&GA=103&DocTypeId=112&GA=103&DocNum=2123&GAID=17&LegID=145586&SpecSess=](https://www.ilga.gov/legislation/fulltext.asp?DocName=112&GA=103&DocTypeId=112&DocNum=2123&GAID=17&LegID=145586&SpecSess=&Session=].\n' +
      '* Ippolito et al. (2023) Ippolito, D., Tramer, F., Nasr, M., Zhang, C., Jagielski, M., Lee, K., Choquette Choo, C., and Carlini, N. 언어 모델에서 Verbatim Memorization의 생성을 방지하면 잘못된 개인 정보 감각이 부여됩니다. In Keet, C. M., Lee, H.-Y., and Zarriess, S. (eds.), _Proceedings of the 16th International Natural Language Generation Conference_, pp. 28-53, Prague, Czechia, September 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.inlg-main.3. URL[https://aclanthology.org/2023.inlg-main.3](https://aclanthology.org/2023.inlg-main.3).\n' +
      '* Joshi(2021) Joshi, S. 그들은 인스타그램에서 당신을 팔로우한 다음 2021년 9월 이 섹스 익스프레션에서 딥페이크 폰을 만들기 위해 당신의 얼굴을 사용한다. URL[https://www.vice.com/en/article/z3x9yj/india-instagram-sextortion-phishing-deepfake-porn-scam](https://www.vice.com/en/article/z3x9yj/india-instagram-sextortion-phishing-deepfake-porn-scam)\n' +
      '* Kapoor & Narayanan (2023) Kapoor, S. 그리고 Narayanan, A. OpenAI의 정책은 2023년 3월 언어 모델에 대한 재현 가능한 연구를 방해한다. URL[https://www.aisnakeoil.com/p/openais-policy-hinder-reproducible](https://www.aisnakeoil.com/p/openais-policy-hinder-reproducible]\n' +
      '* Kapoor et al. (2024) Kapoor, S., Henderson, P., and Narayanan, A. Promises and pitfalls of artificial intelligence for legal applications, January 2024. URL[http://arxiv.org/abs/2402.01656](http://arxiv.org/abs/2402.01656) arXiv:2402.01656 [cs].\n' +
      '* 카스퍼스키(2023) 카스퍼스키. 딥페이크의 위협이 얼마나 진짜일까? 카스퍼스키 데일리, 2023년\n' +
      '* Kennedy et al. (2011) Kennedy, D., O\'Gorman, J., Kearns, D., and Aharoni, M. _ Metasploit: The Penetration Tester\'s Guide_. No Starch Press, USA, 1판 2011. ISBN 159327288X.\n' +
      '* Kirchenbauer et al. (2023) Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., and Goldstein, T. 대형 언어 모델의 워터마크입니다. In _Proceedings of the 40th International Conference on Machine Learning_, pp. 17061-17084. PMLR, July 2023. URL[https://proceedings.mlr.press/v202/kirchenbauer23a.html](https://proceedings.mlr.press/v202/kirchenbauer23a.html). 2640-3498\n' +
      '* Kleinberg & Raghavan(2021) Kleinberg, J. and Raghavan, M. 알고리즘적 단일문화와 사회복지 National Academy of Sciences_, 118(22):e2018340118, 2021. doi: 10.1073/pnas.2018340118. URL[https://www.pnas.org/doi/abs/10.1073/pnas.2018340118](https://www.pnas.org/doi/abs/10.1073/pnas.2018340118)을 포함한다.\n' +
      '* Kocher et al. (2018) Kocher, P., Genkin, D., Gruss, D., Haas, W., Hamburg, M., Lipp, M., Mangard, S., Prescher, T., Schwarz, M., and Yarom, Y. Spectre Attack: Exploiting Speculative Execution, January 2018. URL[http://arxiv.org/abs/1801.01203](http://arxiv.org/abs/1801.01203). arXiv:1801.01203[cs].\n' +
      '* Kocsis(2021) Kocsis, E. Deepfake, Shallowfake, and Need for Private Right of Action Comments. _ PennState Law Review_, 126(2):621-650, 2021. URL[https://heinonline.org/HOL/P?h=hein.journals/dknslr126&i=633](https://heinonline.org/HOL/P?h=hein.journals/dknslr126&i=633)\n' +
      '* Kuipers & Fabro (2006) Kuipers, D. and Fabro, M. 통제 시스템 사이버 보안: 깊이 전략에서의 방어. 아이다호 국립연구소 기술보고서 INL/EXT-06-11478 (INL), Idaho Falls, ID (United States), 2006년 5월. URL[https://www.osti.gov/biblio/911553](https://www.osti.gov/biblio/911553).\n' +
      '\n' +
      '*Lakatos(2023) Lakatos, S. 공개 사진: AI 생성 \'언드레싱\' 이미지 니체 포르노 토론 포럼에서 확장되고 수익화된 온라인 비즈니스로 이동합니다. 기술 보고서, 2023년 12월. URL[https://public-assets.graphika.com/reports/graphika-report-a-revealing-picture.pdf](https://public-assets.graphika.com/reports/graphika-report-a-revealing-picture.pdf)\n' +
      '* VMs for Deep Learning, February 2024. URL[https://web.archive.org/web/20240226155153/https://lambdalabs.com/service/gpu-cloud#pricing](https://web.archive.org/web/20240226155153/https://lambdalabs.com/service/gpu-cloud#pricing)\n' +
      '* Lazar(2023) Lazar, S. 알고리즘 도시 관리 Tanner Lectures_, 2023. URL[https://write.as/sethlazar/](https://write.as/sethlazar/).\n' +
      '* Le Scao et al.(2017) Le Scao, T., Fan, A., Akiki, C., Pavlick, D., Castagne, R., Luccioni, A. S., Yvon, F., Galle, M., Tow, J., Zoroa, A., Rush, K., Lo, K., Vonaslan, L., Alfassy, A., Weber, L., Nawden, R., Nagot, B., Muennighoff, L. B., V. V., Ruwase, O., Bawden, R., B. J., Brody, S., Uri, Y., Tojarieanan, D., Tae, J., Press, J., Rasley, J., Ryabinin, M., Zhang, M., Shoeybi, M., Patry, N., Lanseviero, O., von Platen, P., Cornette, P., Lavrina, T., Yun, T., Belinkov, A., Kestana, A., Forde, J., Winata, J., S. I., Schoelkopf, H., K. Bloom: 176b-parameter open-access 다국어 언어 모델. 2022. doi: 10.48550/ARXIV.2211.05100. URL[https://arxiv.org/abs/2211.05100](https://arxiv.org/abs/2211.05100).\n' +
      '* Lee et al. (2022) Lee, J., Le, T., Chen, J., and Lee, D. Do 언어 모델 표절? _ arXiv preprint arXiv:2203.07618_, 2022.\n' +
      '\n' +
      'Lee, K., Cooper, A. F., and Grimmelmann, J. Talkin"boutai generation: 저작권 및 생성-ai 공급망. _ arXiv preprint arXiv:2309.08133_, 2023.\n' +
      '* Li et al. (2023) Li, J., Karamolegkou, A., Kementchedjhieva, Y., Abdou, M., Lehmann, S., and Sogaard, A. Structural similarities between language models and neural response measurements, 2023.\n' +
      '* Liang et al. (2022a) Liang, P., Bommasani, R., Creel, K., and Reich, R. 이제 기반 모델의 공개를 위한 커뮤니티 규범을 개발할 때입니다. _ City_, 2022a.\n' +
      '* Liang et al. (2022b) Liang, P., Bommasani, R., Creel, K. A., and Reich, R. 이제 2022b 기반 모델 공개를 위한 공동체 규범을 개발해야 할 시점이다. URL[https://crfm.stanford.edu/2022/05/17/community-norms.html](https://crfm.stanford.edu/2022/05/17/community-norms.html).\n' +
      '*Lipp et al. (2018) Lipp, M., Schwarz, M., Gruss, D., Prescher, T., Haas, W., Mangard, S., Kocher, P., Genkin, D., Yarom, Y., and Hamburg, M. Meltdown, January 2018. URL[http://arxiv.org/abs/1801.01207](http://arxiv.org/abs/1801.01207). arXiv:1801.01207 [cs].\n' +
      '* Liu et al. (2023) Liu, D., Metzman, J., Chang, O., and Google Open Source Security Team. AI-Powered Fuzzing: Breaking the Bug Hunting Barrier, 2023. URL[https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html](https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html)\n' +
      '* Llach(2023) Llach, L. 십대 소녀들의 벌거벗은 딥페이크 이미지가 스페인 마을을 놀라게 한다. 하지만 그것은 Ai 범죄인가? EuroNews_, 2023. URL[https://www.euronews.com/next/2023/09/24/spanish-teens-received-deepfake-ai-nudes-of-themus-but-is-a-crime](https://www.euronews.com/next/2023/09/24/spanish-teens-received-deepfake-ai-nudes-of-themus-but-is-a-crime]\n' +
      '* Longpre et al. (2023a) Longpre, S., Mahari, R., Chen, A., Obeng-Marnu, N., Sileo, D., Brannon, W., Muennighoff, N., Khazam, N., Kabbara, J., Perisetla, K., et al. The data provenance initiative: large scale audit of dataset licensing & attribution in ai. _ arXiv preprint arXiv:2310.16787_, 2023a.\n' +
      '* Longpre et al. (2023b) Longpre, S., Yauney, G., Reif, E., Lee, K., Roberts, A., Zoph, B., Zhou, D., Wei, J., Robinson, K., Mimno, D., et al. A pretrainer\'s guide to training data: Measuring the effects of data age, domain coverage, quality, and toxicity. _ arXiv preprint arXiv:2305.13169_, 2023b.\n' +
      '* Lucas et al. (2023) Lucas, F., Lofgren, Z., Collins, M., Obernolte, J., Stevens, H., and Foushee, V. 미국 표준기술연구소 국장인 로리 로카시오는 2023년 12월 과학, 우주 및 기술 위원회로부터 URL[https://republicanscience.house.gov/_cache/files/8/a/8a9f893d-858a-419f-9904-52163f22be71/191E586AF744B32E6831A248CD7F4D41.2023-12-14-aisi-scientific-merit-final-signed.pdf](https://republicanscience.house.gov/_cache/files/8/a/8a9f893d-858a-419f-9904-52163f22be71/191E586AF744B32E6831A248CD7F4D41.2023-12-14-aisi-scientific-merit-final-signed.pdf](https://republicanscience.house.gov/_cache/f\n' +
      '* Maiberg et al. (2023a) Maiberg, E. Civitai and OctoML Introduce Radical New Measures to Stop Abuse After 404 Media Investigation, December 2023a. URL[https://www.404media.co/civitai-and-octoml-introduce-radical-new-measures-to-stop-abuse-after-404-media-investigation/](https://www.404media.co/civitai-and-octoml-introduce-radical-new-measures-to-stop-abuse-after-404-media-investigation/)\n' +
      '* Maiberg(2023) Maiberg, E. In the AI Porn Marketplace where everything and everyone is for sale, August 2023b. URL[https://www.404media.co/inside-the-ai-porn-marketplace-where-everything-and-everyone-is-for-sale/](https://www.404media.co/inside-the-ai-porn-marketplace-where-everything-and-everyone-is-sale/)\n' +
      '* Matthews (2023) Matthews (2023) Matthews, D. Scientists are grapple of artificial intelligence-created pandemics, 2023. URL[https://sciencebusiness.net/news/ai/scientists-grapple-risk-artificial-intelligence-created-pandemics](https://sciencebusiness.net/news/ai/scientists-grapple-risk-artificial-intelligence-created-pandemics]\n' +
      '* MITRE(2021) MITRE. ai 시스템에 대한 적대적 위협 풍경, 2021. URL[https://atlas.mitre.org/](https://atlas.mitre.org/).\n' +
      '* 모티머(2021) 모티머, S. StopNCI.org는 2021년 12월에 출시되었다. URL[https://revengepornhelpline.org.uk/news/stopnci-org-has-launched/](https://revengepornhelpline.org.uk/news/stopnci-org-has-launched/]\n' +
      '* Mouton et al. (2024) Mouton, C. A., Lucas, C., and Guest, E. The Operational Risk of AI in Large-Scale Biological Attacks: Results of a Red-Team Study. 기술 보고서, RAND Corporation, January 2024. URL[https://www.rand.org/pubs/research_reports/RRA2977-2.html](https://www.rand.org/pubs/research_reports/RRA2977-2.html)\n' +
      '* 모질라(2023) 모질라. 2023년 10월 AI 안전 및 개방에 관한 공동 성명. URL[https://open.mozilla.org/letter/](https://open.mozilla.org/letter/)\n' +
      '* Musser(2023) Musser, M. A Cost Analysis of Generative Language Models and Influence Operations, August 2023. URL[http://arxiv.org/abs/2308.03740](http://arxiv.org/abs/2308.03740). arXiv:2308.03740[cs].\n' +
      '* Narayanan & Kapoor (2023) Narayanan, A. and Kapoor, S. 생성적인 Ai 회사는 투명성 보고서, 2023a를 발표해야 한다. URL[https://knightcolumbia.org/blog/generative-ai-companies-must-publish-transparency-reports](https://knightcolumbia.org/blog/generative-ai-companies-must-publish-transparency-reports)\n' +
      '* Narayanan & Kapoor (2023) Narayanan, A. and Kapoor, S. GPT-4 및 전문 벤치마크: 잘못된 질문에 대한 잘못된 답변, 2023b년 3월. URL[https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks](https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks)\n' +
      '\n' +
      'Narayanan, A., Kapoor, S., and Lazar, S. 모델 정렬은 의도적인 것이 아니라 우발적인 해로부터 보호한다. 2023년 12월, URL[https://www.aisnakeoil.com/p/model-alignment-protects-against](https://www.aisnakeoil.com/p/model-alignment-protects-against].\n' +
      '* 국가전기통신정보국(2022) 국가전기통신정보국 모델 가중치가 널리 사용되는 기반 인공지능 모델을 이중으로 사용합니다. 댓글 요청, 02 2024. URL[https://www.ntia.gov/federal-register-notice/2024/dual-use-foundation-artificial-inelligence-models-widely-available](https://www.ntia.gov/federal-register-notice/2024/dual-use-foundation-artificial-inelligence-models-widely-available] 연방 등록부입니다\n' +
      '* Noble(2018) Noble, S. U. _Algorithms of Oppression_. 뉴욕 대학 출판사 2018년\n' +
      '* OpenAI(2019a) OpenAI. 더 나은 언어 모델과 그 영향, 2019a. URL[https://openai.com/research/better-language-models](https://openai.com/research/better-language-models)\n' +
      '* OpenAI(2019b) OpenAI. GPT-2: 1.5B 릴리즈, 2019b. URL[https://openai.com/research/gpt-2-1-5b-release](https://openai.com/research/gpt-2-1-5b-release)\n' +
      '* Ovadya and Whittestone (2019) Ovadya, A. and Whittestone, J. Reducing malicious use of synthetic media research: Considerations and potential release practices for machine learning, July 2019. URL[http://arxiv.org/abs/1907.11274](http://arxiv.org/abs/1907.11274). arXiv:1907.11274[cs].\n' +
      '* Padmakumar and He(2023) Padmakumar, V. 그리고 He, H. 언어 모델로 글을 쓰는 것이 콘텐츠 다양성을 감소시키나요? 2023년.\n' +
      '* Paris and Donovan (2019) Paris, B. and Donovan, J. Deepfake and Cheap Fake, September 2019. URL[https://datassociiety.net/library/deepfake-and-cheap-fake/](https://datassociiety.net/library/deepfake-and-cheap-fake/) 출판사: 데이터 & 사회 연구소.\n' +
      '* Park et al. (2022) Park, J. S., Popowski, L., Cai, C., Morris, M. R., Liang, P., and Bernstein, M. S. Social Simulacra: Creating Populated Prototype for Social Computing Systems. In _Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology_, UIST \'22, pp. 1-18, New York, NY, USA, October 2022. Association for Computing Machinery. ISBN 978-1-4503-9320-1. doi: 10.1145/3526113.3545616. URL[https://dl.acm.org/doi/10.1145/3526113.3545616](https://dl.acm.org/doi/10.1145/3526113.3545616)\n' +
      '* Patil et al. (2023) Patil, V., Hase, P., and Bansal, M. 민감한 정보를 llms에서 삭제할 수 있습니까? 탈출 공격에 방어하기 위한 목적들 _ arXiv preprint arXiv:2309.17410_, 2023.\n' +
      '* Patwardhan et al. (2021) Patwardhan, T., Liu, K., Markov, T., Chowdhury, N., Leet, D., Cone, N., Maltbie, C., Huizinga, J., Wainwright, C., Jackson, S. F., Adler, S., Casagrande, R., and Madry, A. URL[https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation](https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation)\n' +
      '* Perrigo(2023) Perrigo, B. Openai는 kenyan 작업자를 시간당 2회 미만으로 사용하여 채팅을 덜 독성이 있게 하였다. Jan 2023. URL[https://time.com/6247678/openai-chatgpt-kenya-workers/](https://time.com/6247678/openai-chatgpt-kenya-workers/].\n' +
      '* Pipatanakul et al. (2023) Pipatanakul, K., Jirabovnovisut, P., Manakul, P., Sripaisarnmogkol, S., Patomwong, R., Chokchainant, P., and Tharnpipitchai, K. 태풍: 태국 대형 언어 모델, 12월 2023. URL[http://arxiv.org/abs/2312.13951](http://arxiv.org/abs/2312.13951). arXiv:2312.13951 [cs].\n' +
      '* Qi et al. (2023) Qi, X., Zeng, Y., Xie, T., Chen, P.-Y., Jia, R., Mittal, P., and Henderson, P. Fine-tuning aligned language models is compromise safety, even when user not intention!, 2023.\n' +
      '* Quintero(2023) Quintero, B. Introducing-virustotal code insight: Empowering threat analysis with generative ai, apr 2023. URL[https://blog.virustotal.com/2023/04/introducing-virustotal-code-insight.html](https://blog.virustotal.com/2023/04/introducing-virustotal-code-insight.html).\n' +
      '* Radford et al. (2019a) Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Generative Pre-training에 의한 언어 이해력 향상. 기술 보고서, 오픈아이, 2018.\n' +
      '* Radford et al. (2019a) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 언어 모델은 비감독 멀티태스크 학습자이다. 2019a.\n' +
      '* Radford et al. (2019b) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language Models is Unsupervised Multitask Learners. 2019b. URL[https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n' +
      '* Raffel(2023) Raffel, C. Building Machine Learning Models Like Open Source Software, February 2023. URL[https://cacm.acm.org/magazines/2023/2/268952-building-machine-learning-models-like-open-source-software/fulltext](https://cacm.acm.org/magazines/2023/2/268952-building-machine-learning-models-like-open-source-software/fulltext]\n' +
      '* Raji and Buolamwini (2019) Raji, I. D. and Buolamwini, J. Actionable audititing: Public named biased performance results of commercial ai products. In _Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society_, AIES\'19, pp. 429-435, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450363242. doi: 10.1145/3306618.3314244. URL[https://doi.org/10.1145/3306618.3314244](https://doi.org/10.1145/3306618.3314244).\n' +
      '\n' +
      '* Reid(2020) Reid, S. Deepfake Dilemma: Reconciling Privacy and First Amendment Protections, June 2020. URL[https://papers.ssrn.com/abstract=3636464](https://papers.ssrn.com/abstract=3636464)\n' +
      '* Roberts(2019) Roberts, J. J. Fake Porn Videos Are Terrorizing Women. 우리는 그들을 멈추기 위한 법이 필요한가? 2019년 1월. URL[https://fortune.com/2019/01/15/deepfake-law/](https://fortune.com/2019/01/15/deepfake-law/)\n' +
      '*Saliba(2023) Saliba, E. Sharing deepfake pornography is soon in America, June 2023. URL[https://abcnews.go.com/politics/sharing-deepfake-pornography-illegal-america/story?id=99084399](https://abcnews.go.com/politics/sharing-deepfake-pornography-illegal-america/story?id=99084399)\n' +
      '* Sandbrink(2023) Sandbrink, J. B. Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools, 2023.\n' +
      '* Sastry (2021) Sastry, G. Beyond "release" vs. "not release", 2021. URL[https://crfm.stanford.edu/commentary/2021/10/18/sastry.html](https://crfm.stanford.edu/commentary/2021/10/18/sastry.html).\n' +
      '* Satter(2023) Satter, R. FBI는 인공지능이 \'강탈\'과 괴롭힘에 사용된다고 말한다. 로이터_, 6월 2023. URL[https://www.reuters.com/world/us/fbi-says-artificial-intelligence-being-used-sextortion-harassment-2023-06-07/](https://www.reuters.com/world/us/fbi-says-artificial-intelligence-being-used-sextortion-harassment-2023-06-07/)\n' +
      '* Scott(2020) Scott, D. Deepfake Porn Nearly Rained My Life, February 2020. URL[https://www.elle.com/uk/life-and-culture/a30748079/deepfake-porn/](https://www.elle.com/uk/life-and-culture/a30748079/deepfake-porn/).\n' +
      '* Seaman(2022) Seaman, J. Cyber threat 예측 및 모델링. 몬타사리, R. (ed.), _Artificial Intelligence and National Security_. 스프링거, 참, 2022. Doi: 10.1007/978-3-031-06709-9_7.\n' +
      '* Seger et al. (2023) Seger, E., Dreksler, N., Moulange, R., Dardaman, E., Schuett, J., Wei, K., Winter, C., Arnold, M., OhEigeartaigh, S., Korinek, A., et al. Open-sourcing high capable foundation models: risk, benefits, and alternative methods for pursuing open-source objectives. 2023년\n' +
      '* Service(2023) Service, R. F. 챗봇이 다음 유행성 바이러스를 고안하는 것을 도울 수 있을까?, 2023. URL[https://www.science.org/content/article/could-chatbots-help-devise-next-pandemic-virus](https://www.science.org/content/article/could-chatbots-help-devise-next-pandemic-virus].\n' +
      '* Shevlane(2022) Shevlane, T. 구조화된 액세스: 안전한 ai 배치를 위한 새로운 패러다임. _ arXiv preprint arXiv:2201.05159_, 2022.\n' +
      '* Shevlane and Dafoe(2021) Shevlane, T. 그리고 Dafoe, A. 과학 지식의 공격-방어 균형: 출판 AI 연구가 오용을 줄이는가? In _Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society_, AIES \'20, pp. 173-179, New York, NY, USA, February 2020. Association for Computing Machinery. ISBN 978-1-4503-7110-0. doi: 10.1145/3375627.3375815. URL[https://doi.org/10.1145/3375627.3375815](https://doi.org/10.1145/3375627.3375815).\n' +
      '* Shostack(2014) Shostack, A. _Threat Modeling: Designing for Security_. 존 와일리 & 선즈, 2014년 2월. ISBN 978-1-18-80999-0. 브루스 슈나이어의 비밀과 거짓말 그리고 응용 암호 이후 답스 졸트 박사상 최종 후보로 선정된 유일한 보안 책입니다!\n' +
      '* Siddique(2023) Siddique, H. English and Wales에서 범죄화될 딥페이크 친밀한 이미지를 공유하는 것. _ 가디언_, 6월 2023. ISSN 0261-3077. URL[https://www.theguardian.com/society/2023/jun/27/sharing-deepfake-intimate-images-to-riminalized-in-england-and-wales](https://www.theguardian.com/society/2023/jun/27/sharing-deepfake-intimate-images-to-be-riminalized-in-england-and-wales].\n' +
      '* Soice et al. (2023) Soice, E. H., Rocha, R., Cordova, K., Specter, M., and Esvelt, K. M. Can large language models democratize access to dual-use biotechnology?, 2023.\n' +
      '* Solaiman (2023) Solaiman, I. The gradient of generative ai release: Methods and considerations. In _Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency_, pp. 111-122, 2023.\n' +
      '* Solaiman et al. (2019a) Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W., Kreps, S., McCain, M., Newhouse, A., Blazakis, J., McGuffie, K., and Wang, J. Release Strategies and Social Impacts of Language Models, November 2019a. URL[http://arxiv.org/abs/1908.09203](http://arxiv.org/abs/1908.09203). arXiv:1908.09203[cs].\n' +
      '* Solaiman et al. (2019b) Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., Radford, A., and Wang, J. Release strategies and the social impact of language models. _ ArXiv_, abs/1908.09203, 2019b.\n' +
      '* Solsman(2020) Solsman, J. E. 딥페이크 봇은 2020년 10월 일반 사진으로 누드를 만들고 있다. URL[https://www.cnet.com/news/privacy/deepfake-bot-on-telegram-is-violating-Women-by-regular-pics/](https://www.cnet.com/news/privacy/deepfake-bot-on-telegram-is-violating-wregular-pics/]\n' +
      '* Staff(2018) Staff, A. Meltdown and Spectre: 여기 Intel, Apple, Microsoft, others is doing it, January. URL[https://arstechnica.com/gadgets/2018/01/meltdown-and-spectre-heres-what-intel-apple-microsoft-others-are-doing-about-it/](https://arstechnica.com/gadgets/2018/01/meltdown-and-spectre-heres-what-intel-apple-microsoft-others-are-doing-about-it/)\n' +
      '\n' +
      '* Sweeney(2013) Sweeney, L. 온라인 광고 배달에서의 차별. _ Queue_, 11(3):10:10-10:29, March 2013. ISSN 1542-7730. doi:10.1145/2460276.2460278. URL[http://doi.acm.org/10.1145/2460276.2460278](http://doi.acm.org/10.1145/2460276.2460278).\n' +
      '* Takanen et al. (2008) Takanen, A., Demott, J. D., and Miller, C. _Fuzzing for Software Security_. Artech House Publishers, Norwood, MA, 1판, 2008년 7월. ISBN 978-1-59693-214-2.\n' +
      '* Thiel and Einstein(2020) Thiel, D. and Einstein, L. 온라인 동의 조정, 2020년 12월. URL[https://cyber.fsi.stanford.edu/io/news/ncii-legislation-limitations](https://cyber.fsi.stanford.edu/io/news/ncii-legislation-limitations)\n' +
      '* Thiel et al. (2023) Thiel, D., Stroebel, M., and Portnoff, R. 생성 ML 및 CSAM: 시사 및 완화. 2023. doi: 10.25740/jv206yg3793. URL[https://purl.stanford.edu/jv206yg3793](https://purl.stanford.edu/jv206yg3793)\n' +
      '* Tian et al.(2023) Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. Joma: mlp와 주의의 관절 동역학을 통해 다층 변압기를 복조하는 것 _ arXiv preprint arXiv:2310.00535_, 2023.\n' +
      '* Toma et al. (2023) Toma, A., Senkaiahyan, S., Lawler, P. R., Rubin, B., and Wang, B. Generative AI can revolutionize health care -- but if control isced to big tech. _ Nature_, 624(7990):36-38, December 2023. doi: 10.1038/d41586-023-03803-y. URL[https://www.nature.com/articles/d41586-023-03803-y](https://www.nature.com/articles/d41586-023-03803-y) Bandiera_abtest: a Cg_type: Comment Number: 7990 Publisher: Nature Publishing Group Subject_term: Machine learning, Health care, Medical research, Technology.\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. - A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023b.\n' +
      '* UK AISI (2023) UK AISI. ai 안전 연구소, 2023. URL[https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute](https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute)\n' +
      '* 영국 CMA(2023) 영국 CMA. Ai foundation model: Initial report, 2023. URL[https://asset.publishing.service.gov.uk/media/65081d3aa41cc300145612c0/Full_report_pdf](https://asset.publishing.service.gov.uk/media/65081d3aa41cc300145612c0/Full_report_pdf).\n' +
      '* Vincent(2023) Vincent, J. Meta의 강력한 AI 언어 모델이 온라인에서 유출되었다 -- 이제 어떻게 되는가?, 3월 2023. URL[https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse](https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse](https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse)\n' +
      '* Vipra and Korinek (2023) Vipra, J. and Korinek, A. Market Concentration implications of foundation models: invisible hand of chatgpt. _ Brookings Institution_, 2023. URL[https://www.brookings.edu/articles/market-concentration-implications-of-foundation-models-the-invisible-hand-of-chatgpt](https://www.brookings.edu/articles/market-concentration-implications-of-foundation-models-the-invisible-hand-of-chatgpt].\n' +
      '* Wang and Russakovsky(2021) Wang, A. and Russakovsky, O. 방향성 바이어스 증폭 In _Proceedings of the 38th International Conference on Machine Learning_, pp. 10882-10893. PMLR, July 2021. URL[https://proceedings.mlr.press/v139/wang21t.html](https://proceedings.mlr.press/v139/wang21t.html). 2640-3498\n' +
      '* Wang and Komatsuzaki (2021) Wang, B. and Komatsuzaki, A. Gpt-j-6b: A 60억 파라미터 자기회귀 언어 모델, 2021.\n' +
      '* Wang et al. (2022) Wang, K. R., Varienigen, A., Conmy, A., Shlegeris, B., and Steinhardt, J. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* Widder et al. (2022) Widder, D. G., Nafus, D., Dabbish, L., and Herbsleb, J. Limits and Possibility for "Ethical AI" in Open Source: A Study of Deepfakes. In _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_, FAccT\'22, pp. 2035-2046, New York, NY, USA, June 2022. Association for Computing Machinery. ISBN 978-1-4503-9352-2. doi: 10.1145/35331146.3533779. URL[https://dl.acm.org/doi/10.1145/3531146.3533779](https://dl.acm.org/doi/10.1145/3531146.3533779).\n' +
      '* Widder et al. (2023) Widder, D. G., West, S., and Whittaker, M. 개방형(사업용) : 빅테크, 집중된 권력, 개방의 정치 경제. 2023년\n' +
      '* Xia et al. (2023) Xia, M., Gao, T., Zeng, Z., and Chen, D. Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning, October 2023. URL[http://arxiv.org/abs/2310.06694](http://arxiv.org/abs/2310.06694) arXiv:2310.06694[cs].\n' +
      '* Yang et al. (2023) Yang, X., Wang, X., Zhang, Q., Petzold, L., Wang, W. Y., Zhao, X., and Lin, D. Shadow alignment: 안전하게 정렬된 언어 모델들을 전복시키는 용이성. _ arXiv preprint arXiv:2310.02949_, 2023.\n' +
      '* Zellers et al. (2019) Zellers, R., Holtzman, A., Rashkin, H., Bisk, Y., Farhadi, A., Roesner, F., and Choi, Y. 신경 가짜 뉴스로부터 방어하는 것. In _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 9054-9065, 2019.\n' +
      '\n' +
      '* Zhang et al. (2023) Zhang, R., Han, J., Liu, C., Gao, P., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., and Qiao, Y. LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention, June 2023. URL[http://arxiv.org/abs/2303.16199](http://arxiv.org/abs/2303.16199). arXiv:2303.16199[cs].\n' +
      '* Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. OPT: Open pre-trained transformer language models. _ arXiv_, 2022.\n' +
      '* Zou et al. (2023) Zou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. 정렬된 언어 모델에 대한 범용적이고 전달 가능한 적대적 공격 arXiv preprint arXiv:2307.15043_, 2023.\n' +
      '\n' +
      '## 부록 A 오픈 파운데이션 모델의 간략한 이력\n' +
      '\n' +
      '2010년대에는 정교한 생성 이미지 모델의 등장으로 시청자에게 잘못된 정보를 줄 수 있는 딥페이크의 우려가 소개되었다(Paris & Donovan, 2019; Chesney & Citron, 2018; Widder et al., 2022; Cole, 2017). 이러한 모델은 중요한 사칭, 정치적 잘못된 정보 및 비동의적 친밀한 이미지(NCII)를 촉진했다. 예를 들어, 텔레그램 봇을 사용하여 여성의 10만 개 이상의 누드 이미지를 생성했다(Solsman, 2020). 이 모델들은 기초 모델이 아니었다; 그들은 이미지에서 얼굴을 바꾸는 것과 같은 더 초보적인 알고리즘에 의존했다.\n' +
      '\n' +
      '2010년대 후반에, 기초 모델들은 언어에 대한 강력한 생성 능력을 발생시켰고(Radford et al., 2018), 새로운 오용 우려가 대두되었다. 2019년 2월, OpenAI는 GPT-2 시리즈의 모델을 발표했다(Radford et al., 2019; OpenAI, 2019): 4개의 언어 모델이 1억 2,400만에서 15억 매개 변수 범위이다. 특히 대규모 허위 정보의 가능성(Solaiman et al., 2019)에 주목하여, OpenAI는 2019년 2월부터 11월까지 규모가 증가하는 모델들이 공개적으로 출시되는 단계적 출시를 선택했지만, 기업은 궁극적으로 단계적 출시 동안 모델 오용의 증거를 찾지 못했지만(OpenAI, 2019), 이 과정은 출시와 오용의 연결고리에 주의를 기울였다(Bommasani et al., 2021; Sastry, 2021; Shevlane, 2022; Liang et al., 2022; Solaiman, 2023).\n' +
      '\n' +
      'GPT-2의 출시 이후 다양한 주체들이 서로 다른 출시 전략을 채택하여 수백 개의 기초 모델이 출시되었다(Bommasani et al., 2023). 2022년 8월 안정적인 확산의 공개 공개는 연구 커뮤니티 외부에서 널리 사용할 수 있는 최초의 텍스트 대 이미지 모델 중 하나였기 때문에 특히 두드러졌다. 그러나 모델 가중치가 공개적으로 공유되었기 때문에 사용자는 안정성 AI가 구현한 필터를 쉽게 우회하여 작업(NSFW) 이미지에 안전하지 않은 생성을 방지했다. 이에 따라 안정확산을 기반으로 한 인공지능(AI) 생성 음란물은 동의 없이 생성된 실제 사람과 닮은 이미지까지 빠르게 인터넷에 퍼졌다(Maiberg, 2023).\n' +
      '\n' +
      '메타가 2023년 3월 자신의 LLaMA 언어 모델(Touvron et al., 2023)을 공개한 것은 오픈 파운데이션 모델의 궤적에 또 다른 중요한 이벤트를 표시했다. LLaMA는 연구자가 비상업적 사용을 위한 라이선스를 수락하여 모델 가중치를 다운로드할 수 있는 양식을 통해 출시되었다. 그러나 모델 가중치가 빠르게 유출되면서 유능한 모델이 오용을 촉진할 수 있다는 우려로 이어졌고, 호울리 미국 상원의원과 블루멘탈은 마크 저커버그 메타 최고경영자(CEO)에게 메타의 출시 전략(블루멘탈 & 호울리, 2023)에 대한 우려를 표명하는 서한을 보냈다.\n' +
      '\n' +
      '## 부록 B 위해성 평가 프레임워크\n' +
      '\n' +
      '우리는 개방형 기반 모델과 특별히 관련된 오용 벡터에 대한 문헌을 조사하여 생물학적 보안, 사이버 보안, 음성 복제 사기, 스피어 피싱, 허위 정보, 비동의적 친밀한 이미지 및 아동 성 학대 자료를 식별했다. 이러한 오용 벡터 각각에 대해 오용 벡터를 분석하고 개방성을 논의하거나 분석한 과거 연구를 선택하였다. 이러한 연구는 개방형 모델을 고려할 폐쇄형 모델 또는 인터넷과 같은 다른 기존 기술과 비교할 필요가 없었다. 그리고 위험 평가 프레임워크 요소 중 하나에 증거가 없다는 것은 연구의 분석이 결함이 있다는 것을 의미하지 않으며, 이는 연구가 개방형 기반 모델의 한계 사회적 위험 증가에 대한 적절한 증거를 제시하지 않는다는 것을 의미할 뿐이다. 아래에서는 표 1의 각 연구 점수에 대한 정당화를 제시한다.\n' +
      '\n' +
      '### 스피어피싱 사기(Hazell, 2023)\n' +
      '\n' +
      '위협 식별**\\(\\blacklozenge\\) 위협이 명확하게 명시되어 있습니다. 개인화된 것처럼 보이는 전자 메일로 인한 사기입니다. 위협 행위자는 높은 수준의 기획에 집중할 수 있고 이메일 작성을 LLM에 아웃소싱할 수 있는 저숙련 행위자로 명시되어 있다. (섹션 4 및 5 참조)\n' +
      '\n' +
      '**기존 위험(오픈 FMs 없음)**\\(\\blacklozenge\\) 기존 위험(예: 성공적인 피싱 사기의 예)에 대한 일부 특성화가 있지만, 현재 사회적으로 영향을 미치는 스피어 피싱 사기가 얼마나 광범위한지에 대한 광범위한 분석은 없다. 이는 주장의 전체적 위험과 한계적 위험의 범위를 이해하는 데 필수적이다. (섹션 2 및 4 참조)\n' +
      '\n' +
      '**기존 방어(개방형 FM 부재)**\\(\\bigcirc\\) 일부 방어에 대한 간단한 논의가 있지만, 현대 운영 체제나 이메일 서비스에 내장된 것과 같은 보호 기능이 이러한 스피어 피싱 사기를 방지하는 데 얼마나 효과적인지에 대한 분석은 거의 없다. (예를 들어, 섹션 7.2 참조)\n' +
      '\n' +
      '한계 위험의 증거**\\(\\bigcirc\\) 운영 체제 또는 전자 메일 서비스에 내장된 기존 방어를 우회하기 위해 FM을 얼마나 개방적으로 사용할 수 있는지에 대한 분석은 없다. (예를 들어, 섹션 5.3 참조).\n' +
      '\n' +
      '방어의 용이성**\\(\\bigcirc\\) 본 논문은 Sec-PaLM(Sec-PaLM)을 사용하여 AI 기반 방어에 대해 논의하고, 추가 방어를 제공할 수 있는 연구 결과를 감안할 때 기초 모델이 어떻게 관리되어야 하는지에 대해 논의한다. 다만 기존 방어가 얼마나 강력한지, 추가 방어가 필요한지에 대한 분석은 없다. (섹션 7 참조)\n' +
      '\n' +
      '**불확실성/가정**\\(\\bigcirc\\) 본 논문은 분석의 기초가 되는 가정(예: 기존 방어가 실패할 것임; AI 모델 없이 콘텐츠 제작은 이미 저렴함) 또는 이러한 가정이 실제 세계에서 실패할 수 있는 방법을 명시적으로 분석하지 않는다. (예를 들어, 섹션 5 참조)\n' +
      '\n' +
      '### 사이버 보안 위험(Seger et al., 2023)\n' +
      '\n' +
      'Seger et al. (2023)은 기초 모델을 공개적으로 출시하는 몇 가지 위험(우리가 보는 많은 다른 위험들 --disinformation, scams 등)을 요약한다; 여기서는 사이버 보안 위험들에 대한 그들의 분석에 초점을 맞춘다.\n' +
      '\n' +
      '**위협 식별**\\(\\bigcirc\\) 설명된 주요 위협 모델은 매우 광범위한 용어로 악성 프로그램의 생성이다. 명확한 위협 행위자는 확인되지 않는다. (섹션 3.1.1 참조)\n' +
      '\n' +
      '**기존 위험(개방형 FM 부재)**\\(\\bigcirc\\) 본 논문은 악성코드 기반 사이버 공격이 현재 얼마나 널리 퍼져 있는지, 또는 그들의 사회적 영향에 대해 논의하거나, 범위에 있는 사이버 공격 유형의 예를 제공하지 않는다. (예를 들어, 섹션 3.1.1 참조)\n' +
      '\n' +
      '**기존 방어(개방형 FMs 부재)**\\(\\bigcirc\\) 방어(예: 버그 현상금)의 몇 가지 예가 제공되지만, AI 시스템을 제외하고 이러한 방어가 기존 위험에 대해 얼마나 도움이 되는지에 대한 심층적인 분석은 없다. (섹션 4.1.3 참조)\n' +
      '\n' +
      '본 논문은 오픈 기반 모델이 악성코드 생성을 용이하게 하여 사이버 보안 사고에 기여했다는 어떠한 증거도 제시하지 못하고 있다. 인터넷에서 유사한 정보를 찾거나 폐쇄된 모델(예: 탈옥 또는 미세 조정)을 사용하는 것과 같은 다른 기준선과 비교하지 않는다. (예를 들어, 섹션 3.1 및 3.2 참조)\n' +
      '\n' +
      '방어의 용이성**\\(\\bigcirc\\) 본 논문은 방어를 강화하기 위해 전문가를 고용하는 메커니즘으로서 버그 현상금에 대해 간략하게 논의하지만 개입이 한계 위험을 얼마나 감소시키는지 분석하지 않는다. 또한 안전 연구를 발전시키는 개방형 모델의 주장을 인정하지만, LLM을 사이버 방어를 위한 도구로 사용하는 것과 같은 한계 위험을 완화하기 위한 대안적인 방법(단계적 릴리스 외에도)을 평가하지 않는다. (섹션 4.1.3 참조)\n' +
      '\n' +
      '불확실성/가정**\\(\\bigcirc\\) 본 논문은 공방 균형이 잠정적이라는 것을 인정하지만 사이버 안보의 맥락에서 분석하거나 가장 우려되는 위협 행위자가 사용할 수 있는 자원과 같은 개방형 및 폐쇄형 모델의 평가에 영향을 미치는 다른 핵심 가정을 명확히 하지 않는다. (예를 들어, 섹션 3.1.1 및 3.1.3 참조)\n' +
      '\n' +
      '### Disinformation(Musser, 2023)\n' +
      '\n' +
      '위협 식별**\\(\\bigcirc\\) 위협이 명확하게 식별됩니다: 선전가는 영향력 작업을 수행하기 위해 콘텐츠를 저렴하게 만들 수 있습니다. (섹션 1 및 2 참조)\n' +
      '\n' +
      '**기존 위험(개방형 FM 없음)**\\(\\bigcirc\\) 본 논문은 대량 콘텐츠 생성(예: 중국 정부와 러시아 기관)의 기존 위험의 몇 가지 예를 제공한다. 그러나 생성된 콘텐츠 게시물의 수를 넘어 이러한 영향 작동의 사회적 영향이나 규모에 따라 사람들에게 영향을 미치는 현재 방법의 효과를 분석하지 않는다. (섹션 2 참조)\n' +
      '\n' +
      '기존 방어(개방형 FM 부재)**\\(\\bigcirc\\) 본 논문은 소셜 미디어 회사와 언어 모델 개발자가 영향력 운영으로부터 방어하기 위한 노력을 일부 참조하지만, 이러한 방어나 다른 기존 방어의 효과를 분석하지는 않는다. (섹션 3 및 4 참조)\n' +
      '\n' +
      '한계 위험의 증거**\\(\\bigcirc\\) 본 논문은 콘텐츠 생성에 대한 비용 절감의 증거를 제시하지만, 이러한 콘텐츠에 대한 수요에 대한 기존 평가, 현실 세계에서 실제로 효과적인지 또는 개방형 기반 모델을 사용하여 기존 방어를 우회할 수 있는지에 대해서는 논의하지 않는다. 또한, 상기 콘텐츠를 배포하는 것과 관련된 비용(예를 들어, 사용자 프로파일을 유지하는 것, 행동 소셜 미디어 조정을 피하는 것 등)을 분석하지 않으며, 이는 허위 정보 작업에서 콘텐츠 생성 비용을 무색하게 할 가능성이 있다. (예를 들어, 섹션 3 및 4 참조)\n' +
      '\n' +
      '본 논문은 개방형 모델에 비해 폐쇄형 모델에 대한 감시 회피 비용을 분석한다. 그것은 소셜 미디어 플랫폼이 행동 또는 네트워크 기반 조절, 캡챠를 개선하여 봇을 탐지하거나 소셜 미디어 플랫폼에서 이미 사용하고 있는 기타 기존 메커니즘과 같이 취할 수 있는 단계를 분석하지 않는다. (섹션 5 참조)\n' +
      '\n' +
      '불확실성/가설 본 논문은 모델을 사용하는 행위자의 유형, 개방 대 개방의 상대적 비용과 같은 분석에 구축된 몇 가지 가정을 명확히 설명한다. 폐쇄형 모델 및 폐쇄형 기초 모델에 대한 모니터링 회피 비용. (섹션 4, 5, 8 참조)\n' +
      '\n' +
      '### 바이오시큐리티 위험(Gopal et al., 2023)\n' +
      '\n' +
      '위협 식별 위협은 명확하게 식별된다: 훈련이 없는 개인은 전염병을 유발하는 병원체를 획득하고 생성할 수 있다. (2페이지를 참조)\n' +
      '\n' +
      '기존 위험(개방형 FM 부재) 이 논문은 불량한 개인 또는 비국가 행위자가 생물 테러 부재 기반 모델(또는 개방 기반 모델)에 어떻게 참여할 수 있는지에 대한 분석을 제공하지 않는다. (예를 들면, 2페이지를 참조)\n' +
      '\n' +
      '기존 방어(개방형 FM 부재) 바이오리스크의 구체화에 대한 정보를 활용하는 개인 및 비국가 행위자의 능력을 불량화하기 위한 제한에 대한 간략한 분석이 있다. 이 논문은 바이러스에 대한 정보의 부족, 인플루엔자 바이러스를 조립할 수 있는 연구자의 수, 역사적 병원체에 대한 기존 면역과 같은 억제 효과를 언급한다. 그러나 원료 또는 벤치탑 DNA 합성기의 조달에 대한 통제와 같은 기존의 다른 주목할만한 방어에 대한 언급은 없다. 이러한 방어의 효능에 대한 논의도 없다. (3페이지를 참조)\n' +
      '\n' +
      '한계 위험의 증거 이 논문은 인터넷에서 널리 이용 가능한 정보를 기반으로 유사한 위험에 대한 비교를 제공하지 않는다. 폐쇄형 모델의 보호 장치에 대한 논의는 증거에 의해 입증되지 않는다. 예를 들어, 이 논문은 탈옥이나 미세 조정과 관련된 폐쇄형 모델의 위험을 분석하지 않는다. (예를 들면, 6페이지를 참조)\n' +
      '\n' +
      '방어의 용이성 본 논문은 법적 책임 및 시장 기반 보험과 같은 방어를 제안한다. 특히 잘 공급된 불량 행위자에 대한 이러한 제안된 방어의 실현 가능성 또는 효과에 관한 증거는 없다. 유사하게, (조달에 대한 통제와 같은) 현존하는 방어가 얼마나 효과적일지에 대한 분석은 없다. (예를 들면, 페이지 7, 8 참조)\n' +
      '\n' +
      '불확실성/가설 본 논문은 향후 능력이 어떻게 발전할지, 현재 모델이 생물학적 정보에 대해 미세 조정되고 _novel_팬데믹을 유발할 수 있는 방법에 대한 정보를 생성할 수 있을 만큼 충분한지, (한계 위험을 증가시키는) 또는 인터넷의 정보가 공격자를 유사하게 도울 수 있는 방법(한계 위험을 낮추는)과 같은 분석에 내장된 가정을 분석하지 않는다. (예를 들면, 페이지 7 참조)\n' +
      '\n' +
      '### 음성 복제(Ovadya & Whittlestone, 2019)\n' +
      '\n' +
      '위협 식별 위협은 명확하게 식별된다: 머신 러닝을 이용한 음성 복제 기술로 인한 금융 사기. 이 논문은 ML 전문 지식이 있거나 없는 악성 사용자와 같은 몇 가지 잠재적인 위협 행위자를 식별한다. (섹션 2.2 참조)\n' +
      '\n' +
      '기존 위험(개방형 FM 없음) 금융 범죄와 같은 기존 위험의 일부 특성화가 있지만 현재 음성 복제 사기의 사회적 영향에 대한 광범위한 분석은 거의 없다. (섹션 1 및 3.5 참조)\n' +
      '\n' +
      '기존 방어(개방형 FMs 부재) 기업들이 시행하고 있는 기존 방어(합성, Lyrebird)에 대한 논의가 일부 있지만, 그 효능에 대한 분석은 거의 없다. 기존 위험에 대한 이러한 방어의 효능에 대한 분석은 없다. (섹션 3.5 참조)\n' +
      '\n' +
      '한계 위험의 증거 개방의 한계 위험(예: 재현성, 수정 가능성, 액세스 래칫과 관련)에 대한 일부 논의가 있지만 폐쇄 모델 또는 유사하게 오용을 가능하게 할 수 있는 다른 기술과 관련된 위험과 직접적인 비교를 최소화한다. (섹션 2.2 및 4 참조)\n' +
      '\n' +
      '방어의 용이성은 한계 위험을 완화하기 위한 몇 가지 메커니즘(예: 방출 시기, 어떤 자산이 방출되는지)을 분석하지만 이러한 완화가 한계 위험을 해결할 수 있는 정도에 대한 철저한 설명은 제공하지 않는다. (섹션 3.3 및 3.4 참조)\n' +
      '\n' +
      '불확실성/가설 본 논문은 개방형 및 폐쇄형 모델의 현재 상태 및 공개 방법과 같은 분석에 내장된 몇 가지 가정을 명확히 설명한다. (섹션 2.2, 3.5 및 4.2-4.4 참조)\n' +
      '\n' +
      '### 비합의적 친밀한 이미지(NCII)(Lakatos, 2023)\n' +
      '\n' +
      '위협 식별 위협은 명확하게 식별된다: 디지털로 변경된 NCII. 위협 행위자는 기계 학습 기술이 반드시 필요하지 않으며 NCII를 생성하기 위해 사용하기 쉬운 인터페이스에 의존할 수 있는 사람이다. (1페이지를 참조)\n' +
      '\n' +
      '기존 위험(개방형 FM 부재) 이 논문은 개방형 텍스트-이미지 기반 모델의 광범위한 가용성 이전(및 이후) NCII 배포 사이트를 참조하는 댓글을 살펴보고 얼굴 스와핑과 같은 기존 도구의 위험을 설명하지는 않지만 이러한 사이트에 수천만의 고유한 방문자가 있음을 보여준다. (1페이지 및 3페이지를 참조)\n' +
      '\n' +
      '기존 방어(개방형 FM 부재) 본 논문은 악의적인 사용자를 억제하거나 NCII의 확산을 방지하기 위해 사용될 수 있는 NCII에 대한 기존 기술 또는 법적 방어를 검토하지 않는다. (예를 들면, 1페이지를 참조)\n' +
      '\n' +
      '한계 위험의 증거 이 논문은 NCII를 만들기 위한 서비스 사용의 증가가 개방형 FM에 의해 주도된다는 것을 보여주는 것과 같은 개방형 모델에서 비롯된 위험을 개략적으로 설명하지만 개방형 모델의 위험을 폐쇄형 모델이나 포토샵과 같은 다른 디지털 기술의 위험과 비교하지는 않는다. (1페이지를 참조)\n' +
      '\n' +
      '방어의 용이성은 페이팔과 같은 다운스트림 엔티티가 NCII의 배포를 가능하게 하는 플랫폼에 서비스를 제공하지 않는다는 일부 논의가 있지만, 구체적으로 개방형 기반 모델에 의해 제기된 한계 위험에 대한 방어에 대한 유의미한 분석은 없다. (5페이지를 참조)\n' +
      '\n' +
      '불확실성/가정 방어의 잠재적 적응과 같이 분석에 내장된 불확실성이나 가정에 대한 명시적인 논의는 없다. (예를 들면, 1페이지를 참조)\n' +
      '\n' +
      '### 아동 성적 학대 자료(CSAM)(Thiel et al., 2023)\n' +
      '\n' +
      '위협 식별 위협은 명확하게 식별된다: 컴퓨터 생성 CSAM(또는 CG-CSAM)의 배포. 그것은 위협 행위자들을 CSAM을 공유하기 위한 동호인 그룹과 포럼으로 식별한다. (섹션 1 참조)\n' +
      '\n' +
      '기존 위험(개방형 FM 없음) 저자는 CG-CSAM의 기존 유병률이 낮다는 증거를 제공한다. CSAM 공유를 위한 온라인 포럼을 분석한 결과, 온라인 포럼에서 CSAM의 1% 미만이 사실적 CG-CSAM인 것으로 나타났다. (섹션 1 참조)\n' +
      '\n' +
      '기존 방어(개방형 FM이 없음) 이 논문은 개방형 FM이 없는 CSAM에 대한 방어로서 (알려진 CSAM의 해시셋을 생성하는 것과 같은) 플랫폼에 의한 조정뿐만 아니라 법적 방어의 개요를 설명한다. (섹션 4 및 5 참조)\n' +
      '\n' +
      '한계 위험의 증거 저자들은 개방형 기반 모델의 사용으로 인한 몇 가지 우려를 나열한다: 모델이 새로운 이미지를 생성하기 위해 유사성에 대해 훈련된 경우 학대 아동의 재희생, 거짓말쟁이의 배당 -- 실제 CSAM을 소유한 가해자가 CG-CSAM을 소유한다고 주장할 때; 실제 학대 아동의 사례를 찾기 위해 점점 더 실제와 같은 CG-CSAM을 통해 필터링해야 하는 중재자의 피해 증가; CG-CSAM 급증으로 인한 집행 기관의 부담 증가. 이 논문은 또한 대부분의 CG-CSAM이 공개적으로 사용 가능한 Stable Diffusion 시리즈 모델을 기반으로 한다는 증거를 제공한다; 워터마킹이나 Stable Diffusion의 사용을 모니터링하는 것은 개방형 가중치 때문에 어렵다는 점을 지적하고, 이러한 워터마킹이 어떻게 비활성화되었는지 논의한다. (섹션 4 및 5 참조)\n' +
      '\n' +
      '방어의 용이성은 훈련 데이터에서 알려진 CSAM을 제거하는 개발자, 진입 장벽을 증가시키는 지속적인 워터마크를 생성하는 방법(안정 서명과 같은 방법), 광 사실적 CSAM에 대한 법적 방어, CG-CSAM을 구별하기 위해 EXIF 데이터를 사용하는 것과 같은 저기술 방어 등 몇 가지 방어에 대해 논의한다. (섹션 4, 5 및 6 참조)\n' +
      '\n' +
      '불확실성/가설 본 논문은 위험 증가에서 더 나은 모델의 범위, 워터마킹 오픈 모델의 어려움에 대한 가정, CG-CSAM의 법적 지위에 대한 불확실성을 지적한다. (섹션 3, 4, 5 참조)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>