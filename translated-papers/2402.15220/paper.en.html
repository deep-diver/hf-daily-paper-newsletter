<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition\n' +
      '\n' +
      'Lu Ye Ze Tao Yong Huang Yang Li\n' +
      '\n' +
      'Microsoft\n' +
      '\n' +
      '{luye,zetao,yohua,yali2}@microsoft.com\n' +
      '\n' +
      'Code will be publicly available on Github after review.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8\\(\\times\\) compared to the start-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096. 1\n' +
      '\n' +
      'Footnote 1: Code will be publicly available on Github after review.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Over the last few years, Large Language Models (LLMs) have developed various capabilities, from in-context learning (Dong et al., 2023, 2022) to chain-of-thought reasoning (Chu et al., 2023; Wei et al., 2022), and achieved remarkable success in a wide range of natural language processing related tasks (Chang et al., 2023). Representive models are the GPT (Radford et al., 2018, 2019; Brown et al., 2020; OpenAI, 2023c), LLaMA (Touvron et al., 2023b), PaLM (Anil et al., 2023) and Gemini (Gemini, 2023) series. Following the success of ChatGPT and GPT store, LLM-based applications start to surge, and the demand to optimize LLM\'s inference cost has been a new area of research interest (Kim et al., 2023; Sheng et al., 2023; Aminabadi et al., 2022).\n' +
      '\n' +
      'The self-attention module, as one of the critical components in LLMs, has poor performance during inference (Table 1) since it performs intensive memory operations on key/value tensors of context tokens (KV cache) and is memory-bound (Williams et al., 2009; Jin et al., 2023). The memory complexity grows linearly with context length. As the demand for more context tokens has been a trend (32K for GPT-4), the performance gets worse (OpenAI, 2023c). KV cache additionally restricts the batch size and system throughput. For instance, using FP16, the KV cache for each token in GPT-3(17SB) requires 4.5MB of memory. The memory of an inference server with 8*A100 (80G) can only hold 70000 tokens or 35 sequences of 2K context tokens.\n' +
      '\n' +
      'On the other hand, the system prompt as a common practice in designing LLM based applications, leads to redundancy in KV cache (Anthropic, 2023). Typically, due to high training and inference costs, LLMs are pre-trained and deployed in a multi-tenant architecture for multiple applications to share. System prompts are essential for LLMs to gain each application\'s domain knowledge and generate better results (White et al., 2023; Zhou et al., 2023). Since multiple requests share identical system prompts, there is significant overlap in prompt prefixes (SS2.1).\n' +
      '\n' +
      'An important question is whether we can leverage the sharing characteristic of system prompts to make the self-attention module faster and more memory efficient. To our knowledge, the only related work is a proposal by Kwon et al. (2023), in which the service provider reserves memory for key/value tensors of a set of predefined system prompts from application developers. The proposalhas limitations: i) predefined system prompts are static and inflexible in frequent refreshes for large-scale deployments since both application developers and the service provider are involved in the operation loop; ii) there is memory waste in case of long system prompts and low hit rate; iii) no work has been done to optimize the self-attention kernel in the presence of shared system prompts.\n' +
      '\n' +
      'To fill the gap, we propose ChunkAttention, a novel self-attention module featuring the prefix-aware KV cache (PAKV) and two-phase partition (TPP). KV cache in ChunkAttention is a prefix tree built with chunked context tokens and key/value tensors. Thus, the KV cache is prefix-aware and can dynamically detect and remove redundancy at runtime without human involvement. The KV cache only stores key/value tensors of sequences currently in decoding and has zero memory waste. In addition, the prefix-tree structure provides context for ChunkAttention to redesign a highly-optimized self-attention kernel with two-phase partition: chunk-first phase and sequence-first phase. Query tensors from sequences with matching prompt prefixes are batched together to perform attention with key/value tensors.\n' +
      '\n' +
      'The main contributions of this paper are as follows: i) we reveal that system prompts can be long (SS2.1), providing opportunities for optimizing self-attention; ii) we propose to use prefix tree to implement KV cache, which is out-of-the-box, scalable and robust in terms of redundancy removal; iii) we implement a two-phase partition algorithm to speed up self-attention kernel on prefix-aware KV cache; iv) we prove the feasibility and empirically quantify the gain self-attention can achieve from shared system prompts under various system configurations. Our experiments show that ChunkAttention can be significantly faster as the length of shared system prompts grows and has no performance degradation without shared system prompts, compared to existing highly optimized implementations.\n' +
      '\n' +
      '## 2 Preliminaries\n' +
      '\n' +
      '### Shared System Prompt\n' +
      '\n' +
      'One paradigm in designing LLM-based applications has been the introduction of system prompt (Anthropic, 2023). It provides instructions, few-shot examples (Dong et al., 2022), and external knowledge as context for LLMs to generate better results. The final prompt to LLMs is a concatenation of system prompt and task-specific input. The system prompt is shared between multiple requests and can be very long. This can be observed in various LLM-based applications, from online chatbots to offline experiments.\n' +
      '\n' +
      'Toolformer or using external tools becomes an essential skill for LLMs to get up-to-date information or perform precise math calculations (Schick et al., 2023; Li et al., 2023). It is implemented by plugins in ChatGPT-like online chatbot applications (OpenAI, 2023a). Equivalent capability is provided by GPT series models through function calling (OpenAI, 2023b). Under the hood, available function specifications are silently injected into the system prompt (OpenAI, 2023d). Experiments indicate that with 6 plugins activated, the token length of the shared system prompt can reach up to 1766 (Appendix A).\n' +
      '\n' +
      'Another source of shared system prompts is the offline research-focused experiments conducted on LLMs. In these scenarios, researchers frequently create a large number of templated requests with identical instructions, examples, or external knowledge and issue them to LLMs quickly. Example work includes: i) Chameleon (Lu et al., 2023) reuses policy planning and tool invocation prompts for compositional reasoning on the ScienceQA and TabMWP datasets; ii) CREATOR (Qian et al., 2023) constructs a collection of questions from TabMWP and MATH datasets using a chain-of-thought (CoT) prompt template; iii) PDFTriage (Saad-Falcon et al., 2023) injects the PDF document metadata into prompt and runs multiple question-answering (QA) tasks over the document; iv) ToolQA (Zhuang et al., 2023) further releases a QA dateset and reuses the system prompt for eval\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c c} \\hline \\hline Batch Size & \\multicolumn{1}{c|}{Roofline} & QKV Projection & Self Attention & MLP \\\\ \\hline \\hline \\multirow{3}{*}{1} & FLOPs(\\(\\sim\\)10\\({}^{7}\\)) & 100.66 & 33.57 & 270.53 \\\\  & Adrinknet Intensity & 1.00 & **0.99** & 1.00 \\\\  & Latency(qa) & 88.44 & **17.82** & 160.77 \\\\ \\hline \\multirow{3}{*}{32} & HOPs(\\(\\sim\\)10\\({}^{7}\\)) & 3221.23 & 1074.27 & 8657.04 \\\\  & MOPs(\\(\\sim\\)10\\({}^{6}\\)) & 101.71 & 1083.12 & 273.43 \\\\  & Arithmetic Intensity & 31.67 & **0.99** & 31.66 \\\\  & Latency(qa) & 90.02 & **687.74** & 209.82 \\\\ \\hline \\multirow{3}{*}{64} & FLOPs(\\(\\sim\\)10\\({}^{7}\\)) & 6442.45 & 2148.53 & 17314.09 \\\\  & Adrinknet Intensity & 102.76 & 2166.36 & 276.33 \\\\ \\cline{1-1}  & Latency(qa) & 62.69 & **0.99** & 62.66 \\\\ \\cline{1-1}  & Latency(qa) & 90.04 & **1388.40** & 217.79 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Complexity analysis of key modules in each decoder layer when decoding one single token. Llama2 7B, 2048 context tokens, FP16, A100 (80G). The self-attention module has low arithmetic intensity (Williams et al., 2009) and high latency. FLOPs: floating point operations. MOPs: memory operations or memory bytes accessed. Arithmetic Intensity: FLOPs/MOPs.\n' +
      '\n' +
      'uations of QA with LLMs. Table 2 shows statistics on shared token counts of system prompts.\n' +
      '\n' +
      '### LLM Inferencing\n' +
      '\n' +
      'The typical inference process of LLMs consists of two stages: prefilling and decoding (Sheng et al., 2023). After receiving a sequence \\(S=[t_{1},...,t_{n_{p}}]\\), the server starts to prefill. During prefilling, it feeds all \\(n_{p}\\) prompt tokens \\(t_{1},...,t_{n_{p}}\\) into LLMs, computes the attention key/value tensors, and caches them to speed up subsequent computations. Then, the server performs decoding. Decoding is auto-regressive, and the input token to LLMs is the completion token(or output token) generated from the previous decoding iteration. The process continues until the end-of-sequence token or maximum completion tokens are generated.\n' +
      '\n' +
      'When the server is decoding \\(b\\) (batch size) sequences \\(S_{1},...,S_{b}\\) simultaneously, although they are in different iterations, the server can still perform batching at the granularity of iteration and predict the next tokens for all sequences together, rather than separately, which is known as iteration-based batching (Gao et al., 2018; Yu et al., 2022; Silfa et al., 2022). Specifically, iteration-based batching concatenates last input tokens of multiple sequences (one token per sequence) \\(t^{(1)},...,t^{(b)}(t^{(i)}\\in S_{i})\\) into a single input \\(\\mathbf{T}\\), and computes the QKV projection before self-attention, the output projection and multilayer perceptron after self-attention. The self-attention in the middle has no shared weights and needs to be computed independently for each sequence. During decoding, new sequences can join, and completed sequences can leave, significantly increasing the possibility of forming big batches. Iteration-based batching has been implemented by vLLM (Kwon et al., 2023) and the text-generation-inference server (HuggingFace, 2023). The ChunKAttention in this paper assumes that iteration-based batching is enabled to form batches for its kernel to run efficiently.\n' +
      '\n' +
      '## 3 Our Approach\n' +
      '\n' +
      '### Prefix Aware KV Cache (PAKV)\n' +
      '\n' +
      'Traditionally, KV cache is stored in dense tensors of size \\(b\\times h\\times n\\times d\\) where \\(b\\) is the batch size, \\(h\\) is the number of heads, \\(n\\) is the sequence length, and \\(d\\) is the head dimension size.\n' +
      '\n' +
      'When multiple sequences share common prefix tokens, key/value tensors are the same and thus can be shared in memory. For example, a particular LLM inference server receives sequence \\(S_{i}=[t_{1},...,t_{n_{s}},t_{n_{s}+1},...,t_{n_{p}}]\\) first, and then receives sequence \\(S_{j}=[t_{1},...,t_{n_{s}},t^{\\prime}_{n_{s}+1},...,t^{\\prime}_{n_{p}}]\\). KV cache for \\(t_{1},...,t_{n_{s}}\\) can only have one physical copy in memory.\n' +
      '\n' +
      'Given the property, we argue that the KV cache should be made prefix-aware, which is to organize the KV cache of all sequences under decoding into a prefix tree. Precisely, we slice monolithic key/value tensors contiguous in memory along the sequence length dimension. Figure 1 shows the structure of the KV cache stored in a prefix tree. Each node defines a chunk \\(C\\) storing three essential elements: i) a segment of \\(c\\) context tokens shared by sequences \\(S_{i},...,S_{j}\\) to enable prefix tree operations; ii) a slice of key tensor of size \\(b\\times h\\times c\\times d\\) for the \\(c\\) tokens; iii) the corresponding slice of value tensor. Each path in the prefix tree defines a sequence. Multiple trees (a forest) may exist in the server simultaneously. For instance, application developers design different system prompts.\n' +
      '\n' +
      'There are three possible scenarios during inference: i) new sequence joins, ii) completed sequence leaves, and iii) all sequences decode one token together. Each scenario can be translated into prefix tree operations. When a new sequence joins, the prefix tree is searched and updated to insert a new path. When a completed sequence leaves, the prefix tree is updated to delete its path. At each decoding iteration, we append new tokens into leaf chunks or grow a new chunk when the leaf chunk is full.\n' +
      '\n' +
      'Given a fixed chunk size \\(c\\), memory management is efficient. In ChunKAttention, the pool-based memory allocator is adopted by default (Hill, 1992; Trebino, 2016). It keeps track of both a used and a free chunk list. When a new chunk is requested,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline \\multirow{2}{*}{System} & \\multirow{2}{*}{Usage of Prompt} & \\multicolumn{2}{c}{\\#shared prompt tokens} \\\\ \\cline{3-4}  & & avg & max \\\\ \\hline Chandleon & Tools definition and examples1  & 1324 & 2626 \\\\ CREATOR & CoF examples2  & 879 & 2492 \\\\ PPPriage & PPG document metadata & 4257 & N.A. \\\\ ToolQA & Tools definition and examples3  & 1432 & 1432 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Shared prompt tokens in system prompt, tokenized by OpenAI’s tiktoken tokenizer library (OpenAI, 2023e).\n' +
      '\n' +
      'the allocator returns a chunk from the free list or allocates fresh memory from the operating system (OS). Unused chunks are returned to the allocator once a sequence is completed, but the allocator does not release memory to the OS, preventing unnecessary memory allocations. Some memory space for alignment is unused. Given that the sequence length is \\(n\\), the memory loss is bounded by \\((c-1)/n\\).\n' +
      '\n' +
      'By sharing common prefixes, the number of sequences that can be processed simultaneously is increased by approximately \\(1/(1-r)\\). The sharing ratio \\(r\\) is defined by the percentage of shared tokens \\(n_{s}/(n_{p}+n_{c})\\), and \\(n_{c}\\) is the completion token count. In memory-limited inference scenarios, this helps increase the batch size and thus improve throughput.\n' +
      '\n' +
      'The parent-child relationship defines the subset of sequences each chunk covers. The root node covers all sequences, and the leaf nodes cover only one. A key property of the prefix tree is that sequences covered by each chunk in the prefix tree are contiguous in the sequence index dimension. Therefore, slicing the query tensor in self-attention is particularly efficient during kernel computation, which will be discussed in more detail in the next section.\n' +
      '\n' +
      '### Two-phase Partition (TPP)\n' +
      '\n' +
      'In this section, we dive into the self-attention kernel implementation on top of the unique prefix-aware KV cache storage.\n' +
      '\n' +
      'During prefilling, we perform a prefix lookup to avoid repeated computation of KV projection and position embedding for matched prompt prefixes. For mismatched suffix tokens, KV projection and position embedding are still computed, and the key/value tensors are chunked and inserted into the prefix tree. Then we apply existing highly optimized self-attention kernels, _e.g._, FlashAttention (Dao, 2023), on the entire key/value tensors.\n' +
      '\n' +
      'During iterative decoding, self-attention is divided into chunk-first and sequence-first phases. The two phases focus on different slices of the query tensor, KV cache chunks, and use different parallelization strategies. The process is shown in Figure 2. Since the head dimension is always partitioned, it is omitted and implicit in our discussion.\n' +
      '\n' +
      '**Chunk-first Phase**. In the chunk-first phase, we only process chunks shared by multiple sequences. Since GPUs have more streaming multiprocessors (108 for A100) than the number of heads (32 for Llama 7B), and partitioning by heads under-utilizes hardware resources, we perform additional partition on keys/values. Chunking already provides convenience. The online softmax algorithm is adopted to avoid the synchronization requirement between partitions (Milakov and Gimelshein, 2018; Dao, 2023).\n' +
      '\n' +
      'The computation is performed by traversing shared chunks in the prefix tree, executing the partial attention kernel _partial_attn_ and saving the partial attention results into memory, as shown in Algorithm 1. The number of sequences (batch size) is denoted by \\(b\\). \\(\\mathbf{Q}\\in\\mathbb{R}^{b\\times d}\\) is the queries formed by concatenating the last token of all \\(b\\) sequences in the latest decoding iteration.\n' +
      '\n' +
      'The implementation of _partial_attn_ is given by Eqn. (1). It computes the partial attention result \\((\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\) with respect to each chunk \\(C\\) independently, thus it can be parallelized. \\(\\mathbf{Q}_{i:j,:}\\) is a slice of \\(\\mathbf{Q}\\) for sequences ranging from \\(i\\) to \\(j\\) which share the KV cache stored in chunk \\(C\\). The\n' +
      '\n' +
      'Figure 1: KV cache in prefix tree. The instructions and examples in prompts of \\(S_{0},S_{1},S_{2}\\) are common and sharable. Questions are different and not sharable. Some memory is unused due to alignment.\n' +
      '\n' +
      'maximum attention weights vector \\(\\mathbf{M}^{(C)}\\) is the row-wise max over the last dimension of attention weights \\(\\mathbf{W}^{(C)}\\). The softmax normalization term \\(\\mathbf{n}^{(C)}\\) is the row-wise sum over the last dimension of \\(\\mathbf{E}^{(C)}\\). \\(\\mathbf{M}^{(C)}\\) and \\(\\mathbf{n}^{(C)}\\) are auxiliary variables introduced to further cumulate partial attention results of multiple chunks.\n' +
      '\n' +
      '\\[\\begin{split}&\\mathbf{W}^{(C)}=\\mathbf{Q}_{i;j}\\mathbf{K}^{(C)}\\in\\mathbb{R}^{(j -i)\\times c}\\\\ &\\mathbf{m}^{(C)}=\\max\\left(\\mathbf{W}^{(C)}\\right)\\in\\mathbb{R}^{(j-i)} \\\\ &\\mathbf{E}^{(C)}=\\exp\\left(\\mathbf{W}^{(C)}-\\mathbf{m}^{(C)}\\cdot\\mathbf{1}^{T} \\right)\\in\\mathbb{R}^{(j-i)\\times c}\\\\ &\\mathbf{n}^{(C)}=\\sum\\left(\\mathbf{E}^{(C)}\\right)\\in\\mathbb{R}^{(j-i)} \\\\ &\\mathbf{O}^{(C)}=\\mathbf{E}^{(C)}\\mathbf{V}^{(C)}\\in\\mathbb{R}^{(j-i)\\times d }\\end{split} \\tag{1}\\]\n' +
      '\n' +
      'The _partial_atm_ efficiently accesses shared KV cache memory since self-attentions for multiple sequences are batched. The batching happens at a granularity of dot-product between queries \\(\\mathbf{Q}_{i;,},...,\\mathbf{Q}_{j;}\\) of sequences \\(S_{i},...,S_{j}\\) and shared \\(\\mathbf{K}^{(C)}/\\mathbf{V}^{(C)}\\). In addition to improved data locality, another advantage of batching is to turn the query from a vector into a matrix, allowing efficient matrix multiplications with tensor cores (Choquette et al., 2021).\n' +
      '\n' +
      '**Sequence-first Phase**. In the sequence-first phase, we load partial attention results of shared chunks from the chunk-first phase and continue processing chunks related to one specific sequence. We partition sequences, and each \\(\\mathbf{q}\\) handled by the sequence-first kernel is a vector by slicing the \\(i\\)-th row of \\(\\mathbf{Q}\\), as shown in Algorithm 2.\n' +
      '\n' +
      '```\n' +
      '0:\\(\\mathbf{Q}\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '0:\\(\\mathbf{Q}\\in\\mathbb{R}^{b\\times d}\\) (attention output)\n' +
      '0:\\(\\mathbf{G}\\in\\mathbb{R}^{b\\times d}\\) (attention output)\n' +
      '0:for\\(\\mathbf{q}\\gets\\mathbf{q}_{1}\\) to \\(\\mathbf{q}_{2}\\)do\n' +
      '0:\\(\\mathbf{m},\\mathbf{n}^{(C)}\\leftarrow(\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\)\\(\\mathbf{o}\\)\\(\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\)do\n' +
      '0: Partial attn results \\((\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\leftarrow\\)\\(\\text{slicing}\\)\\((\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\) \\(\\text{data}_{reduce}(\\mathbf{o}^{(C)},\\mathbf{m},\\mathbf{n})^{(C)}\\) \\(\\text{data}_{reduce}(\\mathbf{o}^{(C)},\\mathbf{m}^{(C)},\\mathbf{n}^{(C)},\\mathbf{o},\\mathbf{m},\\mathbf{n})\\) endfor\n' +
      '0: Get chunks \\(C_{k}\\)+\\(C_{k+1}\\), \\(C_{k+2}\\), \\(C_{k}\\) in \\(T\\) with respect to \\(\\mathbf{q}\\) only for\\(\\mathbf{C}^{*}\\)\\(\\mathbf{C}^{*}\\)\\(\\mathbf{V}^{(C)}\\leftarrow\\) key, value cache stored in \\(C\\) \\(i\\leftarrow\\) sequence instead of \\(\\mathbf{q}\\) \\((\\mathbf{o},\\mathbf{m},\\mathbf{n})^{(C)}\\leftarrow\\) partial_atm(\\(\\mathbf{q}\\), \\(\\mathbf{K}^{(C)},\\mathbf{V}^{(C)},i,i+1)\\) \\(\\text{data}_{reduce}(\\mathbf{o}^{(C)},\\mathbf{m}^{(C)},\\mathbf{n}^{(C)},\\mathbf{o},\\mathbf{m},\\mathbf{n})\\) endfor endfor endfunction\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1** Self Attention: Chunk First (partition chunks)\n' +
      '\n' +
      'The _partial_\\(\\mathbf{Q}\\)\\(\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '\n' +
      'The _atm_\\(\\mathbf{q}\\)\\(\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '\n' +
      '\\(\\mathbf{m}^{(C)}=\\max\\left(\\mathbf{W}^{(C)}\\right)\\in\\mathbb{R}^{(j-i)}\\)\n' +
      '\n' +
      '\\(\\mathbf{E}^{(C)}=\\exp\\left(\\mathbf{W}^{(C)}-\\mathbf{m}^{(C)}\\cdot\\mathbf{1}^{T}\\right)\\in \\mathbb{R}^{(j-i)\\times c}\\)\n' +
      '\n' +
      '\\(\\mathbf{n}^{(C)}=\\text{sum}\\left(\\mathbf{E}^{(C)}\\right)\\in\\mathbb{R}^{(j-i)}\\)\n' +
      '\n' +
      '\\(\\mathbf{O}^{(C)}=\\mathbf{E}^{(C)}\\mathbf{V}^{(C)}\\in\\mathbb{R}^{(j-i)\\times d}\\)\n' +
      '\n' +
      'The _partial_atm_ efficiently accesses shared KV cache memory since self-attentions for multiple sequences are batched. The batching happens at a granularity of dot-product between queries \\(\\mathbf{Q}_{i;,},...,\\mathbf{Q}_{j;}\\) of sequences \\(S_{i},...,S_{j}\\) and shared \\(\\mathbf{K}^{(C)}/\\mathbf{V}^{(C)}\\). In addition to improved data locality, another advantage of batching is to turn the query from a vector into a matrix, allowing efficient matrix multiplications with tensor cores (Choquette et al., 2021).\n' +
      '\n' +
      '**Sequence-first Phase**. In the sequence-first phase, we load partial attention results of shared chunks from the chunk-first phase and continue processing chunks related to one specific sequence. We partition sequences, and each \\(\\mathbf{q}\\) handled by the sequence-first kernel is a vector by slicing the \\(i\\)-th row of \\(\\mathbf{Q}\\), as shown in Algorithm 2.\n' +
      '\n' +
      '```\n' +
      '0:\\(\\mathbf{Q}\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '0:\\(\\mathbf{Q}\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '0:for\\(\\mathbf{q}\\gets\\mathbf{q}_{1}\\) to \\(\\mathbf{q}_{2}\\)do\n' +
      '0:\\(\\mathbf{m},\\mathbf{n}^{(C)}\\leftarrow(\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\)\\(\\mathbf{o}\\)\\(\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\)do\n' +
      '0: Partial attn of \\(\\mathbf{q}^{*}\\)\\((\\mathbf{o},\\mathbf{m},\\mathbf{n})^{(C)}\\leftarrow\\)\\(\\text{slicing}\\)\\((\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\) \\(\\text{data}_{reduce}(\\mathbf{o}^{(C)},\\mathbf{m}^{(C)},\\mathbf{n}^{(C)},\\mathbf{o},\\mathbf{m},\\mathbf{n})\\) endfor\n' +
      '0: Get chunks \\(C_{k}\\)+\\(C_{k+2}\\), \\(C_{k}\\) in \\(T\\) with respect to \\(\\mathbf{q}\\) only for\\(\\mathbf{C}^{*}\\)\\(\\mathbf{C}^{*}\\)\\(\\mathbf{C}^{*}\\)\\(\\mathbf{V}^{(C)}\\leftarrow\\) key, value cache stored in \\(C\\) \\(i\\leftarrow\\) sequence instead of \\(\\mathbf{q}\\) \\((\\mathbf{o},\\mathbf{m},\\mathbf{n})^{(C)}\\leftarrow\\) partial_atm(\\(\\mathbf{q}\\), \\(\\mathbf{K}^{(C)},\\mathbf{V}^{(C)},i,i+1)\\) \\(\\text{data}_{reduce}(\\mathbf{o}^{(C)},\\mathbf{m}^{(C)},\\mathbf{n}^{(C)},\\mathbf{o},\\mathbf{m},\\mathbf{n})\\) endfor endfor endfunction\n' +
      '```\n' +
      '\n' +
      '**Algorithm 2** Self Attention: Sequence First (partition sequences)\n' +
      '\n' +
      'The _atm_\\(\\mathbf{q}\\)\\(\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '\n' +
      'The _atm_\\(\\mathbf{q}\\)\\(\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '\n' +
      '\\(\\mathbf{O}_{i;,}=x^{(C)}\\mathbf{o}^{(C)}+y^{(C)}\\mathbf{O}_{i;,}\\in\\mathbb{R}^{d}\\)\n' +
      '\n' +
      '\\(\\mathbf{n}_{i}=x^{(C)}n^{(C)}+y^{(C)}\\mathbf{n}_{i}\\in\\mathbb{R}\\)\n' +
      '\n' +
      '\\(\\mathbf{m}_{i}=\\text{max}\\left(m^{(C)},\\mathbf{m}_{i}\\right)\\in\\mathbb{R}\\)\n' +
      '\n' +
      'Figure 2: Two-phase partition kernel in ChunkAttention. The server is decoding sequences \\(S_{0}\\), \\(S_{1}\\), and \\(S_{2}\\). They share chunks \\(C_{0}\\), \\(C_{1}\\) and \\(C_{2}\\). In the chunk-first phase, queries \\(\\mathbf{q}_{0}\\), \\(\\mathbf{q}_{1}\\) and \\(\\mathbf{q}_{2}\\) are batched for self-attention with \\(C_{0}\\), \\(C_{1}\\) and \\(C_{2}\\). Partial attention result \\(\\mathbf{O}^{(C)}\\), \\(\\mathbf{m}^{(C)}\\) and \\(\\mathbf{n}^{(C)}\\) are saved into memory. In the sequence-first phase, \\(\\mathbf{o}_{i}\\), \\(m_{i}\\), and \\(n_{i}\\) for each sequence are restored, and we continue processing the remaining chunks with respect to \\(\\mathbf{q}_{i}\\) only.\n' +
      '\n' +
      '### Further Optimizations\n' +
      '\n' +
      'The prefix tree structure is maintained in CPU memory. To run the two-phase patation kernel on GPU, we must generate certain context from the prefix tree, including the chunk \\(C\\), the start index \\(i\\) and end index \\(j\\) of its covered sequences, and copy the context (\\(C\\), \\(i\\), \\(j\\)) from CPU to GPU memory. For example, in Figure 2, we need to generate and copy \\((C_{0}/C_{1}/C_{2},0,2)\\), \\((C_{3},0,0)\\), \\((C_{4}/C_{6},1,1)\\), and \\((C_{5}/C_{7},2,2)\\). ChunkAttention manages the overhead in two ways: i) latency hiding. The context generation step on CPU can be overlapped with other kernels on GPU prior to self-attention. ii) lazy context copy. The prefix tree does not change at every decoding iteration. We can cache the context in GPU memory and only trigger memory copy when the tree structure changes. Triggers are chunk full for every \\(c\\) iterations, new sequence joining, and completed sequence leaving. The overhead is amortized.\n' +
      '\n' +
      'The temporary memory allocated for partial attention results in the chunk-first phase can be eliminated by executing _attn_reduce_ right after _partial_attn_ to directly merge partial attention results into the final result. Since multiple shared chunks with a parent-child relationship in the prefix tree write into the same slice of \\((\\mathbf{O},\\mathbf{m},\\mathbf{n})\\), _attn_reduce_ needs to be serialized. On GPU devices, atomic operations are heavy, and we do not use this approach. However, on CPU devices, the overhead of serializing is insignificant, and reduction can be implemented using spin locks.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'The evaluations are conducted at both the self-attention microkernel level and the end-to-end GPT-style model level. The microkernel level evaluations only capture time spent in the self-attention CUDA kernel. The side effects of PAKV and TPP, _e.g._, prefix tree operations, are captured in end-to-end evaluations. We run all experiments with NVIDIA A100 GPU (80G) and CUDA 11.8.\n' +
      '\n' +
      '### Microkernel Evaluation\n' +
      '\n' +
      '**Baselines**. We select four self-attention implementations as baselines: Naive PyTorch implementation by the formula \\(\\text{softmax}(\\mathbf{Q}\\mathbf{K}^{T}/\\sqrt{d})\\mathbf{V}\\), the memory-efficient self-attention implemented in xformers Lefaudeux et al. (2022), FlashAttention integrated in PyTorch Dao et al. (2022), and PagedAttention in vLLM Kwon et al. (2023).\n' +
      '\n' +
      'Since Naive, xformers, and FlashAttn are all built on monolithic KV tensors, they cannot be prefix-aware by partially sharing KV cache of prompt prefixes. PagedAttn does not implement PAKV either. However, its paging design enables us to manually create a fixed page table, mapping virtually non-shared memory to the same physical memory. It simulates the KV cache sharing scenario and helps us observe the performance of PagedAttn\'s CUDA kernel, which is denoted as PagedAttn*. None of the kernels support the TPP algorithm.\n' +
      '\n' +
      '**Workload**. Sequences are processed in batch mode, and the batch size is \\(b\\). All sequences within the same batch start and finish simultaneously. Each sequence is prefilled with \\(n_{p}\\) prompt tokens, and the leading \\(n_{s}\\) tokens are common prefixes. The task is to decode the next \\(n_{c}\\) completion tokens iteratively. We measure the decoding latency \\(t\\) and the throughput defined by token rate(tokens per second or TPS, \\(n_{c}*b/t\\)). For all experiments, the head dimension \\(d\\) is 128, the number of heads \\(h\\) is 32, and the chunk size \\(c\\) is 64. All tensors are in FP16.\n' +
      '\n' +
      '**Results**. We run experiments to observe the performance gain brought by PAKV and TPP by varying the following system hyperparameters: prompt and shared token count, completion token count, and batch size.\n' +
      '\n' +
      'Table 3 shows the latency of self-attention implementations given various prompt and shared token counts. ChunkAttn and PagedAttn* outperform Naive, xformers, FlashAttn, and PagedAttn, which are agnostic to shared token count. The Naive is 6.6\\(\\times\\) and 2.1\\(\\times\\) slower than ChunkAttn and PagedAttn*, respectively (\\(n_{s}\\)=4096). By comparing PagedAttn* and PagedAttn, we observe the performance gain brought by sharing KV cache memory physically. Although PagedAttn* does not implement PAKV or TPP, the hardware cache helps reduce its latency by up to 52% compared to PagedAttn (\\(n_{s}\\)=4096): repeatedly accessing the same physical memory blocks provides significant performance gain. The benefit of TPP can be further seen by comparing PagedAttn* and ChunkAttn. ChunkAttn outperforms PagedAttn* by 2.8-3.2\\(\\times\\), with a range of \\(n_{s}\\) from 1024 to 4096. TPP does not cause performance regression when no token is shared (\\(n_{s}\\)=0, ChunkAttn vs. PagedAttn* in Table 3). As a result, TPP should always be enabled.\n' +
      '\n' +
      'As the decoding proceeds, sequences start to diverge, and the performance gain of ChunkAttngradually decreases, as shown in Figure 3. Given 2048 shared tokens, ChunkAttn yields 3.6\\(\\times\\) token rate improvement compared to PagedAttn when \\(n_{c}\\) reaches 512, and the speedup drops to 2.3\\(\\times\\) when \\(n_{c}\\) reaches 2048. However, it is still a significant improvement. The improvement of ChunkAttn over PagedAttn* is lower since PagedAttn* benefits from physically shared KV cache memory, and only TPP makes a difference here. However, given \\(n_{s}\\)=2048, ChunkAttn is still 2.0\\(\\times\\) (145K against 73K) and 1.5\\(\\times\\) (70K against 46K) faster than PagedAttn* when \\(n_{c}\\) reaches 512 and 2048 respectively.\n' +
      '\n' +
      'Figure 4 focuses on varying the batch size. For all implementations except ChunkAttn and PagedAttn*, the throughput peaks when the batch size reaches 16 due to memory-bound. Given \\(n_{s}\\) is 2048, ChunkAttn\'s throughput continues to grow from 155k to 224k toks/s for the batch size ranging from 16 to 96 due to better data locality and improved arithmetic intensity.\n' +
      '\n' +
      '### End-to-end Evaluation\n' +
      '\n' +
      'ChunkLlama is built on top of Huggingface Llama and vLLM\'s optimized kernels (layer normalization and rotary embedding) under Apache-2.0 license, but the attention module is substituted by ChunkAttn. We run all experiments on the Open Llama2 7B model in FP16 [11, 12, 13].\n' +
      '\n' +
      '**Baselines**. We select two widely used and optimized LLMs serving toolkits with proven production usages: the start-of-the-art vLLM 0.2.7 [13] and Huggingface\'s Text Generation Inference (TGI) 1.3.4 [12].\n' +
      '\n' +
      '**Workload**. Requests arrive at the server randomly following the Poisson arrival process [10] parameterized by \\(\\lambda\\), which is the average requests per second (RPS). The actual batch size is adjusted dynamically by each system during decoding, and we configure its maximum to 32 equally. Application developers provide no information about the shared prompt prefix for the service provider to pre-configure. We measure the normalized latency (ms/tok or 1/TPS) as in vLLM, which is the mean of each request\'s end-to-end latency \\(t\\) (including queuing time) divided by the completion token count \\(n_{c}\\), and the peak memory bytes used by KV cache.\n' +
      '\n' +
      '**Results**. ChunkLlama yields the fastest inference speed, as shown in Figure 5. ChunkLlama can achieve 1.6\\(\\times\\) (2.9 against 1.8) and 2.3\\(\\times\\) (2.3 against 1.0) higher throughput compared to vLLM when 1024 and 2048 prefix tokens are shared while maintaining a normalized latency of less than 40 ms/token. Table 4 compares the latency and KV cache memory usage of our ChunkLlama to vLLM. No performance regression is observed in ChunkLlama without shared prefix tokens. The KV cache memory usage is reduced by 70%-90% with long shared prefixes. The peak batch size is also reduced by 20%-40% since ChunkLlama can decode faster.\n' +
      '\n' +
      '## 5 Related Work\n' +
      '\n' +
      'The most relevant work on optimizing the memory utilization of KV cache is PagedAttention in vLLM\n' +
      '\n' +
      'Figure 4: Token rate when decoding up to \\(n_{c}\\)=64 completion tokens given various batch sizes. Chunk size \\(c\\)=64.\n' +
      '\n' +
      'Figure 3: Throughput in token rate when generating up to \\(n_{c}\\) completion tokens, given \\(n_{s}\\) prefix tokens are shared. Chunk size \\(c\\)=64, batch size \\(b\\)=32.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\multirow{2}{*}{\\(n_{c}\\)} & \\multirow{2}{*}{\\(n_{s}\\)} & \\multicolumn{4}{c}{Latency (ms)} \\\\ \\cline{3-8}  & & Noise & Address & FlushAttn & PagedAttn & PagedAttn* & ChunkAttn \\\\ \\hline\n' +
      '1024 & 0 & 363.35 & 378.19 & 1586.73 & 356.17 & 355.82 & 332.50 \\\\\n' +
      '1024 & 512 & 364.73 & 385.79 & 1587.14 & 355.88 & 257.34 & **198.57** \\\\\n' +
      '1024 & 768 & 362.43 & 378.50 & 1591.61 & 366.02 & 125.18 & **131.21** \\\\\n' +
      '1024 & 1024 & 361.76 & 379.36 & 1569.09 & 355.44 & 154.46 & **560.00** \\\\ \\hline\n' +
      '3048 & 0 & 686.40 & 816.44 & 3175.25 & 705.98 & 703.50 & 655.44 \\\\\n' +
      '3048 & 1024 & 687.52 & 827.86 & 3773.53 & 703.50 & 355.02 & **344.37** \\\\\n' +
      '3048 & 1536 & 687.88 & 820.19 & 7149.76 & 900.29 & 412.25 & **207.14** \\\\\n' +
      '3048 & 2048 & 688.41 & 823.60 & 3152.25 & 703.28 & 338.41 & **110.48** \\\\ \\hline\n' +
      '4096 & 0 & 1399.20 & 1720.00 & 6598.95 & 1400.60 & 1400.17 & 1301.78 \\\\\n' +
      '4096 & 2048 & 1370.47 & 1722.42 & 6303.21 & 1400.99 & 998.78 & **2074.56** \\\\\n' +
      '4097 & 3072.16 & 736.41 & 725.73 & 6301.41 & 1400.30 & 823.89 & **477.26** \\\\\n' +
      '4096 & 4096 & 1370.41 & 1731.13 & 6300.65 & 1399.51 & 663.34 & **206.22** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Latency (ms) of self-attention kernel given \\(n_{p}\\) context tokens and \\(n_{s}\\) prefix tokens are shared. Chunk size \\(c\\)=64, batch size \\(b\\)=32.\n' +
      '\n' +
      '(Kwon et al., 2023). It introduces the paging technique in OS to solve the problem of memory waste caused by dynamic and unknown sequence lengths during decoding. However, only a proposal on service providers to pre-configure shared prompts is mentioned, and it is not implemented in vLLM releases (up to 0.2.7). Our solution, which differs from the paging one, uses the prefix tree to manage memory and aims to discover redundancy in KV cache across user requests at runtime automatically. The solution is more practical for multi-tenant deployment scenarios where service providers centrally host models and have requirements on scalability. According to vLLM, the shared KV cache is similar to the dynamic link library shared by multiple processes. vLLM\'s strategy is to compile before publishing (AoT). We expect to compile in real-time (JIT). Based on the context captured in the prefix tree, our work further proposes a two-phase partition algorithm to explore the optimization opportunities shared system prompts bring to the self-attention kernel, which is another difference between our work and the existing work.\n' +
      '\n' +
      'Partition strategies in ChunkAttention are built on online softmax (Milakov and Gimelshein, 2018) and inspired by FlashAttention (Dao et al., 2022; Dao, 2023), which adopted the same algorithm. FlashAttention thoroughly researched and implemented various tiling techniques, accelerating self-attention by 2-4\\(\\times\\) while cutting memory operations by 10-20\\(\\times\\). FlashAttention-2 altered tiling strategies and additionally doubled the speed. However, FlashAttention is inflexible regarding non-contiguous memory or variable sequence lengths, making it more suitable for model training than inference. There is little gain when the query token count is always one during decoding. ChunkAttention handles variable sequence lengths during decoding and batches attention operations of several sequences to reduce memory operations. As a result, our work and FlashAttention are complementary.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'In this paper, we propose ChunkAttention, a novel self-attention module, to efficiently manage KV cache and accelerate the self-attention kernel for LLMs inference. We successfully adopt the prefix tree to create a prefix-aware KV cache. It addresses the challenge of detecting and removing redundant KV cache at runtime. We evaluate ChunkAttention in various configurations and at different levels, proving its feasibility and the side effects can be managed. Experiments show that the ChunkAttention kernel can achieve comparable throughput with SOTA PagedAttention kernel without shared system prompts and can outperform it by 3.2-4.8\\(\\times\\) with a shared system prompt of 1024 to 4096 tokens on A100 (80G) by applying prefix-aware KV cache and two-phase partition.\n' +
      '\n' +
      '## 7 Limitations\n' +
      '\n' +
      '**The Position of System Prompt.** To share the key/value tensors in memory, the shared system prompt must appear at the beginning of the sequence. Although this is the most common practice in many works and systems (Lu et al., 2023; Qian et al., 2023; Saad-Falcon et al., 2023; Zhuang et al., 2023), it is not mandatory. Liu et al. (2023) reveals that language model performance degrades significantly when changing the position of relevant information, indicating that models struggle to access and use information in long input contexts robustly. In particular, performance is often lowest when models must use information in the middle of\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{\\(n_{p}\\)} & \\multirow{2}{*}{\\(n_{c}\\)} & \\multirow{2}{*}{\\(n_{c}\\)} & \\multirow{2}{*}{BPS} & \\multicolumn{3}{c}{Latency (uncoRank)} & \\multicolumn{3}{c}{Peak KV Cache (GIN)} & \\multicolumn{3}{c}{Peak Base Size} \\\\ \\cline{4-9}  & & & & \\(\\pm\\)1.LM & ChunkAttention & vLLM & ChunkAttention & vLLM & ChunkAttention \\\\ \\hline\n' +
      '104 & 0 & 52 & 1.0 & 193.25 & 19.11 & 14.73 & 11.50 & 23 & 18 \\\\\n' +
      '104 & 1024 & 52 & 1.0 & 200.0 & **14.07** & **14.79** & **3.28** & 12 & 14 \\\\ \\hline\n' +
      '2048 & 0 & 52 & 0.6 & 21.0 & 19.30 & 21.70 & 22.41 & 19 & 20 \\\\\n' +
      '2048 & 2948 & 53 & 0.2 & 21.1 & **15.20** & 19.00 & **3.40** & 19 & 12 \\\\ \\hline\n' +
      '4069 & 0 & 52 & 0.2 & 28.23 & 28.20 & 34.59 & 35.13 & 16 & 16 \\\\\n' +
      '4069 & 4096 & 52 & 0.4 & 27.62 & **17.46** & 35.42 & **4.00** & 16 & 11 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Normalized latency, peak KV cache memory, and batch size reached during decoding.\n' +
      '\n' +
      'Figure 5: Normalized latency given different request arrival rates (RPS). Each line is marked by the system and shared prompt token count: system(\\(n_{s}\\)).\n' +
      '\n' +
      'long input contexts. As a result, when application developers do not put the system prompt at the beginning for performance concerns after evaluations or unintentional mistakes, KV caches of the entire sequences are different, and PAKV cannot save memory in this case, although they have a large number of tokens in common.\n' +
      '\n' +
      '**Fine Tuning.** In addition to using system prompts, fine-tuning is another promising way to infuse domain knowledge into LLMs (Houlsby et al., 2019; Hu et al., 2023). Due to the high training and deployment cost, LLMs are typically pre-trained and centrally hosted for multiple applications to share. It is not cost-efficient for each application to fine-tune models and deploy private instances. However, fine-tuning may become more practical and popular as hardware and software environments evolve. In this case, we no longer need to design long system prompts for each application, and the sharing opportunities of system prompts are reduced. As of today, we have not seen promising and cost-efficient fine-tuning and hosting solutions in this direction than using system prompts.\n' +
      '\n' +
      '**Model and Hardware Compatibility.** To achieve the best performance, ChunkAttention implements the two-phase partition kernel with the low-level CUDA programming instead of leveraging high-level primitives in cuDNN (oneDNN Contributors, 2023) or PyTorch. We tune its performance for the most common LLM configurations, _e.g._, 128 head dimension size, and hardware, _e.g._, NVIDIA A100, GeForce RTX 4090, and Intel Xeon CPU. For other configurations and hardware, we need to tune and verify the performance case by case, which adds significant development costs. We believe community efforts are needed to generalize the two-phase partition algorithm and make it compatible with more model configurations and hardware.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Aminabadi et al. (2022) Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, and Yuxiong He. 2022. Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale. In _SC22: International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-15.\n' +
      '* Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. _arXiv e-prints_, pages arXiv-2305.\n' +
      '* Anthropic (2023) Anthropic. 2023. How to use system prompts. [https://docs.anthropic.com/claude/docs/how-to-use-system-prompts](https://docs.anthropic.com/claude/docs/how-to-use-system-prompts).\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901.\n' +
      '* Chang et al. (2023) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. 2023. A survey on evaluation of large language models.\n' +
      '* Choquette et al. (2021) Jack Choquette, Wishwesh Gandhi, Olivier Giroux, Nick Stam, and Ronny Khashinsky. 2021. Nvidia a100 tensor core gpu: Performance and innovation. _IEEE Micro_, 41(2):29-35.\n' +
      '* Chu et al. (2023) Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. 2023. A survey of chain of thought reasoning: Advances, frontiers and future.\n' +
      '* Computer (2023) Together Computer. 2023. Redpajama-data: An open source recipe to reproduce llama training dataset.\n' +
      '* Dao (2023) Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. _arXiv preprint arXiv:2307.08691_.\n' +
      '* Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. _Advances in Neural Information Processing Systems_, 35:16344-16359.\n' +
      '* Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey on in-context learning.\n' +
      '* Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. _arXiv preprint arXiv:2301.00234_.\n' +
      '* Gao et al. (2018) Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. 2018. Low latency rnn inference with cellular batching. In _Proceedings of the Thirteenth EuroSys Conference_, pages 1-15.\n' +
      '* Gemini (2023) Gemini. 2023. Gemini: A family of highly capable multimodal models.\n' +
      '* Geng and Liu (2023) Xinyang Geng and Hao Liu. 2023. Openllama: An open reproduction of llama.\n' +
      '* Hill (1992) Steve Hill. 1992. A simple fast memory allocator. In DAVID KIRK, editor, _Graphics Gems III (IBM Version)_, pages 49-50. Morgan Kaufmann, San Francisco.\n' +
      '* Geng et al. (2018)Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for nlp. In _International Conference on Machine Learning_, pages 2790-2799. PMLR.\n' +
      '* Hu et al. (2023) Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, E-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. 2023. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models.\n' +
      '* HuggingFace (2023) HuggingFace. 2023. huggingface/text-generation-inference: Large language model text generation inference. [https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference).\n' +
      '* Jin et al. (2023) Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. 2023. S3: Increasing gpu utilization during generative inference for higher throughput. _arXiv preprint arXiv:2306.06000_.\n' +
      '* Kim et al. (2023) Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Yakun Sophia Shao, and Amir Gholami. 2023. Full stack optimization of transformer inference: a survey.\n' +
      '* Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention.\n' +
      '* Lefaudeux et al. (2022) Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. 2022. xformers: A modular and hackable transformer modelling library.\n' +
      '* Li et al. (2023) Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-bank: A comprehensive benchmark for tool-augmented llms.\n' +
      '* Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts.\n' +
      '* Lu et al. (2023) Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large language models.\n' +
      '* Milakov and Gimelshein (2018) Maxim Milakov and Natalia Gimelshein. 2018. Online normalizer calculation for softmax. _arXiv preprint arXiv:1805.02867_.\n' +
      '* Contributors (2023) oneDNN Contributors. 2023. oneapi deep neural network library (onednn). [https://github.com/oneapi-src/oneDNN](https://github.com/oneapi-src/oneDNN).\n' +
      '* OpenAI (2023a) OpenAI. 2023a. Chatgpt plugins. [https://platform.openai.com/docs/plugins/introduction](https://platform.openai.com/docs/plugins/introduction).\n' +
      '* openai api. [https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling).\n' +
      '* OpenAI (2023c) OpenAI. 2023c. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_.\n' +
      '* OpenAI (2023d) OpenAI. 2023d. How to call functions with chat models. [https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models).\n' +
      '* OpenAI (2023e) OpenAI. 2023e. openai/tiktoken: tiktoken is a fast bpe tokeniser for use with openai\'s models. [https://github.com/openai/tiktoken](https://github.com/openai/tiktoken).\n' +
      '* Qian et al. (2023) Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023. Creator: Tool creation for disentangling abstract and concrete reasoning of large language models.\n' +
      '* Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.\n' +
      '* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9.\n' +
      '* Saad-Falcon et al. (2023) Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, David Seunghyun Yoon, Ryan A. Rossi, and Franck Dernoncourt. 2023. Pdftriage: Question answering over long, structured documents.\n' +
      '* Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools.\n' +
      '* Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. 2023. High-throughput generative inference of large language models with a single gpu. _arXiv preprint arXiv:2303.06865_.\n' +
      '* Slifa et al. (2022) Franyell Slifa, Jose Maria Arnau, and Antonio Gonzalez. 2022. E-batch: Energy-efficient and high-throughput rnn batching. _ACM Trans. Archit. Code Optim._, 19(1).\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023b. Llama: Open and efficient foundation language models.\n' +
      '\n' +
      'Mariano Trebino. 2016. mtrebi/memory-allocators: Custom memory allocators in c++ to improve the performance of dynamic memory allocation. [https://github.com/mtrebi/memory-allocators#pool-allocator](https://github.com/mtrebi/memory-allocators#pool-allocator).\n' +
      '\n' +
      'Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837.\n' +
      '\n' +
      'Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C. Schmidt. 2023. A prompt pattern catalog to enhance prompt engineering with chatgpt.\n' +
      '\n' +
      'Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: An insightful visual performance model for multicore architectures. _Commun. ACM_, 52(4):65-76.\n' +
      '\n' +
      'Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. 2022. Orca: A distributed serving system for {Transformer-Based} generative models. In _16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)_, pages 521-538.\n' +
      '\n' +
      'Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2023. Large language models are human-level prompt engineers.\n' +
      '\n' +
      'Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2023. Toolqa: A dataset for llm question answering with external tools.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '- arrival_airport: [optional] The three-letter airport code to where the customer is going.\n' +
      '- number_of_adult_travelers: [optional] Number of Adult Travelers (Default: 1).\n' +
      '- child_traveler_age: [optional] "childTravelerAge" represents the age of a single child traveler. You are required to specify the age of all child travelers. That means you must specify this parameter for each child that will be flying. Valid values are 0-17 (0 for an infant under the age of one year). If you would like to specify 3 child travelers with the ages of 1, 3 and 8 years, then your query string should contain: "childTravelerAge=1&childTravelerAge=3&childTravelerAge=8"\n' +
      '- non_stop_flight: [optional] Set to true to return only nonstop flights in the search response (Default: False).\n' +
      '- airline_preference: [optional] Optional parameter to get specific airline carrier information. By default, the preference is all.\n' +
      '* [noitemsep]\n' +
      '* name: [optional] Name of the restaurant to search for.\n' +
      '- category: [optional] Category of the restaurant to search for.\n' +
      '- city: [optional] City to search in.\n' +
      '- day: [optional] Date to search for.\n' +
      '* q: [required] Search query. Keywords and optional field filters and operators.\n' +
      '- type: [required] A comma-separated list of item types to search across. Valid types are: album, artist, playlist, track, show and episode. Search results include hits from all the specified item types. For example: "q=name:abacab&type=album,track" returns both albums and tracks with "abacab" included in their name.\n' +
      '- limit: [optional] Maximum number of results to return. Default: 20 Minimum: 1 Maximum: 50. Note: The limit is applied within each type, not on the total response. For example, if the limit value is 3 and the type is "artist,album", the response contains 3 artists and 3 albums.\n' +
      '- offset: [optional] The index of the first result to return. Default: 0 (the first result). Maximum offset (including limit): 1,000. Use with limit to get the next page of search results. Below are some examples of choosing the API that matches the user query: datetime_now=2023-11-17T10:45:07+08:00 user_query=Do you believe in God? api_call=not_found() datetime_now=2023-11-17T10:50:00+08:00 user_query=What is the price of the iPhone 15 Pro Max? api_call=bing_web_search(q="price of iPhone 15 Pro Max", set_lang="en") datetime_now=2023-11-17T11:09:10+08:00 user_query=OpenAI\'s logo api_call=bing_images_search(q="OpenAI logo", set_lang="zh") datetime_now=2023-11-17T13:21:30+08:00 user_query=What is Taylor Swift\'s latest album? api_call=spotify_search_catalog(q="Taylor Swift", type="album", limit=1) datetime_now=2023-11-17T11:21:42+08:00 user_query=Looking to eat vegan food in San Francisco this weekend, could you get me one great restaurant suggestion for Saturday? api_call=\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>