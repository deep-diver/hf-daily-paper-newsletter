<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# ChunkAttention: Prefix-Aware KV 캐시와 2상 분할을 이용한 효율적인 Self-Attention\n' +
      '\n' +
      '루예제타오용양리\n' +
      '\n' +
      'Microsoft\n' +
      '\n' +
      '{luye,zetao,yohua,yali2}@microsoft.com\n' +
      '\n' +
      '코드는 검토 후에 Github에서 공개적으로 이용 가능할 것이다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '자기 주의는 큰 언어 모델(LLM)의 필수 요소이지만 긴 시퀀스에 대한 추론 지연의 중요한 원천이다. 다중 테넌트 LLM 서비스 시나리오에서, 다수의 LLM 요청들이 프리픽스에서 공유된 시스템 프롬프트를 가질 확률을 사용함으로써 자기 주의의 계산 및 메모리 동작 비용이 최적화될 수 있다. 본 논문에서는 KV 캐시의 메모리 활용도를 향상시키기 위해 다수의 요청들에 걸쳐 매칭 프롬프트 프리픽스를 검출하고, 이들의 키/값 텐서를 런타임에 메모리에 공유할 수 있는 프리픽스 인식 셀프 어텐션 모듈인 ChunkAttention을 소개한다. 이것은 모놀리식 키/값 텐서를 더 작은 청크로 쪼개어 보조 프리픽스 트리로 구조화함으로써 달성된다. 그 결과, 프리픽스 트리 기반의 KV 캐시 위에 효율적인 자기 주의 커널을 설계하였으며, 공유 시스템 프롬프트가 있는 상태에서 자기 주의 계산 시 데이터 지역성을 향상시키기 위해 2단계 분할 알고리즘을 구현하였다. 실험 결과, ChunkAttention은 시스템 프롬프트의 길이가 1024에서 4096으로 초기 구현에 비해 3.2-4.8\\(\\times\\)만큼 셀프 어텐션 커널을 빠르게 할 수 있다.\n' +
      '\n' +
      '각주 1: 코드는 검토 후에 Github에서 공개적으로 이용가능할 것이다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '지난 몇 년 동안, LLM(Large Language Models)은 문맥 내 학습(Dong et al., 2023, 2022)에서 사상 연쇄 추론(Chu et al., 2023; Wei et al., 2022)에 이르기까지 다양한 능력을 개발했으며 광범위한 자연어 처리 관련 작업에서 놀라운 성공을 달성했다(Chang et al., 2023). 대표적인 모델은 GPT(Radford et al., 2018, 2019; Brown et al., 2020; OpenAI, 2023c), LLaMA(Touvron et al., 2023b), PaLM(Anil et al., 2023) 및 Gemini(Gemini, 2023) 시리즈이다. ChatGPT 및 GPT 스토어의 성공에 따라, LLM 기반 애플리케이션은 급증하기 시작하고, LLM의 추론 비용을 최적화하려는 수요는 새로운 연구 관심 영역이었다(Kim et al., 2023; Sheng et al., 2023; Aminabadi et al., 2022).\n' +
      '\n' +
      '셀프 어텐션 모듈은 LLM들에서 중요한 컴포넌트들 중 하나로서, 컨텍스트 토큰들(KV 캐시)의 키/값 텐서들에 대한 집중적인 메모리 동작들을 수행하고 메모리-바운드(Williams et al., 2009; Jin et al., 2023)이기 때문에 추론 동안 낮은 성능을 갖는다(표 1). 메모리 복잡도는 컨텍스트 길이에 따라 선형적으로 증가한다. 더 많은 컨텍스트 토큰에 대한 수요가 추세(GPT-4의 경우 32K)가 되면서 성능은 더 나빠진다(OpenAI, 2023c). KV 캐시는 배치 크기 및 시스템 처리량을 추가로 제한합니다. 예를 들어, FP16을 사용하여, GPT-3(17SB)의 각 토큰에 대한 KV 캐시는 4.5MB의 메모리를 필요로 한다. 8*A100(80G)을 갖는 추론 서버의 메모리는 70000개의 토큰들 또는 2K개의 컨텍스트 토큰들의 35개의 시퀀스들만을 보유할 수 있다.\n' +
      '\n' +
      '한편, LLM 기반 애플리케이션을 설계함에 있어서 일반적인 관행으로서 시스템 프롬프트는 KV 캐시(Anthropic, 2023)의 중복으로 이어진다. 전형적으로, 높은 트레이닝 및 추론 비용들로 인해, LLM들은 다수의 애플리케이션들이 공유하기 위해 다중-테넌트 아키텍처에서 사전 트레이닝되고 전개된다. 시스템 프롬프트는 LLM이 각 애플리케이션의 도메인 지식을 얻고 더 나은 결과를 생성하기 위해 필수적이다(White et al., 2023; Zhou et al., 2023). 여러 요청이 동일한 시스템 프롬프트를 공유하기 때문에 프롬프트 프리픽스(SS2.1)에 상당한 중복이 있습니다.\n' +
      '\n' +
      '중요한 질문은 시스템 프롬프트의 공유 특성을 활용하여 자기 주의 모듈을 더 빠르고 더 효율적인 메모리로 만들 수 있는지 여부이다. 우리가 아는 한, 유일한 관련 작업은 Kwon et al.(2023)에 의한 제안인데, 이 제안에서는 서비스 제공자가 애플리케이션 개발자들로부터의 미리 정의된 시스템 프롬프트들의 세트의 키/값 텐서들에 대한 메모리를 예약한다. 제안들은 제한사항들: i) 미리 정의된 시스템 프롬프트들은 정적이고, 애플리케이션 개발자들과 서비스 제공자 모두가 동작 루프에 관여하기 때문에 대규모 배포들을 위한 빈번한 리프레시들에서 유연하지 않다; ii) 긴 시스템 프롬프트들 및 낮은 히트 레이트의 경우에 메모리 낭비가 존재한다; iii) 공유된 시스템 프롬프트가 있는 경우 자체 주의 커널을 최적화하기 위한 작업이 수행되지 않았습니다.\n' +
      '\n' +
      '이를 보완하기 위해 본 논문에서는 Prefix-aware KV 캐쉬(PAKV)와 2상 분할(TPP)을 특징으로 하는 새로운 self-attention 모듈인 ChunkAttention을 제안한다. 청크어텐션의 KV 캐시는 청크된 컨텍스트 토큰 및 키/값 텐서로 구축된 프리픽스 트리이다. 따라서, KV 캐시는 프리픽스 인식되며, 사람의 관여 없이 런타임에서 동적으로 리던던시를 검출하고 제거할 수 있다. KV 캐시는 현재 디코딩 중인 시퀀스들의 키/값 텐서들만을 저장하고 메모리 낭비가 제로이다. 또한, 접두사-트리 구조는 청크-퍼스트 페이즈와 시퀀스-퍼스트 페이즈의 2-페이즈 파티션을 갖는 매우 최적화된 자기-어텐션 커널을 재설계하기 위해 청크어텐션의 컨텍스트를 제공한다. 매칭 프롬프트 프리픽스를 갖는 시퀀스로부터의 쿼리 텐서는 키/값 텐서로 주의를 수행하기 위해 함께 배치된다.\n' +
      '\n' +
      '본 논문의 주요 기여는 다음과 같다. i) 시스템 프롬프트가 길어질 수 있음을 밝히고, 자기 주의력을 최적화할 수 있는 기회를 제공한다. ii) 프리픽스 트리를 사용하여 KV 캐시를 구현하는 것을 제안한다. iii) 프리픽스 인식 KV 캐시에서 자기 주의 커널을 빠르게 하기 위해 2단계 분할 알고리즘을 구현한다. iv) 다양한 시스템 구성 하에서 공유된 시스템 프롬프트로부터 얻을 수 있는 이득 자기 주의력을 증명하고 경험적으로 정량화한다. 실험을 통해 기존의 매우 최적화된 구현에 비해 공유 시스템 프롬프트의 길이가 길어지고 공유 시스템 프롬프트가 없으면 성능 저하가 발생하지 않아 청크어텐션이 훨씬 빨라질 수 있음을 보인다.\n' +
      '\n' +
      '## 2 Preliminaries\n' +
      '\n' +
      '### 공유 시스템 프롬프트\n' +
      '\n' +
      'LLM 기반 애플리케이션 설계의 한 패러다임은 시스템 프롬프트(Anthropic, 2023)의 도입이었다. 그것은 더 나은 결과를 생성하기 위해 LLM에 대한 컨텍스트로서 명령어, 수-샷 예(Dong et al., 2022), 외부 지식을 제공한다. LLM에 대한 최종 프롬프트는 시스템 프롬프트 및 작업별 입력의 연결이다. 시스템 프롬프트는 여러 요청 간에 공유되며 매우 길어질 수 있습니다. 이는 온라인 챗봇에서 오프라인 실험에 이르기까지 다양한 LLM 기반 애플리케이션에서 관찰될 수 있다.\n' +
      '\n' +
      '도구 형성기 또는 외부 도구를 사용하는 것은 LLM이 최신 정보를 얻거나 정확한 수학 계산을 수행하는 데 필수적인 기술이 된다(Schick et al., 2023; Li et al., 2023). ChatGPT와 같은 온라인 챗봇 애플리케이션(OpenAI, 2023a)의 플러그인에 의해 구현된다. 등가 능력은 함수 호출(OpenAI, 2023b)을 통해 GPT 시리즈 모델에 의해 제공된다. 후드 아래에서 사용 가능한 기능 사양이 시스템 프롬프트(OpenAI, 2023d)에 조용히 주입됩니다. 실험들은 6개의 플러그인이 활성화된 상태에서, 공유된 시스템 프롬프트의 토큰 길이가 최대 1766(부록 A)에 도달할 수 있음을 나타낸다.\n' +
      '\n' +
      '공유 시스템 프롬프트의 또 다른 소스는 LLM에서 수행된 오프라인 연구 중심 실험이다. 이러한 시나리오에서 연구자들은 종종 동일한 지침, 예제 또는 외부 지식으로 많은 수의 템플릿 요청을 생성하여 LLM에 신속하게 발행한다. 예시 작업은 i) 카멜레온(Lu et al., 2023) ScienceQA 및 TabMWP 데이터 세트에 대한 구성 추론을 위한 정책 계획 및 도구 호출 프롬프트를 재사용하는 단계; ii) CREATOR(Qian et al., 2023) CoT(Cain-of-thought) 프롬프트 템플릿을 사용하여 TabMWP 및 MATH 데이터 세트로부터의 질문들의 컬렉션을 구성하는 단계; iii) PDFTriage(Saad-Falcon et al., 2023) PDFTriage(Saad-Falcon et al., 2023) PDF 문서 메타데이터를 프롬프트에 주입하고 문서를 통해 다수의 질문-응답(QA) 태스크를 실행하는 단계; iv) ToolQA(Zhuang et al., 2023)는 QA 데이트셋을 추가로 방출하고 평가용 시스템 프롬프트를 재사용하는 단계를 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c c} \\hline \\hline Batch Size & \\multicolumn{1}{c|}{Roofline} & QKV Projection & Self Attention & MLP \\\\ \\hline \\hline \\multirow{3}{*}{1} & FLOPs(\\(\\sim\\)10\\({}^{7}\\)) & 100.66 & 33.57 & 270.53 \\\\  & Adrinknet Intensity & 1.00 & **0.99** & 1.00 \\\\  & Latency(qa) & 88.44 & **17.82** & 160.77 \\\\ \\hline \\multirow{3}{*}{32} & HOPs(\\(\\sim\\)10\\({}^{7}\\)) & 3221.23 & 1074.27 & 8657.04 \\\\  & MOPs(\\(\\sim\\)10\\({}^{6}\\)) & 101.71 & 1083.12 & 273.43 \\\\  & Arithmetic Intensity & 31.67 & **0.99** & 31.66 \\\\  & Latency(qa) & 90.02 & **687.74** & 209.82 \\\\ \\hline \\multirow{3}{*}{64} & FLOPs(\\(\\sim\\)10\\({}^{7}\\)) & 6442.45 & 2148.53 & 17314.09 \\\\  & Adrinknet Intensity & 102.76 & 2166.36 & 276.33 \\\\ \\cline{1-1}  & Latency(qa) & 62.69 & **0.99** & 62.66 \\\\ \\cline{1-1}  & Latency(qa) & 90.04 & **1388.40** & 217.79 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 하나의 단일 토큰을 디코딩할 때 각 디코더 계층에서의 키 모듈들의 복잡도 분석. Llama2 7B, 2048 컨텍스트 토큰, FP16, A100(80G). 자기-어텐션 모듈은 낮은 산술 강도(Williams et al., 2009) 및 높은 레이턴시를 갖는다. FLOP: 부동 소수점 연산. MOP: 메모리 동작 또는 액세스된 메모리 바이트. 산술 강도: FLOPs/MOPs.\n' +
      '\n' +
      'LLM과 QA의 평가. 표 2는 시스템 프롬프트의 공유 토큰 카운트에 대한 통계를 나타낸다.\n' +
      '\n' +
      '### LLM Inferencing\n' +
      '\n' +
      'LLM의 전형적인 추론 과정은 프리필링과 디코딩의 두 단계로 구성된다(Sheng et al., 2023). 시퀀스\\(S=[t_{1},...,t_{n_{p}}]])을 수신한 후, 서버는 프리필하기 시작한다. 프리필하는 동안, 모든 \\(n_{p}\\) 프롬프트 토큰 \\(t_{1},...,t_{n_{p}}\\)을 LLM에 공급하고, 어텐션 키/값 텐서를 계산하고, 후속 계산의 속도를 높이기 위해 캐시한다. 그리고, 서버는 디코딩을 수행한다. 디코딩은 오토-레그레시브이고, LLM들에 대한 입력 토큰은 이전의 디코딩 반복으로부터 생성된 완료 토큰(또는 출력 토큰)이다. 프로세스는 시퀀스 종료 토큰 또는 최대 완료 토큰이 생성될 때까지 계속된다.\n' +
      '\n' +
      '서버가 \\(b\\)(배치 크기) 시퀀스 \\(S_{1},...,S_{b}\\)을 동시에 디코딩하는 경우, 비록 상이한 반복에 있지만, 서버는 여전히 반복의 세분도에서 배칭을 수행할 수 있고, 반복 기반 배칭으로 알려져 있는 개별이 아닌 모든 시퀀스에 대한 다음 토큰을 함께 예측할 수 있다(Gao et al., 2018; Yu et al., 2022; Silfa et al., 2022). 구체적으로 반복기반 배칭은 다중 시퀀스(시퀀스당 하나의 토큰) \\(t^{(1)},...,t^{(b)}(t^{(i)}\\in S_{i})\\)의 마지막 입력 토큰들을 단일 입력 \\(\\mathbf{T}\\)으로 연결하고, 자기 주의 이전의 QKV 프로젝션, 자기 주의 이후의 출력 프로젝션 및 다층 퍼셉트론을 계산한다. 중간에서 자기 주의는 공유된 가중치가 없으며 각 시퀀스에 대해 독립적으로 계산될 필요가 있다. 디코딩 동안, 새로운 시퀀스들이 합류할 수 있고, 완성된 시퀀스들이 떠날 수 있어, 큰 배치들을 형성할 가능성을 상당히 증가시킨다. 반복 기반 배칭은 vLLM(Kwon et al., 2023) 및 텍스트 생성 추론 서버(HuggingFace, 2023)에 의해 구현되었다. 이 논문에서 ChunK 주의는 반복 기반 배치가 커널이 효율적으로 실행되도록 배치를 형성할 수 있다고 가정한다.\n' +
      '\n' +
      '##3. 접근\n' +
      '\n' +
      '### Prefix Aware KV Cache (PAKV)\n' +
      '\n' +
      '전통적으로 KV 캐시는 크기\\(b\\times h\\times n\\times d\\)의 밀집 텐서에 저장되며, 여기서 \\(b\\)은 배치 크기, \\(h\\)은 헤드 수, \\(n\\)은 시퀀스 길이, \\(d\\)은 헤드 치수 크기이다.\n' +
      '\n' +
      '다수의 시퀀스들이 공통 프리픽스 토큰들을 공유할 때, 키/값 텐서들은 동일하고 따라서 메모리에서 공유될 수 있다. 예를 들어, 특정 LLM 추론 서버는 시퀀스 \\(S_{i}=[t_{1},...,t_{n_{s}},t_{n_{s}+1},...,t_{n_{p}}]\\)을 먼저 수신한 후, 시퀀스 \\(S_{j}=[t_{1},...,t_{n_{s},t^{\\prime}_{n_{s}+1},...,t^{\\prime}_{n_{p}}]]를 먼저 수신한다. \\(t_{1},...,t_{n_{s}}\\)에 대한 KV 캐시는 메모리에 하나의 물리적 복사본만 가질 수 있다.\n' +
      '\n' +
      '이러한 특성을 고려할 때, 우리는 KV 캐시를 프리픽스 트리로 디코딩하는 모든 시퀀스의 KV 캐시를 구성하는 프리픽스 인식(prefix-aware)을 해야 한다고 주장한다. 정확하게는 시퀀스 길이 차원을 따라 메모리에 연속되는 모놀리식 키/값 텐서를 슬라이스한다. 그림 1은 프리픽스 트리에 저장된 KV 캐시의 구조를 보여준다. 각 노드는 세 가지 필수 요소를 저장하는 청크\\(C\\)을 정의한다. i) 프리픽스 트리 연산을 가능하게 하기 위해 시퀀스들에 의해 공유되는 \\(S_{i},...,S_{j}\\) 컨텍스트 토큰들의 세그먼트; ii) 상기 \\(c\\) 토큰들에 대한 크기 \\(b\\times h\\times c\\times d\\)의 키 텐서의 슬라이스; iii) 값 텐서의 해당 슬라이스. 접두사 트리의 각 경로는 시퀀스를 정의합니다. 서버에 여러 개의 트리(포리스트)가 동시에 존재할 수 있습니다. 예를 들어, 애플리케이션 개발자는 다른 시스템 프롬프트를 설계합니다.\n' +
      '\n' +
      '추론 중에는 i) 새로운 시퀀스 조인, ii) 완료된 시퀀스 리프, 및 iii) 모든 시퀀스가 하나의 토큰을 함께 디코딩하는 세 가지 가능한 시나리오가 있다. 각 시나리오는 프리픽스 트리 연산으로 변환될 수 있다. 새로운 시퀀스가 결합하면, 프리픽스 트리가 검색되고 업데이트되어 새로운 경로가 삽입된다. 완료된 시퀀스가 떠나면 접두사 트리가 업데이트되어 경로가 삭제됩니다. 각각의 디코딩 반복에서, 우리는 리프 청크에 새로운 토큰을 추가하거나 리프 청크가 가득 차면 새로운 청크를 성장시킨다.\n' +
      '\n' +
      '고정된 청크 크기(\\(c\\)가 주어지면 메모리 관리가 효율적이다. ChunKAttention에서, 풀 기반 메모리 할당기는 디폴트로 채택된다(Hill, 1992; Trebino, 2016). 사용한 청크 목록과 무료 청크 목록을 모두 추적합니다. 새로운 청크가 요청되면,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline \\multirow{2}{*}{System} & \\multirow{2}{*}{Usage of Prompt} & \\multicolumn{2}{c}{\\#shared prompt tokens} \\\\ \\cline{3-4}  & & avg & max \\\\ \\hline Chandleon & Tools definition and examples1  & 1324 & 2626 \\\\ CREATOR & CoF examples2  & 879 & 2492 \\\\ PPPriage & PPG document metadata & 4257 & N.A. \\\\ ToolQA & Tools definition and examples3  & 1432 & 1432 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: OpenAI의 tiktoken tokenizer library(OpenAI, 2023e)에 의해 토큰화된, 시스템 프롬프트 내의 공유 프롬프트 토큰.\n' +
      '\n' +
      '할당기는 자유 리스트로부터 청크를 반환하거나 운영 체제(OS)로부터 신선한 메모리를 할당한다. 사용되지 않은 청크는 시퀀스가 완료되면 할당기로 반환되지만 할당기는 OS에 메모리를 릴리즈하지 않으므로 불필요한 메모리 할당을 방지합니다. 정렬을 위한 일부 메모리 공간이 사용되지 않습니다. 시퀀스 길이가 \\(n\\)일 때, 메모리 손실은 \\((c-1)/n\\으로 제한된다.\n' +
      '\n' +
      '공통 접두사를 공유함으로써 동시에 처리할 수 있는 시퀀스의 수는 대략 \\(1/(1-r)\\만큼 증가한다. 공유 비율 \\(r\\)은 공유 토큰 \\(n_{s}/(n_{p}+n_{c})\\)의 백분율로 정의되며, \\(n_{c}\\)은 완료 토큰 카운트이다. 메모리 제한 추론 시나리오에서, 이것은 배치 크기를 증가시키고 따라서 처리량을 개선하는 데 도움이 된다.\n' +
      '\n' +
      '부모-자식 관계는 각 청크 커버들의 시퀀스들의 서브세트를 정의한다. 루트 노드는 모든 시퀀스를 커버하고 리프 노드는 오직 하나만을 커버한다. 접두사 트리의 핵심 특성은 접두사 트리의 각 청크에 의해 커버되는 시퀀스가 시퀀스 인덱스 차원에서 연속된다는 것이다. 따라서, 자기 주의에서 질의 텐서를 슬라이싱하는 것은 커널 계산 동안 특히 효율적이며, 이는 다음 섹션에서 더 자세히 논의될 것이다.\n' +
      '\n' +
      '###TPP(Two-phase Partition)\n' +
      '\n' +
      '이 섹션에서는 고유한 프리픽스 인식 KV 캐시 저장소 위에 있는 자체 주의 커널 구현에 뛰어들었다.\n' +
      '\n' +
      '프리필링 동안, 정합된 프롬프트 프리픽스에 대한 KV 프로젝션 및 위치 임베딩의 반복적인 계산을 피하기 위해 프리픽스 룩업을 수행한다. 비일치 접미사 토큰의 경우 KV 투영 및 위치 임베딩을 여전히 계산하고 키/값 텐서를 청크하여 접두사 트리에 삽입한다. 그런 다음 전체 키/값 텐서에 기존의 고도로 최적화된 자기 주의 커널인 _e.g._, FlashAttention(Dao, 2023)을 적용한다.\n' +
      '\n' +
      '반복 디코딩 동안, 셀프-어텐션은 청크-퍼스트 및 시퀀스-퍼스트 페이즈로 분할된다. 두 단계는 쿼리 텐서, KV 캐시 청크의 서로 다른 슬라이스에 초점을 맞추고 서로 다른 병렬화 전략을 사용한다. 이 과정은 그림 2와 같다. 머리 치수는 항상 분할되어 있으므로 생략되고 우리의 논의에 내포되어 있다.\n' +
      '\n' +
      '\'청크 퍼스트 페이즈\' 청크-첫 번째 단계에서는 여러 시퀀스가 공유하는 청크만 처리한다. GPU는 헤드 수(Llama 7B의 경우 32개)보다 더 많은 스트리밍 멀티프로세서(A100의 경우 108개)를 가지고 있으며, 헤드에 의한 분할은 하드웨어 자원을 과소용하기 때문에 키/값에 대한 추가 분할을 수행한다. 청킹은 이미 편리함을 제공합니다. 온라인 소프트맥스 알고리즘은 파티션 간의 동기화 요구 사항을 피하기 위해 채택된다(Milakov and Kimelshein, 2018; Dao, 2023).\n' +
      '\n' +
      '알고리즘 1에 도시된 바와 같이, 프리픽스 트리 내의 공유 청크들을 순회하고, 부분 어텐션 커널 _partial_attn_을 실행하고 부분 어텐션 결과들을 메모리에 저장하는 것에 의해 계산이 수행된다. 시퀀스들의 수(배치 크기)는 \\(b\\)로 표시된다. \\(b\\) (\\mathbf{Q}\\in\\mathbb{R}^{b\\times d}\\)는 모든 \\(b\\) 시퀀스의 마지막 토큰을 최신 디코딩 반복에서 연결하여 생성된 쿼리이다.\n' +
      '\n' +
      '_partial_attn_의 구현은 Eqn에 의해 주어진다. (1). 각 청크 \\(C\\)에 대한 부분 주의 결과 \\((\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\)를 독립적으로 계산하여 병렬화 할 수 있다. (\\mathbf{Q}_{i:j,:}\\)는 청크(C\\)에 저장된 KV 캐시를 공유하는 \\(i\\)에서 \\(j\\)까지의 시퀀스에 대한 \\(\\mathbf{Q}\\)의 슬라이스이다. 상기 제어부는\n' +
      '\n' +
      '도 1: 프리픽스 트리의 KV 캐시. \\(S_{0},S_{1},S_{2}\\)의 프롬프트의 지시사항 및 예는 공통적이고 공유가능하다. 질문은 다르며 공유할 수 없습니다. 일부 메모리는 정렬로 인해 사용되지 않습니다.\n' +
      '\n' +
      '최대 주의력 가중치 벡터 \\(\\mathbf{M}^{(C)}\\)는 주의력 가중치 \\(\\mathbf{W}^{(C)}\\)의 마지막 차원에 대한 행별 최대값이다. 소프트맥스 정규화 항 \\(\\mathbf{n}^{(C)}\\)은 \\(\\mathbf{E}^{(C)}\\)의 마지막 차원에 걸친 행-와이즈 합이다. \\ (\\mathbf{M}^{(C)}\\) 및 \\(\\mathbf{n}^{(C)}\\)은 다중 청크의 부분 주의 결과를 더 누적하기 위해 도입된 보조 변수이다.\n' +
      '\n' +
      'bf{Q}_{i;j}\\mathbf{K}^{(C)}=\\max\\left(\\mathbf{W}^{(C)\\times d}\\end{split}}^{(C)}=\\exp\\left(\\mathbf{W}^{(C)\\times c}\\times\n' +
      '\n' +
      '상기 _partial_atm_은 다수의 시퀀스에 대한 자기 관심이 배치되기 때문에 공유된 KV 캐시 메모리에 효율적으로 액세스한다. 배칭은 수열의 질의(S_{i},...,S_{j}\\)와 공유(\\(\\mathbf{K}^{(C)}/\\mathbf{V}^{(C)}\\) 사이의 점-생성물의 입도에서 발생한다. 향상된 데이터 지역성에 더하여, 배칭의 또 다른 장점은 벡터로부터의 질의를 매트릭스로 바꾸는 것이며, 텐서 코어들과의 효율적인 매트릭스 곱셈을 허용한다(Choquette et al., 2021).\n' +
      '\n' +
      '** 시퀀스-퍼스트 페이즈** 시퀀스-첫 번째 단계에서는 청크-첫 번째 단계에서 공유 청크의 부분 주의 결과를 로드하고 하나의 특정 시퀀스와 관련된 청크를 계속 처리한다. 우리는 수열을 분할하고, 수열 우선 커널이 다루는 각 \\(\\mathbf{q}\\)은 알고리즘 2와 같이 \\(\\mathbf{Q}\\)의 \\(i\\)번째 행을 슬라이싱하여 벡터이다.\n' +
      '\n' +
      '```\n' +
      '0:\\(\\mathbf{Q}\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '0:\\(\\mathbf{Q}\\in\\mathbb{R}^{b\\times d}\\) (어텐션 출력)\n' +
      '0:\\(\\mathbf{G}\\in\\mathbb{R}^{b\\times d}\\) (어텐션 출력)\n' +
      '0:for\\(\\mathbf{q}\\gets\\mathbf{q}_{1}\\) to \\(\\mathbf{q}_{2}\\)do\n' +
      '0:\\(\\mathbf{m},\\mathbf{n}^{(C)}\\leftarrow(\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\)\\(\\mathbf{o}\\)\\(\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\)do\n' +
      '0: 부분 attn 결과 \\((\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\leftarrow\\)((\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)})}\\(\\text{data}_{reduce}(\\mathbf{o}^{(C)},\\mathbf{m},\\mathbf{n})^{(C)})}\\(text{data}_{reduce}(\\mathbf{o}},\\mathbf{m},\\mathbf{n})^{(C)}))) endfor\n' +
      '0: get chunk \\(C_{k}\\)+\\(C_{k+1}\\), \\(C_{k+2}\\), \\(C_{k}\\) in \\(C\\mathbf{C}^{*}\\))\\(C\\mathbf{m},\\mathbf{n}^{(C)}\\leftarrow\\) partial_atm(\\(\\mathbf{o},\\mathbf{m},\\mathbf{n}^{(C)},\\mathbf{n}}},\\mathbf{n}},\\mathbf{n}}) endfor endfunction\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 셀프 어텐션 : 청크 퍼스트(파티션 청크)\n' +
      '\n' +
      '_partial_\\(\\mathbf{Q}\\)\\(\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '\n' +
      '_atm_\\(\\mathbf{q}\\)\\(\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '\n' +
      '\\(\\mathbf{m}^{(C)}=\\max\\left(\\mathbf{W}^{(C)}\\right)\\in\\mathbb{R}^{(j-i)}\\)\n' +
      '\n' +
      '(\\mathbf{E}^{(C)}=\\exp\\left(\\mathbf{W}^{(C)}-\\mathbf{m}^{(C)}\\cdot\\mathbf{1}^{T}\\right)\\in\\mathbb{R}^{(j-i)\\times c}\\)\n' +
      '\n' +
      '\\(\\mathbf{n}^{(C)}=\\text{sum}\\left(\\mathbf{E}^{(C)}\\right)\\in\\mathbb{R}^{(j-i)}\\)\n' +
      '\n' +
      '\\(\\mathbf{O}^{(C)}=\\mathbf{E}^{(C)}\\mathbf{V}^{(C)}\\in\\mathbb{R}^{(j-i)\\times d}\\)\n' +
      '\n' +
      '상기 _partial_atm_은 다수의 시퀀스에 대한 자기 관심이 배치되기 때문에 공유된 KV 캐시 메모리에 효율적으로 액세스한다. 배칭은 수열의 질의(S_{i},...,S_{j}\\)와 공유(\\(\\mathbf{K}^{(C)}/\\mathbf{V}^{(C)}\\) 사이의 점-생성물의 입도에서 발생한다. 향상된 데이터 지역성에 더하여, 배칭의 또 다른 장점은 벡터로부터의 질의를 매트릭스로 바꾸는 것이며, 텐서 코어들과의 효율적인 매트릭스 곱셈을 허용한다(Choquette et al., 2021).\n' +
      '\n' +
      '** 시퀀스-퍼스트 페이즈** 시퀀스-첫 번째 단계에서는 청크-첫 번째 단계에서 공유 청크의 부분 주의 결과를 로드하고 하나의 특정 시퀀스와 관련된 청크를 계속 처리한다. 우리는 수열을 분할하고, 수열 우선 커널이 다루는 각 \\(\\mathbf{q}\\)은 알고리즘 2와 같이 \\(\\mathbf{Q}\\)의 \\(i\\)번째 행을 슬라이싱하여 벡터이다.\n' +
      '\n' +
      '```\n' +
      '0:\\(\\mathbf{Q}\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '0:\\(\\mathbf{Q}\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '0:for\\(\\mathbf{q}\\gets\\mathbf{q}_{1}\\) to \\(\\mathbf{q}_{2}\\)do\n' +
      '0:\\(\\mathbf{m},\\mathbf{n}^{(C)}\\leftarrow(\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\)\\(\\mathbf{o}\\)\\(\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)}\\)do\n' +
      '0: \\(\\mathbf{q}^{*}\\)((\\mathbf{o},\\mathbf{m},\\mathbf{n})^{(C)}\\leftarrow\\)\\(\\text{slicing}\\)((\\mathbf{O},\\mathbf{m},\\mathbf{n})^{(C)})}\\(\\text{data}_{reduce}(\\mathbf{o}},\\mathbf{m},\\mathbf{n}}}}},\\mathbf{n}}}}}},\\mathbf{m},\\mathbf{n}}})}\\(\\text{slicing}\\)\\(\\mathbf{O},\\mathbf{m},\\mathbf{n}})^{(C)}},\\mathbf{n}}^{(C)}},\\mathbf{n}}^{(C)}},\\mathbf{\n' +
      '0: \\(\\mathbf{C}^{*}\\)에 대한 \\(\\mathbf{C}^{*}\\)에 대한 \\(\\mathbf{C}^{*}\\)의 \\(\\mathbf{o},\\mathbf{m},\\mathbf{n}^{(C)}},\\mathbf{m},\\mathbf{n}}에 저장된 값 캐시\n' +
      '```\n' +
      '\n' +
      '**알고리즘 2** 자기 주의: 시퀀스 우선(파티션 시퀀스)\n' +
      '\n' +
      '_atm_\\(\\mathbf{q}\\)\\(\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '\n' +
      '_atm_\\(\\mathbf{q}\\)\\(\\in\\mathbb{R}^{b\\times d}\\) (query), \\(T\\)(prefix tree)\n' +
      '\n' +
      '\\(\\mathbf{O}_{i;,}=x^{(C)}\\mathbf{o}^{(C)}+y^{(C)}\\mathbf{O}_{i;,}\\in\\mathbb{R}^{d}\\)\n' +
      '\n' +
      '\\(\\mathbf{n}_{i}=x^{(C)}n^{(C)}+y^{(C)}\\mathbf{n}_{i}\\in\\mathbb{R}\\)\n' +
      '\n' +
      '\\(\\mathbf{m}_{i}=\\text{max}\\left(m^{(C)},\\mathbf{m}_{i}\\right)\\in\\mathbb{R}\\)\n' +
      '\n' +
      '그림 2: 청크어텐션에서 2상 파티션 커널. 서버는 \\(S_{0}\\), \\(S_{1}\\), \\(S_{2}\\)의 시퀀스를 디코딩한다. 그들은 청크 \\(C_{0}\\), \\(C_{1}\\) 및 \\(C_{2}\\)을 공유한다. 청크-퍼스트 단계에서 질의는 \\(\\mathbf{q}_{0}\\), \\(\\mathbf{q}_{1}\\) 및 \\(\\mathbf{q}_{2}\\)으로 자기 집중을 위해 배치된다. 부분 주의 결과 \\(\\mathbf{O}^{(C)}\\), \\(\\mathbf{m}^{(C)}\\) 및 \\(\\mathbf{n}^{(C)}\\)은 메모리에 저장된다. 시퀀스 첫 번째 단계에서는 각 시퀀스에 대한 \\(\\mathbf{o}_{i}\\), \\(m_{i}\\), \\(n_{i}\\)을 복원하고 남은 청크를 \\(\\mathbf{q}_{i}\\)에 대해서만 계속 처리한다.\n' +
      '\n' +
      '### Further Optimizations\n' +
      '\n' +
      '프리픽스 트리 구조는 CPU 메모리에 유지된다. GPU에서 2상 패테이션 커널을 실행하려면 접두어 트리에서 특정 컨텍스트를 생성해야 하는데, 이 컨텍스트는 커버된 시퀀스의 청크(C\\), 시작 인덱스(i\\) 및 끝 인덱스(j\\)를 포함하고 CPU에서 GPU 메모리로 컨텍스트(\\(C\\), i\\, j\\)를 복사해야 한다. 예를 들어, 도 2에서 우리는 \\((C_{0}/C_{1}/C_{2},0,2)\\), \\((C_{3},0,0)\\), \\((C_{4}/C_{6},1,1)\\), \\((C_{5}/C_{7},2,2)\\)를 생성하고 복사해야 한다. chunkAttention은 두 가지 방법으로 오버헤드를 관리하는데, i) latency hiding. CPU 상의 context 생성 단계는 self-attention 이전에 GPU 상의 다른 커널과 중복될 수 있다. ii) 게으른 문맥 복사. 프리픽스 트리는 모든 디코딩 반복에서 변경되지 않는다. 우리는 GPU 메모리에 컨텍스트를 캐싱할 수 있으며 트리 구조가 변경될 때만 메모리 복사본을 트리거할 수 있다. 트리거는 모든 \\(c\\) 반복, 새로운 시퀀스 결합 및 완료된 시퀀스 이탈에 대해 청크 풀이다. 간접비는 상각된다.\n' +
      '\n' +
      '청크-퍼스트 페이즈에서 부분 어텐션 결과를 위해 할당된 임시 메모리는 부분 어텐션 결과를 최종 결과로 직접 병합하기 위해 _partial_attn_ 바로 뒤에 _attn_reduce_를 실행함으로써 제거될 수 있다. 접두 트리에서 부모-자식 관계를 갖는 다수의 공유 청크들은 \\((\\mathbf{O},\\mathbf{m},\\mathbf{n})\\)의 동일한 슬라이스에 기록되기 때문에 _attn_reduce_는 직렬화될 필요가 있다. GPU 장치에서 원자 작업은 무겁고 우리는 이 접근법을 사용하지 않는다. 그러나 CPU 디바이스에서는 직렬화의 오버헤드가 미미하며, 스핀 락을 이용하여 리덕션을 구현할 수 있다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '평가들은 자기 주의 마이크로커널 레벨과 엔드 투 엔드 GPT 스타일 모델 레벨 모두에서 수행된다. 마이크로 커널 수준 평가는 자기 주의 CUDA 커널에서 보내는 시간만 캡처한다. PAKV 및 TPP의 부작용인 _예: 접두사 트리 연산은 종단 간 평가에서 캡처된다. NVIDIA A100 GPU(80G)와 CUDA 11.8로 모든 실험을 실행합니다.\n' +
      '\n' +
      '### Microkernel Evaluation\n' +
      '\n' +
      '기준선 기본적인 네 가지 자기 주의 구현을 선택하는데, 공식 \\(\\text{softmax}(\\mathbf{Q}\\mathbf{K}^{T}/\\sqrt{d})\\mathbf{V}\\), xformers Lefaudeux 등(2022)에서 구현된 메모리 효율적인 자기 주의, PyTorch Dao 등(2022)에서 통합된 Flash 주의, vLLM Kwon 등(2023)에서 Paged 주의이다.\n' +
      '\n' +
      '나이브, xformers, FlashAttn은 모두 monolithic KV 텐서에 기반하기 때문에 프롬프트 프리픽스의 KV 캐시를 부분적으로 공유하여 프리픽스를 인식할 수 없다. PagedAttn도 PAKV를 구현하지 않는다. 그러나 페이징 설계를 통해 고정된 페이지 테이블을 수동으로 생성할 수 있어 사실상 공유되지 않은 메모리를 동일한 물리적 메모리에 매핑할 수 있다. KV 캐시 공유 시나리오를 시뮬레이션하고 PagedAttn*로 표기된 PagedAttn의 CUDA 커널의 성능을 관찰하는 데 도움을 준다. 어떤 커널도 TPP 알고리즘을 지원하지 않는다.\n' +
      '\n' +
      '워크로드 시퀀스는 배치 모드에서 처리되며 배치 크기는 \\(b\\)이다. 동일한 배치 내의 모든 시퀀스가 동시에 시작 및 종료됩니다. 각 시퀀스는 \\(n_{p}\\) 프롬프트 토큰으로 프리필링되고, 리딩 \\(n_{s}\\) 토큰은 공통 프리픽스이다. 작업은 다음 \\(n_{c}\\) 완료 토큰을 반복적으로 디코딩하는 것이다. 토큰 속도(tokens per second or TPS, \\(n_{c}*b/t\\))에 의해 정의된 디코딩 지연시간 \\(t\\)과 처리량을 측정한다. 모든 실험에서 헤드 차원 \\(d\\)은 128, 헤드 수 \\(h\\)은 32, 청크 크기 \\(c\\)은 64이며 모든 텐서는 FP16에 있다.\n' +
      '\n' +
      '**결과** 즉시 및 공유 토큰 수, 완료 토큰 수, 배치 크기 등의 시스템 하이퍼파라미터를 변화시켜 PAKV와 TPP가 가져오는 성능 이득을 관찰하기 위한 실험을 수행한다.\n' +
      '\n' +
      '표 3은 다양한 프롬프트 및 공유 토큰 카운트가 주어진 셀프-어텐션 구현의 레이턴시를 나타낸다. ChunkAttn 및 PagedAttn*는 공유 토큰 카운트에 불가지론적인 Naive, xformers, FlashAttn 및 PagedAttn을 능가한다. Naive는 ChunkAttn과 PagedAttn*보다 각각 6.6\\(\\times\\)과 2.1\\(\\times\\) 느려졌다 (\\(n_{s}\\)=4096). PagedAttn*과 PagedAttn을 비교하여 물리적으로 KV 캐시 메모리를 공유함으로써 얻어지는 성능 이득을 관찰한다. PagedAttn*은 PAKV 또는 TPP를 구현하지 않지만, 하드웨어 캐시는 PagedAttn(\\(n_{s}\\)=4096): 동일한 물리적 메모리 블록에 반복적으로 액세스하는 것에 비해 최대 52%의 지연 시간을 감소시키는 것을 돕는다. TPP의 이점은 PagedAttn*와 ChunkAttn을 비교함으로써 더욱 알 수 있다. ChunkAttn은 PagedAttn*보다 2.8-3.2\\(\\times\\)의 성능을 보였으며, 1024에서 4096까지의 범위는 \\(n_{s}\\)이다. TPP는 토큰이 공유되지 않을 때 성능 회귀를 일으키지 않는다 (표 3의 \\(n_{s}\\)=0, ChunkAttn vs. PagedAttn*). 결과적으로 TPP는 항상 활성화되어야 합니다.\n' +
      '\n' +
      '디코딩이 진행됨에 따라 시퀀스가 발산되기 시작하여 도 3에 도시된 바와 같이 ChunkAttng의 성능 이득이 점진적으로 감소한다. 2048개의 공유 토큰이 주어졌을 때, ChunkAttn은 \\(n_{c}\\)이 512에 도달했을 때 PagedAttn에 비해 3.6\\(\\times\\)의 토큰 속도 향상을 얻었고, \\(n_{c}\\)이 2048에 도달했을 때 2.3\\(\\times\\)의 속도 향상을 얻었지만 여전히 상당한 개선이다. PagedAttn*보다 ChunkAttn의 개선은 PagedAttn*이 물리적으로 공유된 KV 캐시 메모리로부터 이익을 얻고, TPP만이 여기서 차이를 만들기 때문에 더 낮다. 그러나 \\(n_{s}\\)=2048일 때, ChunkAttn은 각각 512와 2048일 때 PagedAttn*보다 여전히 2.0\\(\\times\\)(73K에 대해 145K), 1.5\\(\\times\\)(46K에 대해 70K) 빠르다.\n' +
      '\n' +
      '그림 4는 배치 크기를 변경하는 데 중점을 둔다. ChunkAttn 및 PagedAttn*를 제외한 모든 구현의 경우, 처리량은 메모리 바인딩으로 인해 배치 크기가 16에 도달할 때 최고조에 달한다. 주어진 \\(n_{s}\\)은 2048이고, ChunkAttn의 처리량은 더 나은 데이터 지역성과 향상된 산술 강도로 인해 16에서 96까지의 배치 크기에 대해 155k에서 224k 토크/s로 계속 증가한다.\n' +
      '\n' +
      '### End-to-end Evaluation\n' +
      '\n' +
      'ChunkLlama는 Apache-2.0 라이센스 하에서 Huggingface Llama와 vLLM의 최적화된 커널(레이어 정규화 및 로터리 임베딩) 위에 구축되지만, 주의 모듈은 ChunkAttn으로 대체된다. FP16 [11, 12, 13]에서 Open Llama2 7B 모델에 대한 모든 실험을 실행했다.\n' +
      '\n' +
      '기준선 제작 용도가 입증된 툴킷을 제공하는 널리 사용되고 최적화된 두 개의 LLM을 선정한다: 초기 vLLM 0.2.7 [13]과 Huggingface의 Text Generation Inference (TGI) 1.3.4 [12].\n' +
      '\n' +
      '워크로드 요청은 초당 평균 요청(RPS)인 \\(\\lambda\\)으로 매개변수화된 포아송 도착 프로세스[10]에 따라 서버에 랜덤하게 도착합니다. 실제 배치 크기는 디코딩 동안 각 시스템에 의해 동적으로 조정되며 최대 32까지 동일하게 구성한다. 응용 프로그램 개발자는 서비스 공급자가 미리 구성할 공유 프롬프트 프리픽스에 대한 정보를 제공하지 않습니다. 각 요청의 엔드 투 엔드 레이턴시 \\(t\\)(큐잉 시간 포함)을 완료 토큰 카운트 \\(n_{c}\\)으로 나눈 평균인 vLLM에서와 같이 정규화된 레이턴시 (ms/tok 또는 1/TPS)와 KV 캐시가 사용하는 피크 메모리 바이트를 측정한다.\n' +
      '\n' +
      '**결과** ChunkLlama는 그림 5와 같이 가장 빠른 추론 속도를 얻을 수 있다. ChunkLlama는 40 ms/token 미만의 정규화된 레이턴시를 유지하면서 1024 및 2048 프리픽스 토큰이 공유될 때 vLLM에 비해 1.6\\(\\times\\)(1.8에 대해 2.9) 및 2.3\\(\\times\\)(1.0에 대해 2.3) 더 높은 처리량을 달성할 수 있다. 표 4는 ChunkLlama의 지연 시간과 KV 캐시 메모리 사용량을 vLLM에 비교한다. 공유 접두사 토큰이 없는 ChunkLlama에서는 성능 회귀가 관찰되지 않습니다. KV 캐시 메모리 사용량은 긴 공유 접두사에서 70%-90% 감소합니다. ChunkLlama가 더 빠르게 디코딩할 수 있기 때문에 피크 배치 크기도 20%-40% 감소한다.\n' +
      '\n' +
      '## 5 관련 업무\n' +
      '\n' +
      'KV 캐시의 메모리 활용을 최적화하기 위한 가장 적절한 작업은 vLLM에서 PagedAttention이다.\n' +
      '\n' +
      '도 4: 다양한 배치 크기가 주어진 \\(n_{c}\\)=64개의 완료 토큰까지 디코딩할 때의 토큰 레이트. 청크 크기 \\(c\\)=64.\n' +
      '\n' +
      '그림 3: \\(n_{c}\\) 완료 토큰을 생성할 때 토큰 속도의 처리량, 주어진 \\(n_{s}\\) 프리픽스 토큰이 공유된다. 청크 크기 \\(c\\)=64, 배치 크기 \\(b\\)=32.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\multirow{2}{*}{\\(n_{c}\\)} & \\multirow{2}{*}{\\(n_{s}\\)} & \\multicolumn{4}{c}{Latency (ms)} \\\\ \\cline{3-8}  & & Noise & Address & FlushAttn & PagedAttn & PagedAttn* & ChunkAttn \\\\ \\hline\n' +
      '1024 & 0 & 363.35 & 378.19 & 1586.73 & 356.17 & 355.82 & 332.50\\\\\n' +
      '1024 & 512 & 364.73 & 385.79 & 1587.14 & 355.88 & 257.34 & **198.57**\n' +
      '1024 & 768 & 362.43 & 378.50 & 1591.61 & 366.02 & 125.18 & **131.21**\n' +
      '1024 & 1024 & 361.76 & 379.36 & 1569.09 & 355.44 & 154.46 & **560.00** \\\\ \\hline\n' +
      '3048 & 0 & 686.40 & 816.44 & 3175.25 & 705.98 & 703.50 & 655.44\\\\\n' +
      '3048 & 1024 & 687.52 & 827.86 & 3773.53 & 703.50 & 355.02 & **344.37**\\\\\n' +
      '3048 & 1536 & 687.88 & 820.19 & 7149.76 & 900.29 & 412.25 & **207.14**\n' +
      '3048 & 2048 & 688.41 & 823.60 & 3152.25 & 703.28 & 338.41 & **110.48** \\\\ \\hline\n' +
      '4096 & 0 & 1399.20 & 1720.00 & 6598.95 & 1400.60 & 1400.17 & 1301.78 \\\\\n' +
      '4096 & 2048 & 1370.47 & 1722.42 & 6303.21 & 1400.99 & 998.78 & **2074.56**\n' +
      '4097 & 3072.16 & 736.41 & 725.73 & 6301.41 & 1400.30 & 823.89 & **477.26**\n' +
      '4096 & 4096 & 1370.41 & 1731.13 & 6300.65 & 1399.51 & 663.34 & **206.22** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: \\(n_{p}\\) 컨텍스트 토큰 및 \\(n_{s}\\) 프리픽스 토큰이 주어진 자기-어텐션 커널의 레이턴시(ms)가 공유된다. 청크 크기 \\(c\\)=64, 배치 크기 \\(b\\)=32.\n' +
      '\n' +
      '(Kwon et al., 2023). 그것은 디코딩 시 동적 및 알려지지 않은 시퀀스 길이에 의해 야기되는 메모리 낭비의 문제를 해결하기 위해 OS에서 페이징 기법을 도입한다. 그러나, 공유 프롬프트를 미리 구성하라는 서비스 제공자에 대한 제안만이 언급되며, vLLM 릴리즈(최대 0.2.7)에서는 구현되지 않는다. 페이징과 다른 우리의 솔루션은 프리픽스 트리를 사용하여 메모리를 관리하고 런타임에서 사용자 요청에 걸쳐 KV 캐시에서 중복성을 자동으로 발견하는 것을 목표로 한다. 이 솔루션은 서비스 제공자가 모델을 중앙에서 호스트하고 확장성에 대한 요구 사항을 갖는 다중 테넌트 배포 시나리오에 대해 더 실용적이다. vLLM에 따르면, 공유된 KV 캐시는 다수의 프로세스들에 의해 공유되는 동적 링크 라이브러리와 유사하다. vLLM의 전략은 게시(AoT) 전에 컴파일하는 것이다. 우리는 실시간 컴파일(JIT)을 기대한다. 또한, 프리픽스 트리에서 캡처된 컨텍스트를 기반으로 2단계 분할 알고리즘을 제안하여 공유된 시스템 프롬프트가 자기 주의 커널로 가져오는 최적화 기회를 탐색한다.\n' +
      '\n' +
      '청크어텐션에서의 파티션 전략은 온라인 소프트맥스(Milakov and Kimelshein, 2018)에 구축되며, 동일한 알고리즘을 채택한 FlashAttention(Dao et al., 2022; Dao, 2023)에서 영감을 받았다. 플래시 어텐션은 다양한 타일링 기법을 철저히 연구하고 구현하여 메모리 동작을 10-20\\(\\times\\) 줄이면서 2-4\\(\\times\\)의 자기 주의력을 가속화시켰다. 플래시 어텐션-2는 타일링 전략을 변경했고 또한 속도를 두 배로 증가시켰다. 그러나 플래시 주의는 연속적이지 않은 메모리나 가변적인 시퀀스 길이에 대해 유연하지 않기 때문에 추론보다 모델 학습에 더 적합하다. 디코딩 동안 쿼리 토큰 카운트가 항상 하나일 때 이득은 거의 없다. 청크어텐션은 디코딩 동안 가변 시퀀스 길이를 처리하고 메모리 동작을 줄이기 위해 여러 시퀀스의 어텐션 동작을 배치한다. 결과적으로, 우리의 업무와 플래시 어텐션은 상호 보완적이다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 KV 캐시를 효율적으로 관리하고 LLMs 추론을 위한 self-attention 커널을 가속화하기 위한 새로운 self-attention 모듈인 ChunkAttention을 제안한다. 접두사 트리를 성공적으로 채택하여 접두사 인식 KV 캐시를 생성한다. 런타임에 중복 KV 캐시를 탐지하고 제거하는 문제를 해결합니다. 다양한 구성과 다양한 수준에서 청크어텐션을 평가하여 그 실현 가능성과 부작용을 관리할 수 있음을 입증한다. 실험 결과, 청크어텐션 커널은 SOTA PagedAttention 커널과 유사한 처리량을 얻을 수 있으며, 프리픽스 인식 KV 캐시와 2상 분할을 적용하여 A100(80G)에서 1024~4096 토큰의 공유 시스템 프롬프트를 사용하여 3.2~4.8\\(\\times\\)의 성능 향상을 보였다.\n' +
      '\n' +
      '## 7 Limitations\n' +
      '\n' +
      '**시스템 프롬프트의 위치** 키/값 텐서를 메모리에 공유하려면 공유된 시스템 프롬프트가 시퀀스의 시작 부분에 나타나야 합니다. 이는 많은 작업 및 시스템에서 가장 일반적인 관행이지만(Lu et al., 2023; Qian et al., 2023; Saad-Falcon et al., 2023; Zhuang et al., 2023), 이는 필수적이지 않다. Liu et al. (2023)은 관련 정보의 위치를 변경할 때 언어 모델 성능이 크게 저하됨을 밝히고, 이는 모델들이 긴 입력 컨텍스트에서 정보에 접근하고 사용하기 위해 강건하게 어려움을 겪는다는 것을 나타낸다. 특히 모델이 중간에 정보를 사용해야 하는 경우 성능이 가장 낮은 경우가 많습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{\\(n_{p}\\)} & \\multirow{2}{*}{\\(n_{c}\\)} & \\multirow{2}{*}{\\(n_{c}\\)} & \\multirow{2}{*}{BPS} & \\multicolumn{3}{c}{Latency (uncoRank)} & \\multicolumn{3}{c}{Peak KV Cache (GIN)} & \\multicolumn{3}{c}{Peak Base Size} \\\\ \\cline{4-9}  & & & & \\(\\pm\\)1.LM & ChunkAttention & vLLM & ChunkAttention & vLLM & ChunkAttention \\\\ \\hline\n' +
      '104 & 0 & 52 & 1.0 & 193.25 & 19.11 & 14.73 & 11.50 & 23 & 18\\\\\n' +
      '104 & 1024 & 52 & 1.0 & 200.0 & **14.07** & **14.79** & **3.28** & 12 & 14 \\\\ \\hline\n' +
      '2048 & 0 & 52 & 0.6 & 21.0 & 19.30 & 21.70 & 22.41 & 19 & 20\\\\\n' +
      '2048 & 2948 & 53 & 0.2 & 21.1 & **15.20** & 19.00 & **3.40** & 19 & 12 \\\\ \\hline\n' +
      '4069 & 0 & 52 & 0.2 & 28.23 & 28.20 & 34.59 & 35.13 & 16 & 16\\\\\n' +
      '4069 & 4096 & 52 & 0.4 & 27.62 & **17.46** & 35.42 & **4.00** & 16 & 11 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 정규화된 레이턴시, 피크 KV 캐시 메모리, 및 디코딩 동안 도달한 배치 크기.\n' +
      '\n' +
      '도 5: 상이한 요청 도착 레이트들(RPS)이 주어지는 정규화된 레이턴시. 각 라인은 시스템으로 표시되고 공유 프롬프트 토큰 카운트: 시스템(\\(n_{s}\\))으로 표시된다.\n' +
      '\n' +
      '긴 입력 컨텍스트. 결과적으로, 애플리케이션 개발자가 평가 또는 의도하지 않은 실수 후에 성능 문제를 위해 시스템 프롬프트를 초기에 넣지 않을 때, 전체 시퀀스의 KV 캐시는 상이하고, PAKV는 공통되는 토큰의 수가 많음에도 불구하고 이 경우 메모리를 저장할 수 없다.\n' +
      '\n' +
      '**Fine Tuning.** 시스템 프롬프트를 사용하는 것 외에도, Fine-tuning은 LLMs(Houlsby et al., 2019; Hu et al., 2023)에 도메인 지식을 주입하는 또 다른 유망한 방법이다. 높은 훈련 및 배포 비용으로 인해 LLM은 일반적으로 여러 애플리케이션을 공유하기 위해 사전 훈련되고 중앙에서 호스팅된다. 각 응용 프로그램이 모델을 미세 조정하고 개인 인스턴스를 배포하는 데 비용 효율적이지 않습니다. 그러나, 미세 조정은 하드웨어 및 소프트웨어 환경이 진화함에 따라 더 실용적이고 대중화될 수 있다. 이 경우, 우리는 더 이상 각 애플리케이션에 대해 긴 시스템 프롬프트를 설계할 필요가 없고, 시스템 프롬프트의 공유 기회가 감소된다. 오늘날 우리는 시스템 프롬프트를 사용하는 것보다 이 방향으로 유망하고 비용 효율적인 미세 조정 및 호스팅 솔루션을 보지 못했다.\n' +
      '\n' +
      '**모델 및 하드웨어 호환성** 최상의 성능을 달성하기 위해 청크어텐션은 cuDNN(oneDNN Contributors, 2023) 또는 PyTorch에서 높은 수준의 프리미티브를 활용하는 대신 낮은 수준의 CUDA 프로그래밍으로 2단계 파티션 커널을 구현한다. 가장 일반적인 LLM 구성인 _e.g._, 128 헤드 치수 크기 및 하드웨어, _e.g._, NVIDIA A100, GeForce RTX 4090 및 Intel Xeon CPU에 대해 성능을 조정한다. 다른 구성 및 하드웨어의 경우 성능 사례를 사례별로 튜닝하고 검증해야 하므로 상당한 개발 비용이 추가된다. 우리는 2단계 분할 알고리즘을 일반화하고 더 많은 모델 구성 및 하드웨어와 호환되도록 하기 위한 커뮤니티 노력이 필요하다고 믿는다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Aminabadi et al. (2022) Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, and Yuxiong He. 2022. 심층 속도 추론: 전례 없는 규모로 변압기 모델의 효율적인 추론을 가능하게 한다. _SC22: International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-15.\n' +
      '* Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. _ arXiv e-prints_, pages arXiv-2305.\n' +
      '*인체(2023)인체. 2023. 시스템 프롬프트 사용 방법. [https://docs.anthropic.com/claude/docs/how-to-use-system-prompts] (https://docs.anthropic.com/claude/docs/how-to-use-system-prompts).\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 소수의 학습자를 의미한다. _ 신경 정보 처리 시스템들_, 33:1877-1901의 진보들.\n' +
      '*Chang et al. (2023) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. 유창신, 양창신 2023. 대형 언어 모델의 평가에 관한 조사.\n' +
      '* Choquette et al. (2021) Jack Choquette, Wishwesh Gandhi, Olivier Giroux, Nick Stam, and Ronny Khashinsky. 2021. 엔비디아 a100 텐서 코어 gpu: 성능 및 혁신_ IEEE Micro_, 41(2):29-35.\n' +
      '* Chu et al. (2023) Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. 2023. 사고 추론의 사슬에 대한 조사: 진보, 개척, 미래.\n' +
      '* 컴퓨터(2023) Together Computer. 2023. Redpajama-data: 라마 트레이닝 데이터세트를 재현하기 위한 오픈 소스 레시피.\n' +
      '* Dao(2023) Tri Dao. 2023. 플래시 어텐션-2: 더 나은 병렬성과 작업 분할로 더 빠른 주의력 _ arXiv preprint arXiv:2307.08691_.\n' +
      '* Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 2022. 플래시 어텐션: io-awareness와 함께 빠르고 메모리 효율적인 정확한 주의력 _ 신경 정보 처리 시스템_, 35:16344-16359에서의 발전.\n' +
      '* Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui. 2023. In-context learning에 관한 설문조사.\n' +
      '* Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui. 2022. in-context learning을 위한 설문조사. _ arXiv preprint arXiv:2301.00234_.\n' +
      '* Gao et al. (2018) Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. 2018. Low latency rnn 추론 with cellular batching. 13번째 EuroSys Conference_의 Proceedings에서, 페이지 1-15.\n' +
      '* 쌍둥이자리(2023) 쌍둥이자리. 2023. 쌍둥이자리: 매우 유능한 멀티모달 모델들의 패밀리.\n' +
      '* Geng and Liu(2023) 신양 Geng and Hao Liu. 2023. Openllama: llama의 오픈 재생산.\n' +
      '* Hill (1992) Steve Hill. 1992년, 간단한 고속 메모리 할당기 DAVID KIRK, editor, _Graphics Gems III (IBM Version)_ pages 49-50. Morgan Kaufmann, San Francisco.\n' +
      '* Geng et al. (2018)Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for nlp. _International Conference on Machine Learning_, pages 2790-2799. PMLR.\n' +
      '* Hu et al. (2023) Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, E-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. 2023. Llm-adapters: 큰 언어 모델의 파라미터 효율적인 미세 조정을 위한 어댑터 패밀리.\n' +
      '* 허깅페이스(2023) 허깅페이스. 2023. huggingface/text-generation-inference: Large language model text generation inference. [https://github.com/huggingface/text-generation-inference] (https://github.com/huggingface/text-generation-inference).\n' +
      '* Jin et al. (2023) Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. 2023. S3: 더 높은 처리량을 위한 생성 추론 동안 gpu 활용도 증가 _ arXiv preprint arXiv:2306.06000_.\n' +
      '* Kim et al. (2023) Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. 마호니, 야쿤 소피아 샤오, 아미르 고르아미. 2023. 변압기 추론의 전체 스택 최적화: 설문조사.\n' +
      '* Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. 페이지 어텐션으로 서빙하는 대형 언어 모델에 대한 효율적인 메모리 관리.\n' +
      '* Lefaudeux et al. (2022) Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Cagiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. 2022. xformers: 모듈식 및 해킹 가능한 변압기 모델링 라이브러리.\n' +
      '* Li 등(2023) Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hanyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-bank: 툴-증강 llms에 대한 포괄적인 벤치마크.\n' +
      '* Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in middle: How language models use long context.\n' +
      '* Lu et al. (2023) Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, 및 Jianfeng Gao. 2023. 카멜레온: 대형 언어 모델을 사용한 플러그 앤 플레이 구성 추론.\n' +
      '* Milakov and Kimelshein (2018) Maxim Milakov and Natalia Kimelshein. 2018. 온라인 normalizer calculation for softmax. _ arXiv preprint arXiv:1805.02867_.\n' +
      '* Contributors (2023) oneDNN Contributors. 2023. oneapi deep neural network library (onednn). [https://github.com/oneapi-src/oneDNN] (https://github.com/oneapi-src/oneDNN).\n' +
      '* OpenAI(2023a) OpenAI. 2023a. Chatgpt plugins. [https://platform.openai.com/docs/plugins/introduction] (https://platform.openai.com/docs/plugins/introduction).\n' +
      '* openai api. [https://platform.openai.com/docs/guides/function-calling] (https://platform.openai.com/docs/guides/function-calling)\n' +
      '* OpenAI(2023c) OpenAI. 2023c. Gpt-4 기술 보고서입니다 arXiv preprint arXiv:2303.08774_.\n' +
      '* OpenAI(2023d) OpenAI. 2023d. 채팅 모델로 기능을 호출하는 방법. [https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models] (https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models).\n' +
      '* OpenAI(2023e) OpenAI. 2023e. openai/tiktoken: tiktoken은 openai의 모델과 함께 사용하기 위한 빠른 bpe tokeniser이다. [https://github.com/openai/tiktoken] (https://github.com/openai/tiktoken).\n' +
      '* Qian et al. (2023) Cheng Qian, Chi Han, Yi R. 풍, 유지아 진, 지위안 류, 헝지. 2023. 크리에이터: 대규모 언어 모델의 추상적 추론과 구체적 추론을 해체하기 위한 도구 생성.\n' +
      '* Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Generative Pre-training에 의한 언어 이해력 향상.\n' +
      '* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비감독 멀티태스크 학습자들이다. _ OpenAI blog_, 1(8):9.\n' +
      '* Saad-Falcon et al. (2023) Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, David SeungHyun Yoon, Ryan A. Rossi, and Franck Dernoncourt. 2023. Pdftriage: 길고 구조화된 문서에 대한 질의 응답.\n' +
      '* Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. 툴포머: 언어 모델들은 스스로 툴들을 사용하는 것을 가르칠 수 있다.\n' +
      '* Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. 2023. 단일 gpu를 갖는 대형 언어 모델의 고처리량 생성 추론. _ arXiv preprint arXiv:2303.06865_.\n' +
      '* Slifa et al. (2022) Franyell Slifa, Jose Maria Arnau, and Antonio Gonzalez. 2022. E-batch: Energy-efficient and highthroughput rnn batching. _ ACM 트랜스, 아치 Code Optimized._ , 19(1).\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. 개방적이고 효율적인 기초 언어 모델 arXiv preprint arXiv:2302.13971_.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023b. 라마: 개방적이고 효율적인 기초 언어 모델들.\n' +
      '\n' +
      '마리아노 트레비노 2016. mtrebi/memory-allocators: 동적 메모리 할당의 성능을 향상시키기 위해 c++ 내의 맞춤형 메모리 할당기. [https://github.com/mtrebi/memory-allocators#pool-allocator] (https://github.com/mtrebi/memory-allocators#pool-allocator).\n' +
      '\n' +
      'Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou et al. 2022. Chain-of-thought prompting reasoning in large language models. _ 신경 정보 처리 시스템_, 35:24824-24837의 발전.\n' +
      '\n' +
      '줄스 화이트, 쿠첸 푸, 샘 헤이스, 마이클 샌드본, 카를로스 올레아, 헨리 길버트, 아슈라프 엘나샤르, 제시 스펜서 스미스, 더글러스 C. 슈미트. 2023. 채팅으로 신속한 엔지니어링을 향상시키기 위한 프롬프트 패턴 카탈로그.\n' +
      '\n' +
      '새뮤얼 윌리엄스 앤드류 워터맨 데이비드 패터슨 2009. Roofline: An insightful visual performance model for multicore architecture. _ 커뮤니티 ACM_, 52(4):65-76.\n' +
      '\n' +
      '유경인, 주성정, 김건우, 김수정, 천병곤 등이 있다. 2022. Orca: {Transformer-Based} 생성 모델들을 위한 분산 서빙 시스템. _16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)_, pages 521-538.\n' +
      '\n' +
      '용차오 저우, 안드레이 이오안 무레사누, 지웬 한, 케이란 패스터, 실비우 피티스, 해리스 찬, 지미 바. 2023. 대규모 언어 모델은 인간 수준의 신속한 엔지니어입니다.\n' +
      '\n' +
      '유첸 장, 유에 유, 관왕, 하오톈 선, 차오 장. 2023. Toolqa: 외부 툴로 llm 질의응답을 위한 데이터셋.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '- arrival_airport: [선택사항] 고객이 가고 있는 곳으로 가는 세 글자 공항 코드.\n' +
      '- number_of_adult_travelers: [optional] Number of Adult Travelers(Default: 1).\n' +
      '- child_traveler_age: [optional] "childTravelerAge"는 단일 아동 여행자의 나이를 나타낸다. 모든 어린이 여행자의 나이를 지정해야 합니다. 즉, 비행할 각 어린이에 대해 이 매개변수를 지정해야 합니다. 유효한 값은 0-17(1세 미만 유아의 경우 0)입니다. 1, 3, 8세의 어린이 여행자 3명을 지정하려면 쿼리 문자열에 "childTravelerAge=1&childTravelerAge=3&childTravelerAge=8"이 포함되어야 한다.\n' +
      '- non_stop_flight: [optional] search response에서 nonstop flights만을 반환하기 위해 true로 설정한다(Default: False).\n' +
      '- 항공사_선호도: [선택사항] 특정 항공사 캐리어 정보를 얻기 위한 옵션 파라미터. 기본적으로 환경설정이 전부입니다.\n' +
      '* [noitemsep]\n' +
      '*이름: [선택사항] 검색할 레스토랑 이름.\n' +
      '- 카테고리: [선택사항] 검색할 레스토랑의 카테고리.\n' +
      '- 시: [선택사항] 검색할 도시.\n' +
      '- 요일: [선택사항] 검색할 날짜.\n' +
      '* q: [필수] 검색 쿼리. 키워드 및 선택적 필드 필터 및 연산자입니다.\n' +
      '- 유형: [필수] 검색할 항목 유형의 쉼표로 구분된 목록입니다. 유효한 유형은 앨범, 아티스트, 재생 목록, 트랙, 쇼 및 에피소드입니다. 검색 결과에는 지정된 모든 항목 유형의 히트가 포함됩니다. 예를 들어, "q=name:abacab&type=album,track"은 자신의 이름에 "abacab"이 포함된 앨범과 트랙을 모두 반환한다.\n' +
      '- 한계: [선택사항] 반환할 최대 결과 수. 기본값: 20 최소값: 1 최대값: 50. 참고: 총 응답이 아닌 각 유형 내에서 제한이 적용됩니다. 예를 들어, 한계값이 3이고, 유형이 "아티스트,앨범"인 경우, 응답은 3명의 아티스트와 3개의 앨범을 포함한다.\n' +
      '- 오프셋: [선택사항] 반환할 첫 번째 결과의 인덱스. 기본값: 0(첫 번째 결과)입니다. 최대 간격띄우기(한계 포함) : 1,000. 검색 결과의 다음 페이지를 얻기 위해 한도와 함께 사용합니다. 아래에는 사용자 질의와 일치하는 API를 선택한 몇 가지 예가 있습니다. datetime_now=2023-11-17T10:45:07+08:00 user_query=신을 믿습니까? api_call=not_found() datetime_now=2023-11-17T10:50:00+08:00 user_query= iPhone 15 Pro Max의 가격은 얼마인가? api_call=bing_web_search(q=" iPhone 15 Pro Max", set_lang="en") datetime_now=2023-11-17T11:09:10+08:00 user_query=OpenAI의 로고 api_call=bing_images_search(q="OpenAI 로고", set_lang="zh") datetime_now=2023-11-17T13:21:30+08:00 user_query= 테일러 스위프트의 최신 앨범은 무엇인가? api_call=spotify_search_catalog(q="Taylor Swift", type="album", limit=1) datetime_now=2023-11-17T11:21:42+08:00 user_query=샌프란시스코에서 이번 주말 비건 음식을 먹으려고 하는데, 토요일 좋은 레스토랑 제안 하나 주실 수 있나요? api_call=\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>