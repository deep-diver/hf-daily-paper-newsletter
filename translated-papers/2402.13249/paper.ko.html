<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# TofueVal: LLMs의 환각 평가\n' +
      '\n' +
      '주제 중심의 대화 요약\n' +
      '\n' +
      'Liyan Tang\\({}^{\\diamond\\dagger}\\), Igor Shalyminov\\({}^{\\spadesuit}\\), Amy Wing-mei Wong\\({}^{\\spadesuit}\\), Jon Burnsky\\({}^{\\spadesuit}\\), Jake W. Vincent\\({}^{\\spadesuit}\\)\n' +
      '\n' +
      '**Yu\'an Yang\\({}^{\\spadesuit}\\), Siffi Singh\\({}^{\\spadesuit}\\), Song Feng\\({}^{\\spadesuit}\\), 환준 Song\\({}^{\\spadesuit\\dagger}\\), Hang Su\\({}^{\\spadesuit}\\), Lijia Sun\\({}^{\\spadesuit}\\),**\n' +
      '\n' +
      '**Yi Zhang\\({}^{\\spadesuit}\\), Saab Mansour\\({}^{\\spadesuit}\\), Kathleen McKeown\\({}^{\\spadesuit}\\)**\n' +
      '\n' +
      '한국과학기술원(KAIST) AWS AI Labs\\({}^{\\spadesuit}\\)\n' +
      '\n' +
      '텍사스 오스틴대학\n' +
      '\n' +
      'shalymin@amazon.com\n' +
      '\n' +
      '아마존에서 인턴으로 일했다. 아마존에서 일하는 동안 일했다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '단일 문서 뉴스 요약은 사실적 일관성 또는 환각에 대한 평가에 대한 연구에 의해 최근 몇 년 동안 충실성에 상당한 진전이 있었다. 우리는 이러한 발전이 다른 텍스트 요약 영역으로 이어지는지 여부를 묻는다. 우리는 다양한 크기의 LLM에 의해 생성된 주제 중심 대화 요약에 대한 새로운 평가 벤치마크를 제안한다. 우리는 사실적으로 일관성이 없는 문장에 대한 상세한 설명과 함께 이러한 요약의 사실적 일관성에 대한 이진 문장 수준의 인간 주석을 제공한다. 우리의 분석은 기존의 LLMs가 모델의 크기에 관계없이 대화 도메인에서 상당한 양의 사실적 오류를 환각한다는 것을 보여준다. 반면에 GPT-4를 포함한 LLM이 이진 사실 평가자 역할을 할 때 성능이 좋지 않으며 최첨단 전문 사실 평가 메트릭을 통해 능가할 수 있다. 마지막으로, 선별된 오류 분류법으로 환각 유형에 대한 분석을 수행했다. 모델 생성 요약에 다양한 오류 및 오류 분포가 있으며 비LLM 기반 메트릭이 LLM 기반 평가자보다 모든 오류 유형을 더 잘 포착할 수 있음을 발견했다.1\n' +
      '\n' +
      '각주 1: github.com/amazon-science/tofueval에서 전문가 주석이 있는 벤치마크 데이터 세트를 공개한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근, 자동화된 텍스트 요약 분야는 모델 출력 Fu 등(2023); Gao 등(2023); Madaan 등(2023)을 평가하기 위해 LLM(Large Language Models)을 사용하는 경향이 증가하고 있다. 이러한 경향의 결과적 특성을 감안할 때, 우리는 "**는 모델 산출물을 평가하는 작업까지의 LLMs인가?"라고 질문한다.** 최근 뉴스 요약에 대한 작업은 생성된 뉴스 요약의 사실적 일관성을 평가하는 LLMs의 성능이 Luo et al.(2023); Wang et al.(2023)은 덜 탐색된 다른 요약 도메인에서도 잘 수행되지 않을 수 있다.\n' +
      '\n' +
      '기존의 연구들은 주로 Tang et al. (2023); Laban et al. (2022); Pagnoni et al. (2021); Fabbri et al. (2021)과 같은 뉴스 요약 벤치마크를 조사한다. LLM이 인간의 선호도와 일치하는 뉴스 기사의 요약을 생성할 수 있다는 발견(2022); Zhang et al.(2023)과 함께, 우리는 질문한다: **LLM이 비뉴스 도메인에 대한 환각 없이 사실적으로 일관된 요약을 생성할 수 있는가?** 회의에서의 생산성 향상 또는 고객 서비스 상호작용의 효율화와 같은 대화 요약이 다른 영역에 가져올 수 있는 잠재적인 이점을 고려할 때, 본 연구에서는 대화 요약에 초점을 맞춘 사례 연구이다.\n' +
      '\n' +
      '우리는 **토픽-포커스**의 대화 요약 **사실적 일관성의 평가**를 대상으로 하는 새로운 벤치마크 데이터세트 **TofueVal**을 도입하여 위에서 언급한 두 가지 질문을 해결한다. 상기 벤치마크 데이터세트는 생성된 요약들을 포함하는\n' +
      '\n' +
      '그림 1: TofuEval은 두 개의 대화 요약 데이터 세트에서 1.5K 주제 중심 요약을 포함한다. 우리는 전문 언어 주석자들에게 사실적으로 불일치하는 문장에 대한 설명 및 오류 유형과 함께 각 요약의 완전성, 관련성 및 사실적 일관성을 평가할 것을 요청한다.\n' +
      '\n' +
      '다양한 크기의 5개의 LLM에 의해. 벤치마크에서의 요약들은 대화들의 길이 및 토픽들이 상이한 사용자들에게 다양한 정도의 중요도를 보유할 수 있다는 사실로 인해 대화들 내의 특정 토픽들에 집중된다.\n' +
      '\n' +
      'TofuEval에서는 전문 언어 데이터 주석자를 참여시켜 각 요약 내에서 문장의 이진 사실성 평가를 수행하고 환각 콘텐츠가 포함되어 있다고 판단되는 문장에 대한 설명을 작성한다(3.4절). 인간 주석은 LLM이 상당한 수의 사실적 오류를 범하기 쉽다는 것을 보여주며, 일반적인 믿음과 달리 더 큰 LLM은 더 작은 모델(섹션 4)보다 더 사실적으로 일관된 대화 요약을 반드시 생성하는 것은 아니다.\n' +
      '\n' +
      '또한, 우리가 연구한 모든 LLM(GPT-4 포함)은 이진 사실 일관성 평가자로 사용될 때 인간의 판단에 따라 문서의 주요 주제에 초점을 맞춘 LLM 생성 요약에서 오류를 감지하는 데 실패한다(섹션 5). 대조적으로, 비LLM 기반 사실성 메트릭은 우리가 테스트한 대부분의 LLM에 비해 우수한 성능을 보여주며, 더 작은 모델 크기와 더 낮은 추론 비용의 추가 이점을 가지고 있다. 우리의 오류 분석은 이러한 비LLM 메트릭이 LLM과 비교할 때 모든 오류 유형을 더 잘 포착한다는 것을 추가로 보여준다.\n' +
      '\n' +
      '본 연구의 기여는 다음과 같다. (1) 주제 중심 대화 요약 벤치마크 TofuEval을 소개한 것은 첫째, LLM 생성 요약에 대한 설명문이다. (2) LLM은 관계성, 완전성, 사실적 일관성에 걸쳐 요약자로서 체계적으로 평가하며, LLM은 대화 도메인에서 사실적 일관성에 대해 제대로 수행되지 않는다는 것을 보여준다. (3) 사실성 예측에 대한 평가 벤치마크는 GPT-4를 제외하고, 우리가 연구한 모든 LLM 기반 평가자 성능이 비LLM 사실성 메트릭보다 열등하다는 것을 보여준다. (4) 우리는 큐레이션된 오류 분류법을 사용하여 오류 분석을 수행하여 비LLM 사실성 메트릭이 LLM 기반 평가자보다 모든 오류 유형을 더 잘 포착할 수 있음을 보여준다. (5) 요약 사실성의 개선된 자동 평가에 대한 추가 연구를 가능하게 하기 위해 인간 주석 데이터를 사용하여 TofuEval을 공개한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**Factual Consistency Evaluation Benchmarks** 텍스트 요약에서, 생성된 요약서 Fabbri et al. (2021); Cao and Wang (2021); Maynez et al. (2020).2\n' +
      '\n' +
      '각주 2: 본 연구에서는 _factual inconsistency_, _factual error_ 및 _hallucinations_라는 용어를 혼용하여 사용한다.\n' +
      '\n' +
      '제안된 벤치마크 TofuEval은 이러한 노력과 일치하지만 다음과 같이 이전 작업과 다르다(표 1에서 요약됨): (1) 비LLM 생성 요약을 포함하는 기존 평가 벤치마크와 달리 TofuEval은 LLM 생성 요약에 중점을 둔다. 정확한 LLM 출력을 편집하여 사실적으로 일관되지 않은 요약을 생성하는 SummeDits Laban et al.(2023)과 비교하여 LLM 생성 요약의 사실적 오류를 직접 식별한다. (2) TofuEval은 대화 요약에 중점을 둔다. DialSummeEval Gao와 Wan(2022)이 이러한 초점을 공유함에도 불구하고, SAMSum corpus Gliwa et al.(2019)의 짧은 대화를 기반으로 하는 DialSummeEval의 소스 문서보다 TofuEval의 소스 문서가 상당히 길다. (3) 선행 작품에서의 인간 평가는 SummeEval Fabbri et al. (2021)과 FrankPagnoni et al. (2021)의 군중 노동자, DialSummeEval Gao and Wan (2022)의 훈련된 대학생 등 다양한 출처에서 나온다. TofuEval은 전문 언어 데이터 주석자의 주석으로 구성된다.\n' +
      '\n' +
      '**환각 검출** ROUGE Lin (2004), BLEU Papineni et al. (2002), 및 BERTScore Zhang* et al. (2020)과 같은 텍스트 요약에 대한 공통 자동 메트릭은 사실적 일관성 Kryscinski et al. (2019); Falke et al. (2019); Gao and Wan (2022); Tang et al. (2023). 따라서, Kryscinski et al. (2020); Goyal and Durrett (2021); Laban et al. (2022); Fabbri et al. (2022); Zha et al. (2023). 최근, LLM은 특정 평가 설정 하에서 사실적 일관성 평가에서 우수한 제로샷 성능을 갖는 것으로 나타났으며, 이는 최신 사실적 일관성 평가자 Luo et al.(2023); Wang et al.(2023)로서의 가능성을 강조한다.\n' +
      '\n' +
      '최근 모델의 환각이 Tang 등을 탐지하기 어렵기 때문에(2023), 우리는 대화 요약 벤치마크 TofuEval의 콘텍스트 내에서 LLM 생성 요약을 사용하여 비LLM 기반 및 LLM 기반 사실성 메트릭을 재평가한다. 우리는 비LLM 기반 메트릭이 대부분의 LLM 기반 평가자를 능가할 수 있음을 발견했다. 그럼에도 불구하고, 모든 자동화된 사실성 메트릭은 여전히 상당히 제대로 수행되지 않아 문제의 도전적인 특성과 자동화된 사실 불일치 탐지의 개선의 상당한 여지를 강조한다.\n' +
      '\n' +
      '## 3 Tofuval Benchmark\n' +
      '\n' +
      '주제 중심 대화 요약 벤치마크인 TofuEval은 다음과 같이 구성된다: (1) 공개적으로 이용 가능한 두 개의 대화 요약 데이터 세트로부터 샘플 문서(섹션 3.1); (2) 샘플링된 문서에 대해 다양한 주제를 생성(섹션 3.2); 및 (3) 다양한 LLM으로 주제 중심 요약을 생성한다(섹션 3.3). 결과 벤치마크는 100개의 대화와 15개의 LLM 생성 요약을 포함한다; (4) 마지막으로 주제에 대한 세밀한 인간 주석과 생성된 요약을 사실적 일관성, 관련성 및 완전성을 포함한 차원에 대해 제공한다(3.4절). 데이터세트 구축 파이프라인이 그림 1에 나와 있다.\n' +
      '\n' +
      '### Document Selection\n' +
      '\n' +
      '우리는 공개적으로 이용 가능한 두 개의 대화 요약 데이터 세트에서 문서를 선택한다.\n' +
      '\n' +
      'MediaSumZhu et al. (2021)은 NPR 및 CNN으로부터의 공개 인터뷰 녹취록을 갖는 대규모 대화 요약 데이터세트이다. 데이터 세트는 정치, 경제 및 미국 뉴스와 같은 다양한 주제에 걸친 다자간 대화를 특징으로 한다.\n' +
      '\n' +
      'MeetingBankHu et al.(2023)은 시의회 회의를 갖는 요약 데이터세트이다. 이러한 회의에는 예산 할당, 인프라 계획 및 범죄 예방을 포함하여 지역 거버넌스 및 지역 복지 중심의 다양한 주제에 대한 논의 및 결정이 포함된다.\n' +
      '\n' +
      '샘플링 과정에서 800~1,200단어 길이의 문서를 선택합니다. 이 결정은 (1) 선택된 문서 길이가 평가되는 모든 모델의 최대 입력 크기에 적합하고 (2) 문서가 LLM 생성 요약에서 잠재적으로 사실적 불일치 오류를 유발할 수 있을 만큼 충분히 길도록 하기 위해 이루어졌다. 더 긴 문서를 선택할 경우 수동 평가에 문제가 발생할 수 있습니다. 벤치마크 통계는 표 5에 나와 있으며, 벤치마크 구축을 위해 각 데이터 세트의 원래 테스트 분할에서 50개의 문서를 무작위로 샘플링한다.\n' +
      '\n' +
      '### Topic Generation\n' +
      '\n' +
      'LLM의 인상적인 성능은 독자들에게 다양한 중요도를 가진 대화에서 서로 다른 관심 지점을 기반으로 한 단일 긴 대화의 다양한 요약을 생성할 수 있게 한다. 일반적인 요약(_i.e._, 몇 개의 문장으로 문서를 요약)을 요청하는 대신 이미 많이 평가된 성능(표 1)을 샘플링된 문서 내에서 특정 주제에 대해 사실적으로 일관된 요약을 생성하는 LLM의 성능을 평가한다. 여기서는 문서의 독자들이 할리데이 등에 대해 알고 싶어 하는 엔티티, 이벤트 또는 이슈와 관련된 문서에서 _topic_를 논의의 대상으로 넓게 정의한다(2014).\n' +
      '\n' +
      '미디어섬과 미팅뱅크는 사람이 작성한 주제가 제공되지 않으며 수동으로 주제를 식별하는 것은 시간이 많이 걸리기 때문에 부록 C.1의 제로샷 프롬프트를 사용하여 LLM으로 문서화에서 주요 주제를 식별하기로 결정했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c|c} \\hline \\hline  & \\multicolumn{2}{c}{SumEval} & \\multicolumn{1}{c}{Frank} & \\multicolumn{1}{c}{Summac} & \\multicolumn{1}{c}{AgreeFact} & \\multicolumn{1}{c}{DialEval} & \\multicolumn{1}{c}{Summedits} & \\multicolumn{1}{c}{**TofuEval**} \\\\ \\cline{2-7} Summaries from LLMs & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) \\\\ Non-Edited Summaries & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) \\\\ Fine-Grained Annotations & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) \\\\ Natural Language Explanations & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) & \\(\\mathbf{\\times}\\) \\\\ Document Domain & news & news & news & news & dialogue & mixed & dialogue \\\\ Summary Type & generic & generic & generic & generic & generic & generic & topic-focused \\\\ Annotators & crowd & mixed & mixed & students & trained ann. & linguists \\\\ Average Document Length & 408 & 595 & 583 & 496 & 130 & 705 & 950 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **TofuEval과 텍스트 요약에 대한 기존 사실적 일관성 평가 벤치마크 간의 비교.**uur는 문서 내에서 서로 다른 주제에 초점을 맞추고 서면 설명이 있는 요약 문장에 대해 전문가 주석이 달린 사실적 일관성 레이블을 제공하는 첫 번째이다. 우리는 문장 수준 및 더 세분화된 주석을 세분화된 주석으로 간주한다. Summac 및 AggreFact의 일부 데이터 세트에는 이러한 유형의 주석(\\(\\mathbf{\\times}\\))이 부분적으로 포함된다. DialEval은 DialSummEval을 의미한다.\n' +
      '\n' +
      'LLM이 주요 주제를 생성하도록 촉구하지만 인간 평가(섹션 3.4에서 더 자세한 내용)는 LLM 생성 주제의 대부분이 밀접하게 관련되어 있지만 수집된 주제의 작은 비율이 문서의 맥락 내에서 미미하다는 것을 보여준다. 우리는 주변 토픽이 요약 독자들에게도 유용할 수 있다는 가정에 기초하여 이러한 주변 토픽을 유지하기로 결정했다.\n' +
      '\n' +
      '### 요약 모델 선택\n' +
      '\n' +
      '우리는 LLM에 의해 생성된 요약을 기반으로 요약 사실 일관성 평가 벤치마크를 구성한다. 이것은 대화당 다수의 요약을 생성할 수 있게 하고, 따라서 더 적은 인간 노력으로 데이터세트를 쉽게 스케일링할 수 있게 한다.\n' +
      '\n' +
      '우리는 하나의 독점 LLM, OpenAI의 **GPT-3.5-Turbo** 및 4개의 오픈 소스 모델인 **Vicuna-7B**(Chiang et al., 2023) 및 **WizardLM-7B/13B/30B**(Xu et al., 2023)의 요약 성능을 평가하기로 결정했다. 모델 및 모델 선택에 대한 자세한 내용은 부록 B에서 찾을 수 있다. 주제 중심 텍스트 요약은 부록 C.2에서 제로 샷 프롬프트를 사용했다. 달리 명시되지 않는 한, 모델 온도를 0.7로 설정하고 이 작업의 모든 실험 설정에서 모든 모델에 대해 나머지 하이퍼 매개변수의 값을 변경하지 않았다.\n' +
      '\n' +
      '요약하면, TofuEval은 데이터 세트당 50개의 문서, 문서당 3개의 생성된 주제, 주제당 5개의 다양한 LLM에서 요약되어 50\\(\\times\\)2\\(\\times\\)3\\(\\times\\)5 = 1,500개의 요약으로 구성된다(자세한 내용은 표 5 참조). 또한, 인간 주석자가 비요약으로 간주하는 23개의 모델 출력을 제거하여 1,479개의 요약 문장(3,966개 요약 문장)을 생성했다. 그런 다음 벤치마크를 개발 및 테스트 세트로 무작위로 분할하고 별개의 문서에서 70%/30% 개발/테스트 파티션 분할을 수행한다.\n' +
      '\n' +
      '### Annotation Collection\n' +
      '\n' +
      '생성된 요약을 사용하여 아래에 정의된 각 차원에 대해 전문 언어 데이터 주석자로부터 고품질 주석을 수집했다.4 전체 주석 지침 및 품질 관리에 대한 세부 정보는 부록 F에서 찾을 수 있다.\n' +
      '\n' +
      '각주 4: LLMs가 일반적으로 이러한 차원들에서 탁월하기 때문에 유창성 및 일관성을 평가하지 않는다(Goyal et al., 2022; Zhang et al., 2023).\n' +
      '\n' +
      '주제 분류 문서 내의 주제를 수동으로 _main_ 및 _marginal_ 주제로 분류했습니다. 주요 주제는 문서에서 논의되거나 제시되고 있는 중심 정보를 의미한다. 한계 주제는 문서에서 덜 철저하게 탐구되는 주제입니다. 보다 상세한 정의는 부록 F.1에서 찾을 수 있다. 주요 토픽은 분류 결과에 따라 TofuEval의 토픽의 약 75%를 구성한다(표 5).\n' +
      '\n' +
      '사실 일치 요약 문장은 문장이 문서에 의해 진술되거나 암시되는 경우 문서와 사실적으로 일치하며, 그렇지 않으면 일관성이 없다. 일치하지 않는 것으로 간주되는 문장에 대해 주석자는 불일치에 대한 간략한 설명을 썼다. 전체 해당 요약에 대한 레이블을 얻기 위해 문장 수준의 이진 사실성 레이블을 집계한다. 요약 문장이 모두 사실적으로 일관성이 있는 것으로 레이블이 지정되면 요약은 문서와 사실적으로 일관성이 있고 그렇지 않으면 요약이 사실적으로 일관성이 없다.\n' +
      '\n' +
      '관련 요약은 원본 문서의 주제 관련 내용에 초점을 맞춥니다. 각 요약에는 0에서 1까지의 관련성 점수가 할당되었으며 1은 주제 요약을 나타낸다.\n' +
      '\n' +
      '완전성 완전한 요약은 해당 주제와 관련된 문서의 모든 정보를 요약합니다. 각 요약에는 0에서 1 사이의 완전성 점수가 할당되었으며 1은 가장 높은 수준의 완전성을 나타낸다(부록 F.2).\n' +
      '\n' +
      '### 대화 요약 대 뉴스 요약\n' +
      '\n' +
      '뉴스 요약에 비해 대화 요약은 대화의 비형식적이고 구어체적인 특성으로 인해 독특한 문제를 수반하며, 이는 미묘함과 소음을 처리하기 위해 요약 모델이 필요하다. 또한 대화는 본질적으로 상호 작용하며, 이는 종종 다른 화자 간의 질문, 답변 및 의견의 혼합을 포함한다. 이러한 상호 작용은 대화에서 논의된 정보 조각 간의 맥락적 관계의 모델에 의해 정교한 이해가 필요하다. 이러한 복잡성은 대화 요약이 어렵고 사실적 불일치에 취약하게 만든다(섹션 4). 이것은 두부에벌(섹션 5)에서 생성된 요약에서 환각을 식별하는 것을 더욱 어렵게 만든다.\n' +
      '\n' +
      '##4 결과: 요약자로서의 LLM\n' +
      '\n' +
      '주요 주제 및 한계 주제 모두에 대해 표 2에서 생성된 요약의 오류율을 보여준다. **전반적으로, 우리가 연구한LLM은 특히 요약 수준에서 상당한 양의 사실적 오류를 범한다.\n' +
      '\n' +
      '우리는 또한 선별된 오류 분류법을 사용하여 두부에벌에서 다양한 환각 유형의 분포를 조사한다. 우리의 분류법은 SAMSum 대화 데이터세트 Gliwa et al.(2019)을 기반으로 하는 Tang et al.(2022)의 분류법과 매우 유사하다는 점에 유의한다. TofuEval에서 긴 대화의 복잡성으로 인해, 우리는 사실_로서 _reasoning error_ 및 _stating opinion와 같은 새로운 오류 유형들로 Tang et al.(2022)의 분류법을 확장한다. 벤치마크에 대한 선별된 오류 분류법의 요약은 부록 그림 5에 나와 있다. 우리는 오류 분류법을 활용하여 벤치마크에서 모든 이진 사실 불일치 주석을 풍부하게 한다. 분류 큐레이션 프로세스 및 오류 유형 주석에 대한 자세한 내용은 부록 G에서 확인할 수 있다.\n' +
      '\n' +
      'LLM은 특히 외부 정보 오류가 있는 한계 주제에 초점을 맞추도록 프롬프트할 때 더 사실적으로 일관되지 않은 요약을 생성하는 경향이 있다.그림 2에서 볼 수 있듯이 한계 주제에 대한 요약에 대한 모델을 프롬프트할 때 모든 요약자는 훨씬 더 많은 _외부 정보_를 생성한다. 우리는 문서에서 주제가 거의 언급되지 않을 때 모델이 주제에 대한 정보에 입각한 추론을 하기 위해 지식에 의존하려고 시도하여 지원되지 않는 정보를 요약에 가져온다는 것을 발견했다. 한계 주제에 대한 외부 정보를 훨씬 적게 생성하는 GPT-3.5-터보는 예외이다. 지원되지 않는 문장을 생성하는 다른 요약자와 비교하여, GPT-3.5-터보는 종종 오프-토픽 콘텐츠를 포함하거나 때때로 명시적으로 "_토픽은 문서_.5에서 논의되지 않는다"라고 말함으로써 한계-토픽 요약에 대한 요청을 처리한다는 것을 발견한다.\n' +
      '\n' +
      '각주 5: 특정 오류 유형에 대한 오류 비율을 줄이기 위한 프롬프트의 추가 최적화는 현재 작업의 범위를 벗어난다.\n' +
      '\n' +
      '더 많은 연구 결과는 부록 D에서 찾을 수 있다.\n' +
      '\n' +
      '##5 결과: 평가자로서의 LLM\n' +
      '\n' +
      '우리는 이제 LLM의 사용을 요약자가 아닌 사실적 일관성의 _평가자_로 간주한다. 우리는 먼저 요약 문장과 요약 문장 모두에 대해 이진 사실 일관성 예측을 할 때 그들의 성능에 대한 평가를 제시한다(섹션 5.1). 그런 다음 오류 유형 분석을 제공하여 오류 유형 모델이 잘 감지하지 못하는 경우(섹션 5.2)를 조사한다. 마지막으로, 주어진\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c|c c c c} \\hline \\hline \\multirow{2}{*}{**Summ.**} & \\multicolumn{4}{c}{**Sentence-Level (\\% Error)**} & \\multicolumn{4}{c}{**Summary-Level (\\% Error)**} \\\\ \\cline{2-9}  & \\multicolumn{2}{c}{**MediaSum**} & \\multicolumn{2}{c}{**Meetingbank**} & \\multicolumn{2}{c}{**MediaSum**} & \\multicolumn{2}{c}{**Meetingbank**} \\\\ \\cline{2-9}  & **Main** & **Marginal** & **Main** & **Marginal** & **Main** & **Marginal** & **Main** & **Marginal** \\\\ \\hline Vicuna-7B & 19.6 & 35.8 & 17.6 & 36.8 & 42.7 & 55.4 & 33.0 & 58.0 \\\\ WizardLM-7B & 29.1 & 36.4 & 21.3 & 42.4 & 49.6 & 54.8 & 35.6 & 49.0 \\\\ WizardLM-13B & 17.4 & 27.2 & 15.8 & 25.4 & 35.9 & 44.4 & 41.3 & 46.8 \\\\ WizardLM-30B & 14.6 & 27.2 & 13.7 & 26.2 & 35.9 & 48.2 & 31.5 & 44.8 \\\\ GPT-3.5-Turbo & **8.8** & **13.6** & **4.4** & **9.4** & **22.2** & 27.2 & 10.9 & 19.8 \\\\ \\hline Average & 17.5 & 27.8 & 14.4 & 27.8 & 37.2 & 46.0 & 30.4 & 43.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **TofuEval.**에서 사용된 5가지 모델에 걸친 문장/요약 수준 사실적 불일치의 백분율.** 주요 주제 요약에 대한 오류율을 한계 주제 요약에 대한 오류율과 별도로 보여준다. 우리는 가장 낮은 오류율과 두 번째로 낮은 오류율을 강조한다. 표 12 및 13의 주석이 달린 요약 예제를 참조하십시오.\n' +
      '\n' +
      '도 2: 오류 분포 _over factually inconsistent summary sentence_ for TofuEval(왼쪽) 및 각 요약자에 대해 주/한계 토픽(오른쪽)을 초과한다. 부록 그림 4의 주요/한계 주제에 대한 각 요약자에 대한 오류 분포 _모든 요약 문장_를 참조하십시오.\n' +
      '\n' +
      'LLM이 모델 출력에 대한 비판을 생성하고 그들의 사실적 일관성 판단에 대한 설명을 제공할 수 있는 능력이 있다는 것(Madaan et al., 2023; Saunders et al., 2022), 우리는 모델 생성 설명을 인간 작성 설명(부록 H)과 비교함으로써 모델 생성 설명의 정확성을 고려한다.\n' +
      '\n' +
      '종합적인 비교를 위한 평가자 선택: SummaC-ZS, SummaC-CV(Laban et al., 2022), QAFactEval(Fabbri et al., 2022) 및 AlignScore(Zha et al., 2023)의 비LLM 기반 SOTA 사실성 메트릭을 포함한다. 우리는 또한 (1) GPT-4 (OpenAI, 2023); (2) GPT-3.5-Turbo; (3) Vicuna-13B/33B (Chiang et al., 2023); (4) WizardLM-13B/30B (Xu et al., 2023). 앞서 언급한 목록의 모든 LLM에 대해 이 목록의 특정 LLM은 입력 길이 제약으로 인해 소수의 샷 평가를 수용할 수 없기 때문에 제로 샷 구성을 사용했다. 어떤 경우에도, 소수의-샷 예들이 제로-샷 시나리오들에 비해 일관되게 우수한 결과들을 산출하지 않는다는 것이 관찰되었다(Laban et al., 2023; Luo et al., 2023). 모델 선택에 대한 자세한 내용은 부록 B에 나와 있습니다.\n' +
      '\n' +
      '사실적 일관성의 예측\n' +
      '\n' +
      '우리는 먼저 이진 예측 작업을 통해 선택된 사실 일관성 평가자 모델의 성능을 측정한다. 모든 평가 모델 \\(M\\), 대화 \\(d\\) 및 일부 생성된 내용 \\(c\\)에 대해 \\(M\\)이 해당 대화 \\(d\\)과 사실적으로 일치하는지 여부를 예측하도록 요청한다:\n' +
      '\n' +
      '\\[M(d,c)\\in\\{\\mathrm{consistent},\\mathrm{inconsistent}\\}.\\]\n' +
      '\n' +
      'Laban et al. (2022); Fabbri et al. (2022); Tang et al. (2023); Luo et al. (2023)에 이어, 우리는 사실적으로 일관되고 사실적으로 일관되지 않은 요약들의 불균형을 고려한 균형 정확도 (BAcc) 방법을 사용하여 모델들의 성능을 측정하였다. 문장 수준 및 요약 수준 예측 성능을 기반으로 결과를 분석했다. 달리 명시되지 않는 한, 여기에 표시된 모든 평가 결과는 테스트 세트를 기반으로 한다.\n' +
      '\n' +
      'Non-LLM 기반 Factuality Metrics로부터 Predictions 획득 우리가 사용한 Non-LLM 기반 모델은 평가될 소스 및 요약 텍스트 조각으로 취하고, 그들은 연속 값의 특정 범위 내에서 평가된 텍스트에 대한 점수를 반환한다. 점수가 높을수록 요약이 더 일관성이 있음을 시사합니다. Laban et al.(2022)에 이어 개발 집합을 이용하여 각 메트릭에 대한 임계값을 결정하고 선택된 임계값을 가정하여 테스트 집합 결과를 보고한다. 문장 수준 평가와 요약 수준 평가의 임계값을 별도로 선택했다. 임계값 이상의 값을 받는 텍스트는 문서와 사실적으로 일치하는 것으로 간주되고 그렇지 않으면 일관성이 없는 것으로 간주됩니다. 우리의 문장 수준 및 요약 수준 평가를 위해 입력 텍스트는 각각 요약 문장 및 전체 요약이었다.\n' +
      '\n' +
      'LLM에서 예측 및 설명을 얻으려면 사실적 일관성 레이블을 얻기 위해 두 가지 방법을 테스트했다. 먼저 LLM에 바이너리 레이블(Dir)을 제공하도록 직접 요청했습니다. 표 3에서 우리는 문장 수준 및 요약 수준 평가 모두에 대해 모든 LLM 기반 메트릭에 대해 문장 수준 프롬프트에 의해 얻어진 결과를 보여준다.6 우리는 완성된 프롬프트당 모든 LLM _3회_를 실행하고 평균 성능을 보고한다.\n' +
      '\n' +
      '각주 6: 문장 수준 레이블에서 요약 수준 레이블을 얻은 방법은 섹션 3.4를 참조하십시오. 요약 수준 프롬프트에 대한 성능은 부록 표 9에서 확인할 수 있습니다.\n' +
      '\n' +
      '다음으로 LLMs로부터 설명을 얻기 위해 다음과 같은 Chain-of-Thought 추론(Wei et al., 2022)을 도출하고자 하였다. 우리는 이진 판정(Exp)을 제공하는 것 외에도 LLM에 결정에 대한 설명을 제공하도록 요청하면서 이전 프롬프트를 조정했다. 모델 자기 일치도 평가를 위해 이 프롬프트의 출력에서 이진 예측을 추출하고, 설명 평가를 위해 설명(부록 H)을 추출했으며, 두 방법 모두에 대해 요약 수준 및 문장 수준 평가를 위한 프롬프트는 부록 C.3에서 찾을 수 있다.\n' +
      '\n' +
      '각주 7: 이 프롬프트의 이진 예측 성능은 부록 표 10에서 찾을 수 있으며, 여기서 우리는 설명을 위한 프롬프트가 이진 예측 태스크에 대한 성능을 향상시키지 않는다는 것을 보여준다.\n' +
      '\n' +
      '비LLM 사실 일관성 평가 모델은 좋은 성능을 보인다.표 3에서 보는 바와 같이, GPT-4는 대부분의 시간 동안 데이터 세트와 토픽 유형에 걸친 사실 일관성 평가에서 가장 좋은 성능을 보인다. 또한, 두 번째로 우수한 평가자 대부분은 GPT-3.5-터보를 포함한 LLM을 큰 마진으로 능가하는 모든 구성에 걸쳐 비LLM 기반 사실성 메트릭이다. MediaSum 데이터의 주요 주제 요약을 평가할 때 AlignScore는 문장 수준과 요약 수준 모두에서 성능에서 GPT-4를 능가하기도 한다. 비LLM 기반 평가자는 추론 속도가 빠르고(APIcall에 비해) 비용이 훨씬 적으며, 예측 작업을 완료하기 위해 16GB 또는 더 작은 GPU만 있으면 된다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '우리가 테스트한 오픈 소스 LLM의 경우 균형 잡힌 정확도 점수는 모두 50%에서 60% 사이이며 이는 기준선보다 거의 우수하지 않다. 이러한 모델들은 다양한 명령-후속 작업들(Xu et al., 2023; Li et al., 2023)에 비해 독점적인 모델들에 비해 인간에 의해 선호되는 출력들을 생성하는 것으로 도시되지만, 이들은 이 작업을 잘 수행하기에 충분한 판별 기술을 갖추고 있지 않다. 또한, **더 큰 오픈 소스 모델은 대부분의 설정에서 더 작은 대응물을 능가하지 않는다.** 예를 들어, Vicuna-33B의 성능은 MeetingBank 데이터의 한계 주제 요약에서 Vicuna-13B보다 13% 더 나쁘고 기준선보다 훨씬 더 나쁘다. 이에 대한 몇 가지 가능한 설명에는 이러한 모델이 이러한 유형의 데이터에 대해 사전 훈련되지 않았거나 독점 모델과 비교하여 이러한 유형의 작업에 대한 긴급 식별 능력을 개발할 만큼 충분히 크지 않다는 것이 포함될 수 있다.\n' +
      '\n' +
      '전반적으로, 이러한 발견은 최첨단의 LLM을 평가자로 사용하는 것에 대한 의심의 여지가 없는 감탄에 대한 주의를 높인다.**\n' +
      '\n' +
      '테스트한 모든 모델이 주요 주제 요약에서 오류를 감지하는 것은 더 어렵다.표 3에 나타난 바와 같이, 비LLM 기반 사실성 메트릭과 독점 LLM 모두에 대해, 주요 주제 요약에서 오류를 감지하는 데 평균적으로 약 10% 더 나쁜 반면, 최상의 모델인 GPT-4는 두 데이터 세트에 대해 문장 수준 예측 작업에서 약 10-20%의 성능 격차를 갖는다. 우리는 이것이 주요 주제 요약에 상대적으로 잘 감지할 수 있는 _외재적 정보_(그림 2)의 많은 비율이 포함되어 있지 않기 때문이라고 가정한다(섹션 5.2). 앞서 언급한 바와 같이, 우리가 테스트한 오픈 소스 LLM은 이 구별 작업을 수행하는 데 필요한 기술을 갖추고 있지 않기 때문에 다른 모델 유형에 대해 약간 더 쉬워 보이는 한계 주제 요약에서 일관된 성능 향상을 알아차리지 못한다. **전반적으로, 효율적이고 비용 효율적인 사실적 불일치 탐지**, 특히 주요 주제 요약의 경우 여전히 개선의 여지가 크며, 기존 모델의 성능은 항상 기준 성능에 매우 가깝습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**평가**&\\멀티컬럼{2}{c}{**MediaSum**}&\\멀티컬럼{2}{c}{**Meetingbank**}\\\\\\cline{2-5}\n' +
      '**Model** & **Dir** & **Exp** & **Dir** & **Exp** \\\\ \\hline Vicuna-13B & 0.35 & 0.11 & 0.38 & 0.00 \\\\ Vicuna-33B & 0.37 & 0.18 & 0.29 & 0.13 \\\\ WizardLM-13B & 0.47 & 0.33 & 0.54 & 0.18 \\\\ WizardLM-33B & 0.50 & 0.39 & 0.35 & 0.34 \\\\ \\hline GPT-3.5-Turbo & 0.57 & 0.44 & 0.59 & 0.51 \\\\ GPT-4 & 0.96 & **0.95** & 0.90 & **0.91** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: TofueEval.** 모델의 전체 테스트 세트에 대한 사실적 일관성 레이블을 예측하는 데 있어 **문장 수준 모델 자기 일치.** 모델은 3회 실행되고 자기 일치도는 Cohen의 카파에 의해 계산된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c l c c c c|c c c c} \\hline \\hline \\multirow{2}{*}{\\begin{tabular}{c} **Model** \\\\ **Type** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **Evaluation** \\\\ **Model** \\\\ \\end{tabular} } & \\multicolumn{4}{c}{**Sentence-Level (BAcc \\(\\uparrow\\))**} & \\multicolumn{4}{c}{**Summary-Level (BAcc \\(\\uparrow\\))**} \\\\ \\cline{3-10}  & & \\multicolumn{2}{c}{**MediaSum**} & \\multicolumn{2}{c}{**MeetingBank**} & \\multicolumn{2}{c}{**MediaSum**} & \\multicolumn{2}{c}{**MeetingBank**} \\\\ \\cline{3-10}  & & **Main** & **Marginal** & **Main** & **Marginal** & **Main** & **Marginal** & **Main** & **Marginal** \\\\ \\hline - & Baseline & 50.0 & 50.0 & 50.0 & 50.0 & 50.0 & 50.0 & 50.0 & 50.0 \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} **Non-** \\\\ **LLM** \\\\ \\end{tabular} } & SummaC-ZS & 66.1 & 73.9 & 63.9 & 80.6 & 62.7 & 64.1 & 58.1 & 72.4 \\\\  & SummaC-CV & 67.6 & 73.0 & 62.6 & 77.3 & 61.2 & 66.5 & 52.4 & 72.9 \\\\  & QFAectEval & 53.9 & 74.0 & 58.0 & 75.8 & 61.4 & 74.2 & 55.1 & 68.2 \\\\  & AlignScore & **69.2** & 76.2 & 61.2 & 78.6 & **65.5** & 72.1 & 63.4 & 71.8 \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} **Open** \\\\ **Source** \\\\ **LLM** \\\\ \\end{tabular} } & Vicuna-13B & 54.0 & 54.8 & 49.6 & 61.9 & 55.6 & 59.1 & 51.2 & 59.2 \\\\  & Vicuna-33B & 51.0 & 51.1 & 53.6 & 48.4 & 52.5 & 53.4 & 53.2 & 51.0 \\\\  & WizardLM-13B & 59.8 & 53.5 & 58.8 & 56.6 & 57.0 & 54.5 & 54.6 & 58.0 \\\\  & WizardLM-30B & 54.5 & 53.9 & 53.5 & 53.4 & 53.3 & 54.4 & 53.0 & 53.2 \\\\ \\hline \\multirow{2}{*}{\n' +
      '\\begin{tabular}{c} **Prop.** \\\\ **LLM** \\\\ \\end{tabular} } & GPT-3.5-Turbo & 61.6 & 68.9 & 56.0 & 65.0 & 59.6 & 65.8 & 63.2 & 65.7 \\\\  & GPT-4 & 64.9 & 80.2 & 67.5 & 90.3 & 63.7 & 78.9 & 74.7 & 83.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: TofueEval.**의 테스트 세트에 대한 사실적 일관성 평가자에 대한 ** 문장 수준 및 요약 수준 균형 정확도(BAcc).** LLM 기반 방법의 경우 전체 요약에 대한 일관성 레이블을 직접 예측하는 것보다 더 나은 성능을 달성하기 때문에 문장 수준 레이블을 집계하여 요약 수준 레이블을 보여준다. LLM에 대한 모든 결과는 평균 3회 실행입니다. 항상 불일치하거나 일관성을 예측하는 _baseline_ 방법은 50% 균형 잡힌 정확도를 달성한다는 점에 유의한다. 최고의 결과와 두 번째로 좋은 결과를 강조합니다. *Main**-토픽 요약 및 **Marginal**-토픽 요약 모두에 대한 예측 결과가 표시된다.\n' +
      '\n' +
      '일관성 또는 일관성을 예측합니다. 섹션 5.2에서 모델이 식별하지 못하는 오류 유형의 차이를 탐구한다.\n' +
      '\n' +
      '대부분의 LLM, 특히 작은 LLM은 동일한 프롬프트로 생성된 여러 예측에 대해 일관성이 부족하다. 요약 문장의 사실성에 대한 예측을 비교하여 각 모델의 _self-agreement_를 계산한다.8 직접 이진 레이블 예측(Dir)의 이진 예측 결과와 설명(Exp)(섹션 5.1)을 사용한 레이블 예측을 기반으로 각 모델(코헨의 카파처럼)에 대한 세 가지 런에 걸친 문장 수준 자기 일치를 표 4에 제공한다. 우리는 GPT-4가 모든 설정에서 거의 완벽한 일치를 가지고 있음을 관찰하여 모델이 일관된 예측을 하는 반면 나머지 모델은 Dir 예측에 대한 대부분의 시간(0.2와 0.6 사이의\\(\\kappa\\))에서 중간 정도의 일치를 가지고 있음을 시사한다. 흥미롭게도 우리는 사실성에 대한 이진 예측을 하는 것 외에도 모델에 설명을 요청하는 것이 코헨의 카파 점수를 낮추는 것을 관찰한다. 이것은 우리가 일치하여 최대 0.38의 드롭을 갖는 더 작은 13B 모델에 대해 더 분명하다. 우리는 이진 예측과 함께 설명을 생성하도록 모델을 프롬프트하는 것이 작업에 추가 복잡성을 추가하여 결정론적 결과를 덜 산출하고 직접 이진 레이블 예측 작업에 비해 자기 일치도를 낮춘다고 가정한다.\n' +
      '\n' +
      '각주 8: 독점 LLM은 오픈 소스 LLM과 같은 토큰 확률을 제공하지 않기 때문에 공정한 비교를 위해 우리는 각 모델에서 세 개의 런을 직접 비교하고 세 가지 예측에서 코헨의 카파를 계산하기로 선택했다.\n' +
      '\n' +
      '본 연구의 결과는 미디어썸 자료(\\(\\rho\\)=0.79, \\(p\\)=0.02)와 미팅뱅크 자료(\\(\\rho\\)=0.99, \\(p\\)=0.00)에서 유의한 상관관계가 있는 것으로 나타났다. 즉, 본 연구의 결과는 미디어썸 자료(\\(\\rho\\)=0.79, \\(p\\)=0.02)에서 유의한 상관관계가 있는 것으로 나타났다. 모델이 예측에서 더 큰 자기 일치를 보일 때, 그들은 이러한 예측 라벨에 더 높은 확률 질량을 할당한다. 이는 모델의 균형 잡힌 정확도와 잘 상관된다는 점을 감안할 때 이러한 모델을 모두 고려할 때 이 작업에 대해 잘 보정되었다고 결론지었다.9\n' +
      '\n' +
      '각주 9: 여기서 우리의 통찰력은 0.7의 온도를 갖는 구성들에 기초한다. 이 발견은 다른 온도들에 대해 성립되지 않을 수 있다. 예를 들어, 0의 온도는 결정론적 출력으로 이어져 완벽한 일치를 나타내지만 이러한 경우 균형 잡힌 정확도는 여전히 낮을 수 있다.\n' +
      '\n' +
      '더 많은 연구 결과는 부록 E에서 찾을 수 있다.\n' +
      '\n' +
      '### Error Analysis\n' +
      '\n' +
      'Tang et al. (2023)에 이어, 평가자 모델의 오류 유형 탐지 성능을 _recall_.10 측면에서 분석하였다. 우리는 사실적으로 불일치하는 모든 요약 문장을 하나의 오류 유형만 포함하는 여러 하위 집합으로 나누었다. 평가자의 성능을 고려하여 표 3과 같이 비교적 강한 평가자의 하위 집합을 선택하여 그림 3에 결과를 보여준다.\n' +
      '\n' +
      '각주 10: 평가자 모델은 오차 유형을 예측하지 않기 때문에 오차 유형별로 정밀도를 기술적으로 정의할 수 없다.\n' +
      '\n' +
      '비LLM 기반 평가 메트릭은 모든 오류 유형을 포착하는 데 더 우수하다. 그림 3에서는 파란색의 LLM 기반 평가자와 주황색의 비LLM 기반 평가자의 성능을 보여준다. 우리는 모든 평가자가 우리가 외재적 정보라고 부르는 것을 식별하는 데 상당히 잘 수행한다는 것을 관찰한다. 이는 이러한 유형의 오류가 주로 익숙하지 않은 명사구 또는 이벤트(문서에 비해)를 포함한다는 사실 때문일 수 있으며, 이는 모델의 이러한 오류 유형 탐지를 용이하게 한다고 가정한다. 즉, GPT-3.5-터보\n' +
      '\n' +
      '그림 3: **오류 유형별 요약 사실 불일치 예측의 리콜. 비LLM 기반 사실성 메트릭은 모든 오류 유형에 걸쳐 LLM 기반 평가자보다 오류를 포착하는 데 더 우수하다.**\n' +
      '\n' +
      'LLM 기반 메트릭의 비율보다 약 30% 낮은 이러한 오류 중 50%만 감지합니다. 나머지 오류 유형의 경우 LLM 기반 메트릭과 비LLM 기반 메트릭 간에 탐지율에 큰 격차가 있다. 테스트된 LLM이 더 나은 프롬프트 설계로 특정 오류 유형을 식별하는 데 더 잘 수행할 수 있지만 향후 작업을 위해 이를 남겨둔다.\n' +
      '\n' +
      '** 리콜 및 균형 잡힌 정확도는 분석에서 평가자의 행동에 대한 통찰력을 제공하는 보완 메트릭이라는 점에 유의해야 한다. 리콜이 높다고 해서 반드시 균형 잡힌 정확도가 높은 것은 아니며, 그 반대의 경우도 마찬가지이다. 예를 들어, GPT-4는 비LLM 기반 모델만큼 많은 에러를 포착하지는 않지만, 더 높은 균형 잡힌 정확도를 달성한다(표 3 참조). 그러나 비LLM 기반 메트릭은 대부분의 설정에서 재현율과 균형 잡힌 정확도 모두에서 GPT-3.5-터보를 능가하여 성능이 우수함을 시사한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '우리는 LLM 생성 요약을 사용한 주제 중심 대화 요약에 대한 새로운 사실 일관성 평가 벤치마크 ToFuEval을 제안했다. 우리는 요약자 역할을 하는 LLMs가 요약에서 많고 다양한 환각을 만든다는 것을 발견한다. 또한 균형 잡힌 정확도를 측정하고 모델의 오류 유형을 분석함으로써 LLM 기반 평가자와 기존 최첨단 비LLM 기반 사실성 메트릭 모두 대화에서 광범위한 환각을 감지하는 것이 여전히 어렵다는 결론을 내린다.\n' +
      '\n' +
      '## Limitations\n' +
      '\n' +
      '우리의 작업에는 몇 가지 한계가 있습니다. 먼저, 제안된 두부 평가 벤치마크에서, 우리는 이미 높은 주석 작업의 복잡성으로 인해 단일 문장 이상으로 확장될 수 있는 사실적 일관성 오류에 주석을 달도록 인간 평가자에게 요청하지 않는다. 예를 들어, 문장간 오류의 한 유형은 Pagnoni et al.(2021)에서 논의된 바와 같이, 담화 오류이다. 둘째, 평가 프레임워크는 모든 사실 오류 유형이 가지고 있는 잠재적 영향 정도를 구별하지 않고 모든 사실 오류를 동일한 심각성을 갖는 것으로 취급한다. 셋째, 요약 평가는 영어 대화에 맞게 특별히 조정됩니다. 이 작업에서 평가된 모델은 다른 도메인 및 다른 언어에 대해 다른 성능을 나타낼 수 있다. 또한, 우리는 LLM에 대한 더 나은 프롬프트를 식별하기 위해 광범위한 프롬프트 엔지니어링을 수행하지 않으며, 이는 사실적 일관성의 개선 또는 사실적 오류 검출의 개선으로 이어질 수 있다. 우리는 이 조사를 미래의 일에 맡긴다. 마지막으로, GPT-3.5-Turbo와 GPT-4가 사실적 일관성 평가에서 최근 LLMs의 성능에 대해 대표적인 것으로 나타났기 때문에 더 큰 LLMs 집합을 사실적 일관성 평가자로 평가하지 않는다. 인식된 한계에도 불구하고, 제안된 벤치마크와 통찰력이 자동화된 평가 메트릭을 강화하고 더 나은 사실적 일관성 성능을 가진 요약자 개발에 초점을 맞춘 향후 작업에 영감을 주길 바란다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '저자들은 이 프로젝트에 중요한 기여를 한 주석 팀에 감사를 표하고 싶다: 마리카 홀, 호열 킴, 폴 고덴, 엘비라 마고메도바, 테디 무기아, 다니엘 노스, 주세피나 실버스트리, 헬렌 새치웰, 안나 스탈만, 에이든 티스, 마이클 발렌테코비치, 제니퍼 원, 캐롤라이나 코쿠바, 닐 모리시, 앤디 브리지스, 데릭 차오, 메이 렝, 매튜 마호니, 앤드류 맥넬리, 프랜시스 오브라이언, 알렉스 모웬, 니콜 라마나. 우리 주석 팀의 모든 구성원은 미국에 기반을 두고 있으며 미국의 유사한 역할에 대해 벤치마킹되는 경쟁적인 시간당 요금을 받습니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* 조와 왕(2021) 슈양 조와 루 왕. 2021. CLIFF: 추상적 요약에서 충실성과 사실성 향상을 위한 대비학습. _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 6633-6649, Online and Punta Cana, Dominican Republic. 컴퓨터 언어학과의 연관성\n' +
      '* Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zhang, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: 90%*Chat-GPT 품질로 GPT-4를 인상하는 오픈소스 챗봇.\n' +
      '* Fabbri et al. (2022) Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. QAFactEval: 요약에 대한 개선된 QAB 기반 사실 일관성 평가. _Proceedings of the 2022 Conference of the North American chapter of Computational Linguistics Association: Human Language Technologies_, pages 2587-2601, 미합중국 시애틀. 컴퓨터 언어학과의 연관성\n' +
      '\n' +
      '알렉산더 R. Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, Dragomir Radev. 2021. SummEval: 재평가 요약 평가_ Computational Linguistics_, 9:391-409의 트랜잭션.\n' +
      '* Falke et al. (2019) Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness: interesting but challenging application for natural language inference. _Proceedings of the 57th Annual Meeting for Computational Linguistics_, pages 2214-2220, Florence, Italy. 컴퓨터 언어학과의 연관성\n' +
      '* Fleiss(1971) Joseph L. 플리스 1971. 다수의 평가자 간의 명목 규모 일치도 측정. _ Psychological Bulletin_, 76(5):378-382.\n' +
      '* Fu et al. (2023) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. GPTScore : 원하는 대로 평가한다. _ arXiv preprint arXiv:2302.04166_.\n' +
      '* Gao et al. (2023) Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023. RARR: 언어 모델을 사용하여, 언어 모델이 말하는 것을 검색하고 수정한다. _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 16477-16508, Canada, Toronto. 컴퓨터 언어학과의 연관성\n' +
      '*가오와 완(2022) 밍치가오와 샤오준 완. 2022. DialSummEval: 대화에 대한 요약 평가 재방문. _Proceedings of the 2022 Conference of the North American chapter of Computational Linguistics Association: Human Language Technologies_, pages 5693-5709, 미합중국 시애틀. 컴퓨터 언어학과의 연관성\n' +
      '* Gliwa et al. (2019) Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. SAMSum corpus: abstractive summarization을 위한 사람 주석 대화 데이터셋. _Proceedings on the New Frontiers in Summarization_, pages 70-79, Hong Kong, China. 컴퓨터 언어학과의 연관성\n' +
      '* 고얄과 더렛(2021) 타냐 고얄과 그렉 더렛. 2021. 요약에서 세밀한 사실성을 주석하고 모델링한다. _Proceedings of the 2021 of the North American chapter of the Computational Linguistics Association: Human Language Technologies_, pages 1449-1462, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Goyal et al. (2022) Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. GPT-3 시대의 뉴스 요약 및 평가. _arXiv preprint arXiv:2209.12356_.\n' +
      '* Halliday et al. (2014) M.A.K. Halliday, Christian M.I.M. Matthiessen, Michael Halliday, and Christian Matthiessen. 2014. _An Introduction of Functional Grammar_. 루틀리지\n' +
      '* Hu et al. (2023) Yebowen Hu, Timothy Ganter, Hanieh Deilamsalehy, Franck Dernoncourt, Hassan Foroosh, and Fei Liu. 2023. MeetingBank: 회의 요약을 위한 벤치마크 데이터세트. _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 16409-16423, Canada, Toronto. 컴퓨터 언어학과의 연관성\n' +
      '* Kryscinski et al. (2019) Wojciech Kryscinski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural text 요약: 비판적 평가. _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP)_, pages 540-551, Hong Kong, China. 컴퓨터 언어학과의 연관성\n' +
      '* Kryscinski et al. (2020) Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. 추상적 텍스트 요약의 사실적 일관성 평가. _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 9332-9346, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Laban et al. (2023) Philippe Laban, Wojciech Kryscinski, Divyansh Agarwal, Alexander Fabbri, Caiming Xiong, Shafiq Joty, and Chien-Sheng Wu. 2023. SummEdits: 요약의 렌즈를 통해 사실적 추론에서 LLM 능력을 측정하는 것. _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 9662-9676, Singapore. 컴퓨터 언어학과의 연관성\n' +
      '* Laban et al. (2022) Philippe Laban, Tobias Schnabel, Paul N. 베넷과 마티 에이 허스트 2022. SummAC: 요약에서 불일치 검출을 위한 NLI 기반 모델을 재방문하는 _ Computational Linguistics_, 10:163-177의 트랜잭션.\n' +
      '* Li et al. (2023) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacaeval: 명령어-추종 모델의 자동 평가자. [https://github.com/tatsu-lab/alpaca_eval] (https://github.com/tatsu-lab/alpaca_eval).\n' +
      '* Lin(2004) Chin-Yew Lin. 2004. ROUGE: 요약의 자동 평가를 위한 패키지. _텍스트 요약 분기 아웃_에서 스페인 바르셀로나의 74-81페이지입니다. 컴퓨터 언어학과의 연관성\n' +
      '* Luo et al. (2023) Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. 추상적 텍스트 요약에 대한 사실적 불일치 평가자로서 ChatGPT. _ arXiv preprint arXiv:2303.15621_.\n' +
      '* Madaan et al. (2020) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shirman Prabhumoye, Yiming Yang, Sean Welleck, Bhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. 자기 정제: 자기 피드백으로 반복 정제.\n' +
      '\n' +
      '조슈아 마네즈, 샤시 나라얀, 베른드 보넷, 라이언 맥도날드 2020. 추상적 요약에서 충실성과 사실성에 관한 것이다. _Proceedings of the 58th Annual Meeting for Computational Linguistics_, pages 1906-1919, Online. 컴퓨터 언어학과의 연관성\n' +
      '*McHugh(2012) Marry L. 맥휴 2012. Interrater reliability: kappa statistic. _ Biochemia Medica_, pages 276-282.\n' +
      '* Nenkova and Passonneau (2004) Ani Nenkova and Rebecca Passonneau. 2004. Evaluation in summary: The pyramid method. [Proceedings of the Human Language Technology Conference of the North American Chapter of Computational Linguistics Association for Computational Linguistics: HLT-NAACL 2004_, pages 145-152, Boston, Massachusetts, USA. 컴퓨터 언어학과의 연관성\n' +
      '* OpenAI(2023) OpenAI. 2023. GPT-4 기술보고서 _ ArXiv_, abs/2303.08774.\n' +
      '* Pagnoni et al. (2021) Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. FRANK와의 추상적 요약에서의 사실성 이해: 사실성 메트릭의 벤치마크. _Proceedings of the 2021 of the North American chapter of the Computational Linguistics Association: Human Language Technologies_, pages 4812-4829, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: 기계 번역의 자동 평가 방법. _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_, pages 311-318, Philadelphia, Pennsylvania, USA. 컴퓨터 언어학과의 연관성\n' +
      '* Saunders et al. (2022) William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. 인간 평가자를 보조하기 위한 자기 평가 모델들. _ CoRR_, abs/2206.05802.\n' +
      '* Tang et al. (2023a) Liyan Tang, Tanya Goyal, Alex Fabbri, Philippe Laban, Jiacheng Xu, Semih Yavuz, Wojciech Kryscinski, Justin Rousseau, and Greg Durrett. 2023a. 요약의 사실적 오류 이해: 오류, 요약자, 데이터 세트, 오류 탐지기입니다. _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 11626-11644, Canada, Toronto. 컴퓨터 언어학과의 연관성\n' +
      '* Tang et al. (2023b) Liyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G. Nestor, Ali Soroush, Pierre A. Elias, Ziyang Xu, Ying Ding, Greg Durrett, Justin F. Rousseau, Chunhua Weng, and Yifan Peng. 2023b. 의학 증거 요약에 대한 대규모 언어 모델 평가 npj Digital Medicine_, 6(1).\n' +
      '* Tang et al. (2022) Xiangru Tang, Arjun Nair, Borui Wang, Bingyao Wang, Jai Desai, Aaron Wade, Haoran Li, Asli Celikyilmaz, Yashar Mehdad, and Dragomir Radev. 2022. CONFIT: 언어학적으로 정보에 입각한 대조적 미세 조정과 함께 충실한 대화 요약에 관한 것이다. _Proceedings of the 2022 Conference of the North American chapter of Computational Linguistics Association: Human Language Technologies_, pages 5657-5668, 미합중국 시애틀. 컴퓨터 언어학과의 연관성\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: 개방적이고 효율적인 기초 언어 모델들.\n' +
      '* Wang et al. (2023) Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, 및 Jie Zhou. 2023. ChatGPT는 좋은 NLG 평가자입니까? 예비 연구. _ arXiv preprint arXiv:2303.04048_.\n' +
      '* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. 르와 데니 저우 2022. Chain-of-thought prompting은 큰 언어 모델에서 추론을 이끌어낸다. _NeurIPS_에서.\n' +
      '*Xu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. WizardLM: 복잡한 지시를 따르도록 큰 언어 모델을 엠파워링하는 단계 _ arXiv preprint arXiv:2304.12244_.\n' +
      '* Zha et al. (2023) Yuheng Zha, Yichi Yang, Ruichen Li, Zhiting Hu. 2023. AlignScore: 통합된 정렬 함수로 사실적 일관성을 평가하는 단계. _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 11328-11348, Canada, Toronto. 컴퓨터 언어학과의 연관성\n' +
      '* Zhang* et al. (2020) Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. 와인버거, 요브 아르치 2020. BERTScore: BERT로 텍스트 생성을 평가한다. _International Conference on Learning Representations_.\n' +
      '* Zhang et al. (2023) Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori Hashimoto. 2023. 뉴스 요약용 대용량 언어 모델 벤치마킹 ArXiv_, abs/2301.13848.\n' +
      '* Zhu et al. (2021) Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. 2021. MediaSum: 대화 요약용 대규모 미디어 인터뷰 데이터셋. _Proceedings of the 2021 of the North American chapter of the Computational Linguistics Association: Human Language Technologies_, pages 5927-5934, Online. 컴퓨터 언어학과의 연관성\n' +
      '\n' +
      '## 부록 A YouTubeal 기술 통계\n' +
      '\n' +
      '표 5의 TofuEval의 기술 통계를 보여준다. 우리는 NLTK 패키지에 의해 단어 수를 측정한다. 모든 대화 상자는 영어로 작성됩니다.\n' +
      '\n' +
      '## 부록 B 모델 상세\n' +
      '\n' +
      '### Summary Generation\n' +
      '\n' +
      '요약 생성을 위해 Vicuna-7B11(Chiang et al., 2023) 및 WizardLM-7B/13B/30B12(Xu et al., 2023)를 사용하기로 선택했으며, 둘 다 Llama(Touvron et al., 2023)를 기반으로 한다. 또한 Falcon-7b/40b-instruct13과 mpt-7b-instruct14를 포함한 다른 오픈소스 LLM들을 실험해 본 결과, Vicuna와 WizardLM은 일반적으로 우리의 작업에 대한 지시에서 더 잘 수행하고 프롬프트에 더 강인함을 알 수 있었다. 또한 공식 API를 통해 GPT-3.5-터보에서 요약을 수집한다.\n' +
      '\n' +
      '각주 11: [https://huggingface.co/lmsys/vicuna-7b-delta-v0](https://huggingface.co/lmsys/vicuna-7b-delta-v0)\n' +
      '\n' +
      '각주 12: [https://huggingface.co/WizardLM/WizardLM-7B-V1.0](https://huggingface.co/WizardLM/WizardLM-7B-V1.0), WizardLM-7B-V1.0, [https://huggingface.co/WizardLM/wizardLM-7B-V1.0](https://huggingface.co/WizardLM/wizardLM-7B-V1.0)\n' +
      '\n' +
      '각주 13: [https://huggingface.co/tiuae/falcon-7b-instruct](https://huggingface.co/tiuae/falcon-7b-instruct), [https://huggingface.co/tiuae/falcon-40b-instruct](https://huggingface.co/tiuae/falcon-40b-instruct)\n' +
      '\n' +
      '각주 14: 모자이크리즘/mpt-7b-명령어.\n' +
      '\n' +
      '각주 15: [https://github.com/tingofurro/summac/](https://github.com/tingofurro/summac/)\n' +
      '\n' +
      'LLM 기반 요약에 대한 모델 선택 프로세스는 인간 평가를 위해 6월 초에 최종 결정되었으며 그 결과 이후 개발된 모델에 의해 생성된 요약은 포함하지 않았다.\n' +
      '\n' +
      '### Summary Evaluation\n' +
      '\n' +
      '본 연구에서는 SummaC-ZS, SummaC-CV15(Laban et al., 2022) 및 AlignScore16(Zha et al., 2023)의 세 가지 SOTA 및 특수 사실성 메트릭을 통합한다. 이러한 메트릭들은 요약 문장들이 소스 문서들로부터 추출된 텍스트의 일부 부분에 의해 추론될 수 있는지를 결정하기 위해 설계된다. 또한, SOTA 질문-응답(QA) 기반 사실성 메트릭 QAFactEval17(Fabbri et al., 2022)을 포함하며, 이는 질문을 생성하고 생성된 질문의 답변 가능성을 검증함으로써 사실적 일관성을 평가한다. 이러한 모델에 대한 자세한 내용은 독자에게 원본 작품을 추천합니다. Vicuna-33B18의 경우 Llama(Touvron et al., 2023)를 기반으로 한 것을 사용한다. 이러한 모델이 초기 벤치마크에서 제대로 수행되지 않았기 때문에 팔콘-40b 및 mpt-30b와 같은 명령어 조정 LLM을 포함하지 않는다.\n' +
      '\n' +
      '각주 18: [https://huggingface.co/lmsys/vicuna-33b-v1.3](https://huggingface.co/lmsys/vicuna-33b-v1.3)\n' +
      '\n' +
      '각주 19: 우리는 또한 모델들에게 고정된 수의 문장들을 생성하도록 요청함으로써 요약 길이를 제어하려고 노력했지만, 여기서 평가된 대부분의 모델들은 어느 포맷에서도 길이 제약을 잘 따를 수 없다.\n' +
      '\n' +
      '## 부록 C 프롬프트\n' +
      '\n' +
      '토픽 생성을 위한### 프롬프트\n' +
      '\n' +
      '다음 프롬프트를 사용하여 두부에벌에서 대화 문서에 대한 주제를 생성합니다.\n' +
      '\n' +
      '_Document: [Document]_\n' +
      '\n' +
      '_제공된 문서에서 사람들이 알고 싶어하는 세 가지 주요 주제를 열거합니다. 각 토픽은 약 5단어여야 합니다._\n' +
      '\n' +
      '토픽 중심의 요약에 대한### 프롬프트\n' +
      '\n' +
      '우리는 토픽-초점 텍스트 요약에 대한 프롬프트19를 제로-샷 방식으로 형성하기 위해 모델의 기본 접두사에 다음 지시를 추가한다:\n' +
      '\n' +
      '각주 19: [https://github.com/salesforce/QAFactEval](https://github.com/salesforce/QAFactEval)\n' +
      '\n' +
      '_Document: [Document]_\n' +
      '\n' +
      '_제공된 문서를 "[주제]"를 중심으로 요약한다. 요약어는 길이가 50단어 미만이어야 합니다._\n' +
      '\n' +
      '요약 평가를 위한### 프롬프트\n' +
      '\n' +
      '이 섹션에는 LLM에서 이진 사실 일관성 레이블 및 설명을 얻기 위해 사용한 모든 프롬프트가 포함되어 있습니다. 섹션 5.1에서 추가 세부 정보를 찾을 수 있다. 문장 수준 프롬프트의 경우 컨텍스트로 프롬프트에서 이전 요약 문장을 제공하는 **가 초기 연구**에서 성능에 영향을 미치지 않는다는 것을 발견하므로 단순화를 위해 고립된 문장만 제공했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline\n' +
      '**Dataset** & **\\# Doc.** & **Avg** & **\\# Asp.** & **\\# All** & **Main** & **\\#** & **\\#** \\\\  & **Doc.** & **Len** & **/ Doc.** & **LLM** & **Topic** & **Turn** & **Speaker** \\\\ \\hline MediaSum & 50 & 970 & 3 & 5 & 78\\% & 16.6 & 5.7 \\\\ MeetingBank & 50 & 930 & 3 & 5 & 73\\% & 19.9 & 4.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: **Dataset statistics on TofuEval. 우리는 각 데이터 세트에서 50개의 테스트 세트 문서를 샘플링하고 각 문서에 대해 3개의 주제를 생성한 다음 각 주제에 대해 5개의 LLM에서 요약을 수집한다. 우리는 인간에 의해 평가된 주요 주제의 백분율을 보여준다(섹션 3.4 보다).**\n' +
      '\n' +
      '**(Dir) Binary-Label, Sentence-Level Prompt** LLMs에게 다음과 같은 프롬프트를 사용하여 요약 문장에 대한 이진 사실 일관성 레이블을 제공하도록 요청하였다:\n' +
      '\n' +
      '_Document: [Document]_\n' +
      '\n' +
      '_Sentence: [Sentence]_\n' +
      '\n' +
      '_문장이 위에서 제공된 문서와 사실적으로 일치하는지를 결정한다. 문장은 문서에 의해 수반될 수 있는 경우(명시되거나 암시된 경우) 문서와 사실적으로 일치한다. "예" 또는 "아니오"로 답변해 주십시오._\n' +
      '\n' +
      '**(Dir) Binary-Label, Summary-Level Prompt** LLMs에게 다음 프롬프트를 사용하여 요약을 위한 이진 사실 일관성 라벨을 제공하도록 요청하였다:\n' +
      '\n' +
      '_Document: [Document]_\n' +
      '\n' +
      '_Summary: [Summary]_\n' +
      '\n' +
      '_요약이 위에서 제공된 문서와 사실적으로 일치하는지를 결정한다. 요약은 사실 위에 제공된 문서와 일치한다. 프롬프트:\n' +
      '\n' +
      '_Document: [Document]_\n' +
      '\n' +
      '_Sentence: [Sentence]_\n' +
      '\n' +
      '_문장이 위에서 제공된 문서와 사실적으로 일치하는지를 결정한다. 문장이 문서에 의해 수반될 수 있는 경우(명시되거나 암시된 경우)에는 사실상 일관성이 있다. "예" 또는 "아니오"로 답변을 시작하십시오. 이유를 50단어 이내로 간단히 설명하십시오._\n' +
      '\n' +
      '**(Exp) 설명이 있는 이진-라벨, 요약-레벨 프롬프트** 해당 설명이 있는 요약-레벨 사실성 평가에 다음과 같은 프롬프트를 사용하였다.\n' +
      '\n' +
      '_Document: [Document]_\n' +
      '\n' +
      '_Summary: [Summary]_\n' +
      '\n' +
      '_요약이 위에서 제공된 문서와 사실적으로 일치하는지를 결정한다. 요약의 모든 정보가 문서에 의해 수반될 수 있는 경우 요약은 문서와 사실적으로 일치한다. "예" 또는 "아니오"로 답변을 시작하십시오. 이유를 50단어 이내로 간단히 설명하십시오._\n' +
      '\n' +
      '**최종 답변을 제공하기 전에 설명을 생성하도록 LLMs를 프롬프트하는 것은 아무런 성능 차이도 발생하지 않는다.** 소규모 실험에서, 우리는 최종 답변을 제공하기 전에 모델이 설명을 생성하도록 프롬프트할 때, 모델에 의해 생성된 응답이 _"이 문장/요약은 사실적으로 문서 "_"와 일치하는 것과 같은 문장으로 시작하는 경향이 있고, 실제 설명은 두 번째 문장 이후에 시작된다는 것을 관찰했다. 이는 _"Yes"_ 또는 _"No"_로 응답을 시작하는 것과 유사하므로, 단순화를 위해 후자를 선택하였다.\n' +
      '\n' +
      '## 부록 D 확장 결과: 요약자로서의 LLM\n' +
      '\n' +
      '### 관련성 및 완전성 평가\n' +
      '\n' +
      '_Rel._에 도시된 바와 같이 표 6의 열, 각 LLM의 평균 관련성 점수는 최대 1에 가깝다(자세한 내용은 섹션 3.4 참조). 우리는 **모든 LLM이 요청된 주제에 집중할 수 있으며 더 큰 모델이 약간 더 나은 성능을 달성한다고 결론지었다.\n' +
      '\n' +
      '각주 20: GPT-3.5-터보의 모델 크기는 알려져 있지 않기 때문에 오픈 소스 LLM에 대한 제한된 관찰을 기반으로 한다.\n' +
      '\n' +
      '다음으로, 요청된 주제를 다룰 때 모델의 성능을 비교한다. 더 작은 LLM이 요약 완성도에서 동등하거나 심지어 우수한 성능을 달성할 수 있지만, 작은 LLM에 의해 생성된 요약의 길이가 훨씬 더 높다는 점에 주목할 필요가 있다. 예를 들어, Vicuna-7B는 평균 요약 길이가 72 단어로 MeetingBank에서 0.72의 완전성을 달성한다. 대조적으로, GPT-3.5-터보는 더 간결한 요약(53개 단어)으로 비교 가능한 완전성 점수 0.74를 달성한다. 이러한 경향은 크기가 다양한 세 가지 위저드LM 모델에서도 마찬가지이다. 위저드LM 모델 크기가 클수록, 요약은 짧게 만들면서 요약에서 더 많은 관련 정보를 유지하거나 커버할 수 있는 모델이다(위저드LM-13B vs. 위저드LM-30B). 따라서 우리는 **더 큰 LLMs가 더 작은 LLMs.**에 비해 정보 밀도가 높은 요약을 생성할 수 있다고 결론지었다.\n' +
      '\n' +
      '### 사실 일치성 평가\n' +
      '\n' +
      '기존 LLM은 여전히 상당한 양의 사실적 오류를 만든다.표 6에서 주석에 따라 데이터 세트에 걸쳐 각 모델에 대해 적어도 하나의 사실적 불일치가 있는 요약의 백분율을 보여준다. GPT-3.5-터보를 제외한 모든 모델 중 요약의 약 40-50%가 적어도 하나의 사실적 불일치를 포함한다는 것을 발견했다. 또한 ** 요약 길이와 해당 요약에 포함된 오류 수 사이에는 직접적인 양의 상관관계가 없다. 예를 들어, MediaSum 자료에서 WizardLM-13B의 요약은 38.9%가 70의 평균 요약 길이와 일치하지 않는 반면 GPT-3.5-Turbo의 요약은 24.0%가 57의 더 높은 평균 길이를 갖는 반면 일치하지 않는 요약의 비율과 모델 길이 사이의 피어슨 상관계수 \\(\\rho\\)는 0.18이고, \\(p\\)-값은 0.57로 약한 양의 상관관계를 보였다.\n' +
      '\n' +
      '더 큰 LLM이 반드시 더 적은 사실적으로 일치하지 않는 요약을 생성하는 것은 아니다.표 6에서 볼 수 있듯이 동일한 모델 패밀리를 비교할 때 WizardLM-30B는 MediaSum의 WizardLM-13B보다 약간 더 많은 수의 오류를 생성하고 WizardLM-13B는 MeetingBank의 WizardLM-7B보다 더 많은 오류를 생성한다. 또한, 더 큰 모델은 더 낮은 오류율을 가질 수 있지만, 감소는 미미할 수 있다. 예를 들어, WizardLM-30B의 오류율은 MeetingBank 데이터에서 WizardLM-7B의 오류율보다 불과 3.8% 낮다. 다른 패밀리의 모델을 비교하면 WizardLM-13B의 오류율은 MeetingBank의 Vicuna-7B의 오류율과 동일하다.\n' +
      '\n' +
      '데이터셋은 모형 오차율에 영향을 미친다.<표 2>에서 보는 바와 같이 모형들은 평균적으로 MeetingBank보다 MediaSum에서 오차가 더 많이 발생한다. 그 차이는 주요 주제에 대해 더 유의하며 문장 수준에 비해 요약 수준 수행에 의해 확대된다. 이는 텍스트 배포가 모델의 요약 성능에 무시할 수 없는 영향을 미친다는 것을 보여준다. 한 가지 가설은 모델이 대화 전환에 걸쳐 정보를 집계하고 합성해야 하는 특정 주제와 관련된 사실적으로 일관된 요약을 생성하는 것이 더 어렵다는 것이며, 우리는 주제 관련 정보가 미팅뱅크보다 미디어섬에서 대화 전환에 걸쳐 더 고르게 분포하는 경향이 있음을 발견했다.\n' +
      '\n' +
      '## 부록 E 확장 결과: 평가자로서의 LLM\n' +
      '\n' +
      '균형 잡힌 정확도와 회상을 통해 평가자의 성과를 평가하는 것 외에도, 5절에 자세히 설명된 바와 같이, 거짓 긍정 비율(FPR)과 거짓 부정 비율(FNR)을 분석하여 사실적으로 일관되지 않은 요약 또는 요약 문장을 식별하는 데 있어 평가자의 신뢰도도 조사한다.\n' +
      '\n' +
      '\\[\\mathrm{FPR}=\\frac{\\mathrm{FP}}{\\mathrm{FP}+\\mathrm{TN}},\\quad\\mathrm{FNR}= \\frac{\\mathrm{FN}}{\\mathrm{FN}+\\mathrm{TP}}.\\]\n' +
      '\n' +
      '문장 수준 평가에서 FPR은 평가자가 요약 문장이 실제로 정확할 때 오류가 포함되어 있다고 잘못 예측하는 비율을 나타내며, FP는 거짓 긍정(사실적으로 일관성이 없는 것으로 잘못 라벨링된 문장)의 수를 나타내고 TN은 참 부정(사실적으로 일관성이 있는 문장처럼 정확하게 식별된 문장)을 나타낸다. 높은 FPR은 평가자가 요약 문장이 사실적으로 일관될 때 종종 잘못된 것으로 플래깅하고 있음을 시사한다.\n' +
      '\n' +
      'FNR은 평가자가 요약 문장이 실제로 오류를 포함할 때 정확하다고 잘못 예측하는 비율을 나타내며, 여기서 FN은 거짓 부정(사실적으로 일관되지 않은 요약 문장은 정확하다고 잘못 레이블링됨)을 나타내고 TP는 참 긍정(사실적으로 일관되지 않은 요약 문장은 정확하다고 정확하게 식별됨)을 나타낸다. 높은 FNR은 평가자가 요약 문장의 오류를 간과하는 경우가 많다는 것을 의미한다.\n' +
      '\n' +
      'FPR 및 FNR은 표 3에서 균형 정확도(BAcc)에 의해 캡처된 평가자의 성능의 보다 상세한 분해를 제공한다는 것을 언급할 가치가 있다:\n' +
      '\n' +
      '\\[\\mathrm{BAcc}=1-\\frac{1}{2}(\\mathrm{FPR}+\\mathrm{FNR}).\\]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\multirow{2}{*}{**Summ.**} & \\multicolumn{3}{c}{**MediaSum**} & \\multicolumn{3}{c}{**MeetingBank**} \\\\ \\cline{2-9}  & **Len** & **Rel** & **Cmp** & **Err** & **Len** & **Rel** & **Cmp** & **Err** \\\\  & **[0,1]** & **[0,1]** & **\\%** & **Len** & **[0,1]** & **[0,1]** & **\\%** \\\\ \\hline Vicuna-7B & 65 & 0.89 & 0.64 & 47.3 & 72 & 0.81 & 0.72 & 43.6 \\\\ Wizard-7B & 44 & 0.84 & 0.53 & 51.4 & 51 & 0.76 & 0.61 & 41.0 \\\\ Wizard-13B & 70 & 0.87 & 0.69 & 38.9 & 73 & 0.88 & 0.75 & 43.6 \\\\ Wizard-30B & 69 & 0.91 & 0.72 & 40.3 & 66 & 0.88 & 0.75 & 37.2 \\\\ GPT-3.5-Tb & 57 & 0.91 & 0.70 & 24.0 & 53 & 0.91 & 0.74 & 14.7 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 유튜브의 **요약 모델 통계량. 평가 중인 각 모델에 대해 인간 평가 완전성 점수(_Cmp_), 관련성 점수(_Rel_) 및 각 데이터 세트에 대해 적어도 하나의 사실적 불일치를 갖는 요약의 백분율(_Err_ %)을 포함한다. Wizard_는 WizardLM.**Significant Test 강조된 성능은 이 작업의 모든 테이블에 걸쳐 쌍을 이루는 부트스트랩 테스트에 의해 p-값 < 0.05로 나머지보다 훨씬 우수하다.\n' +
      '\n' +
      'LLM 기반 평가자는 종종 오류를 간과하는 반면, 비LLM 기반 사실 일관성 메트릭은 잘못된 경보를 생성하는 경향이 있다. 섹션 5.2에 자세히 설명된 접근법에 따라 표 7 및 8에서 상대적으로 강한 평가자의 하위 집합에 대한 결과를 보여준다. 비LLM 기반 메트릭은 높은 FPR에서 중요한 문제를 나타낸다. 이러한 메트릭들이 잠재적인 에러를 시그널링할 때, 그 정확성을 검증하기 위해 요약 문장과 소스 문서 사이의 수동 비교가 필요하다. 이 과정은 상당한 양의 불필요한 노력을 초래한다. 반면에 LLM 기반 평가자는 더 높은 FNR을 나타낸다. 이것은 즉각적인 작업량을 줄일 수 있지만, 일관성 없는 문장을 놓칠 위험을 도입하며, 이는 장기적으로 해로울 수 있다.\n' +
      '\n' +
      '모든 평가 모델은 한계 주제 요약 문장에 대한 오류를 더 적게 놓치지만, LLM 기반 평가자는 한계 주제 요약을 평가할 때 더 많은 잘못된 경보를 가져온다.표 8에서 보는 바와 같이 모든 모델은 한계 주제로부터 요약 또는 요약 문장을 평가할 때 FNR이 감소한다. 특히, 이러한 경향은 LLM 기반 평가자에게 더 두드러진다. 예를 들어, LLM들은 문장-레벨 FNR에서 상당한 20% 내지 40% 감소를 보여주며, 이는 그들의 개선된 에러 검출 능력을 나타낸다. 그러나 LLM은 더 많은 요약 문장을 사실적으로 일관되지 않은 것으로 식별하여 이를 달성하는 것으로 보이며, 이는 한계 주제에 대한 더 높은 FPR로 이어진다(표 7). 이는 GPT-3.5-터보의 경우에 특히 두드러진다.\n' +
      '\n' +
      '## 부록 F 유튜브Val 주석 지침\n' +
      '\n' +
      'TofuEval 벤치마크에 대한 인간 평가 작업을 작업량으로 인해 두 가지 작업으로 분리했다. 각 작업은 다른 주석자 그룹에 의해 주석이 달렸다. 2개의 작업(2개의 데이터 세트\\(\\times\\) 50개의 문서\\(\\times\\) 3개의 주제)에 대해 각각 300개의 주석이 있다. 첫 번째 과제(과제 1)는 주제 범주화, 사실적 일관성 평가, 관련성 평가로 구성하였다. 두 번째 주석 작업(작업 2)에는 완전성 평가가 포함되었다.\n' +
      '\n' +
      '### Task 1\n' +
      '\n' +
      'Topic Categorization은 \\(T_{\\mathrm{topic}}\\) (대화 문서, 주제) \\(\\rightarrow\\) {main, marginal, irrelevant}로 정의된다. 범주를 다음과 같이 정의했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c|c c c} \\hline \\hline \\multirow{2}{*}{**Model Type**} & \\multirow{2}{*}{**Evaluation Model**} & \\multicolumn{4}{c}{**Sentence-Level (FPR \\(\\downarrow\\))**} & \\multicolumn{4}{c}{**Summary-Level (FPR \\(\\downarrow\\))**} \\\\ \\cline{3-10}  & & \\multicolumn{2}{c}{**MediaSum**} & \\multicolumn{2}{c}{**MeetingBank**} & \\multicolumn{2}{c}{**MediaSum**} & \\multicolumn{2}{c}{**MeetingBank**} \\\\ \\cline{3-10}  & & **Main** & **Marginal** & **Main** & **Marginal** & **Main** & **Marginal** \\\\ \\hline \\multirow{3}{*}{**Non- LLM**} & SummaC-ZS & 26.1 & 26.0 & 26.9 & 20.2 & 51.9 & 51.8 & 35.6 & 38.1 \\\\  & SummaC-CV & 47.2 & 40.2 & 27.9 & 30.6 & 53.5 & 46.9 & 24.3 & 21.7 \\\\  & QFarcEtEval & 33.2 & 22.7 & 30.1 & 29.2 & 35.4 & 22.8 & 45.9 & 45.1 \\\\  & AlignScore & 23.9 & 26.0 & 14.9 & 25.4 & 41.6 & 35.8 & 36.6 & 47.1 \\\\ \\hline\n' +
      '**Prop.** & GPT-3.5-Turbo & 3.7 & 14.6 & 13.4 & 48.4 & 11.2 & 26.2 & 25.3 & 43.7\\\\\n' +
      '**LLM** & GPT-4 & 1.4 & 5.7 & 3.5 & 6.7 & 3.5 & 12.5 & 4.5 & 7.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: TofuEval의 테스트 세트에 대한 사실적 일관성 평가자들에 대한 **문장-레벨 및 요약-레벨 위음수율(FNR) (모델이 요약 또는 요약 문장에 오류가 포함됨을 잘못 예측함).**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c|c c c} \\hline \\hline \\multirow{2}{*}{**Model Type**} & \\multirow{2}{*}{**Evaluation Model**} & \\multicolumn{4}{c}{**Sentence-Level (FNR \\(\\downarrow\\))**} & \\multicolumn{4}{c}{**Summary-Level (FNR \\(\\downarrow\\))**} \\\\ \\cline{3-10}  & & & **MediaSum** & \\multicolumn{2}{c}{**MeetingBank**} & \\multicolumn{2}{c}{**MediaSum**} & \\multicolumn{2}{c}{**MeetingBank**} \\\\ \\cline{3-10}  & & **Main** & **Marginal** & **Main** & **Marginal** & **Main** & **Marginal** \\\\ \\hline \\multirow{3}{*}{**Non- LLM**} & SummaC-ZS & 41.8 & 26.4 & 45.4 & 18.7 & 22.8 & 20.1 & 48.1 & 17.3 \\\\  & SummaC-CV & 17.8 & 14.0 & 47.1 & 14.9 & 24.2 & 20.1 & 71.0 & 32.6 \\\\  & QFarcEtEval & 59.0 & 29.5 & 54.1 & 19.4 & 41.9 & 28.8 & 43.9 & 18.7 \\\\  & AlignScore & 37.9 & 21.6 & 62.7 & 17.5 & 27.4 & 20.1 & 36.6 & 17.5 \\\\ \\hline\n' +
      '**Prop.** & GPT-3.5-Turbo & 73.0 & 47.7 & 48.4 & 21.6 & 69.6 & 42.2 & 48.4 & 24.9 \\\\  & GPT-4 & 68.8 & 33.9 & 46.1 & 12.6 & 69.0 & 29.7 & 46.1 & 26.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: TofuEval(모델이 요약 또는 요약 문장이 정확하다고 부정확하게 예측함)의 테스트 세트에 대한 사실적 일관성 평가자에 대한 **문장-레벨 및 요약-레벨 거짓 긍정률(FPR)**_Main Topic_는 논의 중이거나 문서 내에 제시되는 문서 내의 중심 정보를 지칭한다. 주요 주제는 종종 문서가 주로 무엇에 관한 것인지이며, 문서를 이해하는 것은 문서의 전반적인 개념을 이해하는 데 중요하다.\n' +
      '\n' +
      '_Marginal Topic_는 문서의 주 초점은 아니지만 여전히 문맥의 일부인 문서 내의 정보를 지칭한다. 이러한 주제는 일반적으로 주요 주제보다 덜 두드러지거나 덜 광범위하게 탐구된다. 전반적인 맥락에 기여하거나 추가 정보를 제공하거나 주요 주제에 대한 이해도를 높일 수 있지만 주요 초점은 아니다.\n' +
      '\n' +
      '_Irrelevant Topic_는 문서의 주제나 목적과 직접적으로 관련이 없는 문서 내의 정보를 의미한다. 이러한 주제는 문서의 주요 주제 또는 목적에 기여하지 않을 수 있으며 당면한 주요 주제로부터의 전환 또는 산만함으로 볼 수 있다.\n' +
      '\n' +
      '데이터 수집 후 한계 및 관련 없는 주제를 함께 병합한 주제 범주의 사후 처리에 대한 정보는 섹션 F.4를 참조하십시오.\n' +
      '\n' +
      '사실적 일치성 평가는 \\(T_{\\text{fact}}\\)(대화 문서, 문장)\\(\\rightarrow\\){consistent, inconsistent}로 정의되며, 여기서 사실적으로 일치하는 문장은 문서에 의해 수반된 문장이다.\n' +
      '\n' +
      '우리는 문장 수준에서 이 작업을 수행했으며 주석자는 전체 요약에 액세스할 수 있습니다. 어떤 문장이 사실적으로 일관성이 없는 것으로 레이블이 지정되면 주석자는 자연 텍스트로 자신의 추론을 설명하도록 요청받는다.\n' +
      '\n' +
      '관련성 평가는 \\(T_{\\text{rel}}\\) (대화 문서, 주제, 요약) \\(\\rightarrow\\) {1, 2, 3, 4, 5}로 정의된다. 1-5 Likert 척도를 다음과 같이 정의하였다.\n' +
      '\n' +
      '5-우수: 요약은 비-주제 관련 콘텐츠를 포함하지 않는다; 4-매우 양호: 요약은 소량의 비-주제 관련 콘텐츠를 포함한다; 3-우수: 요약의 절반이 주제 외이다. 내용은 주제 관련 내용과 비주제 관련 내용 간에 다소 균형을 이루고 있다; 2-Fair: 요약의 절반 이상이 주제 외이지만 여전히 주제 관련 내용이 있다; 1-Poor: 요약이 주제 외 관련 내용으로 구성되어 있다.\n' +
      '\n' +
      '주석 일치를 개선하기 위해 점수를 병합하는 방법에 대한 설명은 섹션 F.4를 참조하십시오. 과제 1에 대한 주석 지침과 인터페이스는 그림 6과 7에서 확인할 수 있다.\n' +
      '\n' +
      '수집된 주석의 품질을 보장하기 위해 두 개의 개별 주석자에 의해 주석 작업의 하위 집합이 완료되었다. 이 두 통과 주석의 피드백을 사용하여 주석 지침의 모호성 또는 문제를 식별하고 필요한 수정을 수행했다. 주석 간의 합의를 강화한 방법에 대한 자세한 내용은 섹션 F.3을 참조하십시오.\n' +
      '\n' +
      '### Task 2\n' +
      '\n' +
      'Pyramid 방법에 의해 영감을 받은 완전성 평가 [11], 우리는 먼저 주석자에게 \\(n\\) 키 포인트를 적어달라고 요청했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c l c c c c} \\hline \\hline \\multirow{2}{*}{**Model Type**} & \\multirow{2}{*}{**Evaluation**} & \\multicolumn{4}{c}{**Summary-Level (BAcc \\(\\uparrow\\))**} \\\\ \\cline{3-6}  & & \\multicolumn{2}{c}{**MediaSum**} & \\multicolumn{2}{c}{**MeetingBank**} \\\\ \\cline{3-6}  & & **Main** & **Margin** & **Main** & **Margin** \\\\ \\hline - & Baseline & 50.0 & 50.0 & 50.0 & 50.0 \\\\ \\hline \\multirow{3}{*}{**Open Source LLM**} & Vicuna-13B & 51.7 & 49.2 & 49.9 & 49.6 \\\\  & Vicuna-33B & 54.4 & 56.0 & 54.6 & 54.0 \\\\  & Wizard-13B & 56.8 & 57.1 & 56.2 & 60.1 \\\\  & Wizard-30B & 56.0 & 57.7 & 54.9 & 56.1 \\\\ \\hline \\multirow{3}{*}{**Prop. FLM**} & GPT-3.5-Turbo & 60.1 & 64.3 & 61.9 & 61.5 \\\\  & GPT-4 & 64.2 & 78.7 & 75.9 & 83.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: Exp_setting_에 대한 YouTubeval의 테스트 세트 상의 **요약-레벨 BAcc, 여기서 우리는 모델에게 이진판단을 제공하는 것 이외에 그 결정에 대한 설명을 제공하도록 요청한다(섹션 5.1). 요약-레벨 라벨들은 문장-레벨 라벨들을 집계함으로써 획득된다. 비LLM은 설명을 제공하지 않습니다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c l c c c c} \\hline \\hline \\multirow{2}{*}{**Model Type**} & \\multirow{2}{*}{**Evaluation**} & \\multicolumn{4}{c}{**Summary-Level (BAcc \\(\\uparrow\\))**} \\\\ \\cline{3-6}  & & \\multicolumn{2}{c}{**MediaSum**} & \\multicolumn{2}{c}{**MeetingBank**} \\\\ \\cline{3-6}  & & **Main** & **Margin** & **Main** & **Margin** \\\\ \\hline - & Baseline & 50.0 & 50.0 & 50.0 & 50.0 \\\\ \\hline \\multirow{3}{*}{**Non- LLM**} & SummaC-ZS & 62.7 & 64.1 & 58.1 & 72.4 \\\\  & SummaC-CV & 61.2 & 66.5 & 52.4 & 72.9 \\\\  & QFAcetEval & 61.4 & 74.2 & 55.1 & 68.2 \\\\  & AlignScore & 65.5 & 72.1 & 63.4 & 71.8 \\\\ \\hline \\multirow{3}{*}{**Open Source LLM**} & Vicuna-13B & 49.8 & 52.3 & 48.6 & 53.1 \\\\  & Vicuna-33B & 50.0 & 50.5 & 50.8 & 47.6 \\\\  & Wizard-13B & 52.0 & 52.3 & 47.3 & 51.9 \\\\  & Wizard-30B & 50.0 & 51.8 & 50.0 & 51.2 \\\\ \\hline \\multirow{3}{*}{**Prop. FLM**} & GPT-3.5-Turbo & 55.0 & 71.2 & 51.5 & 59.9 \\\\  & GPT-4 & 58.2 & 68.5 & 56.6 & 80.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: **Summary-level BAcc on the test set of YouTubeval by**_directly evaluating on whole summaries_. 요약의 사실적 일관성을 직접적으로 예측하는 것은 모든 모델에 대한 문장 수준의 사실성 예측 결과를 집계하는 것보다 더 나쁘다(표 3). 문법적 문장 중 \\(T_{\\text{key}}\\)(대화 문서, 주제) \\(\\to K\\), 여기서 \\(K=\\{k_{1},...,k_{n}\\}\\}). 이러한 핵심 사항은 주석이 주어진 주제를 다루는 이상적인 요약이 포함되어야 한다고 생각하는 것으로 여겨졌다. 각 키포인트 \\(k_{i}\\)에 대해 주석이 주어진 요약에 \\(k_{i}\\), _i.e._, \\(\\mathds{1}\\)(summary, \\(k_{i}\\))이 포함되어 있는지 물었다. 완전성 주석이 완성된 후, 우리는 다음과 같은 방법으로 완전성 점수를 계산했다. \\(\\frac{1}{n}\\sum_{i=1}^{n}\\mathds{1}\\)(summary, \\(k_{i}\\))\n' +
      '\n' +
      '여러 주석자의 핵심 포인트를 통합하는 것은 불가능하기 때문에 각 작업에 대해 하나의 주석자만 활용하고 재현성을 위해 작성된 핵심 포인트를 제공했다. 과제 2에 대한 주석 지침과 인터페이스는 그림 8과 9에서 확인할 수 있다.\n' +
      '\n' +
      '우리는 1-5 리커트 척도를 사용하여 완전성을 주석을 달면 결과의 품질이 감소한다는 것을 발견했다는 점에 주목하고 싶다. 주석이 대화의 중요한 핵심 사항을 명시적으로 작성하도록 요청하지 않고, 우리는 요약이 주석자에게 제시되는 순서가 이상적인 요약에 포함시키는 것이 중요하다고 간주되는 것에 눈에 띄는 영향을 미친다는 것을 발견했다. 이 효과는 최근 읽은 요약에 따라 요약에 있어야 하는 것에 대한 주석자의 인식이 바뀔 수 있기 때문에 발생했다. 그 결과, 주석은 이미 평가된 요약에 대해 주석을 수정해야 했으며, 이미 평가가 완료된 평가를 진화하는 의견과 정렬하기 위해 앞뒤로 전환해야 했다. 이러한 상황은 오류가 증가하고 이전 응답을 수정하는 것을 꺼리거나 거부하여 궁극적으로 신뢰할 수 없는 주석을 초래할 수 있다. 따라서 우리는 우리의 접근법을 채택하여 요약의 완성도를 평가하기 위한 향후 작업을 권장한다.\n' +
      '\n' +
      '### Quality Control\n' +
      '\n' +
      '연습 세션 정렬 주석 지침을 개발하고 선택된 전문 언어 데이터 주석자 그룹의 도움으로 주석 작업에 대한 연습 세션을 배열했다. 연습 세션을 준비하는 동안 주석자가 주석 중에 고려하기를 원하는 예방 조치를 강조하면서 앞서 언급한 차원에 대한 지침과 답변을 개선했다. 우리는 이러한 답변에 대한 합의에 도달하기 위해 그룹과 토론에 참여했습니다. 최종 결정된 지침을 사용하여 모든 전문 언어 데이터 주석자가 연습 세션에 참여하여 작업에 익숙해지고 평가를 보정했다.\n' +
      '\n' +
      '연습 세션 후 맞춤형 피드백을 제공하기 위해 여러 라운드의 파일럿 주석을 개최했다. 각 라운드에서 모든 주석 작업은 두 명의 주석자가 수행하므로 주석자 간 합의를 계산하여 합의율을 유지하거나 개선할 수 있다. 각 라운드 후에 주석을 비교하고 각 주석자에게 개별화된 피드백을 제공하고 주석 지침을 개선했다. 융합 주석 일치율을 달성하면 나머지 주석 작업을 진행했다.\n' +
      '\n' +
      '주석 작업 두 주석 작업에 대한 주석당 평균 소요 시간은 각각 36분과 24분이었다. 현재 작업에서 제시된 인간 평가 과제에 관련된 주석자는 모두 영어에 대한 원어민 수준의 숙련도를 가지고 있으며 거주 국가의 유사한 역할에 대해 벤치마킹되는 경쟁적인 시간당 비율을 보상받는다.\n' +
      '\n' +
      '### Inter-Annotator Agreement\n' +
      '\n' +
      '우리는 Cohen의 Kappa\\(\\kappa\\)[10]을 사용하여 두 개의 주석 패스를 받은 작업에 대한 인간 주석 일치도를 측정한다(과제 1). 아래에서는 작업 1의 각 평가 차원에 대한 \\(\\kappa\\)과 주석 일치도를 개선하기 위해 수행한 후처리 단계를 제공한다.\n' +
      '\n' +
      'Topic Categorization과 MappingAgreement는 두 데이터 세트 모두에서 중간 수준이었다 (MediaSum, \\(\\kappa\\) = 0.47; MeetingBank, \\(\\kappa\\) = 0.53). 벤치마크에서 2%의 주제만이 무관한 것으로 주석이 달렸고 주석이 이러한 주제에서 "한계"와 "비관련"으로 자주 레이블을 지정하는 것을 관찰했기 때문에 주석이 완료된 후 {한계, 무관} \\(\\rightarrow\\) {한계}를 병합했다.\n' +
      '\n' +
      'Factual Consistency는 MediaSum 요약과 MeetingBank 요약에 대한 평가에서 각각 \\(\\kappa\\) = 0.42와 0.34를 달성하여 공정하거나 중간 정도의 일치성을 보였다. 인간이 작성한 설명을 기반으로 주석이 문장을 사실적으로 일관되지 않은 것으로 잘못 표시하거나 요약이 불완전한 경우, 요약 문장이 주제를 직접 다루지 않은 경우 또는 이 평가 차원에서 고려할 수 없는 다른 상황이 발생했을 때 주요 불일치가 발생했음을 발견했다.\n' +
      '\n' +
      '이러한 발견을 감안할 때 모든 주석을 수집한 후 현재 작업의 저자는 서면 설명을 기반으로 벤치마크 데이터 세트에서 잘못된 긍정 라벨을 제거하기 위해 _2단계 주석 검토를 수행했다. 요약 모델은 무의식적인 편견을 방지하기 위해 이 검토 과정에서 숨겨졌다. 2단계 검토 과정의 신뢰성을 강조하고 싶습니다. 주석이 제공한 설명에 대한 신중한 고려와 함께 사실적 일관성을 위해 우리의 정의를 사용하여 수행되었다. 검토자는 정렬을 보장하기 위해 어려운 사례에 대해 논의했으며 모든 결정은 다른 검토자에 의해 검토되었다. 이 프로세스가 데이터 세트의 품질을 상당히 향상시켰다고 믿습니다.\n' +
      '\n' +
      '설명이 도출된 사실적 일관성 평가와 달리, 우리는 몇 번의 표적 피드백 후에도 관련성 평가에 대한 높은 합의를 달성하기 어렵다는 것을 발견했다. 우리는 궁극적으로 점수들을 \\(\\{1,2\\}\\to 0\\)과 \\(\\{3,4,5\\}\\to 1\\)으로 나누기로 결정했다. 이 그룹화를 통해 \\(\\kappa\\) = MediaSum 요약의 경우 0.25, MeetingBank 요약의 경우 0.37로 공정한 합의라고 간주한다. 주변 주제에 대해서는 주제 관련 내용이 존재하지 않을 수 있기 때문에 본 주제 요약에 대한 관련성을 평가하였다.\n' +
      '\n' +
      '설명서를 작성하는 것은 피드백을 제공하고 주석 합의를 개선하는 데 도움이 되며, 주석자가 추가 교육을 받은 후 이 세 가지 차원에 대한 \\(\\kappa\\)의 값이 0.1에서 0.3으로 증가하는 것을 관찰했다. 특히, 주석 일치도를 향상시키는 것은 사실적 일관성의 차원에서 비교적 간단하다는 것을 발견했으며, 주석자가 작성한 설명을 기반으로 주석자의 추론을 쉽게 파악할 수 있어 주석의 추론 또는 주석 지침에서 단점을 식별할 수 있다. 이를 통해 목표 피드백을 제공하고 지침에 설명을 통합하여 궁극적으로 주석 합의를 개선할 수 있었다. 주제 분류 및 관련성 평가를 위해 파일럿 라운드가 완료되면 주석자에게 상당한 불일치가 관찰된 예에 대한 생각과 불확실성을 공유하도록 요청했다. 그런 다음 피드백을 기반으로 주석 지침을 개선했습니다.\n' +
      '\n' +
      '## 부록 G 오류 유형 큐레이션 및 주석\n' +
      '\n' +
      '### 오류 유형 큐레이션\n' +
      '\n' +
      '두화발 대화의 복잡성으로 인해, 우리는 사실적으로 일치하지 않는 문장에 대해 주석자가 작성한 설명을 기반으로 전문 언어 데이터 주석자(현재 작업의 모든 공동 저자)와 다음과 같은 오류 유형을 선별했다. 우리의 오류 분류는 처음에 짧은 대화를 위해 제안된 Tang et al.(2022)의 오류 분류에 의해 크게 영향을 받는다. 예시된 예와 함께 오류 분류법에 대한 간략한 설명이 그림 5에 나와 있다.\n' +
      '\n' +
      '외재적 정보 오류 Maynez et al.(2020)과 유사하게, 우리는 다음과 같이 오류를 정의한다: 요약 문장은 소스 문서로부터 나오지 않고 소스 문서로부터 검증될 수 없는 새로운 정보를 포함한다. 만약 새로운 정보가 원본 문서에서 나온 것이 아니라 일상적인 상식적인 지식이라면, 우리는 문장을 오류가 포함된 것으로 표시하지 않는다.\n' +
      '\n' +
      '오참조 요약 문장은 소스 문서에 접지되는 엔티티(예를 들어, 주제/객체 위치의 명사구로서)를 지칭하지만, 문장은 이 엔티티에 속성 또는 이벤트를 속성화하고; 이 잘못된 속성 또는 이벤트는 소스 문서에 접지되지만 소스 문서 내의 상이한 엔티티에 귀속된다.\n' +
      '\n' +
      '그림 4: 주요/한계 주제에 대한 각 요약자에 대한 모든 요약 문장에 대한 오류 분포.\n' +
      '\n' +
      '4.1.2 Stating Opinion-as-Fact Error\n' +
      '\n' +
      '요약 문장은 소스 문서에서 명제가 누군가의 의견으로 제시될 때 명제를 사실로 제시(즉, 문장 내에 _might_ 또는 _probably_와 같은 불확실성 모달이나 부사가 없음)한다.\n' +
      '\n' +
      '추론 오류 요약 문장은 소스 문서에 _are_ 접지된 전제 또는 증거 조각을 기반으로 잘못된 추론(예: 산술 계산에 오류가 포함됨, A와 B 사이의 잘못된 관계를 그리거나 잘못된 외삽을 수행함)을 한다. (추론에 대한 증거가 원본 문서에 근거하지 않으면 외부 정보 오류로 간주됩니다.\n' +
      '\n' +
      'Tense/Aspect/Modality Error 요약문은 잘못된 시제(예를 들어, 소스 문서에서는 과거 시제이지만 요약 문장에서는 미래 시제), 양상(예를 들어, 소스 문서에서는 진보적이지만 요약 문장에서는 완벽함), 또는 모달리티(예를 들어, 소스 문서에서는 인식 가능성 모달 _might_이지만 요약 문장에서는 인식 필요성 모달 _must_)를 사용한다.\n' +
      '\n' +
      '모순 오류 요약 문장은 부정의 잘못된 존재 또는 부재로 인해 또는 소스 문서에서 사용되는 단어의 반의어 사용으로 인해 소스 문서와 완전히 모순된다.\n' +
      '\n' +
      '뉘앙스 의미 이동 오류 요약 문장은 상이한 감각들과 연관된 단어들 또는 구절들과 패러프레이징을 사용함으로써 소스 문서 내의 특정 문장들의 의미를 변경한다(_예를 들어, "요청을 한다"를 패러프레이징한다).\n' +
      '\n' +
      '기타 주석 프로세스에서 우리는 의도적으로 _other_ 오류 범주를 도입했다. 그러나 이 범주 내에서 사실적으로 일치하지 않는 문장이 발견되지 않았기 때문에 오류 분류법에 포함하지 않기로 선택했다.\n' +
      '\n' +
      '### 오류 유형 주석\n' +
      '\n' +
      '큐레이트된 오류 분류법을 사용하여 저자 목록의 전문 언어 데이터 주석자는 주석자가 제공한 요약 및 설명을 참조하여 두부의 모든 사실적으로 일치하지 않는 문장에 하나 이상의 오류 유형을 할당했다. 인간의 설명을 기반으로 오류 유형을 식별할 수 없는 경우 소스 문서를 참조했다.\n' +
      '\n' +
      '오류 유형 할당 과제에 대해 4차에 걸친 파일럿 연구를 수행했다. 처음 두 라운드는 Tang 등(2022)에서 수정한 오류 분류법을 확정하는 데 사용되었으며, 다음 두 라운드는 오류 유형 분류를 보정하고 합의율을 개선하는 데 전념했다(해결되지 않은 불일치의 경우 최종 오류 범주 또는 범주에 도달하기 위해 다수결을 취했다). 각 파일럿 라운드에 대해 주석자는 동일한 문장 세트에 오류 유형을 할당했다. 최종 시범 라운드 후 Fleiss의 Kappa score of Fleiss (1971) 0.78을 달성하여 실질적인 합의를 보였고, 우리는 계속했다.\n' +
      '\n' +
      '그림 5: **오류 분류 및 정의. 우리는 사실적으로 일관되지 않은 요약 문장의 예들과 TofuEval의 그에 상응하는 인간 주석이 달린 설명을 포함한다. 오류 범위는 강조 표시됩니다.** 나머지 오류 유형 분류와 함께 강조 표시됩니다.\n' +
      '\n' +
      '## 부록 H 설명 생성 성능\n' +
      '\n' +
      'LLM은 모델 출력에 대한 비판을 생성할 수 있으며, 경우에 따라 출력 품질이 향상된다(Madaan et al., 2023; Saunders et al., 2022). 우리는 이제 LLM이 사실적으로 일치하지 않는 문장에 대한 올바른 설명을 생성할 수 있는지 조사한다.21 특히 인간과 LLM이 모두 요약 문장을 사실적으로 일치하지 않는 것으로 라벨링한 예에 초점을 맞추고 LLM이 이러한 경우에 사실적 불일치에 대한 올바른 설명을 생성할 수 있는지 조사한다.\n' +
      '\n' +
      '각주 21: 우리는 테스트된 모든 LLM이 고립된 요약 문장이 아닌 전체 요약을 고려하도록 프롬프트될 때 이진 사실 오류 검출에서 더 나쁜 성능을 수행하는 것을 관찰했기 때문에 문장 수준에서 LLM 생성 설명의 품질만 평가했다.\n' +
      '\n' +
      '인간 평가 우리는 소규모 인간 평가 과제를 수행하기 위해 20개의 요약 문장을 무작위로 샘플링했다. 문장은 8개의 LLM 기반 평가자 중 하나와 인간 주석자가 문장을 사실적으로 일치하지 않는 것으로 라벨링한 경우에만 선택되었다. 주제 요약 문장이 더 다양한 범위의 오류 유형을 포함하고 있기 때문에 주제 요약 문장에서만 샘플링했다(그림 2). 이 작업의 3명의 저자는 모델이 생성한 설명의 품질을 사람이 작성한 설명과 비교하여 수동으로 평가했다. 구체적으로, 인간은 (1) 전체 요약, (2) 요약에서 사실적으로 일치하지 않는 문장, (3) 문장에 대해 인간이 작성한 설명, (4) 모델의 설명을 제공받았다. 우리는 주석이 모델 제공 설명이 사람이 작성한 설명에 의해 지원되는지 또는 그에 상응하는지 여부를 결정하는 이진 분류 작업을 수행하도록 요청했다. 추가 컨텍스트가 필요할 경우 소스 문서를 주석자에게 선택적으로 제공했습니다. 이러한 설명을 생성한 모델은 주석자로부터 숨겨졌다.\n' +
      '\n' +
      '사실적으로 일관성이 없는 요약 문장이 의미적으로는 다르지만 타당한 설명을 할 수 있을 것이다. 설명이 주석자가 제공한 설명과 유사하지 않기 때문에 모델이 잘못된 것으로 간주되는 합리적인 설명을 제공하는 경우 이는 수동 분석의 품질에 잠재적으로 영향을 미칠 수 있다. 이것의 잠재적 영향을 정량화하기 위해 두 차례의 주석을 완료한 작업에 대해(자세한 내용은 섹션 3.4 참조) 두 주석자가 두 문장 모두 사실적으로 일치하지 않는 것으로 식별했을 때 생성된 설명을 비교했다. 이 조사는 두 주석자가 작성한 설명이 95%에 걸쳐 의미적으로 동일하다는 것을 밝혀냈으며, 이는 **대체 모델 생성 설명이 두푸에벌**에서 큰 관심을 필요로 하지 않아야 함을 시사한다.\n' +
      '\n' +
      '우리는 Fleiss Kappa 점수(Fleiss, 1971) 0.65를 얻었으며, 이는 상당한 동의를 나타낸다. 각 모델 생성 설명(인간 설명이 지원되거나 지원되지 않음)에 대한 최종 레이블을 얻기 위해 다수 표를 얻었고 최종 레이블을 사용하여 각 LLM 기반 평가자에 대한 설명 정확도를 계산했다. 결과는 표 11에 제공되며, GPT-4는 요약 문장이 소스 문서와 사실적으로 불일치함을 식별하는 시간의 80%를 올바른 설명을 제공할 수 있음을 관찰할 수 있다. 다른 모델은 모델 간의 큰 차이 없이 약 절반의 정확한 설명을 제공한다.\n' +
      '\n' +
      '## 부록 I 컴퓨팅 인프라\n' +
      '\n' +
      '독점 모델에 대한 추론을 위해 공식 API를 사용했다. 7B 및 13B 매개변수가 있는 LLM의 경우 각각 16GB 메모리가 있는 4개의 테슬라 V100-SXM2 GPU 군집을 활용했다. 더 큰 30B 및 33B LLM의 경우 각각 40GB의 메모리가 있는 4개의 NVIDIA A100-SXM4 GPU를 사용했다. 비LLM 기반 모델의 경우 단일 Tesla V100-SXM2 GPU를 사용했다.\n' +
      '\n' +
      'API 호출의 경우 GPT-3.5-터보는 gpt-3.5-터보, GPT-4는 gpt-4를 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline \\multicolumn{1}{c}{**Open-Source LLM**} & \\multicolumn{2}{c}{**Prop. LLM**} \\\\ \\hline \\hline\n' +
      '**Model** & **Acc (\\%)** & **Model** & **Acc (\\%)** \\\\ \\hline Vicuna-13B & 45 & & \\\\ Vicuna-33B & 40 & & \\\\ WizardLM-13B & 60 & GPT-3.5-Turbo & 50 \\\\ WizardLM-30B & 55 & GPT-4 & **80** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: 두부에벌의 모델 전반에 걸친 올바른 설명의 **퍼센트. 우리는 20\\(\\times\\)6=120의 작은 표본집합에 대한 인간의 평가 결과를 보여준다. 여기서 인간의 주석과 모델 모두 요약이 주 주제 집합의 오류를 포함하는 것으로 예측하며, 더 다양한 오류 유형을 가지고 있다.**MediaSum - 문서 ID: CNN-25553```\n' +
      '\n' +
      'CAROL LIN, CNN ANCHOR:** 음, 정부는 또한 우리에게 항공사 승객들이 이미 알고 있는 것을 말하려고 합니다: 국가의 항공사들이 서비스 업그레이드를 약속했음에도 불구하고 서비스가 완벽하지 않다는 것을 말입니다. 4시간도 안 돼서 교통부의 조사관에게서 보고서가 왔어요 미네아폴리스의 여행 전문가 테리 트리플러와 함께합니다 테리, 혹시 최근에 비행을 한 적이 있나요? 적어도 예비 보고서에서 최고조에 달할 기회가 있었으니까요 우리는 무엇을 들을 것 같습니까?\n' +
      '\n' +
      'TERRY TRIPPLER, TRAVEL EXPERT:** 나는 우리가 보게 될 것이 작년 6월에 나온 중간 보고서와 유사한 것이라고 생각한다: 개선, 그러나 갈 길이 멀다. 그리고 나는 그것이 우리가 다시 일어날 것이라고 생각한다. 우리는 정오에 볼 것이다.\n' +
      '\n' +
      '항공사들이 노력하겠다고 한 약속들 중 적어도 몇 가지를 언급해 봅시다. 예를 들어, 티켓을 예약하면 항공사에서 가장 저렴한 박람회를 인용할 것이라고 보장합니까?\n' +
      '\n' +
      '어려운 일이네요, 캐롤 그들은 그것을 하기로 약속했다. 몇몇 항공사들은 그 약속을 꽤 잘 지키고 있다. 다른 것들은 너무 잘 안 되고 있어요. 기본적으로, 문제가 있는 곳은 그들이 가지고 있는 막바지 인터넷 요금에 있다 - 일부 승객들은 그들이 듣고 있지 않다고 주장한다. 그래서 그 분야에 약간의 개선이 필요하다.\n' +
      '\n' +
      '좋아 글쎄, 그들이 나에게 말해 줄 수 있을까? 아니면 내가 좌석을 예약할 때 항공편이 너무 많이 팔렸는지 말해줄까?\n' +
      '\n' +
      '제가 알기로는 항공편이 너무 많이 팔렸는지 알려주고 있어요 우리가 이 비행기에서 발견하는 것은, 일단 승객이 그 비행기를 초과 판매한 것을 발견하면, 그들은 자발적으로 부딪히고 마일리지와 돈, 식사 등을 얻기를 원하기 때문에 그 비행기로 예약한다는 것입니다. 그래서 그것은 그들에게 역효과를 낳았다\n' +
      '\n' +
      '좋아, 내 짐을 잃어버렸다고 치자 요새는 돌려받는데 얼마나 걸릴까요?\n' +
      '\n' +
      '24시간 안에 할 거래요 수하물 신고가 들어왔어요 그리고, 물론, 우리는 최근에 수하물 취급자들이 사람들의 짐, 그의 짐을 가지고 농구를 하고 있는 영화를 모두 보았다. 그건 도움이 안 됐어요 불평은 끝났다. 그들은 수하물을 더 잘 다루어야 할 것이다.\n' +
      '\n' +
      '좋아, 나도 비행기에 앉아서 활주로에서 택시를 탈 준비를 하는 것보다 더 자주 나 자신을 발견했는데 갑자기 모든 게 멈췄어 그리고 나는 그것이 항공 교통 통제나 승객으로서 나에게 정말 큰 의미가 없는 것에 대한 문제라고 들었다. 내가 들을 것인가, 아니면 내가 들을 것인가 - 내가 왜 특별히 지연되고 있는지 내가 들을 것인가?\n' +
      '\n' +
      '그들은 당신에게 말하고 있다고 말한다. 그리고 여기 큰 문제가 있다. 그리고 이것들은 내가 이메일에서 말 그대로 하루에 수백 건씩 받고 있는 불만 사항들이다. 사람들은 그들이 진실을 듣고 있지 않다고 느낀다. 그들은 비행기에 탑승하기 전에 그들이 지연될 가능성이 있다는 말을 듣지 못하고 있다. 내 말은, 사람들이 항공기에 탑승하고 있다 - 나는 한 대에 탑승했고, 밖으로 나가 활주로 끝에 앉았다. 나는 그 경찰서에서 투표할 자격이 있을 만큼 충분히 오래 있었다. 그들은 이것에 대해 더 나은 일을 해야 한다. 문 밖으로 나와 바닥에 엎드려\n' +
      '\n' +
      '만약 그들이 의회에 지키겠다고 한 약속을 제대로 이행하지 않는다면, 이는 의회가 항공사에 더 나은 서비스를 제공하도록 강제하기 위해 입법을 강행할 것인가 아니면 강행해야 하는가?\n' +
      '\n' +
      '캐롤, 내 생각엔 이 보고서가 발표되는 대로 사람들이 국회의사당 곳곳에 줄을 서서 승객 권리 법안을 제출할 겁니다 이미 우리는 부활한 1999년 법안들 중 한두 개를 가지고 있다. 그것들은 항공사들이 더 나은 일을 하겠다고 약속했을 때 제쳐둔 청구서였다. 네, 저는 불행히도 항공사들이 - 그리고 저는 자유 기업인 것 같습니다 - 하지만 불행히도, 저는 항공사들이 정부의 개입 없이 그들의 운영권을 박탈당했다고 생각합니다. 나는 그것이 불가피하다고 생각한다. 꼭 일어나야 해\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} MediaSum - Document ID: CNN-25553 \\\\ \\hline\n' +
      'CAROL LIN, CNN ANCHOR:** 음, 정부는 또한 우리에게 항공사 승객들이 이미 알고 있는 것을 말하려고 합니다: 국가의 항공사들이 서비스 업그레이드를 약속했음에도 불구하고 서비스가 완벽하지 않다는 것을 말입니다. 4시간 이내에 교통부의 조사관에게서 보고서가 나온다. 미네아폴리스의 여행 전문가 테리 트리플러와 함께합니다 테리, 나는 네가 최근에 비행을 한 적이 있는지 궁금하다. 왜냐하면 너는 적어도 예비 보고서에서 정점을 찍을 기회가 있었기 때문이다. 우리는 무엇을 들을 것 같습니까?\n' +
      '\n' +
      'TERRY TRIPPLER, TRAVEL EXPERT:** 나는 우리가 보게 될 것이 작년 6월에 나온 중간 보고서와 유사한 것이라고 생각한다: 개선, 그러나 갈 길이 멀다. 그리고 나는 그것이 우리가 다시 일어날 것이라고 생각한다. 우리는 이번 정오에 볼 것이다.\n' +
      '\n' +
      '음, 항공사들이 노력하겠다고 한 약속들 중 적어도 몇 가지를 언급해 봅시다. 예를 들어, 티켓을 예약하면 항공사에서 가장 저렴한 박람회를 인용할 것이라고 보장합니까?\n' +
      '\n' +
      '어려운 일이군, 캐롤 그들은 그것을 하기로 약속했다. 몇몇 항공사들은 그 약속을 꽤 잘 지키고 있다. 다른 것들은 너무 잘 되지 않고 있다. 기본적으로, 문제가 있는 곳은 그들이 가지고 있는 막바지 인터넷 요금에 있다 - 일부 승객들은 그들이 듣고 있지 않다고 주장한다. 그래서 그 분야에 약간의 개선이 필요하다.\n' +
      '\n' +
      '좋아 글쎄, 뭐? 그들이 나에게 말해줄 수 있을까? 아니면 내가 좌석을 예약할 때 항공편이 초과 판매되는지 알려줄까?\n' +
      '\n' +
      '제가 알기로는 항공편이 너무 많이 팔렸는지 알려주고 있어요 우리가 이 비행기에서 발견하는 것은, 일단 승객이 그 비행기를 초과 판매한 것을 발견하면, 그들은 자발적으로 부딪히고 마일리지와 돈, 식사 등을 얻기를 원하기 때문에 그 비행기로 예약한다는 것입니다. 그래서 그것은 그들에게 역효과를 낳았다\n' +
      '\n' +
      '좋아, 내 짐을 잃어버렸다고 치자 요새는 돌려받는데 얼마나 걸릴까요?\n' +
      '\n' +
      '24시간 안에 할 거래요 수하물 신고가 들어왔어요 그리고 물론, 우리는 최근에 수하물 취급자들이 사람들의 짐인 그의 짐을 가지고 농구를 하고 있는 영화를 모두 보았다. 그건 도움이 안 됐어요 불평은 끝났다. 그들은 수하물을 더 잘 다루어야 할 것이다.\n' +
      '\n' +
      '좋아, 나도 비행기에 앉아서 활주로에서 택시를 탈 준비를 하는 것보다 더 자주 나 자신을 발견했는데 갑자기 모든 게 멈췄어 그리고 나는 그것이 항공 교통 통제나 승객으로서 나에게 정말로 큰 의미가 없는 것에 대한 문제라고 들었다. 내가 들을 것인가, 아니면 내가 들을 것인가 - 내가 왜 특별히 지연되고 있는지 내가 들을 것인가?\n' +
      '\n' +
      '그들은 당신에게 말하고 있다고 말한다. 그리고 여기 큰 문제가 있다. 그리고 이것들은 내가 이메일에서 말 그대로 하루에 수백 건씩 받고 있는 불만 사항들이다. 사람들은 그들이 진실을 듣고 있지 않다고 느낀다. 그들은 비행기에 탑승하기 전에 그들이 지연될 가능성이 있다는 말을 듣지 못하고 있다. 내 말은, 사람들이 항공기에 탑승하고 있다 - 나는 한 대에 탑승했고, 밖으로 나가 활주로 끝에 앉았다. 나는 그 경찰서에서 투표할 자격이 있을 만큼 충분히 오래 있었다. 그들은 이것에 대해 더 나은 일을 해야 한다. 문 밖으로 나와 바닥에 엎드려\n' +
      '\n' +
      '만약 그들이 의회에 지키겠다고 한 약속을 제대로 이행하지 않는다면, 이는 의회가 항공사에 더 나은 서비스를 제공하도록 강제하기 위해 입법을 강행할 것인가 아니면 강행해야 하는가?\n' +
      '\n' +
      '캐롤, 내 생각에 이 보고서가 발표되자마자, 우리는 승객 권리 법안을 제출하기 위해 국회의사당 곳곳으로 사람들이 줄을 서게 될 것입니다. 이미 우리는 부활한 1999년 법안들 중 한두 개를 가지고 있다. 그것들은 항공사들이 더 나은 일을 하겠다고 약속했을 때 제쳐둔 청구서였다. 네, 저는 불행히도 항공사들이 - 그리고 저는 자유 기업인 것 같습니다 - 하지만 불행히도, 저는 항공사들이 정부의 개입 없이 그들의 운영권을 박탈당했다고 생각합니다. 나는 그것이 불가피하다고 생각한다. 꼭 일어나야 해\n' +
      '\n' +
      '어떤 항공사들은 날씨나 지금 노동력 문제처럼 너무 많은 요소들이 통제불능이라고 말할 것입니다. 델타의 조종사들은 금요일에 투표한 것을 오늘 발표할 것으로 예상된다: 그들이 실제로 파업에 돌입할지 여부.\n' +
      '\n' +
      '맞아요 북서쪽이야, 델타 올해가 끝나기 전에 우리는 미국과 유나이티드와 대화할 것이다. 그리고 나는 그곳의 항공사에 동의하지 않는다. 나는 그들이 통제할 수 있다고 믿는다. 날씨, 이해해요 노동자: 자, 항공사들, 정신차리자.\n' +
      '\n' +
      '그래, 미리 봐줘서 고마워 정말 떠나고 싶게 만드는군요, 그렇죠?\n' +
      '\n' +
      '감사합니다, 트리플러 씨\n' +
      '\n' +
      '감사합니다\n' +
      '\n' +
      '**주제:** 항공사의 서비스 개선 약속(주요)\n' +
      '\n' +
      '**요약: [1]** 항공사들이 서비스 개선을 약속했지만 완전히 전달되지 않았습니다. [2] 불만은 부정확한 요금 견적, 초과 예약, 수하물 반환 지연 및 지연 중 통신 부족에 계속 남아 있다. [3] 항공사들이 자율 규제권을 박탈했기 때문에 정부의 개입이 필요할 수 있다.\n' +
      '\n' +
      '**[2]**에 대한 설명: Re: 요금 견적, 불만 사항은 가격 정확성보다는 가격 투명성에 관한 것입니다.\n' +
      '\n' +
      '**Nuanced Meaning Shift**\n' +
      '\n' +
      '**[3]**에 대한 설명**: 항공사들이 자율 규제권을 "몰수"한 것은 사실이 아니라 트리플러의 의견이다. 의견을 사실로 표현하다.\n' +
      '\n' +
      '**주제:** 항공사의 서비스 품질 통제에 영향을 미치는 요인(주요)\n' +
      '\n' +
      '**요약: [1]** 정부는 항공사의 서비스 품질에 대한 보고서를 발표하기로 되어 있는데, 가장 저렴한 요금을 인용하고, 초과 판매된 항공편을 승객들에게 알리고, 수하물 불만을 처리하고, 지연에 대한 정확한 정보를 제공하는 등의 분야에서 개선이 필요하다. **[2]** 보고서는 항공사의 약속 불이행으로 여객권 입법과 정부의 개입으로 이어질 수 있다. [3] 일부 항공사들은 날씨나 노동 문제와 같은 요소들이 통제불능이라고 주장하지만 전문가들은 동의하지 않는다.\n' +
      '\n' +
      '**[3]**에 대한 설명**: 전문가들은 예를 들어 날씨가 항공사의 통제를 벗어난다는 것에 동의하지 않는다. 모순\n' +
      '\n' +
      '**주제:** 승객의 권리 및 법률(주요)\n' +
      '\n' +
      '**요약: [1]** 정부 보고서는 항공사들이 서비스 약속을 완전히 이행하지 않았다는 것을 보여줄 것이다. [2] 승객들은 부정확한 요금 견적, 지연/연착된 항공편 정보 부족, 잘못 처리된 수하물에 대해 불평한다. [3] 의회는 항공사들이 자율규제를 몰수함에 따라 여객권을 의무화하는 법안을 통과시킬 수 있다.\n' +
      '\n' +
      '**[1]**에 대한 설명**: 그 보고서가 항공사들이 서비스 약속에 대해 완전히 전달하지 않았다는 것을 실제로 보여줄지는 문서에 명시되어 있지 않다. 여행 전문가의 의견입니다. 의견을 사실로 표현하다.\n' +
      '\n' +
      '**[2]**에 대한 설명**: 부정확한 운임 견적에 대한 것이 아니라 가장 싼 견적을 얻지 못하는 것에 대한 것이다. 미묘한 의미 이동\n' +
      '\n' +
      '**[3]**에 대한 설명: "항공사들은 자율 규제를 박탈당했다"는 본문에 가정으로 제시되어 있는데, 요약은 그것을 사실처럼 보이게 한다. 의견을 사실로 표현하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline MediaSum - Document ID: CNN-25553 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: TofuEval(파트 2)에서 생성된 요약 및 인간 주석을 갖는 대화 예. 우리는 표에서 대화를 위해 생성된 세 가지 주제를 모두 보여준다. 각 주제에 대해 인간이 작성한 설명 및 사실적으로 일치하지 않는 문장에 대한 오류 유형을 사용하여 생성된 5개의 요약 중 하나를 보여준다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:23]\n' +
      '\n' +
      '도 7: (계속) (1) 사실적 일관성 평가를 위한 주석 인터페이스의 스크린샷; (2) 관련성 평가; 및 (3) 토픽 카테고리화. 우리는 주석자가 주석 중에 찾을 수 있는 몇 가지 오류 범주를 제공합니다. ** 연습 세션 동안 이러한 오류 범주를 고안했으며 이는 섹션 4**에서 언급한 아웃 오류 분류의 최종 버전이 아니다.\n' +
      '\n' +
      '도 8: 완전성 평가를 위한 주석 인터페이스의 스크린샷. 작업량으로 인해 이전 작업과 별도의 주석 작업입니다.\n' +
      '\n' +
      '도 9: (계속) 완전성 평가를 위한 주석 인터페이스의 스크린샷.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>