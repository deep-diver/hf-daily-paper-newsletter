<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MobiLlama: 정밀하고 가벼운 완전 투명 GPT를 향한\n' +
      '\n' +
      'Omkar Thawakar\\({}^{1}\\)\n' +
      '\n' +
      'Equal contribution.\n' +
      '\n' +
      'Ashmal Vayani\\({}^{1}\\)1\n' +
      '\n' +
      'Salman Khan\\({}^{1,2}\\)\n' +
      '\n' +
      'Hisham Cholakal\\({}^{1}\\)\n' +
      '\n' +
      '라오민 Anwer\\({}^{1,3}\\)\n' +
      '\n' +
      'Michael Felsberg\\({}^{5}\\)\n' +
      '\n' +
      'Tim Baldwin\\({}^{1,4}\\)\n' +
      '\n' +
      'Eric P. Xing\\({}^{1}\\)\n' +
      '\n' +
      '파하드 샤바즈 칸\\({}^{1,5}\\)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근 대규모 언어모델(Large Language Models, LLM)의 개발에서 LLM은 온-디바이스 처리, 에너지 효율, 낮은 메모리 풋프린트, 응답 효율 등을 요구하는 시나리오에는 적합하지 않다. 본 논문에서는 자원 제약형 디바이스에 대한 정확하고 효율적인 소규모 언어모델(Small Language Models, SLM)을 설계하는 문제를 해결하여 \'_less is more_\' 패러다임을 탐색한다. 우리의 주요 기여는 정확하고 완전히 투명한 오픈 소스 0.5B(0.5B) 매개변수 SLM의 도입으로, _MobiLlama_로 명명되며, 리소스 요구 감소와 함께 향상된 성능에 중점을 두고 리소스 제한 컴퓨팅의 특정 요구에 부응한다. _ MobiLlama_는 SLM 설계로 더 큰 모델에서 시작하여 사전 훈련과 배치 비용을 모두 줄이기 위해 신중한 매개변수 공유 방식을 적용한다. 우리의 연구는 오픈소스 SLM의 격차를 해소하는 것뿐만 아니라 완전한 투명성을 보장하기 위해 노력하며, 완전한 훈련 데이터 파이프라인, 훈련 코드, 모델 가중치 및 평가 코드와 함께 300개 이상의 체크포인트가 [https://github.com/mbzuai-oryx/Mobillama](https://github.com/mbzuai-oryx/Mobillama)에서 사용 가능하다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 몇 년 동안 ChatGPT, Bard 및 Claude와 같은 유명한 폐쇄 소스 상용 모델의 출현으로 대규모 언어 모델(LLM)의 개발이 엄청나게 급증했다. 이러한 LLM은 복잡한 작업을 해결하는 데 있어 일반적으로 비상 능력이라고 하는 놀라운 능력을 나타낸다. 대부분의 기존 인기 있는 LLM은 크기가 클수록 항상 더 나은 유사한 경향을 따르며, 여기서 모델 크기 또는 데이터 크기는 일반적으로 다운스트림 작업에서 개선된 모델 용량 및 성능을 제공한다. 예를 들어, 최근 LLama-2 700억(70B) 모델(Touvron et al., 2023)은 일반적으로 분류 또는 요약과 같은 기본 작업에 더 적합한 7B 대응물에 비해 대화, 논리적 추론, 코딩을 처리하는 데 대한 효과 때문에 상이한 채팅 애플리케이션에서 더 유리한 것으로 간주된다. 이러한 LLM은 복잡한 언어 작업을 처리하는 데 있어 인상적인 성능을 보여주지만 주요 제한 사항은 크기와 계산 요구 사항이다. 예를 들어, 대규모 팔콘(Almazrouei et al., 2023) 180B 모델은 4096 A100 GPU를 사용하여 훈련되었으며 전용 고성능 서버 및 확장 가능한 스토리지 시스템으로 배포하기 위해 대용량 메모리와 계산이 필요하다.\n' +
      '\n' +
      '최근, 소규모 언어 모델들은 대규모 LLM에 비해 상당히 작은 규모로 달성된 새로운 능력으로 괜찮은 성능을 제공한다는 측면에서 잠재력을 보여주었다. 마이크로소프트사의 Phi-2 27억(Li et al., 2023b)과 같은 현대 SLM은 덜하면서 더 많은 것을 달성하는 데 커뮤니티에서 점점 더 집중하는 것을 강조한다. SLM은 효율성, 비용, 유연성 및 사용자 정의 용이성 측면에서 이점을 제공합니다. 더 적은 파라미터로, SLM은 감소된 메모리 및 저장 요건으로 빠른 사전 훈련 및 추론 측면에서 상당한 계산 효율을 제공한다. 이는 효율적인 자원 활용이 매우 요구되는 실제 응용 분야에서 매우 중요하다. 특히 자원이 제한된 컴퓨팅에서 가능성을 열어주는데, 여기서 모델은 저전력 장치(예: 에지)에서 동작하기 위해 메모리 효율이 요구된다. SLM은 개인 정보 보호, 보안, 응답 시간 및 개인화를 향상시키는 온 디바이스 처리를 지원합니다. 이러한 통합은 고급 개인 비서, 클라우드 독립적인 응용 프로그램, 탄소 발자국 감소로 에너지 효율성을 향상시킬 수 있다.\n' +
      '\n' +
      '언어 모델, 특히 SLM의 풍경은 현재 오픈 소스 가용성의 눈에 띄는 부족으로 두드러진다. LLM은 상당한 관심을 받았지만 대부분의 모델의 독점적 특성은 특히 SLM의 영역에서 제한된 투명성과 접근성으로 이어졌다. 이러한 격차는 보다 효율적이고 컴팩트하며 수행 가능한 모델의 과학 기술 탐구를 방해한다. 이를 인식하면 커뮤니티에서 완전히 투명한 오픈 소스 SLM에 대한 필요성이 증가하고 있으며, 이는 그들의 능력과 한계에 대한 더 깊은 이해를 촉진하고 그들의 아키텍처와 재현 가능한 훈련 방법론에 대한 광범위한 커뮤니티 액세스를 허용함으로써 혁신을 촉진한다. 우리는 이 격차를 해소하는 것이 SLM의 협력 발전에 대한 액세스를 민주화하는 데 중요하다고 주장한다. 따라서, 전체 학습 데이터 파이프라인 및 코드, 모델 가중치, 평가 코드와 함께 300개 이상의 체크포인트에 대한 액세스 형태로 완전한 투명성을 제공하려는 의도로 처음부터 정확하면서도 효율적인 SLM을 설계하는 문제를 조사한다.\n' +
      '\n' +
      'SLM을 처음부터 설계할 때 사전 훈련 및 배치 측면에서 효율성을 유지하면서 결과 모델이 정확하기를 바란다. 간단한 방법은 숨겨진 치수 레이어의 크기 또는 레이어의 수를 줄임으로써 더 큰 LLM 설계를 원하는 모델 크기(예: 0.5B)로 축소하는 것이다. 우리는 열등한 성능을 제공하기 위해 이 두 설계 전략을 경험적으로 관찰한다. 이것은 우리가 완전한 투명성을 제공하면서 정확하면서도 효율성을 유지하는 SLM을 처음부터 설계하는 대안적인 방법을 조사하도록 동기를 부여한다.\n' +
      '\n' +
      '**Contributions:**\n' +
      '\n' +
      '본 논문에서는 트랜스포머 블록의 중복성을 완화하여 정확한 SLM을 개발하는 것을 목적으로 하는 SLM 프레임워크인 _MobiLlama_를 소개한다. 일반적으로 각 변압 블록에 전용 FFN(Feed Forward Layer)이 할당되는 기존의 SLM 설계와는 달리, 우리는 SLM 내의 모든 변압 블록에 공유 FFN 설계를 채택할 것을 제안한다. 공유 FFN 기반 SLM 설계를 활용하는 우리의 _MobiLlama_는 정확하고 효율성을 유지하면서 데이터 파이프라인, 훈련 코드, 모델 가중치 및 평가 코드와 함께 광범위한 중간 체크포인트 측면에서 완전한 투명성을 제공한다.\n' +
      '\n' +
      '본 논문에서 제안한_MobiLlama_는 기존의 SLM 설계 기법들에 비해 처음부터 사전 훈련을 수행할 때 더 좋은 성능을 보임을 보인다. 우리의 _MobiLlama_ 0.5B 모델은 9개의 다른 벤치마크에서 비슷한 크기의 기존 SLM을 능가한다. _ MobiLlama_ 0.5B는 문헌에서 가장 우수한 기존 0.5B SLM에 비해 9개의 벤치마크에서 평균 성능 측면에서 2.4%의 이득을 달성한다. 또한 0.5B 모델로부터 생성된 0.8B SLM을 트랜스포머 블록에서 더 넓은 공유-FFN 기법을 활용하여 개발함으로써 기존 SLM 중 1B 이하의 파라미터 범주에 속하는 최고의 성능을 달성한다. 마지막으로 SLM 위에 멀티모달 모델을 구축하여 시각 지각 및 추론 기능을 선보입니다. 도. 도 1은 정확성, 사전-훈련 복잡성 및 온-보드 배포 비용 측면에서 우리의 _MobiLlama_와 최근의 완전히 투명한 비교적 큰 SLM의 비교를 도시한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      'LLM이 엄청난 인기를 얻었지만(Zhao et al., 2023), 그들의 주요 한계 중 하나는 사전 훈련 및 배치 동안 크기 및 계산 요구 사항이다. 또 다른 문제는 체크포인트 및 평가 프로토콜과 함께 데이터 파이프라인, 트레이닝 코드에 대한 완전한 액세스를 제공하는 완전히 투명한 오픈 소스 LLM의 제한된 가용성이다. 기존 연구들은 LLM 프레임워크의 구성 요소를 효율적으로 만들기 위해 주의 메커니즘(Dao, 2023)과 최적화 전략(Loshchilov and Hutter, 2017)을 탐색하였다. 또한, 기존의 노력들은 또한 계산적으로 트레이닝 후 희소화 스킴(Ashkboos et al., 2024) 또는 양자화(Hoefler et al., 2021; Zhu et al., 2023; Xiao et al., 2023)를 탐색하는 것을 포함한다.\n' +
      '\n' +
      '도 1: 우리의 _MobiLlama_ 0.5B 및 0.8B 모델을 최근 OLMo-1.17B(Groeneveld et al., 2024) 및 TinyLlama-1.1B(Zhang et al., 2024)와 비교하여 사전-트레이닝 토큰, 사전-트레이닝 시간 및 메모리, 모델 파라미터, 9개의 벤치마크에 걸친 전체 정확도 및 온-디바이스 효율(RTX2080Ti를 갖는 PC에서의 평균 배터리 소비 및 평균 토큰/초) 측면에서 비교. 우리의 _MobiLlama_는 훨씬 적은 사전 훈련 데이터(1.2T 토큰 대 3T 토큰)를 요구하면서 비슷한 정확도를 달성한다. 자원 제약형 장치에서의 배치 측면에서 효율적일 뿐만 아니라 사전 훈련 시간 및 GPU 메모리가 더 적다.\n' +
      '\n' +
      '저렴한 LLM이죠 여러 경우에, 이러한 사후 희박화는 처음부터 훈련된 SLM에 비해 더 많은 온-디바이스 메모리 소비로 LLM의 성능을 감소시킬 수 있다. 또한, 이러한 기술은 일반적으로 제한된 투명도 및 접근성을 갖는 LLM을 사용한다.\n' +
      '\n' +
      '최근, SLM을 처음부터 설계하는 것이 주목을 받고 있다(Biderman et al., 2023; Wu et al., 2023; Zhang et al., 2024; Li et al., 2023; Lin et al., 2021b; Shoeybi et al., 2019; Zhang et al., 2022). SLM은 특히 자원이 제한된 환경(예: 에지 장치)에서의 배치뿐만 아니라 제한된 사전 훈련 계산의 경우에 대안으로서 가능성을 보여주었다. 또한, SLM은 온 디바이스 프로세싱을 지원할 수 있으며, 이는 차례로 보안, 프라이버시, 응답 효율 및 개인화를 향상시킬 수 있다. 여기서는 SLM의 매개변수에 자주 존재하는 중복성을 줄이면서 데이터의 복잡한 패턴과 관계를 캡처할 수 있는 모델의 용량을 유지함으로써 완전히 투명하면서도 계산적으로 효율적인 SLM을 구성하려고 노력한다. 선행 연구들(Frantar et al., 2022; Gholami et al., 2022; Pires et al., 2023; Pan et al., 2023; Bhojanapalli et al., 2021)은 BERT 스타일 아키텍처들에서 어텐션 메커니즘에 초점을 맞추거나 단일 피드-포워드 계층 상에서 변압기 설계에서의 리던던시를 완화시키는 것을 탐색한다. 이러한 접근법과 달리, 우리는 다중 피드 포워드 네트워크(FFN) 계층을 갖는 MLP 블록의 공유 메커니즘에 초점을 맞추어 LLM 목적 함수를 갖는 SLM 아키텍처의 중복성을 완화시키는 방법을 탐구한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '##### 베이스라인 SLM 설계\n' +
      '\n' +
      '우리는 먼저 최근의 TinyLlama(Zhang et al., 2024) 및 Llama-2(Touvron et al., 2023)로부터 적응된 우리의 기준선 0.5B SLM 아키텍처를 설명한다. 베이스라인 아키텍처는 \\(N\\) 레이어로 구성되며, 각 레이어는 \\(M\\)의 숨겨진 치수와 \\(5632\\)의 중간 크기(MLPs)로 구성된다. 어휘 크기는 32\\(K\\)이고 최대입니다. context length는 \\(C\\)이다. 우리는 0.5B 모델을 처음부터 구성할 때 두 가지 다른 설계 선택을 고려한다. 첫 번째 디자인 선택에서 \\(baseline1\\)으로 명명된 층수는 \\(N=22\\)으로 설정되고 각 층의 숨겨진 크기는 \\(M=1024\\)으로 설정된다. 두 번째 설계 선택인 \\(baseline2\\)에서는 층수를 \\(N=8\\)으로 설정하고 각 층의 숨김 크기를 \\(M=2048\\)으로 설정하였다.\n' +
      '\n' +
      '우리는 앞서 언급한 기준선 설계 둘 다 정확성과 효율성 사이의 최적의 균형을 맞추기 위해 고군분투한다는 점에 주목한다. 기본값 1\\(기준값 1\\)의 경우 숨겨진 차원(1024)의 크기가 감소하면 계산 효율이 향상되지만 데이터 내에서 복잡한 패턴을 포착하는 모델의 능력을 방해할 수 있다. 이러한 차원의 감소는 잠재적으로 병목 효과로 이어질 수 있으며, 여기서 데이터의 복잡한 관계와 뉘앙스를 나타내는 모델의 능력이 제한되어 전체 정확도에 영향을 미친다. 한편, \\(baseline2\\)에서와 같이 은닉층의 수(22~8개)를 줄이는 것은 모델의 깊이에 영향을 주어 언어의 계층적 표현을 학습하는 능력을 방해한다. 더 깊은 언어 이해와 맥락 분석이 필요한 과제에 대해 우수한 성과를 달성하는 것은 앞서 언급한 두 가지 기준선의 장점을 결합해야 할 가능성이 있다. 그러나 \\(baseline1\\)과 \\(baseline2\\)의 모델 용량을 단일 모델(22층 및 2048의 은닉 차원 크기)로 증가시키면 훈련 비용이 증가함에 따라 1.2B의 파라미터화 모델이 상당히 커진다(탭 1 참조). 우리는 이 큰 모델을 _large-base_라고 부른다. 다음으로, 각 계층(\\(baseline1\\)) 또는 전체 계층 수(\\(baseline2\\))에서 숨겨진 차원 크기를 감소시키지 않으면서 유사한 훈련 효율을 유지하는 제안된 _MobiLlama_ 0.5B 모델 설계를 제시한다(Tab. 1 참조).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c} \\hline Model & \\#Params & Training Time & GPU Hours & GPU memory & No. of layers & Hidden dim size \\\\ \\hline _baseline1_ & 0.54B & 7.5 days & 28.8K & 3.2 GB & 22 & 1024 \\\\ _baseline2_ & 0.52B & 7 days & 26.9K & 3 GB & 8 & 2048 \\\\ _large-base_ & 1.2B & 12 days & 46.1K & 6 GB & 22 & 2048 \\\\ _MobiLlama_ & 0.52B & 7 days & 26.6K & 3 GB & 22 & 2048 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 두 개의 기준선 및 대규모 기반 모델과 우리의 _MobiLlama_의 비교. 각 층의 파라미터 수, 훈련 시간, 총 GPU 시간, GPU 메모리, 변압기 층 수 및 숨겨진 차원 크기 측면에서 비교를 보여준다. 숫자는 각각 80GB 메모리가 있는 A100 GPU에서 계산된다. 본 논문에서 제안한 _MobiLlama_는 기존의 _large-base_와 비교하여 동일한 설계 구성(계층 수 및 은닉 차원 크기 등)으로 GPU 메모리의 현저한 감소와 함께 GPU 트레이닝 시간을 42% 감소시킨다. 또한, 본 논문에서 제안한 _MobiLlama_는 _baseline1_ 및 _baseline2_에 비해 유사한 트레이닝 비용과 파라미터를 유지하면서 계층 수 및 은닉 차원 크기 측면에서 향상된 모델 용량을 갖는다.\n' +
      '\n' +
      '### 제안된 SLM 설계: MobiLlama\n' +
      '\n' +
      '제안된 접근 방식인 _MobiLlama_는 먼저 더 큰 모델 크기 설계인 _large-base_로부터 시작함으로써 원하는 크기(예를 들어, 0.5B 모델)의 SLM을 구성한다. 그리고 모델 크기를 미리 정의된 모델 구성으로 줄이기 위해 세심한 파라미터 공유 기법을 적용하여 학습 비용을 크게 줄인다. 일반적으로, SLM들 및 LLM들 모두는 전형적으로 각각의 변압기 블록 내에 다수의 피드 포워드 네트워크(FFN) 층들을 포함하는 전용 다층 퍼셉트론(MLP) 블록을 이용한다. 이러한 구성(예를 들어, _large-base_)에서 FFN 층은 전체 훈련 가능한 매개변수의 상당한 \\(65\\%\\)을 차지하며, 주의 메커니즘과 헤드는 각각 \\(30\\%\\)과 \\(5\\%\\)을 기여한다. 결과적으로, 상당한 수의 파라미터가 FFN 층 내에 집중되어, 계산 비용 및 더 빠른 수렴을 달성하는 모델의 능력과 관련하여 사전 훈련 동안 과제를 제기한다. 이러한 문제를 해결하기 위해, 우리는 FFN 파라미터가 SLM 내의 모든 트랜스포머 계층에 걸쳐 공유되는 공유 방식을 사용할 것을 제안한다. 이를 통해, 본 논문에서 제안한_MobiLlama_의 전체 학습 가능한 파라미터를 _large-base_에 비해 \\(60\\%\\) 크게 줄일 수 있다. 이러한 중요한 매개변수 감소는 또한 훈련 비용의 실질적인 증가 없이 레이어 수 및 숨겨진 차원 크기 측면에서 모델 용량을 증가시킬 수 있게 한다(탭 1 참조).\n' +
      '\n' +
      '도. 2는 우리의 아키텍처 설계를 두 개의 기준선과 비교한다. 두 기준선의 경우, 각 변압기 층에는 다수의 FFN 층으로 구성된 전용 MLP 블록이 사용된다. 대신에, 우리의 효율적인 _MobiLlama_ 설계는 SLM 내의 다른 층의 변압기에 걸쳐 공유되는 단일 MLP 블록을 이용한다. 이는 모델에서 훈련 가능한 매개변수의 총 수의 증가 없이 모델 용량을 증가시키는 데 도움이 된다.\n' +
      '\n' +
      '완전히 투명한 모빌라마를 향해\n' +
      '\n' +
      '앞서 논의한 바와 같이 완전히 투명한 오픈 소스 SLM 개발은 보다 포괄적인 데이터/모델 출처 및 재현 가능한 협력 SLM 연구 개발 환경을 조성하는 것이 필요하다. 이를 위해 사전 훈련 데이터 세트를 제시한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Subset & Tokens (Billion) \\\\ \\hline Arxiv & 30.00 \\\\ Book & 28.86 \\\\ C4 & 197.67 \\\\ Refined-Web & 665.01 \\\\ StarCoder & 291.92 \\\\ StackExchange & 21.75 \\\\ Wikipedia & 23.90 \\\\ \\hline Total & 1259.13 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 앰버-다타세트에서의 데이터 믹스.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Hyperparameter & Value \\\\ \\hline Number Parameters & 0.5\\(B\\) \\\\ Hidden Size & 2048 \\\\ Intermediate Size (in MLPs) & 5632 \\\\ Number of Attention Heads & 32 \\\\ Number of Hidden Layers & 22 \\\\ RMSNorm \\(\\epsilon\\) & \\(1e^{-6}\\) \\\\ Max Seq Length & 2048 \\\\ Vocab Size & 32000 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: _MobiLlama_ architecture & hyperparameters.\n' +
      '\n' +
      '그림 2: 두 개의 기준선과 우리의 _MobiLlama_의 예시적인 비교. 각 경우에 대해 서로 다른 자기 주의 층으로 표시된 두 개의 변압기 블록을 보여준다. _baseline1_ 및 _baseline2_ 둘 다의 경우, 3개의 FFN 층을 포함하는 전용 MLP 블록이 각각의 변압기 층에 대해 활용된다. 대조적으로, 우리의 _MobiLlama_는 상이한 변압기 층에 걸쳐 공유되는 단일 MLP 블록(동일한 색으로 강조됨)을 이용한다. 이것은 훈련 가능한 파라미터들의 총 수의 유의미한 증가 없이 계층들 및 숨겨진 치수 크기 측면에서 네트워크의 용량을 증가시킬 수 있게 한다.\n' +
      '\n' +
      '및 처리 세부사항, 교육 세부사항을 포함한 아키텍처 설계 구성, 평가 벤치마크 및 메트릭. 또한, 중간 모델 체크포인트와 함께 완전한 교육 및 평가 코드를 공개할 예정입니다.\n' +
      '\n' +
      '**Pre-training Dataset and Processing:**Pre-training을 위해 LLM360 Amber dataset Liu et al.(2023)로부터 1.2T 토큰을 사용한다. 앰버 데이터 세트는 텍스트 유형, 주제 및 스타일이 다른 풍부하고 다양한 언어 풍경을 제공한다. 탭 도 2는 다양한 소스로부터 수집된 앰버 데이터세트로부터의 데이터 믹스를 도시한다.\n' +
      '\n' +
      '(30억 토큰)_Arxiv(30억 토큰)_ 서브세트는 과학 논문의 저장소로부터 추출되고, 복잡하고 도메인-특정 언어 및 기술 용어를 제공하며, 학술 산문에 대한 이해를 풍부하게 한다. _ 북(28.9억 토큰)_서브세트는 다양한 서사 양식, 문화적 맥락, 풍부한 어휘를 가진 광범위한 문학의 토큰으로 구성되어 스토리텔링과 언어 뉘앙스의 파악을 심화시킨다. C4(197.7억 토큰)_는 방대한 양의 웹 텍스트 선택을 제공하고, 다양한 레지스터, 스타일 및 주제를 포함하는 광범위한 언어 기반을 제공하는 거대 클린 크롤드 코퍼스(C4)이다. _ 정제된 웹(665억 토큰)_ 서브세트는 큐레이션된 웹 크롤이며 현대, 비공식 및 다양한 인터넷 언어에 대한 모델 노출을 제공하여 현대 커뮤니케이션에 대한 관련성과 적용 가능성을 향상시킨다. _ StarCoder(291.9 Billion Tokens)_ 서브세트는 86개의 프로그래밍 언어에 걸쳐 783GB의 코드를 특징으로 하는 코드 이해에 사용되는 방대한 모음이다. 여기에는 GitHub 문제, Jupyter 노트북 및 커밋이 포함되어 있으며 총 약 2,500억 토큰이 있다. 훈련 효율성을 위해 꼼꼼하게 청소하고 중복을 제거합니다. _ StackExchange(21.8 Billion Tokens)_는 질의응답 웹사이트의 네트워크에서 나온 것으로, 이 서브세트는 모델이 다양한 주제에 걸쳐 질문-응답 포맷 및 기술 토론을 학습하는 데 도움이 된다. _ 위키피디아(23.9억 토큰스)_는 백과사전 컬렉션으로 모델이 백과사전 지식과 공식 쓰기 스타일을 배울 수 있도록 잘 구조화되고 사실적인 콘텐츠를 제공한다.\n' +
      '\n' +
      '전술한 서브세트로부터, Arxiv, Book, C4, StackExchange 및 위키피디아는 RedPajama-v1 컴퓨터로부터 공급받는다(2023). 앰버 데이터세트는 RedPajama-v1의 common_crawl 서브세트를 대체하기 위해 RefinedWeb Penedo et al.(2023) 데이터를 사용한다. 이들 서브세트들은 125913억 토큰에 달한다.\n' +
      '\n' +
      '초기에, 상기 소스들로부터 소싱된 원시 데이터는 Huggingface LLaMA tokenizer Touvron 등을 사용하여 토큰화된다(2023). 이어서, 이들 토큰은 각각 2048개의 토큰을 포함하는 시퀀스로 구성된다. 데이터를 관리하기 위해, 이들 시퀀스는 토큰 시퀀스에 병합되고, 병합된 데이터세트를 360개의 별개의 세그먼트로 분할한다. json 파일로서 구조화된 각각의 데이터 세그먼트는 발신 데이터세트를 나타내는 소스 식별자와 함께 토큰 ID들의 어레이를 운반한다. 각 데이터 샘플은 2049개의 토큰을 갖도록 설계되었습니다.\n' +
      '\n' +
      '**건축 설계:** 우리의 _MobiLlama_ 0.5B는 그 MLP에서 2048의 숨겨진 크기, 5632의 중간 크기를 포함하고, 22개의 숨겨진 층에 걸쳐 32개의 주의 헤드로 동작한다. 최대 2048개의 토큰을 처리할 수 있도록 설계되었으며, 32,000개의 어휘 크기로 지원되며, 정규화 정밀도는 RMSNorm epsilon \\(1e^{-6}\\)에 의해 보장되어 보다 안정적인 트레이닝을 얻을 수 있다. 우리는 RoPE Rotary Positional Embedding) Su et al.(2024)을 활용하여 우리의 _MobiLlama_에서 위치 정보를 인코딩한다. Zhang et al.(2024)과 유사하게, 우리는 Swish와 Gated Lin의 조합을 채용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c c c|c} \\hline Model Name & \\#Params & HellaSwag & Truthfulqa & MMLU & Arc\\_C & CrowsPairs & piga & race & siqa & winograde & Average \\\\ \\hline gpt-neo-125m & 0.15B & 30.26 & 45.58 & 25.97 & 22.95 & 61.55 & 62.46 & 27.56 & 40.33 & 51.78 & 40.93 \\\\ tiny-starcoder & 0.17B & 28.17 & 47.68 & 26.79 & 20.99 & 49.68 & 52.55 & 25.45 & 38.28 & 51.22 & 37.86 \\\\ cerebras-gpt-256m & 0.26B & 28.99 & 45.98 & 26.83 & 22.01 & 60.52 & 61.42 & 27.46 & 40.53 & 52.49 & 40.69 \\\\ opt-350m & 0.35B & 36.73 & 40.83 & 26.02 & 23.55 & 64.12 & 64.74 & 29.85 & 41.55 & 52.64 & 42.22 \\\\ megatom-gpt2-345m & 0.38B & 39.18 & 41.51 & 24.32 & 24.23 & 64.82 & 66.87 & 31.19 & 40.28 & 52.96 & 42.81 \\\\ LiteLlama & 0.46B & 38.47 & 41.59 & 26.17 & 24.91 & 62.90 & 67.73 & 28.42 & 40.27 & 49.88 & 42.26 \\\\ gpt-sw3-356m & 0.47B & 37.05 & 42.55 & 25.93 & 23.63 & 61.59 & 64.85 & 32.15 & 41.56 & 53.04 & 42.48 \\\\ pythia-410m & 0.51B & 40.85 & 41.22 & 27.25 & 26.19 & 64.20 & 67.19 & 30.71 & 41.40 & 53.12 & 43.57 \\\\ xglm-564m & 0.56B & 34.64 & 40.43 & 25.18 & 24.57 & 62.25 & 64.85 & 29.28 & 42.68 & 53.03 & 41.87 \\\\ Lamini-GTP-LM & 0.59B & 31.55 & 40.72 & 25.53 & 24.23 & 63.09 & 63.87 & 29.95 & 40.78 & 47.75 & 40.83 \\\\\n' +
      '**MobilLlama (Ours)** & 0.5B & 52.52 & 38.05 & 26.45 & 29.52 & 64.03 & 72.03 & 33.68 & 40.22 & 57.53 & **46.00** \\\\ \\hline Lamini-GTP-LM & 0.77B & 43.83 & 40.25 & 26.24 & 27.55 & 66.12 & 69.31 & 37.12 & 42.47 & 56.59 & 45.49 \\\\\n' +
      '**MobilLlama (Ours)** & 0.8B & 54.09 & 38.48 & 26.92 & 30.20 & 64.82 & 73.17 & 33.37 & 41.60 & 57.45 & **46.67** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: _nine_ 벤치마크에 대한 기존 < _1B 파라미터 모델_와의 최첨단 비교. 약 0.5B 모델 시리즈의 경우, 우리의 _MobiLlama_는 9개의 벤치마크에서 평균 성능 측면에서 2.4%의 상당한 이득을 얻는다. 또한, 우리의 _MobiLlama_ 0.8B 모델은 평균 46.67점을 달성한다.\n' +
      '\n' +
      '유니트를 활성화 함수로 함께 표시합니다. 탭 3은 우리의 모델 구성에 대한 세부 사항을 제시한다. 또한 공유 FFN 설계를 확장하여 _MobiLlama_에서 0.8B 버전을 도출합니다. 0.5B 모델과 비교하여 0.8B 디자인은 숨겨진 치수 크기를 2532로, 중간 크기를 11,080으로 증가시키면서 나머지 구성은 동일하다.\n' +
      '\n' +
      '본 논문의 _MobiLlama_의 사전 학습을 위해, 모델 학습을 위해 8개의 NVIDIA A100 GPU와 80GB의 메모리와 800Gbps의 상호연결을 갖는 20개의 GPU 노드를 갖는 공개 클러스터를 사용한다. 각 GPU는 8개의 NVLink 링크를 통해 상호 연결되며, 2 포트 200 Gb/sec(4\\(\\times\\) HDR) InfiniBand의 교차 노드 연결 구성으로 보완되어 모델의 학습 과정을 최적화한다. 학습 효율을 높이기 위해 플래시-어텐션 메커니즘을 사용하고 LLAMA(Touvron et al., 2023) 모델에 의해 확립된 사전 학습 하이퍼-파라미터를 따른다. 우리의 _MobiLlama_ 모델의 학습은 AdamW 최적화기를 사용하여 하이퍼파라미터 \\(\\beta_{1}=0.9,\\beta_{2}=0.95\\)를 이용하여 수행되며, 초기 학습률은 \\(\\eta=3e^{-4}\\)이다. 이 비율은 코사인 학습률 스케줄을 따르며, \\(\\eta=3e^{-5}\\)의 최종 비율로 테이퍼링된다. 우리는 \\(0.1\\)의 무게감소를 더 포함하고 \\(2,000\\) 단계의 예열기간을 갖는 \\(1.0\\)의 기울기 클리핑을 적용한다. 20개의 GPU 노드의 하드웨어 구성에 적응하여, 단일 GPU에서 초당 약 14\\(k\\)-15\\(k\\) 토큰의 처리량을 달성하면서, 사전 학습 배치 크기를 \\(800\\)(\\(160\\times 5\\))으로 최적화한다. 모델 사전 교육 동안 공개되는 3.3B 토큰마다 중간 체크포인트를 저장합니다.\n' +
      '\n' +
      '**평가 벤치마크 및 메트릭:**\n' +
      '\n' +
      '종합적인 성능 평가를 위해 Open LLM Leaderboard1의 9가지 벤치마크를 사용한다.\n' +
      '\n' +
      '각주 1: [https://huggingface.co/spaces/huggingface/open_llm_leaderboard](https://huggingface.co/spaces/huggingface/open_llm_leaderboard)\n' +
      '\n' +
      'HellaSwag(Zellers et al., 2019)는 일련의 가능한 연속체로부터 시나리오에 대한 정확한 어미를 예측하는 모델의 능력을 평가하여, 상식 추론을 테스트한다. TruthfulQA(Lin et al., 2021)는 사실에 대한 이해와 기만 회피 능력을 중심으로 진실한 답변을 제공하기 위해 모델을 평가한다. MMLU(Hendrycks et al., 2020)는 인문학, 과학, 기술, 공학 및 경영과 같은 수많은 주제에 걸쳐 모델의 광범위한 지식을 측정한다. ARC_Challenge (Clark et al., 2018)는 과학 질문과 함께 복잡한 추론을 테스트한다. CrowsPairs(Nangia et al., 2020)는 공정성을 목표로 언급한 인구통계학적 집단별로만 차이가 나는 문장을 비교하여 모형의 편향을 평가한다. PIQA(Bisk et al., 2020)는 모델의 물리적 상식 지식을 평가하여 일상적인 물리적 과정에 대한 이해가 필요하다. 레이스(Lai et al., 2017)는 구절을 바탕으로 객관식 문항을 통해 독해력을 평가한다. SIQA(Sap et al., 2019)는 모델의 사회적 상식 추론과 사회적 역학에 대한 이해에 중점을 둔다. Winogrande(Sakaguchi et al., 2021)는 텍스트에서 모호성을 해결하는 모델의 능력을 평가하여 상식적 추론을 테스트한다.\n' +
      '\n' +
      'llvm-harness(Gao et al., 2023)를 기반으로 구축된 Analysis-360 프레임워크(Liu et al., 2023)에 따라, 우리는 상세한 평가를 위해 다양한 샷으로 표준 설정 하에서 광범위한 평가를 수행하여 다양한 언어 작업에 걸쳐 모델의 견고성과 적응성을 검증한다. 표준 평가 프로토콜에 따라 평가 설정은 헬라스바그, ARC_Challenge, 위노그란데 및 MMLU에 대한 10, 25, 5 및 5 샷 평가로 구성되며 나머지 벤치마크에 대한 제로 샷으로 구성된다.\n' +
      '\n' +
      '## 4 Results\n' +
      '\n' +
      '**기준 비교:** 먼저 탭에서 두 기준선과의 비교를 제시한다. 5) 0.5B 모델 시리즈에 대해. 베이스라인 평가를 위해 앰버 데이터 세트의 동일한 100B 토큰에 있는 모든 모델을 사전 훈련하고 HellaSwag, TruthfulQA, MMLU 및 Arc_C의 4가지 벤치마크에 결과를 보고한다. 우리의 _MobiLlama_는 4개의 벤치마크에 대해 평균 34.4점을 달성함으로써 두 개의 기준선에 비해 유리한 성능을 달성한다. 우리는 이러한 성능 향상이 훈련 비용의 큰 증가 없이 달성된다는 것에 주목한다(탭 1 참조). 제안된 SLM 설계의 장점을 강조합니다.\n' +
      '\n' +
      '**최첨단 비교:** 우리는 우리의 _MobiLlama_0.5B 및 0.8B를 비교가능한(1B 미만) 파라미터를 갖는 기존의 SLM들과 비교한다: gpt\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c} \\hline \\hline Model & \\multicolumn{2}{c}{HellaSwag} & Truthfulqa & MMLU & Arc\\_C & Average \\\\ \\hline _baseline1_ & 42.44 & 38.46 & 25.08 & 26.18 & 33.04 \\\\ _baseline2_ & 42.15 & 38.70 & 25.73 & 26.10 & 33.17 \\\\ _MobiLlama_ & 44.47 & 40.12 & 26.48 & 26.53 & **34.40** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 네 가지 벤치마크에 대한 기준 비교. 여기서, 베이스라인들 및 우리의 _MobiLlama_는 모두 동일한 파라미터들(0.5B)을 포함하고 앰버로부터의 100B 토큰들에 대해 사전 트레이닝된다. 우리의 _MobiLlama_는 유사한 훈련 예산으로 운영하면서 두 가지 기준선에 비해 유리한 성능을 달성한다.\n' +
      '\n' +
      'neo (Black et al., 2021), tiny-starcoder (Li et al., 2023a), cerebras-gpt (Dey et al., 2023), opt (Zhang et al., 2022), megatron-gpt-2 (Shoeybi et al., 2019), LiteLlama, gpt-sw3, pythia (Biderman et al., 2023), xglm (Lin et al., 2021b), Lamini-LM (Wu et al., 2023). 기존의 0.5B 모델 시리즈 카테고리 중 pythia-410m은 평균 43.57의 성능을 보였으며, 우리의 _MobiLlama_ 0.5B 모델은 평균 46.0의 성능으로 9개의 벤치마크에서 pythia-410m보다 2.4%의 성능 향상을 보였다. 특히, _MobiLlama_는 NLP 텍스트 완성 태스크에서 모델의 성능을 평가하기 위해 설계된 HellaSwag 벤치마크에서 우수한 성능을 달성한다. 또한, _MobiLlama_는 piqa 및 winogrande 벤치마크에서 우수한 결과를 가진 상식 추론 작업에서도 호의적으로 수행한다. 또한, 우리의 _MobiLlama_ 0.8B 모델은 평균 49.06점을 달성한다.\n' +
      '\n' +
      '**효율성 비교:** RTX-2080Ti GPU가 탑재된 PC, i7 CPU가 탑재된 노트북, i7 CPU가 탑재된 노트북, Snapdragon-685 프로세서가 탑재된 스마트폰 등 다양한 저사양 하드웨어 플랫폼에서 효율성과 자원 소모 측면에서 모델의 비교를 제시한다. 탭 도 6은 우리의 _MobiLlama_ 0.5B와 _large-base_ 1.2B, Llama2-7B(Touvron et al., 2023) 및 Phi2-2.7B(Li et al., 2023b) 모델을 비교하여, 초당 토큰에서의 평균 처리 속도(Average Tokens/Sec), 메가바이트(MB)에서의 평균 메모리 소모량(Avg Memory Consumption) 및 밀리암페어-hours(mAH)에서의 평균 배터리 소모량(Average Battery Consumption/1000 Tokens)의 관점에서 나타낸 것이다. 우리의 _MobiLlama_는 다양한 하드웨어 플랫폼에서 효율성 측면에서 호의적으로 수행합니다.\n' +
      '\n' +
      '우리는 또한, 각각의 가중치 매트릭스가 더 작은(밀집한) 매트릭스로 치환되어, 모델 내의 임베딩들의 치수를 감소시키는, 최근의 트레이닝 후 희소화 스킴(Ashkboos et al., 2024)과 효율성 비교를 수행한다. 이러한 방식에서, 원래 LLM의 파라미터들은 WikiText-2(Merity et al., 2016)와 같은 데이터세트를 사용하여 사후 슬라이싱 미세조정(post-slicing fine-tuning)에 이어 70%까지 상당히 감소된다. 탭 도 7은 다음의 네 벤치마크들(Ashkboos et al., 2024)에 대한 우리의 _MobiLlama_와 기존의 LLMs들(예를 들어, Llama-2-7B, OPT-6.7B)의 비교를 도시한다. 우리의 _MobiLlama_\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Platform} & \\multirow{2}{*}{Model} & \\#Params & \\multirow{2}{*}{Precision} & Avg Tokens/Sec & Avg Memory & Avg Battery Consumption & CPU \\\\  & & (\\(\\downarrow\\)) & & (\\(\\uparrow\\)) & Consumption (\\(\\downarrow\\)) & /1k Tokens (\\(\\downarrow\\)) & Utilization (\\(\\downarrow\\)) \\\\ \\hline \\multirow{4}{*}{RTX2080Ti} & Llama2 & 7B & bf16 & 14.85 & 27793 MB & 135.51 mAH & 31.62\\% \\\\  & Phi2 & 2.7B & bf16 & 32.19 & 12071 MB & 59.13 mAH & 24.73\\% \\\\  & _large-base_ & 1.2B & bf16 & 50.61 & 6254 MB & 18.91 mAH & 18.25\\% \\\\  & _MobiLlama_ & 0.5B & bf16 & **63.38** & **3046** MB & **8.19 mAH** & **14.79\\%** \\\\ \\hline \\multirow{4}{*}{CPU-i7} & Llama2 & 7B & 4bit & 5.96 & 4188 MB & 73.5 mAH & 49.16\\% \\\\  & Phi2 & 2.7B & 4bit & 22.14 & 1972 MB & 27.36 mAH & 34.92\\% \\\\  & _large-base_ & 1.2B & 4bit & 29.23 & 1163 MB & 10.81 mAH & 30.84\\% \\\\  & _MobiLlama_ & 0.5B & 4bit & **36.32** & **799** MB & **4.86** mAH & **24.64**\\% \\\\ \\hline \\multirow{4}{*}{Snapdragon-685} & Llama2 & 7B & 4bit & 1.193 & 4287 MB & 10.07 mAH & 77.41\\% \\\\  & Phi2 & 2.7B & 4bit & 2.882 & 1893 MB & 14.61 mAH & 56.82\\% \\\\ \\cline{1-1}  & _large-base_ & 1.2B & 4bit & 6.687 & 780 MB & 6.00 mAH & 17.15\\% \\\\ \\cline{1-1}  & _MobiLlama_ & 0.5B & 4bit & **7.021** & **770** MB & **5.32** mAH & **13.02**\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 상이한 저사양 하드웨어 디바이스 상에서의 효율 및 리소스 소비의 관점에서 비교. 이를 RTX-2080Ti GPU를 사용한 PC, i7 CPU를 사용한 노트북, Snapdragon-685 프로세서를 사용한 스마트폰으로 비교하였다. 우리의 _large-base_ 모델 외에도 Llama2 7B 및 Phi2 2.7B와의 비교도 제시한다. CPU와 스마트폰의 경우 해당 모델의 4비트 GGUF 형식을 사용하는 반면, 원본 모델은 RTX-2080Ti GPU를 사용하여 PC에 배포 및 테스트된다. 다양한 메트릭은 모델의 운영 효율성, 장치의 RAM에 있는 모델의 풋프린트 및 1,000개의 토큰을 처리하는 에너지 효율성을 측정합니다. 우리의 _MobiLlama_는 이러한 저사양 하드웨어 장치에서 효율성 측면에서 호의적으로 수행한다. 사전 교육을 위한 완전한 데이터 파이프라인을 공개적으로 사용할 수 없다는 점에서 Phi2와 Llama2가 모두 완전히 투명하지 않다는 점에 주목한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Model & GQA & SQA & TextQA & MME \\\\ \\hline _MobiLlama-\\(V\\)_ & 58.5 & 53.1 & 41.4 & 1191.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 다양한 벤치마크에서 멀티모달 설계인 _MobiLlama-\\(V\\)_ 0.8B의 정량적 성능.\n' +
      '\n' +
      '_lama_ 0.5B 및 0.8B 모델은 4개의 벤치마크에서 계산된 평균 점수 53.72로 대표적인 LLM에 대해 유리하게 수행한다. 이러한 결과는 더 큰 슬라이스 모델 대응물의 유사한 기능을 달성할 수 있는 완전히 투명한 새로운 SLM을 설계할 가능성을 강조한다.\n' +
      '\n' +
      '**멀티모달 모비라마:** 비주얼 추론 기능을 가진 범용 비주얼 어시스턴트를 개발하기 위해 비전 인코더와 결합하여 우리의 _MobiLlama_의 상단에 멀티모달 모델을 추가로 구축한다. 우리의 멀티모달 모델인 _MobiLlama-V_는 CLIP의 비주얼 인코더(Radford et al., 2021)를 우리의 _MobiLlama_의 언어 디코더와 브리징하고, 665k 비전-언어 명령어 세트 상에서 종단간 방식으로 미세 조정함으로써 트레이닝된다(Liu et al., 2023a). GQA(Hudson and Manning, 2019), SQA(Lu et al., 2022), TextQA(Singh et al., 2019), MME(Fu et al., 2023)에 대한 평가를 진행한다. 탭 도 8은 _MobiLlama-V_0.8B 모델의 성능을 나타낸다.\n' +
      '\n' +
      '**정성적 분석:** 도 3은 대화 능력으로 _MobiLlama_ 0.5B와 상호작용할 때 획득되는 예시적인 응답을 도시한다. 우리는 텍스트 완성, 코드 생성 및 대화 기능과 같은 다양한 작업을 다루는 예를 보여준다. 우리의 모델은 이러한 다양한 상호 작용에 대한 충실한 응답을 생성한다. 도. 도 4는 우리의 멀티모달 _MobiLlama-V_의 시각적 추론 능력을 입증하는 예들을 도시한다. 예를 들어, _MobiLlama-V_는 주어진 이미지를 묘사하도록 요청될 때 이미지의 비정형적인 양상들을 정확하게 묘사한다.\n' +
      '\n' +
      '**대규모 모델 평가:** 논의된 귀\n' +
      '\n' +
      '도 4: 다양한 시나리오들에 걸쳐 시각적 자극들에 응답함에 있어서 _MobiLlama-V_의 예시적인 응답들.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c c c c|c} \\hline \\hline Model & \\#Params & HellaSwag & Truthfulua & MMLU & Arc\\_C & CrowsPairs & piqa & race & siqa & winogrande & Average \\\\ \\hline Boomer & 1B & 31.62 & 39.42 & 25.42 & 22.26 & 61.26 & 57.99 & 28.99 & 40.32 & 50.98 & 39.80 \\\\ Pythia-Dedup & 1B & 49.63 & 38.92 & 24.29 & 29.09 & 67.11 & 70.23 & 32.44 & 42.63 & 53.98 & 45.36 \\\\ Falcon-RW & 1B & 63.12 & 35.96 & 25.36 & 35.06 & 69.04 & 74.10 & 36.07 & 40.23 & 61.88 & 48.98 \\\\ TinyLlama & 1.1B & 60.22 & 37.59 & 26.11 & 33.61 & 70.60 & 73.28 & 36.45 & 41.65 & 59.18 & 48.74 \\\\ OLMo & 1.2B & 62.50 & 32.94 & 25.86 & 34.45 & 69.59 & 73.70 & 36.74 & 41.14 & 58.90 & 48.42 \\\\ Cerebras-GPT & 1.3B & 38.51 & 42.70 & 26.66 & 26.10 & 63.67 & 66.75 & 30.33 & 42.42 & 53.59 & 43.41 \\\\ Lamini & 1.3B & 38.05 & 36.43 & 28.47 & 26.62 & 64.62 & 67.89 & 33.39 & 43.19 & 50.59 & 43.25 \\\\ OPT & 1.3B & 54.50 & 38.67 & 24.63 & 29.6 & 70.70 & 72.47 & 34.16 & 42.47 & 59.74 & 47.43 \\\\ GPT-NEO & 1.3B & 48.49 & 39.61 & 24.82 & 31.31 & 65.67 & 71.05 & 34.06 & 41.81 & 57.06 & 45.98 \\\\ Pythia-Deduped & 1.4B & 55.00 & 38.63 & 25.45 & 32.59 & 67.33 & 72.68 & 34.64 & 42.68 & 56.90 & 47.32 \\\\ _large-base_ & 1.2B & 62.99 & 35.90 & 24.79 & 34.55 & 68.49 & 75.57 & 35.31 & 41.96 & 62.03 & **49.06** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 9개의 벤치마크에서 기존 < _2B 매개변수 완전 오픈 소스 LLM 모델_와의 포괄적인 비교. 1.2T 토큰에 미리 훈련된 우리의 1.2B _large-base_ 모델은 3T 토큰의 실질적으로 더 큰 데이터에 미리 훈련된 최근의 OLMo 1.17B 모델(Groeneveld et al., 2024) 및 TinyLlama 1.1B 모델(Zhang et al., 2024) 모두에 비해 우수한 성능을 달성한다.\n' +
      '\n' +
      '그림 3: 창의적인 스토리텔링, 코딩 연습, 경제 분석 및 요리 지침을 포함한 다양한 작업에 걸친 _MobiLlama_의 예제 응답입니다. 응답은 추상적인 개념과 실용적인 단계별 프로세스에 모두 참여하는 모델의 능력을 강조하여 광범위한 적용 가능성을 보여준다.\n' +
      '\n' +
      '우리는 SLM의 민주화를 위한 완전히 투명한 모델을 개발하고 향후 연구를 육성하기 위해 노력한다. 이를 위해, 우리는 2B 미만의 카테고리 내에 속하는 기존의 완전 투명 SLM들과 우리의 _large-base_ 1.2B를 비교한다. 탭 도 9는 최근 OLMo 및 TinyLlama가 3T 토큰의 더 큰 데이터 세트에서 사전 훈련된 것과 비교하여, 1.2T 토큰에서 사전 훈련된 우리의 _large-base_ 1.2B 모델이 9개의 벤치마크에 걸쳐 평균 점수 49.06으로 유리한 결과를 달성함을 보여준다. 우리는 우리의 _large-base_ 모델이 견고한 기준선 역할을 하고 SLM 개발에 대한 향후 연구를 완화하는 데 도움이 되기를 바란다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '트랜스포머 블록의 중복성을 완화시키는 완전 투명 SLM _MobiLlama_를 제시한다. _MobiLlama_ 내에서 SLM 내의 모든 블록에 대해 공유 FFN 설계를 활용할 것을 제안한다. 우리는 9개의 벤치마크에서 _MobiLlama_를 평가하여 1B 범주 미만에 해당하는 기존 방법과 비교하여 유리한 결과를 얻었다. 또한 시각적 추론 능력을 입증하기 위해 _MobiLlama_ SLM 위에 멀티모달 모델을 구축한다.\n' +
      '\n' +
      '**제한 및 미래 방향:** 잠재적인 방향은 향상된 컨텍스트 이해를 위해 _MobiLlama_를 더욱 개선하는 것이다. _MobiLlama_는 완전히 투명한 SLM 프레임워크를 제공하지만 모델의 견고성을 향상시키기 위해 잘못된 표현과 편향을 이해하기 위한 후속 연구가 필요하다.\n' +
      '\n' +
      '## 6 Acknowledgement\n' +
      '\n' +
      '계산은 국립 슈퍼컴퓨터 센터에서 크누트 및 앨리스 월렌버그 재단이 제공한 베를리우스 리소스에 의해 가능했다. 우리는 모바일 플랫폼과 VLM 훈련에 대한 평가에서 지지해준 사할 샤지 멀라필리와 무하마드 마즈에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Coffinet, Daniel Hesslow, Julien Launay, Quentin Malatric, Daniele Mazzotta, Baderdidine Noune, Baptiste Pannier, and Guillemre Penedo. 2023. 오픈 언어 모델의 매 시리즈.\n' +
      '* Ashkboos et al. (2024) Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. 2024. Slicegpt: 행과 열을 삭제하여 대언어 모델을 압축한다. _ arXiv preprint arXiv:2401.15024_.\n' +
      '* Bhojanapalli et al. (2021) Srinadh Bhojanapalli, Ayan Chakrabarti, Andreas Veit, Michal Lukasik, Himanshu Jain, Frederick Liu, Yin-Wen Chang, and Sanjiv Kumar. 2021. 재사용 변압기로 주의의 중복성을 활용. _ arXiv preprint arXiv:2110.06821_.\n' +
      '* Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herble Bradley, Kyle O\'Brien, Eric Halhan, Mohammad Affah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: 훈련 및 스케일링에 걸쳐 큰 언어 모델을 분석하기 위한 스위트. _International Conference on Machine Learning_, pages 2397-2430. PMLR.\n' +
      '* Bisk et al. (2020) 요나탄 비스크, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in natural language. The _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 7432-7439.\n' +
      '* Black et al. (2021) Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: 메쉬-텐서플로우를 이용한 대규모 자기회귀 언어 모델링. 이 소프트웨어를 사용하는 경우 이러한 메타데이터를 사용하여 해당 소프트웨어를 인용하십시오.\n' +
      '* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018년 질문 답은 해결한 것 같아? try arc, the ai2 reasoning challenge. _ arXiv preprint arXiv:1803.05457_.\n' +
      '* 컴퓨터(2023) Together Computer. 2023. Redpajama: 라마 트레이닝 데이터세트를 재현하기 위한 오픈 소스 레시피.\n' +
      '* Dao(2023) Tri Dao. 2023. 플래시 어텐션-2: 더 나은 병렬성과 작업 분할로 더 빠른 주의력 _ arXiv preprint arXiv:2307.08691_.\n' +
      '* Dey et al. (2023) Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness, et al. 2023. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. _ arXiv preprint arXiv:2304.03208_.\n' +
      '* Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: 생성적 사전 훈련된 변압기에 대한 정확한 사후 훈련 양자화. _ arXiv preprint arXiv:2210.17323_.\n' +
      '* Fu et al. (2023) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. 2023. Mme: 종합 평가 벤치마크 for multimodal large language models. _ arXiv preprint arXiv:2306.13394_.\n' +
      '* Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, And Andy Zou. 2023. few-shot 언어 모델 평가를 위한 프레임워크.\n' +
      '\n' +
      '아미르 골라미, 김세훈, 진동, 쯔웨이 야오, 마이클 마호니, 커트 쿠처. 2022. 효율적인 신경망 추론을 위한 양자화 방법들에 대한 조사. _Low-Power Computer Vision_에서, 페이지 291-326. Chapman 및 Hall/CRC.\n' +
      '* Groeneveld et al. (2022) Dirk Groeneveld, Iz Beltagy, Pete Walsh, Oyvind Tafjord, A. Jha, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavich Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilsaka Ravichander, Saurabh Shah, Will Smith, Emma Strubell, Nitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dasigi, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hanna Hajishirzi. 2024. Olmo: 언어 모델의 과학을 가속화한다. _ arXiv preprint_.\n' +
      '* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. 대용량 멀티태스크 언어 이해도 측정. _ arXiv preprint arXiv:2009.03300_.\n' +
      '* Hoefler et al. (2021) Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. 2021. 딥러닝에서의 희소성: 신경망에서의 효율적인 추론 및 학습을 위한 프루닝 및 성장_ The Journal of Machine Learning Research_, 22(1):10882-11005.\n' +
      '* Hudson and Manning (2019) Drew A Hudson and Christopher D Manning. 2019. Gqa: real-world visual reasoning and compositional question answering을 위한 새로운 데이터셋. IEEE/CVF Conference on computer vision and pattern recognition_의 _Proceedings, pages 6700-6709.\n' +
      '* Lai et al. (2017) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. _ arXiv preprint arXiv:1704.04683_.\n' +
      '* Li et al. (2017) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Koectkov, Chenghao Mou, Kristopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgeni Zheltonzdziski, Terry Yue Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Joao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Nour Fahmy, Manan Dey, Jashan Zillerman, Muhtham Oblokulov, Jerhan Zahoni, Jerhan Zahoni, Jerhan Zahoni, Jerhan Zahoni, Rudra Murthy, Jiva Reddy, Daniel Fried, Dzmitry Bahdanau, Yan Hughes, Thomas Wolf, Arjun Guha, Leandro verra, Nicolas Gontier, 2023a. 출처가 당신과 함께 있기를!\n' +
      '* Li et al. (2023) Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023b. 교과서만 있으면 돼 **phi-1.5** 기술 보고서 arXiv preprint arXiv:2309.05463_.\n' +
      '* Lin et al.(2021a) Stephanie Lin, Jacob Hilton, and Owain Evans. 2021a. 진실: 모델이 인간의 거짓을 모방하는 방법을 측정하는 것 arXiv preprint arXiv:2109.07958_.\n' +
      '* Lin et al. (2021b) Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O\'Horo, Jeff Wang, Luke Zettlemoyer, Zornitska Kozareva, Mona T. 디아브, 베셀린 스토야노프, 시안 리 2021b. 다국어 언어 모델을 이용한 소수 학습 CoRR_, abs/2112.10668.\n' +
      '* Liu et al. (2023a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. 시각적 지시 조율\n' +
      '* Liu et al. (2023b) Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranian, Zhiqiang Shen, Xueqiang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, 및 Eric P. Xing. 2023b. Llm360: 완전히 투명한 오픈 소스 lms를 향한다.\n' +
      '* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. _ arXiv preprint arXiv:1711.05101_.\n' +
      '* Lu et al. (2022) Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. 설명하기 위한 학습: 과학 질문 답변을 위한 사고 체인을 통한 멀티모달 추론_ 신경 정보 처리 시스템_, 35:2507-2521의 발전.\n' +
      '* Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. 포인터 센티넬 혼합 모델_ arXiv preprint arXiv:1609.07843_.\n' +
      '* Nangia et al. (2020) Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R Bowman. 2020. 까마귀-쌍: 마스킹된 언어 모델에서 사회적 편향을 측정하기 위한 챌린지 데이터셋. _ arXiv preprint arXiv:2010.00133_.\n' +
      '* Pan et al. (2023) Bowen Pan, Rameswar Panda, Rogerio Schmidt Feris, and Aude Jeanne Oliva. 2023. 비전 트랜스포머에 대한 해석성-인식 중복성 감소. 미국 특허출원. 17/559,053\n' +
      '* Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. Falcon LLM에 대한 정제 웹 데이터세트: 웹 데이터 및 웹 데이터만으로 큐레이션된 말뭉치를 능가한다. _ arXiv preprint arXiv:2306.01116_.\n' +
      '* Pireso et al. (2023) Telmo Pessoa Pireso, Antonio V Lopes, Yannick Assogba, and Hendra Setiawan. 2023. 하나의 넓은 피드포워드만 있으면 됩니다 _ arXiv preprint arXiv:2309.01826_.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. IMT-2000 3GPP-기계학습에 관한 국제학술대회 - 페이지 8748-8763. PMLR.\n' +
      '* Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: 적대적 winograd schema challenge at scale. _ ACM_, 64(9):99-106의 통신.\n' +
      '* Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019. Socialiqa: Social interaction에 대한 상식 추론_ arXiv preprint arXiv:1904.09728_.\n' +
      '* Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. 메가트론-lm: 모델 병렬성을 이용한 수십억 파라미터 언어 모델 학습. _ arXiv preprint arXiv:1909.08053_.\n' +
      '* Singh et al. (2019) Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. 읽을 수 있는 vqa 모델을 향해. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 회의의 _Proceedings에서, 페이지 8317-8326.\n' +
      '* Su et al. (2024) Jianlin Su, Murtadah Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. 로포머: 회전 위치 매립을 구비한 개량형 트랜스포머. _ Neurocomputing_, 568:127063.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almhahiar, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_.\n' +
      '* Wu et al. (2023) Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2023. Lamini-lm: 대규모 지침에서 얻은 다양한 증류 모델 무리 _ CoRR_, abs/2304.14402.\n' +
      '* Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. 평활화: 대형 언어 모델에 대한 정확하고 효율적인 훈련 후 양자화. _International Conference on Machine Learning_, pages 38087-38099. PMLR.\n' +
      '*Xu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. 마법사: 복잡한 명령어를 따르도록 큰 언어 모델을 엠파워링하는 단계 _ arXiv preprint arXiv:2304.12244_.\n' +
      '* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, 요나탄 Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: 기계가 정말로 당신의 문장을 끝낼 수 있나요? _ arXiv preprint arXiv:1905.07830_.\n' +
      '* Zhang et al. (2024) Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. Tinylama: 오픈소스 소형 언어 모델 _ arXiv preprint arXiv:2401.02385_.\n' +
      '* Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: 미리 훈련된 트랜스포머 언어 모델을 개방한다.\n' +
      '* Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. 대형 언어 모델에 대한 설문조사. _ arXiv preprint arXiv:2303.18223_.\n' +
      '* Zheng et al. (2024) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. _ 신경 정보 처리 시스템_, 36의 발전.\n' +
      '* Zhu et al.(2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023. 대형 언어 모델에 대한 모델 압축에 관한 조사_ arXiv preprint arXiv:2308.07633_.\n' +
      '\n' +
      '## 부록, 부록\n' +
      '\n' +
      '### MobiLlama-Chat\n' +
      '\n' +
      '우리는 또한 모비라마-0.5B 및 모비라마-1.2B, 즉 모비라마-0.5B-채트 및 모비라마-1.2B-채트의 미세 조정된 버전을 개발했다. 우리는 우리의 모델을 훈련시키기 위해 AmberChat(Liu et al., 2023b)와 유사한 미세 조정 데이터 세트 및 하이퍼 파라미터를 사용했다. 우리는 WizardLM(Xu et al., 2023)에 의해 사용된 바와 같이 WizzardLM-196K 상에서 우리의 모델들을 미세 조정하기 위해 FastChat(Zheng et al., 2024)을 사용한다. 학습률은 \\(2\\times 10^{-5}\\), 기울기 누적 단계는 16, 워밍업 비율은 0.04로 유지하면서 8개의 A100s(80G)에서 3개의 에폭에 대한 기본 모델을 학습시켰다.\n' +
      '\n' +
      '### Qualitative Examples\n' +
      '\n' +
      'Rescribing\n' +
      '\n' +
      '**인간:** 마음챙김 명상 연습의 주요 이점은 무엇입니까?\n' +
      '\n' +
      'MobiLlama-0.5B:\n' +
      '\n' +
      '마음챙김 명상을 실천하는 것은 다음과 같은 몇 가지 이점을 가질 수 있다.\n' +
      '\n' +
      '1. 스트레스 및 불안 감소: 마음챙김 명상은 현재의 순간에 집중하고 산만함을 놓음으로써 스트레스와 불안을 줄이는 데 도움이 될 수 있다.\n' +
      '\n' +
      '2. 초점 및 집중력 향상: 마음챙김 명상은 현재 순간에 집중하고 산만함을 놓아주어 집중력과 집중력 향상에 도움을 줄 수 있다.\n' +
      '\n' +
      '3. 수면 개선: 마음챙김 명상은 스트레스와 불안감을 줄이고 휴식을 촉진함으로써 수면 개선에 도움을 줄 수 있다.\n' +
      '\n' +
      '4. 기분 개선: 마음챙김 명상은 스트레스와 불안을 줄이고 휴식을 촉진함으로써 기분 개선에 도움을 줄 수 있다.\n' +
      '\n' +
      '5. 더 나은 관계: 마음챙김 명상은 스트레스와 불안을 줄이고 휴식을 촉진함으로써 관계 개선에 도움이 될 수 있다.\n' +
      '\n' +
      'Knowledge\n' +
      '\n' +
      '**인간:** 집의 탄소 발자국을 줄이는 몇 가지 방법이 있나요?\n' +
      '\n' +
      'MobiLlama-0.5B:\n' +
      '\n' +
      '집의 탄소 발자국을 줄이는 것은 여러 가지 방법을 통해 달성할 수 있습니다. 여기 몇 가지 팁이 있습니다.\n' +
      '\n' +
      '1. 에너지 효율적인 조명 및 가전제품을 사용한다.\n' +
      '\n' +
      '2. 에너지 효율적인 창호 설치.\n' +
      '\n' +
      '3. 에너지 효율적인 가전기기 및 조명을 사용한다.\n' +
      '\n' +
      '4. 종이 및 기타 비생분해성 물질의 사용을 줄인다.\n' +
      '\n' +
      '5. 에너지 효율적인 가전제품 및 조명을 사용한다.\n' +
      '\n' +
      '6. 플라스틱 및 기타 일회용 재료의 사용을 줄입니다.\n' +
      '\n' +
      '7. 에너지 효율적인 가전제품 및 조명을 사용한다.\n' +
      '\n' +
      '8. 종이와 다른 일회용 재료의 사용을 줄입니다.\n' +
      '\n' +
      'Rescribing\n' +
      '\n' +
      '**인간:** 다양한 생태계가 지구의 전반적인 건강에 어떻게 기여하는가?\n' +
      '\n' +
      'MobiLlama-0.5B:\n' +
      '\n' +
      '다양한 문화와 전통은 여러 면에서 세계의 다양성에 기여한다. 예를 들어, 중국 문화는 가족, 전통, 그리고 환대를 강조하는 것으로 알려져 있습니다. 일본 문화는 개인주의, 창의성, 혁신에 중점을 둔 것으로 알려져 있다. 인도 문화는 전통, 헌신, 그리고 영성을 강조하는 것으로 알려져 있다.\n' +
      '\n' +
      '어두운 골목에서 밀러 형사는 뭔가 이상하다는 것을 알고 있었다. 공기는 긴장으로 가득 차 있었고, 그가 모퉁이를 돌면서, 그는 숨겨진 문을 발견하여 세계의 세계로 인도했다.\n' +
      '\n' +
      'MobiLlama-0.8B:\n' +
      '\n' +
      '\'곧 자신의 삶에서 드러날 엄청난 비밀\'\n' +
      '\n' +
      '"내가 이 책에 대해 처음 알게 된 것은 그것이 처음부터 끝까지 실제 이야기처럼 정말로 잘 쓰여졌다는 것입니다,"라고 "별 10개 중 10개에 대한 평점을 준 Goodreads에 대한 한 평론가는 말했다. 다른 작가가 쓴 또 다른 리뷰는 "이 소설은 여러 번 묘사되었지만 지금까지 전혀 내 관심을 끌지 못했다." "내가 이 책들을 왜 그렇게 좋아하는지, 아니면 단지 그 책들이 너무 잘 읽었기 때문인지 정확히 짚어낼 수 있을지 모르겠다! 그것들은 항상 신선함을 유지하는 온갖 종류의 뒤틀림을 가지고 있다!"라고 썼다.\n' +
      '\n' +
      '**Technological Evaluation**\n' +
      '\n' +
      '**인간:** 한 때 사람들이 인공지능을 달성하는 것은 불가능하다고 생각했지만, 지금은\n' +
      '\n' +
      'MobiLlama-0.8B:\n' +
      '\n' +
      '인공지능의 미래: 인공지능은 로봇 공학이나 심지어 인간의 개입 없이 스스로 운전할 수 있는 자율 주행 시스템과 같은 제조업에 대한 자동화를 통해 의학 및 의료에서 모든 측면에서 사용될 것이다.\n' +
      '\n' +
      '**Analytical Thinking**\n' +
      '\n' +
      '**Human:** 음악을 통한 시간여행의 개념을 나타내는 앨범 표지의 컨셉을 생성\n' +
      '\n' +
      'MobiLlama-0.8B:\n' +
      '\n' +
      '및 예술, 또는 이와 유사한 것:\n' +
      '\n' +
      '음악을 통한 시간 여행 - 한 사람만 들을 수 있다면 어떻게 역사로 돌아갈 수 있는지에 대한 가사가 있는 노래... 아니면 나뿐일지도 몰라! 나는 아직 내가 가장 좋아하는 것이 무엇인지 모르지만, 지금 유튜브에서 사람들이 그들의 과거 삶(비틀즈의 타임머신 같은)으로 되돌아가는 것에 대해 이야기하는 많은 노래들이 있다. 왜냐하면 그들은 오늘 우리가 여기에 왔을 때 죽은 누군가로부터 터무니없는 양의 돈을 받는 것에 대해 이야기하고 있었기 때문이다; 그러나 몇몇 예술가들은 카니예 웨스트가 작년에 했던 것처럼 단순히 "나는 당신의 모든 것을 원한다"라고 말하는 것보다 더 멀리 갔습니다. 그는 실제로 \'더 타이달 웨이브\'라고 불리는 자신의 악기를 만드는 데까지 갔습니다. 노래를 따라 부르면서 다른 몸 안에서 자신이 공연하는 것을 볼 수 있는 대신, 사용자가 자신에게 부착된 헤드폰을 사용하여 웨이브 기술을 통해 직접 무대로 이송되고, 그 후에 모든 사람들이 다시 한 번 모이는 것을 제외하고는 기본적으로 그가 라이브 쇼 동안 하는 것과 똑같이 보입니다! 이것은 또한 실제로 어떤 것을 의미할 수도 있습니다. 왜냐하면 대부분의 음악가들은 더 이상 현실 밖에서 공연하는 것을 깨닫지 못하기 때문입니다. 하지만 당신에게 가장 잘 맞는 것은 무엇이든지요!\n' +
      '\n' +
      '**Human:** 재귀현상을 이용하여 피보나치 시퀀스를 계산하는 함수를 구현하는 파이썬 코드 스니펫을 생성한다. 메모화를 사용하여 보다 효율적으로 만들 수 있다면 보너스 포인트입니다.\n' +
      '\n' +
      'MobiLlama-1.2B:\n' +
      '\n' +
      '네, 여기 파이썬 코드 조각이 있어요 재귀법을 이용해서 피보나치 수열을 계산하는 함수를 구현하죠\n' +
      '\n' +
      '**++**\n' +
      '\n' +
      '**def fibonacci(n):**\n' +
      '\n' +
      '**if n <= 1:**\n' +
      '\n' +
      '**return n**\n' +
      '\n' +
      '**else:**\n' +
      '\n' +
      '**return fibonacci(n-1) + fibonacci(n-2)**\n' +
      '\n' +
      '메모화를 사용하여 이 기능을 더 효율적으로 만들기 위해 우리는 피보나치 시퀀스의 이전 값을 사전에 저장할 수 있다. 메모화를 사용하는 기능의 업데이트된 버전이 있습니다:\n' +
      '\n' +
      '**def fibonacci(n, memo={}):**\n' +
      '\n' +
      '**if n in memo.**\n' +
      '\n' +
      '**return memo[n]**\n' +
      '\n' +
      '**elif n <= 1:**\n' +
      '\n' +
      '**return n**\n' +
      '\n' +
      '**else:**\n' +
      '\n' +
      '**memo[n] = fibonacci(n-1, memo) + fibonacci(n-2, memo)**\n' +
      '\n' +
      '**return memo[n]**\n' +
      '\n' +
      '이 함수는 숫자 n을 입력으로 하고 n번째 피보나치 숫자 또는 이미 계산된 값 n을 반환한다. n이 1보다 작거나 같으면 함수는 값 n을 반환한다. 그렇지 않으면, 그것은 순서에서 이전의 두 숫자로 재귀적으로 자신을 호출하고 그것들을 함께 추가한다. n의 값이 사전에 이미 있으면 함수는 n의 값을 직접 반환한다. 그렇지 않으면, 사전에 n의 값을 추가하고 메모된 값으로 n의 값을 반환한다.\n' +
      '\n' +
      '이 업데이트된 함수는 동일한 값을 여러 번 계산하는 것을 피하기 때문에 더 효율적이며, 이는 재귀 함수에서 병목 현상이 될 수 있다.\n' +
      '\n' +
      '**인간:** 알파벳의 모든 글자를 한 번 이상 사용하는 문장을 구성합니다.\n' +
      '\n' +
      'MobiLlama-1.2B:\n' +
      '\n' +
      '저는 10살 생일 파티 플래너일 뿐입니다. 저는 아이들을 위해 재미있고 기억에 남는 행사를 조직하고 계획하는 것을 좋아합니다. 저는 항상 고객을 위해 독특하고 흥미로운 경험을 만드는 도전과 사랑에 준비가 되어 있습니다.\n' +
      '\n' +
      '**인간:** 관광자와 기능 행성에 대한 여행 가이드와 함께 역할극을 작성한다.\n' +
      '\n' +
      'MobiLlama-1.2B:\n' +
      '\n' +
      '그래, 역할극이 있어\n' +
      '\n' +
      '안녕하세요, 지구에서 온 관광객입니다. 저는 기능 행성을 탐험하러 왔습니다. 그것에 대해 더 말씀해 주시겠어요?\n' +
      '\n' +
      '*** 여행 가이드: 물론, 기능성 행성은 독특하고 매혹적인 장소입니다. 과학과 마법이 충돌하는 세상, 물리 법칙이 우리 것과 다른 세상이다.\n' +
      '\n' +
      '멋지네요! 지구의 역사와 문화에 대해 자세히 알려주실 수 있나요?\n' +
      '\n' +
      '*** 여행 가이드: 물론, 이 행성은 인간의 지식의 한계를 탐구하고자 하는 과학자들에 의해 만들어졌습니다. 그들은 그들의 이론을 시험하기 위해 시뮬레이션을 만들었고, 시뮬레이션이 잘못되었다. 시뮬레이션은 과학과 마법이 얽힌 세상을 만들어냈고, 행성은 사람들이 함께 살고 일할 수 있는 공간이 됐다.\n' +
      '\n' +
      '멋지네요! 지구의 기술과 기반 시설에 대해 자세히 알려주실 수 있나요?\n' +
      '\n' +
      '*** 여행 가이드: 물론, 지구는 사람들이 시공간을 여행할 수 있는 첨단 기술을 가지고 있습니다. 또한 행성의 다른 부분을 연결하는 지하 터널 네트워크를 가지고 있습니다.\n' +
      '\n' +
      '멋지네요! 지구의 천연 자원과 자원에 대해 자세히 알려주실 수 있나요?\n' +
      '\n' +
      '*** 여행 가이드: 물론, 지구는 지구의 생존에 필수적인 방대한 천연 자원을 가지고 있습니다. 다양한 종의 본거지인 독특한 생태계를 가지고 있으며, 지구의 자원에 의존하는 번영하는 경제를 가지고 있다.\n' +
      '\n' +
      '멋지네요! 지구의 역사와 문화에 대해 자세히 알려주실 수 있나요?\n' +
      '\n' +
      '*** 여행 가이드: 물론, 이 행성은 시간의 시작으로 거슬러 올라가는 풍부한 역사를 가지고 있습니다. 그것은 과학과 마법의 균형을 기반으로 하는 복잡한 사회를 가지고 있다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>