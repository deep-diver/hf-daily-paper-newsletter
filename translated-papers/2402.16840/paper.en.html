<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT\n' +
      '\n' +
      'Omkar Thawakar\\({}^{1}\\)\n' +
      '\n' +
      'Equal contribution.\n' +
      '\n' +
      'Ashmal Vayani\\({}^{1}\\)1\n' +
      '\n' +
      'Salman Khan\\({}^{1,2}\\)\n' +
      '\n' +
      'Hisham Cholakal\\({}^{1}\\)\n' +
      '\n' +
      'Rao M. Anwer\\({}^{1,3}\\)\n' +
      '\n' +
      'Michael Felsberg\\({}^{5}\\)\n' +
      '\n' +
      'Tim Baldwin\\({}^{1,4}\\)\n' +
      '\n' +
      'Eric P. Xing\\({}^{1}\\)\n' +
      '\n' +
      'Fahad Shahbaz Khan\\({}^{1,5}\\)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '\\({}^{*}\\)_Bigger the better_\' has been the predominant trend in recent Large Language Models (LLMs) development. However, LLMs do not suit well for scenarios that require on-device processing, energy efficiency, low memory footprint, and response efficiency. These requisites are crucial for privacy, security, and sustainable deployment. This paper explores the \'_less is more_\' paradigm by addressing the challenge of designing accurate yet efficient Small Language Models (SLMs) for resource constrained devices. Our primary contribution is the introduction of an accurate and fully transparent open-source 0.5 billion (0.5B) parameter SLM, named _MobiLlama_, catering to the specific needs of resource-constrained computing with an emphasis on enhanced performance with reduced resource demands. _MobiLlama_ is a SLM design that initiates from a larger model and applies a careful parameter sharing scheme to reduce both the pre-training and the deployment cost. Our work strives to not only bridge the gap in open-source SLMs but also ensures full transparency, where complete training data pipeline, training code, model weights, and over 300 checkpoints along with evaluation codes is available at : [https://github.com/mbzuai-oryx/Mobillama](https://github.com/mbzuai-oryx/Mobillama).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Recent years have witnessed a tremendous surge in the development of Large Language Models (LLMs) with the emergence of prominent closed-source commercial models such as ChatGPT, Bard, and Claude. These LLMs exhibit surprising capabilities, typically called emergent abilities, towards solving complex tasks. Most existing popular LLMs follow a similar trend that bigger is always better, where scaling model size or data size typically provides improved model capacity and performance on downstream tasks. For instance, the recent LLama-2 70 billion (70B) model (Touvron et al., 2023) is considered more favorable in different chat applications due to its effectiveness towards handling dialogues, logical reasoning, coding, compared to its 7B counterpart which is typically better suited for basic tasks such as categorization or summaries. While these LLMs demonstrate impressive performance in handling complex language tasks, a key limitation is their size and computational requirements. For instance, the large-scale Falcon (Almazrouei et al., 2023) 180B model was trained using 4096 A100 GPUs and requires large memory and compute for deployment with dedicated high-performance servers and scalable storage systems.\n' +
      '\n' +
      'Recently, Small Language Models (SLMs) have shown potential in terms of providing decent performance with emergent abilities achieved at a significantly smaller scale compared to their large-scale LLM counterparts. Modern SLMs like Microsoft\'s Phi-2 2.7 billion (Li et al., 2023b) highlight the growing focus in the community on achieving more with less. SLMs offer advantages in terms of efficiency, cost, flexibility, and customizability. With fewer parameters, SLMs offer significant computational efficiency in terms of fast pre-training and inference with reduced memory and storage requirements. This is critical in real-world applications where efficient resource utilization is highly desired. It particularly opens up possibilities in resource-constrained computing, where the models are required to be memory efficient to operate on low-powered devices (e.g., edge). SLMs support on-device processing that enhances privacy, security, response time, and personalization. Such an integration can lead to advanced personal assistants, cloud-independent applications, and improved energy efficiency with a reduced carbon footprint.\n' +
      '\n' +
      'The landscape of language models, especially SLMs, is currently marked by a notable lack of open-source availability. While LLMs have garnered significant attention, the proprietary nature of most models has led to limited transparency and accessibility, particularly in the realm of SLMs. This gap hinders the scientific and technological exploration of these more efficient, compact and performant models. Recognizing this, there\'s a growing need in the community for fully transparent open-source SLMs, which would facilitate a deeper understanding of their capabilities and limitations and spur innovation by allowing broader community access to their architecture and reproducible training methodologies. We argue that bridging this gap is crucial for democratizing access to collaborative advancement for SLMs. Therefore, we investigate the problem of designing accurate yet efficient SLMs from scratch with the intention to provide full transparency in the form of access to entire training data pipeline and code, model weights, more than 300 checkpoints along with evaluation codes.\n' +
      '\n' +
      'When designing a SLM from scratch it is desired that the resulting model is accurate, while maintaining efficiency in terms of pre-training and deployment. A straightforward way is to scale-down a larger LLM design to the desired model size (e.g., 0.5B) by reducing either the size of the hidden dimension layers or the number of layers. We empirically observe both these design strategies to provide inferior performance. This motivates us to look into an alternative way of designing a SLM from scratch that is accurate yet maintains the efficiency, while offering full transparency.\n' +
      '\n' +
      '**Contributions:**\n' +
      '\n' +
      'We introduce a SLM framework, named _MobiLlama_, with an aim to develop accurate SLMs by alleviating the redundancy in the transformer blocks. Different to the conventional SLM design where dedicated feed forward layers (FFN) are typically allocated to each transformer block, we propose to employ a shared FFN design for all the transformer blocks within SLM. Our _MobiLlama_ leveraging a shared FFN-based SLM design is accurate and maintains efficiency, while offering full transparency in terms of data pipeline, training code, model weights and extensive intermediate checkpoints along with evaluation codes.\n' +
      '\n' +
      'We empirically show that our _MobiLlama_ performs favorably compared to conventional SLMs design schemes when performing pre-training from scratch. Our _MobiLlama_ 0.5B model outperforms existing SLMs of similar size on nine different benchmarks. _MobiLlama_ 0.5B achieves a gain of 2.4% in terms of average performance on nine benchmarks, compared to the best existing 0.5B SLM in the literature. We further develop a 0.8B SLM that originates from our 0.5B model by utilizing a wider shared-FFN scheme in transformer blocks, achieving top performance among existing SLMs falling under less than 1B parameters category. Lastly, we build multimodal models on top of our SLM to showcase visual perception and reasoning capabilities. Fig. 1 shows a comparison of our _MobiLlama_ with recent fully transparent relatively larger SLMs in terms of accuracy, pre-training complexity and on-board deployment cost.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'While LLMs have gained tremendous popularity (Zhao et al., 2023), one of their key limitations is the size and computational requirements both during pre-training and deployment. Another issue is limited availability of fully transparent opens-source LLMs that provide complete access to data pipeline, training code along with checkpoints and evaluation protocols. Prior works explore making several components of LLM framework efficient such as, attention mechanism (Dao, 2023) and optimization strategies (Loshchilov and Hutter, 2017). Further, existing efforts also include exploring post-training sparsification schemes (Ashkboos et al., 2024) or quantization (Hoefler et al., 2021; Zhu et al., 2023; Xiao et al., 2023) of computationally\n' +
      '\n' +
      'Figure 1: Comparison of our _MobiLlama_ 0.5B and 0.8B models with recent OLMo-1.17B (Groeneveld et al., 2024) and TinyLlama-1.1B (Zhang et al., 2024) in terms of pre-training tokens, pre-training time and memory, model parameters, overall accuracy across nine benchmarks and on-device efficiency (average battery consumption and average token/second on a PC with RTX2080Ti). Our _MobiLlama_ achieves comparable accuracy while requiring significantly fewer pre-training data (1.2T tokens vs. 3T tokens), lesser pre-training time and GPU memory along with being efficient in terms of deployment on a resource constrained device.\n' +
      '\n' +
      'expensive LLM. In several cases, such a post-hoc sparsification can reduce the performance of LLMs with more on-device memory consumption, compared to a SLM trained from scratch. Further, these techniques typically employ LLMs with limited transparency and accessibility.\n' +
      '\n' +
      'Recently, designing SLMs from scratch have gained attention (Biderman et al., 2023; Wu et al., 2023; Zhang et al., 2024; Li et al., 2023; Lin et al., 2021b; Shoeybi et al., 2019; Zhang et al., 2022). SLMs have shown potential as an alternative especially in case of limited pre-training compute as well as deployment in resource-constrained environments (e.g., edge devices). Further, SLMs can support on-device processing which in turn can enhance security, privacy, response efficiency, and personalization. Here, we strive to construct fully transparent accurate yet computationally efficient SLMs by maintaining the model\'s capacity to capture complex patterns and relationships in data while reducing the redundancy often present in the parameters of SLMs. Prior works (Frantar et al., 2022; Gholami et al., 2022; Pires et al., 2023; Pan et al., 2023; Bhojanapalli et al., 2021) exploring alleviating redundancy in transformer design either focusing on the attention mechanism or on the single feed-forward layer in BERT style architectures. Different from these approaches, we explore alleviating the redundancy in the SLM architectures with an LLM objective function by focusing on the sharing mechanism of MLP blocks having multiple feed-forward network (FFN) layers.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Baseline SLM Design\n' +
      '\n' +
      'We first describe our baseline 0.5B SLM architecture that is adapted from recent TinyLlama (Zhang et al., 2024) and Llama-2 (Touvron et al., 2023). The baseline architecture comprises \\(N\\) layers, where each layer consists of hidden dimensions of \\(M\\) and intermediate size (MLPs) of \\(5632\\). The vocabulary size is 32\\(K\\) and max. context length is \\(C\\). We consider two different design choices when constructing a 0.5B model from scratch. In first design choice, named \\(baseline1\\), the number of layer is set to \\(N=22\\) and hidden size of each layer is set to \\(M=1024\\). In second design choice, named \\(baseline2\\), we set the number of layer to \\(N=8\\) and hidden size of each layer is set to \\(M=2048\\).\n' +
      '\n' +
      'We note that both the aforementioned baseline designs struggle to strike an optimal balance between accuracy and efficiency. While a reduced size of hidden dimensions (1024) in case of \\(baseline1\\) aids in computational efficiency, it can likely hamper the model\'s capacity to capture complex patterns within the data. Such a reduction in dimension can potentially lead to a bottleneck effect, where the model\'s ability to represent intricate relationships and nuances in the data is constrained, thereby affecting the overall accuracy. On the other hand, reducing the number of hidden layers (22 to 8), as in the \\(baseline2\\), affects the model\'s depth that in turn hampers its ability to learn hierarchical representations of the language. Achieving superior performance on tasks requiring deeper linguistic comprehension and contextual analysis likely requires combining the advantages of the two aforementioned baselines. However, increasing the model capacity of \\(baseline1\\) and \\(baseline2\\) into a single model (22 layers and hidden dimension size of 2048) results in a significantly larger parameterized model of 1.2B with increased training cost (see Tab. 1). We name this larger model as _large-base_. Next, we present our proposed _MobiLlama_ 0.5B model design that does not reduce hidden dimension size in each layer (\\(baseline1\\)) or the total number of layers (\\(baseline2\\)), while maintaining a comparable training efficiency (see Tab. 1).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c} \\hline Model & \\#Params & Training Time & GPU Hours & GPU memory & No. of layers & Hidden dim size \\\\ \\hline _baseline1_ & 0.54B & 7.5 days & 28.8K & 3.2 GB & 22 & 1024 \\\\ _baseline2_ & 0.52B & 7 days & 26.9K & 3 GB & 8 & 2048 \\\\ _large-base_ & 1.2B & 12 days & 46.1K & 6 GB & 22 & 2048 \\\\ _MobiLlama_ & 0.52B & 7 days & 26.6K & 3 GB & 22 & 2048 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Comparison of our _MobiLlama_ with the two baselines and the large-base model. We show the comparison in terms of total number of parameters, training time, total GPU hours, GPU memory, number of transformer layers and the hidden dimension size in each layer. The numbers are computed on A100 GPUs with 80 GB memory each. Compared to _large-base_, our _MobiLlama_ reduces the GPU training hours by 42% along with a significant reduction in GPU memory with the same design configuration (number of layers and hidden dimension size etc.). Further, our _MobiLlama_ possesses increased model capacity in terms of number of layers and hidden dimension size while maintaining comparable training cost and parameters, compared to _baseline1_ and _baseline2_.\n' +
      '\n' +
      '### Proposed SLM Design: MobiLlama\n' +
      '\n' +
      'The proposed approach, _MobiLlama_, constructs a SLM of desired sizes (e.g., 0.5B model) by first initiating from a larger model size design, _large-base_. Then, we employ a careful parameter sharing scheme to reduce the model size to a pre-defined model configuration, thereby significantly reducing the training cost. Generally, both SLMs and LLMs typically utilize a dedicated multilayer perceptron (MLP) block comprising multiple feed forward network (FFN) layers within each transformer block. In such a configuration (e.g., _large-base_), the FFN layers account for a substantial \\(65\\%\\) of the total trainable parameters, with attention mechanisms and heads contributing \\(30\\%\\) and \\(5\\%\\), respectively. As a consequence, a significant number of parameters are concentrated within the FFN layers, thereby posing challenges during pre-training with respect to computational cost and the model\'s ability to achieve faster convergence. To address these issues, we propose to use a sharing scheme where the FFN parameters are shared across all transformer layers within the SLM. This enables us to significantly reduce the overall trainable parameters by \\(60\\%\\) in our _MobiLlama_, compared to the _large-base_. Such a significant parameter reduction also enables us to increase the model capacity in terms of number of layers and hidden dimension size without any substantial increase in the training cost (see Tab. 1).\n' +
      '\n' +
      'Fig. 2 compares our architecture design with two baselines. In case of both baselines, a dedicated MLP block that consists of multiple FFN layers is used in each transformer layer. Instead, our efficient _MobiLlama_ design utilizes a single MLP block which is shared across different layers of transformer within the SLM. This helps in increasing the model capacity without any increase in the total number of trainable parameters in the model.\n' +
      '\n' +
      '### Towards Fully Transparent MobiLlama\n' +
      '\n' +
      'As discussed earlier, fully transparent open-source SLM development is desired to foster a more inclusive, data/model provenance, and reproducible collaborative SLM research development environment. To this end, we present here pre-training dataset\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Subset & Tokens (Billion) \\\\ \\hline Arxiv & 30.00 \\\\ Book & 28.86 \\\\ C4 & 197.67 \\\\ Refined-Web & 665.01 \\\\ StarCoder & 291.92 \\\\ StackExchange & 21.75 \\\\ Wikipedia & 23.90 \\\\ \\hline Total & 1259.13 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Data mix in Amber-Dataset.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Hyperparameter & Value \\\\ \\hline Number Parameters & 0.5\\(B\\) \\\\ Hidden Size & 2048 \\\\ Intermediate Size (in MLPs) & 5632 \\\\ Number of Attention Heads & 32 \\\\ Number of Hidden Layers & 22 \\\\ RMSNorm \\(\\epsilon\\) & \\(1e^{-6}\\) \\\\ Max Seq Length & 2048 \\\\ Vocab Size & 32000 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: _MobiLlama_ architecture & hyperparameters.\n' +
      '\n' +
      'Figure 2: Illustrative comparison of our _MobiLlama_ with the two baselines. For each case, we show two transformer blocks denoted by different self-attention layers. In the case of both _baseline1_ and _baseline2_, a dedicated MLP block comprising three FFN layers is utilized for each transformer layer. In contrast, our _MobiLlama_ utilizes a single MLP block (highlighted by the same color) that is shared across different transformer layers. This enables to increase the capacity of the network in terms of layers and hidden dimension size without any significant increase in the total number of trainable parameters.\n' +
      '\n' +
      'and processing details, architecture design configuration with training details, evaluation benchmarks and metrics. In addition, we will publicly release complete training and evaluation codes along with intermediate model checkpoints.\n' +
      '\n' +
      '**Pre-training Dataset and Processing:** For pre-training, we use 1.2T tokens from LLM360 Amber dataset Liu et al. (2023). The Amber dataset provides a rich and varied linguistic landscape having different text types, topics, and styles. Tab. 2 shows the data mix from Amber dataset gathered from various sources.\n' +
      '\n' +
      '_Arxiv (30 Billion Tokens)_ subset is drawn from the repository of scientific papers, provides complex, domain-specific language and technical terminology, enriching the understanding of academic prose. _Book (28.9 Billion Tokens)_ subset comprises tokens from a broad range of literature with diverse narrative styles, cultural contexts, and rich vocabulary, deepening the grasp of storytelling and language nuances. _C4 (197.7 Billion Tokens)_ is the Colossal Clean Crawled Corpus (C4) that offers a vast and cleaned selection of web text, providing a broad linguistic foundation that includes various registers, styles, and topics. _Refined-Web (665 Billion Tokens)_ subset is a curated web crawl and offers the model exposure to contemporary, informal, and varied internet language, enhancing the relevance and applicability to modern communication. _StarCoder (291.9 Billion Tokens)_ subset is a vast collection used for code understanding featuring 783GB of code across 86 programming languages. It includes GitHub issues, Jupyter notebooks, and commits, totaling approximately 250 billion tokens. These are meticulously cleaned and de-duplicated for training efficiency. _StackExchange (21.8 Billion Tokens)_ is from the network of Q&A websites, this subset aids the model in learning question-answering formats and technical discussions across diverse topics. _Wikipedia (23.9 Billion Tokens)_ is an encyclopedia collection, it offers well-structured and factual content that helps the model to learn encyclopedic knowledge and formal writing styles.\n' +
      '\n' +
      'From the above-mentioned subsets, Arxiv, Book, C4, StackExchange and Wikipedia are sourced from RedPajama-v1 Computer (2023). The Amber dataset uses RefinedWeb Penedo et al. (2023) data to replace common_crawl subset of RedPajama-v1. These subsets amount to 1259.13 billion tokens.\n' +
      '\n' +
      'Initially, raw data sourced from the above sources is tokenized using Huggingface LLaMA tokenizer Touvron et al. (2023). Subsequently, these tokens are organized into sequences with each containing 2048 tokens. To manage data, these sequences are merged to the token sequences and divided the amalgamated dataset into 360 distinct segments. Each data segment, structured as a json file, carries an array of token IDs along with a source identifier that denotes the originating dataset. Each data sample is designed to have 2049 tokens.\n' +
      '\n' +
      '**Architecture Design:** Our _MobiLlama_ 0.5B comprises a hidden size of 2048, an intermediate size of 5632 in its MLPs, and operates with 32 attention heads across 22 hidden layers. It is designed to handle sequences up to 2048 tokens long, supported by a vocabulary size of 32,000. The precision in normalization is ensured by an RMSNorm epsilon of \\(1e^{-6}\\) to obtain a more stable training. We utilize RoPE Rotary Positional Embedding) Su et al. (2024) to encode positional information in our _MobiLlama_. Similar to Zhang et al. (2024), we employ a combination of Swish and Gated Lin\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c c c|c} \\hline Model Name & \\#Params & HellaSwag & Truthfulqa & MMLU & Arc\\_C & CrowsPairs & piga & race & siqa & winograde & Average \\\\ \\hline gpt-neo-125m & 0.15B & 30.26 & 45.58 & 25.97 & 22.95 & 61.55 & 62.46 & 27.56 & 40.33 & 51.78 & 40.93 \\\\ tiny-starcoder & 0.17B & 28.17 & 47.68 & 26.79 & 20.99 & 49.68 & 52.55 & 25.45 & 38.28 & 51.22 & 37.86 \\\\ cerebras-gpt-256m & 0.26B & 28.99 & 45.98 & 26.83 & 22.01 & 60.52 & 61.42 & 27.46 & 40.53 & 52.49 & 40.69 \\\\ opt-350m & 0.35B & 36.73 & 40.83 & 26.02 & 23.55 & 64.12 & 64.74 & 29.85 & 41.55 & 52.64 & 42.22 \\\\ megatom-gpt2-345m & 0.38B & 39.18 & 41.51 & 24.32 & 24.23 & 64.82 & 66.87 & 31.19 & 40.28 & 52.96 & 42.81 \\\\ LiteLlama & 0.46B & 38.47 & 41.59 & 26.17 & 24.91 & 62.90 & 67.73 & 28.42 & 40.27 & 49.88 & 42.26 \\\\ gpt-sw3-356m & 0.47B & 37.05 & 42.55 & 25.93 & 23.63 & 61.59 & 64.85 & 32.15 & 41.56 & 53.04 & 42.48 \\\\ pythia-410m & 0.51B & 40.85 & 41.22 & 27.25 & 26.19 & 64.20 & 67.19 & 30.71 & 41.40 & 53.12 & 43.57 \\\\ xglm-564m & 0.56B & 34.64 & 40.43 & 25.18 & 24.57 & 62.25 & 64.85 & 29.28 & 42.68 & 53.03 & 41.87 \\\\ Lamini-GTP-LM & 0.59B & 31.55 & 40.72 & 25.53 & 24.23 & 63.09 & 63.87 & 29.95 & 40.78 & 47.75 & 40.83 \\\\\n' +
      '**MobilLlama (Ours)** & 0.5B & 52.52 & 38.05 & 26.45 & 29.52 & 64.03 & 72.03 & 33.68 & 40.22 & 57.53 & **46.00** \\\\ \\hline Lamini-GTP-LM & 0.77B & 43.83 & 40.25 & 26.24 & 27.55 & 66.12 & 69.31 & 37.12 & 42.47 & 56.59 & 45.49 \\\\\n' +
      '**MobilLlama (Ours)** & 0.8B & 54.09 & 38.48 & 26.92 & 30.20 & 64.82 & 73.17 & 33.37 & 41.60 & 57.45 & **46.67** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: State-of-the-art comparisons with existing < _1B params models_ on _nine_ benchmarks. In case of around 0.5B model series, our _MobiLlama_ achieves a substantial gain of 2.4% in terms of average performance on nine benchmarks. Further, our _MobiLlama_ 0.8B model achieves an average score of 46.67.\n' +
      '\n' +
      'ear Units together as activation functions. Tab. 3 presents details of our model configuration. We also derive a 0.8B version from our _MobiLlama_ by widening the shared FFN design. Compared to the 0.5B model, our 0.8B design increases the hidden dimension size to 2532 and the intermediate size to 11,080 while the rest of the configuration is same.\n' +
      '\n' +
      'For pre-training of our _MobiLlama_, we use a public cluster having 20 GPU nodes each equipped with 8 NVIDIA A100 GPUs with 80 GB memory each and 800 Gbps interconnect for model training. Each GPU is interconnected through 8 NVLink links, complemented by a cross-node connection configuration of 2 port 200 Gb/sec (4\\(\\times\\) HDR) InfiniBand, optimizing the model\'s training process. To further enhance the training efficiency, we employ flash-attention mechanism and follow the pre-training hyper-parameters established by the LLAMA (Touvron et al., 2023) model. Our _MobiLlama_ model\'s training is performed using the AdamW optimizer, leveraging hyperparameters \\(\\beta_{1}=0.9,\\beta_{2}=0.95\\), with an initial learning rate of \\(\\eta=3e^{-4}\\). This rate follows a cosine learning rate schedule, tapering to a final rate of \\(\\eta=3e^{-5}\\). We further incorporate a weight decay of \\(0.1\\) and apply gradient clipping at \\(1.0\\) with a warm-up period over \\(2,000\\) steps. Adapting to our hardware configuration of 20 GPU nodes, we optimize the pre-training batch size to \\(800\\) (\\(160\\times 5\\)), achieving a throughput of approximately 14\\(k\\)-15\\(k\\) tokens per second on a single GPU. During our model pre-training, we save intermediate checkpoints after every 3.3B tokens which will be publicly released.\n' +
      '\n' +
      '**Evaluation Benchmarks and Metrics:**\n' +
      '\n' +
      'For a comprehensive performance evaluation, we use nine different benchmarks from the Open LLM Leaderboard1.\n' +
      '\n' +
      'Footnote 1: [https://huggingface.co/spaces/huggingface/open_llm_leaderboard](https://huggingface.co/spaces/huggingface/open_llm_leaderboard)\n' +
      '\n' +
      'HellaSwag (Zellers et al., 2019) assesses the model\'s ability to predict the correct ending to a scenario from a set of possible continuations, thereby testing common sense reasoning. TruthfulQA (Lin et al., 2021) evaluates the model to provide truthful answers, focusing on its understanding of facts and its ability to avoid deception. MMLU (Hendrycks et al., 2020) measures the model\'s broad knowledge across numerous subjects such as, humanities, science, technology, engineering and management. ARC_Challenge (Clark et al., 2018) tests complex reasoning with science questions. CrowsPairs (Nangia et al., 2020) evaluates the model\'s biases by comparing sentences that differ only by the demographic group mentioned, aiming for fairness. PIQA (Bisk et al., 2020) evaluates the model\'s physical commonsense knowledge, requiring understanding of everyday physical processes. Race (Lai et al., 2017) assesses reading comprehension through multiple-choice questions based on passages. SIQA (Sap et al., 2019) focuses on the model\'s social commonsense reasoning and its understanding of social dynamics. Winogrande (Sakaguchi et al., 2021) evaluates the model\'s ability to resolve ambiguities in text, testing its commonsense reasoning.\n' +
      '\n' +
      'Following the Analysis-360 framework (Liu et al., 2023) that is built on llvm-harness (Gao et al., 2023), we conduct extensive evaluations under the standard settings with varying shots for detailed assessments, validating the model\'s robustness and adaptability across diverse linguistic tasks. Following the standard evaluation protocol, our evaluation setting consists of 10, 25, 5 and 5 shot evaluation for Hellaswag, ARC_Challenge, Winogrande and MMLU, while zero-shot for rest of the benchmarks.\n' +
      '\n' +
      '## 4 Results\n' +
      '\n' +
      '**Baseline Comparison:** We first present a comparison with the two baselines in Tab. 5) for 0.5B model series. For the baseline evaluation, we pre-train all the models on the same 100B tokens from the Amber dataset and report the results on four benchmarks: HellaSwag, TruthfulQA, MMLU, and Arc_C. Our _MobiLlama_ achieves favourable performance compared to the two baselines by achieving an average score of 34.4 over the four benchmarks. We note that this performance improvement is achieved without any significant increase in the training cost (see Tab. 1), highlighting the merits of the proposed SLM design.\n' +
      '\n' +
      '**State-of-the-art Comparison:** We compare our _MobiLlama_ 0.5B and 0.8B with existing SLMs having comparable (less than 1B) parameters: gpt\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c} \\hline \\hline Model & \\multicolumn{2}{c}{HellaSwag} & Truthfulqa & MMLU & Arc\\_C & Average \\\\ \\hline _baseline1_ & 42.44 & 38.46 & 25.08 & 26.18 & 33.04 \\\\ _baseline2_ & 42.15 & 38.70 & 25.73 & 26.10 & 33.17 \\\\ _MobiLlama_ & 44.47 & 40.12 & 26.48 & 26.53 & **34.40** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Baseline comparison on four benchmarks. Here, both the baselines and our _MobiLlama_ comprise the same parameters (0.5B) and are pre-trained on 100B tokens from Amber. Our _MobiLlama_ achieves favorable performance compared to the two baselines, while operating on a similar training budget.\n' +
      '\n' +
      'neo (Black et al., 2021), tiny-starcoder (Li et al., 2023a), cerebras-gpt (Dey et al., 2023), opt (Zhang et al., 2022), megatron-gpt-2 (Shoeybi et al., 2019), LiteLlama, gpt-sw3, pythia (Biderman et al., 2023), xglm (Lin et al., 2021b), Lamini-LM (Wu et al., 2023). Among existing methods falling around 0.5B model series category, pythia-410m achieves an average score of 43.57. Our _MobiLlama_ 0.5B model achieves superior performance with an average score of 46.0, outperforming pythia-410m by 2.4% in terms of average performance on nine benchmarks. Notably, _MobiLlama_ achieves superior performance on the HellaSwag benchmark which is designed to evaluate the model\'s capabilities in the NLP text completion task. Further, _MobiLlama_ also performs favorably on commonsense reasoning tasks with superior results on piqa and winogrande benchmarks. Further, our _MobiLlama_ 0.8B model achieves an average score of 49.06.\n' +
      '\n' +
      '**Efficiency Comparison:** We present the comparison of our model in terms of efficiency and resource consumption on various low-end hardware platforms: a PC with RTX-2080Ti GPU, a laptop with i7 CPU, a laptop with i7 CPU, and a smartphone with Snapdragon-685 processor. Tab. 6 shows the comparison of our _MobiLlama_ 0.5B with _large-base_ 1.2B, Llama2-7B (Touvron et al., 2023) and Phi2-2.7B (Li et al., 2023b) model, in terms of the average processing speed in tokens per second (Average Tokens/Sec), average memory consumption (Avg Memory Consumption) in megabytes (MB), and the average battery consumption (Average Battery Consumption/1000 Tokens) in milliampere-hours (mAH). Our _MobiLlama_ performs favorably in terms of efficiency across different hardware platforms.\n' +
      '\n' +
      'We further perform an efficiency comparison to a recent post-training sparsification scheme (Ashkboos et al., 2024), where each weight matrix is substituted with a smaller (dense) matrix, thereby reducing dimensions of the embeddings in the model. In such a scheme, the parameters of the original LLM are reduced significantly up to 70% followed by post-slicing fine-tuning using a dataset such as WikiText-2 (Merity et al., 2016). Tab. 7 shows the comparison of our _MobiLlama_ with existing LLMs (e.g., Llama-2-7B, OPT-6.7B) on four benchmarks following (Ashkboos et al., 2024). Our _MobiLlama_\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Platform} & \\multirow{2}{*}{Model} & \\#Params & \\multirow{2}{*}{Precision} & Avg Tokens/Sec & Avg Memory & Avg Battery Consumption & CPU \\\\  & & (\\(\\downarrow\\)) & & (\\(\\uparrow\\)) & Consumption (\\(\\downarrow\\)) & /1k Tokens (\\(\\downarrow\\)) & Utilization (\\(\\downarrow\\)) \\\\ \\hline \\multirow{4}{*}{RTX2080Ti} & Llama2 & 7B & bf16 & 14.85 & 27793 MB & 135.51 mAH & 31.62\\% \\\\  & Phi2 & 2.7B & bf16 & 32.19 & 12071 MB & 59.13 mAH & 24.73\\% \\\\  & _large-base_ & 1.2B & bf16 & 50.61 & 6254 MB & 18.91 mAH & 18.25\\% \\\\  & _MobiLlama_ & 0.5B & bf16 & **63.38** & **3046** MB & **8.19 mAH** & **14.79\\%** \\\\ \\hline \\multirow{4}{*}{CPU-i7} & Llama2 & 7B & 4bit & 5.96 & 4188 MB & 73.5 mAH & 49.16\\% \\\\  & Phi2 & 2.7B & 4bit & 22.14 & 1972 MB & 27.36 mAH & 34.92\\% \\\\  & _large-base_ & 1.2B & 4bit & 29.23 & 1163 MB & 10.81 mAH & 30.84\\% \\\\  & _MobiLlama_ & 0.5B & 4bit & **36.32** & **799** MB & **4.86** mAH & **24.64**\\% \\\\ \\hline \\multirow{4}{*}{Snapdragon-685} & Llama2 & 7B & 4bit & 1.193 & 4287 MB & 10.07 mAH & 77.41\\% \\\\  & Phi2 & 2.7B & 4bit & 2.882 & 1893 MB & 14.61 mAH & 56.82\\% \\\\ \\cline{1-1}  & _large-base_ & 1.2B & 4bit & 6.687 & 780 MB & 6.00 mAH & 17.15\\% \\\\ \\cline{1-1}  & _MobiLlama_ & 0.5B & 4bit & **7.021** & **770** MB & **5.32** mAH & **13.02**\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Comparison in terms of efficiency and resource consumption on different low-end hardware devices. We show the comparison on: a PC with RTX-2080Ti GPU, a laptop with i7 CPU and a smartphone with Snapdragon-685 processor. In addition to our _large-base_ model, we also present the comparison with Llama2 7B and Phi2 2.7B. In case of CPU and smartphone, we use 4-bit GGUF format of the corresponding models, whereas the original models are deployed and tested on PC with RTX-2080Ti GPU. The different metrics measure the model’s operational efficiency, model’s footprint in the device’s RAM and the energy efficiency of processing 1,000 tokens. Our _MobiLlama_ performs favorably in terms of efficiency on these low-end hardware devices. We note that both Phi2 and Llama2 are not fully transparent in that the complete data pipeline for pre-training is not publicly available.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Model & GQA & SQA & TextQA & MME \\\\ \\hline _MobiLlama-\\(V\\)_ & 58.5 & 53.1 & 41.4 & 1191.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Quantitative performance of our multimodal design, _MobiLlama-\\(V\\)_ 0.8B, on different benchmarks.\n' +
      '\n' +
      '_lama_ 0.5B and 0.8B models perform favorably against representative LLMs, with an average score of 53.72 computed over four benchmarks. These results highlight the potential of designing new fully transparent SLMs that can achieve comparable capabilities of their larger sliced model counterparts.\n' +
      '\n' +
      '**Multimodal MobiLlama:** We further build a multimodal model on top of our _MobiLlama_ by combining it with a vision encoder to develop a general-purpose visual assistant having visual reasoning capabilities. Our multimodal model, _MobiLlama-V_, is trained by bridging the visual encoder of CLIP (Radford et al., 2021) with the language decoder of our _MobiLlama_, and fine-tuning it in an end-to-end fashion on a 665k vision-language instruction set (Liu et al., 2023a). We conduct evaluation on GQA (Hudson and Manning, 2019), SQA (Lu et al., 2022), TextQA (Singh et al., 2019), and MME (Fu et al., 2023). Tab. 8 shows the performance of _MobiLlama-V_ 0.8B model.\n' +
      '\n' +
      '**Qualitative Analysis:** Fig. 3 shows example responses obtained when interacting with _MobiLlama_ 0.5B with conversation capabilities. We show examples covering different tasks such as, text completion, code generation and conversation capabilities. Our model generates faithful responses to these diverse interactions. Fig. 4 shows examples demonstrating visual reasoning capabilities of our multimodal _MobiLlama-V_. For instance, _MobiLlama-V_ accurately describes the atypical aspects of the image when asked to describe the given image.\n' +
      '\n' +
      '**Evaluating Large-base Model:** As discussed ear\n' +
      '\n' +
      'Figure 4: Example responses of _MobiLlama-V_ in responding to visual stimuli across a range of scenarios.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c c c c|c} \\hline \\hline Model & \\#Params & HellaSwag & Truthfulua & MMLU & Arc\\_C & CrowsPairs & piqa & race & siqa & winogrande & Average \\\\ \\hline Boomer & 1B & 31.62 & 39.42 & 25.42 & 22.26 & 61.26 & 57.99 & 28.99 & 40.32 & 50.98 & 39.80 \\\\ Pythia-Dedup & 1B & 49.63 & 38.92 & 24.29 & 29.09 & 67.11 & 70.23 & 32.44 & 42.63 & 53.98 & 45.36 \\\\ Falcon-RW & 1B & 63.12 & 35.96 & 25.36 & 35.06 & 69.04 & 74.10 & 36.07 & 40.23 & 61.88 & 48.98 \\\\ TinyLlama & 1.1B & 60.22 & 37.59 & 26.11 & 33.61 & 70.60 & 73.28 & 36.45 & 41.65 & 59.18 & 48.74 \\\\ OLMo & 1.2B & 62.50 & 32.94 & 25.86 & 34.45 & 69.59 & 73.70 & 36.74 & 41.14 & 58.90 & 48.42 \\\\ Cerebras-GPT & 1.3B & 38.51 & 42.70 & 26.66 & 26.10 & 63.67 & 66.75 & 30.33 & 42.42 & 53.59 & 43.41 \\\\ Lamini & 1.3B & 38.05 & 36.43 & 28.47 & 26.62 & 64.62 & 67.89 & 33.39 & 43.19 & 50.59 & 43.25 \\\\ OPT & 1.3B & 54.50 & 38.67 & 24.63 & 29.6 & 70.70 & 72.47 & 34.16 & 42.47 & 59.74 & 47.43 \\\\ GPT-NEO & 1.3B & 48.49 & 39.61 & 24.82 & 31.31 & 65.67 & 71.05 & 34.06 & 41.81 & 57.06 & 45.98 \\\\ Pythia-Deduped & 1.4B & 55.00 & 38.63 & 25.45 & 32.59 & 67.33 & 72.68 & 34.64 & 42.68 & 56.90 & 47.32 \\\\ _large-base_ & 1.2B & 62.99 & 35.90 & 24.79 & 34.55 & 68.49 & 75.57 & 35.31 & 41.96 & 62.03 & **49.06** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: Comprehensive comparisons with existing < _2B params fully open-source LLM models_ on 9 benchmarks. Our 1.2B _large-base_ model pre-trained on 1.2T tokens achieves superior performance compared to both the recent OLMo 1.17B model (Groeneveld et al., 2024) and TinyLlama 1.1B model (Zhang et al., 2024), which are pre-trained on a substantially larger data of 3T tokens.\n' +
      '\n' +
      'Figure 3: Example responses from our _MobiLlama_ across a variety of tasks, including creative storytelling, coding exercises, economic analysis, and cooking instructions. The responses highlight the models’ ability to engage with both abstract concepts and practical, step-by-step processes, demonstrating its broad applicability.\n' +
      '\n' +
      'lier, we strive to develop fully transparent models for democratization of SLMs and fostering future research. To this end, we compare our _large-base_ 1.2B with existing fully transparent SLMs falling within the less than 2B category. Tab. 9 shows that compared to recent OLMo and TinyLlama that are pre-trained on a larger dataset of 3T tokens, our _large-base_ 1.2B model pre-trained on 1.2T tokens achieves favourable results with an average score of 49.06 over nine benchmarks. We hope that our _large-base_ model will serve as a solid baseline and help ease future research in SLM development.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'We present a fully transparent SLM, _MobiLlama_, that alleviates redundancy in the transformer block. Within _MobiLlama_, we propose to utilize a shared FFN design for all the blocks within the SLM. We evaluate _MobiLlama_ on nine benchmarks, achieving favourable results compared to existing methods falling under less than 1B category. We also build a multimodal model on top of _MobiLlama_ SLM to demonstrate visual reasoning capabilities.\n' +
      '\n' +
      '**Limitation and Future Direction:** A potential direction is to further improve _MobiLlama_ for enhanced context comprehension. While _MobiLlama_ offers a fully transparent SLM framework, a follow-up study to understand any misrepresentations and biases is desired to improve model\'s robustness.\n' +
      '\n' +
      '## 6 Acknowledgement\n' +
      '\n' +
      'The computations were enabled by the Berzelius resource provided by the Knut and Alice Wallenberg Foundation at the National Supercomputer Centre. We thank Sahal Shaji Mullappilly and Muhammad Maaz for their support in the evaluations on mobile platform and VLM training.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Coffinet, Daniel Hesslow, Julien Launay, Quentin Malatric, Daniele Mazzotta, Baderdidine Noune, Baptiste Pannier, and Guillemre Penedo. 2023. The falcon series of open language models.\n' +
      '* Ashkboos et al. (2024) Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. 2024. Slicegpt: Compress large language models by deleting rows and columns. _arXiv preprint arXiv:2401.15024_.\n' +
      '* Bhojanapalli et al. (2021) Srinadh Bhojanapalli, Ayan Chakrabarti, Andreas Veit, Michal Lukasik, Himanshu Jain, Frederick Liu, Yin-Wen Chang, and Sanjiv Kumar. 2021. Leveraging redundancy in attention with reuse transformers. _arXiv preprint arXiv:2110.06821_.\n' +
      '* Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herble Bradley, Kyle O\'Brien, Eric Halhan, Mohammad Affah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In _International Conference on Machine Learning_, pages 2397-2430. PMLR.\n' +
      '* Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in natural language. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 7432-7439.\n' +
      '* Black et al. (2021) Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadata.\n' +
      '* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_.\n' +
      '* Computer (2023) Together Computer. 2023. Redpajama: An open source recipe to reproduce llama training dataset.\n' +
      '* Dao (2023) Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. _arXiv preprint arXiv:2307.08691_.\n' +
      '* Dey et al. (2023) Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness, et al. 2023. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. _arXiv preprint arXiv:2304.03208_.\n' +
      '* Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers. _arXiv preprint arXiv:2210.17323_.\n' +
      '* Fu et al. (2023) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. 2023. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_.\n' +
      '* Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation.\n' +
      '\n' +
      'Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. 2022. A survey of quantization methods for efficient neural network inference. In _Low-Power Computer Vision_, pages 291-326. Chapman and Hall/CRC.\n' +
      '* Groeneveld et al. (2022) Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavich Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilsaka Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hanna Hajishirzi. 2024. Olmo: Accelerating the science of language models. _arXiv preprint_.\n' +
      '* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_.\n' +
      '* Hoefler et al. (2021) Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. 2021. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. _The Journal of Machine Learning Research_, 22(1):10882-11005.\n' +
      '* Hudson and Manning (2019) Drew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6700-6709.\n' +
      '* Lai et al. (2017) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. _arXiv preprint arXiv:1704.04683_.\n' +
      '* Li et al. (2017) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Koectkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgeni Zheltonzdziski, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Joao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhthasam Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Haliley Scholkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023a. Starcoder: may the source be with you!\n' +
      '* Li et al. (2023) Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023b. Textbooks are all you need ii: **phi-1.5** technical report. _arXiv preprint arXiv:2309.05463_.\n' +
      '* Lin et al. (2021a) Stephanie Lin, Jacob Hilton, and Owain Evans. 2021a. Truthfulqa: Measuring how models mimic human falsehoods. _arXiv preprint arXiv:2109.07958_.\n' +
      '* Lin et al. (2021b) Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O\'Horo, Jeff Wang, Luke Zettlemoyer, Zornitska Kozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. 2021b. Few-shot learning with multilingual language models. _CoRR_, abs/2112.10668.\n' +
      '* Liu et al. (2023a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. Visual instruction tuning.\n' +
      '* Liu et al. (2023b) Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranian, Zhiqiang Shen, Xueqiang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric P. Xing. 2023b. Llm360: Towards fully transparent open-source lms.\n' +
      '* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_.\n' +
      '* Lu et al. (2022) Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. _Advances in Neural Information Processing Systems_, 35:2507-2521.\n' +
      '* Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. _arXiv preprint arXiv:1609.07843_.\n' +
      '* Nangia et al. (2020) Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R Bowman. 2020. Crows-pairs: A challenge dataset for measuring social biases in masked language models. _arXiv preprint arXiv:2010.00133_.\n' +
      '* Pan et al. (2023) Bowen Pan, Rameswar Panda, Rogerio Schmidt Feris, and Aude Jeanne Oliva. 2023. Interpretability-aware redundancy reduction for vision transformers. US Patent App. 17/559,053.\n' +
      '* Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. _arXiv preprint arXiv:2306.01116_.\n' +
      '* Pireso et al. (2023) Telmo Pessoa Pireso, Antonio V Lopes, Yannick Assogba, and Hendra Setiawan. 2023. One wide feedforward is all you need. _arXiv preprint arXiv:2309.01826_.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR.\n' +
      '* Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106.\n' +
      '* Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019. Socialiqa: Commonsense reasoning about social interactions. _arXiv preprint arXiv:1904.09728_.\n' +
      '* Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. _arXiv preprint arXiv:1909.08053_.\n' +
      '* Singh et al. (2019) Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards vqa models that can read. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8317-8326.\n' +
      '* Su et al. (2024) Jianlin Su, Murtadah Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almhahiar, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.\n' +
      '* Wu et al. (2023) Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2023. Lamini-lm: A diverse herd of distilled models from large-scale instructions. _CoRR_, abs/2304.14402.\n' +
      '* Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In _International Conference on Machine Learning_, pages 38087-38099. PMLR.\n' +
      '* Xu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. _arXiv preprint arXiv:2304.12244_.\n' +
      '* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? _arXiv preprint arXiv:1905.07830_.\n' +
      '* Zhang et al. (2024) Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. Tinylama: An open-source small language model. _arXiv preprint arXiv:2401.02385_.\n' +
      '* Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models.\n' +
      '* Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. _arXiv preprint arXiv:2303.18223_.\n' +
      '* Zheng et al. (2024) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. _Advances in Neural Information Processing Systems_, 36.\n' +
      '* Zhu et al. (2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023. A survey on model compression for large language models. _arXiv preprint arXiv:2308.07633_.\n' +
      '\n' +
      '## Appendix A Appendix\n' +
      '\n' +
      '### MobiLlama-Chat\n' +
      '\n' +
      'We also developed the fine-tuned versions of MobiLlama-0.5B and MobiLlama-1.2B, namely MobiLlama-0.5B-Chat and MobiLlama-1.2B-Chat. We used the similar fine-tuning dataset and hyper-parameters as AmberChat (Liu et al., 2023b) to train our models. We use FastChat (Zheng et al., 2024) to fine-tune our models on WizzardLM-196K as used by WizardLM (Xu et al., 2023). We trained our base models for 3 epochs on 8 A100s (80G), keeping the learning rate as \\(2\\times 10^{-5}\\), gradient accumulation steps as 16, and warmup ratio as 0.04.\n' +
      '\n' +
      '### Qualitative Examples\n' +
      '\n' +
      'Rescribing\n' +
      '\n' +
      '**Human:** What are the key benefits of practicing mindfulness meditation?\n' +
      '\n' +
      'MobiLlama-0.5B:\n' +
      '\n' +
      'Practicing mindfulness meditation can have several benefits, including:\n' +
      '\n' +
      '1. Reduced stress and anxiety: Mindfulness meditation can help reduce stress and anxiety by focusing on the present moment and letting go of distractions.\n' +
      '\n' +
      '2. Improved focus and concentration: Mindfulness meditation can help improve focus and concentration by focusing on the present moment and letting go of distractions.\n' +
      '\n' +
      '3. Better sleep: Mindfulness meditation can help improve sleep by reducing stress and anxiety and promoting relaxation.\n' +
      '\n' +
      '4. Better mood: Mindfulness meditation can help improve mood by reducing stress and anxiety and promoting relaxation.\n' +
      '\n' +
      '5. Better relationships: Mindfulness meditation can help improve relationships by reducing stress and anxiety and promoting relaxation.\n' +
      '\n' +
      'Knowledge\n' +
      '\n' +
      '**Human:** What are some ways to reduce the carbon footprint of your home?\n' +
      '\n' +
      'MobiLlama-0.5B:\n' +
      '\n' +
      'Reducing the carbon footprint of your home can be achieved through several ways. Here are some tips:\n' +
      '\n' +
      '1. Use energy-efficient lighting and appliances.\n' +
      '\n' +
      '2. Install energy-efficient windows and doors.\n' +
      '\n' +
      '3. Use energy-efficient appliances and lighting.\n' +
      '\n' +
      '4. Reduce the use of paper and other non-biodegradable materials.\n' +
      '\n' +
      '5. Use energy-efficient appliances and lighting.\n' +
      '\n' +
      '6. Reduce the use of plastic and other single-use materials.\n' +
      '\n' +
      '7. Use energy-efficient appliances and lighting.\n' +
      '\n' +
      '8. Reduce the use of paper and other single-use materials.\n' +
      '\n' +
      'Rescribing\n' +
      '\n' +
      '**Human:** How do diverse ecosystems contribute to the overall health of the planet?\n' +
      '\n' +
      'MobiLlama-0.5B:\n' +
      '\n' +
      'Different cultures and traditions contribute to the diversity of the world in many ways. For example, the Chinese culture is known for its emphasis on family, tradition, and hospitality. The Japanese culture is known for its emphasis on individualism, creativity, and innovation. The Indian culture is known for its emphasis on tradition, devotion, and spirituality.\n' +
      '\n' +
      '**Human:** Create the opening paragraph of a suspenseful thriller: In the dimly lit alley, Detective Miller knew something was off. The air was thick with tension, and as he turned the corner, he discovered a hidden door leading to a world of\n' +
      '\n' +
      'MobiLlama-0.8B:\n' +
      '\n' +
      'icy secrets that would soon be revealed in his own life."\n' +
      '\n' +
      '"The first thing I noticed about this book is how well-written it really felt like an actual story from beginning to end," said one reviewer on Goodreads who also gave her rating for "10 out of ten stars". Another reviewed by another author wrote:"This novel has been described many times over but never quite captured my attention until now...I\'m not sure if you can put your finger on what makes me love these books so much or maybe just because they are such good reads! They have all sorts of twists along their way which keeps them fresh every time!"\n' +
      '\n' +
      '**Technological Evaluation**\n' +
      '\n' +
      '**Human:** Once people considered Artificial Intelligence is something impossible to achieve, but now\n' +
      '\n' +
      'MobiLlama-0.8B:\n' +
      '\n' +
      'The future of artificial intelligence: AI will be used in every aspect from medicine and healthcare through automation for manufacturing industries like robotics or even autonomous driving systems that can drive themselves without human intervention (or at least with less risk).\n' +
      '\n' +
      '**Analytical Thinking**\n' +
      '\n' +
      '**Human:** Create a concept for an album cover that represents the concept of time travel through music\n' +
      '\n' +
      'MobiLlama-0.8B:\n' +
      '\n' +
      'and art, or something similar to this:\n' +
      '\n' +
      'Time Travel Through Music - A song with lyrics about how you can go back in history if only one person would listen...or maybe it\'s just me! I don\'t know what my favorite is yet but there are so many songs out now on YouTube where people talk about going backwards into their past lives (like "Time Machine" by The Beatles) which makes sense because they were talking about getting ridiculous amounts of money from someone who was dead when we get here today; however some artists have gone further than simply saying "I want all your stuff" like Kanye West did last year...he actually went as far as creating his own musical instrument called \'The Tidal Wave\'. It basically looks exactly like he does at times during his live shows except instead of being able to see himself perform inside another body while singing along, users will be transported directly onto stage via wave technology using headphones attached to them..and then afterward everyone gets together again once more!! This could also mean anything really though since most musicians do not even realize themselves performing outside of reality anymore anyway....but whatever works best for YOU!!!\n' +
      '\n' +
      '**Human:** Generate a Python code snippet that implements a function to calculate the Fibonacci sequence using recursion. Bonus points if you can make it more efficient using memoization.\n' +
      '\n' +
      'MobiLlama-1.2B:\n' +
      '\n' +
      'Sure, here\'s a Python code snippet that implements a function to calculate the Fibonacci sequence using recursion:\n' +
      '\n' +
      '**++**\n' +
      '\n' +
      '**def fibonacci(n):**\n' +
      '\n' +
      '**if n <= 1:**\n' +
      '\n' +
      '**return n**\n' +
      '\n' +
      '**else:**\n' +
      '\n' +
      '**return fibonacci(n-1) + fibonacci(n-2)**\n' +
      '\n' +
      'To make this function more efficient using memoization, we can store the previous values of the Fibonacci sequence in a dictionary. Here\'s an updated version of the function that uses memoization:\n' +
      '\n' +
      '**def fibonacci(n, memo={}):**\n' +
      '\n' +
      '**if n in memo:**\n' +
      '\n' +
      '**return memo[n]**\n' +
      '\n' +
      '**elif n <= 1:**\n' +
      '\n' +
      '**return n**\n' +
      '\n' +
      '**else:**\n' +
      '\n' +
      '**memo[n] = fibonacci(n-1, memo) + fibonacci(n-2, memo)**\n' +
      '\n' +
      '**return memo[n]**\n' +
      '\n' +
      'This function takes a number n as input and returns the nth Fibonacci number, or the value n if it\'s already been calculated. If n is less than or equal to 1, the function returns the value n. Otherwise, it recursively calls itself with the previous two numbers in the sequence and adds them together. If the value of n is already in the dictionary, the function returns the value of n directly. Otherwise, it adds the value of n to the dictionary and returns the value of n with the memoized value.\n' +
      '\n' +
      'This updated function is more efficient because it avoids calculating the same value multiple times, which can be a bottleneck in recursive functions.\n' +
      '\n' +
      '**Human:** Construct a sentence that uses every letter of the alphabet at least once.\n' +
      '\n' +
      'MobiLlama-1.2B:\n' +
      '\n' +
      'I am just a 10-year-old\'s birthday party planner. I love organizing and planning fun and memorable events for kids. I am always up for a challenge and love to create unique and exciting experiences for my clients.\n' +
      '\n' +
      '**Human:** Write a role-play with a tourist and a travel guide for a functional planet.\n' +
      '\n' +
      'MobiLlama-1.2B:\n' +
      '\n' +
      'Sure, here\'s a role-play:\n' +
      '\n' +
      '*** Tourist: Hi, I\'m a tourist from Earth. I\'m here to explore the functional planet. Can you tell me more about it?\n' +
      '\n' +
      '*** Travel Guide: Sure, the functional planet is a unique and fascinating place. It\'s a world where science and magic collide, where the laws of physics are different from our own.\n' +
      '\n' +
      '*** Tourist: That sounds amazing! Can you tell me more about the planet\'s history and culture?\n' +
      '\n' +
      '*** Travel Guide: Sure, the planet was created by a group of scientists who wanted to explore the limits of human knowledge. They created a simulation to test their theories, and the simulation went wrong. The simulation created a world where science and magic intertwined, and the planet became a place where people could live and work together.\n' +
      '\n' +
      '*** Tourist: That sounds amazing! Can you tell me more about the planet\'s technology and infrastructure?\n' +
      '\n' +
      '*** Travel Guide: Sure, the planet has advanced technology that allows people to travel through time and space. It also has a network of underground tunnels that connect different parts of the planet.\n' +
      '\n' +
      '*** Tourist: That sounds amazing! Can you tell me more about the planet\'s natural resources and resources?\n' +
      '\n' +
      '*** Travel Guide: Sure, the planet has a vast array of natural resources that are essential for the planet\'s survival. It has a unique ecosystem that is home to a variety of species, and it has a thriving economy that relies on the planet\'s resources.\n' +
      '\n' +
      '*** Tourist: That sounds amazing! Can you tell me more about the planet\'s history and culture?\n' +
      '\n' +
      '*** Travel Guide: Sure, the planet has a rich history that dates back to the beginning of time. It has a complex society that is based on a balance between science and magic.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>