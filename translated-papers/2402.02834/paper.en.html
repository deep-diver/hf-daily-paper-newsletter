<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Shortened LLaMA: A Simple Depth Pruning for Large Language Models\n' +
      '\n' +
      ' Bo-Kyeong Kim\\({}^{1}\\) Geonmin Kim\\({}^{1}\\) Tae-Ho Kim\\({}^{1}\\) Thibault Castells\\({}^{1}\\)\n' +
      '\n' +
      'Shinkook Choi\\({}^{1}\\) Junho Shin\\({}^{1}\\) Hyoung-Kyu Song\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Nota Inc.\n' +
      '\n' +
      '{bokyeong.kim, geonmin.kim, thkim, thibault, shinkook.choi, junho.shin, hyoungkyu.song}@nota.ai\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width _vs._ depth) concerning their impact on LLM inference efficiency. In this work, we show that a simple depth pruning approach can compete with recent width pruning methods in terms of zero-shot task performance. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. We hope this work can help deploy LLMs on local and edge devices.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'The advancement of large language models (LLMs) [23, 24, 25, 26] has brought significant improvements in language-based tasks, enabling versatile applications such as powerful chatbots [23, 25]. However, the deployment of LLMs is constrained by their intensive computational demands. To make LLMs more accessible and efficient for practical use, various optimization strategies have been actively studied over recent years (see [27, 28] for survey). This work focuses on _structured_ pruning [12, 13], which removes groups of unnecessary weights and can facilitate hardware-agnostic acceleration.\n' +
      '\n' +
      'In the context of compressing billion-parameter LLMs, LLM-Pruner [14] and FLAP [11] narrow the network width by pruning coupled structures (e.g., attention heads and their associated weight connections) while maintaining the number of layers. Sheared-LLaMA [15] reduces not only the network width but also its depth by entirely removing some layers. Despite the existence of pruning methods [15, 16, 17] that incorporate both width and depth aspects, there remains a gap in detailed analysis comparing these two factors (width _vs._ depth), specifically in relation to their impact on LLM inference efficiency.\n' +
      '\n' +
      'In addition to substantial model sizes, LLM inference is distinguished by an autoregressive decoding mechanism, which predicts tokens one by one based on the input and the previously generated tokens. This sequential generation process often exhibits a memory-bound nature, leading to considerable underutilization of GPU compute abilities [26, 18]. While expanding batch sizes is a standard way to enhance GPU utilization and throughput, this approach is unfeasible for low-specification GPUs with memory constraints. We aim to improve inference speeds of LLMs, especially under hardware limitations that demand small batch sizes, where we observe that width-only pruning methods are inadequate.\n' +
      '\n' +
      'Depth pruning is often regarded as being less effective in performance compared to width pruning, due to the elimination of bigger and coarse units. Contrary to this prevailing view, we show that a simple depth pruning method coupled with a LoRA retraining phase [14]\n' +
      '\n' +
      'Figure 1: Efficiency of pruned LLaMA-7B models on an NVIDIA H100 GPU. Compared to width pruning of FLAP [11] and LLM-Pruner [14], our depth pruning achieves faster inference with competitive PPL on WikiText2 (left) and offers a better latency-throughput trade-off (right; \\(M\\): batch size). See Section C for additional results.\n' +
      '\n' +
      'can rival recent width pruning studies for LLMs, including LLM-Pruner [14], FLAP [1], and a structured pruning variant of Wanda [15, 16], in terms of zero-shot task capabilities. Moreover, we present that depth pruning markedly improves inference speeds, particularly when hardware restrictions necessitate running LLMs with limited batch sizes (see Figure 1). Our contributions are summarized as follows:\n' +
      '\n' +
      '* In scenarios with limited batch sizes, our work demonstrates that width pruning is difficult to attain actual speedups in LLM\'s autoregressive generation. This aspect has been underexplored in previous works.\n' +
      '* We introduce a simple yet effective strategy for depth pruning of LLMs. We explore various design factors, including the choice of prunable units, the criteria for importance evaluation, and the retraining frequency.\n' +
      '* Our compact LLMs, obtained by excluding several Transformer blocks, achieve inference acceleration. They are for general-purpose use and perform comparably to finely width-pruned models in zero-shot tasks.\n' +
      '\n' +
      '## 2 Problem: Small-batch LLM Inference\n' +
      '\n' +
      'Most LLMs are autoregressive models that sequentially produce tokens, based on the initial prompt and the sequence of tokens previously generated. The token-by-token generation process often involves multiplying large matrices (weights) with smaller matrices or vectors (activations). The primary bottleneck for inference efficiency is memory access operations rather than the speed of mathematical computations (referred to as\'memory-bound\'), leading to suboptimal use of GPU computing power [17]. Though increasing batch sizes is a standard way to enhance GPU computation and throughput, it poses a risk of out-of-memory (OOM) errors, as depicted in Figure 2,1 unless advanced system-level optimizations [17, 18, 19] are applied.\n' +
      '\n' +
      'Footnote 1: Using the HF-Transformers library [16], we ran the LLMs with \\(12\\) input tokens for \\(20\\) batched runs after \\(10\\) warm-ups. Top: Peak GPU compute utilization [17]. Bottom: Mean latency over \\(20\\) runs.\n' +
      '\n' +
      'In this study, our focus is on accelerating the inference of LLMs under small-batch conditions caused by hardware restrictions. Such situations are relevant for deploying LLMs on memory-constrained local devices, which can enhance user experience and data privacy protection. We show that (i) reducing weight shapes via width pruning does not improve generation speeds and can even degrade it when the resulting weight dimensions are unsuitable for GPU capabilities, and (ii) notable speed gains are only achievable through depth pruning that excludes a number of modules entirely.\n' +
      '\n' +
      'Figure 3: Comparison of pruning granularities. Width pruning reduces the size of weight matrices while maintaining the number of matrix-level operations. Depth pruning eliminates entire Transformer blocks, or individual MHA and FFN modules, leading to fewer memory accesses and matrix-level operations.\n' +
      '\n' +
      'Figure 2: Top: GPU compute utilization of (a)–(c) running LLaMA-7B on different NVIDIA GPUs and that of (d) Vicuna-13B. LLM inference is typically constrained by memory access operations, resulting in lower GPU compute usage. Increasing batch sizes can enhance GPU utilization and throughput, but pushing this too far triggers OOM issues. Bottom: Latency results with varying batch sizes and target output lengths (labeled with \\(L\\)). Our depth pruning (blue lines) improves generation speeds over the original models (gray), while width pruning [18] is ineffective (green). The dotted lines show that pruned models can operate with larger batch sizes that cause OOM errors for the original model. The results are obtained with pruning ratios of \\(27\\%\\) for the 7B model and \\(29\\%\\) for the 13B model.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '## 3 Method: Block Pruning\n' +
      '\n' +
      'An LLM is a stack of multiple Transformer blocks [23], each of which contains a pair of multi-head attention (MHA) and feed-forward network (FFN) modules (see Figure 3). We choose this Transformer block as the prunable unit to prioritize reducing inference latency. Our approach is simple and cheap: after identifying unimportant blocks with straightforward metrics, we perform one-shot pruning and light retraining.\n' +
      '\n' +
      '### Evaluation of Block-level Importance\n' +
      '\n' +
      'We consider the following criteria to evaluate the significance of each block, ultimately selecting the Taylor+ and PPL metrics (see Table 5). Specifically, the linear weight matrix is denoted as \\(\\mathbf{W}^{k,n}=\\left[W_{i,j}^{k,n}\\right]\\) with a size of \\((d_{\\mathrm{out}},d_{\\mathrm{in}})\\), where \\(k\\) represents the type of operation (e.g., a query projection in MHA or an up projection in FFN) within the \\(n\\)-th Transformer block. The weight importance scores are calculated at the output neuron level [23], followed by summing2 these scores to assess the block-level importance.\n' +
      '\n' +
      'Footnote 2: In our exploration of various aggregation strategies (i.e., sum, mean, product, and max operations across module and block levels), summing the scores was effective at different pruning ratios.\n' +
      '\n' +
      '**Magnitude (Mag).** This metric [15] is a fundamental baseline in the pruning literature, assuming that weights with smaller norms are less informative. For the block-level analysis, we compute \\(I_{\\mathrm{Magnitude}}^{n}=\\sum_{k}\\sum_{i}\\sum_{j}\\left|W_{i,j}^{k,n}\\right|\\).\n' +
      '\n' +
      '**Taylor.** Assessing the error caused by the removal of a weight parameter helps in identifying its significance. For a given calibration dataset \\(D\\), this can be expressed as the alteration in the training loss \\(\\mathcal{L}\\)[16, 17]: \\(\\left|\\mathcal{L}(W_{i,j}^{k,n};D)-\\mathcal{L}(W_{i,j}^{k,n}=0;D)\\right| \\approx\\frac{\\partial\\mathcal{L}(D)}{\\partial W_{i,j}^{k,n}}W_{i,j}^{k,n}\\), where we omit the second-order derivatives by following [14]. We define the block score as \\(I_{\\mathrm{Taylor}}^{n}=\\sum_{k}\\sum_{i}\\sum_{j}\\left|\\frac{\\partial\\mathcal{ L}(D)}{\\partial W_{i,j}^{k,n}}W_{i,j}^{k,n}\\right|\\).\n' +
      '\n' +
      '**Mag+ and Taylor+.** Upon using the aforementioned metrics, the early blocks are labeled as unimportant, but their removal leads to severe performance drops. Similar to a popular heuristic [17, 16], we preserve the first four and the last two blocks [14] by excluding them from the pruning candidates.\n' +
      '\n' +
      '**Perplexity (PPL).** Redundant blocks contribute less to the model\'s outputs, and their removal leads to smaller degradation in PPL, a commonly used metric for language modeling tasks. In this context, we physically eliminate each block and monitor its influence on PPL using the calibration set \\(D\\): \\(I_{\\mathrm{PPL}}^{n}=\\exp\\left\\{-\\frac{1}{SL}\\sum_{s}\\sum_{l}\\log p_{\\theta^{n }}(x_{l}^{(s)}|x_{<l}^{(s)})\\right\\}\\), where \\(\\theta^{n}\\) denotes the model without its \\(n\\)-th block, and \\(s=1,\\dots,S\\) and \\(l=1,\\dots,L\\) are the indices for sequences and tokens in \\(D\\). The use of PPL can reflect the model\'s behavior by being derived from the next-token prediction loss; it requires only the forward pass, avoiding the need to compute back-propagation gradients [14] and Hessian inverses [23], or to involve a mask learning stage [14]. As shown in Figure 4, several blocks are identified as removable, showing only a slight effect on the PPL metric. The elimination of initial and final blocks significantly degrades the performance, which necessitates keeping them unpruned.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c} \\hline \\multicolumn{1}{c|}{Model} & \\#Param & \\#Block\\({}^{\\dagger}\\) & \\#Head\\({}^{\\dagger}\\) & FFN-D\\({}^{\\dagger}\\) \\\\ \\hline \\multicolumn{1}{c|}{Original 7B} & 6.7B & 32 & 32 & 11008 \\\\ \\hline \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Wanda-sp} & 5.5B & 32 & 26 & 8807 \\\\ \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{FLAP} & 5.4B & 32 & 26.94\\({}^{\\dagger}\\) & 8577.44\\({}^{\\dagger}\\),2078 \\\\ \\multicolumn{1}{c|}{} & LLM-Pruner & 5.4B & 32 & 24 & 8256 \\\\ \\cline{2-5} \\multicolumn{1}{c|}{} & Ours & 5.5B & 26 & 32 & 11008 \\\\ \\hline \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Wanda-sp} & 4.9B & 32 & 23 & 7816 \\\\ \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{FLAP} & 4.9B & 32 & 24.64\\({}^{\\dagger}\\),86 & 7497.12\\({}^{\\dagger}\\),2358.0 \\\\ \\multicolumn{1}{c|}{} & LLM-Pruner & 4.9B & 32 & 21 & 7155 \\\\ \\cline{2-5} \\multicolumn{1}{c|}{} & Ours & 4.9B & 23 & 32 & 11008 \\\\ \\hline \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Wanda-sp} & 4.5B & 32 & 21 & 7156 \\\\ \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{FLAP} & 4.5B & 32 & 23.08\\({}^{\\dagger}\\) & 6781.12440.6 \\\\ \\multicolumn{1}{c|}{} & LLM-Pruner & 4.4B & 32 & 18 & 6054 \\\\ \\cline{2-5} \\multicolumn{1}{c|}{} & Ours & 4.5B & 21 & 32 & 11008 \\\\ \\hline \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Original 13B} & 13.0B & 40 & 40 & 13824 \\\\ \\hline \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Wanda-sp} & 10.5B & 40 & 32 & 11060 \\\\ \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{FLAP} & 10.5B & 40 & 33.78\\({}^{\\dagger}\\) & 10778.742316.0 \\\\ \\multicolumn{1}{c|}{} & LLM-Pruner & 10.3B & 40 & 30 & 10368 \\\\ \\cline{2-5} \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Ours} & 10.5B & 32 & 40 & 13824 \\\\ \\hline \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Wanda-sp} & 9.5B & 40 & 29 & 9954 \\\\ \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{FLAP} & 9.5B & 40 & 31.14\\({}^{\\dagger}\\),06 & 9570.842601.0 \\\\ \\multicolumn{1}{c|}{} & LLM-Pruner & 9.2B & 40 & 26 & 8985 \\\\ \\cline{2-5} \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Ours} & 9.5B & 29 & 40 & 13824 \\\\ \\hline \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Wanda-sp} & 8.4B & 40 & 26 & 8710 \\\\ \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{FLAP} & 8.3B & 40 & 27.51\\({}^{\\dagger}\\),13 & 8326.62374.9 \\\\ \\multicolumn{1}{c|}{} & LLM-Pruner & 8.2B & 40 & 22 & 7603 \\\\ \\cline{1-1} \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Ours} & 8.3B & 25 & 40 & 13824 \\\\ \\hline \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{\\({}^{\\dagger}\\)Reduction ratio for the number of parameters.} & & & & \\\\ \\multicolumn{1}{c|}{\\({}^{\\dagger}\\)Block: \\#Transformer blocks; \\#Head: \\#attention heads of MHA; FFN-D: intermediate size of FFN.} & & & \\\\ \\cline{1-1} \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Ours} & 9.5B & 29 & 40 & 13824 \\\\ \\hline \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Wanda-sp} & 8.4B & 40 & 26 & 8710 \\\\ \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{FLAP} & 8.3B & 40 & 27.51\\({}^{\\dagger}\\),13 & 8326.62374.9 \\\\ \\multicolumn{1}{c|}{} & LLM-Pruner & 8.2B & 40 & 22 & 7603 \\\\ \\hline \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{Ours} & 8.3B & 25 & 40 & 13824 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Pruned architectures on LLaMA-7B and Vicuna-{7B, 13B}-v1.3. While Wanda-sp [23, 23, 23], and LLM-Pruner [14] reduce the network width, our method reduces the network depth. Using LLM-Pruner’s module-level pruning ratios of (25%, 35%, 45%) as benchmarks, we adjust others for comparable parameter numbers.\n' +
      '\n' +
      'Figure 4: Estimated importance of each Transformer block on the calibration set. Blocks with lower\n' +
      '\n' +
      '### One-shot Pruning\n' +
      '\n' +
      'After sorting the block-level importance scores, we prune the less crucial blocks in a single step. Since every block has an identical configuration and it is easy to calculate the number of parameters for one block, we readily decide how many blocks should be removed to meet the target model size.\n' +
      '\n' +
      '### Cost-efficient Retraining\n' +
      '\n' +
      'We efficiently retrain the pruned models with the low-rank adaptation (LoRA) method [11, 10]. The weight matrix of the adapted network is expressed as \\(W_{0}+\\Delta W=W_{0}+BA\\), where \\(W_{0}\\) denotes the initial pretrained weight with a shape of \\((d_{\\mathrm{out}},d_{\\mathrm{in}})\\). The update matrix \\(\\Delta W\\) is decomposed into two trainable parts, \\(B\\) and \\(A\\) with dimensions \\((d_{\\mathrm{out}},r)\\) and \\((r,d_{\\mathrm{in}})\\), where \\(r\\) represents a low rank. We demonstrate that LoRA has the potential to restore the performance of depth-pruned models.\n' +
      '\n' +
      'LoRA-based retraining can be efficiently completed on a single GPU in just a few hours. For example, retraining a model pruned by 20% from 7B parameters takes about 2 hours and 22GB VRAM, while a model reduced by 21% from 13B demands around 3 hours and 35GB VRAM.\n' +
      '\n' +
      '## 4 Experimental Setup\n' +
      '\n' +
      'Model.Our testbed includes LLaMA-7B [13] and Vicuna-{7B, 13B}-v1.3 [14], which are famous open-source LLMs.\n' +
      '\n' +
      'Baseline.We compare the two pruning units, network width _vs._ depth, using the same calibration dataset. The width pruning baseline methods are described below, and we utilize their official code for implementation. Table 1 shows the pruned architectures under similar numbers of parameters.3\n' +
      '\n' +
      'Footnote 3: We used the parameter numbers from LLM-Pruner’s module-level pruning ratios of (25%, 35%, 45%) as the reference and adjusted the pruning ratios for our method and the other baselines.\n' +
      '\n' +
      '* LLM-Pruner [11] employs a Taylor-based importance metric to remove attention heads from MHA and intermediate neurons from FFN. Local pruning is performed to select removable groups within the same module while maintaining uniform dimensions across the examined blocks. Adhering to their practice, the first and last few blocks remain unpruned. Their pruned models and ours are identically retrained with LoRA.\n' +
      '* FLAP [1] uses a fluctuation-based importance metric to explore the recoverability of feature maps after removing weight columns. Global pruning\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c|c c|c c} \\hline \\hline \\multirow{3}{*}{Model} & \\multicolumn{3}{c|}{Zero-shot Performance} & \\multicolumn{3}{c|}{H100 80GB\\({}^{\\ddagger}\\)} & \\multicolumn{3}{c}{RTX3090 24GB\\({}^{\\ddagger}\\)} \\\\ \\cline{2-7}  & \\multicolumn{2}{c|}{PPL\\(\\downarrow\\)} & \\multicolumn{2}{c|}{Ave Acc\\(\\uparrow\\)} & \\multicolumn{2}{c|}{Latency\\(\\downarrow\\)} & \\multicolumn{2}{c|}{Throughput\\(\\uparrow\\)} & \\multicolumn{2}{c}{Latency\\(\\downarrow\\)} & \\multicolumn{2}{c}{Throughput\\(\\uparrow\\)} \\\\  & WikiText2 & PTB & (\\%)\\({}^{\\dagger}\\) & (s) & (tokens/s) & (s) & (tokens/s) \\\\ \\hline \\multicolumn{7}{c}{LLaMA-7B (6.7B)} & 12.6 & 22.1 & 66.3 & 2.4 & 53.7 & 5.1 & 25.0 \\\\ \\hline \\multirow{7}{*}{20\\%} & Wanda-sp & 21.4 & 47.2 & 51.8 & 3.1 & 41.7 & 7.6 & 16.7 \\\\  & FLAP & **17.0** & **30.1** & 59.5 & 3.2 & 40.5 & 7.7 & 16.5 \\\\ Pruned & LLM-Pruner & 17.6 & 30.4 & 61.8 & 3.0 & 43.2 & 6.0 & 21.4 \\\\ (5.5B) & Ours: Taylor+ & 20.2 & 32.3 & **63.5** & **1.9** & **66.0** & **4.5** & **28.4** \\\\  & Ours: PPL & 17.7 & 30.7 & 61.9 & **1.9** & **66.0** & **4.5** & **28.4** \\\\ \\hline \\multirow{7}{*}{27\\%} & Wanda-sp & 50.4 & 106.9 & 42.1 & 3.1 & 41.7 & 8.1 & 16.0 \\\\  & FLAP & 21.3 & 37.1 & 55.8 & 3.2 & 40.2 & 7.8 & 16.5 \\\\ Pruned & LLM-Pruner & **20.5** & 36.1 & 58.7 & 2.9 & 44.0 & 5.6 & 22.9 \\\\ (4.9B) & Ours: Taylor+ & 29.9 & 42.0 & **59.8** & **1.7** & **73.9** & **3.7** & **34.9** \\\\  & Ours: PPL & 20.7 & **36.0** & 57.6 & **1.7** & **73.9** & **3.7** & **34.9** \\\\ \\hline \\multirow{7}{*}{35\\%} & Wanda-sp & 133.6 & 210.1 & 36.9 & 3.1 & 41.6 & 8.0 & 16.1 \\\\  & FLAP & 25.6 & 44.4 & 52.7 & 3.2 & 40.5 & 8.1 & 15.8 \\\\ \\cline{1-1}  & LLM-Pruner & 24.2 & 40.7 & **55.5** & 2.9 & 44.4 & 6.1 & 21.1 \\\\ (4.5B) & Ours: Taylor+ & 33.2 & 58.5 & 55.4 & **1.6** & **80.1** & **3.4** & **37.8** \\\\ \\cline{1-1}  & Ours: PPL & **23.1** & **38.8** & 55.2 & **1.6** & **80.1** & **3.4** & **37.8** \\\\ \\hline \\hline \\end{tabular} \\({}^{\\dagger}\\)Average accuracy on seven commonsense reasoning tasks.\n' +
      '\n' +
      '\\({}^{\\ddagger}\\)Measured with 12 input tokens, 128 output tokens, and a batch size of 1 on a single GPU.\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 2: Zero-shot results of the compressed LLaMA-7B. The width pruning methods of Wanda-sp [23, 14], and LLM-Pruner [11] often degrade inference efficiency due to the GPU-unfriendly weight sizes [1]. In contrast, our depth pruning approach enhances generation speed and competes well in zero-shot task performance. See Section A for detailed results.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c|c} \\hline \\hline \\multirow{3}{*}{Model} & \\multicolumn{3}{c|}{Zero-shot Performance} \\\\ \\cline{2-4}  & & \\multicolumn{2}{c|}{PPL\\(\\downarrow\\)} & \\multicolumn{2}{c}{Ave Acc\\(\\uparrow\\)} \\\\  & & WikiText2 & PTB & (\\%)\\({}^{\\dagger}\\) \\\\ \\hline Vicuna-7B-v1.3 (6.7B) & 17.1 & 63.2 & 65.9 \\\\ \\hline \\multirow{20}{*}{20\\%} & Wanda-sp & 24.4 & 104.0 & 58.5 \\\\  & FLAP & 22.0 & 74.9 & 61.4 \\\\ Pruned & LLM-Pruner & 19.6 & 76.4 & 60.1 \\\\ (5.5B) & Ours: Taylor+ & 21.0 & 72.3 & **62.5** \\\\  & Ours: PPL & **18.8** & **67.9** & 60.7 \\\\ \\hline \\multirow{3}{*}{27\\%} & Wanda-sp & 36.5 & 177.6 & 50.9 \\\\  & FLAP & 27.9 & 88.3 & 57.1 \\\\ \\cline{1-1}  & LLM-Pruner & **22.7** & 87.9 & 57.1 \\\\ (4.9B) & Ours: Taylor+ & 29.8 & 92.0 & **60.2** \\\\ (4.9B) & Ours: PPL & 23.0 & **78.2** & 56.1 \\\\ \\hline \\multirow{3}{*}{35\\%} & Wanda-sp & 73.2 & 386.5 & 39.4 \\\\ (4.9B) & FLAP & 34.6 & 104.8 & 53.7 \\\\ \\cline{1-1}  & LLM-Pruner & 27.6 & 102.0 & 53.5 \\\\ (4.5B) & Ours: Taylor+ & 35.0 & 110.3 & **55.0** \\\\ \\cline{1-1}  & Ours: PPL & **26.6** & **89.4** & 53.3 \\\\ \\hline \\hline \\end{tabular} \\({}^{\\dagger}\\)Average accuracy on seven commonsense reasoning tasks.\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 3: Zero-shot results of the compressed Vicuna-7B. See Section A for detailed results.\n' +
      '\n' +
      'is applied, leading to different widths over distinct modules (see Table 1 for mean and standard deviation values). Instead of retraining, extra bias terms are added into pruned feature maps for performance restoration.\n' +
      '* Wanda-sp is presented in [1] as a variant of Wanda [24] adjusted for structured pruning. The original metric was based on the product of weight magnitudes and input activation norms, which can be interpreted as addressing a local reconstruction objective. Wanda-sp extends this metric in a structured way while using common dimensions among different modules.\n' +
      '\n' +
      'Data.Following [11], we randomly select 10 samples from BookCorpus [12] to compute block-level significance during the pruning stage. We also use this calibration dataset for the baseline methods to ensure a fair comparison. At the LoRA retraining stage, 50K samples of the refined Alpaca [1] are used.\n' +
      '\n' +
      'Evaluation.Following [13], we measure zero-shot accuracy on commonsense reasoning datasets (i.e., BoolQ [1], PIQA [15], HelaSwag [11], WinoGrande [16], ARC-easy [1], ARC-challenge [10], and OpenbookQA [12]) using the Im-evaluation-harness package [1]. We also report zero-shot PPL on WikiText2 [14] and PTB [13].\n' +
      '\n' +
      'Latency and Throughput.We follow [15] to measure the metrics. Given a batch size \\(M\\) and an output sequence length \\(L\\) (excluding the input length), the latency \\(T\\) represents the time required to handle the given prompts and produce \\(ML\\) output tokens. The throughput is computed as \\(ML/T\\). We report the average results from 20 runs after the initial 10 warm-up batches.\n' +
      '\n' +
      'Implementation.We use the Hugging Face\'s Transformers library [20]. For the pruning and retraining phases, an NVIDIA A100 GPU is employed. Experiments involving 7B-size models can be performed on an NVIDIA RTX3090. At the inference stage, we opt for the default configuration, excluding the use of xFormers-optimized attention and advanced options. See Section D for further details.\n' +
      '\n' +
      '## 5 Results\n' +
      '\n' +
      '### Comparison with Existing Work\n' +
      '\n' +
      'Tables 2, 3, and 4 show the zero-shot downstream task performance and inference efficiency of differently pruned models. Diminishing the size of weight matrices through width pruning methods [11, 12, 13] does not lead to latency improvements, as generation speeds with limited input (batch) scales heavily depend on the frequency of memory access operations in LLM inference. This issue is challenging to address solely by reducing the sizes of matrices, unless they are entirely removed. In some cases, it even worsens the inference speed compared to the original model due to GPU-unfriendly operation dimensions (e.g., the hidden sizes of FFN are often not divisible by 8, as shown in Table 1, which hinders the effective utilization of GPU Tensor Cores [1]).\n' +
      '\n' +
      'On the contrary, our depth pruning exhibits speedups through the complete removal of several Transformer blocks, resulting in fewer memory access and matrix-level operations between activations and weights. Moreover, under the same retraining setup as [11], our models achieve zero-shot scores on par with finely width-pruned models.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'Importance Criteria for Block Pruning\n' +
      '\n' +
      'Table 5 presents the results of block pruning using various significance criteria. The basic methods without the \'+\' label fail to maintain essential initial blocks, causing a decline in performance. The Mag+ method, which preserves these critical blocks, partially improves the scores; however, its effectiveness is still inferior compared to the other methods, indicating that relying solely on weight magnitude could\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c|c c|c c} \\hline \\hline \\multirow{3}{*}{Model} & \\multirow{2}{*}{Model} & \\multicolumn{2}{c|}{Zero-shot Performance} & \\multicolumn{2}{c|}{H100 80GB\\({}^{\\ddagger}\\)} & \\multicolumn{2}{c}{RTX3090 24GB\\({}^{\\ddagger}\\)} \\\\ \\cline{3-8}  & & PPL\\(\\downarrow\\) & Ave Acc\\(\\uparrow\\) & Latency\\(\\downarrow\\) & Throughput\\(\\uparrow\\) & Latency\\(\\downarrow\\) & Throughput\\(\\uparrow\\) \\\\  & WikiText2 & PTB & (\\%)\\({}^{\\dagger}\\) & (s) & (tokens/s) & (s) & (tokens/s) \\\\ \\hline Vicuna-13B-v1.3 (13.0B) & 14.7 & 51.6 & 68.3 & 2.8 & 45.5 & OOM & OOM \\\\ \\hline \\multirow{3}{*}{21\\%} & Wanda-sp & 19.0 & 71.8 & 63.6 & 3.8 & 34.1 & 9.8 & 12.9 \\\\  & FLAP & 18.8 & 65.3 & 63.3 & 3.9 & 32.6 & 10.2 & 12.6 \\\\ Pruned & LLM-Pruner & **16.0** & 57.0 & 65.3 & 3.8 & 34.0 & 7.5 & 17.3 \\\\ (10.5B) & Ours: Taylor+ & 18.1 & 61.6 & **66.7** & **2.3** & **55.7** & **5.4** & **23.9** \\\\  & Ours: PPL & 16.1 & **56.5** & 64.9 & **2.3** & **55.7** & **5.4** & **23.9** \\\\ \\hline \\multirow{3}{*}{29\\%} & Wanda-sp & 23.4 & 84.9 & 60.0 & 3.8 & 33.7 & 9.5 & 13.5 \\\\  & FLAP & 22.8 & 78.8 & 61.6 & 3.9 & 33.0 & 10.7 & 12.1 \\\\ Pruned & LLM-Pruner & 19.0 & 66.4 & 62.7 & 3.6 & 35.8 & 8.6 & 15.0 \\\\ (9.5B) & Ours: Taylor+ & 22.0 & 70.3 & **65.1** & **2.1** & **62.0** & **5.3** & **24.2** \\\\  & Ours: PPL & **18.1** & **62.2** & 62.0 & **2.1** & **62.0** & **5.3** & **24.2** \\\\ \\hline \\multirow{3}{*}{37\\%} & Wanda-sp & 36.6 & 123.5 & 52.7 & 3.8 & 33.8 & 10.5 & 12.6 \\\\  & FLAP & 28.7 & 96.2 & 58.3 & 3.9 & 32.9 & 9.7 & 13.2 \\\\ Pruned & LLM-Pruner & 22.2 & 74.0 & 59.7 & 3.6 & 35.6 & 7.1 & 18.0 \\\\ (8.3B) & Ours: Taylor+ & 34.2 & 90.4 & **61.4** & **1.8** & **69.7** & **4.0** & **31.7** \\\\  & Ours: PPL & **22.1** & **73.6** & 59.1 & **1.8** & **69.7** & **4.0** & **31.7** \\\\ \\hline \\hline \\end{tabular} \\({}^{\\dagger}\\)Average accuracy on seven commonsense reasoning tasks.\n' +
      '\n' +
      '\\({}^{\\ddagger}\\)Measured with 12 input tokens, 128 output tokens, and a batch size of 1 on a single GPU.\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 4: Zero-shot results of the compressed Vicuna-13B. See Section A for detailed results.\n' +
      '\n' +
      'be improper for pruning decisions. The Taylor+ criterion enhances accuracy in commonsense reasoning tasks, while the PPL method leads to better generation quality without relying on heuristic selection of pruning candidates.\n' +
      '\n' +
      'Structural Unit for Depth Pruning\n' +
      '\n' +
      'Pruning individual MHA and FFN modules, which are more fine-grained units than Transformer blocks, is also possible. To examine its effect, we measure the impact of removing each module on the PPL of the calibration set and selectively eliminate the unnecessary modules. The same LoRA retraining procedure is conducted.\n' +
      '\n' +
      'Table 6 shows the results of depth pruning at different granularities. For the models with more than 5B parameters, removing individual MHA and FFN modules results in better downstream task accuracy but worse PPL compared to removing entire Transformer blocks. For smaller models than 5B, block-level pruning achieves superior results in terms of all the examined metrics. This differs from the common belief that removing finer units yields better performance.\n' +
      '\n' +
      'Given the collaborative roles of the modules (i.e., MHA captures dependency relations [21], while skip connections and FFN prevent the rank collapse in purely attention-driven networks [14]), it may be suboptimal to treat them in isolation. Taking the 5.3B model in Table 6 as an example, module-level pruning results in consecutive FFNs in some positions, potentially impairing the model\'s ability to handle word interactions. In contrast, with block-level removal, the loss of information could be compensated by neighboring blocks that serve similar functions.\n' +
      '\n' +
      'One-shot _vs._ Iterative Pruning\n' +
      '\n' +
      'For one-shot pruning, multiple blocks are removed simultaneously from the original model, followed by just one phase of retraining. For iterative pruning, the removal of one block coupled with subsequent retraining is repeatedly performed. Here, we use the PPL-based importance criterion for selecting which blocks to remove.\n' +
      '\n' +
      'Figure 5 compares the pruned networks before and after the retraining process. The iteratively pruned models yield better post-pruning results than one-shot pruned ones. However, a single retraining session after one-shot pruning leads to similar performance with iterative pruning. In light of the greatly reduced retraining budget, we opt for one-shot pruning.\n' +
      '\n' +
      'Calibration Data Volume\n' +
      '\n' +
      'The calibration set is employed to assess the weight significance of width pruning baselines and the block-level importance of our method during the pruning phase.\n' +
      '\n' +
      'Table 7 presents the results obtained by varying the number of calibration samples in the BookCorpus dataset. The scores remain relatively stable for the examined methods, suggesting that 10 samples could be sufficient. However, our Taylor+ method encounters a drop in downstream task accuracy when 1K samples are used, leaving the exploration of calibration data characteristics for future research.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline \\hline Depth Pruning & \\multirow{2}{*}{\\#Param} & \\multicolumn{2}{c|}{PPL\\(\\downarrow\\)} & \\multicolumn{1}{c}{Ave Acc\\(\\uparrow\\)} \\\\ Unit & & WikiText2 & PTB & (\\%)\\({}^{\\dagger}\\) \\\\ \\hline Individual MHA \\& FFN & 5.7B & 20.8 & 34.8 & **63.1** \\\\ Transformer Block & 5.7B & **16.9** & **29.3** & 62.8 \\\\ \\hline Individual MHA \\& FFN & 5.3B & 25.2 & 41.3 & **61.1** \\\\ Transformer Block & 5.3B & **18.6** & **33.1** & 60.6 \\\\ \\hline Individual MHA \\& FFN & 4.6B & 38.9 & 58.7 & 52.5 \\\\ Transformer Block & 4.5B & **23.1** & **38.8** & **55.2** \\\\ \\hline Individual MHA \\& FFN & 4.0B & 63.2 & 88.9 & 48.3 \\\\ Transformer Block & 3.9B & **31.1** & **47.3** & **50.6** \\\\ \\hline \\hline \\end{tabular} \\({}^{\\dagger}\\)Average accuracy on seven commonsense reasoning tasks.\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 6: Comparison of depth pruning granularities on LLaMA-7B. Removing entire Transformer blocks instead of individual MHA and FFN modules generally yields better results.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c} \\hline \\hline \\multirow{2}{*}{\\begin{tabular}{c} Evaluation \\\\ Metric \\\\ \\end{tabular} } & \\multirow{2}{*}{Method} & \\multicolumn{4}{c}{\\# Calibration Samples} \\\\  & & 10 & 50 & 100 & 1000 \\\\ \\hline \\multirow{3}{*}{PPL\\(\\downarrow\\) on} & Wanda-sp & 21.4 & 21.4 & 21.7 & 20.8 \\\\  & FLAP & **17.0** & 17.5 & 17.5 & **17.3** \\\\  & LLM-Pruner & 17.6 & **17.2** & **17.0** & OOM\\({}^{\\ddagger}\\) \\\\  & Ours: Taylor+ & 20.2 & 20.2 & 19.0 & 19.6 \\\\  & Ours: PPL & 17.7 & **17.2** & 17.4 & 17.4 \\\\ \\hline \\multirow{3}{*}{\n' +
      '\\begin{tabular}{c} Ave Acc\\(\\uparrow\\) \\\\ (\\%)\\({}^{\\dagger}\\) \\\\ \\end{tabular} } & Wanda-sp & 51.8 & 52.9 & 52.0 & 53.0 \\\\  & FLAP & 59.5 & 59.7 & 59.9 & 60.8 \\\\ \\cline{1-1}  & LLM-Pruner & 61.8 & 61.6 & 61.7 & OOM\\({}^{\\ddagger}\\) \\\\ \\cline{1-1}  & Ours: Taylor+ & **63.5** & **63.5** & **63.9** & **61.7** \\\\ \\cline{1-1}  & Ours: PPL & 61.9 & 61.5 & 61.7 & **61.7** \\\\ \\hline \\hline \\end{tabular} \\({}^{\\dagger}\\)Average accuracy on seven commonsense reasoning tasks.\n' +
      '\n' +
      '\\({}^{\\ddagger}\\)Out-of-memory error on an A100 (80GB) using the official code.\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 7: Impact of calibration data volume. The results of 20%-pruned LLaMA-7B are reported.\n' +
      '\n' +
      'Figure 5: Comparison of one-shot and iterative block pruning on LLaMA-7B. The retraining phase of one-shot pruning effectively bridges the performance gap with iterative pruning.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c|c} \\hline \\hline \\multicolumn{2}{c|}{Block Pruning} & \\multicolumn{2}{c|}{PPL\\(\\downarrow\\)} & \\multicolumn{1}{c}{Ave Acc\\(\\uparrow\\)} \\\\ Criterion & WikiText2 & PTB & (\\%)\\({}^{\\dagger}\\) \\\\ \\hline \\multirow{3}{*}{\\begin{tabular}{c} 20\\% \\\\ Pruned \\\\ (5.5B) \\\\ \\end{tabular} } & Mag- & 7720.7 & 10618.7 & 34.4 \\\\  & Mag- & 19.4 & 36.3 & 56.1 \\\\  & Taylor & 3631.7 & 4327.9 & 35.5 \\\\  & Taylor+ & 20.2 & 32.3 & **63.5** \\\\  & PPL & **17.7** & **30.7** & 61.9 \\\\ \\hline \\multirow{3}{*}{\n' +
      '\\begin{tabular}{c} 35\\% \\\\ Pruned \\\\ (4.5B) \\\\ \\end{tabular} } & Mag & 8490.1 & 14472.1 & 34.9 \\\\  & Mag- & 36.9 & 61.1 & 49.3 \\\\ \\cline{1-1}  & Taylor & 7666.8 & 10913.1 & 35.3 \\\\ \\cline{1-1}  & Taylor+ & 33.2 & 58.5 & **55.4** \\\\ \\cline{1-1}  & PPL & **23.1** & **38.8** & 55.2 \\\\ \\hline \\hline \\end{tabular} \\({}^{\\dagger}\\)Average accuracy on seven commonsense reasoning tasks.\n' +
      '\n' +
      '\\({}^{\\ddagger}\\)Out-of-memory error on an A100 (80GB) using the official code.\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 5: Comparison of pruning criteria on LLaMA-7B. The Taylor+ method excels in commonsense reasoning accuracy, while the PPL criterion leads to better generation performance.\n' +
      '\n' +
      '### Additional Analysis\n' +
      '\n' +
      'Table 8 presents generation examples where the input prompt was sourced from [14]. In terms of linguistic flow and topical consistency, the pruned models yield sentences on par with those from the original model. However, as also noted in [14], the output quality deteriorates when responding to factual questions or producing longer content. To overcome this, integrating more powerful retraining methods may be a valuable future direction.\n' +
      '\n' +
      'Table 9 shows the gains in GPU memory requirements from our depth-pruned models on NVIDIA H100 given 12 input tokens. The larger the batch size, the greater the improvement observed. Notably, our pruned models can handle an output length of 512 and a batch size of 64, unlike the original 13B-parameter model.\n' +
      '\n' +
      '## 6 Related Work\n' +
      '\n' +
      'Numerous techniques have been developed towards efficient LLMs, including knowledge distillation [12, 13], quantization [15, 16], and system-level inference acceleration [13, 14]. In this study, we focus on network pruning [11], which has a long-standing reputation in the model compression field. Beyond its use in relatively small-scale convolutional networks [13, 14] and Transformer models [15, 16, 17], pruning has recently begun to be applied to contemporary LLMs. Several studies [15, 16] employ unstructured and semi-structured [1] pruning by zeroing individual neurons. SparseGPT [15] addresses the layer-wise reconstruction problem for pruning by computing Hessian inverses. Wanda [16] introduces a pruning criterion that involves multiplying weight magnitudes by input feature norms. Despite the plausible performance of pruned models using zero masks, they necessitate specialized support for sparse matrix operations to ensure actual speedups.\n' +
      '\n' +
      'In contrast, structured pruning removes organized patterns, such as entire layers [15], MHA\'s attention heads [13, 14], FFN\'s hidden sizes [12, 15], and some hybrid forms [13, 14, 15, 16, 17], thereby improving inference efficiency in a hardware-agnostic way. To compress LLMs, FLAP [14] and LLM-Pruner [14] eliminate coupled structures in the aspect of net\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{85.4pt}|p{142.3pt}} \\hline Model & Generated Text \\\\ \\hline LLaMA-7B & AI can create a logo in seconds. Why are these new technologies threatening our jobs? AI can do most menial tasks in just a few seconds or minutes. These tasks can be very tedious for human beings to do. For instance, AI can... \\\\ \\hline Wanda-sp & AI can create a logo in seconds rather than years. To this point, this is only the beginning of what AI will do to change our human lives. As time goes on, it’s quite likely that we’ll see even greater use of robots. Even if this all sounds... \\\\ \\hline FLAP & AI can create a logo in seconds and it doesn’t cost you a penny. The new era of technology has opened up many exciting opportunities. Technology, like AI is being utilized for different purposes in different ways, and now,... \\\\ \\hline LLM-Pruner & AI can create a logo in seconds, design a website from scratch and even write blog posts without any human intervention. This revolution is changing the way humans communicate and express ideas. In our view, this technology is a... \\\\ \\hline Ours: Taylor+ & AI can create a logo in seconds or design a new website. There is a lot of software that exists in these two categories. It has become easier and faster for people to use. However, there is still the requirement for the designer or creative... \\\\ \\hline Ours: PPL & AI can create a logo in seconds, and it can design entire websites, all without human intervention. However, there is still a lot of work to be done to perfect this technology, including the development of natural language processing,... \\\\ \\hline Vicuna-13B & AI can create a logo in seconds with its complex algorithms and pre-set designs. But it can never truly understand the essence of a business, the emotions that go into creating a logo or the creativity and uniqueness that comes from... \\\\ \\hline Wanda-sp & AI can create a logo in seconds using a logo design tool. However, a talented graphic designer can create a unique and meaningful logo that stands out from the crowd, captures the essence of the business, and appeals to the target... \\\\ \\hline FLAP & AI can create a logo in seconds. If you don’t like it, you can change it yourself or pay for a designer to do it for you. However, it’s important to note that a logo is more than just an image. It’s a visual representation of a brand or... \\\\ \\hline LLM-Pruner & AI can create a logo in seconds with just one click, but design is a human art, something that is beyond AI capabilities. The logo design is an important part of a company’s identity, and it’s a decision that affects the company’s visual... \\\\ \\hline Ours: Taylor+ & AI can create a logo in seconds, with or without a human’s input. Some people might like this, because they can create a logo for a small business on a budget. Other people, like myself, think that the human touch is important in art... \\\\ \\hline Ours: PPL & AI can create a logo in seconds by combining a variety of different styles and shapes and patterns that make up the logos. However, a logo that is truly a reflection of your brand needs to be designed by a human, a graphic designer.... \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Generation examples from the compressed (top) LLaMA-7B and (bottom) Vicuna-13B. See Section B for additional results.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{85.4pt}|p{142.3pt} p{142.3pt} p{142.3pt} p{142.3pt} p{142.3pt}} \\hline Model & \\multicolumn{3}{c|}{\\(L128\\)} & \\multicolumn{3}{c}{\\(L512\\)} \\\\  & \\(M1\\) & \\(M16\\) & \\(M64\\) & \\(M1\\) & \\(M16\\) & \\(M64\\) \\\\ \\hline\n' +
      '7B & 12.8GB & 16.0GB & 25.8GB & 13.3GB & 25.0GB & 61.8GB \\\\ \\hline\n' +
      '20\\% & 10.5GB & 13.1GB & 21.1GB & 10.9GB & 20.4GB & 50.4GB \\\\\n' +
      '27\\% & 9.4GB & 11.6GB & 18.8GB & 9.7GB & 18.1GB & 44.6GB \\\\\n' +
      '35\\% & 8.6GB & 10.7GB & 17.2GB & 9.0GB & 16.6GB & 40.8GB \\\\ \\hline\n' +
      '13B & 24.8GB & 29.6GB & 44.9GB & 25.5GB & 43.7GB & OOM \\\\ \\hline\n' +
      '21\\% & 19.9GB & 23.8GB & 36.0GB & 20.5GB & 35.0GB & OOM \\\\\n' +
      '29\\% & 18.1GB & 21.7GB & 32.7GB & 18.6GB & 31.8GB & 73.5GB \\\\\n' +
      '37\\% & 15.7GB & 18.8GB & 28.3GB & 16.1GB & 27.5GB & 63.5GB \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: GPU memory requirements for varying sequence lengths (\\(L\\)) and batch sizes (\\(M\\)). The results of the 7B and 13B models and our models with different pruning ratios are reported. Our approach effectively reduces the memory demands of the original models.\n' +
      '\n' +
      'work width while retaining the number of layers. Sheared-LLaMA [21] introduces a mask learning phase aimed at identifying prunable components in both the network\'s width and depth. Our work explores the relatively untapped area of depth-only pruning for multi-billion parameter LLMs, which can markedly accelerate latency while attaining competitive results.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'By introducing a block pruning method, we conduct an in-depth comparative analysis on the impact of network width and depth on LLM compression. Our work involves the one-shot removal of Transformer blocks, determined by evaluating various design choices. Despite its simplicity, our method matches the zero-shot capabilities of recent width pruning techniques. Moreover, it offers significant inference speedups in resource-constrained scenarios that require running LLMs with limited batch sizes, where width pruning falls short. Future research will investigate more potent retraining methods, including full parameter updates and knowledge distillation, alongside an in-depth study of calibration data.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'We thank the Microsoft Startups Founders Hub program and the Gwangju Artificial Intelligence Industry Cluster Agency (AICA) for their generous support of GPU resources, which have contributed to the progress of the NetsPresso R&D project.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [An _et al._2024] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. Fluctuation-based adaptive structured pruning for large language models. In _AAAI_, 2024.\n' +
      '* [Andersch _et al._2019] Michael Andersch, Valerie Sarge, and Paulius Micikevicius. Tensor core dl performance guide. In _NVIDIA GTC_, 2019.\n' +
      '* [Aojun Zhou2021] Junnan Zhu Jianbo Liu Zhijie Zhang Kun Yuan Wenxiu Sun Hongsheng Li Aojun Zhou, Yukun Ma. Learning n:m fine-grained structured sparse neural networks from scratch. In _ICLR_, 2021.\n' +
      '* [Bisk _et al._2020] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In _AAAI_, 2020.\n' +
      '* [Chiang _et al._2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023.\n' +
      '* [Chowdhery _et al._2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n' +
      '* [Clark _et al._2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_, 2018.\n' +
      '* [Clark _et al._2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In _NAACL_, 2019.\n' +
      '* [Dao2023] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. _arXiv preprint arXiv:2307.08691_, 2023.\n' +
      '* [Dettmers _et al._2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. In _NeurIPS_, 2022.\n' +
      '* [Dong _et al._2021] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In _ICML_, 2021.\n' +
      '* [EleutherAI2023] EleutherAI. Language model evaluation harness (package version 3326c54). [https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), 2023.\n' +
      '* [Fan _et al._2020] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. In _ICLR_, 2020.\n' +
      '* [Fang _et al._2023] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards any structural pruning. In _CVPR_, 2023.\n' +
      '* [Frantar and Alistarh2023] Elias Frantar and Dan Alistarh. SparseGPT: Massive language models can be accurately pruned in one-shot. In _ICML_, 2023.\n' +
      '* [Frantar _et al._2023] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for generative pre-trained transformers. In _ICLR_, 2023.\n' +
      '* [Fu _et al._2023] Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. In _ICML_, 2023.\n' +
      '* [Gale _et al._2019] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. In _ICML Workshop_, 2019.\n' +
      '* [Google2023] Google. An important next step on our ai journey. [https://blog.google/technology/ai/bard-google-ai-search-updates/](https://blog.google/technology/ai/bard-google-ai-search-updates/), 2023.\n' +
      '* [He _et al._2019] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In _CVPR_, 2019.\n' +
      '* [Hsieh _et al._2023] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, et al. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In _Findings of ACL_, 2023.\n' +
      '* [Hu _et al._2022] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen. Lora: Low-rank adaptation of large language models. In _ICLR_, 2022.\n' +
      '* [Jin _et al._2023] Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. S3: Increasing gpu utilization during generative inference for higher throughput. In _NeurIPS_, 2023.\n' +
      '* [Kurtic _et al._2023] Eldar Kurtic, Elias Frantar, and Dan Alistarh. Ziplm: Inference-aware structured pruning of language models. In _NeurIPS_, 2023.\n' +
      '* [Kwon _et al._2022] Woosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers. In _NeurIPS_, 2022.\n' +
      '* [Kwon _et al._2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In _SOSP_, 2023.\n' +
      '* [Lagunas _et al._2021] Francois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M. Rush. Block pruning for faster transformers. In _EMNLP_, 2021.\n' +
      '* [LeCun _et al._1989] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In _NeurIPS_, 1989.\n' +
      '* [Lee _et al._2021] Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin. Layer-adaptive sparsity for the magnitude-based pruning. In _ICLR_, 2021.\n' +
      '* [Li _et al._2017a] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. In _ICLR_, 2017.\n' +
      '* [Li _et al._2017b] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. In _ICLR_, 2017.\n' +
      '* [Liu _et al._2021] Zejian Liu, Fanrong Li, Gang Li, and Jian Cheng. Ebert: Efficient bert inference with dynamic structured pruning. In _Findings of ACL_, 2021.\n' +
      '* [Ma _et al._2023] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. In _NeurIPS_, 2023.\n' +
      '* [Marcus _et al._1993] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. _Computational Linguistics_, 19(2):313-330, 1993.\n' +
      '* [Merity _et al._2017] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In _ICLR_, 2017.\n' +
      '* [Michel _et al._2022] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In _NeurIPS_, 2022.\n' +
      '* [Mihaylov _et al._2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In _EMNLP_, 2018.\n' +
      '* [Molchanov _et al._2019] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In _CVPR_, 2019.\n' +
      '* [Nova _et al._2023] Azade Nova, Hanjun Dai, and Dale Schuurmans. Gradient-free structured pruning with unlabeled data. In _ICML_, 2023.\n' +
      '* [NVIDIA2018] NVIDIA. Useful nvidia-smi queries. [https://enterprise-support.nvidia.com/s/article/Useful-nvidia-smi-Queries-2](https://enterprise-support.nvidia.com/s/article/Useful-nvidia-smi-Queries-2), 2018.\n' +
      '* [OpenAI2022] OpenAI. Introducing chatgpt. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt), 2022.\n' +
      '* [OpenAI2023] OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* [Sakaguchi _et al._2019] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. _arXiv preprint arXiv:1907.10641_, 2019.\n' +
      '* [Santacroce _et al._2023] Michael Santacroce, Zixin Wen, Yelong Shen, and Yuanzhi Li. What matters in the structured pruning of generative language models? _arXiv preprint arXiv:2302.03773_, 2023.\n' +
      '* [Scao _et al._2022] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.\n' +
      '* [Sheng _et al._2023] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, et al. Flexgen: High-throughput generative inference of large language models with a single gpu. In _ICML_, 2023.\n' +
      '* [Sun _et al._2024] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A simple and effective pruning approach for large language models. In _ICLR_, 2024.\n' +
      '* [Taori _et al._2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, et al. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca), 2023.\n' +
      '* [Touvron _et al._2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [Vaswani _et al._2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017.\n' +
      '* [Voita _et al._2019] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In _ACL_, 2019.\n' +
      '* [Wan _et al._2023] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, and Mi Zhang. Efficient large language models: A survey. _arXiv preprint arXiv:2312.03863_, 2023.\n' +
      '\n' +
      '* [Wolf _et al._2020] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, et al. Transformers: State-of-the-art natural language processing. In _EMNLP: System Demonstrations_, 2020.\n' +
      '* [Xia _et al._2022] Mengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning learns compact and accurate models. In _ACL_, 2022.\n' +
      '* [Xia _et al._2024] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. In _ICLR_, 2024.\n' +
      '* [Yu _et al._2022] Shixing Yu, Tianlong Chen, Jiayi Shen, Huan Yuan, Jianchao Tan, Sen Yang, Ji Liu, and Zhangyang Wang. Unified visual transformer compression. In _ICLR_, 2022.\n' +
      '* [Zellers _et al._2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In _ACL_, 2019.\n' +
      '* [Zhang _et al._2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.\n' +
      '* [Zhu _et al._2015] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, et al. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In _ICCV_, 2015.\n' +
      '* [Zhu _et al._2023] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A survey on model compression for large language models. _arXiv preprint arXiv:2308.07633_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:11]\n' +
      '\n' +
      'Generation Examples\n' +
      '\n' +
      '## Appendix B Generation Examples\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} \\hline \\hline Model & Generated Text \\\\ \\hline LLaMA-7B & What’s great about the holiday season, it’s a time when people focus less on work and more on families and friends. The holidays are a time when people can really focus on giving and helping others, which is something that the holidays are supposed to be about. In my blog post this week, I’ll be taking a look at a variety of causes that... \\\\ \\hline Wanda-sp & What’s great about the holiday season, though, is its warm and inviting people, and what a chance to meet and share a cuppa with others who understand and have shared similar experiences, a feeling of empathy if you will, and a great opportunity to help others on a difficult journey. So I can say, all is well. But I’m not giving up the fight... \\\\ \\hline FLAP & What’s great about the holiday season, and what makes it such an awesome time of the year, is that it’s the time of the year when people are so generous, and so goodhearted. When you get together with friends and family, and you go through the various things that you’re able to do, when you’re able to give people gifts, when people can see... \\\\ \\hline LLM-Pruner & What’s great about the holiday season, is you can enjoy the company of friends and family. This is the perfect opportunity to gather together, and share all the special memories that you have together. These moments are the ones that make the holiday special. No matter what happens, or what you end up doing, it’s always good to get... \\\\ \\hline Ours: Taylor+ & What’s great about the holiday season, is that it’s a time of year when people can actually take a step back from the normal day-to-day routines and take some time out to relax and enjoy some well-deserved rest and rejuvenation. This might include indulging in a more comfortable or luxury holiday experience and this article aims to help... \\\\ \\hline Ours: PPL & What’s great about the holiday season, in my mind, is that it brings together friends, family, and food. The holiday season is a great time to show some love and have some fun with the people that matter most. Whether you are celebrating Christmas, Hanukah, or New Years, the festivities will be full of joy and memories that will last... \\\\ \\hline \\hline LLaMA-7B & AI can create a logo in seconds. Why are these new technologies threatening our jobs? AI can do most menial tasks in just a few seconds or minutes. These tasks can be very tedious for human beings to do. For instance, AI can identify patterns in large amounts of data so that complex statistical analysis can be carried out faster than a... \\\\ \\hline Wanda-sp & AI can create a logo in seconds rather than years. To this point, this is only the beginning of what AI will do to change our human lives. As time goes on, it’s quite likely that we’ll see even greater use of robots. Even if this all sounds too dramatic, this may still happen. And AI is the best thing that can make it to happen. in 2019. All you... \\\\ \\hline FLAP & AI can create a logo in seconds and it doesn’t cost you a penny. The new era of technology has opened up many exciting opportunities. Technology, like AI is being utilized for different purposes in different ways, and now, technology is becoming a solution to problems that would have been tedious or expensive only a few years ago.... \\\\ \\hline LLM-Pruner & AI can create a logo in seconds, design a website from scratch and even write blog posts without any human intervention. This revolution is changing the way humans communicate and express ideas. In our view, this technology is a great asset but we’ll need to be smart in how we use it to prevent abuse or automation leading to job loss. So,... \\\\ \\hline Ours: Taylor+ & AI can create a logo in seconds or design a new website. There is a lot of software that exists in these two categories. It has become easier and faster for people to use. However, there is still the requirement for the designer or creative director to ensure that the designs are relevant and meaningful to the brand. The Future: AI for creative design... \\\\ \\hline Ours: PPL & AI can create a logo in seconds, and it can design entire websites, all without human intervention. However, there is still a lot of work to be done to perfect this technology, including the development of natural language processing, machine learning and computer vision, as well as ensuring that AI designers are creating work that can be... \\\\ \\hline \\hline LLaMA-7B & Neural network pruning is defined as the process of removing certain weights or neurons in a neural network to reduce the size of the neural network, and improve the training process. Neural networks pruning techniques have been applied successfully on classification, optimization, planning, and speech recognition tasks. A pruned... \\\\ \\hline Wanda-sp & Neural network pruning is defined as any method that removes the redundant information by applying the neural network network. It is the term used to identify the systematic variables when an individual tries to find an algorithm. In addition, it is the method of classification used to define any set of algorithms that have been... \\\\ \\hline FLAP & Neural network pruning is defined as a selection of neurons or weights in a neural network so that the information of the network is reduced to a specified level, and a process of re-training a neural network. The latter one is the most popular method, especially the neural network pruning. In recent years, this field has been studied in... \\\\ \\hline LLM-Pruner & Neural network pruning is defined as reducing the complexity and size of an over-parameterized neural network. The pruning reduces the FLOPs and improves performance for the particular task. Pruning techniques have gained popularity in recent years due to their wide range of applications. This paper aims to explore the pruning... \\\\ \\hline Ours: Taylor+ & Neural network pruning is defined as reducing the weight connections and biases of a neural network, while retaining high performance. Pruning is an effective approach to reducing the number of parameters and improving the efficiency of neural networks. Recently, the concept of neural network shrinking has gained considerable... \\\\ \\hline Ours: PPL & Neural network pruning is defined as the process of removing redundant neurons and connections between nodes in a neural network, while keeping the performance or accuracy to the maximum extent possible. This optimization is aimed at reducing the size and memory consumption of the network while maintaining its ability to perform task... \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 11: Generation examples from the original LLaMA-7B and 20%-compressed models.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} \\hline Model & Generated Text \\\\ \\hline Vicuna-13B & What’s great about the holiday season, beyond the obvious time off and the chance to spend time with loved ones, is the opportunity to slow down and reflect on the year that’s passed. It’s a time to look back at all the accomplishments and growth you’ve experienced, and to set your sights on the year ahead. As a small business owner, the... \\\\ \\hline Wanda-sp & What’s great about the holiday season, besides spending time with family and friends, is that it’s a time of giving. It’s a time to show appreciation for those in your life who make a difference and bring joy to your world. Whether it’s a gift of love, a gift of time, or a gift of money, it’s important to remember that giving is not just about... \\\\ \\hline FLAP & What’s great about the holiday season, especially the days leading up to Christmas, is the opportunity to take a pause and recharge. It’s a time to reflect on the year and all the experiences we’ve had, good or bad. It’s a time to reflect on the people we love and the impact they’ve had on our lives. It’s a time to reflect on what we want... \\\\ \\hline LLM-Pruner & What’s great about the holiday season, aside from the gifts and the holiday parties, is the opportunity to spend time with family and loved ones. One of the things that I love most about holiday seasons is the traditions that surround them, from decorating the tree to making special holiday meals, there’s a sense of nostalgia and warmth that... \\\\ \\hline Ours: Taylor+ & What’s great about the holiday season, is that it allows us to reflect on the year gone by and reflect on the moments that made it special. And while some people might choose to celebrate with the traditional traditions, some choose to do something different. One way to spice up your holiday season is by taking a trip to a far-off place. Why not... \\\\ \\hline Ours: PPL & What’s great about the holiday season, in addition to the fun food, festivities, and spending time with loved ones, is that it creates the perfect opportunity to show people how much you appreciate them. From giving them a gift, to baking their favorite treat, or just taking a moment to acknowledge them in front of others – there’s something... \\\\ \\hline \\hline Vicuna-13B & AI can create a logo in seconds with its complex algorithms and pre-set designs. But it can never truly understand the essence of a business, the emotions that go into creating a logo or the creativity and uniqueness that comes from human input. It’s time to put the human touch back into branding, where a designer and client can collaborate to... \\\\ \\hline Wanda-sp & AI can create a logo in seconds using a logo design tool. However, a talented graphic designer can create a unique and meaningful logo that stands out from the crowd, captures the essence of the business, and appeals to the target audience. A good logo can also make a difference in how customers perceive the business and can even increase... \\\\ \\hline FLAP & AI can create a logo in seconds. If you don’t like it, you can change it yourself or pay for a designer to do it for you. However, it’s important to note that a logo is more than just an image. It’s a visual representation of a brand or organization, and it communicates certain values and characteristics to the viewer. Therefore, it’s essential to... \\\\ \\hline LLM-Pruner & AI can create a logo in seconds with just one click, but design is a human art, something that is beyond AI capabilities. The logo design is an important part of a company’s identity, and it’s a decision that affects the company’s visual branding and reputation for years to come. The logo is the visual representation of a company’s values... \\\\ \\hline Ours: Taylor+ & AI can create a logo in seconds, with or without a human’s input. Some people might like this, because they can create a logo for a small business on a budget. Other people, like myself, think that the human touch is important in art and design. A human designer can create a unique and meaningful logo by incorporating personal... \\\\ \\hline Ours: PPL & AI can create a logo in seconds by combining a variety of different styles and shapes and patterns that make up the logos. However, a logo that is truly a reflection of your brand needs to be designed by a human, a graphic designer. They will bring a creative vision that will make your logo unique, appealing and memorable. They can add... \\\\ \\hline \\hline Vicuna-13B & Neural network pruning is defined as the process of removing redundant or unnecessary connections in a neural network to reduce its size and computational complexity while preserving its performance. The primary motivation behind pruning is to improve the efficiency of deep learning models, especially in resource-constrained... \\\\ \\hline Wanda-sp & Neural network pruning is defined as a method for removing connections in order to reduce the model size, thus allowing for computationally efficient models while retaining accuracy. This technique is useful in reducing the number of neurons in a neural network, which allows for fewer computations during training and reduces the... \\\\ \\hline FLAP & Neural network pruning is defined as the process of reducing the size of a neural network by removing the least significant weights or neurons. The main motivation for pruning is to reduce the computational cost of neural networks while maintaining their accuracy. The key challenge of pruning is to identify the most effective... \\\\ \\hline LLM-Pruner & Neural network pruning is defined as a technique that aims to reduce the size of a neural network by removing redundant and irrelevant connections between the neurons in the network. This approach is based on the observation that a large portion of the connections within the network is redundant and does not contribute to the overall... \\\\ \\hline Ours: Taylor+ & Neural network pruning is defined as the removal of redundant connections within a neural network to achieve a better model fit while retaining the network’s general accuracy. The goal of pruning is to reduce the computational cost and memory footprint of the network. One commonly used pruning method is called weight magnitude... \\\\ \\hline Ours: PPL & Neural network pruning is defined as the task of removing unnecessary or redundant connections in a neural network while retaining its accuracy and performance. This is often done to reduce the memory usage and computational complexity of a neural network, which can be critical when running on devices with limited resources. In... \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 12: Generation examples from the original Vicuna-13B-v1.3 and 21%-compressed models.\n' +
      '\n' +
      '## Appendix C Additional Results of Inference Efficiency\n' +
      '\n' +
      '## Appendix D Implementation Details\n' +
      '\n' +
      'We utilize the Hugging Face\'s Transformers library [20] on a single NVIDIA A100 GPU (80GB VRAM). All the experiments involving 7B-parameter models can be conducted on a single NVIDIA RTX 3090 (24GB VRAM).\n' +
      '\n' +
      '* At the pruning phase, we assess the significance of Transformer blocks using a small calibration set (containing 10 samples from BookCorpus [14] with a sequence length of 128). For the PPL-based criterion, the calibration samples are fed into networks with a single block removed, and this step is iterated across all the blocks in the target model. For the Taylor+ method, we feed the calibration data into the original network to collect backward-gradient matrices. The pruning is completed efficiently within 1 to 2 hours for the 7B- and 13B-sized models.\n' +
      '* At the retraining stage, we apply a LoRA adapter [15] to every projection weight matrix by following [14]. We employ a LoRA rank of 8, a learning rate of 0.0001, and a batch size of 64 over 2 epochs. The retraining costs are notably low, with the entire process being executed on a single GPU. For example, retraining a 20%-pruned model from 7B parameters takes about 2 hours and utilizes 22GB GPU memory, while a 21%-pruned model from 13B parameters requires approximately 3 hours and 35GB VRAM.\n' +
      '* At the inference stage, we maintain default configurations without employing xFormers-optimized attention or additional advanced features.\n' +
      '\n' +
      'Figure 6: Inference efficiency of pruned models on an NVIDIA H100 GPU. Our depth pruning achieves a superior latency-throughput trade-off for various sequence lengths of input and output. In contrast, the width pruning of FLAP [13] and LLM-Pruner [14] degrades efficiency results due to GPU-unfriendly weight dimensions [1] (e.g., the hidden sizes of FFN are often not divisible by 8). The markers labeled with \\(M\\) represent batch sizes. The dotted lines indicate that pruned models can operate with larger batch sizes, avoiding out-of-memory errors encountered by the original model.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>