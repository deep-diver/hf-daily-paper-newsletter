<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# IMUSIC: IMU 기반 얼굴 표정 캡쳐\n' +
      '\n' +
      'Youjia Wang\\({}^{1,2*}\\)\n' +
      '\n' +
      'Yiwen Wu\\({}^{1,2*}\\)\n' +
      '\n' +
      'Ruiqian Li\\({}^{1}\\)\n' +
      '\n' +
      'Hengan Zhou\\({}^{1,2}\\)\n' +
      '\n' +
      'Hongyang Lin\\({}^{1,3}\\)\n' +
      '\n' +
      'Yingwenqi Jiang\\({}^{1}\\)\n' +
      '\n' +
      'Yingsheng Zhu\\({}^{1}\\)\n' +
      '\n' +
      'Guanpeng Long\\({}^{1,4}\\)\n' +
      '\n' +
      'Jingya Wang\\({}^{1}\\)\n' +
      '\n' +
      'Lan Xu\\({}^{1}\\)\n' +
      '\n' +
      'Jingyi Yu\\({}^{1}\\)\n' +
      '\n' +
      '상하이테크대학({}^{1}\\)루미아니테크({}^{3}\\)데모스테크({}^{4}\\)엘란테크(주) {wangyj2, wuyw2023, lirq1, zhouha24, linhy, jangywq, zhuysh, longgp2022, wangjingya, xulanl, 유징yi}@샹haitech.edu.cn}@샹haitech.edu.cn}\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '얼굴 모션 캡처 및 분석을 위해, 지배적인 솔루션들은 일반적으로 프라이버시를 보호할 수 없고 교합에 취약한 시각적 큐들에 기초한다. 관성 측정 유닛(IMU)은 잠재적인 구조 역할을 하지만 주로 전신 모션 캡처를 위해 채택된다. 본 논문에서는 기존의 시각 솔루션에서 상당히 멀리 떨어진 순수한 IMU 신호를 사용하여 얼굴 표정 캡처를 위한 새로운 경로인 갭을 메우기 위한 IMUSIC를 제안한다. 우리 IMUSIC의 핵심 디자인은 3부작이다. 우리는 먼저 해부학 기반 IMU 배치 스킴과 함께 얼굴 캡처에 적합한 마이크로 IMU를 설계한다. 그런 다음 다양한 표정과 성능을 위해 풍부한 쌍 IMU/시각 신호를 제공하는 새로운 IMU-ARKit 데이터 세트에 기여한다. 이러한 독특한 멀티 모달리티는 IMU 기반 얼굴 행동 분석과 같은 미래 방향에 대한 엄청난 잠재력을 가져온다. 또한, IMU-ARKit을 사용하여 순수 IMU 신호로부터 얼굴 블렌드 쉐이프 파라미터를 정확하게 예측하기 위한 강력한 베이스라인 접근법을 소개한다. 특히, 본 논문에서는 트랜스포머 확산 모델을 2단계 훈련전략으로 재구성한다. IMUSIC 프레임워크는 시각적 방법이 흔들리고 동시에 사용자 프라이버시를 보호하는 시나리오에서 정확한 얼굴 캡처를 수행할 수 있게 한다. 우리는 IMUSIC 접근법의 유효성을 검증하기 위해 IMU 구성과 기술 구성 요소에 대해 광범위한 실험을 수행한다. 특히, IMU-SIC는 다양한 잠재적이고 새로운 응용, 즉 프라이버시 보호 얼굴 캡처, 폐색에 대한 하이브리드 캡처 또는 시각적 신호를 통해 종종 보이지 않는 미세한 얼굴 움직임을 감지할 수 있게 한다. 우리 커뮤니티에서 얼굴 캡처 및 분석의 더 많은 가능성을 강화하기 위해 데이터 세트 및 구현을 공개할 것이다._\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '미세한 근육 움직임만큼 미묘한 얼굴 표정은 광범위한 감정을 효과적으로 전달하는 데 핵심적인 역할을 한다. 이러한 비언어적 단서는 구어 이상의 정서적 깊이를 제공하기 때문에 인간 상호 작용에서 특히 중요하다. 해부학적으로 표현적 안면 움직임은 안면 및 삼차 신경을 통해 뇌에 의해 활성화되는 안면 근육의 복잡한 조정에서 나타난다. 상당한 시간 동안, 얼굴 표정을 캡처하고 분석하는 것은 주로 시각적 신호에 의존해 왔다. 3DDFA[31, 101]과 같은 컴퓨터 비전 기술은 단일 이미지로부터 얼굴 기하학 및 표정을 획득하는 신속한 수단을 제공하는 반면, DECA[27]은 더 미세한 얼굴 세부 사항을 복구한다. 애플의 ARKit[5]는 실시간 얼굴 기하학 획득의 상당한 비약을 나타내며, 사용자가 자신의 표현을 모방한 애니메이션 아바타를 생성할 수 있게 한다. 그러나 시각 신호에 기반한 접근 방식은 또한 프라이버시를 보호하지 못하고 폐색 또는 측면 얼굴 포즈에 취약하다는 문제를 제기한다. 또한 시각적으로 추적하기 어려운 미묘한 얼굴 동작을 포착하지 못한다. 본 논문에서는 독립적으로 또는 시각적 캡처와 함께 작동할 수 있는 관성 측정 장치(IMU)의 최신 발전을 활용하여 얼굴 움직임 캡처에 대한 새로운 접근법을 탐구한다.\n' +
      '\n' +
      '초기 모바일 장치에서 사용되는 IMU는 전신 모션 캡처에 점진적으로 수용되어 왔다. 기존의 비전 기반 시스템에 비해 IMU는 휴대성과 최소한의 공간 전제 조건을 자랑한다. 일반적으로 다양한 신체 관절에 부착된 IMU는 신체 모션으로 더 번역할 수 있는 필수 가속도 및 축 각도 데이터를 캡처할 수 있다. 예를 들어, Loper 등에 의한 개척 작업[48]은 SMPL 인체 모델과의 통합을 위해 이 축 각도 데이터를 적응시킴으로써, 전신 모션 캡쳐를 가능하게 한다. 가장 최근에, Yi et al. [90]은 트랜스포머 확산[43]의 안정성과 생성 능력을 활용하여 6개의 IMU만큼 적은 수의 IMU를 사용하여 포괄적인 신체 모션 캡처를 달성했다. 대조적으로, 얼굴 모션 캡쳐를 위한 IMU의 사용은 거의 없다. 그 이유는 도전이 여러 가지이기 때문입니다. 하드웨어 측면에서 IMU를 추가 센서(관성 및 지자기 검출기) 및 통신 모듈(블루투스 및 Wi-Fi와 같은)과 통합하면 얼굴에 엄청나게 큰 크기가 발생한다. 알고리즘 측면에서 IMU는 일반적으로 시각 시스템에 비해 신호 대 잡음 비율이 낮은 더 선명한 데이터를 제공한다. 또한, 공간적 위치가 뼈 회전과 상관관계가 있는 신체 모션 캡처와 달리, 얼굴 표정은 주로 근육에 의해 구동되어 IMU 데이터를 번역하는 데 독특한 도전을 제기한다.\n' +
      '\n' +
      '본 논문에서는 새로운 IMU 기반 얼굴 움직임 포착 솔루션, 즉 IMUSIC(자세한 내용은 그림 1 참조)를 제시한다. IMUSIC는 얼굴 애플리케이션을 위해 특별히 설계된 IMU를 활용하여 소형화를 강조합니다. 특히, IMU의 검출 모듈과 데이터 모듈을 분리함으로써 얼굴에 부착된 장치가 컴팩트하고 가볍다는 것을 보장한다. 이 설계는 자연스러운 얼굴 움직임을 방해하지 않으며 신뢰할 수 있는 데이터 전송 및 동기화를 허용합니다. IMUSIC는 또한 얼굴 표정을 제어하는 특정 근육에 대응하여 IMU를 배치하기 위한 해부학 기반 전략을 채택한다. 또한 IMUSIC는 발현 유도 운동에 면역되고 목 회전에 영향을 받지 않는 위치인 귀 뒤 측두골에 보조 IMU를 부착하여 전체적인 머리 움직임을 제거한다. 우리는 IMU와 시각 신호로 구성된 첫 번째 얼굴 IMU 데이터 세트에 기여하여 CG 커뮤니티에 도움이 된다. 스킨 톤 테이프로 IMU를 덮음으로써 ARKit을 사용하여 해당 비전 신호를 캡처할 수 있었다. 결과 IMU-ARKit 데이터 세트는 다른 언어 말하기, 다른 참가자와 상호작용하면서 표정 만들기, 감정 억양으로 말하기 등을 포함하여 다양한 활동에 참여한 참가자의 신호를 모두 기록한다.\n' +
      '\n' +
      '다음으로, IMU-ARKit 데이터 세트를 사용하여 신경망을 학습하여 IMU 신호에서만 얼굴 표정을 추론한다. 이전의 시각적 기반 기술은 연령, 성별, 민족성 등의 불일치를 설명하기 위해 초대형 참가자의 데이터 세트를 필요로 한다. 수집의 어려움으로 인해 데이터 세트는 여전히 다양성이 부족하다. 따라서 이러한 시각적 데이터 세트에서 가상 IMU 신호를 합성하여 증강한다. 우리는 실제 포획된 대 실제 포획된 대 실제 포획된 대 실제 포획된 대 실제 포획된 대 실제 포획된 대 실제 포획된 대 실제 포획된 대 실제 포획된 대 실제 포획된 대 실제 포획된 대 실제 포획된 대 실제 포획된 대 실제 포획된 대 실제 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 대 실 포획된 IMU 신호를 합성하고 트랜스포머 확산 기반 신경망[43]을 훈련하여 양자 사이의 간격을 좁힐 뿐만 아니라 IMU 신호로부터 직접 블렌드 쉐이프 파라미터를 신뢰성 있게 추론한다. 종합 실험에 따르면 IMUSIC는 조명이 잘 켜진 조건에서 전면 참가자에게 ARKit에 필적하는 성능을 보여준다. 더 중요한 것은 측면 대면 참가자, 폐색 아래 얼굴 또는 ARKit 및 기타 시각적 기반 기술에 일반적으로 문제가 되는 열악한 조명 조건과 같은 도전적인 시나리오를 처리하는 데 훨씬 더 강력하다는 것이다.\n' +
      '\n' +
      'IMUSIC는 대체 얼굴 모션 캡처 시스템 역할을 하는 것 외에도 다양한 새로운 응용 프로그램을 가능하게 한다. 디지털 프라이버시가 가장 중요한 관심사인 시대에 IMU-SIC는 시각적 입력 없이 표정을 캡처하여 개인 시각적 데이터를 보호하는 새로운 방법을 제공한다. 이 기능은 콘텐츠 제작자가 진정한 정체성을 숨기면서 아바타를 애니메이션화하려고 하는 가상 유튜브(VTubers)의 영역에서 특히 중요하다. IMUSIC을 이용하여, VTubers는 그들의 디지털 페르소나의 표현을 익명으로 조작할 수 있는 권한을 부여받는다. 둘째, 시각 신호가 손상된 시나리오에서 전통적인 얼굴 캡처 방법을 보완하기 위한 하이브리드 캡처 시스템을 소개한다. 예시적인 사례는 노래 녹음 중에 있으며, 여기서 연주자의 얼굴 표정은 마이크 또는 유사한 스튜디오 장치에 의해 숨겨질 수 있다. 그러한 경우 예술가들의 입과 볼이 가려져 ARKit과 같은 시각적 도구로 포착된 왜곡된 표현이 나타난다. 대조적으로, IMUSIC는 이러한 폐색된 얼굴 움직임을 매끄럽게 캡처하고 귀중한 가상 음악 생산을 위해 오디오 레코딩과 얼굴 애니메이션의 정확한 동기화를 보장할 수 있다. 또한, IMUSIC는 미묘한 얼굴 움직임을 감지하고 시력 특징 분석에 덜 순응하는 영역에서 표정을 포착하는 탁월한 능력을 보여준다. 예를 들어, 시력 신호에서 불변으로 종종 발생하는 퍼핑으로 인한 미묘한 볼 움직임은 IMU의 방향 데이터를 통해 IMUSIC에 의해 정확하게 캡처될 수 있다. 시각적으로 눈에 띄지 않을 수 있는 입 모서리의 약간의 경련과 같은 다른 미세 표현도 IMUSIC에 의해 안정적으로 캡처될 수 있어 개인의 감정 변화를 감지하기 위한 통찰력을 제공한다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '얼굴 모션 캡쳐를 위해 IMU를 배치하려는 시도는 그림 2와 같다. 이러한 진화를 추적하면 1990년대부터 마커 기반 광학 모션 캡쳐 기술이 해당 분야의 주류 방법론으로 등장했다[10, 28, 32]. 이 접근법에서, 배우들은 모션 추적을 위해 자신의 신체에 반사 마커를 착용할 것이 요구된다. 2000년대 이후 획기적인 개발[18, 22, 55, 81]은 모션 캡처 기술에 혁명을 일으켰다. 이러한 발전은 배우들이 특별한 마커를 착용할 필요 없이 인간의 움직임을 포착하는 것을 가능하게 했다[44, 48, 68]. 그러나 3D 골격 재구성을 위해 기술은 일반적으로 [55]에서와 같이 장[97] 또는 다중 카메라 설정과 같은 고정 위치 장치를 필요로 한다. 이것은 캡처 동안 배우들이 이용할 수 있는 움직임의 범위 및 공간을 실질적으로 제한한다. 이러한 한계를 해결하기 위해, Vlasic et al. [80]은 인간 모션 캡처에서 IMU의 사용을 개척하여, 보다 유연하고 덜 제한적인 방법을 제공한다. 그 후, 이 분야는 급속한 발전을 목격하여 보다 안정적인 시스템의 개발[21, 38, 84]과 더 날카로운 구성의 도입[83, 93]으로 이어졌다. 상업 경기장에서는 소니[73] 및 엑스센스[88]와 같은 회사들이 소비자 등급의 모션 캡처 센서를 출시하여 공간 요구 사항이 감소하고 더 넓은 소비자 기반을 수용하기 위해 운영 범위가 확장된 솔루션을 제공하고 있다.\n' +
      '\n' +
      '얼굴 모캡페이스 형상 표현은 전통적으로 2D 얼굴 랜드마크 포인트들의 희박한 컬렉션에 의존한다. 일부 선구적인 작품[17, 19, 20, 100]은 얼굴 캡처를 위해 2D 얼굴 랜드마크를 능숙하게 활용했다. 3D 시각적 효과에 대한 수요가 증가함에 따라 3D 얼굴 캡처 및 재구성에 대한 관심이 증가하고 있다.\n' +
      '\n' +
      '표정 캡처의 영역에서 블란츠와 베터[11]는 선형 조합을 사용하여 일반적인 얼굴 표현을 위한 새로운 접근법을 제안한 중추적인 발전을 도입했다. 이 방법론은 얼굴 표정을 포착하는 체계적이고 미묘한 방법을 제공함으로써 현장에서 중요한 전환점을 열었다. 상기 필름 및\n' +
      '\n' +
      '도 2: 신체 및 안면 모션 캡쳐의 진화. 이 순서는 마커 기반 캡처에서 마커 없는 캡처로의 진행을 보여주며 IMU 기반 캡처에서 절정을 이룬다. IMU는 전신 모션 캡처에 이미 적용되었으며 IMU 기반 얼굴 캡처(오른쪽 하단)의 공백을 메우기 위해 IMUSIC를 제안한다.\n' +
      '\n' +
      '시각 효과 산업, 여러 연구[9, 12, 53, 54, 56, 65, 94]는 얼굴 마커를 활용하고, 특정 얼굴 표정과 동작을 예측하기 위해 움직임을 시각적으로 추적하고 컴퓨팅했다.\n' +
      '\n' +
      '그런 다음 캡처된 동작은 시각적 효과에서 사실적인 얼굴 애니메이션을 달성하기 위해 사후 제작에서 수동으로 정제된다. 그러나 소비자 수준의 사용자에게 이러한 방법은 엄청나게 비싸고 생산 후 수동 조정에 크게 의존하며 불편하며 위치 제약에 의해 제한된다. 여러 작품[7, 14, 26, 72, 86]은 단일 RGB 이미지에서 얼굴 움직임을 예측하는 마커 프리 접근법을 통해 이 문제를 해결하여 비용을 효과적으로 줄였다. 그 후, 이 방법론에 따라 이 분야의 수많은 과제가 해결되었다. Cao et al. [13], Paysan et al. [60]의 연구는 비디오 지터를 제거하고 견고성을 향상시키기 위해 다양한 전략을 채택함으로써 표현 모션 캡처의 프로세스를 향상시켰다.\n' +
      '\n' +
      '딥 러닝 알고리즘에 의한 이미지 처리 능력의 향상으로, 바티스타 등[8], Laine 등[41], Olszewski 등[59]은 다양한 차원으로부터 얼굴 모션 캡쳐를 개선하였다. CNN 네트워크[31, 33, 75]를 사용한 회귀 기반 방법은 단일 이미지로부터 강건한 결과를 달성하는 반면, 비디오 기반 얼굴 모션 캡쳐 기술[34, 45, 49, 92]은 점점 더 정교해 졌다. Lombardi et al. [47]은 얼굴 애니메이션에서 복잡한 기하학 및 텍스처를 처리하기 위한 딥 외관 모델을 제안한다. 윤 등[91]은 야생에서 이미지를 처리하기 위해 연합된 모델을 사용하고 Cao 등[16]은 모바일 폰으로 고해상도 아바타를 훈련시키기 위해 UAP(universal avatar prior)를 사용한다. 다른 방식으로, Zhang et al. [95]는 4D 스캔으로부터 고 충실도 수행자-특정 얼굴 캡처 방법을 제안한다.\n' +
      '\n' +
      '그러나, 시각 기반 방법 [15, 86]은 폐색으로부터 간섭되기 쉽다. Qammaz와 Argyros[63]의 연구를 참조하면, 그들은 영역의 상당 부분이 가려진 영역에서 정보를 예측하도록 함으로써 가려짐의 영향을 부분적으로 완화시켰다. 최근 몇 년 동안 웨어러블 장치를 기반으로 한 얼굴 캡처가 유망한 결과를 보여주었다. 이러한 방법들은 간섭에 덜 민감하고 특히 휴대성이 있다. 그러나, 위에서 언급된 모든 시각적 기반 방법들은 카메라에 대한 장기간의 노출로 인해 사용자 프라이버시를 손상시킬 위험에 직면한다.\n' +
      '\n' +
      '센서 기반 MocapAs 제조 기술이 발전하고 해당 분야의 연구가 심화됨에 따라 IMU는 일관되고 정밀한 개선의 혜택을 받고 있다. IMU 보정[6, 23, 29, 67] 및 보정[2, 46, 79, 80]의 기본 성능에 초점을 맞춘 연구는 안정적인 데이터 검출을 가능하게 하고 주요 드리프트 문제를 효과적으로 해결했다. 이러한 문제를 해결하면 IMU의 안정성과 정밀도가 향상되어 바디 모캡 응용 분야에서 많은 연구팀에게 인기 있는 선택이 된다. 대부분의 관련 작업은 주로 신체 모캡을 위한 멀티모달 시스템의 일부로 IMU 신호를 사용한다. 영상과 협업하는 IMU 캡처는 가장 광범위하게 연구된 멀티모달 방법 중 하나이다. Maleson et al. [51], Pons-Moll et al. [61, 62], Von Marcard et al. [82]의 초기 작업은 비디오-협업 IMU 시스템들의 개발을 위한 토대를 마련하였고, 궁극적으로, 최근 몇 년 동안, 이러한 Technol\n' +
      '\n' +
      '도 3: IMUSIC의 개요. 먼저 하드웨어 설계와 데이터 획득 파이프라인을 소개한다. 그 후, IMU 신호를 이용한 얼굴 움직임 복구를 위한 데이터 교정 과정과 방법론에 대해 살펴본다. IMUSIC의 배포에 이어 다양한 응용 프로그램을 통해 그 효율성을 입증하여 정밀성과 휴대성을 강조한다.\n' +
      '\n' +
      '학문은 완성도를 달성했다[30, 37, 52, 98]. 깊이 카메라[36, 99], 광학 마커[4] 및 물리 제약 [3, 25, 40, 87, 89]도 IMU 모캡을 위해 일반적으로 사용되는 협업이다. 17개의 별개의 IMU 신호만을 사용하여 신체 동작을 재구성하는 데 초점을 맞춘 노력도 있다[58, 69]. IMU 신호에 대한 연구가 심화되면서 Sparse IMU[39, 66, 71, 74] 개념이 등장하였다. 그 후 SIP[83] 이니셔티브는 IMU 수를 6개로 성공적으로 줄였으며 DIP[39] 프로젝트는 실시간 모캡을 달성하여 이러한 발전을 개선한 TransPose[90] 작업에서 절정에 달했다. 또한 보행 분석을 위해 IMU를 사용하는 것[35]도 더 인기 있는 응용 프로그램 중 하나이다. 시각 기반 방법에 비해 이러한 접근법은 더 넓은 범위의 움직임, 장애물로부터의 자유 및 조명 제약, 얼굴 모션 캡처에 동등하게 적용할 수 있는 이점을 포함하여 상당한 이점을 제공한다.\n' +
      '\n' +
      '그러나 이 방법을 얼굴 적용에 적용하는 것은 간단하지 않다. 처음에는 노이톤[58], 엑스센스[88], 모코피[73]와 같은 상업용 IMU가 상당한 통합을 달성했지만 주로 바디 모캡용으로 설계되었으며 얼굴 캡처를 위해 불균형적으로 크고 무겁다. 하드웨어의 발전으로 이제 더 작은 팔다리의 움직임을 포착하기 위해 IMU를 사용하려는 시도가 있다. Makaussov et al. [50], Mummadi et al. [57] customized IMU gloves for hand posture estimation. 이 돌파구는 더 작은 신체 부위를 재구성하는 IMU의 잠재력에 대해 조명한다. 손 자세를 캡처하는 것은 주로 골격 움직임을 감지하기 위해 IMU를 사용하는 반면, 효과적인 손 움직임 검출을 위해 주로 회전 각도에 의존하지만, 얼굴 캡처는 이러한 특성을 공유하지 않는다. 또한, 손 움직임에서 비교적 고정된 관절과 달리, 대부분의 안면 영역은 해부학적 연구에 의해 통지된 다도 움직임이 가능하다[76]. 우리 연구에서 우리는 광범위한 근육 움직임이 있는 영역에서 IMU의 사용을 최대화하기 위해 해부학적 구조를 참조했다. 그러나 이 전략을 사용하더라도 단순한 회전 데이터를 통해 이러한 움직임을 직접 모델링하는 것은 복잡하다는 것을 증명한다. 가속도계 신호만으로 인간 모캡을 수행하기 위해 게으른 학습 기술을 사용하는 아하[1]는 우리의 접근법에 상당한 영향을 미쳤다. 이 방법의 제한된 성능에도 불구하고, 움직임 예측에서 가속도 신호의 중요성을 드러냈고, 힘의 영향 등 회전만으로는 제공할 수 없는 귀중한 정보를 제공했다. 이 폭로는 IMU 데이터의 처리에서 자세와 가속도 모두에 동등한 중요성을 부여하도록 안내했다.\n' +
      '\n' +
      '## 3 얼굴 IMU\n' +
      '\n' +
      '도. 도 3은 IMUSIC의 개요를 제공한다. 왼쪽 섹션에서는 데이터 수집 프로세스를 간략하게 설명합니다. 참가자들은 다수의 IMU들을 착용하고 대응하는 신호들을 캡처하기 위해 다양한 얼굴 표정들 및 모션들을 수행하도록 요청받는다. 얼굴 움직임을 방해하지 않으면서 얼굴에 동시에 부착할 수 있도록 이 소형 IMU를 설계했습니다. 중간 섹션에서는 네트워크의 데이터 처리 워크플로우를 설명합니다. 수집된 IMU 데이터는 먼저 캘리브레이션을 이용하여 통일된 좌표계에 정렬된다. 그런 다음 정렬된 IMU 신호를 기반으로 얼굴 동작을 재구성하는 IMU2Face 모델을 설정한다. 오른쪽 섹션은 시각 기반 얼굴 캡처의 일부 한계를 해결하기 위한 솔루션을 제공하는 IMUSIC의 잠재적인 응용 프로그램을 강조한다.\n' +
      '\n' +
      '모션 캡처 분야에서 IMU는 객체의 방향과 가속도를 측정하여 객체의 공간 움직임을 반영하는 데 중요한 역할을 한다. Xsens, Sony Mocopi 등과 같은 전신 모션 캡쳐를 위해 설계된 IMU들이 상업적으로 널리 적용되고 있다. 이러한 장치는 일반적으로 감지 센서 및 데이터 전송 모듈을 포함하여 다양한 부분으로 구성되어 있어 얼굴 모션 캡처에 사용하기에는 너무 헐크하다. 또한, 얼굴 캡쳐를 위해 이 모델의 다수의 유닛을 사용하는 것은 심각한 폐색을 초래하여 참가자의 얼굴 표정의 관찰을 방해할 수 있다. 이것은 특히 얼굴 모션 캡처의 고유한 요구 사항과 규모를 충족하도록 맞춤 설계된 IMU의 개발을 필요로 한다.\n' +
      '\n' +
      '우리의 디자인은 얼굴 캡처에 대한 요구 사항을 충족시키기 위해 무게와 크기를 최소화하면서 표준 IMU의 기능을 보존합니다. 도. 4(상단)은 IMU의 크기를 비교한다. 센서 모듈과 데이터 전송 모듈을 분리함으로써 상당한 소형화를 달성했다. 이 디자인을 통해 센서 모듈은 12*10mm만 측정하고 무게는 0.4g에 불과하여 Xsens 모듈 면적의 4분의 1로 완전히 감소했으며 무게는 15%에 불과했다.\n' +
      '\n' +
      '#얼굴 IMU 센서\n' +
      '\n' +
      '도. 4(아래)는 본 연구에서 사용된 특정 하드웨어 구성 요소에 대한 자세한 개요를 제공한다. 센서 모듈은 실리콘 파워의 QMC5883P[64], 측정 범위가 \\(\\pm 30\\) 가우스인 3축 자기장 센서, 3축 자이로스코프와 가속도계가 결합된 QMI8658[64] 통합 칩을 포함하여 총 9축 감지 하위 유닛을 통합한다. 이러한 센서는 60fps의 속도로 공간 위치와 가속도를 정확하게 기록할 수 있다. 데이터 전송 모듈은 주로 ESP32 제어기를 기반으로 한다. UDP 프로토콜을 사용하여 센서 모듈에서 감지된 데이터를 수집하고 수정한다. 또한, Wi-Fi 모듈을 사용하여 계산된 데이터를 호스트 컴퓨터로 전송한다. 이 데이터는 각각의 기록된 인스턴스에서의 시간 스탬프들, 쿼터니언 표현들, 및 가속도 값들을 포함한다.\n' +
      '\n' +
      '저희 페이스 IMU 센서 시스템의 데이터 전송 모듈은 5V 배터리 공급만 있으면 됩니다. 이 설정은 얼굴 IMU 센서 시스템의 휴대성과 착용성을 위한 필수 조건을 제공한다. 나아가 호스트 컴퓨터와의 연결이 Wi-Fi를 통해 이루어짐에 따라 사용자는 Face IMU를 착용한 상태에서 Wi-Fi 신호 범위 내에서 자유롭게 이동할 수 있어 높은 이동도를 구현할 수 있다.\n' +
      '\n' +
      '도. 도 5는 가장 미세한 모션 신호조차도 캡처하는 데 있어 우리의 IMU의 현저한 감도를 예시한다. 이 시연에서 눈썹에 IMU 디텍터를 착용한 참가자는 미묘한 깜박임 동작을 수행한다. (d)에 표시된 파형도에서 우리는 완전한 운동 시퀀스를 명확하게 볼 수 있다: 눈썹이 상승하기 시작할 때 초기 가속으로 시작하여 움직임이 느려질 때 감속 단계가 뒤따르고 눈썹이 움직이지 않을 때 정지 단계에서 절정에 이른다. 그래프는 이러한 단계를 뚜렷하게 구분하며, IMU가 가장 섬세한 움직임까지 감지하는 정밀도를 강조한다. 당사의 보조 비디오는 동작의 보다 역동적인 시각화를 제공합니다.\n' +
      '\n' +
      '### 동기화 및 보정\n' +
      '\n' +
      '우리는 여러 IMU를 사용하여 동시에 얼굴 정보를 캡처하기 위한 필수 기술에 대해 깊이 조사했다. 이를 달성하기 위해서는 동기화 및 보정이라는 두 가지 근본적인 문제를 해결하는 것이 필수적이다. 우리는 하나의 ESP32를 보조 ESP32로 지정하고 다른 ESP32를 동기화 및 보정하기 위한 벤치마크로 사용했다. 이러한 다양한 센서에서 일관성 있고 조정된 데이터 수집을 보장하기 위해 하드웨어 설계 동안 데이터 전송 모듈 내의 이 ESP32에 보정 프로그램을 통합했다. IMU에 의한 데이터 획득 전에, 보조 ESP32의 클록의 데이터 모듈을 기준점으로 하여, 각각의 ESP32가 내부 클록을 외부 참조와 일치시키고, 모든 IMU에 걸쳐 타임스탬프를 동기화시킨다. 이러한 동기화된 신호들로, 회전 행렬은 IMU의 국부 좌표계로부터 세계 좌표계로의 회전을 나타낸다. 특히, IMU의 국부 좌표계로부터 세계 좌표계로의 회전을 나타낸다. 세계 좌표 가속도는 다음과 같이 나타낼 수 있다:\n' +
      '\n' +
      '\\[a^{i}_{j}=(^{\\dagger}\\!R^{i}_{j})^{-1}\\!{}^{\\dagger}\\!a^{i}_{j}. \\tag{1}\\]\n' +
      '\n' +
      '다음으로, 얼굴 구조의 가변성과 IMU 배치의 약간의 불일치의 가능성을 매번 인정하고, 신체 모캡에서 [26, 90]이 사용하는 접근법과 유사한 중성 얼굴 성능의 개념을 채택했다. 참가자들을 위해 IMU를 착용한 후, 각 참가자들은 안면 근육을 이완시키고 중성 상태를 제시했으며 각 IMU의 방향을 기록했다. 후속 계산에서 우리는 이 포즈에 대한 방향을 기준선으로 사용했다:\n' +
      '\n' +
      '\\[{}^{*}\\!R^{i}_{j}=(R^{i}_{\\text{neutral}})^{-1}\\!R^{i}_{j}. \\tag{2}\\]\n' +
      '\n' +
      '두 가지 유형의 움직임이 얼굴에 배치될 때 IMU의 신호에 영향을 미친다는 점을 고려하면, 머리의 전반적인 움직임과 얼굴 표정에 의한 움직임이다. 우리의 초점은 IMU 신호에서 식을 추론하는 것이다. 도 1에 도시된 바와 같다. 도 5의 (b)를 참조하면, 안면 영역 전체에 IMU를 배치할 뿐만 아니라 (c)와 같이 귀 뒤에 보조 IMU를 전략적으로 배치한다. 이 배치는 다른 IMU들에 의해 검출된 배향들 및 가속도들에 대한 일반적인 머리 움직임들의 영향을 완화시키도록 특별히 설계된다. 편의상 보조 IMU의 인덱스를 \\(0\\)으로 정의한다. 교정된 IMU 회전은 다음과 같이 표현될 수 있다.\n' +
      '\n' +
      '[R^{i}_{j}=\\left\\{\\begin{array}{ll}(^{*}\\!R^{i}_{0}}^{-1}\\!R^{i}_{j}&\\text{if }j\\neq 0,\\\\{}^{*}\\!R^{i}_{0}&\\text{otherwise}\\end{array}\\right. \\tag{3}\\text{if }j\\neq 0,\\\\{i}\\!R^{i}_{0}\\text{otherwise}\\end{array}\\right.\n' +
      '\n' +
      '우리는 보정된 IMU 신호를 \\(\\mathcal{S}=\\{\\mathcal{R},\\mathcal{A}\\})로 표시한다.\n' +
      '\n' +
      '그림 4: IMU의 크기 및 아키텍처 설계. 상단: 크기 비교. 저희 IMU는 자연스러운 표정을 방해할 만큼 충분히 작습니다. 하단: 아키텍처 설계입니다. IMU에는 얼굴 유닛과 기본 유닛의 두 가지 주요 구성 요소가 있습니다. 페이스 유닛에는 가속도계, 자이로스코프 및 마그네토미터가 포함되어 있습니다. 얼굴에 부착되어 가속도와 방향 신호를 측정합니다. 얼굴 유닛에 의해 수집된 데이터는 유선으로 주 유닛에 전송된다. 주 유닛은 페이스 유닛에 전원을 공급하고, 데이터를 보정하며, Wi-Fi를 통해 호스트 컴퓨터에 신호를 전달한다.\n' +
      '\n' +
      '##4 IMU 기반 얼굴 모션 캡쳐\n' +
      '\n' +
      '### IMU 배치 및 해부학적 안내\n' +
      '\n' +
      '얼굴 움직임을 정확하게 포착하기 위해서는 얼굴 표면의 별개의 영역에 IMU를 부착하는 것이 필수적이다. 그림 1의 왼쪽 패널에서. 5, 우리는 IMU의 전략적 배치를 제시한다. 이 레이아웃은 안면 근육의 분포에 대한 상세한 분석을 통해 알 수 있다[76]. 우리는 기쁨이나 슬픔과 같은 감정을 표현하는 데 중요한 광대뼈 영역, 말과 관련된 움직임의 기본인 협두근 및 멘탈리스 영역, 미소나 눈썹과 같은 표현의 스펙트럼에 중요한 안와근 영역, 불만이나 우울증과 같은 감정을 전달하는 데 필수적인 전두근 영역을 포함하는 주요 해부학적 랜드마크를 참조하여 뚜렷한 안면 영역을 구분했다. 모든 지정된 지역에서 주요 근육 그룹과 안면 영역에 대한 포괄적인 모니터링을 보장하기 위해 적어도 하나의 IMU를 세심하게 배치했다. 또한, 조밀한 근육 존재 또는 복잡한 움직임을 특징으로 하는 영역에서 이중 IMU 배치를 선택했다. 이 접근법은 이러한 특정 영역에서 완전한 범위의 근육 움직임을 포착하는 데 예외적으로 효과적이다.\n' +
      '\n' +
      '또한, 우리의 디자인 전략은 자연스러운 얼굴 움직임과 참가자의 편안함 모두에 대한 IMU의 영향을 최소화하는 데 급격히 초점을 맞췄다. 특정 안면 부위의 민감도를 인정하여 눈꺼풀과 입 모서리에 IMU를 배치하는 것을 의도적으로 피했는데, 이러한 부위는 광범위한 표현에 중요할 뿐만 아니라 제약이 있는 경우 불편하기 쉽다. IMU가 신중한지 확인하기 위해 우리는 부착을 위해 살색 테이프를 사용하여 시각적 산만함을 최소화하면서 피부와 매끄럽게 혼합할 수 있었다. 배선은 또한 참가자의 표현을 방해하지 않고 수반되는 시각적 데이터의 명확성을 보존하기 위해 얼굴의 주변을 따라 신중하게 라우팅되었다. 이러한 조치를 통해 참가자의 편안함과 표현력을 존중하면서 IMU 데이터와 모든 시각적 기록의 충실도를 유지할 수 있었다.\n' +
      '\n' +
      'IMU-ARKit 데이터 캡처\n' +
      '\n' +
      '블렌드쉐이프 기술은 매우 사실적이고 미묘한 표현을 생성하는 능력으로 인해 얼굴 애니메이션 및 모션 캡처 분야에서 널리 사용되고 있다. 이 기술은 파라메트릭 모델링의 원리로 작동합니다. 구체적으로, 블렌드쉐이프 모델은 블렌드쉐이프 가중치들의 집합에 의해 정의되며, \\(\\mathcal{W}=\\{w_{1},w_{2},\\cdots,w_{m}\\}\\)으로 표시되며, 블렌드쉐이프 모델은 다음과 같이 표현될 수 있다:\n' +
      '\n' +
      '\\[M(\\mathcal{W})=B_{0}+\\sum_{k}^{m}w_{k}B_{k}. \\tag{4}\\]\n' +
      '\n' +
      '여기서 \\(B_{0}\\)은 중립면을 나타내고, \\(B_{k}\\)은 블렌드 쉐이프 기저 벡터이며, \\(m\\)은 블렌드 쉐이프의 수이다. 상이한 블렌드 형상들 사이에서 선형 보간함으로써, 이 접근법은 다수의 얼굴 표정들을 생성할 수 있게 한다.\n' +
      '\n' +
      '우리의 과제는 IMU 데이터\\(\\mathcal{S}\\)에서 이러한 혼합 형상 매개변수\\(\\mathcal{W}\\)을 도출하는 것이다. IMU가 움직임과 방향을 포착할 수 있다는 점을 감안할 때, 우리는 형상 계수를 혼합하기 위해 이러한 물리적 측정을 매핑하는 방법을 제안한다. 이를 위해서는 IMU 판독값을 의미 있는 하이브리드 형상 파라미터로 변환하는 알고리즘의 개발이 필요하다.\n' +
      '\n' +
      '예측을 위한 데이터 기반 솔루션을 실현하기 위해\n' +
      '\n' +
      '도 5: IMUSIC의 데이터 획득 파이프라인. 처음에는 다양한 기본 얼굴 움직임에 해당하는 IMU 신호를 캡처하기 위해 특정 얼굴 위치에 IMU를 배치한다. 동시에, ARKit 파라미터가 기록된다. 데이터 증강 모듈을 위한 시뮬레이터를 병렬로 설계한다.\n' +
      '\n' +
      'IMU를 사용하여 얼굴 블렌드 쉐이프 가중치를 사용하여 그림의 중앙 패널에서 입증된 바와 같이 ARKit 데이터 세트로 얼굴 IMU를 생성하기 시작했다. 5 이 데이터 세트는 모델 훈련을 위한 포괄적인 기반을 보장하기 위해 IMU 및 ARKit의 쌍을 이루는 데이터를 포함하도록 신중하게 컴파일되었다.\n' +
      '\n' +
      '데이터 세트에는 20명의 다른 참가자의 레코드가 포함되어 있습니다. 이 개인들은 모두 18-40세의 연령대에 있고, 영어에 능숙하며, 연기에 약간의 배경을 가지고 있어서 다양하고 생생한 표정을 제공한다. 도 1의 좌측 패널. 도 5(a)는 데이터 수집 셋업의 일 예를 나타낸다. 각 참가자에 대한 \\(11\\) IMU 감지기 세트를 설치하고 획득 좌석에 앉았고, 시각 정보를 캡처한 아이패드 옆에 텔레프롬터 화면이 참가자 바로 앞에 배치되었다. iPad는 시각적 정보를 캡처하기 위해 LiveLinkface[78]를 사용했으며 캡처된 시각적 신호는 RGB 비디오 시퀀스와 ARKit 파라미터의 두 부분으로 나뉘었다.\n' +
      '\n' +
      '공식적인 데이터 수집 과정이 시작되기 전에 참가자들은 자연스럽고 무제한적인 얼굴 움직임을 보장하기 위해 IMU를 착용하는 감각에 적응할 수 있는 시간을 받았다. 그런 다음 참가자는 멘탈리스에 위치한 IMU를 부드럽게 세 번 클릭했으며, 이는 IMU 신호를 시각 신호와 동기화하기 위한 후기 단계로 사용되었다. 각 참가자에 대한 데이터 수집은 3개의 다른 섹션으로 나뉘었으며, 각각은 참가자가 모방할 샘플 비디오가 선행된다. 첫 번째 섹션에서는 참가자들이 제공된 내용을 차분한 톤으로 모국어와 영어가 구분되어 소리 내어 읽는다. 이것은 언어와 관련된 자연스러운 얼굴 움직임을 포착하기 위해 수행되었습니다. 두 번째 세션에서 참가자들은 동일한 내용을 소리 내어 읽도록 요청받았지만 대본의 맥락과 일치하는 특정 감정으로 표정에 감정 레이어를 추가했다. 마지막으로 세 번째 세그먼트는 참가자들에게 특정 의학적 분류에 기반한 일련의 표정을 순차적으로 만들어 전체 범위의 감정과 움직임을 보장하도록 요청했다.\n' +
      '\n' +
      '###### 얼굴 IMU 시뮬레이터 증강\n' +
      '\n' +
      'IMU 데이터 수집 프로그램의 철저함에도 불구하고 훈련 모델을 위해 충분히 큰 데이터 세트를 생성하는 것은 여전히 중요한 과제로 남아 있다. 이는 주로 방대한 양의 IMU-ARKit IMU 데이터를 축적하는 것과 관련된 물류 제약 때문이다. 이 문제를 해결하기 위해, 우리는 다른 유형의 모션 캡처(모캡) 데이터를 IMU-모캡 데이터로 변환하기 위해 인간 IMU 신호를 시뮬레이션하는 데 능숙하게 사용한 [90]의 작업에서 영감을 얻었다. 이 혁신적인 접근법으로 동기부여된 우리는 그림 5의 오른쪽 패널에서 입증된 바와 같이 ARKit 데이터를 사용하여 얼굴 메쉬를 애니메이션화하고 얼굴 표면의 해당 방향 및 가속도 신호를 정확하게 시뮬레이션하도록 설계된 고급 도구인 Face-IMU 시뮬레이터를 개발했다.\n' +
      '\n' +
      'ARKit 데이터 수집을 위한 우리의 방법론은 두 가지 기본 전략을 통합했다. 먼저, 시뮬레이션을 위한 탄탄한 기반을 제공한 풍부한 표정 데이터 소스인 MEAD 데이터셋[85]을 활용했다. 그러나 MEAD는 짧은 문장을 주로 포함하고 특정 표현에 초점을 맞춘 클립이 부족하다는 것을 인식하여 두 번째 전략을 채택했다. 여기에는 LiveLinkFace[78]를 사용하여 추가 발현 서열을 기록하는 것이 포함되어 약 2시간의 특수 클립으로 데이터 세트를 풍부하게 한다. 주로 짧은 텍스트 판독과 표현적 움직임을 포함하는 이러한 추가 시퀀스는 데이터 세트의 범위와 깊이를 크게 확장했다.\n' +
      '\n' +
      'ARKit 시퀀스를 활용하여 식에 따라 각 프레임에 대해 위상적으로 일관된 얼굴 모델을 계산했다. 4 및 ICT 모델[42]에서 제공하는 기반 활용.\n' +
      '\n' +
      'IMU-ARKit 데이터의 IMU(i\\) 위치에 해당하는 각 선택된 정점 \\(i\\)에 대해 각 프레임 \\(j\\)에 대한 IMU \\(v_{j}^{i}\\)의 가속도를 시뮬레이션하기 위해 다음과 같은 공식을 사용했다.\n' +
      '\n' +
      '\\[a_{j}^{i}=\\frac{1}{c\\tau}(v_{j-c}^{i}+v_{j+c}^{i}-2v_{j}^{i}), \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(\\tau\\)은 두 프레임의 시간 간격이고 \\(c\\)은 평활화 정도를 제어하는 상수이다.\n' +
      '\n' +
      '방향을 결정하기 위해 선택된 정점을 포함하는 얼굴 영역에 초점을 맞추었다. 우리는 꼭짓점을 포함하는 면을 \\(\\{v_{j}^{i},v_{j}^{i,\\text{sup}[1]},v_{j}^{i,\\text{sup}[2]}\\}으로 선택한다. 이 정점에 인접한 두 개의 에지를 식별하여 \\(e_{j}^{i,1}=(v_{j}^{i}-v_{j}^{i,\\text{sup}[1]})_{\\text{norm}\\)과 \\(e_{j}^{i,2}=(v_{j}^{i}-v_{j}^{i,\\text{sup}[2]})_{\\text{norm}\\)으로 분류하였다. 이 가장자리에서 얻은 첫 번째 벡터를 IMU 좌표계의 x 축으로 사용했다. 우리는 세계 좌표계에 대한 IMU의 회전 행렬을 다음과 같이 계산했다.\n' +
      '\n' +
      '√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√\n' +
      '\n' +
      'IMU-ARKit 데이터와 마찬가지로 \\(\\prescript{\\text{IR}}{i}{j}\\)을 Eq로 보정한다. 2 및 Eq. 3은 네트워크 훈련에 사용되는 데이터를 얻기 위해 \\(\\mathbf{R}_{j}^{i}\\)이다. IMU의 가속도와 방향을 모두 시뮬레이션하는 이러한 포괄적인 접근법은 데이터 세트가 광범위할 뿐만 아니라 얼굴 움직임의 상세하고 정확한 표현이 풍부하다는 것을 보장한다.\n' +
      '\n' +
      '### IMU 기반 얼굴 추적기\n' +
      '\n' +
      '이 연구에서는 그림 6과 같이 새로운 데이터 흐름을 설정했으며 처음에는 시뮬레이션 및 원시 IMU-ARKit 데이터 모두에 보정 프로세스를 적용했다. 이 절차는 IMU-ARKit 매개변수의 쌍을 이루는 데이터 세트를 생성하는 데 중요한 역할을 했으며, 이는 후속적으로 네트워크의 훈련에 사용되었다.\n' +
      '\n' +
      '네트워크는 보정된 IMU 데이터를 입력으로 가져옵니다. 네트워크에 대한 입력을 단순화하기 위해 우리는 먼저 회전수를 쿼터니언으로 표현한다:\n' +
      '\n' +
      '{l}\\sqrt{\\frac{\\max(0,1+R_{i,11}^{j}+R_{i,22}^{j}+R_{i,33}^{j})}{2},\\sqrt{\\frac{\\max(0,1+R_{i,11}^{j}-R_{i,22}^{j}+R_{i,33}^{j})}\\text{sgn}(R_{i,21}^{j}-R_{i,22}^{j}-R_{i,33}^{j})}\\text{sgn}(R_{i,21}^{j}-R_{i,22}^{j}-R_{i,33}^{j}),\\sqrt{\\frac{\\max(0,1}^{j}-R_{i,11}^{j}-R_{i,22}^{j}}{i,33}^{j}}{2}}\n' +
      '\n' +
      '여기서 \\(\\text{sgn}(\\cdot)\\)는 기호 함수를 나타낸다. 우리는 \\(a_{j}^{i}\\)과 \\(c_{j}^{i}\\in\\mathbb{R}^{7}\\)으로 표기된 \\(c_{j}=\\begin{bmatrix}c_{1},&c_{2},&\\cdots&c_{n}\\end{bmatrix}\\in\\mathbb{R}^{77}\\)으로 표기된 \\(c_{j}^{i}\\)과 \\(c_{j}^{i}\\)을 연결한 후 11개의 IMU 신호를 모두 \\(C_{j}=\\begin{bmatrix}c_{1},&c_{2},&\\cdots&c_{n}\\end{bmatrix}\\in\\mathbb{R}^{77}\\으로 연결한다. 이러한 입력은 세심하게 보정되어 IMU 신호의 정확도와 신뢰성을 모두 보장한다. 세부적으로 imu \\(i\\)의 각 프레임 \\(j\\)에 대해 회전행렬 \\(R_{j}^{i}\\)을 쿼터니온으로 변환한 후 가속도와 함께 결합하여 \\(7\\)차원의 벡터를 구성한다. 본 논문에서는 120\\(120\\)개의 연속된 프레임에서의 11\\(11\\)개의 IMU로부터의 신호를 네트워크 입력으로 사용하는 \\(\\mathbf{C}_{j}\\in\\mathbb{R}^{77\\times 120}\\)에 결합한다.\n' +
      '\n' +
      '우리의 네트워크\\(\\mathbf{\\Psi}(\\cdot)\\)는 초기 Fully Connected (FC) 계층, 변압기 기반 확산 코어, 최종 FC 계층, MLP 임베딩 네트워크의 세 가지 중요한 구성 요소로 구성된다. 우리는 두 개의 FC 층과 변압기 기반 확산 코어를 \\(\\psi(\\cdot)\\)로 표시한다.\n' +
      '\n' +
      '우리의 네트워크는 확산 아키텍처를 중심으로 복잡하게 설계되었습니다. 그 동작의 중심은 반복적 잡음 제거의 개념으로, 최종 예측되는 \\(120\\) 프레임의 블렌드 쉐이프 가중치 \\(\\mathcal{W}_{\\mathbf{\\Psi},j}\\in\\mathbb{R}^{55\\times 120}\\을 달성하기 위해 \\(n\\)회 반복되는 과정이다.\n' +
      '\n' +
      '이 과정의 초기 단계에서 우리는 \\(x_{j}^{n}\\)을 \\(\\mathcal{W}_{\\mathbf{\\Psi},j}\\)과 같은 차원을 갖는 랜덤 잡음으로 설정했다. \\(t\\) 반복을 위해 정수\\(t\\)을 MLP 임베딩 모듈에 공급한다. 그런 다음, 조건에 대한 IMU 신호 \\(\\mathbf{C}_{j}\\)를 \\(x_{j}^{t}\\)와 연결하여 임베딩 출력과 함께 \\(\\psi|\\) 네트워크에 공급하여 \\(x_{j}^{t-1}\\)을 계산한다:\n' +
      '\n' +
      '\\[x_{j}^{t-1}=\\psi(\\text{MLP}(t),x_{j}^{t},\\mathbf{C}_{j}). \\tag{8}\\]\n' +
      '\n' +
      '예측된 블렌드쉐이프 가중치(\\(\\mathcal{W}_{\\mathbf{\\Psi},j}\\)로서 \\(x_{j}^{0}\\)을 얻을 때까지 반복 계산한다.\n' +
      '\n' +
      '훈련 과정에서 지상진리 ARKit 매개변수\\(\\mathcal{W}_{j}\\)는 감독 역할을 한다. L1 손실 함수를 사용하여 네트워크의 성능을 평가하고,\n' +
      '\n' +
      '\\[\\mathcal{L}=\\left|\\mathcal{W}_{\\mathbf{\\Psi}}-\\mathcal{W}\\right|. \\tag{9}\\]\n' +
      '\n' +
      '방정식은 이러한 지상 진실 매개변수에 대해 네트워크의 출력의 정확도를 평가한다.\n' +
      '\n' +
      '우리의 훈련 요법은 네트워크 성능을 최대화하기 위해 각각 설계된 사전 훈련 단계와 피네튠 단계의 두 단계로 체계적으로 나뉜다. 사전 훈련 단계는 페이스-IMU 시뮬레이터에서 얻은 시뮬레이션 데이터를 사용하여 훈련을 포함한다. 이 시뮬레이터는 다양한 표정과 움직임을 제공하여 네트워크가 광범위한 데이터 스펙트럼에서 학습할 수 있도록 하고 일반화 가능성을 보장한다.\n' +
      '\n' +
      '피부 탄력의 변화 및 자기장 유발 오프셋과 같은 시뮬레이션과 IMU-ARKit 데이터 간의 고유한 차이를 인식하여 미세 조정 단계를 구현했다. 이 단계는 실제 IMU-ARKit 데이터로 네트워크를 미세 조정하는 데 전념한다. IMU-ARKit 데이터의 제한된 수량으로 인한 과적합 위험을 피하기 위해 가상 데이터와 IMU-ARKit 데이터를 4:1 비율로 혼합하는 혼합 훈련 접근법을 채택했다. 이 전략은 네트워크가 IMU-ARKit 데이터의 뉘앙스로 인해 이익을 얻지만 가상 데이터가 제공하는 광범위한 학습에 뿌리를 두고 있음을 보장한다. 이러한 균형 잡힌 접근법은 실제 시나리오에서 네트워크의 적응성과 효율성을 향상시킨다.\n' +
      '\n' +
      '그림 6: 훈련 파이프라인. 훈련에는 2단계 접근 방식을 사용하며, 시뮬레이션 데이터를 사용하는 사전 훈련 단계와 실제 데이터와 시뮬레이션 데이터의 적응적으로 결합된 데이터를 사용하는 최종 단계로 나뉜다. 트랜스포머 확산 모듈을 사용하여 IMU 신호에서 식을 예측하고 ARKit 파라미터를 사용하여 출력을 감독한다.\n' +
      '\n' +
      '## 5 Experiment\n' +
      '\n' +
      '본 절에서는 얼굴 모션 캡처 및 재구성에서 IMUSIC의 실험 결과를 제시한다. 구현 측면을 자세히 설명하고 그림 1의 IMU-SIC에 의해 정확하게 재구성된 자산의 갤러리를 표시하는 것으로 시작한다. 7, 우리의 방법으로 달성할 수 있는 정밀도를 강조합니다. 각 참가자에 대해 프레젠테이션을 위해 5가지 다른 표현을 선택했다. 왼쪽 하단에 표시된 것은 IMU 신호 획득과 함께 캡처된 참가자의 사진입니다. IMUSIC을 통해 이러한 IMU 신호로부터 각 프레임에 대한 블렌드쉐이프 파라미터를 도출하였다. 얼굴 자산의 생성 및 렌더링을 위해 드림페이스[96]를 활용하여 얼굴 캡처 기술의 높은 충실도 시각적 출력을 보장했다.\n' +
      '\n' +
      '그리고 IMU 배치 및 개수를 포함한 파이프라인의 모듈에 대한 종합적인 평가를 Sec. 5.1에서 정성 및 정량적으로 수행하고, 그 결과를 Sec. 5.2의 최신 기술과 비교하며, 마지막으로 Sec. 5.3의 IMUSIC에 대한 구체적인 적용 시나리오를 제시한다.\n' +
      '\n' +
      '그림 7: IMUSIC의 갤러리. 각 행은 한 참가자의 5가지 다른 표현식에 해당합니다. 각 하위 그림에 대해 오른쪽 상단 섹션은 IMU 신호에서 재구성된 얼굴 동작을 나타낸다. 왼쪽 하단 섹션에는 이미지 참조가 표시됩니다.\n' +
      '\n' +
      '우리의 네트워크 아키텍처는 Li 등의 작업에 의해 영감을 받았다[43]. 노이즈된 블렌드쉐이프 가중치를 입력으로 사용하고 IMU 데이터를 트랜스포머 조건으로 사용한다. 네트워크 출력이 입력과 동일합니다. 학습률\\(2\\times 10^{-4},\\alpha=0.9,\\beta=0.999\\)을 갖는 Adam을 최적기로 사용한다. 우리는 단일 NVIDIA RTX3090 GPU에서 네트워크를 훈련하고 평가한다. 사전 훈련 과정은 시뮬레이션된 데이터 세트에서 \\(\\approx 4\\) 시간이 소요되고, 페어드 데이터와 단일 아이덴티티에서 최종 과정은 \\(\\approx 0.5\\) 시간이 소요된다.\n' +
      '\n' +
      'IMU 구성의### 평가\n' +
      '\n' +
      'IMU 위치 평가는 해부학적 고려 사항을 기반으로 얼굴 분할 설계를 종합적으로 세부화한다[76]. 이러한 파티션을 설정한 후 중요한 요소는 더 나은 신호 대 잡음비를 얻기 위해 얼굴에 IMU의 최적 배치를 결정하는 것이다. IMU의 높은 감도를 감안할 때 전략적으로 IMU를 배치하면 보다 광범위한 데이터를 캡처할 수 있다. 더욱이, 이러한 유연한 위치들은 더 큰 모션 진폭들을 나타내기 때문에, 이들은 본질적으로 일정한 잡음 레벨들 하에서 더 높은 신호 대 잡음비로 신호들을 캡처한다. 따라서, 특히 특정 지역적 표현 및 움직임 동안 신호 분산이 더 큰 IMU 위치는 더 적응성이 있고 바람직한 배치 포인트로 식별된다.\n' +
      '\n' +
      '우리는 그림 8에서 서로 다른 얼굴 영역에 걸쳐 IMU 배치에 대한 경험적 접근법을 정성적으로 보여준다. 맨 왼쪽에 있는 이미지는 IMU 위치를 선택하기 위한 도식을 보여주며, 얼굴에 흰색 점이 표시되어 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c} \\hline \\hline IMU Index & \\#1 & \\#2 & \\#3 & \\#4 & \\#5 \\\\ \\hline \\multirow{3}{*}{Frontalis Area} & Raising Eyebrows & 0.12 & 0.06 & 0.05 & 0.06 & \\\\  & One-sided From & 0.46 & 0.40 & 0.37 & 0.36 & - \\\\  & Furrowing & 0.21 & 0.08 & 0.05 & 0.10 & \\\\ \\hline \\multirow{3}{*}{Zygomaticus area} & One-sided Pouting & 0.91 & 0.72 & 0.34 & 0.30 & \\\\  & Squinting & 0.30 & 0.23 & 0.19 & 0.18 & - \\\\  & Smile & 0.15 & 0.12 & 0.14 & 0.10 & \\\\ \\hline \\multirow{3}{*}{Buccinator and Mentalis Area} & Reading \\#1 & 0.39 & 1.25 & 1.25 & 1.84 & 4.04 \\\\  & Reading \\#2 & 0.42 & 2.58 & 1.75 & 4.19 & 8.08 \\\\ \\cline{1-1}  & Reading \\#3 & 0.15 & 1.59 & 1.55 & 4.60 & 8.62 \\\\ \\cline{1-1}  & Reading \\#4 & 1.01 & 1.80 & 1.30 & 2.58 & 5.34 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 상이한 영역에서의 IMU 측정. 표의 데이터는 분산 시간\\(10^{-2}\\)으로 표시된다.\n' +
      '\n' +
      '도 8: 얼굴에 IMU 배치에 대한 실험. 이 그림은 선택된 점과 각 얼굴 영역에 대해 수행된 해당 실험을 강조하는 해부학 기반 얼굴 분할을 보여준다. 왼쪽 이미지는 얼굴에서 선택한 점을 보여주는 반면 다른 이미지는 각 특정 영역에 대해 수행된 개별 실험에 대해 자세히 설명한다. 상부 섹션은 각 영역에 할당된 테스트 포인트의 분포 맵을 제시하고, 중간 섹션은 해당 영역과 관련된 주요 표현 및 움직임을 식별하고, 하부 섹션은 각 지정된 지점에 위치한 IMU의 가속 곡선을 나타낸다.\n' +
      '\n' +
      '우리가 선택한 실제 IMU 위치. 왼쪽부터 오른쪽으로 배열된 이미지는 프론탈리스 영역, 광대뼈 영역, 부치네이터 및 멘탈리스 영역에서 테스트 포인트의 실험적 위치를 묘사한다. 최상위 이미지는 각 영역에서 선택된 테스트 IMU 위치를 보여주며 빨간색과 보라색은 우리가 사용한 최종 위치를 나타낸다. 이러한 점은 간섭을 피하고 각 영역의 근육 분포와 정렬하기 위해 전략적으로 선택되었다. 중간 이미지는 IMU를 착용한 참가자가 수행하는 특정 얼굴 움직임을 보여준다. 영상의 하단에 표시된 이러한 움직임 동안 각 IMU에서 수집된 가속도 데이터를 정성적으로 비교했다. 최종 IMU 위치 선택은 모두 상당한 이동 진폭을 나타내는 강한 신호 강도를 나타냈다.\n' +
      '\n' +
      '표 1에서 다양한 안면 영역에 대한 다양한 행동에 대한 분산 비교를 정량적으로 제시하며, 특히 전두엽 영역에서 IMU1이 훨씬 더 높은 분산을 보여 배치에 대한 결정을 안내한다. 부치네이터 및 멘탈리스 영역에서 IMU 1~3은 부치네이터 검출에 전용되며 IMU 4와 5는 멘탈리스에 중점을 둔다. 이 중 IMU 2와 5는 가장 주목할만한 분산을 표시하므로 이 영역에 배치하기 위해 선택한다. 자이오티쿠스 영역의 경우 IMU1의 뚜렷한 분산이 선호 배치점으로 만든다. 또한 그림 8에서 선택된 얼굴 표정과 해당 IMU 데이터를 설명한다. 이는 깜박임 데이터의 안정성을 감소시킬 수 있지만, 다른 눈 움직임의 효율적인 포착을 보장하여 눈의 편안함을 보장한다.\n' +
      '\n' +
      'IMU 배치 실험을 기반으로 IMU 수의 평가는 발현 예측에 대한 IMU 수의 영향을 추가로 테스트했다. 위에서 언급한 11개의 IMU 배치 중 IMU에서 데이터를 순차적으로 균일하게 제거했다. 각 실험에 대해 다른 변수를 통제했으며 둘 다 시뮬레이션 데이터를 사용한 사전 훈련과 실제 데이터를 사용한 미세 조정을 포함하여 2단계 훈련 파이프라인을 사용했다. 그림의 곡선입니다. 도 9는 IMU 수의 각각의 감소에서 IMU 시그널링 정확도에 의한 표현들의 네트워크의 예측이 균일하게 감소됨을 도시한다. 이것은 우리의 IMU가 그들 사이에 낮은 중첩을 가지며 얼굴의 다양한 영역으로부터 신호의 획득을 최대화한다는 것을 나타낸다. 디바이스가 여전히 그것의 프로토타입 단계에 있기 때문에, 배치를 위한 시간 투자는 각각의 추가적인 IMU에 따라 상당히 증가한다. 따라서, 우리는 효율성과 효율성의 균형을 맞추는 실증적 연구를 위해 11개의 IMU를 사용하기로 선택했다.\n' +
      '\n' +
      'IMU 기반 얼굴 캡쳐의### 평가\n' +
      '\n' +
      '첫 번째 IMU 기반 얼굴 캡쳐 기법으로서, 최신 이미지 기반 표정 예측 기법 DECA[27] 및 3DDFA_V2[31]와 비교하였다. IMU-ARKit 데이터 세트의 테스트 세트에 대한 실험을 수행하고 iPad 캡처 이미지를 DECA 및 3DDFA_V2의 입력으로 간주하고 결과를 그림에 표시한다. 10. 2열, 5열 및 6열은 각각 DECA 및 3DDFA_V2의 이미지 재구성 결과와 비교하여 IMU 기반 재구성의 결과를 보여준다. 도면에 예시된 결과로부터, 우리의 재구성 결과는 최첨단 방법의 결과와 유사하다. DECA에 비해 우리의 방법은 눈 재구성에서 약간의 개선을 보여주며, 3DDFA_V2와 비교하여 우리의 접근법은 눈썹 표현을 상대적으로 더 잘 나타낸다. 우리의 방법은 여전히 일부 정렬 오류 문제를 나타내지만, 우리의 접근법은 전체적으로 유사한 수준의 품질을 일관되게 달성한다.\n' +
      '\n' +
      '또한, 데이터 증강과 미세 조정이라는 두 가지 주요 단계에 대한 포괄적인 평가를 수행한다. 각 입력 샘플에 대해 세 가지 별개의 실험 설정을 구현했다. 처음에는 데이터 증강과 미세 조정을 모두 포함하는 전체 파이프라인을 사용하여 결과를 제시한다. 그 후, 우리는 **w/o 시뮬레이션**로 표시된 시뮬레이션 데이터 없이 네트워크를 완전히 훈련한 결과를 보여준다. 마지막으로, 피네튜닝 없이 네트워크 예측을 전개할 때, **w/o 피네튜닝**로 표시되는 결과를 표시한다. 이러한 변형은 그림 2, 3 및 4 열에 설명되어 있다. 도 10을 순차적으로 나타낸다. **w/o 시뮬레이션** 및 **w/o 미세** 모두 구강 재구성에서 더 큰 불일치를 갖는다는 것이 눈에 띈다. 추가적으로, 재구성 결과 **w/o finetune**는 눈 영역에서 더 많은 수의 에러를 나타낸다. 전반적으로 이러한 관찰은 완전한 파이프라인을 사용하면 식을 보다 정확하게 추정할 수 있음을 강조한다.\n' +
      '\n' +
      '정성적 결과에 대한 보충으로 그림 1에 표시된 정량적 평가를 소개한다. 11. [27, 31, 70]에서 영감을 받아 ARKit 캡처와 우리의 예측 사이의 유사성을 나타내는 지표로서 변형된 메쉬 상의 정점당 3D 오차(PVE)[70]를 계산하고, 시각적으로 중요한 영역 상의 유사성을 추가로 설명하기 위해 3D 랜드마크 정점 오차(PVE_LMK)를 계산한다. 또한 ARKit을 사용하여 예측된 블렌드쉐이프 가중치에 대한 MSE를 계산한다. 빨간색 선은 본인을 사용하여 각 프레임에 대한 메트릭을 나타냅니다.\n' +
      '\n' +
      '도 9: PVE 및 MSE를 갖는 IMU의 수. 다양한 수의 IMU를 사용하여 실험을 수행하고, 정량적 평가를 위해 변형된 메쉬에 대한 정점당 오차(PVE)와 블렌드쉐이프 가중치에 대한 MSE를 계산한다.\n' +
      '\n' +
      '포괄적인 2단계 방법과 보라색 및 파란색 선은 단일 단계 접근법을 사용하여 각 프레임에 대한 메트릭을 나타낸다. 완전한 파이프라인의 메트릭이 명확한 이점을 입증한다는 것은 명백하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c|c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c|}{PVE[mm]\\(\\downarrow\\)} & \\multicolumn{3}{c|}{PVE\\_LMK[mm]\\(\\downarrow\\)} & \\multirow{2}{*}{MSE\\(\\downarrow\\)} \\\\ \\cline{2-2} \\cline{5-8}  & median & mean & std & median & mean & std \\\\ \\hline Ours & **0.67** & **0.72** & **0.31** & **1.07** & **1.15** & **0.51** & **0.0057** \\\\\n' +
      '**w/o simulation** & 1.18 & 1.28 & 0.74 & 1.93 & 2.18 & 1.32 & 0.0322\\\\\n' +
      '**w/o finetune** & 1.54 & 1.65 & 0.60 & 2.52 & 2.74 & 1.07 & 0.0187 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 우리 방법의 정량적 절제 연구.\n' +
      '\n' +
      '그림 10: 질적 비교 및 절제 연구. 첫 번째 열에는 참조 이미지가 표시됩니다. 두 번째 열은 파이프라인의 재구성 결과를 보여준다. 세 번째와 네 번째 열은 시뮬레이션 없이 미세 조정 없이 파이프라인의 결과를 보여준다. 열 5 내지 7은 DECA[27], 3DDFA_V2[31] 및 ARKit[5]의 결과를 예시한다.\n' +
      '\n' +
      '우리는 또한 표 2에 특정 수치를 제시하며, 이 정량적 분석은 시뮬레이션 데이터 세트와 미세 조정을 모두 통합하는 전체 파이프라인이 가장 유리한 결과를 산출함을 보여준다. 이는 미세화 단계가 없는 결과에서 과소 적합되고 시뮬레이션 데이터 세트가 없는 경우 과대 적합되는 경향이 있음을 나타낸다. 이러한 발견은 네트워크 교육에서 우리의 접근법의 중요성을 강조한다.\n' +
      '\n' +
      '### Application\n' +
      '\n' +
      '오늘날 디지털 환경에서 프라이버시 보호 페이셜 애니메이션은 특히 페이셜 애니메이션과 디지털 아바타 분야에서 프라이버시 문제가 점점 더 전면에 등장하고 있다. 가상 플랫폼의 인기의 급증은 특히 디지털 아바타를 통해 자신을 표현하기로 선택한 사람들에게 사용자 익명성과 프라이버시를 보호할 필요성을 강조한다. 이 문제는 가상 스트리밍 또는 VTubing과 같은 맥락에서 특히 중요하며, 크리에이터는 종종 광범위한 청중과 참여하면서 개인 정보를 보호하려고 합니다. 전통적인 얼굴 애니메이션 방법은 주로 시각적 데이터 캡처에 의존하며, 일반적으로 카메라 기반 시스템을 통한 모델 위치 파악이 필요하다. 얼굴 움직임을 추적하기 위해 시각적 입력에 의존하는 이러한 방법은 부주의하게 프라이버시를 침해할 수 있다. 더욱이, 이들은 적절한 조명의 필요성 및 사용자의 얼굴에 대한 직접적인 시각적 경로와 같은 환경적 요인에 의해 제약되어, 저조도 설정에서 또는 사용자의 얼굴이 카메라를 향해 일관되게 지향되지 않을 때 도전을 초래한다. 우리의 혁신적인 접근법은 얼굴 애니메이션을 위해 관성 측정 유닛(IMU)을 활용하므로 얼굴의 직접적인 시각적 캡처의 필요성을 제거한다. 이 기술은 시각적 얼굴 데이터의 캡처 및 저장을 피함으로써 사용자 프라이버시를 유지할 뿐만 아니라 전통적인 카메라 기반 시스템에 내재된 한계를 극복한다. IMU 기반 기술로 조명 조건이나 얼굴의 방향과 같은 요소는 더 이상 장애물이 아닙니다. VTubing의 영역에서 우리의 방법은 상당한 이점을 제공한다. 이는 익명성을 선호하는 콘텐츠 제작자에게 프라이버시를 보장하면서 저조도 조건 또는 사용자가 카메라를 직접 마주하지 않을 때와 같은 기존의 시각적 캡처 방법에 적합하지 않은 환경에서 작동하는 문제를 능숙하게 해결한다. 본 출원은 도 1에 예시되어 있다. 12에서 사용자는 IMU 장치를 착용하고 IMUSIC를 사용하여 표현 블렌드 쉐이프 매개변수를 추론한다. 우리는 유니티로 스트리밍되는 일련의 파라미터를 통해 아바타 애니메이션을 용이하게 한다[77]. 그림에 묘사된 것처럼, 우리의 방법은 발표자가 고개를 돌리거나 폐색을 경험하는 것과 같은 복잡한 시나리오에서도 표현을 정확하게 예측한다.\n' +
      '\n' +
      '하이브리드 캡처위(Hybrid CaptureWe)는 이제 IMU 기반 얼굴 캡처와 애플의 ARKit이라는 두 가지 별개의 기술의 능력을 시너지시키는 기술인 하이브리드 캡처의 개념을 탐구한다. 하이브리드 캡처는 IMU 시스템에 의해 예측된 블렌드 쉐이프 매개변수를 ARKit에 의해 검출된 매개변수와 통합하는 것을 수반한다. 이 방법은 ARKit의 시각적 신호들이 가려지는 시나리오들에서 특히 유리하여, IMU-유도된 블렌드쉐이프 신호들이 보충하도록 허용한다.\n' +
      '\n' +
      '그림 11: 데이터 시퀀스에 대한 우리의 방법의 정량적 결과. 우리는 테스트 세트의 시퀀스에 ARKit를 그라운드 진리로 사용하여 프레임당 계산된 PVE, PVE_LMK 및 MSE를 플로팅한다. 그 결과는 우리의 2단계 훈련 전략의 신뢰성을 보여준다.\n' +
      '\n' +
      '도 12: Vtubing의 적용. 이 방법은 얼굴 애니메이션에 IMU를 사용하여 카메라 아래 직접 노출에 대한 요구 사항을 제거한다.\n' +
      '\n' +
      '그리고 시각적 데이터를 대체합니다. 하이브리드 캡처의 구현은 가상 앨범 제작에서 보여집니다. 스튜디오 환경에서 일반적인 이슈는 마이크에 의해 배우의 입이 가려져 ARKit과 같은 시각 기반 시스템이 완전한 얼굴 표정을 포착하는 것을 방해한다는 것이다. 여기서 IMU는 가려진 입 움직임을 적절하게 보상하여 포괄적인 얼굴 포착을 보장한다. 이는 보다 사실적이고 표현적인 가상 퍼포먼스를 가져온다. 도 13에서는 IMUSIC 파이프라인의 파라미터 시퀀스를 사용하여 해당 ARKit-녹화된 시퀀스를 대체하고, Jawopen, Mouthsmile 등과 같은 파라미터를 중심으로 본 응용 프로그램을 설명한다. 결과는 배우의 가려진 입 움직임을 보완하는 데 있어 본 방법의 유효성을 확인한다.\n' +
      '\n' +
      'IMU의 높은 감도를 활용하여 미세한 얼굴 움직임을 정밀하게 감지할 수 있었다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 14를 참조하면, IMU의 효과는 두 번째 행의 어두운 회색 세그먼트로 표시된 참가자의 미묘한 뺨 퍼핑을 캡처하는 데 나타난다. 볼 위치에서 IMU에 의해 측정된 회전을 사용하여 중립 포즈와의 각도를 계산한 다음 \'볼퍼프\' 블렌드 쉐이프 매개변수에 매핑된다.\n' +
      '\n' +
      '또한 ARKit(보라색 라인)에서 감지된 무게에 매핑(빨간색 라인)을 비교하여 IMU 데이터에서 실제 퍼핑 동작과 더 큰 정렬을 보여준다. 특히, 액션이 너무 미묘하기 때문에 ARKit 신호는 벌지에 따라 거의 변하지 않는다. 시각적 명확성을 위해 첫 번째 행에서 주어진 순간(빨간색 삼각형으로 표시)에 참가자의 체크 퍼프를 보여주며, ARKit의 신호 캡처 능력에 비해 IMU의 증가된 감도를 보여준다.\n' +
      '\n' +
      '### 제한과 토론\n' +
      '\n' +
      '우리는 다양한 응용 분야에서 비시각적 얼굴 모션 캡처에 대한 IMUSIC의 강력한 능력을 입증했다. 그럼에도 불구하고 순수 IMU 기반 안면 성능 캡처를 위한 새로운 시험으로서 우리의 접근법은 몇 가지 제한 사항이 있다.\n' +
      '\n' +
      '첫째, 프로토타입 하드웨어에서 신호 검출 모듈은 여전히 양면 테이프와 접착 테이프로 성능의 얼굴 피부에 부착되어야 하며 데이터 전송도 지루한 배선이 필요하다. 재료 과학 및 집적 회로의 기술 동향의 예상되는 지속을 감안할 때 IMU 유사 프로토타입은 더 저렴하고 더 작고 안전하며 더 많은 퍼바이가 되고 있다.\n' +
      '\n' +
      '도 14: 미세한 움직임을 포착하기 위한 애플리케이션. 첫 번째 행에 대해 참가자 퍼핑의 이미지, ARKit에서 감지한 결과 및 IMU를 사용하여 측정한 결과를 보여준다. 그래프의 두 번째 행에서 어두운 회색 영역은 참가자가 볼을 퍼프하는 시간을 나타내고 빨간색과 보라색 선은 각각 우리의 방법과 ARKit으로 얻은 Checkpuff 매개변수를 나타낸다.\n' +
      '\n' +
      '도 13: 가상 앨범 제작 및 노래 녹음의 적용. 이 환경에서 IMUSIC는 가려진 입 움직임을 효과적으로 보상하여 얼굴 표정의 포괄적인 캡처가 보장된다.\n' +
      '\n' +
      '활기차네요 유연한 전자 재료를 사용하는 모션 장갑에서 영감을 얻었으며[57], 그림 1에 개념적으로 묘사된 바와 같이 전자 재료를 사용하여 보다 사용자 친화적이고 착용 가능한 솔루션을 탐구할 수 있다. 15. IMUSIC와 같은 모션 캡처 시스템은 전통적인 얼굴 캡처 기술에 대한 실행 가능한 대안이 될 것이라고 믿는다.\n' +
      '\n' +
      '또한 IMUSIC의 훈련 계획은 DECA 및 ARKit과 같은 비전 기반 접근법에 비해 일반화 가능성을 제한하는 2단계 사전 훈련 후 미세 조정 패러다임에 여전히 의존한다. 이는 주로 IMU-ARKit 데이터 세트의 데이터 부족 때문이다. 데이터 증강 외에도 데이터 의존성을 더욱 줄이기 위해 비감독 및 자체 감독 전략을 활용하는 것이 유망하다[24]. 그러나 시뮬레이션된 IMU 신호와 실제 IMU 신호 간의 차이를 극복하고 실제 데이터를 보다 밀접하게 모방하기 위해 시뮬레이션 모듈을 향상시키기 위해서는 향후 탐사가 필요하다. 더욱이, 현재의 순수 IMU 기반 얼굴 캡처 방식은 특히 IMU 센서를 쉽게 부착할 수 없는 눈꺼풀 근처의 영역에 대해 여전히 극도로 도전적인 얼굴 동작을 캡처하는 데 미치지 못한다. 더 충실하고 프라이버시가 보호되는 얼굴 캡처를 위해 오디오 및 광전자 신호와 같은 다른 비시각적 양식과 우리의 계획을 결합하는 것은 흥미롭다.\n' +
      '\n' +
      '우리 커뮤니티에서 위와 같은 새롭고 흥미로운 얼굴 캡처 경로를 자극하기 위해 IMU-ARKit 데이터 세트와 IMUSIC 접근법의 동반 코드를 공개적으로 사용할 수 있도록 할 것이다. 우리의 접근법은 데이터 기반 IMU 기반 얼굴 추적 및 분석을 위한 연구 방향을 여는 최초의 접근법이다. 따라서, 우리는 접근 가능한 IMUSIC 구현과 IMU-ARKit 데이터 세트가 견고한 초석을 마련하고 향후 탐사의 큰 잠재력을 가져올 것이라고 믿는다. IMUSIC의 고유한 개인 정보 보호 기능에도 불구하고 광범위한 얼굴 데이터의 수집은 여전히 과제로 남아 있다. 우리는 데이터 사용 및 수집 프로세스에 관한 모든 참가자의 사전 동의를 보장했으며 연구에서 윤리적 표준을 준수하기 위한 IRB 허가를 확보했다. 요약하면, IMUSIC는 얼굴 캡처를 위한 새로운 경로를 제시하지만, 그 진화는 하드웨어 설계, 훈련 방법론 및 윤리적 데이터 수집 관행의 발전과 얽혀 있다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '우리는 전통적인 시각 방법과 구별되는 순수 IMU 신호로부터 얼굴 모션 캡처를 위한 새로운 패러다임인 IMUSIC를 도입했다. IMUSIC에서 맞춤형 마이크로 IMU는 광범위한 미묘한 얼굴 움직임을 포착하기 위해 얼굴 해부학과 정렬된 얼굴 영역에 전략적으로 부착된다. 그런 다음 동기 IMU와 다양한 출연자의 다양한 얼굴 표정의 시각적 신호를 가진 최초의 데이터 세트인 IMU-ARKit을 제공한다. 본 논문에서는 IMU-ARKit을 기반으로 새로운 태스크인 IMU 구동 얼굴 모션 캡쳐를 위한 강력한 베이스라인 방법을 제안한다. 구체적으로, 우리는 새로운 2단계 훈련 전략을 사용하여 순수 IMU 신호에서 발현 매개변수를 예측하기 위해 변압기 기반 확산 모델을 조정한다. 우리는 접근법의 효과를 입증하기 위해 광범위한 실험을 수행한다. 그런 다음 익명 가상 유튜버를 위한 프라이버시 보호 얼굴 캡처에서 폐색과 같은 문제를 극복하기 위한 하이브리드 캡처 또는 시각적 카메라를 통해 종종 보이지 않는 미세한 얼굴 움직임을 감지하는 IMUSIC를 사용하는 일련의 새로운 잠재적 애플리케이션을 보여준다. 새로운 작업과 동반 데이터 세트 및 베이스라인 솔루션을 통해 IMUSIC가 얼굴 모션 캡처 및 분석의 연구 경계를 갱신한다고 믿는다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] David W Aha. Lazy learning. In _Lazy learning_, pages 7-10. Springer, 1997.\n' +
      '* [2] Norhafizan Ahmad, Raja Ariffin Raja Ghazilla, Nazirah M Khairi, and Vijayakaskar Kasi. Reviews on various inertial measurement unit (imu) sensor applications. _International Journal of Signal Processing Systems_, 1(2):256-262, 2013.\n' +
      '* [3] Sadegh Aliakbarian, Pashmina Cameron, Federica Bogo, Andrew Fitzgibbon, and Thomas J Cashman. Flag: Flow-based 3d avatar generation from sparse observations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13253-13262, 2022.\n' +
      '* [4] Sheldon Andrews, Ivan Huerta, Taku Komura, Leonid Sigal, and Kenny Mitchell. Real-time physics-based motion capture with sparse sensors. In _Proceedings of the 13th European conference on visual media production (CVMP 2016)_, pages 1-10, 2016.\n' +
      '* [5] Apple. Arkit. [https://developer.apple.com/arkit/](https://developer.apple.com/arkit/), 2023.\n' +
      '* [6] Eric R Bachmann, Robert B McGhee, Xiaoping Yun, and Michael J Zyda. Inertial and magnetic posture tracking for inserting humans into networked virtual environments. In\n' +
      '\n' +
      '그림 15: 개념적 시각화는 차세대 IMU 기반 얼굴 캡처 장치를 보여준다. 이미지는 DALL-E에 의해 생성된다.\n' +
      '\n' +
      '_Proceedings of the ACM symposium on Virtual reality software and technology_, pages 9-16, 2001.\n' +
      '* [7] Linchao Bao, Xiangkai Lin, Yajing Chen, Haoxian Zhang, Sheng Wang, Xuefei Zhe, Di Kang, Haozhi Huang, Xinwei Jiang, Jue Wang, et al. High-fidelity 3d digital human head creation from rgb-d selfies. _ACM Transactions on Graphics (TOG)_, 41(1):1-21, 2021.\n' +
      '* [8] Julio Cesar Batista, Vitor Albiero, Olga RP Bellon, and Luciano Silva. Aumpnet: simultaneous action units detection and intensity estimation on multipose facial images using a single convolutional neural network. In _2017 12th IEEE international conference on automatic face & gesture recognition (FG 2017)_, pages 866-871. IEEE, 2017.\n' +
      '* [9] Thabo Beeler, Fabian Hahn, Derek Bradley, Bernd Bickel, Paul A Beardsley, Craig Gotsman, Robert W Sumner, and Markus H Gross. High-quality passive facial performance capture using anchor frames. _ACM Trans. Graph._, 30(4):75, 2011.\n' +
      '* [10] L Bianchi, D Angelini, GP Orani, and F Lacquantiti. Kinematic coordination in human gait: relation to mechanical energy cost. _Journal of neurophysiology_, 79(4):2155-2170, 1998.\n' +
      '* [11] V Blanz and T Vetter. A morphable model for the synthesis of 3d faces. In _26th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH 1999)_, pages 187-194. ACM Press, 1999.\n' +
      '* [12] Derek Bradley, Wolfgang Heidrich, Tiberiu Popa, and Alla Sheffer. High resolution passive facial performance capture. In _ACM SIGGRAPH 2010 papers_, pages 1-10. 2010.\n' +
      '* [13] Chen Cao, Yanlin Weng, Stephen Lin, and Kun Zhou. 3d shape regression for real-time facial animation. _ACM Transactions on Graphics (TOG)_, 32(4):1-10, 2013.\n' +
      '* [14] Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun Zhou. Facewarehouse: A 3d facial expression database for visual computing. _IEEE Transactions on Visualization and Computer Graphics_, 20(3):413-425, 2013.\n' +
      '* [15] Chen Cao, Derek Bradley, Kun Zhou, and Thabo Beeler. Real-time high-fidelity facial performance capture. _ACM Transactions on Graphics (ToG)_, 34(4):1-9, 2015.\n' +
      '* [16] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz, Michael Zollhoefer, Shun-Suke Saito, Stephen Lombardi, Shih-En Wei, Danielle Belko, Shoou-I Yu, et al. Authentic volumetric avatars from a phone scan. _ACM Transactions on Graphics (TOG)_, 41(4):1-19, 2022.\n' +
      '* [17] Xudong Cao, Yichen Wei, Fang Wen, and Jian Sun. Face alignment by explicit shape regression. _International journal of computer vision_, 107:177-190, 2014.\n' +
      '* [18] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7291-7299, 2017.\n' +
      '* [19] Timothy F Cootes, Christopher J Taylor, David H Cooper, and Jim Graham. Active shape models-their training and application. _Computer vision and image understanding_, 61(1):38-59, 1995.\n' +
      '* [20] Timothy F. Cootes, Gareth J. Edwards, and Christopher J Taylor. Active appearance models. _IEEE Transactions on pattern analysis and machine intelligence_, 23(6):681-685, 2001.\n' +
      '* [21] Juan Antonio Corrales, Francisco A Candelas, and Fernando Torres. Hybrid tracking of human operators using imu/uwb data fusion by a kalman filter. In _Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction_, pages 193-200, 2008.\n' +
      '* [22] Edilson de Aguiar, Christian Theobalt, Marcus Magnor, Holger Theisel, and H-P Seidel. M/sup 3: marker-free model reconstruction and motion tracking from 3d voxel data. In _12th Pacific Conference on Computer Graphics and Applications, 2004. PG 2004. Proceedings._, pages 101-110. IEEE, 2004.\n' +
      '* [23] Michael B Del Rosario, Heba Khamis, Phillip Ngo, Nigel H Lovell, and Stephen J Redmond. Computationally efficient adaptive error-state kalman filter for attitude estimation. _IEEE Sensors Journal_, 18(22):9332-9342, 2018.\n' +
      '* [24] Abdallah Dib, Cedric Thebault, Junghyun Ahn, Philipp-Henri Gosselin, Christian Theobalt, and Louis Chevallier. Towards high fidelity monocular face reconstruction with rich reflectance using self-supervised learning and ray tracing. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 12819-12829, 2021.\n' +
      '* [25] Andrea Dittadi, Sebastian Dziadzio, Darren Cosker, Ben Lundell, Thomas J Cashman, and Jamie Shotton. Full-body motion from a single head-mounted device: Generating smpl poses from partial observations. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11687-11697, 2021.\n' +
      '* [26] Bernhard Egger, William AP Smith, Ayush Tewari, Stefanie Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani, et al. 3d morphable face models--past, present, and future. _ACM Transactions on Graphics (ToG)_, 39(5):1-38, 2020.\n' +
      '* [27] Yao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart. Learning an animatable detailed 3d face model from in-the-wild images. _ACM Transactions on Graphics (ToG)_, 40(4):1-13, 2021.\n' +
      '* [28] Giancarlo Ferrigno, NA Borghese, and Antonio Pedotti. Pattern recognition in 3d automatic human motion analysis. _ISPRS Journal of Photogrammetry and Remote Sensing_, 45(4):227-246, 1990.\n' +
      '* [29] Eric Foxlin. Inertial head-tracker sensor fusion by a complementary separate-bias kalman filter. In _Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium_, pages 185-194. IEEE, 1996.\n' +
      '* [30] Andrew Gilbert, Matthew Trumble, Charles Malleson, Adrian Hilton, and John Collomosse. Fusing visual and inertial sensors with semantics for 3d human pose estimation. _International Journal of Computer Vision_, 127:381-397, 2019.\n' +
      '* [31] Jianzhu Guo, Xiangyu Zhu, Yang Yang, Fan Yang, Zhen Lei, and Stan Z Li. Towards fast, accurate and stable 3d dense face alignment. In _European Conference on Computer Vision_, pages 152-168. Springer, 2020.\n' +
      '\n' +
      '* [32] Yan Guo, Gang Xu, and Saburo Tsuji. Understanding human motion patterns. In _Proceedings of the 12th IAPR International Conference on Pattern Recognition, Vol. 3-Conference C: Signal Processing (Cat. No. 94CH3440-5)_, pages 325-329. IEEE, 1994.\n' +
      '* [33] Yudong Guo, Jianfei Cai, Boyi Jiang, Jianmin Zheng, et al. Cnn-based real-time dense face reconstruction with inverse-rendered photo-realistic face images. _IEEE transactions on pattern analysis and machine intelligence_, 41(6):1294-1307, 2018.\n' +
      '* [34] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and Christian Theobalt. Livecap: Real-time human performance capture from monocular video. _ACM Transactions On Graphics (TOG)_, 38(2):1-17, 2019.\n' +
      '* [35] Julius Hannink, Thomas Kautz, Cristian F Pasiuosta, Karl-Gunter Gassmann, Jochen Klucken, and Bjoern M Eskofier. Sensor-based gait parameter extraction with deep convolutional neural networks. _IEEE journal of biomedical and health informatics_, 21(1):85-93, 2016.\n' +
      '* [36] Thomas Helten, Meinard Muller, Hans-Peter Seidel, and Christian Theobalt. Real-time body tracking with one depth camera and inertial sensors. In _Proceedings of the IEEE international conference on computer vision_, pages 1105-1112, 2013.\n' +
      '* [37] Roberto Henschel, Timo Von Marcard, and Bodo Rosenhahn. Accurate long-term multiple people tracking using video and body-worn imus. _IEEE Transactions on Image Processing_, 29:8476-8489, 2020.\n' +
      '* [38] Fuyang Huang, Ailing Zeng, Minhao Liu, Qiuxia Lai, and Qiang Xu. Deepfuse: An imu-aware network for real-time 3d human pose estimation from multi-view image. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 429-438, 2020.\n' +
      '* [39] Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J Black, Otmar Hilliges, and Gerard Pons-Moll. Deep inertial poser: Learning to reconstruct human pose from sparse inertial measurements in real time. _ACM Transactions on Graphics (TOG)_, 37(6):1-15, 2018.\n' +
      '* [40] Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa Laich, Patrick Snape, and Christian Holz. Avatar-poser: Articulated full-body pose tracking from sparse motion sensing. In _European Conference on Computer Vision_, pages 443-460. Springer, 2022.\n' +
      '* [41] Samuli Laine, Tero Karras, Timo Aila, Antti Herva, Shunsuke Saito, Ronald Yu, Hao Li, and Jaakko Lehtinen. Production-level facial performance capture using deep convolutional neural networks. In _Proceedings of the ACM SIGGRAPH/Eurographics symposium on computer animation_, pages 1-10, 2017.\n' +
      '* [42] Jiaman Li, Zhengfei Kuang, Yajie Zhao, Mingming He, Karl Bladin, and Hao Li. Dynamic facial asset and rig generation from a single scan. _ACM Trans. Graph._, 39(6):215-1, 2020.\n' +
      '* [43] Jiaman Li, Karen Liu, and Jiajun Wu. Ego-body pose estimation via ego-head pose estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17142-17151, 2023.\n' +
      '* [44] Yuwei Li, Minye Wu, Yuyao Zhang, Lan Xu, and Jingyi Yu. Piano: A parametric hand bone model from magnetic resonance imaging. _arXiv preprint arXiv:2106.10893_, 2021.\n' +
      '* [45] Daizong Liu, Hongting Zhang, and Pan Zhou. Video-based facial expression recognition using graph convolutional networks. In _2020 25th International Conference on Pattern Recognition (ICPR)_, pages 607-614. IEEE, 2021.\n' +
      '* [46] Huajun Liu, Xiaolin Wei, Jinxiang Chai, Inwoo Ha, and Taehyun Rhee. Realtime human motion control with a small number of inertial sensors. In _Symposium on interactive 3D graphics and games_, pages 133-140, 2011.\n' +
      '* [47] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser Sheikh. Deep appearance models for face rendering. _ACM Transactions on Graphics (ToG)_, 37(4):1-13, 2018.\n' +
      '* [48] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl: A skinned multi-person linear model. _ACM Transactions on Graphics_, 34(6), 2015.\n' +
      '* [49] Luming Ma and Zhigang Deng. Real-time hierarchical facial performance capture. In _Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games_, pages 1-10, 2019.\n' +
      '* [50] Oleg Makaussov, Mikhail Krassavin, Maxim Zhabinets, and Siamac Fazli. A low-cost, imu-based real-time on device gesture recognition glove. In _2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)_, pages 3346-3351. IEEE, 2020.\n' +
      '* [51] Charles Malleson, Andrew Gilbert, Matthew Trumble, John Collomosse, Adrian Hilton, and Marco Volino. Real-time full-body motion capture from video and imus. In _2017 International Conference on 3D Vision (3DV)_, pages 449-457. IEEE, 2017.\n' +
      '* [52] Charles Malleson, John Collomosse, and Adrian Hilton. Real-time multi-person motion capture from multi-view video and imus. _International Journal of Computer Vision_, 128:1594-1611, 2020.\n' +
      '* [53] Jennifer L McGinley, Richard Baker, Rory Wolfe, and Meg E Morris. The reliability of three-dimensional kinematic gait measurements: a systematic review. _Gait & posture_, 29(3):360-369, 2009.\n' +
      '* [54] Vladimir Medved. _Measurement and Analysis of Human Locomotion_. Springer, 2021.\n' +
      '* [55] Brice Michoud, Erwan Guillou, Hector Briceno, and Saida Bouakaz. Real-time marker-free motion capture from multiple cameras. In _2007 IEEE 11th International Conference on Computer Vision_, pages 1-7. IEEE, 2007.\n' +
      '* [56] Emily Miller, Kenton Kaufman, Trevor Kingsbury, Erik Wolf, Jason Wilken, and Marilynn Wyatt. Mechanical testing for three-dimensional motion analysis reliability. _Gait & posture_, 50:116-119, 2016.\n' +
      '* [57] Chaithanya Kumar Mummadi, Frederic Philips Peter Leo, Keshav Deep Verma, Shivaji Kasireddy, Philipp M Scholl, Jochen Kempfle, and Kristof Van Laerhoven. Real-time and embedded detection of hand gestures with an imu-based glove. In _Informatics_, page 28. MDPI, 2018.\n' +
      '* [58] Noitom. Noitom Motion Capture Systems. [https://www.noitom.com/](https://www.noitom.com/), 2015.\n' +
      '*[59]*[59] 카일 올세우스키, 조셉 J 림, 션수케 사이토, 하오 리. vr hmds를 위한 고 충실도 얼굴 및 음성 애니메이션. _ ACM Transactions on Graphics (TOG)_, 35(6):1-14, 2016.\n' +
      '* [60] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter. A 3d face model for pose and illumination invariant face recognition. In _2009 sixth IEEE international conference on advanced video and signal based surveillance_, pages 296-301. Ieee, 2009.\n' +
      '* [61] Gerard Pons-Moll, Andreas Baak, Thomas Helten, Meinard Muller, Hans-Peter Seidel, and Bodo Rosenhahn. Multisensor-fusion for 3d full-body human motion capture. In _2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pages 663-670. IEEE, 2010.\n' +
      '* [62] Gerard Pons-Moll, Andreas Baak, Juergen Gall, Laura Leal-Taixe, Meinard Mueller, Hans-Peter Seidel, and Bodo Rosenhahn. Outdoor human motion capture using inverse kinematics and von mises-fisher sampling. In _2011 International Conference on Computer Vision_, pages 1243-1250. IEEE, 2011.\n' +
      '* [63] Ammar Qammaz and Antonis A Argyros. A unified approach for occlusion tolerant 3d facial pose capture and gaze estimation using mocapnets. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3178-3188, 2023.\n' +
      '* [64] QST Inc. QST Corporation Limited. [https://www.qstcorp.com/](https://www.qstcorp.com/), 2012.\n' +
      '* [65] Clement Reveredy, Sylvie Gibet, and Caroline Larboulette. Optimal marker set for motion capture of dynamical facial expressions. In _Proceedings of the 8th ACM SIGGRAPH Conference on Motion in Games_, pages 31-36, 2015.\n' +
      '* [66] Qaiser Riaz, Guanhong Tao, Bjorn Kruger, and Andreas Weber. Motion reconstruction using very few accelerometers and ground contacts. _Graphical Models_, 79:23-38, 2015.\n' +
      '* [67] Daniel Roetenberg, Henk J Luinge, Chris TM Baten, and Peter H Veltink. Compensation of magnetic disturbances improves inertial and magnetic sensing of human body segment orientation. _IEEE Transactions on neural systems and rehabilitation engineering_, 13(3):395-405, 2005.\n' +
      '* [68] Javier Romero, Dimitrios Tzionas, and Michael J Black. Embodied hands. _ACM Transactions on Graphics_, 36(6):1-17, 2017.\n' +
      '* [69] Martin Schepers, Matteo Giuberti, Giovanni Bellusci, et al. Xsens mvn: Consistent tracking of human motion using inertial sensing. _Xsens Technol_, 1(8):1-8, 2018.\n' +
      '* [70] Soshi Shimada, Vladislav Golyanik, Patrick Perez, and Christian Theobalt. Decaf: Monocular deformation capture for face and hand interactions, 2023.\n' +
      '* [71] Ronit Slyper and Jessica K Hodgins. Action capture with accelerometers. In _Proceedings of the 2008 ACM SIGGRAPH/Eurographics symposium on computer animation_, pages 193-199, 2008.\n' +
      '* [72] William AP Smith, Alassane Seck, Hannah Dee, Bernard Tiddeman, Joshua B Tenenbaum, and Bernhard Egger. A morphable face albedo model. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5011-5020, 2020.\n' +
      '* [73] SONY. Mobile Motion Capture "mocopi". [https://www.sony.net/Products/mocopi-dev/en/](https://www.sony.net/Products/mocopi-dev/en/), 2023.\n' +
      '* [74] Jochen Tautges, Arno Zinke, Bjorn Kruger, Jan Baumann, Andreas Weber, Thomas Helten, Meinard Muller, Hans-Peter Seidel, and Bernd Eberhardt. Motion reconstruction using sparse accelerometer data. _ACM Transactions on Graphics (ToG)_, 30(3):1-12, 2011.\n' +
      '* [75] Anh Tuan Tran, Tal Hassner, Iacopo Masi, and Gerard Medioni. Regressing robust and discriminative 3d morphable models with a very deep neural network. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5163-5172, 2017.\n' +
      '* [76] Zarins Uldis. _Anatomy of Facial Expressions_. Anatomy Next, Inc., 2017.\n' +
      '* [77] Unity Technologies. Unity. [https://unity.com/](https://unity.com/), 2023. Version 2023.1.\n' +
      '* [78] UnrealEngine. Live link face. [https://apps.apple.com/us/app/live-link-face/id1495370836](https://apps.apple.com/us/app/live-link-face/id1495370836), 2023.\n' +
      '* [79] Rachel V Vitali, Ryan S McGinnis, and Noel C Perkins. Robust error-state kalman filter for estimating imu orientation. _IEEE Sensors Journal_, 21(3):3561-3569, 2020.\n' +
      '* [80] Daniel Vlasic, Rolf Adelsberger, Giovanni Vannucci, John Barnwell, Markus Gross, Wojciech Matusik, and Jovan Popovic. Practical motion capture in everyday surroundings. _ACM transactions on graphics (TOG)_, 26(3):35-es, 2007.\n' +
      '* [81] Daniel Vlasic, Ilya Baran, Wojciech Matusik, and Jovan Popovic. Articulated mesh animation from multi-view silhouettes. In _Acm Siggraph 2008 papers_, pages 1-9. 2008.\n' +
      '* [82] Timo Von Marcard, Gerard Pons-Moll, and Bodo Rosenhahn. Human pose estimation from video and imus. _IEEE transactions on pattern analysis and machine intelligence_, 38(8):1533-1547, 2016.\n' +
      '* [83] Timo Von Marcard, Bodo Rosenhahn, Michael J Black, and Gerard Pons-Moll. Sparse inertial poser: Automatic 3d human pose estimation from sparse imus. In _Computer graphics forum_, pages 349-360. Wiley Online Library, 2017.\n' +
      '* [84] Timo Von Marcard, Roberto Henschel, Michael J Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and a moving camera. In _Proceedings of the European conference on computer vision (ECCV)_, pages 601-617, 2018.\n' +
      '* [85] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. Mead: A large-scale audio-visual dataset for emotional talking-face generation. In _European Conference on Computer Vision_, pages 700-717. Springer, 2020.\n' +
      '* [86] Thibaut Weise, Sofen Bouaziz, Hao Li, and Mark Pauly. Realtime performance-based facial animation. _ACM transactions on graphics (TOG)_, 30(4):1-10, 2011.\n' +
      '\n' +
      '* [87] Alexander Winkler, Jungdam Won, and Yuting Ye. Quest-sim: Human motion tracking from sparse sensors with simulated avatars. In _SIGGRAPH Asia 2022 Conference Papers_, pages 1-8, 2022.\n' +
      '* [88] XSENS. Xsens Technologies B.V. [https://www.xsens.com/](https://www.xsens.com/), 2011.\n' +
      '* [89] Dongseok Yang, Doyeon Kim, and Sung-Hee Lee. Lobstr: Real-time lower-body pose prediction from sparse upper-body tracking signals. In _Computer Graphics Forum_, pages 265-275. Wiley Online Library, 2021.\n' +
      '* [90] Xinyu Yi, Yuxiao Zhou, and Feng Xu. Transpose: Real-time 3d human translation and pose estimation with six inertial sensors. _ACM Transactions on Graphics (TOG)_, 40(4):1-13, 2021.\n' +
      '* [91] Jae Shin Yoon, Takaaki Shiratori, Shoou-I Yu, and Hyun Soo Park. Self-supervised adaptation of high-fidelity face models for monocular performance tracking. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4601-4609, 2019.\n' +
      '* [92] Jun Yu and Zengfu Wang. A video-based facial motion tracking and expression recognition system. _Multimedia Tools and Applications_, 76:14653-14672, 2017.\n' +
      '* [93] Qilong Yuan and I-Ming Chen. Localization and velocity tracking of human via 3 imu sensors. _Sensors and Actuators A: Physical_, 212:25-33, 2014.\n' +
      '* [94] Li Zhang, Noah Snavely, Brian Curless, and Steven M Seitz. Spacetime faces: high resolution capture for modeling and animation. In _ACM SIGGRAPH 2004 Papers_, pages 548-558. 2004.\n' +
      '* [95] Longwen Zhang, Chuxiao Zeng, Qixuan Zhang, Hongyang Lin, Ruixiang Cao, Wei Yang, Lan Xu, and Jingyi Yu. Video-driven neural physically-based facial asset for production, 2022.\n' +
      '* [96] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and Jingyi Yu. Dreamface: Progressive generation of animatable 3d faces under text guidance, 2023.\n' +
      '* [97] Zhengyou Zhang. Microsoft kinect sensor and its effect. _IEEE multimedia_, 19(2):4-10, 2012.\n' +
      '* [98] Zhe Zhang, Chunyu Wang, Wenhu Qin, and Wenjun Zeng. Fusing wearable imus with multi-view images for human pose estimation: A geometric approach. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2200-2209, 2020.\n' +
      '* [99] Zerong Zheng, Tao Yu, Hao Li, Kaiwen Guo, Qionghai Dai, Lu Fang, and Yebin Liu. Hybridfusion: Real-time performance capture using a single depth sensor and sparse imus. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 384-400, 2018.\n' +
      '* [100] Yi Zhou, Wei Zhang, Xiaoou Tang, and Harry Shum. A bayesian mixture model for multi-view face alignment. In _2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\'05)_, pages 741-746. IEEE, 2005.\n' +
      '* [101] Xiangyu Zhu, Xiaoming Liu, Zhen Lei, and Stan Z Li. Face alignment in full pose range: A 3d total solution. _IEEE transactions on pattern analysis and machine intelligence_, 41(1):78-92, 2017.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>