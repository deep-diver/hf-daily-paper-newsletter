<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# IMUSIC: IMU-based Facial Expression Capture\n' +
      '\n' +
      'Youjia Wang\\({}^{1,2*}\\)\n' +
      '\n' +
      'Yiwen Wu\\({}^{1,2*}\\)\n' +
      '\n' +
      'Ruiqian Li\\({}^{1}\\)\n' +
      '\n' +
      'Hengan Zhou\\({}^{1,2}\\)\n' +
      '\n' +
      'Hongyang Lin\\({}^{1,3}\\)\n' +
      '\n' +
      'Yingwenqi Jiang\\({}^{1}\\)\n' +
      '\n' +
      'Yingsheng Zhu\\({}^{1}\\)\n' +
      '\n' +
      'Guanpeng Long\\({}^{1,4}\\)\n' +
      '\n' +
      'Jingya Wang\\({}^{1}\\)\n' +
      '\n' +
      'Lan Xu\\({}^{1}\\)\n' +
      '\n' +
      'Jingyi Yu\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)ShanghaiTech University \\({}^{2}\\)LumiAni Technology \\({}^{3}\\)Deemos Technology \\({}^{4}\\)ElanTech Co., Ltd. {wangyj2, wuyw2023, lirq1, zhouha24, linhy, jiangywq, zhuysh, longgp2022, wangjingya, xulanl, yujingyi}@shanghaitech.edu.cn\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'For facial motion capture and analysis, the dominated solutions are generally based on visual cues, which cannot protect privacy and are vulnerable to occlusions. Inertial measurement units (IMUs) serve as potential rescues yet are mainly adopted for full-body motion capture. In this paper, we propose IMUSIC to fill the gap, a novel path for facial expression capture using purely IMU signals, significantly distant from previous visual solutions. The key design in our IMUSIC is a trilogy. We first design micro-IMUs to suit facial capture, companion with an anatomy-driven IMU placement scheme. Then, we contribute a novel IMU-ARKit dataset, which provides rich paired IMU/visual signals for diverse facial expressions and performances. Such unique multi-modality brings huge potential for future directions like IMU-based facial behavior analysis. Moreover, utilizing IMU-ARKit, we introduce a strong baseline approach to accurately predict facial blendshape parameters from purely IMU signals. Specifically, we tailor a Transformer diffusion model with a two-stage training strategy for this novel tracking task. The IMUSIC framework empowers us to perform accurate facial capture in scenarios where visual methods falter and simultaneously safeguard user privacy. We conduct extensive experiments about both the IMU configuration and technical components to validate the effectiveness of our IMUSIC approach. Notably, IMU-SIC enables various potential and novel applications, i.e., privacy-protecting facial capture, hybrid capture against occlusions, or detecting minute facial movements that are often invisible through visual cues. We will release our dataset and implementations to enrich more possibilities of facial capture and analysis in our community._\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Facial expressions, as subtle as minute muscle movements, play a key role in effectively conveying a wide range of emotions. These nonverbal cues have particular importance in human interaction, as they provide emotional depth beyond spoken words. Anatomically, expressive facial movements emerge from the intricate coordination of facial muscles, activated by the brain through the facial and trigeminal nerves. For a considerable time, capturing and analyzing facial expressions have predominantly depended on visual cues. Computer vision techniques such as 3DDFA [31, 101] offer a rapid means of acquiring facial geometry and expression from a single image, whereas DECA [27] manages to recover even finer facial details. Apple\'s ARKit[5] represents a significant leap in real-time facial geometry acquisition, enabling users to create animated avatars that mimic their expressions. However, approaches based on visual signals also pose challenges: they do not protect privacy and are vulnerable to occlusions or side facial poses. Further, they fail to capture nuanced facial motions that are difficult to track visually. In this paper, we explore a novel approach for facial movement capture by leveraging the latest advances in inertial measurement units (IMUs) that can work independently or in conjunction with visual capture.\n' +
      '\n' +
      'IMUs, initially used in mobile devices, have been progressively embraced in whole-body motion capture. In comparison to conventional vision-based systems, IMUs boast portability and minimal spatial prerequisites. Typically attached to various body joints, IMUs can capture essential acceleration and axis angle data that can further translate to body motion. For example, pioneering work by Loper et al. [48] adapted this axis angle data for integration with the SMPL human body model, thereby enabling full-body motion capture. Most recently, Yi et al. [90] achieved comprehensive body motion capture using as few as six IMUs, leveraging the stability and generative capabilities of Transformer Diffusion [43]. In contrast, there is almost no use of IMU for facial motion capture. The reason is that the challenges are multi-fold. Hardware-wise, integrating IMUs with additional sensors (like inertial and geomagnetic detectors) and communication modules (such as Bluetooth and Wi-Fi) results in a size prohibitive large on faces. Algorithm-wise, IMUs generally offer sparser data with lower signal-to-noise ratios compared to visual systems. Furthermore, unlike body motion capture where spatial positions correlate to bone rotations, facial expressions are predominantly muscle-driven, posing a unique challenge in translating IMU data.\n' +
      '\n' +
      'In this paper, we present a novel IMU-based facial movement capture solution, or IMUSIC (see Fig. 1 for detail). IMUSIC utilizes IMUs specially designed for facial applications, emphasizing miniaturization. Specifically, by separating the detection and data modules of the IMU, we ensure that the device attached to the face is compact and lightweight. This design avoids interfering with natural facial movements and allows reliable data transmission and synchronization. IMUSIC additionally adopts an anatomy-driven strategy for placing IMUs in correspondence with specific muscles that control facial expressions. In addition, IMUSIC attaches an auxiliary IMU to the temporal bone behind the ears, a location immune to expression-induced movement and unaffected by neck rotation, to cancel out global head motion. We contribute the first facial IMU dataset that consists of both IMU and visual signals, benefiting the CG community. By covering the IMUs with skin-toned tapes, we managed to capture corresponding vision signals, e.g., by using ARKit. The resulting IMU-ARKit dataset records both signals from participants engaged in various activities, including speaking different languages, making facial expressions while interacting with another participant, speaking with emotional intonation, etc.\n' +
      '\n' +
      'Next, we use the IMU-ARKit dataset to train a neural network to infer facial expressions solely from IMU signals. Previous visual-based techniques require datasets of an ultra-large number of participants to account for discrepancies in age, gender, ethnicity, etc. Limited by the difficulty of collection, our dataset still lacks diversity. We hence augment it by synthesizing virtual IMU signals from these visual datasets. We discuss in depth the discrepancies between the actual captured vs. synthesized IMU signals and set out to train a Transformer Diffusion based neural network [43] to bridge the gap between the two as well as to reliably infer Blendshape parameters directly from the IMU signals. Comprehensive experiments show that IMUSIC performs comparable to ARKit on front-facing participants under well-lit conditions. More importantly, it is much more robust in handling challenging scenarios like side-facing participants, faces under occlusions, or poor lighting conditions, which are commonly problematic to ARKit and other visual-based techniques.\n' +
      '\n' +
      'In addition to serving as an alternative facial motion capture system, IMUSIC enables various novel applications. In an era where digital privacy is a paramount concern, IMU-SIC offers a novel method to capturing facial expressions without visual input, thereby safeguarding personal visual data. This functionality is particularly crucial in the domain of Virtual YouTube (VTubers), where content creators seek to animate avatars while concealing their true identities. Utilizing IMUSIC, VTubers are empowered to anonymously manipulate the expressions of their digital personas. Secondly, we introduce a hybrid capture system to supplement traditional facial capture methods in scenarios where visual signals are compromised. An exemplary case is during song recordings, where the performer\'s facial expressions might be concealed by microphones or similar studio apparatus. In such instances, the mouth and cheeks of artists are obscured, resulting in distorted expressions captured by visual tools like ARKit. In contrast, IMUSIC can seamlessly capture these occluded facial movements and ensure accurate syncing of audio recordings with facial animations for invaluable virtual music production. Moreover, IMUSIC demonstrates exceptional capability in detecting subtle facial movements and capturing expressions in areas less amenable to vision feature analysis. For instance, subtle cheek movements caused by puffing, which often occur invariant in vision signals, can be precisely captured by IMUSIC through the orientation data of the IMU. Other micro-expressions like the slight twitching of mouth corners, which may go unnoticed visually, can also be reliably captured by IMUSIC, providing insights for detecting an individual\'s emotion changes.\n' +
      '\n' +
      '## 2 Related Works\n' +
      '\n' +
      'Our attempt to deploy IMUs for facial motion capture benefits from the evolutionary trajectory of human motion capture technology as Fig. 2. Tracing this evolution, beginning in the 1990s, marker-based optical motion capture technology emerged as the mainstream methodology in the field [10, 28, 32]. In this approach, actors are required to wear reflective markers on their bodies for motion tracking. Since the 2000s, groundbreaking developments [18, 22, 55, 81] have revolutionized motion capture technology. This advancement has made it possible to capture human motion without the need for actors to wear any special markers [44, 48, 68]. However, for 3D skeletal reconstruction, technologies typically require fixed-position devices like Zhang [97] or multi-camera setups as in [55]. This substantially constrains the range of motion and space available to the actors during capture. To address this limitation, Vlasic et al. [80] pioneered the use of IMUs in human motion capture, offering a more flexible and less restrictive method. Subsequently, this field witnessed rapid advancements, leading to the development of more stable systems [21, 38, 84], and the introduction of sparser configurations [83, 93]. In the commercial arena, companies like Sony [73] and Xsens [88] have launched consumer-grade motion capture sensors, offering solutions with reduced spatial requirements and expanded operational ranges to accommodate a wider consumer base.\n' +
      '\n' +
      'Facial MocapFace shape representation traditionally relies on a sparse collection of 2D facial landmark points. Some pioneering works [17, 19, 20, 100] have adeptly utilized 2D facial landmarks for facial capture. With the growing demand for 3D visual effects, there has been an increasing focus on 3D face capture and reconstruction.\n' +
      '\n' +
      'In the realm of facial expression capture, a pivotal advancement was introduced by Blanz and Vetter [11] who proposed a novel approach for general face representation using linear combinations. This methodology marked a significant turning point in the field, offering a structured and nuanced way to capture facial expressions. In the film and\n' +
      '\n' +
      'Figure 2: Evolution of body and facial motion capture. This sequence illustrates the progression from marker-based, to marker-free capturing, culminating in IMU-based capture. IMUs have already been applied to full-body motion capture, and we propose IMUSIC to fill the void in IMU-based facial capture (bottom right).\n' +
      '\n' +
      'visual effects industry, several studies [9, 12, 53, 54, 56, 65, 94] have utilized facial markers, visually tracking and computing their movement to predict specific facial expressions and motions.\n' +
      '\n' +
      'The captured motions are then manually refined in post-production to achieve realistic facial animations in visual effects. However, for consumer-level users, these methods are prohibitively expensive, heavily reliant on post-production manual adjustments, inconvenient, and limited by location constraints. Several works [7, 14, 26, 72, 86] have addressed this issue through a marker-free approach that predicts facial motions from a single RGB image, effectively reducing costs. Subsequently, numerous challenges in this field have been resolved following this methodology. Research by Cao et al. [13], Paysan et al. [60] has enhanced the process of expression motion capture by employing various strategies to eliminate video jitter and improve robustness.\n' +
      '\n' +
      'With the enhancement of image processing capabilities by deep learning algorithms, Batista et al. [8], Laine et al. [41], Olszewski et al. [59] have improved facial motion capture from various dimensions.Regression based methods with CNN network [31, 33, 75] achieve robust results from a single image, while video-based facial motion capture techniques [34, 45, 49, 92] have become increasingly sophisticated. Lombardi et al. [47] proposes a deep appearance model to handle complex geometry and texture in facial animation. Yoon et al. [91] uses a united model to process images in the wild and Cao et al. [16] uses a universal avatar prior (UAP) to train a high resolution avatar with a mobile phone. In another way, Zhang et al. [95] proposes high-fidelity performer-specific facial capture method from 4D scan.\n' +
      '\n' +
      'However, visually based methods [15, 86] are prone to interference from occlusions. Referring to the works of Qammaz and Argyros [63], they have partially alleviated the influence of occlusion by ensuring that a significant portion of the area predicts information from obscured regions. In recent years, facial capture based on wearable devices has shown promising results. These methods are less susceptible to interference and are notably portable. However, all the visually based methods mentioned above face the risk of compromising user privacy due to prolonged exposure to cameras.\n' +
      '\n' +
      'Sensor-based MocapAs manufacturing technology progresses and research in the field deepens, IMUs are benefiting from consistent and precise improvements. Research focused on the fundamental performance of IMUs calibration [6, 23, 29, 67] and correction [2, 46, 79, 80] have enabled stable data detection and effectively addressed their significant drift issues. Solving these problems improves the stability and precision of IMUs, making them a popular choice for many research teams in body mocap applications. Most related tasks predominantly utilize IMU signals as part of a multimodal system for body mocap. IMU capture in collaboration with video is one of the most extensively researched multimodal methods. The initial work of Malleson et al. [51], Pons-Moll et al. [61, 62], Von Marcard et al. [82] laid the groundwork for the development of video-collaborated IMU systems, and ultimately, in recent years, this technol\n' +
      '\n' +
      'Figure 3: Overview of IMUSIC. We first introduce the hardware design and the data acquisition pipeline. Subsequently, we delve into the data calibration process and the methodology for facial motion recovery utilizing IMU signals. Following the deployment of IMUSIC, we demonstrate its effectiveness through various applications, underlining its precision and portability.\n' +
      '\n' +
      'ogy has achieved a level of perfection[30, 37, 52, 98]. Depth camera[36, 99], Optical markers [4] and physics constraints [3, 25, 40, 87, 89] are also common-used collaboration for IMU mocap. There are also efforts focused on reconstructing body motion using only 17 distinct IMU signals [58, 69]. As research on IMU signals deepens, the Sparse IMU [39, 66, 71, 74] concept emerged. Subsequently, the SIP [83] initiative successfully reduced the number of IMUs to six, while the DIP [39] project accomplished real-time mocap, culminating in the TransPose [90] work which refined these advancements. Additionally, employing IMUs for gait analysis [35] is also one of the more popular applications. Compared to vision-based methods, these approaches offer significant advantages, including a wider range of motion, freedom from obstructions and lighting constraints, benefits that are equally applicable to facial motion capture.\n' +
      '\n' +
      'However, adapting this method for facial application is not straightforward. Initially, while commercial IMUs like noiton [58], Xsens [88], and mocopi [73] have achieved significant integration, they are predominantly designed for body mocap and are disproportionately large and heavy for facial capture. With the advancement of hardware, there are now attempts to use IMUs for capturing movements of smaller limbs. Makaussov et al. [50], Mummadi et al. [57] customized IMU gloves for hand posture estimation. This breakthrough shed light on the potential of IMUs in reconstructing smaller body parts. While capturing hand posture primarily utilizes IMUs to detect skeletal movements, relying chiefly on rotational angles for effective hand motion detection, facial capture does not share this characteristic. Furthermore, unlike the relatively fixed joints in hand movements, most facial regions are capable of multi-degree movements informed by anatomical studies [76]. In our study, we referenced anatomical structures to maximize the use of IMUs in regions with extensive muscle motion. However, even with this strategy, directly modeling these movements through mere rotational data proves to be complex. Aha [1] employing lazy learning techniques to conduct human mocap with just accelerometer signals, has significantly influenced our approach. Despite the limited performance of this method, it revealed the importance of acceleration signals in predicting movements, providing valuable information that rotation alone cannot offer, such as the effects of force. This revelation has guided us to give equal importance to both pose and acceleration in the processing of IMU data.\n' +
      '\n' +
      '## 3 Facial IMU\n' +
      '\n' +
      'Fig. 3 provides an overview of IMUSIC. The left section outlines our data collection process. Participants are requested to wear multiple IMUs and perform a range of facial expressions and motions to capture corresponding signals. We designed these compact IMUs to attach to the face simultaneously without impeding facial movements. In the middle section, we illustrate our network\'s data processing workflow. The collected IMU data is first aligned to a unified coordinate system using calibration. Then we establish an IMU2Face model that reconstructs facial actions based on the aligned IMU signals. The right section highlights the potential applications of IMUSIC, offering solutions to address some limitations of visual-based facial capture.\n' +
      '\n' +
      'Within the field of motion capture, IMU plays a critical role in reflecting the spatial movements of an object by measuring its orientation and acceleration. IMUs designed for full-body motion capture, such as Xsens, Sony Mocopi and others, have been widely applied commercially. These units usually consist of various parts, including detecting sensors and data transmission modules, making them too hulking to be used for facial motion capture. Furthermore, employing multiple units of this model for facial capture can lead to severe occlusion, preventing observation of the participant\'s facial expressions. This necessitates the development of a custom-designed IMU, specifically tailored to meet the unique requirements and scale of facial motion capture.\n' +
      '\n' +
      'Our design preserves the function of standard IMU while minimizing weight and size to cater to the requirements for facial capture. Fig. 4 (top) compares the size of our IMU. We achieved significant miniaturization by separating the sensor module from the data transmission module. This design allowed our sensor module to be exceptionally compact, measuring only 12*10 mm and weighing merely 0.4 grams, a stark reduction to a quarter of the Xsens module\'s area and only 15% of its weight.\n' +
      '\n' +
      '### Facial IMU Sensor\n' +
      '\n' +
      'Fig. 4(down) provides a detailed overview of the specific hardware components utilized in our study. The sensor module incorporates a total of nine-axis sensing sub-units, which include the QMC5883P [64] from Silicon Power, a three-axis magnetic field sensor with a measurement range of \\(\\pm 30\\) gauss, and the QMI8658 [64] integrated chip, which combines a three-axis gyroscope and accelerometer. These sensors are capable of accurately recording spatial positions and accelerations at a rate of 60fps. The data transmission module is primarily based on the ESP32 controller. It employs the UDP protocol to collect and correct data detected by the sensor module. Additionally, we use a Wi-Fi module to transmit the computed data to the host computer. This data includes time stamps, quaternion representations, and acceleration values at each recorded instance.\n' +
      '\n' +
      'The data transmission module of our Face IMU sensor system requires only a 5V battery supply. This setup provides the essential conditions for the portability and wearability of the Face IMU sensor system. Furthermore, as the connection to the host computer is via Wi-Fi, users can move freely within the Wi-Fi signal range while wearing the Face IMU, enabling high degrees of mobility.\n' +
      '\n' +
      'Fig. 5 exemplifies the remarkable sensitivity of our IMU in capturing even the most minute motion signals. In this demonstration, a participant wearing an IMU detector on the eyebrow performs subtle blink movements. In the waveform diagram shown in (d), we can clearly see a complete motion sequence: starting with the initial acceleration as the eyebrow begins its ascent, followed by a deceleration phase as the movement slows, and culminating in a stationary phase when the eyebrow ceases to move. The graph distinctly demarcates these stages, underscoring the precision with which our IMU detects even the most delicate movements. Our supplementary video provides a more dynamic visualization of the motion.\n' +
      '\n' +
      '### Synchronization and Calibration\n' +
      '\n' +
      'We delved deeply into the essential technology for capturing facial information in synchrony using multiple IMUs. To achieve this, it is imperative to address two fundamental challenges: synchronization and calibration. We designated one ESP32 as the auxiliary ESP32, employing it as a benchmark for synchronizing and calibrating the others. To ensure coherent and coordinated data collection from these diverse sensors, we integrated a calibration program into this ESP32 within the data transfer module during hardware design. Before data acquisition by the IMUs, we used the data module of the auxiliary ESP32\'s clock as a reference point. We transmitted pulse signals through the DuPont line to each IMU\'s ESP32 for calibration purposes. Upon receiving this pulse signal, each ESP32 aligns its internal clock with the external reference, synchronizing the timestamps across all IMUs. With these synchronized signals, we obtain the raw signals \\({}^{\\dagger}\\!\\mathcal{S}=\\{^{\\dagger}\\!\\mathcal{R},^{\\dagger}\\!\\mathcal{A}\\}\\) from all the \\(n\\) IMUs, where \\({}^{\\dagger}\\!\\mathcal{R}=\\{^{\\dagger}\\!\\mathcal{R}^{0},^{\\dagger}\\!\\mathcal{R }^{1},\\cdots,^{\\dagger}\\!\\mathcal{R}^{n}\\}\\) represent the rotation matrix sequence of the number \\(i\\) IMU, and \\({}^{\\dagger}\\!\\mathcal{A}=\\{^{\\dagger}\\!\\mathcal{A}^{0},^{\\dagger}\\!\\mathcal{A }^{1},\\cdots,^{\\dagger}\\!\\mathcal{A}^{n}\\}\\) represent the acceleration sequence of the number \\(i\\) IMU. The rotation matrix represents the rotation from the IMU\'s local coordinate system to the world coordinate system. Notably, we denote the index of the auxiliary IMU as 0. For ease of subsequent analyses, we initially convert the acceleration from the local coordinate system to the world coordinate system. For the IMU \\(i\\)\'s data on the frame \\(j\\), we denote the raw acceleration and orientations as \\({}^{\\dagger}\\!a^{i}_{j}\\) and \\({}^{\\dagger}\\!R^{i}_{j}\\). The world coordinate acceleration can be represented as:\n' +
      '\n' +
      '\\[a^{i}_{j}=(^{\\dagger}\\!R^{i}_{j})^{-1}\\!{}^{\\dagger}\\!a^{i}_{j}. \\tag{1}\\]\n' +
      '\n' +
      'Next, acknowledging the variability in facial structures and the potential for slight discrepancies in IMU placement each time, we adopted the concept of a Neutral facial performance, similar to the approach used by [26, 90] in body mocap. After wearing the IMUs for the participants, we had each participant relax the facial muscles, presenting a Neutral state, and recorded the orientation of each IMU \\(\\{R_{\\text{neutral}}\\}\\). In subsequent calculations, we used the orientation relative to this pose as a baseline:\n' +
      '\n' +
      '\\[{}^{*}\\!R^{i}_{j}=(R^{i}_{\\text{neutral}})^{-1}\\!R^{i}_{j}. \\tag{2}\\]\n' +
      '\n' +
      'Considering that two types of movements influence an IMU\'s signal when placed on the face -- the overall movement of the head and the movements caused by facial expressions. Our focus is on deducing expressions from IMU signals. As illustrated in Fig. 5 (b), not only do we deploy IMUs across the facial region, but we also strategically place an auxiliary IMU behind the ear, as shown in (c). This placement is specifically designed to mitigate the impact of general head movements on the orientations and accelerations detected by the other IMUs. For convenience, we define the index of the auxiliary IMU to be \\(0\\). The calibrated IMU rotation can be expressed as\n' +
      '\n' +
      '\\[R^{i}_{j}=\\left\\{\\begin{array}{ll}(^{*}\\!R^{i}_{0})^{-1}\\!{}^{*}\\!R^{i}_{j}& \\text{if }j\\neq 0,\\\\ {}^{*}\\!R^{i}_{0}&\\text{otherwise}\\end{array}\\right. \\tag{3}\\]\n' +
      '\n' +
      'We denote the calibrated IMU signal as \\(\\mathcal{S}=\\{\\mathcal{R},\\mathcal{A}\\}\\).\n' +
      '\n' +
      'Figure 4: Size and architecture design of our IMU. Top: size comparison. Our IMU is small enough to interfere with natural facial expressions. Bottom: architecture design. Our IMU has two main components: the face unit and the primary unit. The face unit contains an Accelerometer, Gyroscope and Magnetometer. It’s attached to the face to measure the acceleration and orientation signal. Data collected by the face unit is transmitted to the primary unit by wire. The primary unit powers the face unit, corrects data, and forwards signals to the host computer via Wi-Fi.\n' +
      '\n' +
      '## 4 IMU-based Facial Motion Capture\n' +
      '\n' +
      '### IMU Placement and Anatomic Guidance\n' +
      '\n' +
      'To accurately capture facial movements, it is imperative to attach IMUs to distinct regions on the surface of the face. In the left panel of Fig. 5, we present our strategic arrangement of IMUs. This layout is informed by a detailed analysis of the distribution of facial muscles[76]. We demarcated distinct facial zones, referencing key anatomical landmarks including: the zygomaticus area, crucial for expressing emotions like joy or sorrow; the buccinator and mentalis area, fundamental for movements pertinent to speech; the orbicularis oculi area, important for a spectrum of expressions such as smiling and frowning; and the frontalis area, integral for conveying sentiments such as dissatisfaction or melancholy. In every designated region, we meticulously placed at least one IMU to ensure comprehensive monitoring of the key muscle groups and facial zones. Additionally, in regions characterized by dense muscle presence or complex movements, we opted for dual IMU placement. This approach is exceptionally effective in capturing a complete range of muscle movements in these specific areas.\n' +
      '\n' +
      'Furthermore, our design strategy was acutely focused on minimizing the IMUs\' impact on both the natural facial movements and the comfort of our participants. Acknowledging the sensitivity of certain facial regions, we intentionally avoided placing IMUs on the eyelids and the corners of the mouth, as these areas are not only crucial for a wide range of expressions but are also prone to discomfort if constrained. To ensure the IMUs were discreet, we utilized flesh-colored tape for attachment, allowing them to blend seamlessly with the skin while minimizing visual distraction. The wiring was also thoughtfully routed along the periphery of the face to avoid interfering with the participant\'s expressions and to preserve the clarity of any accompanying visual data. These measures enabled us to maintain the fidelity of the IMU data and any visual records while respecting the comfort and expressiveness of the participants.\n' +
      '\n' +
      '### Capturing IMU-ARKit Dataset\n' +
      '\n' +
      'Blendshape technology is widely used in the realm of facial animation and motion capture due to its ability to generate highly realistic and nuanced expressions. This technology operates on the principle of parametric modeling. Specifically, a blendshape model is defined by a collection of blendshape weights, denoted as \\(\\mathcal{W}=\\{w_{1},w_{2},\\cdots,w_{m}\\}\\), a blendshape model can be represented as:\n' +
      '\n' +
      '\\[M(\\mathcal{W})=B_{0}+\\sum_{k}^{m}w_{k}B_{k}. \\tag{4}\\]\n' +
      '\n' +
      'where \\(B_{0}\\) represents the neutral face, \\(B_{k}\\) is the blendshape basis vector, and \\(m\\) is the number of blendshapes. By linearly interpolating between different blend shapes, this approach allows for the creation of multiple facial expressions.\n' +
      '\n' +
      'Our challenge is to derive these blend shape parameters \\(\\mathcal{W}\\) from the IMU data \\(\\mathcal{S}\\). Given that the IMU is capable of capturing motion and orientation, we propose a method for mapping these physical measurements to blend shape coefficients. This requires the development of an algorithm that converts IMU readings into meaningful hybrid shape parameters.\n' +
      '\n' +
      'In order to realize a data-driven solution for predicting\n' +
      '\n' +
      'Figure 5: Data acquisition pipeline of IMUSIC. Initially, we position IMUs at specific facial locations to capture IMU signals corresponding to various fundamental facial movements. Simultaneously, ARKit parameters are recorded. We design a simulator for data augmentation module in parallel.\n' +
      '\n' +
      'facial blendshape weights using IMU, we set out to create a facial IMU with the ARKit dataset, as demonstrated in the central panel of Fig. 5 This dataset was carefully compiled to contain paired data from IMU and ARKit to ensure a comprehensive base for model training.\n' +
      '\n' +
      'Our dataset contains records from 20 different participants. These individuals are all in the 18-40 age range, proficient in English, and have some background in acting, thus providing richly varied and vivid facial expressions. The left panel of Fig. 5(a) shows an example of the data collection setup. We installed a set of \\(11\\) IMU detectors for each participant and sat in the acquisition seat, with the telepromter screen placed directly in front of the participant, next to an iPad that captured the visual information. the iPad used LiveLinkface [78] to capture the visual information, and the captured visual signals were divided into two parts: the RGB video sequence and the ARKit Parameters.\n' +
      '\n' +
      'Before the formal data collection process began, participants were given time to acclimatize to the sensation of wearing the IMU to ensure natural and unrestricted facial movements. Participants then gently triple-clicked the IMU located on mentalis, which was used as a later stage to synchronize the IMU signals with the visual signals. Data collection for each participant was divided into three different sections, each preceded by a sample video for the participant to mimic. In the first section, participants read aloud the provided content in a calm tone, with a split between native language and English. This was done to capture the natural facial movements associated with the language. In the second session, participants were asked to read aloud the same content, but with a specific emotion consistent with the context of the script, thus adding emotional layers to the facial expressions. Finally, the third segment asked participants to sequentially make a series of facial expressions that were based on specific medical classifications, ensuring a full range of emotions and movements.\n' +
      '\n' +
      '### Facial IMU Simulator Augmentation\n' +
      '\n' +
      'Despite the thoroughness of our IMU data collection program, generating a sufficiently large dataset for training models remains a significant challenge. This is primarily due to the logistical constraints associated with amassing a vast quantity of IMU-ARKit IMU data. To address this issue, we drew inspiration from the work of [90], who adeptly used simulate human IMU signals to transform other types of motion capture (Mocap) data into IMU-Mocap data. Motivated by this innovative approach, we developed the Face-IMU Simulator, an advanced tool designed to animate facial meshes using ARKit data and accurately simulate the corresponding orientation and acceleration signals on the facial surface, as demonstrated in the right panel of Fig. 5.\n' +
      '\n' +
      'Our methodology for ARKit data collection incorporated two primary strategies. Firstly, we utilized the MEAD dataset [85], a rich source of facial expression data that provided a solid foundation for our simulations. However, recognizing that MEAD predominantly contains short sentences and lacks some clips focused on specific expressions, we adopted a second strategy. This involved recording additional expression sequences using LiveLinkFace [78], thereby enriching our dataset with approximately two hours of specialized clips. These additional sequences, comprising mainly short text readings and expressive movements, significantly broadened the scope and depth of our dataset.\n' +
      '\n' +
      'Leveraging the ARKit sequences, we computed a topologically consistent face model for each frame, adhering to Equation. 4 and utilizing the basis provided by the ICT Model [42].\n' +
      '\n' +
      'For each selected vertex \\(i\\), corresponding to the positions of the IMU-ARKit data\'s IMU \\(i\\), to simulate the IMU \\(v_{j}^{i}\\)\'s acceleration for each frame \\(j\\), we employed the following formula:\n' +
      '\n' +
      '\\[a_{j}^{i}=\\frac{1}{c\\tau}(v_{j-c}^{i}+v_{j+c}^{i}-2v_{j}^{i}), \\tag{5}\\]\n' +
      '\n' +
      'where \\(\\tau\\) is the time interval of two frames, and \\(c\\) is the constant that controls the degree of smoothing.\n' +
      '\n' +
      'To determine orientation, we focused on the facial region containing the selected vertex. We select a face that contains the vertex, denoted as \\(\\{v_{j}^{i},v_{j}^{i,\\text{sup}[1]},v_{j}^{i,\\text{sup}[2]}\\}\\). We identified two adjacent edges to this vertex, labeled as \\(e_{j}^{i,1}=(v_{j}^{i}-v_{j}^{i,\\text{sup}[1]})_{\\text{norm}}\\) and \\(e_{j}^{i,2}=(v_{j}^{i}-v_{j}^{i,\\text{sup}[2]})_{\\text{norm}}\\). The first vector from these edges was utilized as the x-axis for the IMU coordinate system. We calculated the rotation matrix of the IMU relative to the world coordinate system as\n' +
      '\n' +
      '\\[\\prescript{\\text{IR}}{i}{j}=\\left[\\mathbf{e}_{j}^{i,1},\\quad\\frac{\\mathbf{e}_{ j}^{i,1}\\times\\mathbf{e}_{j}^{i,2}}{\\|\\mathbf{e}_{j}^{i,1}\\times\\mathbf{e}_{j}^{i,2} \\|},\\quad\\frac{(\\mathbf{e}_{j}^{i,1}\\times\\mathbf{e}_{j}^{i,2})\\times\\mathbf{e }_{j}^{i,2}}{\\|(\\mathbf{e}_{j}^{i,1}\\times\\mathbf{e}_{j}^{i,2})\\times\\mathbf{e }_{j}^{i,2}\\|}\\right]. \\tag{6}\\]\n' +
      '\n' +
      'As with the IMU-ARKit data, we calibrate \\(\\prescript{\\text{IR}}{i}{j}\\) by Eq. 2 and Eq. 3 to obtain the data used for network training \\(\\mathbf{R}_{j}^{i}\\). This comprehensive approach to simulating both the acceleration and orientation of IMUs ensures that our dataset is not only extensive but also rich in the detailed, accurate representation of facial movements.\n' +
      '\n' +
      '### IMU Based Facial Tracker\n' +
      '\n' +
      'In this study, a novel data flow was established, as depicted in Fig. 6. Initially, a calibration process was applied to both the simulate and the raw IMU-ARKit data. This procedure was instrumental in generating paired data sets of IMU-ARKit parameters, which were subsequently utilized for the training of the network.\n' +
      '\n' +
      'The network ingests calibrated IMU data as its input. To simplify the inputs to the network, we first express the rota tions as quaternions:\n' +
      '\n' +
      '\\[q_{j}^{i}=\\left(\\begin{array}{l}\\sqrt{\\frac{\\max(0,1+R_{i,11}^{j}+R_{i,22}^{j}+R_ {i,33}^{j})}{2}},\\\\ \\sqrt{\\frac{\\max(0,1+R_{i,11}^{j}-R_{i,22}^{j}-R_{i,33}^{j})}{2}}\\text{sgn}(R_{i,32}^{j}-R_{i,23}^{j}),\\\\ \\sqrt{\\frac{\\max(0,1-R_{i,11}^{j}+R_{i,22}^{j}-R_{i,33}^{j})}{2}}\\text{sgn}(R_{i,13}^{j}-R_{i,31}^{j}),\\\\ \\sqrt{\\frac{\\max(0,1-R_{i,11}^{j}-R_{i,22}^{j}+R_{i,33}^{j})}{2}}\\text{sgn}(R_{i,21}^{j}-R_{i,12}^{j})\\end{array}\\right), \\tag{7}\\]\n' +
      '\n' +
      'where \\(\\text{sgn}(\\cdot)\\) represents a symbolic function. We concatenate \\(q_{j}^{i}\\) with \\(a_{j}^{i}\\), denoted as \\(c_{j}^{i}\\in\\mathbb{R}^{7}\\), and then concatenate all 11 IMUs\' signal as \\(C_{j}=\\begin{bmatrix}c_{1},&c_{2},&\\cdots&c_{n}\\end{bmatrix}\\in\\mathbb{R}^{77}\\). These inputs are meticulously calibrated, ensuring both the accuracy and reliability of the IMU signals. In details, for each frame \\(j\\) of imu \\(i\\), we convert the rotation matrix \\(R_{j}^{i}\\) into quaternion, and then put it together with the acceleration to form a \\(7\\)-dimensional vector. We combine the signals from \\(11\\) IMUs in \\(120\\) consecutive frames to \\(\\mathbf{C}_{j}\\in\\mathbb{R}^{77\\times 120}\\) that serves as the input to the network.\n' +
      '\n' +
      'Our network \\(\\mathbf{\\Psi}(\\cdot)\\) comprises three critical components: an initial Fully Connected (FC) layer, a transformer-based diffusion core, a concluding FC layer, and an MLP embedding network. We denote the two FC layer and the transformer-based diffusion core as \\(\\psi(\\cdot)\\).\n' +
      '\n' +
      'Our network is intricately designed around a diffusion architecture. Central to its operation is the concept of iterative denoising, a process repeated \\(n\\) times to achieve the final predicted \\(120\\) frames\' blendshape weights \\(\\mathcal{W}_{\\mathbf{\\Psi},j}\\in\\mathbb{R}^{55\\times 120}\\).\n' +
      '\n' +
      'At the outset of this process, we set \\(x_{j}^{n}\\) as a random noise which has the same dimension with \\(\\mathcal{W}_{\\mathbf{\\Psi},j}\\). For the \\(t\\) iteration, we feed the integer \\(t\\) into the MLP Embedding module. Then, the IMU signal \\(\\mathbf{C}_{j}\\) for the condition is concatenated with \\(x_{j}^{t}\\) and fed into the \\(\\psi|\\) network along with the embedding output, to calculate \\(x_{j}^{t-1}\\):\n' +
      '\n' +
      '\\[x_{j}^{t-1}=\\psi(\\text{MLP}(t),x_{j}^{t},\\mathbf{C}_{j}). \\tag{8}\\]\n' +
      '\n' +
      'We iteratively compute until we get \\(x_{j}^{0}\\) as the predicted blendshape weights \\(\\mathcal{W}_{\\mathbf{\\Psi},j}\\).\n' +
      '\n' +
      'Throughout the training process, ground truth ARKit parameters \\(\\mathcal{W}_{j}\\) serve for supervision. We assess the network\'s performance using an L1 loss function,\n' +
      '\n' +
      '\\[\\mathcal{L}=\\left|\\mathcal{W}_{\\mathbf{\\Psi}}-\\mathcal{W}\\right|. \\tag{9}\\]\n' +
      '\n' +
      'The equation evaluates the accuracy of the network\'s outputs against these ground truth parameters.\n' +
      '\n' +
      'Our training regimen is systematically divided into two stages, the pretrain stage and the finetune stage, each designed to maximize network performance. The pretrain stage involves training with simulate data sourced from our Face-IMU simulator. This simulator provides a diverse array of facial expressions and movements, enabling the network to learn from a broad spectrum of data and ensuring its generalizability.\n' +
      '\n' +
      'Recognizing the inherent differences between simulate and IMU-ARKit data, such as variations in skin elasticity and magnetic field-induced offsets, we implemented a fine-tune stage. This stage is dedicated to finetuning the network with actual IMU-ARKit data. To circumvent the risk of overfitting due to the limited quantity of IMU-ARKit data, we adopted a blended training approach, mixing virtual and IMU-ARKit data in a 4:1 ratio. This strategy ensures that while the network benefits from the nuances of IMU-ARKit data, it remains rooted in the extensive learning provided by the virtual data. Such a balanced approach enhances the network\'s adaptability and efficacy in practical scenarios.\n' +
      '\n' +
      'Figure 6: Training pipeline. We use a two-stage approach for training, dividing into a pre-training phase using simulate data, and a finetune phase using adaptively combined data from real and simulate data. We use a Transformer Diffusion Module to predict expressions from IMU signals and supervise the output using ARKit parameters.\n' +
      '\n' +
      '## 5 Experiment\n' +
      '\n' +
      'In this section, we present the experimental results of IMUSIC in facial motion capture and reconstruction. We begin by detailing the implementation aspects and displaying a gallery of accurately reconstructed assets by IMU-SIC in Fig. 7, highlighting the precision achievable with our method. For each participant, five different expressions were selected for presentation. Displayed in the bottom left corner are photographs of our participants, captured alongside the IMU signal acquisition. Through IMUSIC, we derived blendshape parameters for each frame from these IMU signals. For the generation and rendering of facial assets, we leveraged Dreamface [96], ensuring a high-fidelity visual output of our facial capture technique.\n' +
      '\n' +
      'We then conduct a comprehensive evaluation of our pipeline\'s modules, including the IMU placement and number, assessing both qualitatively and quantitatively in Sec. 5.1. Then, we compare our results with state-of-the-art techniques in the field in Sec. 5.2. Finally, we show specific application scenarios for IMUSIC in Sec. 5.3.\n' +
      '\n' +
      'Figure 7: Gallery of IMUSIC. Each row corresponds to five different expressions of one participant. For each subfigure, the upper right section presents the facial motion reconstructed from IMU signals. The lower left section showcases the image reference.\n' +
      '\n' +
      'Our network architecture is inspired by the work of Li et al. [43]. We use noised blendshape weights as input and IMU data as transformer conditions. The network output is identical to the inputs. We use Adam as the optimizer with learning-rate \\(2\\times 10^{-4},\\alpha=0.9,\\beta=0.999\\). We train and evaluate our network on a single NVIDIA RTX3090 GPU. The pre-training process takes \\(\\approx 4\\) hours on the simulated dataset and \\(\\approx 0.5\\) hours for the finetune process on a single identity with paired data.\n' +
      '\n' +
      '### Evaluation of IMU Configuration\n' +
      '\n' +
      'Evaluation of IMU locationWe comprehensively detail our facial partitioning design based on anatomical considerations [76]. A key factor after establishing these partitions is determining the optimal placement of IMUs on the face to attain a better signal-to-noise ratio. Given the high sensitivity of IMUs, strategically positioning them promises to capture more extensive data. Furthermore, since these flexible positions exhibit larger motion amplitudes, they inherently capture signals with a higher signal-to-noise ratio under constant noise levels. Therefore, IMU locations with greater signal variance, particularly during specific regional expressions and movements, are identified as more adaptable and preferable placement points.\n' +
      '\n' +
      'We qualitatively showcase our empirical approach to IMU placement across different facial regions in Fig. 8. The image on the far left illustrates the schematic for selecting IMU positions, with white dots on the face representing\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c} \\hline \\hline IMU Index & \\#1 & \\#2 & \\#3 & \\#4 & \\#5 \\\\ \\hline \\multirow{3}{*}{Frontalis Area} & Raising Eyebrows & 0.12 & 0.06 & 0.05 & 0.06 & \\\\  & One-sided From & 0.46 & 0.40 & 0.37 & 0.36 & - \\\\  & Furrowing & 0.21 & 0.08 & 0.05 & 0.10 & \\\\ \\hline \\multirow{3}{*}{Zygomaticus area} & One-sided Pouting & 0.91 & 0.72 & 0.34 & 0.30 & \\\\  & Squinting & 0.30 & 0.23 & 0.19 & 0.18 & - \\\\  & Smile & 0.15 & 0.12 & 0.14 & 0.10 & \\\\ \\hline \\multirow{3}{*}{Buccinator and Mentalis Area} & Reading \\#1 & 0.39 & 1.25 & 1.25 & 1.84 & 4.04 \\\\  & Reading \\#2 & 0.42 & 2.58 & 1.75 & 4.19 & 8.08 \\\\ \\cline{1-1}  & Reading \\#3 & 0.15 & 1.59 & 1.55 & 4.60 & 8.62 \\\\ \\cline{1-1}  & Reading \\#4 & 1.01 & 1.80 & 1.30 & 2.58 & 5.34 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: IMU measurement in different regions. The data in the table is expressed as variance times \\(10^{-2}\\).\n' +
      '\n' +
      'Figure 8: Experiment on IMU placement on the face. This figure presents our anatomically-based facial partitioning, highlighting the selected points and the corresponding experiments conducted for each facial region. The left image shows our chosen points on the face, while the other images elaborate on the individual experiments conducted for each specific area. The upper section presents a distribution map of the test points allocated to each region, the middle section identifies the primary expressions and movements associated with that area, and the lower section exhibits the acceleration curves of the IMUs situated at each designated point.\n' +
      '\n' +
      'the actual IMU locations we chose. The images, arranged from left to right, depict the experimental positioning of test points in the Frontalis Area, Zygomaticus Area, and Bucciinator and Mentalis Area. The topmost images show the test IMU positions selected in each region, with red and purple indicating the final positions we utilized. These points were strategically chosen to avoid interference and align with the muscle distribution of each respective area. The middle images demonstrate the specific facial movements performed by participants wearing IMUs. The acceleration data collected from each IMU during these movements, shown in the lower part of the images, was qualitatively compared. Our final IMU position selections all exhibited strong signal strength, indicating significant movement amplitude.\n' +
      '\n' +
      'We quantitatively present the variance comparison for various actions across different facial regions in Table 1. Notably, in the Frontalis Area, IMU1 shows significantly higher variance, guiding our decision for its placement. In the Bucciinator and Mentalis Area, IMUs 1 to 3 are dedicated to Bucciinator detection, with IMUs 4 and 5 focusing on the Mentalis. Among these, IMUs 2 and 5 display the most notable variance and are thus chosen for placement in this region. For the Zygomaticus Area, the distinct variance of IMU1 makes it the preferred placement point. We also illustrate the selected facial expressions and the corresponding IMU data in Fig. 8. Although this may reduce the stability of blink data, it guarantees efficient capture of other eye movements, ensuring the comfort of the eyes.\n' +
      '\n' +
      'Evaluation of IMU numbersBased on the IMU placement experiment, we further tested the effect of the number of IMUs on expression prediction. Among the 11 IMU placements mentioned above, we sequentially and uniformly removed the data from the IMUs. For each experiment, we controlled for other variables, and both used a 2-stage training pipeline, including pre-training using simulated data, and finetuning using real data. The curves in Fig. 9 show that at each reduction in the number of IMUs, the network\'s prediction of expressions by IMU signaling accuracy is uniformly reduced. This indicates that our IMUs have low overlap between them and maximize the acquisition of signals from various regions of the face. As the device is still in its prototype phase, the time investment for deployment significantly increases with each additional IMU. Therefore, we have chosen to use 11 IMUs for our empirical research, balancing efficiency and effectiveness.\n' +
      '\n' +
      '### Evaluation of IMU-based Facial Capture\n' +
      '\n' +
      'As the first IMU-based facial capture scheme, we compared it with the state-of-the-art image based expression prediction method DECA [27] and 3DDFA_V2 [31]. We do the experiment on the testset of IMU-ARKit dataset, and take the iPad captured image as the input of DECA and 3DDFA_V2. The results are shown in Fig. 10. Columns 2, 5, and 6 respectively demonstrate the results of our IMU-based reconstruction compared to the image reconstruction results of DECA and 3DDFA_V2. Judging from the outcomes illustrated in the figures, our reconstruction results are comparable to those of state-of-the-art methods. Compared to DECA, our method demonstrates a slight improvement in eye reconstruction; and in comparison with 3DDFA_V2, our approach yields a relatively better representation of eyebrow expressions. While our method still exhibits some misalignment issues, our approach consistently achieves a comparable level of quality overall.\n' +
      '\n' +
      'Furthermore, we conduct a comprehensive evaluation of two key steps: data augmentation and finetuning. For each input sample, we implemented three distinct experimental settings. Initially, we present results using the full pipeline, incorporating both data augmentation and finetuning. Subsequently, we show results of training the network entirely without simulate data, denoted as **w/o simulate**. Finally, we display results when deploying the network prediction without the finetuning, denoted as **w/o finetune**. These variations are illustrated in columns 2, 3, and 4 of Fig. 10 sequentially. It is noticeable that both **w/o simulate** and **w/o finetune** have greater discrepancies in mouth reconstruction. Additionally, the reconstruction results **w/o finetune** show a greater number of errors in the eye region. Overall, these observations underscore that utilizing the complete pipeline results in more accurate estimations of expressions.\n' +
      '\n' +
      'In supplementary to the qualitative results, we introduce a quantitative evaluation, depicted in Fig. 11. Inspired by [27, 31, 70], we calculate the 3D per vertex error (PVE)[70] on the deformed mesh as an indicator of the similarity between ARKit capture and our prediction, along with the 3D landmark vertex error (PVE_LMK) to further illustrate the similarity on visually significant areas. We also calculate MSE on predicted blendshape weights with ARKit. The red line represents the metrics for each frame using our\n' +
      '\n' +
      'Figure 9: Numbers of IMU with PVE and MSE. We conduct experiments with various numbers of IMU, and calculate the 3D per vertex error (PVE) on deformed mesh and MSE on blendshape weights for quantitative evaluation.\n' +
      '\n' +
      'comprehensive two-step method, while the purple and blue lines represent the metrics for each frame using the single-step approach. It is evident that the metrics of the complete pipeline demonstrate clear advantages.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c|c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c|}{PVE[mm]\\(\\downarrow\\)} & \\multicolumn{3}{c|}{PVE\\_LMK[mm]\\(\\downarrow\\)} & \\multirow{2}{*}{MSE\\(\\downarrow\\)} \\\\ \\cline{2-2} \\cline{5-8}  & median & mean & std & median & mean & std \\\\ \\hline Ours & **0.67** & **0.72** & **0.31** & **1.07** & **1.15** & **0.51** & **0.0057** \\\\\n' +
      '**w/o simulate** & 1.18 & 1.28 & 0.74 & 1.93 & 2.18 & 1.32 & 0.0322 \\\\\n' +
      '**w/o finetune** & 1.54 & 1.65 & 0.60 & 2.52 & 2.74 & 1.07 & 0.0187 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Quantitative ablation study of our method.\n' +
      '\n' +
      'Figure 10: Qualitative comparison and ablation study. The first column displays the reference image. The second column illustrates the reconstruction results of our pipeline. The third and fourth columns show the results of our pipeline without simulation and without finetuning. Columns 5 to 7 illustrate results of DECA [27], 3DDFA_V2 [31] and ARKit [5].\n' +
      '\n' +
      'We also present specific numerical values in Table 2. This quantitative analysis reveals that our full pipeline, integrating both simulate dataset and finetuning, yields the most favorable outcomes. It indicates that there is a tendency towards underfitting in the results without the finetunning step and overfitting without the simulate dataset. These findings underscore the importance of our approach in network training.\n' +
      '\n' +
      '### Application\n' +
      '\n' +
      'Privacy Protected Facial AnimationIn today\'s digital landscape, privacy concerns are increasingly at the forefront, particularly in the field of facial animation and digital avatars. The surge in popularity of virtual platforms underscores the necessity of safeguarding user anonymity and privacy, especially for those who choose to express themselves through digital avatars. This issue is especially critical in contexts like virtual streaming or VTubing, where creators often seek to preserve their privacy while engaging with a broad audience. Traditional facial animation methods predominantly hinge on visual data capture, typically necessitating model localization via camera-based systems. These methods, reliant on visual inputs to track facial movements, can inadvertently breach privacy. Moreover, they are constrained by environmental factors, such as the need for adequate lighting and a direct visual path to the user\'s face, leading to challenges in poorly lit settings or when the user\'s face isn\'t consistently oriented towards the camera. Our innovative approach leverages Inertial Measurement Units (IMUs) for facial animation, thus obviating the need for direct visual capture of the face. This technique not only upholds user privacy by avoiding the capture and storage of visual facial data but also surmounts the limitations inherent in traditional camera-based systems. With our IMU-based technology, factors like lighting conditions or the orientation of the face are no longer impediments. In the realm of VTubing, our method offers significant benefits. It ensures privacy for content creators who prefer anonymity, while adeptly addressing the challenges of operating in environments unsuitable for conventional visual capture methods, such as in low light conditions or when the user is not directly facing the camera. This application is exemplified in Fig. 12, where a user wear our IMU device and employs IMUSIC to deduce expression blendshape parameters. We facilitate avatar animation through a sequence of parameters streamed into Unity [77]. As depicted in the figure, our method accurately predicts expressions even under complex scenarios, such as when the presenter turns their head or experiences occlusion.\n' +
      '\n' +
      'Hybrid CaptureWe now explore the concept of hybrid capture, a technique that synergizes the capabilities of two distinct technologies: IMU-based facial capture and Apple\'s ARKit. Hybrid capture entails integrating blendshape parameters predicted by our IMU system with those detected by ARKit. This method is particularly advantageous in scenarios where ARKit\'s visual signals are obscured, allowing the IMU-derived blendshape signals to supplement\n' +
      '\n' +
      'Figure 11: Quantitative result of our method on a data sequence. We plot the PVE, PVE_LMK and MSE calculated per frame with ARKit as ground truth on a sequence in our testset. The result shows the reliability of our two stage training strategy.\n' +
      '\n' +
      'Figure 12: The application of Vtubing. Our method utilizes IMUs for facial animation, eliminating the requirement for direct exposure under camera.\n' +
      '\n' +
      'and substitute the visual data. Our implementation of hybrid capture is showcased on virtual album production. In studio settings, a common issue is the occlusion of actors\' mouths by microphones, hindering vision-based systems like ARKit from capturing complete facial expressions. Here, IMUs adeptly compensate for the obscured mouth movements, ensuring a comprehensive facial capture. This results in virtual performances that are both more realistic and expressive. In Fig. 13, we illustrate this application, using parameter sequences from the IMUSIC pipeline to replace the corresponding ARKit-recorded sequences, focusing on parameters such as Jawopen, Mouthsmile, etc. The outcomes confirm our method\'s efficacy in supplementing actors\' occluded mouth movements.\n' +
      '\n' +
      'Capture Minute MotionLeveraging the IMU\'s high sensitivity, our study was able to detect minute facial motion with precision. In Fig. 14, the IMU\'s effectiveness is showcased in capturing a participant\'s subtle cheek puffing, marked by the dark gray segment on the second row. We use the rotation measured by the IMU at the cheek position to calculate the angle with the neutral pose, which is then mapped to the \'Cheekpuff\' blendshape parameter.\n' +
      '\n' +
      'We also demonstrate the comparison of our mapping (red line) to the weight detected by ARKit (purple line), revealing a greater alignment with the actual puffing action in the IMU data. Notably, the ARKit signal barely changes with the bulge because the action is so subtle. For visual clarity, we show the participant\'s puff of the check at a given moment (indicated by the red triangle) in the first row, illustrating the IMU\'s heightened sensitivity relative to ARKit\'s signal capture capabilities.\n' +
      '\n' +
      '### Limitation and Discussion\n' +
      '\n' +
      'We have demonstrated the compelling capability of IMUSIC for its non-visual facial motion capture in a variety of applications. Nevertheless, as a novel trial for purely IMU-based facial performance capture, our approach is subject to some limitations.\n' +
      '\n' +
      'First, in our prototyped hardware, the signal detection module still needs to be attached to the facial skins of the performance with double-sided and adhesive tapes, while the data transmission also necessitates tedious wiring. Given the expected continuation of the technological trend of material science and integrated circuits, IMU-like prototypes are becoming cheaper, smaller, safer, and more perva\n' +
      '\n' +
      'Figure 14: Application for capturing minute motion. For the first row, we show an image of the participant puffing slightly, the results detected by ARKit, and the results we measured using the IMU. In the second row of the graph, the dark grey area represents the time the participant puffs his cheek, and the red and purple lines represent the Checkpuff parameters obtained by our method and ARKit respectively.\n' +
      '\n' +
      'Figure 13: The application of virtual album production and song recording. In this setting, IMUSIC effectively compensates for obscured mouth movements, ensuring a comprehensive capture of facial expressions.\n' +
      '\n' +
      'sive. Inspired by the motion gloves using flexible electronic materials [57], it\'s promising to explore more user-friendly and wearable solutions with electronic materials, as conceptually depicted in Fig. 15. We believe that a motion capture system like IMUSIC will become a viable alternative to traditional facial capture technologies.\n' +
      '\n' +
      'Besides, the training scheme in IMUSIC still relies on a two-stage pretrain-then-finetuning paradigm, which limits its generalizability compared to vision-based approaches like DECA and ARKit. This is mainly due to the data scarcity of our IMU-ARKit dataset. Besides our data augmentation, it is promising to utilize the unsupervised and self-supervised strategy [24] to reduce data dependency further. Yet, it requires future exploration to bridge the difference between simulated and actual IMU signals and enhance our simulation module to more closely mimic real data. Moreover, the current purely IMU-based facial capture scheme still falls short of capturing extremely challenging facial motions, especially for those regions near the eyelids, where IMU sensors cannot be easily attached. It\'s interesting to combine our scheme with other non-visual modalities, like audio and optoelectronic signals for more faithful and privacy-protected facial capture.\n' +
      '\n' +
      'To stimulate the above novel and exciting paths of facial capture in our community, we will make both our IMU-ARKit dataset and the companion codes of our IMUSIC approach publicly available. Our approach is the first of its kind to open up the research direction for data-driven IMU-based facial tracking and analysis. Thus, we believe that the accessible IMUSIC implementation and IMU-ARKit dataset will lay a solid cornerstone and bring huge potential for future exploration. Note that despite the inherent privacy-preserving capability of IMUSIC, the collection of extensive facial data remains a challenge. We have ensured informed consent from all participants regarding data usage and collection processes and have secured IRB permission to uphold ethical standards in our research. In summary, while IMUSIC presents a novel path for facial capture, its evolution is intertwined with advancements in hardware design, training methodologies, and ethical data collection practices.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'We have introduced IMUSIC, a novel paradigm for facial motion capture from purely IMU signals, distinct from traditional visual methods. In IMUSIC, our tailored micro-IMUs are strategically attached to face regions aligned with facial anatomy, to capture a wide spectrum of nuanced facial movements. We then provide IMU-ARKit, the first-of-its-kind dataset with synchronous IMU and visual signals of diverse facial expressions from various performers. Based on IMU-ARKit, we propose a strong baseline method for the novel task: IMU-driven facial motion capture. Specifically, we tailor a transformer-based diffusion model to predict the expression parameters from purely IMU signals with a novel two-stage training strategy. We conduct extensive experiments to demonstrate the effectiveness of our approach. We then showcase a series of novel potential applications using IMUSIC, ranging from privacy-protecting facial capture for anonymous Virtual YouTubers, to hybrid capture to overcome challenges like occlusions, or to detect minute facial movements that are often invisible through visual cameras. With the novel task and our companion dataset and baseline solution, we believe IMUSIC renews the research boundary of facial motion capture and analysis.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] David W Aha. Lazy learning. In _Lazy learning_, pages 7-10. Springer, 1997.\n' +
      '* [2] Norhafizan Ahmad, Raja Ariffin Raja Ghazilla, Nazirah M Khairi, and Vijayakaskar Kasi. Reviews on various inertial measurement unit (imu) sensor applications. _International Journal of Signal Processing Systems_, 1(2):256-262, 2013.\n' +
      '* [3] Sadegh Aliakbarian, Pashmina Cameron, Federica Bogo, Andrew Fitzgibbon, and Thomas J Cashman. Flag: Flow-based 3d avatar generation from sparse observations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13253-13262, 2022.\n' +
      '* [4] Sheldon Andrews, Ivan Huerta, Taku Komura, Leonid Sigal, and Kenny Mitchell. Real-time physics-based motion capture with sparse sensors. In _Proceedings of the 13th European conference on visual media production (CVMP 2016)_, pages 1-10, 2016.\n' +
      '* [5] Apple. Arkit. [https://developer.apple.com/arkit/](https://developer.apple.com/arkit/), 2023.\n' +
      '* [6] Eric R Bachmann, Robert B McGhee, Xiaoping Yun, and Michael J Zyda. Inertial and magnetic posture tracking for inserting humans into networked virtual environments. In\n' +
      '\n' +
      'Figure 15: The conceptual visualization showcases the next generation of IMU-based facial capture devices. The image is generated by DALL-E.\n' +
      '\n' +
      '_Proceedings of the ACM symposium on Virtual reality software and technology_, pages 9-16, 2001.\n' +
      '* [7] Linchao Bao, Xiangkai Lin, Yajing Chen, Haoxian Zhang, Sheng Wang, Xuefei Zhe, Di Kang, Haozhi Huang, Xinwei Jiang, Jue Wang, et al. High-fidelity 3d digital human head creation from rgb-d selfies. _ACM Transactions on Graphics (TOG)_, 41(1):1-21, 2021.\n' +
      '* [8] Julio Cesar Batista, Vitor Albiero, Olga RP Bellon, and Luciano Silva. Aumpnet: simultaneous action units detection and intensity estimation on multipose facial images using a single convolutional neural network. In _2017 12th IEEE international conference on automatic face & gesture recognition (FG 2017)_, pages 866-871. IEEE, 2017.\n' +
      '* [9] Thabo Beeler, Fabian Hahn, Derek Bradley, Bernd Bickel, Paul A Beardsley, Craig Gotsman, Robert W Sumner, and Markus H Gross. High-quality passive facial performance capture using anchor frames. _ACM Trans. Graph._, 30(4):75, 2011.\n' +
      '* [10] L Bianchi, D Angelini, GP Orani, and F Lacquantiti. Kinematic coordination in human gait: relation to mechanical energy cost. _Journal of neurophysiology_, 79(4):2155-2170, 1998.\n' +
      '* [11] V Blanz and T Vetter. A morphable model for the synthesis of 3d faces. In _26th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH 1999)_, pages 187-194. ACM Press, 1999.\n' +
      '* [12] Derek Bradley, Wolfgang Heidrich, Tiberiu Popa, and Alla Sheffer. High resolution passive facial performance capture. In _ACM SIGGRAPH 2010 papers_, pages 1-10. 2010.\n' +
      '* [13] Chen Cao, Yanlin Weng, Stephen Lin, and Kun Zhou. 3d shape regression for real-time facial animation. _ACM Transactions on Graphics (TOG)_, 32(4):1-10, 2013.\n' +
      '* [14] Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun Zhou. Facewarehouse: A 3d facial expression database for visual computing. _IEEE Transactions on Visualization and Computer Graphics_, 20(3):413-425, 2013.\n' +
      '* [15] Chen Cao, Derek Bradley, Kun Zhou, and Thabo Beeler. Real-time high-fidelity facial performance capture. _ACM Transactions on Graphics (ToG)_, 34(4):1-9, 2015.\n' +
      '* [16] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz, Michael Zollhoefer, Shun-Suke Saito, Stephen Lombardi, Shih-En Wei, Danielle Belko, Shoou-I Yu, et al. Authentic volumetric avatars from a phone scan. _ACM Transactions on Graphics (TOG)_, 41(4):1-19, 2022.\n' +
      '* [17] Xudong Cao, Yichen Wei, Fang Wen, and Jian Sun. Face alignment by explicit shape regression. _International journal of computer vision_, 107:177-190, 2014.\n' +
      '* [18] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7291-7299, 2017.\n' +
      '* [19] Timothy F Cootes, Christopher J Taylor, David H Cooper, and Jim Graham. Active shape models-their training and application. _Computer vision and image understanding_, 61(1):38-59, 1995.\n' +
      '* [20] Timothy F. Cootes, Gareth J. Edwards, and Christopher J Taylor. Active appearance models. _IEEE Transactions on pattern analysis and machine intelligence_, 23(6):681-685, 2001.\n' +
      '* [21] Juan Antonio Corrales, Francisco A Candelas, and Fernando Torres. Hybrid tracking of human operators using imu/uwb data fusion by a kalman filter. In _Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction_, pages 193-200, 2008.\n' +
      '* [22] Edilson de Aguiar, Christian Theobalt, Marcus Magnor, Holger Theisel, and H-P Seidel. M/sup 3: marker-free model reconstruction and motion tracking from 3d voxel data. In _12th Pacific Conference on Computer Graphics and Applications, 2004. PG 2004. Proceedings._, pages 101-110. IEEE, 2004.\n' +
      '* [23] Michael B Del Rosario, Heba Khamis, Phillip Ngo, Nigel H Lovell, and Stephen J Redmond. Computationally efficient adaptive error-state kalman filter for attitude estimation. _IEEE Sensors Journal_, 18(22):9332-9342, 2018.\n' +
      '* [24] Abdallah Dib, Cedric Thebault, Junghyun Ahn, Philipp-Henri Gosselin, Christian Theobalt, and Louis Chevallier. Towards high fidelity monocular face reconstruction with rich reflectance using self-supervised learning and ray tracing. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 12819-12829, 2021.\n' +
      '* [25] Andrea Dittadi, Sebastian Dziadzio, Darren Cosker, Ben Lundell, Thomas J Cashman, and Jamie Shotton. Full-body motion from a single head-mounted device: Generating smpl poses from partial observations. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11687-11697, 2021.\n' +
      '* [26] Bernhard Egger, William AP Smith, Ayush Tewari, Stefanie Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani, et al. 3d morphable face models--past, present, and future. _ACM Transactions on Graphics (ToG)_, 39(5):1-38, 2020.\n' +
      '* [27] Yao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart. Learning an animatable detailed 3d face model from in-the-wild images. _ACM Transactions on Graphics (ToG)_, 40(4):1-13, 2021.\n' +
      '* [28] Giancarlo Ferrigno, NA Borghese, and Antonio Pedotti. Pattern recognition in 3d automatic human motion analysis. _ISPRS Journal of Photogrammetry and Remote Sensing_, 45(4):227-246, 1990.\n' +
      '* [29] Eric Foxlin. Inertial head-tracker sensor fusion by a complementary separate-bias kalman filter. In _Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium_, pages 185-194. IEEE, 1996.\n' +
      '* [30] Andrew Gilbert, Matthew Trumble, Charles Malleson, Adrian Hilton, and John Collomosse. Fusing visual and inertial sensors with semantics for 3d human pose estimation. _International Journal of Computer Vision_, 127:381-397, 2019.\n' +
      '* [31] Jianzhu Guo, Xiangyu Zhu, Yang Yang, Fan Yang, Zhen Lei, and Stan Z Li. Towards fast, accurate and stable 3d dense face alignment. In _European Conference on Computer Vision_, pages 152-168. Springer, 2020.\n' +
      '\n' +
      '* [32] Yan Guo, Gang Xu, and Saburo Tsuji. Understanding human motion patterns. In _Proceedings of the 12th IAPR International Conference on Pattern Recognition, Vol. 3-Conference C: Signal Processing (Cat. No. 94CH3440-5)_, pages 325-329. IEEE, 1994.\n' +
      '* [33] Yudong Guo, Jianfei Cai, Boyi Jiang, Jianmin Zheng, et al. Cnn-based real-time dense face reconstruction with inverse-rendered photo-realistic face images. _IEEE transactions on pattern analysis and machine intelligence_, 41(6):1294-1307, 2018.\n' +
      '* [34] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and Christian Theobalt. Livecap: Real-time human performance capture from monocular video. _ACM Transactions On Graphics (TOG)_, 38(2):1-17, 2019.\n' +
      '* [35] Julius Hannink, Thomas Kautz, Cristian F Pasiuosta, Karl-Gunter Gassmann, Jochen Klucken, and Bjoern M Eskofier. Sensor-based gait parameter extraction with deep convolutional neural networks. _IEEE journal of biomedical and health informatics_, 21(1):85-93, 2016.\n' +
      '* [36] Thomas Helten, Meinard Muller, Hans-Peter Seidel, and Christian Theobalt. Real-time body tracking with one depth camera and inertial sensors. In _Proceedings of the IEEE international conference on computer vision_, pages 1105-1112, 2013.\n' +
      '* [37] Roberto Henschel, Timo Von Marcard, and Bodo Rosenhahn. Accurate long-term multiple people tracking using video and body-worn imus. _IEEE Transactions on Image Processing_, 29:8476-8489, 2020.\n' +
      '* [38] Fuyang Huang, Ailing Zeng, Minhao Liu, Qiuxia Lai, and Qiang Xu. Deepfuse: An imu-aware network for real-time 3d human pose estimation from multi-view image. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 429-438, 2020.\n' +
      '* [39] Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J Black, Otmar Hilliges, and Gerard Pons-Moll. Deep inertial poser: Learning to reconstruct human pose from sparse inertial measurements in real time. _ACM Transactions on Graphics (TOG)_, 37(6):1-15, 2018.\n' +
      '* [40] Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa Laich, Patrick Snape, and Christian Holz. Avatar-poser: Articulated full-body pose tracking from sparse motion sensing. In _European Conference on Computer Vision_, pages 443-460. Springer, 2022.\n' +
      '* [41] Samuli Laine, Tero Karras, Timo Aila, Antti Herva, Shunsuke Saito, Ronald Yu, Hao Li, and Jaakko Lehtinen. Production-level facial performance capture using deep convolutional neural networks. In _Proceedings of the ACM SIGGRAPH/Eurographics symposium on computer animation_, pages 1-10, 2017.\n' +
      '* [42] Jiaman Li, Zhengfei Kuang, Yajie Zhao, Mingming He, Karl Bladin, and Hao Li. Dynamic facial asset and rig generation from a single scan. _ACM Trans. Graph._, 39(6):215-1, 2020.\n' +
      '* [43] Jiaman Li, Karen Liu, and Jiajun Wu. Ego-body pose estimation via ego-head pose estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17142-17151, 2023.\n' +
      '* [44] Yuwei Li, Minye Wu, Yuyao Zhang, Lan Xu, and Jingyi Yu. Piano: A parametric hand bone model from magnetic resonance imaging. _arXiv preprint arXiv:2106.10893_, 2021.\n' +
      '* [45] Daizong Liu, Hongting Zhang, and Pan Zhou. Video-based facial expression recognition using graph convolutional networks. In _2020 25th International Conference on Pattern Recognition (ICPR)_, pages 607-614. IEEE, 2021.\n' +
      '* [46] Huajun Liu, Xiaolin Wei, Jinxiang Chai, Inwoo Ha, and Taehyun Rhee. Realtime human motion control with a small number of inertial sensors. In _Symposium on interactive 3D graphics and games_, pages 133-140, 2011.\n' +
      '* [47] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser Sheikh. Deep appearance models for face rendering. _ACM Transactions on Graphics (ToG)_, 37(4):1-13, 2018.\n' +
      '* [48] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl: A skinned multi-person linear model. _ACM Transactions on Graphics_, 34(6), 2015.\n' +
      '* [49] Luming Ma and Zhigang Deng. Real-time hierarchical facial performance capture. In _Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games_, pages 1-10, 2019.\n' +
      '* [50] Oleg Makaussov, Mikhail Krassavin, Maxim Zhabinets, and Siamac Fazli. A low-cost, imu-based real-time on device gesture recognition glove. In _2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)_, pages 3346-3351. IEEE, 2020.\n' +
      '* [51] Charles Malleson, Andrew Gilbert, Matthew Trumble, John Collomosse, Adrian Hilton, and Marco Volino. Real-time full-body motion capture from video and imus. In _2017 International Conference on 3D Vision (3DV)_, pages 449-457. IEEE, 2017.\n' +
      '* [52] Charles Malleson, John Collomosse, and Adrian Hilton. Real-time multi-person motion capture from multi-view video and imus. _International Journal of Computer Vision_, 128:1594-1611, 2020.\n' +
      '* [53] Jennifer L McGinley, Richard Baker, Rory Wolfe, and Meg E Morris. The reliability of three-dimensional kinematic gait measurements: a systematic review. _Gait & posture_, 29(3):360-369, 2009.\n' +
      '* [54] Vladimir Medved. _Measurement and Analysis of Human Locomotion_. Springer, 2021.\n' +
      '* [55] Brice Michoud, Erwan Guillou, Hector Briceno, and Saida Bouakaz. Real-time marker-free motion capture from multiple cameras. In _2007 IEEE 11th International Conference on Computer Vision_, pages 1-7. IEEE, 2007.\n' +
      '* [56] Emily Miller, Kenton Kaufman, Trevor Kingsbury, Erik Wolf, Jason Wilken, and Marilynn Wyatt. Mechanical testing for three-dimensional motion analysis reliability. _Gait & posture_, 50:116-119, 2016.\n' +
      '* [57] Chaithanya Kumar Mummadi, Frederic Philips Peter Leo, Keshav Deep Verma, Shivaji Kasireddy, Philipp M Scholl, Jochen Kempfle, and Kristof Van Laerhoven. Real-time and embedded detection of hand gestures with an imu-based glove. In _Informatics_, page 28. MDPI, 2018.\n' +
      '* [58] Noitom. Noitom Motion Capture Systems. [https://www.noitom.com/](https://www.noitom.com/), 2015.\n' +
      '* [59]* [59] Kyle Olszewski, Joseph J Lim, Shunsuke Saito, and Hao Li. High-fidelity facial and speech animation for vr hmds. _ACM Transactions on Graphics (TOG)_, 35(6):1-14, 2016.\n' +
      '* [60] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter. A 3d face model for pose and illumination invariant face recognition. In _2009 sixth IEEE international conference on advanced video and signal based surveillance_, pages 296-301. Ieee, 2009.\n' +
      '* [61] Gerard Pons-Moll, Andreas Baak, Thomas Helten, Meinard Muller, Hans-Peter Seidel, and Bodo Rosenhahn. Multisensor-fusion for 3d full-body human motion capture. In _2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pages 663-670. IEEE, 2010.\n' +
      '* [62] Gerard Pons-Moll, Andreas Baak, Juergen Gall, Laura Leal-Taixe, Meinard Mueller, Hans-Peter Seidel, and Bodo Rosenhahn. Outdoor human motion capture using inverse kinematics and von mises-fisher sampling. In _2011 International Conference on Computer Vision_, pages 1243-1250. IEEE, 2011.\n' +
      '* [63] Ammar Qammaz and Antonis A Argyros. A unified approach for occlusion tolerant 3d facial pose capture and gaze estimation using mocapnets. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3178-3188, 2023.\n' +
      '* [64] QST Inc. QST Corporation Limited. [https://www.qstcorp.com/](https://www.qstcorp.com/), 2012.\n' +
      '* [65] Clement Reveredy, Sylvie Gibet, and Caroline Larboulette. Optimal marker set for motion capture of dynamical facial expressions. In _Proceedings of the 8th ACM SIGGRAPH Conference on Motion in Games_, pages 31-36, 2015.\n' +
      '* [66] Qaiser Riaz, Guanhong Tao, Bjorn Kruger, and Andreas Weber. Motion reconstruction using very few accelerometers and ground contacts. _Graphical Models_, 79:23-38, 2015.\n' +
      '* [67] Daniel Roetenberg, Henk J Luinge, Chris TM Baten, and Peter H Veltink. Compensation of magnetic disturbances improves inertial and magnetic sensing of human body segment orientation. _IEEE Transactions on neural systems and rehabilitation engineering_, 13(3):395-405, 2005.\n' +
      '* [68] Javier Romero, Dimitrios Tzionas, and Michael J Black. Embodied hands. _ACM Transactions on Graphics_, 36(6):1-17, 2017.\n' +
      '* [69] Martin Schepers, Matteo Giuberti, Giovanni Bellusci, et al. Xsens mvn: Consistent tracking of human motion using inertial sensing. _Xsens Technol_, 1(8):1-8, 2018.\n' +
      '* [70] Soshi Shimada, Vladislav Golyanik, Patrick Perez, and Christian Theobalt. Decaf: Monocular deformation capture for face and hand interactions, 2023.\n' +
      '* [71] Ronit Slyper and Jessica K Hodgins. Action capture with accelerometers. In _Proceedings of the 2008 ACM SIGGRAPH/Eurographics symposium on computer animation_, pages 193-199, 2008.\n' +
      '* [72] William AP Smith, Alassane Seck, Hannah Dee, Bernard Tiddeman, Joshua B Tenenbaum, and Bernhard Egger. A morphable face albedo model. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5011-5020, 2020.\n' +
      '* [73] SONY. Mobile Motion Capture "mocopi". [https://www.sony.net/Products/mocopi-dev/en/](https://www.sony.net/Products/mocopi-dev/en/), 2023.\n' +
      '* [74] Jochen Tautges, Arno Zinke, Bjorn Kruger, Jan Baumann, Andreas Weber, Thomas Helten, Meinard Muller, Hans-Peter Seidel, and Bernd Eberhardt. Motion reconstruction using sparse accelerometer data. _ACM Transactions on Graphics (ToG)_, 30(3):1-12, 2011.\n' +
      '* [75] Anh Tuan Tran, Tal Hassner, Iacopo Masi, and Gerard Medioni. Regressing robust and discriminative 3d morphable models with a very deep neural network. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5163-5172, 2017.\n' +
      '* [76] Zarins Uldis. _Anatomy of Facial Expressions_. Anatomy Next, Inc., 2017.\n' +
      '* [77] Unity Technologies. Unity. [https://unity.com/](https://unity.com/), 2023. Version 2023.1.\n' +
      '* [78] UnrealEngine. Live link face. [https://apps.apple.com/us/app/live-link-face/id1495370836](https://apps.apple.com/us/app/live-link-face/id1495370836), 2023.\n' +
      '* [79] Rachel V Vitali, Ryan S McGinnis, and Noel C Perkins. Robust error-state kalman filter for estimating imu orientation. _IEEE Sensors Journal_, 21(3):3561-3569, 2020.\n' +
      '* [80] Daniel Vlasic, Rolf Adelsberger, Giovanni Vannucci, John Barnwell, Markus Gross, Wojciech Matusik, and Jovan Popovic. Practical motion capture in everyday surroundings. _ACM transactions on graphics (TOG)_, 26(3):35-es, 2007.\n' +
      '* [81] Daniel Vlasic, Ilya Baran, Wojciech Matusik, and Jovan Popovic. Articulated mesh animation from multi-view silhouettes. In _Acm Siggraph 2008 papers_, pages 1-9. 2008.\n' +
      '* [82] Timo Von Marcard, Gerard Pons-Moll, and Bodo Rosenhahn. Human pose estimation from video and imus. _IEEE transactions on pattern analysis and machine intelligence_, 38(8):1533-1547, 2016.\n' +
      '* [83] Timo Von Marcard, Bodo Rosenhahn, Michael J Black, and Gerard Pons-Moll. Sparse inertial poser: Automatic 3d human pose estimation from sparse imus. In _Computer graphics forum_, pages 349-360. Wiley Online Library, 2017.\n' +
      '* [84] Timo Von Marcard, Roberto Henschel, Michael J Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and a moving camera. In _Proceedings of the European conference on computer vision (ECCV)_, pages 601-617, 2018.\n' +
      '* [85] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. Mead: A large-scale audio-visual dataset for emotional talking-face generation. In _European Conference on Computer Vision_, pages 700-717. Springer, 2020.\n' +
      '* [86] Thibaut Weise, Sofen Bouaziz, Hao Li, and Mark Pauly. Realtime performance-based facial animation. _ACM transactions on graphics (TOG)_, 30(4):1-10, 2011.\n' +
      '\n' +
      '* [87] Alexander Winkler, Jungdam Won, and Yuting Ye. Quest-sim: Human motion tracking from sparse sensors with simulated avatars. In _SIGGRAPH Asia 2022 Conference Papers_, pages 1-8, 2022.\n' +
      '* [88] XSENS. Xsens Technologies B.V. [https://www.xsens.com/](https://www.xsens.com/), 2011.\n' +
      '* [89] Dongseok Yang, Doyeon Kim, and Sung-Hee Lee. Lobstr: Real-time lower-body pose prediction from sparse upper-body tracking signals. In _Computer Graphics Forum_, pages 265-275. Wiley Online Library, 2021.\n' +
      '* [90] Xinyu Yi, Yuxiao Zhou, and Feng Xu. Transpose: Real-time 3d human translation and pose estimation with six inertial sensors. _ACM Transactions on Graphics (TOG)_, 40(4):1-13, 2021.\n' +
      '* [91] Jae Shin Yoon, Takaaki Shiratori, Shoou-I Yu, and Hyun Soo Park. Self-supervised adaptation of high-fidelity face models for monocular performance tracking. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4601-4609, 2019.\n' +
      '* [92] Jun Yu and Zengfu Wang. A video-based facial motion tracking and expression recognition system. _Multimedia Tools and Applications_, 76:14653-14672, 2017.\n' +
      '* [93] Qilong Yuan and I-Ming Chen. Localization and velocity tracking of human via 3 imu sensors. _Sensors and Actuators A: Physical_, 212:25-33, 2014.\n' +
      '* [94] Li Zhang, Noah Snavely, Brian Curless, and Steven M Seitz. Spacetime faces: high resolution capture for modeling and animation. In _ACM SIGGRAPH 2004 Papers_, pages 548-558. 2004.\n' +
      '* [95] Longwen Zhang, Chuxiao Zeng, Qixuan Zhang, Hongyang Lin, Ruixiang Cao, Wei Yang, Lan Xu, and Jingyi Yu. Video-driven neural physically-based facial asset for production, 2022.\n' +
      '* [96] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and Jingyi Yu. Dreamface: Progressive generation of animatable 3d faces under text guidance, 2023.\n' +
      '* [97] Zhengyou Zhang. Microsoft kinect sensor and its effect. _IEEE multimedia_, 19(2):4-10, 2012.\n' +
      '* [98] Zhe Zhang, Chunyu Wang, Wenhu Qin, and Wenjun Zeng. Fusing wearable imus with multi-view images for human pose estimation: A geometric approach. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2200-2209, 2020.\n' +
      '* [99] Zerong Zheng, Tao Yu, Hao Li, Kaiwen Guo, Qionghai Dai, Lu Fang, and Yebin Liu. Hybridfusion: Real-time performance capture using a single depth sensor and sparse imus. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 384-400, 2018.\n' +
      '* [100] Yi Zhou, Wei Zhang, Xiaoou Tang, and Harry Shum. A bayesian mixture model for multi-view face alignment. In _2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\'05)_, pages 741-746. IEEE, 2005.\n' +
      '* [101] Xiangyu Zhu, Xiaoming Liu, Zhen Lei, and Stan Z Li. Face alignment in full pose range: A 3d total solution. _IEEE transactions on pattern analysis and machine intelligence_, 41(1):78-92, 2017.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>