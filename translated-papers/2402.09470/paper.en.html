<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Rolling Diffusion Models\n' +
      '\n' +
      'David Ruhe\n' +
      '\n' +
      'Jonathan Heek\n' +
      '\n' +
      'Tim Salimans\n' +
      '\n' +
      'Emiel Hoogeboom\n' +
      '\n' +
      'Work done as a Student Researcher at Google. \\({}^{1}\\)Google Deepmind, Amsterdam, Netherlands \\({}^{2}\\)University of Amsterdam, Netherlands. Correspondence to: David Ruhe \\(<\\)david.ruhe@gmail.com\\(>\\), Jonathan Heek, Tim Salimans, Emiel Hoogeboom \\(<\\){jheek, salimans, emieth}@google.com\\(>\\).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Diffusion models have recently been increasingly applied to temporal data such as video, fluid mechanics simulations, or climate data. These methods generally treat subsequent frames equally regarding the amount of noise in the diffusion process. This paper explores _Rolling Diffusion_: a new approach that uses a sliding window denoising process. It ensures that the diffusion process progressively corrupts through time by assigning more noise to frames that appear later in a sequence, reflecting greater uncertainty about the future as the generation process unfolds. Empirically, we show that when the temporal dynamics are complex, Rolling Diffusion is superior to standard diffusion. In particular, this result is demonstrated in a video prediction task using the Kinetics-600 video dataset and in a chaotic fluid dynamics forecasting experiment.\n' +
      '\n' +
      'Machine Learning, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Diffusion models (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Ho et al., 2020) have significantly boosted the field of generative modeling. They provided the fundaments for large-scale text-to-image systems like DALL-E 2 (Ramesh et al., 2022), Imagen (Saharia et al., 2022), Parti (Yu et al., 2022), and Stable Diffusion (Rombach et al., 2022). Other applications of diffusion models include density estimation, text-to-speech, and image editing (Kingma et al., 2021; Gao et al., 2023; Kawar et al., 2023).\n' +
      '\n' +
      'After these successes in these domains, interest in developing diffusion models for time sequences has grown. Prominent recent large-scale works include, e.g., Imagen Video (Ho et al., 2022), Stable Diffusion Video (StabilityAI, 2023). Other impressive results for generating video data have been achieved, e.g., by (Blattmann et al., 2023; Ge et al., 2023; Harvey et al., 2022; Singer et al., 2022; Ho et al., 2022). Applications of sequential generative modeling outside video include, e.g., fluid mechanics or weather and climate modeling (Price et al., 2023; Meng et al., 2022; Lippe et al., 2023).\n' +
      '\n' +
      'What\'s common across many of these works is that they treat the temporal axis as an "extra spatial dimension". That is, they treat the video as a 3D tensor of shape \\(K\\times H\\times W\\). This has several downsides. First, the memory and computational requirements can quickly grow infeasible if one wants to generate long sequences. Second, one is typically interested in being able to _roll out_ generation for a variable number of time steps. Therefore, an alternative angle is a fully autoregressive approach by conditioning on a sequence of input frames and simulating a single output frame, which is then concatenated to the input frames, upon which the recursion can continue. In this case, one has to traverse the entire denoising diffusion chain for every single frame, which is computationally intensive. Additionally, iteratively sampling single frames leads to quick autoregressive error accumulation. A middle ground can be found by jointly generating blocks of frames. However, in this _block-autoregressive_ case, a diffusion model would use the same number of denoising steps for every frame. This is suboptimal since, given a sequence of input frames, the uncertainty about the first upcoming few is much lower than the later ones. Finally, both methods sample frames only jointly with earlier frames, which is potentially a suboptimal parameterization.\n' +
      '\n' +
      'In this work, we propose a new framework called _Rolling Diffusion_, a method that explicitly corrupts data from past to future. This is achieved by reparameterizing the global diffusion time to a _local time_ for each frame. It turns out that by doing this, one can (apart from boundary conditions) completely focus on a local sliding window sequential denoising process. This has several temporal inductive biases, alleviating some of the abovementioned issues.\n' +
      '\n' +
      '1. The model only has to predict the low frequencies (corresponding to their global visual features) for distant frames, whereas the high-frequency details are generated for temporally near frames.\n' +
      '2. Each frame is generated together with both a numberof preceding and succeeding frames.\n' +
      '3. Due to the local sliding window point of view, every frame enjoys the same inductive bias and undergoes a similar sampling procedure regardless of its absolute position in the video.\n' +
      '\n' +
      'These merits are empirically demonstrated in, among others, a video prediction experiment using the Kinetics-600 video dataset and in an experiment involving chaotic fluid mechanics simulations.\n' +
      '\n' +
      '## 2 Background: Diffusion Models\n' +
      '\n' +
      '### Diffusion\n' +
      '\n' +
      'Diffusion models consist of a process that destroys data stochastically, named the diffusion process and a generative process called the denoising process. Let \\(\\mathbf{z}_{t}\\in\\mathbb{R}^{D}\\) denote a latent variable over a diffusion dimension \\(t\\in[0,1]\\). Throughout this work, we will refer to \\(t\\) as the _global (diffusion) time_. Given a datapoint \\(\\mathbf{x}\\in\\mathbb{R}^{D}\\), \\(\\mathbf{x}\\sim q(\\mathbf{x})\\), the diffusion process is designed so that \\(\\mathbf{z}_{0}\\approx\\mathbf{x}\\) and \\(\\mathbf{z}_{1}\\sim\\mathcal{N}(0,1)\\) via the distribution:\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{t}|\\mathbf{x}):=\\mathcal{N}(\\mathbf{z}_{t}|\\alpha_{t}\\mathbf{x},\\sigma_{t}^{2} \\mathbf{I})\\,, \\tag{1}\\]\n' +
      '\n' +
      'where \\(a_{t}\\) and \\(\\sigma_{t}^{2}\\) are strictly positive scalar functions of \\(t\\). We define their _signal-to-noise ratio_\n' +
      '\n' +
      '\\[\\mathrm{SNR}(t):=\\frac{\\alpha_{t}^{2}}{\\sigma_{t}^{2}}\\,, \\tag{2}\\]\n' +
      '\n' +
      'to be _monotonically decreasing_ in \\(t\\). Finally, we let \\(\\alpha_{t}^{2}+\\sigma_{t}^{2}=1\\), corresponding to a _variance-preserving_ process which also implies \\(a_{t}^{2}\\in(0,1]\\) and \\(\\sigma_{t}^{2}\\in(0,1]\\).\n' +
      '\n' +
      'Given the noising process, it can be shown (Sohl-Dickstein et al., 2015) that the _true_ (i.e., optimal) denoising distribution for a single datapoint \\(\\mathbf{x}\\) from time \\(t\\) to time \\(s\\) is given by\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})=\\mathcal{N}(\\mathbf{z}_{s}|\\mu_{t\\to s}(\\mathbf{z}_{t },\\mathbf{x}),\\sigma_{t\\to s}^{2}\\mathbf{I})\\,, \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\mu\\) and \\(\\sigma^{2}\\) are _analytical_ mean and variance functions of \\(t\\), \\(s\\), \\(\\mathbf{x}\\) and \\(\\mathbf{z}_{t}\\). The parameterized generative process \\(p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})\\) is then defined by approximating \\(\\mathbf{x}\\) via a neural network \\(f_{\\theta}:\\mathbb{R}^{D}\\times[0,1]\\to\\mathbb{R}^{D}\\). That is, we set\n' +
      '\n' +
      '\\[p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t}):=q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x}=f_{\\theta}( \\mathbf{z}_{t},t))\\,. \\tag{4}\\]\n' +
      '\n' +
      'The diffusion objective can be expressed as a KL-divergence between the diffusion process and the denoising process, i.e. \\(\\mathrm{KL}(q(\\mathbf{x},\\mathbf{z}_{0},\\dots,\\mathbf{z}_{1})|p(\\mathbf{x},\\mathbf{z}_{0},\\dots, \\mathbf{z}_{1}))\\) which simplifies to (Kingma et al., 2021):\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\theta}(\\mathbf{x}) :=\\mathbb{E}_{t\\sim U(0,1),\\mathbf{e}\\sim\\mathcal{N}(0,1)}\\left[w(t) ||\\mathbf{x}-f_{\\theta}(\\mathbf{z}_{t,\\mathbf{e}},t)||^{2}\\right]\\] \\[\\quad+\\mathcal{L}_{\\mathrm{prior}}+\\mathcal{L}_{\\mathrm{data}}\\,, \\tag{5}\\]\n' +
      '\n' +
      'where \\(\\mathcal{L}_{\\mathrm{prior}}\\) and \\(\\mathcal{L}_{\\mathrm{data}}\\) are typically negligible. The weighting \\(w(t)\\) can be freely specified. In practice, it was found that specific weightings of the loss result in better sample quality (Ho et al., 2020). This is the case for, e.g., _e-loss_(Ho et al., 2020), which corresponds to \\(w(t)=\\mathrm{SNR}(t)\\).\n' +
      '\n' +
      '### Diffusion for temporal data\n' +
      '\n' +
      'If one is interested in generation of temporal data, potentially indefinitely (or beyond typical hardware constraints), one must consider (autoregressive) conditional extension of previously generated data. I.e., given an initial sample \\(\\mathbf{x}^{k}\\sim q(\\mathbf{x})\\) at a temporal index \\(k\\), we want to estimate and sample the conditional distribution \\(p(\\mathbf{x}^{k+1}|\\mathbf{x}^{k})\\). This process can then be extended to videos of arbitrary lengths. As discussed in Section 1, it is not yet clear what kinds of parameterization choices are optimal to estimate this conditional distribution. Further, no temporal inductive bias is typically baked into the denoising process.\n' +
      '\n' +
      '## 3 Rolling Diffusion Models\n' +
      '\n' +
      'We introduce _rolling diffusion models_, merging the arrow of time with the (de)noising process. To formalize this, we first have to discuss the global diffusion model. We will see that the only nontrivial parts of the global process take place locally. Defining the noise schedule locally is advantageous since the resulting model does not depend on the number of frames \\(K\\) and can be unrolled indefinitely.\n' +
      '\n' +
      '### A global perspective\n' +
      '\n' +
      'Let \\(\\mathbf{x}\\in\\mathbb{R}^{D\\times K}\\) be a time series datapoint where \\(K\\) denotes the number of frames and \\(D\\) the dimensionality of each frame. The core idea that allows rolling diffusion is a _reparameterization_ of the diffusion time \\(t\\) to a frame-dependent _local (frame-dependent) time_: i.e.,\n' +
      '\n' +
      '\\[t\\mapsto t_{k}\\,. \\tag{6}\\]\n' +
      '\n' +
      'Note that we still require \\(t_{k}\\in[0,1]\\) for all \\(k\\in\\{0,\\dots,K-1\\}\\). Furthermore, we still have a monotonically decreasing signal-to-noise schedule, ensuring a well-defined diffusion process. However, we now (effectively) have a different signal-to-noise schedule for each frame. In this work, we also always have \\(t_{k}\\leq t_{k+1}\\) i.e., the local time of a frame is smaller than the local time of the next frame. This means we add more noise to future frames: a natural temporal inductive bias. Note that this is not strictly required; one could also have a reverse-time inductive bias or a mixture. An example of such a reparameterization is shown in Figure 1 (left). We depict a map that takes a global diffusion time \\(t\\) (vertical axis) and a frame index \\(k\\) (horizontal axis), and computes a local time \\(t_{k}\\), indicated with a color intensity.\n' +
      '\n' +
      'Forward processWe now redefine the forward process using the local time:\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{t}|\\mathbf{x}):=\\prod_{k=0}^{K-1}\\mathcal{N}(\\mathbf{z}_{t}^{k}|\\alpha_{t_{k}} \\mathbf{x}^{k},\\sigma_{t_{k}}^{2}\\mathbf{I})\\,, \\tag{7}\\]\n' +
      '\n' +
      'where we can _reuse_ the \\(\\alpha\\) and \\(\\sigma\\) functions (now evaluated locally at \\(t_{k}\\)) from before. Here, \\(\\mathbf{x}^{k}\\) denotes the \\(k\\)-th frame of \\(\\mathbf{x}\\).\n' +
      '\n' +
      'True backward process and generative processGiven a tuple \\((s,t)\\), \\(s\\in[0,1]\\), \\(t\\in[0,1]\\), \\(s\\leq t\\), we can divide the frames \\(k\\in\\{0,\\dots,K-1\\}\\) into three categories:\n' +
      '\n' +
      '\\[\\text{clean}(s,t) :=\\left\\{k\\mid s_{k}=t_{k}=0\\right\\}, \\tag{8}\\] \\[\\text{noise}(s,t) :=\\left\\{k\\mid s_{k}=t_{k}=1\\right\\},\\] (9) \\[\\text{win}(s,t) :=\\left\\{k\\mid s_{k}\\in[0,1),t_{k}\\in(s_{k},1]\\right\\}. \\tag{10}\\]\n' +
      '\n' +
      'This categorization can be motivated using the schedule depicted in Figure 1. Given, for example, \\(t=0.5\\) and \\(s=0.375\\), we see that the first frame \\(k=0\\) falls in the first category. At this point in time, \\(\\mathbf{z}_{t_{0}}=\\mathbf{z}_{s_{0}}\\) are identical given that \\(\\lim_{t\\to 0^{+}}\\log\\operatorname{SNR}(t)=\\infty\\). On the other hand, the last frame \\(k=K-1\\) (\\(31\\) in the figure) falls in the second category, i.e., both \\(\\mathbf{z}_{t_{K-1}}\\) and \\(\\mathbf{z}_{s_{K-1}}\\) are distributed as independent standard Gaussians, given that \\(\\lim_{t\\to 1^{-}}\\log\\operatorname{SNR}(t)=-\\infty\\). Finally, the frame \\(k=16\\) falls in the third, most interesting category: the sliding window. As such, observe that the _true_ denoising process can be factorized as:\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})=q(\\mathbf{z}_{s}^{\\text{clean}}|\\mathbf{z}_{t},\\mathbf{x} )q(\\mathbf{z}_{s}^{\\text{noise}}|\\mathbf{z}_{t},\\mathbf{x})q(\\mathbf{z}_{s}^{\\text{win}}|\\mathbf{ z}_{t},\\mathbf{x}). \\tag{11}\\]\n' +
      '\n' +
      'This is helpful because we will see that the only frames that need to be modeled are in the window. Namely, the first factor has\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{s}^{\\text{clean}}|\\mathbf{z}_{t},\\mathbf{x})=\\prod_{k\\in\\text{clean}(s,t)} \\delta(\\mathbf{z}_{s}^{k}|\\mathbf{z}_{t}^{k})\\,. \\tag{12}\\]\n' +
      '\n' +
      'In other words, if \\(\\mathbf{z}_{t}^{k}\\) is already noiseless, then \\(\\mathbf{z}_{s}^{k}\\) will also be noiseless. Regarding the second factor, we see that they are all independently normally distributed:\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{s}^{\\text{noise}}|\\mathbf{z}_{t},\\mathbf{x})=\\prod_{k\\in\\text{noise}(s,t)} \\mathcal{N}(\\mathbf{z}_{s}^{k}|0,\\mathbf{I}). \\tag{13}\\]\n' +
      '\n' +
      'Simply put, in these cases \\(\\mathbf{z}_{s}^{k}\\) is independent noise and does not depend on data at all. Finally, the third factor has a true non-trivial denoising process:\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{s}^{\\text{win}}|\\mathbf{z}_{t},\\mathbf{x})=\\prod_{k\\in\\text{win}(s,t)} \\mathcal{N}(\\mathbf{z}_{s}^{k}|\\mu_{t_{k}\\to s_{k}}(\\mathbf{z}_{t}^{k},\\mathbf{x}^{k}), \\sigma_{t_{k}\\to s_{k}}^{2}\\mathbf{I})\\]\n' +
      '\n' +
      'where \\(\\mu_{t_{k}\\to s_{k}}\\) and \\(\\sigma_{t_{k}\\to s_{k}}^{2}\\) are the analytical mean and variance functions. Note that we can then optimally (w.r.t. a KL-divergence) factorize the generative process similarly:\n' +
      '\n' +
      '\\[p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t}):=p(\\mathbf{z}_{s}^{\\text{clean}}|\\mathbf{z}_{t})p( \\mathbf{z}_{s}^{\\text{noise}}|\\mathbf{z}_{t})p_{\\theta}(\\mathbf{z}_{s}^{\\text{win}}|\\mathbf{z}_ {t})\\,, \\tag{14}\\]\n' +
      '\n' +
      'with \\(p(\\mathbf{z}_{s}^{\\text{clean}}|\\mathbf{z}_{t}):=\\prod_{k\\in\\text{clean}(s,t)}\\delta( \\mathbf{z}_{s}^{k}|\\mathbf{z}_{t}^{k})\\) and \\(p(\\mathbf{z}_{s}^{\\text{noise}}|\\mathbf{z}_{t}):=\\prod_{k\\in\\text{noise}(s,t)}\\mathcal{ N}(\\mathbf{z}_{s}^{k}|0,\\mathbf{I})\\). The only \'interesting\' parameterized part of the generative process then has\n' +
      '\n' +
      '\\[p_{\\theta}(\\mathbf{z}_{s}^{\\text{win}}|\\mathbf{z}_{t}):=\\prod_{k\\in\\text{win}(s,t)}q( \\mathbf{z}_{s}^{k}|\\mathbf{z}_{t},\\mathbf{x}^{k}=f_{\\theta}(\\mathbf{z}_{t},t_{k})). \\tag{15}\\]\n' +
      '\n' +
      'Figure 1: Left: an illustration of a global rolling diffusion process and its local time reparameterization. The global diffusion time \\(t\\) (vertical axis) is mapped to a local time \\(t_{k}\\) for a frame \\(k\\) (horizontal axis). The local time is then used to compute the diffusion parameters \\(\\alpha_{t_{k}}\\) and \\(\\sigma_{t_{k}}\\). On the right, we show how the same _local_ schedule can be applied to each sequence of frames based on the frame index \\(w\\). The nontrivial part of sampling the generative process only occurs in the sliding window as it gets shifted over the sequence.\n' +
      '\n' +
      'In other words, we can _only focus the generative process on the frames that are in the sliding window_. Finally, note that we can _choose_ to not condition the model on all \\(\\mathbf{z}_{t}^{k}\\) that have \\(t_{k}\\) = 0, since frames that are far in the past are likely to be independent of the current frame, and this excessive conditioning would exceed computational constraints. As such, we get\n' +
      '\n' +
      '\\[p_{\\theta}(\\mathbf{z}_{s}^{\\text{win}}|\\mathbf{z}_{t})=p_{\\theta}(\\mathbf{z}_{s}^{\\text{ win}}|\\mathbf{z}_{t}^{\\text{clean}},\\mathbf{z}_{t}^{\\text{win}})\\,. \\tag{16}\\]\n' +
      '\n' +
      'In practice, we approximate \\(p_{\\theta}(\\mathbf{z}_{s}^{\\text{win}}|\\mathbf{z}_{t}^{\\text{clean}},\\mathbf{z}_{t}^{ \\text{win}}):=p_{\\theta}(\\mathbf{z}_{s}^{\\text{win}}|\\mathbf{z}^{\\widehat{\\text{clean} }},\\mathbf{z}_{t}^{\\text{win}})\\), where \\(\\widehat{\\mathbf{z}^{\\widehat{\\text{clean}}}}\\) denotes a specific (potentially empty) subset of \\(\\mathbf{z}_{t}^{\\text{clean}}\\) used for extra conditioning. This typically includes a few frames slightly before the current sliding window.\n' +
      '\n' +
      'In Appendix D, we formalize the above and show that the objective results in\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{win},\\theta}(\\mathbf{x}):= \\mathbb{E}_{t\\sim U(0,1),\\mathbf{\\epsilon}\\sim\\mathcal{N}(0,1)}\\left[ L_{\\text{win},\\theta}(\\mathbf{x};t,\\mathbf{\\epsilon})\\right] \\tag{17}\\]\n' +
      '\n' +
      'with\n' +
      '\n' +
      '\\[L_{\\text{win},\\theta}:=\\sum_{k\\in\\text{win}(t)}a(t_{k})||\\mathbf{x}^{k}-f_{\\theta} ^{k}(\\mathbf{z}_{t,\\mathbf{\\epsilon}}^{\\text{win}},\\widehat{\\mathbf{z}_{t,\\mathbf{\\epsilon}}^ {\\text{clean}}},t)||^{2}\\,,\\]\n' +
      '\n' +
      'where we suppress some arguments for notational convenience.\n' +
      '\n' +
      'Observe Figure 1 again. After training is completed, we can essentially sample from the generative model by traversing the image with the sliding window from the top left to the bottom right. This allows us to completely focus on the local environment, as discussed in the next section.\n' +
      '\n' +
      '### A local perspective\n' +
      '\n' +
      'In the last section, we saw that rolling diffusion allows us to focus the generative process as well as training solely on frames that are in the sliding window. As such, we now have \\(t_{\\text{win}}(t):[0,1]\\rightarrow[0,1]\\) denote a time reparameterization subject to the usual constraints. Note, though, that the shared time-component \\(t\\) is now defined locally. Specifically, running the denoising chain from \\(t=1\\) to \\(t=0\\) will only sample a rolling window such that the first frame is completely noiseless and the following frames still contain some noise. In contrast, the global process described earlier denoises an entire video.\n' +
      '\n' +
      'In principle, the design space of pure rolling diffusion models is enormous. For this reason, we make the following sensible assumptions in addition to the earlier constraints on the signal-to-noise schedule to allow for a sliding window sampling procedure. We would like \\(t_{\\text{win}}\\) to be:\n' +
      '\n' +
      '1. _local_, meaning we allow for sharing and reusing the parameterization across various positions of the sliding window, independent of their absolute locations.\n' +
      '2. _consistent under moving the window_, meaning that \\(t_{\\text{win}}(1)\\) should equal \\(t_{\\text{win}}(0)\\) of the next frame.\n' +
      '\n' +
      'Let \\(W<K\\) be the size of the sliding window, and \\(w\\in\\{0,\\dots,W-1\\}\\) be the local indices of the frames. To satisfy the first assumption, we define the schedule in terms of the local index \\(w\\) (see Figure 1 (right)).\n' +
      '\n' +
      '\\[t_{\\text{win}}:=t_{w}^{W}, \\tag{18}\\]\n' +
      '\n' +
      'where \\(t_{w}^{W}:[0,1]\\rightarrow[0,1]\\) is a monotonically increasing function. For the second, we know \\(t_{w}^{W}\\) must have\n' +
      '\n' +
      '\\[t_{w}^{W}=g((w+t)/W), \\tag{19}\\]\n' +
      '\n' +
      'for some monotonically increasing (in \\(t\\)) function \\(g:[0,1]\\rightarrow[0,1]\\). We will sometimes suppress \\(W\\) for notational convenience. Note that due to the locality of the parameterization, the process can be unfolded indefinitely at test time.\n' +
      '\n' +
      'A linear reparameterizationIn this work we typically put \\(g:=\\mathrm{id}\\), i.e.,\n' +
      '\n' +
      '\\[t_{w}^{\\text{lin}}=(w+t)/W\\,. \\tag{20}\\]\n' +
      '\n' +
      'where \\(w\\in\\{0,\\dots,W-1\\}\\). See Figure 1 (right) for an illustration of how this local schedule is applied to each sequence of frames. Observe that\n' +
      '\n' +
      '\\[t_{w}^{\\text{lin}}\\in[w/W,(w+1)/W]\\subseteq[0,1]\\,. \\tag{21}\\]\n' +
      '\n' +
      'One can extend the linear local time to include clean conditioning frames. Let \\(n_{\\text{cln}}\\) denote the number of clean frames (chosen as a hyperparameters), then the local time for a frame \\(w\\) is:\n' +
      '\n' +
      '\\[t_{w}^{\\text{lin}}(n_{\\text{cln}}):=\\mathrm{clip}\\left(\\frac{w+t-n_{\\text{cln }}}{W-n_{\\text{cln}}}\\right)\\,, \\tag{22}\\]\n' +
      '\n' +
      'where \\(\\mathrm{clip}:\\mathbb{R}\\rightarrow[0,1]\\) clips value between 0 and 1. Although it may seem that this contradicts Equation (19), this is a simple reparametrization.\n' +
      '\n' +
      '### Boundary conditions\n' +
      '\n' +
      'While framing rolling diffusion purely from a local perspective is convenient for training and sampling, it introduces complications at the boundaries of the sliding window. That is, given, e.g., the linear local time reparameterization \\(t_{w}^{\\text{lin}}\\), we have that given the diffusion time \\(t\\) running from 1 to 0, the local times run from \\((\\frac{1}{W},\\frac{2}{W},\\dots\\frac{W}{W})\\) to \\((\\frac{0}{W},\\frac{1}{W},\\dots,\\frac{W-1}{W})\\). Visually, in Figure 1, the rolling sampling procedure can be seen as moving the sliding window over the diagonal linearly from top left to bottom right such that the local times of the frames remain invariant upon shifting. However, this means that placing the window at the very left edge still results in having partially denoised frames. This means that in this local setting, the signal-to-noise ratios are never minimal, i.e., at full noise.\n' +
      '\n' +
      'To account for this, we _co-train_ the rolling diffusion model with an additional schedule that can handle this boundary condition.\n' +
      '\n' +
      '\\[t_{w}^{\\text{init}}:=\\operatorname{clip}\\left(\\frac{w}{W}+t\\right) \\tag{23}\\]\n' +
      '\n' +
      'This _init_ noise schedule can start from random noise and generates a video in the "rolling state". Note that this schedule cannot be used as a sliding window noise schedule, however, it contains \\(t_{w}^{\\text{lin}}\\) as we will see later. That is, at diffusion time \\(t=1\\), this will put all frames to maximum noise, and at \\(t=0\\) the frames will be in the rolling state. To be precise, it starts from local times \\((1,1,\\dots 1)\\) and denoises to \\((0,\\frac{1}{W},\\frac{2}{W},\\dots,\\frac{W-1}{W})\\), after which we can start the previously described local rolling diffusion process. From a visual perspective, in Figure 1, this corresponds to placing the window at the upper left corner and moving it down _vertically_, until it reaches the rolling state that can be used to continue diagonally.\n' +
      '\n' +
      'On the domain \\([0,\\frac{1}{W}]\\), this schedule contains the previous local schedule \\(t_{w}^{\\text{lin}}\\) as a special case This means that the model could be trained solely with \\(t_{w}^{\\text{init}}\\) to handle the boundaries as well as being able to roll out indefinitely. Note, however, that the \\(t_{w}^{\\text{lin}}\\) schedule only gets selected \\(1/W\\) of the time during training (assuming \\(t\\sim U(0,1)\\)). In contrast, this schedule is used almost exclusively at test time, with the exception being the boundary condition on the first \\(W\\) frames. As such, we found it beneficial to include both schedules during training. This blend is achieved by sampling one schedule or the other based on a Bernoulli hyperparameter \\(\\beta\\) controlling the probability of selecting between the two schedules.\n' +
      '\n' +
      '### Local training\n' +
      '\n' +
      'We consider any \\(\\mathbf{x}\\in\\mathbb{R}^{D\\times W}\\), i.e., we are now chunking videos into blocks of \\(W\\) frames From \\(t_{w}\\) one can compute \\(\\alpha_{t_{w}}\\) and \\(\\sigma_{t_{w}}\\) using any SNR schedule and the aforementioned reparameterizations. Note that when \\(n_{\\text{cln}}>0\\) we implicitly get extra clean conditioning. Let \\(\\mathbf{z}\\in\\mathbb{R}^{D\\times W}\\).\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{t}|\\mathbf{x}):=\\prod_{w=0}^{W}\\mathcal{N}(\\mathbf{z}_{t}^{w}|\\alpha_{t_{w }}\\mathbf{x}^{w},\\sigma_{t_{w}}^{2}\\mathbf{I}), \\tag{24}\\]\n' +
      '\n' +
      'The objective now becomes.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{loc},\\theta}(\\mathbf{x}):=\\mathbb{E}_{t\\sim U(0,1),\\mathbf{e}\\sim \\mathcal{N}(0,1)}\\left[L_{\\text{loc},\\theta}(\\mathbf{x};t,\\mathbf{\\epsilon})\\right], \\tag{25}\\]\n' +
      '\n' +
      'where we put\n' +
      '\n' +
      '\\[L_{\\text{loc},\\theta}(\\mathbf{x},t,\\mathbf{\\epsilon}):=\\sum_{w=0}^{W}a(t_{w})||\\mathbf{x}^ {w}-f_{\\theta}^{w}(\\mathbf{z}_{t,\\mathbf{\\epsilon}};t)||^{2}\\,. \\tag{26}\\]\n' +
      '\n' +
      'The training and sampling procedures are summarized in Algorithm 2, Algorithm 3, Algorithm 1. Furthermore, we provide a visual of the rolling sampling loop in Figure 6.\n' +
      '\n' +
      '```\n' +
      '0:\\(p_{\\theta}\\), \\(n_{\\text{cln}}\\), \\(\\mathbf{z}_{0}\\) with local diffusion times \\((0/W,\\dots,(W-1)/W)\\), as e.g. given by Algorithm 3.  Video Prediction \\(\\hat{\\mathbf{x}}\\leftarrow\\{\\mathbf{z}_{0}^{n_{\\text{cln}}}\\}\\) repeat  Sample \\(\\mathbf{z}^{W}\\sim\\mathcal{N}(0,\\mathbf{I})\\) \\(\\mathbf{z}_{1}\\leftarrow\\{\\mathbf{z}_{1}^{0},\\dots,\\mathbf{z}_{W-1}^{W-1},\\mathbf{z}^{W}\\}\\) for\\(t=1,(T-1)/T,\\dots,1/T\\)do  Compute local times \\(t_{w}^{\\text{lin}}(n_{\\text{cln}})\\), \\(w=0,\\dots,W-1\\)  Sample \\(\\mathbf{z}_{t-1/T}\\sim p_{\\theta}(\\mathbf{z}_{t-1/T}|\\mathbf{z}_{t})\\) endfor \\(\\hat{\\mathbf{x}}\\leftarrow\\hat{\\mathbf{x}}\\cup\\{\\mathbf{z}_{0}^{n_{\\text{cln}}}\\}\\) untilCompleted\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1** Rolling Diffusion: Rollout\n' +
      '\n' +
      '## 4 Video diffusion\n' +
      '\n' +
      'Video diffusion has been studied and applied directly in pixel space (Ho et al., 2022; Singer et al., 2022) and in latent space (Blattmann et al., 2023; Ge et al., 2023; He et al., 2022; Yu et al., 2023c), the latter typically empirically being slightly more effective. Furthermore, these videos usually extend the two-dimensional image setting to three (two spatial dimensions and one temporal dimension) without considering autoregressive extension.\n' +
      '\n' +
      'Figure 2: Sample Kolmogorov flow rollout. We observe that ground-truth structures are preserved initially, but the model diverges from the true data later on. Despite this, model is able to generate new turbulent dynamics much later on in the sequence.\n' +
      '\n' +
      'Methods that specifically treat test-time unrolling of video generation include Yang et al. (2023); Harvey et al. (2022). It was shown that directly parameterizing the conditional distribution of future frames given past frames is preferable (Harvey et al., 2022; Tashiro et al., 2021) as opposed to adapting the denoising schedule of an unconditional diffusion model. Note that these previous approaches never explicitly introduced a notion of time in their training procedure, as opposed to Rolling Diffusion. Specifically, Harvey et al. (2022) compare various such conditioning schemes, but do not explicitly consider a temporally adapted noise schedule.\n' +
      '\n' +
      '### Other time-series diffusion models\n' +
      '\n' +
      'Apart from video, sequential diffusion models have also been applied to other time-series data, such as audio (Kong et al., 2021), text (Li et al., 2022), but also scientifically to weather data (Price et al., 2023) or fluid mechanics (Kohl et al., 2023). Lippe et al. (2023) show that incorporating a diffusion-inspired denoising procedure can help recover high frequency information that typically gets lost when using learned numerical PDE solver emulators. Finally, Wu et al. (2023) also study autoregressive models with specialized noising schedules, focusing mostly on text generation.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      'We conduct experiments using data from various domains and explore several conditioning settings. In all our experiments, we use the _Simple Diffusion_ architecture (Hoogeboom et al., 2023) with equal parameters for both standard and rolling diffusion. Specifically, we use two-dimensional spatial convolution blocks after which, in the deepest layers, we have transformer blocks that operate (i.e., attend) both spatially and temporally.\n' +
      '\n' +
      '### Kolmogorov Flow\n' +
      '\n' +
      'First, we run an experiment on simulated fluid dynamics from JaxCFD (Kochkov et al., 2021; Dresdner et al., 2022). Specifically, we use the _Kolmogorov flow_, an instance of the incompressible Navier-Stokes equations. Recently, there has been increasing interest in emulating classical numerical PDE integrators with machine learning models. Various results have shown that these have the capacity to simulate from initial conditions complex systems to high precision (e.g., Li et al. (2020)). To similar ends, generative models are of increasing interest, as they provide several benefits. First, they provide a way to directly obtain marginal distributions over a future state of a physical system, as opposed to numerically rolling out an ensemble of initial conditions. This especially has use-cases in weather or climate modeling, fluid mechanics analyses, and stochastic differential equation studies. Second, they can improve modeling high data frequencies over approaches based on mean-squared error objectives.\n' +
      '\n' +
      'The simulation is based on the following partial differential equation \\(\\frac{\\partial\\mathbf{u}}{\\partial\\tau}+\\nabla\\cdot(\\mathbf{u}\\otimes\\mathbf{ u})=\\nu\\nabla^{2}\\mathbf{u}-\\frac{1}{\\rho}\\nabla p+\\mathbf{f}\\), with \\(\\mathbf{u}:[0,\\mathcal{T}]\\times\\mathbb{R}^{2}\\rightarrow\\mathbb{R}^{2}\\) is the solution, \\(\\otimes\\) the tensor product, \\(\\nu\\) the kinematic viscosity, \\(\\rho\\) the fluid density, \\(p\\) the pressure field, and, finally, \\(\\mathbf{f}\\) the external forcing. We randomly sample viscosities \\(\\nu\\) between \\(5\\cdot 10^{-4}\\) and \\(5\\cdot 10^{-3}\\), and densities between \\(0.5\\) and \\(2\\), making the task of predicting the dynamics non-deterministic. The ground truth data is generated using a finite volume-based direct numerical simulation (DNS) with a maximal time step of \\(0.05\\)s with 6 000 time-steps, corresponding to 300 seconds of simulation time, subsampled every 1.5s. Note that this is much longer than typical simulations, which only consider simulation times in the order of tens of seconds. We simulate 200 000 such samples at \\(64\\times 64\\), corresponding to 2 terabytes of data. Due to the chaotic nature and the long simulation time, a model cannot be expected to predict the exact future state, which makes it an ideal dataset to test long temporal rollouts. Details on the simulation settings can be found in Appendix B. The model is given 2 input frames containing the horizontal and vertical velocities. Note that the long rollout lengths (up to \\(\\pm\\) 60 seconds), together with the large 1.5s strides, make this a more challenging task than the usual "neural emulator" task. E.g., Lippe et al. (2023); Sun et al. (2023) only go up to \\(\\pm 15\\)s with much smaller strides.\n' +
      '\n' +
      'EvaluationTo measure how well the generated data matches the ground truth distribution, we propose a method similar to _Frechet Inception Distance_ (FID) or FVD. We make use of the fact that spatial frequency intensities pro\n' +
      '\n' +
      'Figure 3: FSD results of the Kolmogorov Flow rollout experiment. Lower is better.\n' +
      '\n' +
      'vide a good summary statistic (Sun et al., 2023; Dresdner et al., 2022; Kochkov et al., 2021). Let \\(\\mathbf{f_{\\mathbf{x}}}\\in\\mathbb{R}^{F}\\) denote a vector of spatial-spectral magnitudes computed using a (two-dimensional) _Discrete Fourier Transform_ (DFT) from a variable \\(\\mathbf{x}\\). Then, \\(\\mathbf{F}_{\\mathcal{D}}\\in N\\times F\\) denotes all the frequencies of a (test-time) dataset \\(\\mathcal{D}:=\\{\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{N}\\}\\). Let \\(\\mathbf{F}_{\\theta}\\) denote the Fourier magnitudes of \\(N\\) samples from a generative model. We now compute the Frechet distance between the generated samples and the true data by setting\n' +
      '\n' +
      '\\[\\mathrm{FSD}(\\mathcal{D},\\theta)\\\\ :=\\|\\mathbf{f_{\\mathcal{D}}}-\\mathbf{\\bar{f}_{\\theta}}\\|^{2}+\\mathrm{tr} \\Big{(}\\mathbf{\\Sigma}_{\\mathcal{D}}+\\mathbf{\\Sigma}_{\\theta}-2(\\mathbf{\\Sigma}_{\\mathcal{ D}}\\mathbf{\\Sigma}_{\\theta})^{1/2}\\Big{)} \\tag{27}\\]\n' +
      '\n' +
      'where \\(\\mathbf{\\bar{f}}\\) and \\(\\mathbf{\\Sigma}\\) denote the mean and covariance of the frequencies, respectively. We call this metric the _Frechet Spectral Distance_ (FSD).\n' +
      '\n' +
      'In Figure 3 we present FSD computed from the horizontal velocity fluids of the fluid. Regarding rolling diffusion, we use the \\(t_{w}^{\\text{init}}(n_{\\text{cln}})\\) reparameterization with \\(n_{\\text{cln}}=2\\), and use \\(t_{w}^{\\text{fin}}(n_{\\text{cln}})\\) for long rollouts. It is clear that an autoregressive MSE-based model, as typically used in the literature, is not suitable for this task. For standard diffusion, we iteratively generate \\(W-n_{\\text{cln}}\\) frames, after which we concatenate these to the conditioning and continue the rollout. Rolling diffusion always shifts the window by one, sampling using the process described before. We see that the rolling diffusion is able to consistently outperform the standard diffusion methods, regardless of various conditioning settings and window sizes, denoted with \'\\((n_{\\text{cln}},W-n_{\\text{cln}})\\)\'. We provide an example rollout in Figure 2, where we plot the _vorticity_ of the velocity field. We numerically present our results in Appendix C.\n' +
      '\n' +
      '### BAIR Robot Pushing Dataset\n' +
      '\n' +
      'The Berkeley AI Research (BAIR) robot pushing dataset (Ebert et al., 2017) is a standard benchmark for video prediction. It contains 44 000 videos at \\(64\\times 64\\) of a robot arm pushing objects around. Following previous methods, we condition in on 1 frame and predict the next 15. We evaluate, consistently with previous works, using the _Frechet Video Distance_ (FVD) (Unterthiner et al., 2019). For FVD, we use the I3D network (Carreira and Zisserman, 2017) by comparing \\(100\\times 256\\) model samples against the 256 examples in the evaluation set.\n' +
      '\n' +
      'Regarding rolling diffusion, we use the \\(t_{w}^{\\text{init}}(n_{\\text{cln}})\\) reparameterization to sample the \\(W=16\\) (\\(n_{\\text{cln}}=1\\)) frames to a partially denoised state, and then use \\(t_{w}^{\\text{lin}}(n_{\\text{cln}})\\) to rollout and complete the sampling. Note that standard diffusion samples all 15 frames at once and might be at an advantage since we do not consider autoregressive extension.\n' +
      '\n' +
      'The results are shown in Table 1. We observe that both standard diffusion and rolling diffusion using the same (Simple Diffusion) architecture outperform previous methods. Additionally, we see that there is no significant difference between the standard and rolling framework in this setting. This is because the sampled sequences are, in both cases, indistinguishable from the true data Figure 4. One could say that the models are can completely solve this task, yielding no significant difference in performance. However, it is still interesting that both models are able to outperform existing methods.\n' +
      '\n' +
      '### Kinetics-600\n' +
      '\n' +
      'Finally, we evaluate video prediction on the Kinetics-600 benchmark (Kay et al., 2017; Carreira et al., 2018). It contains approximately 400 000 training videos depicting 600\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Method & FVD (\\(\\downarrow\\)) \\\\ \\hline DVD-GAN (Clark et al., 2019) & 109.8 \\\\ VideoGPT (Yan et al., 2021) & 103.3 \\\\ TriVD-GAN-FP & 103.3 \\\\ Transframer (Nash et al., 2022) & 100 \\\\ CCVS (Le Moing et al., 2021) & 99 \\\\ VideoTransformer (Weissenborn et al., 2019) & 94 \\\\ FitVid (Babeizadeh et al., 2021) & 93.6 \\\\ NUWA (Wu et al., 2022) & 86.9 \\\\ Video Diffusion (Ho et al., 2022b) & 66.9 \\\\ \\hline Standard Diffusion (Ours) & **59.7** \\\\ Rolling Diffusion (Ours) & **59.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Results of the BAIR Robot Pushing baseline experiment.\n' +
      '\n' +
      'Figure 4: Top: Rolling Diffusion rollout on the BAIR Robot Pushing dataset. Bottom: ground-truth.\n' +
      '\n' +
      'different activities rescaled to \\(64\\times 64\\). We run two experiments using this dataset. The first is a baseline experiment in a setting equal to previously published works. The next one specifically tests Rolling Diffusion\'s ability to autoregressively rollout for long sequences.\n' +
      '\n' +
      'BaselineWe compare against previous methods using 5 input frames and 11 output frames, and show the results in Section 5.2 The evaluation metric is again FVD. We note that many of the current SOTA methods are _two-stage_, meaning that they use an autoencoder and run diffusion in latent space. While empirically compelling (Rombach et al., 2022), this makes it hard to isolate the effect of the diffusion model itself. Note that it is not always clear whether the autoencoder parameters are included in the parameter count for two-stage methods, or on what data they are pretrained. Running diffusion using the standard diffusion U-ViT architecture achieves an FVD of 3.9, which is comparable to the best two-stage methods. Rolling diffusion has a strong disadvantage in this case: (1) the baseline generates all frames at once; and (2) with a stride of 1, there is very little dynamics in the 16 frames, mostly suitable for a standard diffusion model. Still, rolling diffusion achieves a FVD of 5.2.\n' +
      '\n' +
      'RolloutNext, we compare the models\' capabilities to autoregressively rollout and show the results in Table 3. All settings use a window size of \\(W=16\\) frames, where set \\(n_{\\text{ch}}\\) to various settings, denoted with \'cond-gen\'. Furthermore, we train rolling diffusion with a mix of the default schedule and a _rescaled_ schedule. The default rolling diffusion schedule \\(t_{w}^{\\text{lin}}\\) is oversampled at a rate of \\(\\beta\\).\n' +
      '\n' +
      'We analyze two settings, one with a stride (also known as frame-skip or frame-step) of 1, rolling out for 64 steps, and another setting with a stride of 8 rolling out for 24 steps, effectively predicting ahead up to the \\(192\\)th frame. In the first setting, standard diffusion performs better, quite possibly due to the invariability of the data. We oversample the linear rolling schedule with a rate of \\(\\beta=0.9\\) to account for the high number of test-time steps. Rolling diffusion consistently wins in the second setting, which is much more dynamic. Note also that single-frame diffusion significantly underperforms here and that larger block autoregression is favorable. See an example rollout in Figure 5. From Appendix F, we get an indication of the effect of the oversampling rate on the performance of rolling diffusion. In this case, slightly oversampling \\(t_{w}^{\\text{lin}}\\) yields the best result.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'We presented _Rolling Diffusion Models_, a new DDPM framework that progressively noises (and denoises) data through time. Validating our method on video and fluid mechanics data, we observed that rolling diffusion\'s natural inductive bias gets most effectively exploited when the data is highly dynamic. This allows for exciting future directions in, e.g., video, audio, and weather or climate modeling.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Method & FVD (\\(\\downarrow\\)) \\\\ \\hline Phenaki (Villegas et al., 2022) & 36.4 \\\\ TrIVD-GAN-FP (Luc et al., 2020) & 25.7\\({}^{\\dagger}\\) \\\\ Video Diffusion (Ho et al., 2022) & 16.2 \\\\ RIN (Jabri et al., 2022) & 10.8 \\\\ MAGVIT (Yu et al., 2023) & 9.9\\({}^{\\dagger}\\) \\\\ MAGVITv2 (Yu et al., 2023) & 4.3\\({}^{\\dagger}\\) \\\\ W.A.L.T.-L (Gupta et al., 2023) & **3.3\\({}^{\\dagger}\\)** \\\\ \\hline Rolling Diffusion (Ours) & \\(5.2\\) \\\\ Standard Diffusion (Ours) & **3.9** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: FVD results of the Kinetics-600 baseline task (stride 1). Two-stage methods are indicated with \\({}^{\\dagger}\\)!’.\n' +
      '\n' +
      'Figure 5: Top: Rolling Diffusion rollout on the Kinetics-600 dataset. Bottom: ground-truth.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline  & Method & cond-gen & FVD \\\\ \\hline _stride=8_ & Standard Diffusion & (5-11) & 58.1 \\\\ _steps=24_ & Rolling \\(\\beta=0.1\\) & (5-11) & **39.8** \\\\ \\hline _stride=1_ & Standard Diffusion & (15-1) & 1369 \\\\ _steps=64_ & Standard Diffusion & (8-8) & 157.1 \\\\  & Standard Diffusion & (5-11) & **123.7** \\\\  & Rolling \\(\\beta=0.9\\) & (5-11) & 211.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Kinetics-600 (Rollout) with 8192 FVD samples, 100 sampling steps per 11 frames. Trained for 300k iterations.\n' +
      '\n' +
      '## Broader Impact\n' +
      '\n' +
      'Sequential generative models, including diffusion models, have a significant societal impact with applications in video generation and scientific research by enabling fast, highly detailed sampling. While they offer the upside of creating more accurate and compelling synthesis in fields ranging from climate modeling to medical imaging, there are notable downsides regarding content authenticity and originality of digital media.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Babaeizadeh et al. (2021) Babaeizadeh, M., Saffar, M. T., Nair, S., Levine, S., Finn, C., and Erhan, D. Fitvid: Overfitting in pixel-level video prediction. _arXiv preprint arXiv:2106.13195_, 2021.\n' +
      '* Blattmann et al. (2023) Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. Align your latents: High-resolution video synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 22563-22575, 2023.\n' +
      '* Carreira & Zisserman (2017) Carreira, J. and Zisserman, A. Quo vadis, action recognition? a new model and the kinetics dataset. In _proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 6299-6308, 2017.\n' +
      '* Carreira et al. (2018) Carreira, J., Noland, E., Banki-Horvath, A., Hillier, C., and Zisserman, A. A short note about kinetics-600. _arXiv preprint arXiv:1808.01340_, 2018.\n' +
      '* Clark et al. (2019) Clark, A., Donahue, J., and Simonyan, K. Adversarial video generation on complex datasets. _arXiv preprint arXiv:1907.06571_, 2019.\n' +
      '* Dresdner et al. (2022) Dresdner, G., Kochkov, D., Norgaard, P., Zepeda-Nunez, L., Smith, J. A., Brenner, M. P., and Hoyer, S. Learning to correct spectral methods for simulating turbulent flows. 2022. doi: 10.48550/ARXIV.2207.00556. URL [https://arxiv.org/abs/2207.00556](https://arxiv.org/abs/2207.00556).\n' +
      '* Ebert et al. (2017) Ebert, F., Finn, C., Lee, A. X., and Levine, S. Self-supervised visual planning with temporal skip connections. _CoRL_, 12:16, 2017.\n' +
      '* Gao et al. (2023) Gao, Y., Morioka, N., Zhang, Y., and Chen, N. E3 tts: Easy end-to-end diffusion-based text to speech. In _2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)_, pp. 1-8. IEEE, 2023.\n' +
      '* Ge et al. (2023) Ge, S., Nah, S., Liu, G., Poon, T., Tao, A., Catanzaro, B., Jacobs, D., Huang, J.-B., Liu, M.-Y., and Balaji, Y. Preserve your own correlation: A noise prior for video diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 22930-22941, 2023.\n' +
      '* Gupta et al. (2023) Gupta, A., Yu, L., Sohn, K., Gu, X., Hahn, M., Fei-Fei, L., Essa, I., Jiang, L., and Lezama, J. Photorealistic video generation with diffusion models. _arXiv preprint arXiv:2312.06602_, 2023.\n' +
      '* Harvey et al. (2022) Harvey, W., Naderiparizi, S., Masrani, V., Weilbach, C., and Wood, F. Flexible diffusion modeling of long videos. _Advances in Neural Information Processing Systems_, 35:27953-27965, 2022.\n' +
      '* He et al. (2022) He, Y., Yang, T., Zhang, Y., Shan, Y., and Chen, Q. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. _arXiv preprint arXiv:2211.13221_, 2022.\n' +
      '* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS_, 2020.\n' +
      '* Ho et al. (2022a) Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022a.\n' +
      '* Ho et al. (2022b) Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. _arXiv:2204.03458_, 2022b.\n' +
      '* Hoogeboom et al. (2023) Hoogeboom, E., Heek, J., and Salimans, T. simple diffusion: End-to-end diffusion for high resolution images. _arXiv preprint arXiv:2301.11093_, 2023.\n' +
      '* Jabri et al. (2022) Jabri, A., Fleet, D. J., and Chen, T. Scalable adaptive computation for iterative generation. _CoRR_, abs/2212.11972, 2022.\n' +
      '* Kawar et al. (2017) Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., and Irani, M. Imagic: Text-based real image editing with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 6007-6017, 2023.\n' +
      '* Kay et al. (2017) Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al. The kinetics human action video dataset. _arXiv preprint arXiv:1705.06950_, 2017.\n' +
      '* Kingma et al. (2021) Kingma, D. P., Salimans, T., Poole, B., and Ho, J. Variational diffusion models. _CoRR_, abs/2107.00630, 2021.\n' +
      '* Kochkov et al. (2021) Kochkov, D., Smith, J. A., Alieva, A., Wang, Q., Brenner, M. P., and Hoyer, S. Machine learning-accelerated computational fluid dynamics. _Proceedings of the National Academy of Sciences_, 118(21), 2021. ISSN 0027-8424. doi: 10.1073/pnas.2101784118. URL [https://www.pnas.org/content/118/21/e2101784118](https://www.pnas.org/content/118/21/e2101784118).\n' +
      '* Kohl et al. (2023) Kohl, G., Chen, L.-W., and Thuerey, N. Turbulent flow simulation using autoregressive conditional diffusion models. _arXiv preprint arXiv:2309.01745_, 2023.\n' +
      '* Kong et al. (2021) Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. DiffWave: A versatile diffusion model for audio synthesis. In _9th International Conference on Learning Representations, ICLR_, 2021.\n' +
      '* Le Moing et al. (2021) Le Moing, G., Ponce, J., and Schmid, C. Ccvs: context-aware controllable video synthesis. _Advances in Neural Information Processing Systems_, 34:14042-14055, 2021.\n' +
      '* Le et al. (2021)* Li et al. (2022) Li, X., Thickstun, J., Gulrajani, I., Liang, P. S., and Hashimoto, T. B. Diffusion-lm improves controllable text generation. _Advances in Neural Information Processing Systems_, 35:4328-4343, 2022.\n' +
      '* Li et al. (2020) Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. Fourier neural operator for parametric partial differential equations. _arXiv preprint arXiv:2010.08895_, 2020.\n' +
      '* Lippe et al. (2023) Lippe, P., Veeling, B. S., Perdikaris, P., Turner, R. E., and Brandstetter, J. Pde-refiner: Achieving accurate long rollouts with neural pde solvers. _arXiv preprint arXiv:2308.05732_, 2023.\n' +
      '* Luc et al. (2020) Luc, P., Clark, A., Dieleman, S., Casas, D. d. L., Doron, Y., Cassirer, A., and Simonyan, K. Transformation-based adversarial video prediction on large-scale data. _arXiv preprint arXiv:2003.04035_, 2020.\n' +
      '* Meng et al. (2022) Meng, C., Gao, R., Kingma, D. P., Ermon, S., Ho, J., and Salimans, T. On distillation of guided diffusion models. _CoRR_, abs/2210.03142, 2022.\n' +
      '* Nash et al. (2022) Nash, C., Carreira, J., Walker, J., Barr, I., Jaegle, A., Malinowski, M., and Battaglia, P. Transframer: Arbitrary frame prediction with generative models. _arXiv preprint arXiv:2203.09494_, 2022.\n' +
      '* Price et al. (2023) Price, I., Sanchez-Gonzalez, A., Alet, F., Ewalds, T., El-Kadi, A., Stott, J., Mohamed, S., Battaglia, P., Lam, R., and Willson, M. Gencast: Diffusion-based ensemble forecasting for medium-range weather. _arXiv preprint arXiv:2312.15796_, 2023.\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with CLIP latents. _CoRR_, abs/2204.06125, 2022.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pp. 10674-10685. IEEE, 2022.\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., and Norouzi, M. Photorealistic text-to-image diffusion models with deep language understanding. _CoRR_, abs/2205.11487, 2022.\n' +
      '* Singer et al. (2022) Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D., Gupta, S., and Taigman, Y. Make-a-video: Text-to-video generation without text-video data. _CoRR_, abs/2209.14792, 2022.\n' +
      '* Sohl-Dickstein et al. (2015) Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In Bach, F. R. and Blei, D. M. (eds.), _Proceedings of the 32nd International Conference on Machine Learning, ICML_, 2015.\n' +
      '* Song & Ermon (2019) Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. In _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS_, 2019.\n' +
      '* StabilityAI (2023) StabilityAI. Introducing stable video diffusion. Nov 2023. URL [https://stability.ai/news/stable-video-diffusion-open-ai-video-model](https://stability.ai/news/stable-video-diffusion-open-ai-video-model). Accessed: 2024-01-25.\n' +
      '* Sun et al. (2023) Sun, Z., Yang, Y., and Yoo, S. A neural pde solver with temporal stencil modeling. _arXiv preprint arXiv:2302.08105_, 2023.\n' +
      '* Tashiro et al. (2021) Tashiro, Y., Song, J., Song, Y., and Ermon, S. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. _Advances in Neural Information Processing Systems_, 34:24804-24816, 2021.\n' +
      '* Unterthiner et al. (2019) Unterthiner, T., van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., and Gelly, S. Fvd: A new metric for video generation. 2019.\n' +
      '* Villegas et al. (2022) Villegas, R., Babaeizadeh, M., Kindermans, P.-J., Moraldo, H., Zhang, H., Saffar, M. T., Castro, S., Kunze, J., and Erhan, D. Phenaki: Variable length video generation from open domain textual description. _arXiv preprint arXiv:2210.02399_, 2022.\n' +
      '* Weissenborn et al. (2019) Weissenborn, D., Tackstrom, O., and Uszkoreit, J. Scaling autoregressive video models. _arXiv preprint arXiv:1906.02634_, 2019.\n' +
      '* Wu et al. (2022) Wu, C., Liang, J., Ji, L., Yang, F., Fang, Y., Jiang, D., and Duan, N. Niwa: Visual synthesis pre-training for neural visual world creation. In _European conference on computer vision_, pp. 720-736. Springer, 2022.\n' +
      '* Wu et al. (2023) Wu, T., Fan, Z., Liu, X., Gong, Y., Shen, Y., Jiao, J., Zheng, H.-T., Li, J., Wei, Z., Guo, J., et al. Ar-diffusion: Autoregressive diffusion model for text generation. _arXiv preprint arXiv:2305.09515_, 2023.\n' +
      '* Yan et al. (2021) Yan, W., Zhang, Y., Abbeel, P., and Srinivas, A. Videogpt: Video generation using vq-vae and transformers. _arXiv preprint arXiv:2104.10157_, 2021.\n' +
      '* Yang et al. (2023) Yang, R., Srivastava, P., and Mandt, S. Diffusion probabilistic modeling for video generation. _Entropy_, 25(10):1469, 2023.\n' +
      '* Zhang et al. (2021)Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., and Wu, Y. Scaling autoregressive models for content-rich text-to-image generation. _CoRR_, abs/2206.10789, 2022.\n' +
      '* Yu et al. (2023a) Yu, L., Cheng, Y., Sohn, K., Lezama, J., Zhang, H., Chang, H., Hauptmann, A. G., Yang, M.-H., Hao, Y., Essa, I., et al. Magvit: Masked generative video transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 10459-10469, 2023a.\n' +
      '* Yu et al. (2023b) Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A. G., et al. Language model beats diffusion-tokenizer is key to visual generation. _arXiv preprint arXiv:2310.05737_, 2023b.\n' +
      '* Yu et al. (2023c) Yu, S., Sohn, K., Kim, S., and Shin, J. Video probabilistic diffusion models in projected latent space. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 18456-18466, 2023c.\n' +
      '\n' +
      'A Hyperparameters\n' +
      '\n' +
      'In this section we denote the hyperparameters for the different experiments. Throughout the experiments we use U-ViTs which are essentially U-Nets with MLP Blocks instead of convolutional layers when self-attention is used in a block. In the PDE experiments are relatively small architecture is used. For BAIR we used a larger architecture, increasing both the channel count and the number of blocks. For K600 we used even larger architectures, because this dataset turned out to be the most difficult to fit. For all the specifications see Table 4, 5 and 6.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Parameter & Value \\\\ \\hline Blocks & [4 + 4, 4 + 4, 5 + 5, 8] \\\\ Channels & [256, 512, 2048, 4096] \\\\ Block Type & [Conv2D, Conv2D, Transformer (axial), Transformer] \\\\ Block Type & [Conv2D, Conv2D, Transformer (axial), Transformer] \\\\ Head Dim & 128 \\\\ Dropout & [0, 0, 0.1, 0.1] \\\\ Downsample & (1, 2, 2) \\\\ Model parametrization & \\(v\\) \\\\ Loss & \\(\\epsilon\\)-loss (\\(x\\)-loss with SNR + 1 weighting) \\\\ Model parametrization & \\(v\\) \\\\ Loss & \\(\\epsilon\\)-loss (\\(x\\)-loss with SNR + 1 weighting) \\\\ Number of Steps & 300 000 (rollout experiments) / 570 000 (standard experiments) \\\\ EMA decay & 0.9999 \\\\ EMA decay & 0.9999 \\\\ learning rate & 1e-4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: PDE Experiments\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Parameter & Value \\\\ \\hline Blocks & [3 + 3, 3 + 3, 3 + 3, 8] \\\\ Channels & [128, 256, 512, 1024] \\\\ Block Type & [Conv2D, Conv2D, Transformer (axial), Transformer] \\\\ Head Dim & 128 \\\\ Dropout & [0, 0.1, 0.1, 0.1] \\\\ Downsample & (1, 2, 2) \\\\ Loss & \\(\\epsilon\\)-loss (\\(x\\)-loss with SNR + 1 weighting) \\\\ Number of Steps & 100 000 (rollout experiments) / 570 000 (standard experiments) \\\\ EMA decay & 0.9999 \\\\ learning rate & 1e-4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: BAIR Experiment\n' +
      '\n' +
      '## Appendix B Simulation Details\n' +
      '\n' +
      'In this section we present the hyperparameters to generate the Kolmogorov Flow simulation data, see Table 7. It is important to note that to introduce uncertainty into an otherwise deterministic system, we vary the viscosity and density parameters, which must then be inferred from the data. This would also make a standard solver very difficult to use in such a setting. Additionally, due to the chaotic nature of the system, it is not deterministically predictable up to arbitrary precision.\n' +
      '\n' +
      'We use the "simple turbulence forcing" option, which combines a driving force with a damping term such that we simulate turbulence in two dimensions.\n' +
      '\n' +
      '## Appendix C Additional Results\n' +
      '\n' +
      'We show in Table 8 the MSE and FSE errors at various time-steps. Note that the MSE model is always optimal in terms of MSE loss, which is as expected. However, in terms of matching the frequency distribution, as measured by FSD, standard diffusion, and in particular rolling diffusion are optimal.\n' +
      '\n' +
      '## Appendix D Rolling Diffusion Objective\n' +
      '\n' +
      'Standard DiffusionFor completeness, we briefly review the diffusion objective. Let \\(T\\in\\mathbb{N}\\) be a finite number of diffusion steps, and \\(i\\in\\{0,\\ldots,T\\}\\) be a diffusion time-step. Kingma et al. (2021) show that the discrete-time \\(D_{\\mathrm{KL}}\\) between \\(q(\\mathbf{x},\\mathbf{z}_{0},\\ldots,\\mathbf{z}_{T})\\) and \\(p(\\mathbf{x},\\mathbf{z}_{0},\\ldots,\\mathbf{z}_{T})\\) can be decomposed as\n' +
      '\n' +
      '\\[D_{\\mathrm{KL}}\\left(q(\\mathbf{x},\\mathbf{z}_{0},\\ldots,\\mathbf{z}_{T})||p( \\mathbf{x},\\mathbf{z}_{0},\\ldots,\\mathbf{z}_{T})\\right) =\\mathbb{E}_{q(\\mathbf{x},\\mathbf{z}_{0},\\ldots,\\mathbf{z}_{T})}\\left[\\log q (\\mathbf{x},\\mathbf{z}_{0},\\ldots,\\mathbf{z}_{T})-\\log p(\\mathbf{x},\\mathbf{z}_{0},\\ldots,\\mathbf{z}_ {T})\\right] \\tag{28}\\] \\[=c+\\underbrace{D_{\\mathrm{KL}}(q(\\mathbf{z}_{T}|\\mathbf{x})||p(\\mathbf{z}_{T }))}_{\\text{Prior Loss}}+\\underbrace{\\mathbb{E}_{q(\\mathbf{z}_{0}|\\mathbf{x})}\\left[- \\log p(\\mathbf{x}|\\mathbf{z}_{0})\\right]}_{\\text{Reconstruction Loss}}+\\mathcal{L}_{D}, \\tag{29}\\]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Parameter & Value \\\\ \\hline Size & 256 \\\\ Viscosity & Uniform random in \\([5.0\\times 10^{-4},5.0\\times 10^{-3}]\\) \\\\ Density & Uniform random in \\([2^{-1},2^{1}]\\) \\\\ Maximum Velocity & 7.0 \\\\ CFL Safety Factor & 0.5 \\\\ Max \\(\\Delta t\\) & 0.05 \\\\ Outer Steps & 6000 \\\\ Grid & 256 \\(\\times\\) 256, domain \\([0,2\\pi]\\times[0,2\\pi]\\) \\\\ Initial Velocity & Filtered velocity field, 3 iterations \\\\ Forcing & Simple turbulence, magnitude=2.5, linear coefficient=-0.1, wavenumber=4 \\\\ Total Simulations & 200,000 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Parameters for Kolmogorov Flow Simulation using JaxCFD\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline  & & \\multicolumn{6}{c}{FSD/MSE \\(\\oplus\\)} \\\\ Method & 1 & 2 & 4 & 8 & 12 & 24 & 48 \\\\ \\hline MSE (2-1) & 304.7 / **14.64** & 687.1 / **137.15** & 1007 / **407.0** & 1649 / **407.0** & 2230 / **407.0** & 5453 / **407.0** & 7504 / **407.0** \\\\ MSE (2-2) & 531.3 / 20.1 & 7205 / 148.3 & 5996 / 407.0 & 7277 / 407.0 & 8996 / 406.1 & 1 \\(\\cdot 10^{4}\\) / 407.0 & 1 \\(\\cdot 10^{4}\\) / 407.0 \\\\ MSE (2-4) & 304.7 / 21.7 & 6684 / 148.9 & 6- 104 / 378.9 & 3 - 10\\(\\cdot 10^{4}\\) / 407.0 & 2 \\(\\cdot 10^{4}\\) / 407.0 & 2 \\(\\cdot 10^{4}\\) / 407.0 \\\\ \\hline Standard Diffusion (2-1) & 39.9 / 267.6 & 573.7 / 192.6 & 142.0 / 710.0 & 297.7 / 794.6 & 399.4 / 773.1 & 442.5 / 758.3 & 763.6 / 732.5 \\\\ Standard Diffusion (2-2) & 59.1 / 47.88 & 86.1 / 334.9 & 112.6 / 167.6 & 32.4 / 81.8 / 15.3 & 3147 / 755.5 & 403.3 / 725.0 & 726.3 / 695.4 \\\\ Standard Diffusion (2-4) & 86.19 / 49.93 & 141.6 / 355.6 & 246.6 / 753.2 & 397.8 / 758.0 & 555.6 / 726.9 & 1094.0 / 701.1 & 2401.0 / 666.3 \\\\ Standard Diffusion (2-8) & 87.0 / 54.0 & 137.7 / 399.2 & 288.3 / 770.1 & 338.7 / 725.3 & 355.5 / 713.6 & 530.6 / 705.5 & 1159 / 748.3 \\\\ \\hline Rolling Diffusion (init noise) (2-4) & 29.59 / 39.72 & **47.44** / 287.8 & **43.39** / 738.4 & **61.93** / 769.0 & 214.32 / 735.7 & 648.53 / 699.1 & 1238.44 / 670.0 \\\\ Rolling Diffusion (init noise) (2-8) & **27.68** / 41.22 & 52.41 / 316.9 & 53.47 / 768.0 & 98.29 / 777.2 & **187.03** / 74.8 & **34.89** / 737.6 & **417.99** / 719.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Kolmogorov Flow Resultswhere \\(c\\) is a data entropy term. The prior and reconstruction loss terms are typically negligible. \\(\\mathcal{L}_{D}\\) is the _diffusion loss_, which is defined as\n' +
      '\n' +
      '\\[\\mathcal{L}_{D}=\\sum_{i=1}^{T}\\mathbb{E}_{q(\\mathbf{z}_{i}|\\mathbf{x})}\\left[D_{\\rm KL}( q(\\mathbf{z}_{s_{i}}|\\mathbf{z}_{t_{i}},\\mathbf{x})||p(\\mathbf{z}_{s_{i}}|\\mathbf{z}_{t_{i}})) \\right]\\,, \\tag{30}\\]\n' +
      '\n' +
      'Further, when \\(T\\to\\infty\\), taking care that \\(s\\to t\\) we get _continuous_ analog of Equation (30) (Kingma et al., 2021):\n' +
      '\n' +
      '\\[\\mathcal{L}_{w}:=\\frac{1}{2}\\mathbb{E}_{t\\sim U(0,1),\\mathbf{\\epsilon}\\sim\\mathcal{ N}(0,1)}\\left[w(\\lambda)\\cdot-\\frac{d\\lambda_{t}}{dt}\\|\\hat{\\mathbf{e}}_{\\theta}( \\mathbf{z}_{t};\\lambda_{t})-\\mathbf{\\epsilon}\\|^{2}\\right]\\,. \\tag{31}\\]\n' +
      '\n' +
      'with \\(w(\\lambda_{t})=1\\), where \\(\\lambda_{t}=\\log{\\rm SNR}(t)\\). The weighting function \\(w(\\lambda_{t})=1\\) is often changed to improve image quality, for instance by being the inverse \\(-1/\\frac{d\\lambda_{t}}{dt}\\) so that the objective is simply constant over \\(\\mathbf{\\epsilon}\\)-loss.\n' +
      '\n' +
      'Rolling Diffusion ObjectiveIn rolling diffusion, the signal-to-noise ratio is kept fixed but one has to account for the local time reparameterization. Recall that \\(t_{k}:=t_{k}(t)\\) denotes the local time reparameterization. Concisely we can say \\({\\rm SNR}_{k}(t):={\\rm SNR}(t_{k})\\). The rolling continuous time objective is:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\infty}=\\mathbb{E}_{t\\sim U(0,1),\\mathbf{\\epsilon}\\sim\\mathcal{N}(0, 1)}\\left[\\sum_{k=1}^{K}-\\frac{d\\lambda(t_{k})}{dt_{k}}\\frac{t_{k}(t)}{dt}\\cdot \\|\\mathbf{\\epsilon}^{k}-\\hat{\\mathbf{e}}_{\\theta}^{k}(\\mathbf{z}_{\\mathbf{\\epsilon},t};t)\\|^{ 2}\\right]\\,, \\tag{32}\\]\n' +
      '\n' +
      'where \\(\\lambda(t):=\\log{\\rm SNR}(t)\\).\n' +
      '\n' +
      'Recall the frame categorization of the main paper, i.e.,\n' +
      '\n' +
      '\\[\\text{clean}(s,t) :=\\left\\{k\\mid s_{k}=t_{k}=0\\right\\}, \\tag{33}\\] \\[\\text{noise}(s,t) :=\\left\\{k\\mid s_{k}=t_{k}=1\\right\\},\\] (34) \\[\\text{win}(s,t) :=\\left\\{k\\mid s_{k}\\in[0,1),t_{k}\\in(s_{k},1]\\right\\}, \\tag{35}\\]\n' +
      '\n' +
      'we can see that \\(t_{k}^{\\prime}(t)=0\\) for \\(k\\in\\text{clean}(t-dt,t)\\) and \\(k\\in\\text{noise}(t-dt,t)\\) and thus the objective only has non-zero loss over the window:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\infty}=\\mathbb{E}_{t\\sim U(0,1),\\mathbf{\\epsilon}\\sim\\mathcal{N}(0, 1)}\\left[\\sum_{k\\in\\text{win}(t)}-\\frac{d\\lambda(t_{k})}{dt_{k}}\\frac{t_{k}(t )}{dt}\\cdot\\|\\mathbf{\\epsilon}^{k}-\\hat{\\mathbf{e}}_{\\theta}^{k}(\\mathbf{z}_{\\mathbf{\\epsilon},t};t)\\|^{2}\\right]\\,, \\tag{36}\\]\n' +
      '\n' +
      'This derivation shows a subtle difference between the global perspective (as used above) and the local perspective. If variational bounds are used, they are equivalent and the reparametrization of the time derivative takes into account the edge conditions such as fully noisy frames and fully determined frames. However, constant \\(\\mathbf{\\epsilon}\\) or \\(v\\) losses on the global perspective _do_ differ from the local perspective, as they do not vanish to zero at \\(t=0\\) and \\(t=1\\).\n' +
      '\n' +
      'Empirically, \\(\\mathbf{\\epsilon}\\)-loss on the window achieved good performance against \\(v\\)-loss. Our rolling diffusion objective could be viewed in two ways: As a constant \\(\\mathbf{\\epsilon}\\) defined on the window, _or_ a globally defined constant \\(\\mathbf{\\epsilon}\\)-loss that masks out everything but the window, essentially taking into account the derivative of the time reparametrization outside of the window.\n' +
      '\n' +
      '## Appendix E Algorithms\n' +
      '\n' +
      'We present algorithmic outlines for training of rolling diffusion as well as sampling at the boundary. The algorithm for autoregressive rollout was presented in the main text.\n' +
      '\n' +
      '## Appendix F Hyperparameter Search for \\(\\beta\\)\n' +
      '\n' +
      '## Appendix G Rescaled Noise Schedule\n' +
      '\n' +
      'For our Kinetics-600 experiments, we used a different noise schedule which can sample from complete noise towards a "rolling state", i.e., at diffusion times \\((\\frac{1}{W},\\frac{2}{W},\\ldots\\frac{W}{W})\\). From there, we can roll out generation using, e.g., the linear rolling sampling schedule \\(t_{w}^{\\text{lin}}\\). The reason is that we hypothesized that the noise schedule \\(t_{w}^{\\text{init}}\\) uses a \\(\\operatorname{clip}\\) operation, which means that will be sampling in \\(\\operatorname{clean}(s,t)\\), which is redundant as outlined in the main paper and Appendix D.\n' +
      '\n' +
      'Another schedule that starts from complete noise and ends at the rolling state is the following:\n' +
      '\n' +
      '\\[t_{w}^{\\text{init,resc}}(t):=\\frac{w}{W}+t\\cdot(1-\\frac{w}{W}) \\tag{37}\\]\n' +
      '\n' +
      'Where we clearly have at \\(t=\\frac{1}{W}\\) that the local times are \\(\\big{(}\\frac{1}{W},\\frac{2}{W},\\ldots,\\frac{W}{W}\\big{)}\\), which is what we need. Note that this schedule is not directly proportional to \\(t\\).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Require:**\\(\\mathcal{D}_{\\text{tr}}:=\\{\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{N}\\}\\), \\(\\mathbf{x}\\in\\mathbb{R}^{D\\times W}\\), \\(n_{\\text{ch}}\\), \\(\\beta\\), \\(f_{\\theta}\\) \\\\\n' +
      '**repeat** & Sample \\(\\mathbf{x}\\) from \\(\\mathcal{D}_{\\text{tr}}\\), \\(t\\sim U(0,1)\\), \\(y\\sim B(\\beta)\\) \\\\\n' +
      '**if**\\(y\\)**then** \\\\   Compute local time \\(t_{w}^{\\text{init}}(n_{\\text{ch}})\\), \\(w=0,\\ldots,W-1\\) \\\\\n' +
      '**else** \\\\   Compute local time \\(t_{w}^{\\text{lin}}(n_{\\text{ch}})\\), \\(w=0,\\ldots,W-1\\) \\\\\n' +
      '**end if** \\\\   Compute \\(\\alpha_{t_{w}}\\) and \\(\\sigma_{t_{w}}\\) for all \\(w=0,\\ldots,W-1\\) \\\\   Sample \\(\\mathbf{z}_{t}\\sim\\sigma(\\mathbf{z}_{t}|\\mathbf{x})\\) using Equation (24) (reparameterized from \\(\\mathbf{\\epsilon}\\sim\\mathcal{N}(0,1)\\)) \\\\   Compute \\(\\hat{\\mathbf{x}}\\leftarrow f_{\\theta}(\\mathbf{z}_{t,\\mathbf{\\epsilon}};t)\\) \\\\   Update \\(\\theta\\) using \\(L_{\\text{loc},\\theta}(\\mathbf{x};t,\\mathbf{\\epsilon})\\) \\\\\n' +
      '**until** Converged \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: Comparison of oversampling rolling v.s. init noise on Kinetics-600 (stride 8 rollout) with 8192 FVD samples, 100 steps per 11 frames.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>